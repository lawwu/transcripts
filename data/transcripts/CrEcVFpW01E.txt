
[00:00:00.000 --> 00:00:00.960]   All right, hi guys.
[00:00:00.960 --> 00:00:04.880]   We're the Image to Latex team, and we
[00:00:04.880 --> 00:00:06.920]   have been working really hard on this project
[00:00:06.920 --> 00:00:08.400]   for the past eight weeks.
[00:00:08.400 --> 00:00:12.440]   And we want to share with you guys what we have done so far.
[00:00:12.440 --> 00:00:14.680]   So real quick, it's our team.
[00:00:14.680 --> 00:00:16.600]   And our team has four members--
[00:00:16.600 --> 00:00:19.280]   Danny, Elle, Elloc, and myself, Kathy.
[00:00:19.280 --> 00:00:21.360]   And those are the picture of our face.
[00:00:21.360 --> 00:00:28.640]   And real quickly, what is the objective of our project,
[00:00:28.640 --> 00:00:30.960]   which is, of course, using deep learning
[00:00:30.960 --> 00:00:33.600]   to translate an image of a mathematic equation
[00:00:33.600 --> 00:00:34.560]   to latex code.
[00:00:34.560 --> 00:00:36.240]   And here is an example.
[00:00:36.240 --> 00:00:38.560]   This is a picture of a mathematic equation,
[00:00:38.560 --> 00:00:41.320]   and here is a corresponding latex code.
[00:00:41.320 --> 00:00:43.220]   And I wanted to talk about a little bit why
[00:00:43.220 --> 00:00:45.880]   we wanted to do this project, because as a data scientist,
[00:00:45.880 --> 00:00:47.960]   or for many researchers, we often
[00:00:47.960 --> 00:00:52.120]   find people in the need of trying to find the latex,
[00:00:52.120 --> 00:00:54.940]   if it's to write a paper, write a Jupyter Notebook,
[00:00:54.940 --> 00:00:57.920]   or even answering Stack Overflow questions.
[00:00:57.920 --> 00:01:00.320]   But latex code is very daunting to start with,
[00:01:00.320 --> 00:01:02.400]   and a lot of people just don't want to write them.
[00:01:02.400 --> 00:01:05.000]   So we figure, why don't we automate this process
[00:01:05.000 --> 00:01:06.680]   and give the time back to people,
[00:01:06.680 --> 00:01:10.320]   focus on what really matters to them, which is their work.
[00:01:10.320 --> 00:01:12.720]   So that led us to this project.
[00:01:12.720 --> 00:01:14.320]   And continue.
[00:01:14.320 --> 00:01:16.000]   So the first is, of course, we need
[00:01:16.000 --> 00:01:17.640]   to see if we have data set.
[00:01:17.640 --> 00:01:20.160]   And lucky for us, this is a problem
[00:01:20.160 --> 00:01:22.080]   that has been previously worked on.
[00:01:22.080 --> 00:01:25.160]   And there is free available data set on the internet
[00:01:25.160 --> 00:01:26.180]   you can just download.
[00:01:26.180 --> 00:01:28.880]   So that gives us a head start.
[00:01:28.880 --> 00:01:31.080]   And here are some examples of the data set.
[00:01:31.080 --> 00:01:34.960]   You can see already that our training data set, or images,
[00:01:34.960 --> 00:01:36.600]   are not the same size.
[00:01:36.600 --> 00:01:38.960]   But that's not the only characteristics of our data
[00:01:38.960 --> 00:01:39.460]   set.
[00:01:39.460 --> 00:01:42.000]   Another thing is they're heavily pre-processed
[00:01:42.000 --> 00:01:44.320]   by the prior research team, which
[00:01:44.320 --> 00:01:46.760]   turns out to be a great limitation of our model,
[00:01:46.760 --> 00:01:49.960]   and it will be discussed later in the presentation.
[00:01:49.960 --> 00:01:52.920]   And a few other things I wanted to stress about our data set
[00:01:52.920 --> 00:01:57.140]   is we're essentially dealing with a very high dimensionality
[00:01:57.140 --> 00:01:57.640]   problem.
[00:01:57.640 --> 00:01:59.140]   Because if you think about it, there
[00:01:59.140 --> 00:02:02.580]   are up to 400 different syntax in latex code.
[00:02:02.580 --> 00:02:05.980]   And our model has to not only pick the right syntax,
[00:02:05.980 --> 00:02:08.620]   but make sure they put them into correct order.
[00:02:08.620 --> 00:02:10.980]   So we're talking about tens of thousands
[00:02:10.980 --> 00:02:13.340]   potential dimensionality as an output.
[00:02:13.340 --> 00:02:15.420]   And that's just a great difficulty
[00:02:15.420 --> 00:02:18.660]   strength to our project.
[00:02:18.660 --> 00:02:21.340]   And with that being said, we created our base model.
[00:02:21.340 --> 00:02:25.360]   So what we did is we kind of researched online
[00:02:25.360 --> 00:02:28.520]   to see if there is a quick and dirty way that we can just
[00:02:28.520 --> 00:02:31.360]   create a base model, and to see where that takes us to.
[00:02:31.360 --> 00:02:34.840]   So interestingly, we find one of the TensorFlow tutorial,
[00:02:34.840 --> 00:02:37.600]   which is doing a little bit similar thing,
[00:02:37.600 --> 00:02:39.720]   but that is for image captioning.
[00:02:39.720 --> 00:02:42.680]   So basically, you give an image and you make a summary of it.
[00:02:42.680 --> 00:02:45.040]   The reason I say it's similar is because they are also
[00:02:45.040 --> 00:02:47.920]   using an encoder and a decoder, which is kind of the approach
[00:02:47.920 --> 00:02:49.680]   that we want to take.
[00:02:49.680 --> 00:02:51.280]   So we followed that tutorial.
[00:02:51.280 --> 00:02:54.100]   We created our vanilla base model.
[00:02:54.100 --> 00:02:57.420]   What we did is we rescaled our image into the same size,
[00:02:57.420 --> 00:02:58.940]   just for the time sake.
[00:02:58.940 --> 00:03:02.260]   And we created a vanilla CNN encoder.
[00:03:02.260 --> 00:03:05.460]   And the output is being pushed into a decoder, which
[00:03:05.460 --> 00:03:08.740]   is constructed by GRU layer.
[00:03:08.740 --> 00:03:11.620]   We also implemented a Batman--
[00:03:11.620 --> 00:03:13.220]   sorry, I can never pronounce that word--
[00:03:13.220 --> 00:03:15.700]   Batman-style attention to our model.
[00:03:15.700 --> 00:03:17.660]   We made sure to overfit one batch
[00:03:17.660 --> 00:03:19.700]   to make sure there's no bug.
[00:03:19.700 --> 00:03:24.880]   And we got a number that is extremely close to zero.
[00:03:24.880 --> 00:03:27.720]   So we thought, OK, we can train on the entire data set.
[00:03:27.720 --> 00:03:30.880]   And you can see that after 14 epochs of training
[00:03:30.880 --> 00:03:34.100]   on entire training data set, we got lost at 0.6.
[00:03:34.100 --> 00:03:35.720]   And we cannot get a lower.
[00:03:35.720 --> 00:03:36.640]   So that is the point.
[00:03:36.640 --> 00:03:39.120]   And we figure, OK, we need to really become creative
[00:03:39.120 --> 00:03:41.200]   and figure out a way to create a model that
[00:03:41.200 --> 00:03:43.600]   is fitting for our own problem.
[00:03:43.600 --> 00:03:45.800]   So that will lead us to the architecture
[00:03:45.800 --> 00:03:47.720]   of the model and the results, which
[00:03:47.720 --> 00:03:49.700]   will be talked about by Al.
[00:03:49.700 --> 00:03:50.200]   All right.
[00:03:50.200 --> 00:03:50.700]   Cool.
[00:03:50.700 --> 00:03:58.040]   All right, so our final model architecture basically
[00:03:58.040 --> 00:04:00.480]   consists of three main components.
[00:04:00.480 --> 00:04:02.960]   The first is still a convolutional neural network
[00:04:02.960 --> 00:04:04.080]   that encodes the image.
[00:04:04.080 --> 00:04:06.080]   The only difference is it doesn't have
[00:04:06.080 --> 00:04:08.120]   any fully connected layers.
[00:04:08.120 --> 00:04:12.160]   So it can handle input images that are of different sizes.
[00:04:12.160 --> 00:04:14.580]   And the data set does have--
[00:04:14.580 --> 00:04:18.960]   so we don't have to have that strict pre-processing step.
[00:04:18.960 --> 00:04:21.160]   The output of the encoder is a feature grid.
[00:04:21.160 --> 00:04:23.880]   And then the next component of our architecture
[00:04:23.880 --> 00:04:25.260]   is the row encoder.
[00:04:25.260 --> 00:04:27.240]   And what that does is it basically
[00:04:27.240 --> 00:04:30.040]   applies a recurrent neural network
[00:04:30.040 --> 00:04:33.000]   across each of the rows of that feature grid.
[00:04:33.000 --> 00:04:35.960]   And the recurrent neural networks use LSTM cells
[00:04:35.960 --> 00:04:36.860]   to do that.
[00:04:36.860 --> 00:04:38.440]   And then the output of the row encoder
[00:04:38.440 --> 00:04:42.160]   gets fed to the final key component of the model, which
[00:04:42.160 --> 00:04:43.560]   is a decoder.
[00:04:43.560 --> 00:04:46.000]   That decoder is another recurrent neural network
[00:04:46.000 --> 00:04:50.800]   using LSTMs that also applies a Luong-style attention
[00:04:50.800 --> 00:04:52.400]   mechanism.
[00:04:52.400 --> 00:04:54.980]   And then at each time step, the output
[00:04:54.980 --> 00:04:58.080]   gets fed to a fully connected softmax layer
[00:04:58.080 --> 00:05:01.320]   to classify the latex symbol.
[00:05:01.320 --> 00:05:03.320]   So for the training experiments, we
[00:05:03.320 --> 00:05:07.220]   tried a bunch of different hyperparameters listed here.
[00:05:07.220 --> 00:05:10.440]   And then the best configuration that we were able to find
[00:05:10.440 --> 00:05:12.520]   was the ones highlighted in green.
[00:05:12.520 --> 00:05:14.560]   And since it's all mangled up, I'm
[00:05:14.560 --> 00:05:17.480]   going to say it was stochastic gradient descent with momentum,
[00:05:17.480 --> 00:05:19.560]   adaptive learning rate based on the validation
[00:05:19.560 --> 00:05:22.520]   score after each epoch, the--
[00:05:22.520 --> 00:05:25.440]   I don't know how to pronounce "her"-- normal weight
[00:05:25.440 --> 00:05:28.160]   initialization for the convolutional layers only,
[00:05:28.160 --> 00:05:31.360]   and an enlarged batch size of 32.
[00:05:31.360 --> 00:05:36.040]   OK, so when we wanted to look at how good our model was,
[00:05:36.040 --> 00:05:37.480]   the first thing that we looked at
[00:05:37.480 --> 00:05:39.280]   was the loss, which is essentially
[00:05:39.280 --> 00:05:42.200]   the value of the error function that the model
[00:05:42.200 --> 00:05:44.960]   optimizes against during training.
[00:05:44.960 --> 00:05:48.920]   For this project, we use the categorical cross-entropy loss
[00:05:48.920 --> 00:05:49.760]   function.
[00:05:49.760 --> 00:05:52.240]   And for comparison reasons, that plot
[00:05:52.240 --> 00:05:56.320]   shows what the best loss of our baseline model, the one
[00:05:56.320 --> 00:05:58.800]   that Cathy talked about earlier, is.
[00:05:58.800 --> 00:06:02.240]   And also for another comparison, this
[00:06:02.240 --> 00:06:05.680]   is the best loss that the state-of-the-art model got.
[00:06:05.680 --> 00:06:07.200]   And by state-of-the-art, I'm simply
[00:06:07.200 --> 00:06:11.520]   referring to a previous study done by an NLP group
[00:06:11.520 --> 00:06:15.440]   at Harvard on that same data set.
[00:06:15.440 --> 00:06:21.560]   And this was our loss across different training iterations
[00:06:21.560 --> 00:06:23.160]   on the data set.
[00:06:23.160 --> 00:06:26.320]   Another way to look at the performance of our model
[00:06:26.320 --> 00:06:28.400]   was through an evaluation metric.
[00:06:28.400 --> 00:06:30.480]   And that metric is the perplexity score.
[00:06:30.480 --> 00:06:31.640]   We didn't come up with that.
[00:06:31.640 --> 00:06:35.640]   It's what the previous studies used for the same data set.
[00:06:35.640 --> 00:06:38.360]   And on the bottom right corner is
[00:06:38.360 --> 00:06:43.640]   a table showing the score for our model, for the SODA model,
[00:06:43.640 --> 00:06:45.840]   on both the training set and the test set.
[00:06:45.840 --> 00:06:47.600]   The test set is not the validation set.
[00:06:47.600 --> 00:06:51.040]   It's another-- we didn't tune the hyperparameters
[00:06:51.040 --> 00:06:52.920]   on that set.
[00:06:52.920 --> 00:06:55.240]   So once we had a model that we were happy with,
[00:06:55.240 --> 00:06:58.280]   we wanted to deploy it so that we could interact with it.
[00:06:58.280 --> 00:07:00.840]   And this is our pipeline.
[00:07:00.840 --> 00:07:04.320]   So we have a web app that uses Flask and Bootstrap.
[00:07:04.320 --> 00:07:06.080]   And then in the back end, we have
[00:07:06.080 --> 00:07:11.280]   a server that's Flask and TensorFlow 2.0, so our model.
[00:07:11.280 --> 00:07:15.240]   So we have a demo for you guys.
[00:07:15.240 --> 00:07:18.400]   Well, we're going to send over this equation here.
[00:07:18.400 --> 00:07:20.680]   And it's on YouTube.
[00:07:20.680 --> 00:07:21.920]   What was that?
[00:07:21.920 --> 00:07:23.680]   Not perfectly, but you'll see the--
[00:07:23.680 --> 00:07:24.400]   Stay tuned.
[00:07:24.400 --> 00:07:24.900]   Yeah.
[00:07:24.900 --> 00:07:27.160]   [LAUGHTER]
[00:07:27.160 --> 00:07:28.640]   It messes up one small--
[00:07:28.640 --> 00:07:30.920]   it does one small mistake, if you can notice it.
[00:07:30.920 --> 00:07:33.080]   So this is the website here.
[00:07:33.080 --> 00:07:38.440]   So we choose a file, and then we have that image that I showed.
[00:07:38.440 --> 00:07:41.160]   Convert, and then that's the LaTeX code.
[00:07:41.160 --> 00:07:45.200]   And put that into a tech file, and then render it.
[00:07:45.200 --> 00:07:50.040]   Render it there.
[00:07:50.040 --> 00:07:59.080]   And that's the equation that it predicted.
[00:07:59.080 --> 00:08:01.880]   So this is the input and output side by side.
[00:08:01.880 --> 00:08:04.560]   So it did pretty good.
[00:08:04.560 --> 00:08:07.920]   It did confuse a theta for a Q. And then it
[00:08:07.920 --> 00:08:12.840]   made the L a subscript, and then also the bottom of the sigma.
[00:08:12.840 --> 00:08:15.080]   But it's pretty good.
[00:08:15.080 --> 00:08:18.960]   And we noticed that most of our samples that we tested
[00:08:18.960 --> 00:08:20.240]   performed like this.
[00:08:20.240 --> 00:08:23.800]   But we wanted to try something more interesting.
[00:08:23.800 --> 00:08:26.040]   And so we used this equation here.
[00:08:26.040 --> 00:08:30.260]   And what's interesting is when we fed this into the model,
[00:08:30.260 --> 00:08:32.420]   it only looked at the top part.
[00:08:32.420 --> 00:08:33.820]   It only predicted that.
[00:08:33.820 --> 00:08:35.260]   And it did it really well.
[00:08:35.260 --> 00:08:39.880]   So we think that it's just because
[00:08:39.880 --> 00:08:43.520]   of how rigid the data set was and how it was pre-processed.
[00:08:43.520 --> 00:08:48.440]   So just a couple of takeaways for the future work on this one.
[00:08:48.440 --> 00:08:50.960]   So yeah, the advantages for us was, well,
[00:08:50.960 --> 00:08:53.720]   we had an existing data set, which makes life much easier.
[00:08:53.720 --> 00:08:56.000]   And we had existing research to build on,
[00:08:56.000 --> 00:08:58.120]   which also makes life easier.
[00:08:58.120 --> 00:08:59.500]   But still, there were challenges.
[00:08:59.500 --> 00:09:02.260]   Data still needed to be processed properly.
[00:09:02.260 --> 00:09:04.800]   And that still takes time.
[00:09:04.800 --> 00:09:07.600]   And also, right now, the data processing pipeline
[00:09:07.600 --> 00:09:09.920]   is pretty rigid for us.
[00:09:09.920 --> 00:09:13.600]   The second aspect, the workflow and tooling, at least for us,
[00:09:13.600 --> 00:09:18.340]   took a lot of time to figure out how to properly establish
[00:09:18.340 --> 00:09:20.200]   the workflow.
[00:09:20.200 --> 00:09:23.480]   What worked for us, TensorFlow 2.0
[00:09:23.480 --> 00:09:25.320]   has the subclassing API, which allows
[00:09:25.320 --> 00:09:30.320]   us to do multiple experiments pretty quickly, in some sense.
[00:09:30.320 --> 00:09:32.720]   The other two, the second one, kind of obvious,
[00:09:32.720 --> 00:09:36.480]   but throwing more compute power and just spinning
[00:09:36.480 --> 00:09:40.280]   multiple VM instances and doing multiple experiments
[00:09:40.280 --> 00:09:42.100]   in parallel definitely helped.
[00:09:42.100 --> 00:09:45.120]   And the third part, which was surprising to us,
[00:09:45.120 --> 00:09:47.740]   just increasing the batch size helped a lot.
[00:09:47.740 --> 00:09:49.440]   And not just that, increasing the batch size
[00:09:49.440 --> 00:09:55.360]   and the initializations helped quickly change the loss.
[00:09:55.360 --> 00:09:58.520]   For future work, what we could extend it to, well,
[00:09:58.520 --> 00:10:00.480]   one of them would be change the encoder, which
[00:10:00.480 --> 00:10:02.580]   is right now just CNN, to something
[00:10:02.580 --> 00:10:04.560]   that's more state of the art.
[00:10:04.560 --> 00:10:07.560]   We would want to automate the preprocessing.
[00:10:07.560 --> 00:10:10.120]   So right now, it only takes images
[00:10:10.120 --> 00:10:12.960]   where the equation is sort of centered.
[00:10:12.960 --> 00:10:16.800]   We would want to make it more generic than that.
[00:10:16.800 --> 00:10:19.720]   So that, I think, would be a pretty cool thing
[00:10:19.720 --> 00:10:22.960]   to be able to do.
[00:10:22.960 --> 00:10:25.040]   And then finally, there's just data generation
[00:10:25.040 --> 00:10:27.400]   and augmentation.
[00:10:27.400 --> 00:10:28.920]   Just from the existing data set, we
[00:10:28.920 --> 00:10:31.880]   could make many more equations from there.
[00:10:31.880 --> 00:10:35.400]   And the last part would be the Bayesian hyperparameter
[00:10:35.400 --> 00:10:36.560]   optimization.
[00:10:36.560 --> 00:10:39.440]   So right now, we haven't been able to get to that point,
[00:10:39.440 --> 00:10:40.760]   but we should be.
[00:10:40.760 --> 00:10:41.360]   Thank you.
[00:10:41.360 --> 00:10:44.720]   [APPLAUSE]

