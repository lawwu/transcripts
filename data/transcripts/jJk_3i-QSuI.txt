
[00:00:00.000 --> 00:00:02.040]   And we are live.
[00:00:02.040 --> 00:00:03.200]   - Great, okay.
[00:00:03.200 --> 00:00:07.000]   So folks are gonna start coming in to the Zoom.
[00:00:07.000 --> 00:00:11.920]   Folks coming in, joining us live on YouTube as well.
[00:00:11.920 --> 00:00:13.520]   Welcome everyone.
[00:00:13.520 --> 00:00:18.000]   We've got another exciting Weights and Biases
[00:00:18.000 --> 00:00:21.020]   deep learning salon for everyone.
[00:00:21.020 --> 00:00:24.840]   So I'll be speaking in the second half of the salon.
[00:00:24.840 --> 00:00:25.680]   So that's exciting.
[00:00:25.680 --> 00:00:27.840]   I talked last about a month ago
[00:00:27.840 --> 00:00:31.800]   and it just was so much fun that I decided I'd do it again.
[00:00:31.800 --> 00:00:33.560]   Before we dive into that
[00:00:33.560 --> 00:00:36.340]   and before I introduce our speaker for this week,
[00:00:36.340 --> 00:00:39.820]   I wanted to talk about the upcoming salon.
[00:00:39.820 --> 00:00:43.880]   So November 24th, Thanksgiving week in the United States,
[00:00:43.880 --> 00:00:47.560]   we're gonna have two folks on to talk about benchmarks,
[00:00:47.560 --> 00:00:50.960]   to talk about two tasks that they want community input
[00:00:50.960 --> 00:00:54.000]   on sort of an opportunity on a shared leaderboard
[00:00:54.000 --> 00:00:56.140]   to contribute to two different projects.
[00:00:56.140 --> 00:00:58.280]   And I'm actually really excited about
[00:00:58.280 --> 00:00:59.960]   these two benchmarks in particular
[00:00:59.960 --> 00:01:02.000]   'cause they're both sort of oriented towards
[00:01:02.000 --> 00:01:04.240]   a more thoughtful, maybe more ethical AI.
[00:01:04.240 --> 00:01:07.280]   First, the Safe Life Benchmark by the Partnership on AI,
[00:01:07.280 --> 00:01:12.200]   which is explicitly about safety and limited side effects
[00:01:12.200 --> 00:01:13.840]   in deep reinforcement learning.
[00:01:13.840 --> 00:01:16.280]   And then the Deep Form Benchmarks.
[00:01:16.280 --> 00:01:19.920]   This is from a group that's trying to get a better handle
[00:01:19.920 --> 00:01:24.080]   on who's spending money on political TV advertisements
[00:01:24.080 --> 00:01:27.900]   from this sort of vaguely structured form data.
[00:01:27.900 --> 00:01:30.580]   So extracting information from forms is really, really hard
[00:01:30.580 --> 00:01:31.700]   and they built this benchmark
[00:01:31.700 --> 00:01:34.180]   to try and learn a little bit more about it.
[00:01:34.180 --> 00:01:37.300]   So that will be two weeks from right now,
[00:01:37.300 --> 00:01:40.320]   November 24th at five o'clock Pacific time.
[00:01:40.320 --> 00:01:45.320]   And the RSVP link for that will be tiny.cc/wb-salon.
[00:01:45.320 --> 00:01:52.660]   You can RSVP for that starting about 10 minutes from now.
[00:01:53.660 --> 00:01:56.400]   And we do this salon every two weeks.
[00:01:56.400 --> 00:01:57.780]   You can catch the salons you've missed
[00:01:57.780 --> 00:01:58.700]   on our YouTube channel.
[00:01:58.700 --> 00:02:00.940]   We've also got some app tutorials
[00:02:00.940 --> 00:02:04.980]   and introductory and advanced lectures in machine learning
[00:02:04.980 --> 00:02:06.640]   if you wanna check those out.
[00:02:06.640 --> 00:02:07.800]   We've also got a Slack forum.
[00:02:07.800 --> 00:02:12.420]   You can hop into it, bit.ly/slack-forum.
[00:02:12.420 --> 00:02:15.720]   Those links will pop up in the YouTube chat
[00:02:15.720 --> 00:02:17.820]   and in the Slack chat in just a minute.
[00:02:19.100 --> 00:02:23.120]   So that's the future of the WNB Salon,
[00:02:23.120 --> 00:02:26.200]   but let's talk about the present.
[00:02:26.200 --> 00:02:28.640]   Presently here, we have Chirag Agarwal
[00:02:28.640 --> 00:02:31.740]   who is joining us from Harvard
[00:02:31.740 --> 00:02:34.200]   where he is a postdoctoral scholar
[00:02:34.200 --> 00:02:36.820]   and did some really exciting and interesting work
[00:02:36.820 --> 00:02:38.920]   on using the variance of the gradient,
[00:02:38.920 --> 00:02:40.400]   something people don't normally track,
[00:02:40.400 --> 00:02:41.880]   don't normally think about,
[00:02:41.880 --> 00:02:44.960]   to track something that people do think quite a bit about.
[00:02:44.960 --> 00:02:46.800]   Where are the outliers in my dataset?
[00:02:46.800 --> 00:02:49.860]   Where are the examples my network's not learning,
[00:02:49.860 --> 00:02:50.900]   you know, not learning about?
[00:02:50.900 --> 00:02:52.160]   Which ones are they?
[00:02:52.160 --> 00:02:54.140]   So I'm really excited to have Chirag on
[00:02:54.140 --> 00:02:56.620]   to talk about that work.
[00:02:56.620 --> 00:03:01.160]   - Awesome.
[00:03:01.160 --> 00:03:02.620]   Thanks for the introduction, Charles.
[00:03:02.620 --> 00:03:04.900]   I think that was a better introduction
[00:03:04.900 --> 00:03:06.800]   to the work that I could have come up with.
[00:03:06.800 --> 00:03:10.800]   So I'm gonna steal that from you.
[00:03:10.800 --> 00:03:13.820]   So, okay.
[00:03:13.820 --> 00:03:15.220]   Can I share my screen now?
[00:03:16.460 --> 00:03:17.680]   - Yeah, go ahead.
[00:03:17.680 --> 00:03:18.520]   - Awesome.
[00:03:18.520 --> 00:03:21.980]   So it looks good.
[00:03:21.980 --> 00:03:25.780]   Awesome.
[00:03:25.780 --> 00:03:27.800]   So yeah, hello to everyone
[00:03:27.800 --> 00:03:30.320]   who are attending today's WNB Salon.
[00:03:30.320 --> 00:03:33.080]   I would start by thanking Charles
[00:03:33.080 --> 00:03:34.820]   for inviting me to the salon today.
[00:03:34.820 --> 00:03:38.140]   In today's talk, I'll be basically discussing
[00:03:38.140 --> 00:03:40.680]   about the work with my fantastic collaborator,
[00:03:40.680 --> 00:03:42.160]   Sarah Hooker from Google,
[00:03:42.160 --> 00:03:44.520]   about estimating example difficulties
[00:03:44.520 --> 00:03:46.200]   using variance of gradients.
[00:03:46.840 --> 00:03:51.840]   So I guess most, before going to the depth of the work,
[00:03:51.840 --> 00:03:55.400]   I would like to just provide some basic motivations
[00:03:55.400 --> 00:03:57.240]   for studying the relative importance
[00:03:57.240 --> 00:04:00.600]   between different samples during the training process.
[00:04:00.600 --> 00:04:04.280]   So understanding individual samples importance
[00:04:04.280 --> 00:04:05.920]   is very important,
[00:04:05.920 --> 00:04:08.480]   given that modern data sets are large,
[00:04:08.480 --> 00:04:10.620]   with sometimes data sets like ImageNet,
[00:04:10.620 --> 00:04:13.760]   they comprise of more than million of images.
[00:04:13.760 --> 00:04:16.480]   So this is not feasible for an individual
[00:04:16.480 --> 00:04:19.560]   to look at individual explanation of each data point
[00:04:19.560 --> 00:04:20.960]   in this huge data sets.
[00:04:20.960 --> 00:04:25.000]   Moreover, it is also critical
[00:04:25.000 --> 00:04:29.400]   to understand the relative importance of individual classes
[00:04:29.400 --> 00:04:32.440]   and inspect images that the model finds easier
[00:04:32.440 --> 00:04:33.460]   or more challenging.
[00:04:33.460 --> 00:04:38.040]   Last but not the least,
[00:04:38.040 --> 00:04:39.140]   most of the current works
[00:04:39.140 --> 00:04:42.640]   in explainable artificial intelligence, XAI,
[00:04:42.640 --> 00:04:45.160]   are focused on explaining model behavior
[00:04:45.160 --> 00:04:48.040]   on single or a group of explanations.
[00:04:48.040 --> 00:04:50.840]   However, these don't actually highlight
[00:04:50.840 --> 00:04:52.560]   the inherent bias of the model
[00:04:52.560 --> 00:04:54.960]   towards features like say,
[00:04:54.960 --> 00:04:57.520]   colors or textures in an image classifier model.
[00:04:57.520 --> 00:05:01.160]   So these were like the motivations
[00:05:01.160 --> 00:05:03.560]   that prompted us to move into this direction
[00:05:03.560 --> 00:05:05.460]   and explore the variance of gradients.
[00:05:05.460 --> 00:05:09.500]   So with respect to explanations,
[00:05:10.640 --> 00:05:12.880]   most of the earliest work in XAI
[00:05:12.880 --> 00:05:15.400]   was based on model gradients.
[00:05:15.400 --> 00:05:18.020]   Now using vanilla gradients is intuitive
[00:05:18.020 --> 00:05:19.720]   as they have a very clear connection
[00:05:19.720 --> 00:05:21.280]   to the neural network function.
[00:05:21.280 --> 00:05:24.640]   Basically, we have an image, let's say,
[00:05:24.640 --> 00:05:26.840]   we can easily calculate its vanilla gradient
[00:05:26.840 --> 00:05:28.520]   by backpropagating the loss
[00:05:28.520 --> 00:05:30.400]   with respect to the input image
[00:05:30.400 --> 00:05:33.800]   and get these gradient representations of the model.
[00:05:33.800 --> 00:05:37.820]   However, following works showed
[00:05:37.820 --> 00:05:39.880]   that gradients are very noisy
[00:05:39.880 --> 00:05:41.360]   and hence uninterpretable.
[00:05:41.360 --> 00:05:44.160]   This led to several works
[00:05:44.160 --> 00:05:46.680]   which tried to improve the gradient maps
[00:05:46.680 --> 00:05:48.560]   by either smoothing them
[00:05:48.560 --> 00:05:52.000]   or using methods like guided backpropagation
[00:05:52.000 --> 00:05:54.020]   or trying to integrate the gradients
[00:05:54.020 --> 00:05:55.680]   between a baseline image,
[00:05:55.680 --> 00:05:56.820]   let's say a gray image,
[00:05:56.820 --> 00:05:57.800]   and the input image,
[00:05:57.800 --> 00:05:59.680]   here the image of the dog.
[00:05:59.680 --> 00:06:01.800]   As one would observe
[00:06:01.800 --> 00:06:04.880]   in all these different gradient maps,
[00:06:04.880 --> 00:06:07.040]   what most researchers were trying to do
[00:06:07.040 --> 00:06:08.880]   was to tune these gradients
[00:06:08.880 --> 00:06:11.400]   so to give them some interpretability
[00:06:11.400 --> 00:06:14.120]   with context to the raw vanilla gradients
[00:06:14.120 --> 00:06:16.640]   so that these maps can be understandable
[00:06:16.640 --> 00:06:19.040]   to end users like us, humans.
[00:06:19.040 --> 00:06:22.480]   However, one may also question
[00:06:22.480 --> 00:06:27.140]   that the gradients can be the best explanation of a model
[00:06:27.140 --> 00:06:29.960]   because that is what the model uses
[00:06:29.960 --> 00:06:31.900]   to learn the given objective function.
[00:06:31.900 --> 00:06:37.640]   This actually brings us to why variants of gradients.
[00:06:37.640 --> 00:06:39.320]   And as I said before,
[00:06:39.320 --> 00:06:41.080]   the raw input vanilla gradients
[00:06:41.080 --> 00:06:44.440]   can be the best explanations of a model in some context.
[00:06:44.440 --> 00:06:49.440]   So we hypothesize that gradients of examples
[00:06:49.440 --> 00:06:51.680]   that can be easily learned by the model
[00:06:51.680 --> 00:06:53.120]   will converge quickly
[00:06:53.120 --> 00:06:56.640]   during the early training of the whole model
[00:06:56.640 --> 00:06:58.640]   as compared to the challenging examples
[00:06:58.640 --> 00:07:02.000]   where the gradients could not converge quickly.
[00:07:02.000 --> 00:07:04.200]   So following this hypothesis,
[00:07:04.200 --> 00:07:07.500]   what we basically do is we calculate a proxy metric
[00:07:07.500 --> 00:07:10.120]   that's like the variance of gradients
[00:07:10.120 --> 00:07:12.120]   for quantifying the relative importance
[00:07:12.120 --> 00:07:14.600]   between individual samples of the training
[00:07:14.600 --> 00:07:15.920]   and the testing datasets.
[00:07:15.920 --> 00:07:19.920]   In this figure, we basically show an input image
[00:07:19.920 --> 00:07:21.720]   that's the image of the bird.
[00:07:21.720 --> 00:07:23.400]   And we show its gradients
[00:07:23.400 --> 00:07:25.920]   as it is evolving during different checkpoints
[00:07:25.920 --> 00:07:28.000]   the whole training process.
[00:07:28.000 --> 00:07:30.040]   This image was taken from the image in dataset
[00:07:30.040 --> 00:07:31.360]   and hence it was trained
[00:07:31.360 --> 00:07:34.800]   using a REST F50 architecture for 90 epoch.
[00:07:34.800 --> 00:07:39.280]   So we can easily calculate the gradients
[00:07:39.280 --> 00:07:41.420]   with respect to any particular checkpoint
[00:07:41.420 --> 00:07:42.560]   throughout the training process.
[00:07:42.560 --> 00:07:47.460]   And that's like one of the best USP of using VOG score.
[00:07:47.460 --> 00:07:50.700]   So using these gradients,
[00:07:50.700 --> 00:07:54.020]   we come up with this VOG scalar score.
[00:07:54.020 --> 00:07:55.500]   In the given equation,
[00:07:55.500 --> 00:07:58.180]   the STI basically represents the gradients
[00:07:58.180 --> 00:08:01.420]   of the particular image at the given checkpoint T.
[00:08:02.380 --> 00:08:04.760]   Mu i is basically the mean gradients
[00:08:04.760 --> 00:08:06.960]   of a given number of checkpoints.
[00:08:06.960 --> 00:08:10.560]   Here, since K determines the number of checkpoints
[00:08:10.560 --> 00:08:13.280]   across which we are trying to calculate the gradients
[00:08:13.280 --> 00:08:15.700]   and N determines the number of superpixels,
[00:08:15.700 --> 00:08:18.480]   number of pixels in the whole images.
[00:08:18.480 --> 00:08:23.480]   So in a way, we can actually interpret the VOG score
[00:08:23.480 --> 00:08:26.220]   as a scalar score,
[00:08:26.220 --> 00:08:30.260]   which represents the VOG across each pixel of the image.
[00:08:30.260 --> 00:08:32.500]   And then we basically take the mean
[00:08:32.500 --> 00:08:35.520]   across the whole VOG across all pixels
[00:08:35.520 --> 00:08:38.440]   to get just one scalar score for a given input image.
[00:08:38.440 --> 00:08:43.920]   So what we do then is calculate these VOG scores
[00:08:43.920 --> 00:08:47.020]   for all the training and the testing images in the dataset
[00:08:47.020 --> 00:08:49.420]   by using the different checkpoints of the model.
[00:08:49.420 --> 00:08:55.120]   So many existing works have shown
[00:08:55.120 --> 00:08:58.000]   that there exists different stages of training
[00:08:58.000 --> 00:08:59.620]   in a deep neural network.
[00:08:59.620 --> 00:09:05.340]   So what we show here is on analyzing the images
[00:09:05.340 --> 00:09:07.060]   using the VOG score,
[00:09:07.060 --> 00:09:10.800]   we see that images having a low VOG scores
[00:09:10.800 --> 00:09:12.400]   are very different than the images
[00:09:12.400 --> 00:09:14.640]   which are having high VOG scores.
[00:09:14.640 --> 00:09:16.060]   In this particular figure,
[00:09:16.060 --> 00:09:19.960]   we trained the VESNET-18 model on C500 dataset,
[00:09:19.960 --> 00:09:23.240]   and we tried to visualize the relative importance
[00:09:23.240 --> 00:09:24.920]   between the Apple class images.
[00:09:26.000 --> 00:09:29.660]   Clearly we see that there's a big cluster of red apples
[00:09:29.660 --> 00:09:31.520]   which have a low VOG score
[00:09:31.520 --> 00:09:34.120]   as compared to the high VOG score apple images
[00:09:34.120 --> 00:09:35.540]   which are more cluttered.
[00:09:35.540 --> 00:09:38.920]   And in some of the images, they are like multiple apples.
[00:09:38.920 --> 00:09:43.720]   On comparing this to the late stage training,
[00:09:43.720 --> 00:09:45.240]   we see similar patterns
[00:09:45.240 --> 00:09:48.920]   where the low VOG score images are more object-centric
[00:09:48.920 --> 00:09:52.120]   and have clear view of the object in interest,
[00:09:52.120 --> 00:09:53.480]   that is apples,
[00:09:53.480 --> 00:09:55.880]   whereas the high VOG samples are more cluttered
[00:09:55.880 --> 00:09:58.360]   and they are taken from different vantage points.
[00:09:58.360 --> 00:10:06.000]   So one quick question that would come
[00:10:06.000 --> 00:10:09.640]   to any model developer or end user is,
[00:10:09.640 --> 00:10:13.120]   can we use VOGs to have a relative ranking
[00:10:13.120 --> 00:10:14.240]   across each classes?
[00:10:14.240 --> 00:10:17.440]   So in this particular experiment,
[00:10:17.440 --> 00:10:20.680]   we divided the training process into two stages,
[00:10:20.680 --> 00:10:22.960]   which is the early stage and the late stage,
[00:10:22.960 --> 00:10:24.560]   comprising of three checkpoints.
[00:10:25.560 --> 00:10:27.640]   The proposed VOG score
[00:10:27.640 --> 00:10:29.760]   then actually discriminates images
[00:10:29.760 --> 00:10:31.540]   with clean and cluttered backgrounds
[00:10:31.540 --> 00:10:36.540]   and higher VOG scores images are usually images
[00:10:36.540 --> 00:10:38.400]   with zoomed in view,
[00:10:38.400 --> 00:10:42.160]   or they are more typical as compared to low VOG scores.
[00:10:42.160 --> 00:10:43.580]   In this particular image,
[00:10:43.580 --> 00:10:48.580]   we trained a ResNet-50 architecture on image and dataset,
[00:10:48.580 --> 00:10:51.800]   and we basically visualized a cluster of images
[00:10:51.800 --> 00:10:53.300]   belonging to the MacPy class.
[00:10:54.220 --> 00:10:56.560]   Very surprisingly, we see the same pattern
[00:10:56.560 --> 00:10:58.520]   as in the CIFAR datasets,
[00:10:58.520 --> 00:11:02.080]   where we see the low VOG samples are much more cleaner
[00:11:02.080 --> 00:11:05.280]   and have the birds centered to the images
[00:11:05.280 --> 00:11:07.480]   as compared to the high VOG scores,
[00:11:07.480 --> 00:11:10.680]   where you'll see zoomed in views of the bird
[00:11:10.680 --> 00:11:12.920]   or cropped images,
[00:11:12.920 --> 00:11:15.160]   which are basically the data augmented portions
[00:11:15.160 --> 00:11:16.000]   of the datasets.
[00:11:16.000 --> 00:11:20.120]   We saw the same pattern for a different class,
[00:11:20.120 --> 00:11:21.840]   which is the Pop Portal in this case,
[00:11:21.840 --> 00:11:25.040]   where the low VOG scores had images
[00:11:25.040 --> 00:11:27.960]   comprising of multiple bottles in the same image,
[00:11:27.960 --> 00:11:32.660]   whereas the high VOG score had Pop Portal images
[00:11:32.660 --> 00:11:34.840]   where let's say a human was involved
[00:11:34.840 --> 00:11:36.760]   or they were taken from the top view
[00:11:36.760 --> 00:11:41.760]   and all the different styles of Pop Portal images.
[00:11:41.760 --> 00:11:47.040]   So these qualitative insights gave us good motivation
[00:11:47.040 --> 00:11:49.620]   that VOG can be actually used
[00:11:49.620 --> 00:11:52.600]   to understand the different clusters of images
[00:11:52.600 --> 00:11:55.880]   during different training stages
[00:11:55.880 --> 00:11:58.540]   and for different classes too.
[00:11:58.540 --> 00:12:05.320]   So moving on to some quantitative results,
[00:12:05.320 --> 00:12:07.280]   here we show that VOG can discriminate
[00:12:07.280 --> 00:12:09.240]   easy and challenging examples easily.
[00:12:09.240 --> 00:12:12.280]   Between CIFAR-10 models,
[00:12:12.280 --> 00:12:17.000]   it achieved a testing accuracy of 89.55%.
[00:12:17.000 --> 00:12:18.800]   It is to be noted that all the models
[00:12:18.800 --> 00:12:20.480]   that I'll be showing now were trained
[00:12:20.480 --> 00:12:22.160]   with 100% accuracy.
[00:12:22.160 --> 00:12:24.040]   That is, there is some sense of overfitting
[00:12:24.040 --> 00:12:26.400]   across all these networks.
[00:12:26.400 --> 00:12:29.440]   So we clearly see that the misclassification rate
[00:12:29.440 --> 00:12:32.080]   is the highest for the VOG scores,
[00:12:32.080 --> 00:12:35.440]   which for the samples which have the high VOG score.
[00:12:35.440 --> 00:12:39.360]   Interestingly, we found the similar patterns
[00:12:39.360 --> 00:12:43.500]   across two more datasets of CIFAR-100 and ImageNet.
[00:12:43.500 --> 00:12:46.820]   Here, if we see carefully,
[00:12:46.820 --> 00:12:49.440]   then the testing accuracies of the CIFAR-100
[00:12:49.440 --> 00:12:51.920]   and ImageNet datasets were much lower,
[00:12:51.920 --> 00:12:54.760]   and that means they were more overfitted.
[00:12:54.760 --> 00:12:56.560]   And as compared to CIFAR-10,
[00:12:56.560 --> 00:12:59.400]   that is why we see a much more linear pattern
[00:12:59.400 --> 00:13:03.280]   between the VOG scores and the misclassification error rate,
[00:13:03.280 --> 00:13:05.080]   where we see a consistent trend
[00:13:05.080 --> 00:13:07.200]   that as the VOG scores increases,
[00:13:07.200 --> 00:13:10.100]   the misclassification error increases.
[00:13:10.100 --> 00:13:15.680]   So these are all good because we can easily use VOGs
[00:13:15.680 --> 00:13:18.720]   to understand the testing datasets.
[00:13:18.720 --> 00:13:21.920]   But now moving back to the training datasets,
[00:13:21.920 --> 00:13:25.640]   then we ask the question that can actually VOG
[00:13:25.640 --> 00:13:29.040]   give us more insights about the behavior of the model
[00:13:29.040 --> 00:13:30.420]   during different stages?
[00:13:30.420 --> 00:13:35.120]   So we took the ImageNet-trained ResNet-50 model,
[00:13:35.120 --> 00:13:38.140]   and we saw that the behavior completely flips
[00:13:38.140 --> 00:13:40.600]   in the early and the late stage.
[00:13:40.600 --> 00:13:43.200]   Now the coordination pattern with accuracy reverses,
[00:13:43.200 --> 00:13:45.920]   as we see from the early to late stage training,
[00:13:45.920 --> 00:13:47.720]   which gives us an interesting viewpoint
[00:13:47.720 --> 00:13:49.720]   into the training dynamics of the model.
[00:13:49.720 --> 00:13:52.360]   That is basically that early on,
[00:13:52.360 --> 00:13:54.480]   the gradient updates have high variance
[00:13:54.480 --> 00:13:56.240]   for the easiest examples,
[00:13:56.240 --> 00:13:57.840]   but once they are being trained,
[00:13:57.840 --> 00:14:01.040]   their VOG scores decreases,
[00:14:01.040 --> 00:14:03.480]   and later on during the late stage process,
[00:14:03.480 --> 00:14:07.360]   the gradient variance is high for only those samples
[00:14:07.360 --> 00:14:09.040]   that the model finds challenging.
[00:14:10.880 --> 00:14:15.040]   So these were more like looking into a trained model,
[00:14:15.040 --> 00:14:17.860]   be it the training datasets or the testing datasets,
[00:14:17.860 --> 00:14:21.080]   but an interesting application that we thought
[00:14:21.080 --> 00:14:23.520]   VOG can be used to is that it can act
[00:14:23.520 --> 00:14:25.700]   like an unsupervised auditing tool.
[00:14:25.700 --> 00:14:29.560]   Now it is to be noted that all the results
[00:14:29.560 --> 00:14:31.200]   that we have seen until now,
[00:14:31.200 --> 00:14:32.600]   the gradients were actually taken
[00:14:32.600 --> 00:14:36.760]   using the ground truth class of the particular image.
[00:14:36.760 --> 00:14:39.160]   But in real world setting,
[00:14:39.160 --> 00:14:42.600]   we often do not know the ground truth class.
[00:14:42.600 --> 00:14:45.040]   So the question comes that can VOG serve
[00:14:45.040 --> 00:14:48.680]   as a good auditing tool if we don't know the predicted class
[00:14:48.680 --> 00:14:50.320]   or if we don't know the ground truth class
[00:14:50.320 --> 00:14:52.080]   of the respective inputs.
[00:14:52.080 --> 00:14:55.520]   So what we did here was we took the back propagated gradient
[00:14:55.520 --> 00:14:58.360]   with the top one predicted class of any image,
[00:14:58.360 --> 00:15:01.840]   and then we tried to visualize the low VOG
[00:15:01.840 --> 00:15:04.080]   and the high VOG samples.
[00:15:05.720 --> 00:15:08.800]   To our surprise, we actually see the similar patterns
[00:15:08.800 --> 00:15:11.240]   as we saw for the ground truth VOG scores,
[00:15:11.240 --> 00:15:13.760]   where we see that images that were correctly predicted
[00:15:13.760 --> 00:15:16.920]   by the model were more object centric
[00:15:16.920 --> 00:15:18.680]   as compared to the high VOG samples
[00:15:18.680 --> 00:15:21.080]   where we see blurred out images
[00:15:21.080 --> 00:15:25.000]   or images where the object was too much zoomed in.
[00:15:25.000 --> 00:15:27.720]   And hence, I mean, that gave us a good indicator
[00:15:27.720 --> 00:15:31.720]   that VOG can actually serve as an unsupervised tool
[00:15:31.720 --> 00:15:34.440]   since we do not use any ground truth information
[00:15:34.440 --> 00:15:35.840]   of the respective images.
[00:15:35.840 --> 00:15:40.840]   Now, all these results are fine,
[00:15:40.840 --> 00:15:45.560]   but to build trust with any end user,
[00:15:45.560 --> 00:15:48.840]   or even if you're giving a model developer
[00:15:48.840 --> 00:15:52.240]   this VOG proxymetric to analyze the model,
[00:15:52.240 --> 00:15:54.920]   a key desirable property for any tool
[00:15:54.920 --> 00:15:56.360]   would be the consistency.
[00:15:56.360 --> 00:16:00.760]   That is basically that we would expect a consistent method
[00:16:00.760 --> 00:16:02.200]   to produce a ranking
[00:16:02.200 --> 00:16:04.800]   with a very closely bounded distribution of scores
[00:16:04.800 --> 00:16:07.040]   across independent runs.
[00:16:07.040 --> 00:16:10.640]   To measure the VOG stability,
[00:16:10.640 --> 00:16:15.640]   we train five networks, five ResNet-18 models on CIFAR-10,
[00:16:15.640 --> 00:16:18.600]   and then we try to visualize the VOG scores
[00:16:18.600 --> 00:16:22.360]   across each of these five different trained models.
[00:16:22.360 --> 00:16:27.040]   All the five models were randomly initialized,
[00:16:27.040 --> 00:16:29.640]   so we can expect some variance.
[00:16:29.640 --> 00:16:32.200]   But when we visualize the VOG scores
[00:16:32.200 --> 00:16:35.520]   with respect to the error performance of these five models,
[00:16:35.520 --> 00:16:38.840]   we saw more or less like all the VOG scores
[00:16:38.840 --> 00:16:42.840]   had the same patterns across the five models,
[00:16:42.840 --> 00:16:47.600]   which talks about the stability of the VOG metric.
[00:16:47.600 --> 00:16:51.680]   So these were basically all the results
[00:16:51.680 --> 00:16:55.320]   that I could have summed up today for this talk.
[00:16:55.320 --> 00:16:58.680]   So the final takeaways for this VOG paper
[00:16:58.680 --> 00:17:01.400]   was that it can serve as an interpretability tool
[00:17:01.400 --> 00:17:04.760]   for ranking both training and the testing samples.
[00:17:04.760 --> 00:17:08.760]   An interesting advantage of VOG
[00:17:08.760 --> 00:17:10.800]   is that it does not need any modification
[00:17:10.800 --> 00:17:12.080]   of the architecture.
[00:17:12.080 --> 00:17:14.280]   Since we always train models
[00:17:14.280 --> 00:17:17.600]   and we always capture snapshots during the training process,
[00:17:17.600 --> 00:17:20.520]   so it doesn't come with any external additional costs.
[00:17:20.520 --> 00:17:22.960]   We can easily just take snapshots of a trained model
[00:17:22.960 --> 00:17:24.360]   and then calculate the VOGs
[00:17:24.360 --> 00:17:26.640]   of the input images in this case.
[00:17:27.520 --> 00:17:32.400]   And the third takeaway would be that the VOG scores
[00:17:32.400 --> 00:17:35.320]   and that can actually aid us in clustering images
[00:17:35.320 --> 00:17:36.480]   with respect to each class
[00:17:36.480 --> 00:17:40.160]   across the training datasets and the testing datasets
[00:17:40.160 --> 00:17:41.800]   so that we can actually understand
[00:17:41.800 --> 00:17:43.680]   the behavior of the model
[00:17:43.680 --> 00:17:47.840]   and also know about the dataset buyers,
[00:17:47.840 --> 00:17:51.560]   whether for say it consists of images
[00:17:51.560 --> 00:17:53.040]   of just one unique nature
[00:17:53.040 --> 00:17:57.680]   or it can have a good diversity across in each image.
[00:17:57.680 --> 00:18:03.040]   I think those are like the final takeaways of our work.
[00:18:03.040 --> 00:18:06.240]   And yep, you can find our paper on archive
[00:18:06.240 --> 00:18:09.320]   and I'm open to questions now.
[00:18:09.320 --> 00:18:12.320]   - Great.
[00:18:12.320 --> 00:18:16.000]   Thanks, Chag for that nice overview of your work.
[00:18:16.000 --> 00:18:17.520]   So I have a lot of questions.
[00:18:17.520 --> 00:18:19.760]   I encourage folks on Zoom and on YouTube
[00:18:19.760 --> 00:18:22.960]   to also chime in if they've got questions of their own.
[00:18:22.960 --> 00:18:24.600]   So either in the Q&A on Zoom
[00:18:24.600 --> 00:18:27.320]   or in the live chat on YouTube.
[00:18:27.320 --> 00:18:31.960]   So I guess just to start us off here.
[00:18:31.960 --> 00:18:37.600]   So how many samples from a class do you need
[00:18:37.600 --> 00:18:41.400]   in order to do this variance of gradient operation?
[00:18:41.400 --> 00:18:44.520]   - So that's an interesting question, Charles.
[00:18:44.520 --> 00:18:46.480]   So the thing is we actually experimented
[00:18:46.480 --> 00:18:47.800]   with different checkpoints,
[00:18:47.800 --> 00:18:50.560]   but we actually found out that as low
[00:18:50.560 --> 00:18:52.240]   as just three checkpoints from a model
[00:18:52.240 --> 00:18:56.400]   can actually serve, can lead to a good VOG approximation
[00:18:56.400 --> 00:18:58.200]   of the whole dataset.
[00:18:58.200 --> 00:19:02.040]   And in all these results that I presented today,
[00:19:02.040 --> 00:19:04.480]   we just took three snapshots of the model.
[00:19:04.480 --> 00:19:07.240]   Like by that I mean that during the early stage,
[00:19:07.240 --> 00:19:08.680]   we just took three snapshots
[00:19:08.680 --> 00:19:11.520]   when the model reached a decent accuracy
[00:19:11.520 --> 00:19:14.880]   for the testing model, for the training dataset.
[00:19:14.880 --> 00:19:18.200]   And then we took the last three checkpoints
[00:19:18.200 --> 00:19:21.200]   during the end cycle of the training process.
[00:19:21.200 --> 00:19:22.600]   So yeah, we just used three.
[00:19:22.600 --> 00:19:24.400]   - I see.
[00:19:24.400 --> 00:19:28.800]   And can you clarify, so if I have a single example,
[00:19:28.800 --> 00:19:33.240]   I'm gonna have, yeah, I'm gonna have a variety
[00:19:33.240 --> 00:19:34.960]   of like gradient values if I back propagate
[00:19:34.960 --> 00:19:38.320]   all the way to the pixel level input, right?
[00:19:38.320 --> 00:19:40.320]   But that variance isn't what you need.
[00:19:40.320 --> 00:19:42.080]   What you need is variance of the gradient
[00:19:42.080 --> 00:19:43.840]   over time through training.
[00:19:43.840 --> 00:19:45.600]   - Exactly.
[00:19:45.600 --> 00:19:48.320]   So what you just described would give us
[00:19:48.320 --> 00:19:50.800]   like the gradient for each snapshot.
[00:19:50.800 --> 00:19:52.920]   That would be like the basically the gradient heat map
[00:19:52.920 --> 00:19:55.080]   of that image at that particular snapshot.
[00:19:55.080 --> 00:19:56.920]   But what we are trying to do here is we're trying
[00:19:56.920 --> 00:19:59.760]   to calculate the variance of those heat maps,
[00:19:59.760 --> 00:20:01.200]   gradient heat maps in this case
[00:20:01.200 --> 00:20:02.760]   across all the different checkpoints
[00:20:02.760 --> 00:20:04.480]   and then calculate the variance.
[00:20:04.480 --> 00:20:05.320]   - I see.
[00:20:05.320 --> 00:20:08.480]   And one set of questions I really wanted
[00:20:08.480 --> 00:20:11.480]   to kind of dive in a little bit is what you can do
[00:20:11.480 --> 00:20:15.680]   with this variance of gradient just on the basis of,
[00:20:15.680 --> 00:20:17.720]   like without doing any training actually.
[00:20:17.720 --> 00:20:19.720]   So like at the very beginning,
[00:20:19.720 --> 00:20:21.840]   I haven't done anything with my data set.
[00:20:21.840 --> 00:20:23.720]   I have a random network.
[00:20:23.720 --> 00:20:25.720]   Like, would you be able to look at the variance
[00:20:25.720 --> 00:20:28.200]   of the gradient like across random initializations
[00:20:28.200 --> 00:20:30.240]   to get a sense for something like this metric
[00:20:30.240 --> 00:20:32.160]   or is it really important that it's the same network,
[00:20:32.160 --> 00:20:34.560]   the same architecture over time?
[00:20:34.560 --> 00:20:36.880]   - So if I understand the question correctly,
[00:20:36.880 --> 00:20:39.200]   are you asking whether we can serve
[00:20:39.200 --> 00:20:43.680]   as a prior information before the actual training starts?
[00:20:43.680 --> 00:20:45.920]   - Yeah, that's the general direction I'm pushing.
[00:20:45.920 --> 00:20:48.360]   The first idea I had in that direction was just like,
[00:20:48.360 --> 00:20:50.800]   let's try five different random initializations
[00:20:50.800 --> 00:20:53.080]   and do VOG across that.
[00:20:53.080 --> 00:20:54.120]   - That's a good question.
[00:20:54.120 --> 00:20:56.120]   So we did try this thing up,
[00:20:56.120 --> 00:21:00.960]   but actually you presented a good point here.
[00:21:00.960 --> 00:21:03.160]   So what you're basically saying is,
[00:21:03.160 --> 00:21:04.960]   in the stability experiment,
[00:21:04.960 --> 00:21:07.920]   we trained five different models.
[00:21:07.920 --> 00:21:10.080]   What you're basically suggesting is we don't train them.
[00:21:10.080 --> 00:21:13.160]   We just initialize them and calculate the VOG
[00:21:13.160 --> 00:21:15.520]   or the uninitialized model.
[00:21:15.520 --> 00:21:17.960]   But then my question would be, if you're not training,
[00:21:17.960 --> 00:21:19.680]   then how would you calculate the variance?
[00:21:19.680 --> 00:21:22.840]   What you will be basically getting is the gradients
[00:21:22.840 --> 00:21:25.800]   for each of those random initialization.
[00:21:25.800 --> 00:21:28.800]   Then we can always do the variance
[00:21:28.800 --> 00:21:32.400]   across all the five different initializations.
[00:21:32.400 --> 00:21:33.800]   That is something we can do.
[00:21:33.800 --> 00:21:35.600]   But then again, if we do that,
[00:21:35.600 --> 00:21:38.800]   then that would not be pertaining to one particular model.
[00:21:38.800 --> 00:21:40.040]   That would be like the variance
[00:21:40.040 --> 00:21:41.440]   across five different runs.
[00:21:43.920 --> 00:21:48.880]   - Yeah, I guess the intuition that I kind of have here
[00:21:48.880 --> 00:21:52.680]   is I guess, yeah, the idea is that the example
[00:21:52.680 --> 00:21:54.960]   sort of maybe jumping across a decision boundary
[00:21:54.960 --> 00:21:56.280]   or something like that.
[00:21:56.280 --> 00:21:57.120]   Right?
[00:21:57.120 --> 00:21:59.480]   But I suppose for a random,
[00:21:59.480 --> 00:22:01.840]   yeah, for different random initializations,
[00:22:01.840 --> 00:22:03.760]   maybe you would expect there to be a lot of jumping
[00:22:03.760 --> 00:22:04.960]   across decision boundaries,
[00:22:04.960 --> 00:22:07.800]   which you wouldn't expect for the same network over time.
[00:22:07.800 --> 00:22:08.680]   - That's true.
[00:22:08.680 --> 00:22:11.160]   And moreover for a random initialized models,
[00:22:11.160 --> 00:22:13.400]   it'd be very interesting to see the gradients
[00:22:13.400 --> 00:22:15.360]   for each of the inputs,
[00:22:15.360 --> 00:22:17.200]   whether all of them will be random
[00:22:17.200 --> 00:22:18.920]   or there will be some.
[00:22:18.920 --> 00:22:20.840]   So like many works have also showed
[00:22:20.840 --> 00:22:22.840]   that even randomly initialized models
[00:22:22.840 --> 00:22:25.920]   can serve as good features for the model.
[00:22:25.920 --> 00:22:28.000]   There was a paper on DB mesh prior.
[00:22:28.000 --> 00:22:31.200]   So they also showed that randomly initialized networks
[00:22:31.200 --> 00:22:33.240]   also have some certain characteristics.
[00:22:33.240 --> 00:22:36.280]   So in that essence, I guess it's interesting
[00:22:36.280 --> 00:22:38.320]   to do this experiment, right?
[00:22:38.320 --> 00:22:42.360]   - Relatedly, I think people have found
[00:22:42.360 --> 00:22:44.600]   that random networks, if you look at the gradient maps
[00:22:44.600 --> 00:22:46.240]   for random networks, those like the things
[00:22:46.240 --> 00:22:47.600]   that you presented at the beginning,
[00:22:47.600 --> 00:22:49.200]   either the raw grading maps or something
[00:22:49.200 --> 00:22:51.440]   like smooth grad or integrated gradients,
[00:22:51.440 --> 00:22:52.960]   you do see a lot of structure
[00:22:52.960 --> 00:22:56.600]   even with a randomly initialized network when you do that.
[00:22:56.600 --> 00:23:01.040]   - We do some, but the images that I showed earlier
[00:23:01.040 --> 00:23:02.320]   were for trained models.
[00:23:02.320 --> 00:23:05.200]   - Yeah, yeah.
[00:23:05.200 --> 00:23:06.120]   This is something I,
[00:23:06.120 --> 00:23:08.160]   so I worked on interpretability for a little bit.
[00:23:08.160 --> 00:23:10.800]   And one of the things that was important in our application
[00:23:10.800 --> 00:23:13.240]   was we wanted to demonstrate that like it was different
[00:23:13.240 --> 00:23:14.760]   from a random network.
[00:23:14.760 --> 00:23:16.920]   And I found at least some people reporting
[00:23:16.920 --> 00:23:20.120]   that they like, there was much, much higher overlap
[00:23:20.120 --> 00:23:21.640]   than you would expect by chance.
[00:23:21.640 --> 00:23:26.640]   It's not the case that the gradient map looks random.
[00:23:26.640 --> 00:23:28.640]   It sort of concentrates on things like the center
[00:23:28.640 --> 00:23:31.320]   of the image and the highest contrast regions of the image.
[00:23:31.320 --> 00:23:32.520]   - Right, right, yeah.
[00:23:32.520 --> 00:23:33.400]   That's a good point.
[00:23:33.400 --> 00:23:34.800]   And I think, yeah, that would be interesting
[00:23:34.800 --> 00:23:37.440]   to see whether those things actually carry forward
[00:23:37.440 --> 00:23:40.360]   to all the images in the particular class.
[00:23:40.360 --> 00:23:41.640]   That would be interesting.
[00:23:41.640 --> 00:23:49.480]   - What possibilities are there for applying this in,
[00:23:49.480 --> 00:23:52.280]   just to start off like a regression problem
[00:23:52.280 --> 00:23:55.080]   based off of computer vision, like a bounding box thing.
[00:23:55.080 --> 00:23:57.360]   Like obviously you no longer have the class
[00:23:57.360 --> 00:23:59.840]   to like organize things by,
[00:23:59.840 --> 00:24:01.440]   but would you be able to detect
[00:24:01.440 --> 00:24:04.880]   maybe more difficult examples or something like that?
[00:24:04.880 --> 00:24:06.400]   - That's a good question actually.
[00:24:06.400 --> 00:24:09.680]   So we haven't looked into it right now,
[00:24:09.680 --> 00:24:12.160]   but I clearly see that we can easily
[00:24:12.160 --> 00:24:14.800]   use the same formulation here.
[00:24:14.800 --> 00:24:18.320]   But in that case, basically we back-publicate the gradient,
[00:24:18.320 --> 00:24:20.040]   not with respect to the ground truth class,
[00:24:20.040 --> 00:24:21.720]   but here, as you're saying,
[00:24:21.720 --> 00:24:24.040]   we have like, let's say the bounding box coordinates.
[00:24:24.040 --> 00:24:25.680]   So we can easily back-publicate the class
[00:24:25.680 --> 00:24:27.720]   using the difference in the coordinates
[00:24:27.720 --> 00:24:29.760]   or using the ground truth bounding box
[00:24:29.760 --> 00:24:31.320]   or the predictor bounding box.
[00:24:31.320 --> 00:24:32.560]   So I think in that sense,
[00:24:32.560 --> 00:24:36.480]   it can be used for object detection or image segmentation,
[00:24:36.480 --> 00:24:38.960]   those all kinds of those different tasks.
[00:24:39.960 --> 00:24:40.800]   - Gotcha.
[00:24:40.800 --> 00:24:45.640]   And what about in application to different domains
[00:24:45.640 --> 00:24:47.600]   where there is also a class,
[00:24:47.600 --> 00:24:51.440]   like say like audio classification or sentiment analysis
[00:24:51.440 --> 00:24:53.000]   and NLP, something like that?
[00:24:53.000 --> 00:24:54.520]   - Yep, I think that's the,
[00:24:54.520 --> 00:24:57.280]   I think I don't want to sound very philosophical,
[00:24:57.280 --> 00:25:01.160]   but I think that's the beauty of VOG in this case,
[00:25:01.160 --> 00:25:03.440]   that it doesn't not depend,
[00:25:03.440 --> 00:25:05.480]   it does not take anything from the model
[00:25:05.480 --> 00:25:06.680]   or the architecture.
[00:25:06.680 --> 00:25:08.440]   It just uses the gradients.
[00:25:08.440 --> 00:25:11.840]   So in any application or domain, as you're mentioning,
[00:25:11.840 --> 00:25:13.960]   if we can by any chance calculate
[00:25:13.960 --> 00:25:15.560]   the back-publicated gradients,
[00:25:15.560 --> 00:25:20.560]   which is the main point in all deep networks
[00:25:20.560 --> 00:25:23.440]   that we use the back-publicated gradients.
[00:25:23.440 --> 00:25:28.320]   So any application doing that can be easily used,
[00:25:28.320 --> 00:25:30.520]   use VOG as a proxy metric.
[00:25:30.520 --> 00:25:35.800]   - So I guess the question I have is,
[00:25:35.800 --> 00:25:40.800]   do you have any indications that it'll work just as well
[00:25:40.800 --> 00:25:42.080]   in one of these other domains?
[00:25:42.080 --> 00:25:46.240]   Because it's not guaranteed that necessarily something,
[00:25:46.240 --> 00:25:48.360]   maybe it's very important that there's say,
[00:25:48.360 --> 00:25:50.320]   these max pooling operations that you find
[00:25:50.320 --> 00:25:53.680]   in convolutional networks to make this work really nicely
[00:25:53.680 --> 00:25:54.960]   or something like that.
[00:25:54.960 --> 00:25:59.960]   - So I myself haven't tried this other applications,
[00:25:59.960 --> 00:26:03.400]   but we have received some emails
[00:26:03.400 --> 00:26:07.120]   regarding interested researchers applying it
[00:26:07.120 --> 00:26:10.800]   to sequencing data or segmentation models.
[00:26:10.800 --> 00:26:12.680]   So yeah, people are using it
[00:26:12.680 --> 00:26:14.920]   as for different applications.
[00:26:14.920 --> 00:26:15.960]   - That's exciting.
[00:26:15.960 --> 00:26:21.480]   I think, yeah, the emphasis I think was in your talk
[00:26:21.480 --> 00:26:26.480]   was on a lot of the dataset cleaning kind of side of things
[00:26:26.480 --> 00:26:28.240]   or the outlier detection kind of side of things.
[00:26:28.240 --> 00:26:29.800]   Can you talk a little bit more about
[00:26:29.800 --> 00:26:31.800]   how you think variants of gradient could be used
[00:26:31.800 --> 00:26:36.800]   for the explainable AI side of things?
[00:26:36.800 --> 00:26:38.720]   - So that's a good point.
[00:26:38.720 --> 00:26:41.160]   So in this case, you're right.
[00:26:41.160 --> 00:26:43.040]   Most of our results were actually focused
[00:26:43.040 --> 00:26:46.160]   on detecting auto-distribution data
[00:26:46.160 --> 00:26:48.800]   or typical examples during the training process.
[00:26:48.800 --> 00:26:51.080]   With respect to explainability,
[00:26:51.080 --> 00:26:52.960]   so the gradient maps are itself,
[00:26:52.960 --> 00:26:57.920]   as I said, in 2013 when the work started,
[00:26:57.920 --> 00:26:59.960]   people actually felt that gradients
[00:26:59.960 --> 00:27:02.880]   can be the best explanation to a model,
[00:27:02.880 --> 00:27:05.320]   which some people still believe that
[00:27:05.320 --> 00:27:08.800]   because it has a clear connection to the model.
[00:27:08.800 --> 00:27:09.640]   So in that case,
[00:27:09.640 --> 00:27:12.520]   we're actually using some notions of explainability
[00:27:12.520 --> 00:27:13.800]   in this case.
[00:27:13.800 --> 00:27:17.080]   Now, the other point that you brought up is,
[00:27:17.080 --> 00:27:20.440]   can we use, rather than using just raw gradient maps,
[00:27:20.440 --> 00:27:23.720]   can we actually use these different variants of gradients,
[00:27:23.720 --> 00:27:26.800]   like the smooth grad or integrated gradients,
[00:27:26.800 --> 00:27:30.960]   and then calculate the VODs on those XAI-based
[00:27:30.960 --> 00:27:32.480]   heat map algorithms?
[00:27:32.480 --> 00:27:34.800]   So that would be something interesting to try.
[00:27:34.800 --> 00:27:36.920]   - I see, yeah.
[00:27:36.920 --> 00:27:38.560]   So the idea is you're actually,
[00:27:38.560 --> 00:27:41.480]   it's like a variance of the map over time,
[00:27:41.480 --> 00:27:43.000]   and that doesn't have to be the gradient.
[00:27:43.000 --> 00:27:44.320]   It could be any other map.
[00:27:44.320 --> 00:27:46.040]   - Exactly, so right, exactly.
[00:27:46.040 --> 00:27:46.880]   That's the point.
[00:27:46.880 --> 00:27:48.760]   So right now we are just taking the VADINA gradients,
[00:27:48.760 --> 00:27:51.840]   but yep, we can always try to
[00:27:51.840 --> 00:27:55.800]   extrapolate other gradient-based methods on this.
[00:27:56.640 --> 00:28:01.200]   - Hmm, that's, yeah, interesting.
[00:28:01.200 --> 00:28:06.480]   So I guess in the last couple minutes of your time,
[00:28:06.480 --> 00:28:08.360]   could you talk a little bit about
[00:28:08.360 --> 00:28:10.480]   sort of the future directions you see,
[00:28:10.480 --> 00:28:11.720]   you taking this work,
[00:28:11.720 --> 00:28:15.600]   and maybe your career in ML?
[00:28:15.600 --> 00:28:19.160]   - So I'll try to make it brief.
[00:28:19.160 --> 00:28:21.800]   So with respect to VOD,
[00:28:21.800 --> 00:28:24.240]   I think there are a few interesting directions.
[00:28:24.240 --> 00:28:25.560]   Some of them, as we discussed,
[00:28:25.560 --> 00:28:29.480]   that can be actually use VOGs,
[00:28:29.480 --> 00:28:31.600]   the results that we have for our image classifier,
[00:28:31.600 --> 00:28:33.160]   and then use it for different applications
[00:28:33.160 --> 00:28:34.320]   and other domains also,
[00:28:34.320 --> 00:28:36.240]   not respect to just images,
[00:28:36.240 --> 00:28:39.760]   but let's say language models and graph networks.
[00:28:39.760 --> 00:28:43.080]   And that's like one very promising direction.
[00:28:43.080 --> 00:28:46.640]   And other one is also to use the VOG scores
[00:28:46.640 --> 00:28:49.680]   and understand the training on the fly,
[00:28:49.680 --> 00:28:52.600]   and basically leverage the VOG scores
[00:28:52.600 --> 00:28:55.160]   to improve the training performance of a model.
[00:28:55.160 --> 00:28:57.000]   That would be interesting direction.
[00:28:57.000 --> 00:29:01.720]   - Yeah, that does seem,
[00:29:01.720 --> 00:29:05.360]   yeah, there's a lot of room still, I think,
[00:29:05.360 --> 00:29:07.520]   to improve what people do during training to,
[00:29:07.520 --> 00:29:08.720]   yeah, four things, yeah,
[00:29:08.720 --> 00:29:11.200]   out of distribution data is a hard,
[00:29:11.200 --> 00:29:12.960]   it's not something you're gonna resolve
[00:29:12.960 --> 00:29:14.840]   with normal gradient descent.
[00:29:14.840 --> 00:29:17.160]   So that sounds like a great direction to take.
[00:29:17.160 --> 00:29:18.360]   - Right, right, exactly.
[00:29:18.360 --> 00:29:22.360]   - All right, well, thanks for coming on, Chirag.
[00:29:22.360 --> 00:29:24.080]   And thanks for presenting some,
[00:29:24.080 --> 00:29:26.440]   and thanks for doing some really great
[00:29:26.440 --> 00:29:27.360]   and interesting research
[00:29:27.360 --> 00:29:29.200]   and being willing to present it to us.
[00:29:29.200 --> 00:29:30.880]   - Thank you, Charles, for inviting me today.
[00:29:30.880 --> 00:29:32.920]   It was a pleasure talking to you.
[00:29:32.920 --> 00:29:35.200]   - Mm-hmm, all right, take care, Chirag.
[00:29:35.200 --> 00:29:36.400]   - Bye.
[00:29:36.400 --> 00:29:38.280]   - All right, peace.
[00:29:38.280 --> 00:29:39.800]   I am actually,
[00:29:39.800 --> 00:29:42.040]   normally this is where I would introduce the next person,
[00:29:42.040 --> 00:29:45.960]   and, you know, but the next person is me.
[00:29:45.960 --> 00:29:47.120]   So I guess I'll just,
[00:29:47.120 --> 00:29:50.800]   I'll hop right into my presentation.
[00:29:50.800 --> 00:29:51.640]   - Sure.
[00:29:51.640 --> 00:29:55.400]   - And take over the screen share, great.
[00:29:55.400 --> 00:30:02.680]   All right, so what I wanted to talk about today
[00:30:02.680 --> 00:30:07.680]   is one of my sort of favorite test problems,
[00:30:07.680 --> 00:30:10.640]   one of my favorite sort of basic,
[00:30:10.640 --> 00:30:12.880]   simple,
[00:30:12.880 --> 00:30:16.640]   simple examples that I have in my head
[00:30:16.640 --> 00:30:18.800]   when I'm thinking about neural network optimization
[00:30:18.800 --> 00:30:20.480]   and neural networks in general.
[00:30:20.480 --> 00:30:22.040]   So the motivating idea here
[00:30:22.040 --> 00:30:24.640]   is that neural network optimization is really complicated.
[00:30:24.640 --> 00:30:26.840]   It's got a lot of pieces to it.
[00:30:26.840 --> 00:30:29.280]   And even though it might feel easy now,
[00:30:29.280 --> 00:30:32.760]   it's not actually that easy of a thing to do.
[00:30:32.760 --> 00:30:35.600]   So, you know, nowadays we've got so many good
[00:30:35.600 --> 00:30:37.400]   architectural ideas.
[00:30:37.400 --> 00:30:39.480]   We've got so many good,
[00:30:39.480 --> 00:30:41.880]   just sort of, you know, built-ins
[00:30:41.880 --> 00:30:44.880]   in the neural network libraries that we use,
[00:30:44.880 --> 00:30:46.400]   like PyTorch and TensorFlow,
[00:30:46.400 --> 00:30:48.720]   that it doesn't strike us as that difficult
[00:30:48.720 --> 00:30:49.840]   to spin up a neural network
[00:30:49.840 --> 00:30:52.400]   and train it on something like image classification.
[00:30:52.400 --> 00:30:55.080]   And this is something that the
[00:30:55.080 --> 00:30:57.600]   Optimization for Deep Learning Review Paper
[00:30:57.600 --> 00:31:01.560]   by Roh You-Sun at U of I handles really,
[00:31:01.560 --> 00:31:04.400]   or mentions, and I think makes a good point about,
[00:31:04.400 --> 00:31:06.880]   that if it was 1980 and you sat down
[00:31:06.880 --> 00:31:08.640]   to try and solve an image classification problem
[00:31:08.640 --> 00:31:11.120]   in neural networks, you probably would fail
[00:31:11.120 --> 00:31:13.640]   'cause you don't have all these nice things
[00:31:13.640 --> 00:31:15.800]   in your toolkit that we have.
[00:31:15.800 --> 00:31:19.520]   And so because of that, I think it's easy for us
[00:31:19.520 --> 00:31:22.400]   to forget that there's a lot of complexity
[00:31:22.400 --> 00:31:24.760]   underneath something like .fit.
[00:31:24.760 --> 00:31:29.480]   And so when it comes to these,
[00:31:29.480 --> 00:31:33.080]   when it comes to trying to understand something difficult
[00:31:33.080 --> 00:31:36.280]   in mathematics, which is in the end,
[00:31:36.280 --> 00:31:39.200]   I think one of the best ways to understand
[00:31:39.200 --> 00:31:40.480]   what's going on in deep learning
[00:31:40.480 --> 00:31:44.000]   in this numerical computing kind of field,
[00:31:44.000 --> 00:31:47.520]   I think the most important thing that is underappreciated
[00:31:47.520 --> 00:31:50.040]   by people who come from a software background
[00:31:50.040 --> 00:31:53.520]   is the importance of simple test problems.
[00:31:53.520 --> 00:31:56.560]   And so I think there's a really great explanation
[00:31:56.560 --> 00:32:01.240]   of this that I got from a friend who finished their PhD
[00:32:01.240 --> 00:32:05.520]   in math at Berkeley around the same time I was doing mine.
[00:32:05.520 --> 00:32:08.720]   And what they said was, "Everyone I know who does math
[00:32:08.720 --> 00:32:11.080]   "does basically the exact same thing
[00:32:11.080 --> 00:32:12.920]   "to make their lives easier."
[00:32:12.920 --> 00:32:14.360]   Whenever they hear a conjecture,
[00:32:14.360 --> 00:32:17.320]   somebody's thought about how something might work
[00:32:17.320 --> 00:32:18.680]   in some branch of math,
[00:32:18.680 --> 00:32:22.320]   they compare it to two examples specific to that branch.
[00:32:22.320 --> 00:32:24.320]   One that's well-behaved and simple
[00:32:24.320 --> 00:32:27.520]   and another that's pathological and counterintuitive.
[00:32:27.520 --> 00:32:29.600]   And if the conjecture is true on both,
[00:32:29.600 --> 00:32:30.800]   then it's probably true.
[00:32:30.800 --> 00:32:31.640]   And if it's not,
[00:32:31.640 --> 00:32:33.920]   then you've got yourself a nice counter example.
[00:32:33.920 --> 00:32:36.480]   So when it comes to trying to understand
[00:32:36.480 --> 00:32:38.240]   and think about machine learning,
[00:32:38.240 --> 00:32:40.320]   like what's going to work well,
[00:32:40.320 --> 00:32:42.440]   what's going to not work well,
[00:32:42.440 --> 00:32:44.640]   why to use a particular piece
[00:32:44.640 --> 00:32:46.040]   or why not to use a particular piece,
[00:32:46.040 --> 00:32:47.640]   or if somebody makes a claim
[00:32:47.640 --> 00:32:51.440]   about the nature of deep neural network training,
[00:32:51.440 --> 00:32:55.280]   I use the same tactic.
[00:32:55.280 --> 00:32:56.960]   And so my two examples,
[00:32:56.960 --> 00:33:00.440]   one of them is memorization tasks.
[00:33:00.440 --> 00:33:03.640]   That's one of my pathological test examples
[00:33:03.640 --> 00:33:04.680]   that if somebody makes a claim
[00:33:04.680 --> 00:33:06.680]   about how neural networks work and how they generalize,
[00:33:06.680 --> 00:33:09.480]   I think, okay, does this explain memorization
[00:33:09.480 --> 00:33:11.320]   in neural networks well?
[00:33:11.320 --> 00:33:13.200]   So specifically a memorization task
[00:33:13.200 --> 00:33:14.960]   is one where you destroy all structure
[00:33:14.960 --> 00:33:17.000]   relating the inputs to the labels.
[00:33:17.000 --> 00:33:21.240]   So if you were to take, say, the MNIST dataset,
[00:33:21.240 --> 00:33:23.400]   and then instead of having the actual targets,
[00:33:23.400 --> 00:33:25.280]   you just shuffle them.
[00:33:25.280 --> 00:33:27.120]   So this little bit of Python code here
[00:33:27.120 --> 00:33:28.240]   takes the MNIST targets
[00:33:28.240 --> 00:33:30.560]   and replaces them with a shuffled version of them.
[00:33:30.560 --> 00:33:31.480]   Turns out neural networks
[00:33:31.480 --> 00:33:33.720]   can still minimize the training loss in this example,
[00:33:33.720 --> 00:33:35.640]   even though there is no, you know,
[00:33:35.640 --> 00:33:38.720]   there's no information there anymore.
[00:33:38.720 --> 00:33:42.640]   It's just the labels are effectively random.
[00:33:42.640 --> 00:33:44.760]   So this is something that people have demonstrated
[00:33:44.760 --> 00:33:47.800]   and it shows that neural networks are capable
[00:33:47.800 --> 00:33:49.440]   of memorizing their datasets,
[00:33:49.440 --> 00:33:53.320]   even if they don't necessarily do that in real life.
[00:33:53.320 --> 00:33:57.680]   So the two sides of this coin
[00:33:57.680 --> 00:33:58.840]   are you have one example
[00:33:58.840 --> 00:34:02.720]   that's sort of the canonical gnarly bad example.
[00:34:02.720 --> 00:34:04.120]   And then you have one example
[00:34:04.120 --> 00:34:06.680]   that's the well-behaved and simple example.
[00:34:06.680 --> 00:34:09.920]   And for me, I use one that is effectively
[00:34:09.920 --> 00:34:14.280]   just multiplying by one with a deep neural network.
[00:34:14.280 --> 00:34:15.720]   So let's go through this.
[00:34:15.720 --> 00:34:18.360]   It's got a mathematical inspiration.
[00:34:18.360 --> 00:34:21.000]   And so I took a mathematical approach here.
[00:34:21.000 --> 00:34:24.440]   So a fully connected network, let's define that.
[00:34:24.440 --> 00:34:27.640]   It's a function of its parameters and its inputs
[00:34:27.640 --> 00:34:29.840]   that alternates between matrix multiplication
[00:34:29.840 --> 00:34:31.160]   and function application.
[00:34:31.160 --> 00:34:33.240]   So it's got all these nonlinearities,
[00:34:33.240 --> 00:34:35.320]   sigma one, two, up to K.
[00:34:35.320 --> 00:34:40.000]   And then it's also got these parameter matrices,
[00:34:40.000 --> 00:34:41.720]   theta up to K.
[00:34:41.720 --> 00:34:45.840]   And we alternate between applying the matrix to the input
[00:34:45.840 --> 00:34:47.920]   and then applying a function to it,
[00:34:47.920 --> 00:34:50.520]   a possibly nonlinear function to it.
[00:34:50.520 --> 00:34:55.840]   An autoencoder network is a specific type of network,
[00:34:55.840 --> 00:34:58.760]   one that compares its outputs directly to its inputs
[00:34:58.760 --> 00:35:01.080]   as opposed to comparing them to something else.
[00:35:01.080 --> 00:35:03.320]   So D is some function that takes two inputs.
[00:35:03.320 --> 00:35:05.040]   If they're the same, it returns zero.
[00:35:05.040 --> 00:35:07.640]   Otherwise it returns something else.
[00:35:07.640 --> 00:35:10.840]   And what D is doing here is it's comparing the input X
[00:35:10.840 --> 00:35:15.000]   to the output of the network F applied to its parameters
[00:35:15.000 --> 00:35:16.320]   and its input X.
[00:35:16.320 --> 00:35:20.240]   And that function there that I'm defining here
[00:35:20.240 --> 00:35:21.720]   through this is the loss function.
[00:35:21.720 --> 00:35:24.360]   It says, okay, the loss function compares the inputs
[00:35:24.360 --> 00:35:25.320]   to the outputs.
[00:35:25.320 --> 00:35:26.880]   And the loss is what we make small
[00:35:26.880 --> 00:35:29.280]   and an autoencoder therefore returns something
[00:35:29.280 --> 00:35:31.000]   that's close to its inputs.
[00:35:31.000 --> 00:35:34.840]   So the case of autoencoders includes
[00:35:34.840 --> 00:35:38.640]   just multiplying numbers by one as an important special case.
[00:35:39.560 --> 00:35:41.720]   So if we just take all of those functions
[00:35:41.720 --> 00:35:43.080]   to be the identity, right?
[00:35:43.080 --> 00:35:46.600]   Just take all the nonlinearities out of our neural network,
[00:35:46.600 --> 00:35:48.520]   we'll end up with something called a linear network.
[00:35:48.520 --> 00:35:50.320]   And linear networks, I'm not the only one
[00:35:50.320 --> 00:35:52.360]   who likes to use these as a test case
[00:35:52.360 --> 00:35:54.000]   for doing theory on neural networks
[00:35:54.000 --> 00:35:55.800]   or for thinking about neural networks.
[00:35:55.800 --> 00:35:59.360]   So there's a nice paper by Sachs, McClelland and Ganguly
[00:35:59.360 --> 00:36:01.960]   that's basically what can we do to understand
[00:36:01.960 --> 00:36:03.280]   not only how neural networks run,
[00:36:03.280 --> 00:36:05.840]   but actually a little bit of the psychology
[00:36:05.840 --> 00:36:08.120]   of how animals and humans learn
[00:36:08.120 --> 00:36:12.160]   just by studying what happens in linear networks.
[00:36:12.160 --> 00:36:14.200]   It's also the case that networks
[00:36:14.200 --> 00:36:16.920]   with rectifying linear units are actually piecewise linear
[00:36:16.920 --> 00:36:17.760]   on their inputs.
[00:36:17.760 --> 00:36:19.120]   So this gives you a sense of like, okay,
[00:36:19.120 --> 00:36:21.240]   if I only change the input a little bit,
[00:36:21.240 --> 00:36:25.600]   a rectified linear network will behave very close
[00:36:25.600 --> 00:36:27.480]   to a linear network.
[00:36:27.480 --> 00:36:29.560]   So it's another reason why this case is useful
[00:36:29.560 --> 00:36:32.760]   for helping you build your intuition about what's going on.
[00:36:32.760 --> 00:36:37.400]   The difference being that because it's simple,
[00:36:37.400 --> 00:36:39.760]   and because it's dropped that nonlinear piece,
[00:36:39.760 --> 00:36:41.800]   it's a lot easier to understand.
[00:36:41.800 --> 00:36:45.680]   So if we also take another step and make this even simpler
[00:36:45.680 --> 00:36:48.040]   and choose the dimension of everything to be one.
[00:36:48.040 --> 00:36:49.760]   So every hidden layer has dimension one,
[00:36:49.760 --> 00:36:51.040]   the input has dimension one,
[00:36:51.040 --> 00:36:53.520]   and so the output has dimension one.
[00:36:53.520 --> 00:36:55.240]   We replace all that matrix multiplication
[00:36:55.240 --> 00:36:58.240]   with just a series of things multiplying together.
[00:36:58.240 --> 00:37:01.960]   And we get a function, looks a little bit like this.
[00:37:01.960 --> 00:37:04.880]   I happened to, I was using this case actually
[00:37:04.880 --> 00:37:06.520]   to help me understand Jack's better.
[00:37:06.520 --> 00:37:10.200]   So that's kind of where this talk came from.
[00:37:10.200 --> 00:37:13.080]   And so in Jack's style NumPy, that looks like,
[00:37:13.080 --> 00:37:16.960]   okay, F of theta and X takes the product of all the thetas
[00:37:16.960 --> 00:37:18.600]   and multiplies them by X.
[00:37:18.600 --> 00:37:21.240]   So the same thing as what's written above.
[00:37:21.240 --> 00:37:25.240]   And it's got this nice little bit of extra Jack stuff here
[00:37:25.240 --> 00:37:27.120]   to make it so that I can now apply this function
[00:37:27.120 --> 00:37:29.280]   on a single input or multiple inputs,
[00:37:29.280 --> 00:37:30.440]   so batch or non-batch,
[00:37:30.440 --> 00:37:33.280]   and I can apply it to one parameter set
[00:37:33.280 --> 00:37:34.960]   or many parameter sets all at once.
[00:37:34.960 --> 00:37:39.080]   So I can effectively apply 10 networks to 10 data points
[00:37:39.080 --> 00:37:42.480]   and get a hundred outputs with just the same function
[00:37:42.480 --> 00:37:44.520]   that I can use for normal forward prop.
[00:37:44.520 --> 00:37:45.840]   So Jack's is pretty great,
[00:37:45.840 --> 00:37:47.480]   is one thing I learned in the process
[00:37:47.480 --> 00:37:49.080]   of building this example.
[00:37:49.080 --> 00:37:52.880]   So the reason why I wanted to do this simple setting,
[00:37:52.880 --> 00:37:54.200]   and one of the things that makes this
[00:37:54.200 --> 00:37:56.040]   a really powerful example,
[00:37:56.040 --> 00:37:59.440]   is because it's so simple that we can actually see
[00:37:59.440 --> 00:38:01.560]   everything that we normally can't see
[00:38:01.560 --> 00:38:04.480]   when we're doing deep learning, for real,
[00:38:04.480 --> 00:38:08.480]   when we're doing it on images with thousands of pixels
[00:38:08.480 --> 00:38:11.560]   in them and trying to predict one of thousands of classes
[00:38:11.560 --> 00:38:14.240]   at the end, the dimension of everything is so big,
[00:38:14.240 --> 00:38:15.440]   the dimension of that input X,
[00:38:15.440 --> 00:38:18.800]   the dimension of the hidden of the parameters theta,
[00:38:18.800 --> 00:38:20.960]   these things are in the thousands, in the millions,
[00:38:20.960 --> 00:38:22.920]   in the billions nowadays.
[00:38:22.920 --> 00:38:26.720]   And so you're hosed for trying to think about that spatially
[00:38:26.720 --> 00:38:28.800]   and for trying to visualize it.
[00:38:28.800 --> 00:38:30.120]   But if we, in this case,
[00:38:30.120 --> 00:38:33.360]   choose the number of hidden layers
[00:38:34.200 --> 00:38:37.480]   to be one, so there's one hidden layer,
[00:38:37.480 --> 00:38:41.760]   and then one output layer, and so two layers total,
[00:38:41.760 --> 00:38:44.520]   then we've reduced this down so much
[00:38:44.520 --> 00:38:47.080]   that now there's only essentially two dimensions,
[00:38:47.080 --> 00:38:49.520]   two parameter dimensions to think about.
[00:38:49.520 --> 00:38:51.520]   And so anything that has to do with the parameters,
[00:38:51.520 --> 00:38:54.920]   we can visualize,
[00:38:54.920 --> 00:38:56.760]   'cause we have two dimensions for the parameters
[00:38:56.760 --> 00:38:58.480]   and then one dimension for whatever it is
[00:38:58.480 --> 00:38:59.680]   we're trying to measure.
[00:39:01.800 --> 00:39:04.000]   So for example, we can look at the network's output
[00:39:04.000 --> 00:39:05.680]   as a function of the parameters here.
[00:39:05.680 --> 00:39:08.640]   So this is a surface here.
[00:39:08.640 --> 00:39:12.600]   So the values there, it gets red as the value gets higher
[00:39:12.600 --> 00:39:14.560]   and it gets blue as the value goes lower
[00:39:14.560 --> 00:39:17.080]   and then in between it is gray.
[00:39:17.080 --> 00:39:20.520]   And that Z axis, which is also the color axis,
[00:39:20.520 --> 00:39:23.440]   is the output of the network on the number one.
[00:39:23.440 --> 00:39:25.840]   So if I put in the number one to this network,
[00:39:25.840 --> 00:39:27.520]   what do the outputs look like?
[00:39:27.520 --> 00:39:31.320]   And it looks like the outputs get big as the parameters get,
[00:39:31.320 --> 00:39:33.200]   as the parameters have the same sign.
[00:39:33.200 --> 00:39:36.760]   So as we go to two, two in the upper right quadrant,
[00:39:36.760 --> 00:39:39.800]   and it gets, and similarly,
[00:39:39.800 --> 00:39:42.160]   the output gets big if both inputs,
[00:39:42.160 --> 00:39:44.960]   if both parameters are negative and large,
[00:39:44.960 --> 00:39:48.080]   that the bottom left corner, which is also red.
[00:39:48.080 --> 00:39:55.720]   So the, in order to finally get a loss function
[00:39:55.720 --> 00:39:57.640]   and get ourselves to the point where we're looking
[00:39:57.640 --> 00:39:59.440]   at a neural network optimization problem,
[00:39:59.440 --> 00:40:02.040]   we just need to choose that difference there,
[00:40:02.040 --> 00:40:05.840]   that D function that compares the inputs to the outputs.
[00:40:05.840 --> 00:40:08.640]   And so if we choose the squared error,
[00:40:08.640 --> 00:40:11.360]   then the loss function is just the squared difference
[00:40:11.360 --> 00:40:12.560]   between the inputs and the outputs,
[00:40:12.560 --> 00:40:14.640]   averaged over a batch.
[00:40:14.640 --> 00:40:16.000]   That's easy to code up.
[00:40:16.000 --> 00:40:20.480]   We just apply the mean of the squared error function
[00:40:20.480 --> 00:40:21.320]   like that.
[00:40:21.320 --> 00:40:24.760]   And that loss is actually simple enough
[00:40:24.760 --> 00:40:27.840]   that it's something we can write out explicitly.
[00:40:27.840 --> 00:40:29.160]   So if we just say, all right,
[00:40:29.160 --> 00:40:32.640]   let's say that our only input is the number one,
[00:40:32.640 --> 00:40:34.480]   there's only one data point.
[00:40:34.480 --> 00:40:37.560]   Then if we look at that as a function of theta,
[00:40:37.560 --> 00:40:39.680]   let's see what the loss looks like.
[00:40:39.680 --> 00:40:44.480]   So first, theta one times theta zero times one
[00:40:44.480 --> 00:40:48.440]   is what F of theta colon one is up there,
[00:40:48.440 --> 00:40:51.120]   theta semicolon one up there is.
[00:40:51.120 --> 00:40:53.120]   That's the definition of what our network does
[00:40:53.120 --> 00:40:54.160]   to its inputs.
[00:40:54.160 --> 00:40:56.320]   So that simplifies things.
[00:40:56.320 --> 00:40:58.800]   And then we can take that and write it out,
[00:40:58.800 --> 00:41:00.640]   it sort of expanded out if we'd like,
[00:41:00.640 --> 00:41:03.280]   it makes it a little bit easier to do other manipulations.
[00:41:03.280 --> 00:41:07.160]   And it's very clear now that this is just a polynomial.
[00:41:07.160 --> 00:41:08.200]   And that's helpful,
[00:41:08.200 --> 00:41:10.680]   especially if you have a bit more of a math background
[00:41:10.680 --> 00:41:12.760]   to recognize something that you may have seen
[00:41:12.760 --> 00:41:15.120]   in other places in mathematics,
[00:41:15.120 --> 00:41:17.040]   in other problems that you've run into,
[00:41:17.040 --> 00:41:20.800]   you've probably run into polynomials before.
[00:41:20.800 --> 00:41:25.720]   Whereas neural networks are maybe a new and curious thing.
[00:41:25.720 --> 00:41:26.560]   On the other hand,
[00:41:26.560 --> 00:41:28.200]   maybe if you know a little bit less about math
[00:41:28.200 --> 00:41:30.280]   and are a little bit less comfortable with polynomials,
[00:41:30.280 --> 00:41:33.040]   it's interesting to see that the simplest version
[00:41:33.040 --> 00:41:37.400]   of a neural network ends up being a polynomial
[00:41:37.400 --> 00:41:39.600]   in terms of its loss function.
[00:41:39.600 --> 00:41:43.080]   So not only can we write it out explicitly,
[00:41:43.080 --> 00:41:44.920]   we can also sort of look at it.
[00:41:44.920 --> 00:41:46.200]   We can directly look at it,
[00:41:46.200 --> 00:41:48.600]   we can visualize this loss function.
[00:41:48.600 --> 00:41:51.560]   So this is the thing that we are moving on
[00:41:51.560 --> 00:41:52.760]   when we do gradient descent.
[00:41:52.760 --> 00:41:54.600]   This is the thing that we are,
[00:41:54.600 --> 00:41:58.520]   because it's in 2D with this one extra dimension,
[00:41:58.520 --> 00:42:00.080]   it's sort of a lot more physical.
[00:42:00.080 --> 00:42:01.120]   It looks a little bit,
[00:42:01.120 --> 00:42:03.640]   if you look at it like a mountain range.
[00:42:03.640 --> 00:42:04.720]   There's two of those,
[00:42:04.720 --> 00:42:07.680]   those two big red mountains coming up off the side,
[00:42:07.680 --> 00:42:09.520]   a nice big valley in the middle.
[00:42:09.520 --> 00:42:14.800]   It's a little bit more physical
[00:42:14.800 --> 00:42:15.880]   and we can think about it
[00:42:15.880 --> 00:42:19.560]   in these sort of more physical terms.
[00:42:19.680 --> 00:42:23.480]   And this, in a way that we couldn't,
[00:42:23.480 --> 00:42:25.520]   if we were looking at a neural network problem
[00:42:25.520 --> 00:42:28.000]   where this is tens of thousands of dimensions.
[00:42:28.000 --> 00:42:31.560]   And even more so,
[00:42:31.560 --> 00:42:32.760]   we can also combine that
[00:42:32.760 --> 00:42:35.240]   with by hand mathematical calculations
[00:42:35.240 --> 00:42:37.280]   to get the values of the gradient.
[00:42:37.280 --> 00:42:39.080]   So if you sort of,
[00:42:39.080 --> 00:42:41.680]   if you crank through that polynomial
[00:42:41.680 --> 00:42:42.920]   that we had previously,
[00:42:42.920 --> 00:42:45.320]   you can calculate the gradient by hand,
[00:42:45.320 --> 00:42:47.840]   uses only the rules of the gradient calculus
[00:42:47.840 --> 00:42:51.320]   or polynomials, which are on the easier side of calculus
[00:42:51.320 --> 00:42:53.800]   when it comes to doing calculus.
[00:42:53.800 --> 00:42:56.120]   And you can actually get an explicit expression
[00:42:56.120 --> 00:42:57.160]   for the gradient.
[00:42:57.160 --> 00:42:59.680]   Of course, in Python,
[00:42:59.680 --> 00:43:01.640]   we rely on automatic differentiation,
[00:43:01.640 --> 00:43:02.600]   but it's helpful,
[00:43:02.600 --> 00:43:04.840]   especially in cases where we use automation
[00:43:04.840 --> 00:43:06.720]   to take some of the difficulty out
[00:43:06.720 --> 00:43:08.720]   of the actual problems we wanna work on
[00:43:08.720 --> 00:43:11.320]   to be able to go back to a simple problem like this one
[00:43:11.320 --> 00:43:13.320]   and actually crank it out,
[00:43:13.320 --> 00:43:15.120]   work on it step-by-step
[00:43:15.120 --> 00:43:18.360]   and see what exactly is being automated away.
[00:43:18.360 --> 00:43:21.720]   And the utility here is that we can actually then look
[00:43:21.720 --> 00:43:22.800]   at not just the loss,
[00:43:22.800 --> 00:43:26.240]   but also the gradient as an overall vector field.
[00:43:26.240 --> 00:43:29.160]   We can look at what the gradient is at every single point
[00:43:29.160 --> 00:43:32.680]   and get a better sense maybe
[00:43:32.680 --> 00:43:35.720]   for how gradient descent is going to behave.
[00:43:35.720 --> 00:43:36.560]   So you can see,
[00:43:36.560 --> 00:43:39.000]   if you take a look at this gradient field
[00:43:39.000 --> 00:43:40.800]   a little bit more closely,
[00:43:40.800 --> 00:43:44.560]   the highest values are very far away from the center.
[00:43:44.560 --> 00:43:46.760]   And then the values drop precipitously.
[00:43:46.760 --> 00:43:50.320]   So these little tiny arrows represent the gradient
[00:43:50.320 --> 00:43:52.320]   at each point they point in the direction
[00:43:52.320 --> 00:43:53.560]   gradient descent would go.
[00:43:53.560 --> 00:43:55.920]   So it's the negative gradient.
[00:43:55.920 --> 00:44:00.640]   And you can see that they're much, much tinier
[00:44:00.640 --> 00:44:03.200]   as we get down to the bottom here.
[00:44:03.200 --> 00:44:05.360]   So the scale of the gradient,
[00:44:05.360 --> 00:44:06.640]   it gets really, really big
[00:44:06.640 --> 00:44:08.240]   when you're far away from a solution.
[00:44:08.240 --> 00:44:10.920]   And then once you get even a little bit close to it,
[00:44:10.920 --> 00:44:12.480]   it shrinks back down.
[00:44:12.480 --> 00:44:14.520]   So this is something that's gonna generally be the case
[00:44:14.520 --> 00:44:15.520]   with neural networks.
[00:44:15.520 --> 00:44:17.680]   And this is one of the reasons why your loss goes down
[00:44:17.680 --> 00:44:19.920]   really, really quickly when you first start,
[00:44:19.920 --> 00:44:21.600]   'cause you tend to start in some area
[00:44:21.600 --> 00:44:24.000]   where you're not doing so well.
[00:44:24.000 --> 00:44:26.040]   And there's obvious big gradients
[00:44:26.040 --> 00:44:28.280]   that help you improve really, really quickly.
[00:44:28.280 --> 00:44:31.560]   So gradient descent is the method
[00:44:31.560 --> 00:44:33.440]   that gets used to optimize neural networks.
[00:44:33.440 --> 00:44:35.520]   We take that gradient and we apply it,
[00:44:35.520 --> 00:44:38.600]   we calculate it, scale it,
[00:44:38.600 --> 00:44:41.000]   flip it around and add it to the value of the parameters.
[00:44:41.000 --> 00:44:43.320]   So we follow those arrows there
[00:44:43.320 --> 00:44:45.800]   that I was showing on the surface.
[00:44:45.800 --> 00:44:50.440]   And so that looks like in Python code,
[00:44:50.440 --> 00:44:53.920]   it's theta minus eta times the gradient.
[00:44:53.920 --> 00:44:56.320]   And I grabbed, it took Jax here,
[00:44:56.320 --> 00:44:58.200]   and I actually vectorized that function,
[00:44:58.200 --> 00:45:00.360]   which gives you the ability to run gradient descent
[00:45:00.360 --> 00:45:03.760]   on multiple networks across batches at the same time,
[00:45:03.760 --> 00:45:06.320]   all with the same function that you would use to train one.
[00:45:06.320 --> 00:45:09.440]   Again, Jax has some pretty cool stuff in it.
[00:45:09.440 --> 00:45:11.400]   Though I feel obligated to point out
[00:45:11.400 --> 00:45:13.840]   that some things are a little bit harder in Jax,
[00:45:13.840 --> 00:45:17.240]   namely that if you wanna try and update the values
[00:45:17.240 --> 00:45:19.120]   of the matrix, say pre-allocate memory
[00:45:19.120 --> 00:45:20.720]   and then touch it later,
[00:45:20.720 --> 00:45:22.880]   Jax makes that a little bit harder.
[00:45:22.880 --> 00:45:26.400]   So I felt like I couldn't be completely positive
[00:45:26.400 --> 00:45:29.560]   without pointing out the one place I ran into some trouble.
[00:45:29.560 --> 00:45:33.160]   But that tiny bit of trouble aside,
[00:45:33.160 --> 00:45:37.560]   that little detail avoided,
[00:45:37.560 --> 00:45:39.920]   it was straightforward to set up
[00:45:39.920 --> 00:45:41.800]   this nice batch gradient descent.
[00:45:41.800 --> 00:45:45.360]   So to run gradient descent for a couple of hundred steps
[00:45:45.360 --> 00:45:47.960]   from a couple of hundred different positions
[00:45:47.960 --> 00:45:50.040]   was pretty fast.
[00:45:50.040 --> 00:45:55.040]   And that gave me this visualization here on this one.
[00:45:55.040 --> 00:45:59.080]   And you can see a large number of the trajectories
[00:45:59.080 --> 00:46:01.640]   are getting sucked towards the center there.
[00:46:01.640 --> 00:46:04.720]   So they're getting sucked towards the value zero, zero,
[00:46:04.720 --> 00:46:07.120]   right in the middle at the origin.
[00:46:07.120 --> 00:46:09.840]   So multiplying the inputs by zero
[00:46:09.840 --> 00:46:13.280]   and that's not what we wanted to do, right?
[00:46:13.280 --> 00:46:14.600]   We wanted to get a solution
[00:46:14.600 --> 00:46:17.000]   where we multiply the inputs by one.
[00:46:17.000 --> 00:46:20.160]   We want the inputs and the outputs to match one another.
[00:46:20.160 --> 00:46:22.680]   And so just looking at this one,
[00:46:22.680 --> 00:46:24.320]   it would look like, okay, well, there's,
[00:46:24.320 --> 00:46:26.080]   sometimes when I do gradient descent,
[00:46:26.080 --> 00:46:27.520]   it's gonna fail on this problem
[00:46:27.520 --> 00:46:31.220]   and sometimes it's gonna succeed.
[00:46:31.220 --> 00:46:35.800]   And so we can see that if we take a look
[00:46:35.800 --> 00:46:37.600]   at the losses over time.
[00:46:37.600 --> 00:46:40.080]   Losses is the kind of plot that you're maybe
[00:46:40.080 --> 00:46:42.480]   actually able to do when you're doing
[00:46:42.480 --> 00:46:44.360]   machine learning at scale.
[00:46:44.360 --> 00:46:46.120]   You can just plot the loss
[00:46:46.120 --> 00:46:48.440]   that you calculate through training.
[00:46:48.440 --> 00:46:50.880]   And so this looks actually quite different
[00:46:50.880 --> 00:46:52.360]   from what you would see during training
[00:46:52.360 --> 00:46:55.360]   in that there are all these traces
[00:46:55.360 --> 00:47:01.840]   that are not converging down to the lowest values.
[00:47:01.840 --> 00:47:05.240]   And this is just across different initializations.
[00:47:05.240 --> 00:47:07.000]   But when we work with typical neural networks,
[00:47:07.000 --> 00:47:09.680]   we use random normalized starting positions, right?
[00:47:09.680 --> 00:47:13.880]   We don't start on these special grid positions.
[00:47:13.880 --> 00:47:17.720]   So we need to randomly initialize things.
[00:47:17.720 --> 00:47:19.280]   And just for fun here,
[00:47:19.280 --> 00:47:21.080]   randomly doing things in JAX
[00:47:21.080 --> 00:47:24.120]   is actually quite a bit more challenging
[00:47:24.120 --> 00:47:27.840]   than it is in other frameworks.
[00:47:27.840 --> 00:47:29.020]   You have to be very explicit
[00:47:29.020 --> 00:47:31.520]   about where you're being random and non-random.
[00:47:31.520 --> 00:47:34.520]   But once I did that,
[00:47:34.520 --> 00:47:36.320]   it was pretty straightforward to do some,
[00:47:36.320 --> 00:47:37.520]   once I figured out how to do that,
[00:47:37.520 --> 00:47:40.640]   it was pretty straightforward to do random initializations.
[00:47:40.640 --> 00:47:43.000]   And things look actually quite a bit different.
[00:47:43.000 --> 00:47:44.040]   So this is, I think,
[00:47:44.040 --> 00:47:47.440]   the same number of initial trajectories,
[00:47:47.440 --> 00:47:48.920]   or at least the same number as the number
[00:47:48.920 --> 00:47:51.680]   that I plotted the loss of before.
[00:47:51.680 --> 00:47:53.480]   But now we can see that actually,
[00:47:53.480 --> 00:47:56.080]   none of them are going towards that center value.
[00:47:56.080 --> 00:47:59.960]   And all of them are actually starting off at lower levels
[00:47:59.960 --> 00:48:02.960]   and going down to at about the same height.
[00:48:02.960 --> 00:48:04.560]   So this is also something that we see
[00:48:04.560 --> 00:48:07.760]   in other neural networks, in bigger neural networks,
[00:48:07.760 --> 00:48:10.200]   so long as we're careful to randomly initialize
[00:48:10.200 --> 00:48:13.520]   and we're careful to set that scale in a thoughtful way.
[00:48:13.520 --> 00:48:15.400]   So to make sure that things are not just random,
[00:48:15.400 --> 00:48:16.560]   but also normalized.
[00:48:16.560 --> 00:48:18.960]   If you look at the starting positions of those,
[00:48:18.960 --> 00:48:22.440]   you'll see they form a nice circle in the XY plane.
[00:48:22.440 --> 00:48:25.560]   And training looks much better.
[00:48:25.560 --> 00:48:26.400]   It looks, you know,
[00:48:26.400 --> 00:48:27.480]   they all seem to be going down
[00:48:27.480 --> 00:48:28.920]   to the same value of the loss.
[00:48:28.920 --> 00:48:30.800]   Perfect score of zero.
[00:48:30.800 --> 00:48:33.500]   Seems like things are going in that direction.
[00:48:33.500 --> 00:48:36.240]   And with the, and one interesting thing here,
[00:48:36.240 --> 00:48:38.960]   so you can see one gets a little bit stuck.
[00:48:38.960 --> 00:48:41.640]   So it heads towards that center there, actually.
[00:48:41.640 --> 00:48:43.720]   Maybe if we head back a slide,
[00:48:43.720 --> 00:48:46.040]   you can see that a couple of these trajectories
[00:48:46.040 --> 00:48:49.260]   head towards the center of the,
[00:48:49.260 --> 00:48:54.720]   they head towards the center of the parameter area.
[00:48:54.720 --> 00:48:57.380]   And they, before heading down
[00:48:57.380 --> 00:48:59.040]   and joining the rest of the trajectories
[00:48:59.040 --> 00:49:01.120]   that are going to the correct solution.
[00:49:02.240 --> 00:49:04.680]   So that's what this trace here represents.
[00:49:04.680 --> 00:49:06.400]   That's one of those examples.
[00:49:06.400 --> 00:49:11.940]   And the, this is something that we do also sometimes see
[00:49:11.940 --> 00:49:13.260]   when we're doing,
[00:49:13.260 --> 00:49:16.200]   when we're applying neural networks at scale.
[00:49:16.200 --> 00:49:19.460]   We see sometimes there are trajectories
[00:49:19.460 --> 00:49:22.800]   which get stuck for a while before jumping off of a cliff.
[00:49:22.800 --> 00:49:27.080]   So it seems like they're probably doing something
[00:49:27.080 --> 00:49:29.040]   kind of similar to what's happening in this problem,
[00:49:29.040 --> 00:49:33.040]   where there's a solution that is close to the,
[00:49:33.040 --> 00:49:36.220]   that is, that looks kind of like this one,
[00:49:36.220 --> 00:49:37.900]   that has this kind of Pringle shape,
[00:49:37.900 --> 00:49:40.620]   where in one direction, the values, the loss goes up,
[00:49:40.620 --> 00:49:42.840]   and in one direction, the value goes down.
[00:49:42.840 --> 00:49:45.300]   If you'll give me just one second,
[00:49:45.300 --> 00:49:48.400]   I'm gonna try and get rid of this
[00:49:48.400 --> 00:49:51.120]   little pop-up thing down here.
[00:49:51.120 --> 00:49:52.120]   Sorry about that.
[00:49:52.960 --> 00:49:55.200]   (whistles)
[00:49:55.200 --> 00:50:03.220]   All right, well, that did not fix it,
[00:50:03.220 --> 00:50:04.760]   so I'm just gonna keep going.
[00:50:04.760 --> 00:50:09.560]   Okay, so that's another little phenomenon
[00:50:09.560 --> 00:50:11.520]   that we can see in this simple problem.
[00:50:11.520 --> 00:50:16.640]   So due to the simplicity of this loss,
[00:50:16.640 --> 00:50:19.880]   we can actually also calculate all the possible solutions.
[00:50:19.880 --> 00:50:21.520]   So at all of the solutions,
[00:50:21.520 --> 00:50:24.280]   it will be the case that the gradient is zero, right?
[00:50:24.280 --> 00:50:26.320]   So we want the gradient to go down over time
[00:50:26.320 --> 00:50:30.300]   as we're optimizing, and that when the gradient is zero,
[00:50:30.300 --> 00:50:32.160]   that could be because we're at a solution.
[00:50:32.160 --> 00:50:35.320]   At least all solutions will have gradient zero.
[00:50:35.320 --> 00:50:38.080]   So let's check what are some values
[00:50:38.080 --> 00:50:39.780]   that'll give us gradient zero.
[00:50:39.780 --> 00:50:44.780]   Well, we'll apply it to the value of the gradient,
[00:50:44.780 --> 00:50:47.000]   which we can calculate by hand.
[00:50:47.000 --> 00:50:48.320]   It's relatively straightforward.
[00:50:48.320 --> 00:50:49.840]   I've skipped the math here
[00:50:49.840 --> 00:50:52.800]   just to keep things going forward.
[00:50:52.800 --> 00:50:55.000]   And unsurprisingly, it's gonna happen
[00:50:55.000 --> 00:50:57.160]   when theta one and theta two
[00:50:57.160 --> 00:50:59.680]   are multiplied together equal one.
[00:50:59.680 --> 00:51:01.800]   So that means that if we multiply the input
[00:51:01.800 --> 00:51:04.140]   by those values, we'll get one.
[00:51:04.140 --> 00:51:08.920]   We'll get the same value out as in,
[00:51:08.920 --> 00:51:11.000]   and so those are our solutions.
[00:51:11.000 --> 00:51:12.920]   And we can actually calculate those,
[00:51:12.920 --> 00:51:17.480]   and the code there at the bottom now shows
[00:51:17.480 --> 00:51:22.480]   the, how you would calculate the positions
[00:51:22.480 --> 00:51:26.520]   of all of those, of all of those solutions,
[00:51:26.520 --> 00:51:31.160]   x and one over x for positive values of x
[00:51:31.160 --> 00:51:33.160]   and for negative values of x.
[00:51:33.160 --> 00:51:39.920]   And now we can actually visualize that solution set.
[00:51:39.920 --> 00:51:42.760]   So we can see that what's happening
[00:51:42.760 --> 00:51:46.520]   is that gradient set is taking us towards that solution
[00:51:46.520 --> 00:51:49.640]   and not towards that, the value in the center
[00:51:49.640 --> 00:51:54.640]   where both of the parameters are zero.
[00:51:54.640 --> 00:51:57.320]   So we can see that they're landing at different points
[00:51:57.320 --> 00:51:58.200]   on that solution set.
[00:51:58.200 --> 00:52:00.560]   And interestingly, they're landing on both the values
[00:52:00.560 --> 00:52:02.840]   that are positive and the values that are negative,
[00:52:02.840 --> 00:52:06.040]   depending on where they were initialized to.
[00:52:06.040 --> 00:52:08.120]   So the interesting thing about that solution set
[00:52:08.120 --> 00:52:10.960]   is basically one corresponds to flipping the value
[00:52:10.960 --> 00:52:13.480]   and then flipping it back to multiply by one.
[00:52:13.480 --> 00:52:15.400]   And one corresponds to just, you know,
[00:52:15.400 --> 00:52:17.960]   multiplying it by two positive numbers,
[00:52:17.960 --> 00:52:19.560]   not flipping the sign.
[00:52:19.560 --> 00:52:21.080]   And so what's interesting about that,
[00:52:21.080 --> 00:52:23.040]   there's a couple of interesting sort of,
[00:52:23.040 --> 00:52:25.920]   I don't know, takeaways I see.
[00:52:25.920 --> 00:52:28.680]   One is that the only one of those solutions
[00:52:28.680 --> 00:52:30.600]   is one that a human would have thought of.
[00:52:30.600 --> 00:52:33.160]   I wouldn't think, I've been given two numbers
[00:52:33.160 --> 00:52:35.120]   to multiply by to get one.
[00:52:35.120 --> 00:52:37.200]   I wouldn't think of using,
[00:52:37.200 --> 00:52:40.520]   I wouldn't think of using two negative numbers to do that.
[00:52:40.520 --> 00:52:42.120]   That wouldn't have immediately jumped out at me.
[00:52:42.120 --> 00:52:43.880]   But neural networks can find solutions
[00:52:43.880 --> 00:52:46.120]   that are very unintuitive to humans.
[00:52:46.120 --> 00:52:52.800]   And the other side of that is also that
[00:52:52.800 --> 00:52:57.440]   neural networks don't tend to find the simplest solution
[00:52:57.440 --> 00:53:00.040]   that maybe humans would immediately jump for,
[00:53:00.040 --> 00:53:02.640]   which is to make both of those values equal to one,
[00:53:02.640 --> 00:53:05.760]   to make, you know, theta one and theta two
[00:53:05.760 --> 00:53:06.600]   both equal to one.
[00:53:06.600 --> 00:53:08.760]   So you're just multiplying by one the entire time.
[00:53:08.760 --> 00:53:10.560]   That's not something,
[00:53:10.560 --> 00:53:13.200]   there's no obvious reason for neural networks to do things
[00:53:13.200 --> 00:53:14.520]   the simple way.
[00:53:14.520 --> 00:53:16.840]   And so if they can avoid doing things the simple way,
[00:53:16.840 --> 00:53:17.680]   they'll do it.
[00:53:17.680 --> 00:53:19.720]   So I think that's another nice lesson
[00:53:19.720 --> 00:53:23.400]   about the way neural networks and machine learning work
[00:53:23.400 --> 00:53:26.440]   that you can take from this simple example.
[00:53:26.440 --> 00:53:28.880]   The other kind of fun thing about this
[00:53:28.880 --> 00:53:31.640]   is if you look at those, they've got this nice shape.
[00:53:31.640 --> 00:53:34.200]   They've got this nice hyperboloid shape,
[00:53:34.200 --> 00:53:37.160]   those solution values in green in that chart.
[00:53:37.160 --> 00:53:41.880]   And that is, you know, it's not a coincidence.
[00:53:41.880 --> 00:53:43.640]   I can take any solution.
[00:53:43.640 --> 00:53:48.240]   I can get another solution by multiplying one weight
[00:53:48.240 --> 00:53:52.400]   by a value A and the other weight by a value one over A.
[00:53:52.400 --> 00:53:55.440]   So that allows me to move along that curve there
[00:53:55.440 --> 00:53:58.800]   or to switch sides if I use a negative sign.
[00:53:58.800 --> 00:54:01.680]   And so I can, there's a very clear algebraic
[00:54:01.680 --> 00:54:06.520]   and geometric structure to this set of solutions.
[00:54:06.520 --> 00:54:08.720]   And that's something that generalizes
[00:54:08.720 --> 00:54:10.680]   to more realistic settings as well.
[00:54:10.680 --> 00:54:12.560]   So for rectified linear networks,
[00:54:12.560 --> 00:54:13.840]   if you make them big enough,
[00:54:13.840 --> 00:54:16.000]   they actually have that same algebraic structure.
[00:54:16.000 --> 00:54:17.920]   It's called a lead group.
[00:54:17.920 --> 00:54:21.680]   And it's in fact a lead group based off of the linear group.
[00:54:21.680 --> 00:54:23.800]   And so, you know, so long as your network
[00:54:23.800 --> 00:54:27.520]   is really, really big and uses rectified linear units,
[00:54:27.520 --> 00:54:28.800]   then it will have something like that.
[00:54:28.800 --> 00:54:30.400]   You can do the same thing where you switch
[00:54:30.400 --> 00:54:34.160]   between the possible solutions in that fashion.
[00:54:34.160 --> 00:54:35.920]   The other thing is that those were two,
[00:54:35.920 --> 00:54:37.600]   essentially two separate basins.
[00:54:37.600 --> 00:54:40.560]   So what I mean by basin is if you look at those two sets
[00:54:40.560 --> 00:54:43.480]   of solutions, they're separated by a hump in the middle.
[00:54:43.480 --> 00:54:44.760]   The hump isn't that high,
[00:54:44.760 --> 00:54:47.160]   especially relative to the really high values of the loss,
[00:54:47.160 --> 00:54:48.760]   but there is this nice hump there
[00:54:48.760 --> 00:54:51.400]   that would be hard to get over with gradient descent.
[00:54:51.400 --> 00:54:56.440]   And so solutions in different basins can indeed
[00:54:56.440 --> 00:55:00.480]   have very different representations and sort of performance
[00:55:00.480 --> 00:55:03.040]   even when they're separated by only a small barrier.
[00:55:03.040 --> 00:55:04.600]   So this is something that people have found
[00:55:04.600 --> 00:55:07.120]   by looking very closely at ensembles
[00:55:07.120 --> 00:55:10.400]   or at least smartly ensemble neural networks.
[00:55:10.400 --> 00:55:13.760]   And then also I used a sort of high dimensional version
[00:55:13.760 --> 00:55:15.920]   of this same problem to improve techniques
[00:55:15.920 --> 00:55:18.480]   for finding zero gradient points of neural networks.
[00:55:18.480 --> 00:55:20.920]   So finding all the places where the gradient is zero,
[00:55:20.920 --> 00:55:23.840]   they're called critical points of the loss.
[00:55:23.840 --> 00:55:25.200]   And it's something that's really useful
[00:55:25.200 --> 00:55:27.000]   for understanding optimization.
[00:55:27.000 --> 00:55:29.800]   I used this multiplying by one example
[00:55:29.800 --> 00:55:32.760]   and it's higher dimensional analogs to understand
[00:55:32.760 --> 00:55:35.200]   the problem of finding critical points better
[00:55:35.200 --> 00:55:37.480]   and to develop better solutions.
[00:55:37.480 --> 00:55:41.160]   So in general, the takeaway here to remind
[00:55:41.160 --> 00:55:43.640]   is that you can use simple problems like this one
[00:55:43.640 --> 00:55:45.400]   to build your understanding
[00:55:45.400 --> 00:55:48.760]   of how a complicated thing works.
[00:55:48.760 --> 00:55:51.080]   Useful ideas and technologies like neural networks
[00:55:51.080 --> 00:55:53.360]   are usually compositions of simpler pieces
[00:55:53.360 --> 00:55:57.080]   or scaling up those simpler pieces in some way.
[00:55:57.080 --> 00:56:00.160]   And those simpler pieces tend to admit base cases
[00:56:00.160 --> 00:56:03.400]   that are particularly intuitive or easy to understand.
[00:56:04.400 --> 00:56:09.400]   And so the last, yeah, I guess the last thing is,
[00:56:09.400 --> 00:56:16.800]   of course you want to combine that
[00:56:16.800 --> 00:56:19.400]   with pathological counter examples.
[00:56:19.400 --> 00:56:22.120]   So, you've got an example like this
[00:56:22.120 --> 00:56:24.920]   where things are all looking nice, working nicely
[00:56:24.920 --> 00:56:27.040]   and things are very straightforward
[00:56:27.040 --> 00:56:29.440]   and easy to work with.
[00:56:29.440 --> 00:56:31.760]   Then there you've got your pathological counter examples
[00:56:31.760 --> 00:56:33.560]   like the memorization task.
[00:56:33.560 --> 00:56:35.480]   And then you've got this nice one-two punch
[00:56:35.480 --> 00:56:37.080]   for tackling difficult concepts.
[00:56:37.080 --> 00:56:40.200]   And this general approach, I think applies to things
[00:56:40.200 --> 00:56:43.120]   not just machine learning, but more broadly
[00:56:43.120 --> 00:56:45.720]   in both software engineering and in math.
[00:56:45.720 --> 00:56:50.280]   So I'll take a second to answer some of the questions
[00:56:50.280 --> 00:56:51.760]   that I saw in the chat there.
[00:56:51.760 --> 00:56:53.720]   So one question, what kinds of problems
[00:56:53.720 --> 00:56:58.240]   where JAX is better suited than other frameworks?
[00:56:58.240 --> 00:57:01.560]   So that question, that's an interesting one.
[00:57:01.560 --> 00:57:04.600]   I think right now, JAX is more suited
[00:57:04.600 --> 00:57:07.960]   for if you want to do fast math
[00:57:07.960 --> 00:57:10.960]   that isn't well handled by the other
[00:57:10.960 --> 00:57:14.000]   like neural network GPU acceleration frameworks.
[00:57:14.000 --> 00:57:17.480]   So it's, if you want to do things that it's generally
[00:57:17.480 --> 00:57:20.280]   it's gonna be something weird that other people
[00:57:20.280 --> 00:57:22.040]   in machine learning are not doing.
[00:57:22.040 --> 00:57:24.720]   So if you want the Jacobian of your neural network
[00:57:24.720 --> 00:57:27.720]   which is like the gradient, but for the outputs
[00:57:27.720 --> 00:57:30.400]   of the neural network, as opposed to the loss
[00:57:30.400 --> 00:57:32.320]   that's something that you can do really well.
[00:57:32.320 --> 00:57:36.280]   And with, it's well supported in JAX in a way
[00:57:36.280 --> 00:57:38.800]   that it for a long time was not supported
[00:57:38.800 --> 00:57:40.120]   in other frameworks.
[00:57:40.120 --> 00:57:43.720]   It's sort of an inheritor to AutoGrad in a lot of ways
[00:57:43.720 --> 00:57:45.200]   if you're familiar with AutoGrad
[00:57:45.200 --> 00:57:47.640]   which was the automatic differentiation software
[00:57:47.640 --> 00:57:50.760]   unaccelerated automatic differentiation software
[00:57:50.760 --> 00:57:52.600]   for a while there.
[00:57:52.600 --> 00:57:57.480]   So I would say what's most useful about JAX actually
[00:57:57.480 --> 00:58:00.480]   is that it's very principled and very thoughtful
[00:58:00.480 --> 00:58:02.960]   on a computational level.
[00:58:02.960 --> 00:58:05.120]   It's all about composition.
[00:58:05.120 --> 00:58:09.560]   It uses a lot of ideas from, uses a lot of ideas
[00:58:09.560 --> 00:58:14.440]   from the theory of functional programming
[00:58:14.440 --> 00:58:17.160]   and from essentially, category theory
[00:58:17.160 --> 00:58:19.760]   and to these branches of abstract math.
[00:58:19.760 --> 00:58:24.200]   And while it might not necessarily help you directly
[00:58:24.200 --> 00:58:26.040]   being able to program things in JAX
[00:58:26.040 --> 00:58:28.080]   the kind of mental exercise you get from it
[00:58:28.080 --> 00:58:29.280]   is certainly helpful.
[00:58:29.280 --> 00:58:32.280]   I felt like I learned quite a bit just by changing away
[00:58:32.280 --> 00:58:34.720]   around the way that I was thinking about my
[00:58:34.720 --> 00:58:36.160]   the various functions and pieces
[00:58:36.160 --> 00:58:37.640]   of my neural network pipeline.
[00:58:37.640 --> 00:58:41.280]   Then the last question, could you train something
[00:58:41.280 --> 00:58:44.080]   like a language model that has a more dynamic structure
[00:58:44.080 --> 00:58:46.680]   that's not neatly divisible into layers?
[00:58:46.680 --> 00:58:48.040]   Absolutely, yeah.
[00:58:48.040 --> 00:58:52.360]   You might run it if you're sort of overriding a value
[00:58:52.360 --> 00:58:54.560]   over and over again, you might find that you need to write
[00:58:54.560 --> 00:58:57.400]   it slightly differently in JAX than you would write it
[00:58:57.400 --> 00:58:59.920]   in a different programming paradigm,
[00:58:59.920 --> 00:59:04.920]   in a different, more, you know, more Pythonic paradigm
[00:59:04.920 --> 00:59:09.480]   like PyTorch, but you can certainly do it
[00:59:09.480 --> 00:59:11.400]   and you'd be able to make things, I think,
[00:59:11.400 --> 00:59:13.240]   very fast, pretty easily.
[00:59:13.240 --> 00:59:17.320]   All right, so those are all the questions that I see
[00:59:17.320 --> 00:59:19.720]   and it looks like it's about six o'clock.
[00:59:19.720 --> 00:59:24.720]   So I will leave you all here and I will catch you all again,
[00:59:24.720 --> 00:59:30.160]   I guess, let me throw this, yeah.
[00:59:30.160 --> 00:59:32.680]   I will catch you all again in two weeks
[00:59:32.680 --> 00:59:35.200]   for the next Weights & Biases Salon,
[00:59:35.200 --> 00:59:38.320]   where we'll have on some folks to talk
[00:59:38.320 --> 00:59:41.840]   about some public benchmarks in AI safety
[00:59:41.840 --> 00:59:46.840]   and in AI for social good in demystifying forms
[00:59:49.400 --> 00:59:51.560]   for political advertisements.
[00:59:51.560 --> 00:59:54.760]   So I'm looking forward to that salon, I hope you are too.

