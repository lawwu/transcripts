
[00:00:00.000 --> 00:00:06.800]   Let's get started. Hey everybody. Welcome to another paper reading group at Wits and Biases.
[00:00:06.800 --> 00:00:11.600]   And today we're going to be looking at the end-to-end object detection with transformers.
[00:00:11.600 --> 00:00:18.000]   So this is a shift from what we've been doing at the paper reading group so far. We've been looking
[00:00:18.000 --> 00:00:23.360]   at mostly common architectures like efficient net v2. We've looked at transformer architectures
[00:00:23.360 --> 00:00:30.880]   like vision transformer, kite paper, and mostly we've looked at classification kind of architectures
[00:00:30.880 --> 00:00:38.240]   and classification related papers. But I thought computer vision is a big field and object
[00:00:38.240 --> 00:00:45.360]   detection is one part of those, is a major part of that computer vision field. So I thought maybe
[00:00:45.360 --> 00:00:49.680]   this could be a good shift and that we should also start looking at object detection papers
[00:00:49.680 --> 00:00:54.240]   given that we've already looked at a lot of transformer papers. So it only made sense to
[00:00:54.240 --> 00:01:00.160]   see where else transformers have been applied. And one place where they've been applied very
[00:01:00.160 --> 00:01:05.280]   efficiently has been in object detection as well apart from language and apart from just computer
[00:01:05.280 --> 00:01:13.680]   classification type of problems. So if you could maybe just double checking that everybody can see
[00:01:13.680 --> 00:01:21.680]   my screen and if you could please just post in the chat that you can see 1db.me/detr. So if I go to
[00:01:21.680 --> 00:01:30.960]   my browser, can you see my browser as well please? So if I go 1db.me, so this is the same as every
[00:01:30.960 --> 00:01:36.400]   paper reading group but for those that are new, for those that have it's the first time in this
[00:01:36.400 --> 00:01:41.280]   paper reading group, if you go to that link 1db, I've just posted, I'll just post that in the chat
[00:01:41.280 --> 00:01:49.120]   as well. So 1db.me/detr. So if you go to that link, it will take you to this paper reading group,
[00:01:49.120 --> 00:01:55.040]   detr, and this is where you'll end up. So just go in there, write a comment. So as we're going
[00:01:55.040 --> 00:02:01.360]   through the paper today, what we're going to do is we're going to, because we also share,
[00:02:01.360 --> 00:02:05.680]   we're also live on YouTube and it's difficult for me to look at the Zoom chat, it just makes
[00:02:05.680 --> 00:02:10.800]   things a bit easier if we post all our comments over here and then I can address those questions
[00:02:10.800 --> 00:02:16.160]   and I can keep coming back to them as we go. Something I do want to point out is that,
[00:02:16.160 --> 00:02:23.200]   so this, the paper that we're looking at is again object detection. So that's the paper,
[00:02:23.200 --> 00:02:27.520]   End-to-End Object Detection with Transformers. And one good thing about this paper is that they
[00:02:27.520 --> 00:02:33.760]   have open source code. So if we go to that URL, which is this repository here, you'll see that
[00:02:33.760 --> 00:02:39.840]   Facebook Research has open sourced the DETR code, Detection Transformer, and this is not a new
[00:02:39.840 --> 00:02:42.960]   transformer. This is not something that's just come out recently. Like we've been discussing
[00:02:42.960 --> 00:02:48.160]   very cutting edge papers recently that have come out within the past two and a half months or
[00:02:48.160 --> 00:02:53.680]   when we were looking at the MLP Mixer paper. But this is more of a paper that's now been stable
[00:02:53.680 --> 00:03:00.640]   and has been there for a while, for I guess a year and a half. And one thing I do want to highlight
[00:03:00.640 --> 00:03:06.880]   is if you're interested in, sorry, one second. One thing I do want to highlight is this annotated
[00:03:06.880 --> 00:03:15.200]   DETR. Is this annotated DETR. Can you, sorry, one second. I'm just able to see this background,
[00:03:15.200 --> 00:03:19.840]   the PowerPoint in the background. Okay. That's gone now. That's better. So if you go to that
[00:03:19.840 --> 00:03:24.400]   blog post link, I will share this in the chat or it would have been shared with you guys as part
[00:03:24.400 --> 00:03:29.200]   of the reminder anyway. But I've just put that in the chat. So anything that we're going to learn
[00:03:29.200 --> 00:03:33.120]   today, if you want to see how that's been implemented in that particular repository
[00:03:33.920 --> 00:03:40.480]   from code, then this is called the annotated DETR. And the simple idea is we go paragraph by paragraph
[00:03:40.480 --> 00:03:46.160]   in the paper, and then we look at the code that's corresponding to that particular paragraph.
[00:03:46.160 --> 00:03:51.200]   So if I go, for example, let's see, which one should I look at? Let's see if I go to the
[00:03:51.200 --> 00:03:59.280]   transformer. And you will see how this is mostly stuff from the DETR transformer. So you'll see
[00:03:59.280 --> 00:04:04.560]   how this is the figure that's related to that transformer. And underneath this, this code and
[00:04:04.560 --> 00:04:09.200]   how the transformer has been implemented. So anything that's been, for example, the transformer
[00:04:09.200 --> 00:04:14.640]   decoder, anything that's been directly copied from the paper is in italics in this paper,
[00:04:14.640 --> 00:04:18.800]   in this blog post, and then you'll see how the transformer decoder has been implemented in
[00:04:18.800 --> 00:04:24.560]   PyTorch. So if you want to have a look at the code, we won't be looking much into the code today.
[00:04:24.560 --> 00:04:29.680]   We'll mostly be spending time just reading the paper. But I just wanted to highlight this as a
[00:04:29.680 --> 00:04:35.520]   resource for anybody that's interested in looking at things and how they're implemented in PyTorch.
[00:04:35.520 --> 00:04:44.080]   So that's that. That's just the introduction about DETR. But let's see now, let's get started
[00:04:44.080 --> 00:04:50.480]   with the paper. Sorry, give me one second. All right. So we're going to look at this end-to-end
[00:04:50.480 --> 00:04:59.760]   detection paper today. So let me bring up my OneNote, my best friend, which is this software.
[00:04:59.760 --> 00:05:10.480]   So as you will see in this part of, in the end-to-end object detection with transformers,
[00:05:10.480 --> 00:05:16.160]   as you will see, I'm also, I haven't been doing object detection for many, many years,
[00:05:16.160 --> 00:05:20.880]   but I have been doing this for quite some time now, a few months. And one thing I realized is
[00:05:20.880 --> 00:05:26.880]   when I was catching up with the literature, I saw how object detection has also advanced
[00:05:26.880 --> 00:05:31.520]   quite a bit. So we started with one stage and two stage detectors in the past. So we used to have
[00:05:31.520 --> 00:05:36.320]   things like RCNN, or we used to have things like YOLO, which are still very relevant in today's
[00:05:36.320 --> 00:05:42.080]   world. So for general object detection, you'll see libraries like, and this is just me providing
[00:05:42.080 --> 00:05:48.480]   context. So you'll see libraries like Detectron2, which is again by Facebook Research, which is a
[00:05:48.480 --> 00:05:56.000]   very popular library that has pretty much a lot of the models that you'll see from a bit older,
[00:05:56.000 --> 00:06:00.480]   like RetinaNet or all those models. And then there's another one, which is MMDetection.
[00:06:00.480 --> 00:06:05.440]   So I think these are both really good repositories to get started with object detection,
[00:06:05.440 --> 00:06:12.720]   but with transformers, there's all of these separate repositories that are there in this
[00:06:12.720 --> 00:06:18.080]   day and age right now, but I think that might change in the future. But anyway, so then what's
[00:06:18.080 --> 00:06:24.000]   the difference in object detection with transformers and all of these past methods that I
[00:06:24.000 --> 00:06:30.240]   just showed you that are part of those repositories? One of the main methods is that first,
[00:06:31.040 --> 00:06:37.040]   this uses transformers. So that's the main one, I guess. In DETR, you're using transformers.
[00:06:37.040 --> 00:06:44.720]   But another one is that this paper looks at object detection as a direct set prediction problem. So
[00:06:44.720 --> 00:06:50.480]   what does that mean? So until now, I'll give you some context again. Until now, what used to happen
[00:06:50.480 --> 00:06:56.080]   is you used to have things like anchor boxes, or we used to have, object detection used to depend
[00:06:56.080 --> 00:07:03.440]   on some prior knowledge. So we used to have region proposal networks. So what would happen
[00:07:03.440 --> 00:07:10.080]   before this paper came around, typically, is that you have an image. Sorry, that's a bit too...
[00:07:10.080 --> 00:07:21.680]   That should be better. So you have an image, and let's say here's an object in the image,
[00:07:21.680 --> 00:07:26.480]   here's an object in the image. And what used to happen or what would happen before this is that
[00:07:26.480 --> 00:07:32.480]   you would pass this through a region proposal network. I'm just going to call it RPN. And what
[00:07:32.480 --> 00:07:37.840]   that RPN does is it finds regions of interest in this image. So that could be a region of interest,
[00:07:37.840 --> 00:07:42.240]   that could be a region of interest. And then based on that, you start doing classification. So you
[00:07:42.240 --> 00:07:47.680]   say, okay, this belongs to class one, and this belongs to class two. So then you're able to do
[00:07:47.680 --> 00:07:54.160]   object detection and classification. But you can see how these region proposal networks have been
[00:07:54.160 --> 00:08:00.800]   quite a bottleneck for quite some time, because mostly they're used as a black box. And also,
[00:08:00.800 --> 00:08:05.520]   you don't have an end-to-end pipeline. What I mean by that is you're dependent on something that's
[00:08:05.520 --> 00:08:10.240]   not part of the object detection system in itself. Like this is not something you can change,
[00:08:10.240 --> 00:08:14.800]   or this is not something you can train. And so you don't have a very end-to-end pipeline.
[00:08:15.360 --> 00:08:22.240]   And what that does is it makes things hard to customize. So then this paper, what they say is
[00:08:22.240 --> 00:08:27.920]   we look at it as a direct set prediction problem. What does that mean? It means that we get rid of
[00:08:27.920 --> 00:08:36.960]   any extra stuff that's any extra or third party or anything that's not part of this paper is not
[00:08:36.960 --> 00:08:43.760]   used in the DETR architecture. So they pretty much say that DETR architecture is like this standalone
[00:08:43.760 --> 00:08:48.720]   architecture that can be used to object that can be used to do object detection in an end-to-end
[00:08:48.720 --> 00:08:53.360]   manner. So from having, you just have to provide the input images and the bounding boxes. And then
[00:08:53.360 --> 00:08:59.120]   based on that, this network on its own is able to do object detection, is able to learn everything
[00:08:59.120 --> 00:09:04.160]   that you want to do. And in fact, when you'll go through the repository, you'll see not only is it
[00:09:04.160 --> 00:09:10.080]   able to do object detection, it's also able to do segmentation. But for this part of the paper
[00:09:10.080 --> 00:09:16.000]   reading group, I'm mostly going to focus on the object detection part. The segmentation is only a
[00:09:16.000 --> 00:09:22.800]   small change in just the network head. But having said that, let's just have a look at the abstract.
[00:09:22.800 --> 00:09:28.480]   So I think one of the main parts of the paper is just the abstract itself. So I actually spend a
[00:09:28.480 --> 00:09:35.040]   lot of time reading the abstract very carefully. So as I said, our approach streamlines the
[00:09:35.040 --> 00:09:40.000]   detection pipeline and it takes away the need for many hand-designed components. So you used to have
[00:09:40.000 --> 00:09:46.640]   things like non-maximum suppression, which is NMS, or anchor generation. So what NMS used to do is
[00:09:46.640 --> 00:09:51.360]   just like a post-processing, as I understand, is that when you have duplicate bounding boxes,
[00:09:51.360 --> 00:09:56.240]   it would just take away the duplicates. So you used to have anchors. So what happens in anchors
[00:09:56.240 --> 00:10:02.240]   is if you have an image, you pretty much divide it into a grid. And then these become your anchor
[00:10:02.240 --> 00:10:08.960]   boxes and you see, OK, is there an object inside? That just becomes as a reference. So what this
[00:10:08.960 --> 00:10:13.360]   paper is saying is that we don't need anchor boxes. We don't need NMS.
[00:10:13.360 --> 00:10:21.760]   So we don't really need any prior knowledge about the task. That's the way how this detection
[00:10:21.760 --> 00:10:30.320]   transformer has been designed. And then it says, given a fixed small set of learned object queries,
[00:10:30.320 --> 00:10:36.160]   DETR, reasons about the relations of the objects and the global image context, to directly output
[00:10:36.160 --> 00:10:41.200]   the final set of predictions in parallel. So the word global, I think, is important because when
[00:10:41.200 --> 00:10:50.080]   you were using region proposal networks, then as you can see, you only have a look at the regions
[00:10:50.080 --> 00:10:54.560]   that have been highlighted or that have been sort of flagged by the region proposal network.
[00:10:54.560 --> 00:11:00.880]   But in this case, you have a look at the whole image when doing object detection in the DETR
[00:11:00.880 --> 00:11:06.960]   architecture. And the main thing is DETR demonstrates accuracy and runtime performance
[00:11:06.960 --> 00:11:13.680]   on par with well established and highly optimized fast R-CNN baseline on the Cocoa object detection
[00:11:13.680 --> 00:11:22.480]   dataset. So not only is this in a way a simpler end to end way of doing things, but it is also
[00:11:22.480 --> 00:11:28.080]   able to get the same accuracy and it is also able to get the same runtime performance as the current
[00:11:28.080 --> 00:11:34.960]   or as the state of art networks or as a very competitive baseline, which has been faster
[00:11:34.960 --> 00:11:42.240]   R-CNN. And then as I've already mentioned, the code is present here and then it's significantly
[00:11:42.240 --> 00:11:50.880]   outperforms competitive baseline. So that's the overall introduction about DETR. So then there's
[00:11:50.880 --> 00:11:56.080]   two or three main things about this architecture, but let's just go through the introduction. So
[00:11:56.080 --> 00:12:00.880]   this is the, I think I've done a shift from the past paper reading groups. And in this one is
[00:12:00.880 --> 00:12:05.680]   that we're actually going to go section by section, but we're going to go in a different,
[00:12:05.680 --> 00:12:10.160]   a slightly different order than we generally do. But I just want to point out this thing.
[00:12:10.160 --> 00:12:17.200]   So modern detectors previously until before DETR, you used to have things like you used to have a
[00:12:17.200 --> 00:12:21.040]   large set of proposals, which I've already mentioned is the region proposal network,
[00:12:21.040 --> 00:12:25.840]   or used to have anchors, or you used to have window centers. So that is how things used to be.
[00:12:25.840 --> 00:12:31.520]   And these were surrogate regression pretty much, because you used to have like a reference point.
[00:12:31.520 --> 00:12:35.680]   And then from that reference point, you would see, okay, this is my bounding box prediction.
[00:12:35.680 --> 00:12:42.640]   How good or bad is it? It's not like classification where you have some predictions from the model,
[00:12:42.640 --> 00:12:48.560]   and then you directly match it to the actual ground truth. So that's not how things used to be,
[00:12:48.560 --> 00:12:52.880]   but that's changing. Well, in 2021, that's changing quite rapidly.
[00:12:52.880 --> 00:13:01.120]   And a big part of that is thanks to the DETR paper. Okay. So then to simplify these pipelines,
[00:13:01.120 --> 00:13:06.240]   which is again, these surrogate regression pipelines, we assign a direct set prediction
[00:13:06.240 --> 00:13:11.840]   approach. And then this is an end to end philosophy. It has led to significant advance.
[00:13:11.840 --> 00:13:17.040]   We've seen like this sort of transformer, or this sort of end to end philosophy, which has led to
[00:13:17.040 --> 00:13:22.000]   advances in other fields. But as they mentioned in this paper, that at the time of writing,
[00:13:22.000 --> 00:13:27.040]   not so much in object detection. So previous attempts, they have these either surrogate
[00:13:27.040 --> 00:13:32.640]   tasks or there's like two stage. But this paper, what it does is that it aims to bridge this gap.
[00:13:32.640 --> 00:13:39.920]   So as you can see in this figure one, you will see that this is just the general overall,
[00:13:41.120 --> 00:13:47.760]   just the general overall idea of how the DETR architecture looks like. Sorry, one second.
[00:13:47.760 --> 00:13:56.160]   Okay. So as you can see, then in this, you will see just how the overall architecture looks like.
[00:13:56.160 --> 00:14:00.800]   So you have some input image, and this is how I mean that this is how it's different,
[00:14:00.800 --> 00:14:05.600]   is that your input image directly goes to a CNN. So you get some set of image features.
[00:14:05.600 --> 00:14:12.320]   What does that mean? You know, we can use CNN as backbones. So what does that mean is that
[00:14:12.320 --> 00:14:16.640]   you could have an efficient backbone, but instead of doing classification, instead of
[00:14:16.640 --> 00:14:21.440]   you could keep the whole architecture until the point you have the global average pooling.
[00:14:21.440 --> 00:14:26.640]   So this CNN would then output some features, right? So if your input is, let's say,
[00:14:26.640 --> 00:14:34.160]   if my batch size is one and number of channels is three, and then I have two to four by two to four,
[00:14:34.160 --> 00:14:39.680]   which is my height and width, two to four by two to four, and then I could pass this image to the
[00:14:39.680 --> 00:14:46.000]   CNN, and I could get some output of the image features, which could be, say, 256. So these,
[00:14:46.000 --> 00:14:51.520]   obviously, when you go through a CNN, your number of channels goes up, and your image dimension
[00:14:51.520 --> 00:14:55.840]   goes down. And I'm just making these numbers up, so don't. We'll look at the exact number of
[00:14:55.840 --> 00:15:01.360]   channels, and we look at the exact numbers just in a while, but I just want to give you a sense of,
[00:15:01.360 --> 00:15:06.560]   like, okay, how do things really work? So what happens is the number of channels will go up,
[00:15:06.560 --> 00:15:11.360]   and as we know, a CNN, it will just reduce the spatial dimension. So you go, like, something
[00:15:11.360 --> 00:15:16.720]   like 48 by 48. So instead of having two to four by two to four, you've reduced the height and width,
[00:15:16.720 --> 00:15:21.120]   but you've also increased the number of channels. So that's the idea. So once you have that,
[00:15:21.120 --> 00:15:28.000]   this now smaller matrix becomes a representation of the image, and then you pass that through a
[00:15:28.000 --> 00:15:34.000]   transformer encoder decoder architecture. We will look at exactly what that architecture is,
[00:15:34.000 --> 00:15:40.880]   but then this is how the DETR is very different from the previous papers that I read about object
[00:15:40.880 --> 00:15:47.040]   detection, is that you get a set of bounding box of pretty much predictions, and you see how these
[00:15:47.040 --> 00:15:51.360]   different colors, they're like different predictions that you get from the architecture,
[00:15:51.360 --> 00:15:58.320]   but you can match that using this idea of bipartite matching loss. So you use this
[00:15:58.320 --> 00:16:04.560]   bipartite matching loss, and you match the predictions with the ground truths directly.
[00:16:04.560 --> 00:16:08.240]   So at this point, you don't need any anchor boxes. You don't need anything,
[00:16:08.240 --> 00:16:14.480]   and this is how the DETR architecture, when we look at it, it looks very simple,
[00:16:14.480 --> 00:16:18.080]   and it is actually very simple, because if you, when you read through this paper, they'll say,
[00:16:18.080 --> 00:16:24.000]   "Oh, we've tried to make the training pipeline really easy," and the DETR architecture on its
[00:16:24.000 --> 00:16:31.280]   own doesn't need any special sort of, any special sort of modules. You just need pretty much,
[00:16:31.280 --> 00:16:36.720]   sorry, one second, any library that can do convolution, and you have the transformer
[00:16:36.720 --> 00:16:40.960]   architecture as part of the library, which is pretty much PyTorch, TensorFlow, Jaxx, or pretty
[00:16:40.960 --> 00:16:45.840]   much any library that you use for deep learning. They have the convolution and transformer
[00:16:45.840 --> 00:16:51.680]   architecture as part of them. So they say, in this paper, there was some line mentioned
[00:16:51.680 --> 00:16:56.080]   somewhere that said, "Oh, because of this, because the architecture is so simple,
[00:16:56.080 --> 00:17:02.240]   that you can actually use any library that you like to implement the DETR architecture." It does
[00:17:02.240 --> 00:17:08.000]   not require any special libraries that have to implement these black box networks like RPN,
[00:17:08.000 --> 00:17:12.880]   or you don't need to worry about anchor generation or all that sort of stuff, which is quite clunky,
[00:17:12.880 --> 00:17:19.120]   and which has been a bottleneck. So that's the idea behind DETR. So as we see,
[00:17:19.120 --> 00:17:25.440]   there's good stuff written here. We adopt an encoder-decoder architecture. So what does this
[00:17:25.440 --> 00:17:33.040]   encoder-decoder architecture mean? Well, that's from attention is all you need transformer.
[00:17:33.040 --> 00:17:38.400]   So as has happened with many fields, like when the transformer architecture came about,
[00:17:38.400 --> 00:17:46.160]   in attention is all you need, not only was that a big, big success in natural language processing,
[00:17:46.160 --> 00:17:52.160]   and let me see where the architecture is. Encoder-decoder stacks. There it is. So that's
[00:17:52.160 --> 00:17:57.520]   the architecture, which is the attention is all you need paper, which introduced multi-headed
[00:17:57.520 --> 00:18:04.640]   tension. And we started, the first time this was applied was for machine translations,
[00:18:04.640 --> 00:18:11.920]   sorry, language translations from say English to German or that sort of objective. But then
[00:18:11.920 --> 00:18:16.160]   people realized that this architecture is actually very helpful for all of these other
[00:18:16.160 --> 00:18:22.400]   tasks as well. Or in computer vision, then the encoder was taken and the vision transformer was
[00:18:22.400 --> 00:18:28.480]   born. So this is me just again, giving context and we're not really looking at the DETR, but I'm just
[00:18:28.480 --> 00:18:33.600]   giving context on how things have evolved. So you saw that there's like this images worth 16 cross
[00:18:33.600 --> 00:18:38.080]   16 words, which is transformers, which pretty much applies this same architecture, but just,
[00:18:38.080 --> 00:18:46.160]   I believe the encoder for classification. So I can see where is the transformer. Do we have
[00:18:46.160 --> 00:18:52.240]   an image? There it is. So you see how it's just using the transformer encoder, but that's again
[00:18:52.240 --> 00:18:59.040]   for classification. Then for object detection, I'll go up. Then for object detection, you have
[00:18:59.040 --> 00:19:03.520]   end to end, which is the DETR architecture. So that's the idea. That's the encoder decoder
[00:19:03.520 --> 00:19:11.520]   architecture. Again, I'm just having a look if there's any important bits that I should cover
[00:19:11.520 --> 00:19:16.720]   here. So this is the main bit, like as you can see in figure one, which is the figure I just
[00:19:16.720 --> 00:19:22.640]   showed you, it's this figure at the top. You can see how you predict all objects at once and it's
[00:19:22.640 --> 00:19:27.120]   trained end to end with set loss function. So we will look at the loss function. We will look at
[00:19:27.120 --> 00:19:32.800]   what this bipartite matching means. And we will look at all of these in a lot more detail in just
[00:19:32.800 --> 00:19:39.600]   the coming few moments. But right now I just want to sort of introduce you to this idea of how DETR
[00:19:39.600 --> 00:19:46.000]   is different. So this is just me kind of providing you an overall sort of objective or that sort of
[00:19:46.000 --> 00:19:52.880]   overall idea of how DETR architecture looks like. So you can see, we don't need spatial anchors. We
[00:19:52.880 --> 00:19:58.720]   don't need NMS. That's just been a repeat. And that's it. This is the bit where I said DETR
[00:19:58.720 --> 00:20:04.640]   doesn't require any customized layers and thus can be reproduced easily in any framework that
[00:20:04.640 --> 00:20:09.680]   contains standard CNN and transformer classes, which means you can implement this in TensorFlow.
[00:20:09.680 --> 00:20:16.080]   You can implement this in PyTorch really, really easily. And that's why the DETR could be its own
[00:20:16.080 --> 00:20:20.960]   separate repo. And when we'll have a look at or when you have a look, go through the annotated
[00:20:20.960 --> 00:20:26.880]   DETR, you'll see that things are really simple and straightforward to implement this architecture.
[00:20:26.880 --> 00:20:34.160]   Okay. So that's the basics of it. This is where they say, you know how until now in the paper
[00:20:34.160 --> 00:20:39.280]   reading groups I've been mentioning ImageNet a lot. Similarly in object detection, you have this
[00:20:39.280 --> 00:20:45.280]   COCO dataset, which is kind of this baseline. And then that's just used for detection instead of
[00:20:46.720 --> 00:20:51.840]   classification. So I believe it just struck me that I haven't really said what object detection
[00:20:51.840 --> 00:20:56.240]   is. I just assume that everybody who's on this call knows what object detection is.
[00:20:56.240 --> 00:21:02.480]   So object detection just means if you have an object in classification, you say this is a horse,
[00:21:02.480 --> 00:21:08.320]   this is a cat, or whatever the object category is. In object detection, you not only classify,
[00:21:08.320 --> 00:21:14.560]   but you also provide the coordinates. So you say, okay, this goes from X1, Y1, or just pretty much
[00:21:14.560 --> 00:21:20.240]   you just provide the coordinates in the image. So that this problem is then object detection.
[00:21:20.240 --> 00:21:26.080]   Sorry, that's something I should have done right at the beginning. But anyway, let's keep going.
[00:21:26.080 --> 00:21:32.640]   If you're here as part of the DETR, I'm sure you'd know what object detection is. So then
[00:21:32.640 --> 00:21:39.920]   one thing I would like to point out here is that DETR demonstrates significantly better
[00:21:39.920 --> 00:21:44.480]   performance on large objects. But one thing that's been pointed out in this paper is that
[00:21:44.480 --> 00:21:51.760]   it however obtains lower performance on small objects. But since this paper was in 2020,
[00:21:51.760 --> 00:21:58.720]   so let's see, I believe it was around September 2020. What exactly? 28th May 2020. Sorry. So it
[00:21:58.720 --> 00:22:07.120]   was in May 2020. And since that time, a lot of new architectures have come up like the deformable
[00:22:07.120 --> 00:22:12.160]   DETR, up DETR, or like there's these other architectures that have come up based on the
[00:22:12.160 --> 00:22:18.400]   DETR architecture. And then they have tried to look at this downside of like the lower performance
[00:22:18.400 --> 00:22:25.760]   on small objects. So that's that. And that's pretty much it in terms of introduction to DETR.
[00:22:25.760 --> 00:22:32.640]   I won't look into the related work because we'll come back to that later. I think I won't look at
[00:22:32.640 --> 00:22:38.320]   the loss function right now as well. So I'm going to skip this part of section three of actually
[00:22:38.320 --> 00:22:42.560]   having a look at the loss function. But we're going to do things a bit differently. We're going
[00:22:42.560 --> 00:22:47.920]   to have a look at the architecture first. So we're going to have a look at how when we pass an image
[00:22:47.920 --> 00:22:53.760]   to this DETR architecture, how does it process that image and how does it really pass how does
[00:22:53.760 --> 00:22:59.520]   it really give out the classification outputs and the object detection outputs. And then we're going
[00:22:59.520 --> 00:23:03.920]   to see and then we're going to go back at the loss function. And then that's when we're going to see
[00:23:03.920 --> 00:23:14.080]   how those outputs can be used to calculate the loss. So let me put that in OneNode. One second.
[00:23:14.080 --> 00:23:18.240]   Oh, I am in OneNode. One second. Sorry.
[00:23:18.240 --> 00:23:28.880]   So if I go and I add a section here. So as I said, the way we're going to look at is the first thing
[00:23:28.880 --> 00:23:39.120]   was the intro. Then we're going to look at the overall architecture. So that's second.
[00:23:39.120 --> 00:23:44.720]   And then finally, we're going to look at the loss. So in this overall architecture, we're going to see
[00:23:44.720 --> 00:23:52.320]   how things look like. So if this is my architecture, let's say this cube is my DETR
[00:23:52.320 --> 00:23:57.360]   architecture. We're going to understand how things look like if I pass in an input image,
[00:23:57.360 --> 00:24:04.880]   then how does it really give me the bounding box and my class probabilities. I'm just going to call
[00:24:04.880 --> 00:24:09.760]   prob. So we're going to have a look at this part of this whole architecture. And then we're going
[00:24:09.760 --> 00:24:14.000]   to come back and we're going to have a look at the loss function. So you're going to see, okay,
[00:24:14.000 --> 00:24:21.680]   now we have our outputs. How do these outputs compare to my ground truth? And then we can see
[00:24:21.680 --> 00:24:28.160]   how to calculate the loss. That's how today is going to look like. But let's see if there's any
[00:24:28.160 --> 00:24:31.360]   questions on that report. So let me go back to that report.
[00:24:31.360 --> 00:24:40.160]   Yes, that is correct. Does surrogate pipeline mean using RPN for anchoring? Well, pretty much
[00:24:40.160 --> 00:24:46.400]   surrogate tasks means just you have a separate task. You don't actually just compare the output
[00:24:46.400 --> 00:24:53.280]   with the ground truth. You have some other third task that does it. In bipartite matching,
[00:24:53.280 --> 00:24:58.000]   how ground truth boxes can be compared while actually trying to identify the same object?
[00:24:58.000 --> 00:25:01.520]   Okay. These are questions we haven't really looked at any of this. So these are questions
[00:25:01.520 --> 00:25:07.280]   we'll come back to later. But let's first go and have a look at the overall architecture.
[00:25:07.280 --> 00:25:12.960]   So let's see that. So in figure two, this is where the overall architecture has been defined.
[00:25:12.960 --> 00:25:16.400]   And I'm going to look at this architecture a slightly bit differently. We're going to have
[00:25:16.400 --> 00:25:22.240]   a look at this figure two. And then I'm also going to show you how the transformer inside looks like.
[00:25:22.240 --> 00:25:29.280]   And then if there's interest, I can also show an overall idea of how these things can be
[00:25:29.280 --> 00:25:35.200]   implemented in code. But we'll get to that in a while. So let's see. Sorry, I've lost the
[00:25:35.200 --> 00:25:41.760]   architecture. So there we are. That's the architecture. So let me actually just copy
[00:25:41.760 --> 00:25:48.800]   paste that in a new part. So this is less clunky. So let's paste this here.
[00:25:48.800 --> 00:25:57.600]   And then, okay. So what happens is, this is what happens. I have my, let's say I have my
[00:25:57.600 --> 00:26:05.120]   input image. So I'm going to saying three channel, two to four. This is two to four,
[00:26:05.120 --> 00:26:08.800]   two to four. Two to four by two to four is just a number that I've come up with.
[00:26:08.800 --> 00:26:15.040]   It could be 448. Usually in object detection, you have higher size images, like 640 by 640. But I'm
[00:26:15.040 --> 00:26:20.880]   just, don't code, like don't take these numbers, take them with a grain of salt. And then these
[00:26:20.880 --> 00:26:25.520]   are just numbers I want to present just so everybody has a good understanding of what's going
[00:26:25.520 --> 00:26:30.400]   on in the overall architecture first. And then you have three channels. So the three channels are red,
[00:26:30.400 --> 00:26:37.680]   green, and blue. Okay. So this is my input, which is my input image. You pass that to the CNN. Now,
[00:26:37.680 --> 00:26:43.840]   this CNN could be a ResNet 50. It could be a ResNet 101. It could be any CNN, like an efficient
[00:26:43.840 --> 00:26:50.240]   net, or it could be any backbone that can extract some information from my input image. So any
[00:26:50.240 --> 00:26:55.120]   backbone that we've looked at in the past could be used to extract information or could be used
[00:26:55.120 --> 00:27:01.120]   to extract image features. So what this would do is that once I have my CNN, what could happen
[00:27:01.120 --> 00:27:06.720]   is that my outputs, again, what's going to happen is that instead of having three channels, I'm
[00:27:06.720 --> 00:27:12.160]   going to have, say, something like, I said 256 in the last time, but let's say I have 2048, or
[00:27:12.160 --> 00:27:18.640]   actually, let's just keep with 256. So let's say I now have 256 channels, and then my feature size
[00:27:18.640 --> 00:27:24.240]   gets reduced to 40 by 40. So that's the image features here, right? These image features are
[00:27:24.240 --> 00:27:33.840]   256 by 40 by 40. And then that's just the output. That's like a representation of my image.
[00:27:34.800 --> 00:27:41.840]   And then I have my positional encoding. So what happens is if you see my input, I'm just showing
[00:27:41.840 --> 00:27:47.600]   you, and I just want to sort of highlight an example, is like if this is 40 by 40, so I'm just
[00:27:47.600 --> 00:27:55.360]   making a grid of like 40 by 40, then each position, the transformer from the attention is
[00:27:55.360 --> 00:28:01.360]   all you need paper, you'd know that the transformer doesn't quite know the order, or like it doesn't
[00:28:01.360 --> 00:28:07.520]   really know what position things are in. So if we pass, the transformer expects the input to be
[00:28:07.520 --> 00:28:17.120]   in a sort of a sequence. So let's say when we were doing translation, if I said I am Aman,
[00:28:17.120 --> 00:28:22.080]   right, the transformer doesn't know that this is position three, or this is position one,
[00:28:22.080 --> 00:28:28.640]   this is position two. The transformer has no idea about the ordering of things. And that's why,
[00:28:28.640 --> 00:28:34.240]   again, this idea of positional encoding is coming from that attention is all you need paper.
[00:28:34.240 --> 00:28:40.080]   So what you do is you pass in positional encodings. I will explain exactly what the
[00:28:40.080 --> 00:28:44.320]   shape of these positional encodings look like. I will explain all of that stuff in detail in just
[00:28:44.320 --> 00:28:52.400]   a little while. But the main idea is then from these features, you need to know how the ordering
[00:28:52.400 --> 00:28:58.240]   of things look like. So you pass in these positional encodings over here, which would be the
[00:28:58.240 --> 00:29:04.000]   same length as my input. So right now, my input is in like a form of a tensor matrix. I could just
[00:29:04.000 --> 00:29:10.240]   pretty much flatten it out. So my input becomes like a flattened image. So this is what we do.
[00:29:10.240 --> 00:29:17.520]   We've had a look at this briefly in the vision transformer, but just from the idea. Now my input
[00:29:17.520 --> 00:29:25.200]   is, again, say, a sequence. And then what you do is that to that input image or that input feature,
[00:29:25.200 --> 00:29:32.000]   which is this flattened image, what you do is you add these positional encodings. So now when we add
[00:29:32.000 --> 00:29:37.760]   positional encodings, we have to basically inject some positional information to the transformer.
[00:29:37.760 --> 00:29:42.640]   So the transformer knows where features are. So the transformer knows, OK, this is the top left,
[00:29:42.640 --> 00:29:46.640]   and this is top right, or this is-- sorry, bottom right. Or this is top right, and this is bottom
[00:29:46.640 --> 00:29:51.040]   left. The transformer needs to have some information about the positions. And these
[00:29:51.040 --> 00:29:56.720]   positional encodings are responsible for doing that. So that's just the basic idea of why we
[00:29:56.720 --> 00:30:01.680]   add positional encodings. And then one thing you'll see, when we pass things through the
[00:30:01.680 --> 00:30:10.000]   transformer encoder, it kind of looks at the image. It looks at these positional encodings,
[00:30:10.000 --> 00:30:15.680]   and then it gives some output. So without looking at what goes on in this transformer architecture,
[00:30:15.680 --> 00:30:19.680]   there's another beautiful, beautiful image that will show you what exactly goes on.
[00:30:20.480 --> 00:30:28.240]   Let's just take this as a black box. Let's say, OK, all of this is a black box. Then I can get my
[00:30:28.240 --> 00:30:35.200]   prediction outputs, which is pretty much-- I get an output vector. That output vector can go into
[00:30:35.200 --> 00:30:39.120]   these feedforward networks, which can say, OK, which can perform the classification, which can
[00:30:39.120 --> 00:30:46.960]   perform the bounding box. So let me show you the transformer, what exactly goes on in the
[00:30:46.960 --> 00:31:03.840]   transformer. So this is the image. OK, thanks for doing that to me. So then that's that. OK.
[00:31:03.840 --> 00:31:12.960]   So what happens is, as I've already said, once you have your input image, that input image gets
[00:31:12.960 --> 00:31:19.520]   reduced to a smaller feature map. So this input image will get reduced to a smaller feature map,
[00:31:19.520 --> 00:31:23.360]   but it will have a lot more channels. So it will look something like that.
[00:31:23.360 --> 00:31:32.320]   So let's say, as I've already posted in this blog post, let's say that looks like 256 by 24 by 29.
[00:31:32.320 --> 00:31:38.880]   So let's say my number of channels are 256, and then my height and width become 24 by 29.
[00:31:39.440 --> 00:31:44.480]   Let's say that's what happens. And then what I could do is I could flatten it out. Like,
[00:31:44.480 --> 00:31:52.480]   this is my 2D grid. This could be one vector, right? Because this is just one 2D matrix. I
[00:31:52.480 --> 00:31:58.640]   could flatten it out, right? So then I could get my outputs in this shape. So OK, all the shapes
[00:31:58.640 --> 00:32:05.840]   are now here. So I could get my outputs. Please feel free to interrupt or post a question. And
[00:32:05.840 --> 00:32:12.320]   this is the most important part of understanding the DETR architecture, and possibly also the most
[00:32:12.320 --> 00:32:18.160]   complicated. But bear with me, stay with me, and let's just spend extra 10, 15 minutes just looking
[00:32:18.160 --> 00:32:23.440]   at what exactly goes on in the transformer and how the DETR architecture is then able to make
[00:32:23.440 --> 00:32:29.840]   those predictions, right? So I'm just trying to marry this bigger image at the top. Without
[00:32:29.840 --> 00:32:34.480]   looking at this, this is all the transformer. So I'm not going to look at the transformer from here.
[00:32:34.480 --> 00:32:40.480]   This transformer is just this image at the bottom, OK? So right now, all I have is
[00:32:40.480 --> 00:32:48.640]   I had my input image here. I passed that through a backbone, which then gave me
[00:32:48.640 --> 00:32:57.280]   696 by 1 by 256, OK? Let's say those are my image features. Where does that 696 by 1 by 256 come
[00:32:57.280 --> 00:33:07.760]   from? When I passed that through the backbone, remember I said we get 256 by 29 by 24, right?
[00:33:07.760 --> 00:33:12.560]   Let's say this is what the backbone is going to do. What I could do is I could flatten this out.
[00:33:12.560 --> 00:33:22.960]   So this becomes 256 by 696, and I could reshape. See, that becomes my input to the transformer.
[00:33:22.960 --> 00:33:34.800]   Is there any question about-- I'll just actually have a look here. In 256 by-- can we say 40
[00:33:34.800 --> 00:33:41.360]   cross 40 is the size of the image reduced from 224 by 224? And what is the intuition for 256 channels?
[00:33:41.360 --> 00:33:48.160]   Well, 256-- OK, so in a backbone, or pretty much in convolutional neural networks, what happens is
[00:33:48.160 --> 00:33:54.240]   you have a bigger image, like you have a massive image as input. And as this image goes through
[00:33:54.240 --> 00:33:59.600]   each of these stages of the convolutional neural network, what happens is the number of channels
[00:33:59.600 --> 00:34:06.640]   keeps on going up, and the image size keeps on going down. So in a way, you don't lose any
[00:34:06.640 --> 00:34:13.360]   computation. You don't lose any information, OK? So then this 40 cross 40 is this smaller-- 40
[00:34:13.360 --> 00:34:18.880]   cross 40 is the smaller representation of the bigger image, which is correct. And then the
[00:34:18.880 --> 00:34:25.600]   number of 256 channels-- in actuality, it is 2048, but then that gets reduced to 256. But let's just
[00:34:25.600 --> 00:34:35.200]   say right now, 256 by 40 by 40, or 256 by 29 by 24. I'm just using 256 by 29 by 24, because that's
[00:34:35.200 --> 00:34:42.160]   what the shape is assumed in this image. So I'm just going to assume that when I have some input,
[00:34:42.160 --> 00:34:49.280]   which is, say, three channel, 2 to 4 by 2 to 4, when I pass that through the backbone,
[00:34:49.280 --> 00:34:59.200]   the output becomes 256 by 29 by 24. So this is a lower resolution feature map, or a smaller
[00:34:59.200 --> 00:35:06.800]   feature map. So instead of my image being massive, it is now smaller, but it has a lot more channels,
[00:35:06.800 --> 00:35:11.520]   OK? That's just passing through a backbone. That shouldn't be-- that's not something
[00:35:12.480 --> 00:35:18.400]   that should be confusing at all, because that's just part of any standard classification.
[00:35:18.400 --> 00:35:26.000]   So that's the idea. So that becomes my input to the transformer, OK? See here, let's go at the
[00:35:26.000 --> 00:35:31.360]   bigger image. So you can see how things get passed to the CNN. And before things are passed to
[00:35:31.360 --> 00:35:40.800]   the transformer from there, I flatten it out. So then my input to the transformer is this. So it
[00:35:40.800 --> 00:35:49.600]   can be thought as I now have a sequence of length 696, where each token is 256 vector long, OK?
[00:35:49.600 --> 00:35:57.440]   That's the idea. Now, after this point, it's just-- after this point, it's just-- it's like
[00:35:57.440 --> 00:36:01.360]   machine translation, or it's like language translation, because I have my input image
[00:36:01.360 --> 00:36:06.720]   converted into a sequence, OK? That's just how we can look at these things. So then what happens is
[00:36:06.720 --> 00:36:11.280]   this is my transformer encoder. This is just-- again, I'm not going to go into the detail of
[00:36:11.280 --> 00:36:15.440]   the transformer encoder. It's just multi-head attention followed by a feedforward neural
[00:36:15.440 --> 00:36:20.560]   network. Again, this is exactly the same as the attention is all you need paper. So where is the
[00:36:20.560 --> 00:36:24.800]   attention is all you need paper? So you can see how that is exactly the same as the attention is
[00:36:24.800 --> 00:36:30.640]   all you need paper. The one thing we do is we're adding these positional encodings. So as I already
[00:36:30.640 --> 00:36:36.400]   said, positional encodings are then-- the transformer needs to know where the relative
[00:36:36.400 --> 00:36:41.840]   position or the position of these objects or these different classes or these different labels
[00:36:41.840 --> 00:36:47.200]   are in the image. So you have these positional encodings, and they're going to be the same size
[00:36:47.200 --> 00:36:56.080]   as my input, right? Why? If this is my input, then I need to have a 40 by 40 grid, or pretty much I
[00:36:56.080 --> 00:37:01.040]   need to have-- that's exactly how many points or that's exactly how many positions I need to cover
[00:37:01.040 --> 00:37:10.320]   for. So I can have my input as 696, and then each position encoding could be-- again, if this is
[00:37:10.320 --> 00:37:14.960]   confusing about position encoding, again, this idea is straight from the attention is all you
[00:37:14.960 --> 00:37:21.680]   need paper. So the position encoding is, instead of it being one number, it's a vector, right?
[00:37:21.680 --> 00:37:28.960]   And because right now we have converted our input image into a sequence of length 696,
[00:37:28.960 --> 00:37:35.440]   then this is how it looks like. So let's say this is 1, 2, 3, so on 696. I should make this a bit
[00:37:35.440 --> 00:37:46.320]   smaller. So my input is now 696. Then for each of these positions, right, 1, 2, 3, so on until 696,
[00:37:46.320 --> 00:37:52.800]   I need to have a position encoding. So that is why my position encoding is, say, 696 by 1 by 256,
[00:37:52.800 --> 00:38:00.080]   because each position is then represented by a 256 length long vector, okay? So that's the idea.
[00:38:00.080 --> 00:38:06.240]   So you input both of them. You input that to my transformer. You get some output from the
[00:38:06.240 --> 00:38:10.480]   transformer encoder, which is going to be the same shape. Again, this is just using the transformer
[00:38:10.480 --> 00:38:17.040]   architecture as is. You get some output from the transformer, which we call it memory, okay? Then
[00:38:17.040 --> 00:38:24.720]   on the decoder side of things, what happens is, now, we haven't really done any object detection
[00:38:24.720 --> 00:38:30.960]   so far, right? We just passed in my input image as a feature. We passed in my spatial position
[00:38:30.960 --> 00:38:36.960]   encodings, which is just useful for the transformer to know the relative position or the position of
[00:38:36.960 --> 00:38:44.080]   all of these 696 sequences. But we haven't really provided any object queries. So what the authors
[00:38:44.080 --> 00:38:51.280]   really did, which is really, really smart of them to do, is what you can do -- oh, sorry, one second.
[00:38:51.280 --> 00:39:04.480]   What you can do is I can have -- in PyTorch, you can have something as nn.embedding. Well, that's
[00:39:04.480 --> 00:39:16.160]   just an embedding. So what that means is I can start with random 100 positions. So the authors
[00:39:16.160 --> 00:39:24.400]   of DETR, they said, okay, I'm going to assume that there's at max 100 objects in my input image,
[00:39:24.400 --> 00:39:32.240]   okay? So my input image right now is this 696-long sequence where each
[00:39:33.360 --> 00:39:40.960]   part is like a 256-long vector, okay? So what they said is I'm going to assume that in my input image,
[00:39:40.960 --> 00:39:50.320]   there's going to be at max 100 objects, right? So what I can do is I can input 100 cross 1 by
[00:39:50.320 --> 00:39:58.880]   256. So I can input that as my position embedding. So let's say this is my object
[00:40:00.240 --> 00:40:10.240]   positional embedding, okay? So the idea is then I'm going to input these 100 random positions
[00:40:10.240 --> 00:40:18.160]   in my -- basically in my transformer decoder. So this is what happens when I pass in my object
[00:40:18.160 --> 00:40:24.160]   queries over here. These are just -- right now, the network hasn't learned anything. These are just
[00:40:24.160 --> 00:40:34.400]   random 100 positions in my input image. And then what I say is, okay, in my input, when I answer
[00:40:34.400 --> 00:40:40.960]   these random -- when I basically input these random object queries, firstly, these object queries will
[00:40:40.960 --> 00:40:46.560]   start to perform self-attention within themselves. So this is what's happening. This is why this is
[00:40:46.560 --> 00:40:52.480]   called self-attention. Because now the objects are like these 100 -- as the network will learn more
[00:40:52.480 --> 00:40:57.600]   and more, it needs to know, like, okay, where the objects are or what the relation is between them,
[00:40:57.600 --> 00:41:01.840]   right? It needs to know, okay, the cat is there, the fence is there, or like in an image, it needs
[00:41:01.840 --> 00:41:07.280]   to know all of these different things. But there's mostly a relation between the objects themselves
[00:41:07.280 --> 00:41:12.880]   as well, right? So what it's going to do is right now, because we have these 100 -- we just provided
[00:41:12.880 --> 00:41:19.120]   the model with random 100 starting points. What the model is going to do is as it's going to learn,
[00:41:19.120 --> 00:41:25.040]   it's going to learn this relationship between these random 100 -- again, when I say 100,
[00:41:25.040 --> 00:41:30.480]   that's what the number the DETR paper authors have chosen. It could be a different number as well.
[00:41:30.480 --> 00:41:35.040]   But this is what they thought. They said, okay, at max in Cocoa, there's going to be 100 objects in
[00:41:35.040 --> 00:41:41.520]   one image. So then you have this 100 objects sort of interacting between themselves. But we haven't
[00:41:41.520 --> 00:41:46.800]   really looked at the image so far, right? That happens here. So once you have this self-attention,
[00:41:46.800 --> 00:41:52.080]   which means these 100 objects, these random 100 starting points interacting between themselves,
[00:41:52.080 --> 00:41:58.000]   you get some output, which is this output over here. You mix that output with the output from
[00:41:58.000 --> 00:42:04.640]   the encoder. So remember, now I have my interactions between the objects themselves.
[00:42:04.640 --> 00:42:10.960]   I mix that with the output from the encoder. What the encoder has done, it already took my
[00:42:10.960 --> 00:42:15.200]   input image, it already took the positional encodings for the input image, and it has been
[00:42:15.200 --> 00:42:21.360]   able to basically look at that input image and has the information about the image. So then we can
[00:42:21.360 --> 00:42:28.640]   mix the information about these 100 random objects with the information of the image. So now the
[00:42:28.640 --> 00:42:36.320]   transformer decoder actually learns to marry the various object positions within the image itself.
[00:42:36.320 --> 00:42:41.920]   I hope that makes sense, because this is exactly what's going on in the DETR architecture. So you
[00:42:41.920 --> 00:42:48.080]   do that over and over and over and over again. You start doing this over and over again few too
[00:42:48.080 --> 00:42:54.880]   many times for, say, a 300 or 350 epochs. By the time, by the end, what you have is you have this
[00:42:54.880 --> 00:43:01.200]   DETR architecture that's then able to learn the position of-- for each image, it's able to predict
[00:43:01.200 --> 00:43:07.840]   these 100 object queries. It started with the random positions in the image itself, but by the
[00:43:07.840 --> 00:43:12.640]   time you finish, it's able to tell, OK, the first object is over here, the second object is over
[00:43:12.640 --> 00:43:17.280]   here, the third object is over here, and so on. And it can say the 100th object is over here. So
[00:43:17.280 --> 00:43:24.160]   that's why you get this output of 100 by 256, which is just the output. Forget the 6, that just
[00:43:24.160 --> 00:43:31.680]   means it's using ox loss. So what that means is, in deep learning terms, because the decoder is
[00:43:32.320 --> 00:43:37.840]   six layers in the DETR, it takes the output from each of these layers. So if you take the output
[00:43:37.840 --> 00:43:44.080]   of 100 by 256 from each of the six layers, your total output becomes like 6 by 100 by 256. I hope
[00:43:44.080 --> 00:43:50.720]   that shouldn't be confusing, but this is exactly what's going on in the DETR architecture. You
[00:43:50.720 --> 00:43:58.560]   start with-- again, I'll summarize using the-- going back to the paper, and then I'll take
[00:43:58.560 --> 00:44:06.720]   questions if there are any. So let's see the DETR. So let's use this image now to try and summarize.
[00:44:06.720 --> 00:44:14.720]   So I start with my input image over here. This is my input image. The input image goes through
[00:44:14.720 --> 00:44:22.000]   the backbone. The backbone will reduce the input image to a smaller feature size, which could be
[00:44:22.000 --> 00:44:28.320]   any-- depends on how deep your CNN is, or depends on all that stuff. But let's say it is
[00:44:28.640 --> 00:44:34.880]   a smaller feature map of 256 channels, and then some height with 40 by 40 or 29 by 26. It's just
[00:44:34.880 --> 00:44:40.480]   the idea is that the backbone is going to reduce my input image into a smaller feature map.
[00:44:40.480 --> 00:44:45.920]   And then for each of the position in that feature map, I'm going to supply a position encoding to
[00:44:45.920 --> 00:44:51.520]   my transformer, because the transformer has no idea about the relative positions of objects in
[00:44:51.520 --> 00:44:56.720]   my image, or it doesn't even know how that image is formed. So that's why we're providing these
[00:44:56.720 --> 00:45:01.920]   position encodings. And then the transformer encoder will do its magic. What does that mean?
[00:45:01.920 --> 00:45:06.480]   That just means that it's going to take my backbone output. It's going to take my position
[00:45:06.480 --> 00:45:11.280]   encoding output. It's going to mix the two. And then because now the transformer has information
[00:45:11.280 --> 00:45:16.480]   about the position of these various pixels in my image, and it also has the image information
[00:45:16.480 --> 00:45:22.960]   itself, it's now able to give some output, which I call, let's say, memory. This memory is this
[00:45:23.520 --> 00:45:30.320]   output from the transformer encoder, which is the transformer's way of encoding my input image
[00:45:30.320 --> 00:45:36.400]   into some matrix inside. So that's just a way of, like, let's just call it an intermediate output.
[00:45:36.400 --> 00:45:43.280]   It's just a representation of my image so that the transformer knows where each of the different
[00:45:43.280 --> 00:45:49.440]   objects are in my image. It's basically just the transformer reading my input image and then
[00:45:49.440 --> 00:45:54.080]   storing that information. It's just like we humans do. We look at an image, and then let's say we
[00:45:54.080 --> 00:45:59.520]   store that information in our eyes that we know, OK, when we look at an image, for example, in front
[00:45:59.520 --> 00:46:06.160]   of me, I can see, OK, the laptop is over here. The door is on my right. So that's just like a map of
[00:46:06.160 --> 00:46:12.720]   what that input image looks like. And then in the transformer decoder side of things, this is where
[00:46:12.720 --> 00:46:19.280]   all the interesting stuff happens, all the interactions happen, is that now if I'm
[00:46:19.280 --> 00:46:24.640]   trying to do object detection, I start with some random positions of these 100 different objects.
[00:46:24.640 --> 00:46:30.160]   So for example, in my room, I could say, OK, I think the desk is going to be on my right. It
[00:46:30.160 --> 00:46:34.960]   doesn't matter if the desk is on the left, OK? It doesn't matter. I can say, OK, I think the desk is
[00:46:34.960 --> 00:46:40.800]   going to be on my right. And then I think the bed is going to be on my left. I think the TV is going
[00:46:40.800 --> 00:46:46.960]   to be in my front. I start with some random positions. Then I mix the information that I have.
[00:46:46.960 --> 00:46:52.960]   Well, OK, it doesn't make sense that the TV is in front of the bed. I hope that this makes sense.
[00:46:52.960 --> 00:46:57.200]   That when the TV is in front of the bed, that doesn't look like a good position. So there has
[00:46:57.200 --> 00:47:02.800]   to be some interaction between these various different objects. So that happens the first time.
[00:47:02.800 --> 00:47:09.680]   And then once you have an interaction between the objects, like, OK, where the TV would be
[00:47:09.680 --> 00:47:14.800]   related to the bed or where the table would be related to the TV and the bed, then you mix that
[00:47:14.800 --> 00:47:20.720]   information with that previous map that I had in my head of the room. So I knew that the door is on
[00:47:20.720 --> 00:47:25.440]   the right. I knew that the screen is in front. So instead of now starting with these random positions
[00:47:25.440 --> 00:47:31.440]   of these 100 objects, I start to learn the position of these 100 objects. So that's what
[00:47:31.440 --> 00:47:37.280]   the decoder is doing. It's mixing the information that we get from the encoder. And it's mixing
[00:47:37.280 --> 00:47:43.680]   these 100 different-- it's mixing these 100 different random object locations. So now the
[00:47:43.680 --> 00:47:48.400]   decoder, as we go through over and over and over and over again, and we do these multiple, multiple
[00:47:48.400 --> 00:47:56.000]   epochs using the loss, it's-- without telling you what the loss is, at this point, then the model is
[00:47:56.000 --> 00:48:01.360]   actually learning the position of these 100 objects. There might not even be 100 objects
[00:48:01.360 --> 00:48:06.640]   in the image. That is fine. We will look at that later. But just have a look at this, that it's
[00:48:06.640 --> 00:48:12.240]   just starting with these 100 positions, but it's now able to mix and match, and it's able to interact.
[00:48:13.120 --> 00:48:18.320]   Based on this information. And then finally, you get some output from the decoder, which I've
[00:48:18.320 --> 00:48:26.240]   already said looks like-- as you can see, the output looks like this. Six by-- in terms of shape,
[00:48:26.240 --> 00:48:34.000]   six by one by 100 by 256. You could pass that output-- where are we? You could pass that output to
[00:48:34.000 --> 00:48:39.600]   any feedforward network to get my class and bounding box, class and bounding box, class and
[00:48:39.600 --> 00:48:47.440]   bounding box. That's how the DETR architecture overall works like. Okay, I've been talking
[00:48:47.440 --> 00:48:59.840]   quite a bit, and now is the time to take some questions. So let's go to questions.
[00:48:59.840 --> 00:49:03.200]   Sorry, one second.
[00:49:03.200 --> 00:49:25.600]   [PAUSE]
[00:49:25.600 --> 00:49:30.720]   All right, let's have a look. I could understand position encoding for 696 as the
[00:49:31.600 --> 00:49:37.600]   size of the image, but how position encoding can happen for 256 channels? What does that mean?
[00:49:37.600 --> 00:49:43.200]   Well, in position encoding, I think I would refer you, Durga, to the "attention is all you need"
[00:49:43.200 --> 00:49:50.400]   paper. It's not like the position encoding is happening for 256 channels. Think of it this way.
[00:49:50.400 --> 00:49:59.680]   A position encoding is a 256 long vector itself. So for each of the 40-- okay, let's do it this way.
[00:49:59.760 --> 00:50:03.120]   [PAUSE]
[00:50:03.120 --> 00:50:11.360]   So your input image is like this, right? Your input image is-- this is my input image. So let's say
[00:50:11.360 --> 00:50:18.960]   this is-- sorry, this should be fine. Let's say this is 29. Let's say this is 24, just as an
[00:50:18.960 --> 00:50:27.760]   example. So this is that, this is that. And this is 256. This is just my input image right now.
[00:50:27.760 --> 00:50:32.560]   Now, for each of these positions-- this position, this position, this position, this, this,
[00:50:32.560 --> 00:50:39.200]   so on-- all of these positions, I have my position encodings as a 256 long vector. Okay, that's that.
[00:50:39.200 --> 00:50:45.520]   So then I can add my position encodings to this input image, and then I have like a-- the
[00:50:45.520 --> 00:50:51.440]   transformer has some idea of looking at the position of these various different points in the
[00:50:53.680 --> 00:50:58.160]   image. I hope that helps. And if it does help, could you maybe please reply to this comment and
[00:50:58.160 --> 00:51:04.160]   just say, okay, thanks. That helped. How did they get the position encoding? Didn't understand that
[00:51:04.160 --> 00:51:12.160]   part. Oh, sorry, which part of position encoding? So in-- you could just start with-- like, with
[00:51:12.160 --> 00:51:19.520]   position encodings, there's two ways of having a position encoding. In the "Attention is all you
[00:51:19.520 --> 00:51:25.360]   need" paper, you can see how you have sinusoids and cosine, which is just-- each position gets
[00:51:25.360 --> 00:51:31.120]   a representation of a 256 long vector. But you could also learn position encoding. So there's--
[00:51:31.120 --> 00:51:36.720]   I think this question is coming from more from the "Attention is all you need" paper than from
[00:51:36.720 --> 00:51:44.800]   the ETR. But the main idea is you can have some representation, some 256 vector long
[00:51:44.800 --> 00:51:48.400]   representation for each of these different positions in my image.
[00:51:48.400 --> 00:51:59.760]   How are we mixing the transformer output of 696 by 1 by 256 with 100? Is it doing an attention--
[00:51:59.760 --> 00:52:05.520]   sorry, this is a-- I don't get this. How are we mixing the transformer output? Did you mean to
[00:52:05.520 --> 00:52:12.480]   say, Ramesh, the backbone output? Yes, the encoder output. Your encoder
[00:52:12.480 --> 00:52:19.520]   output is 696 long sequence, right? And then you use that in the decoder--
[00:52:19.520 --> 00:52:22.960]   Yeah. --object or the query vector,
[00:52:22.960 --> 00:52:27.200]   which is 100 by 1 by 256. Right.
[00:52:27.200 --> 00:52:30.080]   And when it goes in-- You mean all of this here, right?
[00:52:30.080 --> 00:52:31.840]   Yes, yes, yes. This is where the mixing is happening.
[00:52:31.840 --> 00:52:33.760]   Yes. OK. Well, it's just learning to pay
[00:52:33.760 --> 00:52:40.560]   attention to the 696-- well, it's just-- like, a 696, you could think of that as a sequence. So
[00:52:40.560 --> 00:52:46.160]   it's just learning to pay attention to these various objects, and it's learning to pay
[00:52:46.160 --> 00:52:56.320]   attention to the memory. So you can see how my-- you can see the query key and value of the inputs
[00:52:56.320 --> 00:53:01.600]   of this multi-head attention. So I would recommend you, without-- because if I go into the details--
[00:53:01.600 --> 00:53:02.080]   Yeah. --it takes--
[00:53:02.080 --> 00:53:05.840]   No, it makes sense to me. It makes sense to me. I had one more follow-up question on the same thing,
[00:53:06.960 --> 00:53:14.800]   is does it also-- you have m layers in decoder. Does it take the final output of encoder and
[00:53:14.800 --> 00:53:20.720]   feed and use that in all m layers of the decoder? That is correct. So it will take the output from
[00:53:20.720 --> 00:53:25.600]   the encoder, and then it will go into the first layer of the decoder, which goes one by one by
[00:53:25.600 --> 00:53:30.400]   one into the layers. But that's the idea. Instead of-- I think you're thinking it differently.
[00:53:30.400 --> 00:53:34.720]   You're thinking of these decoder layers as being separate, but they're actually stacked. So you
[00:53:34.720 --> 00:53:37.840]   have your first layer, then you have the second layer on top, then you have the third layer on
[00:53:37.840 --> 00:53:43.120]   top, then you have the fourth layer on top. But you can just pass this output from the encoder to
[00:53:43.120 --> 00:53:47.040]   the very first layer, and then you have these other layers stacked.
[00:53:47.040 --> 00:53:52.160]   Oh, so it doesn't pass to all m layers, only to the first layer of decoder?
[00:53:52.160 --> 00:53:58.080]   Yes, that's correct. It passes it to the first layer of the decoder, and then the second
[00:53:58.080 --> 00:54:02.560]   decoder layer gets the output from the first layer of the decoder. The third layer of the
[00:54:02.560 --> 00:54:06.320]   decoder gets the output from the second layer of the decoder, and that's how this whole system
[00:54:06.320 --> 00:54:11.520]   works. Again, this is a question more from attention is all you need than from the DETR
[00:54:11.520 --> 00:54:16.720]   paper. But for all of these questions, I would recommend just going back to just understanding
[00:54:16.720 --> 00:54:24.480]   the transformer architecture quite deeply. But that's the idea. So, Durga, I'm still waiting for
[00:54:24.480 --> 00:54:31.680]   if that explanation that I helped about-- this explanation about position encodings,
[00:54:31.680 --> 00:54:38.240]   does that help or not, please? Let's see if there's more questions. Is all of the--
[00:54:38.240 --> 00:54:46.560]   oh, excellent, thanks. Is all of the learning visual, spatial? I don't even know what that
[00:54:46.560 --> 00:54:51.920]   means. Are colors able to be incorporated? I don't remember any pre-selection talk
[00:54:51.920 --> 00:54:58.880]   as far as I know. What in the TV in front of the bed is neon green? I don't really understand this
[00:54:58.880 --> 00:55:05.360]   part. So I'm really sorry. I don't think I can help answer that question. More about-- that was
[00:55:05.360 --> 00:55:11.040]   just trying to help create an intuition. In terms of colors, because we have reduced the three
[00:55:11.040 --> 00:55:17.920]   channels to 256 channels, you don't have to think of the colors as being three separate channels
[00:55:17.920 --> 00:55:25.200]   anymore. The whole image is now 256 channels, 40 by 40. So it's now a lower resolution matrix.
[00:55:25.200 --> 00:55:30.240]   Now, what those 256 channels are, that's just up to the transformer to learn. So it's a very
[00:55:30.240 --> 00:55:40.720]   deep learning way of doing things. Can you explain why the size is 100 by 1 by 256 and not 100 by
[00:55:40.720 --> 00:55:49.920]   1 by 696? Oh, OK. Again, I would recommend going through the blog post. I guess that's where all
[00:55:49.920 --> 00:55:59.200]   of these questions will get answered in a lot more detail. But I guess the idea is you're asking,
[00:55:59.200 --> 00:56:08.080]   why is this 100 by 1 by 256? 256 is just a dimension. So 256 is just this dimension that
[00:56:08.080 --> 00:56:16.480]   I feel is good enough for my position encoding. So when I am adding stuff over here, this--
[00:56:18.480 --> 00:56:26.560]   I could then just add these 100 by 1 by 256. These are just my random object locations to start
[00:56:26.560 --> 00:56:32.880]   with. So for each of the 100 locations, I'm calculating-- each of the 100 locations are
[00:56:32.880 --> 00:56:41.120]   represented by a 256 long vector. But it hasn't anything to do with the 696. That 696 over here
[00:56:41.120 --> 00:56:47.680]   was just the encoder. That was just the image being reduced. And then the image-- that's just
[00:56:47.680 --> 00:56:52.560]   the image representation from the transformer encoder, which gets mixed over here. So for the
[00:56:52.560 --> 00:56:59.040]   exact shapes and how that marries with 696, again, I would recommend having a look at the blog post.
[00:56:59.040 --> 00:57:05.680]   There you will see every single line of code, every single line of the-- basically, the outputs
[00:57:05.680 --> 00:57:10.720]   being explained on what output marries with what shape. And then I think all of these problems or
[00:57:10.720 --> 00:57:17.440]   all of these doubts will go away. I'm conscious of time. I'm just going to do-- is there a comparison
[00:57:17.440 --> 00:57:21.760]   on FPS? I'm pretty sure there is. It says it's very comparable. And that's part of the paper.
[00:57:21.760 --> 00:57:31.520]   OK. Cool. Let's go back to the-- I still have to cover the loss. And I think we are running out
[00:57:31.520 --> 00:57:38.240]   of time. So I do want to-- in terms of-- I could have-- one thing I do want to clarify is we could
[00:57:38.240 --> 00:57:45.520]   have looked at the DETR paper without looking at the shapes. And I could have just provided a high
[00:57:45.520 --> 00:57:53.440]   level introduction of, oh, we start with an input image. Then the backbone produces some
[00:57:53.440 --> 00:57:58.240]   representation of the image. Then that goes to the encoder. And then that output from the encoder
[00:57:58.240 --> 00:58:05.520]   gets added to the decoder. And I could have done it that way as well. But I think having all these
[00:58:05.520 --> 00:58:12.960]   questions about shapes or how they work with each other, in that sense, you understand the
[00:58:12.960 --> 00:58:21.040]   architecture in a lot more detail. So I took the risk of all of this sounding a bit confusing
[00:58:21.040 --> 00:58:27.440]   when you start with. But when you have a look at maybe this video again when it's up on YouTube,
[00:58:27.440 --> 00:58:34.400]   and then you read the blog post alongside, I think all of these shapes will make sense. So I hope
[00:58:34.400 --> 00:58:45.520]   that that does become the case. So then continuing with the loss function. Again, for the loss
[00:58:45.520 --> 00:58:51.520]   function, I do want to-- I guess for the loss function, we could look at some of the code as
[00:58:51.520 --> 00:58:58.720]   well. But let's see. So where are we? The loss function was section 3. Here it is. That's the
[00:58:58.720 --> 00:59:05.600]   loss function. So what happens right now, forget about all the shapes and forget about, I guess--
[00:59:05.600 --> 00:59:15.440]   let's not worry about all the shapes of what we've learned so far. But let's just now think of this
[00:59:15.440 --> 00:59:28.320]   object or the DETR architecture as this is my DETR architecture. And it's giving me an
[00:59:28.320 --> 00:59:38.960]   output of 100 possible objects. So for each of the 100 objects, it's giving me, say, a bounding box.
[00:59:38.960 --> 00:59:45.120]   So that becomes 100 cross 4. Or for each of the 100 objects, it's also giving me a classification
[00:59:45.120 --> 00:59:49.920]   or a class token. So depending on how many classes we have, I could just have that being classified.
[00:59:49.920 --> 00:59:57.200]   So for each of the 100 random positions of the objects that I started with, it's now going to
[00:59:57.200 --> 01:00:05.520]   give me-- when I pass those things through-- so if I go back to my bigger model, where is that image,
[01:00:05.520 --> 01:00:11.040]   figure 1? So when I go through over here, these set of bounding box predictions are actually going
[01:00:11.040 --> 01:00:16.400]   to include the bounding box coordinates. And they're also going to include the classes.
[01:00:16.400 --> 01:00:24.240]   And then for each image, I'm also going to have ground truth. What does that mean? I'm going to
[01:00:24.240 --> 01:00:32.400]   know, OK, this is my first label. And then this is the exact bounding box. So I need to be able
[01:00:32.400 --> 01:00:42.000]   to match those outputs of 100 outputs from the DETR architecture to the actual ground truth.
[01:00:42.000 --> 01:00:50.240]   And when I look at the ground truth, in ground truth, the actual number of objects are going to
[01:00:50.240 --> 01:00:55.840]   be less than 100. So that's the idea. You start with the number of queries. So you basically say,
[01:00:55.840 --> 01:01:02.560]   OK, I'm just going to assume there's at max 100 objects in my image. So that's why you have these--
[01:01:02.560 --> 01:01:07.440]   you start with 100 random positions of the objects, so 100 random objects, basically.
[01:01:07.440 --> 01:01:15.280]   But I need to be able to match those outputs with either 2 or 3 or 4 or 5 or at max 10,
[01:01:15.840 --> 01:01:20.720]   20 objects that are actually in the image. So how do we do that? We use something called--
[01:01:20.720 --> 01:01:24.000]   sorry, coming back to section 3.
[01:01:24.000 --> 01:01:36.240]   We use something called a set prediction loss. And this set prediction loss is basically called
[01:01:36.240 --> 01:01:43.520]   as a-- you're able to-- then you need to be able to match my 100 outputs with the ground truth.
[01:01:43.520 --> 01:01:50.320]   So I think as part of this, let's just spend some more time. I'm sorry for going a bit over.
[01:01:50.320 --> 01:01:55.360]   But let's spend a little bit more time so we also understand or have at least a basic
[01:01:55.360 --> 01:02:01.440]   understanding of what goes on in set prediction loss. So as you can see, one of the main difficulties
[01:02:01.440 --> 01:02:06.960]   of training is to score predicted objects with respect to the ground truth. Because right now,
[01:02:08.000 --> 01:02:16.000]   you have 100 outputs from the-- you have 100 outputs from the DETR, but you don't actually know
[01:02:16.000 --> 01:02:24.160]   that which output belongs to which ground truth or like-- I guess let me try and explain that
[01:02:24.160 --> 01:02:32.160]   with the help of an image over here. So in my image, let's say I have two objects, right?
[01:02:32.160 --> 01:02:37.440]   Let's say this is my object. I'm just going to call it object 1. And I'm going to call this
[01:02:37.440 --> 01:02:44.720]   object 2. And I have 100 predictions for this image. But I don't really know which of these
[01:02:44.720 --> 01:02:50.560]   100 predictions is for object 1 or which of these 100 predictions is for object 2. We still need to
[01:02:50.560 --> 01:02:57.120]   match because these 100 predictions, let's say they are like this. This is 100 and this is 1.
[01:02:57.120 --> 01:03:06.320]   I still need to know, OK, maybe the 25th prediction that the model made is for this
[01:03:06.320 --> 01:03:11.200]   first object. And the 100th prediction that the model made is for the second object. So we need
[01:03:11.200 --> 01:03:20.080]   to be able to match the outputs from the DETR architecture to the actual objects. And that is
[01:03:20.080 --> 01:03:25.280]   why this task is a little bit difficult. So this is one of the main difficulties of training with
[01:03:25.280 --> 01:03:32.160]   DETR. And they say what we're going to do is we're going to use this bipartite matching.
[01:03:32.160 --> 01:03:37.920]   So what that means is I'm just looking for the word Hungarian loss that they would mention here.
[01:03:37.920 --> 01:03:43.760]   There it is. So what they're going to say is I'm going to be able to match my ground truth,
[01:03:43.760 --> 01:03:50.240]   which they represent as yi, with my predictions, which they say y sigma i or basically y hat.
[01:03:50.240 --> 01:03:56.240]   I'm going to be able to do a Hungarian algorithm. Now what exactly the Hungarian algorithm is?
[01:03:56.240 --> 01:04:08.640]   It's a way of, OK, it's a way of, so let's look at it this way. You have some tasks that you want
[01:04:08.640 --> 01:04:14.880]   to do. Let's say you have three tasks you want to do. And you have three workers. So let's say
[01:04:14.880 --> 01:04:21.840]   my worker one, worker two, worker three. And I have three tasks, T1, T2, T3. So worker one,
[01:04:21.840 --> 01:04:26.960]   let's say it takes seven days. Worker two takes three days. Worker three takes four days. Worker
[01:04:26.960 --> 01:04:33.360]   one for the task two takes five and so on, just like random numbers. And then one, nine, and three.
[01:04:33.360 --> 01:04:42.000]   And what you want to do is, let's say for this type of problem, you want to assign each task
[01:04:42.000 --> 01:04:50.320]   to a worker such that the total time taken to do the three tasks is minimum. And only one worker
[01:04:50.320 --> 01:04:55.360]   can do one task. So when you have this type of a problem, that's where the Hungarian matching
[01:04:55.360 --> 01:05:03.760]   algorithm comes in very handy. So we could say, OK, for this one to be minimum, task one should
[01:05:03.760 --> 01:05:08.240]   go to worker two, task two should go to worker three, and task three should go to worker one.
[01:05:08.240 --> 01:05:14.560]   So that the total days taken is one plus two plus three, which is basically six days. So you can
[01:05:14.560 --> 01:05:20.880]   accomplish all three tasks by giving it to these different workers, and you can finish in six days.
[01:05:20.880 --> 01:05:28.000]   Now, why I wanted to present this and why this is relevant and how this is relevant to Hungarian
[01:05:28.000 --> 01:05:37.520]   algorithm, basically what you do is you calculate a cost matrix. So let me show that in-- actually,
[01:05:37.520 --> 01:05:45.120]   it's much more nicely expressed in code. So you have my outputs.
[01:05:45.120 --> 01:05:55.920]   Sorry, I'm just thinking-- never mind. Don't worry about this for now. So what they do is,
[01:05:55.920 --> 01:05:59.440]   in paper, what they say is, don't worry about the code for now. I'm really sorry for bringing it up.
[01:05:59.440 --> 01:06:05.120]   I will look at it. I'll show you the code and how all of this relates with the code in just a
[01:06:05.120 --> 01:06:12.720]   moment. So what they do is, you have these two objects. So object one is here, object two is
[01:06:12.720 --> 01:06:18.400]   there, and then for the rest of the 98, they add a class phi, which is just saying, okay,
[01:06:18.400 --> 01:06:25.520]   there is no object, and you have no prediction of the bounding box for this. And then you have these
[01:06:27.600 --> 01:06:35.920]   my 100 predictions coming out from the DETR architecture. And what you can do now is you
[01:06:35.920 --> 01:06:43.200]   can calculate-- so these are my various different, say, classes. So this is class one, this is class
[01:06:43.200 --> 01:06:48.800]   two. All of these are phi phi and so on. And then let's say this says this is prediction for class
[01:06:48.800 --> 01:06:53.040]   one, this is prediction of class three, this is prediction of class seven, or however many classes
[01:06:53.040 --> 01:07:00.320]   I have. You could calculate the classification loss. So as you can see over here, it says
[01:07:00.320 --> 01:07:06.880]   I'm calculating my classification loss. So this is just calculating the classification loss between
[01:07:06.880 --> 01:07:14.160]   my predictions and the actual classes. This is just like cross-entropy. So I have my classification
[01:07:14.160 --> 01:07:21.840]   loss calculated. And then what I can do is-- because remember, my predictions and the ground
[01:07:21.840 --> 01:07:26.640]   truth, each of them would have a class label and then would have four coordinates for my bounding
[01:07:26.640 --> 01:07:32.880]   box. So these-- because on the right, these are my class labels. And then I have my four coordinates
[01:07:32.880 --> 01:07:38.720]   for my bounding box. Now, I could take, for example, these coordinates are, say, 100, 50,
[01:07:38.720 --> 01:07:45.360]   70, 80. And let's say the prediction here is 120, 10-- I'm just making these numbers up-- 10, 20,
[01:07:45.360 --> 01:07:51.520]   40. So then I could take the L1 loss, which is just the-- which is just a regression loss,
[01:07:51.520 --> 01:07:59.200]   which is a very common loss. I could take that loss between these two things. That's what happens
[01:07:59.200 --> 01:08:06.640]   over here. And then based on this, I can create my cost matrix, which looks like this. So I have my
[01:08:06.640 --> 01:08:12.720]   cost matrix. And then what I can do is I can say, oh, look, there's like these 100 different tasks,
[01:08:12.720 --> 01:08:18.000]   or there's like these 100 different predictions that are here. And these are my 100 workers.
[01:08:18.000 --> 01:08:24.080]   I need to assign these 100 predictions to these, say, 100 or like these actual objects so that my
[01:08:24.080 --> 01:08:32.800]   cost is a minimum. So now, based on this idea, this becomes-- based on this, you can now say,
[01:08:32.800 --> 01:08:45.120]   OK, the index 1 is associated-- should be like index, say, 29 in my prediction. Then index 2
[01:08:45.120 --> 01:08:51.360]   should be with index 1. So you can do all this matching. So this goes here. This goes here.
[01:08:51.360 --> 01:08:56.880]   This goes there. So basically, now you've been able to match the predictions with the actual
[01:08:56.880 --> 01:09:02.880]   ground truth. And if you have a look at the code, this is exactly what's going on. Forget the flat
[01:09:02.880 --> 01:09:07.520]   flattened parts, but you can see how I get my output probabilities. I get my output bounding
[01:09:07.520 --> 01:09:13.200]   boxes. I get my target IDs. I get my target bounding boxes. What does that mean? I get my
[01:09:13.200 --> 01:09:18.000]   output. So these are my output probabilities. These are going to be the class probabilities,
[01:09:18.000 --> 01:09:22.240]   and these are going to be the target IDs. And then similarly, I get the four coordinates.
[01:09:22.240 --> 01:09:27.920]   So that's what I get over here. And then I can calculate the cost for my classification. I can
[01:09:27.920 --> 01:09:34.400]   calculate the cost for my bounding boxes. The next thing they do say is we add a generalized box IOU.
[01:09:34.400 --> 01:09:41.120]   Forget that for a moment. But then once you have this final cost matrix, which is just this loss,
[01:09:41.120 --> 01:09:46.880]   you can use this linear sum assignment or this Hungarian matching, which I showed this idea
[01:09:46.880 --> 01:09:54.080]   over here. So you can have this idea now. Now you know, OK, which prediction needs to be assigned
[01:09:54.080 --> 01:10:01.040]   to which part of the ground truth. And then once you have that, then that's what becomes loss.
[01:10:01.040 --> 01:10:07.600]   Then you start training it over and over and over again. And this is this idea of-- this is this
[01:10:07.600 --> 01:10:13.680]   idea of this set prediction loss. So you can see how you have the final cost matrix. And then you
[01:10:13.680 --> 01:10:18.240]   can use this linear sum assignment. So what it will do is it will return the indexes. It's going
[01:10:18.240 --> 01:10:23.760]   to say, OK, index 1 should be associated with index 29. And then that's how you finally get.
[01:10:23.760 --> 01:10:29.920]   And once you have those indexes, you can now calculate the loss. So you can calculate the
[01:10:29.920 --> 01:10:37.040]   loss between my actual prediction and the ground truth. And then I can calculate the loss between
[01:10:37.600 --> 01:10:43.760]   this index 2, which got assigned to index 100. And then I could calculate the loss between my
[01:10:43.760 --> 01:10:48.960]   index 3, which got assigned to, let's say-- or let's say this index, which got assigned to this
[01:10:48.960 --> 01:10:55.520]   part. So now I can now basically look at my predictions. And this is a way how the DETR
[01:10:55.520 --> 01:11:05.200]   calculates the loss. So I hope that makes sense. That's this idea of a Hungarian algorithm.
[01:11:06.160 --> 01:11:11.520]   Now I'm happy to take as many questions as you guys have. If I have answers for them, I will
[01:11:11.520 --> 01:11:16.080]   provide those answers. Let's see what the reply to this comment is.
[01:11:16.080 --> 01:11:23.600]   Thank you. We'll look at it. OK. Let's see and have a look ourselves. Let's see where the--
[01:11:23.600 --> 01:11:29.440]   I'm pretty sure it is there. So we are looking at FPS. There we go.
[01:11:30.160 --> 01:11:40.000]   Farstar CNN, that's what the FPS is. DTR, that's what the FPS is. So you want to look at table 1.
[01:11:40.000 --> 01:11:46.880]   I think. Cool. If there's no questions then about the DETR architecture--
[01:11:46.880 --> 01:11:54.320]   oh, no, no. They are ignored. So one thing you will see-- sorry, that's something I should have
[01:11:54.320 --> 01:12:00.320]   mentioned. Thanks, Ramesh, for pointing that out. So one thing you will see if I go in this loss,
[01:12:00.320 --> 01:12:07.680]   you will see-- here you go. So you only add the bounding box loss if your thing is not phi.
[01:12:07.680 --> 01:12:12.880]   Basically, you only look at the bounding box loss if the object is actually present.
[01:12:12.880 --> 01:12:15.760]   So that's a good question. Thanks, Ramesh, for pointing that out.
[01:12:17.840 --> 01:12:26.560]   Cool. Then I guess that's where we'll stop at the DETR if there's no more questions. Next week,
[01:12:26.560 --> 01:12:33.440]   we have a special guest. And I won't be doing any explanation next week. But we have
[01:12:33.440 --> 01:12:41.920]   Aishwarya Kamath. She's joining us. So she is doing her PhD under Yann LeCun. So you can have a look
[01:12:42.800 --> 01:12:47.920]   over here. And then you can see how she's done all these various different papers. And one of
[01:12:47.920 --> 01:12:52.960]   the really nice papers that I recently read, which got pointed to me by somebody else,
[01:12:52.960 --> 01:13:00.320]   was this MDETR, which is this idea of multimodal detection. So now we will see how the DETR
[01:13:00.320 --> 01:13:04.800]   architecture-- because it's all a transformer, right? So we can see how this-- I'll just actually
[01:13:04.800 --> 01:13:12.320]   show you a glimpse of what's happening. So in this idea, we will see how you can say to an image,
[01:13:12.320 --> 01:13:18.400]   you can say you want to find the bounding box location and classification of a pink elephant.
[01:13:18.400 --> 01:13:24.640]   And then it can create a bounding box around the pink elephant. And these are not like masks or
[01:13:24.640 --> 01:13:28.160]   anything. These are the actual pixels. So you could have said a blue elephant, and it would
[01:13:28.160 --> 01:13:33.120]   have created a bounding box around this blue elephant. So we're going to see that in two
[01:13:33.120 --> 01:13:39.280]   weeks from now. So with that being said, thanks, everybody, for joining me today. And I'll see you
[01:13:39.280 --> 01:13:47.920]   guys in two weeks.
[01:13:47.920 --> 01:14:00.160]   [MUSIC PLAYING]

