
[00:00:00.000 --> 00:00:05.040]   You said today is the worst product from us you'll use today.
[00:00:05.040 --> 00:00:10.440]   The model that you use today is the worst AI model that you'll ever use for the rest of your life.
[00:00:10.440 --> 00:00:29.820]   Kevin, so, you know, on this slide, in five years, as the ninth most valuable company in the world,
[00:00:30.520 --> 00:00:35.200]   okay, so $300 billion in enterprise value today is measured by the markets.
[00:00:35.200 --> 00:00:37.440]   They have it going to $1.6 trillion.
[00:00:37.440 --> 00:00:45.200]   I remember when you joined OpenAI, and I asked you the question, can OpenAI get to a trillion dollars, right, in total value?
[00:00:45.200 --> 00:00:53.100]   And you said, that is, like, if I thought it was just a trillion dollars, I wouldn't be nearly as excited as I am about going to OpenAI.
[00:00:53.100 --> 00:00:54.580]   So when you just think about that-
[00:00:54.580 --> 00:00:59.800]   And by the way, less about market cap and, you know, more about, like, the ambition and the opportunity and the impact.
[00:00:59.800 --> 00:01:01.660]   And so that's what I want you to do.
[00:01:01.660 --> 00:01:03.560]   Just blue sky for us today.
[00:01:03.560 --> 00:01:05.520]   How long have you been at OpenAI now?
[00:01:05.520 --> 00:01:06.020]   About a year.
[00:01:06.020 --> 00:01:06.360]   Okay.
[00:01:06.360 --> 00:01:08.280]   So you're a year into this.
[00:01:08.280 --> 00:01:13.420]   A year in, do you think the opportunity is bigger than you thought when you started?
[00:01:13.420 --> 00:01:16.260]   And why would you have it?
[00:01:16.260 --> 00:01:17.340]   And if so, why do you think?
[00:01:17.340 --> 00:01:18.480]   Talk us through that a little bit.
[00:01:19.100 --> 00:01:22.160]   I think it's bigger for sure than I realized.
[00:01:22.160 --> 00:01:31.060]   I think it's been in Sam's head the whole time, so I'm not sure that there's been a revelation over the past year, but it's certainly opened my eyes.
[00:01:31.060 --> 00:01:31.680]   Right, right, right.
[00:01:31.740 --> 00:01:36.020]   I mean, for one, we have grown faster than anything I've ever seen.
[00:01:36.020 --> 00:01:36.360]   Yeah.
[00:01:36.880 --> 00:01:41.720]   And I thought I had seen fast growth when I was at Instagram, for example.
[00:01:41.720 --> 00:01:42.420]   Yeah.
[00:01:43.140 --> 00:01:49.540]   And for those falling at home on slide 24 in the deck, they show the user growth.
[00:01:49.540 --> 00:01:52.180]   And we talked about it earlier, but it's just phenomenal.
[00:01:52.180 --> 00:01:53.040]   Yeah.
[00:01:53.040 --> 00:01:57.440]   So, and as we've grown, we're also deepening usage with the platform.
[00:01:57.440 --> 00:01:58.560]   People are using it more.
[00:01:59.000 --> 00:02:05.300]   So, just from that perspective, we are, I think, bigger today than I would have imagined was possible a year ago.
[00:02:05.300 --> 00:02:05.960]   Walk people through that.
[00:02:05.960 --> 00:02:07.340]   They had a slide on that also.
[00:02:07.340 --> 00:02:14.060]   So, and you said some comments on stage about, you know, as people use it more, the minutes go up.
[00:02:14.060 --> 00:02:14.380]   Yeah.
[00:02:14.380 --> 00:02:16.640]   Well, I mean, we look at weekly active users.
[00:02:16.640 --> 00:02:20.300]   That's sort of the metric that we goal ourselves by from a growth perspective.
[00:02:20.300 --> 00:02:23.680]   Obviously, a lot of other things that we look at as well.
[00:02:23.680 --> 00:02:33.940]   But from a weekly active user, the reason we do that versus monthly is we're actually, you know, we don't want someone that just comes once a month.
[00:02:33.940 --> 00:02:38.320]   That doesn't, that means we're probably not adding all that much value in their life if they're coming once a month.
[00:02:38.320 --> 00:02:42.480]   And over time, I think it's going to be more natural for us to measure daily active users.
[00:02:42.480 --> 00:02:48.980]   Because really, if we're doing our job, then you're using ChatGPT on a daily basis and hopefully multiple times per day.
[00:02:49.460 --> 00:02:53.780]   Because it can help you multiple times per day with all the different things that you encounter in your life.
[00:02:53.780 --> 00:02:57.840]   There's sort of the, are you a monthly user, are you a weekly user, a daily user?
[00:02:57.840 --> 00:03:01.120]   And then, you know, on a given day, how much time are you spending in the product?
[00:03:01.120 --> 00:03:11.400]   And you guys had a chart, your data, not ours, but that showed that people were using ChatGPT a whole lot more, you know, time in per day.
[00:03:12.480 --> 00:03:13.280]   And that's awesome.
[00:03:13.280 --> 00:03:14.060]   That's super exciting.
[00:03:14.060 --> 00:03:17.480]   That just means that we're actually helping people solve problems.
[00:03:17.480 --> 00:03:25.820]   And they're not just using it, you know, like you go back a couple years when it launched, it was really good at various sorts of writing.
[00:03:25.820 --> 00:03:31.940]   If you're going to do copywriting, if you were going to, you know, have it summarize emails or things like that.
[00:03:32.300 --> 00:03:35.400]   And you look today and it's so much broader.
[00:03:35.400 --> 00:03:36.840]   It can help in so many other areas.
[00:03:36.840 --> 00:03:47.500]   People are uploading, you know, that actually my own son had a health thing, had to have a minor surgery, which was not that big of a deal.
[00:03:47.500 --> 00:03:49.040]   At least it was not supposed to be that big of a deal.
[00:03:50.000 --> 00:03:53.880]   And, but there was a tiny chance that there was something more serious, you know?
[00:03:53.880 --> 00:03:57.880]   And so he has the surgery, he's eight, like next day he's playing a soccer game.
[00:03:57.880 --> 00:03:58.740]   He doesn't know any better.
[00:03:58.740 --> 00:03:59.120]   Yes.
[00:03:59.120 --> 00:04:03.300]   But then we're still waiting for like the biopsy of this thing to come back.
[00:04:03.300 --> 00:04:03.520]   Yes.
[00:04:04.000 --> 00:04:12.320]   And we end up getting a paper in the mail, letter in the mail, that has a whole bunch of like complex medical stuff on it.
[00:04:12.320 --> 00:04:12.680]   Right, right.
[00:04:12.680 --> 00:04:14.880]   I'm like, you know, halfway educated.
[00:04:14.880 --> 00:04:16.000]   I couldn't make sense of it.
[00:04:16.000 --> 00:04:16.620]   I didn't know what it means.
[00:04:16.620 --> 00:04:20.060]   And I called the doctor and I couldn't get a hold of her.
[00:04:20.060 --> 00:04:21.260]   She was in a surgery, I think.
[00:04:21.260 --> 00:04:21.500]   Right.
[00:04:21.500 --> 00:04:22.240]   And, you know, she's busy.
[00:04:22.240 --> 00:04:23.320]   She's got a million things going on.
[00:04:23.320 --> 00:04:27.820]   And I'm looking at this and there's a bunch of like scary looking medical terms.
[00:04:27.820 --> 00:04:28.120]   Yeah.
[00:04:28.120 --> 00:04:31.860]   And so the first thing I do, of course, is take a picture of it.
[00:04:31.860 --> 00:04:34.180]   I upload it to ChatGPT and say, hey, should I be worried about this?
[00:04:34.180 --> 00:04:36.280]   And it comes back immediately.
[00:04:36.280 --> 00:04:37.140]   No, you're fine.
[00:04:37.140 --> 00:04:38.360]   This is like completely benign.
[00:04:38.360 --> 00:04:39.000]   Don't worry about it.
[00:04:39.000 --> 00:04:39.800]   You're all good.
[00:04:39.800 --> 00:04:43.080]   But I actually wasn't able to get a hold of the doctor for 72 hours.
[00:04:43.080 --> 00:04:43.320]   Right.
[00:04:43.320 --> 00:04:50.760]   That would have been a really bad 72 hours for me as a parent, sitting there stressing about this note that I can hardly read.
[00:04:50.760 --> 00:05:00.900]   You know, I think what you're describing, you know, as someone who's gone from once a day to 20 times a day, like you learn what it can do.
[00:05:01.040 --> 00:05:03.000]   You learn the kinds of things it can do.
[00:05:03.000 --> 00:05:09.440]   Like you can take a photo of an appliance and ask it how to set the clock or something.
[00:05:09.440 --> 00:05:20.740]   And so one of the reasons I think it expands is people come to understand all the things it can do and all the kinds of prompts.
[00:05:20.740 --> 00:05:29.180]   You know, another one is like I used to do a Google search, go to a web page, find a number, and then put that into a formula and multiply it on a calculator.
[00:05:29.180 --> 00:05:30.580]   It can just do all that.
[00:05:30.580 --> 00:05:30.880]   Right.
[00:05:30.880 --> 00:05:34.140]   Like you can say, go get this and run this.
[00:05:34.140 --> 00:05:39.940]   And now you can connect it to your email and your docs and your calendar, and it can start to be useful in so many other ways.
[00:05:39.940 --> 00:05:45.000]   You know, what you used to have to go search through 20 docs for, now you can go just ask for answers.
[00:05:45.000 --> 00:05:47.100]   So it's just, it's so much more useful.
[00:05:47.100 --> 00:05:52.860]   And I totally agree the way to sort of understand it because it's also constantly evolving, right?
[00:05:52.860 --> 00:05:58.360]   The models get step changes better every two or three months is you just start using it.
[00:05:58.620 --> 00:06:04.980]   You know, the number one advice I have for people when they like ask what they should do with ChatGPT is just start using it.
[00:06:04.980 --> 00:06:14.100]   Because when you start using it for things and you take, you know, little tiny risks and try it for new things that you're not sure if it can do, you realize in most cases it can.
[00:06:14.100 --> 00:06:17.040]   And then it just becomes more and more helpful to you over time.
[00:06:17.040 --> 00:06:18.340]   And I think the numbers reflect that.
[00:06:18.340 --> 00:06:25.100]   When you see, so you're running product, your goal is to drive engagement, drive utility, thrill your customers.
[00:06:25.840 --> 00:06:33.320]   So when, talk to us about the things you're actively doing to take somebody from a monthly to a weekly, from a weekly to a daily.
[00:06:33.320 --> 00:06:39.060]   I'm sure a lot of this is happening organically as Bill describes, but I'm sure you're pressing on the flywheel.
[00:06:39.060 --> 00:06:46.780]   And then if we're at a billion monthlies today, like how do you set a measure for success for you and your team, right?
[00:06:46.780 --> 00:06:49.760]   Like, do you have a goal, a big audacious goal?
[00:06:49.760 --> 00:06:51.760]   I want to get to 2 billion in three years.
[00:06:51.760 --> 00:06:54.060]   Like, how are you thinking about that?
[00:06:54.840 --> 00:06:59.240]   We do less sort of mechanical things of how do I get a monthly to a week.
[00:06:59.240 --> 00:07:12.380]   It's more like we have an incredible research team, which is an absolutely world-class research team that's constantly iterating on models, making the models suddenly able to do things that computers have never been able to do before in the history of computers.
[00:07:13.380 --> 00:07:23.240]   And so we think a lot more about how do we stay really tight with the research team, because it's just this incredible nucleus of, like, creativity and discovery.
[00:07:23.780 --> 00:07:30.460]   And how do we stay really close to them so that we can constantly be building products just on the edge of what the models can do?
[00:07:30.460 --> 00:07:30.880]   Right.
[00:07:30.880 --> 00:07:38.600]   As our feeling is, if we're building just on the edge of what models can do, then we suddenly have this product that can do things that no one's ever seen a product do before.
[00:07:39.120 --> 00:07:50.100]   And even if it's not perfect right when it starts, because we tend to, we believe in iterative deployment, we release early, we release often, like, better to make lots of small mistakes.
[00:07:50.100 --> 00:07:58.120]   And, you know, we kind of collectively as society, like, understand how, what AI is good at, what it's bad at, how we work through it.
[00:07:58.120 --> 00:07:59.920]   We'd rather do that.
[00:07:59.920 --> 00:08:13.740]   And if you build a product that can do amazing things, even if it's only 70%, you know, right at that thing, in two or three months, the next model is going to come along, and it's going to be 95% good at that thing.
[00:08:13.940 --> 00:08:24.260]   And all of a sudden, you know, you've, you've got something great, but, but people, we've, we've sort of brought everybody along with us and learn from them at the same time, like, learn from the way there's, the way that they're using it.
[00:08:24.260 --> 00:08:35.100]   So there's a, I think the, the, the thing that we've really tried to get right, and we're not perfect at this, but I think we're a lot better than we used to be, is the, the really tight loop between research and product.
[00:08:35.100 --> 00:08:35.540]   Yes.
[00:08:36.460 --> 00:08:46.800]   So when we're, when we have that loop, right, and we're, we're solving products, the problems that people have, feedback goes back to research, the research team goes, oh, you, oh, it can't do that very well.
[00:08:46.800 --> 00:08:47.740]   Oh, we can fix that.
[00:08:47.740 --> 00:08:48.140]   Right.
[00:08:48.140 --> 00:08:49.420]   And then the product gets better.
[00:08:49.420 --> 00:08:51.720]   And we have that, like, that's when magic happens.
[00:08:51.720 --> 00:08:57.360]   You said something prophetic on stage today and simple, but I, it really caught me.
[00:08:57.360 --> 00:09:03.700]   You said, you said today is the worst product from us you'll use today.
[00:09:03.700 --> 00:09:09.040]   The model, the model that you use today is the worst AI model that you'll ever use for the rest of your life.
[00:09:09.040 --> 00:09:09.320]   Yeah.
[00:09:09.320 --> 00:09:11.640]   Which is really, it's a simple thing.
[00:09:11.640 --> 00:09:17.420]   And it's really kind of obviously true when you think about it, but it just changes the way you think about building products.
[00:09:17.420 --> 00:09:23.340]   Because I think it, if you think about it the right way, it makes you much more open to building products that only kind of work.
[00:09:23.340 --> 00:09:23.740]   Right.
[00:09:23.740 --> 00:09:33.680]   You know, whether you're us building ChatGPT and other products, or whether you're an enterprise, you know, building some internal tool or, because if the model only kind of work, you know,
[00:09:33.680 --> 00:09:38.100]   If the model can kind of do it, then it's going to be great at it in a few months.
[00:09:38.100 --> 00:09:41.780]   What things are you most excited about that are on the horizon?
[00:09:42.860 --> 00:10:07.080]   I'm, I'm really excited about the, in order for ChatGPT to be truly useful, you need to go from it just answering questions that you have, to it actually doing things for you in the real world to like, you know, ideally, even proactively, to understand the things that you're going to need to do, and help you do that, or, you know, suggest them queue up a bunch of actions for you before you even get there.
[00:10:08.040 --> 00:10:14.500]   And it's why we've been really focused on adding things like connections into Google Docs, and you know, the products and services that you use every day.
[00:10:15.300 --> 00:10:25.020]   We've been really focused on personalization and memory so that, you know, ChatGPT gets to know you and what your preferences are, how you like to interact with it.
[00:10:25.720 --> 00:10:40.340]   In my case, I've like, made sure that it knows about my wife and what she does, you know, she's a seed investor, my kids, how old they are, because then when I ask it, you know, hey, we have some free time, what should we go do?
[00:10:40.660 --> 00:10:41.940]   Or we're going to go on this trip.
[00:10:41.940 --> 00:10:47.720]   It doesn't just recommend random hotels, it recommends hotels that are, that are great for my family.
[00:10:47.720 --> 00:10:48.000]   Yeah.
[00:10:48.000 --> 00:10:52.540]   And that, you know, that kind of thing, each one is small, but it adds up.
[00:10:52.540 --> 00:10:52.880]   Right.
[00:10:52.880 --> 00:11:01.580]   And especially if you think about getting proactive and, you know, really getting to know you and helping with something that is going to, you're going to realize you need to do four hours from now.
[00:11:02.940 --> 00:11:04.440]   Personalization is going to be a big deal.
[00:11:04.440 --> 00:11:05.120]   No doubt.
[00:11:05.120 --> 00:11:10.800]   One quick question on the, on the kind of interactivity between different apps.
[00:11:10.800 --> 00:11:13.300]   There was some kerfuffle last week.
[00:11:13.300 --> 00:11:21.480]   I think Salesforce changed their terms of service in a way that would make it more difficult for other people to, to build models.
[00:11:21.480 --> 00:11:24.680]   How do you, how do you think that's going to play?
[00:11:24.680 --> 00:11:27.760]   Do you think people will put up walls or not?
[00:11:27.760 --> 00:11:32.920]   You know, will they be friendly with you, you know, running a widget on top of that?
[00:11:32.920 --> 00:11:34.000]   For their app or not?
[00:11:34.000 --> 00:11:34.520]   Yeah.
[00:11:34.520 --> 00:11:35.420]   It's a good question.
[00:11:35.420 --> 00:11:40.040]   I think we're going to kind of feel our way there as a, as an industry, as a society.
[00:11:40.040 --> 00:11:46.000]   It's one of the reasons that we believe so much in iterative deployment where we release early, release often.
[00:11:46.000 --> 00:11:50.300]   Cause AI is going to change just about everything.
[00:11:50.300 --> 00:11:50.680]   Yeah.
[00:11:50.680 --> 00:11:52.360]   It's going to change the way the web works.
[00:11:52.360 --> 00:11:54.200]   It's going to change the way we interact with services.
[00:11:54.200 --> 00:11:59.040]   It's going to, you know, and we don't want to do that unilaterally.
[00:11:59.040 --> 00:12:02.400]   We got to, you know, I think the way to do this smoothly is to kind of iterate together.
[00:12:02.400 --> 00:12:03.260]   It makes total sense.
[00:12:03.260 --> 00:12:10.920]   And so, you know, we figure the more we release and put this stuff out in the world, the world, we all kind of get a sense.
[00:12:10.920 --> 00:12:12.120]   It's not like we know all the answers, right?
[00:12:12.120 --> 00:12:16.640]   We all kind of get a sense of where things are going and we can co-evolve in the right way.
[00:12:16.640 --> 00:12:22.220]   I want you to have access to my contacts, my address book, my email, my text, my everything.
[00:12:22.220 --> 00:12:22.620]   Yeah.
[00:12:22.620 --> 00:12:23.620]   And that's your data.
[00:12:23.620 --> 00:12:27.580]   You should be able to take it to whatever AI you want to take your data to.
[00:12:27.580 --> 00:12:28.940]   You're paying for that data.
[00:12:28.940 --> 00:12:40.080]   On those lines, Kevin, like one of the things that Bill and I have talked a lot about on this pod is our concern about kind of like regulatory capture, regulatory intervention.
[00:12:40.800 --> 00:12:43.860]   Folks who are alarmist and saying, this is dangerous.
[00:12:43.860 --> 00:12:45.100]   We need to slow it down.
[00:12:45.100 --> 00:12:48.080]   We need to, you know, we need to really regulate it.
[00:12:48.400 --> 00:12:54.940]   And along those same lines, you know, like whether or not we're going to have open source models or every model is going to be closed.
[00:12:54.940 --> 00:13:03.440]   It seems to me that open AI has been on this journey over the course of let's call it the last year or two where there was a belief.
[00:13:03.440 --> 00:13:09.320]   I think that open AI was in the, you know, go to Washington and regulate everything and kind of slow it down.
[00:13:09.440 --> 00:13:14.060]   And now I think it's like more clear the distinction that's being drawn.
[00:13:14.060 --> 00:13:17.280]   Open AI is talking about launching an open source model here shortly.
[00:13:17.280 --> 00:13:27.540]   You seem to be in the camp of, you know, it's too early to regulate, get out there, accelerate, make sure that we distribute around the world and America's models are doing well.
[00:13:27.540 --> 00:13:31.960]   Is that a fair characterization, you think, of where open AI is today?
[00:13:32.240 --> 00:13:34.500]   I think no matter what, it's important to be engaging, right?
[00:13:34.500 --> 00:13:37.940]   Like, like I said, this is a, AI is going to change everything.
[00:13:37.940 --> 00:13:41.380]   And we got to, I think we do better when we co-evolve.
[00:13:41.380 --> 00:13:46.740]   And so, and it's one of the reasons I was saying earlier, like people should just be trying the models.
[00:13:46.740 --> 00:13:51.280]   I think the same is true of our, of our lawmakers and folks like that.
[00:13:51.280 --> 00:13:55.180]   They should be just like getting to use AI.
[00:13:55.180 --> 00:13:58.360]   You start using it and you're like, oh, this isn't so scary.
[00:13:58.360 --> 00:13:59.160]   This is helpful.
[00:13:59.440 --> 00:14:02.840]   And you understand the nuances, you understand what it's good at, what it's not good at.
[00:14:02.840 --> 00:14:03.080]   Right.
[00:14:03.080 --> 00:14:04.780]   You get a sense for how it's improving.
[00:14:04.780 --> 00:14:06.420]   I think that's really important.
[00:14:06.420 --> 00:14:07.860]   Like context really matters.
[00:14:07.860 --> 00:14:16.460]   So that's, you know, that's one of the things we do when we're in DC is part of it is just like getting together and kind of giving people a sense of here's where it is today.
[00:14:16.460 --> 00:14:17.860]   And here's where we think it's going.
[00:14:17.860 --> 00:14:18.600]   Right.
[00:14:18.600 --> 00:14:20.460]   So that people can make informed decisions.
[00:14:20.460 --> 00:14:25.640]   I don't know if you read Sam Altman's blog that was out this week called The Gentle Singularity.
[00:14:25.640 --> 00:14:28.420]   I would encourage everybody to actually go read it.
[00:14:28.480 --> 00:14:41.120]   And it's this idea, Bill, that it's not this scary thing that, you know, all of a sudden is going to show up, which I think a lot of people were thinking about with AGI, that there was going to be this moment that everything was going to change.
[00:14:41.120 --> 00:14:41.500]   Right.
[00:14:41.500 --> 00:14:46.960]   Sam's like the world has already changed dramatically over the course of the last 24 months.
[00:14:47.360 --> 00:14:51.160]   But people aren't, you know, it's not changed that much about our daily lives.
[00:14:51.160 --> 00:14:56.320]   We just switched from using Google to get 10 blue links to using ChatGPT to get answers to our questions.
[00:14:56.320 --> 00:15:02.840]   I'm now uploading my blood test information to ChatGPT because I want it to tell me things that are useful about it.
[00:15:02.840 --> 00:15:03.080]   Yeah.
[00:15:03.140 --> 00:15:06.360]   I still am at the point where I find those things pretty extraordinary.
[00:15:06.360 --> 00:15:06.920]   Right.
[00:15:06.920 --> 00:15:11.040]   I can't believe that it can do these things and that I'm getting it for next to free.
[00:15:11.940 --> 00:15:13.280]   But it has come.
[00:15:13.280 --> 00:15:20.820]   That level of profound change has come without that big of, like, you know, societal disruption.
[00:15:20.820 --> 00:15:21.220]   Yeah.
[00:15:21.220 --> 00:15:30.480]   And, you know, I think we definitely are in the camp that it is way too early to obstruct American AI with, you know, useless regulations.
[00:15:30.480 --> 00:15:33.640]   Like, we don't even know what we should be regulating.
[00:15:33.640 --> 00:15:38.580]   It's like regulating the auto industry, right, out of existence early.
[00:15:38.580 --> 00:15:46.620]   It took us 40 years to realize we needed seatbelts and airbags and that we needed speed limits and things like that.
[00:15:46.680 --> 00:15:51.980]   And we'll need all those things for AI, but I think it's way too early to be doing that.
[00:15:51.980 --> 00:16:02.920]   And so it's been a welcome, I think, development on our end to see, you know, open AI increasingly vocal about the need to make sure that, you know, we stay open and we stay.
[00:16:02.920 --> 00:16:11.860]   Kevin, one thing you, and I know you didn't give any specifics, but I think you had a question from the audience about the hardware device.
[00:16:12.260 --> 00:16:18.840]   And you did share some philosophical thoughts because I think the audience would be thrilled to hear what you said earlier.
[00:16:18.840 --> 00:16:24.700]   This is about the acquisition of Johnny Ives, you know, business and what you guys intend to do in hardware.
[00:16:24.700 --> 00:16:25.200]   Yeah.
[00:16:25.200 --> 00:16:39.320]   Oh, I just said, and I think this is true of hardware as it is of software, just that AI is going to change everything about the way that we do our jobs, the way that we, you know, get our, get stuff done in our personal lives.
[00:16:39.940 --> 00:16:48.700]   And I think basically every product, service, device, et cetera, that we use will be, will need to be reinvented.
[00:16:48.700 --> 00:16:49.140]   Yes.
[00:16:49.140 --> 00:16:51.540]   And that doesn't mean that the incumbents can't reinvent it.
[00:16:51.540 --> 00:16:52.620]   I think sometimes they will.
[00:16:52.620 --> 00:16:53.060]   Right.
[00:16:53.300 --> 00:16:57.360]   But I don't, I doubt in every case, they will certainly history doesn't tell us that that would be true.
[00:16:57.360 --> 00:17:02.660]   And so I think that means there's a huge opportunity for, for reinvention.
[00:17:02.660 --> 00:17:10.000]   And that's, you know, that's an opportunity for us in places where we think we can play a role, we, where we think we have a perspective, we'll go compete.
[00:17:10.000 --> 00:17:13.080]   It also means great opportunities for startups.
[00:17:13.080 --> 00:17:13.620]   Yes.
[00:17:13.940 --> 00:17:20.480]   And by the way, I do think it means good opportunities for incumbents if they can move quickly and sort of overcome the innovator's dilemma.
[00:17:20.480 --> 00:17:22.440]   That's a really hard thing to do.
[00:17:22.440 --> 00:17:23.000]   No doubt.
[00:17:23.100 --> 00:17:34.260]   So I just think if you look at the, the, the products that you spend your time in on a daily basis and compare it to, you know, five years from now, I think they're going to be dramatic.
[00:17:34.260 --> 00:17:38.900]   Is there any, are you allowed to share any time window where we might find out more?
[00:17:38.900 --> 00:17:40.500]   You'll have to find out.
[00:17:40.500 --> 00:17:41.480]   Okay.
[00:17:41.480 --> 00:17:42.080]   No.
[00:17:42.080 --> 00:17:45.700]   You know, we, we had you do a flyby today.
[00:17:45.700 --> 00:17:49.020]   We're going to have you come on the pod and we'll do a full blown podcast together.
[00:17:49.020 --> 00:17:49.600]   Sounds good.
[00:17:49.600 --> 00:17:52.780]   We're here celebrating the 10th anniversary of East meets West.
[00:17:52.780 --> 00:17:53.180]   Yep.
[00:17:53.180 --> 00:17:57.180]   We were saying earlier, it's an event that we all look forward to go to coming to.
[00:17:57.180 --> 00:18:00.700]   I'm going to, I'm going to ask you this final question, kind of big picture.
[00:18:00.860 --> 00:18:03.700]   They forecasted out five years, right?
[00:18:03.700 --> 00:18:11.740]   Like what, what's possible for, uh, for open AI, skip the time window, whether it's three years or five years or seven years.
[00:18:11.740 --> 00:18:18.760]   What's your degree of confidence that given the value that open AI and chat GPT is delivering into the world.
[00:18:18.760 --> 00:18:22.720]   And we haven't even got into really enterprise and everything you're doing with enterprise.
[00:18:22.720 --> 00:18:26.940]   You just announced a big deal with the government, uh, the defense department, et cetera.
[00:18:27.500 --> 00:18:38.180]   What's your level of confidence that you guys will reach a hundred billion of revenue in the future and kind of be that, you know, uh, AI defining company.
[00:18:38.180 --> 00:18:46.780]   Because at a hundred billion in revenue, it seems to me that this company could be on a path to being the most valuable company, uh, in the world, right?
[00:18:46.780 --> 00:18:53.960]   Uh, five to $10 trillion business in the fullness of time, which frankly would not surprise me for the winner in the age of AI, right?
[00:18:53.960 --> 00:18:56.400]   The winner and consumer and a big company and enterprise.
[00:18:56.400 --> 00:18:58.800]   But now you've been here a year, right?
[00:18:58.800 --> 00:19:02.780]   You joined something that you thought was going to be the biggest thing, uh, of your career.
[00:19:02.900 --> 00:19:05.800]   You've been at extraordinary large and successful companies.
[00:19:05.800 --> 00:19:07.260]   What's your level of confidence?
[00:19:07.260 --> 00:19:10.120]   Look, I think the opportunity is all there in front of us.
[00:19:10.120 --> 00:19:14.000]   I've said like three times now how I think AI is going to change everything.
[00:19:14.000 --> 00:19:14.340]   Everything.
[00:19:14.340 --> 00:19:18.660]   Um, and the path is there and it's up to us to execute.
[00:19:18.660 --> 00:19:19.020]   Yeah.
[00:19:19.020 --> 00:19:24.520]   It's why, I mean, this place, open AI moves faster than any company I've ever worked at in my life.
[00:19:24.520 --> 00:19:24.900]   Wow.
[00:19:24.920 --> 00:19:27.200]   And I thought I had worked at places that move pretty fast.
[00:19:27.200 --> 00:19:28.940]   That's a big, that's a big, Twitter, Instagram.
[00:19:28.940 --> 00:19:29.760]   Yeah.
[00:19:29.760 --> 00:19:32.360]   That's a big, but nothing, nothing compares.
[00:19:32.360 --> 00:19:34.840]   I think it's, you know, we have amazing people on the team.
[00:19:34.840 --> 00:19:40.360]   Uh, we really try and, and, uh, push decision-making and responsibility down into the teams.
[00:19:40.360 --> 00:19:45.420]   I think in an AI world, you need to do that because the, the IMLs are so new.
[00:19:45.420 --> 00:19:48.440]   We don't, you don't, you don't top down know all of the capabilities.
[00:19:48.440 --> 00:19:54.380]   You see them kind of coming through the mist and we're better at finding all of the opportunities
[00:19:54.380 --> 00:19:58.900]   when we have a lot of smart people thinking about what they can do with the model in their
[00:19:58.900 --> 00:19:59.240]   area.
[00:19:59.240 --> 00:19:59.600]   Yeah.
[00:19:59.600 --> 00:20:02.160]   And ditto, by the way, publicly, it's why we release early and often.
[00:20:02.160 --> 00:20:05.660]   We try and get it in people's hands because then they also get a chance to figure out what
[00:20:05.660 --> 00:20:06.340]   the models can be.
[00:20:06.340 --> 00:20:09.800]   So I, I have a lot of confidence in us.
[00:20:09.800 --> 00:20:11.160]   We got to execute.
[00:20:11.160 --> 00:20:11.820]   Yeah.
[00:20:11.820 --> 00:20:15.860]   Like we've got to, we've got to be on top of our game because we have a very serious set
[00:20:15.860 --> 00:20:17.760]   of competitors and, um, and that's real.
[00:20:18.000 --> 00:20:23.980]   But, but man, I like, all right, it's also like, I just feel good.
[00:20:23.980 --> 00:20:29.820]   Like when you look at this team, you, Sarah, Fiji, Greg Brockman, Sam, et cetera.
[00:20:29.820 --> 00:20:35.960]   It's such, even though the company's small bill, it's an extraordinarily deep bench of talented
[00:20:35.960 --> 00:20:37.920]   people who've done this in other places.
[00:20:37.920 --> 00:20:42.080]   Not only does it come down to execution as investor, like I of course want them to succeed
[00:20:42.080 --> 00:20:47.800]   as an investor, but in terms of defining the company that's going to do the right things
[00:20:47.800 --> 00:20:53.740]   in this moment, I feel good to have this company in the lead because, you know, these are great
[00:20:53.740 --> 00:20:55.500]   people that I think are going to do the right things.
[00:20:55.500 --> 00:20:57.460]   And so it's great to have you helping lead the lead.
[00:20:57.460 --> 00:20:58.600]   You're nice to say it.
[00:20:58.600 --> 00:21:01.500]   I think, you know, we tend to get unfair.
[00:21:01.500 --> 00:21:05.820]   The people that you named, uh, you know, I'm beyond excited for Fiji to start all the other
[00:21:05.820 --> 00:21:06.360]   people you named.
[00:21:06.360 --> 00:21:08.140]   Like we tend to get unfair amounts of credit.
[00:21:08.140 --> 00:21:13.200]   It's, I could give you a hundred names of the people that are doing the real work and are
[00:21:13.200 --> 00:21:15.480]   like actually the reason that OpenAI is successful.
[00:21:15.480 --> 00:21:19.160]   Um, and you know, hopefully we can tell their stories over time too.
[00:21:19.160 --> 00:21:19.360]   Yeah, we will.
[00:21:19.360 --> 00:21:19.780]   We will.
[00:21:19.780 --> 00:21:20.120]   All right.
[00:21:20.120 --> 00:21:20.820]   Thanks for being here.
[00:21:20.820 --> 00:21:21.540]   Hey man, thanks so much.
[00:21:21.540 --> 00:21:22.100]   Appreciate it.
[00:21:22.300 --> 00:21:36.000]   As a reminder to everybody, just our opinions, not investment advice.

