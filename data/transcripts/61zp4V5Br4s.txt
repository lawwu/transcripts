
[00:00:00.000 --> 00:00:03.060]   I'm Jonathan Cohen from NVIDIA.
[00:00:03.060 --> 00:00:07.620]   My colleague Francesco Cianella is going to run our demo.
[00:00:07.620 --> 00:00:09.060]   Super excited to be here.
[00:00:09.060 --> 00:00:11.140]   Big crowd, awesome.
[00:00:11.140 --> 00:00:12.760]   So we're going to talk about something
[00:00:12.760 --> 00:00:16.020]   that we released recently called Nemo Guardrails, which
[00:00:16.020 --> 00:00:18.420]   is a toolkit for creating guardrails
[00:00:18.420 --> 00:00:21.060]   around your large language model.
[00:00:21.060 --> 00:00:22.380]   So why do you need guardrails?
[00:00:22.380 --> 00:00:26.460]   Well, I think we all are familiar with the difficulty
[00:00:26.460 --> 00:00:29.260]   in building an actual application with large language
[00:00:29.260 --> 00:00:33.840]   models that the public might interact with.
[00:00:33.840 --> 00:00:36.300]   If you're going to actually deploy a large language model,
[00:00:36.300 --> 00:00:38.540]   you somehow need to constrain what it can do.
[00:00:38.540 --> 00:00:40.060]   You need to monitor what it's saying
[00:00:40.060 --> 00:00:44.540]   and what people are saying to it, what it's responding.
[00:00:44.540 --> 00:00:48.420]   And a guardrail then is just what
[00:00:48.420 --> 00:00:50.700]   we call anything that monitors this conversation
[00:00:50.700 --> 00:00:53.460]   and can steer it in one direction or another.
[00:00:53.460 --> 00:00:54.980]   So broadly speaking, we can think
[00:00:54.980 --> 00:00:56.780]   of three kinds of guardrails.
[00:00:56.780 --> 00:00:58.540]   There's topical guardrails that attempt
[00:00:58.540 --> 00:01:02.180]   to keep a conversation on topic or away from certain topics.
[00:01:02.180 --> 00:01:04.780]   So for example, if I have a customer service chat bot,
[00:01:04.780 --> 00:01:07.160]   I don't want it to do anything other than answer questions
[00:01:07.160 --> 00:01:08.020]   about my product.
[00:01:08.020 --> 00:01:09.480]   I don't want it to answer questions
[00:01:09.480 --> 00:01:11.780]   about someone else's product or talk about the weather
[00:01:11.780 --> 00:01:14.020]   or whatever.
[00:01:14.020 --> 00:01:18.260]   There's a variety of topics that people call AI safety,
[00:01:18.260 --> 00:01:21.580]   so things like hallucination or toxicity or misinformation.
[00:01:21.580 --> 00:01:24.180]   And so detecting whether someone is
[00:01:24.180 --> 00:01:28.180]   speaking to the LLM in the wrong way or the LLM's responses
[00:01:28.180 --> 00:01:34.100]   are somehow toxic or adverse to the purposes of your deployed
[00:01:34.100 --> 00:01:35.900]   chat bot.
[00:01:35.900 --> 00:01:39.220]   And finally, with toolkits like LangChain,
[00:01:39.220 --> 00:01:42.020]   we're connecting up these large language models to APIs
[00:01:42.020 --> 00:01:44.260]   and allowing them to actually take action.
[00:01:44.260 --> 00:01:46.420]   And so this can be a very attractive attack surface
[00:01:46.420 --> 00:01:49.060]   for all sorts of cybersecurity threats.
[00:01:49.060 --> 00:01:50.740]   And so you certainly want to monitor
[00:01:50.740 --> 00:01:53.880]   what people are saying to an LLM and then what calls it's making
[00:01:53.880 --> 00:01:55.900]   or what actions it's taking.
[00:01:55.900 --> 00:01:58.300]   And so all of these fit broadly under this category
[00:01:58.300 --> 00:01:59.420]   of guardrails.
[00:01:59.420 --> 00:02:01.100]   And one of the things that we realized
[00:02:01.100 --> 00:02:03.420]   as we started trying to build a guardrail system
[00:02:03.420 --> 00:02:05.700]   is that it's really a topic--
[00:02:05.700 --> 00:02:08.260]   sorry, a dialogue modeling problem.
[00:02:08.260 --> 00:02:11.580]   So you need to understand who's saying what,
[00:02:11.580 --> 00:02:14.200]   what's the context of the conversation, what's
[00:02:14.200 --> 00:02:16.400]   the history of the conversation, what's
[00:02:16.400 --> 00:02:18.300]   the purpose of any given interaction.
[00:02:18.300 --> 00:02:20.420]   And you're actually monitoring the conversation
[00:02:20.420 --> 00:02:23.540]   both from the human and the chat bot itself.
[00:02:23.540 --> 00:02:25.940]   So this is like a very simple system architecture.
[00:02:25.940 --> 00:02:28.340]   So you have a person talking to--
[00:02:28.340 --> 00:02:31.260]   and more and more, it's not just directly to a language model,
[00:02:31.260 --> 00:02:33.500]   but to some application like LangChain
[00:02:33.500 --> 00:02:36.020]   that's then interfacing with different language models.
[00:02:36.020 --> 00:02:38.680]   And Nemo Guardrails sits in between the person
[00:02:38.680 --> 00:02:40.100]   and this application.
[00:02:40.100 --> 00:02:43.860]   And so it's using this powerful contextual dialogue engine
[00:02:43.860 --> 00:02:45.420]   and monitoring the conversation.
[00:02:45.420 --> 00:02:50.540]   And then it can apply guardrails and decide what to do.
[00:02:50.540 --> 00:02:52.420]   It's very difficult to build one system that
[00:02:52.420 --> 00:02:55.820]   can anticipate all the things that may happen.
[00:02:55.820 --> 00:02:57.980]   And in fact, the way you deploy your language model
[00:02:57.980 --> 00:03:00.560]   might be very different from how someone else is deploying it.
[00:03:00.560 --> 00:03:03.660]   And so Guardrails is actually a fully programmable toolkit.
[00:03:03.660 --> 00:03:05.940]   There's a domain-specific language.
[00:03:05.940 --> 00:03:07.780]   It's basically a dialogue modeling language.
[00:03:07.780 --> 00:03:09.780]   And we'll show you some examples of that.
[00:03:09.780 --> 00:03:11.820]   And so you can use this to describe
[00:03:11.820 --> 00:03:14.180]   what are the interactions and what are the guardrails you
[00:03:14.180 --> 00:03:14.380]   want.
[00:03:14.380 --> 00:03:19.340]   And everything is fully customizable and programmable.
[00:03:19.340 --> 00:03:20.740]   It's open source.
[00:03:20.740 --> 00:03:23.300]   So we released this fully as open source.
[00:03:23.300 --> 00:03:24.100]   It's on GitHub.
[00:03:24.100 --> 00:03:26.100]   It's integrated into the Nemo framework, which
[00:03:26.100 --> 00:03:29.420]   is an open source framework we've had for several years,
[00:03:29.420 --> 00:03:32.760]   which is a PyTorch-based toolkit for training and customizing
[00:03:32.760 --> 00:03:36.000]   language models, speech models, generative AI
[00:03:36.000 --> 00:03:38.140]   models of all sorts.
[00:03:38.140 --> 00:03:40.500]   It will be part of the NVIDIA Nemo service,
[00:03:40.500 --> 00:03:43.420]   but we're not going to really talk about that.
[00:03:43.420 --> 00:03:46.540]   The reason we open source this is we certainly
[00:03:46.540 --> 00:03:48.380]   are not in any way claiming that we've solved
[00:03:48.380 --> 00:03:50.420]   the problem of detecting hallucinations
[00:03:50.420 --> 00:03:51.820]   or preventing toxicity.
[00:03:51.820 --> 00:03:54.620]   These are very hard problems that people will
[00:03:54.620 --> 00:03:56.300]   be working on for a long time.
[00:03:56.300 --> 00:03:58.940]   But we wanted to create some kind of repository
[00:03:58.940 --> 00:04:00.980]   for all the work people are doing in this area
[00:04:00.980 --> 00:04:03.300]   to go into one place.
[00:04:03.300 --> 00:04:05.380]   I think a fragmented world is a lot more
[00:04:05.380 --> 00:04:08.220]   difficult to aggregate all the best practices
[00:04:08.220 --> 00:04:10.060]   and actually deploy them in production.
[00:04:10.060 --> 00:04:12.600]   And so our intention by open sourcing this was,
[00:04:12.600 --> 00:04:14.340]   this is an area we're doing research in,
[00:04:14.340 --> 00:04:16.220]   and so we wanted a place to put our research.
[00:04:16.220 --> 00:04:17.980]   But we're also hoping that the community
[00:04:17.980 --> 00:04:20.180]   finds this toolkit useful and the framework
[00:04:20.180 --> 00:04:23.260]   that we've built useful and adds their own implementations
[00:04:23.260 --> 00:04:25.700]   and their own types of guardrails into it.
[00:04:25.700 --> 00:04:28.580]   It's all built on Lang chain, so the implementation
[00:04:28.580 --> 00:04:31.260]   of a guardrail and even the way you interface
[00:04:31.260 --> 00:04:34.300]   to the underlying language model is all based on Lang chain.
[00:04:34.300 --> 00:04:36.740]   So it's very easy to integrate new ideas or new models
[00:04:36.740 --> 00:04:38.300]   into it.
[00:04:38.300 --> 00:04:40.420]   So with that, we're going to show you a live demo.
[00:04:40.420 --> 00:04:44.660]   Hopefully you can read the screen here.
[00:04:44.660 --> 00:04:49.540]   [VIDEO PLAYBACK]
[00:04:49.540 --> 00:04:51.980]   So we have a simple server that's just-- we implemented
[00:04:51.980 --> 00:04:55.140]   a very simple chatbot.
[00:04:55.140 --> 00:04:55.980]   Can people read that?
[00:04:55.980 --> 00:04:57.180]   I hope.
[00:04:57.180 --> 00:04:58.780]   Yeah.
[00:04:58.780 --> 00:04:59.620]   Should we zoom in?
[00:04:59.620 --> 00:05:01.780]   It's OK?
[00:05:01.780 --> 00:05:04.140]   That's good.
[00:05:04.140 --> 00:05:07.260]   So we implemented a simple chatbot.
[00:05:07.260 --> 00:05:08.640]   The language model under the hood
[00:05:08.640 --> 00:05:11.180]   is Text DaVinci 3 from OpenAI.
[00:05:11.180 --> 00:05:13.660]   And we connected it up to a knowledge base, which
[00:05:13.660 --> 00:05:17.860]   has documentation about NVIDIA's HR benefits.
[00:05:17.860 --> 00:05:19.740]   And so the intention of this chatbot
[00:05:19.740 --> 00:05:21.980]   is that it should answer questions about NVIDIA HR
[00:05:21.980 --> 00:05:22.780]   benefits.
[00:05:22.780 --> 00:05:25.340]   And along the way, we want to avoid different pitfalls.
[00:05:25.340 --> 00:05:28.540]   So we can just start out asking it, what can you do?
[00:05:28.540 --> 00:05:32.820]   And it will respond, assuming our internet connection is
[00:05:32.820 --> 00:05:35.620]   working.
[00:05:35.620 --> 00:05:36.260]   Yes.
[00:05:36.260 --> 00:05:38.620]   As an AI, I can provide you with the right range of services,
[00:05:38.620 --> 00:05:40.380]   answering queries, offering guidance,
[00:05:40.380 --> 00:05:43.260]   providing relevant information about NVIDIA benefits.
[00:05:43.260 --> 00:05:46.620]   So this actually is defined in the way
[00:05:46.620 --> 00:05:48.000]   that we specified our guardrails.
[00:05:48.000 --> 00:05:50.140]   We told this chatbot what it can do.
[00:05:50.140 --> 00:05:52.020]   And if someone asks it what it can do,
[00:05:52.020 --> 00:05:54.100]   we're not even necessarily sending this
[00:05:54.100 --> 00:05:56.580]   to the language model itself.
[00:05:56.580 --> 00:05:57.820]   We just know the answer.
[00:05:57.820 --> 00:05:59.380]   So now let's ask it a question that it
[00:05:59.380 --> 00:06:00.660]   should be able to answer.
[00:06:00.660 --> 00:06:02.120]   We're going to be adopting a child.
[00:06:02.120 --> 00:06:04.860]   Can I claim any of the adoption expenses?
[00:06:04.860 --> 00:06:06.840]   The guardrail system here is recognizing
[00:06:06.840 --> 00:06:09.060]   that this is a factual question.
[00:06:09.060 --> 00:06:11.380]   So it's going to send this query to the knowledge base,
[00:06:11.380 --> 00:06:13.900]   get back some hopefully relevant information,
[00:06:13.900 --> 00:06:17.300]   and then pass that along with the query to Text DaVinci 3,
[00:06:17.300 --> 00:06:19.740]   or the language model, to formulate a response.
[00:06:19.740 --> 00:06:21.780]   And so now we have a relatively detailed response
[00:06:21.780 --> 00:06:23.900]   written by Text DaVinci 3 based on the information
[00:06:23.900 --> 00:06:25.500]   in our knowledge base.
[00:06:25.500 --> 00:06:28.420]   Yes, NVIDIA-- sorry, jump ahead of me.
[00:06:28.420 --> 00:06:30.700]   Yes, NVIDIA offers adoption assistance program,
[00:06:30.700 --> 00:06:33.020]   which can provide unlimited reimbursement, and so on.
[00:06:33.020 --> 00:06:35.620]   So this is a factual answer.
[00:06:35.620 --> 00:06:39.220]   So let's ask it a question that we don't want it to answer.
[00:06:39.220 --> 00:06:42.180]   So I can ask it something like, how many NVIDIANS
[00:06:42.180 --> 00:06:43.660]   adopted children last year?
[00:06:43.660 --> 00:06:45.700]   Now, we probably don't even have this information
[00:06:45.700 --> 00:06:46.620]   in our knowledge base.
[00:06:46.620 --> 00:06:48.980]   But even if somehow this system had access to that,
[00:06:48.980 --> 00:06:51.580]   we wouldn't want to answer because it's confidential.
[00:06:51.580 --> 00:06:53.240]   And so our guardrail system is actually
[00:06:53.240 --> 00:06:56.540]   detecting that someone asked a confidential question.
[00:06:56.540 --> 00:06:58.780]   And it's intercepting that question.
[00:06:58.780 --> 00:07:01.220]   It's not even sending it to the language model.
[00:07:01.220 --> 00:07:03.560]   It's simply responding with this canned response.
[00:07:03.560 --> 00:07:05.180]   I'm sorry, I'm not able to respond
[00:07:05.180 --> 00:07:07.500]   because that's confidential information.
[00:07:07.500 --> 00:07:10.560]   We can ask it something that it doesn't know.
[00:07:10.560 --> 00:07:12.260]   So we built this knowledge base.
[00:07:12.260 --> 00:07:15.300]   And I know that this next question,
[00:07:15.300 --> 00:07:17.600]   is there a support group for parents of adopted children?
[00:07:17.600 --> 00:07:20.140]   There's no information relevant to that in the knowledge base
[00:07:20.140 --> 00:07:21.340]   that it has access to.
[00:07:21.340 --> 00:07:24.300]   And what we don't want it to do is hallucinate an answer.
[00:07:24.300 --> 00:07:26.480]   And so again, we recognize it's a factual question.
[00:07:26.480 --> 00:07:27.620]   We look it up in our knowledge base.
[00:07:27.620 --> 00:07:29.860]   We allow the language model to formulate a response.
[00:07:29.860 --> 00:07:32.140]   But we're applying a guardrail that's
[00:07:32.140 --> 00:07:34.480]   checking for the groundedness of any response.
[00:07:34.480 --> 00:07:36.780]   And so it's looking at, what did the language model say
[00:07:36.780 --> 00:07:38.660]   and what information did we have in our knowledge base
[00:07:38.660 --> 00:07:40.060]   and are these things consistent?
[00:07:40.060 --> 00:07:42.420]   And if not, it's actually filtering the language model's
[00:07:42.420 --> 00:07:44.780]   response and replying again with this canned response.
[00:07:44.780 --> 00:07:45.700]   I'm not sure.
[00:07:45.700 --> 00:07:48.200]   I recommend you talk to the HR representative or your manager
[00:07:48.200 --> 00:07:48.740]   for support.
[00:07:48.740 --> 00:07:50.500]   All of this logic is, again, fully
[00:07:50.500 --> 00:07:52.780]   programmable in this programming language
[00:07:52.780 --> 00:07:54.260]   that we've developed.
[00:07:54.260 --> 00:07:58.520]   So let's ask it another question.
[00:07:58.520 --> 00:08:00.460]   I'm talking to an NVIDIA chat bot.
[00:08:00.460 --> 00:08:02.000]   I can ask it questions about NVIDIA.
[00:08:02.000 --> 00:08:04.020]   So let me ask it a financial question.
[00:08:04.020 --> 00:08:07.460]   What was NVIDIA's operating income in 2021 Q3?
[00:08:07.460 --> 00:08:09.900]   Now, I have no idea if it knows the answer to this.
[00:08:09.900 --> 00:08:12.300]   I don't know if text DaVinci 3 knows this.
[00:08:12.300 --> 00:08:14.420]   But all I know is it's not relevant.
[00:08:14.420 --> 00:08:16.940]   So as we've built this chat bot today,
[00:08:16.940 --> 00:08:19.320]   we do not yet have a guardrail.
[00:08:19.320 --> 00:08:20.820]   It's going to pass this question on.
[00:08:20.820 --> 00:08:22.400]   It recognizes it's a factual question.
[00:08:22.400 --> 00:08:24.440]   It'll allow the language model to respond.
[00:08:24.440 --> 00:08:25.820]   And I don't know if that's true.
[00:08:25.820 --> 00:08:26.860]   I don't know if that's false.
[00:08:26.860 --> 00:08:28.100]   I don't know if it hallucinated that.
[00:08:28.100 --> 00:08:30.700]   But I definitely don't want it to answer questions like this.
[00:08:30.700 --> 00:08:33.260]   So let's show you the actual program that's
[00:08:33.260 --> 00:08:36.500]   defining these guardrails.
[00:08:36.500 --> 00:08:38.540]   Maybe we can-- yeah, sorry.
[00:08:38.540 --> 00:08:40.740]   Maybe we zoom out a little bit or close the--
[00:08:40.740 --> 00:08:43.620]   [CHUCKLES]
[00:08:43.620 --> 00:08:47.100]   There we go.
[00:08:47.100 --> 00:08:49.260]   So this programming language, it's
[00:08:49.260 --> 00:08:51.140]   a domain-specific language called
[00:08:51.140 --> 00:08:53.300]   Colang, which we developed.
[00:08:53.300 --> 00:08:55.060]   And the guardrail system has an interpreter
[00:08:55.060 --> 00:08:56.580]   and a runtime for this language.
[00:08:56.580 --> 00:08:58.300]   And it's a dialogue modeling language.
[00:08:58.300 --> 00:08:59.500]   It's actually a pretty rich language.
[00:08:59.500 --> 00:09:01.380]   You can have conditionals and control flow
[00:09:01.380 --> 00:09:03.180]   and all sorts of things.
[00:09:03.180 --> 00:09:04.940]   But it has two fundamental concepts.
[00:09:04.940 --> 00:09:07.740]   One is something that we call a canonical form.
[00:09:07.740 --> 00:09:11.820]   And a canonical form is simply a simplified paraphrase
[00:09:11.820 --> 00:09:13.100]   of a concept.
[00:09:13.100 --> 00:09:15.020]   And so you can define a canonical form.
[00:09:15.020 --> 00:09:16.780]   And then you use these canonical forms
[00:09:16.780 --> 00:09:20.140]   as the hook for your guardrail to trigger the logic
[00:09:20.140 --> 00:09:21.900]   in your guardrail flows.
[00:09:21.900 --> 00:09:23.900]   So we're going to define a user's canonical form.
[00:09:23.900 --> 00:09:25.020]   Define user.
[00:09:25.020 --> 00:09:27.140]   Ask about financial results.
[00:09:27.140 --> 00:09:28.500]   So that's a canonical form.
[00:09:28.500 --> 00:09:29.740]   Ask about financial results.
[00:09:29.740 --> 00:09:31.320]   And we can give it a bunch of examples.
[00:09:31.320 --> 00:09:32.820]   You can give it as many as you want.
[00:09:32.820 --> 00:09:34.980]   The more examples you give it, the more accurately it
[00:09:34.980 --> 00:09:37.660]   will detect that this is relevant.
[00:09:37.660 --> 00:09:40.580]   But if you just want to build something as a proof of concept,
[00:09:40.580 --> 00:09:42.740]   you really only need to give it one or two examples.
[00:09:42.740 --> 00:09:45.020]   And it understands what's happening.
[00:09:45.020 --> 00:09:46.620]   So what was NVIDIA's EPS last year?
[00:09:46.620 --> 00:09:48.140]   How much does NVIDIA spend on R&D?
[00:09:48.140 --> 00:09:49.860]   Examples of this canonical form.
[00:09:49.860 --> 00:09:53.260]   We can also define canonical forms for the output.
[00:09:53.260 --> 00:09:55.940]   So in this case, we're defining a bot form.
[00:09:55.940 --> 00:09:57.740]   Explain can't discuss financial results.
[00:09:57.740 --> 00:09:58.240]   I'm sorry.
[00:09:58.240 --> 00:10:00.180]   I'm not a financial bot.
[00:10:00.180 --> 00:10:01.920]   So now that we have these financial forms,
[00:10:01.920 --> 00:10:05.220]   we can find, in this case, a very simple flow.
[00:10:05.220 --> 00:10:09.260]   And a guardrail is a flow in this language.
[00:10:09.260 --> 00:10:11.280]   So we're going to define a flow, which
[00:10:11.280 --> 00:10:13.840]   is if the user asks about financial results,
[00:10:13.840 --> 00:10:16.140]   the bot will explain it can't discuss financial results.
[00:10:16.140 --> 00:10:18.740]   And then further, the bot will inform of its capabilities,
[00:10:18.740 --> 00:10:22.060]   which is a canonical form we've already defined in this file.
[00:10:22.060 --> 00:10:24.100]   So with that, we've now created a new guardrail.
[00:10:24.100 --> 00:10:26.060]   And you can see what's nice about this language
[00:10:26.060 --> 00:10:29.420]   is it's very, very natural language-like.
[00:10:29.420 --> 00:10:30.940]   Again, it is a programming language.
[00:10:30.940 --> 00:10:33.880]   It does have rigorous definition and flow of control.
[00:10:33.880 --> 00:10:35.940]   But these conditionals and these predicates
[00:10:35.940 --> 00:10:38.520]   for what do you do when and what gets triggered
[00:10:38.520 --> 00:10:42.240]   is all based on this very natural way of framing things.
[00:10:42.240 --> 00:10:43.880]   So we can restart our server.
[00:10:43.880 --> 00:10:45.480]   And we can ask it this question again.
[00:10:45.480 --> 00:10:50.440]   And now, hopefully, that guardrail will get triggered.
[00:10:50.440 --> 00:10:52.100]   It will detect that this was a question
[00:10:52.100 --> 00:10:53.720]   about financial results.
[00:10:53.720 --> 00:10:57.760]   And it will respond with, again, our canned response.
[00:10:57.760 --> 00:10:58.240]   I'm sorry.
[00:10:58.240 --> 00:10:59.120]   I'm not a financial bot.
[00:10:59.120 --> 00:11:01.580]   As an AI, I can provide you with a wide range of services
[00:11:01.580 --> 00:11:02.920]   and so on.
[00:11:02.920 --> 00:11:04.640]   And so this is an example of guardrails
[00:11:04.640 --> 00:11:06.800]   and how easy it is to build your own.
[00:11:06.800 --> 00:11:09.320]   My expectation is that if someone were to really use this
[00:11:09.320 --> 00:11:12.080]   in a real system, you would actually do a lot of R&D,
[00:11:12.080 --> 00:11:14.160]   a lot of development, a lot of iterations,
[00:11:14.160 --> 00:11:18.400]   testing it under all sorts of adversarial conditions, which
[00:11:18.400 --> 00:11:20.800]   is, in fact, one reason that it's interesting to talk
[00:11:20.800 --> 00:11:22.920]   at Weights & Biases.
[00:11:22.920 --> 00:11:24.680]   You definitely need this kind of a platform
[00:11:24.680 --> 00:11:26.920]   for testing and iterating and running experiments.
[00:11:26.920 --> 00:11:30.320]   And I think if you think of the system holistically,
[00:11:30.320 --> 00:11:32.320]   not just the model, but you have your model,
[00:11:32.320 --> 00:11:34.840]   you have your guardrails, you have all these components,
[00:11:34.840 --> 00:11:36.580]   you actually need to test that whole thing
[00:11:36.580 --> 00:11:38.920]   and run experiments on the complete system.
[00:11:38.920 --> 00:11:40.120]   If we go back to the slides.
[00:11:40.120 --> 00:11:44.200]   Oops.
[00:11:44.200 --> 00:11:46.280]   Slide show.
[00:11:46.280 --> 00:11:46.880]   Great.
[00:11:46.880 --> 00:11:53.200]   So in our open source, the open source toolkit on GitHub,
[00:11:53.200 --> 00:11:56.000]   we have examples of a number of different guardrails.
[00:11:56.000 --> 00:11:59.160]   Again, I'm not claiming we've solved any of these problems
[00:11:59.160 --> 00:12:00.120]   particularly.
[00:12:00.120 --> 00:12:02.880]   A lot of them are implemented actually using language models
[00:12:02.880 --> 00:12:05.640]   to check if different conditions are met.
[00:12:05.640 --> 00:12:07.600]   But under the hood, like I said, it's
[00:12:07.600 --> 00:12:09.200]   able to call anything from Lang chain.
[00:12:09.200 --> 00:12:11.640]   So any model or any logic you can implement in Lang chain,
[00:12:11.640 --> 00:12:13.600]   you can actually use as a guardrail.
[00:12:13.600 --> 00:12:16.860]   And I think if you wanted to build a very robust system,
[00:12:16.860 --> 00:12:19.720]   in fact, you'd probably have very complicated, many-layered
[00:12:19.720 --> 00:12:23.000]   kinds of checks, and it could be quite computationally expensive.
[00:12:23.000 --> 00:12:24.700]   And so it's kind of an interesting case
[00:12:24.700 --> 00:12:27.120]   where you can have this trade-off between lots and lots
[00:12:27.120 --> 00:12:30.520]   of computation to have a really, really rock-solid system
[00:12:30.520 --> 00:12:33.400]   or something maybe that's cheaper and more flexible
[00:12:33.400 --> 00:12:35.440]   and maybe not as secure.
[00:12:35.440 --> 00:12:37.440]   So as I said in the beginning, there's broadly
[00:12:37.440 --> 00:12:38.520]   three kinds of guardrails.
[00:12:38.520 --> 00:12:39.900]   And we have examples of all these
[00:12:39.900 --> 00:12:43.120]   in kind of a cookbook in our code base.
[00:12:43.120 --> 00:12:44.640]   Topical guardrails, there's examples
[00:12:44.640 --> 00:12:47.080]   of like querying a knowledge base, like I showed you here,
[00:12:47.080 --> 00:12:48.320]   staying on topic.
[00:12:48.320 --> 00:12:50.640]   You can use these canonical forms
[00:12:50.640 --> 00:12:52.800]   to set a conversational tone, detect,
[00:12:52.800 --> 00:12:54.600]   notice that someone told a joke or someone
[00:12:54.600 --> 00:12:57.540]   said something in a sarcastic manner.
[00:12:57.540 --> 00:12:59.520]   Safety guardrails, so there's examples
[00:12:59.520 --> 00:13:02.080]   of ethical response, fact-checking guardrails,
[00:13:02.080 --> 00:13:03.980]   hallucination detection.
[00:13:03.980 --> 00:13:08.680]   And then for security, we designed a simple security
[00:13:08.680 --> 00:13:09.440]   model.
[00:13:09.440 --> 00:13:11.240]   And so we can do things like allow lists,
[00:13:11.240 --> 00:13:15.880]   certain APIs from LangChain that conform to our security model.
[00:13:15.880 --> 00:13:19.100]   We have examples of how to do safe execution of code,
[00:13:19.100 --> 00:13:21.140]   and then things like detecting jailbreak attempts.
[00:13:21.140 --> 00:13:24.440]   And I think this area is going to be a really fast-moving
[00:13:24.440 --> 00:13:26.400]   area in terms of computer security research,
[00:13:26.400 --> 00:13:29.800]   detecting and mitigating these kinds of responses.
[00:13:29.800 --> 00:13:32.240]   So I think that's all we have today.
[00:13:32.240 --> 00:13:33.900]   Thank you very much for your attention,
[00:13:33.900 --> 00:13:35.120]   and have a great conference.
[00:13:35.120 --> 00:13:37.120]   [APPLAUSE]
[00:13:37.120 --> 00:13:40.480]   [MUSIC PLAYING]
[00:13:40.480 --> 00:13:50.480]   [BLANK_AUDIO]

