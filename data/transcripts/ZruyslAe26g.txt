
[00:00:00.000 --> 00:00:02.080]   "Go ahead and run your experiments in Iceland.
[00:00:02.080 --> 00:00:03.960]   Let's run that for 50 years and see what happens."
[00:00:03.960 --> 00:00:05.420]   It's weird how everybody's obsessed
[00:00:05.420 --> 00:00:07.560]   with running the experiment in America, right?
[00:00:07.560 --> 00:00:08.720]   They'll balance the long run budget
[00:00:08.720 --> 00:00:11.040]   on the kind of the backs of the poor and the middle class.
[00:00:11.040 --> 00:00:12.900]   Anything that lowers the innovation
[00:00:12.900 --> 00:00:14.920]   in the world's most innovative countries
[00:00:14.920 --> 00:00:17.800]   has negative costs for the entire planet in the long run.
[00:00:17.800 --> 00:00:18.800]   But that's something you'd only see
[00:00:18.800 --> 00:00:20.720]   over the course of 20, 30, 50 years.
[00:00:20.720 --> 00:00:22.740]   And libertarians and open border advocates
[00:00:22.740 --> 00:00:24.800]   are very rarely interested in that kind of timeframe.
[00:00:24.800 --> 00:00:26.160]   But it's worth thinking through why it is
[00:00:26.160 --> 00:00:28.260]   that the successful so-called monarchies
[00:00:28.260 --> 00:00:29.800]   aren't really monarchies, right?
[00:00:29.800 --> 00:00:30.920]   They're really oligarchies.
[00:00:30.920 --> 00:00:34.080]   We should presume that the average skill level of voters,
[00:00:34.080 --> 00:00:38.080]   the average traits that we're bringing from our ancestors
[00:00:38.080 --> 00:00:40.480]   are having an effect on our current productivity
[00:00:40.480 --> 00:00:41.320]   for good and real.
[00:00:41.320 --> 00:00:43.760]   Okay, today I have the pleasure of speaking
[00:00:43.760 --> 00:00:46.320]   with Garrett Jones, who is an economist
[00:00:46.320 --> 00:00:48.240]   at George Mason University.
[00:00:48.240 --> 00:00:52.800]   He's most recently the author of "The Cultural Transplant,"
[00:00:52.800 --> 00:00:55.840]   how migrants make the economies they move to
[00:00:55.840 --> 00:00:57.960]   a lot like the ones they left.
[00:00:57.960 --> 00:01:01.400]   But he's also the author of "10% Less Democracy"
[00:01:01.400 --> 00:01:02.380]   and "Hive Mind."
[00:01:02.380 --> 00:01:04.560]   We'll get into all three of those books.
[00:01:04.560 --> 00:01:06.600]   Garrett, welcome to the podcast.
[00:01:06.600 --> 00:01:08.500]   - Glad to be here, thanks for having me.
[00:01:08.500 --> 00:01:12.640]   - First question, isn't "The Cultural Transplant"
[00:01:12.640 --> 00:01:16.440]   still a continuation of your argument against democracy?
[00:01:16.440 --> 00:01:18.920]   Because isn't one of the reasons we care
[00:01:18.920 --> 00:01:20.360]   about the values of migrants
[00:01:20.360 --> 00:01:21.840]   the fact that we live in a democracy?
[00:01:21.840 --> 00:01:23.320]   So should we view this book
[00:01:23.320 --> 00:01:25.440]   as part of your critique against democracy
[00:01:25.440 --> 00:01:27.600]   rather than against migration specifically?
[00:01:28.400 --> 00:01:32.800]   - Well, I do think that governments and productivity
[00:01:32.800 --> 00:01:36.360]   are shaped by the citizens in a nation
[00:01:36.360 --> 00:01:37.860]   in almost any event.
[00:01:37.860 --> 00:01:42.000]   I think that even as we've seen recently in China,
[00:01:42.000 --> 00:01:45.640]   even in a very strong authoritarian dictatorship,
[00:01:45.640 --> 00:01:47.440]   which some would call totalitarian,
[00:01:47.440 --> 00:01:49.960]   even there the government has to listen to the masses.
[00:01:49.960 --> 00:01:51.600]   So the government can only get so far away
[00:01:51.600 --> 00:01:55.680]   from the masses on average, even in an autocracy.
[00:01:56.860 --> 00:01:59.460]   - If you had to split apart the contribution though,
[00:01:59.460 --> 00:02:03.280]   the impact of migrants on let's say the culture
[00:02:03.280 --> 00:02:05.480]   versus the impact that migrants have on a country
[00:02:05.480 --> 00:02:07.800]   by voting in their political system,
[00:02:07.800 --> 00:02:11.020]   how would you split that apart?
[00:02:11.020 --> 00:02:13.440]   Is mainly the impact, the cultural impact we see
[00:02:13.440 --> 00:02:16.760]   for migration due to the ability of migrants to vote
[00:02:16.760 --> 00:02:19.000]   or because they're just influencing the culture
[00:02:19.000 --> 00:02:20.600]   just by being there?
[00:02:20.600 --> 00:02:21.440]   - I'll cheat a little bit
[00:02:21.440 --> 00:02:23.080]   because we don't get to run experiments on this.
[00:02:23.080 --> 00:02:26.040]   So I just have to kind of guess, make an informed guess.
[00:02:26.040 --> 00:02:27.420]   I'm gonna call it 50/50.
[00:02:27.420 --> 00:02:32.160]   So the way people, the way citizens influence a country
[00:02:32.160 --> 00:02:35.320]   through formal democracy is important,
[00:02:35.320 --> 00:02:37.920]   but citizens end up placing some kind of limits
[00:02:37.920 --> 00:02:39.120]   on the government anyway.
[00:02:39.120 --> 00:02:42.240]   And the people in a country are the,
[00:02:42.240 --> 00:02:44.160]   they're the folks who are gonna work in the firms
[00:02:44.160 --> 00:02:47.680]   and be able to either establish or not establish
[00:02:47.680 --> 00:02:50.160]   those complicated networks of exchange
[00:02:50.160 --> 00:02:52.480]   that are crucial to high productivity.
[00:02:52.480 --> 00:02:54.480]   - I wanna linger on hive mind a little bit
[00:02:54.480 --> 00:02:56.720]   before we talk about the cultural transplant.
[00:02:56.720 --> 00:03:00.980]   If you had to guess, do the benefits of national IQ
[00:03:00.980 --> 00:03:05.480]   come from having a right tail of elites that is smarter,
[00:03:05.480 --> 00:03:08.160]   or is it from not having that strong of a left tail
[00:03:08.160 --> 00:03:10.280]   of people who are low productivity,
[00:03:10.280 --> 00:03:12.700]   more likely to commit crimes and things like that?
[00:03:12.700 --> 00:03:14.620]   In other words, yeah, go ahead.
[00:03:14.620 --> 00:03:18.000]   - Yeah, I think the upper tail is gonna matter more
[00:03:18.000 --> 00:03:21.600]   than the lower tail in the normal range of variation.
[00:03:22.780 --> 00:03:27.480]   And I think part of that is because nations,
[00:03:27.480 --> 00:03:31.340]   at least moderately prosperous nations have found tools
[00:03:31.340 --> 00:03:33.960]   for basically reducing the influence
[00:03:33.960 --> 00:03:35.780]   of the least informed voters
[00:03:35.780 --> 00:03:39.520]   and for basically being able to keep productivity up
[00:03:39.520 --> 00:03:42.460]   even when there are folks who are sort of disrupting
[00:03:42.460 --> 00:03:43.380]   the whole process.
[00:03:43.380 --> 00:03:48.580]   You know, the risks of crime from the lower end
[00:03:48.580 --> 00:03:50.400]   is basically like a probabilistic risk.
[00:03:50.400 --> 00:03:55.140]   It's not like some zero to one switch or anything.
[00:03:55.140 --> 00:03:56.620]   So we're talking about something probabilistic.
[00:03:56.620 --> 00:04:01.620]   And I think that it's the median versus the elite
[00:04:01.620 --> 00:04:04.280]   is the contrast that I find more interesting.
[00:04:04.280 --> 00:04:07.420]   Median voter theorem, you know,
[00:04:07.420 --> 00:04:09.980]   normal the way we often think about democracy
[00:04:09.980 --> 00:04:12.340]   says that the median should matter more
[00:04:12.340 --> 00:04:16.540]   for determining productivity and for shaping institutions.
[00:04:16.540 --> 00:04:20.360]   And I tend to think that that's more important
[00:04:20.360 --> 00:04:22.260]   in democracies for sure.
[00:04:22.260 --> 00:04:23.520]   So when we look at countries,
[00:04:23.520 --> 00:04:24.640]   if you just look at a scatterplot,
[00:04:24.640 --> 00:04:26.720]   just look at the raw data of a scatterplot.
[00:04:26.720 --> 00:04:28.520]   If you look at the few countries that are exceptions
[00:04:28.520 --> 00:04:31.600]   to the rule where the mean IQ
[00:04:31.600 --> 00:04:33.000]   is the best predictor of productivity
[00:04:33.000 --> 00:04:34.140]   compared to elite IQ,
[00:04:34.140 --> 00:04:41.080]   the exceptions are non-democracies and South Africa.
[00:04:41.080 --> 00:04:44.200]   So you see a few places in the Gulf
[00:04:44.200 --> 00:04:46.100]   where there are large migrant communities
[00:04:46.100 --> 00:04:48.240]   who are exceptionally well-educated,
[00:04:48.240 --> 00:04:50.340]   exceptionally cognitively talented,
[00:04:50.340 --> 00:04:53.240]   and that's associated with high productivity.
[00:04:53.240 --> 00:04:54.420]   Those are a couple of Gulf States.
[00:04:54.420 --> 00:04:56.160]   It's probably Qatar, the UAE,
[00:04:56.160 --> 00:04:58.080]   might be Bahrain in there, I'm not sure.
[00:04:58.080 --> 00:04:59.620]   And then you've got South Africa.
[00:04:59.620 --> 00:05:03.640]   Those are the countries where the average test score,
[00:05:03.640 --> 00:05:04.480]   it doesn't have to be IQ.
[00:05:04.480 --> 00:05:07.360]   It could be just PISA, TIMS type stuff.
[00:05:07.360 --> 00:05:08.680]   Those are the exceptions to the rule
[00:05:08.680 --> 00:05:11.320]   that the average IQ, the mean IQ
[00:05:11.320 --> 00:05:13.680]   is the best predictor of national productivity.
[00:05:15.600 --> 00:05:17.080]   - Interesting.
[00:05:17.080 --> 00:05:19.220]   Does that imply the fact that the,
[00:05:19.220 --> 00:05:21.500]   at least in certain contexts,
[00:05:21.500 --> 00:05:25.160]   the elite IQ matters more than the left tail.
[00:05:25.160 --> 00:05:26.360]   Does that imply that we should want
[00:05:26.360 --> 00:05:30.240]   a greater deviation of IQ in a country?
[00:05:30.240 --> 00:05:32.280]   That you just push a button and increase the deviation.
[00:05:32.280 --> 00:05:33.120]   Would that be good?
[00:05:33.120 --> 00:05:34.940]   - No, no, I don't think so.
[00:05:34.940 --> 00:05:38.040]   Oh, if you could just increase the deviation,
[00:05:38.040 --> 00:05:39.800]   holding the mean constant.
[00:05:39.800 --> 00:05:41.680]   Yeah, I think so, in the normal range of variation.
[00:05:41.680 --> 00:05:42.520]   Yeah, yeah, yeah.
[00:05:44.300 --> 00:05:47.020]   And I think that it has more effects.
[00:05:47.020 --> 00:05:49.780]   It's people at the top who tend to be coming up
[00:05:49.780 --> 00:05:52.140]   with the big breakthrough,
[00:05:52.140 --> 00:05:53.380]   the big scientific breakthroughs,
[00:05:53.380 --> 00:05:54.660]   the big intellectual breakthroughs
[00:05:54.660 --> 00:05:56.460]   that end up spilling over to the whole world.
[00:05:56.460 --> 00:05:59.560]   Basically, the positive externalities of innovation.
[00:05:59.560 --> 00:06:02.500]   This is a very, almost Pollyanna-ish,
[00:06:02.500 --> 00:06:06.280]   Paul Romer, new endogenous, new growth theory thing, right?
[00:06:06.280 --> 00:06:08.660]   Which is the innovations of the elite
[00:06:08.660 --> 00:06:13.660]   just swamp the negatives of the lowest guild among us.
[00:06:14.540 --> 00:06:17.580]   - Can we just apply this line of reasoning
[00:06:17.580 --> 00:06:19.620]   to low-skilled immigration as well, then?
[00:06:19.620 --> 00:06:21.580]   That maybe the average goes down,
[00:06:21.580 --> 00:06:23.540]   the average IQ of your country goes down
[00:06:23.540 --> 00:06:27.100]   if you just let in millions of low-skilled immigrants,
[00:06:27.100 --> 00:06:29.180]   and maybe there's some cultural effects to that, too.
[00:06:29.180 --> 00:06:32.180]   But you're also going to,
[00:06:32.180 --> 00:06:34.440]   the elite IQ will still be preserved,
[00:06:34.440 --> 00:06:36.300]   and more elites will come in through the borders
[00:06:36.300 --> 00:06:37.780]   along with the low-skilled migrants.
[00:06:37.780 --> 00:06:41.060]   So then, since we're caring about the deviation anyways,
[00:06:41.060 --> 00:06:43.460]   more immigration might increase the deviation,
[00:06:43.460 --> 00:06:47.140]   and then we just, that's a good thing?
[00:06:47.140 --> 00:06:48.100]   - So notice what you did there,
[00:06:48.100 --> 00:06:51.980]   is you did something that didn't just increase the variant,
[00:06:51.980 --> 00:06:54.140]   you simultaneously increased the variant
[00:06:54.140 --> 00:06:56.680]   and lowered the mean and median, right?
[00:06:56.680 --> 00:07:00.340]   And so, I think that hurting the mean and median
[00:07:00.340 --> 00:07:04.340]   is actually a big cost, especially in democracies,
[00:07:04.340 --> 00:07:07.860]   and so that is very likely to swall the benefits
[00:07:07.860 --> 00:07:12.860]   of the small probability of getting higher elites
[00:07:13.620 --> 00:07:17.780]   folks in as part of a low-skilled immigration policy.
[00:07:17.780 --> 00:07:19.220]   So pulling down the mean or the median,
[00:07:19.220 --> 00:07:24.220]   that swamps the benefits of increasing variance there, yeah.
[00:07:24.220 --> 00:07:29.680]   - Yes, but if you get rid of a migrant's ability to vote,
[00:07:29.680 --> 00:07:30.980]   and I guess you can't do that, but let's assume
[00:07:30.980 --> 00:07:35.860]   you could do that, what is the exact mechanism
[00:07:35.860 --> 00:07:39.820]   by which the cultural values or the lower median
[00:07:39.820 --> 00:07:42.980]   is impacting the elite's ability to produce
[00:07:42.980 --> 00:07:44.580]   these valuable externalities?
[00:07:44.580 --> 00:07:46.420]   You know, like there's a standard comparative advantage
[00:07:46.420 --> 00:07:50.340]   story that they'll do the housework and the cooking
[00:07:50.340 --> 00:07:52.540]   for the elites and they can be more productive.
[00:07:52.540 --> 00:07:55.020]   - Yeah, taking all the institutions as given,
[00:07:55.020 --> 00:07:57.360]   which is what a lot of open borders optimists do,
[00:07:57.360 --> 00:07:58.580]   they take institutions as given,
[00:07:58.580 --> 00:08:00.280]   they take cultural norms as given,
[00:08:00.280 --> 00:08:03.340]   all that micro stuff works out just fine.
[00:08:03.340 --> 00:08:05.820]   I'm totally on board with all that sort of
[00:08:05.820 --> 00:08:08.960]   Adam Smith division of labor, blah, blah, blah.
[00:08:09.920 --> 00:08:13.520]   But institutions are downstream of culture
[00:08:13.520 --> 00:08:16.200]   and cultural norms will be changing,
[00:08:16.200 --> 00:08:18.400]   partly because of what I call spaghetti theory, right?
[00:08:18.400 --> 00:08:21.480]   We meet in the middle, when new folks come to a country,
[00:08:21.480 --> 00:08:22.480]   there's some kind of convergence,
[00:08:22.480 --> 00:08:24.480]   some part where people meet in the middle
[00:08:24.480 --> 00:08:28.800]   between the values that were previously existing
[00:08:28.800 --> 00:08:30.800]   and the values that have shown up
[00:08:30.800 --> 00:08:32.280]   that migrants have brought with them.
[00:08:32.280 --> 00:08:34.520]   So, you know, like I call it spaghetti theory
[00:08:34.520 --> 00:08:38.020]   because when Italians moved to America,
[00:08:38.020 --> 00:08:40.240]   that got Americans eating more spaghetti, right?
[00:08:40.240 --> 00:08:42.440]   And if you just did a simple assimilation analysis,
[00:08:42.440 --> 00:08:45.400]   you'd say, wow, everybody in America eats the same now,
[00:08:45.400 --> 00:08:46.840]   like they eat burgers and spaghetti.
[00:08:46.840 --> 00:08:48.840]   So look, the Italians assimilated,
[00:08:48.840 --> 00:08:50.560]   but migrants assimilate us.
[00:08:50.560 --> 00:08:55.320]   Native Americans certainly changed in response
[00:08:55.320 --> 00:08:57.060]   to the movement of Europeans.
[00:08:57.060 --> 00:09:00.600]   English Americans certainly changed in response
[00:09:00.600 --> 00:09:04.240]   to the migration of German and Irish Americans.
[00:09:04.240 --> 00:09:06.000]   So this meeting in the middle is something
[00:09:06.000 --> 00:09:07.580]   that happens all the time,
[00:09:07.580 --> 00:09:09.060]   and not just through democratic channels,
[00:09:09.060 --> 00:09:11.420]   just through the sort of soft contact of cultural norms
[00:09:11.420 --> 00:09:14.660]   that sociologists and psychologists would understand.
[00:09:14.660 --> 00:09:17.340]   - Now, I'm sure you saw the book that was released,
[00:09:17.340 --> 00:09:20.860]   I think in 2020, titled "Wretched Refuse",
[00:09:20.860 --> 00:09:23.660]   where they showed a slight positive relationship
[00:09:23.660 --> 00:09:27.380]   between immigration and pro-market laws.
[00:09:27.380 --> 00:09:30.020]   And I guess the idea behind that is there's selection effects
[00:09:30.020 --> 00:09:32.180]   in terms of who would come to a country like America
[00:09:32.180 --> 00:09:33.180]   in the first place.
[00:09:33.180 --> 00:09:35.060]   - Well, they never ran the statistical analysis
[00:09:35.060 --> 00:09:37.160]   that would be most useful, I think.
[00:09:37.160 --> 00:09:40.400]   They said that, so this is Powell and Naraste,
[00:09:40.400 --> 00:09:42.040]   they ran a statistical analysis that said,
[00:09:42.040 --> 00:09:44.520]   and they said, in all of the statistical analyses
[00:09:44.520 --> 00:09:47.200]   we've ever run, we've never found negative relationship
[00:09:47.200 --> 00:09:50.280]   between low-skilled migration, any measure of it,
[00:09:50.280 --> 00:09:52.560]   and changes in economic freedom.
[00:09:52.560 --> 00:09:56.840]   And I actually borrowed another one of Powell's data sets,
[00:09:56.840 --> 00:09:59.880]   and I thought, well, how would I check this theory out,
[00:09:59.880 --> 00:10:02.180]   the idea that changes in migration have an effect
[00:10:02.180 --> 00:10:04.360]   on economic freedom?
[00:10:04.360 --> 00:10:06.120]   And I just used the normal economist tool.
[00:10:06.120 --> 00:10:08.640]   I thought about, how do economists check to see
[00:10:08.640 --> 00:10:11.800]   if changes in money, changes in the money supply,
[00:10:11.800 --> 00:10:12.800]   change the price level?
[00:10:12.800 --> 00:10:15.000]   That's what we call the quantity theory, right?
[00:10:15.000 --> 00:10:17.700]   The way you do that is on the X-axis,
[00:10:17.700 --> 00:10:19.200]   you show the change in the money supply,
[00:10:19.200 --> 00:10:22.740]   and on the Y-axis, you show the change in prices, right?
[00:10:22.740 --> 00:10:25.600]   This Milton Friedman's idea, money is always everywhere,
[00:10:25.600 --> 00:10:28.040]   inflation's always everywhere, monetary phenomena.
[00:10:28.040 --> 00:10:28.880]   So that's what I did.
[00:10:28.880 --> 00:10:33.040]   I did this with a student, we co-authored a paper doing this,
[00:10:33.040 --> 00:10:35.340]   and the very first statistical analysis we ran,
[00:10:35.340 --> 00:10:37.400]   we looked at migrants who came from countries
[00:10:37.400 --> 00:10:40.880]   that were substantially more corrupt
[00:10:40.880 --> 00:10:43.120]   than the country's average.
[00:10:43.120 --> 00:10:48.120]   And we looked at the relationship between an increase
[00:10:48.120 --> 00:10:51.300]   in migrants from corrupt countries,
[00:10:51.300 --> 00:10:54.180]   and subsequent changes in economic freedom.
[00:10:54.180 --> 00:10:56.240]   Every single statistical analysis we found
[00:10:56.240 --> 00:10:57.640]   had a negative relationship.
[00:10:57.640 --> 00:11:01.180]   We ran the simplest estimate you could run, right?
[00:11:01.180 --> 00:11:03.180]   Change on change, change in one thing
[00:11:03.180 --> 00:11:04.840]   predicts change in another.
[00:11:04.840 --> 00:11:06.480]   They somehow never got around to running
[00:11:06.480 --> 00:11:09.080]   that very simple statistical analysis.
[00:11:09.080 --> 00:11:10.840]   One change predicts another change.
[00:11:10.840 --> 00:11:14.320]   We found negative relationships every time,
[00:11:14.320 --> 00:11:17.680]   sometimes statistically significant, sometimes not.
[00:11:17.680 --> 00:11:18.840]   Always negative.
[00:11:18.840 --> 00:11:21.920]   Somehow they never found that, I just don't know how.
[00:11:21.920 --> 00:11:23.280]   - But what about the anecdotal evidence
[00:11:23.280 --> 00:11:25.520]   that in the US, for example,
[00:11:25.520 --> 00:11:27.600]   in the periods of the greatest expansion
[00:11:27.600 --> 00:11:29.640]   of the welfare state or government power
[00:11:29.640 --> 00:11:31.460]   during the New Deal or Great Society,
[00:11:31.460 --> 00:11:33.460]   the levels of foreign-born people
[00:11:33.460 --> 00:11:35.700]   were at historical lows.
[00:11:35.700 --> 00:11:38.020]   Is that just a coincidence, or what do you think it is?
[00:11:38.020 --> 00:11:41.280]   - I'm not really interested in migration per se, right?
[00:11:41.280 --> 00:11:43.940]   My story is never that migration per se
[00:11:43.940 --> 00:11:46.780]   does this bad thing, migrants are bad.
[00:11:46.780 --> 00:11:48.460]   That's never my story, right?
[00:11:48.460 --> 00:11:51.060]   As you know, right? - Yeah, yeah.
[00:11:51.060 --> 00:11:54.500]   - But my story is that migrants bring cultural values
[00:11:54.500 --> 00:11:56.500]   from their old country to their new country.
[00:11:56.500 --> 00:11:58.140]   And sometimes those cultural norms
[00:11:58.140 --> 00:11:59.460]   are better than what you've got,
[00:11:59.460 --> 00:12:01.100]   and sometimes they're worse than what you've got,
[00:12:01.100 --> 00:12:03.100]   and sometimes it's just up for debate.
[00:12:03.100 --> 00:12:05.580]   - So if you had to guess, what percentage of the world
[00:12:05.580 --> 00:12:07.780]   has cultural values that are equivalent to
[00:12:07.780 --> 00:12:10.840]   or better than the average of America's?
[00:12:10.840 --> 00:12:13.520]   - Oh, equivalent to or better than?
[00:12:13.520 --> 00:12:14.360]   - Yeah.
[00:12:14.360 --> 00:12:18.060]   - I mean, just off the top of my head, maybe 20%?
[00:12:18.060 --> 00:12:18.900]   I don't know, 30%?
[00:12:18.900 --> 00:12:21.140]   I'll just throw something out there like that, yeah.
[00:12:21.140 --> 00:12:23.040]   - So I mean, like-- - For country averages, right?
[00:12:23.040 --> 00:12:24.580]   Yeah, yeah. - Yeah, yeah.
[00:12:24.580 --> 00:12:27.760]   Currently, we probably don't have,
[00:12:28.620 --> 00:12:30.380]   it would probably be hard for like 20%
[00:12:30.380 --> 00:12:33.600]   of the rest of the world to get into the US.
[00:12:33.600 --> 00:12:36.180]   Would you support some possible policy
[00:12:36.180 --> 00:12:38.940]   that would make it easy for people from those countries
[00:12:38.940 --> 00:12:40.660]   specifically to get to the US,
[00:12:40.660 --> 00:12:43.180]   just to have radical immigration liberalization
[00:12:43.180 --> 00:12:44.020]   from those places?
[00:12:44.020 --> 00:12:47.380]   - That's really not my comparative advantage
[00:12:47.380 --> 00:12:48.340]   to have opinions about that,
[00:12:48.340 --> 00:12:51.780]   but like substantial increases of people
[00:12:51.780 --> 00:12:53.540]   who pass multiple tests,
[00:12:53.540 --> 00:12:55.720]   like let's take the low-hanging fruit
[00:12:55.720 --> 00:12:57.980]   and then move down from there, right?
[00:12:57.980 --> 00:13:02.980]   So people from countries that on average
[00:13:02.980 --> 00:13:06.740]   have, say, higher savings rates,
[00:13:06.740 --> 00:13:09.540]   higher education levels,
[00:13:09.540 --> 00:13:13.380]   higher, what I call SAT deep root scores,
[00:13:13.380 --> 00:13:17.020]   and countries that are, say, half a standard deviation
[00:13:17.020 --> 00:13:18.980]   above the US level on all three, right?
[00:13:18.980 --> 00:13:20.340]   - Why do they have to be higher?
[00:13:20.340 --> 00:13:21.340]   Why not just equivalent?
[00:13:21.340 --> 00:13:24.260]   Like, you get all the gains from trade,
[00:13:24.260 --> 00:13:26.020]   and plus, it can't be equivalent,
[00:13:26.020 --> 00:13:26.860]   so there's no trade.
[00:13:26.860 --> 00:13:28.940]   - Part of the reason is because the entire world
[00:13:28.940 --> 00:13:30.460]   depends on US innovation.
[00:13:30.460 --> 00:13:32.720]   So we should make America as good as possible,
[00:13:32.720 --> 00:13:34.620]   not just slightly better than it is.
[00:13:34.620 --> 00:13:35.980]   So very few firms would find
[00:13:35.980 --> 00:13:38.260]   that their optimal hiring policy would be
[00:13:38.260 --> 00:13:39.300]   hire anyone who's better
[00:13:39.300 --> 00:13:40.540]   than your current stock of employees.
[00:13:40.540 --> 00:13:42.440]   Would you agree with that?
[00:13:42.440 --> 00:13:44.540]   - Yeah, but you have to pay them a salary
[00:13:44.540 --> 00:13:47.500]   if you're just, if somebody just comes to the US,
[00:13:47.500 --> 00:13:48.780]   you don't have to pay them a salary, right?
[00:13:48.780 --> 00:13:50.340]   So if somebody's better,
[00:13:50.340 --> 00:13:52.020]   if somebody's producing more value for a firm,
[00:13:52.020 --> 00:13:53.740]   the salary would pay them, I think.
[00:13:53.740 --> 00:13:55.740]   - Is a firm's job to maximize its profits
[00:13:55.740 --> 00:13:56.820]   or to just make a little bit more
[00:13:56.820 --> 00:13:58.760]   than it's making right now?
[00:13:58.760 --> 00:14:00.000]   - Maximize profits, but--
[00:14:00.000 --> 00:14:00.840]   - Yeah, there you go.
[00:14:00.840 --> 00:14:02.540]   So you find the best people you can.
[00:14:02.540 --> 00:14:05.700]   You know, sports teams that are hiring
[00:14:05.700 --> 00:14:07.140]   don't just say, "We wanna hire people
[00:14:07.140 --> 00:14:08.100]   "who are better than what we got."
[00:14:08.100 --> 00:14:09.780]   They say, "Let's get the best people we can get.
[00:14:09.780 --> 00:14:11.140]   "Why not get the best?"
[00:14:11.140 --> 00:14:14.300]   That was Jimmy Carter's biography.
[00:14:14.300 --> 00:14:15.220]   Why not the best?
[00:14:15.220 --> 00:14:18.900]   - But you can do that along with getting people
[00:14:18.900 --> 00:14:22.180]   who are, you know, on expected terms,
[00:14:22.180 --> 00:14:24.260]   as good as the existing Americans.
[00:14:24.260 --> 00:14:26.380]   - Why do you want this?
[00:14:26.380 --> 00:14:28.340]   This seems like crazy, right?
[00:14:28.340 --> 00:14:29.380]   What are you talking about?
[00:14:29.380 --> 00:14:31.220]   - But I'm not sure--
[00:14:31.220 --> 00:14:32.060]   - Why not the best?
[00:14:32.060 --> 00:14:33.060]   - What is the trade-off there?
[00:14:33.060 --> 00:14:35.820]   No, I'm not saying you don't get the best,
[00:14:35.820 --> 00:14:38.100]   but I'm saying once you've gotten the best,
[00:14:38.100 --> 00:14:39.380]   what is the harm in getting the people
[00:14:39.380 --> 00:14:40.860]   who have equivalent SAT scores
[00:14:40.860 --> 00:14:42.980]   and the rest of the things you mentioned?
[00:14:42.980 --> 00:14:44.460]   - I think part of the reason would be,
[00:14:44.460 --> 00:14:45.700]   you'd wanna find out, I mean,
[00:14:45.700 --> 00:14:47.420]   if you really wanna do something super hardcore,
[00:14:47.420 --> 00:14:48.580]   you'd have to find out what's best
[00:14:48.580 --> 00:14:49.660]   for the planet as a whole.
[00:14:49.660 --> 00:14:51.220]   What's the trade-off between
[00:14:52.860 --> 00:14:55.660]   having the very best, most innovative,
[00:14:55.660 --> 00:14:57.940]   talented, frugal people in America
[00:14:57.940 --> 00:15:00.660]   doing innovating that has benefits for the whole world
[00:15:00.660 --> 00:15:02.900]   versus having an America that's like 40% better,
[00:15:02.900 --> 00:15:04.260]   but where the median's a little bit,
[00:15:04.260 --> 00:15:06.260]   the median of skill's a little bit lower, right?
[00:15:06.260 --> 00:15:09.380]   Because the median's shaping the productivity
[00:15:09.380 --> 00:15:10.360]   of the whole team, right?
[00:15:10.360 --> 00:15:14.220]   This is what it means when you believe in externalities.
[00:15:14.220 --> 00:15:15.580]   - But if you have somebody who's equivalent,
[00:15:15.580 --> 00:15:18.080]   by definition, they're not moving the median down.
[00:15:18.080 --> 00:15:22.260]   - That's, you're totally right about that.
[00:15:22.260 --> 00:15:23.100]   - Yeah.
[00:15:23.100 --> 00:15:23.940]   - But like, why wouldn't I want
[00:15:23.940 --> 00:15:26.060]   the best thing possible, right?
[00:15:26.060 --> 00:15:26.900]   - Okay.
[00:15:26.900 --> 00:15:27.720]   - I'm still trying to figure out
[00:15:27.720 --> 00:15:28.800]   why you wouldn't want the best thing possible.
[00:15:28.800 --> 00:15:29.640]   You're trying to go to the start
[00:15:29.640 --> 00:15:30.480]   of why you don't want the best thing possible.
[00:15:30.480 --> 00:15:31.300]   - I'm not disagreeing with you.
[00:15:31.300 --> 00:15:32.140]   - I'm like, why not?
[00:15:32.140 --> 00:15:32.980]   - I'm not disagreeing with you.
[00:15:32.980 --> 00:15:34.740]   I'm just, I'm a little bit confused
[00:15:34.740 --> 00:15:37.900]   about why that precludes you
[00:15:37.900 --> 00:15:39.780]   from also getting the second best thing possible
[00:15:39.780 --> 00:15:41.220]   at the same time.
[00:15:41.220 --> 00:15:43.660]   'Cause you're not limited to just the best, right?
[00:15:43.660 --> 00:15:45.260]   - Well, because the second best
[00:15:45.260 --> 00:15:48.340]   is going to have a negative externality on the first best.
[00:15:48.340 --> 00:15:50.500]   Everything's externalities.
[00:15:50.500 --> 00:15:51.620]   This is my worldview, right?
[00:15:51.620 --> 00:15:53.300]   Everything's externalities.
[00:15:53.300 --> 00:15:54.340]   You bring in the second best,
[00:15:54.340 --> 00:15:55.180]   you're like, you're not,
[00:15:55.180 --> 00:15:57.500]   that person's gonna make things on average
[00:15:57.500 --> 00:16:00.180]   a little worse for the first best person.
[00:16:00.180 --> 00:16:02.260]   - But it seems like you were explaining earlier
[00:16:02.260 --> 00:16:03.540]   that the negative externalities
[00:16:03.540 --> 00:16:05.500]   are coming from people, from countries
[00:16:05.500 --> 00:16:07.540]   with low SAT scores.
[00:16:07.540 --> 00:16:09.140]   And by the way, SAT, you can explain what that means,
[00:16:09.140 --> 00:16:10.380]   just for the audience who's not familiar
[00:16:10.380 --> 00:16:11.700]   with how you're using that term.
[00:16:11.700 --> 00:16:15.240]   - Oh yeah, so there's three prominent measures
[00:16:15.240 --> 00:16:17.580]   in what's known as the deep roots literature,
[00:16:17.580 --> 00:16:20.260]   and they're widely used.
[00:16:20.260 --> 00:16:22.140]   Two are S and A,
[00:16:22.140 --> 00:16:24.260]   that state history and agricultural history.
[00:16:24.260 --> 00:16:25.700]   That's how many thousands of years
[00:16:25.700 --> 00:16:27.700]   your ancestors have had experience
[00:16:27.700 --> 00:16:28.980]   living under organized states
[00:16:28.980 --> 00:16:30.780]   or living under settled agriculture.
[00:16:30.780 --> 00:16:33.540]   And then the T score is the tech history score.
[00:16:33.540 --> 00:16:35.100]   I use the measure from 1500.
[00:16:35.100 --> 00:16:37.900]   It's basically what fraction of the world's technology
[00:16:37.900 --> 00:16:39.820]   were your ancestors using in 1500
[00:16:39.820 --> 00:16:43.540]   before Columbus and his expanse of conquest
[00:16:43.540 --> 00:16:46.920]   ended up upending the entire world, the world map.
[00:16:48.100 --> 00:16:51.860]   So S, A, and T are all predictors of modern prosperity,
[00:16:51.860 --> 00:16:54.860]   but especially when you adjust for migration.
[00:16:54.860 --> 00:16:55.700]   - Gotcha.
[00:16:55.700 --> 00:16:56.900]   We can come back to this later,
[00:16:56.900 --> 00:16:59.460]   but one of the interesting things I think from the book
[00:16:59.460 --> 00:17:01.500]   was you have this chapter on China
[00:17:01.500 --> 00:17:04.860]   and the Chinese people as a sort of unstoppable force
[00:17:04.860 --> 00:17:06.420]   for free market capitalism.
[00:17:06.420 --> 00:17:09.740]   And it's interesting, as you mentioned in the book,
[00:17:09.740 --> 00:17:14.380]   that China is the poorest majority Chinese country.
[00:17:14.380 --> 00:17:16.420]   What do you think explains why China
[00:17:16.420 --> 00:17:17.900]   is the poorest majority Chinese country?
[00:17:17.900 --> 00:17:19.660]   Maybe are there like nonlinear dynamics here
[00:17:19.660 --> 00:17:23.180]   where if you go from 40 to 90% Chinese,
[00:17:23.180 --> 00:17:24.100]   there's positive effects,
[00:17:24.100 --> 00:17:26.080]   but if you go from 90 to 95% Chinese,
[00:17:26.080 --> 00:17:26.920]   there's too much.
[00:17:26.920 --> 00:17:28.980]   - No, I think it's just, I think just communism is dumb
[00:17:28.980 --> 00:17:32.980]   and it has terrible, like sometimes decades long effects
[00:17:32.980 --> 00:17:34.060]   on institutional quality
[00:17:34.060 --> 00:17:36.180]   that I don't really quite understand.
[00:17:36.180 --> 00:17:37.220]   So I'd say North Korea,
[00:17:37.220 --> 00:17:38.400]   if we had good data on North Korea,
[00:17:38.400 --> 00:17:40.340]   North Korea would be even a bigger
[00:17:40.340 --> 00:17:43.900]   sort of deep-roofed outlier than China is, right?
[00:17:43.900 --> 00:17:46.340]   It's like, don't have a communist dictatorship
[00:17:46.340 --> 00:17:49.220]   in your country seems to be pretty a robust lesson
[00:17:49.220 --> 00:17:51.260]   for a national prosperity.
[00:17:51.260 --> 00:17:53.340]   China's still stuck with a sort of crummy version
[00:17:53.340 --> 00:17:55.280]   of that mistake still.
[00:17:55.280 --> 00:17:57.740]   North Korea, of course, is stuck with an even worse version.
[00:17:57.740 --> 00:18:00.020]   So I think that's, my hunch is that that's,
[00:18:00.020 --> 00:18:03.060]   you know, the overwhelming issue there.
[00:18:03.060 --> 00:18:05.880]   It's something that it's sort of a,
[00:18:05.880 --> 00:18:08.380]   China's stuck in an,
[00:18:08.380 --> 00:18:10.620]   currently China's stuck in an institutional cul-de-sac
[00:18:10.620 --> 00:18:12.740]   and they just don't quite know how to get out of it.
[00:18:12.740 --> 00:18:14.820]   And it's bad for a lot of the people
[00:18:14.820 --> 00:18:16.160]   who live there on average.
[00:18:16.160 --> 00:18:19.060]   If the other side had won the Chinese civil war,
[00:18:19.060 --> 00:18:22.300]   things probably be a lot, lot better off in China today.
[00:18:22.300 --> 00:18:23.760]   - Yeah.
[00:18:23.760 --> 00:18:26.580]   But what does that suggest about the deep-roofed literature?
[00:18:26.580 --> 00:18:28.700]   If the three biggest countries in the world,
[00:18:28.700 --> 00:18:30.480]   China, India, and America,
[00:18:30.480 --> 00:18:33.220]   it under-predicts their performance,
[00:18:33.220 --> 00:18:35.380]   or I'm sorry, in the case of China and India,
[00:18:35.380 --> 00:18:36.960]   it over-predicts their performance.
[00:18:36.960 --> 00:18:38.620]   And in the case of America, it under-predicts.
[00:18:38.620 --> 00:18:40.180]   Does that suggest that maybe the,
[00:18:40.180 --> 00:18:42.300]   how reliable is this if like the three biggest countries
[00:18:42.300 --> 00:18:44.700]   in the world are not adequately accounted for?
[00:18:45.660 --> 00:18:47.320]   - Well, you know, communism is a really big mistake.
[00:18:47.320 --> 00:18:49.820]   I think that's totally accounted for right there.
[00:18:49.820 --> 00:18:54.740]   I think India's under-performance isn't that huge.
[00:18:54.740 --> 00:18:56.680]   The U.S. is a miracle along many ways.
[00:18:56.680 --> 00:19:00.440]   It's, we should draw our lessons from the typical country.
[00:19:00.440 --> 00:19:03.180]   And I think a population-weighted estimate,
[00:19:03.180 --> 00:19:05.560]   I don't think that basically one-third of the knowledge
[00:19:05.560 --> 00:19:09.000]   about the wealth of nations comes from the current GDP
[00:19:09.000 --> 00:19:11.880]   per capita of China, India, and the U.S., right?
[00:19:11.880 --> 00:19:14.280]   I think much less than one-third of the story
[00:19:14.280 --> 00:19:16.240]   of the wealth of nations comes from those three.
[00:19:16.240 --> 00:19:19.160]   And again, in all three cases, though,
[00:19:19.160 --> 00:19:20.800]   if you look at the economic trajectories
[00:19:20.800 --> 00:19:21.960]   of all three of those people,
[00:19:21.960 --> 00:19:23.560]   of all three of those countries,
[00:19:23.560 --> 00:19:26.160]   they're all China and India growing faster
[00:19:26.160 --> 00:19:27.400]   than you'd expect.
[00:19:27.400 --> 00:19:29.160]   And also I wanna point out,
[00:19:29.160 --> 00:19:31.320]   this is the most important point, actually.
[00:19:31.320 --> 00:19:36.120]   When we look at, when Kaplan made this claim, right?
[00:19:36.120 --> 00:19:37.840]   Brian Kaplan has made this claim, right?
[00:19:37.840 --> 00:19:40.360]   That the SAT, that the Ancestry scores,
[00:19:40.360 --> 00:19:45.360]   the Deep Root scores don't predict the prosperity of,
[00:19:45.360 --> 00:19:48.800]   the low performance of India and China.
[00:19:48.800 --> 00:19:52.200]   He only checked the S and the A in the SAT scores.
[00:19:52.200 --> 00:19:53.920]   - Okay.
[00:19:53.920 --> 00:19:55.240]   - Which letter did he not predict?
[00:19:55.240 --> 00:19:57.040]   Which letter did he never test out?
[00:19:57.040 --> 00:19:58.520]   He never tested the T.
[00:19:58.520 --> 00:20:01.220]   What do you think happens when he tests the T?
[00:20:01.220 --> 00:20:03.400]   - Does it predict China and India and America?
[00:20:03.400 --> 00:20:04.720]   - They start, they,
[00:20:04.720 --> 00:20:07.520]   T goes back to being statistically significant again.
[00:20:07.520 --> 00:20:08.480]   - Uh-huh.
[00:20:08.480 --> 00:20:10.200]   - So with T, which we've always known
[00:20:10.200 --> 00:20:12.720]   that's the best of the Deep Root scores,
[00:20:12.720 --> 00:20:15.520]   somehow Kaplan never managed to measure that one.
[00:20:15.520 --> 00:20:17.160]   Just as Powell and Naraste never managed
[00:20:17.160 --> 00:20:18.560]   to run the simplest test,
[00:20:18.560 --> 00:20:21.040]   change in migrant corruption
[00:20:21.040 --> 00:20:22.920]   versus change in economic institutions.
[00:20:22.920 --> 00:20:25.420]   Somehow like the simplest test just never got run.
[00:20:25.420 --> 00:20:29.000]   - Okay, and then what is the impact if you include T?
[00:20:29.000 --> 00:20:30.240]   - I mean, if you look at T,
[00:20:30.240 --> 00:20:34.040]   then contrary to what Kaplan says,
[00:20:34.040 --> 00:20:35.480]   the Deep Root, that Deep Roots measure
[00:20:35.480 --> 00:20:37.060]   is statistically significant.
[00:20:37.060 --> 00:20:39.520]   - Okay.
[00:20:39.520 --> 00:20:41.780]   - Yeah, the puzzle goes away.
[00:20:41.780 --> 00:20:43.940]   - Interesting.
[00:20:43.940 --> 00:20:46.280]   - Yeah, so somehow these guys just never seem to run
[00:20:46.280 --> 00:20:49.040]   like the simple things, the transparent things.
[00:20:49.040 --> 00:20:49.880]   I don't know why.
[00:20:49.880 --> 00:20:51.780]   - Weird, huh?
[00:20:51.780 --> 00:20:55.200]   - The one you mentioned from, what was it, Narasta?
[00:20:55.200 --> 00:20:58.080]   The name of the guy who wrote the Richard at Refuse?
[00:20:58.080 --> 00:21:01.000]   - Yeah, yeah, Powell and Naraste, yeah, yeah.
[00:21:01.000 --> 00:21:02.760]   - You said you did the regression
[00:21:02.760 --> 00:21:04.560]   on institutional corruption
[00:21:04.560 --> 00:21:05.880]   and from the countries they come from?
[00:21:05.880 --> 00:21:07.040]   Is that right?
[00:21:07.040 --> 00:21:08.420]   - So yeah, the measure they use,
[00:21:08.420 --> 00:21:11.080]   I just took Powell's dataset from another study
[00:21:11.080 --> 00:21:13.720]   and it was the percentage of,
[00:21:13.720 --> 00:21:17.920]   it was basically the percentage of your nation's population,
[00:21:17.920 --> 00:21:20.320]   the percentage increase in your nation's population
[00:21:20.320 --> 00:21:23.000]   from relatively poor or corrupt countries.
[00:21:23.000 --> 00:21:25.980]   They had multiple measures, so.
[00:21:25.980 --> 00:21:27.760]   - And what is on the y-axis there?
[00:21:27.760 --> 00:21:29.760]   - Y-axis is change in economic freedom,
[00:21:29.760 --> 00:21:31.040]   that's my preferred one.
[00:21:31.040 --> 00:21:32.560]   There's also a change in corruption one,
[00:21:32.560 --> 00:21:34.760]   which is a noisier indicator.
[00:21:34.760 --> 00:21:35.820]   You get much clearer results
[00:21:35.820 --> 00:21:38.160]   with change in economic freedom, so.
[00:21:38.160 --> 00:21:39.720]   - Gotcha, gotcha.
[00:21:39.720 --> 00:21:42.500]   Now, does the ideas getting harder to find stuff
[00:21:42.500 --> 00:21:44.260]   and great stagnation,
[00:21:44.260 --> 00:21:45.980]   does that imply we should be less worried
[00:21:45.980 --> 00:21:49.100]   about impinging on the innovation engine
[00:21:49.100 --> 00:21:51.900]   in these countries that people might wanna migrate to?
[00:21:51.900 --> 00:21:53.660]   Because worst comes to worst,
[00:21:53.660 --> 00:21:56.360]   it's not like there are a whole bunch of great new theories
[00:21:56.360 --> 00:21:57.960]   that are gonna come out anyways.
[00:21:57.960 --> 00:22:03.160]   - No, I think that it's always good to have great things
[00:22:03.160 --> 00:22:06.600]   and new ideas, yes, new ideas are getting harder to find,
[00:22:06.600 --> 00:22:10.800]   but the awesome ideas that we're still getting
[00:22:10.800 --> 00:22:12.580]   are still worth so much, right?
[00:22:12.580 --> 00:22:16.260]   If we're still increasing lifespan a month, a year,
[00:22:16.260 --> 00:22:18.340]   for every year of research we're doing,
[00:22:18.340 --> 00:22:20.020]   like that just seems great, right?
[00:22:20.020 --> 00:22:22.780]   A decade, that adds a year to life, so.
[00:22:22.780 --> 00:22:25.360]   Just to use a rough ballpark measure there.
[00:22:25.360 --> 00:22:26.860]   - But so we have a lot of these countries
[00:22:26.860 --> 00:22:28.420]   where a lot of innovation is happening.
[00:22:28.420 --> 00:22:31.860]   So let's say we kept one or two of them as,
[00:22:31.860 --> 00:22:36.800]   you know, havens from any potential downsides
[00:22:36.800 --> 00:22:38.080]   from radical changes.
[00:22:38.080 --> 00:22:39.760]   You know, we've already had this in the case of Japan
[00:22:39.760 --> 00:22:41.960]   or South Korea, there's not that much migration there.
[00:22:41.960 --> 00:22:45.720]   What is the harm in then using the other ones
[00:22:45.720 --> 00:22:47.680]   to decrease global poverty by immigration
[00:22:47.680 --> 00:22:48.740]   or something like that?
[00:22:48.740 --> 00:22:52.280]   - Well, it's obviously better to create
[00:22:52.280 --> 00:22:56.960]   a couple of innovation powerhouses rather than none, right?
[00:22:56.960 --> 00:22:58.200]   So obviously that's tight.
[00:22:58.200 --> 00:23:03.180]   But instead I would prefer to have open borders for Iceland.
[00:23:03.180 --> 00:23:05.340]   If the open borders advocates are right
[00:23:05.340 --> 00:23:08.200]   and open borders will have noticeable effect
[00:23:08.200 --> 00:23:10.960]   on institutional quality, then it's great to move,
[00:23:10.960 --> 00:23:13.020]   have our open borders experiment run in a country
[00:23:13.020 --> 00:23:16.200]   that's lightly populated, has a lot of open land
[00:23:16.200 --> 00:23:18.420]   and has good institutional quality.
[00:23:18.420 --> 00:23:20.700]   And Iceland fits the bill perfectly for that.
[00:23:20.700 --> 00:23:24.060]   So we could preserve the institutional innovation skill,
[00:23:24.060 --> 00:23:28.960]   the institutional quality of what I call the I-7.
[00:23:28.960 --> 00:23:32.120]   That's, you know, China, Japan, South Korea,
[00:23:32.120 --> 00:23:35.360]   the US, Germany, UK, France.
[00:23:35.360 --> 00:23:38.440]   And choose any country out of the hundred,
[00:23:38.440 --> 00:23:39.920]   out of the couple of dozen countries
[00:23:39.920 --> 00:23:41.540]   that have good institutional quality,
[00:23:41.540 --> 00:23:44.800]   just pick one of the others that aren't one of those seven.
[00:23:44.800 --> 00:23:46.920]   Pick one that's not an innovation powerhouse
[00:23:46.920 --> 00:23:49.880]   and turn that into your open borders country.
[00:23:49.880 --> 00:23:52.920]   You could, if you wanted to get basically Singapore levels
[00:23:52.920 --> 00:23:54.920]   of population density in Iceland,
[00:23:54.920 --> 00:23:56.720]   that'd be about 300 million people, I think.
[00:23:56.720 --> 00:23:58.500]   I think that's about what the numbers end up looking like,
[00:23:58.500 --> 00:24:00.160]   something like that.
[00:24:00.160 --> 00:24:05.160]   - But the value of open borders comes from the fact
[00:24:05.160 --> 00:24:07.920]   that you're coming to a country with high agglomerations
[00:24:07.920 --> 00:24:09.520]   of talent and capital and other things,
[00:24:09.520 --> 00:24:11.800]   which is not true of Iceland, right?
[00:24:11.800 --> 00:24:14.000]   So it isn't the entire-
[00:24:14.000 --> 00:24:14.840]   - No, no, no.
[00:24:14.840 --> 00:24:15.680]   I thought the whole point of open borders
[00:24:15.680 --> 00:24:16.840]   that there's institutional quality
[00:24:16.840 --> 00:24:18.840]   and there's some exogenous institutions
[00:24:18.840 --> 00:24:22.200]   that make that place more productive than other places.
[00:24:22.200 --> 00:24:24.120]   And so by move, that's my version
[00:24:24.120 --> 00:24:26.120]   of what I've been exposed to as open borders theories,
[00:24:26.120 --> 00:24:28.840]   that institutions exogenously exist.
[00:24:28.840 --> 00:24:31.680]   There's some places have moderately laissez-faire
[00:24:31.680 --> 00:24:33.080]   institutions in their country
[00:24:33.080 --> 00:24:35.320]   and moving a lot more people there
[00:24:35.320 --> 00:24:37.120]   will not reduce the productivity
[00:24:37.120 --> 00:24:38.600]   of the people who are currently there
[00:24:38.600 --> 00:24:40.720]   and they'll become much more productive.
[00:24:40.720 --> 00:24:44.640]   And so like the institute of quality is crucial.
[00:24:44.640 --> 00:24:46.760]   So, I mean, if you're a real geography guy,
[00:24:46.760 --> 00:24:48.600]   you'd be excited about the fact that Iceland
[00:24:48.600 --> 00:24:51.080]   is so far, so close to the North Pole.
[00:24:51.080 --> 00:24:54.120]   'Cause latitude is a predictor of prosperity.
[00:24:54.120 --> 00:24:56.160]   - I want to go back to the thing about,
[00:24:56.160 --> 00:24:58.120]   should we have open borders for that 20%
[00:24:58.120 --> 00:25:03.120]   of the world's population that comes from equivalent SAT
[00:25:03.120 --> 00:25:05.680]   and other sort of cultural traits as America?
[00:25:05.680 --> 00:25:08.600]   'Cause I feel like this is important enough to dwell on.
[00:25:08.600 --> 00:25:10.080]   Yeah, you know, it seems similar to saying
[00:25:10.080 --> 00:25:12.920]   that once you picked up a $100 bill on the floor,
[00:25:12.920 --> 00:25:14.760]   you wouldn't pick up a $20 bill on the floor
[00:25:14.760 --> 00:25:16.680]   'cause you only won the best bill.
[00:25:16.680 --> 00:25:18.760]   The $20 bill's right there, why not pick it up?
[00:25:18.760 --> 00:25:20.200]   So what if we have-
[00:25:20.200 --> 00:25:23.520]   - Yeah, what if the $20 bill turns your $100 bill
[00:25:23.520 --> 00:25:24.920]   into like an $80 bill?
[00:25:24.920 --> 00:25:27.680]   And turns all of your $100 bills into $80 bills.
[00:25:27.680 --> 00:25:29.240]   - But aren't you controlling for that
[00:25:29.240 --> 00:25:31.360]   by saying that they have equivalent scores
[00:25:31.360 --> 00:25:34.600]   along all those cultural tests that you're considering?
[00:25:34.600 --> 00:25:38.240]   - No, because the median,
[00:25:38.240 --> 00:25:40.000]   so take the simple version of my story,
[00:25:40.000 --> 00:25:42.200]   which is the median of the population ends up shaping
[00:25:42.200 --> 00:25:44.040]   the productivity of everybody in the country, right?
[00:25:44.040 --> 00:25:47.240]   Or the mean, the mean skill level ends up shaping
[00:25:47.240 --> 00:25:49.520]   the productivity of the entire population, right?
[00:25:49.520 --> 00:25:50.560]   So that means we end up, I mean,
[00:25:50.560 --> 00:25:51.840]   I try not to map this up.
[00:25:51.840 --> 00:25:54.800]   I don't wanna map this up in a popular book,
[00:25:54.800 --> 00:25:58.360]   but it means we face a trade-off between being small,
[00:25:58.360 --> 00:26:02.600]   a small country with super awesome positive externalities
[00:26:02.600 --> 00:26:05.240]   for all the workers by just selecting the best people.
[00:26:05.240 --> 00:26:07.400]   And every time we lower the average skill level
[00:26:07.400 --> 00:26:09.640]   in the country, we're lowering the average productivity
[00:26:09.640 --> 00:26:10.720]   of everyone else.
[00:26:10.720 --> 00:26:11.560]   We're creating-
[00:26:11.560 --> 00:26:12.800]   - But what if we didn't lower it?
[00:26:12.800 --> 00:26:14.880]   So you have to have skills that are-
[00:26:14.880 --> 00:26:15.720]   - Lower it compared to-
[00:26:15.720 --> 00:26:19.160]   - Greater than the median of a median American.
[00:26:19.160 --> 00:26:21.040]   This is a ceteris paribus story, right?
[00:26:21.040 --> 00:26:23.880]   Like if you could, suppose the US is at 80 now
[00:26:23.880 --> 00:26:26.320]   on a zero to a hundred scale, right?
[00:26:26.320 --> 00:26:28.200]   So it's just that, it's 80.
[00:26:28.200 --> 00:26:31.160]   And you have a choice between being a hundred and being 99.
[00:26:31.160 --> 00:26:37.120]   If you're at 99, the 99 is making all,
[00:26:37.120 --> 00:26:39.880]   compared to the world of average of a hundred,
[00:26:39.880 --> 00:26:42.640]   the world of an average 99 is making,
[00:26:42.640 --> 00:26:45.160]   reducing the productivity of all those hundreds.
[00:26:45.160 --> 00:26:46.000]   - Okay.
[00:26:46.000 --> 00:26:47.440]   - If we chose 90, we're reducing the productivity
[00:26:47.440 --> 00:26:48.840]   of all those hundreds.
[00:26:48.840 --> 00:26:49.680]   - Yes.
[00:26:49.680 --> 00:26:51.240]   Okay, so let's say we admit all the smartest people
[00:26:51.240 --> 00:26:55.000]   in the world and that gets us from 80 to 85.
[00:26:55.000 --> 00:26:57.440]   That's a new median in America.
[00:26:57.440 --> 00:26:59.560]   At that point, and this is because we admitted
[00:26:59.560 --> 00:27:00.880]   a whole bunch of like 99s
[00:27:00.880 --> 00:27:02.320]   that have just increased our average.
[00:27:02.320 --> 00:27:03.280]   - Yeah, yeah.
[00:27:03.280 --> 00:27:05.200]   - At that point, open borders for everybody
[00:27:05.200 --> 00:27:06.600]   who's above 85.
[00:27:06.600 --> 00:27:12.000]   - Like, this ends up being a math problem
[00:27:12.000 --> 00:27:13.840]   that's a little hard to solve with a podcast, right?
[00:27:13.840 --> 00:27:17.440]   'Cause it's the question of, do I want a smaller country
[00:27:17.440 --> 00:27:19.880]   with super high average productivity
[00:27:19.880 --> 00:27:22.760]   or a bigger country with lower average productivity?
[00:27:22.760 --> 00:27:23.960]   And by average productivity,
[00:27:23.960 --> 00:27:26.560]   I don't just mean a compositional effect.
[00:27:26.560 --> 00:27:27.720]   I mean, negative external,
[00:27:27.720 --> 00:27:30.720]   I mean, relatively fewer positive externalities.
[00:27:30.720 --> 00:27:33.800]   So I'll use the term relatively fewer positive externalities
[00:27:33.800 --> 00:27:37.160]   rather than negative externalities, right?
[00:27:37.160 --> 00:27:39.640]   So like, I don't exactly know where this is trade off
[00:27:39.640 --> 00:27:41.120]   is gonna pan out,
[00:27:41.120 --> 00:27:43.600]   but there is a case for a sort of Manhattan.
[00:27:43.600 --> 00:27:45.440]   When people talk about a Manhattan project, right?
[00:27:45.440 --> 00:27:47.720]   They're talking about putting all like a small number
[00:27:47.720 --> 00:27:50.000]   of the smartest people in a room.
[00:27:50.000 --> 00:27:51.160]   And part of the reason you don't want
[00:27:51.160 --> 00:27:53.320]   like the 20th smartest person in the room,
[00:27:53.320 --> 00:27:55.360]   'cause that person's gonna ruin the,
[00:27:55.360 --> 00:27:57.280]   ruin stuff for the other smart people.
[00:27:57.280 --> 00:28:00.520]   It's amazing how your worldview changes
[00:28:00.520 --> 00:28:02.880]   when you see everybody as an externality.
[00:28:02.880 --> 00:28:07.560]   - I'm kind of confused about this because just having,
[00:28:07.560 --> 00:28:10.760]   at some point, you're gonna run out of the smartest people,
[00:28:10.760 --> 00:28:12.520]   the remainder of the smartest people in the world,
[00:28:12.520 --> 00:28:14.240]   if you admitted all the brilliant people.
[00:28:14.240 --> 00:28:15.080]   - Yeah, yeah.
[00:28:15.080 --> 00:28:16.400]   - Like the US population is to begin with,
[00:28:16.400 --> 00:28:18.200]   you're not gonna change the median
[00:28:18.200 --> 00:28:20.280]   that much by doing that, right?
[00:28:20.280 --> 00:28:22.840]   So it's almost a equivalent to just having more births
[00:28:22.840 --> 00:28:23.840]   from the average American.
[00:28:23.840 --> 00:28:25.800]   Like if the average American just had more kids,
[00:28:25.800 --> 00:28:27.440]   the population would still grow
[00:28:27.440 --> 00:28:30.160]   and the relative effect of the brightest people
[00:28:30.160 --> 00:28:31.480]   might dilute a little bit.
[00:28:31.480 --> 00:28:33.280]   But--
[00:28:33.280 --> 00:28:34.640]   - And that maybe that's a huge tragedy.
[00:28:34.640 --> 00:28:36.280]   We don't know without a bunch of extra math
[00:28:36.280 --> 00:28:37.560]   and a bunch of weird assumptions.
[00:28:37.560 --> 00:28:38.600]   We don't know.
[00:28:38.600 --> 00:28:40.240]   So like, there's a point at which I have to say like,
[00:28:40.240 --> 00:28:41.800]   I don't know.
[00:28:41.800 --> 00:28:42.960]   Right? - Okay.
[00:28:42.960 --> 00:28:43.800]   Yeah.
[00:28:43.800 --> 00:28:46.760]   Is diluting the power of the smartest person in America,
[00:28:46.760 --> 00:28:49.680]   like keeping us from having wondrous miracles
[00:28:49.680 --> 00:28:50.760]   all around us all the time?
[00:28:50.760 --> 00:28:54.320]   I mean, probably not, but I don't know.
[00:28:54.320 --> 00:28:55.720]   - But I guess the sort of the meta question
[00:28:55.720 --> 00:28:58.120]   you can ask about this entire debate is,
[00:28:58.120 --> 00:29:00.080]   listen, there's so much literature here
[00:29:00.080 --> 00:29:02.400]   and it's hard to tell what exactly will happen.
[00:29:02.400 --> 00:29:04.440]   You know, it's possible that culture will become worse.
[00:29:04.440 --> 00:29:05.680]   It's possible it will become better.
[00:29:05.680 --> 00:29:07.400]   It's possible it'll stay the same.
[00:29:07.400 --> 00:29:09.040]   Given the fact that there's this ambiguity,
[00:29:09.040 --> 00:29:10.960]   why not just do the thing that
[00:29:10.960 --> 00:29:12.920]   on the first order of fact seems good
[00:29:12.920 --> 00:29:14.360]   and you know, just like moving somebody
[00:29:14.360 --> 00:29:16.560]   who's like in a poor country to a rich country.
[00:29:16.560 --> 00:29:17.800]   First order of fact seems good.
[00:29:17.800 --> 00:29:19.040]   I don't know how the third and fourth order
[00:29:19.040 --> 00:29:19.880]   of fact shapes out.
[00:29:19.880 --> 00:29:22.040]   Let's just do the simple obvious thing.
[00:29:22.040 --> 00:29:24.760]   - I thought that one of the great ideas of economics
[00:29:24.760 --> 00:29:26.160]   is that we have to worry about secondary
[00:29:26.160 --> 00:29:28.360]   and tertiary consequences, right?
[00:29:28.360 --> 00:29:31.200]   - But if we can't even figure out what they are exactly,
[00:29:31.200 --> 00:29:32.600]   why not just do the thing that
[00:29:32.600 --> 00:29:34.440]   at the first order seems good?
[00:29:34.440 --> 00:29:38.160]   - Because if you have a compelling reason
[00:29:38.160 --> 00:29:41.440]   to think that the direction of strength
[00:29:41.440 --> 00:29:43.200]   of the second and third and fourth order things
[00:29:43.200 --> 00:29:45.680]   are negative and the variances are really wide,
[00:29:45.680 --> 00:29:47.120]   then you're just adding a lot more uncertainty
[00:29:47.120 --> 00:29:47.960]   to your outcomes.
[00:29:47.960 --> 00:29:50.680]   So, and adding uncertainty to your outcomes
[00:29:50.680 --> 00:29:52.800]   that has sizable negative tail,
[00:29:52.800 --> 00:29:55.200]   especially for the whole planet, isn't that great?
[00:29:55.200 --> 00:29:57.280]   Go ahead and run your experiments in Iceland.
[00:29:57.280 --> 00:29:59.080]   Let's run that for 50 years and see what happens.
[00:29:59.080 --> 00:30:00.560]   It's weird how everybody's obsessed
[00:30:00.560 --> 00:30:02.800]   with running the experiment in America, right?
[00:30:02.800 --> 00:30:04.800]   Why not run it in Iceland first?
[00:30:04.800 --> 00:30:07.320]   - 'Cause America's got a lot of great institutions, right?
[00:30:07.320 --> 00:30:08.880]   - But we can check and see what that,
[00:30:08.880 --> 00:30:10.280]   Iceland's a great place too.
[00:30:11.160 --> 00:30:13.240]   And I use Iceland as a metaphor, right?
[00:30:13.240 --> 00:30:15.480]   Like it's people are obsessed with running it in America.
[00:30:15.480 --> 00:30:18.920]   Like there's some kind of need, I don't know why.
[00:30:18.920 --> 00:30:20.680]   So let's try in France.
[00:30:20.680 --> 00:30:23.600]   Let's try Northern Ireland.
[00:30:23.600 --> 00:30:27.400]   - Are places with low SAT scores,
[00:30:27.400 --> 00:30:30.440]   and again, SAT, we're not talking about the,
[00:30:30.440 --> 00:30:32.120]   in case you're skipping to this timestamp,
[00:30:32.120 --> 00:30:34.480]   we're not talking about the college test.
[00:30:34.480 --> 00:30:36.760]   - The deep root SAT, state history,
[00:30:36.760 --> 00:30:38.560]   agricultural history, tech history.
[00:30:38.560 --> 00:30:39.680]   - Right, exactly.
[00:30:39.680 --> 00:30:44.560]   - Are those places with low scores on that test,
[00:30:44.560 --> 00:30:47.960]   are they stuck there forever or is there something
[00:30:47.960 --> 00:30:49.720]   that can be done if you are a country
[00:30:49.720 --> 00:30:54.080]   that has had a short or not significant history
[00:30:54.080 --> 00:30:56.840]   of technology or agriculture?
[00:30:56.840 --> 00:30:58.160]   - Well, I start off the book with this,
[00:30:58.160 --> 00:31:00.280]   which I really think that the one thing they could do
[00:31:00.280 --> 00:31:02.240]   is create a welcoming environment
[00:31:02.240 --> 00:31:03.880]   for large numbers of Chinese migrants
[00:31:03.880 --> 00:31:05.640]   to move there persistently.
[00:31:05.640 --> 00:31:06.680]   I don't think that's, of course,
[00:31:06.680 --> 00:31:08.320]   the only thing that could ever work,
[00:31:08.320 --> 00:31:10.520]   but I think it's something that's within the range
[00:31:10.520 --> 00:31:13.560]   of policy for at least some poor countries.
[00:31:13.560 --> 00:31:14.720]   I don't know which ones,
[00:31:14.720 --> 00:31:19.480]   but some poor countries could follow the approach
[00:31:19.480 --> 00:31:21.560]   that many countries in Southeast Asia followed,
[00:31:21.560 --> 00:31:24.720]   which is create an environment that's welcoming enough
[00:31:24.720 --> 00:31:26.480]   to Chinese migrants.
[00:31:26.480 --> 00:31:28.680]   It's the one country in the world with large numbers
[00:31:28.680 --> 00:31:31.880]   of high SAT score,
[00:31:31.880 --> 00:31:36.680]   with a high SAT score culture, large population,
[00:31:36.680 --> 00:31:38.440]   it's enough of an economic failure
[00:31:38.440 --> 00:31:41.720]   for at least a little longer that folks might be able
[00:31:41.720 --> 00:31:44.080]   to be interested in moving to a poorer country
[00:31:44.080 --> 00:31:45.720]   with lower SAT scores.
[00:31:45.720 --> 00:31:47.440]   In a better world, you could do this with North Korea too,
[00:31:47.440 --> 00:31:49.120]   but the population of North Korea isn't big enough
[00:31:49.120 --> 00:31:51.440]   to make a big dent in the world, right?
[00:31:51.440 --> 00:31:54.240]   China's population is big enough, yeah.
[00:31:54.240 --> 00:31:56.280]   - Another thing you have to worry about in those cases,
[00:31:56.280 --> 00:31:59.880]   though, is the risk that if you do become successful
[00:31:59.880 --> 00:32:01.920]   in that country, there's just gonna be a huge backlash
[00:32:01.920 --> 00:32:04.400]   and your resources will get expropriated,
[00:32:04.400 --> 00:32:05.240]   like what happened to--
[00:32:05.240 --> 00:32:06.800]   - Probably so in Indonesia, right?
[00:32:06.800 --> 00:32:09.560]   Yeah, there have been many times across Southeast Asia
[00:32:09.560 --> 00:32:13.120]   where anti-Chinese pogroms have been,
[00:32:13.120 --> 00:32:15.040]   unfortunately, a fact of life.
[00:32:15.040 --> 00:32:17.680]   - Yeah, yeah, or Indians in Uganda under Idi Amin.
[00:32:17.680 --> 00:32:18.520]   - Idi Amin, yeah.
[00:32:18.520 --> 00:32:21.440]   - Yeah, yeah.
[00:32:21.440 --> 00:32:23.360]   Okay, so actually, I'm curious
[00:32:23.360 --> 00:32:24.200]   how you would think about this.
[00:32:24.200 --> 00:32:26.040]   Given the impact of national IQ,
[00:32:26.040 --> 00:32:28.400]   if you're an effective altruist,
[00:32:28.400 --> 00:32:32.320]   are you just handing out iodine tablets across the world?
[00:32:32.320 --> 00:32:34.720]   What are you doing to increase national IQ?
[00:32:34.720 --> 00:32:37.600]   - Yeah, this is something that I, yes.
[00:32:37.600 --> 00:32:41.360]   Finding ways, this is what I call a Flynn cycle.
[00:32:41.360 --> 00:32:43.720]   Like I wish, I'm hoping for a world
[00:32:43.720 --> 00:32:46.080]   where there are enough public health interventions
[00:32:46.080 --> 00:32:48.720]   and probably K through six education interventions
[00:32:48.720 --> 00:32:52.920]   to boost test scores in the world's poorest countries.
[00:32:52.920 --> 00:32:57.920]   And I think that ends up having a virtuous cycle to it,
[00:32:57.920 --> 00:32:59.320]   right, as people get more productive,
[00:32:59.320 --> 00:33:01.200]   then they can afford more public health,
[00:33:01.200 --> 00:33:02.280]   which makes them more productive,
[00:33:02.280 --> 00:33:03.800]   which means they can afford more public health.
[00:33:03.800 --> 00:33:06.000]   I think brain health is an important
[00:33:06.000 --> 00:33:08.120]   and neglected part of child development.
[00:33:08.120 --> 00:33:11.400]   Fortunately, we've done a fair amount
[00:33:11.400 --> 00:33:13.560]   to reduce the amount of environmental lead
[00:33:13.560 --> 00:33:15.720]   in a lot of poor countries.
[00:33:15.720 --> 00:33:19.320]   That's probably having a good effect right now as we speak
[00:33:19.320 --> 00:33:21.000]   in a lot of the world's poorest countries.
[00:33:21.000 --> 00:33:25.040]   You're right, iodine, basic childhood nutrition,
[00:33:25.040 --> 00:33:30.040]   reliable healthcare to prevent the worst kinds
[00:33:30.040 --> 00:33:33.000]   of just mild childhood infections
[00:33:33.000 --> 00:33:36.560]   that are probably creating what economists
[00:33:36.560 --> 00:33:38.680]   sometimes call health insults.
[00:33:38.680 --> 00:33:40.200]   Things that end up just hurting you in a way
[00:33:40.200 --> 00:33:43.160]   that causes an ill-defined long-term cost.
[00:33:43.160 --> 00:33:45.840]   A lot of that's gonna have to show up in the brain.
[00:33:45.840 --> 00:33:48.360]   I'm a big fan of the view that part of the Flynn effect
[00:33:48.360 --> 00:33:51.000]   is basically nutrition and health.
[00:33:51.000 --> 00:33:53.160]   That Flynn wasn't a huge believer in that,
[00:33:53.160 --> 00:33:55.880]   but I think that's certainly important
[00:33:55.880 --> 00:33:57.440]   in the poorest countries, yeah.
[00:33:57.440 --> 00:34:01.360]   - I think Brian showed an open borders.
[00:34:01.360 --> 00:34:06.360]   If you look at the IQ of adoptees from poor countries
[00:34:06.360 --> 00:34:09.280]   who go, Sweden is the only country that collects data,
[00:34:09.280 --> 00:34:13.120]   but if you get adopted by a parent in Sweden,
[00:34:13.120 --> 00:34:17.440]   the half the gap between the averages
[00:34:17.440 --> 00:34:19.600]   of those two countries goes away.
[00:34:19.600 --> 00:34:21.880]   So I mean, is one of the ways you can increase global IQ
[00:34:21.880 --> 00:34:26.280]   just by moving kids to countries with good health outcomes
[00:34:26.280 --> 00:34:28.880]   that will nourish their intelligence?
[00:34:28.880 --> 00:34:30.160]   - Oh, that's a classic short-run
[00:34:30.160 --> 00:34:31.480]   versus long-run effect, right?
[00:34:31.480 --> 00:34:34.440]   So libertarians and open-border advocates
[00:34:34.440 --> 00:34:36.880]   tend to be focused on the short-run static effects.
[00:34:36.880 --> 00:34:41.720]   And so you're right, moving kids from poor countries
[00:34:41.720 --> 00:34:43.200]   to richer countries is probably gonna raise
[00:34:43.200 --> 00:34:45.200]   their test scores quite a lot.
[00:34:45.200 --> 00:34:48.120]   And then the question is, over the longer run,
[00:34:48.120 --> 00:34:50.680]   are those lower skilled folks,
[00:34:50.680 --> 00:34:52.520]   the folks with lower test scores,
[00:34:52.520 --> 00:34:55.440]   going to degrade the institutional quality
[00:34:55.440 --> 00:34:57.520]   of the places they move to, right?
[00:34:57.520 --> 00:34:59.240]   So if you close half the gap
[00:34:59.240 --> 00:35:00.960]   between the poor country and the rich country,
[00:35:00.960 --> 00:35:02.720]   half the gap is still there, right?
[00:35:02.720 --> 00:35:05.760]   And if I'm right, that IQ has big externalities,
[00:35:05.760 --> 00:35:09.640]   then moving people from a lower scoring country
[00:35:09.640 --> 00:35:12.360]   to a richer scoring country and closing half the IQ gap
[00:35:12.360 --> 00:35:15.360]   still means on net you're creating a negative externality
[00:35:15.360 --> 00:35:17.200]   in the country the kids are moving to.
[00:35:17.200 --> 00:35:22.000]   - Yeah, yeah, we can come back to that, but--
[00:35:22.000 --> 00:35:25.680]   - Yeah, yeah, so basically you just look at the question,
[00:35:25.680 --> 00:35:27.720]   is this lowering the mean test scores in your country?
[00:35:27.720 --> 00:35:29.400]   And if it's lowering the mean test scores,
[00:35:29.400 --> 00:35:31.040]   in the long run, it's on average gonna lower
[00:35:31.040 --> 00:35:32.560]   institutional quality, productivity,
[00:35:32.560 --> 00:35:34.800]   savings rates, those things.
[00:35:34.800 --> 00:35:37.840]   It's hard to avoid that outcome, so.
[00:35:37.840 --> 00:35:40.080]   - I don't remember the exact figures,
[00:35:40.080 --> 00:35:42.960]   but didn't Brian address this in the book,
[00:35:42.960 --> 00:35:44.120]   in the "Open Borders" book as well,
[00:35:44.120 --> 00:35:48.600]   that even if there's the national IQ lowers on average,
[00:35:48.600 --> 00:35:52.200]   if you're just, if you're still raising the global IQ,
[00:35:52.200 --> 00:35:53.840]   that it still nets out positive,
[00:35:53.840 --> 00:35:55.320]   or am I remembering that wrong?
[00:35:55.320 --> 00:35:59.000]   - Well, notice what he does is he attributes,
[00:35:59.000 --> 00:36:01.280]   he says there's some productivity that's just in the land
[00:36:01.280 --> 00:36:03.680]   that's just geographic factors.
[00:36:03.680 --> 00:36:05.960]   So basically being close, and so that,
[00:36:05.960 --> 00:36:07.720]   basically moving people away from the equator
[00:36:07.720 --> 00:36:09.600]   boosts productivity substantially.
[00:36:09.600 --> 00:36:11.800]   And again, that's a static result.
[00:36:11.800 --> 00:36:16.760]   The reason I mentioned that ignores all the I7 stuff
[00:36:16.760 --> 00:36:18.720]   that I'm talking about, where anything that lowers
[00:36:18.720 --> 00:36:22.680]   the level of innovation in the world's
[00:36:22.680 --> 00:36:25.360]   most innovative countries has negative costs
[00:36:25.360 --> 00:36:27.120]   for the entire planet in the long run.
[00:36:27.120 --> 00:36:28.120]   But that's something you'd only see
[00:36:28.120 --> 00:36:30.000]   over the course of 20, 30, 50 years.
[00:36:30.000 --> 00:36:32.040]   And libertarians and open border advocates
[00:36:32.040 --> 00:36:34.720]   are very rarely interested in that kind of timeframe.
[00:36:34.720 --> 00:36:38.240]   - Is there any evidence about the impact of migration
[00:36:38.240 --> 00:36:39.720]   on innovation specifically?
[00:36:39.720 --> 00:36:41.840]   So not on the average institutional quality
[00:36:41.840 --> 00:36:44.360]   or on the corruption or whatever,
[00:36:44.360 --> 00:36:47.000]   but just directly the amount of innovation that happens,
[00:36:47.000 --> 00:36:50.080]   or maybe the Nobel prize is won, or things like that.
[00:36:50.080 --> 00:36:52.040]   - No, I mean, I would presume, I think a lot of us
[00:36:52.040 --> 00:36:55.320]   would presume that the European invasion of North America
[00:36:55.320 --> 00:36:58.480]   ended up having positive effects for global innovation.
[00:36:58.480 --> 00:37:00.080]   It's not an invasion that I'm in favor of,
[00:37:00.080 --> 00:37:02.960]   but if you wanna talk crudely about whether migrations
[00:37:02.960 --> 00:37:05.480]   had an effect on innovation, you'd probably have
[00:37:05.480 --> 00:37:07.680]   to include that as any kind of analysis.
[00:37:07.680 --> 00:37:08.760]   - Yeah, yeah.
[00:37:08.760 --> 00:37:11.280]   Do you think that the people who are currently Americans,
[00:37:11.280 --> 00:37:13.920]   but their ancestry traces back to countries
[00:37:13.920 --> 00:37:17.400]   with low SAT scores, is it possible that US GDP
[00:37:17.400 --> 00:37:20.120]   per capita would be higher without that contribution,
[00:37:20.120 --> 00:37:21.800]   or how do you think about that?
[00:37:21.800 --> 00:37:25.360]   - I mean, that it follows from thinking through
[00:37:25.360 --> 00:37:27.480]   the fact that we are all externalities,
[00:37:27.480 --> 00:37:28.640]   positive or negative, right?
[00:37:28.640 --> 00:37:30.400]   I don't know what in any particular,
[00:37:30.400 --> 00:37:31.960]   any one particular country could turn out
[00:37:31.960 --> 00:37:33.480]   to be some exciting exception to the rules,
[00:37:33.480 --> 00:37:36.280]   some interesting anomaly, but on average,
[00:37:36.280 --> 00:37:39.400]   we should presume that the average skill level of voters,
[00:37:39.400 --> 00:37:43.840]   the average traits that we're bringing from the nations
[00:37:43.840 --> 00:37:47.800]   that are the nations of our ancestors is having an effect
[00:37:47.800 --> 00:37:49.960]   on our current productivity for good or ill.
[00:37:49.960 --> 00:37:51.360]   It's just following through the reasoning,
[00:37:51.360 --> 00:37:53.840]   I'd have to say on average, that's most likely,
[00:37:53.840 --> 00:37:57.040]   but there could always be exceptions to the rule.
[00:37:57.040 --> 00:38:00.320]   - I guess we see large disparities in income
[00:38:00.320 --> 00:38:03.560]   between different ethnic groups across the world,
[00:38:03.560 --> 00:38:05.240]   not just in the United States.
[00:38:05.240 --> 00:38:08.000]   Doesn't that suggest that some of the gains
[00:38:08.000 --> 00:38:10.000]   can be privatized from whatever the cultural
[00:38:10.000 --> 00:38:11.440]   or other traits there are?
[00:38:11.440 --> 00:38:14.440]   Because if these over decades and centuries,
[00:38:14.440 --> 00:38:17.000]   these sorts of gaps continue.
[00:38:17.000 --> 00:38:20.520]   - I don't see where that would follow, right?
[00:38:20.520 --> 00:38:24.520]   - If everything is being, if all the externalities
[00:38:24.520 --> 00:38:26.400]   are just being averaged out over time,
[00:38:26.400 --> 00:38:30.120]   what did you expect that these gaps would narrow?
[00:38:30.120 --> 00:38:31.400]   - Well, I mean, I'm being a little rhetorical
[00:38:31.400 --> 00:38:33.360]   when I'm saying everything's literally an externality,
[00:38:33.360 --> 00:38:34.200]   right?
[00:38:34.200 --> 00:38:35.800]   I don't literally believe that's true.
[00:38:35.800 --> 00:38:37.960]   For instance, people with higher education levels
[00:38:37.960 --> 00:38:39.960]   do actually earn more than people
[00:38:39.960 --> 00:38:41.120]   with lower education levels.
[00:38:41.120 --> 00:38:43.520]   So that's literally not an externality, right?
[00:38:43.520 --> 00:38:45.480]   So some of these other cultural traits
[00:38:45.480 --> 00:38:46.680]   that people are bringing with them
[00:38:46.680 --> 00:38:50.600]   from their ancestors' nations of origin
[00:38:50.600 --> 00:38:52.840]   could be one, are likely one source
[00:38:52.840 --> 00:38:54.160]   of these income differences.
[00:38:54.160 --> 00:38:57.120]   I mean, if you think about differences in frugality,
[00:38:57.120 --> 00:38:59.120]   differences in personal responsibility,
[00:38:59.120 --> 00:39:00.800]   which show up in the surveys
[00:39:00.800 --> 00:39:02.200]   that are persistent across generations,
[00:39:02.200 --> 00:39:03.400]   those are likely to have an effect
[00:39:03.400 --> 00:39:07.120]   on long run productivity for you yourself and your family.
[00:39:07.120 --> 00:39:08.360]   Let alone the hive mind stuff,
[00:39:08.360 --> 00:39:10.760]   where we find that there's a positive relationship
[00:39:10.760 --> 00:39:12.800]   between test scores and productivity.
[00:39:12.800 --> 00:39:17.240]   - There was a blogger who took a look at your 2004 paper
[00:39:17.240 --> 00:39:21.960]   about the impact of national IQ on GDP.
[00:39:21.960 --> 00:39:25.880]   And they calculated, so they were just speculating,
[00:39:25.880 --> 00:39:28.880]   let's say you cloned a million John Monoeymans
[00:39:28.880 --> 00:39:32.000]   and assumed that John Monoeyman had an IQ of 180,
[00:39:32.000 --> 00:39:34.240]   then you could, let me just pull up the exact numbers.
[00:39:34.240 --> 00:39:37.560]   You could raise the average IQ of the United States
[00:39:37.560 --> 00:39:39.680]   by 0.21 points.
[00:39:39.680 --> 00:39:43.760]   And if it's true that one IQ point contributes 6%
[00:39:43.760 --> 00:39:48.240]   to increasing GDP, then this proposal would increase
[00:39:48.240 --> 00:39:52.560]   US GDP by 1.26%.
[00:39:52.560 --> 00:39:54.720]   Do you buy these kinds of extrapolations or?
[00:39:54.720 --> 00:39:56.960]   - 1.26%?
[00:39:56.960 --> 00:39:58.360]   - Yeah, yeah, 'cause you're only cloning a million
[00:39:58.360 --> 00:39:59.200]   John Monoeymans.
[00:39:59.200 --> 00:40:00.640]   - Oh, yeah, okay, so it's about one million John Monoeymans.
[00:40:00.640 --> 00:40:03.360]   Yeah, yeah, that sounds, I mean,
[00:40:03.360 --> 00:40:04.680]   that's the kind of thing where I wouldn't expect it
[00:40:04.680 --> 00:40:05.760]   to happen overnight, right?
[00:40:05.760 --> 00:40:09.200]   I tend to think of the IQ externalities
[00:40:09.200 --> 00:40:10.640]   as being two, three generations.
[00:40:10.640 --> 00:40:12.080]   I lump it in with what economists
[00:40:12.080 --> 00:40:13.200]   call organizational capital.
[00:40:13.200 --> 00:40:15.640]   That sounds about right, yeah, yeah, yeah.
[00:40:15.640 --> 00:40:17.160]   Yeah, I can't remember where I saw this.
[00:40:17.160 --> 00:40:19.920]   I think I stumbled across it myself at some point too.
[00:40:19.920 --> 00:40:22.320]   - Yeah, yeah, by the way, his name is Olivaro de Menard
[00:40:22.320 --> 00:40:23.160]   if you wanna find out.
[00:40:23.160 --> 00:40:24.400]   - Oh, okay, yes, yes, okay.
[00:40:24.400 --> 00:40:26.400]   Yeah, it's, I mean, it's in that ballpark, right?
[00:40:26.400 --> 00:40:29.400]   It's just this idea that, and more importantly,
[00:40:29.400 --> 00:40:31.960]   a million John Von Neumanns would be a gift
[00:40:31.960 --> 00:40:33.360]   to the entire planet, right?
[00:40:33.360 --> 00:40:34.760]   - Yep, yep, yep.
[00:40:34.760 --> 00:40:36.880]   - So yeah, if you had a choice of which country
[00:40:36.880 --> 00:40:40.000]   to have the million John Von Neumanns,
[00:40:40.000 --> 00:40:41.640]   it's probably gonna be one of the I7.
[00:40:41.640 --> 00:40:44.840]   Maybe Switzerland would be a good alternative.
[00:40:44.840 --> 00:40:49.560]   - What is the optimal allocation of intelligence
[00:40:49.560 --> 00:40:51.040]   across a country?
[00:40:51.040 --> 00:40:53.480]   'Cause one answer, and I guess this is the default answer
[00:40:53.480 --> 00:40:55.800]   in our society, is you just send them
[00:40:55.800 --> 00:40:57.120]   where they can get paid the most
[00:40:57.120 --> 00:40:58.400]   because that's a good enough proxy
[00:40:58.400 --> 00:41:00.320]   for how much money they're contributing.
[00:41:00.320 --> 00:41:01.880]   And so you have these high agglomerations
[00:41:01.880 --> 00:41:04.240]   of talent and intelligence in places
[00:41:04.240 --> 00:41:06.400]   like Silicon Valley or New York.
[00:41:06.400 --> 00:41:08.880]   And because their contributions there can scale
[00:41:08.880 --> 00:41:10.600]   to the rest of the world, this is actually
[00:41:10.600 --> 00:41:11.960]   where they're producing the most value.
[00:41:11.960 --> 00:41:14.760]   Another is, actually you should disperse them
[00:41:14.760 --> 00:41:16.840]   throughout the country so that they're helping out
[00:41:16.840 --> 00:41:20.640]   communities, their teachers in their local community.
[00:41:20.640 --> 00:41:23.720]   I think there was a result, there was an interesting
[00:41:23.720 --> 00:41:26.960]   anecdotal evidence that during the Great Depression,
[00:41:26.960 --> 00:41:29.520]   the crime in New York went down a ton,
[00:41:29.520 --> 00:41:31.640]   and that was because the cops in New York
[00:41:31.640 --> 00:41:34.880]   were able to hire, they had like 100 applications
[00:41:34.880 --> 00:41:36.680]   for every cop they hired, and so they were able
[00:41:36.680 --> 00:41:37.800]   to hire the best and the brightest,
[00:41:37.800 --> 00:41:40.720]   and there was a whole bunch of new police tactics
[00:41:40.720 --> 00:41:42.440]   that were pioneered at the time.
[00:41:42.440 --> 00:41:45.320]   Anyways, so is the market allocation
[00:41:45.320 --> 00:41:47.360]   of intelligence correct, or do you think
[00:41:47.360 --> 00:41:48.440]   there should be more distribution
[00:41:48.440 --> 00:41:49.480]   of intelligence across the country?
[00:41:49.480 --> 00:41:50.640]   How do you think about that?
[00:41:50.640 --> 00:41:53.840]   - Yeah, I mean, the market signals aren't terrible,
[00:41:53.840 --> 00:41:56.880]   but this is, my inner Paul Romer kicks in
[00:41:56.880 --> 00:42:00.200]   and says, "Innovation is all about externalities,
[00:42:00.200 --> 00:42:01.880]   "and there's market failures everywhere
[00:42:01.880 --> 00:42:04.520]   "when it comes to the field of innovation."
[00:42:04.520 --> 00:42:09.520]   And so, I personally, I mean, I like the idea
[00:42:09.520 --> 00:42:11.840]   of finding ways to allocate them
[00:42:11.840 --> 00:42:15.520]   to STEM-style technical fields,
[00:42:15.520 --> 00:42:18.240]   and we do a fair amount of that,
[00:42:18.240 --> 00:42:20.920]   and maybe we do the, maybe the US does
[00:42:20.920 --> 00:42:22.360]   a pretty good job of that.
[00:42:22.360 --> 00:42:24.080]   I don't have any huge complaints
[00:42:24.080 --> 00:42:27.520]   at the crudest 50,000 foot level.
[00:42:27.520 --> 00:42:31.960]   The fact that people know that there's a status game
[00:42:31.960 --> 00:42:33.960]   that they can play within academia
[00:42:33.960 --> 00:42:35.600]   that are perhaps more satisfying,
[00:42:35.600 --> 00:42:39.840]   or at least as satisfying as the corporate hierarchy stuff.
[00:42:39.840 --> 00:42:43.000]   So, yeah, you don't want 'em all just,
[00:42:43.000 --> 00:42:44.120]   I wouldn't encourage them
[00:42:44.120 --> 00:42:45.960]   to solely follow market signals, right?
[00:42:45.960 --> 00:42:48.040]   I'd encourage them to be more Hansonian
[00:42:48.040 --> 00:42:50.480]   and play a variety of status game,
[00:42:50.480 --> 00:42:54.000]   because the academic and intellectual status game
[00:42:54.000 --> 00:42:55.880]   is worth a lot, both personally,
[00:42:55.880 --> 00:42:58.760]   and then it leads to positive spillovers for society.
[00:42:58.760 --> 00:43:01.000]   - But how about the geographic distribution?
[00:43:01.000 --> 00:43:02.560]   Do you think that it's fine
[00:43:02.560 --> 00:43:06.000]   that there's people leave, smart people leave Kentucky
[00:43:06.000 --> 00:43:08.160]   and go to San Francisco, or?
[00:43:08.160 --> 00:43:10.200]   - Yeah, I'm a big conglomeration guy, yeah.
[00:43:10.200 --> 00:43:11.040]   - Yeah, yeah.
[00:43:11.040 --> 00:43:13.080]   - Yeah, I'm a big conglomeration guy, yeah.
[00:43:13.080 --> 00:43:14.360]   I mean, the internet makes it easier,
[00:43:14.360 --> 00:43:18.320]   but then, like, still, being close to people's,
[00:43:18.320 --> 00:43:19.840]   being in the room's important.
[00:43:19.840 --> 00:43:24.240]   There's something both Hansonian and Girardian here
[00:43:24.240 --> 00:43:27.120]   about, like, we need to find role models to imitate,
[00:43:27.120 --> 00:43:30.240]   and that's probably important for productivity.
[00:43:31.160 --> 00:43:34.200]   - Are there increasing or decreasing returns
[00:43:34.200 --> 00:43:35.840]   to national IQ?
[00:43:35.840 --> 00:43:38.840]   - No, I think,
[00:43:38.840 --> 00:43:44.040]   and my findings were that it was all basically log-linear,
[00:43:44.040 --> 00:43:47.560]   and so log-linear looks crudely like increasing returns,
[00:43:47.560 --> 00:43:48.760]   right?
[00:43:48.760 --> 00:43:52.080]   So, yeah, it looks exponential, right?
[00:43:52.080 --> 00:43:55.600]   So yeah, there's increasing returns to national IQ, yeah.
[00:43:55.600 --> 00:43:56.440]   - Are you--
[00:43:56.440 --> 00:43:58.960]   - But this is a commonplace finding, in a sense,
[00:43:58.960 --> 00:44:02.320]   because so many, like,
[00:44:02.320 --> 00:44:04.760]   all the human capital relationship I'm familiar with
[00:44:04.760 --> 00:44:07.280]   end up having something like a log-linear form,
[00:44:07.280 --> 00:44:09.080]   which is exponential, so.
[00:44:09.080 --> 00:44:09.920]   - Why is that?
[00:44:09.920 --> 00:44:14.160]   - Yeah, there's something multiplicative.
[00:44:14.160 --> 00:44:16.080]   That's what I have, that's all I have to say,
[00:44:16.080 --> 00:44:17.440]   is, like, it's something,
[00:44:17.440 --> 00:44:21.280]   somehow this all taps into Adam Smith's pin factory,
[00:44:21.280 --> 00:44:24.680]   and we have multiplicative, not additive effects
[00:44:24.680 --> 00:44:26.400]   when we're increasing brain power.
[00:44:27.800 --> 00:44:30.120]   I suspect it does have something to do with
[00:44:30.120 --> 00:44:32.640]   a better organization of the division of labor
[00:44:32.640 --> 00:44:35.080]   between people, which ends up happening,
[00:44:35.080 --> 00:44:38.320]   something close to exponential effects on productivity.
[00:44:38.320 --> 00:44:45.280]   - Are you a fan of genetic selection for intelligence
[00:44:45.280 --> 00:44:46.880]   as a means of increasing national IQ,
[00:44:46.880 --> 00:44:49.760]   or do you think that's too much playing at the margins?
[00:44:49.760 --> 00:44:51.240]   - If it's voluntary, I mean,
[00:44:51.240 --> 00:44:52.960]   people should be able to do what they want,
[00:44:52.960 --> 00:44:56.360]   and after a couple decades of experimentation,
[00:44:56.360 --> 00:44:58.440]   I think people would end up finding a path
[00:44:58.440 --> 00:45:01.080]   to government subsidies or tax credits
[00:45:01.080 --> 00:45:01.920]   or something like that.
[00:45:01.920 --> 00:45:03.960]   I think people voluntarily deciding
[00:45:03.960 --> 00:45:08.960]   what kind of kids they wanna have is a good thing,
[00:45:08.960 --> 00:45:12.560]   and so by genetic selection,
[00:45:12.560 --> 00:45:14.800]   I assume you're meaning at the most elementary level,
[00:45:14.800 --> 00:45:17.360]   people testing their embryos the way they do now, right?
[00:45:17.360 --> 00:45:19.960]   So, I mean, we already do a lot of genetic selection
[00:45:19.960 --> 00:45:21.600]   for intelligence.
[00:45:21.600 --> 00:45:24.840]   Anybody you know who's in their mid-30s or beyond
[00:45:24.840 --> 00:45:26.200]   who's had amniocentesis,
[00:45:26.200 --> 00:45:27.840]   they've been doing a form of genetic selection
[00:45:27.840 --> 00:45:28.680]   for intelligence.
[00:45:28.680 --> 00:45:31.320]   So it's a widespread practice already in our culture,
[00:45:31.320 --> 00:45:36.320]   and welcoming that in a voluntary way
[00:45:36.320 --> 00:45:40.000]   is probably going to have good effects for our future.
[00:45:40.000 --> 00:45:42.400]   - What do you make of the fact that GPT-3,
[00:45:42.400 --> 00:45:47.400]   or I think it was Shad GPT, had a measured IQ of 85?
[00:45:47.400 --> 00:45:49.400]   - Yeah, I've seen a few different measures of this, right?
[00:45:49.400 --> 00:45:51.560]   You might've seen multiple measures.
[00:45:51.560 --> 00:45:53.960]   - Yeah, I think it's a sign that basically,
[00:45:53.960 --> 00:45:56.920]   and when you see people using non-IQ tests
[00:45:56.920 --> 00:46:01.920]   to sort of assess the outputs of GPT on long essays,
[00:46:01.920 --> 00:46:06.600]   it does seem to fit into that sort of not quite 100,
[00:46:06.600 --> 00:46:08.600]   but not off by a lot.
[00:46:08.600 --> 00:46:13.520]   Yeah, I mean, I think it's a sign that a lot of mundane,
[00:46:13.520 --> 00:46:17.000]   even fairly complex, moderately complex human interactions
[00:46:17.000 --> 00:46:20.160]   can be simulated by a large language learning model,
[00:46:20.160 --> 00:46:23.280]   and I think that's gonna be rough news
[00:46:23.280 --> 00:46:28.280]   for a lot of people whose life was in the realm of words
[00:46:28.280 --> 00:46:32.280]   and dispensing simple advice and solving simple problems.
[00:46:32.280 --> 00:46:33.840]   That's pretty bad news for their careers.
[00:46:33.840 --> 00:46:36.440]   I'm disappointed to hear that, so.
[00:46:36.440 --> 00:46:37.760]   - Yeah, yeah.
[00:46:37.760 --> 00:46:38.760]   - At least for the transition.
[00:46:38.760 --> 00:46:41.840]   I don't know what's gonna happen after the transition, but.
[00:46:41.840 --> 00:46:43.400]   - Yeah, I'm hoping that's not true
[00:46:43.400 --> 00:46:46.040]   of programmers or economists like you.
[00:46:46.040 --> 00:46:47.040]   - I mean, it might be, right?
[00:46:47.040 --> 00:46:48.600]   I mean, if that's the way it is.
[00:46:48.600 --> 00:46:52.480]   I mean, the car put a lot of people
[00:46:52.480 --> 00:46:55.560]   who took care of horses right out of work, too, so.
[00:46:55.560 --> 00:46:58.280]   - Yep, okay, so let's talk about democracy.
[00:46:58.280 --> 00:47:01.120]   I thought this was also one of your really interesting books.
[00:47:01.120 --> 00:47:02.320]   - Oh, thanks, yeah.
[00:47:02.320 --> 00:47:05.600]   - Even controlling for how much democratic oversight
[00:47:05.600 --> 00:47:08.080]   there is of institutions in the government,
[00:47:08.080 --> 00:47:11.560]   there seems to be a wide discrepancy of how well they work.
[00:47:11.560 --> 00:47:14.160]   Like, the Fed seems to work reasonably well.
[00:47:14.160 --> 00:47:16.320]   I don't know enough about microeconomics
[00:47:16.320 --> 00:47:18.600]   to know how the object-level decisions they make,
[00:47:18.600 --> 00:47:20.920]   but it seems to be a non-corrupt,
[00:47:20.920 --> 00:47:23.120]   like, technocratic organization.
[00:47:23.120 --> 00:47:24.960]   - Enough, yeah.
[00:47:24.960 --> 00:47:26.520]   - Yeah, yeah.
[00:47:26.520 --> 00:47:27.800]   If you look at something like the FDA,
[00:47:27.800 --> 00:47:30.280]   it's also somewhat insulated from democratic processes.
[00:47:30.280 --> 00:47:31.880]   It seems to not work as well.
[00:47:31.880 --> 00:47:34.480]   What determines, controlling for democracy,
[00:47:34.480 --> 00:47:37.080]   what controls, what determines how well
[00:47:37.080 --> 00:47:38.920]   an institution in the government works?
[00:47:38.920 --> 00:47:41.560]   - Well, I think in the case of the Fed,
[00:47:41.560 --> 00:47:43.320]   it really does matter that they,
[00:47:43.320 --> 00:47:48.440]   the people who run it have a guaranteed long-term,
[00:47:48.440 --> 00:47:51.360]   and they print their own money to spend.
[00:47:51.360 --> 00:47:52.720]   So that means that they're basically,
[00:47:52.720 --> 00:47:56.080]   Congress has to really make an effort
[00:47:56.080 --> 00:47:57.280]   to change anything in the Fed.
[00:47:57.280 --> 00:47:58.760]   So they really have the kind of independence
[00:47:58.760 --> 00:47:59.920]   that matters, right?
[00:47:59.920 --> 00:48:01.920]   You know, they have a room of their own.
[00:48:01.920 --> 00:48:04.480]   And the FDA has to come to Congress
[00:48:04.480 --> 00:48:07.160]   for money more or less every year,
[00:48:07.160 --> 00:48:10.800]   and the FDA heads do not have
[00:48:10.800 --> 00:48:12.680]   any kind of security of appointment.
[00:48:12.680 --> 00:48:14.560]   They serve at the pleasure of the president.
[00:48:14.560 --> 00:48:18.080]   So I do think that they don't have real independence.
[00:48:18.080 --> 00:48:20.160]   I do think that they're basically,
[00:48:20.160 --> 00:48:24.440]   they're living in this black,
[00:48:24.440 --> 00:48:26.320]   this area of slack, to use the sort of
[00:48:26.320 --> 00:48:28.800]   McNolgast, poli sci jargon.
[00:48:28.800 --> 00:48:30.600]   They're living in this realm of slack between
[00:48:30.600 --> 00:48:34.560]   the fact that the president doesn't wanna muddle with them,
[00:48:34.560 --> 00:48:35.920]   meddle with them, excuse me,
[00:48:35.920 --> 00:48:37.240]   and the fact that Congress doesn't really
[00:48:37.240 --> 00:48:38.640]   wanna meddle with them.
[00:48:38.640 --> 00:48:40.600]   But on the other hand,
[00:48:40.600 --> 00:48:45.040]   I really think that the FDA and the CDC
[00:48:45.040 --> 00:48:48.240]   are doing what Congress more or less wanted them to do.
[00:48:48.240 --> 00:48:52.480]   They reflect the muddled disarray
[00:48:52.480 --> 00:48:55.080]   that Congress was in over the period of, say, COVID.
[00:48:55.080 --> 00:48:58.640]   I think that's a first order important.
[00:48:58.640 --> 00:49:02.040]   I mean, I do think the fact that FDA and CDC
[00:49:02.040 --> 00:49:06.440]   don't seem to have that culture of raw technocracy
[00:49:06.440 --> 00:49:07.600]   the way the Fed does,
[00:49:07.600 --> 00:49:10.240]   I think that has to be important on its own.
[00:49:10.240 --> 00:49:12.840]   But I think behind that, some of that is just like,
[00:49:13.920 --> 00:49:17.200]   FDA, CDC creatures of Congress much more than the Fed is.
[00:49:17.200 --> 00:49:19.600]   - Should the power of the president be increased?
[00:49:19.600 --> 00:49:21.800]   - No.
[00:49:21.800 --> 00:49:24.560]   No, like the power of independent committees
[00:49:24.560 --> 00:49:25.800]   should be increased.
[00:49:25.800 --> 00:49:27.800]   Like more of Congress should be like the Fed.
[00:49:27.800 --> 00:49:31.160]   My plan for a Fed, for an FDA or CDC reorganization
[00:49:31.160 --> 00:49:33.520]   would be about making them more like the Fed
[00:49:33.520 --> 00:49:37.400]   where they have appointed experts who have long-terms
[00:49:37.400 --> 00:49:38.880]   and they have enough of a long-term
[00:49:38.880 --> 00:49:41.160]   that they can basically feel like they can blow off Congress
[00:49:41.160 --> 00:49:42.560]   and build their own culture.
[00:49:43.760 --> 00:49:47.400]   - So the European Union is an interesting example here
[00:49:47.400 --> 00:49:51.120]   because they also have these appointed technocrats,
[00:49:51.120 --> 00:49:54.360]   but they seem more interested in creating annoying pop-ups
[00:49:54.360 --> 00:49:56.200]   on your websites than with dealing
[00:49:56.200 --> 00:50:00.000]   with the end of economic growth on the continent.
[00:50:00.000 --> 00:50:02.720]   Is this a story where more democracy would have helped
[00:50:02.720 --> 00:50:04.040]   or how do you think about the European Union
[00:50:04.040 --> 00:50:04.920]   in this context?
[00:50:04.920 --> 00:50:08.880]   - No, in the EU, like the European voters
[00:50:08.880 --> 00:50:10.760]   just aren't that excited about democracy.
[00:50:10.760 --> 00:50:12.880]   Oh, excuse me, aren't that excited about markets overall.
[00:50:12.880 --> 00:50:15.440]   The EU is gonna reflect that, right?
[00:50:15.440 --> 00:50:18.680]   What little evidence we have suggests that countries
[00:50:18.680 --> 00:50:21.000]   that are getting ready to join the EU,
[00:50:21.000 --> 00:50:23.040]   they improve their economic freedom scores,
[00:50:23.040 --> 00:50:26.240]   their sort of laissez-faireness on the path
[00:50:26.240 --> 00:50:29.720]   to getting ready for joining the EU.
[00:50:29.720 --> 00:50:32.840]   So, and then they may increase it a little bit afterwards
[00:50:32.840 --> 00:50:33.880]   once they join.
[00:50:33.880 --> 00:50:36.680]   But basically it's like when you're deciding to join the EU,
[00:50:36.680 --> 00:50:39.000]   it's like you decided you have your Rocky training montage
[00:50:39.000 --> 00:50:40.600]   and get more laissez-faire.
[00:50:40.600 --> 00:50:42.880]   And so the EU on net is a message,
[00:50:42.880 --> 00:50:45.000]   it pulls in the direction of markets
[00:50:45.000 --> 00:50:47.520]   compared to where Europe would be otherwise.
[00:50:47.520 --> 00:50:49.240]   I mean, just look at the nations that are in the EU now,
[00:50:49.240 --> 00:50:52.040]   right, a lot of them are east of Germany, right?
[00:50:52.040 --> 00:50:54.560]   And so those are countries that don't have this great,
[00:50:54.560 --> 00:50:57.880]   you know, history of being market-friendly
[00:50:57.880 --> 00:51:00.360]   and a lot of parties aren't that market-friendly.
[00:51:00.360 --> 00:51:02.760]   And yet the EU sort of nags them into their version,
[00:51:02.760 --> 00:51:05.440]   like as much markets as they can handle, so.
[00:51:05.440 --> 00:51:08.160]   - What do you think explains the fact that the European,
[00:51:08.160 --> 00:51:10.040]   Europe as a whole and the voters in there
[00:51:10.040 --> 00:51:12.880]   are less market-friendly than Americans?
[00:51:12.880 --> 00:51:15.160]   I mean, if you look at the sort of deep roots analysis
[00:51:15.160 --> 00:51:17.840]   of Europe, you would think that they should be the most,
[00:51:17.840 --> 00:51:21.400]   most in favor of, I don't know if the deep roots of,
[00:51:21.400 --> 00:51:22.240]   actually maybe the-
[00:51:22.240 --> 00:51:24.320]   - I mean, yeah, compared to the planet as a whole,
[00:51:24.320 --> 00:51:25.320]   they're pretty good, right?
[00:51:25.320 --> 00:51:28.440]   So, I never get that excited
[00:51:28.440 --> 00:51:29.960]   about like the small little distinctions
[00:51:29.960 --> 00:51:31.280]   between the US and Europe,
[00:51:31.280 --> 00:51:33.280]   like these 30% GDP differences,
[00:51:33.280 --> 00:51:35.480]   which are very exciting to pundits
[00:51:35.480 --> 00:51:36.400]   and bloggers and whatever.
[00:51:36.400 --> 00:51:38.640]   I'm like, 30% doesn't matter very much,
[00:51:38.640 --> 00:51:40.080]   that's not really my bailiwick.
[00:51:40.080 --> 00:51:42.040]   What I'm really interested in is the 3000%
[00:51:42.040 --> 00:51:44.440]   between the poorest countries and the richest countries.
[00:51:44.440 --> 00:51:47.000]   So like, I can speculate about Europe.
[00:51:47.000 --> 00:51:48.280]   I don't really have a great answer.
[00:51:48.280 --> 00:51:51.520]   I mean, I think there's something to the naive view
[00:51:51.520 --> 00:51:55.160]   that the Europeans with the most,
[00:51:55.160 --> 00:51:56.920]   what my dad would call gumption,
[00:51:56.920 --> 00:51:59.080]   are those who left and came to America.
[00:51:59.080 --> 00:52:01.040]   Some openness, some adventurousness,
[00:52:01.040 --> 00:52:04.920]   and maybe that's part of what made,
[00:52:04.920 --> 00:52:07.400]   so basically there's a lot of selection working
[00:52:07.400 --> 00:52:11.760]   on the migration side to make America more open
[00:52:11.760 --> 00:52:14.480]   to laissez-faire than Europe would be.
[00:52:14.480 --> 00:52:16.280]   - Does that overall make you more optimistic
[00:52:16.280 --> 00:52:19.160]   about migration to the US from anywhere,
[00:52:19.160 --> 00:52:20.840]   like, you know, the same story?
[00:52:20.840 --> 00:52:21.880]   - Yeah, center of care of us.
[00:52:21.880 --> 00:52:24.920]   Like, America gives people who are really great, right?
[00:52:24.920 --> 00:52:26.560]   I went with you there, yeah.
[00:52:26.560 --> 00:52:30.440]   - Does elite technocratic control work best
[00:52:30.440 --> 00:52:32.560]   in only in high-IQ countries?
[00:52:32.560 --> 00:52:35.920]   Because otherwise you don't have these high-IQ elites
[00:52:35.920 --> 00:52:37.960]   who can make good policies for you,
[00:52:37.960 --> 00:52:39.800]   but you also don't get the democratic protections
[00:52:39.800 --> 00:52:42.000]   against famine and war and things like that.
[00:52:42.000 --> 00:52:46.320]   - Oh, I mean, I don't know.
[00:52:46.320 --> 00:52:50.400]   I think the case for handing things over to elites
[00:52:50.400 --> 00:52:52.160]   is pretty strong in anything
[00:52:52.160 --> 00:52:54.160]   that's moderately democratic, right?
[00:52:54.160 --> 00:52:58.440]   I don't have to be, anything that's substantially
[00:52:58.440 --> 00:53:00.320]   more democratic than the official measure
[00:53:00.320 --> 00:53:01.360]   of Singapore, for instance.
[00:53:01.360 --> 00:53:03.080]   I mean, that's why my book "10% Less Democracy"
[00:53:03.080 --> 00:53:06.560]   really is targeted at the rich, rich democracies.
[00:53:06.560 --> 00:53:10.240]   Once we get too far below the rich democracies,
[00:53:10.240 --> 00:53:12.000]   I figure once you put elites in charge,
[00:53:12.000 --> 00:53:14.040]   they really are just gonna be old-fashioned
[00:53:14.040 --> 00:53:15.160]   Gordon Tullock rent seekers
[00:53:15.160 --> 00:53:17.000]   and steer everything toward themselves
[00:53:17.000 --> 00:53:19.280]   and not give a darn about the masses at all.
[00:53:19.280 --> 00:53:24.280]   So that's, you know, elite control in a democracy,
[00:53:24.280 --> 00:53:28.520]   a lot of elite control in any kind of democracy,
[00:53:28.520 --> 00:53:30.080]   I think is gonna have good effects
[00:53:30.080 --> 00:53:31.760]   if you're really looking at something
[00:53:31.760 --> 00:53:35.680]   that meet the Martin Yessen's definition of a democracy,
[00:53:35.680 --> 00:53:38.280]   competitive market, competitive parties, free press.
[00:53:38.280 --> 00:53:41.720]   - Does Singapore meet that criteria?
[00:53:41.720 --> 00:53:44.080]   - No, because their party aren't really allowed to compete.
[00:53:44.080 --> 00:53:45.120]   I mean, that's pretty obvious.
[00:53:45.120 --> 00:53:48.080]   Yeah, the People's Action Party
[00:53:48.080 --> 00:53:52.360]   really controlled party competition there.
[00:53:52.360 --> 00:53:56.040]   - But I guess Singapore is one of the great examples
[00:53:56.040 --> 00:53:58.880]   of technocratic control.
[00:53:58.880 --> 00:54:00.240]   - They're just an exception to the rule.
[00:54:00.240 --> 00:54:01.640]   Most countries that try to pull off
[00:54:01.640 --> 00:54:04.200]   that lower level democracy wind up much worse.
[00:54:04.200 --> 00:54:07.720]   - What is your opinion of neo-reactionaries?
[00:54:07.720 --> 00:54:10.560]   I guess they're not in favor of 10% less democracy.
[00:54:10.560 --> 00:54:13.040]   They're more in favor of 100% less democracy.
[00:54:13.040 --> 00:54:15.560]   - Yeah, I think they're kind of too much LARPing,
[00:54:15.560 --> 00:54:18.560]   too much romanticizing about the Rohirrim, I guess.
[00:54:18.560 --> 00:54:19.400]   I don't know.
[00:54:19.400 --> 00:54:20.560]   - What is the Rohirrim?
[00:54:20.560 --> 00:54:24.920]   - Yeah, these guys in the Lord of the Rings, you know.
[00:54:24.920 --> 00:54:27.040]   Romanticizing monarchy is a mistake.
[00:54:28.120 --> 00:54:30.560]   It's worth noting that,
[00:54:30.560 --> 00:54:33.720]   as my colleague Gordon Tolick pointed out,
[00:54:33.720 --> 00:54:35.160]   as well as many others,
[00:54:35.160 --> 00:54:40.800]   in equilibrium, kings are almost always king and council,
[00:54:40.800 --> 00:54:42.960]   right, and so it's worth thinking through
[00:54:42.960 --> 00:54:45.120]   why king and council is the equilibrium,
[00:54:45.120 --> 00:54:46.840]   something more like a corporate board
[00:54:46.840 --> 00:54:51.720]   and less like either the libertarian ideal
[00:54:51.720 --> 00:54:54.200]   of the entrepreneur who owns the firm
[00:54:54.200 --> 00:54:56.560]   or the monarch who has the long-term interest
[00:54:56.560 --> 00:54:58.640]   in being a stationary bandit.
[00:54:58.640 --> 00:55:02.040]   In real life, there's this sort of muddled thing in between
[00:55:02.040 --> 00:55:03.160]   that works out as the equilibrium,
[00:55:03.160 --> 00:55:05.840]   even in the successful so-called monarchies.
[00:55:05.840 --> 00:55:07.360]   So it's worth thinking through why it is
[00:55:07.360 --> 00:55:09.480]   that the successful so-called monarchies
[00:55:09.480 --> 00:55:11.040]   aren't really monarchies, right?
[00:55:11.040 --> 00:55:12.800]   They're really oligarchies.
[00:55:12.800 --> 00:55:13.640]   - Yeah, yeah.
[00:55:13.640 --> 00:55:17.840]   If you look at the median voter
[00:55:17.840 --> 00:55:20.600]   in terms of their preferences on economic policies,
[00:55:20.600 --> 00:55:24.440]   it seems like they're probably more
[00:55:24.440 --> 00:55:26.280]   in favor of government involvement
[00:55:26.280 --> 00:55:30.160]   than the actual policies of the United States, for example.
[00:55:30.160 --> 00:55:31.000]   What explains this?
[00:55:31.000 --> 00:55:32.720]   Shouldn't the median voter theorem imply
[00:55:32.720 --> 00:55:35.200]   that we should be much less libertarian as a country?
[00:55:35.200 --> 00:55:38.440]   - Yeah, that's a great point from Brian Kaplan's
[00:55:38.440 --> 00:55:40.440]   excellent book, "Myth of the Rational Voter," right?
[00:55:40.440 --> 00:55:41.360]   Yeah, I think part of it,
[00:55:41.360 --> 00:55:42.680]   I mean, I think his stories are right,
[00:55:42.680 --> 00:55:45.440]   which is that politicians facing re-election
[00:55:45.440 --> 00:55:47.400]   have this trade-off between giving voters
[00:55:47.400 --> 00:55:48.880]   what the voters say they want
[00:55:48.880 --> 00:55:50.640]   and giving the voters the economic growth
[00:55:50.640 --> 00:55:53.680]   that will help politicians get re-elected, right?
[00:55:55.080 --> 00:55:58.640]   So it's a version of saying like,
[00:55:58.640 --> 00:56:00.360]   I don't want you to pull off the Band-Aid,
[00:56:00.360 --> 00:56:02.640]   but I want my wounds to all get better.
[00:56:02.640 --> 00:56:05.280]   So then, so the politician has to,
[00:56:05.280 --> 00:56:06.960]   it's the politician's job to handle
[00:56:06.960 --> 00:56:09.280]   the contradictory demands of the voters.
[00:56:09.280 --> 00:56:11.880]   And by delegating authority to them,
[00:56:11.880 --> 00:56:13.720]   to the voter, to the elected politicians,
[00:56:13.720 --> 00:56:15.720]   you get some of the benefits of elitism,
[00:56:15.720 --> 00:56:18.320]   even in a so-called democracy.
[00:56:18.320 --> 00:56:23.000]   - Over the long run, should we expect any of the tensions
[00:56:23.000 --> 00:56:26.200]   or all the tensions of ethnic diversity to fade away?
[00:56:26.200 --> 00:56:28.080]   Like nobody today worries about
[00:56:28.080 --> 00:56:30.240]   the different Parisian tribes in France
[00:56:30.240 --> 00:56:32.120]   butting heads at the workplace, right?
[00:56:32.120 --> 00:56:34.000]   So over time-- - Yeah, and you're right,
[00:56:34.000 --> 00:56:38.200]   like the anti-German ethnic sentiment in the U.S.,
[00:56:38.200 --> 00:56:39.160]   totally gone, right?
[00:56:39.160 --> 00:56:41.040]   So, you know. - Right, but over time,
[00:56:41.040 --> 00:56:43.400]   so then this is another one of those short-run effects
[00:56:43.400 --> 00:56:46.600]   that you emphasize we should focus less on, right?
[00:56:46.600 --> 00:56:48.160]   - Yeah, that's a good point.
[00:56:48.160 --> 00:56:49.720]   It's possibly, you don't know which,
[00:56:49.720 --> 00:56:52.800]   I mean, the problem is that ethnic conflict
[00:56:52.800 --> 00:56:54.520]   has been a hardy perennial.
[00:56:54.520 --> 00:56:57.160]   It's not the only conflict that people can ever have.
[00:56:57.160 --> 00:57:02.160]   I don't know to what extent they'll,
[00:57:02.160 --> 00:57:03.600]   these things will fade away.
[00:57:03.600 --> 00:57:06.200]   As I emphasize, in the Culture Transplant,
[00:57:06.200 --> 00:57:08.120]   the ethnic diversity channel is actually
[00:57:08.120 --> 00:57:11.520]   the least important of any of the channels I discuss.
[00:57:11.520 --> 00:57:12.960]   And so I'm open to this thought
[00:57:12.960 --> 00:57:15.120]   that what you're saying will actually happen.
[00:57:15.120 --> 00:57:16.440]   And maybe we'll just find something else
[00:57:16.440 --> 00:57:20.760]   to get mad at each other about, like social media tribes
[00:57:20.760 --> 00:57:23.200]   or religious groups.
[00:57:23.200 --> 00:57:25.640]   I mean, it hasn't happened yet
[00:57:25.640 --> 00:57:28.040]   in all the documented human history we have.
[00:57:28.040 --> 00:57:32.280]   People seem to find some ethnic balance for conflict.
[00:57:32.280 --> 00:57:35.200]   It is worth pointing out that the one study that I report,
[00:57:35.200 --> 00:57:37.480]   I think it's a Wagyar co-author piece,
[00:57:37.480 --> 00:57:39.200]   find that when the real,
[00:57:39.200 --> 00:57:40.760]   the source of ethnic conflict happens
[00:57:40.760 --> 00:57:43.760]   when private values are correlated
[00:57:43.760 --> 00:57:46.840]   with ethnic groups, right?
[00:57:46.840 --> 00:57:49.880]   So if cultural values are basically uncorrelated
[00:57:49.880 --> 00:57:52.400]   with ethnicity, then basically there's nothing to fight over.
[00:57:52.400 --> 00:57:53.320]   And that's really what's happened
[00:57:53.320 --> 00:57:56.480]   with a lot of old ethnic battles in the US.
[00:57:56.480 --> 00:57:58.160]   And so you're right.
[00:57:58.160 --> 00:57:59.680]   Some of these things will fade with time.
[00:57:59.680 --> 00:58:02.560]   The problem is that human beings are,
[00:58:02.560 --> 00:58:05.360]   one of our great evils is that we are always looking
[00:58:05.360 --> 00:58:07.520]   for a focal point and we can,
[00:58:07.520 --> 00:58:09.880]   people will use visible appearance
[00:58:09.880 --> 00:58:12.480]   as a horrifying focal point around which
[00:58:12.480 --> 00:58:14.600]   to peg their conflicts.
[00:58:14.600 --> 00:58:16.960]   It's an easy one because our brains are looking
[00:58:16.960 --> 00:58:18.960]   for visual patterns and I don't like that,
[00:58:18.960 --> 00:58:22.400]   but it's something that will probably keep happening.
[00:58:22.400 --> 00:58:24.120]   - One of the interesting points you made in that chapter
[00:58:24.120 --> 00:58:27.360]   was that the benefits of diversity are greatest
[00:58:27.360 --> 00:58:29.120]   when search costs are lower
[00:58:29.120 --> 00:58:31.240]   and the costs of vetting are lower.
[00:58:31.240 --> 00:58:33.600]   How do we make sure that that is true
[00:58:33.600 --> 00:58:35.440]   of non-elite professions?
[00:58:35.440 --> 00:58:37.320]   So if you're looking for a plumber
[00:58:37.320 --> 00:58:38.880]   or if you're looking for a carpenter,
[00:58:38.880 --> 00:58:42.200]   how do you make sure that you can vet them easily?
[00:58:42.200 --> 00:58:43.920]   - I mean, I have to say that this has to be a case
[00:58:43.920 --> 00:58:47.120]   where like Yelp and Google and all these online ratings
[00:58:47.120 --> 00:58:49.320]   have given us tools for checking these things out.
[00:58:49.320 --> 00:58:51.800]   We know we have to be skeptical, of course,
[00:58:51.800 --> 00:58:55.560]   but for people who know that they're good at something,
[00:58:55.560 --> 00:58:58.360]   the cost of entry into a new field has to be much lower
[00:58:58.360 --> 00:58:59.600]   than it was a few decades ago
[00:58:59.600 --> 00:59:02.760]   because 10, 20 good Google reviews
[00:59:02.760 --> 00:59:06.880]   and you can actually enter.
[00:59:06.880 --> 00:59:11.880]   So yeah, lower, basically not banning disclosure of data,
[00:59:11.880 --> 00:59:14.800]   I'd say that's the most important thing we can do.
[00:59:16.200 --> 00:59:17.720]   - You sometimes hear that medical doctors,
[00:59:17.720 --> 00:59:18.920]   I haven't checked up on this a long time,
[00:59:18.920 --> 00:59:22.600]   but apparently medical doctors often make it very risky
[00:59:22.600 --> 00:59:23.880]   to give bad reviews.
[00:59:23.880 --> 00:59:26.760]   Sometimes you get lost, you get a lawsuit or something.
[00:59:26.760 --> 00:59:29.880]   So making that a lot harder is worth it.
[00:59:29.880 --> 00:59:31.720]   We know that some negative reviews
[00:59:31.720 --> 00:59:35.120]   are gonna be malicious and inaccurate,
[00:59:35.120 --> 00:59:38.720]   but the benefits of information flow seem really high.
[00:59:38.720 --> 00:59:41.280]   - Yeah, I thought one of the really interesting chapters
[00:59:41.280 --> 00:59:44.920]   in the "10% Less Democracy" was the chapter
[00:59:44.920 --> 00:59:47.120]   on bondholder democracy.
[00:59:47.120 --> 00:59:51.120]   And I'm curious, corporations are obviously an example
[00:59:51.120 --> 00:59:52.720]   to use here where they do have bondholders
[00:59:52.720 --> 00:59:53.960]   who hold them accountable,
[00:59:53.960 --> 00:59:56.760]   but the average lifespan of a corporation
[00:59:56.760 --> 00:59:58.680]   is 10 years, I believe.
[00:59:58.680 --> 01:00:01.640]   So do you think it would be even shorter
[01:00:01.640 --> 01:00:04.160]   if bondholders had a lesser say on corporations
[01:00:04.160 --> 01:00:08.080]   or what does the transience of the corporation
[01:00:08.080 --> 01:00:10.240]   tell us about their controls?
[01:00:10.240 --> 01:00:12.600]   - Oh, that's a good point.
[01:00:12.640 --> 01:00:14.560]   Well, we can suspect that the average person
[01:00:14.560 --> 01:00:17.160]   who's investing in a corporation makes money, right?
[01:00:17.160 --> 01:00:19.440]   Because otherwise people wouldn't be doing it, right?
[01:00:19.440 --> 01:00:21.280]   On average people, they must work.
[01:00:21.280 --> 01:00:27.160]   So, this is what, right?
[01:00:27.160 --> 01:00:28.240]   You actually have me stumped here.
[01:00:28.240 --> 01:00:29.840]   So can you rephrase the question again?
[01:00:29.840 --> 01:00:31.520]   I'm trying to think through what the question is there.
[01:00:31.520 --> 01:00:32.360]   Yeah.
[01:00:32.360 --> 01:00:36.400]   - If bondholders do extend the longevity
[01:00:36.400 --> 01:00:39.120]   and the long-term thinking of the organizations
[01:00:39.120 --> 01:00:41.040]   on who can hold bonds,
[01:00:41.040 --> 01:00:43.840]   why aren't corporations who give out bonds,
[01:00:43.840 --> 01:00:46.240]   why don't they tend to live longer
[01:00:46.240 --> 01:00:48.040]   than I think the average of 10 years?
[01:00:48.040 --> 01:00:50.280]   It would it be even shorter without bondholders or?
[01:00:50.280 --> 01:00:52.680]   - Oh, I'd say, A, I'd say it'd probably be shorter
[01:00:52.680 --> 01:00:55.160]   without bondholders or any kind of financial monitor.
[01:00:55.160 --> 01:00:57.560]   But second, most corporations just shouldn't live
[01:00:57.560 --> 01:00:58.400]   that long, right?
[01:00:58.400 --> 01:01:01.000]   Most corporations, their ideas that you try out
[01:01:01.000 --> 01:01:03.360]   and then it like, you find out it doesn't work
[01:01:03.360 --> 01:01:04.960]   or it should be bought up by somebody else
[01:01:04.960 --> 01:01:07.080]   or the IP should be sold off.
[01:01:07.080 --> 01:01:09.960]   And so having a lot of companies fade out
[01:01:09.960 --> 01:01:12.400]   is actually on net a good sign.
[01:01:12.400 --> 01:01:15.320]   I think this is really part of the John Haltewanger
[01:01:15.320 --> 01:01:17.320]   line of research that the sort of modern version
[01:01:17.320 --> 01:01:19.160]   of creative destruction research,
[01:01:19.160 --> 01:01:22.840]   which finds that, you know, low productivity firms exiting,
[01:01:22.840 --> 01:01:25.880]   you know, just as naive laissez-faire predicts,
[01:01:25.880 --> 01:01:28.080]   means that those workers and that capital
[01:01:28.080 --> 01:01:30.520]   can get reallocated over to more productive firm.
[01:01:30.520 --> 01:01:33.600]   So the alternative is, you know,
[01:01:33.600 --> 01:01:36.360]   stereotypically Japanese zombie firms, right?
[01:01:36.360 --> 01:01:38.200]   They're kept limping along by banks
[01:01:38.200 --> 01:01:40.440]   that are perhaps under political pressure to lend.
[01:01:40.440 --> 01:01:43.200]   And so a lot of human and physical capital
[01:01:43.200 --> 01:01:45.760]   gets tied up in low productivity projects.
[01:01:45.760 --> 01:01:50.480]   So yeah, a brutal bond market is a good way
[01:01:50.480 --> 01:01:52.960]   to send a market signal to move capital
[01:01:52.960 --> 01:01:56.160]   from low productivity to high productivity projects.
[01:01:56.160 --> 01:02:01.160]   - Why are yields on 30-year fixed treasuries,
[01:02:01.160 --> 01:02:02.480]   why are they so low?
[01:02:02.480 --> 01:02:04.560]   Because theoretically investors should know
[01:02:04.560 --> 01:02:06.480]   that we have a lot of liabilities
[01:02:06.480 --> 01:02:09.360]   in the form of, you know, social security to baby boomers,
[01:02:09.360 --> 01:02:13.160]   and we've radically inflated the money supply very recently
[01:02:13.160 --> 01:02:14.920]   and may do so again.
[01:02:14.920 --> 01:02:16.520]   Do you think the investors are being irrational
[01:02:16.520 --> 01:02:19.000]   with the low yields or what's going on?
[01:02:19.000 --> 01:02:20.240]   - No, no, I'm a fan of the view
[01:02:20.240 --> 01:02:22.440]   that the bondholders are gonna win in the long run,
[01:02:22.440 --> 01:02:25.280]   and that inflation, any kind of super,
[01:02:25.280 --> 01:02:27.160]   like super, super high inflation,
[01:02:27.160 --> 01:02:31.080]   is not gonna be the path of the future.
[01:02:31.080 --> 01:02:32.800]   And what's gonna happen is that,
[01:02:32.800 --> 01:02:34.480]   at least think about the U.S.,
[01:02:34.480 --> 01:02:37.120]   the way the bondholders are gonna win
[01:02:37.120 --> 01:02:40.400]   is that there's gonna be a mixture of tax hikes
[01:02:40.400 --> 01:02:44.880]   and slower spending growth, especially hurting the poor,
[01:02:44.880 --> 01:02:47.720]   and that's how the U.S. is gonna close its fiscal gap.
[01:02:47.720 --> 01:02:48.800]   I don't know particularly what paths
[01:02:48.800 --> 01:02:49.920]   other countries are gonna go through,
[01:02:49.920 --> 01:02:52.280]   but the U.S. has this basically,
[01:02:52.280 --> 01:02:55.960]   this one tool, this one superpower,
[01:02:55.960 --> 01:02:59.720]   sitting in the room that it hasn't used yet,
[01:02:59.720 --> 01:03:02.080]   and it's a VAT, right?
[01:03:02.080 --> 01:03:05.920]   So the U.S. could dramatically increase its tax revenue
[01:03:05.920 --> 01:03:09.600]   through either an overt or disguised value-added tax,
[01:03:09.600 --> 01:03:10.760]   and that would raise a ton of money,
[01:03:10.760 --> 01:03:12.680]   just like it raises in Europe, and that would,
[01:03:12.680 --> 01:03:14.800]   that's the easy way to close the U.S. fiscal gap.
[01:03:14.800 --> 01:03:16.600]   We probably won't even have to get to that.
[01:03:16.600 --> 01:03:20.240]   Just making Medicaid worse slowly over the long run,
[01:03:20.240 --> 01:03:24.320]   maybe making Medicare worse over the long run,
[01:03:24.320 --> 01:03:27.760]   that by itself would close a lot of this fiscal gap.
[01:03:27.760 --> 01:03:30.160]   So basically, I think they'll balance the long-run budget
[01:03:30.160 --> 01:03:32.480]   on the kind of the backs of the poor and the middle class,
[01:03:32.480 --> 01:03:34.840]   that's probably the most likely outcome.
[01:03:34.840 --> 01:03:35.680]   - So are you expecting--
[01:03:35.680 --> 01:03:38.320]   - Hence, no hyperinflation, so.
[01:03:38.320 --> 01:03:40.200]   - But so you're expecting the welfare state
[01:03:40.200 --> 01:03:43.520]   to shrink either in quality or quantity over time?
[01:03:43.520 --> 01:03:46.440]   - In relative terms, compared to the trend.
[01:03:46.440 --> 01:03:48.680]   I mean, if America's still getting, say, 1%,
[01:03:48.680 --> 01:03:50.240]   1.5% richer each year,
[01:03:50.240 --> 01:03:54.440]   then that by itself means that,
[01:03:54.440 --> 01:03:58.280]   that adds a certain level of quality
[01:03:58.280 --> 01:04:00.200]   to healthcare over time.
[01:04:00.200 --> 01:04:02.760]   And so basically, if it ends up staying
[01:04:02.760 --> 01:04:06.360]   the equivalent of, say, 0%, that by itself,
[01:04:06.360 --> 01:04:08.280]   you get healthcare spending in real terms
[01:04:08.280 --> 01:04:09.880]   to be 0% over time,
[01:04:09.880 --> 01:04:14.000]   that would end up closing the gap
[01:04:14.000 --> 01:04:15.720]   when you compound it out long enough, right?
[01:04:15.720 --> 01:04:17.360]   - So yep, yep.
[01:04:17.360 --> 01:04:18.880]   - That kind of thinking.
[01:04:18.880 --> 01:04:21.560]   - So, you know, when List Trust in the U.K.
[01:04:21.560 --> 01:04:23.840]   tried to implement tax reform,
[01:04:23.840 --> 01:04:25.480]   there was a bondholder revolt,
[01:04:25.480 --> 01:04:26.960]   and, you know, she was ousted.
[01:04:26.960 --> 01:04:27.800]   - Yeah.
[01:04:27.800 --> 01:04:28.960]   - Bondholders won right there.
[01:04:28.960 --> 01:04:29.800]   - Yeah.
[01:04:29.800 --> 01:04:33.120]   - So do we already live in the bondholder utopia, or?
[01:04:33.120 --> 01:04:33.960]   - Oh, yeah.
[01:04:33.960 --> 01:04:35.960]   I mean, I think that that was a nice reminder
[01:04:35.960 --> 01:04:39.480]   that basically, contrary to sort of the MMT view,
[01:04:39.480 --> 01:04:42.920]   and the sort of pop Keynesian view,
[01:04:42.920 --> 01:04:44.840]   the debt is no barrier at all,
[01:04:44.840 --> 01:04:49.160]   I think that showed the bondholders
[01:04:49.160 --> 01:04:53.560]   are actually paying attention to long-term signals
[01:04:53.560 --> 01:04:56.640]   of fiscal policy credibility, and they'll take action, yeah.
[01:04:56.640 --> 01:04:57.480]   - Mm-hmm.
[01:04:57.480 --> 01:04:58.520]   - Yes.
[01:04:58.520 --> 01:05:00.480]   - What are the deep roots of Mormonism?
[01:05:00.480 --> 01:05:01.960]   Why do they have such high trust
[01:05:01.960 --> 01:05:04.840]   and such tight-knit communities?
[01:05:04.840 --> 01:05:07.000]   - I mean, I think part of it is that they are,
[01:05:07.000 --> 01:05:10.440]   they reflect a lot of this sort of,
[01:05:10.440 --> 01:05:12.680]   what for then was Western pioneer culture,
[01:05:12.680 --> 01:05:15.840]   upstate New York, Pennsylvania, then Ohio.
[01:05:15.840 --> 01:05:19.760]   I think those communities tended to be
[01:05:19.760 --> 01:05:21.360]   high-trust communities necessarily,
[01:05:21.360 --> 01:05:23.200]   because of the difficult environment
[01:05:23.200 --> 01:05:24.440]   that they were living in.
[01:05:25.960 --> 01:05:28.920]   And there was a lot of selection.
[01:05:28.920 --> 01:05:31.000]   There was a lot of selection over the first few decades
[01:05:31.000 --> 01:05:33.920]   of Mormon history, where those who were willing
[01:05:33.920 --> 01:05:36.120]   to sort of trust the group stayed in,
[01:05:36.120 --> 01:05:38.640]   and those who weren't willing to trust the group
[01:05:38.640 --> 01:05:39.960]   ended up leaving.
[01:05:39.960 --> 01:05:42.160]   And not just trust, but trustworthiness.
[01:05:42.160 --> 01:05:43.880]   I think a lot of people probably got weeded out
[01:05:43.880 --> 01:05:46.760]   because they weren't contributing to the common good.
[01:05:46.760 --> 01:05:48.240]   So I think that basically by the time
[01:05:48.240 --> 01:05:50.360]   the Mormons got established in Utah,
[01:05:50.360 --> 01:05:55.240]   they had already selected for a strong culture
[01:05:55.240 --> 01:06:00.240]   of a kind of in-group pro-sociality.
[01:06:00.240 --> 01:06:05.760]   And I think that helped them weather the storms
[01:06:05.760 --> 01:06:07.120]   of the 19th century.
[01:06:07.120 --> 01:06:09.840]   So the whole 19th century, people were only joining
[01:06:09.840 --> 01:06:12.880]   during the era of polygamy in Utah, right?
[01:06:12.880 --> 01:06:13.840]   If they thought that they were willing
[01:06:13.840 --> 01:06:14.760]   to put up with this, right?
[01:06:14.760 --> 01:06:16.200]   And you know you're signing up
[01:06:16.200 --> 01:06:19.640]   for some kind of deep sociality
[01:06:19.640 --> 01:06:22.400]   with a mixture of a lot of unconventional stuff.
[01:06:22.400 --> 01:06:26.040]   And that foundation really helped.
[01:06:26.040 --> 01:06:28.240]   And the fact that Mormons since then have stayed
[01:06:28.240 --> 01:06:31.600]   as a religion that requires a medium-high level
[01:06:31.600 --> 01:06:33.880]   of commitment also weeds out people
[01:06:33.880 --> 01:06:36.400]   who just aren't willing to make that kind of commitment.
[01:06:36.400 --> 01:06:38.440]   So, I mean, I was raised Mormon.
[01:06:38.440 --> 01:06:41.000]   I wasn't ultimately willing to make the commitment,
[01:06:41.000 --> 01:06:41.840]   and maybe part of the reason is
[01:06:41.840 --> 01:06:43.080]   'cause I'm too much of a free rider.
[01:06:43.080 --> 01:06:45.680]   So Mormons who are left are probably better than me.
[01:06:45.680 --> 01:06:48.040]   - The Mormon church has, I think,
[01:06:48.040 --> 01:06:50.400]   more than $100 billion in assets.
[01:06:51.400 --> 01:06:52.880]   What is it planning on doing with all this money?
[01:06:52.880 --> 01:06:56.120]   That's a tremendous sum.
[01:06:56.120 --> 01:06:56.960]   - I don't know.
[01:06:56.960 --> 01:06:58.720]   I mean, maybe they're planning to hand it to the Savior
[01:06:58.720 --> 01:07:01.360]   when the second coming happens, right?
[01:07:01.360 --> 01:07:05.000]   It's, there's gotta be a great argument
[01:07:05.000 --> 01:07:08.080]   for this option value of just having the wealth there, right?
[01:07:08.080 --> 01:07:11.920]   It must give them a kind of independence from the world
[01:07:11.920 --> 01:07:16.000]   when various storms come along, cultural, political storms.
[01:07:16.000 --> 01:07:20.400]   I don't actually know what their plans are
[01:07:20.400 --> 01:07:22.240]   for what they would do with all the money,
[01:07:22.240 --> 01:07:24.760]   but I do know that like normal economics tells us
[01:07:24.760 --> 01:07:28.080]   that just being frugal is good for the economy overall.
[01:07:28.080 --> 01:07:31.040]   So Steven Landsberg has a great essay in "Praise of Scrooge."
[01:07:31.040 --> 01:07:33.240]   It's especially appropriate for this time of year.
[01:07:33.240 --> 01:07:37.200]   And being frugal means you're building up the capital stock
[01:07:37.200 --> 01:07:39.360]   and you're giving a sort of invisible gift
[01:07:39.360 --> 01:07:40.840]   to future generations.
[01:07:40.840 --> 01:07:43.880]   So Mormon frugality is basically helping build up
[01:07:43.880 --> 01:07:45.240]   the U.S. capital stock
[01:07:45.240 --> 01:07:47.200]   and indirectly the world's capital stock,
[01:07:47.200 --> 01:07:48.520]   helping make us all more productive,
[01:07:48.520 --> 01:07:52.240]   which I think is something that fits in with Mormon values.
[01:07:52.240 --> 01:07:53.080]   - Yeah, yeah.
[01:07:53.080 --> 01:07:54.200]   I think people have pointed out
[01:07:54.200 --> 01:07:56.240]   that people should be spending more money
[01:07:56.240 --> 01:07:58.440]   given the fact that they have so much leftover
[01:07:58.440 --> 01:08:00.080]   by the time they die, usually.
[01:08:00.080 --> 01:08:01.600]   - Yeah, at an individual level,
[01:08:01.600 --> 01:08:04.000]   if you care about your own wellbeing, that's true, yeah.
[01:08:04.000 --> 01:08:06.400]   - Yeah, yeah, but okay, so it's interesting.
[01:08:06.400 --> 01:08:10.040]   Like leaving a large inheritance is socially valuable.
[01:08:10.040 --> 01:08:11.920]   - Yeah, leaving a large inheritance means there's,
[01:08:11.920 --> 01:08:14.320]   you're leaving, you're producing a lot,
[01:08:14.320 --> 01:08:15.520]   but you're not consuming very much.
[01:08:15.520 --> 01:08:17.760]   So that means you're building up the capital stock, so.
[01:08:17.760 --> 01:08:18.600]   - Yep, yep.
[01:08:18.600 --> 01:08:22.400]   And there's also, I think,
[01:08:22.400 --> 01:08:26.680]   a large amount of multi-level marketing schemes
[01:08:26.680 --> 01:08:28.400]   that proliferate in Utah.
[01:08:28.400 --> 01:08:29.240]   - Yeah.
[01:08:29.240 --> 01:08:31.320]   - Is that one of the downsides of high social trust?
[01:08:31.320 --> 01:08:33.080]   - Yeah, 'cause people predate upon it, right?
[01:08:33.080 --> 01:08:35.720]   It strikes me as a total red seeker sort of thing.
[01:08:35.720 --> 01:08:37.600]   I mean, I have to say the knives are really good.
[01:08:37.600 --> 01:08:41.200]   So I'm, everybody I know who's ever had Cutco knives
[01:08:41.200 --> 01:08:42.320]   ends up using a protected.
[01:08:42.320 --> 01:08:44.720]   So that's one of the popular multi-level marketing schemes
[01:08:44.720 --> 01:08:46.560]   that actually gets to men.
[01:08:46.560 --> 01:08:48.080]   A lot of them are targeted at women
[01:08:48.080 --> 01:08:51.000]   through cosmetics, as you might know, so.
[01:08:51.000 --> 01:08:52.720]   But at least the men's one, it works out.
[01:08:52.720 --> 01:08:55.400]   - If you had to implement an immigration system from scratch,
[01:08:55.400 --> 01:08:59.560]   would you actually consider these SAT scores
[01:08:59.560 --> 01:09:01.200]   and these other deep root scores
[01:09:01.200 --> 01:09:02.960]   as part of somebody's admittance?
[01:09:02.960 --> 01:09:05.680]   Or would you just consider the individual level,
[01:09:05.680 --> 01:09:08.640]   your personal skills and education and things like that?
[01:09:08.640 --> 01:09:10.240]   - No, I'd wanna launch a 10 year,
[01:09:10.240 --> 01:09:11.760]   maybe 20 year research project
[01:09:11.760 --> 01:09:13.600]   to figuring how to turn the deep root scores
[01:09:13.600 --> 01:09:15.000]   into something useful.
[01:09:15.120 --> 01:09:18.480]   So I think right now with the deep roots literature,
[01:09:18.480 --> 01:09:20.800]   we're about where Milton Friedman's monetarism was
[01:09:20.800 --> 01:09:22.280]   in the late 60s.
[01:09:22.280 --> 01:09:23.840]   You know, Friedman said,
[01:09:23.840 --> 01:09:26.160]   "Hey, I figured out where inflation comes from
[01:09:26.160 --> 01:09:27.280]   "and we'd be a lot better off
[01:09:27.280 --> 01:09:30.120]   "if we just grew the money supply 3% a year."
[01:09:30.120 --> 01:09:33.760]   And ultimately nobody thought he was right
[01:09:33.760 --> 01:09:35.480]   about the 3% a year thing.
[01:09:35.480 --> 01:09:39.200]   But they did think that he still had a lot of good advice.
[01:09:39.200 --> 01:09:41.840]   So Friedman ended up having a lot of good ideas,
[01:09:41.840 --> 01:09:44.080]   but they weren't policy ready.
[01:09:44.080 --> 01:09:45.160]   And I think that's about where we are
[01:09:45.160 --> 01:09:47.440]   with the deep roots literature right now,
[01:09:47.440 --> 01:09:50.560]   which is, I mean, at most one would use it
[01:09:50.560 --> 01:09:53.160]   as like a small plus factor in a point system,
[01:09:53.160 --> 01:09:55.160]   but I don't even know which points I'd use.
[01:09:55.160 --> 01:09:57.880]   But something along those lines is worth thinking about.
[01:09:57.880 --> 01:10:02.880]   I would never use any quotas or hard cutoffs.
[01:10:02.880 --> 01:10:05.880]   If you think about points-based systems,
[01:10:05.880 --> 01:10:08.400]   10, 20 years of further research
[01:10:08.400 --> 01:10:10.280]   and maybe you'd find a way to put the deep roots
[01:10:10.280 --> 01:10:12.400]   into a points-based system.
[01:10:12.400 --> 01:10:13.240]   - Yeah, yeah.
[01:10:14.120 --> 01:10:15.360]   How was the SPF thing?
[01:10:15.360 --> 01:10:17.800]   - What do you mean?
[01:10:17.800 --> 01:10:20.680]   - So did he just say like, "I'll fly you out"?
[01:10:20.680 --> 01:10:23.440]   - So I was there for the EA Bahamas.
[01:10:23.440 --> 01:10:24.960]   - Oh, okay, okay.
[01:10:24.960 --> 01:10:26.400]   - And then while I was there,
[01:10:26.400 --> 01:10:29.680]   I talked to somebody who knew him and I'm like,
[01:10:29.680 --> 01:10:31.120]   "Hey, I would love to interview him."
[01:10:31.120 --> 01:10:32.800]   - Yeah, yeah, yeah. - I would ask him.
[01:10:32.800 --> 01:10:34.240]   - Yeah. - Yeah, yeah.
[01:10:34.240 --> 01:10:35.760]   It was one of the ones where I actually had to feel
[01:10:35.760 --> 01:10:37.920]   like I would really want to redo that one
[01:10:37.920 --> 01:10:40.400]   because I was aware of some things back then
[01:10:40.400 --> 01:10:41.720]   that were kind of,
[01:10:41.720 --> 01:10:43.600]   that would be worth asking about in retrospect.
[01:10:43.600 --> 01:10:44.800]   And of course it's all in retrospect,
[01:10:44.800 --> 01:10:47.320]   but I should have poked harder at that
[01:10:47.320 --> 01:10:49.920]   rather than asking these sort of philosophical questions
[01:10:49.920 --> 01:10:51.520]   about effective altruism.
[01:10:51.520 --> 01:10:52.360]   - Yeah.
[01:10:52.360 --> 01:10:54.040]   Do you think, I mean, is it as simple
[01:10:54.040 --> 01:10:56.600]   as like he was co-mingling fun?
[01:10:56.600 --> 01:11:00.240]   He lent a bunch of FTX money over to a hedge fund
[01:11:00.240 --> 01:11:01.320]   and then they lost it?
[01:11:01.320 --> 01:11:04.360]   - Yeah, I'm guessing.
[01:11:04.360 --> 01:11:05.440]   - First approximation?
[01:11:05.440 --> 01:11:06.520]   - Yeah, yeah, yeah.
[01:11:06.520 --> 01:11:08.880]   - So it's like, it's old fashioned financial fraud
[01:11:08.880 --> 01:11:12.360]   partly driven by not having a really good board
[01:11:12.360 --> 01:11:14.480]   over him or a good oversight.
[01:11:14.480 --> 01:11:15.320]   - You know what?
[01:11:15.320 --> 01:11:17.040]   I'm really curious to ask you this
[01:11:17.040 --> 01:11:19.480]   because, you know, you're talking hive mind
[01:11:19.480 --> 01:11:23.360]   about the fact that higher IQ people on average
[01:11:23.360 --> 01:11:25.880]   are more cooperative in prisoner's dilemma type situations.
[01:11:25.880 --> 01:11:26.720]   - Yeah, yeah, yeah.
[01:11:26.720 --> 01:11:29.640]   - And I just interviewed Bethany McLean on the podcast.
[01:11:29.640 --> 01:11:31.080]   It hasn't been released yet,
[01:11:31.080 --> 01:11:32.760]   but you know, she wrote "The Smartest Guys in the Room,"
[01:11:32.760 --> 01:11:34.000]   which about Enron, right?
[01:11:34.000 --> 01:11:37.440]   And there is this thing where maybe they are less likely
[01:11:37.440 --> 01:11:39.600]   to commit fraud on average, but when they do,
[01:11:39.600 --> 01:11:40.440]   they're so--
[01:11:40.440 --> 01:11:41.360]   - They're really good at it, right?
[01:11:41.360 --> 01:11:42.440]   - Exactly, right?
[01:11:42.440 --> 01:11:44.640]   So yeah, I don't know.
[01:11:44.640 --> 01:11:47.560]   Maybe one of the downsides of having a high IQ society
[01:11:47.560 --> 01:11:49.400]   is that white people do commit fraud.
[01:11:49.400 --> 01:11:51.160]   They're super successful at it.
[01:11:51.160 --> 01:11:52.000]   You know, I don't know.
[01:11:52.000 --> 01:11:55.400]   - Yeah, I mean, the evils of super smart people
[01:11:55.400 --> 01:11:59.120]   are obviously a huge risk to all of humanity, right?
[01:11:59.120 --> 01:11:59.960]   - Right.
[01:11:59.960 --> 01:12:01.400]   - I don't have to worry about humanity being wiped out
[01:12:01.400 --> 01:12:03.880]   by a bunch of people with sticks and stones, right?
[01:12:03.880 --> 01:12:05.560]   I have to worry about humanity being wiped out
[01:12:05.560 --> 01:12:07.960]   by nuclear weapons, which could only be invented
[01:12:07.960 --> 01:12:09.400]   by smart people.
[01:12:09.400 --> 01:12:11.400]   - Right, yeah, yeah, yeah.
[01:12:11.400 --> 01:12:14.640]   Are smart people more cooperative
[01:12:14.640 --> 01:12:19.080]   in a sort of very calculating sense
[01:12:19.080 --> 01:12:20.800]   that, you know, this game is gonna go on,
[01:12:20.800 --> 01:12:22.640]   so I wanna make sure I preserve my relationships?
[01:12:22.640 --> 01:12:24.200]   Or even in the last turn
[01:12:24.200 --> 01:12:26.400]   of a iterated prisoner's dilemma game,
[01:12:26.400 --> 01:12:28.280]   even in the last turn, they would cooperate.
[01:12:28.280 --> 01:12:30.760]   - No, in the last turn, they walk away, yeah.
[01:12:30.760 --> 01:12:31.600]   - Yeah, yeah.
[01:12:31.600 --> 01:12:35.000]   - Yeah, I think there is no correlation
[01:12:35.000 --> 01:12:38.840]   between intelligence and, say, agreeableness, right?
[01:12:38.840 --> 01:12:40.880]   Normal psychological agreeableness.
[01:12:40.880 --> 01:12:42.760]   And I think that's a broader principle.
[01:12:42.760 --> 01:12:44.600]   Or conscientiousness, for that matter, right?
[01:12:44.600 --> 01:12:46.320]   Psychological conscientiousness.
[01:12:46.320 --> 01:12:48.040]   So Machiavellian intelligence, I think,
[01:12:48.040 --> 01:12:51.240]   is what's driving the link between IQ and cooperation.
[01:12:51.240 --> 01:12:54.720]   So in repeated game, that Machiavellian intelligence,
[01:12:54.720 --> 01:12:58.160]   which a lot of intelligence researchers will talk about,
[01:12:58.160 --> 01:13:00.040]   turns into Hosean intelligence,
[01:13:00.040 --> 01:13:01.880]   where people find a way to grow the pie.
[01:13:01.880 --> 01:13:04.680]   But it's a very cynical, self-interested form
[01:13:04.680 --> 01:13:06.360]   of growing the pie.
[01:13:06.360 --> 01:13:07.200]   - Yeah, yeah.
[01:13:07.200 --> 01:13:08.960]   - And so I don't think that has any,
[01:13:08.960 --> 01:13:13.040]   I don't think it's driven by inherent pro-sociality.
[01:13:13.040 --> 01:13:16.120]   I think it's endogenous pro-sociality,
[01:13:16.120 --> 01:13:18.080]   not exogenous pro-sociality.
[01:13:18.080 --> 01:13:19.800]   And that's a reason to worry about it.
[01:13:19.800 --> 01:13:22.200]   - What happens to these high IQ people
[01:13:22.200 --> 01:13:25.160]   when, if society goes into a sort of zero-sum mode,
[01:13:25.160 --> 01:13:26.960]   where there's not that much economic growth,
[01:13:26.960 --> 01:13:29.120]   and so the only way you can increase your share of the pie
[01:13:29.120 --> 01:13:31.840]   is just by cutting out bigger and bigger slices
[01:13:31.840 --> 01:13:32.680]   of yourself?
[01:13:32.680 --> 01:13:34.520]   - Yeah, then you gotta watch out, right?
[01:13:34.520 --> 01:13:36.400]   It's like the Middle Ages right there, right?
[01:13:36.400 --> 01:13:37.240]   - Yep.
[01:13:37.240 --> 01:13:38.080]   - Yeah, yeah.
[01:13:38.080 --> 01:13:40.560]   - Yeah, interesting.
[01:13:40.560 --> 01:13:42.480]   Awesome.
[01:13:42.480 --> 01:13:44.240]   Garrett, thanks so much for coming on the podcast.
[01:13:44.240 --> 01:13:45.360]   This was interesting.
[01:13:45.360 --> 01:13:47.320]   - Thanks for having me, it's been fantastic.
[01:13:47.320 --> 01:13:48.160]   - Yeah, excellent.
[01:13:48.160 --> 01:13:49.760]   - Thanks for reading my books, appreciate it.
[01:13:49.760 --> 01:13:50.680]   - Yeah, of course.
[01:13:50.680 --> 01:13:53.280]   (upbeat music)
[01:13:53.280 --> 01:13:55.880]   (upbeat music)
[01:13:55.880 --> 01:14:05.880]   [BLANK_AUDIO]

