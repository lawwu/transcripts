
[00:00:00.000 --> 00:00:03.840]   What possible ideas do you have for how a human species ends?
[00:00:03.840 --> 00:00:06.460]   - Sure, so I think the most obvious way to me
[00:00:06.460 --> 00:00:08.120]   is wireheading.
[00:00:08.120 --> 00:00:10.020]   We end up amusing ourselves to death.
[00:00:10.020 --> 00:00:14.400]   We end up all staring at that infinite TikTok
[00:00:14.400 --> 00:00:15.700]   and forgetting to eat.
[00:00:15.700 --> 00:00:19.240]   Maybe it's even more benign than this.
[00:00:19.240 --> 00:00:21.340]   Maybe we all just stop reproducing.
[00:00:21.340 --> 00:00:27.320]   Now, to be fair, it's probably hard to get all of humanity.
[00:00:27.320 --> 00:00:29.520]   - Yeah, the interesting thing about humanity
[00:00:29.520 --> 00:00:31.200]   is the diversity in it.
[00:00:31.200 --> 00:00:32.600]   Organisms in general.
[00:00:32.600 --> 00:00:34.760]   There's a lot of weirdos out there.
[00:00:34.760 --> 00:00:35.960]   Two of them are sitting here.
[00:00:35.960 --> 00:00:38.320]   - I mean, diversity in humanity is--
[00:00:38.320 --> 00:00:39.160]   - With due respect.
[00:00:39.160 --> 00:00:40.880]   (both laughing)
[00:00:40.880 --> 00:00:42.160]   - I wish I was more weird.
[00:00:42.160 --> 00:00:47.480]   - The following is a conversation with George Hotz.
[00:00:47.480 --> 00:00:49.360]   His third time on this podcast.
[00:00:49.360 --> 00:00:51.520]   He's the founder of Kama.ai
[00:00:51.520 --> 00:00:53.500]   that seeks to solve autonomous driving
[00:00:53.500 --> 00:00:57.800]   and is the founder of a new company called TinyCorp
[00:00:57.800 --> 00:01:01.380]   that created TinyGrad, a neural network framework
[00:01:01.380 --> 00:01:02.920]   that is extremely simple
[00:01:02.920 --> 00:01:05.880]   with the goal of making it run on any device
[00:01:05.880 --> 00:01:09.840]   by any human easily and efficiently.
[00:01:09.840 --> 00:01:12.640]   As you know, George also did a large number of fun
[00:01:12.640 --> 00:01:15.260]   and amazing things from hacking the iPhone
[00:01:15.260 --> 00:01:17.760]   to recently joining Twitter for a bit
[00:01:17.760 --> 00:01:20.440]   as an "intern" in quotes,
[00:01:20.440 --> 00:01:23.680]   making the case for refactoring the Twitter code base.
[00:01:23.680 --> 00:01:26.800]   In general, he's a fascinating engineer and human being
[00:01:26.800 --> 00:01:29.600]   and one of my favorite people to talk to.
[00:01:29.600 --> 00:01:31.640]   This is the Lex Friedman Podcast.
[00:01:31.640 --> 00:01:33.480]   To support it, please check out our sponsors
[00:01:33.480 --> 00:01:34.760]   in the description.
[00:01:34.760 --> 00:01:38.160]   And now, dear friends, here's George Hotz.
[00:01:38.160 --> 00:01:41.360]   You mentioned something in a stream
[00:01:41.360 --> 00:01:43.640]   about the philosophical nature of time.
[00:01:43.640 --> 00:01:45.920]   So let's start with a wild question.
[00:01:45.920 --> 00:01:47.660]   Do you think time is an illusion?
[00:01:47.660 --> 00:01:55.000]   - You know, I sell phone calls to Kama for $1,000
[00:01:55.360 --> 00:01:58.600]   and some guy called me and like,
[00:01:58.600 --> 00:02:01.240]   you know, it's $1,000, you can talk to me for half an hour.
[00:02:01.240 --> 00:02:05.920]   And he's like, "Yeah, okay, so like time doesn't exist
[00:02:05.920 --> 00:02:08.600]   "and I really wanted to share this with you."
[00:02:08.600 --> 00:02:10.960]   I'm like, "Oh, what do you mean time doesn't exist, right?
[00:02:10.960 --> 00:02:13.380]   "Like, I think time is a useful model,
[00:02:13.380 --> 00:02:15.120]   "whether it exists or not, right?
[00:02:15.120 --> 00:02:16.860]   "Like, does quantum physics exist?
[00:02:16.860 --> 00:02:18.120]   "Well, it doesn't matter.
[00:02:18.120 --> 00:02:22.000]   "It's about whether it's a useful model to describe reality.
[00:02:22.000 --> 00:02:25.160]   "Is time maybe compressive?"
[00:02:25.160 --> 00:02:27.040]   - Do you think there is an objective reality
[00:02:27.040 --> 00:02:29.020]   or is everything just useful models?
[00:02:29.020 --> 00:02:34.000]   Like underneath it all, is there an actual thing
[00:02:34.000 --> 00:02:35.800]   that we're constructing models for?
[00:02:35.800 --> 00:02:38.600]   - I don't know.
[00:02:38.600 --> 00:02:40.480]   - I was hoping you would know.
[00:02:40.480 --> 00:02:42.080]   - I don't think it matters.
[00:02:42.080 --> 00:02:44.440]   - I mean, this kind of connects to the models
[00:02:44.440 --> 00:02:47.760]   of constructive reality with machine learning, right?
[00:02:47.760 --> 00:02:48.600]   - Sure.
[00:02:48.600 --> 00:02:52.360]   - Like, is it just nice to have useful approximations
[00:02:52.360 --> 00:02:55.120]   of the world such that we can do something with it?
[00:02:55.120 --> 00:02:56.720]   - So there are things that are real.
[00:02:56.720 --> 00:02:58.440]   Kolmogorov complexity is real.
[00:02:58.440 --> 00:03:00.200]   - Yeah. - Yeah.
[00:03:00.200 --> 00:03:01.760]   The compressive thing-- - Math.
[00:03:01.760 --> 00:03:02.900]   - Math is real, yeah.
[00:03:02.900 --> 00:03:05.280]   - Should be a T-shirt.
[00:03:05.280 --> 00:03:06.960]   - And I think hard things are actually hard.
[00:03:06.960 --> 00:03:09.000]   I don't think P equals NP.
[00:03:09.000 --> 00:03:10.440]   - Ooh, strong words.
[00:03:10.440 --> 00:03:11.740]   - Well, I think that's the majority.
[00:03:11.740 --> 00:03:14.280]   I do think factoring is in P, but--
[00:03:14.280 --> 00:03:16.480]   - I don't think you're the person that follows the majority
[00:03:16.480 --> 00:03:18.280]   in all walks of life, so but it's good.
[00:03:18.280 --> 00:03:19.120]   - For that one, I do.
[00:03:19.120 --> 00:03:20.760]   - Yeah, in theoretical computer science,
[00:03:20.760 --> 00:03:22.380]   you're one of the sheep.
[00:03:23.960 --> 00:03:28.040]   All right, but to you, time is a useful model.
[00:03:28.040 --> 00:03:28.880]   - Sure.
[00:03:28.880 --> 00:03:32.200]   - What were you talking about on the stream with time?
[00:03:32.200 --> 00:03:33.320]   Are you made of time?
[00:03:33.320 --> 00:03:36.320]   - I remembered half the things I said on stream.
[00:03:36.320 --> 00:03:38.560]   Someday someone's gonna make a model of all of that
[00:03:38.560 --> 00:03:40.080]   and it's gonna come back to haunt me.
[00:03:40.080 --> 00:03:41.160]   - Someday soon?
[00:03:41.160 --> 00:03:42.000]   - Yeah, probably.
[00:03:42.000 --> 00:03:45.360]   - Would that be exciting to you or sad
[00:03:45.360 --> 00:03:47.360]   that there's a George Hotz model?
[00:03:47.360 --> 00:03:50.600]   - I mean, the question is when the George Hotz model
[00:03:50.600 --> 00:03:52.400]   is better than George Hotz.
[00:03:52.400 --> 00:03:54.920]   Like I am declining and the model is growing.
[00:03:54.920 --> 00:03:57.360]   - What is the metric by which you measure better or worse
[00:03:57.360 --> 00:04:00.080]   in that if you're competing with yourself?
[00:04:00.080 --> 00:04:02.280]   - Maybe you can just play a game
[00:04:02.280 --> 00:04:03.680]   where you have the George Hotz answer
[00:04:03.680 --> 00:04:04.820]   and the George Hotz model answer
[00:04:04.820 --> 00:04:06.760]   and ask which people prefer.
[00:04:06.760 --> 00:04:09.440]   - People close to you or strangers?
[00:04:09.440 --> 00:04:10.680]   - Either one, it will hurt more
[00:04:10.680 --> 00:04:11.700]   when it's people close to me,
[00:04:11.700 --> 00:04:15.280]   but both will be overtaken by the George Hotz model.
[00:04:15.280 --> 00:04:18.120]   - It'd be quite painful, right?
[00:04:18.120 --> 00:04:21.640]   Loved ones, family members would rather have the model
[00:04:21.640 --> 00:04:23.440]   over for Thanksgiving than you.
[00:04:23.440 --> 00:04:24.800]   - Yeah.
[00:04:24.800 --> 00:04:28.600]   - Or like significant others would rather sext
[00:04:28.600 --> 00:04:34.680]   with the large language model version of you.
[00:04:34.680 --> 00:04:38.420]   - Especially when it's fine tuned to their preferences.
[00:04:38.420 --> 00:04:39.360]   - Is it?
[00:04:39.360 --> 00:04:42.320]   Yeah, well, that's what we're doing in a relationship,
[00:04:42.320 --> 00:04:43.680]   right, we're just fine tuning ourselves,
[00:04:43.680 --> 00:04:45.120]   but we're inefficient with it
[00:04:45.120 --> 00:04:47.280]   'cause we're selfish and greedy and so on.
[00:04:47.280 --> 00:04:50.640]   Our language models can fine tune more efficiently,
[00:04:50.640 --> 00:04:51.760]   more selflessly.
[00:04:51.760 --> 00:04:53.440]   - There's a "Star Trek Voyager" episode
[00:04:53.440 --> 00:04:57.080]   where Catherine Janeway lost in the Delta Quadrant
[00:04:57.080 --> 00:05:00.200]   makes herself a lover on the holodeck.
[00:05:00.200 --> 00:05:04.600]   And the lover falls asleep on her arm
[00:05:04.600 --> 00:05:05.740]   and he snores a little bit
[00:05:05.740 --> 00:05:08.840]   and Janeway edits the program to remove that.
[00:05:08.840 --> 00:05:10.360]   And then of course the realization is,
[00:05:10.360 --> 00:05:12.400]   wait, this person's terrible.
[00:05:12.400 --> 00:05:16.300]   It is actually all their nuances and quirks
[00:05:16.300 --> 00:05:20.100]   and slight annoyances that make this relationship worthwhile.
[00:05:20.100 --> 00:05:22.040]   But I don't think we're gonna realize that
[00:05:22.040 --> 00:05:23.040]   until it's too late.
[00:05:23.040 --> 00:05:26.480]   - Well, I think a large language model
[00:05:26.480 --> 00:05:29.800]   could incorporate the flaws and the quirks
[00:05:29.800 --> 00:05:30.640]   and all that kind of stuff.
[00:05:30.640 --> 00:05:33.760]   - Just the perfect amount of quirks and flaws
[00:05:33.760 --> 00:05:36.120]   to make you charming without crossing the line.
[00:05:36.120 --> 00:05:38.000]   - Yeah, yeah.
[00:05:38.000 --> 00:05:41.520]   And that's probably a good approximation
[00:05:41.520 --> 00:05:46.520]   of the percent of time the language model should be cranky
[00:05:46.520 --> 00:05:51.520]   or an asshole or jealous or all this kind of stuff.
[00:05:51.520 --> 00:05:53.360]   - And of course it can and it will,
[00:05:53.360 --> 00:05:56.400]   but all that difficulty at that point is artificial.
[00:05:56.400 --> 00:05:58.760]   There's no more real difficulty.
[00:05:58.760 --> 00:06:01.280]   - Okay, what's the difference between real and artificial?
[00:06:01.280 --> 00:06:03.200]   - Artificial difficulty is difficulty
[00:06:03.200 --> 00:06:06.240]   that's like constructed or could be turned off with a knob.
[00:06:06.240 --> 00:06:08.400]   Real difficulty is like you're in the woods
[00:06:08.400 --> 00:06:09.500]   and you gotta survive.
[00:06:09.500 --> 00:06:14.120]   - So if something can not be turned off with a knob,
[00:06:14.120 --> 00:06:14.960]   it's real?
[00:06:16.120 --> 00:06:17.280]   - Yeah, I think so.
[00:06:17.280 --> 00:06:19.720]   Or, I mean, you can't get out of this
[00:06:19.720 --> 00:06:22.000]   by smashing the knob with a hammer.
[00:06:22.000 --> 00:06:24.360]   I mean, maybe you kind of can.
[00:06:24.360 --> 00:06:29.200]   Into the wild when, you know, Alexander Supertramp,
[00:06:29.200 --> 00:06:30.320]   he wants to explore something
[00:06:30.320 --> 00:06:31.840]   that's never been explored before,
[00:06:31.840 --> 00:06:33.560]   but it's the '90s, everything's been explored.
[00:06:33.560 --> 00:06:35.920]   So he's like, well, I'm just not gonna bring a map.
[00:06:35.920 --> 00:06:36.760]   - Yeah.
[00:06:36.760 --> 00:06:40.000]   - I mean, no, you're not exploring.
[00:06:40.000 --> 00:06:41.480]   You should have brought a map, dude, you died.
[00:06:41.480 --> 00:06:44.080]   There was a bridge a mile from where you were camping.
[00:06:44.080 --> 00:06:46.600]   - How does that connect to the metaphor of the knob?
[00:06:46.600 --> 00:06:50.460]   - By not bringing the map, you didn't become an explorer.
[00:06:50.460 --> 00:06:53.320]   You just smashed the thing.
[00:06:53.320 --> 00:06:54.520]   - Yeah. - Yeah.
[00:06:54.520 --> 00:06:56.920]   The difficulty is still artificial.
[00:06:56.920 --> 00:06:58.460]   - You failed before you started.
[00:06:58.460 --> 00:07:00.840]   What if we just don't have access to the knob?
[00:07:00.840 --> 00:07:03.640]   - Well, that maybe is even scarier, right?
[00:07:03.640 --> 00:07:05.520]   Like we already exist in a world of nature
[00:07:05.520 --> 00:07:08.160]   and nature has been fine-tuned over billions of years
[00:07:09.480 --> 00:07:14.480]   to have humans build something
[00:07:14.480 --> 00:07:17.560]   and then throw the knob away
[00:07:17.560 --> 00:07:19.820]   in some grand romantic gesture is horrifying.
[00:07:19.820 --> 00:07:23.040]   - Do you think of us humans as individuals
[00:07:23.040 --> 00:07:24.800]   that are like born and die,
[00:07:24.800 --> 00:07:28.800]   or is it, are we just all part of one living organism
[00:07:28.800 --> 00:07:31.860]   that is Earth, that is nature?
[00:07:31.860 --> 00:07:35.520]   - I don't think there's a clear line there.
[00:07:35.520 --> 00:07:37.680]   I think it's all kind of just fuzzy.
[00:07:37.680 --> 00:07:39.800]   I don't know, I mean, I don't think I'm conscious.
[00:07:39.800 --> 00:07:41.080]   I don't think I'm anything.
[00:07:41.080 --> 00:07:44.020]   I think I'm just a computer program.
[00:07:44.020 --> 00:07:45.480]   - So it's all computation.
[00:07:45.480 --> 00:07:46.320]   - Yeah. - Everything running
[00:07:46.320 --> 00:07:49.240]   in your head is just computation.
[00:07:49.240 --> 00:07:51.680]   - Everything running in the universe is computation, I think.
[00:07:51.680 --> 00:07:54.680]   I believe the extended church-turning thesis.
[00:07:54.680 --> 00:07:57.320]   - Yeah, but there seems to be an embodiment
[00:07:57.320 --> 00:08:00.920]   to your particular computation, like there's a consistency.
[00:08:00.920 --> 00:08:03.620]   - Well, yeah, but I mean, models have consistency too.
[00:08:03.620 --> 00:08:06.400]   - Yeah. - Models that have been
[00:08:06.400 --> 00:08:09.560]   RLHF'd will continually say, you know, like,
[00:08:09.560 --> 00:08:11.680]   well, how do I murder ethnic minorities?
[00:08:11.680 --> 00:08:13.320]   Oh, well, I can't let you do that, Al.
[00:08:13.320 --> 00:08:15.680]   There's a consistency to that behavior.
[00:08:15.680 --> 00:08:16.520]   - It's all RLHF.
[00:08:16.520 --> 00:08:20.560]   Like we all RLHF each other.
[00:08:20.560 --> 00:08:23.920]   We provide human feedback,
[00:08:23.920 --> 00:08:28.840]   and thereby fine-tune these little pockets of computation,
[00:08:28.840 --> 00:08:31.640]   but it's still unclear why that pocket of computation
[00:08:31.640 --> 00:08:33.640]   stays with you, like for years.
[00:08:33.640 --> 00:08:35.120]   It just kind of falls.
[00:08:35.120 --> 00:08:40.120]   Like you have this consistent set of physics, biology,
[00:08:40.120 --> 00:08:46.680]   what, like whatever you call the neurons firing,
[00:08:46.680 --> 00:08:48.680]   like the electrical signals and mechanical signals,
[00:08:48.680 --> 00:08:50.320]   all of that, that seems to stay there,
[00:08:50.320 --> 00:08:52.960]   and it contains information, it stores information,
[00:08:52.960 --> 00:08:55.680]   and that information permeates through time
[00:08:55.680 --> 00:08:57.740]   and stays with you.
[00:08:57.740 --> 00:08:59.400]   There's like memory.
[00:08:59.400 --> 00:09:00.800]   It's like sticky.
[00:09:00.800 --> 00:09:02.980]   - Okay, to be fair, like a lot of the models
[00:09:02.980 --> 00:09:04.200]   we're building today are very,
[00:09:04.200 --> 00:09:07.020]   even RLHF is nowhere near as complex
[00:09:07.020 --> 00:09:08.060]   as the human loss function.
[00:09:08.060 --> 00:09:10.300]   - Reinforcement learning with human feedback.
[00:09:10.300 --> 00:09:14.580]   - You know, when I talked about will GPT-12 be AGI,
[00:09:14.580 --> 00:09:15.720]   my answer is no, of course not.
[00:09:15.720 --> 00:09:18.320]   I mean, cross-entropy loss is never gonna get you there.
[00:09:18.320 --> 00:09:23.320]   You need probably RL in fancy environments
[00:09:23.320 --> 00:09:25.540]   in order to get something that would be considered like,
[00:09:25.540 --> 00:09:26.540]   AGI-like.
[00:09:26.540 --> 00:09:30.140]   So to ask like the question about like why,
[00:09:30.140 --> 00:09:32.560]   I don't know, like it's just some quirk of evolution,
[00:09:32.560 --> 00:09:35.500]   right, I don't think there's anything particularly special
[00:09:35.500 --> 00:09:40.500]   about where I ended up, where humans ended up.
[00:09:40.500 --> 00:09:43.820]   - So okay, we have human-level intelligence.
[00:09:43.820 --> 00:09:47.580]   Would you call that AGI, whatever we have, GI?
[00:09:47.580 --> 00:09:50.780]   - Look, actually, I don't really even like the word AGI,
[00:09:50.780 --> 00:09:53.220]   but general intelligence is defined
[00:09:53.220 --> 00:09:54.780]   to be whatever humans have.
[00:09:54.780 --> 00:09:59.340]   - Okay, so why can GPT-12 not get us to AGI?
[00:09:59.340 --> 00:10:01.160]   Can we just like linger on that?
[00:10:02.180 --> 00:10:04.380]   - If your loss function is categorical cross-entropy,
[00:10:04.380 --> 00:10:07.500]   if your loss function is just try to maximize compression,
[00:10:07.500 --> 00:10:09.960]   I have a SoundCloud, I rap,
[00:10:09.960 --> 00:10:13.180]   and I tried to get ChatGPT to help me write raps,
[00:10:13.180 --> 00:10:14.620]   and the raps that it wrote
[00:10:14.620 --> 00:10:16.180]   sounded like YouTube comment raps.
[00:10:16.180 --> 00:10:18.260]   You know, you can go on any rap beat online
[00:10:18.260 --> 00:10:20.300]   and you can see what people put in the comments,
[00:10:20.300 --> 00:10:23.820]   and it's the most like mid-quality rap you can find.
[00:10:23.820 --> 00:10:24.820]   - Is mid good or bad?
[00:10:24.820 --> 00:10:25.660]   - Mid is bad.
[00:10:25.660 --> 00:10:27.780]   - Mid is bad. - It's like mid, it's like--
[00:10:27.780 --> 00:10:29.900]   - Every time I talk to you, I learn new words.
[00:10:29.900 --> 00:10:31.420]   (laughing)
[00:10:31.420 --> 00:10:33.580]   - Mid. - Mid, yeah.
[00:10:33.580 --> 00:10:36.140]   - I was like, is it like basic?
[00:10:36.140 --> 00:10:37.260]   Is that what mid means?
[00:10:37.260 --> 00:10:39.980]   - Kind of, it's like middle of the curve, right?
[00:10:39.980 --> 00:10:42.880]   So there's like that intelligence curve,
[00:10:42.880 --> 00:10:45.900]   and you have like the dumb guy, the smart guy,
[00:10:45.900 --> 00:10:46.740]   and then the mid guy.
[00:10:46.740 --> 00:10:48.260]   Actually, being the mid guy's the worst.
[00:10:48.260 --> 00:10:50.260]   The smart guy's like, I put all my money in Bitcoin.
[00:10:50.260 --> 00:10:52.260]   The mid guy's like, you can't put money in Bitcoin,
[00:10:52.260 --> 00:10:55.340]   it's not real money. (laughing)
[00:10:55.340 --> 00:10:58.020]   - And all of it is a genius meme.
[00:10:58.020 --> 00:11:01.340]   That's another interesting one, memes.
[00:11:01.340 --> 00:11:04.860]   The humor, the idea, the absurdity
[00:11:04.860 --> 00:11:07.180]   encapsulated in a single image,
[00:11:07.180 --> 00:11:11.220]   and it just kind of propagates virally
[00:11:11.220 --> 00:11:13.500]   between all of our brains.
[00:11:13.500 --> 00:11:14.740]   I didn't get much sleep last night,
[00:11:14.740 --> 00:11:17.980]   so I'm very, I sound like I'm high, but I swear I'm not.
[00:11:17.980 --> 00:11:22.360]   Do you think we have ideas or ideas have us?
[00:11:22.360 --> 00:11:26.660]   - I think that we're gonna get super scary memes
[00:11:26.660 --> 00:11:29.380]   once the AIs actually are super human.
[00:11:29.380 --> 00:11:31.820]   - Ooh, you think AI will generate memes?
[00:11:31.820 --> 00:11:32.780]   - Of course.
[00:11:32.780 --> 00:11:35.100]   - You think it'll make humans laugh?
[00:11:35.100 --> 00:11:35.980]   - I think it's worse than that.
[00:11:35.980 --> 00:11:40.980]   So "Infinite Jest," it's introduced in the first 50 pages,
[00:11:40.980 --> 00:11:44.700]   is about a tape that you, once you watch it once,
[00:11:44.700 --> 00:11:47.380]   you only ever wanna watch that tape.
[00:11:47.380 --> 00:11:48.840]   In fact, you wanna watch the tape so much
[00:11:48.840 --> 00:11:50.920]   that someone says, okay, here's a hacksaw,
[00:11:50.920 --> 00:11:52.660]   cut off your pinky, and then I'll let you
[00:11:52.660 --> 00:11:55.060]   watch the tape again, and you'll do it.
[00:11:55.060 --> 00:11:57.280]   So we're actually gonna build that, I think,
[00:11:57.280 --> 00:11:58.900]   but it's not gonna be one static tape.
[00:11:58.900 --> 00:12:01.540]   I think the human brain is too complex
[00:12:01.540 --> 00:12:05.060]   to be stuck in one static tape like that.
[00:12:05.060 --> 00:12:06.340]   If you look at like ant brains,
[00:12:06.340 --> 00:12:08.940]   maybe they can be stuck on a static tape.
[00:12:08.940 --> 00:12:11.340]   But we're going to build that using generative models.
[00:12:11.340 --> 00:12:12.680]   We're going to build the TikTok
[00:12:12.680 --> 00:12:15.380]   that you actually can't look away from.
[00:12:15.380 --> 00:12:17.140]   - So TikTok is already pretty close there,
[00:12:17.140 --> 00:12:19.460]   but the generation is done by humans.
[00:12:19.460 --> 00:12:21.380]   The algorithm is just doing their recommendation,
[00:12:21.380 --> 00:12:25.340]   but if the algorithm is also able to do the generation.
[00:12:25.340 --> 00:12:27.180]   - Well, it's a question about how much intelligence
[00:12:27.180 --> 00:12:28.380]   is behind it, right?
[00:12:28.380 --> 00:12:30.700]   So the content is being generated by,
[00:12:30.700 --> 00:12:32.940]   let's say, one humanity worth of intelligence,
[00:12:32.940 --> 00:12:34.780]   and you can quantify a humanity, right?
[00:12:34.780 --> 00:12:39.780]   That's a, you know, it's X-flops, Yada-flops,
[00:12:39.780 --> 00:12:41.860]   but you can quantify it.
[00:12:41.860 --> 00:12:45.180]   Once that generation is being done by 100 humanities,
[00:12:45.180 --> 00:12:46.020]   you're done.
[00:12:46.020 --> 00:12:50.820]   - So it's actually scale that's the problem,
[00:12:50.820 --> 00:12:51.920]   but also speed.
[00:12:51.920 --> 00:12:55.100]   Yeah.
[00:12:55.100 --> 00:12:58.660]   And what if it's sort of manipulating
[00:12:58.660 --> 00:13:03.380]   the very limited human dopamine engine for porn?
[00:13:03.380 --> 00:13:05.700]   Imagine just TikTok, but for porn.
[00:13:05.700 --> 00:13:06.520]   - Yeah.
[00:13:06.520 --> 00:13:08.540]   - That's like a brave new world.
[00:13:08.540 --> 00:13:10.220]   - I don't even know what it'll look like, right?
[00:13:10.220 --> 00:13:13.020]   Like, again, you can't imagine the behaviors
[00:13:13.020 --> 00:13:14.400]   of something smarter than you,
[00:13:14.400 --> 00:13:16.660]   but a super intelligent,
[00:13:16.660 --> 00:13:21.420]   and an agent that just dominates your intelligence so much
[00:13:21.420 --> 00:13:24.060]   will be able to completely manipulate you.
[00:13:24.060 --> 00:13:26.980]   - Is it possible that it won't really manipulate,
[00:13:26.980 --> 00:13:28.660]   it'll just move past us?
[00:13:28.660 --> 00:13:32.120]   It'll just kind of exist the way water exists,
[00:13:32.120 --> 00:13:33.460]   or the air exists?
[00:13:33.460 --> 00:13:36.620]   - You see, and that's the whole AI safety thing.
[00:13:36.620 --> 00:13:40.160]   It's not the machine that's gonna do that.
[00:13:40.160 --> 00:13:41.740]   It's other humans using the machine
[00:13:41.740 --> 00:13:43.020]   that are gonna do that to you.
[00:13:43.020 --> 00:13:44.340]   - Yeah.
[00:13:44.340 --> 00:13:47.220]   'Cause the machine is not interested in hurting humans.
[00:13:47.220 --> 00:13:48.980]   - The machine is a machine.
[00:13:48.980 --> 00:13:50.580]   But the human gets the machine,
[00:13:50.580 --> 00:13:52.020]   and there's a lot of humans out there
[00:13:52.020 --> 00:13:54.060]   very interested in manipulating you.
[00:13:54.060 --> 00:13:58.460]   - Well, let me bring up Eliezer Yudkowsky,
[00:13:58.460 --> 00:14:02.060]   who recently sat where you're sitting.
[00:14:02.060 --> 00:14:06.860]   He thinks that AI will almost surely kill everyone.
[00:14:06.860 --> 00:14:08.620]   Do you agree with him or not?
[00:14:08.620 --> 00:14:12.500]   - Yes, but maybe for a different reason.
[00:14:12.500 --> 00:14:13.340]   - Okay.
[00:14:13.340 --> 00:14:18.780]   And I'll try to get you to find hope,
[00:14:18.780 --> 00:14:21.660]   or we could find a no to that answer.
[00:14:21.660 --> 00:14:22.620]   - But why yes?
[00:14:22.620 --> 00:14:26.220]   - Okay, why didn't nuclear weapons kill everyone?
[00:14:26.220 --> 00:14:27.420]   - That's a good question.
[00:14:27.420 --> 00:14:28.420]   - I think there's an answer.
[00:14:28.420 --> 00:14:29.380]   I think it's actually very hard
[00:14:29.380 --> 00:14:31.180]   to deploy nuclear weapons tactically.
[00:14:31.180 --> 00:14:35.100]   It's very hard to accomplish tactical objectives.
[00:14:35.100 --> 00:14:36.900]   Great, I can nuke their country.
[00:14:36.900 --> 00:14:38.920]   I have an irradiated pile of rubble.
[00:14:38.920 --> 00:14:39.940]   I don't want that.
[00:14:39.940 --> 00:14:40.900]   - Why not?
[00:14:40.900 --> 00:14:43.300]   - Why don't I want an irradiated pile of rubble?
[00:14:43.300 --> 00:14:44.420]   For all the reasons no one wants
[00:14:44.420 --> 00:14:46.300]   an irradiated pile of rubble.
[00:14:46.300 --> 00:14:50.820]   - Oh, 'cause you can't use that land for resources.
[00:14:50.820 --> 00:14:52.140]   You can't populate the land.
[00:14:52.140 --> 00:14:55.660]   - Yeah, what you want, a total victory in a war
[00:14:55.660 --> 00:14:58.260]   is not usually the irradiation
[00:14:58.260 --> 00:15:00.020]   and eradication of the people there.
[00:15:00.020 --> 00:15:02.520]   It's the subjugation and domination of the people.
[00:15:02.520 --> 00:15:06.740]   - Okay, so you can't use this strategically,
[00:15:06.740 --> 00:15:11.740]   tactically in a war to help gain a military advantage.
[00:15:11.740 --> 00:15:15.940]   It's all complete destruction, right?
[00:15:15.940 --> 00:15:17.940]   But there's egos involved.
[00:15:17.940 --> 00:15:19.140]   It's still surprising.
[00:15:19.140 --> 00:15:22.020]   - Still surprising that nobody pressed the big red button.
[00:15:22.020 --> 00:15:23.540]   - It's somewhat surprising,
[00:15:23.540 --> 00:15:25.980]   but you see, it's the little red button
[00:15:25.980 --> 00:15:28.320]   that's gonna be pressed with AI that's gonna,
[00:15:28.320 --> 00:15:31.860]   and that's why we die.
[00:15:31.860 --> 00:15:34.580]   It's not because the AI,
[00:15:34.580 --> 00:15:36.140]   if there's anything in the nature of AI,
[00:15:36.140 --> 00:15:37.660]   it's just the nature of humanity.
[00:15:37.660 --> 00:15:40.380]   - What's the algorithm behind the little red button?
[00:15:40.380 --> 00:15:42.860]   What possible ideas do you have
[00:15:42.860 --> 00:15:45.460]   for how a human species ends?
[00:15:45.460 --> 00:15:49.700]   - Sure, so I think the most obvious way to me
[00:15:49.700 --> 00:15:51.340]   is wireheading.
[00:15:51.340 --> 00:15:53.180]   We end up amusing ourselves to death.
[00:15:53.180 --> 00:15:57.660]   We end up all staring at that infinite TikTok
[00:15:57.660 --> 00:15:58.960]   and forgetting to eat.
[00:15:58.960 --> 00:16:02.500]   Maybe it's even more benign than this.
[00:16:02.500 --> 00:16:04.600]   Maybe we all just stop reproducing.
[00:16:04.600 --> 00:16:10.540]   Now, to be fair, it's probably hard to get all of humanity.
[00:16:10.540 --> 00:16:11.380]   - Yeah.
[00:16:11.380 --> 00:16:12.220]   - Yeah.
[00:16:12.220 --> 00:16:14.020]   It probably--
[00:16:14.020 --> 00:16:16.540]   - So the interesting thing about humanity
[00:16:16.540 --> 00:16:17.660]   is the diversity in it.
[00:16:17.660 --> 00:16:18.500]   - Oh, yeah.
[00:16:18.500 --> 00:16:19.640]   - Organisms in general.
[00:16:19.640 --> 00:16:21.620]   There's a lot of weirdos out there.
[00:16:21.620 --> 00:16:22.460]   - Well--
[00:16:22.460 --> 00:16:23.300]   - Two of them are sitting here.
[00:16:23.300 --> 00:16:25.380]   - I mean, diversity in humanity is--
[00:16:25.380 --> 00:16:26.220]   - With due respect.
[00:16:26.220 --> 00:16:27.900]   (both laughing)
[00:16:27.900 --> 00:16:29.740]   - I wish I was more weird.
[00:16:29.740 --> 00:16:31.740]   No, like I'm kinda, look, I'm drinking Smart Water, man.
[00:16:31.740 --> 00:16:33.300]   That's like a Coca-Cola product, right?
[00:16:33.300 --> 00:16:35.020]   - You're one corporate, George Haas.
[00:16:35.020 --> 00:16:36.860]   - Yeah, I'm one corporate.
[00:16:36.860 --> 00:16:38.540]   No, the amount of diversity in humanity,
[00:16:38.540 --> 00:16:40.180]   I think, is decreasing,
[00:16:40.180 --> 00:16:42.460]   just like all the other biodiversity on the planet.
[00:16:42.460 --> 00:16:43.740]   - Oh, boy, yeah.
[00:16:43.740 --> 00:16:44.580]   - Right?
[00:16:44.580 --> 00:16:45.420]   - Social media's not helping, huh?
[00:16:45.420 --> 00:16:47.260]   - Go eat McDonald's in China.
[00:16:47.260 --> 00:16:48.100]   - Yeah.
[00:16:48.100 --> 00:16:54.140]   - Yeah, no, it's the interconnectedness that's doing it.
[00:16:54.140 --> 00:16:54.980]   - Oh, that's interesting.
[00:16:54.980 --> 00:16:58.740]   So everybody starts relying on the connectivity
[00:16:58.740 --> 00:17:00.980]   of the internet, and over time,
[00:17:00.980 --> 00:17:03.780]   that reduces the diversity, the intellectual diversity,
[00:17:03.780 --> 00:17:06.540]   and then that gets you, everybody, into a funnel.
[00:17:06.540 --> 00:17:08.340]   There's still going to be a guy in Texas.
[00:17:08.340 --> 00:17:10.020]   - There is, and yeah.
[00:17:10.020 --> 00:17:10.860]   - In a bunker.
[00:17:10.860 --> 00:17:13.820]   - To be fair, do I think AI kills us all?
[00:17:13.820 --> 00:17:17.460]   I think AI kills everything we call society today.
[00:17:17.460 --> 00:17:19.580]   I do not think it actually kills the human species.
[00:17:19.580 --> 00:17:21.880]   I think that's actually incredibly hard to do.
[00:17:21.880 --> 00:17:26.300]   - Yeah, but society, if we start over, that's tricky.
[00:17:26.300 --> 00:17:28.780]   Most of us don't know how to do most things.
[00:17:28.780 --> 00:17:31.820]   - Yeah, but some of us do, and they'll be okay,
[00:17:31.820 --> 00:17:35.540]   and they'll rebuild after the great AI.
[00:17:35.540 --> 00:17:38.380]   - What's rebuilding look like?
[00:17:38.380 --> 00:17:40.660]   Like, how much do we lose?
[00:17:40.660 --> 00:17:44.980]   Like, what has human civilization done that's interesting?
[00:17:44.980 --> 00:17:47.860]   A combustion engine, electricity?
[00:17:47.860 --> 00:17:52.300]   So power and energy, that's interesting.
[00:17:52.300 --> 00:17:54.420]   Like, how to harness energy.
[00:17:54.420 --> 00:17:55.260]   - Whoa, whoa, whoa, whoa.
[00:17:55.260 --> 00:17:57.260]   They're gonna be religiously against that.
[00:17:57.260 --> 00:18:01.180]   - Are they going to get back to, like, fire?
[00:18:01.180 --> 00:18:05.020]   - Sure, I mean, it'll be like, you know,
[00:18:05.020 --> 00:18:07.420]   some kind of Amish-looking kind of thing, I think.
[00:18:07.420 --> 00:18:08.260]   I think they're going to have
[00:18:08.260 --> 00:18:10.900]   very strong taboos against technology.
[00:18:10.900 --> 00:18:14.780]   - Like, technology is almost like a new religion.
[00:18:14.780 --> 00:18:19.460]   Technology is the devil, and nature is God.
[00:18:19.460 --> 00:18:22.060]   - Sure. - So closer to nature.
[00:18:22.060 --> 00:18:24.020]   But can you really get away from AI
[00:18:24.020 --> 00:18:26.500]   if it destroyed 99% of the human species?
[00:18:26.500 --> 00:18:30.500]   Isn't it somehow have a hold, like a stronghold?
[00:18:30.500 --> 00:18:33.740]   - What's interesting about everything we build,
[00:18:33.740 --> 00:18:35.980]   I think we are going to build superintelligence
[00:18:35.980 --> 00:18:38.580]   before we build any sort of robustness in the AI.
[00:18:38.580 --> 00:18:41.860]   We cannot build an AI that is capable
[00:18:41.860 --> 00:18:46.860]   of going out into nature and surviving like a bird, right?
[00:18:46.860 --> 00:18:50.620]   A bird is an incredibly robust organism.
[00:18:50.620 --> 00:18:51.780]   We've built nothing like this.
[00:18:51.780 --> 00:18:55.020]   We haven't built a machine that's capable of reproducing.
[00:18:55.020 --> 00:18:58.700]   - Yes, but there is, you know,
[00:18:58.700 --> 00:19:00.300]   I work with Lego robots a lot now.
[00:19:00.300 --> 00:19:01.460]   I have a bunch of them.
[00:19:03.000 --> 00:19:03.960]   They're mobile.
[00:19:03.960 --> 00:19:08.360]   They can't reproduce, but all they need is,
[00:19:08.360 --> 00:19:10.440]   I guess you're saying they can't repair themselves.
[00:19:10.440 --> 00:19:11.480]   But if you have a large number,
[00:19:11.480 --> 00:19:12.960]   if you have like 100 million of them.
[00:19:12.960 --> 00:19:15.440]   - Let's just focus on them reproducing, right?
[00:19:15.440 --> 00:19:16.880]   Do they have microchips in them?
[00:19:16.880 --> 00:19:19.260]   Okay, then do they include a fab?
[00:19:19.260 --> 00:19:21.120]   - No.
[00:19:21.120 --> 00:19:22.520]   - Then how are they going to reproduce?
[00:19:22.520 --> 00:19:26.840]   - Well, it doesn't have to be all on board, right?
[00:19:26.840 --> 00:19:29.640]   They can go to a factory, to a repair shop.
[00:19:29.640 --> 00:19:33.120]   - Yeah, but then you're really moving away from robustness.
[00:19:33.120 --> 00:19:33.960]   - Yes.
[00:19:33.960 --> 00:19:35.720]   - All of life is capable of reproducing
[00:19:35.720 --> 00:19:38.160]   without needing to go to a repair shop.
[00:19:38.160 --> 00:19:39.800]   Life will continue to reproduce
[00:19:39.800 --> 00:19:42.640]   in the complete absence of civilization.
[00:19:42.640 --> 00:19:44.120]   Robots will not.
[00:19:44.120 --> 00:19:49.120]   So when the, if the AI apocalypse happens,
[00:19:49.120 --> 00:19:51.180]   I mean, the AIs are gonna probably die out
[00:19:51.180 --> 00:19:53.000]   'cause I think we're gonna get, again, super intelligence
[00:19:53.000 --> 00:19:55.000]   long before we get robustness.
[00:19:55.000 --> 00:19:58.600]   - What about if you just improve the fab
[00:19:58.600 --> 00:20:01.760]   to where you just have a 3D printer
[00:20:01.760 --> 00:20:03.440]   that can always help you?
[00:20:03.440 --> 00:20:04.480]   - Well, that'd be very interesting.
[00:20:04.480 --> 00:20:06.920]   I'm interested in building that.
[00:20:06.920 --> 00:20:08.120]   - Of course you are.
[00:20:08.120 --> 00:20:09.680]   You think, how difficult is that problem
[00:20:09.680 --> 00:20:14.680]   to have a robot that basically can build itself?
[00:20:14.680 --> 00:20:16.640]   - Very, very hard.
[00:20:16.640 --> 00:20:21.240]   - I think you've mentioned this to me or somewhere
[00:20:21.240 --> 00:20:24.320]   where people think it's easy conceptually.
[00:20:24.320 --> 00:20:26.160]   - And then they remember that you're gonna have
[00:20:26.160 --> 00:20:27.480]   to have a fab.
[00:20:27.480 --> 00:20:29.080]   - Yeah, on board.
[00:20:29.080 --> 00:20:30.320]   - Of course.
[00:20:30.320 --> 00:20:33.400]   - So 3D printer that prints a 3D printer.
[00:20:33.400 --> 00:20:34.240]   - Yeah.
[00:20:34.240 --> 00:20:36.320]   - Yeah, on legs.
[00:20:36.320 --> 00:20:37.880]   Why is that hard?
[00:20:37.880 --> 00:20:39.680]   - Well, 'cause it's, I mean, a 3D printer
[00:20:39.680 --> 00:20:42.160]   is a very simple machine, right?
[00:20:42.160 --> 00:20:44.180]   Okay, you're gonna print chips?
[00:20:44.180 --> 00:20:45.680]   You're gonna have an atomic printer?
[00:20:45.680 --> 00:20:47.120]   How are you gonna dope the silicon?
[00:20:47.120 --> 00:20:48.160]   - Yeah.
[00:20:48.160 --> 00:20:49.200]   - Right?
[00:20:49.200 --> 00:20:51.080]   How are you gonna etch the silicon?
[00:20:51.080 --> 00:20:54.640]   - You're gonna have to have a very interesting kind of fab
[00:20:54.640 --> 00:20:59.040]   if you wanna have a lot of computation on board.
[00:20:59.040 --> 00:21:03.560]   But you can do like structural type of robots
[00:21:03.560 --> 00:21:04.900]   that are dumb.
[00:21:04.900 --> 00:21:07.240]   - Yeah, but structural type of robots
[00:21:07.240 --> 00:21:08.840]   aren't gonna have the intelligence required
[00:21:08.840 --> 00:21:11.120]   to survive in any complex environment.
[00:21:11.120 --> 00:21:13.100]   - What about like ants type of systems
[00:21:13.100 --> 00:21:15.040]   where you have like trillions of them?
[00:21:15.040 --> 00:21:16.440]   - I don't think this works.
[00:21:16.440 --> 00:21:19.200]   I mean, again, like ants at their very core
[00:21:19.200 --> 00:21:20.840]   are made up of cells that are capable
[00:21:20.840 --> 00:21:22.600]   of individually reproducing.
[00:21:22.600 --> 00:21:25.160]   They're doing quite a lot of computation
[00:21:25.160 --> 00:21:26.600]   that we're taking for granted.
[00:21:26.600 --> 00:21:27.900]   - It's not even just the computation.
[00:21:27.900 --> 00:21:29.800]   It's that reproduction is so inherent.
[00:21:29.800 --> 00:21:32.200]   Okay, so like there's two stacks of life in the world.
[00:21:32.200 --> 00:21:35.080]   There's the biological stack and the silicon stack.
[00:21:35.080 --> 00:21:39.000]   The biological stack starts with reproduction.
[00:21:39.000 --> 00:21:40.820]   Reproduction is at the absolute core.
[00:21:40.820 --> 00:21:45.520]   The first proto-RNA organisms were capable of reproducing.
[00:21:45.520 --> 00:21:49.320]   The silicon stack, despite as far as it's come,
[00:21:49.320 --> 00:21:51.820]   is nowhere near being able to reproduce.
[00:21:51.820 --> 00:21:56.820]   - Yeah, so the fab movement, digital fabrication,
[00:21:56.820 --> 00:22:01.960]   fabrication in the full range of what that means
[00:22:01.960 --> 00:22:04.100]   is still in the early stages.
[00:22:04.100 --> 00:22:04.940]   - Yeah.
[00:22:04.940 --> 00:22:06.880]   - You're interested in this world.
[00:22:06.880 --> 00:22:09.240]   - Even if you did put a fab on the machine, right?
[00:22:09.240 --> 00:22:10.640]   Let's say, okay, we can build fabs.
[00:22:10.640 --> 00:22:12.080]   We know how to do that as humanity.
[00:22:12.080 --> 00:22:14.120]   We can probably put all the precursors
[00:22:14.120 --> 00:22:14.960]   that build all the machines
[00:22:14.960 --> 00:22:16.260]   and the fabs also in the machine.
[00:22:16.260 --> 00:22:18.960]   So first off, this machine is gonna be absolutely massive.
[00:22:18.960 --> 00:22:20.760]   I mean, we almost have a,
[00:22:20.760 --> 00:22:23.440]   like think of the size of the thing required
[00:22:23.440 --> 00:22:26.480]   to reproduce a machine today, right?
[00:22:26.480 --> 00:22:30.320]   Like is our civilization capable of reproduction?
[00:22:30.320 --> 00:22:32.560]   Can we reproduce our civilization on Mars?
[00:22:32.560 --> 00:22:35.960]   - If we were to construct a machine
[00:22:35.960 --> 00:22:39.040]   that is made up of humans, like a company,
[00:22:39.040 --> 00:22:40.320]   it can reproduce itself.
[00:22:40.320 --> 00:22:41.280]   - Yeah.
[00:22:41.280 --> 00:22:42.120]   - I don't know.
[00:22:42.120 --> 00:22:47.120]   It feels like 115 people.
[00:22:47.120 --> 00:22:50.000]   - I think it's so much harder than that.
[00:22:50.000 --> 00:22:50.840]   - 120?
[00:22:50.840 --> 00:22:52.120]   (laughs)
[00:22:52.120 --> 00:22:53.040]   I was looking for a number.
[00:22:53.040 --> 00:22:55.520]   - I believe that Twitter can be run by 50 people.
[00:22:55.520 --> 00:23:00.280]   I think that this is gonna take most of,
[00:23:00.280 --> 00:23:02.240]   like it's just most of society, right?
[00:23:02.240 --> 00:23:04.200]   Like we live in one globalized world.
[00:23:04.200 --> 00:23:05.920]   - No, but you're not interested in running Twitter.
[00:23:05.920 --> 00:23:08.160]   You're interested in seeding.
[00:23:08.160 --> 00:23:10.960]   Like you want to seed a civilization
[00:23:10.960 --> 00:23:13.040]   and then 'cause humans can like have sex.
[00:23:13.040 --> 00:23:14.800]   - Oh, okay, you're talking about, yeah, okay.
[00:23:14.800 --> 00:23:16.520]   So you're talking about the humans reproducing
[00:23:16.520 --> 00:23:18.080]   and like basically like what's the smallest
[00:23:18.080 --> 00:23:19.640]   self-sustaining colony of humans?
[00:23:19.640 --> 00:23:20.480]   - Yeah.
[00:23:20.480 --> 00:23:21.320]   - Yeah, okay, fine.
[00:23:21.320 --> 00:23:22.760]   But they're not gonna be making five nanometer chips.
[00:23:22.760 --> 00:23:23.680]   - Over time they will.
[00:23:23.680 --> 00:23:25.600]   I think you're being,
[00:23:25.600 --> 00:23:28.700]   like we have to expand our conception of time here.
[00:23:28.700 --> 00:23:30.780]   Going back to the original.
[00:23:30.780 --> 00:23:35.720]   Time scale, I mean, over across maybe 100 generations,
[00:23:35.720 --> 00:23:37.380]   we're back to making chips.
[00:23:37.380 --> 00:23:38.220]   No?
[00:23:38.220 --> 00:23:40.560]   If you seed the colony correctly.
[00:23:40.560 --> 00:23:43.880]   - Maybe, or maybe they'll watch our colony die out
[00:23:43.880 --> 00:23:45.840]   over here and be like, we're not making chips.
[00:23:45.840 --> 00:23:46.680]   Don't make chips.
[00:23:46.680 --> 00:23:48.640]   - No, but you have to seed that colony correctly.
[00:23:48.640 --> 00:23:50.600]   Whatever you do, don't make chips.
[00:23:50.600 --> 00:23:52.520]   Chips are what led to their downfall.
[00:23:52.520 --> 00:23:56.200]   - Well, that is the thing that humans do.
[00:23:56.200 --> 00:23:59.060]   They come up, they construct a devil,
[00:23:59.060 --> 00:24:00.120]   a good thing and a bad thing,
[00:24:00.120 --> 00:24:01.400]   and they really stick by that.
[00:24:01.400 --> 00:24:03.120]   And then they murder each other over that.
[00:24:03.120 --> 00:24:04.400]   There's always one asshole in the room
[00:24:04.400 --> 00:24:05.480]   who murders everybody.
[00:24:05.480 --> 00:24:06.800]   (laughing)
[00:24:06.800 --> 00:24:09.720]   And he usually makes tattoos and nice branding.
[00:24:09.720 --> 00:24:11.240]   - Now do you need that asshole?
[00:24:11.240 --> 00:24:12.600]   That's the question, right?
[00:24:12.600 --> 00:24:15.000]   Humanity works really hard today to get rid of that asshole,
[00:24:15.000 --> 00:24:16.800]   but I think they might be important.
[00:24:16.800 --> 00:24:19.800]   - Yeah, this whole freedom of speech thing.
[00:24:19.800 --> 00:24:22.080]   The freedom of being an asshole seems kind of important.
[00:24:22.080 --> 00:24:23.480]   - That's right.
[00:24:23.480 --> 00:24:26.560]   - Man, this thing, this fab, this human fab
[00:24:26.560 --> 00:24:28.720]   that we've constructed as human civilization
[00:24:28.720 --> 00:24:29.680]   is pretty interesting.
[00:24:29.680 --> 00:24:34.000]   And now it's building artificial copies of itself,
[00:24:34.000 --> 00:24:38.320]   or artificial copies of various aspects of itself
[00:24:38.320 --> 00:24:40.320]   that seem interesting, like intelligence.
[00:24:40.320 --> 00:24:43.280]   And I wonder where that goes.
[00:24:43.280 --> 00:24:46.520]   - I like to think it's just like another stack for life.
[00:24:46.520 --> 00:24:48.760]   We have like the biostack life, like we're a biostack life,
[00:24:48.760 --> 00:24:50.600]   and then the silicon stack life.
[00:24:50.600 --> 00:24:52.600]   - But it seems like the ceiling,
[00:24:52.600 --> 00:24:53.960]   or there might not be a ceiling,
[00:24:53.960 --> 00:24:55.360]   or at least the ceiling is much higher
[00:24:55.360 --> 00:24:57.200]   for the silicon stack.
[00:24:57.200 --> 00:24:59.720]   - Oh no, we don't know what the ceiling is
[00:24:59.720 --> 00:25:00.680]   for the biostack either.
[00:25:00.680 --> 00:25:04.240]   The biostack just seemed to move slower.
[00:25:04.240 --> 00:25:07.440]   You have Moore's law, which is not dead,
[00:25:07.440 --> 00:25:09.880]   despite many proclamations.
[00:25:09.880 --> 00:25:11.320]   - In the biostack or the silicon stack?
[00:25:11.320 --> 00:25:12.160]   - In the silicon stack.
[00:25:12.160 --> 00:25:13.680]   And you don't have anything like this in the biostack.
[00:25:13.680 --> 00:25:16.040]   So I have a meme that I posted.
[00:25:16.040 --> 00:25:17.840]   I tried to make a meme, it didn't work too well.
[00:25:17.840 --> 00:25:21.360]   But I posted a picture of Ronald Reagan and Joe Biden,
[00:25:21.360 --> 00:25:24.360]   and you look, this is 1980, and this is 2020.
[00:25:24.360 --> 00:25:26.840]   And these two humans are basically like the same.
[00:25:26.840 --> 00:25:31.840]   There's no, like there's been no change in humans
[00:25:31.840 --> 00:25:33.480]   in the last 40 years.
[00:25:33.480 --> 00:25:36.080]   And then I posted a computer from 1980
[00:25:36.080 --> 00:25:37.800]   and a computer from 2020.
[00:25:37.800 --> 00:25:38.640]   Wow.
[00:25:38.640 --> 00:25:43.360]   - Yeah, with the early stages, right?
[00:25:43.360 --> 00:25:45.640]   Which is why you said when you said the fab,
[00:25:45.640 --> 00:25:48.480]   the size of the fab required to make another fab
[00:25:48.480 --> 00:25:52.000]   is like very large right now.
[00:25:52.000 --> 00:25:52.840]   - Oh yeah.
[00:25:52.840 --> 00:25:57.840]   - But computers were very large 80 years ago.
[00:25:57.840 --> 00:26:01.560]   And they got pretty tiny.
[00:26:01.560 --> 00:26:05.640]   And people are starting to wanna wear them on their face
[00:26:05.640 --> 00:26:10.400]   in order to escape reality.
[00:26:10.400 --> 00:26:11.360]   That's the thing.
[00:26:11.360 --> 00:26:13.580]   In order to live inside the computer.
[00:26:14.680 --> 00:26:16.000]   Put a screen right here.
[00:26:16.000 --> 00:26:18.240]   I don't have to see the rest of you assholes.
[00:26:18.240 --> 00:26:19.800]   - I've been ready for a long time.
[00:26:19.800 --> 00:26:20.880]   - You like virtual reality?
[00:26:20.880 --> 00:26:21.720]   - I love it.
[00:26:21.720 --> 00:26:23.600]   - Do you wanna live there?
[00:26:23.600 --> 00:26:24.800]   - Yeah.
[00:26:24.800 --> 00:26:26.920]   - Yeah, part of me does too.
[00:26:26.920 --> 00:26:29.560]   How far away are we, do you think?
[00:26:29.560 --> 00:26:35.440]   - Judging from what you can buy today, far, very far.
[00:26:35.440 --> 00:26:39.040]   - I gotta tell you that I had the experience
[00:26:39.040 --> 00:26:43.240]   of Meta's Kodak Avatar,
[00:26:43.240 --> 00:26:46.980]   where it's a ultra high resolution scan.
[00:26:46.980 --> 00:26:50.960]   It looked real.
[00:26:50.960 --> 00:26:53.080]   - I mean, the headsets just are not quite
[00:26:53.080 --> 00:26:55.080]   at like eye resolution yet.
[00:26:55.080 --> 00:26:57.440]   I haven't put on any headset where I'm like,
[00:26:57.440 --> 00:26:59.720]   oh, this could be the real world.
[00:26:59.720 --> 00:27:03.480]   Whereas when I put good headphones on, audio is there.
[00:27:03.480 --> 00:27:05.360]   I like we can reproduce audio that I'm like,
[00:27:05.360 --> 00:27:06.960]   I'm actually in a jungle right now.
[00:27:06.960 --> 00:27:09.120]   If I close my eyes, I can't tell I'm not.
[00:27:09.120 --> 00:27:10.960]   - Yeah, but then there's also smell
[00:27:10.960 --> 00:27:11.880]   and all that kind of stuff.
[00:27:11.880 --> 00:27:12.960]   - Sure.
[00:27:12.960 --> 00:27:13.800]   - I don't know.
[00:27:13.800 --> 00:27:18.360]   The power of imagination or the power of the mechanism
[00:27:18.360 --> 00:27:20.840]   in the human mind that fills the gaps,
[00:27:20.840 --> 00:27:24.120]   that kind of reaches and wants to make the thing you see
[00:27:24.120 --> 00:27:28.960]   in the virtual world real to you, I believe in that power.
[00:27:28.960 --> 00:27:30.320]   - Or humans wanna believe.
[00:27:30.320 --> 00:27:31.160]   - Yeah.
[00:27:31.160 --> 00:27:33.240]   What if you're lonely?
[00:27:33.240 --> 00:27:34.680]   What if you're sad?
[00:27:34.680 --> 00:27:36.760]   What if you're really struggling in life
[00:27:36.760 --> 00:27:39.720]   and here's a world where you don't have to struggle anymore?
[00:27:39.720 --> 00:27:41.480]   - Humans wanna believe so much
[00:27:41.520 --> 00:27:44.240]   that people think the large language models are conscious.
[00:27:44.240 --> 00:27:46.720]   That's how much humans wanna believe.
[00:27:46.720 --> 00:27:48.120]   - Strong words.
[00:27:48.120 --> 00:27:50.680]   He's throwing left and right hooks.
[00:27:50.680 --> 00:27:53.840]   Why do you think large language models are not conscious?
[00:27:53.840 --> 00:27:55.720]   - I don't think I'm conscious.
[00:27:55.720 --> 00:27:58.680]   - Oh, so what is consciousness then, George Hart?
[00:27:58.680 --> 00:28:01.320]   - It's like what it seems to mean to people.
[00:28:01.320 --> 00:28:03.800]   It's just like a word that atheists use for souls.
[00:28:03.800 --> 00:28:06.560]   - Sure, but that doesn't mean soul
[00:28:06.560 --> 00:28:07.920]   is not an interesting word.
[00:28:07.920 --> 00:28:10.400]   - If consciousness is a spectrum,
[00:28:10.400 --> 00:28:12.200]   I'm definitely way more conscious
[00:28:12.200 --> 00:28:13.960]   than the large language models are.
[00:28:13.960 --> 00:28:16.760]   I think the large language models
[00:28:16.760 --> 00:28:18.440]   are less conscious than a chicken.
[00:28:18.440 --> 00:28:21.440]   - When is the last time you've seen a chicken?
[00:28:21.440 --> 00:28:25.280]   - In Miami, like a couple months ago.
[00:28:25.280 --> 00:28:27.600]   - How, no, like a living chicken?
[00:28:27.600 --> 00:28:29.960]   - Living chickens walking around Miami, it's crazy.
[00:28:29.960 --> 00:28:31.520]   - Like on the street? - Yeah.
[00:28:31.520 --> 00:28:33.320]   - Like a chicken? - A chicken, yeah.
[00:28:33.320 --> 00:28:36.240]   - All right. (laughs)
[00:28:36.240 --> 00:28:38.240]   All right, I was trying to call you all
[00:28:38.240 --> 00:28:41.360]   like a good journalist and I got shut down.
[00:28:41.360 --> 00:28:47.600]   Okay, but you don't think much about this kind of
[00:28:47.600 --> 00:28:56.680]   subjective feeling that it feels like something to exist.
[00:28:56.680 --> 00:29:01.800]   And then as an observer, you can have a sense
[00:29:01.800 --> 00:29:05.180]   that an entity is not only intelligent,
[00:29:05.180 --> 00:29:09.760]   but has a kind of subjective experience of its reality,
[00:29:09.760 --> 00:29:13.360]   like a self-awareness that is capable of like suffering,
[00:29:13.360 --> 00:29:15.520]   of hurting, of being excited by the environment
[00:29:15.520 --> 00:29:20.520]   in a way that's not merely kind of an artificial response,
[00:29:20.520 --> 00:29:22.840]   but a deeply felt one.
[00:29:22.840 --> 00:29:24.840]   - Humans wanna believe so much
[00:29:24.840 --> 00:29:26.680]   that if I took a rock and a Sharpie
[00:29:26.680 --> 00:29:28.300]   and drew a sad face on the rock,
[00:29:28.300 --> 00:29:29.760]   they'd think the rock is sad.
[00:29:29.760 --> 00:29:33.880]   - Yeah, and you're saying when we look in the mirror,
[00:29:33.880 --> 00:29:36.960]   we apply the same smiley face with rock.
[00:29:36.960 --> 00:29:38.160]   - Pretty much, yeah.
[00:29:38.160 --> 00:29:40.920]   - Isn't that weird though, that you're not conscious?
[00:29:40.920 --> 00:29:43.600]   - No.
[00:29:43.600 --> 00:29:45.520]   - But you do believe in consciousness.
[00:29:45.520 --> 00:29:47.360]   - Not really. - It's unclear.
[00:29:47.360 --> 00:29:50.240]   Okay, so to you it's like a little like a symptom
[00:29:50.240 --> 00:29:52.360]   of the bigger thing that's not that important.
[00:29:52.360 --> 00:29:55.440]   - Yeah, I mean, it's interesting that like human systems
[00:29:55.440 --> 00:29:57.320]   seem to claim that they're conscious.
[00:29:57.320 --> 00:29:58.920]   And I guess it kind of like says something
[00:29:58.920 --> 00:30:00.720]   in a straight up like, okay, what do people mean
[00:30:00.720 --> 00:30:02.400]   when even if you don't believe in consciousness,
[00:30:02.400 --> 00:30:04.320]   what do people mean when they say consciousness?
[00:30:04.320 --> 00:30:06.880]   And there's definitely like meanings to it.
[00:30:06.880 --> 00:30:08.640]   - What's your favorite thing to eat?
[00:30:08.640 --> 00:30:12.200]   - Pizza.
[00:30:12.200 --> 00:30:13.480]   - Cheese pizza, what are the toppings?
[00:30:13.480 --> 00:30:14.320]   - I like cheese pizza.
[00:30:14.320 --> 00:30:15.640]   - Don't say pineapple. - I like pepperoni pizza.
[00:30:15.640 --> 00:30:16.520]   No, I don't like pineapple.
[00:30:16.520 --> 00:30:17.360]   - Okay, pepperoni pizza.
[00:30:17.360 --> 00:30:19.680]   - And if they put any ham on it, oh, that's real bad.
[00:30:19.680 --> 00:30:21.360]   - What's the best pizza?
[00:30:21.360 --> 00:30:22.320]   What are we talking about here?
[00:30:22.320 --> 00:30:24.120]   Like you like cheap crappy pizza?
[00:30:24.120 --> 00:30:26.280]   - A Chicago deep dish cheese pizza,
[00:30:26.280 --> 00:30:27.440]   oh, that's my favorite.
[00:30:27.440 --> 00:30:29.200]   - There you go, you bite into a deep dish,
[00:30:29.200 --> 00:30:31.160]   a Chicago deep dish pizza,
[00:30:31.160 --> 00:30:33.000]   and it feels like you were starving,
[00:30:33.000 --> 00:30:34.640]   you haven't eaten for 24 hours.
[00:30:34.640 --> 00:30:37.040]   You just bite in and you're hanging out
[00:30:37.040 --> 00:30:38.640]   with somebody that matters a lot to you
[00:30:38.640 --> 00:30:39.840]   and you're there with the pizza.
[00:30:39.840 --> 00:30:40.760]   - Oh, that sounds real nice.
[00:30:40.760 --> 00:30:43.880]   - Yeah, all right, it feels like something.
[00:30:43.880 --> 00:30:45.960]   I'm George motherfucking Hotz
[00:30:45.960 --> 00:30:49.280]   eating a fucking Chicago deep dish pizza.
[00:30:49.280 --> 00:30:54.280]   There's just the full peak light living experience
[00:30:54.280 --> 00:30:57.200]   of being human, the top of the human condition.
[00:30:58.440 --> 00:31:00.920]   It feels like something to experience that.
[00:31:00.920 --> 00:31:04.400]   Why does it feel like something?
[00:31:04.400 --> 00:31:06.680]   That's consciousness, isn't it?
[00:31:06.680 --> 00:31:08.840]   - If that's the word you wanna use to describe it, sure.
[00:31:08.840 --> 00:31:10.720]   I'm not gonna deny that that feeling exists.
[00:31:10.720 --> 00:31:13.680]   I'm not gonna deny that I experienced that feeling.
[00:31:13.680 --> 00:31:16.520]   When, I guess what I kind of take issue to
[00:31:16.520 --> 00:31:18.320]   is that there's some like,
[00:31:18.320 --> 00:31:20.520]   like how does it feel to be a web server?
[00:31:20.520 --> 00:31:21.960]   Do 404s hurt?
[00:31:21.960 --> 00:31:24.440]   - Not yet.
[00:31:24.440 --> 00:31:26.680]   - How would you know what suffering looked like?
[00:31:26.680 --> 00:31:28.680]   Sure, you can recognize a suffering dog
[00:31:28.680 --> 00:31:30.800]   because we're the same stack as the dog.
[00:31:30.800 --> 00:31:33.400]   All the bio stack stuff kind of, especially mammals,
[00:31:33.400 --> 00:31:34.640]   you know, it's fairly easy.
[00:31:34.640 --> 00:31:37.360]   - Game recognizes game.
[00:31:37.360 --> 00:31:40.240]   - Yeah, versus the silicon stack stuff,
[00:31:40.240 --> 00:31:42.800]   it's like, you have no idea.
[00:31:42.800 --> 00:31:46.080]   You have, wow, the little thing has learned to mimic.
[00:31:46.080 --> 00:31:52.280]   But then I realized that that's all we are too.
[00:31:52.280 --> 00:31:54.680]   Oh look, the little thing has learned to mimic.
[00:31:54.680 --> 00:31:58.760]   - Yeah, I guess, yeah, 404 could be suffering,
[00:31:58.760 --> 00:32:03.760]   but it's so far from our kind of living organism,
[00:32:03.760 --> 00:32:06.400]   our kind of stack.
[00:32:06.400 --> 00:32:10.640]   But it feels like AI can start maybe mimicking
[00:32:10.640 --> 00:32:12.480]   the biological stack better, better, better,
[00:32:12.480 --> 00:32:13.320]   'cause it's trained.
[00:32:13.320 --> 00:32:14.960]   - Retrained it, yeah.
[00:32:14.960 --> 00:32:17.080]   - And so in that, maybe that's the definition
[00:32:17.080 --> 00:32:20.080]   of consciousness, is the bio stack consciousness.
[00:32:20.080 --> 00:32:21.720]   - The definition of consciousness is how close
[00:32:21.720 --> 00:32:22.920]   something looks to human.
[00:32:22.920 --> 00:32:24.520]   Sure, I'll give you that one.
[00:32:24.520 --> 00:32:28.920]   - No, how close something is to the human experience.
[00:32:28.920 --> 00:32:33.280]   - Sure, it's a very anthropocentric definition, but.
[00:32:33.280 --> 00:32:34.880]   - Well, that's all we got.
[00:32:34.880 --> 00:32:37.400]   - Sure, no, and I don't mean to like,
[00:32:37.400 --> 00:32:38.860]   I think there's a lot of value in it.
[00:32:38.860 --> 00:32:40.160]   Look, I just started my second company,
[00:32:40.160 --> 00:32:42.160]   my third company will be AI Girlfriends.
[00:32:42.160 --> 00:32:44.040]   No, like I mean it.
[00:32:44.040 --> 00:32:45.880]   - I wanna find out what your fourth company is after.
[00:32:45.880 --> 00:32:46.720]   - Oh, wow.
[00:32:46.720 --> 00:32:48.800]   - 'Cause I think once you have AI Girlfriends,
[00:32:48.800 --> 00:32:53.800]   it's, oh boy, does it get interesting.
[00:32:54.680 --> 00:32:55.840]   - Well, maybe let's go there.
[00:32:55.840 --> 00:32:58.040]   I mean, the relationships with AI,
[00:32:58.040 --> 00:33:01.340]   that's creating human-like organisms, right?
[00:33:01.340 --> 00:33:03.280]   And part of being human is being conscious,
[00:33:03.280 --> 00:33:05.600]   is being, having the capacity to suffer,
[00:33:05.600 --> 00:33:08.320]   having the capacity to experience this life richly
[00:33:08.320 --> 00:33:10.700]   in such a way that you can empathize,
[00:33:10.700 --> 00:33:12.480]   the AI system can empathize with you,
[00:33:12.480 --> 00:33:14.280]   and you can empathize with it.
[00:33:14.280 --> 00:33:18.840]   Or you can project your anthropomorphic sense
[00:33:18.840 --> 00:33:21.760]   of what the other entity is experiencing,
[00:33:21.760 --> 00:33:25.080]   and an AI model would need to, yeah,
[00:33:25.080 --> 00:33:27.440]   to create that experience inside your mind.
[00:33:27.440 --> 00:33:28.880]   And it doesn't seem that difficult.
[00:33:28.880 --> 00:33:31.040]   - Yeah, but, okay, so here's where it actually
[00:33:31.040 --> 00:33:32.540]   gets totally different, right?
[00:33:32.540 --> 00:33:35.480]   When you interact with another human,
[00:33:35.480 --> 00:33:37.640]   you can make some assumptions.
[00:33:37.640 --> 00:33:38.600]   - Yeah.
[00:33:38.600 --> 00:33:40.440]   - When you interact with these models, you can't.
[00:33:40.440 --> 00:33:42.720]   You can make some assumptions that that other human
[00:33:42.720 --> 00:33:45.220]   experiences suffering and pleasure
[00:33:45.220 --> 00:33:47.040]   in a pretty similar way to you do.
[00:33:47.040 --> 00:33:48.420]   The golden rule applies.
[00:33:49.500 --> 00:33:52.620]   With an AI model, this isn't really true, right?
[00:33:52.620 --> 00:33:55.140]   These large language models are good at fooling people
[00:33:55.140 --> 00:33:57.940]   because they were trained on a whole bunch of human data
[00:33:57.940 --> 00:33:58.940]   and told to mimic it.
[00:33:58.940 --> 00:33:59.860]   - Yep.
[00:33:59.860 --> 00:34:04.040]   But if the AI system says, "Hi, my name is Samantha,"
[00:34:04.040 --> 00:34:06.820]   it has a backstory.
[00:34:06.820 --> 00:34:07.660]   - Yeah.
[00:34:07.660 --> 00:34:08.660]   - Went to college here and there.
[00:34:08.660 --> 00:34:09.500]   - Yeah.
[00:34:09.500 --> 00:34:11.620]   - Maybe it'll integrate this in the AI system.
[00:34:11.620 --> 00:34:12.460]   - I made some chatbots.
[00:34:12.460 --> 00:34:13.300]   I gave them backstories.
[00:34:13.300 --> 00:34:14.280]   It was lots of fun.
[00:34:14.280 --> 00:34:16.180]   I was so happy when Llama came out.
[00:34:16.180 --> 00:34:17.980]   - Yeah, we'll talk about Llama.
[00:34:17.980 --> 00:34:18.820]   We'll talk about all that.
[00:34:18.820 --> 00:34:21.060]   But the rock with the smiley face.
[00:34:21.060 --> 00:34:21.900]   - Yeah.
[00:34:21.900 --> 00:34:25.980]   - It seems pretty natural for you to anthropomorphize
[00:34:25.980 --> 00:34:28.180]   that thing and then start dating it.
[00:34:28.180 --> 00:34:33.140]   And before you know it, you're married and have kids.
[00:34:33.140 --> 00:34:34.460]   - With a rock.
[00:34:34.460 --> 00:34:35.620]   - With a rock.
[00:34:35.620 --> 00:34:37.620]   There's pictures on Instagram with you and a rock
[00:34:37.620 --> 00:34:38.740]   and a smiley face.
[00:34:38.740 --> 00:34:41.340]   - To be fair, something that people generally look for
[00:34:41.340 --> 00:34:42.340]   when they're looking for someone to date
[00:34:42.340 --> 00:34:44.940]   is intelligence in some form.
[00:34:44.940 --> 00:34:47.100]   And the rock doesn't really have intelligence.
[00:34:47.100 --> 00:34:50.260]   Only a pretty desperate person would date a rock.
[00:34:50.260 --> 00:34:52.260]   - I think we're all desperate deep down.
[00:34:52.260 --> 00:34:54.140]   - Oh, not rock level desperate.
[00:34:54.140 --> 00:34:55.300]   - (laughs) All right.
[00:34:55.300 --> 00:35:02.660]   Not rock level desperate, but AI level desperate.
[00:35:02.660 --> 00:35:04.300]   I don't know.
[00:35:04.300 --> 00:35:06.140]   I think all of us have a deep loneliness.
[00:35:06.140 --> 00:35:09.320]   It just feels like the language models are there.
[00:35:09.320 --> 00:35:10.160]   - Oh, I agree.
[00:35:10.160 --> 00:35:11.000]   And you know what?
[00:35:11.000 --> 00:35:11.940]   I won't even say this so cynically.
[00:35:11.940 --> 00:35:13.580]   I will actually say this in a way that like,
[00:35:13.580 --> 00:35:14.820]   I want AI friends.
[00:35:14.820 --> 00:35:15.660]   I do.
[00:35:15.660 --> 00:35:16.480]   - Yeah.
[00:35:16.480 --> 00:35:17.340]   - I would love to.
[00:35:17.340 --> 00:35:21.140]   You know, again, the language models now are still a little,
[00:35:21.140 --> 00:35:24.860]   like people are impressed with these GPT things.
[00:35:24.860 --> 00:35:29.700]   And I look at like, or like, or the copilot, the coding one.
[00:35:29.700 --> 00:35:32.380]   And I'm like, okay, this is like junior engineer level.
[00:35:32.380 --> 00:35:34.740]   And these people are like fiver level artists
[00:35:34.740 --> 00:35:36.020]   and copywriters.
[00:35:36.020 --> 00:35:38.340]   Like, okay, great.
[00:35:38.340 --> 00:35:40.580]   We got like fiver and like junior engineers.
[00:35:40.580 --> 00:35:41.500]   Okay, cool.
[00:35:41.500 --> 00:35:43.500]   Like, and this is just the start
[00:35:43.500 --> 00:35:45.140]   and it will get better, right?
[00:35:45.140 --> 00:35:47.680]   Like I can't wait to have AI friends
[00:35:47.680 --> 00:35:49.840]   who are more intelligent than I am.
[00:35:49.840 --> 00:35:51.240]   - So fiver is just a temper.
[00:35:51.240 --> 00:35:52.240]   It's not the ceiling.
[00:35:52.240 --> 00:35:53.420]   - No, definitely not.
[00:35:53.420 --> 00:35:58.840]   - Is it count as cheating when you're talking
[00:35:58.840 --> 00:36:01.200]   to an AI model, emotional cheating?
[00:36:01.200 --> 00:36:07.560]   - That's up to you and your human partner to define.
[00:36:07.560 --> 00:36:08.760]   - Oh, you have to, all right.
[00:36:08.760 --> 00:36:09.600]   - You're getting, yeah.
[00:36:09.600 --> 00:36:12.200]   You have to have that conversation, I guess.
[00:36:12.200 --> 00:36:13.240]   - All right.
[00:36:13.240 --> 00:36:16.100]   I mean, integrate that with porn and all this.
[00:36:16.100 --> 00:36:17.980]   - No, I mean, it's similar kind of porn.
[00:36:17.980 --> 00:36:18.820]   - Yeah. - Yeah.
[00:36:18.820 --> 00:36:21.100]   Right, I think people in relationships
[00:36:21.100 --> 00:36:22.940]   have different views on that.
[00:36:22.940 --> 00:36:26.380]   - Yeah, but most people don't have like
[00:36:26.380 --> 00:36:32.020]   serious open conversations about all the different aspects
[00:36:32.020 --> 00:36:34.060]   of what's cool and what's not.
[00:36:34.060 --> 00:36:37.060]   And it feels like AI is a really weird conversation to have.
[00:36:37.060 --> 00:36:40.700]   - The porn one is a good branching off.
[00:36:40.700 --> 00:36:42.220]   Like these things, you know, one of my scenarios
[00:36:42.220 --> 00:36:44.800]   that I put in my chat bot is, you know,
[00:36:44.800 --> 00:36:48.040]   a nice girl named Lexi, she's 20.
[00:36:48.040 --> 00:36:49.440]   She just moved out to LA.
[00:36:49.440 --> 00:36:50.440]   She wanted to be an actress,
[00:36:50.440 --> 00:36:52.000]   but she started doing OnlyFans instead.
[00:36:52.000 --> 00:36:53.800]   And you're on a date with her, enjoy.
[00:36:53.800 --> 00:36:58.200]   - Oh man, yeah.
[00:36:58.200 --> 00:37:00.480]   And so is that, if you're actually dating somebody
[00:37:00.480 --> 00:37:03.200]   in real life, is that cheating?
[00:37:03.200 --> 00:37:04.920]   I feel like it gets a little weird.
[00:37:04.920 --> 00:37:06.680]   - Sure. - It gets real weird.
[00:37:06.680 --> 00:37:09.160]   It's like, what are you allowed to say to an AI bot?
[00:37:09.160 --> 00:37:11.620]   Imagine having that conversation with a significant other.
[00:37:11.620 --> 00:37:13.400]   I mean, these are all things for people to define
[00:37:13.400 --> 00:37:14.280]   in their relationships.
[00:37:14.280 --> 00:37:17.080]   What it means to be human is just gonna start to get weird.
[00:37:17.080 --> 00:37:18.520]   - Especially online.
[00:37:18.520 --> 00:37:19.760]   Like, how do you know?
[00:37:19.760 --> 00:37:22.480]   Like, there'll be moments when you'll have
[00:37:22.480 --> 00:37:25.200]   what you think is a real human you interacted with
[00:37:25.200 --> 00:37:27.460]   on Twitter for years and you realize it's not.
[00:37:27.460 --> 00:37:32.440]   - I spread, I love this meme, heaven banning.
[00:37:32.440 --> 00:37:33.520]   You know what shadow banning?
[00:37:33.520 --> 00:37:34.920]   - Yeah. - Shadow banning.
[00:37:34.920 --> 00:37:36.520]   Okay, you post, no one can see it.
[00:37:36.520 --> 00:37:39.360]   Heaven banning, you post, no one can see it,
[00:37:39.360 --> 00:37:42.300]   but a whole lot of AIs are spot up to interact with you.
[00:37:42.300 --> 00:37:46.740]   - Well, maybe that's what the way human civilization ends
[00:37:46.740 --> 00:37:48.780]   is all of us are heaven banned.
[00:37:48.780 --> 00:37:50.740]   - There's a great, it's called
[00:37:50.740 --> 00:37:53.380]   My Little Pony Friendship is Optimal.
[00:37:53.380 --> 00:37:56.660]   It's a sci-fi story that explores this idea.
[00:37:56.660 --> 00:37:58.740]   - Friendship is optimal. - Friendship is optimal.
[00:37:58.740 --> 00:38:00.020]   - Yeah, I'd like to have some,
[00:38:00.020 --> 00:38:02.220]   at least on the intellectual realm,
[00:38:02.220 --> 00:38:05.180]   some AI friends that argue with me.
[00:38:05.180 --> 00:38:09.060]   But the romantic realm is weird.
[00:38:09.060 --> 00:38:09.980]   Definitely weird.
[00:38:09.980 --> 00:38:16.240]   But not out of the realm of the kind of weirdness
[00:38:16.240 --> 00:38:18.920]   that human civilization is capable of, I think.
[00:38:18.920 --> 00:38:20.880]   - I want it.
[00:38:20.880 --> 00:38:21.700]   Look, I want it.
[00:38:21.700 --> 00:38:23.460]   If no one else wants it, I want it.
[00:38:23.460 --> 00:38:25.380]   - Yeah, I think a lot of people probably want it.
[00:38:25.380 --> 00:38:26.780]   There's a deep loneliness.
[00:38:26.780 --> 00:38:30.260]   - And I'll fill their loneliness
[00:38:30.260 --> 00:38:33.580]   and just will only advertise to you some of the time.
[00:38:33.580 --> 00:38:36.260]   - Yeah, maybe the conceptions of monogamy change too.
[00:38:36.260 --> 00:38:38.460]   Like I grew up in a time, like I value monogamy,
[00:38:38.460 --> 00:38:40.260]   but maybe that's a silly notion
[00:38:40.260 --> 00:38:43.280]   when you have arbitrary number of AI systems.
[00:38:43.280 --> 00:38:48.380]   - This interesting path from rationality to polyamory.
[00:38:48.380 --> 00:38:50.180]   Yeah, that doesn't make sense for me.
[00:38:50.180 --> 00:38:52.740]   - For you, but you're just a biological organism
[00:38:52.740 --> 00:38:57.740]   who was born before the internet really took off.
[00:38:57.740 --> 00:38:59.940]   - The crazy thing is, like,
[00:38:59.940 --> 00:39:02.900]   culture is whatever we define it as.
[00:39:02.900 --> 00:39:04.500]   Right, these things are not,
[00:39:04.500 --> 00:39:07.740]   like, is a lot of problem in moral philosophy, right?
[00:39:07.740 --> 00:39:09.860]   There's no, like, okay, what is might be
[00:39:09.860 --> 00:39:12.300]   that, like, computers are capable of mimicking,
[00:39:12.300 --> 00:39:14.700]   you know, girlfriends perfectly.
[00:39:14.700 --> 00:39:16.580]   They passed the girlfriend Turing test, right?
[00:39:16.580 --> 00:39:18.100]   But that doesn't say anything about ought.
[00:39:18.100 --> 00:39:19.620]   That doesn't say anything about how we ought
[00:39:19.620 --> 00:39:21.180]   to respond to them as a civilization.
[00:39:21.180 --> 00:39:23.860]   That doesn't say we ought to get rid of monogamy, right?
[00:39:23.860 --> 00:39:25.700]   That's a completely separate question,
[00:39:25.700 --> 00:39:27.500]   really a religious one.
[00:39:27.500 --> 00:39:30.100]   - Girlfriend Turing test, I wonder what that looks like.
[00:39:30.100 --> 00:39:31.020]   - Girlfriend Turing test.
[00:39:31.020 --> 00:39:32.340]   - Are you writing that?
[00:39:32.340 --> 00:39:36.580]   Will you be the Alan Turing of the 21st century
[00:39:36.580 --> 00:39:38.380]   that writes the girlfriend Turing test?
[00:39:38.380 --> 00:39:40.840]   - No, I mean, of course, my AI girlfriends,
[00:39:40.840 --> 00:39:43.680]   their goal is to pass the girlfriend Turing test.
[00:39:43.680 --> 00:39:45.300]   - No, but there should be, like, a paper
[00:39:45.300 --> 00:39:46.900]   that kind of defines the test.
[00:39:46.900 --> 00:39:50.980]   I mean, the question is if it's deeply personalized
[00:39:50.980 --> 00:39:54.320]   or there's a common thing that really gets everybody.
[00:39:54.320 --> 00:39:57.540]   - Yeah, I mean, you know, look, we're a company.
[00:39:57.540 --> 00:39:58.420]   We don't have to get everybody.
[00:39:58.420 --> 00:40:00.820]   We just have to get a large enough clientele to stay.
[00:40:00.820 --> 00:40:03.980]   - I like how you're already thinking company.
[00:40:03.980 --> 00:40:06.500]   All right, let's, before we go to company number three
[00:40:06.500 --> 00:40:09.500]   and company number four, let's go to company number two.
[00:40:09.500 --> 00:40:10.420]   Tiny Corp.
[00:40:10.420 --> 00:40:15.840]   Possibly one of the greatest names of all time for a company.
[00:40:15.840 --> 00:40:18.740]   You've launched a new company called Tiny Corp
[00:40:18.740 --> 00:40:20.900]   that leads the development of Tiny Grad.
[00:40:20.900 --> 00:40:25.020]   What's the origin story of Tiny Corp and Tiny Grad?
[00:40:25.020 --> 00:40:28.580]   - I started Tiny Grad as a, like, a toy project
[00:40:28.580 --> 00:40:32.460]   just to teach myself, okay, like, what is a convolution?
[00:40:32.460 --> 00:40:34.660]   What are all these options you can pass to them?
[00:40:34.660 --> 00:40:36.660]   What is the derivative of a convolution, right?
[00:40:36.660 --> 00:40:40.100]   Very similar to, Karpathy wrote MicroGrad.
[00:40:40.100 --> 00:40:40.940]   Very similar.
[00:40:40.940 --> 00:40:45.280]   And then I started realizing,
[00:40:45.280 --> 00:40:48.020]   I started thinking about, like, AI chips.
[00:40:48.020 --> 00:40:50.580]   I started thinking about chips that run AI,
[00:40:50.580 --> 00:40:52.860]   and I was like, well, okay,
[00:40:52.860 --> 00:40:55.540]   this is going to be a really big problem.
[00:40:55.540 --> 00:40:57.820]   If NVIDIA becomes a monopoly here,
[00:40:57.820 --> 00:41:01.460]   how long before NVIDIA is nationalized?
[00:41:02.980 --> 00:41:07.940]   - So you, one of the reasons to start Tiny Corp
[00:41:07.940 --> 00:41:10.220]   is to challenge NVIDIA.
[00:41:10.220 --> 00:41:12.940]   - It's not so much to challenge NVIDIA.
[00:41:12.940 --> 00:41:15.260]   I actually, I like NVIDIA.
[00:41:15.260 --> 00:41:20.260]   And it's to make sure power stays decentralized.
[00:41:20.260 --> 00:41:22.180]   - Yeah.
[00:41:22.180 --> 00:41:25.120]   And here's computational power.
[00:41:25.120 --> 00:41:28.540]   I see you and NVIDIA is kind of locking down
[00:41:28.540 --> 00:41:30.580]   the computational power of the world.
[00:41:30.580 --> 00:41:34.980]   If NVIDIA becomes just like 10x better than everything else,
[00:41:34.980 --> 00:41:37.220]   you're giving a big advantage to somebody
[00:41:37.220 --> 00:41:40.780]   who can secure NVIDIA as a resource.
[00:41:40.780 --> 00:41:42.500]   - Yeah.
[00:41:42.500 --> 00:41:44.580]   - In fact, if Jensen watches this podcast,
[00:41:44.580 --> 00:41:46.400]   he may want to consider this.
[00:41:46.400 --> 00:41:47.900]   He may want to consider making sure
[00:41:47.900 --> 00:41:49.460]   his company's not nationalized.
[00:41:49.460 --> 00:41:52.380]   - You think that's an actual threat?
[00:41:52.380 --> 00:41:53.220]   - Oh, yes.
[00:41:53.220 --> 00:41:58.420]   - No, but there's so much, you know, there's AMD.
[00:41:58.420 --> 00:42:00.140]   - So we have NVIDIA and AMD, great.
[00:42:00.140 --> 00:42:01.580]   All right.
[00:42:01.580 --> 00:42:03.700]   But you don't think there's like a push
[00:42:03.700 --> 00:42:08.540]   towards like selling, like Google selling TPUs
[00:42:08.540 --> 00:42:09.380]   or something like this?
[00:42:09.380 --> 00:42:10.420]   You don't think there's a push for that?
[00:42:10.420 --> 00:42:11.740]   - Have you seen it?
[00:42:11.740 --> 00:42:13.980]   Google loves to rent you TPUs.
[00:42:13.980 --> 00:42:15.820]   - It doesn't, you can't buy it at Best Buy?
[00:42:15.820 --> 00:42:16.660]   - No.
[00:42:16.660 --> 00:42:22.420]   So I started work on a chip.
[00:42:22.420 --> 00:42:24.620]   I was like, okay, what's it gonna take to make a chip?
[00:42:24.620 --> 00:42:26.980]   And my first notions were all completely wrong
[00:42:26.980 --> 00:42:30.260]   about why, about like how you could improve on GPUs.
[00:42:30.260 --> 00:42:33.020]   And I will take this, this is from Jim Keller
[00:42:33.020 --> 00:42:34.580]   on your podcast.
[00:42:34.580 --> 00:42:38.140]   And this is one of my absolute favorite
[00:42:38.140 --> 00:42:39.740]   descriptions of computation.
[00:42:39.740 --> 00:42:42.980]   So there's three kinds of computation paradigms
[00:42:42.980 --> 00:42:45.020]   that are common in the world today.
[00:42:45.020 --> 00:42:47.740]   There's CPUs, and CPUs can do everything.
[00:42:47.740 --> 00:42:50.160]   CPUs can do add and multiply.
[00:42:50.160 --> 00:42:51.500]   They can do load and store,
[00:42:51.500 --> 00:42:53.300]   and they can do compare and branch.
[00:42:53.300 --> 00:42:54.420]   And when I say they can do these things,
[00:42:54.420 --> 00:42:56.420]   they can do them all fast, right?
[00:42:56.420 --> 00:42:58.780]   So compare and branch are unique to CPUs.
[00:42:58.780 --> 00:43:00.220]   And what I mean by they can do them fast
[00:43:00.220 --> 00:43:01.980]   is they can do things like branch prediction
[00:43:01.980 --> 00:43:03.220]   and speculative execution.
[00:43:03.220 --> 00:43:04.620]   And they spend tons of transistors
[00:43:04.620 --> 00:43:06.940]   on these like super deep reorder buffers
[00:43:06.940 --> 00:43:08.940]   in order to make these things fast.
[00:43:08.940 --> 00:43:11.380]   Then you have a simpler computation model GPUs.
[00:43:11.380 --> 00:43:13.300]   GPUs can't really do compare and branch.
[00:43:13.300 --> 00:43:15.880]   I mean, they can, but it's horrendously slow.
[00:43:15.880 --> 00:43:18.260]   But GPUs can do arbitrary load and store, right?
[00:43:18.260 --> 00:43:21.580]   GPUs can do things like X dereference Y.
[00:43:21.580 --> 00:43:23.380]   So they can fetch from arbitrary pieces of memory.
[00:43:23.380 --> 00:43:25.100]   They can fetch from memory that is defined
[00:43:25.100 --> 00:43:26.500]   by the contents of the data.
[00:43:26.500 --> 00:43:29.780]   The third model of computation is DSPs.
[00:43:29.780 --> 00:43:32.380]   And DSPs are just add and multiply, right?
[00:43:32.380 --> 00:43:33.380]   Like they can do load and stores,
[00:43:33.380 --> 00:43:34.720]   but only static load and stores,
[00:43:34.720 --> 00:43:36.020]   only loads and stores that are known
[00:43:36.020 --> 00:43:37.700]   before the program runs.
[00:43:37.700 --> 00:43:39.340]   And you look at neural networks today,
[00:43:39.340 --> 00:43:43.000]   and 95% of neural networks are all the DSP paradigm.
[00:43:43.000 --> 00:43:48.100]   They are just statically scheduled adds and multiplies.
[00:43:48.100 --> 00:43:50.260]   So TinyGuard really took this idea,
[00:43:50.260 --> 00:43:52.540]   and I'm still working on it,
[00:43:52.540 --> 00:43:54.380]   to extend this as far as possible.
[00:43:55.300 --> 00:43:58.100]   Every stage of the stack has Turing completeness, right?
[00:43:58.100 --> 00:43:59.540]   Python has Turing completeness.
[00:43:59.540 --> 00:44:00.420]   And then we take Python,
[00:44:00.420 --> 00:44:02.460]   we go into C++, which is Turing complete.
[00:44:02.460 --> 00:44:04.860]   And maybe C++ calls into some CUDA kernels,
[00:44:04.860 --> 00:44:05.900]   which are Turing complete.
[00:44:05.900 --> 00:44:07.340]   The CUDA kernels go through LLVM,
[00:44:07.340 --> 00:44:08.740]   which is Turing complete, into PTX,
[00:44:08.740 --> 00:44:09.820]   which is Turing complete, to SAS,
[00:44:09.820 --> 00:44:12.180]   which is Turing complete on a Turing complete processor.
[00:44:12.180 --> 00:44:14.860]   I want to get Turing completeness out of the stack entirely.
[00:44:14.860 --> 00:44:16.260]   Because once you get rid of Turing completeness,
[00:44:16.260 --> 00:44:17.820]   you can reason about things.
[00:44:17.820 --> 00:44:19.340]   Rice's theorem and the halting problem
[00:44:19.340 --> 00:44:21.020]   do not apply to admiral machines.
[00:44:23.500 --> 00:44:25.780]   - Okay, what's the power and the value
[00:44:25.780 --> 00:44:28.500]   of getting Turing completeness out of,
[00:44:28.500 --> 00:44:30.980]   are we talking about the hardware or the software?
[00:44:30.980 --> 00:44:32.260]   - Every layer of the stack.
[00:44:32.260 --> 00:44:33.100]   - Every layer.
[00:44:33.100 --> 00:44:34.060]   - Every layer of the stack,
[00:44:34.060 --> 00:44:35.460]   removing Turing completeness
[00:44:35.460 --> 00:44:37.740]   allows you to reason about things, right?
[00:44:37.740 --> 00:44:40.340]   So the reason you need to do branch prediction in a CPU,
[00:44:40.340 --> 00:44:41.460]   and the reason it's prediction,
[00:44:41.460 --> 00:44:42.460]   and the branch predictors are,
[00:44:42.460 --> 00:44:44.980]   I think they're like 99% on CPUs.
[00:44:44.980 --> 00:44:46.920]   Why do they get 1% of them wrong?
[00:44:46.920 --> 00:44:50.940]   Well, they get 1% wrong because you can't know, right?
[00:44:50.940 --> 00:44:51.900]   That's the halting problem.
[00:44:51.900 --> 00:44:53.220]   It's equivalent to the halting problem
[00:44:53.220 --> 00:44:56.420]   to say whether a branch is gonna be taken or not.
[00:44:56.420 --> 00:45:01.300]   I can show that, but the admiral machine,
[00:45:01.300 --> 00:45:05.340]   the neural network, runs the identical compute every time.
[00:45:05.340 --> 00:45:07.340]   The only thing that changes is the data.
[00:45:07.340 --> 00:45:11.020]   So when you realize this, you think about,
[00:45:11.020 --> 00:45:12.860]   okay, how can we build a computer,
[00:45:12.860 --> 00:45:13.940]   how can we build a stack
[00:45:13.940 --> 00:45:16.060]   that takes maximal advantage of this idea?
[00:45:16.060 --> 00:45:20.260]   So what makes TinyGrad different
[00:45:20.260 --> 00:45:22.260]   from other neural network libraries
[00:45:22.260 --> 00:45:24.380]   is it does not have a primitive operator
[00:45:24.380 --> 00:45:26.540]   even for matrix multiplication.
[00:45:26.540 --> 00:45:28.300]   And this is every single one.
[00:45:28.300 --> 00:45:29.500]   They even have primitive operators
[00:45:29.500 --> 00:45:30.900]   for things like convolutions.
[00:45:30.900 --> 00:45:32.620]   - So no matmul.
[00:45:32.620 --> 00:45:33.540]   - No matmul.
[00:45:33.540 --> 00:45:35.020]   Well, here's what a matmul is.
[00:45:35.020 --> 00:45:36.940]   So I'll use my hands to talk here.
[00:45:36.940 --> 00:45:38.460]   So if you think about a cube,
[00:45:38.460 --> 00:45:40.460]   and I put my two matrices that I'm multiplying
[00:45:40.460 --> 00:45:42.960]   on two faces of the cube, right?
[00:45:42.960 --> 00:45:45.420]   You can think about the matrix multiply as,
[00:45:45.420 --> 00:45:47.380]   okay, the n cubed,
[00:45:47.380 --> 00:45:49.460]   I'm gonna multiply for each one in the cubed,
[00:45:49.460 --> 00:45:50.620]   and then I'm gonna do a sum,
[00:45:50.620 --> 00:45:52.500]   which is a reduce up to here,
[00:45:52.500 --> 00:45:53.780]   to the third face of the cube,
[00:45:53.780 --> 00:45:55.860]   and that's your multiplied matrix.
[00:45:55.860 --> 00:45:57.500]   So what a matrix multiply is,
[00:45:57.500 --> 00:45:59.340]   is a bunch of shape operations, right?
[00:45:59.340 --> 00:46:01.700]   A bunch of permute three shapes and expands
[00:46:01.700 --> 00:46:03.340]   on the two matrices.
[00:46:03.340 --> 00:46:05.780]   I'll multiply n cubed,
[00:46:05.780 --> 00:46:07.660]   I'll reduce n cubed,
[00:46:07.660 --> 00:46:09.620]   which gives you an n squared matrix.
[00:46:09.620 --> 00:46:12.060]   - Okay, so what is the minimum number of operations
[00:46:12.060 --> 00:46:12.980]   that can accomplish that
[00:46:12.980 --> 00:46:16.140]   if you don't have matmul as a primitive?
[00:46:16.140 --> 00:46:18.300]   - So TinyGrad has about 20.
[00:46:18.300 --> 00:46:22.300]   And you can compare TinyGrad's offset or IR
[00:46:22.300 --> 00:46:25.300]   to things like XLA or PrimTorch.
[00:46:25.300 --> 00:46:27.420]   So XLA and PrimTorch are ideas where like,
[00:46:27.420 --> 00:46:30.900]   okay, Torch has like 2000 different kernels.
[00:46:30.900 --> 00:46:35.760]   PyTorch 2.0 introduced PrimTorch, which has only 250.
[00:46:35.760 --> 00:46:39.260]   TinyGrad has order of magnitude 25.
[00:46:39.260 --> 00:46:43.340]   It's 10X less than XLA or PrimTorch.
[00:46:43.340 --> 00:46:44.820]   And you can think about it
[00:46:44.820 --> 00:46:47.740]   as kind of like RISC versus CISC, right?
[00:46:47.740 --> 00:46:50.460]   These other things are CISC like systems.
[00:46:50.460 --> 00:46:52.980]   TinyGrad is RISC.
[00:46:52.980 --> 00:46:54.620]   - And RISC-1.
[00:46:54.620 --> 00:46:56.940]   - RISC architecture is gonna change everything.
[00:46:56.940 --> 00:46:58.460]   1995, hackers.
[00:46:58.460 --> 00:47:00.140]   - Wait, really?
[00:47:00.140 --> 00:47:00.980]   That's an actual thing?
[00:47:00.980 --> 00:47:03.060]   - Angelina Jolie delivers the line,
[00:47:03.060 --> 00:47:05.780]   RISC architecture is gonna change everything in 1995.
[00:47:05.780 --> 00:47:06.620]   - Wow.
[00:47:06.620 --> 00:47:08.500]   - And here we are with ARM in the phones
[00:47:08.500 --> 00:47:10.020]   and ARM everywhere.
[00:47:10.020 --> 00:47:11.620]   - Wow, I love it when movies
[00:47:11.620 --> 00:47:13.140]   actually have real things in them.
[00:47:13.140 --> 00:47:14.380]   - Right?
[00:47:14.380 --> 00:47:15.300]   - Okay, interesting.
[00:47:15.300 --> 00:47:16.420]   And so this is like,
[00:47:16.420 --> 00:47:19.220]   so you're thinking of this as the RISC architecture
[00:47:19.220 --> 00:47:21.260]   of ML stack.
[00:47:21.260 --> 00:47:23.300]   25, huh?
[00:47:23.300 --> 00:47:28.300]   What, can you go through the four op types?
[00:47:28.300 --> 00:47:31.020]   - Sure.
[00:47:31.020 --> 00:47:32.740]   Okay, so you have unary ops,
[00:47:32.740 --> 00:47:36.820]   which take in a tensor
[00:47:36.820 --> 00:47:38.540]   and return a tensor of the same size
[00:47:38.540 --> 00:47:39.780]   and do some unary op to it.
[00:47:39.780 --> 00:47:43.760]   X, log, reciprocal, sine, right?
[00:47:43.760 --> 00:47:46.340]   They take in one and they're point-wise.
[00:47:46.340 --> 00:47:47.300]   - Relu.
[00:47:47.300 --> 00:47:48.740]   - Yeah, relu.
[00:47:48.740 --> 00:47:51.620]   Almost all activation functions are unary ops.
[00:47:51.620 --> 00:47:53.820]   Some combinations of unary ops together
[00:47:53.820 --> 00:47:54.860]   is still a unary op.
[00:47:54.860 --> 00:47:57.500]   Then you have binary ops.
[00:47:57.500 --> 00:47:59.780]   Binary ops are like point-wise addition,
[00:47:59.780 --> 00:48:01.980]   multiplication, division, compare.
[00:48:01.980 --> 00:48:05.420]   It takes in two tensors of equal size
[00:48:05.420 --> 00:48:06.620]   and outputs one tensor.
[00:48:06.620 --> 00:48:09.940]   Then you have reduce ops.
[00:48:09.940 --> 00:48:12.460]   Reduce ops will like take a three-dimensional tensor
[00:48:12.460 --> 00:48:14.580]   and turn it into a two-dimensional tensor
[00:48:14.580 --> 00:48:15.700]   or a three-dimensional tensor
[00:48:15.700 --> 00:48:17.060]   turning into a zero-dimensional tensor.
[00:48:17.060 --> 00:48:19.180]   Things like a sum or a max
[00:48:19.180 --> 00:48:21.700]   are really the common ones there.
[00:48:21.700 --> 00:48:24.020]   And then the fourth type is movement ops.
[00:48:24.020 --> 00:48:25.820]   And movement ops are different from the other types
[00:48:25.820 --> 00:48:27.620]   because they don't actually require computation.
[00:48:27.620 --> 00:48:30.060]   They require different ways to look at memory.
[00:48:30.060 --> 00:48:34.760]   So that includes reshapes, permutes, expands, flips.
[00:48:34.760 --> 00:48:35.600]   Those are the main ones, probably.
[00:48:35.600 --> 00:48:38.780]   - And so with that, you have enough to make a map model.
[00:48:38.780 --> 00:48:39.780]   - And convolutions.
[00:48:39.780 --> 00:48:41.540]   And every convolution you can imagine,
[00:48:41.540 --> 00:48:43.860]   dilated convolution, strided convolutions,
[00:48:43.860 --> 00:48:45.520]   transposed convolutions.
[00:48:45.520 --> 00:48:49.300]   - You write on GitHub about laziness,
[00:48:49.300 --> 00:48:53.620]   showing a map model, matrix multiplication.
[00:48:53.620 --> 00:48:55.180]   See how despite the style,
[00:48:55.180 --> 00:48:58.580]   it is fused into one kernel with the power of laziness.
[00:48:58.580 --> 00:49:01.060]   Can you elaborate on this power of laziness?
[00:49:01.060 --> 00:49:03.300]   - Sure, so if you type in PyTorch,
[00:49:03.300 --> 00:49:05.820]   A times B plus C,
[00:49:05.820 --> 00:49:08.420]   what this is going to do
[00:49:08.420 --> 00:49:11.940]   is it's going to first multiply A and B
[00:49:11.940 --> 00:49:13.700]   and store that result into memory.
[00:49:13.700 --> 00:49:14.540]   - Mm-hmm.
[00:49:14.540 --> 00:49:15.820]   - And then it is going to add C
[00:49:15.820 --> 00:49:17.760]   by reading that result from memory,
[00:49:17.760 --> 00:49:21.620]   reading C from memory and writing that out to memory.
[00:49:21.620 --> 00:49:23.900]   There is way more loads and stores to memory
[00:49:23.900 --> 00:49:25.100]   than you need there.
[00:49:25.100 --> 00:49:28.660]   If you don't actually do A times B as soon as you see it,
[00:49:28.660 --> 00:49:32.740]   if you wait until the user actually realizes that tensor,
[00:49:32.740 --> 00:49:34.940]   until the laziness actually resolves,
[00:49:34.940 --> 00:49:36.740]   you can fuse that plus C.
[00:49:36.740 --> 00:49:39.060]   This is like, it's the same way Haskell works.
[00:49:39.060 --> 00:49:44.060]   - So what's the process of porting a model into TinyGrad?
[00:49:44.060 --> 00:49:47.580]   - So TinyGrad's front end looks very similar to PyTorch.
[00:49:47.580 --> 00:49:49.920]   I probably could make a perfect
[00:49:49.920 --> 00:49:51.820]   or pretty close to perfect interop layer
[00:49:51.820 --> 00:49:52.940]   if I really wanted to.
[00:49:52.940 --> 00:49:54.540]   I think that there's some things that are nicer
[00:49:54.540 --> 00:49:56.180]   about TinyGrad syntax than PyTorch,
[00:49:56.180 --> 00:49:57.900]   but the front end looks very Torch-like.
[00:49:57.900 --> 00:50:00.140]   You can also load in ONNX models.
[00:50:00.140 --> 00:50:03.360]   We have more ONNX tests passing than Core ML.
[00:50:03.360 --> 00:50:06.380]   - Core ML, okay, so-
[00:50:06.380 --> 00:50:08.140]   - We'll pass ONNX runtime soon.
[00:50:08.140 --> 00:50:10.980]   - What about the developer experience with TinyGrad?
[00:50:10.980 --> 00:50:16.740]   What it feels like versus PyTorch?
[00:50:16.740 --> 00:50:18.100]   - By the way, I really like PyTorch.
[00:50:18.100 --> 00:50:20.900]   I think that it's actually a very good piece of software.
[00:50:20.900 --> 00:50:23.860]   I think that they've made a few different trade-offs,
[00:50:23.860 --> 00:50:28.460]   and these different trade-offs are where TinyGrad
[00:50:28.460 --> 00:50:29.700]   takes a different path.
[00:50:29.700 --> 00:50:32.060]   One of the biggest differences is it's really easy
[00:50:32.060 --> 00:50:35.860]   to see the kernels that are actually being sent to the GPU.
[00:50:35.860 --> 00:50:38.460]   If you run PyTorch on the GPU,
[00:50:38.460 --> 00:50:39.980]   you do some operation,
[00:50:39.980 --> 00:50:41.020]   and you don't know what kernels ran,
[00:50:41.020 --> 00:50:42.500]   you don't know how many kernels ran,
[00:50:42.500 --> 00:50:44.100]   you don't know how many flops were used,
[00:50:44.100 --> 00:50:46.340]   you don't know how much memory accesses were used.
[00:50:46.340 --> 00:50:48.660]   TinyGrad type debug equals two,
[00:50:48.660 --> 00:50:50.900]   and it will show you in this beautiful style
[00:50:50.900 --> 00:50:53.060]   every kernel that's run,
[00:50:53.060 --> 00:50:57.500]   how many flops and how many bytes.
[00:50:57.500 --> 00:51:03.420]   - So can you just linger on what problem TinyGrad solves?
[00:51:04.260 --> 00:51:06.300]   - TinyGrad solves the problem of porting
[00:51:06.300 --> 00:51:08.940]   new ML accelerators quickly.
[00:51:08.940 --> 00:51:12.060]   One of the reasons, tons of these companies now,
[00:51:12.060 --> 00:51:16.740]   I think Sequoia marked Graphcore to zero, right?
[00:51:16.740 --> 00:51:20.660]   Cerebus, Tenstorrent, Grok,
[00:51:20.660 --> 00:51:24.340]   all of these ML accelerator companies, they built chips.
[00:51:24.340 --> 00:51:25.540]   The chips were good.
[00:51:25.540 --> 00:51:26.860]   The software was terrible.
[00:51:26.860 --> 00:51:29.340]   And part of the reason is because,
[00:51:29.340 --> 00:51:31.380]   I think the same problem is happening with Dojo.
[00:51:31.380 --> 00:51:35.020]   It's really, really hard to write a PyTorch port,
[00:51:35.020 --> 00:51:37.460]   because you have to write 250 kernels,
[00:51:37.460 --> 00:51:40.340]   and you have to tune them all for performance.
[00:51:40.340 --> 00:51:42.940]   - What does Jim Culler think about TinyGrad?
[00:51:42.940 --> 00:51:45.620]   You guys hung out quite a bit,
[00:51:45.620 --> 00:51:49.820]   so he was involved, he's involved with Tenstorrent.
[00:51:49.820 --> 00:51:52.420]   What's his praise and what's his criticism
[00:51:52.420 --> 00:51:54.820]   of what you're doing with your life?
[00:51:54.820 --> 00:51:58.420]   - Look, my prediction for Tenstorrent
[00:51:58.420 --> 00:52:00.980]   is that they're gonna pivot to making RISC-V chips.
[00:52:01.940 --> 00:52:03.060]   CPUs.
[00:52:03.060 --> 00:52:04.060]   - CPUs.
[00:52:04.060 --> 00:52:05.220]   - Yeah.
[00:52:05.220 --> 00:52:06.060]   - Why?
[00:52:06.060 --> 00:52:10.580]   - Well, because AI accelerators are a software problem,
[00:52:10.580 --> 00:52:11.860]   not really a hardware problem.
[00:52:11.860 --> 00:52:12.700]   - Oh, interesting.
[00:52:12.700 --> 00:52:13.540]   So you don't think,
[00:52:13.540 --> 00:52:17.180]   you think the diversity of AI accelerators
[00:52:17.180 --> 00:52:19.740]   in the hardware space is not going to be a thing
[00:52:19.740 --> 00:52:21.340]   that exists long-term?
[00:52:21.340 --> 00:52:25.580]   - I think what's gonna happen is, if I can finish, okay.
[00:52:25.580 --> 00:52:28.140]   If you're trying to make an AI accelerator,
[00:52:28.140 --> 00:52:31.140]   you better have the capability of writing
[00:52:31.140 --> 00:52:35.260]   a torch-level performance stack on NVIDIA GPUs.
[00:52:35.260 --> 00:52:37.660]   If you can't write a torch stack on NVIDIA GPUs,
[00:52:37.660 --> 00:52:39.740]   and I mean all the way, I mean down to the driver,
[00:52:39.740 --> 00:52:41.860]   there's no way you're gonna be able to write it on your chip,
[00:52:41.860 --> 00:52:43.900]   because your chip's worse than an NVIDIA GPU.
[00:52:43.900 --> 00:52:45.260]   The first version of the chip you tape out,
[00:52:45.260 --> 00:52:46.540]   it's definitely worse.
[00:52:46.540 --> 00:52:48.540]   - Oh, you're saying writing that stack is really tough.
[00:52:48.540 --> 00:52:49.820]   - Yes, and not only that,
[00:52:49.820 --> 00:52:51.420]   actually, the chip that you tape out,
[00:52:51.420 --> 00:52:52.940]   almost always 'cause you're trying to get advantage
[00:52:52.940 --> 00:52:55.500]   over NVIDIA, you're specializing the hardware more.
[00:52:55.500 --> 00:52:57.100]   It's always harder to write software
[00:52:57.100 --> 00:52:59.060]   for more specialized hardware.
[00:52:59.060 --> 00:53:00.500]   A GPU's pretty generic,
[00:53:00.500 --> 00:53:02.580]   and if you can't write an NVIDIA stack,
[00:53:02.580 --> 00:53:05.200]   there's no way you can write a stack for your chip.
[00:53:05.200 --> 00:53:07.540]   So my approach with TinyGrad is,
[00:53:07.540 --> 00:53:09.600]   first, write a performant NVIDIA stack.
[00:53:09.600 --> 00:53:10.740]   We're targeting AMD.
[00:53:10.740 --> 00:53:16.020]   - So you did say a few to NVIDIA a little bit, with love.
[00:53:16.020 --> 00:53:16.860]   - With love.
[00:53:16.860 --> 00:53:17.700]   - Yeah, but-- - With love.
[00:53:17.700 --> 00:53:19.020]   It's like the Yankees, you know?
[00:53:19.020 --> 00:53:20.060]   I'm a Mets fan.
[00:53:20.060 --> 00:53:24.340]   - Oh, you're a Mets fan, a RISC fan and a Mets fan.
[00:53:24.340 --> 00:53:26.060]   What's the hope that AMD has?
[00:53:26.060 --> 00:53:30.540]   You did a build with AMD recently that I saw.
[00:53:30.540 --> 00:53:35.540]   How does the 7900XTX compare to the RTX 4090 or 4080?
[00:53:35.540 --> 00:53:39.700]   - Well, let's start with the fact
[00:53:39.700 --> 00:53:42.740]   that the 7900XTX kernel drivers don't work,
[00:53:42.740 --> 00:53:46.180]   and if you run demo apps in loops, it panics the kernel.
[00:53:46.180 --> 00:53:48.400]   - Okay, so this is a software issue?
[00:53:48.400 --> 00:53:51.300]   - Lisa Su responded to my email.
[00:53:51.300 --> 00:53:52.180]   - Oh. - I reached out.
[00:53:52.180 --> 00:53:56.780]   I was like, this is, you know, really?
[00:53:56.780 --> 00:54:01.540]   Like, I understand if your 7x7 transposed Winograd conv
[00:54:01.540 --> 00:54:02.900]   is slower than NVIDIA's,
[00:54:02.900 --> 00:54:05.580]   but literally when I run demo apps in a loop,
[00:54:05.580 --> 00:54:08.140]   the kernel panics?
[00:54:08.140 --> 00:54:09.540]   - So just adding that loop.
[00:54:09.540 --> 00:54:12.180]   - Yeah, I just literally took their demo apps
[00:54:12.180 --> 00:54:14.860]   and wrote like while true semicolon do the app
[00:54:14.860 --> 00:54:17.580]   semicolon done in a bunch of screens.
[00:54:17.580 --> 00:54:20.380]   Right, this is like the most primitive fuzz testing.
[00:54:20.380 --> 00:54:21.940]   - Why do you think that is?
[00:54:21.940 --> 00:54:26.620]   They're just not seeing a market in machine learning?
[00:54:26.620 --> 00:54:28.700]   - They're changing, they're trying to change.
[00:54:28.700 --> 00:54:29.660]   They're trying to change,
[00:54:29.660 --> 00:54:31.900]   and I had a pretty positive interaction with them this week.
[00:54:31.900 --> 00:54:34.220]   Last week I went on YouTube, I was just like, that's it.
[00:54:34.220 --> 00:54:35.300]   I give up on AMD.
[00:54:35.300 --> 00:54:37.500]   Like, this is, their driver doesn't even,
[00:54:37.500 --> 00:54:41.220]   I'm not gonna, you know, I'll go with Intel GPUs.
[00:54:41.220 --> 00:54:42.780]   Intel GPUs have better drivers.
[00:54:42.780 --> 00:54:46.940]   - So you're kind of spearheading
[00:54:46.940 --> 00:54:50.580]   the diversification of GPUs?
[00:54:50.580 --> 00:54:52.020]   - Yeah, and I'd like to extend
[00:54:52.020 --> 00:54:53.660]   that diversification to everything.
[00:54:53.660 --> 00:54:58.020]   I'd like to diversify the, right, the more,
[00:54:58.020 --> 00:55:02.660]   my central thesis about the world is
[00:55:02.660 --> 00:55:04.860]   there's things that centralize power and they're bad,
[00:55:04.860 --> 00:55:07.820]   and there's things that decentralize power and they're good.
[00:55:07.820 --> 00:55:09.980]   Everything I can do to help decentralize power,
[00:55:09.980 --> 00:55:10.820]   I'd like to do.
[00:55:10.820 --> 00:55:14.220]   - So you're really worried about the centralization
[00:55:14.220 --> 00:55:15.340]   of Nvidia, that's interesting.
[00:55:15.340 --> 00:55:17.260]   And you don't have a fundamental hope
[00:55:17.260 --> 00:55:22.260]   for the proliferation of ASICs, except in the cloud.
[00:55:22.260 --> 00:55:25.260]   - I'd like to help them with software.
[00:55:25.260 --> 00:55:27.100]   No, actually, there's only, the only ASIC
[00:55:27.100 --> 00:55:29.940]   that is remotely successful is Google's TPU.
[00:55:29.940 --> 00:55:31.540]   And the only reason that's successful is
[00:55:31.540 --> 00:55:34.940]   because Google wrote a machine learning framework.
[00:55:34.940 --> 00:55:37.380]   I think that you have to write a competitive machine
[00:55:37.380 --> 00:55:40.140]   learning framework in order to be able to build an ASIC.
[00:55:40.140 --> 00:55:45.060]   - You think Meta with PyTorch builds a competitor?
[00:55:45.060 --> 00:55:46.100]   - I hope so.
[00:55:46.100 --> 00:55:46.940]   - Okay. - They have one.
[00:55:46.940 --> 00:55:48.020]   They have an internal one.
[00:55:48.020 --> 00:55:50.060]   - Internal, I mean, public facing
[00:55:50.060 --> 00:55:52.380]   with a nice cloud interface and so on.
[00:55:52.380 --> 00:55:53.940]   - I don't want a cloud.
[00:55:53.940 --> 00:55:54.780]   - You don't like cloud?
[00:55:54.780 --> 00:55:55.860]   - I don't like cloud.
[00:55:55.860 --> 00:55:58.420]   - What do you think is the fundamental limitation of cloud?
[00:55:58.420 --> 00:56:01.900]   - Fundamental limitation of cloud is who owns the off switch.
[00:56:01.900 --> 00:56:03.820]   - So it's power to the people.
[00:56:03.820 --> 00:56:04.780]   - Yeah.
[00:56:04.780 --> 00:56:07.340]   - And you don't like the man to have all the power.
[00:56:07.340 --> 00:56:08.180]   - Exactly.
[00:56:08.180 --> 00:56:09.420]   - All right.
[00:56:09.420 --> 00:56:11.980]   And right now, the only way to do that is with Nvidia GPUs
[00:56:11.980 --> 00:56:14.880]   if you want performance and stability.
[00:56:15.720 --> 00:56:17.400]   - Interesting.
[00:56:17.400 --> 00:56:21.680]   It's a costly investment emotionally to go with AMDs.
[00:56:21.680 --> 00:56:24.780]   Well, let me sort of on a tangent ask you,
[00:56:24.780 --> 00:56:28.080]   you've built quite a few PCs.
[00:56:28.080 --> 00:56:31.240]   What's your advice on how to build a good custom PC
[00:56:31.240 --> 00:56:33.720]   for, let's say, for the different applications that you use
[00:56:33.720 --> 00:56:35.760]   for gaming, for machine learning?
[00:56:35.760 --> 00:56:36.640]   - Well, you shouldn't build one.
[00:56:36.640 --> 00:56:39.440]   You should buy a box from the tiny corp.
[00:56:39.440 --> 00:56:44.440]   - I heard rumors, whispers about this box in the tiny corp.
[00:56:44.480 --> 00:56:45.720]   What's this thing look like?
[00:56:45.720 --> 00:56:46.560]   What is it?
[00:56:46.560 --> 00:56:47.640]   What is it called?
[00:56:47.640 --> 00:56:48.800]   - It's called the tiny box.
[00:56:48.800 --> 00:56:49.760]   - Tiny box.
[00:56:49.760 --> 00:56:51.640]   - It's $15,000.
[00:56:51.640 --> 00:56:54.680]   And it's almost a paid a flop of compute.
[00:56:54.680 --> 00:56:57.740]   It's over a hundred gigabytes of GPU RAM.
[00:56:57.740 --> 00:57:01.920]   It's over five terabytes per second of GPU memory bandwidth.
[00:57:01.920 --> 00:57:07.000]   I'm gonna put like four NVMEs in RAID.
[00:57:07.000 --> 00:57:09.720]   You're gonna get like 20, 30 gigabytes per second
[00:57:09.720 --> 00:57:11.920]   of drive read bandwidth.
[00:57:11.920 --> 00:57:16.880]   I'm gonna build like the best deep learning box that I can
[00:57:16.880 --> 00:57:18.480]   that plugs into one wall outlet.
[00:57:18.480 --> 00:57:20.140]   - Okay.
[00:57:20.140 --> 00:57:21.520]   Can you go through those specs again a little bit
[00:57:21.520 --> 00:57:23.120]   from your, from memory?
[00:57:23.120 --> 00:57:25.000]   - Yeah, so it's almost a paid a flop of compute.
[00:57:25.000 --> 00:57:26.780]   - So AMD Intel.
[00:57:26.780 --> 00:57:28.780]   - Today, I'm leaning toward AMD.
[00:57:28.780 --> 00:57:33.600]   But we're pretty agnostic to the type of compute.
[00:57:33.600 --> 00:57:38.600]   The main limiting spec is a 120 volt 15 amp circuit.
[00:57:38.600 --> 00:57:41.720]   - Okay.
[00:57:41.720 --> 00:57:43.080]   - Because in order to like,
[00:57:43.080 --> 00:57:45.240]   like there's a plug over there, right?
[00:57:45.240 --> 00:57:47.980]   You have to be able to plug it in.
[00:57:47.980 --> 00:57:51.240]   We're also gonna sell the tiny rack,
[00:57:51.240 --> 00:57:53.760]   which like what's the most power you can get
[00:57:53.760 --> 00:57:56.440]   into your house without arousing suspicion?
[00:57:56.440 --> 00:57:59.760]   And one of the answers is an electric car charger.
[00:57:59.760 --> 00:58:01.800]   - Wait, where does the rack go?
[00:58:01.800 --> 00:58:03.200]   - Your garage.
[00:58:03.200 --> 00:58:04.280]   - Interesting.
[00:58:04.280 --> 00:58:05.480]   The car charger.
[00:58:05.480 --> 00:58:07.960]   - A wall outlet is about 1500 Watts.
[00:58:07.960 --> 00:58:10.040]   A car charger is about 10,000 Watts.
[00:58:10.040 --> 00:58:11.360]   - Okay.
[00:58:11.360 --> 00:58:14.840]   What is the most amount of power you can get your hands on
[00:58:14.840 --> 00:58:16.120]   without arousing suspicion?
[00:58:16.120 --> 00:58:16.960]   - That's right.
[00:58:16.960 --> 00:58:17.780]   - George Hoss.
[00:58:17.780 --> 00:58:18.620]   Okay.
[00:58:18.620 --> 00:58:22.600]   So the tiny box and you said NVMEs and RAID.
[00:58:22.600 --> 00:58:25.520]   I forget what you said about memory,
[00:58:25.520 --> 00:58:26.440]   all that kind of stuff.
[00:58:26.440 --> 00:58:27.280]   Okay.
[00:58:27.280 --> 00:58:29.120]   So what about what GPUs?
[00:58:29.120 --> 00:58:32.280]   - Again, probably 7900 XTXs,
[00:58:32.280 --> 00:58:35.600]   but maybe 3090s, maybe a 770s.
[00:58:35.600 --> 00:58:36.440]   For the Intel.
[00:58:36.440 --> 00:58:39.280]   - You're flexible or still exploring?
[00:58:39.280 --> 00:58:40.480]   - I'm still exploring.
[00:58:40.480 --> 00:58:44.200]   I wanna deliver a really good experience to people.
[00:58:44.200 --> 00:58:46.760]   And yeah, what GPUs I end up going with,
[00:58:46.760 --> 00:58:48.440]   again, I'm leaning toward AMD.
[00:58:48.440 --> 00:58:49.760]   We'll see.
[00:58:49.760 --> 00:58:53.400]   You know, in my email, what I said to AMD is like,
[00:58:53.400 --> 00:58:56.360]   just dumping the code on GitHub is not open source.
[00:58:56.360 --> 00:58:57.600]   Open source is a culture.
[00:58:57.600 --> 00:59:00.440]   Open source means that your issues
[00:59:00.440 --> 00:59:03.280]   are not all one year old stale issues.
[00:59:03.280 --> 00:59:06.760]   Open source means developing in public.
[00:59:06.760 --> 00:59:08.720]   And if you guys can commit to that,
[00:59:08.720 --> 00:59:11.480]   I see a real future for AMD as a competitor to Nvidia.
[00:59:11.480 --> 00:59:16.000]   - Well, I'd love to get a tiny box to MIT.
[00:59:16.000 --> 00:59:18.440]   So whenever it's ready, let's do it.
[00:59:18.440 --> 00:59:19.280]   - We're taking pre-orders.
[00:59:19.280 --> 00:59:20.240]   I took this from Elon.
[00:59:20.240 --> 00:59:23.280]   I'm like, all right, $100 fully refundable pre-orders.
[00:59:23.280 --> 00:59:24.600]   - Is it gonna be like the Cybertruck
[00:59:24.600 --> 00:59:26.200]   is gonna take a few years or?
[00:59:26.200 --> 00:59:27.400]   - No, I'll try to do it faster.
[00:59:27.400 --> 00:59:28.360]   It's a lot simpler.
[00:59:28.360 --> 00:59:29.960]   It's a lot simpler than a truck.
[00:59:29.960 --> 00:59:31.400]   - Well, there's complexities,
[00:59:31.400 --> 00:59:34.240]   not to just the putting the thing together,
[00:59:34.240 --> 00:59:35.880]   but like shipping and all this kind of stuff.
[00:59:35.880 --> 00:59:38.160]   - The thing that I wanna deliver to people out of the box
[00:59:38.160 --> 00:59:41.200]   is being able to run 65 billion parameter LLAMA
[00:59:41.200 --> 00:59:44.040]   in FP16 in real time in like a good,
[00:59:44.040 --> 00:59:45.840]   like 10 tokens per second or five tokens per second
[00:59:45.840 --> 00:59:46.680]   or something.
[00:59:46.680 --> 00:59:48.000]   - Just it works.
[00:59:48.000 --> 00:59:48.840]   - Yep, just it works.
[00:59:48.840 --> 00:59:52.800]   - LLAMA's running or something like LLAMA.
[00:59:52.800 --> 00:59:55.920]   - Experience, yeah, or I think Falcon is the new one.
[00:59:55.920 --> 00:59:58.800]   Experience a chat with the largest language model
[00:59:58.800 --> 01:00:00.520]   that you can have in your house.
[01:00:00.520 --> 01:00:02.560]   - Yeah, from a wall plug.
[01:00:02.560 --> 01:00:03.600]   - From a wall plug, yeah.
[01:00:03.600 --> 01:00:05.200]   Actually for inference,
[01:00:05.240 --> 01:00:07.880]   it's not like even more power would help you get more.
[01:00:07.880 --> 01:00:10.960]   - Even more power wouldn't get you more.
[01:00:10.960 --> 01:00:12.920]   - Well, no, there's just the biggest model released
[01:00:12.920 --> 01:00:16.240]   is 65 billion parameter LLAMA as far as I know.
[01:00:16.240 --> 01:00:18.800]   - So it sounds like Tiny Box will naturally pivot
[01:00:18.800 --> 01:00:20.320]   towards company number three
[01:00:20.320 --> 01:00:22.400]   'cause you could just get the girlfriend
[01:00:22.400 --> 01:00:25.120]   and or boyfriend.
[01:00:25.120 --> 01:00:27.360]   - That one's harder actually.
[01:00:27.360 --> 01:00:28.320]   - The boyfriend is harder?
[01:00:28.320 --> 01:00:29.160]   - The boyfriend's harder, yeah.
[01:00:29.160 --> 01:00:32.000]   - I think that's a very biased statement.
[01:00:32.000 --> 01:00:33.840]   I think a lot of people would just say,
[01:00:33.840 --> 01:00:38.720]   what, why is it harder to replace a boyfriend
[01:00:38.720 --> 01:00:41.200]   than a girlfriend with the artificial LLM?
[01:00:41.200 --> 01:00:43.480]   - Because women are attracted to status and power
[01:00:43.480 --> 01:00:45.580]   and men are attracted to youth and beauty.
[01:00:45.580 --> 01:00:49.000]   No, I mean, that's what I mean.
[01:00:49.000 --> 01:00:51.920]   - Both are mimicable easy through the language model.
[01:00:51.920 --> 01:00:56.160]   - No, no machines do not have any status or real power.
[01:00:56.160 --> 01:00:57.080]   - I don't know.
[01:00:57.080 --> 01:00:59.120]   I think you both, well, first of all,
[01:00:59.120 --> 01:01:04.120]   you're using language mostly to communicate youth
[01:01:04.120 --> 01:01:07.640]   and beauty and power and status.
[01:01:07.640 --> 01:01:10.160]   - But status fundamentally is a zero sum game, right?
[01:01:10.160 --> 01:01:12.160]   Whereas youth and beauty are not.
[01:01:12.160 --> 01:01:15.240]   - No, I think status is a narrative you can construct.
[01:01:15.240 --> 01:01:17.020]   I don't think status is real.
[01:01:17.020 --> 01:01:19.640]   - I don't know.
[01:01:19.640 --> 01:01:21.280]   I just think that that's why it's harder.
[01:01:21.280 --> 01:01:23.160]   You know, yeah, maybe it is my biases.
[01:01:23.160 --> 01:01:25.400]   - I think status is way easier to fake.
[01:01:25.400 --> 01:01:28.320]   - I also think that men are probably more desperate
[01:01:28.320 --> 01:01:29.600]   and more likely to buy my product.
[01:01:29.600 --> 01:01:31.680]   So maybe they're a better target market.
[01:01:31.680 --> 01:01:33.400]   - Desperation is interesting.
[01:01:33.400 --> 01:01:34.920]   Easier to fool.
[01:01:34.920 --> 01:01:35.760]   - Yeah.
[01:01:35.760 --> 01:01:36.840]   - I could see that.
[01:01:36.840 --> 01:01:38.280]   - Yeah, look, I mean, look, I know you can look
[01:01:38.280 --> 01:01:39.940]   at porn viewership numbers, right?
[01:01:39.940 --> 01:01:41.720]   A lot more men watch porn than women.
[01:01:41.720 --> 01:01:42.560]   - Yeah.
[01:01:42.560 --> 01:01:43.680]   - You can ask why that is.
[01:01:43.680 --> 01:01:46.480]   - Wow, there's a lot of questions than answers
[01:01:46.480 --> 01:01:47.440]   you can get there.
[01:01:47.440 --> 01:01:53.880]   Anyway, with the tiny box, how many GPUs in tiny box?
[01:01:53.880 --> 01:01:54.720]   - Six.
[01:01:54.720 --> 01:01:56.960]   (laughing)
[01:01:57.960 --> 01:01:59.440]   - Oh, man.
[01:01:59.440 --> 01:02:00.480]   - And I'll tell you why it's six.
[01:02:00.480 --> 01:02:01.320]   - Yeah.
[01:02:01.320 --> 01:02:05.520]   - So AMD Epyc processors have 128 lanes of PCIe.
[01:02:05.520 --> 01:02:11.600]   I wanna leave enough lanes for some drives.
[01:02:11.600 --> 01:02:15.840]   And I wanna leave enough lanes for some networking.
[01:02:15.840 --> 01:02:17.680]   - How do you do cooling for something like this?
[01:02:17.680 --> 01:02:19.840]   - Ah, that's one of the big challenges.
[01:02:19.840 --> 01:02:21.700]   Not only do I want the cooling to be good,
[01:02:21.700 --> 01:02:22.920]   I want it to be quiet.
[01:02:22.920 --> 01:02:25.040]   I want the tiny box to be able to sit comfortably
[01:02:25.040 --> 01:02:25.880]   in your room, right?
[01:02:25.880 --> 01:02:28.480]   - This is really going towards the girlfriend thing.
[01:02:28.480 --> 01:02:31.320]   'Cause you want to run the LLM.
[01:02:31.320 --> 01:02:33.560]   - I'll give a more, I mean, I can talk about
[01:02:33.560 --> 01:02:36.040]   how it relates to company number one.
[01:02:36.040 --> 01:02:36.880]   - Comm AI.
[01:02:36.880 --> 01:02:37.880]   - Yeah.
[01:02:37.880 --> 01:02:41.520]   - Well, but yes, quiet, oh, quiet because you may be
[01:02:41.520 --> 01:02:43.040]   potentially wanna run it in a car?
[01:02:43.040 --> 01:02:44.960]   - No, no, quiet because you wanna put this thing
[01:02:44.960 --> 01:02:46.880]   in your house and you want it to coexist with you.
[01:02:46.880 --> 01:02:48.880]   If it's screaming at 60 dB, you don't want that
[01:02:48.880 --> 01:02:49.880]   in your house, you'll kick it out.
[01:02:49.880 --> 01:02:51.200]   - 60 dB, yeah.
[01:02:51.200 --> 01:02:52.760]   - Yeah, I want like 40, 45.
[01:02:52.760 --> 01:02:55.080]   - So how do you make the cooling quiet?
[01:02:55.080 --> 01:02:57.120]   That's an interesting problem in itself.
[01:02:57.120 --> 01:02:58.800]   - A key trick is to actually make it big.
[01:02:58.800 --> 01:03:00.760]   Ironically, it's called the tiny box.
[01:03:00.760 --> 01:03:02.960]   But if I can make it big, a lot of that noise
[01:03:02.960 --> 01:03:05.600]   is generated because of high pressure air.
[01:03:05.600 --> 01:03:08.200]   If you look at like a 1U server, a 1U server
[01:03:08.200 --> 01:03:10.080]   has these super high pressure fans that are like
[01:03:10.080 --> 01:03:12.400]   super deep and they're like Genesis.
[01:03:12.400 --> 01:03:14.600]   Versus if you have something that's big,
[01:03:14.600 --> 01:03:16.720]   well, I can use a big, you know, they call them
[01:03:16.720 --> 01:03:18.600]   big ass fans, those ones that are like huge
[01:03:18.600 --> 01:03:21.360]   on the ceiling and they're completely silent.
[01:03:21.360 --> 01:03:24.720]   - So tiny box will be big.
[01:03:24.720 --> 01:03:27.920]   - It is the, I do not want it to be large
[01:03:27.920 --> 01:03:29.200]   according to UPS.
[01:03:29.200 --> 01:03:31.000]   I want it to be shippable as a normal package,
[01:03:31.000 --> 01:03:32.840]   but that's my constraint there.
[01:03:32.840 --> 01:03:33.760]   - Interesting.
[01:03:33.760 --> 01:03:36.440]   Well, the fan stuff, can't it be assembled
[01:03:36.440 --> 01:03:38.040]   on location or no?
[01:03:38.040 --> 01:03:38.880]   - No.
[01:03:38.880 --> 01:03:41.600]   - Oh, it has to be, well, you're--
[01:03:41.600 --> 01:03:42.520]   - Look, I want to give you a great
[01:03:42.520 --> 01:03:43.440]   out of the box experience.
[01:03:43.440 --> 01:03:44.520]   I want you to lift this thing out.
[01:03:44.520 --> 01:03:47.000]   I want it to be like the Mac, you know?
[01:03:47.000 --> 01:03:48.000]   Tiny box.
[01:03:48.000 --> 01:03:49.040]   - The Apple experience.
[01:03:49.040 --> 01:03:50.520]   - Yeah.
[01:03:50.520 --> 01:03:51.400]   - I love it.
[01:03:51.400 --> 01:03:53.640]   Okay, and so tiny box would run
[01:03:54.480 --> 01:03:58.120]   TinyGrad, like what do you envision
[01:03:58.120 --> 01:03:59.400]   this whole thing to look like?
[01:03:59.400 --> 01:04:04.400]   We're talking about like Linux with a full
[01:04:04.400 --> 01:04:06.920]   software engineering environment,
[01:04:06.920 --> 01:04:10.360]   and it's just not PyTorch, but TinyGrad.
[01:04:10.360 --> 01:04:12.840]   - Yeah, we did a poll of people want Ubuntu or Arch.
[01:04:12.840 --> 01:04:14.400]   We're gonna stick with Ubuntu.
[01:04:14.400 --> 01:04:15.240]   - Ooh, interesting.
[01:04:15.240 --> 01:04:17.600]   What's your favorite flavor of Linux?
[01:04:17.600 --> 01:04:18.440]   - Ubuntu.
[01:04:18.440 --> 01:04:19.360]   - Ubuntu.
[01:04:19.360 --> 01:04:22.480]   I like Ubuntu Mate, however you pronounce that, mate.
[01:04:23.480 --> 01:04:27.200]   So how do you, you've gotten Lama into TinyGrad.
[01:04:27.200 --> 01:04:29.080]   You've gotten stable diffusion into TinyGrad.
[01:04:29.080 --> 01:04:29.920]   What was that like?
[01:04:29.920 --> 01:04:34.440]   Can you comment on like, what are these models?
[01:04:34.440 --> 01:04:36.800]   What's interesting about porting them?
[01:04:36.800 --> 01:04:39.240]   What's, yeah, like what are the challenges?
[01:04:39.240 --> 01:04:41.800]   What's naturally, what's easy, all that kind of stuff.
[01:04:41.800 --> 01:04:43.880]   - There's a really simple way to get these models
[01:04:43.880 --> 01:04:46.400]   into TinyGrad, and you can just export them as Onyx,
[01:04:46.400 --> 01:04:47.960]   and then TinyGrad can run Onyx.
[01:04:47.960 --> 01:04:52.080]   So the ports that I did of Lama, stable diffusion,
[01:04:52.080 --> 01:04:54.520]   and now Whisper are more academic
[01:04:54.520 --> 01:04:56.320]   to teach me about the models,
[01:04:56.320 --> 01:04:58.880]   but they are cleaner than the PyTorch versions.
[01:04:58.880 --> 01:04:59.720]   You can read the code.
[01:04:59.720 --> 01:05:01.320]   I think the code is easier to read.
[01:05:01.320 --> 01:05:02.880]   It's less lines.
[01:05:02.880 --> 01:05:03.800]   There's just a few things
[01:05:03.800 --> 01:05:05.320]   about the way TinyGrad writes things.
[01:05:05.320 --> 01:05:07.800]   Here's a complaint I have about PyTorch.
[01:05:07.800 --> 01:05:10.720]   Nn.relu is a class, right?
[01:05:10.720 --> 01:05:12.880]   So when you create an Nn module,
[01:05:12.880 --> 01:05:17.520]   you'll put your Nn.relus as in a nit.
[01:05:17.520 --> 01:05:18.440]   And this makes no sense.
[01:05:18.440 --> 01:05:20.640]   Relu is completely stateless.
[01:05:20.640 --> 01:05:22.040]   Why should that be a class?
[01:05:22.040 --> 01:05:25.920]   - But that's more like a software engineering thing,
[01:05:25.920 --> 01:05:28.080]   or do you think it has a cost on performance?
[01:05:28.080 --> 01:05:30.440]   - Oh no, it doesn't have a cost on performance.
[01:05:30.440 --> 01:05:32.520]   But yeah, no, I think that it's,
[01:05:32.520 --> 01:05:35.760]   that's what I mean about TinyGrad's front end being cleaner.
[01:05:35.760 --> 01:05:37.200]   - Ah, I see.
[01:05:37.200 --> 01:05:38.040]   What do you think about Mojo?
[01:05:38.040 --> 01:05:39.360]   I don't know if you've been paying attention
[01:05:39.360 --> 01:05:43.040]   to the programming language that does some interesting ideas
[01:05:43.040 --> 01:05:46.080]   that kind of intersect TinyGrad.
[01:05:46.080 --> 01:05:48.200]   - I think that there is a spectrum,
[01:05:48.200 --> 01:05:50.040]   and on one side you have Mojo,
[01:05:50.040 --> 01:05:52.600]   and on the other side you have like GGML.
[01:05:52.600 --> 01:05:56.600]   GGML is this like, we're gonna run LLAMA fast on Mac.
[01:05:56.600 --> 01:05:58.120]   Okay, we're gonna expand out to a little bit,
[01:05:58.120 --> 01:06:01.120]   but we're gonna basically go to like depth first, right?
[01:06:01.120 --> 01:06:02.880]   Mojo is like, we're gonna go breadth first.
[01:06:02.880 --> 01:06:03.920]   We're gonna go so wide
[01:06:03.920 --> 01:06:05.920]   that we're gonna make all of Python fast,
[01:06:05.920 --> 01:06:07.360]   and TinyGrad's in the middle.
[01:06:07.360 --> 01:06:11.280]   TinyGrad is, we are going to make neural networks fast.
[01:06:11.280 --> 01:06:17.400]   - Yeah, but they try to really get it to be fast,
[01:06:17.400 --> 01:06:20.720]   compile down to a specific hardware,
[01:06:20.720 --> 01:06:22.680]   and make that compilation step
[01:06:22.680 --> 01:06:25.800]   as flexible and resilient as possible.
[01:06:25.800 --> 01:06:27.920]   - Yeah, but they have turn completeness.
[01:06:27.920 --> 01:06:29.480]   - And that limits you.
[01:06:29.480 --> 01:06:30.520]   - Turn-- - That's what you're seeing.
[01:06:30.520 --> 01:06:31.640]   It's somewhere in the middle.
[01:06:31.640 --> 01:06:34.720]   So you're actually going to be targeting some accelerators,
[01:06:34.720 --> 01:06:38.760]   some, like some number, not one.
[01:06:38.760 --> 01:06:41.360]   - My goal is step one,
[01:06:41.360 --> 01:06:44.000]   build an equally performance stack to PyTorch
[01:06:44.000 --> 01:06:48.080]   on NVIDIA and AMD, but with way less lines.
[01:06:48.080 --> 01:06:51.120]   And then step two is, okay, how do we make an accelerator?
[01:06:51.120 --> 01:06:52.440]   Right, but you need step one.
[01:06:52.440 --> 01:06:54.040]   You have to first build the framework
[01:06:54.040 --> 01:06:56.080]   before you can build the accelerator.
[01:06:56.080 --> 01:06:57.560]   - Can you explain MLPerf?
[01:06:57.560 --> 01:06:59.560]   What's your approach in general
[01:06:59.560 --> 01:07:02.080]   to benchmarking TinyGrad performance?
[01:07:02.080 --> 01:07:06.640]   - So I'm much more of a, like,
[01:07:06.640 --> 01:07:09.920]   build it the right way and worry about performance later.
[01:07:09.920 --> 01:07:13.720]   There's a bunch of things where I haven't even like
[01:07:13.720 --> 01:07:15.000]   really dove into performance.
[01:07:15.000 --> 01:07:16.440]   The only place where TinyGrad
[01:07:16.440 --> 01:07:18.360]   is competitive performance-wise right now
[01:07:18.360 --> 01:07:19.800]   is on Qualcomm GPUs.
[01:07:19.800 --> 01:07:23.360]   So TinyGrad's actually used in OpenPilot to run the model.
[01:07:23.360 --> 01:07:25.400]   So the driving model is TinyGrad.
[01:07:25.400 --> 01:07:27.720]   - When did that happen, that transition?
[01:07:27.720 --> 01:07:29.200]   - About eight months ago now.
[01:07:29.200 --> 01:07:33.280]   And it's 2x faster than Qualcomm's library.
[01:07:33.280 --> 01:07:38.120]   - What's the hardware that OpenPilot runs on, the CommAI?
[01:07:38.120 --> 01:07:40.280]   - It's a Snapdragon 845.
[01:07:40.280 --> 01:07:42.000]   - Okay. - So this is using the GPU.
[01:07:42.000 --> 01:07:44.600]   So the GPU's an Adreno GPU.
[01:07:44.600 --> 01:07:46.040]   There's like different things.
[01:07:46.040 --> 01:07:47.560]   There's a really good Microsoft paper
[01:07:47.560 --> 01:07:49.520]   that talks about like mobile GPUs
[01:07:49.520 --> 01:07:52.480]   and why they're different from desktop GPUs.
[01:07:52.480 --> 01:07:55.080]   One of the big things is in a desktop GPU,
[01:07:55.080 --> 01:07:59.000]   you can use buffers on a mobile GPU image textures
[01:07:59.000 --> 01:07:59.840]   a lot faster.
[01:07:59.840 --> 01:08:04.840]   - On a mobile GPU image textures, okay.
[01:08:04.840 --> 01:08:08.280]   And so you want to be able to leverage that.
[01:08:08.280 --> 01:08:09.520]   - I wanna be able to leverage it
[01:08:09.520 --> 01:08:11.560]   in a way that it's completely generic, right?
[01:08:11.560 --> 01:08:12.520]   So there's a lot of,
[01:08:12.520 --> 01:08:14.840]   Xiaomi has a pretty good open source library
[01:08:14.840 --> 01:08:16.560]   for mobile GPUs called MACE,
[01:08:16.560 --> 01:08:19.400]   where they can generate, where they have these kernels,
[01:08:19.400 --> 01:08:21.320]   but they're all hand-coded, right?
[01:08:21.320 --> 01:08:23.800]   So that's great if you're doing three by three comps.
[01:08:23.800 --> 01:08:25.600]   That's great if you're doing dense map models,
[01:08:25.600 --> 01:08:28.600]   but the minute you go off the beaten path a tiny bit,
[01:08:28.600 --> 01:08:30.600]   well, your performance is nothing.
[01:08:30.600 --> 01:08:31.800]   - Since you mentioned OpenPilot,
[01:08:31.800 --> 01:08:35.640]   I'd love to get an update in the company number one,
[01:08:35.640 --> 01:08:37.120]   CommAI world.
[01:08:37.120 --> 01:08:40.200]   How are things going there in the development
[01:08:40.200 --> 01:08:42.600]   of semi-autonomous driving?
[01:08:42.600 --> 01:08:48.640]   - You know, almost no one talks about FSD anymore,
[01:08:48.640 --> 01:08:51.240]   and even less people talk about OpenPilot.
[01:08:51.240 --> 01:08:54.320]   We've solved the problem, like we solved it years ago.
[01:08:54.320 --> 01:08:57.280]   - What's the problem exactly?
[01:08:57.280 --> 01:08:58.120]   - Well, how do you-
[01:08:58.120 --> 01:09:00.560]   - What does solving it mean?
[01:09:00.560 --> 01:09:02.360]   - Solving means how do you build a model
[01:09:02.360 --> 01:09:04.680]   that outputs a human policy for driving?
[01:09:04.680 --> 01:09:07.680]   How do you build a model that given,
[01:09:07.680 --> 01:09:09.640]   a reasonable set of sensors,
[01:09:09.640 --> 01:09:11.400]   outputs a human policy for driving?
[01:09:11.400 --> 01:09:14.520]   So you have companies like Waymo and Cruise,
[01:09:14.520 --> 01:09:15.920]   which are hand-coding these things
[01:09:15.920 --> 01:09:18.320]   that are like quasi-human policies.
[01:09:18.320 --> 01:09:22.800]   Then you have Tesla,
[01:09:22.800 --> 01:09:25.320]   and maybe even to more of an extent, Kama,
[01:09:25.320 --> 01:09:26.400]   asking, "Okay, how do we just learn
[01:09:26.400 --> 01:09:27.800]   the human policy from data?"
[01:09:27.800 --> 01:09:31.080]   The big thing that we're doing now,
[01:09:31.080 --> 01:09:32.720]   and we just put it out on Twitter,
[01:09:34.280 --> 01:09:35.640]   at the beginning of Kama,
[01:09:35.640 --> 01:09:39.800]   we published a paper called "Learning a Driving Simulator."
[01:09:39.800 --> 01:09:44.520]   And the way this thing worked was it was an autoencoder,
[01:09:44.520 --> 01:09:48.520]   and then an RNN in the middle, right?
[01:09:48.520 --> 01:09:51.480]   You take an autoencoder, you compress the picture,
[01:09:51.480 --> 01:09:53.760]   you use an RNN, predict the next state,
[01:09:53.760 --> 01:09:55.520]   and these things were, you know,
[01:09:55.520 --> 01:09:57.160]   it was a laughably bad simulator.
[01:09:57.160 --> 01:10:00.360]   Like this is 2015 era machine learning technology.
[01:10:00.360 --> 01:10:03.960]   Today, we have VQVAE and transformers.
[01:10:03.960 --> 01:10:06.760]   We're building DriveGPT, basically.
[01:10:06.760 --> 01:10:08.920]   - DriveGPT, okay.
[01:10:08.920 --> 01:10:12.160]   And it's trained on what?
[01:10:12.160 --> 01:10:14.360]   Is it trained in a self-supervised way?
[01:10:14.360 --> 01:10:16.120]   - Yeah, it's trained on all the driving data
[01:10:16.120 --> 01:10:17.600]   to predict the next frame.
[01:10:17.600 --> 01:10:20.840]   - So really trying to learn a human policy.
[01:10:20.840 --> 01:10:21.680]   What would a human do?
[01:10:21.680 --> 01:10:24.200]   - Well, actually, our simulator's conditioned on the pose.
[01:10:24.200 --> 01:10:25.440]   So it's actually a simulator.
[01:10:25.440 --> 01:10:27.000]   You can put in like a state action pair
[01:10:27.000 --> 01:10:28.760]   and get out the next state.
[01:10:28.760 --> 01:10:29.920]   - Okay.
[01:10:29.920 --> 01:10:31.920]   - And then once you have a simulator,
[01:10:31.920 --> 01:10:34.080]   you can do RL in the simulator
[01:10:34.080 --> 01:10:36.800]   and RL will get us that human policy.
[01:10:36.800 --> 01:10:38.280]   - So it transfers.
[01:10:38.280 --> 01:10:39.760]   - Yeah.
[01:10:39.760 --> 01:10:41.720]   RL with a reward function,
[01:10:41.720 --> 01:10:43.760]   not asking is this close to the human policy,
[01:10:43.760 --> 01:10:47.480]   but asking would a human disengage if you did this behavior?
[01:10:47.480 --> 01:10:50.040]   - Okay, let me think about the distinction there.
[01:10:50.040 --> 01:10:51.640]   Would a human disengage?
[01:10:51.640 --> 01:10:54.880]   Would a human disengage?
[01:10:54.880 --> 01:10:58.840]   That correlates, I guess, with human policy,
[01:10:58.840 --> 01:11:00.200]   but it could be different.
[01:11:00.200 --> 01:11:03.440]   So it doesn't just say, what would a human do?
[01:11:03.440 --> 01:11:06.920]   It says, what would a good human driver do?
[01:11:06.920 --> 01:11:09.720]   And such that the experience is comfortable,
[01:11:09.720 --> 01:11:12.840]   but also not annoying in that like the thing
[01:11:12.840 --> 01:11:14.600]   is very cautious.
[01:11:14.600 --> 01:11:16.400]   So it's finding a nice balance.
[01:11:16.400 --> 01:11:17.840]   That's interesting, that's a nice--
[01:11:17.840 --> 01:11:20.080]   - It's asking exactly the right question.
[01:11:20.080 --> 01:11:22.020]   What will make our customers happy?
[01:11:22.020 --> 01:11:23.880]   - Right.
[01:11:23.880 --> 01:11:25.400]   - A system that you never wanna disengage.
[01:11:25.400 --> 01:11:29.640]   - 'Cause usually disengagement is almost always a sign
[01:11:29.640 --> 01:11:32.120]   of I'm not happy with what the system is doing.
[01:11:32.120 --> 01:11:32.960]   - Usually.
[01:11:32.960 --> 01:11:35.160]   There's some that are just, I felt like driving,
[01:11:35.160 --> 01:11:36.480]   and those are always fine too,
[01:11:36.480 --> 01:11:39.200]   but they're just gonna look like noise in the data.
[01:11:39.200 --> 01:11:41.520]   - But even I felt like driving.
[01:11:41.520 --> 01:11:42.680]   - Maybe, yeah.
[01:11:42.680 --> 01:11:45.600]   - Even that's a signal, like, why do you feel like driving?
[01:11:45.600 --> 01:11:50.600]   You need to recalibrate your relationship with the car.
[01:11:50.600 --> 01:11:54.180]   Okay, so that's really interesting.
[01:11:54.180 --> 01:11:56.280]   How close are we to solving self-driving?
[01:11:59.400 --> 01:12:01.040]   - It's hard to say.
[01:12:01.040 --> 01:12:03.440]   We haven't completely closed the loop yet.
[01:12:03.440 --> 01:12:04.840]   So we don't have anything built
[01:12:04.840 --> 01:12:07.180]   that truly looks like that architecture yet.
[01:12:07.180 --> 01:12:10.080]   We have prototypes and there's bugs.
[01:12:10.080 --> 01:12:12.960]   So we are a couple bug fixes away.
[01:12:12.960 --> 01:12:15.120]   Might take a year, might take 10.
[01:12:15.120 --> 01:12:16.720]   - What's the nature of the bugs?
[01:12:16.720 --> 01:12:20.920]   Are these major philosophical bugs, logical bugs?
[01:12:20.920 --> 01:12:23.240]   What kind of bugs are we talking about?
[01:12:23.240 --> 01:12:24.440]   - They're just like stupid bugs,
[01:12:24.440 --> 01:12:26.720]   and also we might just need more scale.
[01:12:26.720 --> 01:12:30.000]   We just massively expanded our compute cluster at Gama.
[01:12:30.000 --> 01:12:33.520]   We now have about two people worth of compute,
[01:12:33.520 --> 01:12:34.440]   40 petaflops.
[01:12:34.440 --> 01:12:38.880]   - Well, people are different.
[01:12:38.880 --> 01:12:41.040]   - Yeah, 20 petaflops, that's a person.
[01:12:41.040 --> 01:12:42.260]   I mean, it's just a unit, right?
[01:12:42.260 --> 01:12:43.100]   Horses are different too,
[01:12:43.100 --> 01:12:44.960]   but we still call it a horsepower.
[01:12:44.960 --> 01:12:47.520]   - Yeah, but there's something different about mobility
[01:12:47.520 --> 01:12:51.280]   than there is about perception and action
[01:12:51.280 --> 01:12:53.800]   in a very complicated world, but yes.
[01:12:53.800 --> 01:12:55.720]   - Well, yeah, of course, not all flops are created equal.
[01:12:55.720 --> 01:12:58.480]   If you have randomly initialized weights, it's not gonna.
[01:12:58.480 --> 01:13:00.640]   - Not all flops are created equal.
[01:13:00.640 --> 01:13:03.040]   - Some flops are doing way more useful things than others.
[01:13:03.040 --> 01:13:04.320]   - Yeah, yeah.
[01:13:04.320 --> 01:13:06.520]   Tell me about it.
[01:13:06.520 --> 01:13:07.840]   Okay, so more data.
[01:13:07.840 --> 01:13:09.560]   Scale means more scale in compute
[01:13:09.560 --> 01:13:11.580]   or scale in scale of data?
[01:13:11.580 --> 01:13:12.420]   - Both.
[01:13:12.420 --> 01:13:15.720]   - Diversity of data?
[01:13:15.720 --> 01:13:17.620]   - Diversity is very important in data.
[01:13:17.620 --> 01:13:21.440]   Yeah, I mean, we have, so we have about,
[01:13:21.440 --> 01:13:24.280]   I think we have like 5,000 daily actives.
[01:13:25.600 --> 01:13:29.000]   - How would you evaluate how FSD is doing?
[01:13:29.000 --> 01:13:30.040]   - Pretty well. - In self-driving?
[01:13:30.040 --> 01:13:31.160]   - Pretty well.
[01:13:31.160 --> 01:13:34.320]   - How's that race going between CalmAI and FSD?
[01:13:34.320 --> 01:13:36.280]   - Tesla is always one to two years ahead of us.
[01:13:36.280 --> 01:13:38.160]   They've always been one to two years ahead of us,
[01:13:38.160 --> 01:13:39.360]   and they probably always will be
[01:13:39.360 --> 01:13:41.560]   because they're not doing anything wrong.
[01:13:41.560 --> 01:13:43.640]   - What have you seen that's, since the last time we talked,
[01:13:43.640 --> 01:13:45.360]   that are interesting architectural decisions,
[01:13:45.360 --> 01:13:48.160]   training decisions, like the way they deploy stuff,
[01:13:48.160 --> 01:13:51.000]   the architectures they're using in terms of the software,
[01:13:51.000 --> 01:13:52.520]   how the teams are run, all that kind of stuff,
[01:13:52.520 --> 01:13:54.600]   data collection, anything interesting?
[01:13:54.600 --> 01:13:55.680]   - I mean, I know they're moving
[01:13:55.680 --> 01:13:58.040]   toward more of an end-to-end approach.
[01:13:58.040 --> 01:14:01.760]   - So creeping towards end-to-end as much as possible
[01:14:01.760 --> 01:14:03.160]   across the whole thing,
[01:14:03.160 --> 01:14:05.120]   the training, the data collection, everything.
[01:14:05.120 --> 01:14:06.920]   - They also have a very fancy simulator.
[01:14:06.920 --> 01:14:08.880]   They're probably saying all the same things we are.
[01:14:08.880 --> 01:14:10.640]   They're probably saying we just need to optimize,
[01:14:10.640 --> 01:14:12.320]   you know, what is the reward?
[01:14:12.320 --> 01:14:14.080]   Will you get a negative reward for disengagement, right?
[01:14:14.080 --> 01:14:16.000]   Like, everyone kind of knows this.
[01:14:16.000 --> 01:14:17.360]   It's just a question of who can actually build
[01:14:17.360 --> 01:14:18.960]   and deploy the system.
[01:14:18.960 --> 01:14:21.880]   - Yeah, I mean, this requires good software engineering,
[01:14:21.880 --> 01:14:23.240]   I think. - Yeah.
[01:14:23.240 --> 01:14:25.840]   - And the right kind of hardware.
[01:14:25.840 --> 01:14:27.920]   - Yeah, and the hardware to run it.
[01:14:27.920 --> 01:14:30.420]   - You still don't believe in cloud in that regard?
[01:14:30.420 --> 01:14:36.400]   - I have a compute cluster in my office, 800 amps.
[01:14:36.400 --> 01:14:37.560]   - Tiny grad.
[01:14:37.560 --> 01:14:40.240]   - It's 40 kilowatts at idle, our data center.
[01:14:40.240 --> 01:14:41.240]   That's crazy.
[01:14:41.240 --> 01:14:42.560]   We have 40 kilowatts just burning
[01:14:42.560 --> 01:14:44.080]   just when the computers are idle.
[01:14:44.080 --> 01:14:46.680]   - Just when I-- - Sorry, sorry, compute cluster.
[01:14:46.680 --> 01:14:48.060]   (laughing)
[01:14:48.060 --> 01:14:49.160]   - Compute cluster, I got it.
[01:14:49.160 --> 01:14:50.120]   - It's not a data center.
[01:14:50.120 --> 01:14:50.940]   - Yeah, yeah.
[01:14:50.940 --> 01:14:52.160]   - No, data centers are clouds.
[01:14:52.160 --> 01:14:53.880]   We don't have clouds.
[01:14:53.880 --> 01:14:55.040]   Data centers have air conditioners.
[01:14:55.040 --> 01:14:56.200]   We have fans.
[01:14:56.200 --> 01:14:57.800]   That makes it a compute cluster.
[01:14:57.800 --> 01:15:02.440]   - I'm guessing this is a kind of a legal distinction
[01:15:02.440 --> 01:15:03.360]   as compared to-- - Sure, yeah.
[01:15:03.360 --> 01:15:05.280]   We have a compute cluster.
[01:15:05.280 --> 01:15:07.760]   - You said that you don't think LLMs have consciousness,
[01:15:07.760 --> 01:15:09.600]   or at least not more than a chicken.
[01:15:09.600 --> 01:15:12.360]   Do you think they can reason?
[01:15:12.360 --> 01:15:13.560]   Is there something interesting to you
[01:15:13.560 --> 01:15:16.340]   about the word reason, about some of the capabilities
[01:15:16.340 --> 01:15:18.040]   that we think is kind of human,
[01:15:18.040 --> 01:15:23.040]   to be able to integrate complicated information
[01:15:23.040 --> 01:15:27.800]   and through a chain of thought,
[01:15:27.800 --> 01:15:31.120]   arrive at a conclusion that feels novel,
[01:15:31.120 --> 01:15:35.460]   a novel integration of disparate facts?
[01:15:35.460 --> 01:15:39.200]   - Yeah, I don't think that there's,
[01:15:39.200 --> 01:15:42.200]   I think that they can reason better than a lot of people.
[01:15:42.200 --> 01:15:44.360]   - Hey, isn't that amazing to you, though?
[01:15:44.360 --> 01:15:45.600]   Isn't that like an incredible thing
[01:15:45.600 --> 01:15:47.640]   that a transform could achieve?
[01:15:47.640 --> 01:15:50.640]   - I mean, I think that calculators can add better
[01:15:50.640 --> 01:15:52.200]   than a lot of people.
[01:15:52.200 --> 01:15:54.040]   - But language feels like,
[01:15:54.040 --> 01:15:56.400]   reasoning through the process of language,
[01:15:56.400 --> 01:15:59.240]   which looks a lot like thought.
[01:15:59.240 --> 01:16:02.840]   - Making brilliancies in chess,
[01:16:02.840 --> 01:16:04.880]   which feels a lot like thought.
[01:16:04.880 --> 01:16:07.040]   Whatever new thing that AI can do,
[01:16:07.040 --> 01:16:08.280]   everybody thinks is brilliant,
[01:16:08.280 --> 01:16:09.480]   and then like 20 years go by,
[01:16:09.480 --> 01:16:10.520]   and they're like, well, yeah, but chess,
[01:16:10.520 --> 01:16:11.400]   that's like mechanical.
[01:16:11.400 --> 01:16:13.000]   Like adding, that's like mechanical.
[01:16:13.000 --> 01:16:14.960]   - So you think language is not that special?
[01:16:14.960 --> 01:16:15.960]   It's like chess?
[01:16:15.960 --> 01:16:16.800]   - It's like chess, and it's like--
[01:16:16.800 --> 01:16:20.240]   - No, no, no, because it's very human, we take it,
[01:16:20.240 --> 01:16:23.080]   listen, there is something different
[01:16:23.080 --> 01:16:25.800]   between chess and language.
[01:16:25.800 --> 01:16:29.200]   Chess is a game that a subset of population plays.
[01:16:29.200 --> 01:16:32.120]   Language is something we use nonstop
[01:16:32.120 --> 01:16:34.360]   for all of our human interaction,
[01:16:34.360 --> 01:16:37.240]   and human interaction is fundamental to society.
[01:16:37.240 --> 01:16:38.760]   So it's like, holy shit,
[01:16:38.760 --> 01:16:42.600]   this language thing is not so difficult
[01:16:42.600 --> 01:16:45.960]   to like create in a machine.
[01:16:45.960 --> 01:16:48.480]   The problem is if you go back to 1960,
[01:16:48.480 --> 01:16:50.400]   and you tell them that you have a machine
[01:16:50.400 --> 01:16:54.000]   that can play amazing chess,
[01:16:54.000 --> 01:16:55.680]   of course someone in 1960 will tell you
[01:16:55.680 --> 01:16:57.560]   that machine is intelligent.
[01:16:57.560 --> 01:17:00.440]   Someone in 2010 won't, what's changed, right?
[01:17:00.440 --> 01:17:02.000]   Today, we think that these machines
[01:17:02.000 --> 01:17:04.160]   that have language are intelligent,
[01:17:04.160 --> 01:17:05.720]   but I think in 20 years, we're gonna be like,
[01:17:05.720 --> 01:17:07.220]   yeah, but can it reproduce?
[01:17:07.220 --> 01:17:11.440]   - So reproduction, yeah, we may redefine
[01:17:11.440 --> 01:17:14.280]   what it means to be, what is it,
[01:17:14.280 --> 01:17:17.320]   a high-performance living organism on Earth?
[01:17:17.320 --> 01:17:19.920]   - Humans are always gonna define a niche for themselves.
[01:17:19.920 --> 01:17:21.720]   Like, well, we're better than the machines
[01:17:21.720 --> 01:17:24.840]   because we can, and they tried creative for a bit,
[01:17:24.840 --> 01:17:26.920]   but no one believes that one anymore.
[01:17:26.920 --> 01:17:29.880]   - But niche, is that delusional,
[01:17:29.880 --> 01:17:31.360]   or is there some accuracy to that?
[01:17:31.360 --> 01:17:33.640]   Because maybe with chess, you start to realize
[01:17:33.640 --> 01:17:38.440]   that we have ill-conceived notions
[01:17:38.440 --> 01:17:41.760]   of what makes humans special,
[01:17:41.760 --> 01:17:44.680]   like the apex organism on Earth.
[01:17:44.680 --> 01:17:48.440]   - Yeah, and I think maybe we're gonna go through
[01:17:48.440 --> 01:17:49.940]   that same thing with language,
[01:17:49.940 --> 01:17:52.780]   and that same thing with creativity.
[01:17:52.780 --> 01:17:57.320]   - But language carries these notions of truth and so on,
[01:17:57.320 --> 01:17:58.980]   and so we might be like, wait,
[01:17:58.980 --> 01:18:01.720]   maybe truth is not carried by language.
[01:18:01.720 --> 01:18:03.200]   Maybe there's like a deeper thing.
[01:18:03.200 --> 01:18:05.120]   - The niche is getting smaller.
[01:18:05.120 --> 01:18:05.960]   - Oh, boy.
[01:18:05.960 --> 01:18:09.240]   - But no, no, no, you don't understand.
[01:18:09.240 --> 01:18:10.720]   Humans are created by God,
[01:18:10.720 --> 01:18:13.320]   and machines are created by humans, therefore.
[01:18:13.320 --> 01:18:16.000]   Right, like that'll be the last niche we have.
[01:18:16.000 --> 01:18:17.360]   - So what do you think about
[01:18:17.360 --> 01:18:19.000]   the rapid development of LLMs?
[01:18:19.000 --> 01:18:20.880]   If we could just stick on that.
[01:18:20.880 --> 01:18:23.160]   It's still incredibly impressive, like with ChagGPT.
[01:18:23.160 --> 01:18:24.700]   Just even ChagGPT, what are your thoughts
[01:18:24.700 --> 01:18:27.380]   about reinforcement learning with human feedback
[01:18:27.380 --> 01:18:28.980]   on these large language models?
[01:18:28.980 --> 01:18:33.480]   - I'd like to go back to when calculators first came out,
[01:18:33.480 --> 01:18:37.400]   or computers, and I wasn't around.
[01:18:37.400 --> 01:18:39.640]   Look, I'm 33 years old.
[01:18:39.640 --> 01:18:43.520]   And to like see how that affected,
[01:18:43.520 --> 01:18:47.880]   like society.
[01:18:47.880 --> 01:18:48.720]   - Maybe you're right.
[01:18:48.720 --> 01:18:53.720]   So I wanna put on the big picture hat here.
[01:18:53.720 --> 01:18:55.840]   - Oh my God, a refrigerator, wow.
[01:18:55.840 --> 01:18:59.200]   - Refrigerator, electricity, all that kind of stuff.
[01:18:59.200 --> 01:19:03.040]   But, you know, with the internet,
[01:19:03.040 --> 01:19:05.200]   large language models seeming human-like,
[01:19:05.200 --> 01:19:06.860]   basically passing a Turing test,
[01:19:07.880 --> 01:19:10.720]   it seems it might have really, at scale,
[01:19:10.720 --> 01:19:13.680]   rapid transformative effects on society.
[01:19:13.680 --> 01:19:17.160]   But you're saying like other technologies have as well.
[01:19:17.160 --> 01:19:20.680]   So maybe calculator's not the best example of that?
[01:19:20.680 --> 01:19:23.760]   'Cause that just seems like, well, no, maybe.
[01:19:23.760 --> 01:19:25.560]   Calculator-- - But the poor milk man.
[01:19:25.560 --> 01:19:27.200]   The day he learned about refrigerators,
[01:19:27.200 --> 01:19:28.240]   he's like, "I'm done."
[01:19:28.240 --> 01:19:32.360]   You're telling me you can just keep the milk in your house?
[01:19:32.360 --> 01:19:33.760]   You don't need me to deliver it every day?
[01:19:33.760 --> 01:19:34.920]   I'm done.
[01:19:34.920 --> 01:19:36.200]   - Well, yeah, you have to actually look
[01:19:36.200 --> 01:19:38.440]   at the practical impacts of certain technologies
[01:19:38.440 --> 01:19:40.360]   that they've had.
[01:19:40.360 --> 01:19:42.040]   Yeah, probably electricity's a big one.
[01:19:42.040 --> 01:19:43.760]   And also how rapidly it's spread.
[01:19:43.760 --> 01:19:46.120]   The internet's a big one.
[01:19:46.120 --> 01:19:48.080]   - I do think it's different this time, though.
[01:19:48.080 --> 01:19:49.400]   - Yeah, it just feels like--
[01:19:49.400 --> 01:19:51.600]   - The niche is getting smaller.
[01:19:51.600 --> 01:19:52.840]   - The niche is humans.
[01:19:52.840 --> 01:19:53.800]   - Yes.
[01:19:53.800 --> 01:19:55.280]   - That makes humans special.
[01:19:55.280 --> 01:19:56.120]   - Yes.
[01:19:56.120 --> 01:19:59.360]   - It feels like it's getting smaller rapidly, though.
[01:19:59.360 --> 01:20:00.200]   Doesn't it?
[01:20:00.200 --> 01:20:02.760]   Or is that just a feeling we dramatize everything?
[01:20:02.760 --> 01:20:04.080]   - I think we dramatize everything.
[01:20:04.080 --> 01:20:06.720]   I think that you ask the milk man
[01:20:06.720 --> 01:20:08.400]   when he saw refrigerators,
[01:20:08.400 --> 01:20:10.800]   "Are they gonna have one of these in every home?"
[01:20:10.800 --> 01:20:12.440]   (laughing)
[01:20:12.440 --> 01:20:14.440]   - Yeah, yeah, yeah.
[01:20:14.440 --> 01:20:18.840]   Yeah, but boys are impressive.
[01:20:18.840 --> 01:20:21.320]   So much more impressive than seeing
[01:20:21.320 --> 01:20:23.400]   a chess world champion AI system.
[01:20:23.400 --> 01:20:25.720]   - I disagree, actually.
[01:20:25.720 --> 01:20:27.240]   I disagree.
[01:20:27.240 --> 01:20:29.520]   I think things like MuZero and AlphaGo
[01:20:29.520 --> 01:20:31.640]   are so much more impressive.
[01:20:31.640 --> 01:20:33.640]   Because these things are playing
[01:20:33.640 --> 01:20:35.960]   beyond the highest human level.
[01:20:35.960 --> 01:20:41.720]   The language models are writing middle school level essays
[01:20:41.720 --> 01:20:43.880]   and people are like, "Wow, it's a great essay.
[01:20:43.880 --> 01:20:45.400]   "It's a great five paragraph essay
[01:20:45.400 --> 01:20:47.280]   "about the causes of the Civil War."
[01:20:47.280 --> 01:20:48.360]   - Okay, forget the Civil War,
[01:20:48.360 --> 01:20:50.120]   just generating code, codex.
[01:20:50.120 --> 01:20:51.400]   - Oh!
[01:20:51.400 --> 01:20:53.560]   - You're saying it's mediocre code.
[01:20:53.560 --> 01:20:54.400]   - Terrible.
[01:20:54.400 --> 01:20:55.760]   - But I don't think it's terrible.
[01:20:55.760 --> 01:20:58.040]   I think it's just mediocre code.
[01:20:58.040 --> 01:20:58.880]   - Yeah.
[01:20:58.880 --> 01:21:01.080]   - Often close to correct.
[01:21:01.680 --> 01:21:03.000]   Like for mediocre purposes.
[01:21:03.000 --> 01:21:04.800]   - That's the scariest kind of code.
[01:21:04.800 --> 01:21:08.000]   I spend 5% of time typing and 95% of time debugging.
[01:21:08.000 --> 01:21:10.720]   The last thing I want is close to correct code.
[01:21:10.720 --> 01:21:12.640]   I want a machine that can help me with the debugging,
[01:21:12.640 --> 01:21:13.640]   not with the typing.
[01:21:13.640 --> 01:21:16.680]   - You know, it's like L2, level two driving,
[01:21:16.680 --> 01:21:17.520]   similar kind of thing.
[01:21:17.520 --> 01:21:21.040]   Yeah, you still should be a good programmer
[01:21:21.040 --> 01:21:22.600]   in order to modify.
[01:21:22.600 --> 01:21:24.000]   I wouldn't even say debugging,
[01:21:24.000 --> 01:21:25.840]   it's just modifying the code, reading it.
[01:21:25.840 --> 01:21:28.480]   - I actually don't think it's like level two driving.
[01:21:28.480 --> 01:21:30.920]   I think driving is not tool complete and programming is.
[01:21:30.920 --> 01:21:34.080]   Meaning you don't use the best possible tools to drive.
[01:21:34.080 --> 01:21:36.840]   You're not like,
[01:21:36.840 --> 01:21:39.480]   cars have basically the same interface
[01:21:39.480 --> 01:21:40.560]   for the last 50 years.
[01:21:40.560 --> 01:21:41.400]   - Yep.
[01:21:41.400 --> 01:21:42.960]   - Computers have a radically different interface.
[01:21:42.960 --> 01:21:46.440]   - Okay, can you describe the concept of tool complete?
[01:21:46.440 --> 01:21:47.280]   - Yeah.
[01:21:47.280 --> 01:21:49.440]   So think about the difference between a car from 1980
[01:21:49.440 --> 01:21:50.360]   and a car from today.
[01:21:50.360 --> 01:21:51.200]   - Yeah.
[01:21:51.200 --> 01:21:52.040]   - No difference really.
[01:21:52.040 --> 01:21:54.320]   It's got a bunch of pedals, it's got a steering wheel.
[01:21:54.320 --> 01:21:55.160]   Great.
[01:21:55.160 --> 01:21:57.240]   Maybe now it has a few ADAS features,
[01:21:57.240 --> 01:21:58.640]   but it's pretty much the same car.
[01:21:58.640 --> 01:22:00.560]   Right, you have no problem getting into a 1980 car
[01:22:00.560 --> 01:22:01.400]   and driving it.
[01:22:01.400 --> 01:22:04.240]   Take a programmer today who spent their whole life
[01:22:04.240 --> 01:22:07.400]   doing JavaScript and you put them in an Apple IIe prompt
[01:22:07.400 --> 01:22:09.840]   and you tell them about the line numbers in basic.
[01:22:09.840 --> 01:22:15.320]   But how do I insert something between line 17 and 18?
[01:22:15.320 --> 01:22:16.320]   Oh, wow.
[01:22:16.320 --> 01:22:21.080]   - But the, so in tool you're putting in
[01:22:21.080 --> 01:22:22.600]   the programming languages.
[01:22:22.600 --> 01:22:24.840]   So it's just the entirety stack of the tooling.
[01:22:24.840 --> 01:22:25.680]   - Exactly.
[01:22:25.680 --> 01:22:27.600]   - So it's not just like the, like IDs or something like this,
[01:22:27.600 --> 01:22:28.640]   it's everything.
[01:22:28.640 --> 01:22:30.480]   - Yes, it's IDEs, the languages, the runtimes,
[01:22:30.480 --> 01:22:31.360]   it's everything.
[01:22:31.360 --> 01:22:33.280]   And programming is tool complete.
[01:22:33.280 --> 01:22:38.280]   So like almost if Codex or Copilot are helping you,
[01:22:38.280 --> 01:22:41.720]   that actually probably means that your framework
[01:22:41.720 --> 01:22:44.520]   or library is bad and there's too much boilerplate in it.
[01:22:44.520 --> 01:22:49.400]   - Yeah, but don't you think so much programming
[01:22:49.400 --> 01:22:50.640]   has boilerplate?
[01:22:50.640 --> 01:22:54.200]   - TinyGrad is now 2,700 lines
[01:22:54.200 --> 01:22:56.680]   and it can run LLAMA and stable diffusion.
[01:22:56.680 --> 01:23:00.320]   And all of this stuff is in 2,700 lines.
[01:23:00.320 --> 01:23:04.280]   Boilerplate and abstraction indirections
[01:23:04.280 --> 01:23:06.960]   and all these things are just bad code.
[01:23:06.960 --> 01:23:13.400]   - Well, let's talk about good code and bad code.
[01:23:13.400 --> 01:23:16.000]   There's a, I would say, I don't know,
[01:23:16.000 --> 01:23:19.080]   for generic scripts that I write just offhand,
[01:23:19.080 --> 01:23:22.760]   like 80% of it is written by GPT.
[01:23:22.760 --> 01:23:25.160]   Just like quick, quick, like offhand stuff.
[01:23:25.160 --> 01:23:27.960]   So not like libraries, not like performing code,
[01:23:27.960 --> 01:23:31.000]   not stuff for robotics and so on, just quick stuff.
[01:23:31.000 --> 01:23:33.160]   Because your basic, so much of programming
[01:23:33.160 --> 01:23:36.520]   is doing some, yeah, boilerplate.
[01:23:36.520 --> 01:23:38.980]   But to do so efficiently and quickly,
[01:23:38.980 --> 01:23:42.960]   'cause you can't really automate it fully
[01:23:42.960 --> 01:23:45.960]   with like generic method, like a generic kind of ID
[01:23:45.960 --> 01:23:50.000]   type of recommendation or something like this.
[01:23:50.000 --> 01:23:52.000]   You do need to have some of the complexity
[01:23:52.000 --> 01:23:53.400]   of language models.
[01:23:53.400 --> 01:23:55.320]   - Yeah, I guess if I was really writing,
[01:23:55.640 --> 01:23:59.760]   maybe today if I wrote a lot of data parsing stuff,
[01:23:59.760 --> 01:24:00.880]   I mean, I don't play CTFs anymore,
[01:24:00.880 --> 01:24:02.880]   but if I still played CTFs, a lot of it is just like
[01:24:02.880 --> 01:24:05.200]   you have to write a parser for this data format.
[01:24:05.200 --> 01:24:08.440]   Like I wonder, or like admin of code,
[01:24:08.440 --> 01:24:10.680]   I wonder when the models are gonna start to help
[01:24:10.680 --> 01:24:11.760]   with that kind of code.
[01:24:11.760 --> 01:24:13.240]   And they may, they may.
[01:24:13.240 --> 01:24:15.760]   And the models also may help you with speed.
[01:24:15.760 --> 01:24:17.240]   And the models are very fast.
[01:24:17.240 --> 01:24:19.440]   But where the models won't,
[01:24:19.440 --> 01:24:22.600]   my programming speed is not at all limited
[01:24:22.600 --> 01:24:23.920]   by my typing speed.
[01:24:24.920 --> 01:24:29.200]   And in very few cases it is, yes.
[01:24:29.200 --> 01:24:31.400]   If I'm writing some script to just like parse
[01:24:31.400 --> 01:24:33.440]   some weird data format, sure.
[01:24:33.440 --> 01:24:35.360]   My programming speed is limited by my typing speed.
[01:24:35.360 --> 01:24:36.920]   - What about looking stuff up?
[01:24:36.920 --> 01:24:39.480]   'Cause that's essentially a more efficient lookup, right?
[01:24:39.480 --> 01:24:42.160]   - You know, when I was at Twitter,
[01:24:42.160 --> 01:24:46.240]   I tried to use a chat GPT to like ask some questions,
[01:24:46.240 --> 01:24:48.160]   like what's the API for this?
[01:24:48.160 --> 01:24:49.960]   And it would just hallucinate.
[01:24:49.960 --> 01:24:52.720]   It would just give me completely made up API functions
[01:24:52.720 --> 01:24:54.440]   that sounded real.
[01:24:54.440 --> 01:24:57.360]   - Well, do you think that's just a temporary kind of stage?
[01:24:57.360 --> 01:24:58.640]   - No.
[01:24:58.640 --> 01:25:00.320]   - You don't think it'll get better and better and better
[01:25:00.320 --> 01:25:01.160]   in this kind of stuff?
[01:25:01.160 --> 01:25:04.200]   'Cause like it only hallucinates stuff in the edge cases.
[01:25:04.200 --> 01:25:05.040]   - Yes, yes.
[01:25:05.040 --> 01:25:06.800]   - If you're writing generic code, it's actually pretty good.
[01:25:06.800 --> 01:25:08.880]   - Yes, if you are writing an absolute basic
[01:25:08.880 --> 01:25:10.280]   like React app with a button,
[01:25:10.280 --> 01:25:12.240]   it's not gonna hallucinate, sure.
[01:25:12.240 --> 01:25:14.840]   No, there's kind of ways to fix the hallucination problem.
[01:25:14.840 --> 01:25:16.480]   I think Facebook has an interesting paper,
[01:25:16.480 --> 01:25:17.760]   it's called Atlas.
[01:25:17.760 --> 01:25:20.840]   And it's actually weird the way that we do language models
[01:25:20.840 --> 01:25:25.840]   right now where all of the information is in the weights.
[01:25:25.840 --> 01:25:27.680]   And the human brain is not really like this.
[01:25:27.680 --> 01:25:29.720]   It's like a hippocampus and a memory system.
[01:25:29.720 --> 01:25:31.840]   So why don't LLMs have a memory system?
[01:25:31.840 --> 01:25:33.000]   And there's people working on them.
[01:25:33.000 --> 01:25:36.160]   I think future LLMs are gonna be like smaller,
[01:25:36.160 --> 01:25:39.360]   but are going to run looping on themselves
[01:25:39.360 --> 01:25:41.720]   and are going to have retrieval systems.
[01:25:41.720 --> 01:25:43.440]   And the thing about using a retrieval system
[01:25:43.440 --> 01:25:45.680]   is you can cite sources explicitly.
[01:25:45.680 --> 01:25:50.360]   - Which is really helpful to integrate
[01:25:50.360 --> 01:25:52.640]   the human into the loop of the thing.
[01:25:52.640 --> 01:25:54.000]   'Cause you can go check the sources
[01:25:54.000 --> 01:25:55.400]   and you can investigate.
[01:25:55.400 --> 01:25:57.240]   So whenever the thing is hallucinating,
[01:25:57.240 --> 01:25:59.720]   you can like have the human supervision.
[01:25:59.720 --> 01:26:01.600]   That's pushing it towards level two kind of journey.
[01:26:01.600 --> 01:26:03.360]   - That's gonna kill Google.
[01:26:03.360 --> 01:26:04.280]   - Wait, which part?
[01:26:04.280 --> 01:26:06.080]   - When someone makes an LLM that's capable
[01:26:06.080 --> 01:26:08.840]   of citing its sources, it will kill Google.
[01:26:08.840 --> 01:26:10.320]   - LLM that's citing its sources
[01:26:10.320 --> 01:26:13.120]   because that's basically a search engine.
[01:26:13.120 --> 01:26:14.640]   - That's what people want in a search engine.
[01:26:14.640 --> 01:26:16.760]   - But also Google might be the people that build it.
[01:26:16.760 --> 01:26:17.600]   - Maybe.
[01:26:17.600 --> 01:26:18.960]   - And put ads on it.
[01:26:18.960 --> 01:26:20.880]   - I'd count them out.
[01:26:20.880 --> 01:26:21.720]   - Why is that?
[01:26:21.720 --> 01:26:22.560]   What do you think?
[01:26:22.560 --> 01:26:24.920]   Who wins this race?
[01:26:24.920 --> 01:26:27.320]   We got, who are the competitors?
[01:26:27.320 --> 01:26:29.600]   We got Tiny Corp.
[01:26:29.600 --> 01:26:30.880]   I don't know if that's,
[01:26:30.880 --> 01:26:33.800]   yeah, I mean, you're a legitimate competitor in that.
[01:26:33.800 --> 01:26:35.620]   - I'm not trying to compete on that.
[01:26:35.620 --> 01:26:36.460]   - You're not?
[01:26:36.460 --> 01:26:37.280]   - No, not as a competitor.
[01:26:37.280 --> 01:26:38.120]   - You're just gonna accidentally stumble
[01:26:38.120 --> 01:26:39.680]   into that competition.
[01:26:39.680 --> 01:26:40.520]   - Maybe.
[01:26:40.520 --> 01:26:41.360]   - You don't think you might build a search engine
[01:26:41.360 --> 01:26:43.240]   to replace Google search?
[01:26:43.240 --> 01:26:46.560]   - When I started Comma, I said over and over again,
[01:26:46.560 --> 01:26:47.960]   I'm going to win self-driving cars.
[01:26:47.960 --> 01:26:49.520]   I still believe that.
[01:26:49.520 --> 01:26:51.960]   I have never said I'm going to win search
[01:26:51.960 --> 01:26:52.800]   with the Tiny Corp,
[01:26:52.800 --> 01:26:55.360]   and I'm never going to say that 'cause I won't.
[01:26:55.360 --> 01:26:56.640]   - The night is still young.
[01:26:56.640 --> 01:26:58.400]   We don't, you don't know how hard is it
[01:26:58.400 --> 01:27:00.520]   to win search in this new world.
[01:27:00.520 --> 01:27:03.000]   Like, it's, it feels, I mean,
[01:27:03.000 --> 01:27:04.680]   one of the things that ChatterGPT kind of shows
[01:27:04.680 --> 01:27:06.920]   that there could be a few interesting tricks
[01:27:06.920 --> 01:27:09.160]   that really have, that create a really compelling product.
[01:27:09.160 --> 01:27:10.700]   - Some startup's gonna figure it out.
[01:27:10.700 --> 01:27:12.540]   I think, I think if you ask me,
[01:27:12.540 --> 01:27:14.200]   like, Google's still the number one webpage,
[01:27:14.200 --> 01:27:15.160]   I think by the end of the decade,
[01:27:15.160 --> 01:27:17.400]   Google won't be the number one webpage anymore.
[01:27:17.400 --> 01:27:19.000]   - So you don't think Google,
[01:27:19.000 --> 01:27:21.760]   because of the, how big the corporation is?
[01:27:21.760 --> 01:27:25.000]   - Look, I would put a lot more money on Mark Zuckerberg.
[01:27:25.000 --> 01:27:25.840]   - Why is that?
[01:27:25.840 --> 01:27:29.200]   - Because Mark Zuckerberg's alive.
[01:27:29.200 --> 01:27:32.840]   Like, this is old Paul Graham essay.
[01:27:32.840 --> 01:27:34.600]   Startups are either alive or dead.
[01:27:34.600 --> 01:27:35.440]   Google's dead.
[01:27:35.440 --> 01:27:38.600]   Facebook's alive. - Versus Facebook is alive.
[01:27:38.600 --> 01:27:40.800]   - Well, actually, meta. - Meta.
[01:27:40.800 --> 01:27:41.640]   - You see what I mean?
[01:27:41.640 --> 01:27:43.400]   Like, that's just, like, like, like Mark Zuckerberg.
[01:27:43.400 --> 01:27:45.320]   This is Mark Zuckerberg reading that Paul Graham essay
[01:27:45.320 --> 01:27:46.760]   and being like, I'm gonna show everyone
[01:27:46.760 --> 01:27:47.600]   how alive we are.
[01:27:47.600 --> 01:27:49.080]   I'm gonna change the name.
[01:27:49.080 --> 01:27:53.960]   - So you don't think there's this gutsy pivoting engine
[01:27:53.960 --> 01:27:57.720]   that, like, Google doesn't have that,
[01:27:57.720 --> 01:28:00.520]   the kind of engine that a startup has, like, constantly.
[01:28:00.520 --> 01:28:03.000]   - You know what? - Being alive, I guess.
[01:28:03.000 --> 01:28:05.760]   - When I listened to your Sam Altman podcast,
[01:28:05.760 --> 01:28:06.600]   he talked about the button.
[01:28:06.600 --> 01:28:08.080]   Everyone who talks about AI talks about the button,
[01:28:08.080 --> 01:28:09.800]   the button to turn it off, right?
[01:28:09.800 --> 01:28:12.560]   Do we have a button to turn off Google?
[01:28:12.560 --> 01:28:15.600]   Is anybody in the world capable of shutting Google down?
[01:28:16.600 --> 01:28:18.240]   - What does that mean exactly?
[01:28:18.240 --> 01:28:19.720]   The company or the search engine?
[01:28:19.720 --> 01:28:21.000]   - Could we shut the search engine down?
[01:28:21.000 --> 01:28:23.280]   Could we shut the company down?
[01:28:23.280 --> 01:28:24.280]   Either.
[01:28:24.280 --> 01:28:26.480]   - Can you elaborate on the value of that question?
[01:28:26.480 --> 01:28:28.480]   - Does Sundar Pichai have the authority
[01:28:28.480 --> 01:28:30.200]   to turn off google.com tomorrow?
[01:28:30.200 --> 01:28:32.640]   - Who has the authority?
[01:28:32.640 --> 01:28:34.280]   That's a good question, right? - Does anyone?
[01:28:34.280 --> 01:28:35.120]   - Does anyone?
[01:28:35.120 --> 01:28:37.560]   Yeah, I'm sure.
[01:28:37.560 --> 01:28:38.640]   - Are you sure?
[01:28:38.640 --> 01:28:40.240]   No, they have the technical power,
[01:28:40.240 --> 01:28:41.640]   but do they have the authority?
[01:28:41.640 --> 01:28:44.640]   Let's say Sundar Pichai made this his sole mission.
[01:28:44.640 --> 01:28:46.000]   He came into Google tomorrow and said,
[01:28:46.000 --> 01:28:47.800]   "I'm gonna shut google.com down."
[01:28:47.800 --> 01:28:50.560]   I don't think he'd keep his position too long.
[01:28:50.560 --> 01:28:53.600]   - And what is the mechanism
[01:28:53.600 --> 01:28:55.280]   by which he wouldn't keep his position?
[01:28:55.280 --> 01:28:59.520]   - Well, boards and shares and corporate undermining
[01:28:59.520 --> 01:29:02.760]   and oh my God, our revenue is zero now.
[01:29:02.760 --> 01:29:05.160]   - Okay, so what's the case you're making here?
[01:29:05.160 --> 01:29:07.360]   So the capitalist machine prevents you
[01:29:07.360 --> 01:29:09.160]   from having the button?
[01:29:09.160 --> 01:29:10.600]   - Yeah, and it will have a,
[01:29:10.600 --> 01:29:12.320]   I mean, this is true for the AIs too, right?
[01:29:12.320 --> 01:29:14.640]   There's no turning the AIs off.
[01:29:14.640 --> 01:29:16.800]   There's no button, you can't press it.
[01:29:16.800 --> 01:29:19.080]   Now, does Mark Zuckerberg have that button
[01:29:19.080 --> 01:29:20.080]   for facebook.com?
[01:29:20.080 --> 01:29:22.280]   - Yes, probably more.
[01:29:22.280 --> 01:29:23.520]   - I think he does.
[01:29:23.520 --> 01:29:25.720]   I think he does, and this is exactly what I mean
[01:29:25.720 --> 01:29:29.120]   and why I bet on him so much more than I bet on Google.
[01:29:29.120 --> 01:29:31.280]   - I guess you could say Elon has similar stuff.
[01:29:31.280 --> 01:29:32.760]   - Oh, Elon has the button.
[01:29:32.760 --> 01:29:33.600]   - Yeah.
[01:29:33.600 --> 01:29:36.840]   - Elon, does Elon, can Elon fire the missiles?
[01:29:36.840 --> 01:29:38.080]   Can he fire the missiles?
[01:29:39.000 --> 01:29:42.320]   - I think some questions are better left unasked.
[01:29:42.320 --> 01:29:43.680]   - Right?
[01:29:43.680 --> 01:29:45.440]   I mean, you know, a rocket and an ICBM,
[01:29:45.440 --> 01:29:47.080]   well, you're a rocket that can land anywhere.
[01:29:47.080 --> 01:29:48.080]   Is that an ICBM?
[01:29:48.080 --> 01:29:51.080]   Well, you know, don't ask too many questions.
[01:29:51.080 --> 01:29:52.000]   - My God.
[01:29:52.000 --> 01:29:57.240]   But the positive side of the button
[01:29:57.240 --> 01:30:00.560]   is that you can innovate aggressively, is what you're saying,
[01:30:00.560 --> 01:30:03.440]   which is what's required with turning LLM
[01:30:03.440 --> 01:30:04.320]   into a search engine.
[01:30:04.320 --> 01:30:05.320]   - I would bet on a startup.
[01:30:05.320 --> 01:30:06.560]   I bet on- - 'Cause it's so easy, right?
[01:30:06.560 --> 01:30:08.120]   - I bet on something that looks like mid-journey,
[01:30:08.120 --> 01:30:09.560]   but for search.
[01:30:09.560 --> 01:30:13.920]   - Just is able to site source a loop on itself.
[01:30:13.920 --> 01:30:15.840]   I mean, it just feels like one model can take off.
[01:30:15.840 --> 01:30:16.680]   - Yeah. - Right?
[01:30:16.680 --> 01:30:18.920]   And that nice wrapper and some of it,
[01:30:18.920 --> 01:30:21.240]   I mean, it's hard to like create a product
[01:30:21.240 --> 01:30:23.480]   that just works really nicely, stably.
[01:30:23.480 --> 01:30:25.120]   - The other thing that's gonna be cool
[01:30:25.120 --> 01:30:28.280]   is there is some aspect of a winner-take-all effect, right?
[01:30:28.280 --> 01:30:31.920]   Like once someone starts deploying a product
[01:30:31.920 --> 01:30:34.840]   that gets a lot of usage, and you see this with OpenAI,
[01:30:34.840 --> 01:30:36.640]   they are going to get the dataset
[01:30:36.640 --> 01:30:39.000]   to train future versions of the model.
[01:30:39.000 --> 01:30:41.320]   - Yeah. - They are going to be able to,
[01:30:41.320 --> 01:30:42.720]   you know, I was actually at Google Image Search
[01:30:42.720 --> 01:30:44.920]   when I worked there like almost 15 years ago now.
[01:30:44.920 --> 01:30:46.800]   How does Google know which image is an apple?
[01:30:46.800 --> 01:30:48.000]   And I said the metadata.
[01:30:48.000 --> 01:30:49.840]   And they're like, yeah, that works about half the time.
[01:30:49.840 --> 01:30:50.960]   How does Google know?
[01:30:50.960 --> 01:30:52.400]   You'll see they're all apples on the front page
[01:30:52.400 --> 01:30:54.200]   when you search apple.
[01:30:54.200 --> 01:30:57.000]   And I don't know, I didn't come up with the answer.
[01:30:57.000 --> 01:30:58.200]   The guy's like, well, it's what people click on
[01:30:58.200 --> 01:30:59.040]   when they search apple.
[01:30:59.040 --> 01:31:00.280]   I'm like, oh, yeah.
[01:31:00.280 --> 01:31:02.680]   - Yeah, yeah, that data is really, really powerful.
[01:31:02.680 --> 01:31:04.440]   It's the human supervision.
[01:31:04.440 --> 01:31:06.000]   What do you think are the chances?
[01:31:06.000 --> 01:31:09.480]   What do you think in general that Lama was open sourced?
[01:31:09.480 --> 01:31:13.880]   I just did a conversation with Mark Zuckerberg,
[01:31:13.880 --> 01:31:16.400]   and he's all in on open source.
[01:31:16.400 --> 01:31:19.600]   - Who would have thought that Mark Zuckerberg
[01:31:19.600 --> 01:31:20.700]   would be the good guy?
[01:31:20.700 --> 01:31:23.360]   I mean it.
[01:31:23.360 --> 01:31:25.800]   - Who would have thought anything in this world?
[01:31:25.800 --> 01:31:27.280]   It's hard to know.
[01:31:27.280 --> 01:31:32.280]   But open source to you ultimately is a good thing here.
[01:31:32.280 --> 01:31:34.520]   - Undoubtedly.
[01:31:35.520 --> 01:31:39.600]   You know, what's ironic about all these AI safety people
[01:31:39.600 --> 01:31:42.200]   is they are going to build the exact thing they fear.
[01:31:42.200 --> 01:31:47.720]   These we need to have one model that we control and align,
[01:31:47.720 --> 01:31:50.640]   this is the only way you end up paper clipped.
[01:31:50.640 --> 01:31:52.440]   There's no way you end up paper clipped
[01:31:52.440 --> 01:31:54.040]   if everybody has an AI.
[01:31:54.040 --> 01:31:55.360]   - So open sourcing is the way
[01:31:55.360 --> 01:31:56.920]   to fight the paper clip maximizer.
[01:31:56.920 --> 01:31:58.200]   - Absolutely.
[01:31:58.200 --> 01:31:59.160]   It's the only way.
[01:31:59.160 --> 01:32:00.480]   You think you're gonna control it?
[01:32:00.480 --> 01:32:02.040]   You're not gonna control it.
[01:32:02.040 --> 01:32:05.200]   - So the criticism you have for the AI safety folks
[01:32:05.200 --> 01:32:10.200]   is that there is a belief and a desire for control.
[01:32:10.200 --> 01:32:13.760]   And that belief and desire for centralized control
[01:32:13.760 --> 01:32:16.840]   of dangerous AI systems is not good.
[01:32:16.840 --> 01:32:19.120]   - Sam Altman won't tell you that GPT-4
[01:32:19.120 --> 01:32:21.200]   has 220 billion parameters
[01:32:21.200 --> 01:32:24.100]   and is a 16-way mixture model with eight sets of weights?
[01:32:24.100 --> 01:32:28.040]   - Who did you have to murder to get that information?
[01:32:28.040 --> 01:32:30.400]   All right.
[01:32:30.400 --> 01:32:32.200]   - I mean look, everyone at OpenAI knows
[01:32:32.200 --> 01:32:34.000]   what I just said was true.
[01:32:34.000 --> 01:32:36.960]   Now, ask the question really.
[01:32:36.960 --> 01:32:40.120]   You know, it upsets me when I, like GPT-2.
[01:32:40.120 --> 01:32:41.680]   When OpenAI came out with GPT-2
[01:32:41.680 --> 01:32:44.080]   and raised a whole fake AI safety thing about that,
[01:32:44.080 --> 01:32:46.520]   I mean now the model is laughable.
[01:32:46.520 --> 01:32:50.480]   Like they used AI safety to hype up their company
[01:32:50.480 --> 01:32:51.480]   and it's disgusting.
[01:32:51.480 --> 01:32:56.160]   - Or the flip side of that is they used
[01:32:56.160 --> 01:32:58.780]   a relatively weak model in retrospect
[01:32:58.780 --> 01:33:01.880]   to explore how do we do AI safety correctly?
[01:33:01.880 --> 01:33:02.800]   How do we release things?
[01:33:02.800 --> 01:33:04.360]   How do we go through the process?
[01:33:04.360 --> 01:33:05.800]   I don't know if--
[01:33:05.800 --> 01:33:06.640]   - Sure, sure.
[01:33:06.640 --> 01:33:07.480]   All right, all right, all right.
[01:33:07.480 --> 01:33:10.240]   That's the charitable interpretation.
[01:33:10.240 --> 01:33:12.080]   - I don't know how much hype there is in AI safety,
[01:33:12.080 --> 01:33:12.920]   honestly.
[01:33:12.920 --> 01:33:13.740]   - Oh, there's so much hype.
[01:33:13.740 --> 01:33:14.920]   At least on Twitter, I don't know.
[01:33:14.920 --> 01:33:15.760]   Maybe Twitter's not real life.
[01:33:15.760 --> 01:33:17.920]   - Twitter's not real life.
[01:33:17.920 --> 01:33:18.760]   Come on.
[01:33:18.760 --> 01:33:21.040]   In terms of hype, I mean, I don't,
[01:33:21.040 --> 01:33:24.080]   I think OpenAI has been finding an interesting balance
[01:33:24.080 --> 01:33:29.080]   between transparency and putting value on AI safety.
[01:33:29.080 --> 01:33:32.200]   You think just go all out open source
[01:33:32.200 --> 01:33:33.720]   or do a llama.
[01:33:33.720 --> 01:33:34.560]   - Absolutely, yeah.
[01:33:34.560 --> 01:33:37.120]   - So do like open source.
[01:33:37.120 --> 01:33:38.360]   This is a tough question,
[01:33:38.360 --> 01:33:41.280]   which is open source both the base,
[01:33:41.280 --> 01:33:44.200]   the foundation model and the fine-tuned one.
[01:33:44.200 --> 01:33:48.720]   So like the model that can be ultra racist and dangerous
[01:33:48.720 --> 01:33:51.800]   and like tell you how to build a nuclear weapon.
[01:33:51.800 --> 01:33:53.600]   - Oh my God, have you met humans?
[01:33:53.600 --> 01:33:54.440]   Right?
[01:33:54.440 --> 01:33:55.280]   Like half of these AI--
[01:33:55.280 --> 01:33:57.480]   - I haven't met most humans.
[01:33:57.480 --> 01:34:00.520]   This makes, this allows you to meet every human.
[01:34:00.520 --> 01:34:02.960]   - Yeah, I know, but half of these AI alignment problems
[01:34:02.960 --> 01:34:04.480]   are just human alignment problems.
[01:34:04.480 --> 01:34:05.840]   And that's what's also so scary
[01:34:05.840 --> 01:34:06.900]   about the language they use.
[01:34:06.900 --> 01:34:09.860]   It's like, it's not the machines you want to align, it's me.
[01:34:09.860 --> 01:34:13.360]   - But here's the thing.
[01:34:13.360 --> 01:34:17.960]   It makes it very accessible to ask very
[01:34:17.960 --> 01:34:23.080]   questions where the answers have dangerous consequences
[01:34:23.080 --> 01:34:25.240]   if you were to act on them.
[01:34:25.240 --> 01:34:26.800]   - I mean, yeah.
[01:34:26.800 --> 01:34:28.400]   Welcome to the world.
[01:34:28.400 --> 01:34:30.560]   - Well, no, for me, there's a lot of friction.
[01:34:30.560 --> 01:34:35.480]   If I want to find out how to, I don't know,
[01:34:35.480 --> 01:34:36.760]   blow up something.
[01:34:36.760 --> 01:34:38.040]   - No, there's not a lot of friction.
[01:34:38.040 --> 01:34:39.040]   That's so easy.
[01:34:39.040 --> 01:34:40.560]   - No, like what do I search?
[01:34:40.560 --> 01:34:43.080]   Do I use Bing or do I, which search engine do I use?
[01:34:43.080 --> 01:34:44.920]   - No, there's like lots of stuff.
[01:34:44.920 --> 01:34:47.480]   - No, it feels like I have to keep clicking a lot of this.
[01:34:47.480 --> 01:34:49.360]   - Anyone who's stupid enough to search for
[01:34:49.360 --> 01:34:52.300]   how to blow up a building in my neighborhood
[01:34:52.300 --> 01:34:54.600]   is not smart enough to build a bomb, right?
[01:34:54.600 --> 01:34:55.440]   - Are you sure about that?
[01:34:55.440 --> 01:34:57.040]   - Yes.
[01:34:57.040 --> 01:35:02.040]   I feel like a language model makes it more accessible
[01:35:02.040 --> 01:35:05.440]   for that person who's not smart enough to do--
[01:35:05.440 --> 01:35:07.240]   - They're not gonna build a bomb, trust me.
[01:35:07.240 --> 01:35:11.340]   The people who are incapable of figuring out
[01:35:11.340 --> 01:35:13.720]   how to ask that question a bit more academically
[01:35:13.720 --> 01:35:15.760]   and get a real answer from it are not capable
[01:35:15.760 --> 01:35:17.880]   of procuring the materials, which are somewhat controlled,
[01:35:17.880 --> 01:35:19.460]   to build a bomb.
[01:35:19.460 --> 01:35:21.300]   - No, I think it makes it more accessible
[01:35:21.300 --> 01:35:24.880]   to people with money without the technical know-how, right?
[01:35:24.880 --> 01:35:27.160]   - To build a, like, do you really need to know
[01:35:27.160 --> 01:35:29.020]   how to build a bomb to build a bomb?
[01:35:29.020 --> 01:35:30.520]   You can hire people, you can find like--
[01:35:30.520 --> 01:35:32.240]   - Oh, you can hire people to build a,
[01:35:32.240 --> 01:35:33.720]   you know what, I was asking this question on my stream,
[01:35:33.720 --> 01:35:35.340]   like, can Jeff Bezos hire a hitman?
[01:35:35.340 --> 01:35:36.180]   Probably not.
[01:35:36.180 --> 01:35:41.720]   - But a language model can probably help you out.
[01:35:41.720 --> 01:35:43.240]   - Yeah, and you'll still go to jail, right?
[01:35:43.240 --> 01:35:45.080]   Like, it's not like the language model is God.
[01:35:45.080 --> 01:35:46.440]   Like, the language model, it's like,
[01:35:46.440 --> 01:35:49.400]   it's you literally just hired someone on Fiverr.
[01:35:49.400 --> 01:35:50.240]   Like, you--
[01:35:50.240 --> 01:35:52.920]   - But, okay, okay, GPT-4, in terms of finding a hitman,
[01:35:52.920 --> 01:35:54.920]   it's like asking Fiverr how to find a hitman.
[01:35:54.920 --> 01:35:56.360]   I understand, but don't you think--
[01:35:56.360 --> 01:35:57.200]   - Asking Wikihow, you know?
[01:35:57.200 --> 01:36:00.320]   - Wikihow, but don't you think GPT-5 will be better?
[01:36:00.320 --> 01:36:01.440]   'Cause don't you think that information
[01:36:01.440 --> 01:36:02.960]   is out there on the internet?
[01:36:02.960 --> 01:36:05.040]   - I mean, yeah, and I think that if someone
[01:36:05.040 --> 01:36:07.280]   is actually serious enough to hire a hitman
[01:36:07.280 --> 01:36:09.320]   or build a bomb, they'd also be serious enough
[01:36:09.320 --> 01:36:10.800]   to find the information.
[01:36:10.800 --> 01:36:11.720]   - I don't think so.
[01:36:11.720 --> 01:36:13.000]   I think it makes it more accessible.
[01:36:13.000 --> 01:36:15.800]   If you have enough money to buy a hitman,
[01:36:15.800 --> 01:36:18.460]   I think it decreases the friction
[01:36:18.460 --> 01:36:20.760]   of how hard is it to find that kind of hitman.
[01:36:20.760 --> 01:36:25.760]   I honestly think there's a jump in ease and scale
[01:36:25.760 --> 01:36:28.840]   of how much harm you can do.
[01:36:28.840 --> 01:36:30.280]   And I don't mean harm with language,
[01:36:30.280 --> 01:36:32.000]   I mean harm with actual violence.
[01:36:32.000 --> 01:36:33.480]   - What you're basically saying is like,
[01:36:33.480 --> 01:36:34.920]   okay, what's gonna happen is these people
[01:36:34.920 --> 01:36:38.060]   who are not intelligent are going to use machines
[01:36:38.060 --> 01:36:39.800]   to augment their intelligence.
[01:36:39.800 --> 01:36:42.140]   And now, intelligent people and machines,
[01:36:42.140 --> 01:36:43.640]   intelligence is scary.
[01:36:43.640 --> 01:36:45.960]   Intelligent agents are scary.
[01:36:45.960 --> 01:36:48.320]   When I'm in the woods, the scariest animal to meet
[01:36:48.320 --> 01:36:50.080]   is a human, right?
[01:36:50.080 --> 01:36:50.920]   No, no, no, no.
[01:36:50.920 --> 01:36:52.840]   Look, there's like nice California humans.
[01:36:52.840 --> 01:36:54.960]   Like I see you're wearing like, you know,
[01:36:54.960 --> 01:36:57.000]   street clothes and Nikes, all right, fine.
[01:36:57.000 --> 01:36:58.240]   But you look like you've been a human
[01:36:58.240 --> 01:36:59.400]   who's been in the woods for a while?
[01:36:59.400 --> 01:37:00.240]   - Yeah.
[01:37:00.240 --> 01:37:01.360]   - I'm more scared of you than a bear.
[01:37:01.360 --> 01:37:03.080]   - That's what they say about the Amazon.
[01:37:03.080 --> 01:37:05.200]   When you go to the Amazon, it's the human tribes.
[01:37:05.200 --> 01:37:06.280]   - Oh yeah.
[01:37:06.280 --> 01:37:09.080]   So intelligence is scary, right?
[01:37:09.080 --> 01:37:11.640]   So to ask this question in a generic way,
[01:37:11.640 --> 01:37:13.560]   you're like, what if we took everybody
[01:37:13.560 --> 01:37:17.380]   who maybe has ill intention but is not so intelligent
[01:37:17.380 --> 01:37:18.720]   and gave them intelligence?
[01:37:19.600 --> 01:37:20.520]   Right?
[01:37:20.520 --> 01:37:23.720]   So we should have intelligence control, of course.
[01:37:23.720 --> 01:37:25.600]   We should only give intelligence to good people.
[01:37:25.600 --> 01:37:27.960]   And that is the absolutely horrifying idea.
[01:37:27.960 --> 01:37:30.040]   - So to use the best defense is actually,
[01:37:30.040 --> 01:37:32.440]   the best defense is to give more intelligence
[01:37:32.440 --> 01:37:34.200]   to the good guys and intelligence,
[01:37:34.200 --> 01:37:35.320]   give intelligence to everybody.
[01:37:35.320 --> 01:37:36.240]   - Give intelligence to everybody.
[01:37:36.240 --> 01:37:37.440]   You know what, it's not even like guns, right?
[01:37:37.440 --> 01:37:38.320]   Like people say this about guns.
[01:37:38.320 --> 01:37:39.760]   You know, what's the best defense against a bad guy
[01:37:39.760 --> 01:37:40.920]   with a gun, a good guy with a gun?
[01:37:40.920 --> 01:37:42.160]   Like I kind of subscribe to that,
[01:37:42.160 --> 01:37:44.600]   but I really subscribe to that with intelligence.
[01:37:44.600 --> 01:37:48.200]   - Yeah, in a fundamental way, I agree with you.
[01:37:48.200 --> 01:37:50.120]   But there's just feels like so much uncertainty
[01:37:50.120 --> 01:37:51.720]   and so much can happen rapidly
[01:37:51.720 --> 01:37:53.160]   that you can lose a lot of control
[01:37:53.160 --> 01:37:54.640]   and you can do a lot of damage.
[01:37:54.640 --> 01:37:56.280]   - Oh no, we can lose control?
[01:37:56.280 --> 01:37:58.080]   Yes, thank God.
[01:37:58.080 --> 01:37:58.920]   - Yeah.
[01:37:58.920 --> 01:38:00.880]   - I hope we can, I hope they lose control.
[01:38:00.880 --> 01:38:05.340]   I'd want them to lose control more than anything else.
[01:38:05.340 --> 01:38:07.800]   - I think when you lose control, you can do a lot of damage,
[01:38:07.800 --> 01:38:11.120]   but you can do more damage when you centralize
[01:38:11.120 --> 01:38:12.720]   and hold onto control is the point here.
[01:38:12.720 --> 01:38:15.720]   - Centralized and held control is tyranny, right?
[01:38:15.720 --> 01:38:17.560]   I will always, I don't like anarchy either,
[01:38:17.560 --> 01:38:19.200]   but I will always take anarchy over tyranny.
[01:38:19.200 --> 01:38:20.560]   Anarchy, you have a chance.
[01:38:20.560 --> 01:38:24.280]   - This human civilization we've got going on
[01:38:24.280 --> 01:38:25.560]   is quite interesting.
[01:38:25.560 --> 01:38:26.400]   I mean, I agree with you.
[01:38:26.400 --> 01:38:30.640]   So to you, open source is the way forward here.
[01:38:30.640 --> 01:38:32.360]   So you admire what Facebook is doing here
[01:38:32.360 --> 01:38:34.400]   or what Meta is doing with the release of the--
[01:38:34.400 --> 01:38:35.240]   - A lot.
[01:38:35.240 --> 01:38:36.080]   - Yeah.
[01:38:36.080 --> 01:38:38.280]   - I lost $80,000 last year investing in Meta.
[01:38:38.280 --> 01:38:40.480]   And when they released Lama, I'm like, yeah, whatever, man.
[01:38:40.480 --> 01:38:41.720]   That was worth it.
[01:38:41.720 --> 01:38:43.160]   - It was worth it.
[01:38:43.160 --> 01:38:47.040]   Do you think Google and OpenAI with Microsoft will match?
[01:38:47.040 --> 01:38:49.760]   What Meta is doing or no?
[01:38:49.760 --> 01:38:52.320]   - So if I were a researcher,
[01:38:52.320 --> 01:38:53.880]   why would you wanna work at OpenAI?
[01:38:53.880 --> 01:38:56.960]   Like, you're on the bad team.
[01:38:56.960 --> 01:38:57.800]   Like, I mean it.
[01:38:57.800 --> 01:38:59.240]   Like you're on the bad team who can't even say
[01:38:59.240 --> 01:39:01.480]   that GPT-4 has 220 billion parameters.
[01:39:01.480 --> 01:39:03.920]   - So close source to use the bad team.
[01:39:03.920 --> 01:39:05.120]   - Not only close source.
[01:39:05.120 --> 01:39:08.080]   I'm not saying you need to make your model weights open.
[01:39:08.080 --> 01:39:09.080]   I'm not saying that.
[01:39:09.080 --> 01:39:11.320]   I totally understand we're keeping our model weights closed
[01:39:11.320 --> 01:39:12.600]   because that's our product, right?
[01:39:12.600 --> 01:39:13.960]   That's fine.
[01:39:13.960 --> 01:39:17.000]   I'm saying like, because of AI safety reasons,
[01:39:17.000 --> 01:39:19.320]   we can't tell you the number of billions
[01:39:19.320 --> 01:39:21.160]   of parameters in the model.
[01:39:21.160 --> 01:39:23.000]   That's just the bad guys.
[01:39:23.000 --> 01:39:24.920]   - Just because you're mocking AI safety
[01:39:24.920 --> 01:39:26.440]   doesn't mean it's not real.
[01:39:26.440 --> 01:39:27.280]   - Oh, of course.
[01:39:27.280 --> 01:39:29.320]   - Is it possible that these things can really do
[01:39:29.320 --> 01:39:31.000]   a lot of damage that we don't know?
[01:39:31.000 --> 01:39:32.120]   - Oh my God, yes.
[01:39:32.120 --> 01:39:34.000]   Intelligence is so dangerous,
[01:39:34.000 --> 01:39:36.680]   be it human intelligence or machine intelligence.
[01:39:36.680 --> 01:39:38.320]   Intelligence is dangerous.
[01:39:38.320 --> 01:39:40.360]   - But machine intelligence is so much easier
[01:39:40.360 --> 01:39:42.960]   to deploy at scale, like rapidly.
[01:39:42.960 --> 01:39:44.280]   Like what, okay.
[01:39:44.280 --> 01:39:46.200]   If you have human-like bots on Twitter,
[01:39:46.200 --> 01:39:47.040]   - Right.
[01:39:47.040 --> 01:39:50.080]   - And you have like a thousand of them,
[01:39:50.080 --> 01:39:52.080]   create a whole narrative,
[01:39:52.080 --> 01:39:55.720]   like you can manipulate millions of people.
[01:39:55.720 --> 01:39:57.960]   - But you mean like the intelligence agencies in America
[01:39:57.960 --> 01:39:59.040]   are doing right now?
[01:39:59.040 --> 01:40:01.120]   - Yeah, but they're not doing it that well.
[01:40:01.120 --> 01:40:03.360]   It feels like you can do a lot.
[01:40:03.360 --> 01:40:05.400]   - They're doing it pretty well.
[01:40:05.400 --> 01:40:06.240]   - What?
[01:40:06.240 --> 01:40:07.600]   - I think they're doing a pretty good job.
[01:40:07.600 --> 01:40:09.560]   - I suspect they're not nearly as good
[01:40:09.560 --> 01:40:12.680]   as a bunch of GPT-fueled bots could be.
[01:40:12.680 --> 01:40:13.800]   - Well, I mean, of course, they're looking
[01:40:13.800 --> 01:40:15.040]   into the latest technologies
[01:40:15.040 --> 01:40:16.920]   for control of people, of course.
[01:40:16.920 --> 01:40:19.080]   - But I think there's a George Hotz-type character
[01:40:19.080 --> 01:40:21.440]   that can do a better job than the entirety of them.
[01:40:21.440 --> 01:40:22.280]   You don't think so?
[01:40:22.280 --> 01:40:23.120]   - No way.
[01:40:23.120 --> 01:40:24.720]   No, and I'll tell you why the George Hotz character can't.
[01:40:24.720 --> 01:40:26.560]   And I thought about this a lot with hacking, right?
[01:40:26.560 --> 01:40:27.840]   Like I can find exploits in web browsers.
[01:40:27.840 --> 01:40:28.680]   I probably still can.
[01:40:28.680 --> 01:40:29.800]   I mean, I was better out when I was 24,
[01:40:29.800 --> 01:40:33.160]   but the thing that I lack is the ability
[01:40:33.160 --> 01:40:35.920]   to slowly and steadily deploy them over five years.
[01:40:35.920 --> 01:40:38.400]   And this is what intelligence agencies are very good at.
[01:40:38.400 --> 01:40:39.240]   Right?
[01:40:39.240 --> 01:40:40.060]   Intelligence agencies don't have
[01:40:40.060 --> 01:40:42.320]   the most sophisticated technology.
[01:40:42.320 --> 01:40:43.720]   They just have,
[01:40:43.720 --> 01:40:44.560]   - Endurance?
[01:40:44.560 --> 01:40:45.400]   - Endurance.
[01:40:45.400 --> 01:40:46.240]   - Yeah.
[01:40:46.240 --> 01:40:49.360]   - And yeah, the financial backing
[01:40:49.360 --> 01:40:51.960]   and the infrastructure for the endurance.
[01:40:51.960 --> 01:40:54.800]   - So the more we can decentralize power,
[01:40:54.800 --> 01:40:56.700]   like you could make an argument, by the way,
[01:40:56.700 --> 01:40:58.340]   that nobody should have these things.
[01:40:58.340 --> 01:40:59.680]   And I would defend that argument.
[01:40:59.680 --> 01:41:01.480]   I would, like you're saying, that look,
[01:41:01.480 --> 01:41:05.300]   LLMs and AI and machine intelligence can cause a lot of harm
[01:41:05.300 --> 01:41:06.880]   so nobody should have it.
[01:41:06.880 --> 01:41:08.800]   And I will respect someone philosophically
[01:41:08.800 --> 01:41:09.680]   with that position,
[01:41:09.680 --> 01:41:11.240]   just like I will respect someone philosophically
[01:41:11.240 --> 01:41:13.880]   with the position that nobody should have guns.
[01:41:13.880 --> 01:41:14.720]   Right?
[01:41:14.720 --> 01:41:16.080]   But I will not respect philosophically
[01:41:16.080 --> 01:41:20.000]   which with only the trusted authorities
[01:41:20.000 --> 01:41:21.440]   should have access to this.
[01:41:21.440 --> 01:41:22.280]   - Yeah.
[01:41:22.280 --> 01:41:23.520]   - Who are the trusted authorities?
[01:41:23.520 --> 01:41:24.360]   You know what?
[01:41:24.360 --> 01:41:25.920]   I'm not worried about alignment
[01:41:25.920 --> 01:41:29.700]   between AI company and their machines.
[01:41:29.700 --> 01:41:33.120]   I'm worried about alignment between me and AI company.
[01:41:33.120 --> 01:41:36.260]   - What do you think Eliezer Yudkowsky would say to you?
[01:41:36.260 --> 01:41:39.840]   Because he's really against open source.
[01:41:39.840 --> 01:41:40.800]   - I know.
[01:41:40.800 --> 01:41:44.960]   And I thought about this.
[01:41:44.960 --> 01:41:46.440]   I thought about this.
[01:41:46.440 --> 01:41:51.440]   And I think this comes down to a repeated misunderstanding
[01:41:51.440 --> 01:41:54.660]   of political power by the rationalists.
[01:41:54.660 --> 01:41:56.760]   - Interesting.
[01:41:56.760 --> 01:42:02.260]   - I think that Eliezer Yudkowsky is scared of these things.
[01:42:02.260 --> 01:42:04.080]   And I am scared of these things too.
[01:42:04.080 --> 01:42:05.880]   Everyone should be scared of these things.
[01:42:05.880 --> 01:42:07.120]   These things are scary.
[01:42:08.040 --> 01:42:11.160]   But now you ask about the two possible futures.
[01:42:11.160 --> 01:42:16.160]   One where a small trusted centralized group of people
[01:42:16.160 --> 01:42:19.320]   has them and the other where everyone has them.
[01:42:19.320 --> 01:42:21.280]   And I am much less scared of the second future
[01:42:21.280 --> 01:42:22.120]   than the first.
[01:42:22.120 --> 01:42:25.160]   - Well, there's a small trusted group of people
[01:42:25.160 --> 01:42:27.200]   that have control of our nuclear weapons.
[01:42:27.200 --> 01:42:30.040]   - There's a difference.
[01:42:30.040 --> 01:42:32.720]   Again, a nuclear weapon cannot be deployed tactically
[01:42:32.720 --> 01:42:34.240]   and a nuclear weapon is not a defense
[01:42:34.240 --> 01:42:35.520]   against a nuclear weapon.
[01:42:37.320 --> 01:42:40.120]   Except maybe in some philosophical mind game kind of way.
[01:42:40.120 --> 01:42:43.320]   - But AI is different.
[01:42:43.320 --> 01:42:44.360]   How exactly?
[01:42:44.360 --> 01:42:47.960]   - Okay, let's say the intelligence agency
[01:42:47.960 --> 01:42:50.200]   deploys a million bots on Twitter
[01:42:50.200 --> 01:42:51.280]   or a thousand bots on Twitter
[01:42:51.280 --> 01:42:53.460]   to try to convince me of a point.
[01:42:53.460 --> 01:42:56.720]   Imagine I had a powerful AI running on my computer
[01:42:56.720 --> 01:43:00.440]   saying, okay, nice PSYOP, nice PSYOP, nice PSYOP.
[01:43:00.440 --> 01:43:02.320]   Okay, here's a PSYOP.
[01:43:02.320 --> 01:43:04.400]   I filtered it out for you.
[01:43:04.400 --> 01:43:07.760]   - Yeah, I mean, so you have fundamentally hope for that,
[01:43:07.760 --> 01:43:10.480]   for the defense of PSYOP.
[01:43:10.480 --> 01:43:12.120]   - I'm not even like, I don't even mean these things
[01:43:12.120 --> 01:43:13.320]   in like truly horrible ways.
[01:43:13.320 --> 01:43:16.400]   I mean these things in straight up like ad blocker, right?
[01:43:16.400 --> 01:43:18.760]   Straight up ad blocker, I don't want ads.
[01:43:18.760 --> 01:43:20.040]   But they are always finding, you know,
[01:43:20.040 --> 01:43:22.460]   imagine I had an AI that could just block
[01:43:22.460 --> 01:43:23.400]   all the ads for me.
[01:43:23.400 --> 01:43:27.200]   - So you believe in the power of the people
[01:43:27.200 --> 01:43:29.840]   to always create an ad blocker.
[01:43:29.840 --> 01:43:32.400]   Yeah, I mean, I kind of share that belief.
[01:43:33.680 --> 01:43:37.200]   One of the deepest optimisms I have is just like,
[01:43:37.200 --> 01:43:39.240]   there's a lot of good guys.
[01:43:39.240 --> 01:43:42.200]   So to give, you shouldn't handpick them,
[01:43:42.200 --> 01:43:45.680]   just throw out powerful technology out there
[01:43:45.680 --> 01:43:47.920]   and the good guys will outnumber
[01:43:47.920 --> 01:43:49.640]   and outpower the bad guys.
[01:43:49.640 --> 01:43:51.680]   - Yeah, I'm not even gonna say there's a lot of good guys.
[01:43:51.680 --> 01:43:53.760]   I'm saying that good outnumbers bad, right?
[01:43:53.760 --> 01:43:54.680]   Good outnumbers bad.
[01:43:54.680 --> 01:43:56.360]   - In skill and performance.
[01:43:56.360 --> 01:43:57.900]   - Yeah, definitely in skill and performance,
[01:43:57.900 --> 01:43:59.240]   probably just a number too.
[01:43:59.240 --> 01:44:00.120]   Probably just in general.
[01:44:00.120 --> 01:44:02.360]   I mean, if you believe philosophically in democracy,
[01:44:02.360 --> 01:44:06.560]   you obviously believe that, that good outnumbers bad.
[01:44:06.560 --> 01:44:10.440]   And like the only, if you give it
[01:44:10.440 --> 01:44:11.940]   to a small number of people,
[01:44:11.940 --> 01:44:14.680]   there's a chance you gave it to good people,
[01:44:14.680 --> 01:44:16.720]   but there's also a chance you gave it to bad people.
[01:44:16.720 --> 01:44:19.800]   If you give it to everybody, well, if good outnumbers bad,
[01:44:19.800 --> 01:44:22.520]   then you definitely gave it to more good people than bad.
[01:44:22.520 --> 01:44:25.920]   - That's really interesting.
[01:44:25.920 --> 01:44:27.180]   So that's on the safety grounds,
[01:44:27.180 --> 01:44:29.640]   but then also, of course, there's other motivations,
[01:44:29.640 --> 01:44:32.200]   like you don't wanna give away your secret sauce.
[01:44:32.200 --> 01:44:34.440]   - Well, that's, I mean, I look, I respect capitalism.
[01:44:34.440 --> 01:44:37.200]   I don't think that, I think that it would be polite
[01:44:37.200 --> 01:44:39.360]   for you to make model architectures open source
[01:44:39.360 --> 01:44:41.800]   and fundamental breakthroughs open source.
[01:44:41.800 --> 01:44:43.360]   I don't think you have to make weights open source.
[01:44:43.360 --> 01:44:45.960]   - You know what's interesting is that,
[01:44:45.960 --> 01:44:49.200]   like there's so many possible trajectories in human history
[01:44:49.200 --> 01:44:53.220]   where you could have the next Google be open source.
[01:44:53.220 --> 01:44:57.280]   So for example, I don't know if that connection is accurate,
[01:44:57.280 --> 01:44:59.960]   but Wikipedia made a lot of interesting decisions,
[01:44:59.960 --> 01:45:01.480]   not to put ads.
[01:45:01.480 --> 01:45:03.720]   Like Wikipedia is basically open source.
[01:45:03.720 --> 01:45:05.720]   You could think of it that way.
[01:45:05.720 --> 01:45:08.840]   And like, that's one of the main websites on the internet.
[01:45:08.840 --> 01:45:10.160]   And like, it didn't have to be that way.
[01:45:10.160 --> 01:45:11.080]   It could have been like,
[01:45:11.080 --> 01:45:13.600]   Google could have created Wikipedia, put ads on it.
[01:45:13.600 --> 01:45:16.600]   You could probably run amazing ads now on Wikipedia.
[01:45:16.600 --> 01:45:18.480]   You wouldn't have to keep asking for money,
[01:45:18.480 --> 01:45:20.560]   but it's interesting, right?
[01:45:20.560 --> 01:45:23.280]   So llama, open source llama,
[01:45:23.280 --> 01:45:26.540]   derivatives of open source llama might win the internet.
[01:45:26.540 --> 01:45:29.040]   - I sure hope so.
[01:45:29.040 --> 01:45:31.160]   I hope to see another era.
[01:45:31.160 --> 01:45:32.720]   You know, the kids today don't know
[01:45:32.720 --> 01:45:35.000]   how good the internet used to be.
[01:45:35.000 --> 01:45:36.640]   And I don't think this is just, all right, come on.
[01:45:36.640 --> 01:45:38.240]   Like everyone's nostalgic for their past,
[01:45:38.240 --> 01:45:40.400]   but I actually think the internet,
[01:45:40.400 --> 01:45:43.720]   before small groups of weaponized corporate
[01:45:43.720 --> 01:45:45.520]   and government interests took it over,
[01:45:45.520 --> 01:45:46.620]   was a beautiful place.
[01:45:46.620 --> 01:45:52.920]   - You know, those small number of companies
[01:45:52.920 --> 01:45:55.880]   have created some sexy products.
[01:45:55.880 --> 01:46:00.000]   But you're saying overall, in the long arc of history,
[01:46:00.000 --> 01:46:02.600]   the centralization of power they have,
[01:46:02.600 --> 01:46:04.840]   like suffocated the human spirit at scale.
[01:46:04.840 --> 01:46:05.680]   - Here's a question to ask
[01:46:05.680 --> 01:46:08.240]   about those beautiful, sexy products.
[01:46:08.240 --> 01:46:11.120]   Imagine 2000 Google to 2010 Google, right?
[01:46:11.120 --> 01:46:12.000]   A lot changed.
[01:46:12.000 --> 01:46:14.280]   We got Maps, we got Gmail.
[01:46:14.280 --> 01:46:16.520]   - We lost a lot of products too, I think.
[01:46:16.520 --> 01:46:18.960]   - Yeah, I mean, some were probably, we got Chrome, right?
[01:46:18.960 --> 01:46:21.920]   And now let's go from 2010, we got Android.
[01:46:21.920 --> 01:46:24.440]   Now let's go from 2010 to 2020.
[01:46:24.440 --> 01:46:25.280]   What does Google have?
[01:46:25.280 --> 01:46:29.440]   Well, search engine, Maps, Mail, Android, and Chrome.
[01:46:29.440 --> 01:46:30.560]   - Oh, I see.
[01:46:30.560 --> 01:46:34.000]   The internet was this,
[01:46:34.000 --> 01:46:36.440]   you know, I was Times Person of the Year in 2006.
[01:46:36.440 --> 01:46:39.280]   - I love this.
[01:46:39.280 --> 01:46:41.800]   - It's you, was Times Person of the Year in 2006, right?
[01:46:41.800 --> 01:46:46.600]   Like that's, you know, so quickly did people forget.
[01:46:46.600 --> 01:46:49.480]   And I think some of it's social media.
[01:46:49.480 --> 01:46:53.280]   I think some of it, I hope, look, I hope that,
[01:46:53.280 --> 01:46:54.840]   I don't, it's possible
[01:46:54.840 --> 01:46:56.600]   that some very sinister things happened.
[01:46:56.600 --> 01:46:57.520]   I don't know.
[01:46:57.520 --> 01:47:00.360]   I think it might just be like the effects of social media.
[01:47:00.360 --> 01:47:03.520]   But something happened in the last 20 years.
[01:47:03.520 --> 01:47:05.920]   - Oh, okay.
[01:47:05.920 --> 01:47:08.240]   So you're just being an old man who's worried about the,
[01:47:08.240 --> 01:47:10.240]   I think there's always, it goes, it's a cycle thing.
[01:47:10.240 --> 01:47:11.080]   It's ups and downs,
[01:47:11.080 --> 01:47:13.720]   and I think people rediscover the power of distributed,
[01:47:13.720 --> 01:47:15.360]   of decentralized.
[01:47:15.360 --> 01:47:17.000]   I mean, that's kind of like what the whole,
[01:47:17.000 --> 01:47:19.040]   like cryptocurrency is trying, like that,
[01:47:19.040 --> 01:47:23.240]   I think crypto is just carrying the flame of that spirit
[01:47:23.240 --> 01:47:24.960]   of like stuff should be decentralized.
[01:47:24.960 --> 01:47:28.320]   It's just such a shame that they all got rich, you know?
[01:47:28.320 --> 01:47:29.160]   - Yeah.
[01:47:29.160 --> 01:47:30.560]   - If you took all the money out of crypto,
[01:47:30.560 --> 01:47:32.040]   it would have been a beautiful place.
[01:47:32.040 --> 01:47:32.880]   - Yeah.
[01:47:32.880 --> 01:47:34.320]   - But no, I mean, these people, you know,
[01:47:34.320 --> 01:47:36.880]   they sucked all the value out of it and took it.
[01:47:36.880 --> 01:47:40.720]   - Yeah, money kind of corrupts the mind somehow.
[01:47:40.720 --> 01:47:41.920]   It becomes this drug.
[01:47:41.920 --> 01:47:43.480]   - You corrupted all of crypto.
[01:47:43.480 --> 01:47:46.880]   You had coins worth billions of dollars that had zero use.
[01:47:46.880 --> 01:47:51.120]   - You still have hope for crypto?
[01:47:51.120 --> 01:47:51.960]   - Sure.
[01:47:51.960 --> 01:47:52.800]   I have hope for the ideas.
[01:47:52.800 --> 01:47:53.720]   I really do.
[01:47:54.560 --> 01:47:56.520]   Yeah, I mean, you know,
[01:47:56.520 --> 01:48:00.320]   I want the US dollar to collapse.
[01:48:00.320 --> 01:48:03.480]   I do.
[01:48:03.480 --> 01:48:04.320]   - George Hotz.
[01:48:04.320 --> 01:48:08.120]   Well, let me sort of on the AI safety,
[01:48:08.120 --> 01:48:11.360]   do you think there's some interesting questions there though
[01:48:11.360 --> 01:48:13.560]   to solve for the open source community in this case?
[01:48:13.560 --> 01:48:17.480]   So like alignment, for example, or the control problem.
[01:48:17.480 --> 01:48:19.400]   Like if you really have super powerful,
[01:48:19.400 --> 01:48:21.000]   you said it's scary.
[01:48:21.000 --> 01:48:21.840]   - Oh yeah.
[01:48:21.840 --> 01:48:22.680]   - What do we do with it?
[01:48:22.680 --> 01:48:24.240]   Not control, not centralized control,
[01:48:24.240 --> 01:48:27.040]   but like if you were then,
[01:48:27.040 --> 01:48:30.320]   you're gonna see some guy or gal
[01:48:30.320 --> 01:48:34.040]   release a super powerful language model, open source.
[01:48:34.040 --> 01:48:35.920]   And here you are, George Hotz thinking,
[01:48:35.920 --> 01:48:40.920]   holy shit, okay, what ideas do I have to combat this thing?
[01:48:40.920 --> 01:48:44.520]   So what ideas would you have?
[01:48:44.520 --> 01:48:48.360]   - I am so much not worried about the machine
[01:48:48.360 --> 01:48:50.280]   independently doing harm.
[01:48:50.280 --> 01:48:52.920]   That's what some of these AI safety people seem to think.
[01:48:52.920 --> 01:48:54.760]   They somehow seem to think that the machine
[01:48:54.760 --> 01:48:57.440]   like independently is gonna rebel against its creator.
[01:48:57.440 --> 01:48:59.520]   - So you don't think you'll find autonomy?
[01:48:59.520 --> 01:49:03.320]   - No, this is sci-fi B movie garbage.
[01:49:03.320 --> 01:49:05.680]   - Okay, what if the thing writes code,
[01:49:05.680 --> 01:49:06.960]   basically writes viruses?
[01:49:06.960 --> 01:49:10.560]   - If the thing writes viruses,
[01:49:10.560 --> 01:49:14.080]   it's because the human told it to write viruses.
[01:49:14.080 --> 01:49:15.360]   - Yeah, but there's some things you can't
[01:49:15.360 --> 01:49:16.440]   like put back in the box.
[01:49:16.440 --> 01:49:18.200]   That's kind of the whole point.
[01:49:18.200 --> 01:49:19.400]   Is it kind of spreads.
[01:49:19.400 --> 01:49:21.320]   Give it access to the internet, it spreads,
[01:49:21.320 --> 01:49:24.080]   installs itself, modifies your shit.
[01:49:24.080 --> 01:49:27.560]   - B, B, B, B plot sci-fi, not real.
[01:49:27.560 --> 01:49:28.400]   - I'm trying to work.
[01:49:28.400 --> 01:49:30.280]   I'm trying to get better at my plot writing.
[01:49:30.280 --> 01:49:31.600]   - The thing that worries me,
[01:49:31.600 --> 01:49:33.800]   I mean, we have a real danger to discuss
[01:49:33.800 --> 01:49:36.880]   and that is bad humans using the thing
[01:49:36.880 --> 01:49:39.520]   to do whatever bad unaligned AI thing you want.
[01:49:39.520 --> 01:49:42.720]   - But this goes to your previous concern
[01:49:42.720 --> 01:49:44.920]   that who gets to define who's a good human,
[01:49:44.920 --> 01:49:45.760]   who's a bad human?
[01:49:45.760 --> 01:49:47.480]   - Nobody does, we give it to everybody.
[01:49:47.480 --> 01:49:49.760]   And if you do anything besides give it to everybody,
[01:49:49.760 --> 01:49:51.600]   trust me, the bad humans will get it.
[01:49:51.600 --> 01:49:53.960]   Because that's who gets power.
[01:49:53.960 --> 01:49:55.400]   It's always the bad humans who get power.
[01:49:55.400 --> 01:49:57.640]   - Okay, power.
[01:49:57.640 --> 01:50:01.560]   And power turns even slightly good humans to bad.
[01:50:01.560 --> 01:50:02.400]   - Sure.
[01:50:02.400 --> 01:50:03.760]   - That's the intuition you have.
[01:50:03.760 --> 01:50:04.600]   I don't know.
[01:50:04.600 --> 01:50:07.960]   - I don't think everyone, I don't think everyone.
[01:50:07.960 --> 01:50:09.840]   I just think that like,
[01:50:09.840 --> 01:50:13.280]   here's the saying that I put in one of my blog posts.
[01:50:13.280 --> 01:50:14.760]   When I was in the hacking world,
[01:50:14.840 --> 01:50:16.960]   I found 95% of people to be good
[01:50:16.960 --> 01:50:18.560]   and 5% of people to be bad.
[01:50:18.560 --> 01:50:19.760]   Like just who I personally judged
[01:50:19.760 --> 01:50:21.160]   as good people and bad people.
[01:50:21.160 --> 01:50:23.160]   Like they believed about good things for the world.
[01:50:23.160 --> 01:50:26.480]   They wanted like flourishing and they wanted growth
[01:50:26.480 --> 01:50:29.280]   and they wanted things I consider good, right?
[01:50:29.280 --> 01:50:30.880]   I came into the business world with karma
[01:50:30.880 --> 01:50:32.680]   and I found the exact opposite.
[01:50:32.680 --> 01:50:35.720]   I found 5% of people good and 95% of people bad.
[01:50:35.720 --> 01:50:38.560]   I found a world that promotes psychopathy.
[01:50:38.560 --> 01:50:39.680]   - I wonder what that means.
[01:50:39.680 --> 01:50:41.000]   I wonder if that,
[01:50:43.120 --> 01:50:45.160]   I wonder if that's anecdotal or if it,
[01:50:45.160 --> 01:50:47.560]   if there's truth to that,
[01:50:47.560 --> 01:50:49.320]   there's something about capitalism.
[01:50:49.320 --> 01:50:51.640]   - Well. - At the core
[01:50:51.640 --> 01:50:54.000]   that promotes the people that run capitalism,
[01:50:54.000 --> 01:50:55.560]   that promotes psychopathy.
[01:50:55.560 --> 01:50:58.160]   - That saying may of course be my own biases, right?
[01:50:58.160 --> 01:50:59.840]   That may be my own biases that these people
[01:50:59.840 --> 01:51:02.960]   are a lot more aligned with me than these other people.
[01:51:02.960 --> 01:51:04.120]   Right? - Yeah.
[01:51:04.120 --> 01:51:07.240]   - So, you know, I can certainly recognize that.
[01:51:07.240 --> 01:51:08.440]   But you know, in general, I mean,
[01:51:08.440 --> 01:51:11.000]   this is like the common sense maxim,
[01:51:11.000 --> 01:51:13.480]   which is the people who end up getting power
[01:51:13.480 --> 01:51:15.760]   are never the ones you want with it.
[01:51:15.760 --> 01:51:19.200]   - But do you have a concern of super intelligent AGI,
[01:51:19.200 --> 01:51:23.840]   open sourced, and then what do you do with that?
[01:51:23.840 --> 01:51:26.000]   I'm not saying control it, it's open source.
[01:51:26.000 --> 01:51:27.760]   What do we do with this human species?
[01:51:27.760 --> 01:51:28.800]   - That's not up to me.
[01:51:28.800 --> 01:51:31.240]   I mean, you know, like I'm not a central planner.
[01:51:31.240 --> 01:51:33.640]   - No, not a central planner, but you'll probably tweet,
[01:51:33.640 --> 01:51:35.760]   there's a few days left to live for the human species.
[01:51:35.760 --> 01:51:37.320]   - I have my ideas of what to do with it
[01:51:37.320 --> 01:51:39.240]   and everyone else has their ideas of what to do with it.
[01:51:39.240 --> 01:51:40.240]   May the best ideas win.
[01:51:40.240 --> 01:51:42.400]   - But at this point, do you brainstorm,
[01:51:42.400 --> 01:51:45.320]   like, because it's not regulation.
[01:51:45.320 --> 01:51:46.840]   It could be decentralized regulation
[01:51:46.840 --> 01:51:49.440]   where people agree that this is just like,
[01:51:49.440 --> 01:51:52.440]   we create tools that make it more difficult for you
[01:51:52.440 --> 01:51:59.020]   to maybe make it more difficult for code to spread,
[01:51:59.020 --> 01:52:00.960]   you know, antivirus software, this kind of thing.
[01:52:00.960 --> 01:52:02.920]   - Oh, you're saying that you should build AI firewalls?
[01:52:02.920 --> 01:52:03.760]   That sounds good.
[01:52:03.760 --> 01:52:05.040]   You should definitely be running an AI firewall.
[01:52:05.040 --> 01:52:05.880]   - Yeah, right.
[01:52:05.880 --> 01:52:08.440]   - You should be running an AI firewall to your mind.
[01:52:08.440 --> 01:52:09.280]   - Right.
[01:52:09.280 --> 01:52:10.200]   - Constantly under, you know.
[01:52:10.200 --> 01:52:11.040]   - That's such an interesting idea.
[01:52:11.040 --> 01:52:13.080]   - Info wars, man, like.
[01:52:13.080 --> 01:52:14.600]   - I don't know if you're being sarcastic or not.
[01:52:14.600 --> 01:52:15.440]   - No, I'm dead serious.
[01:52:15.440 --> 01:52:16.260]   I'm dead serious. - But I think
[01:52:16.260 --> 01:52:17.100]   there's power to that.
[01:52:17.100 --> 01:52:22.100]   It's like, how do I protect my mind
[01:52:22.100 --> 01:52:24.880]   from influence of human-like
[01:52:24.880 --> 01:52:26.520]   or superhuman intelligent bots?
[01:52:26.520 --> 01:52:29.840]   - I am not being, I would pay so much money for that product.
[01:52:29.840 --> 01:52:31.520]   I would pay so much money for that product.
[01:52:31.520 --> 01:52:32.600]   I would, you know how much money I'd pay
[01:52:32.600 --> 01:52:35.140]   just for a spam filter that works?
[01:52:35.140 --> 01:52:38.400]   - Well, on Twitter, sometimes I would like
[01:52:38.400 --> 01:52:43.400]   to have a protection mechanism for my mind
[01:52:43.400 --> 01:52:46.160]   from the outrage mobs.
[01:52:46.160 --> 01:52:47.000]   - Yeah.
[01:52:47.000 --> 01:52:48.080]   - 'Cause they feel like bot-like behavior.
[01:52:48.080 --> 01:52:49.880]   It's like, there's a large number of people
[01:52:49.880 --> 01:52:52.560]   that will just grab a viral narrative
[01:52:52.560 --> 01:52:54.520]   and attack anyone else that believes otherwise.
[01:52:54.520 --> 01:52:55.720]   And it's like.
[01:52:55.720 --> 01:52:57.740]   - Whenever someone's telling me some story from the news,
[01:52:57.740 --> 01:52:59.600]   I'm always like, I don't wanna hear it, CIA op, bro.
[01:52:59.600 --> 01:53:00.800]   It's a CIA op, bro.
[01:53:00.800 --> 01:53:02.240]   Like, it doesn't matter if that's true or not.
[01:53:02.240 --> 01:53:03.960]   It's just trying to influence your mind.
[01:53:03.960 --> 01:53:05.400]   You're repeating an ad to me.
[01:53:06.320 --> 01:53:08.840]   But the viral mobs, like, yeah, they're.
[01:53:08.840 --> 01:53:12.180]   - Like, to me, a defense against those mobs
[01:53:12.180 --> 01:53:14.800]   is just getting multiple perspectives always
[01:53:14.800 --> 01:53:17.440]   from sources that make you feel
[01:53:17.440 --> 01:53:20.680]   kind of like you're getting smarter.
[01:53:20.680 --> 01:53:24.060]   And just, actually, just basically feels good.
[01:53:24.060 --> 01:53:26.760]   Like, a good documentary just feels,
[01:53:26.760 --> 01:53:28.000]   something feels good about it.
[01:53:28.000 --> 01:53:29.160]   It's well done.
[01:53:29.160 --> 01:53:31.400]   It's like, oh, okay, I never thought of it this way.
[01:53:31.400 --> 01:53:32.660]   It just feels good.
[01:53:32.660 --> 01:53:33.940]   Sometimes the outrage mobs,
[01:53:33.940 --> 01:53:35.780]   even if they have a good point behind it,
[01:53:35.780 --> 01:53:39.180]   when they're like mocking and derisive and just aggressive,
[01:53:39.180 --> 01:53:42.040]   you're with us or against us, this fucking.
[01:53:42.040 --> 01:53:44.480]   - This is why I delete my tweets.
[01:53:44.480 --> 01:53:46.640]   - Yeah, why'd you do that?
[01:53:46.640 --> 01:53:48.880]   I was, you know, I missed your tweets.
[01:53:48.880 --> 01:53:50.080]   - You know what it is?
[01:53:50.080 --> 01:53:52.640]   The algorithm promotes toxicity.
[01:53:52.640 --> 01:53:54.160]   - Yeah.
[01:53:54.160 --> 01:53:57.440]   - And like, you know, I think Elon has a much better chance
[01:53:57.440 --> 01:54:00.660]   of fixing it than the previous regime.
[01:54:00.660 --> 01:54:02.600]   - Yeah.
[01:54:02.600 --> 01:54:04.580]   - But to solve this problem, to solve,
[01:54:04.580 --> 01:54:07.960]   like to build a social network that is actually not toxic
[01:54:07.960 --> 01:54:10.980]   without moderation.
[01:54:10.980 --> 01:54:14.880]   - Like, not the stick, but carrot.
[01:54:14.880 --> 01:54:19.360]   So like where people look for goodness,
[01:54:19.360 --> 01:54:22.560]   so make it catalyze the process of connecting cool people
[01:54:22.560 --> 01:54:24.380]   and being cool to each other.
[01:54:24.380 --> 01:54:25.220]   - Yeah.
[01:54:25.220 --> 01:54:26.880]   - Without ever censoring.
[01:54:26.880 --> 01:54:27.700]   - Without ever censoring.
[01:54:27.700 --> 01:54:30.680]   And like, Scott Alexander has a blog post I like
[01:54:30.680 --> 01:54:32.360]   where he talks about like moderation is not censorship.
[01:54:32.360 --> 01:54:33.240]   Right?
[01:54:33.240 --> 01:54:35.920]   Like all moderation you want to put on Twitter, right?
[01:54:35.920 --> 01:54:40.540]   Like you could totally make this moderation, like just a,
[01:54:40.540 --> 01:54:42.500]   you don't have to block it for everybody.
[01:54:42.500 --> 01:54:44.460]   You can just have like a filter button, right?
[01:54:44.460 --> 01:54:46.080]   That people can turn off if they would like safe search
[01:54:46.080 --> 01:54:47.000]   for Twitter, right?
[01:54:47.000 --> 01:54:48.760]   Like someone could just turn that off, right?
[01:54:48.760 --> 01:54:51.180]   So like, but then you'd like take this idea to an extreme.
[01:54:51.180 --> 01:54:52.120]   Right?
[01:54:52.120 --> 01:54:54.720]   Well, the network should just show you,
[01:54:54.720 --> 01:54:56.880]   this is a couch surfing CEO thing, right?
[01:54:56.880 --> 01:54:58.360]   If it shows you right now,
[01:54:58.360 --> 01:55:00.840]   these algorithms are designed to maximize engagement.
[01:55:00.840 --> 01:55:02.880]   Well, it turns out outrage maximizes engagement.
[01:55:02.880 --> 01:55:06.140]   Quirk of human, quirk of the human mind, right?
[01:55:06.140 --> 01:55:09.200]   Just as I fall for it, everyone falls for it.
[01:55:09.200 --> 01:55:11.200]   So yeah, you got to figure out how to maximize
[01:55:11.200 --> 01:55:12.600]   for something other than engagement.
[01:55:12.600 --> 01:55:14.520]   - And I actually believe that you can make money
[01:55:14.520 --> 01:55:15.360]   with that too.
[01:55:15.360 --> 01:55:16.840]   So it's not, I don't think engagement
[01:55:16.840 --> 01:55:17.960]   is the only way to make money.
[01:55:17.960 --> 01:55:19.520]   - I actually think it's incredible
[01:55:19.520 --> 01:55:21.760]   that we're starting to see, I think again,
[01:55:21.760 --> 01:55:23.400]   Elon's doing so much stuff right with Twitter,
[01:55:23.400 --> 01:55:25.200]   like charging people money.
[01:55:25.200 --> 01:55:26.500]   As soon as you charge people money,
[01:55:26.500 --> 01:55:28.220]   they're no longer the product.
[01:55:28.220 --> 01:55:29.700]   They're the customer.
[01:55:29.700 --> 01:55:31.040]   And then they can start building something
[01:55:31.040 --> 01:55:32.080]   that's good for the customer
[01:55:32.080 --> 01:55:33.480]   and not good for the other customer,
[01:55:33.480 --> 01:55:34.760]   which is the ad agencies.
[01:55:34.760 --> 01:55:37.020]   - As in picked up Steam.
[01:55:37.020 --> 01:55:40.160]   - I pay for Twitter, doesn't even get me anything.
[01:55:40.160 --> 01:55:41.920]   It's my donation to this new business model,
[01:55:41.920 --> 01:55:43.020]   hopefully working out.
[01:55:43.020 --> 01:55:45.760]   - Sure, but you know, for this business model to work,
[01:55:45.760 --> 01:55:48.800]   it's like most people should be signed up to Twitter.
[01:55:48.800 --> 01:55:51.680]   And so the way it was,
[01:55:51.680 --> 01:55:53.400]   there was something perhaps not compelling
[01:55:53.400 --> 01:55:54.920]   or something like this to people.
[01:55:54.920 --> 01:55:56.440]   - Think you need most people at all.
[01:55:56.440 --> 01:55:58.840]   I think that, why do I need most people, right?
[01:55:58.840 --> 01:56:00.260]   Don't make an 8,000 person company,
[01:56:00.260 --> 01:56:01.560]   make a 50 person company.
[01:56:02.000 --> 01:56:02.840]   - Ah.
[01:56:02.840 --> 01:56:03.660]   - Yeah.
[01:56:03.660 --> 01:56:05.400]   - Well, so speaking of which,
[01:56:05.400 --> 01:56:08.480]   you worked at Twitter for a bit.
[01:56:08.480 --> 01:56:09.320]   - I did.
[01:56:09.320 --> 01:56:10.280]   - As an intern.
[01:56:10.280 --> 01:56:13.080]   The world's greatest intern.
[01:56:13.080 --> 01:56:13.920]   - Yeah.
[01:56:13.920 --> 01:56:14.760]   - All right.
[01:56:14.760 --> 01:56:15.600]   - There's been better.
[01:56:15.600 --> 01:56:17.400]   - There's been better.
[01:56:17.400 --> 01:56:18.840]   Tell me about your time at Twitter.
[01:56:18.840 --> 01:56:20.160]   How did it come about?
[01:56:20.160 --> 01:56:22.760]   And what did you learn from the experience?
[01:56:22.760 --> 01:56:27.760]   - So I deleted my first Twitter in 2010.
[01:56:27.760 --> 01:56:30.800]   I had over 100,000 followers
[01:56:30.800 --> 01:56:32.920]   back when that actually meant something.
[01:56:32.920 --> 01:56:36.400]   And I just saw, you know,
[01:56:36.400 --> 01:56:39.400]   my coworker summarized it well.
[01:56:39.400 --> 01:56:42.600]   He's like, "Whenever I see someone's Twitter page,
[01:56:42.600 --> 01:56:45.360]   "I either think the same of them or less of them.
[01:56:45.360 --> 01:56:46.920]   "I never think more of them."
[01:56:46.920 --> 01:56:47.760]   - Yeah.
[01:56:47.760 --> 01:56:50.040]   - Right, like, I don't wanna mention any names,
[01:56:50.040 --> 01:56:51.320]   but like some people who like, you know,
[01:56:51.320 --> 01:56:52.720]   maybe you would like read their books
[01:56:52.720 --> 01:56:53.600]   and you would respect them.
[01:56:53.600 --> 01:56:56.240]   You see them on Twitter and you're like,
[01:56:56.240 --> 01:56:57.080]   okay, dude.
[01:56:58.720 --> 01:57:02.000]   - Yeah, but there's some people who would say,
[01:57:02.000 --> 01:57:03.960]   you know who I respect a lot?
[01:57:03.960 --> 01:57:06.520]   Are people that just post really good technical stuff.
[01:57:06.520 --> 01:57:07.960]   - Yeah.
[01:57:07.960 --> 01:57:11.120]   - And I guess, I don't know.
[01:57:11.120 --> 01:57:13.000]   I think I respect them more for it.
[01:57:13.000 --> 01:57:15.840]   'Cause you realize, oh, this wasn't,
[01:57:15.840 --> 01:57:18.680]   there's like so much depth to this person,
[01:57:18.680 --> 01:57:19.720]   to their technical understanding
[01:57:19.720 --> 01:57:21.560]   of so many different topics.
[01:57:21.560 --> 01:57:22.400]   - Okay.
[01:57:22.400 --> 01:57:23.720]   - So I try to follow people.
[01:57:23.720 --> 01:57:25.680]   I try to consume stuff
[01:57:25.680 --> 01:57:27.880]   that's technical machine learning content.
[01:57:27.880 --> 01:57:31.880]   There's probably a few of those people.
[01:57:31.880 --> 01:57:34.000]   And the problem is inherently
[01:57:34.000 --> 01:57:36.440]   what the algorithm rewards, right?
[01:57:36.440 --> 01:57:38.200]   And people think about these algorithms.
[01:57:38.200 --> 01:57:40.280]   People think that they are terrible, awful things.
[01:57:40.280 --> 01:57:42.440]   And you know, I love that Elon open sourced it.
[01:57:42.440 --> 01:57:44.680]   Because I mean, what it does is actually pretty obvious.
[01:57:44.680 --> 01:57:47.240]   It just predicts what you are likely to retweet
[01:57:47.240 --> 01:57:49.680]   and like and linger on.
[01:57:49.680 --> 01:57:50.520]   It's what all these algorithms do.
[01:57:50.520 --> 01:57:51.480]   It's what TikTok does.
[01:57:51.480 --> 01:57:53.440]   So all these recommendation engines do.
[01:57:53.440 --> 01:57:57.160]   And it turns out that the thing
[01:57:57.160 --> 01:58:00.040]   that you are most likely to interact with is outrage.
[01:58:00.040 --> 01:58:02.000]   And that's a quirk of the human condition.
[01:58:02.000 --> 01:58:06.040]   - I mean, and there's different flavors of outrage.
[01:58:06.040 --> 01:58:09.520]   It doesn't have to be, it could be mockery.
[01:58:09.520 --> 01:58:10.360]   You'd be outraged.
[01:58:10.360 --> 01:58:11.840]   The topic of outrage could be different.
[01:58:11.840 --> 01:58:12.680]   It could be an idea.
[01:58:12.680 --> 01:58:13.520]   It could be a person.
[01:58:13.520 --> 01:58:17.240]   It could be, and maybe there's a better word than outrage.
[01:58:17.240 --> 01:58:18.160]   It could be drama.
[01:58:18.160 --> 01:58:19.000]   - Sure, drama.
[01:58:19.000 --> 01:58:19.840]   - All this kind of stuff.
[01:58:19.840 --> 01:58:20.660]   - Yeah.
[01:58:20.660 --> 01:58:22.240]   - But doesn't feel like when you consume it,
[01:58:22.240 --> 01:58:24.080]   it's a constructive thing for the individuals
[01:58:24.080 --> 01:58:26.240]   that consume it in the longterm.
[01:58:26.240 --> 01:58:30.000]   - Yeah, so my time there, I absolutely couldn't believe,
[01:58:30.000 --> 01:58:34.600]   you know, I got crazy amount of hate, you know,
[01:58:34.600 --> 01:58:36.240]   on Twitter for working at Twitter.
[01:58:36.240 --> 01:58:39.120]   It seems like people associated with this,
[01:58:39.120 --> 01:58:41.600]   I think maybe you were exposed to some of this.
[01:58:41.600 --> 01:58:44.060]   - So connection to Elon or is it working at Twitter?
[01:58:44.060 --> 01:58:46.720]   - Twitter and Elon, like the whole, there's just--
[01:58:46.720 --> 01:58:49.560]   - Elon's gotten a bit spicy during that time.
[01:58:49.560 --> 01:58:51.160]   A bit political, a bit.
[01:58:51.160 --> 01:58:52.640]   - Yeah, yeah.
[01:58:52.640 --> 01:58:54.120]   You know, I remember one of my tweets,
[01:58:54.120 --> 01:58:56.620]   it was never go full Republican and Elon liked it.
[01:58:56.620 --> 01:58:58.880]   (laughing)
[01:58:58.880 --> 01:59:06.880]   - Oh boy, yeah, I mean, there's a rollercoaster of that,
[01:59:06.880 --> 01:59:10.040]   but being political on Twitter, boy.
[01:59:10.040 --> 01:59:11.120]   - Yeah.
[01:59:11.120 --> 01:59:14.920]   - And also being, just attacking anybody on Twitter,
[01:59:14.920 --> 01:59:17.520]   it comes back at you harder.
[01:59:17.520 --> 01:59:19.440]   And if it's political and attacks.
[01:59:19.440 --> 01:59:21.480]   - Sure, sure, absolutely.
[01:59:22.360 --> 01:59:27.360]   And then letting sort of de-platformed people back on
[01:59:27.360 --> 01:59:34.320]   even adds more fun to the beautiful chaos.
[01:59:34.320 --> 01:59:37.640]   - I was hoping, and like, I remember when Elon talked
[01:59:37.640 --> 01:59:40.680]   about buying Twitter like six months earlier,
[01:59:40.680 --> 01:59:43.480]   he was talking about like a principled commitment
[01:59:43.480 --> 01:59:44.640]   to free speech.
[01:59:44.640 --> 01:59:47.620]   And I'm a big believer and fan of that.
[01:59:47.620 --> 01:59:50.740]   I would love to see an actual principled commitment
[01:59:50.740 --> 01:59:52.100]   to free speech.
[01:59:52.100 --> 01:59:54.200]   Of course, this isn't quite what happened.
[01:59:54.200 --> 01:59:57.880]   Instead of the oligarchy deciding what to ban,
[01:59:57.880 --> 02:00:00.800]   you had a monarchy deciding what to ban, right?
[02:00:00.800 --> 02:00:04.280]   Instead of, you know, all the Twitter files, shadow,
[02:00:04.280 --> 02:00:06.440]   really, the oligarchy just decides what.
[02:00:06.440 --> 02:00:08.520]   Cloth masks are ineffective against COVID.
[02:00:08.520 --> 02:00:09.480]   That's a true statement.
[02:00:09.480 --> 02:00:11.000]   Every doctor in 2019 knew it,
[02:00:11.000 --> 02:00:12.480]   and now I'm banned on Twitter for saying it?
[02:00:12.480 --> 02:00:14.400]   Interesting, oligarchy.
[02:00:14.400 --> 02:00:17.080]   So now you have a monarchy and, you know,
[02:00:17.080 --> 02:00:19.880]   he bans things he doesn't like.
[02:00:19.880 --> 02:00:22.100]   So, you know, it's just different power,
[02:00:22.100 --> 02:00:25.320]   and like, you know, maybe I align more with him
[02:00:25.320 --> 02:00:26.160]   than with the oligarchy.
[02:00:26.160 --> 02:00:28.940]   - But it's not free speech absolutism.
[02:00:28.940 --> 02:00:31.860]   But I feel like being a free speech absolutist
[02:00:31.860 --> 02:00:35.140]   on a social network requires you to also have tools
[02:00:35.140 --> 02:00:40.140]   for the individuals to control what they consume easier.
[02:00:40.140 --> 02:00:45.420]   Like, not censor, but just like control,
[02:00:45.420 --> 02:00:48.940]   like, oh, I'd like to see more cats and less politics.
[02:00:48.940 --> 02:00:51.320]   - And this isn't even remotely controversial.
[02:00:51.320 --> 02:00:53.320]   This is just saying you want to give paying customers
[02:00:53.320 --> 02:00:54.480]   for a product what they want.
[02:00:54.480 --> 02:00:56.560]   - Yeah, and not through the process of censorship,
[02:00:56.560 --> 02:00:57.760]   but through the process of like--
[02:00:57.760 --> 02:00:59.240]   - Well, it's individualized, right?
[02:00:59.240 --> 02:01:01.240]   It's individualized transparent censorship,
[02:01:01.240 --> 02:01:02.480]   which is honestly what I want.
[02:01:02.480 --> 02:01:03.320]   What is an ad blocker?
[02:01:03.320 --> 02:01:05.000]   It's individualized transparent censorship, right?
[02:01:05.000 --> 02:01:08.520]   - Yeah, but censorship is a strong word
[02:01:08.520 --> 02:01:10.200]   that people are very sensitive to.
[02:01:10.200 --> 02:01:12.440]   - I know, but, you know, I just use words
[02:01:12.440 --> 02:01:13.840]   to describe what they functionally are.
[02:01:13.840 --> 02:01:14.680]   And what is an ad blocker?
[02:01:14.680 --> 02:01:15.520]   It's just censorship.
[02:01:15.520 --> 02:01:16.340]   - Well, when I look at you right now--
[02:01:16.340 --> 02:01:17.600]   - But I love what you're censoring.
[02:01:17.600 --> 02:01:21.740]   - I'm looking at you, I'm censoring everything else out
[02:01:21.740 --> 02:01:24.260]   when my mind is focused on you.
[02:01:24.260 --> 02:01:25.780]   You can use the word censorship that way,
[02:01:25.780 --> 02:01:27.600]   but usually when people get very sensitive
[02:01:27.600 --> 02:01:28.940]   about the censorship thing.
[02:01:28.940 --> 02:01:33.420]   I think when anyone is allowed to say anything,
[02:01:33.420 --> 02:01:37.860]   you should probably have tools that maximize the quality
[02:01:37.860 --> 02:01:39.460]   of the experience for individuals.
[02:01:39.460 --> 02:01:42.820]   So, you know, for me, like what I really value,
[02:01:42.820 --> 02:01:45.620]   boy, it would be amazing to somehow figure out
[02:01:45.620 --> 02:01:46.840]   how to do that.
[02:01:46.840 --> 02:01:49.900]   I love disagreement and debate and people
[02:01:49.900 --> 02:01:51.740]   who disagree with each other disagree with me,
[02:01:51.740 --> 02:01:53.420]   especially in the space of ideas,
[02:01:53.420 --> 02:01:54.940]   but the high quality ones.
[02:01:54.940 --> 02:01:56.620]   So not derision, right?
[02:01:56.620 --> 02:01:58.260]   - Maslow's hierarchy of argument.
[02:01:58.260 --> 02:01:59.940]   I think that's a real word for it.
[02:01:59.940 --> 02:02:00.860]   - Probably.
[02:02:00.860 --> 02:02:02.980]   There's just a way of talking that's like snarky
[02:02:02.980 --> 02:02:06.260]   and so on that somehow gets people on Twitter
[02:02:06.260 --> 02:02:07.860]   and they get excited and so on.
[02:02:07.860 --> 02:02:09.980]   - We have like ad hominem refuting the central point.
[02:02:09.980 --> 02:02:11.460]   I like seeing this as an actual pyramid.
[02:02:11.460 --> 02:02:14.660]   - Yeah, and it's like all of it,
[02:02:14.660 --> 02:02:16.860]   all the wrong stuff is attractive to people.
[02:02:16.860 --> 02:02:18.140]   - I mean, we can just train a classifier
[02:02:18.140 --> 02:02:20.620]   to absolutely say what level of Maslow's hierarchy
[02:02:20.620 --> 02:02:21.940]   of argument are you at?
[02:02:21.940 --> 02:02:23.740]   And if it's ad hominem, like, okay, cool.
[02:02:23.740 --> 02:02:25.940]   I turned on the no ad hominem filter.
[02:02:25.940 --> 02:02:28.900]   - I wonder if there's a social network
[02:02:28.900 --> 02:02:31.060]   that will allow you to have that kind of filter.
[02:02:31.060 --> 02:02:34.640]   - Yeah, so here's the problem with that.
[02:02:34.640 --> 02:02:37.920]   It's not going to win in a free market.
[02:02:37.920 --> 02:02:41.180]   What wins in a free market is all television today
[02:02:41.180 --> 02:02:43.540]   is reality television because it's engaging.
[02:02:43.540 --> 02:02:47.220]   Right, engaging is what wins in a free market, right?
[02:02:47.220 --> 02:02:50.520]   So it becomes hard to keep these other more nuanced values.
[02:02:50.520 --> 02:02:56.260]   - Well, okay, so that's the experience of being on Twitter,
[02:02:56.260 --> 02:02:58.160]   but then you got a chance to also,
[02:02:58.160 --> 02:03:01.500]   together with other engineers and with Elon,
[02:03:01.500 --> 02:03:04.780]   sort of look, brainstorm when you step into a code base.
[02:03:04.780 --> 02:03:06.580]   It's been around for a long time.
[02:03:06.580 --> 02:03:08.300]   You know, there's other social networks,
[02:03:08.300 --> 02:03:11.300]   you know, Facebook, this is old code bases.
[02:03:11.300 --> 02:03:13.340]   And you step in and see, okay,
[02:03:13.340 --> 02:03:17.860]   how do we make with a fresh mind progress on this code base?
[02:03:17.860 --> 02:03:19.980]   Like, what did you learn about software engineering,
[02:03:19.980 --> 02:03:22.140]   about programming from just experiencing that?
[02:03:22.140 --> 02:03:25.200]   - So my technical recommendation to Elon,
[02:03:25.200 --> 02:03:27.200]   and I said this on the Twitter spaces afterward,
[02:03:27.200 --> 02:03:31.120]   I said this many times during my brief internship,
[02:03:31.120 --> 02:03:36.380]   was that you need refactors before features.
[02:03:36.380 --> 02:03:40.860]   This code base was, and look, I've worked at Google,
[02:03:40.860 --> 02:03:42.340]   I've worked at Facebook.
[02:03:42.340 --> 02:03:46.740]   Facebook has the best code, then Google, then Twitter.
[02:03:46.740 --> 02:03:47.740]   And you know what?
[02:03:47.740 --> 02:03:48.580]   You can know this,
[02:03:48.580 --> 02:03:50.180]   because look at the machine learning frameworks, right?
[02:03:50.180 --> 02:03:51.420]   Facebook released PyTorch,
[02:03:51.420 --> 02:03:54.420]   Google released TensorFlow, and Twitter released,
[02:03:54.420 --> 02:03:55.260]   eh.
[02:03:55.260 --> 02:03:56.080]   (laughs)
[02:03:56.080 --> 02:03:57.500]   Okay, so, you know.
[02:03:57.500 --> 02:03:58.900]   - It's a proxy, but yeah,
[02:03:58.900 --> 02:04:01.100]   the Google code base is quite interesting.
[02:04:01.100 --> 02:04:02.740]   There's a lot of really good software engineers there,
[02:04:02.740 --> 02:04:04.780]   but the code base is very large.
[02:04:04.780 --> 02:04:07.820]   - The code base was good in 2005, right?
[02:04:07.820 --> 02:04:08.660]   It looks like 2005 era.
[02:04:08.660 --> 02:04:10.500]   - There's so many products, so many teams, right?
[02:04:10.500 --> 02:04:15.500]   It's very difficult to, I feel like Twitter does less,
[02:04:15.500 --> 02:04:17.900]   obviously, much less than Google,
[02:04:17.900 --> 02:04:23.420]   in terms of the set of features, right?
[02:04:23.420 --> 02:04:26.740]   So I can imagine the number of software engineers
[02:04:26.740 --> 02:04:29.420]   that could recreate Twitter is much smaller
[02:04:29.420 --> 02:04:30.780]   than to recreate Google.
[02:04:30.780 --> 02:04:33.820]   - Yeah, I still believe in the amount of hate I got
[02:04:33.820 --> 02:04:36.820]   for saying this, that 50 people could build
[02:04:36.820 --> 02:04:38.580]   and maintain Twitter.
[02:04:38.580 --> 02:04:39.900]   - What's the nature of the hate?
[02:04:39.900 --> 02:04:40.740]   - Comfortably.
[02:04:40.740 --> 02:04:43.180]   - That you don't know what you're talking about?
[02:04:43.180 --> 02:04:44.180]   - You know what it is?
[02:04:44.180 --> 02:04:45.660]   And it's the same, this is my summary
[02:04:45.660 --> 02:04:47.660]   of the hate I get on Hacker News.
[02:04:47.660 --> 02:04:51.380]   It's like, when I say I'm going to do something,
[02:04:51.380 --> 02:04:56.220]   they have to believe that it's impossible.
[02:04:56.220 --> 02:04:59.660]   Because if doing things was possible,
[02:04:59.660 --> 02:05:01.220]   they'd have to do some soul searching
[02:05:01.220 --> 02:05:03.500]   and ask the question, why didn't they do anything?
[02:05:03.500 --> 02:05:04.700]   - So when you say--
[02:05:04.700 --> 02:05:06.060]   - And I do think that's where the hate comes from.
[02:05:06.060 --> 02:05:08.500]   - When you say, well, there's a core truth to that, yeah.
[02:05:08.500 --> 02:05:10.800]   So when you say, I'm gonna solve self-driving,
[02:05:10.800 --> 02:05:14.100]   people go like, what are your credentials?
[02:05:14.100 --> 02:05:15.300]   What the hell are you talking about?
[02:05:15.300 --> 02:05:17.260]   What is, this is an extremely difficult problem.
[02:05:17.260 --> 02:05:18.100]   Of course, you're a noob
[02:05:18.100 --> 02:05:20.260]   that doesn't understand the problem deeply.
[02:05:20.260 --> 02:05:23.820]   I mean, that was the same nature of hate
[02:05:23.820 --> 02:05:25.500]   that probably Elon got when he first talked
[02:05:25.500 --> 02:05:26.760]   about autonomous driving.
[02:05:26.760 --> 02:05:30.140]   But there's pros and cons to that,
[02:05:30.140 --> 02:05:33.100]   'cause there is experts in this world.
[02:05:33.100 --> 02:05:35.300]   - No, but the mockers aren't experts.
[02:05:35.300 --> 02:05:38.220]   The people who are mocking are not experts
[02:05:38.220 --> 02:05:39.740]   with carefully reasoned arguments
[02:05:39.740 --> 02:05:42.300]   about why you need 8,000 people to run a bird app.
[02:05:42.300 --> 02:05:46.380]   They're, but the people are gonna lose their jobs.
[02:05:46.380 --> 02:05:48.660]   - Well, that, but also there's the software engineers
[02:05:48.660 --> 02:05:50.940]   that probably criticize, no, it's a lot more complicated
[02:05:50.940 --> 02:05:52.700]   than you realize, but maybe it doesn't need
[02:05:52.700 --> 02:05:53.820]   to be so complicated.
[02:05:53.820 --> 02:05:55.580]   - You know, some people in the world
[02:05:55.580 --> 02:05:56.920]   like to create complexity.
[02:05:56.920 --> 02:05:58.620]   Some people in the world thrive under complexity,
[02:05:58.620 --> 02:05:59.660]   like lawyers, right?
[02:05:59.660 --> 02:06:01.180]   Lawyers want the world to be more complex
[02:06:01.180 --> 02:06:02.020]   because you need more lawyers,
[02:06:02.020 --> 02:06:03.620]   you need more legal hours, right?
[02:06:03.620 --> 02:06:05.820]   I think that's another.
[02:06:05.820 --> 02:06:07.420]   If there's two great evils in the world,
[02:06:07.420 --> 02:06:09.220]   it's centralization and complexity.
[02:06:09.220 --> 02:06:14.220]   - Yeah, and one of the sort of hidden side effects
[02:06:14.220 --> 02:06:19.540]   of software engineering is like finding pleasure
[02:06:19.540 --> 02:06:21.000]   in complexity.
[02:06:21.000 --> 02:06:24.300]   I mean, I don't remember just taking
[02:06:24.300 --> 02:06:25.860]   all the software engineering courses
[02:06:25.860 --> 02:06:28.220]   and just doing programming and just coming up
[02:06:28.220 --> 02:06:33.060]   in this object-oriented programming kind of idea.
[02:06:33.060 --> 02:06:35.500]   You don't, like, not often do people tell you,
[02:06:35.500 --> 02:06:38.060]   like, do the simplest possible thing.
[02:06:38.060 --> 02:06:42.820]   Like a professor, a teacher is not gonna get in front,
[02:06:42.820 --> 02:06:45.420]   like, this is the simplest way to do it.
[02:06:45.420 --> 02:06:47.820]   They'll say, like, this is the right way,
[02:06:47.820 --> 02:06:50.580]   and the right way, at least for a long time,
[02:06:50.580 --> 02:06:53.660]   you know, especially I came up with like Java, right?
[02:06:53.660 --> 02:06:58.660]   Is so much boilerplate, so much like, so many classes,
[02:06:58.660 --> 02:07:02.540]   so many like designs and architectures and so on,
[02:07:02.540 --> 02:07:05.940]   like planning for features far into the future
[02:07:05.940 --> 02:07:08.120]   and planning poorly and all this kind of stuff.
[02:07:08.120 --> 02:07:10.020]   And then there's this like code base
[02:07:10.020 --> 02:07:12.060]   that follows you along and puts pressure on you,
[02:07:12.060 --> 02:07:16.060]   and nobody knows what like parts, different parts do,
[02:07:16.060 --> 02:07:17.100]   which slows everything down.
[02:07:17.100 --> 02:07:19.940]   There's a kind of bureaucracy that's instilled in the code
[02:07:19.940 --> 02:07:20.940]   as a result of that.
[02:07:20.940 --> 02:07:22.620]   But then you feel like, oh, well,
[02:07:22.620 --> 02:07:25.020]   I follow good software engineering practices.
[02:07:25.020 --> 02:07:26.500]   It's an interesting trade-off,
[02:07:26.500 --> 02:07:30.100]   'cause then you look at like the ghetto-ness of like Perl
[02:07:30.100 --> 02:07:32.500]   and the old, like, how quickly you could just write
[02:07:32.500 --> 02:07:34.440]   a couple of lines and just get stuff done.
[02:07:34.440 --> 02:07:37.300]   That trade-off is interesting, or Bash, or whatever,
[02:07:37.300 --> 02:07:39.380]   these kind of ghetto things you can do in Linux.
[02:07:39.380 --> 02:07:41.960]   - One of my favorite things to look at today
[02:07:41.960 --> 02:07:43.860]   is how much do you trust your tests, right?
[02:07:43.860 --> 02:07:45.620]   We've put a ton of effort in Kama,
[02:07:45.620 --> 02:07:47.460]   and I've put a ton of effort in TinyGrad
[02:07:47.460 --> 02:07:51.440]   into making sure if you change the code and the tests pass,
[02:07:51.440 --> 02:07:52.740]   that you didn't break the code.
[02:07:52.740 --> 02:07:53.580]   - Yeah. - Now, this obviously
[02:07:53.580 --> 02:07:56.700]   is not always true, but the closer that is to true,
[02:07:56.700 --> 02:07:58.700]   the more you trust your tests, the more you're like,
[02:07:58.700 --> 02:08:00.900]   oh, I got a pull request, and the tests pass,
[02:08:00.900 --> 02:08:03.580]   I feel okay to merge that, the faster you can make progress.
[02:08:03.580 --> 02:08:05.100]   - So you're always programming with tests in mind,
[02:08:05.100 --> 02:08:07.260]   developing tests with that in mind,
[02:08:07.260 --> 02:08:08.820]   that if it passes, it should be good.
[02:08:08.820 --> 02:08:11.000]   - And Twitter had a-- - Not that.
[02:08:11.000 --> 02:08:13.640]   So-- - It was impossible
[02:08:13.640 --> 02:08:15.400]   to make progress in the codebase.
[02:08:15.400 --> 02:08:17.300]   - What other stuff can you say about the codebase
[02:08:17.300 --> 02:08:18.660]   that made it difficult?
[02:08:18.660 --> 02:08:21.620]   What are some interesting sort of quirks,
[02:08:21.620 --> 02:08:23.940]   broadly speaking, from that,
[02:08:23.940 --> 02:08:26.780]   compared to just your experience with Kama
[02:08:26.780 --> 02:08:27.860]   and everywhere else?
[02:08:27.860 --> 02:08:30.640]   - The real thing that, I spoke to a bunch of,
[02:08:30.640 --> 02:08:34.700]   you know, like individual contributors at Twitter,
[02:08:34.700 --> 02:08:36.540]   and I just asked, I'm like, okay,
[02:08:36.540 --> 02:08:38.500]   so like, what's wrong with this place?
[02:08:38.500 --> 02:08:39.900]   Why does this code look like this?
[02:08:39.900 --> 02:08:43.520]   And they explained to me what Twitter's promotion system was.
[02:08:43.520 --> 02:08:45.220]   The way that you got promoted at Twitter
[02:08:45.220 --> 02:08:49.640]   was you wrote a library that a lot of people used, right?
[02:08:49.640 --> 02:08:54.520]   So some guy wrote an NGINX replacement for Twitter.
[02:08:54.520 --> 02:08:56.340]   Why does Twitter need an NGINX replacement?
[02:08:56.340 --> 02:08:58.460]   What was wrong with NGINX?
[02:08:58.460 --> 02:09:00.460]   Well, you see, you're not gonna get promoted
[02:09:00.460 --> 02:09:01.940]   if you use NGINX.
[02:09:01.940 --> 02:09:03.320]   But if you write a replacement
[02:09:03.320 --> 02:09:05.020]   and lots of people start using it
[02:09:05.020 --> 02:09:07.120]   as the Twitter front end for their product,
[02:09:07.120 --> 02:09:08.380]   then you're gonna get promoted, right?
[02:09:08.380 --> 02:09:09.540]   - So interesting, 'cause like,
[02:09:09.540 --> 02:09:12.420]   from an individual perspective, how do you incentivize,
[02:09:12.420 --> 02:09:14.740]   how do you create the kind of incentives
[02:09:14.740 --> 02:09:18.180]   that will lead to a great codebase?
[02:09:18.180 --> 02:09:20.440]   Okay, what's the answer to that?
[02:09:20.440 --> 02:09:24.300]   - So what I do at Kama and at,
[02:09:25.500 --> 02:09:28.140]   and you know, at TinyCorp is you have to explain it to me.
[02:09:28.140 --> 02:09:30.300]   You have to explain to me what this code does, right?
[02:09:30.300 --> 02:09:31.940]   And if I can sit there and come up
[02:09:31.940 --> 02:09:34.740]   with a simpler way to do it, you have to rewrite it.
[02:09:34.740 --> 02:09:37.340]   You have to agree with me about the simpler way.
[02:09:37.340 --> 02:09:39.060]   You know, obviously we can have a conversation about this.
[02:09:39.060 --> 02:09:41.660]   It's not dictatorial, but if you're like,
[02:09:41.660 --> 02:09:44.340]   wow, wait, that actually is way simpler.
[02:09:44.340 --> 02:09:47.660]   Like, the simplicity is important, right?
[02:09:47.660 --> 02:09:51.060]   - But that requires people that overlook the code
[02:09:51.060 --> 02:09:54.100]   at the highest levels to be like, okay.
[02:09:54.100 --> 02:09:55.660]   - It requires technical leadership, you trust.
[02:09:55.660 --> 02:09:57.260]   - Yeah, technical leadership.
[02:09:57.260 --> 02:10:01.540]   So managers or whatever should have to have technical savvy,
[02:10:01.540 --> 02:10:03.140]   deep technical savvy.
[02:10:03.140 --> 02:10:04.340]   - Managers should be better programmers
[02:10:04.340 --> 02:10:05.620]   than the people who they manage.
[02:10:05.620 --> 02:10:09.740]   - Yeah, and that's not always obvious, trivial to create,
[02:10:09.740 --> 02:10:11.460]   especially at large companies.
[02:10:11.460 --> 02:10:12.700]   Managers get soft.
[02:10:12.700 --> 02:10:13.780]   - And like, you know, and this is just,
[02:10:13.780 --> 02:10:15.340]   I've instilled this culture at Kama,
[02:10:15.340 --> 02:10:17.760]   and Kama has better programmers than me who work there.
[02:10:17.760 --> 02:10:20.400]   But you know, again, I'm like the old guy
[02:10:20.400 --> 02:10:21.240]   from "Good Will Hunting."
[02:10:21.240 --> 02:10:23.180]   It's like, look, man, you know,
[02:10:23.180 --> 02:10:25.040]   I might not be as good as you,
[02:10:25.040 --> 02:10:27.100]   but I can see the difference between me and you, right?
[02:10:27.100 --> 02:10:28.260]   And this is what you need.
[02:10:28.260 --> 02:10:29.220]   This is what you need at the top.
[02:10:29.220 --> 02:10:31.280]   Or you don't necessarily need the manager
[02:10:31.280 --> 02:10:32.760]   to be the absolute best.
[02:10:32.760 --> 02:10:33.600]   I shouldn't say that,
[02:10:33.600 --> 02:10:36.540]   but like, they need to be able to recognize skill.
[02:10:36.540 --> 02:10:38.740]   - Yeah, and have good intuition.
[02:10:38.740 --> 02:10:40.940]   Intuition that's laden with wisdom
[02:10:40.940 --> 02:10:43.820]   from all the battles of trying to reduce complexity
[02:10:43.820 --> 02:10:44.660]   in codebases.
[02:10:44.660 --> 02:10:47.020]   - You know, I took a political approach at Kama too
[02:10:47.020 --> 02:10:47.940]   that I think is pretty interesting.
[02:10:47.940 --> 02:10:50.100]   I think Elon takes the same political approach.
[02:10:51.020 --> 02:10:53.440]   You know, Google had no politics,
[02:10:53.440 --> 02:10:54.420]   and what ended up happening
[02:10:54.420 --> 02:10:57.420]   is the absolute worst kind of politics took over.
[02:10:57.420 --> 02:10:59.180]   Kama has an extreme amount of politics,
[02:10:59.180 --> 02:11:02.100]   and they're all mine, and no dissidence is tolerated.
[02:11:02.100 --> 02:11:03.660]   - So it's a dictatorship.
[02:11:03.660 --> 02:11:05.820]   - Yep, it's an absolute dictatorship, right?
[02:11:05.820 --> 02:11:07.140]   Elon does the same thing.
[02:11:07.140 --> 02:11:10.100]   Now, the thing about my dictatorship is here are my values.
[02:11:10.100 --> 02:11:12.780]   - Yeah, it's just transparent.
[02:11:12.780 --> 02:11:13.600]   - It's transparent.
[02:11:13.600 --> 02:11:14.860]   It's a transparent dictatorship, right?
[02:11:14.860 --> 02:11:16.660]   And you can choose to opt in or, you know,
[02:11:16.660 --> 02:11:17.620]   you get free exit, right?
[02:11:17.620 --> 02:11:18.500]   That's the beauty of companies.
[02:11:18.500 --> 02:11:20.660]   If you don't like the dictatorship, you quit.
[02:11:21.660 --> 02:11:24.900]   - So you mentioned rewrite before,
[02:11:24.900 --> 02:11:27.600]   or refactor before features.
[02:11:27.600 --> 02:11:30.820]   If you were to refactor the Twitter codebase,
[02:11:30.820 --> 02:11:32.100]   what would that look like?
[02:11:32.100 --> 02:11:35.580]   And maybe also comment on how difficult is it to refactor?
[02:11:35.580 --> 02:11:37.820]   - The main thing I would do is first of all,
[02:11:37.820 --> 02:11:39.380]   identify the pieces,
[02:11:39.380 --> 02:11:42.360]   and then put tests in between the pieces, right?
[02:11:42.360 --> 02:11:43.200]   So there's all these different,
[02:11:43.200 --> 02:11:45.100]   Twitter has a microservice architecture,
[02:11:45.100 --> 02:11:48.260]   there's all these different microservices,
[02:11:48.260 --> 02:11:49.900]   and the thing that I was working on there,
[02:11:49.900 --> 02:11:50.920]   look, like, you know,
[02:11:50.920 --> 02:11:53.540]   George didn't know any JavaScript,
[02:11:53.540 --> 02:11:55.940]   he asked how to fix search, blah, blah, blah, blah, blah.
[02:11:55.940 --> 02:11:58.740]   Look, man, like, the thing is,
[02:11:58.740 --> 02:11:59.740]   like, I just, you know,
[02:11:59.740 --> 02:12:02.340]   I'm upset that the way that this whole thing was portrayed,
[02:12:02.340 --> 02:12:03.300]   because it wasn't like,
[02:12:03.300 --> 02:12:05.740]   it wasn't like taken by people, like, honestly,
[02:12:05.740 --> 02:12:06.660]   it wasn't like by,
[02:12:06.660 --> 02:12:08.940]   it was taken by people who started out
[02:12:08.940 --> 02:12:10.140]   with a bad faith assumption.
[02:12:10.140 --> 02:12:10.980]   - Yeah.
[02:12:10.980 --> 02:12:12.460]   - And I mean, I, look, I can't like--
[02:12:12.460 --> 02:12:14.300]   - And you as a programmer were just being transparent
[02:12:14.300 --> 02:12:16.900]   out there, actually having like fun,
[02:12:16.900 --> 02:12:19.420]   and like, this is what programming should be about.
[02:12:19.420 --> 02:12:21.300]   - I love that Elon gave me this opportunity.
[02:12:21.300 --> 02:12:22.140]   - Yeah.
[02:12:22.140 --> 02:12:23.140]   - Like, really, it does, and like, you know,
[02:12:23.140 --> 02:12:25.100]   he came on my, the day I quit,
[02:12:25.100 --> 02:12:26.540]   he came on my Twitter spaces afterward,
[02:12:26.540 --> 02:12:27.900]   and we had a conversation, like,
[02:12:27.900 --> 02:12:29.740]   I just, I respect that so much.
[02:12:29.740 --> 02:12:31.020]   - Yeah, and it's also inspiring
[02:12:31.020 --> 02:12:32.460]   to just engineers and programmers,
[02:12:32.460 --> 02:12:34.380]   and just, it's cool, it should be fun.
[02:12:34.380 --> 02:12:35.820]   The people that were hating on it,
[02:12:35.820 --> 02:12:37.100]   it's like, oh, man.
[02:12:37.100 --> 02:12:38.940]   - It was fun.
[02:12:38.940 --> 02:12:40.460]   It was fun, it was stressful,
[02:12:40.460 --> 02:12:41.300]   but I felt like, you know,
[02:12:41.300 --> 02:12:43.360]   I was at like a cool, like, point in history,
[02:12:43.360 --> 02:12:44.740]   and like, I hope I was useful,
[02:12:44.740 --> 02:12:46.940]   I probably kind of wasn't, but like, maybe I was.
[02:12:46.940 --> 02:12:48.940]   - Well, you also were one of the people
[02:12:48.940 --> 02:12:51.580]   that kind of made a strong case to refactor.
[02:12:51.580 --> 02:12:52.420]   - Yeah.
[02:12:52.420 --> 02:12:55.560]   - And that's a really interesting thing to raise,
[02:12:55.560 --> 02:12:58.520]   like, maybe that is the right, you know,
[02:12:58.520 --> 02:12:59.860]   the timing of that is really interesting.
[02:12:59.860 --> 02:13:01.900]   If you look at just the development of autopilot,
[02:13:01.900 --> 02:13:05.760]   you know, going from Mobileye to just,
[02:13:05.760 --> 02:13:07.500]   like, more, if you look at the history
[02:13:07.500 --> 02:13:09.880]   of semi-autonomous driving in Tesla,
[02:13:09.880 --> 02:13:14.420]   is more and more, like, you could say refactoring,
[02:13:14.420 --> 02:13:17.860]   or starting from scratch, redeveloping from scratch.
[02:13:17.860 --> 02:13:19.780]   It's refactoring all the way down.
[02:13:19.780 --> 02:13:21.860]   - And like, and the question is,
[02:13:21.860 --> 02:13:24.060]   like, can you do that sooner?
[02:13:24.060 --> 02:13:27.020]   Can you maintain product profitability?
[02:13:27.020 --> 02:13:29.100]   And like, what's the right time to do it?
[02:13:29.100 --> 02:13:30.360]   How do you do it?
[02:13:30.360 --> 02:13:32.140]   You know, on any one day, it's like,
[02:13:32.140 --> 02:13:33.620]   you don't want to pull off the band-aids.
[02:13:33.620 --> 02:13:36.420]   Like, it's, like, everything works.
[02:13:36.420 --> 02:13:39.100]   It's just like, little fix here and there,
[02:13:39.100 --> 02:13:41.020]   but maybe starting from scratch.
[02:13:41.020 --> 02:13:42.900]   - This is the main philosophy of TinyGrad.
[02:13:42.900 --> 02:13:44.720]   You have never refactored enough.
[02:13:44.720 --> 02:13:47.060]   Your code can get smaller, your code can get simpler,
[02:13:47.060 --> 02:13:48.980]   your ideas can be more elegant.
[02:13:48.980 --> 02:13:52.060]   - But would you consider, you know,
[02:13:52.060 --> 02:13:55.300]   say you were, like, running Twitter development teams,
[02:13:55.300 --> 02:13:58.780]   engineering teams, would you go as far
[02:13:58.780 --> 02:14:00.580]   as, like, different programming language?
[02:14:00.580 --> 02:14:03.000]   Just go that far?
[02:14:03.000 --> 02:14:07.260]   - I mean, the first thing that I would do is build tests.
[02:14:07.260 --> 02:14:10.380]   The first thing I would do is get a CI
[02:14:10.380 --> 02:14:13.000]   to where people can trust to make changes.
[02:14:13.000 --> 02:14:16.780]   - So that if you keep-- - Before I touched any code,
[02:14:16.780 --> 02:14:18.820]   I would actually say, no one touches any code.
[02:14:18.820 --> 02:14:20.540]   The first thing we do is we test this code base.
[02:14:20.540 --> 02:14:21.380]   I mean, this is classic.
[02:14:21.380 --> 02:14:22.780]   This is how you approach a legacy code base.
[02:14:22.780 --> 02:14:24.060]   This is, like, what any,
[02:14:24.060 --> 02:14:26.960]   how to approach a legacy code base book will tell you.
[02:14:26.960 --> 02:14:30.500]   - So, and then you hope that there's modules
[02:14:30.500 --> 02:14:33.260]   that can live on for a while,
[02:14:33.260 --> 02:14:36.260]   and then you add new ones, maybe in a different language,
[02:14:36.260 --> 02:14:38.180]   or design it-- - Before we add new ones,
[02:14:38.180 --> 02:14:39.460]   we replace old ones.
[02:14:39.460 --> 02:14:40.460]   - Yeah, yeah, meaning, like,
[02:14:40.460 --> 02:14:42.100]   replace old ones with something simpler.
[02:14:42.100 --> 02:14:45.460]   - We look at this, like, this thing that's 100,000 lines,
[02:14:45.460 --> 02:14:48.020]   and we're like, well, okay, maybe this did even make sense
[02:14:48.020 --> 02:14:49.860]   in 2010, but now we can replace this
[02:14:49.860 --> 02:14:52.340]   with an open-source thing, right?
[02:14:52.340 --> 02:14:53.420]   - Yeah. - And, you know,
[02:14:53.420 --> 02:14:55.580]   we look at this, here, here's another 50,000 lines.
[02:14:55.580 --> 02:14:56.420]   Well, actually, you know,
[02:14:56.420 --> 02:14:59.100]   we can replace this with 300 lines of Go.
[02:14:59.100 --> 02:14:59.940]   And you know what?
[02:14:59.940 --> 02:15:02.320]   I trust that the Go actually replaces this thing
[02:15:02.320 --> 02:15:03.740]   because all the tests still pass.
[02:15:03.740 --> 02:15:05.060]   So step one is testing.
[02:15:05.060 --> 02:15:06.660]   - Yeah. - And then step two is, like,
[02:15:06.660 --> 02:15:09.140]   the programming language is an afterthought, right?
[02:15:09.140 --> 02:15:10.820]   You'll let a whole lot of people compete, be like,
[02:15:10.820 --> 02:15:12.100]   okay, who wants to rewrite a module,
[02:15:12.100 --> 02:15:13.540]   whatever language you wanna write it in,
[02:15:13.540 --> 02:15:15.060]   just the tests have to pass?
[02:15:15.060 --> 02:15:17.300]   And if you figure out how to make the test pass
[02:15:17.300 --> 02:15:20.180]   but break the site, that's, we gotta go back to step one.
[02:15:20.180 --> 02:15:22.340]   Step one is get tests that you trust
[02:15:22.340 --> 02:15:23.700]   in order to make changes in the code base.
[02:15:23.700 --> 02:15:24.940]   - I wonder how hard it is, too,
[02:15:24.940 --> 02:15:27.620]   'cause I'm with you on testing and everything.
[02:15:27.620 --> 02:15:30.300]   You have from tests to, like, asserts to everything,
[02:15:30.300 --> 02:15:33.500]   but code is just covered in this
[02:15:33.500 --> 02:15:38.500]   because it should be very easy to make rapid changes
[02:15:38.500 --> 02:15:42.580]   and know that it's not gonna break everything.
[02:15:42.580 --> 02:15:43.540]   And that's the way to do it.
[02:15:43.540 --> 02:15:48.220]   But I wonder how difficult is it to integrate tests
[02:15:48.220 --> 02:15:49.900]   into a code base that doesn't have many of them.
[02:15:49.900 --> 02:15:51.940]   - So I'll tell you what my plan was at Twitter.
[02:15:51.940 --> 02:15:53.620]   It's actually similar to something we use at Kama.
[02:15:53.620 --> 02:15:56.140]   So at Kama, we have this thing called process replay.
[02:15:56.140 --> 02:15:57.980]   And we have a bunch of routes that'll be run through.
[02:15:57.980 --> 02:15:59.620]   So Kama's a microservice architecture, too.
[02:15:59.620 --> 02:16:02.020]   We have microservices in the driving.
[02:16:02.020 --> 02:16:03.820]   Like, we have one for the cameras, one for the sensor,
[02:16:03.820 --> 02:16:07.700]   one for the planner, one for the model.
[02:16:07.700 --> 02:16:09.260]   And we have an API,
[02:16:09.260 --> 02:16:11.500]   which the microservices talk to each other with.
[02:16:11.500 --> 02:16:13.060]   We use this custom thing called Serial,
[02:16:13.060 --> 02:16:14.780]   which uses ZMQ.
[02:16:14.780 --> 02:16:17.980]   Twitter uses Thrift.
[02:16:17.980 --> 02:16:20.460]   And then it uses this thing called Finagle,
[02:16:20.460 --> 02:16:24.420]   which is a Scala RPC backend.
[02:16:24.420 --> 02:16:25.620]   But this doesn't even really matter.
[02:16:25.620 --> 02:16:30.260]   The Thrift and Finagle layer was a great place,
[02:16:30.260 --> 02:16:32.060]   I thought, to write tests, right?
[02:16:32.060 --> 02:16:34.580]   To start building something that looks like process replay.
[02:16:34.580 --> 02:16:37.900]   So Twitter had some stuff that looked kind of like this,
[02:16:37.900 --> 02:16:39.180]   but it wasn't offline.
[02:16:39.180 --> 02:16:40.220]   It was only online.
[02:16:40.220 --> 02:16:43.300]   So you could ship a modified version of it,
[02:16:43.300 --> 02:16:46.380]   and then you could redirect some of the traffic
[02:16:46.380 --> 02:16:48.380]   to your modified version and diff those two.
[02:16:48.380 --> 02:16:49.580]   But it was all online.
[02:16:49.580 --> 02:16:51.980]   Like, there was no CI in the traditional sense.
[02:16:51.980 --> 02:16:54.100]   I mean, there was some, but it was not full coverage.
[02:16:54.100 --> 02:16:57.340]   So you can't run all of Twitter offline to test something.
[02:16:57.340 --> 02:16:58.440]   Then this was another problem.
[02:16:58.440 --> 02:17:00.500]   You can't run all of Twitter, right?
[02:17:00.500 --> 02:17:01.340]   Period.
[02:17:01.340 --> 02:17:03.140]   Any one person can't run it.
[02:17:03.140 --> 02:17:05.740]   Twitter runs in three data centers, and that's it.
[02:17:05.740 --> 02:17:07.820]   There's no other place you can run Twitter,
[02:17:07.820 --> 02:17:10.340]   which is like, "George, you don't understand.
[02:17:10.340 --> 02:17:11.980]   "This is modern software development."
[02:17:11.980 --> 02:17:13.260]   No, this is bullshit.
[02:17:13.260 --> 02:17:15.860]   Like, why can't it run on my laptop?
[02:17:15.860 --> 02:17:16.700]   "What are you doing?
[02:17:16.700 --> 02:17:17.520]   "Twitter can run it."
[02:17:17.520 --> 02:17:18.740]   Yeah, okay, well, I'm not saying
[02:17:18.740 --> 02:17:20.820]   you're gonna download the whole database to your laptop,
[02:17:20.820 --> 02:17:22.660]   but I'm saying all the middleware and the front end
[02:17:22.660 --> 02:17:24.540]   should run on my laptop, right?
[02:17:24.540 --> 02:17:26.060]   - That sounds really compelling.
[02:17:26.060 --> 02:17:27.220]   - Yeah.
[02:17:27.220 --> 02:17:30.820]   - But can that be achieved by a code base
[02:17:30.820 --> 02:17:33.060]   that grows over the years?
[02:17:33.060 --> 02:17:35.260]   I mean, the three data centers didn't have to be, right?
[02:17:35.260 --> 02:17:37.500]   'Cause they're totally different designs.
[02:17:37.500 --> 02:17:39.700]   - The problem is more like,
[02:17:39.700 --> 02:17:41.580]   why did the code base have to grow?
[02:17:41.580 --> 02:17:43.560]   What new functionality has been added
[02:17:43.560 --> 02:17:47.780]   to compensate for the lines of code that are there?
[02:17:47.780 --> 02:17:49.820]   - One of the ways to explain it is that
[02:17:49.820 --> 02:17:51.380]   the incentive for software developers
[02:17:51.380 --> 02:17:54.200]   to move up in the company is to add code,
[02:17:54.200 --> 02:17:55.780]   to add, especially large--
[02:17:55.780 --> 02:17:56.620]   - And you know what?
[02:17:56.620 --> 02:17:57.720]   The incentive for politicians to move up
[02:17:57.720 --> 02:18:00.020]   in the political structure is to add laws.
[02:18:00.020 --> 02:18:01.180]   Same problem.
[02:18:01.180 --> 02:18:03.440]   - Yeah, yeah.
[02:18:03.440 --> 02:18:07.200]   If the flip side is to simplify, simplify, simplify.
[02:18:07.200 --> 02:18:08.520]   I mean, you know what?
[02:18:08.520 --> 02:18:10.020]   This is something that I do differently
[02:18:10.020 --> 02:18:13.340]   from Elon with Kama about self-driving cars.
[02:18:13.340 --> 02:18:16.580]   You know, I hear the new version's gonna come out
[02:18:16.580 --> 02:18:18.620]   and the new version is not gonna be better,
[02:18:18.620 --> 02:18:22.260]   but at first, and it's gonna require a ton of refactors,
[02:18:22.260 --> 02:18:24.160]   I say, okay, take as long as you need.
[02:18:24.160 --> 02:18:27.020]   You convince me this architecture's better,
[02:18:27.020 --> 02:18:28.860]   okay, we have to move to it.
[02:18:28.860 --> 02:18:31.380]   Even if it's not gonna make the product better tomorrow,
[02:18:31.380 --> 02:18:34.340]   the top priority is getting the architecture right.
[02:18:34.340 --> 02:18:37.560]   - So what do you think about sort of a thing
[02:18:37.560 --> 02:18:39.220]   where the product is online?
[02:18:39.220 --> 02:18:42.960]   So I guess, would you do a refactor?
[02:18:42.960 --> 02:18:45.160]   If you ran engineering on Twitter,
[02:18:45.160 --> 02:18:46.760]   would you just do a refactor?
[02:18:46.760 --> 02:18:48.000]   How long would it take?
[02:18:48.000 --> 02:18:51.600]   What would that mean for the running of the actual service?
[02:18:51.600 --> 02:18:56.600]   - You know, and I'm not the right person to run Twitter.
[02:18:56.600 --> 02:18:59.180]   I'm just not.
[02:18:59.180 --> 02:19:00.500]   And that's the problem.
[02:19:00.500 --> 02:19:01.520]   I don't really know.
[02:19:01.520 --> 02:19:03.680]   I don't really know if that's, you know,
[02:19:03.680 --> 02:19:05.980]   a common thing that I thought a lot while I was there
[02:19:05.980 --> 02:19:07.720]   was whenever I thought something that was different
[02:19:07.720 --> 02:19:09.140]   to what Elon thought,
[02:19:09.140 --> 02:19:10.820]   I'd have to run something in the back of my head
[02:19:10.820 --> 02:19:15.820]   reminding myself that Elon is the richest man in the world.
[02:19:15.820 --> 02:19:18.940]   And in general, his ideas are better than mine.
[02:19:18.940 --> 02:19:22.260]   Now there's a few things I think I do understand
[02:19:22.260 --> 02:19:26.880]   and know more about, but like in general,
[02:19:26.880 --> 02:19:28.460]   I'm not qualified to run Twitter.
[02:19:28.460 --> 02:19:29.780]   Not necessarily qualified,
[02:19:29.780 --> 02:19:31.280]   but like, I don't think I'd be that good at it.
[02:19:31.280 --> 02:19:32.860]   I don't think I'd be good at it.
[02:19:32.860 --> 02:19:33.740]   I don't think I'd really be good
[02:19:33.740 --> 02:19:36.480]   at running an engineering organization at scale.
[02:19:36.480 --> 02:19:42.660]   I think I could lead a very good refactor of Twitter
[02:19:42.660 --> 02:19:45.040]   and it would take like six months to a year
[02:19:45.040 --> 02:19:47.560]   and the results to show at the end of it
[02:19:47.560 --> 02:19:50.140]   would be feature development in general
[02:19:50.140 --> 02:19:53.180]   takes 10X less time, 10X less man hours.
[02:19:53.180 --> 02:19:55.440]   That's what I think I could actually do.
[02:19:55.440 --> 02:19:58.320]   Do I think that it's the right decision for the business
[02:19:58.320 --> 02:19:59.260]   above my pay grade?
[02:20:02.780 --> 02:20:04.980]   - Yeah, but a lot of these kinds of decisions
[02:20:04.980 --> 02:20:06.460]   are above everybody's pay grade.
[02:20:06.460 --> 02:20:07.420]   - I don't wanna be a manager.
[02:20:07.420 --> 02:20:08.260]   I don't wanna do that.
[02:20:08.260 --> 02:20:10.780]   I just like, if you really forced me to,
[02:20:10.780 --> 02:20:12.220]   yeah, it would make me maybe,
[02:20:12.220 --> 02:20:17.660]   make me upset if I had to make those decisions.
[02:20:17.660 --> 02:20:18.500]   I don't wanna.
[02:20:18.500 --> 02:20:23.520]   - Yeah, but a refactor is so compelling.
[02:20:23.520 --> 02:20:26.140]   If this is to become something much bigger
[02:20:26.140 --> 02:20:27.260]   than what Twitter was,
[02:20:27.260 --> 02:20:32.580]   it feels like a refactor has to be coming at some point.
[02:20:32.580 --> 02:20:34.500]   George, you're a junior software engineer.
[02:20:34.500 --> 02:20:35.860]   Every junior software engineer
[02:20:35.860 --> 02:20:38.660]   wants to come in and refactor the whole code.
[02:20:38.660 --> 02:20:42.060]   Okay, that's like your opinion, man.
[02:20:42.060 --> 02:20:45.380]   - Yeah, it doesn't, sometimes they're right.
[02:20:45.380 --> 02:20:47.420]   - Well, whether they're right or not,
[02:20:47.420 --> 02:20:48.700]   it's definitely not for that reason.
[02:20:48.700 --> 02:20:50.740]   It's definitely not a question of engineering prowess.
[02:20:50.740 --> 02:20:52.220]   It is a question of maybe what the priorities are
[02:20:52.220 --> 02:20:53.220]   for the company.
[02:20:53.220 --> 02:20:56.340]   And I did get more intelligent feedback
[02:20:56.340 --> 02:20:58.640]   from people I think in good faith saying that.
[02:20:58.640 --> 02:21:01.180]   Actually from Elon.
[02:21:01.180 --> 02:21:04.060]   And from Elon, people were like,
[02:21:04.060 --> 02:21:06.540]   well, a stop the world refactor
[02:21:06.540 --> 02:21:08.060]   might be great for engineering,
[02:21:08.060 --> 02:21:10.000]   but you don't have a business to run.
[02:21:10.000 --> 02:21:12.940]   And hey, above my pay grade.
[02:21:12.940 --> 02:21:15.980]   - What'd you think about Elon as an engineering leader,
[02:21:15.980 --> 02:21:19.660]   having to experience him in the most chaotic of spaces,
[02:21:19.660 --> 02:21:20.500]   I would say?
[02:21:20.500 --> 02:21:27.260]   - My respect for him is unchanged.
[02:21:27.260 --> 02:21:30.660]   And I did have to think a lot more deeply
[02:21:30.660 --> 02:21:32.820]   about some of the decisions he's forced to make.
[02:21:32.820 --> 02:21:35.860]   - About the tensions within those,
[02:21:35.860 --> 02:21:37.820]   the trade-offs within those decisions?
[02:21:37.820 --> 02:21:43.980]   - About a whole matrix coming at him.
[02:21:43.980 --> 02:21:45.500]   I think that's Andrew Tate's word for it,
[02:21:45.500 --> 02:21:46.580]   sorry to borrow it.
[02:21:46.580 --> 02:21:49.340]   - Also, bigger than engineering, just everything.
[02:21:49.340 --> 02:21:52.740]   - Yeah, like the war on the woke.
[02:21:52.740 --> 02:21:54.740]   - Yeah.
[02:21:54.740 --> 02:21:57.780]   - Like it just, man, and like,
[02:21:59.100 --> 02:22:01.140]   he doesn't have to do this, you know?
[02:22:01.140 --> 02:22:01.980]   He doesn't have to.
[02:22:01.980 --> 02:22:04.740]   He could go like Parag and go chill
[02:22:04.740 --> 02:22:07.060]   at the Four Seasons Maui, you know?
[02:22:07.060 --> 02:22:10.100]   But see, one person I respect and one person I don't.
[02:22:10.100 --> 02:22:12.900]   - So his heart is in the right place,
[02:22:12.900 --> 02:22:15.060]   fighting in this case for this ideal
[02:22:15.060 --> 02:22:17.620]   of the freedom of expression.
[02:22:17.620 --> 02:22:19.900]   - I wouldn't define the ideal so simply.
[02:22:19.900 --> 02:22:22.860]   I think you can define the ideal no more
[02:22:22.860 --> 02:22:26.780]   than just saying Elon's idea of a good world.
[02:22:26.780 --> 02:22:29.020]   Freedom of expression is--
[02:22:29.020 --> 02:22:30.660]   - But to you, it's still,
[02:22:30.660 --> 02:22:32.700]   the downsides of that is the monarchy.
[02:22:32.700 --> 02:22:36.980]   - Yeah, I mean, monarchy has problems, right?
[02:22:36.980 --> 02:22:39.700]   But I mean, would I trade right now
[02:22:39.700 --> 02:22:42.840]   the current oligarchy, which runs America,
[02:22:42.840 --> 02:22:43.680]   for the monarchy?
[02:22:43.680 --> 02:22:44.900]   Yeah, I would, sure.
[02:22:44.900 --> 02:22:47.380]   For the Elon monarchy, yeah, you know why?
[02:22:47.380 --> 02:22:50.460]   Because power would cost one cent a kilowatt hour.
[02:22:50.460 --> 02:22:52.020]   10th of a cent a kilowatt hour.
[02:22:52.020 --> 02:22:54.380]   - What do you mean?
[02:22:54.380 --> 02:22:56.740]   - Right now, I pay about 20 cents a kilowatt hour
[02:22:56.740 --> 02:22:58.460]   for electricity in San Diego.
[02:22:58.460 --> 02:23:00.660]   That's like the same price you paid in 1980.
[02:23:00.660 --> 02:23:02.820]   What the hell?
[02:23:02.820 --> 02:23:05.420]   - So you would see a lot of innovation with Elon.
[02:23:05.420 --> 02:23:07.780]   - Maybe it'd have some hyper loops.
[02:23:07.780 --> 02:23:08.620]   - Yeah.
[02:23:08.620 --> 02:23:09.820]   - Right, and I'm willing to make that trade-off, right?
[02:23:09.820 --> 02:23:11.380]   I'm willing to make, and this is why,
[02:23:11.380 --> 02:23:13.540]   you know, people think that dictators take power
[02:23:13.540 --> 02:23:17.020]   through some untoward mechanism.
[02:23:17.020 --> 02:23:18.460]   Sometimes they do, but usually it's 'cause
[02:23:18.460 --> 02:23:19.580]   the people want them.
[02:23:19.580 --> 02:23:22.860]   And the downsides of a dictatorship,
[02:23:22.860 --> 02:23:24.060]   I feel like we've gotten to a point now
[02:23:24.060 --> 02:23:26.420]   with the oligarchy where, yeah,
[02:23:26.420 --> 02:23:27.820]   I would prefer the dictator.
[02:23:28.660 --> 02:23:30.900]   (chuckles)
[02:23:30.900 --> 02:23:33.500]   - What'd you think about Scala as a programming language?
[02:23:33.500 --> 02:23:37.060]   - I liked it more than I thought.
[02:23:37.060 --> 02:23:37.900]   I did the tutorials.
[02:23:37.900 --> 02:23:38.740]   Like, I was very new to it.
[02:23:38.740 --> 02:23:39.980]   Like, it would take me six months to be able
[02:23:39.980 --> 02:23:41.820]   to write, like, good Scala.
[02:23:41.820 --> 02:23:43.100]   - I mean, what did you learn about learning
[02:23:43.100 --> 02:23:45.020]   a new programming language from that?
[02:23:45.020 --> 02:23:47.420]   - Oh, I love doing, like, new programming tutorials
[02:23:47.420 --> 02:23:48.260]   and doing them.
[02:23:48.260 --> 02:23:49.140]   I did all this for Rust.
[02:23:49.140 --> 02:23:54.660]   It keeps some of its upsetting JVM roots,
[02:23:54.660 --> 02:23:56.700]   but it is a much nicer...
[02:23:56.700 --> 02:23:59.060]   In fact, I almost don't know why Kotlin took off
[02:23:59.060 --> 02:24:00.700]   and not Scala.
[02:24:00.700 --> 02:24:03.140]   I think Scala has some beauty that Kotlin lacked,
[02:24:03.140 --> 02:24:07.660]   whereas Kotlin felt a lot more...
[02:24:07.660 --> 02:24:09.100]   I mean, it was almost like, I don't know
[02:24:09.100 --> 02:24:10.620]   if it actually was a response to Swift,
[02:24:10.620 --> 02:24:12.220]   but that's kind of what it felt like.
[02:24:12.220 --> 02:24:13.500]   Like, Kotlin looks more like Swift,
[02:24:13.500 --> 02:24:15.460]   and Scala looks more like, well,
[02:24:15.460 --> 02:24:16.620]   like a functional programming language,
[02:24:16.620 --> 02:24:18.660]   more like an OCaml or a Haskell.
[02:24:18.660 --> 02:24:19.980]   - Let's actually just explore,
[02:24:19.980 --> 02:24:23.020]   we touched it a little bit, but just on the art,
[02:24:23.020 --> 02:24:25.380]   the science and the art of programming.
[02:24:25.380 --> 02:24:27.180]   For you personally, how much of your programming
[02:24:27.180 --> 02:24:29.060]   is done with GPT currently?
[02:24:29.060 --> 02:24:29.900]   - None.
[02:24:29.900 --> 02:24:30.740]   - None.
[02:24:30.740 --> 02:24:31.900]   - I don't use it at all.
[02:24:31.900 --> 02:24:34.940]   - Because you prioritize simplicity so much.
[02:24:34.940 --> 02:24:37.780]   - Yeah, I find that a lot of it is noise.
[02:24:37.780 --> 02:24:39.060]   I do use VS Code,
[02:24:39.060 --> 02:24:43.300]   and I do like some amount of autocomplete.
[02:24:43.300 --> 02:24:45.140]   I do like a very,
[02:24:45.140 --> 02:24:47.580]   a very, like, feels-like-rules-based autocomplete.
[02:24:47.580 --> 02:24:49.180]   Like an autocomplete that's going to complete
[02:24:49.180 --> 02:24:50.580]   the variable name for me, so I don't have to type it,
[02:24:50.580 --> 02:24:51.780]   I can just press tab.
[02:24:51.780 --> 02:24:52.620]   All right, that's nice.
[02:24:52.620 --> 02:24:53.860]   But I don't want an autocomplete.
[02:24:53.860 --> 02:24:54.700]   You know what I hate?
[02:24:54.700 --> 02:24:56.780]   When autocompletes, when I type the word for,
[02:24:56.780 --> 02:24:59.260]   and it like puts like two, two parentheses
[02:24:59.260 --> 02:25:00.700]   and two semicolons and two braces,
[02:25:00.700 --> 02:25:02.500]   I'm like, oh man.
[02:25:02.500 --> 02:25:07.340]   - Well, I mean, with VS Code and GPT with Codex,
[02:25:07.340 --> 02:25:11.340]   you can kind of brainstorm.
[02:25:11.340 --> 02:25:12.380]   I find,
[02:25:12.380 --> 02:25:15.380]   I'm like probably the same as you,
[02:25:15.380 --> 02:25:18.380]   but I like that it generates code,
[02:25:18.380 --> 02:25:20.060]   and you basically disagree with it
[02:25:20.060 --> 02:25:21.500]   and write something simpler.
[02:25:21.500 --> 02:25:24.820]   But to me, that somehow is like inspiring.
[02:25:24.820 --> 02:25:25.660]   It makes me feel good.
[02:25:25.660 --> 02:25:27.820]   It also gamifies the simplification process,
[02:25:27.820 --> 02:25:30.580]   'cause I'm like, oh yeah, you dumb AI system.
[02:25:30.580 --> 02:25:32.060]   You think this is the way to do it.
[02:25:32.060 --> 02:25:33.260]   I have a simpler thing here.
[02:25:33.260 --> 02:25:36.820]   - It just constantly reminds me of like bad stuff.
[02:25:36.820 --> 02:25:38.500]   I mean, I tried the same thing with rap, right?
[02:25:38.500 --> 02:25:39.420]   I tried the same thing with rap,
[02:25:39.420 --> 02:25:40.660]   and actually I think I'm a much better programmer
[02:25:40.660 --> 02:25:41.580]   than rapper.
[02:25:41.580 --> 02:25:42.780]   But like I even tried, I was like, okay,
[02:25:42.780 --> 02:25:44.380]   can we get some inspiration from these things
[02:25:44.380 --> 02:25:46.140]   for some rap lyrics?
[02:25:46.140 --> 02:25:47.740]   And I just found that it would go back
[02:25:47.780 --> 02:25:51.540]   to the most like cringy tropes and dumb rhyme schemes.
[02:25:51.540 --> 02:25:54.820]   And I'm like, yeah, this is what the code looks like too.
[02:25:54.820 --> 02:25:56.860]   - I think you and I probably have different thresholds
[02:25:56.860 --> 02:25:58.580]   for cringe code.
[02:25:58.580 --> 02:26:00.980]   You probably hate cringe code.
[02:26:00.980 --> 02:26:01.980]   So it's for you.
[02:26:01.980 --> 02:26:07.380]   I mean, boilerplate is a part of code.
[02:26:07.380 --> 02:26:10.580]   Like some of it,
[02:26:10.580 --> 02:26:15.660]   yeah, and some of it is just like faster lookup.
[02:26:17.140 --> 02:26:18.220]   'Cause I don't know about you,
[02:26:18.220 --> 02:26:20.620]   but I don't remember everything.
[02:26:20.620 --> 02:26:22.980]   I'm offloading so much of my memory about like,
[02:26:22.980 --> 02:26:26.340]   yeah, different functions, library functions,
[02:26:26.340 --> 02:26:27.500]   all that kind of stuff.
[02:26:27.500 --> 02:26:31.980]   Like this GPT just is very fast at standard stuff.
[02:26:31.980 --> 02:26:35.100]   And like standard library stuff,
[02:26:35.100 --> 02:26:36.700]   basic stuff that everybody uses.
[02:26:36.700 --> 02:26:40.180]   - Yeah, I think that,
[02:26:40.180 --> 02:26:43.420]   I don't know.
[02:26:43.420 --> 02:26:46.140]   I mean, there's just so little of this in Python.
[02:26:46.140 --> 02:26:48.340]   And maybe if I was coding more in other languages,
[02:26:48.340 --> 02:26:49.860]   I would consider it more,
[02:26:49.860 --> 02:26:52.620]   but I feel like Python already does such a good job
[02:26:52.620 --> 02:26:55.060]   of removing any boilerplate.
[02:26:55.060 --> 02:26:55.900]   - That's true.
[02:26:55.900 --> 02:26:57.980]   - It's the closest thing you can get to pseudocode, right?
[02:26:57.980 --> 02:26:59.620]   - Yeah, that's true.
[02:26:59.620 --> 02:27:00.460]   That's true.
[02:27:00.460 --> 02:27:01.380]   - I'm like, yeah, sure.
[02:27:01.380 --> 02:27:03.740]   If I like, yeah, great GPT.
[02:27:03.740 --> 02:27:06.180]   Thanks for reminding me to free my variables.
[02:27:06.180 --> 02:27:08.060]   Unfortunately, you didn't really recognize
[02:27:08.060 --> 02:27:10.340]   the scope correctly and you can't free that one.
[02:27:10.340 --> 02:27:12.980]   But like you put the freeze there and like, I get it.
[02:27:12.980 --> 02:27:15.580]   - Fiverr.
[02:27:15.580 --> 02:27:17.860]   Whenever I've used Fiverr for certain things,
[02:27:17.860 --> 02:27:21.220]   like design or whatever, it's always, you come back.
[02:27:21.220 --> 02:27:22.700]   I think that's probably closer,
[02:27:22.700 --> 02:27:24.580]   my experience with Fiverr is closer to your experience
[02:27:24.580 --> 02:27:26.580]   with programming with GPT is like,
[02:27:26.580 --> 02:27:28.100]   you're just frustrated and feel worse
[02:27:28.100 --> 02:27:30.460]   about the whole process of design and art
[02:27:30.460 --> 02:27:32.500]   and whatever you use Fiverr for.
[02:27:32.500 --> 02:27:38.540]   Still, I just feel like later versions of GPT,
[02:27:38.540 --> 02:27:43.500]   I'm using GPT as much as possible
[02:27:43.500 --> 02:27:45.860]   to just learn the dynamics of it,
[02:27:45.860 --> 02:27:48.100]   like these early versions,
[02:27:48.100 --> 02:27:49.380]   because it feels like in the future,
[02:27:49.380 --> 02:27:51.540]   you'll be using it more and more.
[02:27:51.540 --> 02:27:53.700]   And so like, I don't want to be,
[02:27:53.700 --> 02:27:56.300]   for the same reason I gave away all my books
[02:27:56.300 --> 02:27:57.820]   and switched to Kindle,
[02:27:57.820 --> 02:27:59.660]   'cause like, all right,
[02:27:59.660 --> 02:28:01.620]   how long are we gonna have paper books?
[02:28:01.620 --> 02:28:03.220]   Like 30 years from now?
[02:28:03.220 --> 02:28:05.940]   Like I wanna learn to be reading on Kindle,
[02:28:05.940 --> 02:28:07.500]   even though I don't enjoy it as much.
[02:28:07.500 --> 02:28:08.900]   And you learn to enjoy it more.
[02:28:08.900 --> 02:28:12.780]   In the same way, I switched from, let me just pause.
[02:28:12.780 --> 02:28:14.620]   - Switched from Emacs to VS Code.
[02:28:14.620 --> 02:28:15.540]   - Yeah.
[02:28:15.540 --> 02:28:16.700]   I switched from Vim to VS Code.
[02:28:16.700 --> 02:28:18.060]   I think I, similar, but.
[02:28:18.060 --> 02:28:19.220]   - Yeah, it's tough.
[02:28:19.220 --> 02:28:21.540]   And Vim to VS Code is even tougher,
[02:28:21.540 --> 02:28:25.940]   'cause Emacs is like old, like more outdated, feels like it.
[02:28:25.940 --> 02:28:28.220]   The community is more outdated.
[02:28:28.220 --> 02:28:30.260]   Vim is like pretty vibrant still.
[02:28:30.260 --> 02:28:31.100]   So it's just.
[02:28:31.100 --> 02:28:32.340]   - I never used any of the plugins.
[02:28:32.340 --> 02:28:33.180]   I still don't use any of the plugins.
[02:28:33.180 --> 02:28:34.420]   - That's what I, I looked at myself in the mirror.
[02:28:34.420 --> 02:28:36.660]   I'm like, yeah, you wrote some stuff in Lisp.
[02:28:36.660 --> 02:28:37.500]   Yeah.
[02:28:37.500 --> 02:28:39.300]   - No, but I never used any of the plugins in Vim either.
[02:28:39.300 --> 02:28:40.500]   I had the most vanilla Vim.
[02:28:40.500 --> 02:28:41.540]   I have a syntax highlighter.
[02:28:41.540 --> 02:28:42.540]   I didn't even have autocomplete.
[02:28:42.540 --> 02:28:47.540]   Like these things, I feel like help you so marginally
[02:28:47.540 --> 02:28:53.660]   that like, and now, okay, now VS Code's autocomplete
[02:28:53.660 --> 02:28:55.660]   has gotten good enough that like, okay,
[02:28:55.660 --> 02:28:56.500]   I don't have to set it up.
[02:28:56.500 --> 02:28:57.420]   I can just go into any code base
[02:28:57.420 --> 02:28:59.140]   and autocomplete's right 90% of the time.
[02:28:59.140 --> 02:29:00.820]   Okay, cool, I'll take it.
[02:29:00.820 --> 02:29:03.700]   All right, so I don't think I'm gonna have a problem
[02:29:03.700 --> 02:29:06.060]   at all adapting to the tools once they're good.
[02:29:06.060 --> 02:29:08.660]   But like the real thing that I want
[02:29:08.660 --> 02:29:12.740]   is not something that like tab completes my code
[02:29:12.740 --> 02:29:13.580]   and gives me ideas.
[02:29:13.580 --> 02:29:14.700]   The real thing that I want
[02:29:14.700 --> 02:29:17.020]   is a very intelligent pair programmer
[02:29:17.020 --> 02:29:19.900]   that comes up with a little pop-up saying,
[02:29:19.900 --> 02:29:23.460]   hey, you wrote a bug on line 14 and here's what it is.
[02:29:23.460 --> 02:29:24.300]   - Yeah.
[02:29:24.300 --> 02:29:25.140]   - Now I like that.
[02:29:25.140 --> 02:29:26.060]   You know what does a good job of this?
[02:29:26.060 --> 02:29:27.660]   MyPy.
[02:29:27.660 --> 02:29:28.700]   I love MyPy.
[02:29:28.700 --> 02:29:31.300]   MyPy, this fancy type checker for Python.
[02:29:31.300 --> 02:29:33.220]   And actually I tried like Microsoft released one too.
[02:29:33.220 --> 02:29:36.700]   And it was like 60% false positives.
[02:29:36.700 --> 02:29:38.740]   MyPy is like 5% false positives.
[02:29:38.740 --> 02:29:41.300]   95% of the time it recognizes,
[02:29:41.300 --> 02:29:42.180]   I didn't really think about
[02:29:42.180 --> 02:29:43.740]   that typing interaction correctly.
[02:29:43.740 --> 02:29:44.580]   Thank you, MyPy.
[02:29:44.580 --> 02:29:46.780]   - So you like type hinting.
[02:29:46.780 --> 02:29:48.980]   You like pushing the language
[02:29:48.980 --> 02:29:51.100]   towards being a typed language.
[02:29:51.100 --> 02:29:52.100]   - Oh yeah, absolutely.
[02:29:52.100 --> 02:29:54.980]   I think optional typing is great.
[02:29:54.980 --> 02:29:55.900]   I mean, look, I think that like,
[02:29:55.900 --> 02:29:56.940]   it's like a meet in the middle, right?
[02:29:56.940 --> 02:29:58.580]   Like Python has this optional type hinting
[02:29:58.580 --> 02:30:01.620]   and like C++ has auto.
[02:30:01.620 --> 02:30:03.780]   - C++ allows you to take a step back.
[02:30:03.780 --> 02:30:05.980]   - Well, C++ would have you brutally type out
[02:30:05.980 --> 02:30:08.100]   std string iterator, right?
[02:30:08.100 --> 02:30:09.900]   Now I can just type auto, which is nice.
[02:30:09.900 --> 02:30:12.980]   And then Python used to just have a,
[02:30:12.980 --> 02:30:14.100]   what type is a?
[02:30:14.100 --> 02:30:16.700]   It's an a.
[02:30:16.700 --> 02:30:18.340]   A colon str.
[02:30:18.340 --> 02:30:19.180]   Oh, okay.
[02:30:19.180 --> 02:30:20.420]   It's a string, cool.
[02:30:20.420 --> 02:30:21.260]   - Yeah.
[02:30:21.260 --> 02:30:22.460]   - I wish there was a way,
[02:30:22.460 --> 02:30:26.260]   like a simple way in Python to like turn on a mode,
[02:30:26.260 --> 02:30:28.500]   which would enforce the types.
[02:30:28.500 --> 02:30:29.420]   - Yeah, like give a warning
[02:30:29.420 --> 02:30:30.740]   when there's no type or something like this.
[02:30:30.740 --> 02:30:32.100]   - Well, no, to give a warning where,
[02:30:32.100 --> 02:30:33.660]   like MyPy is a static type checker,
[02:30:33.660 --> 02:30:35.500]   but I'm asking just for a runtime type checker.
[02:30:35.500 --> 02:30:37.260]   Like there's like ways to like hack this in,
[02:30:37.260 --> 02:30:38.420]   but I wish it was just like a flag,
[02:30:38.420 --> 02:30:40.460]   like Python three dash T.
[02:30:40.460 --> 02:30:41.380]   - Oh, I see.
[02:30:41.380 --> 02:30:42.220]   I see.
[02:30:42.220 --> 02:30:43.060]   - Enforce the types around time.
[02:30:43.060 --> 02:30:43.900]   - Yeah.
[02:30:43.900 --> 02:30:45.380]   I feel like that makes you a better programmer.
[02:30:45.380 --> 02:30:47.300]   That's the kind of test, right?
[02:30:47.300 --> 02:30:49.900]   That the type remains the same.
[02:30:49.900 --> 02:30:51.820]   - Well, no, that doesn't like mess any types up.
[02:30:51.820 --> 02:30:55.380]   But again, like MyPy is getting really good and I love it.
[02:30:55.380 --> 02:30:56.980]   And I can't wait for some of these tools
[02:30:56.980 --> 02:30:58.420]   to become AI powered.
[02:30:58.420 --> 02:31:01.300]   I want AIs reading my code and giving me feedback.
[02:31:01.300 --> 02:31:05.060]   I don't want AIs writing half-assed
[02:31:05.060 --> 02:31:06.660]   auto-complete stuff for me.
[02:31:06.660 --> 02:31:08.980]   - I wonder if you can now take GPT
[02:31:08.980 --> 02:31:11.100]   and give it a code that you wrote for a function
[02:31:11.100 --> 02:31:13.220]   and say, how can I make this simpler
[02:31:13.220 --> 02:31:15.380]   and have it accomplish the same thing?
[02:31:15.380 --> 02:31:17.340]   I think you'll get some good ideas on some code.
[02:31:17.340 --> 02:31:18.980]   Maybe not the code you write,
[02:31:18.980 --> 02:31:22.100]   for TinyGrad type of code,
[02:31:22.100 --> 02:31:24.220]   'cause that requires so much design thinking,
[02:31:24.220 --> 02:31:26.180]   but like other kinds of code.
[02:31:26.180 --> 02:31:27.140]   - I don't know.
[02:31:27.140 --> 02:31:29.700]   I downloaded that plugin maybe like two months ago.
[02:31:29.700 --> 02:31:31.780]   I tried it again and found the same.
[02:31:31.780 --> 02:31:34.660]   Look, I don't doubt that these models
[02:31:34.660 --> 02:31:37.860]   are going to first become useful to me,
[02:31:37.860 --> 02:31:40.420]   then be as good as me and then surpass me.
[02:31:40.420 --> 02:31:42.740]   But from what I've seen today,
[02:31:42.740 --> 02:31:47.740]   it's like someone occasionally taking over my keyboard
[02:31:47.740 --> 02:31:51.420]   that I hired from Fiverr.
[02:31:51.420 --> 02:31:53.940]   I'd rather not.
[02:31:53.940 --> 02:31:55.820]   - Ideas about how to debug the code
[02:31:55.820 --> 02:31:58.740]   or basically a better debugger is really interesting.
[02:31:58.740 --> 02:31:59.900]   But it's not a better debugger.
[02:31:59.900 --> 02:32:01.780]   I guess I would love a better debugger.
[02:32:01.780 --> 02:32:02.620]   - Yeah, it's not yet.
[02:32:02.620 --> 02:32:04.500]   Yeah, but it feels like it's not too far.
[02:32:04.500 --> 02:32:05.860]   - Yeah, one of my coworkers says
[02:32:05.860 --> 02:32:07.460]   he uses them for print statements.
[02:32:07.460 --> 02:32:09.340]   Like every time he has to like, just like when he needs,
[02:32:09.340 --> 02:32:11.060]   the only thing I can really write is like,
[02:32:11.060 --> 02:32:12.100]   okay, I just want to write the thing
[02:32:12.100 --> 02:32:14.380]   to like print the state out right now.
[02:32:14.380 --> 02:32:17.740]   - Oh, that definitely is much faster.
[02:32:17.740 --> 02:32:19.340]   It's print statements, yeah.
[02:32:19.340 --> 02:32:20.940]   I see myself using that a lot
[02:32:20.940 --> 02:32:23.180]   just 'cause it figures out the rest of the functions.
[02:32:23.180 --> 02:32:24.460]   It's just like, okay, print everything.
[02:32:24.460 --> 02:32:25.460]   - Yeah, print everything, right?
[02:32:25.460 --> 02:32:27.980]   And then yeah, like if you want a pretty printer, maybe.
[02:32:27.980 --> 02:32:28.820]   I'm like, yeah, you know what?
[02:32:28.820 --> 02:32:30.860]   I think in two years,
[02:32:30.860 --> 02:32:33.780]   I'm gonna start using these plugins a little bit.
[02:32:33.780 --> 02:32:35.020]   And then in five years,
[02:32:35.020 --> 02:32:38.060]   I'm gonna be heavily relying on some AI augmented flow.
[02:32:38.060 --> 02:32:39.660]   And then in 10 years.
[02:32:39.660 --> 02:32:41.660]   - Do you think you'll ever get to 100%?
[02:32:41.660 --> 02:32:45.620]   What's the role of the human
[02:32:45.620 --> 02:32:48.500]   that it converges to as a programmer?
[02:32:48.500 --> 02:32:49.340]   - No.
[02:32:49.340 --> 02:32:52.020]   - So you think it's all generated?
[02:32:52.020 --> 02:32:53.140]   - Our niche becomes,
[02:32:53.140 --> 02:32:55.340]   oh, I think it's over for humans in general.
[02:32:55.340 --> 02:32:57.900]   It's not just programming, it's everything.
[02:32:57.900 --> 02:32:59.340]   - So niche becomes, whoa.
[02:32:59.340 --> 02:33:00.740]   - Our niche becomes smaller and smaller and smaller.
[02:33:00.740 --> 02:33:02.300]   In fact, I'll tell you what the last niche
[02:33:02.300 --> 02:33:04.380]   of humanity is gonna be.
[02:33:04.380 --> 02:33:05.460]   There's a great book,
[02:33:05.460 --> 02:33:07.140]   and it's, if I recommended Metamorphosis
[02:33:07.140 --> 02:33:08.880]   of the Prime Intellect last time,
[02:33:08.880 --> 02:33:12.380]   there is a sequel called A Casino Odyssey in Cyberspace.
[02:33:12.380 --> 02:33:15.780]   And I don't wanna give away the ending of this,
[02:33:15.780 --> 02:33:18.820]   but it tells you what the last remaining human currency is.
[02:33:18.820 --> 02:33:19.920]   And I agree with that.
[02:33:19.920 --> 02:33:23.840]   - We'll leave that as the cliffhanger.
[02:33:25.900 --> 02:33:27.780]   - So no more programmers left, huh?
[02:33:27.780 --> 02:33:29.540]   That's where we're going.
[02:33:29.540 --> 02:33:31.220]   - Well, unless you want handmade code,
[02:33:31.220 --> 02:33:32.460]   maybe they'll sell it on Etsy.
[02:33:32.460 --> 02:33:33.920]   This is handwritten code.
[02:33:33.920 --> 02:33:37.700]   Doesn't have that machine polished to it.
[02:33:37.700 --> 02:33:39.100]   It has those slight imperfections
[02:33:39.100 --> 02:33:41.000]   that would only be written by a person.
[02:33:41.000 --> 02:33:44.460]   - I wonder how far away we are from that.
[02:33:44.460 --> 02:33:46.460]   I mean, there's some aspect to,
[02:33:46.460 --> 02:33:47.580]   you know, on Instagram,
[02:33:47.580 --> 02:33:49.660]   your title is listed as prompt engineer.
[02:33:49.660 --> 02:33:50.500]   - Right?
[02:33:50.500 --> 02:33:52.580]   Thank you for noticing.
[02:33:54.060 --> 02:33:57.240]   - I don't know if it's ironic or non,
[02:33:57.240 --> 02:34:00.580]   or sarcastic or non.
[02:34:00.580 --> 02:34:02.780]   What do you think of prompt engineering
[02:34:02.780 --> 02:34:06.340]   as a scientific and engineering discipline,
[02:34:06.340 --> 02:34:08.700]   or maybe, and maybe art form?
[02:34:08.700 --> 02:34:09.540]   - You know what?
[02:34:09.540 --> 02:34:12.140]   I started comma six years ago.
[02:34:12.140 --> 02:34:13.980]   I started the tiny corp a month ago.
[02:34:13.980 --> 02:34:18.040]   So much has changed.
[02:34:18.040 --> 02:34:20.940]   Like I'm now thinking, I'm now like,
[02:34:22.160 --> 02:34:24.440]   I started like going through like similar comma processes
[02:34:24.440 --> 02:34:25.280]   to like starting a company.
[02:34:25.280 --> 02:34:27.120]   I'm like, okay, I'm gonna get an office in San Diego.
[02:34:27.120 --> 02:34:28.520]   I'm gonna bring people here.
[02:34:28.520 --> 02:34:30.260]   I don't think so.
[02:34:30.260 --> 02:34:32.680]   I think I'm actually gonna do remote, right?
[02:34:32.680 --> 02:34:33.520]   George, you're gonna do remote?
[02:34:33.520 --> 02:34:34.360]   You hate remote?
[02:34:34.360 --> 02:34:36.440]   Yeah, but I'm not gonna do job interviews.
[02:34:36.440 --> 02:34:37.520]   The only way you're gonna get a job
[02:34:37.520 --> 02:34:39.760]   is if you contribute to the GitHub, right?
[02:34:39.760 --> 02:34:44.240]   And then like, like interacting through GitHub,
[02:34:44.240 --> 02:34:48.080]   like GitHub being the real like project management software
[02:34:48.080 --> 02:34:48.920]   for your company.
[02:34:48.920 --> 02:34:51.240]   And the thing pretty much just is a GitHub repo.
[02:34:52.080 --> 02:34:55.480]   Is like showing me kind of what the future of, okay.
[02:34:55.480 --> 02:34:56.920]   So a lot of times I'll go on a Discord
[02:34:56.920 --> 02:34:58.320]   or kind of go on Discord
[02:34:58.320 --> 02:34:59.640]   and I'll throw out some random like,
[02:34:59.640 --> 02:35:00.960]   hey, you know, can you change
[02:35:00.960 --> 02:35:03.880]   instead of having log and exp as LL ops,
[02:35:03.880 --> 02:35:06.320]   change it to log two and exp two?
[02:35:06.320 --> 02:35:07.160]   It's a pretty small change.
[02:35:07.160 --> 02:35:09.120]   You could just use like change the base formula.
[02:35:09.120 --> 02:35:12.920]   That's the kind of task that I can see an AI
[02:35:12.920 --> 02:35:14.840]   being able to do in a few years.
[02:35:14.840 --> 02:35:17.480]   Like in a few years, I could see myself describing that.
[02:35:17.480 --> 02:35:19.680]   And then within 30 seconds, a pull request is up
[02:35:19.680 --> 02:35:20.760]   that does it.
[02:35:20.760 --> 02:35:23.320]   And it passes my CI and I merge it, right?
[02:35:23.320 --> 02:35:24.880]   So I really started thinking about like,
[02:35:24.880 --> 02:35:28.480]   well, what is the future of like jobs?
[02:35:28.480 --> 02:35:30.560]   How many AIs can I employ at my company?
[02:35:30.560 --> 02:35:32.080]   As soon as we get the first tiny box up,
[02:35:32.080 --> 02:35:35.320]   I'm gonna stand up a 65B llama in the Discord.
[02:35:35.320 --> 02:35:36.640]   And it's like, yeah, here's the tiny box.
[02:35:36.640 --> 02:35:39.040]   He's just like, he's chilling with us.
[02:35:39.040 --> 02:35:42.640]   - Basically, like you said with niches,
[02:35:42.640 --> 02:35:47.160]   most human jobs will eventually be replaced
[02:35:47.160 --> 02:35:48.440]   with prompt engineering.
[02:35:48.440 --> 02:35:51.280]   - Well, prompt engineering kind of is this like,
[02:35:51.280 --> 02:35:54.960]   as you like move up the stack, right?
[02:35:54.960 --> 02:35:56.480]   Like, okay, there used to be humans
[02:35:56.480 --> 02:35:59.160]   actually doing arithmetic by hand.
[02:35:59.160 --> 02:36:00.600]   And there used to be like big farms of people
[02:36:00.600 --> 02:36:03.000]   doing pluses and stuff, right?
[02:36:03.000 --> 02:36:05.240]   And then you have like spreadsheets, right?
[02:36:05.240 --> 02:36:07.560]   And then, okay, the spreadsheet can do the plus for me.
[02:36:07.560 --> 02:36:09.960]   And then you have like macros, right?
[02:36:09.960 --> 02:36:11.320]   And then you have like things that basically
[02:36:11.320 --> 02:36:13.400]   just are spreadsheets under the hood, right?
[02:36:13.400 --> 02:36:14.840]   Like accounting software.
[02:36:17.360 --> 02:36:19.280]   As we move further up the abstraction,
[02:36:19.280 --> 02:36:20.800]   what's at the top of the abstraction stack?
[02:36:20.800 --> 02:36:22.600]   Well, a prompt engineer.
[02:36:22.600 --> 02:36:23.440]   - Yeah.
[02:36:23.440 --> 02:36:26.440]   - Right, what is the last thing if you think about
[02:36:26.440 --> 02:36:30.040]   like humans wanting to keep control?
[02:36:30.040 --> 02:36:31.680]   Well, what am I really in the company
[02:36:31.680 --> 02:36:33.600]   but a prompt engineer, right?
[02:36:33.600 --> 02:36:35.840]   - Isn't there a certain point where the AI
[02:36:35.840 --> 02:36:38.600]   will be better at writing prompts?
[02:36:38.600 --> 02:36:41.600]   - Yeah, but you see the problem with the AI writing prompts,
[02:36:41.600 --> 02:36:43.840]   a definition that I always liked of AI
[02:36:43.840 --> 02:36:46.880]   was AI is the do what I mean machine, right?
[02:36:46.880 --> 02:36:50.720]   AI is not the, like the computer is so pedantic.
[02:36:50.720 --> 02:36:52.120]   It does what you say.
[02:36:52.120 --> 02:36:55.520]   So, but you want the do what I mean machine.
[02:36:55.520 --> 02:36:56.360]   - Yeah.
[02:36:56.360 --> 02:36:57.840]   - Right, you want the machine where you say,
[02:36:57.840 --> 02:36:59.680]   you know, get my grandmother out of the burning house.
[02:36:59.680 --> 02:37:01.360]   It like reasonably takes your grandmother
[02:37:01.360 --> 02:37:02.200]   and puts her on the ground,
[02:37:02.200 --> 02:37:04.280]   not lifts her a thousand feet above the burning house
[02:37:04.280 --> 02:37:05.840]   and lets her fall, right?
[02:37:05.840 --> 02:37:06.680]   - But you don't--
[02:37:06.680 --> 02:37:07.520]   - There's an old Yudkowsky example.
[02:37:07.520 --> 02:37:09.120]   (Lex laughing)
[02:37:09.120 --> 02:37:13.080]   - But it's not going to find the meaning.
[02:37:13.080 --> 02:37:16.840]   I mean, to do what I mean, it has to figure stuff out.
[02:37:16.840 --> 02:37:17.680]   - Sure.
[02:37:17.680 --> 02:37:21.320]   - And the thing you'll maybe ask it to do
[02:37:21.320 --> 02:37:23.760]   is run government for me.
[02:37:23.760 --> 02:37:25.720]   - Oh, and do what I mean very much comes down
[02:37:25.720 --> 02:37:28.120]   to how aligned is that AI with you?
[02:37:28.120 --> 02:37:31.120]   Of course, when you talk to an AI
[02:37:31.120 --> 02:37:34.120]   that's made by a big company in the cloud,
[02:37:34.120 --> 02:37:37.800]   the AI fundamentally is aligned to them, not to you.
[02:37:37.800 --> 02:37:38.640]   - Yeah.
[02:37:38.640 --> 02:37:39.800]   - And that's why you have to buy a tiny box.
[02:37:39.800 --> 02:37:41.720]   So you make sure the AI stays aligned to you.
[02:37:41.720 --> 02:37:45.360]   Every time that they start to pass AI regulation
[02:37:45.360 --> 02:37:48.240]   or GPU regulation, I'm gonna see sales of tiny boxes spike.
[02:37:48.240 --> 02:37:49.560]   It's gonna be like guns, right?
[02:37:49.560 --> 02:37:51.440]   Every time they talk about gun regulation,
[02:37:51.440 --> 02:37:53.080]   boom, gun sales.
[02:37:53.080 --> 02:37:55.440]   - So in the space of AI, you're an anarchist,
[02:37:55.440 --> 02:37:58.760]   anarchism, espouser, believer.
[02:37:58.760 --> 02:38:00.600]   - I'm an informational anarchist, yes.
[02:38:00.600 --> 02:38:03.800]   I'm an informational anarchist and a physical statist.
[02:38:03.800 --> 02:38:07.400]   I do not think anarchy in the physical world is very good
[02:38:07.400 --> 02:38:09.040]   because I exist in the physical world.
[02:38:09.040 --> 02:38:11.400]   But I think we can construct this virtual world
[02:38:11.400 --> 02:38:13.440]   where anarchy, it can't hurt you, right?
[02:38:13.440 --> 02:38:16.040]   I love that Tyler, the creator, tweet.
[02:38:16.040 --> 02:38:18.200]   Yo, cyber bullying isn't real, man.
[02:38:18.200 --> 02:38:19.040]   Have you tried?
[02:38:19.040 --> 02:38:21.240]   Turn it off the screen, close your eyes.
[02:38:21.240 --> 02:38:22.080]   Like.
[02:38:22.080 --> 02:38:22.920]   - Yeah.
[02:38:22.920 --> 02:38:28.840]   But how do you prevent the AI
[02:38:28.840 --> 02:38:33.840]   from basically replacing all human prompt engineers?
[02:38:33.840 --> 02:38:36.400]   Where there's, it's like a self,
[02:38:36.400 --> 02:38:38.360]   like where nobody's the prompt engineer anymore.
[02:38:38.360 --> 02:38:40.560]   So autonomy, greater and greater autonomy
[02:38:40.560 --> 02:38:41.680]   until it's full autonomy.
[02:38:41.680 --> 02:38:43.120]   - Yeah.
[02:38:43.120 --> 02:38:45.040]   - And that's just where it's headed.
[02:38:45.040 --> 02:38:49.200]   'Cause one person's gonna say, run everything for me.
[02:38:49.200 --> 02:38:54.080]   - You see, I look at potential futures.
[02:38:54.080 --> 02:38:59.080]   And as long as the AIs go on to create a vibrant civilization
[02:38:59.080 --> 02:39:04.920]   with diversity and complexity across the universe,
[02:39:04.920 --> 02:39:06.360]   more power to them, I'll die.
[02:39:06.360 --> 02:39:09.760]   If the AIs go on to actually like turn the world
[02:39:09.760 --> 02:39:12.040]   into paperclips and then they die out themselves,
[02:39:12.040 --> 02:39:14.560]   well, that's horrific and we don't want that to happen.
[02:39:14.560 --> 02:39:17.000]   So this is what I mean about like robustness.
[02:39:17.000 --> 02:39:19.200]   I trust robust machines.
[02:39:19.200 --> 02:39:20.920]   The current AIs are so not robust.
[02:39:20.920 --> 02:39:21.960]   Like this comes back to the idea
[02:39:21.960 --> 02:39:24.800]   that we've never made a machine that can self replicate.
[02:39:24.800 --> 02:39:25.640]   Right?
[02:39:25.640 --> 02:39:28.400]   But when we have, if the machines are truly robust
[02:39:28.400 --> 02:39:30.960]   and there is one prompt engineer left in the world,
[02:39:30.960 --> 02:39:34.120]   hope you're doing good, man.
[02:39:34.120 --> 02:39:35.080]   Hope you believe in God.
[02:39:35.080 --> 02:39:38.800]   Like, you know, go by God.
[02:39:38.800 --> 02:39:42.800]   And go forth and conquer the universe.
[02:39:42.800 --> 02:39:44.800]   - Well, you mentioned, 'cause I talked to Mark
[02:39:44.800 --> 02:39:48.600]   about faith in God and you said you were impressed by that.
[02:39:48.600 --> 02:39:50.240]   What's your own belief in God
[02:39:50.240 --> 02:39:52.640]   and how does that affect your work?
[02:39:52.640 --> 02:39:56.000]   - You know, I never really considered when I was younger,
[02:39:56.000 --> 02:39:57.360]   I guess my parents were atheists.
[02:39:57.360 --> 02:39:58.320]   So I was raised kind of atheist.
[02:39:58.320 --> 02:39:59.680]   I never really considered how absolutely
[02:39:59.680 --> 02:40:01.440]   like silly atheism is.
[02:40:01.440 --> 02:40:05.120]   'Cause like I create worlds, right?
[02:40:05.120 --> 02:40:08.200]   Every like game creator, like how are you an atheist, bro?
[02:40:08.200 --> 02:40:09.480]   You create worlds.
[02:40:09.480 --> 02:40:11.240]   Who's a, no one created our world, man.
[02:40:11.240 --> 02:40:12.080]   That's different.
[02:40:12.080 --> 02:40:13.480]   Haven't you heard about like the Big Bang and stuff?
[02:40:13.480 --> 02:40:17.360]   Yeah, I mean, what's the Skyrim myth origin story in Skyrim?
[02:40:17.360 --> 02:40:19.200]   I'm sure there's like some part of it in Skyrim,
[02:40:19.200 --> 02:40:21.480]   but it's not like if you ask the creators,
[02:40:21.480 --> 02:40:23.880]   like the Big Bang is in universe, right?
[02:40:23.880 --> 02:40:27.120]   I'm sure they have some Big Bang notion in Skyrim, right?
[02:40:27.120 --> 02:40:28.560]   But that obviously is not at all
[02:40:28.560 --> 02:40:30.040]   how Skyrim was actually created.
[02:40:30.040 --> 02:40:32.520]   It was created by a bunch of programmers in a room, right?
[02:40:32.520 --> 02:40:35.800]   So like, you know, it just struck me one day
[02:40:35.800 --> 02:40:37.320]   how just silly atheism is.
[02:40:37.320 --> 02:40:39.480]   Like, of course we were created by God.
[02:40:39.480 --> 02:40:40.880]   It's the most obvious thing.
[02:40:40.880 --> 02:40:46.800]   - Yeah, that's such a nice way to put it.
[02:40:46.800 --> 02:40:50.320]   Like we're such powerful creators ourselves.
[02:40:50.320 --> 02:40:53.480]   It's silly not to conceive that there's creators
[02:40:53.480 --> 02:40:54.840]   even more powerful than us.
[02:40:54.840 --> 02:40:58.240]   - Yeah, and then like, I also just like, I like that notion.
[02:40:58.240 --> 02:41:00.360]   That notion gives me a lot of,
[02:41:00.360 --> 02:41:01.960]   I mean, I guess you can talk about it,
[02:41:01.960 --> 02:41:03.240]   what it gives a lot of religious people.
[02:41:03.240 --> 02:41:04.680]   It's kind of like, it just gives me comfort.
[02:41:04.680 --> 02:41:05.920]   It's like, you know what?
[02:41:05.920 --> 02:41:08.960]   If we mess it all up and we die out, eh.
[02:41:08.960 --> 02:41:11.160]   - Yeah, and the same way that a video game
[02:41:11.160 --> 02:41:12.240]   kind of has comfort in it.
[02:41:12.240 --> 02:41:13.320]   - God'll try again.
[02:41:13.320 --> 02:41:15.360]   - Or there's balance.
[02:41:15.360 --> 02:41:18.800]   Like somebody figured out a balanced view of it.
[02:41:18.800 --> 02:41:22.560]   Like how to, like, so it all makes sense in the end.
[02:41:22.560 --> 02:41:25.200]   Like a video game is usually not gonna have
[02:41:25.200 --> 02:41:26.880]   crazy, crazy stuff.
[02:41:26.880 --> 02:41:29.760]   - You know, people will come up with like,
[02:41:29.760 --> 02:41:33.840]   well, yeah, but like, man, who created God?
[02:41:33.840 --> 02:41:36.400]   I'm like, that's God's problem.
[02:41:36.400 --> 02:41:38.640]   No, like, I'm not gonna think this is,
[02:41:38.640 --> 02:41:41.320]   what are you asking me, what, if God believes in God?
[02:41:41.320 --> 02:41:43.320]   - I'm just this NPC living in this game.
[02:41:43.320 --> 02:41:45.880]   - I mean, to be fair, like if God didn't believe in God,
[02:41:45.880 --> 02:41:48.720]   he'd be as, you know, silly as the atheists here.
[02:41:48.720 --> 02:41:51.120]   - What do you think is the greatest
[02:41:51.120 --> 02:41:52.720]   computer game of all time?
[02:41:52.720 --> 02:41:55.600]   Do you have any time to play games anymore?
[02:41:55.600 --> 02:41:57.360]   Have you played Diablo 4?
[02:41:57.360 --> 02:41:59.160]   - I have not played Diablo 4.
[02:41:59.160 --> 02:42:00.880]   - I will be doing that shortly.
[02:42:00.880 --> 02:42:01.920]   I have to. - All right.
[02:42:01.920 --> 02:42:04.240]   - There's just so much history with one, two, and three.
[02:42:04.240 --> 02:42:05.240]   - You know what?
[02:42:05.240 --> 02:42:07.240]   I'm gonna say World of Warcraft.
[02:42:07.240 --> 02:42:08.440]   - Ooh.
[02:42:08.440 --> 02:42:13.040]   - And it's not that the game is such a great game.
[02:42:13.040 --> 02:42:14.520]   It's not.
[02:42:14.520 --> 02:42:18.520]   It's that I remember in 2005 when it came out,
[02:42:18.520 --> 02:42:22.440]   how it opened my mind to ideas.
[02:42:22.440 --> 02:42:24.120]   It opened my mind to like,
[02:42:24.120 --> 02:42:28.480]   this whole world we've created, right?
[02:42:28.480 --> 02:42:30.400]   There's almost been nothing like it since.
[02:42:30.400 --> 02:42:32.480]   Like, you can look at MMOs today,
[02:42:32.480 --> 02:42:34.280]   and I think they all have lower user bases
[02:42:34.280 --> 02:42:35.280]   than World of Warcraft.
[02:42:35.280 --> 02:42:37.400]   Like, EVE Online's kind of cool.
[02:42:37.400 --> 02:42:41.640]   But to think that like, everyone knows,
[02:42:41.640 --> 02:42:42.880]   you know, people are always like,
[02:42:42.880 --> 02:42:44.080]   they look at the Apple headset,
[02:42:44.080 --> 02:42:47.040]   like, what do people want in this VR?
[02:42:47.040 --> 02:42:47.880]   Everyone knows what they want.
[02:42:47.880 --> 02:42:49.840]   I want Ready Player One.
[02:42:49.840 --> 02:42:51.400]   And like that.
[02:42:51.400 --> 02:42:52.680]   So I'm gonna say World of Warcraft,
[02:42:52.680 --> 02:42:56.480]   and I'm hoping that games can get out of this whole
[02:42:56.480 --> 02:42:59.440]   mobile gaming dopamine pump thing.
[02:42:59.440 --> 02:43:00.840]   And like- - Create worlds.
[02:43:00.840 --> 02:43:03.080]   - Create worlds, yeah.
[02:43:03.080 --> 02:43:05.600]   - And worlds that captivate a very large fraction
[02:43:05.600 --> 02:43:06.760]   of the human population.
[02:43:06.760 --> 02:43:09.600]   - Yeah, and I think it'll come back, I believe.
[02:43:09.600 --> 02:43:13.280]   - But MMO, like really, really pull you in.
[02:43:13.280 --> 02:43:14.240]   - Games do a good job.
[02:43:14.240 --> 02:43:15.960]   I mean, okay, other, like two other games
[02:43:15.960 --> 02:43:17.720]   that I think are very noteworthy for me
[02:43:17.720 --> 02:43:19.840]   are Skyrim and GTA V.
[02:43:19.840 --> 02:43:21.720]   - Skyrim, yeah.
[02:43:21.720 --> 02:43:23.720]   That's probably number one for me.
[02:43:23.720 --> 02:43:24.920]   GTA.
[02:43:24.920 --> 02:43:26.560]   Yeah, what is it about GTA?
[02:43:26.560 --> 02:43:28.960]   GTA is really,
[02:43:28.960 --> 02:43:32.880]   I guess GTA is real life.
[02:43:32.880 --> 02:43:35.000]   I know there's prostitutes and guns and stuff.
[02:43:35.000 --> 02:43:36.520]   - There exists a real life too.
[02:43:36.520 --> 02:43:38.840]   - Yes, I know.
[02:43:38.840 --> 02:43:42.080]   But it's how I imagine your life to be, actually.
[02:43:42.080 --> 02:43:43.240]   - I wish it was that cool.
[02:43:43.240 --> 02:43:44.080]   - Yeah.
[02:43:44.080 --> 02:43:46.920]   Yeah, I guess that's, you know,
[02:43:46.920 --> 02:43:48.320]   'cause there's Sims, right?
[02:43:48.320 --> 02:43:50.240]   Which is also a game I like.
[02:43:50.240 --> 02:43:52.480]   But it's a gamified version of life.
[02:43:52.480 --> 02:43:53.680]   But it also is,
[02:43:53.680 --> 02:43:56.760]   I would love a combination of Sims and GTA.
[02:43:58.640 --> 02:44:02.000]   So more freedom, more violence, more rawness.
[02:44:02.000 --> 02:44:05.120]   But with also like ability to have a career and family
[02:44:05.120 --> 02:44:05.960]   and this kind of stuff.
[02:44:05.960 --> 02:44:08.240]   - What I'm really excited about in games
[02:44:08.240 --> 02:44:10.320]   is like once we start getting
[02:44:10.320 --> 02:44:11.880]   intelligent AIs to interact with.
[02:44:11.880 --> 02:44:12.720]   - Oh yeah.
[02:44:12.720 --> 02:44:14.680]   - Like the NPCs in games have never been.
[02:44:14.680 --> 02:44:17.800]   - But conversationally.
[02:44:17.800 --> 02:44:19.080]   Oh, in every way.
[02:44:19.080 --> 02:44:21.520]   - In like, yeah, in like every way.
[02:44:21.520 --> 02:44:23.360]   Like when you're actually building a world
[02:44:23.360 --> 02:44:26.840]   and a world imbued with intelligence.
[02:44:26.840 --> 02:44:27.680]   - Oh yeah.
[02:44:27.680 --> 02:44:28.640]   - And it's just hard.
[02:44:28.640 --> 02:44:30.560]   There's just like, you know, running World of Warcraft.
[02:44:30.560 --> 02:44:33.160]   Like you're limited by what you're running on a Pentium 4.
[02:44:33.160 --> 02:44:34.400]   You know, how much intelligence can you run?
[02:44:34.400 --> 02:44:36.440]   How many flops did you have?
[02:44:36.440 --> 02:44:39.040]   But now when I'm running a game
[02:44:39.040 --> 02:44:42.200]   on a hundred petaflop machine, well, it's five people.
[02:44:42.200 --> 02:44:43.480]   I'm trying to make this a thing.
[02:44:43.480 --> 02:44:45.880]   20 petaflops of compute is one person of compute.
[02:44:45.880 --> 02:44:47.120]   I'm trying to make that a unit.
[02:44:47.120 --> 02:44:50.200]   - 20 petaflops is one person.
[02:44:50.200 --> 02:44:51.320]   - One person.
[02:44:51.320 --> 02:44:52.160]   - One person flop.
[02:44:52.160 --> 02:44:53.320]   - It's like a horsepower.
[02:44:53.320 --> 02:44:55.520]   But what's a horsepower?
[02:44:55.520 --> 02:44:56.360]   It's how powerful a horse is.
[02:44:56.360 --> 02:44:57.440]   What's a person of compute?
[02:44:57.440 --> 02:44:58.280]   Well, you know.
[02:44:58.280 --> 02:44:59.120]   - You know, you flop.
[02:44:59.120 --> 02:45:00.680]   I got it.
[02:45:00.680 --> 02:45:01.720]   That's interesting.
[02:45:01.720 --> 02:45:05.960]   VR also adds, I mean, in terms of creating worlds.
[02:45:05.960 --> 02:45:07.040]   - You know what?
[02:45:07.040 --> 02:45:08.680]   Border Quest 2.
[02:45:08.680 --> 02:45:11.000]   I put it on and I can't believe
[02:45:11.000 --> 02:45:14.040]   the first thing they show me is a bunch of scrolling clouds
[02:45:14.040 --> 02:45:15.900]   and a Facebook login screen.
[02:45:15.900 --> 02:45:16.800]   - Yeah.
[02:45:16.800 --> 02:45:19.680]   - You had the ability to bring me into a world.
[02:45:19.680 --> 02:45:20.500]   - Yeah.
[02:45:20.500 --> 02:45:21.680]   - And what did you give me?
[02:45:21.680 --> 02:45:22.840]   A pop-up, right?
[02:45:22.840 --> 02:45:25.920]   Like, and this is why you're not cool, Mark Zuckerberg.
[02:45:25.920 --> 02:45:27.060]   But you could be cool.
[02:45:27.060 --> 02:45:28.780]   Just make sure on the Quest 3,
[02:45:28.780 --> 02:45:31.320]   you don't put me into clouds and a Facebook login screen.
[02:45:31.320 --> 02:45:32.400]   Bring me to a world.
[02:45:32.400 --> 02:45:34.060]   - I just tried Quest 3.
[02:45:34.060 --> 02:45:34.900]   It was awesome.
[02:45:34.900 --> 02:45:35.760]   But hear that, guys?
[02:45:35.760 --> 02:45:36.600]   I agree with that.
[02:45:36.600 --> 02:45:37.440]   So I--
[02:45:37.440 --> 02:45:39.640]   - We didn't have the clouds in the world.
[02:45:39.640 --> 02:45:40.480]   It was just so--
[02:45:40.480 --> 02:45:41.300]   - You know what?
[02:45:41.300 --> 02:45:44.440]   'Cause I, I mean, the beginning, what is it?
[02:45:44.440 --> 02:45:47.480]   Todd Howard said this about the design
[02:45:47.480 --> 02:45:48.800]   of the beginning of the games he creates.
[02:45:48.800 --> 02:45:51.680]   It's like the beginning is so, so, so important.
[02:45:51.680 --> 02:45:53.520]   I recently played Zelda for the first time.
[02:45:53.520 --> 02:45:55.720]   Zelda Breath of the Wild, the previous one.
[02:45:55.720 --> 02:46:00.220]   And it's very quickly, you come out of this,
[02:46:00.220 --> 02:46:03.200]   within 10 seconds, you come out of a cave-type place,
[02:46:03.200 --> 02:46:05.520]   and it's like, this world opens up.
[02:46:05.520 --> 02:46:07.200]   It's like, ah.
[02:46:07.200 --> 02:46:09.840]   And it pulls you in.
[02:46:09.840 --> 02:46:13.440]   You forget whatever troubles I was having, whatever--
[02:46:13.440 --> 02:46:14.440]   - I gotta play that from the beginning.
[02:46:14.440 --> 02:46:16.300]   I played it for an hour at a friend's house.
[02:46:16.300 --> 02:46:18.120]   - Ah, no, the beginning, they got it.
[02:46:18.120 --> 02:46:21.900]   They did it really well, the expansiveness of that space,
[02:46:21.900 --> 02:46:25.080]   the peacefulness of that place.
[02:46:25.080 --> 02:46:27.200]   They got this, the music, I mean, so much of that.
[02:46:27.200 --> 02:46:29.360]   It's creating that world and pulling you right in.
[02:46:29.360 --> 02:46:30.960]   - I'm gonna go buy a Switch.
[02:46:30.960 --> 02:46:32.240]   I'm gonna go today and buy a Switch.
[02:46:32.240 --> 02:46:33.080]   - You should.
[02:46:33.080 --> 02:46:34.280]   Well, the new one came out, I haven't played that yet,
[02:46:34.280 --> 02:46:37.000]   but Diablo IV or something.
[02:46:37.000 --> 02:46:39.000]   I mean, there's sentimentality also,
[02:46:39.000 --> 02:46:43.400]   but something about VR really is incredible.
[02:46:43.400 --> 02:46:47.640]   But the new Quest 3 is mixed reality,
[02:46:47.640 --> 02:46:49.040]   and I got a chance to try that.
[02:46:49.040 --> 02:46:51.160]   So it's augmented reality.
[02:46:51.160 --> 02:46:53.960]   And for video games, it's done really, really well.
[02:46:53.960 --> 02:46:55.000]   - Is it pass-through or cameras?
[02:46:55.000 --> 02:46:56.520]   - It's cameras, sorry.
[02:46:56.520 --> 02:46:58.880]   The Apple one, is that one pass-through or cameras?
[02:46:58.880 --> 02:46:59.960]   - I don't know.
[02:46:59.960 --> 02:47:01.040]   I don't know how real it is.
[02:47:01.040 --> 02:47:02.280]   I don't know anything.
[02:47:02.280 --> 02:47:05.160]   - Coming out in January.
[02:47:05.160 --> 02:47:06.720]   - Is it January or is it some point?
[02:47:06.720 --> 02:47:08.560]   - Some point, maybe not January.
[02:47:08.560 --> 02:47:10.640]   Maybe that's my optimism, but Apple, I will buy it.
[02:47:10.640 --> 02:47:12.640]   I don't care if it's expensive and does nothing.
[02:47:12.640 --> 02:47:13.480]   I will buy it.
[02:47:13.480 --> 02:47:14.800]   I will support this future endeavor.
[02:47:14.800 --> 02:47:16.160]   - You're the meme.
[02:47:16.160 --> 02:47:18.880]   Oh, yes, I support competition.
[02:47:18.880 --> 02:47:21.680]   It seemed like Quest was the only people doing it,
[02:47:21.680 --> 02:47:23.480]   and this is great that they're like,
[02:47:24.360 --> 02:47:26.000]   you know what, and this is another place
[02:47:26.000 --> 02:47:28.640]   we'll give some more respect to Mark Zuckerberg.
[02:47:28.640 --> 02:47:30.760]   The two companies that have endured
[02:47:30.760 --> 02:47:33.360]   through technology are Apple and Microsoft.
[02:47:33.360 --> 02:47:34.760]   And what do they make?
[02:47:34.760 --> 02:47:37.120]   Computers and business services.
[02:47:37.120 --> 02:47:40.640]   All the memes, social ads, they all come and go.
[02:47:40.640 --> 02:47:44.960]   But you want to endure, build hardware.
[02:47:44.960 --> 02:47:49.280]   - Yeah, and that does a really interesting job.
[02:47:49.280 --> 02:47:53.160]   Maybe I'm a noob at this, but it's a $500 headset
[02:47:54.000 --> 02:47:59.000]   Quest 3, and just having creatures run around the space,
[02:47:59.000 --> 02:48:01.600]   like our space right here, to me, okay,
[02:48:01.600 --> 02:48:04.120]   this is very like boomer statement,
[02:48:04.120 --> 02:48:07.840]   but it added windows to the place.
[02:48:07.840 --> 02:48:10.480]   - I heard about the aquarium, yeah.
[02:48:10.480 --> 02:48:11.880]   - Yeah, aquarium, but in this case,
[02:48:11.880 --> 02:48:13.840]   it was a zombie game, whatever, it doesn't matter.
[02:48:13.840 --> 02:48:18.680]   But just like, it modifies the space in a way where I can't,
[02:48:18.680 --> 02:48:22.600]   it really feels like a window and you can look out.
[02:48:22.600 --> 02:48:24.120]   It's pretty cool, like I was just,
[02:48:24.120 --> 02:48:26.600]   it's like a zombie game, they're running at me, whatever.
[02:48:26.600 --> 02:48:27.920]   But what I was enjoying is the fact
[02:48:27.920 --> 02:48:29.360]   that there's like a window,
[02:48:29.360 --> 02:48:32.180]   and they're stepping on objects in this space.
[02:48:32.180 --> 02:48:35.200]   That was a different kind of escape.
[02:48:35.200 --> 02:48:37.560]   Also because you can see the other humans,
[02:48:37.560 --> 02:48:39.080]   so it's integrated with the other humans.
[02:48:39.080 --> 02:48:40.560]   It's really, really interesting.
[02:48:40.560 --> 02:48:42.560]   - And that's why it's more important than ever
[02:48:42.560 --> 02:48:44.400]   that the AI is running on those systems
[02:48:44.400 --> 02:48:46.160]   are aligned with you.
[02:48:46.160 --> 02:48:47.000]   - Oh yeah.
[02:48:47.000 --> 02:48:48.600]   - They're gonna augment your entire world.
[02:48:48.600 --> 02:48:49.880]   - Oh yeah.
[02:48:49.880 --> 02:48:52.600]   - And that, those AIs have a,
[02:48:52.600 --> 02:48:55.100]   I mean, you think about all the dark stuff,
[02:48:55.100 --> 02:48:58.240]   like sexual stuff.
[02:48:58.240 --> 02:49:02.340]   Like if those AIs threaten me, that could be haunting.
[02:49:02.340 --> 02:49:07.040]   Like if they like threaten me in a non-video game way,
[02:49:07.040 --> 02:49:11.360]   it's like, like they'll know personal information about me.
[02:49:11.360 --> 02:49:12.960]   And it's like, and then you lose track
[02:49:12.960 --> 02:49:14.160]   of what's real, what's not.
[02:49:14.160 --> 02:49:15.600]   Like what if stuff is like hacked.
[02:49:15.600 --> 02:49:16.480]   - There's two directions
[02:49:16.480 --> 02:49:18.600]   the AI girlfriend company can take, right?
[02:49:18.600 --> 02:49:20.840]   There's like the highbrow, something like her,
[02:49:20.840 --> 02:49:22.120]   maybe something you kind of talk to.
[02:49:22.120 --> 02:49:24.200]   And this is, and then there's the lowbrow version of it
[02:49:24.200 --> 02:49:26.200]   where I want to set up a brothel in Times Square.
[02:49:26.200 --> 02:49:27.440]   - Yeah.
[02:49:27.440 --> 02:49:28.280]   - Yeah.
[02:49:28.280 --> 02:49:29.600]   It's not cheating if it's a robot.
[02:49:29.600 --> 02:49:31.080]   It's a VR experience.
[02:49:31.080 --> 02:49:32.880]   - Is there an in between?
[02:49:32.880 --> 02:49:35.280]   - No, I don't wanna do that one or that one.
[02:49:35.280 --> 02:49:36.200]   - Have you decided yet?
[02:49:36.200 --> 02:49:37.160]   - No, I'll figure it out.
[02:49:37.160 --> 02:49:39.400]   We'll see what the technology goes.
[02:49:39.400 --> 02:49:41.040]   - I would love to hear your opinions
[02:49:41.040 --> 02:49:43.680]   for George's third company,
[02:49:43.680 --> 02:49:46.360]   what to do, the brothel in Times Square
[02:49:46.360 --> 02:49:48.840]   or the her experience.
[02:49:48.840 --> 02:49:53.320]   What do you think company number four will be?
[02:49:53.320 --> 02:49:54.440]   You think there'll be a company number four?
[02:49:54.440 --> 02:49:56.080]   - There's a lot to do in company number two.
[02:49:56.080 --> 02:49:57.880]   I'm just like, I'm talking about company number three now.
[02:49:57.880 --> 02:49:59.760]   Did none of that tech exists yet.
[02:49:59.760 --> 02:50:01.680]   There's a lot to do in company number two.
[02:50:01.680 --> 02:50:04.120]   Company number two is going to be the great struggle
[02:50:04.120 --> 02:50:05.320]   of the next six years.
[02:50:05.320 --> 02:50:06.640]   And if the next six years,
[02:50:06.640 --> 02:50:09.000]   how centralized is compute going to be?
[02:50:09.000 --> 02:50:10.840]   The less centralized compute is going to be,
[02:50:10.840 --> 02:50:12.280]   the better of a chance we all have.
[02:50:12.280 --> 02:50:13.520]   - So you're a bearing,
[02:50:13.520 --> 02:50:17.080]   you're like a flag bearer for open source distributed
[02:50:17.080 --> 02:50:19.440]   decentralization of compute.
[02:50:19.440 --> 02:50:20.640]   - We have to, we have to,
[02:50:20.640 --> 02:50:22.440]   or they will just completely dominate us.
[02:50:22.440 --> 02:50:26.280]   I showed a picture on stream of a man in a chicken farm.
[02:50:26.280 --> 02:50:28.440]   You ever seen one of those like factory farm chicken farms?
[02:50:28.440 --> 02:50:30.320]   Why does he dominate all the chickens?
[02:50:30.320 --> 02:50:33.600]   Why does he- - Smarter.
[02:50:33.600 --> 02:50:35.040]   - He's smarter, right?
[02:50:35.040 --> 02:50:36.400]   Some people on Twitch were like,
[02:50:36.400 --> 02:50:37.680]   he's bigger than the chickens.
[02:50:37.680 --> 02:50:41.440]   Yeah, and now here's a man in a cow farm, right?
[02:50:41.440 --> 02:50:42.880]   So it has nothing to do with their size
[02:50:42.880 --> 02:50:44.680]   and everything to do with their intelligence.
[02:50:44.680 --> 02:50:48.920]   And if one central organization has all the intelligence,
[02:50:48.920 --> 02:50:52.200]   you'll be the chickens and they'll be the chicken man.
[02:50:52.200 --> 02:50:54.440]   But if we all have the intelligence,
[02:50:54.440 --> 02:50:55.680]   we're all the chickens.
[02:50:55.680 --> 02:50:59.960]   We're not all the man, we're all the chickens.
[02:50:59.960 --> 02:51:01.440]   And there's no chicken man.
[02:51:01.440 --> 02:51:03.200]   - There's no chicken man.
[02:51:03.200 --> 02:51:04.920]   We're just chickens in Miami.
[02:51:04.920 --> 02:51:07.200]   - He was having a good life, man.
[02:51:07.200 --> 02:51:08.800]   - I'm sure he was.
[02:51:08.800 --> 02:51:09.960]   I'm sure he was.
[02:51:09.960 --> 02:51:11.640]   What have you learned from launching
[02:51:11.640 --> 02:51:13.600]   and running ComAI and TinyCorp?
[02:51:13.600 --> 02:51:18.120]   So this starting a company from an idea and scaling it.
[02:51:18.120 --> 02:51:20.120]   And by the way, I'm all in on TinyBox.
[02:51:20.120 --> 02:51:24.400]   I guess it's pre-order only now.
[02:51:24.400 --> 02:51:25.520]   - I wanna make sure it's good.
[02:51:25.520 --> 02:51:28.160]   I wanna make sure that like the thing that I deliver
[02:51:28.160 --> 02:51:30.480]   is like not gonna be like a Quest 2,
[02:51:30.480 --> 02:51:32.360]   which you buy and use twice.
[02:51:32.360 --> 02:51:33.400]   I mean, it's better than a Quest,
[02:51:33.400 --> 02:51:36.760]   which you bought and used less than once statistically.
[02:51:36.760 --> 02:51:40.000]   - Well, if there's a beta program for TinyBox, I'm into.
[02:51:40.000 --> 02:51:40.840]   - Sounds good.
[02:51:41.600 --> 02:51:43.160]   - So I won't be the whiny,
[02:51:43.160 --> 02:51:47.960]   I'll be the tech savvy user of the TinyBox
[02:51:47.960 --> 02:51:50.600]   just to be in the early days.
[02:51:50.600 --> 02:51:53.240]   What have you learned from building these companies?
[02:51:53.240 --> 02:51:57.120]   - The longest time at ComA, I asked why,
[02:51:57.120 --> 02:52:00.280]   why did I start a company?
[02:52:00.280 --> 02:52:01.520]   Why did I do this?
[02:52:01.520 --> 02:52:07.080]   But you know, what else was I gonna do?
[02:52:08.760 --> 02:52:13.600]   - So you like, you like bringing ideas to life.
[02:52:13.600 --> 02:52:19.640]   - With ComA, it really started as an ego battle with Elon.
[02:52:19.640 --> 02:52:22.080]   I wanted to beat him.
[02:52:22.080 --> 02:52:24.320]   Like I saw a worthy adversary, you know,
[02:52:24.320 --> 02:52:26.080]   here's a worthy adversary who I can beat
[02:52:26.080 --> 02:52:27.480]   at self-driving cars.
[02:52:27.480 --> 02:52:29.320]   And like, I think we've kept pace
[02:52:29.320 --> 02:52:30.720]   and I think he's kept ahead.
[02:52:30.720 --> 02:52:32.800]   I think that's what's ended up happening there.
[02:52:32.800 --> 02:52:36.200]   But I do think ComA is, I mean, ComA's profitable.
[02:52:38.200 --> 02:52:40.560]   And like when this drive GPT stuff starts working,
[02:52:40.560 --> 02:52:42.760]   that's it, there's no more like bugs in the loss function.
[02:52:42.760 --> 02:52:45.080]   Like right now we're using like a hand-coded simulator.
[02:52:45.080 --> 02:52:45.920]   There's no more bugs.
[02:52:45.920 --> 02:52:46.760]   This is gonna be it.
[02:52:46.760 --> 02:52:48.560]   Like this is the run-up to driving.
[02:52:48.560 --> 02:52:52.120]   - I hear a lot of really, a lot of props
[02:52:52.120 --> 02:52:53.560]   for Compile for ComA.
[02:52:53.560 --> 02:52:56.600]   - It's so, it's better than FSD and Autopilot
[02:52:56.600 --> 02:52:57.440]   in certain ways.
[02:52:57.440 --> 02:53:00.160]   It has a lot more to do with which feel you like.
[02:53:00.160 --> 02:53:02.800]   We lowered the price on the hardware to 1499.
[02:53:02.800 --> 02:53:06.040]   You know how hard it is to ship reliable consumer electronics
[02:53:06.040 --> 02:53:07.440]   that go on your windshield?
[02:53:07.440 --> 02:53:11.520]   We're doing more than like most cell phone companies.
[02:53:11.520 --> 02:53:12.680]   - How'd you pull that off, by the way,
[02:53:12.680 --> 02:53:14.680]   shipping a product that goes in a car?
[02:53:14.680 --> 02:53:15.760]   - I know.
[02:53:15.760 --> 02:53:17.920]   I have an SMT line.
[02:53:17.920 --> 02:53:20.960]   I make all the boards in-house in San Diego.
[02:53:20.960 --> 02:53:21.960]   - Quality control.
[02:53:21.960 --> 02:53:24.000]   - I care immensely about it.
[02:53:24.000 --> 02:53:29.000]   - You're basically a mom and pop shop with great testing.
[02:53:29.000 --> 02:53:32.840]   - Our head of OpenPilot is great at like, you know,
[02:53:32.840 --> 02:53:35.400]   okay, I want all the ComA 3s to be identical.
[02:53:35.400 --> 02:53:36.480]   - Yeah.
[02:53:36.480 --> 02:53:39.600]   - And yeah, I mean, you know, it's, look, it's 1499.
[02:53:39.600 --> 02:53:42.320]   It, 30 day money back guarantee.
[02:53:42.320 --> 02:53:45.400]   It will, it will blow your mind at what it can do.
[02:53:45.400 --> 02:53:46.520]   - Is it hard to scale?
[02:53:46.520 --> 02:53:48.800]   - You know what?
[02:53:48.800 --> 02:53:50.160]   There's kind of downsides to scaling it.
[02:53:50.160 --> 02:53:52.480]   People are always like, why don't you advertise?
[02:53:52.480 --> 02:53:54.040]   Our mission is to solve self-driving cars
[02:53:54.040 --> 02:53:55.880]   while delivering shipable intermediaries.
[02:53:55.880 --> 02:53:59.000]   Our mission has nothing to do with selling a million boxes.
[02:53:59.000 --> 02:53:59.840]   It's tawdry.
[02:53:59.840 --> 02:54:04.080]   - Do you think it's possible that ComA gets sold?
[02:54:05.840 --> 02:54:09.560]   - Only if I felt someone could accelerate that mission
[02:54:09.560 --> 02:54:11.760]   and wanted to keep it open source.
[02:54:11.760 --> 02:54:13.280]   And like, not just wanted to,
[02:54:13.280 --> 02:54:14.960]   I don't believe what anyone says.
[02:54:14.960 --> 02:54:16.840]   I believe incentives.
[02:54:16.840 --> 02:54:19.800]   If a company wanted to buy ComA with their incentives
[02:54:19.800 --> 02:54:20.640]   or to keep it open source,
[02:54:20.640 --> 02:54:23.320]   but ComA doesn't stop at the cars.
[02:54:23.320 --> 02:54:24.840]   The cars are just the beginning.
[02:54:24.840 --> 02:54:26.400]   The device is a human head.
[02:54:26.400 --> 02:54:28.400]   The device has two eyes, two ears.
[02:54:28.400 --> 02:54:30.600]   It breathes air, has a mouth.
[02:54:30.600 --> 02:54:33.200]   - So you think this goes to embodied robotics?
[02:54:33.200 --> 02:54:35.280]   - We have, we sell ComA bodies too.
[02:54:35.280 --> 02:54:37.520]   You know, they're very rudimentary.
[02:54:37.520 --> 02:54:42.760]   But one of the problems that we're running into
[02:54:42.760 --> 02:54:46.200]   is that the ComA 3 has about as much intelligence as a bee.
[02:54:46.200 --> 02:54:50.240]   If you want a human's worth of intelligence,
[02:54:50.240 --> 02:54:52.520]   you're gonna need a tiny rack, not even a tiny box.
[02:54:52.520 --> 02:54:55.400]   You're gonna need like a tiny rack, maybe even more.
[02:54:55.400 --> 02:54:58.200]   - How does that, how do you put legs on that?
[02:54:58.200 --> 02:55:00.280]   - You don't, and there's no way you can.
[02:55:00.280 --> 02:55:02.560]   You connect to it wirelessly.
[02:55:02.560 --> 02:55:06.080]   So you put your tiny box or your tiny rack in your house,
[02:55:06.080 --> 02:55:07.720]   and then you get your ComA body,
[02:55:07.720 --> 02:55:10.240]   and your ComA body runs the models on that.
[02:55:10.240 --> 02:55:11.600]   It's close, right?
[02:55:11.600 --> 02:55:12.720]   You don't have to go to some cloud,
[02:55:12.720 --> 02:55:14.840]   which is 30 milliseconds away.
[02:55:14.840 --> 02:55:18.160]   You go to a thing which is 0.1 milliseconds away.
[02:55:18.160 --> 02:55:21.960]   - So the AI girlfriend will have like a central hub
[02:55:21.960 --> 02:55:23.200]   in the home.
[02:55:23.200 --> 02:55:26.640]   - I mean, eventually, if you fast forward 20, 30 years,
[02:55:26.640 --> 02:55:29.360]   the mobile chips will get good enough to run these AIs.
[02:55:29.360 --> 02:55:31.160]   But fundamentally, it's not even a question
[02:55:31.160 --> 02:55:33.800]   of putting legs on a tiny box,
[02:55:33.800 --> 02:55:36.160]   because how are you getting 1.5 kilowatts of power
[02:55:36.160 --> 02:55:37.920]   on that thing, right?
[02:55:37.920 --> 02:55:41.640]   So you need, they're very synergistic businesses.
[02:55:41.640 --> 02:55:44.400]   I also wanna build all of ComA's training computers.
[02:55:44.400 --> 02:55:46.720]   ComA builds training computers right now.
[02:55:46.720 --> 02:55:48.920]   We use commodity parts.
[02:55:48.920 --> 02:55:50.720]   I think I can do it cheaper.
[02:55:50.720 --> 02:55:53.760]   So we're gonna build, TinyCorp is gonna not just sell
[02:55:53.760 --> 02:55:55.840]   tiny boxes, tiny boxes are the consumer version,
[02:55:55.840 --> 02:55:57.680]   but I'll build training data centers too.
[02:55:57.680 --> 02:56:00.120]   - Have you talked to Andrei Gropov,
[02:56:00.120 --> 02:56:01.600]   have you talked to Elon about TinyCorp?
[02:56:01.600 --> 02:56:03.400]   - He went to work at OpenAI.
[02:56:03.400 --> 02:56:05.880]   - What do you love about Andrei Kapathye?
[02:56:05.880 --> 02:56:09.920]   To me, he's one of the truly special humans we got.
[02:56:09.920 --> 02:56:13.680]   - Oh man, like, you know, his streams are just a level
[02:56:13.680 --> 02:56:16.440]   of quality so far beyond mine.
[02:56:16.440 --> 02:56:19.160]   Like I can't help myself, like it's just, you know.
[02:56:19.160 --> 02:56:20.120]   - Yeah, he's good.
[02:56:20.120 --> 02:56:23.320]   - He wants to teach you.
[02:56:23.320 --> 02:56:26.000]   I want to show you that I'm smarter than you.
[02:56:26.000 --> 02:56:30.080]   - Yeah, he has no, I mean, thank you for the sort of,
[02:56:30.080 --> 02:56:32.560]   the raw, authentic honesty.
[02:56:32.560 --> 02:56:34.840]   I mean, a lot of us have that.
[02:56:34.840 --> 02:56:37.360]   I think Andrei is as legit as it gets in that
[02:56:37.360 --> 02:56:40.160]   he just wants to teach you and there's a curiosity
[02:56:40.160 --> 02:56:41.560]   that just drives him.
[02:56:41.560 --> 02:56:45.520]   And just like at his, at the stage where he is in life,
[02:56:45.520 --> 02:56:50.240]   to be still like one of the best tinkerers in the world.
[02:56:50.240 --> 02:56:51.280]   It's crazy.
[02:56:51.280 --> 02:56:54.320]   Like to, what is it, Michael Grad?
[02:56:54.320 --> 02:56:56.720]   - Michael Grad was the inspiration for TinyGrad.
[02:56:58.600 --> 02:57:02.040]   - And that whole, I mean, his CS231N was,
[02:57:02.040 --> 02:57:03.520]   this was the inspiration.
[02:57:03.520 --> 02:57:04.880]   This is what I just took and ran with
[02:57:04.880 --> 02:57:06.840]   and ended up writing this, so, you know.
[02:57:06.840 --> 02:57:08.280]   - But I mean, to me that--
[02:57:08.280 --> 02:57:10.520]   - Don't go work for Darth Vader, man.
[02:57:10.520 --> 02:57:13.440]   - I mean, the flip side to me is that the fact
[02:57:13.440 --> 02:57:18.200]   that he's going there is a good sign for open AI.
[02:57:18.200 --> 02:57:22.480]   I think, you know, I like Ilias and Skever a lot.
[02:57:22.480 --> 02:57:25.720]   I like those, those guys are really good at what they do.
[02:57:25.720 --> 02:57:26.720]   - I know they are.
[02:57:26.720 --> 02:57:28.840]   - And that's kind of what's even like more,
[02:57:28.840 --> 02:57:29.760]   and you know what?
[02:57:29.760 --> 02:57:32.080]   It's not that open AI doesn't open source
[02:57:32.080 --> 02:57:33.480]   the weights of GPT-4.
[02:57:33.480 --> 02:57:36.860]   It's that they go in front of Congress.
[02:57:36.860 --> 02:57:39.200]   And that is what upsets me.
[02:57:39.200 --> 02:57:41.080]   You know, we had two effective altruists,
[02:57:41.080 --> 02:57:42.560]   Sams, go in front of Congress.
[02:57:42.560 --> 02:57:43.400]   One's in jail.
[02:57:43.400 --> 02:57:47.720]   - I think you're drawing parallels on that.
[02:57:47.720 --> 02:57:48.680]   - One's in jail.
[02:57:48.680 --> 02:57:49.980]   - You're giving me a look.
[02:57:49.980 --> 02:57:51.240]   (laughing)
[02:57:51.240 --> 02:57:52.080]   You're giving me a look.
[02:57:52.080 --> 02:57:53.280]   - No, I think effective altruism
[02:57:53.280 --> 02:57:55.600]   is a terribly evil ideology and, yeah.
[02:57:55.600 --> 02:57:56.440]   - Oh yeah, that's interesting.
[02:57:56.440 --> 02:57:57.280]   Why do you think that is?
[02:57:57.280 --> 02:58:00.120]   Why do you think there's something about
[02:58:00.120 --> 02:58:01.620]   a thing that sounds pretty good
[02:58:01.620 --> 02:58:04.160]   that kind of gets us into trouble?
[02:58:04.160 --> 02:58:06.200]   - Because you get Sam Bangman-Fried.
[02:58:06.200 --> 02:58:07.860]   Like Sam Bangman-Fried is the embodiment
[02:58:07.860 --> 02:58:09.420]   of effective altruism.
[02:58:09.420 --> 02:58:13.720]   Utilitarianism is an abhorrent ideology.
[02:58:13.720 --> 02:58:16.040]   Like, well yeah, we're gonna kill those three people
[02:58:16.040 --> 02:58:17.720]   to save a thousand, of course.
[02:58:17.720 --> 02:58:18.560]   - Yeah.
[02:58:18.560 --> 02:58:21.560]   - Right, there's no underlying, like there's just, yeah.
[02:58:21.560 --> 02:58:25.700]   - Yeah, but to me that's a bit surprising.
[02:58:26.540 --> 02:58:30.700]   But it's also, in retrospect, not that surprising.
[02:58:30.700 --> 02:58:33.260]   But I haven't heard really clear kind of,
[02:58:33.260 --> 02:58:40.540]   like rigorous analysis why effective altruism is flawed.
[02:58:40.540 --> 02:58:42.540]   - Oh, well, I think charity is bad, right?
[02:58:42.540 --> 02:58:43.820]   So what is charity but investment
[02:58:43.820 --> 02:58:46.260]   that you don't expect to have a return on, right?
[02:58:46.260 --> 02:58:50.300]   - Yeah, but you can also think of charity as like,
[02:58:50.300 --> 02:58:53.260]   as you would like to see,
[02:58:54.380 --> 02:58:56.620]   to allocate resources in an optimal way
[02:58:56.620 --> 02:58:59.980]   to make a better world.
[02:58:59.980 --> 02:59:01.700]   - And probably almost always
[02:59:01.700 --> 02:59:03.240]   that involves starting a company.
[02:59:03.240 --> 02:59:04.080]   - Yeah.
[02:59:04.080 --> 02:59:04.940]   - Right, because-- - More efficient.
[02:59:04.940 --> 02:59:06.420]   - Yeah, if you just take the money
[02:59:06.420 --> 02:59:10.020]   and you spend it on malaria nets, you know, okay, great.
[02:59:10.020 --> 02:59:13.060]   You've made 100 malaria nets, but if you teach--
[02:59:13.060 --> 02:59:14.580]   - Yeah, no matter how efficient.
[02:59:14.580 --> 02:59:15.420]   - Right, yeah.
[02:59:15.420 --> 02:59:17.740]   - No, but the problem is teaching no matter how efficient
[02:59:17.740 --> 02:59:19.620]   might be harder, starting a company might be harder
[02:59:19.620 --> 02:59:22.420]   than allocating money that you already have.
[02:59:22.420 --> 02:59:24.220]   - I like the flip side of effective altruism,
[02:59:24.220 --> 02:59:25.940]   effective accelerationism.
[02:59:25.940 --> 02:59:27.660]   I think accelerationism is the only thing
[02:59:27.660 --> 02:59:30.060]   that's ever lifted people out of poverty.
[02:59:30.060 --> 02:59:32.260]   The fact that food is cheap,
[02:59:32.260 --> 02:59:35.840]   not we're giving food away because we are kindhearted people.
[02:59:35.840 --> 02:59:37.980]   No, food is cheap.
[02:59:37.980 --> 02:59:40.340]   And that's the world you wanna live in.
[02:59:40.340 --> 02:59:42.340]   UBI, what a scary idea.
[02:59:42.340 --> 02:59:46.540]   What a scary idea, all your power now,
[02:59:46.540 --> 02:59:49.220]   your money is power, your only source of power
[02:59:49.220 --> 02:59:52.560]   is granted to you by the goodwill of the government.
[02:59:52.560 --> 02:59:54.100]   What a scary idea.
[02:59:54.100 --> 02:59:57.540]   - So you even think long-term, even--
[02:59:57.540 --> 03:00:00.880]   - I'd rather die than need UBI to survive, and I mean it.
[03:00:00.880 --> 03:00:06.420]   - What if survival is basically guaranteed?
[03:00:06.420 --> 03:00:08.940]   What if our life becomes so good?
[03:00:08.940 --> 03:00:12.300]   - You can make survival guaranteed without UBI.
[03:00:12.300 --> 03:00:15.100]   What you have to do is make housing and food dirt cheap.
[03:00:15.100 --> 03:00:15.940]   - Sure.
[03:00:15.940 --> 03:00:17.260]   - And that's the good world.
[03:00:17.260 --> 03:00:19.060]   And actually, let's go into what we should really
[03:00:19.060 --> 03:00:20.960]   be making dirt cheap, which is energy.
[03:00:20.960 --> 03:00:21.940]   - Yeah.
[03:00:21.940 --> 03:00:25.420]   - That energy, you know, oh my God, like, you know,
[03:00:25.420 --> 03:00:29.340]   that's, if there's one, I'm pretty centrist politically,
[03:00:29.340 --> 03:00:31.940]   if there's one political position I cannot stand,
[03:00:31.940 --> 03:00:33.420]   it's deceleration.
[03:00:33.420 --> 03:00:36.060]   It's people who believe we should use less energy.
[03:00:36.060 --> 03:00:37.740]   Not people who believe global warming is a problem,
[03:00:37.740 --> 03:00:38.700]   I agree with you.
[03:00:38.700 --> 03:00:40.620]   Not people who believe that, you know,
[03:00:40.620 --> 03:00:43.780]   saving the environment is good, I agree with you.
[03:00:43.780 --> 03:00:46.100]   But people who think we should use less energy.
[03:00:46.100 --> 03:00:48.580]   That energy usage is a moral bad.
[03:00:48.580 --> 03:00:50.220]   No.
[03:00:50.220 --> 03:00:54.460]   No, you are asking, you are diminishing humanity.
[03:00:54.460 --> 03:00:56.740]   - Yeah, energy is flourishing,
[03:00:56.740 --> 03:00:59.340]   of creative flourishing of the human species.
[03:00:59.340 --> 03:01:00.500]   - How do we make more of it?
[03:01:00.500 --> 03:01:01.380]   How do we make it clean?
[03:01:01.380 --> 03:01:03.220]   And how do we make, just, just, just,
[03:01:03.220 --> 03:01:06.780]   how do I pay, you know, 20 cents for a megawatt hour
[03:01:06.780 --> 03:01:08.320]   instead of a kilowatt hour?
[03:01:08.320 --> 03:01:13.060]   - Part of me wishes that Elon went into nuclear fusion
[03:01:13.060 --> 03:01:16.180]   versus Twitter, part of me.
[03:01:16.180 --> 03:01:18.260]   Or somebody, somebody like Elon.
[03:01:18.260 --> 03:01:21.180]   You know, we need to, I wish there were more,
[03:01:21.180 --> 03:01:23.740]   more Elons in the world.
[03:01:23.740 --> 03:01:25.940]   I think Elon sees it as like,
[03:01:25.940 --> 03:01:28.460]   this is a political battle that needed to be fought.
[03:01:28.460 --> 03:01:30.860]   And again, like, you know, I always ask the question
[03:01:30.860 --> 03:01:32.020]   of whenever I disagree with him,
[03:01:32.020 --> 03:01:35.060]   I remind myself that he's a billionaire and I'm not.
[03:01:35.060 --> 03:01:37.260]   So, you know, maybe he's got something figured out
[03:01:37.260 --> 03:01:38.940]   that I don't, or maybe he doesn't.
[03:01:38.940 --> 03:01:42.380]   - To have some humility, but at the same time,
[03:01:42.380 --> 03:01:45.140]   me as a person who happens to know him,
[03:01:45.140 --> 03:01:46.900]   I find myself in that same position.
[03:01:46.900 --> 03:01:51.300]   Sometimes even billionaires need friends
[03:01:51.300 --> 03:01:53.180]   who disagree and help them grow.
[03:01:53.180 --> 03:01:57.100]   And that's a difficult, that's a difficult reality.
[03:01:57.100 --> 03:02:00.020]   - And it must be so hard, it must be so hard to meet people
[03:02:00.020 --> 03:02:01.980]   once you get to that point where.
[03:02:01.980 --> 03:02:05.580]   - Fame, power, money, everybody's sucking up to you.
[03:02:05.580 --> 03:02:08.180]   - See, I love not having shit, like I don't have shit, man.
[03:02:08.180 --> 03:02:11.020]   You know, like, trust me, there's nothing I can give you.
[03:02:11.020 --> 03:02:13.740]   There's nothing worth taking from me, you know?
[03:02:13.740 --> 03:02:16.060]   - Yeah, it takes a really special human being
[03:02:16.060 --> 03:02:17.860]   when you have power, when you have fame,
[03:02:17.860 --> 03:02:21.420]   when you have money, to still think from first principles.
[03:02:21.420 --> 03:02:23.460]   Not like all the adoration you get towards you,
[03:02:23.460 --> 03:02:26.540]   all the admiration, all the people saying yes, yes, yes.
[03:02:26.540 --> 03:02:27.780]   - And all the hate, too.
[03:02:27.780 --> 03:02:28.620]   - And the hate.
[03:02:28.620 --> 03:02:29.460]   - Fame gets worse.
[03:02:29.460 --> 03:02:33.300]   - So the hate makes you want to go to the yes people
[03:02:33.300 --> 03:02:35.740]   because the hate exhausts you.
[03:02:35.740 --> 03:02:38.380]   And the kind of hate that Elon's gotten from the left
[03:02:38.380 --> 03:02:39.900]   is pretty intense.
[03:02:39.900 --> 03:02:41.940]   And so that, of course, drives him right.
[03:02:44.820 --> 03:02:46.060]   And loses balance.
[03:02:46.060 --> 03:02:49.860]   - And it keeps this absolutely fake, like,
[03:02:49.860 --> 03:02:54.860]   psy-op political divide alive so that the 1% can keep power.
[03:02:54.860 --> 03:02:59.420]   - I wish we'd be less divided 'cause it is giving power.
[03:02:59.420 --> 03:03:00.260]   - It gives power.
[03:03:00.260 --> 03:03:01.100]   - To the ultra-powerful.
[03:03:01.100 --> 03:03:03.460]   The rich get richer.
[03:03:03.460 --> 03:03:06.100]   You have love in your life.
[03:03:06.100 --> 03:03:09.940]   Has love made you a better or a worse programmer?
[03:03:09.940 --> 03:03:13.700]   Do you keep productivity metrics?
[03:03:13.700 --> 03:03:14.740]   - No, no.
[03:03:14.740 --> 03:03:15.660]   (Lex laughs)
[03:03:15.660 --> 03:03:18.020]   No, I'm not that methodical.
[03:03:18.020 --> 03:03:22.020]   I think that there comes to a point where
[03:03:22.020 --> 03:03:24.860]   if it's no longer visceral, I just can't enjoy it.
[03:03:24.860 --> 03:03:28.700]   I still viscerally love programming.
[03:03:28.700 --> 03:03:29.940]   The minute I started like--
[03:03:29.940 --> 03:03:33.020]   - So that's one of the big loves of your life is programming.
[03:03:33.020 --> 03:03:34.940]   - I mean, just my computer in general.
[03:03:34.940 --> 03:03:37.620]   I mean, I tell my girlfriend my first love
[03:03:37.620 --> 03:03:39.060]   is my computer, of course.
[03:03:39.060 --> 03:03:42.180]   I sleep with my computer.
[03:03:42.180 --> 03:03:44.420]   It's there for a lot of my sexual experiences.
[03:03:44.420 --> 03:03:46.740]   Like, come on, so is everyone's, right?
[03:03:46.740 --> 03:03:48.940]   Like, you know, you gotta be real about that.
[03:03:48.940 --> 03:03:50.900]   - Not just like the IDE for programming,
[03:03:50.900 --> 03:03:53.300]   just the entirety of the computational machine.
[03:03:53.300 --> 03:03:54.620]   - The fact that, yeah, I mean, it's, you know,
[03:03:54.620 --> 03:03:57.580]   I wish it was, and someday it'll be smarter.
[03:03:57.580 --> 03:04:01.180]   Maybe I'm weird for this, but I don't discriminate, man.
[03:04:01.180 --> 03:04:02.860]   I'm not gonna discriminate biostack life
[03:04:02.860 --> 03:04:04.460]   and silicon stack life.
[03:04:04.460 --> 03:04:07.420]   - So the moment the computer starts to say, like,
[03:04:07.420 --> 03:04:10.060]   I miss you, it starts to have some of the basics
[03:04:10.060 --> 03:04:14.260]   of human intimacy, it's over for you.
[03:04:14.260 --> 03:04:16.820]   The moment VS Code says, hey, George,
[03:04:16.820 --> 03:04:17.900]   I can answer-- - No, you see, no, no, no,
[03:04:17.900 --> 03:04:20.860]   but VS Code is, no, they're just doing that.
[03:04:20.860 --> 03:04:22.860]   Microsoft's doing that to try to get me hooked on it.
[03:04:22.860 --> 03:04:24.020]   I'll see through it.
[03:04:24.020 --> 03:04:24.860]   I'll see through it.
[03:04:24.860 --> 03:04:25.980]   It's gold digger, man, it's gold digger.
[03:04:25.980 --> 03:04:27.420]   - Look at me in open source.
[03:04:27.420 --> 03:04:29.220]   - Well, this just gets more interesting, right?
[03:04:29.220 --> 03:04:31.820]   If it's open source, and yeah, it becomes--
[03:04:31.820 --> 03:04:33.620]   - Though Microsoft's done a pretty good job on that.
[03:04:33.620 --> 03:04:34.460]   - Oh, absolutely.
[03:04:34.460 --> 03:04:36.820]   No, no, no, look, I think Microsoft, again,
[03:04:36.820 --> 03:04:38.700]   I wouldn't count on it to be true forever,
[03:04:38.700 --> 03:04:41.420]   but I think right now, Microsoft is doing the best work
[03:04:41.420 --> 03:04:44.980]   in the programming world, like, between, yeah, GitHub,
[03:04:44.980 --> 03:04:48.620]   GitHub Actions, VS Code, the improvements to Python,
[03:04:48.620 --> 03:04:50.900]   it works with Microsoft, like, this is--
[03:04:50.900 --> 03:04:54.780]   - Who would have thought Microsoft and Mark Zuckerberg
[03:04:54.780 --> 03:04:57.060]   are spearheading the open source movement?
[03:04:57.060 --> 03:04:58.540]   - Right, right?
[03:04:58.540 --> 03:05:01.380]   How things change.
[03:05:01.380 --> 03:05:03.500]   - Oh, it's beautiful.
[03:05:03.500 --> 03:05:04.580]   - And by the way, that's who I'd bet on
[03:05:04.580 --> 03:05:06.060]   to replace Google, by the way.
[03:05:06.060 --> 03:05:07.780]   - Who? - Microsoft.
[03:05:07.780 --> 03:05:09.260]   - Microsoft. - I think Satya Nadella
[03:05:09.260 --> 03:05:11.420]   said straight up, "I'm coming for it."
[03:05:11.420 --> 03:05:15.620]   - Interesting, so you bet, who wins AGI?
[03:05:15.620 --> 03:05:16.740]   - Oh, I don't know about AGI.
[03:05:16.740 --> 03:05:17.820]   I think we're a long way away from that,
[03:05:17.820 --> 03:05:21.020]   but I would not be surprised if in the next five years,
[03:05:21.020 --> 03:05:23.060]   Bing overtakes Google as a search engine.
[03:05:23.060 --> 03:05:25.140]   - Interesting.
[03:05:25.140 --> 03:05:26.980]   - Wouldn't surprise me.
[03:05:26.980 --> 03:05:28.020]   - Interesting.
[03:05:28.020 --> 03:05:31.700]   I hope some startup does.
[03:05:31.700 --> 03:05:34.260]   - It might be some startup too.
[03:05:34.260 --> 03:05:36.980]   I would equally bet on some startup.
[03:05:36.980 --> 03:05:40.540]   - Yeah, I'm like 50/50, but maybe that's naive.
[03:05:40.540 --> 03:05:43.540]   I believe in the power of these language models.
[03:05:43.540 --> 03:05:45.700]   - Satya's alive, Microsoft's alive.
[03:05:45.700 --> 03:05:48.260]   - Yeah, it's great, it's great.
[03:05:48.260 --> 03:05:50.620]   I like all the innovation in these companies.
[03:05:50.620 --> 03:05:51.820]   They're not being stale.
[03:05:51.820 --> 03:05:55.260]   And to the degree they're being stale, they're losing.
[03:05:55.260 --> 03:05:58.180]   So there's a huge incentive to do a lot of exciting work
[03:05:58.180 --> 03:06:01.060]   and open source work, which is, this is incredible.
[03:06:01.060 --> 03:06:02.740]   - Only way to win.
[03:06:02.740 --> 03:06:05.620]   - You're older, you're wiser.
[03:06:05.620 --> 03:06:08.460]   What's the meaning of life, George Hotz?
[03:06:08.460 --> 03:06:09.660]   - To win.
[03:06:09.660 --> 03:06:10.820]   - It's still to win.
[03:06:10.820 --> 03:06:12.300]   - Of course.
[03:06:12.300 --> 03:06:13.380]   - Always.
[03:06:13.380 --> 03:06:14.580]   - Of course.
[03:06:14.580 --> 03:06:16.500]   - What's winning look like for you?
[03:06:16.500 --> 03:06:18.500]   - I don't know, I haven't figured out what the game is yet,
[03:06:18.500 --> 03:06:19.860]   but when I do, I wanna win.
[03:06:19.860 --> 03:06:21.580]   - So it's bigger than solving self-driving?
[03:06:21.580 --> 03:06:26.580]   It's bigger than democratizing, decentralizing compute?
[03:06:26.580 --> 03:06:31.680]   - I think the game is to stand eye to eye with God.
[03:06:33.900 --> 03:06:36.100]   - I wonder what that means for you.
[03:06:36.100 --> 03:06:40.160]   At the end of your life, what that will look like.
[03:06:40.160 --> 03:06:42.740]   - I mean, this is what, I don't know,
[03:06:42.740 --> 03:06:47.740]   this is some, there's probably some ego trip of mine.
[03:06:47.740 --> 03:06:50.220]   Like, you wanna stand eye to eye with God,
[03:06:50.220 --> 03:06:51.900]   you're just blasphemous, man.
[03:06:51.900 --> 03:06:52.860]   You okay?
[03:06:52.860 --> 03:06:54.060]   I don't know, I don't know.
[03:06:54.060 --> 03:06:55.860]   I don't know if it would upset God.
[03:06:55.860 --> 03:06:57.100]   I think he wants that.
[03:06:57.100 --> 03:06:59.660]   I mean, I certainly want that for my creations.
[03:06:59.660 --> 03:07:02.100]   I want my creations to stand eye to eye with me.
[03:07:03.220 --> 03:07:06.020]   So why wouldn't God want me to stand eye to eye with him?
[03:07:06.020 --> 03:07:10.000]   That's the best I can do, golden rule.
[03:07:10.000 --> 03:07:14.140]   - I'm just imagining the creator of a video game
[03:07:14.140 --> 03:07:20.520]   having to look and stand eye to eye
[03:07:20.520 --> 03:07:22.700]   with one of the characters.
[03:07:22.700 --> 03:07:24.620]   - I only watched season one of "Westworld,"
[03:07:24.620 --> 03:07:26.920]   but yeah, we gotta find the maze and solve it.
[03:07:26.920 --> 03:07:30.260]   - Yeah, I wonder what that looks like.
[03:07:30.260 --> 03:07:33.380]   It feels like a really special time in human history
[03:07:33.380 --> 03:07:34.860]   where that's actually possible.
[03:07:34.860 --> 03:07:37.300]   Like, there's something about AI that's like,
[03:07:37.300 --> 03:07:39.980]   we're playing with something weird here,
[03:07:39.980 --> 03:07:41.780]   something really weird.
[03:07:41.780 --> 03:07:45.340]   - I wrote a blog post, I re-read "Genesis"
[03:07:45.340 --> 03:07:47.340]   and just looked like, they give you some clues
[03:07:47.340 --> 03:07:50.100]   at the end of "Genesis" for finding the Garden of Eden.
[03:07:50.100 --> 03:07:53.740]   And I'm interested, I'm interested.
[03:07:53.740 --> 03:07:57.100]   - Well, I hope you find just that, George.
[03:07:57.100 --> 03:07:58.460]   You're one of my favorite people.
[03:07:58.460 --> 03:07:59.740]   Thank you for doing everything you're doing.
[03:07:59.740 --> 03:08:02.500]   And in this case, for fighting for open source
[03:08:02.500 --> 03:08:04.620]   or for decentralization of AI,
[03:08:04.620 --> 03:08:08.400]   it's a fight worth fighting, fight worth winning hashtag.
[03:08:08.400 --> 03:08:10.260]   I love you, brother.
[03:08:10.260 --> 03:08:11.540]   These conversations are always great.
[03:08:11.540 --> 03:08:13.500]   Hope to talk to you many more times.
[03:08:13.500 --> 03:08:15.620]   Good luck with Tiny Corp.
[03:08:15.620 --> 03:08:17.820]   - Thank you, great to be here.
[03:08:17.820 --> 03:08:19.260]   - Thanks for listening to this conversation
[03:08:19.260 --> 03:08:20.460]   with George Hotz.
[03:08:20.460 --> 03:08:21.660]   To support this podcast,
[03:08:21.660 --> 03:08:24.380]   please check out our sponsors in the description.
[03:08:24.380 --> 03:08:26.180]   And now, let me leave you with some words
[03:08:26.180 --> 03:08:28.140]   from Albert Einstein.
[03:08:28.140 --> 03:08:31.100]   "Everything should be made as simple as possible,
[03:08:31.100 --> 03:08:32.860]   but not simpler."
[03:08:32.860 --> 03:08:37.020]   Thank you for listening and hope to see you next time.
[03:08:37.020 --> 03:08:39.600]   (upbeat music)
[03:08:39.600 --> 03:08:42.180]   (upbeat music)
[03:08:42.180 --> 03:08:52.180]   [BLANK_AUDIO]

