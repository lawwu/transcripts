
[00:00:00.000 --> 00:00:27.320]   I'm just waiting to make sure I am live on YouTube. Sometimes
[00:00:27.320 --> 00:00:32.320]   there's a small delay with that and I have this pending paranoia
[00:00:32.320 --> 00:00:35.600]   from quite a few days that always makes me double check
[00:00:35.600 --> 00:00:39.480]   things. I can hear and double echo which means we're live on
[00:00:39.480 --> 00:00:43.480]   both platforms. That's awesome. Welcome back everybody. This is
[00:00:43.480 --> 00:00:48.600]   I think the third meeting group. Feels like it's been going on
[00:00:48.600 --> 00:00:51.000]   for a while because we've been learning so many topics. It's
[00:00:51.000 --> 00:00:53.880]   great to see everyone back. Thanks for joining us again.
[00:00:53.880 --> 00:00:56.680]   We've been doing this for quite a few weeks now and I'm excited
[00:00:56.680 --> 00:01:02.040]   to continue learning further with you all. If you're new or
[00:01:02.040 --> 00:01:06.840]   if this is your first Keras meeting group or if you're
[00:01:06.840 --> 00:01:09.560]   watching the recording for the first time, we've been reading
[00:01:09.560 --> 00:01:15.920]   this incredible book called Deep Learning with Python. We're
[00:01:15.920 --> 00:01:18.920]   halfway through it. That shouldn't stop you from joining
[00:01:18.920 --> 00:01:23.080]   us. We're still in the beginning of learning Keras. We'll
[00:01:23.080 --> 00:01:26.680]   actually just start going into the advanced topics if I made
[00:01:26.680 --> 00:01:29.120]   this week. So you're still early. You're welcome to join.
[00:01:29.120 --> 00:01:32.560]   You're welcome to ask as many questions as you want. My only
[00:01:32.560 --> 00:01:35.400]   request is ask as many questions as you want. No question is too
[00:01:35.400 --> 00:01:37.520]   stupid. All questions are welcome. If you're watching the
[00:01:37.520 --> 00:01:41.400]   recording on YouTube, leave a comment below. I always monitor
[00:01:41.400 --> 00:01:45.400]   them. I'm happy to come back and leave any answers to any
[00:01:45.400 --> 00:01:48.800]   questions that you might have. So to remind you last week, we
[00:01:48.800 --> 00:01:52.480]   learned about introduction. We got an introduction to the Keras
[00:01:52.520 --> 00:01:55.400]   API and we understood the basics of machine learning. We
[00:01:55.400 --> 00:01:58.680]   learned about deep learning. Really interesting how does it
[00:01:58.680 --> 00:02:06.440]   stand out against traditional "machine learning". And we of
[00:02:06.440 --> 00:02:08.760]   course started the series with an "Ask me anything with
[00:02:08.760 --> 00:02:12.520]   Franchois" just wanted to remind you in case you haven't
[00:02:12.520 --> 00:02:16.320]   watched that. My tongue is really, really rolling off
[00:02:16.320 --> 00:02:22.480]   today. So I need more chai. This happens during the winters to me
[00:02:22.480 --> 00:02:26.600]   a lot. It's quite cold. So here's the agenda for today. I
[00:02:26.600 --> 00:02:30.400]   want to go into a deep dive of Keras. I'm actually just
[00:02:30.400 --> 00:02:33.600]   following the book and trying to add a few things here and there.
[00:02:33.600 --> 00:02:37.280]   We'll be learning about what are convolutional neural networks,
[00:02:37.280 --> 00:02:42.800]   what makes them so cool. What is image augmentation? And if you've
[00:02:42.800 --> 00:02:47.600]   been around in the Twitter space recently, you would know that
[00:02:51.440 --> 00:02:55.640]   oops, it's gone down the list. But I wanted to point out that
[00:02:55.640 --> 00:03:01.080]   conf next is a really interesting paper that has been
[00:03:01.080 --> 00:03:03.800]   its fourth on the list. So that's not too far. It's
[00:03:03.800 --> 00:03:07.520]   received 2500 stars in like under a week. We'll be
[00:03:07.520 --> 00:03:13.240]   understanding how to implement this architecture in Keras. And
[00:03:13.240 --> 00:03:16.840]   I won't be doing that. I got a lot of help from my incredible
[00:03:16.840 --> 00:03:20.880]   colleague, Shomik. I'll be using his implementation. We actually
[00:03:20.880 --> 00:03:24.400]   read the paper together and he instantly implemented this. So
[00:03:24.400 --> 00:03:27.600]   we'll be learning how to implement this architecture or
[00:03:27.600 --> 00:03:32.160]   cutting edge architecture. As I said, this paper came out just
[00:03:32.160 --> 00:03:35.360]   12 days ago. So it's quite recent. So we're at that point
[00:03:35.360 --> 00:03:39.480]   already where we can go off, learn one of the most recent
[00:03:39.480 --> 00:03:43.120]   architectures in Keras, we can build that and we'll be I'm
[00:03:43.120 --> 00:03:46.560]   quite ambitious with the agendas, but I hope to get to
[00:03:46.560 --> 00:03:50.680]   that point today. We will also be understanding how to log
[00:03:50.720 --> 00:03:53.280]   experiments in weights and biases. We're at the point where
[00:03:53.280 --> 00:03:56.840]   we can start experimenting a lot. So I'll be showing you the
[00:03:56.840 --> 00:04:01.560]   1db callback. And if you don't know what callbacks are, I do
[00:04:01.560 --> 00:04:05.320]   plan to cover it as well. So here's the agenda for today.
[00:04:05.320 --> 00:04:09.720]   There'll be a recap 5 to 10 minutes, then we'll an ambitious
[00:04:09.720 --> 00:04:13.520]   goal, but we'll try to get up until chapter nine. So cover up
[00:04:13.520 --> 00:04:16.880]   until from chapter seven to nine. I'll try to explain image
[00:04:16.880 --> 00:04:20.680]   augmentations via the necessary via the important and then we'll
[00:04:20.680 --> 00:04:24.320]   go to the next implementation and throughout the session, you
[00:04:24.320 --> 00:04:26.680]   all must be familiar, I keep taking breaks to answer any
[00:04:26.680 --> 00:04:30.640]   questions. So please keep the questions coming. Quick recap
[00:04:30.640 --> 00:04:33.440]   from last session, we learned how is deep learning different
[00:04:33.440 --> 00:04:36.160]   from traditional learning, there are fewer knobs to turn, you
[00:04:36.160 --> 00:04:39.840]   just need to set a bunch of hyper parameters. It's lesser
[00:04:39.840 --> 00:04:42.640]   of an effort, but it's also trickier to implement. It takes
[00:04:42.640 --> 00:04:45.840]   longer to train, but it's more accurate. There's this nice
[00:04:45.840 --> 00:04:48.160]   trade off. And as a practitioner, you'll always be
[00:04:48.160 --> 00:04:52.880]   challenged to pick the right option. And I don't have a
[00:04:52.880 --> 00:04:59.080]   suggestion on how to do that. We also did a case study where I
[00:04:59.080 --> 00:05:03.480]   brought up a suggestion of how you would build an app from the
[00:05:03.480 --> 00:05:07.520]   ground up. So chapter six of this book, deep learning with
[00:05:07.520 --> 00:05:12.600]   Python talks about the steps in the lifecycle of model
[00:05:12.600 --> 00:05:16.960]   creation steps in the lifecycle of machine learning. So the case
[00:05:16.960 --> 00:05:20.360]   study was understanding, let's say if you were hired at a
[00:05:20.360 --> 00:05:25.320]   company like, I don't know, Postmates, Uber Eats, how would
[00:05:25.320 --> 00:05:29.800]   you create that up? And I had this big business idea that we,
[00:05:29.800 --> 00:05:34.160]   me and the audience discussed, you all and me discussed. So
[00:05:34.160 --> 00:05:38.080]   that was quite interesting as well, to me at least to go
[00:05:38.080 --> 00:05:40.440]   through that discussion. If you haven't watched that, I would
[00:05:40.440 --> 00:05:42.840]   encourage you to check that out just to understand what are the
[00:05:42.840 --> 00:05:46.160]   steps in building a quote unquote, real world app. And we
[00:05:46.160 --> 00:05:50.720]   also got a vanilla introduction to the Keras API. I always
[00:05:50.720 --> 00:05:54.360]   suggest homebook sometimes, some of you end up doing it,
[00:05:54.360 --> 00:05:57.520]   sometimes you don't, it always makes me happy when I see
[00:05:57.520 --> 00:06:00.520]   someone doing a suggested homebook. But this was the
[00:06:00.520 --> 00:06:07.880]   suggested list I wanted to suggest creating a MVP of the
[00:06:07.880 --> 00:06:11.640]   exercise that I just mentioned, I suggested watching a recent
[00:06:11.640 --> 00:06:15.120]   paper summary or just porting over a model from PyTorch to
[00:06:15.120 --> 00:06:19.320]   Keras. It can be of incredible learning. And I'm sure many
[00:06:19.320 --> 00:06:22.800]   people are familiar with this fact. But if you're not, I, I
[00:06:22.800 --> 00:06:26.320]   would encourage you to really try this. Because all of us are
[00:06:26.320 --> 00:06:29.720]   at that stage where you can really implement a cutting edge
[00:06:29.720 --> 00:06:36.880]   paper or latest deep learning paper from scratch in Keras. And
[00:06:36.880 --> 00:06:41.160]   I'm assuming every one of us joins this to learn to have a
[00:06:41.160 --> 00:06:44.160]   career in this field or to improve our skills. In my
[00:06:44.160 --> 00:06:47.440]   opinion, this is one of the best ways to actually improve your
[00:06:47.440 --> 00:06:51.440]   skills in deep learning. So I just wanted to remind everyone,
[00:06:51.440 --> 00:06:54.720]   hey, that's the homework, consider doing it or don't, it's
[00:06:54.720 --> 00:07:01.480]   totally up to you. So again, we've gone through chapter one
[00:07:01.480 --> 00:07:04.800]   to six, somewhere up until chapter seven, as well, we
[00:07:04.800 --> 00:07:08.640]   actually started going through it. Now we'll aim to cover
[00:07:08.640 --> 00:07:11.680]   working with Keras, introduction to deep learning for computer
[00:07:11.680 --> 00:07:14.400]   vision and advanced deep learning for computer vision.
[00:07:14.400 --> 00:07:18.120]   So again, an ambitious agenda, lots of things to cover. But
[00:07:18.120 --> 00:07:22.280]   again, my focus is to keep it high quality. So if we have more
[00:07:22.280 --> 00:07:26.160]   questions or topics to discuss, I'll focus on that first.
[00:07:26.160 --> 00:07:31.120]   Setting up the stage for why are convolutions important. First
[00:07:31.120 --> 00:07:36.280]   topic in the eighth chapter is convolution. So I'm skipping
[00:07:36.280 --> 00:07:41.040]   ahead to give some theoretical introduction. Now, here's the
[00:07:41.040 --> 00:07:48.680]   problem with fully connected neural networks. They're really
[00:07:48.680 --> 00:07:52.160]   prone to overfitting if you keep this is true to any
[00:07:52.160 --> 00:07:57.480]   architecture, but they are quite prone to overfitting. They are
[00:07:57.480 --> 00:08:03.200]   positionally dependent. So if you want to find out where am I
[00:08:03.200 --> 00:08:06.240]   in this, so if you want to create a same detector, and you
[00:08:06.240 --> 00:08:10.480]   want to point me out in this image, if this image changes a
[00:08:10.480 --> 00:08:13.960]   little bit, or if I move around in the image, the fully
[00:08:13.960 --> 00:08:16.600]   connected neural network, if you remember the weights that we
[00:08:16.600 --> 00:08:21.840]   train on, they will have a hard time reconfiguring themselves,
[00:08:21.840 --> 00:08:25.640]   because it's positionally dependent. It's not positional
[00:08:25.640 --> 00:08:30.520]   invariant. And that's what CNN solve. That's a problem that
[00:08:30.520 --> 00:08:34.560]   CNNs address their positional invariant. So the goal is to be
[00:08:34.560 --> 00:08:38.960]   able to detect say, I'm anywhere in this image. And that is what
[00:08:38.960 --> 00:08:45.720]   we'll achieve with CNNs. And of course, they have too many
[00:08:45.720 --> 00:08:48.720]   parameters, we were looking at one of the smallest data sets,
[00:08:48.720 --> 00:08:51.720]   it's almost the hello world in machine learning, I think we
[00:08:51.720 --> 00:08:55.480]   were looking at MNIST and CIFAR. And we already saw that we were
[00:08:55.480 --> 00:08:58.320]   in the range of millions of parameters. So that's that is
[00:08:58.320 --> 00:09:03.120]   quite a lot of parameters. So when we create fully connected
[00:09:03.120 --> 00:09:07.200]   neural networks, we face these problems. How do we fight that?
[00:09:07.200 --> 00:09:14.160]   How do we address those? We look at convolutions. So how do
[00:09:14.160 --> 00:09:18.440]   convolutions work? Let's go back to this paper that I had
[00:09:18.440 --> 00:09:25.720]   addressed in the first lecture. Let me drop this link. I like
[00:09:25.720 --> 00:09:28.800]   it. So there's the link if you want to check the paper out. You
[00:09:28.800 --> 00:09:31.320]   don't need to read the paper, I'll just give you the gist.
[00:09:31.320 --> 00:09:35.760]   This is a paper by Zeiler and Fergus and they showcased us
[00:09:36.080 --> 00:09:40.240]   what does a convolutional network actually learn. So I'll
[00:09:40.240 --> 00:09:43.520]   give you a second to look closely across these layers. What
[00:09:43.520 --> 00:09:46.240]   they've done is they've visualized every single layer
[00:09:46.240 --> 00:09:55.560]   inside of a neural network. So as you can see, we were trying
[00:09:55.560 --> 00:09:59.560]   to learn general world objects, right? We see if you go towards
[00:09:59.560 --> 00:10:04.720]   the last layer, we are trying to predict humans, I see a dog, I
[00:10:04.720 --> 00:10:08.760]   see some food cans, maybe is that is that a keyboard, I'm
[00:10:08.760 --> 00:10:14.440]   seeing a close up of a bicycle. So a lot of real world things,
[00:10:14.440 --> 00:10:19.400]   right? And neural networks consist of a lot of layers
[00:10:19.400 --> 00:10:24.640]   inside of the first few layers. We're learning about colors, we
[00:10:24.640 --> 00:10:28.400]   learning about the basics of shape. So we see circles, we see
[00:10:28.400 --> 00:10:32.680]   edges. Then towards the later end, we start seeing more
[00:10:32.680 --> 00:10:39.600]   complex shapes, we can see a outline of a car tire that looks
[00:10:39.600 --> 00:10:45.720]   like that to me, we can see a person. And towards the
[00:10:45.720 --> 00:10:49.240]   furthermore, and then we can understand what a dog is. So we
[00:10:49.240 --> 00:10:54.720]   see a silhouette of a dog, we can see more complex images. So
[00:10:54.720 --> 00:10:59.640]   layer by layer, these architectures understand more and
[00:10:59.640 --> 00:11:03.880]   more details. So as you go deeper inside of a CNN model,
[00:11:03.880 --> 00:11:11.040]   inside of a CNN architecture, you start learning more details
[00:11:11.040 --> 00:11:16.040]   about the object about the thing that's of interest to the model.
[00:11:16.040 --> 00:11:19.840]   So here's the intuition how they work right with every layer,
[00:11:19.840 --> 00:11:23.280]   they try to understand one layer of abstraction. And towards the
[00:11:23.280 --> 00:11:26.680]   later in the hopefully, if you've done all things right,
[00:11:27.040 --> 00:11:31.600]   and if you've made a sacrifice to the GPU gods of enough cloud
[00:11:31.600 --> 00:11:35.800]   compute credits, you might be able to just might be able to
[00:11:35.800 --> 00:11:38.840]   predict the right thing inside of the image. So that's how CNN
[00:11:38.840 --> 00:11:45.160]   spoke. Now let's look at that in theory. They perform is the
[00:11:45.160 --> 00:11:47.760]   technical definition, they perform local operations on
[00:11:47.760 --> 00:11:53.240]   neighborhoods, translation invariant. And they have a fewer,
[00:11:53.960 --> 00:11:59.200]   a lot fewer parameters. So here's an image. Quite funnily,
[00:11:59.200 --> 00:12:01.840]   this comes from the deep learning with PyTorch book.
[00:12:01.840 --> 00:12:05.960]   That's another session we hosted if you want to study from there,
[00:12:05.960 --> 00:12:11.600]   but that's where I copied this image from. So inside of every
[00:12:11.600 --> 00:12:16.520]   image, let's say we're trying to understand this bird. Every
[00:12:16.520 --> 00:12:19.600]   image, even my video, even the video you're watching right now
[00:12:19.600 --> 00:12:24.240]   has three basic colors RGB. Now, of course, if you've read if
[00:12:24.240 --> 00:12:28.240]   you've read any mobile phone or TV description, they have 16
[00:12:28.240 --> 00:12:33.440]   million suggested potential colors. That's a different
[00:12:33.440 --> 00:12:38.080]   story. But the base color behind these is red, green and blue. So
[00:12:38.080 --> 00:12:42.320]   every image would have these three colors. Now what do we do
[00:12:42.320 --> 00:12:47.360]   from there, we take all of these, sorry, we take
[00:12:47.360 --> 00:12:56.080]   convolution operators, which we call kernels. These are matrices
[00:12:56.080 --> 00:13:02.840]   of a certain width. And we perform a matrix multiplication
[00:13:02.840 --> 00:13:08.640]   with all of these colors. So now the thing to remember here is
[00:13:11.160 --> 00:13:14.600]   what do you see in this image, you see a person you see, few
[00:13:14.600 --> 00:13:22.200]   monitors, you see this computer, a PC or a PC just sees numbers,
[00:13:22.200 --> 00:13:27.160]   this is a human neural network trying to interpret things. But
[00:13:27.160 --> 00:13:31.640]   to a neural network model, a real and artificial neural
[00:13:31.640 --> 00:13:36.880]   network model, it just sees zeros and ones, or actually
[00:13:36.880 --> 00:13:42.600]   zeros up until 255s across three layers. So every image has RGB
[00:13:42.600 --> 00:13:46.720]   red, green and blue. Those values can range from zero to
[00:13:46.720 --> 00:13:52.880]   255 that denote the brightness. So if it's 255, 255, 255, if
[00:13:52.880 --> 00:13:55.800]   you've created any website, you would know that means white, and
[00:13:55.800 --> 00:13:59.520]   there would be associated hex value with that. Hopefully my
[00:13:59.520 --> 00:14:05.160]   teeth are white. So that would be a perfect array of 255,255,255
[00:14:05.160 --> 00:14:09.800]   just like my chair in the background. But how do we go
[00:14:09.800 --> 00:14:13.040]   from these numbers to understanding what's inside of
[00:14:13.040 --> 00:14:15.760]   that image, right? Again, a computer just understands
[00:14:15.760 --> 00:14:20.400]   numbers, can't understand things beyond that. So what we do is we
[00:14:20.400 --> 00:14:26.560]   take these kernels that we multiply, neural networks are a
[00:14:26.560 --> 00:14:29.360]   bunch of matrix multiplication, I gave that spoiler in the first
[00:14:29.360 --> 00:14:33.560]   lecture. We perform this matrix multiplication after which we
[00:14:33.560 --> 00:14:38.600]   pass it through an activation layer. And then we start getting
[00:14:38.600 --> 00:14:45.000]   these outputs, what outputs these outputs. These
[00:14:45.000 --> 00:14:49.240]   visualizations come from the convolution being performed with
[00:14:49.240 --> 00:14:56.120]   the input, or whatever image we're passing to the model. Now
[00:14:56.120 --> 00:14:59.800]   I know just from the fact of learning this, that that's not
[00:14:59.800 --> 00:15:04.760]   good enough. So I'm stealing this explanation from Jeremy
[00:15:04.760 --> 00:15:09.480]   Howard, and I'll use his Excel sheet. Let me share the right
[00:15:09.480 --> 00:15:16.320]   screen again. To explain this using Microsoft Excel, this was
[00:15:16.320 --> 00:15:19.960]   taken from Fosse. And let me I'll also share the link in a
[00:15:19.960 --> 00:15:25.200]   few minutes. So what I want to achieve here is showcase how do
[00:15:25.200 --> 00:15:31.240]   these work inside of Microsoft Excel. So what Jeremy has done
[00:15:31.240 --> 00:15:35.560]   here, or I might be confused, Rachel might have written this
[00:15:35.560 --> 00:15:38.720]   Jeremy Howard and Rachel Thomas are the creators of Fosse, which
[00:15:38.720 --> 00:15:42.880]   is also one of the world's best deep learning course in my
[00:15:42.880 --> 00:15:46.800]   opinion. So this comes from there. What they've done in this
[00:15:46.800 --> 00:15:51.960]   notebook, they've loaded up an MNIST number. How do I know
[00:15:51.960 --> 00:15:57.760]   that it's 28 by 28. So that's the scale of remembering these
[00:15:57.760 --> 00:16:01.120]   dimensions come from. So they've literally read in this looks
[00:16:01.120 --> 00:16:07.720]   like the logo Swift, the logo of a bird. Or this is how I would
[00:16:07.720 --> 00:16:10.360]   write seven because that's how my writing is. That's what it
[00:16:10.360 --> 00:16:15.440]   looks like to me. This looks like number seven, right. So now
[00:16:15.440 --> 00:16:18.600]   what we want to do here is we want to predict that this is
[00:16:18.600 --> 00:16:23.120]   number seven, or we want to perform a bunch of convolutions.
[00:16:23.120 --> 00:16:28.400]   So again, what is a convolution, we have this filter that we
[00:16:28.400 --> 00:16:35.080]   convolve with the input image. So this output that we get is a
[00:16:35.080 --> 00:16:42.680]   matrix multiplication of the following. So we take this
[00:16:42.680 --> 00:16:47.400]   layer, it's a dot product, I apologize, it's a dot product of
[00:16:47.400 --> 00:16:52.120]   these, that's how you get a single number. But we take this
[00:16:52.120 --> 00:16:59.760]   filter, or kernel and we convolve it with these numbers,
[00:16:59.760 --> 00:17:04.120]   which gives us this output. And then what do we do, we move one
[00:17:04.120 --> 00:17:09.080]   step towards the right, and we do it for these numbers, you
[00:17:09.080 --> 00:17:13.760]   could change this. So you could instead of going sequentially
[00:17:13.760 --> 00:17:17.600]   towards the right. So here's what we've done. We've
[00:17:17.600 --> 00:17:22.360]   multiplied in one area, move to the immediate right pixel. The
[00:17:22.360 --> 00:17:27.440]   thing to note here, what you're seeing right now is 1000
[00:17:27.440 --> 00:17:32.480]   approximately 2000 by 1100 pixels. That's a lot of pixels.
[00:17:32.480 --> 00:17:36.360]   So even if you skip point two, three pixels, no big deal,
[00:17:36.360 --> 00:17:41.680]   right? It's still 2000 you just missing what 600 out of 2000.
[00:17:42.000 --> 00:17:46.160]   You could do that. Maybe compute faster, you can make the matrix
[00:17:46.160 --> 00:17:51.120]   multiplication faster. So the amount by which we jump pixels
[00:17:51.120 --> 00:17:58.360]   is called stride. That's how much we perform a stride by and
[00:17:58.360 --> 00:18:03.880]   the size of the filter or kernel is called kernel size, quite
[00:18:03.880 --> 00:18:07.880]   unsurprisingly, these are the things we change inside of CNN
[00:18:07.880 --> 00:18:13.520]   when designing it. So we do this a lot of times. And then we get
[00:18:13.520 --> 00:18:17.120]   this first layer, which to me looks somewhat still like the
[00:18:17.120 --> 00:18:20.160]   seven that I would write. I don't think anyone would write a
[00:18:20.160 --> 00:18:24.280]   seven like this, maybe just me. And we also have another filter
[00:18:24.280 --> 00:18:28.560]   with which we perform this. You could imagine if this was an RGB
[00:18:28.560 --> 00:18:31.960]   image, you could have multiples of filters. And usually that is
[00:18:31.960 --> 00:18:35.200]   the case you have a lot of these with which you perform these
[00:18:35.200 --> 00:18:41.000]   actions, you get these outputs. And then you do it again. So you
[00:18:41.000 --> 00:18:45.560]   have another filter, perform this action again. And then you
[00:18:45.560 --> 00:18:49.520]   get this output. Now what what just happened, right? We just
[00:18:49.520 --> 00:18:54.240]   went from the one fourth looks like so we're skipping four
[00:18:54.240 --> 00:19:00.720]   pixels. So we one fourth the count to output. Why did that
[00:19:00.720 --> 00:19:04.280]   happen before? Because if you look upwards, we're performing
[00:19:04.280 --> 00:19:11.240]   something known as max pooling. And this is the maximum out of
[00:19:11.240 --> 00:19:15.160]   these four numbers. So when some deep learning paper says you're
[00:19:15.160 --> 00:19:20.840]   performing max pooling, they're just taking the maximum of four
[00:19:20.840 --> 00:19:25.560]   layers. That's a max pool layer. Again, it's really easy to
[00:19:25.560 --> 00:19:28.840]   implement these ideas in code. That's why I would again remind
[00:19:28.840 --> 00:19:32.440]   you to check them out in a documentation because they make
[00:19:32.440 --> 00:19:37.280]   more sense. Maybe visually like this or in documentation, we're
[00:19:37.280 --> 00:19:41.320]   all coders here. We I'm probably on the worst end of the scale,
[00:19:41.320 --> 00:19:43.560]   you all would be on the better end of scale for me and we just
[00:19:43.560 --> 00:19:46.280]   want to get better machine learning. So it's better to
[00:19:46.280 --> 00:19:50.400]   understand these concepts in code as well. So this would be
[00:19:50.400 --> 00:19:57.880]   maximum of a slice of these locations, right in Python. So
[00:19:57.880 --> 00:20:04.160]   we do that for all of the possible pixels inside of this
[00:20:04.160 --> 00:20:08.080]   layer or this output that we got from performing this
[00:20:08.080 --> 00:20:12.920]   convolution. And then we would have a bunch of weights from
[00:20:12.920 --> 00:20:17.600]   which we can take a sum or take a softmax and get a final output,
[00:20:17.600 --> 00:20:23.000]   which would be the probability of is this a seven or not?
[00:20:23.000 --> 00:20:26.120]   Let's say if we're trying to just predict a seven. That is
[00:20:26.120 --> 00:20:31.200]   again, another detail that I'm not going in depth of, but this
[00:20:31.200 --> 00:20:35.240]   is how you perform a convolution. So when you have a
[00:20:35.240 --> 00:20:42.760]   neural network, this is what's happening inside of a CNN. This
[00:20:42.760 --> 00:20:47.480]   is a convolution operator, we take this kernel or filter.
[00:20:47.480 --> 00:20:51.760]   Quite unsurprisingly, we just do a bunch of matrix
[00:20:51.760 --> 00:20:56.560]   multiplication, which give us these outputs. We do that a
[00:20:56.560 --> 00:21:02.120]   bunch of times. And then we maybe perform a max pool, and we
[00:21:02.120 --> 00:21:04.840]   get a probability through some magic that we'll get into later
[00:21:04.840 --> 00:21:10.160]   on. So I'll take a pause to check out any questions. Ankit
[00:21:10.160 --> 00:21:17.640]   has a great question. I'll read that out. How do we decide the
[00:21:17.880 --> 00:21:22.640]   size of these kernels? So again, to remind everyone, since we've
[00:21:22.640 --> 00:21:30.200]   gone into a lot of jargon, this is the size of the kernel here,
[00:21:30.200 --> 00:21:35.800]   it's three by three. And another point that I want to add to
[00:21:35.800 --> 00:21:38.680]   Ankit's question, how would you decide what's right to do right
[00:21:38.680 --> 00:21:43.440]   through experimentation? It's again a lot of experiments, the
[00:21:43.440 --> 00:21:47.600]   paper I pointed out a while ago, conf next actually does a really
[00:21:47.600 --> 00:21:52.160]   well job at this. And I'll point that out. It compares how this
[00:21:52.160 --> 00:21:56.800]   differs. But every single CNN architecture, every model that
[00:21:56.800 --> 00:22:01.360]   you import, right, if you can recall the famous names, think
[00:22:01.360 --> 00:22:05.600]   ResNet, think VGGNet, if you've been in the attention era, think
[00:22:05.600 --> 00:22:14.800]   Swin Transformer, think VIT. These have different strides. I
[00:22:14.800 --> 00:22:16.840]   might be wrong, they might be the same across all of the
[00:22:16.840 --> 00:22:21.000]   papers I mentioned. But it will be similar across or different
[00:22:21.000 --> 00:22:23.360]   across many different architectures. And again, the
[00:22:23.360 --> 00:22:28.720]   kernel size would be different. So the number comes from
[00:22:28.720 --> 00:22:31.960]   experimentation or just following what's been going on.
[00:22:31.960 --> 00:22:34.680]   So Ankit, I would have a suggested homework for you to
[00:22:34.680 --> 00:22:39.040]   try, you know, different filter sizes or kernel sizes and try
[00:22:39.560 --> 00:22:47.680]   different strides. Can you share this Excel sheet? Sure, I'll
[00:22:47.680 --> 00:22:53.480]   just point out the link in a few minutes. Any other questions for
[00:22:53.480 --> 00:22:59.920]   anyone? I don't see any questions yet. So I'll just
[00:22:59.920 --> 00:23:06.680]   double check. Otherwise, I'll continue further. Incredible.
[00:23:06.680 --> 00:23:09.040]   Now I have to go through the painful process of stopping
[00:23:09.040 --> 00:23:12.960]   sharing and resharing my screen. I've gotten much better at this.
[00:23:12.960 --> 00:23:19.080]   And every time say that I end up messing up. So hopefully I
[00:23:19.080 --> 00:23:23.280]   don't jinx it. Let's see if that worked. I think it did.
[00:23:23.280 --> 00:23:31.920]   Awesome. So this is what happens inside of a convolution
[00:23:35.440 --> 00:23:39.520]   operator and a convolutional neural network. We have the
[00:23:39.520 --> 00:23:43.040]   input image, which we take the convolutional kernel and we get
[00:23:43.040 --> 00:23:49.560]   an output. We perform a max pooling. And then we get more
[00:23:49.560 --> 00:23:54.320]   outputs from which we finally do more operations that will
[00:23:54.320 --> 00:23:56.880]   eventually cover get a probability of what's actually
[00:23:56.880 --> 00:24:05.920]   happening inside of there. So I'm trying to think if I wanted
[00:24:05.920 --> 00:24:10.560]   to cover more things that I don't believe so. Again, this
[00:24:10.560 --> 00:24:15.480]   was taken through the fastecourse and I will now paste
[00:24:15.480 --> 00:24:24.000]   the link for the Excel sheet in the chat and highlighted. I
[00:24:24.000 --> 00:24:26.560]   would highly encourage everyone to try this out. In fact, it's
[00:24:26.560 --> 00:24:29.600]   a really good exercise to implement CNNs or any neural
[00:24:29.600 --> 00:24:33.680]   network and inside of Microsoft Excel. It's a really visual
[00:24:33.680 --> 00:24:39.240]   tool. So let me close these tabs and get everything out of the
[00:24:39.240 --> 00:24:44.800]   way. I'll just continue further and quickly explain the other
[00:24:44.800 --> 00:24:51.240]   problems as well. So CNNs cover a lot of different problems or
[00:24:51.240 --> 00:24:55.640]   they're used for a lot of different things. The example
[00:24:55.640 --> 00:24:58.720]   we've been looking at so far is simple image classification,
[00:24:58.720 --> 00:25:02.960]   which tells you if this is a hot dog or not hot dog, hopefully
[00:25:02.960 --> 00:25:06.560]   my image would not be a hot dog. If not, you need to improve your
[00:25:06.560 --> 00:25:09.480]   skills because I'm not, don't take that neural network, don't
[00:25:09.480 --> 00:25:16.080]   embarrass me. Or it could be a dog versus cat classifier. It
[00:25:16.080 --> 00:25:19.240]   could be an image net classifier, which will tell you
[00:25:19.240 --> 00:25:23.400]   what's inside of an image. But there are problems beyond that
[00:25:23.400 --> 00:25:27.680]   as well. So one of the other problems is it's really easy for
[00:25:27.680 --> 00:25:31.920]   the human brain to do this. But if you want to find me inside of
[00:25:31.920 --> 00:25:34.760]   the picture, how would you do that? Or how would you tell a
[00:25:34.760 --> 00:25:35.840]   computer to do that?
[00:25:35.840 --> 00:25:51.720]   One of the ways is, remember, the key point here is, this is
[00:25:51.720 --> 00:25:56.680]   zeros, ones, 255s, if you're being too technical, that the
[00:25:56.680 --> 00:26:00.360]   computer sees. How do you tell it what's inside of an image?
[00:26:00.360 --> 00:26:05.760]   Right? Or how do you visualize it for yourself, you could draw
[00:26:05.760 --> 00:26:09.000]   a box around me. That's one viable approach, right? You
[00:26:09.000 --> 00:26:12.160]   could literally, if this image is printed out, you could take
[00:26:12.160 --> 00:26:15.560]   a pen, draw a circle around me, that's I am you can you can
[00:26:15.560 --> 00:26:20.520]   recognize me hopefully by now. If you want to tell everything
[00:26:20.520 --> 00:26:24.160]   inside of a pixel, you could do the same for everything inside
[00:26:24.160 --> 00:26:28.560]   of the image, draw a circle around me, right? So I am draw
[00:26:28.560 --> 00:26:33.960]   circle around that chair. Right down, it's a chair, that's a
[00:26:33.960 --> 00:26:39.400]   couch, you could point out that's a couch. Or if you want
[00:26:39.400 --> 00:26:42.920]   to understand what's inside of an image, or if you want your
[00:26:42.920 --> 00:26:45.480]   neural network to understand what's inside of an image, you
[00:26:45.480 --> 00:26:52.400]   would label every single pixel with what it is. So image
[00:26:52.400 --> 00:26:57.120]   segmentation. If you look closely with this image, this
[00:26:57.120 --> 00:27:06.360]   could potentially be from a self driving car. Sorry, we're
[00:27:06.360 --> 00:27:11.080]   coloring every single potential possible thing inside of this
[00:27:11.080 --> 00:27:14.360]   image with a different color. But the key thing to note here
[00:27:14.360 --> 00:27:19.040]   is all humans are blue, all cars are red. And trees are
[00:27:19.040 --> 00:27:23.240]   greenish, those won't be trees, but don't get that further into
[00:27:23.240 --> 00:27:27.800]   the distance, maybe. Road signs are blue. So this is image
[00:27:27.800 --> 00:27:30.560]   segmentation, because we're trying to segment every single
[00:27:30.560 --> 00:27:36.080]   pixel out of this image, we're trying to label that. The other
[00:27:36.080 --> 00:27:41.840]   problem I mentioned was object detection. So broadly speaking,
[00:27:41.840 --> 00:27:46.560]   these are the three problems that CNNs address or three
[00:27:46.560 --> 00:27:50.280]   problems inside of computer vision domain. There are
[00:27:50.280 --> 00:27:53.000]   different ways of tackling this, there are a lot of
[00:27:53.000 --> 00:27:58.000]   architectures. But these are the three broad problems. We'll be
[00:27:58.000 --> 00:28:03.160]   going into depth into I believe, image segmentation later on. So
[00:28:03.160 --> 00:28:07.960]   I won't cover these steps from now and jump to chapter seven.
[00:28:08.560 --> 00:28:12.320]   So the first thing I've done here is I've so far, we've
[00:28:12.320 --> 00:28:19.680]   learned what is a CNN, we've learned how it works by looking
[00:28:19.680 --> 00:28:24.840]   at an Excel implementation. And now we'll go back and
[00:28:24.840 --> 00:28:27.920]   understand the Keras functional API and then move towards
[00:28:27.920 --> 00:28:32.480]   learning what a CNN model implementation looks like inside
[00:28:32.480 --> 00:28:36.480]   of Keras. We'll also implement the cutting edge architecture I
[00:28:36.480 --> 00:28:40.560]   mentioned. So let's jump into that. And again, if there's any
[00:28:40.560 --> 00:28:44.800]   thing that's not clear, please, please ask a question. And I'd
[00:28:44.800 --> 00:28:49.680]   love to clarify it for you. I want to mention another thing,
[00:28:49.680 --> 00:28:55.200]   I've used weights and biases to track the experiments here. And
[00:28:55.200 --> 00:28:59.320]   I'll point everyone to this nice callback that we have
[00:28:59.320 --> 00:29:05.320]   implemented for Keras, you can check out this colab to learn
[00:29:05.320 --> 00:29:08.360]   about it. I won't be going into much depth, I'll just give a
[00:29:08.360 --> 00:29:13.960]   highlight of how that works. So now, we're at the point where we
[00:29:13.960 --> 00:29:17.640]   want to learn about Keras. Last week, we learned Keras was
[00:29:17.640 --> 00:29:21.400]   created out of this problem or necessity of having so many
[00:29:21.400 --> 00:29:27.240]   different APIs. We had TensorFlow, Theano, CNTK, MXNet,
[00:29:27.240 --> 00:29:29.360]   if you don't know those names, that's totally fine. This is
[00:29:29.360 --> 00:29:31.880]   from like five years ago, which is equivalent to 100 years in
[00:29:31.880 --> 00:29:35.360]   machine learning. That's where Keras originated from. So we
[00:29:35.360 --> 00:29:39.040]   learned about the history, but right now, it's at the stage
[00:29:39.040 --> 00:29:42.640]   where it's really mature, it's the official high level API for
[00:29:42.640 --> 00:29:46.480]   TensorFlow, which means there would have been a lot of thought
[00:29:46.480 --> 00:29:50.200]   of around what you can do with it. So there are different ways.
[00:29:50.200 --> 00:29:53.360]   There are ways around hey, where you can take things and plug and
[00:29:53.360 --> 00:29:57.920]   play with them. Right? There are ways where you can implement a
[00:29:57.920 --> 00:30:01.520]   new architecture, that's totally feasible. And if you're a
[00:30:01.520 --> 00:30:05.000]   researcher trying a very new idea, you could do that as well.
[00:30:05.000 --> 00:30:10.000]   So this chapter, chapter seven, introduces these different
[00:30:10.000 --> 00:30:12.720]   techniques, or how do you actually do these different
[00:30:12.720 --> 00:30:22.040]   things I mentioned about. So that's what we'll learn now. I
[00:30:22.040 --> 00:30:25.480]   have a lot of screens open through which I keep switching,
[00:30:25.480 --> 00:30:30.560]   which is why you see a small delay. The first way to build a
[00:30:30.560 --> 00:30:36.040]   Keras model is the sequential class. Now let's hop over here
[00:30:36.040 --> 00:30:46.760]   and see what sequential is. Took me to count 2D layer, I don't
[00:30:46.760 --> 00:30:54.560]   want to. I want to go there. Maybe I'll just explain through
[00:30:59.720 --> 00:31:04.480]   the full app itself. Alright, so a sequential model is quite
[00:31:04.480 --> 00:31:07.880]   simple to implement, it just takes a list of layers you want
[00:31:07.880 --> 00:31:11.520]   to add. So what is a dense layer, we can look that up or at
[00:31:11.520 --> 00:31:15.200]   this point, I would expect you to know, it's a network of fully
[00:31:15.200 --> 00:31:20.120]   connected neurons. This is one way of creating a model where we
[00:31:20.120 --> 00:31:23.280]   tell Keras, hey, Keras, please create a model with two dense
[00:31:23.280 --> 00:31:27.440]   layers of these dimensions and having the following activation
[00:31:27.440 --> 00:31:31.360]   functions. You could also incrementally build it. So you
[00:31:31.360 --> 00:31:35.960]   could initialize a sequential model like so. So model stores a
[00:31:35.960 --> 00:31:39.880]   sequential model. And then you could add these layers by using
[00:31:39.880 --> 00:31:45.240]   model.add. To go from there, you'll have to build the model.
[00:31:45.240 --> 00:31:48.120]   And then you can take a look at the weights inside of the model.
[00:31:48.120 --> 00:31:52.760]   So that is one way, another way of building a sequential model.
[00:31:52.760 --> 00:31:54.920]   Again, we're talking simple things. So we're looking at
[00:31:54.920 --> 00:31:59.320]   what's the simplest way of doing that. I really like model.summary.
[00:31:59.320 --> 00:32:02.280]   So if you print out model.summary, it can tell you
[00:32:02.280 --> 00:32:06.680]   actually what's going on inside of the neural network, it can
[00:32:06.680 --> 00:32:10.640]   tell you how many parameters exist, how many trainable
[00:32:10.640 --> 00:32:13.480]   parameters are there, how many are non trainable and all of the
[00:32:13.480 --> 00:32:16.840]   little details inside of the model architecture that you've
[00:32:16.840 --> 00:32:26.120]   defined. Could someone kindly confirm if this text size is
[00:32:26.120 --> 00:32:30.440]   good for everyone, please? I'm assuming it is but if it's not,
[00:32:30.440 --> 00:32:38.040]   please let me know. This, I think also exists in PyTorch.
[00:32:38.040 --> 00:32:41.960]   When it came to PyTorch, I was really happy about this
[00:32:41.960 --> 00:32:46.040]   introduction, but I think it's being deprecated now. I just
[00:32:46.040 --> 00:32:49.040]   want to double check it. It was called named tensors
[00:32:49.040 --> 00:32:55.520]   originally. I'm on the PyTorch documentation. And I wanted to
[00:32:55.520 --> 00:33:00.240]   point out that this, I think PyTorch adopted this from
[00:33:00.240 --> 00:33:04.520]   Keras. I, again, I'm a fan of both the frameworks. I'm just
[00:33:04.520 --> 00:33:07.760]   learning both of them. In PyTorch, you can also name your
[00:33:07.760 --> 00:33:13.240]   variables like so or your tensors like so. I don't want
[00:33:13.240 --> 00:33:17.240]   to confuse you all with the jargon. So the cool thing in
[00:33:17.240 --> 00:33:22.280]   Keras is you can name your entire model. So you can call
[00:33:22.280 --> 00:33:28.040]   this model as well if you want. Or just leave it on the default
[00:33:28.040 --> 00:33:33.200]   name also. I think Keras picks a default name as well. Yuvraj,
[00:33:33.200 --> 00:33:36.680]   thanks for confirming. Yuvraj says the text looks good. So I
[00:33:36.680 --> 00:33:44.680]   trust him. You could name the model followed by individual
[00:33:44.680 --> 00:33:47.520]   layers as well. So when you're adding a layer, you could pass
[00:33:47.520 --> 00:33:51.640]   on a name argument. It's optional. Because I know that
[00:33:51.640 --> 00:33:55.240]   because I've looked inside of the documentation. This is why
[00:33:55.240 --> 00:33:58.000]   I encourage you all to play with the documentation as a reminder,
[00:33:58.000 --> 00:34:02.080]   which I'll keep doing throughout the entirety of this lecture
[00:34:02.080 --> 00:34:07.680]   series. And then you can finally build the model. And then when
[00:34:07.680 --> 00:34:09.680]   you print out the summary, it will tell you, hey, you call
[00:34:09.680 --> 00:34:14.880]   this model your example model. Your first layer, or your first
[00:34:14.880 --> 00:34:18.880]   layer, which you call my first layer, is actually a dense layer
[00:34:18.880 --> 00:34:21.960]   having the following output shape and has these many
[00:34:21.960 --> 00:34:31.160]   parameters. Now code is read more than it's written, right?
[00:34:31.320 --> 00:34:36.560]   Code is meant to be readable by humans. So these little details,
[00:34:36.560 --> 00:34:40.160]   you might not want to do this. We've all been there where you
[00:34:40.160 --> 00:34:43.120]   know, you just want to get the thing to work. And you don't
[00:34:43.120 --> 00:34:46.080]   care about adding doc strings in Python, you don't care about
[00:34:46.080 --> 00:34:50.960]   adding some documentation. But as a favor to yourself in the
[00:34:50.960 --> 00:34:53.640]   future, consider using these features. That's why it was
[00:34:53.640 --> 00:34:57.320]   pointed out in the book. Code is meant to be read more than it's
[00:34:57.320 --> 00:35:00.960]   written even by yourself, even by your teammates. These
[00:35:00.960 --> 00:35:04.960]   features are useful, really useful with that. So consider
[00:35:04.960 --> 00:35:08.720]   naming your models or layers whenever you can. Or if you're
[00:35:08.720 --> 00:35:11.520]   doing something fancy, you know, like I expect your teammates to
[00:35:11.520 --> 00:35:14.320]   be smart enough. This is basic stuff. I hope everyone knows
[00:35:14.320 --> 00:35:17.520]   that. If you're like trying to have something together that you
[00:35:17.520 --> 00:35:22.040]   know, just might work. Probably, that's a good place to name
[00:35:22.040 --> 00:35:25.640]   what's going on or at least leave some form of hint for
[00:35:25.640 --> 00:35:32.800]   yourself. So one thing we need to do before printing the
[00:35:32.800 --> 00:35:36.000]   summary that we haven't done in this approach is we need to tell
[00:35:36.000 --> 00:35:40.600]   the model what could be the input shape. And for that you
[00:35:40.600 --> 00:35:47.520]   can create an input tensor. And I think this is called a pointer
[00:35:47.560 --> 00:36:02.040]   input if I remember correctly. I want to share the correct
[00:36:02.040 --> 00:36:05.720]   jargon, but I believe it's called a pointer input. So we
[00:36:05.720 --> 00:36:09.560]   just tell Keras, hey, this could be the potential input. We're
[00:36:09.560 --> 00:36:12.200]   not telling you what it is. We're just telling you what it
[00:36:12.200 --> 00:36:15.000]   looks like. And that's important for a neural network to actually
[00:36:15.000 --> 00:36:18.640]   work as we've learned because it's matrix multiplication and
[00:36:18.640 --> 00:36:21.280]   that needs to work on a certain dimension, it can only work on a
[00:36:21.280 --> 00:36:24.960]   certain dimension effectively, not effectively actually just
[00:36:24.960 --> 00:36:33.680]   work on a certain dimension. So then from there, we can again
[00:36:33.680 --> 00:36:37.040]   print the summary. That's one of the ways where we create a
[00:36:37.040 --> 00:36:42.520]   model. You could, of course, pass in activation functions
[00:36:42.520 --> 00:36:46.080]   like so. So what we've done in the previous between the
[00:36:46.080 --> 00:36:51.080]   previous and this line of code is we've created a model that
[00:36:51.080 --> 00:36:55.000]   just had this input layer and one dense layer. So the input
[00:36:55.000 --> 00:37:00.640]   tells us what would this layer expect. And then we can add more
[00:37:00.640 --> 00:37:04.840]   layers to it and hey, that does work. It does tell us that you've
[00:37:04.840 --> 00:37:08.000]   added one more layer if you look closely between the differences.
[00:37:09.440 --> 00:37:12.800]   So this is one of the ways of creating models inside of
[00:37:12.800 --> 00:37:18.760]   Keras, the classic API, the sequential class. The next way
[00:37:18.760 --> 00:37:23.200]   to do it is using the functional API. I want to say I think if
[00:37:23.200 --> 00:37:26.040]   you've worked with JavaScript, I believe you might be familiar
[00:37:26.040 --> 00:37:30.800]   with this or if you've watched the JAX series, you might be
[00:37:30.800 --> 00:37:35.880]   more familiar with having taking this approach. I was quite new
[00:37:35.880 --> 00:37:40.280]   to it when I started playing around with JAX. But if this
[00:37:40.280 --> 00:37:43.800]   feels a little non-Pythonic, that's totally fine. I want to
[00:37:43.800 --> 00:37:53.160]   just point that out. Sorry about that. So the next way to create
[00:37:53.160 --> 00:37:57.200]   models, again, we're doing the same thing over and over again.
[00:37:57.200 --> 00:38:00.400]   We want to learn how to create models using different
[00:38:00.400 --> 00:38:03.600]   approaches inside of Keras. So that's what we're doing. We
[00:38:03.600 --> 00:38:05.640]   looked at the sequential class. Now we're looking at the
[00:38:05.640 --> 00:38:10.000]   functional API, following the same simple example. So we
[00:38:10.000 --> 00:38:13.520]   define our input again using Keras.input. And we tell the
[00:38:13.520 --> 00:38:19.640]   shape like so. This is one thing I messed up while trying to
[00:38:19.640 --> 00:38:22.480]   replicate the implementation. This is a way of telling Python
[00:38:22.480 --> 00:38:26.840]   that you don't want to specify the second dimension. So that's
[00:38:26.840 --> 00:38:33.240]   a three comma, nothing tells Python. Again, a little detail,
[00:38:33.240 --> 00:38:37.440]   but it's good to know you could also do it like so. Or just
[00:38:37.440 --> 00:38:40.800]   leave it like that if you're a cool practitioner. Now that we
[00:38:40.800 --> 00:38:44.160]   know we can name the layers, we will be naming them. So we tell
[00:38:44.160 --> 00:38:48.360]   Keras what's the name of this layer. Then we define the
[00:38:48.360 --> 00:38:54.440]   features like so. Key thing to note, we are passing the inputs
[00:38:54.440 --> 00:38:59.560]   this way. So we define the layer and then we pass on the inputs.
[00:39:02.240 --> 00:39:05.760]   Sorry, we don't pass in the inputs we tell or we connect
[00:39:05.760 --> 00:39:10.600]   these like so. The reason for doing this approach is, let me
[00:39:10.600 --> 00:39:19.520]   switch sharing and connect my writing pad real quick. I want
[00:39:19.520 --> 00:39:22.520]   to point out one minor thing inside of the book. This is
[00:39:22.520 --> 00:39:27.040]   known as a graph. Franchot mentioned this to be a graph.
[00:39:27.040 --> 00:39:31.360]   It's not a graph neural network. That is something completely
[00:39:31.360 --> 00:39:36.000]   different. This is referring to how we actually draw these
[00:39:36.000 --> 00:39:43.880]   graphical structures inside of a network architecture
[00:39:43.880 --> 00:39:48.680]   definition. So if you're, let's say defining a model, right, let
[00:39:48.680 --> 00:39:57.640]   me pick up and let's say this is your input. This layer is layer
[00:39:57.640 --> 00:40:04.800]   one. This would have a bunch of neurons, right? So our input
[00:40:04.800 --> 00:40:10.720]   goes from here to here. And then from there, let's say it goes
[00:40:10.720 --> 00:40:16.240]   through an activation function from where it goes through,
[00:40:16.240 --> 00:40:20.040]   let's say more operators or it goes through some form of CNN.
[00:40:20.040 --> 00:40:23.560]   I don't think this is a valid architecture, but for the sake
[00:40:23.560 --> 00:40:27.680]   of creating this graph. So the thing to note here is, this is
[00:40:27.680 --> 00:40:30.640]   the graph we're talking about. This is the type of model that
[00:40:30.640 --> 00:40:33.920]   we're trying to build. And this is how at least in my head and
[00:40:33.920 --> 00:40:39.600]   many people that I know, visualize these models. So we're
[00:40:39.600 --> 00:40:42.720]   trying to find an anecdote similar to this, right? How do
[00:40:42.720 --> 00:40:46.120]   we do this in code? Because I mean, everything we want to do
[00:40:46.120 --> 00:40:50.120]   should be doable in code and understandable in code, right?
[00:40:50.640 --> 00:41:00.920]   So we take layer one, we define layer one. And we tell it, hey,
[00:41:00.920 --> 00:41:07.320]   this is where the stuff is coming from. We take layer two.
[00:41:07.320 --> 00:41:14.000]   And we tell it, hey, you're connected to layer one. So now
[00:41:14.000 --> 00:41:18.440]   we're building this graph again, not a graph neural network, but
[00:41:18.440 --> 00:41:24.280]   this graph of the model. So let's go back to the other
[00:41:24.280 --> 00:41:28.440]   screen, which I'll painfully reshare. I'm on a roll today.
[00:41:28.440 --> 00:41:33.120]   Usually I take too long to switch sharing screens. There
[00:41:33.120 --> 00:41:38.200]   we go. So that's what we are doing here. We can print out the
[00:41:38.200 --> 00:41:42.000]   input shape, we can print out the types. And we can print out
[00:41:42.000 --> 00:41:46.120]   the model summary as well. So if you print out the summary, it's
[00:41:46.120 --> 00:41:48.960]   again a similar model as earlier, the only difference
[00:41:48.960 --> 00:41:52.320]   here is we using the functional API to define the model. So when
[00:41:52.320 --> 00:41:55.800]   we call the model, we'll have to pass in the inputs and the
[00:41:55.800 --> 00:42:00.200]   outputs because right now the state is not being stored. And
[00:42:00.200 --> 00:42:04.000]   that's how we do this. So again, similar model, nothing fancy
[00:42:04.000 --> 00:42:08.520]   here. Now, if you want to work with a multi input multi output
[00:42:08.520 --> 00:42:12.200]   functional model that is possible as well. The reason
[00:42:12.200 --> 00:42:14.840]   this chapter and this collab exists is we're trying to go
[00:42:14.840 --> 00:42:18.000]   from the simple example that just works out of the box to
[00:42:18.000 --> 00:42:21.360]   something that's a bit more complicated step by step. So we
[00:42:21.360 --> 00:42:25.480]   want to incrementally expose ourselves to different problems
[00:42:25.480 --> 00:42:35.120]   and how can we tackle them? Right? So we do that by using the
[00:42:35.120 --> 00:42:39.200]   functional API again. Sorry, I am trying to see if there are
[00:42:39.200 --> 00:42:44.400]   any questions. I always get distracted by that. Someone's
[00:42:44.400 --> 00:42:47.320]   asking what is the reference material, there's no reference
[00:42:47.320 --> 00:42:52.760]   material, we're using the official GitHub repository by
[00:42:52.760 --> 00:42:59.160]   Franscho Jolle. So if you look up, keep learning with Python,
[00:42:59.160 --> 00:43:05.000]   GitHub, we're using these notebooks, and I'm just
[00:43:05.000 --> 00:43:08.680]   providing a revision or a highlight of different chapters.
[00:43:08.680 --> 00:43:12.040]   We're not strictly using the materials on the books because
[00:43:12.040 --> 00:43:15.480]   this book is not freely available. And I expect everyone
[00:43:15.480 --> 00:43:20.080]   to own a copy of this. So we're going through these chapters in
[00:43:20.080 --> 00:43:24.200]   a sequential order. And we're on chapter seven. I hope that
[00:43:24.200 --> 00:43:34.800]   answers your question. Now we're on this problem of having multi
[00:43:34.800 --> 00:43:38.040]   input and multi output functional model. So I think
[00:43:38.040 --> 00:43:42.160]   inside of the chapter, they talked about what if you have a
[00:43:42.160 --> 00:43:45.840]   bunch of words and other inputs that you want to tackle. So it's
[00:43:45.840 --> 00:43:49.400]   an NLP plus tabular problem if you want to strictly put it into
[00:43:49.400 --> 00:43:55.440]   a bucket. You could take an input of the vocab size. And you
[00:43:55.440 --> 00:43:59.400]   could take an input of the text body. And you could take an
[00:43:59.400 --> 00:44:02.400]   input of the text. So now we have these three inputs. Again,
[00:44:02.400 --> 00:44:05.520]   if you go back to the graph we drew, this time we would have
[00:44:05.520 --> 00:44:08.920]   three circles for the input, right? Again, that's why it's
[00:44:08.920 --> 00:44:12.080]   really to me, it's as I speak this, right, it's really simple
[00:44:12.080 --> 00:44:16.000]   to visualize in my head, three circles that connect to three
[00:44:16.000 --> 00:44:19.040]   layers possibly or beyond. So that's a graph you want to be
[00:44:19.040 --> 00:44:22.760]   visualizing in our head. Just take a paper every time you want
[00:44:22.760 --> 00:44:25.640]   to implement something like this, draw that. And then it's
[00:44:25.640 --> 00:44:32.800]   really simple to implement in Keras. We can define the title,
[00:44:32.800 --> 00:44:36.760]   text body, tags, like so. All of these are inputs. So these are
[00:44:36.760 --> 00:44:40.760]   Keras input that have the shape like so which we've defined
[00:44:40.760 --> 00:44:46.040]   above. And for our co workers, mental health, we can give them
[00:44:46.040 --> 00:44:52.320]   names also to remain in love or in harmony with them, we can
[00:44:52.320 --> 00:44:59.360]   give these names. From there we can concatenate. So let's look
[00:44:59.360 --> 00:45:06.880]   at what this does. If I can find it. What does concatenate layer
[00:45:06.880 --> 00:45:14.560]   do? Layer that concatenates a list of inputs, it takes input
[00:45:14.560 --> 00:45:19.320]   as input a list of tensors, all of the same shape except for
[00:45:19.320 --> 00:45:25.400]   concatenation access and returns a single tensor. So let's look
[00:45:25.400 --> 00:45:28.040]   at what's going on here. Let's take a look at this example.
[00:45:28.040 --> 00:45:39.440]   If we concatenate x1 and x2, which would be these two layers,
[00:45:39.440 --> 00:45:44.800]   the shape becomes a stacked model of these or they get
[00:45:44.800 --> 00:45:48.880]   pasted together. That's what's going on here. So now we can
[00:45:48.880 --> 00:45:52.120]   take these three different inputs that we just defined and
[00:45:52.120 --> 00:46:04.160]   concatenate them together. From there, everything is hunky
[00:46:04.160 --> 00:46:07.400]   dory. Now we continue with dense layer, we pass in the
[00:46:07.400 --> 00:46:12.080]   features like so. Since we've passed in the input already,
[00:46:12.080 --> 00:46:19.040]   next thing we take the priority and pass in features further to
[00:46:19.040 --> 00:46:22.920]   this layer. And from there we define another layer called
[00:46:22.920 --> 00:46:28.880]   department like so. I apologize, these are the outputs of
[00:46:28.880 --> 00:46:34.400]   features would be the middle layer. So these three are the
[00:46:34.400 --> 00:46:38.200]   input layers. I actually drew the graph and confused myself.
[00:46:38.200 --> 00:46:44.360]   Let me start again. So we have these three inputs, and we have
[00:46:44.360 --> 00:46:48.080]   these two outputs. So we want to predict the priority and
[00:46:48.080 --> 00:46:51.080]   department based on vocab size, number of tags and number of
[00:46:51.080 --> 00:46:57.360]   departments. And we have these hidden layers. So we pass the
[00:46:57.360 --> 00:47:00.640]   input to the hidden layers, which we call features. And then
[00:47:00.640 --> 00:47:04.680]   we pass the features to the layer that will give us the
[00:47:04.680 --> 00:47:10.800]   outputs. And then we when we create the model, we pass in the
[00:47:10.800 --> 00:47:19.920]   inputs and outputs in a list like so. Saurabh is asking how
[00:47:19.920 --> 00:47:25.440]   parameters are calculated in model summary. Let's see if the
[00:47:25.440 --> 00:47:27.240]   documentation tells us much.
[00:47:27.240 --> 00:47:38.560]   Sorry about that.
[00:47:38.560 --> 00:47:53.080]   Summary method prints a string summary of the network. It
[00:47:53.080 --> 00:47:57.160]   doesn't tell us how that is being done. So we could take a
[00:47:57.160 --> 00:48:01.520]   look at the source code. But quite simply put, if you were to
[00:48:01.520 --> 00:48:06.080]   draw the graph of this model, a dense layer actually just
[00:48:06.080 --> 00:48:11.840]   contains all of these models connected to each other. And if
[00:48:11.840 --> 00:48:16.120]   you take the size of the tensors, so remember when I said
[00:48:16.120 --> 00:48:20.720]   the input image is 2000 by 1100, approximately, it's actually
[00:48:20.720 --> 00:48:24.800]   1920 by 1080. I know that through a painful bunch of
[00:48:24.800 --> 00:48:30.960]   editing work for my podcast, if you multiply those numbers, that
[00:48:30.960 --> 00:48:36.960]   gives you the number of parameters. So assuming your PC
[00:48:36.960 --> 00:48:42.040]   is learning this image, or this is the size of the image that is
[00:48:42.040 --> 00:48:45.880]   being learned, the number of parameters would be the number
[00:48:45.880 --> 00:48:51.880]   of elements inside of this tensor, which would be 1920 by
[00:48:51.880 --> 00:48:56.320]   1080 by 3. And if there are a lot of these, you multiply by
[00:48:56.320 --> 00:48:59.000]   that number, that's the number of parameters that are being
[00:48:59.000 --> 00:49:02.640]   learned. So when you use model.summary, that's how you get
[00:49:02.640 --> 00:49:05.080]   those numbers. That's where that number comes from. Thanks for
[00:49:05.080 --> 00:49:12.880]   the great questions Aurov. So now, we've created this model
[00:49:12.880 --> 00:49:18.080]   that takes three inputs, gives two outputs. How do we train
[00:49:18.080 --> 00:49:21.680]   this? Right, we've just defined the model, we can check if it's
[00:49:21.680 --> 00:49:26.600]   created by printing the model summary. We'll have to define a
[00:49:26.600 --> 00:49:30.280]   few things. So we define the title data like so, where we
[00:49:30.280 --> 00:49:36.640]   define it by creating a randomly initialized NumPy array, which
[00:49:36.640 --> 00:49:41.800]   has the size of samples and vocab size. We define the text
[00:49:41.800 --> 00:49:46.600]   body data as well and tags data. So we define for the sake of
[00:49:46.600 --> 00:49:52.960]   this toy example, randomly initialized inputs that would
[00:49:52.960 --> 00:49:58.640]   correspond to these three inputs. And then we define the
[00:49:58.640 --> 00:50:02.520]   priority data and department data from where we compile the
[00:50:02.520 --> 00:50:06.680]   model. So we pass on the optimizer, we tell it, hey,
[00:50:06.680 --> 00:50:10.280]   these are the loss functions we want to look at. And then we
[00:50:10.280 --> 00:50:14.840]   pass on the matrix. From there, we fit the model, and then
[00:50:14.840 --> 00:50:20.600]   evaluate it on a particular data set. And then we can print out
[00:50:20.600 --> 00:50:25.600]   what's happening inside of the model. Oh, this is quite painful,
[00:50:25.600 --> 00:50:30.680]   right? Because you need to pass in things in the right order. So
[00:50:30.680 --> 00:50:33.880]   when you're fitting the model, this should correspond with the
[00:50:33.880 --> 00:50:37.640]   title data, this should correspond with the NLP data.
[00:50:37.640 --> 00:50:41.400]   That's not how things should be right now this this is getting
[00:50:41.400 --> 00:50:44.240]   out of hand real quickly, because like assume you have 50
[00:50:44.240 --> 00:50:48.360]   inputs. That's, that's really painful. ImageNet has I think
[00:50:48.360 --> 00:50:51.880]   about 1000 classes, I might be wrong about this, we might have
[00:50:51.880 --> 00:50:56.520]   more. So ImageNet 1k I believe has 1000, ImageNet 22k I think
[00:50:56.520 --> 00:51:01.440]   has 22,000 classes. What if you have to pass that? Would you
[00:51:01.440 --> 00:51:04.760]   write this? No, you won't. You'll read in a dictionary. So
[00:51:04.760 --> 00:51:09.880]   again, this I want to point this fact out if you're new to
[00:51:09.880 --> 00:51:16.000]   Python, our dictionary is a way of telling Python, hey, this is
[00:51:16.000 --> 00:51:19.160]   the title. This is the key, this is the value. So you create
[00:51:19.160 --> 00:51:22.120]   these mappings. If you're from the world of computer science,
[00:51:22.120 --> 00:51:24.440]   the only place where you can flex your knowledge would be
[00:51:24.440 --> 00:51:29.200]   calling this a hash map. Again, in Python, it's a dictionary
[00:51:29.200 --> 00:51:34.000]   where you tell Python, hey, Python, please store this data
[00:51:34.000 --> 00:51:38.080]   as title, please store this data as text body, so on and so
[00:51:38.080 --> 00:51:42.120]   forth. And then you can just take this and pass it to the
[00:51:42.120 --> 00:51:45.640]   model when you're working with the model. So you can simply
[00:51:45.640 --> 00:51:51.520]   pass in the dictionary to the model. Anything defined inside
[00:51:51.520 --> 00:51:59.840]   of curly braces in Python would be a dictionary. So now, this
[00:51:59.840 --> 00:52:03.040]   is a cool thing that I learned while reading the book. But
[00:52:03.040 --> 00:52:06.160]   Keras also has a nice utility that lets you print what's
[00:52:06.160 --> 00:52:09.680]   happening inside of the model really useful again. So we have
[00:52:09.680 --> 00:52:14.480]   a title, a text body and tags, which go to this concatenation
[00:52:14.480 --> 00:52:17.520]   process, after which they are passed through dense function,
[00:52:17.520 --> 00:52:21.160]   which gives us these two outputs. So we've created this
[00:52:21.160 --> 00:52:25.120]   multi input, multi output, more than one, right? That's
[00:52:25.120 --> 00:52:31.080]   multiple model. And if you want more details, you can pass in a
[00:52:31.080 --> 00:52:34.560]   flag called show shapes true. Again, this comes from Keras
[00:52:34.560 --> 00:52:38.800]   dot utils plot model. If you want to play around, copy this
[00:52:38.800 --> 00:52:41.800]   look inside of the documentation, see what possible
[00:52:41.800 --> 00:52:47.880]   arguments exist. Or you could simply also print out the layers
[00:52:47.880 --> 00:52:53.200]   like so. So that is another way of creating a model inside of
[00:52:53.200 --> 00:52:59.040]   Keras. Let's see. I think there's more details around this.
[00:52:59.040 --> 00:53:02.640]   So I'll continue further. Any questions so far? If not, we'll
[00:53:02.640 --> 00:53:27.960]   continue with subclassing. Awesome. I don't see any
[00:53:27.960 --> 00:53:32.400]   questions yet. Also, there's a small lag with YouTube. So your
[00:53:32.400 --> 00:53:34.280]   questions might have gone through but I don't see them
[00:53:34.280 --> 00:53:39.200]   because this is all really that happens. I'll come back to them
[00:53:39.200 --> 00:53:43.200]   when I see them. So now we're on this journey of understanding
[00:53:43.200 --> 00:53:48.280]   the possible ways of defining models inside of Keras. And
[00:53:48.280 --> 00:53:50.760]   we've looked at how to do it with sequential API, the
[00:53:50.760 --> 00:53:53.960]   functional API. Now we look at something that if you come from
[00:53:53.960 --> 00:53:57.000]   PyTorch, it's similar to that. So we're subclassing the model
[00:53:57.000 --> 00:54:02.360]   class. In computer science world, it's called inheritance.
[00:54:02.840 --> 00:54:07.960]   So we inherit from the model class. And whenever you inherit
[00:54:07.960 --> 00:54:10.480]   any class, you have to initialize the superclass
[00:54:10.480 --> 00:54:17.200]   constructor. So what is a superclass? When you inherit from
[00:54:17.200 --> 00:54:22.880]   a class, I have inherited genes for my parents. That's the
[00:54:22.880 --> 00:54:27.880]   parent class and the student class that is inheriting. In
[00:54:28.280 --> 00:54:31.800]   Python, you would have to initialize the superclass
[00:54:31.800 --> 00:54:36.680]   parameters like so because I've inherited from this class.
[00:54:36.680 --> 00:54:41.880]   That's a object oriented programming concept. Don't go
[00:54:41.880 --> 00:54:45.200]   from here and try to look up OOP concepts, object oriented
[00:54:45.200 --> 00:54:47.800]   programming concepts. If you're new to programming, or if you're
[00:54:47.800 --> 00:54:50.800]   new to these concepts, or if you've been coding for a while,
[00:54:50.800 --> 00:54:54.280]   and this is the first time you're hearing this, this is not
[00:54:54.280 --> 00:54:57.280]   the place to go on a tangent and learn about that. That's a
[00:54:57.280 --> 00:55:02.840]   rabbit hole that ends on lead code. And that is a, that is a
[00:55:02.840 --> 00:55:07.160]   dark, dark path. So remember, we're here to learn Keras. And
[00:55:07.160 --> 00:55:10.360]   this is one thing that you eventually start to get a hang
[00:55:10.360 --> 00:55:16.080]   of. So we call the super init, which actually tells Python to
[00:55:16.080 --> 00:55:20.840]   please call the init of Keras models. From there, we continue
[00:55:20.840 --> 00:55:25.680]   our regular journey of defining concat layer. So this is a
[00:55:25.680 --> 00:55:29.280]   parameter of customer ticket model class, where we tell it
[00:55:29.280 --> 00:55:36.600]   hey, this is a concat layer, which concatenates stuff. Sorry.
[00:55:36.600 --> 00:55:44.000]   Then we have a mixing layer, which is dense, a dense layer of
[00:55:44.000 --> 00:55:49.320]   dimension like so, an activation function like so. Then we have
[00:55:49.320 --> 00:55:53.640]   a priority scorer, and a department classifier. Now we
[00:55:53.640 --> 00:55:57.000]   need to define a function call, which means whenever we call
[00:55:57.000 --> 00:56:03.120]   this model, this function will get executed. So we tell the
[00:56:03.120 --> 00:56:09.200]   inputs which are titled text body and tags. And then we
[00:56:09.200 --> 00:56:13.640]   create the features like so and outputs, which would be priority
[00:56:13.640 --> 00:56:20.240]   and department. And then we return them from the model. So
[00:56:20.240 --> 00:56:25.400]   now when we create an instance of this model, call would get
[00:56:25.400 --> 00:56:31.480]   executed, and we'll pass in the number of departments to it from
[00:56:31.480 --> 00:56:38.200]   where we can pass in or retrieve the output priority and
[00:56:38.200 --> 00:56:41.960]   department like so. Again, the names don't have to match this
[00:56:41.960 --> 00:56:46.240]   could be just T, this is P, or whatever we just doing that for
[00:56:46.240 --> 00:56:51.840]   readability. And we'll pass in the title data, text body data,
[00:56:51.840 --> 00:56:57.160]   and tags data to the model. That wouldn't train the model itself,
[00:56:57.160 --> 00:57:02.040]   right? Because we've not done any training function here yet.
[00:57:02.040 --> 00:57:07.320]   So we'll still have to compile a model and fit it and evaluate
[00:57:07.320 --> 00:57:14.480]   it. Same thing so far. So this is how we inherit and create a
[00:57:14.680 --> 00:57:19.720]   subclass model inside of Keras. This is the third way of
[00:57:19.720 --> 00:57:25.360]   creating classes of models inside of Keras. I think from
[00:57:25.360 --> 00:57:28.960]   there, they go into how to create a functional model inside
[00:57:28.960 --> 00:57:33.560]   a subclass model. This is quite straightforward. So I'm skipping
[00:57:33.560 --> 00:57:37.800]   that. I have quite the agenda to cover today. So I'm skipping
[00:57:37.800 --> 00:57:43.120]   ahead to a few things. We also learn later in the chapter of
[00:57:43.160 --> 00:57:48.360]   how to write our own matrix. So we define root mean squared
[00:57:48.360 --> 00:57:51.880]   error. And I leave you to reading this bit. Again, this
[00:57:51.880 --> 00:57:56.120]   was quite readable to me and I'm the benchmark. If I can
[00:57:56.120 --> 00:58:00.280]   understand anything, I'm sure everyone reading the book can.
[00:58:00.280 --> 00:58:08.400]   So root mean squared is actually the sum of square root of the
[00:58:08.400 --> 00:58:13.040]   square differences. I am again on the verge of actually
[00:58:13.040 --> 00:58:17.240]   covering this but let me just point out the functions here.
[00:58:17.240 --> 00:58:21.360]   So we square the differences. Just look at this line, please.
[00:58:21.360 --> 00:58:27.480]   We take the square of the differences and then I believe
[00:58:27.480 --> 00:58:32.080]   we take the square root of it. That's all you need to know.
[00:58:32.080 --> 00:58:35.200]   But again, if you look at the mathematical definition of root
[00:58:35.200 --> 00:58:38.520]   mean squared error, you can define it inside of Keras
[00:58:38.520 --> 00:58:42.840]   matrix. One thing to point out here is if you actually go to
[00:58:42.840 --> 00:58:50.360]   Keras matrix, it has a lot of matrix implemented. There are
[00:58:50.360 --> 00:58:53.440]   accuracy, probabilistic regression, classification, image
[00:58:53.440 --> 00:58:57.200]   segmentation. Sometimes, especially with a few Kaggle
[00:58:57.200 --> 00:59:01.280]   competitions, you have these tricky bits where you have a
[00:59:01.280 --> 00:59:07.000]   completely different matrix that you want to work towards. So
[00:59:07.000 --> 00:59:12.000]   for that case, you can define a custom matrix like so. Root mean
[00:59:12.000 --> 00:59:15.080]   squared error, I'm sure it does exist inside of Keras already.
[00:59:15.080 --> 00:59:20.160]   But this basically shows us how to create a class. So we create
[00:59:20.160 --> 00:59:24.200]   a custom class like so. We will have to define how the metric
[00:59:24.200 --> 00:59:29.000]   would update its state, how would it reset itself. And then
[00:59:29.000 --> 00:59:33.520]   you pass this class to the list of matrix that you pass your
[00:59:33.520 --> 00:59:39.920]   model. That's about it. Now the next thing inside of, sorry,
[00:59:40.160 --> 00:59:45.520]   this chapter is callbacks. And this is how I remember
[00:59:45.520 --> 00:59:52.560]   callbacks. So callbacks in code is quite similar to callback in
[00:59:52.560 --> 00:59:58.640]   real life. Please call me back when, please call me back if you
[00:59:58.640 --> 01:00:03.680]   complete the homework of Keras study group. Please call me back
[01:00:03.680 --> 01:00:08.720]   if your accuracy starts to drop. So that's what callback does
[01:00:08.760 --> 01:00:13.880]   inside of Keras. If you're new to callbacks, it takes a small
[01:00:13.880 --> 01:00:22.240]   while to get used to it. It just tells the model to call back the
[01:00:22.240 --> 01:00:27.440]   executor, the Python executor, depending on different
[01:00:27.440 --> 01:00:32.960]   conditions. So the reason for these things to exist is, once
[01:00:32.960 --> 01:00:35.360]   you kick off the training, it's really hard to understand what's
[01:00:35.360 --> 01:00:40.840]   going on inside of the model. These callbacks help us first of
[01:00:40.840 --> 01:00:44.840]   all monitor what's going on inside of the model and also
[01:00:44.840 --> 01:00:48.440]   change different things. So assuming you want to actually
[01:00:48.440 --> 01:00:51.960]   change the learning rate, or things like that, or if you want
[01:00:51.960 --> 01:00:57.800]   to early stop your model, so let's say, if your model is
[01:00:57.800 --> 01:01:00.640]   overfitting, how would you know if your model is overfitting,
[01:01:01.200 --> 01:01:06.960]   the accuracy would start to go down. You don't want to train
[01:01:06.960 --> 01:01:09.720]   your model beyond that because it's starting to overfit. So
[01:01:09.720 --> 01:01:13.160]   you need to stop before that. Early stopping is, if your model
[01:01:13.160 --> 01:01:17.200]   is starting to overfit before your number of specified steps
[01:01:17.200 --> 01:01:20.000]   or epochs is complete, please stop the training. I don't want
[01:01:20.000 --> 01:01:25.200]   to spend more money on cloud credits, cloud compute. Please
[01:01:25.200 --> 01:01:28.680]   Keras stop the training. So we pass a callback called early
[01:01:28.680 --> 01:01:34.960]   stopping, which takes care of that. You could also checkpoint
[01:01:34.960 --> 01:01:39.080]   your model, which means, hey, after every single epoch, Keras
[01:01:39.080 --> 01:01:43.640]   can you please save my model and Keras will do that for you. No
[01:01:43.640 --> 01:01:48.200]   problems. So when you're fitting your model, you pass it this
[01:01:48.200 --> 01:01:52.160]   callback list. Right now it's early stopping. Let's take a
[01:01:52.160 --> 01:02:04.200]   look at what that is. Stop training when a monitor metric
[01:02:04.200 --> 01:02:13.040]   has stopped improving. We can tell Keras, hey, I have the
[01:02:13.040 --> 01:02:17.080]   patience of two epochs. And I want you to monitor the
[01:02:17.080 --> 01:02:21.760]   validation accuracy. If it doesn't improve beyond two epochs,
[01:02:21.800 --> 01:02:25.800]   I will lose my patience of spending more money on GPU
[01:02:25.800 --> 01:02:28.000]   credits, please stop the training and Keras will do that
[01:02:28.000 --> 01:02:33.880]   for you. So this is how you can do it using a simple callback.
[01:02:33.880 --> 01:02:39.040]   And we pass this callback list to Keras. You could write your
[01:02:39.040 --> 01:02:45.040]   own callbacks to perform different functions or different
[01:02:45.040 --> 01:02:50.120]   activities. And these can be performed when, hey, call me
[01:02:50.120 --> 01:02:55.160]   back when training begins. Call me back. This is how I remember
[01:02:55.160 --> 01:02:58.200]   it in my head. If anyone in the audience has a better way of
[01:02:58.200 --> 01:03:01.360]   remembering it, please let me know. But I just have this
[01:03:01.360 --> 01:03:06.640]   analogy that I built. So call back on training beginning, call
[01:03:06.640 --> 01:03:10.480]   back on batch ending, call back on epoch ending. And you can do
[01:03:10.480 --> 01:03:14.920]   different things with that. That's what we learn inside of
[01:03:14.920 --> 01:03:19.360]   this. I wanted to do a walkthrough of how you can use
[01:03:19.360 --> 01:03:25.040]   the Von DB callback to log your experiments. So this comes from
[01:03:25.040 --> 01:03:28.720]   if you've logged into or if you've ever created an account
[01:03:28.720 --> 01:03:34.240]   inside of Weights and Biases, you can check out the
[01:03:34.240 --> 01:03:40.240]   integrations here. We work, Weights and Biases integrates
[01:03:40.240 --> 01:03:44.960]   really well with Keras. So all you have to do to track your
[01:03:44.960 --> 01:03:49.600]   experiments is just pass on the Weights and Biases callback. So
[01:03:49.600 --> 01:03:53.640]   Weights and Biases is an experiment tracking tool that is
[01:03:53.640 --> 01:03:56.800]   quite useful and a lifesaver when you're doing a lot of
[01:03:56.800 --> 01:04:01.360]   experiments. Inside of the book, Tensorflow's Tensorboard is
[01:04:01.360 --> 01:04:03.920]   suggested, I would suggest you to also check out Weights and
[01:04:03.920 --> 01:04:08.520]   Biases. It does a few things beyond that. You can track your
[01:04:08.520 --> 01:04:12.080]   experiments, you can version your data sets, you can evaluate
[01:04:12.080 --> 01:04:15.400]   your models, you can optimize your models. And then you can
[01:04:15.400 --> 01:04:19.200]   also write reports around it. So it basically is the perfect
[01:04:19.200 --> 01:04:24.480]   collaborative version for software 2.0 if I may. Now the
[01:04:24.480 --> 01:04:28.080]   only thing you have to do to get all of this data right when
[01:04:28.080 --> 01:04:32.280]   you're training your model and you want a nice summary that you
[01:04:32.280 --> 01:04:36.760]   can share with a co worker. I'm trying to find the link to the
[01:04:36.760 --> 01:04:45.720]   dashboard that this would have created for me. My apologies, I
[01:04:45.720 --> 01:04:55.880]   can't find it among the list of open things. This is the part
[01:04:55.880 --> 01:05:16.360]   where I embarrass myself in front of everyone. I think this
[01:05:17.360 --> 01:05:28.080]   was the one I ran. Yep, there we go. Maybe I'm confusing
[01:05:28.080 --> 01:05:31.280]   myself again. Sorry, give me one second to quickly find the
[01:05:31.280 --> 01:05:38.360]   correct dashboard. The meantime, I see a question. Will it save
[01:05:38.360 --> 01:05:41.560]   the model weights after two epochs of patience or when it
[01:05:41.560 --> 01:05:44.840]   stopped improving? So that depends on how you define I
[01:05:44.840 --> 01:05:52.600]   will call back where we're checkpointing the model. I'll
[01:05:52.600 --> 01:05:55.680]   share the documentation and can take a look there. That depends
[01:05:55.680 --> 01:06:00.320]   on how you define it, it'll save the model at every epoch. So
[01:06:00.320 --> 01:06:03.680]   it'll be saved quite a few times, but you'll have the
[01:06:03.680 --> 01:06:07.800]   benefit of having a lot of not not going back a lot in time to
[01:06:09.880 --> 01:06:16.760]   find the correct model. I found the correct dashboard. So I'll
[01:06:16.760 --> 01:06:21.200]   go back to sharing that. This is the risk I run of embarrassing
[01:06:21.200 --> 01:06:24.880]   myself, but I'm happy to do that just to host a lecture every
[01:06:24.880 --> 01:06:28.440]   weekend. So here's the nice thing about tracking experiments
[01:06:28.440 --> 01:06:31.600]   if assuming you have a lot of these, it's really easy to
[01:06:31.600 --> 01:06:35.000]   visualize how things change with every epochs, how your
[01:06:35.000 --> 01:06:39.440]   validation accuracy is being changed, or what examples are
[01:06:39.440 --> 01:06:43.240]   you working with. This doesn't look like a frog. It's quite
[01:06:43.240 --> 01:06:46.080]   terrifying to me. This looks like a German shepherd somewhat.
[01:06:46.080 --> 01:06:48.880]   Don't worry, your YouTube connection is fine. These images
[01:06:48.880 --> 01:06:53.960]   are pixelated because it's CIFAR 10. This definitely doesn't
[01:06:53.960 --> 01:06:57.600]   look like a frog to me. It's probably a bus. But again, it's
[01:06:57.600 --> 01:07:03.000]   this visual process of understanding what's inside of
[01:07:03.000 --> 01:07:06.320]   your model, right? So you can do that with weights and biases.
[01:07:06.320 --> 01:07:11.280]   And I'll point everyone to check out the 1db callback, which you
[01:07:11.280 --> 01:07:15.760]   can pass inside of the model fit loop. Again, that's there in the
[01:07:15.760 --> 01:07:18.760]   examples. I wanted to run through this. But in interest of
[01:07:18.760 --> 01:07:23.760]   my ambitious agenda, I'll continue going through the
[01:07:23.760 --> 01:07:29.280]   collabs. So Harpreet, let's answer your questions. Your
[01:07:29.280 --> 01:07:40.400]   question was around model checkpointing. I see it there.
[01:07:40.400 --> 01:07:48.320]   And in the meantime, let's highlight this. So callback to
[01:07:48.320 --> 01:07:54.080]   save Keras model or wait at some frequency, right? Whether to
[01:07:54.080 --> 01:07:59.240]   keep the model that has achieved the best performance so far,
[01:07:59.240 --> 01:08:02.920]   or whether to save the model at the end of every epoch,
[01:08:02.920 --> 01:08:07.160]   regardless of performance. So I apologize, I misremembered. You
[01:08:07.160 --> 01:08:11.240]   have the option of sharing the best model or the model at every
[01:08:11.240 --> 01:08:16.960]   step. Definition of best would be which quantity you want to
[01:08:16.960 --> 01:08:22.000]   monitor to be maximized or minimized. I hope this answers
[01:08:22.000 --> 01:08:24.720]   your question and I'll continue but if it doesn't, please
[01:08:24.720 --> 01:08:25.520]   interrupt me again.
[01:08:25.520 --> 01:08:34.480]   Awesome. So we've understood how we can monitor and
[01:08:34.480 --> 01:08:38.680]   visualize as experiments using a simple callback. And we've
[01:08:38.680 --> 01:08:43.600]   understood what callbacks are, we've looked at how to create a
[01:08:43.600 --> 01:08:46.880]   model using sequential, functional and subclassing
[01:08:46.880 --> 01:08:53.800]   approaches. So let's continue further. From there, we write a
[01:08:53.800 --> 01:08:56.640]   complete training and evaluation loop, which I'll skip because
[01:08:56.640 --> 01:09:01.600]   it's quite straightforward. You can also read the chapter
[01:09:01.600 --> 01:09:05.840]   further to understand how to write a step by step evaluation
[01:09:05.840 --> 01:09:10.680]   loop. The thing I want to point out here is you can call tf.
[01:09:10.680 --> 01:09:17.880]   function. What does this do? This, especially with your model
[01:09:17.880 --> 01:09:23.240]   definitions, tell Keras, tells TensorFlow actually, to compile
[01:09:23.240 --> 01:09:27.000]   your model ahead of time, this makes it run a little faster.
[01:09:27.000 --> 01:09:33.000]   Because instead of running it in an eager mode, or in an
[01:09:33.000 --> 01:09:38.000]   interpreted mode, it compiles the model ahead of time. If you
[01:09:38.000 --> 01:09:49.800]   come from the world of, I want to say, I want to say JAX, but
[01:09:49.800 --> 01:09:52.120]   JAX is a little different. If you come from the world of
[01:09:52.120 --> 01:09:56.840]   PyTorch, similar to JITing things. I think PyTorch derived
[01:09:56.840 --> 01:09:59.880]   the inspiration from here again, without going into the too
[01:09:59.880 --> 01:10:04.000]   political details, but it's again, it's similar across all
[01:10:04.000 --> 01:10:08.080]   frameworks. So what's going on here with telling TensorFlow,
[01:10:08.080 --> 01:10:14.400]   please, please check out this model and do whatever you can to
[01:10:14.400 --> 01:10:18.880]   make it run faster. So the software engineering team behind
[01:10:19.200 --> 01:10:23.440]   TensorFlow has written very smart code that knows things it
[01:10:23.440 --> 01:10:27.640]   can optimize. It goes ahead compiles. So internally,
[01:10:27.640 --> 01:10:32.560]   TensorFlow looks at your model, I see matrix multiplication, we
[01:10:32.560 --> 01:10:36.480]   can do that faster. I'll use a different approach internally to
[01:10:36.480 --> 01:10:42.680]   train your model faster. There's value that can be made. I'm
[01:10:42.680 --> 01:10:46.680]   just throwing stuff off my head. But here's an approach, we can
[01:10:46.680 --> 01:10:49.960]   compile it to make it run faster. All of that happens
[01:10:49.960 --> 01:10:52.920]   behind the scenes. You don't do anything, you just write this
[01:10:52.920 --> 01:10:59.360]   decorator at the function and things are taken care of. It's
[01:10:59.360 --> 01:11:02.720]   easy to write this everywhere. So I don't have a good
[01:11:02.720 --> 01:11:05.880]   suggestion because I'm learning myself of fear and we're not to
[01:11:05.880 --> 01:11:14.040]   use this. But if used correctly, it can speed things up. Awesome.
[01:11:14.240 --> 01:11:18.760]   So that's the end of chapter seven. Any questions so far
[01:11:18.760 --> 01:11:19.800]   pertaining to this?
[01:11:19.800 --> 01:11:36.880]   When we will not use this approach?
[01:11:39.560 --> 01:11:45.680]   Harpreet, I have spent more time in PyTorch in my life. And even
[01:11:45.680 --> 01:11:49.240]   there, I don't know where not to jit things. Many times I forget
[01:11:49.240 --> 01:11:57.280]   to jit things. That's the honest answer I have. So I don't have
[01:11:57.280 --> 01:12:01.520]   the best advice here. I'll try to find an answer through the
[01:12:01.520 --> 01:12:06.000]   Keras experts I know. But off the top of my head, I apologize.
[01:12:06.000 --> 01:12:12.960]   I don't have the best answer to this. Even with JAX, I think
[01:12:12.960 --> 01:12:18.560]   this is easy to pj everything there. It's called pj in JAX.
[01:12:18.560 --> 01:12:22.720]   We have a JAX study group if you want to learn from there. So the
[01:12:22.720 --> 01:12:26.840]   next thing we learn is we just studied about convolutions. I
[01:12:26.840 --> 01:12:33.080]   showed it to you inside of Microsoft Excel. I showed it to
[01:12:33.080 --> 01:12:37.040]   you visually as well. We learned how different layers learn
[01:12:37.040 --> 01:12:40.360]   what's happening inside of them. Now we'll do the easiest part
[01:12:40.360 --> 01:12:46.040]   which is we learn how they work in code. I also have this blog
[01:12:46.040 --> 01:12:49.800]   open which gives another quick reminder. This is how the
[01:12:49.800 --> 01:12:54.360]   outputs are performed or calculated from a neural
[01:12:54.360 --> 01:12:59.920]   network. Thank you Irhum for this blog. So a CNN builds the
[01:12:59.920 --> 01:13:03.720]   outputs inside of the model like so. You don't have to worry
[01:13:03.720 --> 01:13:08.880]   about doing any of that because in Keras, you just import the
[01:13:08.880 --> 01:13:13.440]   layers and you throw in conv2d layers, followed by max pooling.
[01:13:13.440 --> 01:13:17.840]   Do that a bunch of times and your model is ready. So the
[01:13:17.840 --> 01:13:21.800]   theoretical knowledge is important because you need to
[01:13:21.800 --> 01:13:25.080]   understand how these works. Remember, we looked at the
[01:13:25.080 --> 01:13:28.600]   kernel sizes, we looked at the activation, we look at the
[01:13:28.600 --> 01:13:32.840]   number of filters. It's important to know these. But
[01:13:32.840 --> 01:13:35.960]   inside of Keras, it's really easy to implement, you just
[01:13:35.960 --> 01:13:40.480]   create a conv2d layer. And that's a convolution layer that
[01:13:40.480 --> 01:13:43.680]   has filters stored inside of it that perform the action that we
[01:13:43.680 --> 01:13:47.520]   discussed earlier. And they basically take care of all of
[01:13:47.520 --> 01:13:51.920]   the things we were concerned about. You can print out the
[01:13:51.920 --> 01:13:54.880]   summary, of course, take a look at all of these details inside
[01:13:54.880 --> 01:13:58.920]   of the chapter again, glancing over the details, we prove the
[01:13:58.920 --> 01:14:02.760]   point that these perform better while using lesser parameters,
[01:14:02.760 --> 01:14:07.120]   right? If you remember my slides, I claimed that fully
[01:14:07.120 --> 01:14:15.040]   connected models use more parameters. So I think GSS had
[01:14:15.040 --> 01:14:18.640]   asked this question, it's their alias, I assume of how do you
[01:14:18.640 --> 01:14:24.760]   find those parameters, I would encourage you to create a
[01:14:24.760 --> 01:14:27.600]   fully connected network, see how many parameters that has
[01:14:27.600 --> 01:14:30.760]   compared with this and compare the accuracies. That's what we
[01:14:30.760 --> 01:14:37.160]   do inside of the chapters. We understand border fit and
[01:14:37.160 --> 01:14:42.280]   padding, which I'll quickly explain. Or maybe I'll leave to
[01:14:42.280 --> 01:14:46.280]   the next bit. And from there, I believe the chapter goes into
[01:14:46.280 --> 01:14:54.000]   image augmentation. So I believe I'll jump towards explaining
[01:14:54.360 --> 01:15:03.120]   image augmentation. Next. Further in the chapter, the
[01:15:03.120 --> 01:15:09.760]   thing I'm skipping to come back to later is how do you read in
[01:15:09.760 --> 01:15:13.560]   these images? Because again, it's all numbers, right? How do
[01:15:13.560 --> 01:15:17.960]   you feed this data effectively to a model, you could have
[01:15:17.960 --> 01:15:22.400]   millions of images. Have you tried copying a million images?
[01:15:23.360 --> 01:15:26.400]   Have you tried that windows just crashes, you just don't see
[01:15:26.400 --> 01:15:29.400]   the time it's it's always calculating. How do you do that
[01:15:29.400 --> 01:15:35.040]   effectively? Inside of a neural network is also important to
[01:15:35.040 --> 01:15:38.200]   know, right? You could read it from a dictionary, there are
[01:15:38.200 --> 01:15:41.520]   different approaches. Of course, there would be functions that do
[01:15:41.520 --> 01:15:48.160]   this in a very optimized fashion. Right? So this is one
[01:15:48.160 --> 01:15:53.320]   of the ways where we can use a Keras utility that imports from
[01:15:53.320 --> 01:16:00.320]   the directory. And we can simply pass in the input data set.
[01:16:00.320 --> 01:16:07.160]   Many times you have in the real world, especially you have this
[01:16:07.160 --> 01:16:11.120]   problem of not having the data set in the right function. So
[01:16:11.120 --> 01:16:15.240]   now you have two problems, right? Either you put the data
[01:16:15.240 --> 01:16:21.120]   in the correct format. Sure. And then just use a utility or write
[01:16:21.120 --> 01:16:26.560]   a custom data set class for for doing this. So there are two
[01:16:26.560 --> 01:16:37.000]   sides of things to this problem. We'll come back to the data set
[01:16:37.000 --> 01:16:42.280]   class. But now I want to point out image augmentation. So I'll
[01:16:42.280 --> 01:16:46.320]   do that. I'll give a presentation of image
[01:16:46.320 --> 01:16:49.640]   augmentation using the documentations of augmentation.
[01:16:49.640 --> 01:16:53.840]   This is a framework by really smart people who fund Kaggle
[01:16:53.840 --> 01:16:59.480]   competition who are working at incredible industries. And it's
[01:16:59.480 --> 01:17:03.120]   being used across many amazing places as well. Many amazing
[01:17:03.120 --> 01:17:07.280]   industries, I believe it does a lot of great things. It's called
[01:17:07.280 --> 01:17:10.800]   augmentation.ai. It works really well, both with PyTorch and
[01:17:10.800 --> 01:17:17.760]   Keras. So I see many of our community folks, I see Yuvraj, I
[01:17:17.760 --> 01:17:21.880]   see Mayank who's been with us in the PyTorch study group as well.
[01:17:21.880 --> 01:17:26.080]   We learned about augmentation there. And since it works with
[01:17:26.080 --> 01:17:31.720]   Keras, we'll also look at it again now. So Ayushman, who is I
[01:17:31.720 --> 01:17:36.160]   think a Kaggle expert or master, he won a gold medal in a
[01:17:36.160 --> 01:17:40.080]   competition about which we learned from him. He's created
[01:17:40.080 --> 01:17:44.240]   this nice demo that lives inside of the documentation. So we'll
[01:17:44.240 --> 01:17:52.480]   use that to understand how to work with augmentation in
[01:17:52.480 --> 01:18:03.280]   TensorFlow. The reason we want to first of all do any of this
[01:18:03.280 --> 01:18:10.720]   is in the last case study we studied about, we could always
[01:18:11.080 --> 01:18:15.320]   get more data to make our life easier. If you don't have more
[01:18:15.320 --> 01:18:21.200]   data, you can create more data. How do you do that by changing
[01:18:21.200 --> 01:18:25.560]   the image ever so slightly. So you take this image that you're
[01:18:25.560 --> 01:18:29.240]   seeing right now. And if you're trying to build a same detector,
[01:18:29.240 --> 01:18:35.440]   you can rotate this image, you can twist it by a few degrees,
[01:18:35.440 --> 01:18:39.480]   I'm twisting myself I can't, or I actually can rotate my webcam,
[01:18:39.480 --> 01:18:45.720]   which I won't dare to do on a live stream. Those small changes
[01:18:45.720 --> 01:18:52.120]   are enough to improve your model a little bit. Our end goal is to
[01:18:52.120 --> 01:18:56.920]   be able to let's say detect me in any image. If I'm always in
[01:18:56.920 --> 01:18:59.720]   the center of this image, first of all, that model doesn't work
[01:18:59.720 --> 01:19:06.200]   well. So there's a problem with what we call generalization. To
[01:19:06.240 --> 01:19:10.160]   tackle that, you could cut this image off in the right hand
[01:19:10.160 --> 01:19:15.120]   corner. So here's like an irritating way of doing it by
[01:19:15.120 --> 01:19:18.160]   just covering it, but you could just paint all of these pixels
[01:19:18.160 --> 01:19:24.400]   black. So you need ways of doing that and argumentations is a
[01:19:24.400 --> 01:19:28.840]   framework that allows that. Keras also has augmentations
[01:19:28.840 --> 01:19:33.800]   built inside of it. So here are the input images, I won't go
[01:19:33.800 --> 01:19:36.840]   into details of, hey, here's what's going on inside of the
[01:19:36.840 --> 01:19:39.000]   code, I leave you to understanding it. My goal here is
[01:19:39.000 --> 01:19:42.320]   to explain what augmentations are. So we read in all of these
[01:19:42.320 --> 01:19:47.360]   images, the sunflowers, I see flowers, I see a person with a
[01:19:47.360 --> 01:19:51.040]   sunflower. Interesting, I see a dog with a sunflower. Is that a
[01:19:51.040 --> 01:19:56.200]   dog? I apologize if that's a human to the human inside of
[01:19:56.200 --> 01:20:00.480]   that image. So we build pipelines that could do a lot of
[01:20:00.480 --> 01:20:05.160]   things. Right. I mentioned earlier, you could rotate an
[01:20:05.160 --> 01:20:11.000]   image, so you could rotate the image by a few degrees, you
[01:20:11.000 --> 01:20:16.800]   could change the brightness by 10%. That can do wonders for
[01:20:16.800 --> 01:20:20.600]   your neural network. Again, it's all just numbers, right, that
[01:20:20.600 --> 01:20:24.600]   your model is fitting or overfitting to. So to work with
[01:20:24.600 --> 01:20:27.240]   those or to take care of those things, you can change the
[01:20:27.240 --> 01:20:32.200]   compression, the hue saturation, you can change the contrast, you
[01:20:32.200 --> 01:20:36.960]   can flip an image horizontally. That's one thing annoying. I'll
[01:20:36.960 --> 01:20:40.360]   confess this in a live stream. Have you ever tried shaving?
[01:20:40.360 --> 01:20:44.320]   This has happened to me in my college days. Have you tried
[01:20:44.320 --> 01:20:49.480]   shaving with a zoom call, like a zoom meeting where it's just you
[01:20:49.480 --> 01:20:53.040]   by yourself, the image gets mirrored. So it's more difficult
[01:20:53.040 --> 01:20:57.480]   to do that. Or in your sometimes with your phone, I want to say
[01:20:57.480 --> 01:21:00.680]   that happens. And if that's enough to confuse your
[01:21:00.680 --> 01:21:04.000]   humorbrain, it's good enough to confuse neural networks as well.
[01:21:04.000 --> 01:21:07.840]   So we create more data by horizontally flipping it, you
[01:21:07.840 --> 01:21:12.200]   could also vertically flip it. The next question becomes when
[01:21:12.200 --> 01:21:16.360]   do you want to do what? So if you're working with text, right,
[01:21:16.360 --> 01:21:19.840]   you won't want to flip it at all, because that loses its
[01:21:19.840 --> 01:21:24.000]   meaning. So depending on what problem you're working on, you
[01:21:24.000 --> 01:21:28.680]   need some amount of domain expertise, I want to say, to be
[01:21:28.680 --> 01:21:32.360]   able to understand what to perform. So you can do all of
[01:21:32.360 --> 01:21:40.520]   these transforms inside of the model. And let's see if these
[01:21:40.520 --> 01:21:45.760]   are printed out, I might be mixing the PyTorch and Keras
[01:21:45.760 --> 01:21:50.880]   example in my head. So no, these are not printed out. So to
[01:21:50.880 --> 01:21:56.040]   showcase another example, I will go to a blog that I had
[01:21:56.040 --> 01:22:03.000]   written. Again, my goal here is to tell you about a few image
[01:22:03.000 --> 01:22:07.720]   augmentations that exist, and for you to explore them. So I
[01:22:07.720 --> 01:22:12.280]   wrote this blog post that you don't need to read it, honestly,
[01:22:12.280 --> 01:22:16.200]   spend more of your time writing this in Keras, it's a waste of
[01:22:16.200 --> 01:22:19.480]   time to read this blog, I'm giving you enough gist that you
[01:22:19.480 --> 01:22:23.800]   just need. So here are a few suggested image augmentations
[01:22:23.800 --> 01:22:26.320]   that are possible across different frameworks. I'm not
[01:22:26.320 --> 01:22:31.960]   focusing on the framework, this was inside of FastAI. But here
[01:22:31.960 --> 01:22:34.760]   are the possible things you could do in FastAI, try to do
[01:22:34.760 --> 01:22:40.480]   this in Keras suggested homework. You can rotate your
[01:22:40.480 --> 01:22:43.680]   image like so our goal is to create more examples for the
[01:22:43.680 --> 01:22:46.720]   network to learn from. So one of the possible image
[01:22:46.720 --> 01:22:54.720]   augmentations is rotate. Let's see if Keras has that. Let me
[01:22:54.720 --> 01:22:58.480]   move this step closer to the documentation, go here and see
[01:22:58.480 --> 01:23:06.880]   if rotate exists. And we found it. So this is inside of image
[01:23:06.880 --> 01:23:09.440]   pre-processing and augmentation. Looks like we found that here.
[01:23:09.600 --> 01:23:15.920]   So you can also do rescaling center crop and a bunch more of
[01:23:15.920 --> 01:23:19.160]   more things. Right. So as you can see, these also exist
[01:23:19.160 --> 01:23:24.480]   inside of Keras. So you can rotate your image, a real world
[01:23:24.480 --> 01:23:29.840]   use case of this would be if you're working with a secret
[01:23:29.840 --> 01:23:32.640]   agency trying to find all of the swimming pools in your area
[01:23:32.640 --> 01:23:35.880]   with satellite images rotation actually helps improve the
[01:23:35.880 --> 01:23:41.960]   accuracy. As you can see, you can make a cat or trippy cat.
[01:23:41.960 --> 01:23:45.760]   For certain use cases, this might help. I don't think for
[01:23:45.760 --> 01:23:50.080]   this cat detector, it's helpful at all, but you can change the
[01:23:50.080 --> 01:23:54.080]   colors. You can adjust the brightness. So if you observe
[01:23:54.080 --> 01:23:58.960]   that, you know, the images existing inside of your model,
[01:23:58.960 --> 01:24:03.680]   all just have two bright pictures, you can change the
[01:24:03.680 --> 01:24:07.840]   brightness to a lower value. Or you can just play around with
[01:24:07.840 --> 01:24:12.600]   the value still. And that's helpful to see model for certain
[01:24:12.600 --> 01:24:18.200]   cases. My example was I was trying to segment text from a
[01:24:18.200 --> 01:24:21.880]   background and actually sometimes changing the
[01:24:21.880 --> 01:24:29.560]   brightness is helpful if there's a bit of shine on the text. You
[01:24:29.560 --> 01:24:33.560]   can change the contrast. So the example would be if you're
[01:24:33.560 --> 01:24:38.080]   trying to segment text, increasing contrast most of the
[01:24:38.080 --> 01:24:42.520]   time is really helpful. You could crop into image, you could
[01:24:42.520 --> 01:24:46.400]   crop and pad the image. So we crop inside of the image and
[01:24:46.400 --> 01:24:53.240]   pad with zeros. So that's black. My painful chemistry classes
[01:24:53.240 --> 01:24:57.200]   always remind me of a dihedron which has four angles, eight
[01:24:57.200 --> 01:25:01.200]   angles. See, I still don't remember that. You could rotate
[01:25:01.200 --> 01:25:05.360]   your image using a dihedral transform. You could add jitter
[01:25:05.360 --> 01:25:09.120]   which is like adding noise. If you remember the OG days of
[01:25:09.120 --> 01:25:14.760]   watching television. This is how distorted television would look
[01:25:14.760 --> 01:25:19.480]   like. And that's what jitter does. It just adds jitter. So
[01:25:19.480 --> 01:25:23.280]   these are a few, I have more in the blog, but these are a few
[01:25:23.280 --> 01:25:28.360]   possible image augmentations that exist in fasta. That's a
[01:25:28.360 --> 01:25:32.480]   pytorch based framework. But these would also exist in
[01:25:32.480 --> 01:25:37.880]   Keras. I want to say JAX but I don't think they might be in
[01:25:37.880 --> 01:25:42.440]   JAX. Again, the goal here was to understand how image
[01:25:42.440 --> 01:25:45.800]   augmentations, first of all, what they are, how they work and
[01:25:45.800 --> 01:25:49.440]   how are they helpful. So if they are helpful is for you to
[01:25:49.440 --> 01:25:52.720]   figure out, play around with the data set, add them and see if
[01:25:52.720 --> 01:25:57.440]   it helps. It's, it's disappointing sometimes to see
[01:25:57.440 --> 01:26:01.040]   it in practice, because the improvement is really small,
[01:26:01.040 --> 01:26:10.120]   honestly speaking. So that's how image augmentations work. And
[01:26:10.120 --> 01:26:17.400]   you can, of course, call these from Keras layers as well. I
[01:26:17.400 --> 01:26:22.440]   think this is a good point to pause our adventures. I
[01:26:22.440 --> 01:26:28.280]   apologize, because I had again, said this very ambitious agenda
[01:26:28.280 --> 01:26:31.960]   and we got into answering questions. So we're stopping
[01:26:31.960 --> 01:26:36.320]   before implementing a cutting edge architecture. But I'll
[01:26:36.320 --> 01:26:42.400]   give you enough of a teaser if I can. So this was done by
[01:26:43.160 --> 01:26:46.240]   Shoumik, who is my colleague at
[01:26:46.240 --> 01:26:53.680]   Myths and Biases. Shoumik has been doing incredible work in
[01:26:53.680 --> 01:26:58.040]   the world of Keras. He's really smart, and he has been
[01:26:58.040 --> 01:27:02.080]   contributing to the examples inside of the Keras repository.
[01:27:02.080 --> 01:27:07.240]   You should trust him much more for implementations around Keras
[01:27:07.240 --> 01:27:11.840]   than you should trust me. So Shoumik has written this code
[01:27:11.840 --> 01:27:18.040]   that he trusted me to steal from him, which I did. This is a
[01:27:18.040 --> 01:27:21.800]   cutting edge architecture that got a lot of hype. I don't
[01:27:21.800 --> 01:27:25.520]   expect you to be able to read this code. But I want to point
[01:27:25.520 --> 01:27:28.760]   out the fact that, hey, nothing, nothing fancy is going on here,
[01:27:28.760 --> 01:27:33.680]   right? We've, we see similar imports that we've been working
[01:27:33.680 --> 01:27:38.640]   with, we just define a conf block, inside of which we use
[01:27:38.640 --> 01:27:42.960]   the functional API, we do a few things. We write a few
[01:27:42.960 --> 01:27:49.960]   conditions, we define a few more blocks. Block inside of a
[01:27:49.960 --> 01:27:54.680]   model is a collection of layers. We can define an
[01:27:54.680 --> 01:27:59.480]   initializer, and then finally build the model. So just using
[01:27:59.480 --> 01:28:02.320]   these few lines of code again, rushing through it, we'll come
[01:28:02.320 --> 01:28:06.480]   back and take a really deeper look at this. Shoumik, I
[01:28:06.480 --> 01:28:09.920]   believe hasn't published this yet. And he will publish it
[01:28:09.920 --> 01:28:19.720]   soon, of course. Just by doing all of these steps, you get one
[01:28:19.720 --> 01:28:24.400]   of the most recent architectures implemented in Keras. So I'm
[01:28:24.400 --> 01:28:27.120]   not a motivational speaker, but I hope that was motivating
[01:28:27.120 --> 01:28:29.800]   enough for you to understand that we're at this point where
[01:28:29.800 --> 01:28:35.160]   we can build really good CNN models, we've learned enough. We
[01:28:35.160 --> 01:28:37.440]   of course don't know what this model is, I didn't give you that
[01:28:37.440 --> 01:28:40.640]   context. That's on me. I didn't tell you what's going on inside
[01:28:40.640 --> 01:28:44.600]   of this code. We'll come back and understand that. Hopefully,
[01:28:44.600 --> 01:28:47.120]   by then Shoumik would have written a really good blog post.
[01:28:47.120 --> 01:28:52.200]   So I really don't have to do anything. I will try to wrap
[01:28:52.200 --> 01:28:56.800]   things up from here and suggest a few homework. So we have gone
[01:28:56.800 --> 01:28:59.960]   up until I think the initial bit of chapter nine, where we know a
[01:28:59.960 --> 01:29:02.520]   few things about computer vision. Next time, we'll be
[01:29:02.520 --> 01:29:08.960]   taking a look from chapter nine to 11. Again, ambitious agenda,
[01:29:08.960 --> 01:29:14.000]   I know, but we'll also try to cover. Next, homework for you
[01:29:14.000 --> 01:29:17.520]   all, implement a CNN architecture in Keras. I hope I
[01:29:17.520 --> 01:29:21.320]   gave the motivation that you can actually build these things. We
[01:29:21.320 --> 01:29:25.240]   looked at four cells inside of a Colab notebook, written by a
[01:29:25.240 --> 01:29:28.800]   very smart person smarter than I am, which can implement this
[01:29:28.840 --> 01:29:33.160]   model. I would encourage you to start something to Kaggle. I
[01:29:33.160 --> 01:29:36.160]   think Franchot gave you a taste as well inside of the chapter.
[01:29:36.160 --> 01:29:38.720]   It's it's the most addictive thing in machine learning, if
[01:29:38.720 --> 01:29:42.760]   not machine learning itself. This is a point where you can
[01:29:42.760 --> 01:29:46.360]   start playing around with Kaggle competitions as well. So go to
[01:29:46.360 --> 01:29:53.000]   Kaggle, see if there are any image competitions that are of
[01:29:53.000 --> 01:29:56.160]   interest. I see a few questions coming in. I'll address those
[01:29:56.160 --> 01:30:01.360]   before ending. Play with augmentations, try to log
[01:30:01.360 --> 01:30:04.760]   experiments with weights and biases. And try to translate
[01:30:04.760 --> 01:30:08.520]   models from pytorch examples to Keras examples. These aren't
[01:30:08.520 --> 01:30:14.040]   good PRs, these are just for your practice and practice. Or
[01:30:14.040 --> 01:30:19.840]   spending more time in Colab or your own Jupyter notebook is how
[01:30:19.840 --> 01:30:25.400]   you'll get better at, I believe, writing code in Keras. Consider
[01:30:25.560 --> 01:30:29.240]   sharing these learnings in 27 days of Keras. Again, we at
[01:30:29.240 --> 01:30:32.840]   Weights and Biases are happy to send swag every week to people
[01:30:32.840 --> 01:30:37.320]   who write good blog posts. There's no, there's no agenda
[01:30:37.320 --> 01:30:41.320]   for us here. We genuinely want all of you watching this from
[01:30:41.320 --> 01:30:45.000]   the community to spend more time learning. And this is just our
[01:30:45.000 --> 01:30:48.800]   way of trying to encourage you all to participate more in
[01:30:48.800 --> 01:30:55.400]   community activities or trying to create more content. So
[01:30:55.400 --> 01:30:58.640]   let's go to the questions real quick before we wrap up. I know
[01:30:58.640 --> 01:31:03.800]   we're over time. So I'll try to wrap up faster. Ankit is asking,
[01:31:03.800 --> 01:31:09.160]   do we prefer to have a balanced data set as well as good data so
[01:31:09.160 --> 01:31:13.800]   that our model is well trained? With augmentation, we are
[01:31:13.800 --> 01:31:17.280]   distorting the data making it messier. So would too much
[01:31:17.280 --> 01:31:22.200]   augmentation spoil the data? That's a great question. And
[01:31:22.240 --> 01:31:25.080]   again, it'll come through practice, right? You can only
[01:31:25.080 --> 01:31:32.080]   find these answers through practice. Usually, unless it's a
[01:31:32.080 --> 01:31:35.680]   very small data set. So where you have like one example versus
[01:31:35.680 --> 01:31:39.600]   1000 examples, things get really interesting, you might want to
[01:31:39.600 --> 01:31:47.960]   look into Siamese networks. This was the last thing I knew, I'm
[01:31:47.960 --> 01:31:53.720]   assuming many new things would have come up. With augmentation,
[01:31:53.720 --> 01:31:56.680]   again, yes, you could, of course, totally mess up the data
[01:31:56.680 --> 01:32:00.680]   set as well. In practice, you apply this to like a fraction of
[01:32:00.680 --> 01:32:04.280]   the input. And you applied like in a very small percentage
[01:32:04.280 --> 01:32:09.200]   randomly. So it like, if you read any paper, it like gives
[01:32:09.200 --> 01:32:14.080]   you a boost of at best one or 2%. Again, that's where this
[01:32:14.080 --> 01:32:19.160]   starts to come into play. So great question. Again, it you
[01:32:19.160 --> 01:32:21.720]   can only find the answer through experimentation on your own
[01:32:21.720 --> 01:32:27.440]   data set. What writing pad am I using? I don't remember the name
[01:32:27.440 --> 01:32:30.440]   of this. I think I just went to Amazon and searched for writing
[01:32:30.440 --> 01:32:37.240]   parts that work well with macOS. I'll try to find the answer and
[01:32:37.240 --> 01:32:40.080]   share it with you. And maybe you have to check my order history.
[01:32:42.680 --> 01:32:45.280]   I see a few questions from earlier that you Raj has
[01:32:45.280 --> 01:32:50.840]   answered. Thank you, Raj. I don't see any other remaining
[01:32:50.840 --> 01:32:53.440]   questions. So again, thanks everyone for joining. I'll see
[01:32:53.440 --> 01:32:59.480]   you next week. We learn about context, implement or go to the
[01:32:59.480 --> 01:33:02.800]   implementation by someone smarter than me of the
[01:33:02.800 --> 01:33:08.920]   architecture in Keras. And we'll try to learn about time series
[01:33:08.920 --> 01:33:16.400]   as well next week. I see one final comment. You Raj has
[01:33:16.400 --> 01:33:18.560]   answered your question, I believe on kids. Thanks again,
[01:33:18.560 --> 01:33:23.000]   Raj. Awesome. We'll meet again next week. Same time consider
[01:33:23.000 --> 01:33:25.520]   doing the homework, consider writing a blog, consider sharing
[01:33:25.520 --> 01:33:29.120]   it with the community. Thank you for joining again. Let's meet
[01:33:29.120 --> 01:33:34.360]   next week again to continue learning Keras and thanks for
[01:33:34.360 --> 01:33:34.720]   your time.
[01:33:34.720 --> 01:33:35.560]   for your time.
[01:33:35.560 --> 01:33:45.560]   [BLANK_AUDIO]

