
[00:00:00.000 --> 00:00:05.360]   [MUSIC PLAYING]
[00:00:05.360 --> 00:00:06.840]   Welcome, everybody, and thank you
[00:00:06.840 --> 00:00:09.360]   for joining the What's New with Weights and Biases product
[00:00:09.360 --> 00:00:10.080]   event.
[00:00:10.080 --> 00:00:11.600]   My name is Phil Gerbacke.
[00:00:11.600 --> 00:00:14.160]   I lead the product team here at Weights and Biases,
[00:00:14.160 --> 00:00:15.640]   and I'm super excited to showcase
[00:00:15.640 --> 00:00:17.680]   all of the latest and greatest innovation
[00:00:17.680 --> 00:00:21.160]   that our R&D team has delivered these past few months.
[00:00:21.160 --> 00:00:23.200]   We've been working tirelessly to make
[00:00:23.200 --> 00:00:26.160]   our product the best possible product for our users.
[00:00:26.160 --> 00:00:29.040]   The first thing I want to do, I want to get started maybe
[00:00:29.040 --> 00:00:30.920]   a little bit of engagement from the audience.
[00:00:30.920 --> 00:00:32.880]   So can everybody share in the chat
[00:00:32.880 --> 00:00:35.560]   your favorite ML framework, large language
[00:00:35.560 --> 00:00:36.800]   model, or generative model?
[00:00:36.800 --> 00:00:39.120]   What are you all using most recently?
[00:00:39.120 --> 00:00:41.160]   What's everybody using?
[00:00:41.160 --> 00:00:43.120]   What are the latest and greatest technologies
[00:00:43.120 --> 00:00:46.080]   that you've been adopting and taking advantage of?
[00:00:46.080 --> 00:00:48.680]   Just type in the chat your favorite or most interesting
[00:00:48.680 --> 00:00:51.240]   ML technology so we can all get a sense of what's being
[00:00:51.240 --> 00:00:53.880]   used out there in the market.
[00:00:53.880 --> 00:00:56.560]   I should also mention that throughout the day,
[00:00:56.560 --> 00:00:58.200]   ask questions along the way.
[00:00:58.200 --> 00:00:59.640]   So please ask in the chat.
[00:00:59.640 --> 00:01:02.040]   We have a number of machine learning engineers
[00:01:02.040 --> 00:01:05.480]   ready to answer your questions as we're presenting.
[00:01:05.480 --> 00:01:08.560]   We'll also have a Q&A session at the end.
[00:01:08.560 --> 00:01:11.000]   And then one more thing, our amazing events team
[00:01:11.000 --> 00:01:14.560]   will be sending out swag bags for the best questions,
[00:01:14.560 --> 00:01:15.840]   the most engaged.
[00:01:15.840 --> 00:01:18.280]   So please make sure to keep this interactive,
[00:01:18.280 --> 00:01:21.080]   keep engagement high throughout the session.
[00:01:21.080 --> 00:01:23.680]   So let us all know, what are your favorite ML frameworks,
[00:01:23.680 --> 00:01:26.000]   large language models, or generative models
[00:01:26.000 --> 00:01:27.480]   that you're all using?
[00:01:27.480 --> 00:01:29.120]   For me, I've been super impressed
[00:01:29.120 --> 00:01:33.240]   with the likes of DALI 2, Stable Diffusion, Mid-Journey,
[00:01:33.240 --> 00:01:33.840]   and Crayon.
[00:01:33.840 --> 00:01:38.080]   It's truly remarkable what's being built by these teams.
[00:01:38.080 --> 00:01:40.560]   The innovations are really outstanding.
[00:01:40.560 --> 00:01:42.280]   The advancements in these models are really
[00:01:42.280 --> 00:01:43.960]   pushing the industry forward.
[00:01:43.960 --> 00:01:47.120]   And then for me personally, I'm just really proud to deliver
[00:01:47.120 --> 00:01:49.200]   product weights and biases that these teams are
[00:01:49.200 --> 00:01:51.560]   using to develop and debug their models,
[00:01:51.560 --> 00:01:54.200]   to collaborate and improve them, and accelerate their work.
[00:01:54.200 --> 00:01:56.760]   So that's really, really amazing.
[00:01:56.760 --> 00:01:58.600]   Let's jump in.
[00:01:58.600 --> 00:02:00.600]   So the first thing I would like to do
[00:02:00.600 --> 00:02:02.800]   is introduce this all-star lineup.
[00:02:02.800 --> 00:02:05.320]   So it's my pleasure to introduce all of the speakers
[00:02:05.320 --> 00:02:07.080]   and presenters today.
[00:02:07.080 --> 00:02:09.080]   First up, Kerry's going to walk us
[00:02:09.080 --> 00:02:12.120]   through the latest and greatest models capability.
[00:02:12.120 --> 00:02:15.960]   So using weights and biases to serve your entire end-to-end
[00:02:15.960 --> 00:02:18.440]   model lifecycle management needs.
[00:02:18.440 --> 00:02:20.800]   Next, Igor is going to demonstrate
[00:02:20.800 --> 00:02:25.120]   a brand new capability, Launch, the ultimate tool for ML
[00:02:25.120 --> 00:02:27.000]   workload orchestration.
[00:02:27.000 --> 00:02:29.040]   Stacey is going to highlight and demonstrate
[00:02:29.040 --> 00:02:31.280]   a number of fantastic improvements,
[00:02:31.280 --> 00:02:34.600]   including 3D bounding boxes, time series support,
[00:02:34.600 --> 00:02:36.840]   and a fully customizable profile page
[00:02:36.840 --> 00:02:39.640]   that you can publicly showcase all of your machine learning
[00:02:39.640 --> 00:02:40.280]   work.
[00:02:40.280 --> 00:02:42.560]   And then finally, Saf is going to highlight
[00:02:42.560 --> 00:02:44.520]   how the largest companies in the world,
[00:02:44.520 --> 00:02:46.760]   with the most sensitive and strict data governance
[00:02:46.760 --> 00:02:48.920]   and security policies, take advantage of weights
[00:02:48.920 --> 00:02:52.280]   and biases enterprise-grade platform.
[00:02:52.280 --> 00:02:54.600]   So with that, one thing that I'd like to talk about
[00:02:54.600 --> 00:02:57.960]   is how we're thinking about our vision and investment themes.
[00:02:57.960 --> 00:02:59.840]   And so if you think about weights and biases,
[00:02:59.840 --> 00:03:02.640]   really our mission is to build the best tools for ML
[00:03:02.640 --> 00:03:06.200]   developers, and really to create a developer-first ML Ops
[00:03:06.200 --> 00:03:08.200]   platform that serves the enterprise.
[00:03:08.200 --> 00:03:11.520]   And we're achieving this by focusing on three key areas.
[00:03:11.520 --> 00:03:15.080]   The first is a constant and consistent investment
[00:03:15.080 --> 00:03:16.880]   in user feedback and making sure we're
[00:03:16.880 --> 00:03:18.560]   building the best possible product
[00:03:18.560 --> 00:03:20.480]   for our community of users.
[00:03:20.480 --> 00:03:22.000]   And we're actioning on this feedback.
[00:03:22.000 --> 00:03:24.400]   Many of the innovations that you're going to see today
[00:03:24.400 --> 00:03:26.320]   have come right from your suggestions
[00:03:26.320 --> 00:03:29.480]   and based on your feedback when using the product.
[00:03:29.480 --> 00:03:31.840]   The second is expanding our platform
[00:03:31.840 --> 00:03:34.560]   to cover more of the most pressing pain points
[00:03:34.560 --> 00:03:37.240]   that you are experiencing and that our users experience
[00:03:37.240 --> 00:03:38.760]   across the entire machine learning
[00:03:38.760 --> 00:03:40.400]   lifecycle and workflow.
[00:03:40.400 --> 00:03:44.520]   And the third is our investment in an enterprise-grade platform
[00:03:44.520 --> 00:03:47.040]   to make sure that you can use weights and biases no matter
[00:03:47.040 --> 00:03:48.680]   where you work.
[00:03:48.680 --> 00:03:50.800]   And so it's my pleasure to now pass it over
[00:03:50.800 --> 00:03:53.720]   to Keri, who's going to walk us through models.
[00:03:53.720 --> 00:03:54.920]   So Keri, over to you.
[00:03:54.920 --> 00:03:57.200]   KERI ALTHOFF: Thanks for that great introduction, Phil.
[00:03:57.200 --> 00:04:01.560]   So on the WMV platform outline, you
[00:04:01.560 --> 00:04:06.040]   can see here we've got modular tools from having data sets
[00:04:06.040 --> 00:04:08.480]   that you're pre-processing and preparing
[00:04:08.480 --> 00:04:11.760]   through experiment tracking for developing new models,
[00:04:11.760 --> 00:04:16.120]   all the way to plans to build tools for managing models
[00:04:16.120 --> 00:04:19.400]   that are in production and governance on that end.
[00:04:19.400 --> 00:04:22.440]   And so what this more complete picture gives us
[00:04:22.440 --> 00:04:26.280]   is a set of modular tools that can solve pain points
[00:04:26.280 --> 00:04:28.440]   across the ML lifecycle.
[00:04:28.440 --> 00:04:30.680]   And so within this vision, today I'm
[00:04:30.680 --> 00:04:34.080]   going to highlight the model management component.
[00:04:34.080 --> 00:04:36.000]   And this is our key focus right now
[00:04:36.000 --> 00:04:39.080]   for improving the way people can manage the models that they're
[00:04:39.080 --> 00:04:42.200]   already logging to us, make it easier to coordinate
[00:04:42.200 --> 00:04:46.520]   and collaborate centrally, and ultimately have an automation
[00:04:46.520 --> 00:04:48.440]   cycle where you can automatically retrain
[00:04:48.440 --> 00:04:51.240]   a model when you have new data, or you can automatically
[00:04:51.240 --> 00:04:55.640]   evaluate and test a model once you have a new good candidate
[00:04:55.640 --> 00:04:57.920]   that you might want to replace the current model that's
[00:04:57.920 --> 00:04:59.360]   in production with.
[00:04:59.360 --> 00:05:02.800]   So what are the key pain points that we're hearing?
[00:05:02.800 --> 00:05:04.600]   Well, when we're talking to customers,
[00:05:04.600 --> 00:05:07.120]   a key theme of collaboration reproducibility
[00:05:07.120 --> 00:05:08.600]   consistently comes up.
[00:05:08.600 --> 00:05:11.960]   So issues like siloed projects, it's hard to discover.
[00:05:11.960 --> 00:05:14.080]   What are the existing relevant models
[00:05:14.080 --> 00:05:16.840]   that people have already trained my team?
[00:05:16.840 --> 00:05:19.840]   And how can I pull them down and use them?
[00:05:19.840 --> 00:05:22.920]   And if I can find those models, it's often not clear
[00:05:22.920 --> 00:05:26.440]   how to reproduce and reuse those previous pieces of work
[00:05:26.440 --> 00:05:29.200]   without something like a model card.
[00:05:29.200 --> 00:05:32.120]   We also have issues around governance and lineage
[00:05:32.120 --> 00:05:33.440]   frequently come up.
[00:05:33.440 --> 00:05:37.680]   So problems like having ad hoc handoffs between teams,
[00:05:37.680 --> 00:05:40.360]   where there's no standard process where, say,
[00:05:40.360 --> 00:05:43.200]   a researcher hands off a model to an ML engineer
[00:05:43.200 --> 00:05:45.320]   to then productionize.
[00:05:45.320 --> 00:05:47.720]   Sometimes that's even like sending a Slack message
[00:05:47.720 --> 00:05:51.440]   to say something is in the S3 bucket and go look at it.
[00:05:51.440 --> 00:05:55.040]   Now, that process also potentially
[00:05:55.040 --> 00:05:56.720]   causes unclear lineage.
[00:05:56.720 --> 00:05:59.360]   If there's this ad hoc handoff, then maybe it's
[00:05:59.360 --> 00:06:02.240]   not clear where a model really came from.
[00:06:02.240 --> 00:06:05.480]   What's the exact history, the code, the data set versions,
[00:06:05.480 --> 00:06:07.240]   everything that happened upstream
[00:06:07.240 --> 00:06:09.800]   to produce a given set of model weights?
[00:06:09.800 --> 00:06:13.400]   So governance and lineage was a key theme that we kept hearing.
[00:06:13.400 --> 00:06:17.680]   And over here on the right, I've got this funny screenshot
[00:06:17.680 --> 00:06:22.560]   that I love from an ODSC article, where it's basically
[00:06:22.560 --> 00:06:27.920]   a handoff point that's a finder file system with an iPython
[00:06:27.920 --> 00:06:29.760]   notebook.
[00:06:29.760 --> 00:06:32.320]   Training a model, we've got a scraper notebook,
[00:06:32.320 --> 00:06:35.320]   and then a bunch of different experiments and models
[00:06:35.320 --> 00:06:36.560]   loosely in here.
[00:06:36.560 --> 00:06:39.440]   And I've seen before folks basically say, hey,
[00:06:39.440 --> 00:06:40.600]   I got a new job.
[00:06:40.600 --> 00:06:43.280]   Here's my desktop with a bunch of notebooks
[00:06:43.280 --> 00:06:44.240]   and all of my research.
[00:06:44.240 --> 00:06:45.120]   Good luck.
[00:06:45.120 --> 00:06:47.280]   Here's all the stuff that I've been working on.
[00:06:47.280 --> 00:06:49.680]   And without that standardized process
[00:06:49.680 --> 00:06:51.600]   for tracking where something came from
[00:06:51.600 --> 00:06:53.400]   and then organized way of communicating
[00:06:53.400 --> 00:06:57.560]   then how to use it, it ends up becoming kind of a model soup
[00:06:57.560 --> 00:06:59.280]   of different models.
[00:06:59.280 --> 00:07:02.080]   So the last theme that we're really hearing here
[00:07:02.080 --> 00:07:04.280]   is automation and CI/CD.
[00:07:04.280 --> 00:07:07.000]   So right now, there's often a lot of manual testing.
[00:07:07.000 --> 00:07:09.080]   Even once you have a model in production
[00:07:09.080 --> 00:07:11.680]   and are iterating on it, there's still--
[00:07:11.680 --> 00:07:14.400]   a lot of team members are manually running ad hoc scripts
[00:07:14.400 --> 00:07:15.880]   to test new candidate models.
[00:07:15.880 --> 00:07:17.880]   Hopefully on the latest evaluation data set,
[00:07:17.880 --> 00:07:21.440]   but sometimes it's not clear if the model in production
[00:07:21.440 --> 00:07:23.400]   was evaluated on the same test set
[00:07:23.400 --> 00:07:26.760]   as the model of the new candidate that you're testing.
[00:07:26.760 --> 00:07:31.120]   So issues there around kind of a manual process of testing.
[00:07:31.120 --> 00:07:33.040]   And then ad hoc retraining.
[00:07:33.040 --> 00:07:36.240]   So it's often slow to retrain a model on the latest data,
[00:07:36.240 --> 00:07:38.920]   especially if there isn't this automated system to test
[00:07:38.920 --> 00:07:40.840]   and deploy.
[00:07:40.840 --> 00:07:42.240]   So what are our solutions?
[00:07:42.240 --> 00:07:44.000]   How do we make this process better?
[00:07:44.000 --> 00:07:48.160]   Well, in working on WB models, we're
[00:07:48.160 --> 00:07:50.760]   focusing on collaboration reproducibility first.
[00:07:50.760 --> 00:07:53.080]   So having a central model repository,
[00:07:53.080 --> 00:07:56.400]   like this little snippet here that you see on the right.
[00:07:56.400 --> 00:07:59.160]   And that's the live demo that I'll get into in a moment.
[00:07:59.160 --> 00:08:00.840]   So having all of the things that you're
[00:08:00.840 --> 00:08:02.840]   logging in one central place.
[00:08:02.840 --> 00:08:06.840]   Also communicating essentially with model cards and metadata.
[00:08:06.840 --> 00:08:10.280]   So having that context of the exact learning rate
[00:08:10.280 --> 00:08:12.840]   that you use when you train the model, or the exact Git
[00:08:12.840 --> 00:08:14.600]   commit and the diff patch.
[00:08:14.600 --> 00:08:17.240]   Any of the details that you need to reproduce
[00:08:17.240 --> 00:08:19.880]   where that model came from, try to capture that automatically
[00:08:19.880 --> 00:08:23.560]   and make it easy to access from the central page.
[00:08:23.560 --> 00:08:25.760]   For governance and lineage, we're
[00:08:25.760 --> 00:08:28.880]   making it easier to have the standardized tracked lifecycle.
[00:08:28.880 --> 00:08:31.800]   So adding a couple of lines to your different scripts
[00:08:31.800 --> 00:08:33.280]   in your pipeline means that you get
[00:08:33.280 --> 00:08:37.200]   to track exactly when a model moved between different stages.
[00:08:37.200 --> 00:08:39.680]   And we also capture when you move
[00:08:39.680 --> 00:08:42.400]   a model between staging and production
[00:08:42.400 --> 00:08:44.880]   and who signed off on that.
[00:08:44.880 --> 00:08:48.080]   We're also capturing this complete traceable history.
[00:08:48.080 --> 00:08:52.040]   So if you think about a tweak to a preprocessing step
[00:08:52.040 --> 00:08:56.640]   could actually ultimately really affect the accuracy
[00:08:56.640 --> 00:09:00.560]   of the model downstream, then capturing the changes
[00:09:00.560 --> 00:09:03.080]   upstream to that preprocessing step
[00:09:03.080 --> 00:09:04.800]   is also really critical to making
[00:09:04.800 --> 00:09:07.720]   this entire process traceable.
[00:09:07.720 --> 00:09:10.760]   And so with WMB and those couple lines of code
[00:09:10.760 --> 00:09:13.280]   tracking upstream, you can capture that complete history.
[00:09:13.280 --> 00:09:15.640]   And I'll show you what that history looks like in the UI
[00:09:15.640 --> 00:09:17.200]   here in a moment.
[00:09:17.200 --> 00:09:21.320]   And then third, that automation and CI/CD piece,
[00:09:21.320 --> 00:09:24.520]   we're allowing you to have this comprehensive model evaluation
[00:09:24.520 --> 00:09:26.400]   and it happened in a central place.
[00:09:26.400 --> 00:09:27.720]   So you can automatically evaluate
[00:09:27.720 --> 00:09:30.160]   models that are good candidates for the registry.
[00:09:30.160 --> 00:09:33.040]   And my colleague Igor will touch on more of this automation
[00:09:33.040 --> 00:09:37.600]   for eval and retraining in his section right after this.
[00:09:37.600 --> 00:09:40.360]   So now I'll get into a live demo and talk specifically
[00:09:40.360 --> 00:09:42.480]   about the collaboration reproducibility,
[00:09:42.480 --> 00:09:44.720]   having that single pane of glass for all of your model
[00:09:44.720 --> 00:09:47.160]   development, and then governance and lineage,
[00:09:47.160 --> 00:09:50.160]   so how you can keep track of when people move things
[00:09:50.160 --> 00:09:53.040]   between different stages.
[00:09:53.040 --> 00:09:56.680]   So here in my next tab, this is what the model registry
[00:09:56.680 --> 00:09:57.480]   looks like.
[00:09:57.480 --> 00:10:00.480]   What you're seeing here on the right-hand side
[00:10:00.480 --> 00:10:03.480]   in this sidebar are different registered models.
[00:10:03.480 --> 00:10:06.760]   So these are different tasks, like detecting a stoplight
[00:10:06.760 --> 00:10:09.040]   or a model for mapping.
[00:10:09.040 --> 00:10:11.720]   In this case, we're looking at nature classification.
[00:10:11.720 --> 00:10:14.320]   And here we can see each different version
[00:10:14.320 --> 00:10:17.040]   of the model that's been added in here.
[00:10:17.040 --> 00:10:19.200]   So how this works is different researchers
[00:10:19.200 --> 00:10:20.640]   are iterating on this problem.
[00:10:20.640 --> 00:10:23.120]   They've got maybe dozens and dozens of model checkpoints
[00:10:23.120 --> 00:10:24.440]   in different projects.
[00:10:24.440 --> 00:10:25.920]   But when they have a good candidate,
[00:10:25.920 --> 00:10:27.320]   they can link it in here.
[00:10:27.320 --> 00:10:29.280]   And it'll show up here in this table.
[00:10:29.280 --> 00:10:32.080]   So a new candidate is added to the registered model.
[00:10:32.080 --> 00:10:34.480]   And once it's linked, I can then dive
[00:10:34.480 --> 00:10:38.880]   into any of these individual models and see more details.
[00:10:38.880 --> 00:10:40.320]   Now here, my colleague has actually
[00:10:40.320 --> 00:10:43.320]   filled in this really clear model card.
[00:10:43.320 --> 00:10:45.760]   So I can see what the expected inputs and outputs
[00:10:45.760 --> 00:10:48.000]   are for this given model.
[00:10:48.000 --> 00:10:51.640]   And I can also get back and see exactly what the run was
[00:10:51.640 --> 00:10:53.040]   that produced this model.
[00:10:53.040 --> 00:10:56.200]   So if I want to see things like, what were the training curves?
[00:10:56.200 --> 00:11:00.920]   Or what were the layers of the model that you were training?
[00:11:00.920 --> 00:11:02.440]   I can see exactly that.
[00:11:02.440 --> 00:11:04.640]   I can also get back to the exact version of the code.
[00:11:04.640 --> 00:11:06.040]   I can see when it was trained.
[00:11:06.040 --> 00:11:08.960]   I can see the Python version, the configs,
[00:11:08.960 --> 00:11:11.720]   so any hyperparameters.
[00:11:11.720 --> 00:11:14.200]   All of that is accessible just with one click
[00:11:14.200 --> 00:11:16.360]   from the model registry.
[00:11:16.360 --> 00:11:19.000]   Now I can also see a summary of the metadata
[00:11:19.000 --> 00:11:21.120]   here right on the metadata tab.
[00:11:21.120 --> 00:11:24.160]   So I can look at those hyperparameters.
[00:11:24.160 --> 00:11:26.360]   And if I sent this to someone else,
[00:11:26.360 --> 00:11:29.040]   they could pull up this snippet of code
[00:11:29.040 --> 00:11:31.680]   and pull down my model and start to use it
[00:11:31.680 --> 00:11:35.080]   with a simple chunk for usage.
[00:11:35.080 --> 00:11:39.240]   Now for files, you can save anything into a model.
[00:11:39.240 --> 00:11:43.040]   We aren't going to restrict what you're
[00:11:43.040 --> 00:11:46.920]   allowed to save into this versioned folder of data.
[00:11:46.920 --> 00:11:49.480]   So you could imagine having your H5 file in here.
[00:11:49.480 --> 00:11:52.840]   But you could also imagine having additional files that
[00:11:52.840 --> 00:11:54.680]   are maybe helper scripts that someone who's
[00:11:54.680 --> 00:11:56.920]   using the model might need.
[00:11:56.920 --> 00:12:02.040]   So we try to be flexible and agnostic in this regard
[00:12:02.040 --> 00:12:04.640]   so you can decide how you're managing the model that you've
[00:12:04.640 --> 00:12:06.120]   saved here.
[00:12:06.120 --> 00:12:08.440]   And then lineage, this is the exciting part
[00:12:08.440 --> 00:12:11.160]   that I was really looking forward to getting to.
[00:12:11.160 --> 00:12:14.040]   So we're looking right now at a model artifact
[00:12:14.040 --> 00:12:16.080]   that was saved to WMB and then linked
[00:12:16.080 --> 00:12:17.920]   into this registered model.
[00:12:17.920 --> 00:12:19.400]   Now where did it come from?
[00:12:19.400 --> 00:12:21.680]   I can step one step back in the DAC
[00:12:21.680 --> 00:12:24.560]   and see, OK, that's the run that we already opened.
[00:12:24.560 --> 00:12:26.640]   If I open that again, you'll recognize
[00:12:26.640 --> 00:12:29.240]   this is that same run that we were just looking
[00:12:29.240 --> 00:12:32.040]   at for Keras model training.
[00:12:32.040 --> 00:12:34.620]   Now what did that run use?
[00:12:34.620 --> 00:12:38.000]   I can open that up, and that's the training data
[00:12:38.000 --> 00:12:40.440]   that was pulled in to train on.
[00:12:40.440 --> 00:12:43.320]   And I can see exactly the version of data
[00:12:43.320 --> 00:12:45.440]   that was used to train that model.
[00:12:45.440 --> 00:12:49.880]   In this case, I can even get in and see the exact data itself.
[00:12:49.880 --> 00:12:53.640]   And if you zoom out, you can see all of the other steps that
[00:12:53.640 --> 00:12:56.360]   happened upstream of that, ultimately, the model
[00:12:56.360 --> 00:12:57.600]   that we're looking at.
[00:12:57.600 --> 00:12:59.600]   So I can get back and see the full history
[00:12:59.600 --> 00:13:02.400]   of where this thing came from.
[00:13:02.400 --> 00:13:07.980]   Now inside this specific registered model,
[00:13:07.980 --> 00:13:10.060]   we also have different stages.
[00:13:10.060 --> 00:13:11.980]   So we have production.
[00:13:11.980 --> 00:13:13.060]   We have latest.
[00:13:13.060 --> 00:13:15.620]   And you saw that here in the versions table.
[00:13:15.620 --> 00:13:19.900]   So how do I identify who moved this model into production?
[00:13:19.900 --> 00:13:22.420]   Well, here in the action history,
[00:13:22.420 --> 00:13:24.620]   you can see it was me 15 minutes ago.
[00:13:24.620 --> 00:13:26.700]   I added the alias production.
[00:13:26.700 --> 00:13:30.500]   So I moved this version of the model into this state.
[00:13:30.500 --> 00:13:34.080]   And now if I wanted to add a new model in here,
[00:13:34.080 --> 00:13:36.940]   I could also track that on this page.
[00:13:36.940 --> 00:13:37.840]   So let's try that now.
[00:13:37.840 --> 00:13:42.020]   Let's imagine we have a project that we're working on,
[00:13:42.020 --> 00:13:45.260]   like, for example, this model registry end-to-end demo
[00:13:45.260 --> 00:13:46.460]   project.
[00:13:46.460 --> 00:13:48.620]   And here, you'll recognize this project page
[00:13:48.620 --> 00:13:51.960]   if you're tracking experiments with WMB.
[00:13:51.960 --> 00:13:53.780]   So here on the left side, we've got
[00:13:53.780 --> 00:13:56.880]   runs that are tracking different training jobs,
[00:13:56.880 --> 00:13:59.400]   different instances of training a model.
[00:13:59.400 --> 00:14:02.100]   And we've got two different runs, green and purple.
[00:14:02.100 --> 00:14:04.540]   And it looks like over here on the accuracy chart,
[00:14:04.540 --> 00:14:08.460]   green at the end of training is performing just slightly more
[00:14:08.460 --> 00:14:10.300]   poorly than purple.
[00:14:10.300 --> 00:14:15.900]   So I'm going to click into this run here and see, OK,
[00:14:15.900 --> 00:14:16.740]   it looks great.
[00:14:16.740 --> 00:14:21.500]   I can look at any of the files, the model architecture.
[00:14:21.500 --> 00:14:24.960]   And I can pull up the artifacts and see the models
[00:14:24.960 --> 00:14:26.660]   that it produced.
[00:14:26.660 --> 00:14:29.280]   And I know that that last step is the one
[00:14:29.280 --> 00:14:32.660]   that I want to use to then link into the registry.
[00:14:32.660 --> 00:14:35.720]   So I'll go back to my All Versions table.
[00:14:35.720 --> 00:14:39.840]   We can see that, OK, latest was version 5 just now.
[00:14:39.840 --> 00:14:45.340]   And here, I can then pick this latest artifact
[00:14:45.340 --> 00:14:47.560]   and pull it in and link it.
[00:14:47.560 --> 00:14:48.940]   And apologies for my internet.
[00:14:48.940 --> 00:14:51.160]   It looks like that's taking a moment.
[00:14:51.160 --> 00:14:54.280]   So ultimately, what the model registry is providing us here
[00:14:54.280 --> 00:14:57.620]   is a way to capture across the different projects
[00:14:57.620 --> 00:15:01.420]   that you're working on the central list of the best, most
[00:15:01.420 --> 00:15:03.540]   useful versions of models.
[00:15:03.540 --> 00:15:07.500]   And what that will give us is an organized view
[00:15:07.500 --> 00:15:09.540]   that anyone else, even if they haven't
[00:15:09.540 --> 00:15:12.540]   been involved in the process of model training and development,
[00:15:12.540 --> 00:15:15.580]   they'll be able to come in and identify,
[00:15:15.580 --> 00:15:17.380]   here's the model that's in production.
[00:15:17.380 --> 00:15:18.780]   Here's the new candidate.
[00:15:18.780 --> 00:15:22.180]   And this is what I need to know about it in that overview,
[00:15:22.180 --> 00:15:23.920]   in that model card.
[00:15:23.920 --> 00:15:27.740]   And so what that's giving us is that communication,
[00:15:27.740 --> 00:15:32.260]   collaboration, and the central hub, and then that ability
[00:15:32.260 --> 00:15:34.460]   to trace exactly what's happening,
[00:15:34.460 --> 00:15:38.020]   both from the metadata and the lineage that we touched on
[00:15:38.020 --> 00:15:40.860]   and that action history of every step of things
[00:15:40.860 --> 00:15:42.220]   that have changed.
[00:15:42.220 --> 00:15:45.100]   And so ultimately, what this tool is about
[00:15:45.100 --> 00:15:49.980]   is giving you that central place to discuss models as a team
[00:15:49.980 --> 00:15:52.840]   and to hand off models between different stages
[00:15:52.840 --> 00:15:54.660]   of your pipeline.
[00:15:54.660 --> 00:15:58.500]   So now we've talked about those core collaboration
[00:15:58.500 --> 00:16:00.180]   and governance pieces.
[00:16:00.180 --> 00:16:02.780]   Next up is automation and CI/CD.
[00:16:02.780 --> 00:16:05.020]   And this is a really exciting next step.
[00:16:05.020 --> 00:16:06.820]   How do we take those actions that you
[00:16:06.820 --> 00:16:09.940]   saw in the model registry and move models
[00:16:09.940 --> 00:16:11.700]   through that lifecycle?
[00:16:11.700 --> 00:16:15.100]   So that's, for example, I have a new model
[00:16:15.100 --> 00:16:16.420]   that is in production.
[00:16:16.420 --> 00:16:19.860]   And now we've got fresh data coming in from the field.
[00:16:19.860 --> 00:16:22.380]   How do I retrain that model on the latest data
[00:16:22.380 --> 00:16:25.220]   and trigger that automatically when data is available?
[00:16:25.220 --> 00:16:28.340]   Or automatically testing the latest candidate model.
[00:16:28.340 --> 00:16:30.940]   So say a new model is linked in there to the registry.
[00:16:30.940 --> 00:16:34.780]   How do you automatically kick off a test suite?
[00:16:34.780 --> 00:16:38.260]   Or now that model's passed all these tests, it's looking good.
[00:16:38.260 --> 00:16:40.820]   How do we deploy that model to production?
[00:16:40.820 --> 00:16:43.140]   So I'll hand it off to Igor now to talk more
[00:16:43.140 --> 00:16:45.180]   about automation and CI/CD.
[00:16:45.180 --> 00:16:46.740]   Thanks for the introduction.
[00:16:46.740 --> 00:16:49.580]   So I'm happy to talk about our launch product line.
[00:16:49.580 --> 00:16:51.620]   Weights and Biases has always had the vision
[00:16:51.620 --> 00:16:54.380]   to integrate well with your existing infrastructure.
[00:16:54.380 --> 00:16:55.820]   And launch actually gets its name
[00:16:55.820 --> 00:17:00.580]   for being able to launch jobs into that infrastructure.
[00:17:00.580 --> 00:17:02.980]   In terms of the goals of the product line,
[00:17:02.980 --> 00:17:06.140]   Weights and Biases cares deeply about reproducibility.
[00:17:06.140 --> 00:17:07.560]   And so we want to make sure we're
[00:17:07.560 --> 00:17:09.580]   compatible with containerized workflows
[00:17:09.580 --> 00:17:13.260]   and work well with tools such as Docker and Kubernetes.
[00:17:13.260 --> 00:17:15.940]   We want to connect the ML practitioner to compute.
[00:17:15.940 --> 00:17:18.220]   So envision you have a hyperparameter tuning job
[00:17:18.220 --> 00:17:19.220]   that you want to run.
[00:17:19.220 --> 00:17:21.220]   You can then simply send that to a cluster,
[00:17:21.220 --> 00:17:24.980]   parallelize the work, and have the job complete much faster.
[00:17:24.980 --> 00:17:27.380]   And we want to make sure we abstract the way the complexity
[00:17:27.380 --> 00:17:29.380]   of actually using that infrastructure
[00:17:29.380 --> 00:17:32.660]   and making sure that it's used in getting an ROI
[00:17:32.660 --> 00:17:36.140]   by having one central UI within Weights and Biases
[00:17:36.140 --> 00:17:37.820]   that you can launch jobs from.
[00:17:37.820 --> 00:17:40.820]   It's a UI that you're already familiar with your other tasks.
[00:17:40.820 --> 00:17:43.420]   And this notion of building a bridge between the ML
[00:17:43.420 --> 00:17:45.140]   practitioner in your organization
[00:17:45.140 --> 00:17:47.740]   to other personas such as the ML Ops engineer
[00:17:47.740 --> 00:17:49.900]   and working with your IT team.
[00:17:49.900 --> 00:17:52.140]   We know that the practitioner really
[00:17:52.140 --> 00:17:55.220]   wants to focus on building models, running experiments.
[00:17:55.220 --> 00:17:58.060]   They don't necessarily want to deal with configuration
[00:17:58.060 --> 00:18:01.340]   files, Helm charts, and all the specifics around Kubernetes.
[00:18:01.340 --> 00:18:07.420]   So having a seamless way to connect to the infrastructure.
[00:18:07.420 --> 00:18:10.500]   So this is a visual representation of the workflow.
[00:18:10.500 --> 00:18:14.820]   So an ML practitioner may have a piece of local code
[00:18:14.820 --> 00:18:17.100]   that they're iterating on and allowing them
[00:18:17.100 --> 00:18:19.700]   to then easily connect to a cluster,
[00:18:19.700 --> 00:18:22.660]   get access to more resources, and really power up
[00:18:22.660 --> 00:18:25.340]   their workflow.
[00:18:25.340 --> 00:18:27.780]   So let's actually dive into a demo
[00:18:27.780 --> 00:18:30.020]   and being able to create and launch a job
[00:18:30.020 --> 00:18:32.100]   through the Weights and Biases UI.
[00:18:32.100 --> 00:18:33.500]   Weights and Biases is flexible.
[00:18:33.500 --> 00:18:36.700]   So there's actually a variety of ways that we can create a job.
[00:18:36.700 --> 00:18:38.860]   You can run some samples.
[00:18:38.860 --> 00:18:40.980]   Code have one line that corresponds
[00:18:40.980 --> 00:18:42.360]   to Weights and Biases.
[00:18:42.360 --> 00:18:44.660]   You can launch a job via a Docker image.
[00:18:44.660 --> 00:18:48.500]   Weights and Biases accepts a Git shot or a Git URL.
[00:18:48.500 --> 00:18:51.780]   Or you can launch a job directly within the user interface.
[00:18:51.780 --> 00:18:54.500]   And I'll show you how to do that as well.
[00:18:54.500 --> 00:18:58.700]   So if we go to Weights and Biases UI,
[00:18:58.700 --> 00:19:00.780]   you'll see a new tab here that appears.
[00:19:00.780 --> 00:19:02.500]   It's called Launch.
[00:19:02.500 --> 00:19:04.860]   And this is a job queue that now anybody
[00:19:04.860 --> 00:19:08.660]   within your organizations can send jobs to to execute.
[00:19:08.660 --> 00:19:11.060]   Now, there's a one-time setup that's
[00:19:11.060 --> 00:19:13.500]   required to connect Weights and Biases Launch
[00:19:13.500 --> 00:19:15.020]   to your infrastructure.
[00:19:15.020 --> 00:19:18.300]   In this case, my ML ops team has already made that connection.
[00:19:18.300 --> 00:19:20.300]   And this queue can then be subsequently
[00:19:20.300 --> 00:19:25.100]   reused by anybody within the organization.
[00:19:25.100 --> 00:19:28.100]   Now, let me show you how to create a job
[00:19:28.100 --> 00:19:29.480]   via both code and the UI.
[00:19:29.480 --> 00:19:32.900]   So let's start with code.
[00:19:32.900 --> 00:19:36.700]   So this is a simple training script that I have.
[00:19:36.700 --> 00:19:39.300]   This training a model on the fashion MNIST data set
[00:19:39.300 --> 00:19:42.140]   detecting the type of clothing.
[00:19:42.140 --> 00:19:45.660]   I'm using Weights and Biases to log some of the loss metrics,
[00:19:45.660 --> 00:19:47.620]   as well as some of the sample training images
[00:19:47.620 --> 00:19:49.980]   from the training data set.
[00:19:49.980 --> 00:19:54.460]   Now, with this one line of code, Weights and Biases
[00:19:54.460 --> 00:19:56.940]   will automatically capture all the information that's
[00:19:56.940 --> 00:19:58.940]   needed to reproduce this job.
[00:19:58.940 --> 00:20:02.420]   And I'll show you that within the UI.
[00:20:02.420 --> 00:20:05.300]   I'm also passing in some configuration parameters
[00:20:05.300 --> 00:20:06.620]   into the model.
[00:20:06.620 --> 00:20:08.420]   With Weights and Biases Launch, we'll
[00:20:08.420 --> 00:20:10.300]   actually expose these via the UI.
[00:20:10.300 --> 00:20:13.540]   And these will be knobs that you can tune and be able to launch
[00:20:13.540 --> 00:20:16.060]   new experiments from.
[00:20:16.060 --> 00:20:17.900]   So let's run this piece of code.
[00:20:17.900 --> 00:20:28.780]   And if I copy and paste this, you
[00:20:28.780 --> 00:20:33.500]   can see that the job is now executing.
[00:20:33.500 --> 00:20:36.460]   All the results are now being streamed in real time
[00:20:36.460 --> 00:20:37.220]   into the UI.
[00:20:39.860 --> 00:20:42.060]   Now, notice, Weights and Biases, again,
[00:20:42.060 --> 00:20:44.060]   automatically going to capture that the hardware
[00:20:44.060 --> 00:20:45.740]   of the job is running on.
[00:20:45.740 --> 00:20:47.620]   They get information.
[00:20:47.620 --> 00:20:54.020]   We capture the code that the job is running on.
[00:20:54.020 --> 00:21:01.060]   If we capture all of the requirements and dependencies,
[00:21:01.060 --> 00:21:04.620]   if you launch this job with a Docker image,
[00:21:04.620 --> 00:21:07.340]   we can capture the Docker information as well.
[00:21:07.340 --> 00:21:11.660]   Again, the goal is full reproducibility.
[00:21:11.660 --> 00:21:15.140]   So let's actually now add this job to the queue
[00:21:15.140 --> 00:21:16.780]   and execute it.
[00:21:16.780 --> 00:21:19.540]   So if we go to Weights and Biases Launch
[00:21:19.540 --> 00:21:23.020]   and we add a job to the queue, again, we
[00:21:23.020 --> 00:21:25.340]   accept jobs from a variety of these sources.
[00:21:25.340 --> 00:21:30.180]   But let's clone the existing run that I have just completed.
[00:21:30.180 --> 00:21:32.660]   Here, what you see is a parameterized version
[00:21:32.660 --> 00:21:33.540]   of that job.
[00:21:33.540 --> 00:21:35.860]   And remember these knobs that I mentioned
[00:21:35.860 --> 00:21:37.300]   via the training script?
[00:21:37.300 --> 00:21:39.940]   I can now overwrite any of these via the UI.
[00:21:39.940 --> 00:21:41.780]   So if I wanted to change my learning rate
[00:21:41.780 --> 00:21:45.380]   or change the amount of epoch that the model is trained upon,
[00:21:45.380 --> 00:21:48.900]   I can do that directly from the UI.
[00:21:48.900 --> 00:21:52.780]   If I want to then send this job to my cluster
[00:21:52.780 --> 00:21:54.380]   or the infrastructure of my choice,
[00:21:54.380 --> 00:21:57.740]   I can then now select that as a resource of where
[00:21:57.740 --> 00:22:00.140]   the job is going to be run.
[00:22:00.140 --> 00:22:06.380]   So if I push this run, the job has now been enqueued.
[00:22:06.380 --> 00:22:09.660]   And I have an agent set up to automatically listen
[00:22:09.660 --> 00:22:11.020]   for this run.
[00:22:11.020 --> 00:22:15.740]   And it will pick up upon and start rebuilding the run
[00:22:15.740 --> 00:22:18.420]   and start executing.
[00:22:18.420 --> 00:22:20.380]   And notice, everything I'm doing,
[00:22:20.380 --> 00:22:23.260]   I'm just-- I have a sample piece of training code.
[00:22:23.260 --> 00:22:25.300]   I'm just clicking buttons via the UI.
[00:22:25.300 --> 00:22:27.980]   I don't necessarily need to deal with YAML files.
[00:22:27.980 --> 00:22:30.660]   It allows me as a practitioner to much more effectively
[00:22:30.660 --> 00:22:34.780]   collaborate with my MLOps team and with my IT team.
[00:22:34.780 --> 00:22:35.820]   OK, boom.
[00:22:35.820 --> 00:22:38.700]   You see that the job has already started.
[00:22:38.700 --> 00:22:40.900]   And then the new job is now running.
[00:22:40.900 --> 00:22:44.220]   And the results will now be streamed in real time
[00:22:44.220 --> 00:22:45.340]   via the UI as well.
[00:22:45.340 --> 00:22:49.380]   Perfect.
[00:22:49.380 --> 00:22:54.340]   So this job is now running.
[00:22:54.340 --> 00:23:00.140]   So zooming out, again, as Phil and Kerry mentioned,
[00:23:00.140 --> 00:23:02.040]   the vision of Weights and Biases has always
[00:23:02.040 --> 00:23:03.940]   been an end-to-end platform.
[00:23:03.940 --> 00:23:07.300]   And launch is a platform layer and a generic job execution
[00:23:07.300 --> 00:23:12.820]   layer that allow us to expand to other parts of the pipeline.
[00:23:12.820 --> 00:23:15.300]   In a subsequent release, we have actually
[00:23:15.300 --> 00:23:17.180]   just released a functionality that's
[00:23:17.180 --> 00:23:20.460]   known as triggers, where you can trigger these jobs contingent
[00:23:20.460 --> 00:23:21.700]   upon an event.
[00:23:21.700 --> 00:23:25.700]   So envision you log on your data set within Weights and Biases.
[00:23:25.700 --> 00:23:28.020]   You can then automatically trigger a model training
[00:23:28.020 --> 00:23:30.660]   job to run, a model evaluation job to run.
[00:23:30.660 --> 00:23:32.740]   You can trigger a suite of tests that you
[00:23:32.740 --> 00:23:34.580]   can test against all of the metrics
[00:23:34.580 --> 00:23:37.540]   that Weights and Biases captures.
[00:23:37.540 --> 00:23:41.220]   And so launch is a product line.
[00:23:41.220 --> 00:23:42.260]   This is a sneak peek.
[00:23:42.260 --> 00:23:43.740]   It's a private preview product.
[00:23:43.740 --> 00:23:45.580]   But it's undergoing rapid evolution.
[00:23:45.580 --> 00:23:48.340]   So stay tuned for the subsequent releases.
[00:23:48.340 --> 00:23:50.140]   And I'll hand it over to Stacey.
[00:23:50.140 --> 00:23:50.900]   Hi, folks.
[00:23:50.900 --> 00:23:52.300]   Thanks so much, Igor.
[00:23:52.300 --> 00:23:52.860]   I'm Stacey.
[00:23:52.860 --> 00:23:55.460]   And I'm a deep learning engineer here at Weights and Biases.
[00:23:55.460 --> 00:23:58.820]   And I build developer tools for visualization, explainability,
[00:23:58.820 --> 00:23:59.820]   collaboration in AI.
[00:23:59.860 --> 00:24:03.740]   And you can now read a lot more detail on my profile page.
[00:24:03.740 --> 00:24:05.900]   And if you've been familiar with Weights and Biases,
[00:24:05.900 --> 00:24:08.260]   you might have seen the showcase portion, where
[00:24:08.260 --> 00:24:11.060]   if you go to wandb.ai/yourusername,
[00:24:11.060 --> 00:24:13.540]   you can add reports from projects
[00:24:13.540 --> 00:24:14.500]   that you're working on.
[00:24:14.500 --> 00:24:16.860]   And that showcase component is staying.
[00:24:16.860 --> 00:24:18.500]   I have a lot of reports here.
[00:24:18.500 --> 00:24:20.980]   So I'm very excited about the customizable part
[00:24:20.980 --> 00:24:23.820]   of the profile page, where I can write text
[00:24:23.820 --> 00:24:25.100]   about what I'm working on.
[00:24:25.100 --> 00:24:27.420]   I can link to external projects.
[00:24:27.420 --> 00:24:30.100]   I can feature reports.
[00:24:30.100 --> 00:24:32.780]   And I can also nest details so that we
[00:24:32.780 --> 00:24:36.460]   can hop into the 3D object detection project.
[00:24:36.460 --> 00:24:38.820]   You'll see that I can include images.
[00:24:38.820 --> 00:24:44.500]   I can even include equations and blocks of code.
[00:24:44.500 --> 00:24:45.860]   And under the hood, this is really
[00:24:45.860 --> 00:24:49.100]   a report, which I can customize to show whatever
[00:24:49.100 --> 00:24:51.140]   I want on my profile page.
[00:24:51.140 --> 00:24:54.140]   I can even organize all of my projects and reports
[00:24:54.140 --> 00:24:55.100]   by category.
[00:24:55.100 --> 00:24:57.380]   So here are some of my favorite reports.
[00:24:57.380 --> 00:25:01.660]   Here are my reports about language and so on.
[00:25:01.660 --> 00:25:05.900]   And I also want to show a team version of this.
[00:25:05.900 --> 00:25:08.100]   Really quickly, you can also include emoji.
[00:25:08.100 --> 00:25:10.580]   And when you're logged in, you'll get an Edit button.
[00:25:10.580 --> 00:25:12.300]   You can write in this report.
[00:25:12.300 --> 00:25:17.300]   You can order these around and really customize
[00:25:17.300 --> 00:25:20.020]   what you're presenting to the world on your profile page.
[00:25:20.020 --> 00:25:22.340]   We're hoping this inspires folks to share more
[00:25:22.340 --> 00:25:24.220]   about their projects, find collaborators,
[00:25:24.220 --> 00:25:27.460]   get ideas from Weights and Biases.
[00:25:27.460 --> 00:25:32.420]   I want to go back and talk about the 3D objects work
[00:25:32.420 --> 00:25:35.460]   that we've been doing.
[00:25:35.460 --> 00:25:37.740]   In this project, I'm working with the level 5 data
[00:25:37.740 --> 00:25:43.300]   set to annotate some scenes with 3D point clouds.
[00:25:43.300 --> 00:25:47.140]   And we have the ground truth bounding boxes
[00:25:47.140 --> 00:25:48.260]   for this data set.
[00:25:48.260 --> 00:25:49.700]   They'll show up in green.
[00:25:49.700 --> 00:25:52.700]   And some predictions from my very simple model,
[00:25:52.700 --> 00:25:54.740]   they'll be in yellow if it's a close match
[00:25:54.740 --> 00:25:57.220]   and in red if it's a guess.
[00:25:57.220 --> 00:26:01.580]   So you'll see that there's a bunch of guesses which
[00:26:01.580 --> 00:26:02.780]   are actually trees.
[00:26:02.780 --> 00:26:06.420]   And I can customize the exact numerical score here.
[00:26:06.420 --> 00:26:11.100]   We're adding filtering by the labels soon.
[00:26:11.100 --> 00:26:13.020]   You might get multiple candidate boxes.
[00:26:13.020 --> 00:26:14.740]   And hopefully, you're getting some idea
[00:26:14.740 --> 00:26:19.620]   of just how detailed all of this annotation can be.
[00:26:19.620 --> 00:26:21.540]   I have the syntax documented here.
[00:26:21.540 --> 00:26:27.700]   You can set the label to any text and color and then
[00:26:27.700 --> 00:26:28.860]   the numerical score.
[00:26:28.860 --> 00:26:31.060]   And we're actively working on adding multiple camera
[00:26:31.060 --> 00:26:35.620]   viewpoints and working with sequences of scenes.
[00:26:35.620 --> 00:26:38.420]   And if your team is working on 3D point cloud visualization,
[00:26:38.420 --> 00:26:41.260]   we'd love to hear from you on the specific features that
[00:26:41.260 --> 00:26:43.380]   would help.
[00:26:43.380 --> 00:26:45.180]   Great, and it works to switch tabs now.
[00:26:45.180 --> 00:26:47.700]   I'd like to go into a different project.
[00:26:47.700 --> 00:26:50.940]   And thanks for bearing with me here on time series.
[00:26:50.940 --> 00:26:54.140]   So we've been working on support for time series
[00:26:54.140 --> 00:26:58.500]   and really enabling you to log not just the bar charts
[00:26:58.500 --> 00:27:02.580]   and scatter plots and training curves that you're used to,
[00:27:02.580 --> 00:27:05.740]   but have a timestamp on the x-axis.
[00:27:05.740 --> 00:27:09.620]   So to enable this, I simply log a table.
[00:27:09.620 --> 00:27:12.580]   And I can have a timestamp column and one or more metrics
[00:27:12.580 --> 00:27:13.940]   columns here.
[00:27:13.940 --> 00:27:17.380]   And then to visualize this data as multiple series,
[00:27:17.380 --> 00:27:21.740]   I can select a combined plot instead of a table.
[00:27:21.740 --> 00:27:23.860]   On the x dimension, I'll have the timestamp.
[00:27:23.860 --> 00:27:25.940]   And I add dot to timestamp to convert it
[00:27:25.940 --> 00:27:27.620]   to a human-readable format.
[00:27:27.620 --> 00:27:30.900]   On the y dimension here, I have temperature in degrees Celsius.
[00:27:30.900 --> 00:27:34.020]   And this project is using a data set of climate measurements,
[00:27:34.020 --> 00:27:36.500]   which include temperature, atmospheric pressure, humidity,
[00:27:36.500 --> 00:27:38.060]   and so on.
[00:27:38.060 --> 00:27:42.740]   And once I configure that, I can have the real time
[00:27:42.740 --> 00:27:44.860]   from the validation data.
[00:27:44.860 --> 00:27:49.500]   And the observed actual values, the ground truth,
[00:27:49.500 --> 00:27:50.740]   they're shown in black here.
[00:27:50.740 --> 00:27:52.460]   And then all of my model's predictions
[00:27:52.460 --> 00:27:53.860]   are the colorful ones.
[00:27:53.860 --> 00:27:57.300]   And of course, this is a lot of lines.
[00:27:57.300 --> 00:28:00.300]   So I'll walk through that in a bit more detail in a second.
[00:28:00.300 --> 00:28:02.700]   But just to show you how simple the syntax is,
[00:28:02.700 --> 00:28:06.940]   I can call wandb.log and pass in a data frame to the table.
[00:28:06.940 --> 00:28:11.740]   Or I can explicitly list the columns that I'd like to show.
[00:28:11.740 --> 00:28:13.420]   So looking in more detail, here I
[00:28:13.420 --> 00:28:16.340]   have just two variants.
[00:28:16.340 --> 00:28:20.140]   I have the observed values, the ground truth, and block.
[00:28:20.140 --> 00:28:22.740]   And then the baseline model, which is rule-based.
[00:28:22.740 --> 00:28:26.100]   It just repeats the previous observation in light orange.
[00:28:26.100 --> 00:28:28.460]   And I can toggle some of my other models
[00:28:28.460 --> 00:28:30.540]   that I've trained and see, OK, how
[00:28:30.540 --> 00:28:32.380]   does the linear model perform?
[00:28:32.380 --> 00:28:35.060]   OK, and here the red value is a little bit closer
[00:28:35.060 --> 00:28:36.100]   on these metrics.
[00:28:36.100 --> 00:28:37.460]   Maybe it's a little hard to tell.
[00:28:37.460 --> 00:28:41.060]   Maybe I want to look at my category of small two-layer
[00:28:41.060 --> 00:28:41.820]   models.
[00:28:41.820 --> 00:28:43.740]   And you can see that for these three
[00:28:43.740 --> 00:28:45.420]   different components of pressure,
[00:28:45.420 --> 00:28:47.100]   maybe they're getting closer.
[00:28:47.100 --> 00:28:50.340]   And of course, you might want to do some grouping to evaluate
[00:28:50.340 --> 00:28:51.860]   how those models are performing.
[00:28:51.860 --> 00:28:54.340]   And hopefully, this gives you a sense of the detailed
[00:28:54.340 --> 00:28:56.500]   visualization that you can do.
[00:28:56.500 --> 00:28:58.820]   If the colors and line styles are not enough,
[00:28:58.820 --> 00:29:00.740]   you can also add points.
[00:29:00.740 --> 00:29:03.180]   And here I have a lot more metrics.
[00:29:03.180 --> 00:29:04.820]   Besides temperature and pressure,
[00:29:04.820 --> 00:29:06.780]   I have humidity and wind velocity
[00:29:06.780 --> 00:29:08.260]   in the x and y direction.
[00:29:08.260 --> 00:29:10.180]   And if I view them all on one chart,
[00:29:10.180 --> 00:29:12.460]   it might start to get a little bit busy.
[00:29:12.460 --> 00:29:14.460]   So I can go into this configuration here.
[00:29:14.460 --> 00:29:17.380]   And I see all of the row--
[00:29:17.380 --> 00:29:20.180]   the column values that I'm plotting in the series.
[00:29:20.180 --> 00:29:22.420]   And let's say I want to change this one.
[00:29:22.420 --> 00:29:23.860]   I want to make it a point.
[00:29:23.860 --> 00:29:26.820]   And let's make it a diamond.
[00:29:26.820 --> 00:29:31.300]   And then I'll change this to 500, make them really big,
[00:29:31.300 --> 00:29:32.540]   and hit Apply.
[00:29:32.540 --> 00:29:36.380]   And now you'll see that that series is highlighted.
[00:29:36.380 --> 00:29:39.540]   And I can see the difference between some of my models
[00:29:39.540 --> 00:29:41.540]   and the ground truth and get really detailed
[00:29:41.540 --> 00:29:44.220]   about how this is visualized.
[00:29:44.220 --> 00:29:46.500]   Once I have all of this set up, Artifacts
[00:29:46.500 --> 00:29:49.620]   is a great way to version time slices of my data.
[00:29:49.620 --> 00:29:53.580]   So I can have my training data be aliased by the last year
[00:29:53.580 --> 00:29:55.580]   that's included, my validation data
[00:29:55.580 --> 00:29:57.780]   be different years than the ones I'm training on,
[00:29:57.780 --> 00:30:00.140]   and my test data have months as alias.
[00:30:00.140 --> 00:30:01.740]   And then in my script, it's very easy
[00:30:01.740 --> 00:30:04.700]   to evaluate models trained on different subsets
[00:30:04.700 --> 00:30:07.620]   and test them on the different months.
[00:30:07.620 --> 00:30:10.700]   So here I have that for May and July.
[00:30:10.700 --> 00:30:14.980]   And for June here, I'm showing a derived metric.
[00:30:14.980 --> 00:30:17.020]   So I can go into the panel plot configuration.
[00:30:17.020 --> 00:30:20.500]   And I can actually say, subtract my model predictions
[00:30:20.500 --> 00:30:24.100]   from the ground truth, the observed values, and plot that.
[00:30:24.100 --> 00:30:27.940]   So here, the y equals 0 line is the actual values.
[00:30:27.940 --> 00:30:32.140]   And you can see that my baseline is 0.4 off of the actual value
[00:30:32.140 --> 00:30:33.260]   at this point.
[00:30:33.260 --> 00:30:36.220]   And then my dense model is doing the best.
[00:30:36.220 --> 00:30:40.140]   And I get a really flexible way to dynamically query precisely
[00:30:40.140 --> 00:30:42.100]   what I want to know about my models.
[00:30:42.100 --> 00:30:45.660]   I can also group these results by that training time stamp
[00:30:45.660 --> 00:30:48.860]   rate as we add more data, we get better performance.
[00:30:48.860 --> 00:30:51.900]   I can specifically look at how much adding training data
[00:30:51.900 --> 00:30:53.060]   improves different models.
[00:30:53.060 --> 00:30:54.620]   So obviously, for the baseline, it's
[00:30:54.620 --> 00:30:57.540]   not going to change performance because it's rule-based.
[00:30:57.540 --> 00:31:00.140]   For the linear model, adding more data helps.
[00:31:00.140 --> 00:31:05.020]   And then for our two variants, maybe it's in the noise here.
[00:31:05.020 --> 00:31:08.540]   But this is all very easy to customize with this panel plot
[00:31:08.540 --> 00:31:11.980]   configuration where I can set the shape and size
[00:31:11.980 --> 00:31:16.300]   of the points based on a field in my data.
[00:31:16.300 --> 00:31:18.420]   Lastly, I want to show a sneak preview of where
[00:31:18.420 --> 00:31:20.260]   we're going with this that you can also
[00:31:20.260 --> 00:31:22.860]   highlight regions of the chart.
[00:31:22.860 --> 00:31:27.100]   So again, you can look at performance here by model type.
[00:31:27.100 --> 00:31:30.460]   And my simple model variants are doing pretty similarly.
[00:31:30.460 --> 00:31:33.420]   But hopefully, this gives you a preview of how flexible
[00:31:33.420 --> 00:31:34.820]   this time series support will be.
[00:31:34.820 --> 00:31:37.580]   And again, we'd love to hear from you on which features
[00:31:37.580 --> 00:31:39.820]   you would find most useful.
[00:31:39.820 --> 00:31:44.180]   You can also now share these profile links.
[00:31:44.180 --> 00:31:46.020]   And this is an early preview, but we'll
[00:31:46.020 --> 00:31:48.340]   be getting this out very soon.
[00:31:48.340 --> 00:31:51.260]   And thanks so much for your time.
[00:31:51.260 --> 00:31:51.980]   Awesome.
[00:31:51.980 --> 00:31:53.140]   Thanks, Stacey.
[00:31:53.140 --> 00:31:55.740]   It's really exciting to see our product investments come
[00:31:55.740 --> 00:31:57.740]   to life.
[00:31:57.740 --> 00:32:01.540]   Next up, I'm thrilled to dive into powerful new enterprise
[00:32:01.540 --> 00:32:05.160]   capabilities for WMB.
[00:32:05.160 --> 00:32:07.680]   And first up, I want to highlight
[00:32:07.680 --> 00:32:10.740]   that we've recently refactored our approach to tracking
[00:32:10.740 --> 00:32:15.860]   distributed training jobs within our Python SDK.
[00:32:15.860 --> 00:32:17.820]   Users can now easily log and then
[00:32:17.820 --> 00:32:22.080]   evaluate distributed training runs with WMB.
[00:32:22.080 --> 00:32:24.780]   With multiprocessing now generally available
[00:32:24.780 --> 00:32:27.100]   and enabled by default, the user experience
[00:32:27.100 --> 00:32:33.060]   is significantly improved with distributed settings.
[00:32:33.060 --> 00:32:35.700]   Simply call WMB in it from your training script
[00:32:35.700 --> 00:32:39.420]   and automatically track all of your distributed training
[00:32:39.420 --> 00:32:43.120]   runs from single or many processes
[00:32:43.120 --> 00:32:45.620]   and do this without sacrificing performance
[00:32:45.620 --> 00:32:46.900]   or the robustness of your jobs.
[00:32:46.900 --> 00:32:52.580]   Easy grouping via our runs table allows
[00:32:52.580 --> 00:32:56.420]   you to seamlessly explore data from your multi-node training
[00:32:56.420 --> 00:32:57.580]   jobs.
[00:32:57.580 --> 00:33:00.060]   Use our industry-leading ML visualizations
[00:33:00.060 --> 00:33:05.180]   to find exactly what you need from your distributed jobs.
[00:33:05.180 --> 00:33:09.340]   Really cementing WMB as the leading enterprise experiment
[00:33:09.340 --> 00:33:12.660]   tracking solution, our approach to multiprocessing
[00:33:12.660 --> 00:33:15.940]   significantly improves the user experience,
[00:33:15.940 --> 00:33:18.260]   and it ensures the reliability and robustness
[00:33:18.260 --> 00:33:23.700]   to errors or hanging jobs of your distributed training runs.
[00:33:23.700 --> 00:33:26.300]   To learn more about our approach to multiprocessing,
[00:33:26.300 --> 00:33:28.340]   I encourage everyone to check out the guide that's
[00:33:28.340 --> 00:33:30.540]   linked at the bottom of this slide.
[00:33:30.540 --> 00:33:32.940]   And you can go through in detail examples
[00:33:32.940 --> 00:33:35.780]   and understand exactly how our refactoring has
[00:33:35.780 --> 00:33:39.180]   benefited you, our core users.
[00:33:39.180 --> 00:33:42.540]   I'm also really excited to announce WMB Dedicated
[00:33:42.540 --> 00:33:45.020]   Cloud for Azure.
[00:33:45.020 --> 00:33:49.220]   The WMB Dedicated Cloud provides a secure and flexible
[00:33:49.220 --> 00:33:51.540]   managed private cloud environment,
[00:33:51.540 --> 00:33:55.300]   enabling enterprise ML ops at scale.
[00:33:55.300 --> 00:33:57.260]   And with this release, we've added support
[00:33:57.260 --> 00:33:59.220]   from Microsoft Azure.
[00:33:59.220 --> 00:34:02.780]   So now, our Dedicated Cloud supports all three major cloud
[00:34:02.780 --> 00:34:04.140]   providers--
[00:34:04.140 --> 00:34:07.860]   Azure, GCP, and AWS.
[00:34:07.860 --> 00:34:12.140]   Just as a reminder, we offer several deployment options
[00:34:12.140 --> 00:34:14.620]   for the WMB platform.
[00:34:14.620 --> 00:34:16.740]   You can deploy on bare metal servers
[00:34:16.740 --> 00:34:19.900]   in your on-premise data centers, configure
[00:34:19.900 --> 00:34:22.180]   a customer-managed production deployment
[00:34:22.180 --> 00:34:25.100]   on your own private cloud.
[00:34:25.100 --> 00:34:28.780]   We offer secure access via the multi-tenant public cloud
[00:34:28.780 --> 00:34:30.780]   as software as a service.
[00:34:30.780 --> 00:34:34.420]   And then finally, with the WMB Dedicated Cloud
[00:34:34.420 --> 00:34:37.380]   as a managed single-tenant cloud infrastructure,
[00:34:37.380 --> 00:34:40.300]   providing you with your choice of cloud provider and cloud
[00:34:40.300 --> 00:34:41.740]   region.
[00:34:41.740 --> 00:34:46.300]   With the WMB Dedicated Cloud, we offer the infrastructure
[00:34:46.300 --> 00:34:48.700]   management, and we're responsible for the platform
[00:34:48.700 --> 00:34:51.180]   performance, while giving you, the users,
[00:34:51.180 --> 00:34:57.380]   full control over your data with our secure storage connector.
[00:34:57.380 --> 00:34:58.900]   And this secure storage connector
[00:34:58.900 --> 00:35:02.860]   enables you to connect and manage your own secure object
[00:35:02.860 --> 00:35:05.340]   storage.
[00:35:05.340 --> 00:35:08.900]   It's with this configuration that we really
[00:35:08.900 --> 00:35:12.060]   unlock enterprise-grade security controls,
[00:35:12.060 --> 00:35:14.260]   providing isolation guarantees that
[00:35:14.260 --> 00:35:19.380]   are aligned with even the most stringent security protocols.
[00:35:19.380 --> 00:35:22.860]   While we provide you the guarantees
[00:35:22.860 --> 00:35:25.580]   of ensuring uptime of your mission-critical ML
[00:35:25.580 --> 00:35:28.460]   infrastructure.
[00:35:28.460 --> 00:35:31.380]   Collectively, the WMB cloud is trusted
[00:35:31.380 --> 00:35:35.020]   by some of the largest and even the most regulated companies
[00:35:35.020 --> 00:35:39.500]   in the world to deliver ML ops at scale.
[00:35:39.500 --> 00:35:41.020]   And we're really proud to now make it
[00:35:41.020 --> 00:35:44.340]   available on Microsoft Azure.
[00:35:48.180 --> 00:35:50.660]   And throughout today's webinar, you
[00:35:50.660 --> 00:35:52.780]   may have noticed a common theme.
[00:35:52.780 --> 00:35:54.980]   That's that Weights and Biases were really dedicated
[00:35:54.980 --> 00:35:57.900]   to meeting customers where they are.
[00:35:57.900 --> 00:35:59.380]   And with more and more enterprises
[00:35:59.380 --> 00:36:04.100]   relying on Microsoft Enterprise applications and services,
[00:36:04.100 --> 00:36:08.380]   we added a single sign-on authentication with Microsoft.
[00:36:08.380 --> 00:36:11.900]   So in addition to supporting standard identity management
[00:36:11.900 --> 00:36:16.140]   tools like OpenID or Active Directory,
[00:36:16.140 --> 00:36:18.780]   customers can use third-party SSO providers
[00:36:18.780 --> 00:36:21.860]   like Google and GitHub to authenticate users
[00:36:21.860 --> 00:36:22.740]   for WMB access.
[00:36:22.740 --> 00:36:28.260]   Logging into WMB with SSO authentication
[00:36:28.260 --> 00:36:30.460]   provides a really seamless experience
[00:36:30.460 --> 00:36:35.540]   for users with enterprise authentication providers.
[00:36:35.540 --> 00:36:42.100]   And we're thrilled to now offer authentication with Microsoft.
[00:36:42.100 --> 00:36:44.620]   With Azure Single Sign-On, we are
[00:36:44.620 --> 00:36:48.580]   providing a trouble-free and trusted access
[00:36:48.580 --> 00:36:52.180]   to WMB with a single Microsoft login,
[00:36:52.180 --> 00:36:57.060]   and that's without sacrificing any security.
[00:36:57.060 --> 00:36:59.900]   So it's with this release, users can manage authentication
[00:36:59.900 --> 00:37:03.940]   with Microsoft, which is really a trusted standard
[00:37:03.940 --> 00:37:08.300]   with enterprise-proven authentication.
[00:37:08.300 --> 00:37:09.900]   This means that when you as users
[00:37:09.900 --> 00:37:13.780]   invite others to collaborate on the WMB application,
[00:37:13.780 --> 00:37:15.580]   as long as they have a Microsoft account,
[00:37:15.580 --> 00:37:18.780]   they can automatically sign in and create a WMB account
[00:37:18.780 --> 00:37:21.220]   without further IT configuration.
[00:37:21.220 --> 00:37:24.580]   And speaking about security, WMB
[00:37:24.580 --> 00:37:28.380]   is trusted by some of the largest companies in the world
[00:37:28.380 --> 00:37:31.380]   to deliver ML opposite scale.
[00:37:31.380 --> 00:37:32.860]   Customers from around the world, they
[00:37:32.860 --> 00:37:37.620]   entrust us with their sensitive, mission-critical ML data.
[00:37:37.620 --> 00:37:39.500]   And really, nothing is more important to us
[00:37:39.500 --> 00:37:42.900]   than honoring this custodial commitment
[00:37:42.900 --> 00:37:46.940]   to protect all of their mission-critical information.
[00:37:46.940 --> 00:37:50.380]   So today, we're really proud to highlight our security position
[00:37:50.380 --> 00:37:55.020]   with the WMB Security and Compliance Center.
[00:37:55.020 --> 00:37:59.260]   We encourage you to visit security.wmb.ai
[00:37:59.260 --> 00:38:03.100]   to review details of several of our security frameworks,
[00:38:03.100 --> 00:38:05.740]   our regulation alignment, and certifications that
[00:38:05.740 --> 00:38:08.700]   apply not only to our company, but also
[00:38:08.700 --> 00:38:12.620]   to our full suite of products and our deployment options.
[00:38:12.620 --> 00:38:16.460]   The WMB Security Center, it's a centralized self-service
[00:38:16.460 --> 00:38:17.220]   portal.
[00:38:17.220 --> 00:38:20.620]   And it's made with enterprise customers
[00:38:20.620 --> 00:38:25.460]   in mind for fast and efficient evaluation of our enterprise
[00:38:25.460 --> 00:38:28.020]   security readiness.
[00:38:28.020 --> 00:38:30.260]   Within the Security Center, we ensure
[00:38:30.260 --> 00:38:32.140]   that our customers have all the information
[00:38:32.140 --> 00:38:35.900]   they need for fast compliance to common industry security
[00:38:35.900 --> 00:38:38.740]   standards.
[00:38:38.740 --> 00:38:42.300]   So again, we encourage everyone to visit this portal linked here
[00:38:42.300 --> 00:38:49.060]   on screen by going to security.wmb.ai.
[00:38:49.060 --> 00:38:50.860]   And with that, it's now my pleasure
[00:38:50.860 --> 00:38:53.780]   to pass it back to Phil to wrap up the event.
[00:38:53.780 --> 00:38:55.460]   Phil, back over to you.
[00:38:55.460 --> 00:38:55.940]   Thanks, Seth.
[00:38:55.940 --> 00:38:56.820]   That was awesome.
[00:38:56.820 --> 00:38:58.380]   Thanks for that.
[00:38:58.380 --> 00:39:00.500]   Wow, hopefully everybody's as energized as I am.
[00:39:00.500 --> 00:39:03.700]   I mean, this is a fantastic, amazing new capabilities.
[00:39:03.700 --> 00:39:06.020]   These innovations are pushing the industry forward.
[00:39:06.020 --> 00:39:07.700]   I can't wait to see what you all build
[00:39:07.700 --> 00:39:09.820]   on these new capabilities.
[00:39:09.820 --> 00:39:12.260]   Just as a recap, Kerry and Igor showed you
[00:39:12.260 --> 00:39:15.100]   how we're expanding to cover the most pressing pain
[00:39:15.100 --> 00:39:19.100]   points for our users across a broad set of ML workflow
[00:39:19.100 --> 00:39:21.540]   capabilities, namely models and launch,
[00:39:21.540 --> 00:39:24.060]   supporting your end-to-end machine learning lifecycle
[00:39:24.060 --> 00:39:26.340]   and ML workload orchestration needs.
[00:39:26.340 --> 00:39:29.460]   Stacey showcased the latest innovations for our users,
[00:39:29.460 --> 00:39:31.220]   and Seth highlighted how our users,
[00:39:31.220 --> 00:39:33.020]   with the most strict security requirements,
[00:39:33.020 --> 00:39:35.740]   are taking advantage of weights and biases.
[00:39:35.740 --> 00:39:37.860]   If you want to learn more, go deeper
[00:39:37.860 --> 00:39:39.340]   into each of these different areas,
[00:39:39.340 --> 00:39:44.060]   you can check out wandb.me/october18.
[00:39:44.060 --> 00:39:46.420]   So that's the details on the event.
[00:39:46.420 --> 00:39:48.500]   We have a bunch of documentation.
[00:39:48.500 --> 00:39:51.100]   I'm going to take a moment now to highlight
[00:39:51.100 --> 00:39:53.180]   a couple of things from Stacey's demonstration
[00:39:53.180 --> 00:39:55.980]   that we believe are really important.
[00:39:55.980 --> 00:40:00.020]   So the first thing I just want to touch on is time series.
[00:40:00.020 --> 00:40:01.980]   So when working with time series data,
[00:40:01.980 --> 00:40:04.580]   our users are building models to answer the question,
[00:40:04.580 --> 00:40:06.340]   what will happen in the future?
[00:40:06.340 --> 00:40:09.100]   Whether a self-driving car is predicting a future movement
[00:40:09.100 --> 00:40:11.940]   of an obstacle, or a CFO is forecasting sales
[00:40:11.940 --> 00:40:13.500]   for the second half of the year.
[00:40:13.500 --> 00:40:15.340]   And we believe that our users need the power
[00:40:15.340 --> 00:40:18.500]   to optimize the accuracy of their time series models.
[00:40:18.500 --> 00:40:21.420]   They need the ability to quickly backtest performance
[00:40:21.420 --> 00:40:23.620]   and compare with baseline model performance,
[00:40:23.620 --> 00:40:26.300]   and ultimately visualize the results exactly the way
[00:40:26.300 --> 00:40:27.140]   they want.
[00:40:27.140 --> 00:40:28.700]   So these time series improvements
[00:40:28.700 --> 00:40:31.500]   were specifically designed to support our users
[00:40:31.500 --> 00:40:36.020]   as they build the best possible time series forecasting models.
[00:40:36.020 --> 00:40:37.260]   3D bounding boxes.
[00:40:37.260 --> 00:40:40.540]   So we continue to expand first class support for machine
[00:40:40.540 --> 00:40:41.820]   learning media types.
[00:40:41.820 --> 00:40:44.860]   And we're doing that today by enhancing 3D object and point
[00:40:44.860 --> 00:40:45.820]   cloud logging.
[00:40:45.820 --> 00:40:50.100]   So users can log 3D objects, LiDAR point clouds,
[00:40:50.100 --> 00:40:53.300]   have 3D bounding box annotations, prediction scores,
[00:40:53.300 --> 00:40:56.940]   and camera viewpoints to easily render in weights and biases
[00:40:56.940 --> 00:41:00.340]   and explore your media data interactively.
[00:41:00.340 --> 00:41:01.880]   And the last thing I want to highlight
[00:41:01.880 --> 00:41:03.860]   is customizable profile pages.
[00:41:03.860 --> 00:41:05.780]   So as we saw during Stacey's demonstration,
[00:41:05.780 --> 00:41:08.060]   profile pages are a point of pride
[00:41:08.060 --> 00:41:09.980]   amongst our practitioners and teams.
[00:41:09.980 --> 00:41:13.780]   These pages show how users can showcase their work
[00:41:13.780 --> 00:41:15.860]   and share their machine learning knowledge,
[00:41:15.860 --> 00:41:17.740]   demonstrate credibility in the industry
[00:41:17.740 --> 00:41:20.780]   with the world's top ML practitioners and researchers.
[00:41:20.780 --> 00:41:23.860]   And you can think of this as like a GitHub profile for code.
[00:41:23.860 --> 00:41:27.700]   The new profiles allow users to control which information is
[00:41:27.700 --> 00:41:31.260]   publicly available, customize which projects are showcased.
[00:41:31.260 --> 00:41:32.740]   And this new capability allows you
[00:41:32.740 --> 00:41:34.940]   to build an ML portfolio that you're
[00:41:34.940 --> 00:41:36.900]   proud to share and really ignite ideas
[00:41:36.900 --> 00:41:38.300]   throughout the ML community.
[00:41:38.300 --> 00:41:40.380]   So I really can't wait to see what everybody builds
[00:41:40.380 --> 00:41:42.260]   and what you showcase on your profile pages
[00:41:42.260 --> 00:41:44.780]   and what you publish out in the open.
[00:41:44.780 --> 00:41:47.420]   So with that, we're going to transition over
[00:41:47.420 --> 00:41:49.780]   to question and answer.
[00:41:49.780 --> 00:41:52.580]   So that wraps up the formal presentation and content.
[00:41:52.580 --> 00:41:54.420]   We're going to bring everybody on stage here
[00:41:54.420 --> 00:41:55.620]   to answer your questions.
[00:41:55.620 --> 00:41:58.420]   So if you have questions, put them in the chat.
[00:41:58.420 --> 00:42:00.220]   I'm going to make sure all the questions are
[00:42:00.220 --> 00:42:02.380]   routed to the right person here.
[00:42:02.380 --> 00:42:04.940]   And then I want to mention again, our amazing events team
[00:42:04.940 --> 00:42:08.180]   will be sending out swag bags for the best questions, most
[00:42:08.180 --> 00:42:08.700]   engaged.
[00:42:08.700 --> 00:42:11.020]   So please make sure to keep this interactive
[00:42:11.020 --> 00:42:13.260]   and keep the engagement high.
[00:42:13.260 --> 00:42:15.580]   So please put all your questions in the chat.
[00:42:15.580 --> 00:42:18.380]   And we're going to bring all the presenters on stage
[00:42:18.380 --> 00:42:20.180]   here to answer the questions.
[00:42:20.180 --> 00:42:21.820]   So hang tight for a second.
[00:42:21.820 --> 00:42:24.900]   And we'll start Q&A here shortly.
[00:42:24.900 --> 00:42:25.820]   Welcome, everybody.
[00:42:25.820 --> 00:42:29.140]   Time for Q&A. Is everybody ready?
[00:42:29.140 --> 00:42:30.700]   All right.
[00:42:30.700 --> 00:42:34.100]   So first question-- let's see.
[00:42:34.100 --> 00:42:36.500]   So first question, this one looks
[00:42:36.500 --> 00:42:39.820]   like it's for Carrie from Chris.
[00:42:39.820 --> 00:42:42.180]   And so Chris had asked, how do we
[00:42:42.180 --> 00:42:46.620]   track the performance across multiple entities
[00:42:46.620 --> 00:42:47.380]   in the lineage?
[00:42:47.380 --> 00:42:49.820]   And so when viewing the lineage, how
[00:42:49.820 --> 00:42:51.140]   do you start to track performance
[00:42:51.140 --> 00:42:56.860]   across multiple different aspects of a model's lineage?
[00:42:56.860 --> 00:42:57.380]   Sure.
[00:42:57.380 --> 00:42:58.500]   Yeah, great question, Chris.
[00:42:58.500 --> 00:43:01.220]   So it sounds like you're training
[00:43:01.220 --> 00:43:06.620]   a model across different specific customers' data
[00:43:06.620 --> 00:43:10.420]   and then have basically a split where you have the main model
[00:43:10.420 --> 00:43:12.580]   that you've pre-trained and you're fine-tuning
[00:43:12.580 --> 00:43:14.180]   on each of those customers.
[00:43:14.180 --> 00:43:17.660]   And so what I would propose to set that up in the registry
[00:43:17.660 --> 00:43:20.060]   is to have a single registered model and then
[00:43:20.060 --> 00:43:25.420]   a set of different evaluation jobs that train and then test
[00:43:25.420 --> 00:43:29.420]   the model on those different customer test sets.
[00:43:29.420 --> 00:43:32.060]   You could also split that out as separate registered models
[00:43:32.060 --> 00:43:35.060]   per customer for each of the tasks that you have.
[00:43:35.060 --> 00:43:37.900]   And I think this is a great use case and actually something
[00:43:37.900 --> 00:43:40.140]   that we're discussing with a couple of other customers
[00:43:40.140 --> 00:43:40.860]   right now.
[00:43:40.860 --> 00:43:43.220]   What's the nicest way to set that up in the UI?
[00:43:43.220 --> 00:43:45.740]   So if you're interested in trying this out,
[00:43:45.740 --> 00:43:47.500]   actually, we'd love to follow up with you
[00:43:47.500 --> 00:43:49.780]   after this event to help you test out
[00:43:49.780 --> 00:43:52.580]   the model registry for this specific style of use case
[00:43:52.580 --> 00:43:57.180]   with split different customer data sets.
[00:43:57.180 --> 00:43:57.700]   Awesome.
[00:43:57.700 --> 00:43:59.420]   Thanks, Carrie.
[00:43:59.420 --> 00:44:02.420]   The next question, I think it's for me.
[00:44:02.420 --> 00:44:06.140]   So Vincent asked, when is there going to be a mobile app?
[00:44:06.140 --> 00:44:09.660]   And since the last update, the mobile web interface
[00:44:09.660 --> 00:44:12.820]   isn't working as performance.
[00:44:12.820 --> 00:44:15.100]   So Vincent, what I've heard is that our team already
[00:44:15.100 --> 00:44:17.060]   has a bug report and is working on this.
[00:44:17.060 --> 00:44:19.460]   And so we're going to fix the performance of mobile
[00:44:19.460 --> 00:44:20.740]   in short order for you.
[00:44:20.740 --> 00:44:22.260]   Mobile is super important to us.
[00:44:22.260 --> 00:44:25.300]   And so this is something that we've
[00:44:25.300 --> 00:44:29.140]   been investigating and investing in for quite some time.
[00:44:29.140 --> 00:44:30.700]   But it's really, really important
[00:44:30.700 --> 00:44:33.660]   that we serve your users and you wherever
[00:44:33.660 --> 00:44:35.660]   it is that you do your work, whether it's on the go
[00:44:35.660 --> 00:44:37.900]   or through mobile application.
[00:44:37.900 --> 00:44:39.500]   Maybe just a little interactivity,
[00:44:39.500 --> 00:44:42.900]   maybe a show of thumbs up or type in mobile in the chat.
[00:44:42.900 --> 00:44:45.620]   How many folks are using Weights and Biases
[00:44:45.620 --> 00:44:47.620]   on a mobile device or a tablet?
[00:44:47.620 --> 00:44:51.140]   Maybe just send in the chat a thumbs up or mobile,
[00:44:51.140 --> 00:44:53.380]   and then we can at least get a gauge from the audience
[00:44:53.380 --> 00:44:55.060]   on how many people are using us.
[00:44:55.060 --> 00:44:57.140]   But from what I can tell, it's something
[00:44:57.140 --> 00:44:58.220]   that's super important.
[00:44:58.220 --> 00:45:00.180]   Or it's an area of active investment for us.
[00:45:00.180 --> 00:45:02.660]   And we'll make sure that we clean up that experience for you
[00:45:02.660 --> 00:45:05.100]   in short order.
[00:45:05.100 --> 00:45:13.700]   OK, so the next question, this one is from Chris for--
[00:45:13.700 --> 00:45:15.980]   let's see.
[00:45:15.980 --> 00:45:18.620]   I think this one is actually for time series.
[00:45:18.620 --> 00:45:23.700]   So how is time computed in the process?
[00:45:23.700 --> 00:45:25.460]   This one's for Stacey from Chris.
[00:45:25.460 --> 00:45:27.220]   How is time computed in the process?
[00:45:27.220 --> 00:45:30.540]   Is there a good guide for how much you normally see?
[00:45:30.540 --> 00:45:33.620]   Or what is the window in terms of the time window
[00:45:33.620 --> 00:45:35.100]   that you're displaying on the chart?
[00:45:35.100 --> 00:45:36.500]   And how do you go about setting up
[00:45:36.500 --> 00:45:39.180]   the chart to properly display the amount of time
[00:45:39.180 --> 00:45:42.300]   on the x-axes?
[00:45:42.300 --> 00:45:44.500]   Yeah, so these are good questions.
[00:45:44.500 --> 00:45:46.060]   And there's lots of details here.
[00:45:46.060 --> 00:45:49.300]   And we're trying to be maximally flexible about this.
[00:45:49.300 --> 00:45:53.420]   So it's up to you to log for each row the time point.
[00:45:53.420 --> 00:45:57.380]   And you can take the time series or date time format
[00:45:57.380 --> 00:45:58.460]   from Pandas.
[00:45:58.460 --> 00:46:03.340]   You can convert it using the two-time stamp functionality
[00:46:03.340 --> 00:46:04.900]   on the panel plot.
[00:46:04.900 --> 00:46:07.900]   And the number of points that shows up
[00:46:07.900 --> 00:46:10.460]   when you first render the charts is the number of rows
[00:46:10.460 --> 00:46:12.140]   that you send to the table.
[00:46:12.140 --> 00:46:15.900]   And then you can also filter on top of that.
[00:46:15.900 --> 00:46:21.020]   So it's really up to the user to configure these things.
[00:46:21.020 --> 00:46:22.580]   And then maybe a follow-up question
[00:46:22.580 --> 00:46:24.980]   on the same topic for you, Stacey.
[00:46:24.980 --> 00:46:27.500]   This one came from Tung Tran.
[00:46:27.500 --> 00:46:29.700]   Can we compare different groups of data
[00:46:29.700 --> 00:46:33.780]   with the same labels on those time series charts?
[00:46:33.780 --> 00:46:37.100]   Different groups of data with the same labels.
[00:46:37.100 --> 00:46:42.860]   So I think the equivalent-- and I'm thinking out loud here
[00:46:42.860 --> 00:46:45.140]   through how to compare it to my project--
[00:46:45.140 --> 00:46:48.660]   would be that you, instead of grouping by model type,
[00:46:48.660 --> 00:46:51.740]   you would group by the variable of interest for you.
[00:46:51.740 --> 00:46:54.180]   And then you would basically take my report.
[00:46:54.180 --> 00:46:57.700]   But instead of the large and small and linear model
[00:46:57.700 --> 00:47:01.540]   variants, you would have the dimensions of interest for you.
[00:47:01.540 --> 00:47:02.180]   Awesome.
[00:47:02.180 --> 00:47:03.860]   The next one is for Igor.
[00:47:03.860 --> 00:47:08.540]   So Igor, Anisha had asked, what infrastructure targets
[00:47:08.540 --> 00:47:10.740]   are we prioritizing for launch?
[00:47:10.740 --> 00:47:12.780]   What infrastructure does it work with?
[00:47:12.780 --> 00:47:14.620]   Yeah, long-term, we want to interoperate
[00:47:14.620 --> 00:47:16.940]   with all of the large cloud players.
[00:47:16.940 --> 00:47:20.740]   This quarter, we're particularly focused on EKS.
[00:47:20.740 --> 00:47:22.780]   We're building out technical enablement,
[00:47:22.780 --> 00:47:27.420]   how to run locally in terms of Minikube.
[00:47:27.420 --> 00:47:29.540]   Again, high-level think of launch
[00:47:29.540 --> 00:47:32.580]   as kind of a connective tissue layer.
[00:47:32.580 --> 00:47:34.980]   All of the heavy lifting in terms
[00:47:34.980 --> 00:47:37.180]   of the auto-scaling and fault tolerance,
[00:47:37.180 --> 00:47:39.900]   that's all done on the Kubernetes side.
[00:47:39.900 --> 00:47:44.500]   And same thing with whatever infrastructure you choose.
[00:47:44.500 --> 00:47:47.700]   And so after this quarter is really
[00:47:47.700 --> 00:47:50.740]   focused on the Amazon ecosystem.
[00:47:50.740 --> 00:47:56.900]   But we'll subsequently expand to Google and Microsoft.
[00:47:56.900 --> 00:48:02.100]   And we currently also work well with SageMaker as well.
[00:48:02.100 --> 00:48:02.620]   Awesome.
[00:48:02.620 --> 00:48:04.300]   Thanks, Igor.
[00:48:04.300 --> 00:48:07.500]   Next question, I think probably for Stacey is best.
[00:48:07.500 --> 00:48:11.220]   So there's actually a couple of questions around audio files.
[00:48:11.220 --> 00:48:14.140]   So Carlos had asked, are there any investments
[00:48:14.140 --> 00:48:16.660]   or any work going on for more visualization of audio
[00:48:16.660 --> 00:48:19.500]   spectrograms as well as an audio player
[00:48:19.500 --> 00:48:23.380]   to be able to play back MDI files?
[00:48:23.380 --> 00:48:25.180]   Yeah, so we have audio support.
[00:48:25.180 --> 00:48:29.060]   And maybe someone can share an example in the chat
[00:48:29.060 --> 00:48:33.060]   where you can log audio files and you can play them back.
[00:48:33.060 --> 00:48:36.020]   And you can log spectrograms right now.
[00:48:36.020 --> 00:48:40.820]   That would be through the media panel.
[00:48:40.820 --> 00:48:43.420]   But that's definitely something that we're thinking about.
[00:48:43.420 --> 00:48:45.660]   And we'd love to support in more detail.
[00:48:45.660 --> 00:48:48.940]   Awesome.
[00:48:48.940 --> 00:48:51.180]   And then I think the next one is probably for Seth.
[00:48:51.180 --> 00:48:53.380]   So options for a single sign-on.
[00:48:53.380 --> 00:48:57.820]   So single sign-on, how are we thinking about the packaging
[00:48:57.820 --> 00:48:59.420]   and pricing of single sign-on?
[00:48:59.420 --> 00:49:02.100]   And there was a question, will single sign-on
[00:49:02.100 --> 00:49:04.460]   for the enterprise be offered outside of the enterprise
[00:49:04.460 --> 00:49:06.100]   package?
[00:49:06.100 --> 00:49:06.600]   Sure.
[00:49:06.600 --> 00:49:10.980]   So for now, we offer provider single sign-on authentication
[00:49:10.980 --> 00:49:12.340]   for all users.
[00:49:12.340 --> 00:49:16.060]   You'll get the benefit of using Google, GitHub, and now
[00:49:16.060 --> 00:49:19.380]   Microsoft for all users, regardless
[00:49:19.380 --> 00:49:25.340]   of the tier of subscription that you have.
[00:49:25.340 --> 00:49:29.740]   Our enterprise users, they get the most advanced SSO
[00:49:29.740 --> 00:49:33.020]   and authentication, including identity providers
[00:49:33.020 --> 00:49:35.420]   and Active Directory enablement.
[00:49:35.420 --> 00:49:38.580]   So it's really going to benefit those large organizations.
[00:49:38.580 --> 00:49:41.260]   But we have a passion for security
[00:49:41.260 --> 00:49:43.540]   and believe that everyone should have access
[00:49:43.540 --> 00:49:47.140]   to the common protocols for accessing software.
[00:49:47.140 --> 00:49:51.340]   And that's included with our third-party authentication.
[00:49:51.340 --> 00:49:51.820]   Awesome.
[00:49:51.820 --> 00:49:52.300]   Thanks, Seth.
[00:49:52.300 --> 00:49:54.900]   Maybe just to follow up for yourself on that,
[00:49:54.900 --> 00:49:56.620]   there was another question around,
[00:49:56.620 --> 00:49:58.460]   what are the most common deployment
[00:49:58.460 --> 00:50:02.780]   options for Wix and Vysis large customers?
[00:50:02.780 --> 00:50:05.340]   So you had mentioned a few during your update.
[00:50:05.340 --> 00:50:07.060]   What does a typical deployment look like?
[00:50:07.060 --> 00:50:09.460]   And what are some of the more common deployments
[00:50:09.460 --> 00:50:11.660]   that we have for the larger organizations
[00:50:11.660 --> 00:50:13.820]   that we work with?
[00:50:13.820 --> 00:50:14.500]   Sure.
[00:50:14.500 --> 00:50:17.860]   I think we're seeing a really transformational shift
[00:50:17.860 --> 00:50:19.620]   right now.
[00:50:19.620 --> 00:50:23.700]   I can say that, with confidence, the fastest growing deployment
[00:50:23.700 --> 00:50:25.620]   option for our enterprise customers
[00:50:25.620 --> 00:50:29.140]   is the WMB-dedicated cloud.
[00:50:29.140 --> 00:50:30.580]   And that's because users really get
[00:50:30.580 --> 00:50:32.020]   the benefit of both worlds.
[00:50:32.020 --> 00:50:37.820]   They can rely on our world-class engineers and site reliability
[00:50:37.820 --> 00:50:40.820]   engineers to manage uptime and so give confidence
[00:50:40.820 --> 00:50:44.260]   of the underlying architecture while still providing
[00:50:44.260 --> 00:50:47.420]   those enterprises with the flexibility they need to manage
[00:50:47.420 --> 00:50:53.700]   data securely or align to their data privacy protocols.
[00:50:53.700 --> 00:50:57.060]   So that's certainly, by far, the biggest growing segment
[00:50:57.060 --> 00:50:59.380]   that we're seeing of our enterprise adoption
[00:50:59.380 --> 00:51:02.100]   for deployment options.
[00:51:02.100 --> 00:51:02.600]   Awesome.
[00:51:02.600 --> 00:51:03.340]   Thanks, Seth.
[00:51:03.340 --> 00:51:05.660]   Next question from Alexander.
[00:51:05.660 --> 00:51:08.980]   This one might be Seth and/or Stacey.
[00:51:08.980 --> 00:51:12.260]   So the dashboard currently has a maximum sample count
[00:51:12.260 --> 00:51:14.700]   for around 1,500 for all plots.
[00:51:14.700 --> 00:51:16.980]   And if there are more data points
[00:51:16.980 --> 00:51:20.380]   than they're sampled from the larger sample set,
[00:51:20.380 --> 00:51:23.020]   are there plans to set this range manually
[00:51:23.020 --> 00:51:25.940]   or define sampling techniques?
[00:51:25.940 --> 00:51:29.540]   Seth, do you want to take that one or Stacey?
[00:51:29.540 --> 00:51:30.380]   I can take a pass.
[00:51:30.380 --> 00:51:32.180]   And then maybe, Stacey, you want to provide
[00:51:32.180 --> 00:51:34.380]   any additional clarification.
[00:51:34.380 --> 00:51:35.980]   So sampling is an interesting one.
[00:51:35.980 --> 00:51:37.980]   And it's something that we're focused on
[00:51:37.980 --> 00:51:42.580]   while providing speed and reliability of the platform
[00:51:42.580 --> 00:51:46.140]   without augmenting critical information.
[00:51:46.140 --> 00:51:51.380]   So I would suggest that our sampling protocol is now
[00:51:51.380 --> 00:51:54.180]   relying on underlying architecture,
[00:51:54.180 --> 00:52:01.460]   like with tables, and ongoing investments in our platform
[00:52:01.460 --> 00:52:04.540]   are going to make that more accessible, more scalable,
[00:52:04.540 --> 00:52:06.860]   and give more flexibility to customers
[00:52:06.860 --> 00:52:10.540]   to define the actual sampling methodology,
[00:52:10.540 --> 00:52:15.380]   whereas right now we're defining that on the user's behalf.
[00:52:15.380 --> 00:52:15.900]   Awesome.
[00:52:15.900 --> 00:52:17.860]   And then I'll also just add that it's something
[00:52:17.860 --> 00:52:20.660]   that we get asked questions about all the time.
[00:52:20.660 --> 00:52:23.940]   So it's something that's one of the common items
[00:52:23.940 --> 00:52:25.540]   on our backlog and something that we're
[00:52:25.540 --> 00:52:27.940]   looking at adding additional controls for users
[00:52:27.940 --> 00:52:30.460]   to have to be able to customize how the sampling techniques
[00:52:30.460 --> 00:52:32.340]   and really make sure that it's deterministic.
[00:52:32.340 --> 00:52:34.620]   So when you're comparing one project to the next,
[00:52:34.620 --> 00:52:36.540]   it's an apples to apples comparison in terms
[00:52:36.540 --> 00:52:39.100]   of deterministic sampling.
[00:52:39.100 --> 00:52:44.020]   OK, so the next question, this one, Alexander.
[00:52:44.020 --> 00:52:45.100]   So hey, thanks, everybody.
[00:52:45.100 --> 00:52:48.580]   3D visualization is very useful.
[00:52:48.580 --> 00:52:50.940]   When is it planned to actually be released?
[00:52:50.940 --> 00:52:55.340]   So when is the 3D bounding boxes capability planned to go live?
[00:52:55.340 --> 00:52:58.460]   So first, Stacey, on that one.
[00:52:58.460 --> 00:52:59.740]   Yes, I can take this one.
[00:52:59.740 --> 00:53:05.740]   So we release updates to our Python SDK monthly.
[00:53:05.740 --> 00:53:09.300]   And the full capabilities that Stacey highlighted today
[00:53:09.300 --> 00:53:11.260]   will be included in our next release, which
[00:53:11.260 --> 00:53:14.580]   is the first week of November.
[00:53:14.580 --> 00:53:17.500]   So users will have access to all of the capabilities
[00:53:17.500 --> 00:53:19.580]   that Stacey provided in her report.
[00:53:19.580 --> 00:53:22.540]   And I encourage everyone to check out that report as well
[00:53:22.540 --> 00:53:26.220]   with ongoing investments going throughout the quarter.
[00:53:26.220 --> 00:53:28.820]   And then next one for Igor.
[00:53:28.820 --> 00:53:31.660]   So this one comes from Aubrey.
[00:53:31.660 --> 00:53:34.660]   So to clarify, what is the current state
[00:53:34.660 --> 00:53:38.860]   of beta in terms of customer access and user access today?
[00:53:38.860 --> 00:53:41.340]   And then what are the plans for that over the coming months
[00:53:41.340 --> 00:53:42.500]   and quarters?
[00:53:42.500 --> 00:53:45.620]   Yeah, we're working with a select group of customers
[00:53:45.620 --> 00:53:50.180]   that if you're a good fit, then we'll
[00:53:50.180 --> 00:53:52.220]   grant you early access to this functionality.
[00:53:52.220 --> 00:53:53.980]   And then we want to make sure that we're
[00:53:53.980 --> 00:53:58.420]   going to build a tool that can actually solve your use case.
[00:53:58.420 --> 00:54:01.020]   And then you enlist essentially as a design partner.
[00:54:01.020 --> 00:54:04.060]   And so we set up a cadence where we work frequently with you
[00:54:04.060 --> 00:54:06.340]   to iterate upon your feedback.
[00:54:06.340 --> 00:54:08.620]   So if you think you're interested in the launch
[00:54:08.620 --> 00:54:10.860]   functionality, I would connect to your account team.
[00:54:10.860 --> 00:54:12.860]   And we can connect offline on a conversation
[00:54:12.860 --> 00:54:14.420]   to see if it's a good fit.
[00:54:14.420 --> 00:54:17.620]   Awesome.
[00:54:17.620 --> 00:54:20.660]   So I think with that, that's all the time
[00:54:20.660 --> 00:54:23.540]   we have today for questions.
[00:54:23.540 --> 00:54:25.020]   So that's a wrap for today.
[00:54:25.020 --> 00:54:27.260]   Thanks, everybody, for joining.
[00:54:27.260 --> 00:54:29.820]   Thank you for your engagement, participation.
[00:54:29.820 --> 00:54:31.820]   Swag bags will be sent out.
[00:54:31.820 --> 00:54:34.260]   Please follow up on the link.
[00:54:34.260 --> 00:54:37.740]   So wandb.me/october18.
[00:54:37.740 --> 00:54:39.140]   You can get all the details.
[00:54:39.140 --> 00:54:42.900]   You can also contact us for any additional questions.
[00:54:42.900 --> 00:54:43.420]   So thank you.
[00:54:43.420 --> 00:54:45.540]   Thank you for joining today's product event.
[00:54:45.540 --> 00:54:47.500]   Thanks for all the questions and our activity.
[00:54:47.500 --> 00:54:48.820]   Check out all the content.
[00:54:48.820 --> 00:54:50.860]   And I really, really can't wait to see what you all
[00:54:50.860 --> 00:54:51.980]   build with Weights & Biases.
[00:54:51.980 --> 00:54:53.580]   So have a great rest of your day.
[00:54:53.580 --> 00:54:56.620]   [MUSIC PLAYING]
[00:54:56.620 --> 00:54:58.180]   All right.
[00:54:58.340 --> 00:55:01.700]   [MUSIC PLAYING]
[00:55:01.700 --> 00:55:05.060]   [MUSIC PLAYING]
[00:55:05.620 --> 00:55:08.980]   [MUSIC PLAYING]
[00:55:09.500 --> 00:55:12.860]   [MUSIC PLAYING]
[00:55:12.860 --> 00:55:15.440]   (upbeat music)

