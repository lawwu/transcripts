
[00:00:00.000 --> 00:00:03.640]   The following is a conversation with Andrew Ng,
[00:00:03.640 --> 00:00:06.480]   one of the most impactful educators, researchers,
[00:00:06.480 --> 00:00:09.600]   innovators, and leaders in artificial intelligence
[00:00:09.600 --> 00:00:11.960]   and technology space in general.
[00:00:11.960 --> 00:00:15.320]   He co-founded Coursera and Google Brain,
[00:00:15.320 --> 00:00:19.640]   launched Deep Learning AI, Landing AI, and the AI Fund,
[00:00:19.640 --> 00:00:23.120]   and was the chief scientist at Baidu.
[00:00:23.120 --> 00:00:25.920]   As a Stanford professor and with Coursera
[00:00:25.920 --> 00:00:28.680]   and Deep Learning AI, he has helped educate
[00:00:28.680 --> 00:00:32.840]   and inspire millions of students, including me.
[00:00:32.840 --> 00:00:36.360]   This is the Artificial Intelligence Podcast.
[00:00:36.360 --> 00:00:38.560]   If you enjoy it, subscribe on YouTube,
[00:00:38.560 --> 00:00:40.520]   get five stars on Apple Podcasts,
[00:00:40.520 --> 00:00:43.840]   support it on Patreon, or simply connect with me on Twitter
[00:00:43.840 --> 00:00:48.400]   at Lex Friedman, spelled F-R-I-D-M-A-N.
[00:00:48.400 --> 00:00:51.320]   As usual, I'll do one or two minutes of ads now,
[00:00:51.320 --> 00:00:52.760]   and never any ads in the middle
[00:00:52.760 --> 00:00:54.980]   that can break the flow of the conversation.
[00:00:54.980 --> 00:00:56.400]   I hope that works for you
[00:00:56.400 --> 00:00:59.040]   and doesn't hurt the listening experience.
[00:00:59.040 --> 00:01:01.200]   This show is presented by Cash App,
[00:01:01.200 --> 00:01:03.600]   the number one finance app in the App Store.
[00:01:03.600 --> 00:01:07.080]   When you get it, use code LEXPODCAST.
[00:01:07.080 --> 00:01:09.360]   Cash App lets you send money to friends,
[00:01:09.360 --> 00:01:11.860]   buy Bitcoin, and invest in the stock market
[00:01:11.860 --> 00:01:13.760]   with as little as $1.
[00:01:13.760 --> 00:01:16.760]   Brokerage services are provided by Cash App Investing,
[00:01:16.760 --> 00:01:20.800]   a subsidiary of Square, and member SIPC.
[00:01:20.800 --> 00:01:23.120]   Since Cash App allows you to buy Bitcoin,
[00:01:23.120 --> 00:01:25.400]   let me mention that cryptocurrency,
[00:01:25.400 --> 00:01:28.840]   in the context of the history of money, is fascinating.
[00:01:28.840 --> 00:01:31.200]   I recommend "A Scent of Money"
[00:01:31.200 --> 00:01:33.820]   as a great book on this history.
[00:01:33.820 --> 00:01:38.540]   Debits and credits on ledgers started over 30,000 years ago.
[00:01:38.540 --> 00:01:42.280]   The US dollar was created over 200 years ago,
[00:01:42.280 --> 00:01:45.480]   and Bitcoin, the first decentralized cryptocurrency,
[00:01:45.480 --> 00:01:48.220]   released just over 10 years ago.
[00:01:48.220 --> 00:01:51.560]   So given that history, cryptocurrency's still very much
[00:01:51.560 --> 00:01:53.680]   in its early days of development,
[00:01:53.680 --> 00:01:56.700]   but it's still aiming to, and just might,
[00:01:56.700 --> 00:01:59.800]   redefine the nature of money.
[00:01:59.800 --> 00:02:01.680]   So again, if you get Cash App
[00:02:01.680 --> 00:02:03.480]   from the App Store or Google Play,
[00:02:03.480 --> 00:02:07.320]   and use the code LEXPODCAST, you'll get $10,
[00:02:07.320 --> 00:02:10.180]   and Cash App will also donate $10 to FIRST,
[00:02:10.180 --> 00:02:12.200]   one of my favorite organizations
[00:02:12.200 --> 00:02:15.460]   that is helping to advance robotics and STEM education
[00:02:15.460 --> 00:02:17.680]   for young people around the world.
[00:02:17.680 --> 00:02:22.300]   And now, here's my conversation with Andrew Ng.
[00:02:23.200 --> 00:02:25.920]   The courses you taught on machine learning at Stanford,
[00:02:25.920 --> 00:02:29.480]   and later on Coursera that you co-founded,
[00:02:29.480 --> 00:02:31.880]   have educated and inspired millions of people.
[00:02:31.880 --> 00:02:35.080]   So let me ask you, what people or ideas inspired you
[00:02:35.080 --> 00:02:37.160]   to get into computer science and machine learning
[00:02:37.160 --> 00:02:38.160]   when you were young?
[00:02:38.160 --> 00:02:41.600]   When did you first fall in love with the field,
[00:02:41.600 --> 00:02:42.840]   is another way to put it?
[00:02:42.840 --> 00:02:45.440]   - Growing up in Hong Kong and Singapore,
[00:02:45.440 --> 00:02:50.160]   I started learning to code when I was five or six years old.
[00:02:50.160 --> 00:02:53.720]   At that time, I was learning the basic programming language,
[00:02:53.720 --> 00:02:56.200]   and I would take these books and they'll tell you,
[00:02:56.200 --> 00:02:57.960]   type this program into your computer.
[00:02:57.960 --> 00:03:00.120]   So type that program to my computer.
[00:03:00.120 --> 00:03:02.520]   And as a result of all that typing,
[00:03:02.520 --> 00:03:04.680]   I would get to play these very simple,
[00:03:04.680 --> 00:03:07.880]   shoot them up games that I had implemented
[00:03:07.880 --> 00:03:09.920]   on my little computer.
[00:03:09.920 --> 00:03:13.000]   So I thought it was fascinating as a young kid
[00:03:13.000 --> 00:03:15.000]   that I could write this code,
[00:03:15.000 --> 00:03:17.020]   that was really just copying code from a book
[00:03:17.020 --> 00:03:21.040]   into my computer to then play these cool little video games.
[00:03:21.040 --> 00:03:25.200]   Another moment for me was when I was a teenager
[00:03:25.200 --> 00:03:27.840]   and my father, 'cause he's a doctor,
[00:03:27.840 --> 00:03:31.360]   was reading about expert systems and about neural networks.
[00:03:31.360 --> 00:03:32.880]   So he got me to read some of these books
[00:03:32.880 --> 00:03:34.800]   and I thought it was really cool
[00:03:34.800 --> 00:03:36.240]   that you could write a computer
[00:03:36.240 --> 00:03:39.200]   that started to exhibit intelligence.
[00:03:39.200 --> 00:03:41.680]   Then I remember doing an internship
[00:03:41.680 --> 00:03:44.440]   while I was in high school, this was in Singapore,
[00:03:44.440 --> 00:03:47.660]   where I remember doing a lot of photocopying
[00:03:47.660 --> 00:03:50.380]   and I was an office assistant.
[00:03:50.380 --> 00:03:51.580]   And the highlight of my job
[00:03:51.580 --> 00:03:53.820]   was when I got to use the Shredder.
[00:03:53.820 --> 00:03:55.980]   So the teenager in me remember thinking,
[00:03:55.980 --> 00:03:57.820]   boy, this is a lot of photocopying.
[00:03:57.820 --> 00:03:59.660]   If only we could write software, build a robot,
[00:03:59.660 --> 00:04:01.100]   something to automate this,
[00:04:01.100 --> 00:04:03.020]   maybe I could do something else.
[00:04:03.020 --> 00:04:05.480]   So I think a lot of my work since then
[00:04:05.480 --> 00:04:07.660]   has centered on the theme of automation.
[00:04:07.660 --> 00:04:10.060]   Even the way I think about machine learning today,
[00:04:10.060 --> 00:04:12.060]   we're very good at writing learning algorithms
[00:04:12.060 --> 00:04:15.080]   that can automate things that people can do.
[00:04:15.080 --> 00:04:16.960]   Or even launching the first MOOCs,
[00:04:16.960 --> 00:04:20.120]   Mass Open Online Courses, that later led to Coursera,
[00:04:20.120 --> 00:04:23.320]   I was trying to automate what could be automatable
[00:04:23.320 --> 00:04:25.400]   in how I was teaching on campus.
[00:04:25.400 --> 00:04:26.480]   - Process of education,
[00:04:26.480 --> 00:04:30.360]   try to automate parts of that to make it more,
[00:04:30.360 --> 00:04:33.400]   sort of to have more impact from a single teacher,
[00:04:33.400 --> 00:04:34.760]   a single educator.
[00:04:34.760 --> 00:04:37.920]   - Yeah, I felt, teaching at Stanford,
[00:04:37.920 --> 00:04:39.160]   I was teaching machine learning
[00:04:39.160 --> 00:04:41.320]   to about 400 students a year at the time.
[00:04:41.320 --> 00:04:46.140]   And I found myself filming the exact same video every year,
[00:04:46.140 --> 00:04:48.840]   telling the same jokes in the same room.
[00:04:48.840 --> 00:04:50.420]   And I thought, why am I doing this?
[00:04:50.420 --> 00:04:51.860]   Why don't we just take last year's video
[00:04:51.860 --> 00:04:53.080]   and then I can spend my time
[00:04:53.080 --> 00:04:55.340]   building a deeper relationship with students.
[00:04:55.340 --> 00:04:58.020]   So that process of thinking through how to do that,
[00:04:58.020 --> 00:05:00.800]   that led to the first MOOCs that we launched.
[00:05:00.800 --> 00:05:05.420]   - And then you have more time to write new jokes.
[00:05:05.420 --> 00:05:07.540]   Are there favorite memories from your early days
[00:05:07.540 --> 00:05:10.100]   at Stanford teaching thousands of people in person
[00:05:10.100 --> 00:05:13.220]   and then millions of people online?
[00:05:13.220 --> 00:05:17.020]   - You know, teaching online,
[00:05:17.020 --> 00:05:20.340]   what not many people know was that
[00:05:20.340 --> 00:05:22.500]   a lot of those videos were shot
[00:05:22.500 --> 00:05:26.380]   between the hours of 10 PM and 3 AM.
[00:05:26.380 --> 00:05:31.100]   A lot of times, launching the first MOOCs at Stanford,
[00:05:31.100 --> 00:05:32.260]   we'd already announced the course,
[00:05:32.260 --> 00:05:34.380]   about 100,000 people had signed up.
[00:05:34.380 --> 00:05:36.520]   We just started to write the code
[00:05:36.520 --> 00:05:39.260]   and we had not yet actually filmed the videos.
[00:05:39.260 --> 00:05:40.620]   So we had a lot of pressure,
[00:05:40.620 --> 00:05:43.400]   100,000 people waiting for us to produce the content.
[00:05:43.400 --> 00:05:46.220]   So many Fridays, Saturdays,
[00:05:46.220 --> 00:05:49.260]   I would go out, have dinner with my friends,
[00:05:49.260 --> 00:05:51.580]   and then I would think, okay, do you want to go home now?
[00:05:51.580 --> 00:05:54.740]   Or do you want to go to the office to film videos?
[00:05:54.740 --> 00:05:57.820]   And the thought of being able to help 100,000 people
[00:05:57.820 --> 00:05:59.500]   potentially learn machine learning,
[00:05:59.500 --> 00:06:01.940]   fortunately that made me think,
[00:06:01.940 --> 00:06:03.180]   okay, I'm going to go to my office,
[00:06:03.180 --> 00:06:05.460]   go to my tiny little recording studio.
[00:06:05.460 --> 00:06:07.940]   I would adjust my Logic Webcam,
[00:06:07.940 --> 00:06:10.620]   adjust my Wacom tablet,
[00:06:10.620 --> 00:06:12.580]   make sure my lapel mic was on,
[00:06:12.580 --> 00:06:13.940]   and then I would start recording,
[00:06:13.940 --> 00:06:16.060]   often until 2 AM or 3 AM.
[00:06:16.060 --> 00:06:18.460]   I think unfortunately that doesn't show
[00:06:18.460 --> 00:06:20.500]   that it was recorded that late at night,
[00:06:20.500 --> 00:06:22.900]   but it was really inspiring,
[00:06:22.900 --> 00:06:25.580]   the thought that we could create content
[00:06:25.580 --> 00:06:27.940]   to help so many people learn about machine learning.
[00:06:27.940 --> 00:06:29.480]   - How did that feel?
[00:06:29.480 --> 00:06:31.540]   The fact that you're probably somewhat alone,
[00:06:31.540 --> 00:06:33.740]   maybe a couple of friends,
[00:06:33.740 --> 00:06:36.180]   recording with a Logitech webcam
[00:06:36.180 --> 00:06:41.060]   and kind of going home alone at 1 or 2 AM at night,
[00:06:41.060 --> 00:06:43.260]   and knowing that that's going to reach
[00:06:43.260 --> 00:06:45.300]   sort of thousands of people,
[00:06:45.300 --> 00:06:47.020]   eventually millions of people.
[00:06:47.020 --> 00:06:48.980]   What's that feeling like?
[00:06:48.980 --> 00:06:51.700]   I mean, is there a feeling of just satisfaction
[00:06:51.700 --> 00:06:54.180]   of pushing through?
[00:06:54.180 --> 00:06:55.180]   - I think it's humbling.
[00:06:55.180 --> 00:06:58.020]   And I wasn't thinking about what I was feeling.
[00:06:58.020 --> 00:07:00.740]   I think one thing that I'm proud to say
[00:07:00.740 --> 00:07:02.540]   we got right from the early days
[00:07:02.540 --> 00:07:05.260]   was I told my whole team back then
[00:07:05.260 --> 00:07:06.500]   that the number one priority
[00:07:06.500 --> 00:07:08.060]   is to do what's best for learners,
[00:07:08.060 --> 00:07:09.260]   do what's best for students.
[00:07:09.260 --> 00:07:11.580]   And so when I went to the recording studio,
[00:07:11.580 --> 00:07:13.340]   the only thing on my mind was,
[00:07:13.340 --> 00:07:14.180]   what can I say?
[00:07:14.180 --> 00:07:15.260]   How can I design my slides?
[00:07:15.260 --> 00:07:16.660]   What do I need to draw right
[00:07:16.660 --> 00:07:20.620]   to make these concepts as clear as possible for learners?
[00:07:20.620 --> 00:07:21.580]   I think, you know,
[00:07:21.580 --> 00:07:24.140]   I've seen sometimes instructors is tempting to,
[00:07:24.140 --> 00:07:25.500]   "Hey, let's talk about my work.
[00:07:25.500 --> 00:07:27.340]   "Maybe if I teach you about my research,
[00:07:27.340 --> 00:07:30.020]   "someone will cite my papers a couple more times."
[00:07:30.020 --> 00:07:31.940]   And I think one of the things we got right
[00:07:31.940 --> 00:07:33.060]   launched the first few MOOCs,
[00:07:33.060 --> 00:07:34.220]   and later building Coursera
[00:07:34.220 --> 00:07:37.060]   was putting in place that bedrock principle
[00:07:37.060 --> 00:07:38.940]   of let's just do what's best for learners
[00:07:38.940 --> 00:07:40.260]   and forget about everything else.
[00:07:40.260 --> 00:07:43.260]   And I think that that as a guiding principle
[00:07:43.260 --> 00:07:44.820]   turned out to be really important
[00:07:44.820 --> 00:07:46.940]   to the rise of the MOOC movement.
[00:07:46.940 --> 00:07:49.340]   - And the kind of learner you imagined in your mind
[00:07:49.340 --> 00:07:52.700]   is as broad as possible,
[00:07:52.700 --> 00:07:53.980]   as global as possible.
[00:07:53.980 --> 00:07:56.340]   So really try to reach as many people
[00:07:56.340 --> 00:07:59.860]   interested in machine learning and AI as possible.
[00:07:59.860 --> 00:08:01.060]   - I really want to help anyone
[00:08:01.060 --> 00:08:03.300]   that had an interest in machine learning
[00:08:03.300 --> 00:08:04.660]   to break into the field.
[00:08:04.660 --> 00:08:07.060]   And I think sometimes,
[00:08:07.060 --> 00:08:08.260]   I've actually had people ask me,
[00:08:08.260 --> 00:08:09.940]   "Hey, why are you spending so much time
[00:08:09.940 --> 00:08:11.660]   "explaining gradient descent?"
[00:08:11.660 --> 00:08:13.820]   And my answer was,
[00:08:13.820 --> 00:08:15.580]   "If I look at what I think the learner needs
[00:08:15.580 --> 00:08:16.660]   "and what benefit from,
[00:08:16.660 --> 00:08:18.980]   "I felt that having that,
[00:08:18.980 --> 00:08:20.620]   "a good understanding of the foundations,
[00:08:20.620 --> 00:08:21.940]   "kind of back to the basics,
[00:08:21.940 --> 00:08:23.860]   "would put them in a better state
[00:08:23.860 --> 00:08:26.820]   "to then build on a long-term career."
[00:08:26.820 --> 00:08:29.620]   So we've tried to consistently make decisions
[00:08:29.620 --> 00:08:30.620]   on that principle.
[00:08:30.620 --> 00:08:32.700]   - So one of the things you actually revealed
[00:08:32.700 --> 00:08:36.620]   to the narrow AI community at the time
[00:08:36.620 --> 00:08:38.140]   and to the world,
[00:08:38.140 --> 00:08:39.660]   is that the amount of people
[00:08:39.660 --> 00:08:41.100]   who are actually interested in AI
[00:08:41.100 --> 00:08:43.540]   is much larger than we imagined.
[00:08:43.540 --> 00:08:44.900]   By you teaching the class
[00:08:44.900 --> 00:08:47.060]   and how popular it became,
[00:08:47.060 --> 00:08:48.500]   it showed that,
[00:08:48.500 --> 00:08:50.980]   wow, this isn't just a small community
[00:08:50.980 --> 00:08:54.540]   of sort of people who go to NeurIPS
[00:08:54.540 --> 00:08:56.740]   and it's much bigger.
[00:08:56.740 --> 00:08:57.860]   It's developers,
[00:08:57.860 --> 00:08:59.860]   it's people from all over the world.
[00:08:59.860 --> 00:09:00.820]   I mean, I'm Russian,
[00:09:00.820 --> 00:09:03.420]   so everybody in Russia is really interested.
[00:09:03.420 --> 00:09:04.860]   There's a huge number of programmers
[00:09:04.860 --> 00:09:06.580]   who are interested in machine learning.
[00:09:06.580 --> 00:09:10.820]   India, China, South America, everywhere.
[00:09:10.820 --> 00:09:12.300]   There's just millions of people
[00:09:12.300 --> 00:09:13.660]   who are interested in machine learning.
[00:09:13.660 --> 00:09:15.140]   So how big do you get a sense
[00:09:15.140 --> 00:09:17.060]   that the number of people is
[00:09:17.060 --> 00:09:20.460]   that are interested from your perspective?
[00:09:20.460 --> 00:09:22.820]   - I think the number's grown over time.
[00:09:22.820 --> 00:09:23.940]   I think it's one of those things
[00:09:23.940 --> 00:09:26.540]   that maybe it feels like it came out of nowhere,
[00:09:26.540 --> 00:09:27.980]   but there's an inside that building it.
[00:09:27.980 --> 00:09:28.820]   It took years.
[00:09:28.820 --> 00:09:30.380]   It's not those overnight successes
[00:09:30.380 --> 00:09:33.220]   that took years to get there.
[00:09:33.220 --> 00:09:36.060]   My first foray into this type of online education
[00:09:36.060 --> 00:09:37.940]   was when we were filming my Stanford class
[00:09:37.940 --> 00:09:39.500]   and sticking the videos on YouTube
[00:09:39.500 --> 00:09:40.500]   and then some other things.
[00:09:40.500 --> 00:09:42.140]   We had uploaded the whole works and so on,
[00:09:42.140 --> 00:09:44.340]   but it's basically the one hour,
[00:09:44.340 --> 00:09:46.940]   15 minute video that we put on YouTube.
[00:09:46.940 --> 00:09:50.020]   And then we had four or five other versions
[00:09:50.020 --> 00:09:52.140]   of websites that I had built,
[00:09:52.140 --> 00:09:53.900]   most of which you would never have heard of
[00:09:53.900 --> 00:09:55.900]   because they reached small audiences,
[00:09:55.900 --> 00:09:57.580]   but that allowed me to iterate,
[00:09:57.580 --> 00:09:59.060]   allowed my team and me to iterate,
[00:09:59.060 --> 00:10:02.420]   to learn what the ideas that work and what doesn't.
[00:10:02.420 --> 00:10:03.780]   For example, one of the features
[00:10:03.780 --> 00:10:05.860]   I was really excited about and really proud of
[00:10:05.860 --> 00:10:08.460]   was build this website where multiple people
[00:10:08.460 --> 00:10:11.340]   could be logged into the website at the same time.
[00:10:11.340 --> 00:10:13.580]   So today, if you go to a website,
[00:10:13.580 --> 00:10:15.940]   if you're logged in and then I want to log in,
[00:10:15.940 --> 00:10:17.620]   you need to log out if it's the same browser,
[00:10:17.620 --> 00:10:18.940]   same computer.
[00:10:18.940 --> 00:10:20.340]   But I thought, well, what if two people,
[00:10:20.340 --> 00:10:23.260]   say you and me were watching a video together
[00:10:23.260 --> 00:10:24.260]   in front of a computer?
[00:10:24.260 --> 00:10:27.220]   What if a website could have you type your name
[00:10:27.220 --> 00:10:29.100]   and password, have me type my name and password?
[00:10:29.100 --> 00:10:30.260]   And then now the computer knows
[00:10:30.260 --> 00:10:31.860]   both of us are watching together
[00:10:31.860 --> 00:10:33.220]   and it gives both of us credit
[00:10:33.220 --> 00:10:35.420]   for anything we do as a group.
[00:10:35.420 --> 00:10:37.020]   Influences feature rolled it out
[00:10:37.020 --> 00:10:39.740]   in a school in San Francisco.
[00:10:39.740 --> 00:10:41.900]   We had about 20 something users.
[00:10:41.900 --> 00:10:44.020]   Where's the teacher there?
[00:10:44.020 --> 00:10:46.340]   Sacred Heart Cathedral Prep, the teacher was great.
[00:10:46.340 --> 00:10:47.380]   I mean, guess what?
[00:10:47.380 --> 00:10:49.940]   Zero people use this feature.
[00:10:49.940 --> 00:10:51.940]   It turns out people studying online,
[00:10:51.940 --> 00:10:54.060]   they want to watch the videos by themselves.
[00:10:54.060 --> 00:10:56.700]   So you can play back, pause at your own speed
[00:10:56.700 --> 00:10:57.780]   rather than in groups.
[00:10:57.780 --> 00:11:00.580]   So that was one example of a tiny lesson learned
[00:11:00.580 --> 00:11:03.260]   out of many that allows us to hone in
[00:11:03.260 --> 00:11:04.820]   to the set of features.
[00:11:04.820 --> 00:11:06.460]   - And it sounds like a brilliant feature.
[00:11:06.460 --> 00:11:09.380]   So I guess the lesson to take from that is
[00:11:09.380 --> 00:11:13.940]   there's something that looks amazing on paper
[00:11:13.940 --> 00:11:15.260]   and then nobody uses it.
[00:11:15.260 --> 00:11:17.140]   It doesn't actually have the impact
[00:11:17.140 --> 00:11:18.380]   that you think it might have.
[00:11:18.380 --> 00:11:20.460]   I say, yeah, I saw that you've really went
[00:11:20.460 --> 00:11:23.380]   through a lot of different features and a lot of ideas
[00:11:23.380 --> 00:11:24.460]   and to arrive at the final,
[00:11:24.460 --> 00:11:27.700]   at Coursera's final kind of powerful thing
[00:11:27.700 --> 00:11:32.220]   that showed the world that MOOCs can educate millions.
[00:11:32.220 --> 00:11:34.860]   - And I think with the whole machine learning movement
[00:11:34.860 --> 00:11:38.340]   as well, I think it didn't come out of nowhere.
[00:11:38.340 --> 00:11:41.460]   Instead, what happened was as more people learn
[00:11:41.460 --> 00:11:43.460]   about machine learning, they will tell their friends
[00:11:43.460 --> 00:11:44.900]   and their friends will see how it's applicable
[00:11:44.900 --> 00:11:46.020]   to their work.
[00:11:46.020 --> 00:11:48.620]   And then the community kept on growing.
[00:11:48.620 --> 00:11:50.980]   And I think we're still growing.
[00:11:50.980 --> 00:11:53.700]   I don't know in the future what percentage
[00:11:53.700 --> 00:11:56.980]   of our developers will be AI developers.
[00:11:56.980 --> 00:11:59.740]   I could easily see it being north of 50%, right?
[00:11:59.740 --> 00:12:04.660]   Because so many AI developers broadly construed,
[00:12:04.660 --> 00:12:06.540]   not just people doing the machine learning modeling,
[00:12:06.540 --> 00:12:09.620]   but the people building infrastructure, data pipelines,
[00:12:09.620 --> 00:12:11.580]   you know, all the software's surrounding
[00:12:11.580 --> 00:12:13.820]   the core machine learning model.
[00:12:13.820 --> 00:12:15.300]   Maybe it's even bigger.
[00:12:15.300 --> 00:12:18.300]   I feel like today, almost every software engineer
[00:12:18.300 --> 00:12:20.460]   has some understanding of the cloud.
[00:12:20.460 --> 00:12:22.540]   Not all, you know, but maybe it's just
[00:12:22.540 --> 00:12:25.500]   my microcontroller developer doesn't need to deal with cloud.
[00:12:25.500 --> 00:12:28.940]   But I feel like the vast majority of software engineers today
[00:12:28.940 --> 00:12:31.980]   are sort of having an appreciation of the cloud.
[00:12:31.980 --> 00:12:35.060]   I think in the future, maybe we'll approach nearly 100%
[00:12:35.060 --> 00:12:38.020]   of all developers being, you know, in some way,
[00:12:38.020 --> 00:12:40.620]   an AI developer, at least having an appreciation
[00:12:40.620 --> 00:12:41.980]   of machine learning.
[00:12:41.980 --> 00:12:45.060]   - And my hope is that there's this kind of effect
[00:12:45.060 --> 00:12:47.060]   that there's people who are not really interested
[00:12:47.660 --> 00:12:50.980]   in being a programmer or being into software engineering,
[00:12:50.980 --> 00:12:54.500]   like biologists, chemists, and physicists,
[00:12:54.500 --> 00:12:57.180]   even mechanical engineers, all these disciplines
[00:12:57.180 --> 00:13:01.580]   that are now more and more sitting on large datasets.
[00:13:01.580 --> 00:13:03.700]   And here, they didn't think they're interested
[00:13:03.700 --> 00:13:05.700]   in programming until they have this dataset
[00:13:05.700 --> 00:13:07.820]   and they realized there's this set of machine learning tools
[00:13:07.820 --> 00:13:09.420]   that allow you to use the dataset.
[00:13:09.420 --> 00:13:12.180]   So they actually become, they learn to program
[00:13:12.180 --> 00:13:13.580]   and they become new programmers.
[00:13:13.580 --> 00:13:16.220]   So like the, not just, 'cause you've mentioned
[00:13:16.220 --> 00:13:17.780]   a larger percentage of developers
[00:13:17.780 --> 00:13:19.660]   become machine learning people.
[00:13:19.660 --> 00:13:21.900]   The, it seems like more and more,
[00:13:21.900 --> 00:13:25.420]   the kinds of people who are becoming developers
[00:13:25.420 --> 00:13:27.380]   is also growing significantly.
[00:13:27.380 --> 00:13:29.900]   - Yeah, I think once upon a time,
[00:13:29.900 --> 00:13:32.780]   only a small part of humanity was literate,
[00:13:32.780 --> 00:13:33.860]   you know, could read and write.
[00:13:33.860 --> 00:13:36.620]   And maybe you thought, maybe not everyone needs
[00:13:36.620 --> 00:13:37.620]   to learn to read and write.
[00:13:37.620 --> 00:13:42.100]   You know, you just go listen to a few monks, right?
[00:13:42.100 --> 00:13:44.220]   Read to you, and maybe that was enough.
[00:13:44.220 --> 00:13:46.420]   Or maybe we just need a few handful of authors
[00:13:46.420 --> 00:13:47.780]   to write the best sellers,
[00:13:47.780 --> 00:13:50.340]   and then no one else needs to write.
[00:13:50.340 --> 00:13:53.180]   But what we found was that by giving as many people,
[00:13:53.180 --> 00:13:55.380]   you know, in some countries, almost everyone,
[00:13:55.380 --> 00:13:58.060]   basic literacy, it dramatically enhanced
[00:13:58.060 --> 00:13:59.460]   human to human communications.
[00:13:59.460 --> 00:14:01.380]   And we can now write for an audience of one,
[00:14:01.380 --> 00:14:04.980]   such as if I send you an email or you send me an email.
[00:14:04.980 --> 00:14:07.780]   I think in computing, we're still in that phase
[00:14:07.780 --> 00:14:09.660]   where so few people know how to code
[00:14:09.660 --> 00:14:12.140]   that the coders mostly have to code
[00:14:12.140 --> 00:14:14.500]   for relatively large audiences.
[00:14:14.500 --> 00:14:17.140]   But if everyone, or most people,
[00:14:17.140 --> 00:14:20.460]   became developers at some level,
[00:14:20.460 --> 00:14:22.940]   similar to how most people in developed economies
[00:14:22.940 --> 00:14:24.420]   are somewhat literate,
[00:14:24.420 --> 00:14:27.940]   I would love to see the owners of a mom and pop store
[00:14:27.940 --> 00:14:29.100]   be able to write a little bit of code
[00:14:29.100 --> 00:14:32.460]   to customize the TV display for their special this week.
[00:14:32.460 --> 00:14:36.340]   And I think it'll enhance human to computer communications,
[00:14:36.340 --> 00:14:38.780]   which is becoming more and more important in today's world.
[00:14:38.780 --> 00:14:41.740]   - So you think it's possible that machine learning
[00:14:41.740 --> 00:14:45.900]   becomes kind of similar to literacy where,
[00:14:45.900 --> 00:14:49.980]   yeah, like you said, the owners of a mom and pop shop,
[00:14:49.980 --> 00:14:52.180]   is basically everybody in all walks of life
[00:14:52.180 --> 00:14:55.620]   would have some degree of programming capability?
[00:14:55.620 --> 00:14:58.580]   - I could see society getting there.
[00:14:58.580 --> 00:15:00.700]   There's one other interesting thing, you know,
[00:15:00.700 --> 00:15:02.860]   if I go talk to the mom and pop store,
[00:15:02.860 --> 00:15:05.380]   if I talk to a lot of people in their daily professions,
[00:15:05.380 --> 00:15:07.300]   I previously didn't have a good story
[00:15:07.300 --> 00:15:09.340]   for why they should learn to code.
[00:15:09.340 --> 00:15:11.300]   You know, we could give them some reasons.
[00:15:11.300 --> 00:15:13.300]   But what I found with the rise of machine learning
[00:15:13.300 --> 00:15:15.980]   and data science is that I think the number of people
[00:15:15.980 --> 00:15:19.460]   with a concrete use for data science in their daily lives,
[00:15:19.460 --> 00:15:22.860]   in their jobs, may be even larger than the number of people
[00:15:22.860 --> 00:15:25.460]   with a concrete use for software engineering.
[00:15:25.460 --> 00:15:28.180]   For example, actually, if you run a small mom and pop store,
[00:15:28.180 --> 00:15:30.900]   I think if you can analyze the data about your sales,
[00:15:30.900 --> 00:15:34.220]   your customers, I think there's actually real value there,
[00:15:34.220 --> 00:15:37.260]   maybe even more than traditional software engineering.
[00:15:37.260 --> 00:15:39.380]   So I find that for a lot of my friends
[00:15:39.380 --> 00:15:42.940]   in various professions, be it recruiters or accountants
[00:15:42.940 --> 00:15:45.260]   or, you know, people that work in the factories,
[00:15:45.260 --> 00:15:47.420]   which I deal with more and more these days,
[00:15:47.420 --> 00:15:51.340]   I feel if they were data scientists at some level,
[00:15:51.340 --> 00:15:54.540]   they could immediately use that in their work.
[00:15:54.540 --> 00:15:56.900]   So I think that data science and machine learning
[00:15:56.900 --> 00:16:00.460]   may be an even easier entree into the developer world
[00:16:00.460 --> 00:16:03.420]   for a lot of people than the software engineering.
[00:16:03.420 --> 00:16:04.420]   - That's interesting.
[00:16:04.420 --> 00:16:06.940]   And I agree with that, but that's beautifully put.
[00:16:07.860 --> 00:16:11.260]   We live in a world where most courses and talks have slides,
[00:16:11.260 --> 00:16:14.620]   PowerPoint, keynote, and yet you famously
[00:16:14.620 --> 00:16:17.300]   often still use a marker and a whiteboard.
[00:16:17.300 --> 00:16:19.380]   The simplicity of that is compelling,
[00:16:19.380 --> 00:16:22.100]   and for me, at least, fun to watch.
[00:16:22.100 --> 00:16:25.860]   So let me ask, why do you like using a marker and whiteboard
[00:16:25.860 --> 00:16:27.660]   even on the biggest of stages?
[00:16:27.660 --> 00:16:32.380]   - I think it depends on the concepts you want to explain.
[00:16:32.380 --> 00:16:34.900]   For mathematical concepts, it's nice to build up
[00:16:34.900 --> 00:16:37.060]   the equation one piece at a time.
[00:16:37.060 --> 00:16:41.340]   And the whiteboard marker or the pen and stylus
[00:16:41.340 --> 00:16:43.980]   is a very easy way to build up the equation,
[00:16:43.980 --> 00:16:47.420]   build up a complex concept one piece at a time
[00:16:47.420 --> 00:16:48.580]   while you're talking about it.
[00:16:48.580 --> 00:16:51.660]   And sometimes that enhances understandability.
[00:16:51.660 --> 00:16:54.820]   The downside of writing is that it's slow.
[00:16:54.820 --> 00:16:56.380]   And so if you want a long sentence,
[00:16:56.380 --> 00:16:57.380]   it's very hard to write that.
[00:16:57.380 --> 00:16:58.420]   So I think there are pros and cons.
[00:16:58.420 --> 00:17:00.420]   And sometimes I use slides,
[00:17:00.420 --> 00:17:03.220]   and sometimes I use a whiteboard or a stylus.
[00:17:03.220 --> 00:17:06.340]   - The slowness of a whiteboard is also its upside,
[00:17:06.340 --> 00:17:11.340]   'cause it forces you to reduce everything to the basics.
[00:17:11.340 --> 00:17:14.900]   So some of your talks involve the whiteboard.
[00:17:14.900 --> 00:17:17.860]   I mean, there's really not, you go very slowly,
[00:17:17.860 --> 00:17:20.180]   and you really focus on the most simple principles.
[00:17:20.180 --> 00:17:21.620]   And that's a beautiful,
[00:17:21.620 --> 00:17:26.540]   that enforces a kind of a minimalism of ideas
[00:17:26.540 --> 00:17:29.420]   that I think is, surprisingly at least for me,
[00:17:29.420 --> 00:17:31.660]   is great for education.
[00:17:31.660 --> 00:17:34.260]   Like a great talk, I think,
[00:17:34.260 --> 00:17:37.100]   is not one that has a lot of content.
[00:17:37.100 --> 00:17:40.340]   A great talk is one that just clearly says
[00:17:40.340 --> 00:17:41.980]   a few simple ideas.
[00:17:41.980 --> 00:17:42.820]   And I think you,
[00:17:42.820 --> 00:17:46.380]   the whiteboard somehow enforces that.
[00:17:46.380 --> 00:17:49.500]   Peter Abbeel, who's now one of the top roboticists
[00:17:49.500 --> 00:17:51.500]   and reinforcement learning experts in the world,
[00:17:51.500 --> 00:17:53.140]   was your first PhD student.
[00:17:53.140 --> 00:17:56.940]   So I bring him up just because I kind of imagine
[00:17:56.940 --> 00:18:01.940]   this must have been an interesting time in your life.
[00:18:02.340 --> 00:18:04.980]   Do you have any favorite memories of working with Peter,
[00:18:04.980 --> 00:18:08.380]   since you're your first student in those uncertain times,
[00:18:08.380 --> 00:18:13.380]   especially before deep learning really sort of blew up?
[00:18:13.380 --> 00:18:17.820]   Any favorite memories from those times?
[00:18:17.820 --> 00:18:20.740]   - Yeah, I was really fortunate to have had Peter Abbeel
[00:18:20.740 --> 00:18:22.740]   as my first PhD student.
[00:18:22.740 --> 00:18:25.620]   And I think even my long-term professional success
[00:18:25.620 --> 00:18:27.740]   builds on early foundations or early work
[00:18:27.740 --> 00:18:29.980]   that Peter was so critical to.
[00:18:29.980 --> 00:18:33.260]   So I was really grateful to him for working with me.
[00:18:33.260 --> 00:18:36.700]   What not a lot of people know
[00:18:36.700 --> 00:18:41.100]   is just how hard research was and still is.
[00:18:41.100 --> 00:18:44.940]   Peter's PhD thesis was using reinforcement learning
[00:18:44.940 --> 00:18:47.180]   to fly helicopters.
[00:18:47.180 --> 00:18:49.380]   And so, actually even today,
[00:18:49.380 --> 00:18:51.700]   the website heli.stanford.edu,
[00:18:51.700 --> 00:18:53.460]   H-E-L-I.stanford.edu is still up.
[00:18:53.460 --> 00:18:56.340]   You can watch videos of us using reinforcement learning
[00:18:56.340 --> 00:18:58.060]   to make a helicopter fly upside down,
[00:18:58.060 --> 00:19:00.060]   fly loose, so it's cool.
[00:19:00.060 --> 00:19:02.460]   - It's one of the most incredible robotics videos ever.
[00:19:02.460 --> 00:19:03.700]   So people should watch it.
[00:19:03.700 --> 00:19:05.140]   - Oh yeah, thank you. - It's inspiring.
[00:19:05.140 --> 00:19:10.140]   That's from like 2008 or seven or six, like that range.
[00:19:10.140 --> 00:19:13.020]   - Something like that, yeah, so we're 10 years old.
[00:19:13.020 --> 00:19:15.420]   - That was really inspiring to a lot of people, yeah.
[00:19:15.420 --> 00:19:18.900]   - What not many people see is how hard it was.
[00:19:18.900 --> 00:19:22.780]   So Peter and Adam Coates and Morgan Quigley and I
[00:19:22.780 --> 00:19:25.500]   were working on various versions of the helicopter,
[00:19:25.500 --> 00:19:27.460]   and a lot of things did not work.
[00:19:27.460 --> 00:19:29.860]   For example, turns out one of the hardest problems we had
[00:19:29.860 --> 00:19:32.380]   was when the helicopter is flying around upside down,
[00:19:32.380 --> 00:19:34.900]   doing stunts, how do you figure out the position?
[00:19:34.900 --> 00:19:36.860]   How do you localize a helicopter?
[00:19:36.860 --> 00:19:38.900]   So we wanted to try all sorts of things.
[00:19:38.900 --> 00:19:41.220]   Having one GPS unit doesn't work
[00:19:41.220 --> 00:19:42.300]   'cause you're flying upside down,
[00:19:42.300 --> 00:19:44.860]   GPS unit is facing down, so you can't see the satellites.
[00:19:44.860 --> 00:19:48.620]   So we experimented trying to have two GPS units,
[00:19:48.620 --> 00:19:49.980]   one facing up, one facing down.
[00:19:49.980 --> 00:19:51.900]   So if you flip over, that didn't work
[00:19:51.900 --> 00:19:54.340]   'cause the downward facing one couldn't synchronize
[00:19:54.340 --> 00:19:56.740]   if you're flipping quickly.
[00:19:56.740 --> 00:19:59.500]   Morgan Quigley was exploring this crazy,
[00:19:59.500 --> 00:20:02.460]   complicated configuration of specialized hardware
[00:20:02.460 --> 00:20:04.580]   to interpret GPS signals.
[00:20:04.580 --> 00:20:06.820]   A look into FPG is completely insane.
[00:20:06.820 --> 00:20:10.300]   Spent about a year working on that, didn't work.
[00:20:10.300 --> 00:20:14.420]   So I remember Peter, great guy, him and me,
[00:20:14.420 --> 00:20:15.980]   sitting down in my office,
[00:20:15.980 --> 00:20:18.740]   looking at some of the latest things we had tried
[00:20:18.740 --> 00:20:23.260]   that didn't work and saying, "Done it, what now?"
[00:20:23.260 --> 00:20:26.940]   Because we tried so many things and it just didn't work.
[00:20:26.940 --> 00:20:32.260]   In the end, what we did, and Adam Coates was crucial to this,
[00:20:32.260 --> 00:20:34.260]   was put cameras on the ground
[00:20:34.260 --> 00:20:36.980]   and use cameras on the ground to localize the helicopter.
[00:20:36.980 --> 00:20:39.820]   And that solved the localization problem
[00:20:39.820 --> 00:20:42.380]   so that we could then focus on the reinforcement learning
[00:20:42.380 --> 00:20:44.420]   and inverse reinforcement learning techniques
[00:20:44.420 --> 00:20:46.700]   so it didn't actually make the helicopter fly.
[00:20:46.700 --> 00:20:51.780]   And I'm reminded, when I was doing this work at Stanford,
[00:20:51.780 --> 00:20:55.220]   around that time, there was a lot of reinforcement learning
[00:20:55.220 --> 00:20:59.540]   theoretical papers, but not a lot of practical applications.
[00:20:59.540 --> 00:21:03.380]   So the autonomous helicopter work for flying helicopters
[00:21:03.380 --> 00:21:06.620]   was one of the few practical applications
[00:21:06.620 --> 00:21:08.020]   of reinforcement learning at the time,
[00:21:08.020 --> 00:21:11.580]   which caused it to become pretty well-known.
[00:21:11.580 --> 00:21:14.740]   I feel like we might've almost come full circle with today.
[00:21:14.740 --> 00:21:17.580]   There's so much buzz, so much hype, so much excitement
[00:21:17.580 --> 00:21:19.020]   about reinforcement learning.
[00:21:19.020 --> 00:21:21.820]   But again, we're hunting for more applications
[00:21:21.820 --> 00:21:23.140]   of all of these great ideas
[00:21:23.140 --> 00:21:24.740]   that the communities come up with.
[00:21:24.740 --> 00:21:28.260]   - What was the drive, sort of in the face of the fact
[00:21:28.260 --> 00:21:30.140]   that most people are doing theoretical work,
[00:21:30.140 --> 00:21:33.020]   what motivate you in the uncertainty and the challenges
[00:21:33.020 --> 00:21:36.500]   to get the helicopter, sort of to do the applied work,
[00:21:36.500 --> 00:21:38.540]   to get the actual system to work?
[00:21:38.540 --> 00:21:41.860]   Yeah, in the face of fear, uncertainty,
[00:21:41.860 --> 00:21:46.020]   sort of the setbacks that you mentioned for localization.
[00:21:46.020 --> 00:21:47.460]   - I like stuff that works.
[00:21:48.100 --> 00:21:48.940]   - In the physical world.
[00:21:48.940 --> 00:21:51.260]   So like, it's back to the shredder.
[00:21:51.260 --> 00:21:55.460]   - You know, I like theory,
[00:21:55.460 --> 00:21:58.700]   but when I work on theory myself, and this is personal taste,
[00:21:58.700 --> 00:22:00.820]   I'm not saying anyone else should do what I do,
[00:22:00.820 --> 00:22:04.100]   but when I work on theory, I personally enjoy it more
[00:22:04.100 --> 00:22:08.740]   if I feel that the work I do will influence people,
[00:22:08.740 --> 00:22:10.820]   have positive impact, or help someone.
[00:22:10.820 --> 00:22:14.940]   I remember when, many years ago,
[00:22:14.940 --> 00:22:17.780]   I was speaking with a mathematics professor,
[00:22:17.780 --> 00:22:21.300]   and it kind of just said, "Hey, why do you do what you do?"
[00:22:21.300 --> 00:22:23.740]   And then he said, he actually,
[00:22:23.740 --> 00:22:25.780]   he had stars in his eyes when he answered,
[00:22:25.780 --> 00:22:27.980]   and this mathematician,
[00:22:27.980 --> 00:22:29.580]   not from Stanford, different university,
[00:22:29.580 --> 00:22:32.900]   he said, "I do what I do because it helps me
[00:22:32.900 --> 00:22:36.740]   to discover truth and beauty in the universe."
[00:22:36.740 --> 00:22:38.580]   He had stars in his eyes when he said that.
[00:22:38.580 --> 00:22:39.980]   And I thought, that's great.
[00:22:39.980 --> 00:22:42.660]   I don't want to do that.
[00:22:42.660 --> 00:22:44.180]   I think it's great that someone does that,
[00:22:44.180 --> 00:22:45.500]   fully support the people that do it,
[00:22:45.500 --> 00:22:47.060]   a lot of respect for people that do that,
[00:22:47.060 --> 00:22:50.780]   but I am more motivated when I can see a line
[00:22:50.780 --> 00:22:55.660]   to how the work that my teams and I are doing helps people.
[00:22:55.660 --> 00:22:58.580]   The world needs all sorts of people.
[00:22:58.580 --> 00:22:59.500]   I'm just one type.
[00:22:59.500 --> 00:23:01.380]   I don't think everyone should do things
[00:23:01.380 --> 00:23:02.540]   the same way as I do,
[00:23:02.540 --> 00:23:06.100]   but when I delve into either theory or practice,
[00:23:06.100 --> 00:23:08.580]   if I personally have conviction
[00:23:08.580 --> 00:23:10.460]   that here's a pathway to help people,
[00:23:10.460 --> 00:23:13.980]   I find that more satisfying.
[00:23:14.380 --> 00:23:15.860]   To have that conviction.
[00:23:15.860 --> 00:23:17.620]   - That's your path.
[00:23:17.620 --> 00:23:20.020]   You were a proponent of deep learning
[00:23:20.020 --> 00:23:23.260]   before it gained widespread acceptance.
[00:23:23.260 --> 00:23:25.940]   What did you see in this field that gave you confidence?
[00:23:25.940 --> 00:23:27.740]   What was your thinking process like
[00:23:27.740 --> 00:23:29.700]   in that first decade of the,
[00:23:29.700 --> 00:23:31.460]   I don't know what that's called,
[00:23:31.460 --> 00:23:33.820]   2000s, the aughts?
[00:23:33.820 --> 00:23:35.660]   - Yeah, I can tell you the thing we got wrong
[00:23:35.660 --> 00:23:36.980]   and the thing we got right.
[00:23:36.980 --> 00:23:39.740]   The thing we really got wrong was the importance of,
[00:23:39.740 --> 00:23:43.580]   the early importance of unsupervised learning.
[00:23:43.580 --> 00:23:46.740]   So early days of Google Brain,
[00:23:46.740 --> 00:23:48.740]   we put a lot of effort into unsupervised learning
[00:23:48.740 --> 00:23:50.300]   rather than supervised learning.
[00:23:50.300 --> 00:23:51.660]   And there was this argument,
[00:23:51.660 --> 00:23:56.140]   I think it was around 2005 after NeurIPS,
[00:23:56.140 --> 00:23:57.100]   at that time called NIPS,
[00:23:57.100 --> 00:23:58.980]   but now NeurIPS had ended.
[00:23:58.980 --> 00:24:01.980]   And Geoff Hinton and I were sitting in the cafeteria
[00:24:01.980 --> 00:24:03.380]   outside the conference,
[00:24:03.380 --> 00:24:04.900]   we had lunch, we were just chatting.
[00:24:04.900 --> 00:24:06.180]   And Geoff pulled out this napkin,
[00:24:06.180 --> 00:24:08.660]   he started sketching this argument on a napkin.
[00:24:08.660 --> 00:24:10.900]   It was very compelling, I'll repeat it.
[00:24:11.860 --> 00:24:14.180]   Human brain has about a hundred trillion,
[00:24:14.180 --> 00:24:16.940]   so there's 10 to the 14 synaptic connections.
[00:24:16.940 --> 00:24:21.220]   You will live for about 10 to the nine seconds.
[00:24:21.220 --> 00:24:22.100]   That's 30 years.
[00:24:22.100 --> 00:24:24.460]   You actually live for two by 10 to the nine,
[00:24:24.460 --> 00:24:25.860]   maybe three by 10 to the nine seconds.
[00:24:25.860 --> 00:24:27.980]   So just let's say 10 to the nine.
[00:24:27.980 --> 00:24:30.660]   So if each synaptic connection,
[00:24:30.660 --> 00:24:32.740]   each weight in your brain's neural network
[00:24:32.740 --> 00:24:35.020]   has just a one bit parameter,
[00:24:35.020 --> 00:24:37.980]   that's 10 to the 14 bits you need to learn
[00:24:37.980 --> 00:24:41.820]   in up to 10 to the nine seconds of your life.
[00:24:41.820 --> 00:24:43.780]   So via this simple argument,
[00:24:43.780 --> 00:24:46.100]   which is a lot of problems, it's very simplified,
[00:24:46.100 --> 00:24:47.620]   that's 10 to the five bits per second
[00:24:47.620 --> 00:24:49.780]   you need to learn in your life.
[00:24:49.780 --> 00:24:52.580]   And I have a one-year-old daughter,
[00:24:52.580 --> 00:24:56.580]   I am not pointing out 10 to five bits per second
[00:24:56.580 --> 00:24:58.900]   of labels to her.
[00:24:58.900 --> 00:25:02.100]   So, and I think I'm a very loving parent,
[00:25:02.100 --> 00:25:03.660]   but I'm just not gonna do that.
[00:25:03.660 --> 00:25:07.260]   So from this, you know, very crude,
[00:25:07.260 --> 00:25:08.900]   definitely problematic argument.
[00:25:08.900 --> 00:25:11.340]   There's just no way that most of what we know
[00:25:11.340 --> 00:25:13.460]   is through supervised learning.
[00:25:13.460 --> 00:25:15.340]   But where you get so many bits of information
[00:25:15.340 --> 00:25:16.980]   is from sucking in images, audio,
[00:25:16.980 --> 00:25:18.540]   just experiences in the world.
[00:25:18.540 --> 00:25:21.500]   And so that argument,
[00:25:21.500 --> 00:25:23.220]   and there are a lot of known forces argument,
[00:25:23.220 --> 00:25:24.780]   you know, we should go into,
[00:25:24.780 --> 00:25:26.940]   really convinced me that there's a lot of power
[00:25:26.940 --> 00:25:28.220]   to unsupervised learning.
[00:25:28.220 --> 00:25:32.540]   So that was the part that we actually maybe got wrong.
[00:25:32.540 --> 00:25:34.860]   I still think unsupervised learning is really important,
[00:25:34.860 --> 00:25:38.900]   but in the early days, you know, 10, 15 years ago,
[00:25:38.900 --> 00:25:41.220]   a lot of us thought that was the path forward.
[00:25:41.220 --> 00:25:43.460]   - Oh, so you're saying that that perhaps
[00:25:43.460 --> 00:25:45.660]   was the wrong intuition for the time.
[00:25:45.660 --> 00:25:48.540]   - For the time, that was the part we got wrong.
[00:25:48.540 --> 00:25:51.580]   The part we got right was the importance of scale.
[00:25:51.580 --> 00:25:55.900]   So Adam Coates, another wonderful person,
[00:25:55.900 --> 00:25:58.140]   fortunate to have worked with him,
[00:25:58.140 --> 00:26:00.020]   he was in my group at Stanford at the time,
[00:26:00.020 --> 00:26:02.380]   and Adam had run these experiments at Stanford
[00:26:02.380 --> 00:26:06.060]   showing that the bigger we train a learning algorithm,
[00:26:06.060 --> 00:26:07.900]   the better its performance.
[00:26:07.900 --> 00:26:10.140]   And it was based on that,
[00:26:10.140 --> 00:26:12.900]   there was a graph that Adam generated, you know,
[00:26:12.900 --> 00:26:15.780]   where the X-axis, Y-axis lines going up into the right.
[00:26:15.780 --> 00:26:17.540]   So the bigger you make this thing,
[00:26:17.540 --> 00:26:20.340]   the better its performance, accuracy is the vertical axis.
[00:26:20.340 --> 00:26:22.780]   So it was really based on that chart that Adam generated
[00:26:22.780 --> 00:26:23.980]   that he gave me the conviction
[00:26:23.980 --> 00:26:26.260]   that we could scale these models way bigger
[00:26:26.260 --> 00:26:27.900]   than what we could on a few CPUs,
[00:26:27.900 --> 00:26:29.420]   which is what we had at Stanford,
[00:26:29.420 --> 00:26:31.580]   that we could get even better results.
[00:26:31.580 --> 00:26:33.420]   And it was really based on that one figure
[00:26:33.420 --> 00:26:37.100]   that Adam generated that gave me the conviction
[00:26:37.100 --> 00:26:40.180]   to go with Sebastian Thrun to pitch, you know,
[00:26:40.180 --> 00:26:42.820]   starting a project at Google,
[00:26:42.820 --> 00:26:44.140]   which became the Google Brain Project.
[00:26:44.140 --> 00:26:45.740]   - Google Brain, you go find Google Brain.
[00:26:45.740 --> 00:26:49.100]   And there the intuition was scale
[00:26:49.100 --> 00:26:52.300]   will bring performance for the system,
[00:26:52.300 --> 00:26:55.460]   so we should chase a larger and larger scale.
[00:26:55.460 --> 00:27:00.140]   And I think people don't realize how groundbreaking of it,
[00:27:00.140 --> 00:27:02.340]   it's simple, but it's a groundbreaking idea
[00:27:02.340 --> 00:27:06.180]   that bigger datasets will result in better performance.
[00:27:06.180 --> 00:27:08.740]   - It was controversial at the time.
[00:27:08.740 --> 00:27:10.060]   Some of my well-meaning friends,
[00:27:10.060 --> 00:27:11.580]   you know, senior people in the machine learning community,
[00:27:11.580 --> 00:27:13.780]   I won't name, but who's people,
[00:27:13.780 --> 00:27:16.100]   some of whom we know,
[00:27:16.100 --> 00:27:17.900]   my well-meaning friends came
[00:27:17.900 --> 00:27:19.460]   and were trying to give me friendly advice,
[00:27:19.460 --> 00:27:20.980]   like, "Hey, Andrew, why are you doing this?
[00:27:20.980 --> 00:27:21.900]   This is crazy.
[00:27:21.900 --> 00:27:23.300]   It's in the near natural architecture.
[00:27:23.300 --> 00:27:24.900]   Look at these architectures of building.
[00:27:24.900 --> 00:27:26.060]   You just want to go for scale?
[00:27:26.060 --> 00:27:27.420]   Like, this is a bad career move."
[00:27:27.420 --> 00:27:29.900]   So my well-meaning friends, you know,
[00:27:29.900 --> 00:27:32.500]   some of them were trying to talk me out of it.
[00:27:32.500 --> 00:27:36.660]   But I find that if you want to make a breakthrough,
[00:27:36.660 --> 00:27:38.980]   you sometimes have to have conviction
[00:27:38.980 --> 00:27:41.020]   and do something before it's popular,
[00:27:41.020 --> 00:27:43.020]   since that lets you have a bigger impact.
[00:27:43.020 --> 00:27:46.100]   - Let me ask you just in a small tangent on that topic.
[00:27:46.100 --> 00:27:51.100]   I find myself arguing with people saying that greater scale,
[00:27:51.100 --> 00:27:53.500]   especially in the context of active learning,
[00:27:53.500 --> 00:27:56.940]   so very carefully selecting the dataset,
[00:27:56.940 --> 00:27:59.220]   but growing the scale of the dataset,
[00:27:59.220 --> 00:28:01.620]   is going to lead to even further breakthroughs
[00:28:01.620 --> 00:28:02.780]   in deep learning.
[00:28:02.780 --> 00:28:05.900]   And there's currently pushback at that idea,
[00:28:05.900 --> 00:28:09.140]   that larger datasets are no longer...
[00:28:09.140 --> 00:28:11.860]   So you want to increase the efficiency of learning.
[00:28:11.860 --> 00:28:14.020]   You want to make better learning mechanisms.
[00:28:14.020 --> 00:28:16.820]   And I personally believe that just bigger datasets
[00:28:16.820 --> 00:28:20.020]   will still, with the same learning methods we have now,
[00:28:20.020 --> 00:28:21.820]   will result in better performance.
[00:28:21.820 --> 00:28:26.820]   What's your intuition at this time on this dual side?
[00:28:27.980 --> 00:28:30.740]   Is do we need to come up with better architectures
[00:28:30.740 --> 00:28:35.700]   for learning, or can we just get bigger, better datasets
[00:28:35.700 --> 00:28:37.740]   that will improve performance?
[00:28:37.740 --> 00:28:39.260]   - I think both are important.
[00:28:39.260 --> 00:28:40.940]   And it's also problem dependent.
[00:28:40.940 --> 00:28:44.740]   So for a few datasets, we may be approaching
[00:28:44.740 --> 00:28:47.940]   your Bayes error rate, or approaching or surpassing
[00:28:47.940 --> 00:28:49.260]   human level performance.
[00:28:49.260 --> 00:28:51.300]   And then there's that theoretical ceiling
[00:28:51.300 --> 00:28:53.660]   that we will never surpass a Bayes error rate.
[00:28:53.660 --> 00:28:56.260]   But then I think there are plenty of problems
[00:28:56.260 --> 00:28:58.740]   where we're still quite far from either
[00:28:58.740 --> 00:29:00.980]   human level performance or from Bayes error rate.
[00:29:00.980 --> 00:29:05.420]   And bigger datasets with new networks,
[00:29:05.420 --> 00:29:07.140]   without further algorithmic innovation,
[00:29:07.140 --> 00:29:09.460]   will be sufficient to take us further.
[00:29:09.460 --> 00:29:12.900]   But on the flip side, if we look at the recent breakthroughs
[00:29:12.900 --> 00:29:15.540]   using transforming networks or language models,
[00:29:15.540 --> 00:29:18.380]   it was a combination of novel architecture,
[00:29:18.380 --> 00:29:20.660]   but also scale had a lot to do with it.
[00:29:20.660 --> 00:29:23.060]   If we look at what happened with GP2 and BERT,
[00:29:23.060 --> 00:29:26.380]   I think scale was a large part of the story.
[00:29:26.380 --> 00:29:28.300]   - Yeah, that's not often talked about,
[00:29:28.300 --> 00:29:31.020]   is the scale of the dataset it was trained on
[00:29:31.020 --> 00:29:32.500]   and the quality of the dataset,
[00:29:32.500 --> 00:29:37.180]   because there's some, so it was like redded threads
[00:29:37.180 --> 00:29:39.980]   that had, they were uprated highly.
[00:29:39.980 --> 00:29:42.980]   So there's already some weak supervision
[00:29:42.980 --> 00:29:44.860]   on a very large dataset
[00:29:44.860 --> 00:29:47.340]   that people don't often talk about, right?
[00:29:47.340 --> 00:29:50.580]   - I find that today we have maturing processes
[00:29:50.580 --> 00:29:53.540]   and managing code, things like Git, right?
[00:29:53.540 --> 00:29:54.820]   Version control.
[00:29:54.820 --> 00:29:57.500]   It took a long time to evolve the good processes.
[00:29:57.500 --> 00:30:00.380]   I remember when my friends and I were emailing
[00:30:00.380 --> 00:30:02.380]   each other C++ files in email,
[00:30:02.380 --> 00:30:05.260]   but then we had, was it CVS or version Git?
[00:30:05.260 --> 00:30:07.580]   Maybe something else in the future.
[00:30:07.580 --> 00:30:10.780]   We're very immature in terms of tools for managing data
[00:30:10.780 --> 00:30:12.100]   and think about the clean data
[00:30:12.100 --> 00:30:15.380]   and how to solve the very hot, messy data problems.
[00:30:15.380 --> 00:30:18.980]   I think there's a lot of innovation there to be had still.
[00:30:18.980 --> 00:30:22.220]   I love the idea that you were versioning through email.
[00:30:22.220 --> 00:30:23.940]   - I'll give you one example.
[00:30:23.940 --> 00:30:28.940]   When we work with manufacturing companies,
[00:30:28.940 --> 00:30:34.220]   it's not at all uncommon for there to be multiple labelers
[00:30:34.220 --> 00:30:36.380]   that disagree with each other, right?
[00:30:36.380 --> 00:30:40.580]   And so we would, doing the work in visual inspection,
[00:30:40.580 --> 00:30:44.820]   we will take, say a plastic pot and show it to one inspector
[00:30:44.820 --> 00:30:47.300]   and the inspector, sometimes very opinionated,
[00:30:47.300 --> 00:30:48.660]   they'll go, "Clearly that's a defect.
[00:30:48.660 --> 00:30:49.780]   This scratch, unacceptable.
[00:30:49.780 --> 00:30:51.340]   Gotta reject this pot."
[00:30:51.340 --> 00:30:53.460]   Take the same pot to different inspector,
[00:30:53.460 --> 00:30:54.940]   different, very opinionated,
[00:30:54.940 --> 00:30:56.260]   "Clearly the scratch is small.
[00:30:56.260 --> 00:30:57.620]   It's fine. Don't throw it away.
[00:30:57.620 --> 00:30:59.380]   You're going to make us use."
[00:30:59.380 --> 00:31:01.820]   And then sometimes you take the same plastic pot,
[00:31:01.820 --> 00:31:04.300]   show it to the same inspector in the afternoon,
[00:31:04.300 --> 00:31:05.500]   as opposed to in the morning,
[00:31:05.500 --> 00:31:07.540]   and very opinionated go in the morning,
[00:31:07.540 --> 00:31:08.820]   they say, "Clearly this is okay."
[00:31:08.820 --> 00:31:10.540]   In the afternoon, equally confident,
[00:31:10.540 --> 00:31:12.380]   "Clearly this is a defect."
[00:31:12.380 --> 00:31:14.820]   And so what is an AI team supposed to do
[00:31:14.820 --> 00:31:16.900]   if sometimes even one person
[00:31:16.900 --> 00:31:20.420]   doesn't agree with himself or herself in the span of a day?
[00:31:20.420 --> 00:31:23.740]   So I think these are the types of very practical,
[00:31:23.740 --> 00:31:28.740]   very messy data problems that my teams wrestle with.
[00:31:28.740 --> 00:31:32.980]   In the case of large consumer internet companies,
[00:31:32.980 --> 00:31:35.660]   where you have a billion users, you have a lot of data,
[00:31:35.660 --> 00:31:36.500]   you don't worry about it.
[00:31:36.500 --> 00:31:38.420]   Just take the average, it kind of works.
[00:31:38.420 --> 00:31:40.820]   But in a case of other industry settings,
[00:31:40.820 --> 00:31:42.500]   we don't have big data.
[00:31:42.500 --> 00:31:44.580]   If you have just a small data, very small data sets,
[00:31:44.580 --> 00:31:47.620]   maybe you have a hundred defective parts,
[00:31:47.620 --> 00:31:49.860]   or a hundred examples of a defect.
[00:31:49.860 --> 00:31:51.420]   If you have only a hundred examples,
[00:31:51.420 --> 00:31:53.340]   these little labeling errors,
[00:31:53.340 --> 00:31:55.860]   if 10 of your hundred labels are wrong,
[00:31:55.860 --> 00:31:58.660]   that actually is 10% of your data set has a big impact.
[00:31:58.660 --> 00:31:59.740]   So how do you clean this up?
[00:31:59.740 --> 00:32:01.020]   What are you supposed to do?
[00:32:01.020 --> 00:32:05.060]   This is an example of the types of things that my teams,
[00:32:05.060 --> 00:32:06.740]   this is a landing AI example,
[00:32:06.740 --> 00:32:09.180]   are wrestling with to deal with small data,
[00:32:09.180 --> 00:32:10.220]   which comes up all the time
[00:32:10.220 --> 00:32:12.220]   once you're outside consumer internet.
[00:32:12.220 --> 00:32:13.060]   - Yeah, that's fascinating.
[00:32:13.060 --> 00:32:15.340]   So then you invest more effort and time
[00:32:15.340 --> 00:32:18.140]   in thinking about the actual labeling process.
[00:32:18.140 --> 00:32:19.660]   What are the labels?
[00:32:19.660 --> 00:32:22.580]   What are the, how are disagreements resolved
[00:32:22.580 --> 00:32:25.860]   and all those kinds of like pragmatic real world problems.
[00:32:25.860 --> 00:32:27.340]   That's a fascinating space.
[00:32:27.340 --> 00:32:29.700]   - Yeah, I find that actually when I'm teaching at Stanford,
[00:32:29.700 --> 00:32:32.740]   I increasingly encourage students at Stanford
[00:32:32.740 --> 00:32:37.740]   to try to find their own project for the end of term project
[00:32:37.740 --> 00:32:40.460]   rather than just downloading someone else's
[00:32:40.460 --> 00:32:42.060]   nicely clean data set.
[00:32:42.060 --> 00:32:43.460]   It's actually much harder if you need to go
[00:32:43.460 --> 00:32:45.620]   and define your own problem and find your own data set
[00:32:45.620 --> 00:32:48.740]   rather than go to one of the several good websites,
[00:32:48.740 --> 00:32:52.900]   very good websites with clean scoped data sets
[00:32:52.900 --> 00:32:54.300]   that you could just work on.
[00:32:54.300 --> 00:32:57.020]   - You're now running three efforts,
[00:32:57.020 --> 00:33:02.020]   the AI Fund, Landing AI, and DeepLearning.ai.
[00:33:02.020 --> 00:33:04.660]   As you've said, the AI Fund is involved
[00:33:04.660 --> 00:33:06.700]   in creating new companies from scratch.
[00:33:06.700 --> 00:33:08.660]   Landing AI is involved in helping
[00:33:08.660 --> 00:33:10.540]   already established companies do AI
[00:33:10.540 --> 00:33:14.700]   and DeepLearning.ai is for education of everyone else
[00:33:14.700 --> 00:33:18.180]   or of individuals interested of getting into the field
[00:33:18.180 --> 00:33:19.500]   and excelling in it.
[00:33:19.500 --> 00:33:22.340]   So let's perhaps talk about each of these areas.
[00:33:22.340 --> 00:33:27.340]   First, DeepLearning.ai, how, the basic question,
[00:33:27.340 --> 00:33:30.140]   how does a person interested in deep learning
[00:33:30.140 --> 00:33:31.580]   get started in the field?
[00:33:31.580 --> 00:33:35.700]   - DeepLearning.ai is working to create causes
[00:33:35.700 --> 00:33:37.540]   to help people break into AI.
[00:33:37.540 --> 00:33:41.340]   So my machine learning course that I taught
[00:33:41.340 --> 00:33:44.300]   through Stanford remains one of the most popular causes
[00:33:44.300 --> 00:33:45.500]   on Coursera.
[00:33:45.500 --> 00:33:48.540]   - To this day, it's probably one of the courses,
[00:33:48.540 --> 00:33:49.820]   sort of, if I ask somebody,
[00:33:49.820 --> 00:33:52.340]   how did you get into machine learning
[00:33:52.340 --> 00:33:54.180]   or how did you fall in love with machine learning
[00:33:54.180 --> 00:33:55.660]   or what gets you interested,
[00:33:55.660 --> 00:33:59.180]   it always goes back to Andrew Yang at some point.
[00:33:59.180 --> 00:34:03.260]   The amount of people you've influenced is ridiculous.
[00:34:03.260 --> 00:34:05.820]   So for that, I'm sure I speak for a lot of people
[00:34:05.820 --> 00:34:07.140]   say big thank you.
[00:34:07.140 --> 00:34:08.140]   - No, yeah, thank you.
[00:34:08.140 --> 00:34:12.100]   You know, I was once reading a news article,
[00:34:12.100 --> 00:34:15.180]   I think it was tech review
[00:34:15.180 --> 00:34:17.700]   and I'm gonna mess up the statistic,
[00:34:17.700 --> 00:34:20.180]   but I remember reading an article that said
[00:34:20.180 --> 00:34:23.780]   something like one third of our programmers are self-taught.
[00:34:23.780 --> 00:34:25.140]   I may have the number one third wrong,
[00:34:25.140 --> 00:34:25.980]   it was two thirds.
[00:34:25.980 --> 00:34:27.300]   But when I read that article, I thought,
[00:34:27.300 --> 00:34:28.180]   this doesn't make sense.
[00:34:28.180 --> 00:34:29.420]   Everyone is self-taught.
[00:34:29.420 --> 00:34:32.460]   So, 'cause you teach yourself, I don't teach people.
[00:34:32.460 --> 00:34:34.540]   I just- - That's well put.
[00:34:34.540 --> 00:34:38.100]   So yeah, so how does one get started in deep learning
[00:34:38.100 --> 00:34:40.660]   and where does deeplearning.ai fit into that?
[00:34:40.660 --> 00:34:42.420]   - So the deep learning specialization
[00:34:42.420 --> 00:34:44.180]   offered by deep learning.ai is,
[00:34:44.180 --> 00:34:50.060]   I think it was Coursera's top specialization,
[00:34:50.060 --> 00:34:50.900]   it might still be.
[00:34:50.900 --> 00:34:52.980]   So it's a very popular way for people
[00:34:52.980 --> 00:34:54.540]   to take that specialization,
[00:34:54.540 --> 00:34:57.860]   to learn about everything from neural networks
[00:34:57.860 --> 00:35:00.140]   to how to tune in your network.
[00:35:00.140 --> 00:35:01.660]   So what does a conf net do?
[00:35:01.660 --> 00:35:04.180]   What is a RNN or a sequence model
[00:35:04.180 --> 00:35:05.860]   or what is an attention model?
[00:35:05.860 --> 00:35:08.140]   And so the deep learning specialization
[00:35:08.140 --> 00:35:11.020]   steps everyone through those algorithms.
[00:35:11.020 --> 00:35:13.220]   So you deeply understand it and can implement it
[00:35:13.220 --> 00:35:15.300]   and use it for whatever application.
[00:35:15.300 --> 00:35:16.540]   - From the very beginning?
[00:35:16.540 --> 00:35:19.580]   So what would you say are the prerequisites
[00:35:19.580 --> 00:35:22.180]   for somebody to take the deep learning specialization
[00:35:22.180 --> 00:35:25.660]   in terms of maybe math or programming background?
[00:35:25.660 --> 00:35:28.100]   - Yeah, need to understand basic programming
[00:35:28.100 --> 00:35:30.480]   since there are pro exercises in Python.
[00:35:31.340 --> 00:35:34.420]   And the math prereq is quite basic.
[00:35:34.420 --> 00:35:35.980]   So no calculus is needed.
[00:35:35.980 --> 00:35:38.740]   If you know calculus is great, you get better intuitions,
[00:35:38.740 --> 00:35:41.340]   but deliberately try to teach that specialization
[00:35:41.340 --> 00:35:42.740]   without requiring calculus.
[00:35:42.740 --> 00:35:47.340]   So I think high school math would be sufficient.
[00:35:47.340 --> 00:35:49.100]   If you know how to multiply two matrices,
[00:35:49.100 --> 00:35:52.300]   I think that's great.
[00:35:52.300 --> 00:35:54.860]   - So a little basic linear algebra is great.
[00:35:54.860 --> 00:35:56.020]   - Basic linear algebra,
[00:35:56.020 --> 00:36:00.180]   even very, very basic linear algebra in some programming.
[00:36:00.180 --> 00:36:01.300]   I think that people that have done
[00:36:01.300 --> 00:36:02.260]   the machine learning course
[00:36:02.260 --> 00:36:05.180]   will find the deep learning specialization a bit easier,
[00:36:05.180 --> 00:36:06.500]   but it's also possible to jump
[00:36:06.500 --> 00:36:08.420]   into the deep learning specialization directly,
[00:36:08.420 --> 00:36:10.020]   but it'll be a little bit harder
[00:36:10.020 --> 00:36:14.580]   since we tend to go over faster concepts
[00:36:14.580 --> 00:36:16.300]   like how does gradient descent work
[00:36:16.300 --> 00:36:17.540]   and what is the objective function,
[00:36:17.540 --> 00:36:20.300]   which is covered more slowly in the machine learning course.
[00:36:20.300 --> 00:36:22.980]   - Could you briefly mention some of the key concepts
[00:36:22.980 --> 00:36:25.140]   in deep learning that students should learn
[00:36:25.140 --> 00:36:27.820]   that you envision them learning in the first few months,
[00:36:27.820 --> 00:36:29.380]   in the first year or so?
[00:36:29.380 --> 00:36:31.940]   - So if you take the deep learning specialization,
[00:36:31.940 --> 00:36:34.940]   you learn the foundations of what is a neural network,
[00:36:34.940 --> 00:36:36.940]   how do you build up a neural network
[00:36:36.940 --> 00:36:38.860]   from a single logistic unit,
[00:36:38.860 --> 00:36:40.700]   to a stack of layers,
[00:36:40.700 --> 00:36:43.220]   to different activation functions.
[00:36:43.220 --> 00:36:44.940]   You learn how to train the neural networks.
[00:36:44.940 --> 00:36:47.860]   One thing I'm very proud of in that specialization
[00:36:47.860 --> 00:36:50.380]   is we go through a lot of practical know-how
[00:36:50.380 --> 00:36:52.340]   of how to actually make these things work.
[00:36:52.340 --> 00:36:53.420]   So what are the differences
[00:36:53.420 --> 00:36:55.860]   between different optimization algorithms?
[00:36:55.860 --> 00:36:57.380]   What do you do if the algorithm overfits?
[00:36:57.380 --> 00:36:59.140]   So how do you tell if the algorithm is overfitting?
[00:36:59.140 --> 00:37:00.300]   When do you collect more data?
[00:37:00.300 --> 00:37:03.300]   When should you not bother to collect more data?
[00:37:03.300 --> 00:37:06.260]   I find that even today, unfortunately,
[00:37:06.260 --> 00:37:10.060]   there are engineers that will spend six months
[00:37:10.060 --> 00:37:12.660]   trying to pursue a particular direction,
[00:37:12.660 --> 00:37:13.980]   such as collect more data,
[00:37:13.980 --> 00:37:15.940]   because we heard more data is valuable.
[00:37:15.940 --> 00:37:18.380]   But sometimes you could run some tests
[00:37:18.380 --> 00:37:20.500]   and could have figured out six months earlier
[00:37:20.500 --> 00:37:22.060]   that for this particular problem,
[00:37:22.060 --> 00:37:23.980]   collecting more data isn't going to cut it.
[00:37:23.980 --> 00:37:26.300]   So just don't spend six months collecting more data,
[00:37:26.300 --> 00:37:29.380]   spend your time modifying the architecture
[00:37:29.380 --> 00:37:30.300]   or trying something else.
[00:37:30.300 --> 00:37:32.660]   So go through a lot of the practical know-how
[00:37:32.660 --> 00:37:35.460]   so that when someone,
[00:37:35.460 --> 00:37:37.300]   when you take the deep learning specialization,
[00:37:37.300 --> 00:37:39.820]   you have those skills to be very efficient
[00:37:39.820 --> 00:37:42.020]   in how you build these networks.
[00:37:42.020 --> 00:37:44.340]   - So dive right in to play with the network,
[00:37:44.340 --> 00:37:47.300]   to train it, to do the inference on a particular dataset,
[00:37:47.300 --> 00:37:48.580]   to build intuition about it
[00:37:48.580 --> 00:37:52.220]   without building it up too big
[00:37:52.220 --> 00:37:55.740]   to where you spend, like you said, six months learning,
[00:37:55.740 --> 00:37:57.540]   building up your big project
[00:37:57.540 --> 00:38:00.220]   without building any intuition of a small,
[00:38:00.220 --> 00:38:03.540]   a small aspect of the data that could already tell you
[00:38:03.540 --> 00:38:05.700]   everything you need to know about that data.
[00:38:05.700 --> 00:38:09.380]   - Yes, and also the systematic frameworks of thinking
[00:38:09.380 --> 00:38:12.460]   for how to go about building practical machine learning.
[00:38:12.460 --> 00:38:15.460]   Maybe to make an analogy, when we learn to code,
[00:38:15.460 --> 00:38:17.900]   we have to learn the syntax of some programming language,
[00:38:17.900 --> 00:38:21.580]   right, be it Python or C++ or Octave or whatever.
[00:38:21.580 --> 00:38:23.020]   But the equally important
[00:38:23.020 --> 00:38:25.020]   or maybe even more important part of coding
[00:38:25.020 --> 00:38:26.940]   is to understand how to string together
[00:38:26.940 --> 00:38:28.900]   these lines of code into coherent things.
[00:38:28.900 --> 00:38:31.180]   So, you know, when should you put something
[00:38:31.180 --> 00:38:32.940]   in a function call and when should you not?
[00:38:32.940 --> 00:38:34.700]   How do you think about abstraction?
[00:38:34.700 --> 00:38:39.140]   So those frameworks are what makes a programmer efficient,
[00:38:39.140 --> 00:38:41.740]   even more than understanding the syntax.
[00:38:41.740 --> 00:38:44.940]   I remember when I was an undergrad at Carnegie Mellon,
[00:38:44.940 --> 00:38:47.620]   one of my friends would debug their code
[00:38:47.620 --> 00:38:49.340]   by first trying to compile it,
[00:38:49.340 --> 00:38:50.980]   and then it was C++ code.
[00:38:50.980 --> 00:38:53.420]   And then every line that the syntax error,
[00:38:53.420 --> 00:38:54.740]   they want to get rid of the syntax errors
[00:38:54.740 --> 00:38:55.740]   as quickly as possible.
[00:38:55.740 --> 00:38:56.620]   So how do you do that?
[00:38:56.620 --> 00:38:58.420]   Well, they would delete every single line of code
[00:38:58.420 --> 00:38:59.700]   with a syntax error.
[00:38:59.700 --> 00:39:01.740]   So really efficient for getting rid of syntax errors,
[00:39:01.740 --> 00:39:02.980]   but horrible debugging errors.
[00:39:02.980 --> 00:39:05.540]   So I think, so we learn how to debug.
[00:39:05.540 --> 00:39:07.020]   And I think in machine learning,
[00:39:07.020 --> 00:39:09.420]   the way you debug a machine learning program
[00:39:09.420 --> 00:39:12.540]   is very different than the way you do binary search
[00:39:12.540 --> 00:39:14.140]   or whatever, or use a debugger,
[00:39:14.140 --> 00:39:15.140]   like trace through the code
[00:39:15.140 --> 00:39:17.020]   in the traditional software engineering.
[00:39:17.020 --> 00:39:19.020]   So it's an evolving discipline,
[00:39:19.020 --> 00:39:20.820]   but I find that the people that are really good
[00:39:20.820 --> 00:39:22.900]   at debugging machine learning algorithms
[00:39:22.900 --> 00:39:26.140]   are easily 10X, maybe 100X faster
[00:39:26.140 --> 00:39:27.660]   at getting something to work.
[00:39:27.660 --> 00:39:28.500]   So-
[00:39:28.500 --> 00:39:30.460]   - And the basic process of debugging is,
[00:39:30.460 --> 00:39:32.620]   so the bug in this case,
[00:39:32.620 --> 00:39:36.420]   why isn't this thing learning, improving,
[00:39:36.420 --> 00:39:39.340]   sort of going into the questions of overfitting
[00:39:39.340 --> 00:39:40.740]   and all those kinds of things.
[00:39:40.740 --> 00:39:45.340]   That's the logical space that the debugging is happening in
[00:39:45.340 --> 00:39:46.540]   with neural networks.
[00:39:46.540 --> 00:39:50.420]   - Yeah, often the question is, why doesn't it work yet?
[00:39:50.420 --> 00:39:53.060]   Or can I expect this to eventually work?
[00:39:53.060 --> 00:39:54.900]   And what are the things I could try?
[00:39:54.900 --> 00:39:57.580]   Change the architecture, more data, more regularization,
[00:39:57.580 --> 00:39:59.180]   different optimization algorithm,
[00:39:59.180 --> 00:40:02.020]   different types of data.
[00:40:02.020 --> 00:40:04.260]   So to answer those questions systematically,
[00:40:04.260 --> 00:40:05.860]   so that you don't heading down the,
[00:40:05.860 --> 00:40:08.100]   so you don't spend six months heading down the blind alley
[00:40:08.100 --> 00:40:09.820]   before someone comes and says,
[00:40:09.820 --> 00:40:12.180]   why did you spend six months doing this?
[00:40:12.180 --> 00:40:14.060]   - What concepts in deep learning
[00:40:14.060 --> 00:40:16.540]   do you think students struggle the most with?
[00:40:16.540 --> 00:40:19.100]   Or sort of is the biggest challenge for them
[00:40:19.100 --> 00:40:21.740]   once they get over that hill?
[00:40:21.740 --> 00:40:26.660]   It hooks them and it inspires them and they really get it.
[00:40:26.660 --> 00:40:30.300]   - Similar to learning mathematics,
[00:40:30.300 --> 00:40:32.500]   I think one of the challenges of deep learning
[00:40:32.500 --> 00:40:34.140]   is that there are a lot of concepts
[00:40:34.140 --> 00:40:35.740]   that build on top of each other.
[00:40:35.740 --> 00:40:38.900]   If you ask me what's hard about mathematics,
[00:40:38.900 --> 00:40:41.060]   I have a hard time pinpointing one thing.
[00:40:41.060 --> 00:40:42.420]   Is it addition, subtraction?
[00:40:42.420 --> 00:40:43.260]   Is it a carry?
[00:40:43.260 --> 00:40:44.540]   Is it multiplication?
[00:40:44.540 --> 00:40:45.940]   There's just a lot of stuff.
[00:40:45.940 --> 00:40:47.940]   I think one of the challenges of learning math
[00:40:47.940 --> 00:40:49.900]   and of learning certain technical fields
[00:40:49.900 --> 00:40:51.660]   is that there are a lot of concepts.
[00:40:51.660 --> 00:40:53.140]   And if you miss a concept,
[00:40:53.140 --> 00:40:55.540]   then you're kind of missing the prerequisite
[00:40:55.540 --> 00:40:57.180]   for something that comes later.
[00:40:57.180 --> 00:41:01.900]   So in the deep learning specialization,
[00:41:01.900 --> 00:41:04.780]   try to break down the concepts to maximize the odds
[00:41:04.780 --> 00:41:07.020]   of each component being understandable.
[00:41:07.020 --> 00:41:09.340]   So when you move on to the more advanced thing,
[00:41:09.340 --> 00:41:10.860]   we learn conf nets.
[00:41:10.860 --> 00:41:12.380]   Hopefully you have enough intuitions
[00:41:12.380 --> 00:41:14.980]   from the earlier sections to then understand
[00:41:14.980 --> 00:41:18.700]   why we structure conf nets in a certain way.
[00:41:18.700 --> 00:41:23.140]   And then eventually why we build RNNs on LSTMs
[00:41:23.140 --> 00:41:24.860]   or attention model in a certain way,
[00:41:24.860 --> 00:41:27.700]   building on top of the earlier concepts.
[00:41:27.700 --> 00:41:30.980]   Actually, I'm curious, you do a lot of teaching as well.
[00:41:30.980 --> 00:41:33.180]   Do you have a favorite,
[00:41:33.180 --> 00:41:36.380]   this is the hard concept moment in your teaching?
[00:41:36.380 --> 00:41:41.220]   - Well, I don't think anyone's ever turned
[00:41:41.220 --> 00:41:42.380]   the interview on me.
[00:41:43.660 --> 00:41:44.820]   Matthew, you're first.
[00:41:44.820 --> 00:41:46.580]   (laughing)
[00:41:46.580 --> 00:41:49.020]   - I think that's a really good question.
[00:41:49.020 --> 00:41:51.340]   Yeah, it's really hard to capture the moment
[00:41:51.340 --> 00:41:52.180]   when they struggle.
[00:41:52.180 --> 00:41:53.420]   I think you put it really eloquently.
[00:41:53.420 --> 00:41:57.380]   I do think there's moments that are like aha moments
[00:41:57.380 --> 00:41:59.500]   that really inspire people.
[00:41:59.500 --> 00:42:03.420]   I think for some reason, reinforcement learning,
[00:42:03.420 --> 00:42:05.660]   especially deep reinforcement learning
[00:42:05.660 --> 00:42:09.680]   is a really great way to really inspire people
[00:42:09.680 --> 00:42:13.620]   and get what the use of neural networks can do.
[00:42:13.620 --> 00:42:16.700]   Even though neural networks really are just a part
[00:42:16.700 --> 00:42:18.640]   of the deep RL framework,
[00:42:18.640 --> 00:42:21.420]   but it's a really nice way to paint the entirety
[00:42:21.420 --> 00:42:24.920]   of the picture of a neural network being able to learn
[00:42:24.920 --> 00:42:27.900]   from scratch, knowing nothing, and explore the world
[00:42:27.900 --> 00:42:29.200]   and pick up lessons.
[00:42:29.200 --> 00:42:32.080]   I find that a lot of the aha moments happen
[00:42:32.080 --> 00:42:36.400]   when you use deep RL to teach people about neural networks,
[00:42:36.400 --> 00:42:37.960]   which is counterintuitive.
[00:42:37.960 --> 00:42:40.800]   I find like a lot of the inspired sort of fire
[00:42:40.800 --> 00:42:42.400]   in people's passion, people's eyes,
[00:42:42.400 --> 00:42:44.880]   comes from the RL world.
[00:42:44.880 --> 00:42:48.640]   Do you find reinforcement learning to be a useful part
[00:42:48.640 --> 00:42:50.680]   of the teaching process or no?
[00:42:50.680 --> 00:42:53.520]   - I still teach reinforcement learning
[00:42:53.520 --> 00:42:55.640]   in one of my Stanford classes.
[00:42:55.640 --> 00:42:57.520]   And my PhD thesis was on reinforcement learning.
[00:42:57.520 --> 00:42:59.440]   So I currently love the field.
[00:42:59.440 --> 00:43:01.600]   I find that if I'm trying to teach students
[00:43:01.600 --> 00:43:04.640]   the most useful techniques for them to use today,
[00:43:04.640 --> 00:43:07.180]   I end up shrinking the amount of time
[00:43:07.180 --> 00:43:08.840]   I talk about reinforcement learning.
[00:43:08.840 --> 00:43:10.920]   It's not what's working today.
[00:43:10.920 --> 00:43:12.440]   Now our world changes so fast.
[00:43:12.440 --> 00:43:16.040]   Maybe it'll be totally different in a couple of years.
[00:43:16.040 --> 00:43:17.840]   But I think we need a couple more things
[00:43:17.840 --> 00:43:19.680]   for reinforcement learning to get there.
[00:43:19.680 --> 00:43:20.600]   - To actually get there, yeah.
[00:43:20.600 --> 00:43:22.760]   - One of my teams is looking to reinforcement learning
[00:43:22.760 --> 00:43:23.960]   for some robotic control tasks.
[00:43:23.960 --> 00:43:25.280]   So I see the applications,
[00:43:25.280 --> 00:43:28.680]   but if you look at it as a percentage of all of the impact
[00:43:28.680 --> 00:43:30.160]   of the types of things we do,
[00:43:30.160 --> 00:43:35.160]   is at least today, outside of playing video games
[00:43:35.280 --> 00:43:38.500]   in a few of the games, the scope.
[00:43:38.500 --> 00:43:40.980]   Actually at NeurIPS, a bunch of us were standing around
[00:43:40.980 --> 00:43:42.900]   saying, "Hey, what's your best example
[00:43:42.900 --> 00:43:45.340]   "of an actual deployed reinforcement learning application?"
[00:43:45.340 --> 00:43:49.100]   And among senior machine learning researchers.
[00:43:49.100 --> 00:43:51.500]   And again, there are some emerging ones,
[00:43:51.500 --> 00:43:55.340]   but there are not that many great examples.
[00:43:55.340 --> 00:43:58.300]   - I think you're absolutely right.
[00:43:58.300 --> 00:44:02.020]   The sad thing is there hasn't been a big application
[00:44:02.020 --> 00:44:04.900]   impact for real-world application reinforcement learning.
[00:44:04.900 --> 00:44:09.460]   I think its biggest impact to me has been in the toy domain,
[00:44:09.460 --> 00:44:11.400]   in the game domain, in the small example.
[00:44:11.400 --> 00:44:13.660]   That's what I mean for educational purpose.
[00:44:13.660 --> 00:44:16.920]   It seems to be a fun thing to explore neural networks with.
[00:44:16.920 --> 00:44:19.160]   But I think from your perspective,
[00:44:19.160 --> 00:44:21.980]   and I think that might be the best perspective,
[00:44:21.980 --> 00:44:24.820]   is if you're trying to educate with a simple example
[00:44:24.820 --> 00:44:28.440]   in order to illustrate how this can actually be grown
[00:44:28.440 --> 00:44:31.760]   to scale and have a real-world impact,
[00:44:31.760 --> 00:44:33.780]   then perhaps focusing on the fundamentals
[00:44:33.780 --> 00:44:38.780]   of supervised learning in the context of a simple data set,
[00:44:38.780 --> 00:44:41.940]   even like an MNIST data set is the right way,
[00:44:41.940 --> 00:44:43.980]   is the right path to take.
[00:44:43.980 --> 00:44:46.920]   I just, the amount of fun I've seen people have
[00:44:46.920 --> 00:44:48.540]   with reinforcement learning has been great,
[00:44:48.540 --> 00:44:52.900]   but not in the applied impact on the real-world setting.
[00:44:52.900 --> 00:44:55.500]   So it's a trade-off, how much impact you want to have
[00:44:55.500 --> 00:44:57.420]   versus how much fun you want to have.
[00:44:57.420 --> 00:44:58.260]   - Yeah, that's really cool.
[00:44:58.260 --> 00:45:01.340]   And I feel like the world actually needs all sorts.
[00:45:01.340 --> 00:45:02.660]   Even within machine learning,
[00:45:02.660 --> 00:45:05.900]   I feel like deep learning is so exciting,
[00:45:05.900 --> 00:45:08.500]   but the AI team shouldn't just use deep learning.
[00:45:08.500 --> 00:45:11.780]   I find that my teams use a portfolio of tools.
[00:45:11.780 --> 00:45:13.540]   And maybe that's not the exciting thing to say,
[00:45:13.540 --> 00:45:15.740]   but some days we use a neural net,
[00:45:15.740 --> 00:45:20.100]   some days we use a PCA.
[00:45:20.100 --> 00:45:21.620]   Actually, the other day I was sitting down with my team
[00:45:21.620 --> 00:45:22.860]   looking at PC residuals,
[00:45:22.860 --> 00:45:23.940]   trying to figure out what's going on
[00:45:23.940 --> 00:45:25.860]   with PC applied to a manufacturing problem.
[00:45:25.860 --> 00:45:28.340]   And some days we use a probabilistic graphical model,
[00:45:28.340 --> 00:45:29.940]   some days we use a knowledge graph,
[00:45:29.940 --> 00:45:30.780]   which is one of the things
[00:45:30.780 --> 00:45:33.100]   that has tremendous industry impact,
[00:45:33.100 --> 00:45:35.740]   but the amount of chatter about knowledge drafts
[00:45:35.740 --> 00:45:37.380]   in academia is really thin
[00:45:37.380 --> 00:45:39.700]   compared to the actual real-world impact.
[00:45:39.700 --> 00:45:41.460]   So I think reinforcement learning
[00:45:41.460 --> 00:45:42.660]   should be in that portfolio,
[00:45:42.660 --> 00:45:44.460]   and it's about balancing how much we teach
[00:45:44.460 --> 00:45:45.340]   all of these things.
[00:45:45.340 --> 00:45:47.940]   And the world should have diverse skills.
[00:45:47.940 --> 00:45:51.580]   It'd be sad if everyone just learned one narrow thing.
[00:45:51.580 --> 00:45:52.460]   - Yeah, the diverse skill
[00:45:52.460 --> 00:45:55.300]   help you discover the right tool for the job.
[00:45:55.300 --> 00:45:57.380]   What is the most beautiful, surprising,
[00:45:57.380 --> 00:46:00.780]   or inspiring idea in deep learning to you?
[00:46:00.780 --> 00:46:04.660]   Something that captivated your imagination.
[00:46:04.660 --> 00:46:07.220]   Is it the scale that could be,
[00:46:07.220 --> 00:46:09.060]   the performance that could be achieved with scale,
[00:46:09.060 --> 00:46:10.420]   or is there other ideas?
[00:46:10.420 --> 00:46:14.420]   - I think that if my only job
[00:46:14.420 --> 00:46:16.580]   was being an academic researcher,
[00:46:16.580 --> 00:46:18.180]   if an unlimited budget,
[00:46:18.180 --> 00:46:21.900]   and didn't have to worry about short-term impact
[00:46:21.900 --> 00:46:23.860]   and only focus on long-term impact,
[00:46:23.860 --> 00:46:25.460]   I probably spend all my time doing research
[00:46:25.460 --> 00:46:26.700]   on unsupervised learning.
[00:46:27.580 --> 00:46:30.380]   I still think unsupervised learning is a beautiful idea.
[00:46:30.380 --> 00:46:34.500]   At both this past NeurIPS and ICML,
[00:46:34.500 --> 00:46:36.140]   I was attending workshops
[00:46:36.140 --> 00:46:37.540]   or listening to various talks
[00:46:37.540 --> 00:46:39.380]   about self-supervised learning,
[00:46:39.380 --> 00:46:41.620]   which is one vertical segment,
[00:46:41.620 --> 00:46:43.380]   maybe, of sort of unsupervised learning
[00:46:43.380 --> 00:46:45.260]   that I'm excited about.
[00:46:45.260 --> 00:46:46.540]   Maybe just to summarize the idea,
[00:46:46.540 --> 00:46:47.500]   I guess you know the idea,
[00:46:47.500 --> 00:46:48.580]   but I'll describe briefly.
[00:46:48.580 --> 00:46:49.420]   - No, please.
[00:46:49.420 --> 00:46:52.100]   - So here's the example of self-supervised learning.
[00:46:52.100 --> 00:46:54.980]   Let's say we grab a lot of unlabeled images
[00:46:54.980 --> 00:46:55.820]   off the internet.
[00:46:55.820 --> 00:46:58.260]   We have infinite amounts of this type of data.
[00:46:58.260 --> 00:46:59.380]   I'm gonna take each image
[00:46:59.380 --> 00:47:03.100]   and rotate it by a random multiple of 90 degrees.
[00:47:03.100 --> 00:47:06.340]   And then I'm going to train a supervised neural network
[00:47:06.340 --> 00:47:09.060]   to predict what was the original orientation.
[00:47:09.060 --> 00:47:10.980]   So has this been rotated 90 degrees,
[00:47:10.980 --> 00:47:14.380]   180 degrees, 270 degrees, or zero degrees?
[00:47:14.380 --> 00:47:17.700]   So you can generate an infinite amounts of labeled data
[00:47:17.700 --> 00:47:19.060]   because you rotated the image,
[00:47:19.060 --> 00:47:21.420]   so you know what's the ground truth label.
[00:47:21.420 --> 00:47:24.380]   And so various researchers have found
[00:47:24.380 --> 00:47:26.540]   that by taking unlabeled data
[00:47:26.540 --> 00:47:28.460]   and making up labeled datasets
[00:47:28.460 --> 00:47:31.580]   and training a large neural network on these tasks,
[00:47:31.580 --> 00:47:33.820]   you can then take the hidden layer representation
[00:47:33.820 --> 00:47:37.300]   and transfer it to a different task very powerfully.
[00:47:37.300 --> 00:47:39.660]   Learning word embeddings,
[00:47:39.660 --> 00:47:41.380]   where we take a sentence, delete a word,
[00:47:41.380 --> 00:47:44.100]   predict the missing word, which is how we learn.
[00:47:44.100 --> 00:47:46.140]   One of the ways we learn word embeddings
[00:47:46.140 --> 00:47:47.780]   is another example.
[00:47:47.780 --> 00:47:51.100]   And I think there's now this portfolio of techniques
[00:47:51.100 --> 00:47:53.740]   for generating these made-up tasks.
[00:47:53.740 --> 00:47:57.380]   Another one called Jigsaw would be if you take an image,
[00:47:57.380 --> 00:47:59.820]   cut it up into a three by three grid,
[00:47:59.820 --> 00:48:02.100]   so like a nine, three by three puzzle piece,
[00:48:02.100 --> 00:48:05.140]   jump up to nine pieces and have a neural network predict
[00:48:05.140 --> 00:48:08.460]   which of the nine factorial possible permutations
[00:48:08.460 --> 00:48:09.940]   it came from.
[00:48:09.940 --> 00:48:13.700]   So many groups, including OpenAI,
[00:48:13.700 --> 00:48:16.860]   Peter Abbeel's been doing some work on this too,
[00:48:16.860 --> 00:48:20.220]   Facebook, Google Brain, I think DeepMind,
[00:48:20.220 --> 00:48:23.060]   oh, actually Aaron Van Der Oort has great work
[00:48:23.060 --> 00:48:24.460]   on the CPC objective.
[00:48:24.460 --> 00:48:26.220]   So many teams are doing exciting work
[00:48:26.220 --> 00:48:30.500]   and I think this is a way to generate infinite label data.
[00:48:30.500 --> 00:48:33.340]   And I find this a very exciting piece
[00:48:33.340 --> 00:48:34.180]   of unsupervised learning.
[00:48:34.180 --> 00:48:37.260]   - So long-term you think that's going to unlock
[00:48:37.260 --> 00:48:40.060]   a lot of power in machine learning systems
[00:48:40.060 --> 00:48:42.380]   is this kind of unsupervised learning?
[00:48:42.380 --> 00:48:44.020]   - I don't think there's a whole enchilada.
[00:48:44.020 --> 00:48:45.180]   I think it's just a piece of it.
[00:48:45.180 --> 00:48:48.860]   And I think this one piece, self-supervised learning
[00:48:48.860 --> 00:48:50.300]   is starting to get traction.
[00:48:50.300 --> 00:48:53.300]   We're very close to it being useful.
[00:48:53.300 --> 00:48:55.540]   Well, word embeddings is really useful.
[00:48:55.540 --> 00:48:57.180]   I think we're getting closer and closer
[00:48:57.180 --> 00:49:00.380]   to just having a significant real world impact
[00:49:00.380 --> 00:49:02.340]   maybe in computer vision and video.
[00:49:02.340 --> 00:49:05.140]   But I think this concept,
[00:49:05.140 --> 00:49:08.180]   and I think there'll be other concepts around it.
[00:49:08.180 --> 00:49:10.660]   Other unsupervised learning things that I worked on
[00:49:10.660 --> 00:49:12.220]   I've been excited about.
[00:49:12.220 --> 00:49:14.700]   I was really excited about sparse coding
[00:49:14.700 --> 00:49:17.620]   and ICA, slow feature analysis.
[00:49:17.620 --> 00:49:20.180]   I think all of these are ideas that various of us
[00:49:20.180 --> 00:49:21.820]   were working on about a decade ago
[00:49:21.820 --> 00:49:23.300]   before we all got distracted
[00:49:23.300 --> 00:49:25.820]   by how well supervised learning was doing.
[00:49:25.820 --> 00:49:26.660]   - Yeah.
[00:49:26.660 --> 00:49:29.540]   So we would return to the fundamentals
[00:49:29.540 --> 00:49:30.860]   of representation learning
[00:49:30.860 --> 00:49:33.900]   that really started this movement of deep learning.
[00:49:33.900 --> 00:49:35.940]   - I think there's a lot more work that one could explore
[00:49:35.940 --> 00:49:38.340]   around this theme of ideas and other ideas
[00:49:38.340 --> 00:49:40.300]   to come up with better algorithms.
[00:49:40.300 --> 00:49:44.020]   - So if we could return to maybe talk quickly
[00:49:44.020 --> 00:49:46.740]   about the specifics of deeplearning.ai,
[00:49:46.740 --> 00:49:49.540]   the deep learning specialization perhaps.
[00:49:49.540 --> 00:49:51.340]   How long does it take to complete the course,
[00:49:51.340 --> 00:49:52.740]   would you say?
[00:49:52.740 --> 00:49:55.300]   - The official length of the deep learning specialization
[00:49:55.300 --> 00:49:59.020]   is I think 16 weeks, so about four months,
[00:49:59.020 --> 00:50:00.700]   but it's go at your own pace.
[00:50:00.700 --> 00:50:03.660]   So if you subscribe to the deep learning specialization,
[00:50:03.660 --> 00:50:05.820]   there are people that finish that in less than a month
[00:50:05.820 --> 00:50:08.100]   by working more intensely and studying more intensely.
[00:50:08.100 --> 00:50:10.740]   So it really depends on the individual.
[00:50:10.740 --> 00:50:13.460]   Yeah, when we created the deep learning specialization,
[00:50:13.460 --> 00:50:18.100]   we wanted to make it very accessible and very affordable.
[00:50:18.100 --> 00:50:21.820]   And with Coursera and deeplearning.ai's education mission,
[00:50:21.820 --> 00:50:23.500]   one of the things that's really important to me
[00:50:23.500 --> 00:50:27.180]   is that if there's someone for whom paying anything
[00:50:27.180 --> 00:50:29.460]   is a financial hardship,
[00:50:29.460 --> 00:50:32.740]   then just apply for financial aid and get it for free.
[00:50:32.740 --> 00:50:38.100]   - If you were to recommend a daily schedule for people
[00:50:38.100 --> 00:50:39.540]   in learning, whether it's through
[00:50:39.540 --> 00:50:41.380]   the deeplearning.ai specialization
[00:50:41.380 --> 00:50:44.180]   or just learning in the world of deep learning,
[00:50:44.180 --> 00:50:46.900]   what would you recommend?
[00:50:46.900 --> 00:50:50.300]   How would they go about day-to-day sort of specific advice
[00:50:50.300 --> 00:50:52.700]   about learning, about their journey
[00:50:52.700 --> 00:50:54.900]   in the world of deep learning, machine learning?
[00:50:54.900 --> 00:50:59.140]   - I think getting the habit of learning is key,
[00:50:59.140 --> 00:51:01.220]   and that means regularity.
[00:51:01.220 --> 00:51:06.660]   So for example, we send out our weekly newsletter,
[00:51:06.660 --> 00:51:08.100]   The Batch, every Wednesday.
[00:51:08.100 --> 00:51:09.660]   So people know it's coming Wednesday,
[00:51:09.660 --> 00:51:11.660]   you can spend a little bit of time on Wednesday,
[00:51:11.660 --> 00:51:14.500]   catching up on the latest news through The Batch
[00:51:14.500 --> 00:51:17.500]   on Wednesday.
[00:51:17.500 --> 00:51:20.020]   And for myself, I've picked up a habit
[00:51:20.020 --> 00:51:22.620]   of spending some time every Saturday
[00:51:22.620 --> 00:51:24.660]   and every Sunday reading or studying.
[00:51:24.660 --> 00:51:26.700]   And so I don't wake up on a Saturday
[00:51:26.700 --> 00:51:27.780]   and have to make a decision.
[00:51:27.780 --> 00:51:30.220]   Do I feel like reading or studying today or not?
[00:51:30.220 --> 00:51:31.740]   It's just what I do.
[00:51:31.740 --> 00:51:34.260]   And the fact is a habit makes it easier.
[00:51:34.260 --> 00:51:37.580]   So I think if someone can get into that habit,
[00:51:37.580 --> 00:51:40.180]   it's like, you know, just like we brush our teeth
[00:51:40.180 --> 00:51:42.140]   every morning, I don't think about it.
[00:51:42.140 --> 00:51:43.620]   If I thought about it, it's a little bit annoying
[00:51:43.620 --> 00:51:46.060]   to have to spend two minutes doing that,
[00:51:46.060 --> 00:51:49.260]   but it's a habit that it takes no cognitive load,
[00:51:49.260 --> 00:51:50.500]   but this would be so much harder
[00:51:50.500 --> 00:51:53.180]   if we have to make a decision every morning.
[00:51:53.180 --> 00:51:54.780]   So, and actually that's the reason
[00:51:54.780 --> 00:51:56.140]   why I wear the same thing every day as well.
[00:51:56.140 --> 00:51:57.300]   It's just one less decision.
[00:51:57.300 --> 00:51:59.660]   I just get up and wear my blue shirt.
[00:51:59.660 --> 00:52:01.300]   So, but I think if you can get that habit,
[00:52:01.300 --> 00:52:02.980]   that consistency of studying,
[00:52:02.980 --> 00:52:05.740]   then it actually feels easier.
[00:52:05.740 --> 00:52:08.420]   - So yeah, it's kind of amazing.
[00:52:08.420 --> 00:52:12.780]   In my own life, like I play guitar every day for,
[00:52:12.780 --> 00:52:15.660]   I force myself to at least for five minutes play guitar.
[00:52:15.660 --> 00:52:18.260]   It's a ridiculously short period of time,
[00:52:18.260 --> 00:52:20.220]   but because I've gotten into that habit,
[00:52:20.220 --> 00:52:21.860]   it's incredible what you can accomplish
[00:52:21.860 --> 00:52:24.540]   in a period of a year or two years.
[00:52:24.540 --> 00:52:28.380]   You can become, you know, exceptionally good
[00:52:28.380 --> 00:52:30.980]   at certain aspects of a thing by just doing it every day
[00:52:30.980 --> 00:52:32.140]   for a very short period of time.
[00:52:32.140 --> 00:52:34.740]   It's kind of a miracle that that's how it works.
[00:52:34.740 --> 00:52:36.300]   It adds up over time.
[00:52:36.300 --> 00:52:39.700]   - Yeah, and I think it's often not about the burst
[00:52:39.700 --> 00:52:41.980]   of sustained efforts and the all-nighters,
[00:52:41.980 --> 00:52:44.340]   because you can only do that a limited number of times.
[00:52:44.340 --> 00:52:47.340]   It's the sustained effort over a long time.
[00:52:47.340 --> 00:52:50.500]   I think, you know, reading two research papers
[00:52:50.500 --> 00:52:52.060]   is a nice thing to do,
[00:52:52.060 --> 00:52:54.340]   but the power is not reading two research papers.
[00:52:54.340 --> 00:52:57.620]   It's reading two research papers a week for a year.
[00:52:57.620 --> 00:52:58.980]   Then you've read a hundred papers
[00:52:58.980 --> 00:53:02.100]   and you actually learn a lot when you read a hundred papers.
[00:53:02.100 --> 00:53:06.540]   - So regularity and making learning a habit.
[00:53:06.540 --> 00:53:10.300]   Do you have general other study tips
[00:53:10.300 --> 00:53:14.100]   for particularly deep learning that people should,
[00:53:14.100 --> 00:53:15.700]   in their process of learning,
[00:53:15.700 --> 00:53:17.340]   is there some kind of recommendations
[00:53:17.340 --> 00:53:20.460]   or tips you have as they learn?
[00:53:20.460 --> 00:53:22.220]   - One thing I still do
[00:53:22.220 --> 00:53:23.980]   when I'm trying to study something really deeply
[00:53:23.980 --> 00:53:26.500]   is take handwritten notes.
[00:53:26.500 --> 00:53:27.340]   It varies.
[00:53:27.340 --> 00:53:28.340]   I know there are a lot of people
[00:53:28.340 --> 00:53:32.100]   that take the deep learning courses during a commute
[00:53:32.100 --> 00:53:34.460]   or something where it may be more awkward to take notes.
[00:53:34.460 --> 00:53:37.420]   So I know it may not work for everyone,
[00:53:37.420 --> 00:53:40.420]   but when I'm taking courses on Coursera,
[00:53:40.420 --> 00:53:42.380]   and I still take some every now and then,
[00:53:42.380 --> 00:53:43.220]   the most recent one I took
[00:53:43.220 --> 00:53:45.060]   was a course on clinical trials
[00:53:45.060 --> 00:53:46.340]   because I was interested about that.
[00:53:46.340 --> 00:53:48.580]   I got out my little Moleskine notebook
[00:53:48.580 --> 00:53:49.580]   and I was sitting at my desk,
[00:53:49.580 --> 00:53:51.100]   I was just taking down notes
[00:53:51.100 --> 00:53:52.260]   of what the instructor was saying.
[00:53:52.260 --> 00:53:55.500]   And that act, we know that that act of taking notes,
[00:53:55.500 --> 00:53:59.420]   preferably handwritten notes, increases retention.
[00:53:59.420 --> 00:54:02.420]   - So as you're sort of watching the video,
[00:54:02.420 --> 00:54:03.900]   just kind of pausing maybe
[00:54:03.900 --> 00:54:07.900]   and then taking the basic insights down on paper?
[00:54:07.900 --> 00:54:10.100]   - Yeah, so there've been a few studies.
[00:54:10.100 --> 00:54:11.220]   If you search online,
[00:54:11.220 --> 00:54:12.780]   you find some of these studies
[00:54:12.780 --> 00:54:15.220]   that taking handwritten notes,
[00:54:15.220 --> 00:54:18.140]   because handwriting is slower, as we're saying just now,
[00:54:18.140 --> 00:54:23.220]   it causes you to recode the knowledge in your own words more
[00:54:23.220 --> 00:54:26.820]   and that process of recoding promotes long-term retention.
[00:54:26.820 --> 00:54:29.020]   This is as opposed to typing, which is fine.
[00:54:29.020 --> 00:54:30.820]   Again, typing is better than nothing,
[00:54:30.820 --> 00:54:32.860]   or in taking a class and not taking notes is better
[00:54:32.860 --> 00:54:34.460]   than not taking any class at all.
[00:54:34.460 --> 00:54:38.140]   But comparing handwritten notes and typing,
[00:54:38.140 --> 00:54:39.660]   you can usually type faster.
[00:54:39.660 --> 00:54:41.540]   For a lot of people, you can handwrite notes.
[00:54:41.540 --> 00:54:43.060]   And so when people type,
[00:54:43.060 --> 00:54:45.500]   they're more likely to just transcribe verbatim
[00:54:45.500 --> 00:54:46.380]   what they heard,
[00:54:46.380 --> 00:54:49.180]   and that reduces the amount of recoding.
[00:54:49.180 --> 00:54:52.540]   And that actually results in less long-term retention.
[00:54:52.540 --> 00:54:54.420]   - I don't know what the psychological effect there is,
[00:54:54.420 --> 00:54:55.420]   but it's so true.
[00:54:55.420 --> 00:54:57.020]   There's something fundamentally different
[00:54:57.020 --> 00:54:59.660]   about writing, handwriting.
[00:54:59.660 --> 00:55:00.500]   I wonder what that is.
[00:55:00.500 --> 00:55:01.700]   I wonder if it is as simple
[00:55:01.700 --> 00:55:04.420]   as just the time it takes to write is slower.
[00:55:04.420 --> 00:55:08.180]   - Yeah, and because you can't write as many words,
[00:55:08.180 --> 00:55:10.300]   you have to take whatever they said
[00:55:10.300 --> 00:55:12.060]   and summarize it into fewer words.
[00:55:12.060 --> 00:55:13.500]   And that summarization process
[00:55:13.500 --> 00:55:15.980]   requires deeper processing of the meaning,
[00:55:15.980 --> 00:55:17.980]   which then results in better retention.
[00:55:17.980 --> 00:55:19.020]   - That's fascinating.
[00:55:19.020 --> 00:55:22.500]   - Oh, and I've spent, I think, because of Coursera,
[00:55:22.500 --> 00:55:24.220]   I've spent so much time studying pedagogy.
[00:55:24.220 --> 00:55:25.340]   It's actually one of my passions.
[00:55:25.340 --> 00:55:28.140]   I really love learning how to more efficiently
[00:55:28.140 --> 00:55:29.260]   help others learn.
[00:55:30.260 --> 00:55:33.620]   - Yeah, one of the things I do both when creating videos
[00:55:33.620 --> 00:55:35.340]   or when we write the batch is,
[00:55:35.340 --> 00:55:39.220]   I try to think, is one minute spent with us
[00:55:39.220 --> 00:55:42.020]   going to be a more efficient learning experience
[00:55:42.020 --> 00:55:43.900]   than one minute spent anywhere else?
[00:55:43.900 --> 00:55:46.460]   And we really try to, you know,
[00:55:46.460 --> 00:55:48.420]   make it time efficient for the learners
[00:55:48.420 --> 00:55:50.140]   'cause, you know, everyone's busy.
[00:55:50.140 --> 00:55:53.500]   So when we're editing, I often tell my teams,
[00:55:53.500 --> 00:55:55.300]   every word needs to fight for its life.
[00:55:55.300 --> 00:55:56.980]   And if we can delete a word, let's just delete it
[00:55:56.980 --> 00:56:00.220]   and not wait, let's not waste the learners' time.
[00:56:00.220 --> 00:56:02.260]   - Wow, that's so, it's so amazing that you think that way
[00:56:02.260 --> 00:56:04.300]   'cause there is millions of people that are impacted
[00:56:04.300 --> 00:56:06.860]   by your teaching and sort of that one minute spent
[00:56:06.860 --> 00:56:08.420]   has a ripple effect, right?
[00:56:08.420 --> 00:56:11.740]   Through years of time, which is fascinating to think about.
[00:56:11.740 --> 00:56:14.340]   How does one make a career
[00:56:14.340 --> 00:56:16.020]   out of an interest in deep learning?
[00:56:16.020 --> 00:56:18.780]   Do you have advice for people?
[00:56:18.780 --> 00:56:21.460]   We just talked about sort of the beginning, early steps,
[00:56:21.460 --> 00:56:24.420]   but if you want to make it an entire life's journey
[00:56:24.420 --> 00:56:28.780]   or at least a journey of a decade or two, how do you do it?
[00:56:28.780 --> 00:56:31.220]   - So most important thing is to get started.
[00:56:31.220 --> 00:56:32.060]   - Right, of course.
[00:56:32.060 --> 00:56:35.500]   - And I think in the early parts of a career,
[00:56:35.500 --> 00:56:38.820]   coursework, like the deep learning specialization,
[00:56:38.820 --> 00:56:43.900]   is a very efficient way to master this material.
[00:56:43.900 --> 00:56:47.420]   So, because, you know, instructors,
[00:56:47.420 --> 00:56:49.340]   be it me or someone else, or, you know,
[00:56:49.340 --> 00:56:51.860]   Lawrence Moroney teaches our TensorFlow specialization
[00:56:51.860 --> 00:56:53.420]   and other things we're working on,
[00:56:53.420 --> 00:56:56.220]   spend effort to try to make it time efficient
[00:56:56.220 --> 00:56:58.140]   for you to learn a new concept.
[00:56:58.140 --> 00:57:01.100]   So coursework is actually a very efficient way
[00:57:01.100 --> 00:57:02.780]   for people to learn concepts
[00:57:02.780 --> 00:57:05.460]   in the beginning parts of breaking into a new field.
[00:57:05.460 --> 00:57:08.980]   In fact, one thing I see at Stanford,
[00:57:08.980 --> 00:57:10.780]   some of my PhD students want to jump
[00:57:10.780 --> 00:57:11.860]   into research right away.
[00:57:11.860 --> 00:57:13.620]   And I actually tend to say, look,
[00:57:13.620 --> 00:57:15.460]   in your first couple of years as a PhD student,
[00:57:15.460 --> 00:57:18.420]   spend time taking courses, 'cause it lays a foundation.
[00:57:18.420 --> 00:57:20.100]   It's fine if you're less productive
[00:57:20.100 --> 00:57:21.140]   in your first couple of years.
[00:57:21.140 --> 00:57:23.860]   You'll be better off in the longterm.
[00:57:23.860 --> 00:57:24.980]   Beyond a certain point,
[00:57:24.980 --> 00:57:27.820]   there's materials that doesn't exist in courses
[00:57:27.820 --> 00:57:28.940]   because it's too cutting edge.
[00:57:28.940 --> 00:57:30.220]   The course hasn't been created yet.
[00:57:30.220 --> 00:57:31.460]   There's some practical experience
[00:57:31.460 --> 00:57:34.660]   that we're not yet that good as teaching in a course.
[00:57:34.660 --> 00:57:37.780]   And I think after exhausting the efficient coursework,
[00:57:37.780 --> 00:57:40.420]   then most people need to go on
[00:57:40.420 --> 00:57:44.620]   to either ideally work on projects,
[00:57:44.620 --> 00:57:47.220]   and then maybe also continue their learning
[00:57:47.220 --> 00:57:49.700]   by reading blog posts and research papers
[00:57:49.700 --> 00:57:51.140]   and things like that.
[00:57:51.140 --> 00:57:53.020]   Doing projects is really important.
[00:57:53.020 --> 00:57:56.700]   And again, I think it's important to start small
[00:57:56.700 --> 00:57:58.380]   and to just do something.
[00:57:58.380 --> 00:57:59.540]   Today, you read about deep learning,
[00:57:59.540 --> 00:58:00.380]   it feels like, oh, all these people
[00:58:00.380 --> 00:58:01.820]   are doing such exciting things.
[00:58:01.820 --> 00:58:03.620]   What if I'm not building a neural network
[00:58:03.620 --> 00:58:04.460]   that changes the world?
[00:58:04.460 --> 00:58:05.300]   Then what's the point?
[00:58:05.300 --> 00:58:07.100]   Well, the point is sometimes building
[00:58:07.100 --> 00:58:08.780]   that tiny neural network,
[00:58:08.780 --> 00:58:13.060]   be it MNIST or upgrade to a fashion MNIST, to whatever.
[00:58:13.060 --> 00:58:15.380]   So doing your own fun hobby project.
[00:58:15.380 --> 00:58:16.740]   That's how you gain the skills
[00:58:16.740 --> 00:58:18.940]   to let you do bigger and bigger projects.
[00:58:18.940 --> 00:58:21.260]   I find this to be true at the individual level
[00:58:21.260 --> 00:58:23.780]   and also at the organizational level.
[00:58:23.780 --> 00:58:25.540]   For a company to become good at machine learning,
[00:58:25.540 --> 00:58:26.860]   sometimes the right thing to do
[00:58:26.860 --> 00:58:29.900]   is not to tackle the giant project,
[00:58:29.900 --> 00:58:31.940]   is instead to do the small project
[00:58:31.940 --> 00:58:34.060]   that lets the organization learn
[00:58:34.060 --> 00:58:35.340]   and then build out from there.
[00:58:35.340 --> 00:58:39.500]   But this is true both for individuals and for companies.
[00:58:39.500 --> 00:58:41.420]   - To taking the first step
[00:58:41.420 --> 00:58:45.260]   and then taking small steps is the key.
[00:58:45.260 --> 00:58:47.860]   Should students pursue a PhD, do you think?
[00:58:47.860 --> 00:58:49.300]   You can do so much.
[00:58:49.300 --> 00:58:51.540]   That's one of the fascinating things in machine learning.
[00:58:51.540 --> 00:58:54.860]   You can have so much impact without ever getting a PhD.
[00:58:54.860 --> 00:58:56.420]   So what are your thoughts?
[00:58:56.420 --> 00:58:57.740]   Should people go to grad school?
[00:58:57.740 --> 00:58:59.780]   Should people get a PhD?
[00:58:59.780 --> 00:59:02.100]   - I think that there are multiple good options
[00:59:02.100 --> 00:59:05.380]   of which doing a PhD could be one of them.
[00:59:05.380 --> 00:59:08.860]   I think that if someone's admitted to a top PhD program,
[00:59:08.860 --> 00:59:12.140]   you know, at MIT, Stanford, top schools,
[00:59:12.140 --> 00:59:15.700]   I think that's a very good experience.
[00:59:15.700 --> 00:59:19.180]   Or if someone gets a job at a top organization,
[00:59:19.180 --> 00:59:20.540]   at the top AI team,
[00:59:20.540 --> 00:59:24.060]   I think that's also a very good experience.
[00:59:24.060 --> 00:59:25.980]   There are some things you still need a PhD to do.
[00:59:25.980 --> 00:59:27.780]   If someone's aspiration is to be a professor,
[00:59:27.780 --> 00:59:29.180]   you know, at the top academic university,
[00:59:29.180 --> 00:59:31.140]   you just need a PhD to do that.
[00:59:31.140 --> 00:59:33.380]   But if it goes to, you know, start a company,
[00:59:33.380 --> 00:59:35.420]   build a company, do great technical work,
[00:59:35.420 --> 00:59:37.740]   I think a PhD is a good experience.
[00:59:37.740 --> 00:59:40.340]   But I would look at the different options
[00:59:40.340 --> 00:59:41.540]   available to someone, you know,
[00:59:41.540 --> 00:59:43.100]   where are the places where you can get a job?
[00:59:43.100 --> 00:59:45.060]   Where are the places you can get in a PhD program?
[00:59:45.060 --> 00:59:47.660]   And kind of weigh the pros and cons of those.
[00:59:47.660 --> 00:59:50.060]   - So just to linger on that for a little bit longer,
[00:59:50.060 --> 00:59:53.020]   what final dreams and goals do you think people should have?
[00:59:53.020 --> 00:59:57.420]   So what options should they explore?
[00:59:57.420 --> 01:00:01.180]   So you can work in industry, so for a large company,
[01:00:01.180 --> 01:00:03.580]   like Google, Facebook, Baidu,
[01:00:03.580 --> 01:00:06.140]   all these large sort of companies
[01:00:06.140 --> 01:00:09.300]   that already have huge teams of machine learning engineers.
[01:00:09.300 --> 01:00:11.020]   You can also do within industry,
[01:00:11.020 --> 01:00:12.340]   sort of more research groups
[01:00:12.340 --> 01:00:15.180]   that kind of like Google Research, Google Brain.
[01:00:15.180 --> 01:00:17.380]   Then you can also do, like we said,
[01:00:17.380 --> 01:00:20.420]   a professor in academia.
[01:00:20.420 --> 01:00:21.940]   And what else?
[01:00:21.940 --> 01:00:23.980]   Oh, you can build your own company.
[01:00:23.980 --> 01:00:25.180]   You can do a startup.
[01:00:25.180 --> 01:00:28.620]   Is there anything that stands out between those options
[01:00:28.620 --> 01:00:30.900]   or are they all beautiful different journeys
[01:00:30.900 --> 01:00:32.780]   that people should consider?
[01:00:32.780 --> 01:00:34.820]   - I think the thing that affects your experience more
[01:00:34.820 --> 01:00:38.180]   is less are you in this company versus that company
[01:00:38.180 --> 01:00:40.140]   or academia versus industry.
[01:00:40.140 --> 01:00:41.660]   I think the thing that affects your experience most
[01:00:41.660 --> 01:00:43.740]   is who are the people you're interacting with
[01:00:43.740 --> 01:00:45.500]   in a daily basis.
[01:00:45.500 --> 01:00:49.540]   So even if you look at some of the large companies,
[01:00:49.540 --> 01:00:51.900]   the experience of individuals in different teams
[01:00:51.900 --> 01:00:53.060]   is very different.
[01:00:53.060 --> 01:00:56.260]   And what matters most is not the logo above the door
[01:00:56.260 --> 01:00:58.460]   when you walk into the giant building every day.
[01:00:58.460 --> 01:01:00.620]   What matters the most is who are the 10 people,
[01:01:00.620 --> 01:01:03.260]   who are the 30 people you interact with every day.
[01:01:03.260 --> 01:01:04.980]   So I actually tend to advise people,
[01:01:04.980 --> 01:01:07.540]   if you get a job from a company,
[01:01:07.540 --> 01:01:10.300]   ask who is your manager, who are your peers,
[01:01:10.300 --> 01:01:11.460]   who are you actually gonna talk to?
[01:01:11.460 --> 01:01:12.580]   We're all social creatures.
[01:01:12.580 --> 01:01:15.500]   We tend to become more like the people around us.
[01:01:15.500 --> 01:01:17.620]   And if you're working with great people,
[01:01:17.620 --> 01:01:19.180]   you will learn faster.
[01:01:19.180 --> 01:01:20.660]   Or if you get admitted,
[01:01:20.660 --> 01:01:24.260]   if you get a job at a great company or a great university,
[01:01:24.260 --> 01:01:26.860]   maybe the logo you walk in is great,
[01:01:26.860 --> 01:01:28.340]   but you're actually stuck on some team
[01:01:28.340 --> 01:01:31.300]   doing really work that doesn't excite you.
[01:01:31.300 --> 01:01:33.780]   And then that's actually a really bad experience.
[01:01:33.780 --> 01:01:36.340]   So this is true both for universities
[01:01:36.340 --> 01:01:38.140]   and for large companies.
[01:01:38.140 --> 01:01:39.860]   For small companies, you can kind of figure out
[01:01:39.860 --> 01:01:41.980]   who you'd be working with quite quickly.
[01:01:41.980 --> 01:01:43.860]   And I tend to advise people,
[01:01:43.860 --> 01:01:46.780]   if a company refuses to tell you who you work with,
[01:01:46.780 --> 01:01:47.820]   someone say, "Oh, join us.
[01:01:47.820 --> 01:01:49.660]   "The rotation system, we'll figure it out."
[01:01:49.660 --> 01:01:51.700]   I think that's a worrying answer
[01:01:51.700 --> 01:01:56.140]   because it means you may not get sent to,
[01:01:56.140 --> 01:01:58.340]   you may not actually get to a team
[01:01:58.340 --> 01:02:00.940]   with great peers and great people to work with.
[01:02:00.940 --> 01:02:02.660]   - It's actually a really profound advice
[01:02:02.660 --> 01:02:05.140]   that we kind of sometimes sweep.
[01:02:05.140 --> 01:02:08.660]   We don't consider too rigorously or carefully.
[01:02:08.660 --> 01:02:11.500]   The people around you are really often,
[01:02:11.500 --> 01:02:13.140]   especially when you accomplish great things,
[01:02:13.140 --> 01:02:14.700]   it seems the great things are accomplished
[01:02:14.700 --> 01:02:16.780]   because of the people around you.
[01:02:16.780 --> 01:02:21.780]   So it's not about whether you learn this thing or that thing
[01:02:21.780 --> 01:02:25.140]   or like you said, the logo that hangs up top,
[01:02:25.140 --> 01:02:25.980]   it's the people.
[01:02:25.980 --> 01:02:27.540]   That's a fascinating,
[01:02:27.540 --> 01:02:29.460]   and it's such a hard search process
[01:02:29.460 --> 01:02:34.220]   of finding, just like finding the right friends
[01:02:34.220 --> 01:02:37.540]   and somebody to get married with and that kind of thing.
[01:02:37.540 --> 01:02:40.980]   It's a very hard search, it's a people search problem.
[01:02:40.980 --> 01:02:43.740]   - Yeah, but I think when someone interviews
[01:02:43.740 --> 01:02:45.340]   at a university or the research lab
[01:02:45.340 --> 01:02:46.980]   or the large corporation,
[01:02:46.980 --> 01:02:50.300]   it's good to insist on just asking who are the people?
[01:02:50.300 --> 01:02:51.420]   Who is my manager?
[01:02:51.420 --> 01:02:53.900]   And if you refuse to tell me, I'm gonna think,
[01:02:53.900 --> 01:02:55.700]   well, maybe that's 'cause you don't have a good answer.
[01:02:55.700 --> 01:02:57.340]   It may not be someone I like.
[01:02:57.340 --> 01:02:59.500]   - And if you don't particularly connect,
[01:02:59.500 --> 01:03:02.420]   if something feels off with the people,
[01:03:02.420 --> 01:03:06.380]   then don't stick to it.
[01:03:06.380 --> 01:03:08.700]   That's a really important signal to consider.
[01:03:08.700 --> 01:03:09.580]   - Yeah, yeah.
[01:03:09.580 --> 01:03:13.420]   And actually in my Stanford class, CS230,
[01:03:13.420 --> 01:03:14.620]   as well as an ACM talk,
[01:03:14.620 --> 01:03:18.300]   I think I gave like a hour long talk on career advice,
[01:03:18.300 --> 01:03:21.260]   including on the job search process and then some of these.
[01:03:21.260 --> 01:03:23.300]   So you can find those videos online.
[01:03:23.300 --> 01:03:25.140]   - Awesome, and I'll point them.
[01:03:25.140 --> 01:03:27.180]   I'll point people to them, beautiful.
[01:03:27.180 --> 01:03:33.420]   So the AI fund helps AI startups get off the ground,
[01:03:33.420 --> 01:03:34.820]   or perhaps you can elaborate
[01:03:34.820 --> 01:03:37.020]   on all the fun things it's involved with.
[01:03:37.020 --> 01:03:37.860]   What's your advice
[01:03:37.860 --> 01:03:40.860]   on how does one build a successful AI startup?
[01:03:42.380 --> 01:03:44.980]   - In Silicon Valley, a lot of startup failures
[01:03:44.980 --> 01:03:48.500]   come from building a product that no one wanted.
[01:03:48.500 --> 01:03:53.460]   So when, cool technology, but who's gonna use it?
[01:03:53.460 --> 01:03:57.700]   So I think I tend to be very outcome driven
[01:03:57.700 --> 01:03:59.500]   and customer obsessed.
[01:03:59.500 --> 01:04:04.140]   Ultimately, we don't get to vote if we succeed or fail.
[01:04:04.140 --> 01:04:07.020]   It's only the customer that they're the only one
[01:04:07.020 --> 01:04:09.620]   that gets a thumbs up or thumbs down votes in the long term.
[01:04:09.620 --> 01:04:12.140]   In the short term, there are various people
[01:04:12.140 --> 01:04:13.100]   that get various votes,
[01:04:13.100 --> 01:04:16.340]   but in the long term, that's what really matters.
[01:04:16.340 --> 01:04:17.500]   - So as you build a startup,
[01:04:17.500 --> 01:04:19.820]   you have to constantly ask the question,
[01:04:19.820 --> 01:04:24.260]   will the customer give a thumbs up on this?
[01:04:24.260 --> 01:04:25.100]   - I think so.
[01:04:25.100 --> 01:04:27.460]   I think startups that are very customer focused,
[01:04:27.460 --> 01:04:30.420]   customer obsessed, deeply understand the customer
[01:04:30.420 --> 01:04:33.340]   and are oriented to serve the customer
[01:04:33.340 --> 01:04:36.580]   are more likely to succeed.
[01:04:36.580 --> 01:04:37.420]   With the provisional,
[01:04:37.420 --> 01:04:39.100]   I think all of us should only do things
[01:04:39.100 --> 01:04:40.940]   that we think create social good
[01:04:40.940 --> 01:04:42.340]   and moves the world forward.
[01:04:42.340 --> 01:04:44.500]   So I personally don't wanna build
[01:04:44.500 --> 01:04:47.900]   addictive digital products just to sell a lot of ads.
[01:04:47.900 --> 01:04:50.740]   There are things that could be lucrative that I won't do,
[01:04:50.740 --> 01:04:55.340]   but if we can find ways to serve people in meaningful ways,
[01:04:55.340 --> 01:04:59.060]   I think those can be great things to do,
[01:04:59.060 --> 01:05:01.580]   either in the academic setting or in a corporate setting
[01:05:01.580 --> 01:05:03.100]   or a startup setting.
[01:05:03.100 --> 01:05:07.740]   - So can you give me the idea of why you started the AI fund?
[01:05:08.740 --> 01:05:13.340]   - I remember when I was leading the AI group at Baidu,
[01:05:13.340 --> 01:05:15.980]   I had two jobs, two parts of my job.
[01:05:15.980 --> 01:05:17.460]   One was to build an AI engine
[01:05:17.460 --> 01:05:19.140]   to support the existing businesses.
[01:05:19.140 --> 01:05:23.340]   And that was running, just ran, just performed by itself.
[01:05:23.340 --> 01:05:24.860]   The second part of my job at the time,
[01:05:24.860 --> 01:05:27.340]   which was to try to systematically initiate
[01:05:27.340 --> 01:05:29.100]   new lines of businesses
[01:05:29.100 --> 01:05:31.140]   using the company's AI capabilities.
[01:05:31.140 --> 01:05:34.500]   So the self-driving car team came out of my group,
[01:05:34.500 --> 01:05:37.140]   the smart speaker team,
[01:05:37.140 --> 01:05:41.020]   similar to what is Amazon Echo or Alexa in the US,
[01:05:41.020 --> 01:05:42.860]   but we actually announced it before Amazon did.
[01:05:42.860 --> 01:05:46.980]   So Baidu wasn't following Amazon.
[01:05:46.980 --> 01:05:48.780]   That came out of my group,
[01:05:48.780 --> 01:05:53.380]   and I found that to be actually the most fun part of my job.
[01:05:53.380 --> 01:05:56.500]   So what I wanted to do was to build AI fund
[01:05:56.500 --> 01:05:58.300]   as a startup studio
[01:05:58.300 --> 01:06:02.700]   to systematically create new startups from scratch.
[01:06:02.700 --> 01:06:04.940]   With all the things we can now do with AI,
[01:06:04.940 --> 01:06:07.380]   I think the ability to build new teams,
[01:06:07.380 --> 01:06:10.100]   to go after this rich space of opportunities
[01:06:10.100 --> 01:06:12.340]   is a very important way to,
[01:06:12.340 --> 01:06:14.980]   very important mechanism to get these projects done
[01:06:14.980 --> 01:06:16.580]   that I think will move the world forward.
[01:06:16.580 --> 01:06:19.340]   So I've been fortunate to have built a few teams
[01:06:19.340 --> 01:06:21.660]   that had a meaningful, positive impact.
[01:06:21.660 --> 01:06:25.140]   And I felt that we might be able to do this
[01:06:25.140 --> 01:06:27.980]   in a more systematic, repeatable way.
[01:06:27.980 --> 01:06:31.540]   So a startup studio is a relatively new concept.
[01:06:31.540 --> 01:06:35.780]   There are maybe dozens of startup studios right now,
[01:06:35.780 --> 01:06:38.740]   but I feel like all of us,
[01:06:38.740 --> 01:06:40.940]   many teams are still trying to figure out
[01:06:40.940 --> 01:06:43.780]   how do you systematically build companies
[01:06:43.780 --> 01:06:45.460]   with a high success rate?
[01:06:45.460 --> 01:06:49.140]   So I think even a lot of my venture capital friends
[01:06:49.140 --> 01:06:51.740]   are seem to be more and more building companies
[01:06:51.740 --> 01:06:53.140]   rather than investing in companies.
[01:06:53.140 --> 01:06:55.340]   But I find it a fascinating thing to do
[01:06:55.340 --> 01:06:56.700]   to figure out the mechanisms
[01:06:56.700 --> 01:06:59.700]   by which we could systematically build successful teams,
[01:06:59.700 --> 01:07:03.420]   successful businesses in areas that we find meaningful.
[01:07:03.420 --> 01:07:05.820]   - So a startup studio is something,
[01:07:05.820 --> 01:07:09.260]   is a place and a mechanism for startups
[01:07:09.260 --> 01:07:11.220]   to go from zero to success,
[01:07:11.220 --> 01:07:13.740]   to try to develop a blueprint.
[01:07:13.740 --> 01:07:14.820]   - It's actually a place for us
[01:07:14.820 --> 01:07:16.540]   to build startups from scratch.
[01:07:16.540 --> 01:07:21.300]   So we often bring in founders and work with them,
[01:07:21.300 --> 01:07:23.900]   or maybe even have existing ideas
[01:07:23.900 --> 01:07:26.620]   that we match founders with.
[01:07:26.620 --> 01:07:29.660]   And then this launches, hopefully,
[01:07:29.660 --> 01:07:31.140]   into successful companies.
[01:07:31.140 --> 01:07:34.580]   - So how close are you to figuring out a way
[01:07:34.580 --> 01:07:38.460]   to automate the process of starting from scratch
[01:07:38.460 --> 01:07:40.540]   and building successful AI startup?
[01:07:40.540 --> 01:07:44.460]   - Yeah, I think we've been constantly improving
[01:07:44.460 --> 01:07:48.460]   and iterating on our processes, how we do that.
[01:07:48.460 --> 01:07:51.340]   So things like, how many customer calls do we need to make
[01:07:51.340 --> 01:07:53.500]   in order to get customer validation?
[01:07:53.500 --> 01:07:55.260]   How do we make sure this technology can be built?
[01:07:55.260 --> 01:07:58.060]   Quite a lot of our businesses need cutting edge
[01:07:58.060 --> 01:07:59.140]   machine learning algorithms.
[01:07:59.140 --> 01:08:00.660]   So kind of algorithms that are developed
[01:08:00.660 --> 01:08:02.580]   in the last one or two years.
[01:08:02.580 --> 01:08:04.980]   And even if it works in a research paper,
[01:08:04.980 --> 01:08:07.020]   it turns out taking the production is really hard.
[01:08:07.020 --> 01:08:09.500]   There are a lot of issues for making these things work
[01:08:09.500 --> 01:08:14.100]   in the real life that are not widely addressed in academia.
[01:08:14.100 --> 01:08:17.220]   So how do we validate that this is actually doable?
[01:08:17.220 --> 01:08:18.300]   How do you build a team,
[01:08:18.300 --> 01:08:19.940]   get the specialized domain knowledge,
[01:08:19.940 --> 01:08:21.500]   be it in education or healthcare,
[01:08:21.500 --> 01:08:23.140]   or whatever sector we're focusing on?
[01:08:23.140 --> 01:08:24.460]   So I think we've actually getting,
[01:08:24.460 --> 01:08:29.180]   we've been getting much better at giving the entrepreneurs
[01:08:29.180 --> 01:08:32.140]   a high success rate, but I think we're still,
[01:08:32.140 --> 01:08:33.820]   I think the whole world is still
[01:08:33.820 --> 01:08:35.500]   in the early phases of figuring this out.
[01:08:35.500 --> 01:08:39.180]   - But do you think there is some aspects of that process
[01:08:39.180 --> 01:08:41.620]   that are transferable from one startup to another,
[01:08:41.620 --> 01:08:43.220]   to another, to another?
[01:08:43.220 --> 01:08:45.020]   - Yeah, very much so.
[01:08:45.020 --> 01:08:47.700]   You know, starting a company to most entrepreneurs
[01:08:47.700 --> 01:08:50.700]   is a really lonely thing.
[01:08:50.700 --> 01:08:54.500]   And I've seen so many entrepreneurs not know
[01:08:54.500 --> 01:08:56.300]   how to make certain decisions.
[01:08:56.300 --> 01:09:00.100]   Like, when do you need to, how do you do B2B sales?
[01:09:00.100 --> 01:09:02.380]   If you don't know that, it's really hard.
[01:09:02.380 --> 01:09:05.540]   Or how do you market this efficiently
[01:09:05.540 --> 01:09:08.420]   other than buying ads, which is really expensive?
[01:09:08.420 --> 01:09:10.060]   Are there more efficient tactics for that?
[01:09:10.060 --> 01:09:13.060]   Or for a machine learning project,
[01:09:13.060 --> 01:09:15.300]   basic decisions can change the course
[01:09:15.300 --> 01:09:18.460]   of whether a machine learning product works or not.
[01:09:18.460 --> 01:09:21.100]   And so there are so many hundreds of decisions
[01:09:21.100 --> 01:09:22.700]   that entrepreneurs need to make,
[01:09:22.700 --> 01:09:25.780]   and making a mistake in a couple of key decisions
[01:09:25.780 --> 01:09:30.260]   can have a huge impact on the fate of the company.
[01:09:30.260 --> 01:09:33.100]   So I think a startup studio provides a support structure
[01:09:33.100 --> 01:09:34.420]   that makes starting a company
[01:09:34.420 --> 01:09:36.300]   much less of a lonely experience.
[01:09:36.300 --> 01:09:40.020]   And also when facing with these key decisions,
[01:09:40.020 --> 01:09:44.820]   like trying to hire your first VP of engineering,
[01:09:44.820 --> 01:09:46.460]   what's a good selection criteria?
[01:09:46.460 --> 01:09:47.300]   How do you source?
[01:09:47.300 --> 01:09:48.820]   Should I hire this person or not?
[01:09:48.820 --> 01:09:53.140]   By having an ecosystem around the entrepreneurs,
[01:09:53.140 --> 01:09:54.700]   the founders, to help,
[01:09:54.700 --> 01:09:57.460]   I think we help them at the key moments
[01:09:57.460 --> 01:10:01.020]   and hopefully significantly make them more enjoyable
[01:10:01.020 --> 01:10:02.540]   and then higher success rate.
[01:10:02.540 --> 01:10:04.700]   - So they have somebody to brainstorm with
[01:10:04.700 --> 01:10:08.020]   in these very difficult decision points.
[01:10:08.020 --> 01:10:10.980]   - And also to help them recognize
[01:10:10.980 --> 01:10:14.340]   what they may not even realize is a key decision point.
[01:10:14.340 --> 01:10:15.180]   Right?
[01:10:15.180 --> 01:10:17.300]   That's the first and probably the most important part.
[01:10:17.300 --> 01:10:18.220]   Yeah.
[01:10:18.220 --> 01:10:20.100]   - Actually, I can say one other thing.
[01:10:20.100 --> 01:10:23.900]   I think building companies is one thing,
[01:10:23.900 --> 01:10:26.460]   but I feel like it's really important
[01:10:26.460 --> 01:10:30.100]   that we build companies that move the world forward.
[01:10:30.100 --> 01:10:32.580]   For example, within the AI fund team,
[01:10:32.580 --> 01:10:35.620]   there was once an idea for a new company
[01:10:35.620 --> 01:10:37.460]   that if it had succeeded,
[01:10:37.460 --> 01:10:40.220]   would have resulted in people watching a lot more videos
[01:10:40.220 --> 01:10:43.340]   in a certain narrow vertical type of video.
[01:10:43.340 --> 01:10:45.700]   And I looked at it, the business case was fine,
[01:10:45.700 --> 01:10:46.820]   the revenue case was fine,
[01:10:46.820 --> 01:10:48.300]   but I looked at it and just said,
[01:10:48.300 --> 01:10:50.020]   "I don't want to do this.
[01:10:50.020 --> 01:10:52.500]   I don't actually just want to have a lot more people
[01:10:52.500 --> 01:10:53.860]   watch this type of video."
[01:10:53.860 --> 01:10:54.740]   Wasn't educational.
[01:10:54.740 --> 01:10:56.340]   It was educational, maybe.
[01:10:56.340 --> 01:10:59.980]   And so I cut the idea on the basis
[01:10:59.980 --> 01:11:01.980]   that I didn't think it would actually help people.
[01:11:01.980 --> 01:11:05.460]   So whether building companies or work of enterprises
[01:11:05.460 --> 01:11:06.700]   or doing personal projects,
[01:11:06.700 --> 01:11:11.020]   I think it's up to each of us to figure out
[01:11:11.020 --> 01:11:14.140]   what's the difference we want to make in the world.
[01:11:14.140 --> 01:11:15.340]   - With Learning AI,
[01:11:15.340 --> 01:11:17.100]   you help already established companies
[01:11:17.100 --> 01:11:20.220]   grow their AI and machine learning efforts.
[01:11:20.220 --> 01:11:22.980]   How does a large company integrate machine learning
[01:11:22.980 --> 01:11:24.020]   into their efforts?
[01:11:24.020 --> 01:11:27.700]   - AI is a general purpose technology,
[01:11:27.700 --> 01:11:30.540]   and I think it would transform every industry.
[01:11:30.540 --> 01:11:33.820]   Our community has already transformed to a large extent
[01:11:33.820 --> 01:11:35.500]   to software internet sector.
[01:11:35.500 --> 01:11:36.860]   Most software internet companies,
[01:11:36.860 --> 01:11:38.180]   they're outside the top, right?
[01:11:38.180 --> 01:11:40.100]   Five or six or three or four,
[01:11:40.100 --> 01:11:43.340]   already have reasonable machine learning capabilities
[01:11:43.340 --> 01:11:44.220]   or are getting there.
[01:11:44.220 --> 01:11:46.380]   It's still room for improvement.
[01:11:46.380 --> 01:11:49.220]   But when I look outside the software internet sector,
[01:11:49.220 --> 01:11:51.620]   everything from manufacturing, agriculture,
[01:11:51.620 --> 01:11:53.940]   healthcare, logistics, transportation,
[01:11:53.940 --> 01:11:55.540]   there's so many opportunities
[01:11:55.540 --> 01:11:57.980]   that very few people are working on.
[01:11:57.980 --> 01:11:59.780]   So I think the next wave for AI
[01:11:59.780 --> 01:12:03.420]   is for us to also transform all of those other industries.
[01:12:03.420 --> 01:12:04.620]   There was a McKinsey study
[01:12:04.620 --> 01:12:09.620]   estimating $13 trillion of global economic growth.
[01:12:09.700 --> 01:12:11.780]   US GDP is $19 trillion,
[01:12:11.780 --> 01:12:13.340]   so 13 trillion is a big number,
[01:12:13.340 --> 01:12:16.060]   or PWC estimate $16 trillion.
[01:12:16.060 --> 01:12:18.420]   So whatever number is, it's large.
[01:12:18.420 --> 01:12:20.780]   But the interesting thing to me was a lot of that impact
[01:12:20.780 --> 01:12:23.740]   would be outside the software internet sector.
[01:12:23.740 --> 01:12:28.060]   So we need more teams to work with these companies
[01:12:28.060 --> 01:12:29.780]   to help them adopt AI.
[01:12:29.780 --> 01:12:30.900]   And I think this is one of the things
[01:12:30.900 --> 01:12:33.700]   that'll help drive global economic growth
[01:12:33.700 --> 01:12:35.980]   and make humanity more powerful.
[01:12:35.980 --> 01:12:37.900]   - And like you said, the impact is there.
[01:12:37.900 --> 01:12:39.580]   So what are the best industries,
[01:12:39.580 --> 01:12:41.780]   the biggest industries where AI can help,
[01:12:41.780 --> 01:12:44.500]   perhaps outside the software tech sector?
[01:12:44.500 --> 01:12:46.300]   - Frankly, I think it's all of them.
[01:12:46.300 --> 01:12:49.940]   Some of the ones I'm spending a lot of time on
[01:12:49.940 --> 01:12:54.620]   are manufacturing, agriculture, looking into healthcare.
[01:12:54.620 --> 01:12:56.580]   For example, in manufacturing,
[01:12:56.580 --> 01:12:58.740]   we do a lot of work in visual inspection,
[01:12:58.740 --> 01:13:01.460]   where today there are people standing around
[01:13:01.460 --> 01:13:02.940]   using the eye, human eye,
[01:13:02.940 --> 01:13:05.820]   to check if this plastic part or the smartphone
[01:13:05.820 --> 01:13:09.460]   or this thing has a scratch or a dent or something in it.
[01:13:09.460 --> 01:13:12.500]   We can use a camera to take a picture,
[01:13:12.500 --> 01:13:15.500]   use a algorithm, deep learning and other things
[01:13:15.500 --> 01:13:17.900]   to check if it's defective or not,
[01:13:17.900 --> 01:13:20.540]   and thus help factories improve yield
[01:13:20.540 --> 01:13:23.660]   and improve quality and improve throughput.
[01:13:23.660 --> 01:13:25.820]   It turns out the practical problems we run into
[01:13:25.820 --> 01:13:28.180]   are very different than the ones you might read about
[01:13:28.180 --> 01:13:29.620]   in most research papers.
[01:13:29.620 --> 01:13:30.820]   The data sets are really small,
[01:13:30.820 --> 01:13:32.500]   so we face small data problems.
[01:13:32.500 --> 01:13:35.860]   You know, the factories keep on changing the environment,
[01:13:35.860 --> 01:13:38.380]   so it works well on your test set,
[01:13:38.380 --> 01:13:39.420]   but guess what?
[01:13:39.420 --> 01:13:42.060]   You know, something changes in the factory.
[01:13:42.060 --> 01:13:43.580]   The lights go on or off.
[01:13:43.580 --> 01:13:45.180]   Recently, there was a factory
[01:13:45.180 --> 01:13:47.900]   in which a bird flew through the factory
[01:13:47.900 --> 01:13:48.940]   and pooped on something,
[01:13:48.940 --> 01:13:50.860]   and so that changed stuff.
[01:13:50.860 --> 01:13:54.300]   And so increasing our algorithmic robustness,
[01:13:54.300 --> 01:13:57.100]   so all the changes happen in the factory,
[01:13:57.100 --> 01:13:59.300]   I find that we run into a lot of practical problems
[01:13:59.300 --> 01:14:02.660]   that are not as widely discussed in academia,
[01:14:02.660 --> 01:14:05.180]   and it's really fun kind of being on the cutting edge,
[01:14:05.180 --> 01:14:07.500]   solving these problems before, you know,
[01:14:07.500 --> 01:14:09.380]   maybe before many people are even aware
[01:14:09.380 --> 01:14:10.460]   that there is a problem there.
[01:14:10.460 --> 01:14:12.340]   - And that's such a fascinating space.
[01:14:12.340 --> 01:14:13.300]   You're absolutely right,
[01:14:13.300 --> 01:14:16.660]   but what is the first step that a company should take?
[01:14:16.660 --> 01:14:19.460]   It's just a scary leap into this new world
[01:14:19.460 --> 01:14:22.580]   of going from the human eye inspecting
[01:14:22.580 --> 01:14:24.780]   to digitizing that process,
[01:14:24.780 --> 01:14:27.340]   having a camera, having an algorithm.
[01:14:27.340 --> 01:14:28.340]   What's the first step?
[01:14:28.340 --> 01:14:31.260]   Like, what's the early journey that you recommend
[01:14:31.260 --> 01:14:33.580]   that you see these companies taking?
[01:14:33.580 --> 01:14:34.620]   - I published a document
[01:14:34.620 --> 01:14:37.220]   called the "AI Transformation Playbook."
[01:14:37.220 --> 01:14:39.260]   I found this online and taught briefly
[01:14:39.260 --> 01:14:41.620]   in the AI for Everyone course on Coursera
[01:14:41.620 --> 01:14:44.780]   about the long-term journey that companies should take,
[01:14:44.780 --> 01:14:47.580]   but the first step is actually to start small.
[01:14:47.580 --> 01:14:49.500]   I've seen a lot more companies fail
[01:14:49.500 --> 01:14:53.340]   by starting too big than by starting too small.
[01:14:53.340 --> 01:14:54.700]   Take even Google.
[01:14:54.700 --> 01:14:57.620]   You know, most people don't realize how hard it was
[01:14:57.620 --> 01:15:00.620]   and how controversial it was in the early days.
[01:15:00.620 --> 01:15:04.460]   So when I started Google Brain, it was controversial.
[01:15:04.460 --> 01:15:06.180]   People thought deep learning,
[01:15:06.180 --> 01:15:07.460]   tried it, didn't work.
[01:15:07.460 --> 01:15:09.300]   Why would you want to do deep learning?
[01:15:09.300 --> 01:15:12.500]   So my first internal customer within Google
[01:15:12.500 --> 01:15:14.060]   was the Google speech team,
[01:15:14.060 --> 01:15:17.220]   which is not the most lucrative project in Google,
[01:15:17.220 --> 01:15:18.420]   not the most important.
[01:15:18.420 --> 01:15:20.740]   It's not web search or advertising,
[01:15:20.740 --> 01:15:22.860]   but by starting small,
[01:15:22.860 --> 01:15:25.980]   my team helped the speech team
[01:15:25.980 --> 01:15:28.380]   build a more accurate speech recognition system.
[01:15:28.380 --> 01:15:30.820]   And this caused their peers, other teams,
[01:15:30.820 --> 01:15:33.060]   to start to have more faith in deep learning.
[01:15:33.060 --> 01:15:36.500]   My second internal customer was the Google Maps team,
[01:15:36.500 --> 01:15:39.660]   where we used computer vision to read house numbers
[01:15:39.660 --> 01:15:41.140]   from basic street view images
[01:15:41.140 --> 01:15:43.740]   to more accurately locate houses within Google Maps,
[01:15:43.740 --> 01:15:45.940]   so improve the quality of the geodata.
[01:15:45.940 --> 01:15:48.380]   And it was only after those two successes
[01:15:48.380 --> 01:15:50.660]   that I then started a more serious conversation
[01:15:50.660 --> 01:15:52.780]   with the Google Ads team.
[01:15:52.780 --> 01:15:54.220]   - And so there's a ripple effect
[01:15:54.220 --> 01:15:56.900]   that you showed that it works in these cases,
[01:15:56.900 --> 01:15:59.300]   and then it just propagates through the entire company,
[01:15:59.300 --> 01:16:02.980]   that this thing has a lot of value and use for us.
[01:16:02.980 --> 01:16:05.300]   - I think the early small-scale projects,
[01:16:05.300 --> 01:16:07.420]   it helps the teams gain faith,
[01:16:07.420 --> 01:16:11.740]   but also helps the teams learn what these technologies do.
[01:16:11.740 --> 01:16:14.540]   I still remember when our first GPU server,
[01:16:14.540 --> 01:16:17.020]   it was a server under some guy's desk.
[01:16:17.020 --> 01:16:20.580]   And that taught us early important lessons
[01:16:20.580 --> 01:16:25.220]   about how do you have multiple users share a set of GPUs,
[01:16:25.220 --> 01:16:27.180]   which was really not obvious at the time,
[01:16:27.180 --> 01:16:29.420]   but those early lessons were important.
[01:16:29.420 --> 01:16:32.140]   We learned a lot from that first GPU server
[01:16:32.140 --> 01:16:34.100]   that later helped the teams think through
[01:16:34.100 --> 01:16:37.540]   how to scale it up to much larger deployments.
[01:16:37.540 --> 01:16:40.260]   - Are there concrete challenges that companies face
[01:16:40.260 --> 01:16:43.900]   that you see is important for them to solve?
[01:16:43.900 --> 01:16:45.300]   - I think building and deploying
[01:16:45.300 --> 01:16:47.340]   machine learning systems is hard.
[01:16:47.340 --> 01:16:49.820]   There's a huge gulf between something that works
[01:16:49.820 --> 01:16:51.820]   in a Jupyter notebook on your laptop
[01:16:51.820 --> 01:16:53.060]   versus something that runs
[01:16:53.060 --> 01:16:54.620]   in a production deployment setting
[01:16:54.620 --> 01:16:58.420]   in a factory or agriculture plant or whatever.
[01:16:58.420 --> 01:17:00.660]   So I see a lot of people get something to work
[01:17:00.660 --> 01:17:01.500]   on your laptop and say,
[01:17:01.500 --> 01:17:02.340]   "Oh, look what I've done."
[01:17:02.340 --> 01:17:03.980]   And that's great, that's hard.
[01:17:03.980 --> 01:17:05.860]   That's a very important first step,
[01:17:05.860 --> 01:17:09.580]   but a lot of teams underestimate the rest of the steps needed.
[01:17:09.580 --> 01:17:12.500]   So for example, I've heard this exact same conversation
[01:17:12.500 --> 01:17:15.220]   between a lot of machine learning people and business people.
[01:17:15.220 --> 01:17:16.860]   The machine learning person says,
[01:17:16.860 --> 01:17:20.900]   "Look, my algorithm does well on the test set.
[01:17:20.900 --> 01:17:23.500]   It's a clean test set, I didn't peak."
[01:17:23.500 --> 01:17:26.700]   And the business person says, "Thank you very much,
[01:17:26.700 --> 01:17:29.380]   but your algorithm sucks, it doesn't work."
[01:17:29.380 --> 01:17:30.900]   And the machine learning person says,
[01:17:30.900 --> 01:17:33.660]   "No, wait, I did well on the test set."
[01:17:33.660 --> 01:17:38.420]   And I think there is a gulf between what it takes
[01:17:38.420 --> 01:17:40.660]   to do well on a test set on your hard drive
[01:17:40.660 --> 01:17:44.220]   versus what it takes to work well in a deployment setting.
[01:17:44.220 --> 01:17:48.780]   Some common problems, robustness and generalization.
[01:17:48.780 --> 01:17:50.620]   You deploy something in the factory,
[01:17:50.620 --> 01:17:52.660]   maybe they chop down a tree outside the factory
[01:17:52.660 --> 01:17:55.460]   so the tree no longer covers a window
[01:17:55.460 --> 01:17:56.500]   and the lighting is different.
[01:17:56.500 --> 01:17:57.820]   So the test set changes.
[01:17:57.820 --> 01:18:01.300]   And in machine learning, and especially in academia,
[01:18:01.300 --> 01:18:04.060]   we don't know how to deal with test set distributions
[01:18:04.060 --> 01:18:05.580]   that are dramatically different
[01:18:05.580 --> 01:18:07.460]   than the training set distribution.
[01:18:07.460 --> 01:18:08.660]   You know, there's research,
[01:18:08.660 --> 01:18:12.220]   there's stuff like domain annotation, transfer learning.
[01:18:12.220 --> 01:18:13.820]   You know, there are people working on it,
[01:18:13.820 --> 01:18:15.580]   but we're really not good at this.
[01:18:15.580 --> 01:18:18.180]   So how do you actually get this to work?
[01:18:18.180 --> 01:18:21.700]   Because your test set distribution is going to change.
[01:18:21.700 --> 01:18:25.780]   And I think also, if you look at the number of lines of code
[01:18:25.780 --> 01:18:28.660]   in the software system, the machine learning model
[01:18:28.660 --> 01:18:32.740]   is maybe 5% or even fewer relative
[01:18:32.740 --> 01:18:35.540]   to the entire software system you need to build.
[01:18:35.540 --> 01:18:37.260]   So how do you get all that work done
[01:18:37.260 --> 01:18:38.940]   and make it reliable and systematic?
[01:18:38.940 --> 01:18:42.700]   - So good software engineering work is fundamental here
[01:18:42.700 --> 01:18:46.380]   to building a successful small machine learning system.
[01:18:46.380 --> 01:18:49.340]   - Yes, and the software system needs to interface
[01:18:49.340 --> 01:18:50.620]   with people's workloads.
[01:18:50.620 --> 01:18:54.060]   So machine learning is automation on steroids.
[01:18:54.060 --> 01:18:56.420]   If we take one task out of many tasks
[01:18:56.420 --> 01:18:57.260]   that are done in the factory,
[01:18:57.260 --> 01:18:58.900]   so the factory does lots of things.
[01:18:58.900 --> 01:19:00.820]   One task is visual inspection.
[01:19:00.820 --> 01:19:03.980]   If we automate that one task, it can be really valuable,
[01:19:03.980 --> 01:19:06.180]   but you may need to redesign a lot of other tasks
[01:19:06.180 --> 01:19:07.380]   around that one task.
[01:19:07.380 --> 01:19:09.860]   For example, say the machine learning algorithm
[01:19:09.860 --> 01:19:11.020]   says this is defective.
[01:19:11.020 --> 01:19:11.940]   What are you supposed to do?
[01:19:11.940 --> 01:19:12.780]   Do you throw it away?
[01:19:12.780 --> 01:19:14.220]   Do you get a human to double check?
[01:19:14.220 --> 01:19:17.020]   Do you want to rework it or fix it?
[01:19:17.020 --> 01:19:18.620]   So you need to redesign a lot of tasks
[01:19:18.620 --> 01:19:20.780]   around that thing you've now automated.
[01:19:20.780 --> 01:19:23.340]   So planning for the change management
[01:19:23.340 --> 01:19:25.620]   and making sure that the software you write
[01:19:25.620 --> 01:19:27.460]   is consistent with the new workflow.
[01:19:27.460 --> 01:19:28.980]   And you take the time to explain to people
[01:19:28.980 --> 01:19:29.820]   what needs to happen.
[01:19:29.820 --> 01:19:34.820]   So I think what landing AI has become good at,
[01:19:34.820 --> 01:19:37.300]   and then I think we learn by making missteps
[01:19:37.300 --> 01:19:39.100]   and painful experiences.
[01:19:39.100 --> 01:19:43.740]   What we've become good at is working with our partners
[01:19:43.740 --> 01:19:46.500]   to think through all the things beyond
[01:19:46.500 --> 01:19:48.220]   just the machine learning model,
[01:19:48.220 --> 01:19:49.100]   that you put in the notebook,
[01:19:49.100 --> 01:19:51.580]   but to build the entire system,
[01:19:51.580 --> 01:19:54.380]   manage the change process and figure out how to deploy this
[01:19:54.380 --> 01:19:56.860]   in a way that has an actual impact.
[01:19:56.860 --> 01:19:59.980]   The processes that the large software tech companies use
[01:19:59.980 --> 01:20:03.020]   for deploying don't work for a lot of other scenarios.
[01:20:03.020 --> 01:20:07.060]   For example, when I was leading large speech teams,
[01:20:07.060 --> 01:20:09.820]   if the speech recognition system goes down, what happens?
[01:20:09.820 --> 01:20:12.220]   Well, alarms goes off and then someone like me would say,
[01:20:12.220 --> 01:20:15.060]   "Hey, you 20 engineers, please fix this."
[01:20:15.060 --> 01:20:15.900]   Right?
[01:20:15.900 --> 01:20:16.740]   And then we're good.
[01:20:16.740 --> 01:20:19.380]   But if you have a system go down in the factory,
[01:20:19.380 --> 01:20:21.460]   there are not 20 machine learning engineers
[01:20:21.460 --> 01:20:23.940]   sitting around, you can page a duty and have them fix it.
[01:20:23.940 --> 01:20:27.460]   So how do you deal with the maintenance or the DevOps
[01:20:27.460 --> 01:20:30.340]   or the MLOps or the other aspects of this?
[01:20:30.340 --> 01:20:34.060]   So these are concepts that I think landing AI
[01:20:34.060 --> 01:20:36.580]   and a few other teams are on the cutting edge of,
[01:20:36.580 --> 01:20:39.620]   but we don't even have systematic terminology yet
[01:20:39.620 --> 01:20:41.060]   to describe some of the stuff we do
[01:20:41.060 --> 01:20:43.380]   because I think we're inventing it on the fly.
[01:20:43.380 --> 01:20:46.700]   - So you mentioned some people are interested
[01:20:46.700 --> 01:20:48.820]   in discovering mathematical beauty and truth
[01:20:48.820 --> 01:20:49.700]   in the universe,
[01:20:49.700 --> 01:20:53.900]   and you're interested in having a big positive impact
[01:20:53.900 --> 01:20:55.140]   in the world.
[01:20:55.140 --> 01:20:55.980]   So let me ask--
[01:20:55.980 --> 01:20:57.340]   - The two are not inconsistent.
[01:20:57.340 --> 01:20:58.820]   - No, they're all together.
[01:20:58.820 --> 01:21:00.980]   I'm only half joking
[01:21:00.980 --> 01:21:03.580]   'cause you're probably interested a little bit in both.
[01:21:03.580 --> 01:21:06.140]   But let me ask a romanticized question.
[01:21:06.140 --> 01:21:09.580]   So much of the work, your work and our discussion today
[01:21:09.580 --> 01:21:12.100]   has been on applied AI.
[01:21:12.100 --> 01:21:14.620]   Maybe you can even call narrow AI,
[01:21:14.620 --> 01:21:15.900]   where the goal is to create systems
[01:21:15.900 --> 01:21:17.560]   that automate some specific process
[01:21:17.560 --> 01:21:19.820]   that adds a lot of value to the world.
[01:21:19.820 --> 01:21:21.260]   But there's another branch of AI,
[01:21:21.260 --> 01:21:22.940]   starting with Alan Turing,
[01:21:22.940 --> 01:21:25.660]   that kind of dreams of creating human level
[01:21:25.660 --> 01:21:27.500]   or superhuman level intelligence.
[01:21:27.500 --> 01:21:30.500]   Is this something you dream of as well?
[01:21:30.500 --> 01:21:33.300]   Do you think we human beings will ever build
[01:21:33.300 --> 01:21:34.620]   a human level intelligence
[01:21:34.620 --> 01:21:37.300]   or superhuman level intelligence system?
[01:21:37.300 --> 01:21:38.820]   - I would love to get to AGI,
[01:21:38.820 --> 01:21:40.980]   and I think humanity will,
[01:21:40.980 --> 01:21:45.140]   but whether it takes a hundred years or 500 or 5,000,
[01:21:45.140 --> 01:21:47.140]   I find hard to estimate.
[01:21:48.040 --> 01:21:49.560]   - Do you have,
[01:21:49.560 --> 01:21:53.260]   so some folks have worries about the different trajectories
[01:21:53.260 --> 01:21:54.480]   that path would take,
[01:21:54.480 --> 01:21:57.560]   even existential threats of an AGI system.
[01:21:57.560 --> 01:21:59.740]   Do you have such concerns,
[01:21:59.740 --> 01:22:02.380]   whether in the short term or the long term?
[01:22:02.380 --> 01:22:06.920]   - I do worry about the long-term fate of humanity.
[01:22:06.920 --> 01:22:09.400]   I do wonder as well,
[01:22:09.400 --> 01:22:13.320]   I do worry about overpopulation on the planet Mars,
[01:22:13.320 --> 01:22:14.360]   just not today.
[01:22:14.360 --> 01:22:16.040]   I think there will be a day
[01:22:16.040 --> 01:22:18.620]   when maybe someday in the future,
[01:22:18.620 --> 01:22:20.180]   Mars will be polluted,
[01:22:20.180 --> 01:22:21.680]   there are all these children dying,
[01:22:21.680 --> 01:22:23.260]   and someone will look back at this video and say,
[01:22:23.260 --> 01:22:25.060]   "Andrew, how was Andrew so heartless?
[01:22:25.060 --> 01:22:26.660]   He didn't care about all these children
[01:22:26.660 --> 01:22:28.260]   dying on the planet Mars."
[01:22:28.260 --> 01:22:30.380]   And I apologize to the future viewer,
[01:22:30.380 --> 01:22:32.040]   I do care about the children,
[01:22:32.040 --> 01:22:35.700]   but I just don't know how to productively work on that today.
[01:22:35.700 --> 01:22:37.980]   - Your picture will be in the dictionary
[01:22:37.980 --> 01:22:39.300]   for the people who are ignorant
[01:22:39.300 --> 01:22:41.700]   about the overpopulation on Mars.
[01:22:41.700 --> 01:22:44.460]   Yes, so it's a long-term problem.
[01:22:44.460 --> 01:22:45.940]   Is there something in the short term
[01:22:45.940 --> 01:22:47.700]   we should be thinking about
[01:22:47.700 --> 01:22:50.620]   in terms of aligning the values of our AI systems
[01:22:50.620 --> 01:22:54.420]   with the values of us humans?
[01:22:54.420 --> 01:22:56.580]   Sort of something that Stuart Russell
[01:22:56.580 --> 01:22:58.220]   and other folks are thinking about
[01:22:58.220 --> 01:23:00.660]   as this system develops more and more,
[01:23:00.660 --> 01:23:03.380]   we want to make sure that it represents
[01:23:03.380 --> 01:23:05.140]   the better angels of our nature,
[01:23:05.140 --> 01:23:08.720]   the ethics, the values of our society.
[01:23:08.720 --> 01:23:12.940]   - You know, if you take self-driving cars,
[01:23:12.940 --> 01:23:14.340]   the biggest problem with self-driving cars
[01:23:14.340 --> 01:23:17.940]   is not that there's some trolley dilemma,
[01:23:17.940 --> 01:23:19.380]   and you teach this, so you know,
[01:23:19.380 --> 01:23:21.900]   how many times when you are driving your car,
[01:23:21.900 --> 01:23:23.500]   did you face this moral dilemma?
[01:23:23.500 --> 01:23:25.900]   Who do I crash into?
[01:23:25.900 --> 01:23:28.380]   So I think self-driving cars will run into that problem
[01:23:28.380 --> 01:23:31.220]   roughly as often as we do when we drive our cars.
[01:23:31.220 --> 01:23:33.460]   The biggest problem with self-driving cars
[01:23:33.460 --> 01:23:35.860]   is when there's a big white truck across the road,
[01:23:35.860 --> 01:23:38.300]   and what you should do is brake and not crash into it,
[01:23:38.300 --> 01:23:41.260]   and the self-driving car fails and it crashes into it.
[01:23:41.260 --> 01:23:43.620]   So I think we need to solve that problem first.
[01:23:43.620 --> 01:23:45.740]   I think the problem with some of these discussions
[01:23:45.740 --> 01:23:50.740]   about AGI, you know, alignment, the paperclip problem,
[01:23:50.740 --> 01:23:56.340]   is that is a huge distraction from the much harder problems
[01:23:56.340 --> 01:23:58.940]   that we actually need to address today.
[01:23:58.940 --> 01:24:00.620]   Some of the hard problems we need to address today,
[01:24:00.620 --> 01:24:03.780]   I think bias is a huge issue.
[01:24:03.780 --> 01:24:05.620]   I worry about wealth inequality.
[01:24:05.620 --> 01:24:09.340]   AI and internet are causing an acceleration
[01:24:09.340 --> 01:24:10.820]   of concentration of power,
[01:24:10.820 --> 01:24:14.420]   because we can now centralize data, use AI to process it.
[01:24:14.420 --> 01:24:15.900]   And so industry after industry,
[01:24:15.900 --> 01:24:17.620]   we've affected every industry.
[01:24:17.620 --> 01:24:20.460]   So the internet industry has a lot of win-and-take modes
[01:24:20.460 --> 01:24:22.140]   or win-and-take-all dynamics,
[01:24:22.140 --> 01:24:24.380]   but we've infected all these other industries.
[01:24:24.380 --> 01:24:26.260]   So we're also giving these other industries
[01:24:26.260 --> 01:24:28.740]   win-and-take modes or win-and-take-all flavors.
[01:24:28.740 --> 01:24:32.580]   So look at what Uber and Lyft did to the taxi industry.
[01:24:32.580 --> 01:24:34.180]   So we're doing this type of thing to a lot of...
[01:24:34.180 --> 01:24:36.500]   So we're creating tremendous wealth,
[01:24:36.500 --> 01:24:39.980]   but how do we make sure that the wealth is fairly shared?
[01:24:39.980 --> 01:24:41.220]   I think that...
[01:24:41.220 --> 01:24:45.300]   And then how do we help people whose jobs are displaced?
[01:24:45.300 --> 01:24:47.020]   I think education is part of it.
[01:24:47.020 --> 01:24:50.620]   There may be even more that we need to do than education.
[01:24:50.620 --> 01:24:55.100]   I think bias is a serious issue.
[01:24:55.100 --> 01:24:57.180]   There are adverse uses of AI,
[01:24:57.180 --> 01:25:00.580]   like deepfakes being used for various nefarious purposes.
[01:25:00.580 --> 01:25:05.580]   So I worry about some teams, maybe accidentally,
[01:25:05.580 --> 01:25:07.860]   and I hope not deliberately,
[01:25:07.860 --> 01:25:11.060]   making a lot of noise about things,
[01:25:11.060 --> 01:25:13.180]   problems in the distant future,
[01:25:13.180 --> 01:25:15.860]   rather than focusing on some of the much harder problems.
[01:25:15.860 --> 01:25:17.700]   - Yeah, they overshadow the problems
[01:25:17.700 --> 01:25:18.820]   that we have already today.
[01:25:18.820 --> 01:25:21.300]   They're exceptionally challenging, like those you said,
[01:25:21.300 --> 01:25:22.620]   and even the silly ones,
[01:25:22.620 --> 01:25:24.620]   but the ones that have a huge impact,
[01:25:24.620 --> 01:25:26.020]   which is the lighting variation
[01:25:26.020 --> 01:25:28.020]   outside of your factory window.
[01:25:28.020 --> 01:25:31.380]   That ultimately is what makes the difference
[01:25:31.380 --> 01:25:33.220]   between, like you said, the Jupiter notebook
[01:25:33.220 --> 01:25:35.500]   and something that actually transforms
[01:25:35.500 --> 01:25:37.500]   an entire industry, potentially.
[01:25:37.500 --> 01:25:40.060]   - Yeah, and I think, and just to,
[01:25:40.060 --> 01:25:43.220]   some companies, when a regulator comes to you and says,
[01:25:43.220 --> 01:25:45.860]   "Look, your product is messing things up.
[01:25:45.860 --> 01:25:47.860]   "Fixing it may have a revenue impact."
[01:25:47.860 --> 01:25:49.460]   Well, it's much more fun to talk to them
[01:25:49.460 --> 01:25:51.860]   about how you promised not to wipe out humanity
[01:25:51.860 --> 01:25:54.820]   than to face the actually really hard problems we face.
[01:25:54.820 --> 01:25:57.620]   - So your life has been a great journey,
[01:25:57.620 --> 01:26:00.700]   from teaching to research to entrepreneurship.
[01:26:00.700 --> 01:26:01.940]   Two questions.
[01:26:01.940 --> 01:26:04.140]   One, are there regrets,
[01:26:04.140 --> 01:26:07.100]   moments that if you went back, you would do differently?
[01:26:07.100 --> 01:26:10.820]   And two, are there moments you're especially proud of,
[01:26:10.820 --> 01:26:13.260]   moments that made you truly happy?
[01:26:13.260 --> 01:26:15.740]   - You know, I've made so many mistakes.
[01:26:15.740 --> 01:26:20.260]   It feels like every time I discover something,
[01:26:20.260 --> 01:26:25.620]   I go, "Why didn't I think of this five years earlier
[01:26:25.620 --> 01:26:27.300]   "or even 10 years earlier?"
[01:26:27.300 --> 01:26:32.940]   And sometimes I read a book and I go,
[01:26:32.940 --> 01:26:34.900]   "I wish I read this book 10 years ago.
[01:26:34.900 --> 01:26:36.500]   "My life would have been so different."
[01:26:36.500 --> 01:26:37.700]   Although that happened recently.
[01:26:37.700 --> 01:26:38.860]   And then I was thinking,
[01:26:38.860 --> 01:26:41.660]   "If only I read this book when we're starting up Coursera,
[01:26:41.660 --> 01:26:43.900]   "it could have been so much better."
[01:26:43.900 --> 01:26:45.820]   But I discovered the book had not yet been written
[01:26:45.820 --> 01:26:46.740]   when we were starting Coursera,
[01:26:46.740 --> 01:26:48.140]   so that made me feel better.
[01:26:48.140 --> 01:26:53.220]   But I find that the process of discovery,
[01:26:53.220 --> 01:26:54.580]   we keep on finding out things
[01:26:54.580 --> 01:26:56.740]   that seem so obvious in hindsight,
[01:26:56.740 --> 01:26:59.460]   but it always takes us so much longer
[01:26:59.460 --> 01:27:02.180]   than I wish to figure it out.
[01:27:02.180 --> 01:27:05.100]   - So on the second question,
[01:27:06.380 --> 01:27:09.300]   are there moments in your life that,
[01:27:09.300 --> 01:27:12.460]   if you look back, that you're especially proud of
[01:27:12.460 --> 01:27:14.180]   or you're especially happy,
[01:27:14.180 --> 01:27:18.100]   that filled you with happiness and fulfillment?
[01:27:18.100 --> 01:27:19.940]   - Well, two answers.
[01:27:19.940 --> 01:27:21.420]   One, does my daughter know of her?
[01:27:21.420 --> 01:27:22.380]   - Yes, of course.
[01:27:22.380 --> 01:27:23.860]   - She's like, "No matter how much time I spend with her,
[01:27:23.860 --> 01:27:25.420]   "I just can't spend enough time with her."
[01:27:25.420 --> 01:27:26.540]   - Congratulations, by the way.
[01:27:26.540 --> 01:27:27.540]   - Thank you.
[01:27:27.540 --> 01:27:29.580]   And then second is helping other people.
[01:27:29.580 --> 01:27:32.260]   I think, to me, I think the meaning of life
[01:27:32.260 --> 01:27:36.860]   is helping others achieve whatever are their dreams.
[01:27:36.860 --> 01:27:40.140]   And then also to try to move the world forward
[01:27:40.140 --> 01:27:43.700]   by making humanity more powerful as a whole.
[01:27:43.700 --> 01:27:46.540]   So the times that I felt most happy and most proud
[01:27:46.540 --> 01:27:51.540]   was when I felt someone else allowed me the good fortune
[01:27:51.540 --> 01:27:56.140]   of helping them a little bit on the path to their dreams.
[01:27:56.140 --> 01:27:58.660]   - I think there's no better way to end it
[01:27:58.660 --> 01:28:00.860]   than talking about happiness and the meaning of life.
[01:28:00.860 --> 01:28:03.100]   So Andrew, it's a huge honor.
[01:28:03.100 --> 01:28:04.580]   Me and millions of people thank you
[01:28:04.580 --> 01:28:05.820]   for all the work you've done.
[01:28:05.820 --> 01:28:07.020]   Thank you for talking today.
[01:28:07.020 --> 01:28:08.820]   - Thank you so much, thanks.
[01:28:08.820 --> 01:28:11.740]   - Thanks for listening to this conversation with Andrew Ng.
[01:28:11.740 --> 01:28:14.740]   And thank you to our presenting sponsor, Cash App.
[01:28:14.740 --> 01:28:17.300]   Download it, use code LEXPODCAST.
[01:28:17.300 --> 01:28:20.220]   You'll get $10 and $10 will go to FIRST,
[01:28:20.220 --> 01:28:23.340]   an organization that inspires and educates young minds
[01:28:23.340 --> 01:28:26.580]   to become science and technology innovators of tomorrow.
[01:28:26.580 --> 01:28:29.540]   If you enjoy this podcast, subscribe on YouTube,
[01:28:29.540 --> 01:28:31.580]   give it five stars on Apple Podcast,
[01:28:31.580 --> 01:28:35.060]   support on Patreon, or simply connect with me on Twitter
[01:28:35.060 --> 01:28:36.180]   at Lex Friedman.
[01:28:36.180 --> 01:28:39.260]   And now let me leave you with some words of wisdom
[01:28:39.260 --> 01:28:40.300]   from Andrew Ng.
[01:28:40.300 --> 01:28:43.900]   Ask yourself if what you're working on
[01:28:43.900 --> 01:28:46.340]   succeeds beyond your wildest dreams,
[01:28:46.340 --> 01:28:48.840]   would you have significantly helped other people?
[01:28:48.840 --> 01:28:53.220]   If not, then keep searching for something else to work on.
[01:28:53.220 --> 01:28:56.620]   Otherwise, you're not living up to your full potential.
[01:28:57.900 --> 01:29:01.020]   Thank you for listening and hope to see you next time.
[01:29:01.020 --> 01:29:03.600]   (upbeat music)
[01:29:03.600 --> 01:29:06.180]   (upbeat music)
[01:29:06.180 --> 01:29:16.180]   [BLANK_AUDIO]

