
[00:00:00.000 --> 00:00:03.440]   just before OpenAI started, I met Ilya, who you, who you
[00:00:03.440 --> 00:00:06.400]   interviewed. One of the first things he said to me was, look,
[00:00:06.400 --> 00:00:08.480]   the models, they just want to learn, you have to understand
[00:00:08.480 --> 00:00:11.120]   this, the models, they just want to learn. And it was a bit like
[00:00:11.120 --> 00:00:14.320]   a Zen Cohen, like I kind of like, I listened to this, and I
[00:00:14.320 --> 00:00:20.120]   became enlightened. The models just want to learn, you get the
[00:00:20.120 --> 00:00:22.920]   obstacles out of their way, right? You give them, you give
[00:00:22.920 --> 00:00:27.520]   them good data, you, you give them enough space to operate in,
[00:00:27.640 --> 00:00:30.560]   you don't do something stupid, like condition them badly
[00:00:30.560 --> 00:00:33.800]   numerically, and they want to learn, they'll do it, they'll do
[00:00:33.800 --> 00:00:34.120]   it.
[00:00:34.120 --> 00:00:37.720]   There are many people who were aware back at that time,
[00:00:37.720 --> 00:00:40.120]   probably weren't working on it directly. But we're aware that
[00:00:40.120 --> 00:00:42.920]   these things are really good at speech recognition, or at
[00:00:42.920 --> 00:00:48.320]   playing these constrained games. Very few extrapolated from
[00:00:48.320 --> 00:00:52.000]   there, like you and Ilya did to something that is generally
[00:00:52.000 --> 00:00:54.560]   intelligent. What was different about the way you were thinking
[00:00:54.560 --> 00:00:57.400]   about it versus how others think that you went from like, is
[00:00:57.400 --> 00:00:59.440]   getting better at speech in this consistent way, it will get
[00:00:59.440 --> 00:01:01.080]   better at everything in this consistent way?
[00:01:01.080 --> 00:01:04.680]   Yeah, so I genuinely don't know. I mean, at first, when I saw it
[00:01:04.680 --> 00:01:07.920]   for speech, I assumed this was just true for speech or for this
[00:01:07.920 --> 00:01:12.040]   narrow class of models. I think it was just over the period
[00:01:12.040 --> 00:01:16.760]   between 2014 and 2017. I tried it for a lot of things and saw
[00:01:16.760 --> 00:01:19.640]   the same thing over and over again. I watched the same being
[00:01:19.640 --> 00:01:23.920]   true with Dota. I watched the same being true with robotics,
[00:01:23.920 --> 00:01:26.520]   which many people thought of as a counter example. But I just
[00:01:26.520 --> 00:01:29.200]   thought, well, it's hard to get data for robotics. But if we
[00:01:29.200 --> 00:01:32.520]   operate within if we look within the data that we have, we see
[00:01:32.520 --> 00:01:36.240]   the same patterns. And so I don't I don't know, I think
[00:01:36.240 --> 00:01:39.640]   people were very focused on solving the problem in front of
[00:01:39.640 --> 00:01:43.760]   them. Why one person thinks one way and other person is very,
[00:01:43.760 --> 00:01:47.840]   it's very hard to explain. I think people just see it through
[00:01:47.840 --> 00:01:50.280]   a different lens, you know, are looking like vertically instead
[00:01:50.280 --> 00:01:52.440]   of horizontally, they're not thinking about the scaling,
[00:01:52.440 --> 00:01:54.880]   they're thinking about how do I solve my problem? And well, for
[00:01:54.880 --> 00:02:00.280]   robotics, there's not enough data. And so, you know, and so
[00:02:00.280 --> 00:02:02.440]   you know, that can easily abstractable scaling doesn't
[00:02:02.440 --> 00:02:06.080]   work, because we don't have the data. And so I don't, I don't
[00:02:06.080 --> 00:02:08.800]   know, I just, for some reason, and it may just, it may just
[00:02:08.800 --> 00:02:11.640]   have been random chance was obsessed with that particular
[00:02:11.640 --> 00:02:14.600]   direction, this big blob of compute document, which I still
[00:02:14.600 --> 00:02:17.040]   have not made public, I probably should for like, historical
[00:02:17.040 --> 00:02:19.960]   reasons, I don't think it would tell anyone anything they don't
[00:02:19.960 --> 00:02:23.800]   know now. But when I wrote it, I actually said, Look, there are
[00:02:23.800 --> 00:02:27.280]   seven factors that and you know, I wasn't, I wasn't like, these
[00:02:27.280 --> 00:02:29.640]   are the factors, but I was just like, let me give some sense of
[00:02:29.640 --> 00:02:32.600]   the kinds of things that matter and what don't. And so number of
[00:02:32.600 --> 00:02:35.280]   parameters, scale of the model, like, you know, the compute and
[00:02:35.280 --> 00:02:40.120]   compute matters, quantity of data matters, quality of data
[00:02:40.120 --> 00:02:44.000]   matters, loss function matters. So like, you know, are you doing
[00:02:44.000 --> 00:02:46.680]   RL or doing next word prediction, if your loss
[00:02:46.680 --> 00:02:49.560]   function isn't rich, or doesn't incentivize the right thing, you
[00:02:49.560 --> 00:02:53.720]   will, you won't get anything. So those were the key four ones.
[00:02:54.240 --> 00:02:56.280]   Which I think are the core of the hypothesis. But then I said
[00:02:56.280 --> 00:03:00.400]   three more things. One was symmetries, which is basically
[00:03:00.400 --> 00:03:04.200]   like, if your architecture doesn't take into account the
[00:03:04.200 --> 00:03:08.360]   right kinds of symmetries, it doesn't work. Or it's it's very
[00:03:08.360 --> 00:03:12.080]   inefficient. So for example, convolutional neural networks
[00:03:12.080 --> 00:03:15.720]   take into account translational symmetry, LSTM is taking into
[00:03:15.720 --> 00:03:20.480]   account time symmetry. And but a weakness of LSTM is that they
[00:03:20.480 --> 00:03:23.520]   can't attend over the whole context. So there's kind of this
[00:03:23.520 --> 00:03:26.920]   structural weakness, like if a model isn't structurally
[00:03:26.920 --> 00:03:31.440]   capable of like, absorbing and managing things that happened in
[00:03:31.440 --> 00:03:34.280]   a far enough distant past, and it's just like, it's kind of
[00:03:34.280 --> 00:03:36.680]   like, you know, like, the compute doesn't flow, like the
[00:03:36.680 --> 00:03:41.200]   spice doesn't flow. It's like, you can't like, like, the blob
[00:03:41.200 --> 00:03:44.640]   has to be unencumbered, right? It kind of, it's not, it's not
[00:03:44.640 --> 00:03:48.080]   going to work if, if you artificially close things off.
[00:03:48.080 --> 00:03:52.040]   And I think RNNs and LSTMs artificially close things off,
[00:03:52.160 --> 00:03:56.280]   because they close you off to the distant past. And so again,
[00:03:56.280 --> 00:03:59.280]   things need to flow freely. If they don't, it doesn't work. If
[00:03:59.280 --> 00:04:02.440]   you set things up in kind of a way that's, that's set up to
[00:04:02.440 --> 00:04:05.720]   fail, or that doesn't allow the compute to work in an uninhibited
[00:04:05.720 --> 00:04:09.000]   way, then it won't work. And so transformers were kind of within
[00:04:09.000 --> 00:04:12.240]   that, even though I can't remember if the transformer
[00:04:12.240 --> 00:04:15.160]   paper had been published, it was around the same time as I wrote
[00:04:15.160 --> 00:04:16.960]   that document, it might have been just before it might have
[00:04:16.960 --> 00:04:17.720]   been just after.
[00:04:17.720 --> 00:04:21.120]   It sounds like from that view, the way to think about these
[00:04:21.120 --> 00:04:24.800]   algorithmic progresses is not as increasing the power of the
[00:04:24.800 --> 00:04:27.960]   blob of compute, but simply getting rid of the artificial
[00:04:27.960 --> 00:04:30.680]   hindrances that older architectures have. Is that, is
[00:04:30.680 --> 00:04:31.200]   that a fair way to-
[00:04:31.200 --> 00:04:33.000]   That's a little, that, yeah, that's, that's a little how I
[00:04:33.000 --> 00:04:35.920]   think about it. You know, again, if you go back to like, Ilyas,
[00:04:35.920 --> 00:04:39.000]   like the models want to learn, like, like the compute wants to
[00:04:39.000 --> 00:04:42.400]   be free. And like, you know, it's being blocked in various
[00:04:42.400 --> 00:04:44.360]   ways where you like, don't understand that it's being
[00:04:44.360 --> 00:04:46.040]   blocked. And so you need to like, free it up.
[00:04:46.040 --> 00:04:50.200]   Right, right. I love the, the gradients, change that to spice.
[00:04:50.200 --> 00:04:56.120]   Okay. When did it become obvious to you that language is the
[00:04:56.120 --> 00:04:59.480]   means to just feed a bunch of data into these things that, or
[00:04:59.480 --> 00:05:01.360]   was it just, you ran out of other things, like robotics,
[00:05:01.360 --> 00:05:03.040]   there's not enough data, this other thing, there's not enough
[00:05:03.040 --> 00:05:03.400]   data.
[00:05:03.400 --> 00:05:06.600]   Yeah. I mean, I think this whole idea of like the next word
[00:05:06.600 --> 00:05:09.600]   prediction that you could do self-supervised learning, you
[00:05:09.600 --> 00:05:12.960]   know, that together with the idea that it's like, wow, for
[00:05:12.960 --> 00:05:15.520]   predicting the next word, there's so much richness and
[00:05:15.520 --> 00:05:18.120]   structure there. Right. You know, it might say two plus two
[00:05:18.120 --> 00:05:20.920]   equals, and you have to know the answer is four. And you know,
[00:05:20.920 --> 00:05:23.400]   it might be telling the story about a character. And then
[00:05:23.400 --> 00:05:26.600]   basically, it's posing to the model, you know, the equivalent
[00:05:26.600 --> 00:05:28.880]   of these developmental tests that get posed to children, you
[00:05:28.880 --> 00:05:31.760]   know, Mary walks into the room and, you know, puts an item in
[00:05:31.760 --> 00:05:34.600]   there. And then, you know, Chuck walks into the room and removes
[00:05:34.600 --> 00:05:37.560]   the item and Mary doesn't see it. What does Mary think, you
[00:05:37.560 --> 00:05:40.600]   know, so like, so the models are going to have to get this right
[00:05:40.600 --> 00:05:43.200]   in the service of predicting the next word, they're going to
[00:05:43.200 --> 00:05:46.160]   have to solve, you know, solve all these theory of mind
[00:05:46.160 --> 00:05:49.000]   problems, solve all these math problems. And so I did, you
[00:05:49.000 --> 00:05:52.320]   know, I, my thinking was just, well, you know, scale it up as
[00:05:52.320 --> 00:05:55.280]   much as you can, you, you know, there's, there's kind of no
[00:05:55.280 --> 00:05:59.560]   limit to it. And I think I kind of had abstractly that view. But
[00:05:59.560 --> 00:06:03.280]   the thing, of course, that like really solidified and convinced
[00:06:03.280 --> 00:06:07.840]   me was the work that Alec Radford did on GPT-1, which was
[00:06:07.840 --> 00:06:10.400]   not only could you get this, this language model that could
[00:06:10.400 --> 00:06:13.640]   predict things very well, but also you could fine tune it, you
[00:06:13.640 --> 00:06:16.200]   needed to fine tune it in those days to do all these other
[00:06:16.200 --> 00:06:19.560]   tasks. And so I was like, wow, you know, it, this isn't just
[00:06:19.560 --> 00:06:22.960]   some narrow thing where you get the language model, right. It's
[00:06:22.960 --> 00:06:26.000]   sort of halfway to everywhere, right? It's like, you know, you
[00:06:26.000 --> 00:06:28.680]   get the language model, right. And then with a little move in
[00:06:28.680 --> 00:06:31.960]   this direction, it can, you know, it can solve this, this,
[00:06:31.960 --> 00:06:34.840]   you know, logical dereference test or whatever. And, you know,
[00:06:34.840 --> 00:06:37.280]   with this, this other thing, you know, it can, it can solve
[00:06:37.280 --> 00:06:39.800]   translation or something. And then you're like, wow, I think
[00:06:39.800 --> 00:06:41.680]   there's, there's really something to do it. And of
[00:06:41.680 --> 00:06:43.760]   and of course we can really scale it.

