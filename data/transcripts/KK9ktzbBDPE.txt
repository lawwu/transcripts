
[00:00:00.000 --> 00:00:05.300]   Today, I want to give you an argument about why you feel uneasy about your smartphone
[00:00:05.300 --> 00:00:08.860]   that you've likely never heard before.
[00:00:08.860 --> 00:00:12.600]   It is, however, I think an important argument to hear.
[00:00:12.600 --> 00:00:13.700]   Why?
[00:00:13.700 --> 00:00:20.860]   Because it draws from a source that far predates these modern digital technologies.
[00:00:20.860 --> 00:00:24.380]   I'm going to make you an argument about why you're uneasy about your smartphone that goes
[00:00:24.380 --> 00:00:30.980]   back to the foundational moral philosophy of Immanuel Kant.
[00:00:30.980 --> 00:00:36.420]   We're talking about a philosopher from the 1700s who is arguably the most influential
[00:00:36.420 --> 00:00:40.260]   source of moral ideas since the Bible.
[00:00:40.260 --> 00:00:47.840]   And it turns out, if you read Kant correctly, he has a lot to say about decidedly modern
[00:00:47.840 --> 00:00:50.020]   inventions such as Twitter and TikTok.
[00:00:50.020 --> 00:00:52.980]   So I'm going to ask you to stick with me here.
[00:00:52.980 --> 00:00:55.400]   We're going to get a little bit technical, but I'm going to walk you through.
[00:00:55.400 --> 00:00:56.580]   We're not going to get too technical.
[00:00:56.580 --> 00:00:59.260]   All of these ideas will be accessible, and we're going to come out on the other end of
[00:00:59.260 --> 00:01:06.040]   this exploration with a better understanding of the role technology plays in your life
[00:01:06.040 --> 00:01:09.660]   right now, why that makes you uneasy, and changes you can make.
[00:01:09.660 --> 00:01:12.060]   So I think it's a cool thing to add to the argument.
[00:01:12.060 --> 00:01:18.660]   All right, so I'm going to be drawing today entirely from a single academic paper from
[00:01:18.660 --> 00:01:19.660]   2021.
[00:01:19.660 --> 00:01:22.540]   I'll pull it up on the screen here.
[00:01:22.540 --> 00:01:27.620]   I have a preprint version here that I could access, which has a link to the official final
[00:01:27.620 --> 00:01:28.620]   version.
[00:01:28.620 --> 00:01:33.420]   I'll put it on the screen here for people who are watching instead of just listening.
[00:01:33.420 --> 00:01:41.420]   The paper we're going to be drawing from is titled, Is There a Duty to Be a Digital Minimalist?
[00:01:41.420 --> 00:01:48.180]   This was published in the Journal of Applied Philosophy back in the summer of 2021.
[00:01:48.180 --> 00:01:51.380]   For those of you who are tracking at home, that's volume 38, number four.
[00:01:51.380 --> 00:01:55.700]   The authors are Timothy Aylsworth and Clinton Castro.
[00:01:55.700 --> 00:01:59.220]   These are philosophers from Florida International University.
[00:01:59.220 --> 00:02:04.900]   All right, so I'm going to jump through this paper somewhat selectively to pull out what
[00:02:04.900 --> 00:02:08.340]   I think the important parts are.
[00:02:08.340 --> 00:02:11.700]   So I'm actually going to start by jumping ahead here a little bit.
[00:02:11.700 --> 00:02:16.380]   I'll sort of keep up with this on the screen, I suppose, for those who are watching.
[00:02:16.380 --> 00:02:19.300]   But I'm going to read everything, so don't worry if you're just listening.
[00:02:19.300 --> 00:02:22.920]   Okay, so I'm going to jump ahead here.
[00:02:22.920 --> 00:02:25.980]   They're talking here about digital minimalism.
[00:02:25.980 --> 00:02:29.100]   Let me read from the paper.
[00:02:29.100 --> 00:02:33.780]   Authors like Cal Newport, who coined the term digital minimalism, argue that we would be
[00:02:33.780 --> 00:02:38.620]   better off if we restructured our relationships with technology on our own terms.
[00:02:38.620 --> 00:02:42.980]   He understands digital minimalism as, quote, a philosophy of technology use in which you
[00:02:42.980 --> 00:02:48.420]   focus your online time on a small number of carefully selected and optimized activities
[00:02:48.420 --> 00:02:53.540]   that strongly support things you value, and then happily miss out on everything else.
[00:02:53.540 --> 00:02:57.820]   A philosophy of technology use is a personal philosophy that covers which digital tools
[00:02:57.820 --> 00:03:04.140]   we allow into our lives, for what reasons, and under what constraints.
[00:03:04.140 --> 00:03:08.900]   Newport's definition outlines a noble ideal, but we are happy to adopt a less demanding
[00:03:08.900 --> 00:03:12.980]   understanding of this notion, and kind of jumping ahead here a little bit.
[00:03:12.980 --> 00:03:16.840]   We understand a digital minimalist as one whose interactions with digital technology
[00:03:16.840 --> 00:03:21.180]   are intentional, such that they do not conflict with their ends.
[00:03:21.180 --> 00:03:26.580]   For most, being a minimalist will involve a serious reduction, in some cases to the
[00:03:26.580 --> 00:03:31.460]   point of elimination, of interactions with smartphones, smartphone apps, and social media
[00:03:31.460 --> 00:03:32.460]   sites.
[00:03:32.460 --> 00:03:36.100]   For some, it may even require living up to Newport's ideal.
[00:03:36.100 --> 00:03:39.980]   I'm going to jump ahead one more time here.
[00:03:39.980 --> 00:03:43.900]   Newport may very well be right that we have prudential reasons to reduce our smartphone
[00:03:43.900 --> 00:03:44.900]   usage.
[00:03:44.900 --> 00:03:49.020]   Perhaps most people would be better off if they became digital minimalists.
[00:03:49.020 --> 00:03:54.420]   But if the Kantian argument that follows is sound, then we might have even more compelling
[00:03:54.420 --> 00:03:56.860]   reasons to adopt the end of digital minimalism.
[00:03:56.860 --> 00:03:59.180]   We may have moral reasons.
[00:03:59.180 --> 00:04:02.980]   All right, so let's make sense of what just happened there.
[00:04:02.980 --> 00:04:05.820]   They introduced my idea of digital minimalism.
[00:04:05.820 --> 00:04:08.220]   They simplify it a little bit to make it a little bit more general.
[00:04:08.220 --> 00:04:12.940]   But they say, basically, yes, this idea that Newport introduced is one of being very intentional
[00:04:12.940 --> 00:04:18.340]   about how you use your technology so that it supports instead of impeding what you value.
[00:04:18.340 --> 00:04:21.020]   That is the core idea of my book, Digital Minimalism.
[00:04:21.020 --> 00:04:24.180]   It's at the core of my personal technology philosophy.
[00:04:24.180 --> 00:04:29.060]   The key thing they say, this is setting up the argument that we're going to explore,
[00:04:29.060 --> 00:04:37.340]   is they say, look, Cal in his book has what they call prudential reasons for why you should
[00:04:37.340 --> 00:04:38.340]   be a digital minimalist.
[00:04:38.340 --> 00:04:39.340]   What they mean is like practical reasons.
[00:04:39.340 --> 00:04:45.140]   I go through, like, hey, when you let technology get in the way of your values, there's like
[00:04:45.140 --> 00:04:48.580]   all this stuff that you don't do that you would otherwise like to do, and I think you're
[00:04:48.580 --> 00:04:49.580]   going to like your life better.
[00:04:49.580 --> 00:04:54.380]   I'm a sort of pragmatic, practical, direct argument to your experience and intuition.
[00:04:54.380 --> 00:04:59.460]   They're saying, yes, that might all be true, but we are going to make an argument that
[00:04:59.460 --> 00:05:05.160]   draws from Kant that says there is also a moral reason, that we can draw from moral
[00:05:05.160 --> 00:05:12.320]   philosophy that says whether you want to or not, you are obligated to be a digital minimalist.
[00:05:12.320 --> 00:05:17.620]   That now is the argument that is made in this paper that we are going to draw out, a moral
[00:05:17.620 --> 00:05:19.340]   argument for being a digital minimalist.
[00:05:19.340 --> 00:05:20.340]   All right.
[00:05:20.340 --> 00:05:23.420]   So I'm going to jump back to the beginning here.
[00:05:23.420 --> 00:05:27.940]   They set up a quick example, which they use to explore some of the issues with modern
[00:05:27.940 --> 00:05:28.940]   technology.
[00:05:28.940 --> 00:05:32.580]   So they begin by drawing from a quote from a comedian.
[00:05:32.580 --> 00:05:34.620]   So let me read this to you.
[00:05:34.620 --> 00:05:35.620]   I wish I could read.
[00:05:35.620 --> 00:05:40.220]   I really do, says comedian Esther Poviditsky.
[00:05:40.220 --> 00:05:41.540]   I try to read.
[00:05:41.540 --> 00:05:42.540]   I buy books.
[00:05:42.540 --> 00:05:47.260]   I open books, and then I black out, and I'm on Instagram, and I don't know what happened.
[00:05:47.260 --> 00:05:49.240]   To many of us, this is a familiar occurrence.
[00:05:49.240 --> 00:05:54.420]   All too often, we set out to complete a task, but we are interrupted and subsequently derailed
[00:05:54.420 --> 00:05:59.380]   by our wireless mobile devices.
[00:05:59.380 --> 00:06:05.860]   Incidents of this kind might involve a moral failure, for insofar as we are morally required
[00:06:05.860 --> 00:06:12.520]   to cultivate and protect our autonomy, we fail to meet this requirement by falling prey
[00:06:12.520 --> 00:06:14.140]   to mobile phone addiction.
[00:06:14.140 --> 00:06:15.140]   Okay.
[00:06:15.140 --> 00:06:22.600]   So the first link in the chain they're going to make in this moral argument is that our
[00:06:22.600 --> 00:06:27.340]   issues with smartphone usage, as captured by this anecdote of this comedian saying,
[00:06:27.340 --> 00:06:29.800]   "I try to read, but I can't.
[00:06:29.800 --> 00:06:34.000]   I'm going to read this book, but I end up on Instagram," they say this could be seen
[00:06:34.000 --> 00:06:40.120]   as impacting our autonomy, and if it impacts our autonomy, we might be able to find a moral
[00:06:40.120 --> 00:06:41.480]   reason why this is bad.
[00:06:41.480 --> 00:06:47.580]   First, however, they have to establish, is the way we use things like smartphones actually
[00:06:47.580 --> 00:06:51.120]   affecting our autonomy, and what they do here is they go through three different ways that
[00:06:51.120 --> 00:06:54.700]   other thinkers have thought about autonomy, and for each of these says it's basically
[00:06:54.700 --> 00:06:58.640]   self-evident that the behavior we're thinking about, the behavior we observed in that comedian
[00:06:58.640 --> 00:07:03.460]   Povedisky, is violating these definitions of autonomy.
[00:07:03.460 --> 00:07:04.460]   All right.
[00:07:04.460 --> 00:07:05.940]   So let's go through these real quick.
[00:07:05.940 --> 00:07:11.260]   They say, "In order to substantiate the claim that smartphone addiction undermines autonomy,
[00:07:11.260 --> 00:07:16.220]   we must say more about the concept at issue.
[00:07:16.220 --> 00:07:20.200]   Personal autonomy has been defined in a variety of ways, but we believe that a minimal definition
[00:07:20.200 --> 00:07:24.260]   of self-governance is sufficient for our purposes here."
[00:07:24.260 --> 00:07:28.180]   So they're going to go through some examples here of definitions of autonomy.
[00:07:28.180 --> 00:07:32.940]   They say, "Let's return to Povedisky's case from the beginning.
[00:07:32.940 --> 00:07:40.480]   According to what they call the Frankfurt Dworkin model, Povedisky's first order desire
[00:07:40.480 --> 00:07:47.420]   to check Instagram while reading is inconsistent with her higher order desire, i.e., to want
[00:07:47.420 --> 00:07:48.420]   to read."
[00:07:48.420 --> 00:07:54.100]   All right, so the Frankfurt Dworkin model talks about first order and higher order desires,
[00:07:54.100 --> 00:07:57.940]   the things that actually is directing your activities right now versus what you want
[00:07:57.940 --> 00:08:02.580]   to be the case, and if these are out of sync, you have an autonomy problem.
[00:08:02.580 --> 00:08:06.940]   So like by that model, looking at Instagram when you want to read is an autonomy issue
[00:08:06.940 --> 00:08:10.660]   because your higher order desire is, "I want to read," but your first order desire that's
[00:08:10.660 --> 00:08:13.380]   actually directing your activities is looking at Instagram.
[00:08:13.380 --> 00:08:16.420]   All right, here's another model due to Watson.
[00:08:16.420 --> 00:08:22.840]   On Watson's characterization, what is distinctive about compulsive behavior is that the desires
[00:08:22.840 --> 00:08:27.100]   and emotions and questions are more or less radically independent of the evaluational
[00:08:27.100 --> 00:08:29.500]   systems of these agents.
[00:08:29.500 --> 00:08:32.660]   Povedisky's smartphone use is inconsistent with her evaluative judgments about what she
[00:08:32.660 --> 00:08:34.900]   ought to be doing, and the behavior is compulsive.
[00:08:34.900 --> 00:08:40.780]   All right, so Watson says, "If you're doing something that you would evaluate to be not
[00:08:40.780 --> 00:08:45.300]   good or less good than something else, then the behavior must be compulsive."
[00:08:45.300 --> 00:08:52.700]   By the way, that connects to the way psychologists think about behavioral addiction, the persistence
[00:08:52.700 --> 00:08:57.060]   in an activity even though it's, you know, it's not valuable or it's in the way of things
[00:08:57.060 --> 00:08:58.220]   you know to be more valuable.
[00:08:58.220 --> 00:09:02.180]   Finally, they have a model of autonomy due to Bratman.
[00:09:02.180 --> 00:09:08.660]   Bratman defends a model of autonomy that requires harmony between what the agent does and her
[00:09:08.660 --> 00:09:11.780]   more or less long-term plans.
[00:09:11.780 --> 00:09:15.180]   Surely Povedisky's behavior fails on this count as well.
[00:09:15.180 --> 00:09:18.220]   We can suppose that Povedisky, like many of us, would like to read many books over the
[00:09:18.220 --> 00:09:22.440]   course of her life and to develop a disposition of being able to sit and enjoy reading for
[00:09:22.440 --> 00:09:24.140]   long stretches.
[00:09:24.140 --> 00:09:28.300]   The action of looking at her phone compulsively is not consistent with her long-term plans.
[00:09:28.300 --> 00:09:33.020]   All right, so to summarize, no matter which model one adopts, the result is likely to
[00:09:33.020 --> 00:09:34.020]   be the same.
[00:09:34.020 --> 00:09:38.340]   Povedisky is not autonomous with respect to her smartphone usage.
[00:09:38.340 --> 00:09:43.860]   All right, so we've established this first link in our argumentative chain.
[00:09:43.860 --> 00:09:49.800]   The way we use smartphones today seems to be hurting our autonomy.
[00:09:49.800 --> 00:09:55.300]   We can look at several official definitions of autonomy and see that smartphone usage
[00:09:55.300 --> 00:10:00.700]   of the type that we think of, the type in that example, is breaking those models.
[00:10:00.700 --> 00:10:01.700]   Hey, it's Cal.
[00:10:01.700 --> 00:10:06.540]   I wanted to interrupt briefly to say that if you're enjoying this video, then you need
[00:10:06.540 --> 00:10:14.020]   to check out my new book, Slow Productivity, The Lost Art of Accomplishment Without Burnout.
[00:10:14.020 --> 00:10:19.460]   This is like the Bible for most of the ideas we talk about here in these videos.
[00:10:19.460 --> 00:10:24.820]   You can get a free excerpt at calnewport.com/slow.
[00:10:24.820 --> 00:10:26.620]   I know you're going to like it.
[00:10:26.620 --> 00:10:27.620]   Check it out.
[00:10:27.620 --> 00:10:29.580]   Now let's get back to the video.
[00:10:29.580 --> 00:10:33.780]   Okay, so why is that bad?
[00:10:33.780 --> 00:10:37.780]   This is the next link in their moral argument chain.
[00:10:37.780 --> 00:10:43.100]   This is where we turn our attention to Immanuel Kant.
[00:10:43.100 --> 00:10:48.980]   Although some ethicists reject the very notion of duties to oneself, Kant makes them a central
[00:10:48.980 --> 00:10:50.780]   component of his moral theory.
[00:10:50.780 --> 00:10:55.220]   In fact, I'll turn to this in the article here.
[00:10:55.220 --> 00:10:58.780]   For those who are watching along at home, I'm just scrolling.
[00:10:58.780 --> 00:11:03.000]   This is section three.
[00:11:03.000 --> 00:11:07.780]   He says that they take first place and are the most important of all.
[00:11:07.780 --> 00:11:12.020]   He goes so far as to suggest that duties to oneself are the foundation of duties to others,
[00:11:12.020 --> 00:11:16.380]   making them the precondition of all moral duties.
[00:11:16.380 --> 00:11:20.740]   But he worries that they have not been properly understood and claims that no part of morals
[00:11:20.740 --> 00:11:24.980]   has been more defectively treated than this of the duties to oneself.
[00:11:24.980 --> 00:11:28.560]   He thinks that they have been misunderstood as a mere elevation of self-interest, a duty
[00:11:28.560 --> 00:11:32.740]   to promote one's own happiness, which he dismisses as an absurdity.
[00:11:32.740 --> 00:11:38.460]   Rather than grounding such duties in egoism, Kant argues that humanity, i.e. rational nature,
[00:11:38.460 --> 00:11:44.540]   has an absolute inherent value, and this generates self-regarding obligations insofar as the agent
[00:11:44.540 --> 00:11:47.900]   is morally required to respect humanity in her own person.
[00:11:47.900 --> 00:11:51.780]   Thus duties to oneself are derived from the humanity formulation of the categorical imperative,
[00:11:51.780 --> 00:11:57.660]   which tells us that we must always treat humanity, even in our own person, as an end, never merely
[00:11:57.660 --> 00:11:58.660]   as a means.
[00:11:58.660 --> 00:12:05.740]   All right, so Kant is arguing we have a duty to ourselves as much as we have a duty to
[00:12:05.740 --> 00:12:13.700]   other people, and we have a moral duty in particular for self-governing what is most
[00:12:13.700 --> 00:12:15.860]   important about our humanity.
[00:12:15.860 --> 00:12:17.940]   Okay, so what is that?
[00:12:17.940 --> 00:12:20.140]   Jump ahead briefly.
[00:12:20.140 --> 00:12:23.860]   Kant famously claims that human beings, in virtue of their rational agency, have a uniquely
[00:12:23.860 --> 00:12:28.340]   elevated status which he calls "dignity."
[00:12:28.340 --> 00:12:33.220]   So the idea that we are rational beings and no other creatures or objects are gives us
[00:12:33.220 --> 00:12:39.800]   this sort of special value, and preserving what he calls the dignity of ourselves as
[00:12:39.800 --> 00:12:44.180]   rational beings is sort of the highest good.
[00:12:44.180 --> 00:12:49.860]   We have an obligation to help protect this in ourselves and others.
[00:12:49.860 --> 00:12:54.820]   Moving on here, on this view, our actions can either express or fail to express the
[00:12:54.820 --> 00:13:01.020]   kind of respect that is becoming of human dignity.
[00:13:01.020 --> 00:13:08.980]   So he's saying our actions need to be focused on respecting our human dignity, and our human
[00:13:08.980 --> 00:13:11.220]   dignity is based on the idea that we are rational beings.
[00:13:11.220 --> 00:13:14.020]   All right, so we're really in the weeds here, Jesse.
[00:13:14.020 --> 00:13:18.420]   We're deep in moral reasoning, but out of this and a bunch of other words that I'm kind
[00:13:18.420 --> 00:13:26.100]   of skipping, we get to an actual argument form here, all right?
[00:13:26.100 --> 00:13:32.140]   So they end up with three propositions that lead to a conclusion.
[00:13:32.140 --> 00:13:33.980]   Humanity is proposition one.
[00:13:33.980 --> 00:13:38.620]   Humanity, i.e. rational agency, has an objective, unconditional, non-fungible value, which is
[00:13:38.620 --> 00:13:39.980]   dignity.
[00:13:39.980 --> 00:13:44.900]   Proposition two, anything that has dignity ought to be respected as an end and never
[00:13:44.900 --> 00:13:48.780]   treated as a mere means.
[00:13:48.780 --> 00:13:53.020]   Proposition three, if humanity ought to be respected as an end and never treated as a
[00:13:53.020 --> 00:13:58.820]   mere means, then we have an imperfect duty to cultivate and protect our rational agency.
[00:13:58.820 --> 00:14:05.500]   Therefore, the conclusion of these three propositions, we have an imperfect duty to cultivate and
[00:14:05.500 --> 00:14:06.700]   protect our rational agency.
[00:14:06.700 --> 00:14:14.820]   All right, we're getting in the weeds here, logical philosophy, morality all pulled together.
[00:14:14.820 --> 00:14:20.120]   They then put these together, they get to their core argument.
[00:14:20.120 --> 00:14:24.140]   We have an imperfect duty to cultivate and protect our rational agency.
[00:14:24.140 --> 00:14:27.340]   If we have an imperfect duty to cultivate and protect our rational agency, then we ought
[00:14:27.340 --> 00:14:33.460]   to adopt the end of digital minimalism, therefore we ought to adopt the end of digital minimalism.
[00:14:33.460 --> 00:14:38.100]   If you take my discrete mathematics course at Georgetown, where we study propositional
[00:14:38.100 --> 00:14:44.140]   logic, you will actually recognize this argument and could probably turn it into the corresponding
[00:14:44.140 --> 00:14:46.460]   argument form.
[00:14:46.460 --> 00:14:52.060]   So basically we have just done a lot of reasoning based on Kant's ideas that end up with the
[00:14:52.060 --> 00:14:59.680]   conclusion we ought to adopt the end of digital minimalism.
[00:14:59.680 --> 00:15:08.060]   If we simplify all of this, we're basically saying it is important to respect our own
[00:15:08.060 --> 00:15:11.660]   dignity as rational beings.
[00:15:11.660 --> 00:15:18.940]   The way that we use smartphones when we're unintentional robs us of our ability to do
[00:15:18.940 --> 00:15:25.260]   this because it robs us of autonomy, and autonomy is at the core of respecting the dignity of
[00:15:25.260 --> 00:15:28.580]   being a rational being because at the core of being a rational being is the ability to
[00:15:28.580 --> 00:15:32.620]   make decisions about what you do rationally.
[00:15:32.620 --> 00:15:40.020]   The Kantian framework is seeing this core tension between we need to respect the fact
[00:15:40.020 --> 00:15:44.420]   that we are rational beings and smartphones take away our ability to make rational decisions
[00:15:44.420 --> 00:15:52.660]   about what we want to do with our life and our time, therefore an approach to life that
[00:15:52.660 --> 00:15:59.160]   reduces smartphone's ability of taking away our autonomy is justified.
[00:15:59.160 --> 00:16:04.180]   Digital minimalism is sort of the definition of such a life.
[00:16:04.180 --> 00:16:05.660]   It's an approach to digital technology.
[00:16:05.660 --> 00:16:09.240]   It says we want to be intentional, not be robbed of our autonomy, therefore there's
[00:16:09.240 --> 00:16:13.220]   a sort of fundamental Kantian moral argument that we should be very intentional about our
[00:16:13.220 --> 00:16:19.100]   technology use using something like digital minimalism.
[00:16:19.100 --> 00:16:24.100]   Let me read the conclusion of this paper because I think they sum this up very nicely.
[00:16:24.100 --> 00:16:26.100]   They're all lit up here on the screen for those who are watching at home.
[00:16:26.100 --> 00:16:30.260]   All right, so here's the conclusion of the authors in this paper.
[00:16:30.260 --> 00:16:33.940]   We have argued that there is a moral obligation to be intentional about our use of smartphones
[00:16:33.940 --> 00:16:36.420]   and other addictive devices.
[00:16:36.420 --> 00:16:39.780]   We have this duty because we are required to protect the most valuable commodity we
[00:16:39.780 --> 00:16:42.820]   possess, autonomy.
[00:16:42.820 --> 00:16:46.260]   Kant believes that the proper exercise of our autonomy is the only thing that is good
[00:16:46.260 --> 00:16:52.860]   without qualification, something that shines like a jewel having its full worth in itself.
[00:16:52.860 --> 00:16:59.980]   To wantonly forfeit some of our agency by falling prey, the technological heteronomy
[00:16:59.980 --> 00:17:03.980]   is to demonstrate a failure to respect this precious capacity as the treasure that it
[00:17:03.980 --> 00:17:04.980]   is.
[00:17:04.980 --> 00:17:13.140]   All right, so why are we geeking out on, so it's like a technical academic argument for
[00:17:13.140 --> 00:17:17.620]   the type of things we talk about here on the show, and actually not just the type of things,
[00:17:17.620 --> 00:17:19.940]   specifically what we talk about on the show because, of course, they're talking about
[00:17:19.940 --> 00:17:23.940]   my specific digital minimalism philosophy.
[00:17:23.940 --> 00:17:31.060]   It's because I think it is easy in thinking about technology and human flourishing to
[00:17:31.060 --> 00:17:38.020]   fall back on arguments such as, look, kids these days, for example, they're always using
[00:17:38.020 --> 00:17:43.660]   different technology, we get worried about it, but that's just the wheels of progress.
[00:17:43.660 --> 00:17:49.300]   Or we fall back on an argument that says every new technology creates moral panics, right?
[00:17:49.300 --> 00:17:50.600]   And then we get over it.
[00:17:50.600 --> 00:17:52.660]   We worried about the car, but now we just drive cars.
[00:17:52.660 --> 00:17:56.300]   We worried about TV, now we don't worry about TV as much.
[00:17:56.300 --> 00:18:01.180]   It's easy to fall back on these arguments of status quo thinking.
[00:18:01.180 --> 00:18:07.340]   What's critical about this particular argument is it says, no, there's justifications for
[00:18:07.340 --> 00:18:10.700]   our concerns about these technologies that are much more fundamental than thinking about
[00:18:10.700 --> 00:18:13.140]   specific technologies.
[00:18:13.140 --> 00:18:18.180]   Our uneasiness about these technologies is not just a naive reaction to the latest techno
[00:18:18.180 --> 00:18:24.360]   disruption and a long line of techno disruptions that ultimately end up being not so bad.
[00:18:24.360 --> 00:18:32.060]   We are actually reflecting a specific harm, denial of autonomy, and that we can go back
[00:18:32.060 --> 00:18:37.700]   to Kanter before to see that this is at the core of the human experience, is at the core
[00:18:37.700 --> 00:18:39.900]   of what we value as humans.
[00:18:39.900 --> 00:18:44.140]   And so yes, we're uneasy, not because we're naive, we're uneasy because something basic
[00:18:44.140 --> 00:18:48.280]   to our humanity is at stake.
[00:18:48.280 --> 00:18:53.300]   So this Kantian argument is pointing towards the exceptional nature of the issue we face
[00:18:53.300 --> 00:18:55.500]   with things like smartphones.
[00:18:55.500 --> 00:18:59.980]   We cannot just ontologically speaking put in the same category as like any other type
[00:18:59.980 --> 00:19:01.320]   of techno fear.
[00:19:01.320 --> 00:19:06.420]   It is a specific technical fear which requires analysis on its own terms and when we do that,
[00:19:06.420 --> 00:19:12.180]   we see there are specific harms here that cannot be ignored.
[00:19:12.180 --> 00:19:17.420]   So if someone's giving you a lot of trouble about your digital minimalism, if they're
[00:19:17.420 --> 00:19:21.820]   making fun of you or if they're trying to self-justify their own heavy phone use, you
[00:19:21.820 --> 00:19:27.500]   can now throw a lot of sort of annoying technical philosophical terms at them.
[00:19:27.500 --> 00:19:34.240]   You could say things like heteronomy and ontological.
[00:19:34.240 --> 00:19:41.260]   You can mention the categorical imperative, keep dropping the word Kantian and they will
[00:19:41.260 --> 00:19:42.260]   just have to be quiet.
[00:19:42.260 --> 00:19:43.260]   All right.
[00:19:43.260 --> 00:19:44.260]   So there we go.
[00:19:44.260 --> 00:19:49.660]   A nerd argument for a very real issue.
[00:19:49.660 --> 00:19:54.000]   So I actually found that article, Jesse, because there's a follow-up article that said, all
[00:19:54.000 --> 00:20:00.360]   right, if that's true, there's also an obligation others have to protect your autonomy through
[00:20:00.360 --> 00:20:01.960]   digital technology as well.
[00:20:01.960 --> 00:20:04.200]   That there's like a moral imperative not to distract other people.
[00:20:04.200 --> 00:20:05.760]   It's a whole interesting argument.
[00:20:05.760 --> 00:20:09.060]   All right, but enough of that nerdiness.
[00:20:09.060 --> 00:20:11.120]   We got some good questions.
[00:20:11.120 --> 00:20:14.240]   But first, let's hear from a sponsor.
[00:20:14.240 --> 00:20:21.560]   All right, let's talk about our longtime friends at Element.
[00:20:21.560 --> 00:20:26.000]   Element helps anyone stay hydrated without the sugar and other dodgy ingredients found
[00:20:26.000 --> 00:20:30.300]   in popular electrolyte and sports drinks.
[00:20:30.300 --> 00:20:32.640]   It's important to have the right electrolytes, but you don't want sugar.
[00:20:32.640 --> 00:20:33.640]   You don't want junk.
[00:20:33.640 --> 00:20:36.340]   That's why I love Element.
[00:20:36.340 --> 00:20:42.000]   It is a zero sugar electrolyte drink mix and sparkling electrolyte water.
[00:20:42.000 --> 00:20:43.360]   So there's two products here.
[00:20:43.360 --> 00:20:49.760]   The mix you add to your own water or the already canned sparkling water with the electrolyte
[00:20:49.760 --> 00:20:52.720]   mix in it, which you can just grab out of the fridge.
[00:20:52.720 --> 00:20:56.000]   They're born out of a growing body of research revealing that optimal health outcomes occurs
[00:20:56.000 --> 00:21:00.640]   at sodium levels two to three times government recommendations.
[00:21:00.640 --> 00:21:07.960]   So each stick of powder or can of sparkling water delivers a meaningful dose of electrolytes
[00:21:07.960 --> 00:21:11.560]   free of sugar, artificial colors, or other dodgy ingredients.
[00:21:11.560 --> 00:21:14.520]   It's incredibly popular among many different communities.
[00:21:14.520 --> 00:21:17.880]   I use Element almost every day.
[00:21:17.880 --> 00:21:22.920]   Anytime that I feel like I'm dehydrated a little bit, so from like after exercising,
[00:21:22.920 --> 00:21:28.360]   after podcasting, you lose a lot of moisture when you talk, after a long day of lecturing,
[00:21:28.360 --> 00:21:32.880]   I add the drink mix to a big Nalgene full of water.
[00:21:32.880 --> 00:21:33.880]   I drink it every day.
[00:21:33.880 --> 00:21:38.200]   We've got a big box of it because I love that it replaces electrolytes, but it doesn't have
[00:21:38.200 --> 00:21:39.320]   all that stuff I don't want.
[00:21:39.320 --> 00:21:40.320]   It's not the sugar.
[00:21:40.320 --> 00:21:44.480]   It's not the weird dodgy stuff.
[00:21:44.480 --> 00:21:48.240]   There's something else they're offering now to think about this winter.
[00:21:48.240 --> 00:21:51.400]   That is enjoying your Element hot.
[00:21:51.400 --> 00:21:55.600]   So the limited time Element chocolate medley, including the flavors chocolate mint, chocolate
[00:21:55.600 --> 00:22:00.400]   chai, and chocolate raspberry, you could serve those in hot liquid.
[00:22:00.400 --> 00:22:05.560]   So get your electrolyte mix in a sort of refreshing hot liquid after you're running outside or
[00:22:05.560 --> 00:22:06.560]   shoveling snow.
[00:22:06.560 --> 00:22:13.320]   I think that's a fun addition to the Element family for the winter.
[00:22:13.320 --> 00:22:15.520]   Now I have good news.
[00:22:15.520 --> 00:22:21.440]   Members of my community can receive a free Element sample pack with any order when they
[00:22:21.440 --> 00:22:24.440]   purchase through drinkelement.com/deep.
[00:22:24.440 --> 00:22:28.300]   That's drinkelementlmnt.com/deep.
[00:22:28.300 --> 00:22:30.200]   Keep in mind you can try Element totally risk-free.
[00:22:30.200 --> 00:22:32.960]   If you don't like it, give it away to a salty friend and we'll give you your money back.
[00:22:32.960 --> 00:22:33.960]   No questions asked.
[00:22:33.960 --> 00:22:38.840]   They have a very low return rate and very high reorder rate because it is fantastic.
[00:22:38.840 --> 00:22:44.320]   So go to drinkelement.com/deep and you will get a free sample pack with any order.
[00:22:44.320 --> 00:22:47.760]   I also want to talk about our friends at Mint Mobile.
[00:22:47.760 --> 00:22:48.760]   Here's the thing.
[00:22:48.760 --> 00:22:53.380]   I love a great deal as much as the next guy, but I'm not going to crawl through a bed of
[00:22:53.380 --> 00:22:55.120]   hot coals just to save a few bucks.
[00:22:55.120 --> 00:22:57.020]   It has to be easy.
[00:22:57.020 --> 00:22:58.740]   No hoops, no BS.
[00:22:58.740 --> 00:23:02.340]   So when Mint Mobile said it was easy to get wireless for $15 a month with the purchase
[00:23:02.340 --> 00:23:06.400]   of a three-month plan, I called them on it, looked into it.
[00:23:06.400 --> 00:23:10.460]   Turns out it really is that easy to get wireless for $15 a month.
[00:23:10.460 --> 00:23:14.520]   The hardest part of the process is just the time you're going to spend breaking up with
[00:23:14.520 --> 00:23:17.760]   your old cell phone provider.
[00:23:17.760 --> 00:23:22.620]   I have been recommending Mint Mobile to a lot of people recently because my son is in
[00:23:22.620 --> 00:23:23.620]   middle school.
[00:23:23.620 --> 00:23:25.100]   We know a lot of other middle school parents.
[00:23:25.100 --> 00:23:29.880]   They're going through this thought process of my kid needs a phone for X, Y, or Z, taking
[00:23:29.880 --> 00:23:31.800]   the public transportation, getting picked up from practice.
[00:23:31.800 --> 00:23:32.800]   I don't want to give him a smartphone.
[00:23:32.800 --> 00:23:33.800]   What do we do?
[00:23:33.800 --> 00:23:39.040]   I've been telling them just, hey, buy a flip phone on Amazon and give them the $15 a month
[00:23:39.040 --> 00:23:40.120]   plan for Mint Mobile.
[00:23:40.120 --> 00:23:43.600]   It's cheap, it's easy, and they'll have access.
[00:23:43.600 --> 00:23:48.480]   So I've been telling more and more people use Mint Mobile to quickly set up a dumb phone
[00:23:48.480 --> 00:23:49.480]   for their kids.
[00:23:49.480 --> 00:23:53.740]   So if you want to get started, go to mintmobile.com/deep.
[00:23:53.740 --> 00:23:56.840]   There you'll see that right now all three-month plans are only $15 a month, including the
[00:23:56.840 --> 00:23:58.360]   unlimited plan.
[00:23:58.360 --> 00:24:03.240]   All plans come with high-speed data and unlimited talk and text delivered on the nation's largest
[00:24:03.240 --> 00:24:04.640]   5G network.
[00:24:04.640 --> 00:24:08.960]   You can use your own phone, or a phone you just buy on Amazon if you want a dumb phone,
[00:24:08.960 --> 00:24:12.480]   whatever you want to do, with any Mint Mobile plan and bring your phone number along with
[00:24:12.480 --> 00:24:13.920]   all your existing contacts.
[00:24:13.920 --> 00:24:17.180]   Find out how easy it is to switch to Mint Mobile and get three months of premium wireless
[00:24:17.180 --> 00:24:21.160]   for just $15 a month.
[00:24:21.160 --> 00:24:24.840]   To get this new customer offer and your new three-month premium wireless plan for just
[00:24:24.840 --> 00:24:28.240]   $15 a month, go to mintmobile.com/deep.
[00:24:28.240 --> 00:24:30.240]   That's mintmobile.com/deep.
[00:24:30.240 --> 00:24:34.120]   Check out your wireless bill to $15 a month at mintmobile.com/deep.
[00:24:34.120 --> 00:24:38.640]   $45 upfront payment required, which is equivalent to $15 a month.
[00:24:38.640 --> 00:24:40.840]   New customers on first three-month plan only.
[00:24:40.840 --> 00:24:43.080]   Speed slower by 40 gigabytes in unlimited plan.
[00:24:43.080 --> 00:24:44.920]   Additional taxes, fees, or restrictions apply.
[00:24:44.920 --> 00:24:46.160]   See Mint Mobile for detail.
[00:24:46.160 --> 00:24:50.880]   All right, Jesse, let's do some questions.
[00:24:50.880 --> 00:24:53.400]   First question's from Kirsten from Missouri.
[00:24:53.400 --> 00:24:57.040]   "I just ordered digital minimalism to help me stop wasting time.
[00:24:57.040 --> 00:25:00.760]   I tried, without success, to use some distraction-free apps.
[00:25:00.760 --> 00:25:02.480]   Can these apps actually work?"
[00:25:02.480 --> 00:25:07.560]   Well, let me back up a little bit, because it looks like you're thinking about digital
[00:25:07.560 --> 00:25:17.440]   minimalism as a collection of tips for trying to stop wasting time or to be less distracted.
[00:25:17.440 --> 00:25:21.000]   That's the standard cultural paradigm we often have for thinking about advice, especially
[00:25:21.000 --> 00:25:23.200]   for things like digital distraction.
[00:25:23.200 --> 00:25:24.600]   Give me some tips.
[00:25:24.600 --> 00:25:28.520]   I want to read the five ways to save your attention in some sort of magazine article,
[00:25:28.520 --> 00:25:30.880]   and let me put a couple of those into play.
[00:25:30.880 --> 00:25:33.120]   It looks like you heard about distraction-free apps.
[00:25:33.120 --> 00:25:34.280]   I do talk about them in the book.
[00:25:34.280 --> 00:25:35.280]   You're like, "Hey, I tried them.
[00:25:35.280 --> 00:25:37.240]   It didn't work well."
[00:25:37.240 --> 00:25:40.840]   The first thing I want you to understand is that digital minimalism is not a collection
[00:25:40.840 --> 00:25:43.040]   of tips, but a philosophy.
[00:25:43.040 --> 00:25:48.440]   It's a philosophy of technology use, a consistent way of thinking about the role of technology
[00:25:48.440 --> 00:25:52.520]   in your life and how you curate and engage with technologies in your life.
[00:25:52.520 --> 00:25:54.800]   It's a bit of a binary proposition.
[00:25:54.800 --> 00:25:59.620]   You either need to adopt the philosophy or not.
[00:25:59.620 --> 00:26:05.440]   You can't just pick and choose specific things to show up in the philosophy itself.
[00:26:05.440 --> 00:26:10.720]   The metaphor I like to use is cleaning out a closet that's overstuffed with junk.
[00:26:10.720 --> 00:26:15.800]   So if you had a closet that's overstuffed with junk, what you are doing—so the equivalent
[00:26:15.800 --> 00:26:19.480]   in the closet metaphor of like, "Hey, I'm going to try to help my online distraction
[00:26:19.480 --> 00:26:23.480]   because I heard apps might help"—that's the equivalent in our closet metaphor of going
[00:26:23.480 --> 00:26:27.560]   to the container store and being like, "Hey, I bought some organizers."
[00:26:27.560 --> 00:26:31.040]   And then you return to your overstuffed closet and you're like, "Okay, I have a few organizer
[00:26:31.040 --> 00:26:32.800]   bins here that I put some of the stuff in."
[00:26:32.800 --> 00:26:35.640]   Your closet's still a nightmare.
[00:26:35.640 --> 00:26:38.160]   What works better for the closet?
[00:26:38.160 --> 00:26:42.560]   Well the Marie Kondo approach of, "I'm going to empty the whole thing out, empty it to
[00:26:42.560 --> 00:26:43.560]   zero.
[00:26:43.560 --> 00:26:45.880]   I'm going to put everything that was in that closet in the piles.
[00:26:45.880 --> 00:26:49.400]   I'm going to go through and say, 'Which of this stuff do I really, really need?'
[00:26:49.400 --> 00:26:52.240]   And that stuff I will put back into the closet very carefully.
[00:26:52.240 --> 00:26:54.040]   If I don't really need it, it doesn't go back in."
[00:26:54.040 --> 00:26:57.040]   That's how you organize your closet.
[00:26:57.040 --> 00:26:58.320]   It's a net-zero budget.
[00:26:58.320 --> 00:27:03.700]   You start from zero and add back in the stuff you need carefully, not just trying to throw
[00:27:03.700 --> 00:27:07.820]   organizational bins at the junk that's already in there.
[00:27:07.820 --> 00:27:09.640]   So that's what digital minimalism is, right?
[00:27:09.640 --> 00:27:13.280]   Instead of just throwing a particular piece of advice at your digital distraction, you're
[00:27:13.280 --> 00:27:18.480]   going to reinvent your digital life from scratch, taking everything out, reflecting for a month,
[00:27:18.480 --> 00:27:22.400]   and then only adding back in what matters with rules about how you're going to use it.
[00:27:22.400 --> 00:27:23.400]   Okay.
[00:27:23.400 --> 00:27:32.960]   To get to your specific point about distraction-free apps, in this process they can have a use—typically
[00:27:32.960 --> 00:27:35.320]   they're useful training tools.
[00:27:35.320 --> 00:27:39.760]   If there's a particular technology that you need to use but only use in a limited way
[00:27:39.760 --> 00:27:44.520]   and you have a very strong urge to keep going back to it, distraction-free apps can help
[00:27:44.520 --> 00:27:50.080]   you train to resist that urge because it makes it very difficult to access those technologies
[00:27:50.080 --> 00:27:52.600]   in times you don't want to.
[00:27:52.600 --> 00:27:58.040]   Typically what happens with people is after a few months of using distraction-free apps,
[00:27:58.040 --> 00:28:03.580]   they lose the urge to go use that technology compulsively because they've gotten that groove
[00:28:03.580 --> 00:28:08.160]   out of their mind, their reward circuit weakens, and then they don't use them anymore.
[00:28:08.160 --> 00:28:11.960]   So you might use a distraction-free app as part of your efforts to recreate your digital
[00:28:11.960 --> 00:28:17.380]   life, but typically their uses are more temporary, they're a training tool, not a permanent feature
[00:28:17.380 --> 00:28:20.280]   of your digital landscape.
[00:28:20.280 --> 00:28:25.240]   I love the condo advice, how she—and you get rid of something and you express gratitude
[00:28:25.240 --> 00:28:27.320]   to it, and then you're like, "Bye."
[00:28:27.320 --> 00:28:30.360]   TikTok, I express gratitude to the role you play in my life.
[00:28:30.360 --> 00:28:34.200]   But now our journeys have crossed.
[00:28:34.200 --> 00:28:35.200]   Our journeys have parted.
[00:28:35.200 --> 00:28:36.200]   Yeah, she's very calming.
[00:28:36.200 --> 00:28:39.140]   All right, who do we got next?
[00:28:39.140 --> 00:28:43.880]   Next question is from JP, "I get stressed with my goals due to fear of failure, I keep
[00:28:43.880 --> 00:28:47.560]   everything in my head and can't distinguish from urgent and non-urgent, I pretty much
[00:28:47.560 --> 00:28:48.820]   never finish what I start."
[00:28:48.820 --> 00:28:55.640]   Well, look, you can't, in the modern world, you can't organize your life just in your
[00:28:55.640 --> 00:28:56.640]   head.
[00:28:56.640 --> 00:29:02.760]   Just trying to remember what you want to do, all the things you have to do, your priorities,
[00:29:02.760 --> 00:29:06.380]   somehow use this all to make a decision about what to do next.
[00:29:06.380 --> 00:29:08.900]   The human brain can't do that.
[00:29:08.900 --> 00:29:12.100]   It's like trying to teach a bear to drive a car.
[00:29:12.100 --> 00:29:17.200]   It might be funny or terrifying, but it's probably not going to work very well.
[00:29:17.200 --> 00:29:21.880]   The human brain can't, on its own, organize a modern life.
[00:29:21.880 --> 00:29:22.880]   So what do you need to do instead?
[00:29:22.880 --> 00:29:28.960]   Well, you're going to need something like multi-scale planning that is a system that
[00:29:28.960 --> 00:29:34.800]   can get all of the stuff you need to do and your plan for how you're going to tackle it
[00:29:34.800 --> 00:29:40.080]   out of your head and into a sort of trusted permanent system that you can frequently access.
[00:29:40.080 --> 00:29:46.000]   You basically have to extend your mind like a cyborg with other tools to make this complicated
[00:29:46.000 --> 00:29:49.700]   task of organizing your life much more tractable.
[00:29:49.700 --> 00:29:54.380]   So multi-player scanning, which I talk about a lot on this show, has you planning things
[00:29:54.380 --> 00:29:58.080]   on multiple scales, each of which have their own systems to go with it.
[00:29:58.080 --> 00:30:01.060]   At the highest scale, you have a plan for the current season or quarter where you're
[00:30:01.060 --> 00:30:04.460]   making sense of your bigger priorities.
[00:30:04.460 --> 00:30:08.220]   You reference that quarterly or seasonal plan every week.
[00:30:08.220 --> 00:30:11.820]   When you make a weekly plan, you physically write out your weekly plan.
[00:30:11.820 --> 00:30:14.100]   Also when you do your weekly plan, you confront your calendar.
[00:30:14.100 --> 00:30:15.100]   You make adjustments.
[00:30:15.100 --> 00:30:19.300]   You add to your calendar appointments with yourself to work on particularly important
[00:30:19.300 --> 00:30:22.780]   priorities from your quarterly or seasonal plans.
[00:30:22.780 --> 00:30:25.960]   So now you have a written weekly plan and a calendar that's been updated and corrected
[00:30:25.960 --> 00:30:27.660]   for the week.
[00:30:27.660 --> 00:30:31.180]   And then every day, you look at your calendar and your weekly plan when you make a time
[00:30:31.180 --> 00:30:34.900]   block plan for that day where you give every minute of your workday a job.
[00:30:34.900 --> 00:30:40.760]   So you're not just trying to decide on the fly, "What do I want to work on next?"
[00:30:40.760 --> 00:30:44.680]   You've made a plan for the time that remains in your day between meetings and other appointments,
[00:30:44.680 --> 00:30:49.680]   what you want to do at that time, so you can balance your energy with your needs.
[00:30:49.680 --> 00:30:50.680]   You can batch.
[00:30:50.680 --> 00:30:51.680]   You can be efficient.
[00:30:51.680 --> 00:30:55.980]   You can avoid excessive context switching, etc.
[00:30:55.980 --> 00:30:58.740]   So each of these levels, you have different tools.
[00:30:58.740 --> 00:31:01.540]   All of this is going to be supported by a task system where you're going to keep track
[00:31:01.540 --> 00:31:03.620]   of all the obligations you have to do.
[00:31:03.620 --> 00:31:05.980]   You'll look at that task system when you're doing your weekly plans.
[00:31:05.980 --> 00:31:08.820]   You'll reference it during admin blocks in your daily plans.
[00:31:08.820 --> 00:31:16.340]   All of these are external systems with structure around them that you use so that your brain
[00:31:16.340 --> 00:31:20.700]   doesn't have to be responsible on its own for keeping track of what you have to do and
[00:31:20.700 --> 00:31:22.180]   making decisions in an ad hoc way.
[00:31:22.180 --> 00:31:30.620]   So you need something like multi-scale planning if you're going to keep track of your life.
[00:31:30.620 --> 00:31:34.540]   So you shouldn't be worried, I would say, that you're struggling to do this in your
[00:31:34.540 --> 00:31:35.540]   head.
[00:31:35.540 --> 00:31:37.180]   That's a bear driving a car.
[00:31:37.180 --> 00:31:38.340]   That is pretty impossible.
[00:31:38.340 --> 00:31:43.660]   Actually, a true point about bears driving cars, Jesse, really hard to get insured.
[00:31:43.660 --> 00:31:46.380]   It's a really high insurance rate.
[00:31:46.380 --> 00:31:49.460]   Especially if you live in the west coast of Florida.
[00:31:49.460 --> 00:31:54.900]   If you live in the west coast of Florida, yeah, get a bear to drive a car, high insurance
[00:31:54.900 --> 00:31:55.900]   rate.
[00:31:55.900 --> 00:31:56.900]   All right.
[00:31:56.900 --> 00:31:57.900]   What do we got?
[00:31:57.900 --> 00:31:58.900]   Next question is from Joseph.
[00:31:58.900 --> 00:32:03.780]   I'm a teacher looking to improve my efficiency with admin tasks.
[00:32:03.780 --> 00:32:08.900]   When I have a quiz in two different subjects to grade and record, is the task context switching
[00:32:08.900 --> 00:32:13.700]   effect less if I were to grade and record one class than the other or if I were to grade
[00:32:13.700 --> 00:32:15.860]   both and then record both as blocks?
[00:32:15.860 --> 00:32:19.660]   Well, it's a good question because I'm glad you're thinking about context shifting as
[00:32:19.660 --> 00:32:23.620]   more or less the number one productivity poison you want to be wary about.
[00:32:23.620 --> 00:32:25.340]   Long-time listeners know this.
[00:32:25.340 --> 00:32:29.900]   It takes time to switch your target of your attention from one target to another.
[00:32:29.900 --> 00:32:32.980]   So if you're moving your attention back and forth rapidly, you're going to put your mind
[00:32:32.980 --> 00:32:38.200]   into the state of continuous partial attention, which is a self-imposed cognitive deficit.
[00:32:38.200 --> 00:32:41.980]   You make yourself quite literally dumber.
[00:32:41.980 --> 00:32:52.180]   In this case, recording grades into a gradebook is mechanical and largely non-cognitive.
[00:32:52.180 --> 00:32:56.080]   In other words, you don't have to do difficult thinking.
[00:32:56.080 --> 00:32:58.780]   You're just taking numbers, matching it to a name, writing that name in there.
[00:32:58.780 --> 00:32:59.780]   You don't have to do difficult thinking.
[00:32:59.780 --> 00:33:03.300]   You don't have to load up complicated cognitive context.
[00:33:03.300 --> 00:33:08.380]   You don't have to make decisions or pull from complicated memories.
[00:33:08.380 --> 00:33:16.000]   So I'm not too super worried about the context shift price when you go to just entering grades.
[00:33:16.000 --> 00:33:19.740]   Because again, mechanical thing, it's almost like you're working hard on something like
[00:33:19.740 --> 00:33:21.060]   writing a hard book chapter.
[00:33:21.060 --> 00:33:25.900]   If you get up and go make a cup of tea and then come back, that's not actually going
[00:33:25.900 --> 00:33:27.640]   to be a big hit.
[00:33:27.640 --> 00:33:31.620]   That context shift is not going to be a big hit on your primary test because it's mechanical
[00:33:31.620 --> 00:33:33.700]   and not cognitive.
[00:33:33.700 --> 00:33:39.520]   So this is all to say, it doesn't really matter where you put the grade recording.
[00:33:39.520 --> 00:33:40.820]   It's whatever you have preference for.
[00:33:40.820 --> 00:33:43.860]   So you could grade one thing, then grade the other thing, and then have a long block of
[00:33:43.860 --> 00:33:45.460]   just mechanically entering grades.
[00:33:45.460 --> 00:33:49.520]   Or you could grade one thing, enter the grades, grade another thing, enter the grades.
[00:33:49.520 --> 00:33:52.260]   It's not going to make a difference cognitively.
[00:33:52.260 --> 00:33:55.820]   It's just going to be a matter of what's going to feel better for you.
[00:33:55.820 --> 00:34:00.220]   I would suspect the difference would come down to how demanding the grading is.
[00:34:00.220 --> 00:34:05.580]   So if the grading is really hard, and if the subjects between the two things you're grading
[00:34:05.580 --> 00:34:11.180]   the two quizzes is separate, like it's not the same cognitive context, I would enter
[00:34:11.180 --> 00:34:18.700]   the grades right after grading to give your mind a breather.
[00:34:18.700 --> 00:34:25.220]   I would also consider, let's get advanced here, cognitively advanced.
[00:34:25.220 --> 00:34:31.300]   After grading the first thing, before you enter those grades, go look at the second
[00:34:31.300 --> 00:34:39.020]   thing, and maybe read one of the quizzes to try to start loading that cognitive context.
[00:34:39.020 --> 00:34:44.380]   Grade a single quiz of the next thing.
[00:34:44.380 --> 00:34:46.420]   This is going to go slow at first, right?
[00:34:46.420 --> 00:34:51.380]   Because you now have colliding cognitive context as you look at the second quiz for the first
[00:34:51.380 --> 00:34:57.380]   time, that's a separate context from the quiz you just graded, and so there's going to be
[00:34:57.380 --> 00:35:01.660]   a collision as your brain is trying to shut down the context of the first grading block
[00:35:01.660 --> 00:35:03.260]   and load the context of the second.
[00:35:03.260 --> 00:35:07.780]   So grading that first quiz of the second, the first assignment of the first quiz, I
[00:35:07.780 --> 00:35:09.540]   don't know how to say this, Jesse.
[00:35:09.540 --> 00:35:13.300]   It's a quiz, and he has multiple quizzes to grade, and there's two different quizzes.
[00:35:13.300 --> 00:35:14.300]   That make sense?
[00:35:14.300 --> 00:35:15.300]   Yeah.
[00:35:15.300 --> 00:35:16.300]   All right.
[00:35:16.300 --> 00:35:18.340]   So you're in the second type of quiz, and you grade the first quiz from the second type
[00:35:18.340 --> 00:35:22.460]   of quiz, and that's very hard because it's a new context.
[00:35:22.460 --> 00:35:25.860]   Grade just one from the second type of quiz.
[00:35:25.860 --> 00:35:27.760]   Then here's my advanced advice.
[00:35:27.760 --> 00:35:33.500]   Go back and enter the grades from your first quiz, because here's what's happening.
[00:35:33.500 --> 00:35:37.820]   While you are entering the grades from the first type of quiz, your brain is continuing
[00:35:37.820 --> 00:35:41.540]   in the background the process of switching its context over to the second type of quiz.
[00:35:41.540 --> 00:35:46.780]   You initiated that by looking at a single quiz of that second type, and now you go back
[00:35:46.780 --> 00:35:50.520]   and just mechanically enter grades, your brain is going to continue making this switch.
[00:35:50.520 --> 00:35:54.540]   So now when you're done entering those grades and you return to the second type of quiz,
[00:35:54.540 --> 00:35:58.540]   your context has more thoroughly shifted, and your grading is going to get up to speed
[00:35:58.540 --> 00:35:59.540]   much quicker.
[00:35:59.540 --> 00:36:00.540]   All right.
[00:36:00.540 --> 00:36:03.540]   This is kind of an advanced way of thinking about it, but this is what you probably would
[00:36:03.540 --> 00:36:06.140]   see this effect.
[00:36:06.140 --> 00:36:10.420]   If instead you grade the first type of quiz, you enter the grades, then you turn to the
[00:36:10.420 --> 00:36:16.040]   second type of quiz, it might take—you might have to grade five or six students' quizzes
[00:36:16.040 --> 00:36:20.540]   before you get that momentum going of your brain completely shifting.
[00:36:20.540 --> 00:36:25.420]   Whereas with my tactic, you grade the first type of quiz, grade one of the second type,
[00:36:25.420 --> 00:36:28.740]   enter the grades, then return to the rest of the second type, you'll probably get up
[00:36:28.740 --> 00:36:30.340]   to speed much quicker.
[00:36:30.340 --> 00:36:32.340]   This is just, at this point, like attention hacking.
[00:36:32.340 --> 00:36:34.740]   The differences might be minor.
[00:36:34.740 --> 00:36:38.700]   But I do like the type of thinking this induces, which is to think about cognitive context.
[00:36:38.700 --> 00:36:44.940]   It's like one of the most important properties of modern work, and it's the property that
[00:36:44.940 --> 00:36:49.400]   we think almost nothing about in modern office productivity.
[00:36:49.400 --> 00:36:51.920]   We completely disregard it.
[00:36:51.920 --> 00:36:54.060]   We put low friction as a priority.
[00:36:54.060 --> 00:36:56.320]   We put information velocity as a priority.
[00:36:56.320 --> 00:37:00.980]   We put access to tools and data as a priority, and we completely disregard the cost of context
[00:37:00.980 --> 00:37:01.980]   shifting.
[00:37:01.980 --> 00:37:05.100]   So I love any discussion like this that gets us in the weeds on it.
[00:37:05.100 --> 00:37:10.240]   This sort of psychologically aware productivity is really where we should be.
[00:37:10.240 --> 00:37:12.760]   So I appreciate the chance to sort of nerd out on that.
[00:37:12.760 --> 00:37:16.380]   We'll have to have Joseph respond and see how it goes.
[00:37:16.380 --> 00:37:17.380]   Yeah, he should.
[00:37:17.380 --> 00:37:19.860]   So Joseph, if you hear this, let us know if that technique works.
[00:37:19.860 --> 00:37:22.740]   All right, where are we?
[00:37:22.740 --> 00:37:25.020]   Next question is from Francois.
[00:37:25.020 --> 00:37:27.860]   I'm a professor and also have a French podcast.
[00:37:27.860 --> 00:37:31.220]   My university has agreed to include it in my official duties.
[00:37:31.220 --> 00:37:32.820]   I haven't accepted their offer yet.
[00:37:32.820 --> 00:37:38.140]   However, if I do accept it, how should I think about organizing my podcast within the traditional
[00:37:38.140 --> 00:37:41.440]   academic framework of research, teaching, and service?
[00:37:41.440 --> 00:37:45.580]   Should this be included in my academic tasks?
[00:37:45.580 --> 00:37:48.820]   It's a good question, Francois.
[00:37:48.820 --> 00:37:49.980]   I had a French podcast for a while.
[00:37:49.980 --> 00:37:50.980]   I don't know if you know this.
[00:37:50.980 --> 00:37:52.420]   Yeah, and you had a pipe and a hat.
[00:37:52.420 --> 00:37:57.380]   I had a pipe and a hat, and I just did my French accent.
[00:37:57.380 --> 00:38:02.660]   Long story short, I am no longer welcome in the Republic of France.
[00:38:02.660 --> 00:38:08.940]   I have been banned from setting foot in France after they heard my awesome accent.
[00:38:08.940 --> 00:38:10.660]   Francois, it's a good question.
[00:38:10.660 --> 00:38:15.480]   I don't know the French system super well, so I'm going to answer this from the perspective
[00:38:15.480 --> 00:38:21.620]   of the American academic system, which I think is roughly congruent.
[00:38:21.620 --> 00:38:29.020]   All right, so in the American academic system, by far the most important thing for promotion
[00:38:29.020 --> 00:38:32.140]   and recognition is research.
[00:38:32.140 --> 00:38:33.340]   You have to do service.
[00:38:33.340 --> 00:38:36.340]   You need to be a good teacher.
[00:38:36.340 --> 00:38:38.780]   But those alone can't get you promoted or recognized.
[00:38:38.780 --> 00:38:41.660]   It has to be the quality of your research.
[00:38:41.660 --> 00:38:46.220]   So if your university is going to allow you to count your podcast as an official academic
[00:38:46.220 --> 00:38:53.500]   task, I would recommend that you are very clear about which of the three major tasks,
[00:38:53.500 --> 00:39:00.340]   research, service, and teaching, that it counts as, and I would try to make it count as, service.
[00:39:00.340 --> 00:39:05.260]   When you count it as service, what this means is you can reduce the amount of other service
[00:39:05.260 --> 00:39:10.740]   you do, let the podcast take the place of other service obligations so you're not increasing
[00:39:10.740 --> 00:39:16.500]   your time obligations, and critically, you're not reducing the time you spend on research.
[00:39:16.500 --> 00:39:21.780]   Because when it comes to service and promotion, it's a little bit more binary.
[00:39:21.780 --> 00:39:25.900]   Was this person a good citizen of the institution and his community?
[00:39:25.900 --> 00:39:28.500]   Not how good of a service person were they, right?
[00:39:28.500 --> 00:39:32.540]   So if you can use your podcast as a way to reduce other types of service so your overall
[00:39:32.540 --> 00:39:34.580]   time footprint's the same, that's great.
[00:39:34.580 --> 00:39:37.980]   Do not let it, however, impinge on the time you spend doing research.
[00:39:37.980 --> 00:39:41.460]   That's ultimately what matters most.
[00:39:41.460 --> 00:39:44.540]   Trust me, I've gone through two promotions.
[00:39:44.540 --> 00:39:47.900]   I'm done with promotions now, but I went through both my promotions from assistant to associate
[00:39:47.900 --> 00:39:51.700]   with tenure and from associate with tenure to full.
[00:39:51.700 --> 00:39:59.100]   Both of those promotions, I had large portfolios of more public-facing work, and I had to deal
[00:39:59.100 --> 00:40:00.100]   with them carefully.
[00:40:00.100 --> 00:40:03.660]   When I was promoted to associate, I didn't mention my books.
[00:40:03.660 --> 00:40:07.100]   It was all computer science research.
[00:40:07.100 --> 00:40:13.500]   When I went to full, I did mention them, because as we just saw in the deep dive, some of the
[00:40:13.500 --> 00:40:18.300]   work, public-facing work I did also has a very big academic footprint, right?
[00:40:18.300 --> 00:40:21.660]   We just did a whole paper from the Journal of Applied Philosophy in the deep dive that
[00:40:21.660 --> 00:40:24.540]   was responding to my digital minimalism book.
[00:40:24.540 --> 00:40:29.220]   My book, Deep Work, has been cited in academic articles close to 800 times now.
[00:40:29.220 --> 00:40:30.900]   I was just looking at that the other day.
[00:40:30.900 --> 00:40:37.020]   So I did sort of count that more, but I didn't lean into my podcast, even though I was past
[00:40:37.020 --> 00:40:42.180]   a 10 million download point at that point, because it didn't quite fit clearly into it.
[00:40:42.180 --> 00:40:43.180]   It's not research.
[00:40:43.180 --> 00:40:45.940]   I didn't have an agreement like yours that this counted as service.
[00:40:45.940 --> 00:40:48.500]   I sort of had it as sort of on the side.
[00:40:48.500 --> 00:40:50.460]   So I know this world well.
[00:40:50.460 --> 00:40:51.460]   Ultimately promotions matter.
[00:40:51.460 --> 00:40:56.260]   Are you doing work that's influencing the academic culture?
[00:40:56.260 --> 00:40:57.260]   So that's what you've got to be careful about.
[00:40:57.260 --> 00:41:04.060]   So yeah, if you can use your podcast to reduce other service loads, then I think that's great
[00:41:04.060 --> 00:41:07.820]   because your podcast will probably have a higher impact than the other service you're
[00:41:07.820 --> 00:41:08.820]   replacing.
[00:41:08.820 --> 00:41:11.220]   Just don't let it get in the way of research.
[00:41:11.220 --> 00:41:16.020]   So now that you're a full professor, do you still have to write as many papers?
[00:41:16.020 --> 00:41:17.540]   It's more flexible.
[00:41:17.540 --> 00:41:21.260]   Yeah, it's more flexible.
[00:41:21.260 --> 00:41:25.420]   Right now I'm focusing more on technology and digital ethics than I am computer science.
[00:41:25.420 --> 00:41:27.260]   And that's the type of thing you can explore.
[00:41:27.260 --> 00:41:29.620]   It's sort of the advantage of full professoredom in 10 years.
[00:41:29.620 --> 00:41:31.500]   You can make those explorations.
[00:41:31.500 --> 00:41:35.000]   And if you don't like the way it's going, you can switch back to something else.
[00:41:35.000 --> 00:41:40.880]   One of the things that made a big difference for me is that-- so Google Scholar is a quick
[00:41:40.880 --> 00:41:43.060]   way you can keep up on people's publications.
[00:41:43.060 --> 00:41:44.060]   What have they published?
[00:41:44.060 --> 00:41:45.900]   How much have they been cited?
[00:41:45.900 --> 00:41:48.620]   What are their statistics, like their H index, their I-10 index?
[00:41:48.620 --> 00:41:50.220]   What are their total citation counts?
[00:41:50.220 --> 00:41:53.500]   What are their total citation counts by years?
[00:41:53.500 --> 00:41:58.240]   Once Google Scholar figured out-- because I write under two different names.
[00:41:58.240 --> 00:42:02.620]   My academic computer science papers are typically written under Calvin Newport.
[00:42:02.620 --> 00:42:05.400]   And of course, my public-facing writing is under Cal Newport.
[00:42:05.400 --> 00:42:12.900]   When it figured out, oh, Calvin Newport and Cal Newport are the same person, it really
[00:42:12.900 --> 00:42:14.260]   changed my statistics.
[00:42:14.260 --> 00:42:21.900]   So where you saw a bit of a fall off in citations in recent years now shows a steady high level
[00:42:21.900 --> 00:42:27.100]   of citations because the public-facing work on technology I was doing as Cal Newport gets
[00:42:27.100 --> 00:42:29.100]   cited a lot academically.
[00:42:29.100 --> 00:42:34.220]   So it shows a sort of smooth transition from less computer science, more digital ethics.
[00:42:34.220 --> 00:42:38.060]   And the impact is measured by citations as sort of stayed steady.
[00:42:38.060 --> 00:42:41.100]   I thought you were saying the other name was going to be your French name.
[00:42:41.100 --> 00:42:42.100]   Yeah.
[00:42:42.100 --> 00:42:43.100]   Well, yeah.
[00:42:43.100 --> 00:42:44.100]   There's Calvin Newport.
[00:42:44.100 --> 00:42:45.100]   There's Cal Newport.
[00:42:45.100 --> 00:42:48.900]   And there's Pierre, Pierre Le Newport.
[00:42:48.900 --> 00:42:54.060]   Actually, I do have a French-- I do have French heritage.
[00:42:54.060 --> 00:43:04.060]   My paternal grandmother was a Levelle, the Levelle family.
[00:43:04.060 --> 00:43:08.140]   And the Levelle family goes all the way back to the French Huguenots that came over here
[00:43:08.140 --> 00:43:09.140]   pre-Revolution.
[00:43:09.140 --> 00:43:10.380]   And they used to be Levalve.
[00:43:10.380 --> 00:43:11.380]   So it's French.
[00:43:11.380 --> 00:43:14.860]   I have a French Huguenot blood back in there.
[00:43:14.860 --> 00:43:15.860]   All right.
[00:43:15.860 --> 00:43:16.860]   What do we got next?
[00:43:16.860 --> 00:43:17.860]   We have our corner.
[00:43:17.860 --> 00:43:19.500]   Oh, Slow Productivity Corner.
[00:43:19.500 --> 00:43:27.500]   Let's hear that theme music.
[00:43:27.500 --> 00:43:31.180]   So for those who are new, Slow Productivity Corner is the question.
[00:43:31.180 --> 00:43:35.380]   We do one question each week related to my most recent book, Slow Productivity, the Lost
[00:43:35.380 --> 00:43:38.260]   Art of Accomplishment Without Burnout.
[00:43:38.260 --> 00:43:39.260]   Went to Amazon.
[00:43:39.260 --> 00:43:40.260]   So I can see this now.
[00:43:40.260 --> 00:43:44.060]   Went to Amazon's Best Business Books of 2024 and the winner of Best Business Book of the
[00:43:44.060 --> 00:43:51.340]   Year from the S-- something, something, something-- S-A-B-E-W.
[00:43:51.340 --> 00:43:52.340]   It has those letters in it.
[00:43:52.340 --> 00:43:54.020]   There's a huge cash prize, guys.
[00:43:54.020 --> 00:43:58.720]   This is an important award, somewhere between $60,000 and $600,000 award.
[00:43:58.720 --> 00:44:03.420]   My award-winning book, Slow Productivity, we try to do a question each week that comes
[00:44:03.420 --> 00:44:04.420]   from that book.
[00:44:04.420 --> 00:44:05.420]   If you haven't read the book yet, come on.
[00:44:05.420 --> 00:44:06.420]   Get the book.
[00:44:06.420 --> 00:44:07.660]   Half the stuff we talk about comes from it.
[00:44:07.660 --> 00:44:08.660]   All right.
[00:44:08.660 --> 00:44:10.540]   What's our Slow Productivity Corner question of the week?
[00:44:10.540 --> 00:44:12.780]   It's from Madonna's Gold Tooth.
[00:44:12.780 --> 00:44:13.780]   Yikes.
[00:44:13.780 --> 00:44:16.780]   What do you think about the slow living craze on the internet?
[00:44:16.780 --> 00:44:18.780]   And do you think it's just a fad or will it be permanent?
[00:44:18.780 --> 00:44:19.780]   All right.
[00:44:19.780 --> 00:44:21.820]   So do you know what this is, Jesse?
[00:44:21.820 --> 00:44:23.440]   There's a YouTube video.
[00:44:23.440 --> 00:44:24.440]   You have a YouTube video.
[00:44:24.440 --> 00:44:25.440]   Yeah.
[00:44:25.440 --> 00:44:28.660]   It'll explain what slow living is?
[00:44:28.660 --> 00:44:29.940]   Because I don't actually know what this is.
[00:44:29.940 --> 00:44:30.940]   All right.
[00:44:30.940 --> 00:44:31.940]   Let's load this up.
[00:44:31.940 --> 00:44:32.940]   Let's listen here.
[00:44:32.940 --> 00:44:35.660]   We're going to be learning this together, what slow living is, and then I can answer
[00:44:35.660 --> 00:44:38.980]   this question.
[00:44:38.980 --> 00:44:39.980]   Madonna's Gold Tooth.
[00:44:39.980 --> 00:44:40.980]   All right.
[00:44:40.980 --> 00:44:41.980]   There seems to be--
[00:44:41.980 --> 00:44:48.280]   One of the things I am most scared of in my life is looking back with regret.
[00:44:48.280 --> 00:44:52.400]   Looking back at all the little moments that I missed in pursuit of more.
[00:44:52.400 --> 00:44:57.140]   This year, I've been really working on slowing down and trying to be more present for the
[00:44:57.140 --> 00:45:01.580]   little moments that make up most of our lives and making sure that I'm actually building
[00:45:01.580 --> 00:45:04.340]   a life by design and not just by default.
[00:45:04.340 --> 00:45:08.340]   So these are some simple, tiny habits that I have implemented that have really helped
[00:45:08.340 --> 00:45:09.340]   me slow down.
[00:45:09.340 --> 00:45:10.560]   Five minutes of nothing.
[00:45:10.560 --> 00:45:15.540]   This is literally a block that I have on my calendar every day just to have five minutes
[00:45:15.540 --> 00:45:17.420]   of nothing happening.
[00:45:17.420 --> 00:45:21.660]   Doesn't sound like a lot, but when you don't have any music, no podcast, no work that you're
[00:45:21.660 --> 00:45:25.180]   thinking of, no work that you're doing, no book that you're reading for just five minutes
[00:45:25.180 --> 00:45:28.840]   and you sit there and you stare at a wall, not trying to meditate, but you just let your
[00:45:28.840 --> 00:45:31.100]   brain do its thing and you observe it.
[00:45:31.100 --> 00:45:35.640]   It's a beautiful time for me to just reset and make sure that there's actually breaks
[00:45:35.640 --> 00:45:38.380]   in my life to remember that my life is not online.
[00:45:38.380 --> 00:45:42.480]   It's not on a computer screen and sometimes I need physical breaks to make that happen.
[00:45:42.480 --> 00:45:43.480]   Three zones.
[00:45:43.480 --> 00:45:45.980]   For me, this is my sauna, my cold plunge.
[00:45:45.980 --> 00:45:46.980]   All right.
[00:45:46.980 --> 00:45:48.540]   I think I get the idea.
[00:45:48.540 --> 00:45:54.340]   Well, first of all, for those who are just listening instead of watching, the video had
[00:45:54.340 --> 00:46:01.100]   like a faux graininess while they played like really relaxing music and he washed eggs from
[00:46:01.100 --> 00:46:03.760]   the chickens that he just stared at.
[00:46:03.760 --> 00:46:05.760]   I think with that type of music.
[00:46:05.760 --> 00:46:06.760]   I like the music a lot.
[00:46:06.760 --> 00:46:07.760]   Yeah.
[00:46:07.760 --> 00:46:10.560]   I think almost anything seems, almost anything seems profound.
[00:46:10.560 --> 00:46:12.000]   73,000 views.
[00:46:12.000 --> 00:46:13.000]   Yeah.
[00:46:13.000 --> 00:46:19.660]   I mean, almost anything I think will sound sort of important and meaningful and somber.
[00:46:19.660 --> 00:46:27.640]   You could have a video of someone earnestly trying and failing to life threatening in
[00:46:27.640 --> 00:46:31.480]   a life threatening way to get a bear into the car to drive it.
[00:46:31.480 --> 00:46:32.480]   Played to that music.
[00:46:32.480 --> 00:46:33.720]   Like good for him.
[00:46:33.720 --> 00:46:34.720]   Good for him.
[00:46:34.720 --> 00:46:39.160]   The bear's mauling him as he's trying to get him into a Chevy Impala.
[00:46:39.160 --> 00:46:42.000]   Play it to that music and cut to some scenes of someone washing eggs.
[00:46:42.000 --> 00:46:44.700]   You'd be like, "Yeah, life is like a bear trying to get into a car."
[00:46:44.700 --> 00:46:46.760]   Drive to the insurance agency and get the insurance card.
[00:46:46.760 --> 00:46:51.080]   Just him at the insurance agency, just his face ripped up in bandages trying to get the
[00:46:51.080 --> 00:46:52.600]   insurance guy is just shaking his head.
[00:46:52.600 --> 00:46:53.920]   You'd be like, "Yeah, that's...
[00:46:53.920 --> 00:46:54.920]   Play that music."
[00:46:54.920 --> 00:46:58.680]   You're like, "Yeah, it's profound."
[00:46:58.680 --> 00:47:00.120]   So no.
[00:47:00.120 --> 00:47:01.920]   Slow living and slow productivity are different.
[00:47:01.920 --> 00:47:07.560]   So let me tell you how, and then I'm going to tell you what slow living seems to be like
[00:47:07.560 --> 00:47:10.320]   connected to some other stuff we talk about.
[00:47:10.320 --> 00:47:11.960]   Slow productivity is about work.
[00:47:11.960 --> 00:47:13.760]   It's about knowledge work.
[00:47:13.760 --> 00:47:18.960]   It's about how do we define what productivity means in knowledge work.
[00:47:18.960 --> 00:47:22.520]   The core argument of that book is that we have a bad implicit definition that we tend
[00:47:22.520 --> 00:47:27.320]   to fall back on, which is pseudo productivity, which is to use visible effort as a proxy
[00:47:27.320 --> 00:47:29.240]   for useful activity.
[00:47:29.240 --> 00:47:31.440]   Slow productivity is a alternative.
[00:47:31.440 --> 00:47:35.320]   It says our goal in knowledge work should not be to be as busy as possible.
[00:47:35.320 --> 00:47:39.760]   We should instead focus on not doing too many things at the same time, keeping our pace
[00:47:39.760 --> 00:47:43.440]   of work varied and natural, but then really obsessing over quality.
[00:47:43.440 --> 00:47:45.880]   This is a better, more sustainable definition of productivity.
[00:47:45.880 --> 00:47:47.280]   This is about work.
[00:47:47.280 --> 00:47:51.860]   Slow living seems to be about life outside of work and a lot to do with distraction,
[00:47:51.860 --> 00:47:54.320]   especially digital distraction.
[00:47:54.320 --> 00:47:59.000]   So it's probably closer to digital minimalism when it comes to the things I talk and write
[00:47:59.000 --> 00:48:01.280]   about than anything else.
[00:48:01.280 --> 00:48:06.000]   I think if you're a digital minimalist, your life will seem slower in the way that's being
[00:48:06.000 --> 00:48:10.500]   talked about in this video, because a digital minimalist works backwards from their values
[00:48:10.500 --> 00:48:17.240]   to dictate their technology use, and so, you know, if you value your chickens and washing
[00:48:17.240 --> 00:48:20.920]   your eggs or whatever, you are going to be careful about crafting your technological
[00:48:20.920 --> 00:48:24.120]   use so that you're not always looking at your phone and you can't enjoy doing that.
[00:48:24.120 --> 00:48:28.500]   In general, digital minimalists do feel like their lives are slower and richer.
[00:48:28.500 --> 00:48:31.940]   There is a neurological reason for this, right?
[00:48:31.940 --> 00:48:35.200]   Your life is what you pay attention to.
[00:48:35.200 --> 00:48:38.200]   So if you're constantly paying attention to your phone, you perceive your life as very
[00:48:38.200 --> 00:48:44.400]   sort of like fast-paced, emotionally activated, sort of this like really sort of shaky, jittery
[00:48:44.400 --> 00:48:47.640]   world that's always rolling past, because when you're looking at your phone, everything's
[00:48:47.640 --> 00:48:48.640]   moving fast.
[00:48:48.640 --> 00:48:49.640]   Swipe, swipe, swipe.
[00:48:49.640 --> 00:48:50.640]   Tap, tap, tap.
[00:48:50.640 --> 00:48:51.640]   Look at this.
[00:48:51.640 --> 00:48:52.640]   Look at that.
[00:48:52.640 --> 00:48:53.640]   Jump over there.
[00:48:53.640 --> 00:48:56.320]   Time moves fast because you're moving fast on your phone.
[00:48:56.320 --> 00:49:00.900]   Also time moves fast because you're doing this sort of homogenous behavior.
[00:49:00.900 --> 00:49:04.740]   So when you're doing sort of the same thing, you don't have a really good sense of how
[00:49:04.740 --> 00:49:06.280]   long time is.
[00:49:06.280 --> 00:49:08.560]   Time can just sort of unfold.
[00:49:08.560 --> 00:49:13.160]   When you're not on your phone and engaging in specific behaviors, it is just by definition
[00:49:13.160 --> 00:49:16.500]   slower because everything is slower than using your phone.
[00:49:16.500 --> 00:49:20.820]   And because those behaviors are novel, they're different specific things in novel specific
[00:49:20.820 --> 00:49:25.340]   locations, your perception of time is of it being much slower.
[00:49:25.340 --> 00:49:27.660]   Your day seems longer.
[00:49:27.660 --> 00:49:30.140]   Your experience is richer.
[00:49:30.140 --> 00:49:36.220]   So I think digital minimalism will probably lead you to something like slower living.
[00:49:36.220 --> 00:49:38.980]   Start with the digital minimalism and end up at the slower living.
[00:49:38.980 --> 00:49:44.180]   It's sort of a consequence of getting intentional about your life and technology.
[00:49:44.180 --> 00:49:45.960]   But it is quite separate from slow productivity.
[00:49:45.960 --> 00:49:51.200]   They share the same word slow, but they're only connected by this idea of sort of intentionality.
[00:49:51.200 --> 00:49:53.680]   Slow productivity is about your work at your desk.
[00:49:53.680 --> 00:49:55.600]   Slow living is about your life outside of work.
[00:49:55.600 --> 00:49:57.760]   Does that seem reasonable, Jesse?
[00:49:57.760 --> 00:49:58.760]   Yeah.
[00:49:58.760 --> 00:50:00.840]   Maybe we should have chickens in here.
[00:50:00.840 --> 00:50:01.840]   I like that video.
[00:50:01.840 --> 00:50:03.760]   We should do more of that music.
[00:50:03.760 --> 00:50:08.600]   We have kind of music, cooler music like that for the in-depth episodes.
[00:50:08.600 --> 00:50:09.600]   Yeah.
[00:50:09.600 --> 00:50:12.000]   That's a little more like a meditative music.
[00:50:12.000 --> 00:50:13.000]   All right.
[00:50:13.000 --> 00:50:14.440]   Do we have a call this week?
[00:50:14.440 --> 00:50:15.440]   We do.
[00:50:15.440 --> 00:50:16.440]   All right.
[00:50:16.440 --> 00:50:17.440]   Let's hear this.
[00:50:17.440 --> 00:50:18.440]   Hi, Cal.
[00:50:18.440 --> 00:50:23.040]   I've noticed I struggle with tiredness and a lack of focus in the afternoons, especially
[00:50:23.040 --> 00:50:25.600]   during my scheduled deep work blocks.
[00:50:25.600 --> 00:50:30.040]   After lunch, my mind doesn't feel as sharp and I often find myself drifting off and daydreaming.
[00:50:30.040 --> 00:50:35.280]   Do you have any strategies to help maintain focus and mental clarity during these times?
[00:50:35.280 --> 00:50:36.280]   Thanks.
[00:50:36.280 --> 00:50:42.360]   Well, first of all, we have to keep in mind that there's a limited capacity to do deep
[00:50:42.360 --> 00:50:44.000]   work in a given day.
[00:50:44.000 --> 00:50:48.800]   So if you're talking about like highly demanding focused activities, things that require you
[00:50:48.800 --> 00:50:53.200]   to use your full cognitive capacity, probably do those in the morning.
[00:50:53.200 --> 00:50:57.920]   Do those first thing before you've had a lot of context shifts so your mind is still clear.
[00:50:57.920 --> 00:50:58.920]   And be okay.
[00:50:58.920 --> 00:51:02.040]   I got in a good, hard early session.
[00:51:02.040 --> 00:51:05.960]   I'm okay not having to return to these cognitively demanding activities in the afternoon because
[00:51:05.960 --> 00:51:09.080]   I'm just not going to have enough cognitive gas.
[00:51:09.080 --> 00:51:13.120]   If it's more just, "No, I have administrative stuff to do.
[00:51:13.120 --> 00:51:14.120]   I have to take notes.
[00:51:14.120 --> 00:51:15.720]   I have to send emails."
[00:51:15.720 --> 00:51:19.760]   It's not cognitively demanding, but I just sort of lose focus and drift and lose energy
[00:51:19.760 --> 00:51:20.760]   in the afternoon.
[00:51:20.760 --> 00:51:22.960]   Well, that's very common as well.
[00:51:22.960 --> 00:51:26.020]   And a couple things that helps is time blocking.
[00:51:26.020 --> 00:51:29.040]   So instead of having to constantly have an argument with yourself of like, "What should
[00:51:29.040 --> 00:51:30.440]   I do next?
[00:51:30.440 --> 00:51:31.720]   Should I keep doing this?
[00:51:31.720 --> 00:51:33.280]   Should I take a break?"
[00:51:33.280 --> 00:51:36.440]   Time block those afternoons and just make the single commitment to stick to your time
[00:51:36.440 --> 00:51:38.640]   block schedule the best you can.
[00:51:38.640 --> 00:51:43.440]   So you get rid of a lot of that decisional friction that comes from being more freeform
[00:51:43.440 --> 00:51:46.320]   in your approach to your afternoon.
[00:51:46.320 --> 00:51:49.160]   Second in that time block schedule batch.
[00:51:49.160 --> 00:51:54.600]   So let me do a lot of similar tasks together because even if minor, sticking within the
[00:51:54.600 --> 00:51:57.380]   same cognitive context makes it easier.
[00:51:57.380 --> 00:52:01.360]   This can apply even to cleaning out your email inbox.
[00:52:01.360 --> 00:52:04.920]   I recommend if you have like a super stuffed inbox and it's like three o'clock and you're
[00:52:04.920 --> 00:52:10.280]   exhausted but you kind of have to get through it, create a folder or label for the current
[00:52:10.280 --> 00:52:12.000]   messages you're answering.
[00:52:12.000 --> 00:52:16.240]   And then go through and grab a bunch of messages of the same type.
[00:52:16.240 --> 00:52:19.940]   So they're all relevant to the same cognitive context, they're all scheduling messages.
[00:52:19.940 --> 00:52:22.960]   They're all messages related to like an upcoming event.
[00:52:22.960 --> 00:52:27.040]   Move those all to that label or folder and then tackle those just by themselves.
[00:52:27.040 --> 00:52:29.480]   So now you're doing messages without having to change your context.
[00:52:29.480 --> 00:52:33.400]   Then go and grab another type of messages and do the same.
[00:52:33.400 --> 00:52:37.500]   What happens is if you follow the alternative of just sort of doing your emails in the order
[00:52:37.500 --> 00:52:42.200]   they exist in your inbox, you're switching potentially your cognitive context from message
[00:52:42.200 --> 00:52:44.980]   to message to message and that's exhausting.
[00:52:44.980 --> 00:52:46.880]   So that can help as well.
[00:52:46.880 --> 00:52:48.880]   Third, end your day earlier.
[00:52:48.880 --> 00:52:53.520]   Hey, I'm exhausted by three or four that may be like stop your day between three and four.
[00:52:53.520 --> 00:52:54.520]   Time block your day.
[00:52:54.520 --> 00:52:57.800]   If you're doing multi-scale planning, you have a good weekly plan, your weekly plan
[00:52:57.800 --> 00:53:01.000]   is in touch with your quarterly plan.
[00:53:01.000 --> 00:53:03.360]   This is a key idea from my book Slow Productivity.
[00:53:03.360 --> 00:53:07.880]   The second principle, it says work at a natural pace, which says this idea that like the perfect
[00:53:07.880 --> 00:53:12.740]   calibration for humans that do cognitive work is nine to five all out every day is preposterous.
[00:53:12.740 --> 00:53:16.340]   Why would that just happen to be optimal for everyone?
[00:53:16.340 --> 00:53:20.140]   You might find out working till three or 3.30, this is really what's optimal.
[00:53:20.140 --> 00:53:23.980]   You're time blocked, you're on it, and then when you're done, be done.
[00:53:23.980 --> 00:53:26.620]   Or maybe four, maybe two, it could be different for different people.
[00:53:26.620 --> 00:53:32.160]   But don't feel like you're too stuck with it has to be this exact eight hour day.
[00:53:32.160 --> 00:53:34.080]   Some people just run out of gas earlier than others.
[00:53:34.080 --> 00:53:36.740]   Your work might be harder than others, so you need to end earlier.
[00:53:36.740 --> 00:53:41.420]   I talk about in, I don't know if this is in Slow Productivity, I think this is in my book
[00:53:41.420 --> 00:53:43.380]   A World Without Email.
[00:53:43.380 --> 00:53:49.160]   I talk about this type of programming called extreme programming, and it's pair-based and
[00:53:49.160 --> 00:53:54.500]   it's super intense, and it produced fantastic code, but it's super intense.
[00:53:54.500 --> 00:53:59.380]   I report that companies that do this type of coding, they produce really cool stuff,
[00:53:59.380 --> 00:54:02.300]   but they have to let people go home by like 2.30 or 3.00.
[00:54:02.300 --> 00:54:03.740]   It's just too exhausting.
[00:54:03.740 --> 00:54:05.900]   You can't do it till five.
[00:54:05.900 --> 00:54:08.340]   People at first have to go home and take naps.
[00:54:08.340 --> 00:54:13.200]   So don't assume that everyone is perfectly calibrated to work all out till five.
[00:54:13.200 --> 00:54:14.620]   Figure out what works for you.
[00:54:14.620 --> 00:54:18.620]   If you're organized and on the ball, you'll produce good work.
[00:54:18.620 --> 00:54:21.300]   So I would vary it that way as well.
[00:54:21.300 --> 00:54:23.260]   All right, let's see here.
[00:54:23.260 --> 00:54:25.540]   We have a case study.
[00:54:25.540 --> 00:54:29.460]   This is where we have people write in where they talk about their personal experience
[00:54:29.460 --> 00:54:32.540]   putting the type of things we talk about the show in the practice in their own lives.
[00:54:32.540 --> 00:54:36.980]   All right, so today's case study comes from Zach.
[00:54:36.980 --> 00:54:44.180]   Now here's what Zach says, "Recently, I've made a monumental life change for the better
[00:54:44.180 --> 00:54:48.460]   in no small part due to CALS, books, podcast, and newsletter.
[00:54:48.460 --> 00:54:51.020]   I graduated in March of 2020.
[00:54:51.020 --> 00:54:54.740]   While my classes went online, I decided to get my real estate license and pursue my interest
[00:54:54.740 --> 00:54:58.740]   in real estate investments because of the high autonomy and market activity due to the
[00:54:58.740 --> 00:55:01.460]   interest rate environment at the time.
[00:55:01.460 --> 00:55:02.660]   I was successful.
[00:55:02.660 --> 00:55:06.900]   I specialized in commercial investment sales and became proficient in my field because
[00:55:06.900 --> 00:55:09.800]   of my implementation of deep work principles.
[00:55:09.800 --> 00:55:13.380]   The only problem was that I was miserable at work.
[00:55:13.380 --> 00:55:17.340]   My days mostly consisted of cold calling and driving all over the state for client meetings.
[00:55:17.340 --> 00:55:22.660]   So even though I was making decent money and had full autonomy, my lifestyle wasn't great
[00:55:22.660 --> 00:55:25.560]   and it was trending in the wrong direction.
[00:55:25.560 --> 00:55:32.700]   On top of that, I was working mostly solo while I'm a very team oriented person.
[00:55:32.700 --> 00:55:37.420]   After listening to your podcast religiously on my long drives, my mindset began to shift.
[00:55:37.420 --> 00:55:42.580]   I realized I was optimizing for autonomy and money without much thought to lifestyle and
[00:55:42.580 --> 00:55:45.180]   long term life design.
[00:55:45.180 --> 00:55:47.220]   So I saved up some money and quit.
[00:55:47.220 --> 00:55:48.660]   Believe me, this was tough.
[00:55:48.660 --> 00:55:51.380]   Leveraging CALS principles got me far relatively quickly.
[00:55:51.380 --> 00:55:53.100]   So I had a promising career trajectory.
[00:55:53.100 --> 00:55:56.100]   But when I looked at guys way further down the road, they had a lot of material success
[00:55:56.100 --> 00:55:58.580]   without intentional design.
[00:55:58.580 --> 00:56:01.940]   After hunting and interviewing with jobs that align with my long term lifestyle vision for
[00:56:01.940 --> 00:56:06.620]   a few months, they successfully landed a job at a tech startup that provides me a much
[00:56:06.620 --> 00:56:08.740]   better day today.
[00:56:08.740 --> 00:56:11.700]   It's a short, beautiful commute to an office in my favorite part of town.
[00:56:11.700 --> 00:56:15.040]   My work is varied, challenging and interesting.
[00:56:15.040 --> 00:56:19.100]   And most importantly, I'm working with a like minded team who are all just as obsessed about
[00:56:19.100 --> 00:56:21.740]   productivity systems as I am.
[00:56:21.740 --> 00:56:26.380]   I just finished my first week and I'm blown away at what a difference this intentional
[00:56:26.380 --> 00:56:28.260]   change has made in my life.
[00:56:28.260 --> 00:56:31.900]   For the first time in years, I'm bursting with excitement to go to work.
[00:56:31.900 --> 00:56:33.540]   Should I have applied for jobs while working?
[00:56:33.540 --> 00:56:34.540]   Probably.
[00:56:34.540 --> 00:56:36.900]   But I was so burnt out that I had a burn the ships mindset.
[00:56:36.900 --> 00:56:41.540]   I'm eagerly awaiting your next book nearly as much as I am awaiting Brandon Sanderson's
[00:56:41.540 --> 00:56:43.820]   the author of Name of the Wind.
[00:56:43.820 --> 00:56:49.380]   I didn't include that, leave that in there.
[00:56:49.380 --> 00:56:50.380]   Brandon Sanderson.
[00:56:50.380 --> 00:56:51.740]   I still want to go down.
[00:56:51.740 --> 00:56:54.100]   I told you I have an invitation to go see his lair.
[00:56:54.100 --> 00:56:55.100]   Yeah.
[00:56:55.100 --> 00:56:58.380]   I have to do that.
[00:56:58.380 --> 00:57:00.540]   My wife is going on a trip down there.
[00:57:00.540 --> 00:57:01.540]   To Brandon Anderson?
[00:57:01.540 --> 00:57:03.300]   Well, no, not to it.
[00:57:03.300 --> 00:57:04.300]   That would be weird.
[00:57:04.300 --> 00:57:10.020]   She's like, I'm going on a trip, I'm going to be spending a week in Brandon Sanderson's
[00:57:10.020 --> 00:57:11.020]   dungeon.
[00:57:11.020 --> 00:57:12.500]   I'd be, I'd be worried about that.
[00:57:12.500 --> 00:57:14.020]   No, she's going to that part of the country.
[00:57:14.020 --> 00:57:18.620]   And all I could think is like, if that was me going on that trip, I would be able to
[00:57:18.620 --> 00:57:20.420]   see Sanderson's lair.
[00:57:20.420 --> 00:57:21.420]   Yeah.
[00:57:21.420 --> 00:57:26.740]   It'd be interesting if I get there and it's just half of it's just a sex dungeon.
[00:57:26.740 --> 00:57:27.740]   Probably not.
[00:57:27.740 --> 00:57:35.340]   I think he's a pretty straight laced Mormon, but you never know.
[00:57:35.340 --> 00:57:44.260]   The least popular pornographic video of all time is titled "Brandon Sanderson's Sex Dungeon."
[00:57:44.260 --> 00:57:45.260]   Six views.
[00:57:45.260 --> 00:57:46.260]   All right.
[00:57:46.260 --> 00:57:49.500]   That's a great, Zach, I appreciated that.
[00:57:49.500 --> 00:57:51.500]   Two things I want to point out about that case study.
[00:57:51.500 --> 00:57:53.420]   One, lifestyle centric planning.
[00:57:53.420 --> 00:57:56.900]   That's the way to think about your career.
[00:57:56.900 --> 00:58:01.460]   It's one of the most important dials you have to turn in trying to construct your lifestyle.
[00:58:01.460 --> 00:58:03.340]   But what matters is the target lifestyle.
[00:58:03.340 --> 00:58:05.540]   What do you want the day-to-day of your life to be like?
[00:58:05.540 --> 00:58:07.140]   You work backwards from that vision.
[00:58:07.140 --> 00:58:09.700]   That's how you help figure out what work to do or not do.
[00:58:09.700 --> 00:58:16.260]   This is much more effective than either following your passion or just blindly following a clear
[00:58:16.260 --> 00:58:21.820]   metric like money and just hoping by happenstance that will lead you being happy.
[00:58:21.820 --> 00:58:24.980]   The other thing I want to point out about this example though is, okay, Zach started
[00:58:24.980 --> 00:58:28.700]   one job, didn't work out, he switched.
[00:58:28.700 --> 00:58:30.220]   Is that a failure?
[00:58:30.220 --> 00:58:31.220]   No.
[00:58:31.220 --> 00:58:34.100]   It's very common.
[00:58:34.100 --> 00:58:39.980]   Figuring out the components of your ideal lifestyle is difficult and it evolves with
[00:58:39.980 --> 00:58:42.700]   experience.
[00:58:42.700 --> 00:58:48.460]   He had a hypothesis, I think, built on autonomy and financial security.
[00:58:48.460 --> 00:58:54.780]   He had a hypothesis of a lifestyle vision that he thought would be ideal for him.
[00:58:54.780 --> 00:59:00.220]   Zach pursued a job that matched that hypothesis and then learned through real-life experience,
[00:59:00.220 --> 00:59:02.620]   "Oh, there's these other things I care about.
[00:59:02.620 --> 00:59:07.100]   I didn't realize them until I had them not be present in my life.
[00:59:07.100 --> 00:59:12.020]   I didn't realize autonomy without X, Y, and Z wasn't so good, the money thing I don't
[00:59:12.020 --> 00:59:13.260]   care so much about."
[00:59:13.260 --> 00:59:16.420]   Through life experience, he updated his priors.
[00:59:16.420 --> 00:59:20.060]   His vision of the ideal lifestyle evolved and he said, "Great.
[00:59:20.060 --> 00:59:23.140]   Let me now leverage my career capital and make a shift that's going to get me closer
[00:59:23.140 --> 00:59:24.140]   to that lifestyle."
[00:59:24.140 --> 00:59:28.380]   Now, in this case, the career capital he leveraged was literal capital.
[00:59:28.380 --> 00:59:34.340]   He was making good money, so he saved up enough to buy him time to make a switch.
[00:59:34.340 --> 00:59:38.420]   He was early enough in his career that sort of skills-based career capital was less useful
[00:59:38.420 --> 00:59:42.100]   or less important because he was still a pretty early-stage career.
[00:59:42.100 --> 00:59:46.460]   Then he used that money to buy him some time to find a job that focused on other things
[00:59:46.460 --> 00:59:48.900]   he had discovered were important and now he's much happier.
[00:59:48.900 --> 00:59:53.900]   That's lifestyle-centric career planning in action.
[00:59:53.900 --> 00:59:58.580]   It evolves, it's tactical, it's not sexy.
[00:59:58.580 --> 01:00:03.900]   It's not Brandon Sanderson's Sex Dungeon sexy, but it's what over time is going to
[01:00:03.900 --> 01:00:06.460]   make your life more fulfilling.
[01:00:06.460 --> 01:00:09.220]   I've worn my VBLCCP hat a few times now.
[01:00:09.220 --> 01:00:12.860]   I've been wearing my Deep Life hat regularly.
[01:00:12.860 --> 01:00:16.900]   No one has asked me yet or noticed what VBLCCP means.
[01:00:16.900 --> 01:00:20.740]   I haven't got a reaction to it yet, but I'm still thinking we'll find our first.
[01:00:20.740 --> 01:00:25.140]   You get any questions about Deep Life or people just assume it's a brand?
[01:00:25.140 --> 01:00:27.780]   No, no questions yet.
[01:00:27.780 --> 01:00:28.780]   Yeah, I'll see.
[01:00:28.780 --> 01:00:32.420]   I'm going to keep wearing mine until I find a true believer, but I haven't found them
[01:00:32.420 --> 01:00:33.420]   yet.
[01:00:33.420 --> 01:00:36.620]   All right, we've got a cool final segment coming up, a Tech Corner segment, but first
[01:00:36.620 --> 01:00:42.060]   let's hear briefly about another sponsor.
[01:00:42.060 --> 01:00:43.580]   You know what's not fair?
[01:00:43.580 --> 01:00:48.580]   The fact that Netflix hides thousands of shows and movies from you based on your location
[01:00:48.580 --> 01:00:52.940]   and then has the nerve to just keep increasing their prices.
[01:00:52.940 --> 01:00:56.980]   Now you could just cancel your subscription in protest, or you could be smart about it
[01:00:56.980 --> 01:01:05.140]   and make sure you get your full money's worth, like I do, by using ExpressVPN.
[01:01:05.140 --> 01:01:07.260]   So we talk a lot about VPNs on the show.
[01:01:07.260 --> 01:01:10.340]   I'm very clear you should use a VPN.
[01:01:10.340 --> 01:01:14.660]   The way it works very briefly is that instead of just directly accessing a website or a
[01:01:14.660 --> 01:01:19.220]   service with a VPN, you instead connect to a VPN server.
[01:01:19.220 --> 01:01:23.420]   You tell that server with an encrypted message the site and service you actually want to
[01:01:23.420 --> 01:01:24.420]   use.
[01:01:24.420 --> 01:01:28.420]   That server talks to it on your behalf, encrypts the response, and sends it back.
[01:01:28.420 --> 01:01:33.540]   So that means anyone monitoring your internet usage only learns that you're talking to a
[01:01:33.540 --> 01:01:34.720]   VPN server.
[01:01:34.720 --> 01:01:36.140]   They don't learn what site you're talking to.
[01:01:36.140 --> 01:01:39.020]   They don't learn what service you're talking to.
[01:01:39.020 --> 01:01:43.340]   One of the advantages of doing this, beyond just the obvious privacy advantages, the hacking
[01:01:43.340 --> 01:01:48.160]   advantages, the security advantages, is if you connect to a VPN server in a different
[01:01:48.160 --> 01:01:53.140]   location and that server talks to Netflix on your behalf, Netflix thinks you're in that
[01:01:53.140 --> 01:01:54.140]   location.
[01:01:54.140 --> 01:01:57.940]   So ExpressVPN has servers all around the world.
[01:01:57.940 --> 01:02:02.060]   So you can select a server in like whatever geographic zone you care about, and then you'll
[01:02:02.060 --> 01:02:07.900]   get that zone's Netflix content or whatever streaming service you're using when you use
[01:02:07.900 --> 01:02:08.900]   that app.
[01:02:08.900 --> 01:02:14.380]   That's an extra bonus thing you can get, a benefit of using a VPN on top of all the other
[01:02:14.380 --> 01:02:15.540]   ones.
[01:02:15.540 --> 01:02:17.940]   The reason why I like ExpressVPN is that it's easy.
[01:02:17.940 --> 01:02:20.140]   You fire up the app, right?
[01:02:20.140 --> 01:02:23.100]   You can change your location of the server with one click.
[01:02:23.100 --> 01:02:26.940]   When it's on, which is easy to do, you just click to turn it on, you just use all your
[01:02:26.940 --> 01:02:29.180]   websites and apps like normal.
[01:02:29.180 --> 01:02:32.300]   And all this happens transparently in the background.
[01:02:32.300 --> 01:02:36.060]   It works on phones, laptops, tablets, even smart TVs and more.
[01:02:36.060 --> 01:02:37.060]   It's super fast.
[01:02:37.060 --> 01:02:38.060]   It's got high bandwidth.
[01:02:38.060 --> 01:02:39.860]   There are servers all around the world.
[01:02:39.860 --> 01:02:42.740]   So like there's probably one nearby to get the fastest speed.
[01:02:42.740 --> 01:02:45.020]   You can stream in HD with zero buffering through it.
[01:02:45.020 --> 01:02:48.460]   So it's got great sort of best in class speed.
[01:02:48.460 --> 01:02:53.980]   It's rated number one by top tech reviewers like CNET and The Verge.
[01:02:53.980 --> 01:02:58.860]   That's why of the VPNs that are out there, I recommend ExpressVPN.
[01:02:58.860 --> 01:03:03.320]   Right now you can take advantage of ExpressVPN's Black Friday Cyber Monday offer to get the
[01:03:03.320 --> 01:03:07.460]   absolute best VPN deal you'll find all year.
[01:03:07.460 --> 01:03:15.540]   Use my special link expressvpn.com/deep and you'll get four extra months with the 12-month
[01:03:15.540 --> 01:03:21.380]   plan or six extra months with the 24-month plan totally free.
[01:03:21.380 --> 01:03:28.660]   That's expressvpn.com/deep to get an extra four or even six months of ExpressVPN for
[01:03:28.660 --> 01:03:29.660]   free.
[01:03:29.660 --> 01:03:35.780]   I also want to talk about our friends at Shopify.
[01:03:35.780 --> 01:03:42.260]   When you think about businesses whose sales are rocketing like Feastables by Mr. Beast
[01:03:42.260 --> 01:03:48.140]   or Thrive Cosmetics or Silicon Valley's Weekend Uniform supplier Cotopaxi, you think about
[01:03:48.140 --> 01:03:54.860]   an innovative product or a progressive brand or buttoned down marketing.
[01:03:54.860 --> 01:04:01.700]   But an often overlooked secret is how these brands actually do their selling.
[01:04:01.700 --> 01:04:06.480]   The experience of buying from these brands online.
[01:04:06.480 --> 01:04:10.700]   These brands, along with millions of others, use Shopify.
[01:04:10.700 --> 01:04:16.100]   All right, nobody does selling better than Shopify.
[01:04:16.100 --> 01:04:20.420]   It's home of the number one checkout on the planet and the not so secret secret which
[01:04:20.420 --> 01:04:24.940]   is ShopPay that boost conversions up to 50%.
[01:04:24.940 --> 01:04:31.460]   This means that way less carts go abandoned and way more sales get done.
[01:04:31.460 --> 01:04:35.260]   So if you're growing your business, your commerce platform better be ready to sell wherever
[01:04:35.260 --> 01:04:37.820]   your customers are scrolling or strolling.
[01:04:37.820 --> 01:04:42.020]   On the web, in your store, in their feed, and everywhere in between.
[01:04:42.020 --> 01:04:46.820]   Businesses that sell more sell on Shopify.
[01:04:46.820 --> 01:04:53.500]   Upgrade your business and get the same checkout that Feastables or Thrive or Cotopaxi use
[01:04:53.500 --> 01:04:55.940]   when you use Shopify.
[01:04:55.940 --> 01:05:00.340]   Sign up for your $1 per month trial period at shopify.com/deep.
[01:05:00.340 --> 01:05:03.340]   But type that in all lowercase.
[01:05:03.340 --> 01:05:08.980]   Go to shopify.com/deep all lowercase to upgrade your selling today, shopify.com/deep.
[01:05:08.980 --> 01:05:13.220]   All right, Jesse, let's go to our final segment.
[01:05:13.220 --> 01:05:20.680]   All right, for our final segment today, we want to do a triumphant return to my tech
[01:05:20.680 --> 01:05:27.680]   corner segment where we get into a technical topic that is relevant to the type of things
[01:05:27.680 --> 01:05:28.680]   we talk about today.
[01:05:28.680 --> 01:05:32.480]   So I put on my computer science hat a little bit to help give us some more insight on topics
[01:05:32.480 --> 01:05:35.200]   relevant to living a deep life in a distracted world.
[01:05:35.200 --> 01:05:39.360]   All right, in today's tech corner, I want to talk about how do recommendation algorithms
[01:05:39.360 --> 01:05:40.360]   work.
[01:05:40.360 --> 01:05:41.360]   Why?
[01:05:41.360 --> 01:05:47.400]   Because, in part, this is really relevant to the ongoing discussion about social media
[01:05:47.400 --> 01:05:49.960]   and social media regulation.
[01:05:49.960 --> 01:05:57.440]   So if we look at some of the new child safety legislation like COSA or COPA 2.0 or California's
[01:05:57.440 --> 01:06:03.380]   big law, we see that one of the things that they are pushing for is that when kids are
[01:06:03.380 --> 01:06:09.720]   using social media that we have to be careful about what it does recommend or not recommend.
[01:06:09.720 --> 01:06:10.720]   Right?
[01:06:10.720 --> 01:06:16.440]   So you sort of see these arguments, okay, we're not talking about censoring information
[01:06:16.440 --> 01:06:19.840]   that can exist on social media, but we want to be careful about what we recommend or don't
[01:06:19.840 --> 01:06:21.120]   recommend.
[01:06:21.120 --> 01:06:25.320]   We also see this in discussions about things like Twitter or Twitter alternatives like
[01:06:25.320 --> 01:06:30.720]   Threads or Blue Sky, where there's often this notion of the recommendation algorithm can
[01:06:30.720 --> 01:06:32.640]   be tuned up or tuned down.
[01:06:32.640 --> 01:06:35.920]   We saw this a lot with the discussions around Threads when it was released, that they were
[01:06:35.920 --> 01:06:40.800]   tuning down, they claim, the political nature of content and tuning up recommendations for
[01:06:40.800 --> 01:06:41.800]   others.
[01:06:41.800 --> 01:06:48.080]   There's this idea that there is a "algorithm" that is in charge of showing us stuff, and
[01:06:48.080 --> 01:06:53.640]   this algorithm is really important, and we can change this algorithm to change the experience
[01:06:53.640 --> 01:06:56.480]   or maybe strip it out altogether and have an experience without it.
[01:06:56.480 --> 01:06:59.920]   It is at the core of many discussions around social media and its harm, so I thought we
[01:06:59.920 --> 01:07:05.560]   would talk about, well, how do these algorithms actually work?
[01:07:05.560 --> 01:07:11.960]   So what I'm going to do here is greatly simplify the idea of how a sort of machine learning-based
[01:07:11.960 --> 01:07:14.920]   optimization recommendation algorithm actually works.
[01:07:14.920 --> 01:07:21.040]   I want to start by saying there is a spectrum on which these algorithms exist.
[01:07:21.040 --> 01:07:24.360]   So if we look in the social media ecosystem, on one end of the spectrum will be something
[01:07:24.360 --> 01:07:28.960]   like Twitter, which actually is relatively non-algorithmic.
[01:07:28.960 --> 01:07:34.440]   The way curation decisions are made on Twitter, a lot of it is actually cybernetic, which
[01:07:34.440 --> 01:07:39.440]   means it's based on individual humans' decisions to retweet or not, and when those are combined
[01:07:39.440 --> 01:07:43.880]   with the network structure of Twitter, which has power law dynamics, it's really good at
[01:07:43.880 --> 01:07:49.080]   sort of selecting for certain content to have explosive growth and start trending.
[01:07:49.080 --> 01:07:50.680]   But it's largely non-algorithmic.
[01:07:50.680 --> 01:07:53.120]   It's actually just the aggregate of a lot of human decisions.
[01:07:53.120 --> 01:08:00.400]   On the other end of the spectrum is TikTok, which is essentially entirely algorithmic.
[01:08:00.400 --> 01:08:03.920]   It doesn't care who you follow or don't follow or what other people like.
[01:08:03.920 --> 01:08:09.080]   It just uses an algorithm to select what to show you next, then what to show you next,
[01:08:09.080 --> 01:08:10.480]   and what to show you next.
[01:08:10.480 --> 01:08:13.800]   So we're going to be leaning more towards that TikTok side, where really it's like a
[01:08:13.800 --> 01:08:17.520]   computer is deciding, not other humans, what it is you should see.
[01:08:17.520 --> 01:08:20.480]   I'm going to give you a highly simplified way of thinking about this, then we can draw
[01:08:20.480 --> 01:08:23.080]   some conclusions from it afterwards.
[01:08:23.080 --> 01:08:24.720]   All right.
[01:08:24.720 --> 01:08:30.760]   So let's pretend, for the sake of this example, that we're building an algorithm to recommend
[01:08:30.760 --> 01:08:31.760]   TikTok videos.
[01:08:31.760 --> 01:08:35.120]   And I am going to do a lot of drawing here, God help us.
[01:08:35.120 --> 01:08:40.320]   So if you're listening instead of watching, you might want to actually load up the YouTube
[01:08:40.320 --> 01:08:41.320]   version of this.
[01:08:41.320 --> 01:08:42.320]   This is, what are we Jesse?
[01:08:42.320 --> 01:08:43.320]   This is episode 327.
[01:08:43.320 --> 01:08:44.320]   Is that right?
[01:08:44.320 --> 01:08:45.320]   Yep.
[01:08:45.320 --> 01:08:46.320]   All right.
[01:08:46.320 --> 01:08:50.920]   So you just go to, what, the deeplife.com/listen, look for episode 327.
[01:08:50.920 --> 01:08:51.920]   You'll see the video there.
[01:08:51.920 --> 01:08:56.680]   It usually comes up within the same day or the day after the episode lands.
[01:08:56.680 --> 01:08:57.680]   Okay.
[01:08:57.680 --> 01:09:03.800]   So we're TikTok, and we want to recommend videos to a user.
[01:09:03.800 --> 01:09:10.920]   So we need ways, first of all, of describing the videos we have in our collection, and
[01:09:10.920 --> 01:09:13.740]   we want to describe them in a way computers can understand.
[01:09:13.740 --> 01:09:15.360]   So we want to use numbers.
[01:09:15.360 --> 01:09:17.880]   So let's start really simple.
[01:09:17.880 --> 01:09:22.800]   Let's say we're going to assign a single number to every video that we're going to use to
[01:09:22.800 --> 01:09:25.540]   help describe it.
[01:09:25.540 --> 01:09:28.440]   All right.
[01:09:28.440 --> 01:09:32.880]   Hold on a second.
[01:09:32.880 --> 01:09:33.880]   Let me press again.
[01:09:33.880 --> 01:09:34.880]   There we go.
[01:09:34.880 --> 01:09:35.880]   All right.
[01:09:35.880 --> 01:09:36.880]   So now I can draw.
[01:09:36.880 --> 01:09:37.880]   All right.
[01:09:37.880 --> 01:09:43.800]   So we're going to have videos, we're going to describe each videos with a single number.
[01:09:43.800 --> 01:09:50.000]   Let's say, you know, this number, for example, here is going to describe for each video,
[01:09:50.000 --> 01:09:52.800]   the number of cats in that video.
[01:09:52.800 --> 01:09:57.160]   And so we have this a single number on which we can categorize videos.
[01:09:57.160 --> 01:10:03.720]   I drew a line here, and we can imagine this as this is our space in which videos can fall.
[01:10:03.720 --> 01:10:07.280]   And I'm sort of adding numbers to this line.
[01:10:07.280 --> 01:10:13.920]   And in this very simple example, kind of numbered from, you know, zero up to eight.
[01:10:13.920 --> 01:10:16.800]   We could use yellow dots in this example to be different videos, and we can just sort
[01:10:16.800 --> 01:10:21.520]   of place them on this line, depending on how many cats they have in them.
[01:10:21.520 --> 01:10:28.560]   So there's a couple videos with a lot of cats, and some five, a couple over here, and I don't
[01:10:28.560 --> 01:10:31.520]   know, we have fractional numbers of cats, whatever.
[01:10:31.520 --> 01:10:35.600]   So we're describing all these videos by a single number.
[01:10:35.600 --> 01:10:36.940]   Okay.
[01:10:36.940 --> 01:10:41.260]   In this simple example, now let's let a user come along.
[01:10:41.260 --> 01:10:48.840]   And what we do is we want to look at the videos that you are looking at.
[01:10:48.840 --> 01:10:53.700]   And let's say we want to categorize them simply as a video you like or don't like.
[01:10:53.700 --> 01:10:56.960]   So in like the Facebook days, there'd be an actual like button.
[01:10:56.960 --> 01:11:01.000]   The way we think TikTok works is that it actually looks at how long you watch a video.
[01:11:01.000 --> 01:11:03.820]   So if you quickly swipe to the next video, you don't like it.
[01:11:03.820 --> 01:11:07.680]   If you watch it long enough, then we can consider that you do like it.
[01:11:07.680 --> 01:11:13.140]   So we're gonna start showing you videos, and we are going to start, let's say, let's just
[01:11:13.140 --> 01:11:17.780]   keep track of the videos you like, so the videos that you actually watch for a little
[01:11:17.780 --> 01:11:18.780]   while.
[01:11:18.780 --> 01:11:27.260]   And I'm gonna plot those on this same one-dimensional line here with a purple dot, and so maybe
[01:11:27.260 --> 01:11:33.420]   you like a couple videos with five cats, you like one with zero cats, six cats, there's
[01:11:33.420 --> 01:11:38.540]   a three-cat one, another six-cat, maybe one eight, maybe another couple more zero ones.
[01:11:38.540 --> 01:11:44.260]   So I'm just keeping track of, okay, these videos you liked, where did they fall in this
[01:11:44.260 --> 01:11:47.660]   range of number of cats?
[01:11:47.660 --> 01:11:51.300]   Now after we've done this for a while, what I can do, and this is how these sort of basic
[01:11:51.300 --> 01:11:58.820]   algorithms work in a very simplistic way, I can say, okay, where on average, where on
[01:11:58.820 --> 01:12:04.780]   average are these videos you like falling on this single value I care about?
[01:12:04.780 --> 01:12:07.180]   And there's different ways to do this.
[01:12:07.180 --> 01:12:11.880]   You can think about what we're trying to do here is basically find the average point,
[01:12:11.880 --> 01:12:16.460]   think about this as like we're trying to find a point that has like the best overall distance
[01:12:16.460 --> 01:12:24.180]   to all of your points, it's interesting, my controller is weird.
[01:12:24.180 --> 01:12:29.200]   In reality, the way this is typically done is actually trying to minimize the average
[01:12:29.200 --> 01:12:31.660]   square of the distances, don't worry about that here.
[01:12:31.660 --> 01:12:35.980]   What I'm trying to do here is sort of find a point on here that's sort of in the middle,
[01:12:35.980 --> 01:12:39.500]   it's the average, it's minimizing distance to all of your likes.
[01:12:39.500 --> 01:12:42.780]   So you have a bunch of zeros you like here, but you have like a bunch of fives, sixes
[01:12:42.780 --> 01:12:43.780]   and eights.
[01:12:43.780 --> 01:12:48.460]   So, you know, maybe your average is like right where that X is, that's kind of the center
[01:12:48.460 --> 01:12:53.560]   point of where the videos you like fall.
[01:12:53.560 --> 01:12:59.300]   So now when it comes time for me as TikTok to show you another video, what I can do to
[01:12:59.300 --> 01:13:04.780]   be smarter is say, great, I'm going to randomly select you a video from all the videos that
[01:13:04.780 --> 01:13:10.460]   exist, but I'm going to weight the probability that I select a given video depending on how
[01:13:10.460 --> 01:13:16.940]   close it is to this point that we said was kind of at the center of your preferences.
[01:13:16.940 --> 01:13:21.300]   So in here, right, this point is like somewhere between four and five cats is kind of like
[01:13:21.300 --> 01:13:24.780]   the center of your preferences when we measure videos by cats.
[01:13:24.780 --> 01:13:30.100]   So you know, it's possible that I could select you a video out here, but I'm much more likely
[01:13:30.100 --> 01:13:31.220]   to select the videos around here.
[01:13:31.220 --> 01:13:35.020]   So you're gonna get a lot of videos with like four or five cats and sometimes some videos
[01:13:35.020 --> 01:13:37.660]   with less cats and sometimes some videos with more.
[01:13:37.660 --> 01:13:40.820]   But pretty soon you're gonna be like, wow, this is eerie, TikTok has really figured out
[01:13:40.820 --> 01:13:44.220]   that there's kind of, I like videos that have, you know, like a couple armfuls of cats in
[01:13:44.220 --> 01:13:45.700]   them.
[01:13:45.700 --> 01:13:51.380]   Again, this is simplified, but it roughly gets to how these type of things work.
[01:13:51.380 --> 01:13:55.740]   All right, so this is a single number.
[01:13:55.740 --> 01:14:00.780]   Of course, these videos are going to have more dimensions on which we're going to want
[01:14:00.780 --> 01:14:01.820]   to measure them.
[01:14:01.820 --> 01:14:07.940]   But that's okay, because the same thing works even as we go to more dimensions.
[01:14:07.940 --> 01:14:13.420]   So maybe we say, okay, there's two numbers, let me select this here, maybe there's two
[01:14:13.420 --> 01:14:18.300]   numbers by which we want to describe all of our videos.
[01:14:18.300 --> 01:14:26.220]   So one number is the number of cats, and then the another number is like the number of skeletons.
[01:14:26.220 --> 01:14:29.620]   So we could just draw this if you're looking at the screen here as just like another axis,
[01:14:29.620 --> 01:14:31.740]   like now we're in a two dimensional space.
[01:14:31.740 --> 01:14:33.700]   And again, we can do the same thing.
[01:14:33.700 --> 01:14:35.820]   All the videos fall somewhere.
[01:14:35.820 --> 01:14:38.500]   Every video has a spot somewhere in here.
[01:14:38.500 --> 01:14:43.040]   You know, a video with seven cats and one, two, three skeletons would show up right here
[01:14:43.040 --> 01:14:44.040]   in this space.
[01:14:44.040 --> 01:14:49.460]   A video with like zero cats and four skeletons might be over here.
[01:14:49.460 --> 01:14:56.300]   And again, we see, we plot every time you like a video, we kind of plot it in this two
[01:14:56.300 --> 01:15:00.980]   dimensional space, and we can do the exact same thing we did before, where we find like
[01:15:00.980 --> 01:15:06.100]   roughly speaking where the center is by some sort of center metric, okay, roughly speaking,
[01:15:06.100 --> 01:15:08.420]   this is the center of all the videos you've liked.
[01:15:08.420 --> 01:15:11.460]   And so now when we randomly select videos, they're going to be kind of roughly in this
[01:15:11.460 --> 01:15:14.860]   range, like you're gonna see a lot of stuff with a good number of cats and a fair number
[01:15:14.860 --> 01:15:16.500]   of skeletons.
[01:15:16.500 --> 01:15:19.380]   And like, you're very rarely going to see something with like a bunch of cats and a
[01:15:19.380 --> 01:15:23.580]   bunch of skeletons or no cats and you know, whatever, right?
[01:15:23.580 --> 01:15:25.500]   We could do this with three numbers.
[01:15:25.500 --> 01:15:29.300]   Now we would be in three dimensional space and you could imagine there's regions where
[01:15:29.300 --> 01:15:31.500]   you have lots of videos you like in that three dimensional space.
[01:15:31.500 --> 01:15:36.100]   And when we randomly select videos, we select them near there, we can expend the number
[01:15:36.100 --> 01:15:40.340]   of numbers we use to describe these videos, they can get much larger.
[01:15:40.340 --> 01:15:44.940]   And something like TikTok is going to have probably thousands of different numbers, each
[01:15:44.940 --> 01:15:48.300]   describing different parts of these videos.
[01:15:48.300 --> 01:15:52.580]   Now we can't draw this, once we get past three numbers, we can't really draw these in a way
[01:15:52.580 --> 01:15:56.700]   that makes sense to us, but the same mathematics works.
[01:15:56.700 --> 01:15:59.980]   So the videos are described by a ton of numbers.
[01:15:59.980 --> 01:16:03.020]   We keep track of the videos you like.
[01:16:03.020 --> 01:16:09.340]   And then we can select for videos that are in some sense close to the clusters of videos
[01:16:09.340 --> 01:16:10.340]   that you like.
[01:16:10.340 --> 01:16:14.020]   All right, two complications here.
[01:16:14.020 --> 01:16:18.900]   What if you like multiple, there's, if we look in this region where you have a bunch
[01:16:18.900 --> 01:16:23.860]   of clustered videos you like, what if there's like multiple types of videos you like?
[01:16:23.860 --> 01:16:29.700]   This just shows up as like multiple clusters, kind of like multiple clusters in this multidimensional
[01:16:29.700 --> 01:16:32.060]   space of videos that you like.
[01:16:32.060 --> 01:16:34.260]   We have ways of finding a bunch of different center points.
[01:16:34.260 --> 01:16:35.740]   We do things like k-means averaging.
[01:16:35.740 --> 01:16:39.480]   Okay, there's a bunch of different center points that each correspond to like a type
[01:16:39.480 --> 01:16:40.740]   of video that you like.
[01:16:40.740 --> 01:16:46.780]   And so now when we randomly select a video to show you, we're giving extra probabilities
[01:16:46.780 --> 01:16:49.600]   to anything near one of these clusters, and the bigger the cluster, the more likely we
[01:16:49.600 --> 01:16:52.380]   are to show you a video from there.
[01:16:52.380 --> 01:16:56.300]   The other complication, well, how do we know if you like something that you've never seen
[01:16:56.300 --> 01:16:57.300]   before?
[01:16:57.300 --> 01:17:03.400]   TikTok answers this by alternating between just purely showing you something weighted
[01:17:03.400 --> 01:17:06.940]   towards the things you like versus showing you something new.
[01:17:06.940 --> 01:17:12.840]   So it will opportunistically show you new things just to see, give you a chance to show
[01:17:12.840 --> 01:17:15.200]   a preference for things you've never seen before.
[01:17:15.200 --> 01:17:19.060]   That's why when you use TikTok at first, it kind of drifts over time until you finally
[01:17:19.060 --> 01:17:20.900]   stabilize into the clusters you like.
[01:17:20.900 --> 01:17:26.180]   It'll show you a lot more random stuff at first to try to see what you like.
[01:17:26.180 --> 01:17:28.460]   It's like very roughly speaking, something like this is going on.
[01:17:28.460 --> 01:17:33.260]   All right, so here's some conclusions about this.
[01:17:33.260 --> 01:17:37.500]   These algorithms are automatic and agnostic to content details, right?
[01:17:37.500 --> 01:17:46.040]   It's not computer code where you can come in and it has in there political content,
[01:17:46.040 --> 01:17:52.040]   unsafe for kids content, sports content, and you can turn a knob, let's turn down politics
[01:17:52.040 --> 01:17:58.220]   and turn up sports content, or turn down controversial and turn up non-controversial.
[01:17:58.220 --> 01:17:59.660]   It's agnostic to that.
[01:17:59.660 --> 01:18:04.400]   It has all these numbers, most of which, by the way, are figured out using embedding tools
[01:18:04.400 --> 01:18:07.740]   that are machine learning tools so that you don't know what they are in advance, right?
[01:18:07.740 --> 01:18:10.280]   You're not choosing what these numbers are.
[01:18:10.280 --> 01:18:12.960]   The software just figuring out what numbers matter.
[01:18:12.960 --> 01:18:17.360]   That's just automatically plotting your videos that you like or don't like and finding these
[01:18:17.360 --> 01:18:20.280]   sort of center spaces in the space and randomly selecting.
[01:18:20.280 --> 01:18:23.420]   The algorithm has no idea what these spaces are from a content point of view.
[01:18:23.420 --> 01:18:24.840]   It's agnostic to that.
[01:18:24.840 --> 01:18:28.480]   It's selecting vectors that are weighted to be near other vectors that you've expressed
[01:18:28.480 --> 01:18:31.080]   preferences for.
[01:18:31.080 --> 01:18:33.860]   So it can be remarkably effective.
[01:18:33.860 --> 01:18:37.760]   That's why when you purify these algorithms, like TikTok does, it seems eerie, like how
[01:18:37.760 --> 01:18:45.640]   did TikTok figure out that, you know, I like videos about, you know, bears working on crafts?
[01:18:45.640 --> 01:18:49.900]   But this type of exploration of the space and weighted selection will pretty quickly
[01:18:49.900 --> 01:18:54.240]   cluster these things together and the intersection clusters will have a lot of weight.
[01:18:54.240 --> 01:18:56.460]   It will just automatically find these things.
[01:18:56.460 --> 01:19:00.200]   It seems very eerie, but it's actually quite simplistic mathematically what's happening
[01:19:00.200 --> 01:19:02.520]   underneath.
[01:19:02.520 --> 01:19:09.760]   But because it's automatic, they're not nearly as controllable as we think.
[01:19:09.760 --> 01:19:13.620]   Controlling these type of recommendation algorithms is difficult because of their automatic content
[01:19:13.620 --> 01:19:16.520]   agnostic nature.
[01:19:16.520 --> 01:19:23.760]   What we end up needing to do is things like human-in-the-loop dead zone definitions.
[01:19:23.760 --> 01:19:27.360]   So we show a lot of content to real people and we say, "Here's the type of stuff we're
[01:19:27.360 --> 01:19:28.920]   worried about."
[01:19:28.920 --> 01:19:33.920]   And when they see the stuff that, to their human intuition, matches things we're worried
[01:19:33.920 --> 01:19:34.920]   about, they kind of hit a button.
[01:19:34.920 --> 01:19:35.920]   Okay, that's bad.
[01:19:35.920 --> 01:19:36.920]   That's bad.
[01:19:36.920 --> 01:19:37.920]   That's bad.
[01:19:37.920 --> 01:19:41.560]   And they create what you can think of as a like dislike plots in this space.
[01:19:41.560 --> 01:19:45.360]   And then you can find the sort of centers of these spaces of stuff that people or testers
[01:19:45.360 --> 01:19:49.360]   said was bad, and you can reduce weight for videos near those.
[01:19:49.360 --> 01:19:55.360]   It could give you sort of negative probability weight if you're near one of those zones,
[01:19:55.360 --> 01:19:56.360]   right?
[01:19:56.360 --> 01:19:57.720]   But this is, again, it's kind of indirect.
[01:19:57.720 --> 01:20:00.960]   It's not just you coming in saying, "Don't do this type of content."
[01:20:00.960 --> 01:20:05.700]   You have to have humans calling stuff bad, and that translates into this inscrutable
[01:20:05.700 --> 01:20:08.680]   multidimensional space, and it sort of affects the weight.
[01:20:08.680 --> 01:20:14.560]   So it's kind of an imperfect way of trying to tame this algorithm.
[01:20:14.560 --> 01:20:18.420]   Stuff that the human testers haven't seen or clicked on is going to be treated like
[01:20:18.420 --> 01:20:20.480]   anything else.
[01:20:20.480 --> 01:20:24.160]   And so these algorithms, we just, we have to keep this in mind.
[01:20:24.160 --> 01:20:29.580]   Algorithms are automatic and mathematical and not easily tamable in a sort of human
[01:20:29.580 --> 01:20:32.200]   understandable way.
[01:20:32.200 --> 01:20:37.280]   So when thinking about reforms of these technologies, do not think about like a newspaper editor
[01:20:37.280 --> 01:20:38.280]   who's making decisions.
[01:20:38.280 --> 01:20:41.960]   You could just say, "Hey, do less of this."
[01:20:41.960 --> 01:20:43.840]   It's much more automatic than that.
[01:20:43.840 --> 01:20:50.320]   It can give you like eerily successful results in terms of honing in on your interest, but
[01:20:50.320 --> 01:20:54.040]   it's also very hard to keep an algorithm like that successful.
[01:20:54.040 --> 01:20:58.280]   And somehow have it avoid lots of stuff, because it doesn't know what stuff means.
[01:20:58.280 --> 01:21:01.440]   Humans have to get in there, and it's messy at best.
[01:21:01.440 --> 01:21:03.920]   So anyways, I hear a lot about algorithms.
[01:21:03.920 --> 01:21:07.400]   They're often discussed to be these like highly tunable, understandable things.
[01:21:07.400 --> 01:21:12.640]   They're simple algorithmically, but complicated in their effect and complicated to tame.
[01:21:12.640 --> 01:21:14.400]   So there you go, Jesse.
[01:21:14.400 --> 01:21:19.760]   We did philosophy and computer science in the same episode.
[01:21:19.760 --> 01:21:24.440]   We just kind of got our nerd bona fides up here, probably also lost half our audience.
[01:21:24.440 --> 01:21:26.640]   You got a professor, guys.
[01:21:26.640 --> 01:21:27.640]   You have a professor podcasting.
[01:21:27.640 --> 01:21:28.640]   Sometimes you're gonna get some of that.
[01:21:28.640 --> 01:21:30.240]   Anyway, so thank you all for listening.
[01:21:30.240 --> 01:21:31.840]   We'll be back next week with another episode.
[01:21:31.840 --> 01:21:34.040]   It'll be a little bit less heady next week.
[01:21:34.040 --> 01:21:35.040]   We'll see.
[01:21:35.040 --> 01:21:36.040]   See what the feedback is.
[01:21:36.040 --> 01:21:38.080]   But until then, as always, stay deep.
[01:21:38.080 --> 01:21:42.320]   Hey, if you liked today's discussion about the philosophical underpinnings of digital
[01:21:42.320 --> 01:21:48.980]   minimalism, I think you'll also like episode 298 about intentional information in which
[01:21:48.980 --> 01:21:52.800]   we go deep at understanding the role of information and human flourishing.
[01:21:52.800 --> 01:21:53.880]   Check it out.
[01:21:53.880 --> 01:21:54.880]   I think you'll like it.
[01:21:54.880 --> 01:22:02.520]   So today I'm going to argue that we misunderstand the impact of how we obtain information on
[01:22:02.520 --> 01:22:05.320]   the overall quality of our lives.

