
[00:00:00.000 --> 00:00:04.000]   And I'm going to introduce Boris, who works with us.
[00:00:04.000 --> 00:00:09.800]   He's amazing and he built something really cool that especially those of you who are
[00:00:09.800 --> 00:00:12.640]   into NLP are really going to enjoy.
[00:00:12.640 --> 00:00:14.480]   So we're going to kick things off.
[00:00:14.480 --> 00:00:23.000]   I'm going to invite Boris to take the floor and show us what he's got for us.
[00:00:23.000 --> 00:00:30.000]   Boris, we can't hear you if you want to unmute yourself.
[00:00:30.000 --> 00:00:31.000]   >> All right.
[00:00:31.000 --> 00:00:35.000]   I guess you can hear me better now.
[00:00:35.000 --> 00:00:36.000]   >> Yes.
[00:00:36.000 --> 00:00:39.000]   >> So let me move that out.
[00:00:39.000 --> 00:00:40.000]   Okay.
[00:00:40.000 --> 00:00:41.000]   Hi, everybody.
[00:00:41.000 --> 00:00:42.000]   My name is Boris.
[00:00:42.000 --> 00:00:48.000]   Today I'm going to talk to you about a little project that I did recently that I think is
[00:00:48.000 --> 00:00:49.000]   pretty fun.
[00:00:49.000 --> 00:00:57.000]   I called it Hugging Tweets because it's using a hugging face on Twitter.
[00:00:57.000 --> 00:01:05.000]   So how do I show it the way I want?
[00:01:05.000 --> 00:01:06.000]   Anyway.
[00:01:06.000 --> 00:01:07.000]   Sorry.
[00:01:07.000 --> 00:01:13.000]   Let me just adjust my view.
[00:01:13.000 --> 00:01:15.000]   Sorry.
[00:01:15.000 --> 00:01:21.000]   Okay.
[00:01:21.000 --> 00:01:22.000]   It doesn't really matter.
[00:01:22.000 --> 00:01:29.000]   So the idea is basically to be able to generate tweets using hugging face.
[00:01:29.000 --> 00:01:35.000]   And I'm going to show you quickly the library, but very quickly who I am.
[00:01:35.000 --> 00:01:39.000]   I graduated from France and a school in Brazil.
[00:01:39.000 --> 00:01:46.000]   And I studied my background was mainly related to math, science, engineering, computer science.
[00:01:46.000 --> 00:01:49.000]   I was actually before working for the energy industry.
[00:01:49.000 --> 00:01:51.000]   So something totally unrelated.
[00:01:51.000 --> 00:01:55.000]   And something that's pretty cool, I think, about machine learning in general, and probably
[00:01:55.000 --> 00:02:01.000]   a lot of you are in the same place, is we've been able to learn ourselves with all the
[00:02:01.000 --> 00:02:02.000]   resources online.
[00:02:02.000 --> 00:02:08.000]   So I did a lot of the Coursera, Udacity, and all kind of public channels.
[00:02:08.000 --> 00:02:14.000]   And then I was able to learn a bit more about reading the publications that interest me.
[00:02:14.000 --> 00:02:17.000]   And then finally doing my own projects.
[00:02:17.000 --> 00:02:26.000]   So I've been involved doing some kind of AI machine learning projects with Waze and Biosys
[00:02:26.000 --> 00:02:30.000]   now for a little while and started with a competition about colorizing.
[00:02:30.000 --> 00:02:35.000]   And I've been very lucky to work with them because I've been able to do a lot of really
[00:02:35.000 --> 00:02:36.000]   cool projects.
[00:02:36.000 --> 00:02:40.000]   So I'm excited to show you the one that I'm doing today.
[00:02:40.000 --> 00:02:43.000]   So quickly, what is Hugging Tweets?
[00:02:43.000 --> 00:02:52.000]   So the idea is basically you have a user that you choose with your handle.
[00:02:52.000 --> 00:02:57.000]   So for people who shouldn't recognize it, it's André Karpathy, who was like with another
[00:02:57.000 --> 00:03:02.000]   director of AI at the time, does a lot of really cool stuff.
[00:03:02.000 --> 00:03:09.000]   And then what you want to do, you want to create a Hugging Face model with it and train
[00:03:09.000 --> 00:03:15.000]   it by downloading all the data from André Karpathy and optimize it, fine-tune the model
[00:03:15.000 --> 00:03:16.000]   on that.
[00:03:16.000 --> 00:03:18.000]   Then the idea is you just give an input sentence.
[00:03:18.000 --> 00:03:20.000]   So you just give a start.
[00:03:20.000 --> 00:03:24.000]   So for example, I put, "I don't like," and this is what you put.
[00:03:24.000 --> 00:03:29.000]   I think that it is pretty amazing because like I let you read it, but like, "I don't
[00:03:29.000 --> 00:03:30.000]   like this."
[00:03:30.000 --> 00:03:36.000]   "920 AM, forget this little low-code and preprocessor optimization, even if it's neat for top-level
[00:03:36.000 --> 00:03:37.000]   projects."
[00:03:37.000 --> 00:03:39.000]   "927 AM, other useful code examples."
[00:03:39.000 --> 00:03:41.000]   It's not kind of base code.
[00:03:41.000 --> 00:03:44.000]   "937 AM, Python during bug like crazy.
[00:03:44.000 --> 00:03:46.000]   Restarts regular web browsing."
[00:03:46.000 --> 00:03:48.000]   "946 AM, okay, I don't mind.
[00:03:48.000 --> 00:03:49.000]   Maybe I should try that out.
[00:03:49.000 --> 00:03:50.000]   I'll investigate it."
[00:03:50.000 --> 00:03:55.000]   "10 AM, I think I should try Shimigitsu in gear page or the minimalist website if you're
[00:03:55.000 --> 00:03:57.000]   after 10 results."
[00:03:57.000 --> 00:04:00.000]   "But so maybe Google ImageNet on Yelp instead."
[00:04:00.000 --> 00:04:02.000]   "10.05, looking forward to watching it talk."
[00:04:02.000 --> 00:04:07.000]   That's like all really crazy and like impossible to understand, but I feel like it's something
[00:04:07.000 --> 00:04:10.000]   that André Schappa could actually have said.
[00:04:10.000 --> 00:04:15.000]   So it's kind of fun to see that happening.
[00:04:15.000 --> 00:04:17.000]   So now I'm going to show you quickly how it works.
[00:04:17.000 --> 00:04:19.000]   So what is Hugging Face?
[00:04:19.000 --> 00:04:24.000]   For the people who don't know it, honestly, they have a really, really cool documentation.
[00:04:24.000 --> 00:04:27.000]   I would recommend to just go on their website.
[00:04:27.000 --> 00:04:31.000]   The first thing I do when I want to learn about something, I just go on their website
[00:04:31.000 --> 00:04:36.000]   and I go on the docs and I just read all of it.
[00:04:36.000 --> 00:04:41.000]   And some people approach it a bit differently, but I think their documentation is really,
[00:04:41.000 --> 00:04:43.000]   really interesting and really well done.
[00:04:43.000 --> 00:04:47.000]   So I'm going to summarize it really quickly in a few words.
[00:04:47.000 --> 00:04:53.000]   Basically, they have a library that's using Python that's used for natural language processing,
[00:04:53.000 --> 00:04:55.000]   understanding, and generation.
[00:04:55.000 --> 00:05:00.000]   And they provide you some tokenizers to process some text data.
[00:05:00.000 --> 00:05:07.000]   So typically, when you train a model, you won't be able to input directly the text.
[00:05:07.000 --> 00:05:14.000]   You could convert it into some direct input, like character by character or sentence by sentence.
[00:05:14.000 --> 00:05:19.000]   But typically, now there's more advanced tokenizer that does something a bit more complex and
[00:05:19.000 --> 00:05:21.000]   that gives you much better results.
[00:05:21.000 --> 00:05:24.000]   They have a lot of pre-trained models in PyTorch and TensorFlow.
[00:05:24.000 --> 00:05:29.000]   They also let you convert from one to the other, which is pretty cool because some people
[00:05:29.000 --> 00:05:34.000]   like to develop in PyTorch, but maybe they use TensorFlow as production models.
[00:05:34.000 --> 00:05:39.000]   And then something that's really interesting, actually, I'm going back to their website.
[00:05:39.000 --> 00:05:45.000]   I recommend really that for most of the tasks, what you do, they have an examples page.
[00:05:45.000 --> 00:05:52.000]   And basically, on that page, they created scripts that can solve most of the problems
[00:05:52.000 --> 00:05:54.000]   that we want.
[00:05:54.000 --> 00:05:59.000]   If they don't solve it directly, which really, really, they're applicable to a lot of different
[00:05:59.000 --> 00:06:01.000]   tasks.
[00:06:01.000 --> 00:06:04.000]   But if they don't solve it directly, you just choose whatever is the most common.
[00:06:04.000 --> 00:06:05.000]   You look at it.
[00:06:05.000 --> 00:06:07.000]   They're really well written, really clean.
[00:06:07.000 --> 00:06:12.000]   You have arguments if you want to do gradient accumulation.
[00:06:12.000 --> 00:06:16.000]   You do have precision training just by adding command lines.
[00:06:16.000 --> 00:06:18.000]   And they're really, really well done.
[00:06:18.000 --> 00:06:22.000]   So I would definitely recommend you to look at them.
[00:06:22.000 --> 00:06:26.000]   And the other thing that's nice, you can basically go as deep as you want.
[00:06:26.000 --> 00:06:30.000]   If you want to create your own custom models, complex pipeline, I'll let you do that.
[00:06:30.000 --> 00:06:34.000]   So you're as free to directly use it with pre-trained model.
[00:06:34.000 --> 00:06:36.000]   That's fine-tuned anything at all.
[00:06:36.000 --> 00:06:39.000]   Or going into more advanced level.
[00:06:39.000 --> 00:06:43.000]   It's pretty nice, that customization.
[00:06:43.000 --> 00:06:48.000]   The new release that Lavanya was talking about is basically now Hacking Face and Weights
[00:06:48.000 --> 00:06:51.000]   and Biases are directly integrated.
[00:06:51.000 --> 00:06:56.000]   So Weights and Biases is part of the Hacking Face library.
[00:06:56.000 --> 00:06:58.000]   The way it works is really, really easy.
[00:06:58.000 --> 00:07:00.000]   Again, I put a link to the document page.
[00:07:00.000 --> 00:07:03.000]   But the way it works is really, really convenient.
[00:07:03.000 --> 00:07:05.000]   You do just pip install 1db.
[00:07:05.000 --> 00:07:14.000]   And if your Weights and Biases is installed and we did the login in the command line or in your Jupyter notebook or wherever,
[00:07:14.000 --> 00:07:18.000]   your data is automatically going to be logged.
[00:07:18.000 --> 00:07:23.000]   It's not entirely true yet because it's mainly on the PyTorch model.
[00:07:23.000 --> 00:07:28.000]   But so on TensorFlow models, it's not implemented yet.
[00:07:28.000 --> 00:07:31.000]   But it's probably going to happen soon.
[00:07:31.000 --> 00:07:34.000]   I'm going to show you really quickly how it works.
[00:07:34.000 --> 00:07:41.000]   Basically, Hacking Face refactored their entire code, made it much, much cleaner.
[00:07:41.000 --> 00:07:45.000]   And they have that class that's called Trainer.
[00:07:45.000 --> 00:07:52.000]   And in that class, if I search for 1db, you can see basically the program only checks, do you have 1db?
[00:07:52.000 --> 00:07:57.000]   And if it's installed, everything is going to happen automatically.
[00:07:57.000 --> 00:08:06.000]   Your metrics are going to be logged, your config parameters, your losses, and you will be able to see all the things you want.
[00:08:06.000 --> 00:08:11.000]   And we are going to see it in that example.
[00:08:11.000 --> 00:08:15.000]   So now let's go directly into the Hacking Tweets.
[00:08:15.000 --> 00:08:17.000]   How does it work?
[00:08:17.000 --> 00:08:20.000]   So I have a repo.
[00:08:20.000 --> 00:08:24.000]   It's probably going to change really quickly.
[00:08:24.000 --> 00:08:28.000]   And the way it works is very, very simple.
[00:08:28.000 --> 00:08:34.000]   We're going to go through the code to understand a bit better the principle.
[00:08:34.000 --> 00:08:37.000]   Basically, we need to develop the tweets for that.
[00:08:37.000 --> 00:08:42.000]   There's a library that's pretty cool that integrates with the Twitter API.
[00:08:42.000 --> 00:08:45.000]   So you need to have a Twitter development account.
[00:08:45.000 --> 00:08:51.000]   Honestly, all that process takes maybe two, three minutes, and you will have API keys.
[00:08:51.000 --> 00:08:54.000]   And once you have it, there's a cool library called Tweepy.
[00:08:54.000 --> 00:09:02.000]   And with that, you can just make some easy calls through Python to download all tweets and all.
[00:09:02.000 --> 00:09:07.000]   So here what I do typically, I will put the name of a person I want their tweets.
[00:09:07.000 --> 00:09:11.000]   And then you iterate through it and you download it.
[00:09:11.000 --> 00:09:13.000]   Now there's a limit.
[00:09:13.000 --> 00:09:16.000]   You cannot download more than 3,200 tweets.
[00:09:16.000 --> 00:09:21.000]   Actually, if you try to play, 3,200 tweets sounds like a lot.
[00:09:21.000 --> 00:09:23.000]   It's really not a lot.
[00:09:23.000 --> 00:09:25.000]   It's not that much data.
[00:09:25.000 --> 00:09:27.000]   I mean, I'm really far from it.
[00:09:27.000 --> 00:09:30.000]   But some people are much, much beyond that.
[00:09:30.000 --> 00:09:36.000]   And you need to consider that in the tweets, you know, there's like the retweets that are kind of useless.
[00:09:36.000 --> 00:09:40.000]   There's the ones that you just share a URL, and they're useless as well.
[00:09:40.000 --> 00:09:44.000]   So there's a lot of data that's not that interesting.
[00:09:44.000 --> 00:09:50.000]   So all that function does is just to not link tweets.
[00:09:50.000 --> 00:09:59.000]   Then what we want to do, basically to create our data set, we are going to fine tune our model based on GPT-2,
[00:09:59.000 --> 00:10:06.000]   which is the open AI model that was trained on a lot, a lot, a lot of text, but it obviously was not trained on Twitters.
[00:10:06.000 --> 00:10:10.000]   So it works better if the text is not too strange.
[00:10:10.000 --> 00:10:14.000]   So there's a few things that are problematic.
[00:10:14.000 --> 00:10:18.000]   For example, you have a lot of @ to mention people.
[00:10:18.000 --> 00:10:27.000]   And what we do here to clean this up, we replace the @ something like @chiropathy by the name of the person.
[00:10:27.000 --> 00:10:31.000]   So the text doesn't look as strange.
[00:10:31.000 --> 00:10:35.000]   Then we try to clean up the data.
[00:10:35.000 --> 00:10:37.000]   So we have a lot of hashtags.
[00:10:37.000 --> 00:10:45.000]   So imagine you're trying to predict some text, and the person says, "Here is my new paper.
[00:10:45.000 --> 00:10:49.000]   #convnets #ilovegans."
[00:10:49.000 --> 00:10:52.000]   It's not really something that GPT-2 was trained on.
[00:10:52.000 --> 00:10:54.000]   So it really wouldn't work.
[00:10:54.000 --> 00:10:58.000]   So what we do is something we remove, something we change.
[00:10:58.000 --> 00:11:00.000]   There's a lot of improvement I can still do there.
[00:11:00.000 --> 00:11:12.000]   But the idea is you want your text to not look too strange, but you still want to train to have some predictions that look like tweets.
[00:11:12.000 --> 00:11:17.000]   So as you can see so far, most of my work is really on creating the dataset.
[00:11:17.000 --> 00:11:22.000]   And there's a lot of work there and a lot of improvements I want to do are related to that.
[00:11:22.000 --> 00:11:25.000]   Like here, I tried to remove all the retweets.
[00:11:25.000 --> 00:11:27.000]   I tried to clean up the text.
[00:11:27.000 --> 00:11:30.000]   It's presented in some strange manner.
[00:11:30.000 --> 00:11:38.000]   Something also funny, like if I go on Twitter, so I start replacing the names of people, right?
[00:11:38.000 --> 00:11:41.000]   And let's say, for example, I have Jeremy Howard.
[00:11:41.000 --> 00:11:47.000]   And Jeremy Howard, I replaced by his handle, it's Jeremy #mask4howard.
[00:11:47.000 --> 00:11:50.000]   That's not really convenient.
[00:11:50.000 --> 00:11:52.000]   So you need to be able to work with that.
[00:11:52.000 --> 00:11:55.000]   Another one, you know, I want to do LaVania, right?
[00:11:55.000 --> 00:11:58.000]   She has an emoji in her name.
[00:11:58.000 --> 00:12:01.000]   So how do I train on that?
[00:12:01.000 --> 00:12:03.000]   It's not something that GPT-2 is trained on.
[00:12:03.000 --> 00:12:07.000]   So those things are things you need to clean up.
[00:12:07.000 --> 00:12:10.000]   And otherwise, your model is not really going to work.
[00:12:10.000 --> 00:12:15.000]   So once I have all those tweets, I split training data, test data.
[00:12:15.000 --> 00:12:19.000]   All that so far, I only worked on cleaning the dataset, right?
[00:12:19.000 --> 00:12:24.000]   So then that's when it becomes interesting training the model.
[00:12:24.000 --> 00:12:34.000]   So I just have to log in on my 1DB so that I have my run instrumented automatically.
[00:12:34.000 --> 00:12:40.000]   Here I have a variable just so that they are cleanly organized into my HuggingFace project.
[00:12:40.000 --> 00:12:42.000]   But here is what is cool.
[00:12:42.000 --> 00:12:44.000]   My training program is only that.
[00:12:44.000 --> 00:12:46.000]   I'm just using the script.
[00:12:46.000 --> 00:12:49.000]   I didn't write any single line of code.
[00:12:49.000 --> 00:12:51.000]   The scripts are pretty nice.
[00:12:51.000 --> 00:12:56.000]   The main stuff I changed here, you see I say I want a GPT-2 model.
[00:12:56.000 --> 00:12:59.000]   So it's just going to download a pre-trained model.
[00:12:59.000 --> 00:13:01.000]   I say where is my data?
[00:13:01.000 --> 00:13:05.000]   So I have training set, validation set I saved.
[00:13:05.000 --> 00:13:09.000]   I just say I want to evaluate to have validation loss, et cetera.
[00:13:09.000 --> 00:13:12.000]   How much I want to log to have enough points.
[00:13:12.000 --> 00:13:16.000]   The main parameter I changed was that one.
[00:13:16.000 --> 00:13:25.000]   I could train only one batch per GPU because otherwise I would run out of CUDA memory.
[00:13:25.000 --> 00:13:29.000]   And by the way, that can have an impact because by default I think they have eight.
[00:13:29.000 --> 00:13:31.000]   I have only one.
[00:13:31.000 --> 00:13:33.000]   Something I didn't try.
[00:13:33.000 --> 00:13:42.000]   Technically, I should maybe want to accumulate eight batches before updating my weights.
[00:13:42.000 --> 00:13:44.000]   It could have some impact.
[00:13:44.000 --> 00:13:46.000]   I didn't test that yet.
[00:13:46.000 --> 00:13:49.000]   And then I trained on a lot of epochs.
[00:13:49.000 --> 00:13:54.000]   Basically, when I had my first runs, I saw that I start overfitting after four epochs.
[00:13:54.000 --> 00:13:59.000]   So typically it's that for most of the people I used it on.
[00:13:59.000 --> 00:14:02.000]   So I just trained for four epochs.
[00:14:02.000 --> 00:14:04.000]   Then what you do, I want to test it.
[00:14:04.000 --> 00:14:06.000]   So I feed a few sentences.
[00:14:06.000 --> 00:14:09.000]   And then what is cool is it's the same way.
[00:14:09.000 --> 00:14:19.000]   I just run my generation model with a script from Hugging Face again, which is that script.
[00:14:19.000 --> 00:14:23.000]   So I didn't really write anything.
[00:14:23.000 --> 00:14:31.000]   All the part that was related to having a pre-trained model, tokenizing, fine tuning, running generation.
[00:14:31.000 --> 00:14:35.000]   I didn't have to think about that, which is pretty cool.
[00:14:35.000 --> 00:14:39.000]   But I got pretty interesting results.
[00:14:39.000 --> 00:14:45.000]   So here I'm going to go quickly to my dashboard.
[00:14:45.000 --> 00:14:50.000]   And you can see I have predictions on a lot of different people.
[00:14:50.000 --> 00:14:51.000]   And here is my input.
[00:14:51.000 --> 00:14:54.000]   Here is my output.
[00:14:54.000 --> 00:14:58.000]   And a lot of them are actually really funny.
[00:14:58.000 --> 00:15:03.000]   So I had actually a lot of fun running it.
[00:15:03.000 --> 00:15:06.000]   Some of the fun ones were obviously from Chiropathy.
[00:15:06.000 --> 00:15:14.000]   And there was some other stuff like Lavanya, she used a lot of emojis in her tweets, which is kind of funny.
[00:15:14.000 --> 00:15:17.000]   I have some Hugging Face people like Julia.
[00:15:17.000 --> 00:15:21.000]   Here's some Hugging Face emoji in his predictions, which is interesting.
[00:15:21.000 --> 00:15:25.000]   So you see it's kind of learning something.
[00:15:25.000 --> 00:15:33.000]   And then I have like Jan Lecombe talking that he likes French or sometimes he starts speaking in French.
[00:15:33.000 --> 00:15:36.000]   So I have a cool one too.
[00:15:36.000 --> 00:15:40.000]   Just quickly I want to show from Google.
[00:15:40.000 --> 00:15:43.000]   Google AI.
[00:15:43.000 --> 00:15:48.000]   Okay, so Google AI talking about a new paper, right?
[00:15:48.000 --> 00:15:49.000]   C.D. Moe and the VP blog.
[00:15:49.000 --> 00:15:51.000]   I like the title of that paper.
[00:15:51.000 --> 00:16:04.000]   A new algorithm that can simulate motion in NLP speech is being developed that solves the natural language recognition problem of developing computational models for motion-related content and experience.
[00:16:04.000 --> 00:16:06.000]   Whoa, that sounds pretty cool.
[00:16:06.000 --> 00:16:07.000]   That sounds pretty interesting.
[00:16:07.000 --> 00:16:09.000]   I'd like to read that.
[00:16:09.000 --> 00:16:11.000]   That actually sounds like a real paper.
[00:16:11.000 --> 00:16:17.000]   I don't know if it could be actually something, but that's interesting to see that kind of prediction.
[00:16:17.000 --> 00:16:19.000]   And what I like to say that I didn't look that much.
[00:16:19.000 --> 00:16:21.000]   That's kind of strange to me.
[00:16:21.000 --> 00:16:25.000]   It's like you have different losses based on the person you see.
[00:16:25.000 --> 00:16:28.000]   So it's interesting to see where does that come from?
[00:16:28.000 --> 00:16:30.000]   Does it come from the data set?
[00:16:30.000 --> 00:16:34.000]   Does it come from maybe some people use more vocabulary than others?
[00:16:34.000 --> 00:16:38.000]   Basically, the higher your loss is, the more unpredictable you are.
[00:16:38.000 --> 00:16:43.000]   So here you see like Andrew McCarthy is the most unpredictable when you write something.
[00:16:43.000 --> 00:16:45.000]   Then you have people that are more predictable.
[00:16:45.000 --> 00:16:49.000]   The subject comes more often or they use more often the same words.
[00:16:49.000 --> 00:16:57.000]   But you have also interesting stuff like some people start low, but they don't decrease much, while some people start high, but they decrease really, really fast.
[00:16:57.000 --> 00:17:05.000]   So once the program has learned what people are talking about, it kind of trains much faster.
[00:17:05.000 --> 00:17:08.000]   So that's about it.
[00:17:08.000 --> 00:17:12.000]   Here I have a lot of different ideas of things to try.
[00:17:12.000 --> 00:17:15.000]   You can see most of it is related to actually the data set.
[00:17:15.000 --> 00:17:17.000]   That's where I need to work on.
[00:17:17.000 --> 00:17:22.000]   The model and the text generation, I don't need to do anything because Hugging Face does it for me.
[00:17:22.000 --> 00:17:24.000]   So it's pretty cool.
[00:17:24.000 --> 00:17:29.000]   Something that's kind of impressive that I liked is that my data set are really, really small.
[00:17:29.000 --> 00:17:31.000]   In the best case, I have 200 kilobytes.
[00:17:31.000 --> 00:17:34.000]   And sometimes I have less than 100.
[00:17:34.000 --> 00:17:36.000]   When I get 200, it's much, much better.
[00:17:36.000 --> 00:17:42.000]   But typically, a good data set is at least 5 megabytes or much bigger.
[00:17:42.000 --> 00:17:44.000]   So it's like 20 times more.
[00:17:44.000 --> 00:17:46.000]   Some are like 100 times more.
[00:17:46.000 --> 00:17:49.000]   And you still get cool results.
[00:17:49.000 --> 00:17:53.000]   Something that I want to try, too, I want to pre-train on the entire Twitter.
[00:17:53.000 --> 00:17:58.000]   So that basically I would have a model that learns how to make like tweet-like messages.
[00:17:58.000 --> 00:18:01.000]   And then basically you fine tune on the person.
[00:18:01.000 --> 00:18:04.000]   I think that would be a cool iteration to try.
[00:18:04.000 --> 00:18:10.000]   And the last thing I like, too, is that idea, which I think like between the top and bottom layers,
[00:18:10.000 --> 00:18:14.000]   some are more related to predicting the next world.
[00:18:14.000 --> 00:18:22.000]   So it's kind of more like memorization, while some are more related to understanding what the person is talking about.
[00:18:22.000 --> 00:18:25.000]   So, for example, you train on the machine learning people.
[00:18:25.000 --> 00:18:28.000]   Typically, they all talk about AI.
[00:18:28.000 --> 00:18:30.000]   So it's like the lexical field.
[00:18:30.000 --> 00:18:33.000]   And I think those kind of concepts are in different layers.
[00:18:33.000 --> 00:18:38.000]   And maybe if you train them differently, you will get some models that are better on memorization
[00:18:38.000 --> 00:18:47.000]   or some that are more creative and that mainly on the context and the subject.
[00:18:47.000 --> 00:18:48.000]   That's about it.
[00:18:48.000 --> 00:18:50.000]   Feel free to play with it.
[00:18:50.000 --> 00:18:56.000]   I think there's a lot of things to see, and I'd love to see your results there.
[00:18:56.000 --> 00:18:59.000]   There's a lot of other things to try.
[00:18:59.000 --> 00:19:01.000]   I don't know if you have any questions.
[00:19:01.000 --> 00:19:05.000]   I saw my screen beeping a little bit.
[00:19:05.000 --> 00:19:06.000]   Thanks, Boris.
[00:19:06.000 --> 00:19:07.000]   That was amazing.
[00:19:07.000 --> 00:19:09.000]   So we don't -- can people ask questions?
[00:19:09.000 --> 00:19:14.000]   We were having some technical issues, so we were trying to deal with those.
[00:19:14.000 --> 00:19:15.000]   Nothing related to you.
[00:19:15.000 --> 00:19:18.000]   You were perfect.
[00:19:18.000 --> 00:19:24.000]   But if people want to start asking questions, you can do that either in the Q&A or you can do it in the chat.
[00:19:24.000 --> 00:19:30.000]   And then Nick and Kayla, you can pull questions from the YouTube channel.
[00:19:30.000 --> 00:19:31.000]   So I see a question here.
[00:19:31.000 --> 00:19:38.000]   Someone asked, are there plans to support recent biases, logging of NLP tests beyond the table format,
[00:19:38.000 --> 00:19:40.000]   specifically token tagging?
[00:19:40.000 --> 00:19:41.000]   I love this question.
[00:19:41.000 --> 00:19:48.000]   Currently I'm writing something custom with HTML, but it would be nice to have it built in.
[00:19:48.000 --> 00:19:50.000]   So, yes, we are working on that.
[00:19:50.000 --> 00:19:59.000]   I can share with you one report that we haven't made public, but if you would like to test it out and give us some feedback,
[00:19:59.000 --> 00:20:09.000]   you can find -- you can log attention maps, NER, so named entity recognition stuff, part of speech tagging graphs,
[00:20:09.000 --> 00:20:12.000]   and there's a bunch of other cool stuff in there.
[00:20:12.000 --> 00:20:27.000]   We also have built-in support for LIMES, ELI5 predictions, and all of that stuff you can find in this report right here.
[00:20:27.000 --> 00:20:30.000]   But if you're looking for something specific, please let us know.
[00:20:30.000 --> 00:20:34.000]   Like either DM me or something.
[00:20:34.000 --> 00:20:39.000]   >> Actually, just to add on that, the tables, they're not logged directly by the integration right now.
[00:20:39.000 --> 00:20:43.000]   Maybe they would be later if it's something that everybody wants to do.
[00:20:43.000 --> 00:20:49.000]   But what is done automatically are all the metrics and all the reports.
[00:20:49.000 --> 00:20:53.000]   That I just decided to edit myself, and it's pretty easy.
[00:20:53.000 --> 00:21:03.000]   If you go to the documentation, you have like a logging page, and you can log all kind of objects.
[00:21:03.000 --> 00:21:10.000]   And I just went here, I went to log text tables, and you just have a specific format, and I added it in there.
[00:21:10.000 --> 00:21:15.000]   And same if you want to log HTML, you already have some functions you can use.
[00:21:15.000 --> 00:21:17.000]   And you can add your own.
[00:21:17.000 --> 00:21:24.000]   You have all kind of objects that can be added.
[00:21:24.000 --> 00:21:31.000]   >> Boris, I was wondering if we could get a few more example tweets from your Lavanya tweet bot.
[00:21:31.000 --> 00:21:35.000]   I'm just kind of curious what we got there.
[00:21:35.000 --> 00:21:37.000]   >> So, let's go through here quickly.
[00:21:37.000 --> 00:21:41.000]   I'm going to go through this one quickly.
[00:21:41.000 --> 00:21:44.000]   >> And then also maybe we can do Lucas next.
[00:21:44.000 --> 00:21:47.000]   >> Lucas, yeah.
[00:21:47.000 --> 00:21:48.000]   I have a lot of people.
[00:21:48.000 --> 00:21:50.000]   I had way too much fun.
[00:21:50.000 --> 00:21:54.000]   I need to scroll it.
[00:21:54.000 --> 00:21:55.000]   Okay, Lavanya.
[00:21:55.000 --> 00:21:58.000]   So, you see some emojis appearing.
[00:21:58.000 --> 00:22:00.000]   And I'll give you that link.
[00:22:00.000 --> 00:22:02.000]   They're really, really fun.
[00:22:02.000 --> 00:22:08.000]   So, you see, like Lavanya, she wants to create a collection of fiction scripts for women.
[00:22:08.000 --> 00:22:14.000]   So, I learned a lot of stuff from Lavanya without having to read all her tweets' history, which is pretty cool.
[00:22:14.000 --> 00:22:16.000]   Something funny, too, actually.
[00:22:16.000 --> 00:22:18.000]   Let me show you quickly.
[00:22:18.000 --> 00:22:25.000]   Something I didn't know about him is -- where is that?
[00:22:25.000 --> 00:22:30.000]   >> Because we're taking this as facts now, whatever this guy is tweeting.
[00:22:30.000 --> 00:22:31.000]   >> Oh, no, Jack Clark.
[00:22:31.000 --> 00:22:32.000]   Sorry.
[00:22:32.000 --> 00:22:33.000]   Wrong one.
[00:22:33.000 --> 00:22:35.000]   Oh, here.
[00:22:35.000 --> 00:22:40.000]   He wants -- he had a cool one.
[00:22:40.000 --> 00:22:48.000]   He has one that he wants to create a solar electric car, which is pretty interesting.
[00:22:48.000 --> 00:23:01.000]   >> I'm going to pop a link to Boris' GitHub repo so you can actually plug in your own Twitter account or someone else's Twitter account and actually create these and have fun with your friends.
[00:23:01.000 --> 00:23:07.000]   >> Yeah, and you can open, like, at the bottom, you will have a link to the dashboard so you can read all the sentences.
[00:23:07.000 --> 00:23:10.000]   I think some are great, some are so-so.
[00:23:10.000 --> 00:23:14.000]   But what I like is, like, just the first version, first iteration.
[00:23:14.000 --> 00:23:20.000]   There's so many things to improve, and I think it's going to get, like, much, much better from there.
[00:23:20.000 --> 00:23:29.000]   Actually, for Lucas, for Lucas, I had trouble having nice generation because I don't know, I need to look more at his Twitter account.
[00:23:29.000 --> 00:23:34.000]   I think he must be talking about too many random subjects.
[00:23:34.000 --> 00:23:37.000]   One thing, though, is always so positive.
[00:23:37.000 --> 00:23:38.000]   I think that's really good.
[00:23:38.000 --> 00:23:40.000]   I think that is great fun.
[00:23:40.000 --> 00:23:41.000]   I want to help.
[00:23:41.000 --> 00:23:43.000]   And I imagine it's really Lucas.
[00:23:43.000 --> 00:23:45.000]   It's really the way he talks.
[00:23:45.000 --> 00:23:48.000]   So it's kind of fun.
[00:23:48.000 --> 00:23:51.000]   >> I have a question, Boris.
[00:23:51.000 --> 00:23:58.000]   You mentioned to me earlier, like, you spent most of the time on data, not on fine-tuning the model itself.
[00:23:58.000 --> 00:24:02.000]   Can you talk about why you spent most of your time on the data?
[00:24:02.000 --> 00:24:15.000]   >> Yeah, so in most of the projects I've done so far, when I started, like, learning, you always spend all your time on fine-tuning and should I add more layers or should I try another model and all.
[00:24:15.000 --> 00:24:21.000]   And in the end, you know, like, typically just use a model that's famous and that works for your task.
[00:24:21.000 --> 00:24:23.000]   So, for example, I want to do text generation.
[00:24:23.000 --> 00:24:25.000]   Just use GPT-2.
[00:24:25.000 --> 00:24:27.000]   Don't try to change the model.
[00:24:27.000 --> 00:24:29.000]   GPT-2 is good for that.
[00:24:29.000 --> 00:24:33.000]   You won't get a better model than OpenAI easily.
[00:24:33.000 --> 00:24:36.000]   And there are, like, default parameters that just worked.
[00:24:36.000 --> 00:24:41.000]   But when you have a better data set, you get quickly results really fast.
[00:24:41.000 --> 00:24:43.000]   Your model improves.
[00:24:43.000 --> 00:24:46.000]   It's a better investment of your time, I think.
[00:24:46.000 --> 00:24:50.000]   So typically I start improving the model when I have no more idea.
[00:24:50.000 --> 00:24:52.000]   But the data set is really --
[00:24:52.000 --> 00:24:54.000]   >> When you have no more ideas.
[00:24:54.000 --> 00:24:56.000]   >> Yeah, I still have too many.
[00:24:56.000 --> 00:24:59.000]   I'm not going to change the model for a while.
[00:24:59.000 --> 00:25:02.000]   But feel free to change it and maybe you'll get cool stuff.
[00:25:02.000 --> 00:25:05.000]   So that will be interesting.
[00:25:05.000 --> 00:25:07.000]   >> Thanks.
[00:25:07.000 --> 00:25:09.000]   Can you share the link for the Slack channel?
[00:25:09.000 --> 00:25:13.000]   Yes, Kayla, could you share the link?
[00:25:13.000 --> 00:25:18.000]   And then also Boris is going to be in that Slack channel.
[00:25:18.000 --> 00:25:24.000]   So if you want to pop in tomorrow, the day after this weekend to ask him questions, we'd be happy to answer your questions.
[00:25:24.000 --> 00:25:26.000]   Yeah.
[00:25:26.000 --> 00:25:29.000]   And also the integration is officially launching tomorrow.
[00:25:29.000 --> 00:25:35.000]   There's a lot more examples on fine tuning, running hyperparameter optimization through hugging face that are coming tomorrow.
[00:25:35.000 --> 00:25:37.000]   So watch out.
[00:25:37.000 --> 00:25:41.000]   I'll tweet about them and we'll also post them in the Slack so you can check them out.
[00:25:41.000 --> 00:25:43.000]   Thank you, Boris.
[00:25:43.000 --> 00:25:45.000]   That was really good.
[00:25:45.000 --> 00:25:47.000]   >> Thank you.
[00:25:47.000 --> 00:25:49.000]   That was a lot of fun.
[00:25:49.000 --> 00:25:51.000]   >> Up next we have Garry.
[00:25:51.000 --> 00:25:55.000]   He's a professor of privacy or preserving deep learning.
[00:25:55.000 --> 00:26:01.000]   And Garry is a Ph.D. candidate at University of Missouri Kansas City.
[00:26:01.000 --> 00:26:07.000]   And he works at an applied -- he works as an applied scientist at Triple Blind.
[00:26:07.000 --> 00:26:09.000]   So Garry, welcome.
[00:26:09.000 --> 00:26:12.000]   >> Hey, thanks.
[00:26:12.000 --> 00:26:22.000]   I'm going to go ahead and share my screen.
[00:26:22.000 --> 00:26:25.000]   Well, good evening, good afternoon, everyone.
[00:26:25.000 --> 00:26:28.000]   I think there are so many different people, so many different places.
[00:26:28.000 --> 00:26:31.000]   So hello to every one of you.
[00:26:31.000 --> 00:26:41.000]   It's very exciting to speak about and talk about privacy preserving, machine learning in general and more specifically deep learning.
[00:26:41.000 --> 00:27:04.000]   Today I'll be talking about membership inference attacks as just one simple example of how surprisingly easy it is to attack even a well-trained deep learning model that most of those that are shared nowadays and even very critical, some of them deployment and production systems.
[00:27:04.000 --> 00:27:17.000]   Now, the reason for this, and I believe that so many other people -- I don't want to speak for myself, something big, but people have predicted that 2020 will be the year of privacy preserving machine learning.
[00:27:17.000 --> 00:27:22.000]   And this was even long before COVID-19 happened.
[00:27:22.000 --> 00:27:31.000]   And we saw that early this year when this pandemic hit the globe, that technology tried to jump in and tried to find solutions.
[00:27:31.000 --> 00:27:44.000]   One of the specific solutions that has been showing some promising results is the tracking of people who might get infected and the interactions between those people.
[00:27:44.000 --> 00:27:57.000]   However, it was not easy to implement at all, and it was not easy to be adopted in so many different countries and so many even to publish them on the stores because of the privacy preserving issues.
[00:27:57.000 --> 00:28:09.000]   We want to have something that is really good, but at the same time, we also want to make sure that we are not violating the privacy of people and the privacy of users.
[00:28:09.000 --> 00:28:21.000]   And privacy and security is not a new topic, but it has a whole new different perspective and aspects when it comes to artificial intelligence and machine learning.
[00:28:21.000 --> 00:28:36.000]   And this is because, by definition, the development -- before speaking about privacy, the development of a machine learning model or an AI or deep learning algorithm is very different from conventional software engineering.
[00:28:36.000 --> 00:28:47.000]   First of all, some of the differences include that when we are developing a software product, a program, we have a very specific goal.
[00:28:47.000 --> 00:28:53.000]   And the goal there is to meet a set of well-defined functional requirements.
[00:28:53.000 --> 00:28:57.000]   Functional requirements means if I click on a specific button, something should happen, right?
[00:28:57.000 --> 00:29:03.000]   And we also have some non-functional requirements that includes the security and the quality.
[00:29:03.000 --> 00:29:21.000]   And because traditional software engineering is written by humans, the quality of this software heavily relies on the quality of the source code itself, which ultimately means the experience of the software developer has written that source code.
[00:29:21.000 --> 00:29:39.000]   And therefore, once you have a good, well-written, well-tested software system shipped, that software system will continue to operate as expected unless you change something with it.
[00:29:39.000 --> 00:29:43.000]   But this story is completely different when it comes to machine learning.
[00:29:43.000 --> 00:29:48.000]   For one, the goal here is to optimize a specific metric.
[00:29:48.000 --> 00:29:53.000]   That's often the accuracy. You want to maximize your accuracy.
[00:29:53.000 --> 00:29:59.000]   You want to minimize your loss of some type of predictive model.
[00:29:59.000 --> 00:30:05.000]   And therefore, the quality of this produced model is not only affected by the quality of the source code.
[00:30:05.000 --> 00:30:11.000]   Rather, it's heavily affected by what? By the data we are using to generate such models.
[00:30:11.000 --> 00:30:18.000]   Because by definition, machine learning is defined as one of the best definitions.
[00:30:18.000 --> 00:30:22.000]   Machine learning is defined as programs that generalize from data.
[00:30:22.000 --> 00:30:36.000]   So if we look from a very big picture, what we are trying to do is to solve a problem using some automated approach without explicitly programming, without explicitly having to write code for that solution or that program.
[00:30:36.000 --> 00:30:47.000]   So machine learning are programs that generalize from data, and therefore, they heavily rely on the data and all those hyperparameters that take place in generating the specific model.
[00:30:47.000 --> 00:31:00.000]   At the end of the day, deep learning is still an iterative search problem that goes through hundreds to sometimes thousands of iterations before generating a deep learning model.
[00:31:00.000 --> 00:31:12.000]   Not only it is an iterative and it's time consuming, it takes so many different cycles and training iterations before you reach a good model.
[00:31:12.000 --> 00:31:20.000]   But in reality, there are also different people involved in creating a deep learning model or machine learning model.
[00:31:20.000 --> 00:31:25.000]   So you have data scientists that use a wide range of tools to collect the data.
[00:31:25.000 --> 00:31:31.000]   And we just seen in the previous talk that data collection and cleaning is one of the important.
[00:31:31.000 --> 00:31:34.000]   This is true across all machine learning projects.
[00:31:34.000 --> 00:31:39.000]   So there are different people involved here, different tools involved here.
[00:31:39.000 --> 00:31:46.000]   So what we see here is a typical development lifecycle of a machine learning model.
[00:31:46.000 --> 00:31:51.000]   You usually have an idea in mind you want to build it, you need to have an access to a data set.
[00:31:51.000 --> 00:31:55.000]   Usually, as Boris said, you want to have some reference model.
[00:31:55.000 --> 00:31:58.000]   You don't want to create models from scratch, right?
[00:31:58.000 --> 00:32:01.000]   So we start, we create a model.
[00:32:01.000 --> 00:32:03.000]   Usually, I'm a PyTorch fan.
[00:32:03.000 --> 00:32:10.000]   So whether you are a PyTorch fan or a TensorFlow or a Keras, you start by building a model, you train it, you evaluate it.
[00:32:10.000 --> 00:32:18.000]   And that's where we really go through so many different numbers of iterations until we reach a very good result.
[00:32:18.000 --> 00:32:24.000]   So that's usually done by a software data engineer or a machine learning engineer.
[00:32:24.000 --> 00:32:29.000]   Once a machine learning model is created, the task is not done.
[00:32:29.000 --> 00:32:33.000]   You either need to deploy it or expose it through some API.
[00:32:33.000 --> 00:32:43.000]   And even when you do that, you still need to keep monitoring that model to see if there are new instances that your model fails to detect or to do its task on.
[00:32:43.000 --> 00:32:46.000]   So you retrain that model.
[00:32:46.000 --> 00:32:53.000]   So the moral of this long story here is that there are different applications, different people, different experiences.
[00:32:53.000 --> 00:33:02.000]   We usually even experience with different libraries and platforms to generate some good model.
[00:33:02.000 --> 00:33:13.000]   And that makes the models really be exposed to different types of attacks than conventional software services or software systems.
[00:33:13.000 --> 00:33:30.000]   Some of the privacy pinpoints that might hinder the overall application of the model actually happen in places where we can interact with the model.
[00:33:30.000 --> 00:33:40.000]   And therefore, the attacks on a deep learning model could actually be categorized based on how much access do you have to that model.
[00:33:40.000 --> 00:33:49.000]   So in a very big general picture, the attacks on deep learning models could be categorized in two general classes, a white box attack, a black box attack.
[00:33:49.000 --> 00:33:56.000]   The white box attack means you usually are an insider or you have the access to the model.
[00:33:56.000 --> 00:33:59.000]   You know the architecture of the model. You know the parameters of the model.
[00:33:59.000 --> 00:34:05.000]   A black box attack is usually you have no access to the model. You can only query the model.
[00:34:05.000 --> 00:34:10.000]   So run inferences. You submit a data record to the model. You receive the output of that model.
[00:34:10.000 --> 00:34:20.000]   Something like a model that is exposed on Google AutoML where it's just an API. You submit an input. You receive an output.
[00:34:20.000 --> 00:34:34.000]   And therefore, based on the level of interaction or access that you have to a specific model, privacy could be hindered or could be violated in different places and in different ways.
[00:34:34.000 --> 00:34:45.000]   So the first way, of course, is that when we train a deep learning model, we need to make sure that the data we are using to train that model is kept in good hands.
[00:34:45.000 --> 00:35:01.000]   It's kept private and that people who are participating in this study or this statistical analysis or this deep learning model, their information is not being violated and it's not being used for some other malicious activities or tasks.
[00:35:01.000 --> 00:35:15.000]   Some of the simple, not simple, but some of the common attacks to do here on the data sets, specifically those that are available publicly, is to modify such data.
[00:35:15.000 --> 00:35:25.000]   Maybe change the labels and make the true labels become a wrong label, add some noises or inject some wrongly classified data sets and things like that.
[00:35:25.000 --> 00:35:41.000]   Also, if you are a hospital, if you are a hospital and you have a data set and then somebody comes from academia or industry and they say, hey, we have this great idea, we need to have access to your data set, hospitals cannot just share their data sets.
[00:35:41.000 --> 00:35:54.000]   There are regularizations and policies such as HIPAA in the United States and some other European unions and Middle East, there are regulations that prevent sharing of very sensitive data of people.
[00:35:54.000 --> 00:35:58.000]   And therefore, privacy could happen to the training data set.
[00:35:58.000 --> 00:36:04.000]   Sometimes we have access to, there exists lots of data, but we don't have access to it.
[00:36:04.000 --> 00:36:12.000]   Other places where we can also attack the overall development lifecycle if we are an insider.
[00:36:12.000 --> 00:36:32.000]   This is, I hope that people who are developing machine learning models, they are good enough that they are not going to poison the models that they are creating for their industry or for their academic or research usage or future usage or applications.
[00:36:32.000 --> 00:36:44.000]   But people can actually, malicious people, malicious users can actually build some backdoors into their models that make it easier to facilitate attacking such models in the future.
[00:36:44.000 --> 00:36:50.000]   And this is also correct and true in the case of federated learning.
[00:36:50.000 --> 00:36:59.000]   If you're not aware of federated learning, simply speaking, it's an approach that was created a couple of years ago.
[00:36:59.000 --> 00:37:03.000]   And basically federated learning, the idea is very simple.
[00:37:03.000 --> 00:37:11.000]   So if we have a hospital and you have an algorithm provider or somebody who wants to create a deep learning model using data that resides on the hospital side.
[00:37:11.000 --> 00:37:22.000]   Traditionally, we would require the hospital to upload their data to the server or to the person who wants to build this algorithm.
[00:37:22.000 --> 00:37:26.000]   So you send the data to them, and then they build a deep learning model on top of this data.
[00:37:26.000 --> 00:37:37.000]   But to preserve the privacy of this data, and especially in the case of hospitals, we cannot simply share this data and therefore instead of sending the data, we send the model.
[00:37:37.000 --> 00:37:48.000]   But nowadays, federated learning is mostly used on our mobile phones, on Apple watches, and user devices.
[00:37:48.000 --> 00:38:00.000]   So the process here, or the idea here is that when you go every night to your bed, you put your Apple watch or iPhone or Samsung device through a charger.
[00:38:00.000 --> 00:38:04.000]   It is charging, it is idle, it is connected to the internet.
[00:38:04.000 --> 00:38:13.000]   Google at that point, or Apple, is training a small deep learning model to predict what you are going to type next.
[00:38:13.000 --> 00:38:30.000]   So auto correction, Siri, Alexa, and all of those devices, wearable devices as well, that we are using all around the house, they are training every night on our machines so that we don't have to share our private data with the providers, with the servers to train these models.
[00:38:30.000 --> 00:38:45.000]   So they train small models on our devices, and those small models are then sent to the server where they all get aggregated, and then a smarter version will be released, of that model will be released in the next update or the next version, and so on and so forth.
[00:38:45.000 --> 00:39:05.000]   So if you are participating in a federated learning, you can also generate some backdoors in your models, or you can poison your models, or you can do some tricks that will allow you to watch, to attack, and to expose or retrieve sensitive information from the global models that are being built on the server side.
[00:39:05.000 --> 00:39:23.000]   The other type of attacks that, one of the most common attacks actually nowadays is an attack that happens on a model that has already been trained and deployed in a production system, or it's exposed using some REST API or just an API online.
[00:39:23.000 --> 00:39:33.000]   Some of those attacks are adversarial attacks, and I believe one of the talks today will be talking about generating adversarial texts, I believe, something like that.
[00:39:33.000 --> 00:39:46.000]   The main idea here is that in an adversarial attack, you create an input to a model that will fool the model, will make the model produce a wrong result.
[00:39:46.000 --> 00:40:05.000]   I've read a couple of weeks or maybe one month ago actually about researchers from McAfee, the security company, they fooled a Tesla car to see the 35 miles per hour speed limit as an 85 miles per hour speed limit.
[00:40:05.000 --> 00:40:27.000]   So basically they do not have access to the model, to the computer vision model that looks at the speed limits in the Tesla car, but they were able to handcraft a speed limit that for us humans, it looks like 35 miles per hour, but for the deep learning model, it looks like an 85 miles per hour.
[00:40:27.000 --> 00:40:47.000]   And actually, if you see my web browser here, Ian Goodfellow, the head of AI at Google, he's one of the first people who created this, the idea of adversarial attacks.
[00:40:47.000 --> 00:40:56.000]   This is a very famous example where we have a state of the art deep learning model that's able to classify this.
[00:40:56.000 --> 00:41:07.000]   I think this is not a very good picture, but it's able to classify this panda right here as a panda with a confidence level of about 60%, not bad at all.
[00:41:07.000 --> 00:41:14.000]   Using this noise, we are able to handicraft this noise by having a white access to the model.
[00:41:14.000 --> 00:41:25.000]   What basically happening here is that we are trying to find the loss function with respect to the input and then finding the assigned gradient of that, and that will generalize for us some noise.
[00:41:25.000 --> 00:41:36.000]   We want to find a noise that if we add it to this specific picture, the output will still be for us humans an image of a panda.
[00:41:36.000 --> 00:41:49.000]   But this noise, because it's added at the pixel level, now the same state of the art model is classifying this input as a gibbon with almost 100% confidence to score.
[00:41:49.000 --> 00:41:53.000]   So this is where adversarial attacks are happening.
[00:41:53.000 --> 00:42:02.000]   Some of the interesting ones by the researchers, they tried to fool the YOLO deep learning models.
[00:42:02.000 --> 00:42:09.000]   YOLO is one of the models that are capable to detect a human and label it as a person we see here.
[00:42:09.000 --> 00:42:19.000]   This person was able to generate this noise that will now fool this deep learning model to not recognizing this person as a person.
[00:42:19.000 --> 00:42:36.000]   And I have read that some smart entrepreneurs in Hong Kong and South Korea, I believe recently they created fashion or clothes and face masks that have such type of noisy images to fool the facial recognition devices.
[00:42:36.000 --> 00:42:42.000]   So even very well trained models are vulnerable to such attacks.
[00:42:42.000 --> 00:42:51.000]   But what I'm going to explain today is about inference attacks, and specifically membership inference attack.
[00:42:51.000 --> 00:42:54.000]   So what is a membership inference attack?
[00:42:54.000 --> 00:43:06.000]   Basically, this membership inference attack means giving access to a well-trained and a deployed deep learning model somewhere on the internet.
[00:43:06.000 --> 00:43:17.000]   We have a black box access to this model in such a way that we can only push an input to this model and receive the output of this model.
[00:43:17.000 --> 00:43:21.000]   And in this example, the output is a probability distribution.
[00:43:21.000 --> 00:43:27.000]   So perhaps a classification of this specific data sample.
[00:43:27.000 --> 00:43:34.000]   The attack is to take this input along with the output of this model that we aim to attack.
[00:43:34.000 --> 00:43:39.000]   By the way, the model that we aim to attack will be called a target network or the target model.
[00:43:39.000 --> 00:43:46.000]   So we take the input image or whatever data type it is, along with the output of the target model.
[00:43:46.000 --> 00:43:55.000]   And we should be able to tell whether this specific input was used in training this model or not.
[00:43:55.000 --> 00:43:57.000]   Hence the name membership.
[00:43:57.000 --> 00:44:03.000]   So you want to know if this input sample was a member of the training data set of this model.
[00:44:03.000 --> 00:44:08.000]   Now this seems a very trivial example and some people will say, "Why do I even care?"
[00:44:08.000 --> 00:44:19.000]   But this represents a HIPAA violation if the model that we are trying to attack is a model that was trained on electronic health records or patient's information at some hospital.
[00:44:19.000 --> 00:44:26.000]   So that by itself means that this model is already breaking policies and breaking the rules.
[00:44:26.000 --> 00:44:33.000]   And this also means that our deep learning models are leaking lots of information.
[00:44:33.000 --> 00:44:38.000]   So a simple question here, how are we going to perform this attack?
[00:44:38.000 --> 00:44:44.000]   How are we going to check if a deep learning model is leaking or not leaking?
[00:44:44.000 --> 00:44:48.000]   If a simple sample belongs to or doesn't belong to a specific data set?
[00:44:48.000 --> 00:44:51.000]   Notice that my answer is a yes or no.
[00:44:51.000 --> 00:45:05.000]   So if you are really into deep learning, you will say, "Okay, I'm going to build a classifier that's going to take these two inputs and somehow tell me whether these two inputs belong to or do not belong to the training data sample."
[00:45:05.000 --> 00:45:07.000]   So this is correct.
[00:45:07.000 --> 00:45:17.000]   This is how we are going to attack target models to test their information leakage by turning deep learning against itself.
[00:45:17.000 --> 00:45:36.000]   So in other words, simply speaking, we are going to build a binary classifier that will take an input distribution, the probability distribution of some output vector from a model, push it on our attack model, and this attack model will give us either a 0 or 1.
[00:45:36.000 --> 00:45:41.000]   0 means the specific data item did not belong to the training data set.
[00:45:41.000 --> 00:45:49.000]   1 means the specific data item was actually used in training this model, and that means this model is leaking information.
[00:45:49.000 --> 00:45:52.000]   So why will this even work?
[00:45:52.000 --> 00:46:03.000]   This works, and this actually was first shed light on this by Professor Riza Shukri.
[00:46:03.000 --> 00:46:23.000]   He and his team conducted experiments that showed that deep learning models, machine learning models, actually behave differently on data items that they have seen during the training versus data items or input samples they have never seen before.
[00:46:23.000 --> 00:46:25.000]   And this makes sense.
[00:46:25.000 --> 00:46:43.000]   When you overfit your model, and I bet most of us have run into the issue of overfitting, you see that your model starts to really learn the input data very well, the training data very well, but it fails to generalize to a new data or to the testing data set.
[00:46:43.000 --> 00:46:55.000]   And therefore, we take this characteristic of machine learning models, and we use it to attack them, because based on this characteristic, we can tell if a machine learning model is leaking information or not.
[00:46:55.000 --> 00:47:18.000]   Simply speaking, if this probability distribution consisted of about 90% probability being that this sample belongs to this class, and much less samples, and much less distribution in the other two classes, then we can tell and be confident to some level that yes, this data item belongs to that model,
[00:47:18.000 --> 00:47:25.000]   unless this model has done some really very good regularization and generalization techniques.
[00:47:25.000 --> 00:47:37.000]   So, the way we are going to attack this target model, we are going to consider it as a black box, so we don't have access to the architecture, we don't know the architecture of the model.
[00:47:37.000 --> 00:47:42.000]   Sometimes we don't even know whether it's a deep learning model or not. Doesn't matter at all, actually.
[00:47:42.000 --> 00:47:49.000]   Experiments have shown that to be able to attack such a model, we only need to train an attack network.
[00:47:49.000 --> 00:47:59.000]   This attack network will need a data set. The data set is the outputs or the probability vectors or probability distributions of the inputs that were fed into our target model.
[00:47:59.000 --> 00:48:15.000]   And since the most difficult scenario, this attack model is going to be a black box, what we are going to do is we are going to imitate or shadow the performance or another performance, the task of this model.
[00:48:15.000 --> 00:48:26.000]   So, the first step we are going to do is that we are going to build a new algorithm that will shadow or mimic, try to imitate the behavior of this network.
[00:48:26.000 --> 00:48:35.000]   And then we will have full access to those networks and we will push lots of information into them, receive lots of outputs and then train our binary classifier.
[00:48:35.000 --> 00:48:45.000]   So, this is a more overview of the approach. We don't have access to the model that we are trying to attack.
[00:48:45.000 --> 00:48:55.000]   Perhaps that model was built by Google AutoML, by someone else, so we can only run inferences or queries on that model.
[00:48:55.000 --> 00:48:59.000]   So, the first thing we can do is to mimic the behavior of that attack.
[00:48:59.000 --> 00:49:08.000]   So, we know that model classifies whether a specific x-ray shows some specific disease or not.
[00:49:08.000 --> 00:49:15.000]   So, the first step will be to collect or to find a data set that's very similar to our task at hand.
[00:49:15.000 --> 00:49:29.000]   Then what we are going to do is that take that data set and divide it into different subsets, because we are going to use some of the data sets to train the shadow network that we have.
[00:49:29.000 --> 00:49:40.000]   And those will be labeled as N. So, those instances, those data records that we use to train, that were actually used in training the shadow model.
[00:49:40.000 --> 00:49:46.000]   And then we are going to use another part of this data set only for the inference purposes.
[00:49:46.000 --> 00:49:54.000]   So, these data sets will be used to run inferences of the shadow model that we are going to train, but they were never used in training the model.
[00:49:54.000 --> 00:50:08.000]   And then at the end of the day, we are going to have a data set of probability distributions of some data records that were used in training the shadow model.
[00:50:08.000 --> 00:50:16.000]   And a data set of probability distributions of data records that were not used in training the shadow.
[00:50:16.000 --> 00:50:23.000]   So, we have N and out along with probability distributions. We are going to use them into training the attack network.
[00:50:23.000 --> 00:50:34.000]   And now the attack network will study the behavior of the shadow model to be able to distinguish the behavior of the shadow model on the sample data.
[00:50:34.000 --> 00:50:42.000]   Even though this is a simple example, but if you look at the probability distributions here, between data items that belong and do not belong to the training set,
[00:50:42.000 --> 00:50:50.000]   you can probably get an intuitive, very high level idea about membership inference attacks.
[00:50:50.000 --> 00:50:56.000]   Usually to make this work better in reality, we are not going to use only one neural network.
[00:50:56.000 --> 00:51:08.000]   The more shadow models you build, the more you are to build an algorithm that is very similar to the algorithm that we are trying to attack.
[00:51:08.000 --> 00:51:11.000]   Because remember, we don't know the architecture of the algorithm that we are trying to attack.
[00:51:11.000 --> 00:51:17.000]   We don't even know what type of algorithm is it. Is it some conventional machine learning or a neural network?
[00:51:17.000 --> 00:51:28.000]   And therefore, the more shadow models you build, the closer you are to the architecture, to the type of algorithm that you are attacking.
[00:51:28.000 --> 00:51:38.000]   In reality, I've also seen that a very smart idea here is to also build an attack model for every single class.
[00:51:38.000 --> 00:51:41.000]   So, for every single class for the true label.
[00:51:41.000 --> 00:51:52.000]   So, if the true label here was this first label, so we are going to build an attack network that will be able to attack the target model based on the true class output.
[00:51:52.000 --> 00:51:56.000]   Because the probability distribution will be different among the classes as well.
[00:51:56.000 --> 00:52:05.000]   And this is generally true. We've seen that deep learning models tend to perform better on some classes and the same data set than other classes,
[00:52:05.000 --> 00:52:10.000]   because of the data distribution, because of the imbalance problems, and so many other issues.
[00:52:10.000 --> 00:52:20.000]   And therefore, building the attack network, you could also build an attack network for every single class in the shadow model, and that will give you better results.
[00:52:20.000 --> 00:52:26.000]   Just for reference purposes, I have a Google collab notebook here. I can take you through the code.
[00:52:26.000 --> 00:52:32.000]   Nothing really crazy, very straightforward.
[00:52:32.000 --> 00:52:38.000]   But if you don't want to build everything from scratch, I urge you to look at the ML Privacy Meter tool.
[00:52:38.000 --> 00:52:43.000]   So, this was also built by Dr. Shukri, Riza, and others.
[00:52:43.000 --> 00:52:51.000]   They are the ones that actually first written a paper about membership inference attacks and came up with the idea of shadow models.
[00:52:51.000 --> 00:53:09.000]   So, this tool will allow you to just plug in your trained model with the data set, and they will return a meter and very beautiful visualizations about how vulnerable your model about membership inference attacks,
[00:53:09.000 --> 00:53:17.000]   and which of the instances are generating are most sensitive.
[00:53:17.000 --> 00:53:21.000]   So, to implement here, again, PyTorch fan here.
[00:53:21.000 --> 00:53:30.000]   So, nothing crazy. We are loading Cypher dataset, Cypher 10 dataset, and splitting the dataset.
[00:53:30.000 --> 00:53:38.000]   Probably the only different thing here from normal training cycle is the splitting of the dataset.
[00:53:38.000 --> 00:53:47.000]   I'm splitting the 50,000, was it 50,000 Cypher 10? Yes, 50,000 training, splitting it into four different sets.
[00:53:47.000 --> 00:54:03.000]   Each one of these sets has 12,500 images, and I'm using data sampler here, random data sampler from subset random sampler to give me those 12,500 images from each set.
[00:54:03.000 --> 00:54:16.000]   Why I have four, each two of them will be used to train a model, whether it's two shadow models or whether it's one model that you're trying to attack and building a shadow model, it's up to you.
[00:54:16.000 --> 00:54:24.000]   And each one of these models will have will need two sets. One of them is the data sets that were used in training the model.
[00:54:24.000 --> 00:54:35.000]   The other one is the dataset that was not used in training the model. And those two will later consist the labels for your binary classifier.
[00:54:35.000 --> 00:54:39.000]   So, that's it.
[00:54:39.000 --> 00:54:49.000]   Training a regular neural network, a function for evaluating a neural network, reusing some of the code here to build the ResNet neural networks.
[00:54:49.000 --> 00:54:59.000]   I know the problem at hand is very easy, but just trying to use ResNet's pre-trained models.
[00:54:59.000 --> 00:55:04.000]   Just for hysterical reasons, one of them is called Alice, the other one is called Bob.
[00:55:04.000 --> 00:55:10.000]   Now I'm thinking about it, it's a little bit funny, but perhaps I should call the target model and the shadow model, make it more clear.
[00:55:10.000 --> 00:55:16.000]   But one of them is ResNet50, the other one ResNet100, I think 110.
[00:55:16.000 --> 00:55:21.000]   And now we're building the attack model. Nothing crazy here.
[00:55:21.000 --> 00:55:31.000]   Straightforward binary classifier, fully connected layers that we'll use to create the attack network.
[00:55:31.000 --> 00:55:40.000]   More just validation, source code, and our attack is able to achieve almost 80% accuracy.
[00:55:40.000 --> 00:55:56.000]   So, this will also be true for most of the well-known architectures, most of the well-known datasets that we have publicly available.
[00:55:56.000 --> 00:55:59.000]   The MS datasets, the ImageNet, and so on and so forth.
[00:55:59.000 --> 00:56:06.000]   But this also transfers to well-structured applications, such as medical applications.
[00:56:06.000 --> 00:56:18.000]   And we were able to prove in the lab that even some of the well-trained medical models, they are leaking just crazy amounts of data.
[00:56:18.000 --> 00:56:25.000]   If you look at model inversion techniques, not only are you able to tell whether a specific data item existed in the dataset or not,
[00:56:25.000 --> 00:56:31.000]   but you're also able to retrieve some data from that data item.
[00:56:31.000 --> 00:56:36.000]   Model inversion is actually very simple, I think. I can't talk about it for two minutes.
[00:56:36.000 --> 00:56:44.000]   Basically, you can get the outputs of one model and create an attack model that does something like an autoencoder.
[00:56:44.000 --> 00:56:49.000]   So, the bottleneck layer is the output of the target model.
[00:56:49.000 --> 00:56:55.000]   And then you bring a dataset whose data distribution is very similar to the target model that you're trying to attack.
[00:56:55.000 --> 00:57:07.000]   And you just use the autoencoder architecture to reconstruct those inputs from the output of the distribution.
[00:57:07.000 --> 00:57:10.000]   So, this is simply the membership inference attacks.
[00:57:10.000 --> 00:57:20.000]   And this is one of the first steps to show how people have really been bracing to breaking the state of the art of deep learning models
[00:57:20.000 --> 00:57:25.000]   while ignoring very important issues and concerns around them.
[00:57:25.000 --> 00:57:37.000]   And one of the previous, actually before working on this current task, I was working on the lifecycle development.
[00:57:37.000 --> 00:57:40.000]   And actually, Weights and Biases does something really amazing.
[00:57:40.000 --> 00:57:47.000]   And this actually reminds me about the gold rush in California, I think 1840, something like this.
[00:57:47.000 --> 00:57:55.000]   People heard that there's gold in California. Everybody started rushing to mine for gold to be able to find gold, right?
[00:57:55.000 --> 00:58:00.000]   But the true entrepreneurs are those who were able to tackle the important things.
[00:58:00.000 --> 00:58:10.000]   Some people made some money from gold, but the people who made most money at that time are those who bought lands or rented lands.
[00:58:10.000 --> 00:58:18.000]   Those who made the shovels, the boots, and the jeans for people who are mining for gold.
[00:58:18.000 --> 00:58:20.000]   So, something similar happening nowadays.
[00:58:20.000 --> 00:58:27.000]   Everybody since 2010s until now, everybody has been rushing towards creating the best architecture.
[00:58:27.000 --> 00:58:32.000]   And after that, the best models breaking the state of the art.
[00:58:32.000 --> 00:58:42.000]   But the issues of the model management and privacy now are becoming to the surface of our research and our application nowadays.
[00:58:42.000 --> 00:58:52.000]   So, even Tesla cars have been fooled and their models have been fooled into and tricked into producing wrong results.
[00:58:52.000 --> 00:58:56.000]   Very well trained models have done the same thing as well.
[00:58:56.000 --> 00:59:15.000]   And lots of the machine learning and deep learning models that are deployed in very reputable systems are actually leaking some amount about users and the data that are included in training such models.
[00:59:15.000 --> 00:59:20.000]   And therefore, this area is really going to be a very hot topic this year.
[00:59:20.000 --> 00:59:27.000]   And the current pandemic, even though everything around it has been terrible, but has actually accelerated this area.
[00:59:27.000 --> 00:59:38.000]   Because when technology again tried to solve these problems by tracking people who have the virus and their interactions with other people, they were struck and hit by the wall of privacy preserving issues.
[00:59:38.000 --> 00:59:43.000]   So, now we really need to solve and focus on such issues.
[00:59:43.000 --> 00:59:57.000]   So, what are some of the approaches to prevent or to hinder or to slow down privacy leakage, specifically in a membership inference attack?
[00:59:57.000 --> 01:00:07.000]   Intuitively speaking, what we are studying in a membership inference attack is the relationship between the input image and the output of the target model.
[01:00:07.000 --> 01:00:13.000]   So, the output of the target model is most likely a vector of probabilities.
[01:00:13.000 --> 01:00:25.000]   So, if we limit those probabilities to only the best case, only to the top probability, then we are somehow reducing or slowing down membership inference attacks.
[01:00:25.000 --> 01:00:49.000]   So, a very trivial solution could be really effective, but a very dedicated adversaries or people who are able to manage well-designed membership inference attacks will still be able to leak information and know whether a specific item was used in training the model only by the top probability.
[01:00:49.000 --> 01:00:54.000]   Another solution is to coarsen the predictions.
[01:00:54.000 --> 01:01:07.000]   So, just coarsen the predictions means, like, I forgot what's the correct English word there.
[01:01:07.000 --> 01:01:10.000]   Sometimes it just goes into your mind.
[01:01:10.000 --> 01:01:17.000]   But if we have a probability that's 0.095, then we say it's 0.096, for example.
[01:01:17.000 --> 01:01:33.000]   So, we are coarsening the predictions of the vector, and we're not adding noise per se here, but we are improving the results a little bit so that we are trying to give less information about the probability distribution.
[01:01:33.000 --> 01:01:41.000]   Of course, again, by the definition of membership inference attacks, overfit is one of the main reasons, but it is not the only reason.
[01:01:41.000 --> 01:01:52.000]   And therefore, reducing overfitting or generalizing your model, the better you generalize it, the less likely it is to be infected by a membership inference attack.
[01:01:52.000 --> 01:02:07.000]   But, again, regularization is not an easy topic, and having a well-generalized model sometimes means dropping in its accuracy, and that's not preferable for industry.
[01:02:07.000 --> 01:02:26.000]   Even 0.5% increase or decrease in accuracy could mean lots and lots of money for an industry, and therefore they might sacrifice, unfortunately, some user privacy to keep that money flowing or things like that.
[01:02:26.000 --> 01:02:35.000]   One of the potential solutions that has been also coming to the surface lately is the differential privacy.
[01:02:35.000 --> 01:02:39.000]   So, quickly going over it, what is differential privacy?
[01:02:39.000 --> 01:02:45.000]   Basically, one of the main contributors in this field, if you want to read more about it, is Cynthia Dwork.
[01:02:45.000 --> 01:02:50.000]   I believe she was a researcher at Google.
[01:02:50.000 --> 01:03:11.000]   Basically, differential privacy is a guarantee or a protocol that tells you, if you run your statistical query on this dataset, then we guarantee this much that the privacy of people in this dataset will be preserved.
[01:03:11.000 --> 01:03:24.000]   In other words, differential privacy answers the following question, or actually asks the following question.
[01:03:24.000 --> 01:03:35.000]   If I have a dataset, and let's assume that this is some feature about people or participants in this dataset or database that are private.
[01:03:35.000 --> 01:03:41.000]   So, this could be the gender, this could be some type of whether you have a disease or not.
[01:03:41.000 --> 01:03:56.000]   Anyways, this is some private information that we want to make sure that if anybody runs a query on this database, will not affect the privacy of the users who are participating in this dataset.
[01:03:56.000 --> 01:04:17.000]   So, differential privacy says, if you run your query on this dataset, and then you remove one of the participants from this dataset, and use the same query again on the same dataset without a specific participant, will the query, the result of the query, be the same between both queries?
[01:04:17.000 --> 01:04:23.000]   If it is the same and exactly the same, then you have a perfect privacy.
[01:04:23.000 --> 01:04:38.000]   If it is not, this means that the absence or existence of specific data point or specific patient in this dataset is affecting the query, which means that the query is exposing information about this dataset.
[01:04:38.000 --> 01:04:51.000]   And therefore, the question here becomes, or the research problem becomes, can we construct a query that does not change, or maybe change but very little, no matter what we remove from the database?
[01:04:51.000 --> 01:04:59.000]   And to give you a very simple example here, the sum query on a database is not differentially private.
[01:04:59.000 --> 01:05:09.000]   The sum query. So basically, if you want to know the sum of all the data points on your dataset, and this is because if you add this dataset, the query is the sum.
[01:05:09.000 --> 01:05:12.000]   If you added it, the result is one, two, three, four.
[01:05:12.000 --> 01:05:17.000]   If you remove one of these participants, let's say the last participant, now the query will be three.
[01:05:17.000 --> 01:05:28.000]   And simply by subtracting these two queries, we will know that the value of the data point that was removed is one, and therefore we are leaking lots of information about this.
[01:05:28.000 --> 01:05:36.000]   Lots of information, or we're leaking information about specific people in the dataset, and/or specific data records or data items.
[01:05:36.000 --> 01:05:47.000]   So what we do in differential privacy is we create parallel datasets or databases, and each one of them we take one specific item.
[01:05:47.000 --> 01:05:55.000]   We run the query across all of them, and then we find the maximum difference between the query on the original database and any one of the parallel databases.
[01:05:55.000 --> 01:06:01.000]   And that difference, the maximum difference in the query results, is called sensitivity.
[01:06:01.000 --> 01:06:07.000]   And therefore the sensitivity of the sum query here is one, which is a little bit high.
[01:06:07.000 --> 01:06:20.000]   So you want to reduce that sensitivity as much as possible, and when you reduce it very much, that the query on any one of these parallel databases equals to the query on the original database is the same.
[01:06:20.000 --> 01:06:33.000]   Then you have perfect privacy preserving query, which perhaps sometimes might not be useful if you're really not learning anything when you're removing or adding or replacing different people.
[01:06:33.000 --> 01:06:37.000]   And some other times might be just impractical, right?
[01:06:37.000 --> 01:06:41.000]   How does differential privacy work in real life?
[01:06:41.000 --> 01:06:44.000]   Differential privacy focuses on adding noise to the data.
[01:06:44.000 --> 01:06:47.000]   And we add noise to the data in two different ways.
[01:06:47.000 --> 01:06:51.000]   First way is called the global differential privacy.
[01:06:51.000 --> 01:06:57.000]   You run your query on the data on a plain database that has data records in a plain text.
[01:06:57.000 --> 01:07:00.000]   In other words, they are not encrypted.
[01:07:00.000 --> 01:07:03.000]   You have five more minutes left.
[01:07:03.000 --> 01:07:04.000]   I'm sorry.
[01:07:04.000 --> 01:07:05.000]   Okay, cool.
[01:07:05.000 --> 01:07:14.000]   So in a global differential privacy, you run your query on unencrypted data, and then you add noise to your query.
[01:07:14.000 --> 01:07:25.000]   It makes it -- the accuracy results becomes really good, but you have to trust the database curator.
[01:07:25.000 --> 01:07:30.000]   Different way to adding noise is called local differential privacy.
[01:07:30.000 --> 01:07:36.000]   In local differential privacy, we add noise to every data sample by itself.
[01:07:36.000 --> 01:07:40.000]   And one of the very famous ways here is called randomized response.
[01:07:40.000 --> 01:07:44.000]   And the idea here is really fun, and I'm going to end with it.
[01:07:44.000 --> 01:08:01.000]   The idea about it here is that we as humans or as people who are participating in a query or statistical analysis or volunteering with our health records to create a deep learning model, we don't necessarily need to answer the truth.
[01:08:01.000 --> 01:08:08.000]   And this concept comes from some sociology studies, randomized response.
[01:08:08.000 --> 01:08:10.000]   The idea is simple.
[01:08:10.000 --> 01:08:15.000]   So assume the government wants to know what is the percentage of people that use illegal drugs.
[01:08:15.000 --> 01:08:26.000]   Of course, if the government cannot interview every single person and ask them do you smoke or not, we are never going to answer -- never going to give the true answer right.
[01:08:26.000 --> 01:08:36.000]   However, even if the government says I'm not really interested in knowing whether you specifically smoke or not, we're still not going to answer the true response.
[01:08:36.000 --> 01:08:46.000]   So the idea here is that the government in this case is going to give every participant a coin and tell the participant go ahead and flip this coin twice.
[01:08:46.000 --> 01:08:51.000]   If the first flip is heads, then you must answer the truth.
[01:08:51.000 --> 01:08:56.000]   If the first flip is tails, then you answer based on the second flip.
[01:08:56.000 --> 01:09:11.000]   In this way, even if the person says yes, I smoke, the government does not know whether they said yes, they smoke because they flipped the first time heads and they said the true answer or whether they flipped it twice and they answered just randomly.
[01:09:11.000 --> 01:09:18.000]   However, at the end of the day, using statistics, we will be able to know the percentage of people that actually smoke.
[01:09:18.000 --> 01:09:24.000]   So this is how differential privacy can help us in so many different approaches.
[01:09:24.000 --> 01:09:26.000]   So thank you very much.
[01:09:26.000 --> 01:09:28.000]   Sorry for going over time.
[01:09:28.000 --> 01:09:30.000]   I'm not sure.
[01:09:30.000 --> 01:09:32.000]   Maybe I'm not.
[01:09:32.000 --> 01:09:34.000]   But if you have any questions, please feel free to ask them.
[01:09:34.000 --> 01:09:36.000]   I'm on Twitter.
[01:09:36.000 --> 01:09:38.000]   You can always ask me any questions there if you want.
[01:09:38.000 --> 01:09:40.000]   >> Thanks.
[01:09:40.000 --> 01:09:42.000]   People love to talk.
[01:09:42.000 --> 01:09:44.000]   There are a lot of questions.
[01:09:44.000 --> 01:09:46.000]   I just wanted to make sure we have enough time left for everyone.
[01:09:46.000 --> 01:09:48.000]   I'm sorry.
[01:09:48.000 --> 01:09:50.000]   I'm going to ask a question.
[01:09:50.000 --> 01:09:52.000]   I'm going to ask a question.
[01:09:52.000 --> 01:09:54.000]   I'm going to ask a question.
[01:09:54.000 --> 01:09:56.000]   I'm going to ask a question.
[01:09:56.000 --> 01:09:58.000]   I'm going to ask a question.
[01:09:58.000 --> 01:10:00.000]   I'm going to ask a question.
[01:10:00.000 --> 01:10:02.000]   I'm going to ask a question.
[01:10:02.000 --> 01:10:04.000]   I'm going to ask a question.
[01:10:04.000 --> 01:10:06.000]   I'm going to ask a question.
[01:10:06.000 --> 01:10:08.000]   I'm going to ask a question.
[01:10:08.000 --> 01:10:10.000]   I'm going to ask a question.
[01:10:10.000 --> 01:10:12.000]   I'm going to ask a question.
[01:10:12.000 --> 01:10:14.000]   I'm going to ask a question.
[01:10:14.000 --> 01:10:16.000]   I'm going to ask a question.
[01:10:16.000 --> 01:10:18.000]   I'm going to ask a question.
[01:10:18.000 --> 01:10:20.000]   I'm going to ask a question.
[01:10:20.000 --> 01:10:22.000]   I'm going to ask a question.
[01:10:22.000 --> 01:10:24.000]   I'm going to ask a question.
[01:10:24.000 --> 01:10:26.000]   I'm going to ask a question.
[01:10:26.000 --> 01:10:28.000]   I'm going to ask a question.
[01:10:28.000 --> 01:10:30.000]   I'm going to ask a question.
[01:10:30.000 --> 01:10:32.000]   I'm going to ask a question.
[01:10:32.000 --> 01:10:34.000]   I'm going to ask a question.
[01:10:34.000 --> 01:10:36.000]   I'm going to ask a question.
[01:10:36.000 --> 01:10:38.000]   I'm going to ask a question.
[01:10:38.000 --> 01:10:40.000]   I'm going to ask a question.
[01:10:40.000 --> 01:10:42.000]   I'm going to ask a question.
[01:10:42.000 --> 01:10:44.000]   I'm going to ask a question.
[01:10:44.000 --> 01:10:46.000]   I'm going to ask a question.
[01:10:46.000 --> 01:10:48.000]   I'm going to ask a question.
[01:10:48.000 --> 01:10:50.000]   I'm going to ask a question.
[01:10:50.000 --> 01:10:52.000]   I'm going to ask a question.
[01:10:52.000 --> 01:10:54.000]   I'm going to ask a question.
[01:10:54.000 --> 01:10:56.000]   I'm going to ask a question.
[01:10:56.000 --> 01:10:58.000]   I'm going to ask a question.
[01:10:58.000 --> 01:11:00.000]   I'm going to ask a question.
[01:11:00.000 --> 01:11:02.000]   I'm going to ask a question.
[01:11:02.000 --> 01:11:04.000]   I'm going to ask a question.
[01:11:04.000 --> 01:11:06.000]   I'm going to ask a question.
[01:11:06.000 --> 01:11:08.000]   I'm going to ask a question.
[01:11:08.000 --> 01:11:10.000]   I'm going to ask a question.
[01:11:10.000 --> 01:11:12.000]   I'm going to ask a question.
[01:11:12.000 --> 01:11:14.000]   I'm going to ask a question.
[01:11:14.000 --> 01:11:16.000]   I'm going to ask a question.
[01:11:16.000 --> 01:11:18.000]   I'm going to ask a question.
[01:11:18.000 --> 01:11:20.000]   I'm going to ask a question.
[01:11:20.000 --> 01:11:22.000]   I'm going to ask a question.
[01:11:22.000 --> 01:11:24.000]   I'm going to ask a question.
[01:11:24.000 --> 01:11:26.000]   I'm going to ask a question.
[01:11:26.000 --> 01:11:28.000]   I'm going to ask a question.
[01:11:28.000 --> 01:11:30.000]   I'm going to ask a question.
[01:11:30.000 --> 01:11:32.000]   I'm going to ask a question.
[01:11:32.000 --> 01:11:34.000]   I'm going to ask a question.
[01:11:34.000 --> 01:11:36.000]   I'm going to ask a question.
[01:11:36.000 --> 01:11:38.000]   I'm going to ask a question.
[01:11:38.000 --> 01:11:40.000]   I'm going to ask a question.
[01:11:40.000 --> 01:11:42.000]   I'm going to ask a question.
[01:11:42.000 --> 01:11:44.000]   I'm going to ask a question.
[01:11:44.000 --> 01:11:46.000]   I'm going to ask a question.
[01:11:46.000 --> 01:11:48.000]   I'm going to ask a question.
[01:11:48.000 --> 01:11:50.000]   I'm going to ask a question.
[01:11:50.000 --> 01:11:52.000]   I'm going to ask a question.
[01:11:52.000 --> 01:11:54.000]   I'm going to ask a question.
[01:11:54.000 --> 01:11:56.000]   I'm going to ask a question.
[01:11:56.000 --> 01:11:58.000]   I'm going to ask a question.
[01:11:58.000 --> 01:12:00.000]   I'm going to ask a question.
[01:12:00.000 --> 01:12:02.000]   I'm going to ask a question.
[01:12:02.000 --> 01:12:04.000]   I'm going to ask a question.
[01:12:04.000 --> 01:12:06.000]   I'm going to ask a question.
[01:12:06.000 --> 01:12:08.000]   I'm going to ask a question.
[01:12:08.000 --> 01:12:10.000]   I'm going to ask a question.
[01:12:10.000 --> 01:12:12.000]   I'm going to ask a question.
[01:12:12.000 --> 01:12:14.000]   I'm going to ask a question.
[01:12:14.000 --> 01:12:16.000]   I'm going to ask a question.
[01:12:16.000 --> 01:12:18.000]   I'm going to ask a question.
[01:12:18.000 --> 01:12:20.000]   I'm going to ask a question.
[01:12:20.000 --> 01:12:22.000]   I'm going to ask a question.
[01:12:22.000 --> 01:12:24.000]   I'm going to ask a question.
[01:12:24.000 --> 01:12:26.000]   I'm going to ask a question.
[01:12:26.000 --> 01:12:28.000]   I'm going to ask a question.
[01:12:28.000 --> 01:12:30.000]   I'm going to ask a question.
[01:12:30.000 --> 01:12:32.000]   I'm going to ask a question.
[01:12:32.000 --> 01:12:34.000]   I'm going to ask a question.
[01:12:34.000 --> 01:12:36.000]   I'm going to ask a question.
[01:12:36.000 --> 01:12:38.000]   I'm going to ask a question.
[01:12:38.000 --> 01:12:40.000]   I'm going to ask a question.
[01:12:40.000 --> 01:12:42.000]   I'm going to ask a question.
[01:12:42.000 --> 01:12:44.000]   I'm going to ask a question.
[01:12:44.000 --> 01:12:46.000]   I'm going to ask a question.
[01:12:46.000 --> 01:12:48.000]   I'm going to ask a question.
[01:12:48.000 --> 01:12:50.000]   I'm going to ask a question.
[01:12:50.000 --> 01:12:52.000]   I'm going to ask a question.
[01:12:52.000 --> 01:12:54.000]   I'm going to ask a question.
[01:12:54.000 --> 01:12:56.000]   I'm going to ask a question.
[01:12:56.000 --> 01:12:58.000]   I'm going to ask a question.
[01:12:58.000 --> 01:13:00.000]   I'm going to ask a question.
[01:13:00.000 --> 01:13:02.000]   I'm going to ask a question.
[01:13:02.000 --> 01:13:04.000]   I'm going to ask a question.
[01:13:04.000 --> 01:13:06.000]   I'm going to ask a question.
[01:13:06.000 --> 01:13:08.000]   I'm going to ask a question.
[01:13:08.000 --> 01:13:10.000]   I'm going to ask a question.
[01:13:10.000 --> 01:13:12.000]   I'm going to ask a question.
[01:13:12.000 --> 01:13:14.000]   I'm going to ask a question.
[01:13:14.000 --> 01:13:16.000]   I'm going to ask a question.
[01:13:16.000 --> 01:13:18.000]   I'm going to ask a question.
[01:13:18.000 --> 01:13:20.000]   I'm going to ask a question.
[01:13:20.000 --> 01:13:22.000]   I'm going to ask a question.
[01:13:22.000 --> 01:13:24.000]   I'm going to ask a question.
[01:13:24.000 --> 01:13:26.000]   I'm going to ask a question.
[01:13:26.000 --> 01:13:28.000]   I'm going to ask a question.
[01:13:28.000 --> 01:13:30.000]   I'm going to ask a question.
[01:13:30.000 --> 01:13:32.000]   I'm going to ask a question.
[01:13:32.000 --> 01:13:34.000]   I'm going to ask a question.
[01:13:34.000 --> 01:13:36.000]   I'm going to ask a question.
[01:13:36.000 --> 01:13:38.000]   I'm going to ask a question.
[01:13:38.000 --> 01:13:40.000]   I'm going to ask a question.
[01:13:40.000 --> 01:13:42.000]   I'm going to ask a question.
[01:13:42.000 --> 01:13:44.000]   I'm going to ask a question.
[01:13:44.000 --> 01:13:46.000]   I'm going to ask a question.
[01:13:46.000 --> 01:13:48.000]   I'm going to ask a question.
[01:13:48.000 --> 01:13:50.000]   I'm going to ask a question.
[01:13:50.000 --> 01:13:52.000]   I'm going to ask a question.
[01:13:52.000 --> 01:13:54.000]   I'm going to ask a question.
[01:13:54.000 --> 01:13:56.000]   I'm going to ask a question.
[01:13:56.000 --> 01:13:58.000]   I'm going to ask a question.
[01:13:58.000 --> 01:14:00.000]   I'm going to ask a question.
[01:14:00.000 --> 01:14:02.000]   I'm going to ask a question.
[01:14:02.000 --> 01:14:04.000]   I'm going to ask a question.
[01:14:04.000 --> 01:14:06.000]   I'm going to ask a question.
[01:14:06.000 --> 01:14:08.000]   I'm going to ask a question.
[01:14:08.000 --> 01:14:10.000]   I'm going to ask a question.
[01:14:10.000 --> 01:14:12.000]   I'm going to ask a question.
[01:14:12.000 --> 01:14:14.000]   I'm going to ask a question.
[01:14:14.000 --> 01:14:16.000]   I'm going to ask a question.
[01:14:16.000 --> 01:14:18.000]   I'm going to ask a question.
[01:14:18.000 --> 01:14:20.000]   I'm going to ask a question.
[01:14:20.000 --> 01:14:22.000]   I'm going to ask a question.
[01:14:22.000 --> 01:14:24.000]   I'm going to ask a question.
[01:14:24.000 --> 01:14:26.000]   I'm going to ask a question.
[01:14:26.000 --> 01:14:28.000]   I'm going to ask a question.
[01:14:28.000 --> 01:14:30.000]   I'm going to ask a question.
[01:14:30.000 --> 01:14:32.000]   I'm going to ask a question.
[01:14:32.000 --> 01:14:34.000]   I'm going to ask a question.
[01:14:34.000 --> 01:14:36.000]   I'm going to ask a question.
[01:14:36.000 --> 01:14:38.000]   I'm going to ask a question.
[01:14:38.000 --> 01:14:40.000]   I'm going to ask a question.
[01:14:40.000 --> 01:14:42.000]   I'm going to ask a question.
[01:14:42.000 --> 01:14:44.000]   I'm going to ask a question.
[01:14:44.000 --> 01:14:46.000]   I'm going to ask a question.
[01:14:46.000 --> 01:14:48.000]   I'm going to ask a question.
[01:14:48.000 --> 01:14:50.000]   I'm going to ask a question.
[01:14:50.000 --> 01:14:52.000]   I'm going to ask a question.
[01:14:52.000 --> 01:14:54.000]   I'm going to ask a question.
[01:14:54.000 --> 01:14:56.000]   I'm going to ask a question.
[01:14:56.000 --> 01:14:58.000]   I'm going to ask a question.
[01:14:58.000 --> 01:15:00.000]   I'm going to ask a question.
[01:15:00.000 --> 01:15:02.000]   I'm going to ask a question.
[01:15:02.000 --> 01:15:04.000]   I'm going to ask a question.
[01:15:04.000 --> 01:15:06.000]   I'm going to ask a question.
[01:15:06.000 --> 01:15:08.000]   I'm going to ask a question.
[01:15:08.000 --> 01:15:10.000]   I'm going to ask a question.
[01:15:10.000 --> 01:15:12.000]   I'm going to ask a question.
[01:15:12.000 --> 01:15:14.000]   I'm going to ask a question.
[01:15:14.000 --> 01:15:16.000]   I'm going to ask a question.
[01:15:16.000 --> 01:15:18.000]   I'm going to ask a question.
[01:15:18.000 --> 01:15:20.000]   I'm going to ask a question.
[01:15:20.000 --> 01:15:22.000]   I'm going to ask a question.
[01:15:22.000 --> 01:15:24.000]   I'm going to ask a question.
[01:15:24.000 --> 01:15:26.000]   I'm going to ask a question.
[01:15:26.000 --> 01:15:28.000]   I'm going to ask a question.
[01:15:28.000 --> 01:15:30.000]   I'm going to ask a question.
[01:15:30.000 --> 01:15:32.000]   I'm going to ask a question.
[01:15:32.000 --> 01:15:34.000]   I'm going to ask a question.
[01:15:34.000 --> 01:15:36.000]   I'm going to ask a question.
[01:15:36.000 --> 01:15:38.000]   I'm going to ask a question.
[01:15:38.000 --> 01:15:40.000]   I'm going to ask a question.
[01:15:40.000 --> 01:15:42.000]   I'm going to ask a question.
[01:15:42.000 --> 01:15:44.000]   I'm going to ask a question.
[01:15:44.000 --> 01:15:46.000]   I'm going to ask a question.
[01:15:46.000 --> 01:15:48.000]   I'm going to ask a question.
[01:15:48.000 --> 01:15:50.000]   I'm going to ask a question.
[01:15:50.000 --> 01:15:52.000]   I'm going to ask a question.
[01:15:52.000 --> 01:15:54.000]   I'm going to ask a question.
[01:15:54.000 --> 01:15:56.000]   I'm going to ask a question.
[01:15:56.000 --> 01:15:58.000]   I'm going to ask a question.
[01:15:58.000 --> 01:16:00.000]   I'm going to ask a question.
[01:16:00.000 --> 01:16:02.000]   I'm going to ask a question.
[01:16:02.000 --> 01:16:04.000]   I'm going to ask a question.
[01:16:04.000 --> 01:16:06.000]   I'm going to ask a question.
[01:16:06.000 --> 01:16:08.000]   I'm going to ask a question.
[01:16:08.000 --> 01:16:10.000]   I'm going to ask a question.
[01:16:10.000 --> 01:16:12.000]   I'm going to ask a question.
[01:16:12.000 --> 01:16:14.000]   I'm going to ask a question.
[01:16:14.000 --> 01:16:16.000]   I'm going to ask a question.
[01:16:16.000 --> 01:16:18.000]   I'm going to ask a question.
[01:16:18.000 --> 01:16:20.000]   I'm going to ask a question.
[01:16:20.000 --> 01:16:22.000]   I'm going to ask a question.
[01:16:22.000 --> 01:16:24.000]   I'm going to ask a question.
[01:16:24.000 --> 01:16:26.000]   I'm going to ask a question.
[01:16:26.000 --> 01:16:28.000]   I'm going to ask a question.
[01:16:28.000 --> 01:16:30.000]   I'm going to ask a question.
[01:16:30.000 --> 01:16:32.000]   I'm going to ask a question.
[01:16:32.000 --> 01:16:34.000]   I'm going to ask a question.
[01:16:34.000 --> 01:16:36.000]   I'm going to ask a question.
[01:16:36.000 --> 01:16:38.000]   I'm going to ask a question.
[01:16:38.000 --> 01:16:40.000]   I'm going to ask a question.
[01:16:40.000 --> 01:16:42.000]   I'm going to ask a question.
[01:16:42.000 --> 01:16:44.000]   I'm going to ask a question.
[01:16:44.000 --> 01:16:46.000]   I'm going to ask a question.
[01:16:46.000 --> 01:16:48.000]   I'm going to ask a question.
[01:16:48.000 --> 01:16:50.000]   I'm going to ask a question.
[01:16:50.000 --> 01:16:52.000]   I'm going to ask a question.
[01:16:52.000 --> 01:16:54.000]   I'm going to ask a question.
[01:16:54.000 --> 01:16:56.000]   I'm going to ask a question.
[01:16:56.000 --> 01:16:58.000]   I'm going to ask a question.
[01:16:58.000 --> 01:17:00.000]   I'm going to ask a question.
[01:17:00.000 --> 01:17:02.000]   I'm going to ask a question.
[01:17:02.000 --> 01:17:04.000]   I'm going to ask a question.
[01:17:04.000 --> 01:17:06.000]   I'm going to ask a question.
[01:17:06.000 --> 01:17:08.000]   I'm going to ask a question.
[01:17:08.000 --> 01:17:10.000]   I'm going to ask a question.
[01:17:10.000 --> 01:17:12.000]   I'm going to ask a question.
[01:17:12.000 --> 01:17:14.000]   I'm going to ask a question.
[01:17:14.000 --> 01:17:16.000]   I'm going to ask a question.
[01:17:16.000 --> 01:17:18.000]   I'm going to ask a question.
[01:17:18.000 --> 01:17:20.000]   I'm going to ask a question.
[01:17:20.000 --> 01:17:22.000]   I'm going to ask a question.
[01:17:22.000 --> 01:17:24.000]   I'm going to ask a question.
[01:17:24.000 --> 01:17:26.000]   I'm going to ask a question.
[01:17:26.000 --> 01:17:28.000]   I'm going to ask a question.
[01:17:28.000 --> 01:17:30.000]   I'm going to ask a question.
[01:17:30.000 --> 01:17:32.000]   I'm going to ask a question.
[01:17:32.000 --> 01:17:34.000]   I'm going to ask a question.
[01:17:34.000 --> 01:17:36.000]   I'm going to ask a question.
[01:17:36.000 --> 01:17:38.000]   I'm going to ask a question.
[01:17:38.000 --> 01:17:40.000]   I'm going to ask a question.
[01:17:40.000 --> 01:17:42.000]   I'm going to ask a question.
[01:17:42.000 --> 01:17:44.000]   I'm going to ask a question.
[01:17:44.000 --> 01:17:46.000]   I'm going to ask a question.
[01:17:46.000 --> 01:17:48.000]   I'm going to ask a question.
[01:17:48.000 --> 01:17:50.000]   I'm going to ask a question.
[01:17:50.000 --> 01:17:52.000]   I'm going to ask a question.
[01:17:52.000 --> 01:17:54.000]   I'm going to ask a question.
[01:17:54.000 --> 01:17:56.000]   I'm going to ask a question.
[01:17:56.000 --> 01:17:58.000]   I'm going to ask a question.
[01:17:58.000 --> 01:18:00.000]   I'm going to ask a question.
[01:18:00.000 --> 01:18:02.000]   I'm going to ask a question.
[01:18:02.000 --> 01:18:04.000]   I'm going to ask a question.
[01:18:04.000 --> 01:18:06.000]   I'm going to ask a question.
[01:18:06.000 --> 01:18:08.000]   I'm going to ask a question.
[01:18:08.000 --> 01:18:10.000]   I'm going to ask a question.
[01:18:10.000 --> 01:18:12.000]   I'm going to ask a question.
[01:18:12.000 --> 01:18:14.000]   I'm going to ask a question.
[01:18:14.000 --> 01:18:16.000]   I'm going to ask a question.
[01:18:16.000 --> 01:18:18.000]   I'm going to ask a question.
[01:18:18.000 --> 01:18:20.000]   I'm going to ask a question.
[01:18:20.000 --> 01:18:22.000]   I'm going to ask a question.
[01:18:22.000 --> 01:18:24.000]   I'm going to ask a question.
[01:18:24.000 --> 01:18:26.000]   I'm going to ask a question.
[01:18:26.000 --> 01:18:28.000]   I'm going to ask a question.
[01:18:28.000 --> 01:18:30.000]   I'm going to ask a question.
[01:18:30.000 --> 01:18:32.000]   I'm going to ask a question.
[01:18:32.000 --> 01:18:34.000]   I'm going to ask a question.
[01:18:34.000 --> 01:18:36.000]   I'm going to ask a question.
[01:18:36.000 --> 01:18:38.000]   I'm going to ask a question.
[01:18:38.000 --> 01:18:40.000]   I'm going to ask a question.
[01:18:40.000 --> 01:18:42.000]   I'm going to ask a question.
[01:18:42.000 --> 01:18:44.000]   I'm going to ask a question.
[01:18:44.000 --> 01:18:46.000]   I'm going to ask a question.
[01:18:46.000 --> 01:18:48.000]   I'm going to ask a question.
[01:18:48.000 --> 01:18:50.000]   I'm going to ask a question.
[01:18:50.000 --> 01:18:52.000]   I'm going to ask a question.
[01:18:52.000 --> 01:18:54.000]   I'm going to ask a question.
[01:18:54.000 --> 01:18:56.000]   I'm going to ask a question.
[01:18:56.000 --> 01:18:58.000]   I'm going to ask a question.
[01:18:58.000 --> 01:19:00.000]   I'm going to ask a question.
[01:19:00.000 --> 01:19:02.000]   I'm going to ask a question.
[01:19:02.000 --> 01:19:04.000]   I'm going to ask a question.
[01:19:04.000 --> 01:19:06.000]   I'm going to ask a question.
[01:19:06.000 --> 01:19:08.000]   I'm going to ask a question.
[01:19:08.000 --> 01:19:10.000]   I'm going to ask a question.
[01:19:10.000 --> 01:19:12.000]   I'm going to ask a question.
[01:19:12.000 --> 01:19:14.000]   I'm going to ask a question.
[01:19:14.000 --> 01:19:16.000]   I'm going to ask a question.
[01:19:16.000 --> 01:19:18.000]   I'm going to ask a question.
[01:19:18.000 --> 01:19:20.000]   I'm going to ask a question.
[01:19:20.000 --> 01:19:22.000]   I'm going to ask a question.
[01:19:22.000 --> 01:19:24.000]   I'm going to ask a question.
[01:19:24.000 --> 01:19:26.000]   I'm going to ask a question.
[01:19:26.000 --> 01:19:28.000]   I'm going to ask a question.
[01:19:28.000 --> 01:19:30.000]   I'm going to ask a question.
[01:19:30.000 --> 01:19:32.000]   I'm going to ask a question.
[01:19:32.000 --> 01:19:34.000]   I'm going to ask a question.
[01:19:34.000 --> 01:19:36.000]   I'm going to ask a question.
[01:19:36.000 --> 01:19:38.000]   I'm going to ask a question.
[01:19:38.000 --> 01:19:40.000]   I'm going to ask a question.
[01:19:40.000 --> 01:19:42.000]   I'm going to ask a question.
[01:19:42.000 --> 01:19:44.000]   I'm going to ask a question.
[01:19:44.000 --> 01:19:46.000]   I'm going to ask a question.
[01:19:46.000 --> 01:19:48.000]   I'm going to ask a question.
[01:19:48.000 --> 01:19:50.000]   I'm going to ask a question.
[01:19:50.000 --> 01:19:52.000]   I'm going to ask a question.
[01:19:52.000 --> 01:19:54.000]   I'm going to ask a question.
[01:19:54.000 --> 01:19:56.000]   I'm going to ask a question.
[01:19:56.000 --> 01:19:58.000]   I'm going to ask a question.
[01:19:58.000 --> 01:20:00.000]   I'm going to ask a question.
[01:20:00.000 --> 01:20:02.000]   I'm going to ask a question.
[01:20:02.000 --> 01:20:04.000]   I'm going to ask a question.
[01:20:04.000 --> 01:20:06.000]   I'm going to ask a question.
[01:20:06.000 --> 01:20:08.000]   I'm going to ask a question.
[01:20:08.000 --> 01:20:10.000]   I'm going to ask a question.
[01:20:10.000 --> 01:20:12.000]   I'm going to ask a question.
[01:20:12.000 --> 01:20:14.000]   I'm going to ask a question.
[01:20:14.000 --> 01:20:16.000]   I'm going to ask a question.
[01:20:16.000 --> 01:20:18.000]   I'm going to ask a question.
[01:20:18.000 --> 01:20:20.000]   I'm going to ask a question.
[01:20:20.000 --> 01:20:22.000]   I'm going to ask a question.
[01:20:22.000 --> 01:20:24.000]   I'm going to ask a question.
[01:20:24.000 --> 01:20:26.000]   I'm going to ask a question.
[01:20:26.000 --> 01:20:28.000]   I'm going to ask a question.
[01:20:28.000 --> 01:20:30.000]   I'm going to ask a question.
[01:20:30.000 --> 01:20:32.000]   I'm going to ask a question.
[01:20:32.000 --> 01:20:34.000]   I'm going to ask a question.
[01:20:34.000 --> 01:20:36.000]   I'm going to ask a question.
[01:20:36.000 --> 01:20:38.000]   I'm going to ask a question.
[01:20:38.000 --> 01:20:40.000]   I'm going to ask a question.
[01:20:40.000 --> 01:20:42.000]   I'm going to ask a question.
[01:20:42.000 --> 01:20:44.000]   I'm going to ask a question.
[01:20:44.000 --> 01:20:46.000]   I'm going to ask a question.
[01:20:46.000 --> 01:20:48.000]   I'm going to ask a question.
[01:20:48.000 --> 01:20:50.000]   I'm going to ask a question.
[01:20:50.000 --> 01:20:52.000]   I'm going to ask a question.
[01:20:52.000 --> 01:20:54.000]   I'm going to ask a question.
[01:20:54.000 --> 01:20:56.000]   I'm going to ask a question.
[01:20:56.000 --> 01:20:58.000]   I'm going to ask a question.
[01:20:58.000 --> 01:21:00.000]   I'm going to ask a question.
[01:21:00.000 --> 01:21:02.000]   I'm going to ask a question.
[01:21:02.000 --> 01:21:04.000]   I'm going to ask a question.
[01:21:04.000 --> 01:21:06.000]   I'm going to ask a question.
[01:21:06.000 --> 01:21:08.000]   I'm going to ask a question.
[01:21:08.000 --> 01:21:10.000]   I'm going to ask a question.
[01:21:10.000 --> 01:21:12.000]   I'm going to ask a question.
[01:21:12.000 --> 01:21:14.000]   I'm going to ask a question.
[01:21:14.000 --> 01:21:16.000]   I'm going to ask a question.
[01:21:16.000 --> 01:21:18.000]   I'm going to ask a question.
[01:21:18.000 --> 01:21:20.000]   I'm going to ask a question.
[01:21:20.000 --> 01:21:22.000]   I'm going to ask a question.
[01:21:22.000 --> 01:21:24.000]   I'm going to ask a question.
[01:21:24.000 --> 01:21:26.000]   I'm going to ask a question.
[01:21:26.000 --> 01:21:28.000]   I'm going to ask a question.
[01:21:28.000 --> 01:21:30.000]   I'm going to ask a question.
[01:21:30.000 --> 01:21:32.000]   I'm going to ask a question.
[01:21:32.000 --> 01:21:34.000]   I'm going to ask a question.
[01:21:34.000 --> 01:21:36.000]   I'm going to ask a question.
[01:21:36.000 --> 01:21:38.000]   I'm going to ask a question.
[01:21:38.000 --> 01:21:40.000]   I'm going to ask a question.
[01:21:40.000 --> 01:21:42.000]   I'm going to ask a question.
[01:21:42.000 --> 01:21:44.000]   I'm going to ask a question.
[01:21:44.000 --> 01:21:46.000]   I'm going to ask a question.
[01:21:46.000 --> 01:21:48.000]   I'm going to ask a question.
[01:21:48.000 --> 01:21:50.000]   I'm going to ask a question.
[01:21:50.000 --> 01:21:52.000]   I'm going to ask a question.
[01:21:52.000 --> 01:21:54.000]   I'm going to ask a question.
[01:21:54.000 --> 01:21:56.000]   I'm going to ask a question.
[01:21:56.000 --> 01:21:58.000]   I'm going to ask a question.
[01:21:58.000 --> 01:22:00.000]   I'm going to ask a question.
[01:22:00.000 --> 01:22:02.000]   I'm going to ask a question.
[01:22:02.000 --> 01:22:04.000]   I'm going to ask a question.
[01:22:04.000 --> 01:22:06.000]   I'm going to ask a question.
[01:22:06.000 --> 01:22:08.000]   I'm going to ask a question.
[01:22:08.000 --> 01:22:10.000]   I'm going to ask a question.
[01:22:10.000 --> 01:22:12.000]   I'm going to ask a question.
[01:22:12.000 --> 01:22:14.000]   I'm going to ask a question.
[01:22:14.000 --> 01:22:16.000]   I'm going to ask a question.
[01:22:16.000 --> 01:22:18.000]   I'm going to ask a question.
[01:22:18.000 --> 01:22:20.000]   I'm going to ask a question.
[01:22:20.000 --> 01:22:22.000]   I'm going to ask a question.
[01:22:22.000 --> 01:22:24.000]   I'm going to ask a question.
[01:22:24.000 --> 01:22:26.000]   I'm going to ask a question.
[01:22:26.000 --> 01:22:28.000]   I'm going to ask a question.
[01:22:28.000 --> 01:22:30.000]   I'm going to ask a question.
[01:22:30.000 --> 01:22:32.000]   I'm going to ask a question.
[01:22:32.000 --> 01:22:34.000]   I'm going to ask a question.
[01:22:34.000 --> 01:22:36.000]   I'm going to ask a question.
[01:22:36.000 --> 01:22:38.000]   I'm going to ask a question.
[01:22:38.000 --> 01:22:40.000]   I'm going to ask a question.
[01:22:40.000 --> 01:22:42.000]   I'm going to ask a question.
[01:22:42.000 --> 01:22:44.000]   I'm going to ask a question.
[01:22:44.000 --> 01:22:46.000]   I'm going to ask a question.
[01:22:46.000 --> 01:22:48.000]   I'm going to ask a question.
[01:22:48.000 --> 01:22:50.000]   I'm going to ask a question.
[01:22:50.000 --> 01:22:52.000]   I'm going to ask a question.
[01:22:52.000 --> 01:22:54.000]   I'm going to ask a question.
[01:22:54.000 --> 01:22:56.000]   I'm going to ask a question.
[01:22:56.000 --> 01:22:58.000]   I'm going to ask a question.
[01:22:58.000 --> 01:23:00.000]   I'm going to ask a question.
[01:23:00.000 --> 01:23:02.000]   I'm going to ask a question.
[01:23:02.000 --> 01:23:04.000]   I'm going to ask a question.
[01:23:04.000 --> 01:23:06.000]   I'm going to ask a question.
[01:23:06.000 --> 01:23:08.000]   I'm going to ask a question.
[01:23:08.000 --> 01:23:10.000]   I'm going to ask a question.
[01:23:10.000 --> 01:23:12.000]   I'm going to ask a question.
[01:23:12.000 --> 01:23:14.000]   I'm going to ask a question.
[01:23:14.000 --> 01:23:16.000]   I'm going to ask a question.
[01:23:16.000 --> 01:23:18.000]   I'm going to ask a question.
[01:23:18.000 --> 01:23:20.000]   I'm going to ask a question.
[01:23:20.000 --> 01:23:22.000]   I'm going to ask a question.
[01:23:22.000 --> 01:23:24.000]   I'm going to ask a question.
[01:23:24.000 --> 01:23:26.000]   I'm going to ask a question.
[01:23:26.000 --> 01:23:28.000]   I'm going to ask a question.
[01:23:28.000 --> 01:23:30.000]   I'm going to ask a question.
[01:23:30.000 --> 01:23:32.000]   I'm going to ask a question.
[01:23:32.000 --> 01:23:34.000]   I'm going to ask a question.
[01:23:34.000 --> 01:23:36.000]   I'm going to ask a question.
[01:23:36.000 --> 01:23:38.000]   I'm going to ask a question.
[01:23:38.000 --> 01:23:40.000]   I'm going to ask a question.
[01:23:40.000 --> 01:23:42.000]   I'm going to ask a question.
[01:23:42.000 --> 01:23:44.000]   I'm going to ask a question.
[01:23:44.000 --> 01:23:46.000]   I'm going to ask a question.
[01:23:46.000 --> 01:23:48.000]   I'm going to ask a question.
[01:23:48.000 --> 01:23:50.000]   I'm going to ask a question.
[01:23:50.000 --> 01:23:52.000]   I'm going to ask a question.
[01:23:52.000 --> 01:23:54.000]   I'm going to ask a question.
[01:23:54.000 --> 01:23:56.000]   I'm going to ask a question.
[01:23:56.000 --> 01:23:58.000]   I'm going to ask a question.
[01:23:58.000 --> 01:24:00.000]   I'm going to ask a question.
[01:24:00.000 --> 01:24:02.000]   I'm going to ask a question.
[01:24:02.000 --> 01:24:04.000]   I'm going to ask a question.
[01:24:04.000 --> 01:24:06.000]   I'm going to ask a question.
[01:24:06.000 --> 01:24:08.000]   I'm going to ask a question.
[01:24:08.000 --> 01:24:10.000]   I'm going to ask a question.
[01:24:10.000 --> 01:24:12.000]   I'm going to ask a question.
[01:24:12.000 --> 01:24:14.000]   I'm going to ask a question.
[01:24:14.000 --> 01:24:16.000]   I'm going to ask a question.
[01:24:16.000 --> 01:24:18.000]   I'm going to ask a question.
[01:24:18.000 --> 01:24:20.000]   I'm going to ask a question.
[01:24:20.000 --> 01:24:22.000]   I'm going to ask a question.
[01:24:22.000 --> 01:24:24.000]   I'm going to ask a question.
[01:24:24.000 --> 01:24:26.000]   I'm going to ask a question.
[01:24:26.000 --> 01:24:28.000]   I'm going to ask a question.
[01:24:28.000 --> 01:24:30.000]   I'm going to ask a question.
[01:24:30.000 --> 01:24:32.000]   I'm going to ask a question.
[01:24:32.000 --> 01:24:34.000]   I'm going to ask a question.
[01:24:34.000 --> 01:24:36.000]   I'm going to ask a question.
[01:24:36.000 --> 01:24:38.000]   I'm going to ask a question.
[01:24:38.000 --> 01:24:40.000]   I'm going to ask a question.
[01:24:40.000 --> 01:24:42.000]   I'm going to ask a question.
[01:24:42.000 --> 01:24:44.000]   I'm going to ask a question.
[01:24:44.000 --> 01:24:46.000]   I'm going to ask a question.
[01:24:46.000 --> 01:24:48.000]   I'm going to ask a question.
[01:24:48.000 --> 01:24:50.000]   I'm going to ask a question.
[01:24:50.000 --> 01:24:52.000]   I'm going to ask a question.
[01:24:52.000 --> 01:24:54.000]   I'm going to ask a question.
[01:24:54.000 --> 01:24:56.000]   I'm going to ask a question.
[01:24:56.000 --> 01:24:58.000]   I'm going to ask a question.
[01:24:58.000 --> 01:25:00.000]   I'm going to ask a question.
[01:25:00.000 --> 01:25:02.000]   I'm going to ask a question.
[01:25:02.000 --> 01:25:04.000]   I'm going to ask a question.
[01:25:04.000 --> 01:25:06.000]   I'm going to ask a question.
[01:25:06.000 --> 01:25:08.000]   I'm going to ask a question.
[01:25:08.000 --> 01:25:10.000]   I'm going to ask a question.
[01:25:10.000 --> 01:25:12.000]   I'm going to ask a question.
[01:25:12.000 --> 01:25:14.000]   I'm going to ask a question.
[01:25:14.000 --> 01:25:16.000]   I'm going to ask a question.
[01:25:16.000 --> 01:25:18.000]   I'm going to ask a question.
[01:25:18.000 --> 01:25:20.000]   I'm going to ask a question.
[01:25:20.000 --> 01:25:22.000]   I'm going to ask a question.
[01:25:22.000 --> 01:25:24.000]   I'm going to ask a question.
[01:25:24.000 --> 01:25:26.000]   I'm going to ask a question.
[01:25:26.000 --> 01:25:28.000]   I'm going to ask a question.
[01:25:28.000 --> 01:25:30.000]   I'm going to ask a question.
[01:25:30.000 --> 01:25:32.000]   I'm going to ask a question.
[01:25:32.000 --> 01:25:34.000]   I'm going to ask a question.
[01:25:34.000 --> 01:25:36.000]   I'm going to ask a question.
[01:25:36.000 --> 01:25:38.000]   I'm going to ask a question.
[01:25:38.000 --> 01:25:40.000]   I'm going to ask a question.
[01:25:40.000 --> 01:25:42.000]   I'm going to ask a question.
[01:25:42.000 --> 01:25:44.000]   I'm going to ask a question.
[01:25:44.000 --> 01:25:46.000]   I'm going to ask a question.
[01:25:46.000 --> 01:25:48.000]   I'm going to ask a question.
[01:25:48.000 --> 01:25:50.000]   I'm going to ask a question.
[01:25:50.000 --> 01:25:52.000]   I'm going to ask a question.
[01:25:52.000 --> 01:25:54.000]   I'm going to ask a question.
[01:25:54.000 --> 01:25:56.000]   I'm going to ask a question.
[01:25:56.000 --> 01:25:58.000]   I'm going to ask a question.
[01:25:58.000 --> 01:26:00.000]   I'm going to ask a question.
[01:26:00.000 --> 01:26:02.000]   I'm going to ask a question.
[01:26:02.000 --> 01:26:04.000]   I'm going to ask a question.
[01:26:04.000 --> 01:26:06.000]   I'm going to ask a question.
[01:26:06.000 --> 01:26:08.000]   I'm going to ask a question.
[01:26:08.000 --> 01:26:10.000]   I'm going to ask a question.
[01:26:10.000 --> 01:26:12.000]   I'm going to ask a question.
[01:26:12.000 --> 01:26:14.000]   I'm going to ask a question.
[01:26:14.000 --> 01:26:16.000]   I'm going to ask a question.
[01:26:16.000 --> 01:26:18.000]   I'm going to ask a question.
[01:26:18.000 --> 01:26:20.000]   I'm going to ask a question.
[01:26:20.000 --> 01:26:22.000]   I'm going to ask a question.
[01:26:22.000 --> 01:26:24.000]   I'm going to ask a question.
[01:26:24.000 --> 01:26:26.000]   I'm going to ask a question.
[01:26:26.000 --> 01:26:28.000]   I'm going to ask a question.
[01:26:28.000 --> 01:26:30.000]   I'm going to ask a question.
[01:26:30.000 --> 01:26:32.000]   I'm going to ask a question.
[01:26:32.000 --> 01:26:34.000]   I'm going to ask a question.
[01:26:34.000 --> 01:26:36.000]   I'm going to ask a question.
[01:26:36.000 --> 01:26:38.000]   I'm going to ask a question.
[01:26:38.000 --> 01:26:40.000]   I'm going to ask a question.
[01:26:40.000 --> 01:26:42.000]   I'm going to ask a question.
[01:26:42.000 --> 01:26:44.000]   I'm going to ask a question.
[01:26:44.000 --> 01:26:46.000]   I'm going to ask a question.
[01:26:46.000 --> 01:26:48.000]   I'm going to ask a question.
[01:26:48.000 --> 01:26:50.000]   I'm going to ask a question.
[01:26:50.000 --> 01:26:52.000]   I'm going to ask a question.
[01:26:52.000 --> 01:26:54.000]   I'm going to ask a question.
[01:26:54.000 --> 01:26:56.000]   I'm going to ask a question.
[01:26:56.000 --> 01:26:58.000]   I'm going to ask a question.
[01:26:58.000 --> 01:27:00.000]   I'm going to ask a question.
[01:27:00.000 --> 01:27:02.000]   I'm going to ask a question.
[01:27:02.000 --> 01:27:04.000]   I'm going to ask a question.
[01:27:04.000 --> 01:27:06.000]   I'm going to ask a question.
[01:27:06.000 --> 01:27:08.000]   I'm going to ask a question.
[01:27:08.000 --> 01:27:10.000]   I'm going to ask a question.
[01:27:10.000 --> 01:27:12.000]   I'm going to ask a question.
[01:27:12.000 --> 01:27:14.000]   I'm going to ask a question.
[01:27:14.000 --> 01:27:16.000]   I'm going to ask a question.
[01:27:16.000 --> 01:27:18.000]   I'm going to ask a question.
[01:27:18.000 --> 01:27:20.000]   I'm going to ask a question.
[01:27:20.000 --> 01:27:22.000]   I'm going to ask a question.
[01:27:22.000 --> 01:27:24.000]   I'm going to ask a question.
[01:27:24.000 --> 01:27:26.000]   I'm going to ask a question.
[01:27:26.000 --> 01:27:28.000]   I'm going to ask a question.
[01:27:28.000 --> 01:27:30.000]   I'm going to ask a question.
[01:27:30.000 --> 01:27:32.000]   I'm going to ask a question.
[01:27:32.000 --> 01:27:34.000]   I'm going to ask a question.
[01:27:34.000 --> 01:27:36.000]   I'm going to ask a question.
[01:27:36.000 --> 01:27:38.000]   I'm going to ask a question.
[01:27:38.000 --> 01:27:40.000]   I'm going to ask a question.
[01:27:40.000 --> 01:27:42.000]   I'm going to ask a question.
[01:27:42.000 --> 01:27:44.000]   I'm going to ask a question.
[01:27:44.000 --> 01:27:46.000]   I'm going to ask a question.
[01:27:46.000 --> 01:27:48.000]   I'm going to ask a question.
[01:27:48.000 --> 01:27:50.000]   I'm going to ask a question.
[01:27:50.000 --> 01:27:52.000]   I'm going to ask a question.
[01:27:52.000 --> 01:27:54.000]   I'm going to ask a question.
[01:27:54.000 --> 01:27:56.000]   I'm going to ask a question.
[01:27:56.000 --> 01:27:58.000]   I'm going to ask a question.
[01:27:58.000 --> 01:28:00.000]   I'm going to ask a question.
[01:28:00.000 --> 01:28:02.000]   I'm going to ask a question.
[01:28:02.000 --> 01:28:04.000]   I'm going to ask a question.
[01:28:04.000 --> 01:28:06.000]   I'm going to ask a question.
[01:28:06.000 --> 01:28:08.000]   I'm going to ask a question.
[01:28:08.000 --> 01:28:10.000]   I'm going to ask a question.
[01:28:10.000 --> 01:28:12.000]   I'm going to ask a question.
[01:28:12.000 --> 01:28:14.000]   I'm going to ask a question.
[01:28:14.000 --> 01:28:16.000]   I'm going to ask a question.
[01:28:16.000 --> 01:28:18.000]   I'm going to ask a question.
[01:28:18.000 --> 01:28:20.000]   I'm going to ask a question.
[01:28:20.000 --> 01:28:22.000]   I'm going to ask a question.
[01:28:22.000 --> 01:28:24.000]   I'm going to ask a question.
[01:28:24.000 --> 01:28:26.000]   I'm going to ask a question.
[01:28:26.000 --> 01:28:28.000]   I'm going to ask a question.
[01:28:28.000 --> 01:28:30.000]   I'm going to ask a question.
[01:28:30.000 --> 01:28:32.000]   I'm going to ask a question.
[01:28:32.000 --> 01:28:34.000]   I'm going to ask a question.
[01:28:34.000 --> 01:28:36.000]   I'm going to ask a question.
[01:28:36.000 --> 01:28:38.000]   I'm going to ask a question.
[01:28:38.000 --> 01:28:40.000]   I'm going to ask a question.
[01:28:40.000 --> 01:28:42.000]   I'm going to ask a question.
[01:28:42.000 --> 01:28:44.000]   I'm going to ask a question.
[01:28:44.000 --> 01:28:46.000]   I'm going to ask a question.
[01:28:46.000 --> 01:28:48.000]   I'm going to ask a question.
[01:28:48.000 --> 01:28:50.000]   I'm going to ask a question.
[01:28:50.000 --> 01:28:52.000]   I'm going to ask a question.
[01:28:52.000 --> 01:28:54.000]   I'm going to ask a question.
[01:28:54.000 --> 01:28:56.000]   I'm going to ask a question.
[01:28:56.000 --> 01:28:58.000]   I'm going to ask a question.
[01:28:58.000 --> 01:29:00.000]   I'm going to ask a question.
[01:29:00.000 --> 01:29:02.000]   I'm going to ask a question.
[01:29:02.000 --> 01:29:04.000]   I'm going to ask a question.
[01:29:04.000 --> 01:29:06.000]   I'm going to ask a question.
[01:29:06.000 --> 01:29:08.000]   I'm going to ask a question.
[01:29:08.000 --> 01:29:10.000]   I'm going to ask a question.
[01:29:10.000 --> 01:29:12.000]   I'm going to ask a question.
[01:29:12.000 --> 01:29:14.000]   I'm going to ask a question.
[01:29:14.000 --> 01:29:16.000]   I'm going to ask a question.
[01:29:16.000 --> 01:29:18.000]   I'm going to ask a question.
[01:29:18.000 --> 01:29:20.000]   I'm going to ask a question.
[01:29:20.000 --> 01:29:22.000]   I'm going to ask a question.
[01:29:22.000 --> 01:29:24.000]   I'm going to ask a question.
[01:29:24.000 --> 01:29:26.000]   I'm going to ask a question.
[01:29:26.000 --> 01:29:28.000]   I'm going to ask a question.
[01:29:28.000 --> 01:29:30.000]   I'm going to ask a question.
[01:29:30.000 --> 01:29:32.000]   I'm going to ask a question.
[01:29:32.000 --> 01:29:34.000]   I'm going to ask a question.
[01:29:34.000 --> 01:29:36.000]   I'm going to ask a question.
[01:29:36.000 --> 01:29:38.000]   I'm going to ask a question.
[01:29:38.000 --> 01:29:40.000]   I'm going to ask a question.
[01:29:40.000 --> 01:29:42.000]   I'm going to ask a question.
[01:29:42.000 --> 01:29:44.000]   I'm going to ask a question.
[01:29:44.000 --> 01:29:46.000]   I'm going to ask a question.
[01:29:46.000 --> 01:29:48.000]   I'm going to ask a question.
[01:29:48.000 --> 01:29:50.000]   I'm going to ask a question.
[01:29:50.000 --> 01:29:52.000]   I'm going to ask a question.
[01:29:52.000 --> 01:29:54.000]   I'm going to ask a question.
[01:29:54.000 --> 01:29:56.000]   I'm going to ask a question.
[01:29:56.000 --> 01:29:58.000]   I'm going to ask a question.
[01:29:58.000 --> 01:30:00.000]   I'm going to ask a question.
[01:30:00.000 --> 01:30:02.000]   I'm going to ask a question.
[01:30:02.000 --> 01:30:04.000]   I'm going to ask a question.
[01:30:04.000 --> 01:30:06.000]   I'm going to ask a question.
[01:30:06.000 --> 01:30:08.000]   I'm going to ask a question.
[01:30:08.000 --> 01:30:10.000]   I'm going to ask a question.
[01:30:10.000 --> 01:30:12.000]   I'm going to ask a question.
[01:30:12.000 --> 01:30:14.000]   I'm going to ask a question.
[01:30:14.000 --> 01:30:16.000]   I'm going to ask a question.
[01:30:16.000 --> 01:30:18.000]   I'm going to ask a question.
[01:30:18.000 --> 01:30:20.000]   I'm going to ask a question.
[01:30:20.000 --> 01:30:22.000]   I'm going to ask a question.
[01:30:22.000 --> 01:30:24.000]   I'm going to ask a question.
[01:30:24.000 --> 01:30:26.000]   I'm going to ask a question.
[01:30:26.000 --> 01:30:28.000]   I'm going to ask a question.
[01:30:28.000 --> 01:30:30.000]   I'm going to ask a question.
[01:30:30.000 --> 01:30:32.000]   I'm going to ask a question.
[01:30:32.000 --> 01:30:34.000]   I'm going to ask a question.
[01:30:34.000 --> 01:30:36.000]   I'm going to ask a question.
[01:30:36.000 --> 01:30:38.000]   I'm going to ask a question.
[01:30:38.000 --> 01:30:40.000]   I'm going to ask a question.
[01:30:40.000 --> 01:30:42.000]   I'm going to ask a question.
[01:30:42.000 --> 01:30:44.000]   I'm going to ask a question.
[01:30:44.000 --> 01:30:46.000]   I'm going to ask a question.
[01:30:46.000 --> 01:30:48.000]   I'm going to ask a question.
[01:30:48.000 --> 01:30:50.000]   I'm going to ask a question.
[01:30:50.000 --> 01:30:52.000]   I'm going to ask a question.
[01:30:52.000 --> 01:30:54.000]   I'm going to ask a question.
[01:30:54.000 --> 01:30:56.000]   I'm going to ask a question.
[01:30:56.000 --> 01:30:58.000]   I'm going to ask a question.
[01:30:58.000 --> 01:31:00.000]   I'm going to ask a question.
[01:31:00.000 --> 01:31:02.000]   I'm going to ask a question.
[01:31:02.000 --> 01:31:04.000]   I'm going to ask a question.
[01:31:04.000 --> 01:31:06.000]   I'm going to ask a question.
[01:31:06.000 --> 01:31:08.000]   I'm going to ask a question.
[01:31:08.000 --> 01:31:10.000]   I'm going to ask a question.
[01:31:10.000 --> 01:31:12.000]   I'm going to ask a question.
[01:31:12.000 --> 01:31:14.000]   I'm going to ask a question.
[01:31:14.000 --> 01:31:16.000]   I'm going to ask a question.
[01:31:16.000 --> 01:31:18.000]   I'm going to ask a question.
[01:31:18.000 --> 01:31:20.000]   I'm going to ask a question.
[01:31:20.000 --> 01:31:22.000]   I'm going to ask a question.
[01:31:22.000 --> 01:31:24.000]   I'm going to ask a question.
[01:31:24.000 --> 01:31:26.000]   I'm going to ask a question.
[01:31:26.000 --> 01:31:28.000]   I'm going to ask a question.
[01:31:28.000 --> 01:31:30.000]   I'm going to ask a question.
[01:31:30.000 --> 01:31:32.000]   I'm going to ask a question.
[01:31:32.000 --> 01:31:34.000]   I'm going to ask a question.
[01:31:34.000 --> 01:31:36.000]   I'm going to ask a question.
[01:31:36.000 --> 01:31:38.000]   I'm going to ask a question.
[01:31:38.000 --> 01:31:40.000]   I'm going to ask a question.
[01:31:40.000 --> 01:31:42.000]   I'm going to ask a question.
[01:31:42.000 --> 01:31:44.000]   I'm going to ask a question.
[01:31:44.000 --> 01:31:46.000]   I'm going to ask a question.
[01:31:46.000 --> 01:31:48.000]   I'm going to ask a question.
[01:31:48.000 --> 01:31:50.000]   I'm going to ask a question.
[01:31:50.000 --> 01:31:52.000]   I'm going to ask a question.
[01:31:52.000 --> 01:31:54.000]   I'm going to ask a question.
[01:31:54.000 --> 01:31:56.000]   I'm going to ask a question.
[01:31:56.000 --> 01:31:58.000]   I'm going to ask a question.
[01:31:58.000 --> 01:32:00.000]   I'm going to ask a question.
[01:32:00.000 --> 01:32:02.000]   I'm going to ask a question.
[01:32:02.000 --> 01:32:04.000]   I'm going to ask a question.
[01:32:04.000 --> 01:32:06.000]   I'm going to ask a question.
[01:32:06.000 --> 01:32:08.000]   I'm going to ask a question.
[01:32:08.000 --> 01:32:10.000]   I'm going to ask a question.
[01:32:10.000 --> 01:32:12.000]   I'm going to ask a question.
[01:32:12.000 --> 01:32:14.000]   I'm going to ask a question.
[01:32:14.000 --> 01:32:16.000]   I'm going to ask a question.
[01:32:16.000 --> 01:32:18.000]   I'm going to ask a question.
[01:32:18.000 --> 01:32:20.000]   I'm going to ask a question.
[01:32:20.000 --> 01:32:22.000]   I'm going to ask a question.
[01:32:22.000 --> 01:32:24.000]   I'm going to ask a question.
[01:32:24.000 --> 01:32:26.000]   I'm going to ask a question.
[01:32:26.000 --> 01:32:28.000]   I'm going to ask a question.
[01:32:28.000 --> 01:32:30.000]   I'm going to ask a question.
[01:32:30.000 --> 01:32:32.000]   I'm going to ask a question.
[01:32:32.000 --> 01:32:34.000]   I'm going to ask a question.
[01:32:34.000 --> 01:32:36.000]   I'm going to ask a question.
[01:32:36.000 --> 01:32:38.000]   I'm going to ask a question.
[01:32:38.000 --> 01:32:40.000]   I'm going to ask a question.
[01:32:40.000 --> 01:32:42.000]   I'm going to ask a question.
[01:32:42.000 --> 01:32:44.000]   I'm going to ask a question.
[01:32:44.000 --> 01:32:46.000]   I'm going to ask a question.
[01:32:46.000 --> 01:32:48.000]   I'm going to ask a question.
[01:32:48.000 --> 01:32:50.000]   I'm going to ask a question.
[01:32:50.000 --> 01:32:52.000]   I'm going to ask a question.
[01:32:52.000 --> 01:32:54.000]   I'm going to ask a question.
[01:32:54.000 --> 01:32:56.000]   I'm going to ask a question.
[01:32:56.000 --> 01:32:58.000]   I'm going to ask a question.
[01:32:58.000 --> 01:33:00.000]   I'm going to ask a question.
[01:33:00.000 --> 01:33:02.000]   I'm going to ask a question.
[01:33:02.000 --> 01:33:04.000]   I'm going to ask a question.
[01:33:04.000 --> 01:33:06.000]   I'm going to ask a question.
[01:33:06.000 --> 01:33:08.000]   I'm going to ask a question.
[01:33:08.000 --> 01:33:10.000]   I'm going to ask a question.
[01:33:10.000 --> 01:33:12.000]   I'm going to ask a question.
[01:33:12.000 --> 01:33:14.000]   I'm going to ask a question.
[01:33:14.000 --> 01:33:16.000]   I'm going to ask a question.
[01:33:16.000 --> 01:33:18.000]   I'm going to ask a question.
[01:33:18.000 --> 01:33:20.000]   I'm going to ask a question.
[01:33:20.000 --> 01:33:22.000]   I'm going to ask a question.
[01:33:22.000 --> 01:33:24.000]   I'm going to ask a question.
[01:33:24.000 --> 01:33:26.000]   I'm going to ask a question.
[01:33:26.000 --> 01:33:28.000]   I'm going to ask a question.
[01:33:28.000 --> 01:33:30.000]   I'm going to ask a question.
[01:33:30.000 --> 01:33:32.000]   I'm going to ask a question.
[01:33:32.000 --> 01:33:34.000]   I'm going to ask a question.
[01:33:34.000 --> 01:33:36.000]   I'm going to ask a question.
[01:33:36.000 --> 01:33:38.000]   I'm going to ask a question.
[01:33:38.000 --> 01:33:40.000]   I'm going to ask a question.
[01:33:40.000 --> 01:33:42.000]   I'm going to ask a question.
[01:33:42.000 --> 01:33:44.000]   I'm going to ask a question.
[01:33:44.000 --> 01:33:46.000]   I'm going to ask a question.
[01:33:46.000 --> 01:33:48.000]   I'm going to ask a question.
[01:33:48.000 --> 01:33:50.000]   I'm going to ask a question.
[01:33:50.000 --> 01:33:52.000]   I'm going to ask a question.
[01:33:52.000 --> 01:33:54.000]   I'm going to ask a question.
[01:33:54.000 --> 01:33:56.000]   I'm going to ask a question.
[01:33:56.000 --> 01:33:58.000]   I'm going to ask a question.
[01:33:58.000 --> 01:34:00.000]   I'm going to ask a question.
[01:34:00.000 --> 01:34:02.000]   I'm going to ask a question.
[01:34:02.000 --> 01:34:04.000]   I'm going to ask a question.
[01:34:04.000 --> 01:34:06.000]   I'm going to ask a question.
[01:34:06.000 --> 01:34:08.000]   I'm going to ask a question.
[01:34:08.000 --> 01:34:10.000]   I'm going to ask a question.
[01:34:10.000 --> 01:34:12.000]   I'm going to ask a question.
[01:34:12.000 --> 01:34:14.000]   I'm going to ask a question.
[01:34:14.000 --> 01:34:16.000]   I'm going to ask a question.
[01:34:16.000 --> 01:34:18.000]   I'm going to ask a question.
[01:34:18.000 --> 01:34:20.000]   I'm going to ask a question.
[01:34:20.000 --> 01:34:22.000]   I'm going to ask a question.
[01:34:22.000 --> 01:34:24.000]   I'm going to ask a question.
[01:34:24.000 --> 01:34:26.000]   I'm going to ask a question.
[01:34:26.000 --> 01:34:28.000]   I'm going to ask a question.
[01:34:28.000 --> 01:34:30.000]   I'm going to ask a question.
[01:34:30.000 --> 01:34:32.000]   I'm going to ask a question.
[01:34:32.000 --> 01:34:34.000]   I'm going to ask a question.
[01:34:34.000 --> 01:34:36.000]   I'm going to ask a question.
[01:34:36.000 --> 01:34:38.000]   I'm going to ask a question.
[01:34:38.000 --> 01:34:40.000]   I'm going to ask a question.
[01:34:40.000 --> 01:34:42.000]   I'm going to ask a question.
[01:34:42.000 --> 01:34:44.000]   I'm going to ask a question.
[01:34:44.000 --> 01:34:46.000]   I'm going to ask a question.
[01:34:46.000 --> 01:34:48.000]   I'm going to ask a question.
[01:34:48.000 --> 01:34:50.000]   I'm going to ask a question.
[01:34:50.000 --> 01:34:52.000]   I'm going to ask a question.
[01:34:52.000 --> 01:34:54.000]   I'm going to ask a question.
[01:34:54.000 --> 01:34:56.000]   I'm going to ask a question.
[01:34:56.000 --> 01:34:58.000]   I'm going to ask a question.
[01:34:58.000 --> 01:35:00.000]   I'm going to ask a question.
[01:35:00.000 --> 01:35:02.000]   I'm going to ask a question.
[01:35:02.000 --> 01:35:04.000]   I'm going to ask a question.
[01:35:04.000 --> 01:35:06.000]   I'm going to ask a question.
[01:35:06.000 --> 01:35:08.000]   I'm going to ask a question.
[01:35:08.000 --> 01:35:10.000]   I'm going to ask a question.
[01:35:10.000 --> 01:35:12.000]   I'm going to ask a question.
[01:35:12.000 --> 01:35:14.000]   I'm going to ask a question.
[01:35:14.000 --> 01:35:16.000]   I'm going to ask a question.
[01:35:16.000 --> 01:35:18.000]   I'm going to ask a question.
[01:35:18.000 --> 01:35:20.000]   I'm going to ask a question.
[01:35:20.000 --> 01:35:22.000]   I'm going to ask a question.
[01:35:22.000 --> 01:35:24.000]   I'm going to ask a question.
[01:35:24.000 --> 01:35:26.000]   I'm going to ask a question.
[01:35:26.000 --> 01:35:28.000]   I'm going to ask a question.
[01:35:28.000 --> 01:35:30.000]   I'm going to ask a question.
[01:35:30.000 --> 01:35:32.000]   I'm going to ask a question.
[01:35:32.000 --> 01:35:34.000]   I'm going to ask a question.
[01:35:34.000 --> 01:35:36.000]   I'm going to ask a question.
[01:35:36.000 --> 01:35:38.000]   I'm going to ask a question.
[01:35:38.000 --> 01:35:40.000]   I'm going to ask a question.
[01:35:40.000 --> 01:35:42.000]   I'm going to ask a question.
[01:35:42.000 --> 01:35:44.000]   I'm going to ask a question.
[01:35:44.000 --> 01:35:46.000]   I'm going to ask a question.
[01:35:46.000 --> 01:35:48.000]   I'm going to ask a question.
[01:35:48.000 --> 01:35:50.000]   I'm going to ask a question.
[01:35:50.000 --> 01:35:52.000]   I'm going to ask a question.
[01:35:52.000 --> 01:35:54.000]   I'm going to ask a question.
[01:35:54.000 --> 01:35:56.000]   I'm going to ask a question.
[01:35:56.000 --> 01:35:58.000]   I'm going to ask a question.
[01:35:58.000 --> 01:36:00.000]   I'm going to ask a question.
[01:36:00.000 --> 01:36:02.000]   I'm going to ask a question.
[01:36:02.000 --> 01:36:04.000]   I'm going to ask a question.
[01:36:04.000 --> 01:36:06.000]   I'm going to ask a question.
[01:36:06.000 --> 01:36:08.000]   I'm going to ask a question.
[01:36:08.000 --> 01:36:10.000]   I'm going to ask a question.
[01:36:10.000 --> 01:36:12.000]   I'm going to ask a question.
[01:36:12.000 --> 01:36:14.000]   I'm going to ask a question.
[01:36:14.000 --> 01:36:16.000]   I'm going to ask a question.
[01:36:16.000 --> 01:36:18.000]   I'm going to ask a question.
[01:36:18.000 --> 01:36:20.000]   I'm going to ask a question.
[01:36:20.000 --> 01:36:22.000]   I'm going to ask a question.
[01:36:22.000 --> 01:36:24.000]   I'm going to ask a question.
[01:36:24.000 --> 01:36:26.000]   I'm going to ask a question.
[01:36:26.000 --> 01:36:28.000]   I'm going to ask a question.
[01:36:28.000 --> 01:36:30.000]   I'm going to ask a question.
[01:36:30.000 --> 01:36:32.000]   I'm going to ask a question.
[01:36:32.000 --> 01:36:34.000]   I'm going to ask a question.
[01:36:34.000 --> 01:36:36.000]   I'm going to ask a question.
[01:36:36.000 --> 01:36:38.000]   I'm going to ask a question.
[01:36:38.000 --> 01:36:40.000]   I'm going to ask a question.
[01:36:40.000 --> 01:36:42.000]   I'm going to ask a question.
[01:36:42.000 --> 01:36:44.000]   I'm going to ask a question.
[01:36:44.000 --> 01:36:46.000]   I'm going to ask a question.
[01:36:46.000 --> 01:36:48.000]   I'm going to ask a question.
[01:36:48.000 --> 01:36:50.000]   I'm going to ask a question.
[01:36:50.000 --> 01:36:52.000]   I'm going to ask a question.
[01:36:52.000 --> 01:36:54.000]   I'm going to ask a question.
[01:36:54.000 --> 01:36:56.000]   I'm going to ask a question.
[01:36:56.000 --> 01:36:58.000]   I'm going to ask a question.
[01:36:58.000 --> 01:37:00.000]   I'm going to ask a question.
[01:37:00.000 --> 01:37:02.000]   I'm going to ask a question.
[01:37:02.000 --> 01:37:04.000]   I'm going to ask a question.
[01:37:04.000 --> 01:37:06.000]   I'm going to ask a question.
[01:37:06.000 --> 01:37:08.000]   I'm going to ask a question.
[01:37:08.000 --> 01:37:10.000]   I'm going to ask a question.
[01:37:10.000 --> 01:37:12.000]   I'm going to ask a question.
[01:37:12.000 --> 01:37:14.000]   I'm going to ask a question.
[01:37:14.000 --> 01:37:16.000]   I'm going to ask a question.
[01:37:16.000 --> 01:37:18.000]   I'm going to ask a question.
[01:37:18.000 --> 01:37:20.000]   I'm going to ask a question.
[01:37:20.000 --> 01:37:22.000]   I'm going to ask a question.
[01:37:22.000 --> 01:37:24.000]   I'm going to ask a question.
[01:37:24.000 --> 01:37:26.000]   I'm going to ask a question.
[01:37:26.000 --> 01:37:28.000]   I'm going to ask a question.
[01:37:28.000 --> 01:37:30.000]   I'm going to ask a question.
[01:37:30.000 --> 01:37:32.000]   I'm going to ask a question.
[01:37:32.000 --> 01:37:34.000]   I'm going to ask a question.
[01:37:34.000 --> 01:37:36.000]   I'm going to ask a question.
[01:37:36.000 --> 01:37:38.000]   I'm going to ask a question.
[01:37:38.000 --> 01:37:40.000]   I'm going to ask a question.
[01:37:40.000 --> 01:37:42.000]   I'm going to ask a question.
[01:37:42.000 --> 01:37:44.000]   I'm going to ask a question.
[01:37:44.000 --> 01:37:46.000]   I'm going to ask a question.
[01:37:46.000 --> 01:37:48.000]   I'm going to ask a question.
[01:37:48.000 --> 01:37:50.000]   I'm going to ask a question.
[01:37:50.000 --> 01:37:52.000]   I'm going to ask a question.
[01:37:52.000 --> 01:37:54.000]   I'm going to ask a question.
[01:37:54.000 --> 01:37:56.000]   I'm going to ask a question.
[01:37:56.000 --> 01:37:58.000]   I'm going to ask a question.
[01:37:58.000 --> 01:38:00.000]   I'm going to ask a question.
[01:38:00.000 --> 01:38:02.000]   I'm going to ask a question.
[01:38:02.000 --> 01:38:04.000]   I'm going to ask a question.
[01:38:04.000 --> 01:38:06.000]   I'm going to ask a question.
[01:38:06.000 --> 01:38:08.000]   I'm going to ask a question.
[01:38:08.000 --> 01:38:10.000]   I'm going to ask a question.
[01:38:10.000 --> 01:38:12.000]   I'm going to ask a question.
[01:38:12.000 --> 01:38:14.000]   I'm going to ask a question.
[01:38:14.000 --> 01:38:16.000]   I'm going to ask a question.
[01:38:16.000 --> 01:38:18.000]   I'm going to ask a question.
[01:38:18.000 --> 01:38:20.000]   I'm going to ask a question.
[01:38:20.000 --> 01:38:22.000]   I'm going to ask a question.
[01:38:22.000 --> 01:38:24.000]   I'm going to ask a question.
[01:38:24.000 --> 01:38:26.000]   I'm going to ask a question.
[01:38:26.000 --> 01:38:28.000]   I'm going to ask a question.
[01:38:28.000 --> 01:38:30.000]   I'm going to ask a question.
[01:38:30.000 --> 01:38:32.000]   I'm going to ask a question.
[01:38:32.000 --> 01:38:34.000]   I'm going to ask a question.
[01:38:34.000 --> 01:38:36.000]   I'm going to ask a question.
[01:38:36.000 --> 01:38:38.000]   I'm going to ask a question.
[01:38:38.000 --> 01:38:40.000]   I'm going to ask a question.
[01:38:40.000 --> 01:38:42.000]   I'm going to ask a question.
[01:38:42.000 --> 01:38:44.000]   I'm going to ask a question.
[01:38:44.000 --> 01:38:46.000]   I'm going to ask a question.
[01:38:46.000 --> 01:38:48.000]   I'm going to ask a question.
[01:38:48.000 --> 01:38:50.000]   I'm going to ask a question.
[01:38:50.000 --> 01:38:52.000]   I'm going to ask a question.
[01:38:52.000 --> 01:38:54.000]   I'm going to ask a question.
[01:38:54.000 --> 01:38:56.000]   I'm going to ask a question.
[01:38:56.000 --> 01:38:58.000]   I'm going to ask a question.
[01:38:58.000 --> 01:39:00.000]   I'm going to ask a question.
[01:39:00.000 --> 01:39:02.000]   I'm going to ask a question.
[01:39:02.000 --> 01:39:04.000]   I'm going to ask a question.
[01:39:04.000 --> 01:39:06.000]   I'm going to ask a question.
[01:39:06.000 --> 01:39:08.000]   I'm going to ask a question.
[01:39:08.000 --> 01:39:10.000]   I'm going to ask a question.
[01:39:10.000 --> 01:39:12.000]   I'm going to ask a question.
[01:39:12.000 --> 01:39:14.000]   I'm going to ask a question.
[01:39:14.000 --> 01:39:16.000]   I'm going to ask a question.
[01:39:16.000 --> 01:39:18.000]   I'm going to ask a question.
[01:39:18.000 --> 01:39:20.000]   I'm going to ask a question.
[01:39:20.000 --> 01:39:22.000]   I'm going to ask a question.
[01:39:22.000 --> 01:39:24.000]   I'm going to ask a question.
[01:39:24.000 --> 01:39:26.000]   I'm going to ask a question.
[01:39:26.000 --> 01:39:28.000]   I'm going to ask a question.
[01:39:28.000 --> 01:39:30.000]   I'm going to ask a question.
[01:39:30.000 --> 01:39:32.000]   I'm going to ask a question.
[01:39:32.000 --> 01:39:34.000]   I'm going to ask a question.
[01:39:34.000 --> 01:39:36.000]   I'm going to ask a question.
[01:39:36.000 --> 01:39:38.000]   I'm going to ask a question.
[01:39:38.000 --> 01:39:40.000]   I'm going to ask a question.
[01:39:40.000 --> 01:39:42.000]   I'm going to ask a question.
[01:39:42.000 --> 01:39:44.000]   I'm going to ask a question.
[01:39:44.000 --> 01:39:46.000]   I'm going to ask a question.
[01:39:46.000 --> 01:39:48.000]   I'm going to ask a question.
[01:39:48.000 --> 01:39:50.000]   I'm going to ask a question.
[01:39:50.000 --> 01:39:52.000]   I'm going to ask a question.
[01:39:52.000 --> 01:39:54.000]   I'm going to ask a question.
[01:39:54.000 --> 01:39:56.000]   I'm going to ask a question.
[01:39:56.000 --> 01:39:58.000]   I'm going to ask a question.
[01:39:58.000 --> 01:40:00.000]   I'm going to ask a question.
[01:40:00.000 --> 01:40:02.000]   I'm going to ask a question.
[01:40:02.000 --> 01:40:04.000]   I'm going to ask a question.
[01:40:04.000 --> 01:40:06.000]   I'm going to ask a question.
[01:40:06.000 --> 01:40:08.000]   I'm going to ask a question.
[01:40:08.000 --> 01:40:10.000]   I'm going to ask a question.
[01:40:10.000 --> 01:40:12.000]   I'm going to ask a question.
[01:40:12.000 --> 01:40:14.000]   I'm going to ask a question.
[01:40:14.000 --> 01:40:16.000]   I'm going to ask a question.
[01:40:16.000 --> 01:40:18.000]   I'm going to ask a question.
[01:40:18.000 --> 01:40:20.000]   I'm going to ask a question.
[01:40:20.000 --> 01:40:22.000]   I'm going to ask a question.
[01:40:22.000 --> 01:40:24.000]   I'm going to ask a question.
[01:40:24.000 --> 01:40:26.000]   I'm going to ask a question.
[01:40:26.000 --> 01:40:28.000]   I'm going to ask a question.
[01:40:28.000 --> 01:40:30.000]   I'm going to ask a question.
[01:40:30.000 --> 01:40:32.000]   I'm going to ask a question.
[01:40:32.000 --> 01:40:34.000]   I'm going to ask a question.
[01:40:34.000 --> 01:40:36.000]   I'm going to ask a question.
[01:40:36.000 --> 01:40:38.000]   I'm going to ask a question.
[01:40:38.000 --> 01:40:40.000]   I'm going to ask a question.
[01:40:40.000 --> 01:40:42.000]   I'm going to ask a question.
[01:40:42.000 --> 01:40:44.000]   I'm going to ask a question.
[01:40:44.000 --> 01:40:46.000]   I'm going to ask a question.
[01:40:46.000 --> 01:40:48.000]   I'm going to ask a question.
[01:40:48.000 --> 01:40:50.000]   I'm going to ask a question.
[01:40:50.000 --> 01:40:52.000]   I'm going to ask a question.
[01:40:52.000 --> 01:40:54.000]   I'm going to ask a question.
[01:40:54.000 --> 01:40:56.000]   I'm going to ask a question.
[01:40:56.000 --> 01:40:58.000]   I'm going to ask a question.
[01:40:58.000 --> 01:41:00.000]   I'm going to ask a question.
[01:41:00.000 --> 01:41:02.000]   I'm going to ask a question.
[01:41:02.000 --> 01:41:04.000]   I'm going to ask a question.
[01:41:04.000 --> 01:41:06.000]   I'm going to ask a question.
[01:41:06.000 --> 01:41:08.000]   I'm going to ask a question.
[01:41:08.000 --> 01:41:10.000]   I'm going to ask a question.
[01:41:10.000 --> 01:41:12.000]   I'm going to ask a question.
[01:41:12.000 --> 01:41:14.000]   I'm going to ask a question.
[01:41:14.000 --> 01:41:16.000]   I'm going to ask a question.
[01:41:16.000 --> 01:41:18.000]   I'm going to ask a question.
[01:41:18.000 --> 01:41:20.000]   I'm going to ask a question.
[01:41:20.000 --> 01:41:22.000]   I'm going to ask a question.
[01:41:22.000 --> 01:41:24.000]   I'm going to ask a question.
[01:41:24.000 --> 01:41:26.000]   I'm going to ask a question.
[01:41:26.000 --> 01:41:28.000]   I'm going to ask a question.
[01:41:28.000 --> 01:41:30.000]   I'm going to ask a question.
[01:41:30.000 --> 01:41:32.000]   I'm going to ask a question.
[01:41:32.000 --> 01:41:34.000]   I'm going to ask a question.
[01:41:34.000 --> 01:41:36.000]   I'm going to ask a question.
[01:41:36.000 --> 01:41:38.000]   I'm going to ask a question.
[01:41:38.000 --> 01:41:40.000]   I'm going to ask a question.
[01:41:40.000 --> 01:41:42.000]   I'm going to ask a question.
[01:41:42.000 --> 01:41:44.000]   I'm going to ask a question.
[01:41:44.000 --> 01:41:46.000]   I'm going to ask a question.
[01:41:46.000 --> 01:41:48.000]   I'm going to ask a question.
[01:41:48.000 --> 01:41:50.000]   I'm going to ask a question.
[01:41:50.000 --> 01:41:52.000]   I'm going to ask a question.
[01:41:52.000 --> 01:41:54.000]   I'm going to ask a question.
[01:41:54.000 --> 01:41:56.000]   I'm going to ask a question.
[01:41:56.000 --> 01:41:58.000]   I'm going to ask a question.
[01:41:58.000 --> 01:42:00.000]   I'm going to ask a question.
[01:42:00.000 --> 01:42:02.000]   I'm going to ask a question.
[01:42:02.000 --> 01:42:04.000]   I'm going to ask a question.
[01:42:04.000 --> 01:42:06.000]   I'm going to ask a question.
[01:42:06.000 --> 01:42:08.000]   I'm going to ask a question.
[01:42:08.000 --> 01:42:10.000]   I'm going to ask a question.
[01:42:10.000 --> 01:42:12.000]   I'm going to ask a question.
[01:42:12.000 --> 01:42:14.000]   I'm going to ask a question.
[01:42:14.000 --> 01:42:16.000]   I'm going to ask a question.
[01:42:16.000 --> 01:42:18.000]   I'm going to ask a question.
[01:42:18.000 --> 01:42:20.000]   I'm going to ask a question.
[01:42:20.000 --> 01:42:22.000]   I'm going to ask a question.
[01:42:22.000 --> 01:42:24.000]   I'm going to ask a question.
[01:42:24.000 --> 01:42:26.000]   I'm going to ask a question.
[01:42:26.000 --> 01:42:28.000]   I'm going to ask a question.
[01:42:28.000 --> 01:42:30.000]   I'm going to ask a question.
[01:42:30.000 --> 01:42:32.000]   I'm going to ask a question.
[01:42:32.000 --> 01:42:34.000]   I'm going to ask a question.
[01:42:34.000 --> 01:42:36.000]   I'm going to ask a question.
[01:42:36.000 --> 01:42:38.000]   I'm going to ask a question.
[01:42:38.000 --> 01:42:40.000]   I'm going to ask a question.
[01:42:40.000 --> 01:42:42.000]   I'm going to ask a question.
[01:42:42.000 --> 01:42:44.000]   I'm going to ask a question.
[01:42:44.000 --> 01:42:46.000]   I'm going to ask a question.
[01:42:46.000 --> 01:42:48.000]   I'm going to ask a question.
[01:42:48.000 --> 01:42:50.000]   I'm going to ask a question.
[01:42:50.000 --> 01:42:52.000]   I'm going to ask a question.
[01:42:52.000 --> 01:42:54.000]   I'm going to ask a question.
[01:42:54.000 --> 01:42:56.000]   I'm going to ask a question.
[01:42:56.000 --> 01:42:58.000]   I'm going to ask a question.
[01:42:58.000 --> 01:43:00.000]   I'm going to ask a question.
[01:43:00.000 --> 01:43:02.000]   I'm going to ask a question.
[01:43:02.000 --> 01:43:04.000]   I'm going to ask a question.
[01:43:04.000 --> 01:43:06.000]   I'm going to ask a question.
[01:43:06.000 --> 01:43:08.000]   I'm going to ask a question.
[01:43:08.000 --> 01:43:10.000]   I'm going to ask a question.
[01:43:10.000 --> 01:43:12.000]   I'm going to ask a question.
[01:43:12.000 --> 01:43:14.000]   I'm going to ask a question.
[01:43:14.000 --> 01:43:16.000]   I'm going to ask a question.
[01:43:16.000 --> 01:43:18.000]   I'm going to ask a question.
[01:43:18.000 --> 01:43:20.000]   I'm going to ask a question.
[01:43:20.000 --> 01:43:22.000]   I'm going to ask a question.
[01:43:22.000 --> 01:43:24.000]   I'm going to ask a question.
[01:43:24.000 --> 01:43:26.000]   I'm going to ask a question.
[01:43:26.000 --> 01:43:28.000]   I'm going to ask a question.
[01:43:28.000 --> 01:43:30.000]   I'm going to ask a question.
[01:43:30.000 --> 01:43:32.000]   I'm going to ask a question.
[01:43:32.000 --> 01:43:34.000]   I'm going to ask a question.
[01:43:34.000 --> 01:43:36.000]   I'm going to ask a question.
[01:43:36.000 --> 01:43:38.000]   I'm going to ask a question.
[01:43:38.000 --> 01:43:40.000]   I'm going to ask a question.
[01:43:40.000 --> 01:43:42.000]   I'm going to ask a question.
[01:43:42.000 --> 01:43:44.000]   I'm going to ask a question.
[01:43:44.000 --> 01:43:46.000]   I'm going to ask a question.
[01:43:46.000 --> 01:43:48.000]   I'm going to ask a question.
[01:43:48.000 --> 01:43:50.000]   I'm going to ask a question.
[01:43:50.000 --> 01:43:52.000]   I'm going to ask a question.
[01:43:52.000 --> 01:43:54.000]   I'm going to ask a question.
[01:43:54.000 --> 01:43:56.000]   I'm going to ask a question.
[01:43:56.000 --> 01:43:58.000]   I'm going to ask a question.
[01:43:58.000 --> 01:44:00.000]   I'm going to ask a question.
[01:44:00.000 --> 01:44:02.000]   I'm going to ask a question.
[01:44:02.000 --> 01:44:04.000]   I'm going to ask a question.
[01:44:04.000 --> 01:44:06.000]   I'm going to ask a question.
[01:44:06.000 --> 01:44:08.000]   I'm going to ask a question.
[01:44:08.000 --> 01:44:10.000]   I'm going to ask a question.
[01:44:10.000 --> 01:44:12.000]   I'm going to ask a question.
[01:44:12.000 --> 01:44:14.000]   I'm going to ask a question.
[01:44:14.000 --> 01:44:16.000]   I'm going to ask a question.
[01:44:16.000 --> 01:44:18.000]   I'm going to ask a question.
[01:44:18.000 --> 01:44:20.000]   I'm going to ask a question.
[01:44:20.000 --> 01:44:22.000]   I'm going to ask a question.
[01:44:22.000 --> 01:44:24.000]   I'm going to ask a question.
[01:44:24.000 --> 01:44:26.000]   I'm going to ask a question.
[01:44:26.000 --> 01:44:28.000]   I'm going to ask a question.
[01:44:28.000 --> 01:44:30.000]   I'm going to ask a question.
[01:44:30.000 --> 01:44:32.000]   I'm going to ask a question.
[01:44:32.000 --> 01:44:34.000]   I'm going to ask a question.
[01:44:34.000 --> 01:44:36.000]   I'm going to ask a question.
[01:44:36.000 --> 01:44:38.000]   I'm going to ask a question.
[01:44:38.000 --> 01:44:40.000]   I'm going to ask a question.
[01:44:40.000 --> 01:44:42.000]   I'm going to ask a question.
[01:44:42.000 --> 01:44:44.000]   I'm going to ask a question.
[01:44:44.000 --> 01:44:46.000]   I'm going to ask a question.
[01:44:46.000 --> 01:44:48.000]   I'm going to ask a question.
[01:44:48.000 --> 01:44:50.000]   I'm going to ask a question.
[01:44:50.000 --> 01:44:52.000]   I'm going to ask a question.
[01:44:52.000 --> 01:44:54.000]   I'm going to ask a question.
[01:44:54.000 --> 01:44:56.000]   I'm going to ask a question.
[01:44:56.000 --> 01:44:58.000]   I'm going to ask a question.
[01:44:58.000 --> 01:45:00.000]   I'm going to ask a question.
[01:45:00.000 --> 01:45:02.000]   I'm going to ask a question.
[01:45:02.000 --> 01:45:04.000]   I'm going to ask a question.
[01:45:04.000 --> 01:45:06.000]   I'm going to ask a question.
[01:45:06.000 --> 01:45:08.000]   I'm going to ask a question.
[01:45:08.000 --> 01:45:10.000]   I'm going to ask a question.
[01:45:10.000 --> 01:45:12.000]   I'm going to ask a question.
[01:45:12.000 --> 01:45:14.000]   I'm going to ask a question.
[01:45:14.000 --> 01:45:16.000]   I'm going to ask a question.
[01:45:16.000 --> 01:45:18.000]   I'm going to ask a question.
[01:45:18.000 --> 01:45:20.000]   I'm going to ask a question.
[01:45:20.000 --> 01:45:22.000]   I'm going to ask a question.
[01:45:22.000 --> 01:45:24.000]   I'm going to ask a question.
[01:45:24.000 --> 01:45:26.000]   I'm going to ask a question.
[01:45:26.000 --> 01:45:28.000]   I'm going to ask a question.
[01:45:28.000 --> 01:45:30.000]   I'm going to ask a question.
[01:45:30.000 --> 01:45:32.000]   I'm going to ask a question.
[01:45:32.000 --> 01:45:34.000]   I'm going to ask a question.
[01:45:34.000 --> 01:45:36.000]   I'm going to ask a question.
[01:45:36.000 --> 01:45:38.000]   I'm going to ask a question.
[01:45:38.000 --> 01:45:40.000]   I'm going to ask a question.
[01:45:40.000 --> 01:45:42.000]   I'm going to ask a question.
[01:45:42.000 --> 01:45:44.000]   I'm going to ask a question.
[01:45:44.000 --> 01:45:46.000]   I'm going to ask a question.
[01:45:46.000 --> 01:45:48.000]   I'm going to ask a question.
[01:45:48.000 --> 01:45:50.000]   I'm going to ask a question.
[01:45:50.000 --> 01:45:52.000]   I'm going to ask a question.
[01:45:52.000 --> 01:45:54.000]   I'm going to ask a question.
[01:45:54.000 --> 01:45:56.000]   I'm going to ask a question.
[01:45:56.000 --> 01:45:58.000]   I'm going to ask a question.
[01:45:58.000 --> 01:46:00.000]   I'm going to ask a question.
[01:46:00.000 --> 01:46:02.000]   I'm going to ask a question.
[01:46:02.000 --> 01:46:04.000]   I'm going to ask a question.
[01:46:04.000 --> 01:46:06.000]   I'm going to ask a question.
[01:46:06.000 --> 01:46:08.000]   I'm going to ask a question.
[01:46:08.000 --> 01:46:10.000]   I'm going to ask a question.
[01:46:10.000 --> 01:46:12.000]   I'm going to ask a question.
[01:46:12.000 --> 01:46:14.000]   I'm going to ask a question.
[01:46:14.000 --> 01:46:16.000]   I'm going to ask a question.
[01:46:16.000 --> 01:46:18.000]   I'm going to ask a question.
[01:46:18.000 --> 01:46:20.000]   I'm going to ask a question.
[01:46:20.000 --> 01:46:22.000]   I'm going to ask a question.
[01:46:22.000 --> 01:46:24.000]   I'm going to ask a question.
[01:46:24.000 --> 01:46:26.000]   I'm going to ask a question.
[01:46:26.000 --> 01:46:28.000]   I'm going to ask a question.
[01:46:28.000 --> 01:46:30.000]   I'm going to ask a question.
[01:46:30.000 --> 01:46:32.000]   I'm going to ask a question.
[01:46:32.000 --> 01:46:34.000]   I'm going to ask a question.
[01:46:34.000 --> 01:46:36.000]   I'm going to ask a question.
[01:46:36.000 --> 01:46:38.000]   I'm going to ask a question.
[01:46:38.000 --> 01:46:40.000]   I'm going to ask a question.
[01:46:40.000 --> 01:46:42.000]   I'm going to ask a question.
[01:46:42.000 --> 01:46:44.000]   I'm going to ask a question.
[01:46:44.000 --> 01:46:46.000]   I'm going to ask a question.
[01:46:46.000 --> 01:46:48.000]   I'm going to ask a question.
[01:46:48.000 --> 01:46:50.000]   I'm going to ask a question.
[01:46:50.000 --> 01:46:52.000]   I'm going to ask a question.
[01:46:52.000 --> 01:46:54.000]   I'm going to ask a question.
[01:46:54.000 --> 01:46:56.000]   I'm going to ask a question.
[01:46:56.000 --> 01:46:58.000]   I'm going to ask a question.
[01:46:58.000 --> 01:47:00.000]   I'm going to ask a question.
[01:47:00.000 --> 01:47:02.000]   I'm going to ask a question.
[01:47:02.000 --> 01:47:04.000]   I'm going to ask a question.
[01:47:04.000 --> 01:47:06.000]   I'm going to ask a question.
[01:47:06.000 --> 01:47:08.000]   I'm going to ask a question.
[01:47:08.000 --> 01:47:10.000]   I'm going to ask a question.
[01:47:10.000 --> 01:47:12.000]   I'm going to ask a question.
[01:47:12.000 --> 01:47:14.000]   I'm going to ask a question.
[01:47:14.000 --> 01:47:16.000]   I'm going to ask a question.
[01:47:16.000 --> 01:47:18.000]   I'm going to ask a question.
[01:47:18.000 --> 01:47:20.000]   I'm going to ask a question.
[01:47:20.000 --> 01:47:22.000]   I'm going to ask a question.
[01:47:22.000 --> 01:47:24.000]   I'm going to ask a question.
[01:47:24.000 --> 01:47:26.000]   I'm going to ask a question.
[01:47:26.000 --> 01:47:28.000]   I'm going to ask a question.
[01:47:28.000 --> 01:47:30.000]   I'm going to ask a question.
[01:47:30.000 --> 01:47:32.000]   I'm going to ask a question.
[01:47:32.000 --> 01:47:34.000]   I'm going to ask a question.
[01:47:34.000 --> 01:47:36.000]   I'm going to ask a question.
[01:47:36.000 --> 01:47:38.000]   I'm going to ask a question.
[01:47:38.000 --> 01:47:40.000]   I'm going to ask a question.
[01:47:40.000 --> 01:47:42.000]   I'm going to ask a question.
[01:47:42.000 --> 01:47:44.000]   I'm going to ask a question.
[01:47:44.000 --> 01:47:46.000]   I'm going to ask a question.
[01:47:46.000 --> 01:47:48.000]   I'm going to ask a question.
[01:47:48.000 --> 01:47:50.000]   I'm going to ask a question.
[01:47:50.000 --> 01:47:52.000]   I'm going to ask a question.
[01:47:52.000 --> 01:47:54.000]   I'm going to ask a question.
[01:47:54.000 --> 01:47:56.000]   I'm going to ask a question.
[01:47:56.000 --> 01:47:58.000]   I'm going to ask a question.
[01:47:58.000 --> 01:48:00.000]   I'm going to ask a question.
[01:48:00.000 --> 01:48:02.000]   I'm going to ask a question.
[01:48:02.000 --> 01:48:04.000]   I'm going to ask a question.
[01:48:04.000 --> 01:48:06.000]   I'm going to ask a question.
[01:48:06.000 --> 01:48:08.000]   I'm going to ask a question.
[01:48:08.000 --> 01:48:10.000]   I'm going to ask a question.
[01:48:10.000 --> 01:48:12.000]   I'm going to ask a question.
[01:48:12.000 --> 01:48:14.000]   I'm going to ask a question.
[01:48:14.000 --> 01:48:16.000]   I'm going to ask a question.
[01:48:16.000 --> 01:48:18.000]   I'm going to ask a question.
[01:48:18.000 --> 01:48:20.000]   I'm going to ask a question.
[01:48:20.000 --> 01:48:22.000]   I'm going to ask a question.
[01:48:22.000 --> 01:48:24.000]   I'm going to ask a question.
[01:48:24.000 --> 01:48:26.000]   I'm going to ask a question.
[01:48:26.000 --> 01:48:28.000]   I'm going to ask a question.
[01:48:28.000 --> 01:48:30.000]   I'm going to ask a question.
[01:48:30.000 --> 01:48:32.000]   I'm going to ask a question.
[01:48:32.000 --> 01:48:34.000]   I'm going to ask a question.
[01:48:34.000 --> 01:48:36.000]   I'm going to ask a question.
[01:48:36.000 --> 01:48:38.000]   I'm going to ask a question.
[01:48:38.000 --> 01:48:40.000]   I'm going to ask a question.
[01:48:40.000 --> 01:48:42.000]   I'm going to ask a question.
[01:48:42.000 --> 01:48:44.000]   I'm going to ask a question.
[01:48:44.000 --> 01:48:46.000]   I'm going to ask a question.
[01:48:46.000 --> 01:48:48.000]   I'm going to ask a question.
[01:48:48.000 --> 01:48:50.000]   I'm going to ask a question.
[01:48:50.000 --> 01:48:52.000]   I'm going to ask a question.
[01:48:52.000 --> 01:48:54.000]   I'm going to ask a question.
[01:48:54.000 --> 01:48:56.000]   I'm going to ask a question.
[01:48:56.000 --> 01:48:58.000]   I'm going to ask a question.
[01:48:58.000 --> 01:49:00.000]   I'm going to ask a question.
[01:49:00.000 --> 01:49:02.000]   I'm going to ask a question.
[01:49:02.000 --> 01:49:04.000]   I'm going to ask a question.
[01:49:04.000 --> 01:49:06.000]   I'm going to ask a question.
[01:49:06.000 --> 01:49:08.000]   I'm going to ask a question.
[01:49:08.000 --> 01:49:10.000]   I'm going to ask a question.
[01:49:10.000 --> 01:49:12.000]   I'm going to ask a question.
[01:49:12.000 --> 01:49:14.000]   I'm going to ask a question.
[01:49:14.000 --> 01:49:16.000]   I'm going to ask a question.
[01:49:16.000 --> 01:49:18.000]   I'm going to ask a question.
[01:49:18.000 --> 01:49:20.000]   I'm going to ask a question.
[01:49:20.000 --> 01:49:22.000]   I'm going to ask a question.
[01:49:22.000 --> 01:49:24.000]   I'm going to ask a question.
[01:49:24.000 --> 01:49:26.000]   I'm going to ask a question.
[01:49:26.000 --> 01:49:28.000]   I'm going to ask a question.
[01:49:28.000 --> 01:49:30.000]   I'm going to ask a question.
[01:49:30.000 --> 01:49:32.000]   I'm going to ask a question.
[01:49:32.000 --> 01:49:34.000]   I'm going to ask a question.
[01:49:34.000 --> 01:49:36.000]   I'm going to ask a question.
[01:49:36.000 --> 01:49:38.000]   I'm going to ask a question.
[01:49:38.000 --> 01:49:40.000]   I'm going to ask a question.
[01:49:40.000 --> 01:49:42.000]   I'm going to ask a question.
[01:49:42.000 --> 01:49:44.000]   I'm going to ask a question.
[01:49:44.000 --> 01:49:46.000]   I'm going to ask a question.
[01:49:46.000 --> 01:49:48.000]   I'm going to ask a question.
[01:49:48.000 --> 01:49:50.000]   I'm going to ask a question.
[01:49:50.000 --> 01:49:52.000]   I'm going to ask a question.
[01:49:52.000 --> 01:49:54.000]   I'm going to ask a question.
[01:49:54.000 --> 01:49:56.000]   I'm going to ask a question.
[01:49:56.000 --> 01:49:58.000]   I'm going to ask a question.
[01:49:58.000 --> 01:50:00.000]   I'm going to ask a question.
[01:50:00.000 --> 01:50:02.000]   I'm going to ask a question.
[01:50:02.000 --> 01:50:04.000]   I'm going to ask a question.
[01:50:04.000 --> 01:50:06.000]   I'm going to ask a question.
[01:50:06.000 --> 01:50:08.000]   I'm going to ask a question.
[01:50:08.000 --> 01:50:10.000]   I'm going to ask a question.
[01:50:10.000 --> 01:50:12.000]   I'm going to ask a question.
[01:50:12.000 --> 01:50:14.000]   I'm going to ask a question.
[01:50:14.000 --> 01:50:16.000]   I'm going to ask a question.
[01:50:16.000 --> 01:50:18.000]   I'm going to ask a question.
[01:50:18.000 --> 01:50:20.000]   I'm going to ask a question.
[01:50:20.000 --> 01:50:22.000]   I'm going to ask a question.
[01:50:22.000 --> 01:50:24.000]   I'm going to ask a question.
[01:50:24.000 --> 01:50:26.000]   I'm going to ask a question.
[01:50:26.000 --> 01:50:28.000]   I'm going to ask a question.
[01:50:28.000 --> 01:50:30.000]   I'm going to ask a question.
[01:50:30.000 --> 01:50:32.000]   I'm going to ask a question.
[01:50:32.000 --> 01:50:34.000]   I'm going to ask a question.
[01:50:34.000 --> 01:50:36.000]   I'm going to ask a question.
[01:50:36.000 --> 01:50:38.000]   I'm going to ask a question.
[01:50:38.000 --> 01:50:40.000]   I'm going to ask a question.
[01:50:40.000 --> 01:50:42.000]   I'm going to ask a question.
[01:50:42.000 --> 01:50:44.000]   I'm going to ask a question.
[01:50:44.000 --> 01:50:46.000]   I'm going to ask a question.
[01:50:46.000 --> 01:50:48.000]   I'm going to ask a question.
[01:50:48.000 --> 01:50:50.000]   I'm going to ask a question.
[01:50:50.000 --> 01:50:52.000]   I'm going to ask a question.
[01:50:52.000 --> 01:50:54.000]   I'm going to ask a question.
[01:50:54.000 --> 01:50:56.000]   I'm going to ask a question.
[01:50:56.000 --> 01:50:58.000]   I'm going to ask a question.
[01:50:58.000 --> 01:51:00.000]   I'm going to ask a question.
[01:51:00.000 --> 01:51:02.000]   I'm going to ask a question.
[01:51:02.000 --> 01:51:04.000]   I'm going to ask a question.
[01:51:04.000 --> 01:51:06.000]   I'm going to ask a question.
[01:51:06.000 --> 01:51:08.000]   I'm going to ask a question.
[01:51:08.000 --> 01:51:10.000]   I'm going to ask a question.
[01:51:10.000 --> 01:51:12.000]   I'm going to ask a question.
[01:51:12.000 --> 01:51:14.000]   I'm going to ask a question.
[01:51:14.000 --> 01:51:16.000]   I'm going to ask a question.
[01:51:16.000 --> 01:51:18.000]   I'm going to ask a question.
[01:51:18.000 --> 01:51:20.000]   I'm going to ask a question.
[01:51:20.000 --> 01:51:22.000]   I'm going to ask a question.
[01:51:22.000 --> 01:51:24.000]   I'm going to ask a question.
[01:51:24.000 --> 01:51:26.000]   I'm going to ask a question.
[01:51:26.000 --> 01:51:28.000]   I'm going to ask a question.
[01:51:28.000 --> 01:51:30.000]   I'm going to ask a question.
[01:51:30.000 --> 01:51:32.000]   I'm going to ask a question.
[01:51:32.000 --> 01:51:34.000]   I'm going to ask a question.
[01:51:34.000 --> 01:51:36.000]   I'm going to ask a question.
[01:51:36.000 --> 01:51:38.000]   I'm going to ask a question.
[01:51:38.000 --> 01:51:40.000]   I'm going to ask a question.
[01:51:40.000 --> 01:51:42.000]   I'm going to ask a question.
[01:51:42.000 --> 01:51:44.000]   I'm going to ask a question.
[01:51:44.000 --> 01:51:46.000]   I'm going to ask a question.
[01:51:46.000 --> 01:51:48.000]   I'm going to ask a question.
[01:51:48.000 --> 01:51:50.000]   I'm going to ask a question.
[01:51:50.000 --> 01:51:52.000]   I'm going to ask a question.
[01:51:52.000 --> 01:51:54.000]   I'm going to ask a question.
[01:51:54.000 --> 01:51:56.000]   I'm going to ask a question.
[01:51:56.000 --> 01:51:58.000]   I'm going to ask a question.
[01:51:58.000 --> 01:52:00.000]   I'm going to ask a question.
[01:52:00.000 --> 01:52:02.000]   I'm going to ask a question.
[01:52:02.000 --> 01:52:04.000]   I'm going to ask a question.
[01:52:04.000 --> 01:52:06.000]   I'm going to ask a question.
[01:52:06.000 --> 01:52:08.000]   I'm going to ask a question.
[01:52:08.000 --> 01:52:10.000]   I'm going to ask a question.
[01:52:10.000 --> 01:52:12.000]   I'm going to ask a question.
[01:52:12.000 --> 01:52:14.000]   I'm going to ask a question.
[01:52:14.000 --> 01:52:16.000]   I'm going to ask a question.
[01:52:16.000 --> 01:52:18.000]   I'm going to ask a question.
[01:52:18.000 --> 01:52:20.000]   I'm going to ask a question.
[01:52:20.000 --> 01:52:22.000]   I'm going to ask a question.
[01:52:22.000 --> 01:52:24.000]   I'm going to ask a question.
[01:52:24.000 --> 01:52:26.000]   I'm going to ask a question.
[01:52:26.000 --> 01:52:28.000]   I'm going to ask a question.
[01:52:28.000 --> 01:52:30.000]   I'm going to ask a question.
[01:52:30.000 --> 01:52:32.000]   I'm going to ask a question.
[01:52:32.000 --> 01:52:34.000]   I'm going to ask a question.
[01:52:34.000 --> 01:52:36.000]   I'm going to ask a question.
[01:52:36.000 --> 01:52:38.000]   I'm going to ask a question.
[01:52:38.000 --> 01:52:40.000]   I'm going to ask a question.
[01:52:40.000 --> 01:52:42.000]   I'm going to ask a question.
[01:52:42.000 --> 01:52:44.000]   I'm going to ask a question.
[01:52:44.000 --> 01:52:46.000]   I'm going to ask a question.
[01:52:46.000 --> 01:52:48.000]   I'm going to ask a question.
[01:52:48.000 --> 01:52:50.000]   I'm going to ask a question.
[01:52:50.000 --> 01:52:52.000]   I'm going to ask a question.
[01:52:52.000 --> 01:52:54.000]   I'm going to ask a question.
[01:52:54.000 --> 01:52:56.000]   I'm going to ask a question.
[01:52:56.000 --> 01:52:58.000]   I'm going to ask a question.
[01:52:58.000 --> 01:53:00.000]   I'm going to ask a question.
[01:53:00.000 --> 01:53:02.000]   I'm going to ask a question.
[01:53:02.000 --> 01:53:04.000]   I'm going to ask a question.
[01:53:04.000 --> 01:53:06.000]   I'm going to ask a question.
[01:53:06.000 --> 01:53:08.000]   I'm going to ask a question.
[01:53:08.000 --> 01:53:10.000]   I'm going to ask a question.
[01:53:10.000 --> 01:53:12.000]   I'm going to ask a question.
[01:53:12.000 --> 01:53:14.000]   I'm going to ask a question.
[01:53:14.000 --> 01:53:16.000]   I'm going to ask a question.
[01:53:16.000 --> 01:53:18.000]   I'm going to ask a question.
[01:53:18.000 --> 01:53:20.000]   I'm going to ask a question.
[01:53:20.000 --> 01:53:22.000]   I'm going to ask a question.
[01:53:22.000 --> 01:53:24.000]   I'm going to ask a question.
[01:53:24.000 --> 01:53:26.000]   I'm going to ask a question.
[01:53:26.000 --> 01:53:28.000]   I'm going to ask a question.
[01:53:28.000 --> 01:53:30.000]   I'm going to ask a question.
[01:53:30.000 --> 01:53:32.000]   I'm going to ask a question.
[01:53:32.000 --> 01:53:34.000]   I'm going to ask a question.
[01:53:34.000 --> 01:53:36.000]   I'm going to ask a question.
[01:53:36.000 --> 01:53:38.000]   I'm going to ask a question.
[01:53:38.000 --> 01:53:40.000]   I'm going to ask a question.
[01:53:40.000 --> 01:53:42.000]   I'm going to ask a question.
[01:53:42.000 --> 01:53:44.000]   I'm going to ask a question.
[01:53:44.000 --> 01:53:46.000]   I'm going to ask a question.
[01:53:46.000 --> 01:53:48.000]   I'm going to ask a question.
[01:53:48.000 --> 01:53:50.000]   I'm going to ask a question.
[01:53:50.000 --> 01:53:52.000]   I'm going to ask a question.
[01:53:52.000 --> 01:53:54.000]   I'm going to ask a question.
[01:53:54.000 --> 01:53:56.000]   I'm going to ask a question.
[01:53:56.000 --> 01:53:58.000]   I'm going to ask a question.
[01:53:58.000 --> 01:54:00.000]   I'm going to ask a question.
[01:54:00.000 --> 01:54:02.000]   I'm going to ask a question.
[01:54:02.000 --> 01:54:04.000]   I'm going to ask a question.
[01:54:04.000 --> 01:54:06.000]   I'm going to ask a question.
[01:54:06.000 --> 01:54:08.000]   I'm going to ask a question.
[01:54:08.000 --> 01:54:10.000]   I'm going to ask a question.
[01:54:10.000 --> 01:54:12.000]   I'm going to ask a question.
[01:54:12.000 --> 01:54:14.000]   I'm going to ask a question.
[01:54:14.000 --> 01:54:16.000]   I'm going to ask a question.
[01:54:16.000 --> 01:54:18.000]   I'm going to ask a question.
[01:54:18.000 --> 01:54:20.000]   I'm going to ask a question.
[01:54:20.000 --> 01:54:22.000]   I'm going to ask a question.
[01:54:22.000 --> 01:54:24.000]   I'm going to ask a question.
[01:54:24.000 --> 01:54:26.000]   I'm going to ask a question.
[01:54:26.000 --> 01:54:28.000]   I'm going to ask a question.
[01:54:28.000 --> 01:54:30.000]   I'm going to ask a question.
[01:54:30.000 --> 01:54:32.000]   I'm going to ask a question.
[01:54:32.000 --> 01:54:34.000]   I'm going to ask a question.
[01:54:34.000 --> 01:54:36.000]   I'm going to ask a question.
[01:54:36.000 --> 01:54:38.000]   I'm going to ask a question.
[01:54:38.000 --> 01:54:40.000]   I'm going to ask a question.
[01:54:40.000 --> 01:54:42.000]   I'm going to ask a question.
[01:54:42.000 --> 01:54:44.000]   I'm going to ask a question.
[01:54:44.000 --> 01:54:46.000]   I'm going to ask a question.
[01:54:46.000 --> 01:54:48.000]   I'm going to ask a question.
[01:54:48.000 --> 01:54:50.000]   I'm going to ask a question.
[01:54:50.000 --> 01:54:52.000]   I'm going to ask a question.
[01:54:52.000 --> 01:54:54.000]   I'm going to ask a question.
[01:54:54.000 --> 01:54:56.000]   I'm going to ask a question.
[01:54:56.000 --> 01:54:58.000]   I'm going to ask a question.
[01:54:58.000 --> 01:55:00.000]   I'm going to ask a question.
[01:55:00.000 --> 01:55:02.000]   I'm going to ask a question.
[01:55:02.000 --> 01:55:04.000]   I'm going to ask a question.
[01:55:04.000 --> 01:55:06.000]   I'm going to ask a question.
[01:55:06.000 --> 01:55:08.000]   I'm going to ask a question.
[01:55:08.000 --> 01:55:10.000]   I'm going to ask a question.
[01:55:10.000 --> 01:55:12.000]   I'm going to ask a question.
[01:55:12.000 --> 01:55:14.000]   I'm going to ask a question.
[01:55:14.000 --> 01:55:16.000]   I'm going to ask a question.
[01:55:16.000 --> 01:55:18.000]   I'm going to ask a question.
[01:55:18.000 --> 01:55:20.000]   I'm going to ask a question.
[01:55:20.000 --> 01:55:22.000]   I'm going to ask a question.
[01:55:22.000 --> 01:55:24.000]   I'm going to ask a question.
[01:55:24.000 --> 01:55:26.000]   I'm going to ask a question.
[01:55:26.000 --> 01:55:28.000]   I'm going to ask a question.
[01:55:28.000 --> 01:55:30.000]   I'm going to ask a question.
[01:55:30.000 --> 01:55:32.000]   I'm going to ask a question.
