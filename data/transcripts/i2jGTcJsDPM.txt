
[00:00:00.080 --> 00:00:04.080]   Okay, so moving on to our next chapter, getting started with linechain.
[00:00:04.080 --> 00:00:10.080]   In this chapter, we're going to be introducing linechain by building a simple LM powered
[00:00:10.080 --> 00:00:14.920]   assistant that will do various things for us, it will be multimodal, generating some
[00:00:14.920 --> 00:00:20.160]   text, generating images, generate some structured outputs, it will do a few things.
[00:00:20.160 --> 00:00:24.920]   Now to get started, we will go over to the course repo.
[00:00:24.920 --> 00:00:28.240]   All of the code, all the chapters are in here.
[00:00:28.240 --> 00:00:32.700]   There are two ways of running this, either locally or in Google Colab.
[00:00:32.700 --> 00:00:37.480]   We would recommend running in Google Colab because it's just a lot simpler with environments,
[00:00:37.480 --> 00:00:39.820]   but you can also run it locally.
[00:00:39.820 --> 00:00:43.740]   And actually for the capstone, we will be running it locally.
[00:00:43.740 --> 00:00:47.020]   There's no way of us doing that in Colab.
[00:00:47.020 --> 00:00:51.860]   So if you would like to run everything locally, I'll show you how quickly now, if you would
[00:00:51.860 --> 00:00:57.940]   like to run in Colab, which I would recommend at least for the first notebook chapters, just
[00:00:57.940 --> 00:00:58.860]   skip ahead.
[00:00:58.860 --> 00:01:03.400]   There will be chapter points in the timeline of the video.
[00:01:03.400 --> 00:01:07.180]   So for running, running it locally, we've just come down to here.
[00:01:07.180 --> 00:01:10.180]   So this actually tells you everything that you need.
[00:01:10.180 --> 00:01:14.640]   So you will need to install UV, right?
[00:01:14.640 --> 00:01:20.300]   So this is the package manager that we recommend by the Python and package management library.
[00:01:20.300 --> 00:01:22.560]   You don't need to use UV.
[00:01:22.560 --> 00:01:24.020]   You know, it's, it's up to you.
[00:01:24.020 --> 00:01:25.640]   UV is, is very simple.
[00:01:25.640 --> 00:01:26.640]   It works really well.
[00:01:26.640 --> 00:01:29.020]   so I would recommend that.
[00:01:29.020 --> 00:01:32.100]   So you would install it with this command here.
[00:01:32.100 --> 00:01:33.100]   This is on Mac.
[00:01:33.100 --> 00:01:34.680]   So it will be different.
[00:01:34.680 --> 00:01:40.200]   Otherwise, if you are on Windows or otherwise, you can look at the installation guide there and
[00:01:40.200 --> 00:01:41.100]   it will tell you what to do.
[00:01:41.100 --> 00:01:49.020]   And so before we actually do this, what I will do is go ahead and just clone this repo.
[00:01:49.020 --> 00:01:50.820]   So we'll come into here.
[00:01:50.820 --> 00:01:56.820]   I'm going to create like a temp directory for me because I already have the linechain course
[00:01:56.820 --> 00:01:57.320]   in there.
[00:01:57.320 --> 00:02:00.680]   And what I'm going to do is just git clone linechain course.
[00:02:00.680 --> 00:02:01.180]   Okay.
[00:02:01.360 --> 00:02:05.320]   So you will also need to install a git if you don't have that.
[00:02:05.320 --> 00:02:05.780]   Okay.
[00:02:05.780 --> 00:02:08.000]   So we have that.
[00:02:08.000 --> 00:02:10.120]   Then what we'll do is copy this.
[00:02:10.120 --> 00:02:10.560]   Okay.
[00:02:10.560 --> 00:02:15.300]   So this will install Python 3.12.7 for us with this command.
[00:02:15.300 --> 00:02:23.540]   Then this will create a new vnv within that or using Python 3.12.7 that we've installed.
[00:02:23.540 --> 00:02:30.200]   And then uvsync will actually be looking at the pyproject.toml file.
[00:02:30.280 --> 00:02:36.780]   That's like the package installation for the repo and using that to install everything that
[00:02:36.780 --> 00:02:37.140]   we need.
[00:02:37.140 --> 00:02:41.660]   Now, we should actually make sure that we are within the linechain course directory.
[00:02:41.660 --> 00:02:44.340]   And then yes, we can run those three.
[00:02:44.340 --> 00:02:46.560]   And there we go.
[00:02:46.560 --> 00:02:49.440]   So everything should install with that.
[00:02:49.440 --> 00:02:55.640]   Now, if you are in cursor, you can just do cursor.
[00:02:55.640 --> 00:02:59.520]   Or we can run code.ifnvscode.
[00:02:59.520 --> 00:03:01.120]   I'll just be running this.
[00:03:01.120 --> 00:03:03.980]   And then I've opened up the course.
[00:03:03.980 --> 00:03:07.080]   Now, within that course, you have your notebooks.
[00:03:07.080 --> 00:03:11.280]   And then you just run through these, making sure you select your kernel, Python environment,
[00:03:11.280 --> 00:03:15.080]   and making sure you're using the correct vnv from here.
[00:03:15.180 --> 00:03:18.980]   So that should pop up already as this vnv bin Python.
[00:03:18.980 --> 00:03:20.360]   And you'll click that.
[00:03:20.360 --> 00:03:21.960]   And then you can run through.
[00:03:21.960 --> 00:03:24.800]   When you are running locally, don't run these.
[00:03:24.800 --> 00:03:25.620]   You don't need to.
[00:03:25.620 --> 00:03:26.840]   You've already installed everything.
[00:03:26.840 --> 00:03:27.780]   So you don't.
[00:03:28.020 --> 00:03:30.000]   This specifically is for Colab.
[00:03:30.000 --> 00:03:32.320]   So that is running things locally.
[00:03:32.320 --> 00:03:37.000]   Now let's have a look at running things in Colab.
[00:03:37.080 --> 00:03:41.340]   So for running everything in Colab, we have our notebooks in here.
[00:03:41.340 --> 00:03:45.520]   We click through and then we have each of the chapters through here.
[00:03:45.520 --> 00:03:50.240]   So starting with the first chapter, the introduction, which is where we are now.
[00:03:51.500 --> 00:03:56.960]   So what you can do to open this in Colab is either just click this Colab button here,
[00:03:56.960 --> 00:04:03.380]   or if you really want to, for example, maybe this is not loading for you.
[00:04:03.380 --> 00:04:07.360]   What you can do is you can copy the URL at the top here.
[00:04:07.360 --> 00:04:09.360]   You can go over to Colab.
[00:04:09.360 --> 00:04:16.660]   You can go to open GitHub and then just paste that in there and press enter.
[00:04:16.660 --> 00:04:18.640]   And there we go.
[00:04:18.640 --> 00:04:21.100]   We have our notebook.
[00:04:21.360 --> 00:04:21.560]   Okay.
[00:04:21.560 --> 00:04:22.820]   So we're in now.
[00:04:22.820 --> 00:04:26.540]   What we will do first is just install the prerequisites.
[00:04:26.540 --> 00:04:32.720]   So we have line chain, just a lot of line chain packages here, line chain core, line chain
[00:04:32.720 --> 00:04:38.440]   open AI, because we're using open AI and line chain community, which is needed for running
[00:04:38.440 --> 00:04:39.500]   what we're running.
[00:04:39.500 --> 00:04:40.440]   Okay.
[00:04:40.440 --> 00:04:43.300]   So that has installed everything for us.
[00:04:43.300 --> 00:04:50.220]   So we can move on to our first step, which is initializing our LM.
[00:04:50.220 --> 00:04:57.180]   So we're going to be using GPT-40 mini, which is slightly small, but fast, but also cheaper
[00:04:57.180 --> 00:04:57.700]   model.
[00:04:57.700 --> 00:05:00.100]   That is also very good from open AI.
[00:05:01.040 --> 00:05:04.600]   So what we need to do here is get an API key.
[00:05:04.600 --> 00:05:05.220]   Okay.
[00:05:05.220 --> 00:05:11.140]   So for getting that API key, we're going to go to open AI's website and you can see here
[00:05:11.140 --> 00:05:13.780]   that we're opening platform.openai.com.
[00:05:13.780 --> 00:05:16.820]   And then we're going to be going to settings, organization, API keys.
[00:05:17.820 --> 00:05:19.500]   So you can copy that.
[00:05:19.500 --> 00:05:21.080]   I'll just click it from here.
[00:05:21.080 --> 00:05:21.940]   Okay.
[00:05:21.940 --> 00:05:28.080]   So I'm going to go ahead and create a new secret key to actually, just in case you're kind of
[00:05:28.080 --> 00:05:29.300]   looking for where this is.
[00:05:29.300 --> 00:05:32.620]   It's settings, organization, API keys again.
[00:05:32.620 --> 00:05:33.080]   Okay.
[00:05:33.080 --> 00:05:34.380]   Create a new API key.
[00:05:34.700 --> 00:05:36.960]   I'm going to call it line train course.
[00:05:36.960 --> 00:05:41.620]   I'll just put it on the semantic router.
[00:05:41.620 --> 00:05:43.140]   That's just my organization.
[00:05:43.140 --> 00:05:45.520]   You, you put it wherever you want it to be.
[00:05:46.320 --> 00:05:49.060]   And then you would copy your API key.
[00:05:49.060 --> 00:05:50.320]   You can see mine here.
[00:05:50.320 --> 00:05:54.240]   I'm obviously going to revert you out before you see this, but you can try and use it if
[00:05:54.240 --> 00:05:54.960]   you really like.
[00:05:54.960 --> 00:05:58.960]   So I'm going to copy that and I'm going to place it into this little box here.
[00:05:58.960 --> 00:06:04.980]   You could also just place it, put your full API key in here.
[00:06:04.980 --> 00:06:08.140]   It's up to you, but this little box just makes things easier.
[00:06:08.140 --> 00:06:13.200]   Now that what we've basically done there is just pass in our API key.
[00:06:13.200 --> 00:06:19.960]   We're setting our open AI model, GPT-4-0-mini, and what we're going to be doing now is essentially
[00:06:19.960 --> 00:06:23.980]   just connecting and setting up our LLM parameters with line chain.
[00:06:23.980 --> 00:06:31.700]   So we run that, we say, okay, we're using a GPT-4-0-mini, and we're also setting ourselves
[00:06:31.700 --> 00:06:37.800]   up to use two different LLMs here, or two of the same LLM with slightly different settings.
[00:06:37.800 --> 00:06:42.000]   So the first of those is an LLM with a temperature setting of zero.
[00:06:42.000 --> 00:06:49.780]   The temperature setting basically controls almost the randomness of the output of your LLM.
[00:06:49.780 --> 00:06:58.900]   And the way that it works is when an LLM is predicting the sort of next token or next word
[00:06:58.900 --> 00:07:04.660]   in sequence, it'll provide a probability actually for all of the tokens within the LLM's knowledge
[00:07:04.660 --> 00:07:06.740]   base or what the LLM has been trained on.
[00:07:06.740 --> 00:07:13.880]   So what we do when we set a temperature of zero is we say, you are going to give us the
[00:07:13.880 --> 00:07:16.860]   token with highest probability according to you.
[00:07:16.860 --> 00:07:17.300]   Okay.
[00:07:17.300 --> 00:07:24.240]   Whereas when we set a temperature of 0.9, what we're saying is, okay, there's actually an increased
[00:07:24.240 --> 00:07:32.440]   probability of you giving us a token that according to your generated output is not the token with
[00:07:32.440 --> 00:07:34.780]   highest probability according to the LLM.
[00:07:34.780 --> 00:07:39.220]   But what that tends to do is give us more sort of creative outputs.
[00:07:39.220 --> 00:07:40.220]   So that's what the temperature does.
[00:07:40.220 --> 00:07:46.660]   So we are creating a normal LLM and then a more creative LLM with this.
[00:07:46.660 --> 00:07:49.000]   So what are we going to be building?
[00:07:49.000 --> 00:07:53.200]   We're going to be taking a article draft.
[00:07:53.200 --> 00:08:00.420]   So like a draft article from the Aurelio learning page, and we're going to be using the LLM chain
[00:08:00.420 --> 00:08:06.840]   to generate various things that we might find helpful as where, you know, we have this article
[00:08:06.840 --> 00:08:10.100]   draft and we're editing it and just kind of like finalizing it.
[00:08:10.100 --> 00:08:11.580]   So what are those going to be?
[00:08:11.580 --> 00:08:12.640]   You can see them here.
[00:08:12.640 --> 00:08:19.160]   We have the title for the article, the description, an SEO friendly description specifically.
[00:08:19.160 --> 00:08:24.560]   The third one, we're going to be getting the LLM to provide us advice on existing paragraph
[00:08:24.560 --> 00:08:28.820]   and essentially writing a new paragraph for us from that existing paragraph.
[00:08:28.820 --> 00:08:34.360]   And what it's going to do, this is the structured output part, is it's going to write a new version
[00:08:34.360 --> 00:08:35.160]   of that paragraph for us.
[00:08:35.300 --> 00:08:38.180]   And it's going to give us advice on where we can improve our writing.
[00:08:38.180 --> 00:08:43.460]   Then we're going to generate a thumbnail or hero image for our article.
[00:08:43.460 --> 00:08:45.900]   So, you know, nice image that you would put at the top.
[00:08:45.900 --> 00:08:50.220]   So here, we're just going to input our article.
[00:08:50.220 --> 00:08:52.260]   You can put something else in here if you like.
[00:08:52.260 --> 00:08:59.200]   Essentially, this is just a big article that's written a little while back on agents.
[00:09:00.180 --> 00:09:05.640]   And now we can go ahead and stop preparing our prompts, which are essentially the instructions
[00:09:05.640 --> 00:09:06.480]   for our LLM.
[00:09:06.480 --> 00:09:13.480]   So Linechain comes with a lot of different like utilities for prompts.
[00:09:13.480 --> 00:09:16.960]   And we're going to dive into them in a lot more detail, but I do want to just give you
[00:09:16.960 --> 00:09:22.240]   the essentials now, just so you can understand what we're looking at, at least conceptually.
[00:09:22.240 --> 00:09:27.760]   So prompts for chat agents are, at a minimum, broken up into three components.
[00:09:27.760 --> 00:09:29.860]   Those are the system prompt.
[00:09:29.860 --> 00:09:34.680]   This provides instructions to our LLM on how it should behave, what its objective is, and
[00:09:34.680 --> 00:09:37.220]   how it should go about achieving that objective.
[00:09:37.220 --> 00:09:42.840]   Generally, system prompts are going to be a bit longer than what we have here, depending on
[00:09:42.840 --> 00:09:43.480]   the use case.
[00:09:43.480 --> 00:09:45.140]   Then we have our user prompts.
[00:09:45.140 --> 00:09:47.780]   So these are user written messages.
[00:09:48.380 --> 00:09:52.600]   Usually, sometimes we might want to pre-populate those if we want to encourage a particular
[00:09:52.600 --> 00:09:57.300]   type of conversational patterns from our agent.
[00:09:57.300 --> 00:10:01.140]   But for the most part, yes, these are going to be user generated.
[00:10:01.140 --> 00:10:03.280]   Then we have our AI prompts.
[00:10:03.280 --> 00:10:05.820]   So these are, of course, AI generated.
[00:10:05.820 --> 00:10:12.420]   And again, in some cases, we might want to generate those ourselves beforehand or within
[00:10:12.420 --> 00:10:15.360]   a conversation if we have a particular reason for doing so.
[00:10:15.500 --> 00:10:20.380]   But for the most part, you can assume that these are actually user and AI generated.
[00:10:20.380 --> 00:10:27.360]   Now, the LineChain provides us with templates for each one of these prompt types.
[00:10:27.360 --> 00:10:31.740]   Let's go ahead and have a look at what these look like within LineChain.
[00:10:31.740 --> 00:10:35.900]   So to begin, we are looking at this one.
[00:10:35.900 --> 00:10:42.720]   So we have our system message prompt template and human messages, the user that we saw before.
[00:10:43.240 --> 00:10:46.500]   So we have these two system prompt, keeping it quite simple here.
[00:10:46.500 --> 00:10:50.000]   You are an AI system that helps generate article titles, right?
[00:10:50.000 --> 00:10:53.680]   So our first component where we want to generate is article title.
[00:10:53.680 --> 00:10:58.020]   So we're telling the AI that's what we want it to do.
[00:10:58.020 --> 00:10:59.580]   And then here, right?
[00:10:59.580 --> 00:11:06.680]   So here we're actually providing kind of like a template for a user input.
[00:11:06.680 --> 00:11:15.380]   So yes, as I mentioned, user input can be fully generated by a user.
[00:11:15.380 --> 00:11:19.140]   It might be kind of not generated by a user.
[00:11:19.140 --> 00:11:23.820]   It might be setting up a conversation beforehand, which a user would later use.
[00:11:23.820 --> 00:11:27.880]   Or in this scenario, we're actually creating a template.
[00:11:28.520 --> 00:11:34.300]   And what the user will provide us will actually just be inserted here inside article.
[00:11:34.300 --> 00:11:37.100]   And that's why we have this import variables.
[00:11:37.100 --> 00:11:43.220]   So what this is going to do is, okay, we have all of these instructions around here.
[00:11:43.220 --> 00:11:48.040]   They're all going to be provided to OpenAI as if it is the user saying this.
[00:11:48.300 --> 00:11:54.040]   But it will actually just be this here that a user will be providing, okay?
[00:11:54.040 --> 00:11:56.440]   And we might want to also format this a little nicer.
[00:11:56.440 --> 00:11:57.360]   It kind of depends.
[00:11:57.360 --> 00:11:58.900]   This will work as it is.
[00:11:58.900 --> 00:12:04.980]   But we can also put, you know, something like this to make it a little bit clearer to the LM.
[00:12:04.980 --> 00:12:06.760]   Okay, what is the article?
[00:12:06.760 --> 00:12:08.620]   Where are the prompts?
[00:12:08.620 --> 00:12:11.220]   So we have that.
[00:12:11.220 --> 00:12:16.280]   And you can see in this scenario, there's not that much difference between what the system prompt
[00:12:16.280 --> 00:12:17.320]   and the user prompt is doing.
[00:12:17.400 --> 00:12:19.200]   And this is, it's a particular scenario.
[00:12:19.200 --> 00:12:23.400]   It varies when you get into the more conversational stuff, as we will do later.
[00:12:23.400 --> 00:12:31.640]   You'll see that the user prompt is generally more fully user generated or mostly user generated.
[00:12:31.640 --> 00:12:37.640]   And much of these types of instructions, we might actually be putting into the system prompt.
[00:12:37.640 --> 00:12:38.760]   It varies.
[00:12:38.760 --> 00:12:43.780]   And we'll see throughout the course, many different ways of using these different types of prompts
[00:12:43.780 --> 00:12:44.900]   in various different places.
[00:12:45.940 --> 00:12:48.180]   Then, you'll see here.
[00:12:48.180 --> 00:12:50.740]   So I just want to show you how this is working.
[00:12:50.740 --> 00:12:59.980]   We can use this format method on our user prompt here to actually insert something within the article input here.
[00:13:00.180 --> 00:13:04.740]   So we're going to go user prompt format, and then we'll pass in something for article.
[00:13:04.740 --> 00:13:05.300]   Okay.
[00:13:05.300 --> 00:13:10.140]   And we can also maybe format this a little nicer, but I'll just show you this for now.
[00:13:10.140 --> 00:13:11.720]   So we have our human message.
[00:13:11.720 --> 00:13:14.640]   And then inside the content, this is the text that we had.
[00:13:14.920 --> 00:13:16.980]   All right, you can see that we have all of this, right?
[00:13:16.980 --> 00:13:18.280]   And this is what we wrote before.
[00:13:18.280 --> 00:13:20.600]   We wrote all of this, except from this part.
[00:13:20.600 --> 00:13:22.280]   We didn't write this.
[00:13:22.280 --> 00:13:24.280]   Instead of this, we had article.
[00:13:24.280 --> 00:13:25.260]   All right.
[00:13:25.260 --> 00:13:29.660]   So let's format this a little nicer so that we can see.
[00:13:29.660 --> 00:13:30.620]   Okay.
[00:13:30.720 --> 00:13:32.800]   So this is exactly what we wrote up here.
[00:13:32.800 --> 00:13:36.960]   Exactly the same, except from now we have test string instead of article.
[00:13:37.340 --> 00:13:41.940]   So later, when we insert our article, it's going to go inside there.
[00:13:41.940 --> 00:13:42.740]   It's all we're doing.
[00:13:42.740 --> 00:13:45.780]   It's like it's an F string in Python.
[00:13:45.780 --> 00:13:46.120]   Okay.
[00:13:46.120 --> 00:13:49.820]   And this is, again, this is one of the things where people might complain about Langchain.
[00:13:49.820 --> 00:13:54.900]   You know, this sort of thing can be, you know, it seems excessive because you could just do
[00:13:54.900 --> 00:13:55.740]   this with an F string.
[00:13:55.740 --> 00:14:01.380]   But there are, as we'll see later, particularly when you're streaming, just really helpful features
[00:14:01.380 --> 00:14:09.580]   that come with using Langchain's kind of built-in prompt templates or at least message objects
[00:14:09.580 --> 00:14:10.380]   that we'll see.
[00:14:10.380 --> 00:14:15.680]   So, you know, we need to keep that in mind.
[00:14:15.680 --> 00:14:19.380]   Again, as soon as it's more complicated, Langchain can be a bit more useful.
[00:14:19.380 --> 00:14:21.480]   So chat prompt template.
[00:14:21.480 --> 00:14:26.800]   This is basically just going to take what we have here, our system prompt, user prompt.
[00:14:26.800 --> 00:14:29.280]   You could also include some AI prompts in there.
[00:14:29.780 --> 00:14:33.240]   And what it's going to do is merge both of those.
[00:14:33.240 --> 00:14:40.400]   And then when we do format, what it's going to do is put both of those together into a chat
[00:14:40.400 --> 00:14:40.920]   history.
[00:14:40.920 --> 00:14:41.740]   Okay.
[00:14:41.740 --> 00:14:45.600]   So let's see what that looks like first in a more messy way.
[00:14:45.600 --> 00:14:46.740]   Okay.
[00:14:46.740 --> 00:14:50.720]   So you can see we have just the content, right?
[00:14:50.720 --> 00:14:55.240]   So it doesn't include the whole, you know, before we had human message, we're not include,
[00:14:55.240 --> 00:14:56.920]   we're not seeing anything like that here.
[00:14:56.920 --> 00:14:59.160]   Instead, we're just seeing the string.
[00:14:59.160 --> 00:15:01.780]   So now let's switch back to print.
[00:15:01.780 --> 00:15:06.900]   And we can see that what we have is our system message here.
[00:15:06.900 --> 00:15:08.680]   It's just prefixed with this system.
[00:15:08.680 --> 00:15:11.160]   And then we have human and it's prefixed by human.
[00:15:11.160 --> 00:15:12.360]   And then it continues.
[00:15:12.360 --> 00:15:12.880]   Right.
[00:15:12.880 --> 00:15:14.020]   So that's, that's all it's doing.
[00:15:14.020 --> 00:15:16.140]   It's just kind of merging those in some sort of chat.
[00:15:16.140 --> 00:15:19.900]   Like we could also put in like AI messages and they would appear in there as well.
[00:15:19.900 --> 00:15:20.320]   Okay.
[00:15:20.320 --> 00:15:21.740]   So we have that.
[00:15:21.740 --> 00:15:24.660]   Now that is our prompt template.
[00:15:24.660 --> 00:15:30.540]   Let's put that together with an LLM to create what would be in the past line chain be called
[00:15:30.540 --> 00:15:31.620]   an LLM chain.
[00:15:31.620 --> 00:15:36.780]   Now we wouldn't necessarily call it an LLM chain because we're not using the LLM chain abstraction.
[00:15:37.040 --> 00:15:39.140]   It's not super important if that doesn't make sense.
[00:15:39.140 --> 00:15:44.920]   We will go into it in more detail later, particularly in the, in the LLL chapter.
[00:15:45.980 --> 00:15:52.580]   So what this chain will do, you know, think LLM chain is just chains where we're chaining
[00:15:52.580 --> 00:15:54.020]   together these multiple comparements.
[00:15:54.020 --> 00:15:57.500]   It will perform the SEPs prompt formatting.
[00:15:57.500 --> 00:15:59.100]   So that's what I just showed you.
[00:15:59.100 --> 00:16:01.500]   LLM generation.
[00:16:01.500 --> 00:16:07.180]   So sending our prompt to OpenAI, getting a response and getting that output.
[00:16:07.180 --> 00:16:12.080]   So you can also add another SEP here if you want to format that in a particular way.
[00:16:12.080 --> 00:16:15.860]   We're going to be outputting that in a particular format so that we can feed it into
[00:16:15.860 --> 00:16:16.920]   the next set more easily.
[00:16:16.920 --> 00:16:24.460]   But there are also things called output parses, which parse your output in a more dynamic or
[00:16:24.460 --> 00:16:26.360]   complicated way, depending on what you're doing.
[00:16:26.360 --> 00:16:29.240]   So this is our first look at LLL.
[00:16:29.240 --> 00:16:34.880]   I don't want us to focus too much on the syntax here because we will be doing that later, but
[00:16:34.880 --> 00:16:42.760]   I do want you to just understand what is actually happening here and logically, what are we writing?
[00:16:43.200 --> 00:16:49.880]   So all we really need to know right now is we define our inputs with the first dictionary
[00:16:49.880 --> 00:16:50.820]   segment here.
[00:16:50.820 --> 00:16:51.700]   All right.
[00:16:51.700 --> 00:16:56.000]   So this is, you know, our inputs, which we have defined already.
[00:16:56.000 --> 00:16:56.420]   Okay.
[00:16:56.840 --> 00:17:04.500]   So if we come up to our user prompt here, we said the input variable is our article, right?
[00:17:04.500 --> 00:17:08.240]   And we might have also added input variables to the system prompt here as well.
[00:17:08.240 --> 00:17:17.980]   In that case, you know, let's say we had your AI assistant called name, right?
[00:17:18.020 --> 00:17:33.480]   And then what we would have to do down here is we would also have to pass that in.
[00:17:33.480 --> 00:17:33.980]   Right.
[00:17:33.980 --> 00:17:39.080]   So also we would have article, but we would also have name.
[00:17:39.080 --> 00:17:45.760]   So basically we just need to make sure that in here we're including the variables that
[00:17:45.760 --> 00:17:49.600]   have defined as input variables for our, our first prompts.
[00:17:49.600 --> 00:17:50.200]   Okay.
[00:17:50.200 --> 00:17:52.440]   So we can actually go ahead and let's add that.
[00:17:52.440 --> 00:17:54.680]   Uh, so we can see it's in action.
[00:17:54.680 --> 00:18:00.940]   So we'll run this again and just include that or, or re-initialize our first prompt.
[00:18:01.640 --> 00:18:07.460]   So we see that, and if we just have a look at what that means for this format function
[00:18:07.460 --> 00:18:09.940]   here, it means we'll also need to pass in a name.
[00:18:09.940 --> 00:18:10.640]   Okay.
[00:18:10.640 --> 00:18:11.940]   And call it Joe.
[00:18:11.940 --> 00:18:12.700]   Okay.
[00:18:12.700 --> 00:18:14.820]   So Joe, the AI, right?
[00:18:14.820 --> 00:18:16.840]   So you are an AI system called Joe now.
[00:18:16.840 --> 00:18:17.480]   Okay.
[00:18:17.480 --> 00:18:22.600]   So we have Joe, our AI that is going to be fed in through these input variables.
[00:18:22.600 --> 00:18:24.420]   Then we have this pipe operator.
[00:18:24.420 --> 00:18:30.220]   The pipe operator is basically saying whatever is to the left of the pipe operator, which in
[00:18:30.220 --> 00:18:35.320]   this case would be this is going to go into whatever is on the right of the pipe operator.
[00:18:35.320 --> 00:18:36.200]   It's that simple.
[00:18:36.200 --> 00:18:41.200]   Again, we'll, we'll dive into this and kind of break it apart in the LSL chapter, but for
[00:18:41.200 --> 00:18:42.180]   now that's all we need to know.
[00:18:42.180 --> 00:18:48.100]   So this is going to go into our first prompt that is going to perform everything.
[00:18:48.100 --> 00:18:51.660]   It's going to add the name and the article that we've provided into our first prompt.
[00:18:51.660 --> 00:18:53.920]   And then it's going to output that, right?
[00:18:53.920 --> 00:18:56.220]   It's going to output that we have our pipe operator here.
[00:18:56.220 --> 00:19:00.200]   So the output of this is going to go into the input of our next.
[00:19:00.200 --> 00:19:06.400]   So creative LM, then that is going to generate some tokens.
[00:19:06.400 --> 00:19:07.820]   It's going to generate our output.
[00:19:07.820 --> 00:19:11.320]   That output is going to be an AI message.
[00:19:11.320 --> 00:19:19.820]   And as you saw before, if I take this bit out within those message objects, we have this
[00:19:19.820 --> 00:19:20.600]   content field.
[00:19:20.860 --> 00:19:28.480]   OK, so we are actually going to extract the content field out from our AI message to just
[00:19:28.480 --> 00:19:29.260]   get the content.
[00:19:29.260 --> 00:19:30.900]   And that is what we do here.
[00:19:30.900 --> 00:19:35.540]   So we get the AI message out from our LM and then we're extracting the content from that AI
[00:19:35.540 --> 00:19:36.420]   message object.
[00:19:36.420 --> 00:19:40.920]   And we're going to pass it into a dictionary that just contains article title like so.
[00:19:41.440 --> 00:19:43.060]   Okay, we don't need to do that.
[00:19:43.060 --> 00:19:44.920]   We can just get the AI message directly.
[00:19:44.920 --> 00:19:50.520]   I just want to show you how we are using this sort of chain in LCL.
[00:19:50.520 --> 00:19:58.400]   So once we have set up our chain, we then call it or execute it using the invoke method.
[00:19:58.400 --> 00:20:01.160]   Into that, we will need to pass in those variables.
[00:20:01.160 --> 00:20:04.780]   So we have our article already, but we also gave our AI a name now.
[00:20:04.900 --> 00:20:07.120]   So let's add that and we'll run this.
[00:20:07.120 --> 00:20:17.060]   OK, so Joe has generated us a article title, Unlocking the Future, the Rise of Neurosymbolic
[00:20:17.060 --> 00:20:18.380]   AI Agents.
[00:20:18.380 --> 00:20:18.860]   Cool.
[00:20:18.860 --> 00:20:24.920]   Much better name than what I gave that article, which was AI Agents are Neurosymbolic Systems.
[00:20:24.920 --> 00:20:27.540]   No, I don't think I did too bad.
[00:20:27.540 --> 00:20:29.420]   OK, so we have that.
[00:20:30.360 --> 00:20:35.880]   Now, let's continue, and what we're going to be doing is building more of these types
[00:20:35.880 --> 00:20:42.280]   of LLM chain pipelines where we're feeding in some prompts, we're generating something,
[00:20:42.280 --> 00:20:44.760]   getting something and doing something with it.
[00:20:44.760 --> 00:20:48.120]   So, as mentioned, we have the title.
[00:20:48.120 --> 00:20:51.160]   We're now moving on to the description, so we want to generate description.
[00:20:51.160 --> 00:20:53.400]   So we have our human message prompt template.
[00:20:53.400 --> 00:20:57.960]   So this is actually going to go into a similar format as before.
[00:20:58.520 --> 00:21:04.920]   We probably also want to redefine this because I think I'm using the same system message there.
[00:21:04.920 --> 00:21:08.120]   So let's go ahead and modify that.
[00:21:08.120 --> 00:21:15.000]   Or what we could also do is let's just remove the name now because I've shown you that.
[00:21:15.000 --> 00:21:23.880]   So what we could do is you're an AI assistant that helps build good articles, right?
[00:21:23.880 --> 00:21:25.320]   Build good articles.
[00:21:25.320 --> 00:21:30.680]   And we could just use this as our generic system prompt now.
[00:21:30.680 --> 00:21:32.920]   So let's say that's our new system prompt.
[00:21:32.920 --> 00:21:34.280]   Now we have our user prompt.
[00:21:34.280 --> 00:21:36.200]   You're tasked with creating a description for the article.
[00:21:36.200 --> 00:21:38.360]   The article is here for you to examine article.
[00:21:38.360 --> 00:21:40.360]   Here is the article title.
[00:21:40.360 --> 00:21:43.960]   OK, so we need the article title now as well and our input variables.
[00:21:43.960 --> 00:21:47.560]   And then we're going to output an SEO friendly article description.
[00:21:47.560 --> 00:21:52.360]   And we're just saying, just to be certain here, do not output anything other than the description.
[00:21:52.360 --> 00:21:57.160]   So, you know, sometimes an LLM might say, hey, look, this is what I've generated for you.
[00:21:57.160 --> 00:22:00.200]   The reason I think this is good is because so on and so on and so on.
[00:22:00.200 --> 00:22:00.600]   All right.
[00:22:00.600 --> 00:22:06.040]   If you're programmatically taking some output from an LLM, you don't want all of that fluff
[00:22:06.040 --> 00:22:07.720]   around what the LLM has generated.
[00:22:07.720 --> 00:22:11.080]   You just want exactly what you've asked it for.
[00:22:11.080 --> 00:22:14.520]   OK, because otherwise you need to pass out with code and it can get messy
[00:22:14.520 --> 00:22:16.920]   and also just far less reliable.
[00:22:16.920 --> 00:22:20.040]   So we're just saying do not output anything else.
[00:22:20.040 --> 00:22:21.400]   Then we're putting all of these together.
[00:22:21.400 --> 00:22:23.960]   So system prompt and the second user prompt.
[00:22:23.960 --> 00:22:24.600]   This one here.
[00:22:24.600 --> 00:22:28.280]   Putting those together into a new chat prompt template.
[00:22:28.280 --> 00:22:33.880]   And then we're going to feed all that in to another LL chain as we have here to
[00:22:33.880 --> 00:22:37.000]   generate our description.
[00:22:37.000 --> 00:22:38.040]   So let's go ahead.
[00:22:38.040 --> 00:22:39.560]   We invoke that as before.
[00:22:39.560 --> 00:22:43.160]   We'll just make sure we add in the article title that we got from before.
[00:22:43.160 --> 00:22:45.640]   And let's see what we get.
[00:22:45.640 --> 00:22:47.400]   OK, so we have this.
[00:22:47.400 --> 00:22:50.680]   Explore the transformative potential of neuro-symbolic AI agents in.
[00:22:50.680 --> 00:22:54.120]   A little bit long, to be honest.
[00:22:54.120 --> 00:22:56.040]   But yeah, you can see what it's doing here.
[00:22:56.040 --> 00:22:56.280]   Right.
[00:22:56.280 --> 00:22:59.240]   And of course, we could then go in and we see this is kind of too long.
[00:22:59.240 --> 00:23:02.440]   We're like, oh, yeah, SEO friendly description.
[00:23:02.440 --> 00:23:03.880]   Not really.
[00:23:03.880 --> 00:23:05.800]   So we can modify this.
[00:23:06.280 --> 00:23:08.200]   Output the SEO friendly description.
[00:23:08.200 --> 00:23:13.000]   Make sure we don't exceed.
[00:23:13.000 --> 00:23:15.240]   Let me put that on a new line.
[00:23:15.240 --> 00:23:18.440]   Make sure we don't exceed say 200 characters.
[00:23:18.440 --> 00:23:20.040]   Or maybe it's even less to SEO.
[00:23:20.040 --> 00:23:20.440]   I don't.
[00:23:20.440 --> 00:23:21.640]   I don't have a clue.
[00:23:21.640 --> 00:23:23.720]   I'll just say 120 characters.
[00:23:23.720 --> 00:23:26.120]   I do not outplay anything other than the description.
[00:23:26.120 --> 00:23:26.360]   All right.
[00:23:26.360 --> 00:23:31.160]   So we could just go back, modify our prompting, see what that generates again.
[00:23:31.160 --> 00:23:33.000]   OK, so much shorter.
[00:23:33.000 --> 00:23:34.200]   Probably too short now.
[00:23:34.200 --> 00:23:35.400]   But that's fine.
[00:23:35.400 --> 00:23:35.720]   Cool.
[00:23:35.720 --> 00:23:36.680]   So we have that.
[00:23:36.680 --> 00:23:37.720]   We have a summary.
[00:23:37.720 --> 00:23:38.440]   Process that.
[00:23:38.440 --> 00:23:41.560]   And that's now in this dictionary format that we have here.
[00:23:41.560 --> 00:23:42.920]   Cool.
[00:23:42.920 --> 00:23:45.000]   Now the third step.
[00:23:45.000 --> 00:23:49.720]   We want to consume that first article variable with our full article.
[00:23:50.680 --> 00:23:54.600]   And we're going to generate a few different output fields.
[00:23:54.600 --> 00:23:59.400]   So for this, we're going to be using the structured output feature.
[00:23:59.400 --> 00:24:01.320]   So let's scroll down.
[00:24:01.320 --> 00:24:05.160]   And we'll see what that is or what that looks like.
[00:24:05.160 --> 00:24:09.160]   So structured output is essentially we're forcing that limit.
[00:24:09.160 --> 00:24:14.840]   Like it has to output a dictionary with these particular fields.
[00:24:14.840 --> 00:24:17.560]   OK, and we can modify this quite a bit.
[00:24:17.560 --> 00:24:22.680]   But in this scenario, what I want to do is I want there to be an original paragraph.
[00:24:22.680 --> 00:24:22.920]   Right.
[00:24:22.920 --> 00:24:26.040]   So I just want it to regenerate the original paragraph because I'm lazy.
[00:24:26.040 --> 00:24:27.640]   And I don't want to extract it out.
[00:24:27.640 --> 00:24:31.480]   Then I want to get the new edited paragraph.
[00:24:31.480 --> 00:24:34.520]   This is the LN generated improved paragraph.
[00:24:34.520 --> 00:24:39.480]   And then we want to get some feedback because we don't want to just automate ourselves.
[00:24:39.480 --> 00:24:44.760]   We want to augment ourselves and get better with AI rather than just being like,
[00:24:44.760 --> 00:24:46.200]   ah, you do, you do this.
[00:24:46.200 --> 00:24:48.520]   So that's what we do here.
[00:24:48.520 --> 00:24:52.040]   And you can see that here we're using this Pydantic object.
[00:24:52.040 --> 00:24:55.400]   And what Pydantic allows us to do is define these particular fields.
[00:24:55.400 --> 00:24:59.160]   And it also allows us to assign these descriptions to a field.
[00:24:59.160 --> 00:25:02.520]   And Lionchain is actually going to go ahead, read all of this.
[00:25:02.520 --> 00:25:03.400]   Right.
[00:25:03.400 --> 00:25:04.200]   Even reads.
[00:25:04.200 --> 00:25:11.400]   So, for example, we could put integer here and we could actually get a numeric score for our paragraph.
[00:25:11.400 --> 00:25:11.640]   Right.
[00:25:11.640 --> 00:25:12.280]   And we can try that.
[00:25:12.280 --> 00:25:12.520]   Right.
[00:25:12.520 --> 00:25:14.360]   So let's just try that quickly.
[00:25:14.360 --> 00:25:14.920]   I'll show you.
[00:25:14.920 --> 00:25:18.200]   So numeric, numeric score.
[00:25:18.200 --> 00:25:22.920]   In fact, let's even just ignore, let's not put anything here.
[00:25:22.920 --> 00:25:26.440]   So I'm going to put constructive feedback on the original paragraph, but I just put into it.
[00:25:26.440 --> 00:25:27.320]   So let's see what happens.
[00:25:27.320 --> 00:25:28.280]   Okay.
[00:25:28.280 --> 00:25:29.480]   So we have that.
[00:25:29.480 --> 00:25:33.560]   And what I'm going to do is I'm going to get our creative LM.
[00:25:33.560 --> 00:25:37.320]   I'm going to use this with structured output method, and that's actually going to modify
[00:25:37.320 --> 00:25:43.960]   that LM class, create a new LM class that forces that LM to use this structure for the output.
[00:25:43.960 --> 00:25:44.280]   Right.
[00:25:44.280 --> 00:25:49.080]   So passing in paragraph into here, using this, we're creating this new structured LM.
[00:25:50.040 --> 00:25:53.320]   So let's run that and see what happens.
[00:25:53.320 --> 00:25:53.960]   Okay.
[00:25:53.960 --> 00:25:57.400]   So we're going to modify our chain accordingly.
[00:25:57.400 --> 00:26:02.520]   Maybe what I can do is also just remove this bit for now.
[00:26:02.520 --> 00:26:06.280]   So we can just see what the structured LM outputs directly.
[00:26:06.280 --> 00:26:06.840]   And let's see.
[00:26:06.840 --> 00:26:09.880]   Okay.
[00:26:09.880 --> 00:26:14.440]   So now you can see that we actually have that paragraph object, right?
[00:26:14.440 --> 00:26:16.280]   The one we defined up here, which is kind of cool.
[00:26:16.280 --> 00:26:19.720]   And then in there, we have the original paragraph, right?
[00:26:19.720 --> 00:26:21.800]   So this is where this is coming from.
[00:26:21.800 --> 00:26:26.200]   I definitely remember writing something that looks a lot like that.
[00:26:26.200 --> 00:26:27.880]   So I think that is correct.
[00:26:27.880 --> 00:26:29.400]   We have the edited paragraph.
[00:26:29.400 --> 00:26:31.400]   So this is, okay, what it thinks is better.
[00:26:31.400 --> 00:26:37.400]   And then interestingly, the feedback is three, which is weird, right?
[00:26:37.400 --> 00:26:41.480]   Because here we said the constructive feedback on the original paragraph.
[00:26:41.480 --> 00:26:45.400]   But what we're doing when we use this with structured output,
[00:26:45.400 --> 00:26:49.400]   but what Langchain is doing is, is essentially performing a tool call to
[00:26:49.400 --> 00:26:50.120]   OpenAI.
[00:26:50.120 --> 00:26:55.800]   And what a tool call can do is force a particular structure in the output of an LM.
[00:26:55.800 --> 00:27:00.200]   So when we say feedback has to be an integer, no matter what we put here,
[00:27:00.200 --> 00:27:02.200]   it's going to give us an integer.
[00:27:02.200 --> 00:27:05.560]   Because how do you provide constructive feedback with an integer?
[00:27:05.560 --> 00:27:07.240]   It doesn't really make sense.
[00:27:07.240 --> 00:27:13.400]   But because we've set that limitation, that restriction here, that is what it does.
[00:27:13.400 --> 00:27:16.440]   It just gives us the numeric value.
[00:27:16.440 --> 00:27:20.840]   So I'm going to shift that to string and then let's rerun this, see what we get.
[00:27:20.840 --> 00:27:21.080]   Okay.
[00:27:21.080 --> 00:27:24.520]   We should now see that we actually do get constructive feedback.
[00:27:24.520 --> 00:27:27.640]   All right, so yeah, we can see it's quite, quite long.
[00:27:27.640 --> 00:27:32.760]   So the original paragraph effectively communicates limitations with neural AI systems in performing
[00:27:32.760 --> 00:27:33.560]   certain tests.
[00:27:33.560 --> 00:27:38.040]   However, it could benefit from slightly improved clarity and conciseness.
[00:27:38.040 --> 00:27:44.120]   For example, the phrase was becoming clear can be made more direct by changing it to became evident.
[00:27:44.120 --> 00:27:46.200]   Yeah, true.
[00:27:46.200 --> 00:27:47.000]   Thank you very much.
[00:27:47.560 --> 00:27:52.360]   So yeah, now we actually get that feedback, which is pretty nice.
[00:27:52.360 --> 00:27:56.200]   Now let's add in this final step to our chain.
[00:27:56.200 --> 00:27:57.880]   Okay.
[00:27:57.880 --> 00:28:03.000]   And it's just going to pull out our paragraph object here and extract into a dictionary.
[00:28:03.000 --> 00:28:04.600]   We don't necessarily need to do this.
[00:28:04.600 --> 00:28:07.320]   Honestly, I actually kind of prefer it within this paragraph object.
[00:28:07.320 --> 00:28:14.840]   But just so we can see how we would pass things on the other side of the chain.
[00:28:14.840 --> 00:28:18.040]   Okay, so now we can see we've extracted that out.
[00:28:18.040 --> 00:28:18.600]   Cool.
[00:28:18.600 --> 00:28:23.480]   So we have all of that interesting feedback again.
[00:28:23.480 --> 00:28:27.960]   But let's leave it there for the text part of this.
[00:28:27.960 --> 00:28:32.920]   Now let's have a look at the sort of multimodal features that we can work with.
[00:28:32.920 --> 00:28:37.480]   So this is, you know, maybe one of those things that's kind of seems a bit more abstracted,
[00:28:37.480 --> 00:28:40.920]   a little bit complicated where it maybe could be improved.
[00:28:40.920 --> 00:28:44.680]   But, you know, we're not going to really be focusing too much on the multimodal stuff.
[00:28:44.680 --> 00:28:46.280]   We'll be focusing on language.
[00:28:46.280 --> 00:28:48.920]   But I did want to just show you very quickly.
[00:28:48.920 --> 00:28:50.680]   So we want this article to look better.
[00:28:50.680 --> 00:28:51.960]   Okay.
[00:28:51.960 --> 00:29:03.080]   We want to generate a prompt based on the article itself that we can then pass to DALI, the image
[00:29:03.080 --> 00:29:09.080]   generation model from OpenAI, that will then generate an image like a like a thumbnail image for us.
[00:29:09.080 --> 00:29:09.320]   Okay.
[00:29:09.320 --> 00:29:14.760]   So the first step of that is we're actually going to get an element to generate that.
[00:29:14.760 --> 00:29:15.000]   All right.
[00:29:15.000 --> 00:29:18.040]   So we have our prompt that we're going to use for that.
[00:29:18.040 --> 00:29:24.120]   So I'm going to say generate a prompt with less than 500 characters to generate an image based on
[00:29:24.120 --> 00:29:25.080]   the following article.
[00:29:25.080 --> 00:29:26.040]   Okay.
[00:29:26.040 --> 00:29:28.920]   So that's our prompt, you know, super simple.
[00:29:28.920 --> 00:29:31.320]   We're using the generic prompt template here.
[00:29:31.320 --> 00:29:32.040]   You can use that.
[00:29:32.040 --> 00:29:34.120]   You can use user prompt template.
[00:29:34.120 --> 00:29:35.320]   It's up to you.
[00:29:35.320 --> 00:29:38.040]   This is just like the generic prompt template.
[00:29:38.040 --> 00:29:44.440]   Then what we're going to be doing is based on what this outputs, we're then going to feed
[00:29:44.440 --> 00:29:50.280]   that in to this generate and display image function via the image prompt parameter.
[00:29:50.280 --> 00:29:54.440]   That is going to use the DALI API wrapper from line chain.
[00:29:54.440 --> 00:29:59.640]   It's going to run that image prompt, and we're going to get a URL out from that essentially.
[00:29:59.640 --> 00:30:03.000]   And then we're going to read that using SK image here.
[00:30:03.000 --> 00:30:03.320]   All right.
[00:30:03.320 --> 00:30:07.400]   So it's going to read that image URL, going to get the image data, and then we're just going
[00:30:07.400 --> 00:30:08.120]   to display it.
[00:30:08.120 --> 00:30:09.240]   Okay.
[00:30:09.240 --> 00:30:11.880]   So pretty straightforward.
[00:30:11.880 --> 00:30:18.440]   Now, again, this is a L cell thing here that we're doing, and we have this runnable
[00:30:18.440 --> 00:30:24.680]   Lambda thing when we're running functions within L cell, we need to wrap them within this runnable
[00:30:24.680 --> 00:30:25.080]   Lambda.
[00:30:25.080 --> 00:30:31.080]   I, you know, I don't want to go too much into what this is doing here because we do cover
[00:30:31.080 --> 00:30:35.960]   in the L cell chapter, but it's just, you know, all you really need to know is we have a custom
[00:30:35.960 --> 00:30:38.360]   function, wrap it in runnable Lambda.
[00:30:38.360 --> 00:30:44.600]   And then what we get from that, we can use within this here, the L cell syntax.
[00:30:44.600 --> 00:30:46.600]   So what are we doing here?
[00:30:46.600 --> 00:30:47.480]   Let's figure this out.
[00:30:47.480 --> 00:30:53.000]   We are taking our original, that image prompt that we defined just up here, right?
[00:30:53.000 --> 00:30:55.560]   Input variable to that is article.
[00:30:55.560 --> 00:30:56.200]   Okay.
[00:30:56.200 --> 00:31:01.800]   We have our article data being input here, feeding that into our prompt.
[00:31:01.800 --> 00:31:06.520]   From there, we get our message that we then feed into our LM.
[00:31:06.520 --> 00:31:12.360]   From the LM, it's going to generate us a, like an image prompt, like a prompt for generating our
[00:31:12.360 --> 00:31:13.240]   image.
[00:31:13.240 --> 00:31:19.000]   For this article, we can even let's, let's print that out so that we can see what it generates.
[00:31:19.000 --> 00:31:20.680]   Because I'm also kind of curious.
[00:31:20.680 --> 00:31:22.040]   Okay.
[00:31:22.040 --> 00:31:23.720]   So we'll just run that.
[00:31:23.720 --> 00:31:25.560]   And then let's see.
[00:31:25.560 --> 00:31:31.960]   It will feed in that content into our runnable, which is basically this function here.
[00:31:31.960 --> 00:31:33.320]   And we'll see what it generates.
[00:31:33.320 --> 00:31:34.200]   Okay.
[00:31:34.200 --> 00:31:37.240]   Don't expect anything amazing from Dali.
[00:31:37.240 --> 00:31:42.200]   It's not, it's not the best to be honest, but we, at least we see how to use it.
[00:31:42.200 --> 00:31:42.600]   Okay.
[00:31:42.600 --> 00:31:46.040]   So we can see the prompt that was used here.
[00:31:46.040 --> 00:31:49.800]   Create an image that visually represents the concept of neuro symbolic agents.
[00:31:49.800 --> 00:31:55.080]   Depicts a futuristic interface where large language interact with traditional code,
[00:31:55.080 --> 00:31:57.720]   symbolizing integration of, oh my gosh.
[00:31:57.720 --> 00:32:02.760]   Something computation include elements like a brain to represent neural networks.
[00:32:02.760 --> 00:32:06.360]   Gears or circuits or symbolic logic.
[00:32:06.360 --> 00:32:11.720]   And a web of connections illustrating vast use cases of AI agents.
[00:32:11.720 --> 00:32:12.840]   Oh my gosh.
[00:32:12.840 --> 00:32:13.560]   Look at all that.
[00:32:13.560 --> 00:32:16.040]   Big prompt.
[00:32:16.040 --> 00:32:17.160]   Then we get this.
[00:32:17.160 --> 00:32:19.160]   So, you know, Dali is interesting.
[00:32:19.160 --> 00:32:21.160]   I would say we could even take this.
[00:32:21.160 --> 00:32:25.720]   Let's just see what that comes up with in something like mid journey.
[00:32:25.720 --> 00:32:31.240]   And you can see these way cooler images that we get from just another image generation model.
[00:32:31.240 --> 00:32:33.320]   Far better, but pretty cool, honestly.
[00:32:33.320 --> 00:32:39.160]   So in terms of generation images, the phrasing that the prompt itself is actually pretty good.
[00:32:39.800 --> 00:32:43.880]   The image, you know, it could be better, but that's it.
[00:32:43.880 --> 00:32:44.200]   Right.
[00:32:44.200 --> 00:32:50.760]   So with all of that, we've seen a little introduction to what we might building with line chain.
[00:32:50.760 --> 00:32:53.000]   So that's it for our introduction chapter.
[00:32:53.000 --> 00:32:57.960]   As I mentioned, we don't want to go too much into what each of these things is doing.
[00:32:57.960 --> 00:33:00.920]   I just really want to focus on.
[00:33:00.920 --> 00:33:01.240]   Okay.
[00:33:01.240 --> 00:33:05.400]   This is kind of how we're building something with line chain.
[00:33:05.400 --> 00:33:10.440]   This is the overall flow, but we don't really want to be focusing too much on.
[00:33:10.440 --> 00:33:11.000]   Okay.
[00:33:11.000 --> 00:33:18.120]   What exactly LSL is doing or what exactly, you know, this prompt thing is that we're setting up.
[00:33:18.120 --> 00:33:24.040]   We're going to be focusing much more on all of those things and much more in the upcoming chapters.
[00:33:24.040 --> 00:33:31.720]   So for now, we've just seen a little bit of what we can build before diving in, in more detail.
[00:33:31.720 --> 00:33:33.880]   So for now, we're going to be focusing on the next step.
[00:33:33.880 --> 00:33:35.880]   So for now, we're going to be focusing on the next step.
[00:33:35.880 --> 00:33:37.880]   So for now, we're going to be focusing on the next step.
[00:33:37.880 --> 00:33:39.880]   So for now, we're going to be focusing on the next step.
[00:33:39.880 --> 00:33:41.880]   So for now, we're going to be focusing on the next step.
[00:33:41.880 --> 00:33:43.880]   So for now, we're going to be focusing on the next step.
[00:33:43.880 --> 00:33:45.880]   So for now, we're going to be focusing on the next step.
[00:33:45.880 --> 00:33:47.880]   So for now, we're going to be focusing on the next step.

