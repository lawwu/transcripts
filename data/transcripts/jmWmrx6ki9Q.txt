
[00:00:00.000 --> 00:00:02.680]   There can be a thousand refugees from a country you don't like.
[00:00:02.680 --> 00:00:04.340]   You put it on the news and people say,
[00:00:04.340 --> 00:00:06.040]   "Oh, we just couldn't possibly absorb them.
[00:00:06.040 --> 00:00:08.140]   We're at our absolute breaking point. This is terrible."
[00:00:08.140 --> 00:00:09.940]   Student loan forgiveness.
[00:00:09.940 --> 00:00:12.020]   Oi, oi, vey.
[00:00:12.020 --> 00:00:15.360]   I mean, this is one where I think it's very hard to find almost any economist,
[00:00:15.360 --> 00:00:17.520]   no matter how left-wing or progressive,
[00:00:17.520 --> 00:00:21.320]   who really wants to stick their necks out and defend this garbage.
[00:00:21.320 --> 00:00:24.800]   The amount of time that American feminists spend on female infanticide
[00:00:24.800 --> 00:00:28.740]   in China and India, could it even be 1% of the rhetoric?
[00:00:28.740 --> 00:00:30.300]   It's just not something that they care about.
[00:00:30.300 --> 00:00:31.900]   It's not just, "Oh, I can get to be a billionaire.
[00:00:31.900 --> 00:00:33.440]   I'll do this thing and make the money."
[00:00:33.440 --> 00:00:34.940]   It is something that actually, I think,
[00:00:34.940 --> 00:00:37.840]   fosters a whole culture of entrepreneurship.
[00:00:37.840 --> 00:00:39.380]   I mean, again, we've been hanging out in Austin.
[00:00:39.380 --> 00:00:40.680]   All over there, there's a whole bunch of people
[00:00:40.680 --> 00:00:42.720]   who are never going to be billionaires, Dwarkesh.
[00:00:42.720 --> 00:00:44.680]   But, you know, I've told people, like Dwarkesh,
[00:00:44.680 --> 00:00:45.980]   will Dwarkesh ever be a billionaire?
[00:00:45.980 --> 00:00:48.620]   Probably not, but 2%.
[00:00:48.620 --> 00:00:52.060]   Like Dwarkesh is just a mover and a shaker.
[00:00:52.060 --> 00:00:55.260]   Okay. Today, I have the great honor for the third time
[00:00:55.260 --> 00:00:57.200]   of interviewing Brian Kaplan again.
[00:00:57.200 --> 00:00:59.100]   Brian, thanks so much for coming on the podcast.
[00:00:59.100 --> 00:01:01.400]   I've got the great honor of being interviewed by you, Dwarkesh.
[00:01:01.400 --> 00:01:03.900]   You're one of my favorite people in the world.
[00:01:03.900 --> 00:01:07.100]   Yeah, it's a greater pleasure every time, for me at least.
[00:01:07.100 --> 00:01:10.700]   So, let's talk about your book, Don't Be a Feminist.
[00:01:10.700 --> 00:01:15.280]   Is there any margin of representation of women in, like, leadership roles
[00:01:15.280 --> 00:01:17.720]   at which you think there should be introduced bias
[00:01:17.720 --> 00:01:18.740]   to make sure more women get in,
[00:01:18.740 --> 00:01:22.220]   even if the original ratio is not because of bias?
[00:01:22.220 --> 00:01:24.520]   No, I believe in meritocracy.
[00:01:24.520 --> 00:01:26.020]   I think it is a good system.
[00:01:26.020 --> 00:01:30.860]   It is one that almost everyone sees the intuitive appeal of, and it works.
[00:01:30.860 --> 00:01:32.900]   So, just looking at a group and saying,
[00:01:32.900 --> 00:01:34.860]   "We need to get more members of Group X,"
[00:01:34.860 --> 00:01:37.000]   is just the wrong way to be approaching it.
[00:01:37.000 --> 00:01:40.400]   Rather, you need to be focusing on,
[00:01:40.400 --> 00:01:45.640]   "Let's try to figure out the best way of getting the top-quality people here."
[00:01:45.640 --> 00:01:49.940]   But if there's just an astounding ratio of men in certain positions,
[00:01:49.940 --> 00:01:53.980]   could that potentially have an impact on the company's ability to do good business?
[00:01:53.980 --> 00:01:58.680]   That the company would just care about increasing the ratio for that reason alone?
[00:01:58.680 --> 00:02:00.720]   Right. I mean, one can imagine that.
[00:02:00.720 --> 00:02:02.960]   I mean, I think in our culture, it really goes the other way,
[00:02:02.960 --> 00:02:07.920]   and I think that people are more likely to be trying to get rid of men,
[00:02:07.920 --> 00:02:10.660]   despite the fact that the men are delivering value.
[00:02:10.660 --> 00:02:12.360]   I mean, if you really push me into starting to think,
[00:02:12.360 --> 00:02:14.940]   "Well, suppose that you're running a bar. Would you have ladies' night?"
[00:02:14.940 --> 00:02:16.340]   Yeah, I would have ladies' night in a bar,
[00:02:16.340 --> 00:02:20.700]   because that actually works, and it's good business.
[00:02:20.700 --> 00:02:25.140]   But if what you're doing is trying to actually get correct answers to things,
[00:02:25.140 --> 00:02:28.680]   if you're trying to go and make something run effectively,
[00:02:28.680 --> 00:02:32.520]   if you're just trying to make progress, if you're trying to learn new things,
[00:02:32.520 --> 00:02:36.620]   the thing to focus on is what actually leads to knowledge,
[00:02:36.620 --> 00:02:41.420]   and not focusing on just trying to get demographic representation.
[00:02:41.420 --> 00:02:44.300]   And really, I think what we've seen is once you go down that route,
[00:02:44.300 --> 00:02:45.360]   it is a slippery slope.
[00:02:45.360 --> 00:02:51.340]   So, I would actually, besides defending meritocracy on its merits,
[00:02:51.340 --> 00:02:55.580]   also say the slippery slope argument is not one that should be dismissed lightly.
[00:02:55.580 --> 00:02:59.180]   There's a lot of evidence that it really does actually fit the facts,
[00:02:59.180 --> 00:03:03.340]   and when you make an exception of that kind, it really does lead you to bad places.
[00:03:03.340 --> 00:03:05.720]   Okay, but changing topics a little bit,
[00:03:05.720 --> 00:03:09.720]   I wonder if this gives you greater sympathy for immigration restrictionists,
[00:03:09.720 --> 00:03:11.760]   because their argument is similar,
[00:03:11.760 --> 00:03:14.960]   that there's no natural shelling point for your keyhole solutions,
[00:03:14.960 --> 00:03:16.460]   where you let tens of millions of people in,
[00:03:16.460 --> 00:03:18.460]   but you don't give them welfare or voting rights,
[00:03:18.460 --> 00:03:20.060]   that there's a slippery slope, you let them in,
[00:03:20.060 --> 00:03:23.240]   and then eventually the civil rights argument is going to extend to them,
[00:03:23.240 --> 00:03:27.040]   there'll be adverse consequences that these keyhole solutions can't solve for.
[00:03:27.040 --> 00:03:30.800]   So, for there, first of all, I would say maybe.
[00:03:30.800 --> 00:03:34.040]   Yeah, so that is one of the best arguments to keyhole solutions.
[00:03:34.040 --> 00:03:35.920]   I mean, I'm guessing that a lot of your listeners
[00:03:35.920 --> 00:03:37.920]   have no idea what keyhole solutions are, Dorkash,
[00:03:37.920 --> 00:03:41.220]   so maybe we want to back up and explain that.
[00:03:41.220 --> 00:03:42.360]   Go for it.
[00:03:42.360 --> 00:03:44.560]   Sure, so I have a totally unrelated book,
[00:03:44.560 --> 00:03:47.560]   Open Borders, The Science and Ethics of Immigration.
[00:03:47.560 --> 00:03:53.900]   One of the chapters goes over ways of dealing with complaints about immigration
[00:03:53.900 --> 00:03:58.140]   that fall short of stopping people from actually coming
[00:03:58.140 --> 00:04:00.780]   or kicking out people that are already there.
[00:04:00.780 --> 00:04:03.040]   So, we're just to back up a little bit further.
[00:04:03.040 --> 00:04:05.680]   So, most of the book talks about complaints about immigration
[00:04:05.680 --> 00:04:08.720]   and say that they're either totally wrong or overstated.
[00:04:08.720 --> 00:04:09.880]   But then I have another chapter saying,
[00:04:09.880 --> 00:04:12.220]   "All right, fine, maybe you don't agree with that.
[00:04:12.220 --> 00:04:14.420]   But isn't there another way that we could deal with this?"
[00:04:14.420 --> 00:04:18.060]   So, for example, if you're worried about immigrants voting poorly,
[00:04:18.060 --> 00:04:20.460]   you could say, "Fine, we won't extend voting rights to immigrants
[00:04:20.460 --> 00:04:23.200]   or make them wait for a longer time period."
[00:04:23.200 --> 00:04:25.400]   I mean, that's one where I would just say that
[00:04:25.400 --> 00:04:30.500]   the focal point of citizen versus non-citizen is one of the strongest ones.
[00:04:30.500 --> 00:04:34.540]   So, I think that it actually is one that has a lot of stability.
[00:04:34.540 --> 00:04:36.540]   This line of, "Well, you're not a citizen,
[00:04:36.540 --> 00:04:40.180]   therefore something really does have a lot of intuitive appeal."
[00:04:40.180 --> 00:04:42.620]   Although, yes, I do think that Sakeel solutions
[00:04:42.620 --> 00:04:44.960]   would probably not work multi-generationally,
[00:04:44.960 --> 00:04:46.460]   at least unlikely to work.
[00:04:46.460 --> 00:04:50.100]   So, to go and say, "This is a Sakeel solution where you're not a citizen,
[00:04:50.100 --> 00:04:53.060]   your kids are not citizens, their kids after them are not citizens,"
[00:04:53.060 --> 00:04:55.300]   that's one that I think would be hard to maintain.
[00:04:55.300 --> 00:04:56.900]   But again, at the same time, I'd also just say
[00:04:56.900 --> 00:04:59.360]   the problems that people are worrying about,
[00:04:59.360 --> 00:05:02.040]   if they ever were severe, are also getting diluted over time.
[00:05:02.040 --> 00:05:04.300]   So, I wouldn't worry about it so much.
[00:05:04.300 --> 00:05:07.680]   But yes, that is one of the very best objections to Sakeel solutions
[00:05:07.680 --> 00:05:09.540]   that I know of, Dorkesh. Good for you.
[00:05:09.540 --> 00:05:12.520]   Okay. So, going back to feminism,
[00:05:12.520 --> 00:05:16.580]   doesn't, over time, feminism naturally become true?
[00:05:16.580 --> 00:05:20.860]   So, one of the things you can say that the way that society is unfair to men
[00:05:20.860 --> 00:05:25.400]   is that they had to fight in wars or do difficult and dangerous jobs.
[00:05:25.400 --> 00:05:28.000]   But, you know, society over time becomes more peaceful,
[00:05:28.000 --> 00:05:32.160]   or at least has in our timeline, and the difficult jobs get automated.
[00:05:32.160 --> 00:05:37.340]   At the same time, the gains for people who are at the very peak of any discipline,
[00:05:37.340 --> 00:05:40.940]   they keep going up, maybe fairly, but the implication still is that
[00:05:40.940 --> 00:05:44.720]   if men are overrepresented there, again, even for biological reasons,
[00:05:44.720 --> 00:05:47.620]   then the relative gains that they get go up, right?
[00:05:47.620 --> 00:05:49.980]   So, over time, feminism just becomes more true.
[00:05:49.980 --> 00:05:52.280]   Not because society necessarily is discriminating against women,
[00:05:52.280 --> 00:05:56.460]   but just because of the trends in technology.
[00:05:56.460 --> 00:05:59.260]   Once again, I feel like we should just back up a little bit.
[00:05:59.260 --> 00:06:01.260]   What is feminism anyway?
[00:06:01.260 --> 00:06:02.660]   Because if we don't know what that is,
[00:06:02.660 --> 00:06:06.200]   then it's very hard to talk about whether it's becoming more true over time.
[00:06:06.200 --> 00:06:09.260]   In my book, I begin with some popular dictionary definitions
[00:06:09.260 --> 00:06:12.540]   that just say feminism is the theory that women should be
[00:06:12.540 --> 00:06:16.300]   the political, social, economic, cultural equals of men.
[00:06:16.300 --> 00:06:20.880]   I say that this is a terrible definition, which just violates normal usage.
[00:06:20.880 --> 00:06:21.640]   Why?
[00:06:21.640 --> 00:06:25.120]   Well, we actually have public opinion data on, first of all,
[00:06:25.120 --> 00:06:27.560]   whether people are or are not feminists, and second of all,
[00:06:27.560 --> 00:06:32.160]   what they believe about the political, social, economic, cultural equality of women.
[00:06:32.160 --> 00:06:33.720]   And guess what?
[00:06:33.720 --> 00:06:38.700]   An overwhelming majority of people that say they are not feminists
[00:06:38.700 --> 00:06:43.360]   still agree with the equality of women on all those mentions,
[00:06:43.360 --> 00:06:47.200]   which makes you realize, well, that really can't be the definition of feminism.
[00:06:47.200 --> 00:06:50.900]   That would be like saying feminism is the theory that the sky is blue, right?
[00:06:50.900 --> 00:06:52.640]   Well, feminists do believe the sky is blue,
[00:06:52.640 --> 00:06:55.580]   but that isn't what distinguishes feminists from other people.
[00:06:55.580 --> 00:06:57.480]   So, what does distinguish them?
[00:06:57.480 --> 00:07:02.780]   And what I say is that the really distinguishing view of feminism
[00:07:02.780 --> 00:07:07.220]   is that society treats women less fairly than men, right?
[00:07:07.220 --> 00:07:09.320]   The view that society treats women less fairly than men
[00:07:09.320 --> 00:07:11.620]   or treats men more fairly than women.
[00:07:11.620 --> 00:07:14.960]   This definition, I think, fits actual usage.
[00:07:14.960 --> 00:07:16.700]   It would be very strange for someone to say,
[00:07:16.700 --> 00:07:19.540]   "I'm a feminist, but I think that men get terrible treatment in our society
[00:07:19.540 --> 00:07:22.000]   and women are treated like goddesses."
[00:07:22.000 --> 00:07:25.340]   You say, "Well, then you're not really a feminist, are you?
[00:07:25.340 --> 00:07:27.280]   That doesn't make sense," right?
[00:07:27.280 --> 00:07:30.720]   Or, on the other hand, for someone to say that,
[00:07:30.720 --> 00:07:37.420]   "I am not a feminist, but, God, we treat women so terribly, like we're awful," right?
[00:07:37.420 --> 00:07:38.720]   That, again, just would not fit.
[00:07:38.720 --> 00:07:41.720]   So, I'm not saying this is the one true definition,
[00:07:41.720 --> 00:07:46.060]   but rather that it is much closer to what people actually mean by feminism
[00:07:46.060 --> 00:07:48.200]   than what dictionaries say.
[00:07:48.200 --> 00:07:51.300]   Although, to be fair, every now and then, there'll be a better definition.
[00:07:51.300 --> 00:07:55.840]   Like, I think the Wikipedia definition in the second sentence adds on
[00:07:55.840 --> 00:07:59.340]   that it also has the view that women are treated very unfairly.
[00:07:59.340 --> 00:08:02.340]   Could it not just be – maybe another way of defining feminism
[00:08:02.340 --> 00:08:04.920]   is just that we should raise the status of women?
[00:08:04.920 --> 00:08:07.380]   And that's different, slightly, than the fairness issue,
[00:08:07.380 --> 00:08:10.780]   because if you think of a feminist historian, right?
[00:08:10.780 --> 00:08:14.460]   Maybe their contention is not that women were treated unfairly in the past.
[00:08:14.460 --> 00:08:18.000]   Maybe they just want to raise the status of women in the past who are underrepresented.
[00:08:18.000 --> 00:08:20.760]   Or if you think of somebody today who wants to, let's say,
[00:08:20.760 --> 00:08:22.600]   raise the status of Asians in our society, right?
[00:08:22.600 --> 00:08:25.940]   Like, they want to acknowledge the great things that Asians are doing in our society.
[00:08:25.940 --> 00:08:28.700]   Maybe their contention is not even that Asians are treated unfairly.
[00:08:28.700 --> 00:08:29.680]   They just want to raise the status.
[00:08:29.680 --> 00:08:32.440]   So, what do you think of that definition?
[00:08:32.440 --> 00:08:35.420]   So, first of all, could be, but I don't think so.
[00:08:35.420 --> 00:08:36.040]   Here's what I think.
[00:08:36.040 --> 00:08:40.460]   There could be a few people like that, but that's not what the word means in normal use.
[00:08:40.460 --> 00:08:43.680]   If someone were to say, "Women are treated absolutely fantastically,
[00:08:43.680 --> 00:08:46.660]   way better than men, and I want it to get even higher," right?
[00:08:46.660 --> 00:08:47.860]   You'd say, "Hmm."
[00:08:47.860 --> 00:08:54.160]   Well, that's not what I think, like someone might say, "Well, I can still be a feminist and think that."
[00:08:54.160 --> 00:08:57.340]   Okay, but that's not what people, that's not what the word actually means.
[00:08:57.340 --> 00:09:01.500]   It's not what is the typical view of people who call themselves feminists.
[00:09:01.500 --> 00:09:04.840]   The typical view is precisely that women are treated very unfairly.
[00:09:04.840 --> 00:09:08.980]   And also, they want to raise that and alleviate that in a way that's almost by definition.
[00:09:08.980 --> 00:09:12.220]   If you think that someone's being treated unfairly, then to say, "I think they're being treated really unfairly,
[00:09:12.220 --> 00:09:14.380]   but I think that's great that it's unfair."
[00:09:14.380 --> 00:09:17.180]   It's like, "Hmm, it's almost self-contradictory."
[00:09:17.180 --> 00:09:21.700]   I guess I was making a slightly different point, which is not even that these people who want to raise the status of women
[00:09:21.700 --> 00:09:25.360]   want to raise the actual living standards of women in some way.
[00:09:25.360 --> 00:09:28.360]   It's just that they want to raise the rhetorical status.
[00:09:28.360 --> 00:09:33.040]   Yes, but again, if someone were to say, "I think that women are treated absolutely fantastically in society,
[00:09:33.040 --> 00:09:40.440]   way better than men, and we treat like dogs, but I also want women's status to be even higher than it already is,"
[00:09:40.440 --> 00:09:44.620]   that would be something where you could argue, "Well, that person may be still a feminist,"
[00:09:44.620 --> 00:09:50.620]   but that is not what the word means because hardly anyone who calls themselves a feminist
[00:09:50.620 --> 00:09:52.460]   believes that weird thing that you're talking about.
[00:09:52.460 --> 00:09:53.660]   Let me make an analogy.
[00:09:53.660 --> 00:09:56.420]   Let's say you or I are a libertarian, right?
[00:09:56.420 --> 00:09:59.560]   And then we think we should raise the status of billionaires.
[00:09:59.560 --> 00:10:02.360]   No, it's not like we think society mistreats billionaires.
[00:10:02.360 --> 00:10:04.700]   They're pretty fine, right?
[00:10:04.700 --> 00:10:06.940]   But we think their status should be even higher, right?
[00:10:06.940 --> 00:10:08.540]   Yeah. I mean, this just goes to the definition.
[00:10:08.540 --> 00:10:12.980]   In order to find out whether a definition is correct, you just have to think, "Well, how is the word commonly used?"
[00:10:12.980 --> 00:10:17.880]   It's not to say that, logically speaking, it's possible to have a different view or that two things are compatible.
[00:10:17.880 --> 00:10:22.120]   The whole idea of a definition is, ideally, you're trying to find necessary and sufficient conditions
[00:10:22.120 --> 00:10:26.760]   such that everybody who satisfies the conditions falls under the category
[00:10:26.760 --> 00:10:29.360]   and that everybody who doesn't satisfy the conditions doesn't.
[00:10:29.360 --> 00:10:32.660]   In ordinary language, of course, it's notoriously hard to really do that.
[00:10:32.660 --> 00:10:38.200]   Defining a table is actually quite difficult in a necessary and sufficient condition sense,
[00:10:38.200 --> 00:10:45.040]   but we can still say, "Well, a table is not, by definition, something that people sit on."
[00:10:45.040 --> 00:10:46.420]   Right? And if somebody said, "Well, you could sit on a table."
[00:10:46.420 --> 00:10:53.820]   Yeah, I suppose you could sit on a table, but that's not the definition in ordinary use in any language of which I'm aware.
[00:10:53.820 --> 00:10:59.120]   Right? But why don't we actually go to your real question, which we got on this, on the aggression.
[00:10:59.120 --> 00:11:01.520]   So, the question was —
[00:11:01.520 --> 00:11:06.700]   Overall, the left tail of society is being oppressed, and the right tail is being expanded.
[00:11:06.700 --> 00:11:09.080]   Does feminism become more true over time?
[00:11:09.080 --> 00:11:15.280]   And the answer is, we really need to look at all of the main measures to get an idea of this.
[00:11:15.280 --> 00:11:18.700]   So, some of the ones that you're talking about, it does make more sense.
[00:11:18.700 --> 00:11:26.800]   So, as jobs become less physically dangerous, then at least you might say that things are less unfair to men.
[00:11:26.800 --> 00:11:32.720]   Although, in the book, what I say is, even that is a bit more complicated, superficially, at least, on the surface.
[00:11:32.720 --> 00:11:38.720]   The immediate reaction is, society is less fair to men because they do the most dangerous jobs.
[00:11:38.720 --> 00:11:41.960]   Although, I also said, "Yeah, but they get monetary compensation for that."
[00:11:41.960 --> 00:11:45.600]   So, all things considered, you probably shouldn't think of it as unfair.
[00:11:45.600 --> 00:11:53.880]   It's something where it's reasonable to say, "Hey, wait a second. How come men are the ones that are enduring 90% of the workplace deaths?"
[00:11:53.880 --> 00:11:57.340]   And say, "Well, because they're getting 90% of the combat pay."
[00:11:57.340 --> 00:12:01.080]   Broadly construed, it's not mostly actual for combat.
[00:12:01.080 --> 00:12:07.320]   So, anyway, that's one where you should be careful, but at least I can see the possibility there.
[00:12:07.320 --> 00:12:11.160]   I do have a section in the book where I go over what's happening over time.
[00:12:11.160 --> 00:12:18.120]   So, what I'll say is, one big thing that's happened over time is that people have become very hyper-concerned with the mistreatment of women,
[00:12:18.120 --> 00:12:21.760]   which means that feminism is becoming less true as a result.
[00:12:21.760 --> 00:12:27.160]   Because when people are really hyper-concerned that they might be unfair to someone, they are even less likely to be unfair to them.
[00:12:27.160 --> 00:12:32.280]   So, I think that's one thing where feminism has become less true over time.
[00:12:32.280 --> 00:12:41.960]   Another one that I talk about and which I think really does tip the scales, although, again, you really need to go through the book because I do try to work through a lot of different margins.
[00:12:41.960 --> 00:12:52.280]   I think the one that really does settle it against feminism in today's age is precisely the level of false feminist accusations about unfairness.
[00:12:52.280 --> 00:13:02.600]   So, the way that I put it is, when we go over all the objective measures, then you say, "Well, it's close to a wash in terms of which gender is treated more or less fairly overall."
[00:13:02.600 --> 00:13:14.200]   But then you realize, "Yes, but there's one gender that has to endure a whole lot of grossly exaggerated hyperbolic accusations on unfairness, and another gender that gets to make those accusations."
[00:13:14.200 --> 00:13:19.640]   And the gender that has to endure the unfair accusations is men, and the gender that gets to make them is women.
[00:13:19.640 --> 00:13:35.000]   Obviously, not all women make them, not all men receive them, but still, if we're talking about the average fairness of the treatment of men and women in our society, I say that this climate of false accusation and intimidation is what really tips it.
[00:13:35.000 --> 00:13:47.560]   It didn't have to be this way, Dorkash. We could have just had conditions change without a whole lot of flinging of wildly inaccurate accusations, but that's not the world we're in.
[00:13:47.560 --> 00:13:53.080]   Where would you say it was the flipping point? Was there a particular decade that you thought unbalanced things are equal now?
[00:13:53.080 --> 00:14:03.320]   Hmm. Yeah. One of the things I say in the book is that while there's a bunch of ways where you can say that women were treated less fairly in earlier decades,
[00:14:03.320 --> 00:14:08.600]   but I also say there's some that are probably more important overall where women are treated worse.
[00:14:08.600 --> 00:14:25.240]   The main one is paternal support for children, where in 1940, the odds that you could count on the biological father of your children to help you to raise them was maybe 90%, and now it's probably more like 60%, 70%.
[00:14:25.240 --> 00:14:35.800]   That's one of the main ways that I say that women probably are treated less fairly than men, and the unfairness has gotten worse over time.
[00:14:35.800 --> 00:14:49.320]   Again, just to understand, this is not the kind of book that most people are used to where someone argues like a lawyer and they just say, "Look, I've got 20 arguments where I'm right, and everyone who disagrees with me is stupid and doesn't have a leg to stand on."
[00:14:49.320 --> 00:14:59.320]   This is the kind of book that I like to write where I really say, "Let's just calm down and just go through every issue separately, weigh each one on its merits."
[00:14:59.320 --> 00:15:05.320]   There's a bunch of points where someone could say, "Why do you concede that? That makes your argument weaker." I say, "Yeah, I concede it because it's true."
[00:15:05.320 --> 00:15:24.120]   Then at the end, I have my overall judgment. I will just say that the number of books that are written in this terrible modern style of lawyerly reasoning where you basically have a thesis that you just try to defend in every possible way, I don't write books like that.
[00:15:24.120 --> 00:15:33.160]   I try to write books where they are honest and self-reflective and where if there's some weakness in what I'm saying, I don't just acknowledge it if someone points it out.
[00:15:33.160 --> 00:15:40.440]   I try to be the first person to reveal it so that people feel like they can trust me. It's my own conscience.
[00:15:40.440 --> 00:15:49.480]   I don't feel right when I say something if I say, "Well, it's not really quite right. I should say the other thing," so I try to just have to write with candor.
[00:15:49.480 --> 00:15:59.080]   Now, would you say the feminism maybe in the United States is overcorrected, but globally, it's still true in the sense that on average across the world, women are treated more unfairly than men?
[00:15:59.080 --> 00:16:08.040]   If that's the case, then if the US is at the center of global feminism, then of course, they're going to overcorrect here, but overall, they're making the world a better place.
[00:16:08.040 --> 00:16:22.440]   That is a much better argument. I would say that if we think about most of Europe, areas like that, then I think that it's very similar to what's going on in the US.
[00:16:22.440 --> 00:16:31.240]   In the book, I do go over – I start with Saudi Arabia where it's really obvious what's going on and how poorly women are treated in Saudi Arabia.
[00:16:31.240 --> 00:16:38.920]   But then I go over to India and China and just think about plausible rates of female infanticide.
[00:16:38.920 --> 00:16:49.960]   Those are ones where I think it is very likely that overall the treatment of women in India and China is more unfair than that of men.
[00:16:49.960 --> 00:17:03.960]   Saudi Arabia, I'm almost sure that it is. In terms of is the US providing a useful corrective for the world while messing up things in the US, it's possible.
[00:17:03.960 --> 00:17:11.240]   Again, I think the problem is that it does discredit a lot of the reasonable points because they just don't focus on the really big issues.
[00:17:11.240 --> 00:17:18.760]   The amount of time that American feminists spend on female infanticide in China and India, could it even be 1% of the rhetoric?
[00:17:18.760 --> 00:17:37.080]   It's just not something that they care about. I would say that there's more harm being done just by the sheer distraction of putting so much emphasis upon either small or exaggerated or just reverse problems that bother feminists in the first world while ignoring.
[00:17:37.080 --> 00:17:45.400]   Not just ignoring, but indirectly causing people to forget or neglect actual serious problems in some other countries.
[00:17:45.400 --> 00:17:50.840]   But let me apply the argument you make in Open Borders that you can effect change by shifting the Overton window, right?
[00:17:50.840 --> 00:17:57.400]   So advocating for Open Borders, it just shifts immigration policy slightly towards the open end.
[00:17:57.400 --> 00:18:03.800]   Couldn't American feminists make the same point that by making the crazy arguments they make in America, they're making Saudi Arabia more liberal for women?
[00:18:03.800 --> 00:18:08.760]   I would say that when the arguments are crazy, then it's not clear that shifting the Overton window actually happens.
[00:18:08.760 --> 00:18:12.200]   That may be where you discredit the other view.
[00:18:12.200 --> 00:18:19.960]   In particular, I think what I say in that part of the book is that people generally confuse being radical with being unfriendly.
[00:18:19.960 --> 00:18:25.960]   And most of the harm that is done to radical causes is due to the unfriendliness rather than the radicalism.
[00:18:25.960 --> 00:18:30.680]   So in that case, I would say, yeah, well, feminism has a definite friendliness problem.
[00:18:30.680 --> 00:18:42.600]   It is not a movement that goes out of its way to go and make other people feel like they're respected and even if you disagree with me, I want to be your friend and listen to what you have to say and maybe we could go and come to some understanding.
[00:18:42.600 --> 00:18:51.400]   I think it is a movement where the main emotional tenor of the elites is we are totally right and anyone who disagrees had better watch out.
[00:18:51.400 --> 00:18:54.760]   So I think that there is a discrediting of it.
[00:18:54.760 --> 00:19:14.600]   The other thing is just that I think there is too much cultural separation between the feminist movement as we know it and places like China and India where I just don't see that being really angry about exaggerated or false complaints about unfair treatment of women in the United States is going to do anything for infanticide in India.
[00:19:14.600 --> 00:19:21.640]   Correct me if I'm wrong, Dwarkesh, do you see much influence of Western feminism on infanticide in India?
[00:19:21.640 --> 00:19:42.280]   Well, I don't know if infanticide, actually, maybe yes, even on that subject as well, but more generally, like one of the common arguments that say libertarians make about India and its elites is, oh, you know, all of India's elites go to go study in Oxford or something and they learn about the regulations the West is adopting that make no sense for a country with like $2,000 GDP per capita.
[00:19:42.280 --> 00:19:55.240]   I feel like some of the things could be true of feminism where all these American elites go to America, sorry, Indian elites go to American universities and UK universities where they learn about radical feminism and they go back and they adopt some of these things, right?
[00:19:55.240 --> 00:20:05.080]   Yes, although you might remember what Alex Tabarrok says about these very things, which is that you can go to India, have people pushing paper straws on you, and yet the streets are still totally covered in trash.
[00:20:05.080 --> 00:20:11.080]   And the pushing the paper straws probably actually distracts people from the much more serious problem of horrible trash.
[00:20:11.080 --> 00:20:37.320]   And again, I don't know enough about India to speak with any confidence here, but if you go and learn radical feminism in Western universities, come back to India and start complaining about how we need to have more female CEOs in a country where you have millions of female infanticides per year, I think it probably is like the paper straws problem where you are so focused on a trivial problem that maybe is not even a problem at all.
[00:20:37.320 --> 00:20:44.360]   At the same time, you are – that anger blinds you to an actual really serious problem that's going on.
[00:20:44.360 --> 00:20:46.840]   Again, like – but you know India better than me.
[00:20:46.840 --> 00:20:47.560]   I could be wrong.
[00:20:47.560 --> 00:20:52.280]   I believe it's still legal in India or rape within a marriage is still not recognized.
[00:20:52.280 --> 00:20:54.280]   Maybe it was like just recently changed to it.
[00:20:54.280 --> 00:20:54.760]   Yes.
[00:20:54.760 --> 00:20:58.840]   Like let's say a feminist listens to the interview and they say, "Oh, my gosh.
[00:20:58.840 --> 00:21:00.120]   Okay, Brian, maybe you're right.
[00:21:00.120 --> 00:21:02.520]   That society as a whole doesn't mistreat women.
[00:21:02.520 --> 00:21:04.840]   But maybe the cosmos mistreats women."
[00:21:04.840 --> 00:21:08.680]   But all of these things combined make women's lives worse on average than men's lives and
[00:21:08.680 --> 00:21:10.520]   it's not because maybe society mistreats them.
[00:21:10.520 --> 00:21:13.880]   But in some sense, there's still unfairness geared towards women.
[00:21:13.880 --> 00:21:15.320]   What do you make of this argument?
[00:21:15.320 --> 00:21:15.560]   Yes.
[00:21:15.560 --> 00:21:21.000]   So an unfairness where there's no human being that does it, that seems like a very strange
[00:21:21.000 --> 00:21:23.400]   idea to me just at the get-go.
[00:21:23.400 --> 00:21:25.400]   It's like, "Well, so who is unfair to you?
[00:21:25.400 --> 00:21:26.360]   The universe is unfair."
[00:21:26.360 --> 00:21:31.320]   It's like, "Well, I think the correct term there is unfortunate, not unfair."
[00:21:31.320 --> 00:21:36.200]   Although that aside, I would say it's a really interesting question.
[00:21:36.200 --> 00:21:41.720]   Who actually has better lives just as a matter of biological endowments, men or women?
[00:21:41.720 --> 00:21:47.640]   In terms of demonstrative preference, I think the overwhelming result is that most people
[00:21:47.640 --> 00:21:49.720]   just want to remain in whatever gender they're born in.
[00:21:49.720 --> 00:21:52.760]   This is not actually transgenderism.
[00:21:52.760 --> 00:21:53.880]   This is like a genie wish.
[00:21:53.880 --> 00:21:57.480]   If you could change your gender just with a wish, costlessly, perfectly.
[00:21:57.480 --> 00:22:00.680]   I think still a very large majority of people just want to stay with whatever they have
[00:22:00.680 --> 00:22:01.880]   because it's part of their identity.
[00:22:01.880 --> 00:22:05.000]   It's some kind of endowment effect, status quo bias, whatever.
[00:22:05.000 --> 00:22:09.880]   But then if you say, "Okay, yeah, fine, you just want to stay whatever you were because
[00:22:09.880 --> 00:22:11.720]   that's your identity."
[00:22:11.720 --> 00:22:15.880]   But still, if you could put that aside, what would you want to be?
[00:22:15.880 --> 00:22:17.400]   It's a tough question.
[00:22:17.400 --> 00:22:24.520]   So you can say, "Well, women have a harder personality because of higher neuroticism.
[00:22:24.520 --> 00:22:29.480]   They've also got higher agreeableness, so that gives them some other advantages in terms
[00:22:29.480 --> 00:22:30.760]   of getting along with other people.
[00:22:30.760 --> 00:22:36.120]   For example, men's disagreeableness makes it hard for men to just bite their tongues
[00:22:36.120 --> 00:22:38.360]   and shut up when someone's saying something they don't like.
[00:22:38.360 --> 00:22:39.800]   I think that is easier for women to do.
[00:22:39.800 --> 00:22:44.360]   And as you may have noticed, having to shut up and bite your tongue while someone around
[00:22:44.360 --> 00:22:47.080]   you says something stupid you don't like is actually a big part of life.
[00:22:47.080 --> 00:22:49.560]   So that's one thing.
[00:22:49.560 --> 00:22:53.320]   Now, in terms of things that I feel that I would get out of being a woman, just being
[00:22:53.320 --> 00:22:56.040]   able to have as many kids as I wanted, that would matter a lot to me.
[00:22:56.760 --> 00:22:58.680]   So I only have four kids right now.
[00:22:58.680 --> 00:23:01.400]   If it were totally up to me, yeah, I would have had more kids.
[00:23:01.400 --> 00:23:03.640]   And I think as a woman, it would have been easy to do.
[00:23:03.640 --> 00:23:08.600]   So again, there is the issue of how are you going to find a guy that wants to have a lot
[00:23:08.600 --> 00:23:09.080]   of kids.
[00:23:09.080 --> 00:23:15.640]   This is one where I've looked at the data on family size and what determines it.
[00:23:15.640 --> 00:23:22.040]   And it seems like while both men and women in the data seem to have a say on family size,
[00:23:22.040 --> 00:23:25.400]   but it just looks like women's traits have much larger effects.
[00:23:25.400 --> 00:23:27.400]   Men are more likely to just say, "Okay, fine.
[00:23:27.400 --> 00:23:27.960]   Whatever.
[00:23:27.960 --> 00:23:30.200]   We'll do what you want to do on family size."
[00:23:30.200 --> 00:23:35.240]   Whereas women seem to have much more pronounced preferences, which they then tend to get.
[00:23:35.240 --> 00:23:39.480]   So yeah, I think that if I were a woman, I could have had more kids and it would have
[00:23:39.480 --> 00:23:41.400]   been easier for me to do it.
[00:23:41.400 --> 00:23:43.640]   So that would be something that matters to me.
[00:23:43.640 --> 00:23:48.440]   It's not something that matters to everybody, but that's something there.
[00:23:48.440 --> 00:23:52.680]   Again, there is just the nice fact of people caring about your suffering.
[00:23:53.480 --> 00:23:58.920]   In the book, I do talk about the ethos of women and children first, which is very pronounced.
[00:23:58.920 --> 00:24:00.840]   It's not just modern society.
[00:24:00.840 --> 00:24:06.360]   But it is in modern society that we can simultaneously have women and children first, but also have
[00:24:06.360 --> 00:24:08.440]   a lot of rhetoric about how people don't care about women.
[00:24:08.440 --> 00:24:10.840]   And it's like, "Hmm, that's not..."
[00:24:10.840 --> 00:24:12.520]   What do you think of this theory?
[00:24:12.520 --> 00:24:17.160]   That maybe society cares a lot more about women's suffering, but it sympathizes a lot
[00:24:17.160 --> 00:24:18.280]   more with men's success.
[00:24:18.280 --> 00:24:24.280]   So if you think of a default character in a movie or novel, at least for me, the default
[00:24:24.280 --> 00:24:25.080]   is a man.
[00:24:25.080 --> 00:24:27.880]   And then maybe there's some victim, the default is a woman.
[00:24:27.880 --> 00:24:34.440]   But I would rather be the sympathy of some sort of success than get empathy for suffering.
[00:24:34.440 --> 00:24:34.840]   That's interesting.
[00:24:34.840 --> 00:24:36.760]   Do you need sympathy for success?
[00:24:36.760 --> 00:24:38.600]   Or not sympathy, but just like...
[00:24:38.600 --> 00:24:39.160]   Admiration.
[00:24:39.160 --> 00:24:39.960]   Admiration, yeah.
[00:24:39.960 --> 00:24:40.600]   Yeah.
[00:24:40.600 --> 00:24:45.400]   I mean, I guess what I would say is that everybody's got suffering and only a small
[00:24:45.400 --> 00:24:47.400]   share of people have any notable success.
[00:24:48.120 --> 00:24:51.800]   So if all that you knew was you're going to be a man or woman, I would say, "Well,
[00:24:51.800 --> 00:24:55.800]   gee, if I'm a woman, then people will sympathize with my suffering," which is almost definitely
[00:24:55.800 --> 00:24:57.480]   coming because that's the human condition.
[00:24:57.480 --> 00:25:03.560]   Whereas to have admiration for your success, that's something where it just affects a much
[00:25:03.560 --> 00:25:04.520]   smaller number of people.
[00:25:04.520 --> 00:25:10.040]   I know that hanging out in Austin among hyper successful people, this may be biasing your
[00:25:10.040 --> 00:25:10.680]   sample a bit.
[00:25:12.280 --> 00:25:18.200]   But yeah, I do think it's believable that men get more unmitigated admiration for their
[00:25:18.200 --> 00:25:18.840]   success.
[00:25:18.840 --> 00:25:24.600]   And of course, there's also differences in the mating opportunities that you get for
[00:25:24.600 --> 00:25:26.760]   being a successful man versus a successful woman.
[00:25:26.760 --> 00:25:28.200]   So that is there too.
[00:25:28.200 --> 00:25:32.120]   But again, this is something that really is only relevant for a very small share of the
[00:25:32.120 --> 00:25:32.760]   population.
[00:25:32.760 --> 00:25:34.440]   Right.
[00:25:34.440 --> 00:25:37.480]   But then the argument is, "Well, but that small share of the population matters so much
[00:25:37.480 --> 00:25:41.880]   in terms of the story we tell ourselves about our civilization, or just in terms of who
[00:25:41.880 --> 00:25:44.680]   controls more resources overall, right?"
[00:25:44.680 --> 00:25:50.760]   So if being a woman billionaire is harder, maybe for biological reasons, maybe for the
[00:25:50.760 --> 00:25:53.480]   reasons of our society, you could say, "Well, that only affects a small percentage of women
[00:25:53.480 --> 00:25:54.440]   in our society."
[00:25:54.440 --> 00:25:56.200]   But on the other hand, billionaires matter a lot.
[00:25:56.200 --> 00:25:57.720]   Yeah.
[00:25:57.720 --> 00:26:03.320]   In terms of what life is like for most people, the main way they matter is billionaires just
[00:26:03.320 --> 00:26:04.200]   provide awesome stuff.
[00:26:05.480 --> 00:26:12.440]   In terms of the stories that people tell, it's true that if you just go and look at
[00:26:12.440 --> 00:26:17.080]   most classic movies or novels, the main characters are male.
[00:26:17.080 --> 00:26:20.920]   Even in cartoons, actually, the main characters traditionally have been male.
[00:26:20.920 --> 00:26:24.440]   But on the other hand, that's fiction.
[00:26:24.440 --> 00:26:29.880]   In terms of daily life, I'd rather have people be really concerned about me in real life,
[00:26:29.880 --> 00:26:33.720]   but have my perspective underrepresented in stories than the other way around.
[00:26:34.840 --> 00:26:41.000]   So what do you make the argument that employers hold defects in women's personalities much
[00:26:41.000 --> 00:26:44.520]   more against them than they hold defects in men's personalities?
[00:26:44.520 --> 00:26:50.360]   So I think Tyler cited some of this research in his new book on talent, that being too
[00:26:50.360 --> 00:26:53.960]   agreeable or being too aggressive, that harms women more than it harms men.
[00:26:53.960 --> 00:26:57.560]   Again, I would say that it's complicated.
[00:26:57.560 --> 00:27:00.520]   In terms of willingness to fire, I think employers are much more willing to fire men.
[00:27:02.200 --> 00:27:07.480]   For defects, for insubordination, just on the list.
[00:27:07.480 --> 00:27:11.720]   This is a small one, but I think that it is indicative of a broader trend.
[00:27:11.720 --> 00:27:17.240]   So people working at workplaces with dress codes, men are much more likely to be dinged
[00:27:17.240 --> 00:27:21.080]   on dress code violations than women, because for men, there's a definite thing men are
[00:27:21.080 --> 00:27:21.480]   supposed to do.
[00:27:21.480 --> 00:27:24.360]   If you're not doing it, you are in violation.
[00:27:24.360 --> 00:27:27.720]   Women, on the other hand, it's like, "Well, gee, it seems kind of like that's not what
[00:27:27.720 --> 00:27:30.520]   you should be wearing, but I don't want to be the person that says anything about it.
[00:27:31.160 --> 00:27:31.640]   Who knows?
[00:27:31.640 --> 00:27:34.840]   Who am I to judge what a woman ought to be wearing on the job?"
[00:27:34.840 --> 00:27:39.240]   But a man, on the other hand, needs to be wearing a suit in 110-degree weather.
[00:27:39.240 --> 00:27:42.120]   Let's see, what was the high this summer over in Austin?
[00:27:42.120 --> 00:27:49.400]   Why do you think that women have gotten less happy since the '60s?
[00:27:49.400 --> 00:27:50.360]   In the America, at least.
[00:27:50.360 --> 00:27:50.520]   Right.
[00:27:50.520 --> 00:27:55.320]   The main thing I know about this is the Stevenson and Wolfer's research on this.
[00:27:55.320 --> 00:27:58.120]   The main thing to remember is the magnitude.
[00:27:58.120 --> 00:28:02.040]   So, if I remember correctly, I think they find that in the '60s, women had about a
[00:28:02.040 --> 00:28:06.200]   two-percentage-point advantage relative to men in terms of their odds of saying they're
[00:28:06.200 --> 00:28:06.840]   very happy.
[00:28:06.840 --> 00:28:14.440]   So, if something like 25% of men said they were very happy, then 27% of women in the
[00:28:14.440 --> 00:28:15.960]   '60s said that they were very happy.
[00:28:15.960 --> 00:28:20.840]   Whereas now, it seems like women have something like a two-percentage-point deficit relative
[00:28:20.840 --> 00:28:21.240]   to men.
[00:28:21.240 --> 00:28:27.640]   So now, if 25% of men say they're very happy, then 23% of women say they're very happy.
[00:28:27.640 --> 00:28:31.480]   It's always important in these papers to look at those magnitudes because the media
[00:28:31.480 --> 00:28:34.520]   coverage is just going to say, "Oh, women are miserable now."
[00:28:34.520 --> 00:28:37.240]   And it's like, "It's not that women are miserable now.
[00:28:37.240 --> 00:28:40.360]   We're talking a two-percentage-point difference."
[00:28:40.360 --> 00:28:45.320]   It's a data set large enough for this to actually be meaningful, but we do want to
[00:28:45.320 --> 00:28:46.760]   keep it in perspective.
[00:28:46.760 --> 00:28:52.120]   In terms of what's really going on, this is one where the paper probably actually goes
[00:28:52.120 --> 00:28:54.600]   over a bunch of stories and says the obvious ones are all wrong.
[00:28:54.600 --> 00:28:59.000]   That would be what especially Justin Wolfers would normally do, and I think he's usually
[00:28:59.000 --> 00:29:01.160]   right that simple stories about something like this are wrong.
[00:29:01.160 --> 00:29:07.000]   In terms of what I would pursue if I read through the paper and rinded myself of what
[00:29:07.000 --> 00:29:09.880]   they found and then said, "Okay, well, what will work?"
[00:29:09.880 --> 00:29:17.480]   I think I would, on one end, focus on single moms who have become much more common and
[00:29:17.480 --> 00:29:18.520]   their lives really are hard.
[00:29:18.520 --> 00:29:20.840]   So, rise of single motherhood.
[00:29:21.480 --> 00:29:24.600]   I would guess that is one important part of it.
[00:29:24.600 --> 00:29:30.760]   Then I would also be wondering how much of it is actual feminism telling women that you
[00:29:30.760 --> 00:29:33.960]   should be unhappy because the world is unfair and then causes unhappiness.
[00:29:33.960 --> 00:29:35.560]   Again, I'm not saying that these are right.
[00:29:35.560 --> 00:29:36.120]   I don't know.
[00:29:36.120 --> 00:29:38.280]   It's plausible to me.
[00:29:38.280 --> 00:29:43.640]   The main thing I would say about feminism causing unhappiness in the adherents is that
[00:29:43.640 --> 00:29:47.240]   it probably doesn't matter most for most self-identified feminists because most people
[00:29:47.240 --> 00:29:50.760]   just are not that intellectual and they don't think about their ideas very often.
[00:29:51.560 --> 00:29:51.880]   All right.
[00:29:51.880 --> 00:29:55.800]   So, it's one thing to say, "Look, if you believe you're going to hell, that you'll
[00:29:55.800 --> 00:29:56.840]   be unhappy."
[00:29:56.840 --> 00:30:00.440]   It's like, "Well, if you believe it once a year, does it make you unhappy?"
[00:30:00.440 --> 00:30:02.840]   If you remember, "Oh, yeah, once a year, I think I'm going to hell."
[00:30:02.840 --> 00:30:04.360]   The rest of the time, you don't think it.
[00:30:04.360 --> 00:30:07.320]   On the other hand, the person that is all the time thinking, "I'm going to hell.
[00:30:07.320 --> 00:30:07.880]   I'm going to hell."
[00:30:07.880 --> 00:30:09.800]   That person probably will be unhappy.
[00:30:09.800 --> 00:30:15.000]   So, I think that feminism is very likely to reduce the happiness of people who are feminist
[00:30:15.000 --> 00:30:18.280]   elites who take it really seriously, where they're talking about it all the time.
[00:30:18.280 --> 00:30:20.200]   That is likely to cause unhappiness.
[00:30:20.200 --> 00:30:21.640]   I'd be amazed if it didn't.
[00:30:21.640 --> 00:30:24.520]   But on the other hand, for the vast majority of people who say, "Yeah, I'm a feminist.
[00:30:24.520 --> 00:30:25.480]   Moving on."
[00:30:25.480 --> 00:30:27.880]   I don't think it's too likely to be messing up their lives.
[00:30:27.880 --> 00:30:28.280]   Yeah.
[00:30:28.280 --> 00:30:32.360]   That raises the interesting possibility, and this is not my theory, but let's run with
[00:30:32.360 --> 00:30:32.920]   this.
[00:30:32.920 --> 00:30:37.240]   So, feminism has actually gotten more true over time, but it's precisely because of
[00:30:37.240 --> 00:30:38.040]   feminism.
[00:30:38.040 --> 00:30:40.280]   So, maybe it's made elite women more unhappy.
[00:30:40.280 --> 00:30:43.640]   As you said earlier, the amount of single mothers has gone up.
[00:30:43.640 --> 00:30:49.080]   Maybe part of that is because of feminist trends in terms of family formation.
[00:30:49.080 --> 00:30:53.640]   And maybe women prefer to be at home caring for children on average more, but then feminism
[00:30:53.640 --> 00:30:56.200]   encourages them to have careers, which makes them less unhappy.
[00:30:56.200 --> 00:31:00.200]   So, if you add all these things up, and again, mentorship, men are less likely to give it
[00:31:00.200 --> 00:31:01.160]   because of Me Too.
[00:31:01.160 --> 00:31:04.200]   So, add all these things up, maybe they're the result of feminism, but they still make
[00:31:04.200 --> 00:31:05.720]   feminism more right.
[00:31:05.720 --> 00:31:06.440]   Would you agree with that?
[00:31:06.440 --> 00:31:07.880]   Yeah.
[00:31:07.880 --> 00:31:11.720]   If we go back to this definition of feminism, this theory that our society treats women
[00:31:11.720 --> 00:31:18.600]   less fairly than men, if the story is that women have made a lot of false accusations
[00:31:18.600 --> 00:31:22.840]   against men, and then men have responded by changing their behavior, that would seem to
[00:31:22.840 --> 00:31:26.280]   be a strange example of saying that society is treating women less fairly than men.
[00:31:26.280 --> 00:31:33.080]   It would seem to be a case of society is treating men unfairly, and this is having some negative
[00:31:33.080 --> 00:31:34.680]   side effects for women as well.
[00:31:34.680 --> 00:31:42.040]   But it's one where if you really were trying to draw the line and say, "Well, here's
[00:31:42.040 --> 00:31:46.040]   actually one of the weaknesses of the definition that I proposed."
[00:31:46.680 --> 00:31:47.800]   So, foot binding in China.
[00:31:47.800 --> 00:31:53.320]   From my understanding, the main drivers of foot binding in China were women.
[00:31:53.320 --> 00:31:57.000]   So, women are binding feet, and they're also telling their daughters they have to have
[00:31:57.000 --> 00:31:57.720]   their feet bound.
[00:31:57.720 --> 00:32:01.720]   Men seem to care less, actually.
[00:32:01.720 --> 00:32:04.680]   It was more of an intra-female abuse.
[00:32:04.680 --> 00:32:09.640]   This is one where you could say, "Well, but still you might say that in China women are
[00:32:09.640 --> 00:32:12.680]   treated less fairly than men, even though the perpetrators are women."
[00:32:13.960 --> 00:32:17.880]   I think that does actually make sense, and I would just say that the definition that
[00:32:17.880 --> 00:32:21.960]   we use in our society isn't really calibrated to deal with that kind of thing, and in terms
[00:32:21.960 --> 00:32:25.640]   of what the right way to describe it would be is just a bit confusing.
[00:32:25.640 --> 00:32:30.600]   So, it's one where it's useful just to say, "All right, well, if women are mistreating
[00:32:30.600 --> 00:32:34.280]   women and that's what's making women's lives hard, how do we count that?"
[00:32:34.280 --> 00:32:39.000]   And I think I would just say we don't have any really good way of counting it, and it
[00:32:39.000 --> 00:32:42.280]   might be useful to just come up with a new word to describe this kind of thing.
[00:32:42.280 --> 00:32:45.960]   What do you make of Hanania's argument that women's tears win in the marketplace
[00:32:45.960 --> 00:32:46.460]   of ideas?
[00:32:46.460 --> 00:32:51.480]   We might want to back up a little bit and explain what the argument is.
[00:32:51.480 --> 00:33:01.080]   Richard Hanania on his Substack has a very famous essay where he points out that in fiction,
[00:33:01.080 --> 00:33:07.160]   when there is a mob of angry college students, it's very demographically diverse.
[00:33:07.160 --> 00:33:11.560]   But when you look at actual footage, it seems like women are highly overrepresented.
[00:33:12.520 --> 00:33:17.960]   And then he generalizes this to saying a lot of what's going on in terms of cancel culture
[00:33:17.960 --> 00:33:26.040]   and related problems is that women are the main ones that get angry about these things,
[00:33:26.040 --> 00:33:28.600]   and people don't know what to do about it.
[00:33:28.600 --> 00:33:36.760]   So, if I remember correctly, he just says, "A man can, in a way, actually enjoy an argument
[00:33:36.760 --> 00:33:43.160]   with another man, and even if you lose or even if it's a physical fight, you can sort of feel
[00:33:43.160 --> 00:33:47.160]   invigorated by it. We got through this. We resolved something."
[00:33:47.160 --> 00:33:50.120]   He says, "Whereas no guy feels this way about an argument with his wife."
[00:33:50.120 --> 00:33:55.880]   What do I need to do in order for this argument to end as soon as possible
[00:33:55.880 --> 00:33:57.320]   would be a more normal reaction.
[00:33:57.320 --> 00:34:05.000]   This sort of generalizes to a lot of not all social arguments, but ones basically involving
[00:34:05.000 --> 00:34:09.800]   someone being offended or angry or hurt, where he says a lot of what's going on is that it is
[00:34:09.800 --> 00:34:15.640]   mainly women that are presenting these complaints, and that it's hard to deal with it because men
[00:34:15.640 --> 00:34:20.760]   don't want to argue with angry women. It just makes them feel bad, and it's sort of a no-win
[00:34:20.760 --> 00:34:21.400]   situation.
[00:34:21.400 --> 00:34:27.320]   So, anyway, that is Hanenia's argument. Overall, it seemed pretty plausible to me. I haven't
[00:34:27.320 --> 00:34:32.920]   thought about it that much more, but it's one that does seem to make a fair bit of sense in
[00:34:32.920 --> 00:34:35.080]   terms of just what I'm writing about feminism.
[00:34:35.080 --> 00:34:40.840]   One really striking thing is just how one-sided this conversation is. It's a conversation where
[00:34:40.840 --> 00:34:46.280]   women have complaints and men mostly just listen in silence. Or, of course, men will
[00:34:46.280 --> 00:34:50.520]   sometimes complain amongst each other when women aren't around. It's not a real dialogue
[00:34:50.520 --> 00:34:55.320]   where women have complaints about men, and then men are very eager to say, "Oh, but I
[00:34:55.320 --> 00:35:02.360]   have something I would like to say in rebuttal to that." A lot of it is what he calls women's
[00:35:02.360 --> 00:35:10.280]   tears, or you could also just think about its sadness but mingled with or supported
[00:35:10.280 --> 00:35:15.560]   by intimidation of, "If you don't give me what I want, if you don't pretend that you
[00:35:15.560 --> 00:35:20.280]   agree with me, I will be very angry, and I will be fairly sad, and so you should be afraid."
[00:35:20.280 --> 00:35:25.400]   That is, I think, a lot of what's probably going on with the rhetorical dominance of
[00:35:25.400 --> 00:35:33.080]   feminism, which is that people are just afraid to argue against it because, in a way, it
[00:35:33.080 --> 00:35:37.720]   does sort of violate the women and children first ethos, which is if women complain about
[00:35:37.720 --> 00:35:42.600]   something, you aren't supposed to go and say, "I disagree. Your complaints are unjustified."
[00:35:42.600 --> 00:35:45.000]   You're supposed to say, "What can I do to make it better?"
[00:35:45.000 --> 00:35:48.920]   But that seems like a good description of race issues and class issues as well. I don't
[00:35:48.920 --> 00:35:51.880]   know if it's applicable to feminism.
[00:35:51.880 --> 00:35:57.320]   Sort of the main difference there is that people have a lot more first-hand experience
[00:35:57.320 --> 00:36:01.000]   of inter-gender relations, and they spend a lot more time in inter-gender relations
[00:36:01.000 --> 00:36:06.360]   than they spend in all the other ones. The dynamic is probably pretty similar, but in
[00:36:06.360 --> 00:36:12.440]   terms of the really negative first-hand experience that men have, probably Hanenia is right about
[00:36:12.440 --> 00:36:15.640]   that, and that generalizes to bigger issues.
[00:36:15.640 --> 00:36:19.320]   You have an essay about endogenous sexism.
[00:36:19.320 --> 00:36:19.800]   Yeah.
[00:36:19.800 --> 00:36:26.360]   Could this just not be the cause of a society being unfair to a woman, that we start off
[00:36:26.360 --> 00:36:30.360]   with men being in power, they get sexist just because they're around other men and they
[00:36:30.360 --> 00:36:36.680]   like them more? Then the starting position matters a lot, even if men aren't trying
[00:36:36.680 --> 00:36:37.240]   to be sexist.
[00:36:37.240 --> 00:36:45.480]   Let me just back up and explain the argument. The argument says this. Imagine that, in reality,
[00:36:45.480 --> 00:36:51.080]   men and women are equally good in absolutely every way, but people are more likely to have
[00:36:51.080 --> 00:36:53.720]   close friends of their own gender, which is totally true.
[00:36:53.720 --> 00:37:01.880]   If I remember the essay, I think that for men, close friends, the male/female ratio
[00:37:01.880 --> 00:37:07.160]   is like 4 to 1. I think it's the other way around. For men, it's like 6 to 1. For women,
[00:37:07.160 --> 00:37:12.920]   it's like 4 to 1. Most people's close friends are of the same gender. You meet these people,
[00:37:12.920 --> 00:37:16.200]   and they're your close friends. You know them really well, and because you have hand-picked
[00:37:16.200 --> 00:37:18.040]   them, you're going to think well of them, right?
[00:37:18.040 --> 00:37:23.880]   Then the question is, what about people of the opposite gender, and what's your interaction
[00:37:23.880 --> 00:37:28.920]   with them going to be like? What I point out is that a lot of the people of the opposite
[00:37:28.920 --> 00:37:32.680]   gender that you hang out with will be the spouses of your friends, the partners of your
[00:37:32.680 --> 00:37:36.440]   friends. On average, you're going to think worse of them because you didn't pick them.
[00:37:36.440 --> 00:37:41.560]   Basically, there's two filters. I like you because you're my friend, and now I put up
[00:37:41.560 --> 00:37:44.840]   with your partner because that person is your partner.
[00:37:44.840 --> 00:37:51.800]   This means that the women that men are around are going to be the partners of their friends.
[00:37:51.800 --> 00:37:54.680]   They're going to like them less and think less of them than they think of their friends.
[00:37:54.680 --> 00:37:59.160]   On the other hand, the partners of women's friends will be men, and women will get to
[00:37:59.160 --> 00:38:02.360]   know them and say, "Wow, they're not that great. They're at least kind of disappointing
[00:38:02.360 --> 00:38:03.560]   relative to my female friends."
[00:38:03.560 --> 00:38:10.280]   This is an argument about how the illusion of your own gender being superior could arise.
[00:38:11.080 --> 00:38:17.080]   Now, as to whether this is actually the right story, I leave that open. This was just more
[00:38:17.080 --> 00:38:20.680]   of a thought experiment to understand what could happen here.
[00:38:20.680 --> 00:38:28.920]   Could this actually explain the unfair treatment of women in society, or could it lead in this
[00:38:28.920 --> 00:38:33.880]   way? Especially if you start off with men being the gatekeepers for most of the business
[00:38:33.880 --> 00:38:34.680]   world, for example.
[00:38:34.680 --> 00:38:38.920]   The answer is it's totally plausible that it could. That's why we really want to go
[00:38:38.920 --> 00:38:41.400]   to the data and see what we actually find.
[00:38:41.400 --> 00:38:48.040]   In the data that I know of, actually, the evidence of women earning less money than
[00:38:48.040 --> 00:38:52.280]   men while actually doing the same job is actually quite low.
[00:38:52.280 --> 00:38:57.400]   There's very little gender disparity in earnings once you make the obvious statistical adjustments
[00:38:57.400 --> 00:38:59.400]   for being in the same occupation.
[00:38:59.400 --> 00:39:07.320]   The main one that probably is – this one actually has gotten worse for women is mentoring.
[00:39:08.360 --> 00:39:10.200]   Mentoring partly is based upon friendship.
[00:39:10.200 --> 00:39:13.800]   "I like this person. I like working with them, so I will go and help them to go and
[00:39:13.800 --> 00:39:15.400]   acquire more human capital on the job."
[00:39:15.400 --> 00:39:23.240]   This is one that feminism has visibly messed up, and many feminists will, in a strange
[00:39:23.240 --> 00:39:27.560]   way, admit that they have done it while not taking responsibility for the harm.
[00:39:27.560 --> 00:39:32.040]   I've got an essay on that in the book as well.
[00:39:32.040 --> 00:39:36.360]   This is one where, first of all, just looking at the evidence, it is totally standard now
[00:39:36.360 --> 00:39:40.680]   for male managers just to admit that they are reluctant to mentor female employees because
[00:39:40.680 --> 00:39:41.400]   they're so worried.
[00:39:41.400 --> 00:39:45.800]   Then I go and track down a bunch of feminist reaction to this where they basically just
[00:39:45.800 --> 00:39:47.880]   say, "I can't believe how horrible these guys are."
[00:39:47.880 --> 00:39:52.200]   It's like, "Look, you're asking them for a favor to get mentorship.
[00:39:52.200 --> 00:39:53.240]   They're scared.
[00:39:53.240 --> 00:39:59.000]   If someone is scared, do you really want to yell at them more and offer more mostly empty
[00:39:59.000 --> 00:40:03.480]   threats because it's really hard in something that is this informal to actually scare someone
[00:40:03.480 --> 00:40:03.960]   into doing it.
[00:40:03.960 --> 00:40:05.320]   You really do need to win them over."
[00:40:06.120 --> 00:40:10.840]   Tactically, that might be correct, but it seems like just as a matter of is their argument
[00:40:10.840 --> 00:40:13.880]   justified, I can see why they'd be frustrated.
[00:40:13.880 --> 00:40:20.440]   Obviously, you want to point out when there's a sexual harassment allegation, and that may
[00:40:20.440 --> 00:40:21.880]   have the effect of less mentorship.
[00:40:21.880 --> 00:40:27.080]   Is it obvious that you want to point that out?
[00:40:27.080 --> 00:40:32.040]   Part of what I say is that there are different perceptions here.
[00:40:32.040 --> 00:40:33.240]   There are differences of opinion.
[00:40:34.120 --> 00:40:38.440]   If you want to get along with people, a lot of it is saying, "How does it seem from the
[00:40:38.440 --> 00:40:39.560]   other person's point of view?"
[00:40:39.560 --> 00:40:44.600]   Most obviously, do not assume that the most hypersensitive person is correct.
[00:40:44.600 --> 00:40:51.160]   That is, I think, what so much of the problem with mentorship comes down to is hypersensitivity.
[00:40:51.160 --> 00:40:57.960]   I've got another piece in the book where I talk about misunderstandings and how we have
[00:40:57.960 --> 00:41:00.840]   so much lost sight of this very possibility.
[00:41:01.400 --> 00:41:02.920]   There's a conflict between two people.
[00:41:02.920 --> 00:41:04.120]   Who's right and who's wrong?
[00:41:04.120 --> 00:41:09.880]   Well, it could, of course, be that one person is the conscious malefactor going, "Bwahaha,"
[00:41:09.880 --> 00:41:13.080]   and the other person is an obvious victim that no one could deny.
[00:41:13.080 --> 00:41:14.360]   That does happen sometimes.
[00:41:14.360 --> 00:41:18.440]   But I say much more often in the real world, there's a misunderstanding where each person,
[00:41:18.440 --> 00:41:23.800]   because of the imperfection of the human mind and the inability to go and get inside another
[00:41:23.800 --> 00:41:27.880]   person's head, to each person it seems like they're in the right and the other person
[00:41:27.880 --> 00:41:28.520]   is in the wrong.
[00:41:29.160 --> 00:41:33.640]   One of the most helpful ways for people to get along with each other is to realize that
[00:41:33.640 --> 00:41:34.600]   this is the norm.
[00:41:34.600 --> 00:41:39.800]   Most conflicts are caused by misunderstandings, not by deliberate wrongdoing.
[00:41:39.800 --> 00:41:43.480]   This is the way the people who keep their friends keep their friends.
[00:41:43.480 --> 00:41:47.880]   Anytime there's a conflict with a friend, you assume that you're right and your friend
[00:41:47.880 --> 00:41:51.800]   is in the wrong and demand an immediate abject apology, you're going to be losing friends
[00:41:51.800 --> 00:41:52.440]   left and right.
[00:41:55.880 --> 00:41:58.040]   It is a foolish person who does that.
[00:41:58.040 --> 00:42:02.360]   The friendship is more important than any particular issue.
[00:42:02.360 --> 00:42:07.320]   This is not only my personal view, it is the advice that I give to everyone listening.
[00:42:07.320 --> 00:42:08.760]   Keep your friends.
[00:42:08.760 --> 00:42:10.920]   Bend over backwards in order to keep your friends.
[00:42:10.920 --> 00:42:16.280]   Realize that most conflicts are caused by misunderstandings.
[00:42:16.280 --> 00:42:19.080]   It's not the other person is going out of their way to hurt you.
[00:42:19.080 --> 00:42:20.600]   Probably they just don't see it that way.
[00:42:20.600 --> 00:42:25.640]   If you just insist, "I'm right," I demand a full apology and admission of your wrongdoing.
[00:42:26.040 --> 00:42:28.120]   You're probably going to be losing friends and it's a bad idea.
[00:42:28.120 --> 00:42:32.760]   The same thing I think is going on in workplaces where there is an ideology saying that we
[00:42:32.760 --> 00:42:36.600]   should take the side of the most hypersensitive person and this is not a good approach for
[00:42:36.600 --> 00:42:38.040]   human beings to get along with each other.
[00:42:38.040 --> 00:42:40.120]   That's very nice.
[00:42:40.120 --> 00:42:44.280]   What do you make the argument that a lot of these professions that are dominated by men
[00:42:44.280 --> 00:42:49.800]   are not intrinsically things that must appeal to men, but the way that they are taught or
[00:42:49.800 --> 00:42:53.640]   advertised is very conducive to what Neil's finding interesting.
[00:42:54.120 --> 00:42:55.400]   Take computer science, for example.
[00:42:55.400 --> 00:42:59.720]   There's claims that you could teach that or economics in a way that focuses on the
[00:42:59.720 --> 00:43:04.600]   implications on people from those practices rather than just focusing on the abstractions
[00:43:04.600 --> 00:43:06.200]   or the thing-focused stuff.
[00:43:06.200 --> 00:43:10.280]   The argument is that these things shouldn't be inherently interesting to men.
[00:43:10.280 --> 00:43:11.240]   They're just the way they are taught.
[00:43:11.240 --> 00:43:15.160]   The word "inherently" is so overused.
[00:43:15.160 --> 00:43:20.520]   It's one where you say, "Well, are you saying that inherently X?"
[00:43:20.520 --> 00:43:22.760]   It's like, "Well, not inherently X.
[00:43:22.760 --> 00:43:27.960]   You'd have to bend over backwards, move heaven and earth for it not to be so, but I guess
[00:43:27.960 --> 00:43:29.160]   it's not really inherent."
[00:43:29.160 --> 00:43:33.720]   That is a lot of what is worth pointing out.
[00:43:33.720 --> 00:43:39.400]   If you're going to put the standard to that level, then it's going to be hard to find
[00:43:39.400 --> 00:43:39.960]   differences.
[00:43:39.960 --> 00:43:45.400]   You could say, "There's absolutely no way under the sun to go and teach math in a less
[00:43:45.400 --> 00:43:45.960]   male way."
[00:43:47.000 --> 00:43:50.520]   On the other hand, to say, "Well, is it reasonable to expect the whole world to revolve
[00:43:50.520 --> 00:43:53.560]   around making every subject equally appealing to men and women?"
[00:43:53.560 --> 00:43:54.680]   That's right.
[00:43:54.680 --> 00:43:56.360]   That is just an unreasonable demand.
[00:43:56.360 --> 00:44:03.320]   If there's a subject like math that is male-dominated, the reasonable thing is to say, "Well, if
[00:44:03.320 --> 00:44:07.000]   you want to get in on that, you're going to need to go and become simpatico with the
[00:44:07.000 --> 00:44:10.200]   mindset of the people that are already there and then push the margin."
[00:44:10.200 --> 00:44:15.320]   You can say, "It's just so unfair that male ways of doing math are dominant."
[00:44:15.480 --> 00:44:20.920]   Or maybe you could say that it's unfair for someone who's just shown up to demand that
[00:44:20.920 --> 00:44:25.480]   an entire discipline change its way of doing things to make you feel better about it.
[00:44:25.480 --> 00:44:31.720]   Obviously, there are large areas that are very female-dominated and there's no pressure
[00:44:31.720 --> 00:44:37.560]   on women to go and change the way that flower arranging is done or cooking in order to make
[00:44:37.560 --> 00:44:38.600]   it more welcoming to men.
[00:44:39.960 --> 00:44:46.600]   This is one where if you had a really high bar for things are unfair unless the following
[00:44:46.600 --> 00:44:49.560]   rigorous conditions are met, then you're going to see a lot of unfairness in the world.
[00:44:49.560 --> 00:44:54.120]   Although even then, as long as you have an equally high bar for both men and women, I
[00:44:54.120 --> 00:44:56.920]   don't think it's going to make feminism any more true in my definition.
[00:44:56.920 --> 00:45:02.200]   I also just say I think these really high bars are unreasonable.
[00:45:04.920 --> 00:45:12.760]   There was a friend who had these kind of bars saying, "Look, why is it that when we meet
[00:45:12.760 --> 00:45:16.280]   for food, we have to go and meet at standard hours of breakfast, lunch, and dinner?
[00:45:16.280 --> 00:45:19.000]   I actually like meeting in the middle of the night.
[00:45:19.000 --> 00:45:22.360]   Why can't we have half of the time be my way?"
[00:45:22.360 --> 00:45:27.720]   It's like, "Well, yeah, but you're only one person and why should anyone else change?"
[00:45:27.720 --> 00:45:33.240]   It depends upon what subfield you're in as well.
[00:45:33.640 --> 00:45:38.200]   So, there are actually groups of people who really like hanging out in the middle of the
[00:45:38.200 --> 00:45:40.360]   night and say, "Why is it we always have to meet in the middle of the night?
[00:45:40.360 --> 00:45:41.400]   Why can't we do it my way?"
[00:45:41.400 --> 00:45:45.400]   It's like, "Well, you are entering into a subculture that works this way.
[00:45:45.400 --> 00:45:49.320]   You could demand that we totally change our way of doing to accommodate you, but it just
[00:45:49.320 --> 00:45:54.520]   seems like it is an unreasonable imposition on people who are already here."
[00:45:54.520 --> 00:45:59.240]   Now, when you sort of go through the list of different things that people think of as
[00:45:59.240 --> 00:46:05.480]   making something a male or a not male field, sometimes people will treat things like just
[00:46:05.480 --> 00:46:08.280]   acting like there's an objectively correct answer that's a male trait.
[00:46:08.280 --> 00:46:15.640]   If that's a male trait, then we need to keep that trait because that is vital to really
[00:46:15.640 --> 00:46:17.800]   any field where there are right and wrong answers.
[00:46:17.800 --> 00:46:24.120]   That's one where I am very tempted rhetorically to say, "It's just so sexist to say that it's
[00:46:24.120 --> 00:46:26.040]   male to think that things are right and wrong.
[00:46:26.040 --> 00:46:28.360]   I think that is a trait of both genders."
[00:46:29.320 --> 00:46:36.120]   And in a way, I do end the essay saying, "Yes, these are not a male monopoly.
[00:46:36.120 --> 00:46:37.800]   They are not uniquely male virtues.
[00:46:37.800 --> 00:46:41.960]   They are virtues that can and should be enjoyed by all human beings."
[00:46:41.960 --> 00:46:48.520]   At the same time, you might say, "Well, but are the virtues equally represented by both
[00:46:48.520 --> 00:46:49.000]   genders?"
[00:46:49.000 --> 00:46:50.760]   And say, "Well, that's an empirical question.
[00:46:50.760 --> 00:46:51.560]   We've got to look at that."
[00:46:51.560 --> 00:46:56.440]   Shifting subjects, you recently performed at the Comedy Cellar.
[00:46:57.480 --> 00:46:58.760]   How was that experience?
[00:46:58.760 --> 00:47:01.800]   Yeah, that was super fun and a big challenge.
[00:47:01.800 --> 00:47:04.600]   I am a professional public speaker.
[00:47:04.600 --> 00:47:08.280]   Stand-up comedy is professional public speaking.
[00:47:08.280 --> 00:47:12.040]   I was curious about how much transfer of learning there would be.
[00:47:12.040 --> 00:47:16.920]   How many of the things that I know as a regular public speaker can I take with me to do stand-up
[00:47:16.920 --> 00:47:17.400]   comedy?
[00:47:17.400 --> 00:47:20.200]   I'm also just a big fan of stand-up comedy.
[00:47:20.200 --> 00:47:24.440]   If you know me personally, I just find life constantly funny.
[00:47:25.880 --> 00:47:27.320]   You're a very pleasant person to be around.
[00:47:27.320 --> 00:47:29.880]   Life is funny to me.
[00:47:29.880 --> 00:47:31.320]   I like pointing out funny things.
[00:47:31.320 --> 00:47:33.880]   I like using my imagination.
[00:47:33.880 --> 00:47:37.800]   A lot of comedy is also just imagination, saying, "Look, imagine that it was the opposite
[00:47:37.800 --> 00:47:37.960]   way.
[00:47:37.960 --> 00:47:38.840]   What would that be like?"
[00:47:38.840 --> 00:47:43.720]   Anyway, I did get this chance – well, actually, just to back up again.
[00:47:43.720 --> 00:47:48.920]   During COVID, I did just create a wiki of comedy ideas, just on the idea maybe one day
[00:47:48.920 --> 00:47:50.120]   I'll go and do stand-up comedy.
[00:47:51.240 --> 00:47:56.680]   The Comedy Cellar actually has a podcast, kind of like Joe Rogan, where comedians go
[00:47:56.680 --> 00:47:58.200]   and talk about serious issues.
[00:47:58.200 --> 00:47:59.320]   I was invited on that.
[00:47:59.320 --> 00:48:04.120]   As a result, I was able to talk my way into getting to perform on the actual live stage
[00:48:04.120 --> 00:48:08.840]   of the biggest comedy club in New York, so maybe the world.
[00:48:08.840 --> 00:48:16.200]   The main thing I can say about my performance is it was me and nine professional comedians,
[00:48:16.200 --> 00:48:18.280]   and I don't think I was obviously the worst person.
[00:48:19.400 --> 00:48:20.840]   That felt pretty good.
[00:48:20.840 --> 00:48:21.800]   It was a pretty good performance.
[00:48:21.800 --> 00:48:24.680]   I felt good about it.
[00:48:24.680 --> 00:48:31.560]   The main differences that I realized between the kind of public speaking I was used to
[00:48:31.560 --> 00:48:36.120]   doing and what I actually did there – one is the importance of memorizing the script.
[00:48:36.120 --> 00:48:39.720]   It just looks a lot worse if you're reading off a note.
[00:48:39.720 --> 00:48:44.040]   Normally, I have some basic notes, and then I ad lib.
[00:48:44.040 --> 00:48:45.320]   I don't memorize.
[00:48:45.320 --> 00:48:48.920]   The only time I have a script is if I have a very time-constrained debate.
[00:48:48.920 --> 00:48:53.000]   Then I will normally write an opening statement, but otherwise, I don't.
[00:48:53.000 --> 00:48:57.080]   The thing with comedy is it depends so heavily upon exact word choice.
[00:48:57.080 --> 00:49:03.400]   You could go and put the same sentence into Google Translate and then back-translate it
[00:49:03.400 --> 00:49:07.160]   and get another sentence that is synonymous but isn't funny at all.
[00:49:07.160 --> 00:49:11.320]   That was something that I was very mindful of.
[00:49:11.320 --> 00:49:14.840]   Then obviously, there's things like timing and just being able to read an audience.
[00:49:14.840 --> 00:49:17.080]   Being able to read an audience, that's one that I'm more used to.
[00:49:17.640 --> 00:49:20.600]   That was what was so hard during COVID is not being able to look at the faces,
[00:49:20.600 --> 00:49:21.640]   even of a live audience.
[00:49:21.640 --> 00:49:26.680]   I can see their eyes, but I can't tell their emotions or their reactions to their eyes.
[00:49:26.680 --> 00:49:29.000]   I don't know whether I should talk more or less about something.
[00:49:29.000 --> 00:49:32.760]   I don't know whether they're angry or annoyed or curious or bored.
[00:49:32.760 --> 00:49:38.840]   These are all things that I would normally be adjusting my talk for in normal public speaking.
[00:49:38.840 --> 00:49:40.920]   Again, with comedy, it's a bit hard to do.
[00:49:40.920 --> 00:49:45.240]   What successful comedians actually do is they try it about a bunch of different ways,
[00:49:45.240 --> 00:49:48.680]   and then they remember which ways work, which ones don't, and they just keep tweaking it.
[00:49:48.680 --> 00:49:53.560]   Finally, when they do the Netflix special, they have basically done A/B testing on 100
[00:49:53.560 --> 00:49:55.240]   different audiences, and then it sounds great.
[00:49:55.240 --> 00:49:57.480]   But the first time, not that funny.
[00:49:57.480 --> 00:50:01.000]   It didn't occur to me until you mentioned it, but that makes a lot of sense that
[00:50:01.000 --> 00:50:05.640]   the transfer of learning there, in both disciplines, there's a lot of hypotheticals,
[00:50:05.640 --> 00:50:09.560]   thought experiments, putting things in strange situations to see what the result is.
[00:50:09.560 --> 00:50:11.000]   That makes sense.
[00:50:12.760 --> 00:50:15.160]   A lot of it is just not having stage fright.
[00:50:15.160 --> 00:50:22.680]   I probably had just a tiny bit of stage fright at the Comedy Cellar, which normally I would
[00:50:22.680 --> 00:50:23.880]   have basically zero.
[00:50:23.880 --> 00:50:30.040]   But there was a little bit different because it's like, honestly, most of my anxiety was,
[00:50:30.040 --> 00:50:31.080]   "Am I going to forget something?"
[00:50:31.080 --> 00:50:41.640]   Everyone's looking at me, and I actually have a joke in the set about how nothing is scarier
[00:50:41.640 --> 00:50:46.200]   than staying silent while thousands of people stare at you.
[00:50:46.200 --> 00:50:48.840]   So that was a self-referential joke that I worked in there.
[00:50:48.840 --> 00:50:52.120]   I can't remember if it was Robin who said this, but didn't he have a theory about the
[00:50:52.120 --> 00:50:58.520]   reason we have stage fright is because in some way you're showing dominance or status,
[00:50:58.520 --> 00:51:01.960]   and then you don't want to do that if you're not the most confident.
[00:51:01.960 --> 00:51:03.400]   You're making a bid for status.
[00:51:03.400 --> 00:51:04.120]   Right, exactly.
[00:51:04.120 --> 00:51:10.120]   And just remembering in the ancestral environment, we're in these small groups of 20 to 40 people.
[00:51:10.120 --> 00:51:13.560]   If you go and want to speak, you're saying, "I'm one of the most important people in this
[00:51:13.560 --> 00:51:20.440]   band here," and if you're not, or if there's a lot of people say, "That guy is not important,"
[00:51:20.440 --> 00:51:21.080]   then who knows?
[00:51:21.080 --> 00:51:23.320]   They might shove you off the cliff the next time they get a chance.
[00:51:23.320 --> 00:51:24.680]   So yeah, watch out.
[00:51:24.680 --> 00:51:27.880]   Yeah, I wonder if this explains the cringe emotion, just that somebody makes a bid for
[00:51:27.880 --> 00:51:28.840]   status that's not deserved.
[00:51:28.840 --> 00:51:30.840]   Right, yeah, it makes a lot of sense.
[00:51:30.840 --> 00:51:33.960]   Okay, I want to talk about discrimination.
[00:51:33.960 --> 00:51:39.800]   So as you know, there's a Supreme Court case about Harvard and affirmative action.
[00:51:40.360 --> 00:51:45.480]   And you might also know that a lot of companies have filed a brief in favor of Harvard saying
[00:51:45.480 --> 00:51:49.960]   that this affirmative action is necessary for them to hire a diverse workforce that
[00:51:49.960 --> 00:51:53.160]   included Apple, Lyft, General Motors.
[00:51:53.160 --> 00:51:58.280]   So what is the explanation for corporations wanting to extend affirmative action?
[00:51:58.280 --> 00:52:00.520]   Or are they just saying this, but they don't want it?
[00:52:00.520 --> 00:52:01.800]   Right.
[00:52:01.800 --> 00:52:06.920]   I mean, if those individual corporations could press a button that would immunize them to all
[00:52:06.920 --> 00:52:09.080]   employment lawsuits, I think they would press it.
[00:52:09.880 --> 00:52:10.440]   Right.
[00:52:10.440 --> 00:52:13.960]   When you look at their behavior, they don't just give in whenever they get sued.
[00:52:13.960 --> 00:52:18.920]   They have a normal team of lawyers that tries to minimize the damage to the company and
[00:52:18.920 --> 00:52:21.480]   pay as little as possible and make the problem go away.
[00:52:21.480 --> 00:52:23.480]   So I think really what's going on is public relations.
[00:52:23.480 --> 00:52:24.440]   Right.
[00:52:24.440 --> 00:52:26.600]   They are trying to be on that team.
[00:52:26.600 --> 00:52:31.240]   As to whether it's public relations vis-a-vis their consumers or public relations vis-a-vis
[00:52:31.240 --> 00:52:34.680]   other people in the executive boardroom is an interesting question.
[00:52:34.680 --> 00:52:37.640]   I think these days it probably is more of the latter.
[00:52:37.640 --> 00:52:41.560]   Although even under Reagan, actually, there were a bunch of major corporations that did
[00:52:41.560 --> 00:52:45.480]   make a similar statement saying that we want affirmative action to continue.
[00:52:45.480 --> 00:52:53.080]   I think that the real story is that they want to get the status of saying that we are really
[00:52:53.080 --> 00:52:53.640]   in favor of this.
[00:52:53.640 --> 00:52:54.440]   We love this stuff.
[00:52:54.440 --> 00:52:59.400]   But at the same time, if it just went away, they wouldn't voluntarily adopt a policy where
[00:52:59.400 --> 00:53:01.800]   we give you a right to go and sue us for mistreatment.
[00:53:01.800 --> 00:53:04.120]   I think there would still be a lot of propaganda.
[00:53:04.120 --> 00:53:05.800]   I mean, here's the general thing.
[00:53:05.800 --> 00:53:10.360]   You think about this as a species of corporate philanthropy sticking their neck out in favor
[00:53:10.360 --> 00:53:11.480]   of a broad social cause.
[00:53:11.480 --> 00:53:14.840]   Some of you might say, "No, it's self-interest for some reason."
[00:53:14.840 --> 00:53:18.120]   It's like, look, the odds that even Apple is going to change the Supreme Court's mind
[00:53:18.120 --> 00:53:20.200]   is super low, so I don't think it's that.
[00:53:20.200 --> 00:53:22.520]   Basically, what they're doing is a kind of philanthropy.
[00:53:22.520 --> 00:53:24.520]   And what's the deal with corporate philanthropy?
[00:53:24.520 --> 00:53:28.920]   The deal with corporate philanthropy is you are trying to go and, first of all, make the
[00:53:28.920 --> 00:53:33.080]   public like you, but also you're trying to look good and jockey for influence within
[00:53:33.080 --> 00:53:33.800]   your own company.
[00:53:35.480 --> 00:53:39.240]   And one really striking thing about corporate philanthropy, when you look closer, normally
[00:53:39.240 --> 00:53:43.720]   they spend way more resources marketing the philanthropy and letting everyone know, "Oh,
[00:53:43.720 --> 00:53:47.240]   we did all this philanthropy," than they actually spend on the philanthropy.
[00:53:47.240 --> 00:53:53.000]   I had a friend who was a marketing person in charge of publicizing her company's philanthropy.
[00:53:53.000 --> 00:53:57.560]   They gave away $1,000 a year to the Girl Scouts or something like that, and she had a $100,000
[00:53:57.560 --> 00:54:02.040]   salary telling everyone about how great they were for giving this money to the Girl Scouts.
[00:54:02.040 --> 00:54:04.120]   So, I think that's the real story.
[00:54:04.120 --> 00:54:09.640]   Get maximally cynical, without denying the fact that there are true believers now in
[00:54:09.640 --> 00:54:13.800]   corporate boardrooms who are pushing it past the point of profitability.
[00:54:13.800 --> 00:54:14.300]   All right.
[00:54:14.300 --> 00:54:18.760]   The cost of the philanthropy is just the production budget of the TV commercial.
[00:54:18.760 --> 00:54:19.260]   Yeah.
[00:54:19.260 --> 00:54:21.720]   Yeah, it's a rounding error.
[00:54:21.720 --> 00:54:23.960]   The donation is the rounding error.
[00:54:23.960 --> 00:54:25.560]   And the, "Hey, everyone, look at us.
[00:54:25.560 --> 00:54:27.800]   We're so freaking philanthropic."
[00:54:27.800 --> 00:54:28.360]   Yeah.
[00:54:28.360 --> 00:54:31.080]   Okay, so this question was one that actually Tyler suggested I ask you.
[00:54:32.920 --> 00:54:37.240]   In the myth of the rational voter, you say that education makes you more free market.
[00:54:37.240 --> 00:54:40.600]   Now, this may have changed in the meantime, but let's just say that's still true.
[00:54:40.600 --> 00:54:44.040]   If you're not learning anything, why is education making you more free market?
[00:54:44.040 --> 00:54:45.400]   Right.
[00:54:45.400 --> 00:54:49.800]   It's particularly striking that even people who don't seem to take any economics classes
[00:54:49.800 --> 00:54:53.000]   who aren't involved, I think that the best story is peer effects.
[00:54:53.000 --> 00:54:58.040]   When you go to college, you're around other peers who, though not pro-market,
[00:54:58.040 --> 00:55:00.760]   are less anti-market than the general population.
[00:55:01.560 --> 00:55:05.000]   The thing about peer effects, though, is that they really are a double-edged sword
[00:55:05.000 --> 00:55:06.440]   from a social point of view.
[00:55:06.440 --> 00:55:07.720]   Think about this.
[00:55:07.720 --> 00:55:12.200]   Right now, if you are one of the 1% of non-Mormons that goes to Brigham Young University,
[00:55:12.200 --> 00:55:14.440]   what do you think the odds are that you'll convert to Mormonism?
[00:55:14.440 --> 00:55:15.240]   Higher than normal.
[00:55:15.240 --> 00:55:15.740]   Yeah.
[00:55:15.740 --> 00:55:18.600]   I don't know the numbers, but I think it's pretty high.
[00:55:18.600 --> 00:55:22.040]   But suppose that Brigham Young let in all the non-Mormons.
[00:55:22.040 --> 00:55:26.600]   Then what would Brigham Young do for conversion Mormonism?
[00:55:26.600 --> 00:55:27.400]   Probably very little.
[00:55:29.080 --> 00:55:30.680]   Furthermore, you realize, "Huh.
[00:55:30.680 --> 00:55:34.120]   Well, what if those Mormons at Brigham Young were dispersed among a bunch of other schools
[00:55:34.120 --> 00:55:35.400]   where they were a minority?
[00:55:35.400 --> 00:55:39.240]   Seems quite plausible they'd be making a lot more converts over there."
[00:55:39.240 --> 00:55:43.800]   So basically, the thing about peer effects is that if you achieve your peer effects by
[00:55:43.800 --> 00:55:47.080]   segregation — and again, that is, of course, literally what college does.
[00:55:47.080 --> 00:55:51.320]   It takes one part of society and segregates it from another part of society.
[00:55:51.320 --> 00:55:56.120]   Physically, when you're in school and then there's social segregation just caused by
[00:55:56.920 --> 00:56:00.680]   the fact that people want to hang out with other people in their own social — of your
[00:56:00.680 --> 00:56:02.520]   own education levels, that kind of thing.
[00:56:02.520 --> 00:56:06.360]   So in that case, in terms of whether or not education actually makes society overall more
[00:56:06.360 --> 00:56:08.440]   free-market, I think it's totally unclear.
[00:56:08.440 --> 00:56:12.200]   Because basically, the people who go to college, they make each other more free-market.
[00:56:12.200 --> 00:56:17.560]   Same time, they remove from influencing people of other social classes who don't go to college,
[00:56:17.560 --> 00:56:20.680]   who probably then influence each other and make each other less free-market.
[00:56:20.680 --> 00:56:22.200]   I think that's the most plausible story.
[00:56:22.200 --> 00:56:26.120]   But what about the argument that the people who are going to, let's say, elite universities,
[00:56:26.120 --> 00:56:29.480]   these are people who are going to control things, right?
[00:56:29.480 --> 00:56:33.560]   And so if you can engineer a situation in which the peer effects in some particular
[00:56:33.560 --> 00:56:37.960]   direction are very strong at Harvard, maybe because the upper class is very liberal or
[00:56:37.960 --> 00:56:41.960]   woke, they make the underclass even more woke and then it's just like a reinforcing cycle
[00:56:41.960 --> 00:56:43.960]   after every generation of people coming to college.
[00:56:43.960 --> 00:56:48.120]   Then that still matters a lot, even though presumably somebody becomes more right-wing
[00:56:48.120 --> 00:56:49.800]   once they didn't go to Harvard because there's no peers there.
[00:56:49.800 --> 00:56:50.280]   It doesn't matter.
[00:56:50.280 --> 00:56:51.640]   They're not going to be an elite, right?
[00:56:51.640 --> 00:56:53.000]   Or it doesn't matter as much.
[00:56:53.000 --> 00:56:57.240]   Could be, although what we've seen is that we now just have very big gaps between elite
[00:56:57.240 --> 00:56:59.320]   opinion and mass opinion.
[00:56:59.320 --> 00:57:05.320]   Of course, it is a democracy, so if you want to run for office, that is a reason to go
[00:57:05.320 --> 00:57:07.400]   and say, "Yeah, well, what is the actual common view here?"
[00:57:07.400 --> 00:57:09.160]   Not just the view that is common among elites.
[00:57:09.160 --> 00:57:16.600]   But I will say that this is a topic that deserves a lot more study, right?
[00:57:16.600 --> 00:57:21.400]   Now, the other thing to remember is, wouldn't there be peer effects even without college?
[00:57:21.400 --> 00:57:26.840]   So, if elites didn't go to college and instead, they went and did elite apprenticeships at
[00:57:26.840 --> 00:57:31.160]   top corporations instead, I think you'd still wind up getting a very similar elite subculture.
[00:57:31.160 --> 00:57:37.320]   I think that this kind of social segregation is very natural in every human society.
[00:57:37.320 --> 00:57:40.840]   Of course, you can see it under communism very strongly, where it's like, "I don't
[00:57:40.840 --> 00:57:44.680]   want my kid going and playing with a kid whose parents aren't in the communist party."
[00:57:44.680 --> 00:57:46.920]   So, every society has this kind of thing.
[00:57:48.440 --> 00:57:52.360]   Again, if you go and push the dynamics enough — let's put it this way.
[00:57:52.360 --> 00:57:59.320]   If you were the prophet of the Mormon religion — they have a prophet — what would be
[00:57:59.320 --> 00:58:02.920]   the very best thing for you to do to maximize the spread of Mormonism?
[00:58:02.920 --> 00:58:06.920]   It is not at all clear to me that trying to get all Mormons to go to Brigham Young is
[00:58:06.920 --> 00:58:07.720]   a good strategy.
[00:58:07.720 --> 00:58:11.000]   I wonder if there are nonlinear dynamics to this.
[00:58:11.000 --> 00:58:13.320]   Yeah, if you wonder, there's got to be.
[00:58:15.080 --> 00:58:18.920]   As soon as you're talking about nonlinear dynamics, those are hard to understand, right?
[00:58:18.920 --> 00:58:20.200]   Yeah, they're hard to understand.
[00:58:20.200 --> 00:58:23.800]   I would just say, keep a much more open mind about this.
[00:58:23.800 --> 00:58:27.720]   If anyone is listening and wants to do research on this, that sounds cool.
[00:58:27.720 --> 00:58:28.520]   I'll read it.
[00:58:28.520 --> 00:58:29.020]   Right.
[00:58:29.020 --> 00:58:33.320]   I remember you saying that one of the things you're trying to do with your books is influence
[00:58:33.320 --> 00:58:37.160]   elite opinion because elite opinion percolates down to the common view, right?
[00:58:37.160 --> 00:58:44.120]   There are elite subcultures in every society, but they're not the same elite subcultures,
[00:58:44.120 --> 00:58:48.120]   right, so you might care very much about which particular subculture is coming to power.
[00:58:48.120 --> 00:58:51.320]   Although notice that that's one where I'm taking it as a given that we have the current
[00:58:51.320 --> 00:58:55.160]   segregation, and then I'm going to try to go and take advantage of it.
[00:58:55.160 --> 00:58:58.600]   But if it were a question of if I could change the dial of what kind of segregation that
[00:58:58.600 --> 00:59:00.360]   we have, then it's much less clear.
[00:59:00.360 --> 00:59:00.920]   Yeah.
[00:59:00.920 --> 00:59:02.040]   Student loan forgiveness.
[00:59:02.040 --> 00:59:03.960]   What is your reaction?
[00:59:03.960 --> 00:59:06.680]   Oh, oy vey.
[00:59:06.680 --> 00:59:08.360]   Give me a freaking break.
[00:59:08.360 --> 00:59:13.720]   This is one where I think it's very hard to find almost any economist, no matter how
[00:59:13.720 --> 00:59:19.240]   left-wing or progressive, who really wants to stick their necks out and defend this garbage.
[00:59:19.240 --> 00:59:20.840]   Look, it's a regressive transfer.
[00:59:20.840 --> 00:59:23.480]   Why then?
[00:59:23.480 --> 00:59:27.560]   Why is it that someone who is left-wing or progressive would go in favor of it?
[00:59:27.560 --> 00:59:34.040]   It's like, well, because people who have a lot of education and colleges are on our team,
[00:59:34.040 --> 00:59:36.520]   and we just want to go and help our team or something like that.
[00:59:36.520 --> 00:59:42.680]   It really is one where, look, obviously what the forgiveness really means is we're going
[00:59:42.680 --> 00:59:49.480]   to go and transfer the cost of this debt from the elites that actually ran up the bill to
[00:59:49.480 --> 00:59:54.200]   the general population, which includes, of course, a whole lot of people that did not
[00:59:54.200 --> 00:59:58.040]   go to college and did not get whatever premium that you got out of it.
[00:59:58.040 --> 01:00:00.440]   So there's that.
[01:00:00.440 --> 01:00:08.280]   In terms of efficiency, since the people have already gotten the education, you're not even
[01:00:08.280 --> 01:00:11.560]   going and increasing the amount of education if you really think that's good.
[01:00:11.560 --> 01:00:16.520]   The only margin that this is really increasing education is it's making people think, "Well,
[01:00:16.520 --> 01:00:20.920]   maybe there'll be another round of debt forgiveness later on, so I'll rack up more debt."
[01:00:20.920 --> 01:00:24.520]   The actual true price of education is less than it seems to be.
[01:00:24.520 --> 01:00:29.480]   Although even there, you have to say, "Huh, well, but could people knowing this and the
[01:00:29.480 --> 01:00:33.320]   great willingness to borrow actually wind up increasing demand for college and raising
[01:00:33.320 --> 01:00:34.040]   tuition further?"
[01:00:34.040 --> 01:00:35.960]   There's good evidence on that, that it does.
[01:00:35.960 --> 01:00:40.360]   So not at 100%, but still to a substantial degree.
[01:00:40.360 --> 01:00:46.120]   Again, for me, just to back up, that can be my catchphrase, I have a book called The Case
[01:00:46.120 --> 01:00:46.920]   Against Education.
[01:00:46.920 --> 01:00:53.480]   My view is much more extreme than that of almost any normal economist who opposes student
[01:00:53.480 --> 01:00:54.440]   loan debt forgiveness.
[01:00:54.440 --> 01:00:59.400]   I think that the real problem with education is we have way too much of it.
[01:00:59.400 --> 01:01:01.000]   Most of it is very socially wasteful.
[01:01:01.000 --> 01:01:06.360]   What we're doing with student loan forgiveness is we're basically going and transferring
[01:01:06.360 --> 01:01:09.080]   to people who wasted a lot of social resources.
[01:01:09.720 --> 01:01:15.240]   And yeah, that story that it is on the slippery slope to free college for all, in a way, that's
[01:01:15.240 --> 01:01:17.000]   the best argument in favor of it, right?
[01:01:17.000 --> 01:01:20.120]   If you really thought that free college for all was a good idea, then this is putting
[01:01:20.120 --> 01:01:21.640]   us on that slippery slope.
[01:01:21.640 --> 01:01:26.760]   I would say yeah, and that's terrible, because the real problem with education is that we
[01:01:26.760 --> 01:01:28.120]   just spend way too many years in school.
[01:01:28.120 --> 01:01:29.880]   It is generally not socially useful.
[01:01:29.880 --> 01:01:34.760]   Rather, the main reason why it's going on is that it's a way of stamping people's foreheads,
[01:01:34.760 --> 01:01:39.320]   saying that they are better than their competitors, don't throw their application in the trash.
[01:01:39.320 --> 01:01:42.360]   And the more education we get, the more you need to not have your application thrown in
[01:01:42.360 --> 01:01:42.760]   the trash.
[01:01:42.760 --> 01:01:44.120]   Credential inflation.
[01:01:44.120 --> 01:01:49.560]   Since we're talking a lot about inflation these days, the central organizing idea of
[01:01:49.560 --> 01:01:52.680]   what's so wasteful about education in my book is credential inflation.
[01:01:52.680 --> 01:01:56.520]   Saying that, really, when everyone has a college degree, nobody does.
[01:01:56.520 --> 01:01:59.400]   And again, the analogy is very good, right?
[01:01:59.400 --> 01:02:02.600]   So can you make a country rich just by giving everybody a trillion dollars?
[01:02:02.600 --> 01:02:03.880]   You cannot.
[01:02:03.880 --> 01:02:07.960]   All that happens is you wind up raising prices and you cause a lot of harm in the process.
[01:02:07.960 --> 01:02:12.120]   And I say the same thing is going on with the multiplication of credentials.
[01:02:12.120 --> 01:02:13.400]   So let me ask you about that, right?
[01:02:13.400 --> 01:02:17.720]   Because I think for the last 10 years, the proportion of Americans who are getting college
[01:02:17.720 --> 01:02:19.080]   degrees has not gone up, right?
[01:02:19.080 --> 01:02:23.000]   Doesn't the signaling theory imply that it should be going up as a credential gets diluted?
[01:02:23.000 --> 01:02:24.040]   Right.
[01:02:24.040 --> 01:02:26.440]   So actually, if it doesn't go up, then it's not getting diluted.
[01:02:26.440 --> 01:02:30.040]   But so here's the actual story.
[01:02:30.040 --> 01:02:34.600]   What's been going on during the last 10 years, so I have a bunch of bets on this, actually,
[01:02:34.600 --> 01:02:35.720]   and I just won the first one.
[01:02:36.280 --> 01:02:39.720]   When you see that the share that's going to college is going down, that's counting community
[01:02:39.720 --> 01:02:41.400]   college, right?
[01:02:41.400 --> 01:02:45.080]   It's community college that has fallen a bit, which makes sense because the signal sent
[01:02:45.080 --> 01:02:50.040]   by community college is barely better than nothing, except in a few disciplines like
[01:02:50.040 --> 01:02:51.960]   nursing, a few occupations like that.
[01:02:51.960 --> 01:02:54.440]   But for four-year college attendance, it's actually continuing to rise.
[01:02:54.440 --> 01:03:01.160]   The bets that I have are all along the lines of attendance in traditional four-year brick
[01:03:01.160 --> 01:03:03.240]   and mortar colleges will fall no more than 10%.
[01:03:04.200 --> 01:03:07.080]   So I have a whole series of these bets, each with 10-year maturities.
[01:03:07.080 --> 01:03:08.280]   So I just won the first one.
[01:03:08.280 --> 01:03:10.920]   And yeah, I think I'm just going to win a whole bunch of other ones.
[01:03:10.920 --> 01:03:13.720]   Does your undefeated bet record make you more hesitant to take bets?
[01:03:13.720 --> 01:03:18.600]   Because getting the first, you know, 52 to one will just be a huge.
[01:03:18.600 --> 01:03:23.800]   Yeah, like I would be lying if I said it didn't change my emotions.
[01:03:23.800 --> 01:03:24.040]   Yes.
[01:03:24.040 --> 01:03:28.120]   So I do have a record of 23 wins for 23 bets that have resolved.
[01:03:29.240 --> 01:03:31.240]   It is fun to be able to go and say that.
[01:03:31.240 --> 01:03:36.040]   To say I've got 29 out of 30 bets won wouldn't sound as good.
[01:03:36.040 --> 01:03:40.360]   At the same time, I still am totally willing to make bets.
[01:03:40.360 --> 01:03:44.040]   The thing about bets is that it's not like they just come fall into your lap.
[01:03:44.040 --> 01:03:47.320]   You really have to aggressively seek them out because hardly anyone wants to bet.
[01:03:47.320 --> 01:03:51.480]   At this point, you might think that some people would want to bet me just basically saying,
[01:03:51.480 --> 01:03:53.480]   "Well, if I beat him, I'll be the guy that beat him.
[01:03:53.480 --> 01:03:55.800]   And if I lose, then who's ever heard of me anyway?
[01:03:55.800 --> 01:03:56.280]   Who cares?"
[01:03:56.280 --> 01:03:57.960]   But even that doesn't really do very much.
[01:03:58.840 --> 01:04:03.000]   The bet that I'm likely to lose is I do have a global warming bet with
[01:04:03.000 --> 01:04:05.480]   the stand-up economist, Joram Bauman.
[01:04:05.480 --> 01:04:09.080]   So that's one where he gave me three to one odds initially.
[01:04:09.080 --> 01:04:10.440]   So I was expecting to lose.
[01:04:10.440 --> 01:04:17.960]   So he very much enjoys running annual victory laps on Twitter without ever mentioning,
[01:04:17.960 --> 01:04:21.960]   "Yeah, well, you did give me odds because I wasn't saying that I was convinced.
[01:04:21.960 --> 01:04:24.360]   I was saying that I thought that other people were overconfident."
[01:04:25.560 --> 01:04:30.040]   But that's his right to go and run his victory laps.
[01:04:30.040 --> 01:04:34.200]   You're like a UFC fighter who's like, "Who hasn't lost yet?"
[01:04:34.200 --> 01:04:37.080]   Who was it?
[01:04:37.080 --> 01:04:37.560]   Habib.
[01:04:37.560 --> 01:04:40.360]   And then he just retired before he lost a single fight, I believe.
[01:04:40.360 --> 01:04:44.760]   His reason was like, "Oh, my mother doesn't want me to fight anymore."
[01:04:44.760 --> 01:04:45.560]   But one wonders.
[01:04:45.560 --> 01:04:54.280]   Are you hopeful that maybe you right-wing government seeing the education polarization
[01:04:55.000 --> 01:04:59.720]   and also seeing the loan forgiveness as a transfer of wealth towards left-wing elites,
[01:04:59.720 --> 01:05:03.240]   are you hopeful that maybe they will implement education austerity?
[01:05:03.240 --> 01:05:05.240]   Barely.
[01:05:05.240 --> 01:05:11.000]   So yes, just to back up, the big policy reform that I push in the case against education is
[01:05:11.000 --> 01:05:15.400]   education austerity, just spending less on education, making education less accessible.
[01:05:15.400 --> 01:05:21.960]   That's right, making it less accessible, which if I am correct, will be socially desirable
[01:05:21.960 --> 01:05:25.480]   because it will basically mean that credential inflation will be reversed and people will be
[01:05:25.480 --> 01:05:29.880]   able to get the jobs they're going to get anyway while spending fewer years of their lives in
[01:05:29.880 --> 01:05:31.880]   school learning stuff that is not very useful.
[01:05:31.880 --> 01:05:38.120]   The main reason that I'm skeptical about this is even in states where the state government is very
[01:05:38.120 --> 01:05:43.400]   right-wing, it seems that the prestige of the state university systems is sufficiently high
[01:05:43.400 --> 01:05:45.880]   that politicians generally don't want to challenge it.
[01:05:45.880 --> 01:05:51.640]   So in Texas, we both were hanging out around on the University of Texas campus, Dworkash,
[01:05:51.640 --> 01:05:57.800]   and the state capitol of Texas is just a 20-minute walk away, and it seems like the governor of Texas
[01:05:57.800 --> 01:06:03.080]   does not want to have a throwdown with the president of the University of Texas and say,
[01:06:03.080 --> 01:06:04.920]   "Hey, we're sick of this stuff."
[01:06:04.920 --> 01:06:07.480]   Why not?
[01:06:07.480 --> 01:06:11.720]   Probably because the governor of Texas thinks that UT with its great football team is really
[01:06:11.720 --> 01:06:13.160]   popular and that he can't beat them.
[01:06:13.160 --> 01:06:19.880]   Again, while you might think, "Look, we're going to pass a bill saying that all the athletics of
[01:06:19.880 --> 01:06:27.000]   UT are fully funded, but we are going to go and get rid of the following departments," it's just
[01:06:27.000 --> 01:06:28.840]   hard to go and make that.
[01:06:28.840 --> 01:06:34.200]   Probably the president of UT could get the football coach to say, "We stand arm-in-arm,
[01:06:34.200 --> 01:06:37.400]   shoulder-to-shoulder with our fat studies department," or whatever.
[01:06:37.400 --> 01:06:40.120]   That is the concern.
[01:06:40.120 --> 01:06:41.400]   Florida is a little bit different.
[01:06:41.400 --> 01:06:47.720]   It does seem like DeSantis is trying to go and at least make some symbolic efforts against
[01:06:47.720 --> 01:06:50.360]   overwhelming left-wing bias at the University of Florida.
[01:06:50.360 --> 01:06:53.160]   But as far as I know, he isn't doing anything about their budgets, which is, I think,
[01:06:53.160 --> 01:06:54.120]   what they really care about.
[01:06:54.120 --> 01:06:58.760]   You can pass all the laws you want, but if you don't actually mess with their money,
[01:06:58.760 --> 01:07:00.920]   then I don't think they're going to care that much.
[01:07:00.920 --> 01:07:08.760]   The main change that I could least plausibly see is precisely defunding the least popular
[01:07:08.760 --> 01:07:12.280]   departments while saying, "We're going to keep the total budget of the school the same."
[01:07:12.920 --> 01:07:19.080]   So to say, "In the state budget, we are slashing the budgets of the English department,
[01:07:19.080 --> 01:07:23.240]   Women's Studies department, Ethnic Studies department, Sociology."
[01:07:23.240 --> 01:07:28.200]   Basically, any subject that a normal voter would laugh at when you pronounce the name
[01:07:28.200 --> 01:07:30.760]   of the department, maybe they could get away with that.
[01:07:30.760 --> 01:07:36.040]   In a way, it just seems too strategic and literally just requiring too much attention
[01:07:36.040 --> 01:07:38.840]   from politicians to make them realize this.
[01:07:38.840 --> 01:07:41.640]   But that's the easiest thing to see.
[01:07:42.440 --> 01:07:46.440]   Or making them co-sign the loan for student loans, right?
[01:07:46.440 --> 01:07:48.200]   That they have to pay out of their endowments.
[01:07:48.200 --> 01:07:48.520]   Right.
[01:07:48.520 --> 01:07:53.880]   Again, that's one that's just so easy to demagogue and say, "Oh, so what about this
[01:07:53.880 --> 01:07:57.480]   poor student who couldn't do it and now no school wants to accept him because they don't
[01:07:57.480 --> 01:07:58.680]   want to be responsible for it?"
[01:07:58.680 --> 01:08:05.000]   If you go and read Lyndon Johnson's original speech in favor of student loans, it just
[01:08:05.720 --> 01:08:12.360]   takes the social desirability bias dial and turns it up to the absolute deafening maximum,
[01:08:12.360 --> 01:08:16.440]   and it's like, "I believe," let's see, I can't remember Lyndon Johnson's accent,
[01:08:16.440 --> 01:08:19.000]   but let's give him this accent.
[01:08:19.000 --> 01:08:25.480]   "I believe that in America, no student should ever be denied full access to the maximum
[01:08:25.480 --> 01:08:30.360]   opportunities of education, merely because they were born in a family that was too poor
[01:08:30.360 --> 01:08:31.560]   to afford it.
[01:08:31.560 --> 01:08:35.560]   There is no price too high, no sacrifice too great."
[01:08:35.560 --> 01:08:38.120]   Yeah, oh God.
[01:08:38.120 --> 01:08:44.680]   That's the kind of rhetoric, the weaponized nuclear rhetoric that politicians will deploy
[01:08:44.680 --> 01:08:45.640]   to defend this stuff.
[01:08:45.640 --> 01:08:48.600]   And yeah, it's really what I'm up against.
[01:08:48.600 --> 01:08:52.280]   That stuff that is good doesn't sound good.
[01:08:52.280 --> 01:08:52.780]   Yeah.
[01:08:52.780 --> 01:08:58.440]   I was turning Lyndon Johnson into Bill Clinton, I just realized.
[01:08:58.440 --> 01:09:02.760]   Yeah, they're the same Texas, Arkansas.
[01:09:02.760 --> 01:09:05.480]   Yeah, he might be giving that speech from the bathroom every day.
[01:09:05.480 --> 01:09:07.560]   But I don't know if you've heard those stories.
[01:09:07.560 --> 01:09:08.280]   Yeah, yeah.
[01:09:08.280 --> 01:09:15.240]   Why do you think young people are getting more anxious and have higher incidents of
[01:09:15.240 --> 01:09:16.520]   neuroticism overall?
[01:09:16.520 --> 01:09:18.060]   Right.
[01:09:18.060 --> 01:09:24.360]   My first pass on this is always that it's an artifact of measurement of the medicalization
[01:09:24.360 --> 01:09:25.400]   of society.
[01:09:25.400 --> 01:09:26.520]   So when you see things like-
[01:09:26.520 --> 01:09:27.800]   I mean, there are suicide rates as well.
[01:09:27.800 --> 01:09:28.360]   Yes, yes, yes.
[01:09:29.160 --> 01:09:30.200]   That's where I would go next.
[01:09:30.200 --> 01:09:33.400]   But my first pass is always that it's basically dismeasurement.
[01:09:33.400 --> 01:09:38.840]   By modern measures, there was no neuroticism.
[01:09:38.840 --> 01:09:43.160]   There were no psychiatric problems 200 years ago because there were no psychiatrists.
[01:09:43.160 --> 01:09:48.520]   So if you went back 200 years ago and measured it, it's like, "Well, a number of people in
[01:09:48.520 --> 01:09:51.240]   psychiatry offices is zero, so it doesn't exist here."
[01:09:51.240 --> 01:09:54.440]   And then we go and expand it.
[01:09:54.440 --> 01:09:59.240]   Especially when you really go out of your way to make access super easy, to have a big
[01:09:59.240 --> 01:10:05.000]   center on every campus, to de-stigmatize, and at the same time to stigmatize the traditional
[01:10:05.000 --> 01:10:06.760]   substance use for psychiatry like religion.
[01:10:06.760 --> 01:10:10.120]   So traditionally, you have a problem, you go and talk to your priest.
[01:10:10.120 --> 01:10:13.720]   If you stigmatize that, that also says, "Well, I can't talk to priests.
[01:10:13.720 --> 01:10:14.600]   They're scumbags now.
[01:10:14.600 --> 01:10:16.840]   I can't talk to them, so who can I talk to?
[01:10:16.840 --> 01:10:18.040]   I can go to the counseling center.
[01:10:18.040 --> 01:10:19.000]   I can do that."
[01:10:19.000 --> 01:10:19.640]   Now, you're right.
[01:10:19.640 --> 01:10:27.000]   There are some measures where the level of neuroticism, it is not just measuring people
[01:10:27.000 --> 01:10:28.520]   going to a psychiatrist's office.
[01:10:28.520 --> 01:10:30.840]   We actually see this in suicide rates.
[01:10:30.840 --> 01:10:36.280]   Although there, I have a piece where I go over the last 60 or 70 years of suicide rates.
[01:10:36.280 --> 01:10:37.960]   It's really complicated, Rakesh.
[01:10:37.960 --> 01:10:38.600]   Oh, really?
[01:10:38.600 --> 01:10:39.080]   Yeah.
[01:10:39.080 --> 01:10:46.600]   So basically, suicide rates were falling from 1970 to 2000, and then have had a big rebound
[01:10:46.600 --> 01:10:47.080]   since then.
[01:10:47.720 --> 01:10:52.520]   So if I remember, I think their suicide rates now are pretty similar to what they were in
[01:10:52.520 --> 01:10:53.240]   the '50s.
[01:10:53.240 --> 01:10:57.160]   I'd have to go back and double-check those numbers, but that's what I remember.
[01:10:57.160 --> 01:11:00.760]   So then, you've got to have some really complicated theory.
[01:11:00.760 --> 01:11:04.680]   You could have just a bunch of ad hoc theories.
[01:11:04.680 --> 01:11:09.720]   "Okay, well, it was like World War II and the trauma of that, and also being married
[01:11:09.720 --> 01:11:10.760]   to a veteran with trauma.
[01:11:10.760 --> 01:11:13.400]   That could mess women up and make them kill themselves.
[01:11:13.400 --> 01:11:15.960]   And then that changed, and then something else happened."
[01:11:16.840 --> 01:11:24.040]   So that is sort of the Jonathan Haidt case of, "By keeping kids from playing outside
[01:11:24.040 --> 01:11:26.840]   and by infantilizing them, we're making them really anxious."
[01:11:26.840 --> 01:11:29.000]   That doesn't fit with any of the stuff from the '50s.
[01:11:29.000 --> 01:11:33.560]   You could say, "Okay, yeah, well, in the '50s, they were really good in terms of unsupervised
[01:11:33.560 --> 01:11:35.400]   play, but they're really bad in terms of something else."
[01:11:35.400 --> 01:11:43.960]   I mean, really, when people talk about this stuff, what really strikes me is that I felt
[01:11:43.960 --> 01:11:48.200]   like I learned more from just going and looking at the time series as far back as it goes
[01:11:48.200 --> 01:11:53.320]   than from every person that pontificated about what's really going on, just to realize the
[01:11:53.320 --> 01:11:54.520]   numbers are complicated.
[01:11:54.520 --> 01:11:55.720]   No obvious story fits.
[01:11:55.720 --> 01:12:00.040]   So yeah, it would be really ideologically convenient for me to say feminism is leading
[01:12:00.040 --> 01:12:01.160]   women to kill themselves.
[01:12:01.160 --> 01:12:02.040]   The numbers don't work.
[01:12:02.040 --> 01:12:03.720]   So, wrong.
[01:12:03.720 --> 01:12:09.480]   How persistent are the harms from immigration restriction if the world is getting wealthier
[01:12:09.480 --> 01:12:15.240]   by itself anyways, then does it really matter that it will take a few more decades than
[01:12:15.240 --> 01:12:16.200]   it would have otherwise?
[01:12:16.200 --> 01:12:18.920]   You can transfer them here and now, or you can just wait for economic growth to do its
[01:12:18.920 --> 01:12:19.420]   thing there.
[01:12:19.420 --> 01:12:26.200]   The answer is, well, does it really matter if we miss $100 trillion when people are really
[01:12:26.200 --> 01:12:29.640]   poor – during the time that people are poorer than they're probably ever going to be again?
[01:12:29.640 --> 01:12:31.320]   Yeah, I think it really does matter.
[01:12:31.320 --> 01:12:34.840]   Yeah, I'll go with that.
[01:12:37.320 --> 01:12:39.960]   Again, the key thing to understand, just to back up.
[01:12:39.960 --> 01:12:47.240]   In Open Borders, I go over what I consider to be the most powerful argument in favor
[01:12:47.240 --> 01:12:49.800]   of immigration, and it just comes down to this.
[01:12:49.800 --> 01:12:55.640]   We know for a fact, undeniably, that if you go and take a very poor worker from a poor
[01:12:55.640 --> 01:13:01.320]   country and move them to a rich country, almost overnight, their pay multiplies many times.
[01:13:01.320 --> 01:13:04.520]   This is something that you cannot deny while looking at the facts.
[01:13:05.320 --> 01:13:06.920]   Then the question is, why?
[01:13:06.920 --> 01:13:10.520]   Why is it that we can make a Haitian suddenly earn 20 times as much money just by moving
[01:13:10.520 --> 01:13:11.320]   into Miami?
[01:13:11.320 --> 01:13:14.280]   He hasn't even learned English yet, but he's still making 20 times as much money as he
[01:13:14.280 --> 01:13:15.160]   was back in Haiti.
[01:13:15.160 --> 01:13:21.240]   Textbook economic explanation is, we pay immigrants more in rich countries because their productivity
[01:13:21.240 --> 01:13:22.920]   is so much more than it was back home.
[01:13:22.920 --> 01:13:27.000]   Productivity is much higher in rich countries for everyone.
[01:13:27.000 --> 01:13:30.280]   Most of the reason why Haitians are poor is not that there's anything wrong with individual
[01:13:30.280 --> 01:13:31.240]   Haitians.
[01:13:31.240 --> 01:13:33.560]   Most of the reason is that there's something really wrong with Haiti.
[01:13:34.680 --> 01:13:40.840]   If we were to go and get deposited in Port-au-Prince, Dworkesh, we'd be struggling to eke out an
[01:13:40.840 --> 01:13:41.640]   existence as well.
[01:13:41.640 --> 01:13:44.600]   What's messed up is Haiti, not Haitians primarily.
[01:13:44.600 --> 01:13:48.440]   Now this is basically undeniable.
[01:13:48.440 --> 01:13:52.520]   The part that is debatable is, is this scalable?
[01:13:52.520 --> 01:13:57.320]   We can go and make one Haitian vastly better off just by going and moving them to the US,
[01:13:57.320 --> 01:13:59.880]   and it basically pays for itself because he's more productive.
[01:13:59.880 --> 01:14:01.800]   Could we go and move a million Haitians?
[01:14:02.680 --> 01:14:04.440]   Yeah, 10 million.
[01:14:04.440 --> 01:14:07.160]   This is where people might start saying, "Maybe it's not really scalable."
[01:14:07.160 --> 01:14:11.080]   In the book, a lot of the argument really is, "No, it's totally scalable, and this is
[01:14:11.080 --> 01:14:18.120]   just a massive missed opportunity where we really could go and rescue hundreds of millions
[01:14:18.120 --> 01:14:21.720]   of people, really billions of people from poverty in a short amount of time.
[01:14:21.720 --> 01:14:26.840]   We could essentially just fast forward from right now to this future that you're talking
[01:14:26.840 --> 01:14:27.160]   about."
[01:14:27.160 --> 01:14:30.680]   Now, how valuable is this?
[01:14:30.680 --> 01:14:34.440]   Well, if you're someone who thinks on million-year timescales, then not so valuable.
[01:14:34.440 --> 01:14:39.960]   If you're someone who thinks on the timescale of dramatically improving the lives of billions
[01:14:39.960 --> 01:14:42.680]   of people over the next hundred years, then yeah, it's fantastic.
[01:14:42.680 --> 01:14:48.600]   On top of this, worth pointing out that it is almost certain that right now we are missing
[01:14:48.600 --> 01:14:52.600]   some of the greatest human talent that is just trapped in some poor village in India
[01:14:52.600 --> 01:14:55.880]   or China, and we'll never find out what their accomplishment is.
[01:14:55.880 --> 01:15:01.800]   If you think that we are one day going to beat death, right now it is quite likely that
[01:15:01.800 --> 01:15:06.280]   the guy that could have beaten death five or ten years earlier is in some poor village
[01:15:06.280 --> 01:15:07.320]   in India or China.
[01:15:07.320 --> 01:15:12.360]   By not allowing immigration, that person is not going to get to beat death.
[01:15:12.360 --> 01:15:14.040]   If we're going to beat death, we'll beat it eventually.
[01:15:14.040 --> 01:15:19.240]   Just to shave five or ten years off of that, this matters a lot for me.
[01:15:19.240 --> 01:15:21.160]   Those five or ten years really make a difference.
[01:15:21.160 --> 01:15:24.680]   If the technology has just freezed people at the age they're at, like, "I want to get
[01:15:24.680 --> 01:15:28.280]   frozen really quick because I'm going downhill."
[01:15:28.280 --> 01:15:31.480]   But Dwarakesh, even for you, five or ten years, it's a big difference.
[01:15:31.480 --> 01:15:38.680]   If you are an ultra-long-termist, then I guess that you could say open borders is not that
[01:15:38.680 --> 01:15:39.320]   big of a deal.
[01:15:39.320 --> 01:15:47.480]   But in terms of doable policy changes that will lead to the increase of human wealth
[01:15:47.480 --> 01:15:52.280]   of hundreds of trillions of dollars in this century, then it's the best I know of.
[01:15:52.280 --> 01:15:55.400]   Speaking of talent, Tyler Crown and Daniel Gross have a new book about it.
[01:15:55.400 --> 01:16:00.040]   One of the implications, I think, of the book is if talent spotting is something that you
[01:16:00.040 --> 01:16:06.120]   can do pretty reliably with a 1,500-word essay and a Zoom call, doesn't that imply that college
[01:16:06.120 --> 01:16:07.880]   is not necessarily that much about signaling?
[01:16:07.880 --> 01:16:13.160]   Because you don't need four years of cognitively demanding, pointless work.
[01:16:13.160 --> 01:16:16.680]   There's a more convenient way to just get that signal for identifying talent.
[01:16:16.680 --> 01:16:21.480]   So Tyler has a lot of objections to my book, The Case Against Education, but his central
[01:16:21.480 --> 01:16:22.440]   one comes down to this.
[01:16:22.440 --> 01:16:25.080]   "Look, Brian, I hire people.
[01:16:25.080 --> 01:16:26.120]   You don't."
[01:16:26.120 --> 01:16:27.240]   Since then, I've done a little bit.
[01:16:27.240 --> 01:16:29.000]   But basically, "I hire people.
[01:16:29.000 --> 01:16:29.880]   You don't.
[01:16:29.880 --> 01:16:35.880]   And I will just tell you as a fact that I know after a couple of months whether a worker
[01:16:35.880 --> 01:16:39.080]   is good or not, and therefore signaling really cannot be very important."
[01:16:39.080 --> 01:16:40.600]   End of story.
[01:16:40.600 --> 01:16:42.440]   Pulling rank.
[01:16:42.440 --> 01:16:45.560]   So this is the argument that Tyler has right now.
[01:16:45.560 --> 01:16:48.600]   There's a few different responses to this.
[01:16:49.320 --> 01:16:56.120]   The one that is most pleasant and most flattering is to say, "Look, Tyler, you are one employer
[01:16:56.120 --> 01:16:57.240]   in a million.
[01:16:57.240 --> 01:16:58.280]   You're fantastic.
[01:16:58.280 --> 01:16:59.800]   You have incredible capabilities.
[01:16:59.800 --> 01:17:02.120]   You can do this, but you're just one person.
[01:17:02.120 --> 01:17:05.640]   You're not involved in hiring most people, and most employers are nowhere near as good
[01:17:05.640 --> 01:17:08.520]   at you as spotting talent, and so you're still wrong."
[01:17:08.520 --> 01:17:11.400]   So that is sort of the easiest thing to say.
[01:17:11.400 --> 01:17:19.160]   But then in terms of more fundamental arguments, I would just say the idea that with a Zoom
[01:17:19.160 --> 01:17:23.240]   call and this 1,500-word essay that you could spot talent I think is ludicrous.
[01:17:23.240 --> 01:17:23.560]   Really?
[01:17:23.560 --> 01:17:23.800]   Yeah.
[01:17:23.800 --> 01:17:26.280]   Well, the essay can be forged, obviously.
[01:17:26.280 --> 01:17:30.120]   You could say, "Well, let's go and put them in a testing center even then."
[01:17:30.120 --> 01:17:35.400]   If a dream job hangs in the balance, people figure out a way to cheat on that.
[01:17:35.400 --> 01:17:38.760]   And then in terms of the Zoom call, yeah, no.
[01:17:38.760 --> 01:17:42.760]   So I have been hiring artists.
[01:17:42.760 --> 01:17:45.000]   I'm admittedly a notoriously unreliable.
[01:17:45.000 --> 01:17:49.800]   But, yeah, what I found is that even work product is not that predictive of work product
[01:17:49.800 --> 01:17:54.200]   because someone that really wants to get a job can do five great pages in a timely manner,
[01:17:54.200 --> 01:17:57.720]   and then they keep you waiting for years for what you really want out of them.
[01:17:57.720 --> 01:18:00.120]   So it's just not that easy, right?
[01:18:00.120 --> 01:18:01.320]   Not even close.
[01:18:01.320 --> 01:18:06.040]   Now, I think the better argument is not saying that we can find talent with the Zoom call
[01:18:06.040 --> 01:18:09.880]   and the essay, but rather we can find talent by hiring people and watching them for a couple
[01:18:09.880 --> 01:18:10.280]   of months.
[01:18:10.280 --> 01:18:13.000]   That makes more sense.
[01:18:13.000 --> 01:18:19.560]   The problem with that is that it is incredibly expensive to go and hire people and watch
[01:18:19.560 --> 01:18:21.160]   them for a couple of months, right?
[01:18:21.160 --> 01:18:25.560]   You get a stack of applications, hundreds of people thick, and what you are doing there
[01:18:25.560 --> 01:18:29.800]   is trying to figure out ways to say no as quickly as possible and narrow it down.
[01:18:29.800 --> 01:18:35.320]   And you might do this knowing full well that there are three awesome people you've just
[01:18:35.320 --> 01:18:35.880]   thrown away.
[01:18:35.880 --> 01:18:41.560]   Because, yeah, well, I threw three awesome people away, and I also threw away 294 terrible
[01:18:41.560 --> 01:18:45.160]   people, and I don't have any way of finding those other people.
[01:18:45.160 --> 01:18:49.560]   So I call this the diamonds in the rough problem, and I say this is a lot of the reason why
[01:18:49.560 --> 01:18:51.160]   signaling matters so much.
[01:18:51.160 --> 01:18:57.400]   It's a way of getting out of the undifferentiated mass of people who may be or good, who knows,
[01:18:57.400 --> 01:18:59.160]   and into this better pool.
[01:18:59.160 --> 01:19:03.400]   Now, I do also say that another big error in Tyler's "I just watched him for a couple
[01:19:03.400 --> 01:19:09.720]   of months" story is that there is very strong evidence that for multiple reasons, hardly
[01:19:09.720 --> 01:19:14.600]   any employer does the strategy of hire, watch, and then fire if you're disappointing.
[01:19:14.600 --> 01:19:19.880]   The simplest reason is just, well, maybe you're at the 45th percentile of expectations, and
[01:19:19.880 --> 01:19:22.920]   then, well, he's kind of disappointing, but it's not worth going back to the drawing
[01:19:22.920 --> 01:19:23.960]   board again.
[01:19:23.960 --> 01:19:28.680]   But there's also a lot of social and emotional problems with firing people.
[01:19:28.680 --> 01:19:29.960]   People do not like firing.
[01:19:29.960 --> 01:19:34.280]   And then on top of it, of course, there's legal problems, which should not be discounted.
[01:19:34.280 --> 01:19:35.800]   Isn't this just what an internship is, though?
[01:19:36.520 --> 01:19:41.320]   Well, remember, when you apply for an internship, does everybody get the internship?
[01:19:41.320 --> 01:19:46.600]   Yes, but you can have a lower bar for the internship, and then it's like a call option
[01:19:46.600 --> 01:19:48.120]   on being able to hire them.
[01:19:48.120 --> 01:19:49.560]   Yes, so it is there.
[01:19:49.560 --> 01:19:54.040]   Although even there, a lot of what I say people are signaling, it's not just intelligence,
[01:19:54.040 --> 01:19:56.680]   it's not even work ethic, it's just sheer conformity.
[01:19:56.680 --> 01:19:58.120]   So here's the issue.
[01:19:58.120 --> 01:20:01.800]   You might say, "Why can't I just take my Harvard acceptance letter and get hired by
[01:20:01.800 --> 01:20:05.880]   Goldman Sachs and say, 'Hey, look, Harvard said I was good enough.
[01:20:05.880 --> 01:20:08.600]   Harvard has a 98% five-year graduation rate.
[01:20:08.600 --> 01:20:11.080]   Come on, let's just start right now.'"
[01:20:11.080 --> 01:20:16.600]   And what I say is that is so weird in our society for someone who gets into Harvard
[01:20:16.600 --> 01:20:20.360]   to say, "I don't want to go to college at all and show up with this odd offer," that
[01:20:20.360 --> 01:20:24.440]   this is something where Goldman very reasonably is like, "Yeah, well, he got into Harvard,
[01:20:24.440 --> 01:20:27.720]   but this guy's a freak, and we're worried about it."
[01:20:27.720 --> 01:20:30.280]   So I think it's basically the same problem what you're talking about.
[01:20:30.280 --> 01:20:33.880]   But couldn't Goldman say, "Okay, well, maybe you don't want to hire him full-time,
[01:20:33.880 --> 01:20:35.640]   but let's see if we can give him an internship."
[01:20:35.640 --> 01:20:37.240]   And in fact, internships are very common.
[01:20:37.240 --> 01:20:38.520]   Yes.
[01:20:38.520 --> 01:20:42.680]   Although, again, you want to give the internships to people that are checking all the boxes.
[01:20:42.680 --> 01:20:46.200]   The person who is doing the weird thing, you're nervous.
[01:20:46.200 --> 01:20:51.400]   Now, by the way, I've multiple times been on panels where there's some business leader,
[01:20:51.400 --> 01:20:56.440]   and he'll say, "In the business world today, we don't care about credentials.
[01:20:56.440 --> 01:20:59.240]   We only care about hard, demonstrable skills."
[01:20:59.240 --> 01:21:03.640]   And then I always ask the same question, "All right, so how many uncredentialed people have
[01:21:03.640 --> 01:21:07.080]   you personally hired for high-skilled jobs?"
[01:21:07.080 --> 01:21:10.120]   Like, "Well, we haven't done any, but I read in the Wall Street Journal."
[01:21:10.120 --> 01:21:14.120]   Like, "Aha, you're acting as if you've got some first-hand experience.
[01:21:14.120 --> 01:21:20.200]   You're just repeating what you read in the freaking newspaper, which writes stories about
[01:21:20.200 --> 01:21:21.800]   stuff that is atypical.
[01:21:21.800 --> 01:21:28.200]   Of course, you don't write a story about something everybody knows about that's familiar to people.
[01:21:28.200 --> 01:21:33.320]   You go and dredge up some weird platypus and then show everybody.
[01:21:33.320 --> 01:21:34.120]   And say, "Huh?
[01:21:34.120 --> 01:21:36.440]   Platypuses are taking over the ecosystem."
[01:21:36.440 --> 01:21:39.800]   Did this happen at your podcast with Andreessen, by the way?
[01:21:39.800 --> 01:21:43.960]   That would be plausible that it would happen.
[01:21:43.960 --> 01:21:46.920]   So, he said, "We only hire on demonstrable skills."
[01:21:46.920 --> 01:21:50.360]   And I asked him, "So, well, who have you hired without credentials?"
[01:21:50.360 --> 01:21:51.080]   And then what did he say?
[01:21:51.080 --> 01:21:51.640]   I don't remember.
[01:21:51.640 --> 01:21:53.960]   I don't know if you asked him directly, but that was his claim.
[01:21:53.960 --> 01:21:56.520]   I don't know if you followed up that way, but yeah.
[01:21:56.520 --> 01:21:59.400]   Yeah, so here's a good way of thinking about NCS.
[01:21:59.400 --> 01:22:03.960]   So, I have heard that top firms, Google, will sometimes hire winners of contests.
[01:22:03.960 --> 01:22:08.040]   But when I ask people there, "So, how many standardly credentialed employees do you have
[01:22:08.040 --> 01:22:08.760]   in your programming?
[01:22:08.760 --> 01:22:11.320]   And how many contest winners without credentials?"
[01:22:11.320 --> 01:22:15.240]   It's like, "Yeah, three contest winners, thousands of regular credential workers."
[01:22:15.240 --> 01:22:20.600]   So, basically, you have to walk on water to get hired by these firms without having the
[01:22:20.600 --> 01:22:21.400]   regular credentials.
[01:22:21.400 --> 01:22:27.240]   You originally traveled to Eastern Europe in the aftermath of the Ukraine war.
[01:22:27.240 --> 01:22:28.360]   What was that experience like?
[01:22:28.360 --> 01:22:29.800]   And I took my 12-year-old son there, too.
[01:22:29.800 --> 01:22:31.240]   A lot of people are saying, "You're crazy.
[01:22:31.240 --> 01:22:31.960]   What are you doing?
[01:22:31.960 --> 01:22:32.840]   Why would you go there?"
[01:22:32.840 --> 01:22:35.160]   I was super glad that I did.
[01:22:35.160 --> 01:22:37.400]   It was an incredibly exciting trip.
[01:22:37.400 --> 01:22:41.960]   What happened, actually, was that my open borders had been translated into three Eastern
[01:22:41.960 --> 01:22:45.960]   European languages, been translated into Polish, Hungarian, and Czech.
[01:22:45.960 --> 01:22:48.360]   And then I also spoke in Slovakia.
[01:22:48.360 --> 01:22:52.280]   And I had gotten a few different versions, but the Slovakian said, "Yeah, we can read
[01:22:52.280 --> 01:22:52.680]   Czech.
[01:22:52.680 --> 01:22:53.480]   Totally no problem.
[01:22:53.480 --> 01:22:54.680]   It's basically the same language."
[01:22:54.680 --> 01:22:57.720]   So, I got to give talks in all four of those countries.
[01:22:58.680 --> 01:23:08.440]   Especially Poland was really exciting because you could see the pro-Ukrainian, anti-Russian
[01:23:08.440 --> 01:23:10.040]   enthusiasm in the streets.
[01:23:10.040 --> 01:23:17.320]   I was by the train station in Krakow, and there was a Polish guy just screaming, "Fuck
[01:23:17.320 --> 01:23:18.200]   Putin!
[01:23:18.200 --> 01:23:20.040]   Glory to Ukraine!"
[01:23:20.040 --> 01:23:21.400]   And I was saying, "Is that guy drunk?
[01:23:21.400 --> 01:23:22.120]   No, he's not drunk.
[01:23:22.120 --> 01:23:24.840]   He's just a Pole speaking his mind here."
[01:23:26.840 --> 01:23:28.600]   And then I went to the train stations.
[01:23:28.600 --> 01:23:29.960]   They are packed with refugees.
[01:23:29.960 --> 01:23:34.920]   What I did not realize, the refugees were in fantastic spirits because the welcome of
[01:23:34.920 --> 01:23:40.120]   the Polish people, especially, really all over, but the Poles stood out, was so warm,
[01:23:40.120 --> 01:23:44.760]   so strong that the people there were actually feeling good about the situation.
[01:23:44.760 --> 01:23:50.120]   Just to look at their faces, just imagine the stress of fleeing a war zone with your
[01:23:50.120 --> 01:23:50.440]   kids.
[01:23:50.440 --> 01:23:52.520]   That's what was going on.
[01:23:52.520 --> 01:23:56.360]   But I also got to learn some amazing things when I was there.
[01:23:56.360 --> 01:24:01.080]   Since I am the author of Open Orders, you can definitely predict that I would say, "Ah,
[01:24:01.080 --> 01:24:03.080]   well, letting in a lot of refugees won't be a big deal."
[01:24:03.080 --> 01:24:10.920]   But Poland increased its population by 10% in a month, and the country looked fine.
[01:24:10.920 --> 01:24:14.440]   No one there was complaining.
[01:24:14.440 --> 01:24:15.480]   I'm like, "I can see why."
[01:24:15.480 --> 01:24:17.560]   This is not ideology.
[01:24:17.560 --> 01:24:19.640]   This is me walking around and looking at stuff.
[01:24:21.000 --> 01:24:23.000]   Poland was able to go and do this.
[01:24:23.000 --> 01:24:23.320]   Why?
[01:24:23.320 --> 01:24:25.240]   Because where there's a will, there's a way.
[01:24:25.240 --> 01:24:29.320]   There can be a thousand refugees from a country you don't like.
[01:24:29.320 --> 01:24:32.680]   You put it on the news and people say, "Oh, we just couldn't possibly absorb them.
[01:24:32.680 --> 01:24:33.880]   We're at our absolute breaking point.
[01:24:33.880 --> 01:24:34.600]   This is terrible."
[01:24:34.600 --> 01:24:40.040]   On the other hand, you can have millions of refugees come into your country, and if you
[01:24:40.040 --> 01:24:44.280]   like them and sympathize with them, then it's all hunky-dory and it's fine.
[01:24:44.280 --> 01:24:47.640]   Polish policy was also excellent.
[01:24:47.640 --> 01:24:50.600]   Ukrainians are allowed to work the day they show up.
[01:24:51.160 --> 01:24:57.400]   Normally, what you do with the refugees is you say, "Well, you can come here because
[01:24:57.400 --> 01:25:01.720]   you'll be dead if you stay in your own country, but we don't want you getting a job," which
[01:25:01.720 --> 01:25:06.280]   is, as we all know, a bad thing for a human being to do, to produce and contribute to
[01:25:06.280 --> 01:25:07.240]   society.
[01:25:07.240 --> 01:25:12.280]   Much better that we just keep you a gay semi-prisoner on welfare for a few years.
[01:25:12.280 --> 01:25:16.040]   That sounds like a much better thing to do, is normal.
[01:25:17.000 --> 01:25:21.960]   Poland was doing the best thing, where the day you show up, you can get a job here.
[01:25:21.960 --> 01:25:23.240]   You're totally legal to work.
[01:25:23.240 --> 01:25:28.440]   This is a path towards having them become product members of society.
[01:25:28.440 --> 01:25:32.200]   Just to realize how amazing what they're doing in Poland is, this would be like the
[01:25:32.200 --> 01:25:37.480]   US took 30 million refugees in a month, 33 million refugees in a month.
[01:25:37.480 --> 01:25:41.240]   Americans freak out over 10,000.
[01:25:42.920 --> 01:25:47.560]   Just to realize how phony and bogus the complaints are.
[01:25:47.560 --> 01:25:50.760]   Really, it is just a matter of believing and seeing.
[01:25:50.760 --> 01:25:54.920]   If you think that refugees are bad and that immigrants are bad, you will see bad things
[01:25:54.920 --> 01:25:55.880]   happening.
[01:25:55.880 --> 01:25:59.320]   On the other hand, if you're supportive and have a can-do attitude, then you'll say, "Hey,
[01:25:59.320 --> 01:26:01.000]   this is completely doable."
[01:26:01.000 --> 01:26:01.240]   Right.
[01:26:01.240 --> 01:26:04.680]   But I guess if you're skeptical that Americans will have a can-do attitude about it, then
[01:26:04.680 --> 01:26:05.080]   it's still—
[01:26:05.080 --> 01:26:05.240]   Yes.
[01:26:05.240 --> 01:26:11.080]   Well, the can-do attitude mostly just comes down to, "Are you going to get out of the
[01:26:11.080 --> 01:26:12.840]   way and let them do their thing?"
[01:26:12.840 --> 01:26:16.040]   Yeah, so that's the real problem, right?
[01:26:16.040 --> 01:26:18.360]   It's not an argument against accepting them and letting them work.
[01:26:18.360 --> 01:26:22.440]   It's an argument that we're not going to do it and we'll do the wrong thing.
[01:26:22.440 --> 01:26:25.160]   But I remember in The Myth of the Rational Voter, or maybe it was one of the essays in
[01:26:25.160 --> 01:26:29.320]   the book, you point out that a lot of times what liberal economists do is they'll defend
[01:26:29.320 --> 01:26:32.840]   some policy based on the optimal implementation of it.
[01:26:32.840 --> 01:26:35.160]   Like, Paul Grumman would defend the Hagoovian tax.
[01:26:35.160 --> 01:26:35.560]   Yes, yes.
[01:26:35.560 --> 01:26:39.000]   But then in effect, that'll be like arcane environmental regulations.
[01:26:39.000 --> 01:26:39.240]   Yeah.
[01:26:39.240 --> 01:26:40.440]   I wonder if it's similar here where it's like—
[01:26:40.440 --> 01:26:43.160]   Well, that's why in Open Borders, I defend immigration as it really is.
[01:26:43.160 --> 01:26:46.200]   I don't go and say, "We have to do a bunch of other things first."
[01:26:46.200 --> 01:26:50.760]   I say that it's fantastic right here, right now in the real world.
[01:26:50.760 --> 01:26:55.960]   Again, of course, I do have that chapter on Kiel Solutions where I try to meet people
[01:26:55.960 --> 01:26:57.080]   who disagree halfway.
[01:26:57.080 --> 01:27:05.160]   But really, what I learned in Poland is my view of what's doable was expanded quite a
[01:27:05.160 --> 01:27:05.400]   bit.
[01:27:05.400 --> 01:27:08.680]   I think I would have said, "Maybe you can take in two or three percent repopulation a
[01:27:08.680 --> 01:27:10.520]   month without it looking really bad."
[01:27:10.520 --> 01:27:15.080]   Now, again, I'm also the kind of person who will say, "The train station's going to
[01:27:15.080 --> 01:27:18.200]   be full of human misery," and I would have said, "This is still way better than trapping
[01:27:18.200 --> 01:27:19.480]   people in a war zone."
[01:27:19.480 --> 01:27:21.320]   So, I'm that kind of a person.
[01:27:21.320 --> 01:27:23.720]   I'm someone who always says, "Compared to what?
[01:27:23.720 --> 01:27:28.120]   If they're here crying with their children in a train station, it's probably because
[01:27:28.120 --> 01:27:31.000]   back home they would have been crying over a dead body instead."
[01:27:31.000 --> 01:27:35.560]   So, this is still a big improvement, and this is still actually what we should be doing.
[01:27:35.560 --> 01:27:37.720]   But in Poland, I didn't have to make any hard argument.
[01:27:37.720 --> 01:27:41.080]   I could just walk through the train stations full of refugees, and they're happy, and
[01:27:41.080 --> 01:27:45.640]   the kids are playing, and they got their puppies, and there's dog feeding stations, for real.
[01:27:45.640 --> 01:27:47.240]   It's like, "Wow."
[01:27:47.240 --> 01:27:54.200]   I don't have to go and convince myself through logic that this is the best thing that can
[01:27:54.200 --> 01:27:55.880]   happen or the least bad thing that can happen.
[01:27:55.880 --> 01:28:01.320]   I can just walk around and see people are happy and are adjusting to a new life.
[01:28:01.320 --> 01:28:07.320]   How should decolonization have been done to increase the odds of a competent and free
[01:28:07.320 --> 01:28:07.960]   market government?
[01:28:07.960 --> 01:28:11.480]   And also avoiding a total bloodbath.
[01:28:11.480 --> 01:28:12.840]   Let's not forget that.
[01:28:12.840 --> 01:28:17.400]   So, let's say you think you're the opinion that it had to be done, or it was inevitable
[01:28:17.400 --> 01:28:17.800]   at some point.
[01:28:17.800 --> 01:28:20.680]   How could it have been done so that it had the optimal outcome?
[01:28:20.680 --> 01:28:25.320]   First of all, with really high credibility.
[01:28:25.320 --> 01:28:29.000]   This is at a long timetable.
[01:28:29.000 --> 01:28:31.880]   This is where you say, "Look, here's the timetable.
[01:28:31.880 --> 01:28:36.600]   We are going to be partitioning India over the course of 20 years."
[01:28:37.320 --> 01:28:38.440]   We are staying there.
[01:28:38.440 --> 01:28:46.200]   Even tying your hand and saying, "Look, we are issuing a pile of, let's see.
[01:28:46.200 --> 01:28:50.440]   We're issuing government securities where you have to pay us money unless we go and
[01:28:50.440 --> 01:28:51.880]   just give up in the middle of the game."
[01:28:51.880 --> 01:28:55.560]   So, basically, you've got to pre-commit saying, "Look, here is our goal.
[01:28:55.560 --> 01:29:00.360]   Our goal is going to be a peaceful, free, rich India.
[01:29:00.360 --> 01:29:05.320]   We have a plan for how we're going to do this, and we're going to stick to it, and we are
[01:29:05.320 --> 01:29:08.520]   ready to lose a whole lot of our soldiers in order to do this.
[01:29:08.520 --> 01:29:15.080]   Then we are going to go and find people that we are going to very gradually transition
[01:29:15.080 --> 01:29:15.960]   power to.
[01:29:15.960 --> 01:29:17.240]   They have to be reasonable people.
[01:29:17.240 --> 01:29:22.680]   If these are people that are inciting pogroms, we are going to get rid of them.
[01:29:22.680 --> 01:29:25.960]   We are not going to tolerate that kind of thing here.
[01:29:25.960 --> 01:29:32.440]   This is going to be an orderly transition where all of Her Majesty's subjects can anticipate
[01:29:33.160 --> 01:29:37.000]   surviving and a future where they are at peace.
[01:29:37.000 --> 01:29:41.320]   Now, again, probably a lot of people don't realize how badly the partition of India was
[01:29:41.320 --> 01:29:42.600]   botched and decolonization.
[01:29:42.600 --> 01:29:47.240]   But the chaos makes it hard to get accurate numbers, but there's a lot of people saying
[01:29:47.240 --> 01:29:49.480]   millions of people died in pogroms at the end.
[01:29:49.480 --> 01:29:52.440]   So, some of you say, "Oh, maybe it was only 500,000."
[01:29:52.440 --> 01:29:54.680]   This was a complete disaster.
[01:29:54.680 --> 01:29:57.960]   It really came down to the British said they were never going to leave until they did,
[01:29:57.960 --> 01:29:58.600]   really quickly.
[01:29:58.600 --> 01:30:00.200]   So, that's not what you do.
[01:30:00.760 --> 01:30:05.640]   What made the denazification and the US occupation of Japan so successful?
[01:30:05.640 --> 01:30:08.280]   Yeah, because they started off by completely crushing their enemies.
[01:30:08.280 --> 01:30:10.520]   Completely crushing.
[01:30:10.520 --> 01:30:16.760]   And here's the key thing is that in those occupations, there were two things they're
[01:30:16.760 --> 01:30:18.360]   pushing, democracy and human rights.
[01:30:18.360 --> 01:30:20.680]   And they pushed human rights a lot more than democracy.
[01:30:20.680 --> 01:30:23.640]   So, basically, they said, "Yes, we're going to have elections.
[01:30:23.640 --> 01:30:30.120]   And as long as you are completely committed to the denazification of Germany, you can
[01:30:30.120 --> 01:30:33.000]   be elected and you can have a little bit of power.
[01:30:33.000 --> 01:30:37.720]   And then we're going to very slowly devolve power on you while it remains completely clear
[01:30:37.720 --> 01:30:41.080]   that you have no Nazi sympathies, that none of this stuff is going to come back."
[01:30:41.080 --> 01:30:47.400]   So, basically, what happened is there's a complete crushing of what existed before,
[01:30:47.400 --> 01:30:51.160]   and then a rebuilding where democracy is a low priority.
[01:30:51.160 --> 01:30:54.680]   Is democracy yielding good results?
[01:30:54.680 --> 01:30:56.840]   Then we can turn the democracy dial up a little bit.
[01:30:56.840 --> 01:30:58.120]   Same thing in Japan.
[01:30:59.080 --> 01:31:03.000]   So, of course, there's also a whole lot of people that really bad war criminals wind
[01:31:03.000 --> 01:31:04.120]   up getting sentences.
[01:31:04.120 --> 01:31:05.320]   They're quite light overall.
[01:31:05.320 --> 01:31:12.520]   So, if I remember correctly, under 1,000 German war criminals got executed by the US.
[01:31:12.520 --> 01:31:14.840]   You just think about how many probably ought to have gotten executed.
[01:31:14.840 --> 01:31:16.920]   100,000 maybe.
[01:31:16.920 --> 01:31:22.040]   So, people who really had blood on their hands, people who ordered deaths of innocent people
[01:31:22.040 --> 01:31:23.480]   when they could have just not done it.
[01:31:23.480 --> 01:31:28.360]   And again, there's a famous book, Hitler's Willing Executioner, saying this was not
[01:31:28.920 --> 01:31:32.680]   people killing – this was not primarily people murdering innocent people under duress,
[01:31:32.680 --> 01:31:35.720]   where there's a gun at your head saying you shoot another person.
[01:31:35.720 --> 01:31:40.840]   Because what the author looked at that book was what happened to Germans who refused to
[01:31:40.840 --> 01:31:42.680]   participate, and hardly any of them were.
[01:31:42.680 --> 01:31:43.880]   What was the name of this book?
[01:31:43.880 --> 01:31:45.640]   It's called Hitler's Willing Executioner.
[01:31:45.640 --> 01:31:50.040]   Hardly any ethnic Germans who refused to cooperate, even soldiers, were punished.
[01:31:50.040 --> 01:31:52.840]   Cooperating in genocide were punished.
[01:31:52.840 --> 01:31:56.440]   Other than by saying, "Oh, well, we're going to have to transfer you then if you
[01:31:56.440 --> 01:31:58.760]   won't go and murder innocent people, what a jerk you'll be."
[01:31:58.760 --> 01:32:00.200]   So, there's that.
[01:32:00.200 --> 01:32:03.480]   Yeah, but basically that's the kind of thing that should have been done.
[01:32:03.480 --> 01:32:06.840]   Here is the way that I think about it.
[01:32:06.840 --> 01:32:09.720]   You want to do it from a position of strength.
[01:32:09.720 --> 01:32:14.600]   You don't wait before the fanatics have the upper hand and there's blood on the streets
[01:32:14.600 --> 01:32:16.680]   and then say, "Oh, gee, what do we do now?"
[01:32:16.680 --> 01:32:19.640]   You want to jump the gun when things are peaceful.
[01:32:19.640 --> 01:32:23.480]   This is where you say, "We have now, in this time of complete peace and harmony,
[01:32:23.480 --> 01:32:25.720]   worked out a plan for decolonization.
[01:32:25.720 --> 01:32:26.840]   And here's how it's going to work."
[01:32:26.840 --> 01:32:31.160]   So, you never want to make it look like your hand is being forced.
[01:32:31.160 --> 01:32:35.400]   You never want to let fanatics and bloodthirsty people have their status raised by standing
[01:32:35.400 --> 01:32:36.760]   up to you successfully.
[01:32:36.760 --> 01:32:41.400]   You want it to all be happening over their heads so that they just look like losers and
[01:32:41.400 --> 01:32:42.040]   crazy people.
[01:32:42.040 --> 01:32:46.200]   Then you find some people that want to work for a decent, peaceful society.
[01:32:46.200 --> 01:32:50.280]   And also, of course, there's people who are not complete mush heads like Gandhi.
[01:32:52.120 --> 01:32:53.080]   I'll just stick my neck out.
[01:32:53.080 --> 01:32:59.320]   So, Gandhi, he was an apostle of nonviolence.
[01:32:59.320 --> 01:33:00.120]   Good for that.
[01:33:00.120 --> 01:33:04.040]   But he was also just someone who was so dominated by wishful thinking and just trying to pretend
[01:33:04.040 --> 01:33:05.560]   it was not like that.
[01:33:05.560 --> 01:33:10.600]   Pogroms should not be expected after the transition, and they totally were reasonably expected.
[01:33:10.600 --> 01:33:15.640]   It's like a lot of pie-in-the-sky nonsense that he preached.
[01:33:15.640 --> 01:33:19.880]   He's not, like himself, a mass murderer.
[01:33:19.880 --> 01:33:27.400]   He's just a touchy-feely person who should be nowhere near any important decision.
[01:33:27.400 --> 01:33:28.600]   Well, that's what I wanted to ask you.
[01:33:28.600 --> 01:33:31.400]   Yeah, he might have been a good therapist, something like that.
[01:33:31.400 --> 01:33:36.280]   Someone who had a lot of sympathy for other people, but not a reasonable person.
[01:33:36.280 --> 01:33:40.040]   Well, that's really interesting because in the book on political demagoguery,
[01:33:40.040 --> 01:33:44.520]   you make the point that if we were to judge political leaders by normal moral standards,
[01:33:44.520 --> 01:33:45.480]   we would think they were monsters.
[01:33:45.480 --> 01:33:48.920]   The interesting thing with the Gandhi example, and maybe this is a general thing,
[01:33:49.560 --> 01:33:53.560]   is that maybe the people who are moral heroes in an ordinary context,
[01:33:53.560 --> 01:33:55.640]   they make the worst politicians, right?
[01:33:55.640 --> 01:33:59.880]   Do you think there's any correlation between how moral somebody is as a human and
[01:33:59.880 --> 01:34:01.640]   how moral the government they lead is?
[01:34:01.640 --> 01:34:03.320]   They're definitely not the worst people.
[01:34:03.320 --> 01:34:11.080]   The very worst people are people who come to power self-consciously wanting to commit mass murder.
[01:34:11.080 --> 01:34:12.600]   Those are the worst.
[01:34:14.920 --> 01:34:20.040]   If you have a very conventional moral view where the only thing that you judge people by is how
[01:34:20.040 --> 01:34:23.560]   caring they are, then yeah, I think those people make bad leaders.
[01:34:23.560 --> 01:34:28.520]   If you have an effective altruism view, however, where it's not just feeling a lot of caring
[01:34:28.520 --> 01:34:34.760]   emotions, it's being very committed to thinking clearly about the best way to get good results,
[01:34:34.760 --> 01:34:37.800]   then I'll say those people are the heroic people in my view.
[01:34:41.240 --> 01:34:46.680]   If any of them ever got power, I think they would actually generally be good leaders.
[01:34:46.680 --> 01:34:50.760]   Again, of course, they need some experience too, but basically that is the right profile
[01:34:50.760 --> 01:34:56.360]   for a good leader, someone who is very caring but at the same time is very logical.
[01:34:56.360 --> 01:34:57.640]   Yes.
[01:34:57.640 --> 01:35:04.200]   But is there a correlation between conventional virtues that are, I say, honesty, right?
[01:35:04.200 --> 01:35:09.720]   Or being kind towards strangers and being good in an effective altruist sense?
[01:35:09.720 --> 01:35:11.240]   Or are those just completely unrelated?
[01:35:11.240 --> 01:35:12.520]   Let's see.
[01:35:12.520 --> 01:35:17.160]   Kindness towards strangers, I mean, yeah, the sort of kindness towards strangers you
[01:35:17.160 --> 01:35:19.320]   meet firsthand, like being willing to give to a homeless guy.
[01:35:19.320 --> 01:35:20.760]   Yeah.
[01:35:20.760 --> 01:35:24.200]   I mean, again, my guess is that most effective altruists, they are people who would like to
[01:35:24.200 --> 01:35:27.880]   give to homeless people, but they just realize that it's not a good use of the money.
[01:35:27.880 --> 01:35:30.680]   And so I think those are people where they sort of have to suppress their desire to give
[01:35:30.680 --> 01:35:31.240]   to the homeless.
[01:35:31.240 --> 01:35:39.320]   Now, the honesty one, this is one where a lot of politicians say you have to be dishonest to get
[01:35:39.320 --> 01:35:39.880]   things done.
[01:35:39.880 --> 01:35:44.200]   I mean, I would say that there's this whole literature on credibility saying, "No, what
[01:35:44.200 --> 01:35:50.360]   you really want is to be honest in a very credible way such that when you say I'm going
[01:35:50.360 --> 01:35:52.600]   to do something in 20 years, people believe you."
[01:35:52.600 --> 01:35:56.280]   So again, I think that's a lot of what you would have needed for effective decolonization
[01:35:56.280 --> 01:36:00.760]   would be to have people there of ironclad honesty.
[01:36:00.760 --> 01:36:05.080]   So when they say, "Look, I am not leaving just because there are terrorist attacks.
[01:36:05.880 --> 01:36:10.280]   I'm willing to give up 100,000 British soldiers to terrorist attacks before we walk out.
[01:36:10.280 --> 01:36:13.000]   And when I say it, I mean it.
[01:36:13.000 --> 01:36:14.440]   You know I do."
[01:36:14.440 --> 01:36:18.760]   And I think that is actually an important trait for leadership.
[01:36:18.760 --> 01:36:20.200]   I mean, here's an interesting thing.
[01:36:20.200 --> 01:36:23.000]   I don't think Tyler would in any way see this as a negative.
[01:36:23.000 --> 01:36:29.000]   On the one hand, often he does like to impishly say, "Oh, well, you have to be dishonest to
[01:36:29.000 --> 01:36:30.440]   get things done," and so on.
[01:36:30.440 --> 01:36:35.240]   And yet his own leadership style is ultra-honest and it's based upon everyone, not just in
[01:36:35.240 --> 01:36:39.000]   advance believing that something is going to be good for them, but after the fact coming
[01:36:39.000 --> 01:36:42.920]   away and saying, "Wow, that was real leadership," he went and actually left people.
[01:36:42.920 --> 01:36:46.520]   People left the deal years later feeling like things worked out.
[01:36:46.520 --> 01:36:49.560]   So integrity and honesty.
[01:36:49.560 --> 01:36:56.680]   So while we can easily come up with hypotheticals where they're bad and a leader, I think the
[01:36:56.680 --> 01:37:00.600]   real thing is the old saying of "Under-promise, over-deliver."
[01:37:00.600 --> 01:37:05.160]   Don't promise more than you're really willing to do and then try to exceed expectations.
[01:37:06.120 --> 01:37:08.920]   That I would say is a trait of a leader, but I'd say that's not dishonesty.
[01:37:08.920 --> 01:37:13.320]   If someone promises something and they do more for you, they say, "Liar!"
[01:37:13.320 --> 01:37:14.200]   No, it's not.
[01:37:14.200 --> 01:37:15.400]   I wasn't a liar.
[01:37:15.400 --> 01:37:17.160]   I did what I said and I also did more.
[01:37:17.160 --> 01:37:22.840]   Now, I know in general you're not a fan of revolution, but if you had to say it, what
[01:37:22.840 --> 01:37:23.640]   is the best revolution?
[01:37:23.640 --> 01:37:25.880]   The most justified or the one that has the best effects?
[01:37:25.880 --> 01:37:26.280]   Right.
[01:37:26.280 --> 01:37:31.320]   So this is one where there's the cheesy thing of picking something that I don't really consider
[01:37:31.320 --> 01:37:33.080]   revolution and then saying other people do.
[01:37:33.080 --> 01:37:38.360]   So the collapse of communism in Eastern Europe, you could call it a revolution.
[01:37:38.360 --> 01:37:42.120]   I don't think it really was because it was just way too peaceful to count.
[01:37:42.120 --> 01:37:51.800]   If you'll count that, then that is very likely the best example of a revolution.
[01:37:51.800 --> 01:37:54.440]   But again, what you really want is a bloodbath.
[01:37:54.440 --> 01:38:01.960]   In terms of wars, I will say the Korean War is, I think, the best record.
[01:38:01.960 --> 01:38:05.000]   It basically saved two-thirds of the country and it turned out great.
[01:38:05.000 --> 01:38:08.440]   We have a reasonable counterfactual for how awful it would have been in North Korea.
[01:38:08.440 --> 01:38:15.720]   North Korean propaganda might claim it was a revolution, but again, it was a civil war,
[01:38:15.720 --> 01:38:16.840]   not a revolution.
[01:38:16.840 --> 01:38:21.880]   In terms of what would be the really best example, of course, a lot of people want to
[01:38:21.880 --> 01:38:23.960]   do the American Revolution, but I'm not a fan of that.
[01:38:23.960 --> 01:38:26.040]   What could I do?
[01:38:26.680 --> 01:38:33.560]   By the way, on that, you could argue that Britain abolished slavery earlier, but one
[01:38:33.560 --> 01:38:37.640]   interesting argument I've heard recently, not about this in particular, but just generally
[01:38:37.640 --> 01:38:42.040]   made the argument that the end of slavery was a lot more contingent, that there really
[01:38:42.040 --> 01:38:46.440]   wasn't a strong reason for thinking that slavery had to end then, as you told me.
[01:38:46.440 --> 01:38:49.000]   It was pretty profitable at the time.
[01:38:49.000 --> 01:38:51.720]   So if you think it was just a bunch of random things at the time, one of which was the American
[01:38:51.720 --> 01:38:56.120]   Revolution, which led to the end of slavery, then that's not the strong argument.
[01:38:57.080 --> 01:39:02.200]   My view is that it is this British-based anti-slavery movement that's really key,
[01:39:02.200 --> 01:39:04.440]   which then does spread to the colonies.
[01:39:04.440 --> 01:39:06.600]   I think it probably would have spread stronger, actually.
[01:39:06.600 --> 01:39:10.120]   And furthermore, of course, the British have a much better record on getting rid of slavery
[01:39:10.120 --> 01:39:10.600]   peacefully.
[01:39:10.600 --> 01:39:15.000]   Now, it would have cost so much more to do it in the US because there were so many slaves,
[01:39:15.000 --> 01:39:16.520]   so it's complicated.
[01:39:16.520 --> 01:39:18.920]   Let's see.
[01:39:18.920 --> 01:39:24.680]   But again, I'm still trying to come back to the best revolutions.
[01:39:24.680 --> 01:39:27.960]   Again, the other ones I'm thinking of are more of coups than revolutions.
[01:39:27.960 --> 01:39:35.080]   So I think a coup against the Muslim Brotherhood is not great, but still better than letting
[01:39:35.080 --> 01:39:37.960]   those fanatics take over.
[01:39:37.960 --> 01:39:39.320]   So let's see.
[01:39:39.320 --> 01:39:40.360]   I've really got to go through.
[01:39:40.360 --> 01:39:43.080]   The Glorious Revolution.
[01:39:43.080 --> 01:39:48.920]   That one, actually, my understanding is that it really was a real revolution and that people
[01:39:48.920 --> 01:39:52.120]   at the time tried to portray it as totally peaceful, but actually really was not.
[01:39:53.080 --> 01:39:55.720]   So that one, maybe.
[01:39:55.720 --> 01:40:02.680]   Actually, so I mean, if you were to go and do the first Russian revolution against the
[01:40:02.680 --> 01:40:08.120]   Tsar in February, the one that replaced the Tsar with the first democratic government
[01:40:08.120 --> 01:40:12.280]   of Russia, it's like, well, that one probably would have worked out okay if it hadn't been
[01:40:12.280 --> 01:40:13.320]   for Lenin.
[01:40:13.320 --> 01:40:16.280]   So it didn't work out, but it had potential.
[01:40:16.280 --> 01:40:18.920]   How contingent do you think history is overall?
[01:40:18.920 --> 01:40:24.920]   Let's say Lenin wasn't shipped back to Russia in World War I, does the communist takeover
[01:40:24.920 --> 01:40:27.560]   not happen, or was that kind of baked into the cake?
[01:40:27.560 --> 01:40:28.120]   Yeah.
[01:40:28.120 --> 01:40:33.320]   So that one, I know the details really well, and I'll give like 99% without Lenin.
[01:40:33.320 --> 01:40:38.280]   Even with all the other Bolsheviks, it wouldn't have happened because I actually know the
[01:40:38.280 --> 01:40:38.760]   facts.
[01:40:38.760 --> 01:40:40.520]   Read Richard Pipe's The Russian Revolution.
[01:40:40.520 --> 01:40:45.080]   The whole rest of the Bolshevik Party was planning and was taking part of the provisional
[01:40:45.080 --> 01:40:49.880]   government, and then Lenin shows up and reads them the Riot Act, and he has so much intellectual
[01:40:49.880 --> 01:40:52.440]   status, and I don't know what he had.
[01:40:52.440 --> 01:40:56.440]   But anyway, the whole group was against him, and he just reads the Riot Act, and they all
[01:40:56.440 --> 01:40:57.400]   say, "Yes, sir, Lenin.
[01:40:57.400 --> 01:40:58.760]   We're ready for revolution."
[01:40:58.760 --> 01:41:02.920]   So without Lenin, that would just not have happened.
[01:41:02.920 --> 01:41:05.880]   You'd say, well, maybe some years later, something similar would have happened.
[01:41:05.880 --> 01:41:13.160]   Again, if you know the facts, the whole revolution was based upon a tiny number of fanatics
[01:41:13.160 --> 01:41:18.040]   seizing power because they had 2,000 guys following orders in a country where no one
[01:41:18.040 --> 01:41:19.000]   else was following orders.
[01:41:19.000 --> 01:41:25.320]   If you just waited a little while, other forces would have rebuilt, and there would have been
[01:41:25.320 --> 01:41:30.920]   no hope for this tiny minority of lunatics to take over.
[01:41:30.920 --> 01:41:34.440]   So overall, I am a big believer in contingency.
[01:41:34.440 --> 01:41:38.600]   Of course, it does vary.
[01:41:38.600 --> 01:41:43.640]   So things like, was economic growth going to happen one way or another starting in 1800?
[01:41:43.640 --> 01:41:46.840]   Yeah, that, I think, was not so contingent if it didn't happen in Britain.
[01:41:46.840 --> 01:41:49.320]   Because basically, there's just too many things going on.
[01:41:49.320 --> 01:41:53.560]   There's a bunch of different scientific breakthroughs with obvious economic applications.
[01:41:53.560 --> 01:41:56.920]   There's a bunch of countries where they have a business class that's interested in making
[01:41:56.920 --> 01:41:59.080]   more money and trying these ideas out.
[01:41:59.080 --> 01:42:03.560]   So I think that was something where you can say it was quite inevitable.
[01:42:03.560 --> 01:42:08.520]   On the other hand, for almost anything involving major wars, like I say, almost all major
[01:42:08.520 --> 01:42:10.600]   wars could be avoided by one side giving in.
[01:42:10.600 --> 01:42:16.920]   The worry is that then that sets up a greater list of grievances for the next war.
[01:42:16.920 --> 01:42:19.800]   Yeah, well, yeah, that's what the hawks on both sides are always saying.
[01:42:19.800 --> 01:42:21.720]   Sometimes they're right.
[01:42:21.720 --> 01:42:22.440]   Sometimes they're wrong.
[01:42:22.440 --> 01:42:25.240]   Say they're wrong about half the time.
[01:42:25.240 --> 01:42:29.720]   So yeah, I mean, that basically is an argument that is true some of the time, but is just
[01:42:29.720 --> 01:42:30.920]   not at all reliable.
[01:42:30.920 --> 01:42:35.880]   Again, often what happens when you give in is people say, "Oh, well, I thought these
[01:42:35.880 --> 01:42:37.960]   people were completely unreasonable, and we could never make a deal with them.
[01:42:37.960 --> 01:42:41.880]   But it turns out they're not so bad, so we can now de-escalate and get back to peace."
[01:42:41.880 --> 01:42:43.480]   That happens, too.
[01:42:43.480 --> 01:42:45.400]   I mean, people are saying, "You couldn't do that with Hitler."
[01:42:45.400 --> 01:42:47.080]   Yeah, I know you couldn't do that with Hitler.
[01:42:47.080 --> 01:42:50.600]   Hitler was terrible, just to say the most controversial thing of this podcast.
[01:42:50.600 --> 01:42:57.160]   Hitler was not a person that you could negotiate with any long-term success.
[01:42:57.160 --> 01:43:01.400]   But hardly any world leaders are Hitler.
[01:43:01.400 --> 01:43:03.880]   Almost all of them actually can be negotiated with.
[01:43:04.760 --> 01:43:11.080]   Often what they want is so trivial, because politics is so based upon demagoguery, which
[01:43:11.080 --> 01:43:16.040]   means that small symbolic concessions are often all they need to go and thump their
[01:43:16.040 --> 01:43:18.440]   chests and say, "I am a great leader."
[01:43:18.440 --> 01:43:20.280]   Think about it.
[01:43:20.280 --> 01:43:24.040]   Indonesia for 30 years refuses to give up East Timor.
[01:43:24.040 --> 01:43:31.400]   It's like half of one insignificant island, but for all these decades, they're thumping
[01:43:31.400 --> 01:43:33.640]   their chest and saying, "We couldn't possibly do that.
[01:43:33.640 --> 01:43:36.120]   No, that will lead to total collapse."
[01:43:36.120 --> 01:43:37.800]   Endless nonsense.
[01:43:37.800 --> 01:43:38.520]   They're like, "All right, fine.
[01:43:38.520 --> 01:43:39.480]   We'll give it away."
[01:43:39.480 --> 01:43:40.120]   And they give it away.
[01:43:40.120 --> 01:43:41.560]   Now, it wasn't good for East Timor.
[01:43:41.560 --> 01:43:43.640]   The whole thing was a disaster.
[01:43:43.640 --> 01:43:45.480]   But still probably pretty bad.
[01:43:45.480 --> 01:43:52.120]   But it really is the case, as everybody knows in real life, that many conflicts can be avoided
[01:43:52.120 --> 01:43:53.720]   by giving another person what they want.
[01:43:53.720 --> 01:43:58.520]   And if you say that they'll just escalate their demands infinitely, there's a few people
[01:43:58.520 --> 01:43:59.000]   like that.
[01:43:59.000 --> 01:44:01.560]   But most people do not infinitely escalate their demands.
[01:44:01.560 --> 01:44:05.320]   Instead, most people will either say, "Oh, you gave me what I wanted.
[01:44:05.320 --> 01:44:05.640]   Good.
[01:44:05.640 --> 01:44:06.680]   End of story."
[01:44:06.680 --> 01:44:10.760]   Or maybe they'll go and periodically tax you with another demand, which is annoying,
[01:44:10.760 --> 01:44:14.200]   but is still much better than losing a friend or a contact.
[01:44:14.200 --> 01:44:20.120]   Given the irrationality and the demagoguery in the political system, why is it the case
[01:44:20.120 --> 01:44:25.320]   that the society we live in is relatively free, peaceful, prosperous?
[01:44:25.320 --> 01:44:27.720]   I mean, if the average voter is a National Socialist, why?
[01:44:27.720 --> 01:44:31.240]   I said, "Moderate National Socialist, Dworkesh."
[01:44:31.240 --> 01:44:31.960]   That's what I said.
[01:44:31.960 --> 01:44:35.000]   So there's a few things going on.
[01:44:35.000 --> 01:44:37.960]   So first thing is to just remember, well, usually we don't have this.
[01:44:37.960 --> 01:44:43.720]   So the norm is not peaceful, prosperous societies throughout human history.
[01:44:43.720 --> 01:44:46.920]   The norm is impoverished and war-prone societies.
[01:44:46.920 --> 01:44:52.440]   So always keep that in mind if you're saying, "Well, gee, but things are okay now."
[01:44:52.440 --> 01:44:55.080]   I mean, I often think of this as the look out the window test.
[01:44:55.080 --> 01:44:57.240]   "Hey, Dworkesh, is it on fire out there?"
[01:44:57.640 --> 01:45:00.920]   We're not going to turn the camera, but I think that listeners will believe that it
[01:45:00.920 --> 01:45:01.400]   is not on fire.
[01:45:01.400 --> 01:45:02.760]   "How else do you think we're getting this nice lighting?"
[01:45:02.760 --> 01:45:05.240]   Yeah, so it's not on fire.
[01:45:05.240 --> 01:45:06.200]   So there's that.
[01:45:06.200 --> 01:45:08.840]   But still, there is the interesting question, right?
[01:45:08.840 --> 01:45:10.920]   So what's going on with the exceptions?
[01:45:10.920 --> 01:45:13.800]   So a few things.
[01:45:13.800 --> 01:45:21.000]   So one is I think that rich, prosperous people societies generally have less crazy electorates.
[01:45:21.000 --> 01:45:26.200]   The political ideology of the society is just not as terrible as in other places.
[01:45:26.200 --> 01:45:30.280]   And if it's like, "Well, it seems pretty bad," all right, well, there's still not many people
[01:45:30.280 --> 01:45:33.400]   going saying, "We want to go and murder half the population," right?
[01:45:33.400 --> 01:45:36.120]   And you go to other societies, and there's actually people like that.
[01:45:36.120 --> 01:45:40.760]   Yeah, a friend of mine was in India, and he actually saw a pro-nuke Pakistan rally.
[01:45:40.760 --> 01:45:44.120]   All right, I assume this is not normal in India.
[01:45:44.120 --> 01:45:46.920]   Have you ever seen a pro-nuke Pakistan rally?
[01:45:46.920 --> 01:45:49.160]   No, but the naturalists can get pretty crazy.
[01:45:49.160 --> 01:45:49.400]   Yes.
[01:45:49.400 --> 01:45:51.240]   Like over there, they're celebrating.
[01:45:51.320 --> 01:45:53.160]   Yes, but it's like science saying, "Nuke Pakistan."
[01:45:53.160 --> 01:45:57.400]   Preemptively nuke a nuclear power?
[01:45:57.400 --> 01:45:58.440]   What do you think is going to happen?
[01:45:58.440 --> 01:46:08.280]   All right, so that kind of just true, fanatical, sociopathic, bloodthirsty, horrible stuff.
[01:46:08.280 --> 01:46:13.800]   It's common in a lot of societies, but it does seem to be quite reduced in the better
[01:46:13.800 --> 01:46:14.840]   functioning societies.
[01:46:14.840 --> 01:46:17.640]   So that's one thing, right?
[01:46:17.640 --> 01:46:23.560]   And then also, in these better functioning societies, while people's political views
[01:46:23.560 --> 01:46:29.080]   are not terrible, if you go and propose something really awful, even normal people will say,
[01:46:29.080 --> 01:46:32.200]   "No, I don't think that's such a good idea to go and murder all the billionaires.
[01:46:32.200 --> 01:46:34.840]   Maybe tax them at 90 percent, but murder them.
[01:46:34.840 --> 01:46:35.480]   That's too far."
[01:46:35.480 --> 01:46:43.320]   Whereas in most societies, you say that stuff, and then the cackling with glee, "Yeah, let's
[01:46:43.320 --> 01:46:49.240]   strangle the last billionaire in the intestines of the last right-wing talk show radio host.
[01:46:49.240 --> 01:46:52.920]   That is more of the human attitude."
[01:46:52.920 --> 01:46:56.920]   Which, again, is just think about how kids are.
[01:46:56.920 --> 01:47:02.360]   And I say the way that the kids are, this shows us what adults are feeling but hiding.
[01:47:02.360 --> 01:47:05.400]   So kids get angry and they want blood.
[01:47:05.400 --> 01:47:12.120]   And so more effective societies are ones where people are suppressing these atavistic desires
[01:47:12.120 --> 01:47:15.720]   to really just turn society into a total bloodbath.
[01:47:15.720 --> 01:47:17.080]   So there's that.
[01:47:17.080 --> 01:47:22.040]   Something else, starting with public opinion.
[01:47:22.040 --> 01:47:25.640]   I do think the public opinion obviously matters a lot in democracies.
[01:47:25.640 --> 01:47:28.040]   Even most dictatorships, the public opinion matters a lot.
[01:47:28.040 --> 01:47:29.320]   Dictators, they demagogue.
[01:47:29.320 --> 01:47:33.080]   They try to go and win the people over normally.
[01:47:33.080 --> 01:47:39.960]   It's easier because the people that would be the rivals are dead or in prison or terrified.
[01:47:39.960 --> 01:47:43.000]   So that sort of lets you weaponize the demagoguery.
[01:47:43.000 --> 01:47:45.720]   And you're like, "I'm the only one that gets to say I'm anointed by God."
[01:47:45.720 --> 01:47:47.800]   So there's that kind of thing.
[01:47:47.800 --> 01:47:50.840]   But even dictators generally want to be liked by their population.
[01:47:50.840 --> 01:47:52.200]   It makes ruling easier.
[01:47:52.200 --> 01:47:56.680]   Then you only have to terrify 10% of the population into obedience instead of 90%.
[01:47:56.680 --> 01:47:59.080]   So there's that.
[01:47:59.080 --> 01:48:03.880]   Then also just having a better leadership class probably matters too.
[01:48:03.880 --> 01:48:07.640]   Probably there's high correlation between the quality of leaders and the quality of
[01:48:07.640 --> 01:48:09.240]   the public, but not in every case.
[01:48:09.960 --> 01:48:11.880]   So that's another thing to look for.
[01:48:11.880 --> 01:48:17.320]   Then something else, just having constructive interest groups.
[01:48:17.320 --> 01:48:20.840]   For example, I have a pet theory.
[01:48:20.840 --> 01:48:23.160]   So I'm working on a book on housing regulation.
[01:48:23.160 --> 01:48:28.120]   My pet theory is that if we're not from lobbying from developers, basically zero things would
[01:48:28.120 --> 01:48:34.280]   be built in America because building things has almost no demagogic appeal.
[01:48:34.280 --> 01:48:38.440]   There's almost no one who emotionally gets a tear in their eye when they see a skyscraper
[01:48:38.440 --> 01:48:39.880]   go up or a new housing development.
[01:48:39.880 --> 01:48:40.680]   Other than Ayn Rand.
[01:48:40.680 --> 01:48:41.180]   Yeah.
[01:48:41.180 --> 01:48:45.720]   Very rare to be enthused about building stuff.
[01:48:45.720 --> 01:48:47.880]   Yet we need places to live.
[01:48:47.880 --> 01:48:48.840]   We need places to work.
[01:48:48.840 --> 01:48:49.880]   We need places to shop.
[01:48:49.880 --> 01:48:54.360]   On the other hand, there's a bunch of angry people whenever you try to build something.
[01:48:54.360 --> 01:48:57.000]   We call them NIMBYs who have an endless series of complaints.
[01:48:57.000 --> 01:49:06.600]   Traffic, parking, quality, the character of the neighborhood, pollution, on and on.
[01:49:06.600 --> 01:49:07.480]   Every possible complaint.
[01:49:08.120 --> 01:49:13.880]   So the reason why stuff gets built is that there are developers who probably do not really
[01:49:13.880 --> 01:49:18.680]   sincerely believe that they're heroes, but just come and say, "Hey, we can make our money
[01:49:18.680 --> 01:49:19.160]   building stuff.
[01:49:19.160 --> 01:49:21.320]   We provide jobs, income to the community.
[01:49:21.320 --> 01:49:22.120]   Let us build stuff.
[01:49:22.120 --> 01:49:23.720]   Please, please, please, please, please, please, please."
[01:49:23.720 --> 01:49:26.520]   That is, I think, the main reason why.
[01:49:26.520 --> 01:49:29.400]   So having constructive interest groups.
[01:49:29.400 --> 01:49:32.600]   Again, economists mostly talk about interest groups as being bad.
[01:49:32.600 --> 01:49:39.160]   But yeah, thinking about housing, I think that lobbying has very positive effects overall
[01:49:39.160 --> 01:49:40.040]   for housing.
[01:49:40.040 --> 01:49:41.320]   And actually also for immigration.
[01:49:41.320 --> 01:49:45.480]   I think a lot of the main reason why we have as much immigration as we do is that there's
[01:49:45.480 --> 01:49:47.240]   a bunch of corporations that are pushing for it.
[01:49:47.240 --> 01:49:52.440]   As far as developers go, why is it the case that they're not more powerful than they are?
[01:49:52.440 --> 01:49:55.720]   I mean, they're typically like Mancer or Olson, concentrated interest, right?
[01:49:55.720 --> 01:49:57.480]   Yeah, because Mancer was mostly wrong.
[01:49:57.480 --> 01:49:57.880]   Oh, really?
[01:49:57.880 --> 01:49:58.280]   Say more.
[01:49:59.800 --> 01:50:03.720]   Public opinion is much more important for policy than interest groups.
[01:50:03.720 --> 01:50:07.800]   Contrary to what Mancer or Olson said, if you really look at what interest groups do,
[01:50:07.800 --> 01:50:09.800]   normally they're trying to work in the fine print.
[01:50:09.800 --> 01:50:15.560]   Interest groups do not go and try to pass some overall change in US tax law or something
[01:50:15.560 --> 01:50:15.880]   like that.
[01:50:15.880 --> 01:50:18.040]   They say, "Look, that's going to be determined by public opinion.
[01:50:18.040 --> 01:50:20.040]   That's not the kind of thing I can affect.
[01:50:20.040 --> 01:50:26.600]   Maybe I can go and get a sentence change somewhere on page 1037 of the tax code.
[01:50:26.600 --> 01:50:27.560]   Maybe I could get that."
[01:50:28.440 --> 01:50:32.520]   So I say most of the policies that we have are ones that are supported by the general
[01:50:32.520 --> 01:50:33.320]   public.
[01:50:33.320 --> 01:50:36.920]   And you really have to look at details to see cases where interest groups are getting
[01:50:36.920 --> 01:50:39.960]   something that is actually unpopular with the public.
[01:50:39.960 --> 01:50:46.280]   So even things as seemingly straightforward as farm subsidies, economists say, "Oh, well,
[01:50:46.280 --> 01:50:49.560]   obviously most people don't want to go and pay those, but farmers get it."
[01:50:49.560 --> 01:50:50.280]   Yeah, think again.
[01:50:50.280 --> 01:50:53.480]   We look at public opinion, farm subsidies are actually very popular.
[01:50:53.480 --> 01:50:57.800]   And if you ask people that are not in farm states, "Why do you want farm subsidies?"
[01:50:57.800 --> 01:51:01.080]   Usually they just give very prosocial reasons like, "I want to make sure there's enough
[01:51:01.080 --> 01:51:01.320]   food."
[01:51:01.320 --> 01:51:04.360]   And you say, "Well, that's stupid.
[01:51:04.360 --> 01:51:08.360]   We only subsidize a handful of agricultural products, but they're all available."
[01:51:08.360 --> 01:51:13.480]   And it's like, "Well, that's already one step deeper than most voters ever have done
[01:51:13.480 --> 01:51:14.360]   or ever will do."
[01:51:14.360 --> 01:51:14.860]   Yeah.
[01:51:14.860 --> 01:51:17.720]   You know Charles Mann, the author of The Wizard and the Prophet?
[01:51:17.720 --> 01:51:18.360]   Have you?
[01:51:18.360 --> 01:51:19.240]   Yeah, yeah.
[01:51:19.240 --> 01:51:23.080]   I had him on recently and we actually talked about this because he's concerned about water
[01:51:23.080 --> 01:51:24.520]   shortages and shortages of food.
[01:51:25.320 --> 01:51:30.920]   And then, especially when they're used in inefficient ways, like you give it to cattle
[01:51:30.920 --> 01:51:32.280]   instead of consuming it directly.
[01:51:32.280 --> 01:51:33.240]   Yeah, sure, sure.
[01:51:33.240 --> 01:51:36.200]   And then so, yeah, I just pointed out, isn't there an obvious free market solution?
[01:51:36.200 --> 01:51:39.480]   The prices will rise if you're using it in an inefficient way and then it'll just go
[01:51:39.480 --> 01:51:40.360]   to the people who need it the most.
[01:51:40.360 --> 01:51:48.360]   And his claim was, ideally, yes, but the reality is if there's something that is physically
[01:51:48.360 --> 01:51:52.840]   necessary for people to survive, there's just not going to be a political will to put in
[01:51:53.400 --> 01:51:58.520]   actual pricing for water usage, for example, which solves a lot of water shortage problems.
[01:51:58.520 --> 01:52:01.640]   Yeah, it seems like a silly argument because we don't have to go and raise the price of
[01:52:01.640 --> 01:52:04.440]   water up to the level where a few people die of thirst.
[01:52:04.440 --> 01:52:10.440]   So basically, you could go and multiply the price by a factor of 10 or 100 and people
[01:52:10.440 --> 01:52:12.360]   will still be drinking all the water that they want.
[01:52:12.360 --> 01:52:18.200]   They'll just be taking quicker showers or farming will be happening in different parts
[01:52:18.200 --> 01:52:19.480]   of the country, that kind of thing.
[01:52:19.560 --> 01:52:23.640]   Yeah, but I think some of the problems are that in some regions of China where they have
[01:52:23.640 --> 01:52:25.400]   done this, the cost of water is-
[01:52:25.400 --> 01:52:26.120]   Ah, okay, okay.
[01:52:26.120 --> 01:52:27.160]   I was thinking about the US, yes.
[01:52:27.160 --> 01:52:27.640]   Oh, yeah, yeah, yeah.
[01:52:27.640 --> 01:52:29.000]   Like it's going to be a fraction of their income.
[01:52:29.000 --> 01:52:34.760]   But even there, can it really be that there's anywhere in China so poor that the main use
[01:52:34.760 --> 01:52:36.040]   of water is for drinking?
[01:52:36.040 --> 01:52:37.160]   Or are these farmers?
[01:52:37.160 --> 01:52:38.920]   No, no, these are actual-
[01:52:38.920 --> 01:52:40.600]   These are urban residents, yeah.
[01:52:40.600 --> 01:52:44.840]   Yeah, and then it's like a quarter of their, or was it a tenth of their monthly income
[01:52:44.840 --> 01:52:47.160]   just goes to paying the water.
[01:52:47.160 --> 01:52:49.000]   Yes, but that's different from drinking water.
[01:52:49.560 --> 01:52:53.880]   So no matter how poor you are, normally only a very tiny fraction of the water that you
[01:52:53.880 --> 01:52:55.160]   use is drunk.
[01:52:55.160 --> 01:53:00.760]   Most of it would be for bathing or for washing clothes, a little bit for cooking, probably
[01:53:00.760 --> 01:53:03.320]   more for cooking than for actual drinking.
[01:53:03.320 --> 01:53:08.600]   But again, these are all things where you can go and I'm from California.
[01:53:08.600 --> 01:53:12.360]   I remember being a kid, being told only three minutes in the shower, come on, chop, chop.
[01:53:12.360 --> 01:53:13.880]   Even India, yeah, that's also a thing.
[01:53:13.880 --> 01:53:18.840]   Ananya had an essay recently where he, the title was, "Why I Care More About Pronouns
[01:53:18.840 --> 01:53:19.640]   Than Genocide."
[01:53:19.640 --> 01:53:22.280]   And one of the things he pointed out is that-
[01:53:22.280 --> 01:53:23.800]   Writes good titles, I give him that.
[01:53:23.800 --> 01:53:30.360]   That his, the irrational system one part of his mind cares about things that are objectively
[01:53:30.360 --> 01:53:32.280]   less important if you thought about them rationally.
[01:53:32.280 --> 01:53:35.240]   And one of those things is pronouns in comparison to genocide.
[01:53:35.240 --> 01:53:39.560]   So for you, what is the irrational system one thing that you recognize is not that important
[01:53:39.560 --> 01:53:42.280]   in the grand scheme of things, but just bothers you to no end?
[01:53:43.240 --> 01:53:48.360]   I mean, it's easier for me on the positive level where people often say, "Well, you shouldn't
[01:53:48.360 --> 01:53:49.000]   be talking to that guy.
[01:53:49.000 --> 01:53:49.720]   He's a terrible person.
[01:53:49.720 --> 01:53:50.760]   He said certain things."
[01:53:50.760 --> 01:53:52.280]   And I'll say, "Yeah, but he was really nice to me."
[01:53:52.280 --> 01:53:59.800]   So on the positive level, just someone having good manners and being friendly with me goes
[01:53:59.800 --> 01:54:01.000]   a really long way.
[01:54:01.000 --> 01:54:03.240]   I mean, honestly, this isn't something that I'm trying to overcome.
[01:54:03.240 --> 01:54:06.680]   The thing I'm trying to overcome is more being really friendly to people who are not
[01:54:06.680 --> 01:54:07.320]   friendly to me.
[01:54:07.320 --> 01:54:13.880]   So yeah, I guess that the main thing is when people are just very personally rude and unpleasant
[01:54:13.880 --> 01:54:19.640]   to - I still think intellectually that the best thing to do is to turn the other cheek
[01:54:19.640 --> 01:54:25.160]   and just try to, if not win them over, to at least win observers over with how much
[01:54:25.160 --> 01:54:26.760]   more reasonable and fair I am.
[01:54:26.760 --> 01:54:29.800]   But yeah, my instinctive reaction is just to yell back at them.
[01:54:29.800 --> 01:54:34.840]   I mean, I will say that that instinctive reaction just gets weaker and weaker over time because
[01:54:36.280 --> 01:54:39.240]   I am someone who is so uncomfortable with anger.
[01:54:39.240 --> 01:54:45.000]   And part of it is - well, it's not really my personality, but a lot of it is just the
[01:54:45.000 --> 01:54:48.360]   feeling if I ever got angry, I just don't know where I would draw the line.
[01:54:48.360 --> 01:54:50.760]   I'm just worried I would completely flip out.
[01:54:50.760 --> 01:54:53.560]   So I'm too concerned.
[01:54:53.560 --> 01:54:57.400]   If I started yelling at this person, I probably wouldn't just give him one or two cutting
[01:54:57.400 --> 01:54:57.800]   insults.
[01:54:57.800 --> 01:54:59.880]   I'd probably be screaming at him like a lunatic.
[01:54:59.880 --> 01:55:05.000]   So probably I better just keep on the sunny side of life and not even try anger because
[01:55:05.000 --> 01:55:06.120]   I'm just not good at it.
[01:55:06.120 --> 01:55:10.600]   You know that line from The Avengers where Captain America asked the Hulk?
[01:55:10.600 --> 01:55:11.720]   Oh, yeah, yeah, yeah.
[01:55:11.720 --> 01:55:12.280]   Actually, I do.
[01:55:12.280 --> 01:55:14.360]   I'm always angry.
[01:55:14.360 --> 01:55:14.920]   Right.
[01:55:14.920 --> 01:55:16.200]   Yeah, no, I'm not always angry.
[01:55:16.200 --> 01:55:16.840]   Not even close.
[01:55:16.840 --> 01:55:21.320]   Honestly, I'll say there's almost only one thing that really makes me angry and that's
[01:55:21.320 --> 01:55:22.120]   people being angry.
[01:55:22.120 --> 01:55:26.920]   So I do have secondary anger, but I have very little primary anger.
[01:55:26.920 --> 01:55:29.880]   What kind of government would you implement if you had a zero discount rate?
[01:55:29.880 --> 01:55:31.320]   So if you were a strong and long-termist.
[01:55:31.320 --> 01:55:36.120]   So since this is Lunar Society, I'll stick my neck out for anarcho-capitalism and say
[01:55:36.120 --> 01:55:39.560]   that this is really the best system if we can figure out a way of doing it.
[01:55:39.560 --> 01:55:41.240]   So there's no government like no government.
[01:55:41.240 --> 01:55:48.360]   I realize I may be completely blowing all of my credibility, but you can just go and
[01:55:48.360 --> 01:55:49.480]   Google what I have to say about it.
[01:55:49.480 --> 01:55:52.600]   People sometimes say, "Brian, are you an anarchist?"
[01:55:52.600 --> 01:55:53.960]   And I'll say, "Well, not the crazy kind."
[01:55:53.960 --> 01:55:57.480]   But what I mean by this, I'm not someone who thinks that if you just pressed a button and
[01:55:57.480 --> 01:55:58.840]   got rid of the government, things would be good.
[01:55:58.840 --> 01:56:00.120]   I think it'd be a total disaster.
[01:56:00.840 --> 01:56:06.760]   Rather, what I think is that there is another equilibrium that is totally doable if people
[01:56:06.760 --> 01:56:08.280]   realize that it's totally doable.
[01:56:08.280 --> 01:56:15.800]   And this equilibrium is one where we actually have competing police, competing legal systems,
[01:56:15.800 --> 01:56:17.160]   competing court systems.
[01:56:17.160 --> 01:56:22.440]   It's one where if I had an hour, I could not convince anyone that disagrees.
[01:56:22.440 --> 01:56:26.280]   But I believe if you gave me an hour, I could convince you that it's not crazy, which is
[01:56:26.280 --> 01:56:28.280]   what I actually do whenever I talk about this stuff.
[01:56:28.280 --> 01:56:30.360]   I say, "Look, I have a really radical idea now.
[01:56:30.360 --> 01:56:36.520]   I couldn't possibly convince a reasonable person in an hour, so I'm not going to try.
[01:56:36.520 --> 01:56:39.960]   What I'm going to try to do is convince you in an hour that this view, though you still
[01:56:39.960 --> 01:56:41.640]   will think it's wrong, is not crazy."
[01:56:41.640 --> 01:56:48.040]   All right, so we don't have an hour to talk about it, but I think that this is the best,
[01:56:48.040 --> 01:56:49.720]   especially long-term.
[01:56:49.720 --> 01:56:52.840]   If we could get to this equilibrium, it's the best equilibrium to be in.
[01:56:52.840 --> 01:56:59.800]   It's one where it basically, once and for all, solves a whole lot of problems with international
[01:56:59.800 --> 01:57:00.200]   war.
[01:57:00.200 --> 01:57:01.400]   It diffuses nationalism.
[01:57:01.400 --> 01:57:07.000]   It's something that does a lot to take care of just a lot of root causes of human problems.
[01:57:07.000 --> 01:57:10.760]   It is one where basically it dethrones demagogues, right?
[01:57:10.760 --> 01:57:15.160]   Demagogues, there's always a place for people like that.
[01:57:15.160 --> 01:57:16.200]   They'll be running cults.
[01:57:16.200 --> 01:57:19.640]   They'll be involved in religion.
[01:57:19.640 --> 01:57:20.360]   They'll be pundits.
[01:57:21.400 --> 01:57:24.600]   This would be a world where there's just really no longer any government that you can get your
[01:57:24.600 --> 01:57:27.720]   hands on to go and cause horrible problems for the world.
[01:57:27.720 --> 01:57:32.920]   It's one where the demagogues will sort of have lost their main line of employment and
[01:57:32.920 --> 01:57:37.000]   will have to get, if not exactly a real job, then at least a job that doesn't involve
[01:57:37.000 --> 01:57:37.640]   mass murder.
[01:57:37.640 --> 01:57:43.800]   Scott Alexander had a post last night, or was it this morning, where he was making the
[01:57:43.800 --> 01:57:49.480]   point that he's in favor of more taxes on billionaires because the idea is even though
[01:57:49.480 --> 01:57:53.480]   Jeff Bezos has created a lot of consumer surplus, and he certainly hasn't absorbed all of
[01:57:53.480 --> 01:58:00.040]   it, it's not the case that the rewards have to be that high to get Amazon built.
[01:58:00.040 --> 01:58:03.800]   Somebody would have ended up building Amazon anyways, even if the rewards were slightly
[01:58:03.800 --> 01:58:04.120]   lower.
[01:58:04.120 --> 01:58:09.320]   Jeff Bezos himself is not that counterfactually responsible for Amazon.
[01:58:09.320 --> 01:58:10.200]   What do you make of that argument?
[01:58:10.200 --> 01:58:12.120]   Right.
[01:58:12.120 --> 01:58:13.160]   So two things.
[01:58:13.160 --> 01:58:17.240]   One is in economics, we have something called tournament theory, and this says that it can
[01:58:17.240 --> 01:58:22.520]   be extremely socially valuable to go and have seemingly unreasonably large rewards for people
[01:58:22.520 --> 01:58:27.800]   that do something useful because it doesn't just incentivize the winner, it incentivizes
[01:58:27.800 --> 01:58:28.600]   all the potential winners.
[01:58:28.600 --> 01:58:32.840]   So billionaires are not just an inspiration to each other.
[01:58:32.840 --> 01:58:36.120]   It's not just, "Oh, I can get to be a billionaire.
[01:58:36.120 --> 01:58:37.240]   I'll do this thing and make the money."
[01:58:37.240 --> 01:58:42.040]   It is something that actually fosters a whole culture of entrepreneurship.
[01:58:42.040 --> 01:58:43.720]   I mean, again, we've been hanging out in Austin.
[01:58:43.720 --> 01:58:45.880]   All over there, there's a whole bunch of people who are never going to be billionaires
[01:58:45.880 --> 01:58:48.920]   Dwarkesh, but I've told people like Dwarkesh.
[01:58:48.920 --> 01:58:50.280]   Will Dwarkesh ever be a billionaire?
[01:58:50.280 --> 01:58:51.800]   Probably not, but 2%.
[01:58:51.800 --> 01:58:56.200]   Like Dwarkesh is just a mover and a shaker, right?
[01:58:56.200 --> 01:58:59.800]   But if you like, I don't know, where in India was your family from actually?
[01:58:59.800 --> 01:59:00.760]   Gujarat.
[01:59:00.760 --> 01:59:02.680]   So is that like a big city?
[01:59:02.680 --> 01:59:04.680]   Is it a small town or what even was it?
[01:59:04.680 --> 01:59:05.720]   Yeah, it was a big city.
[01:59:05.720 --> 01:59:06.040]   Okay.
[01:59:06.040 --> 01:59:09.480]   So you probably would have gone to IIT or something like that, but imagine if-
[01:59:09.480 --> 01:59:11.320]   I don't know if I could have made it to IIT.
[01:59:11.320 --> 01:59:16.920]   Imagine if your family was out in some rural village and it's like the Indira Gandhi era.
[01:59:16.920 --> 01:59:20.920]   You don't hear about dot-com billionaires and anything.
[01:59:20.920 --> 01:59:26.760]   In terms of just inspiring a generation, I think the billionaires are inspiring a generation
[01:59:26.760 --> 01:59:28.600]   of movers and shakers.
[01:59:28.600 --> 01:59:36.040]   If their earnings were greatly taxed, I think this would really put a dent in it.
[01:59:36.040 --> 01:59:38.840]   I think anytime people speak ill of them, that's putting a dent in it.
[01:59:40.120 --> 01:59:43.640]   I mean, in a way, if you want billionaires to make less money, praise them to the skies
[01:59:43.640 --> 01:59:48.280]   so that more people enter and it drives down the rewards for doing what they do.
[01:59:48.280 --> 01:59:50.280]   So this tournament theory, it does make a lot of sense.
[01:59:50.280 --> 01:59:54.440]   This is a story about why do you pay the CEO so much more than all the next level down?
[01:59:54.440 --> 01:59:57.960]   Is he really that much better than the second best guy?
[01:59:57.960 --> 02:00:02.440]   And it's like, look, this isn't just incentivizing the CEO, it's incentivizing everyone who's there,
[02:00:02.440 --> 02:00:06.840]   right, who could plausibly tell themselves, "Maybe one day, me."
[02:00:06.840 --> 02:00:07.800]   So there's that.
[02:00:08.680 --> 02:00:11.880]   The other thing is actually going back to the historical contingency.
[02:00:11.880 --> 02:00:16.120]   I think business history is less historically contingent, but we actually do have a lot
[02:00:16.120 --> 02:00:20.520]   of evidence that the quality of entrepreneurship and management varies a lot from country to
[02:00:20.520 --> 02:00:25.960]   country, which I think does actually mean that for the really big businesses, it's not
[02:00:25.960 --> 02:00:28.200]   totally clear that it would have happened anyway.
[02:00:28.200 --> 02:00:32.280]   Now, if you broaden it and just say, "Well, would e-commerce have happened?"
[02:00:32.280 --> 02:00:33.720]   Yeah, e-commerce would have happened.
[02:00:33.720 --> 02:00:36.200]   "Would there be one store that was bigger than the others?"
[02:00:36.200 --> 02:00:36.920]   Yeah.
[02:00:36.920 --> 02:00:41.080]   But is it possible the second best thing to Amazon would have been like one-tenth as good?
[02:00:41.080 --> 02:00:43.480]   That actually is not crazy.
[02:00:43.480 --> 02:00:44.440]   I don't know.
[02:00:44.440 --> 02:00:45.720]   I don't know about it.
[02:00:45.720 --> 02:00:52.280]   But to go and just say, "We know that that's not true," seems pretty dogmatic to me.
[02:00:52.280 --> 02:00:57.080]   And again, in the case of Amazon, also striking, what is the second best thing to Amazon?
[02:00:57.080 --> 02:00:59.880]   You can say it's a natural monopoly.
[02:00:59.880 --> 02:01:01.640]   Not true in any other retail that I know of.
[02:01:01.640 --> 02:01:04.920]   So people say like Alibaba, I tried looking at that one.
[02:01:04.920 --> 02:01:05.880]   I'm like, "What is this junk?"
[02:01:05.880 --> 02:01:08.120]   What if it's a Chinese company that's understandable?
[02:01:08.120 --> 02:01:14.120]   But shouldn't they be catering to their — it's the English language version of the site.
[02:01:14.120 --> 02:01:15.400]   Shouldn't they be trying to make me happy?
[02:01:15.400 --> 02:01:22.920]   So that is a case where it doesn't seem like, in any economic sense, it's a natural monopoly
[02:01:22.920 --> 02:01:24.280]   based upon past experience.
[02:01:24.280 --> 02:01:25.080]   Think about cars.
[02:01:25.080 --> 02:01:25.960]   We've got big three.
[02:01:25.960 --> 02:01:27.880]   Why aren't there three things like Amazon?
[02:01:27.880 --> 02:01:33.240]   Maybe it really is the case that no one but Jeff Bezos knew how to do it.
[02:01:33.240 --> 02:01:36.680]   Especially when you realize part of knowing how to do something is knowing how to assemble a team.
[02:01:36.680 --> 02:01:39.400]   It's so easy to say, "You didn't do it.
[02:01:39.400 --> 02:01:40.840]   It was your team that did it."
[02:01:40.840 --> 02:01:44.120]   It's like, "I made my team."
[02:01:44.120 --> 02:01:50.120]   That's what a leader does is they take people that were good in themselves and they fuse
[02:01:50.120 --> 02:01:54.680]   them together and they make them great and maybe self-serving, but it's plausible.
[02:01:54.680 --> 02:01:56.520]   That makes a lot of sense.
[02:01:56.520 --> 02:01:59.800]   Well, Ryan, I want to thank you for giving me so much of your time.
[02:01:59.800 --> 02:02:02.840]   And I especially want to thank you for being the first guest on my podcast.
[02:02:02.840 --> 02:02:06.440]   So we've gone full circle over the third episode, but it would not have been possible at all
[02:02:06.440 --> 02:02:07.000]   without you.
[02:02:07.000 --> 02:02:07.720]   Totally awesome.
[02:02:07.720 --> 02:02:13.080]   And yes, so the books that we've been talking about are both available on Amazon.
[02:02:13.080 --> 02:02:15.880]   So there is How Evil Are Politicians?
[02:02:15.880 --> 02:02:17.560]   Essays on Demagoguery.
[02:02:17.560 --> 02:02:19.320]   And then there is Don't Be a Feminist?
[02:02:19.320 --> 02:02:21.160]   Essays on Genuine Justice.
[02:02:21.160 --> 02:02:23.640]   By the time you get the podcast, both books will be available.
[02:02:23.640 --> 02:02:25.560]   They're real cheap, only $12.
[02:02:25.560 --> 02:02:27.960]   I have not raised the price despite inflation.
[02:02:27.960 --> 02:02:31.080]   I've been thinking about why not and what's the economic theory behind that.
[02:02:31.080 --> 02:02:32.840]   And then the e-book's just $9.99.
[02:02:32.840 --> 02:02:36.440]   So the difference between these two books.
[02:02:36.440 --> 02:02:41.480]   So these books are both collections of my very best essays from 2005 to 2022.
[02:02:41.480 --> 02:02:47.720]   But Don't Be a Feminist has a totally all-new lead essay, one that actually, for years,
[02:02:47.720 --> 02:02:49.320]   I was kind of too nervous to write.
[02:02:49.320 --> 02:02:53.960]   And then as I watched my daughter growing up, I felt like, "No, no, I've got to write this
[02:02:53.960 --> 02:02:54.360]   essay.
[02:02:54.360 --> 02:02:55.720]   I'm going to do it for her."
[02:02:55.720 --> 02:02:57.960]   So the actual first essay is called Don't Be a Feminist?
[02:02:57.960 --> 02:02:58.840]   A Letter to My Daughter.
[02:02:58.840 --> 02:02:59.960]   That is how I frame it.
[02:03:00.680 --> 02:03:02.440]   This is not an angry essay.
[02:03:02.440 --> 02:03:04.200]   Like I said, I'm not an angry person.
[02:03:04.200 --> 02:03:05.720]   I'm not mad at feminists.
[02:03:05.720 --> 02:03:09.400]   Rather, I want to especially help my daughter.
[02:03:09.400 --> 02:03:13.640]   But anyone who's in the same boat as her, I would be thrilled to go and help them as
[02:03:13.640 --> 02:03:13.880]   well.
[02:03:13.880 --> 02:03:21.800]   So like I said, this is not a typical lawyerly book where all I do is just try to come up
[02:03:21.800 --> 02:03:25.640]   with as many arguments in my favor as I can and ignore everything against me.
[02:03:25.640 --> 02:03:28.920]   This is one where I really am trying to grapple with the truth.
[02:03:29.880 --> 02:03:35.320]   And I can honestly say this, my dream is not to upset any reader.
[02:03:35.320 --> 02:03:40.680]   In my dream world, everyone on earth would read my stuff and everyone would be happy
[02:03:40.680 --> 02:03:41.560]   after reading it.
[02:03:41.560 --> 02:03:43.000]   Everyone would be smiling.
[02:03:43.000 --> 02:03:44.840]   Everyone would be feeling grateful.
[02:03:44.840 --> 02:03:50.280]   So I have the saying where I said, "Look, I haven't really won until I have turned every
[02:03:50.280 --> 02:03:51.320]   enemy into a friend."
[02:03:51.320 --> 02:03:54.280]   That may seem quixotic.
[02:03:54.280 --> 02:03:58.040]   Yes, I know it's quixotic, but that is what I am trying to do.
[02:03:58.040 --> 02:03:59.320]   That's what's in my mind.
[02:03:59.880 --> 02:04:04.360]   Of course, you've got to first admit that there's a disagreement before you can begin
[02:04:04.360 --> 02:04:06.840]   trying to change someone's minds and make them feel good about it.
[02:04:06.840 --> 02:04:08.920]   But that really is my dream.
[02:04:08.920 --> 02:04:09.480]   Yeah, yeah.
[02:04:09.480 --> 02:04:14.760]   And anybody who has read Brian's books or met him, we can confirm that the books and
[02:04:14.760 --> 02:04:16.920]   the arguments, the arguments are very good.
[02:04:16.920 --> 02:04:20.680]   And also, they obviously come from a place of kindness and understanding about the other
[02:04:20.680 --> 02:04:21.640]   person's position.
[02:04:21.640 --> 02:04:23.240]   Yeah, we all have problems to our catch.
[02:04:23.240 --> 02:04:26.920]   We are all imperfect, flawed human beings.
[02:04:27.480 --> 02:04:29.320]   But we must rise above it.
[02:04:29.320 --> 02:04:31.480]   So I highly recommend the books.
[02:04:31.480 --> 02:04:33.720]   Brian, thanks so much for coming on to Lunar Society.
[02:04:33.720 --> 02:04:34.360]   It is awesome.
[02:04:34.360 --> 02:04:36.120]   Thanks for coming out here.
[02:04:36.120 --> 02:04:39.880]   Now, this is the first time that I've actually had you right in my office while interviewing.
[02:04:39.880 --> 02:04:44.280]   So as great as Dwarkesh is over Zoom, he's even better in real life.
[02:04:44.280 --> 02:04:47.720]   So all people must try to meet Dwarkesh.
[02:04:47.720 --> 02:04:49.800]   Cool guy, positive person.
[02:04:49.800 --> 02:04:50.680]   Thank you, buddy.
[02:04:50.680 --> 02:04:51.320]   Thank you, Brian.
[02:04:51.320 --> 02:04:54.760]   Hey, thanks for listening.
[02:04:55.560 --> 02:05:01.480]   If you enjoyed that episode, I would really, really, really appreciate it if you could
[02:05:01.480 --> 02:05:02.360]   share it.
[02:05:02.360 --> 02:05:04.680]   This is still a pretty small podcast.
[02:05:04.680 --> 02:05:09.000]   So it is a huge help when any one of you shares an episode that you like.
[02:05:09.000 --> 02:05:10.440]   Post it on Twitter.
[02:05:10.440 --> 02:05:12.840]   Send it to friends who you think might like it.
[02:05:12.840 --> 02:05:14.200]   Put it in your group chats.
[02:05:14.200 --> 02:05:15.480]   Just let the word go forth.
[02:05:15.480 --> 02:05:16.200]   It helps out a ton.
[02:05:16.200 --> 02:05:23.720]   Many thanks to my amazing editor, Graham Bessalou, for producing this podcast.
[02:05:23.720 --> 02:05:29.400]   And to Mia Ayana for creating the amazing transcripts that accompany each episode,
[02:05:29.400 --> 02:05:31.480]   which have helpful links.
[02:05:31.480 --> 02:05:34.840]   And you can find them at the link in the description below.
[02:05:34.840 --> 02:05:39.640]   Remember to subscribe on YouTube and your favorite podcast platforms.
[02:05:39.640 --> 02:05:40.840]   Cheers.
[02:05:40.840 --> 02:05:51.720]   See you next time.

