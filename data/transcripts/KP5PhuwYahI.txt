
[00:00:00.000 --> 00:00:04.860]   There isn't one definition of fairness, right?
[00:00:04.860 --> 00:00:08.700]   If you look at philosophy, whether it's like moral or political philosophy, or you look
[00:00:08.700 --> 00:00:12.780]   at the law, or even you look at like the vibrant community in the computer science community
[00:00:12.780 --> 00:00:13.780]   and machine learning, right?
[00:00:13.780 --> 00:00:16.580]   Who's thinking about algorithmic bias.
[00:00:16.580 --> 00:00:20.060]   One common pattern is that you have multiple definitions of fairness that are mutually
[00:00:20.060 --> 00:00:21.060]   incompatible.
[00:00:21.060 --> 00:00:22.060]   So you have to pick one.
[00:00:22.060 --> 00:00:26.160]   You're listening to Gradient Dissent, a show where we learn about making machine learning
[00:00:26.160 --> 00:00:27.820]   models work in the real world.
[00:00:27.820 --> 00:00:30.140]   I'm your host, Lukas Biewald.
[00:00:30.140 --> 00:00:34.140]   Joaquin Candela is the tech lead for responsible AI at Facebook.
[00:00:34.140 --> 00:00:37.880]   Prior to that, he built the Applied Machine Learning team, which powers all production
[00:00:37.880 --> 00:00:41.840]   applications of AI across all of Facebook's products.
[00:00:41.840 --> 00:00:46.740]   Before that, Joaquin taught at University of Cambridge and worked at Microsoft Research.
[00:00:46.740 --> 00:00:51.780]   Today I'm going to talk to him about fairness and algorithmic bias, scaling and democratizing
[00:00:51.780 --> 00:00:54.020]   AI at Facebook.
[00:00:54.020 --> 00:00:57.420]   You were running the Applied Machine Learning team at Facebook, right?
[00:00:57.420 --> 00:01:01.620]   During a time when there was tons of machine learning innovation going on.
[00:01:01.620 --> 00:01:07.380]   I'd love to hear what was happening when you started working on that and kind of what tooling
[00:01:07.380 --> 00:01:11.780]   was necessary and how that kind of changed over the time that you were working on that.
[00:01:11.780 --> 00:01:13.580]   I think the context is very important here.
[00:01:13.580 --> 00:01:16.860]   So I joined Facebook in mid 2012.
[00:01:16.860 --> 00:01:19.220]   There wasn't a lot of ML people in the company.
[00:01:19.220 --> 00:01:23.860]   And if you think about the two biggest applications, it was a newsfeed ranking on the one hand
[00:01:23.860 --> 00:01:25.140]   and then ads ranking, right?
[00:01:25.140 --> 00:01:26.760]   So two ranking problems.
[00:01:26.760 --> 00:01:32.860]   So as far as the models were concerned, you mostly had binary classifiers that were used
[00:01:32.860 --> 00:01:35.060]   as inputs into a ranking function, right?
[00:01:35.060 --> 00:01:40.260]   So if you think about newsfeed ranking, you would have, oh, so my value function is some
[00:01:40.260 --> 00:01:45.220]   combination of, I give every click a certain score.
[00:01:45.220 --> 00:01:46.740]   I give every comment a score.
[00:01:46.740 --> 00:01:50.500]   I give every share a score, et cetera, right?
[00:01:50.500 --> 00:01:53.060]   And then with that, I build myself a value function.
[00:01:53.060 --> 00:01:57.620]   And so I have all these binary classifiers that predict the probability that someone
[00:01:57.620 --> 00:02:02.380]   will click, share, or comment or whatever before I show them something.
[00:02:02.380 --> 00:02:05.500]   And then I kind of use that to sort of rank content.
[00:02:05.500 --> 00:02:06.820]   And for ads, it's a similar thing.
[00:02:06.820 --> 00:02:13.740]   In ads, back in the prehistorical times, click-based advertising was the big, big, big thing, maybe
[00:02:13.740 --> 00:02:18.500]   like, I don't even remember now, like 15 years ago, 20 years ago, whenever.
[00:02:18.500 --> 00:02:20.460]   And then you had conversions, right?
[00:02:20.460 --> 00:02:24.380]   And then there's more subtle things where you have brand and then not all conversions
[00:02:24.380 --> 00:02:25.660]   are created equal.
[00:02:25.660 --> 00:02:29.220]   And then the other thing that happens, of course, is that the complexity of the content
[00:02:29.220 --> 00:02:30.220]   evolves, right?
[00:02:30.220 --> 00:02:35.620]   Like if you think about when I joined Facebook, a lot of the content was mostly text, right?
[00:02:35.620 --> 00:02:37.700]   Images of course there, fewer videos.
[00:02:37.700 --> 00:02:41.520]   And now that sort of becomes more complex and you have more multimodality.
[00:02:41.520 --> 00:02:50.960]   So I joined Facebook at a time where the company was just IPO-ing and the revenue was flat.
[00:02:50.960 --> 00:02:54.360]   And so there was a huge pressure to try and move the needle in ads.
[00:02:54.360 --> 00:02:56.040]   I joined the ads team.
[00:02:56.040 --> 00:03:02.440]   And one of the big levers to move revenue was like, "Oh, can we get better at predicting
[00:03:02.440 --> 00:03:04.360]   clicks and conversions on ads?"
[00:03:04.360 --> 00:03:11.500]   At the same time, we start to move away from only serving ads on web, on the right-hand
[00:03:11.500 --> 00:03:15.320]   column to actually also serving ads on mobile.
[00:03:15.320 --> 00:03:20.160]   And then actually end of 2012, when I joined, if you look at where people were accessing
[00:03:20.160 --> 00:03:25.120]   Facebook from, web was kind of slowly declining or being stable.
[00:03:25.120 --> 00:03:27.360]   And then mobile was rocketing.
[00:03:27.360 --> 00:03:30.320]   I think they crossed at around the end of 2012, right?
[00:03:30.320 --> 00:03:36.220]   So the types of services you have, the types of things you're predicting starts to increase.
[00:03:36.220 --> 00:03:40.200]   And so the first dilemma that I had was like, "Oh, I looked at what were we doing, right?
[00:03:40.200 --> 00:03:48.280]   And we were using the Soyuz in a way, to go to the space station, nothing fancy, just
[00:03:48.280 --> 00:03:51.160]   like the good old Soyuz, right?
[00:03:51.160 --> 00:03:57.320]   We were using a gradient boosted decision trees as feature transformers, mostly you
[00:03:57.320 --> 00:03:58.700]   could think about it that way.
[00:03:58.700 --> 00:04:07.080]   And then we were using online logistic regression after that, like cascaded with it, right?
[00:04:07.080 --> 00:04:11.660]   So what would you, sorry to interrupt, but what then would you train the intermediate
[00:04:11.660 --> 00:04:12.880]   gradient boosted tree on?
[00:04:12.880 --> 00:04:15.760]   What would be the kind of thing that that would try to predict?
[00:04:15.760 --> 00:04:19.520]   You'd still train them on the event, on the binary event that you're trying to predict,
[00:04:19.520 --> 00:04:20.520]   right?
[00:04:20.520 --> 00:04:22.360]   Like clicks or conversions or whatever.
[00:04:22.360 --> 00:04:28.160]   But obviously you'd benefit from the robustness that that gives you, right?
[00:04:28.160 --> 00:04:32.960]   You don't have to worry too much about scaling and translation and whatever of your features.
[00:04:32.960 --> 00:04:36.840]   But then you would feed them into a simpler model?
[00:04:36.840 --> 00:04:37.840]   Yeah.
[00:04:37.840 --> 00:04:43.960]   What you would then use is the trees themselves, every tree as a categorical feature, as it
[00:04:43.960 --> 00:04:45.360]   were.
[00:04:45.360 --> 00:04:50.560]   And so then your logistic regression model, which we would be training online, has a bunch
[00:04:50.560 --> 00:04:54.440]   of inputs that are categorical, which are like the outputs of each of the trees, right?
[00:04:54.440 --> 00:04:58.280]   So it's basically like relearning, kind of relearning the weights associated to each
[00:04:58.280 --> 00:04:59.280]   leaf.
[00:04:59.280 --> 00:05:00.280]   Oh, interesting.
[00:05:00.280 --> 00:05:01.280]   Wow.
[00:05:01.280 --> 00:05:02.280]   I had not heard of that.
[00:05:02.280 --> 00:05:05.320]   So the thing that's changing then is the sort of the combination of the two.
[00:05:05.320 --> 00:05:09.800]   You train a gradient boosting tree, then you pull the trees apart and then you relearn
[00:05:09.800 --> 00:05:13.280]   the weights of each tree in the combination.
[00:05:13.280 --> 00:05:14.280]   Yeah.
[00:05:14.280 --> 00:05:15.280]   It's a hack, right?
[00:05:15.280 --> 00:05:16.880]   It's not a fully back-propagated model, right?
[00:05:16.880 --> 00:05:17.880]   Sure, sure.
[00:05:17.880 --> 00:05:22.480]   Because you train your trees every few weeks, whenever.
[00:05:22.480 --> 00:05:30.280]   And then you have logistic regression that takes as inputs both, like the binary indicators.
[00:05:30.280 --> 00:05:33.960]   Every one of the trees you train, like hundreds, maybe a couple of thousand trees.
[00:05:33.960 --> 00:05:38.280]   So you have like, and each of the trees has like a dozen leaves or like whatever.
[00:05:38.280 --> 00:05:43.440]   And you treat those as like one out of 12 kind of like encoding, but then you're learning
[00:05:43.440 --> 00:05:44.960]   a weight for each of those.
[00:05:44.960 --> 00:05:47.560]   And you're kind of like learning that sort of like in real time.
[00:05:47.560 --> 00:05:53.960]   And then you have other features that go in side by side that can actually be sort of
[00:05:53.960 --> 00:05:55.600]   continuous features as well.
[00:05:55.600 --> 00:05:56.600]   Cool.
[00:05:56.600 --> 00:05:57.960]   So that's a setup.
[00:05:57.960 --> 00:05:59.480]   That's what I found when I got there.
[00:05:59.480 --> 00:06:03.360]   And so the key decision, since you wanted to talk about like building Applied NL and
[00:06:03.360 --> 00:06:07.960]   all that was the dilemma was the following.
[00:06:07.960 --> 00:06:14.280]   It was like, okay, well, this thing is begging for a proper neural net, right?
[00:06:14.280 --> 00:06:16.040]   To be thrown at it, right?
[00:06:16.040 --> 00:06:24.600]   It's almost like we've handcrafted a Frankenstein type of neural net by having these trees with
[00:06:24.600 --> 00:06:26.920]   logistic regression concatenated to it.
[00:06:26.920 --> 00:06:29.520]   But we're not even like training it together, right?
[00:06:29.520 --> 00:06:33.840]   We first train the trees and we kind of chop the output and then we kind of plug this other
[00:06:33.840 --> 00:06:34.840]   thing to it.
[00:06:34.840 --> 00:06:36.620]   And then that's the thing that we train.
[00:06:36.620 --> 00:06:40.640]   So it was very obvious that doing that would probably give us gains.
[00:06:40.640 --> 00:06:46.640]   And this is actually, so 2012, it was obvious that a neural net would give you an improvement.
[00:06:46.640 --> 00:06:47.640]   I'm trying to remember.
[00:06:47.640 --> 00:06:48.640]   Sort of.
[00:06:48.640 --> 00:06:50.280]   Was that obvious to everyone?
[00:06:50.280 --> 00:06:51.280]   No.
[00:06:51.280 --> 00:06:56.920]   No, because if you think about the, I think it was a Roussel Kudinov and Jeff Hinton,
[00:06:56.920 --> 00:07:01.240]   I might be forgetting some coauthors, so I deeply apologize because I've had a long day
[00:07:01.240 --> 00:07:02.240]   already.
[00:07:02.240 --> 00:07:03.240]   Apologies.
[00:07:03.240 --> 00:07:10.160]   But this was like the big ImageNet paper was with ConvNet, was I think from 2012, if I'm
[00:07:10.160 --> 00:07:11.160]   not mistaken.
[00:07:11.160 --> 00:07:12.160]   That sounds right.
[00:07:12.160 --> 00:07:13.160]   Yeah.
[00:07:13.160 --> 00:07:14.160]   Yeah.
[00:07:14.160 --> 00:07:15.160]   So I don't think it was clear.
[00:07:15.160 --> 00:07:19.080]   I think this was like the beginning of the hockey stick, but I think it wasn't clear.
[00:07:19.080 --> 00:07:22.360]   If that had been like two years later, then it would have been obvious, right?
[00:07:22.360 --> 00:07:23.360]   Right.
[00:07:23.360 --> 00:07:24.360]   At the time it wasn't clear yet.
[00:07:25.160 --> 00:07:28.680]   You always need a couple of months to realize that something happened.
[00:07:28.680 --> 00:07:35.840]   So yeah, the thing that really struck me was that the time it took from doing an experiment,
[00:07:35.840 --> 00:07:38.700]   which a lot of it was really like feature engineering.
[00:07:38.700 --> 00:07:43.920]   Maybe there were some experiments with tuning your learning ratios and tuning the architecture
[00:07:43.920 --> 00:07:47.320]   of your trees and the architecture of your, I mean, logistic regression, although there
[00:07:47.320 --> 00:07:52.080]   isn't a lot of architecture to be tuned with your logistic regression.
[00:07:52.080 --> 00:07:56.200]   The time to go from someone has an idea and starts to do some offline experiments to you
[00:07:56.200 --> 00:08:01.640]   actually have your new click prediction or conversion prediction model for mobile or
[00:08:01.640 --> 00:08:03.040]   whatever in production, right?
[00:08:03.040 --> 00:08:06.360]   And actually materializing those gains would be several weeks.
[00:08:06.360 --> 00:08:09.800]   It'd be like sometimes six weeks, sometimes two months.
[00:08:09.800 --> 00:08:13.080]   And I thought, holy crap, that's not great.
[00:08:13.080 --> 00:08:17.560]   And so the crossroads in a way, on the one hand you're like, okay, this thing I have
[00:08:17.560 --> 00:08:22.400]   is simple, but we're still getting a lot of gains by tuning it.
[00:08:22.400 --> 00:08:27.480]   And on the other hand, I can go and just like replace my Soyuz with something like sophisticated.
[00:08:27.480 --> 00:08:30.320]   So that was like the crossroads.
[00:08:30.320 --> 00:08:34.680]   So do you want to know what I decided to do?
[00:08:34.680 --> 00:08:35.680]   Tell me, yes.
[00:08:35.680 --> 00:08:38.280]   I feel like you're kind of picking on the Soyuz.
[00:08:38.280 --> 00:08:46.120]   I didn't know that was the metaphor for a rudimentary thing.
[00:08:46.120 --> 00:08:48.920]   It's true that the Soyuz, well, I mean, I think the Soyuz is rudimentary in the sense
[00:08:48.920 --> 00:08:58.720]   that the computer systems that the Soyuz has in it are probably 50 years old or something
[00:08:58.720 --> 00:08:59.720]   like that, right?
[00:08:59.720 --> 00:09:01.640]   But they work, right?
[00:09:01.640 --> 00:09:05.960]   So the reason I use a Soyuz analogy is more like it gets the job done.
[00:09:05.960 --> 00:09:10.000]   It's like a gradient boosted decision tree and logistic regression, right?
[00:09:10.000 --> 00:09:15.960]   It's just as an aside, one thing that triggers me these days a little bit is I see people
[00:09:15.960 --> 00:09:20.840]   jump straight, if they'd have to solve an NLP task, they'll use either some sort of
[00:09:20.840 --> 00:09:26.360]   a sequence model, they're using an LSTM, they'll use, what I mean, I guess it's a transformer
[00:09:26.360 --> 00:09:27.360]   or whatever.
[00:09:27.360 --> 00:09:32.440]   And it's like, and sometimes you go like, did you try like a max-end model?
[00:09:32.440 --> 00:09:36.800]   Did you try like a good old bag of words with logistic regression?
[00:09:36.800 --> 00:09:44.040]   And the surprising thing is that I would say between 20 and 50% of the time, you get the
[00:09:44.040 --> 00:09:45.040]   same results.
[00:09:45.040 --> 00:09:50.400]   And then you're like, did you realize how much cheaper this thing is, right?
[00:09:50.400 --> 00:09:52.200]   In terms of anything you care about, right?
[00:09:52.200 --> 00:09:56.320]   Whether it's like a training time, inference time, whatever, right?
[00:09:56.320 --> 00:10:01.000]   So yeah, so basically the big bet there was to say, well, what we need to do here is we
[00:10:01.000 --> 00:10:06.960]   need to actually allow our teams to ship every week.
[00:10:06.960 --> 00:10:09.760]   And that was like the big model, was like ship every week.
[00:10:09.760 --> 00:10:15.240]   Do whatever it takes so that every week we can ship new models to production.
[00:10:15.240 --> 00:10:21.660]   And what that meant was like, oh, we need to dramatically accelerate that path from
[00:10:21.660 --> 00:10:26.520]   I have a new model that I could put in production to it's in production.
[00:10:26.520 --> 00:10:31.720]   And that kind of triggered five years of work.
[00:10:31.720 --> 00:10:32.720]   And so what were the keys?
[00:10:32.720 --> 00:10:37.040]   I mean, tell me the pieces that you needed to build in order to allow that to happen?
[00:10:37.040 --> 00:10:40.100]   Because I'm sure a lot of people listening to this are thinking, I'd like to ship a model
[00:10:40.100 --> 00:10:41.100]   every week.
[00:10:41.100 --> 00:10:43.580]   What do you need in order to do that safely?
[00:10:43.580 --> 00:10:48.600]   It was many things at different levels.
[00:10:48.600 --> 00:10:57.040]   So at a very low level, it's about fitting in seamlessly in whatever infrastructure you
[00:10:57.040 --> 00:10:59.380]   have for inference, right?
[00:10:59.380 --> 00:11:03.740]   And adopting some sort of standards, which seems super easy and trivial, but even that
[00:11:03.740 --> 00:11:06.560]   you shouldn't take for granted.
[00:11:06.560 --> 00:11:13.620]   The part that I thought was even more interesting is that I think what was slowing people down
[00:11:13.620 --> 00:11:17.480]   was probably two or three things.
[00:11:17.480 --> 00:11:23.660]   One was, it was extremely difficult to share work between people because people would be
[00:11:23.660 --> 00:11:27.020]   running experiments in their own dev servers.
[00:11:27.020 --> 00:11:34.260]   And even like having, as we all know, configs back then weren't sort of easily portable.
[00:11:34.260 --> 00:11:39.100]   It would just take you, I don't know, a couple of hours or whatever.
[00:11:39.100 --> 00:11:43.020]   You'd have an energy barrier before I could actually play with what you had done.
[00:11:43.020 --> 00:11:46.900]   The second thing which I think is related is that you started to have a lot of teams
[00:11:46.900 --> 00:11:48.100]   reinventing the wheel.
[00:11:48.100 --> 00:11:52.140]   So a lot of the work that was being done was actually duplicate.
[00:11:52.140 --> 00:11:57.420]   Because as the number of surfaces on which we showed ads sort of kept increasing and
[00:11:57.420 --> 00:12:01.540]   the types of modalities kept increasing, you kind of had teams that focused on one of those
[00:12:01.540 --> 00:12:07.540]   voxels in your tensor of configurations.
[00:12:07.540 --> 00:12:10.900]   And they wouldn't sort of easily talk to each other or the work wouldn't be discoverable.
[00:12:10.900 --> 00:12:13.780]   So the thing number one was automate everything.
[00:12:13.780 --> 00:12:15.240]   You have to automate everything.
[00:12:15.240 --> 00:12:20.340]   You have to make it ridiculously easy and you have to abstract everything from the engineering
[00:12:20.340 --> 00:12:22.220]   trying to deploy something.
[00:12:22.220 --> 00:12:24.980]   Especially because we're growing very fast and you get a lot of people who are joining
[00:12:24.980 --> 00:12:27.340]   the company fresh from somewhere.
[00:12:27.340 --> 00:12:31.020]   Maybe they are good applied researchers, but they're not infra people necessarily.
[00:12:31.020 --> 00:12:33.860]   So abstracting and automating, super important.
[00:12:33.860 --> 00:12:36.220]   The second, shareability.
[00:12:36.220 --> 00:12:40.060]   Make sure that you abstract and encapsulate things in a way where they're super easy to
[00:12:40.060 --> 00:12:41.060]   share.
[00:12:41.060 --> 00:12:43.380]   So I can see what input features are working for you.
[00:12:43.380 --> 00:12:49.100]   If you're working on conversion prediction models for in-game ads or whatever, I can
[00:12:49.100 --> 00:12:51.220]   super easily see that.
[00:12:51.220 --> 00:12:57.340]   Obviously you have infra work again, because the way we store and represent data is very
[00:12:57.340 --> 00:12:58.340]   heterogeneous.
[00:12:58.340 --> 00:13:03.220]   So it's a pain in the butt usually to, even if you're only looking at reproducing training,
[00:13:03.220 --> 00:13:05.780]   depending on what your setup is, that's work.
[00:13:05.780 --> 00:13:12.260]   But then going, obviously the way you run your data pipelines when you're training offline
[00:13:12.260 --> 00:13:17.580]   versus when you're trying to serve in real time is different almost always.
[00:13:17.580 --> 00:13:20.740]   And obviously when you're online, you're on a budget.
[00:13:20.740 --> 00:13:23.660]   So you want to make as few calls as possible when you're serving.
[00:13:23.660 --> 00:13:26.140]   So you've got to sort of figure out how to abstract those things.
[00:13:26.140 --> 00:13:28.940]   And again, hide all complexity.
[00:13:28.940 --> 00:13:32.140]   And then the third one, which I think is really, really interesting, is really think about
[00:13:32.140 --> 00:13:34.300]   collaboration by design.
[00:13:34.300 --> 00:13:39.260]   How can you build an environment where I go in and I can see every single experiment anyone
[00:13:39.260 --> 00:13:43.220]   has run, and I can go and by clicking, I can see, first of all, who they are.
[00:13:43.220 --> 00:13:45.140]   Who they are is huge, right?
[00:13:45.140 --> 00:13:48.300]   Because then I know who to ask, especially if I'm new to the company and you have a company
[00:13:48.300 --> 00:13:50.540]   that's growing fast.
[00:13:50.540 --> 00:13:54.380]   So the equivalent of your get blame or whatever is super important.
[00:13:54.380 --> 00:13:56.700]   You need to know who people are.
[00:13:56.700 --> 00:14:01.900]   The second one, again, is so much is wasted in terms of replicating experiments that someone
[00:14:01.900 --> 00:14:02.900]   has already done.
[00:14:02.900 --> 00:14:06.260]   So bookkeeping is extremely important.
[00:14:06.260 --> 00:14:10.700]   And then the ability to just beg, borrow, and steal bits of pieces, either of feature
[00:14:10.700 --> 00:14:13.340]   computations or models.
[00:14:13.340 --> 00:14:15.980]   We were exposing learning curves and things like that as well.
[00:14:15.980 --> 00:14:18.500]   So you can actually sort of browse them.
[00:14:18.500 --> 00:14:20.780]   And then another component, and I'm not being super organized here.
[00:14:20.780 --> 00:14:23.400]   I think I've said it's three things and I'm at the fifth thing already.
[00:14:23.400 --> 00:14:29.260]   But another one is try to be as modular as possible.
[00:14:29.260 --> 00:14:32.220]   And if possible as well, language agnostic.
[00:14:32.220 --> 00:14:40.180]   And separate out the language or the platform that you're using to kind of specify whatever
[00:14:40.180 --> 00:14:45.900]   models you're building from the definition of a workflow and execution of a workflow.
[00:14:45.900 --> 00:14:50.260]   So really abstracting that away and sort of thinking about, okay, an ML workflow is an
[00:14:50.260 --> 00:14:58.740]   ML workflow and I don't care if you're specifying your models in MATLAB, Octave, Python, PyTorch,
[00:14:58.740 --> 00:15:02.260]   TensorFlow, whatever it is that you're doing.
[00:15:02.260 --> 00:15:06.380]   A lot of the bread and butter that you're doing is kind of common.
[00:15:06.380 --> 00:15:10.380]   So really layer it, modularize it was sort of huge.
[00:15:10.380 --> 00:15:11.380]   It's interesting.
[00:15:11.380 --> 00:15:17.100]   I feel like the things that you're saying are the things that all ML leaders want that
[00:15:17.100 --> 00:15:18.100]   I talk to.
[00:15:18.100 --> 00:15:22.260]   I think the place that they get tripped up, I mean, all the benefits that you're saying
[00:15:22.260 --> 00:15:23.260]   totally make sense.
[00:15:23.260 --> 00:15:27.420]   But I think the sort of downside is that it requires getting everyone's buy-in into kind
[00:15:27.420 --> 00:15:29.620]   of a standard way of doing things.
[00:15:29.620 --> 00:15:34.540]   And I'm curious how you got that because ML practitioners are so hard to hire and they're
[00:15:34.540 --> 00:15:36.860]   often opinionated and working in different parts of the org.
[00:15:36.860 --> 00:15:39.220]   How did you get them all to buy into the same system?
[00:15:39.220 --> 00:15:44.260]   Often opinionated, you say.
[00:15:44.260 --> 00:15:49.820]   I would like to meet one that is not opinionated.
[00:15:49.820 --> 00:15:52.820]   Sometimes not opinionated.
[00:15:52.820 --> 00:15:53.820]   It's tricky.
[00:15:53.820 --> 00:15:57.980]   And this is actually, so you're putting the finger on an amazing point, which is really
[00:15:57.980 --> 00:16:01.100]   almost like a change management.
[00:16:01.100 --> 00:16:03.300]   Very, very hard.
[00:16:03.300 --> 00:16:05.340]   Several reasons why it's hard.
[00:16:05.340 --> 00:16:11.340]   Reason number one, in any fast moving company where you have low hanging fruit, I mean,
[00:16:11.340 --> 00:16:12.820]   this is not unique to ML.
[00:16:12.820 --> 00:16:17.620]   Who's going to actually pause and clean up the kitchen and pay back some tech debt or
[00:16:17.620 --> 00:16:19.780]   build infrastructure so you move faster?
[00:16:19.780 --> 00:16:22.380]   It's like, you're almost like, "Hey, why don't you do it?"
[00:16:22.380 --> 00:16:26.380]   I don't feel like doing it myself.
[00:16:26.380 --> 00:16:29.060]   So that's one challenge.
[00:16:29.060 --> 00:16:33.020]   The other one, of course, is a sense of pride that people have.
[00:16:33.020 --> 00:16:36.260]   I mean, and especially, I used to be in academia.
[00:16:36.260 --> 00:16:42.540]   And in academia, the thing that determines your worth is almost like the praise you get
[00:16:42.540 --> 00:16:47.340]   for the work that you do, but you put your name on your papers.
[00:16:47.340 --> 00:16:52.620]   So culturally, it's tricky to say, "I'm going to surrender some of that for the greater
[00:16:52.620 --> 00:16:53.620]   good."
[00:16:53.620 --> 00:17:01.100]   So the tactic that we took, one of them was just to be ridiculously laser focused.
[00:17:01.100 --> 00:17:04.300]   So look, the one thing that I should have clarified is I never dreamt that one day I
[00:17:04.300 --> 00:17:06.620]   would build the applied machine learning team at Facebook.
[00:17:06.620 --> 00:17:07.620]   That was not the intent.
[00:17:07.620 --> 00:17:10.980]   I was in ads, we're focusing ads.
[00:17:10.980 --> 00:17:14.540]   But even within ads, we already started to have several teams working on several aspects
[00:17:14.540 --> 00:17:15.540]   of the problem.
[00:17:15.540 --> 00:17:19.420]   So at least we worked on generating alignment and a vision within ads.
[00:17:19.420 --> 00:17:23.780]   And that was not like a million people, that was just like a couple of dozen people.
[00:17:23.780 --> 00:17:27.180]   And we were, I think, all feeling the pain and the urgency to move fast.
[00:17:27.180 --> 00:17:31.340]   So it was semi-obvious that this was going to be good.
[00:17:31.340 --> 00:17:32.460]   It was a bold bet.
[00:17:32.460 --> 00:17:37.040]   So you need to kind of generate alignment both on the people who are deploying things
[00:17:37.040 --> 00:17:41.620]   and doing experiments every day, but also obviously get management to give you air cover
[00:17:41.620 --> 00:17:43.900]   because things are going to slow down.
[00:17:43.900 --> 00:17:50.020]   I remember talking to my manager at Fields Coffee end of 2012, where revenue was still
[00:17:50.020 --> 00:17:52.660]   not picking up.
[00:17:52.660 --> 00:17:57.060]   And he was asking me, "Hey, you haven't been shipping models all that often.
[00:17:57.060 --> 00:17:58.060]   What's going on?"
[00:17:58.060 --> 00:17:59.220]   And I'm like, "Oh, actually, we're going to slow down even more."
[00:17:59.220 --> 00:18:02.180]   And he was like, "Explain."
[00:18:02.180 --> 00:18:06.620]   And then you explain, you get into the details, you get buying on the vision at all levels,
[00:18:06.620 --> 00:18:08.980]   but you keep it very, very narrow.
[00:18:08.980 --> 00:18:12.940]   And then what happened once we started to have progress and stuff started to move faster
[00:18:12.940 --> 00:18:19.940]   and you saw productivity increase, then we started to talk to the feed ranking team.
[00:18:19.940 --> 00:18:26.380]   And then the feed ranking team, we decided to join forces for summer 2013.
[00:18:26.380 --> 00:18:33.460]   And that was really interesting because there again, you have to just be laser focused.
[00:18:33.460 --> 00:18:35.020]   Don't think about the features first.
[00:18:35.020 --> 00:18:36.020]   Don't think abstract first.
[00:18:36.020 --> 00:18:39.860]   Don't think about like, it's not like platform first and then we see what happens.
[00:18:39.860 --> 00:18:41.460]   It's like be extremely concrete.
[00:18:41.460 --> 00:18:44.180]   Like, here's the types of things I want to make work.
[00:18:44.180 --> 00:18:51.020]   And also just accept that one day you'll have to rewrite it and that they came.
[00:18:51.020 --> 00:18:54.420]   But for now, you want to prove the hero scenario.
[00:18:54.420 --> 00:18:57.780]   You want to prove like, "Hey, this can actually be amazing."
[00:18:57.780 --> 00:18:58.780]   So that was the approach.
[00:18:58.780 --> 00:19:01.740]   It was like very, very extremely laser focused.
[00:19:01.740 --> 00:19:04.140]   Start very small, start adding people.
[00:19:04.140 --> 00:19:06.980]   Build almost like a community that supports each other.
[00:19:06.980 --> 00:19:10.940]   Really go from a core and then start expanding.
[00:19:10.940 --> 00:19:11.940]   It's interesting.
[00:19:11.940 --> 00:19:16.240]   At Weights & Biases, we make a tool and I actually didn't really realize how similar
[00:19:16.240 --> 00:19:18.700]   our tool's vision is to what you were building.
[00:19:18.700 --> 00:19:22.700]   Our hope is to really help with collaboration and reproducibility.
[00:19:22.700 --> 00:19:25.580]   And sort of the same idea of we really wanted people to be able to find the person that
[00:19:25.580 --> 00:19:29.140]   made the thing and not have to redo all the work from scratch.
[00:19:29.140 --> 00:19:32.560]   And I think we have maybe even more trouble than you getting buy-in, right?
[00:19:32.560 --> 00:19:35.160]   Because no one owes us anything, right?
[00:19:35.160 --> 00:19:37.000]   Why would someone want to use our tool?
[00:19:37.000 --> 00:19:42.220]   And I feel like for us, a big part of it is showing little wins to the individual practitioner.
[00:19:42.220 --> 00:19:48.240]   I feel like there's little details in our product that we try to just give something
[00:19:48.240 --> 00:19:53.840]   helpful right out of the gate to someone new coming in before they do the collaboration
[00:19:53.840 --> 00:19:56.080]   and before they have to really buy into our system.
[00:19:56.080 --> 00:19:59.920]   I wonder if there's any things like that for you or people like, "Oh, I want to be able
[00:19:59.920 --> 00:20:04.440]   to see the system metrics of my runs," or something like that, that got people to use
[00:20:04.440 --> 00:20:05.440]   your stuff.
[00:20:05.440 --> 00:20:07.760]   Yeah, no, excellent question.
[00:20:07.760 --> 00:20:10.960]   I'm just mining you selfishly for features for our product, really.
[00:20:10.960 --> 00:20:12.680]   That was so shameless.
[00:20:12.680 --> 00:20:13.680]   Yeah.
[00:20:13.680 --> 00:20:14.680]   I know.
[00:20:14.680 --> 00:20:15.680]   So obvious.
[00:20:15.680 --> 00:20:21.360]   So one caveat, Lucas, that I have to say, of course, is that when I was very involved
[00:20:21.360 --> 00:20:30.440]   with this stuff in the early days, that was already eight, seven, and six years ago.
[00:20:30.440 --> 00:20:32.680]   So I know that things have changed a lot.
[00:20:32.680 --> 00:20:37.440]   I know that you have open source tools today, which if we had had them, we would have just
[00:20:37.440 --> 00:20:40.640]   used them directly, including maybe Waste and Biases products, right?
[00:20:40.640 --> 00:20:44.600]   So your question is, if you set aside the collaboration benefits and all that, just
[00:20:44.600 --> 00:20:50.720]   in terms of pure individual contributor productivity, why would I care?
[00:20:50.720 --> 00:20:52.680]   I have both news and I have bad news, I guess.
[00:20:52.680 --> 00:20:54.040]   Good news and bad news.
[00:20:54.040 --> 00:20:59.200]   I think the bad news maybe is that some of the problems we solved were actually a bit
[00:20:59.200 --> 00:21:00.200]   Facebook specific.
[00:21:00.200 --> 00:21:02.880]   So I think that's not going to be useful to you.
[00:21:02.880 --> 00:21:07.640]   But in terms of abstraction, right?
[00:21:07.640 --> 00:21:12.080]   And just the ability to almost at the click of a button, the fact that you could actually
[00:21:12.080 --> 00:21:13.080]   clone a workflow...
[00:21:13.080 --> 00:21:14.080]   I'm going to give you an example.
[00:21:14.080 --> 00:21:15.400]   Here's an example.
[00:21:15.400 --> 00:21:20.360]   So you're the Instagram ML team, and Instagram has never been ranked before.
[00:21:20.360 --> 00:21:26.320]   Instagram's feed has been shown in a decreasing chronological order, right?
[00:21:26.320 --> 00:21:29.800]   From most recent to less recent.
[00:21:29.800 --> 00:21:35.320]   And your task was like, "Hey, design me a ranking system for Instagram."
[00:21:35.320 --> 00:21:36.900]   That's kind of like a tall order.
[00:21:36.900 --> 00:21:42.440]   But imagine now that you have an environment where you can actually just look at the production
[00:21:42.440 --> 00:21:46.320]   system that ranks news feed.
[00:21:46.320 --> 00:21:49.040]   There's a lot you can borrow there, right?
[00:21:49.040 --> 00:21:56.040]   And so I think just the ability to borrow, whether it's the features that seem to be
[00:21:56.040 --> 00:22:01.400]   working the best, the models, the training schedule, the hyperparameters, all of that
[00:22:01.400 --> 00:22:03.560]   is a big thing.
[00:22:03.560 --> 00:22:05.920]   In parallel to that, you have abstractions, right?
[00:22:05.920 --> 00:22:12.440]   Again, at Facebook, I don't even know how many distinct and mutually incompatible data
[00:22:12.440 --> 00:22:16.760]   stores we have, but you can imagine, right?
[00:22:16.760 --> 00:22:21.240]   And the fact that the tool will actually abstract that for you is very useful as well.
[00:22:21.240 --> 00:22:24.560]   Then if you have to build a workflow yourself, building workflows is a pain, right?
[00:22:24.560 --> 00:22:28.360]   If you don't have a tool to build workflows, it's just a pain.
[00:22:28.360 --> 00:22:31.560]   And then another one, tools for debugging and automation.
[00:22:31.560 --> 00:22:33.400]   So I'll give you an example of a couple of things.
[00:22:33.400 --> 00:22:36.920]   Tool for automation, automatic feature selection.
[00:22:36.920 --> 00:22:40.360]   The fact that you have a tool that actually scans for every feature you could possibly
[00:22:40.360 --> 00:22:44.040]   use, and then while you're sleeping, it's just making sure that you have maximum machine
[00:22:44.040 --> 00:22:49.760]   utilization and you're just doing whatever feature selection algorithm you want.
[00:22:49.760 --> 00:22:52.280]   I don't care, it doesn't matter, but it's just doing work for you.
[00:22:52.280 --> 00:22:57.680]   So two stories is ads engineers would come in the morning on Monday and they would see
[00:22:57.680 --> 00:22:59.960]   proposals for new models.
[00:22:59.960 --> 00:23:01.200]   And you're like, "Oh, this looks good.
[00:23:01.200 --> 00:23:07.880]   I get a couple of 0.1% points of gain, like what a metric, and that's good."
[00:23:07.880 --> 00:23:14.120]   The other one is one very simple reason ML systems fail is because some data pipeline
[00:23:14.120 --> 00:23:15.120]   fails.
[00:23:15.120 --> 00:23:20.480]   And again, if you have to be checking ad hoc, it's a pain.
[00:23:20.480 --> 00:23:25.000]   Imagine that you have this beautiful dashboard with colors and whatever that just tell you
[00:23:25.000 --> 00:23:30.960]   what features are not working and in which way are they not working anymore.
[00:23:30.960 --> 00:23:34.840]   Is it that you have statistical things that basically produce valid values, but they're
[00:23:34.840 --> 00:23:38.440]   the same all the time, or is it that you get things that are not a number?
[00:23:38.440 --> 00:23:39.440]   What the hell is going on?
[00:23:39.440 --> 00:23:44.480]   That's super useful as well, or tools to look at your learning curves and whatever.
[00:23:44.480 --> 00:23:49.920]   So yeah, these are a bunch of examples of things which if you're an ML engineer, you
[00:23:49.920 --> 00:23:50.920]   want that stuff.
[00:23:50.920 --> 00:23:51.920]   Totally.
[00:23:51.920 --> 00:23:52.920]   That totally makes sense.
[00:23:52.920 --> 00:23:58.880]   I want to make sure we leave plenty of time for the other thread of questions, which is
[00:23:58.880 --> 00:24:00.480]   the new work you're doing.
[00:24:00.480 --> 00:24:04.360]   I think it says on LinkedIn, you're the tech lead for responsible AI.
[00:24:04.360 --> 00:24:05.360]   Yeah.
[00:24:05.360 --> 00:24:07.640]   Which is, it sounds like a tall order.
[00:24:07.640 --> 00:24:10.120]   I mean, there's so many possible questions here.
[00:24:10.120 --> 00:24:14.360]   I was kind of wondering what would be the most interesting, but I guess the genuine
[00:24:14.360 --> 00:24:20.680]   question that's top of mind for me is always, walk me through a real decision where it wasn't
[00:24:20.680 --> 00:24:22.880]   obvious what to do.
[00:24:22.880 --> 00:24:26.480]   And by some kind of analysis or thinking about it, bring your expertise, you were able to
[00:24:26.480 --> 00:24:29.840]   kind of guide Facebook to a better decision.
[00:24:29.840 --> 00:24:31.800]   Does something come to mind?
[00:24:31.800 --> 00:24:34.520]   I'm going to start from the India elections.
[00:24:34.520 --> 00:24:40.880]   This was the biggest election in human history with almost a billion eligible voters.
[00:24:40.880 --> 00:24:43.880]   So what's the challenge and where does AI come in?
[00:24:43.880 --> 00:24:49.520]   Well, the challenge is that there's a lot of concerns of election interference through
[00:24:49.520 --> 00:24:55.160]   the spread of information, which is either false or misleading or voter suppression or
[00:24:55.160 --> 00:24:58.480]   whatever it might be.
[00:24:58.480 --> 00:25:03.360]   And of course, the way you address this, if you're Facebook or a similar company, is you
[00:25:03.360 --> 00:25:07.320]   create guidelines for what things are acceptable and what not.
[00:25:07.320 --> 00:25:10.320]   There's of course, legal constraints as well.
[00:25:10.320 --> 00:25:13.880]   And then you just hire a bunch of humans, as many as you can.
[00:25:13.880 --> 00:25:16.560]   And you would know about that because you've worked on that in the past.
[00:25:16.560 --> 00:25:20.760]   So you have people, humans who are actually processing a queue of work.
[00:25:20.760 --> 00:25:23.600]   And that queue of work is just reviewing posts.
[00:25:23.600 --> 00:25:30.360]   But when you have a country the size of India and the volumes of information or content
[00:25:30.360 --> 00:25:33.680]   that are created every day on Facebook, it's just impossible.
[00:25:33.680 --> 00:25:39.280]   You cannot hire enough humans to review even a decent fraction of everything.
[00:25:39.280 --> 00:25:40.840]   So it's impossible.
[00:25:40.840 --> 00:25:45.520]   So the way you use AI is use AI to prioritize human work.
[00:25:45.520 --> 00:25:52.920]   And the way you do this is, for example, you train a type of classifiers that we have used.
[00:25:52.920 --> 00:25:55.400]   We call them civic classifiers.
[00:25:55.400 --> 00:25:58.600]   And what they do is they try to tell whether the piece of content is just like a picture
[00:25:58.600 --> 00:26:01.800]   of a cat, which is like, whatever, doesn't matter.
[00:26:01.800 --> 00:26:02.800]   Or people like me, I'm a runner.
[00:26:02.800 --> 00:26:04.920]   Is it like, did I post a new run on Strava?
[00:26:04.920 --> 00:26:08.200]   It's like, whatever, it doesn't matter for the elections.
[00:26:08.200 --> 00:26:12.520]   Or whether it's actually someone discussing something that's actually relevant, social
[00:26:12.520 --> 00:26:15.600]   or political or civic issues.
[00:26:15.600 --> 00:26:18.360]   And then at least make sure that that type of content gets coverage.
[00:26:18.360 --> 00:26:20.840]   Okay, so what's the challenge?
[00:26:20.840 --> 00:26:22.240]   We're talking about resource allocation.
[00:26:22.240 --> 00:26:25.720]   We're talking about you have these set of humans that we're paying to protect the elections
[00:26:25.720 --> 00:26:27.880]   from interference.
[00:26:27.880 --> 00:26:32.080]   And now the question is, okay, and we're using AI to prioritize our work.
[00:26:32.080 --> 00:26:37.320]   Well, what happens if your NLP works only for Hindi?
[00:26:37.320 --> 00:26:39.720]   Wait, sorry, could you even back up a second?
[00:26:39.720 --> 00:26:43.840]   Because this is probably obvious to you, but it's not totally obvious to me.
[00:26:43.840 --> 00:26:51.040]   Assuming you had unlimited human resources to do something, what is the thing you're
[00:26:51.040 --> 00:26:52.040]   trying to do?
[00:26:52.040 --> 00:26:55.360]   Because you're not trying to block everything that's on the topic of an election.
[00:26:55.360 --> 00:26:56.360]   So what's the goal?
[00:26:56.360 --> 00:26:59.080]   Yeah, yeah, I should have explained that.
[00:26:59.080 --> 00:27:03.040]   You will block things if they violate laws.
[00:27:03.040 --> 00:27:14.240]   So if you have defamation of public figures with lies or just illegal content, or reduce
[00:27:14.240 --> 00:27:17.760]   the distribution of things that are harmful.
[00:27:17.760 --> 00:27:24.080]   So it's both filtering and reducing distribution of things that violate our community standards
[00:27:24.080 --> 00:27:25.080]   or laws.
[00:27:25.080 --> 00:27:29.720]   So that's the action that you're taking with a combination of humans and AI.
[00:27:29.720 --> 00:27:36.560]   And so the challenge there again is if you look at this from a fairness point of view,
[00:27:36.560 --> 00:27:39.680]   maybe your definition of fairness is that if we're investing a certain amount of human
[00:27:39.680 --> 00:27:47.120]   resources to do this job, that we want to make sure that everyone in India gets protection
[00:27:47.120 --> 00:27:49.160]   from this type of harmful content.
[00:27:49.160 --> 00:27:50.160]   Yeah.
[00:27:50.160 --> 00:27:52.800]   And then the question there becomes, what does that mean?
[00:27:52.800 --> 00:27:57.560]   Because if you think about algorithmic fairness and bias, if you're thinking about using AI
[00:27:57.560 --> 00:28:00.280]   to recommend jobs to people, then you get into...
[00:28:00.280 --> 00:28:02.600]   And you're in the US, you think about protected categories.
[00:28:02.600 --> 00:28:08.560]   You think about gender, age, race or ethnicity and stuff like that, where there's anti-discrimination
[00:28:08.560 --> 00:28:09.560]   laws that exist.
[00:28:09.560 --> 00:28:13.840]   But if you think about this problem in India, you're like, "Oh, politically, what are the
[00:28:13.840 --> 00:28:15.280]   hot areas?"
[00:28:15.280 --> 00:28:21.240]   And then immediately when you work with local people, it's things like caste or religion.
[00:28:21.240 --> 00:28:29.380]   But obviously we don't have that data and it's not clear that we should have that data.
[00:28:29.380 --> 00:28:32.880]   So in the end, you do a bunch of work and you figure out, "Well, what can I do?"
[00:28:32.880 --> 00:28:36.280]   And so we ended up using language and region.
[00:28:36.280 --> 00:28:37.280]   Sorry.
[00:28:37.280 --> 00:28:42.120]   Well, again, if you had caste and ethnicity, I'm sorry, I feel like I'm showing my ignorance
[00:28:42.120 --> 00:28:46.040]   here, but if you had those things, what would you do with that?
[00:28:46.040 --> 00:28:48.960]   What would be the fair thing to do that you're trying to do?
[00:28:48.960 --> 00:28:52.280]   So the challenge with fairness, and that's where we're going to go back all the way to
[00:28:52.280 --> 00:28:57.880]   music somehow, is that there isn't one definition of fairness.
[00:28:57.880 --> 00:29:01.960]   If you look at philosophy, whether it's moral or political philosophy, or you look at the
[00:29:01.960 --> 00:29:06.200]   law, or even you look at the vibrant community in the computer science community and machine
[00:29:06.200 --> 00:29:11.280]   learning who's thinking about algorithmic bias, one common pattern is that you have
[00:29:11.280 --> 00:29:13.820]   multiple definitions of fairness that are mutually incompatible.
[00:29:13.820 --> 00:29:15.760]   So you have to pick one.
[00:29:15.760 --> 00:29:19.960]   So in this case, the one that you could pick is you could say, "Well, I want to make sure
[00:29:19.960 --> 00:29:26.400]   that everyone, irrespective of their caste or religion, is going to see content that
[00:29:26.400 --> 00:29:35.320]   has received a comparable amount of protection against harmful or content that is basically
[00:29:35.320 --> 00:29:36.320]   misleading."
[00:29:36.320 --> 00:29:40.800]   So imagine there's voter suppression type of content there that spreads lies about...
[00:29:40.800 --> 00:29:44.720]   I mean, there's even stuff like just lying about when the election day is or whatever.
[00:29:44.720 --> 00:29:45.720]   I see.
[00:29:45.720 --> 00:29:46.720]   Okay, I see that.
[00:29:46.720 --> 00:29:47.720]   Thanks.
[00:29:47.720 --> 00:29:48.720]   That's helpful.
[00:29:48.720 --> 00:29:49.720]   Yeah.
[00:29:49.720 --> 00:29:50.720]   Then you kind of miss it.
[00:29:50.720 --> 00:29:56.040]   Or maybe lying about what a particular politician stands for, just sort of putting out something
[00:29:56.040 --> 00:29:58.840]   that's completely false or whatever.
[00:29:58.840 --> 00:29:59.840]   So you want to...
[00:29:59.840 --> 00:30:01.000]   Yeah, go ahead.
[00:30:01.000 --> 00:30:04.640]   So I guess one way, just to repeat back what you're saying, one way would be we want to
[00:30:04.640 --> 00:30:09.800]   make sure everyone across groups like caste or religion gets the same level of protection.
[00:30:09.800 --> 00:30:10.800]   Correct.
[00:30:10.800 --> 00:30:13.120]   By actual humans looking at the content.
[00:30:13.120 --> 00:30:14.120]   That's exactly right.
[00:30:14.120 --> 00:30:17.440]   Why might that not be the most fair approach?
[00:30:17.440 --> 00:30:19.400]   Would there be a different argument for a different...
[00:30:19.400 --> 00:30:20.400]   Yeah.
[00:30:20.400 --> 00:30:21.920]   You have situations where...
[00:30:21.920 --> 00:30:27.720]   So here, this would be an equal treatment type of argument where you would say, "We
[00:30:27.720 --> 00:30:29.600]   want to treat everybody the same."
[00:30:29.600 --> 00:30:37.040]   An equal treatment is, I think, in many cultures, sort of like the first instinct that you have.
[00:30:37.040 --> 00:30:38.360]   But you could think about other things.
[00:30:38.360 --> 00:30:40.000]   You could think...
[00:30:40.000 --> 00:30:43.640]   On the one hand, you could dial things more towards equity.
[00:30:43.640 --> 00:30:51.560]   And in equity, you could look at historical disadvantages that some group might have had.
[00:30:51.560 --> 00:30:58.400]   Is there a case where historically, some castes and religions are privileged compared to others?
[00:30:58.400 --> 00:31:04.000]   And the pressure or the amount of misinformation...
[00:31:04.000 --> 00:31:08.000]   If you think about the US, not every group in the US has historically had equal access
[00:31:08.000 --> 00:31:09.000]   to voting.
[00:31:09.000 --> 00:31:14.560]   And even today, voter suppression efforts are not uniformly distributed.
[00:31:14.560 --> 00:31:16.880]   Some groups are actually more targeted than others.
[00:31:16.880 --> 00:31:21.080]   So you could actually say, "No, I'm actually going to understand whether I should prioritize
[00:31:21.080 --> 00:31:24.680]   outcomes for some groups over others."
[00:31:24.680 --> 00:31:28.800]   And if you think about...
[00:31:28.800 --> 00:31:34.560]   There's many sort of public policies in society that actually sort of aim at focusing more
[00:31:34.560 --> 00:31:37.240]   on some groups that have been disadvantaged.
[00:31:37.240 --> 00:31:38.680]   I see.
[00:31:38.680 --> 00:31:40.000]   And so what did you do?
[00:31:40.000 --> 00:31:45.600]   So in this case, we went for the equal treatment approach.
[00:31:45.600 --> 00:31:48.480]   And then what we did, this triggered a whole amount of work.
[00:31:48.480 --> 00:31:51.720]   First of all, we don't have castes and religion.
[00:31:51.720 --> 00:31:57.240]   There's many reasons, there's many risks, why a corporation shouldn't have certain type
[00:31:57.240 --> 00:31:59.560]   of demographic information.
[00:31:59.560 --> 00:32:05.400]   There's a lot of examples in history why it's just dangerous to have repositories of certain
[00:32:05.400 --> 00:32:08.320]   demographic characteristics.
[00:32:08.320 --> 00:32:14.600]   So what we did is we used reasonable alternatives like language and region.
[00:32:14.600 --> 00:32:17.960]   So we said, "Okay, we're going to make sure that all religions in India and not all languages
[00:32:17.960 --> 00:32:23.600]   because there's a huge amount of languages in India, but I think we went for the top...
[00:32:23.600 --> 00:32:29.360]   I don't remember anymore, top 15 plus minus languages are well protected."
[00:32:29.360 --> 00:32:33.840]   And then you can get into things, "Okay, how do I translate that into math and code?"
[00:32:33.840 --> 00:32:35.560]   So you need to look at many levels.
[00:32:35.560 --> 00:32:37.600]   One, you need to look...
[00:32:37.600 --> 00:32:40.860]   The most basic thing is when you look at the data, you look at two things.
[00:32:40.860 --> 00:32:44.560]   You look at representation and then you look at biases in the labels.
[00:32:44.560 --> 00:32:49.420]   So representation, make sure that across, you build yourself your matrix of regions
[00:32:49.420 --> 00:32:54.120]   and languages and make sure that for each of these buckets, you have a sufficient amount
[00:32:54.120 --> 00:32:56.560]   of labeled training data.
[00:32:56.560 --> 00:33:03.220]   And then once you're in one of these buckets, you get yourself some ground truth data.
[00:33:03.220 --> 00:33:07.760]   And that would be a very long conversation to figure out what that is, but expensive,
[00:33:07.760 --> 00:33:10.560]   high quality data that you can use as a reference.
[00:33:10.560 --> 00:33:15.240]   And then you kind of measure, you look at the difference in errors that you have in
[00:33:15.240 --> 00:33:18.680]   your labeling process across all these buckets, and you want to make sure that you don't have
[00:33:18.680 --> 00:33:19.680]   systematic differences.
[00:33:19.680 --> 00:33:21.600]   But of course, that's not enough.
[00:33:21.600 --> 00:33:25.560]   Then you actually look at your models themselves, you train your model and you look at things
[00:33:25.560 --> 00:33:31.720]   like, "Oh, in the prediction errors, do I have systematic differences?"
[00:33:31.720 --> 00:33:37.560]   And one cool thing to look at if you have binary classifiers and you're using...
[00:33:37.560 --> 00:33:43.040]   Whether you would be using the probability that something is civic content to prioritize
[00:33:43.040 --> 00:33:49.880]   review, in that context, it's very reasonable to use actually calibration, to look at the
[00:33:49.880 --> 00:33:56.200]   whole calibration curve and make sure that my calibration curve, which maps scores to
[00:33:56.200 --> 00:34:02.840]   actual outcome rates, make sure that those curves look similar for different groups,
[00:34:02.840 --> 00:34:06.560]   that I'm not over-predicting for one group and under-predicting for another.
[00:34:06.560 --> 00:34:10.120]   Because if I were over-predicting for a particular language, then I would be allocating more
[00:34:10.120 --> 00:34:12.160]   human resources to that language.
[00:34:12.160 --> 00:34:17.480]   And if I'm under-predicting for another, I'm allocating fewer resources, but it's not justified
[00:34:17.480 --> 00:34:23.920]   because that doesn't reflect the actual volume of content that actually needs to be reviewed
[00:34:23.920 --> 00:34:25.560]   for both.
[00:34:25.560 --> 00:34:32.360]   But I guess, is it possible that some language has more banned content?
[00:34:32.360 --> 00:34:33.920]   And then how can you be sure?
[00:34:33.920 --> 00:34:38.800]   It seems like your model would naturally use that as a feature in the model, and then it
[00:34:38.800 --> 00:34:41.360]   would naturally get over-indexed.
[00:34:41.360 --> 00:34:42.680]   How do you back that out?
[00:34:42.680 --> 00:34:47.880]   I think that's where you use calibration.
[00:34:47.880 --> 00:34:57.680]   If you think about a calibration curve, you're looking at how your scatterplot of...
[00:34:57.680 --> 00:34:58.680]   You group your scores, right?
[00:34:58.680 --> 00:35:04.120]   So, the thing is for you to score it zero to one, which we interpret as a probability
[00:35:04.120 --> 00:35:05.120]   of something being...
[00:35:05.120 --> 00:35:08.560]   It needs review, right?
[00:35:08.560 --> 00:35:13.240]   It's true that the distribution of scores is going to be different between languages.
[00:35:13.240 --> 00:35:17.280]   If one language is being more under attack, then you're going to see more stuff with a
[00:35:17.280 --> 00:35:18.280]   higher probability.
[00:35:18.280 --> 00:35:25.400]   But what you really want is once you bucket things by score, you want to look within those
[00:35:25.400 --> 00:35:32.840]   buckets, what's the actual percentage of content that was violating in that bucket.
[00:35:32.840 --> 00:35:37.600]   And you want to make sure that 0.6 means roughly 60% for any language.
[00:35:37.600 --> 00:35:41.800]   And then the number of pieces of content that fall within that bucket is going to be different
[00:35:41.800 --> 00:35:45.360]   between languages, but that's not a problem.
[00:35:45.360 --> 00:35:51.040]   And eventually, as a result of the distributions being different of scores, you end up investing
[00:35:51.040 --> 00:35:53.680]   more or less resources in a language or another.
[00:35:53.680 --> 00:35:58.040]   Or at least you have apples to apples in terms of your risk scores.
[00:35:58.040 --> 00:35:59.040]   I see.
[00:35:59.040 --> 00:36:05.920]   So, you let the model use the language, but then you back it out in sort of a post analysis
[00:36:05.920 --> 00:36:08.000]   based on the actual performance.
[00:36:08.000 --> 00:36:10.000]   Am I explaining it right?
[00:36:10.000 --> 00:36:16.280]   If you're using NLP and you're building different classifiers for different languages, then
[00:36:16.280 --> 00:36:22.400]   inevitably you're using the language in your NLP model.
[00:36:22.400 --> 00:36:28.360]   Having said this, of course, we have cross-lingual embeddings and all these fancy things, obviously,
[00:36:28.360 --> 00:36:35.520]   but you still need some sort of training data.
[00:36:35.520 --> 00:36:42.960]   The question of whether you should use an input signal or not is a long and fascinating
[00:36:42.960 --> 00:36:44.440]   discussion as well.
[00:36:44.440 --> 00:36:52.520]   And I think it is, in my view, somewhat orthogonal to many of the ways in which you would make
[00:36:52.520 --> 00:36:58.960]   sure that you have procedural fairness in your classifier.
[00:36:58.960 --> 00:37:03.000]   So we need another couple of hours to discuss that because that's actually a very active
[00:37:03.000 --> 00:37:04.640]   topic.
[00:37:04.640 --> 00:37:12.080]   One of the papers that explains this well is Cynthia Dwork's and co-workers' paper called
[00:37:12.080 --> 00:37:13.080]   "Fairness Through Awareness."
[00:37:13.080 --> 00:37:18.200]   I'm probably butchering the title, there's more to it, but this is the bit where like,
[00:37:18.200 --> 00:37:23.080]   if you're trying to be fair across genders when you're recommending job offers, should
[00:37:23.080 --> 00:37:27.200]   you actually not use gender as an input to your algorithm or should you use it?
[00:37:27.200 --> 00:37:31.680]   And there's examples that illustrate both positions.
[00:37:31.680 --> 00:37:36.240]   So I don't think it's as easy as to say, "Oh, if I don't use gender as an input to my algorithm,
[00:37:36.240 --> 00:37:39.320]   then I know I'm going to be fine."
[00:37:39.320 --> 00:37:40.320]   And the reason is simple, right?
[00:37:40.320 --> 00:37:45.000]   Is that A, you have a lot of features that correlate with gender anyway, but then also,
[00:37:45.000 --> 00:37:48.960]   if you think about it from a causal perspective, you're going to have certain things you can
[00:37:48.960 --> 00:37:54.800]   measure which have opposing effects, depending on whether you're a male or a female.
[00:37:54.800 --> 00:38:00.640]   For one, females carry babies and get gaps in their CVs.
[00:38:00.640 --> 00:38:05.280]   And so is the effect of a gap in your CV the same, depending on your circumstances, that's
[00:38:05.280 --> 00:38:06.680]   not clear, right?
[00:38:06.680 --> 00:38:15.000]   So causality actually is probably one of the most exciting lenses on fairness in many ways,
[00:38:15.000 --> 00:38:17.400]   but it's super early days.
[00:38:17.400 --> 00:38:18.400]   Interesting.
[00:38:18.400 --> 00:38:22.240]   I guess to ask you another question, it's probably another long question.
[00:38:22.240 --> 00:38:28.200]   And this is one of the ones I always worry about with the fairness and AI stuff.
[00:38:28.200 --> 00:38:34.120]   I guess, how do you engage with the people who are actually affected by these decisions?
[00:38:34.120 --> 00:38:39.280]   It always makes me a little nervous, this idea that scientists go in and get to decide
[00:38:39.280 --> 00:38:40.280]   what's fair.
[00:38:40.280 --> 00:38:41.960]   And I can kind of see why, right?
[00:38:41.960 --> 00:38:44.880]   It's important that someone understands the algorithms.
[00:38:44.880 --> 00:38:46.880]   That's one point of view.
[00:38:46.880 --> 00:38:53.000]   But how did you engage with the folks in India who are affected by this to even decide what's
[00:38:53.000 --> 00:38:55.640]   the fair thing to do?
[00:38:55.640 --> 00:39:02.400]   It's essential for AI practitioners to understand that responsible AI is not primarily an AI
[00:39:02.400 --> 00:39:03.400]   problem.
[00:39:03.400 --> 00:39:04.600]   It's as simple as that.
[00:39:04.600 --> 00:39:08.440]   And you pointed to the question of governance, who should decide?
[00:39:08.440 --> 00:39:09.880]   It's not the AI practitioner.
[00:39:09.880 --> 00:39:11.800]   It's not me, for sure.
[00:39:11.800 --> 00:39:14.640]   So what does that mean in practice?
[00:39:14.640 --> 00:39:19.920]   In practice, what it means is you build something like a fairness maturity framework.
[00:39:19.920 --> 00:39:22.240]   We're building one like this.
[00:39:22.240 --> 00:39:25.600]   You work with ethicists, with lawyers on building it.
[00:39:25.600 --> 00:39:29.800]   You try to capture the different interpretations of fairness that exist.
[00:39:29.800 --> 00:39:33.160]   And what this ends up being is not a tool that tells you what to do.
[00:39:33.160 --> 00:39:37.600]   It's a tool that gives you a big menu of questions that you should ask and consider.
[00:39:37.600 --> 00:39:43.840]   And then what you build is you build processes of consultation where you sort of put the
[00:39:43.840 --> 00:39:49.040]   options on the table and then you have a decision framework where you weigh pros and cons and
[00:39:49.040 --> 00:39:50.040]   risks.
[00:39:50.040 --> 00:39:56.840]   And again, look, this has been used way before AI, these kinds of risk assessments and decision
[00:39:56.840 --> 00:40:00.560]   processes, consultation processes, and so on.
[00:40:00.560 --> 00:40:05.200]   One example of this, I think that is quite interesting is Facebook has built this external
[00:40:05.200 --> 00:40:09.400]   advisory board that is not fully rolled out yet, but it's like 40 people, if I remember
[00:40:09.400 --> 00:40:16.720]   correctly, who represent all kinds of countries and political views and other types of views.
[00:40:16.720 --> 00:40:20.640]   And their goal is going to be in the context of content moderation to kind of look at all
[00:40:20.640 --> 00:40:24.160]   these edge cases that are hard and then come up with recommendations.
[00:40:24.160 --> 00:40:29.680]   Obviously, they're going to carry a very heavy burden of representing lots of people, but
[00:40:29.680 --> 00:40:30.680]   they don't work for Facebook, right?
[00:40:30.680 --> 00:40:33.320]   They're an external body.
[00:40:33.320 --> 00:40:38.240]   And I think that if you want ideas for what to do next, after weights and biases, I feel
[00:40:38.240 --> 00:40:43.400]   like, although I'm sure you're going to be busy with this for a while, is I think the
[00:40:43.400 --> 00:40:52.560]   question of governance in AI is, and how to build infrastructure, and this is like people
[00:40:52.560 --> 00:40:54.760]   infrastructure, right?
[00:40:54.760 --> 00:40:58.840]   For transparency, for accountability, for risk assessments.
[00:40:58.840 --> 00:41:08.040]   You see the recent EU paper on AI scratches the surface by asking some of the big questions
[00:41:08.040 --> 00:41:10.400]   that need to be answered for responsible AI.
[00:41:10.400 --> 00:41:14.800]   I think we're only getting started here, but the thing that I'm most excited about is that
[00:41:14.800 --> 00:41:20.960]   AI is going to replace humans in decision-making across the range of decisions that people
[00:41:20.960 --> 00:41:23.760]   make in any domain, right?
[00:41:23.760 --> 00:41:27.880]   And I think most of the time it's going to be a huge improvement.
[00:41:27.880 --> 00:41:36.040]   But now all of a sudden we need to go through thousands of years of political science, right?
[00:41:36.040 --> 00:41:42.560]   On how do societies govern themselves and bring that in into AI, right?
[00:41:42.560 --> 00:41:47.360]   So that's a pretty freaking daunting task, but I think this is what we're talking about.
[00:41:47.360 --> 00:41:53.080]   And every investment that I see in this is orders of magnitude smaller than it needs
[00:41:53.080 --> 00:41:54.080]   to be.
[00:41:54.080 --> 00:41:58.440]   When we were last talking, we were talking about an actual kind of case study that I
[00:41:58.440 --> 00:42:03.440]   thought was really interesting on voting in India and stopping the spread of misinformation
[00:42:03.440 --> 00:42:08.440]   and how there isn't like kind of one definition of fairness and you kind of give people a
[00:42:08.440 --> 00:42:13.160]   menu of options, which I think is a really interesting perspective.
[00:42:13.160 --> 00:42:18.000]   I guess I'm kind of wondering if you could say a little more about what might be on that
[00:42:18.000 --> 00:42:21.200]   menu of fairness, right?
[00:42:21.200 --> 00:42:25.040]   I think it's so interesting when different people have different ideas of what's fair
[00:42:25.040 --> 00:42:29.440]   and actually like you say it's not your role to resolve it, but you must have opinions
[00:42:29.440 --> 00:42:31.760]   on what feels fair and not.
[00:42:31.760 --> 00:42:33.200]   Yeah, yeah, of course.
[00:42:33.200 --> 00:42:34.280]   No, that makes a lot of sense.
[00:42:34.280 --> 00:42:40.080]   I think that the most important thing to realize, first of all, is that fairness is a bit of
[00:42:40.080 --> 00:42:42.880]   a social construct in a way.
[00:42:42.880 --> 00:42:47.880]   It depends a lot on context and it depends a lot on how a particular society has decided
[00:42:47.880 --> 00:42:49.540]   to govern itself, right?
[00:42:49.540 --> 00:42:53.140]   So fairness ends up being political inevitably.
[00:42:53.140 --> 00:42:58.060]   So let's try to ground this with a very concrete example, right?
[00:42:58.060 --> 00:43:09.340]   So here's three possible interpretations of fairness that find, that resonate both with
[00:43:09.340 --> 00:43:13.420]   moral philosophy interpretations, but also with legal interpretations.
[00:43:13.420 --> 00:43:17.600]   And finally, with mathematical interpretations, because the computer science community is also
[00:43:17.600 --> 00:43:20.560]   building metrics of algorithmic bias, all right?
[00:43:20.560 --> 00:43:23.180]   So here's a three, right?
[00:43:23.180 --> 00:43:26.740]   The first one could be minimum quality of service.
[00:43:26.740 --> 00:43:31.700]   This is also known as minimum threshold in philosophy.
[00:43:31.700 --> 00:43:40.100]   And the idea there is that you want an AI, for example, to work well enough for everybody.
[00:43:40.100 --> 00:43:45.580]   And well enough might mean if you have a computer vision system that detects people, right?
[00:43:45.580 --> 00:43:51.820]   Or detects faces to be able to put like masks on them or whatever, that it works well enough
[00:43:51.820 --> 00:43:58.140]   across things like skin tones and skin reflectance and age and gender and other characteristics,
[00:43:58.140 --> 00:43:59.140]   right?
[00:43:59.140 --> 00:44:00.520]   Like that would be sort of a concrete example.
[00:44:00.520 --> 00:44:04.540]   It doesn't matter if it works a lot better for a group than another, as long as it works
[00:44:04.540 --> 00:44:09.600]   above a certain precision recall for everybody.
[00:44:09.600 --> 00:44:13.980]   The second interpretation would be equality, right?
[00:44:13.980 --> 00:44:20.680]   So if we go back to the India misinformation example, one question there could be, if I
[00:44:20.680 --> 00:44:27.140]   have some measure of accuracy for my, I think we were talking about the civic classifier
[00:44:27.140 --> 00:44:33.260]   that basically identifies among all of the posts about cats and dogs on Facebook, what
[00:44:33.260 --> 00:44:36.460]   are the ones that are actually discussing political issues, right?
[00:44:36.460 --> 00:44:42.860]   Like maybe the political agenda of a particular politician or party, right?
[00:44:42.860 --> 00:44:48.020]   And again, like to recap, we want to find those because we have limited resources in
[00:44:48.020 --> 00:44:52.140]   terms of human reviewers that look at content and check if they violate our policies or
[00:44:52.140 --> 00:44:53.140]   the law.
[00:44:53.140 --> 00:44:56.220]   And so you want the AI to basically prioritize those cues, essentially, right?
[00:44:56.220 --> 00:45:00.620]   This is something, Lucas, that I know you understand very well, because you've worked
[00:45:00.620 --> 00:45:06.140]   on sort of human computation a lot.
[00:45:06.140 --> 00:45:13.620]   So back to equality, in India, obviously, languages and regions have a big social significance,
[00:45:13.620 --> 00:45:14.620]   right?
[00:45:14.620 --> 00:45:18.300]   Because they align with religion, they align with caste, and they align with other sort
[00:45:18.300 --> 00:45:22.660]   of important sort of social dimensions.
[00:45:22.660 --> 00:45:31.060]   So imagine that your civic classifier works well enough for everybody, for all languages
[00:45:31.060 --> 00:45:32.060]   and regions.
[00:45:32.060 --> 00:45:37.140]   Imagine that it's like under predicting a little bit for some language and over predicting
[00:45:37.140 --> 00:45:38.620]   a little bit for another language.
[00:45:38.620 --> 00:45:43.260]   So what would happen is that we would be allocating more human resources for the language where
[00:45:43.260 --> 00:45:47.440]   it over predicts and a bit too few for the one where it under predicts, right?
[00:45:47.440 --> 00:45:50.860]   So there, we actually want to have a higher standard, in a way, of fairness.
[00:45:50.860 --> 00:45:55.060]   We want to say, look, minimum quality of services is maybe not good enough.
[00:45:55.060 --> 00:46:00.380]   We want to make sure that we're offering equal protection against misinformation to everybody
[00:46:00.380 --> 00:46:02.140]   as much as possible.
[00:46:02.140 --> 00:46:08.860]   And then the third interpretation of fairness, which is sort of widely accepted, would be
[00:46:08.860 --> 00:46:12.140]   to go from equality to equity.
[00:46:12.140 --> 00:46:17.420]   And so when we think about equity, we no longer think about equal treatment, right?
[00:46:17.420 --> 00:46:24.500]   We think about, is there any group that deserves special consideration, right?
[00:46:24.500 --> 00:46:29.260]   So we're living this in the US right now, obviously, with a big awareness and awakening
[00:46:29.260 --> 00:46:32.300]   around racial justice, right?
[00:46:32.300 --> 00:46:36.740]   We're obviously, we're paying special attention to the black community in the US.
[00:46:36.740 --> 00:46:42.300]   And the reason we're doing that is because of historical structural disadvantages, right?
[00:46:42.300 --> 00:46:46.300]   So if you took this to India, there might be a legitimate question.
[00:46:46.300 --> 00:46:51.740]   Some people might ask, hey, actually, maybe historically, there's been some groups, some
[00:46:51.740 --> 00:46:57.600]   regions, some languages in India that have suffered more from manipulation or injustice.
[00:46:57.600 --> 00:47:02.500]   So therefore, we actually are going to allocate extra resources to make sure that that group
[00:47:02.500 --> 00:47:04.020]   is really protected.
[00:47:04.020 --> 00:47:09.540]   Because given the same amount of disinformation or misinformation, the harm to that group
[00:47:09.540 --> 00:47:12.140]   will be bigger, relatively speaking, right?
[00:47:12.140 --> 00:47:19.660]   And so these are questions that an AI engineer like me should be asking, but not answering.
[00:47:19.660 --> 00:47:26.220]   It's really important to basically escalate those questions to the local team, to policy
[00:47:26.220 --> 00:47:28.220]   experts, right?
[00:47:28.220 --> 00:47:31.780]   Find ways to involve external people to give an opinion.
[00:47:31.780 --> 00:47:34.180]   So that's what I mean with the menu of options.
[00:47:34.180 --> 00:47:37.920]   Each of those translates in math and in code to a different choice.
[00:47:37.920 --> 00:47:43.140]   But that choice I should not make, neither deliberately nor accidentally, by just picking
[00:47:43.140 --> 00:47:47.020]   something that looks reasonable mathematically, if I don't understand what the implications
[00:47:47.020 --> 00:47:48.260]   are.
[00:47:48.260 --> 00:47:54.260]   Do you find that it's easy to articulate those choices to a non-technical audience?
[00:47:54.260 --> 00:47:57.980]   I feel like you're framing it in a very technical way.
[00:47:57.980 --> 00:48:00.460]   Is it clear to people what they're choosing?
[00:48:00.460 --> 00:48:02.620]   Let me understand your question.
[00:48:02.620 --> 00:48:05.060]   So who would be the audience more concretely?
[00:48:05.060 --> 00:48:11.540]   Well I guess I'm imagining you're saying, "Hey, do we want to treat all regions equally
[00:48:11.540 --> 00:48:15.100]   in the India example or something else?"
[00:48:15.100 --> 00:48:21.380]   And then that something else might be, "We prefer to over-predict some regions."
[00:48:21.380 --> 00:48:24.700]   And I'm trying to picture, I guess that part makes sense.
[00:48:24.700 --> 00:48:28.440]   But it seems like actually there's sort of like this other question of like if we wanted
[00:48:28.440 --> 00:48:33.100]   to sort of do something that I think is kind of like affirmative action, right?
[00:48:33.100 --> 00:48:35.460]   In college admissions I'm picturing.
[00:48:35.460 --> 00:48:40.020]   So if you want to do that, actually then you have to get someone to tell you like kind
[00:48:40.020 --> 00:48:43.260]   of exactly the tuning that they want, right?
[00:48:43.260 --> 00:48:50.260]   And I'm not sure I could even come up with like what's exactly the fair distribution
[00:48:50.260 --> 00:48:51.260]   to apply.
[00:48:51.260 --> 00:48:53.780]   I'm not even sure how I would answer that or I'm not even sure how I would like ask
[00:48:53.780 --> 00:48:58.860]   someone that question in a way that I would get a useful answer out of them.
[00:48:58.860 --> 00:49:03.540]   We certainly don't like walk around in our heads with like exactly a particular distribution
[00:49:03.540 --> 00:49:04.740]   that feels the most fair.
[00:49:04.740 --> 00:49:05.740]   A hundred percent.
[00:49:05.740 --> 00:49:06.740]   Yeah.
[00:49:06.740 --> 00:49:14.220]   And I think that's exactly the reason why equity is the hardest of these three lenses
[00:49:14.220 --> 00:49:16.560]   on fairness, right?
[00:49:16.560 --> 00:49:23.240]   So I think in practice you'll find that most teams, most product teams, most AI engineers
[00:49:23.240 --> 00:49:28.680]   will be either asking questions of minimum quality of service, right?
[00:49:28.680 --> 00:49:31.240]   And if you want, we can talk about how to operationalize that.
[00:49:31.240 --> 00:49:34.040]   It's surprisingly easy actually.
[00:49:34.040 --> 00:49:39.000]   Or questions of equality, of equal treatment, which is conceptually easy but a bit harder
[00:49:39.000 --> 00:49:40.000]   to implement.
[00:49:40.000 --> 00:49:45.000]   When it comes to questions of equity, these are not really questions that are directly
[00:49:45.000 --> 00:49:48.040]   addressed to AI engineers.
[00:49:48.040 --> 00:49:55.160]   These are really questions that are, that the overarching leader of a product needs
[00:49:55.160 --> 00:49:57.320]   to be reasoning about equity.
[00:49:57.320 --> 00:49:59.880]   I'll give you a concrete example.
[00:49:59.880 --> 00:50:06.560]   Adam Mosseri, who leads the Instagram team, has started to make public posts that you
[00:50:06.560 --> 00:50:12.480]   can Google about Instagram and equity.
[00:50:12.480 --> 00:50:17.160]   And basically what he's starting to do, he's starting to, he's initiating a dialogue where
[00:50:17.160 --> 00:50:23.760]   he's saying, "Hey, we would put the interests of communities above the interests of Instagram.
[00:50:23.760 --> 00:50:29.120]   If we feel that a certain product causes unintended harm to a community or that it doesn't serve
[00:50:29.120 --> 00:50:34.480]   it as well as we intended, then we will actually stop and rethink it."
[00:50:34.480 --> 00:50:36.120]   What does that translate exactly?
[00:50:36.120 --> 00:50:40.920]   Like if I'm running ads and I feel like, "Oh, ads isn't working for everybody."
[00:50:40.920 --> 00:50:41.920]   Does that mean I shut it down?
[00:50:41.920 --> 00:50:42.920]   Do I have a percent?
[00:50:42.920 --> 00:50:45.280]   Do I say, "Oh, I cut my losses at minus 10%."
[00:50:45.280 --> 00:50:52.440]   Or if I'm, actually we don't need to stay within the Facebook, Inc. sphere.
[00:50:52.440 --> 00:50:56.520]   I have close friends at Spotify and at Netflix.
[00:50:56.520 --> 00:51:02.800]   The same questions occur there as well in like, "Hey, do we inject some diversity of
[00:51:02.800 --> 00:51:03.880]   content?
[00:51:03.880 --> 00:51:08.160]   Do we allow some producers, some musicians, some filmmakers that are maybe a little bit
[00:51:08.160 --> 00:51:11.160]   in the shade to pop up?"
[00:51:11.160 --> 00:51:15.240]   And then what's the hit that we're taking in terms of our engagement metrics, in terms
[00:51:15.240 --> 00:51:20.840]   of how many songs people listen per day or how many movies or shows people watch per
[00:51:20.840 --> 00:51:21.840]   day and stuff like that.
[00:51:21.840 --> 00:51:27.880]   I don't think there's an exact science on that at all, but it's a very real question
[00:51:27.880 --> 00:51:31.640]   that many people are reasoning about.
[00:51:31.640 --> 00:51:41.080]   The last thing I'll say is that one of the big challenges is a question of governance.
[00:51:41.080 --> 00:51:42.400]   And I think you were alluding to that.
[00:51:42.400 --> 00:51:44.840]   It's a question of who decides.
[00:51:44.840 --> 00:51:47.160]   And if you think about it, we have democratic processes.
[00:51:47.160 --> 00:51:48.360]   I live in Mountain View.
[00:51:48.360 --> 00:51:52.440]   The city of Mountain View decides where we put bicycle lanes.
[00:51:52.440 --> 00:51:55.800]   And of course, they're going to slow down traffic, but they're going to create other
[00:51:55.800 --> 00:51:57.080]   benefits.
[00:51:57.080 --> 00:52:03.240]   They're going to decide on urban density, on things that are trade-offs.
[00:52:03.240 --> 00:52:09.920]   There's obviously in luxury resorts and stuff like that, like in Truckee, I know because
[00:52:09.920 --> 00:52:19.340]   we recently bought a house there, and the city council will demand that you reserve
[00:52:19.340 --> 00:52:28.340]   a certain part of land and building for less expensive dwellings to give access to housing
[00:52:28.340 --> 00:52:30.340]   to everyone.
[00:52:30.340 --> 00:52:34.380]   And in those cases, it's a bit easier because there's a democratic process by which that
[00:52:34.380 --> 00:52:35.660]   city council gets elected.
[00:52:35.660 --> 00:52:37.420]   There's public consultations.
[00:52:37.420 --> 00:52:43.040]   I feel like if I think about one of the challenges that we're facing as technology companies
[00:52:43.040 --> 00:52:48.260]   is this idea of how do we bring in public deliberation and consultation mechanisms in
[00:52:48.260 --> 00:52:49.260]   the decisions we make.
[00:52:49.260 --> 00:52:56.620]   At Facebook, of course, we're launching this external oversight board for content moderation,
[00:52:56.620 --> 00:52:58.420]   which is almost ready to go.
[00:52:58.420 --> 00:53:00.900]   We have all the members identified.
[00:53:00.900 --> 00:53:02.260]   And I think this is only the beginning.
[00:53:02.260 --> 00:53:04.420]   I think that we're going to see a lot more of this.
[00:53:04.420 --> 00:53:06.340]   It was a very long answer.
[00:53:06.340 --> 00:53:07.340]   You have to cut me.
[00:53:07.340 --> 00:53:08.340]   It's a great answer.
[00:53:08.340 --> 00:53:12.340]   And I got to ask, though, you talked about operationalizing.
[00:53:12.340 --> 00:53:17.260]   And I do think that I was kind of thinking as I was asking the last question about getting
[00:53:17.260 --> 00:53:18.340]   the details right.
[00:53:18.340 --> 00:53:22.780]   I think a lot of the mistakes that you see around algorithmic fairness, they're so glaring.
[00:53:22.780 --> 00:53:27.980]   Probably the more important thing for most of the people that are listening to this interview
[00:53:27.980 --> 00:53:29.780]   is how do you operationalize the basic stuff?
[00:53:29.780 --> 00:53:32.900]   How do you make sure your thing isn't egregiously unfair?
[00:53:32.900 --> 00:53:35.500]   Maybe your first definition.
[00:53:35.500 --> 00:53:36.500]   Yeah.
[00:53:36.500 --> 00:53:39.060]   And thank you for asking that question because obviously we should talk about equity as a
[00:53:39.060 --> 00:53:44.820]   society, but as engineers, there's a lot of stuff we can do to make sure that our stuff,
[00:53:44.820 --> 00:53:47.620]   that our crap is built right.
[00:53:47.620 --> 00:53:53.740]   And so obviously I have friends at Twitter, obviously, and I know that there was a very
[00:53:53.740 --> 00:54:00.420]   recent new cycle on the AI that automatically crops images.
[00:54:00.420 --> 00:54:05.620]   And the challenge there is that some people externally actually tested the system and
[00:54:05.620 --> 00:54:10.180]   realized like, oh, if I put a picture of a white person and a black person and they're
[00:54:10.180 --> 00:54:14.180]   a little bit apart and I create some blank in between and it's like a rectangular shaped
[00:54:14.180 --> 00:54:18.020]   picture, then obviously there'll be some cropping to kind of like render or something.
[00:54:18.020 --> 00:54:22.240]   And then disproportionately it seemed to pick the picture of the white person.
[00:54:22.240 --> 00:54:24.420]   So how did that happen?
[00:54:24.420 --> 00:54:26.620]   How does something like that happen?
[00:54:26.620 --> 00:54:32.900]   Well, Twitter says they tested for racial bias, which is great before they launched
[00:54:32.900 --> 00:54:34.740]   this AI as they should.
[00:54:34.740 --> 00:54:36.580]   And I commend them for that.
[00:54:36.580 --> 00:54:39.620]   But the devil, as you say, is in the details.
[00:54:39.620 --> 00:54:43.180]   Do I have the right test sets?
[00:54:43.180 --> 00:54:49.100]   Did I cover skin tones correctly?
[00:54:49.100 --> 00:54:55.220]   And there's a bunch of work by people like Joy Boylan-Winney and Timnit Gebru and many
[00:54:55.220 --> 00:55:01.220]   other coworkers, which is brilliant on having reference papers that kind of propose methodologies
[00:55:01.220 --> 00:55:04.940]   for doing these things.
[00:55:04.940 --> 00:55:15.860]   What we're doing and my advice in general to the community is to invest heavily in transparency.
[00:55:15.860 --> 00:55:22.620]   We have started to do this with media manipulation and deep fakes where we've built and created
[00:55:22.620 --> 00:55:24.060]   data sets.
[00:55:24.060 --> 00:55:26.020]   And this is only the beginning.
[00:55:26.020 --> 00:55:32.140]   I think that for many of these biases, whether it's in computer vision algorithms, in speech
[00:55:32.140 --> 00:55:38.260]   recognition algorithms, in ranking algorithms, in whatever you want, having methodologies
[00:55:38.260 --> 00:55:44.260]   that you can publish and talk about, where applicable data sets that you can share, I
[00:55:44.260 --> 00:55:47.380]   think is going to be the way to go.
[00:55:47.380 --> 00:55:48.380]   I see.
[00:55:48.380 --> 00:55:52.740]   So investing in being transparent about how you're doing things.
[00:55:52.740 --> 00:55:56.580]   And is the benefit there that you can get external people to comment on what you're
[00:55:56.580 --> 00:55:59.980]   doing and kind of catch unexpected errors?
[00:55:59.980 --> 00:56:01.420]   I think there's two benefits.
[00:56:01.420 --> 00:56:03.740]   One is accountability.
[00:56:03.740 --> 00:56:07.580]   Transparency implies accountability in a way.
[00:56:07.580 --> 00:56:12.860]   If I declare to the world that I have methods and processes for testing my stuff, then it
[00:56:12.860 --> 00:56:14.340]   better be that I have them.
[00:56:14.340 --> 00:56:18.060]   And it better be that they don't suck.
[00:56:18.060 --> 00:56:23.180]   And then on the latter part on whether they suck or not, I guess it's just like open sourcing
[00:56:23.180 --> 00:56:24.180]   or publishing.
[00:56:24.180 --> 00:56:25.180]   It's the same philosophy.
[00:56:25.500 --> 00:56:34.820]   If I open source my code, then often I actually get quite a bit of constructive feedback and
[00:56:34.820 --> 00:56:37.180]   the community helps improve it.
[00:56:37.180 --> 00:56:41.340]   To be clear, I think as a community at large, there's been a couple of really good papers.
[00:56:41.340 --> 00:56:47.780]   Google and Microsoft have done excellent work in proposing things like data sheets for data
[00:56:47.780 --> 00:56:53.660]   sets and fact sheets for models and things like that that sort of invite for transparency.
[00:56:53.660 --> 00:56:56.900]   I don't think that we have, we're not at a stage where there's like standards that are
[00:56:56.900 --> 00:57:03.740]   sort of like widely adopted, but my prediction is that that's where the field is going.
[00:57:03.740 --> 00:57:09.260]   How would this work if, I'm just channeling my audience, which is a lot of startups and
[00:57:09.260 --> 00:57:12.420]   companies, not Facebook or Twitter.
[00:57:12.420 --> 00:57:18.340]   How would you go about being transparent if you're a smaller company?
[00:57:18.340 --> 00:57:21.060]   What would you do?
[00:57:21.060 --> 00:57:22.060]   Yeah.
[00:57:22.060 --> 00:57:23.060]   Good question.
[00:57:23.060 --> 00:57:31.660]   I would, where possible, use public data sets of which there exists.
[00:57:31.660 --> 00:57:37.220]   I'm thinking again about, if you look up the work of Joy Boulamwini and Timnit Gebru and
[00:57:37.220 --> 00:57:41.820]   many other co-authors, there are like reference data sets that have been proposed for things
[00:57:41.820 --> 00:57:48.700]   like face detection and gender recognition and so on.
[00:57:48.700 --> 00:57:50.180]   That's just one example.
[00:57:50.180 --> 00:57:52.820]   There's a lot more out there.
[00:57:52.820 --> 00:57:57.640]   Make sure to use them as reference data sets and maybe even report how your algorithms
[00:57:57.640 --> 00:57:58.640]   perform on those.
[00:57:58.640 --> 00:58:03.060]   If you're building a human centric AI application, that would be one.
[00:58:03.060 --> 00:58:10.580]   If you're a startup that's using Azure or Google Cloud, if you're using any of the big
[00:58:10.580 --> 00:58:19.500]   three, like Cloud ML providers, all of them are actually offering libraries to detect
[00:58:19.500 --> 00:58:25.660]   biases in the data and in the predictions of the model.
[00:58:25.660 --> 00:58:29.540]   Definitely use those and report on what you found.
[00:58:29.540 --> 00:58:34.380]   Then the second thing of course is put pressure on these companies.
[00:58:34.380 --> 00:58:40.980]   If they're providing you some pre-learned embeddings, whether it's for computer vision
[00:58:40.980 --> 00:58:45.100]   or for NLP or whatever, ask for transparency.
[00:58:45.100 --> 00:58:56.060]   Ask about are there any funky things going on in these text embeddings?
[00:58:56.060 --> 00:59:05.180]   There's a bunch of papers out there that show that if you train word or sentence embeddings
[00:59:05.180 --> 00:59:10.540]   of big existing corporeal, like Wikipedia or whatever you want, inevitably you're going
[00:59:10.540 --> 00:59:14.620]   to see well documented things.
[00:59:14.620 --> 00:59:26.540]   Computer related nouns tend to fall closer in embedded space to stereotypically male
[00:59:26.540 --> 00:59:27.540]   jobs and et cetera.
[00:59:27.540 --> 00:59:28.540]   The same for females.
[00:59:28.540 --> 00:59:31.540]   That could have nasty consequences downstream.
[00:59:31.540 --> 00:59:34.100]   At least you should be aware of those.
[00:59:34.100 --> 00:59:39.180]   I guess that's a good segue into something I wanted to make sure I talked to you about,
[00:59:39.180 --> 00:59:42.460]   which is diversity inclusion in AI.
[00:59:42.460 --> 00:59:46.700]   I bring this topic up with a little bit of awareness to the fact that we're both middle
[00:59:46.700 --> 00:59:49.660]   aged white guys talking to each other about diversity and inclusion.
[00:59:49.660 --> 00:59:51.420]   You have thought a lot about it.
[00:59:51.420 --> 00:59:55.900]   I'd love to hear what you think can be done, should be done, is being done.
[00:59:55.900 --> 01:00:00.380]   First of all, I'd like to share a story.
[01:00:00.380 --> 01:00:05.580]   I don't know if I'm repeating myself if we talked about that last time.
[01:00:05.580 --> 01:00:13.860]   When Yo-Yo Ma came to NeurIPS in 2018, he came to a workshop where the theme of the
[01:00:13.860 --> 01:00:17.020]   workshop was AI for social good.
[01:00:17.020 --> 01:00:24.100]   One of the questions that was asked to Yo-Yo Ma was, how do you build trustworthy AI?
[01:00:24.100 --> 01:00:29.240]   It's interesting.
[01:00:29.240 --> 01:00:34.140]   He said the most important thing for him was to understand who was behind the AI.
[01:00:34.140 --> 01:00:36.100]   Who are the humans behind the AI?
[01:00:36.100 --> 01:00:37.100]   What are their intentions?
[01:00:37.100 --> 01:00:38.100]   What are their fears?
[01:00:38.100 --> 01:00:40.500]   What is their background?
[01:00:40.500 --> 01:00:42.820]   What perspective do they bring to the table?
[01:00:42.820 --> 01:00:44.540]   What are their values at the end of the day?
[01:00:44.540 --> 01:00:47.180]   What's the values of the human building the AI?
[01:00:47.180 --> 01:00:55.460]   This actually echoes a theme that's very common when talking about building AI responsibly.
[01:00:55.460 --> 01:01:01.300]   One of the main questions we tend to get is, well, how diverse and how inclusive is the
[01:01:01.300 --> 01:01:04.120]   team behind the AI?
[01:01:04.120 --> 01:01:07.660]   Diversity is a tricky concept.
[01:01:07.660 --> 01:01:09.860]   Diversity really means heterogeneity.
[01:01:09.860 --> 01:01:24.300]   It means having a team that has people from different, both innate but also chosen characteristics.
[01:01:24.300 --> 01:01:27.300]   Obviously, I was born in Spain.
[01:01:27.300 --> 01:01:30.260]   I'm a white dude.
[01:01:30.260 --> 01:01:32.300]   That means certain things.
[01:01:32.300 --> 01:01:39.300]   But then I have my political, religious, and maybe sexual and other choices that I made
[01:01:39.300 --> 01:01:40.700]   myself.
[01:01:40.700 --> 01:01:47.620]   That defines who I am and reflects what my both values and circumstances are.
[01:01:47.620 --> 01:01:54.340]   Bringing that diversity into teams is very important because teams that have that diversity
[01:01:54.340 --> 01:01:58.640]   tend to make better decisions because they tend to look at the problem from multiple
[01:01:58.640 --> 01:02:07.060]   angles and they tend to ask more and harder questions.
[01:02:07.060 --> 01:02:10.580]   Inclusion means meaningful representation.
[01:02:10.580 --> 01:02:14.580]   If you're at a cocktail party, it's one thing for everybody to get a cocktail.
[01:02:14.580 --> 01:02:15.580]   There's inclusion.
[01:02:15.580 --> 01:02:16.980]   We're giving cocktails to everybody.
[01:02:16.980 --> 01:02:24.460]   It's a different thing to really sit down and understand what do you like drinking.
[01:02:24.460 --> 01:02:30.700]   Making sure your voice is heard and your experience -- you're not just a token person ticking
[01:02:30.700 --> 01:02:34.420]   a box in the diversity list.
[01:02:34.420 --> 01:02:35.420]   That you be included.
[01:02:35.420 --> 01:02:43.460]   That means creating a space where everyone's voice is heard, where people feel like they
[01:02:43.460 --> 01:02:50.740]   can bring their authentic self to work and express themselves and not be shut down.
[01:02:50.740 --> 01:02:54.340]   A couple of questions.
[01:02:54.340 --> 01:02:56.500]   Question number one is how do you do that?
[01:02:56.500 --> 01:02:58.380]   How do you build a team like that?
[01:02:58.380 --> 01:03:00.980]   Question number two, why do you do it?
[01:03:00.980 --> 01:03:05.700]   Do you build a team like that because it's a necessary condition in order to build responsible
[01:03:05.700 --> 01:03:06.700]   AI?
[01:03:06.700 --> 01:03:09.940]   I'm going to start with the second question because it's tricky.
[01:03:09.940 --> 01:03:14.340]   This is something that we've debated a lot.
[01:03:14.340 --> 01:03:21.220]   One danger, one risk in coupling together the goal of responsible AI and the goal of
[01:03:21.220 --> 01:03:27.740]   diversity and inclusion is that the burden of building responsible AI can inadvertently
[01:03:27.740 --> 01:03:32.460]   fall more heavily on the shoulders of underrepresented people.
[01:03:32.460 --> 01:03:37.140]   This is something that we've seen in many forums, not just at Facebook.
[01:03:37.140 --> 01:03:44.020]   Whenever I'm in a forum that's trying to tackle issues of responsible AI, disproportionately
[01:03:44.020 --> 01:03:51.180]   I find myself in the company of women or underrepresented minorities.
[01:03:51.180 --> 01:03:57.020]   And it's interesting because I guess there's this sense of duty in making sure that the
[01:03:57.020 --> 01:04:01.860]   AI is built responsibly if you are a member of an underrepresented group.
[01:04:01.860 --> 01:04:08.340]   The danger a little bit with ascribing the goal of diversity and inclusion as a means
[01:04:08.340 --> 01:04:13.540]   to achieving responsible AI is that you might inadvertently put that burden on only on a
[01:04:13.540 --> 01:04:14.540]   subgroup.
[01:04:14.540 --> 01:04:19.300]   It's very, very important to actually keep these two goals separate and to say, "No,
[01:04:19.300 --> 01:04:21.500]   building responsibly AI is a duty of everybody."
[01:04:21.500 --> 01:04:22.500]   Period.
[01:04:22.500 --> 01:04:28.060]   Now, let's turn our attention to building diverse and inclusive teams, which will help
[01:04:28.060 --> 01:04:33.740]   with responsible AI goals, but it also will just create teams that make better decisions
[01:04:33.740 --> 01:04:34.740]   overall.
[01:04:34.740 --> 01:04:36.140]   So let's just focus on that part.
[01:04:36.140 --> 01:04:37.140]   So how do you do this?
[01:04:37.140 --> 01:04:38.140]   All right.
[01:04:38.140 --> 01:04:40.940]   I have one more question on that first point.
[01:04:40.940 --> 01:04:48.020]   So I feel like few people would argue that diversity and inclusion is bad, but it does
[01:04:48.020 --> 01:04:55.900]   seem like it comes up particularly frequently in AI, even within CS practices.
[01:04:55.900 --> 01:05:01.020]   I feel like you don't hear a lot about diversity and inclusion in say, databases, but yet we
[01:05:01.020 --> 01:05:05.140]   touch databases probably maybe more than we touch AI systems.
[01:05:05.140 --> 01:05:09.420]   Why do you think that it's such an important topic for AI in particular?
[01:05:09.420 --> 01:05:10.420]   Yeah, no, thank you.
[01:05:10.420 --> 01:05:11.420]   Very, very good point.
[01:05:11.420 --> 01:05:12.420]   Yeah.
[01:05:12.420 --> 01:05:22.620]   I mean, I guess because AI is increasingly replacing humans in making consequential decisions.
[01:05:22.620 --> 01:05:28.300]   Algorithms are used in criminal justice to assist judges with assessing risk and deciding
[01:05:28.300 --> 01:05:36.500]   whether someone can go out on bail or whether they have to wait for trial in jail, in the
[01:05:36.500 --> 01:05:42.940]   education system or in employment to automatically pre-select resumes and things like that.
[01:05:42.940 --> 01:05:43.940]   With huge promise, right?
[01:05:43.940 --> 01:05:48.340]   Because humans are very biased actually at selecting resumes, right?
[01:05:48.340 --> 01:05:49.340]   In medicine, right?
[01:05:49.340 --> 01:05:51.420]   For automatic diagnosis, et cetera, et cetera.
[01:05:51.420 --> 01:05:59.580]   And so the opportunity to reflect and amplify biases that exist in society in these automated
[01:05:59.580 --> 01:06:03.900]   decisions is real and is very high, right?
[01:06:03.900 --> 01:06:05.700]   And so the stakes are very high.
[01:06:05.700 --> 01:06:06.700]   The stakes are higher.
[01:06:06.700 --> 01:06:09.540]   A database is not going to make an automatic decision.
[01:06:09.540 --> 01:06:12.660]   It's going to give you some data.
[01:06:12.660 --> 01:06:15.660]   An AI is actually going to make a prediction or is going to make a decision.
[01:06:15.660 --> 01:06:17.380]   That makes sense.
[01:06:17.380 --> 01:06:18.380]   Okay.
[01:06:18.380 --> 01:06:20.820]   So how do you do it?
[01:06:20.820 --> 01:06:28.540]   So typically, any diversity and inclusion strategy tends to include two big buckets,
[01:06:28.540 --> 01:06:29.540]   right?
[01:06:29.540 --> 01:06:30.540]   One is hiring, right?
[01:06:30.540 --> 01:06:33.460]   Like how do I hire more diverse people?
[01:06:33.460 --> 01:06:36.220]   And the other one is retention and growth, right?
[01:06:36.220 --> 01:06:40.540]   Like if I have an employee base that exists already, how do I make sure that not only
[01:06:40.540 --> 01:06:46.540]   that I don't lose the underrepresented people, but let's just make sure that everyone has
[01:06:46.540 --> 01:06:51.200]   the same opportunities to grow and develop in their career, right?
[01:06:51.200 --> 01:06:56.580]   So that's the first thing is you've got to have a strategy that focuses on finding people,
[01:06:56.580 --> 01:07:00.900]   growing people, keeping people.
[01:07:00.900 --> 01:07:04.180]   But then how do you do this, right?
[01:07:04.180 --> 01:07:07.660]   You could be tempted, we're all engineers and we love numbers and metrics, right?
[01:07:07.660 --> 01:07:10.180]   And I could say, "Wow, this is easy.
[01:07:10.180 --> 01:07:15.220]   Let's just figure out, let's just map every individual to some demographics.
[01:07:15.220 --> 01:07:19.320]   Let's just figure out whether they're underrepresented people or not.
[01:07:19.320 --> 01:07:26.460]   Maybe we treat gender and other dimensions separately so that we don't address..."
[01:07:26.460 --> 01:07:28.900]   Because it's intersectionality issues and all the things like that.
[01:07:28.900 --> 01:07:32.100]   So you want to make sure that, just assume that you have the right metrics, right?
[01:07:32.100 --> 01:07:35.740]   And then you could say, "Okay, so for every team, I have a target number that I want to
[01:07:35.740 --> 01:07:37.100]   hit and so on."
[01:07:37.100 --> 01:07:42.900]   That's very tempting, but it's also fraught with tons and tons of unintended consequences,
[01:07:42.900 --> 01:07:43.900]   right?
[01:07:43.900 --> 01:07:50.060]   Because what might happen is that people, people managers might end up making decisions
[01:07:50.060 --> 01:07:55.860]   that actually take into consideration someone's group membership.
[01:07:55.860 --> 01:07:57.700]   And that could be problematic, right?
[01:07:57.700 --> 01:07:59.760]   For a number of reasons.
[01:07:59.760 --> 01:08:06.300]   Reason number one, you never want anybody to be in a position where they feel like if
[01:08:06.300 --> 01:08:10.300]   they got promoted, they got promoted because they were black or because they were a female,
[01:08:10.300 --> 01:08:11.300]   right?
[01:08:11.300 --> 01:08:14.060]   And you don't want others on the team to point fingers at them and say, "Oh yeah, sure.
[01:08:14.060 --> 01:08:15.740]   You got promoted for that reason," right?
[01:08:15.740 --> 01:08:17.580]   Or "You got hired for that reason," right?
[01:08:17.580 --> 01:08:26.540]   It's extremely important that either whether someone is hired or not, or whether they get
[01:08:26.540 --> 01:08:33.220]   promoted is purely based on merit, right?
[01:08:33.220 --> 01:08:34.740]   So then what do you do?
[01:08:34.740 --> 01:08:45.220]   Well, what you do is that you ensure that everyone has consistent equal opportunities
[01:08:45.220 --> 01:08:47.340]   and the right amount of support, right?
[01:08:47.340 --> 01:08:49.180]   To succeed, right?
[01:08:49.180 --> 01:08:51.900]   You make sure that no one is left behind.
[01:08:51.900 --> 01:08:56.100]   When it comes to hiring, to be very specific, right?
[01:08:56.100 --> 01:09:00.140]   What you can do in hiring is you can use the so-called Rooney rule.
[01:09:00.140 --> 01:09:01.660]   I don't know if you've heard of that rule.
[01:09:01.660 --> 01:09:03.620]   It comes from the NFL.
[01:09:03.620 --> 01:09:09.580]   So the goal of that rule was to increase the racial diversity of football coaches in the
[01:09:09.580 --> 01:09:10.900]   US.
[01:09:10.900 --> 01:09:18.260]   And there was a coach called, I can't remember right now if it was a coach or a team owner.
[01:09:18.260 --> 01:09:24.260]   Anyway, someone in a position of power called Rooney, who basically came up with this idea,
[01:09:24.260 --> 01:09:27.300]   which was like to say, "Hey, what we're going to do is we're going to make sure that we
[01:09:27.300 --> 01:09:35.740]   have a diverse slate of people that we consider whenever there's a job opening," right?
[01:09:35.740 --> 01:09:39.100]   Because one of the dangers, actually one of the biggest risks to diversity at the hiring
[01:09:39.100 --> 01:09:44.900]   side is that people tend to hire their friends, right?
[01:09:44.900 --> 01:09:46.900]   And obviously your friends tend to be like you.
[01:09:46.900 --> 01:09:54.540]   And so what the diverse slate approach or DSA aims to do is to say, "Look, if you have
[01:09:54.540 --> 01:09:58.580]   a position open, you can't just go and do like an opportunistic hiring where you just
[01:09:58.580 --> 01:10:01.780]   go and grab your friend or your pal and you went to college with or whatever.
[01:10:01.780 --> 01:10:08.060]   You actually have to write an inclusive job description, which means that you really focus
[01:10:08.060 --> 01:10:11.660]   on the things that are really relevant to the job, right?
[01:10:11.660 --> 01:10:12.660]   And nothing else.
[01:10:12.660 --> 01:10:17.060]   And then second, you just go out and you make sure that you have a slate of candidates that
[01:10:17.060 --> 01:10:19.180]   you consider that is diverse.
[01:10:19.180 --> 01:10:23.940]   Now once you have that slate, the bar is the same for everybody, right?
[01:10:23.940 --> 01:10:25.420]   And that worked like in the NFL.
[01:10:25.420 --> 01:10:31.340]   I mean, there's a bunch of papers written about this, but don't write, it helps.
[01:10:31.340 --> 01:10:33.620]   So that's one thing you can do, right?
[01:10:33.620 --> 01:10:39.340]   Other things that you can do is make sure that you have outreach programs to create
[01:10:39.340 --> 01:10:43.060]   like awareness about the types of work that exist.
[01:10:43.060 --> 01:10:47.740]   Because again, people tend to reach out or source from sort of familiar circles.
[01:10:47.740 --> 01:10:49.780]   You got to break that barrier.
[01:10:49.780 --> 01:10:56.220]   And then inside the organization on like growing and keeping people, this starts with very
[01:10:56.220 --> 01:10:58.100]   simple things.
[01:10:58.100 --> 01:11:03.940]   Like do you have any kind of training inside the company?
[01:11:03.940 --> 01:11:09.580]   And again, like if you're a small startup, there's a lot of resources.
[01:11:09.580 --> 01:11:20.460]   If you Google diversity and inclusion resources, there's a lot of like awesome talks and resources
[01:11:20.460 --> 01:11:23.180]   out there to just raise awareness.
[01:11:23.180 --> 01:11:24.180]   There's these small things.
[01:11:24.260 --> 01:11:27.820]   There's this training that we have at Facebook called Be the Ally.
[01:11:27.820 --> 01:11:33.340]   And Be the Ally really is about just keeping an eye out for people who may be in a meeting
[01:11:33.340 --> 01:11:38.020]   or at work tend to be like more silent or like aren't noticed.
[01:11:38.020 --> 01:11:41.020]   Maybe their voice isn't heard, right?
[01:11:41.020 --> 01:11:45.300]   Or even maybe you sort of like witness like a microaggression or a macroaggression.
[01:11:45.300 --> 01:11:49.220]   I mean, if it's a macroaggression, then everybody notices in theory, right?
[01:11:49.220 --> 01:11:52.900]   But like keeping an eye for these things and then checking in with people, right?
[01:11:52.900 --> 01:11:56.540]   Like taking it on yourself to sort of go and like say, "Hey, I noticed this thing.
[01:11:56.540 --> 01:11:58.780]   It didn't feel like it was okay to me.
[01:11:58.780 --> 01:11:59.780]   How did you feel?"
[01:11:59.780 --> 01:12:05.820]   And these small things, they make a huge difference in creating an inclusive culture.
[01:12:05.820 --> 01:12:07.020]   On career growth, right?
[01:12:07.020 --> 01:12:14.100]   Like every manager should have a personalized career growth plan for everyone on their team.
[01:12:14.100 --> 01:12:16.100]   And everyone is different, right?
[01:12:16.100 --> 01:12:21.540]   Has different interests, has different strengths, has different areas that they want to develop.
[01:12:21.540 --> 01:12:25.780]   Making sure that you consistently personalize your career growth plan for each of your employees
[01:12:25.780 --> 01:12:27.580]   actually makes a very big difference as well.
[01:12:27.580 --> 01:12:32.060]   Because sometimes if you templatize it and your requirements are like the same, they
[01:12:32.060 --> 01:12:37.060]   might not be adequate for certain people.
[01:12:37.060 --> 01:12:44.140]   That and a lot more, but that kind of like gives you a couple of ideas.
[01:12:44.140 --> 01:12:46.820]   I'm curious if you have any suggestions for this.
[01:12:46.820 --> 01:12:54.140]   So these recordings that we do, we can look at the gender distribution of the people watching
[01:12:54.140 --> 01:12:58.300]   and it actually skews over 90% male.
[01:12:58.300 --> 01:13:01.860]   And it's funny because our user base is actually not that lopsided.
[01:13:01.860 --> 01:13:08.220]   So it feels like maybe there's something we're doing that's not connecting at least with
[01:13:08.220 --> 01:13:09.780]   women that might be watching it.
[01:13:09.780 --> 01:13:12.820]   I guess, how would you approach that?
[01:13:13.500 --> 01:13:17.100]   My initial reaction was, please ask the women who are not watching.
[01:13:17.100 --> 01:13:25.300]   But a couple of thoughts that come to mind, I mean, that one actually is extremely important,
[01:13:25.300 --> 01:13:26.300]   right?
[01:13:26.300 --> 01:13:28.820]   Like doing some user research and understanding like, why is this not resonating?
[01:13:28.820 --> 01:13:31.140]   Why is this not useful to you?
[01:13:31.140 --> 01:13:37.620]   A couple of other thoughts, obviously, one of them is like on the topics that are covered,
[01:13:37.620 --> 01:13:38.620]   right?
[01:13:38.620 --> 01:13:39.620]   Which is actually tied to what I just said, right?
[01:13:39.620 --> 01:13:42.500]   So to understand whether those are not useful.
[01:13:42.500 --> 01:13:46.980]   And then the other one, which I don't know if you already do, is to make sure to have
[01:13:46.980 --> 01:13:51.820]   at least as many female speakers as you have male speakers.
[01:13:51.820 --> 01:13:53.060]   And again, that's an interesting thing, right?
[01:13:53.060 --> 01:13:58.580]   Because you might say, well, maybe the potential audience is actually a majority male and a
[01:13:58.580 --> 01:13:59.580]   minority female.
[01:13:59.580 --> 01:14:01.540]   But that is an interesting point of intervention, right?
[01:14:01.540 --> 01:14:05.260]   Where you could sort of say, well, that's fine, but we're still going to aim for 50/50.
[01:14:05.260 --> 01:14:06.260]   Makes sense.
[01:14:06.260 --> 01:14:12.580]   You know, we always end with two questions and I'd love to do that with you too and kind
[01:14:12.580 --> 01:14:13.580]   of get your thoughts.
[01:14:13.580 --> 01:14:16.500]   These are a little bit open ended and I haven't prepped you for them, but I'm curious what
[01:14:16.500 --> 01:14:17.500]   you say.
[01:14:17.500 --> 01:14:23.540]   So one question we always end with is, what's an underrated aspect of machine learning that
[01:14:23.540 --> 01:14:25.940]   you think people should pay more attention to?
[01:14:25.940 --> 01:14:30.780]   And maybe for you, I'd tailor it to sort of like in ethics and responsible AI, what's
[01:14:30.780 --> 01:14:33.540]   something that you feel like maybe doesn't get the attention it deserves?
[01:14:33.540 --> 01:14:37.700]   It might be the whole topic doesn't, but like within that, is there something you'd especially
[01:14:37.700 --> 01:14:38.700]   like to call out?
[01:14:38.700 --> 01:14:39.700]   Yeah.
[01:14:39.700 --> 01:14:40.980]   Just be aware of averages, you know?
[01:14:40.980 --> 01:14:43.060]   I hate averages and aggregated metrics.
[01:14:43.060 --> 01:14:45.420]   It's as simple as that.
[01:14:45.420 --> 01:14:50.740]   If you want to develop AI more responsibly, try to desegregate your metrics.
[01:14:50.740 --> 01:14:52.420]   You just talked about gender, right?
[01:14:52.420 --> 01:14:56.100]   If you have access to gender or if you have ways in which you can have like the right
[01:14:56.100 --> 01:15:00.580]   sort of explicit informed consent for your users to give their gender and be transparent
[01:15:00.580 --> 01:15:04.900]   about what you're going to use it for, if you have location, because obviously even
[01:15:04.900 --> 01:15:06.940]   if you only take the US, right?
[01:15:06.940 --> 01:15:10.700]   The South is very different from the coast, very different from the center and so on,
[01:15:10.700 --> 01:15:11.700]   right?
[01:15:11.700 --> 01:15:12.700]   Like desegregate your metrics.
[01:15:12.700 --> 01:15:14.780]   And the test that I like to give is this, right?
[01:15:14.780 --> 01:15:20.140]   It's like presumably if you're launching a new refresh of your AI, whatever it might
[01:15:20.140 --> 01:15:23.820]   be, or a new model or whatever, presumably you have a launch criteria, right?
[01:15:23.820 --> 01:15:26.540]   Like you have criteria to launch or not launch.
[01:15:26.540 --> 01:15:30.260]   And most of the time, what I observe is that it's just an all out metric that aggregates
[01:15:30.260 --> 01:15:31.740]   over all users, right?
[01:15:31.740 --> 01:15:33.900]   Well, don't do that.
[01:15:33.900 --> 01:15:40.460]   Instead, kind of desegregate and look across maybe gender, age and location, just simple
[01:15:40.460 --> 01:15:41.500]   things.
[01:15:41.500 --> 01:15:48.140]   And then ask yourself, if you find a bucket that is significant, where your AI performs
[01:15:48.140 --> 01:15:51.340]   the worst, would you still launch?
[01:15:51.340 --> 01:15:53.140]   And like show care, right?
[01:15:53.140 --> 01:15:58.980]   That would be a very sort of practical thing to do.
[01:15:58.980 --> 01:16:04.500]   I would say my reaction is that's a great suggestion for just competently launching
[01:16:04.500 --> 01:16:05.500]   AI also.
[01:16:05.500 --> 01:16:07.540]   So there's no trade off here.
[01:16:07.540 --> 01:16:11.540]   I think definitely looking at distributions of things versus averages is a really good
[01:16:11.540 --> 01:16:12.540]   idea.
[01:16:12.540 --> 01:16:19.540]   Somehow I feel like you get less sort of like Gaussian distributions in AI than you do outside
[01:16:19.540 --> 01:16:20.540]   of it.
[01:16:20.540 --> 01:16:24.660]   So I'm just going to be honest.
[01:16:24.660 --> 01:16:30.140]   I still see like all up averages everywhere I look, so I feel like we don't have like
[01:16:30.140 --> 01:16:32.980]   a discipline of doing it.
[01:16:32.980 --> 01:16:33.980]   Okay.
[01:16:33.980 --> 01:16:36.500]   Now, last question.
[01:16:36.500 --> 01:16:43.100]   I mean, maybe this actually goes back to like all the work that you've done in your career.
[01:16:43.100 --> 01:16:48.540]   But when you look at the kind of productization of machine learning, like taking an ML model
[01:16:48.540 --> 01:16:55.980]   from sort of conception to like actually in production doing something useful, where do
[01:16:55.980 --> 01:17:00.140]   you see the biggest bottlenecks or where do you see the unexpected problems that someone
[01:17:00.140 --> 01:17:06.540]   outside the space might not realize comes up in that process?
[01:17:06.540 --> 01:17:10.260]   Yeah.
[01:17:10.260 --> 01:17:15.940]   A lot of things come to mind, but I think the biggest one that comes to mind is that
[01:17:15.940 --> 01:17:25.620]   your training data is almost never representative of your life data.
[01:17:25.620 --> 01:17:26.900]   And that's just life.
[01:17:26.900 --> 01:17:33.380]   And therefore, I mean, obviously people working on self-driving cars know this very well,
[01:17:33.380 --> 01:17:39.780]   that you might sort of train all of your perception, behavior prediction and planning and whatever,
[01:17:39.780 --> 01:17:40.780]   some data.
[01:17:40.780 --> 01:17:45.740]   And then yes, you're going to have a situation on the road that you had never anticipated.
[01:17:45.740 --> 01:17:46.780]   So there's that.
[01:17:46.780 --> 01:17:50.260]   And that's what would be like sort of like the problem of like the black swans or like
[01:17:50.260 --> 01:17:52.600]   the sort of unlikely events and so on.
[01:17:52.600 --> 01:18:01.260]   But even outside of that, again, then you have like the very gross mistakes that are
[01:18:01.260 --> 01:18:03.220]   sort of super easy to fix.
[01:18:03.220 --> 01:18:09.820]   Again, like if your application of AI is human centric, did you really have in your training
[01:18:09.820 --> 01:18:13.980]   data people of all ages, genders and skin tones or not?
[01:18:13.980 --> 01:18:17.220]   And in a surprising amount of cases, you didn't.
[01:18:17.220 --> 01:18:22.020]   And then you deploy your thing and now there comes an elderly black lady and the thing
[01:18:22.020 --> 01:18:23.660]   is just not working for her.
[01:18:23.660 --> 01:18:27.940]   Or in ASR, there's many different accents of English.
[01:18:27.940 --> 01:18:30.260]   Did you have like proper representation in your data or not?
[01:18:30.260 --> 01:18:32.500]   And most of the time you didn't.
[01:18:32.500 --> 01:18:34.220]   Hard to do.
[01:18:34.220 --> 01:18:35.220]   Yeah.
[01:18:35.220 --> 01:18:36.220]   It is hard to do.
[01:18:36.220 --> 01:18:37.220]   Awesome.
[01:18:37.220 --> 01:18:38.780]   Well, thank you so much.
[01:18:38.780 --> 01:18:43.580]   It's really great to talk and thanks for taking an extra time to do two-parter here.
[01:18:43.580 --> 01:18:44.580]   My pleasure.
[01:18:44.580 --> 01:18:50.620]   And I can't wait to feel embarrassed by watching the recording.
[01:18:50.620 --> 01:18:51.620]   Awesome.
[01:18:51.620 --> 01:18:55.180]   When we first started making these videos, we didn't know if anyone would be interested
[01:18:55.180 --> 01:18:58.260]   or want to see them, but we made them for fun.
[01:18:58.260 --> 01:19:01.220]   And we started off by making videos that would teach people.
[01:19:01.220 --> 01:19:05.180]   And now we get these great interviews with real industry practitioners.
[01:19:05.180 --> 01:19:08.860]   And I love making this available to the whole world so everyone can watch these things for
[01:19:08.860 --> 01:19:09.860]   free.
[01:19:09.860 --> 01:19:12.180]   The more feedback you give us, the better stuff we can produce.
[01:19:12.180 --> 01:19:15.100]   So please subscribe, leave a comment, engage with us.
[01:19:15.100 --> 01:19:16.260]   We really appreciate it.

