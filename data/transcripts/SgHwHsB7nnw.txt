
[00:00:00.000 --> 00:00:07.840]   So it should start live streaming in about like five to 10 seconds, but I never really
[00:00:07.840 --> 00:00:12.460]   know what the timing on the stream start, you know, like you don't necessarily get to
[00:00:12.460 --> 00:00:20.080]   see like exactly when you go live until it's already happened, like just now.
[00:00:20.080 --> 00:00:21.560]   Okay.
[00:00:21.560 --> 00:00:31.280]   All right, well, so welcome everybody watching on YouTube live and a later welcome to the
[00:00:31.280 --> 00:00:35.800]   folks who are going to be tuning in a little bit later to the recorded version.
[00:00:35.800 --> 00:00:40.080]   Welcome that is to the Weights and Biases Deep Learning Salon.
[00:00:40.080 --> 00:00:45.160]   As always, I am your friendly host, Charles.
[00:00:45.160 --> 00:00:47.760]   As I said, these get recorded, they show up on our YouTube channel.
[00:00:47.760 --> 00:00:54.000]   So you can catch those later at wanb.me/youtube if you'd like.
[00:00:54.000 --> 00:00:56.400]   We got a lot of other stuff on that channel.
[00:00:56.400 --> 00:00:58.560]   We got lectures about deep learning.
[00:00:58.560 --> 00:00:59.980]   We got lectures about math.
[00:00:59.980 --> 00:01:01.600]   We got software tutorials.
[00:01:01.600 --> 00:01:04.200]   We got it all.
[00:01:04.200 --> 00:01:06.360]   We do this salon once a month.
[00:01:06.360 --> 00:01:09.360]   So the next one is coming up on May 18th.
[00:01:09.360 --> 00:01:13.200]   Again, five o'clock on a Tuesday Pacific time.
[00:01:13.200 --> 00:01:17.960]   And the next one, we're going to have the hosts of a podcast out of UCLA's student AI
[00:01:17.960 --> 00:01:19.880]   group, ACMAI.
[00:01:19.880 --> 00:01:21.440]   They run an outreach podcast.
[00:01:21.440 --> 00:01:24.280]   The hosts are Ava Osman and Matt Ruiz.
[00:01:24.280 --> 00:01:30.640]   They run this really cool podcast on inspirational figures, representation and inclusion in the
[00:01:30.640 --> 00:01:32.820]   field of AI.
[00:01:32.820 --> 00:01:36.480]   And they have some really great guests I've listened to a little bit.
[00:01:36.480 --> 00:01:41.360]   There's even more guests I want to listen to and I can't wait to talk to them and hear
[00:01:41.360 --> 00:01:47.200]   a little bit more from them about their personal perspectives and thoughts on this really important
[00:01:47.200 --> 00:01:52.440]   topic of how we can make our field, which has so much impact on so many people, more
[00:01:52.440 --> 00:01:56.720]   representative of the people on which it has its impacts.
[00:01:56.720 --> 00:02:03.600]   So that link there is that short link at the top, wanb.me/salon, will be updated to point
[00:02:03.600 --> 00:02:10.960]   to their salon after this one is over.
[00:02:10.960 --> 00:02:14.920]   Some folks maybe catch this, some people missed it, but we now have a paper reading group
[00:02:14.920 --> 00:02:16.520]   every two weeks.
[00:02:16.520 --> 00:02:20.440]   We covered one of our guest today, Polkitz papers, just yesterday.
[00:02:20.440 --> 00:02:24.400]   In two weeks, we're going to cover four papers at once.
[00:02:24.400 --> 00:02:29.600]   And we're going to cover four recent papers in computer vision, EfficientNet V2, the Revisiting
[00:02:29.600 --> 00:02:35.240]   ResNets or ResNet RS architecture, Normalization FreeNets and Vision Transformers.
[00:02:35.240 --> 00:02:39.280]   Do a little compare and contrast, do a little head to head and sort of talk about what's
[00:02:39.280 --> 00:02:45.360]   the current state of the art in computer vision in this realm.
[00:02:45.360 --> 00:02:47.800]   So that's going to be a fun one.
[00:02:47.800 --> 00:02:53.960]   That's a Zoom event, and that's wanb.me/prg to join.
[00:02:53.960 --> 00:03:00.040]   And that's going to be noon on a Sunday for folks specific to time.
[00:03:00.040 --> 00:03:03.160]   If you want to keep up with all the events that we're doing, the Waits for Biases, we've
[00:03:03.160 --> 00:03:07.120]   got a Slack community where you can participate in quick discussions.
[00:03:07.120 --> 00:03:08.120]   We also do AMAs.
[00:03:08.120 --> 00:03:09.840]   We had the CEO of Kaggle a while back.
[00:03:09.840 --> 00:03:16.880]   More recently, we had one of the head of innovation at NASA JPL talking about using ML on the
[00:03:16.880 --> 00:03:17.880]   Mars mission.
[00:03:17.880 --> 00:03:20.360]   So we have lots of interesting folks on.
[00:03:20.360 --> 00:03:25.200]   And then we also have, you know, community, FAQs, tutorials, and organizing all these
[00:03:25.200 --> 00:03:26.720]   events.
[00:03:26.720 --> 00:03:33.400]   The other place to learn about events is our new, is fully connected, our new sort of forum
[00:03:33.400 --> 00:03:37.160]   or social media outlet.
[00:03:37.160 --> 00:03:42.440]   So wanb.me/fc or wanb.ai/fc will take you there.
[00:03:42.440 --> 00:03:45.880]   There's blog posts, there's announcements, some of them about Waits for Biases, a lot
[00:03:45.880 --> 00:03:52.720]   of them about other cool things going on in the world of ML, like Accelerate at Hugging
[00:03:52.720 --> 00:03:58.040]   Face's new library for really fast training, easier distributed training and things like
[00:03:58.040 --> 00:03:59.040]   that.
[00:03:59.040 --> 00:04:04.080]   And then we've got blog posts, digests of things going on in the machine learning world.
[00:04:04.080 --> 00:04:07.720]   So you can catch all that stuff there at fully connected.
[00:04:07.720 --> 00:04:16.840]   All right, that's all my announcements or matters of logistics for today.
[00:04:16.840 --> 00:04:21.480]   So let me go ahead and welcome our speaker.
[00:04:21.480 --> 00:04:25.400]   So Polkett is actually a friend of mine from graduate school.
[00:04:25.400 --> 00:04:28.000]   We were at the University of California, Berkeley together.
[00:04:28.000 --> 00:04:33.960]   And Polkett has gone on to become a professor and continue to do research and follow up
[00:04:33.960 --> 00:04:38.240]   on some of the really exciting and interesting research threads he was tugging on in a little
[00:04:38.240 --> 00:04:42.800]   bit more, I would say some avant garde reinforcement learning ideas.
[00:04:42.800 --> 00:04:46.520]   So I'm really excited to hear what Polkett has been up to since then.
[00:04:46.520 --> 00:04:48.840]   So Polkett, go ahead and take it away.
[00:04:48.840 --> 00:04:51.680]   Thanks, Charles, for the invitation.
[00:04:51.680 --> 00:04:53.680]   It's a pleasure to be here.
[00:04:53.680 --> 00:04:57.920]   And you know, it's really great to see you after quite some time.
[00:04:57.920 --> 00:05:01.960]   So I'm going to share my screen and get started.
[00:05:01.960 --> 00:05:09.040]   You can see my screen, right?
[00:05:09.040 --> 00:05:13.000]   Yep, you're good.
[00:05:13.000 --> 00:05:16.040]   Okay, perfect.
[00:05:16.040 --> 00:05:23.440]   So I'm going to talk about self-supervised robot learning today, and our lab as known
[00:05:23.440 --> 00:05:26.480]   as improbable AI lab.
[00:05:26.480 --> 00:05:32.320]   And the reason for this is we believe AI is not impossible, but it is hard.
[00:05:32.320 --> 00:05:36.200]   And it's the kind of problem that we're super excited to be solving.
[00:05:36.200 --> 00:05:42.000]   So let's consider we have a robot and we give it the following problem.
[00:05:42.000 --> 00:05:46.920]   We have some objects on a table arranged in a particular configuration, and the robot
[00:05:46.920 --> 00:05:50.320]   observes them as an RGB image.
[00:05:50.320 --> 00:05:54.080]   And then we shift these objects to a different configuration.
[00:05:54.080 --> 00:05:56.260]   And this becomes the goal image.
[00:05:56.260 --> 00:06:00.640]   And the task is to come up with actions which can transform the objects in the configuration
[00:06:00.640 --> 00:06:04.760]   on the left to the configuration on the right.
[00:06:04.760 --> 00:06:09.920]   Similarly consider another problem where we have this rope in a particular configuration,
[00:06:09.920 --> 00:06:14.240]   and we give a goal image depicting the rope tied as a knot.
[00:06:14.240 --> 00:06:17.400]   And the question is, can we come up with this sequence of actions?
[00:06:17.400 --> 00:06:23.800]   To solve this problem, one approach is reinforcement learning, which in short, is a problem of
[00:06:23.800 --> 00:06:29.020]   finding a mapping from the states XT to the actions 80.
[00:06:29.020 --> 00:06:33.700]   And when we take the action 80, we go to the next state, XT plus one.
[00:06:33.700 --> 00:06:39.960]   If this next state XT plus one is close to the goal state, we end up getting a reward.
[00:06:39.960 --> 00:06:45.140]   And the whole game is to maximize the sum of rewards.
[00:06:45.140 --> 00:06:51.940]   One thing to note is that this policy at the very beginning is randomly initialized.
[00:06:51.940 --> 00:06:55.120]   That is, the parameters of my network are random.
[00:06:55.120 --> 00:07:00.100]   What this means is that we are essentially hoping that by performing a random set of
[00:07:00.100 --> 00:07:05.940]   actions as a consequence of these random parameters, we're going to go and get some reward.
[00:07:05.940 --> 00:07:11.700]   Because if you don't get any rewards, it means that you're not going to update a parameter
[00:07:11.700 --> 00:07:16.080]   theta, which means that the policy is going to remain random.
[00:07:16.080 --> 00:07:22.140]   So for reinforcement learning to work, it's critical that just by doing a random sample
[00:07:22.140 --> 00:07:25.260]   of actions, we should go and get some reward.
[00:07:25.260 --> 00:07:31.900]   Now imagine, in this particular scenario, how likely is it that by performing a random
[00:07:31.900 --> 00:07:36.440]   sequence of actions, I'm going to go and tie the knot?
[00:07:36.440 --> 00:07:37.440]   Highly unlikely.
[00:07:37.440 --> 00:07:43.060]   It's highly unlikely that RL is going to succeed at this task.
[00:07:43.060 --> 00:07:44.060]   But wait a minute.
[00:07:44.060 --> 00:07:50.380]   I mean, what happened in the domain of video games where RL has been very successful?
[00:07:50.380 --> 00:07:57.020]   What has happened is that the agents take lots and lots of interaction.
[00:07:57.020 --> 00:08:01.360]   They are waiting for around 21 million games in the case of Go, or millions and millions
[00:08:01.360 --> 00:08:06.580]   of interactions in Atari games, just hoping that once in a while, they're going to go
[00:08:06.580 --> 00:08:08.940]   and hit the reward.
[00:08:08.940 --> 00:08:15.820]   Furthermore, in these game setups, the reward has been provided by the human.
[00:08:15.820 --> 00:08:18.160]   But what about in the real world?
[00:08:18.160 --> 00:08:22.140]   How do we get this reward function?
[00:08:22.140 --> 00:08:27.740]   Essentially what we want is a way to distinguish between this image and this image.
[00:08:27.740 --> 00:08:33.740]   Now we cannot really use distance in the pixel space, because if we do that, it's going to
[00:08:33.740 --> 00:08:35.820]   be a bad metric.
[00:08:35.820 --> 00:08:40.580]   Even if the rope is shifted by a small amount, distance in the pixel space can become really
[00:08:40.580 --> 00:08:41.580]   big.
[00:08:41.580 --> 00:08:48.220]   So what we'll have to do to measure reward is to actually train a visual classifier.
[00:08:48.220 --> 00:08:50.060]   But how do we get a visual classifier?
[00:08:50.060 --> 00:08:54.300]   Well, we rely on the human farm.
[00:08:54.300 --> 00:08:58.760]   And we'll need to get labels for this one task that I need to perform.
[00:08:58.760 --> 00:09:01.300]   But what if I change the task to something different?
[00:09:01.300 --> 00:09:05.140]   Well, I'll have to go over and repeat this process again.
[00:09:05.140 --> 00:09:11.580]   And if I change my goal again, then I have to repeat this process all over again.
[00:09:11.580 --> 00:09:16.140]   So in a nutshell, there are several issues in applying reinforcement learning in the
[00:09:16.140 --> 00:09:17.140]   real world.
[00:09:17.140 --> 00:09:19.700]   First, we require lots of data.
[00:09:19.700 --> 00:09:24.100]   Second, it is unclear where do rewards come from.
[00:09:24.100 --> 00:09:29.060]   And the third thing is that the systems end up being very task specific.
[00:09:29.060 --> 00:09:35.460]   So the challenge becomes is how do we overcome these problems?
[00:09:35.460 --> 00:09:42.220]   And one approach that we have pursued is to use self-supervision.
[00:09:42.220 --> 00:09:45.860]   Now what do I mean by self-supervision?
[00:09:45.860 --> 00:09:51.500]   What I mean is, we have had robotic systems which interact with their own environment
[00:09:51.500 --> 00:09:53.740]   with minimum human supervision.
[00:09:53.740 --> 00:09:58.740]   For example, you're seeing this work a couple of years back where this robot is playing
[00:09:58.740 --> 00:10:02.620]   around with this object on its own for a long period of time.
[00:10:02.620 --> 00:10:08.100]   Similarly, you have a robot on the bottom right is playing with the rope and on the
[00:10:08.100 --> 00:10:14.300]   bottom left is going around in an office environment autonomously, completely on its own, collecting
[00:10:14.300 --> 00:10:19.860]   data, trying to understand how to push objects, how to manipulate ropes, or how to move around
[00:10:19.860 --> 00:10:26.100]   office environments.
[00:10:26.100 --> 00:10:28.860]   Now what are these systems doing?
[00:10:28.860 --> 00:10:32.300]   For example, the system observes an image XT.
[00:10:32.300 --> 00:10:36.180]   It conducts an experiment that it chooses an action to perform.
[00:10:36.180 --> 00:10:39.220]   And then we get to the next image.
[00:10:39.220 --> 00:10:45.380]   And with this data, we are essentially trying to learn a model of how things work.
[00:10:45.380 --> 00:10:49.700]   A useful model is to predict what is going to happen next.
[00:10:49.700 --> 00:10:56.580]   So given an image XT and my action 80, we can predict the next state.
[00:10:56.580 --> 00:10:58.580]   Why are these models useful?
[00:10:58.580 --> 00:11:04.820]   Because we can use these models to infer the actions to go from my current state to a desired
[00:11:04.820 --> 00:11:05.820]   goal state.
[00:11:05.820 --> 00:11:12.620]   I'm going to call these models as forward models in pixel space because one possibility
[00:11:12.620 --> 00:11:17.940]   is to predict the future pixel by pixel.
[00:11:17.940 --> 00:11:24.340]   Now there has been a lot of work on this topic, but it also turns out to be an extremely hard
[00:11:24.340 --> 00:11:30.580]   problem to solve because I am expecting my network to predict exactly what will happen
[00:11:30.580 --> 00:11:35.540]   in the future to the pixel level details.
[00:11:35.540 --> 00:11:37.060]   Now the problem is hard.
[00:11:37.060 --> 00:11:41.300]   It doesn't mean that we should not solve it, but it does mean we should really ask the
[00:11:41.300 --> 00:11:45.540]   question, is this the right model to be building?
[00:11:45.540 --> 00:11:50.300]   For example, consider this glass bottle and suppose I go and drop the bottle.
[00:11:50.300 --> 00:11:55.820]   Do I really care about predicting where every piece of glass of the water goes?
[00:11:55.820 --> 00:11:56.820]   Probably not.
[00:11:56.820 --> 00:12:01.020]   Probably what I care about is the fact that the bottle broke down and I should not step
[00:12:01.020 --> 00:12:03.660]   over it.
[00:12:03.660 --> 00:12:09.860]   So the one thing to notice is depending on what feature space we use, it is easier or
[00:12:09.860 --> 00:12:11.780]   hard to make the prediction.
[00:12:11.780 --> 00:12:15.940]   It's easy to predict the bottle breaks, but very hard to predict the exact location of
[00:12:15.940 --> 00:12:17.780]   glasses.
[00:12:17.780 --> 00:12:22.460]   The problem in predicting the easy thing is that bottle breaking is something we require
[00:12:22.460 --> 00:12:24.460]   supervision for.
[00:12:24.460 --> 00:12:31.780]   Versus predicting the image, we don't require supervision because we can just use the future
[00:12:31.780 --> 00:12:35.820]   as the data to make that prediction.
[00:12:35.820 --> 00:12:39.780]   So things that are easy to predict require human supervision, things which are hard to
[00:12:39.780 --> 00:12:43.420]   predict we can do, but it becomes a challenging problem.
[00:12:43.420 --> 00:12:46.900]   So what do we do in this setup?
[00:12:46.900 --> 00:12:53.220]   So let's go in more detail in this process of making predictions in the pixel space.
[00:12:53.220 --> 00:12:59.020]   So what we can do is we have an image XT, we encode into a feature space Z.
[00:12:59.020 --> 00:13:04.940]   And then given the feature space Z and the action, we try to predict the future.
[00:13:04.940 --> 00:13:13.500]   And one possibility was to reconstruct the image as we have seen until now.
[00:13:13.500 --> 00:13:17.980]   Well let's look at what happens when we do this task.
[00:13:17.980 --> 00:13:22.620]   So what I'm showing you is this task from the Mojoko suite, where this half cheetah is
[00:13:22.620 --> 00:13:25.020]   running forward.
[00:13:25.020 --> 00:13:30.220]   And what we did was we trained three different models with three different sizes, small to
[00:13:30.220 --> 00:13:35.900]   large, and what we find is their returns are pretty similar with small increase in performance
[00:13:35.900 --> 00:13:39.820]   when we go from the small model to a large model.
[00:13:39.820 --> 00:13:42.020]   But let's make things a bit more interesting.
[00:13:42.020 --> 00:13:47.420]   Where the background in which this agent is moving is more complex, maybe more representative
[00:13:47.420 --> 00:13:49.740]   of the real world.
[00:13:49.740 --> 00:13:54.100]   One thing that you notice is the performance takes a big hit.
[00:13:54.100 --> 00:13:59.180]   It's much worse, even though the only thing I changed is the background.
[00:13:59.180 --> 00:14:04.780]   The second thing to note is that as the model size is increasing, the performance is also
[00:14:04.780 --> 00:14:07.300]   increasing significantly.
[00:14:07.300 --> 00:14:13.300]   What this thing suggests is that most of the model capacity is consumed by the distractors.
[00:14:13.300 --> 00:14:17.820]   That is, in trying to predict what is going to happen in the future, the model is really
[00:14:17.820 --> 00:14:24.380]   focusing on the background instead of focusing on the relevant stuff.
[00:14:24.380 --> 00:14:30.020]   So how can we make the model care about the relevant stuff?
[00:14:30.020 --> 00:14:35.860]   So one approach which has been popularized by many works such as Dreamer and so on is
[00:14:35.860 --> 00:14:42.860]   to take the observation OT and to encode this into a state representation S plus.
[00:14:42.860 --> 00:14:49.980]   And we could learn the state representation so that I have this parameter theta plus n,
[00:14:49.980 --> 00:14:54.300]   which encode my OT into ST plus.
[00:14:54.300 --> 00:14:59.300]   And the way the constraint we put on ST plus, and then it should be able to reconstruct
[00:14:59.300 --> 00:15:01.140]   the observation OT.
[00:15:01.140 --> 00:15:08.340]   This is a typical paradigm of reconstruction to learn features.
[00:15:08.340 --> 00:15:13.300]   Now once we get this feature space S plus, we can then roll the world forward in this
[00:15:13.300 --> 00:15:21.260]   feature space and then choose the actions which maximize the rewards.
[00:15:21.260 --> 00:15:24.220]   Sometimes these rewards are available to us.
[00:15:24.220 --> 00:15:29.620]   So we can ask the question, instead of solely learning these features based on predicting
[00:15:29.620 --> 00:15:35.940]   the future observation, can we use the reward function to learn task relevant features?
[00:15:35.940 --> 00:15:42.020]   Could we not just solve the hard problem of pixel prediction, but maybe bias our prediction
[00:15:42.020 --> 00:15:46.260]   to features which are relevant to the reward function?
[00:15:46.260 --> 00:15:52.500]   One thing we could do is we can predict the reward function from the state ST plus.
[00:15:52.500 --> 00:15:58.740]   And this leads to a modified objective function where we are also predicting the reward function.
[00:15:58.740 --> 00:16:05.060]   So now I am doing reconstruction plus we are doing the reward function prediction.
[00:16:05.060 --> 00:16:06.900]   What's the catch over there?
[00:16:06.900 --> 00:16:12.020]   The catch is that reward prediction requires very little information.
[00:16:12.020 --> 00:16:13.340]   What do I mean by this?
[00:16:13.340 --> 00:16:18.500]   So consider that I have this agent which I wanted to walk.
[00:16:18.500 --> 00:16:24.300]   And if I wanted to walk say on the right, it is sufficient to represent the velocity
[00:16:24.300 --> 00:16:28.860]   of its center of mass to get the reward.
[00:16:28.860 --> 00:16:30.740]   But how to predict the reward.
[00:16:30.740 --> 00:16:35.560]   But in order to actually make this agent walk, we need much more information.
[00:16:35.560 --> 00:16:41.820]   We need to know all its joint locations to be able to predict what action to take.
[00:16:41.820 --> 00:16:47.380]   So on one hand, there is too much information that we need to predict.
[00:16:47.380 --> 00:16:51.820]   And if you're trying to reconstruct the image, what is this too little information in the
[00:16:51.820 --> 00:16:52.820]   rewards.
[00:16:52.820 --> 00:17:00.180]   So typically, a lot of work focuses on tuning this parameter lambda so we can capture the
[00:17:00.180 --> 00:17:03.260]   right amount of the relevant information.
[00:17:03.260 --> 00:17:11.660]   So one question we asked is, can we make this task easier of tuning this parameter lambda?
[00:17:11.660 --> 00:17:14.980]   This is a work which is shortly going to be out on archive.
[00:17:14.980 --> 00:17:19.620]   It's called Learning Task-Informed Abstractions.
[00:17:19.620 --> 00:17:25.900]   So what we do is we say, well, let's have one encoding S+, which we already talked about.
[00:17:25.900 --> 00:17:32.500]   And this S+ is incentivized to capture all the information about the problem which is
[00:17:32.500 --> 00:17:36.220]   relevant to predicting the reward function.
[00:17:36.220 --> 00:17:44.540]   But let's also encode all the remainder information in the encoding S-.
[00:17:44.540 --> 00:17:52.140]   And S- is chosen in a way that we cannot predict the rewards from S-.
[00:17:52.140 --> 00:17:58.820]   And both S+ and S- cooperate together to predict the observation.
[00:17:58.820 --> 00:18:08.540]   So this way of having S+ and S- as two separate latent spaces try to disambiguate information
[00:18:08.540 --> 00:18:14.420]   which is relevant for the task and information that is irrelevant for the task.
[00:18:14.420 --> 00:18:20.300]   And then we can just do a rollout of the feature in the task relevant feature space.
[00:18:20.300 --> 00:18:23.980]   So we are explicitly separating relevant and irrelevant features.
[00:18:23.980 --> 00:18:26.980]   So what happens when we perform the separation?
[00:18:26.980 --> 00:18:32.260]   Well, if you look at the predictions that we are getting in the future, we are mostly
[00:18:32.260 --> 00:18:38.900]   capturing the relevant agent and ignoring the background, versus that S- ends up capturing
[00:18:38.900 --> 00:18:42.140]   the background.
[00:18:42.140 --> 00:18:49.380]   So with this formulation, we are indeed able to make much better predictions in the future
[00:18:49.380 --> 00:18:52.500]   because we are capturing mostly the relevant stuff.
[00:18:52.500 --> 00:18:57.900]   And this also leads to superior performance, not just on MuJoCo tasks, but also on a bunch
[00:18:57.900 --> 00:19:02.300]   of Atari games that we evaluated on.
[00:19:02.300 --> 00:19:09.380]   So this was one piece on how, even when we have access to a reward function, we can utilize
[00:19:09.380 --> 00:19:15.420]   it to better learn task relevant information.
[00:19:15.420 --> 00:19:20.860]   But many times, as we talked about, just getting this reward function itself is a hard problem
[00:19:20.860 --> 00:19:22.460]   in the real world.
[00:19:22.460 --> 00:19:27.100]   So what do we do in those scenarios?
[00:19:27.100 --> 00:19:31.700]   So remember, what I talked about was building these forward models.
[00:19:31.700 --> 00:19:36.780]   And the reason we were building these forward models is so that we can, at the end, infer
[00:19:36.780 --> 00:19:38.620]   the actions A, D.
[00:19:38.620 --> 00:19:40.580]   And how do we infer this action A, D?
[00:19:40.580 --> 00:19:46.940]   By posing an optimization problem that I want my last state to be close to my goal state.
[00:19:46.940 --> 00:19:55.460]   The thing to note is, what we really care about are these actions A, D. And forward
[00:19:55.460 --> 00:19:59.900]   models are simply a tool to infer these actions.
[00:19:59.900 --> 00:20:04.980]   So one question we can ask is, do I really need to build this forward model?
[00:20:04.980 --> 00:20:07.740]   Or is there an alternative?
[00:20:07.740 --> 00:20:13.180]   Simply building the forward model is trying to solve a very hard and a more general problem
[00:20:13.180 --> 00:20:15.900]   than just inferring the actions.
[00:20:15.900 --> 00:20:23.460]   So as Vladimir Vapnik says, when solving a problem of interest, do not solve a more general
[00:20:23.460 --> 00:20:26.180]   problem as an intermediate step.
[00:20:26.180 --> 00:20:33.220]   That is, how about instead of trying to first build a forward model and then infer the actions,
[00:20:33.220 --> 00:20:38.660]   what about we directly try to predict the actions from my current state and my goal
[00:20:38.660 --> 00:20:39.660]   state?
[00:20:39.660 --> 00:20:43.780]   And this is what we're going to call as the inverse model.
[00:20:43.780 --> 00:20:47.220]   The inverse model has two benefits.
[00:20:47.220 --> 00:20:51.780]   So one benefit is we are directly predicting the actions that we care about.
[00:20:51.780 --> 00:20:58.580]   And the second is, by making this action prediction task, we are able to learn a feature space.
[00:20:58.580 --> 00:21:03.860]   And it turns out that the feature space that we learn by action prediction is quite useful.
[00:21:03.860 --> 00:21:09.260]   In the work done around six years ago now, we showed that if we took data from these
[00:21:09.260 --> 00:21:14.180]   cars moving on roads, and we tried to predict the relative motion between the two images,
[00:21:14.180 --> 00:21:18.500]   which is the self motion of the car, we could learn a feature space that was useful for
[00:21:18.500 --> 00:21:22.860]   image recognition, post-matching, and so on.
[00:21:22.860 --> 00:21:27.900]   So in a nutshell, by doing this prediction task, we're able to learn a useful feature
[00:21:27.900 --> 00:21:29.100]   space.
[00:21:29.100 --> 00:21:34.540]   And now we can learn a forward model in this feature space Z instead of trying to reconstruct
[00:21:34.540 --> 00:21:35.540]   the image.
[00:21:35.540 --> 00:21:40.700]   So inverse model is serving two purposes-- learning a good feature space, but also directly
[00:21:40.700 --> 00:21:43.140]   predicting the action.
[00:21:43.140 --> 00:21:47.660]   So now instead of learning just the forward model, what we're going to do is to learn
[00:21:47.660 --> 00:21:50.940]   both the forward model and the inverse model.
[00:21:50.940 --> 00:21:55.540]   And the inverse model provides an alternative to building the forward model in the feature
[00:21:55.540 --> 00:21:59.860]   space-- in the pixel space.
[00:21:59.860 --> 00:22:04.180]   So now we can take these models and apply it to the robotics systems that we discussed
[00:22:04.180 --> 00:22:05.180]   about before.
[00:22:05.180 --> 00:22:06.180]   The robot collects the data.
[00:22:06.180 --> 00:22:09.260]   We learn these forward inverse models.
[00:22:09.260 --> 00:22:13.220]   Another question is, how well do these systems work?
[00:22:13.220 --> 00:22:16.140]   So for example, I can test these systems.
[00:22:16.140 --> 00:22:19.620]   Suppose I'm observing the world as is, which is my current state.
[00:22:19.620 --> 00:22:22.340]   And now we provide an image of the goal state.
[00:22:22.340 --> 00:22:26.180]   And we want the system to go from the current state to the goal state.
[00:22:26.180 --> 00:22:35.540]   Let's see how well does this pushing model work.
[00:22:35.540 --> 00:22:41.660]   So what you see is this agent is able to displace these new objects it has never seen before
[00:22:41.660 --> 00:22:42.660]   to their goal locations.
[00:22:42.660 --> 00:22:48.340]   So in some ways, this is exciting because we can push new objects.
[00:22:48.340 --> 00:22:51.220]   On the other hand, you might say, well, these objects are rigid objects.
[00:22:51.220 --> 00:22:53.220]   We are trying to push them.
[00:22:53.220 --> 00:22:58.580]   Maybe it's not such a hard problem to learn by self-supervised interaction.
[00:22:58.580 --> 00:23:00.780]   But what about a slightly different problem?
[00:23:00.780 --> 00:23:07.660]   Over here, we have a goal image, which is given as a knot that I want to tie.
[00:23:07.660 --> 00:23:10.260]   And this is my initial image.
[00:23:10.260 --> 00:23:14.500]   This is the same problem we discussed about initially at the beginning of the talk.
[00:23:14.500 --> 00:23:17.620]   So can our method also solve this problem?
[00:23:17.620 --> 00:23:22.900]   To see this, we had this robotic system, which plays around with the rope.
[00:23:22.900 --> 00:23:26.500]   It picks up a rope, keeps it, picks up the rope, keeps it, and keeps on doing it for
[00:23:26.500 --> 00:23:29.220]   a long time.
[00:23:29.220 --> 00:23:33.740]   And now let's test the system with tying a knot.
[00:23:33.740 --> 00:23:39.740]   Let's see what happens.
[00:23:39.740 --> 00:23:42.860]   What we see is the system goes and ties a knot.
[00:23:42.860 --> 00:23:48.980]   Now, as we discussed before, the chance of randomly tying a knot is really small.
[00:23:48.980 --> 00:23:51.980]   It's around almost 0%.
[00:23:51.980 --> 00:23:57.180]   On this task, our system ends up getting a performance of around 18%, which is much better
[00:23:57.180 --> 00:23:59.060]   than random chance.
[00:23:59.060 --> 00:24:07.460]   So what these two examples suggest is some kind of extrapolative generalization.
[00:24:07.460 --> 00:24:13.460]   That is, by building joint forward and inverse models, we have learned useful models which
[00:24:13.460 --> 00:24:18.060]   are able to perform 10 tasks that the user has provided.
[00:24:18.060 --> 00:24:23.660]   So in some ways, by collecting our own data, we're able to learn a model of how things
[00:24:23.660 --> 00:24:24.660]   work.
[00:24:24.660 --> 00:24:28.700]   But you might come and say, well, this performance on this task is only 18%.
[00:24:28.700 --> 00:24:32.860]   While it is much better than random chance, it's not great.
[00:24:32.860 --> 00:24:37.660]   So how can I improve performance even more?
[00:24:37.660 --> 00:24:43.460]   So what I'm saying is that to solve a lot of these complex tasks, we are actually observing
[00:24:43.460 --> 00:24:46.100]   what a human might do and then learn from it.
[00:24:46.100 --> 00:24:49.380]   For example, how to tie this knot.
[00:24:49.380 --> 00:24:53.260]   Learning from demonstrations is a popular topic in robotics.
[00:24:53.260 --> 00:24:59.100]   For example, over here, we are demonstrating how to build a certain tool.
[00:24:59.100 --> 00:25:03.100]   And this human is actually moving the robot one by one.
[00:25:03.100 --> 00:25:09.820]   What is essentially happening is we have access to these observations, xt, and the action
[00:25:09.820 --> 00:25:10.820]   of the robot.
[00:25:10.820 --> 00:25:16.020]   And we know the action because we know how much was the robot moving.
[00:25:16.020 --> 00:25:19.940]   Now this is a typical paradigm for learning from demonstrations.
[00:25:19.940 --> 00:25:26.340]   We have images, xt, actions, at, and then we learn a mapping which goes from my observations
[00:25:26.340 --> 00:25:27.340]   to my actions.
[00:25:27.340 --> 00:25:34.340]   Now while this works, one thing to note is that this is very, very specific to one particular
[00:25:34.340 --> 00:25:35.340]   task.
[00:25:35.340 --> 00:25:42.100]   Because I get actions to only perform this one task and giving these demonstrations is
[00:25:42.100 --> 00:25:43.100]   very tedious.
[00:25:43.100 --> 00:25:47.340]   Could we do something which is more general?
[00:25:47.340 --> 00:25:53.460]   Something where we can just observe what the human is doing and then copy it.
[00:25:53.460 --> 00:25:59.300]   That is, instead of assuming that we have access to actions of the expert demonstrator,
[00:25:59.300 --> 00:26:04.100]   we only have access to the visual observations.
[00:26:04.100 --> 00:26:07.020]   This is something that we can indeed do.
[00:26:07.020 --> 00:26:13.900]   And if we incorporate these human demonstrations which were given by just a sequence of images,
[00:26:13.900 --> 00:26:16.820]   we can improve our success rate to around 65%.
[00:26:16.820 --> 00:26:21.740]   Now if you want to learn more about, you can check out our paper.
[00:26:21.740 --> 00:26:28.260]   In the interest of time, I can't tell you spend more effort discussing this, but happy
[00:26:28.260 --> 00:26:34.020]   to chat about this more in the Q&A session later on.
[00:26:34.020 --> 00:26:40.500]   Then we can take the same model and also apply it to another problem that I briefly mentioned
[00:26:40.500 --> 00:26:42.860]   about before, which is indoor navigation.
[00:26:42.860 --> 00:26:48.260]   So we're here, the system is moving around, collecting its own data.
[00:26:48.260 --> 00:26:49.820]   And now we give it some tasks.
[00:26:49.820 --> 00:26:53.420]   For example, this robot is now in a new environment.
[00:26:53.420 --> 00:26:55.380]   It has no map of the environment.
[00:26:55.380 --> 00:26:59.140]   It just has an RGB camera, no depth sensors.
[00:26:59.140 --> 00:27:01.300]   And it is placed in this new setup.
[00:27:01.300 --> 00:27:05.980]   So it sees this current image and we want to go to the goal image.
[00:27:05.980 --> 00:27:11.780]   Now if I have no information, what am I going to do in this scenario as a human?
[00:27:11.780 --> 00:27:17.540]   I might look around, try to search for where this couch is, and then go towards it.
[00:27:17.540 --> 00:27:19.460]   Let's see what our system ends up doing.
[00:27:19.460 --> 00:27:24.260]   When the system looks around, it tries to search for where the couch is, and then it
[00:27:24.260 --> 00:27:27.020]   goes over there.
[00:27:27.020 --> 00:27:32.380]   But there are many algorithms today, such as SLAM, which can solve this task.
[00:27:32.380 --> 00:27:36.020]   But the cool thing over here is that this behavior is emergent.
[00:27:36.020 --> 00:27:40.780]   We did not have to hard code the fact the robot has to go around, look for the goal
[00:27:40.780 --> 00:27:42.700]   image, and then go over there.
[00:27:42.700 --> 00:27:49.340]   This is an emerging phenomenon of learning the forward and inverse models.
[00:27:49.340 --> 00:27:54.900]   So until now, what we have seen is that by collecting data, we're able to learn models,
[00:27:54.900 --> 00:27:56.820]   but sometimes with the help of humans.
[00:27:56.820 --> 00:28:02.220]   For example, in cases of rope manipulation, we saw a human providing a demonstration.
[00:28:02.220 --> 00:28:07.780]   We can perform goal-directed actions.
[00:28:07.780 --> 00:28:09.420]   So okay, well and great.
[00:28:09.420 --> 00:28:16.180]   Well, we are able to solve some tasks, but these tasks are not really long horizon tasks.
[00:28:16.180 --> 00:28:22.380]   So how can we make long-term plans with lesser and lesser reliance on humans?
[00:28:22.380 --> 00:28:25.260]   This has been the focus of some of our more recent work.
[00:28:25.260 --> 00:28:29.420]   So for example, consider a system where this robot has some primitive skills.
[00:28:29.420 --> 00:28:36.140]   For example, it can pull the object on the right, learned, for example, by self-supervision,
[00:28:36.140 --> 00:28:40.780]   as I discussed before, or we could also have it hard-coded.
[00:28:40.780 --> 00:28:42.580]   Either one of them is fine.
[00:28:42.580 --> 00:28:48.660]   Or another skill of trying to reorient the object, and another skill of trying to pull
[00:28:48.660 --> 00:28:51.420]   the object on the left.
[00:28:51.420 --> 00:28:56.140]   And now what we want to solve is a task which requires a sequence of these skills to go
[00:28:56.140 --> 00:29:00.200]   from a stack position to a goal position.
[00:29:00.200 --> 00:29:06.740]   So my task description is to move this object by some SE3 transformation.
[00:29:06.740 --> 00:29:13.020]   And this we can do because if the object is rigid, all transformations are SE3 transformations.
[00:29:13.020 --> 00:29:14.020]   What are the challenges?
[00:29:14.020 --> 00:29:21.340]   First challenge is this problem is long horizon, that I cannot do it in one sequence of actions.
[00:29:21.340 --> 00:29:25.740]   We're not going to assume we have access to 3D models, and we would want to generalize
[00:29:25.740 --> 00:29:30.380]   to new objects, goals, and environments.
[00:29:30.380 --> 00:29:36.180]   So if we look at a lot of machine learning work, so these machine learning work has been
[00:29:36.180 --> 00:29:41.860]   very good at learning generalizable, low-level primitives, but has been bad at high-level
[00:29:41.860 --> 00:29:42.860]   planning.
[00:29:42.860 --> 00:29:49.020]   On the other hand, a lot of task and motion planning work has been good at high-level
[00:29:49.020 --> 00:29:56.540]   planning, but requires access to privileged information, such as knowing the 3D models.
[00:29:56.540 --> 00:30:01.780]   And therefore, it is unable to generalize to new problems.
[00:30:01.780 --> 00:30:05.900]   So what do we do in this scenario?
[00:30:05.900 --> 00:30:09.700]   So let's first understand why is this problem multistack?
[00:30:09.700 --> 00:30:16.100]   So for example, to manipulate this cuboid to this new position, I first need to reorient
[00:30:16.100 --> 00:30:17.100]   it.
[00:30:17.100 --> 00:30:22.020]   And this reorientation cannot happen on the side, because both the hands cannot reach
[00:30:22.020 --> 00:30:23.020]   there.
[00:30:23.020 --> 00:30:26.780]   So first, I need to push the object in the workspace of my hands.
[00:30:26.780 --> 00:30:33.860]   Then I can tilt it, rotate it, and then move it to the new sub-goal.
[00:30:33.860 --> 00:30:39.500]   So it's necessary to predict a sequence of sub-goals and how to achieve them.
[00:30:39.500 --> 00:30:41.420]   Both of them are required.
[00:30:41.420 --> 00:30:46.340]   And this was work led by graduate student, Anthony.
[00:30:46.340 --> 00:30:49.140]   So now, suppose I have my stack and my goal.
[00:30:49.140 --> 00:30:54.940]   What I want is a sequence of sub-goals to go from x0 to xz.
[00:30:54.940 --> 00:30:59.540]   So we need a high-level planner, which can predict a sequence of sub-goals, and then
[00:30:59.540 --> 00:31:04.100]   a low-level skill, which can go between these sub-goals.
[00:31:04.100 --> 00:31:10.980]   So this paradigm of high-level and low-level planning has been a very common theme, both
[00:31:10.980 --> 00:31:14.180]   in robotics and in reinforcement learning.
[00:31:14.180 --> 00:31:16.820]   But there are several challenges.
[00:31:16.820 --> 00:31:24.220]   So one issue is, suppose that even if the high-level controller is predicting good sub-goals,
[00:31:24.220 --> 00:31:31.780]   but if the low-level is unable to achieve them, then learning the high-level controller
[00:31:31.780 --> 00:31:32.780]   becomes hard.
[00:31:32.780 --> 00:31:36.780]   That is because it is going to get a big variance.
[00:31:36.780 --> 00:31:39.820]   Sometimes I predict the right thing, I cannot do it.
[00:31:39.820 --> 00:31:42.100]   Sometimes I predict the wrong thing, and I cannot do it.
[00:31:42.100 --> 00:31:45.500]   And I don't know why I cannot do it.
[00:31:45.500 --> 00:31:50.260]   But this makes end-to-end learning of the high-level and the low-level quite hard.
[00:31:50.260 --> 00:31:55.700]   So to mitigate this issue, it is sometimes common to learn this in two stages.
[00:31:55.700 --> 00:32:00.500]   For example, first learn the low-level, and then learn the high-level.
[00:32:00.500 --> 00:32:02.100]   But this leads to another issue.
[00:32:02.100 --> 00:32:08.780]   So for example, assume that the low-level can achieve feasible sub-goals.
[00:32:08.780 --> 00:32:13.380]   But the high-level can end up predicting a lot of infeasible sub-goals.
[00:32:13.380 --> 00:32:18.460]   So what will this do is that I'll need to keep on searching for a long time until I
[00:32:18.460 --> 00:32:21.020]   find a feasible sub-goal.
[00:32:21.020 --> 00:32:23.460]   This makes planning inefficient.
[00:32:23.460 --> 00:32:31.100]   So the main insight we had in this book was that we formulated a method which exploits
[00:32:31.100 --> 00:32:37.980]   the fact that we do rigid body manipulation to only predict feasible sub-goals.
[00:32:37.980 --> 00:32:44.340]   And this we can do for a significant subset of rigid body manipulation.
[00:32:44.340 --> 00:32:49.900]   And this makes the high-level planning be much more efficient and plausible on a real
[00:32:49.900 --> 00:32:50.900]   system.
[00:32:50.900 --> 00:32:53.940]   Now, how exactly we do it, the details are in the paper.
[00:32:53.940 --> 00:32:56.580]   I'm going to show you a teaser video.
[00:32:56.580 --> 00:33:01.860]   It shows how the system, for example, can take this block, reorient it, and then place
[00:33:01.860 --> 00:33:08.820]   it on top of a shelf over here, which involves multiple steps of execution.
[00:33:08.820 --> 00:33:11.740]   We can go and quantify the results.
[00:33:11.740 --> 00:33:18.540]   And our learned systems end up performing much better than hand-designed systems.
[00:33:18.540 --> 00:33:21.340]   And this doesn't really just work in simulation.
[00:33:21.340 --> 00:33:28.180]   We can go on the real system and also try with these many different object types, which
[00:33:28.180 --> 00:33:32.780]   are not just cuboids, but for example, cylinders, and so on and so forth.
[00:33:32.780 --> 00:33:38.460]   And you can check out the website for more videos.
[00:33:38.460 --> 00:33:43.820]   So what this part is showing is that we can complete this long-horizon task, but with
[00:33:43.820 --> 00:33:48.740]   limited human supervision.
[00:33:48.740 --> 00:33:54.060]   Now I'm going to briefly talk about one more work, and then I'm going to stop and take
[00:33:54.060 --> 00:33:58.900]   any questions and engage with Charles in Q&A.
[00:33:58.900 --> 00:34:03.900]   So let's discuss one more strategy for doing this long-term planning.
[00:34:03.900 --> 00:34:09.500]   And we're going to use block stacking as a demo task to illustrate this.
[00:34:09.500 --> 00:34:14.100]   So let's consider this problem where I have these blocks, and then we want to reconfigure
[00:34:14.100 --> 00:34:18.860]   these blocks in a certain configuration shown by these three dots.
[00:34:18.860 --> 00:34:22.940]   So over here, as opposed to the rest of the talk, we are not going to be working with
[00:34:22.940 --> 00:34:25.660]   images, but a low-dimensional state space.
[00:34:25.660 --> 00:34:31.740]   So if we just use reinforcement learning on this task, the systems fail.
[00:34:31.740 --> 00:34:37.500]   I mean, they're unable to achieve the task, even if you run them for a long time.
[00:34:37.500 --> 00:34:44.140]   There was work from OpenAI a couple of years back where they were able to stack these blocks,
[00:34:44.140 --> 00:34:51.980]   for example, stack up to four or five blocks, but they were leveraging human demonstrations.
[00:34:51.980 --> 00:35:00.300]   So this problem of stacking four or five blocks was able to solve with 91% accuracy with a
[00:35:00.300 --> 00:35:03.220]   huge number of interactions.
[00:35:03.220 --> 00:35:10.860]   We showed that we can stack a lot more blocks without demonstrations with much fewer data.
[00:35:10.860 --> 00:35:13.740]   And this was work led by Richard Li.
[00:35:13.740 --> 00:35:16.460]   So how do we do this?
[00:35:16.460 --> 00:35:23.300]   So instead of using demonstrations, what we used was the insight that maybe I can provide
[00:35:23.300 --> 00:35:24.300]   a curriculum of tasks.
[00:35:24.300 --> 00:35:30.860]   First, the robot can solve a simpler task, and then go and solve a harder task.
[00:35:30.860 --> 00:35:38.300]   So it turns out that if I use this curriculum of stacking blocks, then if I measure performance
[00:35:38.300 --> 00:35:43.220]   on the y-axis, which is measuring how many blocks we stack, and on the x-axis, the number
[00:35:43.220 --> 00:35:50.460]   of interaction steps, what we find is the performance plateaus at one block very easily.
[00:35:50.460 --> 00:35:53.220]   Why is it that this happens?
[00:35:53.220 --> 00:35:54.940]   The intuition is as following.
[00:35:54.940 --> 00:35:56.420]   Why is this curriculum failing?
[00:35:56.420 --> 00:36:02.740]   For example, if I consider the problem, when we only have one block, we can learn a policy
[00:36:02.740 --> 00:36:06.460]   which goes from this one block and predicts the action.
[00:36:06.460 --> 00:36:09.900]   But now suppose we get two blocks.
[00:36:09.900 --> 00:36:14.940]   Now this policy which I learned on one block may not generalize the situation with two
[00:36:14.940 --> 00:36:15.940]   blocks.
[00:36:15.940 --> 00:36:19.500]   This is because my environment has changed.
[00:36:19.500 --> 00:36:21.700]   My environment now has more blocks.
[00:36:21.700 --> 00:36:28.540]   So if I don't have the right inductive biases, for example, if I just had a fully connected
[00:36:28.540 --> 00:36:33.420]   network, it may not be able to deal very well with the increase in number of blocks.
[00:36:33.420 --> 00:36:38.620]   And so the policy that I learned in one step of the curriculum may not actually generalize
[00:36:38.620 --> 00:36:42.980]   to the next step of the curriculum, which means the curriculum is going to fail and
[00:36:42.980 --> 00:36:46.020]   treat each problem independently.
[00:36:46.020 --> 00:36:49.660]   So in that case, we wouldn't really get a benefit of transfer.
[00:36:49.660 --> 00:36:57.420]   So to overcome this problem, it is critical to use a good representation of the environment.
[00:36:57.420 --> 00:37:02.420]   And in this work, we ended up using a graph neural network.
[00:37:02.420 --> 00:37:07.220]   And it turns out that if you use both the graph neural network and the curriculum, then
[00:37:07.220 --> 00:37:13.420]   the system ends up learning and it's able to learn how to stack a larger number of blocks.
[00:37:13.420 --> 00:37:16.780]   You can go up to six or seven and even more.
[00:37:16.780 --> 00:37:24.420]   And as you can see from the curve, that RENN sequential, which is both graph networks plus
[00:37:24.420 --> 00:37:26.140]   the curriculum is important.
[00:37:26.140 --> 00:37:31.060]   If you remove either one of them, the system fails.
[00:37:31.060 --> 00:37:36.260]   And now the cool thing is because we have this good inductive bias in our architecture,
[00:37:36.260 --> 00:37:39.980]   we can use the system to perform some tasks zero short.
[00:37:39.980 --> 00:37:47.740]   For example, stacking many more blocks, going to unseen goals, so on and so forth.
[00:37:47.740 --> 00:37:49.900]   And then you can check out the videos.
[00:37:49.900 --> 00:37:54.900]   And there are some of these examples of this emergent behaviors.
[00:37:54.900 --> 00:37:59.340]   For example, over here, this agent doesn't know they should remove the block, but it
[00:37:59.340 --> 00:38:03.700]   learned that it should remove the block first and then go pick it up and then place it on
[00:38:03.700 --> 00:38:06.100]   the tower.
[00:38:06.100 --> 00:38:08.500]   So what's the takeaway?
[00:38:08.500 --> 00:38:14.740]   The takeaway is one of the issues with RENN was where do rewards come from.
[00:38:14.740 --> 00:38:20.860]   So we discussed initially about how we can remove the assumption of rewards by building
[00:38:20.860 --> 00:38:22.620]   these forward inverse models.
[00:38:22.620 --> 00:38:25.660]   But sometimes the rewards are useful.
[00:38:25.660 --> 00:38:31.300]   You could construct a task curriculum, but we really need good representations to make
[00:38:31.300 --> 00:38:34.060]   use of the task curriculum.
[00:38:34.060 --> 00:38:40.780]   So in summary, we are building these systems which learn to collect their own data.
[00:38:40.780 --> 00:38:47.580]   Once they get their own data, we use them to build models of how things work, which
[00:38:47.580 --> 00:38:53.220]   can perform new problems, learn from few examples, require minimal supervision.
[00:38:53.220 --> 00:38:57.260]   Then these models of how things work can influence what experiments to run.
[00:38:57.260 --> 00:39:03.580]   But this is some work that I have not discussed, but we have done work on how to self-supervise,
[00:39:03.580 --> 00:39:09.340]   so the agent can collect its own data in more and more interesting ways.
[00:39:09.340 --> 00:39:16.860]   And then I presented some ways of how to use these models to do long-term planning.
[00:39:16.860 --> 00:39:24.340]   And overall, this framework of trying this never-ending loop of collecting data, improving
[00:39:24.340 --> 00:39:31.040]   upon the model, collecting data, improving upon my model, and increasing the knowledge
[00:39:31.040 --> 00:39:36.100]   about the world, sometimes on my own, but sometimes because there's so much to learn
[00:39:36.100 --> 00:39:40.020]   about the world, we have to rely on human supervision.
[00:39:40.020 --> 00:39:44.740]   And this whole framework is what we call less computational, sensory model.
[00:39:44.740 --> 00:39:50.380]   With that, I'm going to stop and take any questions which might be there.
[00:39:50.380 --> 00:39:51.380]   Great.
[00:39:51.380 --> 00:39:58.540]   Yeah, thanks a lot, Pulkit, for sharing that nice tour of the work that you've been doing.
[00:39:58.540 --> 00:40:04.220]   I really liked that vision that you presented at the end, that no one of these approaches
[00:40:04.220 --> 00:40:08.460]   by itself really has any hope of solving this really tough problem.
[00:40:08.460 --> 00:40:14.780]   But maybe if we combine lots of these different sub-pieces into a system that can work continually,
[00:40:14.780 --> 00:40:18.900]   we'll get something that is able to solve these really hard problems.
[00:40:18.900 --> 00:40:25.540]   Is that a fair restatement of your goal or your vision for this work?
[00:40:25.540 --> 00:40:33.060]   So, yes, I think we want to improve each technique by itself, but we also feel that these techniques
[00:40:33.060 --> 00:40:34.980]   are complementary to each other.
[00:40:34.980 --> 00:40:39.140]   So the goal, I think, should be on solving the problem and not so much on the technique
[00:40:39.140 --> 00:40:40.140]   itself.
[00:40:40.140 --> 00:40:41.140]   Interesting.
[00:40:41.140 --> 00:40:48.180]   I also, like, I definitely want to come back to that sort of like those higher-level questions
[00:40:48.180 --> 00:40:52.820]   about solving the problems we want to solve with reinforcement learning.
[00:40:52.820 --> 00:40:57.340]   But I also wanted to drill on just a couple of specific questions about that last paper
[00:40:57.340 --> 00:41:00.980]   that you presented, since that's the one we talked about in our reading group.
[00:41:00.980 --> 00:41:04.960]   So you mentioned that, like, you know, the graph neural network brings a good inductive
[00:41:04.960 --> 00:41:06.620]   bias to the problem.
[00:41:06.620 --> 00:41:08.860]   And I was wondering if you'd talk a little bit more about that.
[00:41:08.860 --> 00:41:11.500]   Like after reading the paper and thinking a little bit about graph neural networks,
[00:41:11.500 --> 00:41:14.980]   I think I know what kind of inductive bias you're thinking of.
[00:41:14.980 --> 00:41:18.260]   But could you just say a little bit about what you think it brings to that task?
[00:41:18.260 --> 00:41:19.260]   Yeah.
[00:41:19.260 --> 00:41:26.420]   So for example, like suppose I only have one block, and then we go to two or three blocks.
[00:41:26.420 --> 00:41:32.020]   The question is, how do I represent two or three blocks in a way that is analogous to
[00:41:32.020 --> 00:41:34.140]   representing a single block?
[00:41:34.140 --> 00:41:38.140]   The simplest thing which comes to mind is, well, how about we do average pooling?
[00:41:38.140 --> 00:41:41.740]   We compute some features of one block, we compute some features of the second block,
[00:41:41.740 --> 00:41:43.780]   then we can average pool them.
[00:41:43.780 --> 00:41:47.460]   Or maybe you can use your favorite recurrent network.
[00:41:47.460 --> 00:41:51.540]   But the problem is, you know, average pooling is a very lossy operation.
[00:41:51.540 --> 00:41:54.620]   We are sacrificing too much information.
[00:41:54.620 --> 00:41:59.860]   What a graph neural network is helping us do is given a collection of blocks, you know,
[00:41:59.860 --> 00:42:06.540]   we can attend to a few blocks, and then learn a representation.
[00:42:06.540 --> 00:42:14.940]   So in a way, it's a more nonlinear way of combining elements of a set to form a representation
[00:42:14.940 --> 00:42:21.100]   of a set as compared to simpler mechanisms and average pooling, max pooling, and so on.
[00:42:21.100 --> 00:42:25.100]   And therefore, when you get a new block, you know, the block is maybe too far away, we
[00:42:25.100 --> 00:42:28.180]   will know that, okay, we don't need to attend to it, because it's not relevant.
[00:42:28.180 --> 00:42:32.620]   But if it is close by, we need to attend to it and account for it.
[00:42:32.620 --> 00:42:39.020]   So that is the kind of inductive bias, you know, that we are talking about over here.
[00:42:39.020 --> 00:42:40.020]   Got it.
[00:42:40.020 --> 00:42:41.020]   Got it.
[00:42:41.020 --> 00:42:44.140]   So you think that the attention is one of the more important parts of that, maybe relative
[00:42:44.140 --> 00:42:47.420]   to the graph structure by itself?
[00:42:47.420 --> 00:42:51.100]   Because graph neural networks really do like often heavily rely on things like average
[00:42:51.100 --> 00:42:57.100]   pooling and max pooling that are highly lossy to like accumulate information across nodes,
[00:42:57.100 --> 00:42:58.100]   right?
[00:42:58.100 --> 00:43:00.180]   Yeah, so it is a mixture.
[00:43:00.180 --> 00:43:01.860]   It is the attention, right?
[00:43:01.860 --> 00:43:07.340]   But this attention, you know, you are doing your average pooling or max pooling in an
[00:43:07.340 --> 00:43:11.860]   abstract feature space after doing self-attention, right?
[00:43:11.860 --> 00:43:14.820]   Instead of doing it directly on the raw features.
[00:43:14.820 --> 00:43:20.780]   So I think maybe sometimes I think it becomes confusing when people use the term graph networks
[00:43:20.780 --> 00:43:24.340]   and probably it's happening when I'm using the term graph networks also.
[00:43:24.340 --> 00:43:29.460]   So one assumption is that I am given a graph, and then I want to do a computation on the
[00:43:29.460 --> 00:43:32.420]   graph and that is a graph network, right?
[00:43:32.420 --> 00:43:35.380]   The other alternative is that I'm not given a graph.
[00:43:35.380 --> 00:43:40.020]   For example, in our setup, we don't assume that we know a graph, but we use self-attention
[00:43:40.020 --> 00:43:42.140]   to learn the graph, right?
[00:43:42.140 --> 00:43:43.780]   So what is learning the graph?
[00:43:43.780 --> 00:43:49.580]   Learning a graph is akin to saying how much should one block attend to the other block,
[00:43:49.580 --> 00:43:50.580]   right?
[00:43:50.580 --> 00:43:52.780]   And that dictates how I should combine these blocks.
[00:43:52.780 --> 00:43:57.700]   So if I'm doing average pooling, what we're assuming is every block is equally important
[00:43:57.700 --> 00:44:00.420]   in the learned representation.
[00:44:00.420 --> 00:44:06.540]   With a graph, self-attention based graph network, we can let each block dictate how important
[00:44:06.540 --> 00:44:10.860]   it is when I am pooling the representations together.
[00:44:10.860 --> 00:44:16.380]   And that is what is helping us easily introduce more blocks because the block is not important,
[00:44:16.380 --> 00:44:21.580]   it won't be, but when it becomes important to the task, it starts getting included.
[00:44:21.580 --> 00:44:22.580]   Got it.
[00:44:22.580 --> 00:44:29.660]   That's actually a cool way of thinking about attention with graph networks that if you
[00:44:29.660 --> 00:44:35.380]   put in an all-to-all connected graph and then you use attention, it's like you're smoothly
[00:44:35.380 --> 00:44:41.500]   learning what graph you actually care about, or maybe task-dependent or state-dependent
[00:44:41.500 --> 00:44:50.440]   graphs that you're learning on top of this whatever base you start off of.
[00:44:50.440 --> 00:44:54.740]   So turning away from that paper, I didn't see any questions in the chat about it.
[00:44:54.740 --> 00:45:02.620]   So just turning over to the higher level kinds of problems in RL, I noticed that you mostly
[00:45:02.620 --> 00:45:09.020]   made use of real robots in most of the work you were doing.
[00:45:09.020 --> 00:45:10.900]   And that has to be a really intentional choice, right?
[00:45:10.900 --> 00:45:15.060]   Because it's so much harder and so much more expensive than just simulating stuff in MuJoCo.
[00:45:15.060 --> 00:45:20.340]   So where do you see the relative utility of doing stuff in simulation versus doing stuff
[00:45:20.340 --> 00:45:21.340]   in real robots?
[00:45:21.340 --> 00:45:27.100]   And do you have a reason for maybe preferring real robots as much as possible?
[00:45:27.100 --> 00:45:28.820]   That's a great question.
[00:45:28.820 --> 00:45:36.340]   So I think one practical thing that we almost always do is to first run things in simulation.
[00:45:36.340 --> 00:45:41.020]   For example, learning task-relevant abstractions, like the first paper I was presenting, everything
[00:45:41.020 --> 00:45:42.020]   was in simulation.
[00:45:42.020 --> 00:45:43.020]   It was on Atari games.
[00:45:43.020 --> 00:45:46.660]   It was on some deep mind-controlled tasks.
[00:45:46.660 --> 00:45:51.900]   Once we are sure that no things are working within simulation, we try to transfer them
[00:45:51.900 --> 00:45:54.940]   on the real system.
[00:45:54.940 --> 00:46:00.700]   So I do think that simulations are a great way to develop algorithms, because doing the
[00:46:00.700 --> 00:46:07.460]   whole loop of algorithm development on a real robotic system, especially with learning,
[00:46:07.460 --> 00:46:10.140]   can be quite challenging.
[00:46:10.140 --> 00:46:14.860]   The second thing, I think the reason to work with a real robot is to remain true to the
[00:46:14.860 --> 00:46:15.860]   problems.
[00:46:15.860 --> 00:46:22.700]   For example, in simulation, it's very easy to assume that segmentation is solved, for
[00:46:22.700 --> 00:46:25.600]   example, and then operate on that assumption.
[00:46:25.600 --> 00:46:32.580]   So in that way, I could make my AI problems be reasoning problems, because I assume my
[00:46:32.580 --> 00:46:36.780]   lower-level primitive or symbols have been given to me.
[00:46:36.780 --> 00:46:40.300]   But turns out in the real world, that is a very hard problem.
[00:46:40.300 --> 00:46:45.860]   Even though we have very good systems like mass-RCNN and other segmentation systems,
[00:46:45.860 --> 00:46:49.900]   getting a good segmentation or getting a good estimate of the force in the real world is
[00:46:49.900 --> 00:46:50.900]   extremely hard.
[00:46:50.900 --> 00:46:58.260]   So these are not just reasoning problems that we need to solve, but they are fundamentally
[00:46:58.260 --> 00:47:02.140]   problems which are hard at the perception level.
[00:47:02.140 --> 00:47:08.060]   So how do we remain true to what the real world presents us with?
[00:47:08.060 --> 00:47:12.780]   It really forces us to work in the real world.
[00:47:12.780 --> 00:47:15.860]   And also, once you see a real system working, it's cool.
[00:47:15.860 --> 00:47:20.220]   It's exciting, as opposed to having something and only working in simulation.
[00:47:20.220 --> 00:47:23.700]   And then there are other reasons also.
[00:47:23.700 --> 00:47:28.820]   For example, simulating audio or simulating touch sensing is much harder.
[00:47:28.820 --> 00:47:33.060]   Or if you have non-rigid materials, those simulations are much slower.
[00:47:33.060 --> 00:47:39.020]   So sometimes it's just easier to get richer data in the real world than to go and really
[00:47:39.020 --> 00:47:45.260]   create that content, either by putting more sensors or you need to hire people who design
[00:47:45.260 --> 00:47:48.500]   the content, which is something which is happening.
[00:47:48.500 --> 00:47:54.060]   I think at the time when simulators are going to improve even more and become more realistic
[00:47:54.060 --> 00:47:57.940]   and have diverse content, we'll be very happy to work only in simulation.
[00:47:57.940 --> 00:48:01.820]   But I don't think that time is just about now.
[00:48:01.820 --> 00:48:10.660]   Yeah, that's interesting that you mentioned some of the issues with simulations here and
[00:48:10.660 --> 00:48:12.860]   with staying true to the problem.
[00:48:12.860 --> 00:48:22.300]   One thing I saw a good talk by Vlad Koltun at IBM, and he said-- or Intel, I think, actually.
[00:48:22.300 --> 00:48:27.940]   Anyway, Vlad Koltun, who did the VisDoom stuff and a lot of cool RL things, his big thing
[00:48:27.940 --> 00:48:34.740]   that he really likes to get around issues with simulation is using relatively abstract
[00:48:34.740 --> 00:48:35.740]   sensors.
[00:48:35.740 --> 00:48:40.180]   So rather than using direct from pixels, using something maybe like a point cloud, or using
[00:48:40.180 --> 00:48:45.820]   something that's detecting distances or things like that, because then you don't actually
[00:48:45.820 --> 00:48:50.300]   need that high quality of a simulation in order to transfer to the real world.
[00:48:50.300 --> 00:48:55.180]   You don't care about specular reflection in your simulation, because there are no reflections.
[00:48:55.180 --> 00:49:01.020]   You've sort of abstracted them away by designing the right sensor.
[00:49:01.020 --> 00:49:06.820]   So I guess it seems like, in your view, a lot of the sensors that you do really care
[00:49:06.820 --> 00:49:11.820]   about, the sensors that you maybe need to solve these problems actually do need to operate
[00:49:11.820 --> 00:49:17.380]   at the level of the full simulation of the video or the audio or the world.
[00:49:17.380 --> 00:49:21.060]   Would you say that that's maybe an accurate statement?
[00:49:21.060 --> 00:49:26.820]   Well, I think there is always a perspective over here.
[00:49:26.820 --> 00:49:28.540]   The perspective is yes.
[00:49:28.540 --> 00:49:34.580]   I think the fact that we can go to a more abstract representation or sensing is a useful
[00:49:34.580 --> 00:49:35.580]   thing.
[00:49:35.580 --> 00:49:40.940]   For example, if I'm only measuring point clouds, I'm getting rid of some of the RGB stuff.
[00:49:40.940 --> 00:49:44.660]   But point clouds also have errors in them.
[00:49:44.660 --> 00:49:48.300]   If you have a specular surface and if you shoot light, the light is not going to come
[00:49:48.300 --> 00:49:50.780]   back to your sensor.
[00:49:50.780 --> 00:49:53.420]   So you are still going to get holes in that scenario.
[00:49:53.420 --> 00:50:01.700]   So I think for some problems, it is easy to abstract out and get good low dimensional
[00:50:01.700 --> 00:50:02.700]   observations.
[00:50:02.700 --> 00:50:05.900]   And in that case, sim-to-real transfer works very well.
[00:50:05.900 --> 00:50:10.700]   So if you look at TextNet, which was work out of Ken Goldberg's lab and Jeff Moeller
[00:50:10.700 --> 00:50:16.220]   at Berkeley, the reason TextNet works in the real world is because they work on depth images
[00:50:16.220 --> 00:50:18.500]   instead of RGB images.
[00:50:18.500 --> 00:50:22.020]   And that makes the transfer problem be much easier.
[00:50:22.020 --> 00:50:27.540]   So I definitely think there are techniques which can make the transfer be much easier.
[00:50:27.540 --> 00:50:33.420]   And having abstract representations is one of those methods.
[00:50:33.420 --> 00:50:38.300]   But I think the downside is sometimes we assume way too much.
[00:50:38.300 --> 00:50:41.780]   For example, we can assume segmentation is solved.
[00:50:41.780 --> 00:50:45.500]   I think that is where I have an issue.
[00:50:45.500 --> 00:50:50.220]   There is a part of the community that believes that computer vision is a separate problem
[00:50:50.220 --> 00:50:52.060]   than reasoning.
[00:50:52.060 --> 00:50:54.020]   I don't think that is true.
[00:50:54.020 --> 00:50:58.900]   I think knowing how to segment is also a reasoning problem.
[00:50:58.900 --> 00:51:04.060]   And we can't just say that let's first solve segmentation, then we can solve control.
[00:51:04.060 --> 00:51:09.180]   I think this is what we tried in '60s, '70s, and for a long time, AI did not work.
[00:51:09.180 --> 00:51:13.420]   I think we really need to solve the problem together.
[00:51:13.420 --> 00:51:19.020]   Yeah, I like that perspective, definitely.
[00:51:19.020 --> 00:51:24.820]   The sensory motor perspective as opposed to the sensation and motion separately kind
[00:51:24.820 --> 00:51:29.900]   of perspective.
[00:51:29.900 --> 00:51:36.220]   One thing I noticed is in a couple of your papers, you talk about long horizon planning.
[00:51:36.220 --> 00:51:40.340]   And then the kinds of tasks that you're thinking about are sort of like putting a box of cereal
[00:51:40.340 --> 00:51:42.780]   onto a shelf.
[00:51:42.780 --> 00:51:46.740]   And I think that kind of underscores a little bit just how hard RL is.
[00:51:46.740 --> 00:51:50.580]   Like doing something that we think of as simple, like moving an object from one place to another
[00:51:50.580 --> 00:51:56.580]   in a different orientation actually is long-term planning and is extremely difficult.
[00:51:56.580 --> 00:52:03.100]   But I'm curious, how well do you think that these kinds of things that you're using for
[00:52:03.100 --> 00:52:09.060]   maybe long-term planning at that scale will transfer to long-term planning at the maybe
[00:52:09.060 --> 00:52:15.260]   like what humans think of as long-term planning, like days, weeks, months, long-term planning
[00:52:15.260 --> 00:52:22.540]   for, or even something maybe even shorter, like playing a game like Montezuma's Revenge
[00:52:22.540 --> 00:52:28.460]   on the Atari, where you have to operate in many different, like almost distinct state
[00:52:28.460 --> 00:52:31.140]   spaces to achieve rewards that are very far away.
[00:52:31.140 --> 00:52:34.500]   Yeah, I think it's an excellent question.
[00:52:34.500 --> 00:52:38.540]   So I think the term long-term planning that we use is related to what is being done in
[00:52:38.540 --> 00:52:39.540]   the field.
[00:52:39.540 --> 00:52:46.620]   Those problems are long-term and it becomes challenging to solve those problems.
[00:52:46.620 --> 00:52:49.140]   Now, how well would these techniques work?
[00:52:49.140 --> 00:52:54.620]   So I mean, right now the work I presented was specifically making use of some assumptions
[00:52:54.620 --> 00:52:58.620]   about rigid body manipulation to solve that problem.
[00:52:58.620 --> 00:53:03.700]   And I think what we really need to do to go to the kind of long-term problems you're talking
[00:53:03.700 --> 00:53:11.220]   about, maybe days or weeks, or maybe even playing a game like Montezuma's Revenge.
[00:53:11.220 --> 00:53:17.260]   So maybe Montezuma's Revenge is, it could be long-term planning.
[00:53:17.260 --> 00:53:21.940]   It may not be long-term planning because many times these games are very long horizon in
[00:53:21.940 --> 00:53:28.740]   the sense that your reward can come on much later, but you don't really have to plan for
[00:53:28.740 --> 00:53:30.380]   that long.
[00:53:30.380 --> 00:53:34.940]   In a sense that you can still do things that are very reactive, but you have to do the
[00:53:34.940 --> 00:53:36.500]   right reactive things in a sequence.
[00:53:36.500 --> 00:53:43.620]   So you can break the long plan into small chunks of plan and still execute the big thing.
[00:53:43.620 --> 00:53:47.860]   But anyways, I mean, just abstracting out of that, I think what is really required is
[00:53:47.860 --> 00:53:48.860]   some notion of memory.
[00:53:48.860 --> 00:53:54.980]   For example, in Montezuma's Revenge, you need to remember, I went there before, there was
[00:53:54.980 --> 00:53:58.220]   nothing there, I need to go somewhere else.
[00:53:58.220 --> 00:54:02.580]   But for example, if I'm planning over days, I need to have a model of what can change
[00:54:02.580 --> 00:54:03.580]   and what cannot change.
[00:54:03.580 --> 00:54:04.580]   So what is persistent?
[00:54:04.580 --> 00:54:10.540]   For example, if I think about the kitchen right now, you know what things are there,
[00:54:10.540 --> 00:54:15.260]   where they are placed roughly, and maybe some things are going to be different.
[00:54:15.260 --> 00:54:21.420]   Now that persistent model allows you to say, if I want to make coffee, figure out where
[00:54:21.420 --> 00:54:22.420]   should I get the milk from.
[00:54:22.420 --> 00:54:27.060]   Right now for an RL agent, it ends up exploring.
[00:54:27.060 --> 00:54:30.860]   If I say, okay, make me coffee, first it will do, okay, where is the milk?
[00:54:30.860 --> 00:54:31.860]   Where is coffee?
[00:54:31.860 --> 00:54:35.380]   It will end up spending a lot of time just exploring these things.
[00:54:35.380 --> 00:54:40.500]   So I think to get long term plans really to work, we need ways in which you can integrate
[00:54:40.500 --> 00:54:47.060]   information, store them around in a persistent way, even when we are not observing the world
[00:54:47.060 --> 00:54:50.660]   or that part of the world, and then leverage them for planning.
[00:54:50.660 --> 00:54:58.300]   So I think this whole incorporation of memory is an exciting research topic, but also something
[00:54:58.300 --> 00:55:02.740]   which in robotics, we haven't been able to make as much progress.
[00:55:02.740 --> 00:55:07.580]   Obviously, we have classical ways, like for example, SLAM.
[00:55:07.580 --> 00:55:09.740]   SLAM is building a map.
[00:55:09.740 --> 00:55:11.740]   Map is kind of a memory.
[00:55:11.740 --> 00:55:14.060]   Memory allows for long term plan.
[00:55:14.060 --> 00:55:18.700]   But then with SLAM, you have issues like, okay, if you just want to avoid obstacles,
[00:55:18.700 --> 00:55:19.700]   everything is fine.
[00:55:19.700 --> 00:55:24.060]   But if you want semantics of where is the door, where are the things, putting the layer
[00:55:24.060 --> 00:55:25.700]   of semantics becomes hard.
[00:55:25.700 --> 00:55:31.500]   You can put semantics by maybe having classifiers like master of CNN, but then what about moving
[00:55:31.500 --> 00:55:32.500]   objects?
[00:55:32.500 --> 00:55:37.100]   So I think defining good memory structures and incorporating them is probably going to
[00:55:37.100 --> 00:55:43.700]   be the thing which leads us to the kind of high level planning we are talking about.
[00:55:43.700 --> 00:55:51.340]   >> Yeah, definitely, I remember when I had Nick Ryder, who is on the GPT-3 paper on a
[00:55:51.340 --> 00:55:57.860]   couple months ago, and we talked a little bit about how one of the next frontiers in
[00:55:57.860 --> 00:56:03.100]   getting better language generation is definitely being able to do memory that is distinct from
[00:56:03.100 --> 00:56:04.100]   attention.
[00:56:04.100 --> 00:56:08.740]   That is not just like working memory, where you can kind of maintain a small amount of
[00:56:08.740 --> 00:56:12.900]   information, but like memory, as you said, where you store it and you can come back to
[00:56:12.900 --> 00:56:14.460]   it later.
[00:56:14.460 --> 00:56:18.340]   That seems like a really tough problem to make differentiable, right?
[00:56:18.340 --> 00:56:23.580]   That seems to be one of the really hard things about it.
[00:56:23.580 --> 00:56:28.460]   Are you real optimistic about our ability to kind of solve this problem of adding memory
[00:56:28.460 --> 00:56:33.760]   to our agents and our networks and our modules, like using the current paradigms of thinking
[00:56:33.760 --> 00:56:36.100]   about deep learning and machine learning?
[00:56:36.100 --> 00:56:42.380]   >> I think you did a very good point that differentiability over a very, very long time
[00:56:42.380 --> 00:56:47.180]   scale is possibly going to be very hard, right?
[00:56:47.180 --> 00:56:51.980]   So I think instead of thinking of these systems as being always differentiable, I think there
[00:56:51.980 --> 00:56:53.860]   are alternatives, right?
[00:56:53.860 --> 00:57:01.260]   So for example, by solving simple problems, we can learn when to assign credit, right?
[00:57:01.260 --> 00:57:06.500]   And therefore, we can learn an algorithm of how to assign credit by doing backprop on
[00:57:06.500 --> 00:57:07.500]   simple problems.
[00:57:07.500 --> 00:57:13.620]   So when we go on bigger problems, we can leverage this algorithm or this mechanism instead of
[00:57:13.620 --> 00:57:16.660]   really doing backprop over there, right?
[00:57:16.660 --> 00:57:21.540]   So I think if you look at some of the meta-learning work, like RL Squared, that what they're doing
[00:57:21.540 --> 00:57:26.540]   is they're saying, let me learn an algorithm which can solve a task, right?
[00:57:26.540 --> 00:57:29.700]   And the meta-learning part is to learn that algorithm.
[00:57:29.700 --> 00:57:37.900]   So I think some of these approaches could help us bypass this long-term credit assignment
[00:57:37.900 --> 00:57:45.180]   problem, which essentially is the crux of using long-term memory, right?
[00:57:45.180 --> 00:57:48.340]   So I mean, I think there'll be work trying to improve backprop.
[00:57:48.340 --> 00:57:53.660]   I think there'll be work trying to leverage backprop so it can transfer some learning
[00:57:53.660 --> 00:57:54.660]   mechanisms.
[00:57:54.660 --> 00:57:59.380]   I mean, personally, I would be more on the second side than the first side.
[00:57:59.380 --> 00:58:02.180]   But let's see how things play out.
[00:58:02.180 --> 00:58:04.820]   Yeah, that's an interesting perspective.
[00:58:04.820 --> 00:58:11.180]   Yeah, thinking of deep learning as differentiable programming, maybe we could differentially
[00:58:11.180 --> 00:58:17.220]   program or differentiably program calling certain sub-modules that are not differentiable.
[00:58:17.220 --> 00:58:22.580]   Like maybe malloc doesn't have to be differentiable, but knowing when to call it does.
[00:58:22.580 --> 00:58:25.020]   Yeah, yeah, yeah, precisely.
[00:58:25.020 --> 00:58:30.900]   That's a very good way to put it, Charles.
[00:58:30.900 --> 00:58:31.900]   Thanks.
[00:58:31.900 --> 00:58:39.100]   One question I did want to make sure to ask, we're almost out of time.
[00:58:39.100 --> 00:58:42.660]   I was actually expecting a little bit more discussion of sort of like curiosity and how
[00:58:42.660 --> 00:58:48.580]   to drive machines and algorithms to explore more.
[00:58:48.580 --> 00:58:52.580]   Could you talk a little bit about what your current perspective is on sort of solving
[00:58:52.580 --> 00:58:59.180]   that problem of ensuring that agents play or that agents explore or that agents are
[00:58:59.180 --> 00:59:02.340]   curious in that phase of the learning process?
[00:59:02.340 --> 00:59:08.540]   Yeah, so a lot of the work, both in reinforcement learning and self-supervised learning is based
[00:59:08.540 --> 00:59:10.260]   on data collection.
[00:59:10.260 --> 00:59:14.580]   And the question becomes is how do we collect interesting data?
[00:59:14.580 --> 00:59:19.220]   So that is where the notion of exploration becomes important.
[00:59:19.220 --> 00:59:23.620]   And one way to explore is to be curious in some way.
[00:59:23.620 --> 00:59:29.860]   So in some of our prior work, we have defined curiosity as prediction error, saying that
[00:59:29.860 --> 00:59:34.700]   if I'm building a model, things which I'm unable to predict well are things I should
[00:59:34.700 --> 00:59:35.700]   be more curious about.
[00:59:35.700 --> 00:59:40.860]   So if you use prediction error as a reward signal, then you're saying things I cannot
[00:59:40.860 --> 00:59:44.900]   predict well are high reward things, so let me try them more.
[00:59:44.900 --> 00:59:48.460]   If you try them more, we might get better at them, and therefore the prediction error
[00:59:48.460 --> 00:59:51.540]   is going to reduce, which is going to reduce the reward.
[00:59:51.540 --> 00:59:55.940]   So I become less interested in trying those things.
[00:59:55.940 --> 01:00:00.100]   So now that creates this never-ending loop where you're always focused on trying new
[01:00:00.100 --> 01:00:02.060]   and new things.
[01:00:02.060 --> 01:00:06.660]   Now the good news is that these algorithms actually work in simulation.
[01:00:06.660 --> 01:00:12.820]   The bad news is it still requires millions of interactions to make these algorithms work.
[01:00:12.820 --> 01:00:17.700]   The second thing which also happens, which I think is a good area of research with curiosity
[01:00:17.700 --> 01:00:24.340]   algorithms is they end up doing what I call as depth-first search, that if they find some
[01:00:24.340 --> 01:00:29.700]   part of the environment they have to explore, they really go deep along that path.
[01:00:29.700 --> 01:00:32.900]   But there can be other parts of the environment which they can ignore.
[01:00:32.900 --> 01:00:36.580]   So it's a difference between breadth-first search and depth-first search.
[01:00:36.580 --> 01:00:40.260]   It seems like many systems we have today are doing depth-first search, so they've become
[01:00:40.260 --> 01:00:43.380]   narrow experts in exploration also.
[01:00:43.380 --> 01:00:48.980]   So I think what we really need to do is to somehow bias them to do more breadth-first
[01:00:48.980 --> 01:00:49.980]   exploration.
[01:00:49.980 --> 01:00:54.740]   So that, I think, is an interesting area to look into.
[01:00:54.740 --> 01:01:00.420]   And the third thing is, if you look at a lot of curiosity works, we are, again, learning
[01:01:00.420 --> 01:01:04.060]   how to be curious on every environment from scratch.
[01:01:04.060 --> 01:01:11.180]   I think some notion of transfer and how to explore is also something which is an exciting
[01:01:11.180 --> 01:01:13.900]   area to look into.
[01:01:13.900 --> 01:01:21.340]   Yeah, that's a-- I hadn't thought of transfer learning at that level.
[01:01:21.340 --> 01:01:26.940]   Though I will say that your description of a curious algorithm doing depth-first search
[01:01:26.940 --> 01:01:32.100]   and only focusing on one possibly irrelevant part of the environment really struck a chord
[01:01:32.100 --> 01:01:35.340]   with me as somebody who went and got a PhD.
[01:01:35.340 --> 01:01:39.180]   I think every grad student can really empathize with those agents.
[01:01:39.180 --> 01:01:43.420]   Yeah, it's like the usual trap.
[01:01:43.420 --> 01:01:45.940]   Something is super exciting, keep on doing that, that, that.
[01:01:45.940 --> 01:01:48.780]   And it has its benefits.
[01:01:48.780 --> 01:01:49.780]   Sometimes that is required.
[01:01:49.780 --> 01:01:56.180]   Grad students putting in that head and saying, let me just keep on doing it for a long time.
[01:01:56.180 --> 01:01:57.900]   And it leads to good things.
[01:01:57.900 --> 01:02:02.780]   But sometimes it's good to have a knob so you can balance two things off.
[01:02:02.780 --> 01:02:08.180]   I think that is a knob that we are missing.
[01:02:08.180 --> 01:02:11.980]   Yeah, that's interesting.
[01:02:11.980 --> 01:02:13.620]   And it's another hyperparameter to tune.
[01:02:13.620 --> 01:02:16.140]   Just toss that into the hyperparameter optimization.
[01:02:16.140 --> 01:02:17.140]   Maybe.
[01:02:17.140 --> 01:02:18.140]   Just kidding.
[01:02:18.140 --> 01:02:19.140]   Just kidding.
[01:02:19.140 --> 01:02:20.980]   There's got to be a better way.
[01:02:20.980 --> 01:02:21.980]   Yeah.
[01:02:21.980 --> 01:02:23.260]   All right.
[01:02:23.260 --> 01:02:25.460]   That's all the time that we have.
[01:02:25.460 --> 01:02:29.380]   So thank you so much, Polkett, for coming on and talking about your work.
[01:02:29.380 --> 01:02:32.460]   It was really a pleasure to hear about all the stuff that you've been doing.
[01:02:32.460 --> 01:02:38.060]   And I look forward to seeing more work in common sense robotics and self-supervised
[01:02:38.060 --> 01:02:40.140]   learning coming out of the Improbable AI Lab.
[01:02:40.140 --> 01:02:45.060]   Well, thanks a lot, Charles, for having me and for hosting me.
[01:02:45.060 --> 01:02:48.980]   And looking forward to more interactions and seeing you sometime soon.
[01:02:48.980 --> 01:02:49.980]   Thank you.
[01:02:49.980 --> 01:02:50.980]   Yep.

