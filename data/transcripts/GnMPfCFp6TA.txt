
[00:00:00.000 --> 00:00:05.200]   By the time that an AI model can get gold in the International Math Olympiad, is that
[00:00:05.200 --> 00:00:10.400]   just AGI given the amount of creative problem solving and chain of thought required to do
[00:00:10.400 --> 00:00:11.400]   that?
[00:00:11.400 --> 00:00:15.700]   I, to be honest, have no idea what people mean when they use the word AGI.
[00:00:15.700 --> 00:00:18.800]   I think if you ask 10 different people like what they mean by it, you're going to get
[00:00:18.800 --> 00:00:20.820]   10 slightly different answers.
[00:00:20.820 --> 00:00:26.160]   And it seems like what people want to get at is a discrete change that I don't think
[00:00:26.160 --> 00:00:27.160]   actually exists.
[00:00:27.160 --> 00:00:30.840]   Where you've got, OK, AI is up to a certain point, they're not AGI.
[00:00:30.840 --> 00:00:33.280]   They might be really smart, but it's not AGI.
[00:00:33.280 --> 00:00:36.840]   And then after some point, that's the benchmark when like now they're, now it's generally
[00:00:36.840 --> 00:00:38.280]   intelligent.
[00:00:38.280 --> 00:00:42.000]   The reason that world model doesn't really fit is it feels a lot more continuous where,
[00:00:42.000 --> 00:00:48.280]   you know, GPT-4 feels general in the sense that you have one training algorithm that
[00:00:48.280 --> 00:00:52.040]   applies to a very, very large set of different kinds of tasks that someone might want to
[00:00:52.040 --> 00:00:53.240]   be able to do.
[00:00:53.240 --> 00:00:54.240]   And that's cool.
[00:00:54.240 --> 00:00:58.560]   That's an invention that people in the 60s might not have expected to be true for the
[00:00:58.560 --> 00:01:01.680]   nature of how artificial intelligence can be programmed.
[00:01:01.680 --> 00:01:04.040]   So it's generally intelligent.
[00:01:04.040 --> 00:01:08.320]   But maybe what people mean by, oh, it's not AGI, is you've got certain benchmarks where,
[00:01:08.320 --> 00:01:11.240]   you know, it's better than most people at some things, but it's not better at most people
[00:01:11.240 --> 00:01:12.240]   than others.
[00:01:12.240 --> 00:01:15.000]   You know, at this point, it's better than most people at math.
[00:01:15.000 --> 00:01:19.000]   You know, it's better than most people at solving AMC problems and like IMO problems.
[00:01:19.000 --> 00:01:21.320]   It's just not better than the best.
[00:01:21.320 --> 00:01:26.360]   And so maybe at the point when it's getting golds in the IMO, that's a sign that, okay,
[00:01:26.360 --> 00:01:28.280]   it's as good as the best.
[00:01:28.280 --> 00:01:33.720]   And we've ticked off another domain, but I don't know, like, is what you mean by AGI
[00:01:33.720 --> 00:01:38.400]   that you've enumerated all the possible domains that something could be good at, and now it's
[00:01:38.400 --> 00:01:40.040]   better than humans at all of them?
[00:01:40.040 --> 00:01:47.040]   Or enough that it could take over substantial fractions of, you know, human jobs or something.
[00:01:47.040 --> 00:01:51.960]   Where right now, it's impressive, but it's not going to be even 1% of GDP.
[00:01:51.960 --> 00:01:55.960]   But in my mind, if it's getting gold in IMO, I mean, having seen some of those problems
[00:01:55.960 --> 00:02:01.560]   from your channel, I'm thinking, wow, that's really coming after podcasters and video animators.
[00:02:01.560 --> 00:02:02.560]   I don't know.
[00:02:02.560 --> 00:02:03.560]   I don't know.
[00:02:03.560 --> 00:02:04.560]   That feels orthogonal.
[00:02:04.560 --> 00:02:06.880]   Because getting a gold in the IMO feels a lot more like being really, really good at
[00:02:06.880 --> 00:02:08.000]   Go or chess.
[00:02:08.000 --> 00:02:09.480]   Like those feel analogous.
[00:02:09.480 --> 00:02:11.200]   Well, it's super creative.
[00:02:11.200 --> 00:02:15.140]   Like I think anyone, I don't know chess as well as the people who are into it, but everything
[00:02:15.140 --> 00:02:19.800]   that I hear from them, the sort of moves that are made and choices have all of the air of
[00:02:19.800 --> 00:02:21.420]   creativity.
[00:02:21.420 --> 00:02:25.040]   I think as soon as they started generating artwork, then everyone else could appreciate,
[00:02:25.040 --> 00:02:29.000]   oh, there's something that deserves to be called creative here.
[00:02:29.000 --> 00:02:33.400]   And the creative side of the math, you know, I don't know how it would look when people
[00:02:33.400 --> 00:02:38.840]   get them to be getting golds at the IMO, but I imagine it's something that looks a little
[00:02:38.840 --> 00:02:42.780]   bit like how AlphaGo is trained, where you have it, like, play with itself a whole bunch.
[00:02:42.780 --> 00:02:48.180]   You know, math lends itself to synthetic data in the ways that a lot of other domains don't.
[00:02:48.180 --> 00:02:52.320]   You could have it produce a lot of proofs in a proof checking language, like Lean, for
[00:02:52.320 --> 00:02:55.300]   example, and just train on a whole bunch of those and like, is this a valid proof?
[00:02:55.300 --> 00:02:56.740]   Is this not a valid proof?
[00:02:56.740 --> 00:03:00.740]   And then counterbalance that with English written versions of something.
[00:03:00.740 --> 00:03:05.180]   And so I imagine what it looks like once you get something that is solving these IMO level
[00:03:05.180 --> 00:03:06.180]   things.
[00:03:06.180 --> 00:03:12.820]   But one of two things, either it writes a very good proof that you feel like is unmotivated.
[00:03:12.820 --> 00:03:16.140]   Because anyone who reads math papers has this feeling that there are two types.
[00:03:16.140 --> 00:03:19.460]   There's the ones where you morally understand why the result should be true.
[00:03:19.460 --> 00:03:22.540]   And then there's the ones where you're like, I can follow the steps.
[00:03:22.540 --> 00:03:23.780]   Why would you have come up with that?
[00:03:23.780 --> 00:03:24.780]   I don't know.
[00:03:24.780 --> 00:03:26.700]   But I guess that shows that the result is true.
[00:03:26.700 --> 00:03:29.200]   And you're left wanting something a little bit more.
[00:03:29.200 --> 00:03:33.700]   And so you could imagine if it produces that on the gold to get a gold in the IMO, is that
[00:03:33.700 --> 00:03:38.900]   the same kind of ability as what is required to replace jobs?
[00:03:38.900 --> 00:03:43.500]   Just like not really like the impediments between where it is now and replacing jobs
[00:03:43.500 --> 00:03:49.120]   feels like a whole different set of things like having a context window that is longer
[00:03:49.120 --> 00:03:53.020]   than some small things such that you can make connections over long periods of time and
[00:03:53.020 --> 00:03:57.740]   build relationships and understand where someone's coming from and the actual problem solving
[00:03:57.740 --> 00:03:58.740]   part of it.
[00:03:58.740 --> 00:04:03.660]   I mean, it's a sign that it would be a more helpful tool, but in the same way that like
[00:04:03.660 --> 00:04:07.320]   Mathematica can help you solve math problems much more effectively.
[00:04:07.320 --> 00:04:10.100]   And tell me why I should be less amazed by it or maybe put it in a different context.
[00:04:10.100 --> 00:04:17.060]   But the reason I would be very impressed is that with chess or something, obviously this
[00:04:17.060 --> 00:04:21.180]   is not all the chess programs are doing, but there's a level of research you can do to
[00:04:21.180 --> 00:04:23.060]   narrow down the possibilities.
[00:04:23.060 --> 00:04:28.700]   And more importantly, in the math example, it seems that with some of the examples you've
[00:04:28.700 --> 00:04:33.740]   illustrated on your channel, for example, the ability to solve the problem is so dependent
[00:04:33.740 --> 00:04:37.420]   on coming up with the right abstraction to think about it.
[00:04:37.420 --> 00:04:41.140]   Coming up with ways of thinking about the problem that are not evident in the problem
[00:04:41.140 --> 00:04:44.860]   itself or in any other problem, in any other test.
[00:04:44.860 --> 00:04:49.900]   That seems different from just a chess game where you don't have to like, what is the
[00:04:49.900 --> 00:04:54.460]   largest structure of this chess game in the same way as you do with the IML problem?
[00:04:54.460 --> 00:04:58.980]   I think you should ask people who know a lot about Go and chess, and I'd be curious to
[00:04:58.980 --> 00:05:03.420]   hear their opinions on it because I imagine what they would say is, if you're going to
[00:05:03.420 --> 00:05:08.140]   be as good at Go as AlphaGo is, you're also not doing tree search, at least exclusively.
[00:05:08.140 --> 00:05:11.860]   It's not dependent on something as growth in that because you get this combinatorial
[00:05:11.860 --> 00:05:15.940]   explosion, which is why people thought that game would be so much harder for so much longer.
[00:05:15.940 --> 00:05:20.340]   There sort of has to be something like a higher level structure in their understanding.
[00:05:20.340 --> 00:05:25.540]   And then, don't get me wrong, I would be super impressed and like anticipate being very impressed
[00:05:25.540 --> 00:05:31.100]   when you get AIs that can solve these IML problems because you're absolutely right.
[00:05:31.100 --> 00:05:33.020]   There's a level of creativity involved.
[00:05:33.020 --> 00:05:39.180]   The only claim I'm making is that being able to do that feels distinct from the impediments
[00:05:39.180 --> 00:05:44.540]   between, I don't know, where we are now and the AIs take over all of our jobs or something.
[00:05:44.540 --> 00:05:48.780]   It just, it seems like it's going to be another one of those boxes that sits this historic
[00:05:48.780 --> 00:05:55.180]   moment analogous to chess and Go, more so than it's going to be analogous to the Industrial
[00:05:55.180 --> 00:05:56.180]   Revolution.
[00:05:56.180 --> 00:05:58.740]   I'm surprised you wouldn't be more compelled.
[00:05:58.740 --> 00:06:00.140]   I am compelled.
[00:06:00.140 --> 00:06:01.140]   Right.
[00:06:01.140 --> 00:06:06.580]   Or just you don't think that skill of, this problem is isomorphic to this completely different
[00:06:06.580 --> 00:06:09.080]   way of thinking about what's happening in the situation.
[00:06:09.080 --> 00:06:13.260]   And here's me going through the 50 steps to put all that together into this one proof.
[00:06:13.260 --> 00:06:18.620]   I'm surprised you don't think that's upstream of a lot of valuable tasks.
[00:06:18.620 --> 00:06:25.460]   I think it's a similar level of how impressed I was with the stable diffusion type stuff
[00:06:25.460 --> 00:06:32.020]   where, you know, you ask it, give me a landscape of beautiful mountains, but made out of like
[00:06:32.020 --> 00:06:34.020]   quartz and gemstones or something.
[00:06:34.020 --> 00:06:37.940]   And it gives you this thing which has all of the essence of a landscape, but it's not
[00:06:37.940 --> 00:06:39.100]   literally a landscape.
[00:06:39.100 --> 00:06:43.660]   And so you realize, okay, there's something, something beyond the literal that's understood
[00:06:43.660 --> 00:06:44.660]   here.
[00:06:44.660 --> 00:06:45.660]   Like, that's very impressive.
[00:06:45.660 --> 00:06:48.340]   It's the same way to solve one of these math problems that requires creativity.
[00:06:48.340 --> 00:06:49.900]   You can't just go from the definitions.
[00:06:49.900 --> 00:06:51.340]   You're 100% right.
[00:06:51.340 --> 00:06:56.260]   You need this element of lateral thinking, which is why we find so much joy in finding
[00:06:56.260 --> 00:06:59.920]   the solutions ourselves or even just seeing other people get those solutions.
[00:06:59.920 --> 00:07:05.060]   It's exactly the kind of joy that you get out of good artistic analogies and comparisons
[00:07:05.060 --> 00:07:06.620]   and mixing and matching.
[00:07:06.620 --> 00:07:08.860]   I'm very impressed by all of that.
[00:07:08.860 --> 00:07:13.100]   It's just, I think it's in the same category and it's maybe I don't have the same opinions
[00:07:13.100 --> 00:07:18.060]   as a lot of other people with this hard line between there's pre-AGI and post-AGI.
[00:07:18.060 --> 00:07:20.660]   I just don't know what they mean by the word AGI.
[00:07:20.660 --> 00:07:23.860]   And I don't think that you're going to have something that's this measurable discreet
[00:07:23.860 --> 00:07:28.700]   step much less that a math tournament is going to be an example of what that discreet step

