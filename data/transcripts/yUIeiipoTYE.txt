
[00:00:00.000 --> 00:00:03.480]   Hi, everyone.
[00:00:03.480 --> 00:00:08.680]   Hope everyone's doing well in quarantine or social distancing right now.
[00:00:08.680 --> 00:00:10.200]   Not going to stir crazy.
[00:00:10.200 --> 00:00:16.360]   Okay, so probably a lot of you are already familiar at this point with the kind of Allen
[00:00:16.360 --> 00:00:22.360]   NLP, well, the Allen AI research challenge on coronavirus.
[00:00:22.360 --> 00:00:25.960]   But anyways, if you aren't, I'll just very briefly say they basically released this large
[00:00:25.960 --> 00:00:33.440]   data set around 45,000 scholarly articles and the idea is to kind of look and see which
[00:00:33.440 --> 00:00:39.840]   ones are the most useful for answering specific questions about the virus.
[00:00:39.840 --> 00:00:42.960]   So you can read this later if you haven't already seen it.
[00:00:42.960 --> 00:00:45.200]   I won't go into too much more details.
[00:00:45.200 --> 00:00:48.640]   But one of the really cool things I think that came out of this kind of immediately,
[00:00:48.640 --> 00:00:53.280]   unlike a lot of other Kaggle challenges, was there was immediately kind of a cooperative
[00:00:53.280 --> 00:00:57.000]   effort instead of a really competitive effort from the get go.
[00:00:57.000 --> 00:01:04.320]   So this group kind of called Corona Y formed and now we have over 500 members, I believe,
[00:01:04.320 --> 00:01:10.120]   on our Slack and a lot of people are specifically working in teams in a very organized way to
[00:01:10.120 --> 00:01:18.020]   try to address some of the real problems and the progress we can kind of make on this issue.
[00:01:18.020 --> 00:01:19.680]   So I found that very interesting.
[00:01:19.680 --> 00:01:23.880]   If you're interested in joining the group, we're always looking for new people and I
[00:01:23.880 --> 00:01:27.560]   can send you over this Slack later.
[00:01:27.560 --> 00:01:34.920]   Specifically what I'm working on, and for some of you this may be kind of review though,
[00:01:34.920 --> 00:01:40.240]   is looking at kind of forming good sentence embeddings or good general purpose embeddings
[00:01:40.240 --> 00:01:43.600]   so to do semantic search on this corpus.
[00:01:43.600 --> 00:01:49.000]   One of my kind of interests for a while has been using transformers to kind of do effective
[00:01:49.000 --> 00:01:50.680]   representation learning.
[00:01:50.680 --> 00:01:55.120]   Some of that has actually been even on the time series side, but returning more to the
[00:01:55.120 --> 00:02:02.560]   NLP side for the moment, I really wanted to see if I could find some good, useful representations.
[00:02:02.560 --> 00:02:10.120]   And one of the really challenging things I think about this task in a sense is that we
[00:02:10.120 --> 00:02:12.680]   have no real evaluation metrics.
[00:02:12.680 --> 00:02:21.520]   All evaluation is qualitative and we really have to rely kind of on, and moreover I guess
[00:02:21.520 --> 00:02:26.960]   many of us don't even really know what would be good results, so we really have to rely
[00:02:26.960 --> 00:02:32.840]   on experts to evaluate what the results are and if they make sense.
[00:02:32.840 --> 00:02:37.600]   But I just wanted to kind of develop using kind of these embeddings and my knowledge
[00:02:37.600 --> 00:02:43.000]   of clustering a good way to quickly cluster things and make it so that the experts could
[00:02:43.000 --> 00:02:46.240]   see if those things kind of make sense.
[00:02:46.240 --> 00:02:53.280]   So yeah, I kind of wrote out this notebook very quickly.
[00:02:53.280 --> 00:02:56.800]   The top part probably you're all used to just downloading and installing.
[00:02:56.800 --> 00:03:01.240]   My main questions are just kind of with this notebook, are raw embeddings useful?
[00:03:01.240 --> 00:03:05.920]   How can we construct an efficient semantic search using these embeddings because there's
[00:03:05.920 --> 00:03:10.760]   always a tradeoff between doing like a full semantic search and the memory required, which
[00:03:10.760 --> 00:03:16.160]   I found all too well when this started repeatedly crashing due to lack of RAM.
[00:03:16.160 --> 00:03:20.120]   And then the other big thing is like, as I said, what does the embedding space look like?
[00:03:20.120 --> 00:03:24.120]   If we display the embedding space and have experts look at it, what can they tell us
[00:03:24.120 --> 00:03:26.240]   whatever it makes sense?
[00:03:26.240 --> 00:03:32.000]   And specifically since Silvano did ask me to say this earlier, I will say it right now,
[00:03:32.000 --> 00:03:36.120]   there's a lot of like machine learning going around with people not really understanding
[00:03:36.120 --> 00:03:44.560]   the problem space and not always understanding how it impacts stuff in a clinical sense or
[00:03:44.560 --> 00:03:46.280]   in a medical sense.
[00:03:46.280 --> 00:03:50.780]   I know I don't know that by myself, so I always want to try to rely on medical experts to
[00:03:50.780 --> 00:03:54.800]   try to evaluate my results and look at those.
[00:03:54.800 --> 00:03:59.560]   And I think you should follow all good machine learning best practices, but then also in
[00:03:59.560 --> 00:04:05.280]   addition really try to collaborate and form these cross team collaborations because we
[00:04:05.280 --> 00:04:08.480]   can't solve it on our own as machine learning experts.
[00:04:08.480 --> 00:04:10.960]   We need that expert advice.
[00:04:10.960 --> 00:04:15.160]   So without further ado, I'll just quickly run through some of this.
[00:04:15.160 --> 00:04:21.560]   So as I say, at this point, I just kind of want to see how these vanilla Cybert embeddings
[00:04:21.560 --> 00:04:22.560]   perform.
[00:04:22.560 --> 00:04:24.760]   So I just essentially loaded the model.
[00:04:24.760 --> 00:04:26.960]   I did a very naive embedding method.
[00:04:26.960 --> 00:04:32.760]   I basically took the average across all word embeddings.
[00:04:32.760 --> 00:04:35.960]   Later I'll show you how I refine this a bit.
[00:04:35.960 --> 00:04:40.620]   And then I did some basic cosine similarity scores.
[00:04:40.620 --> 00:04:43.680]   Some of these seem to actually give meaning kind of here.
[00:04:43.680 --> 00:04:49.000]   We do see like a high correlation, for instance, between coronavirus and MERS, coronavirus
[00:04:49.000 --> 00:04:51.520]   in a random word.
[00:04:51.520 --> 00:04:56.060]   There's still a high correlation, so obviously that's not great.
[00:04:56.060 --> 00:04:59.920]   Going through, kind of just did some of our helper functions.
[00:04:59.920 --> 00:05:02.680]   And then what I really wanted to do is I said plot the embedding space.
[00:05:02.680 --> 00:05:06.640]   So I actually used UMAP, which I find really useful.
[00:05:06.640 --> 00:05:11.360]   I kind of like, it's one of my go-to dimensionality reduction techniques.
[00:05:11.360 --> 00:05:18.520]   So with that, I kind of just plotted the article title embeddings just using this naive method.
[00:05:18.520 --> 00:05:24.400]   And it was kind of nice to see, at least in my own unexpert opinion, having just said
[00:05:24.400 --> 00:05:30.280]   that certain things do seem to form distinct kind of patterns on the cluster.
[00:05:30.280 --> 00:05:33.680]   Like here we can see health capacity management.
[00:05:33.680 --> 00:05:44.000]   I know that's kind of going off screen, but that's pretty much the only thing in this
[00:05:44.000 --> 00:05:45.000]   area.
[00:05:45.000 --> 00:05:48.960]   Then if we look at something like the top of the cluster, then you see there's similar
[00:05:48.960 --> 00:05:53.320]   kinds of article titles grouped together in this part.
[00:05:53.320 --> 00:05:59.040]   So obviously we'd want to get like an epidemiologist or a biochemist to actually thoroughly evaluate
[00:05:59.040 --> 00:06:01.880]   if these make a lot of sense.
[00:06:01.880 --> 00:06:05.300]   So moving down through, I did a couple more clusters.
[00:06:05.300 --> 00:06:08.840]   Then I did kind of a semantic search on the various titles.
[00:06:08.840 --> 00:06:15.800]   So one of the problems, as I said from the get-go, these are 768 dimensional embeddings
[00:06:15.800 --> 00:06:18.920]   that are returned by the BERT model.
[00:06:18.920 --> 00:06:26.360]   So they take up a lot of space, so it's just not practical to really do a full search of
[00:06:26.360 --> 00:06:27.720]   the corpus.
[00:06:27.720 --> 00:06:31.520]   And because I was limited to only embedding 200 articles, I think some of the results
[00:06:31.520 --> 00:06:36.360]   weren't that great to begin with, because I could only essentially embed it in 200 word
[00:06:36.360 --> 00:06:44.040]   chunks due to the-- or 200 article chunks due to the size of the titles.
[00:06:44.040 --> 00:06:47.200]   So yeah, that was definitely a limitation.
[00:06:47.200 --> 00:06:54.320]   I did try as a possible kind of unsupervised evaluation metric.
[00:06:54.320 --> 00:06:58.640]   Specifically I thought, if we have two kind of different queries, like one is coronavirus
[00:06:58.640 --> 00:07:07.960]   person-to-person transmission mechanics, and the other one is coronavirus infection origin
[00:07:07.960 --> 00:07:14.680]   and transmission from animals, these are actually two fairly different questions from kind of
[00:07:14.680 --> 00:07:16.660]   a research standpoint.
[00:07:16.660 --> 00:07:21.560]   So why a normal search engine might return those-- have those return similar results,
[00:07:21.560 --> 00:07:24.400]   ideally we want them to return very different results.
[00:07:24.400 --> 00:07:30.320]   So what I did is I took those two queries, then I embedded the 10 returned results.
[00:07:30.320 --> 00:07:37.040]   And you can see that these aren't very good, because ideally we'd see distinct-- I guess
[00:07:37.040 --> 00:07:42.360]   distinct areas in the kind of embedding space where the different search results should
[00:07:42.360 --> 00:07:44.560]   return just qualitatively.
[00:07:44.560 --> 00:07:45.560]   And they're kind of mixed.
[00:07:45.560 --> 00:07:48.000]   There's kind of even some overlapping ones.
[00:07:48.000 --> 00:07:54.400]   But again, this is kind of just on the partial kind of corpus and not the full one, just
[00:07:54.400 --> 00:07:56.880]   about 200 articles in the search.
[00:07:56.880 --> 00:08:00.600]   So a little bit's kind of understandable.
[00:08:00.600 --> 00:08:04.400]   Later on, I kind of went to embedding abstracts, which of course, full abstracts, which was
[00:08:04.400 --> 00:08:08.640]   even more RAM intensive, unfortunately.
[00:08:08.640 --> 00:08:14.440]   But I did finally combine it with what was called the B25-- BM25 index, which is kind
[00:08:14.440 --> 00:08:20.760]   of a more kind of vanilla search algorithm, similar to TF-IDF with a few slight variations.
[00:08:20.760 --> 00:08:24.720]   And one of the things I found is that when I combine that on the search abstracts with
[00:08:24.720 --> 00:08:31.320]   that and have it return an initial list of 20 results on the full 45,000 articles, and
[00:08:31.320 --> 00:08:38.560]   then reweight those results with semantic search, I did actually get more distinct clusters.
[00:08:38.560 --> 00:08:41.920]   So for instance, here's like coronavirus human to bat transmission.
[00:08:41.920 --> 00:08:46.160]   Cagle kind of cut off some of the edge there.
[00:08:46.160 --> 00:08:49.600]   And here's COVD19 person to person transmission.
[00:08:49.600 --> 00:08:53.800]   And though these aren't perfect, you can see there's kind of like-- these abstracts do
[00:08:53.800 --> 00:08:59.040]   like form, I guess, their own kind of distinct pattern in the embedding space.
[00:08:59.040 --> 00:09:04.040]   And there is some differentiation between the two, unlike the other one where they were
[00:09:04.040 --> 00:09:06.480]   just kind of overlapping.
[00:09:06.480 --> 00:09:08.120]   So that was kind of my first attempt.
[00:09:08.120 --> 00:09:11.020]   I came up with these conclusions and next steps.
[00:09:11.020 --> 00:09:17.880]   The one of the things I've looked at most recently was then fine tuning actually a sentence
[00:09:17.880 --> 00:09:27.760]   transformer model on MedNLI, which is essentially a-- well, it's a natural language inference
[00:09:27.760 --> 00:09:28.760]   data set.
[00:09:28.760 --> 00:09:34.280]   Oh, oops, that's not actually mine, which is a natural language inference data set,
[00:09:34.280 --> 00:09:40.280]   but it can be used to gauge how similar sentences are together based on the labels in that.
[00:09:40.280 --> 00:09:45.760]   So I fine tuned that as full sentence transformer model.
[00:09:45.760 --> 00:09:50.720]   And this model is actually nice because it produces full kind of sentence embeddings.
[00:09:50.720 --> 00:09:55.600]   I haven't done the full clustering analysis on it yet, but from what I've seen from the
[00:09:55.600 --> 00:10:05.720]   initial results, at least qualitatively on a few things like with, for instance, bat
[00:10:05.720 --> 00:10:11.040]   to human transmission and CAML to human transmission mechanism, it rates it like, for instance,
[00:10:11.040 --> 00:10:15.680]   a fairly high similarity score, which I think would be good.
[00:10:15.680 --> 00:10:19.840]   And then, for instance, if you're looking at treatment efficiency of chloroquine on
[00:10:19.840 --> 00:10:25.800]   COPD patients and bat to human transmission coronavirus, it rates it with a fairly lower
[00:10:25.800 --> 00:10:30.320]   similarity score, which we'd want because those are essentially two queries asking very
[00:10:30.320 --> 00:10:31.580]   different questions.
[00:10:31.580 --> 00:10:38.860]   What I've seen qualitatively just on this basic analysis is that it seems to be performing
[00:10:38.860 --> 00:10:41.320]   a lot better, I guess.
[00:10:41.320 --> 00:10:48.980]   Okay, I think that covers most of what I was going to go over.
[00:10:48.980 --> 00:10:53.200]   As I said, it's definitely an interesting project.
[00:10:53.200 --> 00:11:01.240]   And yeah, that was kind of a bit informal, but I was just asked, I think, two days ago
[00:11:01.240 --> 00:11:03.160]   or a day ago to prepare this.
[00:11:03.160 --> 00:11:06.360]   So hopefully it still made sense to people.
[00:11:06.360 --> 00:11:07.360]   Happy to answer any questions.
[00:11:07.360 --> 00:11:08.360]   This is really great.
[00:11:08.360 --> 00:11:09.360]   Thank you so much.
[00:11:09.360 --> 00:11:10.360]   I see questions coming in already.
[00:11:10.360 --> 00:11:11.360]   Can you see the chat?
[00:11:11.360 --> 00:11:12.360]   I'll pull it up.
[00:11:12.360 --> 00:11:26.440]   I might have to stop sharing the screen.
[00:11:26.440 --> 00:11:40.320]   This is the hardest part to figure out, if you're stopping the sharing of the screen.
[00:11:40.320 --> 00:11:41.320]   Okay.
[00:11:41.320 --> 00:11:42.320]   All right.
[00:11:42.320 --> 00:11:46.440]   Okay, so can we use UMAP for other things than you have used it?
[00:11:46.440 --> 00:11:52.280]   Yeah, I mean, I think, yeah, you can use UMAP for any type of clustering.
[00:11:52.280 --> 00:11:56.480]   So anytime you have embeddings or you want to do dimensionality reduction, you can use
[00:11:56.480 --> 00:11:57.480]   UMAP.
[00:11:57.480 --> 00:11:59.920]   It actually serves as kind of a good dimension.
[00:11:59.920 --> 00:12:03.880]   I was thinking of also actually using it, I guess, to maybe reduce the dimensionality
[00:12:03.880 --> 00:12:09.120]   of those 768 dimensional vectors to maybe take up a little less memory.
[00:12:09.120 --> 00:12:12.400]   But it's a good just dimensionality reduction technique in general.
[00:12:12.400 --> 00:12:17.160]   And yeah, I can definitely add some article links to it.
[00:12:17.160 --> 00:12:22.680]   I think I already linked to it in a couple places in my notebook.
[00:12:22.680 --> 00:12:32.160]   Yeah, that's a really good question about the RAM intensiveness.
[00:12:32.160 --> 00:12:34.360]   So yeah, these models are kind of hard at scale.
[00:12:34.360 --> 00:12:39.400]   So that's why I think most people do use some kind of initial search index where you return
[00:12:39.400 --> 00:12:45.200]   initial list of results before doing the kind of similarity scores, which is what I was
[00:12:45.200 --> 00:12:46.260]   looking at.
[00:12:46.260 --> 00:12:52.160]   There are ways, I guess, as I said, to maybe try to use UMAP to reduce the dimensions of
[00:12:52.160 --> 00:12:53.440]   the embedded text.
[00:12:53.440 --> 00:12:55.000]   So that can definitely help too.
[00:12:55.000 --> 00:12:59.840]   I haven't really studied that entirely at this point.
[00:12:59.840 --> 00:13:05.420]   But yeah, it definitely is a question between how much RAM and resources are available and
[00:13:05.420 --> 00:13:08.500]   then how good you want the search results to be.
[00:13:08.500 --> 00:13:12.960]   So that's one of those kind of real world trade-offs you have to weigh.
[00:13:12.960 --> 00:13:16.300]   Do we have any more questions?
[00:13:16.300 --> 00:13:20.060]   Isaac, do you want to tell us--
[00:13:20.060 --> 00:13:23.860]   Yeah, do you mind if I ask one verbally instead of in chat?
[00:13:23.860 --> 00:13:24.860]   Yeah, go ahead.
[00:13:24.860 --> 00:13:29.420]   Yeah, Isaac, could you tell us a little bit more about some of the stuff that the Corona
[00:13:29.420 --> 00:13:31.820]   Y group is up to?
[00:13:31.820 --> 00:13:33.380]   Yeah, sure.
[00:13:33.380 --> 00:13:40.060]   So actually, yeah, in Corona Y, it's kind of created four different main tasks, one
[00:13:40.060 --> 00:13:48.980]   focused on-- you can see them all on the Corona Y page one-- focused on geography and how
[00:13:48.980 --> 00:13:56.700]   geographic factors influence the virus, another focused on transmission specifically, another
[00:13:56.700 --> 00:14:04.940]   focused on vaccines and other therapeutics, and the fourth is on various risk factors
[00:14:04.940 --> 00:14:06.620]   associated with it.
[00:14:06.620 --> 00:14:13.460]   So yeah, there's those four kind of core tasks, which people are doing very specific kind
[00:14:13.460 --> 00:14:15.720]   of NLP efforts on.
[00:14:15.720 --> 00:14:22.740]   So for instance, on the geo section, they're extracting specifically named entities of
[00:14:22.740 --> 00:14:30.060]   locations and sub-level stuff about countries and then combining that with geographic data
[00:14:30.060 --> 00:14:37.020]   to look at how geography impacts it and it's read, at least from the literature.
[00:14:37.020 --> 00:14:41.740]   Then on, say, the vaccines and therapeutics, they're looking at specifically extracting
[00:14:41.740 --> 00:14:45.200]   the vaccine and therapeutics info.
[00:14:45.200 --> 00:14:48.900]   So yeah, there are kind of multiple efforts going on right now.
[00:14:48.900 --> 00:14:53.500]   I've been more focused on kind of the common effort, which is kind of to find general models
[00:14:53.500 --> 00:14:55.380]   that could work across all tasks.
[00:14:55.380 --> 00:14:58.140]   So that's where those kind of sentence embeddings come in.
[00:14:58.140 --> 00:15:03.220]   But yeah, it's kind of definitely an interesting group and a lot of cool things going on with
[00:15:03.220 --> 00:15:04.220]   it.
[00:15:04.220 --> 00:15:07.740]   Do you know what the link to the Slack community is?
[00:15:07.740 --> 00:15:11.220]   I can drop it in the chat if you give it to me.
[00:15:11.220 --> 00:15:12.220]   The link to the Slack?
[00:15:12.220 --> 00:15:16.700]   Yeah, I can send out-- yeah, I can get you a link.
[00:15:16.700 --> 00:15:17.700]   Gotcha.
[00:15:17.700 --> 00:15:19.940]   And then I also posted a link to our Slack community.
[00:15:19.940 --> 00:15:23.940]   I see two more questions, one from Money, one from Jonathan.
[00:15:23.940 --> 00:15:26.780]   Okay, yeah, sure.
[00:15:26.780 --> 00:15:37.100]   So Jonathan asks, have you considered visualizing any attention components for your transformers?
[00:15:37.100 --> 00:15:39.740]   Yeah, I think that could definitely be useful.
[00:15:39.740 --> 00:15:41.660]   I didn't do it too much in that notebook.
[00:15:41.660 --> 00:15:52.820]   But yeah, I think it would be useful to see which words are being kind of weighed in the
[00:15:52.820 --> 00:15:57.820]   language model when embedding the-- and when creating the embedding.
[00:15:57.820 --> 00:16:01.580]   So that definitely would be a good thing to do, see like which-- particularly which tokens
[00:16:01.580 --> 00:16:07.020]   and if it's attending to something like coronavirus or COVD-19 more, that would be helpful to
[00:16:07.020 --> 00:16:08.020]   know.
[00:16:08.020 --> 00:16:09.580]   But yeah, that would be a good next step too.
[00:16:09.580 --> 00:16:12.980]   So I have a question that piggybacks off of that.
[00:16:12.980 --> 00:16:17.780]   So I'm actually building this into VectorBytes right now, attention mechanisms, a way to
[00:16:17.780 --> 00:16:18.780]   visualize them.
[00:16:18.780 --> 00:16:22.660]   What are you using right now to visualize your attention?
[00:16:22.660 --> 00:16:26.180]   Or like for other projects, because you haven't used it in this.
[00:16:26.180 --> 00:16:31.820]   Yeah, so for attention right now, I kind of try to use heat maps and stuff between kind
[00:16:31.820 --> 00:16:37.260]   of the input, you know, whatever the input is and whatever the output sequence is.
[00:16:37.260 --> 00:16:40.140]   So I think that's the big one right now.
[00:16:40.140 --> 00:16:45.740]   I guess you could also look at specific context vectors and kind of visualizing those could
[00:16:45.740 --> 00:16:49.340]   also definitely be helpful.
[00:16:49.340 --> 00:16:55.700]   So are you using Plotly maps right now to visualize the heat maps?
[00:16:55.700 --> 00:16:58.900]   I actually haven't heard of them specifically.
[00:16:58.900 --> 00:17:03.900]   Right now I've kind of done some of my own embedding kind of visualizations of kind of
[00:17:03.900 --> 00:17:07.180]   the activations, but I might look into them.
[00:17:07.180 --> 00:17:12.660]   I haven't done too much into the actual kind of visualizations, but I think that could
[00:17:12.660 --> 00:17:14.580]   definitely help with interpretability.
[00:17:14.580 --> 00:17:15.580]   Cool.
[00:17:15.580 --> 00:17:20.140]   There was another question on dimensionality reduction.
[00:17:20.140 --> 00:17:22.580]   Would you translate it to feature selection?
[00:17:22.580 --> 00:17:23.580]   Is that right?
[00:17:23.580 --> 00:17:27.100]   Yeah, I mean, it's kind of related to that.
[00:17:27.100 --> 00:17:33.100]   It's basically just a UMAP, PCA, T-Sine, they all take kind of like a very high dimensional
[00:17:33.100 --> 00:17:38.100]   vector and then they try to find, you know, the parts of it that really stick out and
[00:17:38.100 --> 00:17:43.220]   like define it in the kind of embedding space and simple terms and then map it to that,
[00:17:43.220 --> 00:17:48.420]   those to map to the low dimensional embedding space.

