
[00:00:00.000 --> 00:00:03.880]   Please join me welcoming Vinod Khosla to stage.
[00:00:03.880 --> 00:00:05.280]   Vinod?
[00:00:05.280 --> 00:00:06.120]   Oh.
[00:00:06.120 --> 00:00:08.080]   (audience cheers)
[00:00:08.080 --> 00:00:08.920]   How are you?
[00:00:08.920 --> 00:00:09.740]   - How are you doing?
[00:00:09.740 --> 00:00:10.580]   - Legend.
[00:00:10.580 --> 00:00:11.420]   Legend.
[00:00:11.420 --> 00:00:14.000]   (upbeat music)
[00:00:14.000 --> 00:00:16.580]   (upbeat music)
[00:00:16.580 --> 00:00:32.360]   Wow.
[00:00:32.360 --> 00:00:33.600]   Legend, how are you?
[00:00:33.600 --> 00:00:34.440]   - Legend.
[00:00:34.440 --> 00:00:35.640]   - Vinod, good to see you.
[00:00:35.640 --> 00:00:38.280]   Just by way of background, I think we all know Vinod.
[00:00:38.280 --> 00:00:41.820]   Vinod is a legend and a friend.
[00:00:43.240 --> 00:00:46.760]   His Indian Army officer dad wanted his kid to join the army
[00:00:46.760 --> 00:00:49.760]   and Vinod decided instead to go into business
[00:00:49.760 --> 00:00:52.120]   after reading about Intel and getting inspired
[00:00:52.120 --> 00:00:55.680]   by co-founder Andy Grove, an immigrant who got funding
[00:00:55.680 --> 00:00:57.380]   for his startup in Silicon Valley.
[00:00:57.380 --> 00:01:00.960]   Vinod later got a master's in biomedical engineering
[00:01:00.960 --> 00:01:03.240]   from Carnegie Mellon on a full scholarship.
[00:01:03.240 --> 00:01:05.600]   Stanford GSB rejected him twice.
[00:01:05.600 --> 00:01:08.920]   He finally got accepted and received his MBA in 1980
[00:01:08.920 --> 00:01:11.880]   and he worked at several startups, some of which failed
[00:01:11.880 --> 00:01:14.360]   and then one of them worked out pretty good,
[00:01:14.360 --> 00:01:15.840]   Sun Microsystems.
[00:01:15.840 --> 00:01:19.160]   Started in 1982 with his Stanford classmates,
[00:01:19.160 --> 00:01:22.560]   Scott McNeely, Andy Bechtolstein and Bill Joy.
[00:01:22.560 --> 00:01:27.600]   Vinod raised $300,000 in seed capital from venture firm
[00:01:27.600 --> 00:01:29.920]   Kleiner Perkins, Caulfield and Byers
[00:01:29.920 --> 00:01:32.560]   and within five years, Sun made a billion dollars
[00:01:32.560 --> 00:01:33.960]   in annual revenue.
[00:01:33.960 --> 00:01:37.160]   He became a GP at Kleiner where he helped create NextGen
[00:01:37.160 --> 00:01:39.760]   which he sold to AMD for 28% of its market cap,
[00:01:39.760 --> 00:01:43.200]   the first successful Intel microprocessor clone company
[00:01:43.200 --> 00:01:45.640]   and his largest return to date,
[00:01:45.640 --> 00:01:48.020]   maybe we'll talk about this today, was Juniper Networks
[00:01:48.020 --> 00:01:50.580]   where the firm's $3 million investment in the 90s
[00:01:50.580 --> 00:01:51.620]   earned $7 billion.
[00:01:51.620 --> 00:01:56.980]   In 2004, he launched Kosella Ventures.
[00:01:56.980 --> 00:02:00.220]   2006, I pitched him and he rejected me
[00:02:00.220 --> 00:02:01.620]   as an investor in my company.
[00:02:01.620 --> 00:02:06.060]   I came back in 2011 and he led my Series B
[00:02:06.060 --> 00:02:08.560]   and I had three competing offers and I picked Vinod.
[00:02:08.560 --> 00:02:11.480]   So Vinod's been a great partner.
[00:02:11.480 --> 00:02:13.680]   By the way, I will also give you guys,
[00:02:13.680 --> 00:02:16.360]   I had offers from, I guess I'll say it,
[00:02:16.360 --> 00:02:19.200]   Founders Fund and Andreessen Horowitz for my Series B.
[00:02:19.200 --> 00:02:21.520]   Vinod gave me the lowest valuation
[00:02:21.520 --> 00:02:23.040]   and I still took his offer.
[00:02:23.040 --> 00:02:23.880]   - Entry price matters.
[00:02:23.880 --> 00:02:25.400]   - Did he add more value?
[00:02:25.400 --> 00:02:29.760]   - Vinod's whole point was VCs typically add negative value.
[00:02:29.760 --> 00:02:32.260]   We're gonna put a senator on your board that we know
[00:02:32.260 --> 00:02:34.600]   that knows this and that firm
[00:02:34.600 --> 00:02:36.180]   and the other board members they brought
[00:02:36.180 --> 00:02:38.200]   and the partners they brought to the table for me
[00:02:38.200 --> 00:02:39.280]   were incredible.
[00:02:39.280 --> 00:02:42.600]   And he's been a great mentor and advisor.
[00:02:42.600 --> 00:02:44.720]   I'll also say I always gave people advice on Vinod
[00:02:44.720 --> 00:02:45.960]   'cause I get a lot of reference calls like,
[00:02:45.960 --> 00:02:47.000]   should I take money from Vinod?
[00:02:47.000 --> 00:02:48.920]   I heard he's a pain in the ass sometimes.
[00:02:48.920 --> 00:02:51.360]   And I'm like, let me tell you something about Vinod.
[00:02:51.360 --> 00:02:53.520]   I've never told you this in public, but I said,
[00:02:53.520 --> 00:02:56.960]   the thing about Vinod is you go into a meeting with Vinod
[00:02:56.960 --> 00:03:00.160]   and he's like the Oracle in Neo, the Matrix.
[00:03:00.160 --> 00:03:02.200]   You go in the kitchen and he's like,
[00:03:02.200 --> 00:03:03.540]   she says, "Know thyself."
[00:03:03.540 --> 00:03:05.680]   She pulls out the chocolate chip cookies.
[00:03:05.680 --> 00:03:07.320]   She says, he says, "Am I the one?"
[00:03:07.320 --> 00:03:08.960]   She says, "You're not the one."
[00:03:08.960 --> 00:03:09.920]   Kicks him out.
[00:03:09.920 --> 00:03:12.840]   He has to ultimately decide what the right decision is.
[00:03:12.840 --> 00:03:16.120]   And Vinod will cast for you a vision of all the things
[00:03:16.120 --> 00:03:18.800]   that you can become and push you
[00:03:18.800 --> 00:03:21.360]   and try and make you extend your vision for your business
[00:03:21.360 --> 00:03:22.900]   and what is possible.
[00:03:22.900 --> 00:03:25.160]   And through that thought, through that conversation,
[00:03:25.160 --> 00:03:26.960]   through the visioneering that he can help you do
[00:03:26.960 --> 00:03:29.320]   as an entrepreneur, sometimes you need to listen
[00:03:29.320 --> 00:03:30.380]   and sometimes you need to ignore
[00:03:30.380 --> 00:03:32.240]   the bullshit advice he gives you.
[00:03:32.240 --> 00:03:36.360]   But I will say that the role he plays is really important
[00:03:36.360 --> 00:03:38.440]   and I've seen it work across some of the most
[00:03:38.440 --> 00:03:40.960]   important companies in Silicon Valley and the world today.
[00:03:40.960 --> 00:03:41.800]   So Vinod, thanks so much for being with us.
[00:03:41.800 --> 00:03:44.400]   - Can I just interrupt with one comment?
[00:03:44.400 --> 00:03:45.240]   - Yes.
[00:03:45.240 --> 00:03:47.300]   - You know, advice to all the entrepreneurs.
[00:03:47.300 --> 00:03:49.820]   Most VCs wanna be your friend.
[00:03:49.820 --> 00:03:53.960]   And so they are hypocritically polite
[00:03:53.960 --> 00:03:56.040]   instead of brutally honest in a way
[00:03:56.040 --> 00:03:59.160]   that can actually help you make a better decision.
[00:03:59.160 --> 00:04:01.520]   And that's sort of religion for me.
[00:04:01.520 --> 00:04:05.520]   I prefer brutal honesty to hypocritical politeness.
[00:04:05.520 --> 00:04:07.040]   And when people don't do that,
[00:04:07.040 --> 00:04:10.040]   they are actually harming the entrepreneur
[00:04:10.040 --> 00:04:12.200]   and having the entrepreneur like them more.
[00:04:12.200 --> 00:04:13.280]   That's not my game.
[00:04:13.280 --> 00:04:14.840]   - And is that what you mean when you say
[00:04:14.840 --> 00:04:16.740]   most VCs add negative value?
[00:04:16.740 --> 00:04:20.000]   - Well, they take a very short-term approach.
[00:04:20.000 --> 00:04:24.800]   Just going through a situation.
[00:04:24.800 --> 00:04:27.880]   Companies being recapped, they asked us to participate.
[00:04:27.880 --> 00:04:31.240]   I said, "You are hanging on to revenue
[00:04:31.240 --> 00:04:33.160]   "that is bad revenue.
[00:04:33.160 --> 00:04:37.220]   "If you get rid of it and restart the company
[00:04:37.220 --> 00:04:42.060]   "with positive revenue in the right technology approach,
[00:04:42.060 --> 00:04:45.320]   "we'll participate and even lead."
[00:04:45.320 --> 00:04:48.320]   But they hung on to revenue since the beginning of this year.
[00:04:48.320 --> 00:04:49.620]   By the way, three years ago,
[00:04:49.620 --> 00:04:51.900]   I asked them to take a technology approach
[00:04:51.900 --> 00:04:54.920]   as opposed to a people approach to the business.
[00:04:54.920 --> 00:04:57.600]   They did not because revenue came fast,
[00:04:57.600 --> 00:05:00.720]   the technology took time to develop.
[00:05:00.720 --> 00:05:02.280]   And they didn't invest in it.
[00:05:02.280 --> 00:05:04.280]   Now they're going back.
[00:05:04.280 --> 00:05:06.200]   I just got an email today saying
[00:05:06.200 --> 00:05:08.680]   they are happy to take our approach
[00:05:08.680 --> 00:05:12.160]   of really investing in the technology, lower burn rate,
[00:05:12.160 --> 00:05:16.200]   not try and show higher revenue so they can be acquired,
[00:05:16.200 --> 00:05:18.000]   instead build a company.
[00:05:18.000 --> 00:05:19.920]   That's fundamentally the kind of thing
[00:05:19.920 --> 00:05:22.360]   I run into constantly.
[00:05:22.360 --> 00:05:25.680]   - So I wanna start our conversation with AI.
[00:05:25.680 --> 00:05:27.280]   I'm gonna kick this off.
[00:05:27.280 --> 00:05:29.120]   You invested in open AI.
[00:05:29.120 --> 00:05:31.080]   You've been investing in AI technology
[00:05:31.080 --> 00:05:33.480]   as people are framing it today for some time.
[00:05:33.480 --> 00:05:35.400]   Can you share with us a little bit
[00:05:35.400 --> 00:05:37.760]   about how you became an investor in open AI
[00:05:37.760 --> 00:05:40.520]   in the context of the AI trajectory
[00:05:40.520 --> 00:05:42.640]   that you've been seeing for some time?
[00:05:42.640 --> 00:05:44.120]   And also maybe give us a little color
[00:05:44.120 --> 00:05:46.280]   'cause we haven't had this question answered on our show
[00:05:46.280 --> 00:05:48.960]   about how open AI went from non-profit to for-profit
[00:05:48.960 --> 00:05:50.800]   and what the future is for open AI.
[00:05:50.800 --> 00:05:53.760]   - Well, let me tell you a more general story first,
[00:05:53.760 --> 00:05:55.280]   starting with the internet.
[00:05:55.280 --> 00:05:58.680]   In 1982, when we started Sun,
[00:05:58.680 --> 00:06:00.440]   we bet on the internet.
[00:06:00.440 --> 00:06:04.360]   Only company to bet on the internet.
[00:06:04.360 --> 00:06:07.640]   1996, since you brought up Juniper,
[00:06:07.640 --> 00:06:11.920]   the internet was growing, but not yet clear.
[00:06:11.920 --> 00:06:15.920]   The senior management of every major telco
[00:06:15.920 --> 00:06:17.480]   in the United States told me
[00:06:17.480 --> 00:06:21.240]   they would never adapt TCP/IP for the internet.
[00:06:21.240 --> 00:06:26.760]   And I said, "Fine, we'll build Juniper to build TCP/IP."
[00:06:26.760 --> 00:06:29.720]   Cisco told me they would never do TCP/IP
[00:06:29.720 --> 00:06:34.240]   above what is probably your home service today, OC12.
[00:06:34.240 --> 00:06:35.080]   Never.
[00:06:35.080 --> 00:06:38.840]   And so we built TCP/IP.
[00:06:38.840 --> 00:06:39.680]   Why?
[00:06:39.680 --> 00:06:42.880]   Because we've seen the internet on an exponential
[00:06:42.880 --> 00:06:45.120]   and the flat part of the exponential
[00:06:45.120 --> 00:06:50.000]   looked very uninteresting and nobody believed in it.
[00:06:50.000 --> 00:06:54.200]   We invested in Juniper for the 2,500x return for us
[00:06:54.200 --> 00:06:57.680]   at Kleiner, one of the largest returns in venture,
[00:06:57.680 --> 00:07:00.480]   at least when Wall Street Journal did a piece
[00:07:00.480 --> 00:07:04.440]   on the Snap IPO of largest venture returns ever.
[00:07:04.440 --> 00:07:07.560]   But we believed in something and did that.
[00:07:07.560 --> 00:07:11.080]   How it relates to open AI?
[00:07:11.080 --> 00:07:16.080]   Probably year 2000, I gave an interview
[00:07:16.080 --> 00:07:18.400]   with the New York Times.
[00:07:18.400 --> 00:07:21.440]   My son just sent it to me, so I remember it.
[00:07:21.440 --> 00:07:24.120]   And I said, "AI will be powerful enough
[00:07:24.120 --> 00:07:29.120]   "we don't know when, but we'll have to redefine
[00:07:29.120 --> 00:07:32.120]   "what it means to be human."
[00:07:32.120 --> 00:07:35.200]   And I understand Stephen Wolfram
[00:07:35.200 --> 00:07:37.520]   had some interesting comments in that area,
[00:07:37.520 --> 00:07:38.960]   so happy to talk about it.
[00:07:38.960 --> 00:07:42.880]   10 years ago, I wrote two blogs.
[00:07:42.880 --> 00:07:46.200]   The first one was called Do We Need Doctors?
[00:07:46.200 --> 00:07:48.480]   With the premise that AI would replace
[00:07:48.480 --> 00:07:51.480]   the expertise of doctors and let them do
[00:07:51.480 --> 00:07:54.520]   the things humans do really well.
[00:07:54.520 --> 00:07:59.040]   So 20% doctor, 80% AI is the thing I defined.
[00:07:59.040 --> 00:08:03.080]   And I wrote another blog called Do We Need Teachers?
[00:08:03.080 --> 00:08:06.080]   Because the only way to scale education
[00:08:06.080 --> 00:08:09.560]   at an equal level for every kid was with AI.
[00:08:09.560 --> 00:08:14.560]   That was January of 2012, so about 12 years ago.
[00:08:14.560 --> 00:08:19.200]   So five years ago, late 2018 is when I started
[00:08:19.200 --> 00:08:22.160]   talking to Sam, the opportunity came up
[00:08:22.160 --> 00:08:25.120]   to do something with OpenAI.
[00:08:25.120 --> 00:08:27.280]   I said, "This is a clear bet.
[00:08:27.280 --> 00:08:29.920]   "I couldn't predict when things will emerge,
[00:08:29.920 --> 00:08:32.760]   "how much capability will happen, how fast."
[00:08:32.760 --> 00:08:35.360]   But you didn't need to know that.
[00:08:35.360 --> 00:08:39.360]   Whether it was in three years, five years, or 10 years,
[00:08:39.360 --> 00:08:43.320]   I was pretty convinced it would be pretty phenomenal
[00:08:43.320 --> 00:08:46.920]   and there'd be practical applications along the way
[00:08:46.920 --> 00:08:48.440]   where it would have a large impact
[00:08:48.440 --> 00:08:50.840]   long before we got to AGI.
[00:08:50.840 --> 00:08:53.840]   So we placed a bet, we placed a long-term bet.
[00:08:53.840 --> 00:08:57.840]   You know, this audience is gonna hear from Bob Mumgard.
[00:08:57.840 --> 00:09:01.120]   You know, five years ago I placed a bet on fusion
[00:09:01.120 --> 00:09:03.120]   for the same reason.
[00:09:03.120 --> 00:09:06.920]   If we get it to work, these are humongous returns,
[00:09:06.920 --> 00:09:10.440]   big bets, but reasonably high probability
[00:09:10.440 --> 00:09:13.640]   of success initially, but very high payback.
[00:09:13.640 --> 00:09:16.320]   And I was so convinced in OpenAI,
[00:09:16.320 --> 00:09:20.280]   I placed twice, our initial investment
[00:09:20.280 --> 00:09:23.960]   was twice the largest previous investment I'd made
[00:09:23.960 --> 00:09:26.800]   in 40 years in venture capital.
[00:09:26.800 --> 00:09:29.240]   Two X, the largest bet I'd ever placed
[00:09:29.240 --> 00:09:31.040]   initially in a company.
[00:09:31.040 --> 00:09:33.400]   - How did that go from non-profit to for-profit
[00:09:33.400 --> 00:09:34.760]   and raise venture money?
[00:09:34.760 --> 00:09:38.760]   - Well, you know, those are details.
[00:09:38.760 --> 00:09:41.280]   (audience laughing)
[00:09:41.280 --> 00:09:43.440]   But I would say the following.
[00:09:43.440 --> 00:09:48.440]   There are plenty of non-profits in Europe
[00:09:48.440 --> 00:09:52.920]   that have for-profit arms, right?
[00:09:52.920 --> 00:09:57.480]   It's a very good model to sustain and generate profit
[00:09:57.480 --> 00:09:59.560]   for a non-profit to scale.
[00:09:59.560 --> 00:10:01.200]   Like I think the European model,
[00:10:01.200 --> 00:10:04.400]   some of these examples are really, really great.
[00:10:04.400 --> 00:10:08.640]   IKEA, Bosch, they have very large non-profit ownership.
[00:10:08.640 --> 00:10:12.120]   And I think that's a very good model to scale.
[00:10:12.120 --> 00:10:14.240]   I mean, I just mentioned, do we need doctors?
[00:10:14.240 --> 00:10:18.240]   My son's building a primary care doctor AI.
[00:10:18.240 --> 00:10:23.240]   My wife's doing a primary care or tutor AI as a non-profit.
[00:10:23.240 --> 00:10:26.520]   It really doesn't matter how you do it.
[00:10:26.520 --> 00:10:29.680]   The question is, how can you gather the resources
[00:10:29.680 --> 00:10:32.480]   to scale the social good you're trying to do,
[00:10:32.480 --> 00:10:35.600]   whether with a doctor or with a tutor
[00:10:35.600 --> 00:10:37.240]   for every kid on the planet?
[00:10:37.240 --> 00:10:39.760]   - There's a lot of discussion about
[00:10:39.760 --> 00:10:43.160]   how jobs might be displaced because of AI.
[00:10:43.160 --> 00:10:45.720]   You've seen, I guess, three or four
[00:10:45.720 --> 00:10:50.540]   of these amazing efficiency cycles.
[00:10:50.540 --> 00:10:51.520]   What's your take on that?
[00:10:51.520 --> 00:10:55.360]   Because this one does feel qualitatively different,
[00:10:55.360 --> 00:10:57.480]   and it does seem to be moving at a faster pace.
[00:10:57.480 --> 00:11:00.120]   So the speed at which AI is moving,
[00:11:00.120 --> 00:11:02.080]   and just the leaps and bounds
[00:11:02.080 --> 00:11:04.000]   of how it's making people augmented,
[00:11:04.000 --> 00:11:07.640]   you're saying 80% AI, 20% doctor.
[00:11:07.640 --> 00:11:09.080]   What do you think this does to jobs,
[00:11:09.080 --> 00:11:11.200]   and do you have those concerns of displacement
[00:11:11.200 --> 00:11:13.800]   that a lot of people seem to be concerned about?
[00:11:13.800 --> 00:11:15.560]   - I think it's a genuine concern.
[00:11:15.560 --> 00:11:16.400]   - Okay.
[00:11:16.400 --> 00:11:20.520]   - The path to getting to that point.
[00:11:20.520 --> 00:11:24.840]   So let me start with that 20, 25 years from now.
[00:11:24.840 --> 00:11:27.240]   I think the need to work will disappear.
[00:11:27.240 --> 00:11:30.640]   People will work because they want to work,
[00:11:30.640 --> 00:11:32.520]   not because they need to work.
[00:11:32.520 --> 00:11:35.680]   And nobody wants a job doing the same thing
[00:11:35.680 --> 00:11:38.680]   for 30 years on an assembly line at GM.
[00:11:38.680 --> 00:11:41.960]   There's a set of jobs nobody wants to do
[00:11:41.960 --> 00:11:45.560]   that we shouldn't really call jobs.
[00:11:45.560 --> 00:11:47.240]   They're absolute--
[00:11:47.240 --> 00:11:49.640]   - Treachery.
[00:11:49.640 --> 00:11:50.880]   - Yeah.
[00:11:50.880 --> 00:11:53.160]   So hopefully those disappear.
[00:11:53.160 --> 00:11:57.040]   I love my job, so 25 years from now, health permitting,
[00:11:57.040 --> 00:11:59.960]   I'll still be doing the same thing, likely.
[00:11:59.960 --> 00:12:02.520]   I'm 68, so, you know,
[00:12:02.520 --> 00:12:06.160]   and Warren Buffett's gone a lot longer than I have.
[00:12:06.160 --> 00:12:08.180]   I'm very passionate about what I do.
[00:12:08.180 --> 00:12:10.440]   I enjoy it so much, I do it for free,
[00:12:10.440 --> 00:12:12.960]   and I'll keep doing it, and other people will.
[00:12:12.960 --> 00:12:15.800]   There will be new jobs.
[00:12:15.800 --> 00:12:20.000]   What you call jobs are what people get paid for.
[00:12:20.000 --> 00:12:23.480]   So it's X Games, part-spend a job.
[00:12:23.480 --> 00:12:25.520]   I don't know, I don't call it a job,
[00:12:25.520 --> 00:12:27.440]   but it is a passion that can pay.
[00:12:27.440 --> 00:12:33.440]   So I do think the nature of jobs will disappear.
[00:12:33.440 --> 00:12:38.440]   Now societies will have the transition from today
[00:12:38.440 --> 00:12:42.020]   to that vision of not needing to work if you don't want to
[00:12:42.020 --> 00:12:46.700]   will be very messy, very political,
[00:12:46.700 --> 00:12:51.580]   very, I would say, controversial.
[00:12:51.580 --> 00:12:54.660]   And I think some countries
[00:12:54.660 --> 00:12:57.720]   will move much faster than others.
[00:12:58.760 --> 00:13:03.760]   I suspect, let's take something I'm very hawkish on, China.
[00:13:03.760 --> 00:13:06.880]   They will take a Tiananmen Square tactic
[00:13:06.880 --> 00:13:10.440]   to enforcing what they think is right for their society
[00:13:10.440 --> 00:13:12.680]   over a 25-year period.
[00:13:12.680 --> 00:13:14.560]   We will have a political process,
[00:13:14.560 --> 00:13:17.720]   which I prefer to the Chinese style.
[00:13:17.720 --> 00:13:20.080]   - Like the debate over the tanks.
[00:13:20.080 --> 00:13:22.280]   - Well, debate always improves things,
[00:13:22.280 --> 00:13:24.000]   but it also does slow down.
[00:13:24.000 --> 00:13:29.000]   And certain societies may choose not to adapt these.
[00:13:29.000 --> 00:13:33.260]   And I think the societies that do that will fall behind.
[00:13:33.260 --> 00:13:38.060]   And I think that's a massive disadvantage
[00:13:38.060 --> 00:13:39.700]   for our political system.
[00:13:39.700 --> 00:13:41.180]   We may choose not to do it.
[00:13:41.180 --> 00:13:43.820]   We've done it before.
[00:13:43.820 --> 00:13:46.700]   But we've also seen massive transitions before.
[00:13:46.700 --> 00:13:50.820]   Agriculture was 50% of US employment in 1900.
[00:13:50.820 --> 00:13:53.700]   By 1970, it was 4%.
[00:13:53.700 --> 00:13:55.900]   So most jobs in that area,
[00:13:55.900 --> 00:13:59.060]   the biggest area of employment, got displaced.
[00:13:59.060 --> 00:14:01.020]   We did manage the transition.
[00:14:01.020 --> 00:14:03.140]   - Well, cost went down and productivity went up.
[00:14:03.140 --> 00:14:04.500]   - Yeah.
[00:14:04.500 --> 00:14:08.220]   So I do think we will have to rethink.
[00:14:08.220 --> 00:14:09.940]   So the one thing I would say is
[00:14:09.940 --> 00:14:14.620]   capitalism is by permission of democracy,
[00:14:14.620 --> 00:14:17.640]   and it can be revoked, that permission.
[00:14:17.640 --> 00:14:20.220]   And I worry that
[00:14:22.660 --> 00:14:27.100]   this messy transition to the vision I'm very excited about
[00:14:27.100 --> 00:14:30.300]   may cause us to change systems.
[00:14:30.300 --> 00:14:33.700]   I do think the nature of capitalism will have to change.
[00:14:33.700 --> 00:14:36.060]   I'm a huge fan of capitalism.
[00:14:36.060 --> 00:14:37.560]   I'm a capitalist.
[00:14:37.560 --> 00:14:41.740]   But I do think the set of things you optimize for,
[00:14:41.740 --> 00:14:46.500]   when economic efficiency isn't the only criteria,
[00:14:46.500 --> 00:14:48.860]   will have to be increased.
[00:14:48.860 --> 00:14:51.220]   So disparity is one that will be increased.
[00:14:51.220 --> 00:14:56.220]   I first wrote a piece of 2014 in Forbes.
[00:14:56.220 --> 00:15:01.200]   It was a long piece that said AI will cause great abundance,
[00:15:01.200 --> 00:15:04.180]   great GDP growth, great productivity growth,
[00:15:04.180 --> 00:15:06.380]   all the things economists measure,
[00:15:06.380 --> 00:15:09.060]   and increasing income disparity.
[00:15:09.060 --> 00:15:10.740]   That was 2014.
[00:15:10.740 --> 00:15:14.560]   And I think I ended that to close this answer off
[00:15:14.560 --> 00:15:18.980]   with we will have to consider UBI in some form.
[00:15:18.980 --> 00:15:22.260]   And I do think that will be a solution,
[00:15:22.260 --> 00:15:24.060]   but meaning for human beings
[00:15:24.060 --> 00:15:26.100]   will be the largest problem we face.
[00:15:26.100 --> 00:15:28.980]   - Which was my follow-up, which is if you give people UBI,
[00:15:28.980 --> 00:15:33.980]   do you have concerns of the idle mind and what happens?
[00:15:33.980 --> 00:15:39.160]   We've seen in other states when 25% of young men
[00:15:39.160 --> 00:15:40.560]   maybe don't have jobs,
[00:15:40.560 --> 00:15:42.380]   their minds wander and bad things happen.
[00:15:42.380 --> 00:15:46.300]   And so what do you think the societal issues will be
[00:15:46.300 --> 00:15:48.040]   if we do offer UBI?
[00:15:49.040 --> 00:15:51.800]   - I think we will have issues,
[00:15:51.800 --> 00:15:55.640]   but we will eliminate other issues.
[00:15:55.640 --> 00:15:58.080]   There's pros and cons to everything.
[00:15:58.080 --> 00:16:01.860]   It's very hard to predict how complex systems behave.
[00:16:01.860 --> 00:16:05.440]   And it's very hard for me to sit here and say,
[00:16:05.440 --> 00:16:08.240]   I have a clear vision of how that happens.
[00:16:08.240 --> 00:16:12.080]   But I, for one, am addicted to learning.
[00:16:12.080 --> 00:16:15.440]   I know other people addicted to producing music.
[00:16:16.400 --> 00:16:18.640]   All of those are reasonable things
[00:16:18.640 --> 00:16:21.820]   for people to find meaning in, or personal meaning.
[00:16:21.820 --> 00:16:25.200]   Yeah, there's others, like I said,
[00:16:25.200 --> 00:16:30.200]   just wanna perfect their participation in a certain sport.
[00:16:30.200 --> 00:16:33.720]   I love X Games because it wasn't even remotely
[00:16:33.720 --> 00:16:36.140]   considered a sport not that long ago.
[00:16:36.140 --> 00:16:39.560]   So things, passions will emerge.
[00:16:39.560 --> 00:16:43.000]   And I think if people have very early in their life
[00:16:43.000 --> 00:16:44.900]   the ability to pursue passions,
[00:16:44.900 --> 00:16:48.920]   I hope that adds the meaning human beings will need.
[00:16:48.920 --> 00:16:51.920]   And in some sense, that's what I was talking about in 2000
[00:16:51.920 --> 00:16:54.560]   when I said we'll have to redefine
[00:16:54.560 --> 00:16:57.280]   what it means to be human.
[00:16:57.280 --> 00:17:01.960]   Your assembly line job won't define you for your whole life.
[00:17:01.960 --> 00:17:04.760]   That'd be a terrible thing.
[00:17:04.760 --> 00:17:05.600]   Yeah.
[00:17:05.600 --> 00:17:07.840]   - In the AI landscape,
[00:17:07.840 --> 00:17:10.440]   just going back to that for a second,
[00:17:10.440 --> 00:17:12.200]   you have a lot of these big companies
[00:17:12.200 --> 00:17:13.960]   who seem to have been caught flat-footed,
[00:17:13.960 --> 00:17:15.380]   so they're acting pretty aggressively.
[00:17:15.380 --> 00:17:18.340]   I think even yesterday there was a leak of something
[00:17:18.340 --> 00:17:21.000]   where Meta's trying to really superpower Llama 2
[00:17:21.000 --> 00:17:23.260]   and then create some high-performance cloud
[00:17:23.260 --> 00:17:25.740]   that they effectively wanna give away.
[00:17:25.740 --> 00:17:27.520]   We talked about this a little bit on the pod,
[00:17:27.520 --> 00:17:29.560]   but the idea seems like the big companies
[00:17:29.560 --> 00:17:33.040]   wanna scorch the earth on the foundational model side.
[00:17:33.040 --> 00:17:34.380]   Then you have some other big companies
[00:17:34.380 --> 00:17:35.680]   that are working on silicon.
[00:17:35.680 --> 00:17:38.180]   So can you just walk us through in your mind
[00:17:38.180 --> 00:17:41.540]   how you've organized, where the value's getting created,
[00:17:41.540 --> 00:17:43.260]   where there'll be product value,
[00:17:43.260 --> 00:17:45.340]   but no economic value, et cetera, et cetera?
[00:17:45.340 --> 00:17:48.120]   - Yeah, look, going back to open AI,
[00:17:48.120 --> 00:17:51.400]   part of my interest in funding open AI
[00:17:51.400 --> 00:17:54.780]   was when we looked at it, when I looked at it,
[00:17:54.780 --> 00:17:57.900]   there was two major centers of excellence in AI.
[00:17:57.900 --> 00:17:58.900]   There was Google.
[00:17:58.900 --> 00:17:59.740]   - DeepMind.
[00:17:59.740 --> 00:18:02.700]   - They had DeepMind and Google Brain, but Google.
[00:18:02.700 --> 00:18:05.380]   And then there was an effort at Baidu.
[00:18:05.380 --> 00:18:08.740]   And I was very worried the Chinese might win that.
[00:18:09.820 --> 00:18:11.660]   I'm hawkish on China.
[00:18:11.660 --> 00:18:14.660]   I have no trust there.
[00:18:14.660 --> 00:18:18.660]   And I love Graham Allison's book, "Destined for War."
[00:18:18.660 --> 00:18:20.900]   I'm a complete believer in that thesis.
[00:18:20.900 --> 00:18:27.700]   But I thought it'd be valuable to create a third center,
[00:18:27.700 --> 00:18:32.180]   and that was part of the motivation in funding open AI.
[00:18:32.180 --> 00:18:34.900]   I'm really glad Meta's doing it,
[00:18:34.900 --> 00:18:36.820]   and others are trying to do it.
[00:18:36.820 --> 00:18:39.700]   And you know, there's efforts in every country,
[00:18:39.700 --> 00:18:42.180]   every country wants their national AI.
[00:18:42.180 --> 00:18:43.700]   I think more competition,
[00:18:43.700 --> 00:18:46.420]   especially in the set of countries
[00:18:46.420 --> 00:18:51.260]   subscribing to Western values, is good.
[00:18:51.260 --> 00:18:53.200]   And I think that will sort out.
[00:18:53.200 --> 00:18:55.220]   But I would say the following.
[00:18:55.220 --> 00:19:00.220]   Almost all the efforts I have seen are very limited.
[00:19:00.220 --> 00:19:03.820]   They're all trying to scale LLMs.
[00:19:03.820 --> 00:19:06.780]   And you know, I spend my time looking at
[00:19:06.780 --> 00:19:11.780]   what besides LLM will play an important role.
[00:19:11.780 --> 00:19:15.460]   I haven't seen one effort among the majors
[00:19:15.460 --> 00:19:17.980]   that isn't following the same model.
[00:19:17.980 --> 00:19:22.980]   More NVIDIA GPUs, scale the parameters, more data,
[00:19:22.980 --> 00:19:28.140]   instead of saying, are there other methods?
[00:19:28.140 --> 00:19:29.660]   So I'll mention, you know,
[00:19:29.660 --> 00:19:32.780]   we made an investment in a symbolic logic idea.
[00:19:32.780 --> 00:19:35.220]   Would that be dramatically additive?
[00:19:35.220 --> 00:19:37.980]   And in, you know, just like open AI
[00:19:37.980 --> 00:19:40.580]   didn't look that interesting for five years ago,
[00:19:40.580 --> 00:19:45.580]   2018, late '18 is when we really made our decision to invest.
[00:19:45.580 --> 00:19:48.940]   It took a while to get organized.
[00:19:48.940 --> 00:19:54.780]   But I'm saying, what else would look like that in 2028?
[00:19:54.780 --> 00:19:55.740]   - Yeah.
[00:19:55.740 --> 00:19:57.740]   - Yeah, would it be symbolic logic?
[00:19:57.740 --> 00:20:00.140]   I'll invest in anybody doing that.
[00:20:00.140 --> 00:20:01.580]   - Are you worried about-- - Is it probabilistic
[00:20:01.580 --> 00:20:05.300]   programming, is it completely other ways,
[00:20:05.300 --> 00:20:07.420]   like multi-agent systems?
[00:20:07.420 --> 00:20:11.420]   Is there are going to be other axes
[00:20:11.420 --> 00:20:12.900]   in which we will see progress?
[00:20:12.900 --> 00:20:14.540]   - Said differently, do you need to make sure
[00:20:14.540 --> 00:20:16.820]   there are other axes so that we're not looking back
[00:20:16.820 --> 00:20:19.900]   a few years from now and there's just massive CUDA lock-in
[00:20:19.900 --> 00:20:22.980]   and nothing can happen without one gatekeeper?
[00:20:22.980 --> 00:20:24.060]   Is that-- - Well, the great thing
[00:20:24.060 --> 00:20:26.140]   about venture and venture capitalists
[00:20:26.140 --> 00:20:28.260]   is they like to place long bets.
[00:20:28.260 --> 00:20:32.180]   So I'll place all those long bets on alternative ways
[00:20:32.180 --> 00:20:36.100]   to learn and not replace, but add to LLMs,
[00:20:36.100 --> 00:20:39.140]   I think is the more likely scenario, though, who knows?
[00:20:39.140 --> 00:20:39.980]   - Yeah.
[00:20:39.980 --> 00:20:46.180]   - But even among just, imagine LLMs do it all.
[00:20:46.180 --> 00:20:50.020]   It's good that there's multiple players.
[00:20:50.020 --> 00:20:53.020]   And so more big companies pop up, great.
[00:20:53.020 --> 00:20:57.900]   I also do think governments will have their own efforts.
[00:20:57.900 --> 00:21:02.500]   China, of course, has one, but plenty of other rumblings
[00:21:02.500 --> 00:21:05.020]   of governments trying to put together
[00:21:05.020 --> 00:21:07.140]   an effort in my country.
[00:21:07.140 --> 00:21:09.100]   - Tell us the-- - And that's all good.
[00:21:09.100 --> 00:21:11.260]   - Tell us, up-leveling away from just AI,
[00:21:11.260 --> 00:21:14.580]   but broadly speaking venture, give us the state of the union
[00:21:14.580 --> 00:21:17.020]   of venture capital in September '23.
[00:21:17.020 --> 00:21:20.540]   - So let me give you a perspective.
[00:21:20.540 --> 00:21:23.980]   I've been doing venture for some 40 years.
[00:21:25.140 --> 00:21:29.140]   In that time, I have not seen,
[00:21:29.140 --> 00:21:30.900]   and people get surprised at this,
[00:21:30.900 --> 00:21:34.580]   one example of a large innovation
[00:21:34.580 --> 00:21:37.260]   that came from a large company or institution.
[00:21:37.260 --> 00:21:42.300]   So the only thing you can do, not one.
[00:21:42.300 --> 00:21:44.580]   (audience applauding)
[00:21:44.580 --> 00:21:49.100]   The only two examples I could find was in the early '70s,
[00:21:49.100 --> 00:21:51.300]   which was, if you go back 50 years,
[00:21:51.300 --> 00:21:56.260]   when I was, Bank of America put debit and credit
[00:21:56.260 --> 00:21:57.420]   on a plastic card.
[00:21:57.420 --> 00:21:59.660]   Like, that wasn't even a major innovation,
[00:21:59.660 --> 00:22:01.700]   but I'll give them that.
[00:22:01.700 --> 00:22:04.300]   But I couldn't think of examples like that.
[00:22:04.300 --> 00:22:06.060]   - Apple with the iPhone, maybe?
[00:22:06.060 --> 00:22:08.340]   - That was Steve Jobs.
[00:22:08.340 --> 00:22:09.980]   I think it was a founder-led,
[00:22:09.980 --> 00:22:12.340]   I should have said founder-led innovation.
[00:22:12.340 --> 00:22:13.180]   - Okay, got it.
[00:22:13.180 --> 00:22:15.460]   - Is a more accurate description.
[00:22:15.460 --> 00:22:16.300]   - I like that one.
[00:22:16.300 --> 00:22:19.380]   - You know, did General Motors do driverless cars,
[00:22:19.380 --> 00:22:21.020]   or Waymo?
[00:22:21.020 --> 00:22:26.020]   Did, would somebody at Avis do Uber?
[00:22:26.020 --> 00:22:31.860]   Would somebody at Hilton do Airbnb?
[00:22:31.860 --> 00:22:35.940]   Would somebody, and Elon's speaking next,
[00:22:35.940 --> 00:22:40.900]   but somebody at Lockheed Martin do SpaceX or Rocket Lab?
[00:22:40.900 --> 00:22:43.860]   We're investors in Rocket Lab, so I like it a little more.
[00:22:43.860 --> 00:22:48.140]   They do have a superior strategy,
[00:22:48.140 --> 00:22:51.020]   but SpaceX has done a great job.
[00:22:51.020 --> 00:22:57.460]   But think, think of every large social change.
[00:22:57.460 --> 00:23:02.540]   Is there a large company that played?
[00:23:02.540 --> 00:23:04.420]   You know, the only thing somebody came up with
[00:23:04.420 --> 00:23:09.420]   was debt structuring that the bankers did.
[00:23:09.420 --> 00:23:14.100]   And I said, they didn't take any risk, right?
[00:23:14.100 --> 00:23:14.940]   - No innovation.
[00:23:14.940 --> 00:23:16.780]   - They took a risk with somebody else's money.
[00:23:16.780 --> 00:23:19.020]   Other people's money, easy to play with,
[00:23:19.020 --> 00:23:22.740]   whether you're in finance or in government.
[00:23:22.740 --> 00:23:24.380]   Other people's money is easy.
[00:23:24.380 --> 00:23:29.500]   But my idea is innovation comes from groups like this.
[00:23:29.500 --> 00:23:33.180]   And I'm completely convinced, I lost the original question,
[00:23:33.180 --> 00:23:35.020]   but this is where innovation will come from.
[00:23:35.020 --> 00:23:35.860]   - State of venture capital.
[00:23:35.860 --> 00:23:36.700]   - What's the state of venture capital?
[00:23:36.700 --> 00:23:37.540]   - I can ask Bob on that.
[00:23:37.540 --> 00:23:39.660]   - Yeah, so any large area,
[00:23:39.660 --> 00:23:41.420]   you'll be hearing from Bob Mumgard,
[00:23:41.420 --> 00:23:44.700]   you know, four or five years ago invested in Fusion.
[00:23:44.700 --> 00:23:47.940]   Now we may fail, I think we are much more likely
[00:23:47.940 --> 00:23:52.660]   to succeed sitting here today than fail, but it's possible.
[00:23:52.660 --> 00:23:56.020]   - Can you talk about that just while you're on Fusion?
[00:23:56.020 --> 00:23:59.340]   We've got both Bob and David from Helion tomorrow morning.
[00:23:59.340 --> 00:24:02.900]   And there's broadly, call it six or some number
[00:24:02.900 --> 00:24:06.180]   of general architectural approaches to solving the--
[00:24:06.180 --> 00:24:08.900]   - Yeah, there's six approaches, a dozen companies.
[00:24:08.900 --> 00:24:10.020]   I think it's--
[00:24:10.020 --> 00:24:11.860]   - Well, now there's 70, but yeah.
[00:24:11.860 --> 00:24:12.820]   I mean, it's just ballooning.
[00:24:12.820 --> 00:24:16.100]   But how do you think about the role that a Commonwealth
[00:24:16.100 --> 00:24:17.740]   or even an open AI, I guess,
[00:24:17.740 --> 00:24:22.460]   how big of a role does capital play in getting there?
[00:24:22.460 --> 00:24:24.580]   'Cause what makes Commonwealth different
[00:24:24.580 --> 00:24:26.180]   is how much capital they've raised,
[00:24:26.180 --> 00:24:29.820]   billion dollar plus at this point, open AI similar.
[00:24:29.820 --> 00:24:34.260]   The decision as an investor,
[00:24:34.260 --> 00:24:36.620]   how much of it is predicated on this team
[00:24:36.620 --> 00:24:37.460]   or this architecture getting it right?
[00:24:37.460 --> 00:24:39.140]   - So let me tell you the Commonwealth story
[00:24:39.140 --> 00:24:41.060]   before Bob has a chance to tell you.
[00:24:41.060 --> 00:24:41.900]   - Is he here by the way?
[00:24:41.900 --> 00:24:42.900]   - Yeah, yeah.
[00:24:42.900 --> 00:24:45.140]   - Yeah, you got it.
[00:24:45.140 --> 00:24:46.500]   - I'm sure he's here somewhere
[00:24:46.500 --> 00:24:48.300]   because I'm meeting him in half an hour.
[00:24:48.300 --> 00:24:49.140]   - Oh, yeah, you must be.
[00:24:49.140 --> 00:24:49.980]   (audience laughing)
[00:24:49.980 --> 00:24:50.820]   - Backstage.
[00:24:50.820 --> 00:24:54.540]   But here's the thing.
[00:24:54.540 --> 00:24:56.900]   They may have raised a lot of money.
[00:24:56.900 --> 00:25:00.460]   It took double digit millions to handle
[00:25:00.460 --> 00:25:04.740]   the single biggest risk in building the architecture, right?
[00:25:04.740 --> 00:25:07.100]   Which was, can you build a 20 Tesla magnet?
[00:25:07.100 --> 00:25:07.940]   - Yeah, the magnet.
[00:25:07.940 --> 00:25:08.780]   - Right?
[00:25:08.780 --> 00:25:11.140]   And Bob and I had this discussion.
[00:25:11.140 --> 00:25:15.100]   I said, if fusion doesn't work,
[00:25:15.100 --> 00:25:17.140]   but you can build a 20 Tesla magnet,
[00:25:17.140 --> 00:25:20.900]   we got lots of interesting applications in nuclear medicine.
[00:25:20.900 --> 00:25:21.740]   - Yeah.
[00:25:21.740 --> 00:25:24.940]   - You know, a fusion reactor is a particle accelerator.
[00:25:24.940 --> 00:25:26.940]   - That technical breakthrough, we can pivot somewhere else.
[00:25:26.940 --> 00:25:30.900]   - Yeah, nuclear medicine, MRI machines, lots of areas.
[00:25:30.900 --> 00:25:33.740]   So let's go build with double digit millions,
[00:25:33.740 --> 00:25:37.780]   which is not a huge amount in the context of the upside
[00:25:37.780 --> 00:25:42.500]   of maybe a bigger upside than the 2,500 X Juniper had.
[00:25:42.500 --> 00:25:44.540]   If you solve the fusion problem,
[00:25:44.540 --> 00:25:46.020]   there's still a 10 X upside
[00:25:46.020 --> 00:25:48.820]   if you just solve the MRI machine problem.
[00:25:48.820 --> 00:25:50.460]   - So you're thinking of probabilities.
[00:25:50.460 --> 00:25:54.780]   - Well, today we are not, but when we started,
[00:25:54.780 --> 00:25:58.060]   that was clearly to look at off ramps, right?
[00:25:58.060 --> 00:26:00.220]   Like where else could we go
[00:26:00.220 --> 00:26:02.180]   if we only had double digit millions?
[00:26:02.180 --> 00:26:04.080]   We built the magnet.
[00:26:04.080 --> 00:26:06.100]   If we didn't build the magnet, we failed.
[00:26:06.100 --> 00:26:07.320]   That's venture capital.
[00:26:07.320 --> 00:26:10.140]   - So how do you know that that bet, that investment,
[00:26:10.140 --> 00:26:12.220]   at this point, is still a good investment,
[00:26:12.220 --> 00:26:13.820]   or you keep putting more capital in,
[00:26:13.820 --> 00:26:15.720]   if there's now six competing architectures,
[00:26:15.720 --> 00:26:17.540]   any one of which could have their own breakthrough,
[00:26:17.540 --> 00:26:19.620]   that could yield better economics?
[00:26:19.620 --> 00:26:22.300]   - So we've continued to invest in it,
[00:26:22.300 --> 00:26:26.140]   mostly because I think Bob's just a phenomenal CEO.
[00:26:26.140 --> 00:26:29.140]   And if anybody can make this happen,
[00:26:29.140 --> 00:26:33.460]   and the world needs it to happen, Bob can.
[00:26:33.460 --> 00:26:35.980]   But I'm also optimistic, and as, you know,
[00:26:35.980 --> 00:26:39.480]   we invest in Sam too, he invested in Helion.
[00:26:39.480 --> 00:26:41.120]   I think there's more than one approach
[00:26:41.120 --> 00:26:42.440]   that could be successful.
[00:26:42.440 --> 00:26:45.440]   In fact, I guess 15 years from now,
[00:26:45.440 --> 00:26:46.600]   we'll be looking at one,
[00:26:46.600 --> 00:26:48.600]   more than one company that's successful.
[00:26:48.600 --> 00:26:50.040]   - Sax, you had a question I wanted to make sure
[00:26:50.040 --> 00:26:50.880]   you got to ask.
[00:26:50.880 --> 00:26:53.000]   - Let's let Vidhut finish that.
[00:26:53.000 --> 00:26:55.400]   - Okay, I thought it was finished, yeah.
[00:26:55.400 --> 00:27:00.040]   So my view is large problems like that,
[00:27:00.040 --> 00:27:03.000]   you'd think GE would solve, not a chance.
[00:27:03.000 --> 00:27:03.840]   - Not a chance.
[00:27:03.840 --> 00:27:04.660]   - Right.
[00:27:04.660 --> 00:27:05.820]   - Not a chance, not a chance.
[00:27:05.820 --> 00:27:06.660]   - Yeah.
[00:27:06.660 --> 00:27:09.340]   - You know, you take any of these,
[00:27:09.340 --> 00:27:12.860]   one of my favorite, you take large problems.
[00:27:12.860 --> 00:27:16.640]   One of my biggest beefs is the traffic in cities.
[00:27:16.640 --> 00:27:19.420]   I actually think we are very much on track
[00:27:19.420 --> 00:27:23.540]   to replace most cars in most cities in 25 years.
[00:27:23.540 --> 00:27:24.380]   How?
[00:27:24.380 --> 00:27:27.540]   By building a different kind of public transit system.
[00:27:27.540 --> 00:27:29.000]   Anybody wanna invest in it?
[00:27:29.000 --> 00:27:30.620]   I'm really excited about it.
[00:27:32.620 --> 00:27:37.060]   You know, Waymo went the way of autonomous cars.
[00:27:37.060 --> 00:27:40.980]   For consumers or taxis,
[00:27:40.980 --> 00:27:45.980]   autonomous cars in public transit
[00:27:45.980 --> 00:27:48.180]   will increase throughput.
[00:27:48.180 --> 00:27:50.660]   The only thing fixed in cities is lane rate.
[00:27:50.660 --> 00:27:52.100]   You have a road certain size,
[00:27:52.100 --> 00:27:55.040]   you don't destroy buildings on both sides.
[00:27:55.040 --> 00:27:58.900]   So if you can increase passengers per hour through it,
[00:27:58.900 --> 00:28:01.100]   and by the way, bicycles don't.
[00:28:01.100 --> 00:28:03.660]   We love bicycles, but they don't.
[00:28:03.660 --> 00:28:05.060]   Scooters don't.
[00:28:05.060 --> 00:28:11.020]   Public transit autonomously driven in small parts
[00:28:11.020 --> 00:28:13.420]   will make public transit on demand,
[00:28:13.420 --> 00:28:17.420]   so unscheduled, on demand, any time of the day or night,
[00:28:17.420 --> 00:28:19.740]   and point to point.
[00:28:19.740 --> 00:28:22.140]   You don't stop for anybody else to get up.
[00:28:22.140 --> 00:28:25.980]   It's a beautiful way to change cities.
[00:28:25.980 --> 00:28:26.980]   And I think it'll happen.
[00:28:26.980 --> 00:28:28.580]   There's no question about it.
[00:28:28.580 --> 00:28:32.220]   - So like Uber Pool, but with a larger mini bus type format.
[00:28:32.220 --> 00:28:33.060]   - No, no, no.
[00:28:33.060 --> 00:28:34.220]   You don't wanna go to mini bus
[00:28:34.220 --> 00:28:35.660]   because then you'll have to stop
[00:28:35.660 --> 00:28:37.180]   for other people to get up.
[00:28:37.180 --> 00:28:38.020]   - So just autonomous.
[00:28:38.020 --> 00:28:40.580]   - If it's two or four person vehicles,
[00:28:40.580 --> 00:28:43.060]   it's faster than your private car.
[00:28:43.060 --> 00:28:45.420]   It doesn't stop at traffic lights.
[00:28:45.420 --> 00:28:46.620]   It's on demand.
[00:28:46.620 --> 00:28:48.020]   You don't have to park it.
[00:28:48.020 --> 00:28:50.420]   I could go on and on.
[00:28:50.420 --> 00:28:53.060]   I don't wanna spend too much time on one particular idea,
[00:28:53.060 --> 00:28:54.740]   but all I'm saying is,
[00:28:54.740 --> 00:28:57.660]   whether it's fusion or public transit,
[00:28:57.660 --> 00:29:00.180]   and we're doing 3D printed buildings.
[00:29:00.180 --> 00:29:03.620]   Another one of my favorites is a new,
[00:29:03.620 --> 00:29:06.140]   and you were asking about this,
[00:29:06.140 --> 00:29:08.740]   a music model to produce music
[00:29:08.740 --> 00:29:13.460]   that is not trained on any YouTube or public music.
[00:29:13.460 --> 00:29:17.780]   So completely IP-free of any constraints.
[00:29:17.780 --> 00:29:19.220]   Like I'm really excited about it.
[00:29:19.220 --> 00:29:21.540]   Little outfit in Australia,
[00:29:21.540 --> 00:29:26.220]   and they've just released a product
[00:29:26.220 --> 00:29:28.380]   that would be the mid journey for music.
[00:29:28.380 --> 00:29:30.740]   Like really exciting.
[00:29:30.740 --> 00:29:32.980]   - What's your favorite genre of music?
[00:29:32.980 --> 00:29:33.820]   What do you like to listen to?
[00:29:33.820 --> 00:29:35.860]   - I'm not a huge music fan, to be honest.
[00:29:35.860 --> 00:29:37.460]   I had to admit it.
[00:29:37.460 --> 00:29:40.620]   But I love a great challenge
[00:29:40.620 --> 00:29:43.180]   when this entrepreneur came to me and said,
[00:29:43.180 --> 00:29:46.340]   "Hey, in some period of time, say 10 years,"
[00:29:46.340 --> 00:29:48.780]   and this was four or five years ago,
[00:29:48.780 --> 00:29:50.860]   and long before AI was a big thing.
[00:29:50.860 --> 00:29:55.740]   He said, "I wanna produce a music, a song,
[00:29:55.740 --> 00:29:59.380]   a top 10 music song untouched by a human."
[00:29:59.380 --> 00:30:02.340]   I said, "I love a challenge like that, let's go."
[00:30:02.340 --> 00:30:04.380]   It literally took me half an hour to say,
[00:30:04.380 --> 00:30:08.460]   "I'm in, no diligence, didn't need all that."
[00:30:08.460 --> 00:30:09.460]   (audience laughing)
[00:30:09.460 --> 00:30:10.300]   - David?
[00:30:10.300 --> 00:30:12.540]   - Yeah, I think where Tamath was gonna go a minute ago
[00:30:12.540 --> 00:30:14.740]   about the State of the Union and VC
[00:30:14.740 --> 00:30:17.100]   is that we talk a lot on the pod about the fact
[00:30:17.100 --> 00:30:18.700]   that we've just been through an asset bubble
[00:30:18.700 --> 00:30:20.980]   that popped last year.
[00:30:20.980 --> 00:30:23.860]   And it really inflated during COVID
[00:30:23.860 --> 00:30:25.140]   with all the money printing that went on,
[00:30:25.140 --> 00:30:27.060]   and it may have been inflating before that
[00:30:27.060 --> 00:30:30.220]   with these ZURP policies going back, I don't know, 15 years.
[00:30:30.220 --> 00:30:33.500]   I guess my question to you is,
[00:30:33.500 --> 00:30:36.700]   how much fakeness do you think that created
[00:30:36.700 --> 00:30:38.420]   in the ecosystem?
[00:30:38.420 --> 00:30:41.420]   Or is there any way to even quantify it?
[00:30:41.420 --> 00:30:44.500]   I mean, I think we see now that the size of venture funds
[00:30:44.500 --> 00:30:48.340]   is contracting, that new funds are gonna be smaller.
[00:30:48.340 --> 00:30:49.460]   Probably there's a lot of VCs
[00:30:49.460 --> 00:30:50.580]   who shouldn't even be in business.
[00:30:50.580 --> 00:30:52.980]   I think you'll be happy about that.
[00:30:52.980 --> 00:30:55.060]   We've estimated on the pod
[00:30:55.060 --> 00:30:56.620]   there's 1,400 unicorns right now.
[00:30:56.620 --> 00:30:59.340]   We've tried to sort of get, well, are half of them fake?
[00:30:59.340 --> 00:31:02.420]   I guess my question to you is,
[00:31:02.420 --> 00:31:05.420]   how much did this asset bubble sort of inflate
[00:31:05.420 --> 00:31:07.060]   everything in VC?
[00:31:07.060 --> 00:31:09.220]   And where do you think it stands right now?
[00:31:09.220 --> 00:31:12.980]   - You know, it inflated things a lot in certain areas,
[00:31:12.980 --> 00:31:16.620]   and it deflated things a lot in certain areas.
[00:31:16.620 --> 00:31:20.140]   But let me give you two analogies to understand this.
[00:31:20.140 --> 00:31:21.380]   Your investors only,
[00:31:21.380 --> 00:31:23.980]   and there's a fair number of investors I hear here,
[00:31:23.980 --> 00:31:26.900]   only care about two emotions, fear and greed.
[00:31:26.900 --> 00:31:29.060]   And they bounce between the two,
[00:31:29.060 --> 00:31:31.060]   and there's nothing in the middle.
[00:31:31.060 --> 00:31:33.500]   And so the key to being a good investor
[00:31:33.500 --> 00:31:37.420]   is staying focused on the long-term in the middle.
[00:31:37.420 --> 00:31:40.940]   So any time you get a hype cycle,
[00:31:40.940 --> 00:31:44.740]   you'll get inflation, and then you will get deflation.
[00:31:44.740 --> 00:31:47.380]   But let me ask you the following question.
[00:31:47.380 --> 00:31:51.260]   1998, there was a dot-com bubble, a bust.
[00:31:52.900 --> 00:31:55.340]   Tell me, and there was clearly a change
[00:31:55.340 --> 00:31:58.660]   in Wall Street prices and valuations and all that.
[00:31:58.660 --> 00:32:01.500]   There was absolutely no bust
[00:32:01.500 --> 00:32:03.860]   in the amount of internet traffic.
[00:32:03.860 --> 00:32:06.820]   So if you look at internet traffic,
[00:32:06.820 --> 00:32:10.180]   you can't tell when the dot-com bust happened.
[00:32:10.180 --> 00:32:13.260]   So I'm saying the reality of using the internet
[00:32:13.260 --> 00:32:16.340]   didn't change just because people had inflated values
[00:32:16.340 --> 00:32:17.980]   and then deflated them.
[00:32:17.980 --> 00:32:21.060]   That's just a fear reaction and a greed reaction.
[00:32:21.060 --> 00:32:23.020]   Same thing happened with the original bubble
[00:32:23.020 --> 00:32:25.740]   in the 1830s in England.
[00:32:25.740 --> 00:32:29.260]   If you got the right to build a railroad between two cities,
[00:32:29.260 --> 00:32:32.780]   you could offer strips on the market, big bubble.
[00:32:32.780 --> 00:32:34.460]   Then a big collapse.
[00:32:34.460 --> 00:32:38.060]   But more railroad was built in the 10 years
[00:32:38.060 --> 00:32:42.100]   after the bubble collapsed in England in the 1830s
[00:32:42.100 --> 00:32:43.580]   than before.
[00:32:43.580 --> 00:32:46.260]   My point is the reality of the underlying business,
[00:32:46.260 --> 00:32:50.060]   if you're adding value, does not change that much.
[00:32:50.060 --> 00:32:52.260]   It can accelerate a little bit or slow down
[00:32:52.260 --> 00:32:54.700]   because less resources available.
[00:32:54.700 --> 00:32:58.260]   But fundamentals is a good way to invest,
[00:32:58.260 --> 00:33:00.620]   sort of the Warren Buffett method,
[00:33:00.620 --> 00:33:04.300]   not the short-term, hey, he's valuing it twice,
[00:33:04.300 --> 00:33:08.860]   so let me pay up, and then you'll pay twice as much as that
[00:33:08.860 --> 00:33:11.020]   or SoftBank will later.
[00:33:11.020 --> 00:33:14.860]   I think that--
[00:33:14.860 --> 00:33:17.940]   - So I think it's a question of you can't time the market.
[00:33:17.940 --> 00:33:18.780]   - You can't time the market.
[00:33:18.780 --> 00:33:22.060]   - As long as you know that this is an investable technology,
[00:33:22.060 --> 00:33:26.020]   an investable trend, timing what price you're coming in at,
[00:33:26.020 --> 00:33:27.980]   I mean, this is an old adage in markets.
[00:33:27.980 --> 00:33:29.780]   - You know, it doesn't really matter
[00:33:29.780 --> 00:33:31.500]   what price you're coming at.
[00:33:31.500 --> 00:33:35.700]   You know, in inflationary times, you pay a little bit more,
[00:33:35.700 --> 00:33:38.500]   but you get less dilution later
[00:33:38.500 --> 00:33:40.220]   because other people are investing,
[00:33:40.220 --> 00:33:44.140]   and then when it goes into a deflationary cycle,
[00:33:44.140 --> 00:33:47.300]   you just have to learn to respond quickly.
[00:33:47.300 --> 00:33:50.860]   And good investors will help guide entrepreneurs correctly
[00:33:50.860 --> 00:33:52.020]   through all those phases.
[00:33:52.020 --> 00:33:54.380]   We've seen a lot of that in climate investing.
[00:33:54.380 --> 00:33:55.660]   - Yeah, for sure.
[00:33:55.660 --> 00:33:58.300]   - You know, very low valuations.
[00:33:58.300 --> 00:34:00.900]   Look, when we invested in Impossible Foods,
[00:34:00.900 --> 00:34:04.340]   it was a $3 million valuation, pre-money.
[00:34:04.340 --> 00:34:05.300]   - Crazy idea.
[00:34:05.300 --> 00:34:08.180]   - Crazy idea, nobody thought,
[00:34:08.180 --> 00:34:10.220]   plant proteins wasn't a word.
[00:34:10.220 --> 00:34:13.580]   Another one of my favorite projects
[00:34:13.580 --> 00:34:16.460]   is replace soy as a protein source on this planet,
[00:34:16.460 --> 00:34:19.140]   and I think we are well on our way to do that.
[00:34:19.140 --> 00:34:20.620]   I'm very excited.
[00:34:20.620 --> 00:34:23.980]   But at that level, it doesn't matter
[00:34:23.980 --> 00:34:26.940]   whether a valuation in the end goes up or down
[00:34:26.940 --> 00:34:28.500]   by a factor of two.
[00:34:28.500 --> 00:34:31.820]   It doesn't really matter if you're adding fundamental value
[00:34:31.820 --> 00:34:33.620]   and long-term things.
[00:34:33.620 --> 00:34:35.820]   - I know that your long-term view
[00:34:35.820 --> 00:34:38.660]   is critical to making Silicon Valley work.
[00:34:38.660 --> 00:34:41.940]   I think that we all kind of appreciate your leadership,
[00:34:41.940 --> 00:34:43.260]   your support, your mentorship,
[00:34:43.260 --> 00:34:45.300]   and the fact that you're willing
[00:34:45.300 --> 00:34:50.220]   to make those big bets in very low probability outcomes,
[00:34:50.220 --> 00:34:52.740]   but if they work, the return is so much more
[00:34:52.740 --> 00:34:56.940]   than that multiple, that ratio that you're getting.
[00:34:56.940 --> 00:34:58.740]   And it matters so much.
[00:34:58.740 --> 00:35:00.540]   - I just wanted to add, it's just inspiring
[00:35:00.540 --> 00:35:03.300]   how much you share with all of us who are behind you
[00:35:03.300 --> 00:35:05.220]   and how much we've all learned from you.
[00:35:05.220 --> 00:35:07.220]   I didn't mention it, but you've taken a lot of time
[00:35:07.220 --> 00:35:09.620]   for Trimoth, you've taken time for me and my career,
[00:35:09.620 --> 00:35:12.380]   and I just think it's like a great gift to us.
[00:35:12.380 --> 00:35:13.940]   - I can end with a quick story.
[00:35:13.940 --> 00:35:16.340]   - We really do have to jump.
[00:35:16.340 --> 00:35:19.820]   - I emailed, I mailed, paper mailed,
[00:35:19.820 --> 00:35:23.220]   40 or 50 people to try to get a job in VC in 2000.
[00:35:23.220 --> 00:35:25.500]   The only single person sent me
[00:35:25.500 --> 00:35:28.340]   a handwritten rejection letter, VK.
[00:35:28.340 --> 00:35:29.180]   - Hey!
[00:35:29.180 --> 00:35:30.300]   (audience applauding)
[00:35:30.300 --> 00:35:31.140]   Give it up.
[00:35:31.140 --> 00:35:32.820]   - Thank you, everybody.
[00:35:32.820 --> 00:35:35.900]   (audience applauding)
[00:35:35.900 --> 00:35:37.580]   - Another standing ovation, wow.
[00:35:37.580 --> 00:35:38.420]   - Thank you.
[00:35:38.420 --> 00:35:39.260]   - Thank you, sir.
[00:35:39.260 --> 00:35:40.100]   - That was great.
[00:35:40.100 --> 00:35:40.940]   - Thank you.
[00:35:40.940 --> 00:35:41.780]   - That was great.
[00:35:41.780 --> 00:35:44.380]   (upbeat music)
[00:35:44.380 --> 00:35:49.380]    Let your beat, let your beat 
[00:35:49.380 --> 00:35:55.940]    Let your winners ride 
[00:35:55.940 --> 00:35:58.900]    And I said 
[00:35:58.900 --> 00:36:00.220]    We open sourced it to the fans 
[00:36:00.220 --> 00:36:02.140]    And they've just gone crazy 
[00:36:02.140 --> 00:36:02.980]    Love you, Wesley 
[00:36:02.980 --> 00:36:07.980]    I see the queen of the king of the line 
[00:36:07.980 --> 00:36:13.780]    Besties are back 
[00:36:13.780 --> 00:36:15.780]   - I'm gonna have my dog take an admission in your driveway.
[00:36:15.780 --> 00:36:18.020]   (laughing)
[00:36:18.020 --> 00:36:20.300]   - I am, I am.
[00:36:20.300 --> 00:36:21.140]   - The natural VVF voice.
[00:36:21.140 --> 00:36:22.380]   - We should all just get a room
[00:36:22.380 --> 00:36:23.940]   and just have one big huge orgy
[00:36:23.940 --> 00:36:25.180]   'cause they're all just useless.
[00:36:25.180 --> 00:36:26.660]   It's like sexual tension
[00:36:26.660 --> 00:36:28.460]   that we just need to release somehow.
[00:36:28.460 --> 00:36:31.060]   - Let your beat, let your beat.
[00:36:31.060 --> 00:36:33.660]   - Let your beat, let your beat.
[00:36:33.660 --> 00:36:35.500]   - We need to get merch.
[00:36:35.500 --> 00:36:36.340]    Besties are back 
[00:36:36.340 --> 00:36:39.340]    I'm going all in 
[00:36:39.340 --> 00:36:41.940]   (upbeat music)
[00:36:41.940 --> 00:36:46.980]    I'm going all in 
[00:36:46.980 --> 00:36:51.620]   Thanks for watching!

