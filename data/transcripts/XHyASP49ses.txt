
[00:00:00.000 --> 00:00:07.840]   From the perspective of deep learning frameworks, you work with fast AI, particularly this framework,
[00:00:07.840 --> 00:00:10.600]   and PyTorch and TensorFlow.
[00:00:10.600 --> 00:00:15.000]   What are the strengths of each platform, your perspective?
[00:00:15.000 --> 00:00:19.880]   So in terms of what we've done our research on and taught in our course, we started with
[00:00:19.880 --> 00:00:28.080]   Theano and Keras, and then we switched to TensorFlow and Keras, and then we switched
[00:00:28.080 --> 00:00:32.520]   to PyTorch, and then we switched to PyTorch and fast AI.
[00:00:32.520 --> 00:00:41.680]   And that kind of reflects a growth and development of the ecosystem of deep learning libraries.
[00:00:41.680 --> 00:00:50.400]   Theano and TensorFlow were great, but were much harder to teach and do research and development
[00:00:50.400 --> 00:00:55.360]   on because they define what's called a computational graph up front, a static graph, where you
[00:00:55.360 --> 00:01:01.240]   basically have to say, "Here are all the things that I'm going to eventually do in my model."
[00:01:01.240 --> 00:01:04.280]   And then later on you say, "Okay, do those things with this data."
[00:01:04.280 --> 00:01:09.120]   And you can't debug them, you can't do them step by step, you can't program them interactively
[00:01:09.120 --> 00:01:11.480]   in a Jupyter notebook and so forth.
[00:01:11.480 --> 00:01:16.360]   PyTorch was not the first, but PyTorch was certainly the strongest entrant to come along
[00:01:16.360 --> 00:01:17.920]   and say, "Let's not do it that way.
[00:01:17.920 --> 00:01:20.620]   Let's just use normal Python.
[00:01:20.620 --> 00:01:24.400]   And everything you know about in Python is just going to work, and we'll figure out how
[00:01:24.400 --> 00:01:30.040]   to make that run on the GPU as and when necessary."
[00:01:30.040 --> 00:01:35.920]   That turned out to be a huge leap in terms of what we could do with our research and
[00:01:35.920 --> 00:01:37.680]   what we could do with our teaching.
[00:01:37.680 --> 00:01:39.680]   Because it wasn't limiting.
[00:01:39.680 --> 00:01:45.080]   Yeah, I mean, it was critical for us for something like DawnBench to be able to rapidly try things.
[00:01:45.080 --> 00:01:48.800]   It's just so much harder to be a researcher and practitioner when you have to do everything
[00:01:48.800 --> 00:01:52.640]   up front and you can't inspect it.
[00:01:52.640 --> 00:01:59.620]   Problem with PyTorch is it's not at all accessible to newcomers because you have to write your
[00:01:59.620 --> 00:02:04.920]   own training loop and manage the gradients and all this stuff.
[00:02:04.920 --> 00:02:08.960]   And it's also not great for researchers because you're spending your time dealing with all
[00:02:08.960 --> 00:02:13.100]   this boilerplate and overhead rather than thinking about your algorithm.
[00:02:13.100 --> 00:02:19.000]   So we ended up writing this very multi-layered API that at the top level you can train a
[00:02:19.000 --> 00:02:23.680]   state-of-the-art neural network and three lines of code, and which kind of talks to
[00:02:23.680 --> 00:02:27.400]   an API, which talks to an API, which talks to an API, which you can dive into at any
[00:02:27.400 --> 00:02:34.560]   level and get progressively closer to the machine kind of levels of control.
[00:02:34.560 --> 00:02:36.800]   And this is the FastAI library.
[00:02:36.800 --> 00:02:43.200]   That's been critical for us and for our students and for lots of people that have won big learning
[00:02:43.200 --> 00:02:47.800]   competitions with it and written academic papers with it.
[00:02:47.800 --> 00:02:49.800]   It's made a big difference.
[00:02:49.800 --> 00:02:56.000]   We're still limited though by Python and particularly this problem with things like recurrent neural
[00:02:56.000 --> 00:03:02.840]   nets, say, where you just can't change things unless you accept it going so slowly that
[00:03:02.840 --> 00:03:04.840]   it's impractical.
[00:03:04.840 --> 00:03:09.400]   So in the latest incarnation of the course and with some of the research we're now starting
[00:03:09.400 --> 00:03:13.520]   to do, we're starting to do stuff, some stuff in Swift.
[00:03:13.520 --> 00:03:20.080]   I think we're three years away from that being super practical, but I'm in no hurry.
[00:03:20.080 --> 00:03:23.800]   I'm very happy to invest the time to get there.
[00:03:23.800 --> 00:03:29.240]   But, you know, with that, we actually already have a nascent version of the FastAI library
[00:03:29.240 --> 00:03:33.920]   for vision running on Swift and TensorFlow.
[00:03:33.920 --> 00:03:37.320]   Because Python for TensorFlow is not going to cut it.
[00:03:37.320 --> 00:03:39.120]   It's just a disaster.
[00:03:39.120 --> 00:03:44.680]   What they did was they tried to replicate the bits that people were saying they like
[00:03:44.680 --> 00:03:51.320]   about PyTorch, this kind of interactive computation, but they didn't actually change their foundational
[00:03:51.320 --> 00:03:53.140]   runtime components.
[00:03:53.140 --> 00:03:57.760]   So they kind of added this like syntax sugar they call TF eager, TensorFlow eager, which
[00:03:57.760 --> 00:04:03.880]   makes it look a lot like PyTorch, but it's 10 times slower than PyTorch to actually do
[00:04:03.880 --> 00:04:05.700]   a step.
[00:04:05.700 --> 00:04:10.520]   So because they didn't invest the time in like retooling the foundations because their
[00:04:10.520 --> 00:04:12.520]   code base is so horribly complex.
[00:04:12.520 --> 00:04:15.560]   Yeah, I think it's probably very difficult to do that kind of retooling.
[00:04:15.560 --> 00:04:17.600]   Yeah, well, particularly the way TensorFlow was written.
[00:04:17.600 --> 00:04:22.560]   It was written by a lot of people very quickly in a very disorganized way.
[00:04:22.560 --> 00:04:26.800]   So like when you actually look in the code, as I do often, I'm always just like, oh, God,
[00:04:26.800 --> 00:04:28.040]   what were they thinking?
[00:04:28.040 --> 00:04:30.640]   It's just, it's pretty awful.
[00:04:30.640 --> 00:04:39.260]   So I'm really extremely negative about the potential future for Python for TensorFlow.
[00:04:39.260 --> 00:04:42.860]   But Swift for TensorFlow can be a different beast altogether.
[00:04:42.860 --> 00:04:49.260]   It can be like, it can basically be a layer on top of MLIR that takes advantage of, you
[00:04:49.260 --> 00:04:54.140]   know, all the great compiler stuff that Swift builds on with LLVM.
[00:04:54.140 --> 00:04:58.780]   And yeah, it could be, I think it will be absolutely fantastic.
[00:04:58.780 --> 00:05:01.060]   Well, you're inspiring me to try.
[00:05:01.060 --> 00:05:06.820]   I haven't truly felt the pain of TensorFlow 2.0 Python.
[00:05:06.820 --> 00:05:08.340]   It's fine by me.
[00:05:08.340 --> 00:05:14.580]   But yeah, I mean, it does the job if you're using like predefined things that somebody's
[00:05:14.580 --> 00:05:16.860]   already written.
[00:05:16.860 --> 00:05:21.100]   But if you actually compare, you know, like I've had to do, because I've been having to
[00:05:21.100 --> 00:05:24.580]   do a lot of stuff with TensorFlow recently, you actually compare like, okay, I want to
[00:05:24.580 --> 00:05:26.580]   write something from scratch.
[00:05:26.580 --> 00:05:30.760]   And I just keep finding it's like, oh, it's running 10 times slower than PyTorch.
[00:05:30.760 --> 00:05:37.140]   So is the biggest cost, let's throw running time out the window, how long it takes you
[00:05:37.140 --> 00:05:38.820]   to program?
[00:05:38.820 --> 00:05:40.200]   That's not too different now.
[00:05:40.200 --> 00:05:43.260]   Thanks to TensorFlow Eager, that's not too different.
[00:05:43.260 --> 00:05:49.460]   But because so many things take so long to run, you wouldn't run it at 10 times slower.
[00:05:49.460 --> 00:05:52.380]   Like you just go like, oh, this is taking too long.
[00:05:52.380 --> 00:05:56.660]   And also, there's a lot of things which are just less programmable, like tf.data, which
[00:05:56.660 --> 00:06:00.500]   is the way data processing works in TensorFlow is just this big mess.
[00:06:00.500 --> 00:06:02.380]   It's incredibly inefficient.
[00:06:02.380 --> 00:06:08.340]   And they kind of had to write it that way because of the TPU problems I described earlier.
[00:06:08.340 --> 00:06:14.140]   So I just, you know, I just feel like they've got this huge technical debt, which they're
[00:06:14.140 --> 00:06:16.820]   not going to solve without starting from scratch.
[00:06:16.820 --> 00:06:18.420]   So here's an interesting question.
[00:06:18.420 --> 00:06:26.140]   If there's a new student starting today, what would you recommend they use?
[00:06:26.140 --> 00:06:31.940]   Well, I mean, we obviously recommend Fast.ai and PyTorch because we teach new students.
[00:06:31.940 --> 00:06:33.100]   And that's what we teach with.
[00:06:33.100 --> 00:06:39.040]   So we would very strongly recommend that because it will let you get on top of the concepts
[00:06:39.040 --> 00:06:41.100]   much more quickly.
[00:06:41.100 --> 00:06:45.300]   So then you'll become an actual and you'll also learn the actual state of the art techniques,
[00:06:45.300 --> 00:06:48.380]   you know, so you actually get world class results.
[00:06:48.380 --> 00:06:56.580]   Honestly, it doesn't much matter what library you learn, because switching from China to
[00:06:56.580 --> 00:07:02.100]   MXNet to TensorFlow to PyTorch is going to be a couple of days work as long as you understand
[00:07:02.100 --> 00:07:04.460]   the foundation as well.
[00:07:04.460 --> 00:07:12.100]   But you think will Swift creep in there as a thing that people start using?
[00:07:12.100 --> 00:07:19.780]   Not for a few years, particularly because like Swift has no data science community,
[00:07:19.780 --> 00:07:22.660]   libraries, tooling.
[00:07:22.660 --> 00:07:29.300]   And the Swift community has a total lack of appreciation and understanding of numeric
[00:07:29.300 --> 00:07:30.300]   computing.
[00:07:30.300 --> 00:07:33.920]   So like they keep on making stupid decisions, you know, for years they've just done dumb
[00:07:33.920 --> 00:07:39.540]   things around performance and prioritization.
[00:07:39.540 --> 00:07:48.580]   That's clearly changing now because the developer of Swift, Chris Latner, is working at Google
[00:07:48.580 --> 00:07:49.900]   on Swift for TensorFlow.
[00:07:49.900 --> 00:07:53.340]   So like that's a priority.
[00:07:53.340 --> 00:07:57.500]   It'll be interesting to see what happens with Apple because like Apple hasn't shown any
[00:07:57.500 --> 00:08:02.980]   sign of caring about numeric programming in Swift.
[00:08:02.980 --> 00:08:08.320]   So I mean, hopefully they'll get off their ass and start appreciating this because currently
[00:08:08.320 --> 00:08:14.220]   all of their low-level libraries are not written in Swift.
[00:08:14.220 --> 00:08:16.620]   They're not particularly Swifty at all.
[00:08:16.620 --> 00:08:19.940]   Stuff like Core ML, they're really pretty rubbish.
[00:08:19.940 --> 00:08:22.820]   So yeah, so there's a long way to go.
[00:08:22.820 --> 00:08:27.660]   But at least one nice thing is that Swift for TensorFlow can actually directly use Python
[00:08:27.660 --> 00:08:34.500]   code and Python libraries in a literally the entire lesson one notebook of fast.ai runs
[00:08:34.500 --> 00:08:37.740]   in Swift right now in Python mode.
[00:08:37.740 --> 00:08:40.380]   So that's a nice intermediate thing.
[00:08:40.380 --> 00:08:40.880]   [END]
[00:08:40.880 --> 00:08:41.880]   1
[00:08:41.880 --> 00:09:04.880]   Page 1 of 3

