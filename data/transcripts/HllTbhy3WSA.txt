
[00:00:00.000 --> 00:00:04.440]   Thanks for inviting me. And Tyler also like it was really
[00:00:04.440 --> 00:00:10.480]   great talk. And yeah, so let me go forward with this talk. So
[00:00:10.480 --> 00:00:13.880]   this talk is not essentially about only our recent paper.
[00:00:13.880 --> 00:00:17.680]   This is essentially about clubbing two similar works that
[00:00:17.680 --> 00:00:22.280]   I had. So I'm just gonna like go through that in this talk. So
[00:00:22.280 --> 00:00:26.600]   first, like a little bit of intro about myself. I'm a PhD
[00:00:26.600 --> 00:00:30.480]   candidate at McGill University and Mila, co supervised by Joel
[00:00:30.480 --> 00:00:34.280]   Pinnu and William L. Hamilton. And also I'm a research intern
[00:00:34.280 --> 00:00:39.160]   at FAIR Montreal. My research interests are kind of in
[00:00:39.160 --> 00:00:42.080]   intersection right now with systematic generalization, which
[00:00:42.080 --> 00:00:46.000]   is kind of a sub part of neural reasoning, and in discrete
[00:00:46.000 --> 00:00:49.840]   domains such as in language or in graphs. And also I dabble a
[00:00:49.840 --> 00:00:53.320]   little bit into dialogue systems. And originally, I'm
[00:00:53.320 --> 00:00:58.080]   from Kolkata, India, and right now I'm in I'm at Montreal. So,
[00:00:58.080 --> 00:01:03.000]   okay, so why, why care about generalization? So ml models
[00:01:03.000 --> 00:01:06.480]   are getting really good at feature extraction. So those of
[00:01:06.480 --> 00:01:10.880]   you have like seen Joshua's stellar talk in last new rips,
[00:01:10.880 --> 00:01:15.840]   he talked about this topic of system one versus system to deep
[00:01:15.840 --> 00:01:19.920]   learning. So what is system one system one is where we are right
[00:01:19.920 --> 00:01:23.360]   now where we have very good models which does feature
[00:01:23.360 --> 00:01:27.200]   extraction. So given a standard data set that model like really
[00:01:27.200 --> 00:01:31.960]   performs quite well on that particular data set. And in the
[00:01:31.960 --> 00:01:36.280]   bottom right corner, you see that I've posted like a top one
[00:01:36.280 --> 00:01:39.680]   accuracy, this is taken from papers from code. And this is a
[00:01:39.680 --> 00:01:43.080]   top one accuracy on image net. And right now the state of the
[00:01:43.080 --> 00:01:48.680]   art is just a little bit shy of 90%. So system one deep learning
[00:01:48.680 --> 00:01:51.160]   works really, really well, we have pretty good feature
[00:01:51.160 --> 00:01:55.360]   extractors. And also the number of parameters used by our models
[00:01:55.360 --> 00:02:00.000]   are like increasing, like really, really fast. But what
[00:02:00.000 --> 00:02:03.680]   this talk is involving is, I'm kind of trying to motivate
[00:02:03.680 --> 00:02:07.520]   towards system to deep learning. So this is reasoning, which
[00:02:07.520 --> 00:02:12.880]   involves inductive reasoning. Basically, you basically when
[00:02:12.880 --> 00:02:16.080]   you learn some skills from your data set, you want to like apply
[00:02:16.080 --> 00:02:21.280]   those skills to some new data set or those data sets which are
[00:02:21.280 --> 00:02:25.080]   kind of produced from that particular data set. And you
[00:02:25.080 --> 00:02:28.120]   want to transfer your known skills. And you also want to
[00:02:28.120 --> 00:02:33.480]   remember your known skills. So basically, this is a slide I've
[00:02:33.480 --> 00:02:36.400]   just like kind of screen grab from your show stock where he
[00:02:36.400 --> 00:02:40.160]   talks more about system one and system two deep learning. So
[00:02:40.160 --> 00:02:43.560]   essentially, the system two or the future is to go towards a
[00:02:43.560 --> 00:02:49.080]   set of models which are kind of more logically driven. So
[00:02:49.080 --> 00:02:53.320]   essentially, whenever you're making like a prediction, you
[00:02:53.320 --> 00:02:57.160]   are making that prediction in a logically valid way, so that you
[00:02:57.160 --> 00:03:01.320]   can reapply your learned skills on top of a data set, which is
[00:03:01.320 --> 00:03:06.040]   kind of derived from the original data set. So that kind
[00:03:06.040 --> 00:03:09.120]   of helps us to go towards a realm of out of domain
[00:03:09.120 --> 00:03:12.880]   distribution. Out of domain is kind of ill defined, like you
[00:03:12.880 --> 00:03:16.760]   can just say that, okay, I train on a subset of images involving
[00:03:16.760 --> 00:03:21.640]   cats, and I want to test on images involving dogs. But in
[00:03:21.640 --> 00:03:24.920]   in case of the out of domain distribution, it's better to
[00:03:24.920 --> 00:03:29.000]   present it in this way that if you if your model can learn the
[00:03:29.000 --> 00:03:33.000]   integral skills or the integral components that is driving a
[00:03:33.000 --> 00:03:37.640]   particular task, can your models use those components and reapply
[00:03:37.640 --> 00:03:41.320]   them to a new task. So that example that I gave, it will
[00:03:41.320 --> 00:03:44.360]   turn up to be let's say you train on a subset of cats and
[00:03:44.360 --> 00:03:49.840]   whether you can recognize a new breed of cat. So how do you know
[00:03:49.840 --> 00:03:53.080]   your model is reasoning. So there are a lot of performance
[00:03:53.080 --> 00:03:56.920]   metrics that we look at specifically, accuracy or in
[00:03:56.920 --> 00:04:01.000]   binary classification, you see on held out test sets, but that
[00:04:01.000 --> 00:04:05.240]   is it like that is the only measure that we are interested
[00:04:05.240 --> 00:04:08.360]   in mostly in deep learning research. So right now, there's
[00:04:08.360 --> 00:04:12.800]   a lot of push of building more interpretable models. We also
[00:04:12.800 --> 00:04:16.520]   want to build a lot more intricate assets. So essentially,
[00:04:16.520 --> 00:04:19.520]   you just not say that, okay, on aggregate, my model is
[00:04:19.520 --> 00:04:23.640]   performing 90%. But what does it show like, does it show that
[00:04:23.640 --> 00:04:26.320]   where your models are failing and how they are failing? And
[00:04:26.320 --> 00:04:29.120]   even if they are feeling like their feet, failures should be
[00:04:29.120 --> 00:04:33.360]   kind of systematic. And also, how do you guarantee that your
[00:04:33.360 --> 00:04:36.800]   model is just not basically rote learning features like given
[00:04:37.280 --> 00:04:40.040]   distribution, which is somewhat similar, but not exactly the
[00:04:40.040 --> 00:04:43.560]   same, whether your models can learn on this kind of setups.
[00:04:43.560 --> 00:04:47.320]   And also how to evaluate the robustness of your model. So
[00:04:47.320 --> 00:04:50.120]   recently, in like in natural language processing literature,
[00:04:50.120 --> 00:04:53.600]   a lot of work is coming up on adversarial attacks. And people
[00:04:53.600 --> 00:04:56.400]   have been shown over and over that with certain types of
[00:04:56.400 --> 00:04:59.080]   adversarial attacks, you can pull a lot of language
[00:04:59.080 --> 00:05:01.840]   classifiers and also a lot of image classifiers. And there's a
[00:05:01.840 --> 00:05:06.520]   lot of defense mechanisms are coming up. But again, like, how
[00:05:06.520 --> 00:05:10.440]   do you evaluate your model on that kind of robustness without
[00:05:10.440 --> 00:05:15.800]   having access to such kind of data. So the key properties in
[00:05:15.800 --> 00:05:18.600]   system to deep learning that I'm trying to propose is that we
[00:05:18.600 --> 00:05:22.400]   want to go towards systematic generalization. So essentially,
[00:05:22.400 --> 00:05:27.600]   we want to learn to extract the core skills of a task. And we
[00:05:27.600 --> 00:05:31.080]   want to learn to apply those skills to solve any task, which
[00:05:31.080 --> 00:05:34.520]   can be produced from such rules. For example, if if you are
[00:05:34.520 --> 00:05:37.400]   learning how to add, you're probably learning how to add
[00:05:37.400 --> 00:05:41.640]   like, let's say, single or double digit numbers. But let's
[00:05:41.640 --> 00:05:44.960]   say on test time, if I give you like, n digit number, then you
[00:05:44.960 --> 00:05:47.520]   should also be able to add because it's the same scale,
[00:05:47.520 --> 00:05:51.240]   you're just applying it over a new distribution. So that is
[00:05:51.240 --> 00:05:56.160]   just a very trivial example. But in in a lot of other cases, you
[00:05:56.160 --> 00:06:00.880]   want to be kind of able to extract the rules that are in a
[00:06:00.920 --> 00:06:04.960]   imbibed in your data set. And you want to make sure that your
[00:06:04.960 --> 00:06:08.760]   model like reapplies those rules. And you want to be
[00:06:08.760 --> 00:06:12.000]   invariant to noise. And to do that, your model must be
[00:06:12.000 --> 00:06:15.560]   logically valid. That is, if your model is like correct at
[00:06:15.560 --> 00:06:19.200]   certain examples, then it should be correct for those examples
[00:06:19.200 --> 00:06:24.480]   throughout a set of distributions. So let's say we
[00:06:24.480 --> 00:06:27.800]   take a simple example, let's say example can be a simple
[00:06:27.800 --> 00:06:32.800]   kinship relation game. Now, if I'm given you a text which says
[00:06:32.800 --> 00:06:36.680]   about a certain kinship relation between characters, you can,
[00:06:36.680 --> 00:06:40.520]   like if I ask you like, okay, how is Carol related to Justin
[00:06:40.520 --> 00:06:43.560]   in this case, then you can easily say that, okay, Carol is
[00:06:43.560 --> 00:06:48.160]   the grandmother of Justin. Now this kinship relations, we know,
[00:06:48.160 --> 00:06:51.840]   like these rules that we already have learned from growing up.
[00:06:51.840 --> 00:06:56.080]   But the cool thing what we do without realizing is that we
[00:06:56.280 --> 00:07:00.200]   reapply these rules. So if I give you a long, long set of
[00:07:00.200 --> 00:07:02.960]   relations, and if I give you like a pen and paper, you can
[00:07:02.960 --> 00:07:06.960]   still solve that you can still like reapply your own thinking
[00:07:06.960 --> 00:07:10.880]   to solve like, okay, this character is related to this
[00:07:10.880 --> 00:07:16.160]   character in this way. So basically, you can think of it
[00:07:16.160 --> 00:07:20.840]   as a graph, also, like, given certain entities and certain
[00:07:20.840 --> 00:07:24.560]   edges, so the edges are essentially denoting relations.
[00:07:24.800 --> 00:07:28.360]   And if you want to do relation prediction, that is, what is the
[00:07:28.360 --> 00:07:32.160]   type of a particular edge, then in this kind of a kinship
[00:07:32.160 --> 00:07:36.880]   relation game, you can easily do that by taking like a walk over
[00:07:36.880 --> 00:07:43.080]   the path and basically resolving that path. So one of my previous
[00:07:43.080 --> 00:07:47.680]   work is named clutter, which is exactly this. So clutter is
[00:07:47.680 --> 00:07:51.480]   essentially a diagnostic benchmark, which is created for
[00:07:51.520 --> 00:07:55.240]   evaluating inductive reasoning from text, it is exactly a
[00:07:55.240 --> 00:07:59.840]   kinship relation game, where the model is essentially given a
[00:07:59.840 --> 00:08:04.280]   blurb of text, which contains a story. And it's a q&a setup
[00:08:04.280 --> 00:08:08.800]   where you're given a question, and then your model has has to
[00:08:08.800 --> 00:08:14.320]   give out an answer on all possible family relations. Now,
[00:08:14.320 --> 00:08:20.360]   each puzzle is built using first order logic. Now, that helps us
[00:08:20.360 --> 00:08:25.360]   a lot in basically making sure that all the puzzles are
[00:08:25.360 --> 00:08:30.800]   verifiable. And the logic is built on a set of rules. Now,
[00:08:30.800 --> 00:08:35.000]   the now the generator is basically fed a set of rules
[00:08:35.000 --> 00:08:38.120]   that we know about family relations. And then the
[00:08:38.120 --> 00:08:41.400]   generator essentially reapplies these rules to create this
[00:08:41.400 --> 00:08:45.880]   intricate puzzle. So this puzzle has essentially two modalities.
[00:08:45.880 --> 00:08:49.320]   So it has the primary modality is the text which we want the
[00:08:49.320 --> 00:08:52.880]   models to perform. And then there is a secondary modality
[00:08:52.880 --> 00:08:56.360]   is graph that is each puzzle also has an underlying graph.
[00:08:56.360 --> 00:09:01.440]   Now, this is also a kind of a diagnostic suite, you can build
[00:09:01.440 --> 00:09:06.040]   your own complex setups on top of this. And then you can see
[00:09:06.040 --> 00:09:10.280]   how your models are systematically generalizing. Now,
[00:09:10.280 --> 00:09:14.200]   just as an example, so this is an input document, this gets
[00:09:14.200 --> 00:09:19.640]   fed to the model. And then the we have a query. So in this
[00:09:19.640 --> 00:09:23.880]   case, we have a query, which is a pair of entities that we want
[00:09:23.880 --> 00:09:28.240]   the relation in. Interestingly, we didn't use a natural language
[00:09:28.240 --> 00:09:31.960]   question that you see in a lot of q&a data sets, because there
[00:09:31.960 --> 00:09:36.360]   was a really nice paper in a MNLP 2018, which shows that a
[00:09:36.360 --> 00:09:40.560]   lot of these models in they do not even understand the entire
[00:09:40.560 --> 00:09:43.480]   context of the question. So let's say if there is a question
[00:09:43.520 --> 00:09:46.800]   is that when did this event occur, the model kind of
[00:09:46.800 --> 00:09:51.080]   performs even well, if you like cut off all the different words
[00:09:51.080 --> 00:09:54.880]   and still focus on the word when this is a like a really great
[00:09:54.880 --> 00:09:59.400]   paper by Zachary Lipton, I would suggest you all to read it. And
[00:09:59.400 --> 00:10:04.200]   so basically, like we replace the question with the set of
[00:10:04.200 --> 00:10:09.400]   like queries. And then the task is simply to predict what is the
[00:10:09.400 --> 00:10:14.640]   relationship between them. Now, this seems very simple. And I
[00:10:14.640 --> 00:10:17.080]   have like heard from a lot of people that they are saying that
[00:10:17.080 --> 00:10:20.000]   okay, this is very simple, because you just have a set of
[00:10:20.000 --> 00:10:22.960]   rules that the model have to learn. And once the model learns
[00:10:22.960 --> 00:10:28.240]   the rule, and then it should perform well, right. So it turns
[00:10:28.240 --> 00:10:34.960]   out no, why let us go towards a bit of the task first. So one of
[00:10:34.960 --> 00:10:37.680]   the tasks or the primary task that we want to look at is
[00:10:37.680 --> 00:10:41.400]   called logical generalization. So in logical generalization,
[00:10:41.400 --> 00:10:45.400]   you're given a set of like puzzles, which are kind of
[00:10:45.400 --> 00:10:49.920]   restricted by their length. So essentially, you're given a lot
[00:10:49.920 --> 00:10:52.640]   of examples, but you're restricting the length of this
[00:10:52.640 --> 00:10:56.960]   puzzle to only contain relationship combinations of
[00:10:56.960 --> 00:11:01.360]   length three. And then you're testing on puzzles, which has
[00:11:01.360 --> 00:11:04.840]   relationship combinations of length more than three. So this
[00:11:04.840 --> 00:11:08.480]   is equivalent to the example I gave before, that if you know
[00:11:08.480 --> 00:11:12.920]   how relationships work, if I give you a long, long passage,
[00:11:12.920 --> 00:11:17.000]   you can still solve that given enough time and compute. So this
[00:11:17.000 --> 00:11:20.560]   is exactly the notion of system do deep learning because the
[00:11:20.560 --> 00:11:24.600]   models have to think the model have to like reapply their own
[00:11:24.600 --> 00:11:30.520]   skills over and over in order to solve this particular task. And
[00:11:30.720 --> 00:11:36.040]   also, we looked at a case of linguistic generalization. So
[00:11:36.040 --> 00:11:40.200]   essentially, we thought that okay, a lot of NLP models are
[00:11:40.200 --> 00:11:44.200]   pretty good these days, because they kind of learn a lot of
[00:11:44.200 --> 00:11:49.080]   features about the the individual words. And then they
[00:11:49.080 --> 00:11:52.000]   whenever they find the similar words, they kind of overfit on
[00:11:52.000 --> 00:11:56.640]   top of that. In order to like, in order to make sure that the
[00:11:56.640 --> 00:12:00.160]   models are not doing it. For each puzzle, we have two
[00:12:00.160 --> 00:12:04.600]   representations of each puzzle. One representation is drawn,
[00:12:04.600 --> 00:12:08.280]   like basically each puzzle is a graph. And then you're applying
[00:12:08.280 --> 00:12:11.600]   a layer of language on top of it. And this layer of language
[00:12:11.600 --> 00:12:14.480]   is what we are collecting through Amazon Mechanical Turk.
[00:12:14.480 --> 00:12:18.240]   And essentially, we are splitting the placeholders of
[00:12:18.240 --> 00:12:22.120]   the Amazon Mechanical Turk into training and testing. So this
[00:12:22.120 --> 00:12:25.440]   becomes really challenging for the model because the model has
[00:12:25.440 --> 00:12:29.800]   never seen the exact syntax before. But if the model
[00:12:29.800 --> 00:12:34.360]   understands how relationships work, then it should be able to
[00:12:34.360 --> 00:12:39.600]   generalize better in this kind of setup. So and also a third
[00:12:39.600 --> 00:12:42.760]   cool thing that we had in clutter was a robustness test.
[00:12:42.760 --> 00:12:47.200]   So as I said that, the since these are built on logic, you
[00:12:47.200 --> 00:12:51.120]   can easily verify whether your logic is sound or not. So let's
[00:12:51.120 --> 00:12:54.440]   say given in this example, if I have given you a family
[00:12:54.440 --> 00:12:58.480]   relation tree like this, and let's say I want I want you to
[00:12:58.480 --> 00:13:02.440]   answer what is the relationship between A and D. So then there
[00:13:02.440 --> 00:13:06.880]   can be two possible paths in the supporting fact. And then your
[00:13:06.880 --> 00:13:09.320]   model has to understand, okay, since there are supporting
[00:13:09.320 --> 00:13:13.480]   facts, I have additional information to do well. While if
[00:13:13.480 --> 00:13:16.760]   I have a tree which has a dangling path, that is kind of a
[00:13:16.760 --> 00:13:21.880]   confounder. And that confounder is like used to make sure that
[00:13:21.880 --> 00:13:25.040]   okay, your model does not perform well, but your model has
[00:13:25.040 --> 00:13:27.880]   to understand, okay, I do not want to go through this path.
[00:13:27.880 --> 00:13:32.240]   And then there is a disconnected fact where you have a edge which
[00:13:32.240 --> 00:13:35.920]   is entirely disconnected from your like your current
[00:13:35.920 --> 00:13:39.520]   resolution path, but then your model has to also disregard
[00:13:39.520 --> 00:13:43.960]   that. So these are really simple, but very powerful
[00:13:43.960 --> 00:13:49.880]   robustness test that we have in Flutter. And so let's go towards
[00:13:49.880 --> 00:13:53.240]   like what are the models we initially used. So we
[00:13:53.240 --> 00:13:55.960]   essentially thought, okay, we have made the task really
[00:13:55.960 --> 00:13:58.920]   difficult for the text or the natural language understanding
[00:13:58.920 --> 00:14:04.120]   models, we also need to provide a benchmark for a model which do
[00:14:04.120 --> 00:14:08.040]   not have to do the parsing. So we use the graph attention
[00:14:08.040 --> 00:14:14.040]   network, which worked solely on top of the raw graphs. And for
[00:14:14.040 --> 00:14:17.360]   models that have access to the text, we used a vast number of
[00:14:17.360 --> 00:14:21.040]   models, including transformer style models such as BERT and
[00:14:21.040 --> 00:14:26.040]   BERT LSTM, which is essentially a layer of LSTM on top of BERT on
[00:14:26.040 --> 00:14:31.800]   top of frozen BERT essentially. So how does the models fare on
[00:14:31.800 --> 00:14:36.480]   generalizability? In the plot on your right, you see that the
[00:14:36.480 --> 00:14:40.640]   generalizability or the accuracy during testing drastically
[00:14:40.640 --> 00:14:46.320]   falls down when we show tasks of more and more length. While if
[00:14:46.320 --> 00:14:50.240]   we provide more training data, like if we provide the model
[00:14:50.240 --> 00:14:54.480]   training data of involving four number of relations, then the
[00:14:54.480 --> 00:14:58.040]   model like perform somewhat better. But the graph neural
[00:14:58.040 --> 00:15:02.200]   network performs the best when given a lot of additional data.
[00:15:02.200 --> 00:15:06.480]   But still the generalization towards like extreme
[00:15:06.480 --> 00:15:10.680]   generalization, what I say like towards the end of the like the
[00:15:10.680 --> 00:15:15.920]   number of 10 relations, then all the models kind of falter. But
[00:15:15.960 --> 00:15:20.200]   graph neural networks are really robust. When we use the
[00:15:20.200 --> 00:15:23.800]   different robustness test, we found that the graph neural
[00:15:23.800 --> 00:15:28.000]   networks almost always outperform all the text
[00:15:28.000 --> 00:15:32.160]   question answering models, which is understandably so because the
[00:15:32.160 --> 00:15:36.200]   graph neural network do not have to do the semantic parsing. So
[00:15:36.200 --> 00:15:40.280]   semantic parsing is one problem, but still the graph neural
[00:15:40.280 --> 00:15:45.040]   networks falter on the length generalization problem. So
[00:15:45.600 --> 00:15:50.400]   basically, clutter was based only on 15 rules. And these 15
[00:15:50.400 --> 00:15:53.560]   rules were kind of a very boiled down set of family relation
[00:15:53.560 --> 00:15:56.760]   rules, which we used in order to make sure that the puzzles do
[00:15:56.760 --> 00:16:00.560]   not have any ambiguous relations. Now, how do we
[00:16:00.560 --> 00:16:04.560]   evaluate the adaptation of the models to different rules? Now,
[00:16:04.560 --> 00:16:08.160]   why is adaptation important? It is important. Let's say, for
[00:16:08.160 --> 00:16:11.360]   example, in recommender systems, you have your model which is
[00:16:11.360 --> 00:16:14.720]   trained on production for certain products. And now your
[00:16:14.720 --> 00:16:19.280]   company wants to sell some other product. Now, you have to adapt
[00:16:19.280 --> 00:16:22.160]   on this new data of products, because you have to learn the
[00:16:22.160 --> 00:16:25.680]   relationship between these new products. But if your model
[00:16:25.680 --> 00:16:29.240]   already knows how to pick up these relationships, it will be
[00:16:29.240 --> 00:16:34.600]   able to pick it up really well on this new data. And also, that
[00:16:34.600 --> 00:16:37.800]   would be really useful later on for smart dialogue agents, which
[00:16:37.800 --> 00:16:41.240]   will be able to like converse with you based on some prior
[00:16:41.240 --> 00:16:45.360]   information that you have already shared with it. Also, we
[00:16:45.360 --> 00:16:48.880]   want to know in system two deep learning that how do we remember
[00:16:48.880 --> 00:16:53.680]   old, old rules? For example, when we are going forward to
[00:16:53.680 --> 00:16:57.440]   newer and newer tasks on your and newer rules, do we still
[00:16:57.440 --> 00:17:02.080]   remember the old rules that we learned before? So these are the
[00:17:02.080 --> 00:17:05.000]   three key requirements for logical generalization, we want
[00:17:05.000 --> 00:17:08.320]   to inductively reason to unseen compositions, which is
[00:17:08.320 --> 00:17:12.520]   compositionality, we want to learn to adapt new rule set,
[00:17:12.520 --> 00:17:16.320]   which is multitask or met or even meta learning. So these are
[00:17:16.320 --> 00:17:20.280]   two different sets of problems. In this paper, we only looked at
[00:17:20.280 --> 00:17:24.400]   multitask, but meta learning can easily be done on top of this
[00:17:24.400 --> 00:17:30.160]   data. And we want to see how do how does the models remember the
[00:17:30.160 --> 00:17:33.720]   learned rules. So given a new set of rules, the learner should
[00:17:33.720 --> 00:17:40.040]   not forget what it has learned before. So now we come to the
[00:17:40.040 --> 00:17:43.840]   current paper, which is, again, another data set. But this data
[00:17:43.840 --> 00:17:47.680]   set is focused purely on graph, because in the previous paper,
[00:17:47.680 --> 00:17:50.560]   we found that the graphs have performed so well. So then we
[00:17:50.560 --> 00:17:53.920]   thought, okay, let's see how far we can stress test graph neural
[00:17:53.920 --> 00:17:58.080]   networks. And this is a multi purpose multi relational graph
[00:17:58.080 --> 00:18:01.800]   data set, which is built using the rules of first order logic.
[00:18:02.440 --> 00:18:06.560]   But unlike clutter, so clutter was only built on top of 15
[00:18:06.560 --> 00:18:09.960]   rules, and still it was challenging enough. In this
[00:18:09.960 --> 00:18:14.360]   work, we went a bit more crazy, we stuck we generated a large
[00:18:14.360 --> 00:18:17.840]   bunch of rules. And then we said, Okay, once we have
[00:18:17.840 --> 00:18:22.120]   generated this large bunch of rules, we want to basically
[00:18:22.120 --> 00:18:25.360]   sample rules from this large set in order to create different
[00:18:25.360 --> 00:18:29.480]   words. Now, different words essentially mean different
[00:18:29.480 --> 00:18:33.640]   buckets. So each bucket contains a set of rules. But these set
[00:18:33.640 --> 00:18:37.760]   of rules are overlapping with each other. So let's say there
[00:18:37.760 --> 00:18:41.080]   could be a scenario where there are two words, the two words
[00:18:41.080 --> 00:18:44.880]   have the exact same set of rules except one. So then that two
[00:18:44.880 --> 00:18:49.480]   words are kind of 95% similar based on number of rules that
[00:18:49.480 --> 00:18:54.160]   are available. So that gives us a really nice way to define
[00:18:54.160 --> 00:18:59.000]   similarity. And similarity is important, because it helps us
[00:18:59.000 --> 00:19:03.320]   to define how far your task is going out of the current
[00:19:03.320 --> 00:19:06.640]   distribution. So when people are interested in out of domain
[00:19:06.640 --> 00:19:09.920]   distribution, the standard problem that people face is that
[00:19:09.920 --> 00:19:13.600]   there is no such metric on different data sets to define
[00:19:13.600 --> 00:19:17.640]   what is exactly an out of domain distribution. But if you define
[00:19:17.640 --> 00:19:21.360]   your data set based on standard first order logic, you get a
[00:19:21.360 --> 00:19:26.280]   really nice interface to do that. And that's exactly what we
[00:19:26.280 --> 00:19:31.520]   did in graph log. So essentially, each, each word. So
[00:19:31.520 --> 00:19:35.760]   these are like w1, w2, wn, these are different worlds. And each
[00:19:35.760 --> 00:19:41.400]   word consists of 5000 graphs in training 1000 in testing. And
[00:19:41.400 --> 00:19:45.520]   then these different graphs. So essentially, we had a total of
[00:19:45.520 --> 00:19:51.400]   57 different worlds. And the number of graphs are also quite
[00:19:51.400 --> 00:19:55.320]   large, because there's 5000 in each of these worlds. And also
[00:19:55.360 --> 00:20:01.680]   each world has its own world specific graph. So basically,
[00:20:01.680 --> 00:20:04.720]   what is a world specific graph. So essentially, when we are
[00:20:04.720 --> 00:20:09.000]   generating graphs for each world, we first generate a huge
[00:20:09.000 --> 00:20:13.480]   graph. So this graph is basically generated by applying
[00:20:13.480 --> 00:20:16.960]   all the rules over and over and over and over. And when you have
[00:20:16.960 --> 00:20:20.240]   this huge graph, you're sampling smaller and smaller graphs from
[00:20:20.240 --> 00:20:23.920]   this huge set. And so these smaller graphs essentially
[00:20:23.920 --> 00:20:27.680]   become your training set. But then another sample of this huge
[00:20:27.680 --> 00:20:31.640]   graph is your context representation. Now, why is
[00:20:31.640 --> 00:20:34.600]   context representation useful? Because if you want to do
[00:20:34.600 --> 00:20:38.040]   multitask learning, if you do not have any sense of context,
[00:20:38.040 --> 00:20:41.520]   then the models would essentially not perform well at
[00:20:41.520 --> 00:20:44.760]   all. Because the model is essentially learning some
[00:20:44.760 --> 00:20:47.880]   objective function in some task, and then it is switching to
[00:20:47.880 --> 00:20:52.760]   another task. And then it would drastically not perform at all.
[00:20:53.520 --> 00:20:56.560]   So what would be a context here. So since we are looking at
[00:20:56.560 --> 00:21:00.320]   relation prediction, the context are essentially the relation
[00:21:00.320 --> 00:21:04.480]   embeddings. And since we are providing a world graph, a model
[00:21:04.480 --> 00:21:08.120]   can take that world graph, and then output certain relation
[00:21:08.120 --> 00:21:11.880]   embeddings that would be useful for that particular world. And
[00:21:11.880 --> 00:21:16.440]   then the more other models can take that particular embedding
[00:21:16.440 --> 00:21:21.960]   and then work on top of it for that current world. So it's very
[00:21:21.960 --> 00:21:25.040]   easy to compute similarity, it's very easy to compute out of
[00:21:25.040 --> 00:21:30.320]   domain distribution. And so with this help of 5000 graphs in each
[00:21:30.320 --> 00:21:33.720]   world, number one, you can perform supervised learning as
[00:21:33.720 --> 00:21:37.000]   usual. So the supervised learning essentially consists of
[00:21:37.000 --> 00:21:41.080]   the compositionality test, because in the test set, you are
[00:21:41.080 --> 00:21:46.000]   essentially figuring out the unique compositions of relations
[00:21:46.000 --> 00:21:49.040]   which you have never seen before. Like you are given to
[00:21:49.040 --> 00:21:52.240]   predict the edge or the relationship between a source
[00:21:52.240 --> 00:21:55.120]   and a sink, whereas the composition or the walk between
[00:21:55.120 --> 00:22:00.200]   the source and a sink is unique. And then you can do multitask
[00:22:00.200 --> 00:22:03.680]   learning. On top of all of these worlds, you can train your
[00:22:03.680 --> 00:22:07.080]   models to do a multitask learning, essentially, that
[00:22:07.080 --> 00:22:10.160]   means that you're training on one world at one epoch and
[00:22:10.160 --> 00:22:12.840]   another world and another epoch. So essentially, you're trying to
[00:22:12.840 --> 00:22:18.000]   learn all the world information at the same time. Now, in the
[00:22:18.000 --> 00:22:21.600]   beginning, we said that we had this set of rules generated in
[00:22:21.600 --> 00:22:24.680]   the beginning, and then we are essentially doing a subset on
[00:22:24.680 --> 00:22:28.880]   this set of rules. So that means by doing a multitask learning,
[00:22:28.880 --> 00:22:34.400]   you will never go into the region of invalid rules, because
[00:22:34.400 --> 00:22:38.000]   all the rules are generated at the same time. So all the rules
[00:22:38.000 --> 00:22:41.520]   are already made sure that all the rules are consistent with
[00:22:41.520 --> 00:22:45.400]   each other. So by doing this kind of multitask learning, you
[00:22:45.400 --> 00:22:48.880]   won't be, you won't have to worry about whether the rules
[00:22:48.880 --> 00:22:51.720]   that you're learning in world A are inconsistent with the rules
[00:22:51.720 --> 00:22:54.520]   that you're learning in world B. Sure, these two rules can be
[00:22:54.520 --> 00:22:57.560]   very different. But if you're putting it together in the same
[00:22:57.560 --> 00:23:02.120]   model, there wouldn't be an inconsistency issue. And
[00:23:02.120 --> 00:23:04.880]   finally, you can do continual learning. So this is this is
[00:23:04.880 --> 00:23:09.200]   highly exciting, because our work is one of the first work in
[00:23:09.200 --> 00:23:13.840]   graph data sets, which has this, like added objective of
[00:23:13.840 --> 00:23:18.240]   continual learning. And there's a lot of recent work, exciting
[00:23:18.240 --> 00:23:20.360]   work coming up in continual learning, where people are
[00:23:20.360 --> 00:23:24.640]   trying to figure out how to basically see that whether your
[00:23:24.640 --> 00:23:29.520]   models are learning to remember on previous tasks or not. Given
[00:23:29.520 --> 00:23:34.680]   this set of world splits, we can define each task as individual
[00:23:34.680 --> 00:23:39.200]   world split. And that is our, like proposal of doing continual
[00:23:39.200 --> 00:23:45.440]   learning purely on top of graphs. So before I jump into
[00:23:45.440 --> 00:23:48.240]   the results, so I want to like talk a little bit about the
[00:23:48.240 --> 00:23:51.600]   models that we use. So essentially, we used a lot of
[00:23:51.600 --> 00:23:55.480]   models involving RGCN, which is a popular model used for
[00:23:55.480 --> 00:24:00.040]   relation prediction, and graph attention networks and GCNs
[00:24:00.040 --> 00:24:04.640]   also. Now, as I said, when you're trying to do multitask
[00:24:04.640 --> 00:24:07.920]   representation, you want to have a context. And how do you get
[00:24:07.920 --> 00:24:12.840]   this context? So we basically we divided the model into two
[00:24:12.840 --> 00:24:16.200]   parts. One is a representation function, which takes into
[00:24:16.200 --> 00:24:20.480]   input the world model, or sorry, the world graph. So essentially,
[00:24:20.480 --> 00:24:24.480]   each, each world has a unique world graph. And this model in
[00:24:24.480 --> 00:24:28.840]   just the world graph and outputs the set of relation embeddings,
[00:24:28.840 --> 00:24:33.080]   which are contextual for that current world graph. And then
[00:24:33.080 --> 00:24:36.280]   another function, which is the composition function, which is
[00:24:36.280 --> 00:24:39.360]   also another graph neural network that takes these
[00:24:39.360 --> 00:24:43.120]   relation embeddings, and it takes all the different 5000
[00:24:43.120 --> 00:24:47.040]   graphs and tries to predict the missing relations on top of it.
[00:24:47.040 --> 00:24:53.840]   So in this kind of domain, we took a lot of combinations of
[00:24:53.840 --> 00:24:58.960]   available, like composition and representation functions. So a
[00:24:58.960 --> 00:25:02.040]   very simple representation function can be just a parameter,
[00:25:02.040 --> 00:25:05.520]   or like just a weight lookup, or basically like an embedding
[00:25:05.520 --> 00:25:10.280]   lookup in terms of an NLP. So why that wouldn't work, because
[00:25:10.280 --> 00:25:13.600]   an embedding lookup is essentially learning to, like
[00:25:13.600 --> 00:25:19.160]   back propagate to each relations from all different worlds. So it
[00:25:19.160 --> 00:25:23.920]   basically does not learn any world specific information. So
[00:25:23.920 --> 00:25:28.640]   on top of that, we can optimize that by using a GCN and a graph
[00:25:28.640 --> 00:25:32.120]   attention network as our representation functions, which
[00:25:32.160 --> 00:25:35.920]   take into account the world graph, and then gives you a set
[00:25:35.920 --> 00:25:44.280]   of weights or basically a set of relations to work on top of. So,
[00:25:44.280 --> 00:25:47.520]   and then the composition function is essentially a model
[00:25:47.520 --> 00:25:52.120]   which can predict relations. So this model has to operate on the
[00:25:52.120 --> 00:25:57.240]   edges. And our GCN was one of the one of the really famous
[00:25:57.240 --> 00:26:00.880]   models, which works on top of these relation prediction tasks.
[00:26:01.440 --> 00:26:04.800]   We had to modify GATT. So for the clutter task also we
[00:26:04.800 --> 00:26:09.600]   modified GATT to attend over edge representations. So
[00:26:09.600 --> 00:26:12.680]   originally, GATT or graph attention networks is basically
[00:26:12.680 --> 00:26:16.080]   during the message passing update, the graph attention
[00:26:16.080 --> 00:26:20.000]   network attends to the different messages that are coming in. But
[00:26:20.000 --> 00:26:23.400]   in our modified edge GATT, the graph attention network is
[00:26:23.400 --> 00:26:26.640]   attending to both the messages that are coming in and also to
[00:26:26.640 --> 00:26:31.400]   the edge representation of the source and the sink. So this
[00:26:31.400 --> 00:26:34.760]   edge GATT is also the same model that we used in clutter and
[00:26:34.760 --> 00:26:40.240]   that's why we use it here also. So how do the models fare on
[00:26:40.240 --> 00:26:44.240]   generalizability? So when we do supervised learning, we find a
[00:26:44.240 --> 00:26:49.040]   nice, like a separation of difficulty of different worlds.
[00:26:49.040 --> 00:26:52.600]   So this difficulty is essentially like a separation
[00:26:52.600 --> 00:26:57.280]   based on the number of unique composition or descriptors
[00:26:57.280 --> 00:27:00.680]   available in each world. So in some of the worlds, the number
[00:27:00.680 --> 00:27:05.920]   of compositions are not enough. So essentially, for each of the
[00:27:05.920 --> 00:27:09.360]   graphs that we generate, these graphs also has the set of noise
[00:27:09.360 --> 00:27:13.000]   that you have seen before. So in clutter that the types of noise
[00:27:13.000 --> 00:27:16.840]   that we generate, we also use the same types of noise here. So
[00:27:16.840 --> 00:27:20.120]   that adds a lot of other difficulty and these
[00:27:20.120 --> 00:27:23.760]   procedurally generated worlds, when we do a supervised
[00:27:23.760 --> 00:27:27.720]   learning, then we have a nice, like proxy, which is the
[00:27:27.840 --> 00:27:33.520]   supervised learning scores. And these scores are basically very
[00:27:33.520 --> 00:27:36.840]   similar to each of the different models. And that gives you how
[00:27:36.840 --> 00:27:42.800]   how much easier difficult each world are. Now, now we do
[00:27:42.800 --> 00:27:46.720]   multitask learning. Now in multitask learning, we increase
[00:27:46.720 --> 00:27:49.920]   the number of worlds and then we see that we hit a saturation
[00:27:49.920 --> 00:27:55.240]   point at 20 worlds. So that can be an outcome of the number of
[00:27:55.240 --> 00:27:59.200]   parameters that we had in our model. Like when if we probably
[00:27:59.200 --> 00:28:02.000]   if we increase the number of parameters, we can increase this
[00:28:02.000 --> 00:28:05.760]   number of saturation. But if you see the accuracy, the even at
[00:28:05.760 --> 00:28:09.880]   number 20 worlds, the highest accuracy is 60%. Whereas in
[00:28:09.880 --> 00:28:15.400]   individual worlds, the accuracy goes towards 80 to 85%. And then
[00:28:15.400 --> 00:28:20.840]   we also did multitask learning on the easy and on similar and
[00:28:20.840 --> 00:28:24.040]   dissimilar splits. So essentially, S is the similar
[00:28:24.040 --> 00:28:28.280]   split where we took the worlds, which have a similar set of
[00:28:28.280 --> 00:28:31.120]   rules, like which are not different, too much from each
[00:28:31.120 --> 00:28:35.320]   other. And D is a set of worlds which have very dissimilar set
[00:28:35.320 --> 00:28:39.920]   of rules. And then in both of the cases, we find that graph
[00:28:39.920 --> 00:28:43.600]   attention network and edge gap combination is performing the
[00:28:43.600 --> 00:28:49.360]   best out of all different baseline. So now we look into
[00:28:49.360 --> 00:28:53.360]   multitask adaptation. So in multitask adaptation, we train
[00:28:53.680 --> 00:28:57.760]   using the multitask objective, and then we adapt based on the
[00:28:57.760 --> 00:29:00.960]   weights learned from this multitask objective. And we want
[00:29:00.960 --> 00:29:04.200]   to see two things. First, we want to see the final
[00:29:04.200 --> 00:29:08.040]   performance after adaptation. And then when we want to see the
[00:29:08.040 --> 00:29:13.240]   adaptation speed, and during this, so just a clarification,
[00:29:13.240 --> 00:29:16.600]   this is not traditional meta learning, this is just simple,
[00:29:16.600 --> 00:29:19.920]   multitask learning, and then you're adapting on the weights
[00:29:19.920 --> 00:29:22.600]   that are learned on the multitask learning model, which
[00:29:22.600 --> 00:29:25.680]   is very similar to what we do in transformer models where you
[00:29:25.680 --> 00:29:29.200]   will do pre training, and then you basically fine tune on your
[00:29:29.200 --> 00:29:34.760]   given task. So in the in the, in the graph you see on the left,
[00:29:34.760 --> 00:29:40.400]   on the orange, like bar is one that we represent, which has
[00:29:40.400 --> 00:29:44.520]   very dissimilar set of rules. And the blue bar is the one
[00:29:44.520 --> 00:29:48.520]   where we have a very similar set of rules. And what we see that
[00:29:48.520 --> 00:29:52.600]   for most of the models, when we do multitask training on
[00:29:52.600 --> 00:29:57.480]   dissimilar set of worlds, then the final adaptation is much
[00:29:57.480 --> 00:30:01.400]   better than when we train on similar set of words. So this is
[00:30:01.400 --> 00:30:05.720]   a very nice finding. And that kind of correlates with a lot of
[00:30:05.720 --> 00:30:08.560]   pre training literature, that whenever you're doing pre
[00:30:08.560 --> 00:30:13.040]   training, you should try to incorporate as much dissimilar
[00:30:13.040 --> 00:30:17.360]   set data points as possible in your pre training pipeline. And
[00:30:17.360 --> 00:30:21.640]   similarly, we see that when we look at the gradient updates,
[00:30:21.640 --> 00:30:26.920]   the models which are trained on dissimilar set of words are
[00:30:26.920 --> 00:30:32.600]   faster in adaptation. And finally, we come into the
[00:30:32.600 --> 00:30:36.920]   results of continual learning. In continual learning, in we
[00:30:36.920 --> 00:30:41.080]   have two plots. So the blue plot is essentially the current task
[00:30:41.080 --> 00:30:45.280]   performance. And the orange plot is the performance of the mean
[00:30:45.560 --> 00:30:49.320]   previous task performances. So the mean of the previous task
[00:30:49.320 --> 00:30:54.360]   performance is basically falls down rapidly. And also it is
[00:30:54.360 --> 00:30:58.960]   pretty bad. And that is where we need to like improve a lot in
[00:30:58.960 --> 00:31:03.000]   graph literature. And this data set essentially gives you a
[00:31:03.000 --> 00:31:06.760]   very, very challenging benchmark to do continual learning. So in
[00:31:06.760 --> 00:31:11.560]   the second plot, I'm showing the continual learning results when
[00:31:11.560 --> 00:31:16.640]   we ordered the words in terms of its difficulty. So recall that
[00:31:16.640 --> 00:31:19.640]   we got a difficulty score while doing the supervised learning
[00:31:19.640 --> 00:31:22.200]   training. So we ordered the models according to the
[00:31:22.200 --> 00:31:26.520]   difficulty score. And then we see that the single task
[00:31:26.520 --> 00:31:31.160]   performance is according to the model, but then the mean of
[00:31:31.160 --> 00:31:35.120]   previous task performance is also pretty bad. And that needs
[00:31:35.120 --> 00:31:38.840]   to be done. So basically, there's a lot of grounds to
[00:31:38.840 --> 00:31:42.400]   cover in terms of continual learning. And we believe this is
[00:31:42.400 --> 00:31:47.000]   a really, really challenging benchmark for it. So essentially
[00:31:47.000 --> 00:31:51.280]   the key takeaways in graph log is that training on diverse
[00:31:51.280 --> 00:31:54.920]   data sets improve the generalization performance. And
[00:31:54.920 --> 00:31:58.600]   pre training on diverse data sets also help to adapt to
[00:31:58.600 --> 00:32:02.440]   newer data sets. And pre training on difficult data sets
[00:32:02.440 --> 00:32:06.560]   improve the model adaptation ability. So difficult also
[00:32:06.560 --> 00:32:09.720]   includes like dissimilar data sets. But if you read the paper,
[00:32:09.720 --> 00:32:15.200]   we also have a topic on how easy and difficult training works in
[00:32:15.200 --> 00:32:19.280]   these kinds of setups. And we have we essentially have a lot
[00:32:19.280 --> 00:32:22.880]   of work to do in order to perform true lifelong learning
[00:32:22.880 --> 00:32:29.000]   in graphs. So in the in the conclusion, we I show you like
[00:32:29.000 --> 00:32:32.280]   basically a snapshot of the two data sets that I've been worked
[00:32:32.280 --> 00:32:36.720]   on on the last couple of years. And essentially, one of them
[00:32:36.720 --> 00:32:39.520]   focuses on the modality of graphs while the other focuses
[00:32:39.520 --> 00:32:44.040]   on the modality of text. And for both of them, I have like
[00:32:44.040 --> 00:32:47.080]   released blog posts. And for graph log, we have released an
[00:32:47.080 --> 00:32:52.720]   API, which is really easy to install pip installable file. We
[00:32:52.720 --> 00:32:56.880]   also are looking to release the similar kind of API for clutter
[00:32:56.880 --> 00:33:00.560]   as well. And using this API, you don't have to worry about data
[00:33:00.560 --> 00:33:04.360]   loading. So I provide data loaders for you and easy to use
[00:33:04.360 --> 00:33:08.920]   experiments. And also easy to use scripts written in pytorch
[00:33:08.920 --> 00:33:13.200]   lightning for you to like, run your experiments just like that.
[00:33:13.200 --> 00:33:18.520]   And we have we have a generator module for both of these data
[00:33:18.520 --> 00:33:22.000]   sets, the generator module for clutter is already out there, we
[00:33:22.000 --> 00:33:25.520]   will be doing a lot of like improvements based on user
[00:33:25.520 --> 00:33:29.200]   feedback on top of it. And also the generator module for graph
[00:33:29.200 --> 00:33:35.520]   log is coming out very soon. So yeah, that's mostly it. I thanks
[00:33:35.520 --> 00:33:40.280]   to my awesome collaborators, Shagun Sodhani, Jin for clutter,
[00:33:40.280 --> 00:33:45.040]   Joel and Will, my supervisors, and also thank you to my
[00:33:45.040 --> 00:33:48.320]   amazing colleagues at Mila and Fair Montreal for all of their
[00:33:48.320 --> 00:33:53.040]   support. And if you have any questions, you can ask me now or
[00:33:53.040 --> 00:33:58.120]   you can like send me a mail. Yeah. And also you can like join
[00:33:58.120 --> 00:34:01.920]   our logical ml slack, which I'm open to answer more questions.
[00:34:01.920 --> 00:34:03.000]   Thanks.
[00:34:03.000 --> 00:34:11.000]   Thanks. Thank you so much. So we have two questions already. So
[00:34:11.000 --> 00:34:14.160]   go ahead and do that. And then the YouTube folks, you can start
[00:34:14.160 --> 00:34:17.040]   asking your questions on YouTube, and we'll get those for
[00:34:17.040 --> 00:34:22.440]   you as well. Alright, so Mohammed asks, what if the
[00:34:22.440 --> 00:34:26.520]   relation is not defined from the text? Would the model still be
[00:34:26.520 --> 00:34:28.480]   forced to predict the value of the edge?
[00:34:28.480 --> 00:34:33.040]   So yeah, that's a very good question. We made sure that the
[00:34:33.040 --> 00:34:37.560]   relations are always defined in the text. So that kind of error
[00:34:37.560 --> 00:34:41.600]   do creep in if you're working a lot with Amazon Mechanical Turk.
[00:34:41.600 --> 00:34:45.840]   And it's a very messy situation out there because you have to do
[00:34:45.840 --> 00:34:49.640]   a lot of like cleaning on the data that you collect. So we
[00:34:49.640 --> 00:34:52.680]   made sure that the data that we collected or the templates that
[00:34:52.680 --> 00:34:57.560]   we collected has all the relation identifiers. And
[00:34:57.560 --> 00:35:00.400]   without them, you cannot like essentially solve the puzzle. So
[00:35:00.400 --> 00:35:02.880]   we made sure that you have the relation IDs.
[00:35:02.880 --> 00:35:09.360]   And then the second question is, rules are super interesting. Is
[00:35:09.360 --> 00:35:13.240]   it possible to inject custom rules into experiments to see
[00:35:13.240 --> 00:35:17.160]   how they perform? Is it also possible? Okay, yeah, just
[00:35:17.160 --> 00:35:17.720]   answer that.
[00:35:17.720 --> 00:35:20.520]   Yeah. So that's a really nice question. And yes, it is
[00:35:20.520 --> 00:35:24.480]   possible. For both of the data sets for clutter, you can just
[00:35:24.480 --> 00:35:28.600]   easily like edit a yaml file and do the generations. So in the
[00:35:28.600 --> 00:35:31.960]   yaml file, the way I define the rules is it's essentially a
[00:35:31.960 --> 00:35:35.200]   first order horn clause. So the horn clause is essentially a
[00:35:35.200 --> 00:35:39.800]   rule which has like a body and a head where the body composed of
[00:35:39.800 --> 00:35:43.920]   like two entities and the head is the output of that entity. So
[00:35:43.920 --> 00:35:47.680]   you can inject rules in that fashion. In case of graph log,
[00:35:47.720 --> 00:35:51.200]   we are essentially generating rules on the fly. So we are not
[00:35:51.200 --> 00:35:55.080]   using a single yaml file. So in case of graph log, what you can
[00:35:55.080 --> 00:35:58.400]   do, you can tweak with that rule generation parameters. So
[00:35:58.400 --> 00:36:01.080]   there's a lot of parameters which says that, okay, how many
[00:36:01.080 --> 00:36:05.160]   symmetric rules we will have, how many, like invertible rules
[00:36:05.160 --> 00:36:08.680]   we will have, and so on, you can increase the set of rules like
[00:36:08.680 --> 00:36:12.880]   do a lot like, if you want to do massive experiments, you can
[00:36:12.880 --> 00:36:16.040]   just play with them, like both are essentially data generators.
[00:36:16.880 --> 00:36:21.440]   Thanks. And then a related question, is it possible to
[00:36:21.440 --> 00:36:24.800]   extract rules created and understood by the model?
[00:36:24.800 --> 00:36:30.920]   Yes. So you, your question is whether it's possible to extract
[00:36:30.920 --> 00:36:34.920]   the rules that are understood by the model. So this is a very
[00:36:34.920 --> 00:36:38.200]   interesting research direction on its own. For graph neural
[00:36:38.200 --> 00:36:41.000]   networks, there is a like several papers which look
[00:36:41.000 --> 00:36:44.360]   exactly at this problem, like using graph attention networks,
[00:36:44.360 --> 00:36:48.160]   they try to like predict which of the edges that the attention
[00:36:48.160 --> 00:36:51.640]   are like kind of highest. And those are the edges that the
[00:36:51.640 --> 00:36:54.800]   model is kind of perceiving. We haven't done that kind of
[00:36:54.800 --> 00:36:57.600]   experiments in here. But that would be a really interesting
[00:36:57.600 --> 00:36:58.760]   way to look at.
[00:36:58.760 --> 00:37:03.320]   Those are all the questions, but
[00:37:03.320 --> 00:37:08.520]   follow up or related question actually to Manny's. So you
[00:37:08.520 --> 00:37:13.280]   presented in for the clutter work, you presented some stuff
[00:37:13.280 --> 00:37:17.920]   about generalization to relations like longer sequences
[00:37:17.920 --> 00:37:24.400]   of relations reasoning to longer. So back in the early 90s, some
[00:37:24.400 --> 00:37:28.320]   folks did work with RNNs to do addition, and you used addition
[00:37:28.320 --> 00:37:31.000]   as an example of a similar problem. And they looked at how
[00:37:31.000 --> 00:37:36.920]   long, like how far out could they, like how long digits could
[00:37:36.920 --> 00:37:39.800]   they could they add together, and they were able to extract
[00:37:39.800 --> 00:37:43.120]   from the RNN, an actual dynamical system that the RNN
[00:37:43.120 --> 00:37:47.320]   was implementing approximately, that could do, that could do
[00:37:47.320 --> 00:37:51.360]   addition to basically infinite precision. Basically, you know,
[00:37:51.360 --> 00:37:55.160]   like it was a they could find a set of weights that could then
[00:37:55.160 --> 00:37:59.600]   do even better than the network had done on the task. So that's
[00:37:59.600 --> 00:38:02.480]   that use some of this sort of specific ideas of RNNs. But I'm
[00:38:02.480 --> 00:38:05.400]   curious if for some of these like logical ML things that get
[00:38:05.400 --> 00:38:10.440]   closer to, you know, traditional computation, if there's any hope
[00:38:10.440 --> 00:38:14.160]   that the networks that we're building and learning that we
[00:38:14.160 --> 00:38:17.960]   might be able to extract back out from them, a an algorithm,
[00:38:17.960 --> 00:38:20.600]   something interpretable, something that's like, compilable
[00:38:20.600 --> 00:38:23.040]   down to traditional code, or whether you think that that's a
[00:38:23.040 --> 00:38:24.440]   gap that can't be crossed.
[00:38:24.440 --> 00:38:28.760]   No, that that is a that is also like an exciting research
[00:38:28.760 --> 00:38:31.640]   direction, what people do in program synthesis. So
[00:38:31.640 --> 00:38:35.360]   essentially, you are trying to like generate the programs which
[00:38:35.360 --> 00:38:38.520]   you are essentially you're trying to optimize the programs
[00:38:38.520 --> 00:38:41.440]   that you generate and on your particular task. So there is a
[00:38:41.440 --> 00:38:46.560]   lot of research going on there. And I am not super familiar with
[00:38:46.560 --> 00:38:50.840]   the research literature, but the way you mentioned it, it would
[00:38:50.840 --> 00:38:54.920]   be like possible, like, you essentially generate a program
[00:38:54.920 --> 00:38:58.280]   that will do kind of recursive computation on top of this data
[00:38:58.280 --> 00:39:02.040]   set. And that recursive computation model is basically
[00:39:02.040 --> 00:39:06.120]   what we need. But that is also like super hard. And I would
[00:39:06.120 --> 00:39:08.960]   also like term that a system to deep learning. And that is what
[00:39:08.960 --> 00:39:10.320]   we want to move towards.
[00:39:10.320 --> 00:39:17.120]   Thanks. We have one more question from him. For those
[00:39:17.120 --> 00:39:19.520]   who are not familiar with a graph neural networks, you have
[00:39:19.520 --> 00:39:22.880]   this sequence of papers that you recommend that people read, or
[00:39:22.880 --> 00:39:24.080]   other resources?
[00:39:24.080 --> 00:39:28.600]   Sure. For graph neural networks, I would suggest to start from
[00:39:28.600 --> 00:39:34.640]   the basic GCN architecture by Thomas Giff. And then you can
[00:39:34.640 --> 00:39:38.200]   look at Peter Velikovic graph attention network, which is
[00:39:38.200 --> 00:39:42.360]   really popular. If you want a condensed survey. So my
[00:39:42.360 --> 00:39:46.160]   supervisor, Will Hamilton is will be releasing a book very,
[00:39:46.160 --> 00:39:49.680]   very soon. So we are super excited, like we actually like
[00:39:49.680 --> 00:39:54.120]   use that book for our course this semester. So that book will
[00:39:54.120 --> 00:39:57.880]   contain like a really, really nice in depth overview of the
[00:39:57.880 --> 00:40:01.520]   entire theory and the applications. So just look out
[00:40:01.520 --> 00:40:04.320]   in his website that that's all I can say,
[00:40:04.320 --> 00:40:07.640]   is a publishing deadline for that yet? Or like, when is it
[00:40:07.640 --> 00:40:08.160]   coming out?
[00:40:08.160 --> 00:40:11.880]   That I don't know, I have to ask him. But it should be should be
[00:40:11.880 --> 00:40:16.880]   coming out soon. You can actually find the online draft
[00:40:16.880 --> 00:40:21.800]   on Will's webpage. So we'll like Will's and Will's webpage will
[00:40:21.800 --> 00:40:27.160]   have a like a link towards his course, which is called 767. And
[00:40:27.200 --> 00:40:31.200]   in that, or it was 766. Yeah. So basically, in that course, you
[00:40:31.200 --> 00:40:35.720]   will find the list of notes. And those notes are ones that he
[00:40:35.720 --> 00:40:37.280]   will compile towards his book.
[00:40:37.280 --> 00:40:44.240]   All right. Those are all the questions I see. Thanks, Charles
[00:40:44.240 --> 00:40:49.120]   for sharing the link. Kostov will be in our sec community at
[00:40:49.120 --> 00:40:53.000]   9am on Friday. answering more questions, we'll dive deeper
[00:40:53.000 --> 00:40:58.520]   into a scape. It's gonna be a great time. So yeah, thank you
[00:40:58.520 --> 00:40:59.320]   for coming.
[00:40:59.320 --> 00:41:01.080]   Thank you so much. Thank you.

