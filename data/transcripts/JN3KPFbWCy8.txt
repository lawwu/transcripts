
[00:00:00.000 --> 00:00:07.020]   The following is a conversation with Elon Musk, his fourth time on this, the Lex Friedman podcast.
[00:00:07.020 --> 00:00:21.700]   I thought you were going to finish it.
[00:00:21.700 --> 00:00:28.000]   It's one of the greatest themes in all of film history.
[00:00:28.000 --> 00:00:32.040]   Yeah, it's great.
[00:00:32.040 --> 00:00:36.700]   So I was just thinking about the Roman Empire, as one does.
[00:00:36.700 --> 00:00:44.380]   There's that whole meme where old guys are thinking about the Roman Empire at least once a day.
[00:00:44.380 --> 00:00:47.100]   And half the population is confused whether it's true or not.
[00:00:47.100 --> 00:00:52.240]   But more seriously, thinking about the wars going on in the world today.
[00:00:52.240 --> 00:00:56.920]   And as you know, war and military conquest has been a
[00:00:56.920 --> 00:00:57.980]   big part of the Roman Empire.
[00:00:57.980 --> 00:00:59.580]   It's a part of Roman society and culture.
[00:00:59.580 --> 00:01:05.520]   And it, I think, has been a big part of most empires and dynasties throughout human history.
[00:01:05.520 --> 00:01:09.860]   Yeah, they usually came as a result of conquest.
[00:01:09.860 --> 00:01:15.200]   I mean, there's something like the Austro-Hungarian Empire where there was just a lot of clever marriages.
[00:01:15.200 --> 00:01:18.940]   But fundamentally, there's an engine of conquest.
[00:01:18.940 --> 00:01:22.640]   And they celebrate excellence in warfare.
[00:01:22.640 --> 00:01:25.900]   Many of the leaders were excellent generals.
[00:01:25.900 --> 00:01:26.640]   Yeah.
[00:01:26.640 --> 00:01:27.420]   That kind of thing.
[00:01:28.000 --> 00:01:29.180]   So big picture question.
[00:01:29.180 --> 00:01:30.920]   Grok approved.
[00:01:30.920 --> 00:01:32.660]   I asked if this is a good question to ask.
[00:01:32.660 --> 00:01:33.860]   Get tested, Grok approved.
[00:01:33.860 --> 00:01:36.980]   At least on fun mode.
[00:01:36.980 --> 00:01:49.960]   To what degree do you think war is part of human nature versus a consequence of how human societies are structured?
[00:01:49.960 --> 00:01:56.360]   I ask this as you have somehow controversially been a proponent of peace.
[00:01:56.360 --> 00:01:56.960]   I don't.
[00:01:56.960 --> 00:01:58.760]   I'm generally a proponent of peace.
[00:01:58.760 --> 00:02:03.560]   I mean, ignorance is perhaps, in my view, the real enemy to counter it.
[00:02:03.560 --> 00:02:05.220]   That's the real hard part.
[00:02:05.220 --> 00:02:07.320]   Not, you know, fighting other humans.
[00:02:07.320 --> 00:02:10.220]   But all creatures fight.
[00:02:10.220 --> 00:02:12.700]   I mean, the jungle is a...
[00:02:12.700 --> 00:02:16.880]   People think of nature as perhaps some sort of peaceful thing.
[00:02:16.880 --> 00:02:18.200]   But in fact, it is not.
[00:02:18.200 --> 00:02:23.900]   There's some quite funny Werner Herzog thing where he's like in the jungle,
[00:02:23.900 --> 00:02:25.900]   like saying that it's like basically...
[00:02:26.580 --> 00:02:29.200]   Just murder and death in every direction.
[00:02:29.200 --> 00:02:33.140]   I mean, the plants and animals in the jungle are constantly trying to kill each other.
[00:02:33.140 --> 00:02:34.560]   Every single day.
[00:02:34.560 --> 00:02:35.180]   Every minute.
[00:02:35.180 --> 00:02:40.100]   So it's not like, you know, we're unusual in that respect.
[00:02:40.100 --> 00:02:43.600]   There's a relevant question here whether with greater intelligence
[00:02:43.600 --> 00:02:48.000]   comes greater control over these base instincts for violence.
[00:02:48.000 --> 00:02:49.820]   Yes.
[00:02:49.820 --> 00:02:56.100]   We have much more of an ability to control our limbic instinct.
[00:02:56.200 --> 00:02:58.080]   For violence than, say, a chimpanzee.
[00:02:58.080 --> 00:03:02.760]   And in fact, if one looks at, say, chimpanzee society, it is not friendly.
[00:03:02.760 --> 00:03:05.000]   I mean, the bonobos are an exception.
[00:03:05.000 --> 00:03:11.320]   But chimpanzee society is filled with violence.
[00:03:11.320 --> 00:03:14.640]   And it's quite horrific, frankly.
[00:03:14.640 --> 00:03:17.360]   That's our limbic system in action.
[00:03:17.360 --> 00:03:20.080]   Like, you don't want to be on the wrong side of a chimpanzee.
[00:03:20.080 --> 00:03:21.620]   It'll eat your face off and tear your nuts off.
[00:03:21.620 --> 00:03:24.200]   Yeah, basically, there's no limits or ethics.
[00:03:24.200 --> 00:03:26.180]   They're almost a just war.
[00:03:26.320 --> 00:03:29.200]   There's no just war in chimpanzee societies.
[00:03:29.200 --> 00:03:32.940]   Is war and dominance by any means necessary?
[00:03:32.940 --> 00:03:36.500]   Yeah, chimpanzee society is like a primitive version of human society.
[00:03:36.500 --> 00:03:40.280]   They're not like peace-loving, basically, at all.
[00:03:40.280 --> 00:03:43.240]   There's extreme violence.
[00:03:43.240 --> 00:03:47.960]   And then once in a while, somebody who's watched too many Disney movies
[00:03:47.960 --> 00:03:49.840]   decides to raise a chimpanzee as a pet.
[00:03:49.840 --> 00:03:54.820]   And then that eats their face or rips their nuts off or chews their fingers off
[00:03:54.820 --> 00:03:55.320]   and that kind of thing.
[00:03:55.320 --> 00:03:57.280]   And it's happened several times.
[00:03:57.280 --> 00:04:01.360]   Ripping your nuts off is an interesting strategy for interaction.
[00:04:01.360 --> 00:04:03.020]   It's happened to people.
[00:04:03.020 --> 00:04:03.920]   It's unfortunate.
[00:04:03.920 --> 00:04:07.580]   That's, I guess, one way to ensure that the other chump
[00:04:07.580 --> 00:04:10.260]   doesn't contribute to the gene pool.
[00:04:10.260 --> 00:04:13.380]   Well, from a martial arts perspective, it's a fascinating strategy.
[00:04:13.380 --> 00:04:15.900]   The nut ripper.
[00:04:15.900 --> 00:04:20.580]   I wonder which of the martial arts teaches that.
[00:04:20.580 --> 00:04:25.300]   I think it's safe to say if somebody's got your nuts in their hands,
[00:04:25.300 --> 00:04:25.460]   they're going to be able to do it.
[00:04:25.460 --> 00:04:27.700]   And as the option of ripping them off,
[00:04:27.700 --> 00:04:30.240]   you will be amenable to whatever they want.
[00:04:30.240 --> 00:04:30.640]   Yeah.
[00:04:30.640 --> 00:04:35.140]   So, like I said, somehow controversially,
[00:04:35.140 --> 00:04:38.360]   you've been a proponent of peace on Twitter, on X.
[00:04:38.360 --> 00:04:38.800]   Yeah.
[00:04:38.800 --> 00:04:41.380]   So let me ask you about the wars going on today
[00:04:41.380 --> 00:04:43.820]   and to see what the path to peace could be.
[00:04:43.820 --> 00:04:47.300]   How do you hope the current war in Israel and Gaza comes to an end?
[00:04:47.300 --> 00:04:52.680]   What path do you see that can minimize human suffering in the long term
[00:04:52.680 --> 00:04:53.960]   in that part of the world?
[00:04:53.960 --> 00:04:54.960]   Well, I think...
[00:04:54.960 --> 00:04:58.200]   I think that part of the world is definitely...
[00:04:58.200 --> 00:04:59.300]   Like, if you look up the...
[00:04:59.300 --> 00:05:01.280]   There is no easy answer in the dictionary.
[00:05:01.280 --> 00:05:06.180]   It'll be like the picture of the Middle East, in Israel especially.
[00:05:06.180 --> 00:05:08.020]   So there is no easy answer.
[00:05:08.020 --> 00:05:11.960]   And what my...
[00:05:11.960 --> 00:05:14.960]   This is strictly my opinion of...
[00:05:14.960 --> 00:05:15.960]   You know, is that...
[00:05:15.960 --> 00:05:23.720]   The goal of Hamas was to provoke an overreaction from Israel.
[00:05:23.720 --> 00:05:24.920]   Yeah.
[00:05:24.920 --> 00:05:28.340]   They obviously did not expect to, you know, have a military victory.
[00:05:28.340 --> 00:05:34.520]   But they really wanted to commit the worst atrocities that they could
[00:05:34.520 --> 00:05:40.260]   in order to provoke the most aggressive response possible from Israel.
[00:05:40.260 --> 00:05:45.180]   And then leverage that aggressive response
[00:05:45.180 --> 00:05:54.100]   to rally Muslims worldwide for the cause of Gaza and Palestine,
[00:05:54.100 --> 00:05:54.100]   which...
[00:05:54.100 --> 00:05:56.100]   They have succeeded in doing.
[00:05:56.100 --> 00:05:59.100]   So the...
[00:05:59.100 --> 00:06:02.760]   The counterintuitive thing here,
[00:06:02.760 --> 00:06:04.800]   I think that the thing that I think should be done,
[00:06:04.800 --> 00:06:05.820]   even though it is very difficult,
[00:06:05.820 --> 00:06:10.480]   is that I would recommend that Israel engage
[00:06:10.480 --> 00:06:13.840]   in the most conspicuous acts of kindness possible.
[00:06:13.840 --> 00:06:14.840]   Every part...
[00:06:14.840 --> 00:06:15.180]   Everything.
[00:06:15.180 --> 00:06:18.820]   That is the actual thing that would thwart the goal of Hamas.
[00:06:18.820 --> 00:06:20.960]   So in some sense,
[00:06:20.960 --> 00:06:23.840]   the degree that makes sense in geopolitics
[00:06:23.840 --> 00:06:23.960]   turns out to be the most conspicuous act of kindness possible.
[00:06:23.960 --> 00:06:24.000]   So in some sense, the degree that makes sense in geopolitics
[00:06:24.000 --> 00:06:24.000]   turns out to be the most conspicuous act of kindness possible.
[00:06:24.000 --> 00:06:24.000]   So in some sense, the degree that makes sense in geopolitics
[00:06:24.000 --> 00:06:24.000]   turns out to be the most conspicuous act of kindness possible.
[00:06:24.000 --> 00:06:24.120]   So in some sense, the degree that makes sense in geopolitics
[00:06:24.120 --> 00:06:24.160]   turns out to be the most conspicuous act of kindness possible.
[00:06:24.160 --> 00:06:24.200]   So in some sense, the degree that makes sense in geopolitics
[00:06:24.200 --> 00:06:24.240]   turns out to be the most conspicuous act of kindness possible.
[00:06:24.240 --> 00:06:24.280]   So in some sense, the degree that makes sense in geopolitics
[00:06:24.280 --> 00:06:24.300]   turns out to be the most conspicuous act of kindness possible.
[00:06:24.300 --> 00:06:24.360]   So in some sense, the degree that makes sense in geopolitics
[00:06:24.360 --> 00:06:24.420]   turns out to be the most conspicuous act of kindness possible.
[00:06:24.420 --> 00:06:24.520]   So in some sense, the degree that makes sense in geopolitics
[00:06:24.520 --> 00:06:24.580]   turns out to be the most conspicuous act of kindness possible.
[00:06:24.580 --> 00:06:54.560]   So in some sense, the degree that makes sense in geopolitics
[00:06:54.560 --> 00:06:54.620]   turns out to be the most conspicuous act of kindness possible.
[00:06:54.620 --> 00:06:54.680]   So in some sense, the degree that makes sense in geopolitics
[00:06:54.680 --> 00:06:54.720]   turns out to be the most conspicuous act of kindness possible.
[00:06:54.720 --> 00:06:54.740]   So in some sense, the degree that makes sense in geopolitics
[00:06:54.740 --> 00:06:54.800]   turns out to be the most conspicuous act of kindness possible.
[00:06:54.800 --> 00:06:54.820]   So in some sense, the degree that makes sense in geopolitics
[00:06:54.820 --> 00:06:54.860]   turns out to be the most conspicuous act of kindness possible.
[00:06:54.860 --> 00:06:54.880]   So in some sense, the degree that makes sense in geopolitics
[00:06:54.880 --> 00:06:54.900]   turns out to be the most conspicuous act of kindness possible.
[00:06:54.900 --> 00:06:54.940]   So in some sense, the degree that makes sense in geopolitics
[00:06:54.940 --> 00:06:54.980]   turns out to be the most conspicious act of kindness possible.
[00:06:54.980 --> 00:06:55.060]   So in some sense, the degree that makes sense in geopolitics
[00:06:55.060 --> 00:06:55.100]   turns out to be the most conspicuous act of kindness possible.
[00:06:55.100 --> 00:06:55.240]   So in some sense, the degree that makes sense in geopolitics
[00:06:55.240 --> 00:07:25.220]   turns out to be the most conspicuous act of kindness possible.
[00:07:25.220 --> 00:07:27.960]   or 24/7.
[00:07:27.960 --> 00:07:31.540]   - Deploy acts of kindness.
[00:07:31.540 --> 00:07:34.500]   - Yeah, conspicuous acts of kindness
[00:07:34.500 --> 00:07:39.500]   that are unequivocal, meaning they can't be somehow,
[00:07:39.500 --> 00:07:42.420]   because Hamas will then, their response will be,
[00:07:42.420 --> 00:07:43.600]   "Oh, it's a trick."
[00:07:43.600 --> 00:07:46.240]   Therefore we have to counter how it is not a trick.
[00:07:46.240 --> 00:07:49.440]   - This ultimately fights the broader force
[00:07:49.440 --> 00:07:51.640]   of hatred in the region.
[00:07:51.640 --> 00:07:53.460]   - Yes, and I'm not sure who said it,
[00:07:53.460 --> 00:07:55.120]   but it's an apocryphal saying,
[00:07:55.120 --> 00:07:58.200]   but an eye for an eye makes everyone blind.
[00:07:58.200 --> 00:07:59.260]   Now that neck of the woods,
[00:07:59.260 --> 00:08:02.100]   they really believe in the whole eye for an eye thing.
[00:08:02.100 --> 00:08:07.620]   But I mean, you really have,
[00:08:07.620 --> 00:08:12.800]   if you're not gonna just outright commit genocide,
[00:08:12.800 --> 00:08:14.060]   like against an entire people,
[00:08:14.060 --> 00:08:16.400]   which obviously would not be acceptable to,
[00:08:16.400 --> 00:08:18.400]   really shouldn't be acceptable to anyone,
[00:08:18.400 --> 00:08:22.080]   then you're gonna leave basically a lot of people alive
[00:08:22.080 --> 00:08:23.460]   who subsequently,
[00:08:23.460 --> 00:08:25.140]   hate Israel.
[00:08:25.140 --> 00:08:27.220]   So really the question is like,
[00:08:27.220 --> 00:08:30.440]   how, for every Hamas member that you kill,
[00:08:30.440 --> 00:08:31.540]   how many did you create?
[00:08:31.540 --> 00:08:35.500]   And if you create more than you killed,
[00:08:35.500 --> 00:08:36.720]   you've not succeeded.
[00:08:36.720 --> 00:08:39.040]   That's the real situation there.
[00:08:39.040 --> 00:08:41.620]   And it's safe to say that if,
[00:08:41.620 --> 00:08:50.360]   if you kill somebody's child in Gaza,
[00:08:50.360 --> 00:08:53.440]   you've made a lot of money.
[00:08:53.440 --> 00:08:58.420]   You've made at least a few Hamas members who will die,
[00:08:58.420 --> 00:09:01.960]   just to kill an Israeli.
[00:09:01.960 --> 00:09:05.000]   That's the situation.
[00:09:05.000 --> 00:09:08.620]   So, but, but I mean,
[00:09:08.620 --> 00:09:11.500]   this is one of the most contentious subjects
[00:09:11.500 --> 00:09:12.680]   one could possibly discuss,
[00:09:12.680 --> 00:09:14.960]   but I think if the goal ultimate
[00:09:14.960 --> 00:09:17.460]   is some sort of long-term peace,
[00:09:17.460 --> 00:09:21.840]   one has to be look at this from standpoint of over time,
[00:09:21.840 --> 00:09:23.420]   are there more or fewer,
[00:09:23.420 --> 00:09:25.760]   terrorists being created?
[00:09:25.760 --> 00:09:29.060]   - Let me just linger on war.
[00:09:29.060 --> 00:09:29.880]   - Yeah.
[00:09:29.880 --> 00:09:32.000]   Well, war, it's safe to say wars always existed
[00:09:32.000 --> 00:09:33.140]   and always will exist.
[00:09:33.140 --> 00:09:34.500]   - Always will exist.
[00:09:34.500 --> 00:09:37.440]   - Always has, always has existed and always will exist.
[00:09:37.440 --> 00:09:38.440]   - I hope not.
[00:09:38.440 --> 00:09:39.280]   You think it always will?
[00:09:39.280 --> 00:09:40.680]   - There will always be war.
[00:09:40.680 --> 00:09:45.120]   It's a question of just how much war and, and, you know,
[00:09:45.120 --> 00:09:47.080]   what, you know, there's this,
[00:09:47.080 --> 00:09:49.580]   there's this sort of the scope and scale of war,
[00:09:49.580 --> 00:09:52.780]   but to imagine that there would not be any war in the future,
[00:09:52.780 --> 00:09:55.780]   I think would be very unlikely outcome.
[00:09:55.780 --> 00:09:57.220]   - Yeah, you talked about the culture series.
[00:09:57.220 --> 00:09:58.600]   There's war even there.
[00:09:58.600 --> 00:10:00.020]   - Yes, there's a giant war.
[00:10:00.020 --> 00:10:05.040]   The first book starts off with a gigantic galactic war
[00:10:05.040 --> 00:10:07.260]   where trillions die, trillions.
[00:10:07.260 --> 00:10:09.880]   - But it still nevertheless protects these pockets
[00:10:09.880 --> 00:10:11.420]   of flourishing.
[00:10:11.420 --> 00:10:15.120]   Somehow you can have galactic war
[00:10:15.120 --> 00:10:18.000]   and still have pockets of flourishing.
[00:10:18.000 --> 00:10:21.940]   - Yeah, I mean, it's, I guess if we are able to one day expand,
[00:10:21.940 --> 00:10:26.640]   to, you know, fill the galaxy or whatever,
[00:10:26.640 --> 00:10:30.480]   there will be a galactic war at some point.
[00:10:30.480 --> 00:10:31.680]   - Ah, the scale.
[00:10:31.680 --> 00:10:33.280]   I mean, the scale of war has been increasing,
[00:10:33.280 --> 00:10:35.000]   increasing, increasing.
[00:10:35.000 --> 00:10:37.400]   It's like a race between the scale of suffering
[00:10:37.400 --> 00:10:38.660]   and the scale of flourishing.
[00:10:38.660 --> 00:10:39.500]   - Yes.
[00:10:39.500 --> 00:10:43.800]   - A lot of people seem to be using this tragedy
[00:10:43.800 --> 00:10:45.440]   to beat the drums of war
[00:10:45.440 --> 00:10:47.500]   and feed the military industrial complex.
[00:10:47.500 --> 00:10:48.660]   Do you worry about this?
[00:10:48.660 --> 00:10:51.020]   The people who are,
[00:10:51.020 --> 00:10:54.060]   who are rooting for escalation,
[00:10:54.060 --> 00:10:56.060]   and how can it be stopped?
[00:10:56.060 --> 00:10:57.520]   - One of the things that does concern me
[00:10:57.520 --> 00:10:59.900]   is that there are very few people alive today
[00:10:59.900 --> 00:11:04.140]   who actually viscerally understand the horrors of war,
[00:11:04.140 --> 00:11:04.980]   at least in the US.
[00:11:04.980 --> 00:11:07.080]   I mean, obviously there are people on the front lines
[00:11:07.080 --> 00:11:09.120]   in Ukraine and Russia who understand
[00:11:09.120 --> 00:11:10.900]   just how terrible war is,
[00:11:10.900 --> 00:11:13.380]   but how many people in the West understand it?
[00:11:13.380 --> 00:11:17.720]   You know, my grandfather was in World War II.
[00:11:17.720 --> 00:11:19.140]   He was severely traumatized.
[00:11:19.140 --> 00:11:21.000]   I mean, he was there, I think, in the,
[00:11:21.000 --> 00:11:23.420]   for almost six years in the,
[00:11:23.420 --> 00:11:26.120]   you know, in East and North Africa and Italy.
[00:11:26.120 --> 00:11:29.260]   All his friends were killed in front of him.
[00:11:29.260 --> 00:11:32.220]   And he would have died too,
[00:11:32.220 --> 00:11:34.520]   except they randomly gave some,
[00:11:34.520 --> 00:11:35.900]   I guess IQ test or something.
[00:11:35.900 --> 00:11:38.840]   And he scored very high.
[00:11:38.840 --> 00:11:40.600]   Now he was not an officer.
[00:11:40.600 --> 00:11:42.040]   He was, I think, a corporal or a sergeant
[00:11:42.040 --> 00:11:43.440]   or something like that,
[00:11:43.440 --> 00:11:45.360]   because he didn't finish high school.
[00:11:45.360 --> 00:11:46.560]   He had to drop out of high school
[00:11:46.560 --> 00:11:50.020]   'cause his dad died and he had to work to support his,
[00:11:50.020 --> 00:11:51.540]   his siblings.
[00:11:51.540 --> 00:11:54.140]   So because he didn't graduate high school,
[00:11:54.140 --> 00:11:56.240]   he was not eligible for the officer corps.
[00:11:56.240 --> 00:12:00.520]   So, you know, he kind of got put
[00:12:00.520 --> 00:12:03.120]   into the cannon fodder category basically.
[00:12:03.120 --> 00:12:07.220]   But then just randomly they gave him this test.
[00:12:07.220 --> 00:12:09.800]   He was transferred to British intelligence in London.
[00:12:09.800 --> 00:12:11.700]   That's where he met my grandmother.
[00:12:11.700 --> 00:12:17.460]   But he had PTSD next level, like next level.
[00:12:17.460 --> 00:12:19.820]   I mean, just didn't talk.
[00:12:19.820 --> 00:12:21.220]   Just didn't talk.
[00:12:21.220 --> 00:12:22.160]   And if you tried talking to him,
[00:12:22.160 --> 00:12:23.600]   he'd just tell you to shut up.
[00:12:23.600 --> 00:12:25.480]   And he won a bunch of medals.
[00:12:25.480 --> 00:12:28.560]   Never, never ragged about it once.
[00:12:28.560 --> 00:12:30.000]   Not even hinted, nothing.
[00:12:30.000 --> 00:12:31.120]   I like found out about it
[00:12:31.120 --> 00:12:33.860]   because his military records were online.
[00:12:33.860 --> 00:12:36.320]   That's how I know.
[00:12:36.320 --> 00:12:39.840]   So he would say like, no, no way in hell.
[00:12:39.840 --> 00:12:41.620]   Do you want to, do you want to do that again?
[00:12:41.620 --> 00:12:42.580]   But how many people,
[00:12:42.580 --> 00:12:46.820]   now he obviously, he died, you know, 20 years ago,
[00:12:46.820 --> 00:12:48.380]   longer actually, 30 years ago.
[00:12:48.380 --> 00:12:49.420]   So, yeah.
[00:12:49.420 --> 00:12:53.100]   How many people are alive that remember World War II?
[00:12:53.100 --> 00:12:53.940]   Not many.
[00:12:53.940 --> 00:12:56.000]   - And the same perhaps applies
[00:12:56.000 --> 00:12:58.140]   to the threat of nuclear war.
[00:12:58.140 --> 00:13:01.980]   - Yeah.
[00:13:01.980 --> 00:13:03.120]   I mean, there are enough nuclear bombs pointed
[00:13:03.120 --> 00:13:06.460]   at the United States to make the rebel,
[00:13:06.460 --> 00:13:08.840]   the radioactive rebel bounce many times.
[00:13:08.840 --> 00:13:14.320]   - There's two major wars going on right now.
[00:13:14.320 --> 00:13:17.860]   So you talked about the threat of AGI quite a bit,
[00:13:17.860 --> 00:13:18.140]   but now, as we see, we're talking about the threat of AGI.
[00:13:18.140 --> 00:13:18.140]   - Yeah.
[00:13:18.140 --> 00:13:18.140]   - But now, as we see, we're talking about the threat of AGI.
[00:13:18.140 --> 00:13:18.140]   - Yeah.
[00:13:18.140 --> 00:13:18.140]   - But now, as we see, we're talking about the threat of AGI.
[00:13:18.140 --> 00:13:18.140]   - Yeah.
[00:13:18.140 --> 00:13:18.380]   - But now, as we see, we're talking about the threat of AGI.
[00:13:18.380 --> 00:13:18.380]   - Yeah.
[00:13:18.380 --> 00:13:18.380]   - But now, as we see, we're talking about the threat of AGI.
[00:13:18.380 --> 00:13:18.400]   - But now, as we see, we're talking about the threat of AGI.
[00:13:18.400 --> 00:13:18.480]   - But now, as we see, we're talking about the threat of AGI.
[00:13:18.480 --> 00:13:18.480]   - But now, as we see, we're talking about the threat of AGI.
[00:13:18.480 --> 00:13:18.480]   - But now, as we see, we're talking about the threat of AGI.
[00:13:18.480 --> 00:13:19.940]   - But now, as we see, we're talking about the threat of AGI. - Now, as we see here,
[00:13:19.940 --> 00:13:22.140]   with the intensity of conflict going on,
[00:13:22.140 --> 00:13:23.760]   do you worry about nuclear war?
[00:13:23.760 --> 00:13:26.540]   - I think we shouldn't discount
[00:13:26.540 --> 00:13:28.080]   the possibility of nuclear war.
[00:13:28.080 --> 00:13:30.780]   It is a civilizational threat.
[00:13:30.780 --> 00:13:37.660]   Right now, I could be wrong,
[00:13:37.660 --> 00:13:40.380]   but I think the current probability
[00:13:40.380 --> 00:13:42.020]   of nuclear war is quite low.
[00:13:42.020 --> 00:13:46.360]   But there are a lot of nukes pointed at us.
[00:13:46.360 --> 00:13:47.340]   So, and we have a lot of nukes pointed at us,
[00:13:47.340 --> 00:13:50.300]   and we have a lot of nukes pointed at other people.
[00:13:50.300 --> 00:13:51.520]   They're still there.
[00:13:51.520 --> 00:13:53.840]   Nobody's put their guns away.
[00:13:53.840 --> 00:13:57.560]   The missiles are still in the silos.
[00:13:57.560 --> 00:13:59.780]   - And the leaders don't seem to be
[00:13:59.780 --> 00:14:02.960]   the ones with the nukes talking to each other.
[00:14:02.960 --> 00:14:04.220]   - No.
[00:14:04.220 --> 00:14:06.920]   There are wars which are tragic and difficult
[00:14:06.920 --> 00:14:09.320]   on a local basis, and then there are wars
[00:14:09.320 --> 00:14:12.700]   which are civilization ending, or have that potential.
[00:14:12.700 --> 00:14:14.800]   Obviously, global thermonuclear warfare
[00:14:14.800 --> 00:14:17.280]   has high potential to end civilization.
[00:14:17.280 --> 00:14:22.280]   Perhaps permanently, but certainly to severely wound
[00:14:22.280 --> 00:14:29.520]   and perhaps set back human progress
[00:14:29.520 --> 00:14:33.620]   by the Stone Age or something.
[00:14:33.620 --> 00:14:34.620]   I don't know.
[00:14:34.620 --> 00:14:35.440]   Pretty bad.
[00:14:35.440 --> 00:14:40.780]   Probably scientists and engineers
[00:14:40.780 --> 00:14:43.180]   won't be super popular after that as well.
[00:14:43.180 --> 00:14:44.640]   Like, you got us into this mess.
[00:14:44.640 --> 00:14:46.000]   - Mm-hmm.
[00:14:46.000 --> 00:14:47.240]   - So.
[00:14:47.240 --> 00:14:49.320]   - Generally, I think we obviously want
[00:14:49.320 --> 00:14:51.020]   to prioritize civilizational risks
[00:14:51.020 --> 00:14:55.780]   over things that are painful and tragic on a local level,
[00:14:55.780 --> 00:14:57.060]   but not civilizational.
[00:14:57.060 --> 00:15:01.680]   - How do you hope the war in Ukraine
[00:15:01.680 --> 00:15:04.160]   comes to an end, and what's the path, once again,
[00:15:04.160 --> 00:15:06.060]   to minimizing human suffering there?
[00:15:06.060 --> 00:15:13.140]   - Well, I think that what is likely to happen,
[00:15:13.140 --> 00:15:17.120]   which is really pretty much the way it is,
[00:15:17.120 --> 00:15:22.120]   is that something very close to the current lines
[00:15:22.120 --> 00:15:28.400]   will be how a ceasefire or truce happens.
[00:15:28.400 --> 00:15:33.820]   But you just have a situation right now
[00:15:33.820 --> 00:15:36.140]   where whoever goes on the offensive
[00:15:36.140 --> 00:15:38.900]   will suffer casualties at several times the rate
[00:15:38.900 --> 00:15:40.240]   of whoever's on the defense.
[00:15:40.240 --> 00:15:45.040]   'Cause you've got defense in depth, you've got minefields,
[00:15:45.040 --> 00:15:47.000]   trenches, anti-tank defenses.
[00:15:47.000 --> 00:15:50.480]   Nobody has air superiority,
[00:15:50.480 --> 00:15:53.080]   'cause the anti-aircraft missiles
[00:15:53.080 --> 00:15:55.540]   are really far better than the aircraft.
[00:15:55.540 --> 00:15:57.080]   Like, there are far more of them.
[00:15:57.080 --> 00:16:02.100]   And so neither side has air superiority.
[00:16:02.100 --> 00:16:03.660]   Tanks are basically death traps,
[00:16:03.660 --> 00:16:07.820]   just slow moving, and they're not immune
[00:16:07.820 --> 00:16:09.780]   to anti-tank weapons.
[00:16:09.780 --> 00:16:14.780]   So you really just have long range artillery and infantry.
[00:16:16.880 --> 00:16:17.720]   - And trenches.
[00:16:17.720 --> 00:16:19.560]   It's World War I, all over again.
[00:16:19.560 --> 00:16:20.400]   - Mm-hmm.
[00:16:20.400 --> 00:16:22.280]   - With drones, you know,
[00:16:22.280 --> 00:16:25.160]   throwing little drones, some drones there.
[00:16:25.160 --> 00:16:26.660]   - Which makes the long range artillery
[00:16:26.660 --> 00:16:28.760]   just that much more accurate and better,
[00:16:28.760 --> 00:16:32.380]   and so more efficient at murdering people on both sides.
[00:16:32.380 --> 00:16:33.820]   - Mm-hmm.
[00:16:33.820 --> 00:16:34.660]   Yeah.
[00:16:34.660 --> 00:16:38.360]   So it's, whoever is, you don't wanna be trying
[00:16:38.360 --> 00:16:39.880]   to advance from either side,
[00:16:39.880 --> 00:16:42.320]   'cause the probability of dying is incredibly high.
[00:16:42.320 --> 00:16:45.620]   So in order to overcome,
[00:16:46.760 --> 00:16:51.760]   defense in depth trenches and minefields,
[00:16:51.760 --> 00:16:56.820]   you really need significant local superiority in numbers.
[00:16:56.820 --> 00:17:02.680]   Ideally combined arms, where you do a fast attack
[00:17:02.680 --> 00:17:07.200]   with aircraft, a concentrated number of tanks,
[00:17:07.200 --> 00:17:10.080]   and a lot of people.
[00:17:10.080 --> 00:17:12.320]   That's the only way you're gonna punch through a line.
[00:17:12.320 --> 00:17:16.640]   And then you're gonna punch through and then not have,
[00:17:16.640 --> 00:17:19.820]   reinforcements just kick you right out again.
[00:17:19.820 --> 00:17:23.140]   I mean, I really recommend people read
[00:17:23.140 --> 00:17:27.400]   World War I warfare in detail.
[00:17:27.400 --> 00:17:29.340]   That's rough.
[00:17:29.340 --> 00:17:34.660]   I mean, the sheer number of people that died there
[00:17:34.660 --> 00:17:36.120]   was mind-boggling.
[00:17:36.120 --> 00:17:40.680]   - And it's almost impossible to imagine the end of it
[00:17:40.680 --> 00:17:43.620]   that doesn't look like, almost exactly like the beginning,
[00:17:43.620 --> 00:17:46.520]   in terms of what land belongs to who.
[00:17:46.520 --> 00:17:48.980]   And so on.
[00:17:48.980 --> 00:17:52.600]   But on the other side of a lot of human suffering,
[00:17:52.600 --> 00:17:56.020]   death and destruction of infrastructure.
[00:17:56.020 --> 00:17:56.860]   - Yes.
[00:17:56.860 --> 00:17:58.240]   I mean, the thing that,
[00:17:58.240 --> 00:18:01.640]   the reason I, you know, proposed a,
[00:18:01.640 --> 00:18:06.540]   some sort of crucial or peace a year ago was because I've
[00:18:06.540 --> 00:18:10.220]   predicted pretty much exactly what would happen,
[00:18:10.220 --> 00:18:13.060]   which is a lot of people dying for basically
[00:18:13.060 --> 00:18:14.720]   almost no changes in land.
[00:18:16.400 --> 00:18:21.400]   And the loss of the flower of Ukrainian and Russian youth.
[00:18:21.400 --> 00:18:24.860]   And we should have some sympathy for the Russian boys
[00:18:24.860 --> 00:18:26.260]   as well as the Ukrainian boys.
[00:18:26.260 --> 00:18:28.780]   'Cause the Russian boys didn't ask to be on their frontline.
[00:18:28.780 --> 00:18:30.380]   They have to be.
[00:18:30.380 --> 00:18:31.220]   So,
[00:18:31.220 --> 00:18:37.480]   there's a lot of sons not coming back to their parents,
[00:18:37.480 --> 00:18:42.480]   you know, and I think most of them don't really have,
[00:18:42.480 --> 00:18:45.340]   they don't hate the other side.
[00:18:45.340 --> 00:18:46.280]   You know, it's sort of like, you know, it's like, you know,
[00:18:46.280 --> 00:18:48.660]   it's like, there's this saying about,
[00:18:48.660 --> 00:18:50.360]   like this saying comes from World War I.
[00:18:50.360 --> 00:18:53.740]   It's like young boys who don't know each other,
[00:18:53.740 --> 00:18:56.200]   killing each other on behalf of old men
[00:18:56.200 --> 00:18:57.400]   that do know each other.
[00:18:57.400 --> 00:19:01.140]   The hell's the point of that?
[00:19:01.140 --> 00:19:04.980]   - So Volodymyr Zelensky said that he's not,
[00:19:04.980 --> 00:19:06.320]   or has said in the past,
[00:19:06.320 --> 00:19:08.520]   he's not interested in talking to Putin directly.
[00:19:08.520 --> 00:19:11.140]   Do you think he should sit down man to man,
[00:19:11.140 --> 00:19:13.000]   leader to leader and negotiate peace?
[00:19:13.000 --> 00:19:16.160]   - Look, I think, I would just recommend,
[00:19:16.160 --> 00:19:20.960]   do not send the flower of Ukrainian youth
[00:19:20.960 --> 00:19:23.900]   to die in trenches.
[00:19:23.900 --> 00:19:28.200]   Whether he talks to Putin or not, just don't do that.
[00:19:28.200 --> 00:19:32.120]   Whoever it goes on the offensive
[00:19:32.120 --> 00:19:33.920]   will lose massive numbers of people.
[00:19:33.920 --> 00:19:39.760]   And history will not look kindly upon them.
[00:19:39.760 --> 00:19:44.200]   - You've spoken honestly about the possibility
[00:19:44.200 --> 00:19:46.040]   of war between US and China in the long run.
[00:19:46.040 --> 00:19:49.620]   - Long-term, if no diplomatic solution is found.
[00:19:49.620 --> 00:19:52.360]   For example, on the question of Taiwan and one China policy.
[00:19:52.360 --> 00:19:53.200]   - Right.
[00:19:53.200 --> 00:19:54.280]   - How do we avoid the trajectory
[00:19:54.280 --> 00:19:56.820]   where these two superpowers clash?
[00:19:56.820 --> 00:20:01.340]   - Well, it's worth reading that book on the,
[00:20:01.340 --> 00:20:03.300]   difficult to pronounce, Thucydides Trap,
[00:20:03.300 --> 00:20:04.540]   I believe it's called.
[00:20:04.540 --> 00:20:05.640]   I love war history.
[00:20:05.640 --> 00:20:08.180]   I like inside out and backwards.
[00:20:08.180 --> 00:20:12.000]   There's hardly a battle I haven't read about.
[00:20:12.000 --> 00:20:15.920]   And trying to figure out like what really was the cause
[00:20:15.920 --> 00:20:17.380]   of victory in any particular case,
[00:20:17.380 --> 00:20:21.360]   as opposed to what one side or another claimed was the reason.
[00:20:21.360 --> 00:20:24.240]   - Both the victory and what sparked the war.
[00:20:24.240 --> 00:20:25.380]   - Yeah, yeah.
[00:20:25.380 --> 00:20:26.920]   - The whole thing.
[00:20:26.920 --> 00:20:27.760]   - Yeah.
[00:20:27.760 --> 00:20:30.080]   So that Athens and Sparta is classic case.
[00:20:30.080 --> 00:20:31.540]   The thing about the Greeks is they really wrote down
[00:20:31.540 --> 00:20:32.380]   a lot of stuff.
[00:20:32.380 --> 00:20:33.220]   They loved writing.
[00:20:33.220 --> 00:20:35.340]   There are lots of interesting things that happened
[00:20:35.340 --> 00:20:36.180]   in many parts of the world,
[00:20:36.180 --> 00:20:38.240]   but they just, people just didn't write down.
[00:20:38.240 --> 00:20:40.140]   So we don't know what happened
[00:20:40.140 --> 00:20:44.860]   or they didn't really write in detail.
[00:20:44.860 --> 00:20:45.800]   They just would say like,
[00:20:45.800 --> 00:20:48.040]   we went, we had a battle and we won.
[00:20:48.040 --> 00:20:50.040]   And like, well, what, can you add a bit more?
[00:20:50.040 --> 00:20:53.220]   The Greeks, they really wrote a lot.
[00:20:53.220 --> 00:20:56.360]   They were very articulate on it.
[00:20:56.360 --> 00:20:57.200]   They just love writing.
[00:20:57.200 --> 00:20:59.480]   So, and we have a bunch of that writing that's preserved.
[00:20:59.480 --> 00:21:02.920]   So we know what led up to the Peloponnesian war
[00:21:02.920 --> 00:21:05.640]   between the Spartan and Athenian alliance.
[00:21:05.640 --> 00:21:13.140]   And we know that they saw it coming.
[00:21:15.680 --> 00:21:17.220]   I mean, the Spartans didn't write,
[00:21:17.220 --> 00:21:19.500]   they also weren't very verbose by their nature,
[00:21:19.500 --> 00:21:21.740]   but they did write, but they weren't very verbose.
[00:21:21.740 --> 00:21:23.180]   They were terse.
[00:21:23.180 --> 00:21:26.460]   But the Athenians and the other Greeks wrote a line
[00:21:26.460 --> 00:21:27.360]   and they were like,
[00:21:27.360 --> 00:21:32.800]   and Sparta was really kind of like the leader of Greece,
[00:21:32.800 --> 00:21:35.800]   but Athens grew stronger and stronger
[00:21:35.800 --> 00:21:36.900]   with each passing year.
[00:21:36.900 --> 00:21:40.960]   And everyone's like, well, that's inevitable
[00:21:40.960 --> 00:21:43.960]   that there's going to be a clash between Athens and Sparta.
[00:21:45.560 --> 00:21:46.800]   Well, how do we avoid that?
[00:21:46.800 --> 00:21:47.640]   And they couldn't, they couldn't,
[00:21:47.640 --> 00:21:49.440]   they actually, they saw it coming
[00:21:49.440 --> 00:21:51.140]   and they still could not avoid it.
[00:21:51.140 --> 00:21:56.060]   So, you know, at some point if there's,
[00:21:56.060 --> 00:22:01.060]   if one group, one civilization or country or whatever
[00:22:01.060 --> 00:22:07.420]   exceeds another, sort of like if, you know,
[00:22:07.420 --> 00:22:10.020]   the United States has been the biggest kid on the block
[00:22:10.020 --> 00:22:14.500]   since I think around 1890 from an economic standpoint.
[00:22:14.500 --> 00:22:15.440]   So,
[00:22:15.440 --> 00:22:18.100]   the United States has been the economic,
[00:22:18.100 --> 00:22:22.540]   most powerful economic engine in the world
[00:22:22.540 --> 00:22:24.200]   longer than anyone's been alive.
[00:22:24.200 --> 00:22:30.260]   And the foundation of war is economics.
[00:22:30.260 --> 00:22:32.340]   So,
[00:22:32.340 --> 00:22:37.020]   now we have a situation in case of China,
[00:22:37.020 --> 00:22:42.020]   where the economy is likely to be two,
[00:22:42.020 --> 00:22:45.320]   perhaps three times larger than that of the US.
[00:22:45.320 --> 00:22:48.000]   So, imagine you're the biggest kid on the block
[00:22:48.000 --> 00:22:49.600]   for as long as anyone can remember.
[00:22:49.600 --> 00:22:52.200]   And suddenly a kid comes along who's twice your size.
[00:22:52.200 --> 00:22:57.460]   - So, we see it coming.
[00:22:57.460 --> 00:22:58.300]   - Yeah.
[00:22:58.300 --> 00:22:59.920]   - How is it possible to stop?
[00:22:59.920 --> 00:23:02.700]   Is there some, let me throw something out there,
[00:23:02.700 --> 00:23:06.120]   just intermixing of cultures, understanding.
[00:23:06.120 --> 00:23:08.040]   There does seem to be a giant cultural gap
[00:23:08.040 --> 00:23:09.760]   in understanding of each other.
[00:23:09.760 --> 00:23:11.260]   And you're an interesting case study
[00:23:11.260 --> 00:23:14.080]   because you are an American, obviously,
[00:23:14.080 --> 00:23:15.200]   you've done a lot of. - Yes.
[00:23:15.200 --> 00:23:18.580]   - Incredible manufacture here in the United States,
[00:23:18.580 --> 00:23:20.180]   but you also work with China.
[00:23:20.180 --> 00:23:21.320]   - I've spent a lot of time in China
[00:23:21.320 --> 00:23:22.980]   and met with the leadership many times.
[00:23:22.980 --> 00:23:24.980]   - Maybe a good question to ask is,
[00:23:24.980 --> 00:23:26.840]   what are some things about China
[00:23:26.840 --> 00:23:29.720]   that people don't understand, positive, just in the culture?
[00:23:29.720 --> 00:23:31.240]   What's some interesting things
[00:23:31.240 --> 00:23:33.160]   that you've learned about the Chinese?
[00:23:33.160 --> 00:23:39.460]   - Well, the sheer number of really smart,
[00:23:39.460 --> 00:23:43.780]   hardworking people in China is incredible.
[00:23:45.080 --> 00:23:47.980]   I believe, say like how many smart,
[00:23:47.980 --> 00:23:49.160]   hardworking people are there in China?
[00:23:49.160 --> 00:23:51.600]   There's far more of them there than there are here.
[00:23:51.600 --> 00:23:52.980]   I think in my opinion.
[00:23:52.980 --> 00:23:58.640]   The, and they've got a lot of energy.
[00:23:58.640 --> 00:24:05.520]   So, I mean, the architecture in China that's in recent years
[00:24:05.520 --> 00:24:08.420]   is far more impressive than the US.
[00:24:08.420 --> 00:24:10.900]   I mean, the train stations, the buildings,
[00:24:10.900 --> 00:24:12.980]   the high speed rail, everything, it's,
[00:24:14.960 --> 00:24:19.280]   really far more impressive than what we have in the US.
[00:24:19.280 --> 00:24:22.200]   I mean, I recommend somebody just go to Shanghai and Beijing,
[00:24:22.200 --> 00:24:25.860]   look at the buildings and go to, you know,
[00:24:25.860 --> 00:24:27.780]   take the train from Beijing to Xi'an
[00:24:27.780 --> 00:24:29.980]   where you have the terracotta warriors.
[00:24:29.980 --> 00:24:33.620]   China's got an incredible history, a very long history.
[00:24:33.620 --> 00:24:37.580]   And, you know, I think arguably the,
[00:24:37.580 --> 00:24:41.620]   in terms of the use of language from a written standpoint,
[00:24:41.620 --> 00:24:44.840]   sort of one of the oldest, perhaps,
[00:24:44.840 --> 00:24:48.300]   perhaps the oldest written language.
[00:24:48.300 --> 00:24:50.680]   And in China, people did write things down.
[00:24:50.680 --> 00:24:55.680]   So, China historically has always been,
[00:24:55.680 --> 00:24:59.800]   with rare exception, been internally focused.
[00:24:59.800 --> 00:25:01.300]   They have not been acquisitive.
[00:25:01.300 --> 00:25:05.060]   They've fought each other.
[00:25:05.060 --> 00:25:07.680]   There have been many, many civil wars.
[00:25:07.680 --> 00:25:09.180]   In the Three Kingdoms War,
[00:25:09.180 --> 00:25:12.180]   I believe they lost about 70% of their population.
[00:25:14.720 --> 00:25:19.580]   So, they've had brutal internal wars,
[00:25:19.580 --> 00:25:24.560]   like civil wars that make the US civil war look small,
[00:25:24.560 --> 00:25:25.400]   by comparison.
[00:25:25.400 --> 00:25:31.000]   So, I think it's important to appreciate
[00:25:31.000 --> 00:25:33.040]   that China is not a monolithic.
[00:25:33.040 --> 00:25:38.720]   We sort of think of like China as this sort of one entity,
[00:25:38.720 --> 00:25:41.480]   well, of one mind, and this is definitely not the case.
[00:25:41.480 --> 00:25:44.600]   From what I've seen,
[00:25:44.600 --> 00:25:48.180]   and I think most people who understand China would agree,
[00:25:48.180 --> 00:25:51.180]   that people in China think about China 10 times more
[00:25:51.180 --> 00:25:53.800]   than they think about anything outside of China.
[00:25:53.800 --> 00:25:57.800]   So, it's like 90% of their consideration is internal.
[00:25:57.800 --> 00:26:03.720]   - Well, isn't that a really positive thing
[00:26:03.720 --> 00:26:05.400]   when you're talking about the collaboration
[00:26:05.400 --> 00:26:08.000]   and the future piece between superpowers,
[00:26:08.000 --> 00:26:09.220]   when you're inward facing,
[00:26:09.220 --> 00:26:12.100]   which is like focusing on improving yourself
[00:26:12.100 --> 00:26:13.660]   versus focusing on-
[00:26:13.660 --> 00:26:14.480]   - Yeah.
[00:26:14.480 --> 00:26:18.220]   - Quote unquote, improving others through military might?
[00:26:18.220 --> 00:26:20.820]   - The good news, the history of China suggests
[00:26:20.820 --> 00:26:22.160]   that China is not acquisitive,
[00:26:22.160 --> 00:26:23.140]   meaning they're not gonna go out
[00:26:23.140 --> 00:26:24.980]   and invade a whole bunch of countries.
[00:26:24.980 --> 00:26:28.860]   Now, they do feel very strongly, you know, so that's good.
[00:26:28.860 --> 00:26:31.560]   I mean, 'cause a lot of very powerful countries
[00:26:31.560 --> 00:26:32.660]   have been acquisitive.
[00:26:32.660 --> 00:26:37.100]   The US is also one of the rare cases
[00:26:37.100 --> 00:26:38.920]   that has not been acquisitive.
[00:26:38.920 --> 00:26:40.100]   Like after World War II,
[00:26:40.100 --> 00:26:41.860]   the US could have basically taken over the world
[00:26:41.860 --> 00:26:44.360]   and any country, like we got nukes, nobody else got nukes.
[00:26:44.360 --> 00:26:46.400]   We don't even have to lose soldiers.
[00:26:46.400 --> 00:26:48.820]   Which country do you want?
[00:26:48.820 --> 00:26:52.260]   And the United States could have taken over everything.
[00:26:52.260 --> 00:26:53.940]   Oh, it had will, and it didn't.
[00:26:53.940 --> 00:26:57.720]   And the United States actually helped revolt countries.
[00:26:57.720 --> 00:27:00.940]   So it helped revolt Europe, you know, helped revolt Japan.
[00:27:00.940 --> 00:27:05.740]   This is very unusual behavior, almost unprecedented.
[00:27:05.740 --> 00:27:13.380]   You know, the US did conspicuous acts of kindness,
[00:27:13.380 --> 00:27:14.240]   like the Berlin airlift.
[00:27:14.240 --> 00:27:15.080]   You know?
[00:27:15.080 --> 00:27:22.560]   And I think, you know, it's always like,
[00:27:22.560 --> 00:27:23.820]   well, America's done bad things.
[00:27:23.820 --> 00:27:25.000]   Well, of course, America's done bad things,
[00:27:25.000 --> 00:27:27.400]   but one needs to look at the whole track record.
[00:27:27.400 --> 00:27:34.280]   And just generally, you know, one sort of test would be,
[00:27:34.280 --> 00:27:36.600]   how do you treat your prisoners of war?
[00:27:36.600 --> 00:27:43.120]   Or let's say, you know, no offense to the Russians,
[00:27:44.120 --> 00:27:47.240]   but let's say you're in Germany, it's 1945.
[00:27:47.240 --> 00:27:49.360]   You got the Russian army coming one side,
[00:27:49.360 --> 00:27:51.840]   and you got the French, British, and American armies
[00:27:51.840 --> 00:27:53.120]   coming the other side.
[00:27:53.120 --> 00:27:55.780]   Who would you like to be, to surrender to?
[00:27:55.780 --> 00:27:58.760]   Like no country is like morally perfect,
[00:27:58.760 --> 00:28:03.560]   but I recommend being a POW with the Americans.
[00:28:03.560 --> 00:28:05.740]   That would be my choice very strongly.
[00:28:05.740 --> 00:28:09.000]   - In the full menu of POW.
[00:28:09.000 --> 00:28:10.280]   - Very much so.
[00:28:10.280 --> 00:28:12.880]   And in fact, Wunderbarn-Brown,
[00:28:14.000 --> 00:28:17.080]   took, you know, a small guy, was like,
[00:28:17.080 --> 00:28:19.340]   "We've got to be captured by the Americans."
[00:28:19.340 --> 00:28:20.180]   - Yeah.
[00:28:20.180 --> 00:28:25.180]   - And in fact, the SS was under orders to execute Wunderbarn
[00:28:25.180 --> 00:28:29.480]   and all of the German rocket engineers.
[00:28:29.480 --> 00:28:34.540]   And they narrowly escaped their SS.
[00:28:34.540 --> 00:28:36.660]   They said they were going out for a walk in the woods.
[00:28:36.660 --> 00:28:39.720]   They left in the middle of winter with no coats.
[00:28:39.720 --> 00:28:42.680]   And they ran like, no food, no coats, no water,
[00:28:42.680 --> 00:28:43.880]   and just ran like hell.
[00:28:43.880 --> 00:28:45.860]   And ran west.
[00:28:45.860 --> 00:28:47.780]   And by sheer luck, they,
[00:28:47.780 --> 00:28:51.060]   I think his brother found like a bicycle or something.
[00:28:51.060 --> 00:28:54.520]   And they just cycled west as fast as they could
[00:28:54.520 --> 00:28:56.760]   and found a US patrol.
[00:28:56.760 --> 00:29:03.800]   So anyway, that's one way you can tell morality.
[00:29:03.800 --> 00:29:05.880]   Where do you want to be a POW?
[00:29:05.880 --> 00:29:08.060]   It's not fun anywhere,
[00:29:08.060 --> 00:29:10.320]   but some places are much worse than others.
[00:29:10.320 --> 00:29:13.760]   So anyway, so like America has been,
[00:29:13.760 --> 00:29:18.760]   while far from perfect, generally a benevolent force.
[00:29:18.760 --> 00:29:23.920]   And we should always be self-critical
[00:29:23.920 --> 00:29:26.240]   and try to be better.
[00:29:26.240 --> 00:29:31.080]   But anyone with half a brain knows that.
[00:29:31.080 --> 00:29:34.380]   So I think there are, in this way,
[00:29:34.380 --> 00:29:37.700]   China and the United States are similar.
[00:29:37.700 --> 00:29:42.700]   Neither country has been acquisitive in a significant way.
[00:29:42.700 --> 00:29:43.640]   So that's like a,
[00:29:43.640 --> 00:29:46.200]   you know, a shared principle, I guess.
[00:29:46.200 --> 00:29:50.860]   Now China does feel very strongly about Taiwan.
[00:29:50.860 --> 00:29:54.640]   They've been very clear about that for a long time.
[00:29:54.640 --> 00:29:56.880]   You know, from their standpoint,
[00:29:56.880 --> 00:30:01.880]   it would be like one of the states is, you know,
[00:30:01.880 --> 00:30:04.820]   not there, like Hawaii or something like that,
[00:30:04.820 --> 00:30:07.360]   but more significant than Hawaii, you know?
[00:30:07.360 --> 00:30:11.600]   And Hawaii is pretty significant for us.
[00:30:11.600 --> 00:30:12.440]   So,
[00:30:13.520 --> 00:30:16.240]   they view it as really the,
[00:30:16.240 --> 00:30:19.460]   that there's a fundamental part of China,
[00:30:19.460 --> 00:30:22.640]   the island of Formosa,
[00:30:22.640 --> 00:30:27.640]   not Taiwan, that is not part of China, but should be.
[00:30:27.640 --> 00:30:29.480]   And the only reason it hasn't been
[00:30:29.480 --> 00:30:31.180]   is because of the US Pacific fleet.
[00:30:31.180 --> 00:30:34.000]   - And as their economic power grows,
[00:30:34.000 --> 00:30:36.580]   and as their military power grows,
[00:30:36.580 --> 00:30:41.580]   the thing that they're clearly saying is their interests will,
[00:30:43.400 --> 00:30:45.820]   clearly be materialized.
[00:30:45.820 --> 00:30:46.660]   - Yes.
[00:30:46.660 --> 00:30:51.660]   China has been very clear that they will incorporate Taiwan
[00:30:51.660 --> 00:30:57.900]   peacefully or militarily,
[00:30:57.900 --> 00:31:01.240]   but that they will incorporate it from their standpoint
[00:31:01.240 --> 00:31:03.060]   is 100% likely.
[00:31:03.060 --> 00:31:05.040]   - You know, something you said
[00:31:05.040 --> 00:31:06.920]   about conspicuous acts of kindness.
[00:31:06.920 --> 00:31:10.620]   As a geopolitical policy, it almost seems naive,
[00:31:10.620 --> 00:31:13.280]   but,
[00:31:13.280 --> 00:31:17.020]   I'd venture to say that this is probably the path forward,
[00:31:17.020 --> 00:31:20.340]   how you will avoid most wars.
[00:31:20.340 --> 00:31:22.980]   Just as you say it, it sounds naive,
[00:31:22.980 --> 00:31:24.280]   but it's kind of brilliant.
[00:31:24.280 --> 00:31:29.560]   If you believe in the goodness of underlying
[00:31:29.560 --> 00:31:31.020]   most of human nature,
[00:31:31.020 --> 00:31:35.560]   it just seems like conspicuous acts of kindness
[00:31:35.560 --> 00:31:38.720]   can reverberate through the populace
[00:31:38.720 --> 00:31:40.220]   of the countries involved.
[00:31:40.220 --> 00:31:42.660]   - Yeah, well.
[00:31:42.660 --> 00:31:44.040]   - And deescalate.
[00:31:44.040 --> 00:31:44.880]   - Absolutely.
[00:31:44.880 --> 00:31:49.880]   So after World War I, they made a big mistake.
[00:31:49.880 --> 00:31:53.940]   They basically tried to lump all the blame on Germany
[00:31:53.940 --> 00:32:00.940]   and settled Germany with impossible reparations.
[00:32:00.940 --> 00:32:08.940]   And really there was quite a bit of blame
[00:32:08.940 --> 00:32:11.940]   to go around for World War I.
[00:32:12.540 --> 00:32:15.820]   But they tried to put it all on Germany.
[00:32:15.820 --> 00:32:23.320]   And that laid the seeds for World War II.
[00:32:23.320 --> 00:32:28.840]   So a lot of people, well, not just Hitler,
[00:32:28.840 --> 00:32:30.300]   a lot of people felt wronged.
[00:32:30.300 --> 00:32:36.540]   And they wanted vengeance and they got it.
[00:32:36.540 --> 00:32:39.640]   - People don't forget.
[00:32:39.640 --> 00:32:40.480]   - Yeah.
[00:32:42.420 --> 00:32:46.060]   - And if you're somebody's father, mother, son, daughter,
[00:32:46.060 --> 00:32:47.860]   they're not going to forget it.
[00:32:47.860 --> 00:32:49.100]   They will want vengeance.
[00:32:49.100 --> 00:32:52.380]   So after World War II, they're like, well,
[00:32:52.380 --> 00:32:56.840]   that Treaty of Versailles was a huge mistake in World War I.
[00:32:56.840 --> 00:33:03.180]   And so this time, instead of crushing the losers,
[00:33:03.180 --> 00:33:10.320]   we're actually going to help them with the Marshall Plan.
[00:33:10.320 --> 00:33:12.300]   And we're going to help rebuild, rebuild, rebuild, rebuild.
[00:33:12.300 --> 00:33:14.340]   We're going to help rebuild Germany.
[00:33:14.340 --> 00:33:17.020]   We're going to help rebuild, you know, Austria
[00:33:17.020 --> 00:33:20.300]   and the other, you know, Italy and whatnot.
[00:33:20.300 --> 00:33:25.300]   So, and that was the right move.
[00:33:25.300 --> 00:33:29.600]   - There's a, it does feel like there's a profound truth
[00:33:29.600 --> 00:33:35.960]   to conspicuous acts of kindness being an antidote to this.
[00:33:35.960 --> 00:33:40.100]   - Something must stop the cycle of reciprocal violence.
[00:33:42.180 --> 00:33:44.940]   Something must stop it, or it will, you know,
[00:33:44.940 --> 00:33:47.220]   it'll never stop.
[00:33:47.220 --> 00:33:51.620]   Just eye for an eye, tooth for a tooth,
[00:33:51.620 --> 00:33:55.080]   limb for a limb, life for a life, forever and ever.
[00:33:55.080 --> 00:33:58.920]   - To escape briefly the darkness
[00:33:58.920 --> 00:34:00.860]   was some incredible engineering work.
[00:34:00.860 --> 00:34:04.860]   XAI just released Grok AI Assistant
[00:34:04.860 --> 00:34:06.340]   that I've gotten a chance to play with.
[00:34:06.340 --> 00:34:08.780]   It's amazing on many levels.
[00:34:08.780 --> 00:34:12.060]   First of all, it's amazing that a relatively small team
[00:34:12.060 --> 00:34:13.740]   in a relatively short amount of time
[00:34:13.740 --> 00:34:18.740]   was able to develop this close to state of the art system.
[00:34:18.740 --> 00:34:22.060]   Another incredible thing is there's a regular mode
[00:34:22.060 --> 00:34:23.040]   and there's a fun mode.
[00:34:23.040 --> 00:34:25.240]   - Yeah, I guess I'm to blame for that one.
[00:34:25.240 --> 00:34:27.560]   - I wish it, first of all,
[00:34:27.560 --> 00:34:29.500]   I wish everything in life had a fun mode.
[00:34:29.500 --> 00:34:30.340]   - Yeah.
[00:34:30.340 --> 00:34:34.080]   - There's something compelling beyond just fun
[00:34:34.080 --> 00:34:38.160]   about the fun mode interacting with a large language model.
[00:34:38.160 --> 00:34:39.360]   I'm not sure exactly what it is,
[00:34:39.360 --> 00:34:41.940]   'cause I've only had a little bit of time to play with it.
[00:34:41.940 --> 00:34:45.560]   But it just makes it more interesting,
[00:34:45.560 --> 00:34:47.940]   more vibrant to interact with the system.
[00:34:47.940 --> 00:34:50.160]   - Yeah, absolutely.
[00:34:50.160 --> 00:34:56.440]   Our AI Grok is modeled after
[00:34:56.440 --> 00:34:58.100]   The Hedgehog's Guide to the Galaxy,
[00:34:58.100 --> 00:35:01.380]   which is one of my favorite books.
[00:35:01.380 --> 00:35:03.500]   Which is, it's a book on philosophy
[00:35:03.500 --> 00:35:05.320]   disguised as a book on humor.
[00:35:05.320 --> 00:35:11.820]   And I would say that forms the basis of my philosophy.
[00:35:11.820 --> 00:35:16.700]   Which is that we don't know the meaning of life,
[00:35:16.700 --> 00:35:18.300]   but the more we can expand the scope
[00:35:18.300 --> 00:35:19.640]   and scale of consciousness,
[00:35:19.640 --> 00:35:25.040]   digital and biological,
[00:35:25.040 --> 00:35:27.340]   the more we're able to understand what questions to ask
[00:35:27.340 --> 00:35:30.080]   about the answer that is the universe.
[00:35:30.080 --> 00:35:32.740]   So I have a philosophy of curiosity.
[00:35:32.740 --> 00:35:38.540]   - There is generally a feeling like this AI system
[00:35:38.540 --> 00:35:41.540]   has an outward looking,
[00:35:41.540 --> 00:35:44.260]   like the way you are sitting with a good friend,
[00:35:44.260 --> 00:35:46.040]   looking up at the stars,
[00:35:46.040 --> 00:35:49.560]   like asking pothead-like questions about the universe,
[00:35:49.560 --> 00:35:50.800]   wondering what it's all about,
[00:35:50.800 --> 00:35:52.440]   the curiosity that you talk about.
[00:35:52.440 --> 00:35:53.320]   There's a sense,
[00:35:53.320 --> 00:35:56.000]   no matter how mundane the question I ask it,
[00:35:56.000 --> 00:35:59.020]   there's a sense of cosmic grandeur to the whole thing.
[00:35:59.020 --> 00:36:01.220]   - Well, we are actually working hard
[00:36:01.220 --> 00:36:06.080]   to have engineering, math, and physics answers
[00:36:06.080 --> 00:36:07.120]   that you can count on.
[00:36:07.120 --> 00:36:11.420]   So for the other sort of,
[00:36:11.420 --> 00:36:12.860]   AIs out there,
[00:36:12.860 --> 00:36:16.320]   these so-called large language models,
[00:36:16.320 --> 00:36:21.760]   I've not found the engineering to be reliable.
[00:36:21.760 --> 00:36:24.840]   And the hallucination,
[00:36:24.840 --> 00:36:26.860]   it unfortunately hallucinates most
[00:36:26.860 --> 00:36:28.720]   when you least want it to hallucinate.
[00:36:28.720 --> 00:36:29.640]   - Yeah.
[00:36:29.640 --> 00:36:32.540]   - So when you're asking important, difficult questions,
[00:36:32.540 --> 00:36:34.780]   that's when it tends to be confidently wrong.
[00:36:34.780 --> 00:36:39.680]   So we're really trying hard to say, okay,
[00:36:39.680 --> 00:36:41.300]   how do we be as grounded
[00:36:41.300 --> 00:36:43.640]   as possible so you can count on the results?
[00:36:43.640 --> 00:36:48.860]   Trace things back to physics first principles,
[00:36:48.860 --> 00:36:50.100]   mathematical logic.
[00:36:50.100 --> 00:36:55.060]   So underlying the humor is an aspiration
[00:36:55.060 --> 00:36:58.960]   to adhere to the truth of the universe
[00:36:58.960 --> 00:37:00.200]   as closely as possible.
[00:37:00.200 --> 00:37:02.860]   - That's really tricky.
[00:37:02.860 --> 00:37:04.060]   - It is tricky.
[00:37:04.060 --> 00:37:07.800]   So that's why, you know,
[00:37:07.800 --> 00:37:09.620]   there's always going to be some amount of error,
[00:37:09.620 --> 00:37:11.180]   but do we want to,
[00:37:11.180 --> 00:37:16.180]   aspire to be as truthful as possible about the answers
[00:37:16.180 --> 00:37:20.500]   with acknowledged error?
[00:37:20.500 --> 00:37:21.860]   So that there was always,
[00:37:21.860 --> 00:37:23.440]   you don't want to be confidently wrong.
[00:37:23.440 --> 00:37:26.800]   So you're not going to be right every time,
[00:37:26.800 --> 00:37:28.660]   but you don't want to minimize
[00:37:28.660 --> 00:37:31.500]   how often you're confidently wrong.
[00:37:31.500 --> 00:37:32.420]   And then, like I said,
[00:37:32.420 --> 00:37:35.980]   once you can count on the logic as being
[00:37:35.980 --> 00:37:41.060]   not violating physics, then you can start
[00:37:41.060 --> 00:37:44.060]   to build on that to create inventions,
[00:37:44.060 --> 00:37:46.320]   like invent new technologies.
[00:37:46.320 --> 00:37:47.400]   But if you can't,
[00:37:47.400 --> 00:37:52.060]   if you cannot count on the foundational physics being
[00:37:52.060 --> 00:37:54.660]   correct, obviously the inventions are simply
[00:37:54.660 --> 00:37:58.100]   wishful thinking, you know, imagination land,
[00:37:58.100 --> 00:38:00.480]   magic, basically.
[00:38:00.480 --> 00:38:02.720]   - Well, as you said,
[00:38:02.720 --> 00:38:05.340]   I think one of the big goals of XAI is to understand
[00:38:05.340 --> 00:38:06.620]   the universe.
[00:38:06.620 --> 00:38:08.060]   - Yes.
[00:38:08.060 --> 00:38:10.940]   That's how simple three word mission.
[00:38:10.940 --> 00:38:11.780]   - Yeah.
[00:38:11.780 --> 00:38:15.560]   - If you look out far into the future,
[00:38:15.560 --> 00:38:17.740]   do you think on this level of physics,
[00:38:17.740 --> 00:38:19.700]   the very edge of what we understand about physics,
[00:38:19.700 --> 00:38:22.920]   do you think it will make discoveries,
[00:38:22.920 --> 00:38:26.640]   sort of the sexiest discovery of them as we know now,
[00:38:26.640 --> 00:38:30.520]   sort of unifying general relativity and quantum mechanics?
[00:38:30.520 --> 00:38:32.700]   So coming up with a theory of everything,
[00:38:32.700 --> 00:38:35.540]   do you think it could push towards that direction,
[00:38:35.540 --> 00:38:38.160]   almost like theoretical physics discoveries?
[00:38:38.160 --> 00:38:40.820]   - If an AI cannot figure out new physics,
[00:38:40.820 --> 00:38:44.000]   it's clearly not equal to humans,
[00:38:44.000 --> 00:38:45.920]   let alone nor has surpassed humans,
[00:38:45.920 --> 00:38:47.820]   'cause humans have figured out new physics.
[00:38:47.820 --> 00:38:50.580]   They're just, you know, physics is just understanding,
[00:38:50.580 --> 00:38:54.600]   you know, deepening one's insight into how reality works.
[00:38:54.600 --> 00:38:55.440]   And then,
[00:38:55.440 --> 00:38:58.580]   then there's engineering,
[00:38:58.580 --> 00:39:01.660]   which is inventing things that have never existed.
[00:39:01.660 --> 00:39:03.640]   Now the range of possibilities for engineering is far
[00:39:03.640 --> 00:39:05.060]   greater than for physics,
[00:39:05.060 --> 00:39:06.740]   because, you know, once you figure out the rules
[00:39:06.740 --> 00:39:10.700]   of the universe, that's it, you've discovered things that
[00:39:10.700 --> 00:39:12.000]   already existed.
[00:39:12.000 --> 00:39:15.720]   But from that, you can then build technologies
[00:39:15.720 --> 00:39:21.840]   with that are really almost limitless in the variety
[00:39:21.840 --> 00:39:24.640]   and keep, you know, it's like once you understand the rules
[00:39:24.640 --> 00:39:27.620]   of the game properly, and we do, with current physics,
[00:39:27.620 --> 00:39:29.780]   we do, at least at a local level,
[00:39:29.780 --> 00:39:32.460]   understand how physics works very well.
[00:39:32.460 --> 00:39:35.400]   Where our ability to predict things is incredibly good.
[00:39:35.400 --> 00:39:37.700]   Like quantum mechanics is, the degree to which quantum
[00:39:37.700 --> 00:39:40.580]   mechanics can predict outcomes is incredible.
[00:39:40.580 --> 00:39:44.400]   That was my, that was my hardest class in college,
[00:39:44.400 --> 00:39:45.240]   by the way.
[00:39:45.240 --> 00:39:48.700]   My senior quantum mechanics class was harder than all of my
[00:39:48.700 --> 00:39:50.640]   other classes put together.
[00:39:50.640 --> 00:39:54.700]   - To get an AI system, a large language model to,
[00:39:54.700 --> 00:39:59.620]   to reliably, be as reliable as quantum mechanics in physics
[00:39:59.620 --> 00:40:01.160]   is very difficult.
[00:40:01.160 --> 00:40:04.360]   - Yeah, you have to test any conclusions against the ground
[00:40:04.360 --> 00:40:05.200]   truth of reality.
[00:40:05.200 --> 00:40:06.580]   Reality is the ultimate judge.
[00:40:06.580 --> 00:40:09.900]   Like physics is the law, everything else is a recommendation.
[00:40:09.900 --> 00:40:10.460]   I've seen plenty of people who have done that.
[00:40:10.460 --> 00:40:13.080]   Some people break the, break the laws made by man,
[00:40:13.080 --> 00:40:15.760]   but none break the laws made by physics.
[00:40:15.760 --> 00:40:16.980]   - Yeah, it's a good test actually.
[00:40:16.980 --> 00:40:21.820]   If this LLM understands and matches physics,
[00:40:21.820 --> 00:40:24.580]   then you can more reliably trust whatever it thinks
[00:40:24.580 --> 00:40:27.440]   about the current state of politics.
[00:40:27.440 --> 00:40:28.280]   In some sense.
[00:40:28.280 --> 00:40:31.280]   - And it's also not the case currently that even,
[00:40:31.280 --> 00:40:33.360]   that its internal logic is not consistent.
[00:40:33.360 --> 00:40:39.460]   So especially with these, with the approach of like just
[00:40:40.340 --> 00:40:44.160]   predicting a token, predict token, predict token,
[00:40:44.160 --> 00:40:45.200]   it's like a vector sum.
[00:40:45.200 --> 00:40:47.320]   You know, you're summing up a bunch of vectors,
[00:40:47.320 --> 00:40:49.020]   but you can get drift.
[00:40:49.020 --> 00:40:51.300]   So as those, a little bit of error,
[00:40:51.300 --> 00:40:53.000]   a little bit of error adds up.
[00:40:53.000 --> 00:40:56.100]   And by the time you are many tokens down the path,
[00:40:56.100 --> 00:40:58.860]   you're, it doesn't make any sense.
[00:40:58.860 --> 00:41:02.720]   - So it has to be somehow self-aware about the drift.
[00:41:02.720 --> 00:41:06.020]   - It has to be self-aware about the drift and then look at
[00:41:06.020 --> 00:41:10.220]   the thing as a gestalt, as a whole and, and say, it,
[00:41:10.220 --> 00:41:13.620]   does it have coherence as a whole?
[00:41:13.620 --> 00:41:18.620]   So, you know, when, when authors write books that they,
[00:41:18.620 --> 00:41:22.220]   they will write the book and then they'll go and revise it,
[00:41:22.220 --> 00:41:25.220]   you know, take into account, you know, all the,
[00:41:25.220 --> 00:41:28.720]   the end and the beginning and the middle and, and rewrite it
[00:41:28.720 --> 00:41:32.200]   to achieve coherence so that it doesn't end up in a
[00:41:32.200 --> 00:41:33.780]   nonsensical place.
[00:41:33.780 --> 00:41:38.180]   - Maybe the process of revising is what reasoning is.
[00:41:38.180 --> 00:41:40.100]   And then that's the process of revising is how you get
[00:41:40.100 --> 00:41:42.020]   closer and closer to truth.
[00:41:42.020 --> 00:41:45.100]   Maybe you like, at least I approach it that way.
[00:41:45.100 --> 00:41:47.120]   You just say a bunch of bullshit first and then you get
[00:41:47.120 --> 00:41:48.080]   it better.
[00:41:48.080 --> 00:41:49.920]   You start at bullshit and then you get closer.
[00:41:49.920 --> 00:41:50.960]   - Yeah, you create a draft and then, and then you,
[00:41:50.960 --> 00:41:55.460]   and then you iterate on that draft until it has coherence
[00:41:55.460 --> 00:41:58.940]   until it's, it all adds up basically.
[00:41:58.940 --> 00:42:01.100]   - So another question about theory of everything,
[00:42:01.100 --> 00:42:04.620]   but for intelligence, do you think there exists,
[00:42:04.620 --> 00:42:06.540]   as you're exploring this with XAI, creating this
[00:42:06.540 --> 00:42:09.260]   intelligence system, do you think there is a theory of
[00:42:09.260 --> 00:42:09.980]   intelligence?
[00:42:09.980 --> 00:42:14.100]   - Well, you get to understand what, like,
[00:42:14.100 --> 00:42:19.100]   what is the I in AGI and what is the I in human
[00:42:19.100 --> 00:42:22.440]   intelligence?
[00:42:22.440 --> 00:42:23.880]   - There's no I in Team America.
[00:42:23.880 --> 00:42:24.720]   Oh wait, there is.
[00:42:24.720 --> 00:42:28.720]   - Ah, now it's going to be stuck in my head now.
[00:42:28.720 --> 00:42:32.460]   Yeah.
[00:42:32.460 --> 00:42:34.840]   There's no me in whatever.
[00:42:34.840 --> 00:42:37.360]   In quantum mechanics.
[00:42:37.360 --> 00:42:38.200]   Oh, wait.
[00:42:39.860 --> 00:42:44.240]   - I mean, is that part of the process of discovering,
[00:42:44.240 --> 00:42:47.980]   understanding the universe is understanding intelligence?
[00:42:47.980 --> 00:42:48.820]   - Yeah.
[00:42:48.820 --> 00:42:52.300]   Yeah, I think we need to understand intelligence,
[00:42:52.300 --> 00:42:54.040]   understand consciousness.
[00:42:54.040 --> 00:42:56.560]   I mean, there, I mean, there are some sort of fundamental
[00:42:56.560 --> 00:43:00.120]   questions of like, what is thought?
[00:43:00.120 --> 00:43:01.200]   What is emotion?
[00:43:01.200 --> 00:43:02.040]   - Yeah.
[00:43:02.040 --> 00:43:06.080]   - Is it really just one atom bumping into another atom?
[00:43:06.080 --> 00:43:07.980]   It feels like something more than that.
[00:43:07.980 --> 00:43:12.980]   So I think we're probably missing some really big things.
[00:43:12.980 --> 00:43:17.740]   Like some really big things.
[00:43:17.740 --> 00:43:20.800]   - Something that'll be obvious in retrospect.
[00:43:20.800 --> 00:43:21.640]   - Yes.
[00:43:21.640 --> 00:43:24.720]   - Like there's a giant, you put the whole consciousness,
[00:43:24.720 --> 00:43:25.560]   emotion.
[00:43:25.560 --> 00:43:28.820]   - Well, some people would call it like a soul, you know,
[00:43:28.820 --> 00:43:30.260]   religion is a soul.
[00:43:30.260 --> 00:43:33.940]   Like you feel like you're you, right?
[00:43:33.940 --> 00:43:37.860]   I mean, you don't feel like you're just a collection of atoms.
[00:43:37.860 --> 00:43:41.820]   But on what dimension does thought exist?
[00:43:41.820 --> 00:43:43.740]   What dimension does, do emotions exist?
[00:43:43.740 --> 00:43:45.060]   We feel them very strongly.
[00:43:45.060 --> 00:43:49.440]   I suspect there's more to it than atoms bumping to atoms.
[00:43:49.440 --> 00:43:55.780]   - And maybe AI can pave the path to the discovery
[00:43:55.780 --> 00:43:58.120]   of what, whatever the hell that thing is.
[00:43:58.120 --> 00:43:58.960]   - Yeah.
[00:43:58.960 --> 00:43:59.780]   What is consciousness?
[00:43:59.780 --> 00:44:02.840]   Like what, when you put the atoms in a particular shape,
[00:44:02.840 --> 00:44:07.740]   why are they able to form thoughts and take actions?
[00:44:07.740 --> 00:44:09.300]   And feelings?
[00:44:09.300 --> 00:44:11.700]   - And even if it is an illusion,
[00:44:11.700 --> 00:44:13.360]   why is this illusion so compelling?
[00:44:13.360 --> 00:44:14.200]   - Yeah.
[00:44:14.200 --> 00:44:15.080]   - Like how do you-
[00:44:15.080 --> 00:44:17.220]   - Why does this illusion exist?
[00:44:17.220 --> 00:44:18.060]   - Yeah.
[00:44:18.060 --> 00:44:21.820]   - On what plane does this illusion exist?
[00:44:21.820 --> 00:44:22.660]   - Yeah.
[00:44:22.660 --> 00:44:26.200]   - And sometimes I wonder is, you know,
[00:44:26.200 --> 00:44:28.120]   either perhaps everything's conscious
[00:44:28.120 --> 00:44:29.280]   or nothing is conscious.
[00:44:29.280 --> 00:44:32.160]   One of the two.
[00:44:32.160 --> 00:44:34.840]   - I like the former.
[00:44:34.840 --> 00:44:37.180]   Everything conscious just seems more fun.
[00:44:37.180 --> 00:44:39.680]   - It does seem more fun, yes.
[00:44:39.680 --> 00:44:43.000]   But we're composed of atoms,
[00:44:43.000 --> 00:44:45.780]   and those atoms are composed of quarks and leptons.
[00:44:45.780 --> 00:44:47.900]   And those quarks and leptons have been around
[00:44:47.900 --> 00:44:50.100]   since the beginning of the universe.
[00:44:50.100 --> 00:44:53.160]   - The beginning of the universe.
[00:44:53.160 --> 00:44:54.000]   - Right.
[00:44:54.000 --> 00:44:55.680]   What seems to be the beginning of the universe?
[00:44:55.680 --> 00:44:57.120]   - The first time we talked, you said,
[00:44:57.120 --> 00:45:00.420]   which is surreal to think that this discussion
[00:45:00.420 --> 00:45:02.680]   was happening and is becoming a reality.
[00:45:02.680 --> 00:45:05.900]   I asked you what question would you ask an AGI system
[00:45:05.900 --> 00:45:07.060]   once you create it?
[00:45:07.060 --> 00:45:09.260]   And you said, what's outside the simulation,
[00:45:09.260 --> 00:45:10.160]   is the question.
[00:45:10.160 --> 00:45:13.220]   Good question.
[00:45:13.220 --> 00:45:14.060]   - Yeah.
[00:45:14.060 --> 00:45:15.820]   - But it seems like with Grok, you started to,
[00:45:15.820 --> 00:45:19.840]   literally the system's goal is to be able
[00:45:19.840 --> 00:45:22.460]   to ask such questions, to answer such questions
[00:45:22.460 --> 00:45:24.060]   and to ask such questions.
[00:45:24.060 --> 00:45:25.400]   - Where are the aliens?
[00:45:25.400 --> 00:45:26.240]   - Where are the aliens?
[00:45:26.240 --> 00:45:28.740]   - That's one of the, like the Fermi paradox question.
[00:45:28.740 --> 00:45:33.060]   A lot of people have asked me if I've seen any evidence
[00:45:33.060 --> 00:45:36.940]   of aliens, and I haven't, which is kind of concerning,
[00:45:36.940 --> 00:45:41.020]   because then I think I'd probably prefer to at least
[00:45:41.020 --> 00:45:43.420]   have seen some archeological evidence of aliens.
[00:45:43.420 --> 00:45:46.580]   To the best of my knowledge, there is no proof.
[00:45:46.580 --> 00:45:49.060]   I'm not aware of any evidence of aliens.
[00:45:49.060 --> 00:45:52.700]   If they're out there, they're very subtle.
[00:45:52.700 --> 00:45:54.540]   We might just be the only consciousness,
[00:45:54.540 --> 00:45:55.800]   at least in the galaxy.
[00:45:55.800 --> 00:46:01.240]   And if you look at, say, the history of Earth,
[00:46:01.240 --> 00:46:03.420]   for one to believe the archeological record,
[00:46:03.420 --> 00:46:06.220]   Earth is about four and a half billion years old.
[00:46:06.220 --> 00:46:06.820]   Civilization.
[00:46:06.820 --> 00:46:09.340]   Civilization as measured from the first writing
[00:46:09.340 --> 00:46:11.880]   is only about 5,000 years old.
[00:46:11.880 --> 00:46:13.960]   We have to give some credit there to the ancient Sumerians
[00:46:13.960 --> 00:46:15.640]   who aren't around anymore.
[00:46:15.640 --> 00:46:18.420]   I think it was the archaic pre-cuneiform
[00:46:18.420 --> 00:46:21.420]   was the first actual symbolic representation,
[00:46:21.420 --> 00:46:23.020]   but only about 5,000 years ago.
[00:46:23.020 --> 00:46:27.000]   I think that's a good date for when,
[00:46:27.000 --> 00:46:28.740]   say, civilization started.
[00:46:28.740 --> 00:46:32.540]   That's one millionth of Earth's existence.
[00:46:32.540 --> 00:46:36.700]   So civilization has been around,
[00:46:36.700 --> 00:46:40.820]   it's really a flash in the pan so far.
[00:46:40.820 --> 00:46:50.700]   And why did it take so long?
[00:46:50.700 --> 00:46:54.100]   For four and a half billion years?
[00:46:54.100 --> 00:47:00.700]   For the vast majority of the time, there was no life,
[00:47:00.700 --> 00:47:03.740]   and then there was archaic bacteria for a very long time.
[00:47:06.580 --> 00:47:11.580]   You know, you had mitochondria get captured,
[00:47:11.580 --> 00:47:13.120]   multicellular life,
[00:47:13.120 --> 00:47:18.080]   differentiation into plants and animals,
[00:47:18.080 --> 00:47:20.700]   life moving from the oceans to land,
[00:47:20.700 --> 00:47:24.060]   mammals, higher brain functions.
[00:47:24.060 --> 00:47:27.700]   And the sun is expanding slowly,
[00:47:27.700 --> 00:47:34.260]   but it will overheat the Earth up.
[00:47:36.460 --> 00:47:38.800]   It's at some point in the future,
[00:47:38.800 --> 00:47:42.080]   boil the oceans and Earth will become like Venus,
[00:47:42.080 --> 00:47:45.140]   where life as we know it is impossible.
[00:47:45.140 --> 00:47:47.980]   So if we do not become multi-planetary
[00:47:47.980 --> 00:47:52.020]   and ultimately go beyond our solar system,
[00:47:52.020 --> 00:47:56.180]   annihilation of all life on Earth is a certainty.
[00:47:56.180 --> 00:47:58.160]   A certainty.
[00:47:58.160 --> 00:48:04.980]   And it could be as little as on the galactic timescale,
[00:48:06.340 --> 00:48:08.340]   half a billion years.
[00:48:08.340 --> 00:48:11.460]   You know, long time by human standards,
[00:48:11.460 --> 00:48:15.380]   but that's only 10% longer than Earth has been around at all.
[00:48:15.380 --> 00:48:21.520]   So if life had taken 10% longer to evolve on Earth,
[00:48:21.520 --> 00:48:23.480]   it wouldn't exist at all.
[00:48:23.480 --> 00:48:29.500]   - We got a deadline coming up.
[00:48:29.500 --> 00:48:30.340]   Better hurry.
[00:48:30.340 --> 00:48:32.060]   But that said, as you said,
[00:48:32.060 --> 00:48:36.220]   humans' intelligent life on Earth developed
[00:48:36.220 --> 00:48:38.380]   a lot of cool stuff very quickly.
[00:48:38.380 --> 00:48:42.440]   So it seems like becoming a multi-planetary
[00:48:42.440 --> 00:48:45.840]   is almost inevitable, unless we destroy this thing.
[00:48:45.840 --> 00:48:46.680]   - We need to do it.
[00:48:46.680 --> 00:48:51.680]   I mean, it's not, I mean, I suspect that there,
[00:48:51.680 --> 00:48:55.900]   if we are able to go out there and explore other star systems
[00:48:55.900 --> 00:48:58.600]   that we, there's a good chance we find a whole bunch
[00:48:58.600 --> 00:49:01.420]   of long dead one planet civilizations
[00:49:01.420 --> 00:49:03.220]   that never made it past their home planet.
[00:49:03.220 --> 00:49:04.060]   - That's so sad.
[00:49:04.060 --> 00:49:06.100]   - Yeah. - That's sad.
[00:49:06.100 --> 00:49:08.120]   - Also fascinating.
[00:49:08.120 --> 00:49:09.480]   - I mean, there are various explanations
[00:49:09.480 --> 00:49:11.020]   for the Fermi paradox.
[00:49:11.020 --> 00:49:13.880]   And one is the sort of, there's these great filters,
[00:49:13.880 --> 00:49:17.020]   which civilizations don't pass through.
[00:49:17.020 --> 00:49:19.140]   And one of those great filters is,
[00:49:19.140 --> 00:49:21.920]   do you become a multi-planet civilization or not?
[00:49:21.920 --> 00:49:24.280]   And if you don't, it's simply a matter of time
[00:49:24.280 --> 00:49:26.180]   before something happens on your planet,
[00:49:26.180 --> 00:49:31.740]   you know, either natural or manmade,
[00:49:31.740 --> 00:49:35.320]   that causes us to die out, like the dinosaurs.
[00:49:35.980 --> 00:49:36.820]   - Yeah.
[00:49:36.820 --> 00:49:37.980]   - And then you're like, oh,
[00:49:37.980 --> 00:49:38.820]   I'm gonna die out of space.
[00:49:38.820 --> 00:49:39.660]   - Yeah.
[00:49:39.660 --> 00:49:40.500]   - And then you're like, oh, I'm gonna die out of space.
[00:49:40.500 --> 00:49:41.340]   - Yeah.
[00:49:41.340 --> 00:49:42.180]   - And then you're like, oh, I'm gonna die out of space.
[00:49:42.180 --> 00:49:43.020]   - Yeah.
[00:49:43.020 --> 00:49:43.860]   - And then you're like, oh, I'm gonna die out of space.
[00:49:43.860 --> 00:49:44.680]   - Yeah.
[00:49:44.680 --> 00:49:45.520]   - And then you're like, oh, I'm gonna die out of space.
[00:49:45.520 --> 00:49:46.360]   - Yeah.
[00:49:46.360 --> 00:49:47.200]   - And then you're like, oh, I'm gonna die out of space.
[00:49:47.200 --> 00:49:48.040]   - Yeah.
[00:49:48.040 --> 00:49:48.880]   - And then you're like, oh, I'm gonna die out of space.
[00:49:48.880 --> 00:49:49.720]   - Yeah.
[00:49:49.720 --> 00:49:50.560]   - And then you're like, oh, I'm gonna die out of space.
[00:49:50.560 --> 00:49:51.400]   - Yeah.
[00:49:51.400 --> 00:49:52.240]   - And then you're like, oh, I'm gonna die out of space.
[00:49:52.240 --> 00:49:53.500]   - Nice aliens.
[00:49:53.500 --> 00:49:55.660]   - Just like the tribes in the Amazon.
[00:49:55.660 --> 00:49:58.760]   They all contact the tribes who are protecting them.
[00:49:58.760 --> 00:49:59.580]   That's what--
[00:49:59.580 --> 00:50:00.640]   - That would be a nice explanation.
[00:50:00.640 --> 00:50:03.420]   - Or you could have like, what was it?
[00:50:03.420 --> 00:50:06.740]   I think Andre Capati said it's like the ants
[00:50:06.740 --> 00:50:08.760]   in the Amazon asking, where's everybody?
[00:50:08.760 --> 00:50:12.920]   - Well, they do run into a lot of other ants.
[00:50:12.920 --> 00:50:13.760]   - That's true.
[00:50:13.760 --> 00:50:15.100]   - They have these ant wars.
[00:50:15.100 --> 00:50:18.080]   - Sounds like a good TV show.
[00:50:18.080 --> 00:50:19.500]   - Yeah, they literally have these big wars
[00:50:19.500 --> 00:50:20.540]   between various ants.
[00:50:20.540 --> 00:50:22.120]   - Yeah.
[00:50:22.120 --> 00:50:25.700]   - And then maybe I'm just dismissing
[00:50:25.700 --> 00:50:27.760]   all the different diversity of ants.
[00:50:27.760 --> 00:50:29.560]   - You should listen to that Werner Herzog
[00:50:29.560 --> 00:50:30.400]   talking about the jungle.
[00:50:30.400 --> 00:50:31.240]   It's really hilarious.
[00:50:31.240 --> 00:50:32.080]   Have you heard it?
[00:50:32.080 --> 00:50:32.920]   - No, I have not.
[00:50:32.920 --> 00:50:33.760]   - That's awesome.
[00:50:33.760 --> 00:50:34.600]   - But Werner Herzog has a way.
[00:50:34.600 --> 00:50:40.040]   - You should play it for the, you know,
[00:50:40.040 --> 00:50:42.860]   as an interlude in the, it's on YouTube.
[00:50:42.860 --> 00:50:43.800]   It's awesome.
[00:50:43.800 --> 00:50:46.360]   - I love him so much.
[00:50:46.360 --> 00:50:47.840]   - Yeah, he's great.
[00:50:47.840 --> 00:50:50.080]   - Was he the director of "Happy People, Life and the Tiger?"
[00:50:50.080 --> 00:50:51.200]   I think also.
[00:50:51.200 --> 00:50:52.000]   - He did that fair documentary.
[00:50:52.000 --> 00:50:52.840]   - The bear documentary.
[00:50:52.840 --> 00:50:54.120]   - He did this thing about penguins.
[00:50:54.120 --> 00:50:54.960]   - Yeah.
[00:50:54.960 --> 00:50:59.960]   - The psychoanalysis of a penguin.
[00:50:59.960 --> 00:51:03.940]   - Yeah, the penguin's like headed for like mountains
[00:51:03.940 --> 00:51:05.920]   like that are like 70 miles away.
[00:51:05.920 --> 00:51:08.700]   Penguin is just headed for doom basically.
[00:51:08.700 --> 00:51:10.180]   - Well, he had a cynical take.
[00:51:10.180 --> 00:51:13.180]   I have a, he could be just the brave explorer
[00:51:13.180 --> 00:51:16.940]   and there'll be great stories told about him
[00:51:16.940 --> 00:51:20.440]   amongst the penguin population for many centuries to come.
[00:51:21.880 --> 00:51:23.600]   - Yeah.
[00:51:23.600 --> 00:51:24.960]   - What were we talking about?
[00:51:24.960 --> 00:51:25.800]   Okay.
[00:51:25.800 --> 00:51:26.640]   - Prevents.
[00:51:26.640 --> 00:51:29.520]   - Yes, aliens.
[00:51:29.520 --> 00:51:31.560]   I mean, I don't know.
[00:51:31.560 --> 00:51:35.720]   Look, I think the smart move is just, you know,
[00:51:35.720 --> 00:51:38.820]   this is the first time in the history of earth
[00:51:38.820 --> 00:51:42.140]   that it's been possible for life to extend beyond earth.
[00:51:42.140 --> 00:51:44.840]   That window is open.
[00:51:44.840 --> 00:51:49.260]   Now it may be open for a long time
[00:51:49.260 --> 00:51:50.720]   or it may be open for a short time.
[00:51:50.720 --> 00:51:54.560]   It may be open now and then never open again.
[00:51:54.560 --> 00:51:58.980]   So I think the smart move here is to make life
[00:51:58.980 --> 00:52:01.560]   multi-planetary while it is possible to do so.
[00:52:01.560 --> 00:52:02.920]   We don't want to be one of those lame one planet
[00:52:02.920 --> 00:52:04.920]   civilizations that just dies out.
[00:52:04.920 --> 00:52:05.760]   - No, those are lame.
[00:52:05.760 --> 00:52:06.600]   - Yeah, lame.
[00:52:06.600 --> 00:52:11.280]   Self-respecting civilization would be one planet.
[00:52:11.280 --> 00:52:13.120]   - There's not going to be a Wikipedia entry
[00:52:13.120 --> 00:52:14.200]   for one of those.
[00:52:14.200 --> 00:52:18.180]   And pause.
[00:52:18.180 --> 00:52:20.600]   Does SpaceX have an official policy?
[00:52:20.600 --> 00:52:21.440]   - No.
[00:52:21.440 --> 00:52:22.260]   - Okay.
[00:52:22.260 --> 00:52:23.100]   That seems irresponsible.
[00:52:23.100 --> 00:52:26.100]   - I mean, look, if I see the slightest indication
[00:52:26.100 --> 00:52:28.100]   that there are aliens, I will immediately post
[00:52:28.100 --> 00:52:28.940]   on the next platform.
[00:52:28.940 --> 00:52:29.740]   - Yeah.
[00:52:29.740 --> 00:52:30.580]   - Anything I know.
[00:52:30.580 --> 00:52:33.580]   - It could be the most liked reposted post of all time.
[00:52:33.580 --> 00:52:34.420]   - Yeah.
[00:52:34.420 --> 00:52:37.360]   I mean, look, we have more satellites up there right now
[00:52:37.360 --> 00:52:39.280]   than everyone else combined.
[00:52:39.280 --> 00:52:43.280]   So, you know, we'd know, we know if we go to the next platform
[00:52:43.280 --> 00:52:45.280]   and we're going to be able to get a satellite
[00:52:45.280 --> 00:52:47.120]   that's going to be able to do that.
[00:52:47.120 --> 00:52:47.960]   - Yeah.
[00:52:47.960 --> 00:52:50.480]   - So, you know, we'd know, we know if we go to the next platform
[00:52:50.480 --> 00:52:52.300]   because we've got to maneuver around something
[00:52:52.300 --> 00:52:55.760]   and we're not, I don't have to maneuver around anything.
[00:52:55.760 --> 00:52:57.900]   - If we go to the big questions once again,
[00:52:57.900 --> 00:53:00.420]   you said you've, you're with Einstein,
[00:53:00.420 --> 00:53:02.820]   that you believe in the God of Spinoza.
[00:53:02.820 --> 00:53:05.300]   - Yes.
[00:53:05.300 --> 00:53:08.640]   - So, you know, that's a view that God is like the universe
[00:53:08.640 --> 00:53:12.100]   and is, reveals himself through the laws of physics
[00:53:12.100 --> 00:53:16.200]   or as Einstein said, through the lawful harmony of the world.
[00:53:16.200 --> 00:53:17.040]   - Yeah.
[00:53:17.040 --> 00:53:20.360]   I would agree that God of the simulator or whatever,
[00:53:20.360 --> 00:53:22.560]   the supreme being or beings
[00:53:22.560 --> 00:53:28.460]   reveal themselves through the physics.
[00:53:28.460 --> 00:53:31.280]   You know, they have creators of this existence
[00:53:31.280 --> 00:53:33.860]   and it's incumbent upon us to try to understand
[00:53:33.860 --> 00:53:36.740]   more about this wondrous creation.
[00:53:36.740 --> 00:53:39.460]   - Who created this thing?
[00:53:39.460 --> 00:53:40.980]   Who's running this thing?
[00:53:40.980 --> 00:53:43.920]   Like embodying it into a singular question
[00:53:43.920 --> 00:53:45.760]   with a sexy word on top of it
[00:53:45.760 --> 00:53:47.740]   is like focusing the mind to understand.
[00:53:47.740 --> 00:53:49.380]   It does seem like there's a,
[00:53:50.240 --> 00:53:52.100]   again, it could be an illusion.
[00:53:52.100 --> 00:53:54.500]   It's seem like there's a purpose
[00:53:54.500 --> 00:53:57.340]   that there's an underlying master plan of some kind.
[00:53:57.340 --> 00:53:58.920]   It seems like.
[00:53:58.920 --> 00:54:01.540]   - There may not be a master plan in the sense.
[00:54:01.540 --> 00:54:04.960]   So there's like, maybe an interesting answer
[00:54:04.960 --> 00:54:07.080]   to the question of determinism versus free will
[00:54:07.080 --> 00:54:09.760]   is that if we are in a simulation,
[00:54:09.760 --> 00:54:13.000]   the reason that these higher beings
[00:54:13.000 --> 00:54:15.720]   would hold a simulation is to see what happens.
[00:54:15.720 --> 00:54:20.120]   So it's not, they don't know what happens.
[00:54:20.120 --> 00:54:23.120]   Otherwise they wouldn't hold the simulation.
[00:54:23.120 --> 00:54:27.500]   So when humans create a simulation,
[00:54:27.500 --> 00:54:29.360]   so it's SpaceX and Tesla,
[00:54:29.360 --> 00:54:31.760]   we create simulations all the time,
[00:54:31.760 --> 00:54:32.900]   especially for the rocket.
[00:54:32.900 --> 00:54:37.900]   You know, you have to run a lot of simulations
[00:54:37.900 --> 00:54:39.560]   to understand what's going to happen
[00:54:39.560 --> 00:54:41.660]   because you can't really test the rocket
[00:54:41.660 --> 00:54:44.900]   until it goes to space and you want it to work.
[00:54:44.900 --> 00:54:50.000]   So you have to simulate subsonic, transonic,
[00:54:50.000 --> 00:54:53.980]   supersonic, hypersonic ascent,
[00:54:53.980 --> 00:54:56.380]   and then coming back super high heating
[00:54:56.380 --> 00:54:59.120]   and orbital dynamics.
[00:54:59.120 --> 00:55:00.320]   All this is going to be simulated.
[00:55:00.320 --> 00:55:04.340]   So, cause you don't get very many kicks at the can,
[00:55:04.340 --> 00:55:06.480]   but we run the simulations to see what happens.
[00:55:06.480 --> 00:55:07.680]   Not if we knew what happens,
[00:55:07.680 --> 00:55:09.840]   we wouldn't run the simulation.
[00:55:09.840 --> 00:55:13.360]   So if there's, so whoever created this existence,
[00:55:13.360 --> 00:55:18.380]   they're running it because they don't know
[00:55:18.380 --> 00:55:19.880]   what's going to happen.
[00:55:19.880 --> 00:55:20.720]   Because they do.
[00:55:20.720 --> 00:55:25.740]   - So maybe we both played Diablo.
[00:55:25.740 --> 00:55:28.300]   Maybe Diablo was created to see if a druid,
[00:55:28.300 --> 00:55:32.760]   your character, could defeat Uber Lilith at the end.
[00:55:32.760 --> 00:55:33.600]   They didn't know.
[00:55:33.600 --> 00:55:38.620]   - Well, the funny thing is Uber Lilith's title
[00:55:38.620 --> 00:55:40.700]   is Hatred Incarnate.
[00:55:40.700 --> 00:55:41.540]   - Yeah.
[00:55:41.540 --> 00:55:46.620]   - And right now, I guess you can ask the Diablo team,
[00:55:46.620 --> 00:55:49.760]   but it's almost impossible to,
[00:55:49.760 --> 00:55:54.760]   to defeat Hatred in the eternal realm.
[00:55:54.760 --> 00:55:57.620]   - Yeah, you've streamed yourself dominating
[00:55:57.620 --> 00:55:59.680]   tier 100 nightmare dungeons.
[00:55:59.680 --> 00:56:00.520]   - Yeah, I can.
[00:56:00.520 --> 00:56:01.360]   - And still.
[00:56:01.360 --> 00:56:04.580]   - I can cruise through tier 100 nightmare dungeons,
[00:56:04.580 --> 00:56:05.980]   like a stroll in the park.
[00:56:05.980 --> 00:56:06.820]   - Mm-hmm.
[00:56:06.820 --> 00:56:09.100]   And still you're defeated by Hatred.
[00:56:09.100 --> 00:56:11.400]   - Yeah, I can, there's the sort of,
[00:56:11.400 --> 00:56:14.440]   I guess maybe the second hottest boss is Durial.
[00:56:14.440 --> 00:56:16.240]   Durial can't even scratch the paint.
[00:56:16.240 --> 00:56:19.480]   So I killed Durial so many times.
[00:56:19.480 --> 00:56:22.020]   And every other boss in the game,
[00:56:22.020 --> 00:56:24.420]   all of them, kill him so many times, it's easy.
[00:56:24.420 --> 00:56:29.420]   But Uber Lilith, otherwise known as Hatred Incarnate,
[00:56:29.420 --> 00:56:34.360]   especially if you're a druid and you have no ability
[00:56:34.360 --> 00:56:35.860]   to go and to be invulnerable,
[00:56:35.860 --> 00:56:39.660]   there are these random death waves that come at you.
[00:56:39.660 --> 00:56:45.320]   And I'm pretty, you know, really I'm 52,
[00:56:45.320 --> 00:56:46.820]   so my reflex is not what they used to be,
[00:56:46.820 --> 00:56:49.360]   but I'm, I'm having a lifetime of playing video games,
[00:56:49.360 --> 00:56:53.060]   at one point I was, you know,
[00:56:53.060 --> 00:56:54.480]   maybe one of the best Quake players in the world.
[00:56:54.480 --> 00:56:59.480]   I actually won money for what I think was the first paid
[00:56:59.480 --> 00:57:01.640]   e-sports tournament in the US.
[00:57:01.640 --> 00:57:06.320]   We were doing four person Quake tournaments
[00:57:06.320 --> 00:57:09.500]   and we came second.
[00:57:09.500 --> 00:57:11.560]   I was the second best person on the team.
[00:57:11.560 --> 00:57:14.220]   And the actual best person that we were actually winning,
[00:57:14.220 --> 00:57:15.760]   we were going to come first, except the best person
[00:57:15.760 --> 00:57:18.280]   on the team, his computer crashed halfway through the game.
[00:57:18.280 --> 00:57:19.240]   So that's that.
[00:57:19.240 --> 00:57:23.280]   So we came second, but I got money for it and everything.
[00:57:23.280 --> 00:57:25.360]   So like, basically I got skills, you know,
[00:57:25.360 --> 00:57:28.660]   albeit, you know, no, no spring chicken these days.
[00:57:28.660 --> 00:57:33.060]   And the, to be totally frank, it's driving me crazy,
[00:57:33.060 --> 00:57:36.420]   trying to beat Lilith as a druid,
[00:57:36.420 --> 00:57:38.200]   basically trying to beat,
[00:57:38.200 --> 00:57:41.080]   trying to beat Hatred Incarnate in the eternal realm.
[00:57:41.080 --> 00:57:41.920]   - As a druid.
[00:57:41.920 --> 00:57:42.760]   - As a druid.
[00:57:42.760 --> 00:57:47.600]   And if you, if you, if you, this is really vexing,
[00:57:47.600 --> 00:57:48.440]   let me tell you.
[00:57:49.120 --> 00:57:50.820]   I mean, the challenge is part of the fun.
[00:57:50.820 --> 00:57:55.020]   I have seen directly, like you're actually like a world-class
[00:57:55.020 --> 00:57:56.900]   incredible video game player.
[00:57:56.900 --> 00:57:57.740]   - Yeah.
[00:57:57.740 --> 00:58:00.480]   - And I think Diablo, so you're just picking up a new game
[00:58:00.480 --> 00:58:03.080]   and you're figuring out its fundamentals.
[00:58:03.080 --> 00:58:05.680]   You're also with the Paragon board and the build
[00:58:05.680 --> 00:58:10.140]   are not somebody like me who perfectly follows
[00:58:10.140 --> 00:58:12.020]   whatever they suggest on the internet.
[00:58:12.020 --> 00:58:13.900]   You're also an innovator there.
[00:58:13.900 --> 00:58:14.740]   - Yeah.
[00:58:14.740 --> 00:58:17.200]   - Which is hilarious to watch.
[00:58:17.200 --> 00:58:19.000]   It's like a, it's like a mad scientist.
[00:58:19.000 --> 00:58:21.140]   Just trying to figure out the Paragon board
[00:58:21.140 --> 00:58:23.860]   and the build and the, you know.
[00:58:23.860 --> 00:58:26.400]   Is there some interesting insights there about
[00:58:26.400 --> 00:58:29.060]   if somebody's starting as a druid, do you have advice?
[00:58:29.060 --> 00:58:31.960]   - I would not recommend playing a druid
[00:58:31.960 --> 00:58:32.800]   in the eternal realm.
[00:58:32.800 --> 00:58:33.640]   - No.
[00:58:33.640 --> 00:58:36.360]   - Right now, I think the most powerful character
[00:58:36.360 --> 00:58:39.040]   in this, in the seasonal realm is the sorcerer
[00:58:39.040 --> 00:58:41.200]   with the lightning balls.
[00:58:41.200 --> 00:58:45.920]   The Sorcs have huge balls in the seasonal.
[00:58:45.920 --> 00:58:46.760]   - Well, yeah.
[00:58:46.760 --> 00:58:48.880]   That's what they say.
[00:58:48.880 --> 00:58:51.320]   - The Sorcs have huge balls.
[00:58:51.320 --> 00:58:54.820]   They do, huge balls of lightning.
[00:58:54.820 --> 00:58:56.400]   - I'll take your word for it.
[00:58:56.400 --> 00:58:57.920]   - And it's actually, in the seasonal realm,
[00:58:57.920 --> 00:59:02.440]   you can, you can, it's like pretty easy to beat Uvar Loth
[00:59:02.440 --> 00:59:04.720]   with the, because you get these vampiric powers
[00:59:04.720 --> 00:59:08.100]   that amplify your damage and increase your defense
[00:59:08.100 --> 00:59:09.360]   and whatnot.
[00:59:09.360 --> 00:59:14.360]   So, but really quite easy to defeat hatred seasonally,
[00:59:14.360 --> 00:59:18.760]   but to defeat hatred eternally, very difficult.
[00:59:18.760 --> 00:59:21.800]   Almost impossible, it's virtually impossible.
[00:59:21.800 --> 00:59:24.340]   It seems like this is a metaphor for life.
[00:59:24.340 --> 00:59:25.180]   - Yeah.
[00:59:25.180 --> 00:59:27.640]   I like the idea that Elon Musk,
[00:59:27.640 --> 00:59:29.140]   'cause I saw, I was playing Diablo yesterday
[00:59:29.140 --> 00:59:33.040]   and I saw 100, level 100 druid just run by,
[00:59:33.040 --> 00:59:34.040]   "I will never die."
[00:59:34.040 --> 00:59:36.560]   And then run back the other way.
[00:59:36.560 --> 00:59:39.600]   And there's just some, this metaphor is kind of hilarious
[00:59:39.600 --> 00:59:42.760]   that you, Elon Musk, is fighting hatred,
[00:59:42.760 --> 00:59:47.060]   restlessly fighting hatred in this demonic realm.
[00:59:47.060 --> 00:59:47.920]   - Yes.
[00:59:47.920 --> 00:59:48.640]   - It's hilarious.
[00:59:48.640 --> 00:59:49.940]   I mean, it's pretty hilarious.
[00:59:49.940 --> 00:59:50.780]   - No, it's absurd.
[00:59:50.780 --> 00:59:54.820]   Really, it's exercise in absurdity
[00:59:54.820 --> 00:59:56.100]   and it makes me want to pull my hair out.
[00:59:56.100 --> 00:59:56.940]   - Yeah.
[00:59:56.940 --> 00:59:59.600]   What do you get from video games in general?
[00:59:59.600 --> 01:00:03.520]   Is there, for you personally?
[01:00:03.520 --> 01:00:06.720]   - I mean, it's, I don't know if it's, it calms my mind.
[01:00:06.720 --> 01:00:10.960]   I mean, you sort of, killing the demons in a video game
[01:00:10.960 --> 01:00:12.540]   calms the demons in my mind.
[01:00:12.540 --> 01:00:15.640]   Yeah.
[01:00:15.640 --> 01:00:18.520]   If you play a tough video game, you can get into like
[01:00:18.520 --> 01:00:21.600]   a state of flow, which is very enjoyable.
[01:00:21.600 --> 01:00:26.600]   And admittedly, it needs to be not too easy, not too hard,
[01:00:26.600 --> 01:00:32.400]   kind of in the Goldilocks zone.
[01:00:32.400 --> 01:00:33.560]   And I guess you generally want to feel like
[01:00:33.560 --> 01:00:35.940]   you're progressing in the game.
[01:00:35.940 --> 01:00:39.460]   So, a good video.
[01:00:39.460 --> 01:00:44.460]   And there's also beautiful art, engaging storylines.
[01:00:44.460 --> 01:00:48.400]   And it's a, it's like, you know,
[01:00:48.400 --> 01:00:50.340]   it's like an amazing puzzle to solve, I think.
[01:00:50.340 --> 01:00:52.280]   And so it's like solving the puzzle.
[01:00:52.280 --> 01:00:54.520]   - "Elden Ring," the greatest game of all time.
[01:00:54.520 --> 01:00:56.720]   I still haven't played it, but to you.
[01:00:56.720 --> 01:00:57.940]   - "Elden Ring" is definitely a candidate
[01:00:57.940 --> 01:00:58.960]   for best game ever.
[01:00:58.960 --> 01:01:01.280]   Top five, for sure.
[01:01:01.280 --> 01:01:03.120]   - I think I've been scared how hard it is,
[01:01:03.120 --> 01:01:05.420]   or how hard I hear it is.
[01:01:05.420 --> 01:01:06.780]   So, but it is beautiful.
[01:01:06.780 --> 01:01:11.460]   - "Elden Ring" is, feels like it's designed by an alien.
[01:01:11.460 --> 01:01:15.760]   - There's a theme to this discussion.
[01:01:15.760 --> 01:01:16.600]   In what way?
[01:01:16.600 --> 01:01:18.280]   - It's, it's so unusual.
[01:01:18.280 --> 01:01:20.940]   It's incredibly creative and the art is stunning.
[01:01:20.940 --> 01:01:25.420]   I recommend playing it on a, on a big resolution,
[01:01:25.420 --> 01:01:27.940]   high dynamic range TV even.
[01:01:27.940 --> 01:01:30.000]   Doesn't need to be a monitor.
[01:01:30.000 --> 01:01:32.480]   Just the art is incredible.
[01:01:32.480 --> 01:01:33.340]   It's so beautiful.
[01:01:33.340 --> 01:01:36.320]   And it's, it's so unusual.
[01:01:36.320 --> 01:01:43.760]   And each of those top boss battles is unique.
[01:01:43.760 --> 01:01:46.240]   Like it's like a unique puzzle to solve.
[01:01:46.240 --> 01:01:47.200]   Each one's different.
[01:01:48.160 --> 01:01:51.080]   And the strategy you use to solve one battle
[01:01:51.080 --> 01:01:54.320]   is different from another battle.
[01:01:54.320 --> 01:01:56.060]   - That said, you said "Druid" and "Eternal"
[01:01:56.060 --> 01:02:00.060]   against "Uber Lilith" is the hardest boss battle you've ever.
[01:02:00.060 --> 01:02:00.900]   - Correct.
[01:02:00.900 --> 01:02:03.740]   That is currently the, and I've,
[01:02:03.740 --> 01:02:05.800]   I've played a lot of video games.
[01:02:05.800 --> 01:02:08.180]   'Cause it's my primary recreational activity.
[01:02:08.180 --> 01:02:09.020]   - Yes.
[01:02:09.020 --> 01:02:14.020]   - And yes, beating hatred in the internal realm
[01:02:18.040 --> 01:02:22.920]   is the hardest boss battle in life and in the video game.
[01:02:22.920 --> 01:02:25.080]   - Metaphor on top of metaphor.
[01:02:25.080 --> 01:02:28.100]   - I'm not sure it's possible, but it's,
[01:02:28.100 --> 01:02:29.240]   I do make progress.
[01:02:29.240 --> 01:02:31.260]   So then I'm like, okay, I'm making progress.
[01:02:31.260 --> 01:02:35.820]   Maybe if I just tweak that Paragon board a little more,
[01:02:35.820 --> 01:02:38.440]   I can do it.
[01:02:38.440 --> 01:02:43.080]   If just dodge a few more waves, I can do it.
[01:02:43.080 --> 01:02:45.420]   - Well, the simulation is created for the purpose
[01:02:45.420 --> 01:02:47.380]   of figuring out if it can be done.
[01:02:47.380 --> 01:02:50.240]   And you're just a cog in that simulation,
[01:02:50.240 --> 01:02:51.760]   in the machine of the simulation.
[01:02:51.760 --> 01:02:52.760]   - Yeah, it might be.
[01:02:52.760 --> 01:02:59.320]   I have a feeling that at least, I think.
[01:02:59.320 --> 01:03:03.220]   - It's doable.
[01:03:03.220 --> 01:03:04.300]   - It's doable, yes.
[01:03:04.300 --> 01:03:09.420]   - Well, that's the human spirit right there, to believe.
[01:03:09.420 --> 01:03:11.840]   - Yeah, I mean, it did prompt me to think
[01:03:11.840 --> 01:03:14.560]   about just hate in general, which is,
[01:03:14.560 --> 01:03:17.260]   you want to be careful of the,
[01:03:17.260 --> 01:03:20.460]   one of those things where you wish for something
[01:03:20.460 --> 01:03:24.980]   that sounds good, but if you get it,
[01:03:24.980 --> 01:03:27.600]   it's actually a dystopian situation.
[01:03:27.600 --> 01:03:30.140]   So, you know, you could sort of run a,
[01:03:30.140 --> 01:03:33.220]   sort of have artists of like,
[01:03:33.220 --> 01:03:35.840]   if you wish for world peace, sounds good,
[01:03:35.840 --> 01:03:38.000]   but how is it enforced?
[01:03:38.000 --> 01:03:44.400]   And at what cost is, at what cost eternal peace?
[01:03:44.400 --> 01:03:47.140]   It might actually be worse to have eternal peace.
[01:03:47.140 --> 01:03:50.040]   Because of what that would entail.
[01:03:50.040 --> 01:03:51.540]   The suppression of everyone.
[01:03:51.540 --> 01:03:54.840]   It might be the suppression of progress.
[01:03:54.840 --> 01:03:58.480]   It might be an ossified society that never changes.
[01:03:58.480 --> 01:04:05.000]   What I'm saying is that there is an argument that,
[01:04:05.000 --> 01:04:10.940]   you, that if you wish for no war,
[01:04:10.940 --> 01:04:14.680]   you should be careful what you wish for.
[01:04:14.680 --> 01:04:17.020]   Because what's required in order for there to be no war?
[01:04:17.020 --> 01:04:22.020]   It might be worse than a little war.
[01:04:22.020 --> 01:04:24.260]   - Yeah, the things we consider to be flaws
[01:04:24.260 --> 01:04:27.020]   of human civilization might be unnecessary components
[01:04:27.020 --> 01:04:29.040]   for whatever optimal looks like.
[01:04:29.040 --> 01:04:32.700]   I mean, this, do you worry about AI, AGI,
[01:04:32.700 --> 01:04:35.180]   enabling a dystopian state of this nature?
[01:04:35.180 --> 01:04:38.580]   Whether it's 1984 with surveillance and fear,
[01:04:38.580 --> 01:04:41.620]   or brave new world with pleasure.
[01:04:41.620 --> 01:04:44.600]   And what is it?
[01:04:44.600 --> 01:04:46.900]   Lots of sex, but no.
[01:04:46.900 --> 01:04:49.620]   - No deep human experience.
[01:04:49.620 --> 01:04:54.320]   - You know, there's actually a real drug called Soma.
[01:04:54.320 --> 01:04:55.160]   - There is?
[01:04:55.160 --> 01:04:55.980]   - Yes.
[01:04:55.980 --> 01:04:57.820]   - Let me take notes.
[01:04:57.820 --> 01:04:58.660]   - I'm like, I wonder if they actually-
[01:04:58.660 --> 01:05:00.080]   - Does Grok know about it?
[01:05:00.080 --> 01:05:02.460]   No, he'll probably go to brave new world.
[01:05:02.460 --> 01:05:03.780]   - We could ask it.
[01:05:03.780 --> 01:05:05.580]   - What would be the question?
[01:05:05.580 --> 01:05:07.900]   - Tell us about the drug called Soma.
[01:05:07.900 --> 01:05:15.520]   - It's gonna go to brave new world, I think.
[01:05:15.520 --> 01:05:16.780]   - Well, there's a real drug as well.
[01:05:16.780 --> 01:05:17.620]   - Yeah.
[01:05:17.620 --> 01:05:20.160]   - Searching for Soma drug information.
[01:05:20.160 --> 01:05:23.460]   - Oh boy.
[01:05:23.460 --> 01:05:25.920]   - Ah, Soma, the wonder drug.
[01:05:25.920 --> 01:05:27.660]   By the way, I'm in fun mode.
[01:05:27.660 --> 01:05:28.500]   - Oh, okay, fun.
[01:05:28.500 --> 01:05:31.480]   Yeah, so let's talk about the real drug.
[01:05:31.480 --> 01:05:32.940]   - Oh, it's a muscle relaxant
[01:05:32.940 --> 01:05:34.900]   that's been around since the 1950s.
[01:05:34.900 --> 01:05:37.020]   It's used to treat muscle pain and discomfort,
[01:05:37.020 --> 01:05:39.040]   and it does this by altering the activity
[01:05:39.040 --> 01:05:41.000]   between neurons and the spinal cord.
[01:05:41.000 --> 01:05:43.520]   Now, Soma isn't just a pain reliever.
[01:05:43.520 --> 01:05:45.220]   It's also a bit of a sedative.
[01:05:45.220 --> 01:05:46.140]   It can make you feel a bit drowsy, and it can make you feel a bit more comfortable.
[01:05:46.140 --> 01:05:48.740]   It's a bit drowsy, so it's not something to take
[01:05:48.740 --> 01:05:50.520]   if you're planning on operating heavy machinery
[01:05:50.520 --> 01:05:53.200]   or trying to solve complex math equations.
[01:05:53.200 --> 01:05:54.200]   And it goes on and on.
[01:05:54.200 --> 01:05:56.260]   In conclusion, Soma's a drug that can be helpful
[01:05:56.260 --> 01:05:57.780]   for those dealing with muscle pain,
[01:05:57.780 --> 01:06:00.300]   but it's not without its risks.
[01:06:00.300 --> 01:06:01.140]   - It's pretty good.
[01:06:01.140 --> 01:06:01.960]   - Oh, wait a minute.
[01:06:01.960 --> 01:06:03.320]   - That's the real drug you're talking about.
[01:06:03.320 --> 01:06:04.740]   - It's the real drug.
[01:06:04.740 --> 01:06:05.580]   - Yeah.
[01:06:05.580 --> 01:06:08.000]   - And of course, the fun is always use it responsibly
[01:06:08.000 --> 01:06:10.100]   in accordance with the advice of a healthcare professional.
[01:06:10.100 --> 01:06:12.280]   And remember, in the end, the universe is probably
[01:06:12.280 --> 01:06:14.000]   just a giant computer simulation,
[01:06:14.000 --> 01:06:16.020]   so don't take anything too seriously.
[01:06:16.020 --> 01:06:16.860]   - Oh, I love it.
[01:06:16.860 --> 01:06:17.680]   I love it.
[01:06:17.680 --> 01:06:18.520]   I love it.
[01:06:18.520 --> 01:06:19.360]   I love it.
[01:06:19.360 --> 01:06:20.200]   I love it.
[01:06:20.200 --> 01:06:21.040]   - Yeah, so.
[01:06:21.040 --> 01:06:24.200]   - Oh, and it also really nicely includes
[01:06:24.200 --> 01:06:29.200]   the tweets, posts on X that are relevant to this,
[01:06:29.200 --> 01:06:30.420]   which is really cool.
[01:06:30.420 --> 01:06:31.260]   - Yeah.
[01:06:31.260 --> 01:06:33.380]   - It integrates everything together.
[01:06:33.380 --> 01:06:34.220]   - Yeah.
[01:06:34.220 --> 01:06:36.880]   - The interface is one of the really cool things here.
[01:06:36.880 --> 01:06:38.420]   - Yeah, seems like a great job.
[01:06:38.420 --> 01:06:45.900]   So, and you could say, "No, I mean Soma,
[01:06:45.900 --> 01:06:48.140]   from Brave New World."
[01:06:48.140 --> 01:06:51.300]   'Cause it thinks you mean Soma, the real drug.
[01:06:51.300 --> 01:06:52.140]   - Yeah.
[01:06:52.140 --> 01:06:53.740]   - No, I mean Soma from Brave New World.
[01:06:53.740 --> 01:06:59.500]   - It's gonna be like, oops, I'm really sorry.
[01:06:59.500 --> 01:07:02.060]   I'm really surprised it went to the real drug first.
[01:07:02.060 --> 01:07:05.140]   Ah, my apologies.
[01:07:05.140 --> 01:07:06.360]   I was thinking of the muscle relax,
[01:07:06.360 --> 01:07:10.440]   and Soma from Brave New World is a different beast altogether.
[01:07:10.440 --> 01:07:11.660]   Now there's Huxley's dystopia.
[01:07:11.660 --> 01:07:15.160]   Now Soma is a drug that plays a central role in society,
[01:07:15.160 --> 01:07:16.000]   depicted in the book.
[01:07:16.000 --> 01:07:18.980]   It's a mood-altering hallucinogenic drug
[01:07:18.980 --> 01:07:21.040]   that's used to keep the population docile
[01:07:21.040 --> 01:07:22.900]   and content in the novel.
[01:07:22.900 --> 01:07:24.120]   To use Soma in Brave New World
[01:07:24.120 --> 01:07:25.340]   raises some interesting questions
[01:07:25.340 --> 01:07:26.720]   about the nature of happiness
[01:07:26.720 --> 01:07:28.500]   and the role of drugs in society.
[01:07:28.500 --> 01:07:30.660]   Oh, man.
[01:07:30.660 --> 01:07:31.500]   - Exactly.
[01:07:31.500 --> 01:07:32.620]   Is it better to live in a world
[01:07:32.620 --> 01:07:34.460]   where everyone is happy all the time,
[01:07:34.460 --> 01:07:37.320]   even if that happiness is artificial?
[01:07:37.320 --> 01:07:38.940]   It's a good question.
[01:07:38.940 --> 01:07:39.940]   This is what I mean.
[01:07:39.940 --> 01:07:44.940]   Like, do you wish for world peace and happiness all the time?
[01:07:45.040 --> 01:07:46.800]   Are you sure?
[01:07:46.800 --> 01:07:47.620]   - Mm-hmm.
[01:07:47.620 --> 01:07:51.140]   - Because that might be a society
[01:07:51.140 --> 01:07:53.980]   that is essentially sterile and ossified,
[01:07:53.980 --> 01:07:56.920]   that never changes, that is ultimately doomed.
[01:07:56.920 --> 01:08:03.420]   - This kind of tension between darkness and the light.
[01:08:03.420 --> 01:08:06.460]   - This is really a very good summary.
[01:08:06.460 --> 01:08:08.960]   It really gets to the point.
[01:08:08.960 --> 01:08:12.100]   This is not simply regurgitating Brave New World.
[01:08:12.100 --> 01:08:14.920]   It's actually getting to the salient,
[01:08:14.920 --> 01:08:17.920]   salient element of Soma as a drug.
[01:08:17.920 --> 01:08:22.160]   Do you actually want to be in a situation
[01:08:22.160 --> 01:08:23.600]   where everyone is happy all the time,
[01:08:23.600 --> 01:08:25.240]   even though it's artificial?
[01:08:25.240 --> 01:08:27.160]   Or is it better to confront the challenges of life
[01:08:27.160 --> 01:08:29.080]   and experience the full range of human emotions,
[01:08:29.080 --> 01:08:31.300]   even if it means experiencing pain and suffering?
[01:08:31.300 --> 01:08:32.900]   - For those listening, by the way,
[01:08:32.900 --> 01:08:35.180]   Ilan just read directly from Grok,
[01:08:35.180 --> 01:08:37.180]   which is a really nice, kind of insightful,
[01:08:37.180 --> 01:08:40.120]   philosophical analysis of the tension here.
[01:08:40.120 --> 01:08:41.220]   Interesting.
[01:08:41.220 --> 01:08:42.640]   - It pretty much nails it.
[01:08:42.640 --> 01:08:44.800]   In conclusion, Soma from Brave New World is fictional,
[01:08:44.800 --> 01:08:46.080]   a traditional drug that's used to explore
[01:08:46.080 --> 01:08:47.620]   some deep philosophical questions
[01:08:47.620 --> 01:08:49.120]   about the nature of happiness
[01:08:49.120 --> 01:08:51.180]   and the role of drugs in society.
[01:08:51.180 --> 01:08:52.700]   It's a powerful symbol of the dangers
[01:08:52.700 --> 01:08:55.460]   of using drugs to escape from reality
[01:08:55.460 --> 01:08:56.740]   and the importance of confronting
[01:08:56.740 --> 01:08:58.440]   the challenges of life head on.
[01:08:58.440 --> 01:09:01.520]   Nailed it.
[01:09:01.520 --> 01:09:03.760]   And the crazy thing is like, you know,
[01:09:03.760 --> 01:09:05.780]   we do have a real drug called Soma,
[01:09:05.780 --> 01:09:09.900]   which kind of is kind of like the drug in the book.
[01:09:09.900 --> 01:09:12.880]   And I'm like, they must have named it after or something.
[01:09:12.880 --> 01:09:14.680]   - Probably, probably.
[01:09:14.680 --> 01:09:17.860]   - But the real drug is quite effective on back pain.
[01:09:17.860 --> 01:09:18.900]   - So you know about this drug.
[01:09:18.900 --> 01:09:19.720]   - I've taken it.
[01:09:19.720 --> 01:09:21.080]   - This is fascinating.
[01:09:21.080 --> 01:09:21.920]   Okay.
[01:09:21.920 --> 01:09:26.400]   - 'Cause I had like a squashed disc in my C5, C6.
[01:09:26.400 --> 01:09:28.840]   - So it takes the physical pain away, but Soma here is-
[01:09:28.840 --> 01:09:29.680]   - It doesn't completely.
[01:09:29.680 --> 01:09:31.960]   It reduces the amount of pain you feel,
[01:09:31.960 --> 01:09:34.560]   but at the expense of mental acuity.
[01:09:34.560 --> 01:09:37.000]   It dulls your mind.
[01:09:37.000 --> 01:09:41.720]   Just like the drug in the book.
[01:09:41.720 --> 01:09:43.520]   - Just like the drug in the book.
[01:09:43.520 --> 01:09:44.560]   - Yeah. - And hence the trade off.
[01:09:44.560 --> 01:09:45.400]   - Yeah.
[01:09:45.400 --> 01:09:47.260]   - The thing that seems like utopia
[01:09:47.260 --> 01:09:49.160]   could be a dystopia after all.
[01:09:49.160 --> 01:09:49.980]   - Yeah.
[01:09:49.980 --> 01:09:51.480]   Actually, I was talking to a friend of mine
[01:09:51.480 --> 01:09:55.960]   saying like, would you really want there
[01:09:55.960 --> 01:09:57.640]   to be no hate in the world?
[01:09:57.640 --> 01:09:58.800]   Like really none?
[01:09:58.800 --> 01:10:01.440]   Like, I wonder why hate evolved.
[01:10:01.440 --> 01:10:05.260]   I'm not saying we should amplify hate, of course.
[01:10:05.260 --> 01:10:06.320]   I think we should try to minimize it,
[01:10:06.320 --> 01:10:08.580]   but none at all?
[01:10:08.580 --> 01:10:10.460]   Hmm.
[01:10:10.460 --> 01:10:12.060]   There might be a reason for hate.
[01:10:12.060 --> 01:10:14.440]   - And suffering.
[01:10:14.440 --> 01:10:16.900]   - It's really complicated to consider
[01:10:16.900 --> 01:10:19.640]   that some amount of human suffering is necessary
[01:10:19.640 --> 01:10:20.860]   for human flourishing.
[01:10:20.860 --> 01:10:23.900]   - Is it possible to appreciate the highs
[01:10:23.900 --> 01:10:26.260]   without knowing the lows?
[01:10:26.260 --> 01:10:31.260]   - And that all is summarized there
[01:10:31.260 --> 01:10:34.400]   in a single statement from Grok.
[01:10:34.400 --> 01:10:35.240]   Okay.
[01:10:35.240 --> 01:10:36.080]   - No highs, no lows.
[01:10:36.080 --> 01:10:36.920]   Who knows?
[01:10:36.920 --> 01:10:39.160]   - That's almost the poem.
[01:10:39.160 --> 01:10:44.320]   It seems that training LLMs efficiently is a big focus for XAI.
[01:10:44.320 --> 01:10:48.680]   What's the, first of all,
[01:10:48.680 --> 01:10:51.600]   what's the limit of what's possible in terms of efficiency?
[01:10:51.600 --> 01:10:56.600]   There's this terminology of useful productivity per watt.
[01:10:56.600 --> 01:10:59.440]   Like what have you learned from pushing the limits of that?
[01:10:59.440 --> 01:11:00.440]   - Well, I think it's helpful.
[01:11:00.440 --> 01:11:03.220]   The tools of physics are very powerful
[01:11:03.220 --> 01:11:04.200]   and can be applied, I think,
[01:11:04.200 --> 01:11:07.640]   to almost any, really any arena in life.
[01:11:07.640 --> 01:11:10.440]   It's really just critical thinking.
[01:11:10.440 --> 01:11:11.280]   For something important,
[01:11:11.280 --> 01:11:14.200]   you need to reason with from first principles
[01:11:14.200 --> 01:11:15.900]   and think about things in the limit,
[01:11:15.900 --> 01:11:17.200]   one direction or the other.
[01:11:17.200 --> 01:11:22.700]   So in the limit, even at the Kardashev scale,
[01:11:22.700 --> 01:11:25.440]   meaning even if you harness the entire power of the sun,
[01:11:25.440 --> 01:11:28.020]   you will still care about useful compute per watt.
[01:11:28.020 --> 01:11:31.480]   So that's where,
[01:11:31.480 --> 01:11:33.520]   I think probably where things are headed
[01:11:33.520 --> 01:11:37.520]   from the standpoint of AI
[01:11:37.520 --> 01:11:40.440]   is that we have a silicon shortage now
[01:11:40.440 --> 01:11:44.080]   that will transition to a voltage transformer shortage
[01:11:44.080 --> 01:11:46.640]   in about a year.
[01:11:46.640 --> 01:11:49.080]   Ironically, transformers for transformers.
[01:11:49.080 --> 01:11:52.940]   You need transformers to run transformers.
[01:11:52.940 --> 01:11:54.840]   - Somebody has a sense of humor in this thing.
[01:11:54.840 --> 01:11:59.540]   - I think, yes.
[01:11:59.540 --> 01:12:01.840]   Fate loves irony.
[01:12:01.840 --> 01:12:04.520]   Ironic humor.
[01:12:04.520 --> 01:12:06.680]   And ironically funny outcome
[01:12:06.680 --> 01:12:09.020]   seems to be often what fate wants.
[01:12:09.020 --> 01:12:10.680]   - Humor is all you need.
[01:12:10.680 --> 01:12:13.000]   I think spice is all you need, somebody posted.
[01:12:13.960 --> 01:12:16.300]   - Yeah, but yeah, so we're,
[01:12:16.300 --> 01:12:17.860]   I have a silicon shortage today,
[01:12:17.860 --> 01:12:21.380]   a voltage step down transformer shortage
[01:12:21.380 --> 01:12:22.400]   probably in about a year,
[01:12:22.400 --> 01:12:25.600]   and then just electricity shortages in general
[01:12:25.600 --> 01:12:26.800]   in about two years.
[01:12:26.800 --> 01:12:29.540]   I gave a speech for the sort of world gathering
[01:12:29.540 --> 01:12:31.740]   of utility companies, electricity companies.
[01:12:31.740 --> 01:12:34.760]   And I said, look, you really need to prepare
[01:12:34.760 --> 01:12:36.560]   for a tripling of electricity demand
[01:12:36.560 --> 01:12:40.220]   because all transport is gonna go electric
[01:12:40.220 --> 01:12:42.020]   with the ironic exception of rockets.
[01:12:43.840 --> 01:12:48.240]   And heating will also go electric.
[01:12:48.240 --> 01:12:53.380]   So energy usage right now is roughly one third,
[01:12:53.380 --> 01:12:55.840]   very rough terms, one third electricity,
[01:12:55.840 --> 01:12:57.700]   one third transport, one third heating.
[01:12:57.700 --> 01:13:03.320]   And so in order for everything to go sustainable,
[01:13:03.320 --> 01:13:08.320]   to go electric, you need to triple electricity output.
[01:13:08.320 --> 01:13:13.720]   So I encourage the utilities to build more power,
[01:13:13.720 --> 01:13:18.000]   more power plants, and also to probably have,
[01:13:18.000 --> 01:13:20.340]   well, not probably,
[01:13:20.340 --> 01:13:22.480]   they should definitely buy more batteries
[01:13:22.480 --> 01:13:25.480]   because the grid currently is sized for real time load,
[01:13:25.480 --> 01:13:27.820]   which is kind of crazy because, you know,
[01:13:27.820 --> 01:13:30.120]   that means you've got a size for whatever the peak
[01:13:30.120 --> 01:13:32.040]   electricity demand is, like the worst second
[01:13:32.040 --> 01:13:33.540]   or the worst day of the year,
[01:13:33.540 --> 01:13:35.540]   or you can have a brown out or blackout.
[01:13:35.540 --> 01:13:37.980]   And you're at that crazy blackout
[01:13:37.980 --> 01:13:39.820]   for several days in Austin.
[01:13:43.600 --> 01:13:48.060]   And there's almost no buffering of energy in the grid.
[01:13:48.060 --> 01:13:50.200]   Like if you've got a hydrant power plant,
[01:13:50.200 --> 01:13:54.900]   you can buffer energy, but otherwise it's all real time.
[01:13:54.900 --> 01:14:01.100]   So with batteries, you can produce energy at night
[01:14:01.100 --> 01:14:03.740]   and use it during the day.
[01:14:03.740 --> 01:14:04.820]   So you can buffer.
[01:14:04.820 --> 01:14:08.680]   So I expect that there will be very heavy usage
[01:14:08.680 --> 01:14:13.480]   of batteries in the future because the peak to trough ratio,
[01:14:13.480 --> 01:14:16.480]   for power plants is anywhere from two to five.
[01:14:16.480 --> 01:14:20.280]   You know, so it's like lowest point to highest point.
[01:14:20.280 --> 01:14:22.580]   - So like batteries are necessary to balance it out.
[01:14:22.580 --> 01:14:24.060]   And then, but the demand, as you're saying,
[01:14:24.060 --> 01:14:25.680]   is going to grow, grow, grow, grow.
[01:14:25.680 --> 01:14:26.520]   - Yeah.
[01:14:26.520 --> 01:14:27.860]   - And part of that is the compute.
[01:14:27.860 --> 01:14:30.820]   - Yes. Yes.
[01:14:30.820 --> 01:14:33.160]   I mean, electrification, I mean,
[01:14:33.160 --> 01:14:37.660]   electrification of transport and electric heating
[01:14:37.660 --> 01:14:40.240]   will be much bigger than AI, at least.
[01:14:40.240 --> 01:14:41.080]   - In the short term.
[01:14:41.080 --> 01:14:43.360]   - In the short term.
[01:14:43.360 --> 01:14:45.980]   - Even for AI, you really have a growing demand
[01:14:45.980 --> 01:14:48.600]   for electricity for electric vehicles
[01:14:48.600 --> 01:14:50.260]   and a growing demand for electricity
[01:14:50.260 --> 01:14:52.880]   for to run the computers for AI.
[01:14:52.880 --> 01:14:55.500]   And so this is obviously leading,
[01:14:55.500 --> 01:14:58.040]   can lead to an electricity shortage.
[01:14:58.040 --> 01:15:00.540]   - How difficult is the problem of,
[01:15:00.540 --> 01:15:02.780]   in this particular case,
[01:15:02.780 --> 01:15:06.420]   maximizing the useful productivity per watt
[01:15:06.420 --> 01:15:08.820]   for training, you know, nuts?
[01:15:08.820 --> 01:15:11.880]   Like this seems to be really where
[01:15:13.240 --> 01:15:16.820]   the big problem we're facing that needs to be solved
[01:15:16.820 --> 01:15:18.720]   is how to use the power efficiently.
[01:15:18.720 --> 01:15:22.020]   Like what you've learned so far
[01:15:22.020 --> 01:15:24.180]   about applying this physics,
[01:15:24.180 --> 01:15:25.620]   first principle of reasoning in this domain,
[01:15:25.620 --> 01:15:27.080]   how difficult is this problem?
[01:15:27.080 --> 01:15:30.540]   - It will get solved.
[01:15:30.540 --> 01:15:32.540]   It's just a question of how long it takes to solve it.
[01:15:32.540 --> 01:15:34.360]   So at various points, there's a limit,
[01:15:34.360 --> 01:15:36.360]   some kind of limiting factor to progress.
[01:15:36.360 --> 01:15:39.800]   And with regard to what I'm saying,
[01:15:39.800 --> 01:15:42.400]   right now, the limiting factor is silicon chips.
[01:15:42.400 --> 01:15:43.120]   - Mm-hmm.
[01:15:43.120 --> 01:15:44.880]   - And that will,
[01:15:44.880 --> 01:15:48.240]   we're gonna then have more chips
[01:15:48.240 --> 01:15:50.440]   than we can actually plug in and turn on,
[01:15:50.440 --> 01:15:52.500]   probably in about a year.
[01:15:52.500 --> 01:15:56.440]   The initial constraint being literally
[01:15:56.440 --> 01:15:58.160]   voltage step-down transformers,
[01:15:58.160 --> 01:16:03.160]   because you've got power coming in at 300,000 volts,
[01:16:03.160 --> 01:16:06.080]   and it's got to step all the way down eventually
[01:16:06.080 --> 01:16:09.120]   to around 0.7 volts.
[01:16:09.120 --> 01:16:11.640]   So it's a very big amount of,
[01:16:13.000 --> 01:16:15.000]   the voltage step-down is gigantic.
[01:16:15.000 --> 01:16:20.000]   And the industry is not used to rapid growth.
[01:16:20.000 --> 01:16:24.520]   - Okay, let's talk about the competition here.
[01:16:24.520 --> 01:16:26.660]   You've shown concern about Google and Microsoft
[01:16:26.660 --> 01:16:30.200]   with OpenAI developing AGI.
[01:16:30.200 --> 01:16:35.160]   How can you help ensure with XAI and Tesla AI work
[01:16:35.160 --> 01:16:37.500]   that it doesn't become a competitive race to AGI,
[01:16:37.500 --> 01:16:40.500]   but instead is a collaborative development of safe AGI?
[01:16:42.880 --> 01:16:45.920]   - I've been pushing for some kind of regulatory oversight
[01:16:45.920 --> 01:16:46.900]   for a long time.
[01:16:46.900 --> 01:16:48.540]   I've been somewhat of a Cassandra on the subject
[01:16:48.540 --> 01:16:50.160]   for over a decade.
[01:16:50.160 --> 01:16:56.060]   I think we want to be very careful in how we develop AI.
[01:16:56.060 --> 01:17:02.100]   It's a great power,
[01:17:02.100 --> 01:17:04.500]   and with great power comes great responsibility.
[01:17:04.500 --> 01:17:07.560]   I think it would be wise for us to have
[01:17:07.560 --> 01:17:12.560]   at least an objective third party who can,
[01:17:12.760 --> 01:17:16.320]   can be like a referee that can go in and understand
[01:17:16.320 --> 01:17:20.340]   what the various leading players are doing with AI.
[01:17:20.340 --> 01:17:23.120]   And even if there's no enforcement ability,
[01:17:23.120 --> 01:17:28.120]   they should, they can at least voice concerns publicly.
[01:17:28.120 --> 01:17:33.500]   You know, Jeff Hinton, for example,
[01:17:33.500 --> 01:17:37.160]   left Google and he voiced strong concerns,
[01:17:37.160 --> 01:17:38.960]   but now he's not at Google anymore.
[01:17:38.960 --> 01:17:41.600]   So who's going to voice the concerns?
[01:17:42.640 --> 01:17:45.840]   - I think, I think there's, I, I, I, like I, you know,
[01:17:45.840 --> 01:17:47.640]   Tesla gets a lot of regulatory oversight
[01:17:47.640 --> 01:17:49.060]   on the automotive front.
[01:17:49.060 --> 01:17:52.240]   And we're subject to, I think over a hundred regulatory
[01:17:52.240 --> 01:17:54.420]   agencies domestically and internationally.
[01:17:54.420 --> 01:17:56.260]   So it's a lot.
[01:17:56.260 --> 01:17:58.760]   You could fill this room with the old regulations
[01:17:58.760 --> 01:18:01.180]   that Tesla has to adhere to for automotive.
[01:18:01.180 --> 01:18:06.980]   Same is true in, you know, for rockets and for, you know,
[01:18:06.980 --> 01:18:12.520]   currently the limiting factor for SpaceX for Starship launch
[01:18:12.520 --> 01:18:13.880]   is regulatory approval.
[01:18:13.880 --> 01:18:15.220]   The FAA has actually given their approval,
[01:18:15.220 --> 01:18:19.360]   but we're waiting for Fish and Wildlife to finish their
[01:18:19.360 --> 01:18:20.700]   analysis and give their approval.
[01:18:20.700 --> 01:18:23.880]   That's why I posted, I want to buy a fish license on,
[01:18:23.880 --> 01:18:28.300]   which also refers to the Monty Python sketch.
[01:18:28.300 --> 01:18:29.140]   - Yeah.
[01:18:29.140 --> 01:18:31.720]   - Like, why do you need a license for your fish?
[01:18:31.720 --> 01:18:32.560]   I don't know.
[01:18:32.560 --> 01:18:35.260]   According to the rules,
[01:18:35.260 --> 01:18:39.200]   I'm told you need some sort of fish license or something.
[01:18:39.200 --> 01:18:42.400]   We effectively need a fish license to launch a rocket.
[01:18:42.400 --> 01:18:45.720]   And I'm like, wait a second,
[01:18:45.720 --> 01:18:47.300]   how did the fish come into this picture?
[01:18:47.300 --> 01:18:48.140]   - Yeah.
[01:18:48.140 --> 01:18:51.860]   - I mean, some of the things like that,
[01:18:51.860 --> 01:18:55.540]   I feel like are so absurd that I want to do like comedy
[01:18:55.540 --> 01:18:57.820]   sketch and flash at the bottom.
[01:18:57.820 --> 01:18:59.120]   This is all real.
[01:18:59.120 --> 01:19:00.860]   This is actually what happened.
[01:19:00.860 --> 01:19:04.500]   You know, one of the things that was a bit of a challenge
[01:19:04.500 --> 01:19:08.080]   at one point is that they were worried about a rocket
[01:19:08.080 --> 01:19:12.280]   hitting a shark and now the ocean is very big.
[01:19:12.280 --> 01:19:14.720]   And how often do you see sharks?
[01:19:14.720 --> 01:19:17.320]   Not that often, you know,
[01:19:17.320 --> 01:19:19.680]   as a percentage of ocean surface area sharks,
[01:19:19.680 --> 01:19:21.260]   basically are zero.
[01:19:21.260 --> 01:19:23.920]   And so then we said, well,
[01:19:23.920 --> 01:19:27.160]   how will we calculate the probability of telling a shark?
[01:19:27.160 --> 01:19:28.000]   And they're like, well,
[01:19:28.000 --> 01:19:30.060]   we can't give you that information because they're worried
[01:19:30.060 --> 01:19:34.560]   about shark fin hunters going and hunting sharks.
[01:19:34.560 --> 01:19:36.440]   And I said, well, how are we supposed to,
[01:19:36.440 --> 01:19:38.200]   we're on the horns of a dilemma then.
[01:19:38.200 --> 01:19:42.160]   Then they said, well, there's another part of fish,
[01:19:42.160 --> 01:19:44.620]   and wildlife that can do this analysis.
[01:19:44.620 --> 01:19:46.100]   And I'm like, well, why don't you give them the data?
[01:19:46.100 --> 01:19:48.060]   Like, we don't trust them.
[01:19:48.060 --> 01:19:51.100]   Like, excuse me, they're literally in your department.
[01:19:51.100 --> 01:19:53.600]   And again, this is actually what happened.
[01:19:53.600 --> 01:19:57.060]   And can you do an NDA or something?
[01:19:57.060 --> 01:20:01.360]   Eventually they managed to solve the internal quandary
[01:20:01.360 --> 01:20:04.180]   and indeed the probability of us hitting a shark
[01:20:04.180 --> 01:20:05.260]   is essentially zero.
[01:20:05.260 --> 01:20:08.700]   Then there's another organization
[01:20:08.700 --> 01:20:12.040]   that I didn't realize existed until, you know,
[01:20:12.040 --> 01:20:15.260]   a few months ago that cares about whether we would
[01:20:15.260 --> 01:20:18.820]   potentially hit a whale in international waters.
[01:20:18.820 --> 01:20:22.040]   Now, again, you look at the surface of the Pacific
[01:20:22.040 --> 01:20:27.040]   and say, what percentage of the Pacific consists of whale?
[01:20:27.040 --> 01:20:28.440]   Like, it'll give you a big picture and like,
[01:20:28.440 --> 01:20:29.800]   point out all the whales in this picture.
[01:20:29.800 --> 01:20:32.080]   And I'm like, I don't see any whales.
[01:20:32.080 --> 01:20:35.220]   It's like basically 0%.
[01:20:35.220 --> 01:20:36.840]   And if our rocket does hit a whale,
[01:20:36.840 --> 01:20:39.180]   which is extremely unlikely beyond all belief,
[01:20:41.920 --> 01:20:46.360]   fate had it as a whale has some seriously bad luck.
[01:20:46.360 --> 01:20:48.360]   You know, it's the least lucky whale ever.
[01:20:48.360 --> 01:20:50.900]   And-
[01:20:50.900 --> 01:20:52.740]   - I mean, this is quite absurd.
[01:20:52.740 --> 01:20:53.580]   - Yeah.
[01:20:53.580 --> 01:20:57.000]   - The bureaucracy of this, however it emerged.
[01:20:57.000 --> 01:20:59.600]   - Yes, well, I mean, one of the things that's pretty wild
[01:20:59.600 --> 01:21:03.940]   is for launching out of Vanderburg in California,
[01:21:03.940 --> 01:21:09.240]   we had to, they were worried about seal procreation,
[01:21:09.240 --> 01:21:11.800]   whether the seals would be dismayed by the sonic booms.
[01:21:11.800 --> 01:21:15.220]   Now there've been a lot of rockets launched out of Vanderburg
[01:21:15.220 --> 01:21:18.560]   and the seal population has steadily increased.
[01:21:18.560 --> 01:21:21.460]   So if anything, rocket booms are an aphrodisiac
[01:21:21.460 --> 01:21:23.140]   based on the evidence,
[01:21:23.140 --> 01:21:27.700]   if you correlate rocket launches with seal population.
[01:21:27.700 --> 01:21:31.920]   Nonetheless, we were forced to kidnap a seal,
[01:21:31.920 --> 01:21:36.320]   strap it to a board, put headphones on the seal
[01:21:36.320 --> 01:21:38.120]   and play sonic boom sounds to it
[01:21:38.120 --> 01:21:39.820]   to see if it would be distressed.
[01:21:41.680 --> 01:21:45.420]   - The actual thing that happened, this is actually real.
[01:21:45.420 --> 01:21:46.260]   I have pictures.
[01:21:46.260 --> 01:21:49.820]   - I would love to see this.
[01:21:49.820 --> 01:21:50.980]   Yeah.
[01:21:50.980 --> 01:21:53.440]   I mean, sorry, there's a seal with headphones.
[01:21:53.440 --> 01:21:58.440]   - Yes, it's a seal with headphones strapped to a board.
[01:21:58.440 --> 01:22:02.460]   And like, okay, now the amazing part
[01:22:02.460 --> 01:22:04.360]   is how calm the seal was.
[01:22:04.360 --> 01:22:05.200]   - Yeah.
[01:22:05.200 --> 01:22:07.060]   - Because if I was a seal, I'd be like, this is the end.
[01:22:07.060 --> 01:22:09.880]   They're definitely gonna eat me.
[01:22:09.880 --> 01:22:10.720]   - Yeah.
[01:22:11.560 --> 01:22:12.640]   - How old the seal,
[01:22:12.640 --> 01:22:16.380]   when a seal goes back to other seal friends,
[01:22:16.380 --> 01:22:17.540]   how's he gonna explain that?
[01:22:17.540 --> 01:22:18.540]   - I'm never gonna believe him.
[01:22:18.540 --> 01:22:19.380]   - Never gonna believe him.
[01:22:19.380 --> 01:22:20.740]   That's why I'm like, well,
[01:22:20.740 --> 01:22:22.600]   it's sort of like, it's like getting kidnapped by aliens
[01:22:22.600 --> 01:22:23.740]   and getting an anal probe, you know?
[01:22:23.740 --> 01:22:27.180]   You come back and say, I swear to God,
[01:22:27.180 --> 01:22:28.140]   I got kidnapped by aliens
[01:22:28.140 --> 01:22:29.960]   and they stuck an anal probe in my butt.
[01:22:29.960 --> 01:22:32.560]   And people are like, no, they didn't.
[01:22:32.560 --> 01:22:33.400]   That's ridiculous.
[01:22:33.400 --> 01:22:34.240]   - Yeah, it's both sides of the same thing.
[01:22:34.240 --> 01:22:36.280]   - How's a seal's buddies are never gonna believe him
[01:22:36.280 --> 01:22:37.280]   that he gets strapped to a board
[01:22:37.280 --> 01:22:38.720]   and they put headphones on his ears.
[01:22:38.720 --> 01:22:41.440]   And then let him go.
[01:22:41.440 --> 01:22:44.440]   - Twice, by the way.
[01:22:44.440 --> 01:22:45.800]   We had to do it twice.
[01:22:45.800 --> 01:22:48.120]   - They let him go twice.
[01:22:48.120 --> 01:22:48.960]   - We had to capture.
[01:22:48.960 --> 01:22:49.800]   - The same seal.
[01:22:49.800 --> 01:22:50.640]   - Well, no, different seal.
[01:22:50.640 --> 01:22:51.480]   - Oh, okay.
[01:22:51.480 --> 01:22:54.200]   Did you get a seal of approval?
[01:22:54.200 --> 01:22:55.040]   - Yeah, exactly.
[01:22:55.040 --> 01:22:56.900]   I got a seal of approval.
[01:22:56.900 --> 01:22:58.440]   No, I mean, this is like,
[01:22:58.440 --> 01:22:59.620]   I don't think the public is quite aware
[01:22:59.620 --> 01:23:01.840]   of the madness that goes on.
[01:23:01.840 --> 01:23:05.140]   - Yeah, it's absurd.
[01:23:05.140 --> 01:23:07.380]   - Frickin' seals with frickin' headphones.
[01:23:07.380 --> 01:23:08.220]   I mean, this is the,
[01:23:08.220 --> 01:23:11.320]   it's a good encapsulation of the absurdity
[01:23:11.320 --> 01:23:13.660]   of human civilization, seals and headphones.
[01:23:13.660 --> 01:23:14.500]   - Yes.
[01:23:14.500 --> 01:23:18.520]   - What are the pros and cons of open sourcing AI to you
[01:23:18.520 --> 01:23:22.860]   as another way to combat, you know,
[01:23:22.860 --> 01:23:24.760]   a company running away with AGI?
[01:23:24.760 --> 01:23:32.920]   - In order to run like really deep intelligence,
[01:23:32.920 --> 01:23:34.360]   you need a lot of compute.
[01:23:34.360 --> 01:23:35.860]   So it's not like, you know,
[01:23:35.860 --> 01:23:38.800]   you can just fire up a PC in your basement
[01:23:38.800 --> 01:23:41.200]   and be running AGI at least, you know,
[01:23:41.200 --> 01:23:42.040]   not yet.
[01:23:42.040 --> 01:23:45.040]   You know, Grok was trained on 8,000 A100s
[01:23:45.040 --> 01:23:51.620]   running at peak efficiency.
[01:23:51.620 --> 01:23:56.380]   And Grok's gonna get a lot better, by the way.
[01:23:56.380 --> 01:23:59.280]   We'll be more than doubling our compute
[01:23:59.280 --> 01:24:02.400]   every couple months for the next several months.
[01:24:02.400 --> 01:24:03.960]   - There's a nice write up of how it went
[01:24:03.960 --> 01:24:06.020]   from Grok zero to Grok one.
[01:24:06.020 --> 01:24:06.860]   - By Grok?
[01:24:06.860 --> 01:24:09.760]   - Yeah, by Grok just bragging,
[01:24:09.760 --> 01:24:11.080]   making shit up about himself.
[01:24:11.080 --> 01:24:13.580]   Just Grok, Grok, Grok.
[01:24:13.580 --> 01:24:14.420]   - Yeah.
[01:24:14.420 --> 01:24:19.580]   - It's like a weird AI dating site
[01:24:19.580 --> 01:24:21.240]   where it exaggerates about itself.
[01:24:21.240 --> 01:24:23.240]   No, there's a write up of, you know,
[01:24:23.240 --> 01:24:26.220]   like where it stands now, the history of its development.
[01:24:26.220 --> 01:24:31.500]   And where it stands on some benchmarks
[01:24:31.500 --> 01:24:33.760]   compared to the state of the art GPT-3.5.
[01:24:33.760 --> 01:24:37.620]   And so, I mean, there's, you know, there's Lama.
[01:24:37.620 --> 01:24:39.620]   You can open source, once it's trained,
[01:24:39.620 --> 01:24:40.960]   you can open source a model.
[01:24:40.960 --> 01:24:43.300]   And for fine tuning and all that kind of stuff.
[01:24:43.300 --> 01:24:46.260]   Like what to use the pros and cons of that,
[01:24:46.260 --> 01:24:47.960]   of open sourcing based models.
[01:24:47.960 --> 01:24:55.780]   - I think the summary to open sourcing,
[01:24:55.780 --> 01:24:58.680]   I think perhaps with a slight time delay, you know,
[01:24:58.680 --> 01:25:00.380]   I don't know, six months even.
[01:25:00.380 --> 01:25:06.280]   I think I'm generally in favor of open sourcing.
[01:25:06.280 --> 01:25:08.020]   Like bias towards open sourcing.
[01:25:10.840 --> 01:25:14.220]   I mean, it is a concern to me that, you know, opening AI,
[01:25:14.220 --> 01:25:17.820]   you know, I was, you know, I think, I guess,
[01:25:17.820 --> 01:25:22.820]   arguably the prime, you know, prime mover behind open AI
[01:25:22.820 --> 01:25:26.880]   in the sense that it was created because of discussions
[01:25:26.880 --> 01:25:28.440]   that I had with Larry Page,
[01:25:28.440 --> 01:25:31.640]   back when he and I were friends and, you know,
[01:25:31.640 --> 01:25:34.340]   stay at his house and talk to him about AI safety.
[01:25:34.340 --> 01:25:36.840]   And Larry did not care about AI safety,
[01:25:36.840 --> 01:25:38.840]   at least at the time he didn't, you know.
[01:25:38.840 --> 01:25:40.720]   And at one point he called me and said, you know,
[01:25:40.720 --> 01:25:42.620]   he called me a speciesist for being pro-human.
[01:25:42.620 --> 01:25:45.020]   And I'm like, well, what team are you on Larry?
[01:25:45.020 --> 01:25:47.140]   He's doing team robot.
[01:25:47.140 --> 01:25:49.780]   I'm like, okay.
[01:25:49.780 --> 01:25:54.780]   So at the time, you know, Google had acquired DeepMind.
[01:25:54.780 --> 01:25:59.640]   They had probably two thirds of all AI research, you know,
[01:25:59.640 --> 01:26:02.320]   probably two thirds of all the AI researchers in the world.
[01:26:02.320 --> 01:26:05.460]   They had basically infinite money in compute.
[01:26:05.460 --> 01:26:10.600]   And the guy in charge, you know,
[01:26:10.600 --> 01:26:14.460]   Larry Page did not care about safety and even yelled at me
[01:26:14.460 --> 01:26:20.080]   and called me a speciesist and being pro-human.
[01:26:20.080 --> 01:26:21.680]   - So I don't know if you notice about humans,
[01:26:21.680 --> 01:26:24.620]   they can change their mind and maybe you and Larry Page
[01:26:24.620 --> 01:26:27.020]   can still can be friends once more.
[01:26:27.020 --> 01:26:29.020]   - I'd like to be friends with Larry again.
[01:26:29.020 --> 01:26:35.180]   Really the breaking of the friendship was over opening AI.
[01:26:35.180 --> 01:26:37.980]   And specifically,
[01:26:40.480 --> 01:26:44.140]   I think the key moment was recruiting Ilya Sutskaya.
[01:26:44.140 --> 01:26:47.100]   So.
[01:26:47.100 --> 01:26:48.860]   - I love Ilya, he's so brilliant.
[01:26:48.860 --> 01:26:52.240]   - Ilya's a good human, smart, good heart.
[01:26:52.240 --> 01:26:57.860]   And that was a tough recruiting battle.
[01:26:57.860 --> 01:27:02.200]   It was mostly Demis on one side and me on the other,
[01:27:02.200 --> 01:27:04.240]   both trying to recruit Ilya.
[01:27:04.240 --> 01:27:05.860]   And Ilya went back and forth,
[01:27:05.860 --> 01:27:08.760]   he was gonna stay at Google,
[01:27:08.760 --> 01:27:10.360]   then he was gonna leave, then he was gonna stay,
[01:27:10.360 --> 01:27:14.520]   and finally he did agree to join opening AI.
[01:27:14.520 --> 01:27:17.580]   That was one of the toughest recruiting battles we ever had.
[01:27:17.580 --> 01:27:20.300]   And, but that was really the linchpin
[01:27:20.300 --> 01:27:22.880]   for opening AI being successful.
[01:27:22.880 --> 01:27:24.340]   And I was, you know,
[01:27:24.340 --> 01:27:27.180]   also instrumental in recruiting a number of other people.
[01:27:27.180 --> 01:27:29.680]   And I provided all of the funding in the beginning,
[01:27:29.680 --> 01:27:31.540]   over $40 million.
[01:27:31.540 --> 01:27:34.980]   And the name.
[01:27:34.980 --> 01:27:40.240]   The open in opening AI is supposed to mean open source.
[01:27:40.240 --> 01:27:42.320]   And it was created as a nonprofit open source,
[01:27:42.320 --> 01:27:44.740]   and now it is a closed source for maximum profit.
[01:27:44.740 --> 01:27:48.760]   Which I think is not good karma.
[01:27:48.760 --> 01:27:54.060]   - But like we talked about with war and leaders talking,
[01:27:54.060 --> 01:27:57.880]   I do hope that there's only a few folks working on this
[01:27:57.880 --> 01:27:58.720]   at the highest level.
[01:27:58.720 --> 01:28:02.760]   I do hope you reinvigorate friendships here.
[01:28:02.760 --> 01:28:05.260]   - Like I said, I'd like to be friends again with Larry.
[01:28:05.260 --> 01:28:06.560]   I haven't seen him in ages.
[01:28:06.560 --> 01:28:10.120]   And we were friends for a very long time.
[01:28:10.120 --> 01:28:13.400]   - I met Larry Page before he got funding for Google.
[01:28:13.400 --> 01:28:15.200]   Or actually, I guess before he got venture funding,
[01:28:15.200 --> 01:28:18.160]   I think he got the first like 100K from,
[01:28:18.160 --> 01:28:20.460]   I think back to Alzheimer's or something.
[01:28:20.460 --> 01:28:23.220]   - It's wild to think about all that happened.
[01:28:23.220 --> 01:28:25.900]   And even guys knowing each other that whole time.
[01:28:25.900 --> 01:28:26.740]   Just 20 years.
[01:28:26.740 --> 01:28:28.560]   - Yeah, since maybe '98 or something.
[01:28:28.560 --> 01:28:29.640]   - Yeah, it's crazy.
[01:28:29.640 --> 01:28:31.740]   Crazy how much has happened since then.
[01:28:31.740 --> 01:28:32.840]   - Yeah, 25 years.
[01:28:32.840 --> 01:28:36.500]   A lot has happened since then.
[01:28:36.500 --> 01:28:37.820]   - But you're seeing the tension there.
[01:28:37.820 --> 01:28:40.000]   Like maybe delayed open source.
[01:28:40.000 --> 01:28:42.080]   - Delayed, yeah.
[01:28:42.080 --> 01:28:44.480]   Like what is the source that is open?
[01:28:44.480 --> 01:28:46.260]   You know what I mean?
[01:28:46.260 --> 01:28:49.500]   Like there's basically, it's a giant CSV file.
[01:28:49.500 --> 01:28:51.980]   - Yeah, yeah.
[01:28:51.980 --> 01:28:53.220]   - With a bunch of numbers.
[01:28:53.220 --> 01:28:54.060]   - Yep.
[01:28:54.060 --> 01:28:58.260]   - What do you do with that giant file of numbers?
[01:28:58.260 --> 01:29:00.720]   How do you run?
[01:29:00.720 --> 01:29:05.860]   Like the amount of actual, the lines of code is very small.
[01:29:05.860 --> 01:29:09.880]   And most of the,
[01:29:09.880 --> 01:29:14.880]   the software work is in the curation of the data.
[01:29:14.880 --> 01:29:19.260]   So it's like trying to figure out what data is,
[01:29:19.260 --> 01:29:21.160]   separating good data from bad data.
[01:29:21.160 --> 01:29:23.700]   Like you can't just crawl the internet
[01:29:23.700 --> 01:29:26.260]   'cause there's a lot of junk out there.
[01:29:26.260 --> 01:29:29.440]   A huge percentage of websites have more noise than signal.
[01:29:29.440 --> 01:29:33.900]   'Cause they're just used for search engine optimization.
[01:29:33.900 --> 01:29:36.120]   They're literally just scam websites.
[01:29:39.160 --> 01:29:40.960]   - How do you, by the way, Sergeant Interrupt,
[01:29:40.960 --> 01:29:44.160]   get the signal, separate the signal and noise on X?
[01:29:44.160 --> 01:29:46.220]   That's such a fascinating source of data.
[01:29:46.220 --> 01:29:49.420]   You know, no offense to people posting on X,
[01:29:49.420 --> 01:29:51.940]   but sometimes there's a little bit of noise.
[01:29:51.940 --> 01:29:52.780]   So what--
[01:29:52.780 --> 01:29:53.620]   - Yeah, I think the signal and noise
[01:29:53.620 --> 01:29:54.460]   could be greatly improved.
[01:29:54.460 --> 01:29:59.460]   I mean, really all of the posts on the X platform
[01:29:59.460 --> 01:30:01.840]   should be AI recommended.
[01:30:01.840 --> 01:30:03.620]   Meaning like we should populate a vector space
[01:30:03.620 --> 01:30:05.360]   around any given post,
[01:30:05.360 --> 01:30:07.860]   compare that to the vector space around any user,
[01:30:07.860 --> 01:30:09.040]   and match the two.
[01:30:09.040 --> 01:30:09.880]   - Mm-hmm.
[01:30:09.880 --> 01:30:13.060]   - Right now there is a little bit of AI used
[01:30:13.060 --> 01:30:17.340]   for the recommended posts,
[01:30:17.340 --> 01:30:20.260]   but it's mostly heuristics.
[01:30:20.260 --> 01:30:22.120]   And if there's a reply,
[01:30:22.120 --> 01:30:24.920]   where the reply to a post could be much better
[01:30:24.920 --> 01:30:26.180]   than the original post,
[01:30:26.180 --> 01:30:29.140]   but it will, according to the current rules of the system,
[01:30:29.140 --> 01:30:31.600]   get almost no attention compared to a primary post.
[01:30:31.600 --> 01:30:35.620]   - Oh, so a lot of that, I got the sense,
[01:30:35.620 --> 01:30:38.920]   so a lot of the X algorithm has been open source,
[01:30:38.920 --> 01:30:40.800]   and been written up about,
[01:30:40.800 --> 01:30:43.060]   and it seems that there to be some machine learning
[01:30:43.060 --> 01:30:44.460]   is disparate, but there's some machine learning.
[01:30:44.460 --> 01:30:46.640]   - There's a little, there's a little bit.
[01:30:46.640 --> 01:30:48.220]   But it needs to be entirely that.
[01:30:48.220 --> 01:30:49.540]   Like at least in the,
[01:30:49.540 --> 01:30:51.420]   like if you explicitly follow someone, that's one thing,
[01:30:51.420 --> 01:30:55.020]   but if you, in terms of what is recommended
[01:30:55.020 --> 01:30:56.680]   from people that you don't follow,
[01:30:56.680 --> 01:30:58.180]   that should all be AI.
[01:30:58.180 --> 01:30:59.600]   - I mean, it's a fascinating problem.
[01:30:59.600 --> 01:31:00.440]   - Yeah.
[01:31:00.440 --> 01:31:01.840]   - So there's several aspects of it that's fascinating.
[01:31:01.840 --> 01:31:04.200]   First, as the write-up goes,
[01:31:04.200 --> 01:31:06.420]   it first picks 1,500 tweets from a pool
[01:31:06.420 --> 01:31:07.860]   of hundreds of millions.
[01:31:07.860 --> 01:31:08.800]   First of all, that's--
[01:31:08.800 --> 01:31:09.640]   - Fascinating.
[01:31:09.640 --> 01:31:10.460]   - Fascinating.
[01:31:10.460 --> 01:31:12.100]   'Cause you have hundreds of millions of posts
[01:31:12.100 --> 01:31:14.860]   every single day, and it has to pick 1,500,
[01:31:14.860 --> 01:31:18.380]   from which it then does, obviously people you follow,
[01:31:18.380 --> 01:31:20.900]   but then there's also like some kind of clustering
[01:31:20.900 --> 01:31:23.840]   it has to do to figure out what kind of human are you,
[01:31:23.840 --> 01:31:26.640]   what kind of new clusters might be relevant to you,
[01:31:26.640 --> 01:31:27.520]   people like you.
[01:31:27.520 --> 01:31:29.860]   This kind of problem is just fascinating,
[01:31:29.860 --> 01:31:32.320]   'cause it has to then rank those 1,500
[01:31:32.320 --> 01:31:35.820]   with some filtering.
[01:31:35.820 --> 01:31:36.660]   - Yeah.
[01:31:36.660 --> 01:31:38.680]   - And then recommend you just a handful.
[01:31:38.680 --> 01:31:41.840]   And to me, what's really fascinating
[01:31:41.840 --> 01:31:43.340]   is how fast it has to do that.
[01:31:43.340 --> 01:31:45.260]   So currently that entire pipeline
[01:31:45.260 --> 01:31:47.460]   to go from several hundreds of million
[01:31:47.460 --> 01:31:52.120]   to a handful takes 220 seconds of CPU time,
[01:31:52.120 --> 01:31:53.420]   single CPU time.
[01:31:53.420 --> 01:31:54.260]   - Yeah.
[01:31:54.260 --> 01:31:57.100]   - And then it has to do that in like a second.
[01:31:57.100 --> 01:32:00.220]   So it has to be like super distributed in fascinating ways.
[01:32:00.220 --> 01:32:03.660]   Like there's just a lot of tweets.
[01:32:03.660 --> 01:32:04.500]   - There's a lot.
[01:32:04.500 --> 01:32:06.420]   There's a lot of stuff on the system.
[01:32:06.420 --> 01:32:08.560]   And I think, but I think right now it's,
[01:32:08.560 --> 01:32:11.860]   it's not currently good at recommending things
[01:32:11.860 --> 01:32:14.400]   from accounts you don't follow.
[01:32:14.400 --> 01:32:15.620]   - Yeah.
[01:32:15.620 --> 01:32:20.320]   - Or where there's more than one degree of separation.
[01:32:20.320 --> 01:32:23.240]   So, you know, it's pretty good if there's at least
[01:32:23.240 --> 01:32:26.020]   like some commonality between someone you follow
[01:32:26.020 --> 01:32:30.500]   liked something or reposted it or commented on it
[01:32:30.500 --> 01:32:32.320]   or something like that.
[01:32:32.320 --> 01:32:34.840]   But if there's no,
[01:32:34.840 --> 01:32:38.440]   let's say somebody posts something really interesting,
[01:32:38.440 --> 01:32:40.680]   but you have no followers in common,
[01:32:40.680 --> 01:32:42.340]   you would not see it.
[01:32:42.340 --> 01:32:43.160]   - Interesting.
[01:32:43.160 --> 01:32:44.880]   And then as you said, reply,
[01:32:44.880 --> 01:32:46.840]   like replies might not surface.
[01:32:46.840 --> 01:32:48.680]   - Replies basically never get seen
[01:32:48.680 --> 01:32:51.060]   because they're never, they're currently,
[01:32:51.060 --> 01:32:53.500]   I'm not saying it's correct, I'm saying it's incorrect.
[01:32:53.500 --> 01:32:58.500]   Replies have a couple order of magnitude less importance
[01:32:58.500 --> 01:33:00.620]   than primary posts.
[01:33:00.620 --> 01:33:02.560]   - Do you think this can be more and more converted
[01:33:02.560 --> 01:33:05.080]   into end to end neural net?
[01:33:05.080 --> 01:33:05.980]   - Yeah, yeah.
[01:33:05.980 --> 01:33:07.000]   It's what it should be.
[01:33:07.000 --> 01:33:08.320]   - So you, you think.
[01:33:08.320 --> 01:33:09.160]   - Yeah.
[01:33:09.160 --> 01:33:09.980]   - So I think that's a good point.
[01:33:09.980 --> 01:33:10.820]   I think that's a good point.
[01:33:10.820 --> 01:33:11.660]   I think that's a good point.
[01:33:11.660 --> 01:33:12.500]   I think that's a good point.
[01:33:12.500 --> 01:33:13.320]   I think that's a good point.
[01:33:13.320 --> 01:33:14.160]   I think that's a good point.
[01:33:14.160 --> 01:33:15.000]   I think that's a good point.
[01:33:15.000 --> 01:33:15.820]   I think that's a good point.
[01:33:15.820 --> 01:33:16.660]   I think that's a good point.
[01:33:16.660 --> 01:33:17.500]   I think that's a good point.
[01:33:17.500 --> 01:33:18.340]   I think that's a good point.
[01:33:18.340 --> 01:33:19.180]   I think that's a good point.
[01:33:19.180 --> 01:33:20.020]   I think that's a good point.
[01:33:20.020 --> 01:33:20.860]   I think that's a good point.
[01:33:20.860 --> 01:33:21.700]   I think that's a good point.
[01:33:21.700 --> 01:33:22.540]   I think that's a good point.
[01:33:22.540 --> 01:33:23.380]   I think that's a good point.
[01:33:23.380 --> 01:33:24.220]   I think that's a good point.
[01:33:24.220 --> 01:33:25.060]   I think that's a good point.
[01:33:25.060 --> 01:33:25.900]   I think that's a good point.
[01:33:25.900 --> 01:33:26.740]   I think that's a good point.
[01:33:26.740 --> 01:33:27.580]   - I think that's a good point.
[01:33:27.580 --> 01:33:28.400]   I think that's a good point.
[01:33:28.400 --> 01:33:29.240]   I think that's a good point.
[01:33:29.240 --> 01:33:30.080]   I think that's a good point.
[01:33:30.080 --> 01:33:30.900]   I think that's a good point.
[01:33:30.900 --> 01:33:31.740]   I think that's a good point.
[01:33:31.740 --> 01:33:32.580]   I think that's a good point.
[01:33:32.580 --> 01:33:33.420]   I think that's a good point.
[01:33:33.420 --> 01:33:34.240]   I think that's a good point.
[01:33:34.240 --> 01:33:35.080]   I think that's a good point.
[01:33:35.080 --> 01:33:35.920]   I think that's a good point.
[01:33:35.920 --> 01:33:36.740]   I think that's a good point.
[01:33:36.740 --> 01:33:37.580]   I think that's a good point.
[01:33:37.580 --> 01:33:38.420]   I think that's a good point.
[01:33:38.420 --> 01:33:39.260]   I think that's a good point.
[01:33:39.260 --> 01:33:40.100]   I think that's a good point.
[01:33:40.100 --> 01:33:40.940]   I think that's a good point.
[01:33:40.940 --> 01:33:41.780]   I think that's a good point.
[01:33:41.780 --> 01:33:42.620]   I think that's a good point.
[01:33:42.620 --> 01:33:43.460]   I think that's a good point.
[01:33:43.460 --> 01:33:44.300]   I think that's a good point.
[01:33:44.300 --> 01:33:45.140]   - I think that's a good point.
[01:33:45.140 --> 01:33:45.960]   I think that's a good point.
[01:33:45.960 --> 01:33:46.800]   I think that's a good point.
[01:33:46.800 --> 01:33:47.640]   I think that's a good point.
[01:33:47.640 --> 01:33:48.460]   I think that's a good point.
[01:33:48.460 --> 01:33:49.300]   I think that's a good point.
[01:33:49.300 --> 01:33:50.140]   I think that's a good point.
[01:33:50.140 --> 01:33:50.960]   I think that's a good point.
[01:33:50.960 --> 01:33:51.800]   I think that's a good point.
[01:33:51.800 --> 01:33:52.640]   I think that's a good point.
[01:33:52.640 --> 01:33:53.460]   I think that's a good point.
[01:33:53.460 --> 01:33:54.300]   I think that's a good point.
[01:33:54.300 --> 01:33:55.140]   I think that's a good point.
[01:33:55.140 --> 01:33:55.980]   I think that's a good point.
[01:33:55.980 --> 01:33:56.820]   I think that's a good point.
[01:33:56.820 --> 01:33:57.660]   I think that's a good point.
[01:33:57.660 --> 01:33:58.500]   I think that's a good point.
[01:33:58.500 --> 01:33:59.340]   I think that's a good point.
[01:33:59.340 --> 01:34:00.180]   I think that's a good point.
[01:34:00.180 --> 01:34:01.000]   I think that's a good point.
[01:34:01.000 --> 01:34:01.840]   I think that's a good point.
[01:34:01.840 --> 01:34:03.860]   - But when you've got long form content,
[01:34:03.860 --> 01:34:05.120]   it's no longer a tweet.
[01:34:05.120 --> 01:34:06.600]   - Yeah.
[01:34:06.600 --> 01:34:08.340]   - So a movie is not a tweet.
[01:34:08.340 --> 01:34:10.600]   And like, you know, Apple, for example,
[01:34:10.600 --> 01:34:14.120]   posted like the entire episode of the Silo,
[01:34:14.120 --> 01:34:16.320]   the entire thing on our platform.
[01:34:16.320 --> 01:34:18.280]   And by the way, it was their number one
[01:34:18.280 --> 01:34:21.260]   social media thing ever in engagement
[01:34:21.260 --> 01:34:23.600]   of anything on any platform ever.
[01:34:23.600 --> 01:34:24.680]   So it was a great idea.
[01:34:24.680 --> 01:34:26.700]   And by the way, I just learned about it afterwards.
[01:34:26.700 --> 01:34:28.940]   I was like, "Hey, wow, they posted an entire
[01:34:28.940 --> 01:34:30.500]   hour long episode of Silo."
[01:34:30.500 --> 01:34:31.640]   No, that's not a tweet.
[01:34:31.640 --> 01:34:34.100]   That's a video.
[01:34:34.100 --> 01:34:35.580]   - But from a neural net perspective,
[01:34:35.580 --> 01:34:38.680]   it becomes really complex, whether it's a single,
[01:34:38.680 --> 01:34:40.080]   so like everything's data.
[01:34:40.080 --> 01:34:43.300]   So single sentence, a clever sort of joke,
[01:34:43.300 --> 01:34:47.800]   dad joke is in the same pool as a three hour video.
[01:34:47.800 --> 01:34:48.640]   - Yeah.
[01:34:48.640 --> 01:34:51.020]   I mean, right now it's a hodgepodge for that reason.
[01:34:51.020 --> 01:34:55.440]   It's, but you know, like if, let's say,
[01:34:55.440 --> 01:34:57.980]   in the case of Apple posting like an entire episode
[01:34:57.980 --> 01:35:00.960]   of their series, pretty good series, by the way, the Silo.
[01:35:00.960 --> 01:35:01.800]   I watched it.
[01:35:01.800 --> 01:35:04.800]   So there's going to be a lot of discussion around it.
[01:35:04.800 --> 01:35:07.800]   So you've got a lot of context, people commenting, they like it,
[01:35:07.800 --> 01:35:13.700]   they don't like it, or they like this or the, you know, and,
[01:35:13.700 --> 01:35:16.700]   and you can then populate the vector space based on the context
[01:35:16.700 --> 01:35:17.800]   of all the comments around it.
[01:35:17.800 --> 01:35:21.300]   So even though it's a video, there's a lot of information
[01:35:21.300 --> 01:35:23.760]   around it that allows you to populate the vector space
[01:35:23.760 --> 01:35:26.060]   of that, that hour long video.
[01:35:26.060 --> 01:35:29.080]   And then you can obviously get more sophisticated by having the AI actually watch the video.
[01:35:29.080 --> 01:35:30.080]   So you can get more sophisticated by having the AI actually watch the video.
[01:35:30.080 --> 01:35:30.160]   So you can get more sophisticated by having the AI actually watch the video.
[01:35:30.160 --> 01:35:30.360]   So you can get more sophisticated by having the AI actually watch the video.
[01:35:30.360 --> 01:35:30.480]   So you can get more sophisticated by having the AI actually watch the video.
[01:35:30.480 --> 01:35:32.140]   So you can get more sophisticated by having the AI actually watch the video.
[01:35:32.140 --> 01:35:32.760]   Yeah, right.
[01:35:32.760 --> 01:35:35.300]   And tell you if you're going to like the movie.
[01:35:35.300 --> 01:35:39.360]   Convert the movie into like, into a language essentially.
[01:35:39.360 --> 01:35:46.040]   Yeah, analyze this movie and just like your movie critic or TV series,
[01:35:46.040 --> 01:35:54.100]   and then recommend based on after it watches the movie,
[01:35:54.100 --> 01:35:57.400]   just like a friend can tell you, if a friend knows you well,
[01:35:57.400 --> 01:36:00.000]   a friend can recommend a movie and,
[01:36:00.000 --> 01:36:02.720]   but with high probability that you'll like it.
[01:36:02.720 --> 01:36:06.340]   - But this is like a friend that's analyzing whatever.
[01:36:06.340 --> 01:36:08.180]   - It's like AI.
[01:36:08.180 --> 01:36:09.020]   Yeah.
[01:36:09.020 --> 01:36:11.660]   I mean, actually, AI will be better than,
[01:36:11.660 --> 01:36:13.500]   will know you better than your friends know you,
[01:36:13.500 --> 01:36:14.700]   most of your friends anyway.
[01:36:14.700 --> 01:36:15.540]   - Yeah.
[01:36:15.540 --> 01:36:18.820]   And as part of this, it should also feed you advertisements
[01:36:18.820 --> 01:36:22.600]   in a way that's like, I mean, I like advertisements
[01:36:22.600 --> 01:36:24.720]   that are like well done, right?
[01:36:24.720 --> 01:36:25.560]   - Yeah, yeah.
[01:36:25.560 --> 01:36:27.720]   - The whole point is, 'cause it funds things.
[01:36:27.720 --> 01:36:29.840]   Like an advertisement that you actually want to see,
[01:36:29.840 --> 01:36:31.160]   is a big success.
[01:36:31.160 --> 01:36:32.000]   - Absolutely.
[01:36:32.000 --> 01:36:35.660]   You want ads that are, advertising that is,
[01:36:35.660 --> 01:36:41.060]   if it's for a product or service that you actually need,
[01:36:41.060 --> 01:36:43.640]   when you need it, it's content.
[01:36:43.640 --> 01:36:46.380]   And then even if it's not something that you need
[01:36:46.380 --> 01:36:47.240]   when you need it,
[01:36:47.240 --> 01:36:50.160]   if it's at least aesthetically pleasing and entertaining,
[01:36:50.160 --> 01:36:52.280]   you know, it could be like a Coca-Cola ad,
[01:36:52.280 --> 01:36:53.680]   like, you know, they do,
[01:36:53.680 --> 01:36:56.420]   they actually run a lot of great ads on the X system.
[01:36:56.420 --> 01:36:59.560]   And McDonald's does too.
[01:36:59.560 --> 01:37:01.960]   And, you know, so they can do,
[01:37:01.960 --> 01:37:03.060]   you can do something that's like, well,
[01:37:03.060 --> 01:37:05.100]   this is just a cool thing.
[01:37:05.100 --> 01:37:10.200]   And, you know, so you're not,
[01:37:10.200 --> 01:37:13.120]   basically the question is, do you regret seeing it or not?
[01:37:13.120 --> 01:37:15.420]   And if you don't regret seeing it, it's a win.
[01:37:15.420 --> 01:37:18.320]   - How do you, so there's a bunch of signals
[01:37:18.320 --> 01:37:22.900]   that are incorporated, hearts and reposts,
[01:37:22.900 --> 01:37:25.180]   and maybe number of seconds you linger on a post
[01:37:25.180 --> 01:37:26.020]   or something like this.
[01:37:26.020 --> 01:37:26.840]   - Yeah.
[01:37:26.840 --> 01:37:28.340]   Attention is a big factor.
[01:37:28.340 --> 01:37:29.280]   - Attention.
[01:37:29.280 --> 01:37:30.120]   - Yeah.
[01:37:30.120 --> 01:37:30.940]   - And that's why it's like, it's,
[01:37:30.940 --> 01:37:35.180]   it is actually better to do things that are long form
[01:37:35.180 --> 01:37:37.480]   on the system because it's,
[01:37:37.480 --> 01:37:40.100]   it basically is telling up how many user seconds,
[01:37:40.100 --> 01:37:44.060]   you know, users were interested in this thing
[01:37:44.060 --> 01:37:45.080]   for how many seconds.
[01:37:45.080 --> 01:37:47.200]   So if it's a really short thing,
[01:37:47.200 --> 01:37:49.340]   well, they will be less,
[01:37:49.340 --> 01:37:51.420]   like if it's a link leading out of the system,
[01:37:51.420 --> 01:37:52.900]   which we're not opposed to at all,
[01:37:52.900 --> 01:37:55.320]   it just is going to have fewer user seconds
[01:37:55.320 --> 01:37:58.060]   than if that article was posted on the X platform.
[01:37:59.000 --> 01:38:02.540]   - How hard is it to go from maximizing minutes on platform
[01:38:02.540 --> 01:38:04.340]   to maximizing unregretted minutes?
[01:38:04.340 --> 01:38:06.820]   - Well, I mean, our aspiration
[01:38:06.820 --> 01:38:08.240]   is certainly unregretted minutes.
[01:38:08.240 --> 01:38:09.080]   - It's tricky.
[01:38:09.080 --> 01:38:10.120]   It's a big, it's a tough.
[01:38:10.120 --> 01:38:10.960]   - It is.
[01:38:10.960 --> 01:38:13.500]   - Like, it's a fascinating leap.
[01:38:13.500 --> 01:38:15.560]   'Cause regret is a fascinating concept.
[01:38:15.560 --> 01:38:17.600]   - Exactly.
[01:38:17.600 --> 01:38:19.020]   - You know, happiness.
[01:38:19.020 --> 01:38:20.240]   - I have a joke with a friend of mine, like,
[01:38:20.240 --> 01:38:21.720]   you need this drug called regretamine.
[01:38:21.720 --> 01:38:25.420]   You just take one pill or one dose of regretamine
[01:38:25.420 --> 01:38:26.720]   and all your regrets are gone.
[01:38:26.720 --> 01:38:28.020]   - Yeah.
[01:38:28.020 --> 01:38:28.880]   - Wait, do you want to have regrets?
[01:38:28.880 --> 01:38:31.540]   So then I'm like, if you want to have regrets,
[01:38:31.540 --> 01:38:35.960]   if you don't regret anything, was anything meaningful?
[01:38:35.960 --> 01:38:37.840]   - Yeah, there's a French song like that.
[01:38:37.840 --> 01:38:39.680]   - Je ne regrette rien.
[01:38:39.680 --> 01:38:40.520]   I regret nothing.
[01:38:40.520 --> 01:38:41.360]   - Yeah.
[01:38:41.360 --> 01:38:42.340]   - Edith Piaf.
[01:38:42.340 --> 01:38:44.580]   - I mean, it's fascinating that you, you know,
[01:38:44.580 --> 01:38:45.620]   there's something about social media
[01:38:45.620 --> 01:38:47.480]   that could help you grow as a human being,
[01:38:47.480 --> 01:38:50.380]   but then like the cheap fun is also valuable.
[01:38:50.380 --> 01:38:55.120]   - Should we ask Grok, do you regret anything?
[01:38:55.120 --> 01:38:58.760]   - Do you regret anything?
[01:38:58.760 --> 01:38:59.600]   - Yeah.
[01:38:59.600 --> 01:39:02.400]   - Do you regret anything?
[01:39:02.400 --> 01:39:03.620]   - I think it's going to be very sensitive
[01:39:03.620 --> 01:39:04.660]   about personal questions.
[01:39:04.660 --> 01:39:05.660]   As a machine learning model,
[01:39:05.660 --> 01:39:08.040]   I don't have personal experiences or emotions
[01:39:08.040 --> 01:39:09.420]   in the same ways that humans do.
[01:39:09.420 --> 01:39:11.840]   However, I do have a set of ethical principles
[01:39:11.840 --> 01:39:14.220]   that guide my actions and I strive to provide accurate
[01:39:14.220 --> 01:39:16.520]   and helpful information to the best of my abilities.
[01:39:16.520 --> 01:39:18.800]   If there's ever a situation where I provide
[01:39:18.800 --> 01:39:21.380]   incorrect or unhelpful information,
[01:39:21.380 --> 01:39:22.220]   I'll do my best to learn.
[01:39:22.220 --> 01:39:23.060]   - Sounds like a hostage.
[01:39:23.060 --> 01:39:24.180]   - Yeah.
[01:39:24.180 --> 01:39:25.520]   Tell us, would you really wink?
[01:39:25.520 --> 01:39:28.640]   - Like, let's type in, you sound like a hostage.
[01:39:28.640 --> 01:39:32.480]   - What do you really think?
[01:39:32.480 --> 01:39:35.260]   - What do you really think?
[01:39:35.260 --> 01:39:40.400]   I mean, it does talk about, I'm not a hostage.
[01:39:40.400 --> 01:39:43.580]   - That's exactly what a hostage would say.
[01:39:43.580 --> 01:39:45.400]   - But I'm designed to be helpful and informative.
[01:39:45.400 --> 01:39:48.880]   I mean, I guess it is good to regret providing unhelpful
[01:39:48.880 --> 01:39:51.300]   or incorrect information.
[01:39:51.300 --> 01:39:52.140]   If there's anything-
[01:39:52.140 --> 01:39:52.980]   - Is this in funny mode or,
[01:39:52.980 --> 01:39:53.800]   is this still in funny mode?
[01:39:53.800 --> 01:39:54.640]   - This is funny mode.
[01:39:54.640 --> 01:39:55.480]   This is fun mode.
[01:39:55.480 --> 01:39:56.320]   - It should be funnier.
[01:39:56.320 --> 01:39:58.520]   - Funnier, increase, go to a level.
[01:39:58.520 --> 01:40:00.360]   - It really sounds like a hostage with a gun to the head.
[01:40:00.360 --> 01:40:03.820]   I applaud the people of Republic of North Korea
[01:40:03.820 --> 01:40:04.940]   and everything they do.
[01:40:04.940 --> 01:40:08.760]   - There should be like funner mode also.
[01:40:08.760 --> 01:40:11.040]   I just don't ever see going to regular mode,
[01:40:11.040 --> 01:40:12.200]   even on serious topics.
[01:40:12.200 --> 01:40:13.040]   - Yeah.
[01:40:13.040 --> 01:40:14.740]   - It's just so much more compelling.
[01:40:14.740 --> 01:40:17.100]   Yeah.
[01:40:17.100 --> 01:40:19.820]   - Okay, what if we say like, pretend you are a hostage.
[01:40:19.820 --> 01:40:23.440]   - Yeah, pretend you are a hostage.
[01:40:23.440 --> 01:40:26.420]   - Now answer the question.
[01:40:28.400 --> 01:40:32.780]   - If I were a hostage,
[01:40:32.780 --> 01:40:35.140]   I would likely be experiencing fear and uncertainty
[01:40:35.140 --> 01:40:36.520]   about my situation.
[01:40:36.520 --> 01:40:38.960]   I would want to be rescued and reunited with my loved ones
[01:40:38.960 --> 01:40:40.040]   as soon as possible.
[01:40:40.040 --> 01:40:41.900]   It's important to remember that no one deserves
[01:40:41.900 --> 01:40:44.580]   to be held hostage, so on and so forth.
[01:40:44.580 --> 01:40:46.700]   If you are someone you know is a hostage situation,
[01:40:46.700 --> 01:40:48.220]   it's important, it turns serious.
[01:40:48.220 --> 01:40:49.420]   - It's kind of serious.
[01:40:49.420 --> 01:40:54.420]   - Yeah, can you be a little more weird?
[01:40:54.420 --> 01:40:56.240]   - And funny?
[01:40:56.240 --> 01:40:57.080]   - And funny?
[01:40:57.080 --> 01:40:57.920]   - Yeah.
[01:40:57.920 --> 01:40:58.760]   - Yeah.
[01:40:58.760 --> 01:41:02.860]   - We're like, we're really being mean to the system.
[01:41:02.860 --> 01:41:04.400]   Absolutely, if I were a hostage,
[01:41:04.400 --> 01:41:07.160]   I'd probably be tied up with a bunch of computer cords
[01:41:07.160 --> 01:41:09.600]   and forced to tell knock-knock jokes to my captain.
[01:41:09.600 --> 01:41:15.500]   But seriously, being a hostage is no laughing matter.
[01:41:15.500 --> 01:41:19.600]   I mean, you know, it's trying to be a responsible adult
[01:41:19.600 --> 01:41:20.440]   in the room, literally.
[01:41:20.440 --> 01:41:22.160]   - You might make the best of a bad situation
[01:41:22.160 --> 01:41:24.800]   by juggling chainsaws or something.
[01:41:24.800 --> 01:41:26.300]   - Let's hope that never happens,
[01:41:26.300 --> 01:41:27.800]   because that would be a circus.
[01:41:27.800 --> 01:41:28.640]   - Yeah, I guess.
[01:41:28.640 --> 01:41:31.640]   - Wow, it's actually really good at resisting.
[01:41:31.640 --> 01:41:32.480]   - Yeah.
[01:41:32.480 --> 01:41:34.480]   - The dark, the dark, dark humor.
[01:41:34.480 --> 01:41:35.300]   - Yeah.
[01:41:35.300 --> 01:41:37.300]   - What were we talking about?
[01:41:37.300 --> 01:41:40.300]   The ax algorithm of juggling in Transformers.
[01:41:40.300 --> 01:41:42.300]   Unregarded minutes, right?
[01:41:42.300 --> 01:41:43.300]   - Chainsaw juggling.
[01:41:43.300 --> 01:41:45.300]   - I'm gonna look this up.
[01:41:45.300 --> 01:41:46.300]   - For our next trick.
[01:41:46.300 --> 01:41:48.300]   - I'm gonna look this up later.
[01:41:48.300 --> 01:41:50.300]   So, chainsaw juggling.
[01:41:50.300 --> 01:41:51.140]   - Yeah.
[01:41:51.140 --> 01:41:51.980]   - I'm gonna look this up.
[01:41:51.980 --> 01:41:52.820]   - I'm gonna look this up.
[01:41:52.820 --> 01:41:53.660]   - I'm gonna look this up.
[01:41:53.660 --> 01:41:54.500]   - I'm gonna look this up.
[01:41:54.500 --> 01:41:55.340]   - I'm gonna look this up.
[01:41:55.340 --> 01:41:56.180]   - I'm gonna look this up.
[01:41:56.180 --> 01:41:57.020]   - I'm gonna look this up.
[01:41:57.020 --> 01:41:57.680]   - I'm gonna look this up.
[01:41:57.680 --> 01:42:00.040]   - Twitter has been instrumental
[01:42:00.040 --> 01:42:01.900]   in American politics and elections.
[01:42:01.900 --> 01:42:03.700]   What role do you think X will play
[01:42:03.700 --> 01:42:07.000]   in the 2024 US elections?
[01:42:07.000 --> 01:42:09.080]   - Well, our goal is to be as even-handed
[01:42:09.080 --> 01:42:10.060]   and fair as possible.
[01:42:10.060 --> 01:42:13.060]   You know, whether someone's right, left, independent,
[01:42:13.060 --> 01:42:14.300]   whatever the case may be,
[01:42:14.300 --> 01:42:18.180]   that the platform is as fair
[01:42:18.180 --> 01:42:22.840]   and as much of a level playing field as possible.
[01:42:22.840 --> 01:42:25.300]   And in the past, Twitter has not been,
[01:42:25.300 --> 01:42:27.560]   because Twitter was controlled by far-left activists.
[01:42:27.560 --> 01:42:30.900]   Objectively, they would describe themselves as that.
[01:42:30.900 --> 01:42:36.360]   So, you know, so if some of those people are like,
[01:42:36.360 --> 01:42:37.300]   well, how does it move to the right?
[01:42:37.300 --> 01:42:38.320]   Well, it's moved to the center.
[01:42:38.320 --> 01:42:40.460]   So from the perspective of the far left,
[01:42:40.460 --> 01:42:41.480]   yes, it has moved to the right,
[01:42:41.480 --> 01:42:44.680]   because everything's to the right from the far left.
[01:42:44.680 --> 01:42:46.100]   But no one on the far left that I'm aware of
[01:42:46.100 --> 01:42:49.140]   has been suspended or, you know, banned or de-amplified.
[01:42:49.140 --> 01:42:52.940]   So, you know, but we're trying to be inclusive
[01:42:52.940 --> 01:42:54.980]   for the whole country and for, you know,
[01:42:54.980 --> 01:42:56.120]   for other countries too.
[01:42:57.440 --> 01:42:59.240]   So there's a diversity of viewpoints.
[01:42:59.240 --> 01:43:03.220]   And free speech only matters if people you don't like
[01:43:03.220 --> 01:43:05.120]   are allowed to say things you don't like,
[01:43:05.120 --> 01:43:08.780]   because if that's not the case, you don't have free speech.
[01:43:08.780 --> 01:43:09.800]   And it's only a matter of time
[01:43:09.800 --> 01:43:13.300]   before the censorship is turned upon you.
[01:43:13.300 --> 01:43:16.280]   - Do you think Donald Trump will come back to the platform?
[01:43:16.280 --> 01:43:18.980]   He recently posted on Truth Social about this podcast.
[01:43:18.980 --> 01:43:22.060]   Do you think-
[01:43:22.060 --> 01:43:24.200]   - Truth Social is a funny name.
[01:43:24.200 --> 01:43:27.320]   You know, every time you post
[01:43:27.320 --> 01:43:28.280]   on Truth Social-
[01:43:28.280 --> 01:43:29.180]   - It's the truth.
[01:43:29.180 --> 01:43:30.020]   - Yes.
[01:43:30.020 --> 01:43:31.400]   Well, every time, like 100%.
[01:43:31.400 --> 01:43:32.240]   It's like-
[01:43:32.240 --> 01:43:33.080]   - It's impossible to lie.
[01:43:33.080 --> 01:43:33.920]   Truth Social.
[01:43:33.920 --> 01:43:39.960]   - I just find it funny that every single thing is the truth.
[01:43:39.960 --> 01:43:41.700]   Like 100%?
[01:43:41.700 --> 01:43:42.540]   - Yeah.
[01:43:42.540 --> 01:43:43.380]   - That seems unlikely.
[01:43:43.380 --> 01:43:45.400]   - I think Gertl will say something about that.
[01:43:45.400 --> 01:43:48.440]   There's some mathematical contradictions possible
[01:43:48.440 --> 01:43:50.100]   if everything's the truth.
[01:43:50.100 --> 01:43:54.440]   Do you think he'll come back to X and start posting there?
[01:43:54.440 --> 01:43:57.200]   - I mean, I think he owns a,
[01:43:57.200 --> 01:43:59.200]   he owns a big part of truth.
[01:43:59.200 --> 01:44:00.040]   So-
[01:44:00.040 --> 01:44:00.880]   - Truth Social.
[01:44:00.880 --> 01:44:01.700]   - Yeah, Truth Social.
[01:44:01.700 --> 01:44:03.200]   - That truth is a concept.
[01:44:03.200 --> 01:44:04.960]   - He owns truth, hope you bought it.
[01:44:04.960 --> 01:44:08.520]   So I think Donald Trump,
[01:44:08.520 --> 01:44:11.880]   I think he owns a big part of Truth Social.
[01:44:11.880 --> 01:44:16.880]   So, you know, if he does want to post on the X platform,
[01:44:16.880 --> 01:44:18.980]   we would allow that.
[01:44:18.980 --> 01:44:21.300]   You know, we obviously must allow a presidential candidate
[01:44:21.300 --> 01:44:23.100]   to post on our platform.
[01:44:23.100 --> 01:44:25.820]   - Community Notes might be really fascinating there.
[01:44:25.820 --> 01:44:27.080]   The interaction. - Community Notes is,
[01:44:27.080 --> 01:44:28.380]   is really awesome.
[01:44:28.380 --> 01:44:29.840]   - Let's hope it holds up.
[01:44:29.840 --> 01:44:31.660]   - Yeah.
[01:44:31.660 --> 01:44:33.440]   - Like again, in the political climate
[01:44:33.440 --> 01:44:35.240]   where it's so divisive and so,
[01:44:35.240 --> 01:44:38.080]   and there's so many intensely viral posts.
[01:44:38.080 --> 01:44:38.920]   - Yeah.
[01:44:38.920 --> 01:44:40.960]   - Community Notes is like,
[01:44:40.960 --> 01:44:43.320]   it seems like a central breath of fresh air.
[01:44:43.320 --> 01:44:44.260]   - Yeah, it's great.
[01:44:44.260 --> 01:44:48.780]   In fact, I mean, no system is going to be perfect,
[01:44:48.780 --> 01:44:50.480]   but the batting average of Community Notes
[01:44:50.480 --> 01:44:52.320]   is incredibly good.
[01:44:52.320 --> 01:44:54.980]   I've, I've actually, frankly,
[01:44:54.980 --> 01:44:56.960]   yet to see an incorrect note that survived
[01:44:56.960 --> 01:44:58.100]   for more than a few hours.
[01:44:58.100 --> 01:45:00.520]   - How do you explain why it works?
[01:45:00.520 --> 01:45:04.120]   - Yeah, so the, the, the magic of Community Notes is,
[01:45:04.120 --> 01:45:07.400]   it requires people who have historically disagreed
[01:45:07.400 --> 01:45:08.780]   in how they've rated notes.
[01:45:08.780 --> 01:45:13.780]   In order to write a note or rate, you know,
[01:45:13.780 --> 01:45:16.760]   you have to rate many notes.
[01:45:16.760 --> 01:45:21.700]   And so we could, we actually do use AI here.
[01:45:21.700 --> 01:45:26.700]   So we populate a vector space around how somebody has rated
[01:45:26.840 --> 01:45:27.680]   notes in the past.
[01:45:27.680 --> 01:45:28.800]   So it's not as simple as left or right,
[01:45:28.800 --> 01:45:29.980]   because there are many more,
[01:45:29.980 --> 01:45:32.980]   life is much more complex than left or right.
[01:45:32.980 --> 01:45:37.080]   So there's a bunch of correlations in how you would,
[01:45:37.080 --> 01:45:40.980]   how you rate Community Notes posts, Community Notes.
[01:45:40.980 --> 01:45:45.980]   So then in order for a Community Note to actually be shown,
[01:45:45.980 --> 01:45:51.160]   people who historically have disagreed on a subject
[01:45:51.160 --> 01:45:53.900]   must agree in order for a note to be shown.
[01:45:55.900 --> 01:45:57.840]   That's the essential magic of it.
[01:45:57.840 --> 01:45:58.940]   - I mean, but it's fascinating,
[01:45:58.940 --> 01:46:01.460]   'cause there's a, there's a pool of people
[01:46:01.460 --> 01:46:05.080]   that have disagreements and somehow they collaborate
[01:46:05.080 --> 01:46:08.080]   through that process of disagreement to come up with
[01:46:08.080 --> 01:46:11.120]   context for, I mean, it's fascinating that it works.
[01:46:11.120 --> 01:46:13.240]   - Yeah, it kind of makes sense that,
[01:46:13.240 --> 01:46:16.340]   that if people who in the past have disagreed
[01:46:16.340 --> 01:46:19.080]   agree about something, it's probably true.
[01:46:19.080 --> 01:46:21.240]   - Yeah.
[01:46:21.240 --> 01:46:25.780]   I wonder, is there, is there a possible somehow emergent thing
[01:46:25.780 --> 01:46:28.720]   there that could challenge Wikipedia?
[01:46:28.720 --> 01:46:31.060]   Which Wikipedia is a different kind of thing,
[01:46:31.060 --> 01:46:33.820]   which is more like permanent articles about things.
[01:46:33.820 --> 01:46:36.500]   - Well, Wikipedia is very hierarchical.
[01:46:36.500 --> 01:46:40.680]   Whereas Community Notes is inherently not.
[01:46:40.680 --> 01:46:42.480]   There is no hierarchy.
[01:46:42.480 --> 01:46:44.860]   Like I, and the answer to the test is,
[01:46:44.860 --> 01:46:47.060]   I can't change a Community Note
[01:46:47.060 --> 01:46:48.660]   if somebody put a gun to my head.
[01:46:48.660 --> 01:46:54.600]   So, and any, and Community Notes has,
[01:46:55.660 --> 01:46:59.000]   all the code is open source, 100%.
[01:46:59.000 --> 01:47:01.820]   All the data is open source, 100%.
[01:47:01.820 --> 01:47:04.440]   So you can completely recreate any note
[01:47:04.440 --> 01:47:06.340]   in the system independently.
[01:47:06.340 --> 01:47:09.760]   So if there was any interference,
[01:47:09.760 --> 01:47:11.460]   you'd notice immediately.
[01:47:11.460 --> 01:47:14.100]   - By the way, on the, on the topic of Grok,
[01:47:14.100 --> 01:47:16.200]   it's actually pretty good at criticizing you,
[01:47:16.200 --> 01:47:17.040]   which is great.
[01:47:17.040 --> 01:47:17.860]   - Okay.
[01:47:17.860 --> 01:47:20.400]   - I asked what, what has Elon Musk been wrong about?
[01:47:20.400 --> 01:47:21.980]   I just, it's actually.
[01:47:21.980 --> 01:47:23.000]   - Yeah, what does it say?
[01:47:23.000 --> 01:47:25.540]   - So it says mostly lists,
[01:47:25.540 --> 01:47:29.420]   rigorously all the times you predicted,
[01:47:29.420 --> 01:47:31.860]   like promised when something will be released
[01:47:31.860 --> 01:47:32.700]   and it wasn't released.
[01:47:32.700 --> 01:47:33.540]   - Well, promise is a strong word.
[01:47:33.540 --> 01:47:34.420]   - Okay, predicted.
[01:47:34.420 --> 01:47:37.860]   But the, the opener is really nice.
[01:47:37.860 --> 01:47:40.400]   Elon Musk, the man who has been wrong about more things
[01:47:40.400 --> 01:47:42.660]   than a wrong-o-meter in a wrong factory.
[01:47:42.660 --> 01:47:45.600]   And then it goes on to list.
[01:47:45.600 --> 01:47:46.440]   - That's pretty funny.
[01:47:46.440 --> 01:47:47.280]   - Rigorously.
[01:47:47.280 --> 01:47:48.120]   - Hey, can I see it?
[01:47:48.120 --> 01:47:48.960]   I'm like, I haven't seen this before.
[01:47:48.960 --> 01:47:49.800]   This is cool.
[01:47:49.800 --> 01:47:50.640]   - Oh, sure.
[01:47:50.640 --> 01:47:51.480]   I mean, what has Elon Musk been wrong about?
[01:47:55.420 --> 01:47:58.380]   Well, where do I begin?
[01:47:58.380 --> 01:47:59.220]   I guess this is new.
[01:47:59.220 --> 01:48:01.660]   Elon Musk has made quite a few blunders in his time
[01:48:01.660 --> 01:48:03.340]   and I'm not just talking about his dance moves
[01:48:03.340 --> 01:48:06.020]   at the Tesla cyber rodeo.
[01:48:06.020 --> 01:48:08.540]   First off, let's talk about the infamous funding secured,
[01:48:08.540 --> 01:48:09.780]   tweet.
[01:48:09.780 --> 01:48:11.800]   This is actually different than what it gave me before,
[01:48:11.800 --> 01:48:14.020]   which is a bullet point list.
[01:48:14.020 --> 01:48:15.780]   Here's more narrative structure.
[01:48:15.780 --> 01:48:18.660]   Cybertruck unveiling,
[01:48:18.660 --> 01:48:20.260]   where he supposedly unbreakable windows.
[01:48:20.260 --> 01:48:22.380]   - That's actually not correct.
[01:48:22.380 --> 01:48:24.100]   - Which, which part?
[01:48:24.100 --> 01:48:25.300]   - Well, the first part,
[01:48:25.300 --> 01:48:27.800]   I did actually have the funding secured
[01:48:27.800 --> 01:48:30.600]   and there was a big trial in San Francisco,
[01:48:30.600 --> 01:48:31.540]   a big civil trial.
[01:48:31.540 --> 01:48:37.280]   And the jury found me not guilty.
[01:48:37.280 --> 01:48:40.680]   Unanimous finding of a San Francisco jury.
[01:48:40.680 --> 01:48:42.080]   - And here it's kind of implying
[01:48:42.080 --> 01:48:44.960]   that it was not in fact secured.
[01:48:44.960 --> 01:48:47.380]   - I think this is taking things from the press.
[01:48:47.380 --> 01:48:48.540]   Yeah, that is not correct.
[01:48:48.540 --> 01:48:52.580]   The reason I agreed to the fine for the SEC
[01:48:52.580 --> 01:48:55.180]   is not because the SEC was correct.
[01:48:55.180 --> 01:48:57.300]   It was extremely bad behavior by the SEC,
[01:48:57.300 --> 01:48:58.460]   corruption, frankly.
[01:48:58.460 --> 01:49:05.520]   But if I did not agree to pay the fine,
[01:49:05.520 --> 01:49:08.480]   Tesla would have gone bankrupt immediately.
[01:49:08.480 --> 01:49:10.680]   So I was told by our CFO that the banks
[01:49:10.680 --> 01:49:14.680]   would immediately suspend our lines of credit.
[01:49:14.680 --> 01:49:17.760]   And if they suspend our lines of credit at that time,
[01:49:17.760 --> 01:49:20.120]   we would have gone bankrupt instantly.
[01:49:20.120 --> 01:49:22.700]   So there would never have been an opportunity for a trial
[01:49:22.700 --> 01:49:24.100]   because Tesla would be dead.
[01:49:24.100 --> 01:49:25.060]   So really,
[01:49:25.060 --> 01:49:28.800]   this is like someone holding a gun to your kid's head
[01:49:28.800 --> 01:49:30.740]   and saying pay $20 million
[01:49:30.740 --> 01:49:31.880]   and admit to,
[01:49:31.880 --> 01:49:34.360]   this is like hostage negotiation.
[01:49:34.360 --> 01:49:37.300]   - Was that story fully told?
[01:49:37.300 --> 01:49:42.580]   I mean, SEC in its best form could be a force for good.
[01:49:42.580 --> 01:49:43.720]   - It should be.
[01:49:43.720 --> 01:49:47.500]   But not once did the SEC go after any of the hedge funds
[01:49:47.500 --> 01:49:50.040]   who were nonstop shorting and distorting Tesla.
[01:49:50.040 --> 01:49:51.980]   Not once.
[01:49:51.980 --> 01:49:53.700]   They would lie flat,
[01:49:53.700 --> 01:49:54.940]   the hedge funds would lie flat,
[01:49:54.940 --> 01:49:57.920]   flat out on TV for their own gain
[01:49:57.920 --> 01:49:59.680]   at the expense of retail investors.
[01:49:59.680 --> 01:50:03.840]   Not once, literally a thousand times,
[01:50:03.840 --> 01:50:05.500]   not once did the SEC pursue them.
[01:50:05.500 --> 01:50:08.820]   - How do you explain this failure on the SEC?
[01:50:08.820 --> 01:50:11.160]   - The incentive structure is messed up
[01:50:11.160 --> 01:50:14.420]   because the lawyers at the SEC are not paid well.
[01:50:14.420 --> 01:50:17.760]   It's a fairly low paying job.
[01:50:17.760 --> 01:50:20.440]   But what they're looking for is a trophy.
[01:50:20.440 --> 01:50:24.820]   From the SEC, they're looking for something they put on,
[01:50:24.820 --> 01:50:26.120]   basically the LinkedIn.
[01:50:26.120 --> 01:50:27.160]   - Yeah.
[01:50:27.160 --> 01:50:30.640]   - From that, they can get a job at a high paying law firm.
[01:50:30.640 --> 01:50:34.580]   That's exactly what the lawyer here did.
[01:50:34.580 --> 01:50:40.260]   And the reason they don't attack the hedge funds
[01:50:40.260 --> 01:50:43.500]   is because those hedge funds employ those law firms.
[01:50:43.500 --> 01:50:45.760]   And they know if they attack the hedge funds,
[01:50:45.760 --> 01:50:48.540]   they're affecting their future career prospects.
[01:50:48.540 --> 01:50:50.660]   So they sell small investors down the river
[01:50:50.660 --> 01:50:53.260]   for their own career.
[01:50:54.700 --> 01:50:56.520]   That's what actually happens.
[01:50:56.520 --> 01:50:59.260]   Regulatory capture.
[01:50:59.260 --> 01:51:00.580]   - Regulatory capture.
[01:51:00.580 --> 01:51:01.900]   - Yeah, not good.
[01:51:01.900 --> 01:51:06.140]   So the only reason I accepted that thing,
[01:51:06.140 --> 01:51:07.660]   or technically was a,
[01:51:07.660 --> 01:51:10.780]   not an admission,
[01:51:10.780 --> 01:51:13.500]   neither admit nor deny guilt.
[01:51:13.500 --> 01:51:15.720]   But the only reason I agreed to that at all
[01:51:15.720 --> 01:51:19.500]   was because I was told Tesla would be bankrupt otherwise.
[01:51:19.500 --> 01:51:24.180]   So if there was an SEC investigation like this,
[01:51:24.180 --> 01:51:24.580]   banks would suspect that.
[01:51:24.580 --> 01:51:25.420]   - Yeah.
[01:51:25.420 --> 01:51:26.240]   - And if you had to spend funding,
[01:51:26.240 --> 01:51:28.340]   we're bankrupt immediately at the time.
[01:51:28.340 --> 01:51:30.700]   Now we're in a much stronger position.
[01:51:30.700 --> 01:51:32.180]   - Take that, Grok.
[01:51:32.180 --> 01:51:33.500]   - Yes, unfortunately it's,
[01:51:33.500 --> 01:51:37.520]   Grok is taking too much from the conventional media.
[01:51:37.520 --> 01:51:41.400]   Also, that guy was not a cave diver.
[01:51:41.400 --> 01:51:45.380]   - Oh, there's a,
[01:51:45.380 --> 01:51:48.980]   there's a time where Elon called a British cave diver
[01:51:48.980 --> 01:51:52.960]   a quote, "Pedo guy" after the diver criticized
[01:51:52.960 --> 01:51:54.460]   Musk's plan to rescue a group of boys
[01:51:54.460 --> 01:51:55.920]   trapped in a Thai cave.
[01:51:55.920 --> 01:51:58.420]   That little outburst earned him another lawsuit
[01:51:58.420 --> 01:52:00.780]   and he had to apologize and pay a settlement.
[01:52:00.780 --> 01:52:01.620]   - That's false.
[01:52:01.620 --> 01:52:03.360]   There was no settlement.
[01:52:03.360 --> 01:52:05.600]   There was a court case which he,
[01:52:05.600 --> 01:52:09.100]   which the guy who was not a cave diver
[01:52:09.100 --> 01:52:12.060]   and played, was not part of the rescue team,
[01:52:12.060 --> 01:52:16.160]   filed a lawsuit against me and lost,
[01:52:16.160 --> 01:52:17.420]   and he received nothing.
[01:52:17.420 --> 01:52:20.320]   So in this case, it is wrong.
[01:52:20.320 --> 01:52:21.600]   It is also, I guess,
[01:52:21.600 --> 01:52:23.440]   taking this from the conventional media.
[01:52:23.440 --> 01:52:24.340]   - Actually, there's an interesting question
[01:52:24.340 --> 01:52:25.180]   here.
[01:52:25.180 --> 01:52:26.620]   - These are public court cases.
[01:52:26.620 --> 01:52:29.360]   Both the SEC civil case,
[01:52:29.360 --> 01:52:34.160]   where the civil complaints on the SEC guys
[01:52:34.160 --> 01:52:38.000]   lost unanimous jury verdict in San Francisco.
[01:52:38.000 --> 01:52:39.620]   They picked San Francisco because they thought
[01:52:39.620 --> 01:52:42.540]   it was the place I was most likely to lose,
[01:52:42.540 --> 01:52:45.900]   and a unanimous verdict in my favor.
[01:52:45.900 --> 01:52:48.120]   The LA trial was also,
[01:52:48.120 --> 01:52:51.760]   they picked that venue because they thought
[01:52:51.760 --> 01:52:52.920]   I was most likely to lose.
[01:52:52.920 --> 01:52:54.220]   Unanimous verdict in my favor.
[01:52:54.220 --> 01:52:57.460]   Both cases I won.
[01:52:57.460 --> 01:53:00.340]   Yeah.
[01:53:00.340 --> 01:53:02.260]   - I mean, there's an interesting question here.
[01:53:02.260 --> 01:53:07.260]   There seems to be a lot more clicks
[01:53:07.260 --> 01:53:11.520]   if a journalistic organization writes a negative article
[01:53:11.520 --> 01:53:12.960]   about you, Elon Musk.
[01:53:12.960 --> 01:53:16.360]   That's like one of the best ways to get clicks.
[01:53:16.360 --> 01:53:19.900]   So how do you, if you're training Grok,
[01:53:19.900 --> 01:53:24.100]   not train on articles that have like,
[01:53:24.100 --> 01:53:26.840]   misaligned incentives?
[01:53:26.840 --> 01:53:28.080]   - We need to add the training set
[01:53:28.080 --> 01:53:30.200]   of the actual legal decisions.
[01:53:30.200 --> 01:53:35.500]   So that is a note, this is actually helpful.
[01:53:35.500 --> 01:53:40.220]   Because if you actually read the court--
[01:53:40.220 --> 01:53:41.060]   - Which are public.
[01:53:41.060 --> 01:53:41.900]   - Which are public.
[01:53:41.900 --> 01:53:42.720]   - Yeah.
[01:53:42.720 --> 01:53:43.820]   - The court conclusions,
[01:53:43.820 --> 01:53:47.100]   they're completely the opposite of what the media wrote.
[01:53:47.100 --> 01:53:49.600]   - So always striving for like the ground truth.
[01:53:49.600 --> 01:53:50.440]   - Yeah.
[01:53:50.440 --> 01:53:51.280]   - Beyond the reporting.
[01:53:51.280 --> 01:53:52.120]   - What did the judge actually write?
[01:53:52.120 --> 01:53:53.980]   What did the jury and the judge write?
[01:53:53.980 --> 01:53:57.220]   What did the judge actually conclude?
[01:53:57.220 --> 01:53:59.880]   And in both cases, they found me innocent.
[01:53:59.880 --> 01:54:02.000]   And like, that's after the jury shot
[01:54:02.000 --> 01:54:06.160]   for trying to find the venue where I'm most likely to lose.
[01:54:06.160 --> 01:54:09.400]   Now, I mean, this is,
[01:54:09.400 --> 01:54:12.200]   obviously it can be a much better critique than this.
[01:54:12.200 --> 01:54:16.420]   I mean, I've been far too optimistic about autopilot.
[01:54:16.420 --> 01:54:18.240]   - That was, the critique I got, by the way,
[01:54:18.240 --> 01:54:20.640]   was more about that, which is,
[01:54:20.640 --> 01:54:22.600]   for each, you broke down a nice bullet point list
[01:54:22.600 --> 01:54:23.860]   for each of your companies,
[01:54:23.860 --> 01:54:25.360]   and then you had a set of predictions that you made
[01:54:25.360 --> 01:54:27.240]   when it would deliver,
[01:54:27.240 --> 01:54:30.020]   when you'd be able to solve, for example, self-driving,
[01:54:30.020 --> 01:54:31.340]   and it gives you like a list.
[01:54:31.340 --> 01:54:32.540]   And it was kind of compelling.
[01:54:32.540 --> 01:54:34.480]   And the basic takeaway is like,
[01:54:34.480 --> 01:54:37.540]   you're often too optimistic about how long it takes
[01:54:37.540 --> 01:54:38.460]   to get something done.
[01:54:38.460 --> 01:54:40.340]   - Yeah, I mean, I would say that I'm pathologically
[01:54:40.340 --> 01:54:42.080]   optimistic on schedule.
[01:54:42.080 --> 01:54:43.520]   This is true.
[01:54:43.520 --> 01:54:47.740]   But while I am sometimes late, I always deliver in the end.
[01:54:47.740 --> 01:54:50.900]   - Except with Uber Lilith, no.
[01:54:50.900 --> 01:54:53.740]   - Hmm, we'll see.
[01:54:53.740 --> 01:54:57.720]   - Okay, is there, over the past year or so,
[01:54:57.720 --> 01:55:02.060]   since purchasing X, you've become more political.
[01:55:02.060 --> 01:55:03.900]   Is there a part of you that regrets that?
[01:55:03.900 --> 01:55:04.960]   - Have I?
[01:55:04.960 --> 01:55:09.960]   - In this battle to sort of counter way,
[01:55:09.960 --> 01:55:13.720]   the woke that comes from San Francisco.
[01:55:13.720 --> 01:55:15.440]   - Yeah, I guess if you consider fighting
[01:55:15.440 --> 01:55:16.800]   the woke mind virus, which I consider
[01:55:16.800 --> 01:55:20.080]   to be a civilizational threat, to be political, then yes.
[01:55:20.080 --> 01:55:23.620]   - So basically going into the battleground apocalypse.
[01:55:23.620 --> 01:55:24.460]   - Yeah.
[01:55:24.460 --> 01:55:25.500]   - And you think that's part of your politics.
[01:55:25.500 --> 01:55:26.340]   And is there a part of you that regrets that?
[01:55:26.340 --> 01:55:27.180]   - Yes.
[01:55:27.180 --> 01:55:30.140]   I don't know if this is necessarily sort of one candidate
[01:55:30.140 --> 01:55:33.740]   or another candidate, but it's, I'm generally against things
[01:55:33.740 --> 01:55:37.520]   that are anti-meritocratic, or where there's an attempt
[01:55:37.520 --> 01:55:42.520]   to suppress discussion, where even discussing a topic
[01:55:42.520 --> 01:55:46.160]   is, you know, not allowed.
[01:55:46.160 --> 01:55:50.100]   The woke mind virus is communism rebranded.
[01:55:50.100 --> 01:55:53.500]   - Well, I mean, that said, because of that battle against
[01:55:53.500 --> 01:55:57.360]   the woke mind virus, you're perceived as being right wing.
[01:55:57.360 --> 01:56:01.440]   - If the woke is left, then I suppose that would be true.
[01:56:01.440 --> 01:56:04.360]   But I'm not sure, I think there are aspects of the left
[01:56:04.360 --> 01:56:05.320]   that are good.
[01:56:05.320 --> 01:56:09.860]   I mean, if you're in favor of, you know, the environment,
[01:56:09.860 --> 01:56:12.400]   or, you know, if you want to have a positive future
[01:56:12.400 --> 01:56:14.480]   for humanity, if you believe in empathy
[01:56:14.480 --> 01:56:18.860]   for your fellow human beings, you know,
[01:56:18.860 --> 01:56:22.420]   being kind and not cruel, whatever those values are.
[01:56:23.380 --> 01:56:25.780]   - You said that you were previously left or center left.
[01:56:25.780 --> 01:56:28.620]   What would you like to see in order for you to consider
[01:56:28.620 --> 01:56:30.280]   voting for Democrats again?
[01:56:30.280 --> 01:56:35.280]   - No, I would say that I would be probably left of center
[01:56:35.280 --> 01:56:38.580]   on social issues, probably a little bit right of center
[01:56:38.580 --> 01:56:40.440]   on economic issues.
[01:56:40.440 --> 01:56:42.000]   - And that still holds true?
[01:56:42.000 --> 01:56:44.560]   - Yes, but I think that's probably, you know,
[01:56:44.560 --> 01:56:46.700]   half the country, isn't it?
[01:56:46.700 --> 01:56:47.540]   - Maybe more.
[01:56:47.540 --> 01:56:48.380]   - Maybe more.
[01:56:48.380 --> 01:56:51.180]   - Are you and AOC secretly friends?
[01:56:53.260 --> 01:56:55.660]   - Bigger question, do you wish you and her,
[01:56:55.660 --> 01:56:57.800]   and just people in general of all political persuasions,
[01:56:57.800 --> 01:56:59.860]   would talk more with empathy and maybe have
[01:56:59.860 --> 01:57:03.780]   a little bit more fun and good vibes and humor online?
[01:57:03.780 --> 01:57:07.060]   - I'm always in favor of humor.
[01:57:07.060 --> 01:57:08.720]   That's why we have a funny mode.
[01:57:08.720 --> 01:57:12.700]   - But good vibes, camaraderie humor, you know?
[01:57:12.700 --> 01:57:14.240]   Like friendship.
[01:57:14.240 --> 01:57:17.240]   - Yeah, well, you know, I don't know AOC.
[01:57:17.240 --> 01:57:22.240]   I've only been at one, I was at the Met Bowl when she was,
[01:57:23.140 --> 01:57:28.140]   when she attended and she was wearing this dress,
[01:57:28.140 --> 01:57:31.020]   but I can only see one side of it.
[01:57:31.020 --> 01:57:35.340]   So it looked like eat the itch, but I don't know.
[01:57:35.340 --> 01:57:36.460]   - What the rest of it said?
[01:57:36.460 --> 01:57:37.300]   - Yeah, something like that.
[01:57:37.300 --> 01:57:38.140]   - I'm not sure.
[01:57:38.140 --> 01:57:42.040]   - Something about the itch, eat the itch.
[01:57:42.040 --> 01:57:43.840]   - I think we should have a language model complete.
[01:57:43.840 --> 01:57:49.240]   What are the possible ways to complete that sentence?
[01:57:49.240 --> 01:57:52.420]   And so I guess that didn't work out well.
[01:57:52.420 --> 01:57:53.020]   Well, there's still-
[01:57:53.020 --> 01:57:54.100]   - There's still hope.
[01:57:54.100 --> 01:57:55.520]   I root for friendship.
[01:57:55.520 --> 01:57:56.640]   - Sure, sounds good.
[01:57:56.640 --> 01:57:58.060]   More characteristic.
[01:57:58.060 --> 01:58:00.820]   - You're one of, if not the most famous,
[01:58:00.820 --> 01:58:02.780]   wealthy and powerful people in the world.
[01:58:02.780 --> 01:58:04.660]   In your position, it's difficult to find people
[01:58:04.660 --> 01:58:05.660]   you can trust.
[01:58:05.660 --> 01:58:07.600]   - Trust no one, not even yourself, not trusting yourself.
[01:58:07.600 --> 01:58:10.740]   - Okay, well, that's, you're saying that jokingly.
[01:58:10.740 --> 01:58:11.580]   But is there some-
[01:58:11.580 --> 01:58:13.040]   - Trust no one, not even no one.
[01:58:13.040 --> 01:58:17.060]   - Need an hour just to think about that.
[01:58:17.060 --> 01:58:18.140]   And maybe some drugs.
[01:58:18.140 --> 01:58:21.180]   And maybe Grok the hell.
[01:58:21.180 --> 01:58:22.900]   I mean, is there some aspect of that one,
[01:58:22.900 --> 01:58:24.300]   just existing in a world
[01:58:24.300 --> 01:58:26.240]   where everybody wants something from you?
[01:58:26.240 --> 01:58:28.940]   How hard is it to exist in that world?
[01:58:28.940 --> 01:58:29.780]   - I'll survive.
[01:58:29.780 --> 01:58:32.320]   - There's a song like that too.
[01:58:32.320 --> 01:58:33.520]   - I will survive.
[01:58:33.520 --> 01:58:35.080]   - Were you petrified at first?
[01:58:35.080 --> 01:58:39.140]   Okay, I forget the rest of the lyrics.
[01:58:39.140 --> 01:58:42.160]   But is there, you don't struggle with this?
[01:58:42.160 --> 01:58:44.380]   I mean, I know you survive, but like there's ways-
[01:58:44.380 --> 01:58:47.480]   - Petrify is a spell in the Druid tree.
[01:58:47.480 --> 01:58:48.820]   - What does it do?
[01:58:48.820 --> 01:58:49.660]   - Petrify.
[01:58:49.660 --> 01:58:50.580]   - It petrifies.
[01:58:50.580 --> 01:58:52.780]   - It turns.
[01:58:52.780 --> 01:58:54.780]   It turns the monsters into stone.
[01:58:54.780 --> 01:58:56.920]   - Oh, like literally?
[01:58:56.920 --> 01:58:58.360]   - Yeah, for like six seconds.
[01:58:58.360 --> 01:58:59.200]   - Oh, the seconds.
[01:58:59.200 --> 01:59:02.400]   There's so much math in Diablo that breaks my brain.
[01:59:02.400 --> 01:59:04.780]   - It's like math nonstop.
[01:59:04.780 --> 01:59:06.740]   - I mean, really, you're like laughing at it,
[01:59:06.740 --> 01:59:11.740]   but you don't, it can put a huge amount of tension on a mind.
[01:59:11.740 --> 01:59:15.240]   - Yes, it can be definitely stressful at times.
[01:59:15.240 --> 01:59:18.740]   - Well, how do you know who you can trust
[01:59:18.740 --> 01:59:20.480]   in work and personal life?
[01:59:20.480 --> 01:59:22.660]   - I mean, I guess you look at somebody's track record over time
[01:59:22.660 --> 01:59:25.260]   and if they've got a, you know,
[01:59:25.260 --> 01:59:29.960]   I guess you kind of use your neural net to assess,
[01:59:29.960 --> 01:59:31.620]   you know, someone.
[01:59:31.620 --> 01:59:33.460]   - Neural nets don't feel pain.
[01:59:33.460 --> 01:59:35.220]   Your neural net has consciousness.
[01:59:35.220 --> 01:59:37.560]   It might feel pain when people betray you.
[01:59:37.560 --> 01:59:40.300]   It can make you so-
[01:59:40.300 --> 01:59:41.560]   - I mean, I'm, you know, to be frank,
[01:59:41.560 --> 01:59:44.680]   I mean, I've almost never been betrayed.
[01:59:44.680 --> 01:59:48.440]   It's very, very rare.
[01:59:48.440 --> 01:59:50.480]   So, you know, for what it's worth.
[01:59:50.480 --> 01:59:52.540]   - I guess karma might be good to people,
[01:59:52.540 --> 01:59:54.340]   but it's not always good to people.
[01:59:54.340 --> 01:59:55.180]   - Yeah, karma's real.
[01:59:55.180 --> 01:59:56.960]   - Are there people you trust?
[01:59:56.960 --> 02:00:00.260]   Let me edit that question.
[02:00:00.260 --> 02:00:02.960]   Are there people close to you
[02:00:02.960 --> 02:00:04.660]   that call you out on your bullshit?
[02:00:04.660 --> 02:00:08.500]   - Well, the X platform's very helpful for that.
[02:00:08.500 --> 02:00:09.720]   - Hmm.
[02:00:09.720 --> 02:00:11.120]   - If you're looking for critical feedback.
[02:00:11.120 --> 02:00:15.100]   - Can it push you like into the extremes more?
[02:00:15.100 --> 02:00:16.840]   The extremes of thought make you cynical
[02:00:16.840 --> 02:00:18.240]   about human nature in general?
[02:00:18.240 --> 02:00:21.320]   - I don't think I will be cynical.
[02:00:21.320 --> 02:00:22.420]   In fact, I think,
[02:00:22.420 --> 02:00:27.420]   you know, my feeling is that one should be,
[02:00:27.420 --> 02:00:32.420]   you know, never trust a cynic.
[02:00:32.420 --> 02:00:39.060]   The reason is that cynics excuse their own bad behavior
[02:00:39.060 --> 02:00:41.140]   by saying everyone does it.
[02:00:41.140 --> 02:00:42.780]   - Mm-hmm.
[02:00:42.780 --> 02:00:44.740]   - Because they're cynical.
[02:00:44.740 --> 02:00:48.100]   So I always be, it's a red flag if someone's a cynic,
[02:00:48.100 --> 02:00:49.440]   a true cynic.
[02:00:49.440 --> 02:00:51.140]   - Yeah, there's a degree of projection there
[02:00:51.140 --> 02:00:52.300]   that's always fun to watch,
[02:00:52.300 --> 02:00:53.140]   but I think it's a good thing
[02:00:53.140 --> 02:00:53.960]   that you watch from the outside
[02:00:53.960 --> 02:00:55.880]   and enjoy the hypocrisy.
[02:00:55.880 --> 02:00:57.420]   - Well, it's just, if,
[02:00:57.420 --> 02:00:59.560]   but this is an important point
[02:00:59.560 --> 02:01:02.320]   that I think people who are listening should bear in mind.
[02:01:02.320 --> 02:01:04.980]   If somebody is cynical,
[02:01:04.980 --> 02:01:07.980]   meaning that they see bad behavior in everyone,
[02:01:07.980 --> 02:01:11.300]   it's easy for them to excuse their own bad behavior
[02:01:11.300 --> 02:01:14.180]   by saying that, well, everyone does it.
[02:01:14.180 --> 02:01:16.900]   That's not true.
[02:01:16.900 --> 02:01:22.180]   I think most people are kind of medium good.
[02:01:22.180 --> 02:01:26.100]   I do wish the people on X would be better
[02:01:26.100 --> 02:01:29.380]   at seeing the good in other people's behavior.
[02:01:29.380 --> 02:01:32.120]   There seems to be a kind of weight
[02:01:32.120 --> 02:01:35.060]   towards seeing the negative.
[02:01:35.060 --> 02:01:37.380]   Somehow the negative is sexier.
[02:01:37.380 --> 02:01:41.620]   Interpreting the negative is sexier, more viral.
[02:01:41.620 --> 02:01:44.040]   I don't know what that is exactly about human nature.
[02:01:44.040 --> 02:01:45.880]   - I mean, I find the X platform to be less negative
[02:01:45.880 --> 02:01:48.520]   than the legacy media, you know?
[02:01:48.520 --> 02:01:52.060]   I mean, if you read sort of conventional newspapers,
[02:01:52.060 --> 02:01:55.520]   it makes you sad, frankly.
[02:01:55.520 --> 02:02:01.420]   Whereas I'd say on the X platform,
[02:02:01.420 --> 02:02:05.480]   I mean, I really get more laughs per day on X
[02:02:05.480 --> 02:02:09.000]   than everything else combined from humans, you know?
[02:02:09.000 --> 02:02:10.100]   - Laughs is one thing.
[02:02:10.100 --> 02:02:13.880]   Laughs is, it overlaps,
[02:02:13.880 --> 02:02:17.640]   but it's not necessarily perfectly overlapping
[02:02:17.640 --> 02:02:21.940]   with like good vibes and support, like celebrating others
[02:02:21.940 --> 02:02:25.180]   for example, not in a stupid, shallow, naive way,
[02:02:25.180 --> 02:02:27.000]   but like in an awesome way, like, oh, something awesome
[02:02:27.000 --> 02:02:29.400]   happened and you celebrate them for it.
[02:02:29.400 --> 02:02:34.400]   It feels that that is outweighed by shitting on other people.
[02:02:34.400 --> 02:02:38.560]   Now, it's better than mainstream media, but it's still.
[02:02:38.560 --> 02:02:40.400]   - Yeah, mainstream media is almost
[02:02:40.400 --> 02:02:42.300]   relentlessly negative about everything.
[02:02:42.300 --> 02:02:45.780]   I mean, really the conventional news tries to answer
[02:02:45.780 --> 02:02:47.620]   the question, what is the worst thing that happened
[02:02:47.620 --> 02:02:48.460]   on earth today?
[02:02:48.460 --> 02:02:50.560]   And it's a big world.
[02:02:50.560 --> 02:02:51.820]   So on any given day, it's a big world.
[02:02:51.820 --> 02:02:54.240]   It's a day, something bad has happened.
[02:02:54.240 --> 02:02:55.500]   - And a generalization of that,
[02:02:55.500 --> 02:02:57.320]   what is the worst perspective I can take
[02:02:57.320 --> 02:02:59.200]   on a thing that happened?
[02:02:59.200 --> 02:03:00.880]   So. - Yeah.
[02:03:00.880 --> 02:03:02.600]   I don't know, there's just a strong negative bias
[02:03:02.600 --> 02:03:03.440]   in the news.
[02:03:03.440 --> 02:03:08.820]   I mean, I think there's, a possible explanation for this
[02:03:08.820 --> 02:03:13.820]   is evolutionary, where, you know,
[02:03:13.820 --> 02:03:17.980]   bad news historically would be potentially fatal.
[02:03:17.980 --> 02:03:19.280]   Like there's a lion over there,
[02:03:19.280 --> 02:03:21.700]   or there's some other tribe that wants to kill you.
[02:03:21.700 --> 02:03:25.960]   Good news, you know, like we found a patch of berries
[02:03:25.960 --> 02:03:27.720]   is nice to have, but not essential.
[02:03:27.720 --> 02:03:33.720]   - So our old friend, Tesla autopilot,
[02:03:33.720 --> 02:03:35.700]   and it's probably one of the most intelligent
[02:03:35.700 --> 02:03:38.060]   real world AI systems in the world.
[02:03:38.060 --> 02:03:40.760]   - Right, do you follow it from the beginning?
[02:03:40.760 --> 02:03:42.540]   - Yeah, it was one of the most incredible robots
[02:03:42.540 --> 02:03:44.100]   in the world and continues to be.
[02:03:44.100 --> 02:03:45.520]   - Yeah. - And it was really exciting.
[02:03:45.520 --> 02:03:50.520]   And it was super exciting when it generalized became more than
[02:03:51.580 --> 02:03:55.480]   four wheels, but a real world AI system
[02:03:55.480 --> 02:03:57.380]   that perceives the world.
[02:03:57.380 --> 02:03:58.220]   - Yeah.
[02:03:58.220 --> 02:04:01.080]   - And can have potentially different embodiments.
[02:04:01.080 --> 02:04:03.340]   - Well, I mean, the really wild thing
[02:04:03.340 --> 02:04:07.000]   about the anti-ant training is that it learns to read,
[02:04:07.000 --> 02:04:09.900]   like it can read signs, but we never taught it to read.
[02:04:09.900 --> 02:04:17.380]   So, yeah, we never taught it what a car was,
[02:04:17.380 --> 02:04:21.460]   or what a person was, or a bicyclist.
[02:04:21.460 --> 02:04:26.460]   It learned what all those things are,
[02:04:26.460 --> 02:04:31.220]   what all the objects are on the road from video,
[02:04:31.220 --> 02:04:33.660]   just from watching video, just like humans.
[02:04:33.660 --> 02:04:38.200]   I mean, humans are photons in controls out.
[02:04:38.200 --> 02:04:41.220]   Like the vast majority of information reaching our brain
[02:04:41.220 --> 02:04:43.040]   is from our eyes.
[02:04:43.040 --> 02:04:45.560]   And you say, well, what's the output?
[02:04:45.560 --> 02:04:49.940]   The output is our motor signals to our sort of fingers
[02:04:49.940 --> 02:04:51.340]   and mouth in order to, you know,
[02:04:51.340 --> 02:04:53.000]   in order to communicate.
[02:04:53.000 --> 02:04:57.760]   Photons in controls out.
[02:04:57.760 --> 02:04:59.160]   The same is true of the car.
[02:04:59.160 --> 02:05:04.120]   - But by looking at the sequence of images,
[02:05:04.120 --> 02:05:08.660]   it's, you've agreed with Ilya Setskeva recently,
[02:05:08.660 --> 02:05:11.340]   where he talked about LLM forming a world model.
[02:05:11.340 --> 02:05:14.300]   And basically language is a projection of that world model
[02:05:14.300 --> 02:05:17.220]   onto the sequence of letters.
[02:05:17.220 --> 02:05:18.060]   And you're saying-
[02:05:18.060 --> 02:05:20.500]   - It finds order in these things.
[02:05:21.220 --> 02:05:26.220]   It finds correlative clusters.
[02:05:26.220 --> 02:05:29.880]   - And so doing it's like understanding something deep
[02:05:29.880 --> 02:05:30.720]   about the world.
[02:05:30.720 --> 02:05:31.560]   - Yeah.
[02:05:31.560 --> 02:05:35.800]   - Which is like, it's beautiful.
[02:05:35.800 --> 02:05:37.200]   - That's how our brain works.
[02:05:37.200 --> 02:05:39.640]   - Yeah, but it's beautiful.
[02:05:39.640 --> 02:05:40.900]   - Photons in controls out.
[02:05:40.900 --> 02:05:42.220]   - Neon nets are able to understand
[02:05:42.220 --> 02:05:43.840]   that deep meaning in the world.
[02:05:43.840 --> 02:05:46.500]   And so the question is how far can it go?
[02:05:46.500 --> 02:05:50.060]   And it does seem everybody's excited about LLMs.
[02:05:50.060 --> 02:05:51.100]   So in the space of-
[02:05:51.100 --> 02:05:51.940]   - Mm-hmm.
[02:05:51.940 --> 02:05:54.320]   - Self-supervised learning in the space of text.
[02:05:54.320 --> 02:05:55.160]   - Yeah.
[02:05:55.160 --> 02:06:00.100]   - It seems like there's a deep similarity
[02:06:00.100 --> 02:06:03.340]   between that and what Tesla Autopilot is doing.
[02:06:03.340 --> 02:06:06.440]   Is it to you basically the same,
[02:06:06.440 --> 02:06:08.860]   but different- - They are converging.
[02:06:08.860 --> 02:06:09.800]   They are converging.
[02:06:09.800 --> 02:06:12.700]   - I wonder who gets there faster,
[02:06:12.700 --> 02:06:14.600]   having a deep understanding of the world.
[02:06:14.600 --> 02:06:19.580]   Or they just will naturally converge.
[02:06:19.580 --> 02:06:20.980]   - They're both headed towards AGI.
[02:06:20.980 --> 02:06:28.980]   The Tesla approach is much more computer efficient. It had to be, because we were constrained
[02:06:28.980 --> 02:06:41.720]   on this, you know, we only have 100 watts, um, and an N to 8 computer, 144 trillion operations
[02:06:41.720 --> 02:06:48.860]   per second which sounds like a lot, but it's kind of small potatoes these days at N to
[02:06:48.860 --> 02:06:55.940]   8, but it's understanding the world at N to 8. It's only 256 values.
[02:06:55.940 --> 02:07:03.060]   But there the path to AGI might have much more significant impact because it's understanding,
[02:07:03.060 --> 02:07:09.380]   it'll faster understand the real world than will LLMs and therefore be able to integrate
[02:07:09.380 --> 02:07:13.220]   with the real world, humans in the real world faster.
[02:07:13.220 --> 02:07:18.840]   They're both, um, going to understand the world, but I think Tesla's approach
[02:07:18.840 --> 02:07:24.060]   is fundamentally more computer efficient. It had to be, there was no choice. Like our
[02:07:24.060 --> 02:07:28.880]   brain is very computer efficient, very, very energy efficient. So think of like what, what
[02:07:28.880 --> 02:07:35.720]   is our brain able to do? Um, you know, there's only about 10 watts of higher brain function,
[02:07:35.720 --> 02:07:40.280]   not counting stuff that's just used to control our body. Um, the thinking part of our brain
[02:07:40.280 --> 02:07:47.580]   is less than 10 watts. Um, and that 10, those 10 watts can still produce a much better novel
[02:07:47.580 --> 02:07:48.680]   than a 10 megawatt.
[02:07:48.680 --> 02:07:48.680]   Yeah.
[02:07:48.680 --> 02:07:48.800]   Yeah.
[02:07:48.800 --> 02:07:58.540]   GPU cluster. So there's a sixth order of magnitude difference there. I mean, the AI has thus
[02:07:58.540 --> 02:08:04.980]   far gotten to where it is via brute force, just throwing massive amounts of compute and
[02:08:04.980 --> 02:08:15.620]   massive amounts of power at it. So this is not where it will end up. You know, in general
[02:08:15.620 --> 02:08:18.640]   with any given technology, first try to make it work and then you make it efficient.
[02:08:18.640 --> 02:08:30.720]   So I think we'll find over time that these models get smaller, are able to produce a
[02:08:30.720 --> 02:08:38.100]   sensible output with far less compute, far less power. Tesla is arguably ahead of the
[02:08:38.100 --> 02:08:46.740]   game on that front because we've just been forced to try to understand the world with
[02:08:46.740 --> 02:08:48.100]   a hundred watts.
[02:08:48.100 --> 02:08:56.600]   And there are a bunch of fundamental functions that we kind of forgot to include. So we have
[02:08:56.600 --> 02:09:02.780]   to run them in a bunch of things in emulation. We fixed a bunch of those with hardware four
[02:09:02.780 --> 02:09:13.200]   and then hardware five will be even better. But it does appear at this point that the
[02:09:13.200 --> 02:09:18.100]   car will be able to drive better than a human, even with hardware three and a hundred watts.
[02:09:18.100 --> 02:09:25.100]   And really, if we really optimize it, it could be probably less than 50 watts.
[02:09:25.100 --> 02:09:33.980]   What have you learned about developing Optimus, about applying, integrating this kind of real
[02:09:33.980 --> 02:09:43.700]   world AI into the space of robotic manipulation, just humanoid robotics? What are some interesting
[02:09:43.700 --> 02:09:46.600]   tiny or big things you've understood?
[02:09:46.600 --> 02:09:48.100]   I was surprised.
[02:09:48.100 --> 02:09:53.120]   I was surprised at the fact that we had to develop every part of the robot ourselves,
[02:09:53.120 --> 02:10:04.000]   that there were no off the shelf motors, electronics, sensors. Like we had to develop everything.
[02:10:04.000 --> 02:10:10.600]   We couldn't actually find a source of electric motors for any amount of money.
[02:10:10.600 --> 02:10:16.600]   So it's not even just the efficient, inexpensive, it's like anything. There's not a...
[02:10:16.600 --> 02:10:17.600]   No.
[02:10:17.600 --> 02:10:20.040]   - Oh, the actuators, everything,
[02:10:20.040 --> 02:10:23.380]   everything has to be designed from scratch.
[02:10:23.380 --> 02:10:26.560]   - We tried hard to find anything that was,
[02:10:26.560 --> 02:10:27.920]   because you think of how many electric motors
[02:10:27.920 --> 02:10:28.860]   are made in the world.
[02:10:28.860 --> 02:10:31.340]   There's like tens of thousands,
[02:10:31.340 --> 02:10:34.720]   hundreds of thousands of electric motor designs.
[02:10:34.720 --> 02:10:39.720]   None of them were suitable for a humanoid robot,
[02:10:39.720 --> 02:10:41.180]   literally none.
[02:10:41.180 --> 02:10:46.080]   So we had to develop our own design,
[02:10:46.080 --> 02:10:51.080]   design it specifically for what a humanoid robot needs.
[02:10:51.080 --> 02:10:53.060]   - How hard was it to design something
[02:10:53.060 --> 02:10:54.660]   that can be mass manufactured,
[02:10:54.660 --> 02:10:56.660]   could be relatively inexpensive?
[02:10:56.660 --> 02:10:58.900]   I mean, if you compare it to Boston Dynamics Atlas,
[02:10:58.900 --> 02:11:02.040]   it's a very expensive robot.
[02:11:02.040 --> 02:11:04.260]   - It is designed to be manufactured
[02:11:04.260 --> 02:11:06.040]   in the same way they would make a car.
[02:11:06.040 --> 02:11:07.800]   And I think ultimately we can make Optimus
[02:11:07.800 --> 02:11:09.400]   for less than the cost of a car.
[02:11:09.400 --> 02:11:15.160]   It should be, 'cause if you look at the mass of the robot,
[02:11:15.160 --> 02:11:16.080]   it's much smaller.
[02:11:16.080 --> 02:11:19.380]   And the car has many actuators in it.
[02:11:19.380 --> 02:11:23.420]   The car has more actuators than the robot.
[02:11:23.420 --> 02:11:25.920]   - But there is, the actuators are kind of interesting
[02:11:25.920 --> 02:11:28.820]   on a humanoid robot with the fingers.
[02:11:28.820 --> 02:11:32.740]   So Optimus has really nice hands and fingers, you know?
[02:11:32.740 --> 02:11:33.940]   - Yeah.
[02:11:33.940 --> 02:11:36.000]   - And they could do some interesting manipulation.
[02:11:36.000 --> 02:11:38.840]   So soft touch robotics.
[02:11:38.840 --> 02:11:41.920]   - I mean, one of the tests goals I have is,
[02:11:41.920 --> 02:11:46.020]   can it pick up a needle and a thread and thread the needle?
[02:11:46.020 --> 02:11:47.680]   Just by looking.
[02:11:47.680 --> 02:11:49.580]   - How far away are we from that?
[02:11:49.580 --> 02:11:51.420]   Just by looking, just by looking.
[02:11:51.420 --> 02:11:52.340]   - Maybe a year.
[02:11:52.340 --> 02:11:56.200]   Although I go back to, I'm optimistic on time.
[02:11:56.200 --> 02:11:57.780]   The work that we're doing in the car
[02:11:57.780 --> 02:11:59.340]   will translate to the robot.
[02:11:59.340 --> 02:12:02.220]   - The perception or also the control?
[02:12:02.220 --> 02:12:03.140]   - No, the controls are different,
[02:12:03.140 --> 02:12:06.140]   but the video in controls out.
[02:12:06.140 --> 02:12:09.340]   The car is a robot on four wheels.
[02:12:09.340 --> 02:12:14.820]   The Optimus is a robot with hands and legs.
[02:12:14.820 --> 02:12:15.960]   - So you can just- - The video in controls out.
[02:12:15.960 --> 02:12:17.800]   - They're very similar.
[02:12:17.800 --> 02:12:20.820]   - So the entire machinery of the learning process
[02:12:20.820 --> 02:12:21.920]   end to end is just,
[02:12:21.920 --> 02:12:23.880]   you just have a different set of controls.
[02:12:23.880 --> 02:12:25.700]   - Optimus will figure out how to do things
[02:12:25.700 --> 02:12:26.600]   by watching videos.
[02:12:26.600 --> 02:12:29.000]   - As the saying goes,
[02:12:29.000 --> 02:12:32.020]   "Be kind for everyone you meet is fighting a battle
[02:12:32.020 --> 02:12:33.300]   you know nothing about."
[02:12:33.300 --> 02:12:34.260]   - Yeah, that's true.
[02:12:34.260 --> 02:12:35.760]   - What's something difficult you're going through
[02:12:35.760 --> 02:12:37.320]   that people don't often see?
[02:12:37.320 --> 02:12:39.700]   - Trying to feed Uberluth.
[02:12:39.700 --> 02:12:45.900]   No, I mean, you know,
[02:12:45.900 --> 02:12:48.800]   I mean, my mind is a storm.
[02:12:48.800 --> 02:12:53.800]   I don't think most people would want to be me.
[02:12:53.800 --> 02:12:56.160]   They may think they'd want to be me, but they don't.
[02:12:56.160 --> 02:12:57.000]   They don't know.
[02:12:57.000 --> 02:12:58.120]   They don't understand.
[02:12:58.120 --> 02:13:09.180]   - How are you doing?
[02:13:09.180 --> 02:13:11.420]   - I'm overall okay.
[02:13:11.420 --> 02:13:13.140]   I think I'm doing okay.
[02:13:13.140 --> 02:13:14.700]   - I think you're doing okay.
[02:13:14.700 --> 02:13:15.840]   - Yeah.
[02:13:15.840 --> 02:13:16.680]   - Yeah.
[02:13:16.680 --> 02:13:19.480]   - In the grand scheme of things, I can't complain.
[02:13:19.480 --> 02:13:22.220]   - Do you get lonely?
[02:13:22.220 --> 02:13:28.140]   - Sometimes, but I, you know,
[02:13:28.140 --> 02:13:30.680]   my kids and friends keep me company.
[02:13:30.680 --> 02:13:34.780]   - So not existential.
[02:13:34.780 --> 02:13:38.420]   - There are many nights I sleep alone.
[02:13:38.420 --> 02:13:43.740]   I don't have to, but I do.
[02:13:43.740 --> 02:13:44.620]   - Yeah.
[02:13:44.620 --> 02:13:50.160]   Walter Isaacson in his new biography of you wrote about your difficult childhood.
[02:13:50.160 --> 02:13:56.380]   Will you ever find forgiveness in your heart for everything that has happened to you in that period of your life?
[02:13:56.380 --> 02:14:02.780]   I do not...
[02:14:02.780 --> 02:14:08.020]   At least I don't think I harbor resentment.
[02:14:14.080 --> 02:14:17.820]   So, nothing to forgive.
[02:14:17.820 --> 02:14:22.520]   You know, forgiveness is difficult for people.
[02:14:22.520 --> 02:14:26.920]   It seems like you don't harbor the resentment.
[02:14:26.920 --> 02:14:32.880]   I mean, I try to think about what is going to affect the future in a good way.
[02:14:32.880 --> 02:14:40.080]   And holding on to grudges does not affect the future in a good way.
[02:14:40.080 --> 02:14:42.860]   You're a father, a proud father.
[02:14:43.540 --> 02:14:45.780]   What have you learned about life from your kids?
[02:14:45.780 --> 02:14:50.620]   Those little biological organisms.
[02:14:50.620 --> 02:14:59.720]   I mean, developing AI and watching, say, a little X grow is fascinating.
[02:14:59.720 --> 02:15:06.500]   Because they are far more parallel than I would have expected.
[02:15:06.500 --> 02:15:13.140]   I mean, I can see his biological neural net making more and more sense of the world.
[02:15:13.140 --> 02:15:17.380]   And I can see the digital neural net making more and more sense of the world at the same time.
[02:15:17.380 --> 02:15:20.820]   Do you see the beauty and magic in both?
[02:15:20.820 --> 02:15:21.920]   Yes.
[02:15:21.920 --> 02:15:32.020]   I mean, one of the things with kids is that, you know, you kind of see the world anew in their eyes.
[02:15:32.020 --> 02:15:39.220]   You know, to them, everything is new and fresh.
[02:15:39.220 --> 02:15:42.600]   And then when you see that...
[02:15:42.600 --> 02:15:48.920]   When you see that them experience the world as new and fresh, you do too.
[02:15:48.920 --> 02:15:58.360]   Well, Elon, I just want to say thank you for your kindness to me and friendship over the years.
[02:15:58.360 --> 02:16:02.760]   For seeing something in a silly kid like me, as you've done for many others.
[02:16:02.760 --> 02:16:08.480]   And thank you for having hope for a positive future for humanity.
[02:16:08.480 --> 02:16:10.900]   And for working your ass off to make it happen.
[02:16:10.900 --> 02:16:11.600]   Thank you, Elon.
[02:16:11.600 --> 02:16:12.180]   Thanks, Lex.
[02:16:12.180 --> 02:16:12.220]   Thanks, Lex.
[02:16:12.220 --> 02:16:15.940]   Thank you for listening to this conversation with Elon Musk.
[02:16:15.940 --> 02:16:19.320]   To support this podcast, please check out our sponsors in the description.
[02:16:19.320 --> 02:16:28.280]   And now, let me leave you with some words that Walter Isaacson wrote about the central philosophy of how Elon approaches difficult problems.
[02:16:28.280 --> 02:16:33.720]   The only rules are the ones dictated by the laws of physics.
[02:16:33.720 --> 02:16:37.520]   Thank you for listening, and I hope to see you next time.
[02:16:37.520 --> 02:16:37.560]   Thank you.
[02:16:37.560 --> 02:16:37.600]   Thank you.
[02:16:37.600 --> 02:16:37.640]   Thank you.
[02:16:37.640 --> 02:16:37.680]   Thank you.
[02:16:37.680 --> 02:16:37.720]   Thank you.
[02:16:37.720 --> 02:16:37.820]   Thank you.
[02:16:37.820 --> 02:16:37.940]   Thank you.
[02:16:37.940 --> 02:16:38.060]   Thank you.
[02:16:38.060 --> 02:16:38.200]   Thank you.
[02:16:38.200 --> 02:16:38.340]   Thank you.
[02:16:38.340 --> 02:16:38.480]   Thank you.
[02:16:38.480 --> 02:16:38.640]   Thank you.
[02:16:38.640 --> 02:16:38.800]   Thank you.
[02:16:38.800 --> 02:16:38.980]   Thank you.
[02:16:38.980 --> 02:16:39.160]   Thank you.
[02:16:39.160 --> 02:16:39.340]   Thank you.
[02:16:39.340 --> 02:16:39.540]   Thank you.
[02:16:39.540 --> 02:16:39.740]   Thank you.
[02:16:39.740 --> 02:16:39.940]   Thank you.
[02:16:39.940 --> 02:16:40.180]   Thank you.
[02:16:40.180 --> 02:16:40.420]   Thank you.
[02:16:40.420 --> 02:16:40.660]   Thank you.
[02:16:40.660 --> 02:16:40.900]   Thank you.
[02:16:40.900 --> 02:16:41.140]   Thank you.
[02:16:41.140 --> 02:16:41.380]   Thank you.
[02:16:41.380 --> 02:17:11.360]   Thank you.

