
[00:00:00.000 --> 00:00:05.480]   AI is a weird field, right?
[00:00:05.480 --> 00:00:10.360]   It's this combination of various fields and it's always been like this, right?
[00:00:10.360 --> 00:00:16.400]   Since the '50s when the term was coined, it's this blend of computer science and neuroscience
[00:00:16.400 --> 00:00:22.200]   and psychology that has always been the case and it continues to be the case.
[00:00:22.200 --> 00:00:26.600]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:26.600 --> 00:00:29.280]   and I'm your host, Lukas Biewald.
[00:00:29.280 --> 00:00:34.360]   Greg Metz is a journalist who's been covering technology for the past few decades.
[00:00:34.360 --> 00:00:39.080]   He recently wrote a book, Genius Makers, which is kind of a historical document up until
[00:00:39.080 --> 00:00:43.640]   the present about artificial intelligence and the people that built the technology behind
[00:00:43.640 --> 00:00:44.640]   it.
[00:00:44.640 --> 00:00:45.640]   I have so many questions about this book.
[00:00:45.640 --> 00:00:49.360]   I can't wait to talk to him.
[00:00:49.360 --> 00:00:55.520]   You are the first non-ML practitioner to ever appear on this podcast, so I'm excited to
[00:00:55.520 --> 00:01:00.480]   do this and we might take things in a different direction than normal, but I was really excited
[00:01:00.480 --> 00:01:01.480]   to talk to you.
[00:01:01.480 --> 00:01:05.280]   I kind of procrastinated on reading your book and then I actually really enjoyed it.
[00:01:05.280 --> 00:01:07.560]   I was kind of afraid that I wouldn't.
[00:01:07.560 --> 00:01:12.600]   The name made me a little worried that it might be a bit over the top or something like
[00:01:12.600 --> 00:01:13.600]   that.
[00:01:13.600 --> 00:01:19.240]   I also felt like typically when you read journalism on topics you know really well, it's hard
[00:01:19.240 --> 00:01:25.640]   not to be critical or feel like the person didn't get something exactly right.
[00:01:25.640 --> 00:01:34.360]   But then it kind of reminded me of that show Silicon Valley, just in its incredibly accurate
[00:01:34.360 --> 00:01:35.360]   details.
[00:01:35.360 --> 00:01:40.600]   I feel like I've been in this world of machine learning and I've been in a world of venture
[00:01:40.600 --> 00:01:46.440]   capital, which are kind of the two main topics that your book covers.
[00:01:46.440 --> 00:01:50.520]   And all the little anecdotes and details, they really just rank true to me.
[00:01:50.520 --> 00:01:56.160]   I felt like you do these things where you explain math, you explain sort of when someone
[00:01:56.160 --> 00:02:01.320]   makes fun of somebody for differentiation by faith, what that means, or you describe
[00:02:01.320 --> 00:02:06.480]   what a TPU does, and you really actually go into technical detail that I'm not even sure
[00:02:06.480 --> 00:02:10.480]   I would necessarily do if I was writing for a mass audience.
[00:02:10.480 --> 00:02:14.100]   And I actually think you are remarkably accurate in that.
[00:02:14.100 --> 00:02:19.280]   And then you sort of describe these very vivid scenes that just seem like...
[00:02:19.280 --> 00:02:22.720]   Sometimes I feel like when you read sort of descriptions after the fact of a scene of
[00:02:22.720 --> 00:02:27.240]   like an acquisition or a fundraiser or something, it's like, "I don't think this journalist
[00:02:27.240 --> 00:02:31.560]   really was getting accurate information," or transcribe it the way...
[00:02:31.560 --> 00:02:35.600]   It just doesn't feel right sometimes, but your book really felt accurate to me.
[00:02:35.600 --> 00:02:41.400]   And it was a really interesting lens for me just on a world that I've been sort of adjacent
[00:02:41.400 --> 00:02:42.860]   to.
[00:02:42.860 --> 00:02:46.880]   Some of the folks have been on our podcast, some of them are customers of ours now.
[00:02:46.880 --> 00:02:50.640]   So I know a lot of the characters in your book, but I don't kind of get to know them
[00:02:50.640 --> 00:02:53.960]   intimately in the way that you clearly got to know them.
[00:02:53.960 --> 00:02:54.960]   So I actually...
[00:02:54.960 --> 00:02:58.080]   The question I was kind of dying to ask you, which has really maybe nothing to do with
[00:02:58.080 --> 00:03:02.980]   ML, is just how did you get so much access and what was your process for researching
[00:03:02.980 --> 00:03:03.980]   this book?
[00:03:03.980 --> 00:03:06.960]   Because there are some details I'm surprised you got someone to tell you.
[00:03:06.960 --> 00:03:10.560]   And it's not like you're sort of recanting interviews that you did.
[00:03:10.560 --> 00:03:11.560]   It's like somehow you just...
[00:03:11.560 --> 00:03:15.820]   It seems like you must have actually sat down with Jeff Hinton for a significant amount
[00:03:15.820 --> 00:03:21.600]   of time to be able to write this, or maybe you have some other process that I don't understand.
[00:03:21.600 --> 00:03:22.600]   No.
[00:03:22.600 --> 00:03:28.160]   I mean, well, I will tell you, but it's really interesting for me, first of all, to hear
[00:03:28.160 --> 00:03:33.240]   what you thought you might get, and then also what you might've gotten in the end after
[00:03:33.240 --> 00:03:34.240]   reading it.
[00:03:34.240 --> 00:03:40.560]   And in a way, there was a time when I started working on this book and really got into it
[00:03:40.560 --> 00:03:46.920]   when I realized it was a really dumb idea, because on one sense, your audience hopefully
[00:03:46.920 --> 00:03:51.200]   is going to be machine learning professionals and researchers who are really steeped in
[00:03:51.200 --> 00:03:52.760]   this stuff.
[00:03:52.760 --> 00:03:59.680]   And if you venture too far outside that world, you're going to get them angry with you and
[00:03:59.680 --> 00:04:01.240]   you're going to lose them.
[00:04:01.240 --> 00:04:08.720]   But ultimately, the goal of the book should be to have any reader pick this up and enjoy
[00:04:08.720 --> 00:04:09.720]   it.
[00:04:09.720 --> 00:04:11.600]   And that should be the goal as well.
[00:04:11.600 --> 00:04:15.520]   And if you move too far towards the machine learning researcher, you're going to get those
[00:04:15.520 --> 00:04:18.360]   people angry and they're not going to take up your book.
[00:04:18.360 --> 00:04:23.520]   And the trick becomes to find the sweet spot right in the middle.
[00:04:23.520 --> 00:04:25.720]   And that's very difficult.
[00:04:25.720 --> 00:04:30.600]   And then on top of that, within the machine learning community, we act like that's a monolithic
[00:04:30.600 --> 00:04:31.800]   thing.
[00:04:31.800 --> 00:04:34.480]   It's actually this huge spectrum as well.
[00:04:34.480 --> 00:04:39.240]   And this is what I get to at the end of the book really, is that you have some people
[00:04:39.240 --> 00:04:44.680]   who believe this is just math and you have other people who where this is something more,
[00:04:44.680 --> 00:04:45.680]   right?
[00:04:45.680 --> 00:04:50.720]   This is sort of this almost, it's almost like a religion and this is going to create AGI.
[00:04:50.720 --> 00:04:54.480]   This is going to create a machine that can do anything the human brain can do.
[00:04:54.480 --> 00:05:00.480]   So within the ML community, you had this spectrum where people really, really disagree.
[00:05:00.480 --> 00:05:06.240]   And the goal is to somehow get all those people interested in your book.
[00:05:06.240 --> 00:05:09.640]   It seems like a mistake to even try that.
[00:05:09.640 --> 00:05:12.640]   But here is what I really believe in.
[00:05:12.640 --> 00:05:14.720]   And this sort of gets back to your question.
[00:05:14.720 --> 00:05:17.360]   Ultimately, this is a book about people, right?
[00:05:17.360 --> 00:05:22.760]   This is a book about some really interesting people and they are interesting in incredibly
[00:05:22.760 --> 00:05:23.760]   different ways.
[00:05:23.760 --> 00:05:27.520]   From Jeff Hinton to Demis Sabas at DeepMind to Jeff Dean.
[00:05:27.520 --> 00:05:29.160]   I can go on down the list.
[00:05:29.160 --> 00:05:33.960]   Tinnick Gebru, who was in the news recently because she really clashed with people at
[00:05:33.960 --> 00:05:36.640]   Google, including Jeff Dean.
[00:05:36.640 --> 00:05:40.860]   These are really interesting people who are really interesting in a lot of different ways.
[00:05:40.860 --> 00:05:43.120]   And ultimately, that's what this book is, right?
[00:05:43.120 --> 00:05:46.080]   It's a book about the people.
[00:05:46.080 --> 00:05:52.880]   And what I realized is that if I can just show who these people are and what their stories
[00:05:52.880 --> 00:05:59.640]   are and how all those stories fit together, then that's what makes it successful, right?
[00:05:59.640 --> 00:06:08.440]   And what that's about, what finding what those people are about, ultimately, it's about spending
[00:06:08.440 --> 00:06:12.600]   the time with them, as you indicated, right?
[00:06:12.600 --> 00:06:15.400]   And that takes a lot of doing, right?
[00:06:15.400 --> 00:06:21.820]   Some people, because they work for these giant companies, you can't really get at them initially.
[00:06:21.820 --> 00:06:25.800]   So you try somebody else and you get some good stories from them and you go back to
[00:06:25.800 --> 00:06:27.960]   the first person and you say, "Hey, I've got this.
[00:06:27.960 --> 00:06:30.000]   What else can you tell me?"
[00:06:30.000 --> 00:06:34.480]   And you develop, in some ways, a relationship with them.
[00:06:34.480 --> 00:06:40.080]   I tend to, even as you get close to these people, keep a little bit of a distance as
[00:06:40.080 --> 00:06:41.080]   a journalist.
[00:06:41.080 --> 00:06:45.040]   I think that's important too, because again, you've got to have an objective view and be
[00:06:45.040 --> 00:06:51.760]   able to really appreciate and rope in the beliefs and the experiences and the points
[00:06:51.760 --> 00:06:54.400]   of view of all these different people.
[00:06:54.400 --> 00:07:01.120]   But it's about years and years and years of gathering information and understanding it
[00:07:01.120 --> 00:07:06.360]   yourself and taking it back to people and say, "Can we talk about this more?"
[00:07:06.360 --> 00:07:10.320]   And then somewhere along the way, you get them to talk.
[00:07:10.320 --> 00:07:14.880]   Well, I can only speak for myself, but I thought it was a really interesting book.
[00:07:14.880 --> 00:07:18.640]   I mean, I couldn't put it down once I started it.
[00:07:18.640 --> 00:07:26.040]   One thing I was wondering about is the ending, I thought was very understated.
[00:07:26.040 --> 00:07:28.920]   It sort of ends with Jeff Hinton, who's kind of the main...
[00:07:28.920 --> 00:07:36.000]   I mean, I feel like he's almost like the main character in your book, kind of saying, "Well,
[00:07:36.000 --> 00:07:37.840]   maybe AGI isn't that important."
[00:07:37.840 --> 00:07:41.120]   How about spoiler alert?
[00:07:41.120 --> 00:07:42.120]   Yes.
[00:07:42.120 --> 00:07:43.120]   Well, you know.
[00:07:43.120 --> 00:07:46.720]   But I thought it doesn't even...
[00:07:46.720 --> 00:07:49.960]   He's kind of like, "Well, would you really want a vacuum cleaner that's..."
[00:07:49.960 --> 00:07:51.200]   This is my takeaway.
[00:07:51.200 --> 00:07:54.960]   I'm curious if I got it wrong or different than your intention, tell me.
[00:07:54.960 --> 00:07:57.160]   But I was thinking, does he really?
[00:07:57.160 --> 00:08:03.080]   A vacuum cleaner that could navigate my house and be smart about when to turn on and off
[00:08:03.080 --> 00:08:04.080]   and stuff.
[00:08:04.080 --> 00:08:05.080]   I mean, it doesn't really exist.
[00:08:05.080 --> 00:08:10.320]   And I actually, I think I would want my vacuum cleaner, I think, to be reasonably smart and
[00:08:10.320 --> 00:08:15.000]   bordering on, if it could reason about the world, that seems like it would actually be
[00:08:15.000 --> 00:08:18.680]   kind of better than a Roomba that can't yet.
[00:08:18.680 --> 00:08:23.560]   I was kind of surprised that Jeff Hinton thinks that, and it sort of felt different than what
[00:08:23.560 --> 00:08:25.480]   most of the people in your book were thinking.
[00:08:25.480 --> 00:08:27.040]   I was just kind of curious.
[00:08:27.040 --> 00:08:31.880]   And then you actually say, you sort of say, "But he kind of invested in some crazy reinforcement
[00:08:31.880 --> 00:08:32.880]   learning company."
[00:08:32.880 --> 00:08:34.360]   So maybe he doesn't even really think that.
[00:08:34.360 --> 00:08:38.120]   I'm kind of just kind of left with, I wonder what's the takeaway here?
[00:08:38.120 --> 00:08:40.160]   And I was just wondering what's kind of your takeaway?
[00:08:40.160 --> 00:08:45.400]   Because you really noticeably never seem to take a position on this stuff.
[00:08:45.400 --> 00:08:48.280]   But I mean, you've been watching the field for decades.
[00:08:48.280 --> 00:08:51.440]   I'm sure you have opinions on this.
[00:08:51.440 --> 00:08:52.440]   Right.
[00:08:52.440 --> 00:08:57.040]   Well, it's interesting that you would in some ways have that takeaway, that the ending is
[00:08:57.040 --> 00:09:01.200]   understated and sort of questioning AGI almost.
[00:09:01.200 --> 00:09:03.480]   My first book review has come out.
[00:09:03.480 --> 00:09:08.200]   There's this trade publication called Kyrkus, which reviews big books.
[00:09:08.200 --> 00:09:11.080]   And their takeaway is completely the opposite.
[00:09:11.080 --> 00:09:18.040]   Like if you read the review, it is Cade Metz is making the case that AGI is possible.
[00:09:18.040 --> 00:09:22.320]   And so it is the completely other end of the spectrum than you.
[00:09:22.320 --> 00:09:26.880]   And I completely understand why the two of you have come to different conclusions.
[00:09:26.880 --> 00:09:29.280]   And it makes me happy because that's my aim.
[00:09:29.280 --> 00:09:31.440]   My aim is not to judge.
[00:09:31.440 --> 00:09:33.980]   My aim is not to make a call.
[00:09:33.980 --> 00:09:40.040]   My aim is to show what is going on in the world and what has gone on over the past 10
[00:09:40.040 --> 00:09:41.040]   years.
[00:09:41.040 --> 00:09:45.400]   And you have someone like Jeff Hinton, who is one of the most respected people in the
[00:09:45.400 --> 00:09:46.760]   field.
[00:09:46.760 --> 00:09:52.120]   At the same time, there are people who can't stand him because they feel like he has gone
[00:09:52.120 --> 00:09:53.280]   too far.
[00:09:53.280 --> 00:09:57.520]   There are people who can't stand him because he doesn't go far enough and doesn't say that
[00:09:57.520 --> 00:09:59.680]   AGI is around the corner.
[00:09:59.680 --> 00:10:07.400]   And you're right, the book ends with him in a way questioning AGI, but it also ends with
[00:10:07.400 --> 00:10:10.180]   him changing, right?
[00:10:10.180 --> 00:10:16.500]   Him embracing some stuff, namely reinforcement learning that he hadn't embraced in the past.
[00:10:16.500 --> 00:10:20.400]   And he sees the value there and he sees it accelerating.
[00:10:20.400 --> 00:10:23.960]   And in a way, he's just not going as far as some other people.
[00:10:23.960 --> 00:10:30.520]   And I think the book ends with some people who take a very different view and who do
[00:10:30.520 --> 00:10:33.320]   think AGI is around the corner.
[00:10:33.320 --> 00:10:36.480]   And it states them very explicitly, right?
[00:10:36.480 --> 00:10:44.740]   And I think it's about showing where all these people are coming from and letting the reader
[00:10:44.740 --> 00:10:48.040]   make their own decision about what's really going to happen.
[00:10:48.040 --> 00:10:50.300]   It's kind of interesting.
[00:10:50.300 --> 00:10:56.980]   One of the things that struck me about your book is you're sort of describing in a historian
[00:10:56.980 --> 00:11:01.760]   feeling way, something that's completely in progress.
[00:11:01.760 --> 00:11:05.720]   There's a whole bunch of things that happen right after your book stops, right?
[00:11:05.720 --> 00:11:10.800]   Like Timnit and Jeff Dean and all the stuff that happened at Google.
[00:11:10.800 --> 00:11:14.220]   And then I was actually thinking, it's funny, like Yoshua, I think in your book, he's not
[00:11:14.220 --> 00:11:17.980]   really doing a lot of commercial stuff in contrast to some of the other characters.
[00:11:17.980 --> 00:11:23.340]   But then I think Element AI recently kind of sold and that was like a bit of a controversy
[00:11:23.340 --> 00:11:25.320]   if that was a good outcome or a bad outcome.
[00:11:25.320 --> 00:11:28.500]   And then OpenAI went, you became a private company.
[00:11:28.500 --> 00:11:30.620]   Did you have a sense of like the book needs to stop here?
[00:11:30.620 --> 00:11:32.540]   Was there other stuff that you kind of wanted to include?
[00:11:32.540 --> 00:11:35.420]   Could there be a sequel to this?
[00:11:35.420 --> 00:11:38.520]   There could, but you know what's interesting about all the stuff you mentioned, and I would
[00:11:38.520 --> 00:11:43.940]   argue like almost everything in the book, it is completely in tune with what happens
[00:11:43.940 --> 00:11:46.380]   in the book, right?
[00:11:46.380 --> 00:11:51.060]   Timnit Gebru is a character in the book and the same things happen in the book.
[00:11:51.060 --> 00:11:52.460]   It's just with a different company, right?
[00:11:52.460 --> 00:11:53.460]   It's with Amazon.
[00:11:53.460 --> 00:11:54.460]   Right.
[00:11:54.460 --> 00:11:55.460]   Okay.
[00:11:55.460 --> 00:11:56.460]   It's not with Google.
[00:11:56.460 --> 00:11:57.460]   And then it happens with Google.
[00:11:57.460 --> 00:12:02.940]   Yoshua Bengio, he has stayed outside or more outside of the commercial realm than Hinton
[00:12:02.940 --> 00:12:04.380]   and Lacoon.
[00:12:04.380 --> 00:12:09.220]   But as the book goes into, like he dips his toes certainly, right?
[00:12:09.220 --> 00:12:12.620]   In the book, it's his partnership with Microsoft.
[00:12:12.620 --> 00:12:17.900]   He's also had one with IBM and with all these people, it's sort of a balancing act.
[00:12:17.900 --> 00:12:23.860]   And I think that's what the book is about is that you have these very idealistic people,
[00:12:23.860 --> 00:12:30.320]   whether it's Timnit or it's Yoshua or Hinton, and they all come into contact with these
[00:12:30.320 --> 00:12:33.000]   forces that are frankly much bigger than them.
[00:12:33.000 --> 00:12:37.300]   These corporate forces, these government forces.
[00:12:37.300 --> 00:12:41.980]   And when that happens, there's going to be conflict.
[00:12:41.980 --> 00:12:47.420]   And all those conflicts that have come since I finished the book, it's all happening in
[00:12:47.420 --> 00:12:48.420]   the book as well.
[00:12:48.420 --> 00:12:55.660]   And what OpenAI, how many times have they gone back and forth as far as what they're
[00:12:55.660 --> 00:12:57.460]   going to do and what they're not going to do?
[00:12:57.460 --> 00:12:59.420]   Are they a not-for-profit?
[00:12:59.420 --> 00:13:01.080]   Are they a public company?
[00:13:01.080 --> 00:13:05.820]   Do they believe in withholding the technology or sharing it?
[00:13:05.820 --> 00:13:13.880]   These things will continue to go back and forth, but the constant is that clash of belief
[00:13:13.880 --> 00:13:22.180]   and then those corporate forces which are about money and about attention and promotion.
[00:13:22.180 --> 00:13:24.480]   I think that those are the constants.
[00:13:24.480 --> 00:13:28.960]   And that's why I really believe in the book is because all those things are just going
[00:13:28.960 --> 00:13:31.780]   to continue to play out in the years to come.
[00:13:31.780 --> 00:13:38.240]   One thing that I really was curious to ask you about is, you set up these dichotomies
[00:13:38.240 --> 00:13:40.140]   that you personify, right?
[00:13:40.140 --> 00:13:45.140]   And Gary Marcus versus Yann LeCun maybe, or Elon Musk versus Zuckerberg.
[00:13:45.140 --> 00:13:46.820]   You should probably say what those are.
[00:13:46.820 --> 00:13:52.100]   People listening to this could probably guess what the dichotomies are here.
[00:13:52.100 --> 00:13:58.740]   I was curious, where do you land on this stuff now that you've talked to everybody?
[00:13:58.740 --> 00:14:05.460]   For example, do you feel like we are overstating the future progress of AI?
[00:14:05.460 --> 00:14:11.140]   It seems like if you take a historical view, like you're taking, it seems to me like ML
[00:14:11.140 --> 00:14:17.620]   has just made this steady incremental progress and people keep moving the goalposts of what
[00:14:17.620 --> 00:14:18.620]   it means to do AGI.
[00:14:18.620 --> 00:14:22.140]   First, you have to win at chess and then you have to win at Go.
[00:14:22.140 --> 00:14:27.700]   Then it's a gift to pass the Turing test, but then that's not even hard enough.
[00:14:27.700 --> 00:14:33.540]   And so for me, when I take a historical view, I sort of imagine steady progress extending
[00:14:33.540 --> 00:14:35.460]   out into the future.
[00:14:35.460 --> 00:14:40.180]   Then for me, when I look at these algorithms, it sure seems like a stretch that they turn
[00:14:40.180 --> 00:14:43.260]   into AGI just with more compute.
[00:14:43.260 --> 00:14:48.420]   So I actually don't even know where I land, but I'm curious where you land on this topic.
[00:14:48.420 --> 00:14:50.660]   Well, I think you're right.
[00:14:50.660 --> 00:14:54.580]   You have to look at this historically and that's what the book does, is that a lot of
[00:14:54.580 --> 00:15:01.780]   the claims that are being made now about AGI and this sort of pervading our lives and sort
[00:15:01.780 --> 00:15:05.500]   of taking away jobs, all that has been around since the fifties.
[00:15:05.500 --> 00:15:07.020]   And I show that in the book.
[00:15:07.020 --> 00:15:08.860]   And in a way, it's just a repeat of that.
[00:15:08.860 --> 00:15:14.820]   Now that said, there has been a huge amount of progress over the past 10 years, which
[00:15:14.820 --> 00:15:16.580]   is what the book really covers.
[00:15:16.580 --> 00:15:19.340]   We've had a huge amount of progress.
[00:15:19.340 --> 00:15:26.300]   What I really believe firmly in as a journalist, particularly as a New York Times reporter,
[00:15:26.300 --> 00:15:32.820]   is I feel like what has happened and what is possible in the public consciousness is
[00:15:32.820 --> 00:15:35.700]   way out of whack.
[00:15:35.700 --> 00:15:41.820]   And a lot of that just has to do with the term artificial intelligence, which has been
[00:15:41.820 --> 00:15:45.220]   thrown around so much over the past 10 years.
[00:15:45.220 --> 00:15:50.820]   That alone gives people a false impression about what is happening and what will happen.
[00:15:50.820 --> 00:15:56.420]   And then, frankly, most people writing about this stuff, for whatever reason, they don't
[00:15:56.420 --> 00:16:01.420]   really understand what's going on and they exaggerate.
[00:16:01.420 --> 00:16:05.940]   And maybe they exaggerate consciously, maybe they exaggerate unconsciously, maybe they
[00:16:05.940 --> 00:16:08.460]   don't know that they're exaggerating.
[00:16:08.460 --> 00:16:13.420]   But if you sit down and you read most of the stuff that's written, you have a false impression.
[00:16:13.420 --> 00:16:20.380]   And that is one thing that I really want to, at least in my small corner of the universe,
[00:16:20.380 --> 00:16:24.940]   try to correct and show people what is really happening.
[00:16:24.940 --> 00:16:29.140]   And the fact of the matter is none of us knows what the future is.
[00:16:29.140 --> 00:16:36.820]   And as much as someone who really believes in AGI might get on this call with us and
[00:16:36.820 --> 00:16:41.260]   get angry at me for not saying AGI is around the corner, the reality, and I think the book
[00:16:41.260 --> 00:16:44.700]   shows this, is that none of us know what the future holds.
[00:16:44.700 --> 00:16:47.940]   And when it comes to AGI, it's an argument.
[00:16:47.940 --> 00:16:49.860]   It's a religious argument.
[00:16:49.860 --> 00:16:51.300]   And I show that in the book.
[00:16:51.300 --> 00:16:55.720]   People with the same experience, the same knowledge, the same respect across the industry
[00:16:55.720 --> 00:16:58.220]   really disagree on this.
[00:16:58.220 --> 00:16:59.220]   But go ahead.
[00:16:59.220 --> 00:17:00.220]   No, no, it's funny.
[00:17:00.220 --> 00:17:06.500]   I guess one thing that's sort of, it's almost in the water, so I don't think to question
[00:17:06.500 --> 00:17:11.180]   it because I kind of swim in it, is why do you think it becomes such a religious argument?
[00:17:11.180 --> 00:17:16.860]   Like why do you think people feel so passionately frustrated that other people don't agree with
[00:17:16.860 --> 00:17:23.820]   them on this particular topic of like, is AGI possible or coming or coming soon?
[00:17:23.820 --> 00:17:30.660]   Well, I think that people are just coming from a very different place when they start
[00:17:30.660 --> 00:17:31.900]   talking about these things.
[00:17:31.900 --> 00:17:40.860]   And one of the things you realize about Silicon Valley is if you're going to be successful,
[00:17:40.860 --> 00:17:44.220]   you've got to really believe in what you're doing.
[00:17:44.220 --> 00:17:49.740]   That's again, either consciously or unconsciously, that's how you attract the money.
[00:17:49.740 --> 00:17:51.580]   That's how you attract the talent.
[00:17:51.580 --> 00:17:53.980]   That's how you get these things to snowball.
[00:17:53.980 --> 00:17:58.380]   Whether you're building a tiny little app that does something simple or you're trying
[00:17:58.380 --> 00:18:00.220]   to build AGI.
[00:18:00.220 --> 00:18:06.020]   So what has happened is that people have taken a rule book that has worked in Silicon Valley
[00:18:06.020 --> 00:18:12.820]   for certain things, let's say Facebook, and they're applying it to this notion that they
[00:18:12.820 --> 00:18:16.460]   can build a machine that can do anything the human brain can do.
[00:18:16.460 --> 00:18:20.940]   So in their mind, they're just doing what everybody else has done.
[00:18:20.940 --> 00:18:24.300]   But AGI is different than Facebook.
[00:18:24.300 --> 00:18:27.740]   That is a goal that is far, far bigger.
[00:18:27.740 --> 00:18:33.380]   And so in their world, they're just doing what everyone around them is doing, has done
[00:18:33.380 --> 00:18:37.780]   for the past however many decades in Silicon Valley.
[00:18:37.780 --> 00:18:44.940]   But for someone else, they're taking a huge step and they just do not see that.
[00:18:44.940 --> 00:18:52.780]   How can you extrapolate from a machine that can play go to a machine that can do anything
[00:18:52.780 --> 00:18:54.340]   the human brain can do?
[00:18:54.340 --> 00:19:01.940]   And if you ask people to describe to you how that's going to happen, at very least that's
[00:19:01.940 --> 00:19:07.100]   hard for them to do, right, describe how that's going to happen, right?
[00:19:07.100 --> 00:19:12.900]   The path that they see is a path that they painted very broad strokes.
[00:19:12.900 --> 00:19:19.580]   And saying I can build a Facebook, there's a path there to building a social networking
[00:19:19.580 --> 00:19:20.580]   app.
[00:19:20.580 --> 00:19:21.580]   We know how to do that.
[00:19:21.580 --> 00:19:23.180]   We don't know how to do this.
[00:19:23.180 --> 00:19:26.180]   And we don't know how to build a self-driving car, right?
[00:19:26.180 --> 00:19:32.180]   That alone is an astronomically difficult project that we don't quite know how to complete
[00:19:32.180 --> 00:19:33.180]   yet.
[00:19:33.180 --> 00:19:37.380]   But people still talk about it in terms like it's already there.
[00:19:37.380 --> 00:19:39.700]   And on one level, you see why they do that.
[00:19:39.700 --> 00:19:43.540]   But on another level, right, it misleads the public.
[00:19:43.540 --> 00:19:47.440]   It misleads people about what's going to happen soon.
[00:19:47.440 --> 00:19:52.540]   So I guess I sense that one opinion that you kind of hold is that there are a lot of overinflated
[00:19:52.540 --> 00:19:58.020]   claims and therefore the public feels like the public does not have a good sense of what's
[00:19:58.020 --> 00:20:01.340]   possible and not possible.
[00:20:01.340 --> 00:20:03.420]   That at the very least is true, right?
[00:20:03.420 --> 00:20:06.940]   You know, who knows?
[00:20:06.940 --> 00:20:10.400]   Tomorrow we may have a new technology that really blows us out of the water.
[00:20:10.400 --> 00:20:17.680]   But what we've seen over the past 10 years with this are repeated overinflated claims
[00:20:17.680 --> 00:20:20.280]   just in the sense of they don't give...
[00:20:20.280 --> 00:20:25.820]   Think about your mother, right, or my mother when they read stories, even in the New York
[00:20:25.820 --> 00:20:32.640]   Times over the past few years, where they assume that tomorrow we're going to have cars
[00:20:32.640 --> 00:20:35.700]   that can drive by themselves all over the place, right?
[00:20:35.700 --> 00:20:38.900]   They can't help but have that assumption because that's the way it's written about.
[00:20:38.900 --> 00:20:43.620]   And journalists write about that way because people like Elon Musk and so many others just
[00:20:43.620 --> 00:20:47.620]   say it's around the corner and they take them at face value, right?
[00:20:47.620 --> 00:20:52.000]   So I think that's really where the problem is, is that you're misleading the general
[00:20:52.000 --> 00:20:53.000]   public.
[00:20:53.000 --> 00:20:55.580]   And I do think that that's a real problem, right?
[00:20:55.580 --> 00:21:01.500]   At a time when our society is grappling with what is true and what is not, let's make more
[00:21:01.500 --> 00:21:09.180]   of an effort to say what is actually possible now and show people what the reality is now
[00:21:09.180 --> 00:21:12.900]   and try to do that in a way that's separate from what might come, right?
[00:21:12.900 --> 00:21:15.780]   The reality is now is that self-driving cars aren't up to the task.
[00:21:15.780 --> 00:21:21.340]   But it's kind of interesting you say that because, well, I wonder if maybe journalists
[00:21:21.340 --> 00:21:22.500]   are at fault then.
[00:21:22.500 --> 00:21:28.620]   Because certainly Elon Musk has a pattern of overstated claims, but I think he might
[00:21:28.620 --> 00:21:29.900]   be a little bit of an outlier.
[00:21:29.900 --> 00:21:34.900]   I mean, you would know better than me, but I feel like when I talk to ML researchers,
[00:21:34.900 --> 00:21:41.340]   they tend to be fairly understated or almost maybe a little too reticent in their claims.
[00:21:41.340 --> 00:21:43.660]   And maybe the ones that rise to the top aren't like that.
[00:21:43.660 --> 00:21:48.020]   But we've done like 30, 40 interviews on this here, and I almost feel like I'm trying to
[00:21:48.020 --> 00:21:51.220]   push people to extrapolate what you're doing.
[00:21:51.220 --> 00:21:52.420]   It seems like a big deal.
[00:21:52.420 --> 00:21:53.420]   I don't know.
[00:21:53.420 --> 00:21:58.500]   When you talk to Jeff Hinton, or actually let's go way back, right?
[00:21:58.500 --> 00:22:02.700]   In your book, you talk about Rosenblatt and then the New York Times, I think.
[00:22:02.700 --> 00:22:06.860]   It seems like a lot of journalists kind of write about what he's doing, saying it's going
[00:22:06.860 --> 00:22:13.180]   to get consciousness soon when he's basically doing a perception without even a second layer.
[00:22:13.180 --> 00:22:14.180]   Exactly.
[00:22:14.180 --> 00:22:15.180]   So what happened there?
[00:22:15.180 --> 00:22:19.140]   Do you think Rosenblatt has a responsibility to communicate better what's going on?
[00:22:19.140 --> 00:22:22.300]   Was he making overinflated claims at that time?
[00:22:22.300 --> 00:22:24.780]   Well, clearly he was, right?
[00:22:24.780 --> 00:22:31.740]   He's telling these reporters that we're going to have systems that could walk and talk and
[00:22:31.740 --> 00:22:35.180]   recreate themselves and somehow venture into space.
[00:22:35.180 --> 00:22:38.140]   And so the reporters are just going to report that.
[00:22:38.140 --> 00:22:39.140]   Right, right.
[00:22:39.140 --> 00:22:40.140]   Okay.
[00:22:40.140 --> 00:22:42.660]   And in a lot of ways, it's not that different now.
[00:22:42.660 --> 00:22:45.940]   And you talk about Elon Musk being an outlier.
[00:22:45.940 --> 00:22:48.420]   That's true and it's not.
[00:22:48.420 --> 00:22:52.700]   Again, you talk about ML researchers.
[00:22:52.700 --> 00:22:55.300]   That is not a monolithic group.
[00:22:55.300 --> 00:22:59.540]   The other thing I want to show people is that even the New York Times has written stories,
[00:22:59.540 --> 00:23:02.580]   AI experts say X, right?
[00:23:02.580 --> 00:23:08.140]   Well, AI experts, that's not one group.
[00:23:08.140 --> 00:23:11.580]   It's this spectrum of people.
[00:23:11.580 --> 00:23:20.820]   And you got to remember, DeepMind and OpenAI are founded on the notion that they are going
[00:23:20.820 --> 00:23:23.460]   to build AGI.
[00:23:23.460 --> 00:23:28.540]   And there are people at those companies who really, really believe that.
[00:23:28.540 --> 00:23:31.180]   And they're at the top of those companies.
[00:23:31.180 --> 00:23:34.740]   And they may not be as cavalier as Elon Musk.
[00:23:34.740 --> 00:23:38.820]   They may not have the megaphone that he has, but they really believe that.
[00:23:38.820 --> 00:23:41.180]   And those are important companies, right?
[00:23:41.180 --> 00:23:44.020]   They have a lot of serious research talent.
[00:23:44.020 --> 00:23:48.100]   Particularly DeepMind has had some really important breakthroughs.
[00:23:48.100 --> 00:23:53.900]   Just recently, the CAFs contest breakthrough, that's really important research that in some
[00:23:53.900 --> 00:23:57.300]   ways is separate from this notion that they're going after AGI.
[00:23:57.300 --> 00:24:02.940]   So these are important, important labs that are founded on this belief, right?
[00:24:02.940 --> 00:24:10.620]   And I've known Demis Hassabis, the co-founder of DeepMind for a long time now.
[00:24:10.620 --> 00:24:17.060]   And whatever you think about that belief of AGI, you got to take that guy seriously, right?
[00:24:17.060 --> 00:24:18.820]   He has a track record.
[00:24:18.820 --> 00:24:22.020]   He is a serious, serious person.
[00:24:22.020 --> 00:24:26.060]   And you may have a problem with a lot of the stuff he has done or said, but you have to
[00:24:26.060 --> 00:24:28.660]   listen to him, right?
[00:24:28.660 --> 00:24:34.340]   And I mean, similarly, the work coming out of OpenAI, it'd be hard to argue it's not
[00:24:34.340 --> 00:24:35.340]   super impressive.
[00:24:35.340 --> 00:24:40.580]   I feel like some people claim that there's a little bit of publicity stunt, but there's
[00:24:40.580 --> 00:24:46.020]   like you talk about the robotic hand manipulating a Rubik's Cube and that's really impressive.
[00:24:46.020 --> 00:24:51.340]   And maybe the Rubik's Cube makes it more fun, but I still think it's an amazing breakthrough
[00:24:51.340 --> 00:24:52.340]   in robotics.
[00:24:52.340 --> 00:24:53.340]   I agree.
[00:24:53.340 --> 00:24:54.340]   I completely agree.
[00:24:54.340 --> 00:24:55.340]   It's both, right?
[00:24:55.340 --> 00:24:59.460]   It is super impressive science on the one hand, and it's a stunt.
[00:24:59.460 --> 00:25:00.460]   It's both.
[00:25:00.460 --> 00:25:07.020]   And me as a New York Times reporter, as a book author, my job is to show you that it
[00:25:07.020 --> 00:25:08.580]   is both, right?
[00:25:08.580 --> 00:25:12.140]   And give you a really real sense of what's going on there.
[00:25:12.140 --> 00:25:15.720]   It's very easy to see that hand, right?
[00:25:15.720 --> 00:25:23.780]   This five fingered robotic hand solve a Rubik's Cube and think AGI is going to happen tomorrow,
[00:25:23.780 --> 00:25:24.780]   right?
[00:25:24.780 --> 00:25:29.540]   If you're not educated in the field, it is super easy to think that.
[00:25:29.540 --> 00:25:34.460]   And so my job is to say, there is an advance here, right?
[00:25:34.460 --> 00:25:40.740]   And you can see it, but there are some chinks in the armor.
[00:25:40.740 --> 00:25:46.100]   And the other thing that I've seen is that not even everyone at OpenAI is aware of the
[00:25:46.100 --> 00:25:47.660]   chinks in the armor, right?
[00:25:47.660 --> 00:25:54.660]   And that hand, while the result is super impressive, there are some caveats there that show you
[00:25:54.660 --> 00:26:00.200]   that even the science isn't quite where you think it might be, let alone sort of the stunty
[00:26:00.200 --> 00:26:03.220]   nature of it, right?
[00:26:03.220 --> 00:26:07.100]   My point over and over again is that these things are complicated.
[00:26:07.100 --> 00:26:12.460]   I guess, maybe this is inserting myself into it, into your story, but I was kind of there
[00:26:12.460 --> 00:26:16.260]   throughout it and I couldn't help, but I keep having this thought.
[00:26:16.260 --> 00:26:22.820]   I was at the Stanford AI Lab in 2003, 2004 at the sort of nadir of interest in neural
[00:26:22.820 --> 00:26:23.820]   nets.
[00:26:23.820 --> 00:26:25.500]   And you talk about this in your book.
[00:26:25.500 --> 00:26:30.380]   And I felt like the zeitgeist there was kind of like, ah, these neural nets are kind of
[00:26:30.380 --> 00:26:32.740]   ... the name is too good.
[00:26:32.740 --> 00:26:36.340]   We use support vector machines, not neural nets.
[00:26:36.340 --> 00:26:37.340]   That's not serious.
[00:26:37.340 --> 00:26:40.300]   And it's like, these people just are trying to hype these things.
[00:26:40.300 --> 00:26:44.260]   And yeah, they sort of work, but they're kind of tweaked to the point where they're overfitting
[00:26:44.260 --> 00:26:50.220]   and serious people wouldn't make a system called a neural net.
[00:26:50.220 --> 00:26:56.340]   And it's been kind of interesting to watch it turn out that the neural net strategy actually
[00:26:56.340 --> 00:26:57.340]   really works.
[00:26:57.340 --> 00:27:01.740]   The perceptron is the base thing that now is used everywhere.
[00:27:01.740 --> 00:27:07.580]   And so I actually kind of feel like maybe the folks I was working with at that time
[00:27:07.580 --> 00:27:09.540]   weren't dreaming enough.
[00:27:09.540 --> 00:27:14.700]   I think it's great that Andrew Ng, when he saw it working, really invested into it.
[00:27:14.700 --> 00:27:19.780]   But I remember you talk about some stories about the skepticism of the progress of neural
[00:27:19.780 --> 00:27:20.780]   nets.
[00:27:20.780 --> 00:27:21.780]   And I vividly remember that.
[00:27:21.780 --> 00:27:25.020]   It's just like, everyone says they have a better algorithm, especially neural nets.
[00:27:25.020 --> 00:27:26.020]   But then they were right.
[00:27:26.020 --> 00:27:30.100]   And I kind of wonder if you feel like there's any lessons to that, because it seems so remarkable
[00:27:30.100 --> 00:27:35.940]   that something would get all this attention and then sort of be thought of as bad and
[00:27:35.940 --> 00:27:38.860]   then kind of come back as the working technology.
[00:27:38.860 --> 00:27:43.420]   I wonder if there's other technologies out there that have followed that same path.
[00:27:43.420 --> 00:27:47.500]   Oh, I think it's an incredible story.
[00:27:47.500 --> 00:27:50.500]   It's amazing that some people kept working on this stuff.
[00:27:50.500 --> 00:27:52.020]   Amazing, right?
[00:27:52.020 --> 00:27:54.860]   That again is at the heart of this book.
[00:27:54.860 --> 00:28:02.340]   And it's something that I have always really been amazed by and impressed by is someone
[00:28:02.340 --> 00:28:08.020]   who keeps working on something, even in the face of everyone telling them it's not going
[00:28:08.020 --> 00:28:09.260]   to work, right?
[00:28:09.260 --> 00:28:11.740]   That is the basis for any good story.
[00:28:11.740 --> 00:28:14.020]   And that certainly happened here.
[00:28:14.020 --> 00:28:15.380]   And it will keep happening.
[00:28:15.380 --> 00:28:20.060]   And in fact, in some ways, you've already come full circle where you have this sort
[00:28:20.060 --> 00:28:25.820]   of the, let's call them the Gary Marcus crowd, who are saying the same things.
[00:28:25.820 --> 00:28:30.900]   Like neural nets don't do everything these guys say they're going to do.
[00:28:30.900 --> 00:28:33.500]   They're limited.
[00:28:33.500 --> 00:28:38.980]   And so in a way, they're still fighting the same battle, all right?
[00:28:38.980 --> 00:28:39.980]   But you're right.
[00:28:39.980 --> 00:28:44.080]   There are other technologies that will come along, have already come along that people
[00:28:44.080 --> 00:28:48.120]   are skeptical of that are going to work in the face of that.
[00:28:48.120 --> 00:28:50.340]   And it takes that, right?
[00:28:50.340 --> 00:28:57.360]   It takes that belief and that determination and just sort of years and years of hard work
[00:28:57.360 --> 00:29:02.060]   to make this stuff do what it's ultimately going to do.
[00:29:02.060 --> 00:29:06.900]   It seems like a lot of the characters in your book, I was kind of struck by, I don't have
[00:29:06.900 --> 00:29:10.040]   a good stat on this, so I could be wrong, but it seemed like a lot of them didn't come
[00:29:10.040 --> 00:29:11.660]   from a computer science background.
[00:29:11.660 --> 00:29:15.320]   It seemed like a remarkable number of them kind of came from biology and neuroscience
[00:29:15.320 --> 00:29:16.500]   and things like that.
[00:29:16.500 --> 00:29:19.260]   Do you have any thoughts on that?
[00:29:19.260 --> 00:29:20.260]   I agree.
[00:29:20.260 --> 00:29:25.980]   And that's another thing that I'm fascinated by is AI is a weird field, right?
[00:29:25.980 --> 00:29:28.820]   It's this combination of various fields.
[00:29:28.820 --> 00:29:30.860]   And it's always been like this, right?
[00:29:30.860 --> 00:29:36.900]   Since the '50s when the term was coined, it's this blend of computer science and neuroscience
[00:29:36.900 --> 00:29:42.640]   and psychology that has always been the case and it continues to be the case.
[00:29:42.640 --> 00:29:48.600]   And this is embodied by, again, my main character, Jeff Hinton, right?
[00:29:48.600 --> 00:29:54.400]   He is someone who didn't come at this from the computer science angle.
[00:29:54.400 --> 00:29:58.560]   And he's still, like one of the running things in the book is that he loves to downplay his
[00:29:58.560 --> 00:30:03.080]   skills as both a computer scientist and a mathematician.
[00:30:03.080 --> 00:30:06.740]   And he doesn't think of himself as either.
[00:30:06.740 --> 00:30:12.040]   He comes at it from that direction and sort of gives this, what is really just math, a
[00:30:12.040 --> 00:30:16.720]   perspective that you wouldn't necessarily expect it to have.
[00:30:16.720 --> 00:30:21.320]   And that bothers some people and some people don't understand that perspective that he
[00:30:21.320 --> 00:30:22.320]   gives it.
[00:30:22.320 --> 00:30:24.800]   But that is how he thinks.
[00:30:24.800 --> 00:30:30.360]   And it has a real influence, not only on how this field has progressed, but it does have
[00:30:30.360 --> 00:30:33.240]   an influence on how people perceive it, right?
[00:30:33.240 --> 00:30:38.920]   People don't understand when he and others, as much as they explain it and re-explain
[00:30:38.920 --> 00:30:46.000]   it, they don't understand them calling a neural network, a facsimile of the human brain.
[00:30:46.000 --> 00:30:50.160]   They don't understand that that's just a metaphor in some ways, right?
[00:30:50.160 --> 00:30:52.040]   But that's part of the way this field works.
[00:30:52.040 --> 00:30:58.080]   Well, I guess from a historical lens, maybe the takeaway is that being an outsider is
[00:30:58.080 --> 00:31:00.880]   an advantage in some ways.
[00:31:00.880 --> 00:31:03.520]   Oh, absolutely.
[00:31:03.520 --> 00:31:04.520]   Absolutely.
[00:31:04.520 --> 00:31:09.240]   And that's sort of the story of Silicon Valley as well, right?
[00:31:09.240 --> 00:31:14.680]   But that doesn't mean that just because you're an outsider, that you're going to be right.
[00:31:14.680 --> 00:31:16.460]   Not every outsider is right.
[00:31:16.460 --> 00:31:19.480]   Some are and some aren't.
[00:31:19.480 --> 00:31:22.160]   And I think that's the story of this book as well.
[00:31:22.160 --> 00:31:25.160]   Probably everyone is going to ask you this question, but I felt like I had to ask it.
[00:31:25.160 --> 00:31:29.240]   Do you have any kind of fun stories that you couldn't fit into the book because they didn't
[00:31:29.240 --> 00:31:33.760]   quite fit or any good anecdotes in all the research you were doing?
[00:31:33.760 --> 00:31:35.120]   That's a good question.
[00:31:35.120 --> 00:31:37.320]   Let me think that over.
[00:31:37.320 --> 00:31:39.440]   Most of it's in there, to tell you the truth.
[00:31:39.440 --> 00:31:45.320]   I mean, like all the good stuff, some of it is just unbelievable.
[00:31:45.320 --> 00:31:47.600]   And it took a long time to get.
[00:31:47.600 --> 00:31:51.320]   And once you have it from one person, you got to get it from another.
[00:31:51.320 --> 00:31:54.200]   So there were a lot of things, right?
[00:31:54.200 --> 00:31:59.640]   Including like the lead story in the book, in the prologue, that I wasn't sure I was
[00:31:59.640 --> 00:32:01.800]   going to be able to get in there.
[00:32:01.800 --> 00:32:03.000]   And thank goodness I did.
[00:32:03.000 --> 00:32:07.080]   Talking about the auction of the company, D&N.
[00:32:07.080 --> 00:32:08.080]   Exactly.
[00:32:08.080 --> 00:32:11.240]   And in particular, the price, right?
[00:32:11.240 --> 00:32:15.760]   That was one of the hardest facts to nail down in the whole process.
[00:32:15.760 --> 00:32:20.080]   I have to tell you, that's the only anecdote in the book I don't totally believe.
[00:32:20.080 --> 00:32:22.960]   It was the one where it just, maybe it's because it's actually true.
[00:32:22.960 --> 00:32:23.960]   It just feels unbelievable.
[00:32:23.960 --> 00:32:26.600]   It is 100% true.
[00:32:26.600 --> 00:32:29.760]   Including wait, so the part that I felt like it might have felt that way to the people
[00:32:29.760 --> 00:32:35.000]   involved, but it's hard to believe it actually happened, is they literally got Google and
[00:32:35.000 --> 00:32:41.600]   Baidu to bid at a particular time, like they're running a Sotheby auction or something.
[00:32:41.600 --> 00:32:43.040]   Are you sure that's true?
[00:32:43.040 --> 00:32:44.040]   That's amazing.
[00:32:44.040 --> 00:32:48.000]   No, it's true because I've talked to, I can't tell you the number of people I've talked
[00:32:48.000 --> 00:32:51.420]   to who were involved in that, directly involved in that.
[00:32:51.420 --> 00:32:52.800]   It's absolutely true.
[00:32:52.800 --> 00:32:54.560]   I guess that's how it goes, right?
[00:32:54.560 --> 00:32:57.320]   The thing that's really true is actually unbelievable.
[00:32:57.320 --> 00:32:59.720]   Right, exactly.
[00:32:59.720 --> 00:33:09.040]   But so many parts of that story are amazingly improbably true because it encapsulates everything,
[00:33:09.040 --> 00:33:10.040]   right?
[00:33:10.040 --> 00:33:16.080]   At the very beginning of this movement, let's call it a movement, or what is it?
[00:33:16.080 --> 00:33:22.120]   Like the very beginning of this explosion in AI hype, in neural networks starting to
[00:33:22.120 --> 00:33:27.360]   work, all the players there who would be involved are already there, right?
[00:33:27.360 --> 00:33:33.440]   From China and Baidu to Google to Microsoft to DeepMind is there, right?
[00:33:33.440 --> 00:33:41.080]   They're all there in this competition that would play out over the next 10 years.
[00:33:41.080 --> 00:33:45.120]   And that whole story came to me in bits and pieces, right?
[00:33:45.120 --> 00:33:49.040]   Over the course of, it was really months or maybe even years.
[00:33:49.040 --> 00:33:54.040]   And as each piece pops into place, you're saying this sounds too perfect to be true,
[00:33:54.040 --> 00:33:57.920]   but you know it's true because it's coming from multiple people.
[00:33:57.920 --> 00:34:03.800]   And it's verified by multiple people and all the perspectives kind of come together.
[00:34:03.800 --> 00:34:06.000]   And some people say, "Well, I won't tell you that."
[00:34:06.000 --> 00:34:10.080]   And then you get it from somebody else and they say, "Okay, yes, it's true."
[00:34:10.080 --> 00:34:15.960]   That's what's most fun about being a journalist is when you get those nuggets that just show
[00:34:15.960 --> 00:34:23.080]   you so much about human nature and also just help your story just fit together in ways
[00:34:23.080 --> 00:34:24.080]   you never expected.
[00:34:24.080 --> 00:34:27.920]   I never expected the book to begin with that, but it had to begin with that because it's
[00:34:27.920 --> 00:34:29.440]   just the greatest story.
[00:34:29.440 --> 00:34:31.840]   It's a good story and you go back to it a lot.
[00:34:31.840 --> 00:34:34.360]   And yeah, it is a great story.
[00:34:34.360 --> 00:34:41.360]   I guess one more just thought that I had reading your book is, I hadn't quite had the timeline
[00:34:41.360 --> 00:34:46.200]   in my head of when neural nets start taking off, but I feel like one thing that's kind
[00:34:46.200 --> 00:34:54.280]   of impressive is I feel like Elon Musk and Zuckerberg and Larry Page, I feel like they
[00:34:54.280 --> 00:35:01.040]   noticed that neural nets were working really well before most academics even noticed it.
[00:35:01.040 --> 00:35:04.560]   I was thinking about the timeline, I was thinking about when...
[00:35:04.560 --> 00:35:08.760]   And I'm in ML, I'm selling to ML companies for the last 15 years.
[00:35:08.760 --> 00:35:11.400]   And I feel like actually they were really early.
[00:35:11.400 --> 00:35:14.700]   How did they figure this out?
[00:35:14.700 --> 00:35:16.200]   It's remarkable, isn't it?
[00:35:16.200 --> 00:35:23.440]   And I think one of the things you can do is contrast the way they reacted.
[00:35:23.440 --> 00:35:25.080]   And you can criticize the way they reacted.
[00:35:25.080 --> 00:35:30.640]   You can say they went too far, of course, but contrast the way Google and Facebook reacted
[00:35:30.640 --> 00:35:33.400]   to the way Microsoft reacted.
[00:35:33.400 --> 00:35:37.760]   And Microsoft did not jump on it the way that those two other companies did.
[00:35:37.760 --> 00:35:43.080]   They didn't see it the way that the leaders of those companies did.
[00:35:43.080 --> 00:35:48.760]   Part of the narrative there, right, in my book is that Jeff Hinton was in Microsoft's
[00:35:48.760 --> 00:35:56.140]   lab doing this stuff with speech and it worked in a way that nobody thought it would work.
[00:35:56.140 --> 00:36:04.800]   Nobody in the ML community, nobody at Microsoft, and it works and they're all shocked, they're
[00:36:04.800 --> 00:36:10.300]   all blown away, but they don't jump on it the way that Google and Facebook did.
[00:36:10.300 --> 00:36:11.940]   That's really, really interesting.
[00:36:11.940 --> 00:36:17.980]   And you do wonder, is it about the age of the company?
[00:36:17.980 --> 00:36:23.600]   Is it about the general area that the company plays in?
[00:36:23.600 --> 00:36:29.900]   Google had a real need for that speech recognition system that Hinton and his students built
[00:36:29.900 --> 00:36:33.700]   in a way that Microsoft didn't, right, because it had Android.
[00:36:33.700 --> 00:36:35.580]   It had a place to put it.
[00:36:35.580 --> 00:36:40.940]   Now, it was also a company that, and this is talking in broad strokes, that would take
[00:36:40.940 --> 00:36:46.140]   new technologies and put them into play far faster than Microsoft would, especially in
[00:36:46.140 --> 00:36:48.060]   those days, right?
[00:36:48.060 --> 00:36:49.060]   That's part of it.
[00:36:49.060 --> 00:36:51.820]   But, you know, in the end, it's a combination of these things, right?
[00:36:51.820 --> 00:36:55.580]   It's the way the leaders think, it's the way the company is built, which in some ways is
[00:36:55.580 --> 00:36:57.420]   a reflection of the leader.
[00:36:57.420 --> 00:37:00.260]   It's about the age of the companies, right?
[00:37:00.260 --> 00:37:05.980]   As these companies get to be a certain size, like Microsoft, it becomes harder to jump
[00:37:05.980 --> 00:37:08.100]   on something.
[00:37:08.100 --> 00:37:14.340]   But like you see in the book, the way that Google jumped on it, and it's astonishing,
[00:37:14.340 --> 00:37:15.340]   right?
[00:37:15.340 --> 00:37:20.060]   You know, there's that conversation between Larry Page and Alan Eustace, you know, where
[00:37:20.060 --> 00:37:22.660]   he says, "You got to bet big on this."
[00:37:22.660 --> 00:37:28.020]   And this is, you're right, this is before even the ML community at large really understood
[00:37:28.020 --> 00:37:29.020]   what was going on.
[00:37:29.020 --> 00:37:33.940]   Larry Page is telling Alan Eustace to basically bet the farm on it.
[00:37:33.940 --> 00:37:39.060]   It's astonishing, it really is.
[00:37:39.060 --> 00:37:43.100]   I guess my takeaway is when I see something working, I'm going to jump on it.
[00:37:43.100 --> 00:37:50.620]   But even then, it's unclear where it's going to go, right?
[00:37:50.620 --> 00:37:56.180]   Like, you know, it works for speech, and then it works for images.
[00:37:56.180 --> 00:38:01.700]   And that image net is such a big moment, but then people in the ML community are still
[00:38:01.700 --> 00:38:03.980]   like, "Is this really going to work with natural language?"
[00:38:03.980 --> 00:38:07.660]   I mean, years later, they're saying that, "Is this really going to work with natural
[00:38:07.660 --> 00:38:09.180]   language?"
[00:38:09.180 --> 00:38:10.180]   And then it does, right?
[00:38:10.180 --> 00:38:17.060]   You know, these large language models, a la GoogleBERT, GPT-3, you know, it really started
[00:38:17.060 --> 00:38:19.500]   to work, and there was real doubt there.
[00:38:19.500 --> 00:38:23.180]   You know, it's hard to see these things, even when you're close to them.
[00:38:23.180 --> 00:38:27.220]   And we could go on down the line, robotics.
[00:38:27.220 --> 00:38:31.820]   It's not clear, even when this stuff works with multiple different areas, whether it's
[00:38:31.820 --> 00:38:34.800]   going to work with the next one.
[00:38:34.800 --> 00:38:37.900]   One theme that also comes up in your book, of course, because we're talking about academics,
[00:38:37.900 --> 00:38:42.540]   is sort of like who gets credit and who doesn't get credit, and where's credit deserved?
[00:38:42.540 --> 00:38:47.260]   And actually, one anecdote that I never knew that you have in your book, despite Wojciech
[00:38:47.260 --> 00:38:51.380]   being a pretty good friend of mine, is that AlexNet was originally called WojNet.
[00:38:51.380 --> 00:38:52.380]   Is that right?
[00:38:52.380 --> 00:38:53.380]   I can't believe you never told me that.
[00:38:53.380 --> 00:39:00.300]   I feel like if I was him, I would be...
[00:39:00.300 --> 00:39:02.540]   It's a great story, right?
[00:39:02.540 --> 00:39:03.540]   Why do we call it AlexNet?
[00:39:03.540 --> 00:39:06.780]   You go to the paper, and the paper doesn't really call it AlexNet.
[00:39:06.780 --> 00:39:08.980]   It's like everybody calls it that.
[00:39:08.980 --> 00:39:16.580]   Well, the way it worked was, and this is in the book, in a much more eloquent way, but
[00:39:16.580 --> 00:39:24.140]   like Google had started to build its own version, basically, and it was Wojciech who did it.
[00:39:24.140 --> 00:39:29.060]   And the way it worked is Google was, is whoever built the thing, you named it after them.
[00:39:29.060 --> 00:39:30.460]   And so that's what they called it.
[00:39:30.460 --> 00:39:36.420]   And then Hinton and Krzyzewski and Zuzkova show up, and they're like, "Why do you call
[00:39:36.420 --> 00:39:37.420]   it that?
[00:39:37.420 --> 00:39:39.580]   It's Krzyzewski who built the thing."
[00:39:39.580 --> 00:39:44.900]   So they just start calling it that, and that's what propagates all over the community.
[00:39:44.900 --> 00:39:50.060]   I think that it's a testament to those guys, right?
[00:39:50.060 --> 00:39:53.940]   They're rightfully so, in a lot of ways, revered in a way.
[00:39:53.940 --> 00:39:55.420]   They had some capital, right?
[00:39:55.420 --> 00:39:59.420]   But it's also just funny how those things work in the tech community.
[00:39:59.420 --> 00:40:04.540]   And sometimes those things are corrected, so to speak.
[00:40:04.540 --> 00:40:05.540]   Sometimes they're not, right?
[00:40:05.540 --> 00:40:06.660]   Well, who do you think...
[00:40:06.660 --> 00:40:10.500]   So is there someone that stands out to you as kind of not getting the credit they deserve?
[00:40:10.500 --> 00:40:15.460]   Because most of the people that the heroes of your book, I think, are really well known,
[00:40:15.460 --> 00:40:17.460]   at least of people listening to this.
[00:40:17.460 --> 00:40:19.900]   But do you feel like someone really...
[00:40:19.900 --> 00:40:26.660]   Do people talk about someone when you interviewed them that doesn't show up in such a big way?
[00:40:26.660 --> 00:40:31.380]   I think Juergen Schmidt, humor is the classic example, right?
[00:40:31.380 --> 00:40:32.960]   He's been written out a lot.
[00:40:32.960 --> 00:40:35.380]   He's written about in my book.
[00:40:35.380 --> 00:40:36.380]   The reality is...
[00:40:36.380 --> 00:40:38.260]   Although I don't know that he comes across so well in your book.
[00:40:38.260 --> 00:40:39.260]   Interesting.
[00:40:39.260 --> 00:40:40.260]   Okay.
[00:40:40.260 --> 00:40:41.260]   I don't know.
[00:40:41.260 --> 00:40:42.260]   I think...
[00:40:42.260 --> 00:40:49.860]   What I was going to say is, with all of this stuff, it's complicated.
[00:40:49.860 --> 00:40:50.860]   Okay?
[00:40:50.860 --> 00:40:51.860]   And let's take...
[00:40:51.860 --> 00:40:56.780]   Well, before we get to Juergen, let's start with Alex Nett.
[00:40:56.780 --> 00:41:05.980]   The reality is, although Alex Krzyzewski and Hinton and Ilya Sutskever did the work on
[00:41:05.980 --> 00:41:11.620]   that and really made it happen, they are building on the work of Jan LeCun, right?
[00:41:11.620 --> 00:41:15.020]   They're using a modified version of his algorithm.
[00:41:15.020 --> 00:41:17.780]   And he's building on the work of so many others.
[00:41:17.780 --> 00:41:20.140]   Everybody's building on everybody else's work.
[00:41:20.140 --> 00:41:25.180]   And on some level, they all deserve credit, right?
[00:41:25.180 --> 00:41:32.380]   And what Schmidt-Huber is saying is, these guys who work for these very big companies
[00:41:32.380 --> 00:41:36.140]   are getting this credit and I'm not, right?
[00:41:36.140 --> 00:41:42.460]   And I really like Juergen and I feel for him.
[00:41:42.460 --> 00:41:46.640]   At the same time, he is out there saying, "Give me credit.
[00:41:46.640 --> 00:41:48.180]   Give me credit."
[00:41:48.180 --> 00:41:51.080]   And that's part of this too, right?
[00:41:51.080 --> 00:41:53.460]   Some people do that.
[00:41:53.460 --> 00:41:56.340]   Some people let the credit come to them, right?
[00:41:56.340 --> 00:41:59.020]   And that's going to be viewed in different ways, right?
[00:41:59.020 --> 00:42:03.220]   Some people are going to criticize Juergen for saying, "Give me credit.
[00:42:03.220 --> 00:42:04.220]   Give me credit."
[00:42:04.220 --> 00:42:14.240]   But I know him and you can't help but feel for him as well because the reason that these
[00:42:14.240 --> 00:42:20.120]   others have gotten so much credit in large part is because they had these giant companies
[00:42:20.120 --> 00:42:22.020]   behind them, right?
[00:42:22.020 --> 00:42:29.620]   And these companies are good at producing and driving narratives.
[00:42:29.620 --> 00:42:33.860]   And some of the narratives that have been out there aren't necessarily true, right?
[00:42:33.860 --> 00:42:38.300]   There have been published stuff, a lot of it came from the companies that don't necessarily
[00:42:38.300 --> 00:42:40.900]   give the real view of these things.
[00:42:40.900 --> 00:42:47.220]   And the real view is that it's more complicated than you think.
[00:42:47.220 --> 00:42:53.020]   Do you think there's a topic in AI that the press should cover more than they do?
[00:42:53.020 --> 00:42:57.340]   I think it's more about, and I guess I'm going back to what I've said before, is the press
[00:42:57.340 --> 00:43:01.780]   needs to cover this in a different way, right?
[00:43:01.780 --> 00:43:03.380]   And with more skepticism, I guess.
[00:43:03.380 --> 00:43:05.060]   With more skepticism.
[00:43:05.060 --> 00:43:08.260]   And look, it is hard.
[00:43:08.260 --> 00:43:14.740]   Again, we're talking about you got to strike the right balance between showing people what's
[00:43:14.740 --> 00:43:17.980]   really going on, but not going too deep in the weeds.
[00:43:17.980 --> 00:43:22.700]   You don't want to lose people and that's a very hard thing to do.
[00:43:22.700 --> 00:43:30.660]   But when it comes to topics, what I will say is that a lot of people have written about
[00:43:30.660 --> 00:43:36.940]   this clash at Google between Tim Neat and the company.
[00:43:36.940 --> 00:43:42.340]   She's saying that she was fired and some people at Google are saying that wasn't the case.
[00:43:42.340 --> 00:43:48.060]   And in a way, it's a very specific argument.
[00:43:48.060 --> 00:43:54.700]   But I think this is really representative of a much larger clash that is going to have
[00:43:54.700 --> 00:43:58.780]   to happen in this field, right?
[00:43:58.780 --> 00:44:05.380]   These language models that are being built, these giant GPT-3 style language models, they
[00:44:05.380 --> 00:44:08.100]   are inherently biased, right?
[00:44:08.100 --> 00:44:14.460]   That is just a fact because human language is biased and these things train on this enormous
[00:44:14.460 --> 00:44:16.500]   amount of text.
[00:44:16.500 --> 00:44:21.500]   They're biased and they spew hate speech and other toxic material.
[00:44:21.500 --> 00:44:23.360]   That's just the reality.
[00:44:23.360 --> 00:44:29.380]   And that's what Tim Neat and others were saying in the paper that was at issue at Google.
[00:44:29.380 --> 00:44:32.420]   That battle is going to...
[00:44:32.420 --> 00:44:34.820]   If these models are going to have to...
[00:44:34.820 --> 00:44:39.340]   If these models are going to continue to progress and they really get out into the world, that
[00:44:39.340 --> 00:44:40.340]   battle is going to happen.
[00:44:40.340 --> 00:44:46.340]   It's going to have to happen on a much larger scale at so many different companies.
[00:44:46.340 --> 00:44:47.340]   And what's the battle?
[00:44:47.340 --> 00:44:52.020]   What are the two visions of the future?
[00:44:52.020 --> 00:45:00.300]   Well, on the one hand, you have a company like Microsoft who put out a much simpler
[00:45:00.300 --> 00:45:04.380]   conversational bot years ago now called Tay, right?
[00:45:04.380 --> 00:45:07.140]   Yeah, of course, I remember that.
[00:45:07.140 --> 00:45:13.540]   Rules-based for the most part, chat bot, and it started spewing hate speech and it created
[00:45:13.540 --> 00:45:20.020]   this huge backlash and they took it away.
[00:45:20.020 --> 00:45:28.540]   Microsoft ostensibly is going to put GPT-3 out in tandem with OpenAI.
[00:45:28.540 --> 00:45:33.220]   That is a clash waiting to happen, right?
[00:45:33.220 --> 00:45:38.020]   Microsoft's got to deal with the fact that these things are biased and that's going to
[00:45:38.020 --> 00:45:40.180]   offend a lot of people, right?
[00:45:40.180 --> 00:45:41.620]   How do you deal with that?
[00:45:41.620 --> 00:45:42.900]   That's an open question.
[00:45:42.900 --> 00:45:48.420]   It's an open question for Microsoft, for Google, for Facebook, for OpenAI.
[00:45:48.420 --> 00:45:54.740]   On the one hand, you have science really progressing and doing amazing things, but you have this
[00:45:54.740 --> 00:45:55.800]   problem.
[00:45:55.800 --> 00:45:58.040]   It's a problem for a lot of people, right?
[00:45:58.040 --> 00:45:59.800]   And some people don't see it as a problem.
[00:45:59.800 --> 00:46:05.540]   They just think we need to release this stuff and get over your issues with the bias and
[00:46:05.540 --> 00:46:06.860]   the hate speech.
[00:46:06.860 --> 00:46:09.220]   But a lot of people think it's a real problem.
[00:46:09.220 --> 00:46:16.300]   And to the extent where that clash is going to have to happen if those models are going
[00:46:16.300 --> 00:46:19.780]   to continue to progress and to get out in the world, right?
[00:46:19.780 --> 00:46:25.140]   You got to find a way to deal with it, whether it's technically or by other means, right?
[00:46:25.140 --> 00:46:30.400]   And that's why I think that that situation at Google is so important because it represents
[00:46:30.400 --> 00:46:32.840]   something much larger that's going on here.
[00:46:32.840 --> 00:46:37.960]   And it's something that the press is going to have to look at as well as all these companies.
[00:46:37.960 --> 00:46:38.960]   Okay.
[00:46:38.960 --> 00:46:39.960]   One more question.
[00:46:39.960 --> 00:46:45.960]   Why is it so easy to demo a thing that's evocative and so hard to turn that into a complete product
[00:46:45.960 --> 00:46:47.920]   that we engage with every day?
[00:46:47.920 --> 00:46:55.380]   I think it's just about aligning the technology with the need, okay?
[00:46:55.380 --> 00:46:59.140]   That OpenAI Rubik's Cube hand, right?
[00:46:59.140 --> 00:47:01.540]   That does not align with any need, right?
[00:47:01.540 --> 00:47:03.760]   We don't need that.
[00:47:03.760 --> 00:47:11.740]   The trick is finding where there's real gain and applying it, right?
[00:47:11.740 --> 00:47:17.800]   And I think that's where people often sort of miss the point, right?
[00:47:17.800 --> 00:47:26.600]   And these neural networks have worked and worked really well in particular areas, right?
[00:47:26.600 --> 00:47:29.840]   They don't work well in other areas.
[00:47:29.840 --> 00:47:35.960]   There's all this hype around AI and sort of remaking how your business operates, that
[00:47:35.960 --> 00:47:36.960]   sort of thing.
[00:47:36.960 --> 00:47:40.160]   But that's something different, right?
[00:47:40.160 --> 00:47:42.340]   There's not always an alignment.
[00:47:42.340 --> 00:47:45.140]   There's an alignment with that DeepMind result, right?
[00:47:45.140 --> 00:47:51.560]   That is something that is a real need and they're going after it.
[00:47:51.560 --> 00:47:54.420]   And in one sense, they solved it.
[00:47:54.420 --> 00:47:56.660]   There's still a lot of work to be done, but that's where-
[00:47:56.660 --> 00:47:58.620]   We're talking about protein folding, right?
[00:47:58.620 --> 00:47:59.660]   Protein folding, right?
[00:47:59.660 --> 00:48:01.220]   The CASP contest, right?
[00:48:01.220 --> 00:48:06.020]   That's something that the world needs and they're going after.
[00:48:06.020 --> 00:48:15.680]   GPT-3, it's not hard to be impressed by it, but it's really hard to see where that's going
[00:48:15.680 --> 00:48:19.800]   to have the practical application.
[00:48:19.800 --> 00:48:25.720]   When you find where it works, it becomes much easier to show people, right?
[00:48:25.720 --> 00:48:29.280]   I think the difficulty is often just sort of a misalignment, if that makes sense.
[00:48:29.280 --> 00:48:31.280]   Yeah, no, that totally makes sense.
[00:48:31.280 --> 00:48:32.280]   All right.
[00:48:32.280 --> 00:48:35.600]   Well, I think that's a good note to end on.
[00:48:35.600 --> 00:48:36.600]   Thank you so much.
[00:48:36.600 --> 00:48:37.600]   That was a lot of fun.
[00:48:37.600 --> 00:48:39.520]   Thanks for answering all my questions.
[00:48:39.520 --> 00:48:40.520]   Thank you.
[00:48:40.520 --> 00:48:45.040]   Glad to do it and really good talking to you as well.
[00:48:45.040 --> 00:48:46.600]   Yeah, real pleasure.
[00:48:46.600 --> 00:48:49.880]   Thanks for listening to another episode of Gradient Dissent.
[00:48:49.880 --> 00:48:54.120]   Doing these interviews are a lot of fun and it's especially fun for me when I can actually
[00:48:54.120 --> 00:48:56.880]   hear from the people that are listening to these episodes.
[00:48:56.880 --> 00:49:00.960]   So if you wouldn't mind leaving a comment and telling me what you think or starting
[00:49:00.960 --> 00:49:04.920]   a conversation, that would make me inspired to do more of these episodes.
[00:49:04.920 --> 00:49:08.520]   And also if you wouldn't mind liking and subscribing, I'd appreciate that a lot.

