
[00:00:00.000 --> 00:00:03.880]   Welcome, everybody to our study group. It's we're, we're talking
[00:00:03.880 --> 00:00:07.140]   about fast AI, Hugging Face, and it's been kindly hosted by the
[00:00:07.140 --> 00:00:10.520]   people over at Wade's Advices. I'm a fan of fast AI. So
[00:00:10.520 --> 00:00:13.940]   anything fast AI, I try to join it. That's probably my only
[00:00:13.940 --> 00:00:17.440]   contribution. And I'll just introduce Wade and that's my
[00:00:17.440 --> 00:00:20.640]   main contribution. But we'll be talking about the course. So
[00:00:20.640 --> 00:00:23.520]   Hugging Face for those of you don't know, Hugging Face has
[00:00:23.520 --> 00:00:26.360]   recently put out a course and we're trying to talk about
[00:00:27.080 --> 00:00:30.160]   recovering that course, the study group will go through it
[00:00:30.160 --> 00:00:34.080]   as well as we'll teach you how to integrate Hugging Face with
[00:00:34.080 --> 00:00:37.920]   fast AI and Wade and a few more people will be talking about it.
[00:00:37.920 --> 00:00:41.840]   Someone mentioned we're the Avengers. So guys, I'm calling
[00:00:41.840 --> 00:00:45.560]   dibs on Iron Man. You all can pick your favorite. I'm just
[00:00:45.560 --> 00:00:49.560]   kidding. But this group will be hosted by all of us. Wade will
[00:00:49.560 --> 00:00:55.480]   be leading today's call. Morgan, Zach, and Aman if you don't know
[00:00:55.480 --> 00:00:58.720]   these guys, they really the leaders of fast AI, if I may,
[00:00:58.720 --> 00:01:01.120]   the community leaders and they've been doing amazing
[00:01:01.120 --> 00:01:04.280]   contribution. My only contribution will be I'll keep
[00:01:04.280 --> 00:01:07.640]   fighting for chai and I'll try to help Wade with the questions
[00:01:07.640 --> 00:01:14.320]   today. So here's the rough agenda. We'll roughly go
[00:01:14.320 --> 00:01:18.440]   through one chapter every week when I say chapter. I just
[00:01:18.440 --> 00:01:20.880]   mentioned the Hugging Face course, we'll roughly try to
[00:01:20.880 --> 00:01:23.440]   cover one chapter. If not, that's fine. We'll get around
[00:01:23.440 --> 00:01:27.800]   to it. And also learn how to integrate fast AI with it. If
[00:01:27.800 --> 00:01:30.520]   you feel that you are new to these and if you feel
[00:01:30.520 --> 00:01:34.800]   intimidated, I'm a beginner intermediate across both of
[00:01:34.800 --> 00:01:37.960]   them. So as long as I can promise you as long as you're
[00:01:37.960 --> 00:01:40.400]   curious and you're interested, you're welcome. There's no
[00:01:40.400 --> 00:01:44.160]   gatekeeping, please keep joining and please keep asking the
[00:01:44.160 --> 00:01:47.000]   questions. We'll make sure that they're answered and everyone is
[00:01:47.000 --> 00:01:51.920]   welcome. So today's call, as I said is being today's session is
[00:01:51.920 --> 00:01:55.440]   being taught by Wade, Wade Gilliam. He's a senior software
[00:01:55.440 --> 00:01:59.960]   developer at UC San Diego. I've known him through the fast AI
[00:01:59.960 --> 00:02:02.640]   community. We were just talking about this, but we've seen each
[00:02:02.640 --> 00:02:05.520]   of those little icons on the forums and on Twitter for so
[00:02:05.520 --> 00:02:08.960]   many years. It's it's a very nice small and warm community,
[00:02:08.960 --> 00:02:11.840]   the fast AI community and Wade has been contributing a lot to
[00:02:11.840 --> 00:02:15.720]   it. He's best known as the creator of blur library, which
[00:02:15.720 --> 00:02:19.200]   as far as I know, was the first integration between the Hugging
[00:02:19.200 --> 00:02:23.280]   Face framework and fast AI. He's also a sci fi fan and
[00:02:23.280 --> 00:02:26.560]   unfortunately, just just kidding, but unfortunately, a
[00:02:26.560 --> 00:02:29.440]   coffee drinker. I won't take more of the call and I'll
[00:02:29.440 --> 00:02:31.920]   quickly hand over to Wade for the rest of the session.
[00:02:31.920 --> 00:02:36.040]   All right, let me share my screen. I just wanted to say I
[00:02:36.040 --> 00:02:39.080]   did not add the unhappy face after coffee.
[00:02:39.080 --> 00:02:44.120]   All right.
[00:02:44.120 --> 00:02:54.760]   Oops, everybody see my mouse cursor moving? Yeah, I can see.
[00:02:54.760 --> 00:02:57.480]   Okay, cool. Put that in presentation mode.
[00:02:57.480 --> 00:03:06.160]   Alright, so welcome. And I'm excited to be here and to see a
[00:03:06.160 --> 00:03:09.520]   lot of friendly faces from the fast AI community and maybe a
[00:03:09.520 --> 00:03:13.280]   few Hugging Face folks on here as well. I was excited when I
[00:03:13.280 --> 00:03:18.000]   put this, the idea up there on Twitter about a Hugging Face
[00:03:18.000 --> 00:03:20.880]   course, the fast AI I've been and I probably got the most
[00:03:20.880 --> 00:03:26.160]   responses out of anything I've ever put on Twitter. And in
[00:03:26.160 --> 00:03:30.040]   addition to myself, I'm glad to also be joined by the Avengers.
[00:03:30.040 --> 00:03:32.600]   I don't think I could be Captain America because I haven't really
[00:03:32.600 --> 00:03:35.800]   been lifting the weight. So that's probably Zack. But
[00:03:35.800 --> 00:03:40.120]   hopefully I can at least be Thor or at worst the Hulk. So
[00:03:40.120 --> 00:03:44.640]   there's a lot of people on this call. So if you have questions
[00:03:44.640 --> 00:03:49.520]   as we go through the material today, just feel free to put
[00:03:49.520 --> 00:03:52.360]   them on there. I'm going to stop periodically. Sanyam is going to
[00:03:52.360 --> 00:03:55.600]   be answering questions as well as hopefully other folks and
[00:03:55.600 --> 00:03:59.040]   if there's any outstanding questions, we'll answer them as
[00:03:59.040 --> 00:04:03.440]   we hit every individual section. And also thanks to the folks at
[00:04:03.440 --> 00:04:09.240]   Weights and Biases, Andrea and Morgan, who I also know from the
[00:04:09.360 --> 00:04:12.440]   fast AI community, I appreciate you all handing all the
[00:04:12.440 --> 00:04:18.120]   logistics and whatnot. So let's begin. So this week, we'll start
[00:04:18.120 --> 00:04:22.840]   by looking at section zero and one and section zero is pretty
[00:04:22.840 --> 00:04:27.360]   simple as the setup. And really, our goal here is to make sure
[00:04:27.360 --> 00:04:32.200]   that you can go through and run the examples in CoLab. And then
[00:04:32.200 --> 00:04:36.880]   the section one is probably going to be the least fast AI
[00:04:36.880 --> 00:04:40.640]   ish part of the study group since it's kind of just covering
[00:04:40.640 --> 00:04:44.440]   the basics of how the transformer architecture works,
[00:04:44.440 --> 00:04:50.360]   and what type of NLP tasks it's suited for and how they work.
[00:04:50.360 --> 00:04:55.160]   And so we're going to look at their high level API, which is
[00:04:55.160 --> 00:04:59.960]   the pipeline API. And we'll see how we can actually just start
[00:04:59.960 --> 00:05:04.000]   using models in Hugging Face within a few lines of code with
[00:05:04.000 --> 00:05:07.720]   raw text to get predictions for a variety of tasks. And then
[00:05:07.720 --> 00:05:11.440]   next week, we'll actually spend some time looking at Blur's high
[00:05:11.440 --> 00:05:16.760]   level API. And I cross it out because it's actually more for
[00:05:16.760 --> 00:05:19.320]   training than just inference, whereas the pipeline API, we're
[00:05:19.320 --> 00:05:21.560]   just looking at inference. So we're looking at pre trained
[00:05:21.560 --> 00:05:25.880]   models that are not pre trained models, but models that folks
[00:05:25.880 --> 00:05:28.720]   have put on the model hub, or Hugging Face is put up there are
[00:05:28.720 --> 00:05:31.720]   models that you've trained, how you can use the pipeline API to
[00:05:32.160 --> 00:05:36.360]   do inference on that. And then we'll also have some resources
[00:05:36.360 --> 00:05:40.280]   and homework, the homework is optional, but I hope you'll be
[00:05:40.280 --> 00:05:43.400]   excited to do it anyways, I guarantee if you do do the
[00:05:43.400 --> 00:05:47.800]   homework, it will help you understand so much more about
[00:05:47.800 --> 00:05:50.960]   how, how all the transformers work. And there's a lot of them.
[00:05:50.960 --> 00:05:53.680]   So when you go to the Hugging Face documentation, you've
[00:05:53.680 --> 00:05:57.240]   probably seen it used to be like, maybe 10 transformer
[00:05:57.240 --> 00:05:59.760]   architectures. Now I don't even know how many are on there. It's
[00:05:59.760 --> 00:06:03.400]   like 50 or it's just like an endless list you scroll through.
[00:06:03.400 --> 00:06:06.840]   And understanding the transformer architecture, the
[00:06:06.840 --> 00:06:09.520]   core architecture that all these things are based on will help
[00:06:09.520 --> 00:06:13.160]   you know which ones to at least narrow your choices down
[00:06:13.160 --> 00:06:17.600]   depending on your task and how things work. So we'll look at
[00:06:17.600 --> 00:06:20.760]   some homework there. And hopefully, everybody will do it.
[00:06:20.760 --> 00:06:26.800]   First off some resources. If you're here, you've obviously
[00:06:27.680 --> 00:06:30.200]   found resource number one. But if you want to share that with
[00:06:30.200 --> 00:06:33.640]   folks who may want to join in over the next four weeks, that's
[00:06:33.640 --> 00:06:37.000]   the link to the registration page for the study group. We
[00:06:37.000 --> 00:06:42.680]   also have a study group discord that's hosted by fast AI. And so
[00:06:42.680 --> 00:06:48.160]   the links here. In addition, I will share my slide deck. So
[00:06:48.160 --> 00:06:51.720]   don't feel the need to write all this stuff down real quickly.
[00:06:51.720 --> 00:06:56.240]   This will be shared and folks will be able to have these links
[00:06:56.920 --> 00:07:04.320]   at their disposal. Next, we have fast AI resources. And so I
[00:07:04.320 --> 00:07:07.880]   think people in the study group are variety of levels. Some have
[00:07:07.880 --> 00:07:11.040]   been doing fast AI for far too long, like myself, ever since it
[00:07:11.040 --> 00:07:15.280]   was taught in FIANO, believe it or not. And others are new to
[00:07:15.280 --> 00:07:20.280]   the course and are maybe doing the reading study group with
[00:07:20.280 --> 00:07:24.680]   Aman in the Waste Spices, fast books, reading a study group.
[00:07:25.040 --> 00:07:29.080]   But these are some really great resources is first the course,
[00:07:29.080 --> 00:07:33.320]   and it's free, it's online, absolutely the best deep
[00:07:33.320 --> 00:07:36.480]   learning course, if you want to get involved and learn how to
[00:07:36.480 --> 00:07:39.480]   actually apply deep learning and how it works and how to build
[00:07:39.480 --> 00:07:42.760]   models, how to put them in production. I think the fast AI
[00:07:42.760 --> 00:07:47.440]   course is the best. And also a tremendous community. And a
[00:07:47.440 --> 00:07:51.680]   testament to that are a lot of the folks that are on this call,
[00:07:51.880 --> 00:07:55.120]   most of them, probably all of them, except for Sonja, because
[00:07:55.120 --> 00:07:59.360]   of the coffee comment are probably smarter than me. And so
[00:07:59.360 --> 00:08:05.760]   a lot of great folks on here who have become exceptional machine
[00:08:05.760 --> 00:08:08.480]   learning and deep learning practitioners through the course
[00:08:08.480 --> 00:08:13.400]   over the last few years. Second is the walk with fast AI course
[00:08:13.400 --> 00:08:17.400]   that Zach has up there. And that's all free as well. And
[00:08:17.400 --> 00:08:20.480]   that's a great compendium to the course. And it answers a lot of
[00:08:20.480 --> 00:08:24.280]   questions and expands on some ideas that come up in the course
[00:08:24.280 --> 00:08:26.960]   but aren't necessarily covered in there. So that's a great
[00:08:26.960 --> 00:08:31.320]   resource as well. And then of course, the fast book and the
[00:08:31.320 --> 00:08:37.360]   fast book reading group. So you can get this book on line, you
[00:08:37.360 --> 00:08:41.720]   can purchase the hard copy, or Jeremy and Sullivan have made it
[00:08:41.720 --> 00:08:45.960]   available for free via Jupyter notebooks. And if you just do a
[00:08:45.960 --> 00:08:50.120]   Google search, you should be able to find that. And then of
[00:08:50.120 --> 00:08:53.280]   course, there's the fast book reading group and I participated
[00:08:53.280 --> 00:08:57.040]   I've been on there a few times just watching and observing and
[00:08:57.040 --> 00:09:00.280]   that's actually a great resource on does a really great job of
[00:09:00.280 --> 00:09:03.000]   going through all the chapters and I think his goals to go
[00:09:03.000 --> 00:09:07.160]   through the entire book. So check that out. In terms of
[00:09:07.160 --> 00:09:11.480]   fasting and hugging face libraries. If there's more, let
[00:09:11.480 --> 00:09:14.120]   us know and we'll add those. But these are the three that I'm
[00:09:14.120 --> 00:09:18.720]   familiar with. The first is adapt NLP. And that's by Novetta.
[00:09:18.720 --> 00:09:22.920]   And that's what Zack Mueller is heading up. And then really, I
[00:09:22.920 --> 00:09:25.880]   think it really wasn't me that started with blur is the fast
[00:09:25.880 --> 00:09:29.760]   hugs by Morgan. He was like the first one I think put fast hugs
[00:09:29.760 --> 00:09:36.040]   up there. Great name, by the way. And I think Ardo now is
[00:09:36.040 --> 00:09:41.440]   heading that up. But fast hugs is kind of like, at least for me
[00:09:41.440 --> 00:09:44.520]   was my initial inspiration for like, Oh, yeah, we can actually
[00:09:44.520 --> 00:09:48.160]   do something with fast AI. And not just with sequence
[00:09:48.160 --> 00:09:51.760]   classification, which is, is where I think facets was focused
[00:09:51.760 --> 00:09:54.640]   on initially, but actually started applying it to the other
[00:09:54.640 --> 00:09:58.360]   NLP tasks. So it you know, summarization, translation,
[00:09:58.360 --> 00:10:02.480]   everything. And so third is blurred. That's the library I
[00:10:02.480 --> 00:10:05.560]   created, I'm still working on it. I think it's like a zero,
[00:10:05.560 --> 00:10:13.200]   zero version 0.26 or something. I don't even know. But speaking
[00:10:13.200 --> 00:10:16.280]   for myself, I would love to get more people involved. So check
[00:10:16.280 --> 00:10:20.280]   it out. There's documentation, the codes there. You can email
[00:10:20.280 --> 00:10:23.080]   me, I would love to get people involved in helping build this
[00:10:23.080 --> 00:10:26.880]   out. And that would be a great way to also develop your
[00:10:26.880 --> 00:10:29.440]   understanding and learning of how transformers work and how
[00:10:29.440 --> 00:10:34.720]   fast AI works. Then ML and data science in general, we got the
[00:10:34.720 --> 00:10:41.400]   chai time data science podcast by Sanyam. And one of these days
[00:10:41.400 --> 00:10:46.360]   we maybe have the dark roasted coffee time data science
[00:10:46.360 --> 00:10:50.400]   podcast, but for right now it's chai. So but check that out. And
[00:10:50.400 --> 00:10:52.880]   Sanyam interviews tons of people in the community. And they're
[00:10:52.880 --> 00:10:57.400]   really great, really great insights into not only what's
[00:10:57.400 --> 00:11:00.840]   happening in the realm of machine learning, but how people
[00:11:00.840 --> 00:11:04.640]   got started, how they develop their careers and whatnot. So
[00:11:04.640 --> 00:11:07.840]   that's a great podcast. And then of course, we got weights and
[00:11:07.840 --> 00:11:12.720]   biases, who are hosting this. And for all things in terms of
[00:11:12.720 --> 00:11:15.760]   tracking your machine learning projects and whatnot, it's super
[00:11:15.760 --> 00:11:21.320]   easy to integrate their product into any type of machine
[00:11:21.320 --> 00:11:23.640]   learning or deep learning project you have. And they got
[00:11:23.640 --> 00:11:28.200]   tons of like, free content and whatnot. And there's folks on
[00:11:28.200 --> 00:11:32.040]   this call as well, if you want to get more information, I could
[00:11:32.040 --> 00:11:38.720]   provide that for you. So let's get started. So number one,
[00:11:38.720 --> 00:11:41.880]   setting up your environment, you'll notice that when you go
[00:11:41.880 --> 00:11:45.160]   through the course, there's a collab button at the top of a
[00:11:45.160 --> 00:11:48.440]   lot of the course pages. And you should be able to open that or
[00:11:48.440 --> 00:11:52.260]   click on that. And it will open that page with the code and
[00:11:52.260 --> 00:11:56.480]   collab. But I noticed that sometimes it's missing
[00:11:58.200 --> 00:12:03.320]   dependencies. And so if you want things to work, this is the
[00:12:03.320 --> 00:12:06.800]   right pip install right here that you want to include is you
[00:12:06.800 --> 00:12:10.200]   want to do transformers with sentence space. I think by
[00:12:10.200 --> 00:12:12.440]   default, it just does pip install transformers, and then
[00:12:12.440 --> 00:12:16.920]   some of the examples will break. So for collab, make sure that
[00:12:16.920 --> 00:12:19.200]   you use this pip install. And we'll actually look at a
[00:12:19.200 --> 00:12:23.160]   notebook, you can see how I set it up. And if you're going to
[00:12:23.160 --> 00:12:29.320]   local virtual environment front, I typically use conda or mini
[00:12:29.320 --> 00:12:34.320]   conda. And then Jeremy Howard recommended to several of us a
[00:12:34.320 --> 00:12:39.800]   product called Mamba that just makes conda work fast. So I've
[00:12:39.800 --> 00:12:42.920]   been using Mamba. And essentially, it's just a fill
[00:12:42.920 --> 00:12:45.600]   in for conda. So wherever you put conda install, you put Mamba
[00:12:45.600 --> 00:12:51.040]   install, and it is a lot faster. So check out those resources. If
[00:12:51.040 --> 00:12:56.120]   you want to go the local route. And then also, fast AI has a
[00:12:56.120 --> 00:13:01.560]   conda channel for installing all things fast AI. And so the link
[00:13:01.560 --> 00:13:05.680]   is right here. I've been using it for blur. And it does make
[00:13:05.680 --> 00:13:10.240]   things simple along with Mamba and really fast to keep updated
[00:13:10.240 --> 00:13:14.320]   with everything. And I'll also mention because there's a lot of
[00:13:14.320 --> 00:13:18.680]   discussion about using poetry to manage your environments. I've
[00:13:18.680 --> 00:13:21.960]   never used it. So if there's folks that are using poetry that
[00:13:21.960 --> 00:13:25.760]   like to share example with the group, I know, personally, I
[00:13:25.760 --> 00:13:30.040]   would love to see a poetry configuration set up with fast
[00:13:30.040 --> 00:13:34.040]   AI and hugging face to see how that works and the benefits. So
[00:13:34.040 --> 00:13:37.160]   I hear a lot about it, but don't know anything from personal
[00:13:37.160 --> 00:13:41.360]   experience. Is the questions coming up yet? Sonja?
[00:13:41.360 --> 00:13:47.600]   No, I don't see any questions. There was one broadly that asked
[00:13:47.640 --> 00:13:51.920]   that asked you should they be doing fast the hugging face API
[00:13:51.920 --> 00:13:54.080]   course or is it better to start from scratch?
[00:13:54.080 --> 00:13:59.160]   The the hugging face course,
[00:13:59.160 --> 00:14:03.680]   or hugging face course or just just learn from scratch? I think
[00:14:03.680 --> 00:14:04.440]   that's the question.
[00:14:04.440 --> 00:14:08.080]   I think the hugging face course is worth doing. And maybe even
[00:14:08.080 --> 00:14:11.960]   for this because, because, especially as we get into week
[00:14:11.960 --> 00:14:15.280]   three, we're going to be looking at more fast AI code. So we're
[00:14:15.280 --> 00:14:17.440]   going to deviate from the course content. So I think it's
[00:14:17.440 --> 00:14:21.280]   actually good if folks can do the course during the week, at
[00:14:21.280 --> 00:14:25.160]   least watch the videos and do a cursory reading so that they
[00:14:25.160 --> 00:14:27.320]   have the background for what we're going to be covering in
[00:14:27.320 --> 00:14:28.040]   the study group.
[00:14:28.040 --> 00:14:31.880]   Yep, no other questions. That's it.
[00:14:31.880 --> 00:14:37.080]   Alright, so essentially, the course that hugging faces put
[00:14:37.080 --> 00:14:40.880]   on it's really great. It has a lot of like, great folks with
[00:14:40.880 --> 00:14:44.760]   video content. They're actually doing some study groups as well.
[00:14:45.040 --> 00:14:47.920]   And the initial course, they've only introduced this part, the
[00:14:47.920 --> 00:14:53.400]   introduction. And so there's four sections on transformer
[00:14:53.400 --> 00:14:57.080]   models using transformers, fine tuning and sharing models and
[00:14:57.080 --> 00:15:02.200]   tokenizers. This is the only part of the course that is out
[00:15:02.200 --> 00:15:05.120]   now, I think the diving in portion comes in the fall and
[00:15:05.120 --> 00:15:09.400]   the advances either late this year or early next year. But
[00:15:09.400 --> 00:15:12.800]   we're going to see with this first four week session, there's
[00:15:12.800 --> 00:15:16.400]   so much that you could actually get going in terms of building
[00:15:16.400 --> 00:15:19.640]   transformers, using them in production, and also sharing
[00:15:19.640 --> 00:15:23.280]   them with the community through the hugging face hub. And in
[00:15:23.280 --> 00:15:25.720]   particular, over the next few weeks, we're going to show how
[00:15:25.720 --> 00:15:30.600]   to actually fine tune these models using blur, adapt NLP,
[00:15:30.600 --> 00:15:34.320]   fast hugs, and then how to take those models and actually share
[00:15:34.320 --> 00:15:38.560]   them on the hub, create a model cart, all that stuff. So I'm
[00:15:38.560 --> 00:15:42.720]   hoping by week four, a lot of folks are training models and
[00:15:42.720 --> 00:15:47.920]   sharing them on the hub and, and contributing to the community,
[00:15:47.920 --> 00:15:52.920]   it should be pretty straightforward. Alright, so the
[00:15:52.920 --> 00:15:56.560]   first section is we got natural language processing, essentially
[00:15:56.560 --> 00:15:59.800]   NLP is a field of linguistics and machine learning focus on
[00:15:59.800 --> 00:16:04.040]   understanding everything related to human language. And this is
[00:16:04.040 --> 00:16:10.280]   really a tough problem in terms of machine learning, because ml
[00:16:10.280 --> 00:16:15.560]   models only understand numbers. So how do we use numbers to
[00:16:15.560 --> 00:16:20.880]   create some type of system that can understand sentiment, or
[00:16:20.880 --> 00:16:25.280]   toxicity or summarize or answer questions. And so it's a really
[00:16:25.280 --> 00:16:30.280]   tough area in machine learning. And some of the the common tasks
[00:16:30.320 --> 00:16:34.480]   that we'll be going through over the next four weeks are sequence
[00:16:34.480 --> 00:16:37.720]   classification tasks. So that's where we're looking at whole
[00:16:37.720 --> 00:16:43.360]   documents, for example, movie reviews, or comments, for
[00:16:43.360 --> 00:16:47.320]   example, and we're trying to classify them, things like
[00:16:47.320 --> 00:16:50.960]   sentiment, whether it's spam, whether it's grammatically
[00:16:50.960 --> 00:16:55.280]   correct, whether two sentences are logically related. And we're
[00:16:55.280 --> 00:16:58.760]   also talking about multi label classification problems. So on
[00:16:58.760 --> 00:17:02.560]   Kaggle, there's a jigsaw toxicity challenge where there's,
[00:17:02.560 --> 00:17:06.040]   I think, seven or eight levels of toxicity. And you want to
[00:17:06.040 --> 00:17:12.080]   predict for every document, whether it actually has none,
[00:17:12.080 --> 00:17:15.200]   one or more of those labels associated or should be
[00:17:15.200 --> 00:17:19.960]   associated with it. Another task as common as token
[00:17:19.960 --> 00:17:24.600]   classification. And here, we're talking about things like named
[00:17:24.600 --> 00:17:28.640]   MC recognition, where we got a text document, and we want to
[00:17:28.640 --> 00:17:31.760]   find where all the people's names mentioned, where all the
[00:17:31.760 --> 00:17:34.480]   geographical locations mission where our organizations
[00:17:34.480 --> 00:17:37.760]   mentioned, we want to identify those entities. So we're looking
[00:17:37.760 --> 00:17:42.760]   there to identify and classify tokens. Also, we have text
[00:17:42.760 --> 00:17:47.520]   generation, which is you complete a, you have a prompt
[00:17:47.520 --> 00:17:50.720]   you start out with, and then it generates a bunch of text that
[00:17:50.720 --> 00:17:54.360]   sometimes makes sense and sometimes doesn't. And a great
[00:17:54.360 --> 00:17:57.520]   example of this is the new GitHub autopilot, which is
[00:17:57.520 --> 00:18:00.840]   actually doing this for code. And it's like scary how good it
[00:18:00.840 --> 00:18:04.920]   is really impressive stuff. And so that's looking there at text
[00:18:04.920 --> 00:18:08.440]   generation. And then another one that we'll also look at is
[00:18:08.440 --> 00:18:11.640]   extractive question and answering. And essentially,
[00:18:11.640 --> 00:18:15.000]   what you do is you feed the model a context like an article
[00:18:15.000 --> 00:18:20.840]   and a question that that article has somewhere in there the
[00:18:20.840 --> 00:18:24.360]   actual answer, and you want the model to provide that segment of
[00:18:24.360 --> 00:18:28.400]   text in the article or context, that is the correct answer. So
[00:18:28.400 --> 00:18:33.160]   these are really kind of common tasks. And deep learning is
[00:18:33.160 --> 00:18:36.960]   where the state of the art is. And there's a lot you can just
[00:18:36.960 --> 00:18:40.840]   accomplish in terms of, of these things using the transformer,
[00:18:40.840 --> 00:18:46.200]   or the transformers library from hugging face. So the transformers
[00:18:46.200 --> 00:18:49.240]   library provides functionality to create and use the shared
[00:18:49.240 --> 00:18:54.120]   models. The model hub contains 1000s of pre trained models that
[00:18:54.120 --> 00:18:57.880]   folks can download and use, we'll take a look at that. It's
[00:18:57.880 --> 00:19:00.960]   got all kinds of filters, you can filter by task by language,
[00:19:00.960 --> 00:19:07.760]   etc. And then right now, we'll take a look at the pipeline API.
[00:19:07.760 --> 00:19:12.240]   And essentially, the pipeline API connects a model with the
[00:19:12.240 --> 00:19:17.120]   necessary pre processing and post processing steps, allowing
[00:19:17.120 --> 00:19:21.240]   us to directly input any text and get an intelligible answer.
[00:19:21.840 --> 00:19:25.600]   And so what the pipeline does is it's going to take the raw text,
[00:19:25.600 --> 00:19:30.600]   it's going to do a process of tokenization that actually
[00:19:30.600 --> 00:19:33.520]   includes tokenization, which is splitting the text into
[00:19:33.520 --> 00:19:38.680]   individual words, and numerical ization, which is for each of
[00:19:38.680 --> 00:19:42.800]   those words, it has a vocab, which is essentially a lookup
[00:19:42.800 --> 00:19:48.640]   list. So it will convert every one of those words to a ID in
[00:19:48.640 --> 00:19:53.880]   that vocab list. And that is the numerical representation that
[00:19:53.880 --> 00:19:57.960]   we start with to feed the text into our models. And we'll look
[00:19:57.960 --> 00:20:02.200]   at how that works. Then it runs it through the those post
[00:20:02.200 --> 00:20:06.560]   processed inputs into the model. And then afterwards, we get
[00:20:06.560 --> 00:20:09.880]   predictions, it cleans those up so that they actually look
[00:20:09.880 --> 00:20:15.280]   understandable to us humans. So we'll take a look at how that
[00:20:15.280 --> 00:20:24.960]   works right now. And hopefully, this is big enough for folks. If
[00:20:24.960 --> 00:20:31.040]   not, let me know I can make my screen bigger. So in terms of
[00:20:31.040 --> 00:20:36.760]   installing in Colab, if you put this little dash qq, you won't
[00:20:36.760 --> 00:20:41.280]   get that all that output that usually just clutters your
[00:20:41.280 --> 00:20:46.040]   screen. So here, I'm actually installing both the
[00:20:46.040 --> 00:20:49.160]   Transformers library with sentence piece and also the
[00:20:49.160 --> 00:20:52.800]   data sets library. And I'm going to import the Transformers
[00:20:52.800 --> 00:20:56.640]   pipeline. And we were talking about a moment ago about
[00:20:56.640 --> 00:21:00.120]   tokenization. So I wanted to just go over that real briefly
[00:21:00.120 --> 00:21:04.600]   what that looks like. So in the first example up here, I'm going
[00:21:04.600 --> 00:21:10.840]   to use the BERT tokenizer. And I just wanted to show you that
[00:21:11.840 --> 00:21:16.120]   if you print out the type I see BERT tokenizer, right. And if I
[00:21:16.120 --> 00:21:18.800]   print tokenizer, you can actually see there's some really
[00:21:18.800 --> 00:21:22.120]   helpful information that HuggingFace has when you look at
[00:21:22.120 --> 00:21:26.920]   the string representation of that object, such as the vocab,
[00:21:26.920 --> 00:21:32.560]   which is how many tokens are in there, the max length, etc. But
[00:21:32.560 --> 00:21:36.920]   the recommended way to actually build HuggingFace objects,
[00:21:36.960 --> 00:21:42.240]   including the tokenizer is by using these auto models. So
[00:21:42.240 --> 00:21:46.760]   there's auto tokenizer, there's auto config, then there's auto
[00:21:46.760 --> 00:21:50.120]   model, there's auto model for task, and that's what they
[00:21:50.120 --> 00:21:54.120]   recommend. And it will automatically infer the correct
[00:21:54.120 --> 00:22:00.400]   objects based on the model name or path that you type in here.
[00:22:00.400 --> 00:22:06.760]   So you can see this code right here is this code right here is
[00:22:06.760 --> 00:22:10.240]   essentially the same except I'm using auto tokenizer. And it
[00:22:10.240 --> 00:22:15.040]   knows to create a BERT tokenizer. And you can see it's
[00:22:15.040 --> 00:22:18.080]   the same information as before. So when you're working with
[00:22:18.080 --> 00:22:23.040]   HuggingFace, you typically want to use these auto tokenizer,
[00:22:23.040 --> 00:22:26.560]   auto config, auto model for when building your HuggingFace
[00:22:26.560 --> 00:22:33.560]   objects. So as I said, when the tokenizer runs on raw text, it
[00:22:33.560 --> 00:22:37.400]   actually is doing more than tokenization. It's also doing
[00:22:37.400 --> 00:22:41.720]   numericalization, which is converting each of the tokens to
[00:22:41.720 --> 00:22:47.040]   an ID that can be looked up in a vocab. And so here I am
[00:22:47.040 --> 00:22:51.800]   tokenizing this text. And if we look at the type of inputs, it's
[00:22:51.800 --> 00:22:55.960]   a type batch encoding, we can convert that to a dictionary by
[00:22:55.960 --> 00:23:01.080]   calling dot data. And if we look at it, we can see that it's
[00:23:01.080 --> 00:23:05.280]   giving us a dictionary of a couple things. So first are the
[00:23:05.280 --> 00:23:10.320]   input IDs. So these are the actual indices into the vocab
[00:23:10.320 --> 00:23:17.120]   for the tokens. In the case of BERT, it's also giving us token
[00:23:17.120 --> 00:23:21.560]   type IDs. And that is because BERT was trained on a next
[00:23:21.560 --> 00:23:25.760]   sentence prediction task. So you had two sentences come in, and
[00:23:25.760 --> 00:23:29.960]   the token type IDs would be zero for the first sentence tokens,
[00:23:29.960 --> 00:23:32.600]   and then one for the second sentence tokens. So it's a way
[00:23:32.600 --> 00:23:35.160]   to different for the model to know to be able to differentiate
[00:23:35.160 --> 00:23:38.920]   between the two sentences. And then we'll talk about this a
[00:23:38.920 --> 00:23:43.560]   little bit more. But there's an attention mask. And this is the
[00:23:43.560 --> 00:23:47.280]   key capability of the transformer architecture is the
[00:23:47.280 --> 00:23:50.880]   ability to as it learns to represent each token to look at
[00:23:50.880 --> 00:23:55.080]   other tokens to to help it learn that representation. And the
[00:23:55.080 --> 00:23:58.880]   attention mask is essentially a way if it's one to say yes, pay
[00:23:58.880 --> 00:24:02.400]   attention to those tokens. If there's a zero within there's a
[00:24:02.400 --> 00:24:05.800]   zero right here. Essentially, it says don't pay attention to
[00:24:05.800 --> 00:24:08.720]   token zero when you're trying to figure out how to represent
[00:24:08.720 --> 00:24:13.600]   these other tokens, ignore that one. And if we look at that
[00:24:13.600 --> 00:24:16.280]   vocab that I was just talking about, here's what it looks
[00:24:16.280 --> 00:24:20.600]   like. Here's I just picked up the first five tokens. So
[00:24:20.600 --> 00:24:25.040]   there's the token, there's the ID. So if hey, was up here, we
[00:24:25.040 --> 00:24:32.800]   would actually see token 16164 in the input IDs right there.
[00:24:32.800 --> 00:24:38.720]   And once we put those ideas, we can actually decode them. And
[00:24:38.720 --> 00:24:43.600]   you can see that a couple of other special tokens have been
[00:24:43.600 --> 00:24:47.240]   added to the text by the bird tokenizer. There's a class token
[00:24:47.280 --> 00:24:56.000]   and a set token. And these are used by the model, right? And if
[00:24:56.000 --> 00:24:59.440]   you want to look in, so when you actually decode the input IDs,
[00:24:59.440 --> 00:25:03.480]   it's going to include those special tokens. And these will
[00:25:03.480 --> 00:25:06.000]   vary depending on what architecture you're actually
[00:25:06.000 --> 00:25:14.080]   using. And we can also look at the actual tokens that it that
[00:25:14.080 --> 00:25:19.240]   the IDs represent. And so for most tokenizers, you're going to
[00:25:19.240 --> 00:25:21.920]   see something like this, and you're going to see characters
[00:25:21.920 --> 00:25:25.240]   like this, because most tokenizers use something called
[00:25:25.240 --> 00:25:29.120]   subword tokenization. And so they have they, they by keeping
[00:25:29.120 --> 00:25:32.680]   a, they keep a relatively small vocab, which is important to
[00:25:32.680 --> 00:25:36.400]   model performance. And they also eliminate a lot of unknown
[00:25:36.400 --> 00:25:38.920]   tokens, which is bad, right? Because if you have an end
[00:25:38.920 --> 00:25:42.280]   tokens in there that are unknown, you can't really learn
[00:25:42.280 --> 00:25:47.480]   anything. And so it used it breaks things up into subwords
[00:25:47.480 --> 00:25:51.600]   for kind of rare words and very common words get their own token
[00:25:51.600 --> 00:25:55.120]   like names, a common word, Wade, especially with my spelling is
[00:25:55.120 --> 00:25:59.080]   not common. So it breaks it up with way then, and then two
[00:25:59.080 --> 00:26:05.160]   hashtags and D. So that's how the tokenizer works. Now, in
[00:26:05.160 --> 00:26:13.120]   terms of the pipeline, we can do a bunch of things with the
[00:26:13.120 --> 00:26:19.280]   models in the Hugging Face model hub. And just real quick, if you
[00:26:19.280 --> 00:26:25.720]   go to hugging face.co, and go to models, this is the hub right
[00:26:25.720 --> 00:26:31.080]   here. We're doing text classification. So you can click
[00:26:31.080 --> 00:26:34.520]   on that to filter by text classification, you can click
[00:26:34.520 --> 00:26:37.680]   one of these models. And it's going to give you a bunch of
[00:26:37.680 --> 00:26:41.360]   information about how it was trained, for example, even
[00:26:41.360 --> 00:26:44.680]   through the inference API, you can actually test it out right
[00:26:44.680 --> 00:26:50.840]   here. And if you want to, if you find a model that's trained for
[00:26:50.840 --> 00:26:55.200]   exactly what you're doing, you can actually just click on here
[00:26:55.200 --> 00:26:59.440]   using transformers, and copy and paste this code. And then you'll
[00:26:59.440 --> 00:27:02.800]   have everything you need in terms of the tokenizer and
[00:27:03.280 --> 00:27:08.400]   model. And probably the biggest thing to keep in mind is that
[00:27:08.400 --> 00:27:12.200]   when you're using the pipeline, these things aren't fine tuned
[00:27:12.200 --> 00:27:16.600]   on your particular tasks. So if you're using a model that's
[00:27:16.600 --> 00:27:19.840]   predicting two labels for sentiment, maybe positive or
[00:27:19.840 --> 00:27:23.040]   negative, and you're trying to train that on the toxicity of
[00:27:23.040 --> 00:27:25.320]   multi label problem, those models aren't going to work
[00:27:25.320 --> 00:27:28.600]   with the pipeline, you would actually have to fine tune that
[00:27:28.600 --> 00:27:32.360]   model on your particular task. That's something we'll look at
[00:27:32.400 --> 00:27:36.240]   in week three. But you can see it's pretty easy to actually
[00:27:36.240 --> 00:27:42.120]   just put the task, the raw test that the raw text and get
[00:27:42.120 --> 00:27:47.200]   results for it. And then if you're interested, you can also
[00:27:47.200 --> 00:27:51.960]   look at, you know, what is the model being used here. So you
[00:27:51.960 --> 00:27:58.680]   can see we just put a sentiment analysis, right? If we look at
[00:27:59.440 --> 00:28:03.560]   the classifier dot model name or path, we can actually see that
[00:28:03.560 --> 00:28:06.760]   the architecture or the checkpoint being used is the
[00:28:06.760 --> 00:28:11.280]   distilled BERT based on case fine tune SST to English
[00:28:11.280 --> 00:28:15.920]   architecture. And so if you're curious of, you know, what's
[00:28:15.920 --> 00:28:21.200]   being used, and as a side note, these distilled versions of BERT
[00:28:21.200 --> 00:28:24.920]   or Roberta are simply smaller versions of the original
[00:28:24.920 --> 00:28:29.920]   architecture. So they're faster, and they only suffer minimal
[00:28:29.920 --> 00:28:35.560]   performance reduction. So for depending on your constraints,
[00:28:35.560 --> 00:28:40.360]   like GPU or time constraints, these might be a really good
[00:28:40.360 --> 00:28:42.480]   option for experimentation, you might want to look at the
[00:28:42.480 --> 00:28:48.840]   distilled versions of BERT or Roberta or whatever. And then
[00:28:48.840 --> 00:28:53.200]   also, through the course, you learn that you can also say,
[00:28:53.240 --> 00:28:56.120]   yeah, I want this task. But let's say there's a specific
[00:28:56.120 --> 00:28:59.800]   model you want to use. So like here, let's say I want to use
[00:28:59.800 --> 00:29:06.000]   the this German sentiment, BERT model that this user right here
[00:29:06.000 --> 00:29:11.600]   created. And you can go ahead and run that. I don't know a lot
[00:29:11.600 --> 00:29:16.280]   of German, but I know that is good. And we can see that yes,
[00:29:16.280 --> 00:29:19.440]   that's a positive statement right there. And so you can
[00:29:19.440 --> 00:29:22.720]   actually go to the model hub, find a model that you want to
[00:29:22.720 --> 00:29:26.160]   work with, and use that instead of the default version that we
[00:29:26.160 --> 00:29:33.600]   saw above. There's also models for zero shot classification.
[00:29:33.600 --> 00:29:39.920]   And these are really interesting models where we actually in
[00:29:39.920 --> 00:29:45.640]   addition to saying, where we actually supply the labels, and
[00:29:45.640 --> 00:29:50.680]   it can still figure out, like, you know, whether or not it's
[00:29:50.680 --> 00:29:53.840]   one of these labels, even though we're defining these on the fly.
[00:29:53.840 --> 00:29:57.240]   I don't know a lot about how these particular models are
[00:29:57.240 --> 00:29:59.800]   trained, it definitely seems like a little bit of black
[00:29:59.800 --> 00:30:04.280]   magic. But as an example, we can pass in some text and tell it
[00:30:04.280 --> 00:30:07.240]   hey, these are the labels we're interested in. And it will try
[00:30:07.240 --> 00:30:10.600]   to infer whether this is about education, politics, and or
[00:30:10.600 --> 00:30:14.360]   business. And you can see here's our labels. And here's our
[00:30:14.360 --> 00:30:19.600]   scores. It got it right and, and, and classify this as being
[00:30:19.600 --> 00:30:25.600]   education. And then again, using that dot model dot main path,
[00:30:25.600 --> 00:30:30.600]   we can actually see what model hugging face is defaulting to
[00:30:30.600 --> 00:30:37.040]   for the, the pipeline API. And then always fun is the text
[00:30:37.040 --> 00:30:40.400]   generation. And so this is where you're going to supply a prompt,
[00:30:40.400 --> 00:30:44.600]   and then it's going to generate a text where he's going to
[00:30:44.600 --> 00:30:47.200]   continue. So here we start with in this course, we will teach
[00:30:47.200 --> 00:30:51.400]   you how to, and we see we will teach you how to get started
[00:30:51.400 --> 00:30:56.240]   using Docker as a tool to manage and whatever. Obviously, this is
[00:30:56.240 --> 00:30:58.920]   wrong. That's one of the dangers with text generation. But it is
[00:30:58.920 --> 00:31:03.800]   kind of interesting to see, you know what it actually produces.
[00:31:03.800 --> 00:31:09.160]   So you have that pipeline available to you as well. And
[00:31:09.160 --> 00:31:12.400]   also, there's a variety, you can see like, I'm not passing
[00:31:12.400 --> 00:31:15.120]   anything in here in terms of controlling the text
[00:31:15.120 --> 00:31:18.120]   generation. But you can there's a bunch of parameters you can
[00:31:18.120 --> 00:31:21.560]   pass into that. And so you can see here, we're doing text
[00:31:21.560 --> 00:31:26.240]   generation, we're using the distilled version of GPT to, and
[00:31:26.240 --> 00:31:29.640]   I'm passing in a max length, and also saying, hey, give me the
[00:31:29.640 --> 00:31:36.680]   two most likely predictions. And for a really good understanding
[00:31:36.680 --> 00:31:42.080]   of those particular parameters, make sure you check out this
[00:31:42.080 --> 00:31:47.080]   article from Hugging Face on how to generate, they go over all
[00:31:47.080 --> 00:31:50.720]   the different parameters, how they work, when you might want
[00:31:50.720 --> 00:31:54.480]   to use one over the other. So check that out, because there's
[00:31:54.480 --> 00:31:58.720]   a lot more to text generation than it may seem on the on the
[00:31:58.720 --> 00:32:03.960]   surface. And then of course, we got language modeling. So for
[00:32:03.960 --> 00:32:07.840]   like mass language modeling, we're we're saying predict,
[00:32:07.840 --> 00:32:11.240]   basically, it's like a fill in the blank type of type of thing.
[00:32:11.760 --> 00:32:17.520]   And so here, I'm returning a couple examples right here. So
[00:32:17.520 --> 00:32:21.000]   all about blank models about mathematical models,
[00:32:21.000 --> 00:32:25.000]   computation, computational models, and it gives you the
[00:32:25.000 --> 00:32:29.440]   score of here, I'm looking at the top two. And then we also
[00:32:29.440 --> 00:32:33.560]   have I mentioned, text classification, and this is
[00:32:33.560 --> 00:32:37.720]   covered in the course. But if you pass in grouped entities,
[00:32:37.720 --> 00:32:43.480]   remember, when we saw the tokenization of my name had w a
[00:32:43.480 --> 00:32:48.560]   y was one token, and then hash hash d was another token. And
[00:32:48.560 --> 00:32:51.920]   when we do name entity recognition, we'd rather it just
[00:32:51.920 --> 00:32:57.400]   say Wade is a person's name, not way and hash has, or d is a
[00:32:57.400 --> 00:33:00.760]   person's name, and group entities takes those sub tokens
[00:33:00.760 --> 00:33:03.720]   and puts them back together. So it reconstructs the original
[00:33:05.000 --> 00:33:10.920]   full word. And so you can see like, it's predicted using this
[00:33:10.920 --> 00:33:15.160]   model that Sylvan is a person's name, hugging face is
[00:33:15.160 --> 00:33:20.520]   organization, and Brooklyn is a location. If you want to see
[00:33:20.520 --> 00:33:25.840]   what group entities equals false does. This is actually just how
[00:33:25.840 --> 00:33:28.800]   a lot of times I code and learn things, I just change the values
[00:33:28.800 --> 00:33:33.160]   and see what happens. You can see that Sylvan's name is
[00:33:33.160 --> 00:33:40.080]   actually broken up into s hash hash y l hash hash va hash s i
[00:33:40.080 --> 00:33:43.920]   n. And they all have I person. And what the group entities
[00:33:43.920 --> 00:33:47.440]   basically did is just say no, that's really just Sylvan put
[00:33:47.440 --> 00:33:50.120]   those all together. And so that's a person. So that's what
[00:33:50.120 --> 00:33:56.560]   the group entities does. Question answering. If we supply
[00:33:56.560 --> 00:34:03.080]   question, and a context, it should pull out the answer
[00:34:03.080 --> 00:34:06.920]   within that text. So the answer is right here. And we can see
[00:34:06.920 --> 00:34:11.680]   that it actually does this with pretty high confidence. So some
[00:34:11.680 --> 00:34:15.360]   about 70%. It says this is where the start token is, this is
[00:34:15.360 --> 00:34:20.160]   where the end token is, that's extractive QA. summarizations
[00:34:20.160 --> 00:34:23.880]   pretty straightforward, take a big text document, summarize it
[00:34:23.880 --> 00:34:30.240]   in one or two sentences. And then we also have translation
[00:34:30.240 --> 00:34:33.240]   tasks, which is also pretty straightforward. You're going to
[00:34:33.240 --> 00:34:37.720]   translate from one language to another. So here, we're using
[00:34:37.720 --> 00:34:44.120]   Marian and T model, and going from French, French to English.
[00:34:44.120 --> 00:34:47.800]   And then we can see that this is the translation right here, this
[00:34:47.800 --> 00:34:51.960]   course is produced by Hugging Face. So again, the course kind
[00:34:51.960 --> 00:34:56.240]   of goes through these little bit more detail. But are there any
[00:34:56.240 --> 00:35:00.720]   questions so far on using the pipeline API or tokenization?
[00:35:00.720 --> 00:35:05.880]   This one question on tokenization by Ritu Brata. So
[00:35:05.880 --> 00:35:09.360]   he's asking how to gain the wisdom of which kind of
[00:35:09.360 --> 00:35:12.440]   tokenization to use for a particular architecture or
[00:35:12.440 --> 00:35:15.920]   datasets. For example, you should be using subword for
[00:35:15.920 --> 00:35:19.360]   German, Finnish and character level for Mandarin. But beyond
[00:35:19.360 --> 00:35:21.640]   that, how do you decide what to use?
[00:35:22.480 --> 00:35:29.120]   Well, so each architecture has basically their tokenization
[00:35:29.120 --> 00:35:31.600]   format they're using. And so you're really forced to use
[00:35:31.600 --> 00:35:34.600]   whatever that architecture is using. And you don't want to
[00:35:34.600 --> 00:35:39.240]   deviate because then you're not able to use models that were
[00:35:39.240 --> 00:35:42.040]   pre trained using that particular type of tokenization
[00:35:42.040 --> 00:35:46.520]   strategy. So probably the best thing to do is to look at the
[00:35:46.520 --> 00:35:55.120]   documentation. If you go to resources, and you can look at
[00:35:55.120 --> 00:35:58.480]   the tokenizers doc, and you can get more information about
[00:35:58.480 --> 00:36:02.400]   different strategies. If you look at the transformers
[00:36:02.400 --> 00:36:07.840]   documentation, you can actually see details about each of the
[00:36:07.840 --> 00:36:11.640]   architectures tokenizers. And you'll be able to discover like
[00:36:11.640 --> 00:36:14.440]   are they multilingual capable? How many languages were they
[00:36:14.440 --> 00:36:17.440]   trained on things like that. So that that would probably give
[00:36:17.440 --> 00:36:18.480]   you the guidance you need.
[00:36:18.480 --> 00:36:22.280]   Yeah, the docs are amazing. There are a few questions
[00:36:22.280 --> 00:36:25.360]   around the link. So after the study group is over, whenever
[00:36:25.360 --> 00:36:27.840]   the email goes out, we'll make sure the collab and all of the
[00:36:27.840 --> 00:36:31.160]   links are there. So make sure you join the faster discord and
[00:36:31.160 --> 00:36:35.240]   you can check out the notebook. One more question, I wish to
[00:36:35.240 --> 00:36:39.040]   include my labels or categories in a named entity recognition
[00:36:39.040 --> 00:36:41.840]   model, any guidance or pointers for that?
[00:36:42.760 --> 00:36:47.160]   So yeah, you can, you can actually create your own labels.
[00:36:47.160 --> 00:36:50.120]   And then what you could do is start with one of the pre
[00:36:50.120 --> 00:36:54.960]   trained models that was maybe trained on a subset, like a real
[00:36:54.960 --> 00:36:59.480]   common one is I think it's the German coal NNL or something
[00:36:59.480 --> 00:37:04.160]   data set, I forget what it is. And you can start with even even
[00:37:04.160 --> 00:37:08.080]   the data set. For example, I have one model, and any our
[00:37:08.080 --> 00:37:12.080]   model that I trained, I started with the German data set. And I
[00:37:12.080 --> 00:37:16.080]   fine tuned it on English. And it was amazing how good it turned
[00:37:16.080 --> 00:37:20.840]   out. And so in week three, we'll spend more time going through
[00:37:20.840 --> 00:37:23.840]   like picking a pre trained model and fine tuning it. But
[00:37:23.840 --> 00:37:26.280]   absolutely, you can go ahead and fine tune it for your own
[00:37:26.280 --> 00:37:28.560]   labels, even if the labels in the pre trained model are
[00:37:28.560 --> 00:37:29.080]   different.
[00:37:29.080 --> 00:37:33.880]   And I just like to nudge everyone towards you know,
[00:37:33.880 --> 00:37:36.520]   writing about this or just publishing your notebooks, use
[00:37:36.520 --> 00:37:40.320]   fast pages or any blog that you want. I'm sure we will talk
[00:37:40.320 --> 00:37:43.640]   about this, but at least I can speak for Wade and I all of this
[00:37:43.640 --> 00:37:46.560]   creates amazing career opportunities for you as well.
[00:37:46.560 --> 00:37:49.440]   So make sure you learn and also write about what you learn.
[00:37:49.440 --> 00:37:54.400]   Yeah, absolutely. I think Parul did something on weights and
[00:37:54.400 --> 00:37:57.080]   biases last week. And, and that's really kind of how she
[00:37:57.080 --> 00:37:59.240]   got started. I think she works at weights and biases. I'm not
[00:37:59.240 --> 00:38:00.520]   100% sure.
[00:38:00.520 --> 00:38:02.520]   But it works at h2o.ai.
[00:38:02.520 --> 00:38:07.880]   Oh, she does. She works at okay, yeah, h2o.ai. But she I think
[00:38:07.880 --> 00:38:12.080]   really got it got started in her career exploded because she was
[00:38:12.080 --> 00:38:16.480]   blogging, and then also giving lectures and things and whatnot.
[00:38:16.480 --> 00:38:19.000]   And so even stuff like this, when you teach people, and I
[00:38:19.000 --> 00:38:21.960]   think this is something that we've all learned from Jeremy
[00:38:21.960 --> 00:38:26.440]   Howard is that when you wherever you're at, you have learned
[00:38:26.440 --> 00:38:29.400]   something that someone is that you can teach someone behind you
[00:38:29.400 --> 00:38:32.440]   that hasn't had that content yet, even if you're just
[00:38:32.440 --> 00:38:36.360]   starting out. And a great way to develop your own understanding
[00:38:36.360 --> 00:38:40.040]   and also help others is to blog about it, or to do things like
[00:38:40.040 --> 00:38:42.400]   this to do a little zoom sessions and study groups and,
[00:38:42.400 --> 00:38:46.760]   and, and teach others. So yeah, that's, that's great advice.
[00:38:46.760 --> 00:38:49.320]   Anything else?
[00:38:49.320 --> 00:38:51.000]   No questions. No other questions.
[00:38:51.000 --> 00:38:55.000]   All right. All right. So let's go back to the
[00:38:55.000 --> 00:39:02.160]   presentation here. So we've gone through the pipeline API, we've
[00:39:02.160 --> 00:39:06.680]   also seen how we can go to the Hugging Face model hub, and use
[00:39:06.680 --> 00:39:09.880]   the inference API there, right on the web, which is really
[00:39:09.880 --> 00:39:18.760]   cool. So transformers, so the big thing in section one is
[00:39:18.760 --> 00:39:21.480]   really like how do transformers work? What does the architecture
[00:39:21.480 --> 00:39:25.240]   look like? And you're not going to have a complete
[00:39:25.240 --> 00:39:28.600]   understanding just by going through the study group or by
[00:39:28.600 --> 00:39:32.080]   going through the course, I'm going to have some links to some
[00:39:32.080 --> 00:39:35.320]   papers and to some articles that you want to read. But
[00:39:35.320 --> 00:39:38.080]   understanding how the transformer architecture works,
[00:39:38.080 --> 00:39:41.440]   the the initial transformer architecture, everything you see
[00:39:41.440 --> 00:39:44.240]   it as a transformer model, Hugging Face is based on it. So
[00:39:44.240 --> 00:39:47.080]   if you want to understand like how it works, you need to
[00:39:47.080 --> 00:39:50.840]   understand the transformer architecture. And essentially,
[00:39:50.840 --> 00:39:57.240]   all transformer models have been trained as language models. So a
[00:39:57.240 --> 00:40:01.520]   language model and the reason why this is key is that a
[00:40:01.520 --> 00:40:04.840]   language model is essentially, you're either trying to predict
[00:40:04.840 --> 00:40:12.920]   the next word given a word and its preceding words, like in the
[00:40:12.920 --> 00:40:16.080]   causal language model, or you have a mass language model where
[00:40:16.080 --> 00:40:19.280]   you've got text and you're trying to figure out what these
[00:40:19.280 --> 00:40:23.400]   mass tokens or corrupted tokens really should refer to. And the
[00:40:23.400 --> 00:40:26.120]   idea of a language model is that if you can build a model that
[00:40:26.120 --> 00:40:30.040]   does either one of these tasks really well, then you've created
[00:40:30.160 --> 00:40:34.720]   a model that understands the particular grammar of the text
[00:40:34.720 --> 00:40:38.720]   that it's saying. And a model that understands has a good
[00:40:38.720 --> 00:40:43.400]   understanding of the text is a model that you can then apply to
[00:40:43.400 --> 00:40:46.640]   downstream tasks related to that text. So it's figuring out
[00:40:46.640 --> 00:40:50.480]   sentiment or figuring out entities or figuring out answers
[00:40:50.480 --> 00:40:53.600]   in there. And so that's why language model modeling is so
[00:40:53.600 --> 00:40:57.240]   critical. And all these transformers and other
[00:40:57.240 --> 00:41:02.800]   architectures like ULM fit and LSTMs, why they're all based on
[00:41:02.800 --> 00:41:06.160]   language models is because they start with that intuitive idea
[00:41:06.160 --> 00:41:08.680]   that yeah, if I have a model that can do this really well,
[00:41:08.680 --> 00:41:14.280]   and predict the next token or predict mass tokens, that I have
[00:41:14.280 --> 00:41:17.240]   something that understands the particular grammar, the
[00:41:17.240 --> 00:41:20.960]   particular language that it's trained on. So that's the whole
[00:41:20.960 --> 00:41:25.280]   idea. And so you take these language models, and a lot of
[00:41:25.280 --> 00:41:31.200]   them have been trained on a lot of text on a lot of GPUs or TPUs
[00:41:31.200 --> 00:41:37.040]   over many days at a lot of cost, things that at least I can't do
[00:41:37.040 --> 00:41:43.320]   with my 1080 TI GPU, and my little DL rig here at the house.
[00:41:43.320 --> 00:41:47.720]   And it's, and so these, these folks have pre trained these big
[00:41:47.720 --> 00:41:51.760]   models, and we can take what they've learned, the weights
[00:41:51.760 --> 00:41:55.640]   that is that is that's learned through that process. And we can
[00:41:55.640 --> 00:41:58.160]   go ahead and grab those models. And through a process called
[00:41:58.160 --> 00:42:00.800]   transfer learning, we can take those pre trained language
[00:42:00.800 --> 00:42:05.800]   models and fine tune them on our particular text. And that's
[00:42:05.800 --> 00:42:09.640]   really where this comes in handy. If you're going through
[00:42:09.640 --> 00:42:12.360]   the fast AI course or about to you'll see that transfer
[00:42:12.360 --> 00:42:16.920]   learning is the thing and fast AI, because most folks just
[00:42:16.920 --> 00:42:20.680]   don't have the ability to go through billions of Wikipedia
[00:42:20.720 --> 00:42:25.520]   articles, and spend 10s or hundreds of 1000s of dollars in
[00:42:25.520 --> 00:42:28.800]   compute costs to build these things. But fortunately, big
[00:42:28.800 --> 00:42:31.720]   companies like Microsoft, open AI and whatnot, they've actually
[00:42:31.720 --> 00:42:35.360]   done that for us. And we can use those models on our own on our
[00:42:35.360 --> 00:42:41.920]   limited resources for our particular raw text and for our
[00:42:41.920 --> 00:42:47.080]   particular constraints. So how do transformers work? So at a
[00:42:47.080 --> 00:42:52.240]   high level, there's two parts, there's an encoder part that our
[00:42:52.240 --> 00:42:55.200]   inputs are going to come through. And this part right
[00:42:55.200 --> 00:42:59.400]   here is optimized to acquire understanding from the input.
[00:42:59.400 --> 00:43:05.000]   And then there is this in it, and also it converts text to
[00:43:05.000 --> 00:43:09.200]   numerical representations. So essentially, each token is going
[00:43:09.200 --> 00:43:13.720]   to get an embedding, which is a higher dimensional numerical
[00:43:13.720 --> 00:43:17.160]   representation that's going to be learned. So we're going to
[00:43:17.160 --> 00:43:21.520]   learn a high dimensional representation for every single
[00:43:21.520 --> 00:43:27.080]   token. And they go through a couple layers. And probably the
[00:43:27.080 --> 00:43:29.680]   most important is the self attention layer, which we'll
[00:43:29.680 --> 00:43:33.640]   look at in a second. And what it outputs is a high level
[00:43:33.640 --> 00:43:38.680]   representation of those inputs. Then what it does, and remember
[00:43:38.680 --> 00:43:40.880]   that the transformer architecture was originally
[00:43:40.880 --> 00:43:44.760]   built to handle translation. And so the inputs would be, for
[00:43:44.760 --> 00:43:51.160]   example, German text, and the outputs would be English text,
[00:43:51.160 --> 00:43:55.200]   so the English translation. So that's what happens in the
[00:43:55.200 --> 00:44:00.320]   encoder, the decoder during training, we will actually feed
[00:44:00.320 --> 00:44:07.120]   in the the, the correct late, the correct tokens, but we use
[00:44:07.120 --> 00:44:11.600]   something in mass language model called a mass self attention
[00:44:11.600 --> 00:44:17.640]   mass to make sure that tokens can't look at subsequent tokens
[00:44:17.640 --> 00:44:22.800]   for building a representation of itself. And we'll take a look at
[00:44:22.800 --> 00:44:27.440]   that as well, a little bit more detail. But here, the encoder is
[00:44:27.440 --> 00:44:30.600]   focused on understanding the input, the decoder is really
[00:44:30.600 --> 00:44:35.760]   focused on generating the output. So generating a high
[00:44:35.760 --> 00:44:39.120]   level representation of that output that we can use for
[00:44:39.120 --> 00:44:43.040]   predicting the next text, and then also for things like being
[00:44:43.040 --> 00:44:48.440]   able to generate a classification prediction. Now,
[00:44:48.440 --> 00:44:50.840]   the biggest thing that we mentioned is this idea of
[00:44:50.840 --> 00:44:56.120]   attention. And essentially, there's three different types of
[00:44:56.120 --> 00:45:00.840]   attention layers in the original transformer architecture. And a
[00:45:00.840 --> 00:45:03.720]   high level definition would be that this layer will tell the
[00:45:03.720 --> 00:45:06.840]   model to pay specific attention to certain words in the
[00:45:06.840 --> 00:45:10.360]   sentence, you passed it, and more or less ignore others when
[00:45:10.360 --> 00:45:14.080]   dealing with the representation of each word. A word by itself
[00:45:14.080 --> 00:45:16.640]   has a meaning that that meaning is deeply affected by the
[00:45:16.640 --> 00:45:21.400]   context, which can be any other word or words before or after
[00:45:21.400 --> 00:45:24.520]   the word being studied. And so this is actually pretty
[00:45:24.520 --> 00:45:28.800]   intuitive, right, that when we're looking at a sentence, the
[00:45:28.800 --> 00:45:32.160]   meaning of each word in that sentence is going to be affected
[00:45:32.160 --> 00:45:36.160]   to one degree or another by words around it. And so this is
[00:45:36.160 --> 00:45:40.080]   an example from Jay Alomar's, the illustrated transformer
[00:45:40.080 --> 00:45:44.480]   article, and the animal didn't cross the street because it was
[00:45:44.480 --> 00:45:48.960]   too tired. And he asked the question, what does it in this
[00:45:48.960 --> 00:45:53.120]   sentence referred to? Is it referring to the street? Or is
[00:45:53.120 --> 00:45:57.600]   it referring to the animal? And as we read this, it's pretty
[00:45:57.600 --> 00:46:02.480]   simple, right? We can understand what it means. But a model is,
[00:46:02.480 --> 00:46:06.120]   you know, computers are pretty dumb. So by using this thing
[00:46:06.120 --> 00:46:10.640]   called attention, we can have it when it's creating, encoding a
[00:46:10.640 --> 00:46:14.720]   representation for it, it can actually learn to pay attention
[00:46:14.720 --> 00:46:21.000]   to the right word, such as animal, whoops, a little too
[00:46:21.000 --> 00:46:27.840]   far there. And so that's the whole idea of how attention
[00:46:27.840 --> 00:46:31.720]   works is basically allowing us to look at words around each
[00:46:31.720 --> 00:46:35.920]   other word to develop a representation of it. So that we
[00:46:35.920 --> 00:46:41.080]   can understand a word's meaning in context. And this is what the
[00:46:41.080 --> 00:46:45.840]   original transformer architecture looks like. And
[00:46:45.840 --> 00:46:48.960]   this will be part of the homework for those who choose to
[00:46:48.960 --> 00:46:52.920]   do it is to actually read these papers, you might be intimidated
[00:46:52.920 --> 00:46:55.200]   and think, I'm not gonna be able to understand any of this, I
[00:46:55.200 --> 00:46:58.200]   guarantee it, you will be able to understand the attention is
[00:46:58.200 --> 00:47:02.040]   all you need. And I've read it probably several times, I still
[00:47:02.040 --> 00:47:05.560]   go back and look at it. Because it's really informative about
[00:47:05.560 --> 00:47:08.200]   not just the transformer architecture, but how to apply
[00:47:08.200 --> 00:47:12.680]   it to different tasks. So this will also be included in the
[00:47:12.680 --> 00:47:15.960]   slide deck that we send out to everybody. And if you're
[00:47:15.960 --> 00:47:22.120]   confused at all by any of the the concepts in the paper, read
[00:47:22.120 --> 00:47:26.600]   alongside the illustrated transformer by Jay. And he's got
[00:47:26.600 --> 00:47:32.200]   all kinds of great animations and graphics that explain in
[00:47:32.200 --> 00:47:36.440]   detail how attention works. And literally probably my favorite
[00:47:36.440 --> 00:47:39.960]   resource in terms of actually all his stuff is really good.
[00:47:39.960 --> 00:47:46.920]   He's got illustrated guides to the transformer to Bert, a
[00:47:46.920 --> 00:47:53.600]   bunch of stuff, great resources to just to keep that in mind. So
[00:47:53.600 --> 00:47:56.320]   the transformer architecture, as I said, there's really three
[00:47:56.320 --> 00:48:02.760]   types of attention. And right here, don't worry so much right
[00:48:02.760 --> 00:48:05.960]   now what multi head attention means. But here we have
[00:48:06.000 --> 00:48:10.320]   bidirectional attention. And so when it's encoding the inputs,
[00:48:10.320 --> 00:48:14.560]   the attention layer is allowed to look at the preceding and
[00:48:14.560 --> 00:48:18.040]   subsequent tokens and figuring out a representation for each
[00:48:18.040 --> 00:48:22.520]   token that's passed through. On the decoder side, we have mass
[00:48:22.520 --> 00:48:27.000]   attention. And as I said before, the idea there is to keep in
[00:48:27.000 --> 00:48:30.920]   mind is that there's a mask. And remember, we saw that attention
[00:48:30.920 --> 00:48:33.520]   mask when we looked at the tokenizer produce that tension
[00:48:33.520 --> 00:48:38.040]   mass. Well, here, the intent, there's an attention mask, and
[00:48:38.040 --> 00:48:42.400]   it essentially ensures that as we are looking during training,
[00:48:42.400 --> 00:48:47.240]   and we're looking at our target text that we're trying to
[00:48:47.240 --> 00:48:52.960]   replicate in the form of predictions, it's essentially
[00:48:52.960 --> 00:48:56.800]   masking subsequent words. So it's not able to look at that,
[00:48:56.800 --> 00:48:59.800]   because when we do inference is going to generate one word at a
[00:48:59.800 --> 00:49:02.760]   time. So it's going to generate word one, and then it's going to
[00:49:02.760 --> 00:49:05.960]   try to predict two from word one, then having word two
[00:49:05.960 --> 00:49:09.000]   predicted, it's going to try to predict word three from word one
[00:49:09.000 --> 00:49:12.920]   and two. So when we're training, we have to mask those future
[00:49:12.920 --> 00:49:17.600]   words out. And then the third form of self attention is
[00:49:17.600 --> 00:49:24.600]   encoder attention. So we're actually going to take the the
[00:49:24.600 --> 00:49:27.280]   representations of each of the tokens that are learned on the
[00:49:27.280 --> 00:49:32.360]   encoder side. And we're going to let the decoder learn how to pay
[00:49:32.360 --> 00:49:36.600]   attention to those outputs as well. And those are really so
[00:49:36.600 --> 00:49:39.320]   those, they're all using a form of attention, there's three of
[00:49:39.320 --> 00:49:42.000]   them. And so that's really the key concept that has made
[00:49:42.000 --> 00:49:49.680]   transformers super popular. So when we go look at hugging face,
[00:49:49.680 --> 00:49:53.040]   we'll see that some transformers are just encoder models. So
[00:49:53.040 --> 00:49:56.440]   they're just the actually any questions coming up yet, Sanyam?
[00:49:56.440 --> 00:50:01.200]   This is a follow up question from earlier. So for the
[00:50:01.200 --> 00:50:04.400]   tokenization strategy, strategy, they were asking about if
[00:50:04.400 --> 00:50:07.400]   they're working on a novel architecture, which approach
[00:50:07.400 --> 00:50:10.560]   should they use for tokenization?
[00:50:10.560 --> 00:50:17.480]   For tokenization? I not sure how to answer that I, if they're
[00:50:17.480 --> 00:50:22.160]   working for a novel approach. Yeah, I don't really know. I
[00:50:22.160 --> 00:50:25.920]   don't know what the best answer to that would be, I would say
[00:50:25.920 --> 00:50:29.640]   post post that on the hugging face forums, and which are
[00:50:29.760 --> 00:50:33.720]   staffed by, you know, all the ML engineers over there and see
[00:50:33.720 --> 00:50:35.160]   what they say in terms of start now.
[00:50:35.160 --> 00:50:39.120]   Apart from that, we're just sharing resources and just
[00:50:39.120 --> 00:50:42.720]   talking about different blog posts around understanding
[00:50:42.720 --> 00:50:45.160]   attention and transformers.
[00:50:45.160 --> 00:50:50.200]   Yeah, anything else? Or is that is that a question?
[00:50:50.200 --> 00:50:51.400]   No, that's it.
[00:50:51.400 --> 00:50:57.040]   Okay. All right. So when you look at transformer models, are
[00:50:57.040 --> 00:50:59.640]   you looking at the transformers library should say,
[00:50:59.960 --> 00:51:03.360]   you'll see that some are encoder only. So they just include that
[00:51:03.360 --> 00:51:07.160]   encoder part that we looked at right here. So they just include
[00:51:07.160 --> 00:51:11.400]   this part. Others are decoder specific. And there's other ones
[00:51:11.400 --> 00:51:15.200]   that are sequences sequence that include both. And just real
[00:51:15.200 --> 00:51:15.920]   briefly,
[00:51:15.920 --> 00:51:20.400]   I'll include some, here's some examples of encoder only ones.
[00:51:20.400 --> 00:51:23.400]   And what I've had really good success with Roberta, if you're
[00:51:23.400 --> 00:51:25.840]   just looking to start something out, try the Roberta
[00:51:25.840 --> 00:51:29.400]   architecture or the distilled version. But essentially, these
[00:51:29.400 --> 00:51:33.240]   are really good for natural language understanding tasks. So
[00:51:33.240 --> 00:51:36.640]   if your task is sequence classification, token
[00:51:36.640 --> 00:51:41.200]   classification, extractive QA, a mass language model, and you're
[00:51:41.200 --> 00:51:44.120]   looking to narrow down your options, think about starting
[00:51:44.120 --> 00:51:49.520]   with an encoder model and seeing how that works. decoder models,
[00:51:49.520 --> 00:51:55.120]   again, which is this part right here, focus on text generation.
[00:51:55.600 --> 00:51:59.600]   So if you're looking to do something like just generating
[00:51:59.600 --> 00:52:03.680]   dynamic text, there's decoder only models you might want to
[00:52:03.680 --> 00:52:07.880]   pay attention to probably the most famous is GPT, GPT, two.
[00:52:07.880 --> 00:52:12.320]   And then a third type of transformer are sequence to
[00:52:12.320 --> 00:52:18.040]   sequence models, which really follow the the the transformer
[00:52:18.040 --> 00:52:20.800]   architecture as a whole, there's the they use the encoder and
[00:52:20.800 --> 00:52:25.400]   decoder. And these are good where you have tasks where the
[00:52:25.400 --> 00:52:30.080]   input length is independent of the output length. So you know,
[00:52:30.080 --> 00:52:32.160]   when you translate a lot of times you might have five
[00:52:32.160 --> 00:52:35.800]   English words, but it may only be three German words, right.
[00:52:35.800 --> 00:52:39.560]   And so they're not going to be the same. And it's also good to
[00:52:39.560 --> 00:52:44.280]   use where the output is highly dependent on the input. And so
[00:52:44.280 --> 00:52:47.120]   this is good for tasks like summarization, right, where you
[00:52:47.120 --> 00:52:50.600]   have a big article and you want to accurately summarize in a
[00:52:50.640 --> 00:52:54.720]   sentence or two, or in translation, or also in
[00:52:54.720 --> 00:52:59.000]   generative QA, where instead of extracting the answer, we're
[00:52:59.000 --> 00:53:03.200]   asking the model to actually just generate the answer. And,
[00:53:03.200 --> 00:53:06.840]   and that's what that is. That's the difference there. And in
[00:53:06.840 --> 00:53:09.240]   terms of sequence of sequence models, I've had a really good
[00:53:09.240 --> 00:53:14.480]   success on summarization using BART, and for and I think
[00:53:14.480 --> 00:53:18.160]   Pegasus, and then on translation, the Marian MT
[00:53:18.200 --> 00:53:21.320]   models have proven to be really well if you're looking for
[00:53:21.320 --> 00:53:26.520]   architectures to start out with. Any question on any of those
[00:53:26.520 --> 00:53:28.520]   three forms of transformers?
[00:53:28.520 --> 00:53:33.080]   No, I don't see anything yet.
[00:53:33.080 --> 00:53:37.800]   And then the last part of this section is bias and
[00:53:37.800 --> 00:53:41.320]   limitations. It's just something to be aware of when you're
[00:53:41.320 --> 00:53:44.200]   building models, especially when you're using pre trained models
[00:53:44.200 --> 00:53:47.800]   that others have made, is that more than likely, there's going
[00:53:47.800 --> 00:53:52.240]   to be some form of bias in the data as a result, or in the
[00:53:52.240 --> 00:53:55.480]   model based on the data was trained on or even the metric it
[00:53:55.480 --> 00:54:01.240]   was optimized for. And if you look at the, the model hub, a
[00:54:01.240 --> 00:54:04.840]   lot of those models will actually have that as something
[00:54:04.840 --> 00:54:08.920]   they includes, which is really nice, right? Because in terms of
[00:54:08.920 --> 00:54:12.960]   mitigating negative effects of this knowledge is, you know,
[00:54:12.960 --> 00:54:16.760]   probably 80% of it, because you have that information, you can
[00:54:16.760 --> 00:54:21.760]   make your end users aware of it, etc. If you're looking for more
[00:54:21.760 --> 00:54:27.360]   information about how, basically where bias comes from, and also
[00:54:27.360 --> 00:54:33.040]   other mitigation strategies. Rachel does a really good job in
[00:54:33.040 --> 00:54:37.440]   chapter three of fastbook to going into this in more detail.
[00:54:37.440 --> 00:54:45.520]   So feel free to check that out as another resource. And any
[00:54:45.520 --> 00:54:48.720]   questions on any of the material? Because that's pretty
[00:54:48.720 --> 00:54:50.320]   much it for section one.
[00:54:50.320 --> 00:54:54.600]   So there's a question around is zero short learning just
[00:54:54.600 --> 00:54:57.080]   encoded? Or is it an encoded decoder module?
[00:54:57.080 --> 00:54:59.920]   I'll say that again, Sonia, I didn't hear all that.
[00:54:59.920 --> 00:55:04.680]   Sorry. So is zero short learning just using an encoder model? Or
[00:55:04.680 --> 00:55:05.920]   is it an encoded decoder?
[00:55:07.160 --> 00:55:16.520]   Let's take a look here. Okay, let's see, did I actually pull
[00:55:16.520 --> 00:55:17.560]   that up there?
[00:55:17.560 --> 00:55:23.040]   tokenizer.
[00:55:23.040 --> 00:55:31.600]   So it's actually using a sequence to sequence. So it's
[00:55:31.600 --> 00:55:36.560]   using BART for that particular task. And so you'll see that
[00:55:36.560 --> 00:55:42.760]   even these sequence to sequence models like BART, they actually
[00:55:42.760 --> 00:55:47.520]   are, there's checkpoints available for classification
[00:55:47.520 --> 00:55:51.000]   tasks. And I'm not exactly sure they might actually discard
[00:55:51.000 --> 00:55:54.880]   parts, I'm actually sure how they use the decoder in those
[00:55:54.880 --> 00:55:56.960]   particular architectures, but those can be used on
[00:55:56.960 --> 00:56:02.160]   classification tasks. And they either probably alter the head
[00:56:02.160 --> 00:56:05.800]   of the decoder is my guess. And they're actually really good
[00:56:05.800 --> 00:56:08.400]   too. So I've actually used BART for classification tasks got
[00:56:08.400 --> 00:56:12.080]   really great results on on that as well. So again, when you look
[00:56:12.080 --> 00:56:17.600]   at these pipeline API, if you after building it do dot model
[00:56:17.600 --> 00:56:21.400]   name or path, you can see the architecture or the checkpoint
[00:56:21.400 --> 00:56:27.760]   that the pipeline model is based on. And then if you go to the
[00:56:27.760 --> 00:56:31.040]   hugging face transformers docs, look up BART, and you can see
[00:56:31.040 --> 00:56:34.200]   exactly how that works, you can actually see the code for the
[00:56:34.200 --> 00:56:37.760]   different checkpoints. And to see really what's going on.
[00:56:37.760 --> 00:56:44.720]   Awesome. So the next question is most language model use next
[00:56:44.720 --> 00:56:48.120]   prediction as their objective. How do you come up with an
[00:56:48.120 --> 00:56:51.480]   objective like mass language models? Any intuition behind
[00:56:51.480 --> 00:56:51.760]   that?
[00:56:51.760 --> 00:56:57.160]   Yeah, so you probably a good another paper to start with, if
[00:56:57.160 --> 00:57:02.160]   you want to kind of learn different strategies, and how
[00:57:02.160 --> 00:57:08.040]   that works is, in terms of mass language models, BERT probably
[00:57:08.040 --> 00:57:11.640]   is the most famous and kind of the foundation for everything.
[00:57:11.640 --> 00:57:16.240]   And they take an approach of dynamically masking every batch
[00:57:16.240 --> 00:57:21.280]   15% of the tokens, and a certain percent are replaced with the
[00:57:21.280 --> 00:57:26.800]   mass token, another percent is replaced with just a wrong token,
[00:57:26.800 --> 00:57:30.480]   and then another percentage actually keeps the same token.
[00:57:31.040 --> 00:57:34.160]   And they do that so that the model doesn't learn to cheat in
[00:57:34.160 --> 00:57:37.560]   terms of how it actually figures things out. And so if you read
[00:57:37.560 --> 00:57:41.160]   this paper, you'll see they've explored other strategies, and
[00:57:41.160 --> 00:57:44.440]   why they went with that and why it turned out to be pretty
[00:57:44.440 --> 00:57:51.840]   successful. There's also a it's the T five paper by Google, they
[00:57:51.840 --> 00:57:55.960]   actually explore a variety of different masking strategies and
[00:57:55.960 --> 00:58:00.360]   what worked well for their the T five architecture. So there's
[00:58:00.360 --> 00:58:02.840]   different things you can do, you can mask tokens, you can mask
[00:58:02.840 --> 00:58:06.600]   words, you can mask like phrases, you know, parts of the
[00:58:06.600 --> 00:58:10.000]   sentence, you can flip things around, you can do all kinds of
[00:58:10.000 --> 00:58:13.840]   things to add noise for the model to figure out like what,
[00:58:13.840 --> 00:58:17.520]   you know how to handle it. So the T five has a lot of example
[00:58:17.520 --> 00:58:20.360]   implementations, but I would start with the BERT paper to
[00:58:20.360 --> 00:58:22.800]   kind of learn more since that's the where it all kind of
[00:58:22.800 --> 00:58:23.320]   started.
[00:58:23.320 --> 00:58:27.320]   Awesome. So there was a question, babe, earlier on
[00:58:27.320 --> 00:58:31.240]   YouTube around research is going on in lost languages and how we
[00:58:31.240 --> 00:58:33.840]   restoring them. I'll take this one because I've interviewed
[00:58:33.840 --> 00:58:37.680]   someone from this. There's a dialect in Japanese, I believe
[00:58:37.680 --> 00:58:43.760]   it's called kuzushiji. And Taryn had has been doing some amazing
[00:58:43.760 --> 00:58:48.600]   research and there's a Kaggle competition as well on this. So
[00:58:48.600 --> 00:58:52.840]   it's about using OCR to really restore these older dialects of
[00:58:52.840 --> 00:58:56.240]   Japanese, which I believe no one speaks today. So it's called K
[00:58:56.240 --> 00:59:01.480]   u z u s h i j i. You can look that up. Anything to add there?
[00:59:01.480 --> 00:59:01.760]   Wait,
[00:59:01.760 --> 00:59:06.920]   I'm glad I didn't have to pronounce that. Yeah, I don't
[00:59:06.920 --> 00:59:09.880]   really know too much about that. But that sounds like really kind
[00:59:09.880 --> 00:59:12.200]   of interesting application of these things for sure.
[00:59:12.200 --> 00:59:17.800]   Awesome. No, no other questions that I see right now.
[00:59:17.800 --> 00:59:23.160]   Okay. So with that, here's some homework. Read the attention is
[00:59:23.160 --> 00:59:26.480]   all you need paper, I guarantee it, you can read it. If you got
[00:59:26.480 --> 00:59:31.720]   questions, hit up the the discord. Like I said, there's a
[00:59:31.720 --> 00:59:36.200]   plenty of people smarter than me that are willing to answer those
[00:59:36.200 --> 00:59:39.480]   questions. You can also look at the BERT paper that I just
[00:59:39.480 --> 00:59:43.520]   mentioned, there's the illustrated BERT Elmo and co or
[00:59:43.520 --> 00:59:47.200]   how NLP crack transfer learning from Jay Alomar that has a lot
[00:59:47.200 --> 00:59:51.120]   of nice animations and graphics to help explain that paper. And
[00:59:51.120 --> 00:59:56.280]   as a third piece is the I mentioned, three of the fast a
[00:59:56.280 --> 01:00:01.520]   hugging face libraries. Take a look at those, find one start
[01:00:01.520 --> 01:00:03.920]   playing with it to get or I've taken a look at all three of
[01:00:03.920 --> 01:00:08.600]   them and find which one fits kind of your style of coding.
[01:00:08.600 --> 01:00:11.160]   There are there similar, but there's also some differences.
[01:00:11.160 --> 01:00:13.880]   So check those out and kind of see get a feel for like, you
[01:00:13.880 --> 01:00:18.160]   know, what, which one you like working with based on its
[01:00:18.320 --> 01:00:21.760]   syntax, design decisions, and then see if you can build a
[01:00:21.760 --> 01:00:24.640]   basic classification model. And if you look at the docs for all
[01:00:24.640 --> 01:00:29.960]   of them, you'll find examples of on how to do that. And that's
[01:00:29.960 --> 01:00:33.920]   it. Unless there's any final questions here.
[01:00:33.920 --> 01:00:39.560]   I think most of them are being answered. So I'll just take a
[01:00:39.560 --> 01:00:43.120]   second to wrap up and just appreciate this study group.
[01:00:43.120 --> 01:00:46.560]   Really like I've, I've not enjoyed university so much
[01:00:46.560 --> 01:00:50.240]   because it's so boring. And like this, this makes it so much fun.
[01:00:50.240 --> 01:00:53.800]   It's almost midnight in India. Wade has sunlight there. I see
[01:00:53.800 --> 01:00:57.120]   Ricardo in the call. He also has some daylight. Mateo is taking
[01:00:57.120 --> 01:01:00.240]   notes. He has his headlight on. So it's it's really this global
[01:01:00.240 --> 01:01:04.600]   classroom that we're joining really. And please do interact
[01:01:04.600 --> 01:01:07.120]   with people. That's what makes this fun. Make sure you join the
[01:01:07.120 --> 01:01:09.840]   fast AI discord. Make sure you join the hugging face forums,
[01:01:09.840 --> 01:01:13.640]   tag us, ask questions, discuss as much as you want. That's why
[01:01:13.640 --> 01:01:16.040]   we're really doing this. I'm sure Wade enjoys this. That's
[01:01:16.040 --> 01:01:19.640]   why he does this. And please join me in thanking Wade and
[01:01:19.640 --> 01:01:22.240]   Wade's in biases for helping us host these.
[01:01:22.240 --> 01:01:25.520]   Yeah, thank you, everybody for coming. Yeah, it's really a lot
[01:01:25.520 --> 01:01:29.480]   of fun for me. Like I definitely my passion is learning. And
[01:01:29.480 --> 01:01:33.960]   there's no way to, to develop your understanding of things,
[01:01:33.960 --> 01:01:37.400]   you know, more so than teaching and things like this. And I
[01:01:37.400 --> 01:01:41.680]   appreciate Sonia, you handling all the chat logistics, so I
[01:01:41.680 --> 01:01:44.960]   could just focus on the easy stuff. And I appreciate
[01:01:44.960 --> 01:01:48.920]   everybody for coming regardless of time zone. And yeah, thank
[01:01:48.920 --> 01:01:49.160]   you.
[01:01:49.160 --> 01:01:53.560]   Awesome. Thanks, everyone. We'll see you next week at the same
[01:01:53.560 --> 01:01:53.840]   time.
[01:01:54.200 --> 01:01:56.520]   Transcribed by https://otter.ai
[01:01:56.520 --> 01:02:14.480]   Transcribed by https://otter.ai

