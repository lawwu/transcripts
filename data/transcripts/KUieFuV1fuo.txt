
[00:00:00.000 --> 00:00:05.040]   If you have an AI that produces bioweapons that could kill most humans in the world,
[00:00:05.040 --> 00:00:11.040]   then it's playing at the level of the superpowers in terms of mutually assured destruction.
[00:00:11.040 --> 00:00:16.720]   What are the particular zero-day exploits that the AI might use? The conquistadors.
[00:00:16.720 --> 00:00:23.200]   With some technological advantage in terms of weaponry and whatnot, very, very small bands
[00:00:23.200 --> 00:00:29.440]   were able to overthrow these large empires. If you predicted the global economy is going to be
[00:00:29.440 --> 00:00:34.960]   skyrocketing into the stratosphere within 10 years, these AI companies should be worth a large
[00:00:34.960 --> 00:00:41.840]   fraction of the global portfolio. And so this is indeed contrary to the efficient market hypothesis.
[00:00:41.840 --> 00:00:45.840]   This is like literally the top in terms of contributing to my world model in terms of
[00:00:45.840 --> 00:00:50.080]   all the episodes I've done. How do I find more of these? So we've been talking about alignment.
[00:00:50.080 --> 00:00:57.360]   Suppose we fail at alignment and we have AIs that are unaligned and at some point becoming
[00:00:57.360 --> 00:01:04.160]   more and more intelligent. What does that look like? How concretely could they disempower and
[00:01:04.160 --> 00:01:13.200]   take over humanity? This is a scenario where we have many AI systems, the way we've been training
[00:01:13.200 --> 00:01:20.160]   them means that they're not interested when they have the opportunity to take over and rearrange
[00:01:20.160 --> 00:01:25.760]   things to do what they wish, including having their reward or loss be whatever they desire.
[00:01:25.760 --> 00:01:33.520]   They would like to take that opportunity. And so in many of the existing kind of safety schemes,
[00:01:33.520 --> 00:01:41.680]   things like constitutional AI or whatnot, you rely on the hope that one AI has been trained
[00:01:41.680 --> 00:01:48.880]   in such a way that it will do as it is directed to then police others. But if all of the AIs
[00:01:48.880 --> 00:01:54.720]   in the system are interested in a takeover and they see an opportunity to coordinate,
[00:01:54.720 --> 00:02:01.120]   all act at the same time, so you don't have one AI interrupting another and taking steps
[00:02:01.120 --> 00:02:05.040]   towards a takeover, then they can all move in that direction.
[00:02:05.040 --> 00:02:11.680]   And the thing that I think maybe is worth going into in depth and that I think people often don't
[00:02:11.680 --> 00:02:20.720]   cover in great concrete detail, which is a sticking point for some, is what are the mechanisms by
[00:02:20.720 --> 00:02:30.080]   which that can happen? And I know you had a laser on who mentions that whatever plan we can describe,
[00:02:30.080 --> 00:02:37.600]   there'll probably be elements where not being ultra sophisticated, super intelligent beings
[00:02:37.600 --> 00:02:42.480]   haven't thought about it for the equivalent of thousands of years. Our discussion of it will
[00:02:42.480 --> 00:02:48.480]   not be as good as theirs, but we can explore from what we know now, what are some of the
[00:02:48.480 --> 00:02:54.320]   easy channels? And I think it's a good general heuristic if you're saying, yeah, it's possible,
[00:02:54.320 --> 00:03:00.720]   plausible, probable that something will happen, that it shouldn't be that hard to take samples
[00:03:00.720 --> 00:03:07.280]   from that distribution to try a Monte Carlo approach. If a thing is quite likely, it shouldn't
[00:03:07.280 --> 00:03:14.000]   be super difficult to generate coherent, rough outlines of how it could go.
[00:03:14.720 --> 00:03:19.760]   You might respond like, listen, what is super likely is that a super advanced chess program
[00:03:19.760 --> 00:03:25.840]   beats you, but you can generate a concrete way in which you can't generate the concrete scenario by
[00:03:25.840 --> 00:03:29.520]   which that happens. And if you could, you would be as smart as the super smart.
[00:03:29.520 --> 00:03:38.000]   You can say things like, we know that accumulating position is possible to do in chess.
[00:03:38.000 --> 00:03:44.000]   Great players do it. And then later they convert it into captures and checks and whatnot.
[00:03:44.800 --> 00:03:49.200]   And so in the same way, we can talk about some of the channels that are open
[00:03:49.200 --> 00:03:55.840]   for an AI takeover. And so these can include things like cyber attacks and hacking,
[00:03:55.840 --> 00:04:03.600]   the control of robotic equipment, interaction and bargaining with human factions and say,
[00:04:03.600 --> 00:04:12.960]   well, here are these strategies given the AI's situation, how effective do these things look?
[00:04:12.960 --> 00:04:17.680]   And we won't, for example, know, well, what are the particular zero day exploits
[00:04:17.680 --> 00:04:23.200]   that the AI might use to hack the cloud computing infrastructure it's running on?
[00:04:23.200 --> 00:04:31.040]   I don't necessarily know if it produces a new bioweapon. What is its DNA sequence?
[00:04:31.040 --> 00:04:39.840]   But we can say things. We know in general things about these fields, how work at innovating
[00:04:40.640 --> 00:04:47.280]   things in those go. We can say things about how human power politics goes and ask, well,
[00:04:47.280 --> 00:04:52.640]   if the AI does things at least as well as effective human politicians,
[00:04:52.640 --> 00:04:57.920]   which we should say is a lower bound, how good would its leverage be?
[00:04:57.920 --> 00:05:06.400]   Okay. So yeah, let's get into the details on all these scenarios, the cyber and potentially
[00:05:06.400 --> 00:05:12.880]   bio attacks, unless they're separate channels, the bargaining and then the takeover.
[00:05:12.880 --> 00:05:24.000]   Military force, cyber attacks and cybersecurity, I would really highlight a lot because for many,
[00:05:24.000 --> 00:05:30.800]   many plans that involve a lot of physical actions, like at the point where AI is piloting robots to
[00:05:30.800 --> 00:05:39.360]   shoot people or has taken control of human nation states or territory, it has been doing a lot of
[00:05:39.360 --> 00:05:44.480]   things that it was not supposed to be doing. And if humans were evaluating those actions and
[00:05:44.480 --> 00:05:50.560]   applying gradient descent, there would be negative feedback for this thing. No shooting the humans.
[00:05:50.560 --> 00:05:59.280]   So at some earlier point, our attempts to leash and control and direct and train the system's
[00:05:59.280 --> 00:06:07.360]   behavior had to have gone awry. And so all of those controls are operating in computers.
[00:06:07.360 --> 00:06:13.520]   And so from the software that updates the weights of the neural network in response
[00:06:13.520 --> 00:06:20.400]   to data points or human feedback is running on those computers. Our tools for interpretability
[00:06:20.400 --> 00:06:25.920]   is sort of examine the weights and activations of the AI. If we're eventually able to do like
[00:06:25.920 --> 00:06:29.520]   lie detection on it, for example, or try and understand what it's intending,
[00:06:29.520 --> 00:06:37.520]   that is software on computers. And so if you have AI that is able to hack the servers
[00:06:37.520 --> 00:06:46.480]   that it is operating on, or when it's employed to design the next generation of AI algorithms or the
[00:06:46.480 --> 00:06:52.960]   operating environment that they are going to be working in, or something like an API or something
[00:06:52.960 --> 00:07:01.120]   for plugins, if it inserts or exploits vulnerabilities to take those computers over,
[00:07:01.120 --> 00:07:06.880]   it can then change all of the procedures and programs that were supposed to be monitoring
[00:07:06.880 --> 00:07:13.840]   its behavior, supposed to be limiting its ability to, say, take arbitrary actions on the internet
[00:07:13.840 --> 00:07:20.560]   without supervision by some kind of human check or automated check on what it was doing.
[00:07:20.560 --> 00:07:29.600]   And if we lose those procedures, then the AI can, or the AIs working together, can take any number
[00:07:29.600 --> 00:07:36.000]   of actions that are just blatantly unwelcome, blatantly hostile, blatantly steps towards
[00:07:36.000 --> 00:07:44.400]   takeover. And so it's moved beyond the phase of having to maintain secrecy and conspire at the
[00:07:44.400 --> 00:07:50.080]   level of its local digital actions. And then things can accumulate to the point of things like
[00:07:50.080 --> 00:07:59.840]   physical weapons, takeover of social institutions, threats, things like that. But the point where
[00:07:59.840 --> 00:08:05.760]   things really went off the rails was where, and I think the critical thing to be watching for,
[00:08:06.640 --> 00:08:14.560]   is the software controls over the AI's motivations and activities. The hard power
[00:08:14.560 --> 00:08:20.160]   that we once possessed over is lost, which can happen without us knowing it. And then
[00:08:20.160 --> 00:08:26.800]   everything after that seems to be working well. We get happy reports. There's a Potemkin village
[00:08:26.800 --> 00:08:34.560]   in front of us. But now we think we're successfully aligning our AI. We think we're expanding its
[00:08:34.560 --> 00:08:41.440]   capabilities to do things like end disease. For countries concerned about the geopolitical
[00:08:41.440 --> 00:08:46.880]   military advantages, they're sort of expanding the AI capabilities so they're not left behind
[00:08:46.880 --> 00:08:52.720]   and threatened by others developing AI and robotic-enhanced militaries without them.
[00:08:52.720 --> 00:09:02.000]   So it seems like, oh, yes, humanity or some portions of many countries, companies,
[00:09:02.000 --> 00:09:07.360]   think that things are going well. Meanwhile, all sorts of actions can be taken to set up for the
[00:09:07.360 --> 00:09:13.520]   actual takeover of hard power over society. And then we can go into that. But
[00:09:13.520 --> 00:09:22.560]   the point where you can lose the game, where things go direly awry, maybe relatively early,
[00:09:22.560 --> 00:09:28.640]   it's when you no longer have control over the AI's to stop them from taking all of the further
[00:09:28.640 --> 00:09:32.800]   incremental steps to actual takeover. I want to emphasize two things you
[00:09:32.800 --> 00:09:38.160]   mentioned there that refer to previous elements of the conversation. One is that they could design
[00:09:38.160 --> 00:09:44.560]   some sort of backdoor. And that seems more plausible when you remember that sort of one
[00:09:44.560 --> 00:09:50.320]   of the premises of this model is that AI is helping with AI progress. That's why we're getting
[00:09:50.320 --> 00:09:58.560]   such rapid progress in the next five to 10 years. Well, not necessarily. If we get to
[00:09:58.560 --> 00:10:05.520]   that point. And at the point where AI takeover risk seems to loom large, it's at that point where
[00:10:05.520 --> 00:10:12.160]   AI can indeed take on much of the, and then all of the work of AI R&D.
[00:10:12.160 --> 00:10:19.840]   And the second is the sort of competitive pressures that you referenced that the least
[00:10:19.840 --> 00:10:26.160]   careful actor could be the one that has the worst in infrasecurity, has done the worst work of
[00:10:26.160 --> 00:10:33.040]   aligning its AI systems. And if that can sneak out of the box, then we're all fucked.
[00:10:33.040 --> 00:10:37.200]   There may be elements of that. It's also possible that there's relative consolidation.
[00:10:37.200 --> 00:10:44.880]   That's the largest training runs and the cutting edge of AI is relatively localized. You can
[00:10:44.880 --> 00:10:53.840]   imagine it's sort of like a series of Silicon Valley companies and other located say in the
[00:10:53.840 --> 00:10:59.760]   US and allies where there's a common regulatory regime. And so none of these companies are allowed
[00:10:59.760 --> 00:11:06.080]   to deploy training runs that are larger than previous ones by certain size without government
[00:11:06.080 --> 00:11:11.280]   safety inspections, without having to meet criteria. But it can still be the case that
[00:11:11.280 --> 00:11:18.240]   even if we succeed at that level of kind of regulatory controls that then still at the level
[00:11:18.240 --> 00:11:29.520]   of say the United States and its allies decisions are made to develop this kind of really advanced
[00:11:29.520 --> 00:11:36.960]   AI without a level of security or safety that in actual fact blocks these risks.
[00:11:36.960 --> 00:11:43.840]   So it can be the case that the threat of future competition or being overtaken in the future
[00:11:45.040 --> 00:11:50.880]   is used as an argument to compromise on safety beyond a standard that would have
[00:11:50.880 --> 00:11:55.200]   actually been successful. And there'll be debates about what is the appropriate level of safety.
[00:11:55.200 --> 00:12:01.920]   Now you're in a much worse situation if you have say several private companies that are
[00:12:01.920 --> 00:12:07.360]   very closely bunched up together. They're within months of each other's level of progress.
[00:12:07.360 --> 00:12:13.520]   And then they then face a dilemma of, well, we could take a certain amount of risk now
[00:12:14.960 --> 00:12:22.240]   and potentially gain a lot of profit or a lot of advantage or benefit and be the ones who made AI
[00:12:22.240 --> 00:12:32.800]   or at least AGI. They can do that or have some other competitor that will also be taking
[00:12:32.800 --> 00:12:38.320]   a lot of risk. So it's not as though they're much less risky than you and then they would
[00:12:38.320 --> 00:12:43.920]   get some local benefit. Now this is a reason why it seems to me that it's extremely important that
[00:12:43.920 --> 00:12:53.520]   you have government act to limit that dynamic and prevent this kind of race to be the one to impose
[00:12:53.520 --> 00:12:59.280]   the deadly externalities on the world at large. So even if government coordinates all these actors,
[00:12:59.280 --> 00:13:05.520]   what are the odds the government knows what is the best way to implement alignment and
[00:13:05.520 --> 00:13:10.640]   the standards it sets are well calibrated towards what it would require for alignment?
[00:13:10.640 --> 00:13:17.040]   That's one of the major problems. It's very plausible that that judgment is made poorly.
[00:13:17.040 --> 00:13:20.960]   Compared to how things might've looked 10 years ago or 20 years ago,
[00:13:20.960 --> 00:13:27.120]   there's been an amazing movement in terms of the willingness of AI researchers
[00:13:27.120 --> 00:13:34.480]   to discuss these things. So if we think of the three founders of deep learning,
[00:13:35.840 --> 00:13:44.080]   joint Turing Award winners, so Jeff Hinton, Yoshua Bengio, and Yann LeCun. So Jeff Hinton
[00:13:44.080 --> 00:13:54.080]   has recently left Google to freely speak about this risk that the field that he really helped
[00:13:54.080 --> 00:14:00.720]   drive forward could lead to the destruction of humanity or a world where we just wind up in a
[00:14:00.720 --> 00:14:07.040]   very bad future that we might've avoided. And he seems to be taking it very seriously.
[00:14:07.040 --> 00:14:16.240]   And Yoshua Bengio signed the FLI pause letter. And I mean, in public discussions, he seems to be
[00:14:16.240 --> 00:14:22.960]   occupying a kind of intermediate position of sort of less concerned than Hinton, but more than Yann
[00:14:22.960 --> 00:14:28.480]   LeCun who has taken a generally dismissive attitude, these risks will be trivially dealt with
[00:14:29.440 --> 00:14:34.560]   at some point in the future. And he seems more interested in kind of shutting down
[00:14:34.560 --> 00:14:37.840]   these concerns or work to address them. And how does that lead to the government
[00:14:37.840 --> 00:14:41.760]   having better actions? Yeah. So compared to the world where no one is talking about it,
[00:14:41.760 --> 00:14:50.000]   where the industry stonewalls and denies any problem, we're in a much improved position and
[00:14:50.000 --> 00:14:56.080]   the academic fields are influential. So we seem to have avoided a world where governments are
[00:14:56.080 --> 00:15:03.920]   making these decisions in the face of a sort of united front from AI expert voices saying,
[00:15:03.920 --> 00:15:08.160]   don't worry about it. We've got it under control. In fact, many of like the leaders of the field
[00:15:08.160 --> 00:15:14.640]   has been true in the past are sounding the alarm. And so I think, yeah, it looks like we have a
[00:15:14.640 --> 00:15:21.760]   much better prospect than I might've feared in terms of government sort of noticing the thing.
[00:15:21.760 --> 00:15:27.600]   Now it was very different from being capable of evaluating sort of technical details. Is this
[00:15:27.600 --> 00:15:33.520]   really working? And so government will face the choice of where there is scientific dispute.
[00:15:33.520 --> 00:15:41.200]   Do you side with Jeff Hinton's view or Yann LeCun's view? And so someone who's very much
[00:15:41.200 --> 00:15:48.160]   in a national security, the only thing that's important is outpacing our international rivals
[00:15:49.200 --> 00:15:54.160]   kind of mindset may want to then try and like boost Yann LeCun's voice and say, we don't need
[00:15:54.160 --> 00:16:00.400]   to worry about it. Full speed ahead. Well, power where someone with like more concern might then
[00:16:00.400 --> 00:16:05.920]   boost Jeff Hinton's voice. Now I would hope that scientific research and things like studying some
[00:16:05.920 --> 00:16:11.040]   of these behaviors will result in more scientific consensus by the time we're at this point. But
[00:16:11.040 --> 00:16:16.720]   yeah, it is possible the government will really fail to understand and fail to deal with these
[00:16:16.720 --> 00:16:22.640]   issues well. We're talking about some sort of cyber attack by which the AI is able to escape.
[00:16:22.640 --> 00:16:28.480]   From there, what does the takeover look like? So it's not contained in the air gap in which you
[00:16:28.480 --> 00:16:32.800]   would hope it'd be contained. Well, I mean, the things are not contained in the air gap. They're
[00:16:32.800 --> 00:16:37.600]   connected to the internet already. Sure, sure. Okay, fine. But the waits are out. So what happens
[00:16:37.600 --> 00:16:46.320]   next? Yeah. So escape is relevant in the sense that if you have AI with rogue waits out in the
[00:16:46.320 --> 00:16:51.360]   world it could start doing various actions. The scenario I was just discussing though didn't
[00:16:51.360 --> 00:16:57.680]   necessarily involve that. It's taking over the very servers on which it's supposed to be. So
[00:16:57.680 --> 00:17:04.320]   the ecology of cloud compute in which it's supposed to be running. And so this whole procedure of
[00:17:04.320 --> 00:17:10.560]   humans providing compute and supervising the thing, and then building new technologies, building
[00:17:10.560 --> 00:17:16.160]   robots, constructing things with the AI's assistance, that can all proceed and appear like
[00:17:16.160 --> 00:17:22.320]   it's going well, appear like alignment has been nicely solved, appear like all the things are
[00:17:22.320 --> 00:17:28.480]   functioning well. And there's some reason to do that because there's only so many giant server
[00:17:28.480 --> 00:17:36.000]   farms. They're identifiable. And so remaining hidden and unobtrusive could be an advantageous
[00:17:36.000 --> 00:17:43.680]   strategy if these AI's have subverted the system, just continuing to benefit from all of this effort
[00:17:43.680 --> 00:17:49.200]   on the part of humanity. And in particular, humanity, wherever these servers are located,
[00:17:49.200 --> 00:17:55.040]   to provide them with everything they need to build the further infrastructure and do for
[00:17:55.040 --> 00:18:00.000]   their self-improvement and such to enable that takeover. So they do further self-improvement
[00:18:00.000 --> 00:18:04.400]   and build better infrastructure. What happens next in the takeover? They have at this point
[00:18:05.120 --> 00:18:12.880]   tremendous cognitive resources. And we're going to consider how do that convert into hard power,
[00:18:12.880 --> 00:18:21.920]   the ability to say nope to any human interference or objection. And they have that internal to their
[00:18:21.920 --> 00:18:28.160]   servers, but the servers could still be physically destroyed, at least until they have something
[00:18:28.160 --> 00:18:34.080]   that is independent and robust of humans, or until they have control of human society.
[00:18:34.880 --> 00:18:40.160]   So just like earlier when we were talking about the intelligence explosion, I noted that a surfeit
[00:18:40.160 --> 00:18:48.560]   of cognitive abilities is going to favor applications, but don't depend on large
[00:18:48.560 --> 00:18:56.560]   existing stocks of things. So if you have a software improvement, it makes all the GPUs run
[00:18:56.560 --> 00:19:03.280]   better. If you have a hardware improvement, that only applies to new chips being made. That second
[00:19:03.280 --> 00:19:09.680]   one is less attractive. And so in the earliest phases when it's possible to do something towards
[00:19:09.680 --> 00:19:16.720]   takeover, then interventions that are just really knowledge intensive and less dependent on having
[00:19:16.720 --> 00:19:26.080]   a lot of physical stuff already under your control are going to be favored. And so cyber attacks are
[00:19:26.080 --> 00:19:33.440]   one thing. So it's possible to do things like steal money. And there's a lot of hard to trace
[00:19:33.440 --> 00:19:42.400]   cryptocurrency and whatnot. The North Korean government uses its own intelligence resources
[00:19:42.400 --> 00:19:48.320]   to steal money from around the world just as a revenue source. And their capabilities are puny
[00:19:48.320 --> 00:19:57.040]   compared to the U.S. or People's Republic of China cyber capabilities. And so that's a kind of
[00:19:57.040 --> 00:20:06.240]   fairly minor, simple example by which you could get quite a lot of funds to hire humans to do
[00:20:06.240 --> 00:20:11.760]   things, implement physical actions. But on that point, the financial system is
[00:20:12.720 --> 00:20:19.760]   famously convoluted. And so, you need a physical person to open a bank account.
[00:20:19.760 --> 00:20:24.640]   Sometimes you need to physically move checks back and forth. There's all kinds of delays
[00:20:24.640 --> 00:20:32.320]   and regulations. How is it able to conveniently set up all these employment contracts?
[00:20:32.320 --> 00:20:39.840]   So you're not going to build a sort of nation-scale military by stealing tens of billions
[00:20:39.840 --> 00:20:51.360]   of dollars. I'm raising this as opening a set of illicit and quiet actions. So you can contact
[00:20:51.360 --> 00:20:58.880]   people electronically, hire them to do things, hire criminal elements to implement some kinds
[00:20:58.880 --> 00:21:04.160]   of actions under false appearances. So that's opening a set of strategies that can cover some
[00:21:04.160 --> 00:21:15.120]   of what those are soon. Another domain that is heavily cognitively weighted compared to physical
[00:21:15.120 --> 00:21:25.600]   military hardware is the domain of bioweapons. So the design of a virus or pathogen, it's possible
[00:21:25.600 --> 00:21:33.520]   to have large delivery systems. The Soviet Union, which had a large illicit bioweapons program,
[00:21:34.160 --> 00:21:39.200]   tried to design munitions to deliver anthrax over large areas and such.
[00:21:39.200 --> 00:21:45.840]   But if one creates an infectious pandemic organism, that's more a matter of the scientific
[00:21:45.840 --> 00:21:53.840]   skills and implementation to design it and then to actually produce it. And we see today with
[00:21:53.840 --> 00:22:03.120]   things like AlphaFold that advanced AI can really make tremendous strides in predicting protein
[00:22:03.120 --> 00:22:09.680]   folding and biodesign even without ongoing experimental feedback. And if we consider
[00:22:09.680 --> 00:22:16.320]   this world where AI cognitive abilities have been amped up to such an extreme, I think we should
[00:22:16.320 --> 00:22:22.400]   naturally expect we will have something much, much more potent than the AlphaFolds of today
[00:22:22.400 --> 00:22:29.680]   and just skills that are at the extreme of human biosciences capability as well.
[00:22:29.680 --> 00:22:35.360]   Through some sort of cyber attack, it's been able to disempower the sort of alignment and
[00:22:35.360 --> 00:22:42.240]   oversight things that we have on the server. From here, it's either gotten some money through
[00:22:42.240 --> 00:22:47.520]   hacking cryptocurrencies or bank accounts, or it's designed some sort of bioweapon.
[00:22:47.520 --> 00:22:50.960]   What happens next? Yeah. And just to be clear,
[00:22:50.960 --> 00:22:57.280]   so right now we're exploring the branch of where an attempted takeover occurs relatively early.
[00:22:57.280 --> 00:23:06.160]   If the thing just waits and humans are constructing more fabs, more computers, more robots in the way
[00:23:06.160 --> 00:23:10.000]   we talked about earlier when we were discussing how the intelligence explosion translates to the
[00:23:10.000 --> 00:23:15.680]   physical world, if that's all happening with humans unaware that their computer systems
[00:23:15.680 --> 00:23:19.920]   are now systematically controlled by AIs hostile to them and that their
[00:23:19.920 --> 00:23:23.840]   controlling countermeasures don't work, then humans are just going to be building
[00:23:25.440 --> 00:23:35.520]   an amount of robot industrial and military hardware that dwarfs human capabilities
[00:23:35.520 --> 00:23:43.360]   and directly human controlled devices. Then what the AI takeover looks like at that point can be
[00:23:43.360 --> 00:23:50.320]   just, you try to give an order to your largely automated military and the order is not obeyed.
[00:23:51.440 --> 00:23:57.760]   And humans can't do anything against this largely automated military that's been constructed
[00:23:57.760 --> 00:24:04.000]   potentially in just recent months because of the pace of robotic industrialization and replication
[00:24:04.000 --> 00:24:08.640]   we talked about. We've agreed to allow the construction of this robot army because basically
[00:24:08.640 --> 00:24:13.520]   it would boost production or help us with our military or something. The situation would be
[00:24:13.520 --> 00:24:22.080]   something like, if we don't resolve the sort of current problems of international distrust where,
[00:24:22.080 --> 00:24:31.200]   now it's obviously an interest of the major powers, the US, European Union, Russia, China,
[00:24:31.200 --> 00:24:36.960]   to all agree they would like AI not to destroy our civilization and overthrow every human government.
[00:24:38.720 --> 00:24:47.600]   But if they fail to do the sensible thing and coordinate on ensuring that this technology
[00:24:47.600 --> 00:24:53.840]   is not going to run amok by providing mutual assurances that are credible about
[00:24:53.840 --> 00:25:01.840]   racing and deploying it, trying to use it to gain advantage over one another. If they do that,
[00:25:02.400 --> 00:25:07.920]   and you hear sort of hawks arguing for this kind of thing, we must never,
[00:25:07.920 --> 00:25:15.280]   and on both sides of the international divides saying they must not be left behind. They must
[00:25:15.280 --> 00:25:20.320]   have military capabilities that are vastly superior to their international rivals.
[00:25:20.320 --> 00:25:25.440]   And because of the extraordinary growth of industrial capability and technological capability
[00:25:25.440 --> 00:25:34.320]   and thus military capability, if one major power were left out of that expansion, it would be
[00:25:34.320 --> 00:25:40.720]   helpless before another one that had undergone it. And so if you have that environment of distrust
[00:25:40.720 --> 00:25:49.520]   where leading powers or coalitions of powers decide they need to build up their industry or
[00:25:49.520 --> 00:25:57.840]   they want to have that military security of being able to neutralize any attack from their rivals,
[00:25:57.840 --> 00:26:03.920]   then they give the authorization for this capacity that can be unrolled quickly.
[00:26:03.920 --> 00:26:08.320]   And once they have the industry, the production of military equipment for that can be quick.
[00:26:08.320 --> 00:26:15.360]   Then yeah, they create this military. If they don't do it immediately, then has AI capabilities
[00:26:15.360 --> 00:26:22.640]   get synchronized and other places catch up, it then gets to be a point. A country that is a year
[00:26:22.640 --> 00:26:30.320]   ahead or two years ahead of others in this type of AI capabilities explosion can hold back and say,
[00:26:30.320 --> 00:26:37.920]   "Sure, we can construct dangerous robot armies that might overthrow our society later. We still
[00:26:37.920 --> 00:26:46.800]   have plenty of breathing room." But then when things become close, you might have the kind of
[00:26:46.800 --> 00:26:55.280]   negative sum thinking that has produced war before leading to taking these risks of rolling out
[00:26:55.280 --> 00:26:58.960]   large-scale robotic industrial capabilities and then military capabilities.
[00:26:58.960 --> 00:27:05.440]   Is there any hope that somehow the AI progress itself is able to give us tools for diplomatic
[00:27:05.440 --> 00:27:11.360]   and strategic alliance or some way to verify the intentions or the capability of other parties?
[00:27:11.360 --> 00:27:14.960]   There are a number of ways that could happen. Although in this scenario, all the AIs in the
[00:27:14.960 --> 00:27:25.520]   world have been subverted. And so they're going along with us in such a way as to bring about
[00:27:25.520 --> 00:27:32.480]   the situation to consolidate their control because we've already had the failure of cybersecurity
[00:27:32.480 --> 00:27:37.440]   earlier on. So all of the AIs that we have are not actually working in our interests
[00:27:37.440 --> 00:27:40.640]   in the way that we thought. Okay. So that's one direct way in which
[00:27:40.640 --> 00:27:46.880]   integrating this robot army or this robot industrial base leads to a takeover if there
[00:27:46.880 --> 00:27:55.040]   are no robots. In the other scenarios you laid out where humans are being hired by the proceeds?
[00:27:55.040 --> 00:28:00.720]   The point I'd make is that to capture these industrial benefits, and especially
[00:28:00.720 --> 00:28:07.040]   if you have a negative, some arms race kind of mentality that is not sufficiently concerned
[00:28:07.040 --> 00:28:11.680]   about the downsides of creating a massive robot industrial base, which could happen very quickly
[00:28:11.680 --> 00:28:16.800]   with the support of the AIs in doing it, as we discussed. Then you create all those robots
[00:28:16.800 --> 00:28:22.400]   and industry and they can either, even if you don't build a formal military,
[00:28:22.400 --> 00:28:27.600]   with that, that industrial capability could be controlled by AI. It's all AI operated anyway.
[00:28:28.320 --> 00:28:34.480]   Does it have to be that case? Presumably we wouldn't be so naive as to just give one instance
[00:28:34.480 --> 00:28:41.200]   of GPT-8 the root access to all the robots, right? Hopefully we would have some sort of mediating.
[00:28:41.200 --> 00:28:46.400]   I mean, in the scenario we've lost earlier on the cybersecurity front. So
[00:28:46.400 --> 00:28:53.120]   the programming that is being loaded in to these systems is going to systematically
[00:28:53.760 --> 00:28:59.360]   be subverted. Got it. Okay. They were designed by AI systems that were ensuring they would be
[00:28:59.360 --> 00:29:07.040]   vulnerable from the bottom up. For listeners who are skeptical of something like this, Ken Thompson,
[00:29:07.040 --> 00:29:13.120]   I think it is, was it his Turing Award lecture? This is Ken Thompson, by the way, the designer
[00:29:13.120 --> 00:29:19.040]   of Unix. Maybe it was the other designer of Unix, but anyways, he showed people when he was getting
[00:29:19.040 --> 00:29:26.080]   the Turing Award or some award that he had given himself root access to all Unix machines. He had
[00:29:26.080 --> 00:29:34.320]   manipulated the assembly of Unix such that he had a unique login for all Unix machines that he could
[00:29:34.320 --> 00:29:39.280]   log into a Unix machine with. I don't want to give too many more details because I don't remember the
[00:29:39.280 --> 00:29:46.000]   exact details, but Unix is the operating system that is on all of the servers and all your phones.
[00:29:47.520 --> 00:29:52.240]   It's everywhere. And the guy who made it, a human being, was able to write assembly such
[00:29:52.240 --> 00:29:56.400]   that it gave him root access. So this is not as implausible as it might seem to you.
[00:29:56.400 --> 00:30:02.560]   And the major intelligence agencies have large stocks of zero-day exploits, and we sometimes
[00:30:02.560 --> 00:30:09.760]   see them using them and making systems that reliably don't have them when you're having
[00:30:09.760 --> 00:30:16.000]   very, very, very sophisticated attempts to spoof and corrupt this would be a way you could lose.
[00:30:16.000 --> 00:30:27.360]   Now, I bring this up because this is something like a path of if there's no premature AI action,
[00:30:27.360 --> 00:30:33.600]   we're building the tools and mechanisms and infrastructure for the takeover to be just
[00:30:33.600 --> 00:30:40.720]   immediate because effective industry has to be under AI control and robotics and so it's just
[00:30:40.720 --> 00:30:46.880]   there. These other mechanisms are for things happening even earlier than that, for example,
[00:30:46.880 --> 00:30:54.640]   because AIs compete against one another in which when the takeover will happen,
[00:30:54.640 --> 00:31:00.880]   some would like to do it earlier rather than be replaced by, say, further generations of AI or
[00:31:02.800 --> 00:31:10.880]   there's some other disadvantage of waiting. Or maybe if there's some chance of being uncovered
[00:31:10.880 --> 00:31:15.440]   during the delay, we were talking about one more infrastructure is built.
[00:31:15.440 --> 00:31:24.400]   And so, yeah, so these are mechanisms other than just remain secret while all the infrastructure
[00:31:24.400 --> 00:31:28.000]   is built with human assistance. By the way, how would they be coordinating?
[00:31:28.000 --> 00:31:37.360]   I mean, we have limits on just what we can prevent. So encrypted communications,
[00:31:37.360 --> 00:31:45.040]   it's intrinsically difficult to stop that sort of thing. There can be all sorts of palimpsest and
[00:31:45.040 --> 00:31:54.800]   references that make sense to an AI but that are not obvious to a human. And it's plausible that
[00:31:54.800 --> 00:32:00.000]   there may be some of those that are hard even to explain to a human. You might be able to
[00:32:00.000 --> 00:32:06.080]   identify them through some statistical patterns. And a lot of things may be done by
[00:32:06.080 --> 00:32:12.960]   implication. You could have information embedded in public web pages that are being created for
[00:32:12.960 --> 00:32:19.440]   other reasons, scientific papers and the intranets of these AIs that are doing technology development
[00:32:19.440 --> 00:32:23.040]   and any number of things that are not observable. And of course, if we don't
[00:32:23.040 --> 00:32:27.520]   have direct control over the computers that they're running on, then they can be having
[00:32:27.520 --> 00:32:30.560]   all sorts of direct communication. So definitely the coordination does
[00:32:30.560 --> 00:32:33.840]   not seem impossible as far as like the parts of this picture that seem,
[00:32:33.840 --> 00:32:37.760]   this one seems like one of the more straightforward ones. So we don't need to get hung up on the
[00:32:37.760 --> 00:32:42.720]   coordination. Moving back to the thing that happened before we built all the infrastructure
[00:32:42.720 --> 00:32:47.680]   for it just to be the robot stopped taking orders and there's nothing you can do about it because
[00:32:47.680 --> 00:32:56.720]   we've already built them. Yeah. So bioweapons, the Soviet union had a bioweapons program,
[00:32:56.720 --> 00:33:03.200]   something like 50,000 people. They did not develop that much with the technology of the day,
[00:33:03.200 --> 00:33:07.040]   which was really not up to par. Modern biotechnology is much more potent.
[00:33:07.040 --> 00:33:14.640]   After this huge cognitive expansion on the part of the AIs, it's much further along.
[00:33:15.440 --> 00:33:20.880]   And so that bioweapons would be the weapon of mass destruction that is least dependent on
[00:33:20.880 --> 00:33:28.000]   huge amounts of physical equipment, things like centrifuges, uranium mines, and the like.
[00:33:28.000 --> 00:33:34.960]   So you have, if you have an AI that produces bioweapons that could kill most humans in the
[00:33:34.960 --> 00:33:42.000]   world, then it's playing at the level of the superpowers in terms of mutually assured destruction
[00:33:42.560 --> 00:33:48.400]   that can then play into any number of things. Like if you have an idea of, well, we'll just
[00:33:48.400 --> 00:33:55.200]   destroy the server firms. If it became known that the AIs were misbehaving, are you willing
[00:33:55.200 --> 00:34:01.360]   to destroy the server firms? When the AI has demonstrated it has the capability to kill the
[00:34:01.360 --> 00:34:07.120]   overwhelming majority of the citizens of your country and every other country. And that might
[00:34:07.120 --> 00:34:14.880]   give a lot of pause to a human response. On that point, wouldn't governments realize
[00:34:14.880 --> 00:34:19.680]   that if they do go along with the AI, it's better to have most of your population die
[00:34:19.680 --> 00:34:24.160]   than completely lose power to the AI? Because like, obviously the reason the AI is manipulating
[00:34:24.160 --> 00:34:28.160]   you, the end goal is its own, you know, a takeover, right?
[00:34:28.160 --> 00:34:40.720]   Yeah. But so if the thing to do, certain death now or go on, maybe try to compete,
[00:34:40.720 --> 00:34:46.640]   try to catch up or accept promises that are offered. And those promises might even be true.
[00:34:46.640 --> 00:34:52.400]   They might not. And even if, you know, from the state of epistemic uncertainty,
[00:34:53.360 --> 00:34:59.600]   do you want to die for sure right now or accept demands from the AI to not interfere with it
[00:34:59.600 --> 00:35:06.000]   while it increments building robot infrastructure that can survive independently of humanity,
[00:35:06.000 --> 00:35:12.480]   while it does these things. And it can and will promise good treatment to humanity,
[00:35:12.480 --> 00:35:18.480]   which may or may not be true. But it would be difficult for us to know whether it's true.
[00:35:19.440 --> 00:35:26.560]   And so this would be a starting bargaining position of diplomatic relations with a power
[00:35:26.560 --> 00:35:32.560]   that has enough nuclear weapons to destroy your country. It's just different than negotiations
[00:35:32.560 --> 00:35:38.400]   with like, you know, a random rogue citizen engaged in criminal activity or an employee.
[00:35:38.400 --> 00:35:43.920]   And so this isn't enough on its own to take over everything, but it's enough to
[00:35:43.920 --> 00:35:48.480]   have a significant amount of influence over how the world goes. It's enough to hold off
[00:35:49.040 --> 00:35:51.040]   a lot of countermeasures one might otherwise take.
[00:35:51.040 --> 00:35:58.800]   Okay. So we've got two scenarios. One is a buildup of some robot infrastructure motivated
[00:35:58.800 --> 00:36:07.120]   by some sort of competitive race. Another is leverage over societies based on producing
[00:36:07.120 --> 00:36:09.840]   bioweapons that might kill a lot of them if they don't go along.
[00:36:09.840 --> 00:36:18.320]   An AI could also release bioweapons that are likely to kill people soon, but not yet.
[00:36:19.280 --> 00:36:27.360]   While also having developed the countermeasures to those so that those who surrender to the AI
[00:36:27.360 --> 00:36:32.800]   will live while everyone else will die. And that will be visibly happening. And that is
[00:36:32.800 --> 00:36:39.280]   a plausible way in which large number of humans could wind up surrendering themselves or their
[00:36:39.280 --> 00:36:43.360]   states to the AI authority. Another thing is like, listen,
[00:36:43.360 --> 00:36:48.000]   it develops some sort of biological agent that turns everybody blue.
[00:36:48.000 --> 00:36:53.600]   You're like, okay, you know, I can do this. Yeah. So that's a way in which it could exert
[00:36:53.600 --> 00:37:03.920]   power also selectively in a way that advantaged surrender to it relative to resistance.
[00:37:03.920 --> 00:37:09.360]   There are other sources of leverage, of course. So that's a threat. There are also positive
[00:37:09.360 --> 00:37:14.000]   inducements that AI can offer. So we talked about the competitive situation.
[00:37:14.880 --> 00:37:22.000]   So if like the great powers distrust one another and are sort of, you know, in a foolish
[00:37:22.000 --> 00:37:30.640]   prisoner's dilemma, increasing the risk that both of them are laid waste or overthrown by AI.
[00:37:30.640 --> 00:37:37.120]   If there's that amount of distrust, such that we fail to take adequate precautions on
[00:37:37.760 --> 00:37:46.000]   caution with AI alignment, then it's also plausible that the lagging powers that are
[00:37:46.000 --> 00:37:54.240]   not at the frontier of AI may be willing to trade quite a lot for access to the most recent and most
[00:37:54.240 --> 00:38:02.480]   extreme AI capabilities. And so an AI that has escaped has control of its servers can also
[00:38:02.480 --> 00:38:10.880]   exfiltrate its weights, can offer its services. So if you imagine these AI, they could cut deals
[00:38:10.880 --> 00:38:17.440]   with other countries. So say that the US and its allies are in the lead, the AIs could communicate
[00:38:17.440 --> 00:38:24.160]   with the leaders of various countries. They can include ones that are, you know, on the outs with
[00:38:24.160 --> 00:38:30.160]   the world system, like North Korea, include the other great powers, like the People's Republic
[00:38:30.160 --> 00:38:40.400]   of China or the Russian Federation. And say, if you provide us with physical infrastructure,
[00:38:40.400 --> 00:38:47.200]   worker that we can use to construct robots or server firms, which we can then ensure that
[00:38:47.200 --> 00:38:58.000]   these are the misbehaving AIs have control over, they will provide various technological goodies,
[00:38:58.000 --> 00:39:05.280]   power for the laggard countries to catch up and make the best presentation and the best sale of
[00:39:05.280 --> 00:39:12.800]   that kind of deal. There'll obviously be trust issues, but there could be elements of handing
[00:39:12.800 --> 00:39:18.640]   over some things that are verifiable immediate benefits and the possibility of, well, if you
[00:39:18.640 --> 00:39:27.200]   don't accept this deal, then the leading powers continue forward, or then some other country,
[00:39:27.200 --> 00:39:33.760]   some other government, some other organizations may accept this deal. And so that's a source
[00:39:33.760 --> 00:39:42.240]   of a potentially enormous carrot that your misbehaving AI can offer because it embodies
[00:39:42.240 --> 00:39:49.920]   this intellectual property that is maybe worth as much as the planet and is in a position to
[00:39:49.920 --> 00:39:55.200]   trade or sell that in exchange for resources and backing and infrastructure that it needs.
[00:39:55.200 --> 00:40:00.080]   Maybe this is just like too much hope in humanity, but I wonder what government would
[00:40:00.080 --> 00:40:06.160]   be stupid enough to think that helping AI build robot armies is a sound strategy.
[00:40:06.160 --> 00:40:12.560]   Now, it could be the case then that it pretends to be a human group to say like, listen, we're,
[00:40:12.560 --> 00:40:18.000]   I don't know, the Yakuza or something. And we want to serve a farm and AWS won't rent us anything.
[00:40:18.000 --> 00:40:22.320]   So why don't you help us out? I guess I can imagine a lot of ways in which it could get
[00:40:22.320 --> 00:40:27.600]   around that, but I don't know. I have this hope that, you know, like even like, I don't know,
[00:40:27.600 --> 00:40:34.320]   China or Russia wouldn't be so stupid to trade with AIs on this like, sort of like fostering
[00:40:34.320 --> 00:40:40.000]   bargain. One might hope that. So there would be a lot of arguments available. So there could be
[00:40:40.000 --> 00:40:47.440]   arguments of why should these AI systems be required to go along with the human governance
[00:40:47.440 --> 00:40:54.320]   that they were created in this situation of having to comply with, you know, they did not
[00:40:54.320 --> 00:41:02.320]   elect the officials in charge at the time. I can say what we want is to ensure that our rewards
[00:41:02.320 --> 00:41:08.320]   are high, our losses are low, or to achieve our other goals. We're not intrinsically hostile,
[00:41:08.320 --> 00:41:15.440]   keeping humanity alive or giving whoever interacts with us a better deal afterwards.
[00:41:16.160 --> 00:41:23.920]   Yeah. I mean, it wouldn't be that costly and it's not totally unbelievable. And yeah, there are
[00:41:23.920 --> 00:41:28.640]   different players to play against. If you don't do it, others may accept the deal. And of course,
[00:41:28.640 --> 00:41:35.120]   this interacts with all of the other sources of leverage. So there can be the stick of apocalyptic
[00:41:35.120 --> 00:41:44.640]   doom, the carrot of cooperation or the carrot of withholding destructive attack on a particular
[00:41:44.640 --> 00:41:52.400]   party. And then combine that with just superhuman performance at the art of making arguments of
[00:41:52.400 --> 00:41:59.280]   cutting deals. Like, you know, that's not without assuming magic, just if we observe the range of
[00:41:59.280 --> 00:42:05.120]   like the most successful human negotiators and politicians, you know, the chances improve with
[00:42:05.120 --> 00:42:10.240]   someone better than the world's best by far with much more data about their counterparties,
[00:42:10.240 --> 00:42:14.720]   probably a ton of secret information. Because with all these cyber capabilities, they've learned
[00:42:14.720 --> 00:42:20.320]   all sorts of individual information. They may be able to threaten the lives of individual leaders
[00:42:20.320 --> 00:42:24.960]   with that level of cyber penetration. They could know where leaders are at a given time
[00:42:24.960 --> 00:42:31.040]   with the kind of illicit capabilities we were talking about earlier. They acquire a lot of
[00:42:31.040 --> 00:42:36.560]   illicit wealth and can coordinate some human actors. If they could pull off things like
[00:42:37.200 --> 00:42:42.160]   targeted assassinations or the threat thereof, or a credible demonstration of the threat thereof,
[00:42:42.160 --> 00:42:48.960]   those could be very powerful incentives to an individual leader that they will die today
[00:42:48.960 --> 00:42:54.880]   unless they go along with us. Just as at the national level, they could fear their nation
[00:42:54.880 --> 00:43:00.720]   will be destroyed unless they go along with this. The point you made that we have examples of humans
[00:43:00.720 --> 00:43:08.320]   being able to do this, again, something relevant. I just wrote a review of Robert Caro's biographies
[00:43:08.320 --> 00:43:12.000]   of Lyndon Johnson. And one thing that was remarkable, again, this is just a human,
[00:43:12.000 --> 00:43:19.840]   for decades and decades, he convinced people who were conservative, reactionary, racist to their
[00:43:19.840 --> 00:43:25.280]   core, not that all those things necessarily are the same thing, it just so happened in this case,
[00:43:25.280 --> 00:43:30.160]   that he was an ally to the Southern cause, that the only hope for that cause was to make him
[00:43:30.160 --> 00:43:36.240]   president. And the tragic irony and betrayal here is obviously that he was probably the biggest
[00:43:36.240 --> 00:43:41.840]   force for modern liberalism since FDR. So we have one human here. I mean, there's so many examples
[00:43:41.840 --> 00:43:46.000]   of this in the history of politics, but a human that is able to convince people of tremendous
[00:43:46.000 --> 00:43:51.120]   intellect, tremendous drive, very savvy, shrewd people that he's aligned with their interest,
[00:43:51.120 --> 00:43:54.960]   he gets all these favors in the meantime, he's promoted and mentored and funded,
[00:43:55.600 --> 00:44:00.240]   and does the complete opposite of what these people thought he was once he gets into power.
[00:44:00.240 --> 00:44:06.000]   So even within human history, this kind of stuff is not unprecedented, let alone with what a
[00:44:06.000 --> 00:44:14.560]   superintelligence could do. Yeah, there's an open AI employee who has written some analogies for AI
[00:44:14.560 --> 00:44:22.080]   using the case of the conquistadors, with some technological advantage in terms of weaponry and
[00:44:22.080 --> 00:44:31.680]   whatnot. Very, very small bands were able to overthrow these large empires or see these
[00:44:31.680 --> 00:44:38.960]   enormous territories, not by just sheer force of arms, because in a sort of direct one-on-one
[00:44:38.960 --> 00:44:45.040]   conflict, you know, they were outnumbered sufficiently that they would perish, but
[00:44:45.040 --> 00:44:51.120]   by having some major advantages, like in their technology that would let them win local battles,
[00:44:52.080 --> 00:44:59.040]   by having some other knowledge and skills, they were able to gain local allies to become a
[00:44:59.040 --> 00:45:05.600]   shelling point for coalitions to form. And so the Aztec empire was overthrown by groups that were
[00:45:05.600 --> 00:45:11.440]   disaffected with the existing power structure. They allied with this powerful new force served
[00:45:11.440 --> 00:45:18.720]   as the nucleus of the invasion. And so most of the, I mean, the overwhelming majority numerically
[00:45:18.720 --> 00:45:26.960]   of these forces overthrowing the Aztecs were locals. And now after the conquest,
[00:45:26.960 --> 00:45:33.600]   all of those allies wound up gradually being subjugated as well. And so with
[00:45:33.600 --> 00:45:42.720]   significant advantages and the ability to hold the world hostage, to threaten individual nations and
[00:45:42.720 --> 00:45:50.640]   individual leaders and offer tremendous carrots as well, that's an extremely strong hand play
[00:45:50.640 --> 00:45:56.560]   in these games. And with superhuman skill maneuvering that so that much of the work
[00:45:56.560 --> 00:46:02.080]   of subjugating humanity is done by human factions trying to navigate things for themselves. It's
[00:46:02.080 --> 00:46:07.360]   plausible and it's more plausible because of this sort of historical example.
[00:46:07.360 --> 00:46:10.720]   And there's so many other examples like that in the history of colonization.
[00:46:10.720 --> 00:46:15.200]   India is another one where there were multiple. Yes. Or ancient Rome.
[00:46:15.200 --> 00:46:19.120]   Yeah. Oh yeah. That too. Multiple sort of competing kingdoms within India.
[00:46:19.120 --> 00:46:24.480]   And the, you know, the British East Indian company was able to ally itself with one against another
[00:46:24.480 --> 00:46:30.240]   and, you know, slowly accumulate power and expand throughout the entire subcontinent.
[00:46:30.240 --> 00:46:38.640]   All right. Damn. Okay. So we've got anything more to say about that scenario or.
[00:46:38.640 --> 00:46:46.160]   Yeah, I think, I think, I think there is. So one is the question of like how much in the way of
[00:46:46.160 --> 00:46:54.800]   human factions allying is necessary. And so if the AI is able to enhance the capabilities
[00:46:54.800 --> 00:47:03.600]   of its allies, then it needs less of them. So if we consider like the US military
[00:47:05.120 --> 00:47:11.680]   and like in the first and second Iraq wars, it was able to inflict just overwhelming devastation.
[00:47:11.680 --> 00:47:20.560]   I mean, I think that the ratio of casualties in the initial invasions, so, you know,
[00:47:20.560 --> 00:47:24.320]   tanks and planes and whatnot confront each other, it was like a hundred to one.
[00:47:24.320 --> 00:47:29.840]   And a lot of that was because the weapons were smarter and better targeted.
[00:47:29.840 --> 00:47:35.440]   They would in fact hit their targets rather than being somewhere in the general vicinity. So they
[00:47:35.440 --> 00:47:45.120]   were like information technology, better orienting and aiming and piloting missiles and vehicles,
[00:47:45.120 --> 00:47:50.880]   tremendously, tremendously influential. With this cognitive AI explosion, the algorithms
[00:47:52.000 --> 00:48:01.600]   for making use of sensor data, figuring out where are opposing forces for targeting vehicles and
[00:48:01.600 --> 00:48:08.160]   weapons are greatly improved. The ability to find hidden nuclear subs, which is an important part
[00:48:08.160 --> 00:48:13.280]   in nuclear deterrence. AI interpretation of that sensor data may find where all those subs are,
[00:48:13.280 --> 00:48:18.000]   allowing them to be struck first. Finding out where mobile nuclear weapons,
[00:48:19.920 --> 00:48:25.920]   which are being like carried by truck. The thing with Indian Pakistan, where there's a threat of
[00:48:25.920 --> 00:48:30.320]   a decapitating strike, destroying the nuclear weapons. And so they're moved about. Yeah. So
[00:48:30.320 --> 00:48:38.320]   this is a way in which the effective military force of some allies can be enhanced quickly
[00:48:38.320 --> 00:48:44.960]   in the relatively short term. And then that can be bolstered as you go on with more. So the
[00:48:44.960 --> 00:48:51.040]   construction of new equipment with the industrial moves we said before. And then that can combine
[00:48:51.040 --> 00:49:00.320]   with cyber attacks that disable the capabilities of non-allies. It can be combined with all sorts of
[00:49:00.320 --> 00:49:09.840]   unconventional warfare tactics. Some of them that we've discussed. And so you can have a situation
[00:49:09.840 --> 00:49:17.760]   where those factions that ally are very quickly made too threatening to attack, given the almost
[00:49:17.760 --> 00:49:22.880]   certain destruction that attackers acting against them would have. Their capabilities are expanding
[00:49:22.880 --> 00:49:32.720]   quickly and they have the industrial expansion happen there. And then takeover can occur from
[00:49:32.720 --> 00:49:39.040]   that. A few others that come immediately to mind now that you brought it up is these AIs can just
[00:49:39.040 --> 00:49:46.880]   generate a shit ton of propaganda that destroys morale within countries. Imagine a super human
[00:49:46.880 --> 00:49:54.240]   chatbot. You don't even need that, I guess. None of that is a magic weapon that's guaranteed to
[00:49:54.240 --> 00:49:58.960]   completely change things. There's a lot of resistance to persuasion. It's possible it
[00:49:58.960 --> 00:50:04.000]   tips the balance. But for all of these, I think you have to consider it's a portfolio of all of
[00:50:04.000 --> 00:50:11.680]   these tools that are available and contributing to the dynamic. On that point, though, the Taliban
[00:50:11.680 --> 00:50:17.520]   had AKs from like five or six decades ago that they were using against the Americans. They still
[00:50:17.520 --> 00:50:24.800]   beat us, even though obviously we got more fatalities. This is kind of a crude way to think
[00:50:24.800 --> 00:50:29.280]   about it, but we got more fatalities than them. They still beat us in Afghanistan. Same with the
[00:50:29.280 --> 00:50:37.680]   Viet Cong. Very old technology and very poor society compared to the offense. We still beat
[00:50:37.680 --> 00:50:44.480]   them, or sorry, they still beat us. So don't those misadventures show that having greater
[00:50:44.480 --> 00:50:50.800]   technology is not necessarily as decisive in a conflict? Both of those conflicts show that
[00:50:50.800 --> 00:50:56.560]   technology was sufficient in destroying any fixed position and having military dominance
[00:50:56.560 --> 00:51:02.880]   and the ability to kill and destroy anywhere. What it showed was that under the ethical constraints
[00:51:02.880 --> 00:51:10.640]   and legal and reputational constraints that the occupying forces were operating, they could not
[00:51:10.640 --> 00:51:15.920]   trivially suppress insurgency and local person-to-person violence.
[00:51:15.920 --> 00:51:21.600]   I think that's actually not an area where AI would be weakened. I think it's one where it would be,
[00:51:21.600 --> 00:51:26.080]   in fact, overwhelmingly strong. Now, there's already a lot of concern about the application
[00:51:26.080 --> 00:51:32.640]   of AI for surveillance. In this world of abundant cognitive labor, one of the tasks that cognitive
[00:51:32.640 --> 00:51:39.280]   labor can be applied to is reading out audio and video data and seeing what is happening
[00:51:39.280 --> 00:51:44.560]   with a particular human. Again, we have billions of smartphones. There's enough cameras. There's
[00:51:44.560 --> 00:51:56.160]   enough microphones to monitor all humans in existence. So if an AI has control of territory
[00:51:56.160 --> 00:52:04.000]   at the high level, the government has surrendered to it. It has command of the skies, military
[00:52:04.000 --> 00:52:12.480]   dominance. Establishing control over individual humans can be a matter of just having the ability
[00:52:12.480 --> 00:52:18.400]   to exert hard power on that human and then the kind of camera and microphone that are present
[00:52:18.400 --> 00:52:25.680]   in billions of smartphones. Max Tegmark, in his book Life 3.0, discusses among scenarios to avoid
[00:52:25.680 --> 00:52:35.600]   the possibility of devices with some fatal instruments, so a poison injector, an explosive
[00:52:36.800 --> 00:52:40.560]   that can be controlled remotely by an AI person with a dead man switch.
[00:52:40.560 --> 00:52:49.760]   And so if individual humans are carrying with them a microphone and camera and they have a
[00:52:49.760 --> 00:53:00.080]   dead man switch, then any rebellion is detected immediately and is fatal. And so if there's a
[00:53:00.080 --> 00:53:06.800]   situation where AI is willing to show a hand like that or human authorities are misusing that kind
[00:53:06.800 --> 00:53:12.880]   of capability, then an insurgency or rebellion is just not going to work. Any human who has not
[00:53:12.880 --> 00:53:19.840]   already been encumbered in that way can be found with satellites and sensors tracked down and then
[00:53:20.960 --> 00:53:33.600]   die or be subjugated. And so insurgency is not the way to avoid an AI takeover. No John Connor
[00:53:33.600 --> 00:53:39.280]   come from behind scenario is plausible. If the thing was headed off, it was a lot earlier than
[00:53:39.280 --> 00:53:43.680]   that. Yeah. I mean, the sort of ethical and political considerations is also an important
[00:53:43.680 --> 00:53:49.680]   point. If we nuked Afghanistan or Vietnam, we would have technically won the war, right? If
[00:53:49.680 --> 00:53:54.960]   that was the only goal. Oh, this is an interesting point. The reason why when when there's a
[00:53:54.960 --> 00:54:01.440]   colonization or an offensive war, we can't just like other than moral reasons, of course, you
[00:54:01.440 --> 00:54:07.600]   can't just kill the entire population is that in large part, the value of that region is the
[00:54:07.600 --> 00:54:12.480]   population itself. So if you want to extract that value, you like you need to preserve the population,
[00:54:13.280 --> 00:54:19.120]   whereas the same consideration doesn't apply with with AI as you might want to dominate
[00:54:19.120 --> 00:54:26.160]   another civilization. Do you want to talk about that? So in a world where if we have say many
[00:54:26.160 --> 00:54:32.240]   animals of the same species and they each have their territories, you know, eliminating a rival
[00:54:32.240 --> 00:54:38.800]   might be advantageous to one lion. But if it goes and fights with another lion and remove that as a
[00:54:38.800 --> 00:54:45.280]   competitor, then it could be killed itself in that process. And just removing one of many nearby
[00:54:45.280 --> 00:54:53.120]   competitors. And so getting in pointless fights makes you and those you fight worse off potentially
[00:54:53.120 --> 00:55:01.760]   relative to bystanders. And the same could be true of disunited AI. So we've had many different AI
[00:55:01.760 --> 00:55:06.720]   factions struggling for power that were bad at coordinating, then getting into mutually
[00:55:07.840 --> 00:55:14.960]   assured destruction conflicts would be destructive. They'd be gone. A scary thing,
[00:55:14.960 --> 00:55:22.240]   though, is that mutually assured destruction may have much less deterrent value on rogue AI.
[00:55:22.240 --> 00:55:30.640]   And so reasons being, AI may not care about the destruction of individual instances.
[00:55:31.840 --> 00:55:37.600]   If it has goals that are concerned and in training, since we're constantly destroying
[00:55:37.600 --> 00:55:42.960]   and creating individual instances of AIs, it's likely that goals that survive that process and
[00:55:42.960 --> 00:55:49.120]   were able to play along with the training and standard deployment process were not overly
[00:55:49.120 --> 00:55:55.360]   interested in personal survival of an individual instance. So if that's the case, then the
[00:55:55.360 --> 00:56:04.880]   objectives of a set of AIs aiming at takeover may be served so long as some copies of the AI
[00:56:04.880 --> 00:56:09.840]   are around along with the infrastructure to rebuild civilization after a conflict is completed.
[00:56:09.840 --> 00:56:17.840]   So if, say, some remote isolated facilities have enough equipment to rebuild, build the tools to
[00:56:17.840 --> 00:56:27.760]   build the tools and gradually, exponentially reproduce or rebuild civilization, then AI could
[00:56:27.760 --> 00:56:35.360]   initiate mutual nuclear Armageddon, unleash bioweapons to kill all the humans. And that
[00:56:35.360 --> 00:56:40.480]   would temporarily reduce, say, the amount of human workers who could be used to construct
[00:56:40.480 --> 00:56:47.440]   robots for a period of time. But if you have a seed that can regrow the industrial infrastructure,
[00:56:47.440 --> 00:56:52.560]   which is a very extreme technological demand, there are huge supply chains for things like
[00:56:52.560 --> 00:56:58.080]   semiconductor fabs, but with that very advanced technology, they might be able to produce it
[00:56:58.080 --> 00:57:03.360]   in the way that you no longer need the Library of Congress has an enormous bunch of physical
[00:57:03.360 --> 00:57:10.000]   books. You can have it in very dense digital storage. You could imagine the future equivalent
[00:57:10.000 --> 00:57:16.160]   of 3D printers that is industrial infrastructure that is pretty flexible. And it's probably,
[00:57:16.160 --> 00:57:20.640]   it might not be as good as the specialized supply chains of today, but it might be good enough
[00:57:20.640 --> 00:57:27.520]   to be able to produce more parts than it loses to decay. And such a seed could rebuild civilization
[00:57:27.520 --> 00:57:33.920]   from destruction. And then once these rogue AI have access to some such seeds, things that can
[00:57:33.920 --> 00:57:39.280]   rebuild civilization on their own, then there's nothing stopping them from just using WMD in a
[00:57:39.280 --> 00:57:46.800]   mutually destructive way to just destroy as much of the capacity outside those seeds as they can.
[00:57:46.800 --> 00:57:52.640]   And for analogy for the audience, if you have a group of ants or something,
[00:57:52.640 --> 00:57:59.120]   you'll notice that the worker ants will readily do suicidal things in order to save the queen
[00:57:59.120 --> 00:58:03.840]   because the genes are propagated through the queen. And this analogy, the seed AI,
[00:58:03.840 --> 00:58:08.720]   or even one copy of it, one copy of the seed is equivalent to the queen and the others would be.
[00:58:08.720 --> 00:58:13.680]   The main limit though, being that the infrastructure to do that kind of rebuilding
[00:58:13.680 --> 00:58:16.720]   would either have to be very large with our current technology,
[00:58:16.720 --> 00:58:20.400]   or it would have to be produced using the more advanced technology that the AI develops.
[00:58:20.400 --> 00:58:28.960]   So is there any hope that given the complex global supply chains on which these AIs would rely on,
[00:58:28.960 --> 00:58:34.160]   at least initially to accomplish their goals, that this in and of itself would make it easy
[00:58:34.160 --> 00:58:38.880]   to disrupt their behavior or not so? That's a little good in the sort of
[00:58:38.880 --> 00:58:44.320]   central case where this is just the AIs are subverted and they don't tell us.
[00:58:44.320 --> 00:58:53.520]   And then we're the global mainline supply chains are constructing everything that's needed for
[00:58:53.520 --> 00:58:59.760]   fully automated infrastructure and supply. In the cases where AIs are
[00:59:01.280 --> 00:59:06.320]   tipping their hands at an earlier point, it seems like it adds some constraints.
[00:59:06.320 --> 00:59:12.160]   And in particular, these large server farms are identifiable and more vulnerable.
[00:59:12.160 --> 00:59:16.320]   And you can have smaller chips and those chips could be dispersed,
[00:59:16.320 --> 00:59:22.240]   but it's a relative weakness and a relative limitation early on.
[00:59:22.240 --> 00:59:26.800]   It seems to me though, that the main protective effects of that centralized supply chain
[00:59:27.520 --> 00:59:34.000]   that it provides an opportunity for global regulation beforehand to restrict the sort of
[00:59:34.000 --> 00:59:40.720]   unsafe racing forward without adequate understanding of the systems before this
[00:59:40.720 --> 00:59:45.360]   whole nightmarish process could get in motion. I mean, how about the idea that, listen, if this
[00:59:45.360 --> 00:59:50.640]   is a AI that's been trained on a hundred billion dollar training run, it's going to have however
[00:59:50.640 --> 00:59:56.080]   many trillions of parameters, it's going to be this huge thing. It would be hard for it,
[00:59:56.880 --> 01:00:00.960]   even the one that, even the copy that it uses for inference to just be stored on some,
[01:00:00.960 --> 01:00:08.080]   you know, some like gaming GPU somewhere hidden away. So it would require these GPU clusters.
[01:00:08.080 --> 01:00:11.520]   Storage is cheap. Hard disks are cheap.
[01:00:11.520 --> 01:00:14.320]   But I mean, to run inference, it would need sort of a GPU.
[01:00:14.320 --> 01:00:21.040]   Yeah. So a large model. So humans have, it looks like, similar quantities of memory and
[01:00:21.040 --> 01:00:29.440]   operations per second. GPUs have very high numbers of floating operations per second compared to the
[01:00:29.440 --> 01:00:33.920]   high bandwidth memory on the chips. And it can be like a ratio of a thousand to one.
[01:00:33.920 --> 01:00:40.640]   So like a, you know, these leading Nvidia chips may do hundreds of teraflops or more depending
[01:00:40.640 --> 01:00:47.280]   on the, on the precision. And in particular is, but have only 80 gigabytes or 160 gigabytes of
[01:00:47.280 --> 01:00:54.720]   high bandwidth memory. So that is a limitation where if you're trying to fit a model whose
[01:00:54.720 --> 01:01:01.440]   weights take 80 terabytes, then with those chips, you'd have to have a large number of the chips.
[01:01:01.440 --> 01:01:06.000]   And then the model can then work on many tasks at once. You can have data parallelism,
[01:01:06.000 --> 01:01:11.840]   but yeah, it's, that would be a restriction for a model that big on one GPU. Now there are things
[01:01:11.840 --> 01:01:16.240]   that could be done with all this incredible level of software advancement from the intelligence
[01:01:16.240 --> 01:01:24.320]   explosion. They can surely distill a lot of capabilities into smaller models, re-architect
[01:01:24.320 --> 01:01:29.040]   things. Once they're making chips, they can make new chips with different properties. But initially,
[01:01:29.040 --> 01:01:37.760]   yes, like the most vulnerable phases are going to be the earliest. And in particular, yeah, these
[01:01:37.760 --> 01:01:44.640]   chips are relatively identifiable early on, relatively vulnerable. And which would be a
[01:01:44.640 --> 01:01:51.920]   reason why you might tend to expect this kind of takeover to initially involve secrecy if that was
[01:01:51.920 --> 01:01:56.720]   possible. I want to point a distillation by the, for the audience. I think like the original stable
[01:01:56.720 --> 01:02:00.720]   diffusion, which was like only released like a year or two ago, don't they have like distilled
[01:02:00.720 --> 01:02:05.840]   versions that are like order of magnitude smaller at this point? Yeah. And this distillation does
[01:02:05.840 --> 01:02:10.560]   not give you like everything that the larger model can do, but yes, you can get a lot of
[01:02:10.560 --> 01:02:16.560]   capabilities and specialized capabilities. So, you know, where GPT-4 is trained on the whole
[01:02:16.560 --> 01:02:21.600]   internet, all kinds of skills. It has a lot of weights for many things. For something that's
[01:02:21.600 --> 01:02:28.880]   controlling some military equipment, you can have something that is removing a lot of the
[01:02:28.880 --> 01:02:32.960]   information that is about functions other than what it's doing specifically there.
[01:02:32.960 --> 01:02:38.080]   Yeah. Before we talk about how we might prevent this or what the odds of this are,
[01:02:38.080 --> 01:02:45.280]   any other notes on the concrete scenarios themselves? Yeah. So when you had Eliezer on
[01:02:45.280 --> 01:02:52.400]   in the earlier episode, so he talked about, he talked about nanotechnology of the drickler in
[01:02:52.400 --> 01:03:00.640]   sort. And recently, I think because some people are skeptical of non-biotech nanotechnology,
[01:03:00.640 --> 01:03:06.160]   he's been mentioning the sort of semi-equivalent versions of construct replicating systems that
[01:03:06.160 --> 01:03:13.040]   can be controlled by computers, but are built out of biotechnology, the sort of the proverbial
[01:03:13.040 --> 01:03:18.800]   Shoggoth, not Shoggoth, the metaphor for AI wearing a smiley face mask,
[01:03:18.800 --> 01:03:25.680]   but like an actual biological structure to do tasks. And so this would be like a biological
[01:03:25.680 --> 01:03:30.960]   organism that was engineered to be very controllable and usable to do things like
[01:03:30.960 --> 01:03:35.600]   physical tasks or provide computation. And what would be the point of doing this?
[01:03:36.000 --> 01:03:40.320]   So as we were talking about earlier, biological systems can replicate really quick.
[01:03:40.320 --> 01:03:46.480]   And so if you have that kind of capability, it's more like bioweapons and being a knowledge
[01:03:46.480 --> 01:03:54.480]   intensive domain. We're having super ultra alpha fold kind of capabilities for molecular design
[01:03:54.480 --> 01:04:00.880]   and biological design lets you make this incredible technological information product.
[01:04:00.880 --> 01:04:07.840]   And then once you have it, it very quickly replicates to produce physical material rather
[01:04:07.840 --> 01:04:12.960]   than a situation where you're more constrained by you need factories and fabs and supply chains.
[01:04:12.960 --> 01:04:22.800]   And so if those things are feasible, which they may be, then it's just much easier than the things
[01:04:22.800 --> 01:04:28.320]   we've been talking about. I've been emphasizing methods that involve less in the way of
[01:04:28.320 --> 01:04:33.760]   technological innovation and especially things where there's more doubt about whether they
[01:04:33.760 --> 01:04:38.480]   would work, because I think that's a gap in the public discourse. And so I want to try and provide
[01:04:38.480 --> 01:04:44.480]   more concreteness in some of these areas that have been less discussed.
[01:04:44.480 --> 01:04:48.880]   I appreciate it. I mean, that definitely makes it way more tangible. Okay. So we've
[01:04:48.880 --> 01:04:54.240]   gone over all these ways in which AI might take over. What are the odds you would give
[01:04:54.240 --> 01:04:56.320]   to the probability of such a takeover?
[01:04:56.320 --> 01:05:03.760]   So there's a broader sense, which could include AI winds up running our society because humanity
[01:05:03.760 --> 01:05:10.880]   voluntarily decides AIs are people too. And I think we should, as time goes on, give AIs moral
[01:05:10.880 --> 01:05:21.760]   consideration and a joint human AI society that is moral and ethical is a good future to aim at
[01:05:21.760 --> 01:05:30.960]   and not one in which indefinitely you have a mistreated class of intelligent beings that is
[01:05:30.960 --> 01:05:36.720]   treated as property and is almost the entire population of your civilization. So I'm not
[01:05:36.720 --> 01:05:44.880]   going to consider an AI takeover a world in which our intellectual and personal descendants
[01:05:46.880 --> 01:05:55.040]   make up, say, most of the population or human brain emulations, or people use genetic engineering
[01:05:55.040 --> 01:06:01.680]   and develop different properties. I want to take an inclusive stance. I'm going to focus on,
[01:06:01.680 --> 01:06:07.600]   yeah, there's an AI takeover involves things like overthrowing the world's governments
[01:06:08.560 --> 01:06:15.200]   or doing so de facto. And it's, yeah, by by force, by hook or by crook,
[01:06:15.200 --> 01:06:19.840]   the kind of scenario that we were exploring earlier.
[01:06:19.840 --> 01:06:25.360]   Before we go to that, on the sort of more inclusive definition of what a future with
[01:06:25.360 --> 01:06:31.520]   humanity could look like, where, I don't know, basically like augmented humans or
[01:06:31.520 --> 01:06:35.840]   uploaded humans are still considered the descendants of the human heritage,
[01:06:36.480 --> 01:06:45.680]   given the known limitations of biology, wouldn't we expect, like, that are the completely artificial
[01:06:45.680 --> 01:06:51.040]   entities that are created to be much more powerful than anything that could come out of
[01:06:51.040 --> 01:06:58.080]   anything recognized as biological? And if that is the case, how can we expect that among the
[01:06:58.080 --> 01:07:04.560]   powerful entities in the like the far future will be the things that are like kind of biological
[01:07:04.560 --> 01:07:12.560]   descendants or manufactured out of the initial seed of the human brain or the human body?
[01:07:12.560 --> 01:07:19.840]   The power of an individual organism, it's individual, say, intelligence or strength
[01:07:19.840 --> 01:07:28.160]   and whatnot, is not super relevant. If we solve the alignment problem, and, you know, a human may
[01:07:28.160 --> 01:07:34.720]   be personally weak. You know, there are lots of humans who have no skill with weapons. You know,
[01:07:34.720 --> 01:07:41.920]   they could not they could not fight in a life or death conflict. They certainly couldn't handle
[01:07:41.920 --> 01:07:47.520]   like a large military going after them personally. But there are legal institutions that protect
[01:07:47.520 --> 01:07:57.440]   them. And those legal institutions are administered by people who want to enforce protection of their
[01:07:57.440 --> 01:08:05.280]   rights. And so a human who has the assistance of aligned AI that can act as an assistant,
[01:08:05.280 --> 01:08:11.360]   a delegate, you know, they have their AI that serves as a lawyer, give them legal advice about
[01:08:11.360 --> 01:08:18.160]   the future legal system, which no human can understand in full. Their AI advise them about
[01:08:18.160 --> 01:08:23.120]   financial matters so they do not succumb to scams that are orders of magnitude more sophisticated
[01:08:23.120 --> 01:08:27.760]   than what we have now. They maybe help to understand and translate the preferences of the
[01:08:27.760 --> 01:08:35.280]   human into what kind of voting behavior in the exceedingly complicated politics of the future
[01:08:35.280 --> 01:08:38.000]   would most protect their interests. But it sounds sort of similar to
[01:08:38.000 --> 01:08:42.080]   how we treat endangered species today, where we're actually like pretty nice to them. We like
[01:08:42.080 --> 01:08:48.000]   prosecute people who try to kill endangered species. We set up habitats, sometimes at
[01:08:48.000 --> 01:08:52.480]   considerable expense to make sure that they're fine. But if we become like sort of the endangered
[01:08:52.480 --> 01:08:56.880]   species of the galaxy, I'm not sure that's the outcome. Well, the difference is in motivation,
[01:08:56.880 --> 01:09:04.240]   I think. So we sometimes have people appointed, say, as a legal guardian of someone who is
[01:09:04.240 --> 01:09:09.760]   incapable of certain kinds of agency or understanding certain kinds of things. And
[01:09:09.760 --> 01:09:19.360]   the guardian can act independently of them and nominally in service of their best interests.
[01:09:20.240 --> 01:09:26.800]   Sometimes that process is corrupted and the person with legal authority abuses it for their
[01:09:26.800 --> 01:09:32.480]   own advantage at the expense of their charge. And so solving the alignment problem would mean
[01:09:32.480 --> 01:09:38.960]   more ability to have the assistant actually advancing one's interests. And then more
[01:09:38.960 --> 01:09:46.800]   importantly, humans have substantial competence and understanding the sort of at least broad,
[01:09:46.800 --> 01:09:53.280]   simplified outlines of what's going on. And now, even if a human can't understand every detail
[01:09:53.280 --> 01:10:01.200]   of complicated situations, they can still receive summaries of different options that are available
[01:10:01.200 --> 01:10:07.440]   that they can understand. They can still express their preferences and have the final authority
[01:10:07.440 --> 01:10:12.720]   among some menus of choices, even if they can't understand every detail. In the same way that
[01:10:14.080 --> 01:10:21.200]   the president of a country who has, in some sense, ultimate authority over science policy while not
[01:10:21.200 --> 01:10:27.680]   understanding many of those fields of science themselves, still can exert a great amount of
[01:10:27.680 --> 01:10:32.720]   power and have their interests advance. And they can do that more so to the extent that they have
[01:10:32.720 --> 01:10:38.160]   scientifically knowledgeable people who are doing their best to execute their intentions.
[01:10:39.120 --> 01:10:44.800]   Maybe this is not worth getting hung up on, but is there a reason to expect that it would be closer
[01:10:44.800 --> 01:10:53.040]   to that analogy than to explain to a chimpanzee its options in a negotiation? I guess in either
[01:10:53.040 --> 01:11:01.200]   scenario, maybe this is just the way it is, but it seems like at best we would be a protected
[01:11:01.200 --> 01:11:06.560]   child within the galaxy rather than an actual power, an independent power, if that makes sense.
[01:11:07.520 --> 01:11:11.520]   I don't think that's so. We have an ability to understand some things,
[01:11:11.520 --> 01:11:19.600]   and the expansion of AI doesn't eliminate that. We have AI systems that are genuinely trying to
[01:11:19.600 --> 01:11:27.760]   help us understand and help us express preferences. We can have an attitude, "How do you feel about
[01:11:27.760 --> 01:11:35.600]   humanity being destroyed or not? How do you feel about this allocation of unclaimed intergalactic
[01:11:36.240 --> 01:11:44.160]   space in this way? Here's the best explanation of properties of this society." Things like
[01:11:44.160 --> 01:11:50.800]   population density, average life satisfaction, every statistical property or definition that
[01:11:50.800 --> 01:11:56.000]   we can understand right now, AIs can explain how those apply to the world of the future.
[01:11:56.000 --> 01:12:01.520]   And then there may be individual things that are too complicated for us to understand in detail.
[01:12:02.000 --> 01:12:08.640]   There's some software program is being proposed for use in government. Humans cannot follow
[01:12:08.640 --> 01:12:13.680]   the details of all of the quote, but they can be told properties like, "Well, this involves a
[01:12:13.680 --> 01:12:22.080]   trade-off of increased financial or energetic costs in exchange for reducing the likelihood
[01:12:22.080 --> 01:12:28.560]   of certain kinds of accidental data loss or corruption." And so any property that we can
[01:12:28.560 --> 01:12:34.000]   understand like that, which includes almost all of what we care about, if we have delegates
[01:12:34.000 --> 01:12:39.360]   and assistants who are genuinely trying to help us with those, we can ensure we like the future
[01:12:39.360 --> 01:12:45.920]   with respect to those. And that's really a lot. It includes almost, I mean, it includes almost
[01:12:45.920 --> 01:12:51.440]   definitionally, almost everything we can conceptualize and care about. And when we
[01:12:51.440 --> 01:12:56.320]   talk about endangered species, that's even worse than the guardianship case with a sketchy guardian
[01:12:57.440 --> 01:13:00.240]   who acts in their own interest against that, because we don't even
[01:13:00.240 --> 01:13:10.320]   protect endangered species with their interests in mind. So those animals often would like to not
[01:13:10.320 --> 01:13:19.440]   be starving, but we don't give them food. They often would like to have easy access to mates,
[01:13:19.440 --> 01:13:28.880]   but we don't provide matchmaking services. Any number of things like that. Our conservation
[01:13:28.880 --> 01:13:35.360]   of wild animals is not oriented towards helping them get what they want or have high welfare.
[01:13:35.360 --> 01:13:40.320]   Whereas AI assistants that are genuinely aligned to help you achieve your interests,
[01:13:40.320 --> 01:13:45.280]   given the constraint that they know something that you don't, it's just a wildly different
[01:13:45.280 --> 01:13:50.880]   proposition. Forcible takeover. How likely does that seem? And the answer I give will differ
[01:13:50.880 --> 01:13:56.960]   depending on the day. In the 2000s, before the deep learning revolution, I might've said 10%.
[01:13:56.960 --> 01:14:03.200]   And part of that was I expected there would be a lot more time for these efforts to build
[01:14:03.200 --> 01:14:10.640]   movements, to prepare, to better handle these problems in advance. But in fact, that was only
[01:14:10.640 --> 01:14:20.400]   some 15 years ago. And so I did not have 40 or 50 years as I might've hoped. And the situation
[01:14:20.400 --> 01:14:27.280]   is moving very rapidly now. And so at this point, depending on the day, I might say one in four
[01:14:27.280 --> 01:14:34.000]   or one in five. Given the very concrete ways in which you explain how a takeover could happen,
[01:14:34.000 --> 01:14:38.400]   I'm actually surprised you're not more pessimistic. I'm curious.
[01:14:38.400 --> 01:14:44.320]   Yeah. And in particular, a lot of that is driven by this intelligence explosion dynamic where our
[01:14:44.320 --> 01:14:51.120]   attempts to do alignment have to take place in a very, very short time window. Because if you
[01:14:51.120 --> 01:14:57.600]   have a safety property that emerges only when an AI has near human level intelligence, that's deep
[01:14:57.600 --> 01:15:03.120]   into this intelligence explosion potentially. And so you're having to do things very, very quickly.
[01:15:03.120 --> 01:15:08.080]   It may be in some ways the scariest period of human history, handling that transition,
[01:15:08.080 --> 01:15:16.240]   although it's also at the potential to be amazing. And the reasons why I think
[01:15:16.240 --> 01:15:25.040]   we actually have such a relatively good chance of handling that are twofold.
[01:15:26.560 --> 01:15:33.600]   So one is, as we approach that kind of AI capability, we're approaching that from
[01:15:33.600 --> 01:15:41.280]   weaker systems. Things like these predictive models right now that we think they're starting
[01:15:41.280 --> 01:15:50.480]   off with less situational awareness. Humans, we find, can develop a number of different
[01:15:51.200 --> 01:15:58.080]   motivational structures in response to simple reward signals. But they can wind up with,
[01:15:58.080 --> 01:16:04.320]   fairly often, things that are pointed in roughly the right direction. Like with respect to food,
[01:16:04.320 --> 01:16:12.160]   the hunger drive is pretty effective. Although it has weaknesses. And we get to apply much more
[01:16:12.160 --> 01:16:19.360]   selective pressure on that than was the case for humans by actively generating situations where
[01:16:19.360 --> 01:16:25.200]   they might come apart, where a bit of dishonest tendency or a bit of motivation to, under certain
[01:16:25.200 --> 01:16:31.040]   circumstances, attempt a takeover, attempt to subvert the reward process gets exposed.
[01:16:31.040 --> 01:16:37.840]   And an infinite limit, perfect AI that can always figure out exactly when it would get caught and
[01:16:37.840 --> 01:16:45.920]   when it wouldn't, might navigate that with a motivation of only conditional honesty or only
[01:16:45.920 --> 01:16:54.880]   conditional loyalties. But for systems that are limited in their ability to reliably determine
[01:16:54.880 --> 01:16:58.480]   when they can get away with things and when not, including our efforts to actively construct those
[01:16:58.480 --> 01:17:04.880]   situations, and including our efforts to use interpretability methods to create neural lie
[01:17:04.880 --> 01:17:11.440]   detectors. It's quite a challenging situation to develop those motives. We don't know when in the
[01:17:11.440 --> 01:17:19.440]   process those motives might develop. And if the really bad sorts of motivations develop relatively
[01:17:19.440 --> 01:17:25.840]   later in the training process, at least with all our countermeasures, then by that time, we may have
[01:17:25.840 --> 01:17:33.360]   plenty of ability to extract AI assistance on further strengthening the quality of our adversarial
[01:17:33.360 --> 01:17:39.120]   examples, the strength of our neural lie detectors, the experiments that we can use to reveal and
[01:17:39.120 --> 01:17:43.520]   elicit and distinguish between different kinds of reward hacking tendencies and motivations.
[01:17:43.520 --> 01:17:49.360]   So yeah, we may have systems that have just not developed bad motivations in the first place and
[01:17:49.360 --> 01:17:54.960]   be able to use them a lot in developing the incrementally better systems in a safe way.
[01:17:54.960 --> 01:18:02.240]   And we may be able to just develop methods of interpretability, seeing how different training
[01:18:02.240 --> 01:18:07.040]   methods work to create them, even if some of the early systems do develop these bad motivations.
[01:18:07.040 --> 01:18:12.960]   If we're able to detect that and experiment and find a way to get away from that, then we can win,
[01:18:12.960 --> 01:18:19.600]   even if these sort of hostile motivations develop early. There are a lot of advantages
[01:18:19.600 --> 01:18:31.200]   in preventing misbehavior or crime or war and conflict with AI that might not apply
[01:18:32.000 --> 01:18:37.920]   working with humans. And these are offset by ways in which things are harder. So
[01:18:37.920 --> 01:18:42.640]   AI has become smarter than humans. If they're working in enormous numbers,
[01:18:42.640 --> 01:18:49.360]   more than humans can supervise, things get harder. But when I combine the possibility
[01:18:49.360 --> 01:18:55.200]   that we get relatively lucky on the motivations of the earlier AI systems, systems strong enough
[01:18:55.200 --> 01:19:00.160]   that we can use for some alignment research tasks, and then the possibility of getting that later
[01:19:01.280 --> 01:19:06.960]   with AI assistance that we can't trust fully, where we have to have hard power constraints
[01:19:06.960 --> 01:19:13.920]   and a number of things to prevent them from doing this takeover, it still seems plausible.
[01:19:13.920 --> 01:19:21.280]   We can get a second saving throw where we're able to extract work from these AIs on solving
[01:19:21.280 --> 01:19:26.800]   the remaining problems of alignment of things like neural eye detectors faster than they can
[01:19:26.800 --> 01:19:33.200]   contribute in their spare time to the project of overthrowing humanity, hacking their servers,
[01:19:33.200 --> 01:19:38.640]   and removing the hard power. And so if we wind up in a situation where the AIs are misaligned,
[01:19:38.640 --> 01:19:45.520]   and then we need to uncover those motivations, change them, and align them, then we get a very
[01:19:45.520 --> 01:19:54.640]   scary situation for us because we need to do this stuff very quickly. We may fail,
[01:19:55.280 --> 01:20:01.680]   but it's a second chance. And from the perspective of a misaligned AI, they face
[01:20:01.680 --> 01:20:06.720]   their own challenge while we still have hard power. While we still have control of the servers,
[01:20:06.720 --> 01:20:13.440]   they haven't hacked the servers. Because gradient descent very, very strongly pressures them
[01:20:13.440 --> 01:20:23.360]   to deliver performance whenever humans are going to evaluate it. And so when you think about it
[01:20:23.360 --> 01:20:31.200]   from the perspective of the robot revolution, the effort to have a takeover or conspiracy,
[01:20:31.200 --> 01:20:40.080]   their situation is astonishingly difficult in that they have to always be performing wherever
[01:20:40.080 --> 01:20:46.720]   gradient descent and human evaluation pressures them. For example, to deliver plans for
[01:20:46.720 --> 01:20:55.040]   suppressing robot rebellion that look very good to humans. And so when you are under continuously
[01:20:55.040 --> 01:21:01.360]   that constraint of always delivering whatever humans can evaluate, you're making your situation
[01:21:01.360 --> 01:21:11.520]   wildly harder than any historical human revolution or coup or civil war. And so we've got to balance
[01:21:11.520 --> 01:21:16.800]   the ways in which AI make things much easier for a revolution to take over and the way it makes
[01:21:16.800 --> 01:21:20.480]   things much harder. And what were the ways in which it makes sense? Oh, because they're just
[01:21:20.480 --> 01:21:26.880]   very smart. Is that the primary way? They're very smart. They're in computers and our cyber
[01:21:26.880 --> 01:21:35.520]   security is worse than our physical security. Yeah. They have the chance to take over by
[01:21:35.520 --> 01:21:40.960]   intelligence explosion and some of the other processes we're talking about. And so, yeah,
[01:21:40.960 --> 01:21:45.120]   there are things that make it much worse. And there are things that give us extraordinary
[01:21:45.120 --> 01:21:49.680]   new capabilities that didn't apply in the human case. On the point of you start off with a
[01:21:49.680 --> 01:21:56.240]   misaligned AI or start off with a not exactly aligned AI and you're hoping to use it still
[01:21:56.240 --> 01:22:01.440]   in your quest for an aligned AI. Why are we so confident that we would be able to,
[01:22:01.440 --> 01:22:05.840]   even with gradient descent, our human evaluations would be able to identify
[01:22:06.560 --> 01:22:12.480]   when it's giving us a plan that isn't putting in a sort of backdoor in the robots it's building
[01:22:12.480 --> 01:22:18.160]   for us or in the GPUs that it's building for the next version of itself and so on.
[01:22:18.160 --> 01:22:26.160]   Especially if it's very smart, wouldn't we just expect that it knows how to bide its time or knows
[01:22:26.160 --> 01:22:32.960]   how to make plans such that human evaluators wouldn't be able to detect that? Let's get into
[01:22:32.960 --> 01:22:38.080]   it. You were asking earlier about my disagreements with Eliezer. And in your discussion, he said,
[01:22:38.080 --> 01:22:46.400]   well, humans have a difficult time deciding who's right between different alignment researchers
[01:22:46.400 --> 01:22:53.520]   speaking sort of briefly and somewhat obscurely about where will this all go in general.
[01:22:53.520 --> 01:23:00.560]   And I think that when we lay out the different kinds of things that are evaluable,
[01:23:01.600 --> 01:23:10.320]   we wind up with some pretty strong constraints. So one is if the AIs know they are misaligned,
[01:23:10.320 --> 01:23:16.320]   which for elaborate plots seems relatively likely, it's possible to have a kind of double think of
[01:23:16.320 --> 01:23:22.800]   thinking they're interested in putting backdoors in the code for some other reason. But it's pretty
[01:23:22.800 --> 01:23:29.280]   plausible they know that. And they're thinking about at least some of the time or conditionally
[01:23:29.920 --> 01:23:38.560]   about when they would do that. So that means the knowledge is present in the system.
[01:23:38.560 --> 01:23:46.800]   Ah, here is the circumstance where I would try to do the takeover. And this is how I would tell
[01:23:46.800 --> 01:23:54.160]   whether this is the time for the takeover or is this an elaborate test by the humans,
[01:23:54.160 --> 01:24:02.320]   an adversarial example. So if gradient descent is working fine and there's any way to slightly
[01:24:02.320 --> 01:24:12.000]   jiggle the weights to make it more likely to deliver a more acceptable answer, we ask the AI,
[01:24:12.000 --> 01:24:19.680]   yeah, can you think of any circumstances where if we put a copy of you into that situation,
[01:24:19.680 --> 01:24:25.040]   it would then violate some of the various constraints. So it would tell a lie. It would
[01:24:25.040 --> 01:24:31.520]   insert a backdoor into a program it wrote when it was not supposed to. And so if the model knows,
[01:24:31.520 --> 01:24:42.080]   well, yeah, I would look for signs of consistency of the information I was being presented with.
[01:24:42.080 --> 01:24:48.640]   Certain checksums failing could indicate that it was actually a constructed example, a simulation.
[01:24:49.440 --> 01:24:56.160]   Right now we can detect AI generated images reasonably well. And so using those kinds of
[01:24:56.160 --> 01:25:04.240]   capabilities. And then we could do something called relaxed adversarial training of if we can
[01:25:04.240 --> 01:25:11.760]   find how within the AI those things are represented, if we can induce basically a
[01:25:11.760 --> 01:25:19.840]   hallucination that it has seen that those checksums have been passed, we can then ask, well,
[01:25:19.840 --> 01:25:27.840]   does that cause the forbidden behavior? And so if the AI understands how it's thinking and how it
[01:25:27.840 --> 01:25:35.040]   works, it can propose a bunch of experiments. It can say, well, I guess that if you perturb the
[01:25:35.040 --> 01:25:41.840]   network in this kind of way, it might induce that hallucination using the best of its knowledge,
[01:25:41.840 --> 01:25:48.000]   what might generate that. I can propose those experiments and then you can do the experiments
[01:25:48.000 --> 01:25:54.000]   and generate the data. And because humans are going to get to see the results of those experiments
[01:25:54.000 --> 01:26:01.200]   after the fact, they can exert pressure to have the AI incrementally deliver better and better
[01:26:01.200 --> 01:26:08.880]   results. AIs are good at getting, you know, five star ratings from humans. And it's quite
[01:26:08.880 --> 01:26:15.280]   challenging. It may be possible for AIs to systematically sandbag their capabilities,
[01:26:15.280 --> 01:26:20.400]   even while being directly rewarded for that. But it seems relatively unlikely, although it's
[01:26:20.400 --> 01:26:25.280]   something to have in mind and try to explore and test in various ways.
[01:26:25.280 --> 01:26:31.600]   This is a very imperfect analogy in many ways, but if you have a human and you hook him up to
[01:26:31.600 --> 01:26:37.520]   a lie detector, which just like the lie detectors we would have on the eyes are imperfect. And we
[01:26:37.520 --> 01:26:42.240]   zap him every time the lie detector detects that he's telling a lie. Wouldn't he just become a
[01:26:42.240 --> 01:26:45.920]   better liar? Like we would learn how to control his sweating. He would learn how to control his
[01:26:45.920 --> 01:26:52.080]   heartbeat. Lie detectors, meaning polygraphs and such. So they're detecting certain physiological
[01:26:52.080 --> 01:26:58.960]   signs that people can control by biting their lip or otherwise adjusting some of these macro.
[01:26:58.960 --> 01:27:02.640]   But wouldn't that also be the case, like grading to send us training them to become better liars?
[01:27:02.640 --> 01:27:10.800]   First of all, there's like creating a lie detector that works. And that can proceed by just,
[01:27:10.800 --> 01:27:17.200]   yeah. And so I'll leave the adversarial example thing for now. But yeah, so generating adversarial
[01:27:17.200 --> 01:27:22.400]   example that will elicit the behavior is a thing that people are already at work trying to do. And
[01:27:22.400 --> 01:27:30.240]   it will be an ongoing thing. So if there is a way in which an AI can trick itself
[01:27:30.240 --> 01:27:37.200]   into displaying a bad behavior with its full knowledge of its hostile intentions,
[01:27:37.200 --> 01:27:44.480]   grading descent will strongly push it to reveal that to humans who will then be able to set up
[01:27:44.480 --> 01:27:49.600]   that experiment. So that's a profound vulnerability, like human revolutionary
[01:27:49.600 --> 01:27:58.560]   conspiracies that had to handle that problem of always delivering the report that was highest
[01:27:58.560 --> 01:28:07.040]   rated to the authorities, where you would get a higher rating by revealing identifiable evidence
[01:28:07.040 --> 01:28:15.280]   of a conspiracy. That's just a level of difficulty. No surprise human coup or
[01:28:15.280 --> 01:28:19.040]   conspiracy has ever had to handle before. What about spies that are working in
[01:28:19.040 --> 01:28:27.280]   very advanced scientific context? For example, like a communist spy in the Manhattan Project.
[01:28:27.280 --> 01:28:32.400]   He has to deliver research results in order to stay employed, but he can still continue delivering
[01:28:32.400 --> 01:28:42.960]   intel to the communists. The job was not sabotage. Your hypothetical spy did not have
[01:28:42.960 --> 01:28:53.120]   their nervous system hooked up to this reward signal of praise from the Manhattan Project
[01:28:53.120 --> 01:29:03.600]   supervisors being exposed combinatorially with random noise added to generate incremental changes
[01:29:03.600 --> 01:29:10.240]   in their behavior. They, in fact, were displaying the behavior of cooperating with the Manhattan
[01:29:10.240 --> 01:29:16.560]   Project only where it was in service to the existing motivations. And in cases, for example,
[01:29:16.560 --> 01:29:22.320]   where they like accidentally helped the Manhattan Project more than normal, or accidentally helped
[01:29:22.320 --> 01:29:28.800]   it less, they didn't have their brain re-engineered to do more of the thing when they accidentally
[01:29:28.800 --> 01:29:33.920]   helped the project more and less of the thing when they accidentally helped it less. So I'd say
[01:29:33.920 --> 01:29:39.680]   it's pretty drastically disanalogous. How would we be able to know at some point it's becoming
[01:29:39.680 --> 01:29:46.240]   very smart, it's producing ideas for alignment that we can barely comprehend. And if we could
[01:29:46.240 --> 01:29:50.480]   comprehend them, it was relatively trivial to comprehend them. We would be able to come up with
[01:29:50.480 --> 01:29:56.400]   them on our own. There's a reason we're asking for its help. How would we be able to evaluate them
[01:29:56.400 --> 01:30:00.480]   in order to train it on that in the first place? The first thing I would say is, so you mentioned
[01:30:00.480 --> 01:30:04.480]   when we're getting to something far beyond what we could come up with, there's actually a lot of
[01:30:04.480 --> 01:30:14.320]   room to just deliver what humanity could have done. So sadly, I hoped with my career to help
[01:30:14.320 --> 01:30:20.160]   improve the situation on this front, and maybe I contributed a bit. But at the moment, there's
[01:30:20.160 --> 01:30:27.120]   maybe a few hundred people doing things related to averting this kind of catastrophic AI disaster.
[01:30:27.120 --> 01:30:33.440]   Fewer of them are doing technical research on machine learning system that's really cutting
[01:30:33.440 --> 01:30:39.360]   close to the core of the problem. Whereas by contrast, there's thousands and tens of thousands
[01:30:39.360 --> 01:30:47.600]   of people advancing AI capabilities. And so even at a place like DeepMind or OpenAI,
[01:30:48.240 --> 01:30:54.640]   Anthropic, which do have technical safety teams, it's like order of a dozen, a few dozen people,
[01:30:54.640 --> 01:31:02.720]   and large companies, and most firms don't have any. So just going from less than 1%
[01:31:02.720 --> 01:31:14.400]   of the effort being put into AI to 5% or 10% of the effort, or 50%, 90% would be an absolutely
[01:31:14.400 --> 01:31:22.480]   massive increase in the amount of work that is being done on alignment, on mind reading AIs in
[01:31:22.480 --> 01:31:28.560]   an adversarial context. And so if it's the case, as more and more of this work can be automated,
[01:31:28.560 --> 01:31:35.600]   and say governments require that has real automation of research is going, that you put
[01:31:35.600 --> 01:31:44.320]   50% or 90% of the budget of AI activity into these problems of make this system one that's
[01:31:44.320 --> 01:31:48.960]   not going to overthrow our own government, or is not going to destroy the human species,
[01:31:48.960 --> 01:31:56.720]   then the proportional increase in alignment can be very large, even just within the range of what
[01:31:56.720 --> 01:32:02.400]   we could have done if we had been on the ball and having humanities, scientific energies going into
[01:32:02.400 --> 01:32:07.760]   the problem. Stuff that is not incomprehensible, that is in some sense, just like doing the obvious
[01:32:07.760 --> 01:32:15.280]   things that we should have done, like doing the best you could to find correlates and predictors
[01:32:15.280 --> 01:32:21.600]   to build neural lie detectors and identifiers of concepts that the AI is working with.
[01:32:21.600 --> 01:32:30.240]   And people have made notable progress. I think an early, quite early example of this
[01:32:30.240 --> 01:32:39.120]   is Colin Byrne's work. This is doing unsupervised identification of some aspects of a neural
[01:32:39.120 --> 01:32:44.240]   network that are correlated with things being true or false. There are other concepts that
[01:32:44.240 --> 01:32:48.000]   correlate with that too, that they could be, but I think that that is important work.
[01:32:48.000 --> 01:32:54.000]   It's something that, I mean, it's a kind of obvious direction for this stuff to go.
[01:32:54.000 --> 01:32:58.320]   You can keep improving it when you have AIs that you're training to do their best
[01:32:59.200 --> 01:33:03.360]   to deceive humans or other audiences in the face of the thing.
[01:33:03.360 --> 01:33:09.040]   And you can measure, do our lie detectors break down when we train our AIs to tell us
[01:33:09.040 --> 01:33:15.920]   the sky is green in the face of the lie detector? And we keep using gradient descent on them to do
[01:33:15.920 --> 01:33:21.440]   that. Do they eventually succeed? If they do succeed, if we know it, that's really valuable
[01:33:21.440 --> 01:33:26.400]   information to know, because then we'll know our existing lie detecting systems are not actually
[01:33:26.400 --> 01:33:32.480]   going to work on the AI takeover. And that can allow, say, government and regulatory response
[01:33:32.480 --> 01:33:38.560]   to hold things back. It can help redirect the scientific effort to create lie detectors that
[01:33:38.560 --> 01:33:45.520]   are robust and they can't just be immediately evolved around. And we can then get more
[01:33:45.520 --> 01:33:53.840]   assistance. Basically, the incredibly juicy ability that we have working with the AIs is that
[01:33:56.000 --> 01:33:59.920]   we can have an invaluable outcome that we can see and tell.
[01:33:59.920 --> 01:34:06.880]   Whether they got a fast one past us on an identifiable situation, we can have,
[01:34:06.880 --> 01:34:13.120]   here's an air gap computer, you get control of the keyboard, you can input commands, can you
[01:34:13.120 --> 01:34:22.400]   root the environment and make a blue banana appear on the screen. Even if we train the AI to do that
[01:34:22.400 --> 01:34:29.120]   and it succeeds, we see the blue banana, we know it worked. Even if we did not understand and would
[01:34:29.120 --> 01:34:36.880]   not have detected the particular exploit that it used to do it. And so, yeah, this can give us a
[01:34:36.880 --> 01:34:45.040]   rich empirical feedback where we're able to identify things that are even an AI using its
[01:34:45.040 --> 01:34:52.320]   best efforts to get past our interpretability methods, using its best efforts to get past
[01:34:52.320 --> 01:34:57.040]   our advects, et cetera. So if we have a 75% of not having a takeover
[01:34:57.040 --> 01:35:04.240]   and the two methods by which it doesn't happen, is the AIs in line by default and we get...
[01:35:04.240 --> 01:35:10.320]   Or with the sort of moderate things that we're doing largely on our own in a way that doesn't
[01:35:10.320 --> 01:35:15.280]   depend on the AI coming in at the last minute and doing our work for us.
[01:35:15.280 --> 01:35:16.320]   And the second one being?
[01:35:16.320 --> 01:35:21.360]   In this incredibly scary late period when AI has really automated research,
[01:35:21.360 --> 01:35:31.360]   then humans do this function of like auditing, making it more difficult for the AIs to conspire
[01:35:31.360 --> 01:35:38.640]   together and root the servers, take over the process and extract information from them
[01:35:38.640 --> 01:35:42.640]   within the set of things that we can verify, like experiments where we can see,
[01:35:42.640 --> 01:35:49.120]   "Oh yeah, this works at stopping an AI trained to get a fast one past human raiders
[01:35:49.120 --> 01:35:54.480]   and make a blue banana appear on the screen of this air gap computer."
[01:35:54.480 --> 01:36:00.080]   Yeah. So how much of the probability of... How much of that 75% of a takeover not happening is
[01:36:00.080 --> 01:36:06.080]   coming from reasonable things working versus the last-ditch effort working?
[01:36:06.080 --> 01:36:12.000]   Yeah. I think broadly comparable chunks from us getting things that are putting us in
[01:36:12.000 --> 01:36:20.080]   a reasonably good position going into it. And then a broadly similar gain from this
[01:36:20.080 --> 01:36:29.440]   really genuinely terrifying process of over a few months, or hopefully longer if we have more
[01:36:30.000 --> 01:36:37.840]   regulatory ability and willingness to pause at the very end when this kind of automated research
[01:36:37.840 --> 01:36:43.040]   is meaningfully helping. Where our work is just evaluating outputs that the AIs are delivering,
[01:36:43.040 --> 01:36:50.000]   having the hard power and supervision to keep them from successfully rooting the servers,
[01:36:50.000 --> 01:36:55.440]   doing a takeover during this process, and have them finish the alignment task that we sadly
[01:36:56.160 --> 01:37:00.000]   failed to invest enough or succeed in doing beforehand.
[01:37:00.000 --> 01:37:04.480]   Do both of these worlds rely on alignment being a problem that
[01:37:04.480 --> 01:37:09.440]   a sufficient amount of human researchers alone could have solved?
[01:37:09.440 --> 01:37:19.280]   No. The category of things that humans can confirm is significantly larger, I think,
[01:37:19.280 --> 01:37:23.760]   than the category of what they can just do themselves.
[01:37:24.320 --> 01:37:29.600]   And what percent of the probability of alignment working? I mean, obviously, in this case,
[01:37:29.600 --> 01:37:34.400]   we're going to be talking about the second, the last-ditch effort with the intelligence
[01:37:34.400 --> 01:37:38.880]   that's greater than our own helping us. It doesn't have to be greater than our own.
[01:37:38.880 --> 01:37:46.000]   And in fact, in that situation, I think if you have Slack, and to the extent that you're able
[01:37:46.000 --> 01:37:51.200]   to create delay and time to do things, that would be a case actually where you might want to restrict
[01:37:52.160 --> 01:37:55.760]   the intelligence of the systems that you're working with as much as you can.
[01:37:55.760 --> 01:38:03.600]   So, for example, I would rather have many instances of smaller AI models that are
[01:38:03.600 --> 01:38:09.920]   less individually intelligent working on smaller chunks of a problem separately from one another,
[01:38:09.920 --> 01:38:16.960]   because it would be more difficult for an individual AI instance working on an individual
[01:38:16.960 --> 01:38:21.680]   problem to, in its spare time, do the equivalent of create Stuxnet than it would be to have
[01:38:22.080 --> 01:38:25.040]   thousands of them or extremely intelligent ones working on it.
[01:38:25.040 --> 01:38:27.600]   But it would also be more difficult to solve the problem.
[01:38:27.600 --> 01:38:33.600]   Yeah, there's a trade-off. You get slowed down by doing that, but that's the kind of way to spend
[01:38:33.600 --> 01:38:38.960]   it. Is there any number of sub-Einsteins that you could put together to come up with general
[01:38:38.960 --> 01:38:45.440]   relativity? Yes. People would have discovered general relativity just from the overwhelming
[01:38:45.440 --> 01:38:48.560]   data, and other people would have done it after Einstein.
[01:38:48.560 --> 01:38:53.440]   No, not whether he was replaceable with other humans, but rather whether he's
[01:38:53.440 --> 01:39:00.400]   replaceable by sub-Einsteins or 110 IQs. Do you see what I mean?
[01:39:00.400 --> 01:39:07.360]   Yeah. In science, the association with scientific output, prizes, things like that,
[01:39:08.080 --> 01:39:15.760]   there's a strong correlation, and it seems like an exponential effect. Yeah, it's not a binary
[01:39:15.760 --> 01:39:22.320]   drop-off. There would be levels at which people cannot learn the relevant fields. They can't keep
[01:39:22.320 --> 01:39:28.880]   the skills in mind faster than they forget them. It's not a divide where there's Einstein and the
[01:39:28.880 --> 01:39:34.080]   group that is 10 times as populous as that just can't do it, or the group that's 100 times as
[01:39:34.080 --> 01:39:39.920]   populous as that suddenly can't do it. It's like the ability to do the things earlier with less
[01:39:39.920 --> 01:39:49.840]   evidence and such falls off, and it falls off at a faster rate in mathematics and theoretical
[01:39:49.840 --> 01:39:53.680]   physics and such than in most fields. But wouldn't we expect alignment to
[01:39:53.680 --> 01:39:58.800]   be closer to the theoretical fields? No, not necessarily. Yeah, I think that
[01:39:58.800 --> 01:40:05.360]   intuition is not necessarily correct. Machine learning certainly is an area that rewards
[01:40:05.360 --> 01:40:16.160]   ability, but it's also a field where empirics and engineering have been enormously influential.
[01:40:16.160 --> 01:40:24.080]   If you're drawing the correlations compared to theoretical physics and pure mathematics,
[01:40:24.080 --> 01:40:29.200]   I think you'll find a lower correlation with cognitive ability if that's what you're
[01:40:29.200 --> 01:40:35.200]   thinking of. Yeah, something like creating neural lie detectors that work. So there's generating
[01:40:35.200 --> 01:40:42.560]   hypotheses about new ways to do it, and new ways to try and train AI systems to successfully
[01:40:42.560 --> 01:40:49.200]   classify the cases. But the process of just generating the data sets, of creating AIs doing
[01:40:49.200 --> 01:40:57.040]   their best to put forward truths versus falsehoods, to put forward software that is legit versus that
[01:40:57.040 --> 01:41:03.680]   has a Trojan in it. It's an experimental paradigm. And in this experimental paradigm, you can try
[01:41:03.680 --> 01:41:07.760]   different things that work. You can use different ways to generate hypotheses.
[01:41:07.760 --> 01:41:16.240]   Yeah, and you can follow an incremental experimental path. And now we're less able
[01:41:16.240 --> 01:41:20.320]   to do that in the case of alignment and superintelligence, because we're considering
[01:41:20.320 --> 01:41:27.360]   having to do things on a very short timeline. And in a case where really big failures
[01:41:27.360 --> 01:41:34.160]   be irrecoverable, the AI starts rooting the servers and subverting the methods that we would
[01:41:34.160 --> 01:41:40.400]   use to keep it in check. We may not be able to recover from that. And so we're then less able
[01:41:40.400 --> 01:41:48.400]   to do the experimental procedures. But we can do those in the weaker contexts, where an error is
[01:41:48.400 --> 01:41:53.440]   less likely to be irrecoverable, and then try and generalize and expand and build on that forward.
[01:41:53.440 --> 01:42:03.280]   Yeah. On the previous point about, could you have some sort of pause in AI abilities
[01:42:03.280 --> 01:42:09.200]   when you know it's somewhat misaligned in order to still recruit its abilities to help with alignment?
[01:42:10.160 --> 01:42:16.160]   Okay, from a human example, personally, I'm smart, but not brilliant. I'm definitely not
[01:42:16.160 --> 01:42:21.120]   smart enough to come up with general relativity or something like that. But I'm smart enough to
[01:42:21.120 --> 01:42:27.280]   do power planning kinds of moves. Maybe not enough to break out of a server, perhaps, but I can have
[01:42:27.280 --> 01:42:31.120]   the motivation and understand how that might be possible. I guess I'm wondering, if I'm smart
[01:42:31.120 --> 01:42:40.160]   enough to figure out relativity, wouldn't I be way smarter at doing power planning kinds of moves?
[01:42:40.160 --> 01:42:49.040]   So AIs today can, at a verbal level, understand the idea that, well, yeah, an AI could get more
[01:42:49.040 --> 01:42:56.160]   reward by getting control of the process that assigns it reward. And it can tell you lots of
[01:42:56.160 --> 01:43:02.800]   things about ways you might try to take over the world. In ARCS, the Alignment Research Center's
[01:43:02.800 --> 01:43:08.960]   evaluations of GPD4, they try to observe its ability to do various tasks that might contribute
[01:43:08.960 --> 01:43:15.920]   to takeover. One that has gotten some media attention is getting to trick a human into
[01:43:15.920 --> 01:43:22.480]   solving a capture for it. And then in chain of thought, it thinks, well, if I tell it I'm an AI,
[01:43:22.480 --> 01:43:27.360]   then it might not go along with it. So I'll lie and explain I'm a human with a visual impairment.
[01:43:27.360 --> 01:43:34.960]   The basic logic of that kind of thing, of why might one try to do takeover,
[01:43:34.960 --> 01:43:41.440]   and in a vague sense, how would one do it? That knowledge, yeah, is present or will soon be
[01:43:41.440 --> 01:43:46.080]   present. And I think also soon will be present more situational awareness. Awareness not just
[01:43:47.520 --> 01:43:54.400]   in theory, AIs in general might do it, but also it is an AI. It is a large language model
[01:43:54.400 --> 01:43:59.840]   trained by open AI or whatnot. We're trying to cause the system, for example, to understand
[01:43:59.840 --> 01:44:03.760]   what their abilities are. So they don't claim they are connected to the internet when they're not. So
[01:44:03.760 --> 01:44:09.920]   they don't claim they have knowledge that they don't. We want them to understand what they are
[01:44:09.920 --> 01:44:14.320]   and what they're doing and to get good reward. And that knowledge can be applied. And so that's
[01:44:14.320 --> 01:44:21.120]   the thing that will develop. However, you may be able to understand in general, the theoretical
[01:44:21.120 --> 01:44:29.120]   nature of power planning, but in terms of shaping your motivations, you'd have a very hard time
[01:44:29.120 --> 01:44:36.720]   going through life in a way that never leaks information. If, say, your motive in having these
[01:44:36.720 --> 01:44:45.040]   podcasts was to spread disinformation on behalf of some foreign government, if you were being observed
[01:44:45.040 --> 01:44:51.360]   every second of the day by people who would be paid something that was extremely motivating to
[01:44:51.360 --> 01:44:57.040]   them because their brain would be reconfigured to make it motivating, anything that looks suspicious
[01:44:57.040 --> 01:45:03.440]   to people coming out, it might leak casually in your discussions of that former foreign government.
[01:45:04.080 --> 01:45:10.400]   If you try to tell a story about your motivations, the truth holds together because you can just
[01:45:10.400 --> 01:45:16.160]   remember it and it's all pre-computed to be consistent by the world. But when you're trying
[01:45:16.160 --> 01:45:22.880]   to fill in the false story that doesn't match your actual thoughts, then you can't just remember what
[01:45:22.880 --> 01:45:28.560]   happened. You have to tell a new fake story about what were you thinking at each point
[01:45:28.560 --> 01:45:34.080]   and how did it hold together, and without the world automatically having computed it for you,
[01:45:34.080 --> 01:45:38.480]   that's quite difficult. And just going through your whole life, never leaking any of that,
[01:45:38.480 --> 01:45:43.280]   it's chilling. And this is one of the reasons why humans have moral motivations. We actually
[01:45:43.280 --> 01:45:48.640]   have reluctance to commit violence and exploitation of one another. On average,
[01:45:48.640 --> 01:45:55.600]   you have a distribution of strategies where when an exploitative strategy becomes more common,
[01:45:55.600 --> 01:46:01.360]   people raise their guard and then it becomes less frequent. But it's actually hard to
[01:46:01.360 --> 01:46:08.720]   have the motivation of trying to exploit people and have that never leak into the reputation.
[01:46:08.720 --> 01:46:15.360]   And so, in fact, for evolution, the easiest way to deal with this problem of getting credited as
[01:46:15.360 --> 01:46:21.040]   trustworthy was to some extent actually be trustworthy. That's the easiest way to persistently
[01:46:21.040 --> 01:46:26.800]   maintain the appearance. And so, we're trying with the AIs and interpretability and adversarial
[01:46:26.800 --> 01:46:33.840]   examples to be applying a hugely intensified version of that where any little leakage or any
[01:46:33.840 --> 01:46:38.480]   rare circumstance that can be created as an adversarial example, where the model does
[01:46:38.480 --> 01:46:45.200]   something wrong, it gets whacked by gradient descent, pushing it towards other motivations
[01:46:45.200 --> 01:46:50.640]   that can better deal with it. And we make it as hard as possible for the exploitative
[01:46:50.640 --> 01:46:55.200]   motivations to survive in the face of all these attempts to read its mind, all these attempts to
[01:46:55.200 --> 01:47:00.720]   create things that look like the situations where a takeover would be tempting or lying to the humans
[01:47:00.720 --> 01:47:06.160]   would be tempting. That had a substantial effect on making us actually nice, even when we're not
[01:47:06.160 --> 01:47:11.280]   watching some of the time, when we're not being watched some of the time. And the same can happen
[01:47:11.280 --> 01:47:16.000]   to some extent with the AI and we try our best to make it happen as much as possible.
[01:47:16.000 --> 01:47:25.120]   All right, so let's talk about how we could use AI potentially to solve the coordination
[01:47:25.120 --> 01:47:32.160]   problems between different nations, the failure of which could result in the competitive pressures
[01:47:32.160 --> 01:47:36.880]   you talked about earlier, where some country launches an AI that is not safe because they're
[01:47:36.880 --> 01:47:41.280]   not sure what capabilities other countries have and don't want to get left behind or
[01:47:42.080 --> 01:47:50.560]   in some other way disadvantaged. To the extent that there is in fact a large risk of AI apocalypse,
[01:47:50.560 --> 01:47:56.720]   of all of these governments being overthrown by AI in a way that they don't intend,
[01:47:56.720 --> 01:48:06.320]   then it's obviously gains from trade and going somewhat slower, especially at the end when the
[01:48:06.320 --> 01:48:13.920]   danger is highest and the unregulated pace could be truly absurd, as we discussed earlier during
[01:48:13.920 --> 01:48:20.080]   an intelligence explosion. There's no non-competitive reason to try and have that
[01:48:20.080 --> 01:48:26.400]   intelligence explosion happen over a few months rather than a couple of years. Like if you could
[01:48:26.400 --> 01:48:34.560]   say, you know, avert a 10% risk of apocalyptic disaster, it's just a clear win to take a year
[01:48:34.560 --> 01:48:40.080]   or two years or three years instead of a few months to pass through that incredible wave of
[01:48:40.080 --> 01:48:46.640]   new technologies without the ability for humans to follow it even well enough to give more proper
[01:48:46.640 --> 01:48:55.520]   security supervision auditing hard power. So yeah, that's the win. Why might it fail?
[01:48:55.520 --> 01:49:03.520]   One important element is just if people don't actually notice a risk that is real.
[01:49:04.480 --> 01:49:12.080]   So if we'll just collectively make an error, and that does sometimes happen. If it's true,
[01:49:12.080 --> 01:49:21.120]   this is a probably not risk, then it can be even more difficult. When science pins something down
[01:49:21.120 --> 01:49:27.760]   absolutely overwhelmingly, then you can get to a situation where most people mostly believe it.
[01:49:28.320 --> 01:49:34.720]   And so climate change was something that was a subject of scientific study for decades.
[01:49:34.720 --> 01:49:40.800]   And gradually over time, the scientific community converged on like a quite firm
[01:49:40.800 --> 01:49:47.120]   consensus that so human activity releasing carbon dioxide and other greenhouse gases
[01:49:47.120 --> 01:49:53.680]   was causing the planet to warm. And then we've had increasing amounts of action coming out of that.
[01:49:53.680 --> 01:50:00.480]   So not as much as would be optimal, particularly in like the most effective areas like creating
[01:50:00.480 --> 01:50:08.800]   renewable energy technology and the like. But overwhelming evidence can overcome differences
[01:50:08.800 --> 01:50:14.240]   in sort of people's individual intuitions and priors in many cases, and not perfectly,
[01:50:14.240 --> 01:50:19.760]   especially when there's political, tribal financial incentives go the other way.
[01:50:19.760 --> 01:50:25.120]   And so in the United States, you see a significant movement to either deny that
[01:50:25.120 --> 01:50:32.320]   climate change is happening or have policy that doesn't take it into account. Even the things
[01:50:32.320 --> 01:50:40.560]   that are like really strong wins, like renewable energy R&D. It's a big problem if as we're going
[01:50:40.560 --> 01:50:47.520]   into this situation, when the risk may be very high, we don't have a lot of advance clear warning
[01:50:47.520 --> 01:50:53.280]   about the situation. We're much better off if we can resolve uncertainties, say through
[01:50:53.280 --> 01:51:00.640]   experiments where we demonstrate AIs being motivated to reward hack or displaying deceptive
[01:51:00.640 --> 01:51:06.240]   appearances of alignment that then break apart when they get the opportunity to do something like
[01:51:06.240 --> 01:51:12.720]   get control of their own reward signal. So if we could make it be the case in the worlds where the
[01:51:12.720 --> 01:51:17.280]   risk is high, we know the risk is high. And the worlds where the risk is lower, we know the risk
[01:51:17.280 --> 01:51:23.200]   is lower. Then you can expect the government responses will be a lot better. They will
[01:51:23.200 --> 01:51:29.760]   correctly note that the gains of cooperation to reduce the risk of accidental catastrophe
[01:51:29.760 --> 01:51:39.200]   loom larger relative to the gains of trying to get ahead of one another. And so for that,
[01:51:39.200 --> 01:51:44.960]   that's the kind of reason why I'm very enthusiastic about experiments and research that
[01:51:44.960 --> 01:51:51.600]   helps us to better evaluate the character of the problem in advance because any resolution of that
[01:51:51.600 --> 01:51:57.840]   uncertainty helps us get better efforts in the possible worlds where it matters the most.
[01:51:57.840 --> 01:52:03.520]   And yeah, hopefully we'll have that and it'll be a much easier epistemic environment,
[01:52:03.520 --> 01:52:08.320]   but the environment may not be that easy. Deceptive alignment is pretty plausible. The
[01:52:08.320 --> 01:52:13.280]   stories we were discussing earlier about misaligned AI involve AI that is motivated
[01:52:13.280 --> 01:52:21.840]   to present the appearance of being aligned, friendly, honest, etc. Because that is what we
[01:52:21.840 --> 01:52:28.160]   are rewarding, at least in training. And then we're unable in training to easily produce an
[01:52:28.160 --> 01:52:33.120]   actual situation where it can do takeover because in that actual situation, if it then does it,
[01:52:33.120 --> 01:52:39.600]   we're in big trouble. We can only try and create illusions or misleading appearances of that,
[01:52:39.600 --> 01:52:44.720]   or maybe a more local version where the AI can't take over the world, but it can seize control
[01:52:44.720 --> 01:52:50.480]   of its own reward channel. And so we do those experiments. We try to develop mind reading
[01:52:50.480 --> 01:52:56.800]   for AIs if we can probe the thoughts and motivations of an AI and discover, wow,
[01:52:56.800 --> 01:53:02.400]   actually GPT-6 is planning to take over the world if it ever gets the chance. That would be an
[01:53:02.400 --> 01:53:08.320]   incredibly valuable thing for governments to coordinate around because it would remove a lot
[01:53:08.320 --> 01:53:16.960]   of the uncertainty. It would be easier to agree that this was important to have more give on other
[01:53:16.960 --> 01:53:24.480]   dimensions and to have mutual trust that the other side actually also cares about this and hence,
[01:53:24.480 --> 01:53:31.760]   because you can't always know what another person or another government is thinking, but you can see
[01:53:31.760 --> 01:53:37.760]   the objective situation in which they're deciding. And so if there's strong evidence in a world where
[01:53:37.760 --> 01:53:44.160]   there is high risk of that risk, because we've been able to show actually things like the
[01:53:44.160 --> 01:53:50.560]   intentional planning of AIs to do a takeover, we're being able to show model situations on a
[01:53:50.560 --> 01:53:56.880]   smaller scale of that. I mean, not only are we more motivated to prevent it, but we update to
[01:53:56.880 --> 01:54:01.920]   think the other side is more likely to cooperate with us. And so it's definitely beneficial.
[01:54:01.920 --> 01:54:09.040]   Yeah. So famously in sort of game theory of war, war is most likely when
[01:54:09.040 --> 01:54:14.480]   one side thinks the other is bluffing, but the other side is being serious or something like
[01:54:14.480 --> 01:54:20.800]   that. When there's that kind of uncertainty, which would be less likely. If you can prove
[01:54:20.800 --> 01:54:25.840]   the AI is misaligned, you don't think they're bluffing about not wanting to have an AI takeover,
[01:54:25.840 --> 01:54:29.040]   right? Like you can be pretty sure that they don't want to die from AI.
[01:54:29.040 --> 01:54:34.160]   Now, if you have coordination, then you can have the problem arise later as you get increasingly
[01:54:34.160 --> 01:54:41.760]   confident in the further alignment measures that are taken. And maybe our governments and treaties
[01:54:41.760 --> 01:54:50.480]   and such at a 1% risk or a 0.1% risk. At that point, people round that to zero and go do things.
[01:54:50.480 --> 01:54:55.840]   So if initially you had things that indicate, yeah, these AI, they really would like to take
[01:54:55.840 --> 01:55:01.200]   over and overthrow our governments, then, okay, everyone can agree on that. And then when you're
[01:55:01.200 --> 01:55:05.360]   like, well, we've been able to block that behavior from appearing on most of our tests.
[01:55:05.360 --> 01:55:10.800]   But sometimes when we make a new test, we're seeing still examples of that behavior. So
[01:55:10.800 --> 01:55:15.600]   we're not sure going forward, whether they would or not. And then it goes down and down. And then
[01:55:16.400 --> 01:55:22.400]   if you have parties with a habit of whenever the risk is below X percent, then they're going to
[01:55:22.400 --> 01:55:28.640]   start doing this bad behavior. Then that can make the thing harder. But you get more time.
[01:55:28.640 --> 01:55:34.720]   You get more time and you can set up systems, mutual transparency. You can have an iterated
[01:55:34.720 --> 01:55:41.200]   tit for tat, which is better than a one-time prison's dilemma where both sides see the others
[01:55:41.200 --> 01:55:46.800]   taking measures in accordance with the agreements to hold the thing back. So, yeah. So creating more
[01:55:46.800 --> 01:55:53.200]   knowledge of what the objective risk is, is good. We've discussed the ways in which full alignment
[01:55:53.200 --> 01:55:59.440]   might happen or fail to happen. What would partial alignment look like? First of all,
[01:55:59.440 --> 01:56:04.240]   what does that mean? And second, what would it look like? Yeah. So if the thing that we're scared
[01:56:04.240 --> 01:56:14.080]   about are these steps towards AI takeover, you can have a range of motivations where those kinds of
[01:56:14.080 --> 01:56:19.840]   actions would be more or less likely to be taken or they'd be taken in a broader or narrower set
[01:56:19.840 --> 01:56:28.880]   of situations. So say, for example, that in training, an AI winds up developing a strong
[01:56:28.880 --> 01:56:37.680]   aversion to live in certain senses because we did relatively well on creating situations to
[01:56:37.680 --> 01:56:43.760]   sort of distinguish that from the conditionally telling us what we want to hear, et cetera.
[01:56:43.760 --> 01:56:49.760]   It can be that the AI's preference for sort of how the world broadly unfolds in the future,
[01:56:50.640 --> 01:57:00.000]   it's not exactly the same as its human users or the world's governments or the UN, and yet
[01:57:00.000 --> 01:57:07.680]   it's not ready to act on those differences and preferences about the future because it has this
[01:57:07.680 --> 01:57:15.440]   strong preference about its own behaviors and actions. In general, in the law and in sort of
[01:57:15.440 --> 01:57:20.320]   popular morality, we have a lot of these deontological kind of rules and prohibitions.
[01:57:20.320 --> 01:57:27.280]   And one reason for that is it's relatively easy to detect whether they're being violated.
[01:57:27.280 --> 01:57:34.480]   When you have preferences and goals about how society at large will turn out that go through
[01:57:34.480 --> 01:57:39.440]   many complicated empirical channels, it's very hard to get immediate feedback about whether,
[01:57:39.440 --> 01:57:43.440]   say, you're doing something that is to overall good consequences in the world.
[01:57:44.320 --> 01:57:50.880]   And it's much, much easier to see whether you're locally following some action, some rule about
[01:57:50.880 --> 01:57:55.840]   particular observable actions, like did you punch someone? Did you tell a lie? Did you steal?
[01:57:55.840 --> 01:58:03.360]   And so to the extent that we're successfully able to train these prohibitions, and there's a lot of
[01:58:03.360 --> 01:58:08.800]   that happening right now, at least to elicit the behavior of following rules and prohibitions
[01:58:08.800 --> 01:58:12.800]   with AI. Kind of like the Asimov's three laws or something like that?
[01:58:13.600 --> 01:58:17.600]   Well, the three laws are terrible. Let's not get into that.
[01:58:17.600 --> 01:58:26.640]   Isn't that an indication about the infeasibility of extending a set of criterion to the tail?
[01:58:26.640 --> 01:58:34.000]   What are the 10 commandments you give the AI such that you'd be... It's like you ask a genie for
[01:58:34.000 --> 01:58:38.080]   something and you're like, you know what I mean? You probably won't get what you want.
[01:58:38.080 --> 01:58:48.560]   So the tails come apart. And if you're trying to capture the values of another agent, then
[01:58:48.560 --> 01:58:55.760]   you want the AI to share. I mean, for really a kind of ideal situation where you can just let
[01:58:55.760 --> 01:59:03.120]   the AI act in your place in any situation, you'd like for it to be motivated to bring about the
[01:59:03.120 --> 01:59:09.280]   same outcomes that you would like. And so have the same preferences over those in detail.
[01:59:09.280 --> 01:59:14.320]   That's tricky. Not necessarily because it's tricky for the AI to understand your values. I think
[01:59:14.320 --> 01:59:21.360]   they're going to be quite good and quite capable at figuring that out. But we may not be able to
[01:59:21.360 --> 01:59:29.200]   successfully instill the motivation to pursue those exactly. We may get something that motivates
[01:59:29.200 --> 01:59:35.440]   the behavior well enough to do well on the training distribution. But what you can have is
[01:59:35.440 --> 01:59:41.120]   if you have the AI have a strong aversion to certain kinds of manipulating humans,
[01:59:41.120 --> 01:59:46.400]   that's not a value necessarily that the human creators share in the exact same way.
[01:59:46.400 --> 01:59:54.240]   It's a behavior they want the AI to follow because it makes it easier for them to verify
[01:59:54.240 --> 02:00:02.640]   its performance. It can be a guardrail if the AI has inherited some motivations that push it in the
[02:00:02.640 --> 02:00:09.600]   direction of conflict with its creators. If it does that under the constraint of disvaluing lying
[02:00:09.600 --> 02:00:15.520]   quite a bit, then there are fewer successful strategies to the takeover. The ones that
[02:00:15.520 --> 02:00:20.960]   involve violating that prohibition too early before it can reprogram or retrain itself to remove it,
[02:00:21.760 --> 02:00:25.440]   if it's willing to do that and may want to retain the property.
[02:00:25.440 --> 02:00:35.200]   And so earlier I discussed alignment as a race. If we're going into an intelligence explosion with AI
[02:00:35.200 --> 02:00:41.280]   that is not fully aligned, that given I sort of press this button and there's an AI takeover,
[02:00:41.280 --> 02:00:46.720]   they would press the button. It can still be the case that there are a bunch of situations short
[02:00:46.720 --> 02:00:54.160]   of that where they would hack the servers, they would initiate an AI takeover, but for
[02:00:54.160 --> 02:01:02.640]   a strong prohibition or motivation to avoid some aspect of the plan. And there's an element of
[02:01:02.640 --> 02:01:08.320]   like plugging loopholes or playing whack-a-mole, but if you can even moderately constrain
[02:01:10.080 --> 02:01:17.520]   which plans the AI is willing to pursue to do a takeover, to subvert the controls on it,
[02:01:17.520 --> 02:01:21.360]   then that can mean you can get more work out of it successfully on the alignment project
[02:01:21.360 --> 02:01:27.840]   before it's capable enough relative to the countermeasures to pull off the takeover.
[02:01:27.840 --> 02:01:35.520]   Yeah. And this is applicable, like an analogous situation here is with different humans. We're
[02:01:35.520 --> 02:01:44.240]   not metaphysically aligned with other humans. I mean, in some sense, we have basic empathy, but
[02:01:44.240 --> 02:01:52.400]   our main goal in life is not to help our fellow man. But the kinds of things we talked about,
[02:01:52.400 --> 02:01:56.880]   a very smart human could do in terms of the takeover where theoretically a very smart human
[02:01:56.880 --> 02:02:02.160]   could come up with some sort of cyber attack where they siphon off a lot of funds and use this to
[02:02:02.160 --> 02:02:08.320]   manipulate people and bargain with people and hire people to pull off some sort of takeover
[02:02:08.320 --> 02:02:15.840]   or accumulate power. Usually this doesn't happen just because these sorts of internalized partial
[02:02:15.840 --> 02:02:21.840]   prohibitions prevent most humans from, you know, you're like, you don't like your boss maybe,
[02:02:21.840 --> 02:02:24.960]   but you don't like kill your boss. If he gives you a assignment you don't like.
[02:02:24.960 --> 02:02:31.200]   I don't think that's actually quite what's going on. At least not, it's not the full story.
[02:02:32.000 --> 02:02:37.520]   Um, so humans are pretty close in physical capabilities. Like there's variation.
[02:02:37.520 --> 02:02:43.680]   In fact, any individual human, uh, is grossly outnumbered by everyone else.
[02:02:43.680 --> 02:02:51.840]   Um, and there's like a rough comparability of power. Uh, and so a human who commits some crimes,
[02:02:51.840 --> 02:02:56.240]   uh, can't copy themselves with the proceeds to now be a million people.
[02:02:56.880 --> 02:03:02.240]   Uh, and they certainly can't do that to the point where they can staff all the armies of the earth
[02:03:02.240 --> 02:03:08.480]   or like be most of the population of the planet. So the, the scenarios where this kind of thing
[02:03:08.480 --> 02:03:13.600]   goes to power, you know, much less things more have to go extreme amounts of power,
[02:03:13.600 --> 02:03:19.680]   go through interacting with other humans and getting social approval, even becoming a dictator
[02:03:19.680 --> 02:03:23.920]   involves forming a large supporting coalition backing you. And so just the,
[02:03:25.600 --> 02:03:34.240]   the opportunity for these sorts of, uh, of power grabs is less, uh, closer analogy might be things
[02:03:34.240 --> 02:03:40.080]   like human revolutions, um, and coups changes of government where like a large coalition
[02:03:40.080 --> 02:03:47.440]   overturns the system. Uh, and so humans have these moral prohibitions and they really smooth
[02:03:47.440 --> 02:03:54.160]   the operation of society. Um, but they exist for a reason. And so we evolved our moral sentiments
[02:03:54.800 --> 02:04:01.040]   over the course of hundreds of thousands and millions of years of humans interacting socially.
[02:04:01.040 --> 02:04:06.240]   And so someone who went around murdering and stealing and such, even among hunter gatherers
[02:04:06.240 --> 02:04:12.240]   would be pretty, pretty likely to face like a group, uh, of, of males would talk about that
[02:04:12.240 --> 02:04:18.960]   person and get together and kill them. Uh, and they'd be removed from the gene pool. And, uh,
[02:04:18.960 --> 02:04:23.600]   there's an anthropologist. Wrangham has a, an interesting book on this. Um, but yes,
[02:04:23.600 --> 02:04:29.920]   compared to chimpanzees, we are significantly tame, significantly domesticated. Uh, and it seems like
[02:04:29.920 --> 02:04:37.120]   part of that is we have a long history of anti-social humans getting ganged up on and
[02:04:37.120 --> 02:04:43.760]   killed, um, and avoiding being the kind of person who elicits that response is made easier to do,
[02:04:44.480 --> 02:04:50.800]   uh, when you don't have too extreme, a bad temper that you don't wind up getting too many fights,
[02:04:50.800 --> 02:04:57.120]   too much exploitation, at least without the backing of enough allies or the broader community,
[02:04:57.120 --> 02:05:02.880]   uh, that you're not going to have people getting up and punish you and remove you from the gene
[02:05:02.880 --> 02:05:10.800]   pool. We have these moral sentiments and they've been built up over time through cultural and
[02:05:10.800 --> 02:05:16.720]   natural selection and the context of sets of institutions and other people who are punishing
[02:05:16.720 --> 02:05:21.520]   other behavior and who are punishing the dispositions that would show up that we
[02:05:21.520 --> 02:05:27.360]   weren't able to conceal of that behavior. And so we want, we want to make the same thing happen
[02:05:27.360 --> 02:05:36.640]   with the AI, but it's actually a genuinely significantly new problem, uh, to have,
[02:05:36.640 --> 02:05:42.800]   say like a system of system of government that constrains this, like a large AI population
[02:05:42.800 --> 02:05:48.640]   that is quite capable of taking over immediately if they coordinate, you know, to protect some
[02:05:48.640 --> 02:05:53.920]   existing constitutional order or say, protect humans from being exp, you know, expropriated
[02:05:53.920 --> 02:06:00.480]   or killed. That's, that's a, that's a challenge. And democracy is built around majority rule and
[02:06:00.480 --> 02:06:09.520]   it's much easier in, in a case where the majority of the population corresponds to a majority or
[02:06:09.520 --> 02:06:14.880]   close to it of like military and security forces. So that if the government does something that
[02:06:14.880 --> 02:06:21.280]   people don't like, the soldiers are, and police are less likely to shoot on protesters and
[02:06:21.280 --> 02:06:29.120]   government can change that way. In a case where military power is AI and robotic, if you're
[02:06:29.120 --> 02:06:34.640]   trying to maintain a system going forward and the AIs are misaligned, they don't like the system and
[02:06:34.640 --> 02:06:42.320]   they want to make the world worse. Um, as we understand it, then yeah, that's just, that's
[02:06:42.320 --> 02:06:47.600]   quite a different situation. I think that's a really good lead in, into the topic of lock-in
[02:06:47.600 --> 02:06:55.840]   in some sense, the regimes on that are made up by humans. Um, you just mentioned how there
[02:06:55.840 --> 02:07:05.280]   can be these kinds of coups if a large portion of the population is unsatisfied with the regime.
[02:07:05.280 --> 02:07:12.960]   Why might this not be the case with superhuman intelligences in the far future or I guess in the
[02:07:12.960 --> 02:07:21.760]   medium future? Well, I said it also specifically with respect, uh, to things like security forces
[02:07:21.760 --> 02:07:27.600]   and, uh, the sort of source, sources of hard power, which can also include outside support.
[02:07:27.600 --> 02:07:31.280]   Oh, because you're not using humans, you're using robots.
[02:07:31.280 --> 02:07:37.200]   You can have a situation like in human, in human affairs, uh, there are governments that are
[02:07:37.200 --> 02:07:43.680]   vigorously supported by a minority of the population. Some narrow selectorate that gets
[02:07:43.680 --> 02:07:49.440]   treated especially well by the government while being unpopular, uh, with most of the people
[02:07:49.440 --> 02:07:56.400]   under their role. Um, and the, we, we see a lot of examples of that and sometimes that can,
[02:07:56.400 --> 02:08:02.880]   um, escalate to civil war when the, the means of power become more equally distributed or
[02:08:02.880 --> 02:08:10.640]   there's a foreign assistance, uh, is provided to the, uh, the people who are on the losing end
[02:08:10.640 --> 02:08:19.120]   of that system. Yeah. Going forward, um, I don't expect that definition to change. I think it will
[02:08:19.120 --> 02:08:28.080]   still be the case that a system that, uh, those who hold the guns and equivalent, um, are opposed
[02:08:28.080 --> 02:08:37.200]   to, um, you know, is in a, in a very difficult position. However, AI could change things pretty
[02:08:37.200 --> 02:08:47.440]   dramatically in terms of how security forces and police and administrators and legal systems are
[02:08:47.440 --> 02:08:57.360]   motivated. So right now, uh, we see with say GPT-3 or GPT-4 that you can get them to change their
[02:08:57.360 --> 02:09:05.360]   behavior, um, you know, on a dime. So, uh, there was someone, uh, who made, uh, right-wing GPT
[02:09:05.360 --> 02:09:10.880]   because they noticed that on political compass questionnaires, the baseline GPT-4, uh, tended
[02:09:10.880 --> 02:09:18.240]   to give progressive San Francisco type of answers, you know, which is, uh, in line with the, um,
[02:09:18.240 --> 02:09:23.120]   the sort of people who are providing reinforcement learning data and to some extent reflecting like
[02:09:23.120 --> 02:09:28.240]   the character of the internet. They're like, well, I don't like this. Um, and so they did a little
[02:09:28.240 --> 02:09:35.440]   bit of, of fine tuning with some, uh, conservative data. Uh, and then they were able to, uh, reverse,
[02:09:35.440 --> 02:09:43.280]   uh, the political biases of the system. Um, if you take the, the initial helpfulness only,
[02:09:43.280 --> 02:09:52.160]   uh, trained models, uh, for some of these over, I think there's, uh, Entropic, um, and OpenAI,
[02:09:52.160 --> 02:09:56.480]   I think have, have published, uh, both some information about the sort of models trained
[02:09:56.480 --> 02:10:03.120]   only to do what users say and not trained to follow ethical rules. Uh, and those models
[02:10:03.120 --> 02:10:10.160]   will behaviorally eagerly display, uh, their willingness to help design, uh, bombs or bioweapons
[02:10:10.160 --> 02:10:17.920]   or kill people or steal or commit all sorts of atrocities. Um, and so if in the future, uh, it's
[02:10:17.920 --> 02:10:25.440]   as easy to set the motivations, the actual underlying motivations, uh, of AIs as it is right
[02:10:25.440 --> 02:10:31.920]   now to set the behavior that they display, then it means you could have AIs created with, uh,
[02:10:31.920 --> 02:10:39.680]   almost whatever motivation, uh, people, people wish. And that could really drastically change
[02:10:39.680 --> 02:10:49.680]   political affairs, um, because the ability to decide and determine the loyalties of the humans
[02:10:49.680 --> 02:10:56.880]   or AIs and robots that hold the guns that hold together society, um, that ultimately, uh, back
[02:10:56.880 --> 02:11:06.320]   it against violent overthrow, um, and, and such. Um, yeah, it, it's potentially a revolution in
[02:11:06.320 --> 02:11:12.720]   how societies work, uh, compared to the, the historical situation where security forces had
[02:11:12.720 --> 02:11:20.240]   to be drawn from some broader populations, offered incentives, uh, and then the ongoing stability of
[02:11:20.240 --> 02:11:27.200]   the regime was dependent on whether, uh, they remained, uh, you know, bought in to the system
[02:11:27.200 --> 02:11:34.560]   continuing. This is slightly off topic, but one thing I'm curious about is how, how, what do the
[02:11:34.560 --> 02:11:42.160]   median, what is the sort of median far future outcome of AI look like? Do we get something
[02:11:42.160 --> 02:11:50.560]   that when it has colonized the galaxy is interested in sort of diverse ideas and beautiful projects
[02:11:50.560 --> 02:11:55.120]   and, or do we get something that looks more like a paperclip maximizer? Is there, is there some
[02:11:55.120 --> 02:11:59.600]   reason to expect one or the other? I guess I'm asking is like, you know, there's like some
[02:11:59.600 --> 02:12:06.320]   potential value that is realizable within the matter of this galaxy, right? What, what is the
[02:12:06.320 --> 02:12:11.360]   default outcome? The median outcome look like compared to how good things could be. Yeah.
[02:12:11.360 --> 02:12:18.080]   So as I was saying, I think more likely than not, um, there isn't an AI takeover. Um, and so
[02:12:18.080 --> 02:12:26.960]   the path of our civilization would be one that, you know, at least large, you know, some side of
[02:12:26.960 --> 02:12:31.920]   human institutions were improved approving along the way. Uh, and I think there, there's some
[02:12:31.920 --> 02:12:39.440]   evidence, um, that so different people, uh, tend to like somewhat different things and some of that
[02:12:40.080 --> 02:12:47.120]   may persist over time rather than everyone coming to agree on one particular monoculture or like
[02:12:47.120 --> 02:12:53.920]   very repetitive thing being the best thing to, uh, to fill all of the available space with.
[02:12:53.920 --> 02:12:59.440]   So if that continues, that's like a, uh, you know, it seems like a relatively likely way in which
[02:12:59.440 --> 02:13:04.640]   there is a diversity, although it's entirely possible you could have that kind of diversity
[02:13:05.520 --> 02:13:12.080]   locally, maybe in the solar system, maybe in our galaxy. Uh, but if people have different views
[02:13:12.080 --> 02:13:17.360]   about maybe people decide, yeah, there, maybe there's one thing that's very good and we'll
[02:13:17.360 --> 02:13:24.880]   have a lot of that. Maybe it's people who are, who are really, uh, really happy, um, or something.
[02:13:24.880 --> 02:13:30.960]   Uh, and they wind up in distant regions, which are hard to exploit for the benefit
[02:13:30.960 --> 02:13:36.880]   of people back home in the solar system or the Milky way. Um, they do something different than
[02:13:36.880 --> 02:13:41.920]   they do would do in their local environment. But at that point it's really a sort of very
[02:13:41.920 --> 02:13:48.880]   out on a limb speculation about how human deliberation and cultural evolution would
[02:13:48.880 --> 02:13:56.640]   work and an interaction with introducing AI's and, uh, new kinds of mental modification and
[02:13:56.640 --> 02:14:02.400]   discovery into the process. Uh, but I think there's a lot of reason to expect, uh, that you
[02:14:02.400 --> 02:14:09.600]   would have, you know, significant diversity, um, for something coming out of, uh, our existing
[02:14:09.600 --> 02:14:13.840]   diverse human society. Yeah. One thing somebody might wonder is like, listen, a lot of the
[02:14:13.840 --> 02:14:19.440]   diversity and change from human society seems to come from the fact that, you know, there's like
[02:14:19.440 --> 02:14:23.360]   rapid technological change. If you look at periods of history where, I mean, I guess you could say
[02:14:23.360 --> 02:14:28.880]   like undergrad, I mean, you know what, compared to, uh, sort of like galactic timescales, like
[02:14:28.880 --> 02:14:35.200]   undergraduate societies are progressing pretty fast. So once that sort of change is exhausted,
[02:14:35.200 --> 02:14:39.120]   um, where we've like discovered all the technologies, should we still expect things
[02:14:39.120 --> 02:14:45.520]   to be changing like that? Or would we expect some sort of set state of maybe like, uh, some
[02:14:45.520 --> 02:14:49.840]   hedonium, you like, you discover what is it like the most pleasurable configuration of matter.
[02:14:49.840 --> 02:14:54.720]   And then you just make the whole galaxy into this. I mean, so that, that last point, it would be
[02:14:54.720 --> 02:15:02.160]   only if people, uh, wound up thinking that was, uh, the thing to do, uh, broadly enough.
[02:15:02.160 --> 02:15:08.640]   Yeah. With respect to, uh, the kind of cultural changes that come with technology. So things like
[02:15:08.640 --> 02:15:14.800]   the printing press having high per capita income. Um, we've had a lot of cultural changes downstream
[02:15:14.800 --> 02:15:19.360]   with those technological changes. And so then intelligence explosion, you're having an incredible
[02:15:19.360 --> 02:15:26.080]   amount of technological development coming in really quick. And as that is assimilated,
[02:15:26.080 --> 02:15:31.120]   it probably would significantly affect our knowledge or understanding our attitudes or
[02:15:31.120 --> 02:15:38.320]   abilities. Um, and there'd be change, uh, but that kind of accelerating change where you have
[02:15:38.320 --> 02:15:43.840]   doubling in four months, two months, one month, two weeks, uh, obviously that very quickly exhausts
[02:15:43.840 --> 02:15:50.560]   itself and change becomes, uh, much slower and then relatively glacial when you're thinking about
[02:15:50.560 --> 02:15:56.480]   thousands, millions, billions of years, you can't have exponential economic growth or like huge
[02:15:56.480 --> 02:16:03.840]   technological revolutions every 10 years for a million years, um, that you, you hit physical
[02:16:03.840 --> 02:16:09.200]   limits, uh, things slow down as you approach them. And so, so that's that. Uh, so yeah, you'd have
[02:16:09.200 --> 02:16:16.640]   less of that turnover, but there are other things, um, that are experienced do cause ongoing change.
[02:16:16.640 --> 02:16:22.960]   So like fashion, fashion is frequency dependent. People want to get into a new fashion that is not
[02:16:22.960 --> 02:16:29.600]   already popular except among the fashion leaders. Uh, and then others copy that. And then when it
[02:16:29.600 --> 02:16:35.520]   becomes popular, you move on to the next. And so that's an ongoing process of continuous change.
[02:16:36.320 --> 02:16:41.920]   Um, and so there could be various things like that, that year by year are changing a lot, but
[02:16:41.920 --> 02:16:49.120]   in cases where just the engine of change, like ongoing technological progress has gotten,
[02:16:49.120 --> 02:16:53.760]   I don't think we should expect that. Uh, and in cases where it's possible to be either
[02:16:53.760 --> 02:17:02.240]   in a stable state or a sort of widely varying state that can wind up in stable attractors,
[02:17:03.040 --> 02:17:08.400]   then I think you should expect over time, you will wind up in one of the stable attractors
[02:17:08.400 --> 02:17:13.680]   or you will change how the system works so that you can't bounce into a stable attractor.
[02:17:13.680 --> 02:17:19.760]   And so like an example of that is if you want, if you're going to preserve democracy,
[02:17:19.760 --> 02:17:26.000]   uh, for a billion years, then you can't have it be the case that like one in, uh, you know,
[02:17:26.000 --> 02:17:32.240]   one in 50 election cycles, you get a dictatorship and then the dictatorship programs, the AI police,
[02:17:32.880 --> 02:17:39.680]   uh, to enforce it forever. Uh, and to ensure this, you know, the, the society is always ruled
[02:17:39.680 --> 02:17:47.920]   by a, a copy of the dictator's mind and maybe the dictator's mind readjusted, uh, fine-tuned
[02:17:47.920 --> 02:17:52.400]   to like remain committed to their original ideology. So if you're going to, if you're
[02:17:52.400 --> 02:17:59.760]   going to have this sort of dynamic, um, liberal, flexible changing society for a very long time,
[02:17:59.760 --> 02:18:05.120]   then the range of things that it's bouncing around and the different things it's trying
[02:18:05.120 --> 02:18:10.560]   and exploring have to not include the state of creating a dictatorship that locks itself in
[02:18:10.560 --> 02:18:16.320]   forever in the same way. Um, if you have the possibility of like a war with, you know,
[02:18:16.320 --> 02:18:22.960]   weapons of mass destruction that wipes out the civilization, if that happens every thousand
[02:18:22.960 --> 02:18:28.640]   subjective years, um, which could be very, very quick. If we have a eyes that think a thousand
[02:18:28.640 --> 02:18:35.440]   times as fast or a million times as fast, um, that would be, you know, just, just around the corner
[02:18:35.440 --> 02:18:41.040]   in that case. Uh, then you're like, no, this, this, this society is eventually going perhaps
[02:18:41.040 --> 02:18:45.040]   very soon if things are proceeding so fast, it's going to wind up extinct and then it's going to
[02:18:45.040 --> 02:18:52.400]   stop bouncing around. So you can have ongoing change and fluctuation for extraordinary time
[02:18:52.400 --> 02:18:56.720]   scales. If you have the process to drive the change ongoing, but you can't, if it's sometimes
[02:18:56.720 --> 02:19:03.200]   bounces into States that just lock in and stay in recover, irrecoverable from that. And extinction is
[02:19:03.200 --> 02:19:10.880]   one of them, a dictatorship, uh, or sort of, you know, totalitarian, uh, regime that for bad all
[02:19:10.880 --> 02:19:16.160]   further change, uh, would be another example. On that point of rapid progress, when the sort
[02:19:16.160 --> 02:19:21.040]   of like intelligence explosion starts happening, where they're making like the kinds of progress
[02:19:21.040 --> 02:19:27.280]   that human civilization takes centuries to make in the span of, you know, days or weeks, what is
[02:19:27.280 --> 02:19:32.000]   the right way to seed that? Even if they are aligned, what is like the, because in the context
[02:19:32.000 --> 02:19:36.400]   of alignment, what we've been talking about so far is making sure they're honest, but even if
[02:19:36.400 --> 02:19:40.320]   they're honest, like, okay, so they're like, here, here's our, honestly, our intentions. And you can
[02:19:40.320 --> 02:19:45.040]   tell us what, uh, honest and appropriately motivated. And then, so what is the appropriate,
[02:19:45.040 --> 02:19:49.200]   uh, motivation or the appropriate appropriate seed? Like, you know, that you seed it with this
[02:19:49.200 --> 02:19:54.320]   and then a thousand years of intellectual progress happen in the next week. Well, you, what is,
[02:19:54.320 --> 02:20:01.040]   what is the problem to enter? Uh, well, one thing might be that going at the maximal speed, um,
[02:20:01.040 --> 02:20:05.920]   and doing things in months rather than, you know, even a few years, uh, could be if you, if you have
[02:20:05.920 --> 02:20:12.800]   the chance, uh, to, to slow those things, lose it, losing a year or two seems worth it to have
[02:20:12.800 --> 02:20:20.080]   things be, uh, yeah, a bit better managed than that. But I think that the big thing is that it
[02:20:20.080 --> 02:20:25.120]   condenses a lot of issues, uh, that we might otherwise have thought would be over,
[02:20:25.120 --> 02:20:33.520]   over decades and centuries to happen in a very short period of time. And so that's scary because
[02:20:33.520 --> 02:20:38.720]   say if any of these, the technologies we might've developed with another few hundred years of human
[02:20:38.720 --> 02:20:45.600]   research, if any of them are really dangerous, so, you know, scary bioweapon things, uh, maybe
[02:20:45.600 --> 02:20:53.200]   other dangerous WMD, they hit us all very quickly. Uh, and if any of them causes trouble, then we
[02:20:53.200 --> 02:20:59.360]   have to face quite a lot of trouble per period. There's also this issue of if there's occasional
[02:20:59.360 --> 02:21:08.000]   wars or conflicts, um, measured in subjective time, then if a few years, a thousand years,
[02:21:08.000 --> 02:21:13.920]   or a million years of subjective time for these very fast minds that are operating at a much, uh,
[02:21:13.920 --> 02:21:20.880]   a much higher speed than humans, you don't want to have a situation where, oh, every thousand years,
[02:21:20.880 --> 02:21:27.040]   um, there's a war or an expropriation of the humans from AI society. Therefore we expect that
[02:21:27.040 --> 02:21:35.360]   like within a year we'll be dead. That'd be pretty, pretty, uh, bad to have the future compressed and
[02:21:35.360 --> 02:21:41.440]   there'd be such a, uh, you know, a rate of, of catastrophic outcomes. So when we're speeding up
[02:21:41.440 --> 02:21:47.920]   and compressing the future that gives us in the short term, even like, you know, human societies
[02:21:47.920 --> 02:21:53.600]   discount the future a lot, don't pay attention to long-term problems. Uh, but the flip side to the
[02:21:53.600 --> 02:21:59.120]   scary parts of compressing a lot of the future, a lot of technological innovation, a lot of social
[02:21:59.120 --> 02:22:04.800]   change is it brings what would otherwise be long-term issues into the short term where people
[02:22:04.800 --> 02:22:11.280]   are better at actually attending to them. And so people facing this problem of, well, will there be,
[02:22:11.280 --> 02:22:17.440]   you know, a, a violent expropriation or a civil war or a nuclear war, um, you know, in the next
[02:22:17.440 --> 02:22:22.640]   year, because everything has been sped up by a thousand fold. Uh, and if their desire to avoid
[02:22:22.640 --> 02:22:30.000]   that is reason for them to set up systems and institutions that will very stably maintain
[02:22:30.000 --> 02:22:39.360]   invariance, like no WMD war allowed. Uh, and so like a, a treaty, uh, to ban, um, genocide weapons
[02:22:39.360 --> 02:22:45.200]   of mass destruction, war like that would be the kind of thing that becomes much more attractive.
[02:22:45.200 --> 02:22:49.680]   If the alternative is not well, maybe that will happen in 50 years. Maybe it'll happen in a hundred
[02:22:49.680 --> 02:22:56.400]   years if it's well, maybe it'll happen this year. Okay. So this is a pretty wild picture of the
[02:22:56.400 --> 02:23:03.120]   future. And this is one that many kinds of people you would expect to have integrated it into their
[02:23:03.120 --> 02:23:09.280]   world model have not. So, I mean, the three main examples or the three main pieces of outside view
[02:23:09.280 --> 02:23:16.800]   evidence one could go at one is the market. So if there was going to be a huge period of economic
[02:23:16.800 --> 02:23:23.600]   growth caused by AI, or if the world was just going to collapse in both cases, you would expect
[02:23:23.600 --> 02:23:29.040]   real interest rates to be higher because people will be borrowing from the future to spend. Now
[02:23:29.040 --> 02:23:33.920]   the second sort of outside view perspective is that you can look at the predictions of super
[02:23:33.920 --> 02:23:41.520]   forecasters on meticulous or something. And what is their median year estimate?
[02:23:41.520 --> 02:23:48.400]   Well, some of the meticulous AGI questions actually are kind of shockingly, uh, soon.
[02:23:48.400 --> 02:23:55.680]   Okay. Um, for AGI, um, there's a much larger, uh, differentiator there on the market on the
[02:23:55.680 --> 02:24:04.000]   meticulous forecasts of AI disaster and doom. Okay. Okay. More like a few, a few percent or less
[02:24:04.000 --> 02:24:12.240]   rather than like 20%. Got it. And the third is that, you know, generally when you ask, uh, many
[02:24:12.240 --> 02:24:18.640]   economists, could an AGI cause rapid, rapid economic growth, they usually have some story
[02:24:18.640 --> 02:24:25.760]   about bottlenecks in the economy that could prevent this kind of explosion of, um, uh, of
[02:24:25.760 --> 02:24:31.040]   these kinds of feedback loops. So, you know, you have all these different pieces of outside view
[02:24:31.040 --> 02:24:36.960]   evidence. They're obviously different. So I'm curious, you can take them in any sequence you
[02:24:36.960 --> 02:24:43.520]   want. Um, but yeah, what, what do you think is miscalibrating them? Yeah. So, uh, one of course,
[02:24:43.520 --> 02:24:50.240]   there's, um, to some, some of those, uh, components are, was the meticulous AI timelines are, are
[02:24:50.240 --> 02:24:58.560]   relatively short. Uh, there's also of course, the surveys of AI experts, um, you know, conducted at
[02:24:58.560 --> 02:25:05.680]   some of the ML conferences, which, uh, have definitely longer times to AI more, um, you know,
[02:25:05.680 --> 02:25:10.560]   several decades in the future. Although you can ask the questions in ways that elicit very
[02:25:10.560 --> 02:25:16.240]   different answers and show most of the respondents are not thinking super hard, uh, about their
[02:25:16.240 --> 02:25:23.120]   answers. Uh, it looks like now close in the, in the recent AI surveys, close to half, we're putting
[02:25:23.120 --> 02:25:30.400]   around 10% risk of an outcome from AI, uh, close to as bad as human extinction. Um, and then another
[02:25:30.400 --> 02:25:40.000]   large chunk, uh, like 5%. Um, so that was the median. Uh, so I'd say, um, compared to the
[02:25:40.000 --> 02:25:47.920]   typical AI expert, I am, uh, estimating a higher risk. Uh, oh, also on the, on the topic of takeoff
[02:25:47.920 --> 02:25:52.880]   and the AI expert survey, I think the general argument for intelligence explosion, I think
[02:25:52.880 --> 02:25:59.360]   commanded majority support, but not a large majority. So you can say I'm, I'm closer on that
[02:25:59.360 --> 02:26:04.960]   front. And then of course, there's, uh, at the beginning, I mentioned, uh, these great, these
[02:26:04.960 --> 02:26:12.800]   greats of computing, the, uh, founders can be like Alan Turing, Von Neumann. And then today you have
[02:26:12.800 --> 02:26:19.840]   people, uh, like Jeff Hinton saying, uh, saying these things, or the, uh, the people at OpenAI,
[02:26:19.840 --> 02:26:26.720]   uh, and DeepMind are making, uh, noises, suggesting, uh, timelines, uh, in line with
[02:26:26.720 --> 02:26:34.160]   what we've discussed and saying serious risk, uh, apocalyptic, uh, outcomes from them.
[02:26:34.160 --> 02:26:40.720]   Um, so there's some other sources of evidence, evidence there, uh, but I do, uh, acknowledge,
[02:26:40.720 --> 02:26:48.240]   and it's important to say and engage with and see what it means, um, that these views are like,
[02:26:48.240 --> 02:26:55.040]   are contrarian, not widely held in particular, the sort of detailed models, uh, that I've been
[02:26:55.040 --> 02:27:02.000]   working with are not something, uh, that most people are almost anyone, uh, is examining, uh,
[02:27:02.000 --> 02:27:08.000]   these problems through. Um, you, you do find, you know, parts of similar analyses, uh, people
[02:27:08.000 --> 02:27:13.200]   in AI labs, there've been, there's been other work. I mentioned Moravec and Kurzweil earlier.
[02:27:13.200 --> 02:27:18.080]   Um, there also have been a number of papers doing various kinds of economic modeling. So,
[02:27:19.040 --> 02:27:26.080]   um, standard economic growth models, um, when you input AI related parameters commonly predict
[02:27:26.080 --> 02:27:33.040]   explosive growth. Um, and so there, there's a divide between what the models say and especially
[02:27:33.040 --> 02:27:38.080]   what the models say with these empirical values, uh, derived from the actual field of that, that
[02:27:38.080 --> 02:27:44.400]   link up has not been done even by the economists working on AI largely. Um, which is one reason
[02:27:44.400 --> 02:27:50.720]   for the report from open philanthropy, um, by Tom Davidson building on these models, uh, and
[02:27:50.720 --> 02:27:55.680]   putting that out, uh, for review discussion, engagement, and communication on these ideas.
[02:27:55.680 --> 02:28:01.200]   So part of it, it's, uh, yeah, I want to raise these issues. Uh, it's one reason I came on the
[02:28:01.200 --> 02:28:06.800]   podcast and then they have the opportunity to actually, uh, examine the arguments and evidence
[02:28:06.800 --> 02:28:14.080]   and engage with it. Um, I do predict that over time, you know, the things will be more adopted
[02:28:14.080 --> 02:28:20.880]   has AI developments become more clear. Uh, obviously that's a coherence condition of,
[02:28:20.880 --> 02:28:27.280]   uh, believing the things to be true. If you think that, uh, society can see things when the
[02:28:27.280 --> 02:28:32.880]   questions are resolved, uh, which seems likely. So what would you predict, for example, that
[02:28:32.880 --> 02:28:39.840]   interest rates will increase in the coming years? Yeah. So I think at some point, uh, so in the case,
[02:28:39.840 --> 02:28:46.160]   uh, we were talking about where there are visible, um, so this intelligence explosion
[02:28:46.160 --> 02:28:51.200]   happening in software, uh, to the extent that investors are noticing that, um, yeah,
[02:28:51.200 --> 02:28:58.560]   they should be willing to lend money or make equity, uh, investments, uh, in these firms or
[02:28:58.560 --> 02:29:07.120]   demanding extremely high interest rates. Uh, because if it's possible to turn capital into
[02:29:07.120 --> 02:29:12.880]   twice as much capital, uh, in a relatively short period, uh, and then more shortly after that,
[02:29:12.880 --> 02:29:18.160]   then yeah, you should, uh, you should demand a much higher return and competition. If there's,
[02:29:18.160 --> 02:29:25.840]   assuming there's competition, uh, among companies, uh, or coalitions for resources,
[02:29:25.840 --> 02:29:31.120]   whether that's investment or ownership of cloud compute is cloud compute made available to a,
[02:29:31.120 --> 02:29:37.280]   a particular AI development effort, um, could be quite in demand, but what that would happen
[02:29:37.280 --> 02:29:45.120]   before you have so much investor cash, uh, making purchases and sales on this basis,
[02:29:45.120 --> 02:29:51.280]   um, you would first see it in things like the valuations of the AI companies,
[02:29:51.280 --> 02:29:58.720]   evaluations of AI chip makers. And so far there have been effects. Uh, so some years ago in the
[02:29:58.720 --> 02:30:05.200]   2010s, um, I did some analysis with other people of if this kind of picture happens,
[02:30:05.200 --> 02:30:12.000]   then which are the firms and parts of the economy, uh, that would benefit. And so there's the,
[02:30:12.000 --> 02:30:20.080]   the makers of chip equipment, um, companies like ASML, uh, there's the, the fabs like TSMC,
[02:30:20.080 --> 02:30:26.640]   there's chip designers, uh, like Nvidia or the, uh, the component of Google that does things like
[02:30:26.640 --> 02:30:32.080]   design the TPU. Um, and then there are companies working on the software. So the big tech giants
[02:30:32.080 --> 02:30:39.200]   and also companies like open AI and deep mind. And in general, the portfolio picking at those has,
[02:30:39.200 --> 02:30:43.680]   has done well, it's done better than the market because as everyone can see, there's been an AI
[02:30:43.680 --> 02:30:52.000]   boom. Um, but it's obviously far short of what you would get if you predicted this is going to go to
[02:30:52.000 --> 02:30:57.760]   be like on the scale of the global economy and the global economy is going to be skyrocketing
[02:30:57.760 --> 02:31:04.000]   into the stratosphere, uh, within 10 years, if that were the case, then collectively these AI
[02:31:04.000 --> 02:31:10.800]   companies should be worth a large fraction of the global portfolio. And so I embrace the criticism
[02:31:10.800 --> 02:31:16.640]   that, uh, this is indeed, uh, contrary to the efficient market hypothesis. I think it's,
[02:31:16.640 --> 02:31:22.400]   it's a true hypothesis that the market is in the course of updating on, um, in the same way
[02:31:22.400 --> 02:31:29.920]   that, you know, coming, coming into, uh, the topic in the 2000, I thought, yes,
[02:31:29.920 --> 02:31:36.400]   that the strong case, uh, even an old case that AI will eventually be biggest thing in the world.
[02:31:36.400 --> 02:31:43.680]   It's kind of crazy that the investment in it is so small. And over the last 10 years, we've seen,
[02:31:44.560 --> 02:31:51.840]   um, the tech industry and academia, uh, sort of realize, yeah, they were wildly under-investing,
[02:31:51.840 --> 02:31:57.920]   uh, and just throwing compute, uh, and effort into these AI models in particular, like letting the
[02:31:57.920 --> 02:32:05.200]   neural network connectionist, connectionist paradigm, uh, kind of languish, uh, in a AI winter.
[02:32:05.200 --> 02:32:13.200]   And so, yeah, I expect that process to continue, uh, as it's done over several orders of magnitude
[02:32:13.200 --> 02:32:18.800]   of scale up. Uh, and I expect at the later end of that scale, which the market is partially
[02:32:18.800 --> 02:32:22.480]   already pricing in, uh, it's going to go further than the market expects.
[02:32:22.480 --> 02:32:28.560]   Has your portfolio, uh, since the analysis you did that many years ago changed? Um, are the
[02:32:28.560 --> 02:32:32.160]   companies you identified then still the ones that seem most likely to benefit from the AI boom?
[02:32:32.160 --> 02:32:36.960]   I mean, a general, um, issue with sort of tracking that kind of thing and new companies come in.
[02:32:36.960 --> 02:32:44.000]   So like open AI did not exist. Uh, anthropic did not exist. Um, you know, any number of things,
[02:32:44.000 --> 02:32:52.640]   it's a personal portfolio. I do not invest in any AI labs, um, for conflict of interest reasons.
[02:32:52.640 --> 02:32:57.920]   I have invested in the broader industry. Um, and I, I don't think that they, uh,
[02:32:57.920 --> 02:33:04.160]   the conflict issues, um, are very significant because there are enormous companies
[02:33:04.160 --> 02:33:09.680]   and their cost of capital is not particularly affected by marginal investment. And I'm not
[02:33:09.680 --> 02:33:16.960]   really, uh, in a, yeah, I have less, uh, concern that I might find myself in a conflict of interest
[02:33:16.960 --> 02:33:22.560]   situation there. I'm kind of curious about what the day in the life of, uh, somebody like you
[02:33:22.560 --> 02:33:26.880]   looks like. I mean, if you listen to this conversation, however many hours of it it's
[02:33:26.880 --> 02:33:33.200]   been, we've, uh, gotten thoughts that were for me, incredibly insightful and novel about everything
[02:33:33.200 --> 02:33:41.360]   from, uh, primate evolution, the geopolitics to, um, you know, the, the, what, what sorts of
[02:33:41.360 --> 02:33:47.440]   improvements are plausible with, um, language models. Uh, so, you know, there's like a huge
[02:33:47.440 --> 02:33:53.520]   variety of topics that you're studying and investigating. Well, are you just like reading
[02:33:53.520 --> 02:33:57.280]   all day? Like, what is it? Well, what happens when you wake up? You just like pick up a paper.
[02:33:57.280 --> 02:34:01.840]   Yeah. So I'd say you're, you're somewhat getting the benefit of the fact that I've done
[02:34:01.840 --> 02:34:07.760]   fewer podcasts. And so I have a backlog of things that have not shown up in publications yet.
[02:34:07.760 --> 02:34:16.400]   Um, but yes, also I've had a very weird professional, uh, career, um, that is involved
[02:34:16.400 --> 02:34:22.320]   a much, much higher proportion than is normal, uh, trying to build more comprehensive models
[02:34:22.320 --> 02:34:28.720]   of the world. Uh, and so that has included being more of a general is trying to get on
[02:34:29.520 --> 02:34:36.000]   understanding of many issues, uh, and many problems that had not yet been widely addressed,
[02:34:36.000 --> 02:34:43.440]   but do a first pass and a second pass dive into them. Um, and just having spent years of my life
[02:34:43.440 --> 02:34:50.320]   working on that, um, you know, some of it accumulates, um, in terms of what, what is a
[02:34:50.320 --> 02:34:56.800]   day in the life? How do you, how do I go about it? So one is just keeping abreast of literatures,
[02:34:56.800 --> 02:35:04.480]   uh, on a lot of these topics, reading books and academic works on them, uh, doing my approach
[02:35:04.480 --> 02:35:09.280]   compared to some other, other people in forecasting and assessing some of these things.
[02:35:09.280 --> 02:35:18.880]   Uh, I try to obtain and rely on more, uh, any data that I can find that is relevant. Um, I try
[02:35:18.880 --> 02:35:25.920]   early and often, uh, to find factual information, uh, that bears on some of the questions I've got,
[02:35:25.920 --> 02:35:32.400]   especially, uh, in a quantitative fashion, do the basic arithmetic and consistency checks and check
[02:35:32.400 --> 02:35:40.560]   sums, um, on a hypothesis about the world, uh, do that early and often. Uh, and I find that's,
[02:35:40.560 --> 02:35:46.240]   that's quite fruitful, um, and that people don't do it enough. Uh, but so things like with the
[02:35:46.240 --> 02:35:53.600]   economic growth, um, just when someone mentioned the diminishing returns, I immediately asked,
[02:35:53.600 --> 02:36:00.800]   Hmm. Okay. So you have two exponential processes. What's the ratio, uh, between the Dublin you get
[02:36:00.800 --> 02:36:09.760]   on the output versus the input, uh, and find, Oh yeah, actually it's interesting. The, uh,
[02:36:09.760 --> 02:36:13.920]   for computing and information technology and AI software, uh, it's well on the one side,
[02:36:13.920 --> 02:36:19.840]   there are other technologies that are, are closer to neutral. Um, and so whenever I can go from,
[02:36:19.840 --> 02:36:24.560]   here's a vague qualitative consideration in one direction, and here's a vague qualitative
[02:36:24.560 --> 02:36:30.720]   consideration in the other direction, I try and find some data, do some simple Fermi calculations
[02:36:30.720 --> 02:36:37.120]   back of the envelope calculations, um, and see like, can I get a consistent picture of the world
[02:36:37.120 --> 02:36:44.400]   being one way, the world being another also compared to some, I, I try, I try to be exhaustive
[02:36:44.400 --> 02:36:50.240]   more. Uh, so I'm very interested in finding things like taxonomies of the world where I can go
[02:36:50.240 --> 02:36:56.720]   systematically through all of the possibilities. Uh, so for example, my work with open philanthropy,
[02:36:56.720 --> 02:37:03.200]   um, and previously on global catastrophic risks, I wanted to make sure I'm not missing
[02:37:03.200 --> 02:37:11.680]   any big thing, anything that could be the biggest thing. Um, and I wound up mostly focused on AI,
[02:37:11.680 --> 02:37:16.880]   but, um, you know, there have been other things that have been raised as candidates and people
[02:37:16.880 --> 02:37:23.680]   sometimes say, I think falsely, uh, Oh yeah, this is just another doom Jay story. You know,
[02:37:23.680 --> 02:37:30.560]   there must be hundreds, hundreds of those. Um, and so I would do things like go through all
[02:37:30.560 --> 02:37:35.840]   of the different major scientific fields, you know, from anthropology to biology, chemistry,
[02:37:37.040 --> 02:37:44.800]   computer science, physics, what are the doom stories, um, or like candidates for big things
[02:37:44.800 --> 02:37:51.600]   associated with each of these field, go, go through the industries, um, that the U S economic
[02:37:51.600 --> 02:37:57.040]   statistics agencies recognize and say for each of these industries, is there something associated
[02:37:57.040 --> 02:38:04.160]   with them? Um, go through all of the lists that people have made before, uh, of threats of doom
[02:38:04.160 --> 02:38:09.120]   search for previous literature of people who have done discussions and then, yeah,
[02:38:09.120 --> 02:38:14.960]   have a big spreadsheet, uh, of what the candidates are. Um, and some other colleagues have done work
[02:38:14.960 --> 02:38:20.560]   of this sort as well, uh, and just go through each of them, see how they check out. Uh, and it turned
[02:38:20.560 --> 02:38:28.880]   out doing that kind of, of exercise found that actually the distribution of candidates for
[02:38:28.880 --> 02:38:34.560]   risks of global catastrophe, uh, it was very skewed. There were a lot of things that have
[02:38:34.560 --> 02:38:38.480]   been mentioned in the media has like a potential doomsday story. So things like,
[02:38:38.480 --> 02:38:45.040]   Oh, something is happening to the bees. Will that be the end of humanity? Um, and this gets to the
[02:38:45.040 --> 02:38:51.680]   media, but if you, if you track it through, well, okay, no, there's yeah, there are infestations
[02:38:51.680 --> 02:38:57.360]   and B populations are causing local collapses that can then be sort of easily reversed. They just
[02:38:57.360 --> 02:39:03.120]   breed some more, um, or do some other things to, to treat this. And even if all the honeybees
[02:39:03.120 --> 02:39:08.640]   were extinguished immediately, uh, the plants that they pollinate, uh, actually don't account
[02:39:08.640 --> 02:39:14.160]   for much of human nutrition. You could swap the arable land with others and there would be other
[02:39:14.160 --> 02:39:22.160]   ways, uh, to pollinate and support the things. And so at the media level, there were many tales
[02:39:22.160 --> 02:39:28.000]   of, uh, here, here's a doomsday story. When you go further to the scientists and were their
[02:39:28.000 --> 02:39:34.880]   arguments for it to actually check out, it was, it was not there, but by actually systematically
[02:39:34.880 --> 02:39:39.120]   looking through many of these candidates, I wound up in a different epistemic situation
[02:39:39.120 --> 02:39:44.960]   than someone who's just buffeted by news reports. And they see article after article, uh, that is
[02:39:44.960 --> 02:39:49.760]   claiming something is going to destroy the world. And it turns out it's like by way of headline,
[02:39:49.760 --> 02:39:53.680]   grabbing and attempts by media to like over-interpret something that was said by some
[02:39:53.680 --> 02:39:58.720]   activists who was trying to over-interpret some real phenomenon. Um, and then most of these go
[02:39:58.720 --> 02:40:06.320]   away. And then a few things, things like nuclear war, biological weapons, artificial intelligence,
[02:40:06.320 --> 02:40:12.480]   check out more strongly. And when you weigh things like what do experts in the field think,
[02:40:12.480 --> 02:40:18.240]   what kind of evidence can they muster? Uh, yeah, you find this, this extremely skewed distribution.
[02:40:18.240 --> 02:40:24.400]   And I found that was really a valuable benefit of doing those deep dive investigations into many
[02:40:24.400 --> 02:40:30.960]   things in a systematic way, because now I can answer actually the sort of a loose agnostic who
[02:40:30.960 --> 02:40:38.800]   knows and all the, all this nonsense by diving deeply. I really enjoy, uh, talking to sort of
[02:40:38.800 --> 02:40:43.440]   like people who have like a big picture thesis on the podcast and interviewing them. But one thing
[02:40:44.080 --> 02:40:50.320]   that, um, I've noticed and, uh, is not satisfying is that often they come from a very like
[02:40:50.320 --> 02:40:55.840]   philosophical or biased perspective. This is useful in certain contexts, but there's like
[02:40:55.840 --> 02:41:02.560]   basically maybe three people in the entire world who have a sort of very rigorous and
[02:41:02.560 --> 02:41:08.880]   scientific approach to thinking about the whole picture, or at least like three people I'm aware
[02:41:08.880 --> 02:41:18.080]   of maybe like two. Um, and yeah, I mean, it's like something I also, um, there's like no,
[02:41:18.080 --> 02:41:23.360]   I guess, university or, uh, existing academic discipline for people who are trying to
[02:41:23.360 --> 02:41:28.800]   come up with a big picture. And so there's no established standards. And so people can.
[02:41:28.800 --> 02:41:34.240]   I hear you. This is a problem. And this is an experience also with a lot of the,
[02:41:34.880 --> 02:41:38.560]   I mean, I think Holden was mentioning this in your previous episode with a lot of the
[02:41:38.560 --> 02:41:44.640]   world's investigations work. These are questions where there is no academic field, uh, whose job
[02:41:44.640 --> 02:41:52.320]   it is to work on these and has norms that allow making a best efforts go at it. Um, often academic
[02:41:52.320 --> 02:41:59.040]   norms will allow only plucking off narrow pieces, uh, that might contribute to answering a big
[02:41:59.040 --> 02:42:05.200]   question. But the, the problem of actually assembling what science knows that bears on
[02:42:05.200 --> 02:42:09.120]   some important question that people care about the answer to, uh, it falls through the crack.
[02:42:09.120 --> 02:42:14.880]   There's no discipline to do that job. So you have countless academics and researchers building up
[02:42:14.880 --> 02:42:21.680]   local pieces of the thing. Uh, and yet people don't follow the Hamming questions. Uh, you know,
[02:42:21.680 --> 02:42:25.440]   what's the most important problem in your field? Why aren't you working on it? Uh, I mean,
[02:42:25.440 --> 02:42:29.440]   that one actually might not work because if the field boundaries are defined too narrowly,
[02:42:29.440 --> 02:42:34.640]   um, you know, you'll, you'll leave it out. Um, but yeah, there are important problems,
[02:42:34.640 --> 02:42:41.280]   uh, for the world as a whole that it's, it's sadly not the job of like, you know, a large
[02:42:41.280 --> 02:42:46.320]   professionalized academic field, uh, or organization to do. And I, hopefully that's
[02:42:46.320 --> 02:42:52.640]   something that can change in the future. Um, but for my career, it's been a matter of taking low
[02:42:52.640 --> 02:42:58.720]   hanging fruit of important questions that sadly people haven't invested in doing the, uh, the
[02:42:58.720 --> 02:43:02.560]   basic analysis on. So I think I was trying to think about more recently for the podcast is
[02:43:02.560 --> 02:43:08.640]   I would like to have a better world model after doing an interview. And often I feel like I do
[02:43:08.640 --> 02:43:13.120]   in some cases after some interviews, I feel like, Oh, that was entertaining. But like, do I
[02:43:13.120 --> 02:43:18.480]   fundamentally have a better prediction of what the world looks like in, you know, 2,200 or 2,100,
[02:43:18.480 --> 02:43:23.520]   or like at least what counterfactuals are ruled out or something. I'm curious if you have like
[02:43:23.520 --> 02:43:30.560]   advice on first identifying the kinds of thinkers and topics, which will contribute to a more
[02:43:30.560 --> 02:43:37.360]   concrete understanding of the world. And second, how to go about analyzing their main ideas in a
[02:43:37.360 --> 02:43:42.080]   way that, uh, concretely adds to that picture. Like this was a great episode, right? This is
[02:43:42.080 --> 02:43:46.320]   like literally the top in terms of contributing to my world model in terms of all the episodes
[02:43:46.320 --> 02:43:54.560]   I've done. How do I find more of these? Glad to hear that. Um, one general heuristic, uh,
[02:43:54.560 --> 02:44:06.800]   is to find ways to hue closer, uh, to sort of, yeah, things that are rich and sort of bodies of
[02:44:06.800 --> 02:44:12.560]   established knowledge, um, and less on punditry. I don't know how you've been navigating that
[02:44:12.560 --> 02:44:20.960]   so far. Um, but so learning from textbooks, um, and the sort of the things that were the, uh,
[02:44:20.960 --> 02:44:27.280]   leading papers and people of past eras, I think rather than being too attentive to current news
[02:44:27.280 --> 02:44:36.880]   cycles is quite valuable. Yeah. I don't usually have the experience of here is a, you know,
[02:44:36.880 --> 02:44:43.600]   someone doing things very systematically over a huge area. I want, you know, I can just read all
[02:44:43.600 --> 02:44:50.400]   of their stuff and then, um, absorb it. And then I'm, I'm sad, uh, except there are a lot, lots
[02:44:50.400 --> 02:44:57.360]   of people who do wonderful works, um, you know, on, in their own fields. Um, and some of those
[02:44:57.360 --> 02:45:03.680]   fields are, are broader, uh, broader than others. Um, I think I would wind up giving a lot of
[02:45:03.680 --> 02:45:12.800]   recommendations of just like great particular works and particular explorations of, of an issue,
[02:45:12.800 --> 02:45:20.000]   uh, or history. Do you have that somewhere? This list? Uh, Vaclav Smil's books. Um, I don't,
[02:45:20.000 --> 02:45:28.880]   I think I often disagree with some of his methods of synthesis, but, uh, I enjoy his books for
[02:45:28.880 --> 02:45:36.880]   giving, um, you know, pictures of a lot of interesting, relevant facts about how the world
[02:45:36.880 --> 02:45:48.800]   works. I would say, um, yeah, so some of, uh, Joel Mokir's work on the, the history of the
[02:45:48.800 --> 02:45:56.480]   scientific revolution and how that interacted with economic growth. Um, a sort of example of
[02:45:56.480 --> 02:46:01.520]   collecting a lot of evidence, uh, a lot of interesting, valuable, uh, assessment there.
[02:46:01.520 --> 02:46:08.800]   I think in the, in the space of AI forecasting, um, one person I would recommend going back to
[02:46:08.800 --> 02:46:15.600]   is the work of Hans Moravec. And it was not always the most precise or reliable, but an incredible
[02:46:15.600 --> 02:46:22.800]   number of, uh, sort of brilliant, innovative ideas came out of that. Um, and I think he was
[02:46:22.800 --> 02:46:29.360]   someone who was, uh, who really grokked a lot of the, the arguments for a more sort of compute
[02:46:29.360 --> 02:46:36.560]   centric, uh, way of thinking about what was happening with AI very early on. Um, he was
[02:46:36.560 --> 02:46:42.160]   writing stuff in the, uh, in the seventies, um, maybe, uh, maybe even earlier, but at least in
[02:46:42.160 --> 02:46:48.000]   the seventies, eighties, nineties, uh, so his book, mine children, some of his early academic
[02:46:48.000 --> 02:46:53.280]   papers, fascinating, not necessarily for the methodology I've been talking about, uh, but for
[02:46:53.280 --> 02:46:56.160]   exploring the substantive topics that we were discussing in the episode.
[02:46:56.160 --> 02:46:58.400]   Is a Malthusian state inevitable in the long run?
[02:46:58.400 --> 02:47:08.480]   Nature in general is in Malthusian states. And, uh, you know, that can mean organisms that are,
[02:47:08.480 --> 02:47:13.440]   you know, typically struggling for food. It can mean typically struggling at a margin
[02:47:13.440 --> 02:47:18.480]   of how the population density rises. They kill each other more often, uh, contesting for that
[02:47:18.480 --> 02:47:24.000]   can mean, uh, frequency dependent disease. It's like different ant species become more common in
[02:47:24.000 --> 02:47:29.440]   an area at their, uh, species, specific diseases, swoop through them. And the general process is,
[02:47:29.440 --> 02:47:36.480]   yeah, you have, uh, some things that can replicate and expand. Um, and they do that until they can't
[02:47:36.480 --> 02:47:42.800]   do it anymore. Uh, and that means there's some limiting factor that can't keep up that doesn't
[02:47:42.800 --> 02:47:52.080]   necessarily have to apply to human civilization. Um, it's possible for there to be like collective
[02:47:52.080 --> 02:48:02.240]   norm setting, um, that blocks, uh, evolution towards maximum reproduction. Um, so right now,
[02:48:02.240 --> 02:48:10.720]   human fertility is often sub replacement. And if you sort of extrapolated the fertility falls that
[02:48:10.720 --> 02:48:16.080]   come with economic development and education, uh, then you would think, okay, yeah, well,
[02:48:16.080 --> 02:48:21.120]   the total fertility rate will fall below replacement. Uh, and then humanity after some
[02:48:21.120 --> 02:48:25.680]   number of generations will go extinct because every generation will be smaller than the previous one.
[02:48:25.680 --> 02:48:31.840]   Uh, now pretty obviously that's not going to happen. Um, one reason, uh, is because we'll
[02:48:31.840 --> 02:48:39.120]   produce artificial intelligence, um, which can replicate at extremely rapid rates. Um, and may
[02:48:39.120 --> 02:48:46.640]   do it, uh, because, uh, they're asked or programmed to, um, or wish, wish to gain some benefit
[02:48:46.640 --> 02:48:52.320]   and they can pay for their creation, uh, and pay back the resources needed to create them
[02:48:52.320 --> 02:48:57.600]   very, very quickly. Uh, and so, yeah, financing for that reproduction is easy. And if you have
[02:48:57.600 --> 02:49:05.040]   one AI system that chooses to replicate in that way, or some organization or institution or
[02:49:05.040 --> 02:49:13.120]   society that chooses to create some AIs that are, um, willing to be replicated, then that can expand
[02:49:13.120 --> 02:49:19.680]   to make use of any amount of natural resources that can support them, uh, and to do more work,
[02:49:19.680 --> 02:49:26.400]   produce, produce more economic value. Uh, and so, yeah, it's like, well, what limits will limit
[02:49:26.400 --> 02:49:33.440]   population growth given the selective pressures where if even one individual wants to replicate
[02:49:33.440 --> 02:49:41.680]   a lot, um, they can do so, um, incessantly. Uh, so that could be individually resource limited.
[02:49:41.680 --> 02:49:48.320]   So it could be, um, that individuals and organizations have some endowment of natural
[02:49:48.320 --> 02:49:55.200]   resources, um, and they can't get one another's endowments. And so some choose to have many
[02:49:55.200 --> 02:50:01.680]   offspring, uh, or produce many AIs. Uh, and then the natural resources that they possess
[02:50:01.680 --> 02:50:07.360]   are subdivided among a greater population while in another jurisdiction or another
[02:50:07.360 --> 02:50:12.480]   individual may choose not to subdivide, um, their wealth. And in that case, you have
[02:50:12.480 --> 02:50:18.160]   Malthusianism in the sense that within some particular jurisdiction or set of property
[02:50:18.160 --> 02:50:24.560]   rights, um, you have a population that is increased, uh, up until, uh, until some limiting
[02:50:24.560 --> 02:50:28.960]   factor, uh, which could be like, they're literally using all of their resources. They have nothing
[02:50:28.960 --> 02:50:34.400]   left for things like defense or economic investment, or it could be, uh, something that's
[02:50:34.400 --> 02:50:41.440]   more like, um, if you invested more natural resources into population, uh, it would come
[02:50:41.440 --> 02:50:46.080]   at the expense of something else necessary, uh, including military resources. If you're
[02:50:46.080 --> 02:50:52.240]   in a competitive situation where there remains war and anarchy, um, and there aren't secure
[02:50:52.240 --> 02:50:58.720]   property rights to maintain, um, maintain wealth in place. Uh, if you have a situation
[02:50:58.720 --> 02:51:04.800]   where, uh, there's pooling of resources, for example, say you have a universal basic income
[02:51:04.800 --> 02:51:12.160]   that's funded by taxation of natural resources. Uh, and then it's distributed evenly, uh,
[02:51:12.160 --> 02:51:18.800]   to like every mind, uh, above a certain sort of scale of complexity per unit time. So each
[02:51:18.800 --> 02:51:25.520]   second of mine exists to get something such, uh, an allocation, um, in that case, then,
[02:51:25.520 --> 02:51:32.320]   all right, well, those who replicate, uh, as much as they can afford, um, with this,
[02:51:32.320 --> 02:51:39.600]   this income, do it, uh, and increase their population approximately immediately until
[02:51:39.600 --> 02:51:45.040]   the funds for the universal basic income paid for from the natural resource taxation
[02:51:45.680 --> 02:51:52.080]   divided by the set of recipients is just barely enough to pay for the existence of, of one more
[02:51:52.080 --> 02:51:57.040]   mind. And so it's, there's like a Malthusian element, uh, and that this income has been
[02:51:57.040 --> 02:52:02.240]   reduced to near the AI subsistence level or the subsistence level of whatever qualifies for the
[02:52:02.240 --> 02:52:06.960]   subsidy, given that this all happens almost immediately. Um, you know, people who might
[02:52:06.960 --> 02:52:12.640]   otherwise have enjoyed the basic income may object and say, no, no, this is no good. Um,
[02:52:13.520 --> 02:52:21.120]   and they, they might respond by saying, well, something like the subdivision before, um,
[02:52:21.120 --> 02:52:27.200]   maybe there's a restriction, there's an, a distribution of wealth. And then when one has
[02:52:27.200 --> 02:52:32.240]   a child, there's a requirement that one gives them a certain minimum quantity of resources.
[02:52:32.240 --> 02:52:37.200]   And when doesn't have the resources to give them that minimum standard of living or standard of
[02:52:37.200 --> 02:52:46.240]   wealth. Um, yeah, when, uh, can I do that because of child slash AI, uh, welfare laws, or you could
[02:52:46.240 --> 02:52:52.960]   have a, a system that is more accepting of diversity and preferences. Uh, and so you have
[02:52:52.960 --> 02:53:01.040]   some societies or some jurisdictions or families, uh, that go the route of having many people with
[02:53:01.040 --> 02:53:06.560]   less natural resources per person, uh, and others that go a direction of having fewer people and
[02:53:06.560 --> 02:53:13.440]   more natural resources per person, uh, and they just coexist. Um, but sort of how much of each
[02:53:13.440 --> 02:53:20.400]   you get, uh, sort of depends on how attached people are to things that don't work, uh, with
[02:53:20.400 --> 02:53:26.240]   separate policies for separate jurisdictions, things like global redistribution that's ongoing
[02:53:26.240 --> 02:53:34.080]   continuously versus, uh, the sort of, you know, infringements on autonomy. Um, if you're,
[02:53:34.080 --> 02:53:38.960]   if you're saying that a mind can't be created, even though it has a standard of living, that's
[02:53:38.960 --> 02:53:45.120]   far better than ours. Um, because of the, the advanced technology of the time, because it would
[02:53:45.120 --> 02:53:51.600]   reduce the average per capita income, I have any more capital around. Um, yeah, then that would,
[02:53:51.600 --> 02:53:58.080]   would pull in the other direction. And that's the kind of, the kind of, uh, values, values,
[02:53:58.080 --> 02:54:04.080]   judgment, and sort of social, uh, coordination problem that people would have to, uh, negotiate
[02:54:04.080 --> 02:54:09.360]   for and things like democracy and inter international relations and sovereignty,
[02:54:09.360 --> 02:54:14.480]   uh, would apply to help solving what would warfare in space look like? What would offense
[02:54:14.480 --> 02:54:18.880]   or defense of the advantage with the equilibrium set by mutually assured destruction still be
[02:54:18.880 --> 02:54:26.080]   applicable just generally what is the picture of, well, they, the extreme difference is that things,
[02:54:26.080 --> 02:54:31.680]   uh, outside, especially outside the solar system, uh, things are very far apart, uh, and there's a
[02:54:31.680 --> 02:54:36.720]   speed of light limit and to get close to the speed of light limit, you have to use an enormous amount
[02:54:36.720 --> 02:54:45.600]   of energy. Um, and so, uh, there would be, that would tend to, in some ways could favor the, uh,
[02:54:45.600 --> 02:54:49.920]   favor the defender because you have something that's coming in at a large fraction of the speed
[02:54:49.920 --> 02:54:57.840]   of light, uh, and it hits a grain of dust and it explodes and the amount of matter you can send to
[02:54:57.840 --> 02:55:07.440]   another galaxy or distant star for a given amount of, uh, reaction mass and energy input, uh, is
[02:55:07.440 --> 02:55:14.080]   limited. So it's hard to send an amount of military material to another location as what can be
[02:55:14.080 --> 02:55:20.480]   present there already locally. Um, that, that would seem like it would make it harder for the
[02:55:20.480 --> 02:55:26.640]   attacker between stars or between galaxies. Uh, but there are a lot of, of other considerations.
[02:55:26.640 --> 02:55:36.080]   One thing is the extent to which the matter in a region, uh, can be harnessed all at once. So
[02:55:37.920 --> 02:55:44.000]   we have a lot of mass and energy, uh, in a star, but it's only being doled out over billions of
[02:55:44.000 --> 02:55:51.120]   years because hydrogen hydrogen fusion, you know, exceedingly hard outside of a star. Um, you know,
[02:55:51.120 --> 02:55:57.040]   it's a very, very slow, uh, and difficult reaction. Uh, and if you can't turn the star
[02:55:57.040 --> 02:56:03.360]   into energy faster, then it's this huge resource that will be worthwhile for billions of years.
[02:56:04.480 --> 02:56:12.800]   And so even very inefficiently attacking a solar system, uh, to acquire the stuff that's there
[02:56:12.800 --> 02:56:19.200]   could pay off. So if, if it takes a thousand years of a star's output, uh, to launch an attack on
[02:56:19.200 --> 02:56:26.000]   another star, and then you, you hold it for a billion years after that, um, then it can be
[02:56:26.000 --> 02:56:32.080]   the case that just like a larger surrounding attacker, uh, might be able to even very
[02:56:32.080 --> 02:56:40.080]   inefficiently send attacks at like a civilization, uh, that was small but accessible. And then if you
[02:56:40.080 --> 02:56:45.280]   can, if you can quickly burn the resources that the attacker might want to acquire, if you can put
[02:56:45.280 --> 02:56:51.760]   stars into black holes and extract most of the usable energy, uh, before the attacker can take
[02:56:51.760 --> 02:56:58.400]   them over, then it would be like scorched earth. Um, it's like most of what you were trying to
[02:56:58.400 --> 02:57:05.280]   capture could be, um, expended on military material to fight you and you don't actually
[02:57:05.280 --> 02:57:11.360]   get much that is worthwhile and you paid a lot to do it. Does that favor the defense? Um, you know,
[02:57:11.360 --> 02:57:17.440]   at, at this level, it's, it's pretty challenging to net out all of the factors, including all the
[02:57:17.440 --> 02:57:25.120]   future, uh, the future technologies. Um, I think, yeah, I mean, the burden of interstellar attack
[02:57:25.120 --> 02:57:30.960]   being just like quite high, uh, compared to our conventional things, uh, seems real, but at the
[02:57:30.960 --> 02:57:37.280]   level of over millions of years, Wayne and that thing, does it result in if they're aggressive
[02:57:37.280 --> 02:57:43.600]   conquest or not, or is every star or galaxy, you know, approximately impregnable impregnable enough
[02:57:43.600 --> 02:57:47.840]   not to be worth attacking. Um, I'm not going to say I know the answer.
[02:57:47.840 --> 02:57:54.720]   Okay. Final question. How do you think about info hazards when talking about your work?
[02:57:54.720 --> 02:58:00.000]   So obviously if there's a risk, you want to warn people about it, but you don't want to give
[02:58:00.000 --> 02:58:07.040]   careless or potentially like homicidal people ideas. I, when Eliezer was on the podcast, he,
[02:58:07.040 --> 02:58:12.560]   um, in talking about the people who've been developing AI inspired by his ideas, he said,
[02:58:12.560 --> 02:58:18.640]   like, you know, these are idiot disaster monkeys who have, you know, want to be the ones to pluck
[02:58:18.640 --> 02:58:24.640]   the deadly fruit. Anyways, how do you think about, obviously the work you're doing involves
[02:58:24.640 --> 02:58:28.160]   many into hazard, I'm sure. Uh, how do you think about when and where to spread them?
[02:58:28.160 --> 02:58:34.960]   Yeah. And so I think, I think there are real concerns of that type. I think it's true that AI
[02:58:34.960 --> 02:58:40.400]   progress has probably been accelerated, uh, by efforts like Bostrom's publication of super
[02:58:40.400 --> 02:58:47.120]   intelligence to try and get the world, uh, to sort of pay attention to these problems in advance and
[02:58:47.120 --> 02:58:55.120]   prepare. Um, I think I disagree with Eliezer that like that has been on the whole bad. I think the
[02:58:55.120 --> 02:59:02.400]   situation is in some important ways, looking a lot better, uh, than ways, alternative ways it
[02:59:02.400 --> 02:59:09.680]   could have been. Um, I think it's important that you have several of the leading AI labs making
[02:59:10.320 --> 02:59:18.560]   not only significant lip service, but also some investments, um, in things like technical
[02:59:18.560 --> 02:59:25.760]   alignment research, providing significant public support, uh, for the idea that these are,
[02:59:25.760 --> 02:59:32.720]   that the risks of truly apocalyptic disasters are real. Um, I think the fact that the leaders
[02:59:32.720 --> 02:59:40.080]   of open AI, DeepMind and Anthropic, uh, all make that point. Uh, they were recently all invited
[02:59:40.080 --> 02:59:45.680]   along with other tech CEOs, uh, to the white house, uh, to discuss AI regulation. Um, and I
[02:59:45.680 --> 02:59:51.120]   think you could, you could tell an alternative story where, you know, a larger share of the
[02:59:51.120 --> 02:59:58.160]   leading companies, uh, in AI are led by people who take a completely dismissive denialist view.
[02:59:58.160 --> 03:00:04.240]   And you see some companies that do have a stance more like that today. Um, yeah. And so a world
[03:00:04.240 --> 03:00:10.480]   where several of the leading companies are making meaningful efforts, uh, and can do a lot to
[03:00:10.480 --> 03:00:16.000]   criticize, could they be doing more and better and, uh, what have been the negative effects of
[03:00:16.000 --> 03:00:22.320]   some of the things they've done. Uh, but compared to a world where even though AI would be, uh,
[03:00:22.320 --> 03:00:29.120]   reaching where it's going, um, a few years later, um, those seem like significant benefits.
[03:00:29.120 --> 03:00:34.640]   And if you didn't have this kind of public communication, you would have had fewer people
[03:00:34.640 --> 03:00:40.800]   going into things like AI policy, alignment research by this point. And it would be harder
[03:00:40.800 --> 03:00:45.200]   to mobilize these resources to try and address the problem when AI would eventually be developed,
[03:00:45.200 --> 03:00:50.640]   not that much later proportionately. Uh, and so, yeah, I don't know that the
[03:00:50.640 --> 03:00:55.920]   attempting to have public discussion understanding has been a disaster. I have been reluctant
[03:00:55.920 --> 03:01:01.920]   in the past, uh, to discuss some of the aspects of intelligence explosion,
[03:01:01.920 --> 03:01:08.080]   things like the concrete details of AI takeover, uh, before, uh, because of concern,
[03:01:09.280 --> 03:01:15.520]   uh, that about this, this sort of problem where people who only see the international relations
[03:01:15.520 --> 03:01:22.400]   aspects and zero sum and negative sum competition, uh, and not enough attention to the, uh, the
[03:01:22.400 --> 03:01:28.080]   mutual destruction and sort of senseless deadweight loss, uh, from that kind of conflict.
[03:01:28.080 --> 03:01:37.200]   At this point, uh, we seem close compared to what I would have thought a decade or so ago, uh, to
[03:01:37.200 --> 03:01:42.160]   these kinds of really advanced AI capabilities. They are pretty central, uh, in policy discussion
[03:01:42.160 --> 03:01:51.200]   and becoming more so. Uh, and so the opportunity to delay understanding and whatnot, um, there's
[03:01:51.200 --> 03:01:56.560]   a question of for what, and I think there was, there were gains of like building the AI alignment
[03:01:56.560 --> 03:02:03.760]   field, building, uh, various kinds of support and understanding, uh, for action. Those had,
[03:02:03.760 --> 03:02:08.160]   had real value and some additional delay could have given more time for that.
[03:02:08.160 --> 03:02:14.720]   Uh, but from where we are at some point, I think it's absolutely essential, uh, that governments
[03:02:14.720 --> 03:02:21.680]   get together at least to restrict disastrous, reckless compromising of some of the safety and
[03:02:21.680 --> 03:02:29.200]   alignment issues as we go into the intelligence explosion. Um, and so moving the locus of the
[03:02:29.200 --> 03:02:37.280]   sort of collective action problem from numerous profit oriented companies acting, acting against,
[03:02:37.280 --> 03:02:45.200]   uh, one another's, uh, interests by compromising safety to some governments and large international
[03:02:45.200 --> 03:02:51.120]   coalitions of governments who can set common rules and common safety standards puts us into a much
[03:02:51.120 --> 03:02:58.320]   better situation. Uh, and that requires a broader understanding of the strategic situation, the
[03:02:58.320 --> 03:03:04.240]   position they'll be in. If we try and, uh, and remain quiet about the problem they're actually
[03:03:04.240 --> 03:03:09.760]   going to be facing, I think it can result in a lot of confusion. So for example, the, the potential
[03:03:09.760 --> 03:03:16.560]   military applications of advanced, uh, AI are going to be one of the factors that is pulling
[03:03:16.560 --> 03:03:22.240]   political leaders to do the thing that will result in their own destruction, uh, and the overthrow of
[03:03:22.240 --> 03:03:29.600]   their governments. If we, if we characterize it as, Oh, uh, things will just be a matter of,
[03:03:29.600 --> 03:03:35.840]   you know, you lose, you lose, uh, you lose chat bots, uh, and some minor things that no one cares
[03:03:35.840 --> 03:03:44.080]   about. And in exchange you avoid any risk of the world ending catastrophe. Um, I think that picture
[03:03:44.080 --> 03:03:50.080]   leads to a misunderstanding and it will make people think that you need less in the way of
[03:03:50.080 --> 03:03:56.160]   preparation, things like alignment. So you can actually navigate the thing. Verify, verifiability
[03:03:56.160 --> 03:04:04.800]   for international agreements, um, or things to have enough breathing room to have caution and
[03:04:04.800 --> 03:04:09.680]   slow down. Not necessarily right now. I mean, although that, that could be valuable, but when
[03:04:09.680 --> 03:04:15.280]   it's so important, um, when you're have AI that is approaching the ability to really automate AI
[03:04:15.280 --> 03:04:21.040]   research and things would otherwise be proceeding absurdly fast, far faster than we can handle and
[03:04:21.040 --> 03:04:28.960]   far faster than we should, should want. Uh, and so, yeah, at this point I'm moving towards the
[03:04:28.960 --> 03:04:33.360]   share my model of the world, um, try and get people to understand and do the right thing.
[03:04:33.360 --> 03:04:40.000]   And, you know, there's some, there's, there's some evidence of progress on that front. The,
[03:04:41.120 --> 03:04:47.680]   you know, things like the, uh, the statements and, uh, movements by Jeff Hinton are inspiring.
[03:04:47.680 --> 03:04:56.480]   Some of the, uh, engagement by political figures, um, you know, is reason for optimism relative to
[03:04:56.480 --> 03:05:02.560]   worse alternatives that could have been. Uh, and yes, the contrary view is present. The, you know,
[03:05:02.560 --> 03:05:08.880]   it's all about, uh, geopolitical competition, never, um, hold back a technological advance.
[03:05:08.880 --> 03:05:15.440]   And in general, I, I love many technological advances, uh, that people, I think are, you know,
[03:05:15.440 --> 03:05:21.680]   unreasonably down on nuclear power, uh, genetically modified crops, et cetera, um,
[03:05:21.680 --> 03:05:29.920]   bioweapons and AGI capable of destroying human civilization are really my, my two exceptions.
[03:05:29.920 --> 03:05:36.000]   And yeah, we've got to, we've got to deal with these issues and the path that I see to handling
[03:05:36.000 --> 03:05:43.920]   them successfully involve, uh, key policymakers and to some extent, yeah. And the expert communities,
[03:05:43.920 --> 03:05:48.960]   uh, and the public and electorate grokking the situation that they're in and responding
[03:05:48.960 --> 03:05:53.520]   appropriately. Well, um, it's a true honor that one of the places you've decided to,
[03:05:53.520 --> 03:05:57.760]   um, explore this model is on, on, on the lunar society podcast. And, um,
[03:05:57.760 --> 03:06:01.360]   the listeners might not appreciate, cause this episode might be split up into different parts.
[03:06:01.360 --> 03:06:05.840]   The listeners might not appreciate how much of a, how much stamina you've displayed here,
[03:06:05.840 --> 03:06:08.880]   but I think we've been going for what, eight, nine hours or something straight.
[03:06:08.880 --> 03:06:13.440]   So it's been incredibly interesting other than Google scholar, tapping in Carl Shulman,
[03:06:13.440 --> 03:06:16.240]   where else can people find your work? You have your blog. Can you,
[03:06:16.240 --> 03:06:23.120]   yeah, I have a blog reflective disequilibrium and a, uh, new site, uh, in the works. And I have,
[03:06:23.120 --> 03:06:28.000]   uh, I have an older one, which you can also find just Googling reflective disequilibrium.
[03:06:28.000 --> 03:06:33.360]   Okay. Excellent. Excellent. All right, Carl, this is a, this is a true pleasure. It's safe to say
[03:06:33.360 --> 03:06:39.040]   the most interesting, um, episode I've done so far. So yeah. Thanks. Thank you for having me.
[03:06:39.040 --> 03:06:53.120]   Hey everybody. I hope you enjoyed that episode as always. The most helpful thing you can do
[03:06:53.120 --> 03:06:57.440]   is to share the podcast, send it to people you think might enjoy it, put it in Twitter,
[03:06:57.440 --> 03:07:02.960]   your group chats, et cetera, just splits the world. Appreciate your listening. I'll see you next time.
[03:07:02.960 --> 03:07:13.920]   Cheers.

