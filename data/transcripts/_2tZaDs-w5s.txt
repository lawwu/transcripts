
[00:00:00.000 --> 00:00:17.100]   All right let's get started. Hi everyone, thanks for coming. I'm Pablo I work in the
[00:00:17.100 --> 00:00:23.240]   Azure AI team at Microsoft and in this session we'll talk about RAG at scale
[00:00:23.240 --> 00:00:29.720]   and in particular I'll focus on the retrieval portion of the RAG pattern. So
[00:00:29.720 --> 00:00:33.320]   plan for the session is we'll do a quick recap just make sure that we all have
[00:00:33.320 --> 00:00:37.820]   this we use the same terms for the same things and then I'll go through kind of
[00:00:37.820 --> 00:00:42.400]   different dimensions of scale and I'll comment a little bit on how do we tackle
[00:00:42.400 --> 00:00:46.160]   this in the context of AI search what we've learned from doing this and what
[00:00:46.160 --> 00:00:53.840]   are we doing to make it easier to scale these applications. So kind of as a by
[00:00:53.840 --> 00:00:59.440]   means of a quick recap when it comes to bringing your own domain knowledge to
[00:00:59.440 --> 00:01:03.040]   to work together with with language models you effectively you have three
[00:01:03.040 --> 00:01:08.800]   options either you you can do prompt engineering and you know while it's easy
[00:01:08.800 --> 00:01:13.120]   to dismiss you can go a long way with prompt engineering along especially these
[00:01:13.120 --> 00:01:18.040]   days you know models have longer longer context and whatnot. If that's not enough
[00:01:18.040 --> 00:01:22.660]   sometimes the challenge is more along the lines of I want to teach particular
[00:01:22.660 --> 00:01:27.040]   patterns of or I want the model to learn jargon of some in some particular vertical
[00:01:27.040 --> 00:01:30.640]   domain and things like that and for that fine-tuning is often a good option
[00:01:30.640 --> 00:01:36.400]   however in many many of these cases what I want is to have the model work over a set
[00:01:36.400 --> 00:01:40.320]   of data that the model didn't see during training. This could be my application data
[00:01:40.320 --> 00:01:45.360]   my company data company information about my users or anything like that and for that
[00:01:45.360 --> 00:01:52.720]   the prevailing pattern right now is to use retrieval augmented generation. Effectively what that means
[00:01:52.720 --> 00:01:58.080]   is if what if you want the model to know facts then what you do is you kind of separate the reasoning
[00:01:58.080 --> 00:02:02.880]   piece of the picture from the knowledge piece of the picture you lean on the language model for the
[00:02:02.880 --> 00:02:08.800]   reasoning capabilities and you use an external knowledge base to model what you know about a particular
[00:02:08.800 --> 00:02:13.440]   domain the data that you have and you and the way you glue them together is like in principle
[00:02:13.440 --> 00:02:17.600]   mechanically very simple of course then it gets complicated because you know life is never that
[00:02:17.600 --> 00:02:24.080]   easy but in principle you have some orchestration component that you know takes the task at hand
[00:02:24.080 --> 00:02:28.320]   let's say you have a chat application and then the user takes a turn and asks the next question
[00:02:28.320 --> 00:02:35.040]   the orchestration component hits some knowledge base seeking for pieces of information that could
[00:02:35.040 --> 00:02:40.160]   be used to produce an answer to this question and then grabs a bunch of candidates and then you go to
[00:02:40.160 --> 00:02:45.360]   the language model and then give it a bunch of instructions your candidates and ask the model to
[00:02:45.360 --> 00:02:51.600]   produce like an answer to the to the user question that's kind of the essence of the pattern we all know
[00:02:51.600 --> 00:02:56.160]   that in practice it usually takes multiple goals and there's a lot of tuning in the middle and whatnot
[00:02:56.160 --> 00:03:03.440]   but the fundamentals boil down to that so just by way of context like how many of you are doing are
[00:03:03.440 --> 00:03:10.480]   creating and working on right applications today everyone okay excellent um so with that backdrop
[00:03:10.480 --> 00:03:16.160]   what i wanted to do is talk about what are the pressure points when it when you scale these applications
[00:03:16.160 --> 00:03:21.520]   what one thing that has been fascinating to see is you know we had we had the opportunity to be
[00:03:21.520 --> 00:03:26.880]   involved in this space uh from very early like azure open ai has been there from you know the early days of
[00:03:26.880 --> 00:03:33.840]   scaled language models and one thing we saw was all of i'd say last year 2023 everybody built a prototype
[00:03:33.840 --> 00:03:39.760]   of something to kind of learn and figure out what could be done with this technology um and uh the
[00:03:39.760 --> 00:03:45.040]   interesting shift to this year to 2024 it's been these applications are going to production and when you
[00:03:45.040 --> 00:03:49.760]   go to production you go from oh this demo is really cool to all these users are you know are using it at the
[00:03:49.760 --> 00:03:57.040]   the same time and they want more they want more data they want more faster answers and whatnot um so
[00:03:57.040 --> 00:04:03.280]   the result of that is that you know when before we could focus only on figuring out the interaction
[00:04:03.280 --> 00:04:07.760]   model and the applicability of this technology now you know elements of scale also also play a role
[00:04:07.760 --> 00:04:14.720]   and scale can takes multiple kind of flavors like you these things things tend to scale in volume because
[00:04:14.720 --> 00:04:19.040]   one of the things that happens is when your application works well users come back or the leadership of the
[00:04:19.040 --> 00:04:23.600]   organization come back comes back and says let's put all the data there and then you have to deal with
[00:04:23.600 --> 00:04:29.840]   it um uh also the rate of change of the data kind of increases uh and the query load increases as
[00:04:29.840 --> 00:04:35.120]   well because you know more people are using the stuff um also workflows tend to get more complicated
[00:04:35.120 --> 00:04:40.240]   at first you go like oh like how complicated it can be i'll take the question search the thing and then
[00:04:40.240 --> 00:04:45.200]   send it to the model well it turns out that that sometimes it works but often it doesn't so you end up
[00:04:45.200 --> 00:04:49.680]   doing this multi-step workflows that hit the retrieval system and the language model multiple times
[00:04:49.680 --> 00:04:56.480]   um and that taxes all the systems and they all have to scale also in in the kind of in the spirit of
[00:04:56.480 --> 00:05:01.840]   now let's put all the data there now you have to deal with more uh more data types different kinds
[00:05:01.840 --> 00:05:08.800]   different data sources and whatnot um so um let's cover each each of these dimensions of scale um in detail
[00:05:09.520 --> 00:05:15.280]   and what i'll do is i'll cover it in the context of azure ai search that's where i work mostly uh
[00:05:15.280 --> 00:05:20.480]   and uh because it's you know the way we think about this in azure is we want to produce a system that
[00:05:20.480 --> 00:05:26.080]   is that encompasses entire retrieval problem uh it's of course it has this kind of vector database
[00:05:26.080 --> 00:05:32.560]   capabilities the vector based retrieval emerged as a very uh useful solution in many contexts so
[00:05:32.560 --> 00:05:38.640]   uh we have full support for that but we also brought into it like uh years and years of microsoft
[00:05:38.640 --> 00:05:44.160]   experience in retrieval systems in the more general sense um and uh we integrate them together so you
[00:05:44.160 --> 00:05:50.480]   don't have to connect a bunch of parts uh to have a proper uh high quality retrieval system it all comes
[00:05:50.480 --> 00:05:55.360]   integrated and of course you know we integrate into the rest of azure so it's easy to pull data in to
[00:05:55.360 --> 00:06:01.040]   connect to the other data sources and whatnot um and you know azure is is a place that is used to build
[00:06:01.040 --> 00:06:05.280]   some of the largest applications out there so all the kind of enterprise readiness comes
[00:06:05.280 --> 00:06:09.600]   pre-built in you know from security to compliance to all these things that you don't want to deal
[00:06:09.600 --> 00:06:12.560]   with directly you want to kind of build on a platform that is already taken care of
[00:06:12.560 --> 00:06:21.360]   so while there are multiple moving parts to retrieval systems what we've seen the last uh 18 months
[00:06:21.360 --> 00:06:28.240]   or so since the emergence of uh kind of right patterns at scale is vector retrieval is an important
[00:06:28.240 --> 00:06:32.880]   part of the solution and you know you you can see why like the interesting part about vector retrieval is
[00:06:32.880 --> 00:06:38.720]   like it's incredibly effective at getting this kind of soft conceptual similarity uh and put it to work
[00:06:38.720 --> 00:06:43.920]   right away so in azure ai search we built a system that has kind of complete a complete feature set when
[00:06:43.920 --> 00:06:51.120]   it comes to uh vector search including uh a fast approximate nearest neighbor search uh and as well as
[00:06:51.120 --> 00:06:56.160]   also exhaustive search sometimes you want precise search either to create baselines uh to know how kind of your
[00:06:56.160 --> 00:07:02.320]   recall performance is looking and things like that um also often applications need to combine uh vector
[00:07:02.320 --> 00:07:07.680]   search with the rest of the queries you need to filter uh filter things you need to uh slice and dice
[00:07:07.680 --> 00:07:11.200]   like you know filter select the columns you want to project and things like that like effectively
[00:07:11.200 --> 00:07:16.080]   treat it like a database that also does retrieval um and we also see multiple scenarios where the
[00:07:16.080 --> 00:07:20.560]   documents have multiple queries maybe for different parts of the content or for different data types that use
[00:07:20.560 --> 00:07:26.080]   different embeddings um and uh and sometimes the queries need multiple vectors so we try to make
[00:07:26.080 --> 00:07:31.840]   it as this all these specific needs come up as you're building an application you'll find answers to
[00:07:31.840 --> 00:07:34.640]   all of them directly in azure search let me show you this in action
[00:07:37.040 --> 00:07:45.520]   uh so let me start with a very simple example so what i'll do is uh i'm just here i have a small uh
[00:07:45.520 --> 00:07:52.400]   jupyter notebook and what i'll do is i connect to um i can set up a default credential point at my azure
[00:07:52.400 --> 00:07:57.360]   search service and i'll create an index from scratch so this is all like all it takes to to create an
[00:07:57.360 --> 00:08:02.880]   index once you have a service provisioned so in this case i create a few fields i'll create a categorical
[00:08:02.880 --> 00:08:07.600]   field like this serves as metadata that you want sometimes to attach to to be attached to to this
[00:08:07.600 --> 00:08:12.800]   i create a text field sometimes you want to mix text and vectors um and i'll talk a little bit more
[00:08:12.800 --> 00:08:17.680]   about this later and i'll create a vector field in in this case this is a toy example so dimension is
[00:08:17.680 --> 00:08:21.760]   three of course that's not very useful you know in practice in practice this is kind of in the hundreds
[00:08:21.760 --> 00:08:27.520]   or maybe thousands of dimensions i'll also say what strategy you want the system to use for vector
[00:08:27.520 --> 00:08:34.480]   search in this case i'm using hnsw which is a well-known um uh graph-based algorithm for um for indexing
[00:08:34.480 --> 00:08:41.040]   vectors so when i run this and i hope uh like the network was a little wonky so oh yeah perfect um so
[00:08:41.040 --> 00:08:45.280]   now i have an index i'm going to get a kind of a client to it and then i'm going to index data and
[00:08:45.280 --> 00:08:50.560]   you can see here this is a very simple case these are my vectors um this is my full text and these are
[00:08:50.560 --> 00:08:55.520]   my cut these are my categorical data bits so you can let us do all the ingestion for you or you can
[00:08:55.520 --> 00:09:00.880]   push data into the index in this case i'm pushing data explicitly into the index and once you have
[00:09:00.880 --> 00:09:05.840]   data uh if you look at here like we have some vectors and we have some categories so you can
[00:09:05.840 --> 00:09:10.480]   first you can just search and because it's in this case i'm indexing vectors when you search you search
[00:09:10.480 --> 00:09:15.920]   with a vector uh so i can i can search for that and say these are the two closest the reference vector
[00:09:15.920 --> 00:09:22.880]   that i gave and i can do a um a few a few things kind of incrementally like for example if i want to
[00:09:22.880 --> 00:09:28.480]   combine text and keyword search i can also add search text right here uh i'm gonna go hello
[00:09:28.480 --> 00:09:34.880]   and now i'm searching for both uh the the text and the vectors and then we fuse the result and rank
[00:09:34.880 --> 00:09:40.160]   appropriately um and and often in applications you also need to filter stuff you can see here
[00:09:40.160 --> 00:09:46.240]   i have a's and b's in categories so i can i can like write filters uh for example here i'll say category
[00:09:46.240 --> 00:09:53.920]   equals a uh and then you know i only get the a's uh and this is a trivial example but of course you
[00:09:53.920 --> 00:09:59.280]   can do full filter expressions and ans and ours and and uh ranges and and whatnot and the filters are
[00:09:59.280 --> 00:10:04.000]   fast so so even if you have uh hundreds of millions of documents these are not a problem
[00:10:05.600 --> 00:10:11.600]   so in that one if you have a k-nearest neighbors k-nearest neighbors equals two how come they had
[00:10:11.600 --> 00:10:16.880]   three results oh because uh a great question because so what i what i was telling this the the system
[00:10:16.880 --> 00:10:23.200]   here is from the vectors retrieve two candidates but then i was like i also told it go to the keyword
[00:10:23.200 --> 00:10:28.240]   side and retrieve a bunch of candidates from there and then fuse them uh so only two of them were from
[00:10:28.240 --> 00:10:35.440]   vectors but diffuse diffusion of the two uh selected three i actually like if i uh i can make this a
[00:10:35.440 --> 00:10:40.880]   larger number and that sometimes is useful um to get more candidates and then still i can say
[00:10:40.880 --> 00:10:47.520]   as a result of that get the keyword ones too and then rank the top end so basically separate how many
[00:10:47.520 --> 00:10:49.600]   candidates you want from how many you want to return
[00:10:53.840 --> 00:10:58.960]   so so those are the basics of a vector engine but there are a few uh key elements to consider when
[00:10:58.960 --> 00:11:03.520]   you're actually building an application in production the probably the most the most salient one is
[00:11:03.520 --> 00:11:08.640]   quality like in the end your application works well for your users when they ask a question and they
[00:11:08.640 --> 00:11:13.520]   get the answer they're looking for and that is highly highly influenced by whether your retrieval
[00:11:13.520 --> 00:11:18.720]   system actually found the uh the bits of information that you wanted uh that you wanted to produce that
[00:11:18.720 --> 00:11:25.120]   answer um in in ai search the way we do this is we do um what kind of most uh kind of more sophisticated
[00:11:25.120 --> 00:11:30.960]   search engines do that where you use a two-stage retrieval system first stage is recall oriented and
[00:11:30.960 --> 00:11:36.640]   uses vectors and keywords and kind of all these recall oriented tricks to produce as many candidates as we can
[00:11:36.640 --> 00:11:43.040]   find and then the second stage re-ranks those candidates using like a like a larger model that you may be
[00:11:43.040 --> 00:11:48.560]   let's say you have 100 million vectors in your database uh you you want to use something fast to go
[00:11:48.560 --> 00:11:53.280]   from 100 million to a small set but then once you have a small set you can afford to learn run a
[00:11:53.280 --> 00:11:59.200]   larger more sophisticated language um sorry ranking model uh to create better quality ranking so that's
[00:11:59.200 --> 00:12:05.040]   what we're doing in the l2 stage so when you turn this on you effectively get better results and uh what
[00:12:05.040 --> 00:12:09.280]   you can see here there is a link at the bottom happy to share it later uh with more details on the
[00:12:09.280 --> 00:12:14.880]   numbers but you can see that um so from the left this is what you get when you just do keyword search using
[00:12:14.880 --> 00:12:21.440]   bm25 which is well known scoring approach second one is only vectors using ada open ai ada vectors third
[00:12:21.440 --> 00:12:27.280]   one is using fusion combined vectors and keywords and the fourth one is using fusion plus uh re-ranking
[00:12:27.280 --> 00:12:33.360]   um re-ranking step where across the board we see better results just out of the box when uh when re-ranking
[00:12:33.360 --> 00:12:41.360]   is enabled yes so the re-ranking is the semantic re-ranking is that is that another cosine similarity
[00:12:41.360 --> 00:12:46.480]   style thing or is it actually great great question the question is is it a cosine similarity thing no
[00:12:46.480 --> 00:12:52.240]   so the thing bind coders are useful when uh because you can do you can encode all your documents as
[00:12:52.240 --> 00:12:57.200]   vectors and then the query as a vector um and then you can evaluate them fast because you are only comparing
[00:12:57.200 --> 00:13:03.200]   similarity but that means that at no point the model sees the the document and the query at the same time so the
[00:13:03.200 --> 00:13:09.040]   re-ranker is uh these type of rankers are often called cross encoders and uh these are transformer
[00:13:09.040 --> 00:13:14.480]   models that you feed them the document and the query and you ask them to predict a label of the
[00:13:14.480 --> 00:13:19.360]   correspondence of the query to the document so they are much better positioned to produce a high quality
[00:13:19.360 --> 00:13:25.680]   result uh but they are they're they're at inference time or rather at query time you're running an inference
[00:13:25.680 --> 00:13:31.120]   step so um you can do it only on a smaller set you couldn't do it on the entire data set it wouldn't be
[00:13:31.120 --> 00:13:33.920]   practical uh at least not for interactive performance applications
[00:13:33.920 --> 00:13:41.440]   speed this is like 100 milliseconds give or take for a model like this depends on the cross encoder that
[00:13:41.440 --> 00:13:46.240]   you that you use ours our trade-off between you know make it fast enough but make it very high quality
[00:13:46.240 --> 00:13:52.480]   we landed on 100 milliseconds ballpark um uh and we found that that works well in terms of interactive
[00:13:52.480 --> 00:13:56.000]   performance because the majority of the latency ends up hidden on the llm call
[00:13:56.560 --> 00:14:04.080]   uh and you still get very high quality results um the other thing i won't drill into this but uh often
[00:14:04.080 --> 00:14:10.320]   uh once again often the other dimension of getting quality out of the system is to narrow the data
[00:14:10.320 --> 00:14:16.080]   set if you know discrete metadata elements that can help you narrow the data set that's the most effective
[00:14:16.080 --> 00:14:22.000]   way uh then do all the ranking tricks on top of the resulting set but uh first uh scoping is uh it's
[00:14:22.000 --> 00:14:34.080]   a very effective way to uh to get quality up yes question for the keyword part of keyword search oh so
[00:14:34.080 --> 00:14:37.440]   if you look at um let me show you this example so if you look at what i did here
[00:14:38.400 --> 00:14:44.800]   is uh you give us keywords you give us text and then you give us a vector as well or you just give
[00:14:44.800 --> 00:14:49.520]   us the text and we'll turn it into a vector too uh so we need the original text the vector alone is not
[00:14:49.520 --> 00:14:54.160]   enough because we can't go back to a keyword no so i mean if you have the right application let's say
[00:14:54.160 --> 00:14:58.640]   you have a chapter yeah how can you know ahead of time what kind of keyword can be added up to the
[00:14:58.640 --> 00:15:07.680]   uh oh so um usually in the rag app the question is what of the conversations so far do you want to
[00:15:07.680 --> 00:15:13.200]   use for search is that is that the question no i understand but my question is so if you want to use
[00:15:13.200 --> 00:15:22.880]   because the new measures you showed that if you search yes how do you like for victim service
[00:15:22.880 --> 00:15:49.200]   yeah i maybe we should talk after the talk because i'm not sure i understand the question like usually
[00:15:49.200 --> 00:15:55.440]   like you in the index you have the set the text and the vectors uh and then from the user you extract
[00:15:55.440 --> 00:16:00.400]   a candidate search for for that this is before you send it to llm uh but maybe we should chat right
[00:16:00.400 --> 00:16:11.520]   after we talk on the details um so let me go back here so the other dimension of scaling is um is uh just
[00:16:11.520 --> 00:16:18.480]   show how many vectors and how much content you can fit in one of your indexes uh and for that in ai search
[00:16:18.480 --> 00:16:22.560]   like one thing we learned last year uh in the beginning of this year is that it went from
[00:16:22.560 --> 00:16:28.880]   everybody had tiny indexes to everybody put all their data in these systems uh so we we uh accommodated
[00:16:28.880 --> 00:16:33.440]   for that by significantly increasing the limits like for most of the search services uh you will get
[00:16:33.440 --> 00:16:38.800]   anywhere 10 between 10 and 12 times the vector density on on the same skews and we didn't change prices
[00:16:38.800 --> 00:16:44.240]   or anything just so you can build larger applications on the same setup with these new limits you can build
[00:16:44.240 --> 00:16:49.600]   multi-billion vector apps by just provisioning a service and uploading the data and surprisingly
[00:16:49.600 --> 00:16:54.400]   straightforward like a year ago the billion data set vectors was the thing there was a curiosity used
[00:16:54.400 --> 00:16:58.640]   for uh for benchmarking and now you can just create an index and upload them which is very impressive to
[00:16:58.640 --> 00:17:04.240]   to to see i'm not going to drain the slide i put it here for reference uh these are kind of all the new
[00:17:04.240 --> 00:17:11.600]   limits that uh that we have are significantly higher than the ones we had before um one of the things
[00:17:11.600 --> 00:17:16.880]   that has been exciting for us to watch uh to watch grow is uh you know among our customers one of them
[00:17:16.880 --> 00:17:22.080]   is open ai themselves uh where they have a lot of rag workloads like when you have files in chat gpt or
[00:17:22.080 --> 00:17:27.440]   when you use the assistance api all of those you can create these vector stores inside their system and
[00:17:27.440 --> 00:17:32.960]   uh all of those are backed by ai search uh and when we increase these limits um one of the things they
[00:17:32.960 --> 00:17:39.280]   did is they increased the limits they give to their users by 500 times um so it's been impressive to
[00:17:39.280 --> 00:17:46.160]   see how fast they grow and it's been fun to kind of see uh kind of first um uh from up close how a system
[00:17:46.160 --> 00:17:50.240]   can scale at like this big and uh and still be fast and responsive and whatnot
[00:17:52.240 --> 00:17:58.720]   um so finally the the other thing you can do is sometimes the limits higher limits are enough
[00:17:58.720 --> 00:18:02.400]   sometimes you want to push even more data into it so the other thing we've been working on is
[00:18:02.400 --> 00:18:07.920]   quantization where we you can use narrower types like instead of using full floats you can use ints
[00:18:07.920 --> 00:18:13.600]   uh like int eights uh and they just simply use less space at the trade-off for a little bit of quality
[00:18:13.600 --> 00:18:19.440]   and interestingly you can even do single bit quantization and i confess that when people said hey
[00:18:19.440 --> 00:18:24.480]   like we're gonna do metrics for single bit i felt people were wasting their time uh but it actually
[00:18:24.480 --> 00:18:31.120]   works it works surprisingly well our evaluations show that they are still for some models in the mid uh
[00:18:31.120 --> 00:18:36.080]   low to mid 90 percent of the original performance and other companies have seen the same thing like
[00:18:36.080 --> 00:18:41.120]   for example this is an evaluation from cohere but separate company um and they also see like about
[00:18:41.120 --> 00:18:47.600]   almost 95 percent of the original precision is preserved when uh when using big encoding and you can get the
[00:18:47.600 --> 00:18:53.040]   precision remaining precision back by re-ranking uh when you're done so surprising that it works but
[00:18:53.040 --> 00:18:59.440]   you go for float 32 to one bit that's 30 or 2x the vector density um and it's faster because you just
[00:18:59.440 --> 00:19:04.640]   do humming distance much much faster than computing on a small number of a smaller number of bits instead of
[00:19:04.640 --> 00:19:10.400]   cosine similarity or something like that on a wider wider set so because of this we now support all these
[00:19:10.400 --> 00:19:18.480]   types in ai search as well um i'm gonna skip this slide in the interest of time um so and uh you can
[00:19:18.480 --> 00:19:24.240]   do quantization yourself or you can just enable it and we will do quantization uh for you and uh if we do
[00:19:24.240 --> 00:19:29.920]   quantization for you we will also store the original uh precision data there which means we can do
[00:19:29.920 --> 00:19:36.400]   over sampling where we query at the quantized kind of compressed uh version of the vectors but we have the full
[00:19:36.400 --> 00:19:43.120]   precision stored stashed on the side uh so later we can re-rank at full precision uh and so you can
[00:19:43.120 --> 00:19:50.320]   effectively choose between you want a highly compressed uh index that is a little lower quality but but but
[00:19:50.320 --> 00:19:55.680]   but larger or it's a little slower but larger uh or you just don't compress it and then you get the quality
[00:19:55.680 --> 00:20:00.800]   up so effectively you can choose any of the three uh and it's effectively up to you uh what you want to
[00:20:00.800 --> 00:20:07.600]   prioritize but we give you control for all three dimensions and then the last thing i wanted to
[00:20:07.600 --> 00:20:13.120]   touch on is that the other challenge is you have to keep adding data sources that you bring into these
[00:20:13.120 --> 00:20:19.200]   rag systems um and each of them you connect to them differently you enumerate changes differently
[00:20:19.200 --> 00:20:25.280]   um and that's that's just not where you want to spend your time um so we have this ingestion system
[00:20:25.280 --> 00:20:30.400]   that includes integrated vectorization as part of ai search where if the data is in azure whether it's
[00:20:30.400 --> 00:20:35.120]   block storage one lake or customs to be we will connect deal with all the security and all of
[00:20:35.120 --> 00:20:40.320]   that we will automatically track changes so it's not a one-shot thing but as as the data changes we'll
[00:20:40.320 --> 00:20:45.760]   pick up only the changes and we'll only in process the changes so the cost is also incremental you don't
[00:20:45.760 --> 00:20:51.280]   pay as you update the stuff for the entire set and then we'll deal with all file formats you know pdfs
[00:20:51.280 --> 00:20:57.680]   office documents images and unpack the nested formats and whatnot do chunking do vectorization and land
[00:20:57.680 --> 00:21:02.160]   land it on an index all in one go and this is an industrial strength pipeline that you set up once
[00:21:02.160 --> 00:21:08.080]   and then it continuously runs uh after that as your data changes it reflects the changes so you can focus
[00:21:08.080 --> 00:21:12.640]   on the right stack you know the workflow how you query the system but you don't have to think about
[00:21:12.640 --> 00:21:17.360]   how the data makes it there if the data is anywhere in azure will index it and create like an index that
[00:21:17.360 --> 00:21:24.960]   follows the original data automatically all right and and with that i know i raced through this content
[00:21:24.960 --> 00:21:29.760]   in these 20 minutes i'll be hanging out outside if anybody wants to chat or have questions um and i would
[00:21:29.760 --> 00:21:36.320]   encourage you to go try ai search today here's a link to the starting point um uh and azure subscriptions
[00:21:36.320 --> 00:21:41.280]   include include a free instance of ai search so you can even give it a shot in a minute without uh without
[00:21:41.280 --> 00:21:52.800]   having to pay for any of the stuff with that thank you

