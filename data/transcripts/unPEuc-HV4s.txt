
[00:00:00.000 --> 00:00:02.600]   (gentle music)
[00:00:02.600 --> 00:00:04.520]   - This is what I mean when I say the complexity
[00:00:04.520 --> 00:00:05.560]   for a machine learning team
[00:00:05.560 --> 00:00:07.360]   is actually exponentially increasing
[00:00:07.360 --> 00:00:11.040]   is you have to look at these other machine driven ways
[00:00:11.040 --> 00:00:13.360]   to increase the quality of your data
[00:00:13.360 --> 00:00:15.200]   and augment your data sets.
[00:00:15.200 --> 00:00:17.080]   - You're listening to Gradient Dissent,
[00:00:17.080 --> 00:00:19.560]   a show about machine learning in the real world.
[00:00:19.560 --> 00:00:21.560]   And I'm your host, Lucas Biewald.
[00:00:21.560 --> 00:00:26.900]   Jahan is VP of AI Platform and Data Services
[00:00:26.900 --> 00:00:28.600]   at Motorola Solutions,
[00:00:28.600 --> 00:00:31.480]   where he's responsible for a vast number
[00:00:31.480 --> 00:00:33.040]   of machine learning models
[00:00:33.040 --> 00:00:35.160]   in lots of different applications
[00:00:35.160 --> 00:00:36.860]   running live in production.
[00:00:36.860 --> 00:00:39.520]   This is a very practical, useful conversation
[00:00:39.520 --> 00:00:40.720]   and I hope you enjoy it.
[00:00:40.720 --> 00:00:44.280]   You know, I was thinking that probably the place
[00:00:44.280 --> 00:00:48.400]   to start here is actually, you know, what Motorola does.
[00:00:48.400 --> 00:00:49.940]   I feel like Motorola has this like brand
[00:00:49.940 --> 00:00:51.240]   for people my age. - Oh, definitely.
[00:00:51.240 --> 00:00:53.080]   - Making phones. - I was just telling,
[00:00:53.080 --> 00:00:55.600]   I was telling Kelly that actually even strange,
[00:00:55.600 --> 00:00:58.040]   I mean, I started my career actually out of,
[00:00:58.040 --> 00:00:58.920]   when I finished my PhD,
[00:00:58.920 --> 00:01:01.480]   I started actually working at Motorola before,
[00:01:01.480 --> 00:01:02.960]   right around the time the iPhone came out
[00:01:02.960 --> 00:01:05.440]   and I actually worked on mobile devices.
[00:01:05.440 --> 00:01:07.900]   And then after a stint at other companies came back.
[00:01:07.900 --> 00:01:10.520]   So I think like telling people what,
[00:01:10.520 --> 00:01:13.040]   definitely that brand is stuck in people's heads
[00:01:13.040 --> 00:01:15.200]   when they think of Motorola, they think of those things,
[00:01:15.200 --> 00:01:17.360]   which is not what this company does at all.
[00:01:17.360 --> 00:01:19.120]   So. - Right, right.
[00:01:19.120 --> 00:01:19.960]   So why don't we start there?
[00:01:19.960 --> 00:01:23.600]   What are like the key things that your company does?
[00:01:23.600 --> 00:01:26.720]   - Yeah, so Motorola Solutions essentially
[00:01:26.720 --> 00:01:29.800]   is completely focused on the safety and security
[00:01:29.800 --> 00:01:32.680]   of the community and enterprises.
[00:01:32.680 --> 00:01:36.140]   So essentially there are a couple of different segments
[00:01:36.140 --> 00:01:36.980]   of the business.
[00:01:36.980 --> 00:01:40.080]   One focuses on enterprise security and video security,
[00:01:40.080 --> 00:01:41.660]   physical access control.
[00:01:41.660 --> 00:01:43.580]   And then the other focuses on public safety
[00:01:43.580 --> 00:01:44.960]   for first responders.
[00:01:44.960 --> 00:01:47.640]   So essentially, you know, everything from
[00:01:47.640 --> 00:01:51.840]   when a 911 call comes in to dispatch units
[00:01:51.840 --> 00:01:54.880]   and then resolution of the incident and case closure,
[00:01:54.880 --> 00:01:57.600]   that's Motorola Solutions focus.
[00:01:57.600 --> 00:01:59.400]   I think in the public safety space,
[00:01:59.400 --> 00:02:01.680]   we're most well known for our mission critical
[00:02:01.680 --> 00:02:03.080]   communications infrastructure,
[00:02:03.080 --> 00:02:05.720]   which has been something that first responders
[00:02:05.720 --> 00:02:08.480]   have relied on for decades now,
[00:02:08.480 --> 00:02:10.400]   which is, you know, when times get rough,
[00:02:10.400 --> 00:02:12.200]   when you see firefighters, you know,
[00:02:12.200 --> 00:02:14.060]   charging into burning buildings,
[00:02:14.060 --> 00:02:16.120]   the radio is what they really focus on,
[00:02:16.120 --> 00:02:18.920]   especially back when, you know,
[00:02:18.920 --> 00:02:20.680]   communication coverage through broadband
[00:02:20.680 --> 00:02:22.760]   was much more sparse than it is now.
[00:02:22.760 --> 00:02:24.560]   And it's still a huge challenge in many parts
[00:02:24.560 --> 00:02:26.000]   of the world, not just the United States,
[00:02:26.000 --> 00:02:28.040]   but even overseas in the UK.
[00:02:28.040 --> 00:02:32.320]   So in general, that is Motorola Solutions mission
[00:02:32.320 --> 00:02:35.640]   essentially to, you know, provide safety and security
[00:02:35.640 --> 00:02:37.240]   for those two segments.
[00:02:37.240 --> 00:02:39.800]   Essentially it's the same audience.
[00:02:39.800 --> 00:02:41.220]   It's making the community safer,
[00:02:41.220 --> 00:02:44.260]   but in terms of how the product portfolio is situated,
[00:02:44.260 --> 00:02:47.480]   it's basically those two segments of the business.
[00:02:47.480 --> 00:02:48.320]   - Interesting.
[00:02:48.320 --> 00:02:50.640]   And I guess, so how does AI fit into that?
[00:02:50.640 --> 00:02:51.780]   I can think of lots of different ways,
[00:02:51.780 --> 00:02:54.420]   but like practically, you know, what goes on?
[00:02:54.420 --> 00:02:58.080]   This seems like a really high stakes place
[00:02:58.080 --> 00:03:01.080]   to introduce artificial intelligence.
[00:03:01.080 --> 00:03:02.280]   - It definitely is.
[00:03:02.280 --> 00:03:05.480]   And if I thought, if I think about kind of my journey
[00:03:05.480 --> 00:03:06.400]   coming to the company,
[00:03:06.400 --> 00:03:09.360]   I've really worked in consumer most of my career.
[00:03:09.360 --> 00:03:12.120]   So machine learning, we kind of took it for granted
[00:03:12.120 --> 00:03:15.440]   that it's just a tool that you use, you know,
[00:03:15.440 --> 00:03:17.280]   the applications and services you're building,
[00:03:17.280 --> 00:03:19.840]   you use it to essentially accelerate, automate
[00:03:19.840 --> 00:03:21.320]   and help decision-making.
[00:03:21.320 --> 00:03:23.920]   Here you do the same things, except like you said,
[00:03:23.920 --> 00:03:27.880]   the opportunity cost for some decisions
[00:03:27.880 --> 00:03:29.280]   that may be incorrect,
[00:03:29.280 --> 00:03:31.840]   and also bridging that human understanding
[00:03:31.840 --> 00:03:33.700]   is quite high.
[00:03:33.700 --> 00:03:36.560]   So, I mean, I would say the mission is still the same
[00:03:36.560 --> 00:03:38.080]   for all of us who work in machine learning,
[00:03:38.080 --> 00:03:40.600]   where we wanna kind of maximize human potential
[00:03:40.600 --> 00:03:42.400]   and use it as assistive tool.
[00:03:42.400 --> 00:03:47.800]   I think the reason that this is so important here
[00:03:47.800 --> 00:03:49.920]   is that many of our users
[00:03:49.920 --> 00:03:51.960]   are in very high stress situations.
[00:03:51.960 --> 00:03:55.280]   So when your cognitive bandwidth is limited,
[00:03:55.280 --> 00:03:57.400]   your ability to make decisions as a human
[00:03:57.400 --> 00:03:58.560]   is definitely hampered.
[00:03:58.560 --> 00:04:01.080]   Now, one thing that is kind of,
[00:04:01.080 --> 00:04:02.920]   that flows complimentary to that
[00:04:02.920 --> 00:04:05.040]   is that the amount of data is exploding.
[00:04:05.040 --> 00:04:07.280]   The amount of data that these users have to consider
[00:04:07.280 --> 00:04:08.920]   day in, day out is exploding,
[00:04:08.920 --> 00:04:13.560]   whether it's a 911 call taker or a security guard,
[00:04:13.560 --> 00:04:16.620]   more video, more audio, more unstructured texts,
[00:04:16.620 --> 00:04:19.120]   more structured data, more communication.
[00:04:19.120 --> 00:04:20.240]   So then the question becomes,
[00:04:20.240 --> 00:04:23.160]   how can I use AI to be able to simplify that?
[00:04:23.160 --> 00:04:25.620]   And I think it's not just an AI problem,
[00:04:25.620 --> 00:04:27.360]   it's also a usability problem.
[00:04:27.360 --> 00:04:29.240]   And actually it's funny, this weekend,
[00:04:29.240 --> 00:04:31.800]   I was reading a book by Katie Swindler,
[00:04:31.800 --> 00:04:34.200]   which is on life and death design.
[00:04:34.200 --> 00:04:35.280]   And there's increasingly,
[00:04:35.280 --> 00:04:38.540]   there's a lot of these kind of usability considerations
[00:04:38.540 --> 00:04:41.200]   for designing for people in high stress situations.
[00:04:41.200 --> 00:04:42.840]   And I think once you get past
[00:04:42.840 --> 00:04:46.200]   the kind of the frozen response
[00:04:46.200 --> 00:04:49.740]   where your prefrontal cortex kicks in
[00:04:49.740 --> 00:04:52.280]   and then you're like, okay, now what do I need to do?
[00:04:52.280 --> 00:04:54.680]   I think one of the things that really stuck out for me
[00:04:54.680 --> 00:04:56.180]   was designing for experts,
[00:04:56.180 --> 00:04:58.440]   because in public safety and even in video security,
[00:04:58.440 --> 00:04:59.960]   you have a lot of expert users,
[00:04:59.960 --> 00:05:02.480]   whether there's someone who's been watching video
[00:05:02.480 --> 00:05:03.680]   for years and years,
[00:05:03.680 --> 00:05:06.140]   they know exactly what's happening on every single camera.
[00:05:06.140 --> 00:05:09.200]   They know the playbook that when something goes awry,
[00:05:09.200 --> 00:05:10.440]   what to do.
[00:05:10.440 --> 00:05:15.040]   Expert users typically, when you speed things up for them,
[00:05:15.040 --> 00:05:18.320]   they tend to do better because they automate a lot of the,
[00:05:18.320 --> 00:05:20.840]   I would say the standard stuff,
[00:05:20.840 --> 00:05:23.040]   the stuff that kind of has to happen
[00:05:23.040 --> 00:05:25.400]   ahead of actually using their brain
[00:05:25.400 --> 00:05:26.600]   to actually figure out a problem,
[00:05:26.600 --> 00:05:27.920]   they automate a lot of it.
[00:05:27.920 --> 00:05:29.960]   Whereas if you take a novice user,
[00:05:29.960 --> 00:05:33.000]   and I'll get to why this is important in a second,
[00:05:33.000 --> 00:05:36.000]   for novice users, they do wanna think it through
[00:05:36.000 --> 00:05:38.440]   before they get to anything.
[00:05:38.440 --> 00:05:40.640]   I think for expert users,
[00:05:40.640 --> 00:05:43.640]   that's becoming a luxury that many of these roles
[00:05:43.640 --> 00:05:44.920]   don't have any more staffing,
[00:05:44.920 --> 00:05:45.900]   it's challenging.
[00:05:45.900 --> 00:05:49.360]   I don't know how much the audience knows about,
[00:05:49.360 --> 00:05:50.480]   I would say public safety,
[00:05:50.480 --> 00:05:51.560]   but a lot of those roles,
[00:05:51.560 --> 00:05:52.880]   like when you call 911,
[00:05:52.880 --> 00:05:55.200]   your life is essentially in the hands of
[00:05:55.200 --> 00:05:56.920]   someone who's taking that call,
[00:05:56.920 --> 00:05:58.520]   figuring out what's going on
[00:05:58.520 --> 00:06:01.780]   and bridging that help to you when you need it.
[00:06:01.780 --> 00:06:04.560]   Those expert users are now churning much, much more,
[00:06:04.560 --> 00:06:07.360]   in which case training becomes a huge problem.
[00:06:07.360 --> 00:06:10.160]   So expert users tend to do better
[00:06:10.160 --> 00:06:12.000]   because they've kind of simplified the workflow.
[00:06:12.000 --> 00:06:14.140]   And I think this is where AI can really help
[00:06:14.140 --> 00:06:15.360]   in that process.
[00:06:15.360 --> 00:06:18.000]   And I think for novice users,
[00:06:18.000 --> 00:06:20.200]   AI can bridge some of that gap.
[00:06:20.200 --> 00:06:22.340]   They don't have years of expertise to fall back on
[00:06:22.340 --> 00:06:24.000]   where AI can help bridge some of that
[00:06:24.000 --> 00:06:25.400]   so that they can actually focus
[00:06:25.400 --> 00:06:27.680]   their attention more effectively.
[00:06:27.680 --> 00:06:29.760]   - So could we get a little more specific
[00:06:29.760 --> 00:06:31.480]   about like a single use case
[00:06:31.480 --> 00:06:35.120]   and kind of what your software is doing in that?
[00:06:35.120 --> 00:06:38.880]   - Yeah, so let's take video security, for example.
[00:06:38.880 --> 00:06:41.480]   So typically, traditionally,
[00:06:41.480 --> 00:06:42.720]   when you think about video security,
[00:06:42.720 --> 00:06:45.020]   you think of someone who's watching video.
[00:06:45.020 --> 00:06:45.900]   Now as a company,
[00:06:45.900 --> 00:06:47.660]   one of the not star goals that we have
[00:06:47.660 --> 00:06:50.320]   is that no one should watch video in the limit
[00:06:50.320 --> 00:06:51.820]   'cause it's actually impossible.
[00:06:51.820 --> 00:06:54.600]   It's so ridiculous when you actually visit one of these,
[00:06:54.600 --> 00:06:56.420]   whether it's the security operations center
[00:06:56.420 --> 00:06:57.540]   or real-time crime center,
[00:06:57.540 --> 00:06:58.940]   you'll see how ridiculous it is
[00:06:58.940 --> 00:07:01.300]   to have someone watching all of that video.
[00:07:01.300 --> 00:07:04.940]   So, and the second is managing disparate systems,
[00:07:04.940 --> 00:07:07.420]   whether it's enterprise or public safety,
[00:07:07.420 --> 00:07:09.540]   you have a lot of different vendors in the space.
[00:07:09.540 --> 00:07:11.560]   The space is extremely fragmented.
[00:07:11.560 --> 00:07:13.940]   I think about it a little bit like healthcare sometimes
[00:07:13.940 --> 00:07:15.820]   where you have information,
[00:07:15.820 --> 00:07:17.880]   it's just present in different systems.
[00:07:17.880 --> 00:07:20.100]   So the question really becomes,
[00:07:20.100 --> 00:07:21.540]   how can you centrally manage that?
[00:07:21.540 --> 00:07:23.540]   And we'll talk about cloud and AI a little bit,
[00:07:23.540 --> 00:07:26.100]   maybe as we go on,
[00:07:26.100 --> 00:07:30.040]   but it's really about how do you optimize that response?
[00:07:30.040 --> 00:07:30.920]   So we use analytics,
[00:07:30.920 --> 00:07:33.460]   we use AI to be able to help the operated,
[00:07:33.460 --> 00:07:35.000]   not only focus on what matters
[00:07:35.000 --> 00:07:36.740]   within an individual video stream,
[00:07:36.740 --> 00:07:38.900]   but also across those different video streams
[00:07:38.900 --> 00:07:40.220]   and different systems,
[00:07:40.220 --> 00:07:41.880]   be able to surface relevant information.
[00:07:41.880 --> 00:07:43.240]   And relevance is really, I think,
[00:07:43.240 --> 00:07:46.940]   the key part of what we're focusing on now.
[00:07:46.940 --> 00:07:49.140]   - And can we get even a little more specific?
[00:07:49.140 --> 00:07:51.720]   Like where are we?
[00:07:51.720 --> 00:07:53.840]   Like what's a customer?
[00:07:53.840 --> 00:07:55.640]   What are they trying to do?
[00:07:55.640 --> 00:07:58.720]   I mean, I know nothing about video security,
[00:07:58.720 --> 00:08:00.960]   so I think you really need to like walk me through it.
[00:08:00.960 --> 00:08:02.560]   - Okay, so let's set the stage.
[00:08:02.560 --> 00:08:04.360]   So for enterprise,
[00:08:04.360 --> 00:08:06.680]   some of our biggest customers are, for example, school.
[00:08:06.680 --> 00:08:11.660]   School is a very unique operating environment, I would say,
[00:08:11.660 --> 00:08:13.020]   especially in the United States,
[00:08:13.020 --> 00:08:15.220]   with a lot of the issues that have happened
[00:08:15.220 --> 00:08:16.420]   here and continue.
[00:08:16.420 --> 00:08:18.980]   So typically you have two classes of users
[00:08:18.980 --> 00:08:19.860]   in video security.
[00:08:19.860 --> 00:08:22.080]   You have those who monitor video,
[00:08:22.080 --> 00:08:26.660]   so a SOC, where they essentially pay people to watch video
[00:08:26.660 --> 00:08:29.060]   and essentially deal with alerts from the system.
[00:08:29.060 --> 00:08:30.840]   At that point, you have systems
[00:08:30.840 --> 00:08:33.460]   that some may be not AI enhanced at all,
[00:08:33.460 --> 00:08:35.580]   no analytics at all, where you're just watching video
[00:08:35.580 --> 00:08:36.840]   and you have to watch the video
[00:08:36.840 --> 00:08:38.920]   and then essentially deal with it as a human.
[00:08:38.920 --> 00:08:41.740]   Increasingly, many of those video security systems have AI.
[00:08:41.740 --> 00:08:45.500]   So you're actually watching events and you're viewing video.
[00:08:45.500 --> 00:08:48.400]   The second class of user is not watching video at all.
[00:08:48.400 --> 00:08:51.700]   In fact, it is very rare to have someone spend the whole day
[00:08:51.700 --> 00:08:54.580]   at their desk in many of these cases, especially a school.
[00:08:54.580 --> 00:08:57.020]   You might have a single roaming security guard
[00:08:57.020 --> 00:08:58.980]   who is essentially going about their job,
[00:08:58.980 --> 00:09:00.700]   checking on different things in the school,
[00:09:00.700 --> 00:09:04.580]   dealing with student-related issues, tending to the staff.
[00:09:04.580 --> 00:09:07.340]   The only thing they have is a mobile phone in their pocket
[00:09:07.340 --> 00:09:08.960]   where you may be getting alerts
[00:09:08.960 --> 00:09:11.180]   from your underlying video security system.
[00:09:11.180 --> 00:09:13.060]   So essentially, you have to figure out
[00:09:13.060 --> 00:09:14.860]   how to deal with those alerts,
[00:09:14.860 --> 00:09:17.060]   including the accuracy of those alerts
[00:09:17.060 --> 00:09:19.340]   and triaging that to the right response.
[00:09:19.340 --> 00:09:22.740]   But that's basically the two customer bases that we have.
[00:09:22.740 --> 00:09:25.140]   So the problem we wanna be able to solve is
[00:09:25.140 --> 00:09:28.180]   how do we get the most relevant alerts to those customers
[00:09:28.180 --> 00:09:30.740]   and build a user experience where they can effectively deal
[00:09:30.740 --> 00:09:33.220]   with the situation when they're under stress?
[00:09:33.220 --> 00:09:35.220]   - And so what does an alert look like?
[00:09:35.220 --> 00:09:38.540]   - So an alert may be just an event
[00:09:38.540 --> 00:09:41.060]   that comes from a video security system.
[00:09:41.060 --> 00:09:44.580]   So for example, many of the cameras that we built today
[00:09:44.580 --> 00:09:46.860]   have AI embedded in them.
[00:09:46.860 --> 00:09:49.500]   That AI essentially allows you to set up different rules.
[00:09:49.500 --> 00:09:50.940]   The customer sets up different rules.
[00:09:50.940 --> 00:09:53.900]   So for example, they may set up a line crossing rule
[00:09:53.900 --> 00:09:56.420]   that says, "Okay, when someone crosses this line,
[00:09:56.420 --> 00:09:57.580]   "send me an event."
[00:09:57.580 --> 00:09:59.460]   That event will basically have,
[00:09:59.460 --> 00:10:01.760]   "Okay, this rule was triggered.
[00:10:01.760 --> 00:10:03.660]   "Here's a snapshot of what happened."
[00:10:03.660 --> 00:10:07.460]   So a person crossing the line
[00:10:07.460 --> 00:10:08.540]   and some other metadata
[00:10:08.540 --> 00:10:10.140]   depending on how the rule is configured.
[00:10:10.140 --> 00:10:12.500]   So essentially an alert will be,
[00:10:12.500 --> 00:10:16.580]   it's very similar to an event condition action type workflow
[00:10:16.580 --> 00:10:19.340]   where the action is performed by the human,
[00:10:19.340 --> 00:10:21.980]   but the event is usually taken care of by AI,
[00:10:21.980 --> 00:10:22.820]   typically today.
[00:10:22.820 --> 00:10:25.900]   So whether it's using some combination of object detection
[00:10:25.900 --> 00:10:27.540]   and tracking classification,
[00:10:27.540 --> 00:10:30.140]   and then the condition is usually set up by humans.
[00:10:30.140 --> 00:10:31.260]   And we should talk about that
[00:10:31.260 --> 00:10:34.500]   because as a company and from an AI perspective,
[00:10:34.500 --> 00:10:36.740]   we don't believe that rules are the right way to go,
[00:10:36.740 --> 00:10:38.780]   even though much of what we know as AI
[00:10:38.780 --> 00:10:40.820]   came from rule-based systems.
[00:10:40.820 --> 00:10:42.940]   Configuring a system using rules
[00:10:42.940 --> 00:10:44.720]   makes it very difficult for humans
[00:10:44.720 --> 00:10:47.220]   to be able to take what they have in their head visually
[00:10:47.220 --> 00:10:48.580]   and then map that to something
[00:10:48.580 --> 00:10:50.380]   that they need to look out for in the future.
[00:10:50.380 --> 00:10:52.820]   Because proactively,
[00:10:52.820 --> 00:10:55.380]   you set up all of this configuration in the rule,
[00:10:55.380 --> 00:10:58.400]   which depends on analytics metadata, AI metadata,
[00:10:58.400 --> 00:11:00.620]   but most of us typically don't know
[00:11:00.620 --> 00:11:01.980]   what's going to happen in the future.
[00:11:01.980 --> 00:11:03.700]   So this has happened at setup.
[00:11:03.700 --> 00:11:06.100]   This might never be changed for a long time
[00:11:06.100 --> 00:11:08.620]   because it may be complex to go in and change it.
[00:11:08.620 --> 00:11:10.580]   But me as a human, when I see something,
[00:11:10.580 --> 00:11:12.500]   I know that it's not right.
[00:11:12.500 --> 00:11:15.180]   That's one thing as humans, we do a very, very good job of
[00:11:15.180 --> 00:11:16.980]   is that when we visually see something,
[00:11:16.980 --> 00:11:18.260]   we can reason about the fact
[00:11:18.260 --> 00:11:20.100]   that there's something gone awry there.
[00:11:20.100 --> 00:11:22.680]   There's something that we wanna know about in the future.
[00:11:22.680 --> 00:11:24.700]   So the way we're building systems today
[00:11:24.700 --> 00:11:27.180]   is to be able to get closer to how humans think
[00:11:27.180 --> 00:11:30.160]   and allow humans to essentially visually specify
[00:11:30.160 --> 00:11:31.340]   the things that they care about
[00:11:31.340 --> 00:11:33.420]   so that we can essentially push this workflow
[00:11:33.420 --> 00:11:36.220]   from being largely a very reactive workflow.
[00:11:36.220 --> 00:11:38.300]   And I say that across public safety and enterprise
[00:11:38.300 --> 00:11:39.660]   to a more proactive workflow.
[00:11:39.660 --> 00:11:41.900]   And this is where AI can really kind of help.
[00:11:41.900 --> 00:11:46.940]   - And so how do you frame the problem as a vision problem?
[00:11:46.940 --> 00:11:50.660]   Like, are you trying to track all the people and objects
[00:11:50.660 --> 00:11:53.620]   and then set up a rule that like if a person goes
[00:11:53.620 --> 00:11:57.100]   in this area where there's not supposed to be a person,
[00:11:57.100 --> 00:11:58.260]   we fire an event?
[00:11:58.260 --> 00:12:01.000]   Or is it more unstructured?
[00:12:01.000 --> 00:12:05.200]   Like if we have training data that's like people in the area
[00:12:05.200 --> 00:12:07.760]   they're not supposed to be, people not,
[00:12:07.760 --> 00:12:09.320]   and then we're just sort of like looking for
[00:12:09.320 --> 00:12:11.560]   like a custom model to flag something.
[00:12:11.560 --> 00:12:12.760]   - That's a really good question.
[00:12:12.760 --> 00:12:15.600]   So I would say the majority of use cases
[00:12:15.600 --> 00:12:18.240]   and most vendors in this space,
[00:12:18.240 --> 00:12:20.040]   and many vendors have transitioned
[00:12:20.040 --> 00:12:21.740]   into deep learning based models,
[00:12:21.740 --> 00:12:23.960]   which really opened up from a vision standpoint
[00:12:23.960 --> 00:12:25.560]   what we can do, obviously.
[00:12:25.560 --> 00:12:28.020]   But typically you have object detection
[00:12:28.020 --> 00:12:29.860]   is kind of the core to everything,
[00:12:29.860 --> 00:12:31.700]   which is people and vehicles are the biggest thing
[00:12:31.700 --> 00:12:33.300]   that you care about in most of these.
[00:12:33.300 --> 00:12:35.940]   Doesn't matter, vertical specific or otherwise,
[00:12:35.940 --> 00:12:37.900]   that's, you've got to have that.
[00:12:37.900 --> 00:12:39.500]   If you're running analytics on a camera,
[00:12:39.500 --> 00:12:40.900]   you're also doing tracking, obviously,
[00:12:40.900 --> 00:12:43.020]   'cause tracking can provide you a lot of other pieces
[00:12:43.020 --> 00:12:45.220]   of metadata, not only to make your object detection
[00:12:45.220 --> 00:12:48.940]   more efficient, but also you can create different rules
[00:12:48.940 --> 00:12:52.180]   around, you know, speed, direction, things like that.
[00:12:52.180 --> 00:12:55.500]   So how it's done today is you have a set of analytics
[00:12:55.500 --> 00:12:57.540]   that run, whether it's at the edge,
[00:12:57.540 --> 00:12:58.720]   on a server or in the cloud,
[00:12:58.720 --> 00:13:00.620]   and we should talk about distributed computation
[00:13:00.620 --> 00:13:02.100]   because I think this is a key part
[00:13:02.100 --> 00:13:03.700]   of where we're going from an AI perspective
[00:13:03.700 --> 00:13:05.460]   that I think is a little bit different
[00:13:05.460 --> 00:13:06.980]   from where we are today.
[00:13:06.980 --> 00:13:08.340]   You typically have those analytics
[00:13:08.340 --> 00:13:10.740]   generating primitives metadata around.
[00:13:10.740 --> 00:13:13.420]   I detected a person, okay, I further class,
[00:13:13.420 --> 00:13:15.680]   I sub classified this person based on attributes
[00:13:15.680 --> 00:13:18.060]   I understand, same thing for vehicles.
[00:13:18.060 --> 00:13:20.500]   Now I take that metadata and I set up different rules,
[00:13:20.500 --> 00:13:21.340]   as you said.
[00:13:21.340 --> 00:13:24.680]   So, you know, if I want to know, for example,
[00:13:24.680 --> 00:13:27.420]   if a blue car is what I care about,
[00:13:27.420 --> 00:13:30.480]   I can use that metadata to my advantage to set up a rule.
[00:13:30.480 --> 00:13:33.120]   That rule fires, you get an alert,
[00:13:33.120 --> 00:13:35.580]   and then that alert goes back to what we talked about
[00:13:35.580 --> 00:13:39.060]   at the start, where a human can take some action on it.
[00:13:39.060 --> 00:13:41.380]   - And does it get as detailed as like, you know,
[00:13:41.380 --> 00:13:44.220]   these specific people can go here and these, you know,
[00:13:44.220 --> 00:13:46.380]   like if they're not sort of on this list of people,
[00:13:46.380 --> 00:13:48.420]   don't let them, you know, go in this area.
[00:13:48.420 --> 00:13:51.560]   Like how advanced does this get?
[00:13:51.560 --> 00:13:54.560]   - Yeah, so that's, again, I think a very important point.
[00:13:54.560 --> 00:13:59.380]   So detection and even matching happens at a level
[00:13:59.380 --> 00:14:01.900]   on a computer vision domain where we don't need identity,
[00:14:01.900 --> 00:14:02.840]   for example.
[00:14:02.840 --> 00:14:05.320]   What you're talking about is now connecting that metadata
[00:14:05.320 --> 00:14:06.500]   with identity.
[00:14:06.500 --> 00:14:09.860]   So I do not want a particular person to enter
[00:14:09.860 --> 00:14:11.560]   because this person is on a watch list.
[00:14:11.560 --> 00:14:14.860]   It might be someone who's dangerous and so on and so forth.
[00:14:14.860 --> 00:14:17.820]   So that's when you get into things like OCR
[00:14:17.820 --> 00:14:19.900]   and facial recognition, for example,
[00:14:19.900 --> 00:14:21.900]   where now you're connecting identity
[00:14:21.900 --> 00:14:24.420]   with those descriptive AI analytics,
[00:14:24.420 --> 00:14:26.860]   where I'm actually just, I don't know who it is,
[00:14:26.860 --> 00:14:29.500]   but I know how to find the person in the visual domain,
[00:14:29.500 --> 00:14:30.340]   for example.
[00:14:30.340 --> 00:14:33.580]   So that is something customers can do on their sites.
[00:14:33.580 --> 00:14:36.140]   And that information is managed completely by them.
[00:14:36.140 --> 00:14:39.700]   But in terms of getting the analytics down from, you know,
[00:14:39.700 --> 00:14:42.020]   an object to an actual individual,
[00:14:42.020 --> 00:14:44.260]   you need that second piece of information
[00:14:44.260 --> 00:14:45.820]   to be able to connect identity.
[00:14:45.820 --> 00:14:47.980]   - Interesting.
[00:14:47.980 --> 00:14:52.340]   I guess then, do your models typically run on the edge
[00:14:52.340 --> 00:14:55.140]   or the cloud, or is there some kind of hybrid situation?
[00:14:55.140 --> 00:14:57.100]   Like how do you handle that?
[00:14:57.100 --> 00:14:58.860]   - Yeah, so that's a great question.
[00:14:58.860 --> 00:15:01.640]   So for our analytics, we use all three.
[00:15:01.640 --> 00:15:04.860]   And kind of our vision is that AI really needs
[00:15:04.860 --> 00:15:06.500]   to be democratized for users,
[00:15:06.500 --> 00:15:08.540]   regardless of the equipment that they're using.
[00:15:08.540 --> 00:15:11.420]   So some people may invest a lot in edge hardware,
[00:15:11.420 --> 00:15:13.560]   where it's typically quite expensive,
[00:15:13.560 --> 00:15:16.220]   but you can run a lot of the AI computation efficiently
[00:15:16.220 --> 00:15:17.060]   at the edge.
[00:15:17.060 --> 00:15:21.220]   We use a variety of AI SOCs, depending on the platform.
[00:15:22.180 --> 00:15:24.740]   But we also leverage distributed computation,
[00:15:24.740 --> 00:15:26.820]   'cause one of the usability factors
[00:15:26.820 --> 00:15:28.460]   that we think is important is the ability
[00:15:28.460 --> 00:15:30.380]   to centrally manage information
[00:15:30.380 --> 00:15:31.940]   that come from your AI models.
[00:15:31.940 --> 00:15:34.120]   So for users, that's a game changer.
[00:15:34.120 --> 00:15:35.740]   And so we distribute computation.
[00:15:35.740 --> 00:15:37.500]   It should be transparent to the user.
[00:15:37.500 --> 00:15:40.100]   You might have a cheap IP camera, for example,
[00:15:40.100 --> 00:15:42.540]   but you still wanna get the benefits of AI.
[00:15:42.540 --> 00:15:45.180]   So at that point, you may be doing the bulk
[00:15:45.180 --> 00:15:46.620]   of your computation in the cloud,
[00:15:46.620 --> 00:15:48.900]   or on a server on-premises.
[00:15:48.900 --> 00:15:50.460]   How you make that cost effective,
[00:15:50.460 --> 00:15:51.980]   there are some interesting things that we do
[00:15:51.980 --> 00:15:52.820]   to do that.
[00:15:52.820 --> 00:15:56.180]   I think the biggest benefit, how we think about the edge
[00:15:56.180 --> 00:15:58.900]   when it comes to AI in vision is,
[00:15:58.900 --> 00:16:01.660]   the camera really tells you where to look.
[00:16:01.660 --> 00:16:04.700]   And so once you can focus attention,
[00:16:04.700 --> 00:16:06.980]   then you can actually be much more opinionated
[00:16:06.980 --> 00:16:09.300]   and sure about how much computation you spend
[00:16:09.300 --> 00:16:10.860]   to analyze that attention.
[00:16:10.860 --> 00:16:12.740]   But in the limit, we typically can deal
[00:16:12.740 --> 00:16:14.100]   with very simple cameras
[00:16:14.100 --> 00:16:16.600]   that essentially only have motion-based alerts,
[00:16:16.600 --> 00:16:17.900]   which can be very noisy,
[00:16:17.900 --> 00:16:19.780]   'cause they can be triggered constantly.
[00:16:19.780 --> 00:16:21.900]   And then our cloud AI essentially is able
[00:16:21.900 --> 00:16:23.460]   to analyze that and figure out
[00:16:23.460 --> 00:16:26.500]   if that's actually a true event or not.
[00:16:26.500 --> 00:16:29.460]   - Interesting, so the primary reason for going to edge
[00:16:29.460 --> 00:16:33.020]   is just that it's faster and uses less bandwidth.
[00:16:33.020 --> 00:16:33.860]   Is that right?
[00:16:33.860 --> 00:16:36.380]   I sort of thought there might be kind of data privacy issues
[00:16:36.380 --> 00:16:39.940]   that would cause a lot of customers to go local.
[00:16:39.940 --> 00:16:41.580]   - Absolutely, I think customers have lots
[00:16:41.580 --> 00:16:42.460]   of different reasons.
[00:16:42.460 --> 00:16:44.500]   So I mean, outside of the technical challenges
[00:16:44.500 --> 00:16:46.180]   around bandwidth and compute,
[00:16:46.180 --> 00:16:49.020]   absolutely, some customers prefer to manage their data
[00:16:49.020 --> 00:16:50.540]   entirely on-premises.
[00:16:50.540 --> 00:16:51.980]   And they have that option.
[00:16:51.980 --> 00:16:54.380]   That's essentially the way we build our system.
[00:16:54.380 --> 00:16:56.460]   They have the option of doing that.
[00:16:56.460 --> 00:16:59.780]   I think increasingly customers are seeing the benefits
[00:16:59.780 --> 00:17:00.860]   of centrally managing it,
[00:17:00.860 --> 00:17:02.540]   even if their data is on-premises,
[00:17:02.540 --> 00:17:03.940]   which is where federated systems
[00:17:03.940 --> 00:17:05.420]   become very, very important, right?
[00:17:05.420 --> 00:17:08.900]   So how do I bring the benefits of centrally managed AI
[00:17:08.900 --> 00:17:10.820]   while still operating on data,
[00:17:10.820 --> 00:17:13.220]   AI metadata that is generated on-premises?
[00:17:13.220 --> 00:17:15.260]   And we do have solutions that also do that.
[00:17:15.260 --> 00:17:18.580]   So for example, I may be able to conduct
[00:17:18.580 --> 00:17:21.380]   a natural language search from the cloud,
[00:17:21.380 --> 00:17:24.460]   but that cloud search gets executed on-premises.
[00:17:24.460 --> 00:17:26.660]   So if I'm doing a similarity search, for example,
[00:17:26.660 --> 00:17:29.180]   where I'm essentially searching in the embedding space
[00:17:29.180 --> 00:17:32.900]   for an answer, I may not be storing any of that data
[00:17:32.900 --> 00:17:34.540]   in the cloud, it may be on-premises.
[00:17:34.540 --> 00:17:37.620]   Flexibility is key, I think, both in terms of privacy
[00:17:37.620 --> 00:17:40.180]   and in terms of managing compute and bandwidth.
[00:17:40.180 --> 00:17:44.100]   - And I guess, how do you think about
[00:17:44.100 --> 00:17:46.540]   the evaluation of your models?
[00:17:46.540 --> 00:17:48.740]   Like maybe let's just take the object recognition
[00:17:48.740 --> 00:17:51.300]   to be really concrete.
[00:17:51.300 --> 00:17:54.020]   Like I would think that every customer
[00:17:54.020 --> 00:17:57.700]   would have sort of different levels of quality
[00:17:57.700 --> 00:17:58.540]   in their object recognition,
[00:17:58.540 --> 00:17:59.980]   depending on what cameras they're using,
[00:17:59.980 --> 00:18:04.820]   like what the background looks like even.
[00:18:04.820 --> 00:18:07.940]   And then I would imagine that both kinds of errors
[00:18:07.940 --> 00:18:08.780]   are bad for you, right?
[00:18:08.780 --> 00:18:11.380]   Like you don't want false positives, obviously,
[00:18:11.380 --> 00:18:14.140]   and then you also don't want false negatives,
[00:18:14.140 --> 00:18:15.500]   'cause there's sort of operator fatigue,
[00:18:15.500 --> 00:18:18.020]   but then also I'd imagine that you might be violating
[00:18:18.020 --> 00:18:22.180]   contracts if you sort of miss like a real event
[00:18:22.180 --> 00:18:23.380]   that you wanna trigger.
[00:18:23.380 --> 00:18:25.420]   So how do you think about that?
[00:18:25.420 --> 00:18:26.940]   - That's an excellent question.
[00:18:26.940 --> 00:18:29.180]   And I think it also comes back to one of the reasons
[00:18:29.180 --> 00:18:31.940]   why we work with weights and biases,
[00:18:31.940 --> 00:18:34.740]   and then is essentially exactly that.
[00:18:34.740 --> 00:18:36.380]   Evaluation of these things have gotten
[00:18:36.380 --> 00:18:38.340]   a lot more complicated and nuanced,
[00:18:38.340 --> 00:18:40.180]   I would say, even over the last few years,
[00:18:40.180 --> 00:18:42.660]   where initially a lot of vendors,
[00:18:42.660 --> 00:18:45.100]   including us, train one model essentially
[00:18:45.100 --> 00:18:46.580]   that is deployed on the camera,
[00:18:46.580 --> 00:18:48.860]   and you can essentially buy a camera and use it anywhere.
[00:18:48.860 --> 00:18:50.500]   What we've learned over the last few years
[00:18:50.500 --> 00:18:52.700]   is that these different form factors,
[00:18:52.700 --> 00:18:53.860]   different fields of view,
[00:18:53.860 --> 00:18:56.060]   different environmental conditions mean that
[00:18:56.060 --> 00:18:59.340]   it really doesn't matter how much data you have,
[00:18:59.340 --> 00:19:01.580]   there's always going to be an element to that
[00:19:01.580 --> 00:19:05.380]   that needs to be essentially adapted to the customer.
[00:19:05.380 --> 00:19:07.820]   So in terms of evaluation,
[00:19:07.820 --> 00:19:09.620]   how we look at it is we kind of break
[00:19:09.620 --> 00:19:11.420]   the machine learning components down
[00:19:11.420 --> 00:19:13.860]   to kind of its primitive pieces.
[00:19:13.860 --> 00:19:15.380]   So for an object detector,
[00:19:15.380 --> 00:19:17.580]   we may not change that as often,
[00:19:17.580 --> 00:19:19.220]   because that is something that uses
[00:19:19.220 --> 00:19:21.340]   the most diverse data that we have,
[00:19:21.340 --> 00:19:23.380]   is as generalizable as possible.
[00:19:23.380 --> 00:19:24.980]   The one thing you might do in that case
[00:19:24.980 --> 00:19:27.820]   is you may train different variants, different resolutions.
[00:19:27.820 --> 00:19:29.660]   You might have different models
[00:19:29.660 --> 00:19:31.460]   that deal with thermal, for example,
[00:19:31.460 --> 00:19:32.980]   we have thermal cameras that's trained
[00:19:32.980 --> 00:19:36.380]   on specific data that is specific to that.
[00:19:36.380 --> 00:19:39.020]   So you have models that don't change that often,
[00:19:39.020 --> 00:19:41.380]   but essentially provide you that core signal
[00:19:41.380 --> 00:19:42.540]   of where to look.
[00:19:42.540 --> 00:19:44.060]   And then as you go downstream,
[00:19:44.060 --> 00:19:45.900]   you might start getting much more specialized
[00:19:45.900 --> 00:19:48.740]   where a user might have very specific notion
[00:19:48.740 --> 00:19:51.380]   of what attributes they want to sub classify.
[00:19:51.380 --> 00:19:53.500]   And it's not the same across different users.
[00:19:53.500 --> 00:19:55.140]   This is where the cloud helps
[00:19:55.140 --> 00:19:58.580]   and also where we can deploy additional types of models.
[00:19:58.580 --> 00:19:59.780]   So in terms of evaluation,
[00:19:59.780 --> 00:20:01.540]   I would say that those types of models
[00:20:01.540 --> 00:20:05.700]   I evaluated on a much more fine grained case basis.
[00:20:05.700 --> 00:20:08.220]   So customer by customer, region by region,
[00:20:08.220 --> 00:20:10.660]   we kind of cluster different types of customer types
[00:20:10.660 --> 00:20:12.860]   to be able to understand how the models are doing.
[00:20:12.860 --> 00:20:16.020]   And I think part of it is also our customers
[00:20:16.020 --> 00:20:18.580]   have gotten a lot more used to video analytics
[00:20:18.580 --> 00:20:21.300]   driven systems where they will come to us and say,
[00:20:21.300 --> 00:20:24.140]   "Hey, this isn't performing this well in this situation."
[00:20:24.140 --> 00:20:27.580]   At that point, we'll go and take a look with the customer.
[00:20:27.580 --> 00:20:30.500]   We would collect data to try to understand what's going on.
[00:20:30.500 --> 00:20:32.180]   And then our machine learning teams would dig in
[00:20:32.180 --> 00:20:33.780]   to try to make adjustments
[00:20:33.780 --> 00:20:35.300]   essentially based on that customer.
[00:20:35.300 --> 00:20:37.340]   So being able to actually do that
[00:20:37.340 --> 00:20:38.900]   and scale a machine learning team
[00:20:38.900 --> 00:20:40.580]   has been one of the big challenges
[00:20:40.580 --> 00:20:41.940]   I would say in the last couple of years
[00:20:41.940 --> 00:20:44.900]   where we've really focused on the best tools.
[00:20:44.900 --> 00:20:47.540]   So started with data and data operations.
[00:20:47.540 --> 00:20:50.660]   So we worked with figure eight when they transitioned up
[00:20:50.660 --> 00:20:52.580]   and I know that's kind of like your background.
[00:20:52.580 --> 00:20:55.620]   And when I came in and started running the team over here,
[00:20:55.620 --> 00:20:57.140]   that was my number one concern,
[00:20:57.140 --> 00:21:00.460]   which is data, data annotation and data efficiency.
[00:21:00.460 --> 00:21:02.180]   I think now we've got a good handle
[00:21:02.180 --> 00:21:04.180]   on working with different annotation vendors.
[00:21:04.180 --> 00:21:05.780]   And we realized that you kind of need
[00:21:05.780 --> 00:21:08.740]   a multiplicity of annotators and different,
[00:21:08.740 --> 00:21:11.100]   coverage to be able to do these different tasks.
[00:21:11.100 --> 00:21:12.700]   Now what's happened is evaluation
[00:21:12.700 --> 00:21:14.180]   has become a bigger problem,
[00:21:14.180 --> 00:21:17.580]   which is how do I connect my inference infrastructure
[00:21:17.580 --> 00:21:19.660]   with my machine learning evaluation tools?
[00:21:19.660 --> 00:21:21.940]   How do I visualize that information
[00:21:21.940 --> 00:21:23.380]   so the different stakeholders,
[00:21:23.380 --> 00:21:25.220]   whether it's a firmware engineer
[00:21:25.220 --> 00:21:27.140]   or a machine learning engineer can see
[00:21:27.140 --> 00:21:30.060]   how did my new model do compared to my previous model?
[00:21:30.060 --> 00:21:32.620]   And this specific customer problem that we fixed,
[00:21:32.620 --> 00:21:34.620]   how did it affect all the other customers?
[00:21:34.620 --> 00:21:38.700]   And so managing annotated image datasets,
[00:21:38.700 --> 00:21:41.220]   custom visualizations, adding loggers
[00:21:41.220 --> 00:21:43.380]   to our visualization system,
[00:21:43.380 --> 00:21:45.740]   so it can actually deal with our machine learning
[00:21:45.740 --> 00:21:47.860]   training repos and our model zoos.
[00:21:47.860 --> 00:21:50.980]   This has become probably the biggest area of concentration
[00:21:50.980 --> 00:21:53.660]   I would say in the last 12 to 18 months,
[00:21:53.660 --> 00:21:56.220]   which is why we use tools like weights and biases
[00:21:56.220 --> 00:21:58.220]   across our team, because it makes it impossible
[00:21:58.220 --> 00:22:00.820]   to actually, we can do the evaluation,
[00:22:00.820 --> 00:22:02.980]   it's bookkeeping for those measuring
[00:22:02.980 --> 00:22:04.420]   across those different datasets
[00:22:04.420 --> 00:22:07.540]   and actually increasing our speed to be able to do that
[00:22:07.540 --> 00:22:09.620]   is probably the biggest, I would say
[00:22:09.620 --> 00:22:12.300]   that the barrier to entry right now is probably that
[00:22:12.300 --> 00:22:14.540]   in terms of getting new models out.
[00:22:14.540 --> 00:22:18.220]   - So does every customer kind of get their own evaluation
[00:22:18.220 --> 00:22:20.980]   before it goes live?
[00:22:20.980 --> 00:22:23.460]   - Typically no, 'cause we have so many customers,
[00:22:23.460 --> 00:22:25.540]   we have thousands and thousands of customers,
[00:22:25.540 --> 00:22:27.740]   so that becomes very difficult.
[00:22:27.740 --> 00:22:30.660]   I would say the thing that we've learned to do is,
[00:22:30.660 --> 00:22:32.660]   so a lot of customers, if they have an issue
[00:22:32.660 --> 00:22:34.900]   with a particular model or the analytics
[00:22:34.900 --> 00:22:36.420]   will come through support,
[00:22:36.420 --> 00:22:38.580]   it'll get triaged into our machine learning team,
[00:22:38.580 --> 00:22:39.500]   for example.
[00:22:39.500 --> 00:22:41.620]   What we try to do is figure out
[00:22:41.620 --> 00:22:43.860]   if what this customer is seeing, is it common?
[00:22:43.860 --> 00:22:45.860]   What other customers may be having this issue?
[00:22:45.860 --> 00:22:47.940]   How do we kind of cluster and segment that
[00:22:47.940 --> 00:22:49.460]   so that we can go off to the problem?
[00:22:49.460 --> 00:22:51.500]   Because as you know, time,
[00:22:51.500 --> 00:22:53.140]   machine learning time is precious.
[00:22:53.140 --> 00:22:55.420]   And so we try not to solve problems
[00:22:55.420 --> 00:22:57.900]   that end up truly being a one-off.
[00:22:57.900 --> 00:22:59.540]   There may be other ways that a customer
[00:22:59.540 --> 00:23:01.460]   may be able to deal with that problem.
[00:23:01.460 --> 00:23:03.740]   And also you have to separate model performance
[00:23:03.740 --> 00:23:06.460]   from installation and other things like that,
[00:23:06.460 --> 00:23:08.860]   where a customer, like their camera might have moved.
[00:23:08.860 --> 00:23:11.020]   They might have not positioned it
[00:23:11.020 --> 00:23:12.700]   in the right way for that task.
[00:23:12.700 --> 00:23:14.620]   And actually this is some of the stuff
[00:23:14.620 --> 00:23:16.060]   that we're doing now in the cloud
[00:23:16.060 --> 00:23:18.620]   to be able to do things like camera help.
[00:23:18.620 --> 00:23:21.100]   Use machine learning to determine
[00:23:21.100 --> 00:23:24.140]   if the camera has moved, if there's a spider web on it.
[00:23:24.140 --> 00:23:25.740]   I mean, things as simple as that,
[00:23:25.740 --> 00:23:27.340]   because initially that used to just hit
[00:23:27.340 --> 00:23:28.380]   the machine learning team.
[00:23:28.380 --> 00:23:31.620]   And they were like, why are you seeing bad performance?
[00:23:31.620 --> 00:23:34.820]   Okay, after a bunch of back and forth with the customer,
[00:23:34.820 --> 00:23:36.380]   you find out this is the problem.
[00:23:36.380 --> 00:23:37.700]   Okay, we want to automate that.
[00:23:37.700 --> 00:23:38.780]   We want to use machine learning
[00:23:38.780 --> 00:23:40.780]   to help us find issues like that.
[00:23:40.780 --> 00:23:44.020]   And so that's not as exciting as,
[00:23:44.020 --> 00:23:47.180]   maybe developing the next object detector,
[00:23:47.180 --> 00:23:48.900]   focusing on a new backbone
[00:23:48.900 --> 00:23:50.340]   that gives us much better performance.
[00:23:50.340 --> 00:23:53.420]   It's things like this that really help us save time
[00:23:53.420 --> 00:23:54.340]   in the machine learning team
[00:23:54.340 --> 00:23:56.700]   so that we can do more interesting things.
[00:23:56.700 --> 00:24:01.380]   - This is probably not a well formulated question,
[00:24:01.380 --> 00:24:02.220]   but I'm just kind of curious,
[00:24:02.220 --> 00:24:04.980]   how many models are you working on at any given time?
[00:24:04.980 --> 00:24:09.980]   Like how many models do you have live in customer sites?
[00:24:09.980 --> 00:24:13.220]   - So, because, you know,
[00:24:13.220 --> 00:24:15.900]   so I run the AI team at Motorola.
[00:24:15.900 --> 00:24:17.220]   It's not just computer vision,
[00:24:17.220 --> 00:24:19.980]   it's speech and audio, language and NLP.
[00:24:19.980 --> 00:24:21.300]   So across all of those,
[00:24:21.300 --> 00:24:23.460]   that's probably a very large number of models.
[00:24:23.460 --> 00:24:26.580]   If we focus just on kind of the video space,
[00:24:26.580 --> 00:24:29.060]   I would say we're still looking at, you know,
[00:24:29.060 --> 00:24:30.060]   under a hundred models.
[00:24:30.060 --> 00:24:31.700]   So like tens of models.
[00:24:31.700 --> 00:24:33.420]   And very, very specifically,
[00:24:33.420 --> 00:24:34.940]   we've tried to keep a handle on it
[00:24:34.940 --> 00:24:38.220]   because I think there is definitely a need
[00:24:38.220 --> 00:24:40.020]   for more custom solutions,
[00:24:40.020 --> 00:24:41.780]   but managing those solutions,
[00:24:41.780 --> 00:24:43.140]   as you pointed out before,
[00:24:43.140 --> 00:24:45.300]   we wanna make sure that we have the ability
[00:24:45.300 --> 00:24:46.740]   to monitor those effectively,
[00:24:46.740 --> 00:24:48.740]   be able to evaluate those effectively.
[00:24:48.740 --> 00:24:51.780]   So it's still a relatively small number of models,
[00:24:51.780 --> 00:24:54.420]   but they touch many, many, many, many customers.
[00:24:54.420 --> 00:24:58.660]   And so monitoring and evaluation becomes a huge problem,
[00:24:58.660 --> 00:25:00.340]   as well as going back to annotation.
[00:25:00.340 --> 00:25:01.780]   I mean, we're looking at other things
[00:25:01.780 --> 00:25:04.820]   like weak labeling approaches, confident learning.
[00:25:04.820 --> 00:25:07.140]   Alex Ratner over at Stanford,
[00:25:07.140 --> 00:25:10.100]   you know, Snorkel AI, the company that they spun out.
[00:25:10.100 --> 00:25:12.380]   We used Snorkel actually a few years ago
[00:25:12.380 --> 00:25:14.180]   when it was an open source project.
[00:25:14.180 --> 00:25:17.460]   And it was difficult mostly because of all the engineering
[00:25:17.460 --> 00:25:19.540]   and plumbing needed to actually make that happen.
[00:25:19.540 --> 00:25:22.020]   And now that's what I think Alex's startup is doing now
[00:25:22.020 --> 00:25:22.900]   with Snorkel flow.
[00:25:22.900 --> 00:25:25.260]   And, you know, I talked to him, you know, recently,
[00:25:25.260 --> 00:25:27.460]   I think it's solutions like that,
[00:25:27.460 --> 00:25:30.340]   we really need to get into the edge cases for AI.
[00:25:30.340 --> 00:25:33.380]   I think you don't have data from all these customers
[00:25:33.380 --> 00:25:35.700]   and a lot of customers don't feel comfortable sharing data,
[00:25:35.700 --> 00:25:37.540]   which is completely, I think, fine.
[00:25:37.540 --> 00:25:39.540]   We have to find other ways to solve the problem.
[00:25:39.540 --> 00:25:43.460]   So another example is a company called Clean Lab,
[00:25:43.460 --> 00:25:45.380]   which is how do you learn with noisy data?
[00:25:45.380 --> 00:25:47.860]   You know, at that point,
[00:25:47.860 --> 00:25:50.500]   you've accumulated a massive amount of data
[00:25:50.500 --> 00:25:52.020]   from different places,
[00:25:52.020 --> 00:25:54.260]   label quality may be highly questionable.
[00:25:54.260 --> 00:25:55.700]   So then the question becomes,
[00:25:55.700 --> 00:25:57.700]   okay, how do I actually reason across that
[00:25:57.700 --> 00:25:58.500]   in a systematic way?
[00:25:58.500 --> 00:25:59.220]   I know you're smiling
[00:25:59.220 --> 00:26:01.460]   'cause these are the exact things that I think,
[00:26:01.460 --> 00:26:02.900]   you know, why weights and biases
[00:26:02.900 --> 00:26:04.500]   helps a lot of its customers deal with.
[00:26:04.500 --> 00:26:06.900]   But I think this is what I mean
[00:26:06.900 --> 00:26:09.540]   when I say the complexity for a machine learning team
[00:26:09.540 --> 00:26:11.300]   is actually exponentially increasing
[00:26:11.300 --> 00:26:14.980]   is you have to look at these other machine driven ways
[00:26:14.980 --> 00:26:17.300]   to increase the quality of your data
[00:26:17.300 --> 00:26:18.980]   and augment your data sets.
[00:26:18.980 --> 00:26:20.500]   So now you have to evaluate those.
[00:26:20.500 --> 00:26:23.780]   You have to evaluate models that actually help your data,
[00:26:23.780 --> 00:26:25.300]   which also need to be evaluated.
[00:26:25.300 --> 00:26:27.460]   And so I think just getting a handle on that
[00:26:27.460 --> 00:26:29.620]   is one of the challenges that we have.
[00:26:29.620 --> 00:26:32.100]   - So this is a really practical question,
[00:26:32.100 --> 00:26:33.540]   but I feel like a lot of people ask me
[00:26:33.540 --> 00:26:35.540]   what best practices here.
[00:26:35.540 --> 00:26:38.580]   When a customer complains,
[00:26:38.580 --> 00:26:39.780]   like someone is like,
[00:26:39.780 --> 00:26:41.620]   "Hey, you know, this thing did the wrong thing
[00:26:41.620 --> 00:26:42.500]   in this situation."
[00:26:42.500 --> 00:26:45.940]   How do you actually triage that?
[00:26:45.940 --> 00:26:48.660]   And what are the like likely things
[00:26:48.660 --> 00:26:50.740]   that you end up doing to fix it?
[00:26:50.740 --> 00:26:52.820]   - Yeah, really good question.
[00:26:52.820 --> 00:26:55.220]   I think, especially when it's distributed
[00:26:55.220 --> 00:26:56.260]   computer computation,
[00:26:56.260 --> 00:26:58.500]   where they've got a camera that's running an AI model
[00:26:58.500 --> 00:26:59.780]   or a bunch of AI models,
[00:26:59.780 --> 00:27:01.140]   they've got a server on premises
[00:27:01.140 --> 00:27:03.140]   that is also running AI and cloud.
[00:27:03.140 --> 00:27:04.660]   So the customer doesn't care
[00:27:04.660 --> 00:27:06.500]   and they shouldn't care where any of that is running.
[00:27:06.500 --> 00:27:10.420]   They'll just say a particular event is false positive.
[00:27:10.420 --> 00:27:13.060]   The false positive rate has increased dramatically.
[00:27:13.060 --> 00:27:16.500]   I'm seeing this problem, go solve it.
[00:27:16.500 --> 00:27:19.780]   And so typically that hits our support team.
[00:27:19.780 --> 00:27:22.420]   And I think we are continually trying to make sure
[00:27:22.420 --> 00:27:24.740]   that we're giving our support teams the better tools
[00:27:24.740 --> 00:27:25.860]   to be able to triage it.
[00:27:25.860 --> 00:27:28.260]   I know this is a problem that you guys
[00:27:28.260 --> 00:27:29.380]   are very familiar with as well,
[00:27:29.380 --> 00:27:31.300]   where otherwise it's a sieve.
[00:27:31.300 --> 00:27:34.260]   It passes straight through down to the AI team.
[00:27:34.260 --> 00:27:36.340]   And now you have machine learning engineers
[00:27:36.340 --> 00:27:39.620]   and data scientists getting involved way too early.
[00:27:39.620 --> 00:27:42.740]   - And okay, before you, sorry, before you go on,
[00:27:42.740 --> 00:27:44.580]   I'm just curious, are there any tools that you've used
[00:27:44.580 --> 00:27:46.180]   that you kind of recommend?
[00:27:46.180 --> 00:27:50.180]   Like are you using any kind of ML explainability tools
[00:27:50.180 --> 00:27:51.300]   or is it kind of home built?
[00:27:51.300 --> 00:27:52.100]   And if it's home built,
[00:27:52.100 --> 00:27:54.500]   what kinds of things are you showing to the support team?
[00:27:54.500 --> 00:27:55.940]   - So it's a mix.
[00:27:55.940 --> 00:27:59.860]   So first thing that we built a lot of in-house
[00:27:59.860 --> 00:28:01.300]   is a lot of visual tools.
[00:28:01.300 --> 00:28:03.220]   So you know the system,
[00:28:03.220 --> 00:28:04.820]   if you can get video from the system,
[00:28:04.820 --> 00:28:07.060]   you have video clips or you have images.
[00:28:07.060 --> 00:28:08.100]   How can I feed that in
[00:28:08.100 --> 00:28:10.420]   and dump diagnostic data immediately so that,
[00:28:10.420 --> 00:28:12.580]   and then distill that diagnostic data
[00:28:12.580 --> 00:28:15.380]   so that support team can at least try to figure out
[00:28:15.380 --> 00:28:16.580]   where the problem is.
[00:28:16.580 --> 00:28:18.420]   Is it happening in object detection?
[00:28:18.420 --> 00:28:20.820]   Is it happening because of a classification problem?
[00:28:20.820 --> 00:28:22.580]   Is it happening because of the environment?
[00:28:22.580 --> 00:28:23.620]   The environment has changed
[00:28:23.620 --> 00:28:25.860]   where your performance dropped off a cliff.
[00:28:25.860 --> 00:28:29.860]   So we built some homegrown tools specifically for cameras
[00:28:29.860 --> 00:28:31.780]   because the camera is probably one
[00:28:31.780 --> 00:28:33.460]   of the most difficult things to debug
[00:28:33.460 --> 00:28:37.460]   because you have essentially AI running on a firmware build.
[00:28:37.460 --> 00:28:39.460]   We do have to do manual field testing
[00:28:39.460 --> 00:28:40.500]   of those cameras as well.
[00:28:40.500 --> 00:28:43.860]   You can't just test it upstream when we generate a model.
[00:28:43.860 --> 00:28:46.340]   So a lot of that homegrown tools
[00:28:46.340 --> 00:28:48.180]   are particularly to deal with our cameras
[00:28:48.180 --> 00:28:50.340]   where we can dump the data and understand it.
[00:28:50.340 --> 00:28:53.380]   Explainability is a very interesting point.
[00:28:53.380 --> 00:28:54.820]   We're trying to do more of that
[00:28:54.820 --> 00:28:57.140]   where we're trying to work with a few more tools
[00:28:57.140 --> 00:29:00.420]   that exist out there where not only can we get
[00:29:00.420 --> 00:29:01.860]   some of that meaningful information out,
[00:29:01.860 --> 00:29:03.620]   we can map it to what they understand.
[00:29:03.620 --> 00:29:05.780]   Because as you go up the stack,
[00:29:05.780 --> 00:29:07.460]   different levels of sophistication,
[00:29:07.460 --> 00:29:09.060]   I would say in terms of what we run.
[00:29:09.060 --> 00:29:11.140]   I think the really important part
[00:29:11.140 --> 00:29:14.020]   is feeding that information back into our evaluation
[00:29:14.020 --> 00:29:15.300]   where you started.
[00:29:15.300 --> 00:29:17.140]   If we have a problem with a customer
[00:29:17.140 --> 00:29:18.980]   and the support team is able to identify it,
[00:29:18.980 --> 00:29:21.140]   maybe they pass it to our QA team.
[00:29:21.140 --> 00:29:24.020]   So now the QA team has more sophisticated tools.
[00:29:24.020 --> 00:29:26.980]   They actually do use weights and biases today.
[00:29:26.980 --> 00:29:30.900]   So for example, they can go and check
[00:29:30.900 --> 00:29:33.540]   the machine learning teams last whatever X releases
[00:29:33.540 --> 00:29:35.220]   and all of the results are there.
[00:29:35.220 --> 00:29:37.460]   They can go and run an evaluation by themselves.
[00:29:37.460 --> 00:29:39.140]   We've made it as turnkey as possible.
[00:29:39.140 --> 00:29:41.540]   So the level at which the AI team operates
[00:29:41.540 --> 00:29:43.380]   is different to where the QA team operates
[00:29:43.380 --> 00:29:44.340]   where it's dead simple.
[00:29:44.340 --> 00:29:46.740]   We've put some kind of abstracted UI on top of it
[00:29:46.740 --> 00:29:50.020]   where they can essentially run the same type of evaluation
[00:29:50.020 --> 00:29:51.940]   over the new data that has the problem,
[00:29:51.940 --> 00:29:55.460]   be able to understand, okay, where the problem is happening
[00:29:55.460 --> 00:29:56.900]   and then involve the AI team
[00:29:56.900 --> 00:29:59.860]   where they can jump in and actually do this.
[00:29:59.860 --> 00:30:02.100]   I mean, I can't underplay how big a difference
[00:30:02.100 --> 00:30:05.380]   this has made because initially all of those requests
[00:30:05.380 --> 00:30:07.060]   were coming straight into the AI team
[00:30:07.060 --> 00:30:10.020]   where we're getting overwhelmed with requests
[00:30:10.020 --> 00:30:11.700]   and a lot of it is triage.
[00:30:11.700 --> 00:30:14.740]   I would say like 70% of the time,
[00:30:14.740 --> 00:30:16.900]   I would say on average is triaging
[00:30:16.900 --> 00:30:18.020]   and identifying the problem.
[00:30:18.020 --> 00:30:20.660]   Fixing the problem is typically not too bad
[00:30:20.660 --> 00:30:22.660]   with the exception of problems
[00:30:22.660 --> 00:30:24.580]   where you have gaps in your data
[00:30:24.580 --> 00:30:26.500]   or something more fundamental that you need to fix.
[00:30:26.500 --> 00:30:30.180]   - And what are the like kind of fundamental problems
[00:30:30.180 --> 00:30:30.820]   that you might have?
[00:30:30.820 --> 00:30:32.820]   Like the ones that are really tough to fix?
[00:30:32.820 --> 00:30:35.540]   - I think typically that happens
[00:30:35.540 --> 00:30:38.500]   when our datasets essentially don't have coverage
[00:30:38.500 --> 00:30:42.020]   where you essentially hit a particular environment
[00:30:42.020 --> 00:30:43.060]   or a field of view
[00:30:43.060 --> 00:30:46.580]   where you just don't have the training data in the model
[00:30:46.580 --> 00:30:48.500]   to be able to actually adequately deal with it.
[00:30:48.500 --> 00:30:52.580]   Or actually you might have a new model.
[00:30:52.580 --> 00:30:55.620]   So for example, some of the new models we're working on
[00:30:55.620 --> 00:30:57.380]   specifically focus on identifying
[00:30:57.380 --> 00:30:59.620]   very small objects at distance.
[00:30:59.620 --> 00:31:01.860]   That is a very difficult problem
[00:31:01.860 --> 00:31:03.380]   because it's difficult for a human
[00:31:03.380 --> 00:31:05.060]   and it's difficult for a CNN.
[00:31:05.060 --> 00:31:08.500]   When you try to disambiguate something at 300 meters,
[00:31:08.500 --> 00:31:09.780]   it's basically a patch.
[00:31:09.780 --> 00:31:12.420]   I mean, at that point, you're just doing motion detection.
[00:31:12.420 --> 00:31:14.980]   So you have to think outside the box a little bit
[00:31:14.980 --> 00:31:17.140]   in terms of figuring out what that is.
[00:31:17.140 --> 00:31:19.780]   But typically that's one example
[00:31:19.780 --> 00:31:21.460]   where many of our customers,
[00:31:21.460 --> 00:31:24.100]   they still use AI for perimeter protection.
[00:31:24.100 --> 00:31:26.820]   So object detection at range
[00:31:26.820 --> 00:31:30.500]   is something that is a constant query, I would say,
[00:31:30.500 --> 00:31:33.380]   especially after we moved to deep learning-based analytics.
[00:31:33.380 --> 00:31:36.500]   In some cases, customers think that previous generation
[00:31:36.500 --> 00:31:38.660]   of cascade-based models like worked better
[00:31:38.660 --> 00:31:40.900]   because they don't actually have to do detection.
[00:31:40.900 --> 00:31:43.860]   It's essentially blob identification and motion detection.
[00:31:43.860 --> 00:31:46.660]   So when they lost some of that capability,
[00:31:46.660 --> 00:31:49.060]   they're like, "Well, why isn't the CNN,
[00:31:49.060 --> 00:31:50.980]   why isn't the object detector actually picking this up?"
[00:31:50.980 --> 00:31:52.820]   And you kind of have to explain it to them.
[00:31:52.820 --> 00:31:55.220]   One of the things that we're very proud of today
[00:31:55.220 --> 00:31:56.340]   is we've been able to combine
[00:31:56.340 --> 00:31:57.700]   some of those techniques together
[00:31:57.700 --> 00:31:59.620]   where typically you'll get a detection
[00:31:59.620 --> 00:32:01.380]   that ends up being very low confidence,
[00:32:01.380 --> 00:32:05.060]   where you typically wouldn't pass the threshold for an alarm.
[00:32:05.060 --> 00:32:09.060]   Whereas now for those low confidence detections,
[00:32:09.060 --> 00:32:10.740]   we can, under certain circumstances,
[00:32:10.740 --> 00:32:12.180]   combining different types of metadata,
[00:32:12.180 --> 00:32:13.540]   we can say, "Let's take a second look
[00:32:13.540 --> 00:32:15.460]   using a different technique to be able to say,
[00:32:15.460 --> 00:32:18.100]   is this actually an alarm that a customer might care about?"
[00:32:18.100 --> 00:32:20.500]   To be able to combine those things together.
[00:32:20.500 --> 00:32:22.660]   And I think that's just a larger narrative
[00:32:22.660 --> 00:32:24.020]   around multimodal analytics.
[00:32:24.020 --> 00:32:26.180]   I mean, I think for the most part,
[00:32:26.180 --> 00:32:28.180]   object detection is largely commoditized.
[00:32:28.180 --> 00:32:30.980]   If you look at what startups need to do
[00:32:30.980 --> 00:32:32.900]   to get a viable object detector today,
[00:32:32.900 --> 00:32:35.540]   whether it's using the latest YOLO variant or whatever,
[00:32:35.540 --> 00:32:38.660]   most people can get going pretty quickly.
[00:32:38.660 --> 00:32:40.740]   I think where you end up having issues
[00:32:40.740 --> 00:32:43.700]   is exactly the areas that you've been asking me questions on,
[00:32:43.700 --> 00:32:45.140]   which is the edge cases.
[00:32:45.140 --> 00:32:47.220]   Whether it's extreme range,
[00:32:47.220 --> 00:32:48.420]   certain types of conditions
[00:32:48.420 --> 00:32:50.340]   where you might not have the training data.
[00:32:50.340 --> 00:32:53.540]   I think this is where customers end up having problems.
[00:32:53.540 --> 00:32:58.020]   So to go beyond that, I think this is almost getting to a part,
[00:32:58.020 --> 00:33:00.500]   I won't say exactly, where speech recognition got to,
[00:33:00.500 --> 00:33:03.060]   where it got to good enough very, very quickly,
[00:33:03.060 --> 00:33:06.900]   where essentially gains in training ASR models
[00:33:06.900 --> 00:33:09.860]   typically wasn't worth the exponential effort.
[00:33:10.820 --> 00:33:13.380]   So then everything shifted to natural language.
[00:33:13.380 --> 00:33:15.780]   It's like, okay, well, the transcripts that I'm generating
[00:33:15.780 --> 00:33:16.820]   are pretty good.
[00:33:16.820 --> 00:33:19.940]   Now, how can I do language-based tasks more effectively?
[00:33:19.940 --> 00:33:22.420]   And there's a bunch of NLP work that we're doing in that area.
[00:33:22.420 --> 00:33:24.820]   And I think NLP has become a huge influence
[00:33:24.820 --> 00:33:26.020]   for us to envision as well.
[00:33:26.020 --> 00:33:29.700]   I mean, this past CVPR, for example,
[00:33:29.700 --> 00:33:31.620]   everything was language plus vision.
[00:33:31.620 --> 00:33:34.820]   Whether they're jointly trained models
[00:33:34.820 --> 00:33:36.260]   or separately reasoning,
[00:33:36.260 --> 00:33:39.060]   using language to reason across vision-based models.
[00:33:39.060 --> 00:33:41.700]   This is something that we've been looking at for a while.
[00:33:41.700 --> 00:33:43.780]   So I would say like two big trends
[00:33:43.780 --> 00:33:45.220]   in the computer vision space.
[00:33:45.220 --> 00:33:47.700]   One was unsupervised, semi-supervised learning.
[00:33:47.700 --> 00:33:49.380]   You've seen, you know, Meta and Google
[00:33:49.380 --> 00:33:50.500]   and other companies like that
[00:33:50.500 --> 00:33:53.060]   really show what's possible at extreme scale.
[00:33:53.060 --> 00:33:55.780]   And then secondly is effectively using language,
[00:33:55.780 --> 00:33:57.940]   not only to understand human intent,
[00:33:57.940 --> 00:34:01.140]   but also to interpret what the user is seeing.
[00:34:01.140 --> 00:34:03.780]   And like, this is exactly the question you asked me before,
[00:34:03.780 --> 00:34:06.660]   where when you get an alert today,
[00:34:06.660 --> 00:34:11.540]   that event image pair is not terribly explainable, right?
[00:34:11.540 --> 00:34:13.300]   It's, if you have a lot of training,
[00:34:13.300 --> 00:34:15.300]   you can look at that event and that image and say,
[00:34:15.300 --> 00:34:17.140]   okay, I kind of know what's going on,
[00:34:17.140 --> 00:34:19.060]   but being able to take that result
[00:34:19.060 --> 00:34:22.420]   and in just plain language, explain what's happening,
[00:34:22.420 --> 00:34:24.340]   not only helps us digest it better
[00:34:24.340 --> 00:34:26.100]   from a cognitive bandwidth standpoint,
[00:34:26.100 --> 00:34:28.180]   but it's just way, way better to go.
[00:34:28.180 --> 00:34:29.460]   Yes, I want to capture that.
[00:34:29.460 --> 00:34:31.780]   And I want that alert to happen again.
[00:34:31.780 --> 00:34:34.980]   And I think this is where we're really, really hyper-focused
[00:34:34.980 --> 00:34:38.180]   on using language as the glue to be able to essentially
[00:34:38.180 --> 00:34:40.980]   move away from logic-based rules
[00:34:40.980 --> 00:34:43.140]   and use the way we naturally think about problems
[00:34:43.140 --> 00:34:45.060]   to be able to capture future alerts,
[00:34:45.060 --> 00:34:48.340]   which is also why, I mean, two sides of our business,
[00:34:48.340 --> 00:34:49.620]   you asked about alerting.
[00:34:49.620 --> 00:34:51.860]   The other side is forensic and search.
[00:34:51.860 --> 00:34:55.780]   We truly believe that everything we're doing in search,
[00:34:55.780 --> 00:34:59.220]   which is heavily NLP-based and NLP+ vision-based
[00:34:59.220 --> 00:35:01.540]   can help us bridge the gap to help users
[00:35:01.540 --> 00:35:04.500]   actually create new alerts that they can look for,
[00:35:04.500 --> 00:35:05.620]   proactively.
[00:35:05.620 --> 00:35:05.860]   So, yeah.
[00:35:05.860 --> 00:35:08.020]   I think, sorry, I think you need to give me
[00:35:08.020 --> 00:35:09.380]   like another real-world example
[00:35:09.380 --> 00:35:12.820]   of what this forensic search looks like.
[00:35:12.820 --> 00:35:15.300]   Like, why am I doing this and how does it work?
[00:35:15.300 --> 00:35:17.140]   Okay, so today, forensic capability
[00:35:17.140 --> 00:35:18.340]   is in a video management system.
[00:35:18.340 --> 00:35:21.060]   So leaving aside alerts, now I know something happened.
[00:35:21.060 --> 00:35:22.980]   Now I've got to figure out why it happened
[00:35:22.980 --> 00:35:24.900]   or where a particular person is.
[00:35:24.900 --> 00:35:27.460]   So now I fall back to using my search engine,
[00:35:27.460 --> 00:35:28.420]   essentially, in a video system.
[00:35:28.420 --> 00:35:30.100]   Sorry, I think you need to make this even simpler.
[00:35:30.100 --> 00:35:31.140]   Like, why am I doing this?
[00:35:31.140 --> 00:35:32.980]   Like someone broke into my school
[00:35:32.980 --> 00:35:35.220]   and I want to, why is this hard?
[00:35:35.220 --> 00:35:35.940]   I would think it would just,
[00:35:35.940 --> 00:35:37.380]   you'd sort of look at the video feed
[00:35:37.380 --> 00:35:39.380]   and see what's going on.
[00:35:39.380 --> 00:35:42.820]   I'm sure that's a stupid, naive interpretation, but.
[00:35:42.820 --> 00:35:44.500]   - So a couple of different reasons.
[00:35:44.500 --> 00:35:47.860]   So very, very simple retail use case, loss prevention.
[00:35:47.860 --> 00:35:50.340]   Something has gone missing off the shelf, for example,
[00:35:50.340 --> 00:35:52.100]   or someone stole something.
[00:35:52.100 --> 00:35:53.140]   I know that that happened.
[00:35:53.140 --> 00:35:55.780]   How do I trace it back to figure out who it was?
[00:35:55.780 --> 00:35:56.660]   When did it happen?
[00:35:56.660 --> 00:35:59.140]   For a school, for example,
[00:35:59.140 --> 00:36:01.780]   you know something terrible may be happening
[00:36:01.780 --> 00:36:04.420]   where you're reacting to what happened.
[00:36:04.420 --> 00:36:07.780]   The question is, how do I know where that originated?
[00:36:07.780 --> 00:36:09.620]   How do I make it safer next time
[00:36:09.620 --> 00:36:11.460]   so that it doesn't happen again?
[00:36:11.460 --> 00:36:14.340]   And how do I gather information beyond a single camera?
[00:36:14.340 --> 00:36:16.420]   I think this is the crux of the use case, actually,
[00:36:16.420 --> 00:36:19.220]   where many sites have multiple cameras.
[00:36:19.220 --> 00:36:23.140]   A lot of analytics today focus on single camera events.
[00:36:23.140 --> 00:36:25.780]   So a single camera is going to generate an event for you.
[00:36:25.780 --> 00:36:27.620]   Now the person has moved on.
[00:36:27.620 --> 00:36:29.060]   They're in a different camera.
[00:36:29.060 --> 00:36:31.060]   They're in a different part of the site.
[00:36:31.060 --> 00:36:32.740]   This is where search really helps,
[00:36:32.740 --> 00:36:35.220]   particularly things like similarity-based search,
[00:36:35.220 --> 00:36:38.260]   because now I can use that visual cue of who it was
[00:36:38.260 --> 00:36:40.500]   and search across all my cameras.
[00:36:40.500 --> 00:36:42.100]   This is really where they dip
[00:36:42.100 --> 00:36:43.700]   into the investigative space now,
[00:36:43.700 --> 00:36:46.580]   where they saw something happening on a single camera.
[00:36:46.580 --> 00:36:49.220]   They take what they saw, whether it's a person or vehicle,
[00:36:49.220 --> 00:36:50.580]   enter it into the system.
[00:36:50.580 --> 00:36:53.860]   And now the system will show you occurrences of that person,
[00:36:53.860 --> 00:36:56.100]   that object across many, many different cameras,
[00:36:56.100 --> 00:36:58.340]   where now I can go deeper and understand
[00:36:58.340 --> 00:37:00.020]   where is that person now?
[00:37:00.020 --> 00:37:01.300]   Where was that person?
[00:37:01.300 --> 00:37:03.060]   And where is that person potentially going
[00:37:03.060 --> 00:37:04.580]   so I can get ahead of the situation?
[00:37:04.580 --> 00:37:09.460]   - And am I asking these questions in natural language then?
[00:37:09.460 --> 00:37:11.700]   Is that what the interface looks like?
[00:37:11.700 --> 00:37:15.620]   - So that's where we're focusing a lot of our R&D effort today.
[00:37:15.620 --> 00:37:17.300]   Today, if I had to say, like,
[00:37:17.300 --> 00:37:20.180]   there's two forms you can interrogate a search system.
[00:37:20.180 --> 00:37:22.340]   Visually, so essentially you can give it an image
[00:37:22.340 --> 00:37:25.380]   or an image crop of something, an object of interest,
[00:37:25.380 --> 00:37:26.740]   and systems respond to that.
[00:37:26.740 --> 00:37:28.580]   So essentially we can search the embedding space
[00:37:28.580 --> 00:37:31.060]   to be able to figure out if it's a vehicle
[00:37:31.060 --> 00:37:32.660]   or a person or whatever else.
[00:37:32.660 --> 00:37:34.260]   There is structured search,
[00:37:34.260 --> 00:37:36.180]   where you're looking for a particular attribute.
[00:37:36.180 --> 00:37:38.500]   So I'm forming my query kind of in the form of,
[00:37:38.500 --> 00:37:41.300]   you know, like a man with a green shirt, for example.
[00:37:41.300 --> 00:37:42.660]   What we're doing right now,
[00:37:42.660 --> 00:37:44.260]   and we have been working on for a while,
[00:37:44.260 --> 00:37:45.460]   and you'll start seeing soon,
[00:37:45.460 --> 00:37:47.860]   is we essentially want to make that as easy
[00:37:47.860 --> 00:37:49.700]   as searching for things on the internet,
[00:37:49.700 --> 00:37:52.900]   where you can essentially phrase that in natural language.
[00:37:52.900 --> 00:37:55.220]   We can use that natural language representation
[00:37:55.220 --> 00:37:57.380]   then to do more interesting things in terms of,
[00:37:57.380 --> 00:37:59.540]   you know, being able to bridge what's on the,
[00:37:59.540 --> 00:38:01.380]   in the vision domain with the language domain.
[00:38:01.380 --> 00:38:02.980]   - Wow, that's really cool.
[00:38:02.980 --> 00:38:04.740]   It sounds almost like Star Trek or something.
[00:38:04.740 --> 00:38:08.580]   - But I think on consumer side, it's natural for us, right?
[00:38:08.580 --> 00:38:10.340]   It's funny, like a lot of these verticals,
[00:38:10.340 --> 00:38:12.900]   actually I got similar comments where they're like,
[00:38:12.900 --> 00:38:14.180]   that seems like science fiction,
[00:38:14.180 --> 00:38:16.100]   but if you think about consumer applications,
[00:38:16.100 --> 00:38:18.980]   we are very used to doing that today as humans.
[00:38:18.980 --> 00:38:20.740]   But in a lot of these verticals,
[00:38:20.740 --> 00:38:22.900]   whether it's healthcare or public safety
[00:38:22.900 --> 00:38:26.420]   or enterprise security, that's just not how they do things,
[00:38:26.420 --> 00:38:29.140]   because the systems are just simply not sophisticated enough
[00:38:29.140 --> 00:38:31.300]   to be able to understand human intent
[00:38:31.300 --> 00:38:33.460]   and map human intent to structured data.
[00:38:33.460 --> 00:38:37.940]   One of the big problems actually that we worked on initially
[00:38:37.940 --> 00:38:42.500]   was a lot of our knowledge base lies in relational databases.
[00:38:42.500 --> 00:38:43.700]   So then the question becomes,
[00:38:43.700 --> 00:38:46.180]   how do I bridge what I'm seeing visually
[00:38:46.180 --> 00:38:48.580]   or what I'm expressing in natural language
[00:38:48.580 --> 00:38:50.020]   to structured data?
[00:38:50.020 --> 00:38:52.660]   I mean, there's a ton of very interesting work now
[00:38:52.660 --> 00:38:54.180]   using transformer-based models
[00:38:54.180 --> 00:38:56.420]   to be able to actually figure out
[00:38:56.420 --> 00:38:57.860]   from an indexing standpoint,
[00:38:57.860 --> 00:39:00.340]   how do I actually query those structured data systems
[00:39:00.340 --> 00:39:02.420]   based on naturally what humans are saying?
[00:39:02.420 --> 00:39:05.140]   And we think that's the future.
[00:39:05.140 --> 00:39:07.300]   I think making it easier for users
[00:39:07.300 --> 00:39:08.980]   to get information out of systems
[00:39:08.980 --> 00:39:10.660]   is really the bottleneck today.
[00:39:10.660 --> 00:39:13.060]   And many of the systems are too complex for users
[00:39:13.060 --> 00:39:14.980]   to actually figure out how,
[00:39:14.980 --> 00:39:17.060]   if I have to think which search to use,
[00:39:17.060 --> 00:39:19.140]   I've already lost valuable time.
[00:39:19.140 --> 00:39:21.220]   And in our business, losing valuable time means,
[00:39:21.220 --> 00:39:23.460]   as you said at the start of the conversation,
[00:39:23.460 --> 00:39:24.340]   is a huge problem.
[00:39:24.340 --> 00:39:25.700]   - Well, it's funny.
[00:39:25.700 --> 00:39:28.100]   You know, I feel like I obviously,
[00:39:28.100 --> 00:39:29.220]   when I'm talking to a friend,
[00:39:29.220 --> 00:39:31.140]   I like using natural language,
[00:39:31.140 --> 00:39:34.340]   but when I'm engaging with a computer,
[00:39:34.340 --> 00:39:37.620]   I feel like these natural language interfaces
[00:39:37.620 --> 00:39:39.700]   have kind of gotten a bad reputation over the years
[00:39:39.700 --> 00:39:41.860]   for sort of like over-promising
[00:39:41.860 --> 00:39:43.140]   and then just being sort of frustrating
[00:39:43.140 --> 00:39:44.420]   when it's not doing the thing you want.
[00:39:44.420 --> 00:39:47.780]   You don't know what's the next thing you should do.
[00:39:47.780 --> 00:39:48.980]   I mean, I guess, do you feel like
[00:39:48.980 --> 00:39:52.420]   the natural language understanding technology
[00:39:52.420 --> 00:39:55.620]   has gotten to the point where this is really feasible?
[00:39:55.620 --> 00:39:57.460]   Like, I feel like I don't actually engage,
[00:39:57.460 --> 00:40:02.180]   maybe ever with a question and answering,
[00:40:02.180 --> 00:40:03.700]   an automated question and answering system
[00:40:03.700 --> 00:40:05.060]   that seems to work really well.
[00:40:05.060 --> 00:40:07.380]   - I think that's actually
[00:40:07.380 --> 00:40:08.660]   a really, really good observation.
[00:40:08.660 --> 00:40:10.020]   And I would say, I agree with you.
[00:40:10.020 --> 00:40:11.540]   I mean, first impressions matter, right?
[00:40:11.540 --> 00:40:13.620]   If you use one of the voice assistants
[00:40:13.620 --> 00:40:15.540]   and it doesn't work for you a couple of times,
[00:40:15.540 --> 00:40:16.820]   most people will abandon it
[00:40:16.820 --> 00:40:19.300]   because they just assume that the coverage isn't there.
[00:40:19.300 --> 00:40:21.540]   I still think it's a huge challenge in general
[00:40:21.540 --> 00:40:23.700]   because the language space is so vast
[00:40:23.700 --> 00:40:26.180]   and users can interpret their intents
[00:40:26.180 --> 00:40:27.620]   in so many different ways.
[00:40:27.620 --> 00:40:29.940]   One thing we have to our advantage
[00:40:29.940 --> 00:40:31.540]   in what Motorola does is,
[00:40:31.540 --> 00:40:33.940]   our vocabulary is actually fairly narrow.
[00:40:33.940 --> 00:40:35.700]   If you think about safety and security,
[00:40:35.700 --> 00:40:37.700]   whether it's public safety or enterprise security,
[00:40:37.700 --> 00:40:39.060]   you're generally asking,
[00:40:39.060 --> 00:40:41.220]   you wanna ask the same sort of things.
[00:40:41.220 --> 00:40:42.900]   You know, the five Ws, for example,
[00:40:42.900 --> 00:40:44.980]   like you're looking for a person or a vehicle
[00:40:44.980 --> 00:40:47.060]   and you're describing the attributes.
[00:40:47.060 --> 00:40:50.740]   So I would say that the domain space of intent is narrower,
[00:40:50.740 --> 00:40:51.780]   but it's much deeper.
[00:40:51.780 --> 00:40:54.020]   So you need to perform really, really well
[00:40:54.020 --> 00:40:57.140]   on those very fine grain parts of the intent.
[00:40:57.140 --> 00:41:00.260]   So for us, natural language, actually,
[00:41:00.260 --> 00:41:02.100]   the last couple of years of work that we've been doing
[00:41:02.100 --> 00:41:03.300]   has been very promising
[00:41:03.300 --> 00:41:05.540]   because not only can we constrain our models.
[00:41:05.540 --> 00:41:08.900]   So if you look at a task like captioning, for example,
[00:41:08.900 --> 00:41:11.300]   captioning is a very difficult task to get right.
[00:41:11.300 --> 00:41:13.140]   You need a lot of data to be able to perform
[00:41:13.140 --> 00:41:14.340]   really, really well.
[00:41:14.340 --> 00:41:16.980]   If I think about something like captioning for us,
[00:41:16.980 --> 00:41:19.460]   we can really constrain the space that we're looking for
[00:41:19.460 --> 00:41:21.860]   because we're looking for those same things.
[00:41:21.860 --> 00:41:23.300]   And so we can really double down
[00:41:23.300 --> 00:41:25.060]   on what datasets we're using
[00:41:25.060 --> 00:41:26.420]   and how we train those models
[00:41:26.420 --> 00:41:28.260]   where they can perform really well.
[00:41:28.260 --> 00:41:30.580]   And so that's where I think for us,
[00:41:30.580 --> 00:41:31.780]   language is very promising
[00:41:31.780 --> 00:41:33.860]   because of the type of problem space that we're in.
[00:41:33.860 --> 00:41:35.140]   - That makes sense.
[00:41:35.140 --> 00:41:37.940]   I guess a practical question I have,
[00:41:37.940 --> 00:41:39.540]   given that you're running all these models
[00:41:39.540 --> 00:41:42.340]   on live feeds of information,
[00:41:42.340 --> 00:41:44.420]   like you actually really are kind of running at scale
[00:41:44.420 --> 00:41:46.340]   and probably need really high uptime.
[00:41:46.340 --> 00:41:47.460]   Like how do you actually,
[00:41:47.460 --> 00:41:51.060]   like what's your production environment look like?
[00:41:51.060 --> 00:41:53.060]   Is this another thing where you're using third-party tools
[00:41:53.060 --> 00:41:54.420]   or you've built something yourself?
[00:41:54.420 --> 00:41:59.140]   - So we use, so the DevOps situation gets quite complex,
[00:41:59.140 --> 00:42:00.500]   especially when you're thinking about
[00:42:00.500 --> 00:42:05.140]   data that's running on-premises as well as in the cloud.
[00:42:05.140 --> 00:42:07.300]   I think a lot of the ways we're bridging that
[00:42:07.300 --> 00:42:09.220]   is essentially like I said at the start,
[00:42:09.220 --> 00:42:10.660]   we're using central management.
[00:42:10.660 --> 00:42:13.220]   So a lot of our cloud software runs
[00:42:13.220 --> 00:42:15.700]   pretty much the same way as any other vendor runs at scale
[00:42:15.700 --> 00:42:18.980]   and we have redundancy and failover support for that.
[00:42:18.980 --> 00:42:21.620]   At the edge, it's really about monitoring.
[00:42:21.620 --> 00:42:24.420]   So it's making sure that we have good information
[00:42:24.420 --> 00:42:26.580]   about what our cameras are doing,
[00:42:26.580 --> 00:42:28.100]   the help of those cameras,
[00:42:28.100 --> 00:42:30.420]   being able to get the right metadata
[00:42:30.420 --> 00:42:31.780]   to understand model performance
[00:42:31.780 --> 00:42:33.700]   so that we know when something's going wrong.
[00:42:33.700 --> 00:42:38.660]   So I think we use a couple of different tools today
[00:42:38.660 --> 00:42:41.460]   that we've built because we are dealing with
[00:42:41.460 --> 00:42:44.260]   formats from our own cameras and data
[00:42:44.260 --> 00:42:46.100]   that's highly proprietary.
[00:42:46.100 --> 00:42:49.060]   But I think we're always looking for other tools
[00:42:49.060 --> 00:42:50.820]   where we can essentially centralize
[00:42:50.820 --> 00:42:52.580]   a lot of that monitoring capability
[00:42:52.580 --> 00:42:54.100]   because it is very complex.
[00:42:54.100 --> 00:42:56.020]   You have multiple pieces of hardware and software
[00:42:56.020 --> 00:42:57.060]   running together.
[00:42:57.060 --> 00:42:59.860]   So it's not just my cloud service went down,
[00:42:59.860 --> 00:43:01.620]   it's okay, my camera malfunctioned
[00:43:01.620 --> 00:43:03.060]   and now things aren't working there,
[00:43:03.060 --> 00:43:04.900]   in which case everything downstream
[00:43:04.900 --> 00:43:06.340]   is not gonna work either.
[00:43:06.340 --> 00:43:09.380]   So I would say it's a work in progress
[00:43:09.380 --> 00:43:11.380]   in terms of making sure that we have good coverage
[00:43:11.380 --> 00:43:13.300]   as our solutions become more distributed.
[00:43:13.300 --> 00:43:16.020]   - Are things like data drift,
[00:43:16.020 --> 00:43:19.300]   like real issues for you that you look to detect?
[00:43:19.300 --> 00:43:20.500]   - Absolutely.
[00:43:20.500 --> 00:43:25.060]   Especially I think when it's the first model of its type
[00:43:25.060 --> 00:43:27.060]   or it's a new capability that we release,
[00:43:27.060 --> 00:43:29.700]   we spend a lot of time in-house
[00:43:29.700 --> 00:43:31.780]   being able to test a lot of that stuff
[00:43:31.780 --> 00:43:36.580]   across as big a diverse and comprehensive data set
[00:43:36.580 --> 00:43:37.060]   as possible.
[00:43:37.060 --> 00:43:38.260]   But when it's out in the field,
[00:43:38.260 --> 00:43:41.300]   we start seeing things like data drift happening
[00:43:41.300 --> 00:43:44.100]   where it goes back to the question you asked before,
[00:43:44.100 --> 00:43:45.860]   as we learn from customers,
[00:43:45.860 --> 00:43:48.020]   that's one way we can alleviate that,
[00:43:48.020 --> 00:43:50.100]   which is a customer might have an issue,
[00:43:50.100 --> 00:43:52.660]   we might recognize that being a common issue
[00:43:52.660 --> 00:43:54.340]   where we can address some of those,
[00:43:54.340 --> 00:43:56.900]   but we're also proactively looking at our models
[00:43:56.900 --> 00:43:59.140]   and seeing how can we combat things like data drift.
[00:43:59.140 --> 00:44:02.020]   So for example, things like synthetic data
[00:44:02.020 --> 00:44:05.220]   have become a huge tool for us in certain areas
[00:44:05.220 --> 00:44:08.420]   where we're either unable to collect real data
[00:44:08.420 --> 00:44:10.500]   or there's sensitivity around collecting
[00:44:10.500 --> 00:44:12.660]   that sort of data where we simply don't do it.
[00:44:12.660 --> 00:44:16.020]   How do we augment our models with those gaps that we have?
[00:44:16.020 --> 00:44:18.900]   And we work with a number of companies
[00:44:18.900 --> 00:44:20.660]   on the synthetic data front
[00:44:20.660 --> 00:44:22.980]   and we're doing a lot of that in-house as well
[00:44:22.980 --> 00:44:24.900]   where we're trying to fill some of those gaps
[00:44:24.900 --> 00:44:27.140]   to make our models as generalizable as possible.
[00:44:27.140 --> 00:44:30.020]   But as you know, it's definitely a work in progress
[00:44:30.020 --> 00:44:31.220]   in terms of keeping a handle,
[00:44:31.220 --> 00:44:33.540]   especially as the number of models kind of explode.
[00:44:33.540 --> 00:44:37.460]   - Wow, you're one of the first people that I've talked to,
[00:44:37.460 --> 00:44:41.140]   maybe the Waymo head of research was the other one,
[00:44:41.140 --> 00:44:44.740]   but most people I feel like think of synthetic data
[00:44:44.740 --> 00:44:47.940]   as more of a theoretical thing
[00:44:47.940 --> 00:44:50.420]   that they're sort of working on using in the future.
[00:44:50.420 --> 00:44:51.460]   It's interesting to talk to someone
[00:44:51.460 --> 00:44:53.620]   that's actually using synthetic data today
[00:44:53.620 --> 00:44:56.020]   to improve the models.
[00:44:56.020 --> 00:44:58.580]   I'm curious, I mean, if you want to name any vendors
[00:44:58.580 --> 00:45:01.460]   that are working well for you or techniques that worked well,
[00:45:01.460 --> 00:45:04.260]   I'm sure that would be useful to the people listening.
[00:45:04.260 --> 00:45:05.380]   - I mean, I can mention,
[00:45:06.020 --> 00:45:08.100]   so we worked with a company called AI Riveria,
[00:45:08.100 --> 00:45:11.460]   actually got acquired by Meta not too long ago.
[00:45:11.460 --> 00:45:13.620]   So that was a very public vendor.
[00:45:13.620 --> 00:45:14.420]   There are a couple of others
[00:45:14.420 --> 00:45:15.780]   that we're talking to right now
[00:45:15.780 --> 00:45:17.860]   that I probably can't share the names just yet,
[00:45:17.860 --> 00:45:21.940]   but I think one of the areas, you're right,
[00:45:21.940 --> 00:45:24.660]   there's a lot of, I would say,
[00:45:24.660 --> 00:45:26.580]   like misinformation and misunderstanding
[00:45:26.580 --> 00:45:29.540]   about how synthetic data is useful.
[00:45:29.540 --> 00:45:31.860]   I think there's one camp that believes
[00:45:31.860 --> 00:45:33.860]   that you can use purely synthetic data
[00:45:33.860 --> 00:45:35.220]   to train certain types of models.
[00:45:35.220 --> 00:45:36.020]   And that may be true,
[00:45:36.020 --> 00:45:38.260]   especially certain classification tasks.
[00:45:38.260 --> 00:45:40.020]   You'd benefit a lot from essentially
[00:45:40.020 --> 00:45:43.380]   just purely using synthetic data to cover the domain gaps
[00:45:43.380 --> 00:45:44.580]   that you might have.
[00:45:44.580 --> 00:45:45.860]   I think where it gets trickier
[00:45:45.860 --> 00:45:48.260]   is when you have a non-trivial amount of real data
[00:45:48.260 --> 00:45:51.700]   and you want to be able to augment that with synthetic data.
[00:45:51.700 --> 00:45:53.780]   At that point, it's really funny
[00:45:53.780 --> 00:45:55.700]   because initially we started working with vendors
[00:45:55.700 --> 00:45:57.060]   as dataset providers.
[00:45:57.060 --> 00:45:59.540]   So essentially like you'd work with them,
[00:45:59.540 --> 00:46:02.020]   give requirements and they would deliver a dataset to you
[00:46:02.020 --> 00:46:04.340]   and you do all the training and experimentation in-house.
[00:46:04.340 --> 00:46:05.780]   And then you realize very quickly
[00:46:05.780 --> 00:46:08.100]   that actually you need to do it end to end.
[00:46:08.100 --> 00:46:10.420]   And now you see a lot of companies actually doing that
[00:46:10.420 --> 00:46:13.460]   where some of them are actually also selling tools
[00:46:13.460 --> 00:46:14.820]   for other companies that say,
[00:46:14.820 --> 00:46:16.980]   "Okay, you can generate your data.
[00:46:16.980 --> 00:46:18.660]   These are the knobs that we're going to give you.
[00:46:18.660 --> 00:46:20.180]   And you can retrain your models
[00:46:20.180 --> 00:46:21.940]   and do that kind of in an iterative way."
[00:46:21.940 --> 00:46:23.700]   And that's really where we've landed today,
[00:46:23.700 --> 00:46:26.020]   where you can't really think of synthetic data
[00:46:26.020 --> 00:46:27.860]   as something you get from a vendor.
[00:46:27.860 --> 00:46:29.060]   It really needs to be part
[00:46:29.060 --> 00:46:31.220]   of the machine learning development process.
[00:46:31.220 --> 00:46:32.740]   And for us actually right now,
[00:46:32.740 --> 00:46:34.500]   where synthetic data is the most useful
[00:46:34.500 --> 00:46:36.660]   is testing and evaluation.
[00:46:36.660 --> 00:46:39.060]   Especially if you think about analytics
[00:46:39.060 --> 00:46:41.380]   that go beyond single object
[00:46:41.380 --> 00:46:42.980]   and you're thinking about groups of things,
[00:46:42.980 --> 00:46:45.860]   whether they're groups of cars or people,
[00:46:45.860 --> 00:46:47.460]   this is a very, very difficult thing
[00:46:47.460 --> 00:46:48.900]   to be able to collect data for.
[00:46:48.900 --> 00:46:51.700]   Even more, I won't go into this now,
[00:46:51.700 --> 00:46:53.780]   but like when you think about anomaly detection,
[00:46:53.780 --> 00:46:55.300]   especially of a high dimensional data,
[00:46:55.300 --> 00:46:57.780]   like it becomes extremely difficult to test these things
[00:46:57.780 --> 00:47:00.260]   because these events are so rare to begin with, right?
[00:47:00.260 --> 00:47:03.140]   So you absolutely need to have synthetic data
[00:47:03.140 --> 00:47:04.100]   to be able to do that.
[00:47:04.100 --> 00:47:05.620]   And I think for the most part,
[00:47:05.620 --> 00:47:07.060]   rather than training,
[00:47:07.060 --> 00:47:08.500]   though we've done some of that as well
[00:47:08.500 --> 00:47:10.180]   for certain types of use cases,
[00:47:10.180 --> 00:47:13.460]   particularly sub-classifications,
[00:47:13.460 --> 00:47:15.300]   attribute classification for certain things,
[00:47:15.300 --> 00:47:18.740]   because obviously you basically have infinite ability
[00:47:18.740 --> 00:47:21.940]   to vary things like color and hue and texture
[00:47:21.940 --> 00:47:22.740]   and things like that.
[00:47:22.740 --> 00:47:24.580]   But testing is huge,
[00:47:24.580 --> 00:47:26.420]   especially for things like groups
[00:47:26.420 --> 00:47:28.340]   where you want certain patterns,
[00:47:28.340 --> 00:47:29.860]   you're trying to mimic certain patterns.
[00:47:29.860 --> 00:47:31.380]   We went back to schools.
[00:47:31.380 --> 00:47:32.580]   When people are panicked,
[00:47:32.580 --> 00:47:34.580]   especially when you think about a building
[00:47:34.580 --> 00:47:36.340]   that has entrances and exits,
[00:47:36.340 --> 00:47:39.140]   there are very specific patterns of human motion
[00:47:39.140 --> 00:47:40.500]   that you're not gonna be able to collect.
[00:47:40.500 --> 00:47:41.620]   And hopefully you never will
[00:47:41.620 --> 00:47:44.180]   because those things hopefully don't happen that often.
[00:47:44.180 --> 00:47:45.620]   And working with synthetic data
[00:47:45.620 --> 00:47:48.660]   and essentially incorporating it into our end-to-end pipeline
[00:47:48.660 --> 00:47:49.700]   is what we're doing today
[00:47:49.700 --> 00:47:52.100]   so we can very quickly model out those scenarios.
[00:47:52.100 --> 00:47:53.700]   - Wow.
[00:47:53.700 --> 00:47:55.460]   I mean, it's funny.
[00:47:55.460 --> 00:47:57.460]   I feel like synthetic data companies come to me
[00:47:57.460 --> 00:47:59.060]   for advice all the time.
[00:47:59.060 --> 00:48:02.980]   And I always feel like it'll be very clear
[00:48:02.980 --> 00:48:05.460]   if your synthetic data is working to help a customer
[00:48:05.460 --> 00:48:07.140]   and then you'll have a great business.
[00:48:07.140 --> 00:48:08.740]   But that part seems really hard to do.
[00:48:08.740 --> 00:48:12.260]   I would imagine modeling people in a panic
[00:48:12.260 --> 00:48:14.660]   is probably an unusual use case,
[00:48:14.660 --> 00:48:15.780]   but incredibly important.
[00:48:15.780 --> 00:48:18.820]   And you better get it really right if you're gonna try to--
[00:48:18.820 --> 00:48:20.340]   - I think it's the same thing, actually.
[00:48:20.340 --> 00:48:23.780]   I mean, I get a lot of startups coming to me and saying,
[00:48:23.780 --> 00:48:25.380]   "Hey, we would like to offer this to you.
[00:48:25.380 --> 00:48:28.340]   Especially data startups at this point
[00:48:28.340 --> 00:48:30.180]   and ML ops startups."
[00:48:30.180 --> 00:48:31.140]   Focus, honestly.
[00:48:31.140 --> 00:48:32.660]   And you've seen that if you,
[00:48:32.660 --> 00:48:34.180]   again, going back to the latest CVPR,
[00:48:34.180 --> 00:48:37.140]   there was a huge push on synthetic data at CVPR,
[00:48:37.140 --> 00:48:40.660]   including the release and commitment to a new open dataset,
[00:48:40.660 --> 00:48:42.660]   for example, for synthetic data.
[00:48:42.660 --> 00:48:45.780]   I think the community, especially the academic community,
[00:48:45.780 --> 00:48:48.100]   they just simply don't know what these companies are doing
[00:48:48.100 --> 00:48:49.860]   and where they're focusing in terms of
[00:48:49.860 --> 00:48:53.380]   what outcomes they're looking to enable.
[00:48:53.380 --> 00:48:54.980]   And I would say that's the same advice
[00:48:54.980 --> 00:48:56.820]   I give a lot of the synthetic data companies.
[00:48:56.820 --> 00:48:58.100]   These are my problems.
[00:48:58.100 --> 00:48:58.900]   So for example,
[00:48:58.900 --> 00:49:01.380]   I want to be able to get a lot of data
[00:49:01.380 --> 00:49:02.980]   about human attributes
[00:49:02.980 --> 00:49:05.060]   where I don't want to collect real data.
[00:49:05.060 --> 00:49:08.020]   Can you build photorealistic data
[00:49:08.020 --> 00:49:10.500]   that is good enough for me to be able to train a model,
[00:49:10.500 --> 00:49:11.140]   for example,
[00:49:11.140 --> 00:49:14.500]   or focus on a specific vertical.
[00:49:14.500 --> 00:49:15.780]   Verticals where it's difficult
[00:49:15.780 --> 00:49:17.460]   to be able to collect real data.
[00:49:17.460 --> 00:49:19.380]   And I think that's what we're starting to see.
[00:49:19.380 --> 00:49:21.860]   Like if I look at a few different startups now,
[00:49:21.860 --> 00:49:23.860]   they're really trying to find their niche.
[00:49:23.860 --> 00:49:25.460]   The other part is tooling.
[00:49:25.460 --> 00:49:28.500]   I think this is one area where I pushed very hard initially
[00:49:28.500 --> 00:49:29.300]   when we were looking at,
[00:49:29.300 --> 00:49:31.540]   and they simply weren't ready to share their tools
[00:49:31.540 --> 00:49:33.380]   because they were building it in-house
[00:49:33.380 --> 00:49:35.700]   to be able to generate datasets for their customers.
[00:49:35.700 --> 00:49:37.300]   And I think that is one thing
[00:49:37.300 --> 00:49:40.180]   where if you have a machine learning team,
[00:49:40.180 --> 00:49:41.460]   like you're not outsourcing
[00:49:41.460 --> 00:49:42.660]   your machine learning development,
[00:49:42.660 --> 00:49:44.420]   you're actually doing it in-house.
[00:49:44.420 --> 00:49:46.660]   Those end-to-end tools that you can incorporate
[00:49:46.660 --> 00:49:48.660]   into your machine learning development life cycle
[00:49:48.660 --> 00:49:49.540]   are super important.
[00:49:49.540 --> 00:49:51.380]   That is when I think a lot of companies
[00:49:51.380 --> 00:49:53.620]   will start to see value of things like synthetic data
[00:49:53.620 --> 00:49:57.380]   when they can actually develop, train, and test iteratively
[00:49:57.380 --> 00:49:59.620]   to be able to see how it's helping them.
[00:49:59.620 --> 00:50:02.900]   - I'm kind of curious as a startup founder,
[00:50:02.900 --> 00:50:07.380]   AI Reverie was a customer of ours too.
[00:50:07.380 --> 00:50:09.940]   And so we saw that they got popped by Meta
[00:50:09.940 --> 00:50:11.380]   and congrats to them.
[00:50:11.380 --> 00:50:14.180]   But did that experience make you a little more nervous
[00:50:14.180 --> 00:50:15.300]   about working with startups?
[00:50:15.300 --> 00:50:17.220]   - It's a really good question.
[00:50:17.220 --> 00:50:18.820]   Actually, I think about this all the time
[00:50:18.820 --> 00:50:21.060]   because a lot of the startups that you talk to,
[00:50:21.060 --> 00:50:22.820]   they may be here and then not here
[00:50:22.820 --> 00:50:23.620]   in a couple of months.
[00:50:23.620 --> 00:50:26.500]   And so kind of tying yourself very deeply
[00:50:26.500 --> 00:50:28.900]   to one ends up being problematic.
[00:50:28.900 --> 00:50:30.660]   So I would say just in general,
[00:50:30.660 --> 00:50:33.540]   we like companies or at least our team
[00:50:33.540 --> 00:50:34.580]   and what the company does,
[00:50:34.580 --> 00:50:37.380]   we like companies that focus on platform and tools
[00:50:37.380 --> 00:50:39.140]   and build things in a very modular way.
[00:50:39.140 --> 00:50:41.540]   Because not only does it help us really understand
[00:50:41.540 --> 00:50:43.380]   what value there is in that,
[00:50:43.380 --> 00:50:46.020]   like for example, data visualization, huge problem.
[00:50:46.020 --> 00:50:48.500]   You don't want, like before we had data scientists
[00:50:48.500 --> 00:50:50.820]   building all different types of visualization,
[00:50:51.380 --> 00:50:55.060]   hard to share, hard to have a library of those things
[00:50:55.060 --> 00:50:56.420]   to be able to replicate.
[00:50:56.420 --> 00:50:57.620]   Same things around data.
[00:50:57.620 --> 00:50:58.980]   If we tied ourselves to a company
[00:50:58.980 --> 00:51:00.740]   that was just generating data for us
[00:51:00.740 --> 00:51:01.940]   and then they went away
[00:51:01.940 --> 00:51:03.140]   and we have no idea actually
[00:51:03.140 --> 00:51:05.300]   how to generate that data ourselves,
[00:51:05.300 --> 00:51:06.820]   I think that becomes problematic.
[00:51:06.820 --> 00:51:10.100]   So I think companies that focus on tools and platform
[00:51:10.100 --> 00:51:12.900]   where we understand what makes them great
[00:51:12.900 --> 00:51:15.300]   because they're focusing on a very specific problem.
[00:51:15.300 --> 00:51:17.300]   But we also have the intuition behind
[00:51:17.300 --> 00:51:19.700]   what problem they're solving
[00:51:19.700 --> 00:51:22.100]   so that we can start to invest in it more in-house.
[00:51:22.100 --> 00:51:23.860]   So the synthetic data is a great example.
[00:51:23.860 --> 00:51:26.740]   I don't think it can be a completely outsourced thing.
[00:51:26.740 --> 00:51:29.620]   Companies are gonna go after little slices of the problem.
[00:51:29.620 --> 00:51:31.620]   I think if you're really gonna be all in on it,
[00:51:31.620 --> 00:51:33.460]   you have to invest in tools and technology
[00:51:33.460 --> 00:51:34.340]   on your end as well.
[00:51:34.340 --> 00:51:36.980]   And so I think just as a general rule of thumb,
[00:51:36.980 --> 00:51:38.740]   that's what we try to look at as companies
[00:51:38.740 --> 00:51:41.380]   that are a little bit more open in terms of
[00:51:41.380 --> 00:51:44.180]   how they're building systems
[00:51:44.180 --> 00:51:46.020]   and have a good diversity of customers
[00:51:46.020 --> 00:51:47.380]   so that we're not the only ones
[00:51:47.380 --> 00:51:49.300]   relying on this one capability
[00:51:49.300 --> 00:51:52.100]   so that they will tune the solution
[00:51:52.100 --> 00:51:54.020]   because we are their biggest customer, for example.
[00:51:54.020 --> 00:51:56.100]   That becomes problematic as you know.
[00:51:56.100 --> 00:51:58.580]   - Are there any other kind of common mistakes
[00:51:58.580 --> 00:52:01.780]   that a guy like me makes pitching a guy like you?
[00:52:01.780 --> 00:52:03.540]   Like when startups come to you,
[00:52:03.540 --> 00:52:04.660]   do you have any advice for them
[00:52:04.660 --> 00:52:10.260]   to I guess be a good vendor to Motorola?
[00:52:10.260 --> 00:52:11.780]   - I think honestly,
[00:52:11.780 --> 00:52:13.940]   and it's probably just a pet peeve of mine,
[00:52:13.940 --> 00:52:16.580]   but very few companies actually do any homework
[00:52:16.580 --> 00:52:18.020]   on what we do.
[00:52:18.020 --> 00:52:19.700]   So they're pitching something
[00:52:19.700 --> 00:52:21.780]   which if you just spent 30 seconds
[00:52:21.780 --> 00:52:23.220]   kind of looking at what we're doing,
[00:52:23.220 --> 00:52:25.460]   it probably didn't make sense to pitch it.
[00:52:25.460 --> 00:52:27.060]   And I think the second part is
[00:52:27.060 --> 00:52:30.660]   the volume of pitches are so high right now,
[00:52:30.660 --> 00:52:33.700]   especially in machine learning ops or computer vision
[00:52:33.700 --> 00:52:37.780]   or whatever NLP that usually people like me
[00:52:37.780 --> 00:52:38.580]   who have to look at it,
[00:52:38.580 --> 00:52:40.260]   we have a very small amount of time
[00:52:40.260 --> 00:52:42.100]   to be able to actually make a decision.
[00:52:42.100 --> 00:52:43.780]   And I think when I look at it,
[00:52:43.780 --> 00:52:45.300]   when I try to make decisions,
[00:52:45.300 --> 00:52:46.740]   people is the number one thing.
[00:52:46.740 --> 00:52:48.580]   Like what's the quality of people?
[00:52:48.580 --> 00:52:50.100]   I don't care what problem you're solving.
[00:52:50.100 --> 00:52:52.340]   Like what's the quality of the company?
[00:52:52.340 --> 00:52:53.540]   Where did they come from?
[00:52:53.540 --> 00:52:54.900]   What problems did they solve?
[00:52:54.900 --> 00:52:56.660]   That's for me is number one.
[00:52:56.660 --> 00:52:57.860]   Number two is,
[00:52:57.860 --> 00:52:59.460]   did they take a little bit of time
[00:52:59.460 --> 00:53:01.620]   to pitch me on what they think
[00:53:01.620 --> 00:53:03.380]   is a good use of their technology
[00:53:03.380 --> 00:53:04.900]   for the problems that I'm solving?
[00:53:04.900 --> 00:53:08.500]   I think those two things help me make decisions
[00:53:08.500 --> 00:53:09.540]   relatively quickly.
[00:53:09.540 --> 00:53:12.420]   And I think you can tell,
[00:53:12.420 --> 00:53:14.660]   I would say the founders or the companies who care
[00:53:14.660 --> 00:53:17.380]   when they maybe limit the number of people
[00:53:17.380 --> 00:53:18.580]   that they engage with.
[00:53:18.580 --> 00:53:20.020]   And, but when they do engage,
[00:53:20.020 --> 00:53:21.060]   they've done their homework
[00:53:21.060 --> 00:53:23.540]   and they kind of know that they feel strongly
[00:53:23.540 --> 00:53:25.540]   that what they're building could benefit the company.
[00:53:25.540 --> 00:53:29.140]   And I think, so, I mean, Alex is one example,
[00:53:29.140 --> 00:53:31.220]   Alex Ratner, like I knew of Snorkel.
[00:53:31.220 --> 00:53:33.460]   We'd actually spent a bunch of development time
[00:53:33.460 --> 00:53:34.340]   using Snorkel.
[00:53:34.340 --> 00:53:36.820]   And I think that was a very easy relationship.
[00:53:36.820 --> 00:53:38.580]   I mean, and he himself reached out,
[00:53:38.580 --> 00:53:39.620]   which made it super easy
[00:53:39.620 --> 00:53:41.060]   'cause we were able to dive straight in,
[00:53:41.060 --> 00:53:43.220]   right into what problems are you solving
[00:53:43.220 --> 00:53:44.420]   with your company?
[00:53:44.420 --> 00:53:46.900]   And get engaged with them and say,
[00:53:46.900 --> 00:53:48.900]   okay, now we know what path you're on.
[00:53:48.900 --> 00:53:50.580]   We know that you're someone
[00:53:50.580 --> 00:53:52.740]   we probably want to keep working with at some point.
[00:53:52.740 --> 00:53:54.180]   And so that made it easy.
[00:53:54.180 --> 00:53:55.300]   - Awesome.
[00:53:55.300 --> 00:53:57.220]   Well, I'm sure that's useful advice.
[00:53:57.220 --> 00:54:00.820]   Maybe we should end with our two questions
[00:54:00.820 --> 00:54:01.700]   that we always end with.
[00:54:01.700 --> 00:54:04.740]   The second to last one is,
[00:54:04.740 --> 00:54:07.700]   what's a topic in machine learning
[00:54:07.700 --> 00:54:09.060]   that you think is underrated?
[00:54:09.060 --> 00:54:11.860]   - Oh, that's a tough one
[00:54:11.860 --> 00:54:13.700]   'cause I think so many problems
[00:54:13.700 --> 00:54:16.020]   are kind of, I would say out there.
[00:54:16.020 --> 00:54:18.100]   I still am a very, very strong believer
[00:54:18.100 --> 00:54:20.020]   in multitask learning and meta learning.
[00:54:20.020 --> 00:54:23.860]   I think you've seen the kind of academic community
[00:54:23.860 --> 00:54:24.740]   go in that direction,
[00:54:24.740 --> 00:54:26.740]   but now we're starting to see real results.
[00:54:26.740 --> 00:54:28.340]   I mean, I'll point out one thing
[00:54:28.340 --> 00:54:29.700]   that it's not so recent,
[00:54:29.700 --> 00:54:31.220]   but came out of meta again,
[00:54:31.220 --> 00:54:32.740]   which was Groknet,
[00:54:32.740 --> 00:54:35.060]   which is essentially using multitask learning
[00:54:35.060 --> 00:54:37.220]   to be able to basically do
[00:54:37.220 --> 00:54:38.740]   very accurate product recognition.
[00:54:38.740 --> 00:54:39.700]   Now we don't do any of that.
[00:54:39.700 --> 00:54:41.700]   We're not an e-commerce company at all,
[00:54:41.700 --> 00:54:43.140]   but one lesson there,
[00:54:43.140 --> 00:54:44.740]   at least that I learned was
[00:54:44.740 --> 00:54:47.780]   being able to have a single model
[00:54:47.780 --> 00:54:50.900]   that does well across a variety of classification tasks
[00:54:50.900 --> 00:54:52.820]   and is trained and optimized jointly
[00:54:52.820 --> 00:54:55.860]   is something that is very important.
[00:54:55.860 --> 00:54:58.340]   And it used to always be that you choose
[00:54:58.340 --> 00:55:00.420]   a particular loss function that you'd care about.
[00:55:00.420 --> 00:55:03.060]   Now you use a multiplicity of different loss functions,
[00:55:03.060 --> 00:55:04.500]   some which were not even intended
[00:55:04.500 --> 00:55:05.860]   for that particular task.
[00:55:05.860 --> 00:55:07.780]   So for example, Groknet uses ArcFace,
[00:55:07.780 --> 00:55:12.020]   which was developed for face recognition,
[00:55:12.020 --> 00:55:13.860]   but they're not using with anything
[00:55:13.860 --> 00:55:14.980]   to do with face recognition.
[00:55:14.980 --> 00:55:16.100]   They're essentially using that
[00:55:16.100 --> 00:55:17.940]   to be able to find the cosine similarity
[00:55:17.940 --> 00:55:20.500]   between different embeddings in a very varied space.
[00:55:20.500 --> 00:55:21.700]   We do the same thing.
[00:55:21.700 --> 00:55:24.020]   We started out having N different models.
[00:55:24.020 --> 00:55:27.140]   We want to get it down to some N minus X amount,
[00:55:27.140 --> 00:55:29.220]   again, to the point of managing different models.
[00:55:29.220 --> 00:55:30.820]   So I would say multitask learning
[00:55:30.820 --> 00:55:32.580]   and meta learning, I think is still,
[00:55:32.580 --> 00:55:33.940]   people think it's science fiction
[00:55:33.940 --> 00:55:37.060]   because I think academia-wise,
[00:55:37.060 --> 00:55:38.100]   a lot of people look at that and go,
[00:55:38.100 --> 00:55:39.780]   "I'll come back to it in three or five years
[00:55:39.780 --> 00:55:41.060]   when it's kind of ready to use."
[00:55:41.060 --> 00:55:42.900]   But I think picking your spots,
[00:55:42.900 --> 00:55:43.940]   I think this is one area
[00:55:43.940 --> 00:55:47.140]   where I think it's grossly underrated.
[00:55:47.140 --> 00:55:49.380]   The second I would say is user experience.
[00:55:49.380 --> 00:55:50.180]   I didn't talk about it,
[00:55:50.180 --> 00:55:52.420]   but in addition to the artificial intelligence team,
[00:55:52.420 --> 00:55:54.260]   I lead the user experience research
[00:55:54.260 --> 00:55:55.620]   and design team here at Motorola.
[00:55:55.620 --> 00:55:57.060]   And I think those two things
[00:55:57.060 --> 00:55:58.740]   are critically essential to each other.
[00:55:58.740 --> 00:56:00.820]   It used to be that we would just develop algorithms
[00:56:00.820 --> 00:56:03.140]   in a vacuum, then go to the designers and go,
[00:56:03.140 --> 00:56:05.540]   "Hey, can you help me design some software around it?"
[00:56:05.540 --> 00:56:06.900]   And I think we don't do that anymore.
[00:56:06.900 --> 00:56:08.740]   We start with a human problem.
[00:56:08.740 --> 00:56:10.340]   We try to design the experience
[00:56:10.340 --> 00:56:11.300]   and then we try to figure out
[00:56:11.300 --> 00:56:14.020]   how the model can actually fit in that workflow.
[00:56:14.020 --> 00:56:16.180]   And I think any machine learning company
[00:56:16.180 --> 00:56:18.260]   should really, really consider that,
[00:56:18.260 --> 00:56:21.460]   especially when you go pitch your solutions to someone
[00:56:21.460 --> 00:56:23.540]   and you're still trying to explain it to them
[00:56:23.540 --> 00:56:25.140]   after 30 minutes, I think then,
[00:56:25.140 --> 00:56:27.060]   you probably need to tackle that.
[00:56:27.060 --> 00:56:29.140]   So I would say those are the two factors,
[00:56:29.140 --> 00:56:29.860]   for me at least.
[00:56:29.860 --> 00:56:33.220]   - Yeah, we hear that user experience thing
[00:56:33.220 --> 00:56:34.100]   over and over and over.
[00:56:34.100 --> 00:56:38.020]   It's interesting how there's a lot of movement
[00:56:38.020 --> 00:56:39.620]   back and forth between ML leaders
[00:56:39.620 --> 00:56:40.580]   and product leaders, I think,
[00:56:40.580 --> 00:56:41.620]   which is super cool.
[00:56:41.620 --> 00:56:42.180]   - Yeah.
[00:56:42.180 --> 00:56:46.580]   - I guess my final question is,
[00:56:46.580 --> 00:56:52.100]   when you look at speccing an ML application
[00:56:52.100 --> 00:56:54.820]   to deploy live in production,
[00:56:54.820 --> 00:56:56.580]   where do you see the biggest bottleneck is?
[00:56:56.580 --> 00:56:59.940]   What's the hardest part about getting a new model
[00:56:59.940 --> 00:57:00.660]   into production?
[00:57:00.660 --> 00:57:03.220]   - Yeah, so I'll answer that specifically
[00:57:03.220 --> 00:57:04.580]   on the Motorola solution side,
[00:57:04.580 --> 00:57:06.020]   'cause that's probably what your customers
[00:57:06.020 --> 00:57:07.220]   are interested in.
[00:57:07.220 --> 00:57:09.940]   So for us, especially if it involves edge hardware,
[00:57:09.940 --> 00:57:12.420]   the complexity is, as you know,
[00:57:12.420 --> 00:57:14.340]   synchronizing software release cycle
[00:57:14.340 --> 00:57:15.700]   with a hardware release cycle.
[00:57:15.700 --> 00:57:17.380]   That is difficult 'cause you have deadlines,
[00:57:17.380 --> 00:57:19.140]   you have supply chain issues and things like that.
[00:57:19.140 --> 00:57:20.980]   So having to do that.
[00:57:20.980 --> 00:57:23.700]   The second part is, I would say the easy part
[00:57:23.700 --> 00:57:26.180]   is getting a viable model out of research,
[00:57:26.180 --> 00:57:28.180]   if you will, out of R&D.
[00:57:28.180 --> 00:57:29.940]   Like we are very well equipped to do that.
[00:57:29.940 --> 00:57:31.380]   We have great tools,
[00:57:31.380 --> 00:57:34.580]   increasingly our training infrastructure is automated.
[00:57:34.580 --> 00:57:35.940]   We do training in the cloud.
[00:57:35.940 --> 00:57:37.780]   We have on-prem compute
[00:57:37.780 --> 00:57:41.220]   through our distributed training methodologies.
[00:57:41.220 --> 00:57:43.140]   The problem is once we have a viable model,
[00:57:43.140 --> 00:57:45.700]   and typically there was a framework issue before,
[00:57:45.700 --> 00:57:48.980]   whereas now I think we use interchange formats like ONNX,
[00:57:48.980 --> 00:57:50.100]   we can get a model out
[00:57:50.100 --> 00:57:51.940]   that is somewhat framework independent.
[00:57:51.940 --> 00:57:54.500]   Second is how do I optimize for the platform?
[00:57:54.500 --> 00:57:56.180]   If it's NVIDIA, I might have to use something
[00:57:56.180 --> 00:57:57.140]   like TensorRT.
[00:57:57.140 --> 00:57:58.660]   If I'm using an AI SoC,
[00:57:58.660 --> 00:58:01.140]   I need to be able to use that company's tools
[00:58:01.140 --> 00:58:03.460]   to be able to not only optimize the model,
[00:58:03.460 --> 00:58:05.060]   do post-training quantization,
[00:58:05.060 --> 00:58:05.860]   which is not trivial.
[00:58:05.860 --> 00:58:06.820]   Now you've got to see,
[00:58:06.820 --> 00:58:09.060]   did I lose anything in terms of my accuracy?
[00:58:09.060 --> 00:58:11.700]   So getting a model that looks good
[00:58:11.700 --> 00:58:13.940]   on as much data as we have,
[00:58:13.940 --> 00:58:16.420]   then optimizing it for a particular platform,
[00:58:16.420 --> 00:58:18.980]   that part is complex
[00:58:18.980 --> 00:58:20.420]   because we have to deal with
[00:58:20.420 --> 00:58:22.260]   a bunch of different platforms.
[00:58:22.260 --> 00:58:23.860]   I think once we've got there,
[00:58:23.860 --> 00:58:25.460]   I think the question of,
[00:58:25.460 --> 00:58:26.580]   is it good enough?
[00:58:26.580 --> 00:58:28.420]   This is something that machine learning teams
[00:58:28.420 --> 00:58:29.300]   struggle with a lot.
[00:58:29.300 --> 00:58:31.460]   And I think if you distribute that task
[00:58:31.460 --> 00:58:33.780]   between like a QA team or a test team,
[00:58:33.780 --> 00:58:35.140]   for example, and the AI team,
[00:58:35.780 --> 00:58:37.300]   there are very big differences
[00:58:37.300 --> 00:58:39.060]   in opinion on what might be good enough.
[00:58:39.060 --> 00:58:40.500]   You might go do field testing
[00:58:40.500 --> 00:58:42.740]   and you might test two particular scenes,
[00:58:42.740 --> 00:58:44.420]   only two fields of view,
[00:58:44.420 --> 00:58:46.660]   and say, the model is doing terribly here.
[00:58:46.660 --> 00:58:48.980]   Machine learning team will come back and say,
[00:58:48.980 --> 00:58:51.060]   well, our data set is way, way bigger than that.
[00:58:51.060 --> 00:58:52.740]   We trained on like a million images
[00:58:52.740 --> 00:58:53.860]   across many different scenes.
[00:58:53.860 --> 00:58:55.700]   And we think in general, it performs well.
[00:58:55.700 --> 00:58:58.260]   How are you going to do that
[00:58:58.260 --> 00:58:59.940]   when it's statistically insignificant
[00:58:59.940 --> 00:59:01.140]   on the manual testing side?
[00:59:01.140 --> 00:59:03.860]   So I would say optimization and testing,
[00:59:03.860 --> 00:59:06.100]   especially if you're trying to get these things out
[00:59:06.100 --> 00:59:07.460]   across multiple platforms.
[00:59:07.460 --> 00:59:09.460]   - And I guess fixing the problems
[00:59:09.460 --> 00:59:11.300]   with the real world tests on hard also.
[00:59:11.300 --> 00:59:12.900]   - Indeed it is.
[00:59:12.900 --> 00:59:15.060]   It's the first, I mean, finding candidate sites,
[00:59:15.060 --> 00:59:16.500]   are you doing it the right way?
[00:59:16.500 --> 00:59:18.260]   And how do you scale that again?
[00:59:18.260 --> 00:59:19.540]   Which is why we're trying to use things
[00:59:19.540 --> 00:59:21.620]   like synthetic data a little bit more effectively.
[00:59:21.620 --> 00:59:24.420]   And one change we made was our AI data team
[00:59:24.420 --> 00:59:26.580]   originally only served our machine learning team.
[00:59:26.580 --> 00:59:29.940]   Now the AI data team also serves our platform team
[00:59:29.940 --> 00:59:31.460]   and our test team as well,
[00:59:31.460 --> 00:59:33.860]   which has started to get bridged that gap a little bit
[00:59:33.860 --> 00:59:35.380]   in terms of test coverage.
[00:59:35.380 --> 00:59:36.820]   - Awesome.
[00:59:36.820 --> 00:59:37.780]   Well, thanks so much for your time.
[00:59:37.780 --> 00:59:38.580]   This is really fun.
[00:59:38.580 --> 00:59:41.220]   - Oh, thanks because I really appreciated it.
[00:59:41.220 --> 00:59:42.100]   Nice conversation.
[00:59:42.100 --> 00:59:43.220]   - Yeah, thank you.
[00:59:44.180 --> 00:59:45.860]   - If you're enjoying these interviews
[00:59:45.860 --> 00:59:47.620]   and you want to learn more,
[00:59:47.620 --> 00:59:49.700]   please click on the link to the show notes
[00:59:49.700 --> 00:59:52.820]   in the description where you can find links
[00:59:52.820 --> 00:59:54.420]   to all the papers that are mentioned,
[00:59:54.420 --> 00:59:56.820]   supplemental material, and a transcription
[00:59:56.820 --> 00:59:58.340]   that we work really hard to produce.
[00:59:58.340 --> 00:59:59.220]   So check it out.
[00:59:59.220 --> 01:00:01.480]   out.

