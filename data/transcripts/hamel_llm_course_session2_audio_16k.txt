
[00:00:00.000 --> 00:00:02.240]   and your questions with Wing.
[00:00:02.240 --> 00:00:06.640]   And then we will have some time for Zach
[00:00:06.640 --> 00:00:11.360]   to share about parallelism and Hugging Face Accelerate.
[00:00:11.360 --> 00:00:15.840]   Very quick run-through of fine-tuning on Modal.
[00:00:15.840 --> 00:00:19.760]   And we'll have a little bit of time at the end of this for Q&A.
[00:00:19.760 --> 00:00:23.120]   So with all that said, I'm going to get started.
[00:00:23.120 --> 00:00:25.240]   The most frequent question that I
[00:00:25.240 --> 00:00:28.400]   get from people when they're first starting to fine-tune
[00:00:28.400 --> 00:00:32.120]   is they're really related to--
[00:00:32.120 --> 00:00:34.960]   I'm going to call it model capacity, which is how much are
[00:00:34.960 --> 00:00:36.440]   we going to be able to learn.
[00:00:36.440 --> 00:00:40.200]   The two parts of that are, what model should I fine-tune off of?
[00:00:40.200 --> 00:00:43.320]   And then the question, which is simultaneously more technical,
[00:00:43.320 --> 00:00:44.760]   but I think has an easier answer,
[00:00:44.760 --> 00:00:46.920]   because the answer is almost always the same,
[00:00:46.920 --> 00:00:50.960]   which is, should I use LoRa, or should I do a full fine-tune?
[00:00:50.960 --> 00:00:54.440]   I'm going to give a shorter answer to the base model,
[00:00:54.440 --> 00:00:56.720]   and then I'll walk you through what
[00:00:56.720 --> 00:00:58.280]   it means to fine-tune with LoRa.
[00:00:58.280 --> 00:01:01.800]   But then I think the answer there,
[00:01:01.800 --> 00:01:03.640]   despite it being useful to understand LoRa,
[00:01:03.640 --> 00:01:05.240]   because you're going to use it a lot,
[00:01:05.240 --> 00:01:07.120]   you should almost always, in my opinion,
[00:01:07.120 --> 00:01:09.120]   be using LoRa rather than full fine-tunes.
[00:01:09.120 --> 00:01:13.800]   But the first part of this is, what base model do you use?
[00:01:13.800 --> 00:01:16.080]   So there are two dimensions to this.
[00:01:16.080 --> 00:01:18.600]   So one is, what model size?
[00:01:18.600 --> 00:01:21.560]   Do I use a 7 billion, or 13, or 70 billion,
[00:01:21.560 --> 00:01:24.920]   or some other size parameter model?
[00:01:24.920 --> 00:01:29.800]   And then the second is, what model family do I use?
[00:01:29.800 --> 00:01:35.400]   So do I use Lama 2, Lama 3, Mistral, Zephyr, Gemma,
[00:01:35.400 --> 00:01:37.120]   whatever else?
[00:01:37.120 --> 00:01:40.640]   On the model size, I think different people
[00:01:40.640 --> 00:01:44.120]   will have different experiences.
[00:01:44.120 --> 00:01:49.280]   I have almost-- I've never fine-tuned a 70 billion
[00:01:49.280 --> 00:01:51.120]   parameter model, and it's not that we can't.
[00:01:51.120 --> 00:01:54.200]   It's actually with-- thanks to Axolotl and Accelerate,
[00:01:54.200 --> 00:01:59.120]   it's not so, so difficult. But I've fine-tuned 7 billion
[00:01:59.120 --> 00:02:00.640]   and 13 billion parameter models.
[00:02:00.640 --> 00:02:03.360]   I think most of the use cases I have,
[00:02:03.360 --> 00:02:05.360]   the breadth of what we are asking the model to do
[00:02:05.360 --> 00:02:07.200]   is not so, so wide.
[00:02:07.200 --> 00:02:11.240]   And so my experience has been that fine-tuning a 7 billion
[00:02:11.240 --> 00:02:12.800]   parameter model versus 13--
[00:02:12.800 --> 00:02:15.760]   actually, the 7 billion parameter model,
[00:02:15.760 --> 00:02:19.920]   the output quality of these for the projects I've worked on
[00:02:19.920 --> 00:02:22.880]   has been close enough that I never felt the need
[00:02:22.880 --> 00:02:26.680]   to deal with the parallelism required
[00:02:26.680 --> 00:02:28.280]   for much larger models.
[00:02:28.280 --> 00:02:32.000]   So I typically ended up using just 7 billion parameter
[00:02:32.000 --> 00:02:32.800]   models.
[00:02:32.800 --> 00:02:34.040]   Those are a little bit faster.
[00:02:34.040 --> 00:02:37.200]   It's a little bit easier to get a GPU that those run on.
[00:02:37.200 --> 00:02:39.280]   And if you look at the download counts,
[00:02:39.280 --> 00:02:41.960]   this is not a perfect proxy for what others are doing,
[00:02:41.960 --> 00:02:44.400]   but it's some proxy for what others are doing.
[00:02:44.400 --> 00:02:47.680]   And you do see that 7 billion parameter
[00:02:47.680 --> 00:02:51.360]   models are the most popular.
[00:02:51.360 --> 00:02:53.200]   And these are not instruction-tuned models.
[00:02:53.200 --> 00:02:56.080]   So these are models that people are typically
[00:02:56.080 --> 00:02:56.880]   fine-tuning off of.
[00:02:56.880 --> 00:03:00.720]   And you see that the 7 billion parameter model
[00:03:00.720 --> 00:03:04.400]   is the most popular.
[00:03:04.400 --> 00:03:06.320]   And then for people who want to know just
[00:03:06.320 --> 00:03:12.480]   what is fine-tuning, I covered that in some depth
[00:03:12.480 --> 00:03:14.160]   in the first lesson.
[00:03:14.160 --> 00:03:18.000]   So you can go back to that.
[00:03:18.000 --> 00:03:24.560]   Then the second question is, which model family do I use?
[00:03:24.560 --> 00:03:27.440]   This is one where, again, thanks to the way
[00:03:27.440 --> 00:03:29.680]   that it's been abstracted from Axolotl,
[00:03:29.680 --> 00:03:34.080]   it is extremely easy to try different models,
[00:03:34.080 --> 00:03:37.360]   especially if they all fit on the same GPU,
[00:03:37.360 --> 00:03:40.480]   or even if you have to boot up a new instance.
[00:03:40.480 --> 00:03:41.800]   That's also not so, so hard.
[00:03:41.800 --> 00:03:44.600]   But it's extremely easy to try different models
[00:03:44.600 --> 00:03:48.400]   and just do a vibes check.
[00:03:48.400 --> 00:03:51.480]   I tend to just do whatever is fashionable.
[00:03:51.480 --> 00:03:56.440]   So a recently-released model is Llama3.
[00:03:56.440 --> 00:03:59.120]   And if I were starting with something today,
[00:03:59.120 --> 00:04:00.840]   I would just use Llama3, not because I've
[00:04:00.840 --> 00:04:03.680]   thought about it in incredible, incredible depth,
[00:04:03.680 --> 00:04:07.320]   but rather because it's just a newly-released model that's
[00:04:07.320 --> 00:04:10.120]   widely known to be reasonably good.
[00:04:10.120 --> 00:04:12.240]   If you want to find out what's fashionable,
[00:04:12.240 --> 00:04:16.280]   there are many places to find that out.
[00:04:16.280 --> 00:04:17.480]   You could go to HuggingFace.
[00:04:17.480 --> 00:04:20.800]   And then for models, there's a way to sort by hotness
[00:04:20.800 --> 00:04:23.040]   and just see what's hot.
[00:04:23.040 --> 00:04:25.960]   The LocalLlama subreddit is a community
[00:04:25.960 --> 00:04:29.640]   of people who think about these things a lot.
[00:04:29.640 --> 00:04:33.480]   And that's a good place to look at and just--
[00:04:33.480 --> 00:04:35.840]   for running models, though it has "local" in the name.
[00:04:35.840 --> 00:04:37.000]   They spend a lot of time just thinking
[00:04:37.000 --> 00:04:39.280]   about different models and how they behave differently.
[00:04:39.280 --> 00:04:42.360]   So LocalLlama is another community
[00:04:42.360 --> 00:04:48.960]   to look up if you want to choose a model.
[00:04:48.960 --> 00:04:53.280]   But I think people over-index on this,
[00:04:53.280 --> 00:04:57.240]   and that if you run a couple of models that are just
[00:04:57.240 --> 00:05:04.080]   the most popular models at the time, that should be good
[00:05:04.080 --> 00:05:07.080]   enough, and you will probably improve on that immensely
[00:05:07.080 --> 00:05:09.880]   by trying many more models.
[00:05:09.880 --> 00:05:15.280]   And I'll talk in a couple of slides about why that is.
[00:05:15.280 --> 00:05:20.240]   The second problem, lower-reversible fine-tuning,
[00:05:20.240 --> 00:05:24.480]   is a question of when you fine-tune the model,
[00:05:24.480 --> 00:05:27.680]   are you going to--
[00:05:27.680 --> 00:05:29.320]   let me start with an image.
[00:05:29.320 --> 00:05:32.560]   So if we imagine that we've got one layer that goes
[00:05:32.560 --> 00:05:35.960]   from an input to the output, I'm going to, for a second,
[00:05:35.960 --> 00:05:38.760]   actually simplify the transformer architecture
[00:05:38.760 --> 00:05:41.880]   so we don't think about a query matrix and keys and values.
[00:05:41.880 --> 00:05:45.280]   And imagine this almost is just, for the moment,
[00:05:45.280 --> 00:05:46.280]   a feed-forward network.
[00:05:46.280 --> 00:05:50.520]   So you've just got one layer that we're going to look at,
[00:05:50.520 --> 00:05:54.520]   and it's going to take an input that is really
[00:05:54.520 --> 00:05:56.440]   an embedding of the meaning of the text up
[00:05:56.440 --> 00:05:58.880]   to that point in the string, and it's
[00:05:58.880 --> 00:06:03.960]   going to output another vector representation.
[00:06:03.960 --> 00:06:06.880]   In most of these models, the inputs and outputs
[00:06:06.880 --> 00:06:10.800]   are somewhere on the order of 4,000 dimensions.
[00:06:10.800 --> 00:06:16.240]   And so just for that one layer, you'd
[00:06:16.240 --> 00:06:19.360]   have 4,000-dimensional input, 4,000-dimensional output,
[00:06:19.360 --> 00:06:22.240]   so that matrix would be 4,000 by 4,000.
[00:06:22.240 --> 00:06:25.160]   That would be 16 million weights.
[00:06:25.160 --> 00:06:28.320]   And the idea behind LoRa is that we
[00:06:28.320 --> 00:06:33.880]   can learn something that you can add to that original matrix
[00:06:33.880 --> 00:06:39.840]   that is much lower dimensional and that will still
[00:06:39.840 --> 00:06:41.760]   change the behavior in a similar way,
[00:06:41.760 --> 00:06:44.680]   but will have many fewer weights.
[00:06:44.680 --> 00:06:51.960]   And as a result, it can be fine-tuned on less GPU
[00:06:51.960 --> 00:06:52.960]   with less RAM.
[00:06:52.960 --> 00:06:58.360]   And I think it's safe to say that the vast, vast majority
[00:06:58.360 --> 00:07:00.200]   of fine-tuning that happens is either LoRa,
[00:07:00.200 --> 00:07:03.240]   or I'll talk about QLoRa, which is going to work functionally
[00:07:03.240 --> 00:07:04.720]   in a similar way.
[00:07:04.720 --> 00:07:06.520]   But the vast majority that happens is LoRa.
[00:07:06.520 --> 00:07:11.440]   And I think for everyone in this course,
[00:07:11.440 --> 00:07:12.840]   you should use LoRa for a while.
[00:07:12.840 --> 00:07:14.840]   And maybe someday you'll do a full fine-tune.
[00:07:14.840 --> 00:07:19.120]   But you, as a practitioner, may never need full fine-tunes.
[00:07:19.120 --> 00:07:22.520]   There are some theoretical reasons that full fine-tunes,
[00:07:22.520 --> 00:07:26.480]   if you have a lot of data, could be higher performance.
[00:07:26.480 --> 00:07:29.600]   And Zach or Wing or Hamill can contradict me here.
[00:07:29.600 --> 00:07:34.160]   But I think for most people, LoRa is all you need.
[00:07:34.160 --> 00:07:38.640]   Unless you guys want to jump in and correct me,
[00:07:38.640 --> 00:07:42.160]   I'm going to say a bit about just how LoRa works.
[00:07:42.160 --> 00:07:48.560]   So we want to make some changes to a 4,000 by 4,000 matrix,
[00:07:48.560 --> 00:07:53.240]   which is the original weights.
[00:07:53.240 --> 00:07:58.000]   And we do that by having two matrices that we're
[00:07:58.000 --> 00:07:59.960]   going to multiply together.
[00:07:59.960 --> 00:08:02.000]   Those of you who remember your linear algebra
[00:08:02.000 --> 00:08:06.960]   will know that if you have a 4,000 by 16 matrix times a 16
[00:08:06.960 --> 00:08:10.400]   by 4,000 matrix, that is 4,000 by 4,000.
[00:08:10.400 --> 00:08:13.160]   So if we multiply these two pieces together,
[00:08:13.160 --> 00:08:16.080]   that is going to create a new matrix
[00:08:16.080 --> 00:08:18.560]   that we can add to the original weights.
[00:08:18.560 --> 00:08:21.920]   So it can change the original weights quite a bit.
[00:08:21.920 --> 00:08:25.440]   But the number of parameters that are required here--
[00:08:25.440 --> 00:08:28.880]   so each of these, this one is 4,000 by 16.
[00:08:28.880 --> 00:08:30.960]   And this one is 16 by 4,000.
[00:08:30.960 --> 00:08:33.120]   So if we said, how many parameters is that?
[00:08:33.120 --> 00:08:39.400]   That's each of these two matrices on the right
[00:08:39.400 --> 00:08:43.960]   is 16 by 4,000 as the number of parameters.
[00:08:43.960 --> 00:08:45.360]   You have two of those.
[00:08:45.360 --> 00:08:48.000]   So now we have 128,000 weights that we
[00:08:48.000 --> 00:08:52.640]   are going to need to fit when we're fine tuning.
[00:08:52.640 --> 00:08:54.200]   That's a lot less than 16 million.
[00:08:54.200 --> 00:08:59.160]   And as a result, it just requires a lot less RAM.
[00:08:59.160 --> 00:09:04.400]   And GPU VRAM is frequently a binding constraint
[00:09:04.400 --> 00:09:05.920]   as we train our models.
[00:09:05.920 --> 00:09:09.720]   And as a result, it's nice to be able to reduce that RAM
[00:09:09.720 --> 00:09:12.080]   usage by using LoRa.
[00:09:12.080 --> 00:09:17.160]   And you'll see that it's just a configuration flag.
[00:09:17.160 --> 00:09:27.840]   So it's quite easy to do this in Axolotl.
[00:09:27.840 --> 00:09:31.680]   The other piece, which is, I think,
[00:09:31.680 --> 00:09:35.440]   conceptually also somewhat complex to understand well,
[00:09:35.440 --> 00:09:42.440]   but extremely easy to use, is going from LoRa to QLoRa.
[00:09:42.440 --> 00:09:45.800]   So here we had each of these matrices.
[00:09:45.800 --> 00:09:50.800]   And those are just numbers, or each element in those
[00:09:50.800 --> 00:09:51.360]   is numbers.
[00:09:51.360 --> 00:09:57.160]   Numbers are stored in computers with a number of bits.
[00:09:57.160 --> 00:10:00.200]   And if you store it with many, many bits,
[00:10:00.200 --> 00:10:02.880]   then you get very fine gradations
[00:10:02.880 --> 00:10:04.320]   of what that number can be.
[00:10:04.320 --> 00:10:11.280]   So you can go 2 to 2.0000001 and 2.0000002 and so on.
[00:10:11.280 --> 00:10:14.520]   So we tend to think of those almost as being continuous.
[00:10:14.520 --> 00:10:21.480]   QLoRa is dividing the possible values for numbers
[00:10:21.480 --> 00:10:26.400]   into a smaller set of values.
[00:10:26.400 --> 00:10:28.760]   So for instance, if you start with something
[00:10:28.760 --> 00:10:30.240]   that is stored in 16 bits, you can
[00:10:30.240 --> 00:10:32.600]   think of that as almost continuous.
[00:10:32.600 --> 00:10:35.360]   If the lowest value that you want to be able to store
[00:10:35.360 --> 00:10:39.240]   is minus 2, and the highest is, just to pick a number, 2.4,
[00:10:39.240 --> 00:10:41.360]   you've got lots of numbers in between there,
[00:10:41.360 --> 00:10:44.240]   QLoRa will divide that space so that it
[00:10:44.240 --> 00:10:45.680]   can be stored in 4 bits.
[00:10:45.680 --> 00:10:47.960]   The number of possible values there is 2 to the 4,
[00:10:47.960 --> 00:10:49.920]   so it's 16 values.
[00:10:49.920 --> 00:10:52.920]   The exact way that we choose the 16 values
[00:10:52.920 --> 00:10:55.080]   is a technical topic that I think
[00:10:55.080 --> 00:10:58.600]   isn't worth going into in this moment.
[00:10:58.600 --> 00:11:01.080]   There's some details about how you do backpropagation there
[00:11:01.080 --> 00:11:04.560]   that we don't really need to know in practice.
[00:11:04.560 --> 00:11:08.480]   But by storing every number in 4 bits,
[00:11:08.480 --> 00:11:12.640]   you cut down on the memory usage by quite a bit.
[00:11:12.640 --> 00:11:16.160]   And so a lot of people do this.
[00:11:16.160 --> 00:11:19.320]   You'll see, again, that this is not so complex to do.
[00:11:19.320 --> 00:11:25.280]   And in practice, it saves some RAM,
[00:11:25.280 --> 00:11:31.000]   and it has some small impact on results.
[00:11:31.000 --> 00:11:33.280]   But I think my intuition would have
[00:11:33.280 --> 00:11:35.200]   been that it has a bigger impact on results
[00:11:35.200 --> 00:11:37.240]   than I've actually observed it having,
[00:11:37.240 --> 00:11:39.160]   and I think most people would agree with that.
[00:11:39.160 --> 00:11:42.400]   And so a lot of people run QLoRa models
[00:11:42.400 --> 00:11:47.240]   or train with QLoRa, either as their default first step,
[00:11:47.240 --> 00:11:48.780]   or at the very least, it's something
[00:11:48.780 --> 00:11:49.780]   that they do frequently.
[00:11:49.780 --> 00:11:51.520]   And again, we'll show you how to do that.
[00:11:51.520 --> 00:11:55.480]   And it's shockingly easy.
[00:11:55.480 --> 00:11:57.640]   So--
[00:11:57.640 --> 00:12:00.200]   Maybe it's a good time to just pause for a second.
[00:12:00.200 --> 00:12:03.120]   Wing, do you-- Wing, Zach, even--
[00:12:03.120 --> 00:12:07.040]   do you have any opinions on QLoRa/LoRa
[00:12:07.040 --> 00:12:10.000]   when you use them, any observations, feelings?
[00:12:10.000 --> 00:12:11.360]   Do you agree?
[00:12:11.360 --> 00:12:16.200]   Any-- yeah, any further thoughts?
[00:12:16.200 --> 00:12:21.400]   I know that sometimes people see a difference
[00:12:21.400 --> 00:12:25.160]   between the actual losses or some of the evaluations
[00:12:25.160 --> 00:12:28.040]   that you get during fine tuning with QLoRa,
[00:12:28.040 --> 00:12:31.360]   because what's happening is you've quantized the weights,
[00:12:31.360 --> 00:12:32.840]   and then you're training on those.
[00:12:32.840 --> 00:12:34.960]   But then when you merge those LoRa's back
[00:12:34.960 --> 00:12:39.440]   into the original model, because the quantization--
[00:12:39.440 --> 00:12:43.080]   there's quantization errors due to quantization,
[00:12:43.080 --> 00:12:46.600]   that you're not actually getting the exact same model
[00:12:46.600 --> 00:12:47.720]   that you trained.
[00:12:47.720 --> 00:12:50.080]   So there has been some debate over that.
[00:12:50.080 --> 00:12:55.800]   I don't-- I personally don't feel like that's a huge issue.
[00:12:55.800 --> 00:12:58.240]   Otherwise, people would not be using it anymore.
[00:12:58.240 --> 00:13:01.280]   So that's really the only thing that I have about that.
[00:13:01.280 --> 00:13:03.560]   And I think there was also something that I personally
[00:13:03.560 --> 00:13:07.200]   didn't understand with QLoRa, what the quantization was.
[00:13:07.200 --> 00:13:09.320]   I think there were double quantization,
[00:13:09.320 --> 00:13:11.840]   and there's some nuances with that
[00:13:11.840 --> 00:13:13.720]   as well when you're quantizing the weights,
[00:13:13.720 --> 00:13:19.120]   maybe, if Dan understands that better than me.
[00:13:19.120 --> 00:13:21.120]   I think I don't.
[00:13:21.120 --> 00:13:23.760]   One of the speakers-- so at workshop four,
[00:13:23.760 --> 00:13:28.360]   we're going to have Travis Adair, who
[00:13:28.360 --> 00:13:30.840]   is the CTO of Predabase.
[00:13:30.840 --> 00:13:33.760]   But he built LoRaX, which is a serving framework.
[00:13:33.760 --> 00:13:37.840]   And he talked about some of the quantization errors
[00:13:37.840 --> 00:13:40.680]   as you merge the weights back.
[00:13:40.680 --> 00:13:44.960]   I think he has thought about this way more deeply
[00:13:44.960 --> 00:13:46.320]   than I have.
[00:13:46.320 --> 00:13:50.080]   And so I know that I'm looking forward to workshop four
[00:13:50.080 --> 00:13:53.560]   so I can hear his description of what
[00:13:53.560 --> 00:13:54.600]   he's done about this issue.
[00:13:54.600 --> 00:13:56.800]   But yeah, I don't know much more about it than that.
[00:13:56.800 --> 00:13:57.300]   Yeah.
[00:13:57.300 --> 00:14:09.160]   All of this is--
[00:14:09.160 --> 00:14:15.120]   like I said, there are so many places in AI and before that,
[00:14:15.120 --> 00:14:18.120]   ML, where it's tempting to get really
[00:14:18.120 --> 00:14:20.760]   detailed about all sorts of things
[00:14:20.760 --> 00:14:22.760]   that seem very mathematical.
[00:14:22.760 --> 00:14:26.720]   The payoff to doing that, even though most of us
[00:14:26.720 --> 00:14:28.240]   were good at math from an early age
[00:14:28.240 --> 00:14:30.920]   and we're told, I used to do a lot of math,
[00:14:30.920 --> 00:14:33.200]   anything with hyperparameters while sounding cool
[00:14:33.200 --> 00:14:37.080]   has a much, much lower payoff than spending that time
[00:14:37.080 --> 00:14:41.360]   looking at your data and improving your data.
[00:14:41.360 --> 00:14:43.280]   And you might think, my data is what it is.
[00:14:43.280 --> 00:14:44.120]   How can I improve it?
[00:14:44.120 --> 00:14:46.360]   And so when we get to Hamel's--
[00:14:46.360 --> 00:14:50.800]   what Hamel shows about his work with Honeycomb,
[00:14:50.800 --> 00:14:52.920]   you'll see that you actually can improve your data.
[00:14:52.920 --> 00:15:00.760]   And the payoff to improving your data is so, so large.
[00:15:00.760 --> 00:15:04.120]   I think Hamel made a comment about--
[00:15:04.120 --> 00:15:06.800]   many of you might know who Technium is, but I don't--
[00:15:06.800 --> 00:15:08.200]   Hamel, you wanted to jump in here.
[00:15:08.200 --> 00:15:16.560]   Yeah, anyway, improving your data, the payoffs are massive
[00:15:16.560 --> 00:15:19.080]   and you should do more of that.
[00:15:19.080 --> 00:15:20.840]   One of the things that we're going
[00:15:20.840 --> 00:15:23.480]   to switch into from the abstract,
[00:15:23.480 --> 00:15:26.560]   like, hey, here are some ideas to how do we implement this.
[00:15:26.560 --> 00:15:29.920]   One of the things that I loved about Axolotl
[00:15:29.920 --> 00:15:32.200]   when I switched from--
[00:15:32.200 --> 00:15:36.400]   use it-- so Axolotl is a wrapper for lower-level HuggingFace
[00:15:36.400 --> 00:15:37.240]   libraries.
[00:15:37.240 --> 00:15:40.680]   One of the things that I most loved about this switch
[00:15:40.680 --> 00:15:43.840]   from HuggingFace lower-level libraries that
[00:15:43.840 --> 00:15:46.760]   give you a lot of granular control to using Axolotl
[00:15:46.760 --> 00:15:50.720]   is that Axolotl was so easy to use that I never
[00:15:50.720 --> 00:15:53.480]   thought about, oh, what's the error in my code?
[00:15:53.480 --> 00:15:56.000]   And I just spent actually less time looking at code.
[00:15:56.000 --> 00:15:58.680]   And I spent more time just psychologically
[00:15:58.680 --> 00:16:00.480]   looking at my data.
[00:16:00.480 --> 00:16:05.440]   And so the ease of changing some things around
[00:16:05.440 --> 00:16:08.880]   and being able to run things freed up some mental space
[00:16:08.880 --> 00:16:12.640]   for me to focus on my data, which
[00:16:12.640 --> 00:16:15.600]   was said is a great thing to do.
[00:16:15.600 --> 00:16:18.080]   And also, if you just use the examples--
[00:16:18.080 --> 00:16:19.760]   and I'll show you some of the examples.
[00:16:19.760 --> 00:16:23.320]   There are a lot of just best practices and default values
[00:16:23.320 --> 00:16:26.000]   that are built in.
[00:16:26.000 --> 00:16:30.120]   It does a lot of smart things as defaults.
[00:16:30.120 --> 00:16:34.040]   I'm going to-- there are a couple of things
[00:16:34.040 --> 00:16:36.480]   that I quite like that it does that we
[00:16:36.480 --> 00:16:37.720]   don't have time to cover.
[00:16:37.720 --> 00:16:39.800]   And so I'm going to make a couple of videos
[00:16:39.800 --> 00:16:43.960]   and then just post them either in the Discord
[00:16:43.960 --> 00:16:48.040]   or in Maven or on the Maven portal or both,
[00:16:48.040 --> 00:16:50.200]   quite possibly both, showing things
[00:16:50.200 --> 00:16:52.440]   like sample packing, which is a quite clever thing
[00:16:52.440 --> 00:16:57.000]   that it does that speeds up your training process.
[00:16:57.000 --> 00:17:00.120]   But it has a lot of things that you could spend a lot of time
[00:17:00.120 --> 00:17:01.960]   figuring out for yourself.
[00:17:01.960 --> 00:17:06.520]   Or you could just use some of these examples in Axolotl
[00:17:06.520 --> 00:17:10.120]   and change relatively few things and have a lot of best
[00:17:10.120 --> 00:17:12.520]   practices built in by default.
[00:17:12.520 --> 00:17:15.280]   So I have loved--
[00:17:15.280 --> 00:17:16.960]   Wing, thank you.
[00:17:16.960 --> 00:17:19.600]   I've loved using Axolotl.
[00:17:19.600 --> 00:17:20.600]   And--
[00:17:20.600 --> 00:17:22.120]   >>One thing I want to-- maybe it's
[00:17:22.120 --> 00:17:24.440]   worth lingering on for a second--
[00:17:24.440 --> 00:17:30.360]   is, Wing-- I'll let Wing tell the story.
[00:17:30.360 --> 00:17:39.280]   Have you been surprised by the level of what kind of people
[00:17:39.280 --> 00:17:45.960]   are able to fine-tune models, like really competitive ones,
[00:17:45.960 --> 00:17:51.040]   without knowing any deep mathematics or things
[00:17:51.040 --> 00:17:52.080]   like that?
[00:17:52.080 --> 00:17:57.840]   >>Yeah, I mean, I think if you just sort of--
[00:17:57.840 --> 00:17:59.880]   like, if you think about actually the most popular
[00:17:59.880 --> 00:18:03.080]   model, I think generally with Titanium's Hermes models
[00:18:03.080 --> 00:18:05.920]   and those sort of ones, they're generally very popular.
[00:18:05.920 --> 00:18:08.640]   And if you actually talk to Ryan,
[00:18:08.640 --> 00:18:13.080]   he's also very much like me, where he doesn't quite
[00:18:13.080 --> 00:18:16.120]   get deep into transformers and the math and all of that
[00:18:16.120 --> 00:18:20.560]   and just wants to trade models and focus on good data.
[00:18:20.560 --> 00:18:26.200]   So really, all of his models are really good.
[00:18:26.200 --> 00:18:29.240]   There are people like--
[00:18:29.240 --> 00:18:35.000]   I think, let's say, I think Miguel Tesoro is it,
[00:18:35.000 --> 00:18:37.280]   with the--
[00:18:37.280 --> 00:18:39.600]   I forget which models he has that he releases.
[00:18:39.600 --> 00:18:42.120]   I mean, I think his background is more deep learning.
[00:18:42.120 --> 00:18:44.320]   But he also uses Axolotl.
[00:18:44.320 --> 00:18:46.280]   And there's a lot of--
[00:18:46.280 --> 00:18:50.160]   they don't really need to go deep into the transformers.
[00:18:50.160 --> 00:18:53.080]   And so, yeah, like Dan was saying,
[00:18:53.080 --> 00:18:55.360]   they just are able to spend more time focusing
[00:18:55.360 --> 00:18:59.560]   on just procuring good data and doing data synthesis
[00:18:59.560 --> 00:19:01.880]   rather than thinking about all of the--
[00:19:01.880 --> 00:19:03.800]   everything else that goes on underneath the hood.
[00:19:03.800 --> 00:19:10.120]   Great.
[00:19:10.120 --> 00:19:15.040]   OK, let's get one level more tactical or concrete.
[00:19:15.040 --> 00:19:19.360]   So using Axolotl, we are-- some people here
[00:19:19.360 --> 00:19:20.440]   have used it a bunch.
[00:19:20.440 --> 00:19:22.600]   We're going to make the assumption that most of you
[00:19:22.600 --> 00:19:26.880]   have either used it very, very little or, I think, even more
[00:19:26.880 --> 00:19:29.920]   when we did a survey at some point of some students.
[00:19:29.920 --> 00:19:32.040]   Most of you have not used it at all.
[00:19:32.040 --> 00:19:35.720]   So this is going to be really a how do you just actively get
[00:19:35.720 --> 00:19:37.720]   started.
[00:19:37.720 --> 00:19:40.800]   I think you'll be surprised that it is not so, so difficult
[00:19:40.800 --> 00:19:42.760]   to run your first job.
[00:19:42.760 --> 00:19:44.440]   And I highly recommend doing that.
[00:19:44.440 --> 00:19:46.640]   You'll just feel different about yourself
[00:19:46.640 --> 00:19:50.160]   as someone in this space once you've run a couple of jobs
[00:19:50.160 --> 00:19:52.720]   and you feel like a practitioner now.
[00:19:52.720 --> 00:19:55.480]   So I highly recommend using it.
[00:19:55.480 --> 00:20:00.280]   The way to get started is if you go to the Axolotl--
[00:20:00.280 --> 00:20:04.080]   actually, I would start with just googling GitHub Axolotl.
[00:20:04.080 --> 00:20:07.360]   If you go to the Axolotl repo, there
[00:20:07.360 --> 00:20:09.800]   is a separate documentation page.
[00:20:09.800 --> 00:20:13.720]   But just the readme is fantastic and has
[00:20:13.720 --> 00:20:15.720]   most of what you'll need.
[00:20:15.720 --> 00:20:18.200]   I'm going to point out a couple of things
[00:20:18.200 --> 00:20:23.000]   that you should look for while you are in that readme.
[00:20:23.000 --> 00:20:26.400]   So the very first is examples.
[00:20:26.400 --> 00:20:29.560]   I mentioned earlier that there are a lot of examples.
[00:20:29.560 --> 00:20:34.360]   Axolotl takes YAML config files.
[00:20:34.360 --> 00:20:38.880]   And the config files are reasonably long.
[00:20:38.880 --> 00:20:40.480]   Maybe Wing could do it.
[00:20:40.480 --> 00:20:43.000]   But I don't think there is anyone else who
[00:20:43.000 --> 00:20:45.280]   could open up them or have a blinking cursor
[00:20:45.280 --> 00:20:47.440]   and then just type one out beginning to end
[00:20:47.440 --> 00:20:48.360]   and get it right.
[00:20:48.440 --> 00:20:52.760]   So you and almost everyone else will
[00:20:52.760 --> 00:20:54.960]   go to one of these examples, copy it.
[00:20:54.960 --> 00:20:56.840]   The first time, you should just run it.
[00:20:56.840 --> 00:20:58.160]   And I'll show you how to do that.
[00:20:58.160 --> 00:21:00.680]   But then you're likely to change one or two parameters.
[00:21:00.680 --> 00:21:02.280]   Probably the first one that you might change
[00:21:02.280 --> 00:21:03.640]   is the data set that you use.
[00:21:03.640 --> 00:21:06.960]   But you might change one or two other parameters, rerun it.
[00:21:06.960 --> 00:21:09.280]   And it will always be an experience
[00:21:09.280 --> 00:21:11.880]   of taking something that works and then changing it
[00:21:11.880 --> 00:21:14.480]   around a little bit rather than starting from scratch.
[00:21:14.480 --> 00:21:17.160]   So you're going to use these examples
[00:21:17.160 --> 00:21:20.080]   to show you one of them.
[00:21:20.080 --> 00:21:20.720]   So here's one.
[00:21:20.720 --> 00:21:28.360]   This is to fine tune a Mistral 7b model with QLORA.
[00:21:28.360 --> 00:21:31.680]   So the first, the very top, is showing you
[00:21:31.680 --> 00:21:34.960]   what is the model that I'm fine tuning off of.
[00:21:34.960 --> 00:21:35.920]   So this is QLORA.
[00:21:35.920 --> 00:21:40.640]   So here we are loading in 4-bit.
[00:21:40.640 --> 00:21:41.640]   We have a data set.
[00:21:41.640 --> 00:21:44.200]   I'll show you that data set in a moment.
[00:21:44.200 --> 00:21:48.960]   We're going to store the data set after the prep phase
[00:21:48.960 --> 00:21:50.800]   in some location.
[00:21:50.800 --> 00:21:54.720]   We're going to have some validation data.
[00:21:54.720 --> 00:21:57.240]   Most of these you won't change that frequently.
[00:21:57.240 --> 00:22:00.040]   Sample packing, I'll make a separate video about.
[00:22:00.040 --> 00:22:04.120]   This LORA_R is related to the size of those LORA matrices.
[00:22:04.120 --> 00:22:06.240]   That's that matrix that I was showing earlier.
[00:22:06.240 --> 00:22:08.360]   LORA_alpha is a scaling parameter.
[00:22:08.360 --> 00:22:10.520]   I wouldn't worry about some of these bottom ones.
[00:22:10.520 --> 00:22:12.520]   I think the ones that you probably
[00:22:12.520 --> 00:22:16.320]   want to focus on up front would be actually--
[00:22:16.320 --> 00:22:17.760]   it's not the easiest one to change,
[00:22:17.760 --> 00:22:19.360]   so you could change something else just
[00:22:19.360 --> 00:22:21.040]   to get an experience of changing it.
[00:22:21.040 --> 00:22:23.320]   But when you really start working on your own use cases,
[00:22:23.320 --> 00:22:27.920]   the first one you'll change is the data set.
[00:22:27.920 --> 00:22:32.960]   And the format of the data set is--
[00:22:32.960 --> 00:22:35.160]   so there are a lot of different formats.
[00:22:35.160 --> 00:22:36.800]   I think one of the things that's really
[00:22:36.800 --> 00:22:41.240]   nice about Axolotl is that out there in the wild,
[00:22:41.240 --> 00:22:43.120]   data is stored in a variety of formats.
[00:22:43.120 --> 00:22:46.880]   And if you tell Axolotl what formats it's stored in,
[00:22:46.880 --> 00:22:50.440]   you can use most of, if not all, of the common formats.
[00:22:50.440 --> 00:22:52.680]   So this is a format called alpaca.
[00:22:52.680 --> 00:23:01.120]   But each row or each sample has an instruction to the model.
[00:23:01.120 --> 00:23:03.080]   Optionally, some input you'll see in these.
[00:23:03.080 --> 00:23:04.640]   Most of those are empty.
[00:23:04.640 --> 00:23:07.560]   It has the output, which is what we want the model
[00:23:07.560 --> 00:23:09.120]   to learn to reproduce.
[00:23:09.120 --> 00:23:13.160]   And then it has some text, which will go above these.
[00:23:13.160 --> 00:23:14.960]   So the text would be, below is an instruction
[00:23:14.960 --> 00:23:17.960]   that describes a task, blah, blah, blah.
[00:23:17.960 --> 00:23:20.240]   And then you'll have a question, like, what is the world's most
[00:23:20.240 --> 00:23:20.920]   famous--
[00:23:20.920 --> 00:23:22.520]   who is the world's most famous painter?
[00:23:22.520 --> 00:23:25.320]   And then here is the training output,
[00:23:25.320 --> 00:23:27.520]   which is what we're going to train on and try and have
[00:23:27.520 --> 00:23:31.560]   the model learn to replicate the behavior of.
[00:23:31.560 --> 00:23:33.880]   So just to kind of stop there for a second
[00:23:33.880 --> 00:23:36.960]   and talk about the config files.
[00:23:36.960 --> 00:23:43.760]   So when I start a project, I look at the examples, too.
[00:23:43.760 --> 00:23:45.520]   I message Wing sometimes.
[00:23:45.520 --> 00:23:47.080]   Now, not everybody can message Wing.
[00:23:47.080 --> 00:23:48.960]   Please don't message Wing with--
[00:23:48.960 --> 00:23:54.880]   please don't DDOS him with questions like that.
[00:23:54.880 --> 00:23:57.480]   There is a Slack channel, an Axolotl--
[00:23:57.480 --> 00:24:00.040]   sorry, a Discord channel.
[00:24:00.040 --> 00:24:01.960]   I think Wing looks like he's getting the link
[00:24:01.960 --> 00:24:06.080]   and putting it in the Discord right now.
[00:24:06.080 --> 00:24:08.800]   And that's a good place to kind of trade configs.
[00:24:08.800 --> 00:24:12.280]   But yeah, starting with a known good config is a good idea.
[00:24:12.280 --> 00:24:16.400]   It's like, hey, I'm training this model that just came out.
[00:24:16.400 --> 00:24:17.480]   Does anyone have a config?
[00:24:17.480 --> 00:24:20.800]   And usually, either by searching that Discord
[00:24:20.800 --> 00:24:22.960]   or looking at the examples or something else,
[00:24:22.960 --> 00:24:24.520]   you can find a config.
[00:24:24.520 --> 00:24:28.960]   And there's a lot of times in Hugging Face repos you can find--
[00:24:28.960 --> 00:24:33.480]   nowadays, you can find Axolotl configs as well.
[00:24:33.480 --> 00:24:36.640]   Wing, do you have any other tips on where to find configs
[00:24:36.640 --> 00:24:41.320]   or how people should go about it?
[00:24:41.320 --> 00:24:46.920]   >>Wing: Yeah, depending on some model creators.
[00:24:46.920 --> 00:24:50.000]   I know, personally, I try and include sort of the model
[00:24:50.000 --> 00:24:51.640]   configs when I'm releasing models,
[00:24:51.640 --> 00:24:53.760]   either somewhere in the repo or in the README.
[00:24:53.760 --> 00:24:58.400]   I think Axolotl, by default, also stores in your README.
[00:24:58.400 --> 00:25:00.840]   It'll store the Axolotl config.
[00:25:00.840 --> 00:25:03.680]   So sometimes, if you go through Hugging Face,
[00:25:03.680 --> 00:25:05.320]   there is a link where you can find
[00:25:05.320 --> 00:25:08.960]   models that are tagged that were trained by Axolotl.
[00:25:08.960 --> 00:25:11.440]   Depending on whether or not they modify their README,
[00:25:11.440 --> 00:25:15.200]   you can sort of get configs from there as well.
[00:25:15.200 --> 00:25:18.080]   But other than that, I think a lot of times,
[00:25:18.080 --> 00:25:21.040]   you'll see some examples in the Discord people have.
[00:25:21.040 --> 00:25:27.160]   And I'm happy to also help just with various things,
[00:25:27.160 --> 00:25:28.560]   depending on what--
[00:25:28.560 --> 00:25:31.400]   but it's generally pretty self-explanatory most
[00:25:31.400 --> 00:25:34.640]   of the time, I think.
[00:25:34.640 --> 00:25:37.720]   Usually, you're taking little bits from one config
[00:25:37.720 --> 00:25:39.400]   and maybe combining with another piece,
[00:25:39.400 --> 00:25:46.400]   whether it's FSDP or DeepSpeed or the LoR versus QLoR versus--
[00:25:46.400 --> 00:25:49.200]   most of the various configurations
[00:25:49.200 --> 00:25:51.200]   are pretty composable with each other.
[00:25:51.200 --> 00:25:53.760]   And if they're not, I believe we do enough validation
[00:25:53.760 --> 00:25:55.920]   that it will tell you that it's not composable.
[00:25:55.920 --> 00:26:02.560]   Sounds good.
[00:26:02.560 --> 00:26:03.320]   Yeah.
[00:26:03.320 --> 00:26:03.840]   OK.
[00:26:03.840 --> 00:26:05.520]   And then a lot of those--
[00:26:05.520 --> 00:26:08.720]   there are a lot of other parameters.
[00:26:08.720 --> 00:26:10.760]   I won't go through these in--
[00:26:10.760 --> 00:26:12.200]   I won't go through most of these.
[00:26:12.200 --> 00:26:14.920]   Most of them, you won't change.
[00:26:14.920 --> 00:26:16.680]   But I will say a couple of things.
[00:26:16.680 --> 00:26:20.800]   One is many of us like using Weights and Biases.
[00:26:20.800 --> 00:26:23.360]   It's a very nice Weights and Biases integration
[00:26:23.360 --> 00:26:24.760]   in Axolotl.
[00:26:24.760 --> 00:26:27.120]   You'll even see a config from Haml
[00:26:27.120 --> 00:26:30.920]   later on that shows you how to fill this in.
[00:26:30.920 --> 00:26:33.960]   Microbatch size is just the--
[00:26:33.960 --> 00:26:37.200]   basically, batch size per GPU.
[00:26:37.200 --> 00:26:39.320]   Yeah, and a lot of this stuff you
[00:26:39.320 --> 00:26:42.560]   won't change in the near future.
[00:26:42.560 --> 00:26:45.920]   And so like I said, I highly recommend
[00:26:45.920 --> 00:26:49.040]   starting with any of the example configs
[00:26:49.040 --> 00:26:51.400]   and then changing it just small pieces.
[00:26:51.400 --> 00:26:53.320]   Don't get overwhelmed by all the things
[00:26:53.320 --> 00:26:56.120]   that you aren't changing.
[00:26:56.120 --> 00:27:01.400]   Then once you have your config, the next step is to run it.
[00:27:01.400 --> 00:27:06.640]   Like I said, I think this GitHub readme is so, so useful.
[00:27:06.640 --> 00:27:09.200]   So after you've got your example,
[00:27:09.200 --> 00:27:12.960]   click on the Quickstart section.
[00:27:12.960 --> 00:27:18.360]   And that will bring you to a set of--
[00:27:18.360 --> 00:27:21.400]   depending on how we count, either three or four commands.
[00:27:21.400 --> 00:27:24.320]   So the reason this, while it looks like four could be three,
[00:27:24.320 --> 00:27:26.320]   is that there are three steps.
[00:27:26.320 --> 00:27:29.320]   So one is pre-processing your data.
[00:27:29.320 --> 00:27:33.280]   The second is this training step.
[00:27:33.280 --> 00:27:34.720]   And then after that, you're going
[00:27:34.720 --> 00:27:40.840]   to want to just test out the model that you've trained.
[00:27:40.840 --> 00:27:43.280]   So there is a CLI tool to do that.
[00:27:43.280 --> 00:27:44.920]   That's this third step.
[00:27:44.920 --> 00:27:47.200]   And Haml will actually show another way to do this.
[00:27:47.200 --> 00:27:49.120]   The thing that I like to do is there's also,
[00:27:49.120 --> 00:27:53.240]   if you run this bottom version instead of the third,
[00:27:53.240 --> 00:27:57.320]   that launches a very lightweight Gradio app
[00:27:57.320 --> 00:27:59.520]   so that you can just, in the web,
[00:27:59.520 --> 00:28:01.240]   type something into a form.
[00:28:01.240 --> 00:28:02.720]   And that gets sent to the model.
[00:28:02.720 --> 00:28:04.360]   And inference happens.
[00:28:04.360 --> 00:28:05.600]   And then the output is shown.
[00:28:05.600 --> 00:28:10.320]   So I quite like using this bottom step.
[00:28:10.320 --> 00:28:11.240]   You will--
[00:28:11.240 --> 00:28:14.000]   I think it's worth mentioning, you only
[00:28:14.000 --> 00:28:16.400]   want to do this to spot check your model.
[00:28:16.400 --> 00:28:17.760]   This is not for production.
[00:28:17.760 --> 00:28:20.120]   You don't want to do inference necessarily in production
[00:28:20.120 --> 00:28:20.920]   with this.
[00:28:20.920 --> 00:28:21.400]   Yeah.
[00:28:21.400 --> 00:28:23.760]   And we'll cover inference in production
[00:28:23.760 --> 00:28:28.520]   in the deployment workshop.
[00:28:28.520 --> 00:28:29.040]   Sorry.
[00:28:29.040 --> 00:28:31.560]   I lost my train of thought.
[00:28:31.560 --> 00:28:34.760]   So you will not remember these commands.
[00:28:34.760 --> 00:28:36.240]   The thing that I hope you remember
[00:28:36.240 --> 00:28:41.640]   is that everything you want is in the GitHub repo.
[00:28:41.640 --> 00:28:44.520]   And this one is in the Quickstart.
[00:28:44.520 --> 00:28:46.560]   But it's just the series of commands.
[00:28:46.560 --> 00:28:51.320]   So what does it look like if you run that?
[00:28:51.320 --> 00:28:53.480]   I'm going to show you--
[00:28:53.480 --> 00:28:57.320]   some of the text here is going to be relatively small.
[00:28:57.320 --> 00:28:59.880]   So we'll come back, and I'll show you a screenshot
[00:28:59.880 --> 00:29:02.360]   that you can see some stuff in more detail.
[00:29:02.360 --> 00:29:04.640]   But this is just a very quick view
[00:29:04.640 --> 00:29:06.760]   of what happens when you train the model.
[00:29:06.760 --> 00:29:08.920]   So actually, I'm going to make sure
[00:29:08.920 --> 00:29:13.960]   that you can see it in reasonably high depth.
[00:29:13.960 --> 00:29:17.600]   So here, I am typing out that first preprocess command.
[00:29:17.600 --> 00:29:20.560]   I use the debug flag.
[00:29:20.560 --> 00:29:24.120]   And we'll talk about the debug flag, whether you use it or not
[00:29:24.120 --> 00:29:25.240]   when he gets to his section.
[00:29:25.240 --> 00:29:27.160]   But I kind of like using it.
[00:29:27.160 --> 00:29:32.040]   And when you do that, there's some output here.
[00:29:32.040 --> 00:29:34.440]   In a moment, I'm going to go into that in more depth.
[00:29:34.440 --> 00:29:36.800]   And then after that, I run the next command
[00:29:36.800 --> 00:29:38.240]   that was shown on that last screen.
[00:29:38.240 --> 00:29:42.240]   This is just doing training.
[00:29:42.240 --> 00:29:44.800]   And that kicks off training.
[00:29:44.800 --> 00:29:48.840]   And then training, depending on the amount of data you have,
[00:29:48.840 --> 00:29:54.480]   can take minutes, hours, I suppose, sometimes days,
[00:29:54.480 --> 00:29:56.840]   though the projects I do--
[00:29:56.840 --> 00:29:59.000]   actually, I have one project where it can take days.
[00:29:59.000 --> 00:30:04.920]   But it's typically an hour or so, and sometimes much less.
[00:30:04.920 --> 00:30:10.640]   So let me go to the next slide.
[00:30:10.640 --> 00:30:16.560]   In there, there was a section that it printed out
[00:30:16.560 --> 00:30:20.160]   from the preprocessing step with the debug flag
[00:30:20.160 --> 00:30:21.840]   that it would be easier to overlook,
[00:30:21.840 --> 00:30:26.080]   but I think is really critical for your understanding of what
[00:30:26.080 --> 00:30:27.440]   is happening here.
[00:30:27.440 --> 00:30:32.760]   So though we started with data that had multiple fields,
[00:30:32.760 --> 00:30:36.120]   your model is going to train on a string.
[00:30:36.120 --> 00:30:37.360]   Or I'll show you in a moment.
[00:30:37.360 --> 00:30:39.160]   It's actually a string and one other piece.
[00:30:39.160 --> 00:30:40.760]   But it's going to train on a string.
[00:30:40.760 --> 00:30:44.240]   And so this is showing you the template
[00:30:44.240 --> 00:30:49.600]   for what does that string look like that we
[00:30:49.600 --> 00:30:52.960]   create in the preprocessing step and then that we later
[00:30:52.960 --> 00:30:57.360]   use for modeling.
[00:30:57.360 --> 00:30:59.680]   So we have, say, there's an instruction and input
[00:30:59.680 --> 00:31:01.400]   and output.
[00:31:01.400 --> 00:31:06.200]   And actually, those are, for each sample, just filling in.
[00:31:06.200 --> 00:31:07.480]   Here's the instruction.
[00:31:07.480 --> 00:31:08.280]   Here's the output.
[00:31:08.280 --> 00:31:10.080]   Here's the text.
[00:31:10.080 --> 00:31:13.200]   When you use this for inference, you're
[00:31:13.200 --> 00:31:16.240]   going to want to provide everything up
[00:31:16.240 --> 00:31:18.640]   through this response part, but then not the output,
[00:31:18.640 --> 00:31:19.720]   because you wouldn't know the output when
[00:31:19.720 --> 00:31:20.840]   you use this for inference.
[00:31:20.840 --> 00:31:24.800]   But this template is showing you what the string looks like.
[00:31:24.800 --> 00:31:27.600]   And then we're going to use that autocomplete type logic
[00:31:27.600 --> 00:31:29.720]   so that we provide everything before the output
[00:31:29.720 --> 00:31:32.800]   and our model will provide the output.
[00:31:32.800 --> 00:31:35.760]   It's actually-- this looks like it's just a string.
[00:31:35.760 --> 00:31:37.640]   There is one other piece that I think
[00:31:37.640 --> 00:31:42.840]   is important for your understanding of fine tuning
[00:31:42.840 --> 00:31:43.880]   that is shown here.
[00:31:43.880 --> 00:31:48.960]   So it's actually a string and a mask.
[00:31:48.960 --> 00:31:51.240]   So I'm going to go back here for a moment.
[00:31:51.240 --> 00:31:55.400]   When you calculate your loss function, which
[00:31:55.400 --> 00:31:58.200]   is part of, for those of you who are familiar with deep learning,
[00:31:58.200 --> 00:32:00.920]   which is part of figuring out how do we change our parameters
[00:32:00.920 --> 00:32:03.680]   to change the model's behavior, we
[00:32:03.680 --> 00:32:06.880]   don't want to train the model to write the words below
[00:32:06.880 --> 00:32:08.800]   as an instruction that describes a task.
[00:32:08.800 --> 00:32:10.320]   And we actually don't even--
[00:32:10.320 --> 00:32:15.280]   the input here is a proxy for what the users of your app's
[00:32:15.280 --> 00:32:16.080]   input will be.
[00:32:16.080 --> 00:32:18.600]   So we don't want to train the model to be the user.
[00:32:18.600 --> 00:32:23.680]   We want it to instead be good at responding to user inputs.
[00:32:23.680 --> 00:32:30.200]   And so these pieces up front are not going to inform the loss.
[00:32:30.200 --> 00:32:34.680]   So when we look at the output, we
[00:32:34.680 --> 00:32:37.520]   can look at it on a token-by-token basis.
[00:32:37.520 --> 00:32:41.240]   So somewhere in there, there was some input.
[00:32:41.240 --> 00:32:44.600]   And there were the words that appropriately completes
[00:32:44.600 --> 00:32:46.720]   the request with a period.
[00:32:46.720 --> 00:32:48.240]   Each of these are tokens.
[00:32:48.240 --> 00:32:50.560]   And before that, we have pairs of--
[00:32:50.560 --> 00:32:53.400]   the word that is token ID 2899.
[00:32:53.400 --> 00:32:57.880]   But because we don't want it to feed into the loss,
[00:32:57.880 --> 00:33:01.280]   we have the first piece of this tuple here
[00:33:01.280 --> 00:33:03.680]   is minus 100, which is just a way of preventing it
[00:33:03.680 --> 00:33:06.680]   from influencing the loss and thus influencing
[00:33:06.680 --> 00:33:08.280]   the behavior of our model.
[00:33:08.280 --> 00:33:14.080]   If you look at the output, that's in green here.
[00:33:14.080 --> 00:33:16.000]   And for those, we have the token ID.
[00:33:16.000 --> 00:33:18.000]   Then we also have the purpose of calculating
[00:33:18.000 --> 00:33:20.400]   a loss with token is this.
[00:33:20.400 --> 00:33:21.240]   And it's the same.
[00:33:21.240 --> 00:33:23.720]   So there is a flag, which I think
[00:33:23.720 --> 00:33:25.360]   is called train on inputs, that will
[00:33:25.360 --> 00:33:26.960]   let you change this behavior.
[00:33:26.960 --> 00:33:28.920]   But broadly speaking, this is just
[00:33:28.920 --> 00:33:31.520]   showing that this is a way of being
[00:33:31.520 --> 00:33:34.920]   able to see very clearly what are the tokens that
[00:33:34.920 --> 00:33:36.920]   are influencing--
[00:33:36.920 --> 00:33:38.600]   that are the inputs to the model and what
[00:33:38.600 --> 00:33:40.440]   are the tokens that are influencing loss
[00:33:40.440 --> 00:33:43.920]   and that are eventually going to be the outputs of the model
[00:33:43.920 --> 00:33:46.240]   or that we're training the model to output.
[00:33:46.240 --> 00:33:49.720]   Wayne, do you use that debug thing in--
[00:33:49.720 --> 00:33:52.240]   Oh, yeah, all the time because--
[00:33:52.240 --> 00:33:55.480]   mostly because I want to be sure that the tokenization is
[00:33:55.480 --> 00:33:58.360]   correct because oftentimes, I'm using ChatML.
[00:33:58.360 --> 00:34:00.640]   And so because it's not a default token,
[00:34:00.640 --> 00:34:02.680]   I just want to make sure I didn't mess anything up
[00:34:02.680 --> 00:34:06.880]   in setting those special tokens for ChatML
[00:34:06.880 --> 00:34:10.880]   and just to double check that the outputs look right.
[00:34:10.880 --> 00:34:13.160]   Just so people know, ChatML is a specific type
[00:34:13.160 --> 00:34:14.720]   of prompt template.
[00:34:14.720 --> 00:34:20.360]   So if you go back to the previous slide that Dan had,
[00:34:20.360 --> 00:34:24.160]   this, I believe, is an alpaca template.
[00:34:24.160 --> 00:34:25.200]   This is alpaca?
[00:34:25.200 --> 00:34:28.000]   Yeah, so this is a specific type of template.
[00:34:28.000 --> 00:34:30.320]   And yeah, ChatML is different.
[00:34:30.320 --> 00:34:33.640]   And in general, Chat templates tend to be a little more--
[00:34:33.640 --> 00:34:36.120]   there's a slight complexity or nuance to them
[00:34:36.120 --> 00:34:38.560]   that instruction tuning templates, I think,
[00:34:38.560 --> 00:34:41.880]   arguably are a little simpler.
[00:34:41.880 --> 00:34:42.400]   OK.
[00:34:42.400 --> 00:34:43.640]   Sorry, I didn't mean to cut you off, Wayne.
[00:34:43.640 --> 00:34:44.640]   You can keep going.
[00:34:44.640 --> 00:34:45.360]   Yeah, no.
[00:34:45.360 --> 00:34:46.440]   I mean, that was really--
[00:34:46.440 --> 00:34:49.240]   and then checking the end tokens,
[00:34:49.240 --> 00:34:52.600]   making sure that the stop tokens are in there correctly.
[00:34:52.600 --> 00:34:56.680]   And just because sometimes if it's not in there,
[00:34:56.680 --> 00:34:59.920]   you can get a model that just starts to ramble on and on
[00:34:59.920 --> 00:35:00.680]   and never stops.
[00:35:00.680 --> 00:35:04.280]   So it's just a good spot check for myself.
[00:35:04.280 --> 00:35:07.480]   And especially in multi-turn conversations,
[00:35:07.480 --> 00:35:09.880]   just to make sure that it's masking out
[00:35:09.880 --> 00:35:11.920]   the responses correctly.
[00:35:11.920 --> 00:35:15.400]   And you can see that because it'll go red, green, red, green,
[00:35:15.400 --> 00:35:16.480]   red, green.
[00:35:16.480 --> 00:35:18.080]   So yeah, it's just an easy spot check.
[00:35:18.080 --> 00:35:21.200]   And the color, having the colors just
[00:35:21.200 --> 00:35:25.640]   makes it easy to just glance at it just to--
[00:35:25.640 --> 00:35:26.520]   without having to--
[00:35:26.520 --> 00:35:27.400]   because that is hard.
[00:35:27.400 --> 00:35:30.240]   That is actually really hard on the eyes to try and debug.
[00:35:30.240 --> 00:35:30.880]   So yeah.
[00:35:30.880 --> 00:35:40.040]   Let me show this last step.
[00:35:40.040 --> 00:35:42.200]   So we've done training.
[00:35:42.200 --> 00:35:43.280]   There is one more command.
[00:35:43.280 --> 00:35:45.080]   I'm going to show the Gradio version of it.
[00:35:45.080 --> 00:35:47.600]   So let me pause this for a moment,
[00:35:47.600 --> 00:35:50.080]   then switch over to--
[00:35:50.080 --> 00:35:52.240]   make sure that we're looking at this in the highest
[00:35:52.240 --> 00:35:53.920]   possible resolution.
[00:35:53.920 --> 00:35:58.240]   So the last step was to kick off the app.
[00:35:58.240 --> 00:36:00.080]   I'm going to run this accelerate launch,
[00:36:00.080 --> 00:36:05.240]   have the inference command pass in the right YAML file,
[00:36:05.240 --> 00:36:08.640]   the directory with the LoRa, and then this Gradio flag.
[00:36:08.640 --> 00:36:09.680]   This kicks off an app.
[00:36:09.680 --> 00:36:15.040]   You can click on that link, open something in the browser,
[00:36:15.040 --> 00:36:20.480]   and you can type and test things in the browser.
[00:36:20.480 --> 00:36:22.600]   So that was that last step.
[00:36:22.600 --> 00:36:25.440]   Again, you won't remember all of these pieces,
[00:36:25.440 --> 00:36:27.760]   but you should remember that they're in the Quick Start.
[00:36:27.760 --> 00:36:29.200]   And you can refer back to this.
[00:36:29.200 --> 00:36:32.840]   And again, super highly recommend
[00:36:32.840 --> 00:36:36.280]   before all the things get on your to-do list
[00:36:36.280 --> 00:36:39.280]   that you run through this so that you have hands-on
[00:36:39.280 --> 00:36:42.520]   experience using Axolotl.
[00:36:42.520 --> 00:36:47.120]   And with that, let me hand it off to Hamel
[00:36:47.120 --> 00:36:51.320]   to go through a case study, which
[00:36:51.320 --> 00:36:54.600]   is the Honeycomb case study.
[00:36:54.600 --> 00:36:55.400]   So you want to--
[00:36:55.400 --> 00:36:57.680]   Hamel, do you want to take over sharing?
[00:36:57.680 --> 00:37:00.680]   Yeah, let me do that right now.
[00:37:00.680 --> 00:37:01.180]   OK.
[00:37:06.920 --> 00:37:10.520]   Let's see here.
[00:37:10.520 --> 00:37:11.720]   Let me start the slide show.
[00:37:11.720 --> 00:37:16.840]   Is that sharing good?
[00:37:16.840 --> 00:37:18.320]   OK, thank you.
[00:37:18.320 --> 00:37:22.400]   OK, so we covered the--
[00:37:22.400 --> 00:37:24.840]   there is a through example in the workshops,
[00:37:24.840 --> 00:37:26.320]   in the fine-tuning workshops.
[00:37:26.320 --> 00:37:28.840]   And that's this use case of Honeycomb.
[00:37:28.840 --> 00:37:31.500]   And we discussed it in the first workshop.
[00:37:31.500 --> 00:37:32.920]   Because we have so many students,
[00:37:32.920 --> 00:37:35.000]   I'm going to just go over it really quickly again.
[00:37:35.000 --> 00:37:37.720]   The case study is you have--
[00:37:37.720 --> 00:37:42.240]   there is a company called Honeycomb that I've worked with.
[00:37:42.240 --> 00:37:44.520]   And Honeycomb is an observability platform.
[00:37:44.520 --> 00:37:46.600]   It's a telemetry system that allows
[00:37:46.600 --> 00:37:49.160]   you to log all kinds of data.
[00:37:49.160 --> 00:37:50.960]   And it tells you things like-- it
[00:37:50.960 --> 00:37:55.240]   helps you diagnose if parts of your application are slow,
[00:37:55.240 --> 00:37:59.080]   or there's bugs, somewhere like that, or something like that.
[00:37:59.080 --> 00:38:03.880]   It's kind of similar to Datadog in some ways.
[00:38:03.880 --> 00:38:09.440]   Honeycomb has a domain-specific query language called HQL.
[00:38:09.440 --> 00:38:10.920]   And one of the things they want to do
[00:38:10.920 --> 00:38:14.800]   is reduce the burden of people learning HQL.
[00:38:14.800 --> 00:38:19.400]   And so what they did is they released a alpha product
[00:38:19.400 --> 00:38:21.760]   that allowed users to type in natural language queries.
[00:38:21.760 --> 00:38:23.960]   So instead of learning the Honeycomb query language,
[00:38:23.960 --> 00:38:25.560]   you can just type in your question.
[00:38:25.560 --> 00:38:30.840]   And so the way it works is you have two inputs to the LLM.
[00:38:30.840 --> 00:38:32.560]   You have the user's query.
[00:38:32.560 --> 00:38:34.480]   And then you have the user schema.
[00:38:34.480 --> 00:38:37.280]   The user schema is retrieved with a RAG-type approach.
[00:38:37.280 --> 00:38:38.880]   We don't have to get into that.
[00:38:38.880 --> 00:38:40.680]   So with these two inputs, there is a prompt.
[00:38:40.680 --> 00:38:42.440]   And then out comes a Honeycomb query.
[00:38:42.440 --> 00:38:50.400]   So that's the sort of high-level overview, just to remind you.
[00:38:50.400 --> 00:38:52.080]   So let's jump right into the case study.
[00:38:52.080 --> 00:38:53.480]   For the case study, I'm just going
[00:38:53.480 --> 00:38:57.800]   to be walking through some slides.
[00:38:57.800 --> 00:39:03.400]   And let me open this GitHub repo.
[00:39:03.400 --> 00:39:06.400]   So it's github.com/parlance/labs/fdcourse.
[00:39:06.400 --> 00:39:07.860]   You don't have to open it right now.
[00:39:07.860 --> 00:39:10.200]   I actually just would follow along with what I'm doing.
[00:39:10.200 --> 00:39:12.440]   Is this repo right?
[00:39:12.440 --> 00:39:13.960]   So I'm going to open--
[00:39:13.960 --> 00:39:17.120]   actually, let me open the repo, just to show you.
[00:39:17.120 --> 00:39:18.680]   So it's a repo that looks like this.
[00:39:18.680 --> 00:39:19.840]   I'm just going to go through the notebooks.
[00:39:19.840 --> 00:39:22.840]   They're number 1 through 8.
[00:39:22.840 --> 00:39:26.720]   And Dan, tell me if you can see the text on my screen,
[00:39:26.720 --> 00:39:29.680]   or it's too small, or whatnot.
[00:39:29.680 --> 00:39:33.040]   I've got a big monitor, but it looks really clear to me.
[00:39:33.040 --> 00:39:33.540]   Good, Zach.
[00:39:33.540 --> 00:39:35.680]   OK.
[00:39:35.680 --> 00:39:36.760]   OK, so let me just--
[00:39:36.760 --> 00:39:38.220]   I'm going to go through some steps.
[00:39:38.220 --> 00:39:39.880]   These steps are not necessarily linear,
[00:39:39.880 --> 00:39:41.280]   but it'll give you a good idea.
[00:39:41.280 --> 00:39:46.360]   I'm going to be focusing a lot on what we did with Honeycomb
[00:39:46.360 --> 00:39:47.920]   to fine-tune a model.
[00:39:47.920 --> 00:39:52.240]   And a lot of the steps are going to be around data set curation,
[00:39:52.240 --> 00:39:56.280]   and data filtering, and debugging, and evaluation.
[00:39:56.280 --> 00:40:00.480]   Because we're not, as Dan mentioned,
[00:40:00.480 --> 00:40:04.600]   we're not really focused on the model so much.
[00:40:04.600 --> 00:40:07.400]   And so basically, I just want to go through the prompt real quick.
[00:40:07.400 --> 00:40:09.120]   So this is the Honeycomb prompt.
[00:40:09.120 --> 00:40:11.840]   It's basically the system prompt, Honeycomb AI,
[00:40:11.840 --> 00:40:13.280]   suggest users queries.
[00:40:13.280 --> 00:40:14.360]   This is one of the inputs.
[00:40:14.360 --> 00:40:16.600]   This is the schema.
[00:40:16.600 --> 00:40:19.560]   There is this long, fixed part of the prompt,
[00:40:19.560 --> 00:40:22.440]   which is a query specification, which
[00:40:22.440 --> 00:40:27.040]   is just a bit of a programming, like a very terse programming
[00:40:27.040 --> 00:40:30.360]   guide to the Honeycomb query language.
[00:40:30.360 --> 00:40:37.840]   There's some tips, and there is some few-shot examples
[00:40:37.840 --> 00:40:41.400]   of queries, of questions, or user queries, and then
[00:40:41.400 --> 00:40:42.880]   Honeycomb queries.
[00:40:42.880 --> 00:40:44.520]   So there's a few-shot examples.
[00:40:44.520 --> 00:40:48.200]   And then finally, this is a completion model.
[00:40:48.200 --> 00:40:51.040]   So when Honeycomb launches, they use the completion API,
[00:40:51.040 --> 00:40:51.960]   and so the chat API.
[00:40:51.960 --> 00:40:55.360]   And so they're just completing this
[00:40:55.360 --> 00:40:58.520]   based on the user's question, which is templated.
[00:40:58.520 --> 00:41:04.640]   So the interesting thing is, you could
[00:41:04.640 --> 00:41:08.360]   see that there's a lot of stuff in this prompt.
[00:41:08.360 --> 00:41:11.560]   All of this stuff is fixed every single time
[00:41:11.560 --> 00:41:13.200]   in this particular situation.
[00:41:13.200 --> 00:41:18.560]   So the few-shot examples, plus the tips, plus the--
[00:41:18.560 --> 00:41:19.920]   sorry, I didn't go over the tips.
[00:41:19.920 --> 00:41:22.000]   The tips are just additional instructions.
[00:41:22.000 --> 00:41:24.880]   So all of this stuff is fixed, except for the columns
[00:41:24.880 --> 00:41:25.760]   and the question.
[00:41:25.760 --> 00:41:28.240]   So that's a lot of boilerplate to be sending
[00:41:28.240 --> 00:41:29.920]   to a large language model.
[00:41:29.920 --> 00:41:33.040]   But then also, it's hard to specify everything
[00:41:33.040 --> 00:41:34.640]   you want in this prompt.
[00:41:34.640 --> 00:41:37.280]   No matter how hard you try, you hit a wall.
[00:41:37.280 --> 00:41:43.280]   And that's where fine-tuning moved the needle.
[00:41:43.280 --> 00:41:46.560]   So Honeycomb launched this product.
[00:41:46.560 --> 00:41:49.000]   There's a link to the blog post.
[00:41:49.000 --> 00:41:51.880]   It's kind of neat to read it.
[00:41:51.880 --> 00:41:54.400]   And yeah, it just talks about the same thing.
[00:41:54.400 --> 00:41:58.520]   You type in a natural language query and outcomes,
[00:41:58.520 --> 00:42:02.400]   how comes Honeycomb query, and you can read about it.
[00:42:02.400 --> 00:42:04.920]   I don't want to go too deeply into that.
[00:42:04.920 --> 00:42:09.840]   So the goal in this case was to encourage more users
[00:42:09.840 --> 00:42:10.680]   to write queries.
[00:42:10.680 --> 00:42:16.360]   So the bar isn't super high in terms of it has to be perfect.
[00:42:16.360 --> 00:42:19.320]   But one thing we had to do is write evals.
[00:42:19.320 --> 00:42:23.520]   So one of the things you should think about is writing evals.
[00:42:23.520 --> 00:42:27.120]   After you do some prompt engineering,
[00:42:27.120 --> 00:42:32.240]   you may prototype with a large language model
[00:42:32.240 --> 00:42:36.600]   just off the shelf, if you can, just
[00:42:36.600 --> 00:42:39.160]   to get an idea of how well it works off the shelf.
[00:42:39.160 --> 00:42:42.440]   So with Honeycomb, so what do I mean by evals?
[00:42:42.440 --> 00:42:45.720]   So I have this blog post about evals.
[00:42:45.720 --> 00:42:48.080]   I won't go through it in too much detail.
[00:42:48.080 --> 00:42:50.880]   But there's different levels of evals.
[00:42:50.880 --> 00:42:55.960]   Level 1 is unit tests, where you write assertions.
[00:42:55.960 --> 00:42:59.600]   And then there's level 2 and level 3.
[00:42:59.600 --> 00:43:03.960]   And I'll be going through level 1 and 2.
[00:43:03.960 --> 00:43:06.720]   Level 3 is A/B testing.
[00:43:06.720 --> 00:43:09.400]   So basically, the idea is you want this virtuous cycle
[00:43:09.400 --> 00:43:12.320]   where you have evaluation at the center.
[00:43:12.320 --> 00:43:14.120]   And the Honeycomb example is actually
[00:43:14.120 --> 00:43:15.960]   a really good use case, because it's
[00:43:15.960 --> 00:43:19.240]   very narrow and simplified.
[00:43:19.240 --> 00:43:25.320]   And it allows you to get what I'm talking about.
[00:43:25.320 --> 00:43:28.040]   So basically, you don't have to understand this code.
[00:43:28.040 --> 00:43:32.480]   But just know that for the level 1 evals,
[00:43:32.480 --> 00:43:34.200]   when I'm talking about level 1 evals,
[00:43:34.200 --> 00:43:37.160]   I'm talking about assertions and unit tests
[00:43:37.160 --> 00:43:39.560]   that don't involve calls to a large language model.
[00:43:39.560 --> 00:43:42.080]   These are rules that you can think of,
[00:43:42.080 --> 00:43:44.040]   that you can run almost instantaneously
[00:43:44.040 --> 00:43:46.680]   and get feedback about whether your model is
[00:43:46.680 --> 00:43:48.640]   doing the right thing.
[00:43:48.640 --> 00:43:49.960]   And so there's some code here.
[00:43:49.960 --> 00:43:51.380]   And I'm just showing you this code
[00:43:51.380 --> 00:43:53.520]   so you know that it's real, in case
[00:43:53.520 --> 00:43:54.680]   you want to see an example.
[00:43:54.680 --> 00:43:56.600]   But essentially, what I'm doing is
[00:43:56.600 --> 00:43:58.800]   I'm just testing different things about the Honeycomb
[00:43:58.800 --> 00:44:01.040]   query for correctness.
[00:44:01.040 --> 00:44:02.640]   I'm testing if it's valid JSON.
[00:44:02.640 --> 00:44:05.800]   I'm testing if there's invalid columns in the query
[00:44:05.800 --> 00:44:08.440]   based on the schema, if there's invalid filters.
[00:44:08.440 --> 00:44:10.280]   You don't have to know the specifics of this.
[00:44:10.280 --> 00:44:15.720]   You just know that there's lots of different level 1 evals.
[00:44:15.720 --> 00:44:18.120]   And you don't necessarily need to write it like this.
[00:44:18.120 --> 00:44:21.160]   But I'm just giving you an idea that you
[00:44:21.160 --> 00:44:24.680]   need to write these assertions.
[00:44:24.680 --> 00:44:30.600]   And also, just let know, also, that I had
[00:44:30.600 --> 00:44:32.400]   to iterate on this quite a bit.
[00:44:32.400 --> 00:44:34.740]   Don't expect that you're going to get all the assertions
[00:44:34.740 --> 00:44:35.760]   right the first time.
[00:44:35.760 --> 00:44:38.480]   There's an iterative loop where you kind of--
[00:44:38.480 --> 00:44:39.960]   throughout this whole process, you
[00:44:39.960 --> 00:44:41.800]   have to update these level 1 evals.
[00:44:41.800 --> 00:44:43.520]   You'll notice more and more failure modes.
[00:44:43.520 --> 00:44:48.240]   And I had to work really hard on this to get something
[00:44:48.240 --> 00:44:49.120]   that I was happy with.
[00:44:49.120 --> 00:44:55.120]   And then you also want to use these evals.
[00:44:55.120 --> 00:44:57.440]   You want to write them in such a way, these assertions,
[00:44:57.440 --> 00:44:59.480]   that you can use them in different places.
[00:44:59.480 --> 00:45:01.600]   So you not only want to use it for tests.
[00:45:01.600 --> 00:45:04.120]   You also want to use these evals to filter out
[00:45:04.120 --> 00:45:06.040]   bad data for fine tuning.
[00:45:06.040 --> 00:45:07.720]   You want to use it for curation.
[00:45:07.720 --> 00:45:09.720]   And you also want to use it in inference.
[00:45:09.720 --> 00:45:12.000]   So you can do self-healing.
[00:45:12.000 --> 00:45:15.760]   And so I have encapsulated this query checker.
[00:45:15.760 --> 00:45:17.560]   Again, you don't have to know what this is.
[00:45:17.560 --> 00:45:19.000]   It just gives you an idea.
[00:45:19.000 --> 00:45:23.960]   Like, hey, I'm using these assertions in different places.
[00:45:23.960 --> 00:45:26.960]   And because this use case is oversimplified,
[00:45:26.960 --> 00:45:29.880]   this kind of way of organizing your code may not work for you.
[00:45:29.880 --> 00:45:33.760]   You have to do what works for you in that situation.
[00:45:33.760 --> 00:45:37.080]   But just know that it's here.
[00:45:37.080 --> 00:45:38.280]   And I already went over this.
[00:45:38.280 --> 00:45:40.000]   Assertions are not just for tests.
[00:45:40.000 --> 00:45:42.960]   They're also for filtering, and curating, and inference.
[00:45:42.960 --> 00:45:46.200]   And yeah, definitely look at the blog post.
[00:45:46.200 --> 00:45:48.240]   OK.
[00:45:48.240 --> 00:45:50.400]   So one thing that you will often have
[00:45:50.400 --> 00:45:53.800]   to do when you're fine tuning is acquire data.
[00:45:53.800 --> 00:45:56.120]   And a lot of times, you don't have the data
[00:45:56.120 --> 00:45:59.320]   in an applied use case.
[00:45:59.320 --> 00:46:00.160]   So what do you do?
[00:46:00.160 --> 00:46:03.960]   Like, in the Honeycomb, in real life,
[00:46:03.960 --> 00:46:06.480]   my counterpart, Philip, who I was working with,
[00:46:06.480 --> 00:46:09.560]   didn't have lots of data.
[00:46:09.560 --> 00:46:13.480]   He launched this to production.
[00:46:13.480 --> 00:46:15.840]   But then, not only did not have lots of data,
[00:46:15.840 --> 00:46:17.120]   a lot of that data was private.
[00:46:17.120 --> 00:46:19.280]   And I can't see that data.
[00:46:19.280 --> 00:46:23.240]   And so he gave me about 1,000 examples.
[00:46:23.240 --> 00:46:27.840]   And I wanted to set aside a fair amount of those examples
[00:46:27.840 --> 00:46:32.720]   like in the eval set so I could test the model.
[00:46:32.720 --> 00:46:34.480]   So I wasn't really left with much.
[00:46:34.480 --> 00:46:39.440]   And so the question is, OK, what do I do from here?
[00:46:39.440 --> 00:46:43.800]   So a lot of you, if you're in the wild,
[00:46:43.800 --> 00:46:46.080]   and you're trying to build something in large language
[00:46:46.080 --> 00:46:50.080]   models, and you're trying to fine tune it,
[00:46:50.080 --> 00:46:54.480]   it's good to know about how to generate synthetic data.
[00:46:54.480 --> 00:46:56.000]   There's no hard and fast rule, again,
[00:46:56.000 --> 00:46:57.320]   about how many examples you need.
[00:46:57.320 --> 00:47:03.920]   I just generate as many examples as I feasibly can,
[00:47:03.920 --> 00:47:07.400]   just based on intuition, based on how much it costs,
[00:47:07.400 --> 00:47:10.040]   how much time it takes.
[00:47:10.040 --> 00:47:14.280]   I end up generating 30,000 examples synthetically.
[00:47:14.280 --> 00:47:16.400]   But I kind of went overboard.
[00:47:16.400 --> 00:47:17.600]   So you don't have to do that.
[00:47:17.600 --> 00:47:20.440]   Just use your intuition based on your budget and what you have.
[00:47:20.440 --> 00:47:25.320]   So you can do this with prompting.
[00:47:25.320 --> 00:47:27.520]   So let me give you a concrete example.
[00:47:27.520 --> 00:47:30.040]   Because if I just say, hey, you can use a large language model
[00:47:30.040 --> 00:47:32.640]   to synthetically generate data, you're like, well, how?
[00:47:32.640 --> 00:47:33.440]   What does that mean?
[00:47:33.440 --> 00:47:35.360]   And I think for every use case is different.
[00:47:35.360 --> 00:47:37.560]   But let me show you what we did for Honeycomb.
[00:47:37.560 --> 00:47:41.880]   So the prompt is basically the same exact prompt
[00:47:41.880 --> 00:47:44.280]   that you've seen before, except there's
[00:47:44.280 --> 00:47:47.520]   a second part that says, OK, you are
[00:47:47.520 --> 00:47:50.720]   given the following three inputs--
[00:47:50.720 --> 00:47:53.640]   natural language query, a list of candidate columns,
[00:47:53.640 --> 00:47:54.400]   and the query.
[00:47:54.400 --> 00:47:56.360]   Your goal is to generate correct variations
[00:47:56.360 --> 00:47:59.360]   of the combination of NLQ candidate columns and query
[00:47:59.360 --> 00:48:01.120]   to build a synthetic data set.
[00:48:01.120 --> 00:48:02.640]   You can build a synthetic data set
[00:48:02.640 --> 00:48:06.120]   by rewording the query and substituting the column name.
[00:48:06.120 --> 00:48:09.360]   Response should be JSON with the following keys,
[00:48:09.360 --> 00:48:12.360]   so on and so forth.
[00:48:12.360 --> 00:48:16.320]   And then basically, yeah, I'm giving it the inputs now
[00:48:16.320 --> 00:48:19.000]   and then saying, please basically perform
[00:48:19.000 --> 00:48:20.320]   data augmentation.
[00:48:20.320 --> 00:48:24.040]   So substitute, like rewrite the natural language query,
[00:48:24.040 --> 00:48:27.520]   substitute the columns, and substitute the query.
[00:48:27.520 --> 00:48:30.080]   And basically, I'm able to generate lots and lots
[00:48:30.080 --> 00:48:31.840]   of synthetic data this way.
[00:48:31.840 --> 00:48:36.080]   Now, you might be wondering, is that good data?
[00:48:36.080 --> 00:48:38.240]   Is it duplicated, like all this stuff?
[00:48:38.240 --> 00:48:38.960]   Yes.
[00:48:38.960 --> 00:48:41.920]   And you have to clean it up, which
[00:48:41.920 --> 00:48:44.280]   I'll talk about in a second.
[00:48:44.280 --> 00:48:46.080]   But just know that, for example, you
[00:48:46.080 --> 00:48:48.440]   want to use those level one assertions as your first line
[00:48:48.440 --> 00:48:48.960]   of defense.
[00:48:48.960 --> 00:48:50.240]   A lot of the stuff that doesn't come out of this
[00:48:50.240 --> 00:48:52.960]   is going to be junk, maybe, or some amount of it.
[00:48:52.960 --> 00:48:53.840]   You want to get rid of it.
[00:48:53.840 --> 00:48:56.320]   So the level one assertion is already going to help you.
[00:48:56.320 --> 00:48:59.480]   And it's going to help you throughout this whole thing.
[00:48:59.480 --> 00:49:01.400]   OK, so you have a way of getting lots of data.
[00:49:01.400 --> 00:49:02.280]   This is how you do it.
[00:49:02.280 --> 00:49:03.880]   I'm not going to show you the code of doing that.
[00:49:03.880 --> 00:49:05.160]   It's fairly straightforward.
[00:49:05.160 --> 00:49:10.280]   It's like, use your favorite large model to do this.
[00:49:10.280 --> 00:49:12.680]   Use the most powerful model you feel comfortable with
[00:49:12.680 --> 00:49:16.680]   to help you generate the synthetic data.
[00:49:16.680 --> 00:49:18.760]   And then, OK, so the next step in this
[00:49:18.760 --> 00:49:20.640]   is preparing the data for Axolotl.
[00:49:20.640 --> 00:49:27.960]   So usually, what I do is I go through a run.
[00:49:27.960 --> 00:49:29.240]   I run all the way through.
[00:49:29.240 --> 00:49:32.000]   And I see what's going wrong.
[00:49:32.000 --> 00:49:34.760]   And then I come back and improve it.
[00:49:34.760 --> 00:49:37.080]   You don't want to just try to make your data perfect
[00:49:37.080 --> 00:49:40.680]   the first time and then go through it.
[00:49:40.680 --> 00:49:43.000]   You want to go all the way through, see some predictions,
[00:49:43.000 --> 00:49:45.200]   make sure the plumbing works, et cetera.
[00:49:45.200 --> 00:49:48.320]   And then you can come back and curate and filter the data.
[00:49:48.320 --> 00:49:51.320]   That's what I recommend, because you can get stuck.
[00:49:51.320 --> 00:49:53.720]   It's good to know where the problems are and have an idea.
[00:49:58.520 --> 00:50:02.520]   So you want to prepare your data to look like this, in this case,
[00:50:02.520 --> 00:50:07.640]   because I'm using the shared GPT alpaca format.
[00:50:07.640 --> 00:50:09.060]   And I'll tell you what that means.
[00:50:09.060 --> 00:50:10.840]   Basically, if you're in Axolotl, there's
[00:50:10.840 --> 00:50:13.720]   this config, shared GPT, and alpaca.
[00:50:13.720 --> 00:50:19.640]   And let me just open the docs so you can see that.
[00:50:19.640 --> 00:50:21.280]   So there's the data set formats.
[00:50:21.280 --> 00:50:23.120]   This is the Axolotl docs.
[00:50:23.120 --> 00:50:25.880]   There's different formats.
[00:50:25.880 --> 00:50:27.760]   I'm using a conversation format.
[00:50:27.760 --> 00:50:29.960]   And there's a shared GPT.
[00:50:29.960 --> 00:50:34.240]   And you can see, shared GPT, you have to structure your data like this.
[00:50:34.240 --> 00:50:38.000]   You have conversations, and then you have from and value.
[00:50:38.000 --> 00:50:44.280]   And you have different roles, like the from can either be human or GPT,
[00:50:44.280 --> 00:50:45.200]   and then the value.
[00:50:45.200 --> 00:50:49.680]   You can also have a system prompt, which I do have in this case,
[00:50:49.680 --> 00:50:50.520]   which I'll show you.
[00:50:50.520 --> 00:50:52.880]   But anyways, you can see that follows that here.
[00:50:52.880 --> 00:50:56.960]   I have this conversation where I have a system prompt,
[00:50:56.960 --> 00:50:59.200]   then a human, then GPT.
[00:50:59.200 --> 00:51:01.600]   Now, why is that?
[00:51:01.600 --> 00:51:06.560]   Well, that's the way that Axolotl expects your data for this format.
[00:51:06.560 --> 00:51:09.600]   But also, it's important because if you remember
[00:51:09.600 --> 00:51:16.600]   Dan talking about the train on inputs, not training on inputs,
[00:51:16.600 --> 00:51:18.640]   so this is considered an input.
[00:51:18.640 --> 00:51:23.680]   The system role in the human question is considered inputs.
[00:51:23.680 --> 00:51:27.680]   And the output is considered is this, the query.
[00:51:27.680 --> 00:51:31.240]   And so what we're doing is we are only penalizing the model.
[00:51:31.240 --> 00:51:36.280]   We're forcing the model to basically learn to get the right query
[00:51:36.280 --> 00:51:42.280]   and not trying to have it predict what the question is, if that makes sense.
[00:51:42.280 --> 00:51:44.680]   So you organize your data like this to this JSONL.
[00:51:44.680 --> 00:51:51.000]   And let's take a look at the config.
[00:51:51.000 --> 00:51:54.080]   So the thing you want to pay attention to here,
[00:51:54.080 --> 00:51:55.600]   Dan already went over the config.
[00:51:55.600 --> 00:51:58.480]   But in this case, change the data set.
[00:51:58.480 --> 00:52:00.480]   This is a local data set.
[00:52:00.480 --> 00:52:03.200]   But I have this, basically, the sample data.
[00:52:03.200 --> 00:52:06.200]   And I have this synthetic queries.
[00:52:06.200 --> 00:52:08.440]   And you can look at what that looks like if you want.
[00:52:08.440 --> 00:52:10.320]   It's in that GitHub repo at this path.
[00:52:10.320 --> 00:52:16.400]   And then also, the train on inputs is also false.
[00:52:16.400 --> 00:52:20.640]   There's a key in here, train on inputs, which I'll let you find.
[00:52:20.640 --> 00:52:23.720]   I don't want to try to hunt for this right now.
[00:52:23.720 --> 00:52:26.720]   It's right here, train on inputs.
[00:52:26.720 --> 00:52:28.680]   And then also, you want to change--
[00:52:28.680 --> 00:52:30.760]   if you're going to run this example, which you can,
[00:52:30.760 --> 00:52:35.120]   and I'll show you how, you need to change the following things
[00:52:35.120 --> 00:52:36.280]   in your config.
[00:52:36.280 --> 00:52:39.800]   You won't be able to access my Weights and Biases account.
[00:52:39.800 --> 00:52:43.400]   And you won't be able to access my Hugging Face account.
[00:52:43.400 --> 00:52:45.360]   You probably want to create your own.
[00:52:45.360 --> 00:52:48.960]   And so what Axolotl does is you can log--
[00:52:48.960 --> 00:52:51.000]   as Dan mentioned, you can log all the training metrics
[00:52:51.000 --> 00:52:52.480]   to Weights and Biases.
[00:52:52.480 --> 00:52:57.960]   And then also, you can also put in a Hugging Face model repo.
[00:52:57.960 --> 00:53:04.280]   And it will upload your model to that repo, which is super handy.
[00:53:04.280 --> 00:53:07.400]   It'll do that at the very end.
[00:53:07.400 --> 00:53:08.800]   And I'll show you what all this--
[00:53:08.800 --> 00:53:11.000]   I'll show you some examples of what this looks like.
[00:53:11.000 --> 00:53:12.680]   OK, so prepared the data.
[00:53:12.680 --> 00:53:14.080]   You got your config file.
[00:53:14.080 --> 00:53:16.120]   Now what do you do?
[00:53:16.120 --> 00:53:17.680]   So what I like to do is I don't ever
[00:53:17.680 --> 00:53:21.120]   jump straight into training, ever, because I'm dumb,
[00:53:21.120 --> 00:53:23.760]   and I make a lot of mistakes in data set preparation.
[00:53:23.760 --> 00:53:25.200]   I always make-- do something wrong.
[00:53:25.200 --> 00:53:29.440]   And honestly, I think a lot of people do something wrong here.
[00:53:29.440 --> 00:53:31.560]   And so what I like to do is look at the data.
[00:53:31.560 --> 00:53:35.520]   I like to double check how Axolotl is preparing the data.
[00:53:35.520 --> 00:53:39.720]   And the way I do that is I do this Axolotl preprocess command.
[00:53:39.720 --> 00:53:46.200]   And that will basically flatten the data
[00:53:46.200 --> 00:53:49.760]   and assemble it in the right format.
[00:53:49.760 --> 00:53:52.560]   You can see all the different commands by using help.
[00:53:52.560 --> 00:53:56.640]   So I just show that here, just for reference.
[00:53:56.640 --> 00:54:00.600]   And so I like to look at the data manually.
[00:54:00.600 --> 00:54:03.040]   There's that debug thing that Dan showed.
[00:54:03.040 --> 00:54:06.880]   But I like to look at it manually,
[00:54:06.880 --> 00:54:09.760]   just so I can play with it a bit more,
[00:54:09.760 --> 00:54:13.040]   manipulate it, inspect things.
[00:54:13.040 --> 00:54:17.400]   So basically, what happens is when you preprocess the data,
[00:54:17.400 --> 00:54:20.240]   Axolotl dumps that data by default
[00:54:20.240 --> 00:54:23.520]   into this last run prepared directory.
[00:54:23.520 --> 00:54:27.600]   And that is a HuggingFaceDatasets format.
[00:54:27.600 --> 00:54:29.840]   And so you can load that HuggingFaceDatasets format
[00:54:29.840 --> 00:54:30.480]   and inspect it.
[00:54:30.480 --> 00:54:33.840]   And that's what I'm doing here with this code.
[00:54:33.840 --> 00:54:38.720]   Basically, you can see it has sort of flattened that JSONL
[00:54:38.720 --> 00:54:41.800]   into a format that looks like this.
[00:54:41.800 --> 00:54:44.720]   And that is the alpaca format.
[00:54:44.720 --> 00:54:46.280]   Just like how Dan showed earlier,
[00:54:46.280 --> 00:54:52.120]   you have this instruction and then response.
[00:54:52.120 --> 00:54:58.720]   And so yeah, what I recommend is check multiple examples,
[00:54:58.720 --> 00:54:59.840]   make sure it looks right.
[00:54:59.840 --> 00:55:02.340]   Make sure you didn't put the wrong thing in the wrong place
[00:55:02.340 --> 00:55:05.480]   or have things in there that you didn't intend in your data.
[00:55:05.480 --> 00:55:06.520]   Happens all the time.
[00:55:09.560 --> 00:55:11.960]   One thing that I'll mention is, yeah,
[00:55:11.960 --> 00:55:13.360]   there are these spaces right here.
[00:55:13.360 --> 00:55:16.840]   You might be wondering, what the hell is that?
[00:55:16.840 --> 00:55:18.760]   It's a little bit of a tricky issue.
[00:55:18.760 --> 00:55:21.160]   It's kind of some artifact about the way
[00:55:21.160 --> 00:55:27.960]   Axolotl assembles tokens.
[00:55:27.960 --> 00:55:30.420]   I don't know if Wing wants to say something about this yet.
[00:55:30.420 --> 00:55:32.760]   But I found it not to be an issue
[00:55:32.760 --> 00:55:38.080]   as long as you're consistent with inference time.
[00:55:38.080 --> 00:55:39.280]   And I'll talk more about that.
[00:55:39.280 --> 00:55:41.000]   And I have a blog post about that as well.
[00:55:41.000 --> 00:55:46.600]   OK, there's also verbose debugging,
[00:55:46.600 --> 00:55:48.000]   which Dan already covered.
[00:55:48.000 --> 00:55:54.320]   And basically, yeah, you could just debug flag.
[00:55:54.320 --> 00:55:55.720]   The special tokens are here.
[00:55:55.720 --> 00:55:57.320]   And that's worth paying attention to.
[00:55:57.320 --> 00:55:58.680]   But there's the red, green.
[00:55:58.680 --> 00:56:00.920]   I'm not going to go through that again.
[00:56:00.920 --> 00:56:04.800]   And then, yeah, it's always good to spot check
[00:56:04.800 --> 00:56:07.160]   what these tokens are and if it's correct.
[00:56:07.160 --> 00:56:09.240]   So for example, what is this token?
[00:56:09.240 --> 00:56:11.520]   You might be wondering, you haven't done this before.
[00:56:11.520 --> 00:56:12.720]   What the hell is that token?
[00:56:12.720 --> 00:56:13.920]   Is that wrong?
[00:56:13.920 --> 00:56:17.000]   OK, that's a new line.
[00:56:17.000 --> 00:56:20.960]   But yeah, if you want to go into what's
[00:56:20.960 --> 00:56:24.560]   going on with the tokens, there is this blog post here.
[00:56:24.560 --> 00:56:27.040]   I'm not going to go through it now, but just tokenization
[00:56:27.040 --> 00:56:28.800]   gotchas.
[00:56:28.800 --> 00:56:30.720]   As an exercise for y'all, you might
[00:56:30.720 --> 00:56:33.160]   want to go through this blog post as a homework
[00:56:33.160 --> 00:56:38.080]   and take a look and see if it's something
[00:56:38.080 --> 00:56:40.440]   that you find that matters.
[00:56:40.440 --> 00:56:43.240]   I was really super paranoid about these small things
[00:56:43.240 --> 00:56:45.200]   like spaces, but I found that it didn't matter.
[00:56:45.200 --> 00:56:47.800]   And I actually discuss this a lot with Wing.
[00:56:47.800 --> 00:56:49.720]   But Wing, do you have any opinions on this?
[00:56:49.720 --> 00:56:53.800]   Is he here?
[00:56:53.800 --> 00:56:57.000]   Might not be here.
[00:56:57.000 --> 00:57:00.200]   No worries?
[00:57:00.200 --> 00:57:02.640]   OK, I'm just going to go straight on to the next.
[00:57:02.640 --> 00:57:09.920]   So OK, that was data set preparation.
[00:57:09.920 --> 00:57:14.040]   Now we're going to talk about training.
[00:57:14.040 --> 00:57:16.280]   We've already seen the config file.
[00:57:16.280 --> 00:57:20.480]   The config file is also located at this location,
[00:57:20.480 --> 00:57:22.280]   which I will go through.
[00:57:22.280 --> 00:57:24.760]   You can see it's been uploaded to Hugging Face.
[00:57:24.760 --> 00:57:26.520]   There is a link in the notebook, so you
[00:57:26.520 --> 00:57:29.560]   don't have to memorize what you're seeing on my screen.
[00:57:29.560 --> 00:57:35.400]   To run training, you run this Accelerate Launch Axolotl
[00:57:35.400 --> 00:57:36.120]   command.
[00:57:36.120 --> 00:57:38.200]   And Zach is going to be talking about Accelerate.
[00:57:38.200 --> 00:57:42.400]   I don't want to go into that deep rabbit hole right now.
[00:57:42.400 --> 00:57:44.440]   I'll just let Zach talk about Accelerate in a bit.
[00:57:44.440 --> 00:57:51.520]   If you notice, I have a Weights and Biases config here.
[00:57:51.520 --> 00:57:53.280]   And this Weights and Biases entity
[00:57:53.280 --> 00:57:54.880]   is just basically like a GitHub org,
[00:57:54.880 --> 00:57:57.120]   and the project is basically like the repo.
[00:57:57.120 --> 00:58:01.240]   And so when you do that, Axolotl will upload--
[00:58:01.240 --> 00:58:04.280]   you can log your training runs to Weights and Biases.
[00:58:04.280 --> 00:58:06.800]   Let me show you Weights and Biases real quick.
[00:58:06.800 --> 00:58:08.880]   So Weights and Biases looks like this.
[00:58:08.880 --> 00:58:10.760]   It's a bunch of runs.
[00:58:10.760 --> 00:58:14.520]   And you can just log your runs and the results,
[00:58:14.520 --> 00:58:17.120]   look at your training loss curves.
[00:58:17.120 --> 00:58:20.960]   I'm not going to spend too much time on this.
[00:58:20.960 --> 00:58:25.240]   But just know that it's there if you want to look at it.
[00:58:25.240 --> 00:58:28.080]   So basically, with training, what did I do?
[00:58:28.080 --> 00:58:29.320]   I tried different parameters.
[00:58:29.320 --> 00:58:30.680]   I varied the learning rate.
[00:58:30.680 --> 00:58:34.600]   So first of all, I took a--
[00:58:34.600 --> 00:58:36.560]   so it was Mistral 7b.
[00:58:36.560 --> 00:58:38.640]   So I went into the examples.
[00:58:38.640 --> 00:58:43.560]   I asked in the Discord, so on and so forth, what is the best--
[00:58:43.560 --> 00:58:46.840]   what's the best config for Mistral?
[00:58:46.840 --> 00:58:50.400]   And I started with that.
[00:58:50.400 --> 00:58:51.880]   And so I varied the learning rate.
[00:58:51.880 --> 00:58:54.160]   I tried different learning rate schedulers.
[00:58:54.160 --> 00:58:57.200]   I actually tried different distributed schemes,
[00:58:57.200 --> 00:58:59.880]   like using DeepSpeed, like DeepSpeed 0, 1, 2, 3,
[00:58:59.880 --> 00:59:01.920]   just to test stuff.
[00:59:01.920 --> 00:59:03.600]   I mean, not that it matters.
[00:59:03.600 --> 00:59:06.640]   But actually, this is a small model,
[00:59:06.640 --> 00:59:09.520]   so it fit on my GPU just fine.
[00:59:09.520 --> 00:59:11.520]   But yeah, I mainly just varied the learning rate
[00:59:11.520 --> 00:59:13.240]   and the bat size.
[00:59:13.240 --> 00:59:16.800]   Another thing is there's sample packing
[00:59:16.800 --> 00:59:21.960]   that you might want to try to save GPU space,
[00:59:21.960 --> 00:59:25.280]   or to save the amount of VRAM you need,
[00:59:25.280 --> 00:59:29.120]   or increase throughput.
[00:59:29.120 --> 00:59:31.560]   But Dan will upload a video for that,
[00:59:31.560 --> 00:59:34.440]   or talk about that in a little bit more detail later on.
[00:59:34.440 --> 00:59:39.440]   So when the training is done, it's uploaded.
[00:59:39.440 --> 00:59:40.720]   If you put your HuggingFace ID, it's
[00:59:40.720 --> 00:59:42.440]   uploaded to HuggingFace, which is here.
[00:59:42.440 --> 00:59:46.440]   So this example of this model is here.
[00:59:46.440 --> 00:59:48.600]   You don't need to know what is here.
[00:59:48.600 --> 00:59:50.600]   I don't want you to kind of--
[00:59:50.600 --> 00:59:52.320]   you can look at this later.
[00:59:52.320 --> 00:59:55.120]   And I'll go through some of this code in a bit.
[00:59:55.120 --> 00:59:57.600]   So the next thing you want to do after you train your model
[00:59:57.600 --> 01:00:00.760]   is to sanity check it.
[01:00:00.760 --> 01:00:02.440]   And there's a lot of different ways
[01:00:02.440 --> 01:00:03.960]   you can sanity check your model.
[01:00:03.960 --> 01:00:10.040]   I like to-- you can use the way that Dan mentioned earlier
[01:00:10.040 --> 01:00:12.320]   by using axolotl directly.
[01:00:12.320 --> 01:00:17.680]   However, I like to actually use code to up to--
[01:00:17.680 --> 01:00:22.160]   and use the HuggingFace transformers to actually make
[01:00:22.160 --> 01:00:22.840]   this work.
[01:00:22.840 --> 01:00:26.800]   Hey, Dan, I think, like, Wing may be trying to open his
[01:00:26.800 --> 01:00:27.800]   camera, potentially.
[01:00:27.800 --> 01:00:28.360]   I don't know.
[01:00:28.360 --> 01:00:36.320]   OK, so sanity check the model.
[01:00:36.320 --> 01:00:37.960]   This is the HuggingFace repo where
[01:00:37.960 --> 01:00:40.120]   the model is uploaded into.
[01:00:40.120 --> 01:00:42.440]   Don't be confused that this says, like, Parlance Labs,
[01:00:42.440 --> 01:00:44.160]   and the other config says Hamil.
[01:00:44.160 --> 01:00:46.360]   That's because I changed the name of the repo,
[01:00:46.360 --> 01:00:48.440]   and I didn't want to break the links.
[01:00:48.440 --> 01:00:53.360]   But yeah, so this is just code about basically pulling
[01:00:53.360 --> 01:00:55.480]   that model from HuggingFace.
[01:00:55.480 --> 01:00:58.400]   And then this is the template.
[01:00:58.400 --> 01:01:01.000]   So another reason why sanity check things this way
[01:01:01.000 --> 01:01:04.640]   is I want to make sure that I understand the template
[01:01:04.640 --> 01:01:06.600]   and that it works.
[01:01:06.600 --> 01:01:08.480]   Because I had my own, like, basically--
[01:01:08.480 --> 01:01:10.320]   yeah, and the way I want to do is
[01:01:10.320 --> 01:01:12.400]   I just want two inputs, a natural language query
[01:01:12.400 --> 01:01:13.240]   and the columns.
[01:01:13.240 --> 01:01:14.680]   There's different ways to do this.
[01:01:14.680 --> 01:01:18.560]   You can use-- HuggingFace has, like, a templating system
[01:01:18.560 --> 01:01:19.280]   that you can use.
[01:01:19.280 --> 01:01:20.760]   I'm not going to go into it.
[01:01:20.760 --> 01:01:24.440]   But I like to, like, make sure I understand the template.
[01:01:24.440 --> 01:01:26.760]   And so that's what I have here is I have this template.
[01:01:26.760 --> 01:01:29.400]   It's basically the same thing.
[01:01:29.400 --> 01:01:33.360]   And this is just code to, like, run it.
[01:01:33.360 --> 01:01:36.280]   But basically, it's just, like, sanity checking examples.
[01:01:36.280 --> 01:01:38.480]   OK, so nothing too crazy going on here.
[01:01:38.480 --> 01:01:40.240]   I just have some natural language queries
[01:01:40.240 --> 01:01:41.880]   and some schemas, and I'm checking
[01:01:41.880 --> 01:01:45.200]   to make sure that it works.
[01:01:45.200 --> 01:01:46.680]   That's what you should do.
[01:01:46.680 --> 01:01:47.640]   That's the first thing you should do.
[01:01:47.640 --> 01:01:48.480]   OK, great.
[01:01:48.480 --> 01:01:49.680]   So we've done all this stuff.
[01:01:49.680 --> 01:01:50.960]   We trained the model.
[01:01:50.960 --> 01:01:53.400]   We sanity checked that at least, like, the plumbing works,
[01:01:53.400 --> 01:01:56.880]   and some results maybe look plausible.
[01:01:56.880 --> 01:01:58.800]   So the next thing you want to do is, like--
[01:01:58.800 --> 01:02:01.080]   so the question is, like, is this any good?
[01:02:01.080 --> 01:02:01.800]   Yeah, it passes.
[01:02:01.800 --> 01:02:03.880]   Like, you can see, like, these level 1 evals.
[01:02:03.880 --> 01:02:06.280]   You can track the different metrics of the level 1 evals.
[01:02:06.280 --> 01:02:08.600]   You can know, like, which assertions are failing,
[01:02:08.600 --> 01:02:10.280]   how-- you know, like, what kind of errors
[01:02:10.280 --> 01:02:11.920]   are you getting the most.
[01:02:11.920 --> 01:02:14.280]   That's all good.
[01:02:14.280 --> 01:02:16.960]   But then, like, beyond the level 1 assertions,
[01:02:16.960 --> 01:02:21.320]   after you conquer those, like, are these, like, good or bad?
[01:02:21.320 --> 01:02:23.600]   So when I shared--
[01:02:23.600 --> 01:02:27.280]   so I launched this model onto Replicate for inference.
[01:02:27.280 --> 01:02:28.820]   And we'll go through inference later,
[01:02:28.820 --> 01:02:31.320]   so don't want to, like, get stuck on that.
[01:02:31.320 --> 01:02:33.160]   It's, like, you know, and allowed--
[01:02:33.160 --> 01:02:35.120]   it did some sanity-- more sanity checking.
[01:02:35.120 --> 01:02:41.240]   And basically, like, Philip did some sanity checking and said,
[01:02:41.240 --> 01:02:45.360]   OK, this model is OK, but it's not great.
[01:02:45.360 --> 01:02:48.280]   It's still making some mistakes in some places.
[01:02:48.280 --> 01:02:51.360]   And actually, it turns out that the data
[01:02:51.360 --> 01:02:58.480]   that we used to expand, that data wasn't great either.
[01:02:58.480 --> 01:03:01.440]   And this will happen all the time.
[01:03:01.440 --> 01:03:05.480]   And you might find this when you're doing--
[01:03:05.480 --> 01:03:07.840]   and, like, basically, you have to do some error analysis
[01:03:07.840 --> 01:03:10.880]   and figure out, like, OK, if a result isn't great,
[01:03:10.880 --> 01:03:12.020]   like, why is that?
[01:03:12.020 --> 01:03:14.480]   And one way to do that is, like, to look at the data,
[01:03:14.480 --> 01:03:16.800]   look at the training data, try to debug.
[01:03:16.800 --> 01:03:18.760]   Like, in this case, I looked at similar queries
[01:03:18.760 --> 01:03:21.280]   in the training data and tried to see what was happening.
[01:03:21.280 --> 01:03:23.380]   And we found that, OK, like, actually, the training
[01:03:23.380 --> 01:03:26.480]   data could be better.
[01:03:26.480 --> 01:03:29.320]   Then, you know, like, things are passing the level 1 test just
[01:03:29.320 --> 01:03:32.040]   fine, but they're not, like, the greatest queries.
[01:03:32.040 --> 01:03:34.440]   They're syntactically correct.
[01:03:34.440 --> 01:03:35.720]   And so what do we do now?
[01:03:35.720 --> 01:03:39.960]   So, like, one thing you might be wondering is, OK, like,
[01:03:39.960 --> 01:03:40.920]   are we stuck?
[01:03:40.920 --> 01:03:42.520]   Do we have to stop here?
[01:03:42.520 --> 01:03:45.440]   Like, the data is meh.
[01:03:45.440 --> 01:03:47.720]   Like, and Philip doesn't have time
[01:03:47.720 --> 01:03:50.360]   to sit there and label a bunch of data
[01:03:50.360 --> 01:03:54.720]   or write better queries because he doesn't have time.
[01:03:54.720 --> 01:03:58.040]   So what do you do now?
[01:03:58.040 --> 01:04:01.360]   OK, like, what you can do is, basically,
[01:04:01.360 --> 01:04:04.400]   you want to try to encode the knowledge of Philip
[01:04:04.400 --> 01:04:06.640]   and his opinions into a model.
[01:04:06.640 --> 01:04:08.440]   Like, you want to, like, see, like,
[01:04:08.440 --> 01:04:14.000]   can you have, like, Philip as an AI in this situation?
[01:04:14.000 --> 01:04:20.640]   So what I did is I started building an LLM as a judge.
[01:04:20.640 --> 01:04:27.640]   And basically, it's the same exact original prompt,
[01:04:27.640 --> 01:04:30.000]   but basically, like, that you've seen before,
[01:04:30.000 --> 01:04:33.400]   but with an instruction that you are going
[01:04:33.400 --> 01:04:38.240]   to be a query validator, OK?
[01:04:38.240 --> 01:04:39.960]   You are an expert query evaluator
[01:04:39.960 --> 01:04:41.920]   that has advanced capabilities, judges a query good or not,
[01:04:41.920 --> 01:04:43.120]   blah, blah, blah.
[01:04:43.120 --> 01:04:45.080]   And then there's a bunch of few-shot examples
[01:04:45.080 --> 01:04:52.880]   here of, you know, like, inputs, NLQ, columns, query,
[01:04:52.880 --> 01:04:55.000]   and critiques.
[01:04:55.000 --> 01:05:00.200]   And basically, what I did is I did a bunch of--
[01:05:00.200 --> 01:05:02.600]   so how did I get this?
[01:05:02.600 --> 01:05:05.760]   In this case, I used a very uncool, low-technology
[01:05:05.760 --> 01:05:08.120]   technique by using a spreadsheet.
[01:05:08.120 --> 01:05:12.800]   And I sent Philip a spreadsheet every day for a few weeks
[01:05:12.800 --> 01:05:15.680]   and had him write critiques.
[01:05:15.680 --> 01:05:20.760]   And over time, what I did is I aligned the model as much
[01:05:20.760 --> 01:05:24.360]   as possible with Philip so that it was agreeing with him
[01:05:24.360 --> 01:05:26.480]   in the critiques it was writing.
[01:05:26.480 --> 01:05:29.000]   And I kind of kept tweaking the few-shot examples
[01:05:29.000 --> 01:05:32.960]   and the instructions until we were both satisfied
[01:05:32.960 --> 01:05:38.560]   that this LLM as a judge was doing a good job.
[01:05:38.560 --> 01:05:42.080]   And the thing that was really good about this is, like--
[01:05:42.080 --> 01:05:44.520]   and so I talk about this a little bit more detail
[01:05:44.520 --> 01:05:48.000]   in the blog post when we talk about level 2 human and model
[01:05:48.000 --> 01:05:50.160]   eval.
[01:05:50.160 --> 01:05:51.120]   I don't want to go--
[01:05:51.120 --> 01:05:52.840]   there's a lot you can say about this.
[01:05:52.840 --> 01:05:54.960]   Like, there's different ways you could do this.
[01:05:54.960 --> 01:05:56.720]   I just want to give you an idea so
[01:05:56.720 --> 01:05:59.640]   that you have it, like, the general process in your mind
[01:05:59.640 --> 01:06:05.160]   and you know that this is a tool in your toolbox.
[01:06:05.160 --> 01:06:07.280]   It's impossible to teach everything
[01:06:07.280 --> 01:06:10.520]   I know about it in such a small session.
[01:06:10.520 --> 01:06:14.040]   But what I will say is, yeah, like,
[01:06:14.040 --> 01:06:16.560]   when you have the result of this,
[01:06:16.560 --> 01:06:18.640]   you get a bunch of critiques.
[01:06:18.640 --> 01:06:22.800]   And you can use those critiques to actually make
[01:06:22.800 --> 01:06:23.880]   the data better.
[01:06:23.880 --> 01:06:26.160]   And you can use the same LLM as a judge
[01:06:26.160 --> 01:06:29.200]   to filter and curate the data, like filter out bad queries.
[01:06:29.200 --> 01:06:31.800]   Hey, like, try to make the data better.
[01:06:31.800 --> 01:06:34.720]   Given a critique, can you make the query better?
[01:06:34.720 --> 01:06:39.560]   If it still can't make the query better, then you filter it out.
[01:06:39.560 --> 01:06:46.320]   So that's kind of like a sort of what we went through.
[01:06:49.120 --> 01:06:53.240]   And so basically from there, you can curate your data.
[01:06:53.240 --> 01:06:56.040]   So like what I mentioned before, first thing
[01:06:56.040 --> 01:06:58.520]   is you can, like, fix the bad data.
[01:06:58.520 --> 01:07:00.000]   Again, using a large language model,
[01:07:00.000 --> 01:07:03.240]   it's like you're giving the following inputs in a critique.
[01:07:03.240 --> 01:07:05.440]   And then it's output the improved query
[01:07:05.440 --> 01:07:09.600]   and just output the improved query.
[01:07:09.600 --> 01:07:11.280]   That's one way you could try to, like,
[01:07:11.280 --> 01:07:13.080]   increase the quality of the data.
[01:07:13.080 --> 01:07:16.400]   But then also, like I mentioned, you want to filter the data.
[01:07:16.400 --> 01:07:18.920]   There's many different ways to filter the data.
[01:07:18.920 --> 01:07:20.680]   And when you talk about data set curation,
[01:07:20.680 --> 01:07:22.440]   there's a lot of things that you can do.
[01:07:22.440 --> 01:07:27.600]   And like filtering, again, you want
[01:07:27.600 --> 01:07:30.920]   to use both your level one evals that I mentioned,
[01:07:30.920 --> 01:07:32.120]   like those assertions.
[01:07:32.120 --> 01:07:34.920]   You want to use these level two evals
[01:07:34.920 --> 01:07:36.760]   to do even more filtering.
[01:07:36.760 --> 01:07:39.000]   But then also, you commonly have other filters
[01:07:39.000 --> 01:07:42.680]   that you will find, that you'll see, like, different things
[01:07:42.680 --> 01:07:43.400]   in the data set.
[01:07:43.400 --> 01:07:45.800]   You're like, oh, like, things in this part of the data set
[01:07:45.800 --> 01:07:46.640]   are garbage.
[01:07:46.640 --> 01:07:48.160]   Or like, hey, the model is making
[01:07:48.160 --> 01:07:49.560]   a certain kind of mistake.
[01:07:49.560 --> 01:07:52.560]   Let me filter that mistake out.
[01:07:52.560 --> 01:07:54.480]   And then you have to decide whether or not
[01:07:54.480 --> 01:07:57.040]   you have to go acquire data for that mistake.
[01:07:57.040 --> 01:08:01.760]   So one example of that that's not necessarily a test,
[01:08:01.760 --> 01:08:04.360]   but it's a filtering technique, is in this case,
[01:08:04.360 --> 01:08:07.360]   I noticed there was a lot of either low-complexity queries,
[01:08:07.360 --> 01:08:09.200]   like super, super simple queries,
[01:08:09.200 --> 01:08:11.400]   or really, really high-complexity queries
[01:08:11.400 --> 01:08:14.480]   with, like, lots of operations, lots and lots of filters
[01:08:14.480 --> 01:08:15.960]   that didn't make any sense.
[01:08:15.960 --> 01:08:21.720]   So basically, I had some code that filtered those out, OK?
[01:08:21.720 --> 01:08:23.200]   In the more general case, there is
[01:08:23.200 --> 01:08:27.040]   a tool called Lilac, which kind of, like,
[01:08:27.040 --> 01:08:31.280]   helps you find more general things
[01:08:31.280 --> 01:08:34.440]   that you might be interested in filtering out of your data,
[01:08:34.440 --> 01:08:37.840]   in searching your data, and also finding duplicates.
[01:08:37.840 --> 01:08:41.360]   So another part of curation is to get rid of duplicates.
[01:08:41.360 --> 01:08:45.560]   You don't want-- like, OK, we did a lot of data augmentation
[01:08:45.560 --> 01:08:46.560]   and things like that.
[01:08:46.560 --> 01:08:49.480]   You might have lots of data that looks very similar or too
[01:08:49.480 --> 01:08:52.600]   similar, and that's not going to be good.
[01:08:52.600 --> 01:08:54.240]   Because what ends up happening is, like,
[01:08:54.240 --> 01:08:57.080]   you're going to, like, overweight on those examples.
[01:08:57.080 --> 01:08:59.840]   So, like, there's a lot of sophisticated things
[01:08:59.840 --> 01:09:00.640]   you can do.
[01:09:00.640 --> 01:09:03.760]   You should start with dumb things if you can, obviously.
[01:09:03.760 --> 01:09:06.280]   So, like, in this case, there's three parts.
[01:09:06.280 --> 01:09:08.120]   There's three main parts of this data set.
[01:09:08.120 --> 01:09:10.640]   There's the natural language query, there's the schema,
[01:09:10.640 --> 01:09:12.160]   and there's the output.
[01:09:12.160 --> 01:09:15.520]   And so one dumb thing you can do is,
[01:09:15.520 --> 01:09:19.240]   like, to drop any data where there's
[01:09:19.240 --> 01:09:23.880]   a pair that is, like, duplicated.
[01:09:23.880 --> 01:09:25.920]   Within those three, there's a pair of two
[01:09:25.920 --> 01:09:27.080]   that are duplicated.
[01:09:27.080 --> 01:09:28.120]   That's, like, one thing.
[01:09:28.120 --> 01:09:30.200]   And then I did-- there's another thing you can do.
[01:09:30.200 --> 01:09:32.840]   You can do, like, semantic searching
[01:09:32.840 --> 01:09:35.520]   and see semantic deduplication.
[01:09:35.520 --> 01:09:38.080]   You know, that's why in Lilac, for example,
[01:09:38.080 --> 01:09:42.520]   you have, like, fuzzy concept search and things like that.
[01:09:42.520 --> 01:09:43.920]   So that you can--
[01:09:43.920 --> 01:09:45.560]   and then you have, like, clustering and things
[01:09:45.560 --> 01:09:46.040]   like that.
[01:09:46.040 --> 01:09:48.440]   So you can kind of, like, look at data,
[01:09:48.440 --> 01:09:51.200]   try to maximize its diversity, clean out
[01:09:51.200 --> 01:09:55.280]   things that are, like, too much duplication.
[01:09:55.280 --> 01:09:57.400]   So that's kind of, like, an end-to-end overview.
[01:09:57.400 --> 01:10:00.360]   Like, the idea is, like, this is not a linear process.
[01:10:00.360 --> 01:10:02.360]   I went through this in, like, one through eight.
[01:10:02.360 --> 01:10:05.640]   But just know that, like, I have to go back and forth
[01:10:05.640 --> 01:10:06.960]   between all these different steps
[01:10:06.960 --> 01:10:09.200]   and do these things indifferently
[01:10:09.200 --> 01:10:11.440]   as I hit various things.
[01:10:11.440 --> 01:10:14.000]   Like, you know, like I mentioned,
[01:10:14.000 --> 01:10:18.760]   I have to constantly rewrite the level one evals.
[01:10:18.760 --> 01:10:23.800]   You know, or I might decide to redo the level two evals.
[01:10:23.800 --> 01:10:28.000]   But this is-- again, this is a very simple example just
[01:10:28.000 --> 01:10:31.080]   to give you a concrete use case, to give you
[01:10:31.080 --> 01:10:34.040]   the idea of the workflow.
[01:10:34.040 --> 01:10:36.120]   So that is the Honeycomb use case.
[01:10:40.800 --> 01:10:44.280]   Let's-- let me just quickly talk about debugging Axolotl.
[01:10:44.280 --> 01:10:46.280]   I'm going to switch gears.
[01:10:46.280 --> 01:10:49.600]   So, like, when you're using Axolotl,
[01:10:49.600 --> 01:10:51.640]   it's really important if you're going to use some software
[01:10:51.640 --> 01:10:53.040]   that you know how to debug it.
[01:10:53.040 --> 01:10:56.520]   And I just want to call your attention to this--
[01:10:56.520 --> 01:11:01.280]   these docs that will show you how to debug Axolotl.
[01:11:01.280 --> 01:11:03.400]   But there's these guidelines here
[01:11:03.400 --> 01:11:05.200]   that I think are really important.
[01:11:05.200 --> 01:11:06.960]   So if you're going to debug Axolotl, like,
[01:11:06.960 --> 01:11:09.200]   something is going wrong, you want
[01:11:09.200 --> 01:11:12.120]   to make sure that, number one, using the latest
[01:11:12.120 --> 01:11:14.600]   version of Axolotl.
[01:11:14.600 --> 01:11:16.360]   You also want to eliminate concurrency
[01:11:16.360 --> 01:11:18.640]   as much as possible.
[01:11:18.640 --> 01:11:21.520]   So basically, make sure you're only using one GPU and one
[01:11:21.520 --> 01:11:23.160]   data set process.
[01:11:23.160 --> 01:11:25.320]   Use a small data set.
[01:11:25.320 --> 01:11:26.920]   Use a small model.
[01:11:26.920 --> 01:11:29.560]   You want to minimize iteration time.
[01:11:29.560 --> 01:11:31.120]   And also, you want to clear caches.
[01:11:31.120 --> 01:11:33.200]   Clearing caches is huge, like, especially
[01:11:33.200 --> 01:11:35.800]   if you're trying to debug something about data set
[01:11:35.800 --> 01:11:37.400]   formation, like, hey, it's like, you
[01:11:37.400 --> 01:11:40.320]   don't think, like, your prompt is getting assembled correctly
[01:11:40.320 --> 01:11:41.280]   or something like that.
[01:11:41.280 --> 01:11:45.960]   You want to clear your cache, because that can trip you up.
[01:11:45.960 --> 01:11:48.720]   I also have-- there was a bunch of questions in the Zoom
[01:11:48.720 --> 01:11:52.840]   about how do you connect the Docker container that if you
[01:11:52.840 --> 01:11:55.760]   want to run Axolotl in, and, like, that's
[01:11:55.760 --> 01:11:58.640]   really connected to debugging, actually, in a way.
[01:11:58.640 --> 01:12:02.840]   Like, because you can use VS Code to do that.
[01:12:02.840 --> 01:12:06.480]   And I have some videos and tutorials in the Axolotl docs
[01:12:06.480 --> 01:12:09.680]   that show you how to do that, either with Docker or not
[01:12:09.680 --> 01:12:13.240]   using Docker, and how to attach to a remote host,
[01:12:13.240 --> 01:12:15.920]   and things like that.
[01:12:15.920 --> 01:12:20.600]   Let me go back to the slides.
[01:12:20.600 --> 01:12:21.720]   And I already covered this.
[01:12:21.720 --> 01:12:28.240]   OK, so I went through-- we went through a lot.
[01:12:28.240 --> 01:12:30.880]   I'm just going to stop and ask you,
[01:12:30.880 --> 01:12:35.520]   is there anything else on your mind in terms of things,
[01:12:35.520 --> 01:12:39.120]   like tips you might have for people using Axolotl
[01:12:39.120 --> 01:12:40.920]   that you'd like to highlight?
[01:12:40.920 --> 01:12:47.840]   I don't have any off the top of my head.
[01:12:47.840 --> 01:12:49.600]   It usually comes when people ask questions.
[01:12:49.600 --> 01:12:52.120]   I remember, oh, you should do this, this, or this.
[01:12:52.120 --> 01:12:55.920]   But I don't have any off the top of my head right now.
[01:12:55.920 --> 01:12:58.440]   No worries.
[01:12:58.440 --> 01:13:01.480]   There are a couple of-- maybe now's a good time.
[01:13:01.480 --> 01:13:04.400]   There are a couple of questions in the Q&A.
[01:13:04.400 --> 01:13:06.240]   But actually, some are listed as answered,
[01:13:06.240 --> 01:13:09.520]   but for everyone to be able to hear them.
[01:13:09.520 --> 01:13:11.200]   How about this one?
[01:13:11.200 --> 01:13:13.600]   How do you predict how long a fine-tuning job
[01:13:13.600 --> 01:13:15.320]   will take before you start it?
[01:13:15.320 --> 01:13:17.680]   Do you have any recommendations there?
[01:13:17.680 --> 01:13:24.800]   That one is relatively hard to answer.
[01:13:24.800 --> 01:13:28.800]   It depends on model size, lower full fine-tune,
[01:13:28.800 --> 01:13:31.640]   the GPUs you're using, the number of GPUs.
[01:13:31.640 --> 01:13:34.040]   If you're using deep speed 0.2, 0.3,
[01:13:34.040 --> 01:13:35.720]   when you're having offload, it's just
[01:13:35.720 --> 01:13:40.480]   there's so many factors that can affect the amount of time
[01:13:40.480 --> 01:13:44.200]   that it takes to fine-tune a model that it's used--
[01:13:44.200 --> 01:13:47.480]   I think once you have a gauge on a specific data
[01:13:47.480 --> 01:13:51.200]   set and on certain parameters that you're going--
[01:13:51.200 --> 01:13:54.920]   or hyperparameters that you're going to use for a specific set
[01:13:54.920 --> 01:13:58.480]   of experiments, you can usually get a good gauge from that.
[01:13:58.480 --> 01:14:03.760]   But I don't have a good all-around formula
[01:14:03.760 --> 01:14:05.240]   that works for everybody.
[01:14:05.240 --> 01:14:08.240]   Yeah.
[01:14:08.240 --> 01:14:12.040]   We're just looking through any of the other questions
[01:14:12.040 --> 01:14:12.540]   that--
[01:14:12.540 --> 01:14:16.920]   Yeah, we can come back.
[01:14:16.920 --> 01:14:18.920]   We've got a lot of questions.
[01:14:18.920 --> 01:14:22.000]   Just a second ago, talking about--
[01:14:22.000 --> 01:14:26.200]   someone had asked about doing a fine-tune
[01:14:26.200 --> 01:14:28.560]   and then improving, doing what Hamels was saying,
[01:14:28.560 --> 01:14:30.480]   improving the data, and then whether or not
[01:14:30.480 --> 01:14:33.280]   you should start from scratch again or fine-tune
[01:14:33.280 --> 01:14:34.400]   over that fine-tune model.
[01:14:34.400 --> 01:14:36.720]   And I think one of the things, when you think about that,
[01:14:36.720 --> 01:14:41.960]   is if your model is already getting pretty close to being
[01:14:41.960 --> 01:14:46.480]   overfit, just fine-tuning that again for more epochs
[01:14:46.480 --> 01:14:50.280]   is just going to definitely overfit at that point.
[01:14:50.280 --> 01:14:52.440]   And you should really consider just
[01:14:52.440 --> 01:14:56.600]   cleaning up the original data and adding in the new,
[01:14:56.600 --> 01:15:00.000]   improved data, and then just starting from scratch again
[01:15:00.000 --> 01:15:02.880]   at that point on the base model.
[01:15:02.880 --> 01:15:05.080]   Yeah, I always start again from scratch
[01:15:05.080 --> 01:15:06.680]   when I improve my data.
[01:15:06.680 --> 01:15:08.960]   I haven't thought about trying to keep going.
[01:15:08.960 --> 01:15:15.520]   OK, I think we probably should move forward because I'm
[01:15:15.520 --> 01:15:16.640]   looking at time as well.
[01:15:16.640 --> 01:15:21.560]   I think the next thing that I might want to do
[01:15:21.560 --> 01:15:24.680]   is jump right into Zach's.
[01:15:24.680 --> 01:15:25.440]   Sure, let's do it.
[01:15:25.440 --> 01:15:32.200]   How do I--
[01:15:32.200 --> 01:15:33.800]   it looks like I can take over for you,
[01:15:33.800 --> 01:15:37.360]   so less for you to worry about.
[01:15:37.360 --> 01:15:38.600]   We're all seeing me all right?
[01:15:38.600 --> 01:15:39.440]   Yeah.
[01:15:39.440 --> 01:15:41.520]   Perfect.
[01:15:41.520 --> 01:15:42.760]   All right, hey, everyone.
[01:15:42.760 --> 01:15:44.180]   My name is Zach Mueller, and we're
[01:15:44.180 --> 01:15:46.680]   going to be talking about scaling model training
[01:15:46.680 --> 01:15:48.160]   as you get more compute.
[01:15:48.160 --> 01:15:51.160]   How do these people wind up doing that?
[01:15:51.160 --> 01:15:54.520]   So a little about me--
[01:15:54.520 --> 01:15:57.200]   I'm the technical lead for the Hug and Face Accelerate project,
[01:15:57.200 --> 01:16:00.040]   and I handle a lot of the internals
[01:16:00.040 --> 01:16:02.680]   when it comes to the Transformers trainer.
[01:16:02.680 --> 01:16:06.400]   I'm also a humongous API design geek.
[01:16:06.400 --> 01:16:08.880]   And before we start talking about how
[01:16:08.880 --> 01:16:10.720]   do they go about doing this sort of what
[01:16:10.720 --> 01:16:13.440]   we call distributed training, let's
[01:16:13.440 --> 01:16:17.040]   get a general understanding of model GPU usage.
[01:16:17.040 --> 01:16:20.080]   So we were talking about how you can use things like Loras
[01:16:20.080 --> 01:16:22.040]   to reduce some of the memory overhead,
[01:16:22.040 --> 01:16:26.000]   but how much memory overhead do certain models actually use?
[01:16:26.000 --> 01:16:28.160]   We can sort of guess what that number winds up
[01:16:28.160 --> 01:16:31.120]   being if we're using like vanilla full fine tuning,
[01:16:31.120 --> 01:16:33.360]   so without using Loras.
[01:16:33.360 --> 01:16:36.440]   And then you can sort of convert some of it later.
[01:16:36.440 --> 01:16:38.560]   The assumptions that you basically have to have
[01:16:38.560 --> 01:16:41.320]   are we're going to use the Atom Optimizer,
[01:16:41.320 --> 01:16:44.380]   and we're going to start with a batch size of 1.
[01:16:44.380 --> 01:16:47.520]   And for example, let's take BERT base case, right?
[01:16:47.520 --> 01:16:49.600]   So that's going to be 108 million parameters.
[01:16:49.600 --> 01:16:52.400]   How much GPU space am I going to need to train that?
[01:16:52.400 --> 01:16:55.180]   Well, each parameter in a model is 4 bytes.
[01:16:55.180 --> 01:16:57.860]   And the backward pass usually takes about two times
[01:16:57.860 --> 01:16:58.960]   the model size.
[01:16:58.960 --> 01:17:01.920]   And the optimizer step takes about four times that,
[01:17:01.920 --> 01:17:03.600]   one for the model, one for the gradients,
[01:17:03.600 --> 01:17:06.560]   and two for the optimizer when it comes to Atom.
[01:17:06.560 --> 01:17:09.360]   So after doing all this computation,
[01:17:09.360 --> 01:17:13.560]   you wind up getting to 1.6 gigs is
[01:17:13.560 --> 01:17:17.600]   needed to train on a batch size of 1 for BERT.
[01:17:17.600 --> 01:17:20.240]   With mixed precision, that's knocked down by half
[01:17:20.240 --> 01:17:24.320]   because while the model is still in full precision, which
[01:17:24.320 --> 01:17:27.160]   I'll go over why that's important in a moment,
[01:17:27.160 --> 01:17:28.760]   the gradients wind up taking less
[01:17:28.760 --> 01:17:32.240]   because the gradients themselves are in half bit.
[01:17:32.240 --> 01:17:34.640]   And so we're able to fit and roughly guess
[01:17:34.640 --> 01:17:38.920]   that it's probably going to take about a gig to two gigs overall
[01:17:38.920 --> 01:17:42.720]   when we're training on BERT.
[01:17:42.720 --> 01:17:45.480]   Now let's talk about why that matters.
[01:17:45.480 --> 01:17:48.720]   All right, so that's great if you have 12 to 24 gigs of GPU
[01:17:48.720 --> 01:17:49.760]   space, right?
[01:17:49.760 --> 01:17:51.320]   Typical consumer card.
[01:17:51.320 --> 01:17:53.820]   But what happens when we scale that up, right?
[01:17:53.820 --> 01:17:57.180]   So if we look at LLAMA 3 8 billion, 8 billion parameters,
[01:17:57.180 --> 01:17:59.040]   loading the model in is going to take you
[01:17:59.040 --> 01:18:02.100]   in full precision 28 gigs.
[01:18:02.100 --> 01:18:03.860]   Gradients are another 28 gigs.
[01:18:03.860 --> 01:18:05.940]   Backward pass gets you to 56.
[01:18:05.940 --> 01:18:08.940]   And suddenly, you're somewhere between 56 and 112 gigs
[01:18:08.940 --> 01:18:10.340]   of VRAM.
[01:18:10.340 --> 01:18:14.060]   I know I certainly don't have 56 gigs on a single card,
[01:18:14.060 --> 01:18:16.120]   let alone 112.
[01:18:16.120 --> 01:18:21.980]   If we want to avoid things like peft, what do we do?
[01:18:21.980 --> 01:18:24.520]   This is where the concept of distributed training comes in,
[01:18:24.520 --> 01:18:27.220]   or how do we make sure that we can use multiple GPUs
[01:18:27.220 --> 01:18:29.860]   to achieve what we want?
[01:18:29.860 --> 01:18:31.700]   So there's three different kinds of training
[01:18:31.700 --> 01:18:35.020]   when we think about it at the hardware level.
[01:18:35.020 --> 01:18:36.420]   So we have single GPU, right?
[01:18:36.420 --> 01:18:37.900]   So that's no distributed techniques.
[01:18:37.900 --> 01:18:40.980]   You are running it straight off of whatever GPU you have.
[01:18:40.980 --> 01:18:43.900]   We have the concept of distributed data parallelism.
[01:18:43.900 --> 01:18:48.020]   And this works by having a full model on every device,
[01:18:48.020 --> 01:18:51.220]   but the data is chunked and split up between every GPU.
[01:18:51.220 --> 01:18:53.580]   Another way to think about that is, essentially, we
[01:18:53.580 --> 01:18:55.980]   can process the data faster because we're
[01:18:55.980 --> 01:19:00.740]   sending chunks of our full batch across multiple GPUs
[01:19:00.740 --> 01:19:03.380]   to speed up the training time.
[01:19:03.380 --> 01:19:04.940]   And the last part that I'll mostly
[01:19:04.940 --> 01:19:08.900]   be covering in today's talk is fully sharded data parallelism,
[01:19:08.900 --> 01:19:11.060]   FSDP, and deep speed.
[01:19:11.060 --> 01:19:14.140]   And these are the key areas that was sort of hinted
[01:19:14.140 --> 01:19:16.860]   at in the earlier discussions, where, essentially, we
[01:19:16.860 --> 01:19:20.140]   can split chunks of the model in optimizer states
[01:19:20.140 --> 01:19:21.980]   across multiple GPUs.
[01:19:21.980 --> 01:19:25.460]   And what that allows is, rather than having the limit of DDP,
[01:19:25.460 --> 01:19:28.940]   where we're stuck with, say, 240 90s at 24 gigs,
[01:19:28.940 --> 01:19:31.740]   that's all I can use, in memory, it
[01:19:31.740 --> 01:19:35.660]   acts as a single 48-gigabyte GPU when
[01:19:35.660 --> 01:19:37.540]   we think about the total RAM that we
[01:19:37.540 --> 01:19:39.500]   can play with to train models.
[01:19:39.500 --> 01:19:41.300]   And that's the secret to how you can train
[01:19:41.300 --> 01:19:44.700]   these larger and larger models.
[01:19:44.700 --> 01:19:48.420]   Now, what is fully sharded data parallelism?
[01:19:48.420 --> 01:19:51.180]   The general idea here is you take your model,
[01:19:51.180 --> 01:19:54.980]   and we're going to create what's called shards of the model.
[01:19:54.980 --> 01:19:57.420]   So let's say, taking the model, we
[01:19:57.420 --> 01:19:59.860]   could imagine a shard being split perfectly
[01:19:59.860 --> 01:20:01.540]   in half, the first half of the model
[01:20:01.540 --> 01:20:03.140]   and the second half of the model.
[01:20:03.140 --> 01:20:06.700]   And depending on how we configure FSDP,
[01:20:06.700 --> 01:20:08.980]   certain chunks of the training loop
[01:20:08.980 --> 01:20:13.220]   will happen in that VRAM space.
[01:20:13.220 --> 01:20:16.920]   And then, depending on what points occur during that,
[01:20:16.920 --> 01:20:18.820]   occasionally, Torch needs to know
[01:20:18.820 --> 01:20:21.240]   what's happening with that other model chunk,
[01:20:21.240 --> 01:20:22.860]   because it's all the same model, and we
[01:20:22.860 --> 01:20:25.140]   need to get the gradients all aligned.
[01:20:25.140 --> 01:20:28.980]   So these call what are called communications.
[01:20:28.980 --> 01:20:30.980]   And generally, you want less of these,
[01:20:30.980 --> 01:20:33.980]   because it's essentially time spent on your GPUs just talking
[01:20:33.980 --> 01:20:35.780]   to each other and trading information.
[01:20:35.780 --> 01:20:37.340]   You're not training anything.
[01:20:37.340 --> 01:20:38.780]   You're not processing data.
[01:20:38.780 --> 01:20:41.620]   It is quite literally just your two GPUs trading notes
[01:20:41.620 --> 01:20:43.380]   on how they think the model should be,
[01:20:43.380 --> 01:20:46.700]   and then correcting themselves.
[01:20:46.700 --> 01:20:50.460]   Now, I'm not going to really go too much in-depth
[01:20:50.460 --> 01:20:53.500]   into every single thing FSDP can do.
[01:20:53.500 --> 01:20:55.900]   What I am going to talk about is, in my opinion,
[01:20:55.900 --> 01:20:58.220]   the most important ones when it comes
[01:20:58.220 --> 01:21:00.700]   to training in low-resource areas
[01:21:00.700 --> 01:21:04.540]   and when you're using FSDP, and how
[01:21:04.540 --> 01:21:07.580]   you dictate how those weights and gradients and parameters
[01:21:07.580 --> 01:21:09.020]   get sharded.
[01:21:09.020 --> 01:21:10.780]   And on top of that, I'm going to cover
[01:21:10.780 --> 01:21:12.620]   some of the important ones I needed
[01:21:12.620 --> 01:21:15.260]   when I was doing a full fine-tune of LLAMA3 8
[01:21:15.260 --> 01:21:18.500]   billion without PEFT on 240 90s.
[01:21:18.500 --> 01:21:22.940]   Spoiler alert, it was very slow.
[01:21:22.940 --> 01:21:24.420]   So the first part of this is what
[01:21:24.420 --> 01:21:26.180]   we call a sharding strategy.
[01:21:26.180 --> 01:21:28.340]   And the general idea here is this
[01:21:28.340 --> 01:21:31.420]   is us telling FSDP how we want to split
[01:21:31.420 --> 01:21:34.980]   all of these different things that take up VRAM.
[01:21:34.980 --> 01:21:37.380]   So with full shard, as it sounds like,
[01:21:37.380 --> 01:21:38.700]   everything is going to be split--
[01:21:38.700 --> 01:21:42.580]   our optimizer state, our gradient, and our parameters.
[01:21:42.580 --> 01:21:45.860]   With shard grad op, which is optimizer,
[01:21:45.860 --> 01:21:48.500]   instead, we're just sharding the optimizer state
[01:21:48.500 --> 01:21:49.860]   and the gradients.
[01:21:49.860 --> 01:21:52.180]   And then, essentially, the model will
[01:21:52.180 --> 01:21:55.900]   be split when we're not using it and then joined back together
[01:21:55.900 --> 01:21:58.380]   when we are, such as during the backward pass.
[01:21:58.380 --> 01:22:00.160]   This reduces some of the memory overhead,
[01:22:00.160 --> 01:22:03.300]   because we still need more than the original model,
[01:22:03.300 --> 01:22:05.860]   because we're still fitting the entire model in VRAM.
[01:22:05.860 --> 01:22:10.880]   But it reduces that training VRAM a little bit for us.
[01:22:10.880 --> 01:22:12.740]   We have a technique called no shard, which,
[01:22:12.740 --> 01:22:14.460]   as that sounds like, that's just going
[01:22:14.460 --> 01:22:16.420]   to be distributed data parallelism.
[01:22:16.420 --> 01:22:18.740]   We're not sharding anything.
[01:22:18.740 --> 01:22:20.980]   And then the last part is a new thing
[01:22:20.980 --> 01:22:24.460]   that PyTorch has come out with called hybrid sharding.
[01:22:24.460 --> 01:22:26.460]   And it's kind of like full shard,
[01:22:26.460 --> 01:22:29.980]   where we're fully sharding absolutely everything,
[01:22:29.980 --> 01:22:32.500]   including the optimizer states, gradients, and parameters.
[01:22:32.500 --> 01:22:34.820]   However, if you're training on multi-node--
[01:22:34.820 --> 01:22:39.500]   so multiple computers are training a big model at once--
[01:22:39.500 --> 01:22:45.400]   it keeps a copy of the entire model on each of those nodes.
[01:22:45.400 --> 01:22:46.900]   That's important, because remember
[01:22:46.900 --> 01:22:49.760]   how I said communications slow down things a lot?
[01:22:49.760 --> 01:22:53.200]   Hybrid shard lets us reduce the communications from, I think,
[01:22:53.200 --> 01:22:55.680]   three down to two, if not one.
[01:22:55.680 --> 01:22:59.300]   And so your training speed is increased, honestly,
[01:22:59.300 --> 01:23:02.680]   to some extent, exponentially, depending on how long
[01:23:02.680 --> 01:23:07.760]   it takes for your computers to talk to each other.
[01:23:07.760 --> 01:23:11.240]   So the next part is, we know how we're
[01:23:11.240 --> 01:23:13.160]   going to split the memory, right?
[01:23:13.160 --> 01:23:14.680]   But how do we split the model?
[01:23:14.680 --> 01:23:17.800]   Because we need some way to tell FSCP, all right,
[01:23:17.800 --> 01:23:18.920]   I have this model.
[01:23:18.920 --> 01:23:22.360]   How do I want to split it in between my GPUs?
[01:23:22.360 --> 01:23:26.280]   With Accelerate, with Axolotl, with Transformers,
[01:23:26.280 --> 01:23:29.000]   we use two different nomenclatures--
[01:23:29.000 --> 01:23:31.600]   transformer-based wrap and size-based wrap.
[01:23:31.600 --> 01:23:33.100]   Transformer, as it sounds like, is
[01:23:33.100 --> 01:23:35.280]   very specific to Transformers.
[01:23:35.280 --> 01:23:37.080]   With this, you need to declare the layer
[01:23:37.080 --> 01:23:37.960]   you want to split on.
[01:23:37.960 --> 01:23:40.560]   So this could be a BERT layer or a LLAMA layer.
[01:23:40.560 --> 01:23:43.640]   Usually, Transformers has good defaults and good helpers
[01:23:43.640 --> 01:23:45.520]   to help you figure out what that is.
[01:23:45.520 --> 01:23:48.320]   The other version is more manual.
[01:23:48.320 --> 01:23:51.000]   And basically, you're just telling FSCP,
[01:23:51.000 --> 01:23:55.500]   after x amount of parameters, go ahead and split the model.
[01:23:55.500 --> 01:23:57.360]   That's great, because it works out of the box.
[01:23:57.360 --> 01:24:00.520]   That's bad, because there could be speed increases
[01:24:00.520 --> 01:24:02.360]   that you might be missing by having,
[01:24:02.360 --> 01:24:07.560]   say, each head of a Mistro model on a separate GPU.
[01:24:07.560 --> 01:24:10.320]   So that way, it can handle its own computations
[01:24:10.320 --> 01:24:13.720]   much faster than needing to wait to communicate with other GPUs.
[01:24:13.720 --> 01:24:19.480]   Now, the next part, which was particularly important for me,
[01:24:19.480 --> 01:24:22.440]   is the idea of offloading parameters.
[01:24:22.440 --> 01:24:26.000]   And what this says is, OK, I have 48 gigs of VRAM right now,
[01:24:26.000 --> 01:24:28.360]   if I'm assuming 240-90s.
[01:24:28.360 --> 01:24:29.360]   And I can't fit that.
[01:24:29.360 --> 01:24:30.680]   I can't train on it.
[01:24:30.680 --> 01:24:32.960]   Well, I'm going to accept that I still want to do it.
[01:24:32.960 --> 01:24:35.520]   I don't want to go by through a cloud provider.
[01:24:35.520 --> 01:24:40.080]   And so FSCP will let us offload gradients and model parameters
[01:24:40.080 --> 01:24:41.680]   into RAM.
[01:24:41.680 --> 01:24:44.840]   Now, as that sounds like, that's going to be extremely slow,
[01:24:44.840 --> 01:24:48.200]   because we're taking things from the GPU to the CPU,
[01:24:48.200 --> 01:24:50.440]   and now just shoving it in RAM.
[01:24:50.440 --> 01:24:53.440]   But it lets us train as big a model as, essentially,
[01:24:53.440 --> 01:24:55.640]   you have available in RAM.
[01:24:55.640 --> 01:24:58.600]   So case in point, when I was doing a full fine-tune
[01:24:58.600 --> 01:25:03.320]   with Llama 3 8 billion to match a paper that came out,
[01:25:03.320 --> 01:25:05.440]   I wound up needing to use offload parameters,
[01:25:05.440 --> 01:25:08.160]   because as we saw earlier, 8 billion
[01:25:08.160 --> 01:25:10.080]   requires about 50 gigs or so.
[01:25:10.080 --> 01:25:12.480]   I only have 48.
[01:25:12.480 --> 01:25:14.600]   And it was going to take like 72 hours
[01:25:14.600 --> 01:25:18.880]   to do four iterations through my data versus an hour or two
[01:25:18.880 --> 01:25:20.440]   on an H100.
[01:25:20.440 --> 01:25:24.160]   So yes, it's cool that you know how to use these tools,
[01:25:24.160 --> 01:25:26.560]   and it can help you train things locally.
[01:25:26.560 --> 01:25:28.920]   Make sure to double check, though,
[01:25:28.920 --> 01:25:31.920]   A, what your time constraint is, and B, what your budget is.
[01:25:31.920 --> 01:25:34.880]   Because I can run it for free, and it can take longer,
[01:25:34.880 --> 01:25:38.360]   or I can pay $5 and go finish it in an hour.
[01:25:38.360 --> 01:25:40.280]   Depending on how much time you have available,
[01:25:40.280 --> 01:25:45.640]   each solution has different opportunities.
[01:25:45.640 --> 01:25:48.480]   Now, another critical part, in my opinion,
[01:25:48.480 --> 01:25:53.000]   when it comes to doing FSCP that Accelerating Transformers has
[01:25:53.000 --> 01:25:55.640]   is this idea of CPU RAM efficient loading.
[01:25:55.640 --> 01:25:58.720]   And also this idea of sync module states.
[01:25:58.720 --> 01:26:01.800]   So if you're familiar with Accelerate's big model
[01:26:01.800 --> 01:26:03.480]   inference, that's fine.
[01:26:03.480 --> 01:26:05.560]   I'll give you a brief summary.
[01:26:05.560 --> 01:26:07.760]   Basically, PyTorch lets us use this thing
[01:26:07.760 --> 01:26:09.980]   called device equals meta.
[01:26:09.980 --> 01:26:12.560]   And that essentially is the skeleton of your model.
[01:26:12.560 --> 01:26:13.680]   The weights aren't loaded.
[01:26:13.680 --> 01:26:16.000]   It can't really do computations too well.
[01:26:16.000 --> 01:26:17.480]   But it's just the skeleton for us
[01:26:17.480 --> 01:26:20.040]   to eventually load weights into.
[01:26:20.040 --> 01:26:24.800]   So rather than loading LLAMA 8 billion on eight GPUs--
[01:26:24.800 --> 01:26:28.600]   so now we need eight times the amount of RAM of our model
[01:26:28.600 --> 01:26:30.600]   to load it in at once.
[01:26:30.600 --> 01:26:34.280]   So that's going to be easily 100, 200 gigs,
[01:26:34.280 --> 01:26:35.800]   if I'm not mistaken.
[01:26:35.800 --> 01:26:39.600]   Instead, we send all the other versions onto that meta device.
[01:26:39.600 --> 01:26:41.400]   So they take up no RAM.
[01:26:41.400 --> 01:26:44.760]   And then we load all of the weights only on one of them.
[01:26:44.760 --> 01:26:49.080]   And so then, when we're ready to do FSCP--
[01:26:49.080 --> 01:26:51.000]   well, we already know we're starting the model.
[01:26:51.000 --> 01:26:54.120]   So we just tell the first node to send those weights
[01:26:54.120 --> 01:26:58.320]   to whatever node or GPU needs that particular chunk
[01:26:58.320 --> 01:26:59.400]   of weights.
[01:26:59.400 --> 01:27:03.080]   And this really helps keep your RAM size low.
[01:27:03.080 --> 01:27:05.840]   And you don't suddenly sit there with crashes because, oh, no,
[01:27:05.840 --> 01:27:06.960]   you ran out of CPU memory.
[01:27:06.960 --> 01:27:09.920]   Because fun fact, you will redline this quite often,
[01:27:09.920 --> 01:27:12.680]   I've found, at least in this particular scenario.
[01:27:12.680 --> 01:27:20.320]   Now, I've talked about FSCP a lot.
[01:27:20.320 --> 01:27:22.480]   And I've assumed that you knew context about Axolotl,
[01:27:22.480 --> 01:27:24.120]   Transformers, and all this stuff.
[01:27:24.120 --> 01:27:25.800]   Let's take it back and just focus
[01:27:25.800 --> 01:27:28.160]   on Accelerate, which you might not
[01:27:28.160 --> 01:27:32.240]   know is the foundation of a lot of your favorite libraries.
[01:27:32.240 --> 01:27:36.760]   So practically all of Transformers and HuggingFace
[01:27:36.760 --> 01:27:39.760]   as a whole relies on Accelerate.
[01:27:39.760 --> 01:27:42.480]   Same with Axolotl, FastAI, anything
[01:27:42.480 --> 01:27:46.120]   Lucidrain does at this point, as well as Cornea.
[01:27:46.120 --> 01:27:48.400]   And the general idea with Accelerate
[01:27:48.400 --> 01:27:51.520]   is it's essentially three frameworks.
[01:27:51.520 --> 01:27:52.960]   You have a command line interface
[01:27:52.960 --> 01:27:56.120]   that Hamil and Wing already showed us
[01:27:56.120 --> 01:27:58.920]   whenever they were doing Accelerate launch.
[01:27:58.920 --> 01:28:00.620]   You have a training library, which
[01:28:00.620 --> 01:28:03.680]   is under the hood what is doing all of this distributed
[01:28:03.680 --> 01:28:05.600]   training fairly easily.
[01:28:05.600 --> 01:28:09.940]   And then the big model inference that I mentioned a moment ago.
[01:28:09.940 --> 01:28:12.400]   For the sake of this talk, we're not talking about big model
[01:28:12.400 --> 01:28:12.760]   inference.
[01:28:12.760 --> 01:28:14.520]   We don't particularly care about that here.
[01:28:14.520 --> 01:28:16.240]   We're just caring about fine-tuning LLMs.
[01:28:16.240 --> 01:28:18.920]   So we're going to focus on the first two.
[01:28:18.920 --> 01:28:21.480]   So you need about three commands to really get everything
[01:28:21.480 --> 01:28:22.160]   going.
[01:28:22.160 --> 01:28:24.120]   The first is accelerate-config.
[01:28:24.120 --> 01:28:27.760]   This is used to configure the environment.
[01:28:27.760 --> 01:28:31.960]   This is also what Wing has managed
[01:28:31.960 --> 01:28:35.200]   to wrap around beautifully when he shows his Accelerate launch
[01:28:35.200 --> 01:28:37.860]   commands, because his config files can directly
[01:28:37.860 --> 01:28:42.240]   be used for doing Accelerate launch, which is phenomenal.
[01:28:42.240 --> 01:28:44.320]   The second part is estimate-memory,
[01:28:44.320 --> 01:28:45.960]   which goes through those calculations
[01:28:45.960 --> 01:28:48.080]   I showed a moment ago whenever I was playing around
[01:28:48.080 --> 01:28:50.680]   with the idea of, well, how much VRAM can I use?
[01:28:50.680 --> 01:28:52.380]   And the last part is accelerate-launch,
[01:28:52.380 --> 01:28:54.340]   which is how you run your script.
[01:28:54.340 --> 01:28:57.920]   Let's look at why these matter.
[01:28:57.920 --> 01:29:01.200]   Launching and distributed training sucks.
[01:29:01.200 --> 01:29:03.400]   There's a lot of different ways you can do it.
[01:29:03.400 --> 01:29:05.320]   There's a lot of different commands you can run,
[01:29:05.320 --> 01:29:07.320]   some of it's PyTorch, some of it's DeepSpeed.
[01:29:07.320 --> 01:29:10.260]   And all of them have slightly different commands.
[01:29:10.260 --> 01:29:13.360]   So here, if you just do Python script.py,
[01:29:13.360 --> 01:29:16.440]   it's not going to train in any distributed scenario.
[01:29:16.440 --> 01:29:18.180]   Most will get model parallelism, but you
[01:29:18.180 --> 01:29:19.840]   won't get distributed data parallelism.
[01:29:19.840 --> 01:29:22.720]   FSTP don't work, won't work.
[01:29:22.720 --> 01:29:24.900]   Torch run and DeepSpeed are the main two commands
[01:29:24.900 --> 01:29:26.480]   you can use to run.
[01:29:26.480 --> 01:29:30.280]   This will basically say, torch run, run on a single computer
[01:29:30.280 --> 01:29:31.940]   with two GPUs, my script.
[01:29:31.940 --> 01:29:33.820]   And then it does some things in the background
[01:29:33.820 --> 01:29:36.080]   to help make sure that works.
[01:29:36.080 --> 01:29:37.700]   And that's a lot of different commands
[01:29:37.700 --> 01:29:39.840]   that you have to know and remember.
[01:29:39.840 --> 01:29:42.520]   And so accelerate-launch is here to just say, OK,
[01:29:42.520 --> 01:29:45.360]   tell me what you're doing, and I'll make sure
[01:29:45.360 --> 01:29:48.400]   that we're running it.
[01:29:48.400 --> 01:29:50.360]   It operates by these config files,
[01:29:50.360 --> 01:29:54.280]   similar to what, again, we were showing in the next model.
[01:29:54.280 --> 01:29:59.000]   And these essentially define how we want certain things to run.
[01:29:59.000 --> 01:30:02.560]   So here we're saying, I have a local machine
[01:30:02.560 --> 01:30:06.880]   that's multi-GPU running with BF16 mixed precision
[01:30:06.880 --> 01:30:09.600]   on eight GPUs.
[01:30:09.600 --> 01:30:13.160]   With FSTP, on the other hand, we can go through and specify
[01:30:13.160 --> 01:30:17.640]   everything we want to use with FSTP using a config.
[01:30:17.640 --> 01:30:19.920]   And this way, accelerate-launch just
[01:30:19.920 --> 01:30:23.400]   knows, hey, we're going to make sure that we train in FSTP
[01:30:23.400 --> 01:30:25.560]   if we're using accelerate.
[01:30:25.560 --> 01:30:28.920]   And that's all you need to do from a launching perspective.
[01:30:28.920 --> 01:30:31.400]   And if you're using axolotl or transformers,
[01:30:31.400 --> 01:30:33.120]   this is all you need to do.
[01:30:33.120 --> 01:30:35.840]   The next part I'm going to show is the internals of it
[01:30:35.840 --> 01:30:38.880]   on the low level of how accelerate works
[01:30:38.880 --> 01:30:41.040]   and how you can use accelerate specifically.
[01:30:41.040 --> 01:30:42.920]   But do remember, this isn't necessarily
[01:30:42.920 --> 01:30:48.760]   needed if you're using things like axolotl or transformers.
[01:30:48.760 --> 01:30:50.560]   So the general idea with accelerate
[01:30:50.560 --> 01:30:53.120]   is we want a low-level way to make sure
[01:30:53.120 --> 01:30:56.760]   that this can essentially be device agnostic and compute
[01:30:56.760 --> 01:30:58.000]   agnostic.
[01:30:58.000 --> 01:31:00.000]   So make sure you have your code running on a Mac,
[01:31:00.000 --> 01:31:03.280]   running on a Windows machine, running on a GPU,
[01:31:03.280 --> 01:31:05.680]   running on CPU, running on TPUs.
[01:31:05.680 --> 01:31:13.360]   And it does so in a minimally intrusive and ideally not very
[01:31:13.360 --> 01:31:14.520]   complex manner.
[01:31:14.520 --> 01:31:17.200]   You create an accelerator, and you just
[01:31:17.200 --> 01:31:18.960]   have to prepare all your things.
[01:31:18.960 --> 01:31:19.540]   And that's it.
[01:31:19.540 --> 01:31:21.440]   You're off to the races.
[01:31:21.440 --> 01:31:24.560]   Switch your accelerator-- or switch your backwards function
[01:31:24.560 --> 01:31:26.440]   to use accelerator.backwards.
[01:31:26.440 --> 01:31:31.880]   And on a whole, that's most of what you need to do.
[01:31:31.880 --> 01:31:35.480]   How it winds up working is similar to FSTP.
[01:31:35.480 --> 01:31:39.920]   Accelerate will do the data sharding for you
[01:31:39.920 --> 01:31:43.320]   in taking your data and splitting it across GPUs.
[01:31:43.320 --> 01:31:48.080]   It also operates by essentially having one global step.
[01:31:48.080 --> 01:31:51.800]   So an easy way to think about it is
[01:31:51.800 --> 01:31:55.760]   if we're training on eight GPUs versus a single GPU.
[01:31:55.760 --> 01:31:58.520]   So if a single GPU had a batch size of 16,
[01:31:58.520 --> 01:32:00.780]   and now we're training on eight GPUs,
[01:32:00.780 --> 01:32:04.020]   the equivalent in Accelerate to get the same exact training
[01:32:04.020 --> 01:32:07.140]   would have each GPU have a batch size of two,
[01:32:07.140 --> 01:32:10.000]   because 2 times 8 is 16.
[01:32:10.000 --> 01:32:12.680]   And so what winds up happening is this lets us successfully
[01:32:12.680 --> 01:32:17.320]   scale our training that should have roughly the same results
[01:32:17.320 --> 01:32:19.480]   when training on a single GPU versus training
[01:32:19.480 --> 01:32:23.080]   on multiple GPUs without needing to worry about,
[01:32:23.080 --> 01:32:24.720]   oh, do I need to step my scheduler more?
[01:32:24.720 --> 01:32:26.320]   Oh, do I need to adjust my learning rate more?
[01:32:26.320 --> 01:32:27.480]   Oh, do I need to do this?
[01:32:27.480 --> 01:32:28.400]   Do I need to do that?
[01:32:28.400 --> 01:32:31.980]   It's the same amount of data being processed at one time,
[01:32:31.980 --> 01:32:35.440]   and everything else is just done for you.
[01:32:35.440 --> 01:32:40.220]   Now, the next part of this, I want
[01:32:40.220 --> 01:32:43.100]   to talk about some very specific tweaks
[01:32:43.100 --> 01:32:47.940]   that we do to protect you from dumb decisions.
[01:32:47.940 --> 01:32:50.060]   The first part is mixed precision.
[01:32:50.060 --> 01:32:52.300]   This is a bit different than maybe your normal idea
[01:32:52.300 --> 01:32:53.940]   of mixed precision.
[01:32:54.000 --> 01:32:58.680]   We don't convert the model weights to BF16 and FP16
[01:32:58.680 --> 01:33:00.280]   when we're training with Accelerate,
[01:33:00.280 --> 01:33:02.960]   and we try our hardest to make sure that doesn't happen.
[01:33:02.960 --> 01:33:06.600]   Instead, we wrap the forward pass with AutoCast instead
[01:33:06.600 --> 01:33:08.840]   to just convert the gradients.
[01:33:08.840 --> 01:33:11.920]   This preserves the original precision of our weights
[01:33:11.920 --> 01:33:16.480]   and leads to stable training and better fine tuning later on.
[01:33:16.480 --> 01:33:18.920]   Because-- and this is very important--
[01:33:18.920 --> 01:33:23.240]   if you go to BF16, you are stuck in BF16.
[01:33:23.240 --> 01:33:26.220]   There was a whole issue a few months ago with transformers
[01:33:26.220 --> 01:33:28.520]   where some quality of some fine-tuned models
[01:33:28.520 --> 01:33:29.780]   weren't doing well.
[01:33:29.780 --> 01:33:33.060]   This was the cause.
[01:33:33.060 --> 01:33:34.820]   Now, going a bit more than that, if you're
[01:33:34.820 --> 01:33:38.340]   familiar with or keeping up to date with Efficient Memory
[01:33:38.340 --> 01:33:40.140]   Training, you might have heard of something
[01:33:40.140 --> 01:33:42.740]   called Transformer's Engine, or MSAMP.
[01:33:42.740 --> 01:33:46.460]   The idea behind this is we make use of 4090s, H100s,
[01:33:46.460 --> 01:33:48.300]   and do training in 8-bit.
[01:33:48.300 --> 01:33:50.460]   Now, this is different than quantization.
[01:33:50.460 --> 01:33:54.020]   You are actually training on raw native 8-bit.
[01:33:54.020 --> 01:33:57.100]   So 8-bits, and that's all you have.
[01:33:57.100 --> 01:33:59.100]   A lot of mistakes I see people do with this,
[01:33:59.100 --> 01:34:01.560]   especially with the Nvidia examples,
[01:34:01.560 --> 01:34:04.220]   is they do the prior thing of converting the entire model
[01:34:04.220 --> 01:34:07.220]   into BF16 and then train.
[01:34:07.220 --> 01:34:09.900]   That leads to huge instabilities during training,
[01:34:09.900 --> 01:34:11.260]   and generally people's performance
[01:34:11.260 --> 01:34:12.900]   hasn't been the best.
[01:34:12.900 --> 01:34:16.620]   I've also heard rumors, though, that even this can go bad.
[01:34:16.620 --> 01:34:18.780]   So it's always worth playing around
[01:34:18.780 --> 01:34:22.300]   with, if you have the ability, FP16 versus non-FP16,
[01:34:22.300 --> 01:34:27.060]   and that includes BF16, and testing out what levels
[01:34:27.060 --> 01:34:28.380]   can be in 8-bit.
[01:34:28.380 --> 01:34:30.940]   Because with Transformer's Engine,
[01:34:30.940 --> 01:34:32.740]   it's still using the AutoCast.
[01:34:32.740 --> 01:34:36.460]   And so the computations, rather than being done in 16-bit,
[01:34:36.460 --> 01:34:38.300]   are done in 8-bit.
[01:34:38.300 --> 01:34:41.300]   And then if you're playing around with MSAMP,
[01:34:41.300 --> 01:34:45.460]   that lets you experimentally go even further with this.
[01:34:45.460 --> 01:34:49.940]   And so we can get to a point where if we do 0.3,
[01:34:49.940 --> 01:34:52.080]   almost everything is in 8-bit.
[01:34:52.080 --> 01:34:53.900]   Your master weights are in 16-bit,
[01:34:53.900 --> 01:34:56.180]   and your optimizers stay through even in 8-bit.
[01:34:56.180 --> 01:34:57.820]   I'm scared to play around with that.
[01:34:57.820 --> 01:35:00.780]   I don't know necessarily how good that is.
[01:35:00.780 --> 01:35:02.420]   I need to play around with it, and that's
[01:35:02.420 --> 01:35:04.620]   sort of what I'm using the LLAMA3 training for,
[01:35:04.620 --> 01:35:06.700]   to just toy around with these things.
[01:35:06.700 --> 01:35:09.360]   But it opens up opportunities, if you
[01:35:09.360 --> 01:35:12.620]   have the compute, to do this.
[01:35:12.620 --> 01:35:14.820]   Now, the last part I'm going to very briefly talk about,
[01:35:14.820 --> 01:35:16.980]   and we can talk about this more in my office hours,
[01:35:16.980 --> 01:35:21.060]   is DeepSpeed by Microsoft and Fully Sharded Data Parallelism.
[01:35:21.060 --> 01:35:24.180]   These two are almost the exact same.
[01:35:24.180 --> 01:35:28.660]   DeepSpeed has a few tweaks and calls things a bit differently.
[01:35:28.660 --> 01:35:31.700]   But if you've done it in FSTP, it
[01:35:31.700 --> 01:35:34.300]   can be done in DeepSpeed and vice versa.
[01:35:34.300 --> 01:35:36.660]   A wonderful community member recently
[01:35:36.660 --> 01:35:39.340]   posted some documentation where he directly
[01:35:39.340 --> 01:35:41.420]   talked about this parameter in DeepSpeed
[01:35:41.420 --> 01:35:43.380]   is this parameter in FSTP.
[01:35:43.380 --> 01:35:45.700]   And generally what I've seen, it's
[01:35:45.700 --> 01:35:48.900]   a mix of if people prefer DeepSpeed or FSTP.
[01:35:48.900 --> 01:35:50.500]   It's usually a matter of, do you want
[01:35:50.500 --> 01:35:52.660]   to go with Microsoft and do their thing,
[01:35:52.660 --> 01:35:55.440]   or stick with PyTorch and just stay native?
[01:35:55.440 --> 01:35:57.460]   But either can be used interchangeably,
[01:35:57.460 --> 01:36:01.540]   as long as you're careful about setting up the config.
[01:36:01.540 --> 01:36:05.140]   So as a whole, Accelerate helps you scale out training,
[01:36:05.140 --> 01:36:07.700]   especially with using FSTP and DeepSpeed,
[01:36:07.700 --> 01:36:11.060]   to train these big models across a number of GPUs.
[01:36:11.060 --> 01:36:13.420]   You can use techniques like FB8 to potentially speed up
[01:36:13.420 --> 01:36:16.460]   training and reduce some of the computational overhead.
[01:36:16.460 --> 01:36:18.980]   But when using mixed precision in general,
[01:36:18.980 --> 01:36:21.260]   especially with FB8, be very careful
[01:36:21.260 --> 01:36:23.820]   about how you're doing it, because you could potentially
[01:36:23.820 --> 01:36:28.260]   lock yourself into that weight for you and everyone else.
[01:36:28.260 --> 01:36:32.620]   So I'll post this presentation, of course, in the Discord.
[01:36:32.620 --> 01:36:34.420]   But there's some handy links there
[01:36:34.420 --> 01:36:36.420]   that will help get you started with Accelerate,
[01:36:36.420 --> 01:36:39.140]   go through some concept guides to understand
[01:36:39.140 --> 01:36:41.780]   some of the internals, and really get you going.
[01:36:41.780 --> 01:36:43.900]   So yeah, there we go.
[01:36:43.900 --> 01:36:45.220]   Let's look at some questions.
[01:36:45.220 --> 01:36:52.020]   Let's see.
[01:36:52.020 --> 01:36:52.780]   I have one here.
[01:36:52.780 --> 01:36:55.720]   I thought that DeepSpeed 03 is the same as FSTP,
[01:36:55.720 --> 01:36:57.180]   but the other options in DeepSpeed
[01:36:57.180 --> 01:36:58.900]   weren't necessarily equivalent.
[01:36:58.900 --> 01:37:03.380]   It's gotten to a point where there's some equivalencies now.
[01:37:03.380 --> 01:37:04.820]   The chart talks about it.
[01:37:04.820 --> 01:37:08.060]   03 is definitely the equivalent of FSTP.
[01:37:08.060 --> 01:37:09.700]   But there's some tweaks that you can do,
[01:37:09.700 --> 01:37:13.460]   because FSTP gives you options to only upload certain things.
[01:37:13.460 --> 01:37:20.420]   I just want to mention that, OK, I didn't show you--
[01:37:20.420 --> 01:37:23.380]   there's a DeepSpeed and FSTP configs.
[01:37:23.380 --> 01:37:25.580]   Like, when you want to do multi-GPU training
[01:37:25.580 --> 01:37:29.980]   in Axolotl, you have to supply a config file.
[01:37:29.980 --> 01:37:32.540]   I'll show you some examples of those.
[01:37:36.500 --> 01:37:38.540]   Whenever Zach's done, I'll share my screen.
[01:37:38.540 --> 01:37:39.620]   Yep, sorry.
[01:37:39.620 --> 01:37:40.700]   I accidentally hit a link.
[01:37:40.700 --> 01:37:41.340]   There you go.
[01:37:41.340 --> 01:37:43.860]   OK, I'll just do it right now.
[01:37:43.860 --> 01:37:45.720]   Let me find--
[01:37:45.720 --> 01:37:48.740]   Can I add some clarification while we're pulling that up?
[01:37:48.740 --> 01:37:50.340]   Yeah.
[01:37:50.340 --> 01:37:53.260]   So one of the things, especially for the FSTP
[01:37:53.260 --> 01:37:55.580]   part in the Axolotl configs, is we
[01:37:55.580 --> 01:38:00.260]   try and move those FSTP-specific configs into the Axolotl.
[01:38:00.260 --> 01:38:03.220]   And then it maps them into Accelerate.
[01:38:03.220 --> 01:38:05.140]   What we found was that a lot of people
[01:38:05.140 --> 01:38:09.900]   were running Accelerate config and then setting things,
[01:38:09.900 --> 01:38:11.980]   and then they would go and use Axolotl,
[01:38:11.980 --> 01:38:14.540]   and they would have a mismatch in certain parameters.
[01:38:14.540 --> 01:38:17.420]   And what would happen was it just would break in a lot
[01:38:17.420 --> 01:38:18.940]   of situations.
[01:38:18.940 --> 01:38:21.140]   So what we actually recommended people do,
[01:38:21.140 --> 01:38:23.780]   we have a warning saying, just remove your Accelerate config,
[01:38:23.780 --> 01:38:27.660]   and then we will map all of those configurations
[01:38:27.660 --> 01:38:30.380]   that normally get set by Accelerate through--
[01:38:30.380 --> 01:38:33.100]   I think Accelerate uses environment variables
[01:38:33.100 --> 01:38:35.500]   to communicate that under the hood
[01:38:35.500 --> 01:38:37.820]   anyways when you use Accelerate launch.
[01:38:37.820 --> 01:38:40.740]   So we just mimic a lot of that just
[01:38:40.740 --> 01:38:47.340]   to avoid some of the headache of doing it one--
[01:38:47.340 --> 01:38:49.940]   running Accelerate config and getting a mismatch later on
[01:38:49.940 --> 01:38:51.740]   that just caused a lot of support issues.
[01:38:51.740 --> 01:38:53.780]   So that's just something that--
[01:38:53.780 --> 01:38:55.060]   No, that makes perfect sense.
[01:38:55.060 --> 01:38:56.900]   That's exactly the solution I recommend.
[01:38:56.900 --> 01:38:59.380]   Even I'm debating on rewriting half
[01:38:59.380 --> 01:39:01.820]   of our internals for the FSTP and DeepSpeed plugin
[01:39:01.820 --> 01:39:04.380]   because I don't necessarily want to rely on environment
[01:39:04.380 --> 01:39:05.100]   variables.
[01:39:05.100 --> 01:39:07.940]   And even setting it up, I'm sure, as you've experienced,
[01:39:07.940 --> 01:39:10.820]   normally is problematic at best.
[01:39:10.820 --> 01:39:13.620]   So yeah, that's a very smart way to go about it
[01:39:13.620 --> 01:39:14.660]   because it's--
[01:39:14.660 --> 01:39:16.900]   even we've had users that report issues and are like,
[01:39:16.900 --> 01:39:19.380]   well, it's because you set up your config wrong
[01:39:19.380 --> 01:39:21.500]   and you're using something else.
[01:39:21.500 --> 01:39:24.780]   Yeah, I mean, and so that's what you heard from Zach today
[01:39:24.780 --> 01:39:28.460]   about stage 1 to 3, BF16, all of that.
[01:39:28.460 --> 01:39:32.100]   That's all background that you might want to know,
[01:39:32.100 --> 01:39:34.180]   so demystify a little bit about what
[01:39:34.180 --> 01:39:36.420]   is happening when you supply these configs.
[01:39:36.420 --> 01:39:38.940]   What I do, honestly, is I just use a config.
[01:39:38.940 --> 01:39:45.020]   Again, I just use one of these, like 0, 1, 2, 3, or the BF16
[01:39:45.020 --> 01:39:49.180]   one, and kind of use it off the shelf, and then maybe consult.
[01:39:49.180 --> 01:39:50.740]   Zach has written a lot about this.
[01:39:50.740 --> 01:39:53.220]   I actually look at his presentation.
[01:39:53.220 --> 01:39:55.900]   He's given similar versions of this before and posted online.
[01:39:55.900 --> 01:39:58.340]   He will, today, post his slides.
[01:39:58.340 --> 01:40:00.220]   And I kind of fiddle with it a bit sometimes.
[01:40:00.220 --> 01:40:02.340]   But honestly, I just use ones that work
[01:40:02.340 --> 01:40:04.060]   if I want to parallelize my model,
[01:40:04.060 --> 01:40:05.820]   especially if you're using a bigger model
[01:40:05.820 --> 01:40:08.140]   and parallelize it across the GPUs,
[01:40:08.140 --> 01:40:10.460]   then I'll pick the right config.
[01:40:10.460 --> 01:40:14.260]   And you specify-- you have these configs in the Axelot or repo,
[01:40:14.260 --> 01:40:18.100]   and then you supply it to the config, the main config.
[01:40:18.100 --> 01:40:21.380]   I'll show you an example when we talk about modal in a second.
[01:40:21.380 --> 01:40:23.820]   Can I add a clarification on this one specifically?
[01:40:23.820 --> 01:40:24.540]   Yeah.
[01:40:24.540 --> 01:40:30.860]   With 0, 1, and 0, 2, specifically for DeepSpeed,
[01:40:30.860 --> 01:40:34.540]   I think the BF16 and FP16 can be set to auto,
[01:40:34.540 --> 01:40:37.540]   because DeepSpeed doesn't care about it
[01:40:37.540 --> 01:40:39.380]   until after the trainer is loaded.
[01:40:39.380 --> 01:40:42.140]   But for 0, 3, specifically--
[01:40:42.140 --> 01:40:44.380]   and I see Zach nodding his head--
[01:40:44.380 --> 01:40:47.020]   it needs to know ahead of time, specifically,
[01:40:47.020 --> 01:40:48.460]   that you're using BF16.
[01:40:48.460 --> 01:40:54.700]   So you can't set auto in the 0, 3 config
[01:40:54.700 --> 01:40:56.220]   if you want to use BF16.
[01:40:56.220 --> 01:41:00.060]   So that's why it's set as there's a specific 0, 3 BF16,
[01:41:00.060 --> 01:41:03.980]   because it needs to know that you want to load it in BF16
[01:41:03.980 --> 01:41:08.180]   before the trainer sees it or something along those lines.
[01:41:08.180 --> 01:41:11.460]   Maybe Zach can explain it better than I can.
[01:41:11.460 --> 01:41:13.420]   No, that's a pretty good explanation of it.
[01:41:13.420 --> 01:41:16.620]   It's something with DeepSpeed when
[01:41:16.620 --> 01:41:20.860]   it comes to setting up the actual call to DeepSpeed
[01:41:20.860 --> 01:41:22.380]   and initializing everything.
[01:41:22.380 --> 01:41:25.240]   It has to know well beforehand what we're actually doing,
[01:41:25.240 --> 01:41:26.940]   which makes it a little annoying whenever
[01:41:26.940 --> 01:41:28.580]   we're dealing with configs in that way.
[01:41:28.580 --> 01:41:39.140]   OK, I think we should probably move on
[01:41:39.140 --> 01:41:42.860]   to the next thing, which is training on modal.
[01:41:42.860 --> 01:41:44.980]   Or Zach, I just want to make sure you're done with--
[01:41:44.980 --> 01:41:46.180]   Yep, you're good.
[01:41:46.180 --> 01:41:47.740]   All right.
[01:41:47.740 --> 01:41:50.860]   So there's a lot of different ways you can train models.
[01:41:50.860 --> 01:41:55.500]   You can use RunPod, which Dan showed earlier.
[01:41:55.500 --> 01:41:56.820]   That was done on RunPod.
[01:41:56.820 --> 01:41:59.180]   That was the recording.
[01:41:59.180 --> 01:42:02.900]   If you look at the Axolotl docs, actually, it'll
[01:42:02.900 --> 01:42:05.020]   tell you a bit about RunPod.
[01:42:05.020 --> 01:42:06.620]   If you just search from RunPod here,
[01:42:06.620 --> 01:42:08.260]   you'll find a little bit there.
[01:42:08.260 --> 01:42:11.100]   But also, there's a Docker container
[01:42:11.100 --> 01:42:14.860]   for Axolotl, which is what you want to use most of the time.
[01:42:15.860 --> 01:42:18.860]   Wing, do you want to say anything about that?
[01:42:18.860 --> 01:42:21.300]   Like, what's your preferred way of running?
[01:42:21.300 --> 01:42:23.300]   How do you run it, stuff?
[01:42:23.300 --> 01:42:25.140]   Like, what's your compute?
[01:42:25.140 --> 01:42:30.980]   So on my local 3090s, I don't use Docker containers,
[01:42:30.980 --> 01:42:32.780]   just mostly because it's light development,
[01:42:32.780 --> 01:42:36.220]   and it's just not amenable to using Docker containers
[01:42:36.220 --> 01:42:37.340]   for that.
[01:42:37.340 --> 01:42:41.420]   But for general debugging issues that people are seeing,
[01:42:41.420 --> 01:42:44.380]   I will just generally just spin up a Docker container
[01:42:44.380 --> 01:42:49.060]   on my RunPod and debug the issue there,
[01:42:49.060 --> 01:42:51.220]   because it's just an environment that
[01:42:51.220 --> 01:42:55.980]   doesn't have all of the mess and mismatch of various packages
[01:42:55.980 --> 01:42:59.620]   that I might not have updated.
[01:42:59.620 --> 01:43:00.860]   Makes sense.
[01:43:00.860 --> 01:43:02.700]   And then, yeah, if you look at the readme,
[01:43:02.700 --> 01:43:06.340]   there's a whole bunch of stuff there about it.
[01:43:06.340 --> 01:43:07.900]   OK, so modal.
[01:43:07.900 --> 01:43:09.540]   What the hell is modal?
[01:43:09.540 --> 01:43:12.140]   So actually, so OK, like, just some general rule
[01:43:12.140 --> 01:43:15.020]   about this conference.
[01:43:15.020 --> 01:43:17.260]   We were pretty selective about the tools
[01:43:17.260 --> 01:43:19.420]   that we brought in to this conference
[01:43:19.420 --> 01:43:20.620]   that I'm going to talk about.
[01:43:20.620 --> 01:43:22.540]   I'm only going to talk about tools that I use
[01:43:22.540 --> 01:43:23.380]   or that I like.
[01:43:23.380 --> 01:43:27.300]   There's like hundreds of tools.
[01:43:27.300 --> 01:43:31.300]   One that I really like is modal.
[01:43:31.300 --> 01:43:33.060]   So what is modal?
[01:43:33.060 --> 01:43:37.380]   Modal is actually this really cool cloud native way
[01:43:37.380 --> 01:43:39.620]   to run Python code.
[01:43:39.620 --> 01:43:41.620]   And the thing that's really interesting about it
[01:43:41.620 --> 01:43:44.460]   is it has this--
[01:43:44.460 --> 01:43:47.740]   like, one innovation is it feels like local development,
[01:43:47.740 --> 01:43:49.340]   but it's actually remote development.
[01:43:49.340 --> 01:43:51.500]   It has nothing to do with fine tuning right now.
[01:43:51.500 --> 01:43:52.660]   I'm just telling you a little bit about modal
[01:43:52.660 --> 01:43:53.820]   so you have some background.
[01:43:53.820 --> 01:43:58.740]   And basically, it's also massively parallel.
[01:43:58.740 --> 01:44:02.180]   You can get-- so things like Axolotl,
[01:44:02.180 --> 01:44:06.140]   it can easily do fine tuning.
[01:44:06.140 --> 01:44:11.380]   Actually, like, Wing, how do you do hyperparameter search?
[01:44:12.180 --> 01:44:15.260]   With your Axolotl training, like, what do you like to do?
[01:44:15.260 --> 01:44:16.340]   - It's manual right now.
[01:44:16.340 --> 01:44:19.500]   It's like, you know, it's like changing learning rates,
[01:44:19.500 --> 01:44:20.900]   but yeah.
[01:44:20.900 --> 01:44:22.580]   - Makes sense.
[01:44:22.580 --> 01:44:26.220]   So like, a lot of times I do use something like modal
[01:44:26.220 --> 01:44:30.900]   or I'll use modal to do things like hyperparameter tuning.
[01:44:30.900 --> 01:44:32.700]   There's different ways to do hyperparameter tuning.
[01:44:32.700 --> 01:44:35.020]   It's not something you should focus on,
[01:44:35.020 --> 01:44:35.940]   like, in the beginning.
[01:44:35.940 --> 01:44:37.340]   And it's totally fine to do it manual.
[01:44:37.340 --> 01:44:38.460]   I do a lot of things manually.
[01:44:38.460 --> 01:44:41.660]   I use bash scripts sometimes
[01:44:41.660 --> 01:44:43.420]   to do, like, many different Axolotl runs.
[01:44:43.420 --> 01:44:46.220]   So it's very, like, Python native.
[01:44:46.220 --> 01:44:49.460]   There's these modal docs, which are here.
[01:44:49.460 --> 01:44:50.980]   If you're just getting started in modal,
[01:44:50.980 --> 01:44:53.780]   actually, like, to really experience this, like,
[01:44:53.780 --> 01:44:57.260]   magic of modal where you're like,
[01:44:57.260 --> 01:45:00.660]   what am I talking about this, like, local but it's remote?
[01:45:00.660 --> 01:45:02.460]   Like, what does that even mean?
[01:45:02.460 --> 01:45:04.140]   I don't even know how to explain it to you
[01:45:04.140 --> 01:45:08.100]   without you, like, trying it yourself.
[01:45:08.100 --> 01:45:09.300]   So, like, this is, like, I,
[01:45:09.300 --> 01:45:11.180]   so there's a lot of docs here in, like, modal.
[01:45:11.180 --> 01:45:12.940]   You can go through, like, the hello, getting started one.
[01:45:12.940 --> 01:45:14.900]   But I actually think, like,
[01:45:14.900 --> 01:45:18.540]   what I like to show people first is this, like, web endpoint one.
[01:45:18.540 --> 01:45:20.420]   I'm not gonna demo it right now 'cause I don't have time,
[01:45:20.420 --> 01:45:22.380]   but basically just, like, try it out.
[01:45:22.380 --> 01:45:24.860]   And basically what you wanna do is, like,
[01:45:24.860 --> 01:45:26.180]   you can change the code
[01:45:26.180 --> 01:45:28.540]   and you can see it change in production in real time.
[01:45:28.540 --> 01:45:31.420]   And you don't have to do these, like, deploys,
[01:45:31.420 --> 01:45:33.220]   like, constant deploys to change code.
[01:45:33.220 --> 01:45:35.500]   It's, like, this really iterative,
[01:45:35.500 --> 01:45:36.940]   like, interesting thing.
[01:45:36.940 --> 01:45:39.980]   And I built, like, lots of tools in modal.
[01:45:39.980 --> 01:45:41.380]   I have built, like, this transcript,
[01:45:41.380 --> 01:45:44.700]   meeting transcript summarizer with modal.
[01:45:44.700 --> 01:45:46.980]   There's also weights and biases, webhooks.
[01:45:46.980 --> 01:45:49.740]   The links are that are gonna be in the slides.
[01:45:49.740 --> 01:45:51.500]   So I won't labor that too much.
[01:45:51.500 --> 01:45:56.340]   The one thing about, so for modal, for axolotl,
[01:45:56.340 --> 01:45:59.340]   they have this repo called llm-finetuning.
[01:45:59.340 --> 01:46:03.220]   And it's a little bit different than,
[01:46:03.220 --> 01:46:05.460]   it's, like, wraps axolotl.
[01:46:05.460 --> 01:46:06.700]   So that's interesting.
[01:46:06.700 --> 01:46:09.300]   Like, axolotl is already wrapping so much.
[01:46:09.300 --> 01:46:11.540]   Why would we need to wrap axolotl?
[01:46:11.540 --> 01:46:13.620]   Well, actually, like, it's kind of interesting.
[01:46:13.620 --> 01:46:16.140]   Like, if you have a workflow that you really like,
[01:46:16.140 --> 01:46:19.620]   you might want to abstract it a little bit more.
[01:46:19.620 --> 01:46:22.100]   And plus, you can get all the benefits of modal
[01:46:22.100 --> 01:46:22.980]   by doing that.
[01:46:22.980 --> 01:46:27.460]   Certain things you might wanna know about this repo is,
[01:46:27.460 --> 01:46:29.980]   when you run the train,
[01:46:29.980 --> 01:46:31.620]   it automatically merges the LoRa
[01:46:31.620 --> 01:46:34.620]   back into the base model for you by default.
[01:46:34.620 --> 01:46:36.340]   You can turn it off.
[01:46:36.340 --> 01:46:38.180]   And then also, like, one key thing is,
[01:46:38.180 --> 01:46:39.980]   there's a data flag you have to pass.
[01:46:39.980 --> 01:46:43.780]   You can't rely on the data set in the config file.
[01:46:43.780 --> 01:46:45.660]   You have to pass a data flag.
[01:46:45.660 --> 01:46:49.020]   And then the deep speed config
[01:46:49.020 --> 01:46:51.140]   comes from the axolotl repo itself.
[01:46:51.140 --> 01:46:54.460]   So you have to reference sort of, like,
[01:46:54.460 --> 01:46:58.420]   the axolotl repo, what I was showing earlier,
[01:46:58.420 --> 01:47:03.180]   it's kind of like these are mounted into the environment,
[01:47:03.180 --> 01:47:04.660]   this deep speed configs.
[01:47:04.660 --> 01:47:07.820]   So it's kind of like a beginner's way of using
[01:47:07.820 --> 01:47:10.140]   sort of axolotl with modal,
[01:47:10.140 --> 01:47:14.340]   but it is something to try first.
[01:47:14.340 --> 01:47:16.220]   And like, it's kind of like, you can tweak it.
[01:47:16.220 --> 01:47:18.260]   You can tweak it, you can change the code.
[01:47:18.260 --> 01:47:22.940]   But basically, like, you know, there's the readme here.
[01:47:22.940 --> 01:47:24.260]   There's a way to get started.
[01:47:24.260 --> 01:47:28.860]   Obviously, you have to, you know, start modal, install it.
[01:47:28.860 --> 01:47:32.380]   And essentially, like, what you do is you clone this repo
[01:47:32.380 --> 01:47:35.820]   and then you launch this fine tuning job.
[01:47:35.820 --> 01:47:37.820]   And basically, like, this command,
[01:47:37.820 --> 01:47:41.780]   the detach thing just makes it run in the back,
[01:47:41.780 --> 01:47:42.820]   like, makes it run in the background
[01:47:42.820 --> 01:47:46.180]   so where you can do other things.
[01:47:46.180 --> 01:47:49.980]   But there's this, here's the entry point.
[01:47:49.980 --> 01:47:51.260]   This is basically where we're wrapping
[01:47:51.260 --> 01:47:56.060]   the axolotl CLI command in this train function.
[01:47:56.060 --> 01:48:00.020]   And then you pass in the config file and then the data.
[01:48:01.060 --> 01:48:03.900]   Okay, so it's like very similar to running axolotl,
[01:48:03.900 --> 01:48:05.740]   just wrapping axolotl.
[01:48:05.740 --> 01:48:08.860]   I'm gonna do a really quick video
[01:48:08.860 --> 01:48:12.060]   of what that looks like here.
[01:48:12.060 --> 01:48:16.420]   So, you know, just do a modal run.
[01:48:16.420 --> 01:48:21.820]   And then basically, you know, it will go ahead
[01:48:21.820 --> 01:48:25.900]   and do your axolotl run if you want.
[01:48:25.900 --> 01:48:29.420]   And this is like running the exact example in the repo.
[01:48:30.860 --> 01:48:31.860]   And you can do the same things.
[01:48:31.860 --> 01:48:33.460]   You can put your weights and biases
[01:48:33.460 --> 01:48:36.020]   and your hugging face token and so on and so forth.
[01:48:36.020 --> 01:48:43.500]   So let me go back to the example.
[01:48:43.500 --> 01:48:44.380]   Oh, sorry.
[01:48:44.380 --> 01:48:47.740]   Let me go back to the repo, sorry.
[01:48:47.740 --> 01:48:49.660]   And just to point out here,
[01:48:49.660 --> 01:48:51.380]   just to navigate yourself in the repo,
[01:48:51.380 --> 01:48:55.700]   there's this, actually, I'm gonna hit the period
[01:48:55.700 --> 01:48:58.220]   on my keyboard to show you VS Code real quick
[01:48:58.220 --> 01:48:59.860]   so I can just show you some code.
[01:49:00.860 --> 01:49:05.860]   And so the source code, like the code for modal
[01:49:05.860 --> 01:49:08.220]   is in this source folder.
[01:49:08.220 --> 01:49:11.380]   And the training part is maybe what you wanna take a look at
[01:49:11.380 --> 01:49:13.900]   if you're curious on like what is happening.
[01:49:13.900 --> 01:49:16.820]   And the entry point that we demoed right now
[01:49:16.820 --> 01:49:18.340]   is this train function.
[01:49:18.340 --> 01:49:20.220]   So there'll be a train function here.
[01:49:20.220 --> 01:49:24.980]   There'll be, you know, in this file right here.
[01:49:28.740 --> 01:49:31.220]   Let's see, and then the common.py,
[01:49:31.220 --> 01:49:32.900]   that's actually the setup.
[01:49:32.900 --> 01:49:35.020]   Okay, that sets up the environment.
[01:49:35.020 --> 01:49:38.420]   That sets up the Docker container
[01:49:38.420 --> 01:49:41.460]   and installs some dependencies
[01:49:41.460 --> 01:49:43.660]   and makes your secrets come in.
[01:49:43.660 --> 01:49:45.220]   You don't have to worry about this.
[01:49:45.220 --> 01:49:48.220]   I wouldn't actually look at this like in the beginning.
[01:49:48.220 --> 01:49:49.340]   I'm just showing you around
[01:49:49.340 --> 01:49:51.620]   so that if you wanted to dig in, you could check it out.
[01:49:51.620 --> 01:49:52.860]   I think it's pretty cool.
[01:49:52.860 --> 01:49:56.860]   And then one thing I want to point out
[01:49:56.860 --> 01:49:59.460]   is like there's these config files.
[01:49:59.460 --> 01:50:02.740]   If you wanna run the demo in the readme out of the box,
[01:50:02.740 --> 01:50:06.940]   there's this like very small training run
[01:50:06.940 --> 01:50:08.980]   that basically over fits on purpose.
[01:50:08.980 --> 01:50:13.620]   You just have to know that, okay, the data set here,
[01:50:13.620 --> 01:50:15.700]   this is just, this will get replaced
[01:50:15.700 --> 01:50:18.340]   by whatever the data flag that you pass in.
[01:50:18.340 --> 01:50:23.100]   And then you just know that like,
[01:50:23.100 --> 01:50:26.780]   okay, for this deep speed is actually being used here.
[01:50:26.780 --> 01:50:29.580]   So that's what we just talked about.
[01:50:29.580 --> 01:50:31.660]   That was the background that Zach gave.
[01:50:31.660 --> 01:50:35.380]   And this is actually being mounted from the Axolotl repo.
[01:50:35.380 --> 01:50:38.900]   'Cause remember the Axolotl repo has this deep speed configs
[01:50:38.900 --> 01:50:40.140]   and this is being used.
[01:50:40.140 --> 01:50:46.900]   So this is just orienting you to that.
[01:50:46.900 --> 01:50:50.420]   And let's go back to the slides.
[01:50:50.420 --> 01:50:53.860]   Oops, how do I go to the next slide?
[01:50:56.660 --> 01:50:59.260]   Another thing you might wanna do is debug the data.
[01:50:59.260 --> 01:51:00.900]   So like you can run an end to end,
[01:51:00.900 --> 01:51:02.500]   but remember I told you, you don't wanna do that.
[01:51:02.500 --> 01:51:03.980]   You don't wanna just train stuff.
[01:51:03.980 --> 01:51:05.260]   So if you wanna do your,
[01:51:05.260 --> 01:51:07.100]   have your own data inside modal,
[01:51:07.100 --> 01:51:09.660]   there I have this notebook here.
[01:51:09.660 --> 01:51:13.460]   So let's go to this notebook, whoops.
[01:51:13.460 --> 01:51:20.660]   Let me just go to the repo and go back
[01:51:20.660 --> 01:51:24.860]   and go to the notebook.
[01:51:24.860 --> 01:51:27.420]   So I have this notebook here about inspecting data.
[01:51:27.420 --> 01:51:34.780]   Okay, and I'm just gonna change this GitHub to nbsanity
[01:51:34.780 --> 01:51:37.460]   'cause it's easier to read.
[01:51:37.460 --> 01:51:43.780]   And basically this, you kinda do the same thing is like,
[01:51:43.780 --> 01:51:49.820]   just make sure, this is a way that you can inspect the data.
[01:51:49.820 --> 01:51:51.500]   So you can do modal run,
[01:51:51.500 --> 01:51:53.980]   but then pass a pre-proc only flag.
[01:51:54.940 --> 01:51:59.940]   And what happens is the logs will print out a run tag.
[01:51:59.940 --> 01:52:02.300]   And with that run tag,
[01:52:02.300 --> 01:52:06.860]   you can see the last run prepared folder essentially.
[01:52:06.860 --> 01:52:09.460]   And like the last run prepared folder,
[01:52:09.460 --> 01:52:12.460]   you can just get that data and analyze it
[01:52:12.460 --> 01:52:13.780]   the exact same way that I showed you
[01:52:13.780 --> 01:52:17.220]   in the Honeycomb example, essentially, which is like,
[01:52:17.220 --> 01:52:18.940]   you know, and then print out
[01:52:18.940 --> 01:52:21.300]   just to make sure the data is in the right format.
[01:52:21.300 --> 01:52:22.260]   So I think that's important.
[01:52:22.260 --> 01:52:24.700]   You might wanna do that if you're using this
[01:52:24.700 --> 01:52:26.740]   and just, this is a notebook that might help you.
[01:52:26.740 --> 01:52:31.260]   Okay, I think that's it.
[01:52:31.260 --> 01:52:33.540]   And yeah, we can do Q&A.
[01:52:33.540 --> 01:52:45.900]   - Okay, let's talk about, I will MC Q&A.
[01:52:45.900 --> 01:52:49.940]   We have some questions that were answered,
[01:52:49.940 --> 01:52:52.540]   but just so that people hear the answer,
[01:52:52.540 --> 01:52:56.900]   I'm gonna do a mix of open questions and answered questions.
[01:52:56.900 --> 01:53:03.140]   A couple of, in case they're common questions,
[01:53:03.140 --> 01:53:04.340]   will office hours be recorded?
[01:53:04.340 --> 01:53:05.660]   The answer there is yes.
[01:53:05.660 --> 01:53:15.420]   Are tiny models like 5.3 more or less suited for fine tuning?
[01:53:15.420 --> 01:53:16.980]   You answered that in text,
[01:53:16.980 --> 01:53:19.460]   but for others to hear it since it was highly voted,
[01:53:19.460 --> 01:53:22.060]   gonna tackle that, Hamel or anyone else?
[01:53:22.060 --> 01:53:25.020]   - I usually don't go smaller
[01:53:25.020 --> 01:53:26.300]   than a 7 billion parameter model
[01:53:26.300 --> 01:53:28.700]   'cause I haven't had to go smaller than that.
[01:53:28.700 --> 01:53:31.540]   Like, that's like a really sweet spot for me
[01:53:31.540 --> 01:53:34.660]   'cause the models are like kind of good enough
[01:53:34.660 --> 01:53:36.820]   and they're small enough, but I don't know,
[01:53:36.820 --> 01:53:39.740]   Wing or anyone else, do you have any opinions on this
[01:53:39.740 --> 01:53:40.780]   or seen anything?
[01:53:40.780 --> 01:53:47.060]   - I haven't spent a lot of time with the 5.3 models,
[01:53:47.060 --> 01:53:48.300]   mostly 'cause I wasn't impressed
[01:53:48.300 --> 01:53:49.980]   by, I guess, the 5.1 models
[01:53:49.980 --> 01:53:52.420]   and I feel like they were just way too small.
[01:53:52.420 --> 01:53:56.460]   And there's, I think with the smaller models,
[01:53:56.460 --> 01:53:57.980]   just the reasoning is worse.
[01:53:57.980 --> 01:54:01.020]   So I just, Lama 3 is good enough and it works.
[01:54:01.020 --> 01:54:03.020]   So yeah, it's 7 billion.
[01:54:03.020 --> 01:54:08.260]   - But how to determine the adapter rank?
[01:54:08.260 --> 01:54:09.180]   There are actually two parameters,
[01:54:09.180 --> 01:54:10.060]   this wasn't part of the question,
[01:54:10.060 --> 01:54:12.340]   but there are two parameters that go together.
[01:54:12.340 --> 01:54:15.420]   There's the adapter rank and then the adapter alpha.
[01:54:15.420 --> 01:54:18.700]   Someone said how to determine the adapter rank.
[01:54:18.700 --> 01:54:23.660]   What do you guys have to have for that one?
[01:54:23.660 --> 01:54:29.220]   - I just copied the config, so I don't determine anything.
[01:54:29.220 --> 01:54:32.260]   - Yeah, that's one of those hyper-parameters
[01:54:32.260 --> 01:54:33.860]   you should play with if you,
[01:54:33.860 --> 01:54:35.900]   assuming you have like good evaluations
[01:54:35.900 --> 01:54:39.300]   and to just understand, like, is your model,
[01:54:39.300 --> 01:54:42.380]   is that lower at that rank sufficient
[01:54:42.380 --> 01:54:43.820]   to like get good accuracy
[01:54:43.820 --> 01:54:45.540]   on what your downstream use case is?
[01:54:45.540 --> 01:54:50.540]   So 32, 16 and 32 is like a typically a good starting point
[01:54:50.540 --> 01:54:52.580]   that you see most people use.
[01:54:52.580 --> 01:54:54.740]   And then, so for rank it's,
[01:54:54.740 --> 01:54:56.420]   and then for alpha is usually,
[01:54:56.420 --> 01:54:59.860]   I believe the papers say it should be 2x the rank,
[01:54:59.860 --> 01:55:01.140]   2x the rank.
[01:55:01.140 --> 01:55:03.140]   And then if you're using something like,
[01:55:03.140 --> 01:55:04.900]   I think it was like RS Laura,
[01:55:04.900 --> 01:55:06.500]   it has something to do with the square root,
[01:55:06.500 --> 01:55:08.260]   but I try not to get into that.
[01:55:08.260 --> 01:55:12.300]   - There's a blog post, I'm forgetting,
[01:55:12.300 --> 01:55:13.660]   I think by Sebastian Rushka,
[01:55:13.660 --> 01:55:16.580]   where he actually does a grid search
[01:55:16.580 --> 01:55:19.260]   and talks about what works for those.
[01:55:19.260 --> 01:55:21.620]   So I'll try and share that with the community.
[01:55:21.620 --> 01:55:24.100]   Yeah.
[01:55:24.100 --> 01:55:25.900]   - Yeah, there's another thing that I do,
[01:55:25.900 --> 01:55:27.900]   and this is kind of a weird answer.
[01:55:27.900 --> 01:55:32.180]   I actually asked my friends who are a lot smarter than me.
[01:55:32.180 --> 01:55:34.660]   So there's this guy, Jono Whitaker.
[01:55:34.660 --> 01:55:38.060]   He like really understands a lot of stuff.
[01:55:38.060 --> 01:55:39.820]   I'm like, "Hey, what rank do you think I should use for this?"
[01:55:39.820 --> 01:55:41.660]   And he gives me some tips.
[01:55:41.660 --> 01:55:44.220]   Jono is actually speaking in this conference.
[01:55:44.220 --> 01:55:45.740]   He might not talk exactly about this,
[01:55:45.740 --> 01:55:47.060]   but he has a really cool talk
[01:55:47.060 --> 01:55:48.940]   called "Napkin Math for Fine Tuning,"
[01:55:48.940 --> 01:55:51.780]   which you should check out.
[01:55:51.780 --> 01:55:56.980]   - Yeah, I'm going to switch over to some open questions.
[01:55:56.980 --> 01:55:59.900]   I'll take the one that's listed up top.
[01:55:59.900 --> 01:56:04.420]   I have a custom evaluation or benchmark for my model.
[01:56:04.420 --> 01:56:06.460]   Is there a way I can get it to run periodically
[01:56:06.460 --> 01:56:09.740]   during fine tuning to see how the training is going so far
[01:56:09.740 --> 01:56:11.380]   against that evaluation metric?
[01:56:11.380 --> 01:56:12.580]   This is actually something that I've wanted.
[01:56:12.580 --> 01:56:13.420]   I don't know the answer to it,
[01:56:13.420 --> 01:56:15.780]   but it's something that I've wanted in the past.
[01:56:15.780 --> 01:56:23.260]   Wing, I think that's, since I just read it.
[01:56:23.260 --> 01:56:24.940]   Does that question make sense to you?
[01:56:24.940 --> 01:56:26.380]   Do you understand the question?
[01:56:26.380 --> 01:56:27.220]   - No.
[01:56:27.220 --> 01:56:28.580]   - Can you have like an evaluation function
[01:56:28.580 --> 01:56:31.300]   in Axolotl or something, some callback or something?
[01:56:31.300 --> 01:56:34.420]   Like if you want to compute some like custom
[01:56:34.420 --> 01:56:38.420]   evaluation metrics, like how do you deal?
[01:56:38.420 --> 01:56:39.260]   Do you do that?
[01:56:39.260 --> 01:56:41.260]   Do you, how you deal with it?
[01:56:41.260 --> 01:56:45.820]   - Like there's like the tiny benchmarks that you can run
[01:56:45.820 --> 01:56:48.980]   sort of against sort of the more standard benchmarks.
[01:56:48.980 --> 01:56:53.980]   As far as trying to get more like custom evaluations,
[01:56:53.980 --> 01:56:58.500]   it's not really supported right now.
[01:56:58.500 --> 01:57:01.780]   I think you could do things by adding like callbacks
[01:57:01.780 --> 01:57:06.780]   on the evaluation loop maybe, and like doing some janky,
[01:57:07.820 --> 01:57:12.820]   you know, pulling from like disk, like things you wanted
[01:57:12.820 --> 01:57:17.340]   to, I guess, so here's something you could probably try.
[01:57:17.340 --> 01:57:22.340]   So there is a way, I think on the evaluation,
[01:57:22.340 --> 01:57:25.740]   if you were to specify a custom test data set
[01:57:25.740 --> 01:57:30.100]   for your evaluations, you can have it generate predictions
[01:57:30.100 --> 01:57:33.380]   for those at certain steps and then log those out
[01:57:33.380 --> 01:57:34.740]   to weights and biases.
[01:57:34.740 --> 01:57:37.740]   And then you could like pull those from weights and biases
[01:57:37.740 --> 01:57:40.500]   and then do your own like evaluations using like LN
[01:57:40.500 --> 01:57:43.460]   as a judge or something along those lines.
[01:57:43.460 --> 01:57:45.180]   That would be one way you could do it,
[01:57:45.180 --> 01:57:48.060]   but there's nothing like directly integrated right now
[01:57:48.060 --> 01:57:50.180]   that sort of streamlined for that.
[01:57:50.180 --> 01:57:56.620]   - How would you do that dumping of predictions in Axolotl?
[01:57:56.620 --> 01:57:59.380]   Like how would you do that?
[01:57:59.380 --> 01:58:01.780]   - Yeah, so it's already built in.
[01:58:01.780 --> 01:58:04.500]   I think there's something called the eval table,
[01:58:04.500 --> 01:58:08.020]   something setting in Axolotl.
[01:58:08.020 --> 01:58:10.820]   What it does is it will pull some number of prompts
[01:58:10.820 --> 01:58:12.740]   from your test data set,
[01:58:12.740 --> 01:58:17.540]   and then run predictions during the evaluation step
[01:58:17.540 --> 01:58:22.140]   and then log those out to, log those to weights and biases.
[01:58:22.140 --> 01:58:26.420]   I think it's like eval table something.
[01:58:26.420 --> 01:58:29.620]   It's a little bit flaky.
[01:58:29.620 --> 01:58:34.340]   So it's not like a top level thing that I've used.
[01:58:34.340 --> 01:58:37.300]   I think there was a contributor who submitted that,
[01:58:37.300 --> 01:58:39.700]   yeah, eval table size and eval max.
[01:58:39.700 --> 01:58:44.340]   So I believe the table size is the number of,
[01:58:44.340 --> 01:58:47.420]   yeah, the number of predictions that you wanna do.
[01:58:47.420 --> 01:58:51.060]   And then the number of max tokens is how long you wanna like,
[01:58:51.060 --> 01:58:53.300]   how many tokens you would like it to generate
[01:58:53.300 --> 01:58:54.500]   during that eval step.
[01:58:54.500 --> 01:58:57.300]   - That makes sense.
[01:58:57.300 --> 01:59:01.660]   - Good question, I like this one.
[01:59:01.660 --> 01:59:05.140]   Given Axolotl as a wrapper for some PuggingFace libraries,
[01:59:05.140 --> 01:59:07.340]   are there any important edge cases of functionality
[01:59:07.340 --> 01:59:09.740]   that you can do in the lower level libraries
[01:59:09.740 --> 01:59:12.220]   that aren't yet possible in Axolotl?
[01:59:12.220 --> 01:59:17.020]   - I'm sure there are a lot of things that you could do.
[01:59:17.020 --> 01:59:17.860]   - There's tons, yeah.
[01:59:17.860 --> 01:59:19.060]   There's tons, 'cause then you're operating
[01:59:19.060 --> 01:59:20.500]   at the code level, yeah.
[01:59:20.500 --> 01:59:22.420]   - It's hard to be there with everything else
[01:59:22.420 --> 01:59:25.540]   that goes on underneath, so like, yeah.
[01:59:25.540 --> 01:59:27.820]   - Yeah, you can have custom callbacks and stuff.
[01:59:27.820 --> 01:59:31.500]   You can do this eval thing that we were just talking about.
[01:59:31.500 --> 01:59:33.100]   You can do all kinds of stuff.
[01:59:33.100 --> 01:59:34.860]   - Yeah, I think it would especially be like,
[01:59:34.860 --> 01:59:36.980]   at the speed that Wing can implement
[01:59:36.980 --> 01:59:38.860]   whatever we chuck into Accelerate,
[01:59:38.860 --> 01:59:41.980]   and more specifically, we can then chuck into the trainer,
[01:59:41.980 --> 01:59:43.300]   and it's whatever that gap is,
[01:59:43.300 --> 01:59:46.300]   is the bleeding edge that you don't have access to.
[01:59:46.300 --> 01:59:47.580]   You know, and so like, that could be like,
[01:59:47.580 --> 01:59:50.300]   new FSTP techniques, new deep speed techniques
[01:59:50.300 --> 01:59:52.380]   that get added that we need to update in Accelerate,
[01:59:52.380 --> 01:59:54.140]   and then push to the trainer.
[01:59:54.140 --> 01:59:56.020]   That I think, for the most part,
[01:59:56.020 --> 01:59:57.540]   should be the most major gap,
[01:59:57.540 --> 01:59:59.980]   because we try and shove everything we can
[01:59:59.980 --> 02:00:01.860]   in Accelerate into the trainer
[02:00:01.860 --> 02:00:03.460]   that then Wing gets for free.
[02:00:03.460 --> 02:00:08.780]   - But I think this flexibility for callbacks
[02:00:08.780 --> 02:00:11.460]   during training with whatever you want to do,
[02:00:11.460 --> 02:00:15.020]   like, at each batch, or at whatever frequency,
[02:00:15.020 --> 02:00:16.660]   to calculate custom evaluation metrics,
[02:00:16.660 --> 02:00:19.500]   or stuff your data, who knows where,
[02:00:19.500 --> 02:00:21.540]   that would be like, the sort of thing.
[02:00:21.540 --> 02:00:22.820]   There aren't a ton of use cases for that,
[02:00:22.820 --> 02:00:26.140]   but doing stuff in between batches,
[02:00:26.140 --> 02:00:30.620]   these sort of callbacks seems like an example.
[02:00:30.620 --> 02:00:32.100]   - Yeah, but you might be wondering like,
[02:00:32.100 --> 02:00:34.460]   okay, if you, why use Axolotl?
[02:00:34.460 --> 02:00:36.220]   Is it worth bringing that up again?
[02:00:36.220 --> 02:00:38.580]   And I just want to like, one example,
[02:00:38.580 --> 02:00:40.260]   is like, because there's a lot of stuff
[02:00:40.260 --> 02:00:41.700]   that you need to glue together,
[02:00:41.700 --> 02:00:43.740]   especially if you don't have a lot of GPUs.
[02:00:43.740 --> 02:00:47.020]   So like, one example that came out recently is like,
[02:00:47.020 --> 02:00:51.180]   you know, QLORA working with FSTP,
[02:00:51.180 --> 02:00:53.260]   for the longest time didn't work,
[02:00:53.260 --> 02:00:57.540]   and the AnswerAI team kind of enabled that.
[02:00:57.540 --> 02:00:59.260]   And then within hours,
[02:00:59.260 --> 02:01:02.700]   Wing, like, glued it into Axolotl,
[02:01:02.700 --> 02:01:05.140]   like, really before anyone else.
[02:01:05.140 --> 02:01:10.140]   So I was able to use it like, almost right away.
[02:01:10.140 --> 02:01:13.100]   And Wing keeps doing that, like, over and over again,
[02:01:13.100 --> 02:01:14.700]   for like, anything that happens.
[02:01:14.700 --> 02:01:17.620]   Like, you know, the LMS space is like,
[02:01:17.620 --> 02:01:20.660]   changing extremely fast, like, from,
[02:01:20.700 --> 02:01:22.780]   you know, LMS, to QLORA.
[02:01:22.780 --> 02:01:24.140]   And it's like, really fast.
[02:01:24.140 --> 02:01:26.740]   So I was able to use it like that for a long time.
[02:01:26.740 --> 02:01:27.780]   And it's like, I don't know,
[02:01:27.780 --> 02:01:29.460]   like, I was able to use it for a long time.
[02:01:29.460 --> 02:01:31.260]   And it's like, I've been using it for a long time.
[02:01:31.260 --> 02:01:32.580]   And it's like, I've been using it for a long time.
[02:01:32.580 --> 02:01:33.660]   And it's like, I've been using it for a long time.
[02:01:33.660 --> 02:01:34.500]   And it's like, I've been using it for a long time.
[02:01:34.500 --> 02:01:35.340]   And it's like, I've been using it for a long time.
[02:01:35.340 --> 02:01:36.180]   And it's like, I've been using it for a long time.
[02:01:36.180 --> 02:01:37.020]   And it's like, I've been using it for a long time.
[02:01:37.020 --> 02:01:37.860]   And it's like, I've been using it for a long time.
[02:01:37.860 --> 02:01:38.700]   And it's like, I've been using it for a long time.
[02:01:38.700 --> 02:01:43.700]   And it's like, I've been using it for a long time.
[02:01:43.700 --> 02:01:48.700]   And it's like, I've been using it for a long time.
[02:01:48.700 --> 02:01:53.700]   And it's like, I've been using it for a long time.
[02:01:53.700 --> 02:01:58.700]   And it's like, I've been using it for a long time.
[02:01:58.700 --> 02:02:03.700]   And it's like, I've been using it for a long time.
[02:02:03.700 --> 02:02:08.700]   And it's like, I've been using it for a long time.
[02:02:08.700 --> 02:02:13.700]   And it's like, I've been using it for a long time.
[02:02:13.700 --> 02:02:18.700]   And it's like, I've been using it for a long time.
[02:02:18.700 --> 02:02:23.700]   And it's like, I've been using it for a long time.
[02:02:23.700 --> 02:02:28.700]   And it's like, I've been using it for a long time.
[02:02:28.700 --> 02:02:33.700]   And it's like, I've been using it for a long time.
[02:02:33.700 --> 02:02:38.700]   And it's like, I've been using it for a long time.
[02:02:38.700 --> 02:02:43.700]   And it's like, I've been using it for a long time.
[02:02:43.700 --> 02:02:48.700]   And it's like, I've been using it for a long time.
[02:02:48.700 --> 02:02:53.700]   And it's like, I've been using it for a long time.
[02:02:53.700 --> 02:02:58.700]   And it's like, I've been using it for a long time.
[02:02:58.700 --> 02:03:03.700]   And it's like, I've been using it for a long time.
[02:03:03.700 --> 02:03:08.700]   And it's like, I've been using it for a long time.
[02:03:08.700 --> 02:03:13.700]   And it's like, I've been using it for a long time.
[02:03:13.700 --> 02:03:18.700]   And it's like, I've been using it for a long time.
[02:03:18.700 --> 02:03:23.700]   And it's like, I've been using it for a long time.
[02:03:23.700 --> 02:03:28.700]   And it's like, I've been using it for a long time.
[02:03:28.700 --> 02:03:33.700]   And it's like, I've been using it for a long time.
[02:03:33.700 --> 02:03:38.700]   And it's like, I've been using it for a long time.
[02:03:38.700 --> 02:03:43.700]   And it's like, I've been using it for a long time.
[02:03:43.700 --> 02:03:48.700]   And it's like, I've been using it for a long time.
[02:03:48.700 --> 02:03:53.700]   And it's like, I've been using it for a long time.
[02:03:53.700 --> 02:03:58.700]   And it's like, I've been using it for a long time.
[02:03:58.700 --> 02:04:03.700]   And it's like, I've been using it for a long time.
[02:04:03.700 --> 02:04:08.700]   And it's like, I've been using it for a long time.
[02:04:08.700 --> 02:04:13.700]   And it's like, I've been using it for a long time.
[02:04:13.700 --> 02:04:18.700]   And it's like, I've been using it for a long time.
[02:04:18.700 --> 02:04:23.700]   And it's like, I've been using it for a long time.
[02:04:23.700 --> 02:04:28.700]   And it's like, I've been using it for a long time.
[02:04:28.700 --> 02:04:33.700]   And it's like, I've been using it for a long time.
[02:04:33.700 --> 02:04:38.700]   And it's like, I've been using it for a long time.
[02:04:38.700 --> 02:04:43.700]   And it's like, I've been using it for a long time.
[02:04:43.700 --> 02:04:48.700]   And it's like, I've been using it for a long time.
[02:04:48.700 --> 02:04:53.700]   And it's like, I've been using it for a long time.
[02:04:53.700 --> 02:04:58.700]   And it's like, I've been using it for a long time.
[02:04:58.700 --> 02:05:03.700]   And it's like, I've been using it for a long time.
[02:05:03.700 --> 02:05:08.700]   And it's like, I've been using it for a long time.
[02:05:08.700 --> 02:05:13.700]   And it's like, I've been using it for a long time.
[02:05:13.700 --> 02:05:18.700]   And it's like, I've been using it for a long time.
[02:05:18.700 --> 02:05:23.700]   And it's like, I've been using it for a long time.
[02:05:23.700 --> 02:05:28.700]   And it's like, I've been using it for a long time.
[02:05:28.700 --> 02:05:33.700]   And it's like, I've been using it for a long time.
[02:05:33.700 --> 02:05:38.700]   And it's like, I've been using it for a long time.
[02:05:38.700 --> 02:05:43.700]   And it's like, I've been using it for a long time.
[02:05:43.700 --> 02:05:48.700]   And it's like, I've been using it for a long time.
[02:05:48.700 --> 02:05:53.700]   And it's like, I've been using it for a long time.
[02:05:53.700 --> 02:05:58.700]   And it's like, I've been using it for a long time.
[02:05:58.700 --> 02:06:03.700]   And it's like, I've been using it for a long time.
[02:06:03.700 --> 02:06:08.700]   And it's like, I've been using it for a long time.
[02:06:08.700 --> 02:06:13.700]   And it's like, I've been using it for a long time.
[02:06:13.700 --> 02:06:18.700]   And it's like, I've been using it for a long time.
[02:06:18.700 --> 02:06:23.700]   And it's like, I've been using it for a long time.
[02:06:23.700 --> 02:06:28.700]   And it's like, I've been using it for a long time.
[02:06:28.700 --> 02:06:33.700]   And it's like, I've been using it for a long time.
[02:06:33.700 --> 02:06:38.700]   And it's like, I've been using it for a long time.
[02:06:38.700 --> 02:06:43.700]   And it's like, I've been using it for a long time.
[02:06:43.700 --> 02:06:48.700]   And it's like, I've been using it for a long time.
[02:06:48.700 --> 02:06:53.700]   And it's like, I've been using it for a long time.
[02:06:53.700 --> 02:06:58.700]   And it's like, I've been using it for a long time.
[02:06:58.700 --> 02:07:03.700]   And it's like, I've been using it for a long time.
[02:07:03.700 --> 02:07:08.700]   And it's like, I've been using it for a long time.
[02:07:08.700 --> 02:07:13.700]   And it's like, I've been using it for a long time.
[02:07:13.700 --> 02:07:18.700]   And it's like, I've been using it for a long time.
[02:07:18.700 --> 02:07:23.700]   And it's like, I've been using it for a long time.
[02:07:23.700 --> 02:07:28.700]   And it's like, I've been using it for a long time.
[02:07:28.700 --> 02:07:33.700]   And it's like, I've been using it for a long time.
[02:07:33.700 --> 02:07:38.700]   And it's like, I've been using it for a long time.
[02:07:38.700 --> 02:07:43.700]   And it's like, I've been using it for a long time.
[02:07:43.700 --> 02:07:48.700]   And it's like, I've been using it for a long time.
[02:07:48.700 --> 02:07:53.700]   And it's like, I've been using it for a long time.
[02:07:53.700 --> 02:07:58.700]   And it's like, I've been using it for a long time.
[02:07:58.700 --> 02:08:03.700]   And it's like, I've been using it for a long time.
[02:08:03.700 --> 02:08:08.700]   And it's like, I've been using it for a long time.
[02:08:08.700 --> 02:08:13.700]   And it's like, I've been using it for a long time.
[02:08:13.700 --> 02:08:18.700]   And it's like, I've been using it for a long time.
[02:08:18.700 --> 02:08:23.700]   And it's like, I've been using it for a long time.
[02:08:23.700 --> 02:08:28.700]   And it's like, I've been using it for a long time.
[02:08:28.700 --> 02:08:33.700]   And it's like, I've been using it for a long time.
[02:08:33.700 --> 02:08:38.700]   And it's like, I've been using it for a long time.
[02:08:38.700 --> 02:08:43.700]   And it's like, I've been using it for a long time.
[02:08:43.700 --> 02:08:48.700]   And it's like, I've been using it for a long time.
[02:08:48.700 --> 02:08:53.700]   And it's like, I've been using it for a long time.
[02:08:53.700 --> 02:08:58.700]   And it's like, I've been using it for a long time.
[02:08:58.700 --> 02:09:03.700]   And it's like, I've been using it for a long time.
[02:09:03.700 --> 02:09:08.700]   And it's like, I've been using it for a long time.
[02:09:08.700 --> 02:09:13.700]   And it's like, I've been using it for a long time.
[02:09:13.700 --> 02:09:18.700]   And it's like, I've been using it for a long time.
[02:09:18.700 --> 02:09:23.700]   And it's like, I've been using it for a long time.
[02:09:23.700 --> 02:09:28.700]   And it's like, I've been using it for a long time.
[02:09:28.700 --> 02:09:33.700]   And it's like, I've been using it for a long time.
[02:09:33.700 --> 02:09:38.700]   And it's like, I've been using it for a long time.
[02:09:38.700 --> 02:09:43.700]   And it's like, I've been using it for a long time.
[02:09:43.700 --> 02:09:48.700]   And it's like, I've been using it for a long time.
[02:09:48.700 --> 02:09:53.700]   And it's like, I've been using it for a long time.
[02:09:53.700 --> 02:09:58.700]   And it's like, I've been using it for a long time.
[02:09:58.700 --> 02:10:03.700]   And it's like, I've been using it for a long time.
[02:10:03.700 --> 02:10:08.700]   And it's like, I've been using it for a long time.
[02:10:08.700 --> 02:10:13.700]   And it's like, I've been using it for a long time.
[02:10:13.700 --> 02:10:18.700]   And it's like, I've been using it for a long time.
[02:10:18.700 --> 02:10:23.700]   And it's like, I've been using it for a long time.
[02:10:23.700 --> 02:10:28.700]   And it's like, I've been using it for a long time.
[02:10:28.700 --> 02:10:33.700]   And it's like, I've been using it for a long time.
[02:10:33.700 --> 02:10:38.700]   And it's like, I've been using it for a long time.
[02:10:38.700 --> 02:10:43.700]   And it's like, I've been using it for a long time.
[02:10:43.700 --> 02:10:48.700]   And it's like, I've been using it for a long time.
[02:10:48.700 --> 02:10:53.700]   And it's like, I've been using it for a long time.
[02:10:53.700 --> 02:10:58.700]   And it's like, I've been using it for a long time.
[02:10:58.700 --> 02:11:03.700]   And it's like, I've been using it for a long time.
[02:11:03.700 --> 02:11:08.700]   And it's like, I've been using it for a long time.
[02:11:08.700 --> 02:11:13.700]   And it's like, I've been using it for a long time.
[02:11:13.700 --> 02:11:18.700]   And it's like, I've been using it for a long time.
[02:11:18.700 --> 02:11:23.700]   And it's like, I've been using it for a long time.
[02:11:23.700 --> 02:11:28.700]   And it's like, I've been using it for a long time.
[02:11:28.700 --> 02:11:33.700]   And it's like, I've been using it for a long time.
[02:11:33.700 --> 02:11:38.700]   And it's like, I've been using it for a long time.
[02:11:38.700 --> 02:11:43.700]   And it's like, I've been using it for a long time.
[02:11:43.700 --> 02:11:48.700]   And it's like, I've been using it for a long time.
[02:11:48.700 --> 02:11:53.700]   And it's like, I've been using it for a long time.
[02:11:53.700 --> 02:11:58.700]   And it's like, I've been using it for a long time.
[02:11:58.700 --> 02:12:03.700]   And it's like, I've been using it for a long time.
[02:12:03.700 --> 02:12:08.700]   And it's like, I've been using it for a long time.
[02:12:08.700 --> 02:12:13.700]   And it's like, I've been using it for a long time.
[02:12:13.700 --> 02:12:18.700]   And it's like, I've been using it for a long time.
[02:12:18.700 --> 02:12:23.700]   And it's like, I've been using it for a long time.
[02:12:23.700 --> 02:12:28.700]   And it's like, I've been using it for a long time.
[02:12:28.700 --> 02:12:33.700]   And it's like, I've been using it for a long time.
[02:12:33.700 --> 02:12:38.700]   And it's like, I've been using it for a long time.
[02:12:38.700 --> 02:12:43.700]   And it's like, I've been using it for a long time.
[02:12:43.700 --> 02:12:48.700]   And it's like, I've been using it for a long time.
[02:12:48.700 --> 02:12:53.700]   And it's like, I've been using it for a long time.
[02:12:53.700 --> 02:12:58.700]   And it's like, I've been using it for a long time.
[02:12:58.700 --> 02:13:03.700]   And it's like, I've been using it for a long time.
[02:13:03.700 --> 02:13:08.700]   And it's like, I've been using it for a long time.
[02:13:08.700 --> 02:13:13.700]   And it's like, I've been using it for a long time.
[02:13:13.700 --> 02:13:18.700]   And it's like, I've been using it for a long time.
[02:13:18.700 --> 02:13:23.700]   And it's like, I've been using it for a long time.
[02:13:23.700 --> 02:13:28.700]   And it's like, I've been using it for a long time.
[02:13:28.700 --> 02:13:33.700]   And it's like, I've been using it for a long time.
[02:13:33.700 --> 02:13:38.700]   And it's like, I've been using it for a long time.
[02:13:38.700 --> 02:13:43.700]   And it's like, I've been using it for a long time.
[02:13:43.700 --> 02:13:48.700]   And it's like, I've been using it for a long time.
[02:13:48.700 --> 02:13:53.700]   And it's like, I've been using it for a long time.
[02:13:53.700 --> 02:13:58.700]   And it's like, I've been using it for a long time.
[02:13:58.700 --> 02:14:03.700]   And it's like, I've been using it for a long time.
[02:14:03.700 --> 02:14:08.700]   And it's like, I've been using it for a long time.
[02:14:08.700 --> 02:14:13.700]   And it's like, I've been using it for a long time.
[02:14:13.700 --> 02:14:18.700]   And it's like, I've been using it for a long time.
[02:14:18.700 --> 02:14:23.700]   And it's like, I've been using it for a long time.
[02:14:23.700 --> 02:14:28.700]   And it's like, I've been using it for a long time.
[02:14:28.700 --> 02:14:33.700]   And it's like, I've been using it for a long time.
[02:14:33.700 --> 02:14:38.700]   And it's like, I've been using it for a long time.
[02:14:38.700 --> 02:14:43.700]   And it's like, I've been using it for a long time.
[02:14:43.700 --> 02:14:48.700]   And it's like, I've been using it for a long time.
[02:14:48.700 --> 02:14:53.700]   And it's like, I've been using it for a long time.
[02:14:53.700 --> 02:14:58.700]   And it's like, I've been using it for a long time.
[02:14:58.700 --> 02:15:03.700]   And it's like, I've been using it for a long time.
[02:15:03.700 --> 02:15:08.700]   And it's like, I've been using it for a long time.
[02:15:08.700 --> 02:15:13.700]   And it's like, I've been using it for a long time.
[02:15:13.700 --> 02:15:18.700]   And it's like, I've been using it for a long time.
[02:15:18.700 --> 02:15:23.700]   And it's like, I've been using it for a long time.
[02:15:23.700 --> 02:15:28.700]   And it's like, I've been using it for a long time.
[02:15:28.700 --> 02:15:33.700]   And it's like, I've been using it for a long time.
[02:15:33.700 --> 02:15:38.700]   And it's like, I've been using it for a long time.
[02:15:38.700 --> 02:15:43.700]   And it's like, I've been using it for a long time.
[02:15:43.700 --> 02:15:48.700]   And it's like, I've been using it for a long time.
[02:15:48.700 --> 02:15:53.700]   And it's like, I've been using it for a long time.
[02:15:53.700 --> 02:15:58.700]   And it's like, I've been using it for a long time.
[02:15:58.700 --> 02:16:03.700]   And it's like, I've been using it for a long time.
[02:16:03.700 --> 02:16:08.700]   And it's like, I've been using it for a long time.
[02:16:08.700 --> 02:16:13.700]   And it's like, I've been using it for a long time.
[02:16:13.700 --> 02:16:18.700]   And it's like, I've been using it for a long time.
[02:16:18.700 --> 02:16:23.700]   And it's like, I've been using it for a long time.
[02:16:23.700 --> 02:16:28.700]   And it's like, I've been using it for a long time.
[02:16:28.700 --> 02:16:33.700]   And it's like, I've been using it for a long time.
[02:16:33.700 --> 02:16:38.700]   And it's like, I've been using it for a long time.
[02:16:38.700 --> 02:16:43.700]   And it's like, I've been using it for a long time.
[02:16:43.700 --> 02:16:48.700]   And it's like, I've been using it for a long time.
[02:16:48.700 --> 02:16:53.700]   And it's like, I've been using it for a long time.
[02:16:53.700 --> 02:16:58.700]   And it's like, I've been using it for a long time.
[02:16:58.700 --> 02:17:03.700]   And it's like, I've been using it for a long time.
[02:17:03.700 --> 02:17:08.700]   And it's like, I've been using it for a long time.
[02:17:08.700 --> 02:17:13.700]   And it's like, I've been using it for a long time.
[02:17:13.700 --> 02:17:18.700]   And it's like, I've been using it for a long time.
[02:17:18.700 --> 02:17:23.700]   And it's like, I've been using it for a long time.
[02:17:23.700 --> 02:17:28.700]   And it's like, I've been using it for a long time.
[02:17:28.700 --> 02:17:33.700]   And it's like, I've been using it for a long time.
[02:17:33.700 --> 02:17:38.700]   And it's like, I've been using it for a long time.
[02:17:38.700 --> 02:17:43.700]   And it's like, I've been using it for a long time.
[02:17:43.700 --> 02:17:48.700]   And it's like, I've been using it for a long time.
[02:17:48.700 --> 02:17:53.700]   And it's like, I've been using it for a long time.
[02:17:53.700 --> 02:17:58.700]   And it's like, I've been using it for a long time.
[02:17:58.700 --> 02:18:03.700]   And it's like, I've been using it for a long time.
[02:18:03.700 --> 02:18:08.700]   And it's like, I've been using it for a long time.
[02:18:08.700 --> 02:18:13.700]   And it's like, I've been using it for a long time.
[02:18:13.700 --> 02:18:18.700]   And it's like, I've been using it for a long time.
[02:18:18.700 --> 02:18:23.700]   And it's like, I've been using it for a long time.
[02:18:23.700 --> 02:18:28.700]   And it's like, I've been using it for a long time.
[02:18:28.700 --> 02:18:33.700]   And it's like, I've been using it for a long time.
[02:18:33.700 --> 02:18:38.700]   And it's like, I've been using it for a long time.
[02:18:38.700 --> 02:18:43.700]   And it's like, I've been using it for a long time.
[02:18:43.700 --> 02:18:48.700]   And it's like, I've been using it for a long time.
[02:18:48.700 --> 02:18:53.700]   And it's like, I've been using it for a long time.
[02:18:53.700 --> 02:18:58.700]   And it's like, I've been using it for a long time.
[02:18:58.700 --> 02:19:03.700]   And it's like, I've been using it for a long time.
[02:19:03.700 --> 02:19:08.700]   And it's like, I've been using it for a long time.
[02:19:08.700 --> 02:19:13.700]   And it's like, I've been using it for a long time.
[02:19:13.700 --> 02:19:18.700]   And it's like, I've been using it for a long time.
[02:19:18.700 --> 02:19:23.700]   And it's like, I've been using it for a long time.
[02:19:23.700 --> 02:19:28.700]   And it's like, I've been using it for a long time.
[02:19:28.700 --> 02:19:33.700]   And it's like, I've been using it for a long time.
[02:19:33.700 --> 02:19:38.700]   And it's like, I've been using it for a long time.
[02:19:38.700 --> 02:19:43.700]   And it's like, I've been using it for a long time.
[02:19:43.700 --> 02:19:48.700]   And it's like, I've been using it for a long time.
[02:19:48.700 --> 02:19:53.700]   And it's like, I've been using it for a long time.
[02:19:53.700 --> 02:19:58.700]   And it's like, I've been using it for a long time.
[02:19:58.700 --> 02:20:03.700]   And it's like, I've been using it for a long time.
[02:20:03.700 --> 02:20:08.700]   And it's like, I've been using it for a long time.
[02:20:08.700 --> 02:20:13.700]   And it's like, I've been using it for a long time.
[02:20:13.700 --> 02:20:18.700]   And it's like, I've been using it for a long time.
[02:20:18.700 --> 02:20:23.700]   And it's like, I've been using it for a long time.
[02:20:23.700 --> 02:20:28.700]   And it's like, I've been using it for a long time.
[02:20:28.700 --> 02:20:33.700]   And it's like, I've been using it for a long time.
[02:20:33.700 --> 02:20:38.700]   And it's like, I've been using it for a long time.
[02:20:38.700 --> 02:20:43.700]   And it's like, I've been using it for a long time.
[02:20:43.700 --> 02:20:48.700]   And it's like, I've been using it for a long time.
[02:20:48.700 --> 02:20:53.700]   And it's like, I've been using it for a long time.
[02:20:53.700 --> 02:20:58.700]   And it's like, I've been using it for a long time.
[02:20:58.700 --> 02:21:03.700]   And it's like, I've been using it for a long time.
[02:21:03.700 --> 02:21:08.700]   And it's like, I've been using it for a long time.
[02:21:08.700 --> 02:21:13.700]   And it's like, I've been using it for a long time.
[02:21:13.700 --> 02:21:18.700]   And it's like, I've been using it for a long time.
[02:21:18.700 --> 02:21:23.700]   And it's like, I've been using it for a long time.
[02:21:23.700 --> 02:21:28.700]   And it's like, I've been using it for a long time.
[02:21:28.700 --> 02:21:33.700]   And it's like, I've been using it for a long time.
[02:21:33.700 --> 02:21:38.700]   And it's like, I've been using it for a long time.
[02:21:38.700 --> 02:21:43.700]   And it's like, I've been using it for a long time.
[02:21:43.700 --> 02:21:48.700]   And it's like, I've been using it for a long time.
[02:21:48.700 --> 02:21:53.700]   And it's like, I've been using it for a long time.
[02:21:53.700 --> 02:21:58.700]   And it's like, I've been using it for a long time.
[02:21:58.700 --> 02:22:03.700]   And it's like, I've been using it for a long time.
[02:22:03.700 --> 02:22:08.700]   And it's like, I've been using it for a long time.
[02:22:08.700 --> 02:22:13.700]   And it's like, I've been using it for a long time.
[02:22:13.700 --> 02:22:18.700]   And it's like, I've been using it for a long time.
[02:22:18.700 --> 02:22:23.700]   And it's like, I've been using it for a long time.
[02:22:23.700 --> 02:22:28.700]   And it's like, I've been using it for a long time.
[02:22:28.700 --> 02:22:33.700]   And it's like, I've been using it for a long time.
[02:22:33.700 --> 02:22:38.700]   And it's like, I've been using it for a long time.
[02:22:38.700 --> 02:22:43.700]   And it's like, I've been using it for a long time.
[02:22:43.700 --> 02:22:48.700]   And it's like, I've been using it for a long time.
[02:22:48.700 --> 02:22:53.700]   And it's like, I've been using it for a long time.
[02:22:53.700 --> 02:22:58.700]   And it's like, I've been using it for a long time.
[02:22:58.700 --> 02:23:03.700]   And it's like, I've been using it for a long time.
[02:23:03.700 --> 02:23:08.700]   And it's like, I've been using it for a long time.
[02:23:08.700 --> 02:23:13.700]   And it's like, I've been using it for a long time.
[02:23:13.700 --> 02:23:18.700]   And it's like, I've been using it for a long time.
[02:23:18.700 --> 02:23:23.700]   And it's like, I've been using it for a long time.
[02:23:23.700 --> 02:23:28.700]   And it's like, I've been using it for a long time.
[02:23:28.700 --> 02:23:33.700]   And it's like, I've been using it for a long time.
[02:23:33.700 --> 02:23:38.700]   And it's like, I've been using it for a long time.
[02:23:38.700 --> 02:23:43.700]   And it's like, I've been using it for a long time.
[02:23:43.700 --> 02:23:48.700]   And it's like, I've been using it for a long time.
[02:23:48.700 --> 02:23:53.700]   And it's like, I've been using it for a long time.
[02:23:53.700 --> 02:23:58.700]   And it's like, I've been using it for a long time.
[02:23:58.700 --> 02:24:03.700]   And it's like, I've been using it for a long time.
[02:24:03.700 --> 02:24:08.700]   And it's like, I've been using it for a long time.
[02:24:08.700 --> 02:24:13.700]   And it's like, I've been using it for a long time.
[02:24:13.700 --> 02:24:18.700]   And it's like, I've been using it for a long time.
[02:24:18.700 --> 02:24:23.700]   And it's like, I've been using it for a long time.
[02:24:23.700 --> 02:24:28.700]   And it's like, I've been using it for a long time.
[02:24:28.700 --> 02:24:33.700]   And it's like, I've been using it for a long time.
[02:24:33.700 --> 02:24:38.700]   And it's like, I've been using it for a long time.
[02:24:38.700 --> 02:24:43.700]   And it's like, I've been using it for a long time.
[02:24:43.700 --> 02:24:48.700]   And it's like, I've been using it for a long time.
[02:24:48.700 --> 02:24:53.700]   And it's like, I've been using it for a long time.
[02:24:53.700 --> 02:24:58.700]   And it's like, I've been using it for a long time.
[02:24:58.700 --> 02:25:03.700]   And it's like, I've been using it for a long time.
[02:25:03.700 --> 02:25:08.700]   And it's like, I've been using it for a long time.
[02:25:08.700 --> 02:25:13.700]   And it's like, I've been using it for a long time.
[02:25:13.700 --> 02:25:18.700]   And it's like, I've been using it for a long time.
[02:25:18.700 --> 02:25:23.700]   And it's like, I've been using it for a long time.
[02:25:23.700 --> 02:25:28.700]   And it's like, I've been using it for a long time.
[02:25:28.700 --> 02:25:33.700]   And it's like, I've been using it for a long time.
[02:25:33.700 --> 02:25:38.700]   And it's like, I've been using it for a long time.
[02:25:38.700 --> 02:25:43.700]   And it's like, I've been using it for a long time.
[02:25:43.700 --> 02:25:48.700]   And it's like, I've been using it for a long time.
[02:25:48.700 --> 02:25:53.700]   And it's like, I've been using it for a long time.
[02:25:53.700 --> 02:25:58.700]   And it's like, I've been using it for a long time.
[02:25:58.700 --> 02:26:03.700]   And it's like, I've been using it for a long time.
[02:26:03.700 --> 02:26:08.700]   And it's like, I've been using it for a long time.
[02:26:08.700 --> 02:26:13.700]   And it's like, I've been using it for a long time.
[02:26:13.700 --> 02:26:18.700]   And it's like, I've been using it for a long time.
[02:26:18.700 --> 02:26:23.700]   And it's like, I've been using it for a long time.
[02:26:23.700 --> 02:26:28.700]   And it's like, I've been using it for a long time.
[02:26:28.700 --> 02:26:33.700]   And it's like, I've been using it for a long time.
[02:26:33.700 --> 02:26:38.700]   And it's like, I've been using it for a long time.
[02:26:38.700 --> 02:26:43.700]   And it's like, I've been using it for a long time.
[02:26:43.700 --> 02:26:48.700]   And it's like, I've been using it for a long time.
[02:26:48.700 --> 02:26:53.700]   And it's like, I've been using it for a long time.
[02:26:53.700 --> 02:26:58.700]   And it's like, I've been using it for a long time.
[02:26:58.700 --> 02:27:03.700]   And it's like, I've been using it for a long time.
[02:27:03.700 --> 02:27:08.700]   And it's like, I've been using it for a long time.
[02:27:08.700 --> 02:27:13.700]   And it's like, I've been using it for a long time.
[02:27:13.700 --> 02:27:18.700]   And it's like, I've been using it for a long time.
[02:27:18.700 --> 02:27:23.700]   And it's like, I've been using it for a long time.
[02:27:23.700 --> 02:27:28.700]   And it's like, I've been using it for a long time.
[02:27:28.700 --> 02:27:33.700]   And it's like, I've been using it for a long time.
[02:27:33.700 --> 02:27:38.700]   And it's like, I've been using it for a long time.
[02:27:38.700 --> 02:27:43.700]   And it's like, I've been using it for a long time.
[02:27:43.700 --> 02:27:48.700]   And it's like, I've been using it for a long time.
[02:27:48.700 --> 02:27:53.700]   And it's like, I've been using it for a long time.
[02:27:53.700 --> 02:27:58.700]   And it's like, I've been using it for a long time.
[02:27:58.700 --> 02:28:03.700]   And it's like, I've been using it for a long time.
[02:28:03.700 --> 02:28:08.700]   And it's like, I've been using it for a long time.
[02:28:08.700 --> 02:28:13.700]   And it's like, I've been using it for a long time.
[02:28:13.700 --> 02:28:18.700]   And it's like, I've been using it for a long time.
[02:28:18.700 --> 02:28:23.700]   And it's like, I've been using it for a long time.
[02:28:23.700 --> 02:28:28.700]   And it's like, I've been using it for a long time.
[02:28:28.700 --> 02:28:33.700]   And it's like, I've been using it for a long time.
[02:28:33.700 --> 02:28:38.700]   And it's like, I've been using it for a long time.
[02:28:38.700 --> 02:28:43.700]   And it's like, I've been using it for a long time.
[02:28:43.700 --> 02:28:48.700]   And it's like, I've been using it for a long time.
[02:28:48.700 --> 02:28:53.700]   And it's like, I've been using it for a long time.
[02:28:53.700 --> 02:28:58.700]   And it's like, I've been using it for a long time.
[02:28:58.700 --> 02:29:03.700]   And it's like, I've been using it for a long time.
[02:29:03.700 --> 02:29:08.700]   And it's like, I've been using it for a long time.
[02:29:08.700 --> 02:29:13.700]   And it's like, I've been using it for a long time.
[02:29:13.700 --> 02:29:18.700]   And it's like, I've been using it for a long time.
[02:29:18.700 --> 02:29:23.700]   And it's like, I've been using it for a long time.
[02:29:23.700 --> 02:29:28.700]   And it's like, I've been using it for a long time.
[02:29:28.700 --> 02:29:33.700]   And it's like, I've been using it for a long time.
[02:29:33.700 --> 02:29:38.700]   And it's like, I've been using it for a long time.
[02:29:38.700 --> 02:29:43.700]   And it's like, I've been using it for a long time.
[02:29:43.700 --> 02:29:48.700]   And it's like, I've been using it for a long time.
[02:29:48.700 --> 02:29:53.700]   And it's like, I've been using it for a long time.
[02:29:53.700 --> 02:29:58.700]   And it's like, I've been using it for a long time.
[02:29:58.700 --> 02:30:03.700]   And it's like, I've been using it for a long time.
[02:30:03.700 --> 02:30:08.700]   And it's like, I've been using it for a long time.
[02:30:08.700 --> 02:30:13.700]   And it's like, I've been using it for a long time.
[02:30:13.700 --> 02:30:18.700]   And it's like, I've been using it for a long time.
[02:30:18.700 --> 02:30:23.700]   And it's like, I've been using it for a long time.
[02:30:23.700 --> 02:30:28.700]   And it's like, I've been using it for a long time.
[02:30:28.700 --> 02:30:33.700]   And it's like, I've been using it for a long time.
[02:30:33.700 --> 02:30:38.700]   And it's like, I've been using it for a long time.
[02:30:38.700 --> 02:30:43.700]   And it's like, I've been using it for a long time.
[02:30:43.700 --> 02:30:48.700]   And it's like, I've been using it for a long time.
[02:30:48.700 --> 02:30:53.700]   And it's like, I've been using it for a long time.
[02:30:53.700 --> 02:30:58.700]   And it's like, I've been using it for a long time.
[02:30:58.700 --> 02:31:03.700]   And it's like, I've been using it for a long time.
[02:31:03.700 --> 02:31:08.700]   And it's like, I've been using it for a long time.
[02:31:08.700 --> 02:31:13.700]   And it's like, I've been using it for a long time.
[02:31:13.700 --> 02:31:18.700]   And it's like, I've been using it for a long time.
[02:31:18.700 --> 02:31:23.700]   And it's like, I've been using it for a long time.
[02:31:23.700 --> 02:31:28.700]   And it's like, I've been using it for a long time.
[02:31:28.700 --> 02:31:33.700]   And it's like, I've been using it for a long time.
[02:31:33.700 --> 02:31:38.700]   And it's like, I've been using it for a long time.
[02:31:38.700 --> 02:31:43.700]   And it's like, I've been using it for a long time.
[02:31:43.700 --> 02:31:48.700]   And it's like, I've been using it for a long time.
[02:31:48.700 --> 02:31:53.700]   And it's like, I've been using it for a long time.
[02:31:53.700 --> 02:31:58.700]   And it's like, I've been using it for a long time.
[02:31:58.700 --> 02:32:03.700]   And it's like, I've been using it for a long time.
[02:32:03.700 --> 02:32:08.700]   And it's like, I've been using it for a long time.
[02:32:08.700 --> 02:32:13.700]   And it's like, I've been using it for a long time.
[02:32:13.700 --> 02:32:18.700]   And it's like, I've been using it for a long time.
[02:32:18.700 --> 02:32:23.700]   And it's like, I've been using it for a long time.
[02:32:23.700 --> 02:32:28.700]   And it's like, I've been using it for a long time.
[02:32:28.700 --> 02:32:33.700]   And it's like, I've been using it for a long time.
[02:32:33.700 --> 02:32:38.700]   And it's like, I've been using it for a long time.
[02:32:38.700 --> 02:32:43.700]   And it's like, I've been using it for a long time.
[02:32:43.700 --> 02:32:48.700]   And it's like, I've been using it for a long time.
[02:32:48.700 --> 02:32:53.700]   And it's like, I've been using it for a long time.
[02:32:53.700 --> 02:32:58.700]   And it's like, I've been using it for a long time.
[02:32:58.700 --> 02:33:03.700]   And it's like, I've been using it for a long time.
[02:33:03.700 --> 02:33:08.700]   And it's like, I've been using it for a long time.
[02:33:08.700 --> 02:33:13.700]   And it's like, I've been using it for a long time.
[02:33:13.700 --> 02:33:18.700]   And it's like, I've been using it for a long time.
[02:33:18.700 --> 02:33:23.700]   And it's like, I've been using it for a long time.
[02:33:23.700 --> 02:33:28.700]   And it's like, I've been using it for a long time.
[02:33:28.700 --> 02:33:33.700]   And it's like, I've been using it for a long time.
[02:33:33.700 --> 02:33:38.700]   And it's like, I've been using it for a long time.
[02:33:38.700 --> 02:33:43.700]   And it's like, I've been using it for a long time.
[02:33:43.700 --> 02:33:48.700]   And it's like, I've been using it for a long time.
[02:33:48.700 --> 02:33:53.700]   And it's like, I've been using it for a long time.
[02:33:53.700 --> 02:33:58.700]   And it's like, I've been using it for a long time.
[02:33:58.700 --> 02:34:03.700]   And it's like, I've been using it for a long time.
[02:34:03.700 --> 02:34:08.700]   And it's like, I've been using it for a long time.
[02:34:08.700 --> 02:34:13.700]   And it's like, I've been using it for a long time.
[02:34:13.700 --> 02:34:18.700]   And it's like, I've been using it for a long time.
[02:34:18.700 --> 02:34:23.700]   And it's like, I've been using it for a long time.
[02:34:23.700 --> 02:34:28.700]   And it's like, I've been using it for a long time.
[02:34:28.700 --> 02:34:33.700]   And it's like, I've been using it for a long time.
[02:34:33.700 --> 02:34:38.700]   And it's like, I've been using it for a long time.
[02:34:38.700 --> 02:34:43.700]   And it's like, I've been using it for a long time.

