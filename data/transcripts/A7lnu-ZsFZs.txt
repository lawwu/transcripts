
[00:00:00.000 --> 00:00:03.440]   [MUSIC PLAYING]
[00:00:03.440 --> 00:00:09.360]   First of all, welcome everybody to another session
[00:00:09.360 --> 00:00:11.240]   at Widths and Biases on a different day
[00:00:11.240 --> 00:00:12.560]   at a different hour.
[00:00:12.560 --> 00:00:16.000]   But this time, it's a different type of an event.
[00:00:16.000 --> 00:00:17.400]   It's a code first event.
[00:00:17.400 --> 00:00:20.960]   It's the first time we're starting
[00:00:20.960 --> 00:00:22.280]   to look into ML frameworks.
[00:00:22.280 --> 00:00:24.520]   And we have with us Sylvain.
[00:00:24.520 --> 00:00:27.360]   So Sylvain, thank you very much for joining us.
[00:00:27.360 --> 00:00:30.960]   I know it's like 7 AM and really early your time.
[00:00:30.960 --> 00:00:32.400]   And you had a flight yesterday.
[00:00:32.400 --> 00:00:34.240]   But thanks very much for making the effort
[00:00:34.240 --> 00:00:36.960]   and joining us today at Widths and Biases.
[00:00:36.960 --> 00:00:41.240]   We really, really appreciate you taking the time.
[00:00:41.240 --> 00:00:42.120]   Thanks for having me.
[00:00:42.120 --> 00:00:44.280]   I'm super glad to be here.
[00:00:44.280 --> 00:00:45.800]   Really, I'm more excited.
[00:00:45.800 --> 00:00:48.000]   I think I'm going to be the one person who's
[00:00:48.000 --> 00:00:49.440]   going to be the most excited today.
[00:00:49.440 --> 00:00:51.720]   But we'll talk about that very soon.
[00:00:51.720 --> 00:00:53.200]   So thanks, everybody.
[00:00:53.200 --> 00:00:56.920]   Just in terms of what ML frameworks is,
[00:00:56.920 --> 00:00:58.840]   so as deep learning engineers, I feel
[00:00:58.840 --> 00:01:02.440]   we are surrounded by these great, great ML frameworks,
[00:01:02.440 --> 00:01:05.080]   like Hugging Face, Accelerate being one of them,
[00:01:05.080 --> 00:01:07.720]   that makes our lives really, really easy.
[00:01:07.720 --> 00:01:10.920]   And we don't realize the amount of effort,
[00:01:10.920 --> 00:01:13.240]   the amount of work that goes into building
[00:01:13.240 --> 00:01:14.280]   these kinds of frameworks.
[00:01:14.280 --> 00:01:17.280]   And I thought, and we at Widths and Biases
[00:01:17.280 --> 00:01:20.480]   thought that it would be a lovely idea to maybe start
[00:01:20.480 --> 00:01:22.560]   a series where we could start looking
[00:01:22.560 --> 00:01:24.000]   into these kinds of frameworks.
[00:01:24.000 --> 00:01:26.040]   And the one that we're going to be looking today
[00:01:26.040 --> 00:01:28.000]   is Hugging Face, Accelerate.
[00:01:28.000 --> 00:01:30.480]   I won't go into much detail about what
[00:01:30.480 --> 00:01:32.000]   Hugging Face, Accelerate is.
[00:01:32.000 --> 00:01:34.840]   I think we'll leave that to Sylvain.
[00:01:34.840 --> 00:01:36.600]   But just in a few words, it really
[00:01:36.600 --> 00:01:38.040]   helps with distributed learning.
[00:01:38.040 --> 00:01:42.440]   And you can train models on various different hardwares
[00:01:42.440 --> 00:01:44.920]   with minimal lines of code.
[00:01:44.920 --> 00:01:48.160]   So the agenda for today, then, is going to look like this.
[00:01:48.160 --> 00:01:50.800]   We're going to have a welcome and introduction.
[00:01:50.800 --> 00:01:52.920]   We're just going to spend a few minutes on that.
[00:01:52.920 --> 00:01:55.200]   We're going to move to introduction to Hugging Face,
[00:01:55.200 --> 00:01:56.040]   Accelerate.
[00:01:56.040 --> 00:01:58.000]   That's going to be by Sylvain.
[00:01:58.000 --> 00:02:00.560]   Then we're going to look at some live coding examples.
[00:02:00.560 --> 00:02:03.240]   This could be looking into the source code
[00:02:03.240 --> 00:02:06.480]   or just looking at some design ideas.
[00:02:06.480 --> 00:02:08.920]   And then towards the end, we're going to have an
[00:02:08.920 --> 00:02:12.040]   Ask Me Anything with Sylvain.
[00:02:12.040 --> 00:02:13.440]   So Sylvain joins us today.
[00:02:13.440 --> 00:02:14.480]   And I'm so excited.
[00:02:14.480 --> 00:02:17.400]   Sylvain is a research engineer at Hugging Face.
[00:02:17.400 --> 00:02:19.280]   Sylvain has been one of my heroes.
[00:02:19.280 --> 00:02:21.920]   I still want to call you a fast AI hero,
[00:02:21.920 --> 00:02:23.480]   but now you're my Hugging Face hero.
[00:02:23.480 --> 00:02:26.680]   So thanks for all the years of hard work
[00:02:26.680 --> 00:02:28.920]   that you did at Fast AI.
[00:02:28.920 --> 00:02:31.440]   And now you're at Hugging Face and doing
[00:02:31.440 --> 00:02:32.920]   all this excellent work.
[00:02:32.920 --> 00:02:36.400]   I think when you joined, the first tweet I saw
[00:02:36.400 --> 00:02:38.520]   is like, Sylvain's adding some hugs today.
[00:02:38.520 --> 00:02:40.160]   So I think you replaced all the images.
[00:02:40.160 --> 00:02:42.040]   That was the very, very first day,
[00:02:42.040 --> 00:02:45.520]   I believe, that the tweet went out from Hugging Face.
[00:02:45.520 --> 00:02:47.800]   And now you've got Hugging Face Accelerate
[00:02:47.800 --> 00:02:50.920]   and you've got Transformers and all the excellent work
[00:02:50.920 --> 00:02:51.720]   that you're doing.
[00:02:51.720 --> 00:02:54.280]   So really, really appreciate all the hard work
[00:02:54.280 --> 00:02:55.840]   that you do for the community, Sylvain.
[00:02:55.840 --> 00:02:59.360]   And thanks for joining us.
[00:02:59.360 --> 00:03:00.840]   Thanks a lot.
[00:03:00.840 --> 00:03:05.080]   And so I wanted to mention the Hugging Face course, which
[00:03:05.080 --> 00:03:08.400]   I took a deep part in preparing and organizing.
[00:03:08.400 --> 00:03:09.320]   Oh, you had a slide.
[00:03:09.320 --> 00:03:10.480]   So perfect.
[00:03:10.480 --> 00:03:11.400]   No, please go for it.
[00:03:11.400 --> 00:03:12.240]   Please, please.
[00:03:12.240 --> 00:03:16.240]   So yeah, this is very exciting because we launched that
[00:03:16.240 --> 00:03:18.240]   three, four weeks ago.
[00:03:18.240 --> 00:03:21.520]   So this is a course aimed at not exactly beginners.
[00:03:21.520 --> 00:03:23.840]   If you're a beginner, I would deeply
[00:03:23.840 --> 00:03:27.200]   recommend Fast.AI MOOC, not only because I worked there,
[00:03:27.200 --> 00:03:30.560]   but because it's really, really good to learn from scratch
[00:03:30.560 --> 00:03:32.800]   what deep learning is with a good first approach.
[00:03:32.800 --> 00:03:34.880]   And then once you've done that MOOC,
[00:03:34.880 --> 00:03:36.800]   this is what would go next if you
[00:03:36.800 --> 00:03:39.040]   want to learn more about what Transformers model are,
[00:03:39.040 --> 00:03:41.680]   what the Transformers library is, how to use it.
[00:03:41.680 --> 00:03:45.160]   And so for now, the first section of the course is out.
[00:03:45.160 --> 00:03:48.360]   And we are aiming to release the second section in the fall.
[00:03:48.360 --> 00:03:50.120]   So the first section is going to teach you
[00:03:50.120 --> 00:03:53.880]   how to download a pre-trained model like BERT or GPT-2
[00:03:53.880 --> 00:03:56.800]   and how to fine tune it on a text classification task
[00:03:56.800 --> 00:03:59.280]   or how to use it for inference.
[00:03:59.280 --> 00:04:02.480]   And then share the results of your fine-tuned model
[00:04:02.480 --> 00:04:04.400]   back with the community so that anyone
[00:04:04.400 --> 00:04:05.840]   can use it around the world.
[00:04:05.840 --> 00:04:07.640]   And then in the section two of the course,
[00:04:07.640 --> 00:04:11.480]   we'll dive into all the classic NLP tasks for token
[00:04:11.480 --> 00:04:13.520]   classification, question answering,
[00:04:13.520 --> 00:04:16.840]   all the sequence-to-sequence problems.
[00:04:16.840 --> 00:04:20.000]   And yeah, that's pretty much it for the course.
[00:04:20.000 --> 00:04:20.520]   Excellent.
[00:04:20.520 --> 00:04:23.720]   I've been doing the course myself
[00:04:23.720 --> 00:04:25.560]   on and off for the past couple of weeks.
[00:04:25.560 --> 00:04:26.360]   And I really love it.
[00:04:26.360 --> 00:04:29.080]   I'm through to this chapter three.
[00:04:29.080 --> 00:04:32.000]   And I think the course is really presented in a nice way.
[00:04:32.000 --> 00:04:34.360]   And it's really easy to understand.
[00:04:34.360 --> 00:04:36.880]   And you see Sylvain there helping everybody
[00:04:36.880 --> 00:04:37.840]   with all the questions.
[00:04:37.840 --> 00:04:43.000]   And I believe there were live sessions as well in the past.
[00:04:43.000 --> 00:04:44.880]   Sylvain, are those still recorded?
[00:04:44.880 --> 00:04:46.560]   Are those still the answers?
[00:04:46.560 --> 00:04:49.000]   Yeah, you can find the recordings of the live sessions
[00:04:49.000 --> 00:04:51.280]   on our YouTube channel.
[00:04:51.280 --> 00:04:54.560]   And we'll do probably a few more when we release the section
[00:04:54.560 --> 00:04:54.840]   two.
[00:04:54.840 --> 00:04:57.840]   We'll do a few live sessions to go over the section one again
[00:04:57.840 --> 00:04:59.880]   before we release section two.
[00:04:59.880 --> 00:05:00.400]   Excellent.
[00:05:00.400 --> 00:05:02.040]   I'm really, really excited.
[00:05:02.040 --> 00:05:04.760]   And those of us joining us from Fastbook,
[00:05:04.760 --> 00:05:07.560]   Sylvain is the co-author of Fastbook as well.
[00:05:07.560 --> 00:05:11.240]   So huge thanks to Sylvain for writing with Jeremy
[00:05:11.240 --> 00:05:13.600]   and co-authoring Fastbook that we've
[00:05:13.600 --> 00:05:15.840]   been looking into for the past seven weeks at Weights
[00:05:15.840 --> 00:05:17.000]   and Biases.
[00:05:17.000 --> 00:05:20.920]   So Sylvain is a guy with many hats, as we now know.
[00:05:20.920 --> 00:05:22.760]   As is common for most of our events,
[00:05:22.760 --> 00:05:27.680]   we'll use this link, 1db.me/accelerate.
[00:05:27.680 --> 00:05:30.040]   The reason for that is we're also live on YouTube.
[00:05:30.040 --> 00:05:32.760]   So it's really hard to follow questions.
[00:05:32.760 --> 00:05:34.400]   But if you go to that link--
[00:05:34.400 --> 00:05:37.640]   can you guys see my browser?
[00:05:37.640 --> 00:05:38.600]   OK, perfect.
[00:05:38.600 --> 00:05:42.600]   So I'll just paste that link in the chat as well.
[00:05:42.600 --> 00:05:49.000]   So 1db.me/accelerate.
[00:05:49.000 --> 00:05:50.280]   Thanks, Andrea.
[00:05:50.280 --> 00:05:52.040]   That should take us all to this link.
[00:05:52.040 --> 00:05:54.360]   And if you have a question, just write a comment here.
[00:05:54.360 --> 00:05:57.200]   And you can pretty much post a comment.
[00:05:57.200 --> 00:06:00.440]   So this is what we use for live questions and discussions.
[00:06:00.440 --> 00:06:02.400]   And I'll be monitoring that.
[00:06:02.400 --> 00:06:05.560]   So that being said, over to you, Sylvain.
[00:06:05.560 --> 00:06:08.000]   Tell us what Hugging Face Accelerate is.
[00:06:08.000 --> 00:06:08.800]   Sure.
[00:06:08.800 --> 00:06:09.880]   Let me share my screen.
[00:06:09.880 --> 00:06:10.880]   And launch.
[00:06:10.880 --> 00:06:27.360]   If it does want to launch.
[00:06:27.360 --> 00:06:32.560]   OK, can you see my screen all right?
[00:06:32.560 --> 00:06:34.200]   Yes, we can see your screen.
[00:06:34.200 --> 00:06:38.040]   So Hugging Face Accelerate, in a nutshell, as you said,
[00:06:38.040 --> 00:06:41.320]   it's a library that's designed to help
[00:06:41.320 --> 00:06:44.920]   with distributed training and distributed evaluation.
[00:06:44.920 --> 00:06:50.480]   And so to show you what problem it solves and how it solves it,
[00:06:50.480 --> 00:06:53.080]   I'm going to start from that script, which
[00:06:53.080 --> 00:06:56.880]   is a very simple training loop in PyTorch.
[00:06:56.880 --> 00:07:01.880]   So you see we have defined a device.
[00:07:01.880 --> 00:07:05.080]   And we put our model onto that device.
[00:07:05.080 --> 00:07:06.840]   We define a training data loader.
[00:07:06.840 --> 00:07:09.640]   We have, let's say, three epochs and a certain number
[00:07:09.640 --> 00:07:10.680]   of training steps.
[00:07:10.680 --> 00:07:12.440]   And we define the learning rate scheduler.
[00:07:12.440 --> 00:07:15.360]   For instance, we can emulate the learning rate from a value
[00:07:15.360 --> 00:07:16.520]   to 0.
[00:07:16.520 --> 00:07:19.000]   And the progress bar to show that everything is going right.
[00:07:19.000 --> 00:07:21.960]   And then we put the model in training mode.
[00:07:21.960 --> 00:07:23.720]   We loop through the epochs.
[00:07:23.720 --> 00:07:26.000]   We loop through the data loader.
[00:07:26.000 --> 00:07:28.600]   We place our batch on the device, which
[00:07:28.600 --> 00:07:32.000]   is a GPU if you have a GPU available or the CPU otherwise.
[00:07:32.000 --> 00:07:33.800]   And then so this is an example that's
[00:07:33.800 --> 00:07:39.520]   from transformers to get the transformers model always
[00:07:39.520 --> 00:07:43.840]   computes the loss when you have your labels inside your batch.
[00:07:43.840 --> 00:07:47.120]   And you can access it by the dot loss attribute
[00:07:47.120 --> 00:07:48.120]   once you have your loss.
[00:07:48.120 --> 00:07:50.000]   But if you have any other kind of model here,
[00:07:50.000 --> 00:07:52.200]   the code would change slightly to compute the loss
[00:07:52.200 --> 00:07:53.440]   as you prefer.
[00:07:53.440 --> 00:07:55.560]   Once we have the loss, we do a loss.backward,
[00:07:55.560 --> 00:07:58.360]   an optimizer.step, learning rate scheduler.step.
[00:07:58.360 --> 00:08:00.640]   We don't forget to zero the grad for the next step.
[00:08:00.640 --> 00:08:02.720]   And we update the progress bar so
[00:08:02.720 --> 00:08:04.520]   that it shows the progress.
[00:08:04.520 --> 00:08:07.120]   And so what I'm going to show is what
[00:08:07.120 --> 00:08:09.240]   you need to change in this training loop
[00:08:09.240 --> 00:08:12.880]   if you want it to run on several GPUs but not one,
[00:08:12.880 --> 00:08:15.720]   if you want it to run on TPUs, or if you
[00:08:15.720 --> 00:08:18.000]   want to run it with mixed precision training.
[00:08:18.000 --> 00:08:21.200]   And I'll accelerate twice to solve all of those problems
[00:08:21.200 --> 00:08:24.800]   at once with very few lines of code to add.
[00:08:24.800 --> 00:08:28.000]   So let's see for distributed training.
[00:08:28.000 --> 00:08:30.480]   So as you can see, there is a lot of code that changes here.
[00:08:30.480 --> 00:08:33.320]   And I almost-- as a man who has run in B just
[00:08:33.320 --> 00:08:36.280]   before we started, I also forgot a few of our lines of code.
[00:08:36.280 --> 00:08:38.680]   So it adds quite a bit.
[00:08:38.680 --> 00:08:41.000]   So you have got some imports.
[00:08:41.000 --> 00:08:44.240]   You have to initialize something, generally
[00:08:44.240 --> 00:08:45.520]   with the backend and CLL.
[00:08:45.520 --> 00:08:51.440]   So this is to make sure that all of your processes
[00:08:51.440 --> 00:08:54.520]   can speak to each other and are ready for training.
[00:08:54.520 --> 00:08:57.200]   And then instead of using the default device
[00:08:57.200 --> 00:09:01.480]   that we have before, you have to have arguments, for instance,
[00:09:01.480 --> 00:09:03.560]   with the arc path library that are
[00:09:03.560 --> 00:09:07.480]   going to tell you which rank is your GPU.
[00:09:07.480 --> 00:09:09.040]   Is it GPU number 0?
[00:09:09.040 --> 00:09:11.360]   Is it GPU number 1?
[00:09:11.360 --> 00:09:15.400]   And then you set the device with that.
[00:09:15.400 --> 00:09:19.120]   Once this is done, instead of putting your model
[00:09:19.120 --> 00:09:20.840]   to the device directly, you have to wrap it
[00:09:20.840 --> 00:09:23.440]   inside a distributed data parallel object.
[00:09:23.440 --> 00:09:25.040]   And for the training data loader,
[00:09:25.040 --> 00:09:27.440]   this is where things get really ugly.
[00:09:27.440 --> 00:09:29.720]   If you want your training data loader
[00:09:29.720 --> 00:09:32.680]   to be distributed but shuffled, this is--
[00:09:32.680 --> 00:09:34.960]   and we'll look at the design behind it later.
[00:09:34.960 --> 00:09:36.880]   But this is the hardest problem when
[00:09:36.880 --> 00:09:38.400]   you're doing distributed training,
[00:09:38.400 --> 00:09:42.240]   is to make sure that your randomness is
[00:09:42.240 --> 00:09:44.680]   the same for all the processes.
[00:09:44.680 --> 00:09:48.240]   Because when you launch a distributed training,
[00:09:48.240 --> 00:09:52.040]   you launch your same script on GPU 0 and GPU 1, et cetera.
[00:09:52.040 --> 00:09:55.160]   And they have to have the same random numbers for some things,
[00:09:55.160 --> 00:09:56.960]   like shuffling the data.
[00:09:56.960 --> 00:09:59.000]   Otherwise, they are going to be not
[00:09:59.000 --> 00:10:01.000]   training on the same samples.
[00:10:01.000 --> 00:10:04.720]   But for some other operations, like data augmentation,
[00:10:04.720 --> 00:10:07.560]   you want them to have different random numbers.
[00:10:07.560 --> 00:10:09.680]   Otherwise, you don't have-- you will
[00:10:09.680 --> 00:10:11.840]   have the same data augmentation on all the processes,
[00:10:11.840 --> 00:10:15.480]   so it won't be as useful as real data augmentation.
[00:10:15.480 --> 00:10:17.320]   So to do that with distributed training,
[00:10:17.320 --> 00:10:19.240]   you have to define a distributed sampler.
[00:10:19.240 --> 00:10:21.200]   You have to pass it to your data loader.
[00:10:21.200 --> 00:10:23.320]   And the line of code I forgot here
[00:10:23.320 --> 00:10:25.280]   is that at the beginning of your epoch,
[00:10:25.280 --> 00:10:28.840]   you have to set to that sampler that's the epoch, setEpoch,
[00:10:28.840 --> 00:10:30.200]   with the setEpoch method.
[00:10:30.200 --> 00:10:33.680]   You have to tell it we are at epoch 0, 1, 2, et cetera,
[00:10:33.680 --> 00:10:36.480]   which is where the random number generators are actually
[00:10:36.480 --> 00:10:39.160]   synchronized.
[00:10:39.160 --> 00:10:41.200]   So this is it for distributed training.
[00:10:41.200 --> 00:10:45.560]   If you look at TPUs, it becomes even more complicated.
[00:10:45.560 --> 00:10:47.200]   With a few more imports, you still
[00:10:47.200 --> 00:10:49.920]   have the unit process group.
[00:10:49.920 --> 00:10:54.440]   And the device changes slightly.
[00:10:54.440 --> 00:10:58.280]   You have to use an XLA device from the Torch XLA library.
[00:10:58.280 --> 00:11:00.320]   So basically, for distributed training
[00:11:00.320 --> 00:11:01.840]   with distributed data parallel, you
[00:11:01.840 --> 00:11:04.360]   have to run a new API, which is distributed data parallel.
[00:11:04.360 --> 00:11:05.840]   If you want to use TPU training, you
[00:11:05.840 --> 00:11:09.920]   have to run a second API, which is called Torch XLA.
[00:11:09.920 --> 00:11:14.240]   So this is that new library that gives you the device.
[00:11:14.240 --> 00:11:18.000]   The data loader you have to do is a little bit more complicated
[00:11:18.000 --> 00:11:20.920]   because after defining it with the sampler,
[00:11:20.920 --> 00:11:23.120]   you have to wrap it inside a parallel data loader,
[00:11:23.120 --> 00:11:27.560]   parallel loader, and use the per-device loader method here.
[00:11:27.560 --> 00:11:30.320]   I also forget the synchronization point here.
[00:11:30.320 --> 00:11:32.880]   And then, instead of doing optimizer.step,
[00:11:32.880 --> 00:11:36.720]   you have to do something like XM.optimizer.step optimizer,
[00:11:36.720 --> 00:11:42.080]   where XM is the shortcut for the Torch XLA library.
[00:11:42.080 --> 00:11:45.240]   And I'm just showing the very basic training loop.
[00:11:45.240 --> 00:11:47.800]   If you want to do more things, like saving your model
[00:11:47.800 --> 00:11:50.920]   or evaluating with distributed evaluations,
[00:11:50.920 --> 00:11:53.360]   you are going to have much more lines of codes, which
[00:11:53.360 --> 00:11:55.640]   are going to be different for distributed data parallel,
[00:11:55.640 --> 00:11:59.360]   different for TPUs, because those are two different APIs.
[00:11:59.360 --> 00:12:02.080]   And each time you do the modifications,
[00:12:02.080 --> 00:12:05.200]   your scripts cannot run on CPU.
[00:12:05.200 --> 00:12:06.920]   So if you want to debug your scripts,
[00:12:06.920 --> 00:12:11.080]   you have to somehow make some if and else statements everywhere
[00:12:11.080 --> 00:12:14.920]   to have the code available for just one process on CPU
[00:12:14.920 --> 00:12:20.240]   and have the code available for multiple GPUs or TPU.
[00:12:20.240 --> 00:12:22.200]   Mixed precision training is, thankfully,
[00:12:22.200 --> 00:12:25.800]   a little bit more simple to use without any verbal.
[00:12:25.800 --> 00:12:28.560]   You just have to import this AutoCAD thing
[00:12:28.560 --> 00:12:32.000]   and create a gradient scaler, because when
[00:12:32.000 --> 00:12:33.760]   you do mixed precision training, there
[00:12:33.760 --> 00:12:37.520]   is this operation called loss scaling to avoid
[00:12:37.520 --> 00:12:39.000]   your gradient from vanishing.
[00:12:39.000 --> 00:12:40.800]   So if you don't know anything about that,
[00:12:40.800 --> 00:12:43.720]   I definitely recommend reading the corresponding chapter
[00:12:43.720 --> 00:12:48.040]   in the fast book that Aman mentioned.
[00:12:48.040 --> 00:12:51.280]   So once you have that, you have to compute your loss
[00:12:51.280 --> 00:12:54.160]   inside the Autocad decorator, like here.
[00:12:54.160 --> 00:12:56.840]   And then instead of doing loss.backward,
[00:12:56.840 --> 00:13:00.200]   you have to do scalar.scale loss.backward.
[00:13:00.200 --> 00:13:03.360]   So this is easier, but it's still a new API to learn.
[00:13:03.360 --> 00:13:05.200]   And when you do that, your scripts
[00:13:05.200 --> 00:13:07.320]   won't be able to run on CPU anymore,
[00:13:07.320 --> 00:13:09.920]   because this decorator here is going
[00:13:09.920 --> 00:13:13.920]   to execute the forward pass of the model in arch precision
[00:13:13.920 --> 00:13:15.040]   in FP16.
[00:13:15.040 --> 00:13:16.960]   And most of the operations are not
[00:13:16.960 --> 00:13:19.240]   implemented on CPU in FP16.
[00:13:19.240 --> 00:13:22.000]   So again, even if the API is a little bit simpler,
[00:13:22.000 --> 00:13:24.840]   you will have to have if statements everywhere,
[00:13:24.840 --> 00:13:28.320]   because this code won't be able to run on CPU
[00:13:28.320 --> 00:13:32.360]   if you want to debug it locally, for instance.
[00:13:32.360 --> 00:13:34.880]   So Accelerate tries to solve this problem,
[00:13:34.880 --> 00:13:36.440]   and especially the fact that you have
[00:13:36.440 --> 00:13:40.320]   to put if and two problems, to be more exact.
[00:13:40.320 --> 00:13:42.040]   So one of them is the fact that you
[00:13:42.040 --> 00:13:45.600]   have to put if then else statement everywhere
[00:13:45.600 --> 00:13:48.480]   for your script to still be able to run on CPU, GPU,
[00:13:48.480 --> 00:13:50.160]   multiple GPUs, or TPU.
[00:13:50.160 --> 00:13:52.600]   And if you want to use platforms like mixed precision
[00:13:52.600 --> 00:13:54.400]   or deep speed.
[00:13:54.400 --> 00:13:56.000]   And the other problem it tries to solve
[00:13:56.000 --> 00:13:58.600]   is the fact that you have to learn a new API each time
[00:13:58.600 --> 00:14:01.280]   you want to do a new kind of distributed training.
[00:14:01.280 --> 00:14:04.360]   So with Accelerate, you have to learn one API,
[00:14:04.360 --> 00:14:07.360]   and it's going to handle all of those for you.
[00:14:07.360 --> 00:14:09.840]   So it's a different kind of distributed setup.
[00:14:09.840 --> 00:14:13.320]   CPU, GPU, multiple GPUs on one or several machines,
[00:14:13.320 --> 00:14:15.600]   a TPU or a TPU pad.
[00:14:15.600 --> 00:14:19.960]   And it also handles the training techniques
[00:14:19.960 --> 00:14:23.640]   to accelerate or reduce the memory of your trainings,
[00:14:23.640 --> 00:14:25.760]   like mixed precision or deep speed, which
[00:14:25.760 --> 00:14:30.080]   is a bit like mixed precision, but with--
[00:14:30.080 --> 00:14:31.720]   I mean, which is using mixed precision,
[00:14:31.720 --> 00:14:34.240]   but it takes it to the next level.
[00:14:34.240 --> 00:14:36.280]   When you do mixed precision, for instance,
[00:14:36.280 --> 00:14:37.400]   if you know a little bit about it,
[00:14:37.400 --> 00:14:40.680]   you have two copies of your model, one in FP16,
[00:14:40.680 --> 00:14:43.040]   half precision, and one in full precision.
[00:14:43.040 --> 00:14:45.040]   And deep speed, for instance, make sure
[00:14:45.040 --> 00:14:48.040]   that's a copy in full precision is on your CPU
[00:14:48.040 --> 00:14:50.280]   if you use a technique called CPU offload,
[00:14:50.280 --> 00:14:54.800]   or even on an NVMe if you use a technique called zero
[00:14:54.800 --> 00:14:58.760]   infinity, which reduces the memory you're
[00:14:58.760 --> 00:15:00.200]   using on your GPU by a lot.
[00:15:00.200 --> 00:15:01.840]   So it's kind of--
[00:15:01.840 --> 00:15:03.440]   it goes as fast as mixed precision,
[00:15:03.440 --> 00:15:05.200]   and it tries to take it to the next level
[00:15:05.200 --> 00:15:08.600]   by also reducing the memory footprint of your mixed precision
[00:15:08.600 --> 00:15:10.680]   training.
[00:15:10.680 --> 00:15:13.200]   And so how does it do all of that?
[00:15:13.200 --> 00:15:18.480]   So like I said, we try to make it
[00:15:18.480 --> 00:15:20.640]   as simple as possible in terms of lines of code,
[00:15:20.640 --> 00:15:22.760]   but it's still going to be a new API to learn.
[00:15:22.760 --> 00:15:24.040]   There is no way around that.
[00:15:24.040 --> 00:15:26.440]   But the promise is that it's the only API
[00:15:26.440 --> 00:15:28.440]   you will have to learn for distributed training,
[00:15:28.440 --> 00:15:31.080]   because you won't have to remember anything
[00:15:31.080 --> 00:15:32.880]   about distributed data parallel.
[00:15:32.880 --> 00:15:35.040]   You won't have to learn about Torch XLA.
[00:15:35.040 --> 00:15:36.520]   You won't have to learn about what's
[00:15:36.520 --> 00:15:38.280]   in the Torch.cuda.conf model.
[00:15:38.280 --> 00:15:41.040]   Accelerate is going to handle all of that for you.
[00:15:41.040 --> 00:15:42.040]   So there is a new import.
[00:15:42.040 --> 00:15:45.840]   And all the initialization stuff is done by creating
[00:15:45.840 --> 00:15:48.640]   an accelerator object.
[00:15:48.640 --> 00:15:51.920]   If you want to be as close as possible
[00:15:51.920 --> 00:15:54.920]   to your original training loop and still handle the device
[00:15:54.920 --> 00:15:57.480]   placement by yourself, you have to take the device
[00:15:57.480 --> 00:15:59.800]   from the accelerator object, which
[00:15:59.800 --> 00:16:03.720]   is going to know if it's a CPU, if it's a GPU,
[00:16:03.720 --> 00:16:06.760]   if you are running on multiple GPU, which GPU you're running
[00:16:06.760 --> 00:16:09.040]   on, if you're running on multiple TPUs, which GPU
[00:16:09.040 --> 00:16:10.200]   you're on.
[00:16:10.200 --> 00:16:12.760]   And then the main point of access is here.
[00:16:12.760 --> 00:16:15.840]   You just pass to the accelerator.prepare method
[00:16:15.840 --> 00:16:19.520]   everything, the data loader, the model, and the optimizer.
[00:16:19.520 --> 00:16:22.440]   And it's going to run your training for you
[00:16:22.440 --> 00:16:23.480]   on any kind of setup.
[00:16:23.480 --> 00:16:26.920]   The only final line of code you have to modify is this one.
[00:16:26.920 --> 00:16:28.720]   Or instead of doing last.backward,
[00:16:28.720 --> 00:16:31.400]   you have to do accelerator.backward_last,
[00:16:31.400 --> 00:16:33.960]   which is one we couldn't avoid for things
[00:16:33.960 --> 00:16:36.560]   like mixed position training.
[00:16:36.560 --> 00:16:37.960]   And that's pretty much it.
[00:16:37.960 --> 00:16:40.120]   Again, this is on the simplest training loop.
[00:16:40.120 --> 00:16:41.920]   We are not talking about saving the model
[00:16:41.920 --> 00:16:44.520]   or printing only on process 0 or doing
[00:16:44.520 --> 00:16:45.920]   distributing evaluation.
[00:16:45.920 --> 00:16:48.480]   For each of those, you will have to learn a few more instructions.
[00:16:48.480 --> 00:16:50.560]   But you also have to learn a few more instructions
[00:16:50.560 --> 00:16:54.200]   with distributed data parallel or byte or checks for TPU.
[00:16:54.200 --> 00:16:57.040]   And again, we try to make them as simple as possible
[00:16:57.040 --> 00:17:03.720]   and keep the change you have to do as minimal as possible.
[00:17:03.720 --> 00:17:05.960]   And yeah, at this stage, I also want
[00:17:05.960 --> 00:17:08.480]   to note that you have platforms.
[00:17:08.480 --> 00:17:10.240]   You have third-party libraries that
[00:17:10.240 --> 00:17:13.360]   sit on top of PyTorch that can handle all the distributed
[00:17:13.360 --> 00:17:14.360]   training for you.
[00:17:14.360 --> 00:17:18.240]   But they usually rely on something like a trainer class
[00:17:18.240 --> 00:17:20.200]   or maybe it's another name, but something that
[00:17:20.200 --> 00:17:22.040]   encapsulates the whole training.
[00:17:22.040 --> 00:17:24.480]   And Accelerate has been designed for people
[00:17:24.480 --> 00:17:27.480]   who don't want a class that encapsulates a training loop,
[00:17:27.480 --> 00:17:28.960]   but for people who would still want
[00:17:28.960 --> 00:17:32.880]   to write their own training loop and customize it as they want.
[00:17:32.880 --> 00:17:35.120]   Because yeah, those big classes--
[00:17:35.120 --> 00:17:36.960]   and I've read some of them, there
[00:17:36.960 --> 00:17:38.880]   is one I maintain in Transformers.
[00:17:38.880 --> 00:17:41.320]   They're great for beginners that want to easily fine-tune
[00:17:41.320 --> 00:17:44.120]   a model, but sometimes more advanced users
[00:17:44.120 --> 00:17:45.920]   really want to dive into the training loop
[00:17:45.920 --> 00:17:47.880]   to have the training do exactly what they want.
[00:17:47.880 --> 00:17:51.720]   And in those big classes, it's often a bit of a pain
[00:17:51.720 --> 00:17:54.160]   to learn which methods you have to override
[00:17:54.160 --> 00:17:57.440]   or where you have to send specific callbacks
[00:17:57.440 --> 00:18:00.880]   to be able to change something in the training loop.
[00:18:00.880 --> 00:18:04.080]   So Accelerate takes another direction
[00:18:04.080 --> 00:18:07.480]   by staying as close as possible to a PyTorch training loop
[00:18:07.480 --> 00:18:09.920]   with just a few instructions to add,
[00:18:09.920 --> 00:18:13.560]   so that your script runs on any kind of distributed training.
[00:18:13.560 --> 00:18:15.120]   And so this is the first diff.
[00:18:15.120 --> 00:18:17.320]   The second diff, which is more recommended,
[00:18:17.320 --> 00:18:20.440]   is to let Accelerate handle all device placement,
[00:18:20.440 --> 00:18:22.640]   so that you remove the model.toDevice
[00:18:22.640 --> 00:18:27.920]   and you remove the line that places the batch on the device.
[00:18:27.920 --> 00:18:30.800]   And you just let Accelerate handle all of that for you,
[00:18:30.800 --> 00:18:32.800]   which is safer, especially on TPU.
[00:18:32.800 --> 00:18:36.840]   Because on TPU, if you, for instance,
[00:18:36.840 --> 00:18:40.120]   create your optimizer, then place your model on the TPU,
[00:18:40.120 --> 00:18:41.720]   the model is going to be disconnected
[00:18:41.720 --> 00:18:43.360]   from your optimizer.
[00:18:43.360 --> 00:18:45.440]   And so it's not going to be trained at all, which
[00:18:45.440 --> 00:18:46.600]   is a bug--
[00:18:46.600 --> 00:18:48.600]   I mean, I don't know if it's a bug or a design,
[00:18:48.600 --> 00:18:51.720]   but which is a design that can confuse a lot of users.
[00:18:51.720 --> 00:18:53.960]   So Accelerate, even if you don't define them
[00:18:53.960 --> 00:18:56.240]   in the right order, it's going to reconnect your model
[00:18:56.240 --> 00:18:59.640]   with your optimizer when you let it completely
[00:18:59.640 --> 00:19:01.080]   handle the device placement.
[00:19:01.080 --> 00:19:03.640]   It's going to take a few issues like that,
[00:19:03.640 --> 00:19:06.120]   that are very common pitfalls for beginners.
[00:19:06.120 --> 00:19:08.520]   It tries to make them as smooth as possible
[00:19:08.520 --> 00:19:12.000]   and either solve them or raise an error message that's
[00:19:12.000 --> 00:19:15.000]   going to tell the user, oh, something went wrong.
[00:19:15.000 --> 00:19:15.680]   Be sure to do that.
[00:19:16.680 --> 00:19:23.200]   And the last thing Accelerate does
[00:19:23.200 --> 00:19:26.320]   is when you actually have to launch your script.
[00:19:26.320 --> 00:19:29.440]   So once you've created your script, for instance,
[00:19:29.440 --> 00:19:31.480]   like in the second slide, we've touched
[00:19:31.480 --> 00:19:32.960]   the distributed data parallel.
[00:19:32.960 --> 00:19:35.960]   If you want to launch it on several processes,
[00:19:35.960 --> 00:19:37.520]   it can be a bit hard as well.
[00:19:37.520 --> 00:19:41.320]   You have to remember, you have to use the torch.distributed.launch
[00:19:41.320 --> 00:19:41.880]   command.
[00:19:41.880 --> 00:19:44.040]   And then it has a whole series of arguments
[00:19:44.040 --> 00:19:45.800]   you have to remember.
[00:19:45.800 --> 00:19:48.240]   If you want to launch it on two machines,
[00:19:48.240 --> 00:19:53.080]   there are more arguments to tell that there are two machines.
[00:19:53.080 --> 00:19:56.080]   So you have to tell that machine zero, that it's machine zero,
[00:19:56.080 --> 00:19:57.760]   machine one, that it's machine one.
[00:19:57.760 --> 00:20:00.800]   The IP address, they communicate with each other.
[00:20:00.800 --> 00:20:02.920]   And then the training script and all the arguments
[00:20:02.920 --> 00:20:03.880]   to the training script.
[00:20:03.880 --> 00:20:07.560]   So another thing that Accelerate solves is adding--
[00:20:07.560 --> 00:20:09.120]   and of course, yeah, this is a command
[00:20:09.120 --> 00:20:10.400]   for distributed data parallel.
[00:20:10.400 --> 00:20:13.320]   But on TPUs, it's going to be something different.
[00:20:13.320 --> 00:20:15.200]   So Accelerate tries to solve all of that
[00:20:15.200 --> 00:20:18.360]   by just introducing one simple command--
[00:20:18.360 --> 00:20:21.760]   two, actually-- an Accelerate config command, which
[00:20:21.760 --> 00:20:24.600]   when you type it, the terminal is
[00:20:24.600 --> 00:20:27.160]   going to ask you a few questions about the setup you're
[00:20:27.160 --> 00:20:28.320]   running on.
[00:20:28.320 --> 00:20:31.160]   Because usually, if you are on a machine that has, let's say,
[00:20:31.160 --> 00:20:34.320]   four GPUs, you will want to run all your training on the four
[00:20:34.320 --> 00:20:34.920]   GPUs.
[00:20:34.920 --> 00:20:37.160]   I mean, sometimes you will want to do half and half.
[00:20:37.160 --> 00:20:40.240]   But it relies on configurations that you
[00:20:40.240 --> 00:20:42.040]   can create with this command.
[00:20:42.040 --> 00:20:43.840]   And once you have that configuration ready,
[00:20:43.840 --> 00:20:45.200]   you can just do Accelerate launch,
[00:20:45.200 --> 00:20:46.760]   and then your training script and the argument
[00:20:46.760 --> 00:20:47.360]   to your training script.
[00:20:47.360 --> 00:20:49.160]   You don't have to remember anything
[00:20:49.160 --> 00:20:52.080]   about the number of machines, which machine it is,
[00:20:52.080 --> 00:20:53.960]   the IP address, et cetera.
[00:20:53.960 --> 00:20:56.880]   And if you are on Colab or in a Jupyter Notebook,
[00:20:56.880 --> 00:20:59.600]   there is also a Notebook launchRaw function, which
[00:20:59.600 --> 00:21:01.520]   is going to-- if you write a training
[00:21:01.520 --> 00:21:04.040]   function in another cell, which contains all your training
[00:21:04.040 --> 00:21:07.440]   code, it's going to spawn processes and launch
[00:21:07.440 --> 00:21:09.360]   this training function in parallel
[00:21:09.360 --> 00:21:11.200]   on the multiple GPUs you have if you
[00:21:11.200 --> 00:21:14.000]   are on a machine with several GPUs,
[00:21:14.000 --> 00:21:16.680]   or on eight GPUs if you are on a Colab Notebook
[00:21:16.680 --> 00:21:18.240]   with a TPU backend.
[00:21:18.240 --> 00:21:20.880]   If you're on a Colab Notebook with just one GPU for the GPU
[00:21:20.880 --> 00:21:22.960]   backend, you don't need a special launch function,
[00:21:22.960 --> 00:21:25.360]   because it's just one process.
[00:21:25.360 --> 00:21:27.720]   And yeah, that's pretty much everything
[00:21:27.720 --> 00:21:31.200]   about Accelerate in a nutshell.
[00:21:31.200 --> 00:21:33.360]   Yeah, I just want to add, Sylvain,
[00:21:33.360 --> 00:21:36.480]   that seeing this a few months ago,
[00:21:36.480 --> 00:21:40.160]   launching either on TPU, learning XMP.spawn
[00:21:40.160 --> 00:21:41.920]   and all those commands, and then seeing
[00:21:41.920 --> 00:21:43.520]   all of those different things, and then
[00:21:43.520 --> 00:21:45.920]   trying to debug those errors, or trying
[00:21:45.920 --> 00:21:48.200]   to do it for multiple GPUs, or trying
[00:21:48.200 --> 00:21:51.480]   to do it for single GPUs, everything was a pain.
[00:21:51.480 --> 00:21:54.960]   And then when Hugging Face Accelerate got launched,
[00:21:54.960 --> 00:21:58.920]   I actually was so surprised at how easy it is.
[00:21:58.920 --> 00:22:01.560]   That was really my first reaction, was like, oh,
[00:22:01.560 --> 00:22:05.240]   in five lines of code now, or a few lines of code at least,
[00:22:05.240 --> 00:22:06.840]   you can now replace everything.
[00:22:06.840 --> 00:22:08.680]   And the model just worked.
[00:22:08.680 --> 00:22:11.040]   The GPUs were being utilized properly.
[00:22:11.040 --> 00:22:13.480]   And my first reaction was surprised,
[00:22:13.480 --> 00:22:15.480]   and then I was astonished, and then I was happy.
[00:22:15.480 --> 00:22:18.520]   So those were the different stages at how that worked.
[00:22:18.520 --> 00:22:21.400]   And now I'm super happy to see that there's
[00:22:21.400 --> 00:22:23.360]   a library like this, that you don't have
[00:22:23.360 --> 00:22:24.760]   to use any special libraries.
[00:22:24.760 --> 00:22:27.280]   You can still stay native to PyTorch,
[00:22:27.280 --> 00:22:30.680]   and you can just add Accelerate to the code, which
[00:22:30.680 --> 00:22:34.160]   I have been doing for pretty much all my training,
[00:22:34.160 --> 00:22:36.720]   in the past at least couple of months,
[00:22:36.720 --> 00:22:38.280]   I've been using Accelerate.
[00:22:38.280 --> 00:22:40.560]   So that's really great to see.
[00:22:40.560 --> 00:22:42.760]   I do have a couple of examples, just in case
[00:22:42.760 --> 00:22:46.000]   I did want to share with the audience,
[00:22:46.000 --> 00:22:48.400]   just on how different things are.
[00:22:48.400 --> 00:22:50.080]   Or Sylvain, did you have some examples?
[00:22:50.080 --> 00:22:54.160]   Do you want to show the documentation examples
[00:22:54.160 --> 00:22:58.080]   on training multiple, and seeing this in practice?
[00:22:58.080 --> 00:22:59.840]   Yeah, so I was thinking maybe you
[00:22:59.840 --> 00:23:02.040]   can start with a collab, and see how
[00:23:02.040 --> 00:23:03.640]   to train with ATPs and collabs.
[00:23:03.640 --> 00:23:04.920]   So this is all free.
[00:23:04.920 --> 00:23:06.680]   And if you are on a collab, and if you just
[00:23:06.680 --> 00:23:11.360]   click runtime, or is it run, chain run type, type.
[00:23:11.360 --> 00:23:13.720]   So I'm already on TPU, because I launched the notebook
[00:23:13.720 --> 00:23:15.840]   to test it just before the webinar.
[00:23:15.840 --> 00:23:19.240]   But you should be on none if you just open collab.
[00:23:19.240 --> 00:23:22.400]   So you would just click on TPU, you save, and then poof.
[00:23:22.400 --> 00:23:25.000]   You have ATPs available waiting for you.
[00:23:25.000 --> 00:23:25.520]   And so--
[00:23:25.520 --> 00:23:26.920]   And where can we find the notebook?
[00:23:26.920 --> 00:23:28.280]   Sorry, Sylvain, to cut you, but did you
[00:23:28.280 --> 00:23:29.720]   want to show everybody where we can
[00:23:29.720 --> 00:23:32.680]   find the documentation and the notebook for Accelerate,
[00:23:32.680 --> 00:23:35.160]   just in case anybody wants to get started?
[00:23:35.160 --> 00:23:38.160]   So if I go on the readme of Accelerate on GitHub,
[00:23:38.160 --> 00:23:41.000]   I can find the documentation here, for instance,
[00:23:41.000 --> 00:23:42.440]   when I click on that little badge.
[00:23:42.440 --> 00:23:45.720]   Or it's also mentioned with the river link afterward.
[00:23:45.720 --> 00:23:47.520]   Let me just give you the example.
[00:23:47.520 --> 00:23:49.760]   So here's a link to the documentation.
[00:23:49.760 --> 00:23:53.720]   So it's on huggingface.co/doc/accelerate.
[00:23:53.720 --> 00:23:57.200]   And then the notebook, you can find it on this readme.
[00:23:57.200 --> 00:23:58.960]   There is even an open in collab button.
[00:23:58.960 --> 00:24:01.240]   So if you scroll the readme to launching your training
[00:24:01.240 --> 00:24:04.280]   from a notebook, you can find the notebook on GitHub
[00:24:04.280 --> 00:24:09.600]   and an open in collab badge that you just have to click on.
[00:24:09.600 --> 00:24:11.720]   And so yeah, this notebook, so you just
[00:24:11.720 --> 00:24:13.920]   have to pip install the library you need.
[00:24:13.920 --> 00:24:16.160]   So my example is using transformers and data sets.
[00:24:16.160 --> 00:24:18.880]   So I'm pip installing from that.
[00:24:18.880 --> 00:24:24.120]   And then you have to install the Cloud TPU client
[00:24:24.120 --> 00:24:28.160]   library with the wheels for Python 6LA.
[00:24:28.160 --> 00:24:30.440]   So this is the command right now.
[00:24:30.440 --> 00:24:34.280]   Be sure to update it each time PyTorch has a new release.
[00:24:34.280 --> 00:24:36.680]   Because since Wednesday, they do a new release,
[00:24:36.680 --> 00:24:38.600]   it's not going to be, for instance, 1.9 here.
[00:24:38.600 --> 00:24:39.800]   It's going to be 1.10.
[00:24:39.800 --> 00:24:42.680]   So the command can change slightly.
[00:24:42.680 --> 00:24:44.080]   And then I think--
[00:24:44.080 --> 00:24:47.040]   I don't know why I didn't master install of accelerate.
[00:24:47.040 --> 00:24:49.520]   Maybe because I wanted to test that it didn't break anything.
[00:24:49.520 --> 00:24:53.120]   But this shouldn't be necessary here.
[00:24:53.120 --> 00:24:54.520]   So this installs everything.
[00:24:54.520 --> 00:24:57.920]   And then suppose all the imports we'll need.
[00:24:57.920 --> 00:25:02.000]   If you have the TPU runtime, you will see those waiting
[00:25:02.000 --> 00:25:03.480]   for TPU to be startup.
[00:25:03.480 --> 00:25:06.560]   That's the first time you do the from accelerate import
[00:25:06.560 --> 00:25:10.560]   accelerator line, which is the main object,
[00:25:10.560 --> 00:25:13.040]   as we saw during the presentation.
[00:25:13.040 --> 00:25:14.480]   And so in this example, I'm going
[00:25:14.480 --> 00:25:18.840]   to find you in the BERT model onto the MRPC data set.
[00:25:18.840 --> 00:25:21.400]   It's kind of my go-to example, because the MRPC data
[00:25:21.400 --> 00:25:22.280]   set is super small.
[00:25:22.280 --> 00:25:24.240]   So the training runs very fast.
[00:25:24.240 --> 00:25:26.200]   And I can quickly iterate on that.
[00:25:26.200 --> 00:25:27.400]   So you can load it.
[00:25:27.400 --> 00:25:29.080]   If you don't know the data sets library,
[00:25:29.080 --> 00:25:32.080]   I recommend to check it out, because you can download
[00:25:32.080 --> 00:25:34.040]   and cache your data sets in just one line of code
[00:25:34.040 --> 00:25:36.000]   with the load data set command.
[00:25:36.000 --> 00:25:38.560]   And then you have this kind of dictionary
[00:25:38.560 --> 00:25:41.600]   with a training set, a validation set, and a test set.
[00:25:41.600 --> 00:25:44.360]   And each of those sets has several columns.
[00:25:44.360 --> 00:25:46.520]   So the MRPC data set is a data set
[00:25:46.520 --> 00:25:48.240]   that contains pairs of sentences.
[00:25:48.240 --> 00:25:50.840]   So sentence one, sentence two, and a label.
[00:25:50.840 --> 00:25:55.160]   And the task is to determine if those two sentences are
[00:25:55.160 --> 00:25:57.120]   similar or not.
[00:25:57.120 --> 00:25:58.560]   You can look at an example.
[00:25:58.560 --> 00:26:02.160]   So to access the training step, we index with train.
[00:26:02.160 --> 00:26:03.640]   Like I said, it's like a dictionary.
[00:26:03.640 --> 00:26:07.040]   And then we can index if this is the first element
[00:26:07.040 --> 00:26:08.280]   of the training data set.
[00:26:08.280 --> 00:26:10.200]   We can see the two sentences.
[00:26:10.200 --> 00:26:13.440]   The label one is--
[00:26:13.440 --> 00:26:14.880]   I didn't print the correspondence.
[00:26:14.880 --> 00:26:18.200]   I think it stands for equivalent,
[00:26:18.200 --> 00:26:21.320]   because those two sentences kind of look the same.
[00:26:21.320 --> 00:26:23.000]   We don't really care about the function,
[00:26:23.000 --> 00:26:24.920]   but here is a quick visualization
[00:26:25.240 --> 00:26:28.440]   of random samples of the training set.
[00:26:28.440 --> 00:26:31.160]   So we can see sentences that are equivalent, sentences
[00:26:31.160 --> 00:26:34.120]   that are not equivalent.
[00:26:34.120 --> 00:26:38.200]   And so then to preprocess the whole data set
[00:26:38.200 --> 00:26:40.200]   and make it ready for the model, this code
[00:26:40.200 --> 00:26:41.520]   should look super familiar.
[00:26:41.520 --> 00:26:44.280]   If you've already looked at the Bigging Face course,
[00:26:44.280 --> 00:26:47.000]   for instance, if you've used the Transformers library
[00:26:47.000 --> 00:26:49.000]   in the past, but in the Transformers library
[00:26:49.000 --> 00:26:52.000]   to preprocess the text, you have to use a tokenizer, which
[00:26:52.000 --> 00:26:55.080]   is going to parse the text in small chunks called tokens
[00:26:55.080 --> 00:26:58.840]   and then convert those tokens into the unique IDs
[00:26:58.840 --> 00:27:01.600]   in the vocabulary of the model you're using.
[00:27:01.600 --> 00:27:04.000]   So this is done with the AutoTokenizer class.
[00:27:04.000 --> 00:27:07.600]   And it has a front-pre-train method
[00:27:07.600 --> 00:27:12.120]   for which we can use the model checkpoint we defined earlier.
[00:27:12.120 --> 00:27:14.200]   I'm going super quickly, because that's not really
[00:27:14.200 --> 00:27:15.920]   the point of today.
[00:27:15.920 --> 00:27:17.880]   We can preprocess spell sentences.
[00:27:17.880 --> 00:27:21.640]   We define a tokenized function that preprocesses samples
[00:27:21.640 --> 00:27:23.560]   or several samples at once.
[00:27:23.560 --> 00:27:26.920]   And we have a math method in the data sets
[00:27:26.920 --> 00:27:29.200]   that we can apply, that we can use
[00:27:29.200 --> 00:27:32.960]   to apply the tokenized functions on every sample in the data set.
[00:27:32.960 --> 00:27:36.280]   And we remove the columns of our text
[00:27:36.280 --> 00:27:40.320]   to keep only the columns of numbers.
[00:27:40.320 --> 00:27:42.680]   And once we've tokenized things, so we keep our labels,
[00:27:42.680 --> 00:27:43.960]   because they were before.
[00:27:43.960 --> 00:27:47.240]   And the text are replaced by input IDs, attention masks,
[00:27:47.240 --> 00:27:50.360]   and token type IDs, which are the three inputs the model is
[00:27:50.360 --> 00:27:51.720]   going to expect.
[00:27:51.720 --> 00:27:54.360]   As input IDs are the IDs of each token,
[00:27:54.360 --> 00:27:57.800]   the attention mask tells the model where there is padding
[00:27:57.800 --> 00:27:59.640]   and where there is real tokens.
[00:27:59.640 --> 00:28:01.560]   So it doesn't pay attention to the padding.
[00:28:01.560 --> 00:28:04.360]   And the token type IDs, it's because we have two sentences.
[00:28:04.360 --> 00:28:06.840]   So it tells the model which one is the first sentence
[00:28:06.840 --> 00:28:09.080]   and which one is the second sentence.
[00:28:09.080 --> 00:28:13.480]   And once we've done all of that, we
[00:28:13.480 --> 00:28:15.880]   will download and cache the model by using the AutoModel
[00:28:15.880 --> 00:28:18.680]   for sequence classification class.
[00:28:18.680 --> 00:28:20.600]   So we use the same model checkpoint.
[00:28:20.600 --> 00:28:21.760]   We have two labels here.
[00:28:21.760 --> 00:28:24.400]   So we tell that to our model.
[00:28:24.400 --> 00:28:25.440]   So we download it.
[00:28:25.440 --> 00:28:26.400]   We cache it.
[00:28:26.400 --> 00:28:28.200]   It's going to remove the pre-training ad
[00:28:28.200 --> 00:28:29.800]   and replace it with a classification
[00:28:29.800 --> 00:28:30.880]   ad with two labels.
[00:28:30.880 --> 00:28:32.280]   So we can do fine tuning.
[00:28:32.280 --> 00:28:34.520]   And now we are in the code of the training loop.
[00:28:34.520 --> 00:28:36.400]   So we create our training data loader.
[00:28:36.400 --> 00:28:37.840]   And with Accelerate, we don't have
[00:28:37.840 --> 00:28:40.200]   to worry about defining a distributed sampler.
[00:28:40.200 --> 00:28:42.160]   It's just with shuffle equal true.
[00:28:42.160 --> 00:28:43.960]   It's going to take care of everything.
[00:28:43.960 --> 00:28:46.760]   And if you add your custom sampler,
[00:28:46.760 --> 00:28:47.960]   it's going to work as well.
[00:28:47.960 --> 00:28:51.320]   And you won't have to write a distributed version of it.
[00:28:51.320 --> 00:28:53.680]   That was actually the first thing that motivated me
[00:28:53.680 --> 00:28:57.360]   to create this library is when doing code for transformers,
[00:28:57.360 --> 00:29:01.200]   I had to define a sampler for when
[00:29:01.200 --> 00:29:03.640]   in LP, for instance, where we want to shuffle the data set,
[00:29:03.640 --> 00:29:06.640]   but still have to give a sample text of the same length.
[00:29:06.640 --> 00:29:08.160]   So this is a specific sampler.
[00:29:08.160 --> 00:29:11.960]   But if I wanted it to work in distributed mode,
[00:29:11.960 --> 00:29:14.080]   I had to write a second version of the same sampler
[00:29:14.080 --> 00:29:15.360]   that was distributed.
[00:29:15.360 --> 00:29:17.760]   And with Accelerate, we will never have to do that again,
[00:29:17.760 --> 00:29:19.440]   because it will take care of everything.
[00:29:19.440 --> 00:29:21.040]   We'll see how it's just after.
[00:29:21.040 --> 00:29:23.320]   So with this function, create our data loaders.
[00:29:23.320 --> 00:29:25.360]   This is a bit of debugging code to look
[00:29:25.360 --> 00:29:27.440]   at everything that's correct.
[00:29:27.440 --> 00:29:31.280]   We have a metric that comes from the data sets library.
[00:29:31.280 --> 00:29:33.320]   And the API looks like the load data set,
[00:29:33.320 --> 00:29:34.640]   but it's with load metric.
[00:29:34.640 --> 00:29:36.560]   And it's going to load the exact metric that's
[00:29:36.560 --> 00:29:41.240]   used for this MRPC data set in the academic papers, which
[00:29:41.240 --> 00:29:44.120]   are accuracy on F1.
[00:29:44.120 --> 00:29:46.880]   I defined a few hyperparameters here, learning rate,
[00:29:46.880 --> 00:29:49.240]   number of epochs I want to do, training batch size.
[00:29:49.240 --> 00:29:51.160]   And since I'm training on 80 PUs,
[00:29:51.160 --> 00:29:53.080]   everything is going to be multiplied by 8.
[00:29:53.080 --> 00:29:56.040]   So that's why 8 for training batch size is going to be fine.
[00:29:56.040 --> 00:29:59.600]   And it's going to be 64 in for real.
[00:29:59.600 --> 00:30:01.800]   And like I said, I have to define everything
[00:30:01.800 --> 00:30:02.880]   in a training function.
[00:30:02.880 --> 00:30:04.560]   So let's look at that training function,
[00:30:04.560 --> 00:30:07.240]   because this is really what we are talking about today.
[00:30:07.240 --> 00:30:09.680]   So we define that accelerator object,
[00:30:09.680 --> 00:30:11.680]   like in the slide that we're presenting you.
[00:30:11.680 --> 00:30:14.480]   And then there's a few more codes than before,
[00:30:14.480 --> 00:30:17.800]   because I'm adding a few things that
[00:30:17.800 --> 00:30:19.560]   wasn't in my base training loop.
[00:30:19.560 --> 00:30:21.480]   So the first one is I don't want the logs
[00:30:21.480 --> 00:30:23.640]   to be repeated eight times, which
[00:30:23.640 --> 00:30:25.080]   is going to be the case by default,
[00:30:25.080 --> 00:30:27.480]   because I'm going to launch eight processes in parallel,
[00:30:27.480 --> 00:30:30.200]   each process for one TPU.
[00:30:30.200 --> 00:30:33.560]   So to make sure that I'm only logging on the main process
[00:30:33.560 --> 00:30:36.880]   and not on the other ones, I'm using this if accelerator
[00:30:36.880 --> 00:30:38.400]   that is main process.
[00:30:38.400 --> 00:30:39.960]   And when it's a main process, I'm
[00:30:39.960 --> 00:30:43.080]   setting the logging to that for positive.
[00:30:43.080 --> 00:30:45.040]   And when it's not, I'm sticking the logging
[00:30:45.040 --> 00:30:46.160]   to the lower of a positive.
[00:30:46.160 --> 00:30:47.760]   So error is like--
[00:30:47.760 --> 00:30:50.160]   it's only going to tell me something if there is an error.
[00:30:50.160 --> 00:30:51.880]   And otherwise, for data sets, it's
[00:30:51.880 --> 00:30:53.280]   super-vulnerable in info mode.
[00:30:53.280 --> 00:30:54.840]   So that's why I'm using warning here.
[00:30:54.840 --> 00:30:57.880]   And for transformer, I'm using the info mode.
[00:30:57.880 --> 00:31:00.520]   I'm creating my data loaders exactly as I
[00:31:00.520 --> 00:31:04.760]   would for a training script on one CPU.
[00:31:04.760 --> 00:31:08.040]   I'm setting the seed before instantiating the model,
[00:31:08.040 --> 00:31:13.040]   since the head of the model is going to be removed and replaced
[00:31:13.040 --> 00:31:14.960]   with a randomly initialized head.
[00:31:14.960 --> 00:31:16.760]   If I want to replace the traceable training,
[00:31:16.760 --> 00:31:20.160]   I have to set the seed before instantiating the model.
[00:31:20.160 --> 00:31:23.520]   This is an error I see often in issues on the forum.
[00:31:23.520 --> 00:31:25.000]   People are like, oh, it's not--
[00:31:25.000 --> 00:31:26.560]   why didn't get the same result?
[00:31:26.560 --> 00:31:29.320]   Because make sure you always set your seed
[00:31:29.320 --> 00:31:31.600]   before instantiating your model.
[00:31:31.600 --> 00:31:32.760]   I create an optimizer.
[00:31:32.760 --> 00:31:34.240]   I took AdamW here.
[00:31:34.240 --> 00:31:36.240]   This is the AdamW from the Transformers library,
[00:31:36.240 --> 00:31:40.960]   but it's not very different from the one in the Torch library.
[00:31:40.960 --> 00:31:44.440]   And the main line of code, as we saw in the presentation,
[00:31:44.440 --> 00:31:47.600]   is to send all of those objects to the accelerator.prepare
[00:31:47.600 --> 00:31:48.960]   method.
[00:31:48.960 --> 00:31:52.920]   And then, very important, always create your learning rate
[00:31:52.920 --> 00:31:57.000]   scheduler after sending all your objects to the prepare method,
[00:31:57.000 --> 00:31:58.800]   especially the training data loader.
[00:31:58.800 --> 00:31:59.440]   Why is that?
[00:31:59.440 --> 00:32:01.840]   Because when you do training data loader,
[00:32:01.840 --> 00:32:03.480]   when you send your training data loader
[00:32:03.480 --> 00:32:05.280]   to the accelerator.prepare method,
[00:32:05.280 --> 00:32:06.800]   it's going to change length.
[00:32:06.800 --> 00:32:07.280]   Why?
[00:32:07.280 --> 00:32:10.400]   Because the data loader you created,
[00:32:10.400 --> 00:32:12.120]   it's going to go over the whole data set.
[00:32:12.120 --> 00:32:14.640]   And when Accelerate goes over that,
[00:32:14.640 --> 00:32:16.040]   it's going to, behind the scenes,
[00:32:16.040 --> 00:32:18.560]   shard the data set so that each of your process
[00:32:18.560 --> 00:32:22.400]   only looks one part of the data set.
[00:32:22.400 --> 00:32:25.440]   Otherwise, you will be training on the same data
[00:32:25.440 --> 00:32:28.120]   on each process, which is not very useful.
[00:32:28.120 --> 00:32:30.160]   So here we have eight TPUs.
[00:32:30.160 --> 00:32:32.960]   So there are going to be eight processes.
[00:32:32.960 --> 00:32:35.160]   So it's going to shard your data set in eight.
[00:32:35.160 --> 00:32:38.000]   And your training data loader will have a length
[00:32:38.000 --> 00:32:40.040]   that's eight times smaller.
[00:32:40.040 --> 00:32:41.480]   So for the learning rate scheduler,
[00:32:41.480 --> 00:32:44.640]   it's very important to use that updated length and not
[00:32:44.640 --> 00:32:45.720]   old length.
[00:32:45.720 --> 00:32:48.760]   So if I had defined it before sending everything
[00:32:48.760 --> 00:32:52.200]   to accelerator.prepare, I would have had a learning rate
[00:32:52.200 --> 00:32:54.160]   scheduler that had the wrong length.
[00:32:54.160 --> 00:32:56.120]   And so it would have not--
[00:32:56.120 --> 00:33:00.160]   for instance, if it's something that just goes from a certain
[00:33:00.160 --> 00:33:05.600]   learning rate to zero, it would have stopped way before zero.
[00:33:05.600 --> 00:33:07.440]   Same way for defining my progress bar.
[00:33:07.440 --> 00:33:09.560]   It's using the length of the training data loader.
[00:33:09.560 --> 00:33:12.160]   I should do that after saying my training data loader
[00:33:12.160 --> 00:33:14.400]   to the accelerator.prepare process.
[00:33:14.400 --> 00:33:18.320]   And again, I'm using the accelerator.is_main process
[00:33:18.320 --> 00:33:22.840]   Boolean to know if I should actually show that progress
[00:33:22.840 --> 00:33:23.400]   bar or not.
[00:33:23.400 --> 00:33:26.160]   So that varies one progress bar and that eight progress bar
[00:33:26.160 --> 00:33:29.480]   that kind of updates at the same time.
[00:33:29.480 --> 00:33:31.680]   Once I've done that, I'm ready for the training code.
[00:33:31.680 --> 00:33:33.440]   So the training code is pretty standard.
[00:33:33.440 --> 00:33:37.440]   I'm putting my model in training mode, computing the loss.
[00:33:37.440 --> 00:33:39.640]   I'm using accelerator.backward_loss
[00:33:39.640 --> 00:33:40.680]   as we saw in the slides.
[00:33:40.680 --> 00:33:42.280]   And then all the rest, as you see,
[00:33:42.280 --> 00:33:44.440]   is like classic byte-toss training loop.
[00:33:44.440 --> 00:33:46.720]   And I've also added the evaluation loop.
[00:33:46.720 --> 00:33:49.440]   So I'm putting the model in evaluation mode.
[00:33:49.440 --> 00:33:53.200]   I'm going to gather all the predictions and all the labels.
[00:33:53.200 --> 00:33:55.640]   So I'm going through my data loader.
[00:33:55.640 --> 00:33:58.640]   I've computed my outputs with a Torch.no_grad decorator
[00:33:58.640 --> 00:34:01.400]   to avoid spending memories on the gradients.
[00:34:01.400 --> 00:34:02.520]   Yes, I'm still there.
[00:34:02.520 --> 00:34:09.720]   So this is it.
[00:34:09.720 --> 00:34:11.640]   You can't really leave your training run
[00:34:11.640 --> 00:34:13.000]   without you on the call up.
[00:34:13.000 --> 00:34:15.560]   They want you to still be able to respond to those
[00:34:15.560 --> 00:34:17.640]   source captures sometimes.
[00:34:17.640 --> 00:34:19.160]   And so I'm going to--
[00:34:19.160 --> 00:34:20.640]   so this is another method.
[00:34:20.640 --> 00:34:22.160]   Like I said during the presentation,
[00:34:22.160 --> 00:34:23.560]   if you want to do evaluation, you
[00:34:23.560 --> 00:34:26.200]   are going to have to learn a few more methods from accelerates.
[00:34:26.200 --> 00:34:28.440]   So this is what I was talking about.
[00:34:28.480 --> 00:34:32.680]   Once your model has computed those predictions here,
[00:34:32.680 --> 00:34:35.240]   so each process has computed a prediction
[00:34:35.240 --> 00:34:36.600]   on a part of the data set.
[00:34:36.600 --> 00:34:38.080]   And you have to put that together
[00:34:38.080 --> 00:34:39.360]   to get all your predictions.
[00:34:39.360 --> 00:34:41.040]   And same for the labels.
[00:34:41.040 --> 00:34:42.600]   The labels, the model had access to,
[00:34:42.600 --> 00:34:45.800]   were just like one part of the data set.
[00:34:45.800 --> 00:34:48.280]   And so to put them all back together,
[00:34:48.280 --> 00:34:50.960]   it's just very simple accelerator.gather method.
[00:34:50.960 --> 00:34:53.920]   You pass the predictions, and here
[00:34:53.920 --> 00:34:56.920]   we're happening the results to the old predictions array.
[00:34:56.920 --> 00:35:00.040]   And with that, that's the only thing we have to think about.
[00:35:00.040 --> 00:35:01.920]   Once this is done, we have all our predictions
[00:35:01.920 --> 00:35:03.040]   and all our labels.
[00:35:03.040 --> 00:35:05.320]   Oh, no, there's a second thing we have to think about.
[00:35:05.320 --> 00:35:06.000]   Sorry.
[00:35:06.000 --> 00:35:07.840]   There's a second thing we have to think about
[00:35:07.840 --> 00:35:12.960]   is that for when you do a distributed training,
[00:35:12.960 --> 00:35:16.920]   as like I said, your data set is going to split in here
[00:35:16.920 --> 00:35:18.440]   eight small parts.
[00:35:18.440 --> 00:35:22.600]   And for several reasons, especially for TPUs,
[00:35:22.600 --> 00:35:25.760]   we need those eight parts to have exactly the same length.
[00:35:25.760 --> 00:35:28.400]   So what Accelerate does behind the scene
[00:35:28.400 --> 00:35:30.840]   is that it's going to add a few samples at the end,
[00:35:30.840 --> 00:35:33.240]   samples from the beginning, so that your data
[00:35:33.240 --> 00:35:35.440]   set is around multiple--
[00:35:35.440 --> 00:35:37.480]   has a length that is around multiple of eight.
[00:35:37.480 --> 00:35:41.080]   So this is why you have a few predictions too many
[00:35:41.080 --> 00:35:42.560]   and a few labels too many.
[00:35:42.560 --> 00:35:44.840]   And so you have to truncate the result
[00:35:44.840 --> 00:35:47.800]   to the length of your actual validation data set.
[00:35:47.800 --> 00:35:49.400]   Otherwise, you will get--
[00:35:49.400 --> 00:35:51.440]   I mean, it's not going to be the end of the world,
[00:35:51.440 --> 00:35:53.440]   but you will get slightly more prediction at the end
[00:35:53.440 --> 00:35:55.240]   that are exactly the same as the beginning.
[00:35:55.240 --> 00:35:58.400]   So your metrics will be slightly off.
[00:35:58.400 --> 00:36:00.080]   And once this is all done, so the metric
[00:36:00.080 --> 00:36:01.600]   from the data set library, you just
[00:36:01.600 --> 00:36:04.000]   pass into metric.compute all your prediction
[00:36:04.000 --> 00:36:05.040]   and all your labels.
[00:36:05.040 --> 00:36:07.120]   And poof, you go to results.
[00:36:07.120 --> 00:36:11.200]   And so all of that, like I said, is in a training function.
[00:36:11.200 --> 00:36:13.600]   And I just have to do from accelerate import notebook
[00:36:13.600 --> 00:36:16.240]   launcher and do notebook launcher training function.
[00:36:16.240 --> 00:36:22.560]   I just run that command just before I started the webinar.
[00:36:22.560 --> 00:36:24.120]   So it's going to display something
[00:36:24.120 --> 00:36:26.160]   that, rushing a training on a TPU course.
[00:36:26.160 --> 00:36:29.880]   And then you get all the logs, your progress bar,
[00:36:29.880 --> 00:36:31.640]   and then the metric results.
[00:36:31.640 --> 00:36:34.080]   Let's see if it executes quickly.
[00:36:34.080 --> 00:36:37.000]   I just wanted to show you the output before we're clicking,
[00:36:37.000 --> 00:36:39.240]   because otherwise, if it takes too long,
[00:36:39.240 --> 00:36:40.560]   we're going to ignore it.
[00:36:40.560 --> 00:36:43.880]   So what usually happens when you train on TPUs in Colab
[00:36:43.880 --> 00:36:45.960]   is that you will get this for a while.
[00:36:45.960 --> 00:36:49.400]   This is just for because it's waiting for us
[00:36:49.400 --> 00:36:52.080]   to actually get you the TPUs, because I'm
[00:36:52.080 --> 00:36:54.280]   thinking that before you execute that line,
[00:36:54.280 --> 00:36:58.600]   they didn't really allocate any of those machines for you.
[00:36:58.600 --> 00:37:01.160]   So this is why you have a bit of waiting time here.
[00:37:01.160 --> 00:37:02.800]   And then once it's done, you can see
[00:37:02.800 --> 00:37:04.440]   like these both are standard logs.
[00:37:04.440 --> 00:37:05.640]   So I'm loading my model.
[00:37:05.640 --> 00:37:09.840]   So it's telling me what config it is, turning the weights.
[00:37:09.840 --> 00:37:11.880]   It's probably-- I know, I had said
[00:37:11.880 --> 00:37:13.560]   the data sets are logs to warning.
[00:37:13.560 --> 00:37:16.200]   So it didn't tell me it's loading it from the cache,
[00:37:16.200 --> 00:37:17.440]   but it did.
[00:37:17.440 --> 00:37:19.480]   And then the second thing to be aware of
[00:37:19.480 --> 00:37:21.960]   is when you arrive here, I don't know
[00:37:21.960 --> 00:37:24.200]   if it's going to show it, because it's the second time
[00:37:24.200 --> 00:37:25.000]   I execute this out.
[00:37:25.000 --> 00:37:28.200]   But usually, what happens is that the first two iterations
[00:37:28.200 --> 00:37:29.400]   are going to be super slow.
[00:37:29.400 --> 00:37:30.880]   This is completely normal.
[00:37:30.880 --> 00:37:34.280]   That's because when using PyTorch XLA on TPU,
[00:37:34.280 --> 00:37:36.480]   what happens behind the scene is that PyTorch XLA
[00:37:36.480 --> 00:37:39.280]   is going to interpret and compile the whole graph,
[00:37:39.280 --> 00:37:41.840]   the graph being the whole forward path and then
[00:37:41.840 --> 00:37:42.680]   the backward path.
[00:37:42.680 --> 00:37:46.120]   And so you can see, yeah, the first time I executed it,
[00:37:46.120 --> 00:37:48.880]   my first step, it was telling me I had two hours to wait.
[00:37:48.880 --> 00:37:51.320]   So this is going to be a bit faster, because that actually
[00:37:51.320 --> 00:37:52.520]   cached the completion.
[00:37:52.520 --> 00:37:54.400]   And so once those first steps are done,
[00:37:54.400 --> 00:37:56.640]   you can see the rest is going to go super quickly.
[00:37:56.640 --> 00:37:58.600]   We have even in one minute, the training
[00:37:58.600 --> 00:37:59.520]   is going to be finished.
[00:37:59.520 --> 00:38:02.120]   And it's going over those batches.
[00:38:02.120 --> 00:38:04.400]   And this is with a relatively small batch size,
[00:38:04.400 --> 00:38:06.480]   because I have eight--
[00:38:06.480 --> 00:38:09.320]   the batch size is eight per process, so 64 overall.
[00:38:09.320 --> 00:38:11.480]   If you have a higher batch size, your training
[00:38:11.480 --> 00:38:14.160]   is going to go way faster.
[00:38:14.160 --> 00:38:16.840]   So that's it for this training example.
[00:38:16.840 --> 00:38:19.000]   And yeah, you can see it's printing us
[00:38:19.000 --> 00:38:20.880]   the accuracy on F1's core.
[00:38:20.880 --> 00:38:23.040]   So our model is training.
[00:38:23.040 --> 00:38:24.720]   So yeah, that's it for--
[00:38:24.720 --> 00:38:26.320]   And correct me if I'm wrong, Sylvain.
[00:38:26.320 --> 00:38:29.480]   If you have multi-GPUs on a Jupyter Notebook,
[00:38:29.480 --> 00:38:33.840]   before Hugging Face Accelerate or having the Notebook Launcher
[00:38:33.840 --> 00:38:36.520]   function, I don't think there was
[00:38:36.520 --> 00:38:40.120]   a way to launch a training script on multiple GPUs
[00:38:40.120 --> 00:38:42.280]   before Accelerate.
[00:38:42.280 --> 00:38:44.840]   I mean, it would be very complex to do so.
[00:38:44.840 --> 00:38:48.640]   Yeah, so the functions exist, but they are not
[00:38:48.640 --> 00:38:51.080]   super well documented in the PyTorch library.
[00:38:51.080 --> 00:38:55.040]   And I hadn't seen any example of something doing that.
[00:38:55.040 --> 00:38:58.400]   I think there is something in PyTorch Lightning that
[00:38:58.400 --> 00:39:00.760]   makes your training happen on multiple GPUs
[00:39:00.760 --> 00:39:03.000]   if you are launching it from a Notebook.
[00:39:03.000 --> 00:39:04.600]   I'm not entirely sure.
[00:39:04.600 --> 00:39:10.280]   But it's like the [INAUDIBLE] you can't run your custom
[00:39:10.280 --> 00:39:11.880]   training script with that.
[00:39:11.880 --> 00:39:13.880]   They don't have a launcher like that.
[00:39:13.880 --> 00:39:17.960]   So yeah, I think that the only--
[00:39:17.960 --> 00:39:19.480]   Accelerate is the only library that
[00:39:19.480 --> 00:39:20.720]   provides something like that.
[00:39:20.720 --> 00:39:23.080]   For TPU training, you had some sample codes,
[00:39:23.080 --> 00:39:25.160]   but it was way more complex than just one code
[00:39:25.160 --> 00:39:27.320]   to Notebook Launcher like this.
[00:39:27.320 --> 00:39:31.480]   But they had multiple examples of how to do that.
[00:39:31.480 --> 00:39:32.160]   Yeah, totally.
[00:39:32.160 --> 00:39:35.080]   That's what I remember, is how complex it used to be.
[00:39:35.080 --> 00:39:38.440]   And I think then also, when I used
[00:39:38.440 --> 00:39:41.920]   to do multiple GPU training, and I'm trying to use TQDM,
[00:39:41.920 --> 00:39:43.960]   or you have to be always--
[00:39:43.960 --> 00:39:46.200]   if args.mainprocess, you have to always
[00:39:46.200 --> 00:39:48.760]   be very cautious of those things.
[00:39:48.760 --> 00:39:50.680]   But now you can use accelerate.print,
[00:39:50.680 --> 00:39:53.000]   or you can use accelerate.uselocalprocess,
[00:39:53.000 --> 00:39:57.000]   or things like that, which is a really nice convenience
[00:39:57.000 --> 00:39:58.040]   to have.
[00:39:58.040 --> 00:40:01.440]   Those small little things really make a bit of a difference.
[00:40:01.440 --> 00:40:03.600]   Yeah, I think you're right to remind me.
[00:40:03.600 --> 00:40:05.520]   I didn't mention accelerate.print at the end.
[00:40:05.520 --> 00:40:08.480]   But yeah, accelerate.print, it replaces print,
[00:40:08.480 --> 00:40:11.200]   but it's only going to print once,
[00:40:11.200 --> 00:40:12.640]   or instead of printing eight times.
[00:40:12.640 --> 00:40:16.400]   So it's the easiest way to load things.
[00:40:16.400 --> 00:40:18.400]   Yeah, do you mind if I quickly share my screen?
[00:40:18.400 --> 00:40:22.560]   I just want to show one thing related to this.
[00:40:22.560 --> 00:40:23.240]   Please go ahead.
[00:40:23.240 --> 00:40:29.160]   So I just wanted to quickly bring up,
[00:40:29.160 --> 00:40:31.160]   when Hugging Face was--
[00:40:31.160 --> 00:40:34.720]   Accelerate was first released, we also have--
[00:40:34.720 --> 00:40:37.600]   I wrote a blog post about the accelerate functions.
[00:40:37.600 --> 00:40:40.000]   If you're curious about what exactly
[00:40:40.000 --> 00:40:43.840]   is going on inside, or how some of that works, it is outdated.
[00:40:43.840 --> 00:40:45.760]   So it would be appreciated if someone wants
[00:40:45.760 --> 00:40:47.240]   to go in and update that.
[00:40:47.240 --> 00:40:49.360]   But this is basically a nice summary
[00:40:49.360 --> 00:40:53.040]   of what Sylvain has presented so far, in my opinion.
[00:40:53.040 --> 00:40:56.120]   These are the five or four main steps you have to do.
[00:40:56.120 --> 00:40:58.720]   If you're using just Torch, you have to have an init process
[00:40:58.720 --> 00:40:59.560]   group.
[00:40:59.560 --> 00:41:02.240]   You have to look after local world size, or ranks,
[00:41:02.240 --> 00:41:04.080]   of your different processes.
[00:41:04.080 --> 00:41:06.600]   Then you have to wrap your Torch data loaders.
[00:41:06.600 --> 00:41:09.120]   And all of this is then just being replaced
[00:41:09.120 --> 00:41:12.080]   with these few lines of code.
[00:41:12.080 --> 00:41:14.360]   So I just want to quickly show, actually,
[00:41:14.360 --> 00:41:19.920]   one example on how training looks like with--
[00:41:19.920 --> 00:41:24.480]   so just in that, you'll see this--
[00:41:24.480 --> 00:41:28.120]   just in that blog post, you'll see there's two links
[00:41:28.120 --> 00:41:29.000]   to two scripts.
[00:41:29.000 --> 00:41:30.360]   When I was writing the blog post,
[00:41:30.360 --> 00:41:32.480]   I basically wrote two small scripts.
[00:41:32.480 --> 00:41:34.160]   One is training accelerate.
[00:41:34.160 --> 00:41:36.120]   And this is computer vision.
[00:41:36.120 --> 00:41:39.640]   And then the other one is just using pure data
[00:41:39.640 --> 00:41:40.640]   distributed parallel.
[00:41:40.640 --> 00:41:45.120]   And it's using-- it's basically, if you go in there
[00:41:45.120 --> 00:41:47.160]   and you can see how to download the data set,
[00:41:47.160 --> 00:41:48.400]   it has some links.
[00:41:48.400 --> 00:41:51.600]   And it's basically just using ImageNet, which is a smaller
[00:41:51.600 --> 00:41:52.520]   version of ImageNet.
[00:41:52.520 --> 00:41:55.360]   So I just wanted to share how, really, it's
[00:41:55.360 --> 00:41:58.560]   a bit of a job to do things before accelerate.
[00:41:58.560 --> 00:41:59.680]   So I had to--
[00:41:59.680 --> 00:42:02.600]   you have a special launch script that you
[00:42:02.600 --> 00:42:05.640]   have to use, as Sylvain has already mentioned.
[00:42:05.640 --> 00:42:07.840]   But I just wanted to put those things into context
[00:42:07.840 --> 00:42:09.800]   on how, really, those things look like.
[00:42:09.800 --> 00:42:15.440]   So if I go in and I say .ddp/sh, which is calling those things,
[00:42:15.440 --> 00:42:16.880]   and there's already an error.
[00:42:16.880 --> 00:42:18.440]   So I did something.
[00:42:18.440 --> 00:42:27.680]   Let's see if that works.
[00:42:27.680 --> 00:42:28.400]   Oh, look at that.
[00:42:28.400 --> 00:42:29.880]   Again, there's something wrong.
[00:42:29.880 --> 00:42:32.240]   Inverted ritual for int.
[00:42:32.240 --> 00:42:33.760]   Return int proc.
[00:42:33.760 --> 00:42:34.800]   So there's-- all right.
[00:42:35.200 --> 00:42:38.200]   I actually just tested the DDP code just before this.
[00:42:38.200 --> 00:42:42.920]   So it looks like there's some error somewhere.
[00:42:42.920 --> 00:42:45.120]   It's basically in the launcher.
[00:42:45.120 --> 00:42:45.880]   Let me see.
[00:42:45.880 --> 00:42:47.720]   I've got my two GPUs.
[00:42:47.720 --> 00:42:49.800]   But--
[00:42:49.800 --> 00:42:53.200]   I think you have to tell us the number of GPUs
[00:42:53.200 --> 00:42:57.880]   because you have $1 at the beginning for numproc.
[00:42:57.880 --> 00:42:59.400]   Yeah, that takes that from--
[00:42:59.400 --> 00:43:00.480]   oh, yes, you're right.
[00:43:00.480 --> 00:43:01.800]   I just have to say--
[00:43:01.800 --> 00:43:03.240]   I just have to do that.
[00:43:03.240 --> 00:43:03.920]   Yeah, yeah.
[00:43:03.920 --> 00:43:06.040]   Thank you.
[00:43:06.040 --> 00:43:08.440]   This is what happens when you're trying to do things live.
[00:43:08.440 --> 00:43:14.920]   So anyway, I just want to share how that looks like.
[00:43:14.920 --> 00:43:18.320]   So we have-- this is just without the convenience
[00:43:18.320 --> 00:43:20.440]   of Accelerate right now.
[00:43:20.440 --> 00:43:23.400]   But you'll see how things will be printed multiple times
[00:43:23.400 --> 00:43:26.000]   or GPUs being utilized, but things
[00:43:26.000 --> 00:43:28.720]   would be being printed multiple times.
[00:43:28.720 --> 00:43:32.200]   Or the loss you have to really look after
[00:43:32.200 --> 00:43:34.440]   and the script underneath looks really complicated.
[00:43:34.440 --> 00:43:36.120]   So when you can see the logs, that's
[00:43:36.120 --> 00:43:38.440]   how those logs look like.
[00:43:38.440 --> 00:43:40.840]   Even if I'm just using a simple print
[00:43:40.840 --> 00:43:43.000]   and it's being printed twice or thrice,
[00:43:43.000 --> 00:43:45.040]   and even the progress bars are multiple times,
[00:43:45.040 --> 00:43:48.000]   and all that stuff is really a bit of a mess.
[00:43:48.000 --> 00:43:51.760]   But whereas if I use the same thing with Accelerate,
[00:43:51.760 --> 00:43:54.560]   I can just use my launch--
[00:43:54.560 --> 00:43:57.440]   I don't have to worry about that special launch command.
[00:43:57.440 --> 00:44:00.720]   And I can just say Accelerate launch train Accelerate.py.
[00:44:00.720 --> 00:44:01.960]   And that does the same thing.
[00:44:01.960 --> 00:44:04.440]   It's still using distributed data parallel
[00:44:04.440 --> 00:44:07.520]   and it's using multiple GPUs.
[00:44:07.520 --> 00:44:10.240]   But this time, it's going to be much nicer in a way
[00:44:10.240 --> 00:44:12.440]   that there's going to be one training loop,
[00:44:12.440 --> 00:44:13.840]   there's going to be one eval loop,
[00:44:13.840 --> 00:44:15.680]   and there's going to be one print statement.
[00:44:15.680 --> 00:44:20.440]   So I just want to share how, Sylvain, your work has really
[00:44:20.440 --> 00:44:23.480]   made our works a lot easier.
[00:44:23.480 --> 00:44:25.800]   And I'm also using this for almost all the Kaggle
[00:44:25.800 --> 00:44:27.280]   computations or anything that I do.
[00:44:27.280 --> 00:44:28.480]   So that's much neater.
[00:44:28.480 --> 00:44:32.000]   And that really is a small line of code,
[00:44:32.000 --> 00:44:36.360]   but it makes a difference in all those different areas
[00:44:36.360 --> 00:44:37.360]   that we touch.
[00:44:37.360 --> 00:44:40.280]   So it would be lovely if you could share
[00:44:40.280 --> 00:44:43.320]   what goes on in the notebook launcher,
[00:44:43.320 --> 00:44:48.120]   or what exactly some insights into distributed data parallel
[00:44:48.120 --> 00:44:51.440]   and how the design came about and what
[00:44:51.440 --> 00:44:54.680]   you had to do to then come up with a library of Accelerate
[00:44:54.680 --> 00:44:57.440]   and the convenience that it is.
[00:44:57.440 --> 00:45:06.800]   Sure, let me just pull the source code.
[00:45:06.800 --> 00:45:08.040]   I lost my VSCode, sorry.
[00:45:08.040 --> 00:45:09.400]   I have to find it back.
[00:45:09.400 --> 00:45:20.400]   And then share my screen.
[00:45:27.160 --> 00:45:31.160]   How do I make this a little bit bigger for you?
[00:45:31.160 --> 00:45:32.680]   Do you know?
[00:45:32.680 --> 00:45:35.360]   I haven't presented VSCode before.
[00:45:35.360 --> 00:45:37.080]   I think you just press Control plus
[00:45:37.080 --> 00:45:39.640]   if you're on a Windows or Command plus, and it will zoom in.
[00:45:39.640 --> 00:45:40.280]   Perfect.
[00:45:40.280 --> 00:45:43.120]   Thank you so much.
[00:45:43.120 --> 00:45:47.280]   So this is-- so I mean the source code of the Accelerate
[00:45:47.280 --> 00:45:49.600]   repo.
[00:45:49.600 --> 00:45:54.640]   So there is really one thing I really wanted to show you,
[00:45:54.640 --> 00:45:56.440]   and then I can show anything that's
[00:45:56.440 --> 00:45:58.000]   related to any questions.
[00:45:58.000 --> 00:46:04.240]   But the central piece of the library
[00:46:04.240 --> 00:46:08.320]   is because, yeah, let's go maybe look at what happens when
[00:46:08.320 --> 00:46:10.160]   we do an Accelerate.prepare.
[00:46:10.160 --> 00:46:13.320]   So that's, as I said, that's the main entry point
[00:46:13.320 --> 00:46:16.440]   of using the library.
[00:46:16.440 --> 00:46:18.440]   So when you do Accelerate.prepare,
[00:46:18.440 --> 00:46:20.720]   it's very some stuff, like I said,
[00:46:20.720 --> 00:46:24.400]   like TPU should fix optimizer that's
[00:46:24.400 --> 00:46:28.680]   related to if you define your model first,
[00:46:28.680 --> 00:46:30.080]   and then your optimizer, and then
[00:46:30.080 --> 00:46:31.640]   set your model on the device.
[00:46:31.640 --> 00:46:34.320]   There's going to be a mess happening on TPUs,
[00:46:34.320 --> 00:46:37.760]   and this is Accelerate trying to fix that mess.
[00:46:37.760 --> 00:46:41.560]   But yeah, one thing that happens is when you do Accelerate,
[00:46:41.560 --> 00:46:43.400]   it's just returning the same thing
[00:46:43.400 --> 00:46:47.880]   and passing everything you gave to the private prepare one
[00:46:47.880 --> 00:46:48.480]   method.
[00:46:48.480 --> 00:46:50.080]   And that's a private prepare one method
[00:46:50.080 --> 00:46:53.160]   is delegating the data loader to prepare the data loader,
[00:46:53.160 --> 00:46:55.000]   the model to prepare model, and the optimizer
[00:46:55.000 --> 00:46:56.360]   to prepare optimizer.
[00:46:56.360 --> 00:46:59.560]   And I won't go over prepare model or prepare optimizer
[00:46:59.560 --> 00:47:01.640]   because those are reasonably simple.
[00:47:01.640 --> 00:47:04.320]   It's just like wrapping the model in the proper container
[00:47:04.320 --> 00:47:06.840]   or wrapping optimizer in the proper container.
[00:47:06.840 --> 00:47:10.240]   But the prepare data loader is where all the complexity is
[00:47:10.240 --> 00:47:11.000]   there.
[00:47:11.000 --> 00:47:13.160]   As I said during my first presentation,
[00:47:13.160 --> 00:47:15.160]   the key thing when you're on this training
[00:47:15.160 --> 00:47:17.000]   is making sure that the randomness is
[00:47:17.000 --> 00:47:19.240]   the same on all your processes.
[00:47:19.240 --> 00:47:20.760]   And that's the thing that's the--
[00:47:20.760 --> 00:47:23.240]   I mean, in my opinion, that's the other to fix.
[00:47:23.240 --> 00:47:25.400]   And it's always a source of multiple bugs
[00:47:25.400 --> 00:47:28.000]   that's super hard to track.
[00:47:28.000 --> 00:47:32.240]   And so it's all realized deep inside
[00:47:32.240 --> 00:47:37.160]   into this function called synchronized RNG state, which
[00:47:37.160 --> 00:47:38.400]   is going to--
[00:47:38.400 --> 00:47:41.040]   it takes several kinds of RNG.
[00:47:41.040 --> 00:47:45.160]   So it can synchronize a Torch RNG state, a CUDA RNG state,
[00:47:45.160 --> 00:47:49.120]   the Accelerate RNG states, or a specific generator RNG state.
[00:47:49.120 --> 00:47:52.760]   So RNG stands for random number generator.
[00:47:52.760 --> 00:47:56.600]   And it's all your computer is making,
[00:47:56.600 --> 00:47:59.120]   pretending to do random numbers because it can't actually
[00:47:59.120 --> 00:48:00.760]   make real random numbers.
[00:48:00.760 --> 00:48:04.160]   So there is a thing called a random number generator.
[00:48:04.160 --> 00:48:06.720]   And you can make sure that two random generator are
[00:48:06.720 --> 00:48:08.840]   going to output the same random numbers if you
[00:48:08.840 --> 00:48:10.840]   give them the same state.
[00:48:10.840 --> 00:48:13.680]   So this is what this function is doing.
[00:48:13.680 --> 00:48:18.400]   It's taking the random generator state from everywhere
[00:48:18.400 --> 00:48:19.680]   and synchronizing them.
[00:48:19.680 --> 00:48:22.680]   So you can see, for instance, if you're familiar with DDP,
[00:48:22.680 --> 00:48:24.960]   it's using the broadcast function
[00:48:24.960 --> 00:48:31.800]   to send the RNG state from the process 0
[00:48:31.800 --> 00:48:35.880]   to all the other processes.
[00:48:35.880 --> 00:48:42.960]   And same for the other kind of distributed devices.
[00:48:42.960 --> 00:48:47.000]   Sorry for the wait to cut you, but even I have a question.
[00:48:47.000 --> 00:48:47.880]   Yeah.
[00:48:47.880 --> 00:48:50.040]   How is this different from setting a seed?
[00:48:50.040 --> 00:48:52.240]   Why can't we just set a seed?
[00:48:52.240 --> 00:48:57.200]   Or you know how we set seeds in Python.
[00:48:57.200 --> 00:48:58.720]   And how is this different?
[00:48:58.720 --> 00:49:03.120]   And why do we have to go in depth of range and basically
[00:49:03.120 --> 00:49:04.520]   all of this function?
[00:49:04.520 --> 00:49:06.240]   So this is a very good question.
[00:49:06.240 --> 00:49:09.840]   So when you set the seed, the thing is--
[00:49:09.840 --> 00:49:14.400]   so two things.
[00:49:14.400 --> 00:49:19.560]   When setting the seed, I would have to--
[00:49:19.560 --> 00:49:22.160]   I mean, the user would have to actually add a new line of code
[00:49:22.160 --> 00:49:23.200]   to set the seed.
[00:49:23.200 --> 00:49:26.480]   When I'm doing this, it's completely
[00:49:26.480 --> 00:49:29.160]   inintrusive because it actually pulls the random generator
[00:49:29.160 --> 00:49:31.880]   state and then make them all equal across all the processes.
[00:49:31.880 --> 00:49:33.720]   So the user doesn't have to worry
[00:49:33.720 --> 00:49:35.200]   about adding a new line of code.
[00:49:35.200 --> 00:49:37.200]   But then more than that, if you set--
[00:49:37.200 --> 00:49:41.080]   for instance, if you set the seed,
[00:49:41.080 --> 00:49:43.880]   you are going to forget everything
[00:49:43.880 --> 00:49:44.800]   that happens before.
[00:49:44.800 --> 00:49:47.120]   So if each time you want to synchronize your random
[00:49:47.120 --> 00:49:49.600]   generator states, instead of synchronizing them,
[00:49:49.600 --> 00:49:50.880]   you set a new seed for them.
[00:49:50.880 --> 00:49:52.600]   You set the same seed as before for them,
[00:49:52.600 --> 00:49:54.160]   you're kind of going to loop always
[00:49:54.160 --> 00:49:56.920]   with the same random number, which is something
[00:49:56.920 --> 00:49:57.840]   you don't really want.
[00:49:57.840 --> 00:49:59.480]   So for instance, if at each epoch
[00:49:59.480 --> 00:50:00.720]   I was setting the same seed, I would
[00:50:00.720 --> 00:50:02.120]   loop through the same randomness.
[00:50:02.120 --> 00:50:03.400]   That wouldn't really work.
[00:50:03.400 --> 00:50:05.000]   So then I would have to do something,
[00:50:05.000 --> 00:50:06.800]   which is what PyTorch does behind the scenes
[00:50:06.800 --> 00:50:08.160]   with a distributed sampler.
[00:50:08.160 --> 00:50:11.280]   Set the seed at each epoch by setting a fixed value
[00:50:11.280 --> 00:50:14.400]   plus the number of epochs so that it changes at each epoch.
[00:50:14.400 --> 00:50:16.320]   But it's kind of clumsy.
[00:50:16.320 --> 00:50:19.560]   Whereas here, if the user decided to set the seed,
[00:50:19.560 --> 00:50:20.960]   the seed is respected.
[00:50:20.960 --> 00:50:23.720]   What I'm just doing is that, yeah, at this stage,
[00:50:23.720 --> 00:50:25.640]   I'm taking the random number--
[00:50:25.640 --> 00:50:28.640]   the state of the random number generator in process 0
[00:50:28.640 --> 00:50:33.800]   and sending it to all the other random number generators.
[00:50:33.800 --> 00:50:38.280]   And one of the other key things is this generator thing here.
[00:50:38.280 --> 00:50:42.280]   Because for PyTorch, you have two ways of setting the seed.
[00:50:42.280 --> 00:50:44.720]   Either you set the seed for the whole PyTorch library,
[00:50:44.720 --> 00:50:48.920]   or I mean just the GPU or just the GPU or the Torch
[00:50:48.920 --> 00:50:50.400]   library on CPU.
[00:50:50.400 --> 00:50:54.520]   Or you set the seed of just a small random number generator.
[00:50:54.520 --> 00:50:59.560]   And this is what we use to make sure when we synchronize
[00:50:59.560 --> 00:51:00.880]   all that other data.
[00:51:00.880 --> 00:51:02.960]   Because if you look--
[00:51:02.960 --> 00:51:05.720]   let's go look at prepared data loader, which
[00:51:05.720 --> 00:51:09.200]   should be here.
[00:51:09.200 --> 00:51:15.200]   So what's happening is that the data loader, especially--
[00:51:15.200 --> 00:51:17.200]   so it appeared-- the generator thing
[00:51:17.200 --> 00:51:19.480]   appeared in PyTorch 1.5 or 1.6.
[00:51:19.480 --> 00:51:21.360]   I can't remember exactly, but around that time.
[00:51:21.360 --> 00:51:24.400]   And that's what is used now when you have a random sampler,
[00:51:24.400 --> 00:51:24.920]   for instance.
[00:51:24.920 --> 00:51:28.680]   A random sampler has its own random number generator.
[00:51:28.680 --> 00:51:31.920]   And this is the random number generator
[00:51:31.920 --> 00:51:37.680]   that's going to decide the new permutation of your training
[00:51:37.680 --> 00:51:39.240]   set at each new bug.
[00:51:39.240 --> 00:51:42.440]   And it's an internal generator, which is a very good thing.
[00:51:42.440 --> 00:51:44.160]   Because otherwise, if you set the seed
[00:51:44.160 --> 00:51:45.880]   to make sure you have the same randomness,
[00:51:45.880 --> 00:51:48.400]   you also get the same randomness for the data augmentation
[00:51:48.400 --> 00:51:49.200]   you're doing.
[00:51:49.200 --> 00:51:52.760]   So we want to actually synchronize the seeds just
[00:51:52.760 --> 00:51:54.080]   for that generator.
[00:51:54.080 --> 00:51:58.080]   So that all our processes use the same randomness
[00:51:58.080 --> 00:52:00.080]   to shuffle the training set.
[00:52:00.080 --> 00:52:02.720]   But then, the random number generator
[00:52:02.720 --> 00:52:04.160]   that is used for data augmentation,
[00:52:04.160 --> 00:52:06.240]   we want them to be different on each process.
[00:52:06.240 --> 00:52:08.880]   Because we want each process to apply a different kind
[00:52:08.880 --> 00:52:11.040]   of data augmentation.
[00:52:11.040 --> 00:52:16.800]   So by using this here, when I'm doing the synchronized RNGA
[00:52:16.800 --> 00:52:20.040]   states, I'm just synchronizing the state of the generator.
[00:52:20.040 --> 00:52:22.320]   Unless the user went out of their way
[00:52:22.320 --> 00:52:25.280]   to set here that they want something different,
[00:52:25.280 --> 00:52:28.680]   it's only going to set the synchronized seeds
[00:52:28.680 --> 00:52:30.560]   for this internal random generator.
[00:52:30.560 --> 00:52:32.800]   So the only randomness that's going to be the same
[00:52:32.800 --> 00:52:41.400]   is the shuffling for each data loader on each process.
[00:52:41.400 --> 00:52:43.120]   And this works, like I said before,
[00:52:43.120 --> 00:52:45.160]   if you have any kind of random samplers.
[00:52:45.160 --> 00:52:48.440]   As long as that random samplers relies on an internal generator
[00:52:48.440 --> 00:52:50.560]   like the random sampler from PyTorch,
[00:52:50.560 --> 00:52:52.400]   this is all going to work exactly the same.
[00:52:52.400 --> 00:52:54.160]   And you can launch a distributed training
[00:52:54.160 --> 00:52:56.600]   with that random number generator.
[00:52:56.600 --> 00:52:57.960]   And it's all going to work.
[00:52:57.960 --> 00:52:59.880]   So yeah, that's the main thing that's happening
[00:52:59.880 --> 00:53:01.040]   behind prepared data loader.
[00:53:01.040 --> 00:53:03.800]   The other thing is the sharding.
[00:53:03.800 --> 00:53:05.880]   Like I said before, it works either
[00:53:05.880 --> 00:53:09.200]   for an iterable data set or for a data set, regular data set,
[00:53:09.200 --> 00:53:11.240]   which is done with a batch sampler.
[00:53:11.240 --> 00:53:14.240]   This is the thing that, for instance,
[00:53:14.240 --> 00:53:17.960]   takes your data 0, 1, up until 7.
[00:53:17.960 --> 00:53:20.080]   And it says, for instance, the first process
[00:53:20.080 --> 00:53:21.440]   is going to have 0, 1, 2, 3.
[00:53:21.440 --> 00:53:24.320]   And the second process is going to have 4, 5, 6, 7.
[00:53:24.320 --> 00:53:27.080]   So those are the two main things behind the prepared data
[00:53:27.080 --> 00:53:30.840]   loader, which is where the bulk of the work of the Accelerate
[00:53:30.840 --> 00:53:36.280]   library is really inside prepared data loader method.
[00:53:36.280 --> 00:53:39.040]   The rest is more like having if and then else statement.
[00:53:39.040 --> 00:53:41.320]   And if we're on GPU, we put it on that wrapper.
[00:53:41.320 --> 00:53:43.360]   If you're on CPU, we put it in that wrapper.
[00:53:43.360 --> 00:53:45.520]   If you're on GPU, we put the model on that wrapper,
[00:53:45.520 --> 00:53:46.960]   et cetera, et cetera.
[00:53:46.960 --> 00:53:47.920]   It's easier to manage.
[00:53:47.920 --> 00:53:49.760]   It's more like the verbose code because there
[00:53:49.760 --> 00:53:54.880]   are lots of tests, but not anything really interesting
[00:53:54.880 --> 00:53:56.800]   to look at.
[00:53:56.800 --> 00:53:59.360]   So how much of an effort did it go?
[00:53:59.360 --> 00:54:02.080]   And how were the design decisions made?
[00:54:02.080 --> 00:54:03.960]   Is that through experience?
[00:54:03.960 --> 00:54:07.200]   And you found these are the problems users were facing?
[00:54:07.200 --> 00:54:08.560]   And straight after this question,
[00:54:08.560 --> 00:54:10.520]   we'll also go into the questions from--
[00:54:10.520 --> 00:54:12.280]   we have some questions on the report.
[00:54:12.280 --> 00:54:15.400]   So I'll just ask, how much of an effort is it really to--
[00:54:15.400 --> 00:54:18.560]   how much time goes into building something like an Accelerate?
[00:54:18.560 --> 00:54:21.520]   And if you could take us through a story
[00:54:21.520 --> 00:54:26.640]   or how things really went about.
[00:54:26.640 --> 00:54:29.080]   So the main story behind Accelerate
[00:54:29.080 --> 00:54:32.880]   is that when I arrived at HackingFace
[00:54:32.880 --> 00:54:34.920]   in the first few months, I was working a lot
[00:54:34.920 --> 00:54:39.040]   on the examples, which were all brought by a trainer class,
[00:54:39.040 --> 00:54:42.280]   which, like I said before, is super useful for beginners
[00:54:42.280 --> 00:54:44.400]   who don't really want to dive into the training loop
[00:54:44.400 --> 00:54:46.280]   or data scientists that just want, yeah, train the model.
[00:54:46.280 --> 00:54:49.720]   I don't care how you do it, but just train and fine tune it.
[00:54:49.720 --> 00:54:53.320]   But so Tom, our CSO, was like, I really hate that trainer.
[00:54:53.320 --> 00:54:55.400]   I want to be able to write my own training loop.
[00:54:55.400 --> 00:54:58.720]   Can you build a lighter trainer that will still
[00:54:58.720 --> 00:55:01.360]   let me write any training loop I want?
[00:55:01.360 --> 00:55:03.320]   Like, it sounds super complicated.
[00:55:03.320 --> 00:55:04.880]   I'm not sure that's possible.
[00:55:04.880 --> 00:55:07.680]   But still, I started to look into it.
[00:55:07.680 --> 00:55:11.680]   And so, yeah, I think it was last November, November,
[00:55:11.680 --> 00:55:14.040]   December mostly, where I worked.
[00:55:14.040 --> 00:55:17.800]   So the easy part that I knew was super easy to do and possible
[00:55:17.800 --> 00:55:20.200]   was the role, like I said, model and optimizer.
[00:55:20.200 --> 00:55:21.880]   It's, yeah, you put it in that wrapper,
[00:55:21.880 --> 00:55:24.400]   so that's easy to have an API that does that
[00:55:24.400 --> 00:55:26.560]   without being super intrusive.
[00:55:26.560 --> 00:55:29.280]   It was the whole data thing that was boring me.
[00:55:29.280 --> 00:55:31.600]   So it took me, yeah, one month, one month and a half
[00:55:31.600 --> 00:55:34.040]   to arrive at what I just presented in terms of code,
[00:55:34.040 --> 00:55:35.920]   like the whole sharding thing, and then the whole
[00:55:35.920 --> 00:55:40.080]   synchronizing random number generators
[00:55:40.080 --> 00:55:44.160]   to make sure that it was possible to actually do that.
[00:55:44.160 --> 00:55:47.400]   And then the design decision were mostly motivated by,
[00:55:47.400 --> 00:55:48.720]   yeah, let's have--
[00:55:48.720 --> 00:55:51.360]   we want the least amount of lines of code
[00:55:51.360 --> 00:55:52.720]   the user has to change.
[00:55:52.720 --> 00:55:56.320]   And we've got code still running as perfectly
[00:55:56.320 --> 00:55:58.080]   without lying to them.
[00:55:58.080 --> 00:56:00.240]   Like, this is-- with those changes,
[00:56:00.240 --> 00:56:04.440]   it actually runs on the TPU or on the multiple GPUs.
[00:56:04.440 --> 00:56:05.400]   That's brilliant.
[00:56:05.400 --> 00:56:06.920]   Thanks for that background.
[00:56:06.920 --> 00:56:08.600]   I think it really makes me fall in love
[00:56:08.600 --> 00:56:11.600]   with Hugging Tricks Accelerate even more,
[00:56:11.600 --> 00:56:13.600]   knowing the background of the story.
[00:56:13.600 --> 00:56:16.360]   So we'll go quickly into some questions.
[00:56:16.360 --> 00:56:18.600]   I think Viril has a question about how
[00:56:18.600 --> 00:56:22.640]   to launch a script on a particular GPU ID.
[00:56:22.640 --> 00:56:24.160]   On a what?
[00:56:24.160 --> 00:56:26.040]   On a particular GPU ID.
[00:56:26.040 --> 00:56:28.280]   So for that, there is no--
[00:56:28.280 --> 00:56:30.440]   I mean, that's something we could add in the future.
[00:56:30.440 --> 00:56:33.520]   But for now, we rely on the CUDA visible devices
[00:56:33.520 --> 00:56:35.760]   via environment variable, like PyTorch.
[00:56:35.760 --> 00:56:40.040]   So if you just set CUDA_VISIBLE_DEVICES=
[00:56:40.040 --> 00:56:43.320]   the ID of your GPU, and then your command,
[00:56:43.320 --> 00:56:46.480]   you can have your training running on specific--
[00:56:46.480 --> 00:56:48.560]   so if you want, for instance, if you have four GPUs
[00:56:48.560 --> 00:56:52.400]   and you want to use GPU 1 and 2, you just say 1, 2,
[00:56:52.400 --> 00:56:53.240]   in that variable.
[00:56:53.240 --> 00:56:56.600]   And then your script is going to run on those two GPUs.
[00:56:56.600 --> 00:56:57.320]   Excellent.
[00:56:57.320 --> 00:56:59.640]   Do you mind if I quickly share my screen just for the--
[00:56:59.640 --> 00:57:00.280]   Oh, yeah, yeah.
[00:57:00.280 --> 00:57:00.800]   Sorry.
[00:57:00.800 --> 00:57:02.720]   --so everybody knows where I'm--
[00:57:02.720 --> 00:57:04.480]   Can you stop sharing?
[00:57:04.480 --> 00:57:05.160]   That's OK.
[00:57:05.160 --> 00:57:07.320]   I'll go to the--
[00:57:07.320 --> 00:57:09.160]   OK, here we are.
[00:57:09.160 --> 00:57:11.000]   So then another question is--
[00:57:11.000 --> 00:57:13.240]   I'll zoom in a little bit.
[00:57:13.240 --> 00:57:15.480]   Is model weights saving and loading
[00:57:15.480 --> 00:57:17.760]   same in the case of Accelerate?
[00:57:17.760 --> 00:57:20.160]   If I'm using multi-GPU/TPU training,
[00:57:20.160 --> 00:57:23.040]   do I have to make any changes to save or load weights?
[00:57:23.040 --> 00:57:24.480]   Yeah, that's an excellent question.
[00:57:24.480 --> 00:57:26.200]   And yes, you do have to make changes,
[00:57:26.200 --> 00:57:30.040]   because you have to make changes in distributed data parallel
[00:57:30.040 --> 00:57:32.520]   or in TPU training.
[00:57:32.520 --> 00:57:37.000]   So again, you have to learn one or two new functions.
[00:57:37.000 --> 00:57:41.280]   The main things are you have to use accelerator.save instead
[00:57:41.280 --> 00:57:44.680]   of torch.save to save your state dictionary.
[00:57:44.680 --> 00:57:48.240]   And the second is that you will probably want to--
[00:57:48.240 --> 00:57:52.920]   the second is that you have to unwrap your model,
[00:57:52.920 --> 00:57:56.200]   because as we saw in our first example,
[00:57:56.200 --> 00:57:59.280]   when you have a distributed training on multiple GPUs
[00:57:59.280 --> 00:58:01.560]   or TPU, your model is actually wrapped in something
[00:58:01.560 --> 00:58:03.160]   called distributed data parallel.
[00:58:03.160 --> 00:58:04.920]   And then it adds-- your state dictionary
[00:58:04.920 --> 00:58:09.360]   is a bit messy, because it adds a module in the tree.
[00:58:09.360 --> 00:58:13.000]   So there is an accelerator.unwrapModel method.
[00:58:13.000 --> 00:58:14.600]   So you-- yeah.
[00:58:14.600 --> 00:58:16.440]   If you want to go through the documentation,
[00:58:16.440 --> 00:58:17.760]   maybe, Amman, on your screen.
[00:58:17.760 --> 00:58:19.440]   Yeah, I'll go through that.
[00:58:19.440 --> 00:58:21.480]   There is a small section of the documentation
[00:58:21.480 --> 00:58:26.680]   that's showing the two lines of code that you have to change.
[00:58:26.680 --> 00:58:30.880]   If you go in the quick tour, I think it's going to be there.
[00:58:34.040 --> 00:58:36.560]   And then you should have other caveats on the left,
[00:58:36.560 --> 00:58:40.080]   or you can scroll.
[00:58:40.080 --> 00:58:44.960]   And then in this other caveat, you have saving model.
[00:58:44.960 --> 00:58:46.360]   And if you go there, you can see.
[00:58:46.360 --> 00:58:48.960]   So this is-- there is an accelerator.waitForEveryone.
[00:58:48.960 --> 00:58:52.600]   That's to make sure that all the processes are finished
[00:58:52.600 --> 00:58:54.560]   and wait for each other at this point,
[00:58:54.560 --> 00:58:58.760]   just in case some of the process still changes something
[00:58:58.760 --> 00:58:59.640]   to the model.
[00:58:59.640 --> 00:59:02.800]   The unwrapModel method that's removing,
[00:59:02.800 --> 00:59:04.480]   but it's kind of like unwrapModel.
[00:59:04.480 --> 00:59:06.760]   It's going to be the same thing as your original model.
[00:59:06.760 --> 00:59:09.240]   And then you use accelerator.save instead of
[00:59:09.240 --> 00:59:09.960]   torch.save.
[00:59:09.960 --> 00:59:13.720]   Perfect.
[00:59:13.720 --> 00:59:18.000]   And we have another question.
[00:59:18.000 --> 00:59:21.560]   What about training on GPUs across multiple machines,
[00:59:21.560 --> 00:59:24.720]   each with single GPU or mixed?
[00:59:24.720 --> 00:59:26.160]   So this works exactly the same.
[00:59:26.160 --> 00:59:28.160]   You don't have to change anything in your code.
[00:59:28.160 --> 00:59:29.600]   It's only in the launching command
[00:59:29.600 --> 00:59:30.880]   that it's going to change.
[00:59:30.880 --> 00:59:32.840]   And so if you go through accelerator,
[00:59:32.840 --> 00:59:36.520]   launching command is either with bytorch.distributed.launch.
[00:59:36.520 --> 00:59:39.080]   Or if you use accelerate.config, it's
[00:59:39.080 --> 00:59:41.640]   going to ask you on each machine how many GPUs you have,
[00:59:41.640 --> 00:59:42.960]   how many machines you have.
[00:59:42.960 --> 00:59:45.080]   And you can just put everything--
[00:59:45.080 --> 00:59:49.360]   I can show that off the accelerate.config.
[00:59:49.360 --> 00:59:49.880]   So if you--
[00:59:49.880 --> 00:59:55.200]   If I go accelerate.config and I press Enter,
[00:59:55.200 --> 00:59:57.080]   is this way all the settings go, Sylvain?
[00:59:57.080 --> 00:59:57.760]   Is that correct?
[00:59:57.760 --> 00:59:59.200]   I go this machine for now?
[00:59:59.200 --> 01:00:00.400]   You go this machine for now.
[01:00:00.400 --> 01:00:02.440]   So AWS is also a new experimental feature
[01:00:02.440 --> 01:00:04.040]   that's going to launch your training.
[01:00:04.040 --> 01:00:06.920]   Actually, it's going to spawn a new instance on AWS
[01:00:06.920 --> 01:00:08.920]   using AWS SageMaker.
[01:00:08.920 --> 01:00:11.640]   Train there, and when your training is finished,
[01:00:11.640 --> 01:00:12.920]   turn off the instance.
[01:00:12.920 --> 01:00:16.880]   So that's an experimental feature you can use as well.
[01:00:16.880 --> 01:00:19.480]   And so you say which type of machine are you using.
[01:00:19.480 --> 01:00:22.920]   So it would be multi-GPU here.
[01:00:22.920 --> 01:00:24.520]   And then number of different machines.
[01:00:24.520 --> 01:00:26.040]   So let's say you have two machines.
[01:00:26.040 --> 01:00:27.560]   You can say two if you are just one.
[01:00:27.560 --> 01:00:28.280]   Say one.
[01:00:28.280 --> 01:00:31.360]   And then you have to say the rank of this specific machine.
[01:00:31.360 --> 01:00:33.400]   You will have to run accelerate.config on the two
[01:00:33.400 --> 01:00:35.680]   machines, obviously, to tell which one is zero,
[01:00:35.680 --> 01:00:39.080]   which one is one.
[01:00:39.080 --> 01:00:43.600]   And then the IP address to communicate.
[01:00:43.600 --> 01:00:44.600]   I'm just going to press--
[01:00:44.600 --> 01:00:45.320]   I'll just say--
[01:00:45.320 --> 01:00:45.840]   Yeah, yeah.
[01:00:45.840 --> 01:00:46.920]   --abc.
[01:00:46.920 --> 01:00:47.800]   You can say anything.
[01:00:47.800 --> 01:00:50.320]   It's not going to--
[01:00:50.320 --> 01:00:53.120]   And the port, if you want, it's not necessarily necessary.
[01:00:53.120 --> 01:00:54.740]   So those are all the arguments that you
[01:00:54.740 --> 01:00:56.320]   would have to pass torgetry.launch.
[01:00:56.320 --> 01:00:59.760]   But it's actually-- it's not an argument you have to remember.
[01:00:59.760 --> 01:01:01.240]   It's a question you have to answer.
[01:01:01.240 --> 01:01:03.840]   So I find it's way easier that way.
[01:01:03.840 --> 01:01:07.400]   So you can use experimental deep speed on top of that.
[01:01:07.400 --> 01:01:09.680]   You can press Enter or voice.
[01:01:09.680 --> 01:01:11.360]   Deep speed is not-- so that's OK.
[01:01:11.360 --> 01:01:13.920]   And yeah, then there was the last question of if you
[01:01:13.920 --> 01:01:14.960]   want to use FP16.
[01:01:14.960 --> 01:01:16.760]   I remember the last question because that's
[01:01:16.760 --> 01:01:18.520]   the one that comes after deep speed.
[01:01:18.520 --> 01:01:19.480]   It's like, if you want to use--
[01:01:19.480 --> 01:01:20.000]   Yeah, yeah.
[01:01:20.000 --> 01:01:22.480]   So if you use FP16, you would have the question,
[01:01:22.480 --> 01:01:23.720]   do you want to use FP16?
[01:01:23.720 --> 01:01:25.560]   And so, yeah, you have an error for this speed
[01:01:25.560 --> 01:01:27.440]   if you don't have the speed installed.
[01:01:27.440 --> 01:01:28.720]   Sorry.
[01:01:28.720 --> 01:01:29.240]   That's OK.
[01:01:29.240 --> 01:01:30.520]   And then that's how easy it is.
[01:01:30.520 --> 01:01:32.760]   And then you can just say accelerate launch.
[01:01:32.760 --> 01:01:33.440]   Launch, yeah.
[01:01:33.440 --> 01:01:36.600]   And it will just launch the script based on that config.
[01:01:36.600 --> 01:01:39.200]   And if you want to have several configuration in your machine,
[01:01:39.200 --> 01:01:40.240]   it's completely possible.
[01:01:40.240 --> 01:01:43.160]   So the config actually is just taught in your cache.
[01:01:43.160 --> 01:01:45.520]   So if you go into your own cache,
[01:01:45.520 --> 01:01:47.880]   so it should be in your own directory, .cache.
[01:01:51.600 --> 01:02:00.120]   .cache, and then again, face, and then accelerate.
[01:02:00.120 --> 01:02:02.840]   So here, you can see you have your default config that
[01:02:02.840 --> 01:02:05.040]   has been generated by this command.
[01:02:05.040 --> 01:02:07.160]   And you can just copy that config
[01:02:07.160 --> 01:02:08.440]   and create a new config file.
[01:02:08.440 --> 01:02:10.840]   And you have-- when you do accelerate launch,
[01:02:10.840 --> 01:02:14.840]   you can specify with the --config file,
[01:02:14.840 --> 01:02:16.840]   you can specify the path to another config file.
[01:02:16.840 --> 01:02:18.480]   So if you have a machine with four GPUs
[01:02:18.480 --> 01:02:20.720]   and you want to have a config where you use all four,
[01:02:20.720 --> 01:02:22.160]   or config when you use two of them,
[01:02:22.160 --> 01:02:25.160]   it's completely possible and super easy to navigate.
[01:02:25.160 --> 01:02:27.120]   And you don't have to use accelerate config
[01:02:27.120 --> 01:02:29.400]   then every time, because then it's just--
[01:02:29.400 --> 01:02:33.480]   you can just pass in the config command, which is great.
[01:02:33.480 --> 01:02:35.040]   I didn't know about this.
[01:02:35.040 --> 01:02:37.080]   This is awesome.
[01:02:37.080 --> 01:02:39.720]   OK, we'll move on to more questions.
[01:02:39.720 --> 01:02:41.960]   Can hugging face accelerate support
[01:02:41.960 --> 01:02:44.080]   some kind of swarm training?
[01:02:44.080 --> 01:02:45.160]   I don't know what that is.
[01:02:45.160 --> 01:02:47.160]   I'm assuming, Sylvain, you do.
[01:02:47.160 --> 01:02:52.360]   For example, if I got access to 100 TPU VM preemptible,
[01:02:52.360 --> 01:02:56.080]   how can I take advantage of these VM to train a huge model?
[01:02:56.080 --> 01:03:00.720]   And how can it handle if one VM is disconnected?
[01:03:00.720 --> 01:03:02.800]   I haven't tried any of those.
[01:03:02.800 --> 01:03:09.840]   So I don't know what swarm training is either.
[01:03:09.840 --> 01:03:15.280]   If you have 100 TPU VM, it's going to use them to train.
[01:03:15.280 --> 01:03:19.080]   That's with the TPU pod support.
[01:03:19.080 --> 01:03:24.160]   But it won't do anything special if one VM is disconnected.
[01:03:24.160 --> 01:03:25.440]   I mean, there's the same way.
[01:03:25.440 --> 01:03:27.280]   Your training doesn't checkpoint anything
[01:03:27.280 --> 01:03:29.640]   if you don't try to checkpoint anything.
[01:03:29.640 --> 01:03:32.680]   So my guess is that you would need
[01:03:32.680 --> 01:03:35.040]   to have in your training loop something that checkpoints
[01:03:35.040 --> 01:03:37.920]   regularly to be able to reload from there
[01:03:37.920 --> 01:03:40.320]   if something goes wrong.
[01:03:40.320 --> 01:03:41.960]   OK, we're already over.
[01:03:41.960 --> 01:03:43.720]   So just very conscious of time, we
[01:03:43.720 --> 01:03:46.960]   will take Morgan's question as the last one.
[01:03:46.960 --> 01:03:50.040]   Does Accelerate have all the deep speed functionality
[01:03:50.040 --> 01:03:52.120]   as in Transformers?
[01:03:52.120 --> 01:03:58.520]   So for now, it's limited to 0.2, 0.3, and 0.0 infinity,
[01:03:58.520 --> 01:04:03.160]   which is what the training parts in Transformers handles.
[01:04:03.160 --> 01:04:06.320]   But it gives us nothing to handle like initializing
[01:04:06.320 --> 01:04:08.440]   your model with deep speed.
[01:04:08.440 --> 01:04:10.320]   So there are several tricks you can
[01:04:10.320 --> 01:04:13.160]   use to make sure that you initialize it
[01:04:13.160 --> 01:04:14.800]   without taking too much CPU RAM.
[01:04:14.800 --> 01:04:18.080]   So there is nothing for that in Accelerate yet.
[01:04:18.080 --> 01:04:19.440]   Excellent.
[01:04:19.440 --> 01:04:21.840]   That's it for the questions then.
[01:04:21.840 --> 01:04:24.200]   Sylvain, is there anything else you wanted to present?
[01:04:24.200 --> 01:04:26.400]   Or if we missed anything or there's
[01:04:26.400 --> 01:04:31.200]   anything about Accelerate, some maybe caveat?
[01:04:31.200 --> 01:04:33.360]   I talked a lot about it, and you helped
[01:04:33.360 --> 01:04:35.640]   me figure out what I forgot.
[01:04:35.640 --> 01:04:38.560]   So I think I'm good on my side.
[01:04:38.560 --> 01:04:42.240]   Thanks a lot for having me to talk about that library.
[01:04:42.240 --> 01:04:43.720]   No, thank you, Sylvain.
[01:04:43.720 --> 01:04:45.640]   Hugging face Accelerate has been something
[01:04:45.640 --> 01:04:48.960]   that has saved many, many hours of my time of debugging
[01:04:48.960 --> 01:04:50.760]   and writing new code every time.
[01:04:50.760 --> 01:04:53.360]   So thanks for the library, and thanks for the awesome work
[01:04:53.360 --> 01:04:55.200]   that you've been doing.
[01:04:55.200 --> 01:04:57.840]   And really, really excited to have you today
[01:04:57.840 --> 01:04:59.840]   and appreciate your time.
[01:04:59.840 --> 01:05:01.280]   So thanks very much, Sylvain.
[01:05:01.280 --> 01:05:03.720]   And thanks, everybody, for joining.
[01:05:03.720 --> 01:05:05.520]   Yeah, thanks a lot again for having me,
[01:05:05.520 --> 01:05:06.920]   and thanks, everybody, for joining.
[01:05:06.920 --> 01:05:10.280]   [MUSIC PLAYING]
[01:05:10.280 --> 01:05:13.920]   [MUSIC PLAYING]
[01:05:13.960 --> 01:05:17.320]   [MUSIC PLAYING]
[01:05:17.320 --> 01:05:20.680]   [MUSIC PLAYING]
[01:05:21.240 --> 01:05:24.600]   [MUSIC PLAYING]
[01:05:24.600 --> 01:05:27.180]   (upbeat music)
[01:05:27.180 --> 01:05:28.280]   (upbeat music)

