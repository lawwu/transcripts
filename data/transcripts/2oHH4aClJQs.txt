
[00:00:00.000 --> 00:00:01.540]   A lot of people were saying like,
[00:00:01.540 --> 00:00:03.980]   oh, this whole idea of game theory, it's just nonsense.
[00:00:03.980 --> 00:00:05.480]   And if you really want to make money,
[00:00:05.480 --> 00:00:07.220]   you got to like look into the other person's eyes
[00:00:07.220 --> 00:00:10.640]   and read their soul and figure out what cards they have.
[00:00:10.640 --> 00:00:13.320]   But what happened was where we played our bot
[00:00:13.320 --> 00:00:15.140]   against four top heads up, no limit,
[00:00:15.140 --> 00:00:17.020]   hold them poker players.
[00:00:17.020 --> 00:00:19.460]   And the bot wasn't trying to adapt to them.
[00:00:19.460 --> 00:00:20.780]   It wasn't trying to exploit them.
[00:00:20.780 --> 00:00:22.780]   It wasn't trying to do these mind games.
[00:00:22.780 --> 00:00:25.220]   It was just trying to approximate the Nash equilibrium
[00:00:25.220 --> 00:00:26.260]   and it crushed them.
[00:00:28.660 --> 00:00:31.100]   The following is a conversation with Noah Brown,
[00:00:31.100 --> 00:00:32.740]   research scientist at FAIR,
[00:00:32.740 --> 00:00:35.500]   Facebook AI research group at Meta AI.
[00:00:35.500 --> 00:00:38.340]   He co-created the first AI system
[00:00:38.340 --> 00:00:40.600]   that achieved superhuman level performance
[00:00:40.600 --> 00:00:44.580]   in no limit Texas hold them, both heads up and multiplayer.
[00:00:44.580 --> 00:00:48.540]   And now recently he co-created an AI system
[00:00:48.540 --> 00:00:51.360]   that can strategically out negotiate humans
[00:00:51.360 --> 00:00:52.900]   using natural language
[00:00:52.900 --> 00:00:55.340]   in a popular board game called diplomacy,
[00:00:55.340 --> 00:00:58.760]   which is a war game that emphasizes negotiation.
[00:00:58.760 --> 00:01:02.460]   This is Alex Friedman podcast to support it.
[00:01:02.460 --> 00:01:04.900]   Please check out our sponsors in the description.
[00:01:04.900 --> 00:01:08.440]   And now dear friends, here's Noam Brown.
[00:01:08.440 --> 00:01:12.820]   You've been a lead on three amazing AI projects.
[00:01:12.820 --> 00:01:15.140]   So we've got Libratus that solved
[00:01:15.140 --> 00:01:17.540]   or at least achieved human level performance
[00:01:17.540 --> 00:01:19.740]   on no limit Texas hold them poker
[00:01:19.740 --> 00:01:22.140]   with two players, heads up.
[00:01:22.140 --> 00:01:26.140]   You got Pleribus that solved no limit Texas hold them poker
[00:01:26.140 --> 00:01:28.100]   with six players.
[00:01:28.100 --> 00:01:30.220]   And just now you have Cicero.
[00:01:30.220 --> 00:01:32.980]   These are all names of systems that solved
[00:01:32.980 --> 00:01:35.780]   or achieved human level performance
[00:01:35.780 --> 00:01:37.660]   on the game of diplomacy,
[00:01:37.660 --> 00:01:39.980]   which for people who don't know
[00:01:39.980 --> 00:01:42.140]   is a popular strategy board game.
[00:01:42.140 --> 00:01:46.980]   It was loved by JFK, John F. Kennedy and Henry Kissinger
[00:01:46.980 --> 00:01:51.980]   and many other big famous people in the decades since.
[00:01:52.100 --> 00:01:54.780]   So let's talk about poker and diplomacy today.
[00:01:54.780 --> 00:01:59.180]   First poker, what is the game of no limit Texas hold them?
[00:01:59.180 --> 00:02:00.460]   And how is it different from chess?
[00:02:00.460 --> 00:02:02.300]   - Well, no limit Texas hold them poker
[00:02:02.300 --> 00:02:05.100]   is the most popular variant of poker in the world.
[00:02:05.100 --> 00:02:06.540]   So, you know, you go to a casino,
[00:02:06.540 --> 00:02:08.420]   you play sit down at the poker table.
[00:02:08.420 --> 00:02:11.080]   The game that you're playing is no limit Texas hold them.
[00:02:11.080 --> 00:02:12.500]   If you watch movies about poker,
[00:02:12.500 --> 00:02:14.520]   like casino Royale or rounders,
[00:02:14.520 --> 00:02:15.360]   the game that they're playing
[00:02:15.360 --> 00:02:17.420]   is no limit Texas hold them poker.
[00:02:17.420 --> 00:02:20.940]   Now it's very different from limit hold them
[00:02:20.940 --> 00:02:23.380]   in that you can bet any amount of chips that you want.
[00:02:23.380 --> 00:02:26.060]   And so the stakes escalate really quickly.
[00:02:26.060 --> 00:02:28.660]   You start out with like one or $2 in the pot.
[00:02:28.660 --> 00:02:29.740]   And then by the end of the hand,
[00:02:29.740 --> 00:02:32.460]   you've got like $1,000 in there maybe.
[00:02:32.460 --> 00:02:34.980]   - So the option to increase the number very aggressively
[00:02:34.980 --> 00:02:36.340]   and very quickly is always there.
[00:02:36.340 --> 00:02:38.780]   - Right, the no limit aspect is there's no limits
[00:02:38.780 --> 00:02:39.980]   to how much you can bet.
[00:02:39.980 --> 00:02:42.220]   You know, in limit hold them,
[00:02:42.220 --> 00:02:43.420]   there's like $2 in the pot,
[00:02:43.420 --> 00:02:45.400]   you can only bet like $2.
[00:02:45.400 --> 00:02:47.900]   But if you got $10,000 in front of you,
[00:02:47.900 --> 00:02:50.500]   you're always welcome to put $10,000 into the pot.
[00:02:50.500 --> 00:02:52.780]   - So I've got a chance to hang out with Phil Helmuth
[00:02:52.780 --> 00:02:55.780]   who plays all these different variants of poker.
[00:02:55.780 --> 00:02:57.220]   And correct me if I'm wrong,
[00:02:57.220 --> 00:03:01.280]   but it seems like no limit rewards crazy
[00:03:01.280 --> 00:03:02.900]   versus the other ones rewards
[00:03:02.900 --> 00:03:05.220]   more kind of calculated strategy.
[00:03:05.220 --> 00:03:07.620]   Or no, because you're sort of looking
[00:03:07.620 --> 00:03:10.420]   from an analytic perspective,
[00:03:10.420 --> 00:03:14.980]   is strategy also rewarded in no limit Texas hold them?
[00:03:14.980 --> 00:03:17.220]   - I think both variants reward strategy.
[00:03:17.220 --> 00:03:20.220]   But I think what's different about no limit hold them
[00:03:20.220 --> 00:03:23.220]   is it's much easier to get jumpy.
[00:03:23.220 --> 00:03:26.580]   You know, you go in there thinking you're gonna play
[00:03:26.580 --> 00:03:28.900]   for like $100 or something.
[00:03:28.900 --> 00:03:31.100]   And suddenly there's like, you know, $1,000 in the pot.
[00:03:31.100 --> 00:03:32.540]   A lot of people can't handle that.
[00:03:32.540 --> 00:03:33.900]   - Can you define jumpy?
[00:03:33.900 --> 00:03:35.220]   - When you're playing poker,
[00:03:35.220 --> 00:03:37.300]   you always want to choose the action
[00:03:37.300 --> 00:03:39.420]   that's going to maximize your expected value.
[00:03:39.420 --> 00:03:41.260]   It's kind of like with investing, right?
[00:03:41.260 --> 00:03:42.620]   Like if you're ever in a situation
[00:03:42.620 --> 00:03:44.820]   where the amount of money that's at stake
[00:03:44.820 --> 00:03:49.240]   is going to have a material impact on your life,
[00:03:49.240 --> 00:03:51.860]   then you're gonna play in a more risk averse style.
[00:03:51.860 --> 00:03:53.780]   You know, if somebody makes a huge bet,
[00:03:53.780 --> 00:03:55.620]   you're gonna, if you're playing no limit hold them
[00:03:55.620 --> 00:03:57.540]   and somebody makes a huge bet,
[00:03:57.540 --> 00:03:58.900]   there might come a point where you're like,
[00:03:58.900 --> 00:04:00.780]   this is too much money for me to handle.
[00:04:00.780 --> 00:04:03.100]   Like I can't risk this amount.
[00:04:03.100 --> 00:04:05.540]   And that's what throws a lot of people off.
[00:04:05.540 --> 00:04:07.860]   So that's the big difference I think
[00:04:07.860 --> 00:04:09.900]   between no limit and limit.
[00:04:09.900 --> 00:04:11.420]   - What about on the action side
[00:04:11.420 --> 00:04:14.140]   when you're actually making that big bet?
[00:04:14.140 --> 00:04:15.460]   That's what I mean by crazy.
[00:04:15.460 --> 00:04:18.580]   I was trying to refer to the technical,
[00:04:18.580 --> 00:04:20.700]   the technical term of crazy,
[00:04:20.700 --> 00:04:24.460]   meaning use the big jump in the bet
[00:04:24.460 --> 00:04:26.380]   to completely throw off the other person
[00:04:26.380 --> 00:04:30.340]   in terms of their ability to reason optimally.
[00:04:30.340 --> 00:04:31.160]   - I think that's right.
[00:04:31.160 --> 00:04:34.500]   I think one of the key strategies in poker
[00:04:34.500 --> 00:04:38.020]   is to put the other person into an uncomfortable position.
[00:04:38.020 --> 00:04:40.860]   And if you're doing that, then you're playing poker well.
[00:04:40.860 --> 00:04:42.380]   And there's a lot of opportunities to do that
[00:04:42.380 --> 00:04:43.580]   in no limit hold them.
[00:04:43.580 --> 00:04:45.980]   You know, you can have like $50 in there,
[00:04:45.980 --> 00:04:47.860]   you throw in a thousand dollar bet,
[00:04:47.860 --> 00:04:51.020]   and you know, that's sometimes if you do it right,
[00:04:51.020 --> 00:04:53.200]   it puts the other person in a really tough spot.
[00:04:53.200 --> 00:04:56.380]   Now it's also possible that you make huge mistakes that way.
[00:04:56.380 --> 00:04:57.980]   And so it's really easy to lose a lot of money
[00:04:57.980 --> 00:05:00.740]   in no limit hold them if you don't know what you're doing.
[00:05:00.740 --> 00:05:02.540]   But there's a lot of upside potential too.
[00:05:02.540 --> 00:05:04.700]   - So when you build systems, AI systems
[00:05:04.700 --> 00:05:06.540]   that play these games, we'll talk about poker,
[00:05:06.540 --> 00:05:08.620]   we'll talk about diplomacy.
[00:05:08.620 --> 00:05:12.700]   Are you drawn in in part by the beauty of the game itself,
[00:05:12.700 --> 00:05:17.200]   AI aside, or is it to you primarily a fascinating
[00:05:17.880 --> 00:05:20.160]   problem set for the AI to solve?
[00:05:20.160 --> 00:05:21.840]   - I'm drawn in by the beauty of the game.
[00:05:21.840 --> 00:05:25.520]   When I started playing poker when I was in high school,
[00:05:25.520 --> 00:05:29.960]   and the idea to me that there is a correct,
[00:05:29.960 --> 00:05:32.440]   an objectively correct way of playing poker.
[00:05:32.440 --> 00:05:34.280]   And if you could figure out what that is,
[00:05:34.280 --> 00:05:38.220]   then you're making unlimited money basically.
[00:05:38.220 --> 00:05:41.160]   That's like a really fascinating concept to me.
[00:05:41.160 --> 00:05:44.200]   And so I was fascinated by the strategy of poker,
[00:05:44.200 --> 00:05:46.280]   even when I was like 16 years old.
[00:05:46.280 --> 00:05:47.440]   It wasn't until like much later
[00:05:47.440 --> 00:05:49.000]   that I actually worked on poker AIs.
[00:05:49.000 --> 00:05:51.580]   - So there was a sense that you can solve poker,
[00:05:51.580 --> 00:05:54.440]   like in the way you can solve chess, for example,
[00:05:54.440 --> 00:05:57.440]   or checkers, I believe checkers got solved, right?
[00:05:57.440 --> 00:05:59.440]   - Yeah, checkers is completely solved.
[00:05:59.440 --> 00:06:00.680]   - Optimal strategy. - Optimal strategy.
[00:06:00.680 --> 00:06:01.840]   It's impossible to beat the AI.
[00:06:01.840 --> 00:06:03.040]   - Yeah, and so in that same way,
[00:06:03.040 --> 00:06:05.800]   you could technically solve chess.
[00:06:05.800 --> 00:06:07.560]   - You could solve chess, you could solve poker.
[00:06:07.560 --> 00:06:08.800]   - You could solve poker.
[00:06:08.800 --> 00:06:12.400]   - So this gets into the concept of a Nash equilibrium.
[00:06:12.400 --> 00:06:14.200]   - So it is a Nash equilibrium.
[00:06:14.200 --> 00:06:19.080]   - Okay, so in any finite two-player zero-sum game,
[00:06:19.080 --> 00:06:21.960]   there is an optimal strategy that if you play it,
[00:06:21.960 --> 00:06:24.360]   you are guaranteed to not lose an expectation
[00:06:24.360 --> 00:06:26.600]   no matter what your opponent does.
[00:06:26.600 --> 00:06:29.960]   And this is kind of a radical concept to a lot of people,
[00:06:29.960 --> 00:06:31.880]   but it's true in chess, it's true in poker,
[00:06:31.880 --> 00:06:34.960]   it's true in any finite two-player zero-sum game.
[00:06:34.960 --> 00:06:36.880]   And to give some intuition for this,
[00:06:36.880 --> 00:06:39.020]   you can think of rock, paper, scissors.
[00:06:39.020 --> 00:06:41.800]   In rock, paper, scissors, if you randomly choose
[00:06:41.800 --> 00:06:43.320]   between throwing rock, paper, and scissors
[00:06:43.320 --> 00:06:44.560]   with equal probability,
[00:06:44.560 --> 00:06:46.740]   then no matter what your opponent does,
[00:06:46.740 --> 00:06:48.440]   you are not going to lose an expectation.
[00:06:48.440 --> 00:06:51.160]   You're not going to lose an expectation in the long run.
[00:06:51.160 --> 00:06:53.080]   Now, the same is true for poker.
[00:06:53.080 --> 00:06:54.440]   There exists some strategy,
[00:06:54.440 --> 00:06:56.360]   some really complicated strategy,
[00:06:56.360 --> 00:06:57.600]   that if you play that,
[00:06:57.600 --> 00:07:00.440]   you are guaranteed to not lose money in the long run.
[00:07:00.440 --> 00:07:01.880]   And I should say, this is for two-player poker.
[00:07:01.880 --> 00:07:03.280]   Six-player poker is a different story.
[00:07:03.280 --> 00:07:05.400]   - Yeah, it's a beautiful giant mess.
[00:07:05.400 --> 00:07:08.240]   When you say in expectation,
[00:07:08.240 --> 00:07:10.760]   you're guaranteed not to lose in expectation.
[00:07:10.760 --> 00:07:12.680]   What does in expectation mean?
[00:07:12.680 --> 00:07:14.240]   - Poker's a very high variance game.
[00:07:14.240 --> 00:07:15.520]   So you're going to have hands where you win,
[00:07:15.520 --> 00:07:16.480]   you're going to have hands with your lose.
[00:07:16.480 --> 00:07:18.220]   Even if you're playing the perfect strategy,
[00:07:18.220 --> 00:07:19.400]   you can't guarantee that you're going to win
[00:07:19.400 --> 00:07:20.680]   every single hand.
[00:07:20.680 --> 00:07:22.480]   But if you play for long enough,
[00:07:22.480 --> 00:07:25.040]   then you are guaranteed to at least break even,
[00:07:25.040 --> 00:07:27.600]   and in practice, probably win.
[00:07:27.600 --> 00:07:29.080]   - So that's in expectation,
[00:07:29.080 --> 00:07:32.240]   the size of your stack, generally speaking.
[00:07:32.240 --> 00:07:33.760]   Now, that doesn't include anything
[00:07:33.760 --> 00:07:36.080]   about the fact that you can go broke.
[00:07:36.080 --> 00:07:37.520]   It doesn't include any of those kinds
[00:07:37.520 --> 00:07:39.520]   of normal real-world limitations.
[00:07:39.520 --> 00:07:42.440]   You're talking in a theoretical world.
[00:07:42.440 --> 00:07:44.720]   What about the zero-sum aspect?
[00:07:44.720 --> 00:07:46.040]   How big of a constraint is that?
[00:07:46.040 --> 00:07:48.480]   How big of a constraint is finite?
[00:07:48.480 --> 00:07:51.800]   - So finite's not a huge constraint.
[00:07:51.800 --> 00:07:54.500]   So I mean, most games that you play are finite in size.
[00:07:54.500 --> 00:07:55.620]   It's also true, actually,
[00:07:55.620 --> 00:07:57.760]   that there exists this perfect strategy
[00:07:57.760 --> 00:07:59.360]   in many infinite games as well.
[00:07:59.360 --> 00:08:01.360]   Technically, the game has to be compact.
[00:08:01.360 --> 00:08:03.880]   There are some edge cases
[00:08:03.880 --> 00:08:05.200]   where you don't have a Nash equilibrium
[00:08:05.200 --> 00:08:06.720]   in a two-player zero-sum game.
[00:08:06.720 --> 00:08:08.760]   So you can think of a game where,
[00:08:08.760 --> 00:08:10.000]   if we're playing a game where whoever names
[00:08:10.000 --> 00:08:11.920]   the bigger number is the winner,
[00:08:11.920 --> 00:08:13.400]   there's no Nash equilibrium to that game.
[00:08:13.400 --> 00:08:14.680]   - 17, okay. - Yeah, exactly.
[00:08:14.680 --> 00:08:16.040]   18, but you beat.
[00:08:16.040 --> 00:08:17.320]   - You win again.
[00:08:17.320 --> 00:08:18.840]   You're good at this.
[00:08:18.840 --> 00:08:20.960]   - I've played a lot of games.
[00:08:20.960 --> 00:08:24.040]   - Okay, so that's, and then the zero-sum aspect.
[00:08:24.040 --> 00:08:26.120]   The zero-sum. - Zero-sum aspect.
[00:08:26.120 --> 00:08:28.360]   So there exists a Nash equilibrium
[00:08:28.360 --> 00:08:30.320]   in non-two-player zero-sum games as well.
[00:08:30.320 --> 00:08:31.760]   And by the way, just to clarify what I mean
[00:08:31.760 --> 00:08:34.520]   by two-player zero-sum, I mean there's two players,
[00:08:34.520 --> 00:08:36.920]   and whatever one player wins, the other player loses.
[00:08:36.920 --> 00:08:38.840]   So if we're playing poker and I win $50,
[00:08:38.840 --> 00:08:41.160]   that means that you're losing $50.
[00:08:41.160 --> 00:08:44.480]   Now, outside of two-player zero-sum games,
[00:08:44.480 --> 00:08:46.840]   there still exists Nash equilibria,
[00:08:46.840 --> 00:08:48.480]   but they're not as meaningful.
[00:08:48.480 --> 00:08:51.360]   Because you can think of a game like Risk.
[00:08:51.360 --> 00:08:54.120]   If everybody else on the board decides
[00:08:54.120 --> 00:08:55.920]   to team up against you and take you out,
[00:08:55.920 --> 00:08:57.480]   there's no perfect strategy you can play
[00:08:57.480 --> 00:08:59.280]   that's gonna guarantee that you win there.
[00:08:59.280 --> 00:09:00.600]   There's just nothing you can do.
[00:09:00.600 --> 00:09:02.720]   So outside of two-player zero-sum games,
[00:09:02.720 --> 00:09:05.040]   there's no guarantee that you're going to win
[00:09:05.040 --> 00:09:07.160]   by playing a Nash equilibrium.
[00:09:07.160 --> 00:09:08.920]   - Have you ever tried to model in
[00:09:08.920 --> 00:09:11.320]   the other aspects of the game,
[00:09:11.320 --> 00:09:15.360]   which is like the pleasure you draw from playing the game?
[00:09:15.360 --> 00:09:18.440]   And then if you're a professional poker player,
[00:09:18.440 --> 00:09:20.840]   if you're exciting, even if you lose,
[00:09:20.840 --> 00:09:25.520]   the money you would get from the attention you get
[00:09:25.520 --> 00:09:27.320]   to the sponsors and all that kind of stuff,
[00:09:27.320 --> 00:09:31.000]   is that, that would be a fun thing to model in.
[00:09:31.000 --> 00:09:33.440]   Or is that make it sort of super complex
[00:09:33.440 --> 00:09:36.920]   to include the human factor in its full complexity?
[00:09:36.920 --> 00:09:38.480]   - I think you bring up a couple of good points there.
[00:09:38.480 --> 00:09:41.200]   So I think a lot of professional poker players,
[00:09:41.200 --> 00:09:42.920]   I mean, they get a huge amount of money,
[00:09:42.920 --> 00:09:44.640]   not from actually playing poker,
[00:09:44.640 --> 00:09:47.600]   but from the sponsorships and having a personality
[00:09:47.600 --> 00:09:49.520]   that people want to tune in and watch.
[00:09:49.520 --> 00:09:53.360]   That's a big way to make a name for yourself in poker.
[00:09:53.360 --> 00:09:55.040]   - I just wonder from an AI perspective,
[00:09:55.040 --> 00:09:57.440]   if you create, and we'll talk about this more,
[00:09:57.440 --> 00:10:01.720]   maybe AI system that also talks trash
[00:10:01.720 --> 00:10:02.960]   and all that kind of stuff,
[00:10:02.960 --> 00:10:05.240]   that that becomes part of the function to maximize.
[00:10:05.240 --> 00:10:08.560]   So it's not just optimal poker play.
[00:10:08.560 --> 00:10:10.200]   Maybe sometimes you want to be chaotic.
[00:10:10.200 --> 00:10:11.800]   Maybe sometimes you want to be suboptimal
[00:10:11.800 --> 00:10:15.480]   and you lose the chaos.
[00:10:15.480 --> 00:10:18.240]   And maybe sometimes you want to be overly aggressive
[00:10:18.240 --> 00:10:21.840]   because the audience loves that.
[00:10:21.840 --> 00:10:22.960]   That'd be fascinating.
[00:10:22.960 --> 00:10:24.200]   - I think what you're getting at here
[00:10:24.200 --> 00:10:25.880]   is that there's a difference between making an AI
[00:10:25.880 --> 00:10:28.040]   that wins a game and an AI that's fun to play with.
[00:10:28.040 --> 00:10:28.880]   - Yeah.
[00:10:28.880 --> 00:10:29.720]   - Yeah.
[00:10:29.720 --> 00:10:30.540]   - Or fun to watch.
[00:10:30.540 --> 00:10:31.440]   So those are all different things,
[00:10:31.440 --> 00:10:33.200]   fun to play with and fun to watch.
[00:10:33.200 --> 00:10:37.640]   - Yeah, and I think, I've heard talks from game designers
[00:10:37.640 --> 00:10:39.880]   and they say people that work on AI
[00:10:39.880 --> 00:10:42.800]   for actual recreational games that people play.
[00:10:42.800 --> 00:10:44.440]   And they say, yeah, there's a big difference
[00:10:44.440 --> 00:10:46.280]   between trying to make an AI that actually wins.
[00:10:46.280 --> 00:10:49.280]   And you look at a game like "Civilization",
[00:10:49.280 --> 00:10:53.400]   the way that the AIs play is not optimal for trying to win.
[00:10:53.400 --> 00:10:54.800]   They're playing a different game.
[00:10:54.800 --> 00:10:55.960]   They're trying to have personalities.
[00:10:55.960 --> 00:10:59.000]   They're trying to be fun and engaging.
[00:10:59.000 --> 00:11:00.960]   And that makes for a better game.
[00:11:00.960 --> 00:11:01.800]   - Yeah.
[00:11:01.800 --> 00:11:02.640]   - And we also talk about NPCs.
[00:11:02.640 --> 00:11:03.960]   We just talked to Todd Howard,
[00:11:03.960 --> 00:11:06.720]   who is the creator of "Fallout" and the "Elder Scrolls" series
[00:11:06.720 --> 00:11:10.560]   and "Starfield", the new game coming out.
[00:11:10.560 --> 00:11:13.160]   And the creator of what I think is the greatest game
[00:11:13.160 --> 00:11:15.640]   of all time, which is "Skyrim" and the NPCs there.
[00:11:15.640 --> 00:11:18.300]   The AI that governs that whole game is very interesting,
[00:11:18.300 --> 00:11:20.780]   but the NPCs also are super interesting.
[00:11:20.780 --> 00:11:25.640]   And considering what language models might do to NPCs
[00:11:25.640 --> 00:11:29.800]   in an open world RPG role-playing game,
[00:11:29.800 --> 00:11:31.020]   it's super exciting.
[00:11:31.020 --> 00:11:33.440]   - Yeah, honestly, I think this is one
[00:11:33.440 --> 00:11:35.420]   of the first applications where we're going to see
[00:11:35.420 --> 00:11:38.640]   real consumer interaction with large language models.
[00:11:38.640 --> 00:11:42.720]   I guess "Elder Scrolls VI" is in development now.
[00:11:42.720 --> 00:11:44.680]   They're probably pretty close to finishing it,
[00:11:44.680 --> 00:11:48.160]   but I would not be surprised at all if "Elder Scrolls VII"
[00:11:48.160 --> 00:11:49.880]   was using large language models for their NPCs.
[00:11:49.880 --> 00:11:51.080]   - No, they're not.
[00:11:51.080 --> 00:11:52.680]   I mean, I'm not saying anything.
[00:11:52.680 --> 00:11:53.640]   I'm not saying anything.
[00:11:53.640 --> 00:11:55.880]   - Okay, this is me speculating, not you.
[00:11:55.880 --> 00:11:59.280]   - No, but they're just releasing the "Starfield" game.
[00:11:59.280 --> 00:12:00.520]   They do one game at a time.
[00:12:00.520 --> 00:12:01.360]   - Yeah.
[00:12:01.360 --> 00:12:04.260]   - And so whatever it is, whenever the date is,
[00:12:04.260 --> 00:12:07.000]   I don't know what the date is, calm down,
[00:12:07.000 --> 00:12:11.000]   but it would be, I don't know, like 2024, '25, '26.
[00:12:11.000 --> 00:12:12.720]   So it's actually very possible
[00:12:12.720 --> 00:12:14.240]   that would include language models.
[00:12:14.240 --> 00:12:19.200]   - I was listening to this talk by a gaming executive
[00:12:19.200 --> 00:12:20.840]   when I was in grad school.
[00:12:20.840 --> 00:12:24.160]   And one of the questions that a person in the audience asked
[00:12:24.160 --> 00:12:26.140]   is why are all these games so focused
[00:12:26.140 --> 00:12:27.780]   on fighting and killing?
[00:12:27.780 --> 00:12:30.880]   And the person responded that it's just so much harder
[00:12:30.880 --> 00:12:34.080]   to make an AI that can talk with you and cooperate with you
[00:12:34.080 --> 00:12:36.600]   than it is to make an AI that can fight you.
[00:12:36.600 --> 00:12:39.680]   And I think once this technology develops further
[00:12:39.680 --> 00:12:41.380]   and you can reach a point where like
[00:12:41.380 --> 00:12:44.160]   not every single line of dialogue has to be scripted,
[00:12:44.160 --> 00:12:46.320]   it unlocks a lot of potential for new kinds of games,
[00:12:46.320 --> 00:12:49.280]   like much more like positive interactions
[00:12:49.280 --> 00:12:50.360]   that are not so focused on fighting.
[00:12:50.360 --> 00:12:52.080]   And I'm really looking forward to that.
[00:12:52.080 --> 00:12:53.120]   - It might not be positive.
[00:12:53.120 --> 00:12:54.440]   It might be just drama.
[00:12:54.440 --> 00:12:56.760]   So you'll be in like a "Call of Duty" game
[00:12:56.760 --> 00:12:57.720]   and instead of doing the shooting,
[00:12:57.720 --> 00:13:00.920]   you'll just be hanging out and like arguing with an AI
[00:13:00.920 --> 00:13:03.720]   about like passive aggressive.
[00:13:03.720 --> 00:13:05.200]   And then you won't be able to sleep that night.
[00:13:05.200 --> 00:13:07.780]   You have to return and continue the argument
[00:13:07.780 --> 00:13:10.520]   that you were emotionally hurt.
[00:13:10.520 --> 00:13:15.040]   I mean, yeah, I think that's actually an exciting world.
[00:13:15.040 --> 00:13:17.640]   Whatever is the drama, the chaos that we love,
[00:13:17.640 --> 00:13:19.480]   the push and pull of human connection,
[00:13:19.480 --> 00:13:22.240]   I think it's possible to do that in the video game world.
[00:13:22.240 --> 00:13:24.280]   And I think you could be messier
[00:13:24.280 --> 00:13:26.320]   and make more mistakes in a video game world,
[00:13:26.320 --> 00:13:28.840]   which is why it would be a nice place.
[00:13:28.840 --> 00:13:32.680]   And also it doesn't have as deep
[00:13:32.680 --> 00:13:34.360]   of a real psychological impact
[00:13:34.360 --> 00:13:35.640]   because inside video games,
[00:13:35.640 --> 00:13:39.380]   it's kind of understood that you're in a not a real world.
[00:13:39.380 --> 00:13:42.000]   So whatever crazy stuff AI does,
[00:13:42.000 --> 00:13:43.880]   we have some flexibility to play.
[00:13:43.880 --> 00:13:46.320]   Just like with a game of diplomacy, it's a game.
[00:13:46.320 --> 00:13:48.800]   This is not real geopolitics, not real war.
[00:13:48.800 --> 00:13:49.880]   It's a game.
[00:13:49.880 --> 00:13:53.960]   So you can have a little bit of fun, a little bit of chaos.
[00:13:53.960 --> 00:13:56.160]   Okay, back to "Nashville Coyote Pair."
[00:13:56.160 --> 00:13:58.300]   How do we find the Nash equilibrium?
[00:13:58.300 --> 00:14:00.580]   - All right, so there's different ways
[00:14:00.580 --> 00:14:01.720]   to find a Nash equilibrium.
[00:14:01.720 --> 00:14:04.680]   So the way that we do it
[00:14:04.680 --> 00:14:07.300]   is with this process called self-play.
[00:14:07.300 --> 00:14:09.180]   Basically, we have this algorithm
[00:14:09.180 --> 00:14:11.800]   that starts by playing totally randomly,
[00:14:11.800 --> 00:14:13.960]   and it learns how to play the game
[00:14:13.960 --> 00:14:15.640]   by playing against itself.
[00:14:15.640 --> 00:14:19.560]   So it will start playing the game totally randomly,
[00:14:19.560 --> 00:14:21.240]   and then if it's playing poker,
[00:14:21.240 --> 00:14:25.680]   it'll eventually get to the end of the game and make $50.
[00:14:26.200 --> 00:14:28.260]   And then it will review all of the decisions
[00:14:28.260 --> 00:14:30.420]   that it made along the way and say,
[00:14:30.420 --> 00:14:31.420]   what would have happened
[00:14:31.420 --> 00:14:34.080]   if I had chosen this other action instead?
[00:14:34.080 --> 00:14:36.760]   If I had raised here instead of called,
[00:14:36.760 --> 00:14:38.460]   what would the other player have done?
[00:14:38.460 --> 00:14:40.220]   And because it's playing against a copy of itself,
[00:14:40.220 --> 00:14:42.560]   it's able to do that counterfactual reasoning.
[00:14:42.560 --> 00:14:45.340]   So it can say, okay, well, if I took this action
[00:14:45.340 --> 00:14:46.600]   and the other person takes this action,
[00:14:46.600 --> 00:14:47.940]   and then I take this action,
[00:14:47.940 --> 00:14:50.520]   and eventually I make $150 instead of 50.
[00:14:51.440 --> 00:14:56.160]   And so it updates the regret value for that action.
[00:14:56.160 --> 00:14:58.040]   Regret is basically like how much does it regret
[00:14:58.040 --> 00:15:00.540]   having not played that action in the past?
[00:15:00.540 --> 00:15:03.740]   And when it encounters that same situation again,
[00:15:03.740 --> 00:15:05.720]   it's going to pick actions that have higher regret
[00:15:05.720 --> 00:15:07.680]   with higher probability.
[00:15:07.680 --> 00:15:10.920]   Now, it'll just keep simulating the games this way.
[00:15:10.920 --> 00:15:14.880]   It'll keep accumulating regrets for different situations.
[00:15:14.880 --> 00:15:16.760]   And in the long run,
[00:15:16.760 --> 00:15:18.600]   if you pick actions that have higher regret
[00:15:18.600 --> 00:15:20.760]   with higher probability in the correct way,
[00:15:20.760 --> 00:15:23.580]   it's proven to converge to a Nash equilibrium.
[00:15:23.580 --> 00:15:26.440]   - Even for super complex games?
[00:15:26.440 --> 00:15:28.560]   Even for imperfect information games?
[00:15:28.560 --> 00:15:29.440]   - It's true for all games.
[00:15:29.440 --> 00:15:31.520]   It's true for chess, it's true for poker.
[00:15:31.520 --> 00:15:33.480]   It's particularly useful for poker.
[00:15:33.480 --> 00:15:34.640]   - So this is the method
[00:15:34.640 --> 00:15:36.400]   of counterfactual regret minimization?
[00:15:36.400 --> 00:15:37.880]   - This is counterfactual regret minimization.
[00:15:37.880 --> 00:15:39.400]   - That doesn't have to do with self-play,
[00:15:39.400 --> 00:15:41.680]   it has to do with just any,
[00:15:41.680 --> 00:15:44.520]   if you follow this kind of process, self-play or not,
[00:15:44.520 --> 00:15:48.280]   you will be able to arrive at an optimal set of actions.
[00:15:48.280 --> 00:15:50.120]   - So this counterfactual regret minimization
[00:15:50.120 --> 00:15:51.440]   is a kind of self-play.
[00:15:51.440 --> 00:15:53.120]   It's a principled kind of self-play
[00:15:53.120 --> 00:15:55.360]   that's proven to converge to Nash equilibria,
[00:15:55.360 --> 00:15:57.680]   even in imperfect information games.
[00:15:57.680 --> 00:15:59.200]   Now you can have other forms of self-play
[00:15:59.200 --> 00:16:00.680]   and people use other forms of self-play
[00:16:00.680 --> 00:16:02.120]   for perfect information games,
[00:16:02.120 --> 00:16:04.800]   where you have more flexibility,
[00:16:04.800 --> 00:16:07.720]   the algorithm doesn't have to be as theoretically sound
[00:16:07.720 --> 00:16:09.760]   in order to converge to that class of games
[00:16:09.760 --> 00:16:11.800]   because it's a simpler setting.
[00:16:11.800 --> 00:16:14.920]   - Sure, so I kind of, in my brain,
[00:16:14.920 --> 00:16:17.280]   the word self-play has mapped to neural networks,
[00:16:17.280 --> 00:16:19.000]   but we're speaking something bigger
[00:16:19.000 --> 00:16:20.000]   than just neural networks.
[00:16:20.000 --> 00:16:21.320]   It could be anything.
[00:16:21.320 --> 00:16:25.000]   The self-play mechanism is just the mechanism
[00:16:25.000 --> 00:16:26.120]   of a system playing itself.
[00:16:26.120 --> 00:16:26.960]   - Exactly, yeah.
[00:16:26.960 --> 00:16:28.840]   Self-play is not tied specifically to neural nets,
[00:16:28.840 --> 00:16:32.040]   it's a kind of reinforcement learning, basically.
[00:16:32.040 --> 00:16:35.720]   And I would also say this process of trying to reason,
[00:16:35.720 --> 00:16:36.840]   oh, what would the value have been
[00:16:36.840 --> 00:16:39.280]   if I had taken this other action instead?
[00:16:39.280 --> 00:16:40.920]   This is very similar to how humans learn
[00:16:40.920 --> 00:16:42.960]   to play a game like poker.
[00:16:42.960 --> 00:16:44.160]   You probably played poker before
[00:16:44.160 --> 00:16:46.120]   and with your friends, you probably ask,
[00:16:46.120 --> 00:16:48.520]   oh, what do you have called me if I raise there?
[00:16:49.400 --> 00:16:52.440]   That's a person trying to do the same kind of learning
[00:16:52.440 --> 00:16:54.920]   from a counterfactual that the AI is doing.
[00:16:54.920 --> 00:16:56.360]   - Okay, and if you do that at scale,
[00:16:56.360 --> 00:16:59.680]   you're gonna be able to learn an optimal policy.
[00:16:59.680 --> 00:17:01.800]   - Yeah, now where the neural nets come in,
[00:17:01.800 --> 00:17:04.720]   I said, okay, if it's in that situation again,
[00:17:04.720 --> 00:17:07.520]   then it will choose the action that has high regret.
[00:17:07.520 --> 00:17:10.640]   Now, the problem is that poker is such a huge game.
[00:17:10.640 --> 00:17:12.360]   I think No Limit Texas Hold'em,
[00:17:12.360 --> 00:17:13.240]   the version that we were playing,
[00:17:13.240 --> 00:17:15.920]   has 10 to the 161 different decision points,
[00:17:15.920 --> 00:17:17.080]   which is more than the number of atoms
[00:17:17.080 --> 00:17:18.160]   in the universe squared.
[00:17:18.160 --> 00:17:19.040]   - That's heads up?
[00:17:19.040 --> 00:17:20.240]   - That's heads up, yeah.
[00:17:20.240 --> 00:17:21.600]   - 10 to the 161, you said?
[00:17:21.600 --> 00:17:22.960]   - Yeah, I mean, it depends on the number of chips
[00:17:22.960 --> 00:17:24.240]   that you have, the stacks and everything,
[00:17:24.240 --> 00:17:27.080]   but the version that we were playing was 10 to the 161.
[00:17:27.080 --> 00:17:29.360]   - Which I assume would be a somewhat simplified version
[00:17:29.360 --> 00:17:34.240]   anyway, 'cause I bet there's some step function
[00:17:34.240 --> 00:17:36.160]   you had for bets.
[00:17:36.160 --> 00:17:38.760]   - Oh, no, no, no, I'm saying we played the full game.
[00:17:38.760 --> 00:17:39.920]   You can bet whatever amount you want.
[00:17:39.920 --> 00:17:41.280]   Now, the bot maybe was constrained
[00:17:41.280 --> 00:17:43.200]   in what it considered for bet sizes,
[00:17:43.200 --> 00:17:44.720]   but the person on the other side
[00:17:44.720 --> 00:17:45.800]   could bet whatever they wanted.
[00:17:45.800 --> 00:17:49.520]   - Yeah, I mean, 161 plus or minus 10 doesn't matter.
[00:17:49.520 --> 00:17:50.360]   - Yeah, yeah.
[00:17:50.360 --> 00:17:54.960]   And so the way neural nets help out here is,
[00:17:54.960 --> 00:17:57.160]   you know, you don't have to run into the same exact situation
[00:17:57.160 --> 00:17:58.280]   'cause that's never gonna happen again.
[00:17:58.280 --> 00:18:00.160]   The odds of you running into the same exact situation
[00:18:00.160 --> 00:18:03.160]   are pretty slim, but if you run into a similar situation,
[00:18:03.160 --> 00:18:05.120]   then you can generalize from other states
[00:18:05.120 --> 00:18:07.000]   that you've been in that kind of look like that one,
[00:18:07.000 --> 00:18:09.160]   and you can say like, well, these other situations,
[00:18:09.160 --> 00:18:10.400]   I had high regret for this action,
[00:18:10.400 --> 00:18:12.680]   and so maybe I should play that action here as well.
[00:18:12.680 --> 00:18:15.000]   - Which is the more complex game?
[00:18:15.000 --> 00:18:18.560]   Chess or poker or go or poker?
[00:18:18.560 --> 00:18:19.400]   Do you know?
[00:18:19.400 --> 00:18:21.120]   - That is a controversial question.
[00:18:21.120 --> 00:18:21.960]   - Okay.
[00:18:21.960 --> 00:18:22.780]   - I'm gonna-
[00:18:22.780 --> 00:18:24.040]   - It's like somebody's screaming on Reddit right now.
[00:18:24.040 --> 00:18:25.720]   It depends on which subreddit you're on.
[00:18:25.720 --> 00:18:27.200]   Is it chess or is it poker?
[00:18:27.200 --> 00:18:28.360]   - I'm sure like David Silver's
[00:18:28.360 --> 00:18:29.600]   gonna get really angry at me.
[00:18:29.600 --> 00:18:30.440]   - Yeah.
[00:18:30.440 --> 00:18:31.760]   - I'll say, I'm gonna say poker actually,
[00:18:31.760 --> 00:18:33.960]   and I think for a couple of reasons.
[00:18:33.960 --> 00:18:36.200]   - They're not here to defend themselves.
[00:18:36.200 --> 00:18:37.760]   - So first of all,
[00:18:37.760 --> 00:18:39.460]   you have the imperfect information aspect.
[00:18:39.460 --> 00:18:43.760]   And so it's, we can go into that,
[00:18:43.760 --> 00:18:47.040]   but like once you introduce imperfect information,
[00:18:47.040 --> 00:18:49.080]   things get much more complicated.
[00:18:49.080 --> 00:18:50.920]   - So we should say,
[00:18:50.920 --> 00:18:54.360]   maybe you can describe what is seen to the players,
[00:18:54.360 --> 00:18:57.960]   what is not seen in the game of Texas Hold 'Em.
[00:18:57.960 --> 00:18:59.040]   - Yeah, so Texas Hold 'Em,
[00:18:59.040 --> 00:19:02.640]   you get two cards face down that only you see.
[00:19:02.640 --> 00:19:04.560]   And so that's the hidden information of the game.
[00:19:04.560 --> 00:19:06.600]   The other players also all get two cards face down
[00:19:06.600 --> 00:19:08.000]   that only they see.
[00:19:08.000 --> 00:19:10.000]   And so you have to kind of, as you're playing,
[00:19:10.000 --> 00:19:12.640]   reason about like, okay, what do they think I have?
[00:19:12.640 --> 00:19:13.680]   What do they have?
[00:19:13.680 --> 00:19:15.480]   What do they think I think they have?
[00:19:15.480 --> 00:19:16.320]   That kind of stuff.
[00:19:16.320 --> 00:19:20.080]   And that's kind of where bluffing comes into play, right?
[00:19:20.080 --> 00:19:22.640]   Because the fact that you can bluff,
[00:19:22.640 --> 00:19:25.740]   the fact that you can bet with a bad hand and still win,
[00:19:25.740 --> 00:19:27.600]   is because they don't know what your cards are.
[00:19:27.600 --> 00:19:28.440]   - Right.
[00:19:28.440 --> 00:19:29.680]   - And that's the key difference
[00:19:29.680 --> 00:19:31.880]   between a perfect information game like poker,
[00:19:31.880 --> 00:19:34.120]   sorry, like chess and go,
[00:19:34.120 --> 00:19:36.040]   and imperfect information games like poker.
[00:19:36.040 --> 00:19:38.440]   - This is what trash talk looks like.
[00:19:38.440 --> 00:19:41.960]   The implied statement is,
[00:19:41.960 --> 00:19:44.840]   the game I solved is much tougher.
[00:19:44.840 --> 00:19:46.880]   But yeah, so when you're playing,
[00:19:46.880 --> 00:19:48.440]   I'm just gonna do random questions here.
[00:19:48.440 --> 00:19:51.240]   So when you're playing your opponent
[00:19:51.240 --> 00:19:53.080]   under imperfect information,
[00:19:53.080 --> 00:19:56.240]   is there some degree to which you're trying
[00:19:56.240 --> 00:19:58.820]   to estimate the range of hands that they have?
[00:19:58.820 --> 00:20:01.320]   Or is that not part of the algorithm?
[00:20:01.320 --> 00:20:04.280]   So what are the different approaches
[00:20:04.280 --> 00:20:06.680]   to the imperfect information game?
[00:20:06.680 --> 00:20:08.320]   - So the key thing to understand
[00:20:08.320 --> 00:20:11.140]   about why imperfect information makes things difficult,
[00:20:11.140 --> 00:20:13.200]   is that you have to worry not just about
[00:20:13.200 --> 00:20:14.720]   which actions to play,
[00:20:14.720 --> 00:20:17.400]   but the probability that you're gonna play those actions.
[00:20:17.400 --> 00:20:21.280]   So you think about rock, paper, scissors, for example,
[00:20:21.280 --> 00:20:24.360]   rock, paper, scissors is an imperfect information game.
[00:20:24.360 --> 00:20:25.200]   - Right.
[00:20:25.200 --> 00:20:26.600]   - Because you don't know what I'm about to throw.
[00:20:26.600 --> 00:20:28.520]   - I do, but yeah, usually not, yeah.
[00:20:28.520 --> 00:20:30.360]   - Yeah, and so you can't just say like,
[00:20:30.360 --> 00:20:32.240]   I'm just gonna throw a rock every single time,
[00:20:32.240 --> 00:20:34.160]   because the other person's gonna figure that out
[00:20:34.160 --> 00:20:35.260]   and notice a pattern,
[00:20:35.260 --> 00:20:37.280]   and then suddenly you're gonna start losing.
[00:20:37.280 --> 00:20:38.600]   And so you don't just have to figure out
[00:20:38.600 --> 00:20:39.640]   like which action to play,
[00:20:39.640 --> 00:20:42.020]   you have to figure out the probability that you play it.
[00:20:42.020 --> 00:20:45.320]   And really importantly, the value of an action
[00:20:45.320 --> 00:20:47.580]   depends on the probability that you're gonna play it.
[00:20:47.580 --> 00:20:50.380]   So if you're playing rock every single time,
[00:20:50.380 --> 00:20:51.900]   that value is really low.
[00:20:51.900 --> 00:20:54.640]   But if you're never playing rock,
[00:20:54.640 --> 00:20:55.960]   you play rock like 1% of the time,
[00:20:55.960 --> 00:20:57.620]   then suddenly the other person
[00:20:57.620 --> 00:20:59.780]   is probably gonna be throwing scissors.
[00:20:59.780 --> 00:21:00.820]   And when you throw a rock,
[00:21:00.820 --> 00:21:03.340]   the value of that action is gonna be really high.
[00:21:03.340 --> 00:21:04.580]   Now you take that to poker,
[00:21:04.580 --> 00:21:09.100]   what that means is the value of bluffing, for example,
[00:21:09.100 --> 00:21:10.600]   if you're the kind of person that never bluffs
[00:21:10.600 --> 00:21:13.240]   and you have this reputation as somebody that never bluffs,
[00:21:13.240 --> 00:21:14.480]   and suddenly you bluff,
[00:21:14.480 --> 00:21:16.560]   there's a really good chance that that bluff is gonna work
[00:21:16.560 --> 00:21:18.160]   and you're gonna make a lot of money.
[00:21:18.160 --> 00:21:19.880]   On the other hand, if you got a reputation,
[00:21:19.880 --> 00:21:21.720]   like if they seen you play for a long time and they see,
[00:21:21.720 --> 00:21:24.700]   oh, you're the kind of person that's bluffing all the time,
[00:21:24.700 --> 00:21:26.200]   when you bluff, they're not gonna buy it
[00:21:26.200 --> 00:21:27.040]   and they're gonna call you down
[00:21:27.040 --> 00:21:28.920]   and you're gonna lose a lot of money.
[00:21:28.920 --> 00:21:31.240]   And that, finding that balance
[00:21:31.240 --> 00:21:33.240]   of how often you should be bluffing
[00:21:33.240 --> 00:21:37.240]   is the key challenge of a game of poker.
[00:21:37.240 --> 00:21:40.220]   And you contrast that with a game like chess,
[00:21:40.220 --> 00:21:43.880]   it doesn't matter if you're opening with the queen's gambit
[00:21:43.880 --> 00:21:46.140]   10% of the time or 100% of the time,
[00:21:46.140 --> 00:21:48.240]   the value, the expected value is the same.
[00:21:48.240 --> 00:21:54.560]   So that's why we need these algorithms that understand,
[00:21:54.560 --> 00:21:56.840]   not just we have to figure out what actions are good,
[00:21:56.840 --> 00:21:57.720]   but the probabilities,
[00:21:57.720 --> 00:21:59.600]   we need to get the exact probabilities correct.
[00:21:59.600 --> 00:22:02.300]   And that's actually when we created the bot Libratus,
[00:22:02.300 --> 00:22:03.540]   Libratus means balanced
[00:22:03.540 --> 00:22:06.160]   because the algorithm that we designed
[00:22:06.160 --> 00:22:08.200]   was designed to find that right balance
[00:22:08.200 --> 00:22:10.920]   of how often it should play each action.
[00:22:10.920 --> 00:22:13.720]   - The balance of how often in the key sort of branching
[00:22:13.720 --> 00:22:15.320]   is the bluff or not to bluff.
[00:22:15.320 --> 00:22:18.800]   Is that a good crude simplification
[00:22:18.800 --> 00:22:21.080]   of the major decision in poker?
[00:22:21.080 --> 00:22:22.200]   - It's a good simplification,
[00:22:22.200 --> 00:22:23.880]   I think that's like the main tension,
[00:22:23.880 --> 00:22:27.720]   but it's not just how often to bluff or not to bluff,
[00:22:27.720 --> 00:22:29.740]   it's like, how often should you bet in general?
[00:22:29.740 --> 00:22:33.280]   How often should you, what kind of bet should you make?
[00:22:33.280 --> 00:22:35.240]   Should you bet big or should you bet small?
[00:22:35.240 --> 00:22:37.720]   And with which hands?
[00:22:37.720 --> 00:22:40.680]   And so this is where the idea of a range comes from,
[00:22:40.680 --> 00:22:43.200]   because when you're bluffing with a particular hand
[00:22:43.200 --> 00:22:44.880]   in a particular spot,
[00:22:44.880 --> 00:22:46.220]   you don't want there to be a pattern
[00:22:46.220 --> 00:22:47.440]   for the other person to pick up on.
[00:22:47.440 --> 00:22:48.520]   You don't want them to figure out,
[00:22:48.520 --> 00:22:50.700]   oh, whenever this person is in this spot,
[00:22:50.700 --> 00:22:51.840]   they're always bluffing.
[00:22:51.840 --> 00:22:53.560]   And so you have to reason about,
[00:22:53.560 --> 00:22:58.240]   okay, would I also bet with a good hand in this spot?
[00:22:58.240 --> 00:22:59.700]   You wanna be unpredictable.
[00:22:59.700 --> 00:23:01.980]   So you have to think about what would I do
[00:23:01.980 --> 00:23:04.280]   if I had this different set of cards?
[00:23:04.280 --> 00:23:08.400]   - Is there explicit estimation of like a theory of mind
[00:23:08.400 --> 00:23:09.960]   that the other person has about you,
[00:23:09.960 --> 00:23:13.140]   or is that just a emergent thing that happens?
[00:23:13.140 --> 00:23:17.280]   - The way that the bots handle it,
[00:23:17.280 --> 00:23:18.160]   that are really successful,
[00:23:18.160 --> 00:23:19.660]   they have an explicit theory of mind.
[00:23:19.660 --> 00:23:21.920]   So they're explicitly reasoning about
[00:23:21.920 --> 00:23:25.400]   what's the common knowledge belief?
[00:23:25.400 --> 00:23:27.320]   What do you think I have?
[00:23:27.320 --> 00:23:28.800]   What do I think you have?
[00:23:28.800 --> 00:23:31.060]   What do you think I think you have?
[00:23:31.060 --> 00:23:32.680]   It's explicitly reasoning about that.
[00:23:32.680 --> 00:23:34.360]   - Is there multiple yous there?
[00:23:34.360 --> 00:23:37.360]   So maybe that's jumping ahead to six players,
[00:23:37.360 --> 00:23:40.720]   but is there a stickiness to the person?
[00:23:40.720 --> 00:23:42.060]   So it's an iterative game.
[00:23:42.060 --> 00:23:43.600]   You're playing the same person.
[00:23:43.600 --> 00:23:47.480]   There's a stickiness to that, right?
[00:23:47.480 --> 00:23:49.720]   You're gathering information as you play.
[00:23:49.720 --> 00:23:53.320]   It's not every hand is a new hand.
[00:23:53.320 --> 00:23:57.240]   Is there a continuation in terms of estimating
[00:23:57.240 --> 00:23:59.340]   what kind of player I'm facing here?
[00:23:59.340 --> 00:24:00.180]   - That's a good question.
[00:24:00.180 --> 00:24:02.780]   So you could approach the game that way.
[00:24:02.780 --> 00:24:04.500]   The way that the bots do it,
[00:24:04.500 --> 00:24:06.660]   they don't, and the way that humans approach it also,
[00:24:06.660 --> 00:24:07.940]   expert human players,
[00:24:07.940 --> 00:24:10.580]   the way they approach it is to basically assume
[00:24:10.580 --> 00:24:13.300]   that you know my strategy.
[00:24:13.300 --> 00:24:16.460]   So I'm going to try to pick a strategy
[00:24:16.460 --> 00:24:18.380]   where even if I were to play it for 10,000 hands
[00:24:18.380 --> 00:24:20.200]   and you could figure out exactly what it was,
[00:24:20.200 --> 00:24:21.440]   you still wouldn't be able to beat it.
[00:24:21.440 --> 00:24:22.740]   Basically what that means is I'm trying
[00:24:22.740 --> 00:24:24.380]   to approximate the Nash equilibrium.
[00:24:24.380 --> 00:24:25.620]   I'm trying to be perfectly balanced
[00:24:25.620 --> 00:24:27.940]   because if I'm playing the Nash equilibrium,
[00:24:27.940 --> 00:24:30.700]   even if you know what my strategy is,
[00:24:30.700 --> 00:24:33.140]   like I said, I'm still unbeatable in expectation.
[00:24:33.140 --> 00:24:35.740]   So that's what the bot aims for.
[00:24:35.740 --> 00:24:36.940]   And that's actually what a lot
[00:24:36.940 --> 00:24:38.700]   of expert poker players aim for as well,
[00:24:38.700 --> 00:24:41.140]   to start by playing the Nash equilibrium.
[00:24:41.140 --> 00:24:42.500]   And then maybe if they spot weaknesses
[00:24:42.500 --> 00:24:43.660]   in the way you're playing,
[00:24:43.660 --> 00:24:44.940]   then they can deviate a little bit
[00:24:44.940 --> 00:24:46.200]   to take advantage of that.
[00:24:46.200 --> 00:24:49.500]   - They aim to be unbeatable in expectation.
[00:24:49.500 --> 00:24:50.660]   Okay.
[00:24:50.660 --> 00:24:52.780]   So who's the greatest poker player of all time
[00:24:52.780 --> 00:24:54.180]   and why is it Phil Hellmuth?
[00:24:54.180 --> 00:24:55.280]   So this is for Phil.
[00:24:56.220 --> 00:25:00.260]   So he's known at least in part
[00:25:00.260 --> 00:25:02.700]   for maybe playing suboptimally
[00:25:02.700 --> 00:25:04.820]   and he still wins a lot.
[00:25:04.820 --> 00:25:06.100]   It's a bit chaotic.
[00:25:06.100 --> 00:25:09.940]   So maybe can you speak from an AI perspective
[00:25:09.940 --> 00:25:12.340]   about the genius of his madness
[00:25:12.340 --> 00:25:14.420]   or the madness of his genius?
[00:25:14.420 --> 00:25:16.660]   So playing suboptimally, playing chaotically
[00:25:16.660 --> 00:25:21.820]   as a way to make it hard to pin down
[00:25:21.820 --> 00:25:23.740]   about what your strategy is.
[00:25:23.740 --> 00:25:25.060]   - So, okay.
[00:25:25.060 --> 00:25:26.420]   The thing that I should explain first of all
[00:25:26.420 --> 00:25:28.580]   with like Nash equilibrium,
[00:25:28.580 --> 00:25:30.080]   it doesn't mean that it's predictable.
[00:25:30.080 --> 00:25:31.060]   The whole point of it is that
[00:25:31.060 --> 00:25:32.880]   you're trying to be unpredictable.
[00:25:32.880 --> 00:25:35.480]   Now, I think when somebody like Phil Hellmuth
[00:25:35.480 --> 00:25:36.940]   might be really successful,
[00:25:36.940 --> 00:25:38.540]   is not in being unpredictable,
[00:25:38.540 --> 00:25:42.240]   but in being able to take advantage
[00:25:42.240 --> 00:25:43.660]   of the other player and figure out
[00:25:43.660 --> 00:25:45.700]   where they're being predictable
[00:25:45.700 --> 00:25:49.280]   or guiding the other player into thinking
[00:25:49.280 --> 00:25:51.340]   that you have certain weaknesses
[00:25:51.340 --> 00:25:53.080]   and then understanding how they're going
[00:25:53.080 --> 00:25:54.140]   to change their behavior.
[00:25:54.140 --> 00:25:57.000]   They're going to deviate from a Nash equilibrium style
[00:25:57.000 --> 00:25:58.540]   of play to try to take advantage
[00:25:58.540 --> 00:25:59.500]   of those perceived weaknesses
[00:25:59.500 --> 00:26:00.860]   and then counter exploit them.
[00:26:00.860 --> 00:26:02.700]   So you kind of get into the mind games there.
[00:26:02.700 --> 00:26:04.980]   - So you think about these heads up poker
[00:26:04.980 --> 00:26:07.700]   as a dance between two agents,
[00:26:07.700 --> 00:26:08.880]   I guess, are you playing the cards
[00:26:08.880 --> 00:26:10.460]   or are you playing the player?
[00:26:10.460 --> 00:26:12.900]   - So this gets down to a big argument
[00:26:12.900 --> 00:26:15.500]   in the poker community and the academic community.
[00:26:15.500 --> 00:26:16.900]   For a long time, there was this debate
[00:26:16.900 --> 00:26:19.140]   of like what's called GTO,
[00:26:19.140 --> 00:26:22.820]   game theory optimal poker or exploitative play.
[00:26:22.820 --> 00:26:25.820]   And up until about like 2017,
[00:26:25.820 --> 00:26:27.140]   when we did the Labradors match,
[00:26:27.140 --> 00:26:29.660]   I think actually exploitative play had the advantage.
[00:26:29.660 --> 00:26:31.140]   A lot of people were saying like,
[00:26:31.140 --> 00:26:33.580]   oh, this whole idea of game theory, it's just nonsense.
[00:26:33.580 --> 00:26:35.080]   And if you really want to make money,
[00:26:35.080 --> 00:26:36.820]   you got to like look into the other person's eyes
[00:26:36.820 --> 00:26:40.260]   and read their soul and figure out what cards they have.
[00:26:40.260 --> 00:26:42.960]   But what happened was people started adopting
[00:26:42.960 --> 00:26:44.560]   the game theory optimal strategy
[00:26:44.560 --> 00:26:46.860]   and they were making good money.
[00:26:46.860 --> 00:26:50.140]   And they weren't trying to adapt so much to the other player.
[00:26:50.140 --> 00:26:52.740]   They were just trying to play the Nash equilibrium.
[00:26:52.740 --> 00:26:54.340]   And then what really solidified it, I think,
[00:26:54.340 --> 00:26:56.940]   was the Labradors match,
[00:26:56.940 --> 00:26:59.360]   where we played our bot against four top heads up,
[00:26:59.360 --> 00:27:01.620]   no limit hold 'em poker players.
[00:27:01.620 --> 00:27:04.060]   And the bot wasn't trying to adapt to them.
[00:27:04.060 --> 00:27:05.380]   It wasn't trying to exploit them.
[00:27:05.380 --> 00:27:07.380]   It wasn't trying to do these mind games.
[00:27:07.380 --> 00:27:09.820]   It was just trying to approximate the Nash equilibrium
[00:27:09.820 --> 00:27:11.540]   and it crushed them.
[00:27:11.540 --> 00:27:16.540]   I think, you know, we were playing for $50, $100 blinds.
[00:27:16.540 --> 00:27:19.660]   And over the course of about 120,000 hands,
[00:27:19.660 --> 00:27:21.500]   it made close to $2 million.
[00:27:21.500 --> 00:27:22.900]   - 120,000 hands.
[00:27:22.900 --> 00:27:24.060]   - 120,000 hands.
[00:27:24.060 --> 00:27:24.900]   - Against humans.
[00:27:24.900 --> 00:27:26.580]   - Yeah, and this was fake money to be clear.
[00:27:26.580 --> 00:27:27.740]   So there was real money at stake.
[00:27:27.740 --> 00:27:28.620]   There was $200,000.
[00:27:28.620 --> 00:27:30.340]   - First of all, all money is fake.
[00:27:30.340 --> 00:27:33.780]   But that's a different conversation.
[00:27:33.780 --> 00:27:36.140]   We give it meaning.
[00:27:36.140 --> 00:27:39.180]   It's a phenomena that gets meaning
[00:27:39.180 --> 00:27:43.340]   from our complex psychology as a human civilization.
[00:27:43.340 --> 00:27:45.180]   It's emerging from the collective intelligence
[00:27:45.180 --> 00:27:46.380]   of the human species.
[00:27:46.380 --> 00:27:47.280]   But that's not what you mean.
[00:27:47.280 --> 00:27:49.140]   You mean like there's literally,
[00:27:49.140 --> 00:27:50.380]   you can't buy stuff with it.
[00:27:50.380 --> 00:27:52.300]   Okay, can you actually step back
[00:27:52.300 --> 00:27:55.900]   and take me through that competition?
[00:27:55.900 --> 00:27:56.740]   - Yeah, okay.
[00:27:56.740 --> 00:27:59.620]   So when I was in grad school,
[00:27:59.620 --> 00:28:00.460]   there was this thing called
[00:28:00.460 --> 00:28:02.180]   the annual computer poker competition,
[00:28:02.180 --> 00:28:04.620]   where every year all the different research labs
[00:28:04.620 --> 00:28:06.940]   that were working on AI for poker would get together.
[00:28:06.940 --> 00:28:07.780]   They would make a bot.
[00:28:07.780 --> 00:28:09.700]   They would play them against each other.
[00:28:09.700 --> 00:28:14.300]   And we made a bot that actually won the 2014 competition,
[00:28:14.300 --> 00:28:16.220]   the 2016 competition.
[00:28:16.220 --> 00:28:19.300]   And so we decided we're gonna take this bot, build on it,
[00:28:19.300 --> 00:28:22.460]   and play against real top professional
[00:28:22.460 --> 00:28:25.020]   heads up no limit Texas Hold'em poker players.
[00:28:25.020 --> 00:28:29.220]   So we invited four of the world's best players
[00:28:29.220 --> 00:28:30.620]   in this specialty.
[00:28:30.620 --> 00:28:33.260]   And we challenged them to 120,000 hands of poker
[00:28:33.260 --> 00:28:34.740]   over the course of 20 days.
[00:28:34.740 --> 00:28:40.220]   And we had $200,000 in prize money at stake,
[00:28:40.220 --> 00:28:42.060]   where it would basically be divided among them,
[00:28:42.060 --> 00:28:44.700]   depending on how well they did relative to each other.
[00:28:44.700 --> 00:28:46.060]   So we wanted to have some incentive
[00:28:46.060 --> 00:28:47.780]   for them to play their best.
[00:28:47.780 --> 00:28:50.940]   - Did you have a confidence 2014, 16,
[00:28:50.940 --> 00:28:52.580]   that this is even possible?
[00:28:52.580 --> 00:28:53.860]   How much doubt was there?
[00:28:53.860 --> 00:28:56.660]   - So we did a competition actually in 2015,
[00:28:56.660 --> 00:28:58.940]   where we also played against professional poker players
[00:28:58.940 --> 00:29:02.080]   and the bot lost by a pretty sizable margin actually.
[00:29:02.080 --> 00:29:05.820]   Now there were some big improvements from 2015 to 2017.
[00:29:05.820 --> 00:29:06.660]   And so-
[00:29:06.660 --> 00:29:07.660]   - Can you speak to the improvements?
[00:29:07.660 --> 00:29:08.940]   Is it computational in nature?
[00:29:08.940 --> 00:29:11.420]   Is it the algorithm, the methods?
[00:29:11.420 --> 00:29:12.940]   - It was really an algorithmic approach.
[00:29:12.940 --> 00:29:13.900]   That was the difference.
[00:29:13.900 --> 00:29:18.900]   So 2015, it was much more focused on trying to come up
[00:29:18.900 --> 00:29:20.780]   with a strategy upfront,
[00:29:20.780 --> 00:29:23.180]   like trying to solve the entire game of poker,
[00:29:23.180 --> 00:29:25.420]   like, and then just have a lookup table
[00:29:25.420 --> 00:29:27.460]   where you're saying like, "Oh, I'm in this situation.
[00:29:27.460 --> 00:29:29.420]   "What's the strategy?"
[00:29:29.420 --> 00:29:30.980]   The approach that we took in 2017
[00:29:30.980 --> 00:29:32.820]   was much more search-based.
[00:29:32.820 --> 00:29:36.260]   It was trying to say, "Okay, well, let me in real time
[00:29:36.260 --> 00:29:38.760]   "try to compute a much better strategy
[00:29:38.760 --> 00:29:40.940]   "than what I had pre-computed
[00:29:40.940 --> 00:29:42.620]   "by playing against myself during self-play."
[00:29:42.620 --> 00:29:47.020]   - What is the search space for poker?
[00:29:47.020 --> 00:29:48.420]   What are you searching over?
[00:29:48.420 --> 00:29:50.700]   What's that look like?
[00:29:50.700 --> 00:29:53.940]   There's different actions like raising, calling.
[00:29:53.940 --> 00:29:55.520]   Yeah, what are the actions?
[00:29:55.520 --> 00:29:59.620]   Is it just a search over actions?
[00:29:59.620 --> 00:30:02.740]   - So in a game like chess, the search is like,
[00:30:02.740 --> 00:30:04.920]   "Okay, I'm in this chess position
[00:30:04.920 --> 00:30:06.920]   "and I can move these different pieces
[00:30:06.920 --> 00:30:08.340]   "and see where things end up."
[00:30:08.340 --> 00:30:09.860]   In poker, what you're searching over
[00:30:09.860 --> 00:30:12.980]   is the actions that you can take for your hand,
[00:30:12.980 --> 00:30:14.960]   the probabilities that you take those actions,
[00:30:14.960 --> 00:30:17.300]   and then also the probabilities that you take other actions
[00:30:17.300 --> 00:30:19.140]   with other hands that you might have.
[00:30:19.140 --> 00:30:22.980]   And that's kind of like hard to wrap your head around.
[00:30:22.980 --> 00:30:26.220]   Like, why are you searching over these other hands
[00:30:26.220 --> 00:30:28.500]   that you might have and trying to figure out
[00:30:28.500 --> 00:30:30.740]   what you would do with those hands?
[00:30:30.740 --> 00:30:35.460]   And the idea is, again, you wanna always be balanced
[00:30:35.460 --> 00:30:36.840]   and unpredictable.
[00:30:36.840 --> 00:30:39.160]   And so if you're a search algorithm that's saying like,
[00:30:39.160 --> 00:30:41.300]   "Oh, I want to raise with this hand."
[00:30:41.300 --> 00:30:43.700]   Well, in order to know whether that's a good action,
[00:30:43.700 --> 00:30:44.780]   like let's say it's a bluff.
[00:30:44.780 --> 00:30:46.140]   Let's say you have a bad hand and you're saying like,
[00:30:46.140 --> 00:30:48.220]   "Oh, I think I should be betting here
[00:30:48.220 --> 00:30:50.320]   "with this really bad hand and bluffing."
[00:30:50.320 --> 00:30:52.060]   Well, that's only a good action
[00:30:52.060 --> 00:30:55.380]   if you're also betting with a strong hand.
[00:30:55.380 --> 00:30:57.020]   Otherwise, it's an obvious bluff.
[00:30:57.020 --> 00:30:59.300]   - So if your action in some sense
[00:30:59.300 --> 00:31:02.060]   maximizes your unpredictability,
[00:31:02.060 --> 00:31:04.220]   so that action could be mapped by your opponent
[00:31:04.220 --> 00:31:07.220]   to a lot of different hands, then that's a good action.
[00:31:07.220 --> 00:31:09.680]   - Basically what you wanna do is put your opponent
[00:31:09.680 --> 00:31:11.040]   into a tough spot.
[00:31:11.040 --> 00:31:13.500]   So you want them to always have some doubt,
[00:31:13.500 --> 00:31:14.600]   like, "Should I call here?
[00:31:14.600 --> 00:31:15.900]   "Should I fold here?"
[00:31:15.900 --> 00:31:18.920]   And if you are raising in the appropriate balance
[00:31:18.920 --> 00:31:20.440]   between bluffs and good hands,
[00:31:20.440 --> 00:31:21.800]   then you're putting them into that tough spot.
[00:31:21.800 --> 00:31:22.800]   And so that's what we're trying to do.
[00:31:22.800 --> 00:31:24.760]   We're always trying to search for a strategy
[00:31:24.760 --> 00:31:26.840]   that would put the opponent into a difficult position.
[00:31:26.840 --> 00:31:29.880]   - Can you give a metric that you're trying to maximize
[00:31:29.880 --> 00:31:30.720]   or minimize?
[00:31:30.720 --> 00:31:32.120]   Does this have to do with the regret thing
[00:31:32.120 --> 00:31:35.360]   that we're talking about in terms of putting your opponent
[00:31:35.360 --> 00:31:37.380]   in a maximally tough spot?
[00:31:37.380 --> 00:31:39.200]   - Yeah, ultimately what you're trying to maximize
[00:31:39.200 --> 00:31:41.740]   is your expected winnings, your expected value,
[00:31:41.740 --> 00:31:43.660]   the amount of money that you're gonna walk away from,
[00:31:43.660 --> 00:31:46.300]   assuming that your opponent was playing optimally
[00:31:46.300 --> 00:31:47.420]   in response.
[00:31:47.420 --> 00:31:49.420]   So you're gonna assume that your opponent
[00:31:49.420 --> 00:31:53.540]   is also playing as well as possible
[00:31:53.540 --> 00:31:55.100]   a Nash equilibrium approach,
[00:31:55.100 --> 00:31:56.700]   because if they're not,
[00:31:56.700 --> 00:31:59.060]   then you're just gonna make more money.
[00:31:59.060 --> 00:32:01.340]   Anything that deviates, by definition,
[00:32:01.340 --> 00:32:03.780]   the Nash equilibrium is the strategy
[00:32:03.780 --> 00:32:06.300]   that does the best in expectation.
[00:32:06.300 --> 00:32:07.860]   And so if you're deviating from that,
[00:32:07.860 --> 00:32:09.580]   then you're just, they're gonna lose money.
[00:32:09.580 --> 00:32:11.140]   And since it's a two player zero sum game,
[00:32:11.140 --> 00:32:12.420]   that means you're gonna make money.
[00:32:12.420 --> 00:32:15.300]   - So there's not an explicit, like objective function
[00:32:15.300 --> 00:32:18.860]   that maximizes the toughness of the spot they're put in.
[00:32:18.860 --> 00:32:22.060]   You're always, this is from like a self play
[00:32:22.060 --> 00:32:24.100]   reinforcement learning perspective.
[00:32:24.100 --> 00:32:26.100]   You're just trying to maximize winnings
[00:32:26.100 --> 00:32:27.780]   and the rest is implicit.
[00:32:27.780 --> 00:32:28.620]   - That's right, yeah.
[00:32:28.620 --> 00:32:30.380]   So what we're actually trying to maximize
[00:32:30.380 --> 00:32:31.780]   is the expected value,
[00:32:31.780 --> 00:32:33.420]   given that the opponent is playing optimally
[00:32:33.420 --> 00:32:34.520]   in response to us.
[00:32:34.520 --> 00:32:36.700]   Now in practice, what that ends up looking like
[00:32:36.700 --> 00:32:39.640]   is it's putting the opponent into difficult situations
[00:32:39.640 --> 00:32:41.860]   where there's no obvious decision to be made.
[00:32:41.860 --> 00:32:44.060]   - So the system doesn't know anything
[00:32:44.060 --> 00:32:46.340]   about the difficulty of the situation.
[00:32:46.340 --> 00:32:47.180]   - Not at all, doesn't care.
[00:32:47.180 --> 00:32:49.260]   - Okay, in my head it was getting excited
[00:32:49.260 --> 00:32:51.700]   whenever I was making the other, the opponent sweat.
[00:32:51.700 --> 00:32:55.260]   Okay, so you're, in 2015, you didn't do as well.
[00:32:55.260 --> 00:32:57.880]   So what's the journey from that to a system
[00:32:57.880 --> 00:33:00.380]   that in your mind could have a chance?
[00:33:00.380 --> 00:33:04.460]   - So 2015, we got, we beat pretty badly
[00:33:04.460 --> 00:33:06.740]   and we actually learned a lot from that competition.
[00:33:06.740 --> 00:33:09.580]   And in particular, what became clear to me
[00:33:09.580 --> 00:33:11.620]   is that the way the humans were approaching the game
[00:33:11.620 --> 00:33:15.000]   was very different from how the bot was approaching the game.
[00:33:15.000 --> 00:33:17.660]   The bot would not be doing search.
[00:33:17.660 --> 00:33:19.380]   It would just be trying to compute,
[00:33:19.380 --> 00:33:21.140]   it would do like months of self play.
[00:33:21.140 --> 00:33:23.180]   It would just be playing against itself for months,
[00:33:23.180 --> 00:33:24.460]   but then when it's actually playing the game,
[00:33:24.460 --> 00:33:26.020]   it would just act instantly.
[00:33:26.020 --> 00:33:28.700]   And the humans, when they're in a tough spot,
[00:33:28.700 --> 00:33:31.540]   they would sit there and think for sometimes
[00:33:31.540 --> 00:33:34.220]   even like five minutes about whether they're gonna call
[00:33:34.220 --> 00:33:35.060]   or fold a hand.
[00:33:35.060 --> 00:33:39.720]   And it became clear to me that that's,
[00:33:39.720 --> 00:33:41.540]   there's a good chance that that's what's missing
[00:33:41.540 --> 00:33:42.380]   from our bot.
[00:33:42.380 --> 00:33:45.020]   So I actually did some initial experiments
[00:33:45.020 --> 00:33:46.300]   to try to figure out how much of a difference
[00:33:46.300 --> 00:33:47.480]   does this actually make?
[00:33:47.480 --> 00:33:49.020]   And the difference was huge.
[00:33:49.020 --> 00:33:50.900]   - As a signal to the human player,
[00:33:50.900 --> 00:33:52.260]   how long you took to think?
[00:33:52.260 --> 00:33:53.100]   - No, no, no, I'm not saying
[00:33:53.100 --> 00:33:54.200]   that there were any timing tells.
[00:33:54.200 --> 00:33:55.780]   I was saying when the human,
[00:33:55.780 --> 00:33:57.380]   like the bot would always act instantly.
[00:33:57.380 --> 00:33:59.780]   It wouldn't try to come up with a better strategy
[00:33:59.780 --> 00:34:04.420]   in real time over what it had pre-computed during training.
[00:34:04.420 --> 00:34:06.580]   Whereas the human, like they have all this intuition
[00:34:06.580 --> 00:34:09.640]   about how to play, but they're also in real time
[00:34:09.640 --> 00:34:12.420]   leveraging their ability to think,
[00:34:12.420 --> 00:34:14.000]   just to search, to plan,
[00:34:14.000 --> 00:34:16.140]   and coming up with an even better strategy
[00:34:16.140 --> 00:34:17.420]   than what their intuition would say.
[00:34:17.420 --> 00:34:18.980]   - So you're saying that there's,
[00:34:18.980 --> 00:34:20.500]   you're doing, that's what you mean by
[00:34:20.500 --> 00:34:22.300]   you're doing search also.
[00:34:22.300 --> 00:34:26.700]   You have an intuition and search on top of that,
[00:34:26.700 --> 00:34:28.580]   looking for a better solution.
[00:34:28.580 --> 00:34:30.220]   - Yeah, that's what I mean by search.
[00:34:30.220 --> 00:34:33.420]   That instead of acting instantly,
[00:34:33.420 --> 00:34:36.060]   a neural net usually gives you a response
[00:34:36.060 --> 00:34:37.420]   in like a hundred milliseconds or something.
[00:34:37.420 --> 00:34:39.100]   It depends on the size of the net.
[00:34:39.100 --> 00:34:42.920]   But if you can leverage extra computational resources,
[00:34:42.920 --> 00:34:46.420]   you can possibly get a much better outcome.
[00:34:46.420 --> 00:34:48.580]   And we did some experiments
[00:34:48.580 --> 00:34:50.920]   in small scale versions of poker.
[00:34:50.920 --> 00:34:53.660]   And what we found was that
[00:34:53.660 --> 00:34:58.380]   if you do a little bit of search, even just a little bit,
[00:34:58.380 --> 00:35:01.580]   it was the equivalent of making your,
[00:35:01.580 --> 00:35:03.600]   you know, your pre-computed strategy.
[00:35:03.600 --> 00:35:05.320]   Like you can kind of think of it as your neural net,
[00:35:05.320 --> 00:35:06.980]   a thousand times bigger.
[00:35:06.980 --> 00:35:08.520]   It was just a little bit of search.
[00:35:08.520 --> 00:35:10.960]   And it just like blew away all of the research
[00:35:10.960 --> 00:35:11.800]   that we had been working on
[00:35:11.800 --> 00:35:15.820]   and trying to like scale up this like pre-computed solution.
[00:35:15.820 --> 00:35:19.740]   It was dwarfed by the benefit that we got from search.
[00:35:19.740 --> 00:35:22.440]   - Can you just linger on what you mean by search here?
[00:35:22.440 --> 00:35:26.060]   You're searching over a space of actions
[00:35:26.060 --> 00:35:29.020]   for your hand and for other hands.
[00:35:29.020 --> 00:35:32.140]   How are you selecting the other hands to search over?
[00:35:32.140 --> 00:35:34.040]   - So yeah. - Randomly?
[00:35:34.040 --> 00:35:35.780]   - No, it's all the other hands that you could have.
[00:35:35.780 --> 00:35:37.640]   So when you're playing no limit Texas hold 'em,
[00:35:37.640 --> 00:35:39.100]   you've got two face down cards.
[00:35:39.100 --> 00:35:43.860]   And so that's 52 choose two, 1,326 different combinations.
[00:35:43.860 --> 00:35:45.020]   Now that's actually a little bit lower
[00:35:45.020 --> 00:35:46.620]   because there's face up cards in the middle.
[00:35:46.620 --> 00:35:48.340]   And so you can eliminate those as well.
[00:35:48.340 --> 00:35:50.140]   But you're looking at like around a thousand
[00:35:50.140 --> 00:35:51.980]   different possible hands that you can have.
[00:35:51.980 --> 00:35:54.480]   And so when we're doing, when the bot's doing search,
[00:35:54.480 --> 00:35:56.120]   it's thinking explicitly,
[00:35:56.120 --> 00:35:58.300]   there are these thousand different hands that I could have.
[00:35:58.300 --> 00:36:00.660]   There are these thousand different hands that you could have.
[00:36:00.660 --> 00:36:03.440]   Let me try to figure out what would be a better strategy
[00:36:03.440 --> 00:36:07.780]   than what I've pre-computed for these hands and your hands.
[00:36:07.780 --> 00:36:12.780]   - Okay, so that search, how do you fuse that
[00:36:12.780 --> 00:36:15.700]   with what the neural net is telling you
[00:36:15.700 --> 00:36:19.040]   or what the train system is telling you?
[00:36:19.040 --> 00:36:22.040]   - Yeah, so you kind of like,
[00:36:22.040 --> 00:36:25.920]   where the train system comes in is the value at the end.
[00:36:25.920 --> 00:36:29.820]   So there's, you only look so far ahead.
[00:36:29.820 --> 00:36:31.800]   You look like maybe one round ahead.
[00:36:31.800 --> 00:36:32.620]   So if you're on the flop,
[00:36:32.620 --> 00:36:34.360]   you're looking to the start of the turn.
[00:36:34.360 --> 00:36:38.800]   And at that point you can use the pre-computed solution
[00:36:38.800 --> 00:36:42.900]   to figure out what's the value here of this strategy.
[00:36:42.900 --> 00:36:46.880]   - Is it of a single action, essentially in that spot?
[00:36:46.880 --> 00:36:49.760]   You're getting a value or is it the value
[00:36:49.760 --> 00:36:52.200]   of the entire series of actions?
[00:36:52.200 --> 00:36:53.500]   - Well, it's kind of both
[00:36:53.500 --> 00:36:55.560]   because you're trying to maximize the value
[00:36:55.560 --> 00:36:57.860]   for the hand that you have,
[00:36:57.860 --> 00:36:59.900]   but in the process, in order to maximize the value
[00:36:59.900 --> 00:37:01.080]   of the hand that you have,
[00:37:01.080 --> 00:37:03.080]   you have to figure out what would I be doing
[00:37:03.080 --> 00:37:04.680]   with all these other hands as well.
[00:37:04.680 --> 00:37:06.280]   - Okay, but are you in the search
[00:37:06.280 --> 00:37:09.240]   or was going to the end of the game?
[00:37:09.240 --> 00:37:11.520]   - In Libratus, we did.
[00:37:11.520 --> 00:37:14.200]   So we only use search starting on the turn.
[00:37:14.200 --> 00:37:17.000]   And then we searched all the way to the end of the game.
[00:37:17.000 --> 00:37:18.080]   - The turn, the river.
[00:37:18.080 --> 00:37:22.120]   Can we take it through the terminology?
[00:37:22.120 --> 00:37:23.600]   - Yeah, there's four rounds of poker.
[00:37:23.600 --> 00:37:26.800]   So there's the pre-flop, the flop, the turn and the river.
[00:37:26.800 --> 00:37:30.920]   And so we would start doing search halfway through the game.
[00:37:30.920 --> 00:37:33.000]   Now the first half of the game, that was all pre-computed.
[00:37:33.000 --> 00:37:34.480]   It would just act instantly.
[00:37:34.480 --> 00:37:37.080]   And then when it got to the halfway point,
[00:37:37.080 --> 00:37:39.040]   then it would always search to the end of the game.
[00:37:39.040 --> 00:37:40.120]   Now we later improved this
[00:37:40.120 --> 00:37:41.280]   so it wouldn't have to search all the way
[00:37:41.280 --> 00:37:42.120]   to the end of the game.
[00:37:42.120 --> 00:37:45.000]   It would actually search just a few moves ahead.
[00:37:45.000 --> 00:37:48.400]   But that came later and that drastically reduced
[00:37:48.400 --> 00:37:51.240]   the amount of computational resources that we needed.
[00:37:51.240 --> 00:37:53.200]   - But the moves, 'cause you can keep betting
[00:37:53.200 --> 00:37:54.160]   on top of each other.
[00:37:54.160 --> 00:37:55.080]   That's what you mean by moves.
[00:37:55.080 --> 00:37:58.800]   So like that's where you don't just get one bet
[00:37:58.800 --> 00:37:59.920]   per turn of poker.
[00:37:59.920 --> 00:38:02.640]   You can have multiple arbitrary number of bets, right?
[00:38:02.640 --> 00:38:04.600]   - Right, I'm trying to think like,
[00:38:04.600 --> 00:38:06.640]   I'm gonna bet and then what are you gonna do in response?
[00:38:06.640 --> 00:38:08.320]   Are you gonna raise me or are you gonna call?
[00:38:08.320 --> 00:38:10.120]   And then if you raise, what should I do?
[00:38:10.120 --> 00:38:12.720]   So it's reasoning about that whole process
[00:38:12.720 --> 00:38:15.560]   up until the end of the game in the case of Libratus.
[00:38:15.560 --> 00:38:18.640]   - So for Libratus, what's the most number of re-raises
[00:38:18.640 --> 00:38:19.560]   have you ever seen?
[00:38:19.560 --> 00:38:23.760]   - You probably cap out at like five or something
[00:38:23.760 --> 00:38:26.680]   because at that point you're basically all in.
[00:38:26.680 --> 00:38:30.040]   - I mean, is there like interesting patterns like that
[00:38:30.040 --> 00:38:31.840]   that you've seen that the game does?
[00:38:31.840 --> 00:38:34.600]   Like you'll have like alpha zero doing way more sacrifices
[00:38:34.600 --> 00:38:36.920]   than humans usually do.
[00:38:36.920 --> 00:38:40.840]   Is there something like Libratus was constantly re-raising
[00:38:40.840 --> 00:38:43.040]   or something like that that you've noticed?
[00:38:43.040 --> 00:38:44.480]   - There was something really interesting
[00:38:44.480 --> 00:38:46.440]   that we observed with Libratus.
[00:38:46.440 --> 00:38:49.360]   So humans, when they're playing poker,
[00:38:49.360 --> 00:38:51.480]   they usually size their bets relative
[00:38:51.480 --> 00:38:52.460]   to the size of the pot.
[00:38:52.460 --> 00:38:55.200]   So if the pot has $100 in there,
[00:38:55.200 --> 00:38:57.600]   maybe you bet like $75 or somewhere around there,
[00:38:57.600 --> 00:38:59.560]   somewhere between like 50 and $100.
[00:38:59.560 --> 00:39:03.360]   And with Libratus, we gave it the option
[00:39:03.360 --> 00:39:05.120]   to basically bet whatever it wanted.
[00:39:05.120 --> 00:39:07.560]   It was actually really easy for us to say like,
[00:39:07.560 --> 00:39:09.600]   oh, if you want, you can bet like 10 times the pot.
[00:39:09.600 --> 00:39:11.080]   And we didn't think it would actually do that.
[00:39:11.080 --> 00:39:13.840]   It was just like, why not give it the option?
[00:39:13.840 --> 00:39:15.120]   And then during the competition,
[00:39:15.120 --> 00:39:16.680]   it actually started doing this.
[00:39:16.680 --> 00:39:18.600]   And by the way, this was like a very last minute decision
[00:39:18.600 --> 00:39:19.760]   on our part to add this option.
[00:39:19.760 --> 00:39:23.640]   And so we did not think the bot would do this.
[00:39:23.640 --> 00:39:25.640]   And I was actually kind of worried
[00:39:25.640 --> 00:39:27.780]   when it did start to do this, like, oh, is this a problem?
[00:39:27.780 --> 00:39:28.720]   Like humans don't do this.
[00:39:28.720 --> 00:39:29.880]   Like is it screwing up?
[00:39:29.880 --> 00:39:33.360]   But it would put the humans into really difficult spots
[00:39:33.360 --> 00:39:34.720]   when it would do that.
[00:39:34.720 --> 00:39:38.200]   Because you could imagine like you have the second best hand
[00:39:38.200 --> 00:39:40.160]   that's possible given the board.
[00:39:40.160 --> 00:39:41.000]   And you're thinking like,
[00:39:41.000 --> 00:39:42.240]   oh, you're in a really great spot here.
[00:39:42.240 --> 00:39:46.680]   And suddenly the bot bets $20,000 into a $1,000 pot.
[00:39:46.680 --> 00:39:48.600]   And it's basically saying like,
[00:39:48.600 --> 00:39:51.860]   I have the best hand or I'm bluffing.
[00:39:51.860 --> 00:39:53.920]   And you having the second best hand,
[00:39:53.920 --> 00:39:56.440]   like now you get a really tough choice to make.
[00:39:56.440 --> 00:39:59.080]   And so the humans would sometimes think like five
[00:39:59.080 --> 00:40:01.080]   or 10 minutes about like, what do you do?
[00:40:01.080 --> 00:40:03.120]   Should I call? Should I fold?
[00:40:03.120 --> 00:40:06.000]   And when I saw the humans like really struggling
[00:40:06.000 --> 00:40:07.400]   with that decision, like that's when I realized like,
[00:40:07.400 --> 00:40:09.760]   oh, actually this is maybe a good thing to do after all.
[00:40:09.760 --> 00:40:13.440]   - And of course the system is a no that it's making,
[00:40:13.440 --> 00:40:16.880]   again, like we said, that it's putting them in a tough spot.
[00:40:16.880 --> 00:40:19.880]   It's just, that's part of the optimal,
[00:40:19.880 --> 00:40:21.280]   the game theory optimal.
[00:40:21.280 --> 00:40:22.480]   - Right, from the bot's perspective,
[00:40:22.480 --> 00:40:23.900]   it's just doing the thing
[00:40:23.900 --> 00:40:26.500]   that's going to make it the most money.
[00:40:26.500 --> 00:40:28.200]   And the fact that it's putting the humans
[00:40:28.200 --> 00:40:30.640]   in a difficult spot, like that's just, you know,
[00:40:30.640 --> 00:40:32.200]   a side effect of that.
[00:40:32.200 --> 00:40:35.040]   And this was, I think the one thing,
[00:40:35.040 --> 00:40:35.880]   I mean, there were a few things
[00:40:35.880 --> 00:40:36.840]   that the humans walked away from,
[00:40:36.840 --> 00:40:39.320]   but this was the number one thing
[00:40:39.320 --> 00:40:41.680]   that the humans walked away from the competition saying like,
[00:40:41.680 --> 00:40:43.800]   we need to start doing this.
[00:40:43.800 --> 00:40:46.200]   And now these overbets, what are called overbets,
[00:40:46.200 --> 00:40:48.760]   have become really common in high-level poker play.
[00:40:48.760 --> 00:40:50.760]   - Have you ever talked to like somebody like Daniel
[00:40:50.760 --> 00:40:52.120]   DeGranio about this?
[00:40:52.120 --> 00:40:54.120]   He seems to be a student of the game.
[00:40:54.120 --> 00:40:55.480]   - I did actually have a conversation
[00:40:55.480 --> 00:40:56.840]   with Daniel DeGranio once, yeah.
[00:40:56.840 --> 00:40:59.600]   I was visiting the Isle of Man
[00:40:59.600 --> 00:41:02.140]   to talk to poker stars about AI.
[00:41:02.140 --> 00:41:04.680]   And Daniel DeGranio was there
[00:41:04.680 --> 00:41:07.560]   when we had dinner together with some other people.
[00:41:07.560 --> 00:41:10.080]   And yeah, he was really interested in it.
[00:41:10.080 --> 00:41:11.720]   He mentioned that he was like, you know,
[00:41:11.720 --> 00:41:14.440]   excited about like learning from these AIs.
[00:41:14.440 --> 00:41:16.360]   - So he wasn't scared, he was excited.
[00:41:16.360 --> 00:41:17.200]   - He was excited.
[00:41:17.200 --> 00:41:20.000]   And he honestly, he wanted to play against the bot.
[00:41:20.000 --> 00:41:23.400]   He thought he had a decent chance of beating it.
[00:41:23.400 --> 00:41:26.920]   I think, you know, this was like several years ago
[00:41:26.920 --> 00:41:30.560]   when I think it was like not as clear to everybody
[00:41:30.560 --> 00:41:32.680]   that, you know, the AIs were taking over.
[00:41:32.680 --> 00:41:34.400]   I think now people recognize that like
[00:41:34.400 --> 00:41:36.440]   if you're playing against a bot,
[00:41:36.440 --> 00:41:38.640]   there's like no chance that you have in a game like poker.
[00:41:38.640 --> 00:41:41.120]   - So consistently the bots will win.
[00:41:41.120 --> 00:41:45.600]   The bots have heads up and in other variants too.
[00:41:45.600 --> 00:41:49.240]   So multi, six player Texas hold 'em,
[00:41:49.240 --> 00:41:51.800]   no limit Texas hold 'em, the bots win?
[00:41:51.800 --> 00:41:52.640]   - Yeah, that's the case.
[00:41:52.640 --> 00:41:54.320]   So I think there is some debate about like,
[00:41:54.320 --> 00:41:56.160]   is it true for every single variant of poker?
[00:41:56.160 --> 00:41:59.080]   I think for every single variant of poker,
[00:41:59.080 --> 00:42:00.920]   if somebody really put in the effort,
[00:42:00.920 --> 00:42:04.320]   they can make an AI that would beat all humans at it.
[00:42:04.320 --> 00:42:06.760]   We've focused on the most popular variants.
[00:42:06.760 --> 00:42:08.720]   So heads up, no limit Texas hold 'em.
[00:42:08.720 --> 00:42:13.000]   And then we followed that up with six player poker as well,
[00:42:13.000 --> 00:42:14.560]   where we managed to make a bot
[00:42:14.560 --> 00:42:16.360]   that beat expert human players.
[00:42:16.360 --> 00:42:18.720]   And I think even there now,
[00:42:18.720 --> 00:42:20.840]   it's pretty clear that humans don't stand a chance.
[00:42:20.840 --> 00:42:22.720]   - See, I would love to hook up an AI system
[00:42:22.720 --> 00:42:26.560]   that looks at EEG, like how,
[00:42:26.560 --> 00:42:29.760]   like actually tries to optimize the toughness of the spot
[00:42:29.760 --> 00:42:31.320]   it puts a human in.
[00:42:31.320 --> 00:42:34.000]   And I would love to see how different is that
[00:42:34.000 --> 00:42:35.520]   from the game theory optimal.
[00:42:35.520 --> 00:42:39.120]   So you try to maximize the heart rate of the human player,
[00:42:39.120 --> 00:42:42.720]   like the freaking out over a long period of time.
[00:42:42.720 --> 00:42:46.480]   I wonder if there's going to be different strategies
[00:42:46.480 --> 00:42:49.760]   that emerge that are close in terms of effectiveness.
[00:42:49.760 --> 00:42:53.360]   'Cause something tells me you could still be
[00:42:53.360 --> 00:42:56.440]   achieve superhuman level performance
[00:42:56.440 --> 00:42:58.560]   by just making people sweat.
[00:42:58.560 --> 00:43:00.560]   - I feel like that there's a good chance
[00:43:00.560 --> 00:43:01.600]   that that is the case, yeah.
[00:43:01.600 --> 00:43:03.840]   If you're able to see like,
[00:43:03.840 --> 00:43:06.760]   that it's like a decent proxy for score, right?
[00:43:06.760 --> 00:43:09.760]   And this is actually like the common poker wisdom
[00:43:09.760 --> 00:43:12.880]   where they're teaching players, before there were bots,
[00:43:12.880 --> 00:43:14.760]   and they were trying to teach people how to play poker.
[00:43:14.760 --> 00:43:16.560]   They would say like, the key to the game
[00:43:16.560 --> 00:43:18.600]   is to put your opponent into difficult spots.
[00:43:18.600 --> 00:43:20.520]   It's a good estimate for
[00:43:20.520 --> 00:43:21.800]   if you're making the right decision.
[00:43:21.800 --> 00:43:23.320]   - So what else can you say about
[00:43:23.320 --> 00:43:27.240]   the fundamental role of search in poker?
[00:43:27.240 --> 00:43:30.240]   And maybe if you can also relate it to chess and go
[00:43:30.240 --> 00:43:31.420]   in these games,
[00:43:31.420 --> 00:43:35.780]   what's the role of search to solving these games?
[00:43:35.780 --> 00:43:39.240]   - Yeah, I think a lot of people under,
[00:43:39.240 --> 00:43:40.760]   this is true for the general public
[00:43:40.760 --> 00:43:42.500]   and I think it's true for the AI community.
[00:43:42.500 --> 00:43:45.040]   A lot of people underestimate the importance of search
[00:43:45.040 --> 00:43:47.080]   for these kinds of game AI results.
[00:43:48.300 --> 00:43:52.720]   An example of this is TD Gammon that came out in 1992.
[00:43:52.720 --> 00:43:55.640]   This was the first real instance of a neural net
[00:43:55.640 --> 00:43:57.120]   being used in a game AI.
[00:43:57.120 --> 00:43:58.240]   It's a landmark achievement.
[00:43:58.240 --> 00:44:00.560]   It was actually the inspiration for AlphaZero
[00:44:00.560 --> 00:44:02.000]   and it used search.
[00:44:02.000 --> 00:44:05.000]   It used two-ply search to figure out its next move.
[00:44:05.000 --> 00:44:06.320]   You got Deep Blue.
[00:44:06.320 --> 00:44:09.960]   There, it was very heavily focused on search,
[00:44:09.960 --> 00:44:13.320]   looking many, many moves ahead farther than any human could.
[00:44:13.320 --> 00:44:15.640]   And that was key for why it won.
[00:44:15.640 --> 00:44:18.080]   And then even with something like AlphaGo,
[00:44:18.080 --> 00:44:21.060]   I mean, AlphaGo is commonly hailed
[00:44:21.060 --> 00:44:24.780]   as a landmark achievement for neural nets, and it is,
[00:44:24.780 --> 00:44:26.780]   but there's also this huge component of search,
[00:44:26.780 --> 00:44:29.060]   Monte Carlo Tree Search to AlphaGo,
[00:44:29.060 --> 00:44:31.420]   that was key, absolutely essential
[00:44:31.420 --> 00:44:33.420]   for the AI to be able to beat top humans.
[00:44:33.420 --> 00:44:37.740]   I think a good example of this is you look at
[00:44:37.740 --> 00:44:39.900]   the latest versions of AlphaGo,
[00:44:39.900 --> 00:44:41.560]   like it was called AlphaZero,
[00:44:41.560 --> 00:44:45.020]   and there's this metric called Elo rating
[00:44:45.020 --> 00:44:47.220]   where you can compare different humans
[00:44:47.220 --> 00:44:49.340]   and you can compare bots to humans.
[00:44:49.340 --> 00:44:53.220]   Now, a top human player is around 3,600 Elo,
[00:44:53.220 --> 00:44:55.460]   maybe a little bit higher now.
[00:44:55.460 --> 00:44:59.940]   AlphaZero, the strongest version, is around 5,200 Elo.
[00:44:59.940 --> 00:45:02.980]   But if you take out the search that's being done
[00:45:02.980 --> 00:45:05.280]   at test time, and by the way, what I mean by search
[00:45:05.280 --> 00:45:07.880]   is the planning ahead, the thinking of like,
[00:45:07.880 --> 00:45:10.320]   oh, if I move my, if I place this stone here
[00:45:10.320 --> 00:45:11.540]   and then he does this,
[00:45:11.540 --> 00:45:12.980]   and then you look like five moves ahead
[00:45:12.980 --> 00:45:15.860]   and you see like what the board state looks like,
[00:45:15.860 --> 00:45:17.220]   that's what I mean by search.
[00:45:17.220 --> 00:45:19.780]   If you take out the search that's done during the game,
[00:45:19.780 --> 00:45:21.860]   the Elo rating drops to around 3,000.
[00:45:21.860 --> 00:45:26.980]   So even today, what, seven years after AlphaGo,
[00:45:26.980 --> 00:45:29.080]   if you take out the Monte Carlo Tree Search
[00:45:29.080 --> 00:45:32.620]   that's being done at when playing against the human,
[00:45:32.620 --> 00:45:34.040]   the bots are not superhuman.
[00:45:34.040 --> 00:45:38.300]   Nobody has made a raw neural net that is superhuman in Go.
[00:45:38.300 --> 00:45:43.120]   - That's worth lingering on, that's quite profound.
[00:45:43.120 --> 00:45:45.460]   So without search, that just means
[00:45:45.460 --> 00:45:48.740]   looking at the next move and saying,
[00:45:48.740 --> 00:45:49.840]   this is the best move.
[00:45:49.840 --> 00:45:52.920]   So having a function that estimates accurately
[00:45:52.920 --> 00:45:55.940]   what the best move is without search.
[00:45:55.940 --> 00:45:58.380]   - Yeah, and all these bots, they have the,
[00:45:58.380 --> 00:46:00.700]   what's called a policy network, where it will tell you,
[00:46:00.700 --> 00:46:03.540]   this is what the neural net thinks is the next best move.
[00:46:03.540 --> 00:46:08.500]   And it's kind of like the intuition that a human has.
[00:46:08.500 --> 00:46:09.980]   You know, the human looks at the board
[00:46:09.980 --> 00:46:14.580]   and any Go or chess master will be able to tell you like,
[00:46:14.580 --> 00:46:17.780]   oh, instantly, here's what I think the right move is.
[00:46:17.780 --> 00:46:19.740]   And the bot is able to do the same thing.
[00:46:19.740 --> 00:46:22.620]   But just like how a human, grandmaster,
[00:46:22.620 --> 00:46:25.180]   can make a better decision if they have more time to think,
[00:46:25.180 --> 00:46:27.660]   when you add on this Monte Carlo Tree Search,
[00:46:27.660 --> 00:46:30.460]   the bot is able to make a better decision.
[00:46:30.460 --> 00:46:32.900]   - Yeah, I mean, of course a human is doing something
[00:46:32.900 --> 00:46:35.240]   like search in their brain, but it's not,
[00:46:35.240 --> 00:46:38.780]   I hesitate to draw a hard line,
[00:46:38.780 --> 00:46:41.700]   but it's not like a Monte Carlo Tree Search.
[00:46:41.700 --> 00:46:46.700]   It's more like sequential language model generation.
[00:46:46.700 --> 00:46:48.580]   So it's like a different, it's a,
[00:46:48.580 --> 00:46:50.820]   the neural network is doing the searching.
[00:46:50.820 --> 00:46:53.740]   I wonder what the human brain is doing in terms of searching
[00:46:53.740 --> 00:46:55.900]   'cause you're doing that like computation.
[00:46:55.900 --> 00:46:57.220]   A human is computing.
[00:46:57.220 --> 00:46:58.980]   They have intuition, they have gut,
[00:46:58.980 --> 00:47:02.260]   they have a really strong ability to estimate,
[00:47:02.260 --> 00:47:03.980]   you know, amongst the top players,
[00:47:03.980 --> 00:47:05.900]   of what is good and not position
[00:47:05.900 --> 00:47:08.300]   without calculating all the details.
[00:47:08.300 --> 00:47:10.220]   But they're still doing search in their head,
[00:47:10.220 --> 00:47:11.980]   but it's a different kind of search.
[00:47:11.980 --> 00:47:12.940]   Have you ever thought about like,
[00:47:12.940 --> 00:47:15.420]   what is the difference between the human,
[00:47:15.420 --> 00:47:17.900]   the search that the human is performing
[00:47:17.900 --> 00:47:21.180]   versus what computers are doing?
[00:47:21.180 --> 00:47:22.020]   - I have thought a lot about that,
[00:47:22.020 --> 00:47:24.180]   and I think it's a really important question.
[00:47:24.180 --> 00:47:27.820]   So the AI in Alpha and Alphas in AlphaGo
[00:47:27.820 --> 00:47:29.300]   or any of these Go AIs,
[00:47:29.300 --> 00:47:30.860]   they're all doing Monte Carlo Tree Search,
[00:47:30.860 --> 00:47:32.660]   which is a particular kind of search.
[00:47:32.660 --> 00:47:36.380]   And it's actually a symbolic tabular search.
[00:47:36.380 --> 00:47:38.860]   It uses the neural net to guide its search,
[00:47:38.860 --> 00:47:42.740]   but it isn't actually like full on neural net.
[00:47:42.740 --> 00:47:46.180]   Now, that kind of search is very successful
[00:47:46.180 --> 00:47:48.580]   in these kinds of like perfect information board games
[00:47:48.580 --> 00:47:50.060]   like chess and Go.
[00:47:50.060 --> 00:47:52.060]   But if you take it to a game like poker, for example,
[00:47:52.060 --> 00:47:52.900]   it doesn't work.
[00:47:52.900 --> 00:47:56.660]   It can't understand the concept of hidden information.
[00:47:56.660 --> 00:47:58.740]   It doesn't understand the balance that you have to strike
[00:47:58.740 --> 00:48:00.540]   between like the amount that you're raising
[00:48:00.540 --> 00:48:02.180]   versus the amount that you're calling.
[00:48:02.180 --> 00:48:04.580]   And in every one of these games,
[00:48:04.580 --> 00:48:06.660]   you see a different kind of search.
[00:48:06.660 --> 00:48:08.700]   And the human brain is able to plan
[00:48:08.700 --> 00:48:11.980]   for all these different games in a very general way.
[00:48:11.980 --> 00:48:12.900]   Now, I think that's one thing
[00:48:12.900 --> 00:48:14.380]   that we're missing from AI today.
[00:48:14.380 --> 00:48:16.260]   And I think it's a really important missing piece.
[00:48:16.260 --> 00:48:20.500]   The ability to plan and reason more generally
[00:48:20.500 --> 00:48:23.820]   across a wide variety of different settings.
[00:48:23.820 --> 00:48:26.380]   - In a way where the general reasoning
[00:48:26.380 --> 00:48:29.740]   makes you better at each one of the games, not worse.
[00:48:29.740 --> 00:48:30.900]   - Yeah, so you can kind of think of it
[00:48:30.900 --> 00:48:32.820]   as like neural nets today,
[00:48:32.820 --> 00:48:34.780]   they'll give you like Transformers, for example,
[00:48:34.780 --> 00:48:37.620]   are super general, but they'll give you,
[00:48:37.620 --> 00:48:40.540]   it'll output an answer in like 100 milliseconds.
[00:48:40.540 --> 00:48:41.380]   And if you tell it like,
[00:48:41.380 --> 00:48:43.420]   "Oh, you've got five minutes to give me a decision,
[00:48:43.420 --> 00:48:45.620]   feel free to take more time to make a better decision."
[00:48:45.620 --> 00:48:47.300]   It's not gonna know what to do with that.
[00:48:47.300 --> 00:48:50.700]   But a human, if you're playing a game like chess,
[00:48:50.700 --> 00:48:51.860]   they're gonna give you a very different answer
[00:48:51.860 --> 00:48:53.140]   depending on if you say,
[00:48:53.140 --> 00:48:54.420]   "Oh, you've got 100 milliseconds
[00:48:54.420 --> 00:48:55.780]   or you've got five minutes."
[00:48:55.780 --> 00:49:00.020]   - Yeah, I mean, people have started using
[00:49:00.020 --> 00:49:02.980]   Transformers language models in an iterative way
[00:49:02.980 --> 00:49:04.580]   that does improve the answer
[00:49:04.580 --> 00:49:08.100]   or like showing the work kind of idea.
[00:49:08.100 --> 00:49:09.020]   - Yeah, they got this thing called
[00:49:09.020 --> 00:49:10.060]   chain of thought reasoning.
[00:49:10.060 --> 00:49:11.340]   And that's, I think-
[00:49:11.340 --> 00:49:12.460]   - Super promising, right?
[00:49:12.460 --> 00:49:15.860]   - Yeah, and I think it's a good step in the right direction.
[00:49:15.860 --> 00:49:17.740]   I would kind of like say it's similar
[00:49:17.740 --> 00:49:20.340]   to Monte Carlo rollouts in a game like chess.
[00:49:20.340 --> 00:49:22.220]   There's a kind of search that you can do
[00:49:22.220 --> 00:49:23.060]   where you're saying like,
[00:49:23.060 --> 00:49:25.420]   "I'm gonna roll out my intuition and see like,
[00:49:25.420 --> 00:49:27.460]   without really thinking,
[00:49:27.460 --> 00:49:28.740]   what are the better decisions I can make
[00:49:28.740 --> 00:49:30.740]   farther down the path?
[00:49:30.740 --> 00:49:32.820]   What would I do if I just acted according to intuition
[00:49:32.820 --> 00:49:33.980]   for the next 10 moves?"
[00:49:34.980 --> 00:49:36.980]   And that gets you an improvement,
[00:49:36.980 --> 00:49:40.780]   but I think that there's much richer kinds of planning
[00:49:40.780 --> 00:49:42.140]   that we could do.
[00:49:42.140 --> 00:49:44.300]   - So when Labradors actually beat the poker players,
[00:49:44.300 --> 00:49:45.900]   what did that feel like?
[00:49:45.900 --> 00:49:46.740]   What was that?
[00:49:46.740 --> 00:49:48.980]   I mean, actually on that day,
[00:49:48.980 --> 00:49:49.980]   what were you feeling like?
[00:49:49.980 --> 00:49:51.700]   Were you nervous?
[00:49:51.700 --> 00:49:55.100]   I mean, poker was one of the games that you thought
[00:49:55.100 --> 00:49:56.540]   like is not gonna be solvable
[00:49:56.540 --> 00:49:57.700]   'cause it's the human factor.
[00:49:57.700 --> 00:50:00.220]   So at least in the narratives,
[00:50:00.220 --> 00:50:02.820]   we'll tell ourselves the human factor
[00:50:02.820 --> 00:50:05.300]   is so fundamental to the game of poker.
[00:50:05.300 --> 00:50:06.460]   - Yeah, the Labradors competition
[00:50:06.460 --> 00:50:07.820]   was super stressful for me.
[00:50:07.820 --> 00:50:11.340]   Also, I mean, I was working on this
[00:50:11.340 --> 00:50:13.100]   like basically continuously for a year
[00:50:13.100 --> 00:50:14.460]   leading up to the competition.
[00:50:14.460 --> 00:50:16.580]   I mean, for me, it became like very clear,
[00:50:16.580 --> 00:50:18.340]   like, okay, this is the search technique,
[00:50:18.340 --> 00:50:19.620]   this is the approach that we need.
[00:50:19.620 --> 00:50:21.260]   And then I spent a year working on this
[00:50:21.260 --> 00:50:23.020]   pretty much like nonstop.
[00:50:23.020 --> 00:50:24.420]   - Can we actually get into details?
[00:50:24.420 --> 00:50:26.860]   Like what programming languages is it written in?
[00:50:26.860 --> 00:50:30.220]   What's some interesting implementation details
[00:50:30.220 --> 00:50:33.340]   that are like fun/painful?
[00:50:33.340 --> 00:50:35.860]   - Yeah, so one of the interesting things about Labradors
[00:50:35.860 --> 00:50:37.620]   is that we had no idea what the bar was
[00:50:37.620 --> 00:50:39.940]   to actually beat top humans.
[00:50:39.940 --> 00:50:41.500]   We could play against like our prior bots
[00:50:41.500 --> 00:50:42.860]   and that kind of gives us some sense of like,
[00:50:42.860 --> 00:50:44.060]   are we making progress?
[00:50:44.060 --> 00:50:45.900]   Are we going in the right direction?
[00:50:45.900 --> 00:50:48.060]   But we had no idea like what the bar actually was.
[00:50:48.060 --> 00:50:50.900]   And so we threw a huge amount of resources
[00:50:50.900 --> 00:50:52.980]   at trying to make the strongest bot possible.
[00:50:52.980 --> 00:50:55.540]   So we use C++, it was parallelized.
[00:50:55.540 --> 00:50:58.340]   We were using, I think like a thousand CPUs,
[00:50:58.340 --> 00:51:00.140]   maybe more actually.
[00:51:00.140 --> 00:51:02.260]   And today that sounds like nothing,
[00:51:02.260 --> 00:51:04.740]   but for a grad student back in 2016,
[00:51:04.740 --> 00:51:06.500]   that was a huge amount of resources.
[00:51:06.500 --> 00:51:09.220]   - Well, it's still a lot for even any grad student today.
[00:51:09.220 --> 00:51:11.060]   It's still tough to get,
[00:51:11.060 --> 00:51:15.180]   or even to allow yourself to think in that,
[00:51:15.180 --> 00:51:18.900]   in terms of scale at CMU, at MIT, anything like that.
[00:51:18.900 --> 00:51:21.980]   - Yeah, and talking about terabytes of memory.
[00:51:21.980 --> 00:51:24.300]   So it was a very parallelized,
[00:51:24.300 --> 00:51:25.620]   and it had to be very fast too,
[00:51:25.620 --> 00:51:28.740]   because the more games that you could simulate,
[00:51:28.740 --> 00:51:30.140]   the stronger the bot would be.
[00:51:30.140 --> 00:51:33.500]   - So is there some like John Carmack style,
[00:51:33.500 --> 00:51:35.980]   like efficiencies you had to come up with,
[00:51:35.980 --> 00:51:39.100]   like an efficient way to represent the hand,
[00:51:39.100 --> 00:51:40.180]   all that kind of stuff?
[00:51:40.180 --> 00:51:42.460]   - There were all sorts of optimizations that I had to make
[00:51:42.460 --> 00:51:44.500]   to try to get this thing to run as fast as possible.
[00:51:44.500 --> 00:51:46.620]   They were like, how do you minimize the latency?
[00:51:46.620 --> 00:51:48.860]   How do you like, you know, package things together
[00:51:48.860 --> 00:51:50.940]   so that like you minimize the amount of communication
[00:51:50.940 --> 00:51:52.340]   between the different nodes?
[00:51:52.340 --> 00:51:55.180]   How do you like optimize the algorithms
[00:51:55.180 --> 00:51:56.860]   so that you can, you know,
[00:51:56.860 --> 00:51:58.180]   try to squeeze out more and more
[00:51:58.180 --> 00:51:59.780]   from the game that you're actually playing?
[00:51:59.780 --> 00:52:01.220]   All these kinds of different decisions
[00:52:01.220 --> 00:52:03.980]   that I, you know, had to make.
[00:52:03.980 --> 00:52:05.020]   - Just a fun question.
[00:52:05.020 --> 00:52:07.580]   What IDE did you use?
[00:52:07.580 --> 00:52:10.620]   What for C++ at the time?
[00:52:10.620 --> 00:52:12.780]   - I think I used Visual Studio actually.
[00:52:12.780 --> 00:52:13.620]   - Okay. - Yeah.
[00:52:13.620 --> 00:52:15.580]   - Is that still carried through to today?
[00:52:15.580 --> 00:52:17.060]   - VS Code is what I use today.
[00:52:17.060 --> 00:52:17.900]   It seems like it's pretty popular.
[00:52:17.900 --> 00:52:19.980]   - It's the community, basically converged on.
[00:52:19.980 --> 00:52:20.820]   Okay, cool.
[00:52:20.820 --> 00:52:25.340]   So you got this super optimized C++ system,
[00:52:25.340 --> 00:52:27.740]   and then you show up to the day of competition.
[00:52:28.220 --> 00:52:29.220]   - Yeah.
[00:52:29.220 --> 00:52:30.940]   - Humans versus machine.
[00:52:30.940 --> 00:52:34.420]   How did it feel throughout the day?
[00:52:34.420 --> 00:52:35.380]   - Super stressful.
[00:52:35.380 --> 00:52:38.340]   I mean, I thought going into it
[00:52:38.340 --> 00:52:40.020]   that we had like a 50/50 chance.
[00:52:40.020 --> 00:52:42.820]   Because basically I thought if they play
[00:52:42.820 --> 00:52:45.780]   in a totally normal style, I think we'll squeak out a win.
[00:52:45.780 --> 00:52:47.620]   But there's always a chance
[00:52:47.620 --> 00:52:49.820]   that they can find some weakness in the bot.
[00:52:49.820 --> 00:52:52.460]   And if they do, and we're playing like for 20 days,
[00:52:52.460 --> 00:52:53.660]   120,000 hands of poker.
[00:52:53.660 --> 00:52:56.340]   They have a lot of time to find weaknesses in the system.
[00:52:56.340 --> 00:52:58.820]   And if they do, we're gonna get crushed.
[00:52:58.820 --> 00:52:59.860]   And that's actually what happened
[00:52:59.860 --> 00:53:01.900]   in the previous competition.
[00:53:01.900 --> 00:53:03.300]   The humans, they started out,
[00:53:03.300 --> 00:53:05.500]   it wasn't like they were winning from the start.
[00:53:05.500 --> 00:53:06.780]   But then they found these weaknesses
[00:53:06.780 --> 00:53:08.140]   that they could take advantage of.
[00:53:08.140 --> 00:53:09.900]   And for the next 10 days,
[00:53:09.900 --> 00:53:12.980]   they were just crushing the bot, stealing money from it.
[00:53:12.980 --> 00:53:14.300]   - What were the weaknesses they found?
[00:53:14.300 --> 00:53:16.820]   Like maybe over betting was effective,
[00:53:16.820 --> 00:53:17.660]   that kind of stuff.
[00:53:17.660 --> 00:53:19.620]   So certain betting strategies worked?
[00:53:19.620 --> 00:53:21.980]   - What they found is, yeah, over betting,
[00:53:21.980 --> 00:53:23.020]   like betting certain amounts,
[00:53:23.020 --> 00:53:24.060]   the bot would have a lot of trouble
[00:53:24.060 --> 00:53:25.460]   dealing with those sizes.
[00:53:25.460 --> 00:53:29.500]   And then also, when the bot got
[00:53:29.500 --> 00:53:31.820]   into really difficult all-in situations,
[00:53:31.820 --> 00:53:35.220]   it wasn't able to, because it wasn't doing search,
[00:53:35.220 --> 00:53:37.980]   it had to clump different hands together
[00:53:37.980 --> 00:53:40.540]   and it would treat them identically.
[00:53:40.540 --> 00:53:42.700]   And so it wouldn't be able to distinguish,
[00:53:42.700 --> 00:53:44.860]   you know, like having a king high flush
[00:53:44.860 --> 00:53:46.180]   versus an ace high flush.
[00:53:46.180 --> 00:53:48.180]   And in some situations that really matters a lot.
[00:53:48.180 --> 00:53:50.660]   And so they could put the bot into those situations
[00:53:50.660 --> 00:53:52.820]   and then the bot would just bleed money.
[00:53:52.820 --> 00:53:54.100]   - Clever humans.
[00:53:54.100 --> 00:53:57.500]   Okay, so I didn't realize it was over 20 days.
[00:53:57.500 --> 00:54:02.500]   So what were the humans like over those 20 days?
[00:54:02.500 --> 00:54:04.460]   And what was the bot like?
[00:54:04.460 --> 00:54:06.700]   - So we had set up the competition, you know,
[00:54:06.700 --> 00:54:09.140]   like I said, there was $200,000 in prize money
[00:54:09.140 --> 00:54:12.100]   and they would get paid a fraction of that
[00:54:12.100 --> 00:54:14.380]   depending on how well they did relative to each other.
[00:54:14.380 --> 00:54:16.420]   So I was kind of hoping that they wouldn't work together
[00:54:16.420 --> 00:54:18.420]   to try to find weaknesses in the bot,
[00:54:18.420 --> 00:54:20.220]   but they enter the competition
[00:54:20.220 --> 00:54:22.700]   with their like number one objective being to beat the bot.
[00:54:22.700 --> 00:54:24.900]   And they didn't care about like individual glory.
[00:54:24.900 --> 00:54:26.540]   They were like, we're all gonna work as a team
[00:54:26.540 --> 00:54:28.100]   to try to take down this bot.
[00:54:28.100 --> 00:54:31.020]   And so they immediately started comparing notes.
[00:54:31.020 --> 00:54:34.380]   What they would do is they would coordinate
[00:54:34.380 --> 00:54:36.380]   looking at different parts of the strategy
[00:54:36.380 --> 00:54:39.060]   to try to, you know, find out weaknesses.
[00:54:39.060 --> 00:54:41.940]   And then at the end of the day,
[00:54:41.940 --> 00:54:43.780]   we actually sent them a log of all the hands
[00:54:43.780 --> 00:54:45.900]   that were played and what cards the bot had
[00:54:45.900 --> 00:54:46.900]   on each of those hands.
[00:54:46.900 --> 00:54:48.460]   - Oh wow. - Yeah.
[00:54:48.460 --> 00:54:49.380]   - That's gutsy.
[00:54:49.380 --> 00:54:50.380]   - Yeah, it was honestly,
[00:54:50.380 --> 00:54:52.020]   and I'm not sure why we did that in retrospect,
[00:54:52.020 --> 00:54:53.700]   but I mean, I'm glad we did it
[00:54:53.700 --> 00:54:54.780]   'cause we ended up winning anyway,
[00:54:54.780 --> 00:54:57.260]   but that if you've ever played poker before,
[00:54:57.260 --> 00:54:58.900]   like that is golden information.
[00:54:58.900 --> 00:55:01.220]   I mean, to know, usually when you play poker,
[00:55:01.220 --> 00:55:03.380]   you see about a third of the hands to show down
[00:55:03.380 --> 00:55:06.420]   and to just hand them all the cards
[00:55:06.420 --> 00:55:08.380]   that the bot had on every single hand,
[00:55:08.380 --> 00:55:11.740]   that was just a gold mine for them.
[00:55:11.740 --> 00:55:13.700]   And so then they would review the hands
[00:55:13.700 --> 00:55:14.700]   and try to see like, okay,
[00:55:14.700 --> 00:55:16.860]   could they find patterns in the bot, weaknesses?
[00:55:16.860 --> 00:55:19.500]   And could they, then they would coordinate
[00:55:19.500 --> 00:55:20.940]   and study together and try to figure out,
[00:55:20.940 --> 00:55:23.180]   okay, now this person's gonna explore
[00:55:23.180 --> 00:55:24.620]   this part of the strategy for weaknesses.
[00:55:24.620 --> 00:55:25.700]   This person's gonna explore this part
[00:55:25.700 --> 00:55:28.060]   of the strategy for weaknesses.
[00:55:28.060 --> 00:55:30.180]   - It's a kind of psychological warfare,
[00:55:30.180 --> 00:55:31.500]   showing them the hands.
[00:55:31.500 --> 00:55:32.540]   - Yeah.
[00:55:32.540 --> 00:55:33.900]   - I mean, I'm sure you didn't think of it that way,
[00:55:33.900 --> 00:55:36.900]   but like doing that means you're confident
[00:55:36.900 --> 00:55:38.740]   in the bot's ability to win.
[00:55:38.740 --> 00:55:39.980]   - Well, that's one way of putting it.
[00:55:39.980 --> 00:55:41.540]   I wasn't super confident.
[00:55:41.540 --> 00:55:42.380]   - Yeah.
[00:55:42.380 --> 00:55:44.140]   - So, going in, like I said,
[00:55:44.140 --> 00:55:46.580]   I think I had like 50/50 odds on us winning.
[00:55:46.580 --> 00:55:49.620]   When we actually, when we announced the competition,
[00:55:49.620 --> 00:55:52.820]   the poker community decided to gamble on who would win.
[00:55:52.820 --> 00:55:55.260]   And their initial odds against us were like four to one.
[00:55:55.260 --> 00:55:56.700]   They were really convinced that the humans
[00:55:56.700 --> 00:55:58.460]   were gonna pull out a win.
[00:55:58.460 --> 00:56:02.980]   The bot ended up winning for three days straight.
[00:56:02.980 --> 00:56:04.660]   And even then after three days,
[00:56:04.660 --> 00:56:06.660]   the betting odds were still just 50/50.
[00:56:06.660 --> 00:56:11.180]   And then at that point, it started to look like
[00:56:11.180 --> 00:56:13.740]   the humans were coming back.
[00:56:13.740 --> 00:56:14.900]   They started to like, you know,
[00:56:14.900 --> 00:56:17.540]   but poker is a very high variance game.
[00:56:17.540 --> 00:56:19.500]   And I think what happened is like,
[00:56:19.500 --> 00:56:21.140]   they thought that they spotted some weaknesses
[00:56:21.140 --> 00:56:22.660]   that weren't actually there.
[00:56:22.660 --> 00:56:24.100]   And then around day eight,
[00:56:24.100 --> 00:56:25.220]   it was just very clear
[00:56:25.220 --> 00:56:27.300]   that they were getting absolutely crushed.
[00:56:27.300 --> 00:56:30.380]   And from that point, I mean, for a while there,
[00:56:30.380 --> 00:56:32.460]   I was super stressed out thinking like,
[00:56:32.460 --> 00:56:33.700]   oh my God, the humans are coming back
[00:56:33.700 --> 00:56:35.460]   and they've found weaknesses
[00:56:35.460 --> 00:56:37.340]   and now we're just gonna lose the whole thing.
[00:56:37.340 --> 00:56:39.620]   But no, it ended up going in the other direction
[00:56:39.620 --> 00:56:42.780]   and the bot ended up like crushing them in the long run.
[00:56:42.780 --> 00:56:45.100]   - How did it feel at the end?
[00:56:45.100 --> 00:56:47.700]   Like as a human being,
[00:56:47.700 --> 00:56:49.340]   as a person who loves,
[00:56:49.340 --> 00:56:51.580]   appreciates the beauty of the game of poker
[00:56:51.580 --> 00:56:55.500]   and as a person who appreciates the beauty of AI,
[00:56:55.500 --> 00:56:58.740]   is there, did you feel a certain kind of way about it?
[00:56:58.740 --> 00:57:01.560]   - I felt a lot of things, man.
[00:57:01.560 --> 00:57:03.660]   I mean, at that point in my life,
[00:57:03.660 --> 00:57:05.980]   I had spent five years working on this project
[00:57:05.980 --> 00:57:09.580]   and it was a huge sense of accomplishment.
[00:57:09.580 --> 00:57:11.380]   I mean, to spend five years working on something
[00:57:11.380 --> 00:57:12.780]   and finally see it succeed.
[00:57:12.780 --> 00:57:16.060]   Yeah, I wouldn't trade that for anything in the world.
[00:57:16.060 --> 00:57:18.020]   - Yeah, because that's a real benchmark.
[00:57:18.020 --> 00:57:23.020]   It's not like getting some percent accuracy on a data set.
[00:57:23.020 --> 00:57:26.380]   This is like real, this is real world.
[00:57:26.380 --> 00:57:28.660]   It's just a game, but it's also a game
[00:57:28.660 --> 00:57:30.700]   that means a lot to a lot of people.
[00:57:30.700 --> 00:57:33.420]   And this is humans doing their best to beat the machine.
[00:57:33.420 --> 00:57:36.460]   So this is a real benchmark, unlike anything else.
[00:57:36.460 --> 00:57:39.460]   - Yeah, and I mean, this is what I had been dreaming about
[00:57:39.460 --> 00:57:41.380]   since I was like 16 playing poker,
[00:57:41.380 --> 00:57:43.200]   you know, with my friends in high school.
[00:57:43.200 --> 00:57:45.220]   The idea that you could find a strategy
[00:57:46.220 --> 00:57:48.060]   to approximate the Nash equilibrium,
[00:57:48.060 --> 00:57:51.420]   be able to beat all the poker players in the world with it.
[00:57:51.420 --> 00:57:55.540]   So to actually see that come to fruition and be realized,
[00:57:55.540 --> 00:57:58.460]   that was, it's kind of magical.
[00:57:58.460 --> 00:58:00.540]   - Yeah, especially money is on the line too.
[00:58:00.540 --> 00:58:02.940]   It's different than chess.
[00:58:02.940 --> 00:58:05.780]   And that aspect, like people get,
[00:58:05.780 --> 00:58:08.100]   that's why you want to look at betting markets
[00:58:08.100 --> 00:58:11.500]   if you want to actually understand what people really think.
[00:58:11.500 --> 00:58:14.220]   And in the same sense, poker, it's really high stakes
[00:58:14.220 --> 00:58:15.580]   'cause it's money.
[00:58:15.580 --> 00:58:18.820]   And to solve that game, that's an amazing accomplishment.
[00:58:18.820 --> 00:58:23.820]   So the leap from that to multi-way six player poker,
[00:58:23.820 --> 00:58:27.460]   what's, how difficult is that jump?
[00:58:27.460 --> 00:58:28.940]   And what are some interesting differences
[00:58:28.940 --> 00:58:32.500]   between heads up poker and multi-way poker?
[00:58:32.500 --> 00:58:34.100]   - Yeah, so I mentioned, you know,
[00:58:34.100 --> 00:58:37.100]   Nash equilibrium in two player zero-sum games.
[00:58:37.100 --> 00:58:38.180]   If you play that strategy,
[00:58:38.180 --> 00:58:40.180]   you are guaranteed to not lose an expectation
[00:58:40.180 --> 00:58:41.880]   no matter what your opponent does.
[00:58:41.880 --> 00:58:43.340]   Now, once you go to six player poker,
[00:58:43.340 --> 00:58:45.420]   you're no longer playing a two player zero-sum game.
[00:58:45.420 --> 00:58:46.580]   And so there was a lot of debate
[00:58:46.580 --> 00:58:49.220]   among the academic community and among the poker community
[00:58:49.220 --> 00:58:51.440]   about how well these techniques would extend
[00:58:51.440 --> 00:58:54.600]   beyond just two player heads up poker.
[00:58:54.600 --> 00:58:57.780]   Now, what I had come to realize is that
[00:58:57.780 --> 00:59:00.900]   the techniques actually I thought
[00:59:00.900 --> 00:59:03.300]   really would extend to six player poker
[00:59:03.300 --> 00:59:05.100]   because even though in theory,
[00:59:05.100 --> 00:59:06.980]   they don't give you these guarantees
[00:59:06.980 --> 00:59:08.820]   outside of two player zero-sum games,
[00:59:08.820 --> 00:59:11.780]   in practice, it still gives you a really strong strategy.
[00:59:11.780 --> 00:59:13.860]   Now, there were a lot of complications
[00:59:13.860 --> 00:59:16.100]   that would come up with six player poker
[00:59:16.100 --> 00:59:17.860]   besides like the game theoretic aspect.
[00:59:17.860 --> 00:59:21.100]   I mean, for one, the game is just exponentially larger.
[00:59:21.100 --> 00:59:24.460]   So the main thing that allowed us
[00:59:24.460 --> 00:59:26.560]   to go from two player to six player
[00:59:26.560 --> 00:59:29.260]   was the idea of depth limited search.
[00:59:29.260 --> 00:59:31.980]   So I said before, like, you know, we would do search,
[00:59:31.980 --> 00:59:34.020]   we would plan out, the bot would plan out
[00:59:34.020 --> 00:59:35.540]   like what it's going to do next
[00:59:35.540 --> 00:59:37.100]   and for the next several moves.
[00:59:37.100 --> 00:59:39.380]   And in Libratus, that search was done
[00:59:39.380 --> 00:59:41.340]   extending all the way to the end of the game.
[00:59:41.340 --> 00:59:46.340]   So it would have to start from the turn onwards,
[00:59:46.340 --> 00:59:49.540]   like looking maybe 10 moves ahead,
[00:59:49.540 --> 00:59:51.340]   it would have to figure out
[00:59:51.340 --> 00:59:53.580]   what it was doing for all those moves.
[00:59:53.580 --> 00:59:55.060]   Now, when you get to six player poker,
[00:59:55.060 --> 00:59:57.260]   it can't do that exhaustive search anymore
[00:59:57.260 --> 00:59:59.160]   'cause the game is just way too large.
[00:59:59.160 --> 01:00:03.060]   But by only having to look a few moves ahead
[01:00:03.060 --> 01:00:06.140]   and then stopping there and substituting a value estimate
[01:00:06.140 --> 01:00:08.700]   of like how good is that strategy at that point,
[01:00:08.700 --> 01:00:12.200]   then we're able to do a much more scalable form of search.
[01:00:12.200 --> 01:00:15.300]   - Is there something cool,
[01:00:15.300 --> 01:00:17.060]   looking at the paper right now,
[01:00:17.060 --> 01:00:20.220]   is there something cool in the paper in terms of graphics?
[01:00:20.220 --> 01:00:22.460]   A game tree traversal via Monte Carlo.
[01:00:22.460 --> 01:00:24.020]   - I think if you go down a bit.
[01:00:24.020 --> 01:00:29.220]   - Figure one, an example of equilibrium selection problem.
[01:00:29.220 --> 01:00:30.980]   Ooh, so yeah.
[01:00:30.980 --> 01:00:32.820]   What do we know about equilibria
[01:00:32.820 --> 01:00:34.780]   when there's multiple players?
[01:00:34.780 --> 01:00:38.120]   - So when you go outside of two players, you're a sum.
[01:00:38.120 --> 01:00:39.980]   So a Nash equilibrium is a set of strategies,
[01:00:39.980 --> 01:00:41.680]   like one strategy for each player,
[01:00:41.680 --> 01:00:43.380]   where no player has an incentive
[01:00:43.380 --> 01:00:45.340]   to switch to a different strategy.
[01:00:45.340 --> 01:00:48.300]   And so you can kind of think of it as like,
[01:00:48.300 --> 01:00:51.620]   imagine you have a game where there's a ring.
[01:00:51.620 --> 01:00:52.740]   That's actually the visual here.
[01:00:52.740 --> 01:00:55.320]   You got a ring and the object of the game
[01:00:55.320 --> 01:00:58.780]   is to be as far away from the other players as possible.
[01:00:58.780 --> 01:01:01.540]   There's a Nash equilibrium is for all the players
[01:01:01.540 --> 01:01:04.580]   to be spaced equally apart around this ring.
[01:01:04.580 --> 01:01:06.660]   But there's infinitely many different Nash equilibria.
[01:01:06.660 --> 01:01:08.100]   There's infinitely many ways
[01:01:08.100 --> 01:01:11.100]   to space four dots along a ring.
[01:01:11.100 --> 01:01:14.220]   And if every single player independently
[01:01:14.220 --> 01:01:16.220]   computes a Nash equilibrium,
[01:01:16.220 --> 01:01:18.900]   then there's no guarantee that the joint strategy
[01:01:18.900 --> 01:01:22.680]   that they're all playing is going to be a Nash equilibrium.
[01:01:22.680 --> 01:01:24.580]   They're just gonna be like random dots
[01:01:24.580 --> 01:01:25.540]   scattered along this ring,
[01:01:25.540 --> 01:01:27.480]   rather than four coordinated dots
[01:01:27.480 --> 01:01:28.820]   being equally spaced apart.
[01:01:28.820 --> 01:01:30.380]   - Is it possible to sort of optimally
[01:01:30.380 --> 01:01:31.780]   do this kind of selection,
[01:01:32.940 --> 01:01:37.620]   to do the selection of the equilibria you're chasing?
[01:01:37.620 --> 01:01:40.220]   So is there like a meta problem to be solved here?
[01:01:40.220 --> 01:01:42.540]   - So the meta problem is in some sense,
[01:01:42.540 --> 01:01:45.340]   how do you understand the Nash equilibria
[01:01:45.340 --> 01:01:47.340]   that the other players are going to play?
[01:01:47.340 --> 01:01:51.420]   And even if you do that, again,
[01:01:51.420 --> 01:01:53.140]   there's no guarantee that you're going to win.
[01:01:53.140 --> 01:01:58.140]   So, if you're playing risk, like I said,
[01:01:58.140 --> 01:02:00.980]   and all the other players decide to team up against you,
[01:02:00.980 --> 01:02:01.800]   you're gonna lose.
[01:02:01.800 --> 01:02:03.940]   Nash equilibrium doesn't help you there.
[01:02:03.940 --> 01:02:06.420]   And so there is this big debate about
[01:02:06.420 --> 01:02:08.540]   whether Nash equilibrium and all these techniques
[01:02:08.540 --> 01:02:10.460]   that compute it are even useful
[01:02:10.460 --> 01:02:13.080]   once you go outside of two player zero-sum games.
[01:02:13.080 --> 01:02:15.160]   Now, I think for many games,
[01:02:15.160 --> 01:02:17.020]   there is a valid criticism here.
[01:02:17.020 --> 01:02:18.080]   And I think when we talk about,
[01:02:18.080 --> 01:02:19.800]   when we go to something like diplomacy,
[01:02:19.800 --> 01:02:23.500]   we run into this issue that the approach
[01:02:23.500 --> 01:02:25.980]   of trying to approximate a Nash equilibrium
[01:02:25.980 --> 01:02:27.820]   doesn't really work anymore.
[01:02:27.820 --> 01:02:30.520]   But it turns out that in six player poker,
[01:02:30.520 --> 01:02:33.220]   because six player poker is such an adversarial game,
[01:02:33.220 --> 01:02:35.820]   where none of the players
[01:02:35.820 --> 01:02:38.160]   really try to work with each other,
[01:02:38.160 --> 01:02:40.180]   the techniques that were used in two player poker
[01:02:40.180 --> 01:02:41.960]   to try to approximate an equilibrium,
[01:02:41.960 --> 01:02:43.820]   those still end up working in practice
[01:02:43.820 --> 01:02:45.340]   in six player poker as well.
[01:02:45.340 --> 01:02:49.200]   - There's some deep way in which six player poker
[01:02:49.200 --> 01:02:53.700]   is just a bunch of heads up poker, like games in one.
[01:02:53.700 --> 01:02:55.540]   It's like embedded in it.
[01:02:55.540 --> 01:03:00.100]   So the competitiveness is more fundamental to poker
[01:03:00.100 --> 01:03:01.620]   than the cooperation.
[01:03:01.620 --> 01:03:02.440]   - Right, yeah.
[01:03:02.440 --> 01:03:03.780]   Poker is just such an adversarial game.
[01:03:03.780 --> 01:03:05.280]   There's no real cooperation.
[01:03:05.280 --> 01:03:07.380]   In fact, you're not even allowed to cooperate in poker.
[01:03:07.380 --> 01:03:08.340]   It's considered collusion.
[01:03:08.340 --> 01:03:09.500]   It's against the rules.
[01:03:09.500 --> 01:03:12.260]   And so for that reason,
[01:03:12.260 --> 01:03:13.680]   the techniques end up working really well.
[01:03:13.680 --> 01:03:16.120]   And I think that's true more broadly
[01:03:16.120 --> 01:03:18.300]   in extremely adversarial games in general.
[01:03:18.300 --> 01:03:20.100]   - But that's sort of in practice
[01:03:20.100 --> 01:03:22.380]   versus being able to prove something.
[01:03:22.380 --> 01:03:23.200]   - That's right.
[01:03:23.200 --> 01:03:24.360]   Nobody has a proof that that's the case.
[01:03:24.360 --> 01:03:26.780]   And it could be that six player poker
[01:03:26.780 --> 01:03:28.760]   belongs to some class of games
[01:03:28.760 --> 01:03:33.340]   where approximating an equilibrium through self-play
[01:03:33.340 --> 01:03:34.580]   provably works well.
[01:03:34.580 --> 01:03:37.520]   And there are other classes of games
[01:03:37.520 --> 01:03:39.080]   beyond just two players, zero sum,
[01:03:39.080 --> 01:03:40.900]   where this is proven to work well.
[01:03:40.900 --> 01:03:43.220]   So there are these kinds of games called potential games,
[01:03:43.220 --> 01:03:44.360]   which I won't go into.
[01:03:44.360 --> 01:03:45.860]   It's kind of like a complicated concept,
[01:03:45.860 --> 01:03:49.480]   but there are classes of games
[01:03:49.480 --> 01:03:53.200]   where this approach to approximating an equilibrium
[01:03:53.200 --> 01:03:54.800]   is proven to work well.
[01:03:54.800 --> 01:03:57.060]   Now, six player poker is not known to belong
[01:03:57.060 --> 01:03:57.920]   to one of those classes,
[01:03:57.920 --> 01:03:59.860]   but it is possible that there is some classic games
[01:03:59.860 --> 01:04:01.820]   where it either provably performs well
[01:04:01.820 --> 01:04:04.200]   or provably performs not that badly.
[01:04:04.200 --> 01:04:08.180]   - So what are some interesting things about Pleribus
[01:04:08.180 --> 01:04:10.900]   that was able to achieve human level performance
[01:04:10.900 --> 01:04:13.660]   on this or superhuman level performance
[01:04:13.660 --> 01:04:16.180]   on the six player version of poker?
[01:04:16.180 --> 01:04:18.300]   - Personally, I think the most interesting thing
[01:04:18.300 --> 01:04:22.780]   about Pleribus is that it was so much cheaper than Libratus.
[01:04:22.780 --> 01:04:25.540]   I mean, Libratus, if you had to put a price tag
[01:04:25.540 --> 01:04:27.740]   on the computational resources that went into it,
[01:04:27.740 --> 01:04:31.160]   I would say the final training run took about $100,000.
[01:04:31.160 --> 01:04:34.360]   You go to Pleribus, the final training run
[01:04:34.360 --> 01:04:37.940]   would cost like less than $150 on AWS.
[01:04:37.940 --> 01:04:41.540]   - Is this normalized to computational inflation?
[01:04:41.540 --> 01:04:46.200]   So meaning, does this just have to do with the fact
[01:04:46.200 --> 01:04:49.120]   that Pleribus was trained like a year later?
[01:04:49.120 --> 01:04:50.240]   - No, no, no, it's not.
[01:04:50.240 --> 01:04:51.920]   I mean, first of all, like, yeah,
[01:04:51.920 --> 01:04:55.000]   computing resources are getting cheaper every day,
[01:04:55.000 --> 01:04:57.680]   but you're not gonna see a thousand fold decrease
[01:04:57.680 --> 01:05:00.600]   in the computational resources over two years
[01:05:00.600 --> 01:05:02.060]   or even anywhere close to that.
[01:05:02.060 --> 01:05:04.720]   The real improvement was algorithmic improvements
[01:05:04.720 --> 01:05:08.440]   and in particular, the ability to do depth limited search.
[01:05:08.440 --> 01:05:12.420]   - So does depth limited search also work for Libratus?
[01:05:12.420 --> 01:05:13.260]   - Yeah, yes.
[01:05:13.260 --> 01:05:15.760]   So where this depth limited search came from is,
[01:05:15.760 --> 01:05:17.440]   you know, I developed this technique
[01:05:17.440 --> 01:05:21.080]   and ran it on two player poker first
[01:05:21.080 --> 01:05:24.140]   and that reduced the computational resources needed
[01:05:24.140 --> 01:05:26.240]   to make an AI that was superhuman
[01:05:26.240 --> 01:05:28.800]   from, you know, $100,000 for Libratus
[01:05:28.800 --> 01:05:31.600]   to something you could train on your laptop.
[01:05:31.600 --> 01:05:35.940]   - What do you learn from that, from that discovery?
[01:05:35.940 --> 01:05:38.080]   - What I would take away from that
[01:05:38.080 --> 01:05:40.200]   is that algorithmic improvements really do matter.
[01:05:40.200 --> 01:05:43.360]   - How would you describe the more general case
[01:05:43.360 --> 01:05:45.200]   of limited depth search?
[01:05:45.200 --> 01:05:48.120]   So it's basically constraining the scale, temporal,
[01:05:48.120 --> 01:05:51.400]   or in some other way of the computation you're doing,
[01:05:51.400 --> 01:05:53.280]   in some clever way.
[01:05:53.280 --> 01:05:56.700]   So like with, like how else can you significantly
[01:05:56.700 --> 01:05:59.640]   constrain computation, right?
[01:05:59.640 --> 01:06:02.200]   - Well, I think the idea is that we want to be able
[01:06:02.200 --> 01:06:04.160]   to leverage search as much as possible.
[01:06:04.160 --> 01:06:05.960]   And the way that we were doing it in Libratus
[01:06:05.960 --> 01:06:08.600]   required us to search all the way to the end of the game.
[01:06:08.600 --> 01:06:09.960]   Now, if you're playing a game like chess,
[01:06:09.960 --> 01:06:11.440]   the idea that you're gonna search always
[01:06:11.440 --> 01:06:14.120]   to the end of the game is kind of unimaginable, right?
[01:06:14.120 --> 01:06:15.360]   Like there's just so many situations
[01:06:15.360 --> 01:06:17.480]   where you just won't be able to use search in that case
[01:06:17.480 --> 01:06:20.960]   or the cost would be, you know, prohibitive.
[01:06:20.960 --> 01:06:25.860]   And this technique allowed us to leverage search
[01:06:25.860 --> 01:06:27.920]   and without having to pay such a huge
[01:06:27.920 --> 01:06:29.480]   computational cost for it,
[01:06:29.480 --> 01:06:31.760]   and be able to apply it more broadly.
[01:06:31.760 --> 01:06:33.920]   - So to what degree did you use neural nets
[01:06:33.920 --> 01:06:36.600]   for Libratus and Pleribus?
[01:06:36.600 --> 01:06:40.320]   And more generally, what role do neural nets have to play
[01:06:40.320 --> 01:06:44.640]   in superhuman level performance in poker?
[01:06:44.640 --> 01:06:46.760]   - So we actually did not use neural nets at all
[01:06:46.760 --> 01:06:49.220]   for Libratus or Pleribus.
[01:06:49.220 --> 01:06:52.920]   And a lot of people found this surprising back in 2017.
[01:06:52.920 --> 01:06:55.440]   I think they find it surprising today
[01:06:55.440 --> 01:06:58.540]   that we were able to do this without using any neural nets.
[01:06:58.540 --> 01:07:01.360]   And I think the reason for that,
[01:07:01.360 --> 01:07:04.920]   I mean, I think neural nets are incredibly powerful
[01:07:04.920 --> 01:07:06.840]   and the techniques that are used today,
[01:07:06.840 --> 01:07:11.760]   even for poker AIs, do rely quite heavily on neural nets.
[01:07:11.760 --> 01:07:14.160]   But it wasn't the main challenge for poker.
[01:07:14.160 --> 01:07:17.320]   Like I think what neural nets are really good for,
[01:07:17.320 --> 01:07:20.440]   if you're in a situation where finding features
[01:07:20.440 --> 01:07:23.000]   for a value function is really difficult,
[01:07:23.000 --> 01:07:24.720]   then neural nets are really powerful.
[01:07:24.720 --> 01:07:26.360]   And this was the problem in Go, right?
[01:07:26.360 --> 01:07:28.760]   Like the problem in Go was that,
[01:07:28.760 --> 01:07:30.600]   or the final problem in Go at least,
[01:07:30.600 --> 01:07:33.880]   was that nobody had a good way of looking at a board
[01:07:33.880 --> 01:07:35.920]   and figuring out who was winning or losing,
[01:07:35.920 --> 01:07:38.500]   describing through a simple algorithm
[01:07:38.500 --> 01:07:40.300]   who was winning or losing.
[01:07:40.300 --> 01:07:42.800]   And so there neural nets were super helpful
[01:07:42.800 --> 01:07:44.680]   because you could just feed in a ton
[01:07:44.680 --> 01:07:46.920]   of different board positions into this neural net,
[01:07:46.920 --> 01:07:48.360]   and it would be able to predict then
[01:07:48.360 --> 01:07:49.800]   who was winning or losing.
[01:07:49.800 --> 01:07:53.400]   But in poker, the features weren't the challenge.
[01:07:53.400 --> 01:07:57.640]   The challenge was how do you design a scalable algorithm
[01:07:57.640 --> 01:08:00.920]   that would allow you to find this balanced strategy
[01:08:00.920 --> 01:08:03.660]   that would understand that you have to bluff
[01:08:03.660 --> 01:08:05.560]   with the right probability?
[01:08:05.560 --> 01:08:08.000]   - So can that be somehow incorporated
[01:08:08.000 --> 01:08:10.040]   into the value function?
[01:08:10.040 --> 01:08:14.860]   The complexity of poker that you've described?
[01:08:14.860 --> 01:08:17.360]   - Yeah, so the way the value functions work in poker,
[01:08:17.360 --> 01:08:19.260]   like the latest and greatest poker AIs,
[01:08:19.260 --> 01:08:22.020]   they do use neural nets for the value function.
[01:08:22.020 --> 01:08:24.380]   The way it's done is very different
[01:08:24.380 --> 01:08:26.360]   from how it's done in a game like chess or Go,
[01:08:26.360 --> 01:08:31.100]   because in poker, you have to reason about beliefs.
[01:08:31.100 --> 01:08:35.600]   And so the value of a state depends on the beliefs
[01:08:35.600 --> 01:08:39.300]   that players have about what the different cards are.
[01:08:39.300 --> 01:08:41.700]   Like if you have pocket aces,
[01:08:41.700 --> 01:08:44.460]   then whether that's a really, really good hand
[01:08:44.460 --> 01:08:46.980]   or just an okay hand depends on whether you know
[01:08:46.980 --> 01:08:48.440]   I have pocket aces.
[01:08:48.440 --> 01:08:50.580]   Rather, if you know that I have pocket aces,
[01:08:50.580 --> 01:08:53.420]   then if I bet, you're gonna fold immediately.
[01:08:53.420 --> 01:08:55.720]   But if you think that I have a really bad hand,
[01:08:55.720 --> 01:08:58.320]   then I could bet with pocket aces and make a ton of money.
[01:08:58.320 --> 01:09:02.740]   So the value function in poker these days
[01:09:02.740 --> 01:09:05.100]   takes the beliefs as an input,
[01:09:05.100 --> 01:09:08.200]   which is very different from how chess and Go AIs work.
[01:09:08.200 --> 01:09:11.440]   - So as a person who appreciates the game,
[01:09:13.700 --> 01:09:16.940]   who do you think is the greatest poker player of all time?
[01:09:16.940 --> 01:09:19.140]   - That's a tough question.
[01:09:19.140 --> 01:09:20.900]   - Can AI help answer that question?
[01:09:20.900 --> 01:09:24.860]   Can you actually analyze the quality of play?
[01:09:24.860 --> 01:09:28.940]   So the chess engines can give estimates
[01:09:28.940 --> 01:09:30.100]   of the quality of play.
[01:09:30.100 --> 01:09:34.060]   I wonder if there's a,
[01:09:34.060 --> 01:09:37.700]   is there an Elo rating type of system for poker?
[01:09:37.700 --> 01:09:40.200]   I suppose you could, but there's just not enough.
[01:09:41.220 --> 01:09:43.700]   You would have to play a lot of games, right?
[01:09:43.700 --> 01:09:45.100]   A very large number of games,
[01:09:45.100 --> 01:09:46.380]   like more than you would in chess.
[01:09:46.380 --> 01:09:49.740]   The deterministic game makes it easier to estimate Elo.
[01:09:49.740 --> 01:09:50.580]   I think.
[01:09:50.580 --> 01:09:52.660]   - I think it is much harder to estimate
[01:09:52.660 --> 01:09:54.180]   something like Elo rating in poker.
[01:09:54.180 --> 01:09:55.320]   I think it's doable.
[01:09:55.320 --> 01:09:57.700]   The problem is that the game is very high variance.
[01:09:57.700 --> 01:09:59.140]   So you could play,
[01:09:59.140 --> 01:10:02.100]   you could be profitable in poker for a year
[01:10:02.100 --> 01:10:03.540]   and you could actually be a bad player
[01:10:03.540 --> 01:10:05.260]   just because the variance is so high.
[01:10:05.260 --> 01:10:07.340]   I mean, you've got top professional poker players
[01:10:07.340 --> 01:10:08.980]   that would lose for a year
[01:10:08.980 --> 01:10:12.300]   just because they're on a really bad streak.
[01:10:12.300 --> 01:10:16.180]   - So for Elo, you have to have a nice clean way of saying
[01:10:16.180 --> 01:10:20.700]   if player A played player B and A beats B,
[01:10:20.700 --> 01:10:22.700]   that says something, that's a signal.
[01:10:22.700 --> 01:10:24.580]   In poker, that's a very noisy signal.
[01:10:24.580 --> 01:10:25.540]   - It's a very noisy signal.
[01:10:25.540 --> 01:10:26.500]   Now there is a signal there.
[01:10:26.500 --> 01:10:29.300]   And so you could do this calculation.
[01:10:29.300 --> 01:10:31.540]   It would just be much harder.
[01:10:31.540 --> 01:10:35.180]   But the same way that AIs have now taken over chess
[01:10:35.180 --> 01:10:40.180]   and all the top professional chess players train with AIs,
[01:10:40.180 --> 01:10:42.220]   the same is true for poker.
[01:10:42.220 --> 01:10:46.180]   The game has become a very computational,
[01:10:46.180 --> 01:10:48.220]   people train with AIs to try to find out
[01:10:48.220 --> 01:10:49.900]   where they're making mistakes,
[01:10:49.900 --> 01:10:52.980]   try to learn from the AIs to improve their strategy.
[01:10:52.980 --> 01:10:57.740]   So now, yeah, so the game has been revolutionized
[01:10:57.740 --> 01:11:00.180]   in the past five years by the development of AI
[01:11:00.180 --> 01:11:01.020]   in this sport.
[01:11:01.020 --> 01:11:03.060]   - The skill with which you avoided the question
[01:11:03.060 --> 01:11:05.180]   of the greatest of all time was impressive.
[01:11:05.180 --> 01:11:08.020]   - So my feeling is that it's a difficult question
[01:11:08.020 --> 01:11:10.620]   because just like in chess,
[01:11:10.620 --> 01:11:13.220]   where you can't really compare Magnus Carlsen today
[01:11:13.220 --> 01:11:17.260]   to Garry Kasparov, because the game has evolved so much.
[01:11:17.260 --> 01:11:23.180]   The poker players today are so far beyond the skills
[01:11:23.180 --> 01:11:27.420]   of people that were playing even 10 or 20 years ago.
[01:11:27.420 --> 01:11:30.980]   So you look at the kinds of all-stars that were on ESPN
[01:11:30.980 --> 01:11:33.260]   at the height of the poker boom,
[01:11:33.260 --> 01:11:35.540]   pretty much all those players are actually not that good
[01:11:35.540 --> 01:11:39.380]   at the game today, at least the strategy aspect.
[01:11:39.380 --> 01:11:42.180]   I mean, they might still be good at reading the player
[01:11:42.180 --> 01:11:44.340]   at the other side of the table and trying to figure out
[01:11:44.340 --> 01:11:45.620]   are they bluffing or not?
[01:11:45.620 --> 01:11:48.340]   But in terms of the actual computational strategy
[01:11:48.340 --> 01:11:50.860]   of the game, a lot of them have really struggled
[01:11:50.860 --> 01:11:52.780]   to keep up with that development.
[01:11:52.780 --> 01:11:55.900]   Now, so for that reason, I'll give an answer
[01:11:55.900 --> 01:11:58.140]   and I'm gonna say Daniel Legranio,
[01:11:58.140 --> 01:11:59.820]   who you actually had on the podcast recently,
[01:11:59.820 --> 01:12:00.980]   I saw it was a great episode.
[01:12:00.980 --> 01:12:02.300]   - I love this so much.
[01:12:02.300 --> 01:12:03.660]   (laughing)
[01:12:03.660 --> 01:12:05.780]   And Phil's gonna hate this so much.
[01:12:05.780 --> 01:12:08.540]   - And I'm gonna give him credit
[01:12:08.540 --> 01:12:11.580]   because he is one of the few old school,
[01:12:11.580 --> 01:12:14.180]   really strong players that have kept up
[01:12:14.180 --> 01:12:15.180]   with the development of AI.
[01:12:15.180 --> 01:12:17.860]   - So he is trying to, he's constantly studying
[01:12:17.860 --> 01:12:19.780]   the game theory optimal way of playing.
[01:12:19.780 --> 01:12:20.900]   - Exactly, yeah.
[01:12:20.900 --> 01:12:23.260]   And I think a lot of the old school poker players
[01:12:23.260 --> 01:12:24.780]   have just kind of given up on that aspect
[01:12:24.780 --> 01:12:26.860]   and I gotta give Daniel Legranio credit
[01:12:26.860 --> 01:12:29.620]   for keeping up with all the developments
[01:12:29.620 --> 01:12:31.380]   that are happening in the sport.
[01:12:31.380 --> 01:12:32.500]   - Yeah, it's fascinating to watch.
[01:12:32.500 --> 01:12:34.740]   It's fascinating to watch where it's headed.
[01:12:34.740 --> 01:12:38.260]   Yeah, so there you go, some love for Daniel.
[01:12:38.260 --> 01:12:40.780]   Quick pause, bath and break?
[01:12:40.780 --> 01:12:42.180]   - Yeah, let's do it.
[01:12:42.180 --> 01:12:45.260]   - Let's go from poker to diplomacy.
[01:12:45.260 --> 01:12:48.320]   What is at a high level the game of diplomacy?
[01:12:48.320 --> 01:12:51.180]   - Yeah, so I talked a lot about two players,
[01:12:51.180 --> 01:12:52.020]   zero sum games.
[01:12:52.020 --> 01:12:54.340]   And what's interesting about diplomacy
[01:12:54.340 --> 01:12:59.340]   is that it's very different from these adversarial games
[01:12:59.500 --> 01:13:02.820]   like chess, go, poker, even Starcraft and Dota.
[01:13:02.820 --> 01:13:05.900]   Diplomacy has a much bigger cooperative element to it.
[01:13:05.900 --> 01:13:07.580]   It's a seven player game.
[01:13:07.580 --> 01:13:10.220]   It was actually created in the fifties
[01:13:10.220 --> 01:13:13.700]   and it takes place before World War I.
[01:13:13.700 --> 01:13:16.780]   It's like a map of Europe with seven great powers
[01:13:16.780 --> 01:13:20.540]   and they're all trying to form alliances with each other.
[01:13:20.540 --> 01:13:22.440]   There's a lot of negotiation going on.
[01:13:22.440 --> 01:13:25.540]   And so the whole focus of the game
[01:13:25.540 --> 01:13:28.980]   is on forming alliances with the other players
[01:13:28.980 --> 01:13:30.340]   to take on the other players.
[01:13:30.340 --> 01:13:32.740]   - England, Germany, Russia, Turkey,
[01:13:32.740 --> 01:13:35.580]   Austria, Hungary, Italy, and France.
[01:13:35.580 --> 01:13:36.900]   - That's right, yeah.
[01:13:36.900 --> 01:13:41.000]   So the way the game works is on each turn,
[01:13:41.000 --> 01:13:43.820]   you spend about five to 15 minutes
[01:13:43.820 --> 01:13:46.220]   talking to the other players in privates
[01:13:46.220 --> 01:13:48.820]   and you make all sorts of deals with them.
[01:13:48.820 --> 01:13:50.820]   You say like, "Hey, let's work together.
[01:13:50.820 --> 01:13:53.040]   Let's team up against this other player."
[01:13:53.040 --> 01:13:54.660]   Because the only way that you can make progress
[01:13:54.660 --> 01:13:57.560]   is by working with somebody else against the others.
[01:13:58.740 --> 01:14:01.300]   And then after that negotiation period is done,
[01:14:01.300 --> 01:14:05.440]   all the players simultaneously submit their moves
[01:14:05.440 --> 01:14:07.780]   and they're all executed at the same time.
[01:14:07.780 --> 01:14:09.280]   And so you can tell people like,
[01:14:09.280 --> 01:14:11.780]   "Hey, I'm gonna support you this turn,"
[01:14:11.780 --> 01:14:13.260]   but then you don't follow through with it.
[01:14:13.260 --> 01:14:14.980]   And they're only gonna figure that out
[01:14:14.980 --> 01:14:16.980]   once they see the moves being read off.
[01:14:16.980 --> 01:14:18.540]   - How much of it is natural language,
[01:14:18.540 --> 01:14:21.060]   like written actual text?
[01:14:21.060 --> 01:14:22.780]   How much is like,
[01:14:22.780 --> 01:14:25.920]   you're actually saying phrases that are structured?
[01:14:25.920 --> 01:14:27.940]   - So there's different ways to play the game.
[01:14:27.940 --> 01:14:29.140]   You can play it in person,
[01:14:29.140 --> 01:14:31.780]   and in that case, it's all natural language.
[01:14:31.780 --> 01:14:32.940]   Free form communication.
[01:14:32.940 --> 01:14:34.500]   There's no constraints on the kinds of deals
[01:14:34.500 --> 01:14:37.700]   that you can make, the kinds of things that you can discuss.
[01:14:37.700 --> 01:14:38.960]   You can also play it online.
[01:14:38.960 --> 01:14:41.660]   So you can send long emails back and forth.
[01:14:41.660 --> 01:14:46.580]   You can play it live online or over voice chat.
[01:14:46.580 --> 01:14:49.020]   But the focus, the important thing to understand
[01:14:49.020 --> 01:14:51.060]   is that this is unstructured communication.
[01:14:51.060 --> 01:14:52.980]   You can say whatever you want.
[01:14:52.980 --> 01:14:54.660]   You can make any sorts of deals that you want
[01:14:54.660 --> 01:14:56.940]   and everything is done privately.
[01:14:56.940 --> 01:15:00.020]   So it's not like you're all around the board together
[01:15:00.020 --> 01:15:01.620]   having a conversation.
[01:15:01.620 --> 01:15:03.460]   You're grabbing somebody going off into a corner
[01:15:03.460 --> 01:15:05.720]   and conspiring behind everybody else's back
[01:15:05.720 --> 01:15:07.020]   about what you're planning.
[01:15:07.020 --> 01:15:10.780]   - And there's no limit in theory to the conversation
[01:15:10.780 --> 01:15:12.580]   you can have directly with one person.
[01:15:12.580 --> 01:15:13.420]   - That's right.
[01:15:13.420 --> 01:15:14.580]   You can make all sorts of,
[01:15:14.580 --> 01:15:15.460]   you can talk about anything.
[01:15:15.460 --> 01:15:16.300]   You can say like,
[01:15:16.300 --> 01:15:17.780]   "Hey, let's have a long-term alliance against this guy."
[01:15:17.780 --> 01:15:18.620]   You can say like,
[01:15:18.620 --> 01:15:20.020]   "Hey, can you support me this turn?
[01:15:20.020 --> 01:15:23.020]   And in return, I'll do this other thing for you next turn."
[01:15:23.020 --> 01:15:24.720]   Or, you know, yeah,
[01:15:24.720 --> 01:15:26.960]   just you can talk about like what you talked about
[01:15:26.960 --> 01:15:27.800]   with somebody else
[01:15:27.800 --> 01:15:30.760]   and gossip about like what they're planning.
[01:15:30.760 --> 01:15:32.060]   The way that I would describe the game
[01:15:32.060 --> 01:15:34.840]   is that it's kind of like a mix between Risk,
[01:15:34.840 --> 01:15:37.620]   poker, and the TV show "Survivor."
[01:15:37.620 --> 01:15:40.420]   There's like this big element of like trying to,
[01:15:40.420 --> 01:15:43.680]   yeah, there's a big social element.
[01:15:43.680 --> 01:15:45.440]   And the best way that I would describe the game
[01:15:45.440 --> 01:15:47.600]   is that it's really a game about people
[01:15:47.600 --> 01:15:48.800]   rather than the pieces.
[01:15:48.800 --> 01:15:52.420]   - So Risk, because it is a map,
[01:15:52.420 --> 01:15:54.900]   it's kind of war game-like.
[01:15:54.900 --> 01:15:58.720]   Poker, because there's a game theory component
[01:15:58.720 --> 01:16:00.760]   that's very kind of strategic.
[01:16:00.760 --> 01:16:01.840]   So you could convert it
[01:16:01.840 --> 01:16:04.200]   into an artificial intelligence problem.
[01:16:04.200 --> 01:16:06.400]   And then Survivor, because of the social component,
[01:16:06.400 --> 01:16:07.880]   strong social component.
[01:16:07.880 --> 01:16:09.240]   I saw that somebody said online
[01:16:09.240 --> 01:16:12.240]   that the internet version of the game
[01:16:12.240 --> 01:16:14.920]   has this quality of that it's easier
[01:16:14.920 --> 01:16:17.560]   to almost to do like role-playing.
[01:16:17.560 --> 01:16:19.520]   As opposed to being yourself,
[01:16:19.520 --> 01:16:21.360]   you can actually like be the,
[01:16:21.360 --> 01:16:24.300]   like really imagine yourself as the leader of France
[01:16:24.300 --> 01:16:25.500]   or Russia and so on.
[01:16:25.500 --> 01:16:28.320]   Like really pretend to be that person.
[01:16:28.320 --> 01:16:32.820]   It's actually fun to really lean into being that leader.
[01:16:32.820 --> 01:16:34.700]   - Yeah, so some players do go this route
[01:16:34.700 --> 01:16:37.060]   where they just like kind of view it as a strategy game,
[01:16:37.060 --> 01:16:39.100]   but also a role-playing game where they can like act out,
[01:16:39.100 --> 01:16:41.500]   like, what would I be like if I was, you know,
[01:16:41.500 --> 01:16:43.580]   a leader of France in 1900?
[01:16:43.580 --> 01:16:44.500]   - I'll forfeit right away.
[01:16:44.500 --> 01:16:45.500]   No, I'm just kidding.
[01:16:45.500 --> 01:16:50.260]   And they sometimes use like the old-timey language
[01:16:50.260 --> 01:16:53.700]   to like, or how they imagined the elites
[01:16:53.700 --> 01:16:54.960]   would talk at that time.
[01:16:54.960 --> 01:16:57.840]   Anyway, so what are the different turns of the game?
[01:16:57.840 --> 01:16:59.660]   Like what are the rounds?
[01:16:59.660 --> 01:17:01.160]   - Yeah, so on every turn,
[01:17:01.160 --> 01:17:03.360]   you got like a bunch of different units
[01:17:03.360 --> 01:17:04.200]   that you start out with.
[01:17:04.200 --> 01:17:07.480]   So you start out controlling like just a few units
[01:17:07.480 --> 01:17:09.720]   and the object of the game is to gain control
[01:17:09.720 --> 01:17:10.560]   of a majority of the map.
[01:17:10.560 --> 01:17:13.120]   If you're able to do that, then you've won the game.
[01:17:13.120 --> 01:17:15.280]   But like I said, the only way that you're able to do that
[01:17:15.280 --> 01:17:16.840]   is by working with other players.
[01:17:16.840 --> 01:17:19.440]   So on every turn, you can issue a move order.
[01:17:19.440 --> 01:17:20.780]   So for each of your units,
[01:17:20.780 --> 01:17:23.960]   you can move them to an adjacent territory,
[01:17:23.960 --> 01:17:26.340]   or you can keep them where they are,
[01:17:26.340 --> 01:17:30.880]   or you can support a move or a hold of a different unit.
[01:17:30.880 --> 01:17:31.720]   So--
[01:17:31.720 --> 01:17:32.560]   - What are the territories?
[01:17:32.560 --> 01:17:34.300]   Well, how is the map divided up?
[01:17:34.300 --> 01:17:36.980]   - It's kind of like risk where the map is divided up
[01:17:36.980 --> 01:17:38.960]   into like 50 different territories.
[01:17:38.960 --> 01:17:42.460]   Now you can enter a territory
[01:17:42.460 --> 01:17:45.300]   if you're moving into that territory with more supports
[01:17:45.300 --> 01:17:47.000]   than the person that's in there
[01:17:47.000 --> 01:17:48.900]   or the person that's trying to move in there.
[01:17:48.900 --> 01:17:51.660]   So if you're moving in and there's somebody already there,
[01:17:51.660 --> 01:17:54.380]   then if neither of you have support, it's a one versus one
[01:17:54.380 --> 01:17:56.880]   and you'll bounce back and neither of you will make progress.
[01:17:56.880 --> 01:17:58.620]   If you have a unit that's supporting
[01:17:58.620 --> 01:18:01.300]   that move into the territory, then it's a two versus one
[01:18:01.300 --> 01:18:02.300]   and you'll kick them out
[01:18:02.300 --> 01:18:03.980]   and they'll have to retreat somewhere.
[01:18:03.980 --> 01:18:04.900]   - What does support mean?
[01:18:04.900 --> 01:18:06.620]   - Support is like, it's an action
[01:18:06.620 --> 01:18:07.700]   that you can issue in the game.
[01:18:07.700 --> 01:18:10.060]   So you can say this unit, you write down,
[01:18:10.060 --> 01:18:13.140]   this unit is supporting this other unit into this territory.
[01:18:13.140 --> 01:18:16.100]   - Are these units from opposing forces?
[01:18:16.100 --> 01:18:17.020]   - They could be, they could be.
[01:18:17.020 --> 01:18:18.620]   And this is where the interesting aspect
[01:18:18.620 --> 01:18:19.460]   of the game comes in
[01:18:19.460 --> 01:18:22.340]   because you can support your own units into territory,
[01:18:22.340 --> 01:18:24.460]   but you can also support other people's units
[01:18:24.460 --> 01:18:25.300]   into territories.
[01:18:25.300 --> 01:18:28.740]   And so that's what the negotiations really revolve around.
[01:18:28.740 --> 01:18:30.860]   - But you don't have to do the thing you say
[01:18:30.860 --> 01:18:32.860]   you're going to do, right?
[01:18:32.860 --> 01:18:33.700]   - Yeah, and so this is--
[01:18:33.700 --> 01:18:34.940]   - So you can say, I'm gonna support you,
[01:18:34.940 --> 01:18:36.940]   but then backstab the person.
[01:18:36.940 --> 01:18:38.420]   - Yeah, that's absolutely right.
[01:18:38.420 --> 01:18:41.020]   - And that tension is core to the game?
[01:18:41.020 --> 01:18:43.420]   - That tension is absolutely core to the game.
[01:18:43.420 --> 01:18:46.640]   The fact that you can make all sorts of promises,
[01:18:46.640 --> 01:18:49.060]   but you have to reason about the fact that like,
[01:18:49.060 --> 01:18:50.300]   hey, they might not trust you
[01:18:50.300 --> 01:18:51.900]   if you say you're gonna do something,
[01:18:51.900 --> 01:18:54.140]   or they might be lying to you
[01:18:54.140 --> 01:18:56.300]   when they say that they're gonna support you.
[01:18:56.300 --> 01:18:59.340]   - So maybe just to jump back,
[01:18:59.340 --> 01:19:01.360]   what's the history of the game in general?
[01:19:01.360 --> 01:19:03.820]   Is it true that Henry Kissinger loved the game
[01:19:03.820 --> 01:19:05.300]   and JFK and all those?
[01:19:05.300 --> 01:19:07.340]   I've heard like a bunch of different people that,
[01:19:07.340 --> 01:19:08.900]   or is that just one of those things
[01:19:08.900 --> 01:19:10.420]   that the cool kids say they do,
[01:19:10.420 --> 01:19:11.500]   but they don't actually play?
[01:19:11.500 --> 01:19:13.380]   - So the game was created in the '50s.
[01:19:13.380 --> 01:19:14.620]   - Yeah.
[01:19:14.620 --> 01:19:18.120]   - And from what I understand, it was JFK's,
[01:19:18.120 --> 01:19:19.600]   it was played in like the JFK White House,
[01:19:19.600 --> 01:19:21.080]   Henry Kissinger's favorite game.
[01:19:21.080 --> 01:19:22.320]   I don't know if it's true,
[01:19:22.320 --> 01:19:23.880]   but that's definitely what I've heard.
[01:19:23.880 --> 01:19:27.160]   - It's interesting that they went with World War I
[01:19:27.160 --> 01:19:29.600]   when it was created after World War II.
[01:19:29.600 --> 01:19:32.620]   - So the story that I've heard for the creation of the game
[01:19:32.620 --> 01:19:36.640]   is it was created by somebody that had looked at
[01:19:36.640 --> 01:19:39.200]   the history of the 20th century,
[01:19:39.200 --> 01:19:43.280]   and they saw World War I as a failure of diplomacy.
[01:19:43.280 --> 01:19:44.120]   So-
[01:19:44.120 --> 01:19:44.960]   - Yeah.
[01:19:44.960 --> 01:19:47.200]   - They saw the fact that this war broke out
[01:19:47.200 --> 01:19:49.880]   as like the diplomats of all these countries
[01:19:49.880 --> 01:19:51.440]   really failed to prevent a war.
[01:19:51.440 --> 01:19:52.640]   And he wanted to create a game
[01:19:52.640 --> 01:19:55.140]   that would basically teach people about diplomacy.
[01:19:55.140 --> 01:20:00.900]   And it's really fascinating that in his ideal version
[01:20:00.900 --> 01:20:03.640]   of the game of diplomacy, nobody actually wins the game.
[01:20:03.640 --> 01:20:05.880]   Because the whole point is that if somebody is about to win,
[01:20:05.880 --> 01:20:08.200]   then the other players should be able to work together
[01:20:08.200 --> 01:20:10.200]   to stop that person from winning.
[01:20:10.200 --> 01:20:11.920]   And so the ideal version of the game
[01:20:11.920 --> 01:20:13.800]   is just one where nobody actually wins.
[01:20:13.800 --> 01:20:16.760]   And it's kind of has a nice like wholesome take home message
[01:20:16.760 --> 01:20:19.840]   then that war is ultimately futile.
[01:20:19.840 --> 01:20:20.660]   And-
[01:20:20.660 --> 01:20:25.800]   - And that optimal, that futile optimal
[01:20:25.800 --> 01:20:28.080]   could be achieved through great diplomacy.
[01:20:28.080 --> 01:20:28.920]   - Yeah.
[01:20:28.920 --> 01:20:32.580]   - So is there some asymmetry in terms of
[01:20:32.580 --> 01:20:35.160]   which is more powerful, Russia versus Germany
[01:20:35.160 --> 01:20:38.100]   versus France and so on?
[01:20:38.100 --> 01:20:40.760]   - So I think the general consensus is that France
[01:20:40.760 --> 01:20:42.120]   is the strongest power in the game.
[01:20:42.120 --> 01:20:44.200]   But the beautiful thing about diplomacy
[01:20:44.200 --> 01:20:45.860]   is that it's self-balancing, right?
[01:20:45.860 --> 01:20:48.380]   So the fact that France has an inherited advantage
[01:20:48.380 --> 01:20:51.200]   from the beginning means that the other players
[01:20:51.200 --> 01:20:53.000]   are less likely to work with it.
[01:20:53.000 --> 01:20:54.800]   - I saw that Russia has four units
[01:20:54.800 --> 01:20:56.260]   versus four of something
[01:20:56.260 --> 01:20:58.000]   that the others have three of something.
[01:20:58.000 --> 01:20:58.840]   - That's true, yeah.
[01:20:58.840 --> 01:20:59.840]   So Russia starts off with four units
[01:20:59.840 --> 01:21:02.040]   while all the other players start with three.
[01:21:02.040 --> 01:21:04.560]   But Russia is also in a much more vulnerable position
[01:21:04.560 --> 01:21:06.400]   because they have to like,
[01:21:06.400 --> 01:21:07.920]   they have a lot more neighbors as well.
[01:21:07.920 --> 01:21:08.760]   - Got it.
[01:21:08.760 --> 01:21:11.000]   Larger territory, more, yeah, right.
[01:21:11.000 --> 01:21:13.040]   More border to defend.
[01:21:13.040 --> 01:21:17.680]   Okay, what else is important to know about the rules?
[01:21:17.680 --> 01:21:20.040]   So how many rounds are there?
[01:21:20.040 --> 01:21:22.120]   Like, is this iterative game?
[01:21:22.120 --> 01:21:23.840]   Is it finite?
[01:21:23.840 --> 01:21:25.440]   Do you just keep going indefinitely?
[01:21:25.440 --> 01:21:30.040]   - Usually the game lasts, I would say about 15 or 20 turns.
[01:21:30.040 --> 01:21:32.240]   There's in theory, no limit.
[01:21:32.240 --> 01:21:34.280]   It could last longer, but at some point,
[01:21:34.280 --> 01:21:35.880]   I mean, if you're playing a house game with friends,
[01:21:35.880 --> 01:21:37.880]   at some point you just get tired and you all agree like,
[01:21:37.880 --> 01:21:40.480]   okay, we're gonna end the game here and call it a draw.
[01:21:41.320 --> 01:21:43.320]   If you're playing online, there's usually like set limits
[01:21:43.320 --> 01:21:45.000]   on when the game will actually end.
[01:21:45.000 --> 01:21:47.600]   - And what's the end, what's the termination condition?
[01:21:47.600 --> 01:21:52.600]   Like, does one country have to conquer everything else?
[01:21:52.600 --> 01:21:55.200]   - So if somebody is able to actually gain control
[01:21:55.200 --> 01:21:57.480]   of a majority of the map, then they've won the game.
[01:21:57.480 --> 01:21:59.880]   And that is a solo victory as it's called.
[01:21:59.880 --> 01:22:01.480]   Now that pretty rarely happens,
[01:22:01.480 --> 01:22:03.320]   especially with strong players, because like I said,
[01:22:03.320 --> 01:22:06.240]   the game is designed to incentivize the other players
[01:22:06.240 --> 01:22:07.760]   to put a stop to that and all work together
[01:22:07.760 --> 01:22:09.240]   to stop the superpower.
[01:22:09.640 --> 01:22:12.800]   Usually what ends up happening is that, you know,
[01:22:12.800 --> 01:22:16.640]   all the players agree to a draw and then the score,
[01:22:16.640 --> 01:22:20.360]   the win is divided among the remaining players.
[01:22:20.360 --> 01:22:21.760]   There's a lot of different scoring systems.
[01:22:21.760 --> 01:22:26.400]   The one that we used in our research basically gives
[01:22:26.400 --> 01:22:29.800]   a score relative to how much control you have of the map.
[01:22:29.800 --> 01:22:32.640]   So the more that you control, the higher your score.
[01:22:32.640 --> 01:22:35.600]   - What's the history of using this game
[01:22:35.600 --> 01:22:37.800]   as a benchmark for AI research?
[01:22:37.800 --> 01:22:39.720]   Do people use it?
[01:22:39.720 --> 01:22:42.880]   - Yeah, so people have been working on AI for diplomacy
[01:22:42.880 --> 01:22:44.360]   since about the '80s.
[01:22:44.360 --> 01:22:47.640]   There was some really exciting research back then,
[01:22:47.640 --> 01:22:50.760]   but the approach that was taken was very different
[01:22:50.760 --> 01:22:51.600]   from what we see today.
[01:22:51.600 --> 01:22:54.120]   I mean, the research in the '80s was a very rule-based
[01:22:54.120 --> 01:22:56.160]   approach, kind of a heuristic approach.
[01:22:56.160 --> 01:22:57.800]   It was very in line with the kind of research
[01:22:57.800 --> 01:22:59.520]   that was being done in the '80s.
[01:22:59.520 --> 01:23:01.640]   You know, basically trying to encode human knowledge
[01:23:01.640 --> 01:23:03.560]   into the strategy of the AI.
[01:23:03.560 --> 01:23:04.840]   - Sure.
[01:23:04.840 --> 01:23:06.280]   - And, you know, it's understandable.
[01:23:06.280 --> 01:23:08.600]   I mean, the game is so incredibly different
[01:23:08.600 --> 01:23:12.080]   and so much more complicated than the kinds of games
[01:23:12.080 --> 01:23:15.720]   that people were working on like chess and go and poker
[01:23:15.720 --> 01:23:20.080]   that it was honestly even hard to like start
[01:23:20.080 --> 01:23:22.280]   making any progress in diplomacy.
[01:23:22.280 --> 01:23:24.960]   - Can you just formulate what is the problem
[01:23:24.960 --> 01:23:27.600]   from an AI perspective and why is it hard?
[01:23:27.600 --> 01:23:29.640]   Why is it a challenging game to solve?
[01:23:29.640 --> 01:23:31.320]   - So there's a lot of aspects in diplomacy
[01:23:31.320 --> 01:23:33.640]   that make it a huge challenge.
[01:23:33.640 --> 01:23:36.720]   First of all, you have the natural language components.
[01:23:36.720 --> 01:23:39.280]   And I think this really is what makes it
[01:23:39.280 --> 01:23:42.400]   arguably the most difficult game
[01:23:42.400 --> 01:23:43.880]   among like the major benchmarks.
[01:23:43.880 --> 01:23:46.320]   The fact that you have to,
[01:23:46.320 --> 01:23:49.760]   it's not about moving pieces on the board.
[01:23:49.760 --> 01:23:53.520]   Your action space is basically all the different sentences
[01:23:53.520 --> 01:23:56.160]   that you could communicate to somebody else in this game.
[01:23:56.160 --> 01:23:57.480]   And-
[01:23:57.480 --> 01:23:59.400]   - Is there, can we just like linger on that?
[01:23:59.400 --> 01:24:04.400]   So is part of it like the ambiguity in the language?
[01:24:04.400 --> 01:24:07.920]   If it was like very strict,
[01:24:07.920 --> 01:24:10.040]   if you narrowed the set of possible sentences
[01:24:10.040 --> 01:24:10.880]   you could do it,
[01:24:10.880 --> 01:24:12.720]   would that simplify the game significantly?
[01:24:12.720 --> 01:24:16.640]   - The real difficulty is the breadth of things
[01:24:16.640 --> 01:24:17.840]   that you can talk about.
[01:24:17.840 --> 01:24:20.960]   You can have natural language in other games
[01:24:20.960 --> 01:24:22.280]   like Settlers of Catan, for example,
[01:24:22.280 --> 01:24:23.760]   like you could have a natural language,
[01:24:23.760 --> 01:24:25.400]   Settlers of Catan AI.
[01:24:25.400 --> 01:24:26.920]   But the things that you're gonna talk about
[01:24:26.920 --> 01:24:29.320]   are basically like, am I trading you two sheep for a wood
[01:24:29.320 --> 01:24:30.680]   or three sheep for a wood?
[01:24:30.680 --> 01:24:33.560]   Whereas in a game like Diplomacy,
[01:24:33.560 --> 01:24:35.760]   the breadth of conversations that you're going to have
[01:24:35.760 --> 01:24:38.120]   are like, am I going to support you?
[01:24:38.120 --> 01:24:39.520]   Are you gonna support me in return?
[01:24:39.520 --> 01:24:41.800]   Which units are gonna do what?
[01:24:41.800 --> 01:24:45.240]   What did this other person promise you?
[01:24:45.240 --> 01:24:47.040]   They're lying because they told this other person
[01:24:47.040 --> 01:24:49.080]   that they're gonna do this instead.
[01:24:49.080 --> 01:24:50.280]   If you help me out this turn,
[01:24:50.280 --> 01:24:52.480]   then in the future I'll do these things
[01:24:52.480 --> 01:24:53.640]   that will help you out.
[01:24:53.640 --> 01:24:58.200]   The depth and breadth of these conversations
[01:24:58.200 --> 01:25:01.000]   is really complicated.
[01:25:01.000 --> 01:25:03.200]   And it's all being done in natural language.
[01:25:03.200 --> 01:25:05.400]   Now you could approach it,
[01:25:05.400 --> 01:25:06.640]   and we actually considered doing this,
[01:25:06.640 --> 01:25:09.200]   like having a simplified language
[01:25:09.200 --> 01:25:12.640]   to make this complexity smaller.
[01:25:12.640 --> 01:25:16.360]   But ultimately we thought the most impactful way
[01:25:16.360 --> 01:25:19.400]   of doing this research would be to address
[01:25:19.400 --> 01:25:21.360]   the natural language component head on
[01:25:21.360 --> 01:25:24.320]   and just try to go for the full game upfront.
[01:25:24.320 --> 01:25:25.880]   - Just looking at sample games
[01:25:25.920 --> 01:25:27.880]   and what the conversations look like.
[01:25:27.880 --> 01:25:30.560]   Greetings England, this should prove to be a fun game
[01:25:30.560 --> 01:25:32.760]   since all the private press
[01:25:32.760 --> 01:25:35.200]   is going to be made public at the end.
[01:25:35.200 --> 01:25:37.440]   At the least it will be interesting to see
[01:25:37.440 --> 01:25:39.320]   if the press changes because of that.
[01:25:39.320 --> 01:25:40.440]   Anyway, good, okay.
[01:25:40.440 --> 01:25:41.840]   So there's like a--
[01:25:41.840 --> 01:25:43.560]   - Yeah, that's just kind of like the generic greetings
[01:25:43.560 --> 01:25:44.400]   at the beginning of the game.
[01:25:44.400 --> 01:25:46.160]   I think that the meat comes a little bit later
[01:25:46.160 --> 01:25:48.040]   when you're starting to talk about
[01:25:48.040 --> 01:25:50.120]   specific strategy and stuff.
[01:25:50.120 --> 01:25:52.280]   - I agree there are a lot of advantages
[01:25:52.280 --> 01:25:54.640]   to the two of us keeping in touch
[01:25:54.640 --> 01:25:58.000]   and our nations make strong natural allies
[01:25:58.000 --> 01:25:58.880]   in the middle game.
[01:25:58.880 --> 01:26:02.480]   So that kind of stuff, making friends, making enemies.
[01:26:02.480 --> 01:26:03.800]   - Yeah, or like if you look at the next line,
[01:26:03.800 --> 01:26:05.040]   so the person saying like,
[01:26:05.040 --> 01:26:08.640]   I've heard bits about a Lepanto and an octopus opening
[01:26:08.640 --> 01:26:10.280]   and basically telling Austria like,
[01:26:10.280 --> 01:26:11.760]   hey, just a heads up, you know,
[01:26:11.760 --> 01:26:13.040]   I've heard these whispers about like
[01:26:13.040 --> 01:26:14.960]   what might be going on behind your back.
[01:26:14.960 --> 01:26:18.680]   - Yeah, so there's all kinds of complexities
[01:26:18.680 --> 01:26:22.760]   in the language of that, right?
[01:26:22.760 --> 01:26:25.200]   Like to interpret what the heck that means.
[01:26:25.200 --> 01:26:27.920]   It's hard for us humans, but for AI it's even harder
[01:26:27.920 --> 01:26:30.120]   'cause you have to understand like at every level
[01:26:30.120 --> 01:26:31.800]   the semantics of that.
[01:26:31.800 --> 01:26:34.240]   - Right, I mean, there's the complexity in understanding
[01:26:34.240 --> 01:26:36.640]   when somebody is saying this to me, what does that mean?
[01:26:36.640 --> 01:26:38.640]   And then there's also the complexity of like,
[01:26:38.640 --> 01:26:40.120]   should I be telling this person this?
[01:26:40.120 --> 01:26:42.080]   Like I've overheard these whispers,
[01:26:42.080 --> 01:26:43.520]   should I be telling this person that like,
[01:26:43.520 --> 01:26:46.720]   hey, you might be getting attacked by this other power?
[01:26:46.720 --> 01:26:51.720]   - Okay, so how are we supposed to think about?
[01:26:51.800 --> 01:26:54.160]   Okay, so that's the natural language.
[01:26:54.160 --> 01:26:56.160]   How do you even begin trying to solve this game?
[01:26:56.160 --> 01:27:00.120]   It seems like the Turing test on steroids.
[01:27:00.120 --> 01:27:02.280]   - Yeah, and I mean, there's the natural language aspect.
[01:27:02.280 --> 01:27:04.320]   And then even besides the natural language aspect,
[01:27:04.320 --> 01:27:07.640]   you also have the cooperative elements of the game.
[01:27:07.640 --> 01:27:09.840]   And I think this is actually something
[01:27:09.840 --> 01:27:11.160]   that I find really interesting.
[01:27:11.160 --> 01:27:15.240]   If you look at all the previous game AI breakthroughs,
[01:27:15.240 --> 01:27:17.240]   they've all happened in these purely adversarial games
[01:27:17.240 --> 01:27:19.080]   where you don't actually need to understand
[01:27:19.080 --> 01:27:20.480]   how humans play the game.
[01:27:20.480 --> 01:27:23.040]   It's all just AI versus AI, right?
[01:27:23.040 --> 01:27:27.680]   Like you look at checkers, chess, go, poker,
[01:27:27.680 --> 01:27:31.280]   Starcraft, Dota 2, like in some of those cases,
[01:27:31.280 --> 01:27:33.840]   they leveraged human data, but they never needed to.
[01:27:33.840 --> 01:27:38.520]   They were always just trying to have a scalable algorithm
[01:27:38.520 --> 01:27:41.720]   that then they could throw a lot of computational resources
[01:27:41.720 --> 01:27:45.280]   at a lot of memory at, and then eventually it would converge
[01:27:45.280 --> 01:27:47.760]   to an approximation of a Nash equilibrium.
[01:27:47.760 --> 01:27:51.920]   This perfect strategy that in a two player zero-sum game
[01:27:51.920 --> 01:27:53.520]   guarantees that they're going to be able
[01:27:53.520 --> 01:27:55.840]   to not lose to any opponent.
[01:27:55.840 --> 01:27:58.320]   - So you can't leverage self-play to solve this game.
[01:27:58.320 --> 01:27:59.840]   - You can leverage self-play,
[01:27:59.840 --> 01:28:02.760]   but it's no longer sufficient to beat humans.
[01:28:02.760 --> 01:28:05.240]   - So how do you integrate the human into the loop of this?
[01:28:05.240 --> 01:28:08.560]   - So what you have to do is incorporate human data.
[01:28:08.560 --> 01:28:11.000]   And to kind of give you some intuition
[01:28:11.000 --> 01:28:12.000]   for why this is the case,
[01:28:12.000 --> 01:28:13.800]   like imagine you're playing a negotiation game,
[01:28:13.800 --> 01:28:18.600]   like diplomacy, but you're training completely from scratch
[01:28:18.600 --> 01:28:19.920]   without any human data.
[01:28:19.920 --> 01:28:23.200]   The AI is not going to suddenly like figure out
[01:28:23.200 --> 01:28:24.360]   how to communicate in English.
[01:28:24.360 --> 01:28:27.280]   It's going to figure out some weird robot language
[01:28:27.280 --> 01:28:29.040]   that only it will understand.
[01:28:29.040 --> 01:28:30.440]   And then when you stick that in a game
[01:28:30.440 --> 01:28:31.960]   with six other humans,
[01:28:31.960 --> 01:28:34.320]   they're going to think this person's talking gibberish
[01:28:34.320 --> 01:28:35.480]   and they're just going to ally with each other
[01:28:35.480 --> 01:28:37.280]   and team up against the bot,
[01:28:37.280 --> 01:28:38.400]   or not even team up against the bot,
[01:28:38.400 --> 01:28:39.880]   but just not work with the bot.
[01:28:39.880 --> 01:28:43.640]   And so in order to be able to play this game with humans,
[01:28:43.640 --> 01:28:46.280]   it has to understand the human way of playing the game,
[01:28:46.280 --> 01:28:48.680]   not this machine way of playing the game.
[01:28:48.680 --> 01:28:50.600]   - Yeah, yeah, that's fascinating.
[01:28:50.600 --> 01:28:51.720]   So, right.
[01:28:51.720 --> 01:28:54.960]   That's a nuanced thing to understand
[01:28:54.960 --> 01:28:58.440]   'cause a chess playing program
[01:28:58.440 --> 01:29:01.000]   doesn't need to play like a human to beat a human.
[01:29:01.000 --> 01:29:01.840]   - Exactly.
[01:29:01.840 --> 01:29:03.560]   - But here you have to play like a human
[01:29:03.560 --> 01:29:04.680]   in order to beat them.
[01:29:04.680 --> 01:29:06.040]   - Or at least you have to understand
[01:29:06.040 --> 01:29:07.000]   how humans play the game
[01:29:07.000 --> 01:29:08.920]   so that you can understand how to work with them.
[01:29:08.920 --> 01:29:10.640]   If they have certain expectations
[01:29:10.640 --> 01:29:13.160]   about what does it mean to be a good ally?
[01:29:13.160 --> 01:29:16.240]   What does it mean to have like a reciprocal relationship
[01:29:16.240 --> 01:29:17.520]   where we're working together?
[01:29:17.520 --> 01:29:20.040]   You have to abide by those conventions.
[01:29:20.040 --> 01:29:20.880]   And if you don't,
[01:29:20.880 --> 01:29:23.120]   they're just going to work with somebody else instead.
[01:29:23.120 --> 01:29:26.360]   - Do you think of this as a clean,
[01:29:26.360 --> 01:29:28.800]   in some deep sense of the spirit of the Turing test
[01:29:28.800 --> 01:29:30.480]   as formulated by Alan Turing?
[01:29:30.480 --> 01:29:32.460]   Is it, in some sense,
[01:29:32.460 --> 01:29:35.060]   this is what the Turing test actually looks like?
[01:29:35.060 --> 01:29:40.520]   - So, because of open-ended natural language conversation
[01:29:40.520 --> 01:29:44.080]   it seems like very difficult to evaluate.
[01:29:44.080 --> 01:29:46.040]   Like here at a high stakes
[01:29:46.040 --> 01:29:47.760]   where humans are trying to win a game,
[01:29:47.760 --> 01:29:51.960]   that seems like how you actually perform the Turing test.
[01:29:51.960 --> 01:29:53.720]   - I think it's different from the Turing test.
[01:29:53.720 --> 01:29:55.880]   Like the way that the Turing test is formulated,
[01:29:55.880 --> 01:29:59.120]   it's about trying to distinguish a human from a machine
[01:29:59.120 --> 01:30:02.760]   and seeing, oh, could the machine successfully pass
[01:30:02.760 --> 01:30:04.640]   as a human in this adversarial setting
[01:30:04.640 --> 01:30:07.320]   where the player is trying to figure out
[01:30:07.320 --> 01:30:08.960]   whether it's a machine or a human.
[01:30:08.960 --> 01:30:11.960]   Whereas in diplomacy, it's not about trying to figure out
[01:30:11.960 --> 01:30:14.400]   whether this player is a human or a machine.
[01:30:14.400 --> 01:30:17.560]   It's ultimately about whether I can work with this player
[01:30:17.560 --> 01:30:19.880]   regardless of whether they are a human or a machine.
[01:30:19.880 --> 01:30:22.980]   And can the machine do that better than a human can?
[01:30:22.980 --> 01:30:26.100]   - Yeah, I'm going to think about that,
[01:30:26.100 --> 01:30:31.100]   but that just feels like the implied requirement for that
[01:30:31.100 --> 01:30:33.500]   is for the machine to be human-like.
[01:30:33.500 --> 01:30:35.880]   - I think that's true,
[01:30:35.880 --> 01:30:39.080]   that if you're going to play in this human game,
[01:30:39.080 --> 01:30:43.360]   you have to somehow adapt to the human surroundings
[01:30:43.360 --> 01:30:44.720]   and the human play style.
[01:30:44.720 --> 01:30:47.220]   - And to win, you have to adapt.
[01:30:47.220 --> 01:30:50.080]   So you can't, if you're the outsider,
[01:30:50.080 --> 01:30:51.600]   if you're not human-like,
[01:30:51.600 --> 01:30:53.840]   I feel like that's a losing strategy.
[01:30:53.840 --> 01:30:55.800]   - I think that's correct, yeah.
[01:30:55.800 --> 01:30:57.100]   - Yeah, so, okay.
[01:30:57.100 --> 01:31:00.720]   What are the complexities here?
[01:31:00.720 --> 01:31:02.640]   What was your approach to it?
[01:31:02.640 --> 01:31:03.480]   - Before I get to that,
[01:31:03.480 --> 01:31:04.720]   one thing I should explain
[01:31:04.720 --> 01:31:07.320]   why we decided to work on Diplomacy.
[01:31:07.320 --> 01:31:10.560]   So basically what happened is in 2019,
[01:31:10.560 --> 01:31:15.320]   I was wrapping up the work on six-player poker on Pluribus
[01:31:15.320 --> 01:31:17.880]   and was trying to think about what to work on next.
[01:31:17.880 --> 01:31:20.840]   And I had been seeing all these other breakthroughs
[01:31:20.840 --> 01:31:21.680]   happening in AI.
[01:31:21.680 --> 01:31:24.600]   I mean, 2019, you have StarCraft,
[01:31:24.600 --> 01:31:26.920]   you have AlphaStar beating humans in StarCraft,
[01:31:26.920 --> 01:31:30.000]   you've got the Dota 2 stuff happening at OpenAI,
[01:31:30.000 --> 01:31:32.520]   you have GPT-2 or GPT-3 coming,
[01:31:32.520 --> 01:31:34.520]   I think it was GPT-2 at the time.
[01:31:34.520 --> 01:31:37.560]   And it became clear that AI was progressing
[01:31:37.560 --> 01:31:39.400]   really, really rapidly.
[01:31:39.400 --> 01:31:42.440]   And people were throwing out these other games
[01:31:42.440 --> 01:31:46.800]   about what should be the next challenge for multi-agent AI.
[01:31:46.800 --> 01:31:50.060]   And I just felt like we had to aim bigger.
[01:31:50.060 --> 01:31:54.480]   If you look at a game like chess or a game like Go,
[01:31:54.480 --> 01:31:56.320]   they took decades for researchers
[01:31:56.320 --> 01:31:59.440]   to ultimately reach superhuman performance at.
[01:31:59.440 --> 01:32:02.480]   I mean, chess took 40 years of AI research
[01:32:02.480 --> 01:32:03.960]   Go took another 20 years.
[01:32:03.960 --> 01:32:08.720]   And we thought that diplomacy
[01:32:08.720 --> 01:32:10.760]   would be this incredibly difficult challenge
[01:32:10.760 --> 01:32:12.840]   that could easily take a decade
[01:32:12.840 --> 01:32:14.920]   to make an AI that could play competently.
[01:32:14.920 --> 01:32:18.200]   But we felt like that was a goal worth aiming for.
[01:32:18.200 --> 01:32:21.520]   And so honestly, I was kind of reluctant
[01:32:21.520 --> 01:32:22.400]   to work on it at first
[01:32:22.400 --> 01:32:24.960]   because I thought it was like too far
[01:32:24.960 --> 01:32:26.120]   out of the realm of possibility.
[01:32:26.120 --> 01:32:28.680]   But I was talking to a coworker of mine, Adam Lear,
[01:32:28.680 --> 01:32:30.040]   and he was basically saying like,
[01:32:30.040 --> 01:32:31.480]   "Why not aim for it?
[01:32:31.480 --> 01:32:33.000]   "We'll learn some interesting things along the way
[01:32:33.000 --> 01:32:34.480]   "and maybe it'll be possible."
[01:32:34.480 --> 01:32:36.320]   And so we decided to go for it.
[01:32:36.320 --> 01:32:38.880]   And I think it was the right choice
[01:32:38.880 --> 01:32:43.000]   considering just how much progress there was in AI
[01:32:43.000 --> 01:32:45.480]   and that progress has continued in the years since.
[01:32:45.480 --> 01:32:50.240]   - So winning in diplomacy, what does that really look like?
[01:32:50.240 --> 01:32:54.520]   It means talking to six other players,
[01:32:54.520 --> 01:32:57.480]   six other entities, agents,
[01:32:57.480 --> 01:33:01.020]   and convincing them of stuff
[01:33:01.020 --> 01:33:03.600]   that you want them to be convinced of.
[01:33:03.600 --> 01:33:06.120]   Like what exactly, I'm trying to get like,
[01:33:06.120 --> 01:33:08.400]   to deeply understand what the problem is.
[01:33:08.400 --> 01:33:14.120]   - Ultimately, the problem is simple to quantify, right?
[01:33:14.120 --> 01:33:16.360]   Like you're going to play this game with humans
[01:33:16.360 --> 01:33:18.840]   and you want your score on average
[01:33:18.840 --> 01:33:21.080]   to be as high as possible.
[01:33:21.080 --> 01:33:22.600]   You know, if you can say like,
[01:33:22.600 --> 01:33:26.560]   "I am winning more than any human alive,"
[01:33:26.560 --> 01:33:28.600]   then you're a champion diplomacy player.
[01:33:30.000 --> 01:33:31.620]   Now, ultimately we didn't reach that.
[01:33:31.620 --> 01:33:32.860]   We got to human level performance.
[01:33:32.860 --> 01:33:35.480]   We actually, so we played about 40 games
[01:33:35.480 --> 01:33:38.280]   with real humans online.
[01:33:38.280 --> 01:33:40.420]   The bot came in second out of all players
[01:33:40.420 --> 01:33:41.980]   that played five or more games.
[01:33:41.980 --> 01:33:46.580]   And so not like number one, but way, way higher than-
[01:33:46.580 --> 01:33:48.460]   - What was the expertise level?
[01:33:48.460 --> 01:33:49.580]   Are they beginners?
[01:33:49.580 --> 01:33:51.940]   Are they intermediate players, advanced players?
[01:33:51.940 --> 01:33:52.880]   Do you have a sense?
[01:33:52.880 --> 01:33:53.780]   - That's a great question.
[01:33:53.780 --> 01:33:55.940]   And so I think this kind of goes into
[01:33:55.940 --> 01:33:58.060]   how do you measure the performance in diplomacy?
[01:33:58.060 --> 01:34:00.160]   And I would argue that when you're measuring performance
[01:34:00.160 --> 01:34:01.360]   in a game like this,
[01:34:01.360 --> 01:34:03.160]   you don't actually want to measure it
[01:34:03.160 --> 01:34:05.760]   in games with all expert players.
[01:34:05.760 --> 01:34:08.580]   It's kind of like if you're developing a self-driving car,
[01:34:08.580 --> 01:34:11.220]   you don't want to measure that car on the road
[01:34:11.220 --> 01:34:13.400]   with a bunch of expert stunt drivers.
[01:34:13.400 --> 01:34:16.400]   You want to put it on a road of like an actual American city
[01:34:16.400 --> 01:34:19.880]   and see is this car crashing less often
[01:34:19.880 --> 01:34:21.920]   than an expert driver would.
[01:34:21.920 --> 01:34:24.080]   So that's the metric that we've used.
[01:34:24.080 --> 01:34:26.480]   We're saying like, we're going to stick this game,
[01:34:26.480 --> 01:34:27.860]   we're going to stick this bot in games
[01:34:27.860 --> 01:34:30.460]   with a wide variety of skill levels.
[01:34:30.460 --> 01:34:33.500]   And then are we doing better than a strong
[01:34:33.500 --> 01:34:36.820]   or expert human player would in the same situation?
[01:34:36.820 --> 01:34:37.820]   - That's quite brilliant.
[01:34:37.820 --> 01:34:39.740]   'Cause I played a lot of sports in my life,
[01:34:39.740 --> 01:34:42.220]   like I did tennis, judo, whatever.
[01:34:42.220 --> 01:34:44.900]   And it's somehow almost easier
[01:34:44.900 --> 01:34:46.940]   to go against experts almost always.
[01:34:46.940 --> 01:34:47.780]   I don't know.
[01:34:47.780 --> 01:34:51.020]   I think they're more predictable in the quality of play.
[01:34:51.020 --> 01:34:54.260]   The space of strategies you're operating under
[01:34:54.260 --> 01:34:56.760]   is narrower against experts.
[01:34:56.760 --> 01:34:57.600]   It's more fun.
[01:34:57.600 --> 01:34:59.480]   It's really frustrating to go against beginners.
[01:34:59.480 --> 01:35:01.960]   Also 'cause beginners talk trash to you
[01:35:01.960 --> 01:35:03.760]   when they somehow do beat you.
[01:35:03.760 --> 01:35:05.800]   So that's a human thing that they add
[01:35:05.800 --> 01:35:07.100]   is not to be worried about that.
[01:35:07.100 --> 01:35:10.720]   But yeah, the variance in strategies is greater,
[01:35:10.720 --> 01:35:12.000]   especially with natural language.
[01:35:12.000 --> 01:35:13.960]   It's just all over the place then.
[01:35:13.960 --> 01:35:14.800]   - Yeah.
[01:35:14.800 --> 01:35:17.320]   And honestly, when you look at
[01:35:17.320 --> 01:35:20.520]   what makes a good human diplomacy player,
[01:35:20.520 --> 01:35:22.060]   obviously they're able to handle themselves
[01:35:22.060 --> 01:35:23.280]   in games with other expert humans,
[01:35:23.280 --> 01:35:24.780]   but where they really shine
[01:35:24.780 --> 01:35:26.640]   is when they're playing with these weak players.
[01:35:26.640 --> 01:35:29.000]   And they know how to take advantage
[01:35:29.000 --> 01:35:30.520]   of the fact that they're a weak player,
[01:35:30.520 --> 01:35:33.320]   that they won't be able to pull off a stab as well,
[01:35:33.320 --> 01:35:35.400]   or that they have certain tendencies
[01:35:35.400 --> 01:35:36.720]   and they can take them under their wing
[01:35:36.720 --> 01:35:38.240]   and persuade them to do things
[01:35:38.240 --> 01:35:41.080]   that might not even be in their interest.
[01:35:41.080 --> 01:35:42.480]   The really good diplomacy players
[01:35:42.480 --> 01:35:45.080]   are able to take advantage of the fact
[01:35:45.080 --> 01:35:47.520]   that there are some weak players in the game.
[01:35:47.520 --> 01:35:50.360]   - Okay, so if you have to incorporate human play data,
[01:35:50.360 --> 01:35:51.680]   how do you do that?
[01:35:51.680 --> 01:35:53.320]   How do you do that in order to train
[01:35:53.320 --> 01:35:55.960]   an AI system to play diplomacy?
[01:35:55.960 --> 01:35:58.660]   - Yeah, so that's really the crux of the problem.
[01:35:58.660 --> 01:36:02.440]   How do we leverage the benefits of self-play
[01:36:02.440 --> 01:36:04.040]   that have been so successful
[01:36:04.040 --> 01:36:06.180]   in all these other previous games,
[01:36:06.180 --> 01:36:10.800]   while keeping the strategy as human compatible as possible?
[01:36:10.800 --> 01:36:14.760]   And so what we did is we first trained a language model,
[01:36:14.760 --> 01:36:17.960]   and then we made that language model controllable
[01:36:17.960 --> 01:36:21.560]   on a set of intents, what we call intents,
[01:36:21.560 --> 01:36:24.160]   which are basically like an action that we want to play
[01:36:24.160 --> 01:36:27.160]   and an action that we would like the other player to play.
[01:36:27.160 --> 01:36:29.400]   And so this gives us a way to generate dialogue
[01:36:29.400 --> 01:36:31.880]   that's not just trying to imitate the human style,
[01:36:31.880 --> 01:36:34.320]   whatever a human would say in this situation,
[01:36:34.320 --> 01:36:36.720]   but to actually give it an intent,
[01:36:36.720 --> 01:36:38.800]   a purpose in its communication.
[01:36:38.800 --> 01:36:40.600]   We can talk about a specific move
[01:36:40.600 --> 01:36:42.600]   or we can make a specific request.
[01:36:42.600 --> 01:36:45.560]   And the determination of what that move is
[01:36:45.560 --> 01:36:50.560]   that we're discussing comes from a strategic reasoning model
[01:36:50.640 --> 01:36:53.000]   that uses reinforcement learning and planning.
[01:36:53.000 --> 01:36:57.000]   - So the computing the intents for all the players,
[01:36:57.000 --> 01:36:58.920]   how is that done?
[01:36:58.920 --> 01:37:01.840]   Just as a starting point,
[01:37:01.840 --> 01:37:03.280]   is that with reinforcement learning
[01:37:03.280 --> 01:37:04.640]   or is that just optimal,
[01:37:04.640 --> 01:37:06.840]   determining what the optimal is for intents?
[01:37:06.840 --> 01:37:11.240]   - It's a combination of reinforcement learning and planning.
[01:37:11.240 --> 01:37:14.280]   Actually very similar to how we approached poker
[01:37:14.280 --> 01:37:18.200]   and how people have approached chess and Go as well.
[01:37:18.200 --> 01:37:22.920]   We're using self-play and search to try to figure out
[01:37:22.920 --> 01:37:25.440]   what is an optimal move for us
[01:37:25.440 --> 01:37:26.920]   and what is a desirable move
[01:37:26.920 --> 01:37:28.920]   that we would like this other player to play.
[01:37:28.920 --> 01:37:32.960]   Now, the difference between the way that we approached
[01:37:32.960 --> 01:37:35.400]   reinforcement learning and search in this game
[01:37:35.400 --> 01:37:37.000]   versus those previous games
[01:37:37.000 --> 01:37:38.840]   is that we have to keep it human compatible.
[01:37:38.840 --> 01:37:41.800]   We have to understand how the other person
[01:37:41.800 --> 01:37:43.680]   is likely to play rather than just assuming
[01:37:43.680 --> 01:37:45.480]   that they're gonna play like a machine.
[01:37:45.480 --> 01:37:48.160]   - And how language gets them to play
[01:37:48.160 --> 01:37:52.760]   in a way that maximize the chance of following the intent
[01:37:52.760 --> 01:37:54.000]   you want them to follow.
[01:37:54.000 --> 01:37:55.720]   Okay, how do you do that?
[01:37:55.720 --> 01:37:58.160]   How do you connect language to intent?
[01:37:58.160 --> 01:38:01.480]   - So the way that RL and planning is done
[01:38:01.480 --> 01:38:03.160]   is actually not using language.
[01:38:03.160 --> 01:38:07.640]   So we're coming up with this plan for the action
[01:38:07.640 --> 01:38:09.120]   that we're gonna play and the other person's gonna play
[01:38:09.120 --> 01:38:11.800]   and then we feed that action into the dialogue model
[01:38:11.800 --> 01:38:14.120]   that will then send a message according to those plans.
[01:38:14.120 --> 01:38:19.120]   - So the language model there is mapping action to...
[01:38:19.120 --> 01:38:21.720]   - To message. - To message.
[01:38:21.720 --> 01:38:22.720]   One word at a time.
[01:38:22.720 --> 01:38:25.400]   - Basically one message at a time.
[01:38:25.400 --> 01:38:27.360]   So we'll feed into the dialogue model,
[01:38:27.360 --> 01:38:29.320]   like here are the actions that you should be discussing.
[01:38:29.320 --> 01:38:30.160]   Here's the message,
[01:38:30.160 --> 01:38:33.560]   here's like the content of the message
[01:38:33.560 --> 01:38:35.040]   that we would like you to send
[01:38:35.040 --> 01:38:37.040]   and then it will actually generate a message
[01:38:37.040 --> 01:38:37.920]   that corresponds to that.
[01:38:37.920 --> 01:38:39.800]   - Okay, does this actually work?
[01:38:39.800 --> 01:38:41.240]   - It works surprisingly well.
[01:38:41.240 --> 01:38:42.080]   - Okay, how...
[01:38:42.080 --> 01:38:43.840]   (laughs)
[01:38:43.840 --> 01:38:47.480]   Oh man, the number of ways it probably goes horribly,
[01:38:47.480 --> 01:38:50.080]   I would have imagined it goes horribly wrong.
[01:38:50.080 --> 01:38:54.000]   So how the heck is it effective at all?
[01:38:54.000 --> 01:38:55.880]   - I mean, there are a lot of ways that this could fail.
[01:38:55.880 --> 01:38:59.720]   So for example, I mean, you could have a situation
[01:38:59.720 --> 01:39:01.420]   where you're basically like,
[01:39:01.420 --> 01:39:04.400]   we don't tell the language model,
[01:39:04.400 --> 01:39:07.000]   like here are the pieces of our action
[01:39:07.000 --> 01:39:07.840]   or the other person's action
[01:39:07.840 --> 01:39:09.160]   that you should be communicating.
[01:39:09.160 --> 01:39:11.480]   And so like, let's say you're about to attack somebody,
[01:39:11.480 --> 01:39:12.880]   you probably don't wanna tell them
[01:39:12.880 --> 01:39:14.240]   that you're going to attack them,
[01:39:14.240 --> 01:39:15.840]   but there's nothing in the language,
[01:39:15.840 --> 01:39:17.080]   like the language model is not very smart
[01:39:17.080 --> 01:39:17.920]   at the end of the day.
[01:39:17.920 --> 01:39:20.200]   So it doesn't really have a way of knowing like,
[01:39:20.200 --> 01:39:21.560]   well, what should I be talking about?
[01:39:21.560 --> 01:39:22.440]   Should I tell this person
[01:39:22.440 --> 01:39:24.240]   that I'm about to attack them or not?
[01:39:24.240 --> 01:39:27.400]   So we have to like develop a lot of other techniques
[01:39:27.400 --> 01:39:28.620]   that deal with that.
[01:39:28.620 --> 01:39:31.120]   Like one of the things we do for example,
[01:39:31.120 --> 01:39:34.660]   is we try to calculate if I'm going to send this message,
[01:39:34.660 --> 01:39:37.360]   what would I expect the other person to do in response?
[01:39:37.360 --> 01:39:38.640]   So if it's a message like,
[01:39:38.640 --> 01:39:40.240]   hey, I'm gonna attack you this turn,
[01:39:40.240 --> 01:39:42.640]   they're probably gonna, you know, attack us
[01:39:42.640 --> 01:39:44.640]   or defend against that attack.
[01:39:44.640 --> 01:39:47.120]   And so we have a way of recognizing like,
[01:39:47.120 --> 01:39:48.920]   hey, sending this message
[01:39:48.920 --> 01:39:52.240]   is a negative expected value action
[01:39:52.240 --> 01:39:54.760]   and we should not send this message.
[01:39:54.760 --> 01:39:57.160]   - So you have for particular kinds of messages,
[01:39:57.160 --> 01:39:59.520]   you have like an extra function
[01:39:59.520 --> 01:40:03.200]   that does the, estimates the value of that message.
[01:40:03.200 --> 01:40:05.760]   - Yeah, so we have these kinds of filters that like-
[01:40:05.760 --> 01:40:06.600]   - So it's a filter.
[01:40:06.600 --> 01:40:10.040]   So there's a good, is that filter in your network
[01:40:10.040 --> 01:40:11.800]   or is it rule-based?
[01:40:11.800 --> 01:40:13.200]   - That's a neural network.
[01:40:13.200 --> 01:40:15.120]   So we're, well, it's a combination.
[01:40:15.120 --> 01:40:18.120]   It's a neural network, but it's also using planning.
[01:40:18.120 --> 01:40:20.040]   It's trying to compute like,
[01:40:20.040 --> 01:40:23.720]   what is the policy that the other players are going to play
[01:40:23.720 --> 01:40:26.120]   given that this message has been sent?
[01:40:26.120 --> 01:40:28.760]   And then is that better than not sending the message?
[01:40:28.760 --> 01:40:30.560]   - I feel like that's how my brain works too.
[01:40:30.560 --> 01:40:33.800]   Like there's a language model that generates random crap
[01:40:33.800 --> 01:40:37.160]   and then there's these other neural nets
[01:40:37.160 --> 01:40:39.000]   that are essentially filters.
[01:40:39.000 --> 01:40:41.200]   At least that's what I tweet.
[01:40:41.200 --> 01:40:43.400]   I'll usually, my process of tweeting,
[01:40:43.400 --> 01:40:46.300]   I'll think of something and it's hilarious to me.
[01:40:46.300 --> 01:40:48.080]   And then about five seconds later,
[01:40:48.080 --> 01:40:49.880]   the filter network comes in and says,
[01:40:49.880 --> 01:40:52.320]   no, no, that's not funny at all.
[01:40:52.320 --> 01:40:53.980]   I mean, there's something interesting
[01:40:53.980 --> 01:40:55.500]   to that kind of process.
[01:40:55.500 --> 01:40:58.640]   So you have a set of actions that you want,
[01:40:58.640 --> 01:41:02.080]   you have an intent that you want to achieve,
[01:41:02.080 --> 01:41:04.000]   an intent that you want your opponent to achieve,
[01:41:04.000 --> 01:41:05.760]   then you generate messages.
[01:41:05.760 --> 01:41:07.880]   And then you evaluate if those messages
[01:41:07.880 --> 01:41:12.880]   will achieve the goal you want.
[01:41:12.880 --> 01:41:15.280]   - Yeah, and we're filtering for several things.
[01:41:15.280 --> 01:41:17.960]   We're filtering like, is this a sensible message?
[01:41:17.960 --> 01:41:19.720]   So sometimes language models will send,
[01:41:19.720 --> 01:41:23.800]   will generate messages that are just like totally nonsense.
[01:41:23.800 --> 01:41:25.140]   And we try to filter those out.
[01:41:25.140 --> 01:41:29.240]   We also try to filter out messages that are basically lies.
[01:41:29.240 --> 01:41:32.260]   So diplomacy has this reputation as a game
[01:41:32.260 --> 01:41:35.200]   that's really about deception and lying,
[01:41:35.200 --> 01:41:38.280]   but we try to actually minimize the amount
[01:41:38.280 --> 01:41:40.200]   that the bot would lie.
[01:41:40.200 --> 01:41:42.440]   This was actually mostly a-
[01:41:42.440 --> 01:41:43.760]   - Or are you?
[01:41:43.760 --> 01:41:44.600]   No, I'm just kidding.
[01:41:44.600 --> 01:41:45.440]   All right, go ahead.
[01:41:45.440 --> 01:41:46.360]   (laughing)
[01:41:46.360 --> 01:41:47.680]   - I mean, like part of the reason for this
[01:41:47.680 --> 01:41:51.120]   is that we actually found that lying
[01:41:51.120 --> 01:41:53.240]   would make the bot perform worse in the long run.
[01:41:53.240 --> 01:41:55.240]   It would end up with a lower score.
[01:41:55.240 --> 01:41:56.840]   Because once the bot lies,
[01:41:56.840 --> 01:41:59.660]   people would never trust it again.
[01:41:59.660 --> 01:42:02.000]   And trust is a huge aspect of the game of diplomacy.
[01:42:02.000 --> 01:42:02.920]   - I'm taking notes here,
[01:42:02.920 --> 01:42:06.960]   'cause I think this applies to life lessons too.
[01:42:06.960 --> 01:42:08.520]   - Oh, I think it's a really, yeah, really strong-
[01:42:08.520 --> 01:42:11.040]   - So like lying is a dangerous thing to do.
[01:42:11.040 --> 01:42:15.000]   Like you want to avoid obvious lying.
[01:42:15.000 --> 01:42:17.000]   - Yeah, I mean, I think when people play diplomacy
[01:42:17.000 --> 01:42:18.280]   for the first time,
[01:42:18.280 --> 01:42:21.120]   they approach it as a game of deception and lying.
[01:42:21.120 --> 01:42:24.920]   And they, ultimately, if you talk to top diplomacy players,
[01:42:24.920 --> 01:42:26.760]   what they'll tell you is that diplomacy
[01:42:26.760 --> 01:42:28.200]   is a game about trust
[01:42:28.200 --> 01:42:30.560]   and being able to build trust in an environment
[01:42:30.560 --> 01:42:33.360]   that encourages people to not trust anyone.
[01:42:33.360 --> 01:42:36.040]   So that's the ultimate tension in diplomacy.
[01:42:36.040 --> 01:42:38.320]   How can this AI reason
[01:42:38.320 --> 01:42:41.040]   about whether you are being honest in your communication?
[01:42:41.040 --> 01:42:44.560]   And how can the AI persuade you that it is being honest
[01:42:44.560 --> 01:42:45.400]   when it is telling you that,
[01:42:45.400 --> 01:42:48.160]   "Hey, I'm actually going to support you this turn."
[01:42:48.160 --> 01:42:49.520]   - Is there some sense,
[01:42:49.520 --> 01:42:50.960]   I don't know if you step back and think,
[01:42:50.960 --> 01:42:55.960]   that this process will indirectly
[01:42:55.960 --> 01:42:57.960]   help us study human psychology?
[01:42:59.200 --> 01:43:01.600]   So like if trust is the ultimate goal,
[01:43:01.600 --> 01:43:03.920]   wouldn't that help us understand
[01:43:03.920 --> 01:43:07.400]   what are the fundamental aspects of forming trust
[01:43:07.400 --> 01:43:10.000]   between humans and between humans and AI?
[01:43:10.000 --> 01:43:11.880]   I mean, that's a really, really important question
[01:43:11.880 --> 01:43:14.920]   that's much bigger than strategy games.
[01:43:14.920 --> 01:43:15.960]   It's how can,
[01:43:15.960 --> 01:43:19.520]   that's fundamental to the human-robot interaction problem.
[01:43:19.520 --> 01:43:23.840]   How do we form trust between intelligent entities?
[01:43:23.840 --> 01:43:25.880]   - So one of the things I'm really excited about
[01:43:25.880 --> 01:43:26.840]   with diplomacy,
[01:43:27.780 --> 01:43:30.580]   there's never really been a good domain
[01:43:30.580 --> 01:43:32.620]   to investigate these kinds of questions.
[01:43:32.620 --> 01:43:35.780]   And diplomacy gives us a domain
[01:43:35.780 --> 01:43:38.860]   where trust is really at the center of it.
[01:43:38.860 --> 01:43:40.580]   And it's not just like you've hired
[01:43:40.580 --> 01:43:43.660]   a bunch of mechanical Turkers that are being paid
[01:43:43.660 --> 01:43:47.060]   and trying to get through the task as quickly as possible.
[01:43:47.060 --> 01:43:48.940]   You have these people that are really invested
[01:43:48.940 --> 01:43:49.860]   in the outcome of the game,
[01:43:49.860 --> 01:43:52.980]   and they're really trying to do the best that they can.
[01:43:52.980 --> 01:43:56.140]   And so I'm really excited that we're able to,
[01:43:56.140 --> 01:43:58.820]   we actually have put together this,
[01:43:58.820 --> 01:44:00.240]   we're open sourcing all of our models,
[01:44:00.240 --> 01:44:03.540]   we're open sourcing all of the code,
[01:44:03.540 --> 01:44:05.860]   and we're making the data that we've used
[01:44:05.860 --> 01:44:07.580]   available to researchers
[01:44:07.580 --> 01:44:10.980]   so that they can investigate these kinds of questions.
[01:44:10.980 --> 01:44:12.140]   - So the data of the different,
[01:44:12.140 --> 01:44:15.260]   the human and the AI play of diplomacy,
[01:44:15.260 --> 01:44:16.660]   and the models that you use
[01:44:16.660 --> 01:44:20.240]   for the generation of the messages and the filtering.
[01:44:20.240 --> 01:44:22.700]   - Yeah, not just even the data of the AI
[01:44:22.700 --> 01:44:23.700]   playing with the humans,
[01:44:23.700 --> 01:44:26.880]   but all the training data that we had,
[01:44:26.880 --> 01:44:28.420]   that we use to train the AI
[01:44:28.420 --> 01:44:30.300]   to understand how humans play the game.
[01:44:30.300 --> 01:44:31.580]   We're setting up a system
[01:44:31.580 --> 01:44:34.580]   where researchers will be able to apply
[01:44:34.580 --> 01:44:36.420]   to be able to gain access to that data
[01:44:36.420 --> 01:44:38.220]   and be able to use it in their own research.
[01:44:38.220 --> 01:44:41.060]   - We should say, what is the name of the system?
[01:44:41.060 --> 01:44:42.340]   - We're calling the bot Cicero.
[01:44:42.340 --> 01:44:43.180]   - Cicero.
[01:44:43.180 --> 01:44:45.580]   And what's the name, like you're open sourcing,
[01:44:45.580 --> 01:44:49.140]   what's the name of the repository and the project?
[01:44:49.140 --> 01:44:51.900]   Is it also just called Cicero the big project?
[01:44:51.900 --> 01:44:53.300]   Or is it still coming up with a name?
[01:44:53.300 --> 01:44:56.820]   - The data set comes from this website, webdiplomacy.net,
[01:44:56.820 --> 01:44:59.260]   is this site that's been online for like 20 years now.
[01:44:59.260 --> 01:45:00.720]   And it's one of the main sites
[01:45:00.720 --> 01:45:02.800]   that people use to play diplomacy on it.
[01:45:02.800 --> 01:45:06.180]   We've got like 50,000 games of diplomacy
[01:45:06.180 --> 01:45:09.380]   with natural language communication,
[01:45:09.380 --> 01:45:11.180]   over 10 million messages.
[01:45:11.180 --> 01:45:14.820]   So it's a pretty massive data set that people can use to,
[01:45:14.820 --> 01:45:16.540]   we're hoping that the academic community
[01:45:16.540 --> 01:45:18.540]   and the research community is able to use it
[01:45:18.540 --> 01:45:21.140]   for all sorts of interesting research questions.
[01:45:21.140 --> 01:45:23.980]   - So do you, from having studied this game,
[01:45:23.980 --> 01:45:28.060]   is this a sufficiently rich problem space
[01:45:28.060 --> 01:45:31.700]   to explore this kind of human AI interaction?
[01:45:31.700 --> 01:45:32.540]   - Yeah, absolutely.
[01:45:32.540 --> 01:45:36.100]   And I think it's maybe the best data set
[01:45:36.100 --> 01:45:37.420]   that I can think of out there
[01:45:37.420 --> 01:45:41.420]   to investigate these kinds of questions of negotiation,
[01:45:41.420 --> 01:45:44.220]   trust, persuasion.
[01:45:44.220 --> 01:45:45.900]   I wouldn't say it's the best data set in the world
[01:45:45.900 --> 01:45:49.660]   for human AI interaction, that's a very broad field.
[01:45:49.660 --> 01:45:52.100]   But I think that it's definitely up there as like,
[01:45:52.100 --> 01:45:54.660]   if you're really interested in language models
[01:45:54.660 --> 01:45:57.260]   interacting with humans in a setting
[01:45:57.260 --> 01:45:59.660]   where their incentives are not fully aligned,
[01:45:59.660 --> 01:46:02.900]   this seems like an ideal data set for investigating that.
[01:46:02.900 --> 01:46:07.900]   - So you have a paper with some impressive results
[01:46:07.900 --> 01:46:11.460]   and just an impressive paper that taken this problem on.
[01:46:11.460 --> 01:46:13.560]   What's the most exciting thing to you
[01:46:13.560 --> 01:46:16.780]   in terms of the results from the paper?
[01:46:16.780 --> 01:46:18.860]   - Well, I think there's a few--
[01:46:18.860 --> 01:46:20.660]   - Ideas or results?
[01:46:20.660 --> 01:46:23.180]   - Yeah, I think there's a few aspects of the results
[01:46:23.180 --> 01:46:25.460]   and that I think are really exciting.
[01:46:25.460 --> 01:46:27.980]   So first of all, the fact that we were able to achieve
[01:46:27.980 --> 01:46:29.900]   such strong performance,
[01:46:29.900 --> 01:46:33.780]   I was surprised by and pleasantly surprised by.
[01:46:33.780 --> 01:46:37.060]   So we played 40 games of diplomacy with real humans
[01:46:37.060 --> 01:46:40.860]   and the bot placed second out of all players
[01:46:40.860 --> 01:46:42.380]   that have played five or more games.
[01:46:42.380 --> 01:46:44.020]   So it's about 80 players total,
[01:46:44.020 --> 01:46:46.700]   19 of whom played five or more games
[01:46:46.700 --> 01:46:49.200]   and the bot was ranked second out of those players.
[01:46:49.200 --> 01:46:53.860]   And the bot was really good in two dimensions.
[01:46:53.860 --> 01:46:56.660]   One, being able to establish strong connections
[01:46:56.660 --> 01:46:58.060]   with the other players on the board,
[01:46:58.060 --> 01:47:01.600]   being able to like persuade them to work with it,
[01:47:01.600 --> 01:47:02.800]   being able to coordinate with them
[01:47:02.800 --> 01:47:04.980]   about like how it's going to work with them.
[01:47:04.980 --> 01:47:07.820]   And then also the raw tactical
[01:47:07.820 --> 01:47:09.940]   and strategic aspects of the game,
[01:47:09.940 --> 01:47:12.260]   being able to understand
[01:47:12.260 --> 01:47:13.700]   what the other players are likely to do,
[01:47:13.700 --> 01:47:15.540]   being able to model their behavior
[01:47:15.540 --> 01:47:17.700]   and respond appropriately to that,
[01:47:17.700 --> 01:47:19.820]   the bot also really excelled at.
[01:47:19.820 --> 01:47:22.460]   - What are some interesting things that the bot said?
[01:47:22.460 --> 01:47:26.260]   By the way, are you allowed to swear in the,
[01:47:26.260 --> 01:47:28.100]   like are there rules to what you're allowed to say
[01:47:28.100 --> 01:47:29.560]   and not in diplomacy?
[01:47:29.560 --> 01:47:30.860]   - You can say whatever you want.
[01:47:30.860 --> 01:47:32.700]   I think the site will get very angry at you
[01:47:32.700 --> 01:47:34.700]   if you start like threatening somebody.
[01:47:34.700 --> 01:47:36.420]   And we actually--
[01:47:36.420 --> 01:47:37.780]   - Like if you threaten somebody,
[01:47:37.780 --> 01:47:39.420]   you're supposed to do it politely.
[01:47:39.420 --> 01:47:41.780]   - Yeah, politely, you know, like keep it in character.
[01:47:43.700 --> 01:47:46.780]   We actually had a researcher watching the bot 24/7,
[01:47:46.780 --> 01:47:47.660]   well, whenever we play a game,
[01:47:47.660 --> 01:47:49.100]   we had a bot watching it to make sure
[01:47:49.100 --> 01:47:50.580]   that it wouldn't go off the rails
[01:47:50.580 --> 01:47:52.420]   and start like threatening somebody or something like that.
[01:47:52.420 --> 01:47:55.520]   - I would just love it if the bot started like mocking,
[01:47:55.520 --> 01:47:56.460]   mocking everybody,
[01:47:56.460 --> 01:47:59.620]   like some weird quirky strategies would emerge.
[01:47:59.620 --> 01:48:01.540]   That have you seen anything interesting that you,
[01:48:01.540 --> 01:48:02.880]   huh, that's a weird,
[01:48:02.880 --> 01:48:05.940]   that's a weird behavior,
[01:48:05.940 --> 01:48:09.060]   either the filter or the language model
[01:48:09.060 --> 01:48:10.900]   that was weird to you.
[01:48:10.900 --> 01:48:12.900]   - That was, yeah, there were definitely like
[01:48:12.900 --> 01:48:15.660]   things that the bot would do
[01:48:15.660 --> 01:48:17.460]   that were not in line with like
[01:48:17.460 --> 01:48:19.500]   how humans would approach the game.
[01:48:19.500 --> 01:48:22.340]   And that, in a good way, the humans actually,
[01:48:22.340 --> 01:48:24.580]   you know, we've talked to some expert diplomacy players
[01:48:24.580 --> 01:48:27.180]   about these results and their takeaway is that,
[01:48:27.180 --> 01:48:29.020]   well, maybe humans are approaching this the wrong way.
[01:48:29.020 --> 01:48:31.500]   And this is actually like the right way to play the game.
[01:48:31.500 --> 01:48:35.300]   - So what's required to win?
[01:48:35.300 --> 01:48:37.860]   Like what does it mean to mess up
[01:48:37.860 --> 01:48:41.380]   or to exploit the suboptimal behavior of a player?
[01:48:41.380 --> 01:48:45.500]   Like is there optimally rational behavior
[01:48:45.500 --> 01:48:48.780]   and irrational behavior that you need to estimate,
[01:48:48.780 --> 01:48:49.620]   that kind of stuff?
[01:48:49.620 --> 01:48:51.060]   Like what stands out to you?
[01:48:51.060 --> 01:48:53.860]   Like, is there a crack that you can exploit?
[01:48:53.860 --> 01:48:58.300]   Is there like a weakness that you can exploit in the game
[01:48:58.300 --> 01:49:00.200]   that everybody's looking for?
[01:49:00.200 --> 01:49:05.060]   - Well, I think you're asking kind of two questions there.
[01:49:05.060 --> 01:49:08.300]   So one, like modeling the irrationality
[01:49:08.300 --> 01:49:10.100]   and the suboptimality of humans.
[01:49:10.700 --> 01:49:13.420]   You can't, in diplomacy,
[01:49:13.420 --> 01:49:15.700]   you can't treat all the other players like they're machines.
[01:49:15.700 --> 01:49:17.060]   And if you do that,
[01:49:17.060 --> 01:49:19.420]   you're going to end up playing really poorly.
[01:49:19.420 --> 01:49:20.780]   And so we actually ran this experiment.
[01:49:20.780 --> 01:49:24.060]   So we trained a bot in a two player,
[01:49:24.060 --> 01:49:26.220]   zero-sum version of diplomacy,
[01:49:26.220 --> 01:49:27.800]   the same way that you might approach a game
[01:49:27.800 --> 01:49:29.700]   like chess or poker.
[01:49:29.700 --> 01:49:30.940]   And the bot was superhuman.
[01:49:30.940 --> 01:49:32.700]   It would crush any competitor.
[01:49:32.700 --> 01:49:35.060]   And then we took that same training approach
[01:49:35.060 --> 01:49:37.300]   and we trained a bot for the full seven player version
[01:49:37.300 --> 01:49:40.300]   of the game through self-play without any human data.
[01:49:40.300 --> 01:49:42.300]   And we stuck it in a game with six humans
[01:49:42.300 --> 01:49:43.800]   and it got destroyed.
[01:49:43.800 --> 01:49:44.820]   Even in the version of the game
[01:49:44.820 --> 01:49:47.740]   where there's no explicit natural language communication,
[01:49:47.740 --> 01:49:49.100]   it still got destroyed
[01:49:49.100 --> 01:49:50.960]   because it just wouldn't be able to understand
[01:49:50.960 --> 01:49:52.580]   how the other players were approaching the game
[01:49:52.580 --> 01:49:54.140]   and be able to work with that.
[01:49:54.140 --> 01:49:56.780]   - Can you just linger on that,
[01:49:56.780 --> 01:49:58.520]   meaning like there's an individual,
[01:49:58.520 --> 01:50:00.620]   there's an individual personality to each player
[01:50:00.620 --> 01:50:01.940]   and then you're supposed to remember that.
[01:50:01.940 --> 01:50:06.360]   But what do you mean it's not able to understand the players?
[01:50:06.360 --> 01:50:07.580]   - Well, it would, for example,
[01:50:07.580 --> 01:50:11.080]   expect the human to support it in a certain way
[01:50:11.080 --> 01:50:13.740]   when the human would simply like,
[01:50:13.740 --> 01:50:16.420]   think like, no, I'm not supposed to support you here.
[01:50:16.420 --> 01:50:17.300]   It's kind of like, you know,
[01:50:17.300 --> 01:50:19.460]   if you develop a self-driving car
[01:50:19.460 --> 01:50:21.100]   and it's trained completely from scratch
[01:50:21.100 --> 01:50:22.800]   with other self-driving cars,
[01:50:22.800 --> 01:50:25.020]   it might learn to drive on the left side of the road.
[01:50:25.020 --> 01:50:26.380]   And that's a totally reasonable thing to do
[01:50:26.380 --> 01:50:28.260]   if you're with these other self-driving cars
[01:50:28.260 --> 01:50:30.120]   that are also driving on the left side of the road.
[01:50:30.120 --> 01:50:33.020]   But if you put it in an American city, it's gonna crash.
[01:50:33.020 --> 01:50:34.700]   - But I guess the intuition I'm trying to build up
[01:50:34.700 --> 01:50:37.660]   is why does it then crush a human player heads up
[01:50:37.660 --> 01:50:40.600]   versus multiple?
[01:50:40.600 --> 01:50:42.920]   - This is an aspect of two players zero sum
[01:50:42.920 --> 01:50:45.160]   versus games that involve cooperation.
[01:50:45.160 --> 01:50:47.340]   So in a two player zero sum game,
[01:50:47.340 --> 01:50:50.320]   you can do self-play from scratch
[01:50:50.320 --> 01:50:52.600]   and you will arrive at the Nash equilibrium
[01:50:52.600 --> 01:50:56.600]   where you don't have to worry about the other player
[01:50:56.600 --> 01:50:58.520]   playing in a very human suboptimal style.
[01:50:58.520 --> 01:50:59.400]   That's just gonna be,
[01:50:59.400 --> 01:51:02.440]   the only way that deviating from a Nash equilibrium
[01:51:04.120 --> 01:51:06.640]   would change things is if it helped you.
[01:51:06.640 --> 01:51:09.200]   - So what's the dynamic of cooperation
[01:51:09.200 --> 01:51:11.280]   that's effective in diplomacy?
[01:51:11.280 --> 01:51:14.920]   Do you always have to have one friend in the game?
[01:51:14.920 --> 01:51:18.160]   - You always want to maximize your friends
[01:51:18.160 --> 01:51:19.460]   and minimize your enemies.
[01:51:19.460 --> 01:51:23.040]   - Got it.
[01:51:23.040 --> 01:51:28.040]   And boy, and the lying comes into play there.
[01:51:28.040 --> 01:51:32.280]   So the more friends you have, the better.
[01:51:32.280 --> 01:51:34.120]   - Yeah, I mean, I guess you have to attack somebody
[01:51:34.120 --> 01:51:35.320]   or else you're not gonna make progress.
[01:51:35.320 --> 01:51:39.160]   - Right, so that's the tension, but this is too real.
[01:51:39.160 --> 01:51:40.120]   This is too real.
[01:51:40.120 --> 01:51:42.800]   This is too close to geopolitics
[01:51:42.800 --> 01:51:45.320]   of actual military conflict in the world.
[01:51:45.320 --> 01:51:47.520]   Okay, that's fascinating.
[01:51:47.520 --> 01:51:49.280]   So that cooperation element
[01:51:49.280 --> 01:51:51.440]   is what makes the game really, really hard.
[01:51:51.440 --> 01:51:53.800]   - Yeah, and to give you an example
[01:51:53.800 --> 01:51:57.320]   of how this suboptimality and irrationality comes into play,
[01:51:57.320 --> 01:52:01.640]   there's a really common situation in the game of diplomacy
[01:52:01.640 --> 01:52:04.200]   that where one player starts to win
[01:52:04.200 --> 01:52:05.960]   and they're at the point where they're controlling
[01:52:05.960 --> 01:52:07.680]   about half the map.
[01:52:07.680 --> 01:52:09.000]   And the remaining players
[01:52:09.000 --> 01:52:11.000]   who have all been fighting each other the whole game
[01:52:11.000 --> 01:52:12.800]   all have to work together now
[01:52:12.800 --> 01:52:14.180]   to stop this other player from winning
[01:52:14.180 --> 01:52:15.740]   or else everybody's gonna lose.
[01:52:15.740 --> 01:52:18.800]   And it's kind of like "Game of Thrones."
[01:52:18.800 --> 01:52:19.960]   I don't know if you've seen the show
[01:52:19.960 --> 01:52:22.040]   where you got the others coming from the north
[01:52:22.040 --> 01:52:24.400]   and all the people have to work out their differences
[01:52:24.400 --> 01:52:26.580]   and stop them from taking over.
[01:52:26.580 --> 01:52:30.320]   And the bot will do this.
[01:52:30.320 --> 01:52:32.040]   The bot will work with the other players
[01:52:32.040 --> 01:52:33.920]   to stop the superpower from winning.
[01:52:33.920 --> 01:52:36.840]   But if it's trained from scratch
[01:52:36.840 --> 01:52:38.400]   or it doesn't really have a good grounding
[01:52:38.400 --> 01:52:39.720]   in how humans approach it,
[01:52:39.720 --> 01:52:43.000]   it will also at the same time attack the other players
[01:52:43.000 --> 01:52:44.380]   with its extra units.
[01:52:44.380 --> 01:52:46.060]   So all the units that are not necessary
[01:52:46.060 --> 01:52:47.540]   to stop the superpower from winning,
[01:52:47.540 --> 01:52:50.300]   it will use those to grab as many centers as possible
[01:52:50.300 --> 01:52:51.760]   from the other players.
[01:52:51.760 --> 01:52:55.020]   And in totally rational play,
[01:52:55.020 --> 01:52:56.880]   the other players should just live with that.
[01:52:56.880 --> 01:52:57.760]   They have to understand like,
[01:52:57.760 --> 01:53:00.840]   "Hey, a score of one is better than a score of zero.
[01:53:00.840 --> 01:53:04.120]   "So, okay, he's grabbed my centers,
[01:53:04.120 --> 01:53:06.080]   "but I'll just deal with it."
[01:53:06.080 --> 01:53:08.160]   But humans don't act that way, right?
[01:53:08.160 --> 01:53:10.560]   The human gets really angry at the bot
[01:53:10.560 --> 01:53:12.280]   and ends up throwing the game
[01:53:12.280 --> 01:53:15.160]   because I'm gonna screw you over
[01:53:15.160 --> 01:53:17.560]   because you did something that's not fair to me.
[01:53:17.560 --> 01:53:19.660]   - Got it.
[01:53:19.660 --> 01:53:20.720]   And are you supposed to model that?
[01:53:20.720 --> 01:53:24.840]   Is the bot supposed to model that kind of human frustration?
[01:53:24.840 --> 01:53:25.660]   - Yeah, exactly.
[01:53:25.660 --> 01:53:29.540]   So that is something that seems almost impossible to model
[01:53:29.540 --> 01:53:31.100]   purely from scratch without any human data.
[01:53:31.100 --> 01:53:32.760]   It's a very cultural thing.
[01:53:32.760 --> 01:53:36.980]   And so you need human data to be able to understand that,
[01:53:36.980 --> 01:53:38.780]   "Hey, that's how humans behave."
[01:53:38.780 --> 01:53:40.100]   And you have to work around that.
[01:53:40.100 --> 01:53:42.300]   It might be suboptimal, it might be irrational,
[01:53:42.300 --> 01:53:47.220]   but that's an aspect of humanity that you have to deal with.
[01:53:47.220 --> 01:53:49.540]   - So how difficult is it to train on human data
[01:53:49.540 --> 01:53:51.340]   given that human data is very limited
[01:53:51.340 --> 01:53:55.380]   versus what a purely self-play mechanism can generate?
[01:53:55.380 --> 01:53:57.100]   - That's actually one of the major challenges
[01:53:57.100 --> 01:53:58.220]   that we faced in the research,
[01:53:58.220 --> 01:53:59.980]   that we had a good amount of human data.
[01:53:59.980 --> 01:54:01.380]   We had about 50,000 games.
[01:54:01.380 --> 01:54:05.460]   What we try to do is leverage as much self-play as possible
[01:54:05.460 --> 01:54:08.780]   while still leveraging the human data.
[01:54:08.780 --> 01:54:11.660]   So what we do is we do self-play,
[01:54:11.660 --> 01:54:14.240]   very similar to how it's been done in poker and Go,
[01:54:14.240 --> 01:54:17.700]   but we try to regularize the self-play
[01:54:17.700 --> 01:54:18.880]   towards the human data.
[01:54:18.880 --> 01:54:21.220]   Basically, the way to think about it is
[01:54:23.100 --> 01:54:28.100]   we penalize the bot for choosing actions
[01:54:28.100 --> 01:54:32.240]   that are very unlikely under the human data set.
[01:54:32.240 --> 01:54:34.260]   - How do you know?
[01:54:34.260 --> 01:54:36.380]   Is there some kind of function that says,
[01:54:36.380 --> 01:54:38.260]   "This is human-like and not?"
[01:54:38.260 --> 01:54:41.980]   - Yeah, so we train a bot through supervised learning
[01:54:41.980 --> 01:54:44.140]   to model the human play as much as possible.
[01:54:44.140 --> 01:54:47.900]   So we basically train a neural net on those 50,000 games,
[01:54:47.900 --> 01:54:50.080]   and that gives us an approximate,
[01:54:50.080 --> 01:54:52.620]   that gives us a policy that resembles to some extent
[01:54:52.620 --> 01:54:54.300]   how humans actually play the game.
[01:54:54.300 --> 01:54:57.260]   Now, this isn't a perfect model of human play
[01:54:57.260 --> 01:54:58.580]   because we don't have unlimited data.
[01:54:58.580 --> 01:55:01.500]   We don't have unlimited neural net capacity,
[01:55:01.500 --> 01:55:03.580]   but it gives us some approximation.
[01:55:03.580 --> 01:55:05.220]   - Is there some data on the internet
[01:55:05.220 --> 01:55:07.620]   that's useful besides just diplomacy?
[01:55:07.620 --> 01:55:09.900]   So on the language side of things, is there some,
[01:55:09.900 --> 01:55:11.300]   can you go to like Reddit?
[01:55:11.300 --> 01:55:16.620]   And so sort of background model formulation
[01:55:16.620 --> 01:55:18.620]   that's useful for the game of diplomacy.
[01:55:18.620 --> 01:55:19.460]   - Yeah, absolutely.
[01:55:19.460 --> 01:55:20.300]   And so for the language model,
[01:55:20.300 --> 01:55:22.780]   which is kind of like a separate question,
[01:55:22.780 --> 01:55:25.420]   we didn't use the language model during self-play training,
[01:55:25.420 --> 01:55:29.020]   but we pre-trained the language model
[01:55:29.020 --> 01:55:32.660]   on tons of internet data as much as possible.
[01:55:32.660 --> 01:55:34.320]   And then we fine-tuned it specifically
[01:55:34.320 --> 01:55:35.700]   on the diplomacy games.
[01:55:35.700 --> 01:55:38.540]   So we are able to leverage the wider data set
[01:55:38.540 --> 01:55:41.520]   in order to fill in some of the gaps
[01:55:41.520 --> 01:55:44.340]   in how communication happens more broadly
[01:55:44.340 --> 01:55:47.100]   besides just specifically in these diplomacy games.
[01:55:47.100 --> 01:55:47.920]   - Okay, cool.
[01:55:47.920 --> 01:55:50.760]   What are some interesting things that came to life
[01:55:50.760 --> 01:55:53.280]   from this work to you?
[01:55:53.280 --> 01:55:58.280]   Like what are some insights about games
[01:55:58.280 --> 01:56:01.200]   where natural language is involved
[01:56:01.200 --> 01:56:04.400]   and cooperation, deep cooperation is involved?
[01:56:04.400 --> 01:56:06.040]   - Well, I think there's a few insights.
[01:56:06.040 --> 01:56:11.040]   So first of all, the fact that you can't rely purely
[01:56:11.040 --> 01:56:12.920]   or even largely on self-play,
[01:56:12.920 --> 01:56:14.640]   that you really have to have an understanding
[01:56:14.640 --> 01:56:16.240]   of how humans approach the game.
[01:56:17.480 --> 01:56:19.120]   I think that that's one of the major conclusions
[01:56:19.120 --> 01:56:20.640]   that I'm drawing from this work.
[01:56:20.640 --> 01:56:23.700]   And that is, I think, applicable more broadly
[01:56:23.700 --> 01:56:25.000]   to a lot of different games.
[01:56:25.000 --> 01:56:26.800]   So we've actually already taken the approaches
[01:56:26.800 --> 01:56:28.760]   that we've used in diplomacy and tried them
[01:56:28.760 --> 01:56:31.840]   on a cooperative card game called Hanabi.
[01:56:31.840 --> 01:56:34.440]   And we've had a lot of success in that game as well.
[01:56:34.440 --> 01:56:39.200]   On the language side, I think the fact
[01:56:39.200 --> 01:56:43.080]   that we were able to control the language model
[01:56:43.080 --> 01:56:47.240]   through this intense approach was very effective.
[01:56:47.240 --> 01:56:49.880]   And it allowed us, instead of just imitating
[01:56:49.880 --> 01:56:52.720]   how humans would communicate, we're able to go beyond that
[01:56:52.720 --> 01:56:57.720]   and able to feed into its superhuman strategies
[01:56:57.720 --> 01:57:02.600]   that it can then generate messages corresponding to.
[01:57:02.600 --> 01:57:04.800]   - Is there something you could say about detecting
[01:57:04.800 --> 01:57:07.900]   whether a person or AI is lying or not?
[01:57:07.900 --> 01:57:13.260]   - The bot doesn't explicitly try to calculate
[01:57:13.260 --> 01:57:15.140]   whether somebody is lying or not.
[01:57:15.140 --> 01:57:18.040]   But what it will do is try to predict
[01:57:18.040 --> 01:57:19.920]   what actions they're going to take,
[01:57:19.920 --> 01:57:22.200]   given the communications, given the messages
[01:57:22.200 --> 01:57:23.480]   that they've sent to us.
[01:57:23.480 --> 01:57:24.840]   So given our conversation,
[01:57:24.840 --> 01:57:26.040]   what do I think you're going to do?
[01:57:26.040 --> 01:57:28.800]   And implicitly, there is a calculation
[01:57:28.800 --> 01:57:30.800]   about whether you're lying to me in that.
[01:57:30.800 --> 01:57:34.620]   Based on your messages, if I think you're going
[01:57:34.620 --> 01:57:37.480]   to attack me this turn, even though your messages say
[01:57:37.480 --> 01:57:40.320]   that you're not, then essentially the bot
[01:57:40.320 --> 01:57:42.200]   is predicting that you're lying.
[01:57:42.200 --> 01:57:45.420]   But it doesn't view it as lying the same way
[01:57:45.420 --> 01:57:47.260]   that we would view it as lying.
[01:57:47.260 --> 01:57:51.100]   - But you could probably reformulate with all the same data
[01:57:51.100 --> 01:57:54.700]   and make a classifier lying or not.
[01:57:54.700 --> 01:57:56.620]   - Yeah, I think you could do that.
[01:57:56.620 --> 01:57:58.280]   That was not something that we were focused on,
[01:57:58.280 --> 01:58:00.240]   but I think that it is possible that,
[01:58:00.240 --> 01:58:03.340]   if you came up with some measurements of like,
[01:58:03.340 --> 01:58:04.660]   what does it mean to tell a lie?
[01:58:04.660 --> 01:58:06.000]   Because there's a spectrum, right?
[01:58:06.000 --> 01:58:10.540]   Like if you're withholding some information, is that a lie?
[01:58:10.540 --> 01:58:11.860]   If you're mostly telling the truth,
[01:58:11.860 --> 01:58:15.100]   but you forgot to mention this one action out of 10,
[01:58:15.100 --> 01:58:16.500]   is that a lie?
[01:58:16.500 --> 01:58:19.340]   It's hard to draw the line, but if you're willing to do that
[01:58:19.340 --> 01:58:22.580]   and then you could possibly use it to--
[01:58:22.580 --> 01:58:25.600]   - This feels like an argument inside a relationship now.
[01:58:25.600 --> 01:58:27.860]   What constitutes a lie?
[01:58:27.860 --> 01:58:32.260]   Depends what you mean by the definition of the word is.
[01:58:32.260 --> 01:58:37.260]   Okay, still it's fascinating because trust and lying
[01:58:37.260 --> 01:58:41.700]   is all intermixed into this and it's language models
[01:58:41.700 --> 01:58:43.540]   that are becoming more and more sophisticated.
[01:58:43.540 --> 01:58:45.860]   It's just a fascinating space to explore.
[01:58:45.860 --> 01:58:52.340]   What do you see as the future of this work
[01:58:52.340 --> 01:58:56.580]   that is inspired by the breakthrough performance
[01:58:56.580 --> 01:58:58.580]   that you're getting here with diplomacy?
[01:58:58.580 --> 01:59:03.220]   - I think there's a few different directions
[01:59:03.220 --> 01:59:04.180]   to take this work.
[01:59:04.180 --> 01:59:09.740]   I think really what it's showing us is the potential
[01:59:09.740 --> 01:59:10.740]   that language models have.
[01:59:10.740 --> 01:59:12.220]   I mean, I think a lot of people didn't think
[01:59:12.220 --> 01:59:14.980]   that this kind of result was possible even today,
[01:59:14.980 --> 01:59:17.540]   despite all the progress that's been made in language models.
[01:59:17.540 --> 01:59:21.420]   And so it shows us how we can leverage the power
[01:59:21.420 --> 01:59:24.260]   of things like self-play on top of language models
[01:59:24.260 --> 01:59:27.520]   to get increasingly better performance.
[01:59:27.520 --> 01:59:30.700]   And the ceiling is really much higher
[01:59:30.700 --> 01:59:32.340]   than what we have right now.
[01:59:32.340 --> 01:59:37.140]   - Is this transferable somehow to chatbots
[01:59:37.140 --> 01:59:39.620]   for the more general task of dialogue?
[01:59:40.620 --> 01:59:43.100]   So there is a kind of negotiation here,
[01:59:43.100 --> 01:59:46.780]   a dance between entities that are trying to cooperate
[01:59:46.780 --> 01:59:49.860]   and at the same time, a little bit adversarial,
[01:59:49.860 --> 01:59:53.880]   which I think maps somewhat to the general,
[01:59:53.880 --> 01:59:59.900]   the entire process of Reddit or like internet communication.
[01:59:59.900 --> 02:00:02.300]   You're cooperating, you're adversarial,
[02:00:02.300 --> 02:00:05.180]   you're having debates, you're having a camaraderie,
[02:00:05.180 --> 02:00:06.780]   all that kind of stuff.
[02:00:06.780 --> 02:00:08.700]   - I think one of the things that's really useful
[02:00:08.700 --> 02:00:11.540]   about diplomacy is that we have a well-defined
[02:00:11.540 --> 02:00:12.880]   value function.
[02:00:12.880 --> 02:00:15.260]   There is a well-defined score that the bot
[02:00:15.260 --> 02:00:16.660]   is trying to optimize.
[02:00:16.660 --> 02:00:20.580]   And in a setting like a general chatbot setting,
[02:00:20.580 --> 02:00:24.380]   it would need that kind of objective
[02:00:24.380 --> 02:00:26.420]   in order to fully leverage the techniques
[02:00:26.420 --> 02:00:27.460]   that we've developed.
[02:00:27.460 --> 02:00:30.940]   - What about like what we talked about earlier
[02:00:30.940 --> 02:00:33.380]   with NPCs inside video games?
[02:00:33.380 --> 02:00:35.140]   Like how can it be used to create
[02:00:36.420 --> 02:00:41.420]   for Elder Scrolls VI more compelling NPCs
[02:00:41.420 --> 02:00:44.740]   that you could talk to instead of committing
[02:00:44.740 --> 02:00:48.020]   all kinds of violence with a sword and fighting dragons,
[02:00:48.020 --> 02:00:49.940]   just sitting in a tavern and drink all day
[02:00:49.940 --> 02:00:51.580]   and talk to the chatbot?
[02:00:51.580 --> 02:00:53.300]   - The way that we've approached AI in diplomacy
[02:00:53.300 --> 02:00:56.380]   is you condition the language on an intent.
[02:00:56.380 --> 02:00:59.380]   Now that intent in diplomacy is an action,
[02:00:59.380 --> 02:01:00.420]   but it doesn't have to be.
[02:01:00.420 --> 02:01:04.340]   And you can imagine, you could have NPCs
[02:01:04.340 --> 02:01:06.700]   in video games or the metaverse or whatever,
[02:01:06.700 --> 02:01:09.500]   where there's some intent or there's some objective
[02:01:09.500 --> 02:01:10.540]   that they're trying to maximize,
[02:01:10.540 --> 02:01:12.200]   and you can specify what that is.
[02:01:12.200 --> 02:01:17.460]   And then the language can correspond to that intent.
[02:01:17.460 --> 02:01:19.820]   Now, I'm not saying that this is happening imminently,
[02:01:19.820 --> 02:01:22.500]   but I'm saying that this is like a future application
[02:01:22.500 --> 02:01:25.020]   potentially of this direction of research.
[02:01:25.020 --> 02:01:27.820]   - So what's the more general formulation of this?
[02:01:27.820 --> 02:01:30.940]   Making self-play be able to scale the way self-play does
[02:01:30.940 --> 02:01:33.740]   and still maintain human-like behavior.
[02:01:33.740 --> 02:01:37.460]   - The way that we've approached self-play in diplomacy
[02:01:37.460 --> 02:01:42.460]   is we're trying to come up with good intents
[02:01:42.460 --> 02:01:43.980]   to condition the language model on.
[02:01:43.980 --> 02:01:46.660]   And the space of intents is actions
[02:01:46.660 --> 02:01:47.980]   that can be played in the game.
[02:01:47.980 --> 02:01:51.420]   Now, there is the potential to have a broader set of intents,
[02:01:51.420 --> 02:01:56.420]   things like long-term cooperation or long-term objectives
[02:01:56.420 --> 02:02:01.080]   or gossip about what another player was saying.
[02:02:01.080 --> 02:02:03.140]   These are things that we're currently not conditioning
[02:02:03.140 --> 02:02:07.140]   the language model on, and so we're not able to control it
[02:02:07.140 --> 02:02:08.500]   to say like, "Oh, you should be talking
[02:02:08.500 --> 02:02:09.800]   "about this thing right now."
[02:02:09.800 --> 02:02:12.540]   But it's quite possible that you could expand
[02:02:12.540 --> 02:02:14.900]   the scope of intents to be able to allow it
[02:02:14.900 --> 02:02:16.080]   to talk about those things.
[02:02:16.080 --> 02:02:17.820]   Now, in the process of doing that,
[02:02:17.820 --> 02:02:20.220]   the self-play would become much more complicated.
[02:02:20.220 --> 02:02:23.820]   And so that is a potential for future work.
[02:02:23.820 --> 02:02:25.580]   - Okay, the increase in the number of intents.
[02:02:25.580 --> 02:02:30.580]   I still am not quite clear how you keep the self-play
[02:02:32.400 --> 02:02:34.960]   integrated into the human world.
[02:02:34.960 --> 02:02:35.800]   - Yeah.
[02:02:35.800 --> 02:02:39.380]   - I'm a little bit loose on understanding how you do that.
[02:02:39.380 --> 02:02:43.240]   - So we train in neural nets to imitate the human data
[02:02:43.240 --> 02:02:44.640]   as closely as possible,
[02:02:44.640 --> 02:02:47.000]   and that's what we call the anchor policy.
[02:02:47.000 --> 02:02:49.320]   And now when we're doing self-play,
[02:02:49.320 --> 02:02:50.920]   the problem with the anchor policy
[02:02:50.920 --> 02:02:53.680]   is that it's not a perfect approximation
[02:02:53.680 --> 02:02:54.960]   of how humans actually play.
[02:02:54.960 --> 02:02:56.520]   Because we don't have infinite data,
[02:02:56.520 --> 02:03:00.220]   because we don't have unlimited neural network capacity,
[02:03:00.220 --> 02:03:03.040]   it's actually a relatively suboptimal approximation
[02:03:03.040 --> 02:03:04.680]   of how humans actually play.
[02:03:04.680 --> 02:03:06.820]   And we can improve that approximation
[02:03:06.820 --> 02:03:10.220]   by adding planning and RL.
[02:03:10.220 --> 02:03:13.880]   And so what we do is we get a better approximation,
[02:03:13.880 --> 02:03:17.040]   a better model of human play by,
[02:03:17.040 --> 02:03:19.080]   during the self-play process,
[02:03:19.080 --> 02:03:24.080]   we say you can deviate from this human anchor policy
[02:03:24.080 --> 02:03:26.440]   if there is an action that has, you know,
[02:03:26.440 --> 02:03:28.280]   particularly high expected value.
[02:03:29.240 --> 02:03:32.260]   But it would have to be a really high expected value
[02:03:32.260 --> 02:03:36.180]   in order to deviate from this human-like policy.
[02:03:36.180 --> 02:03:37.500]   So you basically say,
[02:03:37.500 --> 02:03:39.500]   try to maximize your expected value
[02:03:39.500 --> 02:03:40.920]   while at the same time,
[02:03:40.920 --> 02:03:44.060]   stay as close as possible to the human policy.
[02:03:44.060 --> 02:03:46.580]   And there is a parameter that controls
[02:03:46.580 --> 02:03:50.660]   the relative weighting of those competing objectives.
[02:03:50.660 --> 02:03:52.620]   - So the question I have
[02:03:52.620 --> 02:03:55.380]   is how sophisticated can the anchor policy get?
[02:03:56.500 --> 02:03:59.840]   So I have a policy that approximates human behavior, right?
[02:03:59.840 --> 02:04:00.680]   - Yeah.
[02:04:00.680 --> 02:04:03.440]   - So as you increase the number of intents,
[02:04:03.440 --> 02:04:08.340]   as you generalize the space in which this is applicable,
[02:04:08.340 --> 02:04:11.260]   and given that the human data is limited,
[02:04:11.260 --> 02:04:14.260]   try to anticipate a policy that works
[02:04:14.260 --> 02:04:17.800]   for a much larger number of cases.
[02:04:17.800 --> 02:04:19.500]   Like how difficult is the process
[02:04:19.500 --> 02:04:22.640]   of forming a damn good anchor policy?
[02:04:22.640 --> 02:04:23.480]   - Well, it really comes down
[02:04:23.480 --> 02:04:25.280]   to how much human data you have.
[02:04:25.280 --> 02:04:27.600]   So it's all about scaling the human data.
[02:04:27.600 --> 02:04:30.040]   - I think the more human data you have, the better.
[02:04:30.040 --> 02:04:32.680]   And I think that that's going to be the major bottleneck
[02:04:32.680 --> 02:04:37.000]   in scaling to more complicated domains.
[02:04:37.000 --> 02:04:39.600]   But that said, there might be the potential,
[02:04:39.600 --> 02:04:40.800]   just like in the language model,
[02:04:40.800 --> 02:04:43.640]   where we leveraged tons of data on the internet
[02:04:43.640 --> 02:04:46.740]   and then specialized it for diplomacy.
[02:04:46.740 --> 02:04:47.900]   There is the future potential
[02:04:47.900 --> 02:04:50.820]   that you can leverage huge amounts of data across the board
[02:04:50.820 --> 02:04:53.600]   and then specialize it in the data set
[02:04:53.600 --> 02:04:54.560]   that you have for diplomacy.
[02:04:54.560 --> 02:04:56.360]   And that way you're essentially augmenting
[02:04:56.360 --> 02:04:58.540]   the amount of data that you have.
[02:04:58.540 --> 02:05:00.440]   - To what degree does this apply
[02:05:00.440 --> 02:05:06.400]   to the general, the real world diplomacy, the geopolitics?
[02:05:06.400 --> 02:05:11.040]   You know, there's a game theory has a history
[02:05:11.040 --> 02:05:13.120]   of being applied to understand
[02:05:13.120 --> 02:05:16.000]   and to give us hope about nuclear weapons, for example.
[02:05:16.000 --> 02:05:17.820]   The mutually assured destruction
[02:05:17.820 --> 02:05:21.020]   is a game theoretic concept that you can formulate.
[02:05:21.020 --> 02:05:23.080]   Some people say it's oversimplified,
[02:05:23.080 --> 02:05:24.800]   but nevertheless, here we are
[02:05:24.800 --> 02:05:27.320]   and we somehow haven't blown ourselves up.
[02:05:27.320 --> 02:05:32.320]   Do you see a future where this kind of system
[02:05:32.320 --> 02:05:35.960]   can be used to help us make decisions,
[02:05:35.960 --> 02:05:37.760]   geopolitical decisions in the world?
[02:05:37.760 --> 02:05:40.800]   - Well, like I said, the original motivation
[02:05:40.800 --> 02:05:44.060]   for the game of diplomacy was the failures of World War I,
[02:05:44.060 --> 02:05:46.680]   the diplomatic failures that led to war.
[02:05:46.680 --> 02:05:50.520]   And the real take-home message of diplomacy is that,
[02:05:50.520 --> 02:05:53.000]   you know, if people approach diplomacy,
[02:05:53.000 --> 02:05:57.380]   the right way, then war is ultimately unsuccessful.
[02:05:57.380 --> 02:06:00.440]   The way that I see it, war is
[02:06:00.440 --> 02:06:02.020]   an inherently negative sum game, right?
[02:06:02.020 --> 02:06:04.400]   There's always a better outcome than war
[02:06:04.400 --> 02:06:06.020]   for all the parties involved.
[02:06:06.020 --> 02:06:10.400]   And my hope is that, you know, as AI progresses,
[02:06:10.400 --> 02:06:12.480]   then maybe this technology could be used
[02:06:12.480 --> 02:06:17.000]   to help people make better decisions across the board
[02:06:17.000 --> 02:06:19.240]   and, you know, hopefully avoid
[02:06:19.240 --> 02:06:21.360]   negative sum outcomes like war.
[02:06:21.360 --> 02:06:24.120]   - Yeah, I mean, I just came back from Ukraine.
[02:06:24.120 --> 02:06:25.440]   I'm going back there.
[02:06:25.440 --> 02:06:30.200]   On deep personal levels, think a lot about
[02:06:30.200 --> 02:06:34.360]   how peace can be achieved.
[02:06:34.360 --> 02:06:36.520]   And I'm a big believer in conversation,
[02:06:36.520 --> 02:06:39.920]   leaders getting together and having conversations
[02:06:39.920 --> 02:06:42.560]   and trying to understand each other.
[02:06:42.560 --> 02:06:44.820]   Yeah, it's fascinating to think
[02:06:44.820 --> 02:06:46.120]   whether each one of those leaders
[02:06:46.120 --> 02:06:48.560]   can run a simulation ahead of time.
[02:06:48.560 --> 02:06:50.000]   Like if I'm an asshole,
[02:06:50.840 --> 02:06:52.080]   (chuckles)
[02:06:52.080 --> 02:06:53.520]   what are the possible consequences?
[02:06:53.520 --> 02:06:56.720]   If I'm nice, what are the possible consequences?
[02:06:56.720 --> 02:07:01.200]   My guess is that if the president of the United States
[02:07:01.200 --> 02:07:06.200]   got together with Vladimir Zelensky and Vladimir Putin,
[02:07:06.200 --> 02:07:10.240]   that there would be significant benefits
[02:07:10.240 --> 02:07:14.880]   to the president of the United States not having the ego
[02:07:14.880 --> 02:07:19.280]   of kind of playing down, of giving away a lot of chips
[02:07:19.280 --> 02:07:22.200]   for the future success of a world.
[02:07:22.200 --> 02:07:24.500]   So giving a lot of power to the two presidents
[02:07:24.500 --> 02:07:27.300]   of the competing nations to achieve peace.
[02:07:27.300 --> 02:07:29.120]   That's my guess,
[02:07:29.120 --> 02:07:31.640]   but it'd be nice to run a bunch of simulations.
[02:07:31.640 --> 02:07:33.280]   But then you have to have human data, right?
[02:07:33.280 --> 02:07:35.840]   You really, 'cause it's like the game of diplomacy
[02:07:35.840 --> 02:07:37.760]   is fundamentally different than geopolitics.
[02:07:37.760 --> 02:07:39.040]   You need data.
[02:07:39.040 --> 02:07:42.120]   You need like, I guess that's the question I have.
[02:07:42.120 --> 02:07:44.160]   Like how transferable is this to,
[02:07:44.160 --> 02:07:47.600]   like I don't know, any kind of negotiation, right?
[02:07:47.600 --> 02:07:50.160]   Like to any kind of, some local, I don't know,
[02:07:50.160 --> 02:07:52.480]   a bunch of lawyers like arguing,
[02:07:52.480 --> 02:07:55.360]   like a divorce, like divorce lawyers.
[02:07:55.360 --> 02:07:56.800]   Like how transferable is this
[02:07:56.800 --> 02:07:58.840]   to all kinds of human negotiation?
[02:07:58.840 --> 02:08:00.440]   - Well, I feel like this isn't a question
[02:08:00.440 --> 02:08:01.440]   that's unique to diplomacy.
[02:08:01.440 --> 02:08:03.880]   I mean, I think you look at RL breakthroughs,
[02:08:03.880 --> 02:08:05.160]   reinforcement learning breakthroughs
[02:08:05.160 --> 02:08:06.280]   in previous games as well,
[02:08:06.280 --> 02:08:09.200]   like AI for StarCraft, AI for Atari.
[02:08:09.200 --> 02:08:11.780]   You haven't really seen it deployed in the real world
[02:08:11.780 --> 02:08:13.840]   because you have these problems of,
[02:08:13.840 --> 02:08:16.600]   it's really hard to collect a lot of data
[02:08:16.600 --> 02:08:21.280]   and you don't have a well-defined action space.
[02:08:21.280 --> 02:08:23.360]   You don't have a well-defined reward function.
[02:08:23.360 --> 02:08:25.520]   These are all things that you really need
[02:08:25.520 --> 02:08:27.200]   for reinforcement learning
[02:08:27.200 --> 02:08:29.440]   and planning to be really successful today.
[02:08:29.440 --> 02:08:32.800]   Now, there are some domains where you do have that.
[02:08:32.800 --> 02:08:35.360]   Code generation is one example.
[02:08:35.360 --> 02:08:37.520]   Theorem proving mathematics, that's another example
[02:08:37.520 --> 02:08:39.000]   where you have a well-defined action space.
[02:08:39.000 --> 02:08:40.920]   You have a well-defined reward function.
[02:08:40.920 --> 02:08:42.840]   And those are the kinds of domains
[02:08:42.840 --> 02:08:45.560]   where I can see RL in the short term
[02:08:45.560 --> 02:08:47.120]   being incredibly powerful.
[02:08:47.120 --> 02:08:51.120]   But yeah, I think that those are the barriers
[02:08:51.120 --> 02:08:53.300]   to deploying this at scale in the real world.
[02:08:53.300 --> 02:08:55.280]   But the hope is that in the long run,
[02:08:55.280 --> 02:08:57.080]   we'll be able to get there.
[02:08:57.080 --> 02:08:59.900]   - Yeah, but see, diplomacy feels like closer
[02:08:59.900 --> 02:09:02.560]   to the real world than does StarCraft.
[02:09:02.560 --> 02:09:04.620]   Like 'cause it's natural language, right?
[02:09:04.620 --> 02:09:06.200]   You're operating in the space of intents
[02:09:06.200 --> 02:09:07.640]   and in the space of natural language,
[02:09:07.640 --> 02:09:09.520]   that feels very close to the real world.
[02:09:09.520 --> 02:09:12.880]   And it also feels like you could get data on that
[02:09:12.880 --> 02:09:14.760]   from the internet.
[02:09:14.760 --> 02:09:17.400]   - Yeah, and that's why I do think that diplomacy
[02:09:17.400 --> 02:09:20.360]   is taking a big step closer to the real world
[02:09:20.360 --> 02:09:21.440]   than anything that's came before
[02:09:21.440 --> 02:09:23.200]   in terms of game AI breakthroughs.
[02:09:23.200 --> 02:09:27.920]   The fact that we're communicating in natural language,
[02:09:27.920 --> 02:09:30.640]   we're leveraging the fact that we have this
[02:09:30.640 --> 02:09:35.320]   like general data set of dialogue and communication
[02:09:35.320 --> 02:09:37.100]   from a breadth of the internet.
[02:09:37.100 --> 02:09:39.940]   That is a big step in that direction.
[02:09:39.940 --> 02:09:44.020]   We're not 100% there, but we're getting closer at least.
[02:09:44.020 --> 02:09:47.320]   - So if we actually return back to poker and chess,
[02:09:47.320 --> 02:09:48.920]   are some of the ideas that you're learning here
[02:09:48.920 --> 02:09:52.320]   with diplomacy, could you construct AI systems
[02:09:52.320 --> 02:09:55.080]   that play like humans?
[02:09:55.080 --> 02:10:00.080]   Like make for a fun opponent in a game of chess?
[02:10:00.080 --> 02:10:01.240]   - Yeah, absolutely.
[02:10:01.240 --> 02:10:03.080]   We've already started looking into this direction a bit.
[02:10:03.080 --> 02:10:05.440]   So we tried to use the techniques that we've developed
[02:10:05.440 --> 02:10:08.840]   for diplomacy to make chess and go AIs.
[02:10:08.840 --> 02:10:13.080]   And what we found is that it led to much more human-like
[02:10:13.080 --> 02:10:15.600]   strong chess and go players.
[02:10:15.600 --> 02:10:19.620]   The way that AIs like Stockfish today play
[02:10:19.620 --> 02:10:21.580]   is in a very inhuman style.
[02:10:21.580 --> 02:10:23.460]   It's very strong, but it's very different
[02:10:23.460 --> 02:10:25.120]   from how humans play.
[02:10:25.120 --> 02:10:27.020]   And so we can take the techniques
[02:10:27.020 --> 02:10:28.280]   that we've developed for diplomacy.
[02:10:28.280 --> 02:10:32.400]   We do something similar in chess and go,
[02:10:32.400 --> 02:10:36.440]   and we end up with a bot that's both strong and human-like.
[02:10:36.440 --> 02:10:39.440]   To elaborate on this a bit,
[02:10:39.440 --> 02:10:43.000]   like one way to approach making a human-like
[02:10:43.000 --> 02:10:47.560]   AI for chess is to collect a bunch of human games,
[02:10:47.560 --> 02:10:49.560]   like a bunch of human grandmaster games,
[02:10:49.560 --> 02:10:52.200]   and just to supervise learning on those games.
[02:10:52.200 --> 02:10:53.880]   But the problem is that if you do that,
[02:10:53.880 --> 02:10:57.200]   what you end up with is an AI that's substantially weaker
[02:10:57.200 --> 02:10:59.760]   than the human grandmasters that you trained on.
[02:10:59.760 --> 02:11:03.520]   Because the neural net is not able to approximate
[02:11:03.520 --> 02:11:06.240]   the nuance of the strategy.
[02:11:06.240 --> 02:11:08.600]   This goes back to the planning thing that I mentioned,
[02:11:08.600 --> 02:11:10.520]   the search thing that I talked about before,
[02:11:10.520 --> 02:11:12.640]   that these human grandmasters, when they're playing,
[02:11:12.640 --> 02:11:15.560]   they're using search and they're using planning.
[02:11:15.560 --> 02:11:17.640]   And the neural net alone,
[02:11:17.640 --> 02:11:19.280]   unless you have a massive neural net
[02:11:19.280 --> 02:11:20.400]   that's like a thousand times bigger
[02:11:20.400 --> 02:11:22.080]   than what we have right now,
[02:11:22.080 --> 02:11:25.720]   it's not able to approximate those details very effectively.
[02:11:25.720 --> 02:11:28.560]   And on the other hand,
[02:11:28.560 --> 02:11:32.480]   you can leverage search and planning very heavily,
[02:11:32.480 --> 02:11:34.120]   but then what you end up with is an AI
[02:11:34.120 --> 02:11:35.800]   that plays in a very different style
[02:11:35.800 --> 02:11:37.720]   from how humans play the game.
[02:11:37.720 --> 02:11:40.020]   Now, if you strike this intermediate balance
[02:11:40.020 --> 02:11:43.280]   by setting the regularization parameters correctly
[02:11:43.280 --> 02:11:44.440]   and say, you can do planning,
[02:11:44.440 --> 02:11:46.880]   but try to keep it close to the human policy,
[02:11:46.880 --> 02:11:49.800]   then you end up with an AI that plays
[02:11:49.800 --> 02:11:54.080]   in both a very human-like style and a very strong style.
[02:11:54.080 --> 02:11:55.820]   And you can actually even tune it
[02:11:55.820 --> 02:11:58.320]   to have a certain ELO rating.
[02:11:58.320 --> 02:12:01.480]   So you can say, play in the style of like a 2800 ELO human.
[02:12:01.480 --> 02:12:04.920]   - I wonder if you could do specific type of humans
[02:12:04.920 --> 02:12:09.520]   or categories of humans, not just skill, but style.
[02:12:09.520 --> 02:12:10.400]   - Yeah, I think so.
[02:12:10.400 --> 02:12:13.720]   And so this is where the research gets interesting.
[02:12:13.720 --> 02:12:16.760]   Like, one of the things that I was thinking about is,
[02:12:16.760 --> 02:12:18.280]   and this is actually already being done,
[02:12:18.280 --> 02:12:19.880]   there's a researcher at the University of Toronto
[02:12:19.880 --> 02:12:21.760]   that's working on this,
[02:12:21.760 --> 02:12:23.200]   is to make an AI that plays
[02:12:23.200 --> 02:12:25.480]   in the style of a particular player.
[02:12:25.480 --> 02:12:26.960]   Like Magnus Carlsen, for example,
[02:12:26.960 --> 02:12:29.680]   you can make an AI that plays like Magnus Carlsen.
[02:12:29.680 --> 02:12:31.440]   And then where I think this gets interesting is like,
[02:12:31.440 --> 02:12:33.320]   maybe you're up against Magnus Carlsen
[02:12:33.320 --> 02:12:35.180]   in the world championship or something,
[02:12:35.180 --> 02:12:37.800]   you can play against this Magnus Carlsen bot
[02:12:37.800 --> 02:12:40.480]   to prepare against the real Magnus Carlsen.
[02:12:40.480 --> 02:12:42.600]   And you can try to explore strategies
[02:12:42.600 --> 02:12:46.000]   that he might struggle with and try to figure out like,
[02:12:46.000 --> 02:12:48.880]   how do you beat this player in particular?
[02:12:48.880 --> 02:12:51.040]   On the other hand, you can also have Magnus Carlsen
[02:12:51.040 --> 02:12:53.700]   working with this bot to try to figure out where he's weak
[02:12:53.700 --> 02:12:56.260]   and where he needs to improve his strategy.
[02:12:56.260 --> 02:12:59.540]   And so I can envision this future
[02:12:59.540 --> 02:13:03.600]   where data on specific chess and Go players
[02:13:03.600 --> 02:13:06.300]   becomes extremely valuable because you can use that data
[02:13:06.300 --> 02:13:08.180]   to create specific models
[02:13:08.180 --> 02:13:10.040]   of how these particular players play.
[02:13:10.040 --> 02:13:13.400]   - So increasingly human-like behavior in bots, however,
[02:13:13.400 --> 02:13:17.120]   as you've mentioned, makes cheating,
[02:13:17.120 --> 02:13:19.060]   cheat detection much harder.
[02:13:19.060 --> 02:13:20.040]   - It does, yeah.
[02:13:20.040 --> 02:13:23.540]   The way that cheat detection works in a game like poker
[02:13:23.540 --> 02:13:26.100]   and a game like chess and Go, from what I understand,
[02:13:26.100 --> 02:13:30.180]   is trying to see like, is this person making moves
[02:13:30.180 --> 02:13:35.180]   that are very common among chess AIs or AIs in general?
[02:13:36.160 --> 02:13:40.680]   But very uncommon among top human players.
[02:13:40.680 --> 02:13:43.920]   And if you have the development of these AIs
[02:13:43.920 --> 02:13:46.280]   that play in a very strong style,
[02:13:46.280 --> 02:13:48.280]   but also a very human-like style,
[02:13:48.280 --> 02:13:51.280]   then that poses serious challenges for cheat detection.
[02:13:51.280 --> 02:13:54.280]   - And it makes you now ask yourself a hard question
[02:13:54.280 --> 02:13:56.720]   about what is the role of AI systems
[02:13:56.720 --> 02:13:59.800]   as they become more and more integrated in our society?
[02:13:59.800 --> 02:14:02.800]   And this kind of human AI integration
[02:14:03.620 --> 02:14:08.620]   has some deep ethical issues that we should be aware of.
[02:14:08.620 --> 02:14:13.140]   And also it's a kind of cybersecurity challenge, right?
[02:14:13.140 --> 02:14:14.260]   For it to make, you know,
[02:14:14.260 --> 02:14:17.100]   one of the assumptions we have when we play games
[02:14:17.100 --> 02:14:20.140]   is that there's a trust that it's only humans involved.
[02:14:20.140 --> 02:14:24.740]   And the better AI systems we create,
[02:14:24.740 --> 02:14:26.140]   which makes it super exciting,
[02:14:26.140 --> 02:14:28.700]   human-like AI systems with different styles of humans
[02:14:28.700 --> 02:14:29.740]   is really exciting,
[02:14:29.740 --> 02:14:32.060]   but then we have to have the defenses
[02:14:32.060 --> 02:14:33.600]   better and better and better
[02:14:33.600 --> 02:14:36.880]   if we're to trust that we can enjoy
[02:14:36.880 --> 02:14:40.760]   human versus human game in a deeply fair way.
[02:14:40.760 --> 02:14:41.600]   It's fascinating.
[02:14:41.600 --> 02:14:44.320]   It's just, it's humbling.
[02:14:44.320 --> 02:14:47.160]   - Yeah, I think there's a lot of like negative potential
[02:14:47.160 --> 02:14:48.240]   for this kind of technology,
[02:14:48.240 --> 02:14:49.760]   but you know, at the same time,
[02:14:49.760 --> 02:14:51.600]   there's a lot of upside for it as well.
[02:14:51.600 --> 02:14:53.520]   So, you know, for example, right now,
[02:14:53.520 --> 02:14:55.720]   it's really hard to learn how to get better
[02:14:55.720 --> 02:14:57.800]   in games like chess and poker and Go
[02:14:57.800 --> 02:15:00.000]   because the way that the AI plays
[02:15:00.000 --> 02:15:02.280]   is so foreign and incomprehensible.
[02:15:02.280 --> 02:15:04.520]   But if you have these AIs that are playing,
[02:15:04.520 --> 02:15:05.360]   you know, you can say like,
[02:15:05.360 --> 02:15:08.240]   oh, I'm a 2000 Elo human, how do I get to 2200?
[02:15:08.240 --> 02:15:10.760]   Now you can have an AI that plays in the style
[02:15:10.760 --> 02:15:14.160]   of a 2200 Elo human, and that will help you get better.
[02:15:14.160 --> 02:15:16.840]   Or, you know, you mentioned this problem of like,
[02:15:16.840 --> 02:15:19.440]   how do you know that you're actually playing with humans
[02:15:19.440 --> 02:15:22.000]   when you're playing like online and in video games?
[02:15:22.000 --> 02:15:24.760]   Well, now we have the potential of populating
[02:15:24.760 --> 02:15:28.560]   these like virtual worlds with agents,
[02:15:28.560 --> 02:15:30.720]   like AI agents that are actually fun to play with,
[02:15:30.720 --> 02:15:33.680]   and you don't have to always be playing with other humans
[02:15:33.680 --> 02:15:36.320]   to, you know, have a fun time.
[02:15:36.320 --> 02:15:38.560]   So yeah, a lot of upside potential too.
[02:15:38.560 --> 02:15:40.320]   And I think, you know, with any sort of tool,
[02:15:40.320 --> 02:15:42.760]   there's the potential for a lot of greatness
[02:15:42.760 --> 02:15:44.520]   and a lot of downsides as well.
[02:15:44.520 --> 02:15:47.080]   - So in the paper that I got a chance to look at,
[02:15:47.080 --> 02:15:50.600]   there's a section on ethical considerations.
[02:15:50.600 --> 02:15:51.640]   What's in that section?
[02:15:51.640 --> 02:15:53.560]   What are some ethical considerations here?
[02:15:53.560 --> 02:15:55.760]   Is it some of the stuff we already talked about?
[02:15:55.760 --> 02:15:57.520]   - There's some things that we've already talked about.
[02:15:57.520 --> 02:16:01.240]   I think specific to diplomacy, you know,
[02:16:01.240 --> 02:16:04.960]   there's also the challenge that the game is,
[02:16:04.960 --> 02:16:07.360]   you know, there is a deception aspect to the game.
[02:16:07.360 --> 02:16:11.360]   And so, you know, developing language models
[02:16:11.360 --> 02:16:14.280]   that are capable of deception is I think a dicey issue
[02:16:14.280 --> 02:16:16.160]   and something that, you know,
[02:16:16.160 --> 02:16:18.800]   makes research on diplomacy particularly challenging.
[02:16:18.800 --> 02:16:24.480]   And, you know, so those kinds of issues of like,
[02:16:24.480 --> 02:16:25.920]   should we even be developing AIs
[02:16:25.920 --> 02:16:27.120]   that are capable of lying to people?
[02:16:27.120 --> 02:16:28.840]   That's something that we have to, you know,
[02:16:28.840 --> 02:16:30.480]   think carefully about.
[02:16:30.480 --> 02:16:31.320]   - That's so cool.
[02:16:31.320 --> 02:16:32.680]   I mean, you have to do that kind of stuff
[02:16:32.680 --> 02:16:35.080]   in order to figure out where the ethical lines are.
[02:16:35.080 --> 02:16:37.120]   But I can see in the future it being illegal
[02:16:37.120 --> 02:16:41.840]   to have a consumer product that lies.
[02:16:41.840 --> 02:16:44.080]   - Yeah, yeah.
[02:16:44.080 --> 02:16:46.240]   - Like your personal assistant AI system
[02:16:46.240 --> 02:16:48.360]   is always have to tell the truth.
[02:16:48.360 --> 02:16:50.960]   But if I ask it, do I look,
[02:16:50.960 --> 02:16:53.360]   did I get fatter over the past month?
[02:16:53.360 --> 02:16:55.820]   I sure as hell want that AI system to lie to me.
[02:16:56.880 --> 02:17:01.320]   So there's a trade off between lying and being nice.
[02:17:01.320 --> 02:17:02.560]   We have to somehow find,
[02:17:02.560 --> 02:17:04.920]   what is the ethics in that?
[02:17:04.920 --> 02:17:07.400]   And we're back to discussions inside relationships.
[02:17:07.400 --> 02:17:08.600]   Anyway, what were you saying?
[02:17:08.600 --> 02:17:09.840]   - Oh, yeah, I was saying like, yeah,
[02:17:09.840 --> 02:17:11.520]   that's kind of going to the question of like,
[02:17:11.520 --> 02:17:12.360]   what is a lie?
[02:17:12.360 --> 02:17:13.840]   You know, is a white lie a bad lie?
[02:17:13.840 --> 02:17:15.080]   Is it an ethical lie?
[02:17:15.080 --> 02:17:16.840]   You know, those kinds of questions.
[02:17:16.840 --> 02:17:20.520]   - Boy, we return time and time again
[02:17:20.520 --> 02:17:23.280]   to deep human questions as we design AI systems.
[02:17:23.280 --> 02:17:24.360]   That's exactly what they do.
[02:17:24.360 --> 02:17:26.680]   They put a mirror to humanity
[02:17:26.680 --> 02:17:29.040]   to help us understand ourselves.
[02:17:29.040 --> 02:17:31.000]   - There's also the issue of like, you know,
[02:17:31.000 --> 02:17:32.360]   in these diplomacy experiments,
[02:17:32.360 --> 02:17:35.320]   in order to do a fair comparison,
[02:17:35.320 --> 02:17:36.920]   you know, what we found is that
[02:17:36.920 --> 02:17:40.760]   there's an inherent anti-AI bias in these kinds of games.
[02:17:40.760 --> 02:17:43.160]   So we actually played a tournament
[02:17:43.160 --> 02:17:45.000]   in a non-language version of the game,
[02:17:45.000 --> 02:17:47.000]   where, you know, we told the participants like,
[02:17:47.000 --> 02:17:49.440]   hey, in every single game, there's going to be an AI.
[02:17:49.440 --> 02:17:51.960]   And what we found is that the humans
[02:17:51.960 --> 02:17:53.580]   would spend basically the entire game,
[02:17:53.580 --> 02:17:55.040]   like trying to figure out who the bot was.
[02:17:55.040 --> 02:17:56.480]   And then as soon as they thought they figured it out,
[02:17:56.480 --> 02:17:58.480]   they would all team up and try to kill it.
[02:17:58.480 --> 02:18:03.880]   And, you know, overcoming that inherent anti-AI bias
[02:18:03.880 --> 02:18:05.520]   is a challenge.
[02:18:05.520 --> 02:18:10.520]   - On the flip side, I think when robots become the enemy,
[02:18:10.520 --> 02:18:14.560]   that's when we get to heal our human divisions,
[02:18:14.560 --> 02:18:16.800]   and then we can become one.
[02:18:16.800 --> 02:18:18.000]   As long as we have one enemy,
[02:18:18.000 --> 02:18:20.680]   it's that Reagan thing when the aliens show up,
[02:18:20.680 --> 02:18:22.360]   that's when we put our side,
[02:18:22.360 --> 02:18:25.420]   our divisions will become one human species.
[02:18:25.420 --> 02:18:26.560]   - Right, we might have our differences,
[02:18:26.560 --> 02:18:27.760]   but we're at least all human.
[02:18:27.760 --> 02:18:30.280]   - At least we all hate the robots.
[02:18:30.280 --> 02:18:31.640]   No, no, no, no.
[02:18:31.640 --> 02:18:33.120]   I think there will be actually in the future
[02:18:33.120 --> 02:18:35.440]   something like a civil rights movement for robots.
[02:18:35.440 --> 02:18:38.200]   I think that's the fascinating thing about AI systems,
[02:18:38.200 --> 02:18:41.360]   and that is they ask, they force us to ask about
[02:18:41.360 --> 02:18:44.660]   ethical questions about what is sentience,
[02:18:44.660 --> 02:18:47.040]   what is, how do we feel about systems
[02:18:47.040 --> 02:18:48.200]   that are capable of suffering,
[02:18:48.200 --> 02:18:50.400]   or capable of displaying suffering?
[02:18:50.400 --> 02:18:53.800]   And how do we design products that show emotion and not?
[02:18:53.800 --> 02:18:54.920]   How do we feel about that?
[02:18:54.920 --> 02:18:56.920]   Lying is another topic.
[02:18:56.920 --> 02:19:00.060]   Are we going to allow bots to lie and not?
[02:19:00.060 --> 02:19:02.440]   And where's the balance between being nice
[02:19:02.440 --> 02:19:04.640]   and telling the truth?
[02:19:04.640 --> 02:19:07.040]   I mean, these are all fascinating human questions,
[02:19:07.040 --> 02:19:09.160]   and it's so exciting to be in a century
[02:19:09.160 --> 02:19:11.800]   where we can create systems that
[02:19:11.800 --> 02:19:15.040]   take these philosophical questions
[02:19:15.040 --> 02:19:17.280]   that have been asked for centuries,
[02:19:17.280 --> 02:19:20.100]   and now we can engineer them inside systems,
[02:19:20.100 --> 02:19:22.280]   where you really have to answer them,
[02:19:22.280 --> 02:19:26.960]   because you'll have transformational impact on human society
[02:19:26.960 --> 02:19:30.040]   depending on what you design inside those systems.
[02:19:30.040 --> 02:19:31.400]   It's fascinating.
[02:19:31.400 --> 02:19:33.480]   And like you said, I feel like diplomacy
[02:19:33.480 --> 02:19:35.960]   is a step towards the direction of the real world,
[02:19:35.960 --> 02:19:38.880]   applying these RL methods towards the real world.
[02:19:38.880 --> 02:19:41.960]   From all the breakthrough performances
[02:19:41.960 --> 02:19:44.380]   in Go and Chess and StarCraft and Dota,
[02:19:44.380 --> 02:19:46.760]   this feels like the real world.
[02:19:46.760 --> 02:19:49.300]   Especially now my mind's been on war,
[02:19:49.300 --> 02:19:50.720]   and military conflict.
[02:19:50.720 --> 02:19:53.320]   This feels like it can give us some deep insights
[02:19:53.320 --> 02:19:56.640]   about human behavior at the large geopolitical scale.
[02:19:56.640 --> 02:20:03.520]   What do you think is the breakthrough
[02:20:03.520 --> 02:20:08.960]   or the directions of work that will take us
[02:20:08.960 --> 02:20:13.400]   towards solving intelligence, towards creating AGI systems?
[02:20:13.400 --> 02:20:15.600]   You've been a part of creating,
[02:20:15.600 --> 02:20:19.220]   by the way, we should say a part of great teams
[02:20:19.220 --> 02:20:23.040]   that do this, of creating systems
[02:20:23.040 --> 02:20:24.860]   that achieve breakthrough performances
[02:20:24.860 --> 02:20:28.400]   on before thought unsolvable problems,
[02:20:28.400 --> 02:20:32.040]   like poker, multiplayer poker, diplomacy.
[02:20:32.040 --> 02:20:35.680]   We're taking steps towards that direction.
[02:20:35.680 --> 02:20:37.520]   What do you think it takes to go all the way
[02:20:37.520 --> 02:20:40.640]   to create superhuman level intelligence?
[02:20:40.640 --> 02:20:41.480]   - You know, there's a lot of people
[02:20:41.480 --> 02:20:43.360]   trying to figure that out right now.
[02:20:43.360 --> 02:20:46.400]   And I should say, the amount of progress that's been made,
[02:20:46.400 --> 02:20:49.360]   especially in the past few years is truly phenomenal.
[02:20:49.360 --> 02:20:52.360]   I mean, you look at where AI was 10 years ago,
[02:20:52.360 --> 02:20:53.840]   and the idea that you can have AIs
[02:20:53.840 --> 02:20:56.200]   that can generate language and generate images
[02:20:56.200 --> 02:20:57.480]   the way they're doing today,
[02:20:57.480 --> 02:20:59.440]   and able to play a game like diplomacy
[02:20:59.440 --> 02:21:03.400]   was just unthinkable, even five years ago,
[02:21:03.400 --> 02:21:04.560]   let alone 10 years ago.
[02:21:04.560 --> 02:21:11.240]   Now, there are aspects of AI that I think are still lacking.
[02:21:11.240 --> 02:21:15.240]   I think there's general agreement
[02:21:15.240 --> 02:21:17.560]   that one of the major issues with AI today
[02:21:17.560 --> 02:21:19.920]   is that it's very data inefficient.
[02:21:19.920 --> 02:21:24.000]   It requires a huge number of samples of training examples
[02:21:24.000 --> 02:21:25.320]   to be able to train.
[02:21:25.320 --> 02:21:27.320]   You know, you look at an AI that plays Go,
[02:21:27.320 --> 02:21:29.920]   and it needs millions of games of Go
[02:21:29.920 --> 02:21:32.000]   to learn how to play the game well.
[02:21:32.000 --> 02:21:34.160]   Whereas a human can pick it up in like, you know,
[02:21:34.160 --> 02:21:36.560]   I don't know, how many games does a human Go player,
[02:21:36.560 --> 02:21:38.720]   Go grandmaster play in their lifetime?
[02:21:38.720 --> 02:21:41.280]   Probably, you know, in the thousands
[02:21:41.280 --> 02:21:42.780]   or tens of thousands, I guess.
[02:21:45.040 --> 02:21:46.200]   So that's one issue.
[02:21:46.200 --> 02:21:47.080]   - Data efficiency.
[02:21:47.080 --> 02:21:48.680]   - Overcoming this challenge of data efficiency.
[02:21:48.680 --> 02:21:50.120]   And this is particularly important
[02:21:50.120 --> 02:21:55.120]   if we want to deploy AI systems in real world settings
[02:21:55.120 --> 02:21:56.480]   where they're interacting with humans,
[02:21:56.480 --> 02:21:58.440]   because, you know, for example, with robotics,
[02:21:58.440 --> 02:22:01.360]   it's really hard to generate a huge number of samples.
[02:22:01.360 --> 02:22:04.160]   It's a different story when you're working in these,
[02:22:04.160 --> 02:22:06.100]   you know, totally virtual games
[02:22:06.100 --> 02:22:08.520]   where you can play a million games and it's no big deal.
[02:22:08.520 --> 02:22:10.480]   - I was planning on just launching like a thousand
[02:22:10.480 --> 02:22:12.320]   of these robots in Austin.
[02:22:12.320 --> 02:22:14.520]   I don't think it's illegal for legged robots
[02:22:14.520 --> 02:22:16.920]   to roam the streets and just collect data.
[02:22:16.920 --> 02:22:17.760]   - That's not a crazy idea.
[02:22:17.760 --> 02:22:18.580]   - Of course, the worst that could happen.
[02:22:18.580 --> 02:22:20.080]   - Yeah, I mean, that's one way
[02:22:20.080 --> 02:22:21.640]   to overcome the data efficiency problem.
[02:22:21.640 --> 02:22:23.800]   It's like scale it, yeah.
[02:22:23.800 --> 02:22:26.280]   - Like I actually tried to see if there's a law
[02:22:26.280 --> 02:22:30.080]   against robots, like legged robots just operating
[02:22:30.080 --> 02:22:34.040]   in the streets of a major city and there isn't,
[02:22:34.040 --> 02:22:35.160]   I couldn't find any.
[02:22:35.160 --> 02:22:38.840]   So I'll take it all the way to the Supreme Court.
[02:22:38.840 --> 02:22:40.840]   Robot rights.
[02:22:40.840 --> 02:22:42.320]   Okay, anyway, sorry, you were saying.
[02:22:42.320 --> 02:22:45.880]   So what are the ideas for getting,
[02:22:45.880 --> 02:22:48.040]   becoming more data efficient?
[02:22:48.040 --> 02:22:50.840]   - I mean, that's the trillion dollar question in AI today.
[02:22:50.840 --> 02:22:52.840]   I mean, if you can figure out how to make AI systems
[02:22:52.840 --> 02:22:57.520]   more data efficient, then that's a huge breakthrough.
[02:22:57.520 --> 02:22:58.960]   So nobody really knows right now.
[02:22:58.960 --> 02:23:01.480]   - It could be just a gigantic background model,
[02:23:01.480 --> 02:23:03.780]   language model, and then you do,
[02:23:03.780 --> 02:23:07.360]   the training becomes like prompting that model
[02:23:10.000 --> 02:23:11.960]   to essentially do a kind of querying,
[02:23:11.960 --> 02:23:14.580]   a search into the space of the things it's learned
[02:23:14.580 --> 02:23:17.480]   to customize that to whatever problem you're trying to solve.
[02:23:17.480 --> 02:23:20.080]   So maybe if you form a large enough language model,
[02:23:20.080 --> 02:23:21.920]   you can go quite a long way.
[02:23:21.920 --> 02:23:24.160]   - I think there's some truth to that.
[02:23:24.160 --> 02:23:26.840]   I mean, you look at the way humans approach
[02:23:26.840 --> 02:23:29.500]   a game like poker, they're not coming at it from scratch.
[02:23:29.500 --> 02:23:31.400]   They're coming at it with a huge amount
[02:23:31.400 --> 02:23:34.640]   of background knowledge about how humans work,
[02:23:34.640 --> 02:23:38.040]   how the world works, the idea of money.
[02:23:38.040 --> 02:23:41.400]   So they're able to leverage that kind of information
[02:23:41.400 --> 02:23:44.380]   to pick up the game faster.
[02:23:44.380 --> 02:23:45.920]   So it's not really a fair comparison
[02:23:45.920 --> 02:23:47.160]   to then compare it to an AI
[02:23:47.160 --> 02:23:48.260]   that's like learning from scratch.
[02:23:48.260 --> 02:23:49.640]   And maybe one of the ways that we address
[02:23:49.640 --> 02:23:53.560]   this sample complexity problem is by allowing AIs
[02:23:53.560 --> 02:23:55.040]   to leverage that general knowledge
[02:23:55.040 --> 02:23:56.740]   across a ton of different domains.
[02:23:56.740 --> 02:24:01.600]   - So, like I said, you did a lot of incredible work
[02:24:01.600 --> 02:24:04.720]   in the space of research and actually building systems.
[02:24:04.720 --> 02:24:07.720]   What advice would you give to, let's start with beginners.
[02:24:07.720 --> 02:24:09.860]   What advice would you give to beginners
[02:24:09.860 --> 02:24:11.840]   interested in machine learning?
[02:24:11.840 --> 02:24:13.600]   Just they're at the very start of their journey,
[02:24:13.600 --> 02:24:15.280]   they're in high school and college,
[02:24:15.280 --> 02:24:18.240]   thinking like this seems like a fascinating world.
[02:24:18.240 --> 02:24:19.840]   What advice would you give them?
[02:24:19.840 --> 02:24:24.800]   - I would say that there are a lot of people
[02:24:24.800 --> 02:24:27.640]   working on similar aspects of machine learning
[02:24:27.640 --> 02:24:31.040]   and to not be afraid to try something a bit different.
[02:24:31.040 --> 02:24:35.640]   My own path in AI is pretty atypical
[02:24:35.640 --> 02:24:37.520]   for a machine learning researcher today.
[02:24:37.520 --> 02:24:39.920]   I mean, I started out working on game theory
[02:24:39.920 --> 02:24:43.740]   and then shifting more towards reinforcement learning
[02:24:43.740 --> 02:24:44.880]   as time went on.
[02:24:44.880 --> 02:24:46.600]   And that actually had a lot of benefits, I think,
[02:24:46.600 --> 02:24:48.960]   because it allowed me to look at these problems
[02:24:48.960 --> 02:24:49.920]   in a very different way
[02:24:49.920 --> 02:24:54.120]   from the way a lot of machine learning researchers view it.
[02:24:54.120 --> 02:24:58.360]   And that comes with drawbacks in some respects.
[02:24:58.360 --> 02:25:00.560]   Like I think there's definitely aspects of machine learning
[02:25:00.560 --> 02:25:04.600]   where I'm weaker than most of the researchers out there,
[02:25:04.600 --> 02:25:06.800]   but I think that diversity of perspective,
[02:25:07.640 --> 02:25:10.160]   when I'm working with my teammates,
[02:25:10.160 --> 02:25:11.320]   there's something that I'm bringing to the table
[02:25:11.320 --> 02:25:12.880]   and there's something that they're bringing to the table.
[02:25:12.880 --> 02:25:13.920]   And that kind of collaboration
[02:25:13.920 --> 02:25:15.760]   becomes very fruitful for that reason.
[02:25:15.760 --> 02:25:18.640]   - So there could be problems like poker,
[02:25:18.640 --> 02:25:20.960]   like you've chosen diplomacy,
[02:25:20.960 --> 02:25:23.120]   there could be problems like that still out there
[02:25:23.120 --> 02:25:24.600]   that you can just tackle,
[02:25:24.600 --> 02:25:26.440]   even if it seems extremely difficult.
[02:25:26.440 --> 02:25:30.520]   - I think that there's a lot of challenges left.
[02:25:30.520 --> 02:25:34.560]   And I think having a diversity of viewpoints
[02:25:34.560 --> 02:25:36.480]   and backgrounds is really helpful
[02:25:36.480 --> 02:25:38.800]   for working together to figure out
[02:25:38.800 --> 02:25:40.360]   how to tackle those kinds of challenges.
[02:25:40.360 --> 02:25:42.360]   - So as a beginner, so that,
[02:25:42.360 --> 02:25:45.720]   I would say that's more for like a grad student
[02:25:45.720 --> 02:25:46.880]   where they already built up a base,
[02:25:46.880 --> 02:25:49.200]   like a complete beginner, what's a good journey.
[02:25:49.200 --> 02:25:51.360]   So for you that was doing some more
[02:25:51.360 --> 02:25:53.880]   in the math side of things, doing game theory,
[02:25:53.880 --> 02:25:56.760]   all that, so it's basically build up a foundation
[02:25:56.760 --> 02:25:59.440]   in something, so programming, mathematics,
[02:25:59.440 --> 02:26:03.400]   it could even be physics, but build that foundation.
[02:26:03.400 --> 02:26:05.400]   - Yeah, I would say build a strong foundation
[02:26:05.400 --> 02:26:07.400]   in math and computer science and statistics
[02:26:07.400 --> 02:26:08.280]   and these kinds of areas,
[02:26:08.280 --> 02:26:11.920]   but don't be afraid to try something that's different
[02:26:11.920 --> 02:26:13.080]   and learn something that's different
[02:26:13.080 --> 02:26:15.680]   from the thing that everybody else is doing
[02:26:15.680 --> 02:26:17.680]   to get into machine learning.
[02:26:17.680 --> 02:26:20.920]   There's value in having a different background
[02:26:20.920 --> 02:26:22.040]   than everybody else.
[02:26:22.040 --> 02:26:26.320]   Yeah, so, but certainly having a strong math background,
[02:26:26.320 --> 02:26:28.080]   especially in things like linear algebra
[02:26:28.080 --> 02:26:30.560]   and statistics and probability
[02:26:30.560 --> 02:26:33.320]   are incredibly helpful today for learning about
[02:26:33.320 --> 02:26:35.440]   and understanding machine learning.
[02:26:35.440 --> 02:26:37.000]   - Do you think one day we'll be able to,
[02:26:37.000 --> 02:26:39.680]   since you're taking steps from poker to diplomacy,
[02:26:39.680 --> 02:26:43.840]   one day we'll be able to figure out
[02:26:43.840 --> 02:26:45.280]   how to live life optimally?
[02:26:45.280 --> 02:26:49.360]   - Well, what is it, like in poker and diplomacy,
[02:26:49.360 --> 02:26:52.120]   you need a value function, you need to have a reward system.
[02:26:52.120 --> 02:26:55.080]   And so what does it mean to live a life that's optimal?
[02:26:55.080 --> 02:26:59.200]   - So, okay, so then you can exactly like lay down
[02:26:59.200 --> 02:27:02.240]   a reward function being like, I wanna be rich
[02:27:02.240 --> 02:27:07.240]   or I want to be in a happy relationship.
[02:27:07.240 --> 02:27:10.920]   And then you'll say, well, do X.
[02:27:10.920 --> 02:27:15.760]   - There's a lot of talk today about in AI safety circles
[02:27:15.760 --> 02:27:19.160]   about like misspecification of reward function.
[02:27:19.160 --> 02:27:22.920]   So you say like, okay, my objective is to be rich
[02:27:22.920 --> 02:27:24.600]   and maybe the AI tells you like, okay,
[02:27:24.600 --> 02:27:26.080]   well, if you wanna maximize the probability
[02:27:26.080 --> 02:27:27.440]   that you're rich, go rob a bank.
[02:27:27.440 --> 02:27:28.280]   - Sure.
[02:27:28.280 --> 02:27:30.560]   - And so you wanna, is that really what you want?
[02:27:30.560 --> 02:27:33.360]   Is your objective really to be rich at all costs
[02:27:33.360 --> 02:27:35.040]   or is it more nuanced than that?
[02:27:35.040 --> 02:27:38.060]   - So the unintended consequences, yeah.
[02:27:38.060 --> 02:27:44.480]   Yeah, so maybe life is more about defining
[02:27:44.480 --> 02:27:47.400]   the reward function that minimizes
[02:27:47.400 --> 02:27:50.760]   the unintended consequences than it is about
[02:27:50.760 --> 02:27:53.920]   the actual policy that gets you to the reward function.
[02:27:53.920 --> 02:27:57.360]   Maybe life is just about constantly updating
[02:27:57.360 --> 02:27:58.480]   the reward function.
[02:27:59.640 --> 02:28:01.080]   I think one of the challenges in life
[02:28:01.080 --> 02:28:04.280]   is figuring out exactly what that reward function is.
[02:28:04.280 --> 02:28:06.360]   Sometimes it's pretty hard to specify.
[02:28:06.360 --> 02:28:09.680]   The same way that trying to handcraft the optimal policy
[02:28:09.680 --> 02:28:12.280]   in a game like chess is really difficult.
[02:28:12.280 --> 02:28:15.920]   It's not so clear cut what the reward function is for life.
[02:28:15.920 --> 02:28:18.080]   - I think one day AI will figure it out.
[02:28:18.080 --> 02:28:21.960]   And I wonder what that would be.
[02:28:21.960 --> 02:28:24.880]   Until then, I just really appreciate
[02:28:24.880 --> 02:28:25.840]   the kind of work you're doing.
[02:28:25.840 --> 02:28:28.960]   And it's really fascinating taking a leap
[02:28:28.960 --> 02:28:33.960]   into a more and more real-world-like problem space
[02:28:33.960 --> 02:28:38.680]   and just achieving incredible results
[02:28:38.680 --> 02:28:40.240]   by applying reinforcement learning.
[02:28:40.240 --> 02:28:42.600]   Now, since I saw your work on poker,
[02:28:42.600 --> 02:28:44.360]   you've been a constant inspiration.
[02:28:44.360 --> 02:28:46.240]   It's an honor to get to finally talk to you
[02:28:46.240 --> 02:28:47.800]   and this is really fun.
[02:28:47.800 --> 02:28:49.400]   - Thanks for having me.
[02:28:49.400 --> 02:28:52.400]   - Thanks for listening to this conversation with Noah Brown.
[02:28:52.400 --> 02:28:54.660]   To support this podcast, please check out our sponsors
[02:28:54.660 --> 02:28:56.120]   in the description.
[02:28:56.120 --> 02:28:59.560]   And now, let me leave you with some words from Sun Tzu
[02:28:59.560 --> 02:29:01.720]   in "The Art of War."
[02:29:01.720 --> 02:29:04.960]   "The whole secret lies in confusing the enemy
[02:29:04.960 --> 02:29:08.120]   so that he cannot fathom our real intent."
[02:29:08.120 --> 02:29:12.440]   Thank you for listening and hope to see you next time.
[02:29:12.440 --> 02:29:15.020]   (upbeat music)
[02:29:15.020 --> 02:29:17.600]   (upbeat music)
[02:29:17.600 --> 02:29:27.600]   [BLANK_AUDIO]

