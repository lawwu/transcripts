
[00:00:00.520 --> 00:00:02.520]   Thank you very much for joining us.
[00:00:02.520 --> 00:00:04.520]   Thank you so much for joining us today.
[00:00:04.520 --> 00:00:10.160]   We've got one last very, very special event.
[00:00:10.160 --> 00:00:14.740]   So I'm excited to announce that we'll have a fireside chat with Adam D'Angelo, co-founder
[00:00:14.740 --> 00:00:16.660]   and CEO at Quora.
[00:00:16.660 --> 00:00:19.240]   And this will be the final event of the evening.
[00:00:19.240 --> 00:00:24.020]   Adam was the first CTO at Facebook and has been building Quora for the last 15 years.
[00:00:24.020 --> 00:00:29.120]   And Quora was a first mover in incorporating Gen.AI and building Po, which we'll talk a
[00:00:29.120 --> 00:00:30.620]   lot about.
[00:00:30.620 --> 00:00:36.000]   So please help me welcome Adam D'Angelo to the stage.
[00:00:36.000 --> 00:00:45.180]   Thank you for being here.
[00:00:45.180 --> 00:00:50.940]   So I thought the best way to kick this off was to ask Po for a question that I should ask
[00:00:50.940 --> 00:00:51.940]   you.
[00:00:51.940 --> 00:00:53.580]   And so this is what it came up with.
[00:00:53.580 --> 00:00:59.000]   Can you share the inspiration behind Po and how it fits into your vision for the future
[00:00:59.000 --> 00:01:00.500]   of knowledge sharing and AI?
[00:01:00.500 --> 00:01:01.500]   Sure.
[00:01:01.500 --> 00:01:07.560]   So first of all, thanks so much for having me.
[00:01:07.560 --> 00:01:16.500]   We started looking into AI at Quora, into large language models when we saw GPT-3.
[00:01:16.500 --> 00:01:23.380]   Q and we started experimenting with using it to generate answers on Quora.
[00:01:23.380 --> 00:01:34.000]   And we learned pretty quickly that the sort of the right paradigm for humans getting knowledge
[00:01:34.000 --> 00:01:40.740]   from large language models was going to be something that looked more like chat than like the sort
[00:01:40.740 --> 00:01:45.240]   human library orientation of the Quora product.
[00:01:45.240 --> 00:01:47.300]   Did you think that before chat GPT?
[00:01:47.300 --> 00:01:48.300]   Yeah.
[00:01:48.300 --> 00:01:49.300]   Yeah.
[00:01:49.300 --> 00:01:51.300]   This was before chat GPT.
[00:01:51.300 --> 00:01:57.240]   So through that experimentation, we decided that this was such a paradigm shift that it sort
[00:01:57.240 --> 00:01:59.240]   of called for a new kind of interface.
[00:01:59.240 --> 00:02:00.240]   interface.
[00:02:00.240 --> 00:02:04.740]   And so we set out to build a private chat product.
[00:02:04.740 --> 00:02:11.740]   And for us, because we weren't building the large language models ourselves, we thought a
[00:02:11.740 --> 00:02:15.740]   lot about, like, what is our role in this ecosystem that's going to emerge?
[00:02:15.740 --> 00:02:25.740]   And we decided to make a bet that there would be a wide diversity of both of language models and also
[00:02:25.740 --> 00:02:30.240]   of products built on top of those language models, which today we call agents.
[00:02:30.240 --> 00:02:36.240]   And we thought there would be a need for a sort of common interface to all of those.
[00:02:36.240 --> 00:02:43.140]   And so in the same way that the web browser was very important for the development of the
[00:02:43.140 --> 00:02:50.540]   internet, we thought that there was a need for a sort of a single interface that people
[00:02:50.540 --> 00:02:51.540]   could use.
[00:02:51.540 --> 00:02:57.860]   So before the web browser, if you were building an internet product, you had to build client
[00:02:57.860 --> 00:03:03.180]   software, server software, build your own protocol, and that was just a ton of work.
[00:03:03.180 --> 00:03:05.180]   And so you had a limited number of applications.
[00:03:05.180 --> 00:03:11.480]   You had things like SMTP for email, you had IRC, you had FTP.
[00:03:11.480 --> 00:03:18.100]   But anyone who wanted to make something like, you know, a hobbyist trying to make a new product,
[00:03:18.100 --> 00:03:22.940]   they could not do that until the web browser came along and just greatly reduced the barrier
[00:03:22.940 --> 00:03:27.180]   to building basically an internet product.
[00:03:27.180 --> 00:03:33.180]   And so after the web browser, you had things like home pages where anyone could have a home
[00:03:33.180 --> 00:03:40.220]   page, you had a lot of these like FAQ pages with all kinds of explosion of internet products
[00:03:40.220 --> 00:03:42.260]   that were enabled by the web browser.
[00:03:42.260 --> 00:03:49.740]   And so our hope with Poe is to have the same effect for AI where we make it so that anyone
[00:03:49.740 --> 00:03:56.440]   who is either training a model or building a product on top of a model can just plug into
[00:03:56.440 --> 00:04:07.400]   Poe and we provide iOS, Android, Windows, Mac clients, a web interface, we provide monetization,
[00:04:07.400 --> 00:04:14.420]   we provide history and sort of all this work that you need to get an AI product to millions
[00:04:14.420 --> 00:04:16.600]   of consumers around the world.
[00:04:16.600 --> 00:04:21.680]   So we just sort of have a single place to do all of that.
[00:04:21.680 --> 00:04:22.680]   Yeah.
[00:04:22.680 --> 00:04:26.180]   I think one of the interesting things that we'll probably dive into is I think you are one of
[00:04:26.180 --> 00:04:30.420]   the few kind of like consumer facing companies that we have on stage today.
[00:04:30.420 --> 00:04:36.760]   And so I'm curious, how are you seeing consumers use and interact with AI?
[00:04:36.760 --> 00:04:39.680]   What are the common kind of like patterns and use cases?
[00:04:39.680 --> 00:04:43.800]   It's really varied and I think it's interesting.
[00:04:43.800 --> 00:04:49.000]   One of the great things about large language models is that they are so general and they're
[00:04:49.000 --> 00:04:51.920]   just so capable of doing almost anything.
[00:04:51.920 --> 00:04:56.280]   And that makes it challenging to build on top of them, but it makes it just a great experience
[00:04:56.280 --> 00:04:57.280]   as a user.
[00:04:57.280 --> 00:04:58.600]   You can continually find new use cases.
[00:04:58.600 --> 00:05:03.260]   So we have a very wide variety of use cases on Poe.
[00:05:03.260 --> 00:05:14.020]   There's everything from writing assistance to question answering to things like role play,
[00:05:14.020 --> 00:05:19.820]   there's homework help, there's a lot of assistance in people doing their jobs.
[00:05:19.820 --> 00:05:23.520]   There's a lot of media, a lot of marketing usage, a lot of people creating media.
[00:05:23.520 --> 00:05:27.540]   We support image and video and audio models.
[00:05:27.540 --> 00:05:37.480]   And the central value proposition that we've landed on for consumers is Poe is one place
[00:05:37.480 --> 00:05:43.440]   where you can get all the different AI products under a single subscription.
[00:05:43.440 --> 00:05:49.520]   And that's very appealing for a certain set of people, especially true for people who are
[00:05:49.520 --> 00:05:54.020]   either developers building something on top of AI because they want to try a lot of different
[00:05:54.020 --> 00:05:55.020]   models.
[00:05:55.020 --> 00:05:56.020]   It's valuable for marketers.
[00:05:56.020 --> 00:06:00.500]   They often want to create things with many different market models.
[00:06:00.500 --> 00:06:07.060]   And it's just anyone who's sort of like an AI enthusiast, this is a pretty attractive value
[00:06:07.060 --> 00:06:08.060]   for them.
[00:06:08.060 --> 00:06:09.060]   Yeah.
[00:06:09.060 --> 00:06:12.480]   Going to Poe, you can scroll down the list of all the different models on there.
[00:06:12.480 --> 00:06:13.760]   And there's a lot.
[00:06:13.760 --> 00:06:17.020]   What are you seeing in terms of which ones are people using?
[00:06:17.020 --> 00:06:18.340]   Which ones are most popular?
[00:06:18.340 --> 00:06:19.340]   Why?
[00:06:19.340 --> 00:06:22.880]   You guys have a really unique vantage point into how people are using all these different
[00:06:22.880 --> 00:06:23.880]   models.
[00:06:23.880 --> 00:06:24.880]   Yeah.
[00:06:24.880 --> 00:06:25.880]   Yeah.
[00:06:25.880 --> 00:06:26.880]   So we actually just published this blog post.
[00:06:26.880 --> 00:06:30.880]   It's called the AI ecosystem report, which I would encourage people to check out if you're
[00:06:30.880 --> 00:06:31.880]   interested.
[00:06:31.880 --> 00:06:37.320]   If you just do a Google search for Poe blog, you'll find it as the first entry right now.
[00:06:37.320 --> 00:06:45.320]   I'd say the interesting story of the last few months is really the growth of reasoning models.
[00:06:45.320 --> 00:06:54.320]   So this includes 03, 04 mini, Gemini 2.5.
[00:06:54.320 --> 00:06:59.320]   It includes Claude, Sonnet 3.7.
[00:06:59.320 --> 00:07:02.320]   And there's the DeepSeek models.
[00:07:02.320 --> 00:07:06.320]   There's going to be a growing set of models in this category.
[00:07:06.320 --> 00:07:11.320]   But those have sort of really grown in usage recently.
[00:07:11.320 --> 00:07:13.320]   It's just incredibly powerful what they can do.
[00:07:13.320 --> 00:07:16.320]   Especially if you're doing something relating to writing code.
[00:07:16.320 --> 00:07:20.320]   The reasoning really adds a boost to your accuracy.
[00:07:20.320 --> 00:07:26.320]   I know Anthropic published some study on how people were using Claude and they found that
[00:07:26.320 --> 00:07:28.320]   coding was an abnormally high percentage.
[00:07:28.320 --> 00:07:31.320]   Do you think that's same of people who are using Poe?
[00:07:31.320 --> 00:07:35.320]   I think it's probably a little bit different.
[00:07:35.320 --> 00:07:40.320]   I'd say, you know, we have a biased group of users, which are the, you know, we do surveys
[00:07:40.320 --> 00:07:43.320]   of our users and ask why are you using Poe?
[00:07:43.320 --> 00:07:48.320]   And the top reason is that it's a place where you can get all the AI in one place.
[00:07:48.320 --> 00:07:53.320]   And so we sort of have a biased selection for the kind of people who want to use multiple
[00:07:53.320 --> 00:07:54.320]   models.
[00:07:54.320 --> 00:08:00.320]   I think in the same way, I would guess that Anthropic tends to get more developer-oriented
[00:08:00.320 --> 00:08:01.320]   users.
[00:08:01.320 --> 00:08:06.320]   We do have a pretty decent percentage of usage that is code related.
[00:08:06.320 --> 00:08:09.320]   But I'm sure it's not as high as Anthropic.
[00:08:09.320 --> 00:08:10.320]   Yeah.
[00:08:10.320 --> 00:08:12.320]   You mentioned a few different modalities.
[00:08:12.320 --> 00:08:14.320]   Voice, images.
[00:08:14.320 --> 00:08:17.320]   Which ones are you seeing kind of resonate?
[00:08:17.320 --> 00:08:21.320]   I know OpenAI, when they launched their kind of image support, all the Studio Ghibli images
[00:08:21.320 --> 00:08:23.320]   went crazy viral.
[00:08:23.320 --> 00:08:24.320]   Are you seeing similar things?
[00:08:24.320 --> 00:08:28.320]   Or, you know, we had someone talking about voice earlier and how powerful that was.
[00:08:28.320 --> 00:08:29.320]   Yeah.
[00:08:29.320 --> 00:08:33.320]   I mean, there's certainly a lot of excitement around the new image models.
[00:08:33.320 --> 00:08:38.320]   And we have the OpenAI model, the GPT-40 image gen.
[00:08:38.320 --> 00:08:42.320]   It's called GPT-Image-1 in their API.
[00:08:42.320 --> 00:08:45.320]   And that's what we have it under on Poe.
[00:08:45.320 --> 00:08:46.320]   It's a great model.
[00:08:46.320 --> 00:08:54.320]   I'd say overall, to answer your question, text models still dominate usage.
[00:08:54.320 --> 00:08:59.320]   And we've been thinking about why this is.
[00:08:59.320 --> 00:09:07.320]   I think that there's something where the image and video models are still not great.
[00:09:07.320 --> 00:09:11.320]   Like, you know, GPT-Image-1 was a huge step forward.
[00:09:11.320 --> 00:09:20.320]   But we're still not at the point where you can reliably get graphics that are sort of like useful
[00:09:20.320 --> 00:09:26.320]   to use in, say, a presentation relative to what a designer can do.
[00:09:26.320 --> 00:09:33.320]   Whereas when you, you know, with the text models, especially with reasoning, the quality is great.
[00:09:33.320 --> 00:09:34.320]   Right?
[00:09:34.320 --> 00:09:39.320]   It's much better than what people can create themselves a lot of the time.
[00:09:39.320 --> 00:09:45.320]   And so, like, just like the economic value there is a lot higher right now.
[00:09:45.320 --> 00:09:50.320]   I think this may change as the image and video models get stronger.
[00:09:50.320 --> 00:09:53.320]   And, yeah, we'll just have to see.
[00:09:53.320 --> 00:09:56.320]   But our view is we're just trying to provide a general interface.
[00:09:56.320 --> 00:10:01.320]   So wherever the market goes, we want to provide the best service there.
[00:10:01.320 --> 00:10:02.320]   Yeah.
[00:10:02.320 --> 00:10:07.320]   One question I have on that is, like, how much do people care about the model they're using?
[00:10:07.320 --> 00:10:09.320]   And why do they care?
[00:10:09.320 --> 00:10:12.320]   Like, I think, you know, if you mentioned some of the use cases, and it sounds like you have
[00:10:12.320 --> 00:10:17.320]   specific kind of, like, agents for, you know, doing different things.
[00:10:17.320 --> 00:10:19.320]   Do people care about the model under the hood?
[00:10:19.320 --> 00:10:22.320]   Like, this has always been something that I've thought about.
[00:10:22.320 --> 00:10:23.320]   Yeah.
[00:10:23.320 --> 00:10:24.320]   I mean, it varies.
[00:10:24.320 --> 00:10:29.320]   And I think this is back to the selection effect, where the kind of people use Poe tend to care.
[00:10:29.320 --> 00:10:37.320]   I would say that a lot of the use cases are around someone trying to be the best
[00:10:37.320 --> 00:10:40.320]   best they can be at some job they're trying to do.
[00:10:40.320 --> 00:10:46.320]   So you might have, like, a writer, someone doing creative writing, someone writing a book.
[00:10:46.320 --> 00:10:51.320]   They don't just want to, like, have one model they can consult.
[00:10:51.320 --> 00:11:01.320]   They want to try -- they're trying to find, like, the right wording for something, or generate ideas, or find inconsistencies in some passage.
[00:11:01.320 --> 00:11:04.320]   And they want to run that through a bunch of different models.
[00:11:04.320 --> 00:11:07.320]   And they'll be very particular about it.
[00:11:07.320 --> 00:11:16.320]   And we know, because, you know, occasionally a model will -- a provider of the model will go down, and we'll have downtime, and people are very unhappy.
[00:11:16.320 --> 00:11:19.320]   They don't just switch over to the other ones that are up.
[00:11:19.320 --> 00:11:20.320]   Interesting.
[00:11:20.320 --> 00:11:24.320]   Do you let them run it side by side to compare the outputs, or one at a time?
[00:11:24.320 --> 00:11:28.320]   We let people query multiple models in parallel.
[00:11:28.320 --> 00:11:32.320]   So there's a syntax on PoE where you say @, and then the model name.
[00:11:32.320 --> 00:11:37.320]   And you can stack many of these @ mentions in the front of a message.
[00:11:37.320 --> 00:11:41.320]   And then in parallel, we'll send it to all the bots.
[00:11:41.320 --> 00:11:46.320]   For the reasoning models, you mentioned they're better at things.
[00:11:46.320 --> 00:11:49.320]   Do you think part of the appeal is also the reasoning traces that you can see?
[00:11:49.320 --> 00:11:57.320]   I know when Deep Seek launched kind of their chat app, a lot of people really liked that, and they theorized that's why it was getting so much attention.
[00:11:57.320 --> 00:12:00.320]   Do you see that UX trick being interesting?
[00:12:00.320 --> 00:12:03.320]   Or is it really just -- they're just better, and people just care about the quality?
[00:12:03.320 --> 00:12:08.320]   You know, I don't know where the market is going to shake out on sharing the reasoning traces.
[00:12:08.320 --> 00:12:17.320]   I don't know how much -- certainly people like that with Deep Seek, but I don't know how much of that was novelty.
[00:12:17.320 --> 00:12:26.320]   And especially as these models get optimized and they get faster, it's just a lot that you don't care to say.
[00:12:26.320 --> 00:12:36.320]   And so it may be that we end up on showing summaries of the reasoning traces.
[00:12:36.320 --> 00:12:41.320]   Or maybe that they're hidden, maybe they're hidden behind a click, something like that.
[00:12:41.320 --> 00:12:45.320]   For now, I think people like seeing the reasoning when you can provide it.
[00:12:45.320 --> 00:12:49.320]   But I don't know if that's something we're going to want long-term.
[00:12:49.320 --> 00:12:50.320]   Yeah.
[00:12:50.320 --> 00:12:57.320]   One of the things that Poe allows for, besides just accessing the raw models, is accessing kind of like agents built in a variety of ways.
[00:12:57.320 --> 00:13:02.320]   So Asaf, who was talking earlier, who's the creator of GPT Researcher, they have a bot on Poe.
[00:13:02.320 --> 00:13:07.320]   How do people create these bots on Poe?
[00:13:07.320 --> 00:13:12.320]   So there's a few different options depending on your level of technical sophistication.
[00:13:12.320 --> 00:13:19.320]   So the simplest option is we allow prompting.
[00:13:19.320 --> 00:13:22.320]   And so you can just put in a prompt and a base model.
[00:13:22.320 --> 00:13:23.320]   And you choose a base model.
[00:13:23.320 --> 00:13:25.320]   So you can choose an existing model on Poe.
[00:13:25.320 --> 00:13:27.320]   And you add a prompt to it.
[00:13:27.320 --> 00:13:29.320]   And you create what's called a prompt bot.
[00:13:29.320 --> 00:13:35.320]   And that bot is just going to use the base model and follow the instructions in the prompt.
[00:13:35.320 --> 00:13:42.320]   And it's very simple, but this is actually pretty valuable for people because it saves them from having to repeatedly enter the prompt.
[00:13:42.320 --> 00:13:48.320]   It also means that you can, once you create a bot like this, you can share it with other people.
[00:13:48.320 --> 00:13:55.320]   We have a whole category in Poe where you can explore the bots that other people have shared and see the most popular ones and search for them.
[00:13:55.320 --> 00:14:03.320]   Do you see that people are mostly creating bots for themselves or mostly public ones that they're sharing?
[00:14:03.320 --> 00:14:07.320]   A lot of it is creating bots for yourself.
[00:14:07.320 --> 00:14:14.320]   But then most of the usage of bots is using bots created by other people.
[00:14:14.320 --> 00:14:20.320]   But I think most of the bots are not shared or are just used by the creator only.
[00:14:20.320 --> 00:14:23.320]   So those can both be true at the same time.
[00:14:23.320 --> 00:14:36.320]   And so if you share the bot, then you can monetize it and basically you'll get a cut of what people are paying Poe to access it.
[00:14:36.320 --> 00:14:44.320]   We've talked a little bit about what the agent engineer or agent builder profile looks like.
[00:14:44.320 --> 00:14:46.320]   Who's building these bots?
[00:14:46.320 --> 00:14:50.320]   Yeah, so the prompt bots, I mean, it's a real art.
[00:14:50.320 --> 00:14:51.320]   Prompting is a real art.
[00:14:51.320 --> 00:14:57.320]   And I think that, you know, the art has changed as the models get more powerful, especially with reasoning models.
[00:14:57.320 --> 00:15:00.320]   But I think it's still a real art.
[00:15:00.320 --> 00:15:08.320]   And the kind of people creating them, they tend to not be very technical, but they are people who can sort of like empathize with the model.
[00:15:08.320 --> 00:15:19.320]   And they're just very persistent in trying the model in many different cases and understanding what needs to go in the prompt.
[00:15:19.320 --> 00:15:23.320]   And then we have people who can create what we call server bots.
[00:15:23.320 --> 00:15:25.320]   And a server bot is pretty simple.
[00:15:25.320 --> 00:15:35.320]   You just give us the URL of your server and we will make an HTTP query over to your server every time the user sends a message to the bot.
[00:15:35.320 --> 00:15:45.320]   And whatever you return from that request, it will go into the Poe message as a response to the user.
[00:15:45.320 --> 00:15:57.320]   And so this is more useful for if you want to do something more complex than just a prompt, if you want to have an agent, if you want to query outside data sources.
[00:15:57.320 --> 00:16:01.320]   GPT researcher, for example, is a server bot.
[00:16:01.320 --> 00:16:08.320]   And this is more sophisticated developers who are creating these.
[00:16:08.320 --> 00:16:10.320]   And also AI model developers.
[00:16:10.320 --> 00:16:18.320]   So just to give you a good example, there's this really niche model, I think it's cool, called retro diffusion.
[00:16:18.320 --> 00:16:32.320]   And just two people, they train this model to generate pixel art for games and animations as well in the same kind of pixel art style.
[00:16:32.320 --> 00:16:37.320]   And the model is just super tuned for exactly that use case.
[00:16:37.320 --> 00:16:41.320]   And they set up a server bot for their model.
[00:16:41.320 --> 00:16:47.320]   That's something that you have to use a server bot for because they're doing inference with GPUs.
[00:16:47.320 --> 00:16:49.320]   And they're hosting all of that.
[00:16:49.320 --> 00:16:54.320]   Do you see that most people who create server bots are training and hosting their own model?
[00:16:54.320 --> 00:16:59.320]   Or is it more around the workflows and things like that?
[00:16:59.320 --> 00:17:02.320]   Originally, it was mostly people hosting their own models.
[00:17:02.320 --> 00:17:10.320]   Over time, it has become more of agents and more complex applications built on top.
[00:17:10.320 --> 00:17:17.320]   But we have companies like Together, Fireworks.
[00:17:17.320 --> 00:17:19.320]   There's a whole category, FAL.
[00:17:19.320 --> 00:17:23.320]   These companies that host open source models, they generally set up server bots.
[00:17:23.320 --> 00:17:27.320]   And for these companies, for everyone, it's sort of a way to...
[00:17:27.320 --> 00:17:33.320]   If your company is not set up to reach consumers all over the world,
[00:17:33.320 --> 00:17:37.320]   like if you're a developer-oriented company or if you're just a small team,
[00:17:37.320 --> 00:17:41.320]   then setting up a server bot is a way to reach a much bigger audience
[00:17:41.320 --> 00:17:48.320]   and just basically generate extra money and get feedback from a broad audience.
[00:17:48.320 --> 00:17:51.320]   How does the monetization work?
[00:17:51.320 --> 00:17:54.320]   So it's totally flexible.
[00:17:54.320 --> 00:17:58.320]   The creators can choose what they want to charge users.
[00:17:58.320 --> 00:18:02.320]   And we turn whatever the creators want to charge in dollars,
[00:18:02.320 --> 00:18:06.320]   we turn that into a number of points that users get.
[00:18:06.320 --> 00:18:11.320]   And so users have a limited number of points depending on how much they're spending on Poe per month.
[00:18:11.320 --> 00:18:16.320]   And we're basically just passing through the cost from the creator to the user.
[00:18:16.320 --> 00:18:19.320]   But we have creators that are making a lot of money.
[00:18:19.320 --> 00:18:24.320]   I mean, there's companies making in the millions of dollars per year through...
[00:18:24.320 --> 00:18:26.320]   In the millions of dollars per year?
[00:18:26.320 --> 00:18:27.320]   Yeah.
[00:18:27.320 --> 00:18:31.320]   And there's individual people making in the hundreds of thousands
[00:18:31.320 --> 00:18:34.320]   and tens of thousands of dollars per year.
[00:18:34.320 --> 00:18:38.320]   So there's a real economy developing here.
[00:18:38.320 --> 00:18:41.320]   What are these prompt bots? Are these server bots?
[00:18:41.320 --> 00:18:46.320]   What are they doing that's providing that much value?
[00:18:46.320 --> 00:18:47.320]   I mean, it's a mix.
[00:18:47.320 --> 00:18:51.320]   Sometimes it's training their own models.
[00:18:51.320 --> 00:18:54.320]   Sometimes it's hosting open source models.
[00:18:54.320 --> 00:19:01.320]   Some companies are very good at like optimizing the cost of inference of certain models.
[00:19:01.320 --> 00:19:03.320]   Sometimes it's agents.
[00:19:03.320 --> 00:19:07.320]   Sometimes it's prompt bots where it's just like some people are really good at prompting.
[00:19:07.320 --> 00:19:09.320]   Sometimes people are really good at marketing.
[00:19:09.320 --> 00:19:13.320]   And that's another component of particularly for prompt bots.
[00:19:13.320 --> 00:19:20.320]   Just like building enthusiasm from internet users about using your bot.
[00:19:20.320 --> 00:19:23.320]   But this is something that like, you know, it's an opportunity.
[00:19:23.320 --> 00:19:28.320]   If you're good at marketing and you're good at prompting, you can create these bots.
[00:19:28.320 --> 00:19:32.320]   And, you know, we have people making a living off this.
[00:19:32.320 --> 00:19:34.320]   You mentioned agents.
[00:19:34.320 --> 00:19:38.320]   Are these bots taking actions for people at the moment?
[00:19:38.320 --> 00:19:44.320]   Or are they mostly kind of like just still conversational and, yeah, read-only, not write?
[00:19:44.320 --> 00:19:46.320]   They're mostly read-only.
[00:19:46.320 --> 00:19:53.320]   They're -- we haven't -- one of the places we'd love to go as a platform is to enable more of these like real-world actions.
[00:19:53.320 --> 00:20:05.320]   We're not there yet, but so right now it's basically bots that are effectively don't have side effects is a way to think about it.
[00:20:05.320 --> 00:20:12.320]   So the whole action of the bot is to create some artifact that it then returns to you in the chat.
[00:20:12.320 --> 00:20:15.320]   We've got -- we've got hundreds of developers here.
[00:20:15.320 --> 00:20:21.320]   If you were a developer, what type of bot would you build or what would you recommend people explore?
[00:20:21.320 --> 00:20:24.320]   Where do you think there's an unmet need?
[00:20:24.320 --> 00:20:29.320]   I think the like agents category is probably where there's the most opportunity.
[00:20:29.320 --> 00:20:32.320]   Obviously, if you can train models, you should do that.
[00:20:32.320 --> 00:20:39.320]   But I think the like building on top of the models, building things that are more sophisticated than just a simple prompt,
[00:20:39.320 --> 00:20:44.320]   but not as sophisticated as like training a new model or fine-tuning.
[00:20:44.320 --> 00:20:47.320]   There's just -- there's like so much to be built.
[00:20:47.320 --> 00:20:51.320]   You know, you can -- can and should build it with Langchain, but it's --
[00:20:51.320 --> 00:20:59.320]   there's just this like incredible space that has opened up to make agents that are useful to people.
[00:20:59.320 --> 00:21:06.320]   And yeah, I would experiment with that and see where you get traction.
[00:21:06.320 --> 00:21:13.320]   And you might have said this earlier, but agents with side effects, will those be coming soon?
[00:21:13.320 --> 00:21:15.320]   How are you thinking about that in general?
[00:21:15.320 --> 00:21:18.320]   Because I imagine there's a bunch of risks obviously associated with that.
[00:21:18.320 --> 00:21:19.320]   Yeah.
[00:21:19.320 --> 00:21:20.320]   No, we'd love to support that.
[00:21:20.320 --> 00:21:27.320]   You know, and I think things like MCP have made it easier for there to be like standards around how to do that.
[00:21:27.320 --> 00:21:31.320]   We'd -- we will get to that at some point.
[00:21:31.320 --> 00:21:34.320]   Right now we have a lot of -- we have a lot of competing priorities.
[00:21:34.320 --> 00:21:44.320]   So we've had to focus and we're hard at work on some things that will unblock the way for us to provide a great experience around that.
[00:21:44.320 --> 00:21:53.320]   So, zooming -- zooming out a little bit, you made a -- you made a reference earlier to kind of like the early days of the Internet and kind of like things there.
[00:21:53.320 --> 00:21:57.320]   You also built an Internet company, Quora.
[00:21:57.320 --> 00:21:59.320]   How do these two things compare?
[00:21:59.320 --> 00:22:00.320]   What's the -- what are the differences?
[00:22:00.320 --> 00:22:01.320]   What are the similarities?
[00:22:01.320 --> 00:22:07.320]   There's -- I mean, a lot is similar.
[00:22:07.320 --> 00:22:16.320]   But I think the -- the big difference is sort of just like the pace that the environment is changing at.
[00:22:16.320 --> 00:22:17.320]   So --
[00:22:17.320 --> 00:22:20.320]   Largely because of the foundation models and --
[00:22:20.320 --> 00:22:22.320]   Yeah, because of the AI and just the change.
[00:22:22.320 --> 00:22:29.320]   You know, just every -- every month there are new models, there's new modalities, there's agents, there's MCP, there's tools.
[00:22:29.320 --> 00:22:37.320]   There's just the -- the sort of like the environment that we exist within is changing incredibly quickly.
[00:22:37.320 --> 00:22:54.320]   Whereas with Quora, when we started in -- we started in 2009 and launched the first version in 2010, it was -- I'd say it took maybe like every few years there was a significant shift.
[00:22:54.320 --> 00:23:09.320]   You know, there was a big shift with mobile and there was a sort of shift with -- with creators becoming more -- more of like a constituency among like the Internet.
[00:23:09.320 --> 00:23:18.320]   But it was -- you know, we had to sort of grow with the Internet, with internationalization as different -- different countries started to -- to grow online.
[00:23:18.320 --> 00:23:25.320]   But it was -- the -- the piece of change of the environment was -- the environment was pretty stable.
[00:23:25.320 --> 00:23:35.320]   And that allowed us to sort of focus internally and -- and sort of invest a lot in internal abstractions and -- and just sort of like polishing things.
[00:23:35.320 --> 00:23:41.320]   And -- and -- and there's a certain way of doing things that works when -- when you're in a stable environment.
[00:23:41.320 --> 00:23:48.320]   In this environment, things change so fast that we just need to sort of constantly adapt and react.
[00:23:48.320 --> 00:23:52.320]   Like it doesn't make sense to plan years ahead in this environment.
[00:23:52.320 --> 00:23:54.320]   How far ahead do you plan?
[00:23:54.320 --> 00:24:00.320]   We -- I'd say our plans go out about two months right now.
[00:24:00.320 --> 00:24:09.320]   That's -- that's -- that's how -- that's how far we have -- we have -- we have some very -- hopefully you'll -- you'll all see some -- some great stuff launch in the next few months.
[00:24:09.320 --> 00:24:13.320]   And then we will evaluate -- I mean, we could -- we could plan further than that.
[00:24:13.320 --> 00:24:15.320]   But I just think, you know, we're going to get two months from that.
[00:24:15.320 --> 00:24:17.320]   We're going to launch the stuff we're working on.
[00:24:17.320 --> 00:24:23.320]   And then we're going to -- it's going to -- the things you would think of to do then are different than what you would think of to do now.
[00:24:23.320 --> 00:24:27.320]   That -- that -- that may make answering my -- my final question hard.
[00:24:27.320 --> 00:24:29.320]   But what do you -- what do you see coming down the pipeline?
[00:24:29.320 --> 00:24:31.320]   What are you most excited about?
[00:24:31.320 --> 00:24:33.320]   Where do you think some of the space is going?
[00:24:33.320 --> 00:24:36.320]   Are there things at Poe that we should be keeping an eye out for?
[00:24:36.320 --> 00:24:44.320]   You know, I -- I think the main thing is just the power of the models is going to keep increasing.
[00:24:44.320 --> 00:24:46.320]   The -- the pace this year has been incredible, right?
[00:24:46.320 --> 00:24:49.320]   It was just -- as the reasoning models have -- have gotten stronger and stronger.
[00:24:49.320 --> 00:24:54.320]   I'm particularly excited about code generation applications.
[00:24:54.320 --> 00:25:10.320]   So we have -- we have a tool within Poe we call App Creator that uses -- uses LLMs underneath to -- to help you generate interfaces and -- interfaces for bots on Poe.
[00:25:10.320 --> 00:25:14.320]   So if you want, like, a graphic interface, a web interface.
[00:25:14.320 --> 00:25:28.320]   And it's -- it's good now, but it's -- I -- I can just see where that is going to get to in the next six months as the code generation abilities of -- of all these models keep -- keep growing.
[00:25:28.320 --> 00:25:37.320]   It's -- I mean, it's -- it's -- it's just -- the amount of software that we're going to be able to -- to generate and the ecosystem that's going to emerge around that is -- it's -- it's -- it's going to be incredible.
[00:25:37.320 --> 00:25:39.320]   So I'm -- I'm just really excited for -- for that.
[00:25:39.320 --> 00:25:41.320]   That's a great note to end it on.
[00:25:41.320 --> 00:25:42.320]   All right.
[00:25:42.320 --> 00:25:46.320]   Let's give Adam a big round of applause.
[00:25:46.320 --> 00:25:48.320]   Thank you for coming out.
[00:25:48.320 --> 00:25:53.320]   Thank you.
[00:25:53.320 --> 00:25:57.320]   Thank you guys all for being a big part of Interrupt.
[00:25:57.320 --> 00:25:58.320]   The first time we did it.
[00:25:58.320 --> 00:26:02.320]   It's been a really fun day for myself and -- and hopefully for all of you.
[00:26:02.320 --> 00:26:06.320]   And hopefully it inspires you to build lots of agents.
[00:26:06.320 --> 00:26:11.320]   I want to -- I want to give a special thank to our presenting sponsor, Cisco Experience.
[00:26:11.320 --> 00:26:15.320]   Their partnership and support, not only today, but they were also a big part of the workshop yesterday.
[00:26:15.320 --> 00:26:21.320]   Has been fantastic and has -- has given a lot to think about and -- and -- and learn from.
[00:26:21.320 --> 00:26:23.320]   I also want to thank our amazing speakers.
[00:26:23.320 --> 00:26:24.320]   We had a lot of them.
[00:26:24.320 --> 00:26:29.320]   If you missed any of their talks, we'll have recordings up right afterwards on our Interrupt website.
[00:26:29.320 --> 00:26:36.320]   And -- and I also want to thank the Langchain community champions and Slack contributors.
[00:26:36.320 --> 00:26:41.320]   You may have seen them wandering around, as well as the Langchain ambassadors.
[00:26:41.320 --> 00:26:46.320]   These are a big part of the people that make Langchain what Langchain is.
[00:26:46.320 --> 00:26:48.320]   And -- and there's a lot of them.
[00:26:48.320 --> 00:26:51.320]   And many of them have flown from across the world to be here today.
[00:26:51.320 --> 00:26:56.320]   So if you see them around, we'll have kind of like a happy hour for the next two hours.
[00:26:56.320 --> 00:26:57.320]   Make sure to go up to them.
[00:26:57.320 --> 00:27:00.320]   A lot of them have done incredible things in the community.
[00:27:00.320 --> 00:27:07.320]   And on that note, as we wrap up, I'd like to invite you all to join us for our closing reception sponsored by Data Stacks.
[00:27:07.320 --> 00:27:12.320]   It's a good opportunity to continue any conversations that you already started or start new ones.
[00:27:12.320 --> 00:27:14.320]   And with that, thank you guys for being a part.
[00:27:14.320 --> 00:27:15.320]   Thank you for coming.
[00:27:15.320 --> 00:27:16.320]   And hope you enjoyed.
[00:27:16.320 --> 00:27:17.320]   Thank you.
[00:27:17.320 --> 00:27:17.320]   Thank you.
[00:27:17.320 --> 00:27:18.320]   Thank you.
[00:27:18.320 --> 00:27:19.320]   Thank you.
[00:27:19.320 --> 00:27:20.320]   Thank you.
[00:27:20.320 --> 00:27:21.320]   Thank you.
[00:27:21.320 --> 00:27:22.320]   Thank you.
[00:27:22.320 --> 00:27:52.300]   Thank you.

