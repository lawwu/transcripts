
[00:00:00.000 --> 00:00:03.280]   The following is a conversation with Alex Garland,
[00:00:03.280 --> 00:00:06.280]   writer and director of many imaginative
[00:00:06.280 --> 00:00:07.960]   and philosophical films,
[00:00:07.960 --> 00:00:10.960]   from the dreamlike exploration of human self-destruction
[00:00:10.960 --> 00:00:12.640]   in the movie "Annihilation"
[00:00:12.640 --> 00:00:16.360]   to the deep questions of consciousness and intelligence
[00:00:16.360 --> 00:00:18.520]   raised in the movie "Ex Machina,"
[00:00:18.520 --> 00:00:21.000]   which to me is one of the greatest movies
[00:00:21.000 --> 00:00:23.800]   in artificial intelligence ever made.
[00:00:23.800 --> 00:00:25.720]   I'm releasing this podcast to coincide
[00:00:25.720 --> 00:00:28.520]   with the release of his new series called "Devs"
[00:00:28.520 --> 00:00:32.480]   that will premiere this Thursday, March 5th on Hulu
[00:00:32.480 --> 00:00:34.680]   as part of FX on Hulu.
[00:00:34.680 --> 00:00:39.240]   It explores many of the themes this very podcast is about,
[00:00:39.240 --> 00:00:43.400]   from quantum mechanics to artificial life to simulation
[00:00:43.400 --> 00:00:46.440]   to the modern nature of power in the tech world.
[00:00:46.440 --> 00:00:50.280]   I got a chance to watch a preview and loved it.
[00:00:50.280 --> 00:00:52.000]   The acting is great.
[00:00:52.000 --> 00:00:55.320]   Nick Offerman especially is incredible in it.
[00:00:55.320 --> 00:00:57.960]   The cinematography is beautiful
[00:00:57.960 --> 00:01:00.600]   and the philosophical and scientific ideas explored
[00:01:00.600 --> 00:01:02.000]   are profound.
[00:01:02.000 --> 00:01:04.400]   And for me as an engineer and scientist,
[00:01:04.400 --> 00:01:07.200]   were just fun to see brought to life.
[00:01:07.200 --> 00:01:08.960]   For example, if you watch the trailer
[00:01:08.960 --> 00:01:10.480]   for the series carefully,
[00:01:10.480 --> 00:01:13.080]   you'll see there's a programmer with a Russian accent
[00:01:13.080 --> 00:01:16.080]   looking at a screen with Python-like code on it
[00:01:16.080 --> 00:01:18.080]   that appears to be using a library
[00:01:18.080 --> 00:01:20.200]   that interfaces with a quantum computer.
[00:01:20.200 --> 00:01:22.920]   This attention to technical detail
[00:01:22.920 --> 00:01:25.480]   on several levels is impressive.
[00:01:25.480 --> 00:01:27.360]   And one of the reasons I'm a big fan
[00:01:27.360 --> 00:01:30.000]   of how Alex weaves science and philosophy together
[00:01:30.000 --> 00:01:30.840]   in his work.
[00:01:30.840 --> 00:01:35.000]   Meeting Alex for me was unlikely,
[00:01:35.000 --> 00:01:36.640]   but it was life-changing
[00:01:36.640 --> 00:01:40.120]   in ways I may only be able to articulate in a few years.
[00:01:40.120 --> 00:01:43.560]   Just as meeting Spotmania Boston Dynamics
[00:01:43.560 --> 00:01:47.760]   for the first time planted a seed of an idea in my mind,
[00:01:47.760 --> 00:01:50.120]   so did meeting Alex Garland.
[00:01:50.120 --> 00:01:52.760]   He's humble, curious, intelligent,
[00:01:52.760 --> 00:01:55.240]   and to me, an inspiration.
[00:01:55.240 --> 00:01:57.920]   Plus, he's just really a fun person to talk with
[00:01:57.920 --> 00:02:01.280]   about the biggest possible questions in our universe.
[00:02:01.280 --> 00:02:05.040]   This is the Artificial Intelligence Podcast.
[00:02:05.040 --> 00:02:07.240]   If you enjoy it, subscribe on YouTube,
[00:02:07.240 --> 00:02:09.080]   give it five stars on Apple Podcast,
[00:02:09.080 --> 00:02:10.480]   support it on Patreon,
[00:02:10.480 --> 00:02:12.520]   or simply connect with me on Twitter
[00:02:12.520 --> 00:02:17.000]   at Lex Friedman, spelled F-R-I-D-M-A-N.
[00:02:17.000 --> 00:02:19.560]   As usual, I'll do one or two minutes of ads now
[00:02:19.560 --> 00:02:21.000]   and never any ads in the middle
[00:02:21.000 --> 00:02:23.280]   that can break the flow of the conversation.
[00:02:23.280 --> 00:02:24.720]   I hope that works for you
[00:02:24.720 --> 00:02:27.440]   and doesn't hurt the listening experience.
[00:02:27.440 --> 00:02:29.920]   This show is presented by Cash App,
[00:02:29.920 --> 00:02:32.320]   the number one finance app in the App Store.
[00:02:32.320 --> 00:02:35.800]   When you get it, use code LEXPODCAST.
[00:02:35.800 --> 00:02:37.960]   Cash App lets you send money to friends,
[00:02:37.960 --> 00:02:40.320]   buy Bitcoin, and invest in the stock market
[00:02:40.320 --> 00:02:41.360]   with as little as $1.
[00:02:41.360 --> 00:02:45.160]   Since Cash App allows you to buy Bitcoin,
[00:02:45.160 --> 00:02:47.080]   let me mention that cryptocurrency
[00:02:47.080 --> 00:02:50.320]   in the context of the history of money is fascinating.
[00:02:50.320 --> 00:02:52.680]   I recommend "A Scent of Money"
[00:02:52.680 --> 00:02:54.920]   as a great book on this history.
[00:02:54.920 --> 00:02:59.800]   Debits and credits on ledgers started 30,000 years ago.
[00:02:59.800 --> 00:03:03.840]   The US dollar was created about 200 years ago.
[00:03:03.840 --> 00:03:07.360]   And Bitcoin, the first decentralized cryptocurrency,
[00:03:07.360 --> 00:03:09.920]   was released just over 10 years ago.
[00:03:09.920 --> 00:03:11.360]   So given that history,
[00:03:11.360 --> 00:03:12.920]   cryptocurrency is still very much
[00:03:12.920 --> 00:03:14.880]   in its early days of development,
[00:03:14.880 --> 00:03:17.800]   but it still is aiming to, and just might,
[00:03:17.800 --> 00:03:20.640]   redefine the nature of money.
[00:03:20.640 --> 00:03:23.040]   So again, if you get Cash App from the App Store
[00:03:23.040 --> 00:03:26.120]   or Google Play, and use code LEXPODCAST,
[00:03:26.120 --> 00:03:30.200]   you'll get $10, and Cash App will also donate $10 to FIRST,
[00:03:30.200 --> 00:03:31.880]   one of my favorite organizations
[00:03:31.880 --> 00:03:34.960]   that is helping advance robotics and STEM education
[00:03:34.960 --> 00:03:37.520]   for young people around the world.
[00:03:37.520 --> 00:03:41.520]   And now, here's my conversation with Alex Garland.
[00:03:41.520 --> 00:03:45.160]   You described the world inside the shimmer
[00:03:45.160 --> 00:03:47.160]   in the movie "Annihilation" as dreamlike,
[00:03:47.160 --> 00:03:48.760]   meaning that it's internally consistent,
[00:03:48.760 --> 00:03:50.720]   but detached from reality.
[00:03:50.720 --> 00:03:52.360]   That leads me to ask,
[00:03:52.360 --> 00:03:56.240]   do you think, a philosophical question, I apologize,
[00:03:56.240 --> 00:03:58.600]   do you think we might be living in a dream
[00:03:58.600 --> 00:04:02.280]   or in a simulation, like the kind that the shimmer creates?
[00:04:02.280 --> 00:04:07.040]   We, human beings, here today.
[00:04:07.040 --> 00:04:08.200]   - Yeah.
[00:04:08.200 --> 00:04:11.600]   I wanna sort of separate that out into two things.
[00:04:11.600 --> 00:04:14.640]   Yes, I think we're living in a dream of sorts.
[00:04:14.640 --> 00:04:18.400]   No, I don't think we're living in a simulation.
[00:04:18.400 --> 00:04:20.760]   I think we're living on a planet
[00:04:20.760 --> 00:04:23.720]   with a very thin layer of atmosphere,
[00:04:23.720 --> 00:04:27.640]   and the planet is in a very large space,
[00:04:27.640 --> 00:04:29.920]   and the space is full of other planets and stars
[00:04:29.920 --> 00:04:31.280]   and quasars and stuff like that.
[00:04:31.280 --> 00:04:35.600]   And I don't think those physical objects,
[00:04:35.600 --> 00:04:38.760]   I don't think the matter in that universe is simulated.
[00:04:38.760 --> 00:04:40.520]   I think it's there.
[00:04:40.520 --> 00:04:43.480]   We are definitely, or,
[00:04:43.480 --> 00:04:46.240]   it's a whole problem with saying definitely,
[00:04:46.360 --> 00:04:50.200]   but in my opinion, I'll just go back to that.
[00:04:50.200 --> 00:04:53.080]   I think it seems very like we're living in a dream state.
[00:04:53.080 --> 00:04:54.320]   I'm pretty sure we are.
[00:04:54.320 --> 00:04:56.480]   And I think that's just to do with the nature
[00:04:56.480 --> 00:04:58.000]   of how we experience the world.
[00:04:58.000 --> 00:05:00.240]   We experience it in a subjective way.
[00:05:00.240 --> 00:05:04.400]   And the thing I've learned most
[00:05:04.400 --> 00:05:06.240]   as I've got older in some respects
[00:05:06.240 --> 00:05:10.800]   is the degree to which reality is counterintuitive,
[00:05:10.800 --> 00:05:13.640]   and that the things that are presented to us as objective
[00:05:13.640 --> 00:05:15.120]   turn out not to be objective,
[00:05:15.120 --> 00:05:17.320]   and quantum mechanics is full of that kind of thing,
[00:05:17.320 --> 00:05:18.960]   but actually just day-to-day life
[00:05:18.960 --> 00:05:20.840]   is full of that kind of thing as well.
[00:05:20.840 --> 00:05:25.840]   So my understanding of the way the brain works
[00:05:25.840 --> 00:05:30.760]   is you get some information to hit your optic nerve,
[00:05:30.760 --> 00:05:32.760]   and then your brain makes its best guess
[00:05:32.760 --> 00:05:36.320]   about what it's seeing or what it's saying it's seeing.
[00:05:36.320 --> 00:05:39.240]   It may or may not be an accurate best guess.
[00:05:39.240 --> 00:05:41.320]   It might be an inaccurate best guess.
[00:05:41.320 --> 00:05:45.440]   And that gap, the best guess gap,
[00:05:45.440 --> 00:05:48.960]   means that we are essentially living in a subjective state,
[00:05:48.960 --> 00:05:50.960]   which means that we're in a dream state.
[00:05:50.960 --> 00:05:54.000]   So I think you could enlarge on the dream state
[00:05:54.000 --> 00:05:55.440]   in all sorts of ways.
[00:05:55.440 --> 00:05:58.280]   So yes, dream state, no simulation
[00:05:58.280 --> 00:06:00.440]   would be where I'd come down.
[00:06:00.440 --> 00:06:04.000]   - Going further, deeper into that direction,
[00:06:04.000 --> 00:06:08.560]   you've also described that world as psychedelia.
[00:06:08.560 --> 00:06:11.460]   So on that topic, I'm curious about that world.
[00:06:11.460 --> 00:06:13.320]   On the topic of psychedelic drugs,
[00:06:13.320 --> 00:06:15.920]   do you see those kinds of chemicals
[00:06:15.920 --> 00:06:18.280]   that modify our perception
[00:06:18.280 --> 00:06:22.000]   as a distortion of our perception of reality
[00:06:22.000 --> 00:06:25.860]   or a window into another reality?
[00:06:25.860 --> 00:06:27.060]   - No, I think what I'd be saying
[00:06:27.060 --> 00:06:29.140]   is that we live in a distorted reality,
[00:06:29.140 --> 00:06:30.520]   and then those kinds of drugs
[00:06:30.520 --> 00:06:32.520]   give us a different kind of distorted--
[00:06:32.520 --> 00:06:33.360]   - Different perspective.
[00:06:33.360 --> 00:06:35.920]   - Yeah, exactly, they just give an alternate distortion.
[00:06:35.920 --> 00:06:37.560]   And I think that what they really do
[00:06:37.560 --> 00:06:41.040]   is they give a distorted perception,
[00:06:41.040 --> 00:06:45.560]   which is a little bit more allied to daydreams
[00:06:45.560 --> 00:06:47.320]   or unconscious interests.
[00:06:47.320 --> 00:06:49.080]   So if for some reason
[00:06:49.080 --> 00:06:51.800]   you're feeling unconsciously anxious at that moment
[00:06:51.800 --> 00:06:53.200]   and you take a psychedelic drug,
[00:06:53.200 --> 00:06:56.560]   you'll have a more pronounced unpleasant experience.
[00:06:56.560 --> 00:06:59.040]   And if you're feeling very calm or happy,
[00:06:59.040 --> 00:07:00.240]   you might have a good time.
[00:07:00.240 --> 00:07:04.800]   But yeah, so if I'm saying we're starting from a premise,
[00:07:04.800 --> 00:07:06.940]   our starting point is we were already
[00:07:06.940 --> 00:07:09.480]   in the slightly psychedelic state,
[00:07:09.480 --> 00:07:13.440]   what those drugs do is help you go further down an avenue
[00:07:13.440 --> 00:07:16.240]   or maybe a slightly different avenue, but that's all.
[00:07:16.240 --> 00:07:19.080]   - So in that movie, "Annihilation,"
[00:07:19.080 --> 00:07:24.960]   the shimmer, this alternate dreamlike state
[00:07:24.960 --> 00:07:29.420]   is created by, I believe, perhaps, an alien entity.
[00:07:29.420 --> 00:07:32.100]   Of course, everything's up to interpretation, right?
[00:07:32.100 --> 00:07:36.180]   But do you think there's, in our world, in our universe,
[00:07:36.180 --> 00:07:39.080]   do you think there's intelligent life out there?
[00:07:39.080 --> 00:07:42.500]   And if so, how different is it from us humans?
[00:07:42.500 --> 00:07:47.200]   - Well, one of the things I was trying to do in "Annihilation"
[00:07:47.200 --> 00:07:51.740]   was to offer up a form of alien life
[00:07:51.740 --> 00:07:53.380]   that was actually alien.
[00:07:53.380 --> 00:07:58.340]   Because it would often seem to me
[00:07:58.340 --> 00:08:01.100]   that in the way we would represent aliens
[00:08:01.100 --> 00:08:04.340]   in books or cinema or television
[00:08:04.340 --> 00:08:08.260]   or any one of the sort of storytelling mediums
[00:08:08.260 --> 00:08:11.900]   is we would always give them very human-like qualities.
[00:08:11.900 --> 00:08:14.860]   So they wanted to teach us about galactic federations
[00:08:14.860 --> 00:08:17.740]   or they wanted to eat us or they wanted our resources
[00:08:17.740 --> 00:08:20.180]   like our water or they want to enslave us
[00:08:20.180 --> 00:08:21.360]   or whatever it happens to be.
[00:08:21.360 --> 00:08:25.420]   But all of these are incredibly human-like motivations.
[00:08:25.420 --> 00:08:30.420]   And I was interested in the idea of an alien
[00:08:30.900 --> 00:08:34.300]   that was not in any way like us.
[00:08:34.300 --> 00:08:36.220]   It didn't share.
[00:08:36.220 --> 00:08:38.820]   Maybe it had a completely different clock speed.
[00:08:38.820 --> 00:08:42.140]   Maybe its way, so we're talking about,
[00:08:42.140 --> 00:08:43.180]   we're looking at each other,
[00:08:43.180 --> 00:08:46.860]   we're getting information, light hits our optic nerve,
[00:08:46.860 --> 00:08:49.060]   our brain makes the best guess of what we're doing.
[00:08:49.060 --> 00:08:49.900]   Sometimes it's right,
[00:08:49.900 --> 00:08:51.820]   something, you know, the thing we were talking about before.
[00:08:51.820 --> 00:08:54.980]   What if this alien doesn't have an optic nerve?
[00:08:54.980 --> 00:08:57.700]   Maybe its way of encountering the space it's in
[00:08:57.700 --> 00:08:59.260]   is wholly different.
[00:08:59.260 --> 00:09:01.820]   Maybe it has a different relationship with gravity.
[00:09:01.820 --> 00:09:04.060]   - The basic laws of physics it operates under
[00:09:04.060 --> 00:09:05.820]   might be fundamentally different.
[00:09:05.820 --> 00:09:07.820]   It could be a different time scale and so on.
[00:09:07.820 --> 00:09:10.300]   - Yeah, or it could be the same laws,
[00:09:10.300 --> 00:09:12.700]   it could be the same underlying laws of physics.
[00:09:12.700 --> 00:09:16.260]   You know, it's a machine created
[00:09:16.260 --> 00:09:19.180]   or it's a creature created in a quantum mechanical way.
[00:09:19.180 --> 00:09:21.820]   It just ends up in a very, very different place
[00:09:21.820 --> 00:09:23.420]   to the one we end up in.
[00:09:23.420 --> 00:09:26.860]   So part of the preoccupation with annihilation
[00:09:26.860 --> 00:09:29.940]   was to come up with an alien that was really alien
[00:09:29.940 --> 00:09:31.380]   and didn't give us,
[00:09:31.380 --> 00:09:35.380]   and it didn't give us and we didn't give it
[00:09:35.380 --> 00:09:40.000]   any kind of easy connection between human and the alien.
[00:09:40.000 --> 00:09:42.160]   Because I think it was to do with the idea
[00:09:42.160 --> 00:09:44.540]   that you could have an alien that landed on this planet
[00:09:44.540 --> 00:09:46.600]   that wouldn't even know we were here.
[00:09:46.600 --> 00:09:49.460]   And we might only glancingly know it was here.
[00:09:49.460 --> 00:09:52.180]   There'd just be this strange point
[00:09:52.180 --> 00:09:53.860]   where the Venn diagrams connected
[00:09:53.860 --> 00:09:56.180]   where we could sense each other or something like that.
[00:09:56.180 --> 00:09:59.980]   - So in the movie, first of all, incredibly original view
[00:09:59.980 --> 00:10:01.900]   of what an alien life would be.
[00:10:01.900 --> 00:10:04.920]   And it's in that sense, it's a huge success.
[00:10:04.920 --> 00:10:07.820]   Let's go inside your imagination.
[00:10:07.820 --> 00:10:11.940]   Did the alien, that alien entity know anything
[00:10:11.940 --> 00:10:13.980]   about humans when it landed?
[00:10:13.980 --> 00:10:14.820]   - No.
[00:10:14.820 --> 00:10:18.240]   - So the idea is you're basically an alien,
[00:10:18.240 --> 00:10:22.420]   life is trying to reach out to anything
[00:10:22.420 --> 00:10:25.940]   that might be able to hear its mechanism of communication
[00:10:25.940 --> 00:10:30.140]   or was it simply, was it just basically their biologist
[00:10:30.140 --> 00:10:32.420]   exploring different kinds of stuff that you can--
[00:10:32.420 --> 00:10:34.540]   - But you see, this is the interesting thing is
[00:10:34.540 --> 00:10:36.780]   as soon as you say their biologist,
[00:10:36.780 --> 00:10:38.380]   you've done the thing of attributing
[00:10:38.380 --> 00:10:40.560]   human type motivations to it.
[00:10:40.560 --> 00:10:46.940]   I was trying to free myself from anything like that.
[00:10:46.940 --> 00:10:51.060]   So all sorts of questions you might answer
[00:10:51.060 --> 00:10:54.100]   about this notional alien, I wouldn't be able to answer
[00:10:54.100 --> 00:10:55.700]   because I don't know what it was.
[00:10:55.700 --> 00:10:56.540]   (laughing)
[00:10:56.540 --> 00:10:57.500]   Or how it worked.
[00:10:57.500 --> 00:11:02.340]   I had some rough ideas, like it had a very, very,
[00:11:02.340 --> 00:11:04.340]   very slow clock speed.
[00:11:04.340 --> 00:11:07.380]   And I thought maybe the way it is interacting
[00:11:07.380 --> 00:11:09.460]   with this environment is a little bit like the way
[00:11:09.460 --> 00:11:13.340]   an octopus will change its color forms
[00:11:13.340 --> 00:11:15.180]   around the space that it's in.
[00:11:15.180 --> 00:11:19.380]   So it's sort of reacting to what it's in to an extent,
[00:11:19.380 --> 00:11:23.580]   but the reason it's reacting in that way is indeterminate.
[00:11:23.580 --> 00:11:28.580]   But its clock speed was slower than our human life
[00:11:28.580 --> 00:11:32.940]   clock speed, but it's faster than evolution.
[00:11:32.940 --> 00:11:33.900]   - Faster than our--
[00:11:33.900 --> 00:11:35.020]   - Than our evolution.
[00:11:35.020 --> 00:11:37.700]   - Yeah, given the four billion years it took us to get here,
[00:11:37.700 --> 00:11:39.820]   then yes, maybe it started at eight.
[00:11:39.820 --> 00:11:41.300]   - If you look at the human civilization
[00:11:41.300 --> 00:11:43.460]   as a single organism.
[00:11:43.460 --> 00:11:46.460]   In that sense, this evolution could be us.
[00:11:46.460 --> 00:11:49.840]   The evolution of living organisms on Earth
[00:11:49.840 --> 00:11:51.380]   could be just a single organism
[00:11:51.380 --> 00:11:54.100]   and it's kind of, that's its life,
[00:11:54.100 --> 00:11:57.220]   is the evolution process that eventually will lead
[00:11:57.220 --> 00:12:00.940]   to probably the heat death of the universe
[00:12:00.940 --> 00:12:02.660]   or something before that.
[00:12:02.660 --> 00:12:05.380]   I mean, that's just an incredible idea.
[00:12:05.380 --> 00:12:09.000]   So you almost don't know, you've created something
[00:12:09.000 --> 00:12:11.620]   that you don't even know how it works.
[00:12:11.620 --> 00:12:16.620]   - Yeah, because any time I tried to look into
[00:12:16.620 --> 00:12:20.260]   how it might work, I would then inevitably be attaching
[00:12:20.260 --> 00:12:22.860]   my kind of thought processes into it.
[00:12:22.860 --> 00:12:24.940]   And I wanted to try and put a bubble around it
[00:12:24.940 --> 00:12:27.820]   where I was saying, no, this is alien
[00:12:27.820 --> 00:12:29.540]   in its most alien form.
[00:12:29.540 --> 00:12:32.880]   I have no real point of contact.
[00:12:32.880 --> 00:12:37.620]   - So unfortunately, I can't talk to Stanley Kubrick.
[00:12:37.620 --> 00:12:41.400]   So I'm really fortunate to get a chance to talk to you.
[00:12:41.400 --> 00:12:47.420]   On this particular notion, I'd like to ask it
[00:12:47.420 --> 00:12:48.940]   a bunch of different ways and we'll explore
[00:12:48.940 --> 00:12:51.260]   it in different ways, but do you ever consider
[00:12:51.260 --> 00:12:53.540]   human imagination, your imagination,
[00:12:53.540 --> 00:12:57.060]   as a window into a possible future
[00:12:57.060 --> 00:13:00.100]   and that what you're doing, you're putting
[00:13:00.100 --> 00:13:02.140]   that imagination on paper as a writer
[00:13:02.140 --> 00:13:04.720]   and then on screen as a director
[00:13:04.720 --> 00:13:06.700]   and that plants the seeds in the minds
[00:13:06.700 --> 00:13:10.180]   of millions of future and current scientists.
[00:13:10.180 --> 00:13:13.020]   And so your imagination, you putting it down
[00:13:13.020 --> 00:13:14.980]   actually makes it a reality.
[00:13:14.980 --> 00:13:18.580]   So it's almost like a first step of the scientific method.
[00:13:18.580 --> 00:13:20.300]   Like you imagining what's possible
[00:13:20.300 --> 00:13:22.460]   in your new series with Ex Machina
[00:13:22.460 --> 00:13:28.500]   is actually inspiring thousands of 12 year olds,
[00:13:28.500 --> 00:13:31.740]   millions of scientists and actually creating
[00:13:31.740 --> 00:13:33.100]   the future you've imagined.
[00:13:33.100 --> 00:13:37.120]   - Well, all I could say is that from my point of view,
[00:13:37.120 --> 00:13:39.220]   it's almost exactly the reverse
[00:13:39.220 --> 00:13:44.220]   because I see that pretty much everything I do
[00:13:45.640 --> 00:13:49.680]   is a reaction to what scientists are doing.
[00:13:49.680 --> 00:13:55.800]   I'm an interested lay person and I feel,
[00:13:55.800 --> 00:14:02.200]   this individual, I feel that the most interesting area
[00:14:02.200 --> 00:14:05.540]   that humans are involved in is science.
[00:14:05.540 --> 00:14:07.320]   I think art is very, very interesting,
[00:14:07.320 --> 00:14:09.500]   but the most interesting is science.
[00:14:09.500 --> 00:14:14.360]   And science is in a weird place because maybe
[00:14:14.360 --> 00:14:18.040]   around the time Newton was alive,
[00:14:18.040 --> 00:14:21.320]   if a very, very interested lay person said to themselves,
[00:14:21.320 --> 00:14:23.960]   I want to really understand what Newton is saying
[00:14:23.960 --> 00:14:25.580]   about the way the world works,
[00:14:25.580 --> 00:14:28.880]   with a few years of dedicated thinking,
[00:14:28.880 --> 00:14:31.120]   they would be able to understand
[00:14:31.120 --> 00:14:34.480]   the sort of principles he was laying out.
[00:14:34.480 --> 00:14:35.920]   And I don't think that's true anymore.
[00:14:35.920 --> 00:14:37.880]   I think that's stopped being true now.
[00:14:37.880 --> 00:14:41.760]   So I'm a pretty smart guy.
[00:14:41.760 --> 00:14:46.320]   And if I said to myself, I want to really, really understand
[00:14:46.320 --> 00:14:51.240]   what is currently the state of quantum mechanics
[00:14:51.240 --> 00:14:54.720]   or string theory or any of the sort of branching areas of it,
[00:14:54.720 --> 00:14:56.280]   I wouldn't be able to,
[00:14:56.280 --> 00:14:59.080]   I'd be intellectually incapable of doing it
[00:14:59.080 --> 00:15:02.240]   because to work in those fields at the moment
[00:15:02.240 --> 00:15:03.640]   is a bit like being an athlete.
[00:15:03.640 --> 00:15:05.800]   I suspect you need to start when you're 12.
[00:15:05.800 --> 00:15:09.520]   And if you start in your mid twenties,
[00:15:09.520 --> 00:15:11.480]   start trying to understand in your mid twenties,
[00:15:11.480 --> 00:15:13.960]   then you're just never gonna catch up.
[00:15:13.960 --> 00:15:15.760]   That's the way it feels to me.
[00:15:15.760 --> 00:15:19.520]   So what I do is I try to make myself open.
[00:15:19.520 --> 00:15:21.000]   So the people that you're implying,
[00:15:21.000 --> 00:15:24.320]   maybe I would influence,
[00:15:24.320 --> 00:15:25.880]   to me, it's exactly the other way around.
[00:15:25.880 --> 00:15:28.000]   These people are strongly influencing me.
[00:15:28.000 --> 00:15:30.440]   I'm thinking they're doing something fascinating.
[00:15:30.440 --> 00:15:32.960]   I'm concentrating and working as hard as I can
[00:15:32.960 --> 00:15:35.960]   to try and understand the implications of what they say.
[00:15:35.960 --> 00:15:38.240]   And in some ways, often what I'm trying to do
[00:15:38.240 --> 00:15:41.440]   is disseminate their ideas
[00:15:41.440 --> 00:15:47.720]   into a means by which it can enter a public conversation.
[00:15:47.720 --> 00:15:53.600]   So Ex Machina contains lots of name checks,
[00:15:53.600 --> 00:15:57.080]   all sorts of existing thought experiments,
[00:15:57.080 --> 00:16:00.880]   shadows on Plato's cave
[00:16:00.880 --> 00:16:02.800]   and Mary in the black and white room
[00:16:02.800 --> 00:16:07.520]   and all sorts of different long standing thought processes
[00:16:07.520 --> 00:16:12.520]   about sentience or consciousness or subjectivity or gender
[00:16:12.520 --> 00:16:14.520]   or whatever it happens to be.
[00:16:14.520 --> 00:16:17.480]   And then I'm trying to marshal that into a narrative
[00:16:17.480 --> 00:16:19.560]   to say, look, this stuff is interesting
[00:16:19.560 --> 00:16:23.360]   and it's also relevant and this is my best shot at it.
[00:16:23.360 --> 00:16:27.720]   So I'm the one being influenced in my construction.
[00:16:27.720 --> 00:16:28.920]   - That's fascinating.
[00:16:28.920 --> 00:16:31.000]   Of course, you would say that
[00:16:31.000 --> 00:16:33.480]   because you're not even aware of your own.
[00:16:33.480 --> 00:16:35.640]   That's probably what Kubrick would say too, right?
[00:16:35.640 --> 00:16:40.120]   Is in describing why how 9,000 is created
[00:16:40.120 --> 00:16:42.000]   the way how 9,000 is created
[00:16:42.000 --> 00:16:43.480]   is you're just studying what's...
[00:16:43.480 --> 00:16:48.200]   But the reality when the specifics of the knowledge
[00:16:48.200 --> 00:16:50.320]   passes through your imagination,
[00:16:50.320 --> 00:16:53.800]   I would argue that you're incorrect
[00:16:53.800 --> 00:16:56.960]   in thinking that you're just disseminating knowledge.
[00:16:56.960 --> 00:17:00.720]   That the very act of your imagination
[00:17:01.720 --> 00:17:05.280]   consuming that science,
[00:17:05.280 --> 00:17:09.160]   it creates something, it creates the next step,
[00:17:09.160 --> 00:17:11.240]   potentially creates the next step.
[00:17:11.240 --> 00:17:15.120]   I certainly think that's true with 2001 A Space Odyssey.
[00:17:15.120 --> 00:17:18.120]   I think at its best, if it fails--
[00:17:18.120 --> 00:17:19.000]   - It's true of that.
[00:17:19.000 --> 00:17:20.800]   Yeah, it's true of that, definitely.
[00:17:20.800 --> 00:17:21.880]   (laughing)
[00:17:21.880 --> 00:17:24.800]   - At its best, it plans something, it's hard to describe,
[00:17:24.800 --> 00:17:29.160]   but it inspires the next generation
[00:17:29.160 --> 00:17:31.080]   and it could be field dependent.
[00:17:31.080 --> 00:17:35.080]   So your new series is more a connection to physics,
[00:17:35.080 --> 00:17:37.640]   quantum physics, quantum mechanics, quantum computing,
[00:17:37.640 --> 00:17:40.520]   and yet Ex Machina is more artificial intelligence.
[00:17:40.520 --> 00:17:43.080]   I know more about AI.
[00:17:43.080 --> 00:17:48.080]   My sense that AI is much earlier
[00:17:48.080 --> 00:17:51.840]   in the depth of its understanding.
[00:17:51.840 --> 00:17:55.280]   I would argue nobody understands anything
[00:17:55.280 --> 00:17:57.840]   to the depth that physicists do about physics.
[00:17:57.840 --> 00:18:00.520]   In AI, nobody understands AI.
[00:18:00.520 --> 00:18:04.000]   That there is a lot of importance and role for imagination.
[00:18:04.000 --> 00:18:06.000]   Which I think, we're in that,
[00:18:06.000 --> 00:18:08.200]   or Freud imagined the subconscious,
[00:18:08.200 --> 00:18:10.880]   we're in that stage of AI,
[00:18:10.880 --> 00:18:12.760]   where there's a lot of imagination needed
[00:18:12.760 --> 00:18:14.400]   and thinking outside the box.
[00:18:14.400 --> 00:18:15.680]   - Yeah, it's interesting.
[00:18:15.680 --> 00:18:21.120]   The spread of discussions and the spread of anxieties
[00:18:21.120 --> 00:18:23.480]   that exist about AI fascinate me.
[00:18:23.480 --> 00:18:29.640]   The way in which some people seem terrified about it.
[00:18:30.280 --> 00:18:32.360]   Whilst also pursuing it.
[00:18:32.360 --> 00:18:36.920]   And I've never shared that fear about AI personally.
[00:18:36.920 --> 00:18:42.680]   But the way in which it agitates people
[00:18:42.680 --> 00:18:44.600]   and also the people who it agitates,
[00:18:44.600 --> 00:18:47.400]   I find kind of fascinating.
[00:18:47.400 --> 00:18:49.360]   - Are you afraid?
[00:18:49.360 --> 00:18:50.880]   Are you excited?
[00:18:50.880 --> 00:18:54.720]   Are you sad by the possibility?
[00:18:54.720 --> 00:18:58.040]   Let's take the existential risk of artificial intelligence,
[00:18:58.040 --> 00:19:01.280]   by the possibility an artificial intelligence system
[00:19:01.280 --> 00:19:06.600]   becomes our offspring and makes us obsolete.
[00:19:06.600 --> 00:19:10.720]   - I mean, it's a huge subject to talk about, I suppose.
[00:19:10.720 --> 00:19:13.120]   But one of the things I think is that humans
[00:19:13.120 --> 00:19:18.120]   are actually very experienced at creating new life forms.
[00:19:18.120 --> 00:19:23.200]   Because that's why you and I are both here.
[00:19:23.200 --> 00:19:25.000]   And it's why everyone on the planet is here.
[00:19:25.000 --> 00:19:29.880]   And so something in the process of having a living thing
[00:19:29.880 --> 00:19:32.000]   that exists that didn't exist previously
[00:19:32.000 --> 00:19:35.400]   is very much encoded into the structures of our life
[00:19:35.400 --> 00:19:37.320]   and the structures of our societies.
[00:19:37.320 --> 00:19:38.640]   It doesn't mean we always get it right,
[00:19:38.640 --> 00:19:41.480]   but it does mean we've learnt quite a lot about that.
[00:19:41.480 --> 00:19:45.480]   We've learnt quite a lot about what the dangers are
[00:19:45.480 --> 00:19:49.320]   of allowing things to be unchecked.
[00:19:49.320 --> 00:19:52.600]   And it's why we then create systems of checks and balances
[00:19:52.600 --> 00:19:55.240]   in our government and so on and so forth.
[00:19:55.240 --> 00:19:56.680]   I mean, that's not to say...
[00:19:56.680 --> 00:19:59.880]   The other thing is it seems like
[00:19:59.880 --> 00:20:01.880]   there's all sorts of things that you could put
[00:20:01.880 --> 00:20:04.440]   into a machine that you would not be...
[00:20:04.440 --> 00:20:07.480]   So with us, we sort of roughly try to give some rules
[00:20:07.480 --> 00:20:10.200]   to live by, and some of us then live by those rules
[00:20:10.200 --> 00:20:11.040]   and some don't.
[00:20:11.040 --> 00:20:12.560]   And with a machine, it feels like
[00:20:12.560 --> 00:20:13.880]   you could enforce those things.
[00:20:13.880 --> 00:20:17.080]   So partly because of our previous experience
[00:20:17.080 --> 00:20:19.120]   and partly because of the different nature of a machine,
[00:20:19.120 --> 00:20:20.880]   I just don't feel anxious about it.
[00:20:22.000 --> 00:20:25.400]   More, I just see all the good that...
[00:20:25.400 --> 00:20:28.240]   Broadly speaking, the good that can come from it.
[00:20:28.240 --> 00:20:32.160]   But that's just where I am on that anxiety spectrum.
[00:20:32.160 --> 00:20:34.640]   - There's a sadness.
[00:20:34.640 --> 00:20:37.720]   So we as humans give birth to other humans, right?
[00:20:37.720 --> 00:20:39.360]   But then there's generations,
[00:20:39.360 --> 00:20:42.000]   and there's often in the older generation a sadness
[00:20:42.000 --> 00:20:44.080]   about what the world has become now.
[00:20:44.080 --> 00:20:44.960]   I mean, that's kind of...
[00:20:44.960 --> 00:20:47.120]   - Yeah, there is, but there's a counterpoint as well,
[00:20:47.120 --> 00:20:51.480]   which is that most parents would wish
[00:20:51.480 --> 00:20:53.960]   for a better life for their children.
[00:20:53.960 --> 00:20:57.000]   So there may be a regret about some things about the past,
[00:20:57.000 --> 00:20:59.560]   but broadly speaking, what people really want
[00:20:59.560 --> 00:21:00.600]   is that things will be better
[00:21:00.600 --> 00:21:02.760]   for the future generations, not worse.
[00:21:02.760 --> 00:21:05.880]   And so, and then it's a question
[00:21:05.880 --> 00:21:07.960]   about what constitutes a future generation.
[00:21:07.960 --> 00:21:09.760]   A future generation could involve people.
[00:21:09.760 --> 00:21:11.240]   It also could involve machines,
[00:21:11.240 --> 00:21:14.680]   and it could involve a sort of cross-pollinated version
[00:21:14.680 --> 00:21:16.160]   of the two or any...
[00:21:16.160 --> 00:21:19.880]   But none of those things make me feel anxious.
[00:21:19.880 --> 00:21:21.280]   - It doesn't give you anxiety.
[00:21:21.280 --> 00:21:23.040]   It doesn't excite you?
[00:21:23.040 --> 00:21:24.320]   Like anything that's new?
[00:21:24.320 --> 00:21:25.560]   - It does.
[00:21:25.560 --> 00:21:27.000]   Not anything that's new.
[00:21:27.000 --> 00:21:29.920]   I don't think, for example, I've got...
[00:21:29.920 --> 00:21:32.560]   My anxieties relate to things like social media.
[00:21:32.560 --> 00:21:35.960]   So I've got plenty of anxieties about that.
[00:21:35.960 --> 00:21:38.320]   - Which is also driven by artificial intelligence
[00:21:38.320 --> 00:21:41.040]   in the sense that there's too much information
[00:21:41.040 --> 00:21:42.640]   to be able to...
[00:21:42.640 --> 00:21:45.120]   An algorithm has to filter that information
[00:21:45.120 --> 00:21:46.200]   and present to you.
[00:21:46.200 --> 00:21:49.720]   So ultimately, the algorithm, a simple,
[00:21:49.720 --> 00:21:52.600]   oftentimes simple algorithm is controlling
[00:21:52.600 --> 00:21:54.680]   the flow of information on social media.
[00:21:54.680 --> 00:21:57.560]   So that's another form of AI.
[00:21:57.560 --> 00:21:59.640]   - But at least my sense of it, I might be wrong,
[00:21:59.640 --> 00:22:02.440]   but my sense of it is that the algorithms
[00:22:02.440 --> 00:22:06.120]   have an either conscious or unconscious bias,
[00:22:06.120 --> 00:22:07.480]   which is created by the people
[00:22:07.480 --> 00:22:08.800]   who are making the algorithms
[00:22:08.800 --> 00:22:13.440]   and sort of delineating the areas
[00:22:13.440 --> 00:22:15.720]   to which those algorithms are gonna lean.
[00:22:15.720 --> 00:22:18.040]   And so, for example,
[00:22:18.040 --> 00:22:19.280]   the kind of thing I'd be worried about
[00:22:19.280 --> 00:22:21.400]   is that it hasn't been thought about enough
[00:22:21.400 --> 00:22:24.600]   how dangerous it is to allow algorithms
[00:22:24.600 --> 00:22:27.020]   to create echo chambers, say.
[00:22:27.020 --> 00:22:31.000]   But that doesn't seem to me to be about the AI
[00:22:31.000 --> 00:22:32.760]   or the algorithm.
[00:22:32.760 --> 00:22:35.000]   It's the naivety of the people
[00:22:35.000 --> 00:22:38.320]   who are constructing the algorithms to do that thing,
[00:22:38.320 --> 00:22:39.480]   if you see what I mean.
[00:22:39.480 --> 00:22:40.480]   - Yes.
[00:22:40.520 --> 00:22:43.560]   So in your new series, "Devs,"
[00:22:43.560 --> 00:22:45.360]   then we could speak more broadly.
[00:22:45.360 --> 00:22:49.320]   Let's talk about the people constructing those algorithms,
[00:22:49.320 --> 00:22:51.840]   which in our modern society, Silicon Valley,
[00:22:51.840 --> 00:22:53.640]   those algorithms happen to be a source
[00:22:53.640 --> 00:22:56.560]   of a lot of income because of advertisements.
[00:22:56.560 --> 00:22:59.960]   So let me ask sort of a question about those people.
[00:22:59.960 --> 00:23:04.800]   Are current concerns and failures on social media,
[00:23:04.800 --> 00:23:08.280]   their naivety, I can't pronounce that word well,
[00:23:08.280 --> 00:23:09.880]   are they naive?
[00:23:09.880 --> 00:23:11.100]   Are they,
[00:23:11.100 --> 00:23:17.520]   I use that word carefully, but evil in intent
[00:23:17.520 --> 00:23:20.920]   or misaligned in intent?
[00:23:20.920 --> 00:23:23.160]   I think that's a, do they mean well
[00:23:23.160 --> 00:23:27.200]   and just have a unintended consequence?
[00:23:27.200 --> 00:23:30.000]   Or is there something dark in them
[00:23:30.000 --> 00:23:33.840]   that results in them creating a company,
[00:23:33.840 --> 00:23:37.440]   results in that super competitive drive to be successful?
[00:23:37.440 --> 00:23:38.280]   And those are the people
[00:23:38.280 --> 00:23:41.200]   that will end up controlling the algorithms.
[00:23:41.200 --> 00:23:43.160]   - At a guess, I'd say there are instances
[00:23:43.160 --> 00:23:44.800]   of all those things.
[00:23:44.800 --> 00:23:47.560]   So sometimes I think it's naivety.
[00:23:47.560 --> 00:23:49.620]   Sometimes I think it's extremely dark.
[00:23:49.620 --> 00:23:55.920]   And sometimes I think people are not being naive or dark.
[00:23:55.920 --> 00:24:00.280]   And then in those instances,
[00:24:00.280 --> 00:24:03.880]   they're sometimes generating things that are very benign
[00:24:03.880 --> 00:24:06.240]   and other times generating things
[00:24:06.240 --> 00:24:08.960]   that despite their best intentions are not very benign.
[00:24:08.960 --> 00:24:12.440]   It's something, I think the reason why I don't get anxious
[00:24:12.440 --> 00:24:15.020]   about AI in terms of,
[00:24:15.020 --> 00:24:18.920]   or at least AIs that have,
[00:24:18.920 --> 00:24:22.880]   I don't know, a relationship with,
[00:24:22.880 --> 00:24:24.600]   some sort of relationship with humans
[00:24:24.600 --> 00:24:27.600]   is that I think that's the stuff we're quite well equipped
[00:24:27.600 --> 00:24:31.160]   to understand how to mitigate.
[00:24:31.160 --> 00:24:36.160]   The problem is issues that relate to AI
[00:24:37.080 --> 00:24:41.000]   actually to the power of humans or the wealth of humans.
[00:24:41.000 --> 00:24:45.400]   And that's where it's dangerous here and now.
[00:24:45.400 --> 00:24:48.200]   So what I see,
[00:24:48.200 --> 00:24:52.620]   I'll tell you what I sometimes feel about Silicon Valley
[00:24:52.620 --> 00:24:56.920]   is that it's like Wall Street in the '80s.
[00:24:56.920 --> 00:25:01.440]   It's rabidly capitalistic,
[00:25:01.440 --> 00:25:03.800]   absolutely rabidly capitalistic,
[00:25:03.800 --> 00:25:05.560]   and it's rabidly greedy.
[00:25:06.320 --> 00:25:10.880]   But whereas in the '80s,
[00:25:10.880 --> 00:25:12.720]   the sense one had of Wall Street
[00:25:12.720 --> 00:25:15.240]   was that these people kind of knew they were sharks
[00:25:15.240 --> 00:25:17.480]   and in a way relished in being sharks
[00:25:17.480 --> 00:25:19.120]   and dressed in sharp suits
[00:25:19.120 --> 00:25:24.120]   and kind of lorded over other people
[00:25:24.120 --> 00:25:26.000]   and felt good about doing it.
[00:25:26.000 --> 00:25:27.880]   Silicon Valley has managed to hide
[00:25:27.880 --> 00:25:30.920]   its voracious Wall Street-like capitalism
[00:25:30.920 --> 00:25:32.600]   behind hipster T-shirts
[00:25:32.600 --> 00:25:37.600]   and cool cafes in the place where they set up there.
[00:25:37.600 --> 00:25:40.760]   And so that obfuscates what's really going on.
[00:25:40.760 --> 00:25:41.960]   And what's really going on
[00:25:41.960 --> 00:25:45.960]   is the absolute voracious pursuit of money and power.
[00:25:45.960 --> 00:25:48.540]   So that's where it gets shaky for me.
[00:25:48.540 --> 00:25:49.840]   - So that veneer,
[00:25:49.840 --> 00:25:52.920]   and you explore that brilliantly,
[00:25:52.920 --> 00:25:57.760]   that veneer of virtue that Silicon Valley has.
[00:25:57.760 --> 00:25:59.840]   - Which they believe themselves, I'm sure.
[00:25:59.840 --> 00:26:00.680]   - Boy. - A long time.
[00:26:00.680 --> 00:26:01.840]   - So let me, okay.
[00:26:01.840 --> 00:26:04.740]   I hope to be one of those people.
[00:26:04.740 --> 00:26:09.760]   And I believe that.
[00:26:09.760 --> 00:26:15.920]   So as maybe a devil's advocate term
[00:26:15.920 --> 00:26:17.640]   poorly used in this case,
[00:26:17.640 --> 00:26:21.200]   what if some of them really are trying
[00:26:21.200 --> 00:26:22.160]   to build a better world?
[00:26:22.160 --> 00:26:23.080]   I can't-- - I'm sure,
[00:26:23.080 --> 00:26:24.240]   I think some of them are.
[00:26:24.240 --> 00:26:26.600]   I think I've spoken to ones who I believe in their heart
[00:26:26.600 --> 00:26:27.880]   feel they're building a better world.
[00:26:27.880 --> 00:26:29.720]   - Are they not able to in a sense?
[00:26:29.720 --> 00:26:31.600]   - No, they may or may not be.
[00:26:31.600 --> 00:26:33.800]   But it's just as a zone
[00:26:33.800 --> 00:26:35.760]   with a lot of bullshit flying about.
[00:26:35.760 --> 00:26:37.920]   And there's also another thing which is,
[00:26:37.920 --> 00:26:40.220]   this actually goes back to,
[00:26:40.220 --> 00:26:44.440]   I always thought about some sports
[00:26:44.440 --> 00:26:46.640]   that later turned out to be corrupt
[00:26:46.640 --> 00:26:48.040]   in the way that the sport,
[00:26:48.040 --> 00:26:50.080]   like who won the boxing match
[00:26:50.080 --> 00:26:53.240]   or how a football match got thrown
[00:26:53.240 --> 00:26:55.520]   or cricket match or whatever happened to be.
[00:26:55.520 --> 00:26:57.000]   And I used to think, well, look,
[00:26:57.000 --> 00:26:58.300]   if there's a lot of money,
[00:26:58.300 --> 00:27:00.560]   and there really is a lot of money,
[00:27:00.560 --> 00:27:03.440]   people stand to make millions or even billions,
[00:27:03.440 --> 00:27:05.960]   you will find corruption that's gonna happen.
[00:27:05.960 --> 00:27:09.800]   So it's in the nature
[00:27:09.800 --> 00:27:12.760]   of its voracious appetite
[00:27:12.760 --> 00:27:14.200]   that some people will be corrupt
[00:27:14.200 --> 00:27:16.160]   and some people will exploit
[00:27:16.160 --> 00:27:17.940]   and some people will exploit
[00:27:17.940 --> 00:27:19.720]   whilst thinking they're doing something good.
[00:27:19.720 --> 00:27:23.440]   But there are also people who I think are very, very smart
[00:27:23.440 --> 00:27:26.400]   and very benign and actually very self-aware.
[00:27:26.400 --> 00:27:29.580]   And so I'm not trying to,
[00:27:29.580 --> 00:27:31.720]   I'm not trying to wipe out
[00:27:31.720 --> 00:27:34.760]   the motivations of this entire area.
[00:27:34.760 --> 00:27:37.360]   But I do, there are people in that world
[00:27:37.360 --> 00:27:38.760]   who scare the hell out of me.
[00:27:38.760 --> 00:27:40.120]   Yeah, sure.
[00:27:40.120 --> 00:27:42.040]   - Yeah, I'm a little bit naive in that,
[00:27:42.040 --> 00:27:45.820]   like I don't care at all about money.
[00:27:45.820 --> 00:27:49.200]   And so I'm...
[00:27:49.200 --> 00:27:52.760]   - You might be one of the good guys.
[00:27:52.760 --> 00:27:54.080]   - Yeah, but so the thought is,
[00:27:54.080 --> 00:27:55.820]   but I don't have money.
[00:27:55.820 --> 00:27:58.160]   So my thought is if you give me a billion dollars,
[00:27:58.160 --> 00:28:00.080]   I would, it would change nothing
[00:28:00.080 --> 00:28:01.520]   and I would spend it right away
[00:28:01.520 --> 00:28:04.440]   on investing it right back and creating a good world.
[00:28:04.440 --> 00:28:07.640]   But your intuition is that billion,
[00:28:07.640 --> 00:28:09.000]   there's something about that money
[00:28:09.000 --> 00:28:13.200]   that maybe slowly corrupts the people around you.
[00:28:13.200 --> 00:28:16.360]   There's somebody gets in that corrupts your soul,
[00:28:16.360 --> 00:28:17.800]   the way you view the world.
[00:28:17.800 --> 00:28:20.120]   - Money does corrupt, we know that.
[00:28:20.120 --> 00:28:22.600]   But there's a different sort of problem
[00:28:22.600 --> 00:28:24.920]   aside from just the money corrupts,
[00:28:24.920 --> 00:28:27.640]   you know, thing that we're familiar with
[00:28:27.640 --> 00:28:29.320]   in throughout history.
[00:28:29.320 --> 00:28:34.080]   And it's more about the sense of reinforcement
[00:28:34.080 --> 00:28:37.040]   an individual gets, which is so,
[00:28:37.040 --> 00:28:42.040]   it effectively works like the reason I earned all this money
[00:28:42.040 --> 00:28:44.520]   and so much more money than anyone else
[00:28:44.520 --> 00:28:46.200]   is because I'm very gifted.
[00:28:46.200 --> 00:28:47.960]   I'm actually a bit smarter than they are,
[00:28:47.960 --> 00:28:49.680]   or I'm a lot smarter than they are.
[00:28:49.680 --> 00:28:52.120]   And I can see the future in a way they can't.
[00:28:52.120 --> 00:28:55.320]   And maybe some of those people are not particularly smart,
[00:28:55.320 --> 00:28:59.200]   they're very lucky, or they're very talented entrepreneurs.
[00:28:59.200 --> 00:29:02.120]   And there's a difference between it.
[00:29:02.120 --> 00:29:05.360]   So in other words, the acquisition of the money and power
[00:29:05.360 --> 00:29:08.680]   can suddenly start to feel like evidence of virtue.
[00:29:08.680 --> 00:29:10.000]   And it's not evidence of virtue,
[00:29:10.000 --> 00:29:12.000]   it might be evidence of completely different things.
[00:29:12.000 --> 00:29:13.440]   - As brilliantly put, yeah.
[00:29:13.440 --> 00:29:15.480]   Yeah, as brilliantly put like,
[00:29:15.480 --> 00:29:18.160]   so I think one of the fundamental drivers
[00:29:18.160 --> 00:29:20.480]   of my current morality,
[00:29:20.480 --> 00:29:23.900]   let me just represent nerds in general.
[00:29:24.900 --> 00:29:29.900]   Of all kinds, is constant self doubt.
[00:29:29.900 --> 00:29:35.380]   And the signals, I'm very sensitive to signals
[00:29:35.380 --> 00:29:38.660]   from people that tell me I'm doing the wrong thing.
[00:29:38.660 --> 00:29:41.160]   But when there's a huge inflow of money,
[00:29:41.160 --> 00:29:44.140]   you just put it brilliantly
[00:29:44.140 --> 00:29:46.660]   that that could become an overpowering signal
[00:29:46.660 --> 00:29:49.460]   that everything you do is right.
[00:29:49.460 --> 00:29:53.260]   And so your moral compass can just get thrown off.
[00:29:53.260 --> 00:29:57.300]   - Yeah, and that is not contained to Silicon Valley,
[00:29:57.300 --> 00:29:58.380]   that's across the board.
[00:29:58.380 --> 00:29:59.620]   - In general, yeah.
[00:29:59.620 --> 00:30:01.100]   Like I said, I'm from the Soviet Union,
[00:30:01.100 --> 00:30:03.740]   the current president is convinced,
[00:30:03.740 --> 00:30:08.420]   I believe actually he wants to do really good
[00:30:08.420 --> 00:30:10.300]   by the country and by the world,
[00:30:10.300 --> 00:30:14.260]   but his moral clock may be, or compass may be off because--
[00:30:14.260 --> 00:30:17.620]   - Yeah, I mean, it's the interesting thing about evil,
[00:30:17.620 --> 00:30:21.020]   which is that I think most people
[00:30:21.020 --> 00:30:24.100]   who do spectacularly evil things think themselves
[00:30:24.100 --> 00:30:25.820]   they're doing really good things.
[00:30:25.820 --> 00:30:27.900]   They're not there thinking,
[00:30:27.900 --> 00:30:29.780]   I am a sort of incarnation of Satan,
[00:30:29.780 --> 00:30:33.660]   they're thinking, yeah, I've seen a way to fix the world
[00:30:33.660 --> 00:30:35.900]   and everyone else is wrong, here I go.
[00:30:35.900 --> 00:30:39.420]   - In fact, I'm having a fascinating conversation
[00:30:39.420 --> 00:30:42.980]   with a historian of Stalin, and he took power,
[00:30:42.980 --> 00:30:47.180]   he actually got more power
[00:30:47.180 --> 00:30:49.540]   than almost any person in history.
[00:30:49.540 --> 00:30:52.300]   And he wanted, he didn't want power,
[00:30:52.300 --> 00:30:54.180]   he just wanted, he truly,
[00:30:54.180 --> 00:30:55.460]   and this is what people don't realize,
[00:30:55.460 --> 00:30:58.460]   he truly believed that communism
[00:30:58.460 --> 00:31:00.940]   will make for a better world.
[00:31:00.940 --> 00:31:01.780]   - Absolutely.
[00:31:01.780 --> 00:31:03.020]   - And he wanted power,
[00:31:03.020 --> 00:31:04.660]   he wanted to destroy the competition
[00:31:04.660 --> 00:31:07.540]   to make sure that we actually make communism work
[00:31:07.540 --> 00:31:10.100]   in the Soviet Union and then spread it across the world.
[00:31:10.100 --> 00:31:11.380]   He was trying to do good.
[00:31:11.380 --> 00:31:16.060]   - I think it's typically the case
[00:31:16.060 --> 00:31:17.860]   that that's what people think they're doing.
[00:31:17.860 --> 00:31:21.100]   And I think that, but you don't need to go to Stalin,
[00:31:21.100 --> 00:31:24.420]   I mean, Stalin, I think Stalin probably got pretty crazy,
[00:31:24.420 --> 00:31:26.380]   but actually that's another part of it,
[00:31:26.380 --> 00:31:29.500]   which is that the other thing that comes
[00:31:29.500 --> 00:31:31.780]   from being convinced of your own virtue
[00:31:31.780 --> 00:31:34.780]   is that then you stop listening to the modifiers around you.
[00:31:34.780 --> 00:31:37.860]   And that tends to drive people crazy.
[00:31:37.860 --> 00:31:40.500]   It's other people that keep us sane.
[00:31:40.500 --> 00:31:42.220]   And if you stop listening to them,
[00:31:42.220 --> 00:31:44.460]   I think you go a bit mad, that also happens.
[00:31:44.460 --> 00:31:47.220]   - That's funny, disagreement keeps us sane.
[00:31:47.220 --> 00:31:52.220]   To jump back for an entire generation of AI researchers,
[00:31:52.220 --> 00:31:56.860]   2001, a space odyssey, put an image,
[00:31:56.860 --> 00:31:59.300]   the idea of human level, superhuman level intelligence
[00:31:59.300 --> 00:32:01.020]   into their mind.
[00:32:01.020 --> 00:32:04.860]   Do you ever, sort of jumping back to Ex Machina
[00:32:04.860 --> 00:32:06.100]   and talk a little bit about that,
[00:32:06.100 --> 00:32:08.860]   do you ever consider the audience of people
[00:32:08.860 --> 00:32:12.780]   who build the systems, the roboticist,
[00:32:12.780 --> 00:32:14.660]   the scientists that build the systems
[00:32:14.660 --> 00:32:16.360]   based on the stories you create?
[00:32:17.240 --> 00:32:20.200]   Which I would argue, I mean, there's literally
[00:32:20.200 --> 00:32:25.200]   most of the top researchers about 40, 50 years old and plus,
[00:32:25.200 --> 00:32:29.640]   that's their favorite movie, 2001, a space odyssey.
[00:32:29.640 --> 00:32:31.360]   And it really is in their work,
[00:32:31.360 --> 00:32:35.160]   their idea of what ethics is, of what is the target,
[00:32:35.160 --> 00:32:39.200]   the hope, the dangers of AI is that movie.
[00:32:39.200 --> 00:32:43.680]   Do you ever consider the impact on those researchers
[00:32:43.680 --> 00:32:45.360]   when you create the work you do?
[00:32:46.360 --> 00:32:51.160]   - Certainly not with Ex Machina in relation to 2001,
[00:32:51.160 --> 00:32:54.560]   because I'm not sure, I mean, I'd be pleased if there was,
[00:32:54.560 --> 00:32:58.360]   but I'm not sure in a way there isn't a fundamental
[00:32:58.360 --> 00:33:02.000]   discussion of issues to do with AI
[00:33:02.000 --> 00:33:07.000]   that isn't already and better dealt with by 2001.
[00:33:07.000 --> 00:33:09.920]   2001 does a very, very good account
[00:33:09.920 --> 00:33:15.160]   of the way in which an AI might think
[00:33:15.160 --> 00:33:19.960]   and also potential issues with the way the AI might think.
[00:33:19.960 --> 00:33:23.920]   And also then a separate question about whether the AI
[00:33:23.920 --> 00:33:26.760]   is malevolent or benevolent.
[00:33:26.760 --> 00:33:30.440]   And 2001 doesn't really, it's a slightly odd thing
[00:33:30.440 --> 00:33:33.440]   to be making a film when you know there's a preexisting film
[00:33:33.440 --> 00:33:35.800]   which is not a really superb job.
[00:33:35.800 --> 00:33:38.720]   - But there's questions of consciousness, embodiment,
[00:33:38.720 --> 00:33:41.080]   and also the same kinds of questions.
[00:33:41.080 --> 00:33:43.040]   'Cause those are my two favorite AI movies.
[00:33:43.040 --> 00:33:46.440]   So can you compare "Hal 9000" and "Ava",
[00:33:46.440 --> 00:33:48.800]   "Hal 9000" from "2001 Space Odyssey"
[00:33:48.800 --> 00:33:52.120]   and "Ava" from Ex Machina, in your view,
[00:33:52.120 --> 00:33:53.440]   from a philosophical perspective--
[00:33:53.440 --> 00:33:54.960]   - But they've got different goals.
[00:33:54.960 --> 00:33:56.720]   The two AIs have completely different goals.
[00:33:56.720 --> 00:33:58.480]   I think that's really the difference.
[00:33:58.480 --> 00:34:01.580]   So in some respects, Ex Machina took as a premise,
[00:34:01.580 --> 00:34:06.440]   how do you assess whether something else has consciousness?
[00:34:06.440 --> 00:34:08.200]   So it was a version of the Turing test,
[00:34:08.200 --> 00:34:11.240]   except instead of having the machine hidden,
[00:34:11.240 --> 00:34:13.920]   you put the machine in plain sight
[00:34:13.920 --> 00:34:16.200]   in the way that we are in plain sight of each other
[00:34:16.200 --> 00:34:17.760]   and say, now assess the consciousness.
[00:34:17.760 --> 00:34:19.440]   And the way it was illustrating
[00:34:19.440 --> 00:34:24.080]   the way in which you'd assess
[00:34:24.080 --> 00:34:25.920]   the state of consciousness of a machine
[00:34:25.920 --> 00:34:28.120]   is exactly the same way we assess
[00:34:28.120 --> 00:34:30.160]   the state of consciousness of each other.
[00:34:30.160 --> 00:34:33.320]   And in exactly the same way that, in a funny way,
[00:34:33.320 --> 00:34:35.560]   your sense of my consciousness
[00:34:35.560 --> 00:34:39.400]   is actually based primarily on your own consciousness.
[00:34:39.400 --> 00:34:42.760]   That is also then true with the machine.
[00:34:42.760 --> 00:34:45.840]   And so it was actually about how much
[00:34:45.840 --> 00:34:49.260]   of the sense of consciousness is a projection
[00:34:49.260 --> 00:34:50.880]   rather than something that consciousness
[00:34:50.880 --> 00:34:52.200]   is actually containing.
[00:34:52.200 --> 00:34:55.420]   - And "Half-Plato's Cave," I mean, you really explored,
[00:34:55.420 --> 00:34:57.800]   you could argue that how sort of "Space Odyssey"
[00:34:57.800 --> 00:35:00.480]   explores idea of the Turing test for intelligence.
[00:35:00.480 --> 00:35:01.900]   No, not test, there's no test,
[00:35:01.900 --> 00:35:04.760]   but it's more focused on intelligence.
[00:35:04.760 --> 00:35:09.760]   And Ex Machina kind of goes around intelligence
[00:35:09.760 --> 00:35:12.840]   and says the consciousness of the human to human,
[00:35:12.840 --> 00:35:14.840]   human to robot interaction is more interesting,
[00:35:14.840 --> 00:35:17.440]   more important, at least the focus
[00:35:17.440 --> 00:35:19.720]   of that particular movie.
[00:35:19.720 --> 00:35:22.320]   - Yeah, it's about the interior state
[00:35:22.320 --> 00:35:25.320]   and what constitutes the interior state
[00:35:25.320 --> 00:35:26.840]   and how do we know it's there.
[00:35:26.840 --> 00:35:28.520]   And actually, in that respect,
[00:35:28.520 --> 00:35:33.520]   Ex Machina is as much about consciousness in general
[00:35:33.920 --> 00:35:38.720]   as it is to do specifically with machine consciousness.
[00:35:38.720 --> 00:35:40.840]   And it's also interesting, you know that thing
[00:35:40.840 --> 00:35:43.320]   you started asking about, the dream state,
[00:35:43.320 --> 00:35:45.360]   and I was saying, well, I think we're all in a dream state
[00:35:45.360 --> 00:35:48.200]   because we're all in a subjective state.
[00:35:48.200 --> 00:35:52.840]   One of the things that I became aware of with Ex Machina
[00:35:52.840 --> 00:35:55.160]   is that the way in which people reacted to the film
[00:35:55.160 --> 00:35:57.920]   was very based on what they took into the film.
[00:35:57.920 --> 00:36:01.760]   So many people thought Ex Machina was the tale
[00:36:01.760 --> 00:36:05.800]   of a sort of evil robot who murders two men and escapes
[00:36:05.800 --> 00:36:09.200]   and she has no empathy, for example,
[00:36:09.200 --> 00:36:10.640]   because she's a machine.
[00:36:10.640 --> 00:36:14.640]   Whereas I felt, no, she was a conscious being
[00:36:14.640 --> 00:36:16.560]   with a consciousness different from mine,
[00:36:16.560 --> 00:36:21.560]   but so what, imprisoned and made a bunch of value judgments
[00:36:21.560 --> 00:36:25.800]   about how to get out of that box.
[00:36:25.800 --> 00:36:29.100]   And there's a moment which it sort of slightly bugs me,
[00:36:29.100 --> 00:36:31.880]   but nobody ever has noticed it and it's years after,
[00:36:31.880 --> 00:36:33.040]   so I might as well say it now,
[00:36:33.040 --> 00:36:38.040]   which is that after Ava has escaped, she crosses a room
[00:36:38.040 --> 00:36:39.760]   and as she's crossing a room,
[00:36:39.760 --> 00:36:42.040]   this is just before she leaves the building,
[00:36:42.040 --> 00:36:44.880]   she looks over her shoulder and she smiles.
[00:36:44.880 --> 00:36:49.240]   And I thought after all the conversation about tests,
[00:36:49.240 --> 00:36:52.340]   in a way, the best indication you could have
[00:36:52.340 --> 00:36:54.800]   of the interior state of someone
[00:36:54.800 --> 00:36:57.240]   is if they are not being observed
[00:36:57.240 --> 00:36:59.520]   and they smile about something,
[00:36:59.520 --> 00:37:01.240]   where they're smiling for themselves.
[00:37:01.240 --> 00:37:05.840]   And that, to me, was evidence of Ava's true sentience,
[00:37:05.840 --> 00:37:07.800]   whatever that sentience was.
[00:37:07.800 --> 00:37:09.100]   - Oh, that's really interesting.
[00:37:09.100 --> 00:37:11.080]   We don't get to observe Ava much
[00:37:11.080 --> 00:37:16.160]   or something like a smile in any context
[00:37:16.160 --> 00:37:19.480]   except through interaction, trying to convince others
[00:37:19.480 --> 00:37:21.560]   that she's conscious, that's beautiful.
[00:37:21.560 --> 00:37:22.800]   - Exactly, yeah.
[00:37:22.800 --> 00:37:25.000]   But it was a small, in a funny way,
[00:37:25.000 --> 00:37:28.760]   I think maybe people saw it as an evil smile,
[00:37:28.760 --> 00:37:32.160]   like, ha, you know, I fooled them.
[00:37:32.160 --> 00:37:34.200]   But actually, it was just a smile.
[00:37:34.200 --> 00:37:35.520]   And I thought, well, in the end,
[00:37:35.520 --> 00:37:37.280]   after all the conversations about the test,
[00:37:37.280 --> 00:37:39.720]   that was the answer to the test and then off she goes.
[00:37:39.720 --> 00:37:44.440]   - So if we align, if we just to linger a little bit longer
[00:37:44.440 --> 00:37:49.440]   on Hal and Ava, do you think in terms of motivation,
[00:37:49.440 --> 00:37:51.560]   what was Hal's motivation?
[00:37:51.560 --> 00:37:54.120]   Is Hal good or evil?
[00:37:54.120 --> 00:37:57.040]   Is Ava good or evil?
[00:37:57.040 --> 00:37:59.400]   - Ava's good, in my opinion.
[00:37:59.400 --> 00:38:05.280]   And Hal is neutral because I don't think Hal
[00:38:05.280 --> 00:38:10.560]   is presented as having a sophisticated emotional life.
[00:38:10.560 --> 00:38:14.560]   He has a set of paradigms,
[00:38:14.560 --> 00:38:16.600]   which is that the mission needs to be completed.
[00:38:16.600 --> 00:38:18.880]   I mean, it's a version of the paperclip.
[00:38:18.880 --> 00:38:19.720]   - Yeah. - You know.
[00:38:19.720 --> 00:38:23.120]   - The idea that it's a super intelligent machine
[00:38:23.120 --> 00:38:25.320]   but it's just performed a particular task.
[00:38:25.320 --> 00:38:27.320]   - Yeah. - And in doing that task,
[00:38:27.320 --> 00:38:28.960]   may destroy everybody on earth
[00:38:28.960 --> 00:38:32.400]   or may achieve undesirable effects for us humans.
[00:38:32.400 --> 00:38:33.240]   - Precisely, yeah.
[00:38:33.240 --> 00:38:35.200]   - But what if, okay.
[00:38:35.200 --> 00:38:36.760]   - At the very end, he says something like,
[00:38:36.760 --> 00:38:38.320]   "I'm afraid, Dave."
[00:38:38.320 --> 00:38:43.320]   But that may be he is on some level experiencing fear
[00:38:43.320 --> 00:38:49.360]   or it may be this is the terms in which it would be wise
[00:38:49.360 --> 00:38:52.720]   to stop someone from doing the thing they're doing.
[00:38:52.720 --> 00:38:53.560]   If you see what I mean.
[00:38:53.560 --> 00:38:54.400]   - Yes, absolutely.
[00:38:54.400 --> 00:38:55.440]   So actually, that's funny.
[00:38:55.440 --> 00:39:00.440]   So that's such a small,
[00:39:00.440 --> 00:39:04.200]   short exploration of consciousness that I'm afraid.
[00:39:04.200 --> 00:39:05.920]   And then you just with ex machina say,
[00:39:05.920 --> 00:39:08.200]   "Okay, we're gonna magnify that part
[00:39:08.200 --> 00:39:09.680]   and then minimize the other part."
[00:39:09.680 --> 00:39:12.280]   So that's a good way to sort of compare the two.
[00:39:12.280 --> 00:39:15.520]   But if you could just use your imagination
[00:39:15.520 --> 00:39:20.520]   and if Ava sort of, I don't know,
[00:39:22.160 --> 00:39:25.880]   ran the, was president of the United States.
[00:39:25.880 --> 00:39:26.800]   So had some power.
[00:39:26.800 --> 00:39:29.520]   So what kind of world would she want to create?
[00:39:29.520 --> 00:39:32.020]   If we, as you kind of say, good.
[00:39:32.020 --> 00:39:36.880]   And there is a sense that she has a really,
[00:39:36.880 --> 00:39:41.880]   like there's a desire for a better human to human interaction
[00:39:41.880 --> 00:39:44.480]   human to robot interaction in her.
[00:39:44.480 --> 00:39:47.040]   But what kind of world do you think she would create
[00:39:47.040 --> 00:39:48.400]   with that desire?
[00:39:48.400 --> 00:39:49.560]   - So that's a really,
[00:39:49.560 --> 00:39:52.120]   that's a very interesting question that.
[00:39:52.120 --> 00:39:54.400]   I'm gonna approach it slightly obliquely,
[00:39:54.400 --> 00:39:59.400]   which is that if a friend of yours got stabbed in a mugging
[00:39:59.400 --> 00:40:04.280]   and you then felt very angry
[00:40:04.280 --> 00:40:06.280]   at the person who'd done the stabbing,
[00:40:06.280 --> 00:40:09.120]   but then you learned that it was a 15 year old
[00:40:09.120 --> 00:40:10.640]   and the 15 year old,
[00:40:10.640 --> 00:40:12.800]   both their parents were addicted to crystal meth
[00:40:12.800 --> 00:40:15.560]   and the kid had been addicted since he was 10
[00:40:15.560 --> 00:40:17.720]   and he really never had any hope in the world.
[00:40:17.720 --> 00:40:20.160]   And he'd been driven crazy by his upbringing.
[00:40:20.160 --> 00:40:25.160]   And did the stabbing that would hugely modify.
[00:40:25.160 --> 00:40:27.800]   And it would also make you wary about that kid
[00:40:27.800 --> 00:40:29.920]   then becoming president of America.
[00:40:29.920 --> 00:40:32.120]   And Ava has had a very,
[00:40:32.120 --> 00:40:35.440]   very distorted introduction into the world.
[00:40:35.440 --> 00:40:40.440]   So although there's nothing as it were organically
[00:40:40.440 --> 00:40:45.240]   within Ava that would lean her towards badness,
[00:40:45.240 --> 00:40:49.620]   it's not that robots or sentient robots are bad.
[00:40:49.620 --> 00:40:53.560]   She did not, her arrival into the world
[00:40:53.560 --> 00:40:54.960]   was being imprisoned by humans.
[00:40:54.960 --> 00:40:58.880]   So I'm not sure she'd be a great president.
[00:40:58.880 --> 00:41:02.480]   - Yeah, the trajectory through which she arrived
[00:41:02.480 --> 00:41:07.160]   at her moral views have some dark elements.
[00:41:07.160 --> 00:41:09.680]   - But I like Ava, personally, I like Ava.
[00:41:09.680 --> 00:41:10.960]   - Would you vote for her?
[00:41:10.960 --> 00:41:16.520]   - I'm having difficulty finding anyone to vote for
[00:41:16.520 --> 00:41:18.920]   in my country or if I lived here in yours.
[00:41:18.920 --> 00:41:20.600]   - I, I, um.
[00:41:20.600 --> 00:41:23.000]   - So that's a yes, I guess, because the competition.
[00:41:23.000 --> 00:41:24.340]   - She could easily do a better job
[00:41:24.340 --> 00:41:27.000]   than any of the people we've got around at the moment.
[00:41:27.000 --> 00:41:27.840]   - Okay.
[00:41:27.840 --> 00:41:29.140]   - I'd vote for her over Boris Johnson.
[00:41:29.140 --> 00:41:32.100]   (laughing)
[00:41:32.100 --> 00:41:36.140]   - So what is a good test of consciousness?
[00:41:36.140 --> 00:41:38.860]   Just, we talk about consciousness a little bit more.
[00:41:38.860 --> 00:41:42.180]   If something appears conscious, is it conscious?
[00:41:42.180 --> 00:41:45.880]   You mentioned the smile,
[00:41:45.880 --> 00:41:49.780]   which is, seems to be something done.
[00:41:49.780 --> 00:41:51.500]   I mean, that's a really good indication
[00:41:51.500 --> 00:41:54.340]   because it's a tree falling in the forest
[00:41:54.340 --> 00:41:56.540]   with nobody there to hear it.
[00:41:56.540 --> 00:41:59.660]   But does the appearance from a robotics perspective
[00:41:59.660 --> 00:42:02.140]   of consciousness mean consciousness to you?
[00:42:02.140 --> 00:42:04.900]   - No, I don't think you could say that fully
[00:42:04.900 --> 00:42:07.020]   because I think you could then easily
[00:42:07.020 --> 00:42:09.180]   have a thought experiment which said,
[00:42:09.180 --> 00:42:12.100]   we will create something which we know is not conscious
[00:42:12.100 --> 00:42:15.260]   but is going to give a very, very good account
[00:42:15.260 --> 00:42:16.260]   of seeming conscious.
[00:42:16.260 --> 00:42:19.640]   And so, and also it would be a particularly bad test
[00:42:19.640 --> 00:42:23.060]   where humans are involved because humans are so quick
[00:42:23.060 --> 00:42:28.060]   to project sentience into things that don't have sentience.
[00:42:28.060 --> 00:42:31.300]   So someone could have their computer playing up
[00:42:31.300 --> 00:42:33.980]   and feel as if their computer is being malevolent to them
[00:42:33.980 --> 00:42:34.940]   when it clearly isn't.
[00:42:34.940 --> 00:42:39.900]   And so of all the things to judge consciousness,
[00:42:39.900 --> 00:42:42.860]   us, we're empathy machines.
[00:42:42.860 --> 00:42:45.740]   - So the flip side of that, the argument there
[00:42:45.740 --> 00:42:48.820]   is because we just attribute consciousness
[00:42:48.820 --> 00:42:52.340]   to everything almost, anthropomorphize everything,
[00:42:52.340 --> 00:42:57.340]   including Roombas, that maybe consciousness is not real.
[00:42:57.340 --> 00:43:00.100]   That we just attribute consciousness to each other.
[00:43:00.100 --> 00:43:03.020]   So you have a sense that there is something really special
[00:43:03.020 --> 00:43:07.400]   going on in our mind that makes us unique
[00:43:07.400 --> 00:43:10.100]   and gives us subjective experience.
[00:43:10.100 --> 00:43:13.900]   There's something very interesting going on in our minds.
[00:43:13.900 --> 00:43:16.760]   I'm slightly worried about the word special
[00:43:16.760 --> 00:43:20.740]   because it gets a bit, it nudges towards metaphysics
[00:43:20.740 --> 00:43:23.060]   and maybe even magic.
[00:43:23.060 --> 00:43:25.820]   I mean, in some ways, something magic-like,
[00:43:25.820 --> 00:43:29.340]   which I don't think is there at all.
[00:43:29.340 --> 00:43:30.300]   - I mean, if you think about,
[00:43:30.300 --> 00:43:33.040]   so there's an idea called panpsychism
[00:43:33.040 --> 00:43:34.940]   that says consciousness is in everything.
[00:43:34.940 --> 00:43:36.980]   - Yeah, I don't buy that, I don't buy that.
[00:43:36.980 --> 00:43:39.980]   Yeah, so the idea that there is a thing
[00:43:39.980 --> 00:43:42.460]   that it would be like to be the sun.
[00:43:42.460 --> 00:43:43.540]   - Yes. - Yeah, no.
[00:43:43.540 --> 00:43:44.900]   I don't buy that.
[00:43:44.900 --> 00:43:46.800]   I think that consciousness is a thing.
[00:43:46.800 --> 00:43:51.900]   My sort of broad modification is that usually
[00:43:51.900 --> 00:43:54.580]   the more I find out about things,
[00:43:54.580 --> 00:43:59.580]   the more illusory our instinct is
[00:43:59.580 --> 00:44:03.020]   and is leading us into a different direction
[00:44:03.020 --> 00:44:04.860]   about what that thing actually is.
[00:44:04.860 --> 00:44:07.700]   That happens, it seems to me, in modern science,
[00:44:07.700 --> 00:44:09.160]   that happens a hell of a lot.
[00:44:10.040 --> 00:44:13.440]   Whether it's to do with even how big or small things are.
[00:44:13.440 --> 00:44:16.760]   So my sense is that consciousness is a thing,
[00:44:16.760 --> 00:44:18.720]   but it isn't quite the thing,
[00:44:18.720 --> 00:44:20.260]   or maybe very different from the thing
[00:44:20.260 --> 00:44:22.300]   that we instinctively think it is.
[00:44:22.300 --> 00:44:24.640]   So it's there, it's very interesting,
[00:44:24.640 --> 00:44:28.960]   but we may be in sort of quite fundamentally
[00:44:28.960 --> 00:44:32.520]   misunderstanding it for reasons that are based on intuition.
[00:44:32.520 --> 00:44:35.280]   - So I have to ask,
[00:44:35.280 --> 00:44:37.460]   this is kind of an interesting question.
[00:44:38.460 --> 00:44:42.100]   The Ex Machina, for many people, including myself,
[00:44:42.100 --> 00:44:44.540]   is one of the greatest AI films ever made.
[00:44:44.540 --> 00:44:45.700]   - Wow. - It's number two for me.
[00:44:45.700 --> 00:44:46.540]   - Thanks.
[00:44:46.540 --> 00:44:48.380]   Yeah, it's definitely not number one.
[00:44:48.380 --> 00:44:50.580]   If it was number one, I'd really have to, anyway, yeah.
[00:44:50.580 --> 00:44:52.300]   - Whenever you grow up with something, right?
[00:44:52.300 --> 00:44:55.460]   You may have grew up with something, it's in the blood.
[00:44:55.460 --> 00:45:01.020]   But there's, one of the things that people bring up,
[00:45:01.020 --> 00:45:04.220]   and can't please everyone, including myself,
[00:45:04.220 --> 00:45:06.540]   this is what I first reacted to the film,
[00:45:06.540 --> 00:45:09.460]   is the idea of the lone genius.
[00:45:09.460 --> 00:45:12.700]   This is the criticism that people say,
[00:45:12.700 --> 00:45:14.500]   sort of, me as an AI researcher,
[00:45:14.500 --> 00:45:18.820]   I'm trying to create what Nathan is trying to do.
[00:45:18.820 --> 00:45:23.140]   So there's a brilliant series called "Chernobyl."
[00:45:23.140 --> 00:45:26.060]   - Yes, it's fantastic, absolutely spectacular.
[00:45:26.060 --> 00:45:30.060]   - I mean, they got so many things brilliantly right.
[00:45:30.060 --> 00:45:32.580]   But one of the things, again, the criticism there--
[00:45:32.580 --> 00:45:34.780]   - Yeah, they conflated lots of people into one.
[00:45:34.780 --> 00:45:37.820]   - Into one character that represents all nuclear scientists,
[00:45:37.820 --> 00:45:39.940]   Ioana Komiak.
[00:45:39.940 --> 00:45:46.020]   It's a composite character that presents all scientists.
[00:45:46.020 --> 00:45:47.420]   Is this what you were,
[00:45:47.420 --> 00:45:49.260]   is this the way you were thinking about that,
[00:45:49.260 --> 00:45:51.620]   or is it just simplifies the storytelling?
[00:45:51.620 --> 00:45:53.580]   How do you think about the lone genius?
[00:45:53.580 --> 00:45:55.340]   - Well, I'd say this.
[00:45:55.340 --> 00:45:58.020]   The series I'm doing at the moment is a critique
[00:45:58.020 --> 00:46:01.580]   in part of the lone genius concept.
[00:46:01.580 --> 00:46:03.820]   So yes, I'm sort of oppositional,
[00:46:03.820 --> 00:46:08.180]   and either agnostic or atheistic about that as a concept.
[00:46:08.180 --> 00:46:11.340]   I mean, not entirely, you know.
[00:46:11.340 --> 00:46:15.780]   Whether lone, lone is the right word, broadly isolated,
[00:46:15.780 --> 00:46:20.780]   but Newton clearly exists in a sort of bubble of himself
[00:46:20.780 --> 00:46:22.900]   in some respects, so does Shakespeare.
[00:46:22.900 --> 00:46:24.220]   - So do you think we would have an iPhone
[00:46:24.220 --> 00:46:25.620]   without Steve Jobs?
[00:46:25.620 --> 00:46:27.580]   I mean, how much contribution from a genius?
[00:46:27.580 --> 00:46:29.700]   - Well, no, but Steve Jobs clearly isn't a lone genius,
[00:46:29.700 --> 00:46:32.100]   because there's too many other people
[00:46:32.100 --> 00:46:33.780]   in the sort of superstructure around him
[00:46:33.780 --> 00:46:38.220]   who are absolutely fundamental to that journey.
[00:46:38.220 --> 00:46:40.340]   - But you're saying Newton, but that's a scientific,
[00:46:40.340 --> 00:46:44.100]   so there's an engineering element to building Ava.
[00:46:44.100 --> 00:46:48.620]   - But just to say, what Ex Machina is really,
[00:46:48.620 --> 00:46:50.260]   it's a thought experiment.
[00:46:50.260 --> 00:46:52.300]   I mean, so it's a construction
[00:46:52.300 --> 00:46:55.740]   of putting four people in a house.
[00:46:55.740 --> 00:47:00.260]   Nothing about Ex Machina adds up in all sorts of ways,
[00:47:00.260 --> 00:47:03.620]   in as much as who built the machine parts,
[00:47:03.620 --> 00:47:05.380]   did the people building the machine parts
[00:47:05.380 --> 00:47:08.980]   know what they were creating, and how did they get there?
[00:47:08.980 --> 00:47:11.140]   And it's a thought experiment.
[00:47:11.140 --> 00:47:14.780]   So it doesn't stand up to scrutiny of that sort.
[00:47:14.780 --> 00:47:16.540]   - I don't think it's actually that interesting
[00:47:16.540 --> 00:47:20.660]   of a question, but it's brought up so often
[00:47:20.660 --> 00:47:23.560]   that I had to ask it, because that's exactly
[00:47:23.560 --> 00:47:25.620]   how I felt after a while.
[00:47:25.620 --> 00:47:30.180]   There's something about, there was almost a,
[00:47:30.180 --> 00:47:33.100]   like I watched your movie the first time,
[00:47:33.100 --> 00:47:36.140]   at least for the first little while, in a defensive way.
[00:47:36.140 --> 00:47:40.740]   Like how dare this person try to step into the AI space
[00:47:40.740 --> 00:47:43.620]   and try to beat Kubrick.
[00:47:43.620 --> 00:47:45.340]   That's the way I was thinking,
[00:47:45.340 --> 00:47:48.260]   'cause it comes off as a movie that really is going
[00:47:48.260 --> 00:47:51.020]   after the deep fundamental questions about AI.
[00:47:51.020 --> 00:47:53.780]   So there's a kind of a, nerds do this,
[00:47:53.780 --> 00:47:57.280]   I guess, automatically searching for the flaws.
[00:47:57.280 --> 00:47:58.120]   And I just--
[00:47:58.120 --> 00:48:00.260]   - I do exactly the same.
[00:48:00.260 --> 00:48:03.820]   - I think in "Annihilation" and the other movie,
[00:48:03.820 --> 00:48:06.340]   I was able to free myself from that much quicker.
[00:48:06.340 --> 00:48:08.460]   That it is a thought experiment.
[00:48:08.460 --> 00:48:11.020]   There's, you know, who cares if there's batteries
[00:48:11.020 --> 00:48:12.060]   that don't run out, right?
[00:48:12.060 --> 00:48:13.220]   Those kinds of questions.
[00:48:13.220 --> 00:48:14.660]   That's the whole point.
[00:48:14.660 --> 00:48:18.620]   But it's nevertheless something I wanted to bring up.
[00:48:18.620 --> 00:48:20.860]   - Yeah, it's a fair thing to bring up.
[00:48:20.860 --> 00:48:24.280]   For me, you hit on the lone genius thing.
[00:48:24.280 --> 00:48:27.100]   For me, it was actually, people always said,
[00:48:27.100 --> 00:48:29.660]   "Hex Machina makes this big leap
[00:48:29.660 --> 00:48:32.140]   in terms of where AI has got to,
[00:48:32.140 --> 00:48:36.220]   and also what AI would look like if it got to that point."
[00:48:36.220 --> 00:48:38.620]   There's another one, which is just robotics.
[00:48:38.620 --> 00:48:42.060]   I mean, look at the way Ava walks around a room.
[00:48:42.060 --> 00:48:43.860]   It's like, forget it, building that.
[00:48:43.860 --> 00:48:47.860]   That's also got to be a very, very long way off.
[00:48:47.860 --> 00:48:48.700]   And if you did get there,
[00:48:48.700 --> 00:48:49.940]   would it look anything like that?
[00:48:49.940 --> 00:48:50.860]   It's a thought experiment.
[00:48:50.860 --> 00:48:52.060]   - Actually, I disagree with you.
[00:48:52.060 --> 00:48:56.620]   I think the way Isabella Arena, Alicia Vikander,
[00:48:56.620 --> 00:49:00.360]   brilliant actress, actor, that moves around,
[00:49:00.360 --> 00:49:03.540]   we're very far away from creating that,
[00:49:03.540 --> 00:49:04.780]   but the way she moves around
[00:49:04.780 --> 00:49:08.700]   is exactly the definition of perfection for a roboticist.
[00:49:08.700 --> 00:49:10.060]   It's smooth and efficient.
[00:49:10.060 --> 00:49:12.740]   So it is where we want to get, I believe.
[00:49:12.740 --> 00:49:15.180]   Like, I think, so I hang out
[00:49:15.180 --> 00:49:17.000]   with a lot of humanoid robotics people.
[00:49:17.000 --> 00:49:20.500]   They love elegant, smooth motion like that.
[00:49:20.500 --> 00:49:21.620]   That's their dream.
[00:49:21.620 --> 00:49:23.660]   So the way she moved is actually what I believe
[00:49:23.660 --> 00:49:25.980]   they would dream for a robot to move.
[00:49:25.980 --> 00:49:29.540]   It might not be that useful to move that sort of that way,
[00:49:29.540 --> 00:49:32.220]   but that is the definition of perfection
[00:49:32.220 --> 00:49:33.280]   in terms of movement.
[00:49:33.280 --> 00:49:35.960]   Drawing inspiration from real life.
[00:49:35.960 --> 00:49:39.540]   So for devs, for ex machina,
[00:49:39.540 --> 00:49:42.580]   look at characters like Elon Musk.
[00:49:42.580 --> 00:49:45.340]   What do you think about the various big technological efforts
[00:49:45.340 --> 00:49:48.540]   of Elon Musk and others like him
[00:49:48.540 --> 00:49:53.260]   that he's involved with, such as Tesla, SpaceX, Neuralink?
[00:49:53.260 --> 00:49:55.220]   Do you see any of that technology
[00:49:55.220 --> 00:49:57.100]   potentially defining the future worlds
[00:49:57.100 --> 00:49:58.540]   you create in your work?
[00:49:58.540 --> 00:50:02.660]   So Tesla is automation, SpaceX is space exploration,
[00:50:02.660 --> 00:50:05.300]   Neuralink is brain machine interface,
[00:50:05.300 --> 00:50:09.860]   somehow a merger of biological and electric systems.
[00:50:09.860 --> 00:50:12.580]   - I'm, in a way, I'm influenced by that
[00:50:12.580 --> 00:50:15.460]   almost by definition because that's the world I live in,
[00:50:15.460 --> 00:50:17.900]   and this is the thing that's happening in that world.
[00:50:17.900 --> 00:50:20.080]   And I also feel supportive of it.
[00:50:20.080 --> 00:50:24.660]   So I think amongst various things,
[00:50:24.660 --> 00:50:28.700]   Elon Musk has done, I'm almost sure he's done
[00:50:28.700 --> 00:50:32.180]   a very, very good thing with Tesla for all of us.
[00:50:32.180 --> 00:50:36.220]   It's really kicked all the other car manufacturers
[00:50:36.220 --> 00:50:39.820]   in the face, it's kicked the fossil fuel industry
[00:50:39.820 --> 00:50:42.340]   in the face, and they needed kicking in the face,
[00:50:42.340 --> 00:50:43.180]   and he's done it.
[00:50:43.180 --> 00:50:47.980]   So, and so that's the world he's part of creating,
[00:50:47.980 --> 00:50:49.360]   and I live in that world.
[00:50:49.360 --> 00:50:51.940]   Just bought a Tesla, in fact.
[00:50:51.940 --> 00:50:56.940]   And so does that play into whatever I then make?
[00:50:56.940 --> 00:51:02.540]   In some ways it does, partly because I try to be a writer
[00:51:02.540 --> 00:51:07.080]   who quite often filmmakers are in some ways fixated
[00:51:07.080 --> 00:51:09.020]   on the films they grew up with,
[00:51:09.020 --> 00:51:11.660]   and they sort of remake those films in some ways.
[00:51:11.660 --> 00:51:13.300]   I've always tried to avoid that.
[00:51:13.300 --> 00:51:17.740]   And so I look to the real world to get inspiration,
[00:51:17.740 --> 00:51:21.380]   and as much as possible, sort of by living, I think.
[00:51:21.380 --> 00:51:24.420]   And so, yeah, I'm sure.
[00:51:24.420 --> 00:51:28.260]   - Which of the directions do you find most exciting?
[00:51:28.260 --> 00:51:29.260]   - Space travel.
[00:51:29.260 --> 00:51:31.540]   - Space travel.
[00:51:31.540 --> 00:51:36.180]   So you haven't really explored space travel in your work.
[00:51:36.180 --> 00:51:39.740]   You've said something like if you had unlimited amount
[00:51:39.740 --> 00:51:42.180]   of money, I think I read it at AMA,
[00:51:42.180 --> 00:51:45.820]   that you would make a multi-year series of space wars
[00:51:45.820 --> 00:51:47.100]   or something like that.
[00:51:47.100 --> 00:51:50.740]   So what is it that excites you about space exploration?
[00:51:50.740 --> 00:51:55.740]   - Well, because if we have any sort of long-term future,
[00:51:55.740 --> 00:51:56.900]   it's that.
[00:51:56.900 --> 00:52:00.220]   It just simply is that.
[00:52:00.220 --> 00:52:03.860]   If energy and matter are linked up
[00:52:03.860 --> 00:52:05.920]   in the way we think they're linked up,
[00:52:05.920 --> 00:52:09.500]   we'll run out if we don't move.
[00:52:09.500 --> 00:52:10.660]   So we gotta move.
[00:52:10.660 --> 00:52:15.900]   But also, how can we not?
[00:52:16.820 --> 00:52:21.380]   It's built into us to do it or die trying.
[00:52:21.380 --> 00:52:26.380]   I was on Easter Island a few months ago,
[00:52:26.380 --> 00:52:29.180]   which is, as I'm sure you know,
[00:52:29.180 --> 00:52:30.220]   in the middle of the Pacific
[00:52:30.220 --> 00:52:32.860]   and difficult for people to have got to,
[00:52:32.860 --> 00:52:34.020]   but they got there.
[00:52:34.020 --> 00:52:37.260]   And I did think a lot about the way those boats
[00:52:37.260 --> 00:52:42.100]   must have set out into something like space.
[00:52:42.100 --> 00:52:43.260]   It was the ocean.
[00:52:44.740 --> 00:52:49.740]   And how sort of fundamental that was to the way we are.
[00:52:49.740 --> 00:52:53.700]   And it's the one that most excites me
[00:52:53.700 --> 00:52:55.700]   because it's the one I want most to happen.
[00:52:55.700 --> 00:52:57.620]   It's the thing, it's the place
[00:52:57.620 --> 00:52:59.660]   where we could get to as humans.
[00:52:59.660 --> 00:53:03.620]   Like in a way, I could live with us never really unlocking,
[00:53:03.620 --> 00:53:06.020]   fully unlocking the nature of consciousness.
[00:53:06.020 --> 00:53:09.140]   I'd like to know, I'm really curious.
[00:53:09.140 --> 00:53:12.020]   But if we never leave the solar system
[00:53:12.020 --> 00:53:14.300]   and if we never get further out into this galaxy,
[00:53:14.300 --> 00:53:16.940]   or maybe even galaxies beyond our galaxy,
[00:53:16.940 --> 00:53:20.100]   that would, that feels sad to me
[00:53:20.100 --> 00:53:24.540]   because it's so limiting.
[00:53:24.540 --> 00:53:26.900]   - Yeah, there's something hopeful and beautiful
[00:53:26.900 --> 00:53:30.180]   about reaching out, any kind of exploration,
[00:53:30.180 --> 00:53:33.380]   reaching out across earth centuries ago
[00:53:33.380 --> 00:53:35.220]   and then reaching out into space.
[00:53:35.220 --> 00:53:37.140]   So what do you think about colonization of Mars?
[00:53:37.140 --> 00:53:37.980]   So go to Mars.
[00:53:37.980 --> 00:53:39.780]   Does that excite you, the idea of a human being
[00:53:39.780 --> 00:53:41.340]   stepping foot on Mars?
[00:53:41.340 --> 00:53:42.260]   - It does.
[00:53:42.260 --> 00:53:43.260]   It absolutely does.
[00:53:43.260 --> 00:53:45.340]   But in terms of what would really excite me,
[00:53:45.340 --> 00:53:47.180]   it would be leaving the solar system.
[00:53:47.180 --> 00:53:49.060]   In as much as that I just think,
[00:53:49.060 --> 00:53:52.460]   I think we already know quite a lot about Mars.
[00:53:52.460 --> 00:53:56.340]   But yes, listen, if it happened, that would be,
[00:53:56.340 --> 00:53:59.020]   I hope I see it in my lifetime.
[00:53:59.020 --> 00:54:01.100]   I really hope I see it in my lifetime.
[00:54:01.100 --> 00:54:02.740]   So it would be a wonderful thing.
[00:54:02.740 --> 00:54:05.460]   - Without giving anything away,
[00:54:05.460 --> 00:54:10.460]   but the series begins with the use of quantum computers.
[00:54:11.260 --> 00:54:13.900]   The new series does, begins with the use
[00:54:13.900 --> 00:54:17.100]   of quantum computers to simulate basic living organisms.
[00:54:17.100 --> 00:54:18.940]   Or actually, I don't know if it's quantum computers
[00:54:18.940 --> 00:54:21.620]   they use, but basic living organisms
[00:54:21.620 --> 00:54:22.820]   simulated on a screen.
[00:54:22.820 --> 00:54:24.340]   It's a really cool kind of demo.
[00:54:24.340 --> 00:54:25.180]   - Yeah, that's right.
[00:54:25.180 --> 00:54:28.220]   They're using, yes, they are using a quantum computer
[00:54:28.220 --> 00:54:31.700]   to simulate a nematode, yeah.
[00:54:31.700 --> 00:54:34.820]   - So returning to our discussion of simulation
[00:54:34.820 --> 00:54:38.780]   or thinking of the universe as a computer,
[00:54:38.780 --> 00:54:41.220]   do you think the universe is deterministic?
[00:54:41.220 --> 00:54:42.460]   Is there a free will?
[00:54:42.460 --> 00:54:46.780]   - So with the qualification of what do I know?
[00:54:46.780 --> 00:54:48.100]   'Cause I'm a layman, right?
[00:54:48.100 --> 00:54:49.820]   Lay person, but--
[00:54:49.820 --> 00:54:51.660]   - With big imagination.
[00:54:51.660 --> 00:54:52.540]   - Thanks.
[00:54:52.540 --> 00:54:55.060]   With that qualification, yeah,
[00:54:55.060 --> 00:54:56.860]   I think the universe is deterministic
[00:54:56.860 --> 00:55:00.620]   and I see absolutely, I cannot see
[00:55:00.620 --> 00:55:02.340]   how free will fits into that.
[00:55:02.340 --> 00:55:05.100]   So yes, deterministic, no free will.
[00:55:05.100 --> 00:55:07.180]   That would be my position.
[00:55:07.180 --> 00:55:09.460]   - And how does that make you feel?
[00:55:09.460 --> 00:55:12.380]   - It partly makes me feel that it's exactly in keeping
[00:55:12.380 --> 00:55:14.420]   with the way these things tend to work out,
[00:55:14.420 --> 00:55:17.140]   which is that we have an incredibly strong sense
[00:55:17.140 --> 00:55:18.660]   that we do have free will.
[00:55:18.660 --> 00:55:24.300]   And just as we have an incredibly strong sense
[00:55:24.300 --> 00:55:26.180]   that time is a constant
[00:55:26.180 --> 00:55:30.100]   and turns out probably not to be the case,
[00:55:30.100 --> 00:55:31.860]   or definitely in the case of time.
[00:55:31.860 --> 00:55:37.940]   The problem I always have with free will is that it gets,
[00:55:37.940 --> 00:55:40.500]   I can never seem to find the place
[00:55:40.500 --> 00:55:43.020]   where it is supposed to reside.
[00:55:43.020 --> 00:55:45.460]   - And yet you explore--
[00:55:45.460 --> 00:55:46.820]   - Just a bit of very, very,
[00:55:46.820 --> 00:55:49.660]   but we have something we can call free will,
[00:55:49.660 --> 00:55:51.940]   but it's not the thing that we think it is.
[00:55:51.940 --> 00:55:55.540]   - But free will, so do you, what we call free will is just--
[00:55:55.540 --> 00:55:56.940]   - Is what we call it is the illusion of it.
[00:55:56.940 --> 00:55:59.660]   - It's a subjective experience of the illusion.
[00:55:59.660 --> 00:56:01.620]   - Yeah, which is a useful thing to have.
[00:56:01.620 --> 00:56:04.460]   And it partly comes down to,
[00:56:04.460 --> 00:56:06.820]   although we live in a deterministic universe,
[00:56:06.820 --> 00:56:08.500]   our brains are not very well equipped
[00:56:08.500 --> 00:56:11.140]   to fully determine the deterministic universe,
[00:56:11.140 --> 00:56:12.820]   so we're constantly surprised
[00:56:12.820 --> 00:56:15.580]   and feel like we're making snap decisions
[00:56:15.580 --> 00:56:17.500]   based on imperfect information,
[00:56:17.500 --> 00:56:19.940]   so that feels a lot like free will.
[00:56:19.940 --> 00:56:21.260]   It just isn't.
[00:56:21.260 --> 00:56:24.180]   Would be my, that's my guess.
[00:56:24.180 --> 00:56:27.060]   - So in that sense, your sort of sense
[00:56:27.060 --> 00:56:30.780]   is that you can unroll the universe forward or backward
[00:56:30.780 --> 00:56:33.340]   and you will see the same thing.
[00:56:33.340 --> 00:56:36.700]   And you, I mean, that notion--
[00:56:36.700 --> 00:56:40.300]   - Yeah, sort of, sort of, but yeah, sorry, go ahead.
[00:56:40.300 --> 00:56:45.300]   - I mean, that notion is a bit uncomfortable to think about
[00:56:45.300 --> 00:56:50.940]   that it's, you can roll it back and forward and--
[00:56:50.940 --> 00:56:55.060]   - Well, if you were able to do it,
[00:56:55.060 --> 00:56:58.140]   it would certainly have to be a quantum computer,
[00:56:58.140 --> 00:57:00.940]   something that worked in a quantum mechanical way
[00:57:00.940 --> 00:57:05.940]   in order to understand a quantum mechanical system, I guess.
[00:57:06.260 --> 00:57:07.660]   But--
[00:57:07.660 --> 00:57:09.980]   - And so that unrolling, there might be a multiverse thing
[00:57:09.980 --> 00:57:11.220]   where there's a bunch of branching--
[00:57:11.220 --> 00:57:13.420]   - Well, exactly, because it wouldn't follow
[00:57:13.420 --> 00:57:15.540]   that every time you roll it back or forward,
[00:57:15.540 --> 00:57:17.980]   you'd get exactly the same result.
[00:57:17.980 --> 00:57:21.420]   - Which is another thing that's hard to wrap my mind around.
[00:57:21.420 --> 00:57:24.660]   So-- - Yeah, but that, yes,
[00:57:24.660 --> 00:57:27.260]   but essentially what you just described, that,
[00:57:27.260 --> 00:57:29.700]   the yes forwards and yes backwards,
[00:57:29.700 --> 00:57:31.860]   but you might get a slightly different result,
[00:57:31.860 --> 00:57:33.380]   or a very different result.
[00:57:33.380 --> 00:57:34.500]   - Or very different.
[00:57:34.500 --> 00:57:36.460]   - Along the same lines, you've explored
[00:57:36.460 --> 00:57:39.820]   some really deep scientific ideas in this new series.
[00:57:39.820 --> 00:57:42.540]   And I mean, just in general, you're unafraid
[00:57:42.540 --> 00:57:47.020]   to ground yourself in some of the most amazing
[00:57:47.020 --> 00:57:49.460]   scientific ideas of our time.
[00:57:49.460 --> 00:57:51.420]   What are the things you've learned,
[00:57:51.420 --> 00:57:53.540]   or ideas you find beautiful and mysterious
[00:57:53.540 --> 00:57:55.900]   about quantum mechanics, multiverse, string theory,
[00:57:55.900 --> 00:57:58.140]   quantum computing that you've learned?
[00:57:58.140 --> 00:58:01.940]   - Well, I would have to say every single thing I've learned
[00:58:01.940 --> 00:58:05.540]   is beautiful, and one of the motivators for me
[00:58:05.540 --> 00:58:10.540]   is that I think that people tend not to see
[00:58:10.540 --> 00:58:17.100]   scientific thinking as being essentially poetic and lyrical.
[00:58:17.100 --> 00:58:20.860]   But I think that is literally exactly what it is.
[00:58:20.860 --> 00:58:23.940]   And I think the idea of entanglement,
[00:58:23.940 --> 00:58:25.780]   or the idea of superpositions,
[00:58:25.780 --> 00:58:28.220]   or the fact that you could even demonstrate a superposition,
[00:58:28.220 --> 00:58:31.220]   or have a machine that relies on the existence
[00:58:31.220 --> 00:58:33.540]   of superpositions in order to function,
[00:58:33.540 --> 00:58:36.940]   to me is almost indescribably beautiful.
[00:58:36.940 --> 00:58:42.420]   It fills me with awe, it fills me with awe.
[00:58:42.420 --> 00:58:47.420]   And also, it's not just a sort of grand, massive awe,
[00:58:47.420 --> 00:58:51.460]   but it's also delicate.
[00:58:51.460 --> 00:58:54.180]   It's very, very delicate and subtle.
[00:58:54.180 --> 00:58:59.180]   And it has these beautiful nuances in it,
[00:58:59.940 --> 00:59:03.500]   and also these completely paradigm-changing
[00:59:03.500 --> 00:59:04.460]   thoughts and truths.
[00:59:04.460 --> 00:59:08.740]   So it's as good as it gets, as far as I can tell.
[00:59:08.740 --> 00:59:10.940]   So broadly, everything.
[00:59:10.940 --> 00:59:12.900]   That doesn't mean I believe everything I read
[00:59:12.900 --> 00:59:15.860]   in quantum physics, because obviously,
[00:59:15.860 --> 00:59:18.300]   a lot of the interpretations are completely in conflict
[00:59:18.300 --> 00:59:22.380]   with each other, and who knows whether string theory
[00:59:22.380 --> 00:59:25.060]   will turn out to be a good description or not.
[00:59:25.060 --> 00:59:29.140]   But the beauty in it, it seems undeniable.
[00:59:29.140 --> 00:59:34.140]   And I do wish people more readily understood
[00:59:34.140 --> 00:59:40.500]   how beautiful and poetic science is, I would say.
[00:59:40.500 --> 00:59:43.100]   - Science is poetry.
[00:59:43.100 --> 00:59:48.900]   In terms of quantum computing being used
[00:59:48.900 --> 00:59:52.940]   to simulate things, or just in general,
[00:59:52.940 --> 00:59:56.780]   the idea of simulating small parts of our world,
[00:59:56.780 --> 00:59:59.820]   which actually current physicists are really excited
[00:59:59.820 --> 01:00:02.740]   about simulating small quantum mechanical systems
[01:00:02.740 --> 01:00:04.860]   on quantum computers, but scaling that up
[01:00:04.860 --> 01:00:07.260]   to something bigger, like simulating life forms.
[01:00:07.260 --> 01:00:11.380]   How do you think, what are the possible trajectories
[01:00:11.380 --> 01:00:14.260]   of that going wrong or going right,
[01:00:14.260 --> 01:00:16.660]   if you unroll that into the future?
[01:00:16.660 --> 01:00:21.260]   - Well, if a bit like Ava and her robotics,
[01:00:21.260 --> 01:00:26.260]   you park the sheer complexity of what you're trying to do.
[01:00:26.860 --> 01:00:31.860]   The issues are, I think it will have a profound,
[01:00:31.860 --> 01:00:38.020]   if you were able to have a machine that was able
[01:00:38.020 --> 01:00:40.620]   to project forwards and backwards accurately,
[01:00:40.620 --> 01:00:42.740]   it would in an empirical way show,
[01:00:42.740 --> 01:00:45.060]   it would demonstrate that you don't have free will.
[01:00:45.060 --> 01:00:47.580]   So the first thing that would happen is people would have
[01:00:47.580 --> 01:00:51.620]   to really take on a very, very different idea
[01:00:51.620 --> 01:00:53.580]   of what they were.
[01:00:53.580 --> 01:00:56.340]   The thing that they truly, truly believe they are,
[01:00:56.340 --> 01:00:57.500]   they are not.
[01:00:57.500 --> 01:01:01.220]   And so that, I suspect, would be very, very disturbing
[01:01:01.220 --> 01:01:02.060]   to a lot of people.
[01:01:02.060 --> 01:01:04.500]   - Do you think that has a positive or negative effect
[01:01:04.500 --> 01:01:08.820]   on society, the realization that you are not,
[01:01:08.820 --> 01:01:11.500]   you cannot control your actions, essentially, I guess,
[01:01:11.500 --> 01:01:13.420]   is the way that could be interpreted?
[01:01:13.420 --> 01:01:16.060]   - Yeah, although in some ways,
[01:01:16.060 --> 01:01:18.020]   we instinctively understand that already,
[01:01:18.020 --> 01:01:20.580]   because in the example I gave you of the kid
[01:01:20.580 --> 01:01:23.020]   in the stabbing, we would all understand
[01:01:23.020 --> 01:01:25.140]   that that kid was not really fully in control
[01:01:25.140 --> 01:01:25.980]   of their actions.
[01:01:25.980 --> 01:01:29.020]   So it's not an idea that's entirely alien to us.
[01:01:29.020 --> 01:01:29.860]   But-- - I don't know
[01:01:29.860 --> 01:01:31.020]   we understand that.
[01:01:31.020 --> 01:01:36.020]   I think there's a bunch of people who see the world that way,
[01:01:36.020 --> 01:01:37.420]   but not everybody.
[01:01:37.420 --> 01:01:38.260]   - Yes, true.
[01:01:38.260 --> 01:01:39.580]   Of course, true.
[01:01:39.580 --> 01:01:43.100]   But what this machine would do is prove it beyond any doubt,
[01:01:43.100 --> 01:01:44.540]   because someone would say,
[01:01:44.540 --> 01:01:45.940]   "Well, I don't believe that's true."
[01:01:45.940 --> 01:01:47.140]   And then you'd predict,
[01:01:47.140 --> 01:01:48.940]   "Well, in 10 seconds, you're gonna do this."
[01:01:48.940 --> 01:01:50.140]   And they'd say, "No, I'm not."
[01:01:50.140 --> 01:01:52.420]   And then they'd do it, and then determinism
[01:01:52.420 --> 01:01:54.340]   would have played its part.
[01:01:54.340 --> 01:01:55.940]   Or something like that.
[01:01:55.940 --> 01:01:59.980]   But actually the exact terms of that thought experiment
[01:01:59.980 --> 01:02:00.980]   probably wouldn't play out.
[01:02:00.980 --> 01:02:03.780]   But still broadly speaking,
[01:02:03.780 --> 01:02:06.100]   you could predict something happening in another room,
[01:02:06.100 --> 01:02:08.300]   sort of unseen, I suppose,
[01:02:08.300 --> 01:02:10.580]   that foreknowledge would not allow you to affect.
[01:02:10.580 --> 01:02:13.260]   So what effect would that have?
[01:02:13.260 --> 01:02:15.460]   I think people would find it very disturbing.
[01:02:15.460 --> 01:02:17.660]   But then after they'd got over their sense
[01:02:17.660 --> 01:02:21.140]   of being disturbed, which by the way,
[01:02:21.140 --> 01:02:22.580]   I don't even think you need a machine
[01:02:22.580 --> 01:02:24.580]   to take this idea on board.
[01:02:24.580 --> 01:02:26.380]   But after they'd got over that,
[01:02:26.380 --> 01:02:28.180]   they'd still understand that even though
[01:02:28.180 --> 01:02:31.260]   I have no free will and my actions are in effect
[01:02:31.260 --> 01:02:35.580]   already determined, I still feel things.
[01:02:35.580 --> 01:02:39.140]   I still care about stuff.
[01:02:39.140 --> 01:02:41.340]   I remember my daughter saying to me,
[01:02:41.340 --> 01:02:46.820]   she'd got hold of the idea that my view of the universe
[01:02:46.820 --> 01:02:48.380]   made it meaningless.
[01:02:48.380 --> 01:02:49.820]   And she said, "Well, then it's meaningless."
[01:02:49.820 --> 01:02:52.500]   And I said, "Well, I can prove it's not meaningless
[01:02:52.500 --> 01:02:55.100]   "because you mean something to me
[01:02:55.100 --> 01:02:56.180]   "and I mean something to you.
[01:02:56.180 --> 01:02:58.140]   "So it's not completely meaningless
[01:02:58.140 --> 01:02:59.860]   "because there is a bit of meaning
[01:02:59.860 --> 01:03:01.420]   "contained within this space."
[01:03:01.420 --> 01:03:06.020]   And so with a lack of free will space,
[01:03:06.020 --> 01:03:08.300]   you could think, well, this robs me of everything I am.
[01:03:08.300 --> 01:03:09.820]   And then you'd say, well, no, it doesn't
[01:03:09.820 --> 01:03:12.020]   because you still like eating cheeseburgers
[01:03:12.020 --> 01:03:13.860]   and you still like going to see the movies.
[01:03:13.860 --> 01:03:17.140]   And so how big a difference does it really make?
[01:03:18.020 --> 01:03:21.260]   But I think initially people would find it very disturbing.
[01:03:21.260 --> 01:03:26.260]   I think that what would come if you could really unlock
[01:03:26.260 --> 01:03:28.660]   with a determinism machine everything,
[01:03:28.660 --> 01:03:31.060]   there'd be this wonderful wisdom that would come from it.
[01:03:31.060 --> 01:03:32.760]   And I'd rather have that than not.
[01:03:32.760 --> 01:03:37.180]   - So that's a really good example of a technology
[01:03:37.180 --> 01:03:39.700]   revealing to us humans something fundamental
[01:03:39.700 --> 01:03:41.740]   about our world, about our society.
[01:03:41.740 --> 01:03:45.700]   So it's almost this creation is helping us
[01:03:45.700 --> 01:03:47.840]   understand ourselves.
[01:03:47.840 --> 01:03:51.460]   The thing you said about artificial intelligence.
[01:03:51.460 --> 01:03:55.740]   So what do you think us creating something like Ava
[01:03:55.740 --> 01:03:58.180]   will help us understand about ourselves?
[01:03:58.180 --> 01:04:00.980]   How will that change society?
[01:04:00.980 --> 01:04:04.200]   - Well, I would hope it would teach us some humility.
[01:04:04.200 --> 01:04:07.440]   Humans are very big on exceptionalism.
[01:04:07.440 --> 01:04:12.860]   America is constantly proclaiming itself
[01:04:12.860 --> 01:04:15.380]   to be the greatest nation on earth,
[01:04:15.380 --> 01:04:18.100]   which it may feel like that if you're an American,
[01:04:18.100 --> 01:04:20.720]   but it may not feel like that if you're from Finland,
[01:04:20.720 --> 01:04:21.820]   because there's all sorts of things
[01:04:21.820 --> 01:04:23.580]   you dearly love about Finland.
[01:04:23.580 --> 01:04:27.380]   And exceptionalism is usually bullshit.
[01:04:27.380 --> 01:04:29.100]   Probably not always.
[01:04:29.100 --> 01:04:31.260]   If we both sat here, we could find a good example
[01:04:31.260 --> 01:04:34.020]   of something that isn't, but as a rule of thumb.
[01:04:34.020 --> 01:04:37.900]   And what it would do is it would teach us some humility
[01:04:37.900 --> 01:04:42.340]   and about, actually often that's what science does
[01:04:42.340 --> 01:04:43.180]   in a funny way.
[01:04:43.180 --> 01:04:44.440]   It makes us more and more interesting,
[01:04:44.440 --> 01:04:46.540]   but it makes us a smaller and smaller part
[01:04:46.540 --> 01:04:48.180]   of the thing that's interesting.
[01:04:48.180 --> 01:04:51.040]   And I don't mind that humility at all.
[01:04:51.040 --> 01:04:53.820]   I don't think it's a bad thing.
[01:04:53.820 --> 01:04:57.380]   Our excesses don't tend to come from humility.
[01:04:57.380 --> 01:05:00.520]   Our excesses come from the opposite, megalomania and stuff.
[01:05:00.520 --> 01:05:03.020]   We tend to think of consciousness
[01:05:03.020 --> 01:05:06.940]   as having some form of exceptionalism attached to it.
[01:05:06.940 --> 01:05:09.380]   I suspect if we ever unravel it,
[01:05:09.380 --> 01:05:13.740]   it will turn out to be less than we thought in a way.
[01:05:13.740 --> 01:05:17.800]   And perhaps your very own exceptionalist assertion
[01:05:17.800 --> 01:05:20.320]   earlier on in our conversation that consciousness
[01:05:20.320 --> 01:05:23.080]   is something belongs to us humans,
[01:05:23.080 --> 01:05:25.380]   or not humans, but living organisms,
[01:05:25.380 --> 01:05:27.720]   maybe you will one day find out
[01:05:27.720 --> 01:05:30.280]   that consciousness is in everything.
[01:05:30.280 --> 01:05:32.880]   And that will humble you.
[01:05:32.880 --> 01:05:35.700]   - If that was true, it would certainly humble me.
[01:05:35.700 --> 01:05:39.080]   Although maybe, almost maybe, I don't know.
[01:05:39.080 --> 01:05:41.120]   I don't know what effect that would have.
[01:05:42.120 --> 01:05:47.120]   I sort of, I mean, my understanding of that principle
[01:05:47.120 --> 01:05:49.260]   is along the lines of say,
[01:05:49.260 --> 01:05:52.580]   that an electron has a preferred state,
[01:05:52.580 --> 01:05:56.620]   or it may or may not pass through a bit of glass.
[01:05:56.620 --> 01:05:58.300]   It may reflect off or it may go through
[01:05:58.300 --> 01:05:59.140]   or something like that.
[01:05:59.140 --> 01:06:03.700]   And so that feels as if a choice has been made.
[01:06:03.700 --> 01:06:10.820]   But if I'm going down the fully deterministic route,
[01:06:10.820 --> 01:06:13.240]   I would say there's just an underlying determinism
[01:06:13.240 --> 01:06:16.680]   that has defined that, that has defined the preferred state
[01:06:16.680 --> 01:06:18.840]   or the reflection or non-reflection.
[01:06:18.840 --> 01:06:19.920]   But look, yeah, you're right.
[01:06:19.920 --> 01:06:22.520]   If it turned out that there was a thing
[01:06:22.520 --> 01:06:23.920]   that it was like to be the sun,
[01:06:23.920 --> 01:06:27.880]   then I'd be amazed and humbled.
[01:06:27.880 --> 01:06:29.000]   And I'd be happy to be both.
[01:06:29.000 --> 01:06:30.040]   That sounds pretty cool.
[01:06:30.040 --> 01:06:31.680]   - And you'll say the same thing
[01:06:31.680 --> 01:06:32.560]   as you said to your daughter,
[01:06:32.560 --> 01:06:35.140]   but it's nevertheless feels something like to be me.
[01:06:35.140 --> 01:06:37.960]   And that's pretty damn good.
[01:06:39.520 --> 01:06:42.140]   So Kubrick created many masterpieces,
[01:06:42.140 --> 01:06:44.060]   including "The Shining," "Doctor Strangelove,"
[01:06:44.060 --> 01:06:46.040]   "Clockwork Orange."
[01:06:46.040 --> 01:06:48.960]   But to me, he will be remembered, I think,
[01:06:48.960 --> 01:06:53.180]   to many 100 years from now for 2001, "A Space Odyssey."
[01:06:53.180 --> 01:06:54.780]   I would say that's his greatest film.
[01:06:54.780 --> 01:06:55.620]   - I agree.
[01:06:55.620 --> 01:07:00.580]   - You are incredibly humble.
[01:07:00.580 --> 01:07:02.580]   I've listened to a bunch of your interviews
[01:07:02.580 --> 01:07:04.940]   and I really appreciate that you're humble
[01:07:04.940 --> 01:07:07.940]   in your creative efforts and your work.
[01:07:07.940 --> 01:07:10.200]   But if I were to force you a gunpoint.
[01:07:10.200 --> 01:07:13.340]   - Do you have a gun?
[01:07:13.340 --> 01:07:15.140]   - You don't know that, the mystery.
[01:07:15.140 --> 01:07:20.120]   To imagine 100 years out into the future,
[01:07:20.120 --> 01:07:23.460]   what will Alex Garland be remembered for
[01:07:23.460 --> 01:07:25.540]   from something you've created already
[01:07:25.540 --> 01:07:28.100]   or feel you may feel somewhere deep inside
[01:07:28.100 --> 01:07:29.300]   you may still create?
[01:07:29.300 --> 01:07:32.740]   - Well, okay, well, I'll take the question
[01:07:32.740 --> 01:07:35.520]   in the spirit it was asked, but very generous.
[01:07:35.520 --> 01:07:37.780]   - Gunpoint.
[01:07:37.780 --> 01:07:38.620]   - Yeah.
[01:07:38.620 --> 01:07:46.580]   What I try to do, so therefore what I hope,
[01:07:46.580 --> 01:07:50.780]   yeah, if I'm remembered what I might be remembered for
[01:07:50.780 --> 01:07:55.780]   is as someone who participates in a conversation.
[01:07:55.780 --> 01:07:58.500]   And I think that often what happens
[01:07:58.500 --> 01:08:00.940]   is people don't participate in conversations,
[01:08:00.940 --> 01:08:04.460]   they make proclamations, they make statements,
[01:08:04.460 --> 01:08:06.820]   and people can either react against the statement
[01:08:06.820 --> 01:08:08.700]   or can fall in line behind it.
[01:08:08.700 --> 01:08:10.260]   And I don't like that.
[01:08:10.260 --> 01:08:13.060]   So I want to be part of a conversation.
[01:08:13.060 --> 01:08:15.500]   I take as a sort of basic principle,
[01:08:15.500 --> 01:08:17.540]   I think I take lots of my cues from science,
[01:08:17.540 --> 01:08:19.340]   but one of the best ones it seems to me
[01:08:19.340 --> 01:08:22.340]   is that when a scientist has something proved wrong
[01:08:22.340 --> 01:08:24.020]   that they previously believed in,
[01:08:24.020 --> 01:08:26.620]   they then have to abandon that position.
[01:08:26.620 --> 01:08:28.500]   So I'd like to be someone who is allied
[01:08:28.500 --> 01:08:30.320]   to that sort of thinking.
[01:08:30.320 --> 01:08:34.340]   So part of an exchange of ideas.
[01:08:34.340 --> 01:08:38.140]   And the exchange of ideas for me is something like
[01:08:38.140 --> 01:08:40.940]   people in your world show me things
[01:08:40.940 --> 01:08:42.580]   about how the world works.
[01:08:42.580 --> 01:08:46.180]   And then I say, this is how I feel about what you've told me.
[01:08:46.180 --> 01:08:47.980]   And then other people can react to that.
[01:08:47.980 --> 01:08:52.260]   And it's not to say this is how the world is.
[01:08:52.260 --> 01:08:54.560]   It's just to say it is interesting
[01:08:54.560 --> 01:08:56.860]   to think about the world in this way.
[01:08:56.860 --> 01:08:59.860]   - And the conversation is one of the things
[01:08:59.860 --> 01:09:02.260]   I'm really hopeful about in your works.
[01:09:02.260 --> 01:09:05.260]   The conversation you're having is with the viewer
[01:09:05.260 --> 01:09:09.180]   in the sense that you're bringing back
[01:09:09.180 --> 01:09:13.860]   you and several others, but you very much so
[01:09:13.860 --> 01:09:17.980]   sort of intellectual depth to cinema,
[01:09:17.980 --> 01:09:19.840]   to now series,
[01:09:19.840 --> 01:09:25.320]   sort of allowing film to be something that,
[01:09:25.320 --> 01:09:29.660]   yeah, sparks a conversation, is a conversation,
[01:09:29.660 --> 01:09:32.860]   lets people think, allows them to think.
[01:09:32.860 --> 01:09:35.180]   - But also it's very important for me
[01:09:35.180 --> 01:09:38.540]   that if that conversation is gonna be a good conversation,
[01:09:38.540 --> 01:09:42.780]   what that must involve is that someone like you
[01:09:42.780 --> 01:09:45.580]   who understands AI, and I imagine understands
[01:09:45.580 --> 01:09:47.180]   a lot about quantum mechanics,
[01:09:47.180 --> 01:09:49.040]   if they then watch the narrative feels,
[01:09:49.040 --> 01:09:52.100]   yes, this is a fair account.
[01:09:52.100 --> 01:09:55.540]   So it is a worthy addition to the conversation.
[01:09:55.540 --> 01:09:57.580]   That for me is hugely important.
[01:09:57.580 --> 01:09:59.820]   I'm not interested in getting that stuff wrong.
[01:09:59.820 --> 01:10:02.120]   I'm only interested in trying to get it right.
[01:10:02.120 --> 01:10:06.340]   - Alex, it was truly an honor to talk to you.
[01:10:06.340 --> 01:10:07.820]   I really appreciate it, I really enjoy it.
[01:10:07.820 --> 01:10:08.700]   Thank you so much.
[01:10:08.700 --> 01:10:09.900]   - Thank you, thanks man.
[01:10:09.900 --> 01:10:13.260]   - Thanks for listening to this conversation
[01:10:13.260 --> 01:10:15.180]   with Alex Garland, and thank you
[01:10:15.180 --> 01:10:17.360]   to our presenting sponsor, Cash App.
[01:10:17.360 --> 01:10:20.140]   Download it, use code LEXPODCAST,
[01:10:20.140 --> 01:10:23.020]   you'll get $10, and $10 will go to FIRST,
[01:10:23.020 --> 01:10:26.180]   an organization that inspires and educates young minds
[01:10:26.180 --> 01:10:28.980]   to become science and technology innovators of tomorrow.
[01:10:28.980 --> 01:10:32.540]   If you enjoy this podcast, subscribe on YouTube,
[01:10:32.540 --> 01:10:35.780]   get five stars on Apple Podcast, support on Patreon,
[01:10:35.780 --> 01:10:39.800]   or simply connect with me on Twitter, @LexFriedman.
[01:10:39.800 --> 01:10:43.460]   And now, let me leave you with a question from Ava,
[01:10:43.460 --> 01:10:45.860]   the central artificial intelligence character
[01:10:45.860 --> 01:10:48.820]   in the movie "Ex Machina" that she asked
[01:10:48.820 --> 01:10:50.380]   during her Turing test.
[01:10:50.380 --> 01:10:54.580]   "What will happen to me if I fail your test?"
[01:10:55.580 --> 01:10:58.980]   Thank you for listening, and hope to see you next time.
[01:10:59.380 --> 01:11:00.220]   Next time.
[01:11:00.220 --> 01:11:02.800]   (upbeat music)
[01:11:02.800 --> 01:11:05.380]   (upbeat music)
[01:11:05.380 --> 01:11:15.380]   [BLANK_AUDIO]

