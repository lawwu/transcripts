
[00:00:00.000 --> 00:00:13.040]   Go live now. And I'll wait to make sure we're live. I'm
[00:00:13.040 --> 00:00:18.440]   always a bit paranoid about ensuring we're live. I can, I
[00:00:18.440 --> 00:00:21.800]   can hear myself I've messed up so many times that one simple
[00:00:21.800 --> 00:00:26.760]   thing. So I always get worried a little. Hey, everybody. Thanks.
[00:00:26.800 --> 00:00:31.080]   Thanks for joining us. On on a Saturday. I'm super excited to
[00:00:31.080 --> 00:00:34.280]   host Boris today. I'll introduce him in a minute. Boris, thanks
[00:00:34.280 --> 00:00:37.240]   for your time today. I'll quickly introduce by reminding
[00:00:37.240 --> 00:00:41.200]   everyone of what the session is, talk a bit about 27 days of
[00:00:41.200 --> 00:00:48.120]   JAX and then hand it over to Boris. Let me share my screen.
[00:00:48.120 --> 00:00:54.160]   So unfortunately, I am the co host as well. But fortunately,
[00:00:54.160 --> 00:00:57.360]   Boris will be doing most of the talking today. He'll be talking
[00:00:57.360 --> 00:01:03.040]   about how to implement doll. Is that the correct pronunciation?
[00:01:03.040 --> 00:01:07.560]   I don't know. I pronounce it as well.
[00:01:07.560 --> 00:01:12.480]   Okay, so we will learn how to generate images from a prompt
[00:01:12.480 --> 00:01:15.760]   boards will be telling you more about this. That's the theme of
[00:01:15.760 --> 00:01:19.200]   today's session. And in case you've missed the previous one,
[00:01:19.200 --> 00:01:23.160]   we had Jonathan Hick from the Google brain team, he talked
[00:01:23.160 --> 00:01:28.680]   about the future of ML research in JAX and flags. So be sure to
[00:01:28.680 --> 00:01:32.680]   check that out. That's up on our YouTube page. I missed
[00:01:32.680 --> 00:01:36.520]   announcing the 27 days of JAX winners. So just wanted to thank
[00:01:36.520 --> 00:01:40.640]   everyone once again, this is from last week. But I still
[00:01:40.640 --> 00:01:43.520]   wanted to mention all of these incredible posts. And if you're
[00:01:43.520 --> 00:01:49.080]   just joining 27 days of JAX is a is first of all my way of
[00:01:49.080 --> 00:01:52.000]   hitting a dent in the universe creating a dent where I get to
[00:01:52.000 --> 00:01:56.040]   choose the number I didn't like 30 I like 27 more. But it's an
[00:01:56.040 --> 00:02:00.160]   initiator to invite everyone to write more stuff in JAX to code
[00:02:00.160 --> 00:02:03.760]   more in JAX and share their work. I wanted to give a shout
[00:02:03.760 --> 00:02:07.120]   out to Bhadre. She's been very consistently writing about very
[00:02:07.120 --> 00:02:12.640]   simple concepts and sharing these neat screenshots of the
[00:02:12.640 --> 00:02:18.520]   one from the recent one here is a simple MLP with JAX. So wish
[00:02:18.560 --> 00:02:23.480]   also has been sharing incredible notebooks on his I think he has
[00:02:23.480 --> 00:02:26.960]   a GitHub repository at this point. So just wanted to give a
[00:02:26.960 --> 00:02:31.600]   shout out to these two people. And I just got around to reading
[00:02:31.600 --> 00:02:37.440]   this. Yuvraj has sent in this amazing blog post, he has two of
[00:02:37.440 --> 00:02:40.240]   them now. And I think these are better than the series that we
[00:02:40.240 --> 00:02:42.720]   have been hosting. So once the series is complete, I'll just
[00:02:42.720 --> 00:02:46.280]   send everyone to his blog post because they are the refined
[00:02:46.280 --> 00:02:49.960]   version of what we have been hosting here. I'll post all of
[00:02:49.960 --> 00:02:52.200]   these links, but I would highly encourage everyone to check
[00:02:52.200 --> 00:02:58.520]   these out. As a reminder, this is being co hosted this meetup
[00:02:58.520 --> 00:03:01.440]   is being co hosted with the meetup group, JAX global
[00:03:01.440 --> 00:03:05.400]   meetup. So please be sure to join that group as well for all
[00:03:05.400 --> 00:03:10.600]   future notifications of these meetups. And one more logistics
[00:03:10.600 --> 00:03:15.240]   mention or logistical mention I have been reading a lot of
[00:03:15.280 --> 00:03:18.240]   Europe's papers and we're hosting a Europe's official
[00:03:18.240 --> 00:03:20.440]   Europe's meetup tomorrow where I will be going through a bunch
[00:03:20.440 --> 00:03:24.080]   of papers. So if that's of interest, I'll share this link
[00:03:24.080 --> 00:03:30.040]   immediately afterwards and you can join us tomorrow. Sorry for
[00:03:30.040 --> 00:03:33.680]   all the logistical stuff. I'd love to introduce Boris now he's
[00:03:33.680 --> 00:03:36.760]   a machine learning engineer at Bateson biases. He's also the
[00:03:36.760 --> 00:03:39.960]   lead creator of Dolly mini which he'll be talking about more.
[00:03:39.960 --> 00:03:43.920]   Boris, I don't think you've left any open source framework,
[00:03:43.920 --> 00:03:47.200]   right? You've contributed to transformers, fast AI, PyTorch,
[00:03:47.200 --> 00:03:49.520]   lightning, which which one remains from your list?
[00:03:49.520 --> 00:03:53.680]   Now, there's a lot I played with. Initially, I started with
[00:03:53.680 --> 00:03:56.640]   the Keras, TensorFlow, then slowly I went to the PyTorch
[00:03:56.640 --> 00:04:01.360]   side. And right now I like a lot, I like a lot, like PyTorch
[00:04:01.360 --> 00:04:04.440]   based framework. So those that you see here, transformers,
[00:04:04.440 --> 00:04:07.800]   fast AI, PyTorch, lightning are really good. The transformers
[00:04:07.800 --> 00:04:12.280]   does a bit everything but JAX, I'm having fun with it. It may
[00:04:12.280 --> 00:04:16.840]   be a bit, I guess it's maybe a bit more difficult at the start
[00:04:16.840 --> 00:04:20.520]   when you're used to PyTorch and all, but it's really worth it
[00:04:20.520 --> 00:04:23.960]   whenever you try to scale a model. So when I have something
[00:04:23.960 --> 00:04:27.520]   simple, I still like, you know, there's so much code in PyTorch,
[00:04:27.520 --> 00:04:30.440]   it may be easier. But when you have something that you know, at
[00:04:30.440 --> 00:04:33.000]   some point, it's going to need to scale and all JAX is very
[00:04:33.000 --> 00:04:36.080]   cool, because your code, you don't have to change pretty much
[00:04:36.080 --> 00:04:40.440]   anything. And it automatically runs on to eight TPUs, or a pod
[00:04:40.440 --> 00:04:44.720]   of 200 TPUs and all, and you have nothing to do. It's really
[00:04:44.720 --> 00:04:45.240]   nice.
[00:04:45.240 --> 00:04:48.800]   Sanyam Bhutani: But did you find the concepts mind boggling? I
[00:04:48.800 --> 00:04:51.080]   know this is turning into an interview, but I'd love to just
[00:04:51.080 --> 00:04:55.240]   ask you this. So when I switch, sorry, when I switched from
[00:04:55.240 --> 00:04:58.280]   PyTorch to JAX, I was new to functional programming. And I
[00:04:58.280 --> 00:05:02.280]   was finding these stateless operations different and these
[00:05:02.280 --> 00:05:04.680]   little concepts were mind confusing. But like, you
[00:05:04.680 --> 00:05:06.680]   familiar with those? Or did you take your learning?
[00:05:07.840 --> 00:05:10.360]   Fabien Cottier: No, it was it was actually very confusing. But
[00:05:10.360 --> 00:05:14.160]   when I started with it was when Hugging Face organized that,
[00:05:14.160 --> 00:05:19.800]   that competition for JAX with Google. And basically, I didn't
[00:05:19.800 --> 00:05:23.760]   try to build something from scratch in JAX. No, they had,
[00:05:23.760 --> 00:05:26.280]   and we'll see through it, but they had some really, they have
[00:05:26.280 --> 00:05:30.320]   some really well written code. And you know, I was like, Oh, to
[00:05:30.320 --> 00:05:34.560]   do my to do the Alimini, I would just have to tweak that part. So
[00:05:34.560 --> 00:05:38.400]   I would learn on either the specific components I needed. I
[00:05:38.400 --> 00:05:42.600]   actually did the introduction to JAX. I did it after having the
[00:05:42.600 --> 00:05:45.880]   first version of the Alimini. Actually, well later, I maybe
[00:05:45.880 --> 00:05:48.920]   had already a second version, I still had not seen the
[00:05:48.920 --> 00:05:51.840]   introduction to JAX. I would have just learned what was need
[00:05:51.840 --> 00:05:54.920]   for the training loop. But I still didn't know how do I use
[00:05:54.920 --> 00:05:59.280]   a FlexLinux? How do I use Optax? I went into that after because
[00:05:59.280 --> 00:06:01.880]   I was like, I want to customize stuff and it's time to learn.
[00:06:02.920 --> 00:06:04.920]   Sanyam Bhutani: So you follow the top down approach where you
[00:06:04.920 --> 00:06:07.280]   implemented things and learned along the way.
[00:06:07.280 --> 00:06:10.640]   Bruno Zadeh: Yeah, really. And I would say like, there's a lot
[00:06:10.640 --> 00:06:14.400]   of, there's less tutorials, obviously, than what you find in
[00:06:14.400 --> 00:06:19.120]   other frameworks. But typically, I try to I always get
[00:06:19.120 --> 00:06:22.200]   inspiration from something I go on the Google, Google JAX
[00:06:22.200 --> 00:06:25.080]   projects, they have and I read how did they do their training
[00:06:25.080 --> 00:06:29.880]   code, and I try to understand some of it. There's definitely
[00:06:29.880 --> 00:06:33.800]   some things that I don't get all the time. But there's a lot of
[00:06:33.800 --> 00:06:35.920]   good code and you learn from it.
[00:06:35.920 --> 00:06:41.240]   Sanyam Bhutani: That's that's fascinating to know. I'll again
[00:06:41.240 --> 00:06:44.720]   share my screen for another moment and just remind everyone
[00:06:44.720 --> 00:06:47.360]   to post their questions in this thread. So if you head over to
[00:06:47.360 --> 00:06:50.720]   this link, which I'll just share after I stop sharing, to take
[00:06:50.720 --> 00:06:53.240]   you to our forums, you can simply reply and ask any
[00:06:53.240 --> 00:06:56.240]   questions. And I'll keep checking them and interrupting
[00:06:56.240 --> 00:07:01.480]   Boris to answer those. And one thing I missed, be sure to check
[00:07:01.480 --> 00:07:07.560]   out Boris on Twitter, his handle is Boris Nema. And with that,
[00:07:07.560 --> 00:07:11.200]   I'll hand it over to you to tell us more about Dollymini.
[00:07:11.200 --> 00:07:16.120]   Bruno Zadeh: Alright, so let me share my screen.
[00:07:16.120 --> 00:07:24.800]   Sanyam Bhutani: I can see the Hugging Face website.
[00:07:25.360 --> 00:07:29.840]   Bruno Zadeh: Okay, so I'm going to do a very quick, quick
[00:07:29.840 --> 00:07:33.760]   introduction to Dollymini and what it is. So Dollymini is a
[00:07:33.760 --> 00:07:39.040]   program where you write text. So for example, view of the beach
[00:07:39.040 --> 00:07:45.240]   under the sunset. I chose one that worked. Some don't work as
[00:07:45.240 --> 00:07:50.040]   well. When I ran those inferences last time, I had to
[00:07:50.040 --> 00:07:53.280]   talk to 20 minutes before I realized like the demo was not
[00:07:53.280 --> 00:07:56.920]   working anymore. Because it's running pretty fast. And it's
[00:07:56.920 --> 00:08:01.080]   actually running on a TPU. And that are graciously offered by
[00:08:01.080 --> 00:08:08.240]   the Google TLC program. And so here we see some pictures. And
[00:08:08.240 --> 00:08:12.200]   the problem is like it had crashed. It had crashed. And so
[00:08:12.200 --> 00:08:17.120]   I couldn't, I couldn't use it. And it was very sad. I was in
[00:08:17.120 --> 00:08:20.040]   panic a few 10 minutes before and I was able to launch it
[00:08:20.200 --> 00:08:24.920]   right before the presentation. I'm happy that it works. So so
[00:08:24.920 --> 00:08:28.560]   you can type those prompts and it goes pretty fast. I'm going
[00:08:28.560 --> 00:08:32.080]   to type one more complicated, for example, which is one that I
[00:08:32.080 --> 00:08:36.840]   like the Eiffel Tower on the moon, which is one that I want
[00:08:36.840 --> 00:08:40.960]   to have right. It's not right yet. But eventually it will be
[00:08:40.960 --> 00:08:46.920]   pretty cool. So actually, the time that it runs, you see, we're
[00:08:46.920 --> 00:08:52.200]   waiting maybe 20 seconds. There's actually 128 inferences.
[00:08:52.200 --> 00:08:57.760]   And those are ranked by clip. And clip gives a score and
[00:08:57.760 --> 00:09:01.400]   decide which one are the best. And those are the ones that that
[00:09:01.400 --> 00:09:08.360]   are displayed. So let's see quickly what it makes here. And
[00:09:08.360 --> 00:09:11.600]   it goes faster or slower depending on how many people use
[00:09:11.600 --> 00:09:14.880]   it. Okay, so here you see,
[00:09:15.760 --> 00:09:17.600]   I like the third one. Yeah, that's
[00:09:17.600 --> 00:09:23.000]   realistic. So this is actually not the latest model, the one
[00:09:23.000 --> 00:09:25.440]   we have on the face, we're training another one that is
[00:09:25.440 --> 00:09:29.040]   faster. That that is that is better as been trained on more
[00:09:29.040 --> 00:09:32.480]   data. But we've not released it yet, because it was more for
[00:09:32.480 --> 00:09:36.120]   experimentation. We're really going to scale it up to a much
[00:09:36.120 --> 00:09:38.800]   larger model because it was the same size we wanted to learn
[00:09:38.800 --> 00:09:40.920]   something based on the data set, etc.
[00:09:40.920 --> 00:09:44.480]   Can we can we try another one, a person drinking chai on a
[00:09:44.480 --> 00:09:45.760]   mountain or something like that?
[00:09:45.760 --> 00:09:50.080]   Yeah, yeah, let's try something else. I have to
[00:09:50.080 --> 00:09:53.680]   do.
[00:09:53.680 --> 00:09:56.840]   Sorry.
[00:09:56.840 --> 00:09:59.920]   No worries. I don't see that particular.
[00:09:59.920 --> 00:10:04.800]   Oops, the wrong one. How do I move the bar of the
[00:10:04.800 --> 00:10:08.640]   you can click and drag it around.
[00:10:08.640 --> 00:10:13.160]   Click and drag it. Oh, yeah. Thank you. You're helping me a
[00:10:13.160 --> 00:10:13.560]   lot.
[00:10:14.000 --> 00:10:16.080]   Okay, which one did you want to try?
[00:10:16.080 --> 00:10:19.360]   Maybe a person drinking chai on a mountain.
[00:10:19.360 --> 00:10:21.800]   A person drinking
[00:10:21.800 --> 00:10:25.240]   chai on a mountain.
[00:10:25.240 --> 00:10:32.240]   So the encoder that we are using right now is very bad at people.
[00:10:32.240 --> 00:10:36.440]   And the people look look really, really scary.
[00:10:36.440 --> 00:10:41.240]   So it's it's really due to the encoder mainly.
[00:10:42.480 --> 00:10:46.600]   Which like the eyes are weird. And it's not been learned as
[00:10:46.600 --> 00:10:50.400]   well. So I think it's gonna be pretty scary. So it works
[00:10:50.400 --> 00:10:53.720]   better when you say a nightmare person drinking chai on the
[00:10:53.720 --> 00:10:54.320]   mountains.
[00:10:54.320 --> 00:10:58.920]   It will do the exact same results, except you're like, oh,
[00:10:58.920 --> 00:11:00.160]   okay, it fits the prompt.
[00:11:00.160 --> 00:11:07.080]   So you see some guy but I don't think you see so much chai. You
[00:11:07.080 --> 00:11:08.720]   see some people on the mountain, right?
[00:11:08.720 --> 00:11:11.320]   Yeah, that looks like that looks like me.
[00:11:11.320 --> 00:11:13.240]   I'll ask the audience, what do you guys think?
[00:11:13.240 --> 00:11:15.280]   But that that looks like me a bit.
[00:11:15.280 --> 00:11:19.120]   You'll notice something interesting. If I zoom in a bit,
[00:11:19.120 --> 00:11:23.480]   you see watermarks, right, that are generated on the pictures,
[00:11:23.480 --> 00:11:28.600]   which is interesting, but which is not good. So it's something
[00:11:28.600 --> 00:11:31.200]   that is being fixed, actually, that I think we won't have that
[00:11:31.200 --> 00:11:34.000]   prime in the next version. And the reason is we remove them
[00:11:34.000 --> 00:11:34.960]   from the data set.
[00:11:34.960 --> 00:11:39.760]   So it's it's a challenge always to detect the watermarks. But
[00:11:40.560 --> 00:11:44.720]   I'm going to show super quickly how it looks like. So if you go
[00:11:44.720 --> 00:11:47.160]   on there, and if you go on our GitHub, you'll see a link to the
[00:11:47.160 --> 00:11:50.600]   report, you will be able to understand better how it works.
[00:11:50.600 --> 00:11:53.160]   I won't go too much through it, we have time to play with the
[00:11:53.160 --> 00:11:57.520]   code. But I'm going to show just some cool stuff. It's pretty
[00:11:57.520 --> 00:12:02.640]   complex. It's pretty, no, sorry, it's not complex. It's pretty
[00:12:02.640 --> 00:12:08.960]   complete, I would say. There's a lot of stuff and Khalid make
[00:12:08.960 --> 00:12:14.160]   those graphs, which are really, really good. So if I go on this
[00:12:14.160 --> 00:12:19.280]   one, for example, let's see the inference pipeline, I think this
[00:12:19.280 --> 00:12:24.880]   is the most interesting. So our model is basically the BART
[00:12:24.880 --> 00:12:29.680]   model and the BART model is made of two things, a decoder and an
[00:12:29.680 --> 00:12:30.240]   encoder.
[00:12:30.240 --> 00:12:33.280]   So this is the overview of Dolly Mini, right?
[00:12:33.280 --> 00:12:33.640]   Exactly.
[00:12:33.640 --> 00:12:34.840]   The implementation, okay.
[00:12:36.000 --> 00:12:40.080]   That's right. So here it's like what it is during training. But
[00:12:40.080 --> 00:12:42.320]   I think it's easier a bit to learn what happens during
[00:12:42.320 --> 00:12:47.280]   inference. So that's sec to sec, that box is BART. BART has an
[00:12:47.280 --> 00:12:51.280]   encoder and a decoder. So it's like a regular transformer
[00:12:51.280 --> 00:12:55.120]   except you have, you have you separate the encoder and
[00:12:55.120 --> 00:12:59.280]   decoder. So typically it's used for let's say translation, you
[00:13:00.240 --> 00:13:06.640]   feed a sentence in English maybe, and then it's encoded
[00:13:06.640 --> 00:13:08.960]   here and you have like some embeddings and at the end it
[00:13:08.960 --> 00:13:13.040]   goes through the decoder, which has a similar structure, but has
[00:13:13.040 --> 00:13:17.120]   different weights. And at the end the decoder will output the
[00:13:17.120 --> 00:13:21.680]   sentence maybe in French or in Spanish or in any language. And
[00:13:21.680 --> 00:13:27.360]   they're a bit special because the encoder here is like B
[00:13:27.360 --> 00:13:31.040]   directional. So it's like BART, you read the sentence from
[00:13:31.040 --> 00:13:33.840]   both sides because you have it from the start, you have access
[00:13:33.840 --> 00:13:37.600]   to all the information. Typically those are used when you
[00:13:37.600 --> 00:13:41.840]   do masked model, things like that. And the decoder, what it
[00:13:41.840 --> 00:13:48.240]   does, it's like a GPT model. So it outputs one token at a time.
[00:13:48.240 --> 00:13:52.320]   So it's good for generation and it's what we use for the
[00:13:52.320 --> 00:13:56.560]   images. So those images actually, they're encoded. I'm
[00:13:56.560 --> 00:14:00.800]   going to go quickly on here, but there's a model. Yeah.
[00:14:00.800 --> 00:14:03.840]   - No, sorry. Please continue.
[00:14:03.840 --> 00:14:07.840]   - Yeah. So for the images, because typically you would try
[00:14:07.840 --> 00:14:11.520]   to output maybe pixel and predict pixel, but what we use,
[00:14:11.520 --> 00:14:15.040]   we use an encoder. We want to have it into tokens like we have
[00:14:15.040 --> 00:14:21.120]   for NLP. So basically we have an encoder, which is a VQGAN,
[00:14:21.120 --> 00:14:24.160]   but there's some other ones, OpenAI use another one that
[00:14:24.160 --> 00:14:27.600]   they call the DVAE and that's what they use for that. It's a
[00:14:27.600 --> 00:14:30.800]   bit blurrier, but the goal of the VQGAN basically is going to
[00:14:30.800 --> 00:14:34.560]   look at, get a little patch and it's going to transform it into
[00:14:34.560 --> 00:14:37.920]   a token. So it will be just a number, maybe from one to 10,000
[00:14:37.920 --> 00:14:41.920]   and you have to imagine one token will be, for example, a
[00:14:41.920 --> 00:14:46.000]   blue square. One token will be maybe a yellow square with a
[00:14:46.000 --> 00:14:49.920]   green dot on it. So it's kind of the vocabulary become an
[00:14:49.920 --> 00:14:55.280]   image vocabulary. So those are the tokens that we try to
[00:14:55.280 --> 00:14:59.280]   predict. So we feed basically a text and we'll try to predict a
[00:14:59.280 --> 00:15:03.600]   number, which will be basically a code that is associated to
[00:15:03.600 --> 00:15:07.360]   the VQGAN or to whatever image encoder we have. Then we decode
[00:15:07.360 --> 00:15:12.080]   it and it becomes an image. So if I go here on the inference,
[00:15:12.080 --> 00:15:15.520]   basically I give that white snow-covered mountain under blue
[00:15:15.520 --> 00:15:16.560]   sky during daytime.
[00:15:17.120 --> 00:15:20.640]   Sorry, for the training dataset, do you have like a set of
[00:15:20.640 --> 00:15:23.520]   annotations or what is it being trained on?
[00:15:23.520 --> 00:15:28.560]   That is right. The training dataset is exactly that. It's
[00:15:28.560 --> 00:15:33.680]   image with captions. The most common dataset used for that is
[00:15:33.680 --> 00:15:38.320]   conceptual captions. So that's the one we used. But nowadays
[00:15:38.320 --> 00:15:41.120]   you have like larger dataset. There's like, I don't know if
[00:15:41.120 --> 00:15:44.400]   you heard, there's a dataset called Lion and what they do is
[00:15:44.400 --> 00:15:48.480]   they take the images from command crawl. They look at the
[00:15:48.480 --> 00:15:51.840]   alt text, you know, in HTML you have often like the alt text
[00:15:51.840 --> 00:15:55.680]   when the image is not displayed. And then they do something
[00:15:55.680 --> 00:16:00.800]   smart. They check the clip score between that alt text and that
[00:16:00.800 --> 00:16:03.520]   image. And if it's close, that means it's probably a good
[00:16:03.520 --> 00:16:07.520]   sample. So it's added to their dataset. And right now they have
[00:16:07.520 --> 00:16:11.200]   like 400 million images and their next version is probably
[00:16:11.200 --> 00:16:15.440]   going into the billion images. As a reference, this model was
[00:16:15.440 --> 00:16:18.640]   trained on about 10 to 15 million images. So it's much
[00:16:18.640 --> 00:16:19.140]   smaller.
[00:16:19.140 --> 00:16:24.000]   Sorry, you were talking about the inference pipeline.
[00:16:24.000 --> 00:16:28.320]   Oh yeah, sorry. So here we have the decoder, which basically is
[00:16:28.320 --> 00:16:32.480]   going to make sequences of tokens and it's going to make a
[00:16:32.480 --> 00:16:36.560]   few different, you know, it's like a, like a NGPT model. You
[00:16:36.560 --> 00:16:41.120]   generate different samples. So in our case, we generate 128.
[00:16:42.000 --> 00:16:46.160]   Then we decode it with the VQGAN. So the VQGAN basically,
[00:16:46.160 --> 00:16:48.960]   or the image encoder, obviously has a decoder associated. So I
[00:16:48.960 --> 00:16:51.920]   go from an image to some code and I also have the decoder,
[00:16:51.920 --> 00:16:55.360]   which goes back to an image. And maybe it's going to decode
[00:16:55.360 --> 00:16:59.120]   those images into that. And then we go one step further, which
[00:16:59.120 --> 00:17:03.280]   is what OpenAI does with DALI. We are going to give a score
[00:17:03.280 --> 00:17:07.600]   between that image and the initial prompt. And it's going
[00:17:07.600 --> 00:17:13.440]   to say, oh, maybe a score of 23%, 29%. It's a score over one.
[00:17:13.440 --> 00:17:17.200]   And we give the ones that have the highest score and it helps
[00:17:17.200 --> 00:17:22.560]   you bring like higher quality images. So I don't know if you
[00:17:22.560 --> 00:17:25.120]   have any questions about the architecture.
[00:17:25.120 --> 00:17:29.120]   I'll just check if the audience has any questions.
[00:17:29.120 --> 00:17:36.640]   Not yet. I'll just remind everyone and I'll post the link
[00:17:36.640 --> 00:17:39.520]   again. Please feel free to ask any questions and I'll keep
[00:17:39.520 --> 00:17:40.960]   interrupting Boris as they come up.
[00:17:40.960 --> 00:17:44.880]   Okay. Feel free to stop me. Otherwise, I will go into the
[00:17:44.880 --> 00:17:52.960]   code. All right. So let's go to see some code. So this is an
[00:17:52.960 --> 00:18:00.160]   interesting notebook, this one. It's like the how to do
[00:18:00.160 --> 00:18:05.120]   inference once we have the model. And we'll look after into
[00:18:05.120 --> 00:18:09.040]   more details for the model is. But it gives a cool use of
[00:18:09.040 --> 00:18:12.160]   JAX, I think in that because it's pretty fast compared to
[00:18:12.160 --> 00:18:16.720]   other model you see. So I just run that cell because setting
[00:18:16.720 --> 00:18:22.080]   dependencies is never fun to wait for. So then you will see
[00:18:22.080 --> 00:18:25.200]   I have my three model, right? One is the DALI model, one is
[00:18:25.200 --> 00:18:29.600]   the VQGAN and one is CLIP. And you see like I can give the
[00:18:29.600 --> 00:18:34.080]   model, have a path and have a commit ID. What I have
[00:18:34.080 --> 00:18:36.960]   basically for my model, I use a bit the Hugging Face style
[00:18:36.960 --> 00:18:40.560]   where you give a reference to a repository. I extended it to be
[00:18:40.560 --> 00:18:44.640]   able to give a 1DB artifact because sometimes I maybe I run
[00:18:44.640 --> 00:18:49.520]   a lot of experiments, they're saved as artifacts. And then I
[00:18:49.520 --> 00:18:52.400]   want to quickly see if they are good. And basically the best
[00:18:52.400 --> 00:18:55.120]   one I have, I release it on Hugging Face Hub.
[00:18:55.120 --> 00:19:03.040]   So what is cool when you do a model, the Hugging Face way, I
[00:19:03.040 --> 00:19:05.680]   should have probably run that cell before because it's gonna
[00:19:05.680 --> 00:19:10.320]   be a bit long actually thinking about it now. But what is cool
[00:19:10.320 --> 00:19:15.120]   about Hugging Face is basically they have those function. So my
[00:19:15.120 --> 00:19:19.440]   model is called custom flex bot for conditional generation,
[00:19:19.440 --> 00:19:24.800]   which inherits from flex bot. And it's a model from Hugging
[00:19:24.800 --> 00:19:29.600]   Face and their models, they have those from pre-trained. And
[00:19:29.600 --> 00:19:32.640]   from pre-trained is really cool because you just give a
[00:19:32.640 --> 00:19:37.920]   reference to maybe a folder that has your model and it loads
[00:19:37.920 --> 00:19:40.480]   everything correctly, the configuration, it loads the
[00:19:40.480 --> 00:19:45.040]   model, loads the weight. It has also functions to save your
[00:19:45.040 --> 00:19:49.760]   models. It has functions to push to the hub. So you have a bit
[00:19:49.760 --> 00:19:52.960]   to structure it in a certain way, but you take advantage of
[00:19:52.960 --> 00:19:57.680]   a lot of code that has been already built by Transformer.
[00:19:57.680 --> 00:20:01.280]   So on that one, we use it. We use it also for another reason,
[00:20:01.280 --> 00:20:05.440]   is the competition was organized by Hugging Face. They had some
[00:20:05.440 --> 00:20:07.760]   code that was a really good starter code. So we're like,
[00:20:07.760 --> 00:20:11.680]   how we, how to tweak it so it works as fast as possible.
[00:20:11.680 --> 00:20:16.000]   Sanyam Bhutani: Makes sense. I'll interrupt you to ask a few
[00:20:16.000 --> 00:20:20.880]   questions from the previous architecture diagram. So this
[00:20:20.880 --> 00:20:23.840]   is by Ramesh. He's asking training pipeline seems to be
[00:20:23.840 --> 00:20:29.360]   VQGAN encoder, but then cross entropy loss, which part of this
[00:20:29.360 --> 00:20:32.000]   was trained was to VQGAN encoder trained?
[00:20:32.000 --> 00:20:34.800]   Fabien Cottier - Customer Service Manager, Verify (CI):
[00:20:34.800 --> 00:20:38.400]   In the training pipeline? Yeah. So the VQGAN encoder here. Okay.
[00:20:38.400 --> 00:20:41.840]   What it does in the training pipeline.
[00:20:41.840 --> 00:20:42.880]   Sanyam Bhutani - Professor, Verify (CI):
[00:20:42.880 --> 00:20:45.040]   So they're asking what particular part was being
[00:20:45.040 --> 00:20:47.520]   trained here? Was it just the VQGAN encoder?
[00:20:47.520 --> 00:20:48.080]   Fabien Cottier - Customer Service Manager, Verify (CI):
[00:20:48.080 --> 00:20:52.880]   Oh, okay. So here the VQGAN is fixed. We use a pre-trained one.
[00:20:52.880 --> 00:20:57.920]   What is trained is this, is the BART encoder and the BART decoder.
[00:20:57.920 --> 00:21:02.560]   Actually, we started from a BART encoder. We started from a
[00:21:02.560 --> 00:21:06.400]   pre-trained one, which was one that was trained for text
[00:21:06.400 --> 00:21:09.680]   summarization. What is nice is it already has a lot of
[00:21:09.680 --> 00:21:13.600]   interesting weights. And the BART decoder, we took it out
[00:21:13.600 --> 00:21:16.960]   because basically it's goal before was to summarize the text.
[00:21:16.960 --> 00:21:20.400]   And now instead of summarize text, we wanted to output the
[00:21:20.400 --> 00:21:23.760]   tokens from the VQGAN. So we want those tokens to be the same
[00:21:23.760 --> 00:21:27.440]   as these ones. And the VQGAN encoder, basically it has the
[00:21:27.440 --> 00:21:30.640]   image. That's what we want to predict. We want to predict
[00:21:30.640 --> 00:21:35.360]   that image. So the VQGAN encoder will tell you which token we
[00:21:35.360 --> 00:21:39.840]   want. And we want to, without looking at the VQGAN encoder,
[00:21:39.840 --> 00:21:43.360]   we want to try to have the same ones. So then we compare these
[00:21:43.360 --> 00:21:47.040]   ones to these ones by using a cross-entropy loss that we try
[00:21:47.040 --> 00:21:51.200]   to minimize. Does it answer the question?
[00:21:51.200 --> 00:21:52.560]   Sanyam Bhutani (Foreign Language):
[00:21:52.560 --> 00:21:55.280]   I don't see a follow-up question, but I'll ask another
[00:21:55.280 --> 00:21:59.600]   one. I think that was quite clear to me. Why do you generate
[00:21:59.600 --> 00:22:03.680]   many sample candidate prompts during the inference? So the
[00:22:03.680 --> 00:22:05.440]   reason for generating multiple prompts.
[00:22:05.440 --> 00:22:10.000]   Paul Mardis: Right. So we do that because there's a bit of
[00:22:10.000 --> 00:22:13.760]   noise. Sometimes the images are not as good. If I generate only
[00:22:13.760 --> 00:22:18.800]   four, some will be better than others for sure. So the idea is
[00:22:18.800 --> 00:22:22.560]   I generate a lot. So I generate 100. OpenAI generates, I think,
[00:22:22.560 --> 00:22:28.160]   500. And then they try to clip the ones that are the best. It's
[00:22:28.160 --> 00:22:34.080]   almost as if you use GPT and you are able to -- I'm going to do
[00:22:34.080 --> 00:22:38.480]   like 20 different predictions. And clip is good because it's
[00:22:38.480 --> 00:22:41.200]   able to tell you which ones are the best. And it works pretty
[00:22:41.200 --> 00:22:44.320]   well. So it improves the results. I don't need the clip
[00:22:44.320 --> 00:22:47.680]   part. I don't need to do many. If I want, I can create a new
[00:22:47.680 --> 00:22:54.240]   one. But just a way to have higher quality results by
[00:22:54.240 --> 00:22:57.920]   filtering. You could say clip is a filter. It's remove the ones
[00:22:57.920 --> 00:22:58.800]   that are not very good.
[00:22:58.800 --> 00:23:04.320]   Sanyam Bhutani: Awesome. So going back to the previous
[00:23:04.320 --> 00:23:08.320]   question, there's a follow up on that. Training part is clear to
[00:23:08.320 --> 00:23:11.520]   them, but how is it connected to inference? How do you generate
[00:23:11.520 --> 00:23:16.880]   multiple candidate images from one bar decoder output? Is the
[00:23:16.880 --> 00:23:18.960]   ranking or clip part being loaned?
[00:23:18.960 --> 00:23:22.960]   Fabien Cottier: Okay. So to do several ones, it's a bit like
[00:23:22.960 --> 00:23:27.360]   GPT. So you need to imagine my model is like just a
[00:23:27.360 --> 00:23:31.200]   transformer. And what it does, it has a function like generate.
[00:23:31.200 --> 00:23:39.120]   And that generate, you have the -- it samples sentences based
[00:23:39.120 --> 00:23:42.400]   on probabilities. So I'm going to start maybe -- I'm going to
[00:23:42.400 --> 00:23:45.760]   give all that text. And the decoder, I'm going to give him a
[00:23:45.760 --> 00:23:49.040]   special token to tell him to start. There's a special token.
[00:23:49.040 --> 00:23:53.840]   And then it's going to output not necessarily one patch only.
[00:23:53.840 --> 00:23:56.320]   It's going to output a distribution of probabilities.
[00:23:56.320 --> 00:23:59.760]   It's going to say, okay, I think there's 20% chance it needs
[00:23:59.760 --> 00:24:04.160]   to be blue. 10% chance it needs to be a bit green or white in
[00:24:04.160 --> 00:24:08.640]   the bottom. And what we do, we do like just a random sampling
[00:24:08.640 --> 00:24:13.120]   from it. So you can have a lot of generation just from those.
[00:24:13.120 --> 00:24:15.840]   And once you've selected one -- so I decided I wanted the blue
[00:24:15.840 --> 00:24:19.920]   square -- the next depends also on what you selected previously.
[00:24:19.920 --> 00:24:25.520]   So you keep on doing that generation of sampling according
[00:24:25.520 --> 00:24:28.960]   to your probabilistic distribution, which is a bit the
[00:24:28.960 --> 00:24:33.200]   same as GPT does. It can generate a lot of sentences based
[00:24:33.200 --> 00:24:39.120]   on the same start of prompt. I don't know if that was the
[00:24:39.120 --> 00:24:41.280]   question or not. But --
[00:24:41.280 --> 00:24:44.720]   >> I'll just wait another minute because Ramesh is still
[00:24:44.720 --> 00:24:49.920]   typing. So I think he might follow up on this. It's always
[00:24:49.920 --> 00:24:53.520]   interesting. I watch these recaps and it feels weird, but
[00:24:53.520 --> 00:24:56.240]   this is happening in real time and people take one or two
[00:24:56.240 --> 00:24:59.360]   minutes to type the question. So there's always these pauses.
[00:24:59.360 --> 00:25:04.720]   >> That's fine. I can run the next one in the meantime.
[00:25:04.720 --> 00:25:10.880]   So you see the main thing I did here basically is easy. I
[00:25:10.880 --> 00:25:15.520]   loaded my model, which basically my model is the -- we're
[00:25:15.520 --> 00:25:19.920]   going to see on the graph. What I loaded, I loaded this part.
[00:25:19.920 --> 00:25:24.160]   This is my model. Right? I have a tokenizer, which my
[00:25:24.160 --> 00:25:29.120]   tokenizer is basically what's going to read the text and turn
[00:25:29.120 --> 00:25:32.960]   it into tokens that I'm going to feed to the model. VQGAN,
[00:25:32.960 --> 00:25:36.640]   it's for my encoder and decoder. In that case, I just need a
[00:25:36.640 --> 00:25:39.520]   decoder. I do inference. So I'm going to have tokens. I want
[00:25:39.520 --> 00:25:44.480]   to transform them back into an image. And then clip is made of
[00:25:44.480 --> 00:25:48.320]   two parts. There's clip, which is the model, and the processor.
[00:25:48.320 --> 00:25:51.520]   What it does, it formats the input data in a certain way.
[00:25:51.520 --> 00:25:54.240]   It's really easy to use because it's a nugging phase. I have
[00:25:54.240 --> 00:25:56.960]   like a from pre-trained and that's it. I can use it.
[00:25:56.960 --> 00:26:03.360]   So then we do something that is interesting. Mainly when you
[00:26:03.360 --> 00:26:08.400]   have like -- if you have like -- if you play on the TPU V3 or
[00:26:08.400 --> 00:26:11.600]   something provided by Google, I don't know if some of you have
[00:26:11.600 --> 00:26:15.200]   access, but what is nice, you have that replicate function.
[00:26:15.200 --> 00:26:19.840]   And actually, you know, I may have the risk of breaking some
[00:26:19.840 --> 00:26:28.640]   things, but we can play a bit with it maybe. If I do -- if we
[00:26:28.640 --> 00:26:36.320]   looked at maybe clip params right now. So -- oh, it didn't
[00:26:36.320 --> 00:26:39.120]   run. Oh, no, sorry. It was clip.params.
[00:26:39.120 --> 00:26:47.440]   So clip.params contains all my parameters. But you need to see
[00:26:47.440 --> 00:26:54.080]   it's actually on the CPU. So it's actually present only once.
[00:26:54.080 --> 00:26:59.840]   So if I try to do inference with it, I will have to -- I will
[00:26:59.840 --> 00:27:04.080]   have to do it on -- I will have to transfer those parameters on
[00:27:04.080 --> 00:27:07.120]   all the instances of my TPU, which is going to be slow.
[00:27:07.120 --> 00:27:12.640]   So JAX and FLAX offers a lot of cool functions, which is, for
[00:27:12.640 --> 00:27:16.400]   example, replicate. And if I launch the replicate function,
[00:27:16.400 --> 00:27:20.320]   what it does, those parameters that are from my model, from my
[00:27:20.320 --> 00:27:24.640]   VQGAN and from clip, they are going to be copied and
[00:27:24.640 --> 00:27:28.000]   distributed on each device. So when I do that, they
[00:27:28.000 --> 00:27:32.160]   automatically push to each device. I had some little problem
[00:27:32.160 --> 00:27:35.760]   with Scalab, which is -- I'm going to run that function. I
[00:27:35.760 --> 00:27:39.920]   had some little problem with Scalab, which were that you can
[00:27:39.920 --> 00:27:43.280]   choose a TPU instance, but somehow the model -- on the large
[00:27:43.280 --> 00:27:46.800]   model, it's slow and it doesn't really work well. So here I'm
[00:27:46.800 --> 00:27:49.280]   actually on the GPU, but the logic is exactly the same.
[00:27:49.280 --> 00:27:52.960]   Basically, if you compared it to PyTorch, here I'm moving the
[00:27:52.960 --> 00:27:57.760]   parameters to the GPU. And if I had eight GPUs, I would move the
[00:27:57.760 --> 00:28:03.040]   same parameters to the eight GPUs. So it's a pretty cool
[00:28:03.040 --> 00:28:05.600]   function. What it does, basically, it's going to be
[00:28:05.600 --> 00:28:09.520]   faster because the GPU or the TPU is going to be responsible
[00:28:09.520 --> 00:28:12.800]   for the calculation and the parameters are already there. So
[00:28:12.800 --> 00:28:15.440]   you go much, much faster, even more for large models.
[00:28:15.440 --> 00:28:21.520]   Here is very interesting because we start playing with Pmap. I
[00:28:21.520 --> 00:28:25.040]   don't know if you went through Pmap in your class.
[00:28:25.600 --> 00:28:26.480]   We have covered that.
[00:28:26.480 --> 00:28:31.440]   Okay. So let's look at the second one because it's a bit
[00:28:31.440 --> 00:28:35.200]   easier. But basically, I have my function from VecuGAN that
[00:28:35.200 --> 00:28:41.520]   decode, and I use Pmap so that I can do it on all the instances
[00:28:41.520 --> 00:28:46.800]   of TPU or GPU at the same time. And same with Clip. What is
[00:28:46.800 --> 00:28:50.080]   interesting with it is it doesn't do just the distribution.
[00:28:50.080 --> 00:28:54.640]   It also compiles the model. So even if you have only one device
[00:28:54.640 --> 00:28:58.720]   here, I have only one device, it will run much, much, much
[00:28:58.720 --> 00:29:03.360]   faster. That one is slightly different. So I have my model
[00:29:03.360 --> 00:29:06.560]   that generates, and you see I have an argument which is
[00:29:06.560 --> 00:29:12.080]   static broadcast argnums. What does it mean? Basically, I'm
[00:29:12.080 --> 00:29:15.760]   saying that if I change those two values, which are topK and
[00:29:15.760 --> 00:29:20.560]   topP, which is basically for generating samples, I'm saying
[00:29:20.560 --> 00:29:22.880]   if I change those values, you need to recompile.
[00:29:24.480 --> 00:29:27.600]   And otherwise, whenever you see them, imagine that they are
[00:29:27.600 --> 00:29:31.360]   static. They will never be changed. And why do I do that?
[00:29:31.360 --> 00:29:35.360]   Typically, it's in a generate function, which is a sampling
[00:29:35.360 --> 00:29:42.240]   function. topP and topK says basically, what in my probability
[00:29:42.240 --> 00:29:45.520]   distribution do I want to keep only the 50 best samples, and I
[00:29:45.520 --> 00:29:50.080]   choose only from those 50 best samples. And here it says, do I
[00:29:50.080 --> 00:29:54.480]   want to go only up to 95% probability and don't choose
[00:29:54.480 --> 00:30:00.640]   from the tiny probabilities of the less common samples. And
[00:30:00.640 --> 00:30:04.240]   here you can give none. None, it means you don't care,
[00:30:04.240 --> 00:30:07.360]   basically. Take whatever you want from the entire
[00:30:07.360 --> 00:30:11.760]   distribution. And in the program that does generate, there's
[00:30:11.760 --> 00:30:16.560]   some if-then. And if-then is not very good with JAX, because
[00:30:16.560 --> 00:30:18.800]   it wants to compile. You want to know everything in advance.
[00:30:19.520 --> 00:30:23.840]   So if it's linked to some input, which here would be topK, topP,
[00:30:23.840 --> 00:30:28.240]   if I have none or if I have an actual value, the function is
[00:30:28.240 --> 00:30:34.560]   different. So in JAX, you have a way to use JAX.lax.count or
[00:30:34.560 --> 00:30:39.680]   something like that, which basically will compile the two
[00:30:39.680 --> 00:30:43.040]   possibilities and will be able to handle the if. The only
[00:30:43.040 --> 00:30:45.600]   problem is your compilation then is much bigger, because you
[00:30:45.600 --> 00:30:47.680]   want to have the two possibilities. And in that case,
[00:30:47.680 --> 00:30:52.320]   it was not really worth it. Note also that what I did is not
[00:30:52.320 --> 00:30:57.360]   necessarily super smart, because if I have topK equal 10 and
[00:30:57.360 --> 00:31:02.000]   then I have topK equal 20, technically you wouldn't need to
[00:31:02.000 --> 00:31:05.840]   re-pmap again and recompile, because I use the same function.
[00:31:05.840 --> 00:31:10.000]   I don't know if there's any question linked to that.
[00:31:10.000 --> 00:31:13.280]   No, I don't see any question there.
[00:31:14.000 --> 00:31:18.080]   And if I'd put just a static value, a fixed value of topK,
[00:31:18.080 --> 00:31:21.200]   topP, you wouldn't need that. It's something not necessarily
[00:31:21.200 --> 00:31:27.520]   that important. So here I create a random key that you need in
[00:31:27.520 --> 00:31:30.320]   a JAX, random number generator.
[00:31:30.320 --> 00:31:36.480]   I have something in the Alimini. We have now a text normalizer,
[00:31:36.480 --> 00:31:40.400]   which does a few things, which for example, you have the
[00:31:40.400 --> 00:31:44.480]   prompt, is the Eiffel Tower on the moon? And we're going to
[00:31:44.480 --> 00:31:45.520]   see what it does.
[00:31:45.520 --> 00:31:51.440]   So it just simplifies a bit the text. So here you see it made
[00:31:51.440 --> 00:31:56.640]   it lowercase and it added a space before. And I just wanted
[00:31:56.640 --> 00:31:59.840]   to try to simplify the tokenization, because I never
[00:31:59.840 --> 00:32:03.760]   like the fact that there's going to be captions that have
[00:32:03.760 --> 00:32:08.080]   Eiffel and some will be capitalized, some not, and there
[00:32:08.080 --> 00:32:10.640]   will be different tokens. And if they are the start of the
[00:32:10.640 --> 00:32:13.440]   sentence, the same, it's different tokens. So you can
[00:32:13.440 --> 00:32:18.880]   have like four ways of encoding Eiffel. But if I add always a
[00:32:18.880 --> 00:32:22.160]   space and if I always make it lowercase, it will always be
[00:32:22.160 --> 00:32:25.680]   encoded the same way. So I feel like I'm helping the program a
[00:32:25.680 --> 00:32:28.960]   little bit. And I do other stuff linked to punctuation,
[00:32:28.960 --> 00:32:32.640]   simplifying, removing accents and things like that. It's
[00:32:32.640 --> 00:32:36.000]   always good to think of how can you help your model learn
[00:32:36.000 --> 00:32:39.040]   faster if it's something that you think doesn't matter for
[00:32:39.040 --> 00:32:44.240]   you as a human. It may not matter. So then what we do, we
[00:32:44.240 --> 00:32:50.160]   repeat the prompt. Why do we do that? I have maybe eight
[00:32:50.160 --> 00:32:53.360]   devices and I'm going to send one prompt to each of the
[00:32:53.360 --> 00:32:56.960]   devices. So it's the same as my parameters. But you could
[00:32:56.960 --> 00:33:00.720]   imagine that if I wanted, I could have different prompt and
[00:33:00.720 --> 00:33:02.480]   have a different prompt on each device.
[00:33:03.440 --> 00:33:07.520]   But just to clarify for the audience, you're talking about
[00:33:07.520 --> 00:33:11.440]   a TPU and one TPU, unlike a GPU, has just eight cores. And
[00:33:11.440 --> 00:33:14.240]   those are like really powerful cores. So here you're talking
[00:33:14.240 --> 00:33:17.440]   about running one prompt on one core. That's why these are
[00:33:17.440 --> 00:33:18.320]   like eight cores.
[00:33:18.320 --> 00:33:24.480]   That's right. So yeah, if you have a TPU v3.8, it's one node,
[00:33:24.480 --> 00:33:30.720]   but there's eight TPU instances. So in that case, basically,
[00:33:30.720 --> 00:33:34.800]   you can do eight. It's almost if it was eight different cards.
[00:33:34.800 --> 00:33:37.120]   So you can do eight calculations in parallel.
[00:33:37.120 --> 00:33:41.600]   In that case, I have only one GPU. So actually, I don't
[00:33:41.600 --> 00:33:46.080]   repeat it. But I mean, I just, I could have just put it into
[00:33:46.080 --> 00:33:49.520]   bracket, but this will let it work if I have like, if I'm on
[00:33:49.520 --> 00:33:54.720]   a TPU v3.8, it will make it work faster because I will do
[00:33:54.720 --> 00:33:59.440]   eight inferences at a time. Something that's a bit tricky,
[00:33:59.440 --> 00:34:02.160]   we're going to see after. So yeah, I tokenize it. So
[00:34:02.160 --> 00:34:05.680]   tokenize, you see, it's basically, it looked at my text
[00:34:05.680 --> 00:34:08.240]   and it transformed it into tokens like you would have with
[00:34:08.240 --> 00:34:12.880]   GPT. And then the rest, it's padded. My one is just a padding
[00:34:12.880 --> 00:34:16.400]   token. And you see zero is the start, two is the end of my
[00:34:16.400 --> 00:34:20.960]   sentence. One is just the pads. And here, the attention mask
[00:34:20.960 --> 00:34:24.000]   is basically all the zeros whenever I finished my sentence.
[00:34:24.000 --> 00:34:27.600]   Means no need to look at it. Don't calculate the loss based
[00:34:27.600 --> 00:34:35.200]   on that. So here is the short function is the one I talked
[00:34:35.200 --> 00:34:39.760]   about is the one of a list of tokens. So imagine that I have
[00:34:39.760 --> 00:34:44.160]   like eight devices here, it would be a list of eight, eight
[00:34:44.160 --> 00:34:50.080]   tokens, let's say, or eight token prompts. And what I do, I
[00:34:50.080 --> 00:34:55.120]   cut it into eight. So maybe I have 16, I would put two on each
[00:34:55.120 --> 00:34:58.480]   device. I need directly put it into the device. So it's
[00:34:58.480 --> 00:35:01.440]   something that to me was a bit hard at the start. What are the
[00:35:01.440 --> 00:35:04.960]   cool functions I should use? And you have a lot of interesting
[00:35:04.960 --> 00:35:07.600]   functions in flags and all, and they are not all at the same
[00:35:07.600 --> 00:35:11.520]   place. And they may be tricky to find. And the way you find
[00:35:11.520 --> 00:35:15.760]   them is you look at a cool script from other people. And
[00:35:15.760 --> 00:35:18.640]   like, oh, there's that function. I wonder what it's for. And it
[00:35:18.640 --> 00:35:20.960]   does a lot of smart things behind the scene.
[00:35:23.520 --> 00:35:27.360]   So here, for example, I say I want 64 prediction. I'm going
[00:35:27.360 --> 00:35:29.600]   to start running the function because it's going to be a bit
[00:35:29.600 --> 00:35:34.160]   slow. I should have added a timer because it's very
[00:35:34.160 --> 00:35:40.320]   interesting what happens. I had my Pmap function. So you see my
[00:35:40.320 --> 00:35:47.440]   P generate, P decode. This is a Pmap function. And what happens
[00:35:47.440 --> 00:35:51.440]   the first time you see now it's blocked at that line. And it's
[00:35:51.440 --> 00:35:56.880]   compiling that function. And it takes a little while to compile
[00:35:56.880 --> 00:36:00.720]   it. But what is cool is once it's compiled, it's really fast
[00:36:00.720 --> 00:36:05.520]   to run. So it compiled it. It must have run it. Now it's using
[00:36:05.520 --> 00:36:09.040]   the PD code from my VQGAN. And it's stuck there, and it's
[00:36:09.040 --> 00:36:13.440]   compiling it. And you see, so it's been stuck. Like now it
[00:36:13.440 --> 00:36:16.160]   went to one and took a while. And you see now it's moving much
[00:36:16.160 --> 00:36:22.320]   faster. So I have one maybe-- it went from maybe the first one
[00:36:22.320 --> 00:36:25.680]   was, I don't know, 20, 30 seconds. And now it's almost one
[00:36:25.680 --> 00:36:28.800]   per second. You will see the iteration. It's slowly going
[00:36:28.800 --> 00:36:33.440]   down. I'm going to look quickly at the code while it's running.
[00:36:33.440 --> 00:36:39.440]   So here what I say, I want to do, let's say, 20-- let's say I
[00:36:39.440 --> 00:36:43.600]   want to do 16 predictions. And let's say if I had eight
[00:36:43.600 --> 00:36:47.920]   devices, I'm going to say, OK, I have eight devices. So I need
[00:36:47.920 --> 00:36:51.040]   to run the loop on it twice. Because each time I'm going to
[00:36:51.040 --> 00:36:54.400]   run it, I'm going to have eight predictions, one on each device.
[00:36:54.400 --> 00:37:02.320]   And what do I do? I need a key, which is the key that-- it's my
[00:37:02.320 --> 00:37:06.720]   random number generator. And that key is going to be used,
[00:37:06.720 --> 00:37:11.840]   basically, to do my sampling of the probability distribution.
[00:37:11.840 --> 00:37:15.440]   So when I do generate, remember I have basically a probability
[00:37:15.440 --> 00:37:19.040]   distribution of tokens for my images. I need to sample from
[00:37:19.040 --> 00:37:24.080]   it. And the way you do it in JAX, you need to give it-- you
[00:37:24.080 --> 00:37:27.360]   need to give it a number that is going to be basically kind of
[00:37:27.360 --> 00:37:30.480]   your seed. It's not necessarily your seed. It's a random number
[00:37:30.480 --> 00:37:34.720]   generator. And the problem is if you give the same number to
[00:37:34.720 --> 00:37:37.920]   each of your devices, you will have the same generation on all
[00:37:37.920 --> 00:37:43.040]   the devices. So here what I do each time I create a new key.
[00:37:43.040 --> 00:37:46.160]   And then there's another cool function, which is simple
[00:37:46.160 --> 00:37:50.000]   function, but it's called shardprngkey. And basically,
[00:37:50.000 --> 00:37:53.520]   what does it do? It took my subkey. So let's say my subkey
[00:37:53.520 --> 00:37:57.600]   was a random number, which was 25. What it does, it takes that
[00:37:57.600 --> 00:38:01.760]   number, and it uses it to create eight new numbers, like I have
[00:38:01.760 --> 00:38:05.440]   eight devices. So it's going to send a new unique numbers to
[00:38:05.440 --> 00:38:09.200]   each of the devices. Those things are not always
[00:38:09.200 --> 00:38:12.480]   straightforward, but you see it in some code, and then you just
[00:38:12.480 --> 00:38:16.000]   put it back. And if I had to redo some code in JAX, I would
[00:38:16.000 --> 00:38:19.440]   just be inspired from the same program so that I don't have to
[00:38:19.440 --> 00:38:24.000]   think again about it. When you generate the image, there's a--
[00:38:24.000 --> 00:38:27.040]   in that model, there's a beginning of sequence character.
[00:38:27.040 --> 00:38:31.920]   There's a special token we don't need. So I remove it. Then
[00:38:31.920 --> 00:38:35.920]   this is my VecuGAN model. It's going to decode it. So it's
[00:38:35.920 --> 00:38:40.880]   going to go from that sequence of token to images. After that,
[00:38:40.880 --> 00:38:44.080]   basically, I want the number to be between zero and one, and I
[00:38:44.080 --> 00:38:47.120]   want to-- you know you want the correct format for images.
[00:38:47.120 --> 00:38:50.640]   That's all it is. It's an I multiplied by 255, and those
[00:38:50.640 --> 00:38:56.320]   are int. So this is like RGB images, basically. So at that
[00:38:56.320 --> 00:39:01.680]   step, I had 64 predictions. We said we wanted 64 predictions.
[00:39:02.480 --> 00:39:06.080]   And now what I'm going to do, I have my CLIP model. So the
[00:39:06.080 --> 00:39:12.080]   processor is the same thing. It's just a function that
[00:39:12.080 --> 00:39:15.040]   prepares the input. For example, in CLIP, they want it to be
[00:39:15.040 --> 00:39:20.400]   resized to 228. They want it to be normalized to maybe zero and
[00:39:20.400 --> 00:39:24.640]   between 0.5 and 0.5. It's not exactly that. There's a
[00:39:24.640 --> 00:39:27.920]   different normalization parameter. But use the processor
[00:39:27.920 --> 00:39:31.920]   function. It does all what is needed. CLIP, basically, you
[00:39:31.920 --> 00:39:40.720]   give it that input, which was my text, and also the tokens. So
[00:39:40.720 --> 00:39:44.720]   CLIP inputs at that moment contains both the text, which I
[00:39:44.720 --> 00:39:48.720]   gave, which was-- what did I give as text? The Eiffel Tower
[00:39:48.720 --> 00:39:53.760]   on the moon. So basically, CLIP inputs contain both the text
[00:39:53.760 --> 00:39:59.120]   and contains the tokens, the images formatted in a certain
[00:39:59.120 --> 00:40:02.640]   way. And the logits, basically, is the score. How does it match
[00:40:02.640 --> 00:40:07.920]   that prompt? And then what we can do, we sort them based on
[00:40:07.920 --> 00:40:11.920]   what he thinks is the highest score. And we display the image
[00:40:11.920 --> 00:40:15.440]   and we show what score it found for that image is. So you're
[00:40:15.440 --> 00:40:19.840]   going to see why we do a lot of inferences now. This is the one
[00:40:19.840 --> 00:40:24.320]   that, to him, had the best score. I don't think it's-- you
[00:40:24.320 --> 00:40:28.160]   don't have to agree on the time with CLIP. But basically,
[00:40:28.160 --> 00:40:29.600]   that's the one you rank the highest.
[00:40:29.600 --> 00:40:33.920]   I agree with it. I like it.
[00:40:33.920 --> 00:40:36.240]   Yeah, it's not necessarily too bad.
[00:40:36.240 --> 00:40:38.160]   I don't know about the audience.
[00:40:38.160 --> 00:40:42.960]   This one is not bad for the Eiffel Tower, but it's clearly
[00:40:42.960 --> 00:40:45.840]   not on the moon. I don't think they have that blue sky over
[00:40:45.840 --> 00:40:50.880]   there right now. And then you see-- so here, you have a way
[00:40:50.880 --> 00:40:56.240]   to see what CLIP think of the 64 images, and based on the best
[00:40:56.240 --> 00:41:01.440]   to worst. And you see why you do a lot. It's like, if I go
[00:41:01.440 --> 00:41:06.720]   lower and if I go really to the end, they're going to be very
[00:41:06.720 --> 00:41:11.520]   bad. Like, I don't see any Eiffel Tower here. But they
[00:41:11.520 --> 00:41:16.240]   become worse and worse, right? Like, here, I don't really know
[00:41:16.240 --> 00:41:20.800]   what that is. But somehow, it was created. You see, it has
[00:41:20.800 --> 00:41:24.800]   no-- nothing very interesting. So all the last ones are
[00:41:24.800 --> 00:41:28.400]   non-relevant. And people use CLIP basically for that, because
[00:41:28.400 --> 00:41:32.400]   when you generate random images, a lot of them become really
[00:41:32.400 --> 00:41:36.400]   bad, and you want to take them out. You keep only the best. So
[00:41:36.400 --> 00:41:39.360]   at this point, you have nothing related to the Eiffel Tower.
[00:41:40.400 --> 00:41:43.280]   If it's a more simple prompt, like if I had put view of the
[00:41:43.280 --> 00:41:47.200]   beach under the sunset, it works pretty well. Most of them would
[00:41:47.200 --> 00:41:50.720]   be good, but some would be bad. There's also a way to make it
[00:41:50.720 --> 00:41:56.960]   better. Is right now, you see, I used my generate-- my top key,
[00:41:56.960 --> 00:42:02.880]   top key, I put it to none. If I had put it to some value, you
[00:42:02.880 --> 00:42:06.720]   know, maybe a lower value, it could potentially be better.
[00:42:08.320 --> 00:42:11.680]   Now, I decide to sample from a token that has an extremely low
[00:42:11.680 --> 00:42:14.800]   probability. I still accept to sample from it.
[00:42:14.800 --> 00:42:19.440]   So that is all the inference work. I don't know if there's
[00:42:19.440 --> 00:42:20.400]   any question on that.
[00:42:20.400 --> 00:42:26.960]   There's one question. So encoded images generates n number of
[00:42:26.960 --> 00:42:31.280]   encodings, and they're asking, what is gen_top_k? Here are the
[00:42:31.280 --> 00:42:32.400]   encodings rated.
[00:42:35.760 --> 00:42:38.240]   Sorry, can you repeat? I'm not sure I--
[00:42:38.240 --> 00:42:42.000]   Sure. So the question is, what is gen_top_k?
[00:42:42.000 --> 00:42:47.440]   Oh, OK. OK, OK. So those are typically, they call it top_k
[00:42:47.440 --> 00:42:51.040]   and top_p. But in my program, I already had a top_k and top_p,
[00:42:51.040 --> 00:42:55.120]   so I added gen. It's for the generator, and it's for the
[00:42:55.120 --> 00:43:01.200]   generator, basically. Whenever it doesn't get immediately a
[00:43:01.200 --> 00:43:04.480]   patch on an image as a prediction, it gets a
[00:43:04.480 --> 00:43:08.080]   distribution of probability. And basically, let's say I had
[00:43:08.080 --> 00:43:14.480]   to put gen_top_k equal 20. What it means is I want you to look
[00:43:14.480 --> 00:43:19.920]   only at the 20 likeliest patches and sample from it. So let's
[00:43:19.920 --> 00:43:22.800]   say I have the first sample. It's a blue patch. It says
[00:43:22.800 --> 00:43:27.600]   there's 10% of chance. That's what you need to show. Maybe
[00:43:27.600 --> 00:43:31.920]   the-- then there's the different patch. It says there's 5%. And
[00:43:31.920 --> 00:43:35.600]   in total, it gives a probability for maybe you have 10,000
[00:43:35.600 --> 00:43:40.800]   patches. When I say gen_top_k equal 20, I say, OK, I want you
[00:43:40.800 --> 00:43:44.800]   to choose one that is based on the probabilities, but choose
[00:43:44.800 --> 00:43:50.320]   only from the 20 best ones, the 20 likeliest one according to
[00:43:50.320 --> 00:43:53.200]   the model. And gen_top_p is a bit similar, but it's
[00:43:53.200 --> 00:43:58.640]   probability. So I say, for example, 0.9. So I say, OK,
[00:43:59.600 --> 00:44:03.040]   order all your probabilities for each patch from the
[00:44:03.040 --> 00:44:08.240]   likeliest to the less likeliest. And I want you to stop when
[00:44:08.240 --> 00:44:12.560]   you have 90% of chances to have selected the right one. So all
[00:44:12.560 --> 00:44:14.800]   the ones that have very low probability, I want you to
[00:44:14.800 --> 00:44:18.640]   never sample from them. And you could say, OK, why don't I put
[00:44:18.640 --> 00:44:22.400]   gen_top_k equal 1 and just show me the one that is the
[00:44:22.400 --> 00:44:25.520]   likeliest all the time? And the problem is, like, for each
[00:44:25.520 --> 00:44:29.440]   prompt, I would have only one prediction. And I want to have
[00:44:29.440 --> 00:44:33.360]   a lot of images. And also, when you have only one prediction,
[00:44:33.360 --> 00:44:36.800]   typically your model basically is going to take less risk. And
[00:44:36.800 --> 00:44:39.600]   it will be like uniform background. And it's not always
[00:44:39.600 --> 00:44:42.400]   the best choice. So you will need to be able to choose from
[00:44:42.400 --> 00:44:46.640]   at least, I don't know, 100 maybe different patches or
[00:44:46.640 --> 00:44:50.000]   sometimes 1,000. For some reason, people like to choose
[00:44:50.000 --> 00:44:55.280]   like 128 to 56. You know, we get used to choosing multiple on
[00:44:55.280 --> 00:44:58.480]   two. It doesn't necessarily make sense. It doesn't have any
[00:44:58.480 --> 00:45:04.080]   advantage. But I know people like to choose multiple of two
[00:45:04.080 --> 00:45:07.840]   for everything, even when it's not necessarily relevant.
[00:45:07.840 --> 00:45:09.920]   Sanyam Bhutani: Maybe it makes good NFTs.
[00:45:09.920 --> 00:45:12.960]   Dr. Olivier Grellier: Yeah, maybe, maybe.
[00:45:12.960 --> 00:45:17.360]   Sanyam Bhutani: So there's another question. Thanks for
[00:45:17.360 --> 00:45:20.480]   that answer. There's another question on training. So in
[00:45:20.480 --> 00:45:24.400]   CNNs and RNNs, the question is, you usually look at gradients.
[00:45:24.400 --> 00:45:28.240]   But in transformers, we mostly just look at the loss value. So
[00:45:28.240 --> 00:45:31.680]   because of this, did you face any problems in training Dolly
[00:45:31.680 --> 00:45:34.720]   or any other problems you faced while you were trying to train
[00:45:34.720 --> 00:45:35.280]   this model?
[00:45:35.280 --> 00:45:38.240]   Dr. Olivier Grellier: You still look at both. So I was going to
[00:45:38.240 --> 00:45:43.600]   go to the model, but maybe we can go to the training code.
[00:45:43.600 --> 00:45:52.640]   Can I zoom it in? I'm not sure. It's too small for you, right?
[00:45:52.640 --> 00:45:57.120]   Sanyam Bhutani: I think it's command plus to zoom in. I don't
[00:45:57.120 --> 00:45:59.280]   use VS code a lot. Okay.
[00:45:59.280 --> 00:46:05.600]   Or it might be command shift plus to zoom in.
[00:46:05.600 --> 00:46:08.880]   Dr. Olivier Grellier: Command shift plus. Okay.
[00:46:08.880 --> 00:46:16.960]   Now the problem is all my keys are all changed because I have,
[00:46:16.960 --> 00:46:20.880]   I reprogram it a bit differently. Maybe there's a short
[00:46:20.880 --> 00:46:21.680]   bit in here of...
[00:46:21.680 --> 00:46:25.040]   Sanyam Bhutani: Someone mentioned it's clear. I think we
[00:46:25.040 --> 00:46:25.680]   can continue.
[00:46:25.680 --> 00:46:31.120]   Dr. Olivier Grellier: Okay. Oops. Why did it become smaller?
[00:46:31.120 --> 00:46:37.680]   All right. Sorry about that. I need to find... But basically,
[00:46:37.680 --> 00:46:41.280]   you actually, even in the training code, you have a loss
[00:46:41.280 --> 00:46:45.680]   and you have a gradient all the time. The way it works, we're
[00:46:45.680 --> 00:46:51.040]   going to look at the loss quickly. But if I go to loss
[00:46:51.040 --> 00:46:54.640]   function, we have just my cross entropy, the one we were
[00:46:54.640 --> 00:47:02.240]   talking about. And then I have from that loss, when I train,
[00:47:02.240 --> 00:47:05.760]   I compute the loss. We're going to look a bit more at it if we
[00:47:05.760 --> 00:47:09.200]   have the time. But then I calculate the gradient. So
[00:47:09.200 --> 00:47:12.320]   typically, you always calculate a loss during your training.
[00:47:12.320 --> 00:47:16.560]   Once you calculate a loss, you calculate the gradient of all
[00:47:16.560 --> 00:47:20.080]   your parameters based on that to know how much do they need to
[00:47:20.080 --> 00:47:25.040]   change to minimize that loss. So we also do the same. We
[00:47:25.040 --> 00:47:29.120]   calculate that. We calculate the gradients. And then we apply
[00:47:29.120 --> 00:47:34.640]   those gradients. So we modify the parameters so that they go
[00:47:34.640 --> 00:47:38.400]   into the direction of the gradient and minimize the loss.
[00:47:38.400 --> 00:47:41.120]   So we have both the loss and gradient here.
[00:47:41.120 --> 00:47:49.760]   Maybe we can look quickly at the model because I think it's
[00:47:49.760 --> 00:47:55.760]   pretty interesting. If I look at the model that we have, it's
[00:47:55.760 --> 00:48:00.800]   not many lines of code. It's extremely short, right? Why is
[00:48:00.800 --> 00:48:05.440]   it not many lines of code? Because basically, we just use a
[00:48:05.440 --> 00:48:09.520]   model that was implemented by HuggingFace. And this is the
[00:48:09.520 --> 00:48:15.680]   model from HuggingFace, which is like the bot. But it's a long
[00:48:15.680 --> 00:48:19.200]   code. And there's lots of things in it that happen.
[00:48:19.360 --> 00:48:24.160]   In transformers, the way they do it, they have some function
[00:48:24.160 --> 00:48:28.240]   initWeights. Typically, you have to create really two
[00:48:28.240 --> 00:48:31.600]   functions, which is initWeights, which is how you're going to
[00:48:31.600 --> 00:48:34.240]   define the weights initially, which is something you always
[00:48:34.240 --> 00:48:39.040]   need to do in JAX. And then you have your forward pass.
[00:48:39.040 --> 00:48:45.040]   But when we looked at the model, we realized that basically,
[00:48:45.040 --> 00:48:48.080]   this is the function they have. And in the setup, they
[00:48:48.080 --> 00:48:53.120]   initialize what layers they're going to use. And there's the
[00:48:53.120 --> 00:48:57.120]   size of the vocabulary, the model, they have some embeddings.
[00:48:57.120 --> 00:49:02.000]   So typically, in bot, even if I translate from English to
[00:49:02.000 --> 00:49:05.440]   French, I use the same vocabulary. And then they have
[00:49:05.440 --> 00:49:09.040]   an encoder and a decoder that use both the same configuration
[00:49:09.040 --> 00:49:14.560]   parameters, the same embedding tokens. So maybe what we can do,
[00:49:14.560 --> 00:49:18.800]   we just create some embedding that are different for the text
[00:49:18.800 --> 00:49:23.200]   and for the images. And then the decoder and the encoder, we
[00:49:23.200 --> 00:49:25.840]   make sure they're fully separate. They don't use the
[00:49:25.840 --> 00:49:30.800]   same configuration. And the reason is our decoder, our image
[00:49:30.800 --> 00:49:33.680]   tokens, our image patch, they have a different dimension.
[00:49:33.680 --> 00:49:39.520]   Maybe there's 16,000 possibilities. Just for text,
[00:49:39.520 --> 00:49:42.480]   maybe there's only a vocabulary of 8,000.
[00:49:43.920 --> 00:49:47.280]   So there's a few differences. Also, maybe the length for our
[00:49:47.280 --> 00:49:51.200]   image, we need to, we have, I think, 256 tokens for an image.
[00:49:51.200 --> 00:49:55.920]   And maybe, you know, this is not the same that you will want
[00:49:55.920 --> 00:50:01.360]   for the encoder. So the way we built it, really, is we just
[00:50:01.360 --> 00:50:05.040]   inherit from it. And you see, we create the shared, we kept the
[00:50:05.040 --> 00:50:08.960]   same name. So all the function work out of the box, which is
[00:50:08.960 --> 00:50:12.640]   the embedding of the encoder. But then we create another one
[00:50:12.640 --> 00:50:16.000]   for the decoder. And you see, the only thing that changes,
[00:50:16.000 --> 00:50:22.400]   like vocab size, becomes image vocab size. That's all. Then
[00:50:22.400 --> 00:50:26.800]   the encoder uses those embeddings. And the decoder, we
[00:50:26.800 --> 00:50:32.560]   make it use those embeddings. But there's not many more
[00:50:32.560 --> 00:50:35.760]   secrets. Then it's just about setting the right configuration,
[00:50:35.760 --> 00:50:39.440]   making sure you pass the right tokens. But that's all that was
[00:50:39.440 --> 00:50:44.160]   needed for building our model, actually, which is pretty cool.
[00:50:44.160 --> 00:50:51.440]   There's one question on how do you load the pre-trained weights
[00:50:51.440 --> 00:50:54.880]   for the BART or VQGAN model when you're initializing them?
[00:50:54.880 --> 00:51:04.320]   Okay. For the VQGAN model, basically, whether it's for BART
[00:51:04.320 --> 00:51:07.680]   or for the VQGAN, we have a function in the interface, which
[00:51:07.680 --> 00:51:13.840]   is called from_pretrained. That's all it is. But then there's
[00:51:13.840 --> 00:51:16.640]   another function. So let's say you have your model that
[00:51:16.640 --> 00:51:21.040]   initially doesn't know anything. And you start training it. And
[00:51:21.040 --> 00:51:24.480]   you built it to beat the hugging face way. So there's a few
[00:51:24.480 --> 00:51:28.560]   things to see, but it's not necessarily that hard. But then
[00:51:28.560 --> 00:51:31.600]   you can just call that function save_pretrained, and it saves
[00:51:31.600 --> 00:51:35.680]   the weights in a folder. And then what you do, you do a
[00:51:35.680 --> 00:51:38.720]   from_pretrained from that folder, and it loads it for you.
[00:51:38.720 --> 00:51:41.280]   So you don't really have to think too much about it.
[00:51:41.280 --> 00:51:46.400]   Then the weights, actually, will be inside model. They will
[00:51:46.400 --> 00:51:55.920]   be called model.params. That's how it works. So the model,
[00:51:55.920 --> 00:51:59.680]   typically, in a hugging face, they have the parameters inside
[00:51:59.680 --> 00:52:03.040]   of it. But you can also pass parameters, because you need to
[00:52:03.040 --> 00:52:07.120]   be the stateful versus stateless of Shagg's force you a bit to
[00:52:07.120 --> 00:52:10.080]   write code, where you pass explicitly the parameters. And
[00:52:10.080 --> 00:52:13.200]   when you pass them, it will basically remove the parameters
[00:52:13.200 --> 00:52:16.560]   that it has stated to a model and set the new one for it.
[00:52:16.560 --> 00:52:19.520]   We can look a bit at some cool pieces of code, if you want,
[00:52:19.520 --> 00:52:26.960]   into the training model. I always like to look at the
[00:52:26.960 --> 00:52:30.640]   import, because it gives you an idea of what is being used. So
[00:52:30.640 --> 00:52:34.480]   all the first section is boring. It's like classical stuff.
[00:52:34.480 --> 00:52:39.440]   Data sets, this is from a hugging face, basically. It's a
[00:52:39.440 --> 00:52:43.680]   way to load the data set. But the Shagg's stuff, so you have
[00:52:43.680 --> 00:52:47.280]   Shagg's and Shagg's.numpy. We're going to play with array.
[00:52:47.280 --> 00:52:51.520]   Then you have something interesting here. It's optacs.
[00:52:51.520 --> 00:52:55.680]   So optacs, those are-- I don't know. Have you used it already?
[00:52:55.680 --> 00:52:58.240]   Not in the sessions, not yet.
[00:52:58.960 --> 00:53:03.360]   So optacs, those are optimizers built by DeepMind, and they are
[00:53:03.360 --> 00:53:06.640]   well integrated into Flux. So if you go into Flux tutorials,
[00:53:06.640 --> 00:53:09.600]   often they have their own optimizers, but they are kind of
[00:53:09.600 --> 00:53:14.560]   switching towards optacs. So I think a lot of things are well
[00:53:14.560 --> 00:53:18.240]   integrated for optacs optimizers. And optacs optimizers
[00:53:18.240 --> 00:53:21.360]   is very nice, because you say, OK, I want Adam. It's like in
[00:53:21.360 --> 00:53:24.400]   PyTorch. Say I just want Adam, and I want those parameters. And
[00:53:24.400 --> 00:53:27.760]   it gives you a function, and it gives you the state of the
[00:53:27.760 --> 00:53:31.120]   optimizer. So the state will contain the gradients, things
[00:53:31.120 --> 00:53:34.960]   like that. And you will be able to apply it very easily.
[00:53:34.960 --> 00:53:39.840]   We're going to see quickly how-- I'm going to try to go to the
[00:53:39.840 --> 00:53:45.280]   pieces I find often. You have some jugs here. OK, unreplicate,
[00:53:45.280 --> 00:53:49.360]   replicate. We saw a bit. It's like giving the parameters to
[00:53:49.360 --> 00:53:52.240]   all your devices. And unreplicate is the opposite. I have
[00:53:52.240 --> 00:53:57.120]   my parameters maybe on my eight TPUs, and I want to bring them
[00:53:57.120 --> 00:54:01.040]   back to CPU. And in jugs, maybe my parameters, they're all the
[00:54:01.040 --> 00:54:05.360]   same across all these TPUs, because all the operations are
[00:54:05.360 --> 00:54:08.240]   happening parallel and are the same when you update the
[00:54:08.240 --> 00:54:12.080]   parameters or things like that. So basically, when you do
[00:54:12.080 --> 00:54:15.280]   unreplicate, you say, just take the one from the first TPU, put
[00:54:15.280 --> 00:54:20.000]   it on CPU. Here, this is something I used to save models
[00:54:20.000 --> 00:54:23.600]   and all. Train state is what I want to show. I think it's cool.
[00:54:24.240 --> 00:54:28.480]   GetMetrix is a handy function, which is like you have an array
[00:54:28.480 --> 00:54:31.440]   where you accumulate metrics, and you want to bring it back
[00:54:31.440 --> 00:54:36.320]   into a jugs array where you can optimize things. But we're
[00:54:36.320 --> 00:54:39.440]   going to look mainly at the train state. Have you used train
[00:54:39.440 --> 00:54:40.640]   state already or not yet?
[00:54:40.640 --> 00:54:42.080]   >> Not yet.
[00:54:42.080 --> 00:54:44.560]   >> You have looked at it?
[00:54:44.560 --> 00:54:48.080]   >> I don't think so. Not yet.
[00:54:48.080 --> 00:54:51.600]   >> Train state, so it's a cool way to organize all your
[00:54:51.600 --> 00:54:55.600]   parameters while you're training a model. So the way it
[00:54:55.600 --> 00:54:58.880]   works, actually, we should look at the train state function.
[00:54:58.880 --> 00:55:02.880]   It's even more interesting, which I opened here. Train
[00:55:02.880 --> 00:55:07.920]   state is something that belongs to Flux training. So the way
[00:55:07.920 --> 00:55:10.960]   they do it, because you always have to pass your parameters
[00:55:10.960 --> 00:55:14.960]   explicitly, your code can become messy. You have parameters
[00:55:14.960 --> 00:55:18.160]   everywhere. You have like values, jugs values everywhere
[00:55:18.160 --> 00:55:21.760]   that you update. So train state is a way to keep it well
[00:55:21.760 --> 00:55:26.720]   organized and to put everything well together. So imagine it's
[00:55:26.720 --> 00:55:30.240]   kind of a data class, and it keeps track of a few things. So
[00:55:30.240 --> 00:55:33.920]   the training step, then there's the apply function. So the
[00:55:33.920 --> 00:55:38.720]   apply function is typically your model for what? You have your
[00:55:38.720 --> 00:55:44.960]   parameters, and then TX is your optimizer. So this is your --
[00:55:44.960 --> 00:55:49.280]   that's going to be, for example, Adam. And opt state is your
[00:55:49.280 --> 00:55:53.840]   optimizer state. So, for example, you chose Adam. It
[00:55:53.840 --> 00:55:58.240]   needs to be -- to keep track of the gradients and to keep track
[00:55:58.240 --> 00:56:03.360]   of momentums and things like that to apply the updates. But
[00:56:03.360 --> 00:56:06.000]   then you have cool stuff. You have a function create. What it
[00:56:06.000 --> 00:56:10.080]   does is it will initialize the optimizer state initially. So
[00:56:10.080 --> 00:56:13.040]   you remember in Flux, you always need to initialize the
[00:56:13.040 --> 00:56:15.840]   state. So it will do it for you, for the optimizer. So I will
[00:56:15.840 --> 00:56:18.720]   just say, okay, I want the update -- I want the Adam
[00:56:18.720 --> 00:56:22.480]   optimizer. And then you have the -- you just call it apply
[00:56:22.480 --> 00:56:26.080]   gradients function. And the apply gradient function is like
[00:56:26.080 --> 00:56:29.760]   a magic function that does everything that you need. And
[00:56:29.760 --> 00:56:34.000]   what it does, basically, it calls my optimizer, my
[00:56:34.000 --> 00:56:38.320]   optimizer from optax. So this is really well integrated with
[00:56:38.320 --> 00:56:42.960]   optax, that, by the way. So my optimizer from optax will
[00:56:42.960 --> 00:56:47.200]   calculate the updates. And the updates, what it is, it's like
[00:56:47.200 --> 00:56:50.800]   how much am I supposed to change each weight to go in the
[00:56:50.800 --> 00:56:55.600]   direction of the gradient based on that learning rate, maybe.
[00:56:55.600 --> 00:56:59.760]   And then you have the new opt state, which is, okay, let's
[00:56:59.760 --> 00:57:03.600]   keep track of the new momentums. And then you have a new
[00:57:03.600 --> 00:57:08.000]   function in optax, which is apply updates. And apply
[00:57:08.000 --> 00:57:10.960]   updates, it just basically you have your parameters, and you
[00:57:10.960 --> 00:57:14.960]   do your parameter plus the update. We could look at it to
[00:57:14.960 --> 00:57:19.440]   verify. But that's all it does. You see, you have a tree
[00:57:19.440 --> 00:57:22.320]   multimap here, and that's p plus u. It just adds your
[00:57:22.320 --> 00:57:28.160]   updates to it. And then self.replace is like, okay, my
[00:57:28.160 --> 00:57:32.800]   step now is step plus one. My parameters have been updated.
[00:57:32.800 --> 00:57:35.920]   And my optimizer state has been updated. And I can add stuff to
[00:57:35.920 --> 00:57:39.840]   it, which is cool. So if I look quickly at mine, I added a few
[00:57:39.840 --> 00:57:43.440]   things to it. I have a random number generator for the
[00:57:43.440 --> 00:57:47.760]   dropout. So it's well organized. It's there. I have the epoch. I
[00:57:47.760 --> 00:57:50.320]   like to keep track of the epoch. I like to keep track of the
[00:57:50.320 --> 00:57:53.920]   training time, training sample. So you just add random things
[00:57:53.920 --> 00:57:58.240]   to it. And what is cool is when I use it, I'm going to go at
[00:57:58.240 --> 00:58:06.720]   another state. Let's go to my training function quickly.
[00:58:06.720 --> 00:58:10.400]   Because what is nice, all what I showed you in Optax, I know how
[00:58:10.400 --> 00:58:13.120]   it works now. But when I used it, I didn't know how it worked.
[00:58:13.120 --> 00:58:16.880]   And what is cool, I didn't have to know, because I'm using all
[00:58:16.880 --> 00:58:20.720]   those convenient functions that do everything I need behind the
[00:58:20.720 --> 00:58:24.560]   scenes. So you have the approach of let's learn really how Jaxx
[00:58:24.560 --> 00:58:28.240]   works, which is really cool, which I'm doing only now. And
[00:58:28.240 --> 00:58:31.680]   then you have the approach of, OK, let's build things with
[00:58:31.680 --> 00:58:39.040]   magic functions. And I like the magic functions of Jaxx. You
[00:58:39.040 --> 00:58:42.480]   just need to find them. But you see here, for example, this is
[00:58:42.480 --> 00:58:47.520]   my training step. It's really, really small. This is all the
[00:58:47.520 --> 00:58:51.120]   training step, right? It's extremely small. I compute my
[00:58:51.120 --> 00:59:01.440]   loss. So I took my labels. I get my logits by applying basically
[00:59:01.440 --> 00:59:06.720]   my model forward function on my batch, giving the parameters,
[00:59:06.720 --> 00:59:11.200]   giving my dropout. I say I'm training. When you see your
[00:59:11.200 --> 00:59:13.600]   training, basically, it's to make sure you calculate the
[00:59:13.600 --> 00:59:16.640]   gradients. It can be maybe to make sure you pass the dropout.
[00:59:16.640 --> 00:59:19.840]   Like in PowerTorch, when you have a mode, you don't pass any
[00:59:19.840 --> 00:59:23.920]   dropout. I have my loss function. In my case, it's a
[00:59:23.920 --> 00:59:28.400]   cross entropy. I defined it here. You can see in OPTAX. Why
[00:59:28.400 --> 00:59:34.160]   OPTAX? Because then it does all the magic for me. And then I
[00:59:34.160 --> 00:59:38.240]   have those functions. I calculate my gradients. And once
[00:59:38.240 --> 00:59:42.080]   I calculate my gradients, it's on all the devices I calculate
[00:59:42.080 --> 00:59:47.280]   the mean across the entire batch. And then I just call that
[00:59:47.280 --> 00:59:50.640]   function that we just saw, apply gradients, which is basically
[00:59:50.640 --> 00:59:53.760]   going to call the optimizer, calculate the necessary updates,
[00:59:53.760 --> 00:59:57.680]   replace the parameters, and update the optimizer state.
[00:59:57.680 --> 01:00:04.160]   So I think that's the interesting part. The evolution
[01:00:04.160 --> 01:00:08.240]   part is very similar. And in the same way, you call the Pmap
[01:00:08.240 --> 01:00:12.240]   function so that it goes faster. The only problem I have with
[01:00:12.240 --> 01:00:15.840]   Pmap, sometimes I have a bug and you wait maybe 10 minutes for
[01:00:15.840 --> 01:00:19.280]   your model to compile. And then you can see that you have a
[01:00:19.280 --> 01:00:25.920]   problem. And maybe it's too late. Or you waited a long time.
[01:00:25.920 --> 01:00:27.520]   I don't know. Do you have any questions so far?
[01:00:27.520 --> 01:00:32.000]   >> There's one question. So far, it's all clear. Christian is
[01:00:32.000 --> 01:00:36.000]   asking, where do you merge parameters from all models into
[01:00:36.000 --> 01:00:37.040]   a single dictionary?
[01:00:37.040 --> 01:00:43.200]   >> Where do I merge the parameters from all models?
[01:00:43.200 --> 01:00:45.040]   >> Or how do you merge them?
[01:00:45.040 --> 01:00:50.720]   >> How do you merge them? So actually, the model for -- if
[01:00:50.720 --> 01:00:54.160]   you're talking about the VQDAN and all, I keep them as separate
[01:00:54.160 --> 01:00:58.000]   models. So I don't merge them at all. I call the two forward
[01:00:58.000 --> 01:01:02.800]   separately. For the encoder and decoder, basically, in the
[01:01:02.800 --> 01:01:06.080]   parameters, you have different keys. It's just a dictionary.
[01:01:06.080 --> 01:01:10.080]   And one key may be called encoder.bias. One key will be
[01:01:10.080 --> 01:01:14.720]   encoder.weight. Or maybe encoder.weight.1, because I have
[01:01:14.720 --> 01:01:21.760]   a lot of feed forward, et cetera. That's how they are
[01:01:21.760 --> 01:01:24.640]   structured. In that case, I actually didn't have to do it
[01:01:24.640 --> 01:01:30.160]   because I'm reusing a model. So I just adapted it and I
[01:01:30.160 --> 01:01:33.040]   subclassed the model. I'm actually building a new model
[01:01:33.040 --> 01:01:37.680]   where we do that. But typically, that's how you organize. You
[01:01:37.680 --> 01:01:41.120]   organize all the parameters with a dictionary.
[01:01:41.120 --> 01:01:48.640]   >> I think that answers. Christian, you can also unmute
[01:01:48.640 --> 01:01:54.400]   yourself if you want to follow. >> Hey, Sandeep. Yeah, I'm on
[01:01:54.400 --> 01:01:57.200]   my phone. >> We can hear you.
[01:01:57.200 --> 01:02:02.160]   >> Okay. Yeah. Okay. Nice. Hey. >> Hey.
[01:02:02.160 --> 01:02:09.280]   >> Yeah. I was very curious about this because it seems
[01:02:09.280 --> 01:02:16.480]   that this model, you're not merging because you have a
[01:02:16.480 --> 01:02:20.080]   bunch of models, but the training is only for one of
[01:02:20.080 --> 01:02:21.760]   them right now. >> That's right.
[01:02:21.760 --> 01:02:27.280]   >> Okay. Yeah. I'm always very curious when you have to merge
[01:02:27.280 --> 01:02:33.280]   two models. >> Okay. If you optimize both at the same
[01:02:33.280 --> 01:02:39.600]   time, you mean? >> Yeah. Because then you have to
[01:02:39.600 --> 01:02:42.640]   kind of merge them somehow. >> Yeah.
[01:02:42.640 --> 01:02:46.640]   >> And I'm always looking for ways to do that easier.
[01:02:46.640 --> 01:02:51.120]   >> Yeah. So here in that case, actually, to train as fast as
[01:02:51.120 --> 01:02:56.400]   possible, you see this is our training pipeline. So the
[01:02:56.400 --> 01:02:59.760]   VQGAN encode the images and we compare to it. In my training
[01:02:59.760 --> 01:03:02.800]   code, you don't see the VQGAN at all. And you don't see clip
[01:03:02.800 --> 01:03:07.520]   at all. So why? Because I have the text. And instead of
[01:03:07.520 --> 01:03:13.200]   passing an image, I preencoded all the images. So I save a lot
[01:03:13.200 --> 01:03:16.480]   of time. I don't need to load the VQGAN at all. So my input
[01:03:16.480 --> 01:03:22.000]   is actually text plus already encoded data, encoded patches.
[01:03:22.000 --> 01:03:25.600]   And I'm just trying to go from that text to that list of
[01:03:25.600 --> 01:03:32.560]   numbers, which makes it very convenient and very fast. If
[01:03:32.560 --> 01:03:37.360]   you have two models, what is tricky is managing the -- they
[01:03:37.360 --> 01:03:40.880]   are going to take more space, obviously. In that case, the
[01:03:40.880 --> 01:03:43.920]   fact I don't have that, I can have probably a much larger
[01:03:43.920 --> 01:03:50.160]   batch. But then you need to know, like, what is your loss?
[01:03:50.160 --> 01:03:53.600]   Do you optimize one and the other? Do you optimize both at
[01:03:53.600 --> 01:03:57.280]   the same time? Is there a risk you try to think of, okay, like
[01:03:57.280 --> 01:04:00.480]   in the GANs, it's always really tricky, right? Okay, if I
[01:04:00.480 --> 01:04:03.520]   optimize this, is there a risk that this one is going to learn
[01:04:03.520 --> 01:04:08.960]   the wrong thing? Those are always difficult. But the
[01:04:08.960 --> 01:04:11.440]   parameters, when it's two different models, I like to keep
[01:04:11.440 --> 01:04:14.880]   them separate. So VQGAN and BART, I keep them completely
[01:04:14.880 --> 01:04:18.400]   separate. VQGAN is one model that can be used by itself. The
[01:04:18.400 --> 01:04:22.480]   BART is a model that can be used by itself. But then, for
[01:04:22.480 --> 01:04:26.400]   example, the encoder and decoder are part of the same model. So
[01:04:26.400 --> 01:04:31.040]   in that case, it's just a dictionary of -- it's just --
[01:04:31.040 --> 01:04:32.840]   they are completely separate. >>
[01:04:32.840 --> 01:04:41.360]   So the other one is constant with respect to the
[01:04:41.360 --> 01:04:44.960]   optimization. So you don't -- basically, you don't need it.
[01:04:44.960 --> 01:04:50.880]   That is nice. >> Yeah. >> Okay. Yeah, that was my
[01:04:50.880 --> 01:04:59.840]   question. >> Yeah. I don't know if there's other
[01:04:59.840 --> 01:05:04.720]   questions. Something cool that I like to do, too, is -- you
[01:05:04.720 --> 01:05:07.520]   know, like you train those big models and they may not work
[01:05:07.520 --> 01:05:10.640]   well or maybe they are training, you're not sure. You need to
[01:05:10.640 --> 01:05:13.280]   make sure from the start to kind of build a pipeline where
[01:05:13.280 --> 01:05:16.960]   you're going to be able to check your model. So make sure you
[01:05:16.960 --> 01:05:20.560]   know you do the basic logging losses and all. And if you can,
[01:05:20.560 --> 01:05:24.000]   you know, at some point you need to save your model and try
[01:05:24.000 --> 01:05:27.200]   playing with it, do some inference. So this model,
[01:05:27.200 --> 01:05:29.920]   initially, you know, it was created something very bad,
[01:05:29.920 --> 01:05:33.840]   random and all. But after training it for a little bit or
[01:05:33.840 --> 01:05:39.120]   maybe for a day, then we put view of the beach by night, view
[01:05:39.120 --> 01:05:42.400]   of the beach during the day. And we see -- we don't recognize
[01:05:42.400 --> 01:05:46.000]   anything yet, but we see when we put view of the beach by night,
[01:05:46.000 --> 01:05:48.960]   it's like very dark colors. So you know it's night. You know,
[01:05:48.960 --> 01:05:52.000]   it becomes very black. And when you put view by the day, it's
[01:05:52.000 --> 01:05:55.360]   like very clear. So like, okay, it's probably starting to learn
[01:05:55.360 --> 01:05:59.200]   something. So I like to have a pipeline. And you know, you
[01:05:59.200 --> 01:06:03.520]   have a loop where every so often I log my model as an
[01:06:03.520 --> 01:06:06.880]   artifact. And then what I can do is like have my collab next to
[01:06:06.880 --> 01:06:10.960]   it. Maybe I have another TPU or I just have a collab where I
[01:06:10.960 --> 01:06:13.760]   load that model and I start playing with it. So as the
[01:06:13.760 --> 01:06:16.480]   model is training from the start, it's good to do inference
[01:06:16.480 --> 01:06:20.400]   quickly. Because maybe the loss is decreasing. Maybe it's not.
[01:06:20.400 --> 01:06:25.120]   But doing quick inference, you get to see what goes wrong or
[01:06:25.120 --> 01:06:28.560]   not. It's always something very useful.
[01:06:28.560 --> 01:06:35.600]   This one question, how are you avoiding mood collapse and noise
[01:06:35.600 --> 01:06:36.240]   in images?
[01:06:36.240 --> 01:06:45.120]   So in the VQGAN, it's a problem that there is in the image
[01:06:45.120 --> 01:06:48.640]   encoder. There is mood collapse. So the faces are not
[01:06:48.640 --> 01:06:53.200]   very good. And there's a lot of issues with it. And the other
[01:06:53.200 --> 01:06:59.440]   way where they use Gumball, which is a technique where you
[01:06:59.440 --> 01:07:06.640]   basically spread a bit more the scores. And what it does, you
[01:07:06.640 --> 01:07:11.040]   try to sample from some more tokens. And little by little,
[01:07:11.040 --> 01:07:14.160]   you decrease that temperature to really sample from the best
[01:07:14.160 --> 01:07:17.440]   one. And during training, you have a bit less mood collapse.
[01:07:17.440 --> 01:07:22.480]   You use a bit more the codebook or the vocabulary that you have.
[01:07:22.480 --> 01:07:27.520]   Our VQGAN, for example, we have a vocabulary of 16,000 tokens.
[01:07:27.520 --> 01:07:31.280]   But I think we use only maybe 10% or 20% of the tokens. Some
[01:07:31.280 --> 01:07:34.880]   never appear. Because we use one that doesn't use Gumball.
[01:07:34.880 --> 01:07:39.360]   That's not optimized. But it's smaller. Then there's ways that
[01:07:39.360 --> 01:07:42.080]   are-- there's some that use interesting technique where the
[01:07:42.080 --> 01:07:47.200]   project-- there's a paper from Google called Improved VQGAN
[01:07:47.200 --> 01:07:49.680]   that has very interesting technique where they project all
[01:07:49.680 --> 01:07:54.320]   the codebook onto a sphere and sample from it. It's a good
[01:07:54.320 --> 01:07:59.120]   technique. On that model itself, the BART model, we don't really
[01:07:59.120 --> 01:08:03.520]   have any problem of mood collapse. I think it's more--
[01:08:03.520 --> 01:08:06.320]   we have a problem there because it's a GAN. And GANs are known
[01:08:06.320 --> 01:08:10.480]   to have that type of problem. Here, we don't really have that
[01:08:10.480 --> 01:08:14.560]   problem. If you have a very diverse set of images, you will
[01:08:14.560 --> 01:08:18.320]   be fine. That's actually a big challenge then. You have your
[01:08:18.320 --> 01:08:21.440]   model that works well. It's you need to spend a lot of time on
[01:08:21.440 --> 01:08:24.640]   the data set and make sure that the set is good. And we have
[01:08:24.640 --> 01:08:28.400]   those watermarks. Find a way to remove-- have no more images
[01:08:28.400 --> 01:08:31.520]   with watermarks. Or you can have predictions that you don't
[01:08:31.520 --> 01:08:35.200]   like. Or maybe not enough illustrations. Or maybe my model
[01:08:35.200 --> 01:08:37.520]   is really good at making T-shirts. Well, maybe I have too
[01:08:37.520 --> 01:08:43.600]   much e-commerce data in my data set. And try to see that while
[01:08:43.600 --> 01:08:47.280]   doing inference is good. So actually, I have a workflow
[01:08:47.280 --> 01:08:51.440]   where twice a day, I do inferences on 100 prompts. And
[01:08:51.440 --> 01:08:54.880]   it's automated. And then I look at the images. I'm like, oh,
[01:08:54.880 --> 01:08:57.360]   it's really good at that, but bad at that. And you get some
[01:08:57.360 --> 01:09:01.440]   insight on the data set on what works, what doesn't work.
[01:09:06.720 --> 01:09:08.320]   Yeah, I think that answers that. Thank you.
[01:09:08.320 --> 01:09:19.920]   The inference collab that I just showed, I don't-- it's not
[01:09:19.920 --> 01:09:24.080]   the one that is on the GitHub repo yet. It will be there
[01:09:24.080 --> 01:09:27.840]   soon. We updated the model. So I'm scared of having too many
[01:09:27.840 --> 01:09:32.960]   times breaking changes. Because now people use, and then there's
[01:09:32.960 --> 01:09:36.640]   a break change line to re-change a lot of things. So I'm going to
[01:09:36.640 --> 01:09:41.920]   do a big break and try not to have too many. And hopefully,
[01:09:41.920 --> 01:09:46.000]   next week, it becomes more stable. And I will put it there.
[01:09:46.000 --> 01:09:49.680]   But I'll be happy to even share that one if you want to play
[01:09:49.680 --> 01:09:51.840]   with it or see how the inference is done.
[01:09:51.840 --> 01:09:58.240]   By the way, what I was talking about, which is cool, you
[01:09:58.240 --> 01:10:01.600]   remember when we did the inference? It took a little
[01:10:01.600 --> 01:10:04.560]   while initially. But let's say if I run it again now,
[01:10:05.360 --> 01:10:08.000]   basically, remember, we had to wait a little while before
[01:10:08.000 --> 01:10:10.640]   doing the first inference. Now it's already doing it
[01:10:10.640 --> 01:10:11.280]   immediately.
[01:10:11.280 --> 01:10:15.440]   Because the model has been compiled. So those are the cool
[01:10:15.440 --> 01:10:19.760]   stuff of Pmap. And if I didn't use Pmap, even though I'm on
[01:10:19.760 --> 01:10:23.520]   only one GPU and I run the inference, it would be much,
[01:10:23.520 --> 01:10:24.640]   much, much slower.
[01:10:24.640 --> 01:10:33.280]   Oh, by the way, I had some-- if you want to optimize this, I
[01:10:33.280 --> 01:10:35.200]   think you could do a little bit better.
[01:10:35.200 --> 01:10:37.120]   Yeah, I want to know.
[01:10:37.120 --> 01:10:42.240]   Because here, Pgenerate is Pmapped, right?
[01:10:42.240 --> 01:10:43.520]   That's right.
[01:10:43.520 --> 01:10:48.880]   But then encoded images, it seems like it's kind of a
[01:10:48.880 --> 01:10:52.080]   structure, right? What is that? Sequences? Sequences is--
[01:10:52.080 --> 01:10:54.560]   It's like an umpire array, I think.
[01:10:54.560 --> 01:10:59.920]   It's an umpire array? So when you execute an operation on
[01:10:59.920 --> 01:11:02.960]   something that is the result of Pmap, that brings it into
[01:11:02.960 --> 01:11:10.080]   the CPU. So when you slice, I think that is sending it to
[01:11:10.080 --> 01:11:13.920]   the host. And in the next ones, you're sending it back to
[01:11:13.920 --> 01:11:18.400]   the machines. So there's a little bit of I/O going there,
[01:11:18.400 --> 01:11:25.360]   which I think the easiest way to optimize it would be to
[01:11:25.360 --> 01:11:31.760]   get this loop and put it into its own Pmap function,
[01:11:31.760 --> 01:11:34.080]   something like that. I mean, if you're interested in going
[01:11:34.080 --> 01:11:34.720]   really, really fast.
[01:11:34.720 --> 01:11:38.320]   Oh, that's good. So you would Pmap the entire thing together?
[01:11:38.320 --> 01:11:39.440]   Yeah.
[01:11:39.440 --> 01:11:45.360]   Okay. Typically, I Pmap the big, slow functions. And then the
[01:11:45.360 --> 01:11:48.560]   small stuff like this, I don't Pmap it. But it's true that
[01:11:48.560 --> 01:11:52.000]   there's potentially some advantage by combining the two.
[01:11:52.000 --> 01:11:54.160]   I don't know.
[01:11:54.160 --> 01:11:58.080]   So it's a device array. You see, it's actually a device array.
[01:11:58.080 --> 01:12:01.360]   So I don't know. Does it-- it probably doesn't go on CPU
[01:12:01.360 --> 01:12:01.840]   then, right?
[01:12:01.840 --> 01:12:06.480]   If you have a device array, that means it went to the host.
[01:12:06.480 --> 01:12:08.480]   Oh, okay.
[01:12:08.480 --> 01:12:14.640]   Because, yeah, you executed this slicing and that slicing
[01:12:14.640 --> 01:12:20.800]   is on the host. So if you want to avoid it, I don't know.
[01:12:20.800 --> 01:12:28.960]   Like, yeah, maybe Pmap the whole thing, something like that.
[01:12:30.560 --> 01:12:34.400]   That is-- yeah. So you have to not Pmap the other ones and
[01:12:34.400 --> 01:12:37.280]   then Pmap the big step.
[01:12:37.280 --> 01:12:44.000]   No, that's good. You know, we had a problem at some point,
[01:12:44.000 --> 01:12:49.040]   which is like in the training data set, we were like-- our
[01:12:49.040 --> 01:12:54.240]   labels are the input shifted by one, right? We passed the
[01:12:54.240 --> 01:12:57.680]   tokens and we tried to predict the next tokens. So we just
[01:12:57.680 --> 01:13:01.120]   have to shift the labels by one. And initially, it was
[01:13:01.120 --> 01:13:06.880]   implemented in JAX with a complicated way to shift-- just
[01:13:06.880 --> 01:13:12.160]   to shift an array by one. And then, you know, we-- actually,
[01:13:12.160 --> 01:13:14.560]   it was during the competition, you know, HuggingFace found
[01:13:14.560 --> 01:13:19.120]   those stuff out. But then they put it into NumPy, that
[01:13:19.120 --> 01:13:22.400]   alloader and all, and suddenly went much, much, much faster.
[01:13:22.400 --> 01:13:24.400]   Oh, interesting. Yeah.
[01:13:26.720 --> 01:13:30.800]   The other thing I think you can do, but I don't know. I'm just
[01:13:30.800 --> 01:13:34.720]   saying this out of logic, but I don't know if XLA actually does
[01:13:34.720 --> 01:13:40.800]   this. So encoded images should be distributed when it comes
[01:13:40.800 --> 01:13:42.960]   out of pgenerate. Right.
[01:13:42.960 --> 01:13:52.720]   So if you do this small step, the removeBOS, in a Pmap
[01:13:52.720 --> 01:13:56.240]   function, it can be like a lambda. You create a small
[01:13:56.240 --> 01:14:00.560]   function that only removes the first element.
[01:14:00.560 --> 01:14:01.520]   Right.
[01:14:01.520 --> 01:14:05.040]   Maybe it does it on the cluster, like it does it on the
[01:14:05.040 --> 01:14:09.120]   array. So you don't have to move them. You do it like-- you
[01:14:09.120 --> 01:14:10.960]   operate over them, distribute it, and then--
[01:14:10.960 --> 01:14:13.840]   Yeah, that's a good idea.
[01:14:13.840 --> 01:14:18.480]   Yeah, that avoids synchronization with the host.
[01:14:18.480 --> 01:14:20.320]   Yeah.
[01:14:20.320 --> 01:14:25.680]   No, that's good. I mean, I'm happy to hear those tricks.
[01:14:25.680 --> 01:14:28.000]   And we need to-- that's why I also want to clean the code,
[01:14:28.000 --> 01:14:31.840]   make it a bit more accessible so more people can tweak it and
[01:14:31.840 --> 01:14:32.400]   improve it.
[01:14:32.400 --> 01:14:36.640]   So to give some quick news, I think we're going to be-- we're
[01:14:36.640 --> 01:14:39.360]   going to be trying maybe hopefully next week or the week
[01:14:39.360 --> 01:14:43.360]   after to scale it up and run it on a TPU pod.
[01:14:43.360 --> 01:14:47.680]   So the model, it would be nice if it's at least like five
[01:14:47.680 --> 01:14:49.200]   times bigger or something like that.
[01:14:49.200 --> 01:14:52.480]   And I think it's going to make a huge difference.
[01:14:53.600 --> 01:14:57.200]   I think it's going to make-- I have big hopes on it.
[01:14:57.200 --> 01:14:58.080]   Let's see what happens.
[01:14:58.080 --> 01:15:02.400]   But we need to adapt a few stuff on the code and loading the
[01:15:02.400 --> 01:15:04.240]   data and the model.
[01:15:04.240 --> 01:15:05.760]   But yeah.
[01:15:05.760 --> 01:15:07.600]   That's awesome.
[01:15:07.600 --> 01:15:13.040]   Like, this notebook is just for demo?
[01:15:13.040 --> 01:15:15.680]   Or is it kind of the main when you've been coding it?
[01:15:15.680 --> 01:15:17.280]   No, no.
[01:15:17.280 --> 01:15:19.840]   The notebook is just for demo.
[01:15:19.840 --> 01:15:22.000]   It's just to explain how to do inference.
[01:15:23.360 --> 01:15:25.680]   But we do it quite different.
[01:15:25.680 --> 01:15:30.240]   We do it a bit the same way when we use-- we have our big
[01:15:30.240 --> 01:15:34.320]   function backend that when you use the app, it runs on a TPU.
[01:15:34.320 --> 01:15:36.640]   The main difference that we do, we batch the prompts.
[01:15:36.640 --> 01:15:41.600]   So like, for example, there's 10 users asking for a prompt,
[01:15:41.600 --> 01:15:43.760]   and we put 10 prompts at the same time.
[01:15:43.760 --> 01:15:46.080]   And we put those 10 prompts on each device.
[01:15:46.080 --> 01:15:51.280]   So it's the only change we really have.
[01:15:51.280 --> 01:15:53.200]   The rest is very similar in logic.
[01:15:53.200 --> 01:15:56.080]   Oh, OK.
[01:15:56.080 --> 01:15:56.720]   Very interesting.
[01:15:56.720 --> 01:15:58.080]   I'll take a look at your repo.
[01:15:58.080 --> 01:16:00.320]   Maybe I'll leave you a suggestion.
[01:16:00.320 --> 01:16:01.760]   Yeah, that's cool.
[01:16:01.760 --> 01:16:02.880]   No, that will be cool.
[01:16:02.880 --> 01:16:07.520]   I think next week-- my goal is by early next week, maybe we add
[01:16:07.520 --> 01:16:08.400]   a new model.
[01:16:08.400 --> 01:16:13.360]   Like, the model has been rewritten a bit by Suraj to help
[01:16:13.360 --> 01:16:18.400]   us then scale to have a model that doesn't fit on one single
[01:16:18.400 --> 01:16:19.360]   TPU.
[01:16:19.360 --> 01:16:22.800]   So it's something we don't have right now, but we may not run
[01:16:22.800 --> 01:16:23.920]   that immediately.
[01:16:23.920 --> 01:16:27.120]   But basically, we will have the possibility of having a model
[01:16:27.120 --> 01:16:30.640]   that is too big for one TPU, and that has to be split over
[01:16:30.640 --> 01:16:31.140]   multiple.
[01:16:31.140 --> 01:16:33.840]   So that's going to be cool, too.
[01:16:33.840 --> 01:16:36.320]   Oh, very interesting.
[01:16:36.320 --> 01:16:39.680]   You plan on using Pidgit or some of the other--
[01:16:39.680 --> 01:16:44.880]   Yeah, we use Pidgit, and we use the mesh, and we use the ways
[01:16:44.880 --> 01:16:51.840]   to decide when to split the weights across devices, how to
[01:16:51.840 --> 01:16:53.440]   do with the attention and all that.
[01:16:53.440 --> 01:16:54.900]   Yeah.
[01:16:54.900 --> 01:16:56.480]   Oh, very interesting.
[01:16:56.480 --> 01:17:01.360]   Yeah, Jonathan last week was kind of giving a tour into
[01:17:01.360 --> 01:17:02.880]   those possibilities.
[01:17:02.880 --> 01:17:04.720]   Oh, yeah.
[01:17:04.720 --> 01:17:05.760]   No, this is cool.
[01:17:05.760 --> 01:17:08.240]   So me, I'm not familiar yet with it.
[01:17:08.240 --> 01:17:11.600]   Suraj did it, and it looks pretty cool because you can
[01:17:11.600 --> 01:17:14.000]   decide, let's say you have eight TPUs.
[01:17:14.000 --> 01:17:19.120]   OK, do a model parallel on two and data parallel on four, and
[01:17:19.120 --> 01:17:25.120]   you have two times four TPUs, and each model is on two TPUs,
[01:17:25.120 --> 01:17:26.960]   and you have data parallel in between.
[01:17:26.960 --> 01:17:30.160]   But the way you use it, once you created well--
[01:17:30.160 --> 01:17:33.520]   once you created well the weights and all needs to be
[01:17:33.520 --> 01:17:35.840]   distributed, it's like almost magic.
[01:17:35.840 --> 01:17:38.320]   It just all work and distributed over.
[01:17:38.320 --> 01:17:39.360]   It's pretty nice.
[01:17:39.440 --> 01:17:42.320]   There's a lot of-- I think that's the thing, Jax.
[01:17:42.320 --> 01:17:45.120]   There's a lot of bit work you have to do initially to set
[01:17:45.120 --> 01:17:48.560]   up things, but then whenever you need to scale or do other
[01:17:48.560 --> 01:17:50.880]   things, it's pretty nice and really fast.
[01:17:50.880 --> 01:17:56.640]   Yeah, I'd love to learn more about PID.
[01:17:56.640 --> 01:18:02.880]   From what we were showing, it seemed very kind of magical
[01:18:02.880 --> 01:18:03.440]   in a sense.
[01:18:03.440 --> 01:18:05.520]   It's kind of, oh, we will do it for you.
[01:18:05.520 --> 01:18:06.740]   That's right.
[01:18:08.560 --> 01:18:09.360]   Very interesting.
[01:18:09.360 --> 01:18:10.080]   Very uncool.
[01:18:10.080 --> 01:18:18.160]   In the interest of time, there are a few questions that I'd
[01:18:18.160 --> 01:18:18.720]   like to ask.
[01:18:18.720 --> 01:18:21.760]   Is there any other point you wanted to cover, Boris?
[01:18:21.760 --> 01:18:28.240]   Sorry, I'm asking if there were any other points you wanted
[01:18:28.240 --> 01:18:29.360]   to cover in your presentation.
[01:18:29.360 --> 01:18:30.400]   Oh, no, no, no.
[01:18:30.400 --> 01:18:33.440]   No, I think that's pretty all.
[01:18:33.440 --> 01:18:38.080]   The rest is pretty straightforward.
[01:18:38.080 --> 01:18:40.800]   I just wanted to show what I just showed a bit, maybe a bit
[01:18:40.800 --> 01:18:44.400]   fast, but the train state and optax and all together, I think
[01:18:44.400 --> 01:18:46.240]   they are pretty convenient.
[01:18:46.240 --> 01:18:49.520]   There's some things that are sometimes a bit tricky, but
[01:18:49.520 --> 01:18:54.800]   overall, the Jax team and optax and all, I raised issues many
[01:18:54.800 --> 01:18:58.320]   times, and they reply always pretty well and take the time
[01:18:58.320 --> 01:19:00.640]   to give big answers.
[01:19:00.640 --> 01:19:04.080]   And you may have to wait a bit, but they give really nice
[01:19:04.080 --> 01:19:05.280]   answers, and it's nice.
[01:19:07.760 --> 01:19:11.040]   Yeah, I was really surprised by how active they are on the
[01:19:11.040 --> 01:19:12.000]   issues page.
[01:19:12.000 --> 01:19:15.600]   So I'll move on to the questions, and we have about 10
[01:19:15.600 --> 01:19:16.240]   minutes left.
[01:19:16.240 --> 01:19:19.120]   So reminder to everyone, please feel free to ask any
[01:19:19.120 --> 01:19:19.620]   questions.
[01:19:19.620 --> 01:19:26.000]   Have you considered using VQVAE rather than VQGAN to avoid
[01:19:26.000 --> 01:19:26.500]   collapse?
[01:19:26.500 --> 01:19:27.840]   Yes.
[01:19:27.840 --> 01:19:30.000]   Maybe I don't need to share my screen.
[01:19:30.000 --> 01:19:30.800]   OK.
[01:19:30.800 --> 01:19:37.680]   So yeah, I actually looked at it at some point, and we compared
[01:19:37.680 --> 01:19:38.160]   it.
[01:19:38.160 --> 01:19:42.240]   We used the VQGAN from Taming Transformers, and on their
[01:19:42.240 --> 01:19:45.840]   report, they have a cool reproduction notebook where
[01:19:45.840 --> 01:19:48.800]   basically you can give the image, give the scale, and they
[01:19:48.800 --> 01:19:52.880]   will show this is the reconstruction with the VQVAE of
[01:19:52.880 --> 01:19:53.520]   OpenAI.
[01:19:53.520 --> 01:19:56.240]   This is the reconstruction with our model.
[01:19:56.240 --> 01:19:59.200]   This is the reconstruction, and you can compare all of it,
[01:19:59.200 --> 01:20:00.560]   and there's really pros and cons.
[01:20:00.560 --> 01:20:04.080]   And basically, the VQGAN does weird stuff.
[01:20:04.080 --> 01:20:07.440]   Sometimes, you know, the eyes are going to be weird, and some
[01:20:07.440 --> 01:20:09.360]   parts, the edge are going to be strange.
[01:20:09.360 --> 01:20:14.400]   The VQVAE from OpenAI, the main problem is it becomes blurry.
[01:20:14.400 --> 01:20:20.320]   And for example, you lose the texture, but maybe the face
[01:20:20.320 --> 01:20:22.800]   would be just blurry, looks slightly less strange.
[01:20:22.800 --> 01:20:27.440]   In the end, we preferred a bit the VQGAN.
[01:20:27.440 --> 01:20:31.120]   Now we're playing with just the VQGAN, and maybe we'll use the
[01:20:31.120 --> 01:20:32.320]   model that's a bit bigger.
[01:20:32.320 --> 01:20:35.840]   But eventually, I think there's going to be models much better
[01:20:35.840 --> 01:20:38.320]   than that coming up next year.
[01:20:38.320 --> 01:20:41.600]   And if they don't come fast enough, maybe we'll work on that
[01:20:41.600 --> 01:20:41.920]   next.
[01:20:41.920 --> 01:20:46.480]   But there's models that are really, if you look at the paper,
[01:20:46.480 --> 01:20:49.280]   like improved VQGAN or something like that, it's really cool.
[01:20:49.280 --> 01:20:53.840]   And you see actually so many papers related to image encoding.
[01:20:53.840 --> 01:20:57.920]   And less models are public, and less can be played with.
[01:20:57.920 --> 01:21:03.920]   So people still use VQGAN and VQVAE, but some are very, very
[01:21:03.920 --> 01:21:04.480]   promising.
[01:21:05.360 --> 01:21:05.860]   OK.
[01:21:05.860 --> 01:21:08.480]   That's interesting to know.
[01:21:08.480 --> 01:21:11.360]   I'll keep an eye out for the models you mentioned.
[01:21:11.360 --> 01:21:14.000]   Another one by Yuvraj.
[01:21:14.000 --> 01:21:19.440]   Is there any use of using Pmap when you're training on one GPU
[01:21:19.440 --> 01:21:19.940]   machine?
[01:21:19.940 --> 01:21:25.440]   Do I use-- sorry, if I use Pmap on a GPU?
[01:21:25.440 --> 01:21:30.480]   On a single GPU, will it be effective still for this use case?
[01:21:30.480 --> 01:21:34.080]   So I think if you use Pmap on a single GPU, it would be the
[01:21:34.080 --> 01:21:38.000]   same as if you use JIT.
[01:21:38.000 --> 01:21:40.640]   It would be the same as if you JIT the function.
[01:21:40.640 --> 01:21:43.840]   So the only thing it does, it just compiles it to make it
[01:21:43.840 --> 01:21:44.340]   efficient.
[01:21:44.340 --> 01:21:50.320]   Using Pmap, we do actually the exact same thing, I think.
[01:21:50.320 --> 01:21:54.480]   The only difference that is nice is if at some point you have
[01:21:54.480 --> 01:21:56.400]   two GPUs, you don't have to change your code.
[01:21:56.400 --> 01:21:58.800]   It will work on the two GPUs.
[01:21:58.800 --> 01:22:00.660]   Got it.
[01:22:01.920 --> 01:22:04.800]   OK, so there are a few general questions by Ramesh.
[01:22:04.800 --> 01:22:08.160]   I'll try to summarize them.
[01:22:08.160 --> 01:22:12.160]   Are there any things you like about JAX versus PyTorch?
[01:22:12.160 --> 01:22:15.360]   Or do you like to-- things where you prefer PyTorch versus
[01:22:15.360 --> 01:22:16.800]   things where you prefer JAX?
[01:22:16.800 --> 01:22:23.040]   Yeah, so actually, in general, when I have a certain problem,
[01:22:23.040 --> 01:22:26.640]   I look, is there already a network that exists that does
[01:22:26.640 --> 01:22:27.680]   the job?
[01:22:27.680 --> 01:22:30.800]   And is there already a repo that exists that does the job?
[01:22:30.800 --> 01:22:33.760]   And whatever language it's in, I'm just going to use that.
[01:22:33.760 --> 01:22:37.840]   I know, when that works, you don't have to spend too much
[01:22:37.840 --> 01:22:38.640]   time.
[01:22:38.640 --> 01:22:40.880]   So when something exists, I do that.
[01:22:40.880 --> 01:22:45.920]   I kind of like Fast.ai too, because if you have something
[01:22:45.920 --> 01:22:50.320]   slightly custom, but oh, I want a ResNet, but I have like four
[01:22:50.320 --> 01:22:54.000]   channels, and I have like-- you want to customize a few little
[01:22:54.000 --> 01:22:54.320]   things.
[01:22:54.320 --> 01:22:58.000]   It's really, really fast to have something in a few lines of
[01:22:58.000 --> 01:22:58.720]   code.
[01:22:58.720 --> 01:23:01.520]   If it doesn't have to be perfect, it will be convenient.
[01:23:01.520 --> 01:23:06.400]   And JAX, right now, I used it to play.
[01:23:06.400 --> 01:23:11.280]   But DALI, I think, is a really large model.
[01:23:11.280 --> 01:23:13.280]   And I think if you have something that needs to be really
[01:23:13.280 --> 01:23:15.120]   large, you use JAX.
[01:23:15.120 --> 01:23:18.480]   If I have a small model, nowadays, would I use JAX?
[01:23:18.480 --> 01:23:23.280]   I may use JAX, but not necessarily for the right
[01:23:23.280 --> 01:23:26.240]   reasons, just because I want to become better at it and
[01:23:26.240 --> 01:23:26.960]   underlearn it.
[01:23:27.520 --> 01:23:30.880]   But in terms of efficiency, just use what you're comfortable
[01:23:30.880 --> 01:23:32.560]   with or what you find.
[01:23:32.560 --> 01:23:34.240]   In the end, they're all the same, kind of.
[01:23:34.240 --> 01:23:35.520]   They all do the same.
[01:23:35.520 --> 01:23:38.800]   The function-- even the functions are almost the same
[01:23:38.800 --> 01:23:39.300]   names.
[01:23:39.300 --> 01:23:42.320]   Just JAX is slightly different.
[01:23:42.320 --> 01:23:45.280]   It reminds me a bit of TensorFlow and all.
[01:23:45.280 --> 01:23:49.360]   And I must say the PyTorch writing, to me, is a bit more
[01:23:49.360 --> 01:23:51.280]   fun than JAX.
[01:23:51.280 --> 01:23:56.160]   But now that I have all that code in JAX, it's not that hard.
[01:23:56.160 --> 01:23:57.040]   You get used to it.
[01:23:57.600 --> 01:23:58.100]   Awesome.
[01:23:58.100 --> 01:24:00.800]   I don't see any questions.
[01:24:00.800 --> 01:24:04.160]   So in the interest of time, I'll try to wrap up quickly.
[01:24:04.160 --> 01:24:06.720]   I again want to thank Boris.
[01:24:06.720 --> 01:24:09.520]   Boris is currently visiting his parents, and he's still
[01:24:09.520 --> 01:24:12.000]   interested in joining us on a weekend.
[01:24:12.000 --> 01:24:15.280]   To learn more about his awesome projects, which I'm sure
[01:24:15.280 --> 01:24:18.480]   will keep coming, please make sure you follow him on Twitter
[01:24:18.480 --> 01:24:19.520]   @borisfema.
[01:24:19.520 --> 01:24:24.000]   And apart from that, I also wanted to mention that tomorrow
[01:24:24.000 --> 01:24:26.320]   we're hosting an official Eurips talk.
[01:24:26.880 --> 01:24:29.200]   An official Eurips paper reading group.
[01:24:29.200 --> 01:24:32.960]   So if that is of interest to you, please feel free to join.
[01:24:32.960 --> 01:24:36.720]   Boris, anything else you want to mention before we wrap up?
[01:24:36.720 --> 01:24:39.120]   No, thanks a lot for inviting me.
[01:24:39.120 --> 01:24:42.400]   It's a pleasure, a big honor to be on your podcast.
[01:24:42.400 --> 01:24:47.520]   And yeah, if anybody has questions, feel free to hit me
[01:24:47.520 --> 01:24:47.760]   up.
[01:24:47.760 --> 01:24:51.440]   And if you want to work on DALI-- there's actually a big
[01:24:51.440 --> 01:24:53.440]   DALI community on Discord.
[01:24:54.400 --> 01:24:56.000]   My GitHub for DALI Mini.
[01:24:56.000 --> 01:24:58.960]   But it's the same as DALI PyTorch, actually.
[01:24:58.960 --> 01:25:02.160]   It comes from DALI PyTorch, which is a bunch of people
[01:25:02.160 --> 01:25:05.520]   excited about solving text to image.
[01:25:05.520 --> 01:25:07.360]   So join in there.
[01:25:07.360 --> 01:25:10.240]   And you have a lot of people excited and doing different
[01:25:10.240 --> 01:25:10.480]   things.
[01:25:10.480 --> 01:25:12.240]   Some people playing with models.
[01:25:12.240 --> 01:25:13.920]   Some people having random ideas.
[01:25:13.920 --> 01:25:19.200]   Asking some people starting to develop their own things.
[01:25:19.200 --> 01:25:20.480]   So it's cool to join.
[01:25:20.480 --> 01:25:22.480]   Amazing.
[01:25:22.480 --> 01:25:25.680]   And also before ending, I'll quickly mention-- so this
[01:25:25.680 --> 01:25:28.720]   week's winner of 27 days of JAX is Yuvraj.
[01:25:28.720 --> 01:25:33.440]   I'll post the link to his GitHub where he's been creating
[01:25:33.440 --> 01:25:34.640]   these notebooks in the chat.
[01:25:34.640 --> 01:25:37.840]   Yuvraj, thank you for writing these awesome reports.
[01:25:37.840 --> 01:25:40.320]   If it's of interest to you, we'll reach out and offer some
[01:25:40.320 --> 01:25:40.720]   swag.
[01:25:40.720 --> 01:25:42.560]   Please feel free to take us up on that.
[01:25:42.560 --> 01:25:44.880]   And I look forward to reading more of your posts.
[01:25:44.880 --> 01:25:46.480]   So please keep them coming.
[01:25:46.480 --> 01:25:50.400]   Apart from that, I want to remind everyone, this is the
[01:25:50.400 --> 01:25:52.640]   last JAX meetup of this year.
[01:25:52.640 --> 01:25:53.920]   We'll be back next year.
[01:25:53.920 --> 01:25:56.800]   Krishan will continue hosting them on the meetup group as
[01:25:56.800 --> 01:25:57.520]   well.
[01:25:57.520 --> 01:26:01.760]   And I hope everyone has a fantastic holiday break.
[01:26:01.760 --> 01:26:04.880]   And I hope you join the NeurIPS meetup.
[01:26:04.880 --> 01:26:06.400]   If not, we'll see you next year.
[01:26:06.400 --> 01:26:08.320]   And thanks again, Boris, for your time.
[01:26:08.320 --> 01:26:09.920]   This was really fascinating.
[01:26:09.920 --> 01:26:13.120]   And I really like the chai image that was generated.
[01:26:13.120 --> 01:26:14.800]   Thanks a lot.

