
[00:00:00.000 --> 00:00:05.760]   day. We're going to hear from folks at ML at Berkeley, an undergraduate student organization,
[00:00:05.760 --> 00:00:11.440]   about some of the work that they did over this past semester. So we're going to hear from three
[00:00:11.440 --> 00:00:15.680]   separate teams that worked on three different projects, each very different from the other,
[00:00:15.680 --> 00:00:20.160]   but all attacking a different subfield in deep learning and machine learning.
[00:00:20.160 --> 00:00:25.200]   And I'm really excited to hear from all of them about their work. We're going to start
[00:00:25.200 --> 00:00:31.360]   with some folks who worked on what was more of an ML engineering project. So we're going to be
[00:00:31.360 --> 00:00:39.040]   hearing from Ashwat and Andre on their design of a computer vision based video gesture overlay for
[00:00:39.040 --> 00:00:45.600]   Zoom, which they will, if the demo gods are with us today, demonstrate live as part of the
[00:00:45.600 --> 00:00:52.800]   salon. So Andre and Ashwat, take it away. >> Awesome. Thank you, Charles. Give me one sec.
[00:00:52.800 --> 00:01:03.920]   Okay, sweet. And yeah, okay. Awesome. So hello and welcome to our presentation,
[00:01:03.920 --> 00:01:08.640]   which is video gesture recognition and overlay. This is a presentation, a final project that we
[00:01:08.640 --> 00:01:13.680]   did for the new member education program here at MLab and machine learning at Berkeley.
[00:01:13.680 --> 00:01:16.240]   And yeah, we're excited. We hope you enjoy that presentation.
[00:01:16.240 --> 00:01:21.600]   So first and foremost, I want to introduce the team. My name is Ashwat,
[00:01:21.600 --> 00:01:25.840]   and I worked with Aria, Andre, and Saurabh this semester on this project. Andre will be joining
[00:01:25.840 --> 00:01:29.920]   us for the second half of this presentation, and Aria and Saurabh will hopefully be online
[00:01:29.920 --> 00:01:33.360]   answering questions on the YouTube live stream, as well as at the end to answer questions.
[00:01:33.360 --> 00:01:39.040]   So first and foremost, what was the goal of this entire project? Why did we do this?
[00:01:39.040 --> 00:01:43.840]   Let me ask you this. How many times this year on Zoom has someone tried to screen share and they
[00:01:43.840 --> 00:01:48.000]   asked you the question, the dreaded question, can you see my screen? To which the answer is almost
[00:01:48.000 --> 00:01:54.080]   99% yes. And then you wait this awkward silence until someone finally says, unmutes, they say the
[00:01:54.080 --> 00:01:57.920]   one word yes, and then they mute themselves again. We thought to ourselves, like, there's got to be
[00:01:57.920 --> 00:02:01.360]   a better way. There's got to be a way to use machine learning to prevent the need to unmute,
[00:02:01.360 --> 00:02:05.200]   to just say a quick word like yes, or I have a question, or I have something to say, or goodbye.
[00:02:05.200 --> 00:02:09.920]   On top of that, we wanted to make the Zoom equivalent of emoji react, because emojis are
[00:02:09.920 --> 00:02:13.040]   pretty much like a second language to all of us right now, and all of us know it. So like,
[00:02:13.040 --> 00:02:17.040]   how do we combine the two things, make it kind of fun? And last but not least, we said, how do we
[00:02:17.040 --> 00:02:21.360]   combine some relatively simple concepts to solve a real world problem? In this case, being a
[00:02:21.360 --> 00:02:27.040]   convenience on Zoom. So on the right side, you can see a GIF of me, where I'm doing a demo of what we
[00:02:27.040 --> 00:02:31.520]   created. So I'm gone for a second, I'll be back. But pretty much what I'm trying to do is you can
[00:02:31.520 --> 00:02:35.680]   see my hands will make all these gestures on the camera, and an icon will show up because the camera
[00:02:35.680 --> 00:02:39.360]   recognizes what I'm trying to say. So this would be a comment. I believe the next one would be like
[00:02:39.360 --> 00:02:44.720]   a goodbye, a peace sign. So yeah, we'll get into detail about that. So now, the first step when it
[00:02:44.720 --> 00:02:48.560]   comes to trying to do any kind of gesture recognition is to find out where in the frame
[00:02:48.560 --> 00:02:53.520]   your hand is in the first place. So for us, we decided to use some off the shelf tracking modules.
[00:02:53.520 --> 00:02:58.560]   So for hand tracking, we decided to use Victor D'Bia on GitHub. He had a great GitHub repository
[00:02:58.560 --> 00:03:04.080]   about this. And then for face tracking, we decided to use the good old reliable hard cascades. So
[00:03:04.080 --> 00:03:07.760]   here are some animations showing how this works. On the bottom left is Victor D'Bia's GitHub.
[00:03:07.760 --> 00:03:13.120]   I believe he trained this using transfer learning on mobile.me too, and definitely was really good,
[00:03:13.120 --> 00:03:17.120]   and that was a great foundation. And on the bottom right, we have the hard cascades for facial hard
[00:03:17.120 --> 00:03:20.960]   cascades. And we use both of these as like a foundation to build off of. And then again,
[00:03:20.960 --> 00:03:24.640]   we had to take this, kind of modify it up a bit, change the inputs so that we could make it as
[00:03:24.640 --> 00:03:29.360]   tailored as possible for our specific ML model, which Andrei will talk about in the future slides.
[00:03:29.360 --> 00:03:33.760]   So a quick detour into how hard cascades work. I just want to quickly talk about this.
[00:03:33.760 --> 00:03:38.960]   So right off the bat, Har is the name of a Hungarian mathematician. I believe he came
[00:03:38.960 --> 00:03:42.640]   up with something called hard sequences in mathematics, which led to hard wavelets.
[00:03:42.640 --> 00:03:46.800]   Hard wavelets are intuitively similar to hard features. And that's where we came here today.
[00:03:46.800 --> 00:03:51.440]   So hard features are pretty much very good at identifying edges and lines in an image.
[00:03:51.440 --> 00:03:55.600]   And I think this was proposed by Viola and Jones in the Viola-Jones algorithm in a paper they wrote
[00:03:55.600 --> 00:04:01.360]   to CVPR back in 2001, if you know what that is. But overall, what it does is it takes the average
[00:04:01.360 --> 00:04:05.360]   of all the pixels in the white region, the average of all the pixels in the black region,
[00:04:05.360 --> 00:04:09.120]   subtracts the two means. And if they're around a certain threshold, then you pretty much have a
[00:04:09.120 --> 00:04:14.640]   good confidence that this is an edge or a line in the image. Now, the next natural question is,
[00:04:14.640 --> 00:04:18.320]   well, there's edges and lines, but how does that apply to my face? Because everyone's face is
[00:04:18.320 --> 00:04:21.840]   different. There's no edges or lines directly in your face. And that's what this diagram shows
[00:04:21.840 --> 00:04:26.560]   beautifully. Pretty much, your eyes can be captured very well with an edge feature, as shown in this.
[00:04:26.560 --> 00:04:30.000]   A nose can be captured and a mouth can be captured with the line features relatively well.
[00:04:30.000 --> 00:04:34.400]   So in a sense, these hard features will kind of map and really reliably map to parts of your face
[00:04:34.400 --> 00:04:38.080]   where they'll match up closely. Because the pixel intensity between the luminosity between, I guess,
[00:04:38.080 --> 00:04:42.000]   like your nose and the surrounding surroundings. And pretty much, that's how it works.
[00:04:42.000 --> 00:04:47.280]   And next, an animation showing how this all works together. So what it does is it slides a window
[00:04:47.280 --> 00:04:50.880]   across the entire image, and it runs these features across every single thing and gets
[00:04:50.880 --> 00:04:54.560]   like some kind of-- how many ever hits it can kind of get. So the next question naturally is,
[00:04:54.560 --> 00:04:58.320]   we have thousands of thousands of these hard features, and we have so many windows. That's
[00:04:58.320 --> 00:05:02.720]   a lot of calculations for a real-time face detector. So how does it work so well? And
[00:05:02.720 --> 00:05:06.880]   that's where the cascading parts comes into play. So it's called a hard cascade because a cascade
[00:05:06.880 --> 00:05:11.440]   of hard features. And what it does is pretty much it takes a few low-level core features,
[00:05:11.440 --> 00:05:15.200]   runs it across a window in an image. And again, if it doesn't pass the initial features, like if
[00:05:15.200 --> 00:05:18.240]   it doesn't pass the test, I guess, in the beginning, there's no reason it would pass the more
[00:05:18.240 --> 00:05:22.480]   intricate or detailed test later on. So thus, you can discard an image or a part of the image right
[00:05:22.480 --> 00:05:27.280]   away. And thus, at the end, the parts of the image that passed all of these cascading tests
[00:05:27.280 --> 00:05:31.120]   will be classified as a face because it kind of like passed all of the cascade of hard features
[00:05:31.120 --> 00:05:34.480]   that I wanted to find. And that's what's returned as a bounding box to the user.
[00:05:35.680 --> 00:05:39.280]   Now, moving on to gesture classes. So on the left side, you can see all of the
[00:05:39.280 --> 00:05:44.480]   like emojis showing what kind of hand gestures we use for these classes. And the middle is the
[00:05:44.480 --> 00:05:50.560]   class that we wanted to identify as. And on the right side is the icon that we wanted to show.
[00:05:50.560 --> 00:05:55.200]   So for us, we thought the most core things people want to say that they don't need to unmute for is,
[00:05:55.200 --> 00:06:00.240]   you know, I have a question or, you know, yes, sounds good. You can say, no, please stop. I
[00:06:00.240 --> 00:06:03.680]   have something to say, a comment, one or two fingers, peace sign, like I'm going to go goodbye,
[00:06:03.680 --> 00:06:07.840]   end of the call. And last but not least, my personal favorite, and I think one of the most
[00:06:07.840 --> 00:06:12.800]   important is the out of frame. And the reason for this is because I think that many times I've seen
[00:06:12.800 --> 00:06:16.960]   this in calls myself at university this past semester, when people would leave their camera,
[00:06:16.960 --> 00:06:20.880]   but they leave their video on, which leaves their entire background to the viewer's, you know,
[00:06:20.880 --> 00:06:25.200]   pleasure, whatever they may be. And usually it's not a very bad thing because I mean, I hope it's
[00:06:25.200 --> 00:06:29.200]   not a bad thing, but oftentimes we've all heard stories of, you know, things going awry when
[00:06:29.200 --> 00:06:33.040]   things in your background are being shown. So we thought to ourselves, when someone walks away
[00:06:33.040 --> 00:06:37.360]   from the camera, this now, our software will automatically cover up the entire video stream.
[00:06:37.360 --> 00:06:41.200]   So it says away from keyboard. And when you walk back into frame, it'll detect, it'll know that
[00:06:41.200 --> 00:06:45.680]   you showed back up and then it'll remove the mask. So thus this entire process is fully automated.
[00:06:45.680 --> 00:06:50.480]   No need to click a button or click a, like a mouse click or a keyboard click. All you got to do is
[00:06:50.480 --> 00:06:54.560]   just raise your hands or simply leave the camera and everything is done completely by itself.
[00:06:54.560 --> 00:06:59.920]   Cause it's detecting every step of the way. Now I'm going to pass the mic off to Andre and yeah,
[00:06:59.920 --> 00:07:04.880]   take it away. Thanks Ashwat. So for the second part of this project, we want to build a gesture
[00:07:04.880 --> 00:07:09.520]   class for, and our plan was to use a CNN and train on some sort of hand gesture data set.
[00:07:09.520 --> 00:07:14.240]   What we found for this is this American sign language alphabet data set from Kaggle. It has 29
[00:07:14.240 --> 00:07:19.120]   classes with 3000 images per class. Now that sounds like a lot of images, but it's actually
[00:07:19.120 --> 00:07:24.160]   not in practice because these are frames to a video, meaning adjacent images are almost identical.
[00:07:24.160 --> 00:07:28.720]   So to avoid wasting training time on those, we only use every 10th image from the data set.
[00:07:28.720 --> 00:07:33.040]   We also do extensive data augmentation because as you can see on the right, there's not a lot
[00:07:33.040 --> 00:07:38.480]   of variation to the shape of the hand and to the, to the background that the data set uses.
[00:07:38.480 --> 00:07:43.360]   So we use color jitter to adjust the contrast, brightness, and hue of these images. We use random
[00:07:43.360 --> 00:07:48.560]   rotation to change the orientation by some small random angle. And we use random resize crops to
[00:07:48.560 --> 00:07:54.320]   simulate what the effect when your hand moves farther away or closer to the camera. Now,
[00:07:54.320 --> 00:07:58.240]   when we were choosing the architecture, we had two main objectives in mind. The first is to choose a
[00:07:58.240 --> 00:08:04.240]   model with low model complexity and specifically fast feed forward runs, because we want to be
[00:08:04.240 --> 00:08:09.120]   classifying on frames of a video in real time. So the faster the feed forward, the better.
[00:08:09.120 --> 00:08:14.000]   And the second objective was to choose a model that were more easily generalized to this task,
[00:08:14.000 --> 00:08:19.760]   because as the problem, as I mentioned before, our main problem was the status of lacking
[00:08:19.760 --> 00:08:24.720]   variation. That's why we use convolutional filters that are pre-trend on ImageNet for this task,
[00:08:24.720 --> 00:08:30.560]   because the images in ImageNet include all kinds of colors, textures, and shapes. So these learned
[00:08:30.560 --> 00:08:35.280]   convolution filters should help us avoid overfitting. Now, it also helps with training
[00:08:35.280 --> 00:08:39.280]   speed because we won't be training all the filters from scratch. We use weights and biases to log
[00:08:39.280 --> 00:08:43.520]   our training. And what we found is that rather than doing a standard transfer learning, where
[00:08:43.520 --> 00:08:47.680]   we freeze all the convolution filters during training, it was actually better to allow them
[00:08:47.680 --> 00:08:54.400]   to tune themselves a bit to the hand images, probably because this helps them specialize
[00:08:54.400 --> 00:08:57.680]   to detecting the specific shape of the fingers and palm.
[00:08:57.680 --> 00:09:05.840]   Now, one technique that MobileNet V2 uses to reduce the number of parameters is what's called
[00:09:05.840 --> 00:09:10.000]   a depth-wise separable convolution. Now, if you look at our standard convolution layer,
[00:09:10.000 --> 00:09:14.080]   we have these kernels that convolve across all the channels of the input image. They essentially
[00:09:14.080 --> 00:09:18.000]   collapse the input channels into one output channel. And in order to produce more output
[00:09:18.000 --> 00:09:22.800]   channels, we would need more of these k by k by c kernels. Now, on the other hand, a depth-wise
[00:09:22.800 --> 00:09:27.440]   separable convolution breaks down the convolution into two steps. The first is a per-channel
[00:09:27.440 --> 00:09:32.880]   convolution, where you apply separate k by k kernels to each channel of the input image to
[00:09:32.880 --> 00:09:38.160]   produce this intermediate image of the same depth as your input. Then we use one-by-one
[00:09:38.160 --> 00:09:42.320]   convolutions to combine information across all channels of the intermediate image to produce
[00:09:42.320 --> 00:09:47.040]   the output image. And same as before, to have more output channels, we need more one-by-one
[00:09:47.040 --> 00:09:52.160]   convolutions. This already makes the advantage very apparent because now to have more output
[00:09:52.160 --> 00:09:56.320]   channels, you only need to make more of these one-by-one convolutions rather than the k by k.
[00:09:56.320 --> 00:10:01.200]   And this also drastically reduces how many multiplications we need to compute this
[00:10:01.200 --> 00:10:06.240]   convolution. And the other advantage is that now we're also adding more non-linearity.
[00:10:06.240 --> 00:10:09.920]   We add an extra activation step at the one-by-one convolution.
[00:10:09.920 --> 00:10:18.000]   Okay, cool. And so now Andre is going to try to get the live demo set up, which he's going to do
[00:10:18.000 --> 00:10:22.080]   from his live video camera. And in the meantime, I always want to go again through the results.
[00:10:22.080 --> 00:10:26.240]   So again, this is just to clarify a fist means no, which you can see on the bottom left.
[00:10:26.240 --> 00:10:30.400]   A palm means I have a question. Naturally, people in high school, you raise your hand for a question.
[00:10:30.400 --> 00:10:33.840]   Okay, we thought the perfect sign would be great for okay.
[00:10:33.840 --> 00:10:41.280]   And then when it comes to having a comment, we decided to just raise a finger, like I got
[00:10:41.280 --> 00:10:45.520]   something to say, one or two fingers. Away from keyboard, again, we just showed this so you can
[00:10:45.520 --> 00:10:48.960]   see what's happening, but pretty much this entire camera would be covered as I showed in the demo
[00:10:48.960 --> 00:10:53.680]   and the GIF that I had. And then last but not least, we have the peace sign. And the peace
[00:10:53.680 --> 00:10:59.360]   sign, just again, peace sign, goodbye, makes a waving icon, and that should work. So yeah,
[00:10:59.360 --> 00:11:02.640]   that's the end of our presentation. I want to thank Weights and Biases for having us
[00:11:02.640 --> 00:11:06.400]   on this podcast, and again, MLAB. And Andre, are you ready?
[00:11:06.400 --> 00:11:14.240]   I just need to turn off my virtual background. Okay, sweet. I'll stop sharing.
[00:11:15.840 --> 00:11:23.040]   So the first gesture is a fist, should be no. And okay is yes. Palm is a question.
[00:11:23.040 --> 00:11:31.040]   And one finger up is comment. And a peace sign would be goodbye.
[00:11:31.040 --> 00:11:36.320]   Then when I move out of the frame, it should cover my screen with a graphic.
[00:11:40.160 --> 00:11:46.480]   And come back, it should move away. Yep. That was our presentation. Thank you.
[00:11:46.480 --> 00:11:53.760]   Great. Thanks so much, Andre and Ashwat. It's really impressive work that your team put together.
[00:11:53.760 --> 00:11:59.920]   And so I think it was a pretty short timeline as well. Can you say how long it took to develop that
[00:11:59.920 --> 00:12:05.600]   from concept to execution? Yeah, that's a great question. I think we did start,
[00:12:05.600 --> 00:12:09.840]   I think I forgot. Semester has been quite a long semester, but I think we started around
[00:12:09.840 --> 00:12:13.520]   the beginning of November and spent about a month or so. One thing that was difficult,
[00:12:13.520 --> 00:12:17.680]   definitely the fact that like half of our team was PST and half of it was in like China and
[00:12:17.680 --> 00:12:22.720]   Singapore. So times are differences, but we still, you know, we made it work. And yeah, it was great.
[00:12:22.720 --> 00:12:30.240]   Gotcha. Yeah, I imagine that's an experience a lot of teams are having in this year. I've been doing
[00:12:30.240 --> 00:12:37.520]   a lot of online teaching and some of that has been on Europe time on like, yeah, Indian and
[00:12:37.520 --> 00:12:44.480]   China time. So yeah, it's been a wild year for time zone. And yeah, impressive that you put that
[00:12:44.480 --> 00:12:52.560]   together in just a few weeks. I had a couple of questions. So one about the, you used a non-deep
[00:12:52.560 --> 00:12:58.800]   learning approach at the beginning for sort of selecting out your bounding box. And so I wanted
[00:12:58.800 --> 00:13:04.400]   to hear a little bit more about that, sort of like how'd you choose that particular hard cascade
[00:13:04.400 --> 00:13:10.320]   approach? How'd you find it? What other options did you consider? And can you talk more about the
[00:13:10.320 --> 00:13:16.720]   decisions you made as in that part of the process? Yeah, for sure. I guess I can take that one. So
[00:13:16.720 --> 00:13:20.640]   I guess in the beginning, yeah, when it came to hand face tracking, we initially, I think our
[00:13:20.640 --> 00:13:25.600]   team wanted to create our own from scratch. But then again, given the limited timeframe, we decided
[00:13:25.600 --> 00:13:29.760]   to go off the shelf because the core of the project came from the ML side. But with that being said,
[00:13:29.760 --> 00:13:34.320]   in terms of alternatives, I definitely will admit that one of the, the hand tracking was amazing.
[00:13:34.320 --> 00:13:38.080]   I'll just admit that. Victor Dubia did a great job. But when it came to face tracking, I think
[00:13:38.080 --> 00:13:42.240]   one of the downfalls of hard cascades is as I talked about in the slide of the hard features,
[00:13:42.240 --> 00:13:45.920]   like if you're wearing like sunglasses or maybe even a mask, which is common in this day and age,
[00:13:45.920 --> 00:13:49.680]   like it blocks your nose, your mouth and your eyes, and that can actually be a downfall. So we
[00:13:49.680 --> 00:13:54.480]   thought like some other methods could be, I believe there's other things like D-Lib is something in
[00:13:54.480 --> 00:14:01.040]   C++ that can work. There's something called MTCNN. I forgot what exactly that stood for. And most
[00:14:01.040 --> 00:14:04.400]   importantly, I think YOLOv3 is used for self-driving cars for detecting different objects.
[00:14:04.400 --> 00:14:09.040]   And that's more deep learning based. And if it was deep learning based, we could train it so that
[00:14:09.040 --> 00:14:13.520]   you could make like a YOLOv3 version for like while wearing a mask or while wearing sunglasses.
[00:14:13.520 --> 00:14:19.520]   So it makes it more robust overall. So yeah. I see. Was part of it also an attempt to sort
[00:14:19.520 --> 00:14:24.480]   of like cut down on the amount of computation, like is the hard cascade applied to the whole
[00:14:24.480 --> 00:14:30.320]   image cheaper than doing something like YOLOv3 or has it that advanced to the point where the
[00:14:30.320 --> 00:14:36.240]   computation difference isn't that big? So I believe that, so we haven't exactly
[00:14:36.240 --> 00:14:40.080]   empirically tested the differences between, I guess like YOLO versus hard cascades,
[00:14:40.080 --> 00:14:44.320]   but I do believe hard cascades is pretty fast. I personally have used it in the past and it's
[00:14:44.320 --> 00:14:49.120]   definitely an old algorithm. I think CVPR 2001 was when Viola and Jones made the initial algorithm
[00:14:49.120 --> 00:14:54.080]   and proposal on the paper. So definitely we've come quite a far away, but yeah, I don't really
[00:14:54.080 --> 00:14:58.400]   know about whether it'd be faster, but for our purposes, it works. We're like, don't break
[00:14:58.400 --> 00:15:03.600]   something that doesn't, keep something that works, I guess. Yeah. Yeah, definitely. There's an appeal
[00:15:03.600 --> 00:15:08.640]   to the simplicity of something like hard cascades. It looks like somebody in the chat is suggesting
[00:15:08.640 --> 00:15:13.120]   multitask cascaded convolutional neural networks as a potential option.
[00:15:13.120 --> 00:15:17.680]   Yes. Yeah. Sorry. MTCNN, that's what I meant to say. That was, yeah, that's one. Awesome.
[00:15:18.160 --> 00:15:26.800]   Got you. Ah, okay. That's MTCNN. Cool. Cool. I guess, what would you think, I don't know what
[00:15:26.800 --> 00:15:33.120]   your plans are with this particular project, but let's pretend that this was a minimum viable
[00:15:33.120 --> 00:15:37.920]   product at a company. What do you think your next steps would be with this project? What do you
[00:15:37.920 --> 00:15:44.080]   think, does it need in order to be able to scale or enable to be usable in different domains?
[00:15:45.840 --> 00:15:53.760]   What do you think you would do next? I think it would be cool if, so now we have these few
[00:15:53.760 --> 00:15:57.680]   gestures that we had in mind when we were building this project, right? I think it would be cool to
[00:15:57.680 --> 00:16:04.240]   have a more general gesture recognizer that can perhaps learn to cast our particular gesture with
[00:16:04.240 --> 00:16:10.640]   just a few examples. So that way, different people using this application for different tasks,
[00:16:10.640 --> 00:16:15.120]   they can customize it to their own needs. So maybe they want to, their line of work needs
[00:16:15.120 --> 00:16:22.640]   a particular signal and they could customize it for that. Nice. So I also noticed the way,
[00:16:22.640 --> 00:16:27.760]   it seems like the way you framed it, it's an image convolution, like image classification
[00:16:27.760 --> 00:16:33.040]   task. Did you think at all about how you might extend this to video?
[00:16:33.040 --> 00:16:40.480]   Yeah. I had some idea about how to do it for video, but we didn't really have time to implement it.
[00:16:40.480 --> 00:16:45.440]   So I heard that one way to do it is to have a CNN extract the features and then feed that into an
[00:16:45.440 --> 00:16:51.280]   RNN that takes different frames from the video, which then takes the time series and then produces
[00:16:51.280 --> 00:16:59.200]   some temporary results. Yeah. That's an interesting suggestion. Yeah. I think if you had the compute
[00:16:59.200 --> 00:17:04.560]   transformer type architecture, it could probably handle something like that. But getting that to
[00:17:04.560 --> 00:17:08.640]   work during somebody's live Zoom presentation sounds like a really tough challenge.
[00:17:09.600 --> 00:17:18.000]   There's a question in the YouTube chat. Just the last one here. They are looking for slides.
[00:17:18.000 --> 00:17:22.240]   Will you be able to share these slides? Do you mind if I share them on the video?
[00:17:22.240 --> 00:17:27.200]   Yeah, most definitely. I can send you the link maybe after and then we can make it public for
[00:17:27.200 --> 00:17:34.880]   all access. Yeah. Great. Yeah. I'll put it on the video. Awesome. All right. So I'd love to be able
[00:17:34.880 --> 00:17:40.000]   to ask more questions of all of these groups, but we've got three different great presentations to
[00:17:40.000 --> 00:17:47.600]   talk about. So we've got to move on to the next one. So next up, we have a group that worked on
[00:17:47.600 --> 00:17:57.360]   semantic convolutions on essentially alternatives to convolutional neural networks for computer
[00:17:57.360 --> 00:18:02.960]   vision tasks based, I believe, around capsule nets. So we'll be hearing from Domas and
[00:18:02.960 --> 00:18:11.600]   Kenny on this project. So go ahead, take it away. Awesome. Thank you so much. Yeah. So we really
[00:18:11.600 --> 00:18:16.160]   explored the idea of semantic interpretation of convolutional networks. I want you to get the
[00:18:16.160 --> 00:18:22.960]   next slide. With our objective being vision networks are largely not super interpretable.
[00:18:22.960 --> 00:18:29.200]   And this is more of a research-based project than some of the previous presentation. The intuition
[00:18:29.200 --> 00:18:34.960]   of hierarchical layers of progressively understanding more complicated features, you know,
[00:18:34.960 --> 00:18:39.360]   you have an edge to a part detector to an object detector, doesn't really empirically work out in
[00:18:39.360 --> 00:18:44.320]   practice. You end up getting this big mess. Some of these undesirable properties, there's a whole
[00:18:44.320 --> 00:18:48.320]   laundry list here. We'll get into the more detail, but an over-reliance on texture and need for data
[00:18:48.320 --> 00:18:52.720]   augmentation, as you saw with the previous group, to show all different orientations of your hands
[00:18:52.720 --> 00:18:58.480]   and upside down and whatnot, ways in order for the network to really learn the full picture.
[00:18:58.480 --> 00:19:08.800]   Next slide. So here we have a single activated neuron in a sense to show how confused it can get.
[00:19:08.800 --> 00:19:12.720]   We have the same neuron looking at the eyes, the whiskers of the cat, the legs, and the front of
[00:19:12.720 --> 00:19:19.680]   the car. And so obviously with the next slide, it's doing all three at once, which makes absolutely
[00:19:19.680 --> 00:19:28.640]   no sense. Hence our quest for semantic meaning. So CNNs are brittle. With the Statue of Liberty,
[00:19:28.640 --> 00:19:32.160]   you understand that it is the Statue of Liberty, whether you see it from the back, whether you see
[00:19:32.160 --> 00:19:36.240]   it at nighttime with different shades and textures and colors, but that's not quite obvious to the
[00:19:36.240 --> 00:19:41.520]   network. Hence the need for data augmentation, and which really blows open gaping holes in
[00:19:41.520 --> 00:19:45.760]   modern architectures. We look to the kind of, you know, biology for some motivation.
[00:19:46.720 --> 00:19:54.080]   Next slide, please. And the way that we do it is we reason in a set of invariant primitives. We
[00:19:54.080 --> 00:20:00.160]   don't think of objects in isolation. We think of them as things that are kind of detached from the
[00:20:00.160 --> 00:20:06.400]   orientation, right? And in actual brains, you have circuits that fire whenever the object in any
[00:20:06.400 --> 00:20:11.120]   orientation is present. If the cat is upside down, if it's backwards, if it's dark out, the same
[00:20:11.120 --> 00:20:16.160]   circuit fires for the same reason. So we have a set of invariant primitives. We have a set of
[00:20:16.160 --> 00:20:23.520]   fires for the cat. Next slide. And we do this by imposing geometric coordinate frames upon the
[00:20:23.520 --> 00:20:27.600]   object. So here's Jeffrey Hinton, the creator of the network we're going to show you. He walked
[00:20:27.600 --> 00:20:32.000]   down the halls of MIT with two blocks and asked professors to create a tetrahedron with those
[00:20:32.000 --> 00:20:37.760]   blocks. And none of these brilliant professors, leaders in their fields could do so, even though
[00:20:37.760 --> 00:20:42.080]   they're geniuses, because they think in terms of coordinate frames. They think in terms of geometry.
[00:20:42.080 --> 00:20:47.440]   And the blocks in isolation look like nothing that can form a tetrahedron, yet when you make
[00:20:47.440 --> 00:20:51.840]   the tetrahedron, it's obvious because you impose a coordinate frame, you impose geometry. Next slide.
[00:20:51.840 --> 00:20:59.120]   So the solution is Hinton's Cat's Law networks impose geometric structure on CNNs. So you learn
[00:20:59.120 --> 00:21:04.800]   object representations invariant to viewpoint. Your object does not depend on the way that you
[00:21:04.800 --> 00:21:12.160]   look at it. It just exists. Next slide. So empirically, these networks are very data
[00:21:12.160 --> 00:21:16.400]   efficient. You don't have to augment your data set. You look at an object once and you understand
[00:21:16.400 --> 00:21:20.560]   the way that it works. And because of this, it's also robust to adversarial attacks and shifts in
[00:21:20.560 --> 00:21:24.720]   the way that you present your input to your network. And there's less redundancy. And you
[00:21:24.720 --> 00:21:31.120]   get state-of-the-art performance with an order of magnitude less data. I'm going to turn over to
[00:21:31.120 --> 00:21:37.600]   Mos to explain some of what we did. All right. So here's a bit of an example of what some of
[00:21:37.600 --> 00:21:42.000]   the low-level feature maps look like in CNNs. And you can see that there's a lot of redundancy.
[00:21:42.000 --> 00:21:48.640]   There's a lot of these same frequency activations, which are just rotations of each other. And you
[00:21:48.640 --> 00:21:54.960]   see similar things in the color spaces as well. So that's something that we're trying to avoid with
[00:21:54.960 --> 00:22:03.520]   our usage of capsule networks. So the paper that we're re-implementing is stacked capsule auto
[00:22:03.520 --> 00:22:09.520]   encoders. And this architecture is a combination of two submodels, which is a part capsule auto
[00:22:09.520 --> 00:22:17.600]   encoder and an object capsule auto encoder. The first model, it predicts parts and their poses.
[00:22:17.600 --> 00:22:23.120]   So the parts here are these different little bits that compose the house and tree and such.
[00:22:23.120 --> 00:22:28.480]   And these are the poses that are then predicted. Then the second part of the model predicts
[00:22:28.480 --> 00:22:35.840]   objects. So it associates each of these parts with an object. So it can find that this tree
[00:22:35.840 --> 00:22:42.480]   is composed of this leaf part as well as a bottom trunk part. OK. So I'm going to go into a bit more
[00:22:42.480 --> 00:22:48.560]   detail on the part capsule auto encoder. So this is the first stage, which predicts the positions.
[00:22:48.560 --> 00:22:58.000]   It actually does this using a CNN. And it also uses a decoder, which learns templates.
[00:22:58.000 --> 00:23:05.200]   So in the forward pass of the encoder, you get these poses as well as some descriptive features
[00:23:05.200 --> 00:23:09.600]   about each of the parts. And these are reassembled in the decoder with these
[00:23:09.600 --> 00:23:13.760]   templates to produce a reconstruction. And this is optimized with the log likelihood loss.
[00:23:15.440 --> 00:23:21.120]   And here's some visualizations of what the composition looks like. So these are the learned
[00:23:21.120 --> 00:23:27.760]   templates or parts, which are then transformed and then concatenated together to form this
[00:23:27.760 --> 00:23:34.080]   reconstruction. And you can sort of track using this color coding how each thing sort of
[00:23:34.080 --> 00:23:40.400]   comes together to have this final result. And this is the reconstruction without the
[00:23:40.400 --> 00:23:45.920]   colorization. So you can see it's a very realistic looking tree. And you can interpret these templates
[00:23:45.920 --> 00:23:51.520]   that it learns as strokes. The transformations are like the strokes on canvas. And your final
[00:23:51.520 --> 00:23:59.120]   thing is the digit, of course. So during our experimentation, we studied the evolution of these
[00:23:59.120 --> 00:24:04.000]   different parts, which are learned. So at the very start, it starts with this random noise
[00:24:04.000 --> 00:24:08.960]   initialization. Then it gradually gets chiseled out into more meaningful parts. So here you can
[00:24:08.960 --> 00:24:14.720]   see roughly there's an edge. Here we have some double edges. Here we have a more flat looking
[00:24:14.720 --> 00:24:25.120]   edge. And here we have a very circular object. And over time, we can see that at the very top,
[00:24:25.120 --> 00:24:31.040]   the transformations of each of these parts are pretty nonsensical and naive. It's totally random
[00:24:31.040 --> 00:24:37.600]   noise just getting warped around. But gradually, they get positioned more and more sensibly.
[00:24:38.320 --> 00:24:43.920]   And as the templates at the same time are chiseled out, you end up with these very high quality
[00:24:43.920 --> 00:24:52.160]   reconstructions. So on the very left, on the left column, these are the target images. And the right
[00:24:52.160 --> 00:24:57.600]   are the reconstructions. You can see that they're reconstructed fairly faithfully. And at the very
[00:24:57.600 --> 00:25:02.880]   bottom, you can see the corresponding templates, which are being utilized for this last reconstruction.
[00:25:05.200 --> 00:25:11.040]   OK, so the second part of the SAE model is the object capsule autoencoder, which takes each of
[00:25:11.040 --> 00:25:20.480]   these poses of parts and also feature information and tries to explain the sort of mess of parts.
[00:25:20.480 --> 00:25:25.680]   So it associates them with objects, learning the relative positioning of each of the parts
[00:25:25.680 --> 00:25:32.800]   in the object. And then using this model, we want to optimize the likelihood of what it sees.
[00:25:33.760 --> 00:25:40.160]   So in the final model, we train both autoencoders jointly to optimize
[00:25:40.160 --> 00:25:44.400]   the task for unsupervised object classification.
[00:25:44.400 --> 00:25:50.400]   So empirically, the paper gets really good results on the MNIST and SPHN.
[00:25:50.400 --> 00:25:55.200]   There's still quite a bit of room for improvement, which is what we're trying to address with this
[00:25:55.200 --> 00:26:00.800]   project. So one of the issues is the engineering difficulty. The original template
[00:26:00.800 --> 00:26:09.040]   implementation was in TensorFlow. And it's quite messy because it's research quality code.
[00:26:09.040 --> 00:26:15.200]   So we ported that to PyTorch. And so far, we finished the PCAE re-implementation,
[00:26:15.200 --> 00:26:21.040]   which you saw the results of earlier. And we're currently working on the OCE re-implementation.
[00:26:21.040 --> 00:26:26.720]   We're also working on other improvements, such as reducing the training time, the OCE,
[00:26:27.360 --> 00:26:34.800]   as well as improving the depth of the network. So we want to be able to stack multiple OCEs
[00:26:34.800 --> 00:26:40.160]   after the network so that we can get more rich compositionality in our network structure.
[00:26:40.160 --> 00:26:45.440]   We also did quite a bit of hyperparameter tuning. And we found quite a few interesting
[00:26:45.440 --> 00:26:49.360]   key engineering decisions that the paper doesn't really talk much about.
[00:26:49.360 --> 00:26:55.520]   Yeah, so this is the work we did. Re-implementation, hyperparams,
[00:26:56.400 --> 00:27:01.280]   found some interesting factors. We also did some experiments on USPS, which is very much like
[00:27:01.280 --> 00:27:06.800]   MNIST. But it's a bit different stylistically. So we verified that it works there. And we're
[00:27:06.800 --> 00:27:13.200]   also working on extending reconstructions to color images. They never do anything with color
[00:27:13.200 --> 00:27:19.600]   in their work. OK, so Kenny also did some optimizations. So I'll let him explain this.
[00:27:19.600 --> 00:27:25.600]   Yeah, so basically, the loss is driven by a closed form log likelihood. That's evaluating
[00:27:25.600 --> 00:27:30.880]   the efficacy of capsules and routing their parts to the correct capsule, which is a very complicated
[00:27:30.880 --> 00:27:36.000]   equation. And as you can see, when calculated in closed form, it's considered a computation.
[00:27:36.000 --> 00:27:40.000]   We can leverage a technique called Markov chain Monte Carlo simulation to sample from this
[00:27:40.000 --> 00:27:45.760]   distribution and create an approximate distribution from which we can then pick off
[00:27:45.760 --> 00:27:51.600]   the peak of the distribution, the maximum a posteriori, the map, if you will, to route our
[00:27:51.600 --> 00:27:56.560]   capsule to the part from our sample distribution such that we can use less computation. This is
[00:27:56.560 --> 00:28:01.280]   one of the things we're exploring. We're also exploring using adjustment argmax value to drive
[00:28:01.280 --> 00:28:07.280]   loss in the back propagation instead of a closed form log likelihood, which we speculate. And we
[00:28:07.280 --> 00:28:11.840]   have synthetic preliminary data to show it's not entirely necessary. And it would save a lot of
[00:28:11.840 --> 00:28:19.360]   time and training. All right, so some future work that we're looking to do is finishing up our
[00:28:19.360 --> 00:28:24.160]   PyTorch part. And we have some big ideas which we're shooting for. So one is the RGB dataset
[00:28:24.160 --> 00:28:30.800]   performance. Hopefully, we can get this to speed up to work on the actual OCE model. We also want
[00:28:30.800 --> 00:28:36.560]   to add more capsule layers that we can scale to more complex datasets, like for object classification
[00:28:36.560 --> 00:28:43.040]   in ImageNet or Cypher. And we also want to modernize the PCAE CNN. Right now, it's a very
[00:28:43.040 --> 00:28:49.200]   simple, small CNN architecture. So yeah, that's all. Any questions?
[00:28:49.200 --> 00:28:57.600]   I've got a lot of questions. So we'll try and squeeze as many of them as we can in the minutes
[00:28:57.600 --> 00:29:05.840]   that we have. So you mentioned that it was hard to train because it had a strange loss dynamics.
[00:29:05.840 --> 00:29:11.920]   Could you unpack that a little bit? Maybe I'll talk about the actual
[00:29:11.920 --> 00:29:15.200]   strain of the computation that Damas can talk about. It's kind of the dynamics of pairing,
[00:29:15.200 --> 00:29:23.840]   encoder, likelihoods. So the likelihood itself is a very-- it's a closed form distribution that you
[00:29:23.840 --> 00:29:28.800]   have to calculate with each forward pass in closed form and then derive the argmax from that value,
[00:29:28.800 --> 00:29:35.440]   from that distribution each time. In a sense, it's just a cumbersome way. You can't leverage
[00:29:35.440 --> 00:29:40.960]   sort of automatic differentiation techniques to do things speedily. That's the problem we ran into.
[00:29:40.960 --> 00:29:44.320]   And then Damas also ran into some issues with the PCAE.
[00:29:44.320 --> 00:29:49.760]   Yeah. So what we spent the most time on turning the PCAE is actually getting a nice
[00:29:49.760 --> 00:30:00.880]   balance between the chiseling of these templates. So some extremes that we had to sort of dig
[00:30:00.880 --> 00:30:06.480]   ourselves out of is one, in some cases, the templates weren't actually in frame. So it never
[00:30:06.480 --> 00:30:12.000]   like quite latched on and started training the model to actually make usage of each of these
[00:30:12.000 --> 00:30:16.240]   templates. So they ended up just being totally unused in some cases. In other cases, we had way
[00:30:16.240 --> 00:30:22.480]   too much transformation and had really weird transform parts. Like we had super zoomed in
[00:30:22.480 --> 00:30:30.800]   templates, which had some interesting behaviors. Another one was-- we just didn't have any
[00:30:30.800 --> 00:30:35.840]   transformation going on. So there's a very fine balance to strike in between those two.
[00:30:36.160 --> 00:30:44.080]   I see. Do you think that just it's a matter of picking the right sort of hyperparameters and
[00:30:44.080 --> 00:30:51.520]   defaults to make these capsule nets behave a little bit better during training? The way that
[00:30:51.520 --> 00:30:56.400]   we've sort of come across the right ways to initialize layers, using things like batch norm,
[00:30:56.400 --> 00:31:01.120]   et cetera, to make confets easy to train? Or do you think that it's inherently just going to be
[00:31:01.120 --> 00:31:06.560]   harder to train something like a capsule net than something like a feedforward regular convolutional
[00:31:06.560 --> 00:31:16.400]   net? So we found that actually after we made one key discovery, which is that we want to learn
[00:31:16.400 --> 00:31:23.040]   the transformations or the transformations outputted by our CNN need to be in inverse space.
[00:31:23.040 --> 00:31:28.720]   So basically, we output some 3 by 3 transformation matrix. We invert that and then use that.
[00:31:29.440 --> 00:31:35.680]   And basically, what that means is you can think of it as the space which you're optimizing over
[00:31:35.680 --> 00:31:41.920]   has a totally different morph to it. So it's going to be expanded in some regions and shrunk on some
[00:31:41.920 --> 00:31:46.160]   other regions. So because of that, you get a much better loss surface to optimize over. And
[00:31:46.160 --> 00:31:51.120]   everything just starts magically working. It makes sense. And to also answer your question
[00:31:51.120 --> 00:31:56.480]   in terms of tooling, I think some things will need to change a little bit. You're not building
[00:31:56.480 --> 00:32:02.080]   these vector Jacobian product type computations. You're really looking at this likelihood. And
[00:32:02.080 --> 00:32:07.120]   there's going to need some more well-defined computational building blocks to make that
[00:32:07.120 --> 00:32:11.840]   happen. We're going to pursue sampling to see if we can get an approximate answer to that question,
[00:32:11.840 --> 00:32:16.720]   especially because we don't have to be super specific. When we generate this distribution
[00:32:16.720 --> 00:32:22.080]   over capsules which are good, we don't need to be very specific over the region where the map is.
[00:32:22.080 --> 00:32:28.160]   We can be in a general area to route the correct capsule. So that suggests that this could work.
[00:32:28.160 --> 00:32:30.000]   But there are other things we need to look at as well.
[00:32:30.000 --> 00:32:38.400]   I see. Yeah. To Dimas' point, I know that in other domains, actually sampling,
[00:32:38.400 --> 00:32:44.000]   the parameterization, it's very well known that the parameterization of your model is very
[00:32:44.000 --> 00:32:48.880]   important. And there are tricks like taking inverses and taking logarithms that help a lot
[00:32:48.880 --> 00:32:54.000]   in getting a well-conditioned loss surface. So it could be, yeah, that those are the kinds of
[00:32:54.000 --> 00:33:00.000]   things that need to be discovered. And to your point, yeah, there's a lot more tooling that
[00:33:00.000 --> 00:33:06.320]   could be built for probabilistic programming and for differentiable probabilistic programming that
[00:33:06.320 --> 00:33:12.720]   could probably unlock a lot more capacity in things like capsule nets that might marry those
[00:33:12.720 --> 00:33:20.240]   two approaches. Yeah, like we were just using the PyMC3 Bayesian stat package which had no sort of
[00:33:20.240 --> 00:33:24.400]   fine-tuned computational optimization. We still got decent results with synthetic data. So
[00:33:24.400 --> 00:33:29.600]   something for improvement there. Definitely. There's a question in the chat.
[00:33:29.600 --> 00:33:37.760]   Testing semantic correctness can be via saliency on expected inputs, like on MNIST, the difference
[00:33:37.760 --> 00:33:43.520]   between digits 5 and 6, see that the circle of the 6 is complete or not. Have you looked into
[00:33:43.520 --> 00:33:46.960]   this? I guess they're asking, have you-- you mentioned that you wanted to make convents
[00:33:46.960 --> 00:33:52.000]   more interpretable. Have you looked at the behavior of your capsule net to see if it is
[00:33:52.000 --> 00:33:56.880]   more interpretable, if you see these sorts of semantic distinctions like they're talking about?
[00:33:56.880 --> 00:34:05.680]   So we haven't used any of the techniques which are standard for CNNs. In fact, in our surveys,
[00:34:05.680 --> 00:34:11.120]   we actually found that there's a good amount of controversy as to what these visualizations are
[00:34:11.120 --> 00:34:18.400]   actually doing. So we wanted to try and move away from these post-hoc attempts to understand what's
[00:34:18.400 --> 00:34:25.280]   happening so that we can build the understanding into the network. So what the network is doing
[00:34:25.280 --> 00:34:29.760]   is clear from the structure of the network. You don't have to do any inspection afterwards.
[00:34:32.480 --> 00:34:39.040]   I see. That's an interesting point. And that actually brings me to the last question,
[00:34:39.040 --> 00:34:45.680]   one of the major questions that I had, which is also a tough question. The vibe in the ML
[00:34:45.680 --> 00:34:52.080]   community over the last year has shifted really heavily away from inductive bias with the success
[00:34:52.080 --> 00:35:00.880]   of transformer models on multiple domains where they don't have necessarily a super great inductive
[00:35:00.880 --> 00:35:09.680]   bias, which sparked, I believe, Richard Sutton's famous post, The Bitter Lesson, that in the end,
[00:35:09.680 --> 00:35:16.960]   scaling compute tends to beat out these architectures with special things built into
[00:35:16.960 --> 00:35:26.400]   them. So I'm curious, do you think that's a fad? Or how do you think that this taste for
[00:35:26.400 --> 00:35:32.880]   transformers and generic architectures dovetails with-- broadly dovetails with your particular
[00:35:32.880 --> 00:35:39.520]   interest in these more biased, structured architectures? I think there's a lot of parallel.
[00:35:39.520 --> 00:35:42.800]   I think these are almost the same thing. At least they're converging towards the same approach.
[00:35:42.800 --> 00:35:46.800]   Attention mechanisms are taking permutation-invariant sets of objects and deriving
[00:35:46.800 --> 00:35:51.280]   importance that are irrespective of how far away those things are apart from each other.
[00:35:51.280 --> 00:35:54.400]   The same essential mechanism is happening when a capsule is aboding on a part.
[00:35:55.280 --> 00:36:00.640]   And in fact, the architecture, it draws a lot of similarities with hierarchical temporal models,
[00:36:00.640 --> 00:36:04.880]   so those Bayesian inference type models that are biologically inspired. All three of them seem to
[00:36:04.880 --> 00:36:10.080]   be converging towards that general trend of attention. I think that's where this is all going.
[00:36:10.080 --> 00:36:18.400]   Another counterpoint is that models with really good bias typically have better data efficiency.
[00:36:18.400 --> 00:36:21.760]   So this was actually shown in capsules. They're actually an order of magnitude
[00:36:22.320 --> 00:36:25.840]   better than other typical approaches to MNIST classification.
[00:36:25.840 --> 00:36:34.720]   That's a good point. And to Kenny's point, so the research group that I worked at when I was a grad
[00:36:34.720 --> 00:36:38.320]   student at Berkeley, the Redwood Center for Theoretical Neuroscience, is very interested
[00:36:38.320 --> 00:36:42.480]   in a lot of the ideas that you've mentioned-- dynamic routing, hierarchical temporal memory.
[00:36:42.480 --> 00:36:50.240]   It was originally created by Jeff Hawkins of Numenta. So if you haven't swung by the Redwood
[00:36:50.240 --> 00:36:54.160]   Center's Zoom meetings, you definitely should email me afterwards if you want to hear more
[00:36:54.160 --> 00:37:00.560]   about that. That's a good point, that the attention mechanism there is very, very similar.
[00:37:00.560 --> 00:37:04.000]   It's all about invariance. I mean, that's what Hawkins talks about in his book, right?
[00:37:04.000 --> 00:37:08.320]   That's the key to biological interpretation, is learning invariant representation of objects.
[00:37:08.320 --> 00:37:09.280]   That's what all these things are doing.
[00:37:09.280 --> 00:37:16.960]   Neat. All right. Well, that is all the time that we have, so we'll have to continue that
[00:37:16.960 --> 00:37:22.240]   interesting conversation on a later date. So thanks to Kenny and Damas and their team for
[00:37:22.240 --> 00:37:27.840]   presenting that cool work on capsule nets. And let's go to our final set of speakers.
[00:37:27.840 --> 00:37:38.720]   So we just had Damas and Kenny, and now we have, I believe, Pedro and Oliver, who will be presenting
[00:37:38.720 --> 00:37:48.080]   their work on adversarial attacks. So also a little bit more research-oriented than our first
[00:37:48.080 --> 00:37:53.840]   video gesture overlay, but maybe sort of in the intersection of what's of interest to researchers
[00:37:53.840 --> 00:37:58.160]   and what's of interest to folks doing ML engineering. So Oliver and Pedro, take it away.
[00:37:58.160 --> 00:38:02.480]   Thank you. Let me share screens.
[00:38:03.360 --> 00:38:11.440]   Okay. So my name is Oliver, and this semester, we have been, my team and I, me, Vincent, Nabil,
[00:38:11.440 --> 00:38:18.160]   and Pedro have been working with Nicholas Carlini on attacking adversarial defenses. And I know
[00:38:18.160 --> 00:38:24.080]   that's very general, if you know anything about the field. And so we're trying to own a specific
[00:38:24.080 --> 00:38:30.320]   type of defense, and we're trying to work with Nicholas Carlini on attacking adversarial defenses.
[00:38:30.320 --> 00:38:35.040]   We're trying to own a specific type of defense and try and come up with a more general attack
[00:38:35.040 --> 00:38:42.160]   framework so that we can have a better metric in a lot of these papers, because some of these
[00:38:42.160 --> 00:38:46.400]   defenses claim to work really well, but they don't actually attack themselves very well, because
[00:38:46.400 --> 00:38:51.920]   that doesn't really help their claims. And we really want a better, more principled way to
[00:38:51.920 --> 00:38:58.960]   attack these defenses. So first some preliminaries. So in case you don't know, an adversarial attack
[00:38:58.960 --> 00:39:04.480]   on a neural network is a perturbation of some input, which causes the network to misclassify
[00:39:04.480 --> 00:39:10.160]   the input. So in this image at the bottom, you have all of these inputs, and the line is splitting
[00:39:10.160 --> 00:39:13.760]   them up well. And then if you allow some perturbations, you allow them to move around,
[00:39:13.760 --> 00:39:19.440]   then you can get misclassifications. And then we make sure that the constraint, or we make sure
[00:39:19.440 --> 00:39:24.800]   there's a constraint so that the perturbation isn't too big, so the original images don't get too far
[00:39:24.800 --> 00:39:30.000]   away. And this is a really big vulnerability in safety critical fields. For example, self-driving
[00:39:30.000 --> 00:39:35.040]   cars, because you can have things like hiding a stop sign from the computer vision model of the
[00:39:35.040 --> 00:39:39.440]   car, and that's really, really dangerous in such a safety critical field. So defending against these
[00:39:39.440 --> 00:39:45.920]   kinds of attacks is really important. So here is a sample attack, just a really simple example
[00:39:45.920 --> 00:39:52.480]   that's used for a lot of just like prototyping. It's called the fast gradient sign method. And
[00:39:52.480 --> 00:39:57.520]   what you're doing is you take the image, and then instead of doing gradient descent on the
[00:39:57.520 --> 00:40:02.000]   parameters of a neural network like you'd normally do when training it, you do gradient descent on
[00:40:02.000 --> 00:40:05.600]   the image itself. So you take the gradient with respect to the image, and then you add that
[00:40:05.600 --> 00:40:10.800]   instead. And what this does is it forces your loss to increase when you add this perturbation.
[00:40:10.800 --> 00:40:14.560]   And so as you can see in this example, we take the original image, we add a bunch of what looks
[00:40:14.560 --> 00:40:19.840]   like random noise, but it's actually specifically selected to attack this model. And it doesn't
[00:40:19.840 --> 00:40:24.480]   look like the image has changed at all, but it completely changes the classification and increases
[00:40:24.480 --> 00:40:29.360]   the confidence drastically. And this is really dangerous. And so finally, here is an example
[00:40:29.360 --> 00:40:37.360]   defense, which was proposed in Madry 2017, which is one of the most important papers in this field,
[00:40:37.360 --> 00:40:42.480]   where you can use FDSM or some other similar attacks and use it as like a form of data
[00:40:42.480 --> 00:40:47.520]   augmentation during the training, where you do your normal training, and then in the middle,
[00:40:47.520 --> 00:40:51.440]   you take some examples, you take your batch that you're training on, and then you get some
[00:40:51.440 --> 00:40:55.840]   adversary, you attack them while you're training using the current model, and then you pass those
[00:40:55.840 --> 00:40:59.840]   in as more training examples. And then you hope that this makes your model more robust and gives
[00:40:59.840 --> 00:41:05.120]   you like a more nonlinear surface like this that fits the perturbations better. And then I'm going
[00:41:05.120 --> 00:41:11.040]   to pass it off to Pedro. Okay, so now that we see why having adversary robustness is important,
[00:41:11.040 --> 00:41:16.240]   let's talk about why it's hard to do. So looking at this from the attacker's perspective, we see
[00:41:16.240 --> 00:41:20.800]   that all they really have to do is to maximize this equation, which really means that they have
[00:41:20.800 --> 00:41:27.120]   to maximize the loss, adding some perturbation onto the original image, the loss compared to some
[00:41:27.120 --> 00:41:32.800]   truth value. And so we have this loss being very high, and the perturbation be very small or within
[00:41:32.800 --> 00:41:38.400]   some bound. The first perspective here is that it's hard to construct a theoretical model of what
[00:41:38.400 --> 00:41:44.080]   the attacker is doing, and how they actually create these adversarial examples. This optimization
[00:41:44.080 --> 00:41:49.360]   problem is nonlinear and non-convex, so it's hard to generalize overall to see how different
[00:41:49.360 --> 00:41:54.800]   attackers make these adversarial examples. The second perspective is that adversarial robustness,
[00:41:54.800 --> 00:41:59.040]   which means just protecting yourself against adversarial attacks, requires models to have
[00:41:59.040 --> 00:42:05.120]   good outputs for all inputs. And it's pretty easy for an attacker to make an adversarial attack,
[00:42:05.120 --> 00:42:09.200]   but it's very hard for a defender to defend against all possible adversarial attacks.
[00:42:12.800 --> 00:42:17.440]   And here we have an example of adversarial detection. What this is, is what if we could
[00:42:17.440 --> 00:42:22.640]   have a layer that before we input anything into our model, we have a layer that tells you that,
[00:42:22.640 --> 00:42:27.440]   yes, this is an adversarial attack, or no, this is a good piece of input that we want to continue
[00:42:27.440 --> 00:42:31.840]   with. And so this picture shows just a layer in between the adversarial example and the actual
[00:42:31.840 --> 00:42:36.720]   model that will tell us that information. And the problem with this is that some models are not able
[00:42:36.720 --> 00:42:41.440]   to ignore this input. And so what this layer would do is, if it's good, pass it on. If it's bad,
[00:42:41.440 --> 00:42:46.480]   throw it away. But if we go back to the self-driving car example, the self-driving cars
[00:42:46.480 --> 00:42:50.560]   can't throw away an image. They have to make a decision. If they see a stop sign, they can't
[00:42:50.560 --> 00:42:57.840]   believe that it's adversarial and just do nothing about it. And so talking about some very rudimentary
[00:42:57.840 --> 00:43:03.600]   defenses, the idea here is that what if we have in between our adversarial example and our actual
[00:43:03.600 --> 00:43:08.320]   model a layer that pre-processes the image. So this is not exactly throwing away the image. This
[00:43:08.320 --> 00:43:13.040]   is just pre-processing it, throwing it through some function. And in this case, we'll look at
[00:43:13.040 --> 00:43:17.920]   the Gaussian blur. The idea is just to blur the image, a normal Gaussian blur. And the intuition
[00:43:17.920 --> 00:43:22.400]   is that what if we can smooth out the high frequency noise, and we can hope that the adversarial
[00:43:22.400 --> 00:43:27.600]   component of the input is in that noise. And we can smooth that out and essentially throw only
[00:43:27.600 --> 00:43:34.960]   the adversarial noise away. It turns out that this is not a very good idea. And we can actually
[00:43:34.960 --> 00:43:39.600]   attack this very easily. And the idea here is just we have to fool both the original model and the
[00:43:39.600 --> 00:43:43.920]   defended model. If we try and fool any of them individually, let's say we have the defended
[00:43:43.920 --> 00:43:48.080]   model, which has that Gaussian blur pre-processed step. If we defend that, we might produce
[00:43:48.080 --> 00:43:53.840]   adversarial examples that the original model cannot produce good outputs for. And so we want
[00:43:53.840 --> 00:43:58.720]   to attack both the original model and the defended model. And we can chop this together just using
[00:43:58.720 --> 00:44:02.640]   a linear combination on the losses and take the gradients with respect to their inputs,
[00:44:02.640 --> 00:44:06.800]   and then produce adversarial examples that work for both the original and the defended model.
[00:44:06.800 --> 00:44:13.040]   A defense that's a little more complicated uses ensemble methods. And the idea behind the
[00:44:13.040 --> 00:44:17.680]   ensemble is just that, OK, you have an input. I'm going to give it to, in this picture,
[00:44:17.680 --> 00:44:21.840]   four different models. And the models are going to produce some output and vote on the output
[00:44:21.840 --> 00:44:28.320]   or have some average of the output. So the actual ensemble prediction will be some function of the
[00:44:28.320 --> 00:44:33.440]   outputs of those models. And in this case, we can have two different types of models. The first is
[00:44:33.440 --> 00:44:38.240]   the full precision model, which gives high accuracy but low robustness. And the second is the low
[00:44:38.240 --> 00:44:43.120]   precision model, which has high robustness and low accuracy. The high robustness factor of the low
[00:44:43.120 --> 00:44:48.000]   precision model comes from quantization. When I say quantization, I just mean chopping off some
[00:44:48.000 --> 00:44:53.040]   of the bits of the input so that, essentially, we're taking away some of the lower precision
[00:44:53.040 --> 00:44:57.920]   parts of the image. And the idea is that, what if we can get the best of both worlds? We want
[00:44:57.920 --> 00:45:03.840]   the high accuracy and the high robustness of the models. And that's the idea behind the ensemble
[00:45:03.840 --> 00:45:10.000]   method. It turns out that this does boost adversarial accuracy by about 15.3%. But we'll
[00:45:10.000 --> 00:45:16.320]   talk about why it doesn't quite work as well as we want it to on the next slide. We can attack this.
[00:45:16.320 --> 00:45:21.680]   The insight here is that this defense just masks the gradient signal. It doesn't actually really
[00:45:21.680 --> 00:45:28.160]   increase the robustness. A big consequence of ensembles is that it masks the gradient because
[00:45:28.160 --> 00:45:33.920]   it uses several different models that either vote or average on an output. And so what we found was
[00:45:33.920 --> 00:45:39.280]   that the full precision models have a very, very similar loss surface to the loss surface of the
[00:45:39.280 --> 00:45:44.160]   entire ensemble. And because this is the case, the gradients are not very well hidden. We can just
[00:45:44.160 --> 00:45:50.400]   retrieve them by using the full precision model. And so what we do is we just take the gradients
[00:45:50.400 --> 00:45:53.760]   from the full precision model, find some adversarial example using methods that are
[00:45:53.760 --> 00:45:59.440]   already known like FGSM or PGD, and then we can plug this in through the actual entire ensemble.
[00:45:59.440 --> 00:46:01.920]   And this works a majority of the time.
[00:46:01.920 --> 00:46:12.480]   OK. And then a second paper that we've been working on is there's another proposed
[00:46:12.480 --> 00:46:17.280]   defense which seems quite different from the last one. It does use ensembles as well,
[00:46:17.280 --> 00:46:21.760]   but the mechanisms underneath rather than doing quantization to increase robustness
[00:46:21.760 --> 00:46:28.960]   is they use some work from error correction codes. So they try to use error correcting codes to
[00:46:28.960 --> 00:46:35.520]   increase their robustness. And so the key insight is that when you do adversarial perturbations for
[00:46:35.520 --> 00:46:42.000]   a neural network, you're taking your inputs and then you're perturbing it, and then you're trying
[00:46:42.000 --> 00:46:47.600]   to change the outputs. And so when you have the one-hot encoding like you would normally do,
[00:46:47.600 --> 00:46:53.760]   all that you have to do is change a single bit to get a new output. If you have 1, 0, 0,
[00:46:53.760 --> 00:46:58.160]   all you have to do is change that to 0, 1, 0. And that technically just requires changing two bits.
[00:46:58.160 --> 00:47:03.840]   You reduce the 1 to a 0, and then you increase the 0 to a 1. And so that's called the Hamming
[00:47:03.840 --> 00:47:07.920]   distance between those two bit codes. And what error correcting codes do is they let you have
[00:47:07.920 --> 00:47:14.560]   a larger Hamming distance between these encodings for the outputs. And what they do is they design
[00:47:14.560 --> 00:47:19.920]   a codebook that instead of using the identity function, which is the one-hot encoding code word,
[00:47:19.920 --> 00:47:27.760]   where you have just 1, 0, 0, 0, 0, 1, 0, 0, et cetera, you use more complicated code words here.
[00:47:27.760 --> 00:47:32.640]   And so here are some pictures from the paper where on the left is the original models just
[00:47:32.640 --> 00:47:37.760]   without this new loss. And it's pretty easy to go from low probabilities for a class to high
[00:47:37.760 --> 00:47:41.760]   probabilities for a different class. You just have to cross this little boundary in the middle here.
[00:47:41.760 --> 00:47:46.960]   But in the middle one, it's much harder to get from something in the red side. And you take
[00:47:46.960 --> 00:47:52.960]   some harder traversal through the green, and then you get to the blue. And so it makes it so that
[00:47:52.960 --> 00:47:59.040]   your model is more robust, and it's harder to change codes. And so in practice, they use Hadamard
[00:47:59.040 --> 00:48:04.240]   codes. And so this is an example. Instead of using the identity matrix and one-hot encoding,
[00:48:04.240 --> 00:48:08.240]   you use these codes that have a larger Hamming distance between them. And so it makes it harder
[00:48:08.240 --> 00:48:13.760]   to flip the bits, and it makes it harder to perturb the inputs, to give a fake example.
[00:48:13.760 --> 00:48:19.360]   And then for the loss that they use is you just take some sigmoid, some increasing function
[00:48:19.360 --> 00:48:24.640]   bounded between 0, 1 of the outputs of the neural network, and then you do the dot product of that
[00:48:24.640 --> 00:48:30.720]   with the corresponding row for whatever class this is an example of. And then you do a soft
[00:48:30.720 --> 00:48:34.640]   max-like loss, where you end up getting a probability out of this. And then you maximize
[00:48:34.640 --> 00:48:39.520]   this loss over the average over the batch. And so this gives you actually a probability. And it
[00:48:39.520 --> 00:48:43.840]   turns out that this paper, in addition to improving robustness, also increases the probability
[00:48:43.840 --> 00:48:49.600]   estimates. Because a lot of neural networks, they output the probability of it being a class. But
[00:48:49.600 --> 00:48:54.160]   lots of times, we just ignore that and think, oh, it is this class or it isn't this class.
[00:48:54.160 --> 00:48:57.600]   But it turns out that this can give you more robust probability estimates as well.
[00:48:57.600 --> 00:49:02.400]   And then finally, they also just throw in an ensemble for the same reason as the
[00:49:02.400 --> 00:49:07.680]   impure model. Because it'll just increase the effective Hamming distance between all of the
[00:49:07.680 --> 00:49:13.680]   codes if we have lots of different codes at the same time, like working together to vote on a
[00:49:13.680 --> 00:49:18.160]   single output. So when you're training these, each classifier has different parameters. They
[00:49:18.160 --> 00:49:21.840]   don't share parameters. And then they're just solving a different classification problem.
[00:49:21.840 --> 00:49:28.640]   Because you have different encodings of the outputs. And then finally, we were trying to
[00:49:28.640 --> 00:49:32.480]   attack this paper and show that it didn't really work as well as they claimed. And the reason that
[00:49:32.480 --> 00:49:35.840]   we knew that it wouldn't work as well as they claimed is because if you look at this graph here,
[00:49:35.840 --> 00:49:40.720]   the two outputs that are kind of their state-of-the-art methods are the green and the
[00:49:40.720 --> 00:49:47.360]   blue, the light blue over here. And if you look at epsilon 0.5, they claim that their accuracy is
[00:49:47.360 --> 00:49:52.560]   around 40% to 50% for an epsilon value of 0.5, which is the bounds for our perturbations.
[00:49:52.560 --> 00:49:59.600]   But at this bound level, our images, our pixels, are mapped between 0 and 1. And so if you think
[00:49:59.600 --> 00:50:05.920]   about it, if you have an epsilon of 0.5, you can turn any image into just complete grayscale and
[00:50:05.920 --> 00:50:11.040]   just take any image and convert it to all 0.5 for every pixel value in the image. And that fits
[00:50:11.040 --> 00:50:15.520]   within the bounds of this perturbation, which means that's a valid perturbation for this value.
[00:50:16.480 --> 00:50:21.200]   Like that's a valid attack for this constraint. But if you get that and there's 10 classes and
[00:50:21.200 --> 00:50:27.200]   every image is exactly the same grayscale image, there's no way you could get above 10% accuracy.
[00:50:27.200 --> 00:50:34.400]   Yet they report 40% to 50% accuracy. So something is not working properly with their model or they're
[00:50:34.400 --> 00:50:39.680]   not attacking it properly, which was kind of suspicious. And so similar to the impure defense,
[00:50:39.680 --> 00:50:44.640]   we noticed that the error-correcting code defense here was performing gradient masking.
[00:50:44.640 --> 00:50:49.520]   And the way that we realized this was in this loss function here, you realize that
[00:50:49.520 --> 00:50:53.760]   they take some sigmoid function. And in the actual code, when you look at this,
[00:50:53.760 --> 00:50:59.280]   they take the log of this. They take a log in the loss somewhere. And after they take this log,
[00:50:59.280 --> 00:51:03.200]   it causes the gradients to get really, really, really small. And then it causes a vanishing
[00:51:03.200 --> 00:51:09.520]   gradient issue. And so in effect, what that does is it just masks the gradient similar to before,
[00:51:09.520 --> 00:51:12.960]   where it makes the gradients really hard to read and there's not very much information from them
[00:51:12.960 --> 00:51:17.440]   because of that. But what you can do is you just take a similar model where you get a smooth
[00:51:17.440 --> 00:51:21.600]   approximation of the gradients and you get rid of this vanishing gradient problem. And it's not the
[00:51:21.600 --> 00:51:26.480]   exact same model, but it's pretty accurate and almost exactly the same, just without the masking
[00:51:26.480 --> 00:51:32.240]   gradients. And then you just attack that instead and use those attacks for the new model. And what
[00:51:32.240 --> 00:51:38.320]   that does is it reduces the accuracy already from 50% to 15%. And that was the first thing we noticed.
[00:51:38.320 --> 00:51:42.240]   And we're still working on this, but it was a pretty drastic improvement already from even the
[00:51:42.240 --> 00:51:48.560]   claims in the paper. And so that's a general theme with a lot of these papers is that they claim that
[00:51:48.560 --> 00:51:54.720]   they work very, very well. And then you try to attack them using stronger attacks. And they don't
[00:51:54.720 --> 00:51:59.440]   work as well as they originally claimed. And it seems like they didn't put in all of their effort
[00:51:59.440 --> 00:52:05.920]   to doing the attacks, which makes sense because their focus is the defense. But it's not really
[00:52:05.920 --> 00:52:10.480]   a valid claim that their defense works amazingly if we can attack it by putting in a little bit
[00:52:10.480 --> 00:52:14.560]   more effort. And so the goal is to generalize these attacks that we've been doing. There are
[00:52:14.560 --> 00:52:20.240]   some similarities. We plan to generalize these attacks to a larger class of defenses so that we
[00:52:20.240 --> 00:52:26.080]   can find the commonalities between the errors in a lot of these defenses and figure out a more
[00:52:26.080 --> 00:52:31.600]   principled way to attack them. And in particular, we really want to focus on the class of detection
[00:52:31.600 --> 00:52:38.160]   defenses that Pedro mentioned at the beginning, where you try to either discard or not discard
[00:52:38.160 --> 00:52:41.360]   images based on whether it's an adversarial example or not. And so that's what we're going
[00:52:41.360 --> 00:52:47.360]   to be shifting to soon now that we have a good amount of practice attacking some defenses.
[00:52:47.360 --> 00:52:56.480]   So yeah. And now if anybody has any questions. >> Great. Yeah, I've got a lot of questions. I
[00:52:56.480 --> 00:53:02.480]   think I'm going to start off with one that's closer to a comment, but I'm hoping to spark
[00:53:02.480 --> 00:53:07.440]   a discussion. So you mentioned that the error correcting code approach gives you better
[00:53:07.440 --> 00:53:12.400]   calibrated outputs in terms of the class probabilities. One thing I've noticed when
[00:53:12.400 --> 00:53:19.440]   I've done work on adversarial attacks and defenses is that people's baselines tend to be really badly
[00:53:19.440 --> 00:53:25.280]   calibrated because the default of a neural network is to be badly calibrated. Those class probabilities
[00:53:25.280 --> 00:53:32.800]   that you get out on held out data are not good. If it says it's 0.1, that it's some other digit
[00:53:32.800 --> 00:53:40.560]   or some other class, it's actually maybe more like 25 or 30% probability. And so there are ways to
[00:53:40.560 --> 00:53:46.000]   fix that. And what I found is that if you just make a model that's a little bit better calibrated,
[00:53:46.000 --> 00:53:52.640]   your adversarial defense, you actually become slightly more adversarially robust. And so it's
[00:53:52.640 --> 00:53:59.360]   very easy when you're just designing a defense to accidentally just calibrate your model better.
[00:53:59.360 --> 00:54:03.760]   And then in the process, that makes it look like you've come up with a new way to become--
[00:54:03.760 --> 00:54:10.960]   to have a better defense. I'm curious, I guess, to turn that into a question. Did you notice
[00:54:10.960 --> 00:54:15.200]   anything like that when you were looking at all the different defenses and superior attacks that
[00:54:15.200 --> 00:54:21.120]   you-- in your review of the literature and the work you did? So I can ask Pedro if he noticed
[00:54:21.120 --> 00:54:26.720]   anything for the imp here. But it seems like something like that is going on here because
[00:54:26.720 --> 00:54:30.400]   we know how to attack lots of models at the moment. There's lots of defenses that have been
[00:54:30.400 --> 00:54:36.960]   published. And Carlini has been instrumental in attacking lots of them. And what happens in this
[00:54:36.960 --> 00:54:41.920]   one is they claim to work really well, but it clearly doesn't. So my guess is that for our
[00:54:41.920 --> 00:54:46.000]   model, there is something like that going on. The effect of this paper is just that it gives
[00:54:46.000 --> 00:54:51.360]   you better calibration. And then they throw lots of theory in there. And in effect, it's really
[00:54:51.360 --> 00:54:55.840]   just improving the calibration. And then as a result, it gives you a little bit better robustness.
[00:54:55.840 --> 00:54:59.760]   But then you just have to fiddle with a little bit of the attack parameters. And then you can
[00:54:59.760 --> 00:55:04.800]   get it to work. I'm not sure if you noticed anything like that as well, Pedro. Yes. So I
[00:55:04.800 --> 00:55:09.920]   think that's really a major theme. We're seeing a lot of these papers from iClear in Europe being
[00:55:09.920 --> 00:55:15.120]   broken very easily because all they do is mask gradients. And really what they try to do, they
[00:55:15.120 --> 00:55:22.320]   focus on making it hard to retrieve these gradients so that we cannot run FGSM or PGD or other attacks
[00:55:22.320 --> 00:55:26.480]   on them. But they're not really making it more robust. They're just making it harder to get
[00:55:26.480 --> 00:55:31.920]   those gradients. And the problem with that is that they can laser focus on, here's a model that
[00:55:31.920 --> 00:55:37.760]   is hard to get gradients from. But we can just build a substitute model that is on our own accord,
[00:55:37.760 --> 00:55:45.280]   has our own rules, that has smooth gradients that we can attack ourselves. And those adversarial
[00:55:45.280 --> 00:55:49.120]   examples that come out of those attacks from models we build that are similar to the models
[00:55:49.120 --> 00:55:53.680]   that are claimed to be robust, we can just transfer those over and they work a majority of the time.
[00:55:53.680 --> 00:56:03.040]   On that front, I guess, what do you think could be done to make research in adversarial attacks
[00:56:03.040 --> 00:56:09.120]   and defenses better to avoid this problem? Do you think that there are some sort of institutional
[00:56:09.120 --> 00:56:15.360]   changes that could be done that could help this? Or is it just we just have to
[00:56:15.360 --> 00:56:22.880]   learn to do better when we do this research? I think it's kind of hard. Maybe there is
[00:56:22.880 --> 00:56:27.840]   institutional stuff you could do. I'm not sure. But I think the issue is that a lot of these
[00:56:27.840 --> 00:56:34.320]   attacks are very specifically tailored to every single defense. And so what happens is people
[00:56:34.320 --> 00:56:41.680]   take some general attack that is just very general and you have to tailor it a lot. And maybe for
[00:56:41.680 --> 00:56:48.720]   these examples, they do gradient masking. And so we have to do lots of stuff to do, for example,
[00:56:48.720 --> 00:56:53.360]   FGSM. We would do FGSM on a different model and then use those attacks. And you have to tailor
[00:56:53.360 --> 00:56:58.800]   it a lot to every specific defense. And a lot of the defenses simply just take the attack out of
[00:56:58.800 --> 00:57:04.720]   the box and apply it and say, this doesn't work. And this is the strongest attack that exists.
[00:57:04.720 --> 00:57:09.120]   Therefore, our model works really well. And they don't really put in the amount of effort that they
[00:57:09.120 --> 00:57:14.480]   should to attack their model really, really hard. And I guess it makes sense from their perspective.
[00:57:14.480 --> 00:57:18.240]   If they spend a couple of months working on this defense and then you spend a couple of months
[00:57:18.240 --> 00:57:23.600]   attacking it, and then you actually attack it, then all of your time is wasted. And people don't
[00:57:23.600 --> 00:57:28.880]   really post negative results as often. Maybe that's an issue with the field as a whole. But
[00:57:28.880 --> 00:57:33.600]   negative results aren't as popular to post. And so it seems like if you did that kind of thing,
[00:57:33.600 --> 00:57:38.720]   then there would be a lot less defense papers. And so they just end up not attacking them as
[00:57:38.720 --> 00:57:42.800]   well as they should. And so that's kind of one of our goals is that we want to be able to have
[00:57:42.800 --> 00:57:47.120]   a more principled way to attack these methods. And if we do come up with something like that, then
[00:57:47.120 --> 00:57:53.200]   a lot of the claims that these defenses can make if they do claim to be defendable,
[00:57:53.200 --> 00:57:57.600]   or if they do claim to be able to defend such a principled attack, we would hope that that would
[00:57:57.600 --> 00:58:03.120]   improve the field as a whole. And it makes these attacks, it makes their claims much more accurate
[00:58:03.120 --> 00:58:12.960]   when they say that we are actually robust. >> Yeah, I suppose in other scientific fields,
[00:58:12.960 --> 00:58:19.760]   there's a lot more peer review and a lot more rigorous peer review than in ML, which has brought
[00:58:19.760 --> 00:58:25.040]   its benefits and its drawbacks. And perhaps one of the drawbacks is that it's easier to get a paper
[00:58:25.040 --> 00:58:30.720]   published that hasn't been subjected to a rigorous enough attempt to counter claim.
[00:58:30.720 --> 00:58:37.920]   >> Yeah, but I guess, I mean, we still do see the scientific method happening, right? People
[00:58:37.920 --> 00:58:44.640]   post a defense and then Karlini comes and attacks it. And so that's the peer review in action,
[00:58:44.640 --> 00:58:45.720]   I guess. >>
[00:58:45.720 --> 00:58:53.920]   Yeah, it's slower, or maybe not even slower. It's just public instead of private, which is maybe.
[00:58:53.920 --> 00:58:55.320]   >> Yeah. >>
[00:58:58.000 --> 00:59:04.560]   Great, it's really great to see the work that you've done and that everybody at MLab was able
[00:59:04.560 --> 00:59:09.760]   to do in this semester. Really impressive the amount of work that you're able to get done.
[00:59:09.760 --> 00:59:16.320]   And so I'm looking forward to seeing more from you all, both as you stay at MLab and as you move on
[00:59:16.320 --> 00:59:28.640]   to become ML engineers and researchers and full members of the field. Very exciting.
[00:59:28.640 --> 00:59:40.960]   >> Thank you.
[00:59:40.960 --> 00:59:43.540]   (upbeat music)
[00:59:43.540 --> 00:59:45.540]   [music]

