
[00:00:00.000 --> 00:00:10.000]   After buying Twitter for $44 billion, Musk`s time as CEO has been a whirlwind. Shares of
[00:00:10.000 --> 00:00:16.840]   Musk`s other major company, Tesla, have plummeted more than 30 percent since he took over Twitter.
[00:00:16.840 --> 00:00:20.440]   As is often the case, his next move is unclear.
[00:00:20.440 --> 00:00:24.240]   I go as far to say that he`s demonstrating some erratic behavior.
[00:00:24.240 --> 00:00:34.600]   Go fuck yourself. Is that clear? I hope it is. Hey, Bob, share in the audience.
[00:00:34.600 --> 00:00:43.120]   Elon Musk`s cooperation and/or relationships with other countries is worthy of being looked
[00:00:43.120 --> 00:00:44.120]   at.
[00:00:44.120 --> 00:00:48.840]   The Biden administration has just announced its second investigation into Elon Musk in
[00:00:48.840 --> 00:00:49.840]   less than a week.
[00:00:49.840 --> 00:00:55.360]   Both the Tesla and SpaceX, there`s a product roadmap that they`re on, and that whether
[00:00:55.360 --> 00:01:02.360]   Elon is in the building or not is not going to impact the plan that they have.
[00:01:02.360 --> 00:01:05.880]   People said he`d never get the rocket in space. He did that. People said the roads here would
[00:01:05.880 --> 00:01:10.360]   never get delivered. He did that. People said he`d never get 100 of them done. He`s got
[00:01:10.360 --> 00:01:11.360]   200 done.
[00:01:11.360 --> 00:01:15.560]   As an entrepreneur, you can`t listen to the noise. And you certainly can`t listen to losers
[00:01:15.560 --> 00:01:22.640]   who have never accomplished anything with their life, who are obsessing about you. Please.
[00:01:22.640 --> 00:01:27.880]   We`re out there among the stars and we`re a multi-planet species across many planets
[00:01:27.880 --> 00:01:36.920]   and many star systems.
[00:01:36.920 --> 00:01:40.840]   This is a great future, and that`s what we should strive for.
[00:01:40.840 --> 00:01:45.760]   To our customer support, at your service.
[00:01:45.760 --> 00:01:50.760]   Nearly every VC I speak with, every CEO is looking to Elon`s behavior and saying that`s
[00:01:50.760 --> 00:01:54.600]   a model for how you can challenge your team to achieve the impossible in an impossibly
[00:01:54.600 --> 00:01:55.600]   difficult environment.
[00:01:55.600 --> 00:02:08.600]   And you can see those grid fins on your left-hand screen rotating and turning to guide the booster.
[00:02:08.600 --> 00:02:19.840]   And you can see the water below and we have blast zone.
[00:02:19.840 --> 00:02:24.260]   He`s just a visionary like I`ve never seen.
[00:02:24.260 --> 00:02:26.480]   How on earth would you bet against him?
[00:02:26.480 --> 00:02:30.840]   Elon seems to be on track to be not only the world`s richest man, but the world`s first
[00:02:30.840 --> 00:02:31.840]   trillionaire.
[00:02:31.840 --> 00:02:37.160]   Elon basically has had over the last 10 or 15 years an incredible amount of challenges
[00:02:37.160 --> 00:02:38.640]   that he`s overcome.
[00:02:38.640 --> 00:02:43.000]   Probably had to deal with stuff that most of us would have broken under, and he just
[00:02:43.000 --> 00:02:44.000]   fought through it.
[00:02:44.000 --> 00:02:49.240]   And the guy just basically bended all the haters until he crushed their souls.
[00:02:49.240 --> 00:02:53.240]   And I just think that that`s incredible.
[00:02:53.240 --> 00:03:11.000]   My greatest entrepreneur of this generation, Elon Musk.
[00:03:11.000 --> 00:03:17.760]   I`m going to take this.
[00:03:17.760 --> 00:03:18.760]   All right.
[00:03:18.760 --> 00:03:22.240]   Thanks for taking the time.
[00:03:22.240 --> 00:03:26.560]   How are you doing, brother?
[00:03:26.560 --> 00:03:29.560]   You keeping busy?
[00:03:29.560 --> 00:03:30.560]   Yeah.
[00:03:30.560 --> 00:03:37.440]   I mean, it`s rarely a slow week.
[00:03:37.440 --> 00:03:41.640]   I mean, in the world as well.
[00:03:41.640 --> 00:03:42.640]   Yeah.
[00:03:42.640 --> 00:03:46.600]   I mean, any given week, it just seems like the thing is getting out of here.
[00:03:46.600 --> 00:03:47.600]   It`s definitely a simulation.
[00:03:47.600 --> 00:03:50.640]   We`ve agreed on this at this point.
[00:03:50.640 --> 00:03:56.360]   I mean, look, if we are in some alien Netflix series, I think the ratings are high.
[00:03:56.360 --> 00:03:57.360]   Yeah.
[00:03:57.360 --> 00:03:59.200]   The ratings are high.
[00:03:59.200 --> 00:04:04.040]   How are the freedom of speech wars going?
[00:04:04.040 --> 00:04:10.160]   This is a - you`ve been at war for two years now.
[00:04:10.160 --> 00:04:11.160]   Yes.
[00:04:11.160 --> 00:04:13.640]   The price of freedom of speech is not cheap, is it?
[00:04:13.640 --> 00:04:16.800]   I think it`s like 44 billion, something like that.
[00:04:16.800 --> 00:04:21.040]   Give or take a billion.
[00:04:21.040 --> 00:04:22.040]   Yeah.
[00:04:22.040 --> 00:04:25.680]   It`s pretty nutty.
[00:04:25.680 --> 00:04:32.440]   There is like this weird movement to quell free speech kind of around the world.
[00:04:32.440 --> 00:04:37.040]   And that`s something we should be very concerned about.
[00:04:37.040 --> 00:04:41.400]   You have to ask, like, why was the First Amendment like a high priority?
[00:04:41.400 --> 00:04:43.760]   It was like number one.
[00:04:43.760 --> 00:04:50.000]   Is because people came from countries where if you spoke freely, you would be imprisoned
[00:04:50.000 --> 00:04:51.800]   or killed.
[00:04:51.800 --> 00:04:54.920]   And they were like, well, we`d like to not have that here.
[00:04:54.920 --> 00:04:59.080]   Because that was terrible.
[00:04:59.080 --> 00:05:03.160]   And actually, you know, there`s a lot of places in the world right now, if you are critical
[00:05:03.160 --> 00:05:07.560]   of the government, you get imprisoned or killed.
[00:05:07.560 --> 00:05:08.560]   Right.
[00:05:08.560 --> 00:05:09.560]   Yeah.
[00:05:09.560 --> 00:05:10.560]   We`d like to not have that.
[00:05:10.560 --> 00:05:11.560]   Are you concerned --
[00:05:11.560 --> 00:05:12.560]   Can I add to that?
[00:05:12.560 --> 00:05:13.560]   Yeah.
[00:05:13.560 --> 00:05:19.560]   I mean, I suspect this is a receptive audience to that message.
[00:05:19.560 --> 00:05:24.560]   Yeah.
[00:05:24.560 --> 00:05:29.120]   I think we always thought that the West was the exception to that, that we knew there
[00:05:29.120 --> 00:05:32.280]   were authoritarian places around the world, but we thought that in the West, we`d have
[00:05:32.280 --> 00:05:33.280]   freedom of speech.
[00:05:33.280 --> 00:05:36.440]   And we`ve seen, like you said, it seems like a global movement.
[00:05:36.440 --> 00:05:41.660]   In Britain, you`ve got teenagers being put in prison for memes, opposing --
[00:05:41.660 --> 00:05:47.040]   It`s like you like to -- you like to Facebook post, throw them in the prison.
[00:05:47.040 --> 00:05:48.040]   Yeah.
[00:05:48.040 --> 00:05:55.040]   People have got an actual, you know, prison for, like, obscure comments on social media.
[00:05:55.040 --> 00:05:56.040]   Not even shitposting.
[00:05:56.040 --> 00:05:57.040]   Like, not even --
[00:05:57.040 --> 00:05:58.040]   Yeah.
[00:05:58.040 --> 00:05:59.040]   It`s crazy.
[00:05:59.040 --> 00:06:00.040]   Look, this is when --
[00:06:00.040 --> 00:06:01.040]   Pablo got thrown in prison recently.
[00:06:01.040 --> 00:06:02.040]   Right.
[00:06:02.040 --> 00:06:03.040]   You`ve got Pablo --
[00:06:03.040 --> 00:06:04.040]   I`m like, I was pretty shook about that.
[00:06:04.040 --> 00:06:06.800]   I mean, what is the massive crime that --
[00:06:06.800 --> 00:06:07.800]   Right.
[00:06:07.800 --> 00:06:12.800]   Pavel in France, and then, of course, we got Brazil with Judge Voldemort.
[00:06:12.800 --> 00:06:15.400]   That one seems like the one that impacts you the most.
[00:06:15.400 --> 00:06:19.380]   Can you -- what`s the latest on that?
[00:06:19.380 --> 00:06:30.080]   Well, we -- I guess we are trying to figure out is there some reasonable solution in Brazil.
[00:06:30.080 --> 00:06:35.400]   The -- you know, the concern -- I mean, I want to just make sure that this is framed
[00:06:35.400 --> 00:06:36.400]   correctly.
[00:06:36.400 --> 00:06:46.240]   And, you know, funny memes aside, the nature of the concern was that, at least at XCorp,
[00:06:46.240 --> 00:06:53.240]   we had the perception that we were being asked to do things that violated Brazilian law.
[00:06:53.240 --> 00:07:02.080]   So, obviously, we cannot, as an American company, impose American laws and values on other countries
[00:07:02.080 --> 00:07:06.120]   that -- you know, we wouldn`t get very far if we did that.
[00:07:06.120 --> 00:07:13.360]   But we do, you know, think that if a country`s laws are a particular way, and we are being
[00:07:13.360 --> 00:07:19.280]   asked to -- what we think we are being asked to break them, then -- and be silent about
[00:07:19.280 --> 00:07:21.960]   it, then, obviously, that is no good.
[00:07:21.960 --> 00:07:29.080]   So I just want to be clear, because sometimes it comes across as Elon is trying to just
[00:07:29.080 --> 00:07:36.280]   be a crazy whatever, billionaire, and demand outrageous things from other countries.
[00:07:36.280 --> 00:07:51.920]   And, you know, while that is true, in addition, there are other things that I think are -- I
[00:07:51.920 --> 00:07:58.400]   think are valid, which is, like, we obviously can`t -- you know, I think any given thing
[00:07:58.400 --> 00:08:04.400]   that we do at XCorp, we`ve got to be able to explain in the light of day, and not feel
[00:08:04.400 --> 00:08:11.760]   that it was dishonorable, or, you know, we did the wrong thing, you know?
[00:08:11.760 --> 00:08:15.560]   So we don`t -- that was the -- that`s the nature of the concern.
[00:08:15.560 --> 00:08:24.160]   So we actually are in sort of discussions with the, you know, judicial authorities in
[00:08:24.160 --> 00:08:32.320]   Brazil to try to, you know, run this to ground, like, what`s actually going on?
[00:08:32.320 --> 00:08:37.640]   Like, if we`re being asked to break the law, Brazilian law, then that obviously should
[00:08:37.640 --> 00:08:42.120]   not be -- should not sit well with the Brazilian judiciary.
[00:08:42.120 --> 00:08:46.140]   And if we`re not, and we`re mistaken, we`d like to understand how we`re mistaken.
[00:08:46.140 --> 00:08:48.320]   I think that`s a -- that`s a pretty reasonable position.
[00:08:48.320 --> 00:08:55.840]   I`m a bit concerned, as your friend, that you`re going to go to one of these countries,
[00:08:55.840 --> 00:08:58.360]   and I`m going to wake up one day, and you`re going to get arrested, and, like, I`m going
[00:08:58.360 --> 00:08:59.960]   to have to go bail you out or something.
[00:08:59.960 --> 00:09:02.360]   Like, this is -- feels very acute, like --
[00:09:02.360 --> 00:09:03.360]   Yes.
[00:09:03.360 --> 00:09:05.040]   I mean, it`s not a joke now.
[00:09:05.040 --> 00:09:08.640]   Like, they`re literally saying, like, you know, it`s not just Biden saying, like, we
[00:09:08.640 --> 00:09:12.800]   have to look into that guy, now it`s become quite literal, like, this -- I don`t know,
[00:09:12.800 --> 00:09:16.600]   who was the guy who just wrote the -- was it the Guardian piece about, like --
[00:09:16.600 --> 00:09:21.760]   Oh, yeah, yeah, there`ve been three articles, and I think in the past three weeks --
[00:09:21.760 --> 00:09:22.760]   Robert Reich.
[00:09:22.760 --> 00:09:23.760]   Yeah.
[00:09:23.760 --> 00:09:24.760]   But it wasn`t just him.
[00:09:24.760 --> 00:09:25.760]   Yeah.
[00:09:25.760 --> 00:09:26.760]   Three different articles.
[00:09:26.760 --> 00:09:27.760]   Three different articles.
[00:09:27.760 --> 00:09:28.760]   Yeah.
[00:09:28.760 --> 00:09:29.760]   That doesn`t -- that`s a trend.
[00:09:29.760 --> 00:09:35.520]   Calling for me to be imprisoned in the Guardian, you know, Guardian of what?
[00:09:35.520 --> 00:09:38.960]   What are they protecting, exactly?
[00:09:38.960 --> 00:09:41.840]   Guardian of -- I don`t know.
[00:09:41.840 --> 00:09:42.840]   Authoritarianism?
[00:09:42.840 --> 00:09:43.840]   Yeah.
[00:09:43.840 --> 00:09:44.840]   Guardian of -- yeah.
[00:09:44.840 --> 00:09:45.840]   Yeah.
[00:09:45.840 --> 00:09:46.840]   Censorship?
[00:09:46.840 --> 00:09:47.840]   Censorship.
[00:09:47.840 --> 00:09:53.000]   But the premise here is that you bought this thing, this online forum, this communication
[00:09:53.000 --> 00:09:58.720]   platform, and you`re allowing people to use it to express themselves, therefore, you have
[00:09:58.720 --> 00:09:59.720]   to be jailed.
[00:09:59.720 --> 00:10:02.080]   I don`t understand the logic here.
[00:10:02.080 --> 00:10:03.080]   Right.
[00:10:03.080 --> 00:10:05.560]   So, what do you think they`re actually afraid of at this point?
[00:10:05.560 --> 00:10:08.000]   What`s the motivation here?
[00:10:08.000 --> 00:10:13.440]   Well, I mean, I think -- if somebody`s afraid -- if somebody`s sort of trying to push a false
[00:10:13.440 --> 00:10:20.480]   premise on the world, then that premise can be undermined with public dialogue, then they
[00:10:20.480 --> 00:10:24.760]   will be opposed to public dialogue on that premise, because they wish that false premise
[00:10:24.760 --> 00:10:25.760]   to prevail.
[00:10:25.760 --> 00:10:26.760]   Right.
[00:10:26.760 --> 00:10:34.080]   So, that`s, I think, you know, the issue there is, if they don`t like the truth, you know,
[00:10:34.080 --> 00:10:35.640]   then we want to suppress it.
[00:10:35.640 --> 00:10:46.080]   So, now, you know, the sort of -- what we`re trying to do with XCorp is -- I distinguish
[00:10:46.080 --> 00:10:48.000]   that from my son, who`s also called X.
[00:10:48.000 --> 00:10:49.000]   Yes.
[00:10:49.000 --> 00:10:50.000]   Right.
[00:10:50.000 --> 00:10:53.640]   You have parental goals, and then you have goals for the company.
[00:10:53.640 --> 00:10:54.640]   Everything`s just called X, basically.
[00:10:54.640 --> 00:10:55.640]   Yes.
[00:10:55.640 --> 00:10:56.640]   Very difficult disambiguation.
[00:10:56.640 --> 00:10:57.640]   The car, the son.
[00:10:57.640 --> 00:10:58.640]   Yes.
[00:10:58.640 --> 00:10:59.640]   It`s everything.
[00:10:59.640 --> 00:11:09.960]   So, what we`re trying to do is simply adhere to the, you know, the laws in a country.
[00:11:09.960 --> 00:11:15.760]   So, if something is illegal in the United States, or if it`s illegal in, you know, Europe
[00:11:15.760 --> 00:11:22.520]   or Brazil or wherever it might be, then we will take it down, and we`ll suspend the account,
[00:11:22.520 --> 00:11:30.280]   because we`re not, you know, there to make the laws, we -- but if speech is not illegal,
[00:11:30.280 --> 00:11:31.280]   then what are we doing?
[00:11:31.280 --> 00:11:32.280]   Okay.
[00:11:32.280 --> 00:11:39.920]   Now, we`re injecting ourselves in as a censor, and where does it stop, and who decides?
[00:11:39.920 --> 00:11:43.280]   So, where does that path lead?
[00:11:43.280 --> 00:11:45.800]   I think it leads to a bad place.
[00:11:45.800 --> 00:11:51.760]   So, if the people in a country want the laws to be different, they should make the laws
[00:11:51.760 --> 00:11:52.760]   different.
[00:11:52.760 --> 00:11:56.960]   But otherwise, we`re going to obey the law in each jurisdiction.
[00:11:56.960 --> 00:11:57.960]   Right.
[00:11:57.960 --> 00:11:58.960]   And some of these European --
[00:11:58.960 --> 00:11:59.960]   That`s it.
[00:11:59.960 --> 00:12:00.960]   It`s not more complicated than that.
[00:12:00.960 --> 00:12:01.960]   We`re not trying to flout the law.
[00:12:01.960 --> 00:12:02.960]   I`m going to be clear about that.
[00:12:02.960 --> 00:12:06.560]   We`re trying to adhere to the law, and if the laws change, we will change.
[00:12:06.560 --> 00:12:08.320]   And if the laws don`t change, we won`t.
[00:12:08.320 --> 00:12:11.280]   We`re just literally trying to adhere to the law.
[00:12:11.280 --> 00:12:12.280]   It`s pretty straightforward.
[00:12:12.280 --> 00:12:13.280]   There`s some European --
[00:12:13.280 --> 00:12:14.280]   Yes.
[00:12:14.280 --> 00:12:15.280]   It`s very straightforward.
[00:12:15.280 --> 00:12:18.720]   And if somebody thinks we`re not adhering to the law, well, they can file a lawsuit.
[00:12:18.720 --> 00:12:19.720]   Bingo.
[00:12:19.720 --> 00:12:20.720]   Also very straightforward.
[00:12:20.720 --> 00:12:21.720]   Yes.
[00:12:21.720 --> 00:12:24.440]   But what about countries that don`t want people to promote Nazi propaganda?
[00:12:24.440 --> 00:12:25.440]   Yes.
[00:12:25.440 --> 00:12:26.640]   They have some sensitivity to it.
[00:12:26.640 --> 00:12:27.640]   Well, it is illegal.
[00:12:27.640 --> 00:12:28.640]   It is illegal in those countries.
[00:12:28.640 --> 00:12:29.640]   Yes, it is illegal.
[00:12:29.640 --> 00:12:32.280]   And in those countries, if somebody puts that up, you take it down.
[00:12:32.280 --> 00:12:33.280]   Yes.
[00:12:33.280 --> 00:12:35.760]   But they typically file something and say, "Take this down."
[00:12:35.760 --> 00:12:36.760]   Yes.
[00:12:36.760 --> 00:12:39.360]   No, in some cases, it is just obviously illegal.
[00:12:39.360 --> 00:12:45.160]   Like, you don`t need to file a lawsuit for, you know, if something is just, you know,
[00:12:45.160 --> 00:12:46.160]   unequivocally illegal.
[00:12:46.160 --> 00:12:47.160]   We can literally read the law.
[00:12:47.160 --> 00:12:48.160]   This violates the law.
[00:12:48.160 --> 00:12:49.160]   You know, anyone --
[00:12:49.160 --> 00:12:50.160]   Why not?
[00:12:50.160 --> 00:12:51.160]   Anyone can see that.
[00:12:51.160 --> 00:12:55.600]   You know, you don`t need -- like, if somebody is stealing, you don`t need -- let me check
[00:12:55.600 --> 00:12:56.600]   the law on that.
[00:12:56.600 --> 00:12:57.600]   Yes.
[00:12:57.600 --> 00:12:58.600]   Okay.
[00:12:58.600 --> 00:12:59.600]   Oh, no.
[00:12:59.600 --> 00:13:00.600]   They`re stealing stuff.
[00:13:00.600 --> 00:13:01.600]   Let`s talk about it.
[00:13:01.600 --> 00:13:03.160]   So, we had J.D. Vance here this morning.
[00:13:03.160 --> 00:13:05.440]   He did a great job.
[00:13:05.440 --> 00:13:09.880]   And you know, one of the things is there`s this image on X of, like, basically, like,
[00:13:09.880 --> 00:13:15.960]   you, Bobby, Trump, and J.D. are like the Avengers, I guess.
[00:13:15.960 --> 00:13:19.840]   And then there`s another meme where you`re in front of a desk where it says, "D-O-G-E."
[00:13:19.840 --> 00:13:20.840]   Yes.
[00:13:20.840 --> 00:13:22.560]   The Department of Governmental Efficiency.
[00:13:22.560 --> 00:13:23.560]   Yes.
[00:13:23.560 --> 00:13:24.560]   Yes.
[00:13:24.560 --> 00:13:25.560]   I posted that one.
[00:13:25.560 --> 00:13:26.560]   Tell us about it.
[00:13:26.560 --> 00:13:31.000]   I made it using Grok, the Grok image generator.
[00:13:31.000 --> 00:13:32.000]   And I posted it.
[00:13:32.000 --> 00:13:33.000]   Tell us about --
[00:13:33.000 --> 00:13:34.000]   And put it to my profile.
[00:13:34.000 --> 00:13:35.000]   Seek for efficiency.
[00:13:35.000 --> 00:13:40.000]   How do you do it?
[00:13:40.000 --> 00:13:46.600]   Well, I mean, I --
[00:13:46.600 --> 00:13:52.120]   I think with great difficulty, but, you know, look, it`s been a long time since there was
[00:13:52.120 --> 00:13:59.120]   a serious effort to reduce the size of government and to remove absurd regulations.
[00:13:59.120 --> 00:14:00.120]   Yeah.
[00:14:00.120 --> 00:14:04.240]   And, you know, last time there was a really concerted effort on that front was Reagan
[00:14:04.240 --> 00:14:05.240]   in the early `80s.
[00:14:05.240 --> 00:14:14.200]   We`re 40 years away from a serious effort to remove, you know, regulations that don`t
[00:14:14.200 --> 00:14:18.120]   serve the greater good and reduce the size of government.
[00:14:18.120 --> 00:14:23.480]   And I think it`s just -- if we don`t do that, then what`s happening is that we get regulations
[00:14:23.480 --> 00:14:28.920]   and laws accumulating every year until eventually everything`s illegal.
[00:14:28.920 --> 00:14:32.800]   And that`s why we can`t get major infrastructure projects done in the United States.
[00:14:32.800 --> 00:14:37.160]   Like, if you look at the absurdity of the California high-speed rail, I think they spent
[00:14:37.160 --> 00:14:42.000]   $7 billion and have a 1,600-foot segment that doesn`t actually have rail in it.
[00:14:42.000 --> 00:14:44.240]   I mean, your tax dollars at work, I mean --
[00:14:44.240 --> 00:14:45.240]   Yeah.
[00:14:45.240 --> 00:14:46.240]   What are we doing?
[00:14:46.240 --> 00:14:50.640]   That`s the expense of 1,600 feet of concrete, you know.
[00:14:50.640 --> 00:14:57.480]   And I mean, I think it`s like, you know, I realize sometimes I`m perhaps a little optimistic
[00:14:57.480 --> 00:15:06.360]   with schedules, but, you know, I mean, I wouldn`t be doing the things I`m doing if I was, you
[00:15:06.360 --> 00:15:15.360]   know, not an optimist, so -- but at the current trend, you know, California high-speed rail
[00:15:15.360 --> 00:15:17.440]   might finish sometime next century.
[00:15:17.440 --> 00:15:18.440]   Maybe.
[00:15:18.440 --> 00:15:19.440]   Probably not.
[00:15:19.440 --> 00:15:20.440]   We`re just --
[00:15:20.440 --> 00:15:22.000]   We`ll have teleportation by that time, so --
[00:15:22.000 --> 00:15:23.000]   Yeah, exactly.
[00:15:23.000 --> 00:15:28.320]   We just have AI do everything at that point.
[00:15:28.320 --> 00:15:34.360]   So, I think you really think of, you know, the United States and many countries, it`s
[00:15:34.360 --> 00:15:40.360]   arguably worse than the EU, as being like Gulliver tied down by a million little strings.
[00:15:40.360 --> 00:15:46.920]   And like any one given regulation is not that bad, but you`ve got a million of them, or
[00:15:46.920 --> 00:15:48.760]   millions actually.
[00:15:48.760 --> 00:15:55.680]   And then eventually you just can`t get anything done, and this is a massive tax on the consumer,
[00:15:55.680 --> 00:15:56.680]   on the people.
[00:15:56.680 --> 00:16:02.360]   It`s just they don`t realize that there`s this massive tax in the form of irrational
[00:16:02.360 --> 00:16:03.360]   regulations.
[00:16:03.360 --> 00:16:10.880]   I`m going to give you a recent example that, you know, is just insane, is that like SpaceX
[00:16:10.880 --> 00:16:19.200]   was fined by the EPA $140,000 for they claimed dumping potable water on the ground, drinking
[00:16:19.200 --> 00:16:20.200]   water.
[00:16:20.200 --> 00:16:26.400]   So, and we`re like, this is at Starbase, and we`re like, it`s -- we`re in a tropical thunderstorm
[00:16:26.400 --> 00:16:30.640]   region, that stuff comes from the sky all the time.
[00:16:30.640 --> 00:16:36.920]   And there was no actual harm done, you know, it was just water to cool the launch pad during
[00:16:36.920 --> 00:16:40.440]   liftoff, and there`s zero harm done.
[00:16:40.440 --> 00:16:42.240]   Like, and they`re like, they agree, yes, there`s zero harm done.
[00:16:42.240 --> 00:16:44.800]   And we`re like, okay, so there`s no harm done.
[00:16:44.800 --> 00:16:49.320]   And you want us to pay $140,000 fine, it`s like, yes, because you didn`t have a permit.
[00:16:49.320 --> 00:16:50.320]   Okay.
[00:16:50.320 --> 00:16:56.600]   We didn`t know there was a permit needed for zero harm, fresh water being on the ground
[00:16:56.600 --> 00:17:00.400]   in a place that where fresh water falls from the sky all the time.
[00:17:00.400 --> 00:17:01.400]   Got it.
[00:17:01.400 --> 00:17:02.400]   Next to the ocean.
[00:17:02.400 --> 00:17:03.400]   Next to the ocean.
[00:17:03.400 --> 00:17:05.520]   Because there`s a little bit of water there, too.
[00:17:05.520 --> 00:17:06.520]   Yeah.
[00:17:06.520 --> 00:17:08.520]   I mean, sometimes it rains so much, the roads are flooded.
[00:17:08.520 --> 00:17:12.360]   So we`re like, you know, how does this make any sense?
[00:17:12.360 --> 00:17:13.360]   Yeah.
[00:17:13.360 --> 00:17:17.880]   And then, like, they were like, well, we`re not going to process any more of your -- any
[00:17:17.880 --> 00:17:21.880]   more of your applications for launch, for Starship launch, unless you pay this $140,000.
[00:17:21.880 --> 00:17:27.320]   They just ransomed us, and we`re like, okay, so we paid $140,000, but it was -- it`s like,
[00:17:27.320 --> 00:17:28.320]   this is no good.
[00:17:28.320 --> 00:17:30.880]   At this rate, we`re never going to get to Mars.
[00:17:30.880 --> 00:17:37.520]   I mean, that`s the -- that`s the confounding part here is we`re acting against our own
[00:17:37.520 --> 00:17:38.520]   self-interest.
[00:17:38.520 --> 00:17:44.280]   You know, when you look at -- we do have to make -- putting aside fresh water, but, hey,
[00:17:44.280 --> 00:17:47.360]   you know, there -- the rocket makes a lot of noise.
[00:17:47.360 --> 00:17:50.600]   So I`m certain there`s some complaints about noise once in a while.
[00:17:50.600 --> 00:17:53.680]   But sometimes you want to have a party, or you want to make progress, and there`s a little
[00:17:53.680 --> 00:17:58.520]   bit of noise, therefore, you know, we trade off a little bit of noise for massive progress
[00:17:58.520 --> 00:17:59.840]   or even fun.
[00:17:59.840 --> 00:18:03.400]   So, like, when did we stop being able to make those tradeoffs?
[00:18:03.400 --> 00:18:10.240]   But talk about the difference between California and Texas, where you and I now reside.
[00:18:10.240 --> 00:18:15.600]   Texas, you were able to build the Gigafactory, I remember when you got the plot of land,
[00:18:15.600 --> 00:18:21.280]   and then our -- it seemed like it was less than two years when you had the party to open
[00:18:21.280 --> 00:18:22.280]   it.
[00:18:22.280 --> 00:18:23.280]   Where we -- yeah.
[00:18:23.280 --> 00:18:28.200]   From start of construction to completion was 14 months.
[00:18:28.200 --> 00:18:29.200]   Fourteen.
[00:18:29.200 --> 00:18:30.200]   Fourteen months.
[00:18:30.200 --> 00:18:32.000]   Is there anywhere on the planet that would go faster?
[00:18:32.000 --> 00:18:34.640]   Is, like, China faster than that?
[00:18:34.640 --> 00:18:35.640]   China was 11 months.
[00:18:35.640 --> 00:18:36.640]   Got it.
[00:18:36.640 --> 00:18:42.120]   So, Texas, China, 11 and 14 months, California, how many months?
[00:18:42.120 --> 00:18:46.960]   And just to give you a sense of size, the -- Tesla Gigafactory in China is three times
[00:18:46.960 --> 00:18:48.780]   the size of the Pentagon.
[00:18:48.780 --> 00:18:50.120]   Which was the biggest building in America.
[00:18:50.120 --> 00:18:52.480]   No, there were bigger buildings, but the Pentagon was a pretty big one.
[00:18:52.480 --> 00:18:53.480]   Yeah.
[00:18:53.480 --> 00:18:54.480]   Or it was the biggest at the time.
[00:18:54.480 --> 00:18:56.360]   In units of Pentagon, it's, like, three.
[00:18:56.360 --> 00:18:57.360]   Okay.
[00:18:57.360 --> 00:18:59.400]   Three Pentagons and counting.
[00:18:59.400 --> 00:19:00.400]   Yeah.
[00:19:00.400 --> 00:19:01.400]   Got it.
[00:19:01.400 --> 00:19:04.480]   In 14 months.
[00:19:04.480 --> 00:19:09.880]   Just the regulatory approvals in California would have taken two years.
[00:19:09.880 --> 00:19:12.040]   So that's the issue.
[00:19:12.040 --> 00:19:14.280]   Where do you think the regulation helps?
[00:19:14.280 --> 00:19:17.000]   Like for the people that will say, "We need some checks and balances.
[00:19:17.000 --> 00:19:20.360]   We can't have some -- because for every good actor like you, there'll be a bad actor."
[00:19:20.360 --> 00:19:22.280]   So where is that line, then?
[00:19:22.280 --> 00:19:23.280]   Yeah.
[00:19:23.280 --> 00:19:32.080]   I mean, I haven't -- sort of, you know, in sort of doing a sensible deregulation and
[00:19:32.080 --> 00:19:36.520]   reduction in the size of government is just, like, be very public about it and say, like,
[00:19:36.520 --> 00:19:40.440]   which of these rules do you -- if the public is really excited about a rule and wants to
[00:19:40.440 --> 00:19:42.720]   keep it, we'll just keep it.
[00:19:42.720 --> 00:19:47.080]   And here, the thing about the rule is, if, like, if the rule is, you know, turns out
[00:19:47.080 --> 00:19:49.960]   to be a bad, we'll just put it right back.
[00:19:49.960 --> 00:19:50.960]   Okay.
[00:19:50.960 --> 00:19:51.960]   And then, you know, problem solved.
[00:19:51.960 --> 00:19:55.160]   It's, like, it's easy to add rules, but we don't actually have a process for getting
[00:19:55.160 --> 00:19:56.440]   rid of them.
[00:19:56.440 --> 00:19:57.440]   That's the issue.
[00:19:57.440 --> 00:19:59.560]   There's no garbage collection for rules.
[00:19:59.560 --> 00:20:08.640]   When we were watching you work, David and I and Antonio, in that first month at Twitter,
[00:20:08.640 --> 00:20:13.840]   which was all hands on deck, and you were doing zero-based budgeting, you really quickly
[00:20:13.840 --> 00:20:15.520]   got the costs under control.
[00:20:15.520 --> 00:20:21.640]   And then, miraculously, everybody said this site will go down, and you added 50 more features.
[00:20:21.640 --> 00:20:22.640]   So maybe explain.
[00:20:22.640 --> 00:20:23.640]   Yeah.
[00:20:23.640 --> 00:20:24.640]   Because this is the first thing.
[00:20:24.640 --> 00:20:29.680]   Yeah, there were, like, so many articles, like, that this is -- Twitter is dead forever.
[00:20:29.680 --> 00:20:32.960]   There's no way it could possibly even continue at all.
[00:20:32.960 --> 00:20:33.960]   Yeah.
[00:20:33.960 --> 00:20:34.960]   It was almost like the press was rooting for you to fail.
[00:20:34.960 --> 00:20:35.960]   It's like, all right, team.
[00:20:35.960 --> 00:20:36.960]   Let's write the obituary.
[00:20:36.960 --> 00:20:37.960]   Like, here's the obituary.
[00:20:37.960 --> 00:20:39.440]   And they were all saying their goodbyes on Twitter.
[00:20:39.440 --> 00:20:40.440]   Remember that?
[00:20:40.440 --> 00:20:41.440]   Yeah, yeah.
[00:20:41.440 --> 00:20:42.440]   Yeah.
[00:20:42.440 --> 00:20:43.440]   They were all leaving and saying their goodbyes, because the site was going to melt down.
[00:20:43.440 --> 00:20:44.440]   Yes.
[00:20:44.440 --> 00:20:45.440]   Yes.
[00:20:45.440 --> 00:20:46.440]   Totally failing.
[00:20:46.440 --> 00:20:47.440]   All the journalists left.
[00:20:47.440 --> 00:20:48.440]   Yeah.
[00:20:48.440 --> 00:20:52.760]   Which is, if you ever want to, like, hang out with a bunch of hall monitors, oh my god,
[00:20:52.760 --> 00:20:53.760]   Threads is amazing.
[00:20:53.760 --> 00:20:57.880]   Every time I go over there and post, they're, like, they're really triggered.
[00:20:57.880 --> 00:20:58.880]   Yeah.
[00:20:58.880 --> 00:21:03.960]   I mean, if you like being condemned repeatedly, then, you know, for reasons that make no sense,
[00:21:03.960 --> 00:21:05.320]   then Threads is the way to go.
[00:21:05.320 --> 00:21:08.680]   It's really, it's the most miserable place on earth.
[00:21:08.680 --> 00:21:12.000]   If Disney's the happiest, this is the anti-Disney.
[00:21:12.000 --> 00:21:16.720]   But if we were to go into government, you went into the Department of Education or pick
[00:21:16.720 --> 00:21:17.720]   the department.
[00:21:17.720 --> 00:21:19.720]   You've worked with a lot of them, actually.
[00:21:19.720 --> 00:21:21.280]   You can't go in there and zero-base budget.
[00:21:21.280 --> 00:21:22.720]   OK, we get it.
[00:21:22.720 --> 00:21:29.880]   But if you could just pair 2%, 3%, 4%, 5% of those organizations, what kind of impact
[00:21:29.880 --> 00:21:30.880]   would that have?
[00:21:30.880 --> 00:21:31.880]   Yeah.
[00:21:31.880 --> 00:21:34.840]   I mean, I think we'd need to do more than that.
[00:21:34.840 --> 00:21:35.840]   I think.
[00:21:35.840 --> 00:21:36.840]   Ideally.
[00:21:36.840 --> 00:21:37.840]   Yeah.
[00:21:37.840 --> 00:21:38.840]   Compounding every year.
[00:21:38.840 --> 00:21:39.840]   2%, 3% a year.
[00:21:39.840 --> 00:21:42.640]   I mean, it would be better than what's happening now.
[00:21:42.640 --> 00:21:43.640]   Yeah.
[00:21:43.640 --> 00:21:55.160]   Look, I think we've, you know, if Trump wins, and I suspect there are people with mixed feelings
[00:21:55.160 --> 00:22:03.360]   about whether that should happen, but we do have an opportunity to do kind of a once-in-a-lifetime
[00:22:03.360 --> 00:22:06.640]   deregulation and reduction in the size of government.
[00:22:06.640 --> 00:22:12.360]   Because the other thing, besides the regulations, America is also going bankrupt extremely quickly.
[00:22:12.360 --> 00:22:16.560]   And nobody seems to, everyone seems to be sort of whistling past the graveyard on this
[00:22:16.560 --> 00:22:17.560]   one.
[00:22:17.560 --> 00:22:21.040]   They're all grabbing the silverware.
[00:22:21.040 --> 00:22:24.720]   Everyone's stuffing their pockets in the silverware before the Titanic sinks.
[00:22:24.720 --> 00:22:30.120]   Well, you know, the Defense Department budget is a very big budget, OK?
[00:22:30.120 --> 00:22:36.280]   It's a trillion dollars a year, DOD, Intel, it's a trillion dollars.
[00:22:36.280 --> 00:22:42.680]   And interest payments on the national debt just exceeded the Defense Department budget.
[00:22:42.680 --> 00:22:48.640]   They're over a trillion dollars a year, just in interest, and rising.
[00:22:48.640 --> 00:22:55.120]   We're adding a trillion dollars to our debt, which our kids and grandkids are going to
[00:22:55.120 --> 00:23:01.160]   have to pay somehow, you know, every three months.
[00:23:01.160 --> 00:23:04.600]   And then soon it's going to be every two months, and then every month.
[00:23:04.600 --> 00:23:08.320]   And then the only thing we'll be able to pay is interest.
[00:23:08.320 --> 00:23:14.760]   And it's just, you know, it's just like a person at scale that has racked up too much
[00:23:14.760 --> 00:23:23.980]   credit card debt, and this does not have a good ending.
[00:23:23.980 --> 00:23:26.000]   And so we have to reduce the spending.
[00:23:26.000 --> 00:23:30.760]   Let me ask one question, because I've brought this up a lot, and the counterargument I hear,
[00:23:30.760 --> 00:23:35.400]   which I disagree with, but the counterargument I hear from a lot of politicians is if we
[00:23:35.400 --> 00:23:39.840]   reduce spending, because right now, if you add up federal, state, and local government
[00:23:39.840 --> 00:23:44.180]   spending, it's between 40 and 50 percent of GDP.
[00:23:44.180 --> 00:23:48.960]   So nearly half of our economy is supported by government spending, and nearly half of
[00:23:48.960 --> 00:23:54.520]   people in the United States are dependent directly or indirectly on government checks,
[00:23:54.520 --> 00:23:58.200]   and either through contractors that the government pays, or they're employed by a government
[00:23:58.200 --> 00:23:59.200]   entity.
[00:23:59.200 --> 00:24:06.240]   So if you go in and you take too hard an ax too fast, you will have significant contraction,
[00:24:06.240 --> 00:24:08.160]   job loss, and recession.
[00:24:08.160 --> 00:24:10.300]   What's the balancing act, Elon?
[00:24:10.300 --> 00:24:15.600]   Just thinking realistically, because I'm 100 percent on board with you, the next set of
[00:24:15.600 --> 00:24:25.960]   steps, however, assume Trump wins and you become the chief D-O-G-E, D-O-G-E, D-O, like
[00:24:25.960 --> 00:24:26.960]   double G-E.
[00:24:26.960 --> 00:24:34.680]   Yeah, and I think the challenge is how quickly can we go in, how quickly can things change,
[00:24:34.680 --> 00:24:35.680]   and without ...
[00:24:35.680 --> 00:24:41.160]   I'll let that on my business card.
[00:24:41.160 --> 00:24:42.160]   Yeah.
[00:24:42.160 --> 00:24:44.360]   Without all the contraction and job loss.
[00:24:44.360 --> 00:24:45.360]   You don't have Scott.
[00:24:45.360 --> 00:24:46.360]   Yeah.
[00:24:46.360 --> 00:24:49.240]   So I guess, how do you really address it when so much of the economy and so many people's
[00:24:49.240 --> 00:24:52.240]   jobs and livelihoods are dependent on government spending?
[00:24:52.240 --> 00:24:59.760]   Well, I mean, I do think it's sort of, you know, it's a false dichotomy.
[00:24:59.760 --> 00:25:03.320]   It's not like no government spending is going to happen.
[00:25:03.320 --> 00:25:06.820]   You really have to say, like, is it the right level?
[00:25:06.820 --> 00:25:13.220]   And just remember that, you know, any given person, if they are doing things in a less
[00:25:13.220 --> 00:25:17.760]   efficient organization versus a more efficient organization, their contribution to the economy,
[00:25:17.760 --> 00:25:20.600]   their net output of goods and services will reduce.
[00:25:20.600 --> 00:25:25.920]   I mean, you've got a couple of clear examples between East Germany and West Germany, North
[00:25:25.920 --> 00:25:26.920]   Korea and South Korea.
[00:25:26.920 --> 00:25:29.680]   I mean, North Korea, they're starving.
[00:25:29.680 --> 00:25:31.440]   South Korea, it's, like, amazing.
[00:25:31.440 --> 00:25:32.440]   It's the future.
[00:25:32.440 --> 00:25:34.720]   It's the compounding effect of productivity gains.
[00:25:34.720 --> 00:25:35.720]   Yeah.
[00:25:35.720 --> 00:25:36.800]   Yeah, it's night and day.
[00:25:36.800 --> 00:25:42.160]   And so in North Korea, you've got 100% government, and in South Korea, you've got probably, I
[00:25:42.160 --> 00:25:43.360]   don't know, 40% government.
[00:25:43.360 --> 00:25:44.360]   It's not zero.
[00:25:44.360 --> 00:25:45.360]   Yeah.
[00:25:45.360 --> 00:25:48.440]   And yet you've got a standard of living that is probably 10 times higher in South Korea.
[00:25:48.440 --> 00:25:49.440]   At least.
[00:25:49.440 --> 00:25:52.020]   At least, exactly.
[00:25:52.020 --> 00:25:57.440]   And then East and West Germany, in West Germany, just thinking in terms of cars, I mean, you
[00:25:57.440 --> 00:26:02.360]   had BMW, Porsche, Audi, Mercedes.
[00:26:02.360 --> 00:26:09.200]   And East Germany, which is a random line on a map, the only car you could get was a Trabant,
[00:26:09.200 --> 00:26:13.120]   which is basically a lawnmower with a shell in it.
[00:26:13.120 --> 00:26:14.680]   And it was extremely unsafe.
[00:26:14.680 --> 00:26:18.080]   There was a 20-year wait.
[00:26:18.080 --> 00:26:24.800]   So you put your kid on the list as soon as they're conceived.
[00:26:24.800 --> 00:26:33.320]   And even then, only, I think, a quarter of people maybe got this lousy car.
[00:26:33.320 --> 00:26:38.560]   So that's just an interesting example of basically the same people, different operating system.
[00:26:38.560 --> 00:26:45.960]   And it's not like West Germany was some capitalist heaven.
[00:26:45.960 --> 00:26:49.920]   It was quite socialist, actually.
[00:26:49.920 --> 00:26:56.000]   So when you look, probably it was half government in West Germany and 100% government in East
[00:26:56.000 --> 00:26:57.000]   Germany.
[00:26:57.000 --> 00:27:04.560]   And again, sort of a 5 to 10x standard of living difference, and even qualitatively
[00:27:04.560 --> 00:27:06.000]   vastly better.
[00:27:06.000 --> 00:27:10.440]   And it's obviously, so many people have these, amazingly, in this modern era, this debate
[00:27:10.440 --> 00:27:11.680]   as to which system is better.
[00:27:11.680 --> 00:27:14.120]   Well, I'll tell you which system is better.
[00:27:14.120 --> 00:27:18.200]   The one that doesn't need to build the wall to keep people in, OK?
[00:27:18.200 --> 00:27:19.200]   That's how you can tell.
[00:27:19.200 --> 00:27:20.200]   OK?
[00:27:20.200 --> 00:27:21.200]   Yeah.
[00:27:21.200 --> 00:27:22.200]   It's a dead giveaway.
[00:27:22.200 --> 00:27:23.200]   Spoiler alert.
[00:27:23.200 --> 00:27:24.200]   Dead giveaway.
[00:27:24.200 --> 00:27:28.400]   Are they climbing the wall to get out or come in?
[00:27:28.400 --> 00:27:31.900]   You have to build a barrier to keep people in.
[00:27:31.900 --> 00:27:35.200]   That is the bad system.
[00:27:35.200 --> 00:27:37.800]   It wasn't West Berlin that built the wall, OK?
[00:27:37.800 --> 00:27:41.520]   They were like, you know, anyone who wants to flee West Berlin, go ahead.
[00:27:41.520 --> 00:27:44.780]   Speaking of walls.
[00:27:44.780 --> 00:27:49.880]   And if you look at sort of the flux of boats from Cuba, there's a large number of boats
[00:27:49.880 --> 00:27:56.200]   from Cuba, and there's a bunch of free boats that anyone can take to go back to Cuba.
[00:27:56.200 --> 00:27:57.200]   They're empty on the way back.
[00:27:57.200 --> 00:27:58.200]   Plenty of seats.
[00:27:58.200 --> 00:27:59.680]   There's like, hey, wow, an abandoned boat.
[00:27:59.680 --> 00:28:03.720]   I could use this boat to go to Cuba, where they have communism.
[00:28:03.720 --> 00:28:04.720]   Awesome.
[00:28:04.720 --> 00:28:09.240]   And yet nobody picks up those boats and does it.
[00:28:09.240 --> 00:28:10.240]   Amazing.
[00:28:10.240 --> 00:28:12.520]   You've given this a lot of thought.
[00:28:12.520 --> 00:28:13.520]   Yeah.
[00:28:13.520 --> 00:28:16.880]   Wait, so your point is jobs will be created if we cut government spending in half.
[00:28:16.880 --> 00:28:20.760]   Jobs will be created fast enough to make up for, right, just the countries.
[00:28:20.760 --> 00:28:21.760]   Yes.
[00:28:21.760 --> 00:28:27.160]   Obviously, you know, I'm not suggesting that people, you know, have like immediately tossed
[00:28:27.160 --> 00:28:31.000]   out with no severance and, you know, now can't pay their mortgage.
[00:28:31.000 --> 00:28:36.440]   There needs to be some reasonable off-ramp where, yeah, so a reasonable off-ramp where,
[00:28:36.440 --> 00:28:41.040]   you know, they're still, you know, earning, they're still receiving money, but have like,
[00:28:41.040 --> 00:28:46.000]   I don't know, a year or two to find jobs in the private sector, which they will find.
[00:28:46.000 --> 00:28:48.200]   And then they will be in a different operating system.
[00:28:48.200 --> 00:28:50.440]   Again, you can see the difference.
[00:28:50.440 --> 00:28:52.440]   East Germany was incorporated into West Germany.
[00:28:52.440 --> 00:28:55.760]   Living standards in East Germany rose dramatically.
[00:28:55.760 --> 00:29:01.840]   Well, in four years, if you could shrink the size of the government with Trump, what would
[00:29:01.840 --> 00:29:04.600]   be a good target, just in terms of like ballpark?
[00:29:04.600 --> 00:29:08.040]   I mean, are you trying to get me assassinated before this even happens?
[00:29:08.040 --> 00:29:09.040]   No.
[00:29:09.040 --> 00:29:10.040]   No.
[00:29:10.040 --> 00:29:11.040]   Pick a low number.
[00:29:11.040 --> 00:29:12.760]   I mean, you know, there's that old phrase, "Go postal."
[00:29:12.760 --> 00:29:13.760]   I mean, it's like they might.
[00:29:13.760 --> 00:29:14.760]   Yeah.
[00:29:14.760 --> 00:29:15.760]   I mean.
[00:29:15.760 --> 00:29:16.760]   So we'll keep the post office.
[00:29:16.760 --> 00:29:19.640]   I mean, I'm going to need a lot of security details, guys.
[00:29:19.640 --> 00:29:20.640]   Yes.
[00:29:20.640 --> 00:29:28.720]   I mean, the sheer number of disgruntled workers, former government employees, is quite a scary
[00:29:28.720 --> 00:29:29.720]   number.
[00:29:29.720 --> 00:29:30.720]   I mean, I might not make it.
[00:29:30.720 --> 00:29:33.640]   I was saying a low digits every year for four years would be palatable.
[00:29:33.640 --> 00:29:34.640]   Yeah.
[00:29:34.640 --> 00:29:35.640]   And I like your idea of an offer.
[00:29:35.640 --> 00:29:40.920]   Yeah, but the thing is that if it's not done, like if you have a once in a lifetime, once
[00:29:40.920 --> 00:29:46.800]   in a generation opportunity, and you don't take serious action, and then you have four
[00:29:46.800 --> 00:29:51.120]   years to get it done, and if it doesn't get done, then.
[00:29:51.120 --> 00:29:52.320]   How serious is Trump about this?
[00:29:52.320 --> 00:29:53.840]   Like, you've talked to him about it.
[00:29:53.840 --> 00:29:54.840]   Yeah.
[00:29:54.840 --> 00:29:55.840]   Yeah.
[00:29:55.840 --> 00:29:56.840]   He's serious.
[00:29:56.840 --> 00:29:57.840]   He is very serious about it.
[00:29:57.840 --> 00:29:58.840]   Got it.
[00:29:58.840 --> 00:30:02.680]   No, I think actually the reality is that if we get rid of nonsense regulations and shift
[00:30:02.680 --> 00:30:10.080]   people from the government sector to the private sector, we will have immense prosperity, and
[00:30:10.080 --> 00:30:12.480]   I think we will have a golden age in this country.
[00:30:12.480 --> 00:30:13.480]   And it'll be fantastic.
[00:30:13.480 --> 00:30:23.580]   You have a bunch of critical milestones coming up.
[00:30:23.580 --> 00:30:24.580]   Yeah.
[00:30:24.580 --> 00:30:31.840]   In fact, there's a very exciting launch that is maybe happening tonight, so if the weather
[00:30:31.840 --> 00:30:38.400]   is holding up, then I'm going to leave here, head to Cape Canaveral for the Polaris Dawn
[00:30:38.400 --> 00:30:42.640]   mission, which is a private mission, so funded by Jared Isaacman.
[00:30:42.640 --> 00:30:52.200]   And he's an awesome guy, and this will be the first commercial spacewalk, and it'll
[00:30:52.200 --> 00:30:58.600]   be at the highest altitude since Apollo, so it's the furthest from Earth that anyone's
[00:30:58.600 --> 00:30:59.600]   gone.
[00:30:59.600 --> 00:31:00.600]   Yeah.
[00:31:00.600 --> 00:31:01.600]   And what comes after that?
[00:31:01.600 --> 00:31:02.600]   Let's assume that's successful.
[00:31:02.600 --> 00:31:03.600]   I sure hope so, man.
[00:31:03.600 --> 00:31:12.720]   No pressure.
[00:31:12.720 --> 00:31:26.160]   Yeah, astronaut safety is, man, if I had all the wishes I could say about, that'd be the
[00:31:26.160 --> 00:31:28.600]   one to put it on.
[00:31:28.600 --> 00:31:32.200]   So space is dangerous.
[00:31:32.200 --> 00:31:44.040]   So yeah, the next milestone after that would be the next flight of Starship, which, you
[00:31:44.040 --> 00:31:47.920]   know, Starship is, the next flight of Starship is ready to fly.
[00:31:47.920 --> 00:31:52.640]   We are waiting for regulatory approval.
[00:31:52.640 --> 00:32:01.040]   You know, it really should not be possible to build a giant rocket faster than the paper
[00:32:01.040 --> 00:32:03.280]   can move from one desk to another.
[00:32:03.280 --> 00:32:08.360]   That stamp is really hard.
[00:32:08.360 --> 00:32:09.360]   Approved.
[00:32:09.360 --> 00:32:10.360]   Yeah.
[00:32:10.360 --> 00:32:11.360]   Approved.
[00:32:11.360 --> 00:32:22.640]   You ever see that movie Zootopia, there's like a sloth coming in for the approval.
[00:32:22.640 --> 00:32:27.560]   Yeah, they accidentally tell a joke, and I was like, "Oh no, this is going to take a
[00:32:27.560 --> 00:32:28.560]   long time."
[00:32:28.560 --> 00:32:29.560]   Sorry, sorry.
[00:32:29.560 --> 00:32:38.280]   Yeah, Zootopia, you know, the funny thing is, like, so I went to the DMV about, I don't
[00:32:38.280 --> 00:32:45.320]   know, a year later after Zootopia to get my license renewal, and the guy in an exercise
[00:32:45.320 --> 00:32:52.840]   of incredible self-awareness had the sloth from Zootopia in his cube, and he was actually
[00:32:52.840 --> 00:32:53.840]   Swift.
[00:32:53.840 --> 00:32:54.840]   Yeah.
[00:32:54.840 --> 00:32:57.560]   With the mandate, "Beat the sloth."
[00:32:57.560 --> 00:32:58.560]   Yeah, no.
[00:32:58.560 --> 00:32:59.560]   "Beat personal agency.
[00:32:59.560 --> 00:33:00.560]   Personal agency."
[00:33:00.560 --> 00:33:08.520]   No, I mean, sometimes people think the government is more competent than it is.
[00:33:08.520 --> 00:33:11.160]   I'm not saying that there aren't competent people in the government, they're just in
[00:33:11.160 --> 00:33:13.840]   an operating system that is inefficient.
[00:33:13.840 --> 00:33:18.520]   Once you move them to a more efficient operating system, their output is dramatically greater,
[00:33:18.520 --> 00:33:25.480]   as we've seen, you know, when East Germany was reintegrated with West Germany, and the
[00:33:25.480 --> 00:33:33.400]   same people were vastly more prosperous, with a basically half-capitalist operating system.
[00:33:33.400 --> 00:33:42.400]   So, but I mean, for a lot of people, like, their most direct experience with the government
[00:33:42.400 --> 00:33:47.960]   is the DMV, and then the important thing to remember is that the government is the DMV
[00:33:47.960 --> 00:33:48.960]   at scale.
[00:33:48.960 --> 00:33:49.960]   Right.
[00:33:49.960 --> 00:33:50.960]   That's the government.
[00:33:50.960 --> 00:33:54.120]   Got the mental picture.
[00:33:54.120 --> 00:33:56.480]   How much do you want to scale it?
[00:33:56.480 --> 00:33:57.480]   Yeah.
[00:33:57.480 --> 00:33:58.480]   Yeah.
[00:33:58.480 --> 00:34:03.600]   Elon, sorry, can you go back to Chamath's question on Starship?
[00:34:03.600 --> 00:34:08.080]   So you announced just the other day Starship going to Mars in two years, and-
[00:34:08.080 --> 00:34:09.080]   By the way.
[00:34:09.080 --> 00:34:10.080]   Huh?
[00:34:10.080 --> 00:34:11.080]   Yeah, yeah, yeah.
[00:34:11.080 --> 00:34:12.080]   Yeah, yeah.
[00:34:12.080 --> 00:34:16.520]   And then four years for a crude aspirational launch in the next window?
[00:34:16.520 --> 00:34:18.360]   And how much is the government involved, and NASA involved?
[00:34:18.360 --> 00:34:25.120]   I'm not saying, like, say you watch by these, you know, but these, but based on our current
[00:34:25.120 --> 00:34:31.680]   progress where with Starship we were able to successfully reach orbital velocity twice,
[00:34:31.680 --> 00:34:37.680]   we were able to achieve soft landings of the booster and the ship in water, and that's
[00:34:37.680 --> 00:34:41.000]   despite the ship having, you know, half its flaps corked off.
[00:34:41.000 --> 00:34:45.200]   You can see the video on the X platform, it's quite exciting.
[00:34:45.200 --> 00:34:54.600]   So, you know, we think we'll be able to launch reliably and repeatedly and quite quickly.
[00:34:54.600 --> 00:35:01.760]   And the fundamental Holy Grail breakthrough for rocketry, the fundamental breakthrough
[00:35:01.760 --> 00:35:08.880]   that is needed for life to become multi-planetary is a rapidly reusable, reliable rocket.
[00:35:08.880 --> 00:35:18.560]   With a pirate, somehow, throw a pirate in there.
[00:35:18.560 --> 00:35:27.460]   So Starship is the first rocket design where success is one of the possible outcomes with
[00:35:27.460 --> 00:35:28.460]   full reusability.
[00:35:28.460 --> 00:35:34.120]   So, you know, for any given project you have to say, this is the circle, so we'll write
[00:35:34.120 --> 00:35:42.280]   that, here's the circle, and it is success, the success dot in the circle is success in
[00:35:42.280 --> 00:35:43.840]   the set of possible outcomes.
[00:35:43.840 --> 00:35:50.160]   That's, you know, it sounds pretty obvious, but there are often projects where that success
[00:35:50.160 --> 00:35:53.240]   is not in the set of possible outcomes.
[00:35:53.240 --> 00:36:00.080]   And so Starship not only is full reusability in the set of possible outcomes, it is being
[00:36:00.080 --> 00:36:06.240]   proven with each launch, and I'm confident we'll succeed, it's simply a matter of time.
[00:36:06.240 --> 00:36:14.200]   And if we can get some improvement in the speed of regulation, we could actually move
[00:36:14.200 --> 00:36:17.720]   a lot faster.
[00:36:17.720 --> 00:36:27.040]   So that would be very helpful, and in fact, if something isn't done about reducing regulation
[00:36:27.040 --> 00:36:32.120]   and sort of speeding up approvals, and to be clear, I'm not talking about anything unsafe,
[00:36:32.120 --> 00:36:39.320]   it's simply the processing of the safe thing can be done as fast as the rocket is built,
[00:36:39.320 --> 00:36:47.240]   not slower, then we could become a space-faring civilization and a multi-planet species and
[00:36:47.240 --> 00:36:49.000]   be out there among the stars in the future.
[00:36:49.000 --> 00:37:02.640]   And it's incredibly important that we have things that we find inspiring, that you look
[00:37:02.640 --> 00:37:06.600]   to the future and say the future's going to be better than the past, things to look forward
[00:37:06.600 --> 00:37:08.880]   to.
[00:37:08.880 --> 00:37:14.880]   Like kids are a good way to assess this, like what are kids fired up about?
[00:37:14.880 --> 00:37:23.280]   If you could say, you could be an astronaut on Mars, you could maybe one day go beyond
[00:37:23.280 --> 00:37:30.120]   the solar system, we could make Star Trek, Starfleet Academy real.
[00:37:30.120 --> 00:37:32.200]   That is an exciting future.
[00:37:32.200 --> 00:37:34.200]   That is inspiring.
[00:37:34.200 --> 00:37:39.720]   I mean, you need things that move your heart.
[00:37:39.720 --> 00:37:52.720]   Life can't just be about solving one miserable problem after another, there's got to be things
[00:37:52.720 --> 00:37:54.920]   that you look forward to as well.
[00:37:54.920 --> 00:38:00.280]   Yeah, and do you think you might have to move it to a different jurisdiction to move faster?
[00:38:00.280 --> 00:38:03.320]   I've always wondered if like...
[00:38:03.320 --> 00:38:07.360]   Rocket technology is considered advanced weapons technology, so we can't just go do it...
[00:38:07.360 --> 00:38:08.360]   In another country?
[00:38:08.360 --> 00:38:09.360]   Yes.
[00:38:09.360 --> 00:38:10.360]   That's interesting.
[00:38:10.360 --> 00:38:12.040]   And if we don't do it, other countries could do it.
[00:38:12.040 --> 00:38:20.360]   I mean, they're so far behind us, but theoretically, there is a national security, you know, justification
[00:38:20.360 --> 00:38:21.360]   here.
[00:38:21.360 --> 00:38:24.560]   If somebody can put their thinking caps on, like, do we want to have this technology that
[00:38:24.560 --> 00:38:28.080]   you're building, the team's working so hard on, stolen by other countries?
[00:38:28.080 --> 00:38:31.340]   And then, you know, maybe they don't have as much red tape.
[00:38:31.340 --> 00:38:35.000]   I wish people were trying to steal it.
[00:38:35.000 --> 00:38:44.360]   So no one's trying to steal it, it's just too crazy, basically.
[00:38:44.360 --> 00:38:45.360]   And that's for you.
[00:38:45.360 --> 00:38:46.360]   Yeah, it's way too crazy.
[00:38:46.360 --> 00:38:53.920]   Elon, what do you think is going on that led to Boeing building the Starliner the way that
[00:38:53.920 --> 00:38:54.920]   they did?
[00:38:54.920 --> 00:38:57.000]   They were able to get it up.
[00:38:57.000 --> 00:38:59.280]   But not complete.
[00:38:59.280 --> 00:39:01.240]   But can't complete.
[00:39:01.240 --> 00:39:02.240]   They can't finish.
[00:39:02.240 --> 00:39:03.240]   Can't finish.
[00:39:03.240 --> 00:39:04.240]   I don't understand.
[00:39:04.240 --> 00:39:09.280]   You're going to have to go up and finish.
[00:39:09.280 --> 00:39:16.960]   Well, I mean, I think Boeing is a company that is, you know, they actually do so much
[00:39:16.960 --> 00:39:21.100]   business with the government, they have sort of impedance matched to the government.
[00:39:21.100 --> 00:39:25.800]   So they're like basically one notch away from the government, maybe, they're not far from
[00:39:25.800 --> 00:39:29.080]   the government from an efficiency standpoint, because they derive so much of their revenue
[00:39:29.080 --> 00:39:30.080]   from the government.
[00:39:30.080 --> 00:39:34.240]   And a lot of people think, well, SpaceX is super dependent on the government.
[00:39:34.240 --> 00:39:42.780]   And actually, no, most of our revenue is commercial.
[00:39:42.780 --> 00:39:50.720]   And there's, I think, at least up until perhaps recently, because they have a new CEO who
[00:39:50.720 --> 00:39:53.320]   actually shows up in the factory.
[00:39:53.320 --> 00:39:58.280]   And the CEO before that, I think, had a degree in accounting and never went to the factory,
[00:39:58.280 --> 00:40:02.240]   and didn't know how airplanes flew.
[00:40:02.240 --> 00:40:10.140]   So I think if you are in charge of a company that makes airplanes fly, and a spacecraft
[00:40:10.140 --> 00:40:18.200]   go to orbit, then it can't be a total mystery as to how they work.
[00:40:18.200 --> 00:40:25.000]   So you know, I'm like, sure, if somebody is like running Coke or Pepsi, and they're like
[00:40:25.000 --> 00:40:31.880]   great at marketing or whatever, that's fine, because it's not a sort of technology-dependent
[00:40:31.880 --> 00:40:32.880]   business.
[00:40:32.880 --> 00:40:39.600]   Or if they're running a financial consulting, and their degree is in accounting, that makes
[00:40:39.600 --> 00:40:40.600]   sense.
[00:40:40.600 --> 00:40:46.440]   But I think if you're the cavalry captain, you should know how to ride a horse.
[00:40:46.440 --> 00:40:47.440]   Pretty basic.
[00:40:47.440 --> 00:40:48.440]   Yeah.
[00:40:48.440 --> 00:40:49.440]   Yeah.
[00:40:49.440 --> 00:40:50.440]   Yeah.
[00:40:50.440 --> 00:40:54.520]   It's like, it's disconcerting if the cavalry captain just falls off the horse.
[00:40:54.520 --> 00:40:57.520]   He's not going to inspire the team.
[00:40:57.520 --> 00:40:58.520]   I'm sorry.
[00:40:58.520 --> 00:40:59.520]   I'm scared of horses.
[00:40:59.520 --> 00:41:04.040]   He gets on backwards, and I'm like, oops.
[00:41:04.040 --> 00:41:08.400]   Shifting gears to AI, Peter was here earlier, and he was talking about how so far the only
[00:41:08.400 --> 00:41:11.920]   company to really make money off AI is NVIDIA with the chips.
[00:41:11.920 --> 00:41:18.640]   Do you have a sense yet of where you think the big applications will be from AI?
[00:41:18.640 --> 00:41:23.200]   Is it going to be in enabling self-driving, is it going to be enabling robots, is it transforming
[00:41:23.200 --> 00:41:24.200]   industries?
[00:41:24.200 --> 00:41:29.000]   I mean, it's still, I think, early in terms of where the big business impact is going
[00:41:29.000 --> 00:41:30.000]   to be.
[00:41:30.000 --> 00:41:42.960]   Do you have a sense yet?
[00:41:42.960 --> 00:41:49.800]   I think the spending on AI probably runs ahead of, I mean, it does run ahead of the revenue
[00:41:49.800 --> 00:41:50.800]   right now.
[00:41:50.800 --> 00:41:53.800]   There's no question about that.
[00:41:53.800 --> 00:42:02.120]   But the rate of improvement of AI is faster than any technology I've ever seen by far.
[00:42:02.120 --> 00:42:10.360]   And it's, I mean, for example, the Turing test used to be a thing.
[00:42:10.360 --> 00:42:16.960]   Now your basic open source, random LLM, you're writing on a frigging Raspberry Pi probably
[00:42:16.960 --> 00:42:22.560]   could beat the Turing test.
[00:42:22.560 --> 00:42:36.440]   So there's, I think actually the good future of AI is one of immense prosperity, where
[00:42:36.440 --> 00:42:43.280]   there is an age of abundance, no shortage of goods and services.
[00:42:43.280 --> 00:42:47.600]   Everyone can have whatever they want, unless, except for things we artificially define to
[00:42:47.600 --> 00:42:52.120]   be scarce, like some special artwork.
[00:42:52.120 --> 00:42:57.240]   But anything that is a manufactured good or provided service will, I think, with the advent
[00:42:57.240 --> 00:43:06.320]   of AI plus robotics, that the cost of goods and services will trend to zero.
[00:43:06.320 --> 00:43:13.680]   I'm not saying it'll be actually zero, but it'll be, everyone will be able to have anything
[00:43:13.680 --> 00:43:15.640]   they want.
[00:43:15.640 --> 00:43:16.640]   That's the good future.
[00:43:16.640 --> 00:43:20.840]   Of course, in my view, that's probably 80% likely.
[00:43:20.840 --> 00:43:28.960]   Look on the bright side, only 20% probability of annihilation, nothing.
[00:43:28.960 --> 00:43:31.520]   Is the 20%, what does that look like?
[00:43:31.520 --> 00:43:33.520]   I don't know, man.
[00:43:33.520 --> 00:43:38.200]   I mean, frankly, I do have to go engage in some degree of deliberate suspension of disbelief
[00:43:38.200 --> 00:43:46.280]   with respect to AI in order to sleep well, and even then, because I think the actual
[00:43:46.280 --> 00:43:50.440]   issue, the most likely issue is like, well, how do we find meaning in a world where AI
[00:43:50.440 --> 00:43:53.640]   can do everything we can do, but better?
[00:43:53.640 --> 00:44:00.360]   That is perhaps the bigger challenge, although at this point, I know more and more people
[00:44:00.360 --> 00:44:07.800]   who are retired, and they seem to enjoy that life, but I think that maybe there'll be some
[00:44:07.800 --> 00:44:13.880]   crisis of meaning, because the computer can do everything you can do, but better, so maybe
[00:44:13.880 --> 00:44:25.040]   that'll be a challenge, but really, you need the end effectors, you need the autonomous
[00:44:25.040 --> 00:44:32.760]   cars, and you need the humanoid robots, or general purpose robots, but once you have
[00:44:32.760 --> 00:44:44.760]   general purpose humanoid robots, and autonomous vehicles, really, you can build anything,
[00:44:44.760 --> 00:44:48.920]   and I think that there's no actual limit to the size of the economy.
[00:44:48.920 --> 00:44:57.600]   I mean, there's obviously the mass of Earth, that'll be one limit, but the economy is really
[00:44:57.600 --> 00:45:02.080]   just the average productivity per person times number of people.
[00:45:02.080 --> 00:45:03.840]   That's the economy.
[00:45:03.840 --> 00:45:10.840]   And if you've got humanoid robots that can do, where there's no real limit on the number
[00:45:10.840 --> 00:45:17.680]   of humanoid robots, and they can operate very intelligently, then there's no actual limit
[00:45:17.680 --> 00:45:21.240]   to the economy, there's no meaningful limit to the economy.
[00:45:21.240 --> 00:45:27.000]   You guys just turned on Colossus, which is like the largest private compute cluster,
[00:45:27.000 --> 00:45:30.080]   I guess, of GPUs anywhere, is that right?
[00:45:30.080 --> 00:45:34.440]   It's the most powerful supercomputer of any kind.
[00:45:34.440 --> 00:45:38.200]   Which sort of speaks to what David said, and kind of what Peter said, which is a lot of
[00:45:38.200 --> 00:45:45.360]   the kind of economic value so far of AI has entirely gone to NVIDIA, but there are people
[00:45:45.360 --> 00:45:48.800]   with alternatives, and you're actually one with an alternative, now you have a very specific
[00:45:48.800 --> 00:45:53.920]   case, because Dojo's really about images, and large images, huge video.
[00:45:53.920 --> 00:46:03.120]   Yeah, I mean, the Tesla problem is different from the sort of LLM problem.
[00:46:03.120 --> 00:46:11.920]   The nature of the intelligence is actually, and what matters in the AI is different to
[00:46:11.920 --> 00:46:17.480]   the point you just made, which is that in Tesla's case, the context is very long.
[00:46:17.480 --> 00:46:18.480]   So we've got gigabytes of context.
[00:46:18.480 --> 00:46:19.480]   Gigabytes of context, yeah.
[00:46:19.480 --> 00:46:27.640]   Yeah, you've got billions of tokens of context, a nutty amount of context, because you've
[00:46:27.640 --> 00:46:35.640]   got seven cameras, and if you've got several, let's say you've got a minute of several high
[00:46:35.640 --> 00:46:38.400]   def cameras, then that's gigabytes.
[00:46:38.400 --> 00:46:47.080]   So the Tesla problem is you've got to compress a gigantic context into the pixels that actually
[00:46:47.080 --> 00:46:56.760]   matter, and condense that over a time, so you've got to, in both the time dimension
[00:46:56.760 --> 00:47:05.080]   and the space dimension, you've got to compress the pixels in space and the pixels in time,
[00:47:05.080 --> 00:47:12.320]   and then have that inference done on a tiny computer, relatively speaking, a few hundred
[00:47:12.320 --> 00:47:18.320]   gigabytes, it's a Tesla-designed AI inference computer, which is by the way still the best,
[00:47:18.320 --> 00:47:21.160]   there isn't a better thing we could buy from suppliers.
[00:47:21.160 --> 00:47:25.720]   So the Tesla-designed AI inference computer that's in the cars is better than anything
[00:47:25.720 --> 00:47:31.900]   we could buy from any supplier, just by the way, that's kind of a, the Tesla AI chip team
[00:47:31.900 --> 00:47:32.900]   is extremely good.
[00:47:32.900 --> 00:47:36.280]   You guys, in the design, there was a technical paper, and there was a deck that somebody
[00:47:36.280 --> 00:47:40.560]   on your team from Tesla published, and it was stunning to me.
[00:47:40.560 --> 00:47:44.560]   You designed your own transport control layer over Ethernet, you were like, "Ah, Ethernet's
[00:47:44.560 --> 00:47:48.880]   not good enough for us," and you have this TT-COE or something, and you're like, "Oh,
[00:47:48.880 --> 00:47:52.120]   we're just going to reinvent Ethernet and string these chips," it's pretty incredible
[00:47:52.120 --> 00:47:54.640]   stuff that's happening over there.
[00:47:54.640 --> 00:47:55.640]   Yeah.
[00:47:55.640 --> 00:48:01.120]   No, the Tesla chip design team is extremely good.
[00:48:01.120 --> 00:48:07.380]   But is there a world where, for example, other people over time that need some sort of video
[00:48:07.380 --> 00:48:12.680]   use case or image use case could theoretically, you'd say, "Oh, why not, I have some extra
[00:48:12.680 --> 00:48:16.960]   cycles over here," which would kind of make you a competitor of NVIDIA, it's not intentionally
[00:48:16.960 --> 00:48:25.200]   per se, but ... Yeah, I mean, there's this training and inference,
[00:48:25.200 --> 00:48:31.280]   and we do have those two projects at Tesla, we've got Dojo, which is the training computer,
[00:48:31.280 --> 00:48:42.420]   and then our inference chip, which is in every car, inference computer, and Dojo, we've only
[00:48:42.420 --> 00:48:51.020]   had Dojo 1, Dojo 2 is, we should have Dojo 2 in volume towards the end of next year,
[00:48:51.020 --> 00:49:02.700]   and that will be, we think, sort of comparable to a B200 type system, a training system,
[00:49:02.700 --> 00:49:15.880]   and so, I guess there's some potential for that to be used as a service, but Dojo is
[00:49:15.880 --> 00:49:26.340]   just kind of like, I guess I have some improved confidence in Dojo, but I think we won't really
[00:49:26.340 --> 00:49:33.200]   know how good Dojo is until probably version 3, it usually takes three major iterations
[00:49:33.200 --> 00:49:38.720]   on a technology for it to be excellent, and we'll only have the second major iteration
[00:49:38.720 --> 00:49:46.760]   next year, the third iteration, I don't know, maybe late, you know, 26 or something like
[00:49:46.760 --> 00:49:47.760]   that.
[00:49:47.760 --> 00:49:52.300]   How's the Optimist project going, I remember when we talked last, and you said this publicly,
[00:49:52.300 --> 00:49:58.820]   that it's in doing some light testing inside the factory, so it's actually being useful,
[00:49:58.820 --> 00:50:03.380]   what's the build of materials, and when, you know, for something like that at scale, so
[00:50:03.380 --> 00:50:06.260]   when you start making it like you're making the Model 3 now, and there's a million of
[00:50:06.260 --> 00:50:11.660]   them coming off the factory line, what would they cost, $20,000, $30,000, $40,000 you think?
[00:50:11.660 --> 00:50:19.200]   Yeah, I mean, I've discovered really that anything made in sufficient volume will asymptotically
[00:50:19.200 --> 00:50:29.580]   approach the cost of its materials, so some things are constrained by the cost of intellectual
[00:50:29.580 --> 00:50:36.580]   property and like paying for patents and stuff, so a lot of what's in a chip is like paying
[00:50:36.580 --> 00:50:42.620]   royalties and depreciation of the chip fab, but the actual marginal cost of the chips
[00:50:42.620 --> 00:50:51.020]   is very low, so Optimist is obviously a humanoid robot, it weighs much less and is much smaller
[00:50:51.020 --> 00:51:00.420]   than a car, so you could expect that in high volume, and I'd say you also probably need
[00:51:00.420 --> 00:51:07.060]   three production versions of Optimist, so you need to refine the design at least three
[00:51:07.060 --> 00:51:13.500]   major times, and then you need to scale production to sort of the million unit plus per year
[00:51:13.500 --> 00:51:25.040]   level, and I think at that point, the labor and materials on Optimist is probably not
[00:51:25.040 --> 00:51:27.140]   much more than $10,000.
[00:51:27.140 --> 00:51:28.580]   And that's a decade-long journey, maybe?
[00:51:28.580 --> 00:51:39.020]   Basically, think of it like Optimist will cost less than a small car, so at scale volume
[00:51:39.020 --> 00:51:47.380]   with the three major iterations of technology, and so if a small car costs $25,000, it's
[00:51:47.380 --> 00:51:54.340]   probably like $20,000 for an Optimist, for a humanoid robot that can be your buddy like
[00:51:54.340 --> 00:51:59.820]   a combination of R2-D2 and C3PO, but better.
[00:51:59.820 --> 00:52:03.500]   I honestly think people are going to get really attached to their humanoid robot, because
[00:52:03.500 --> 00:52:07.880]   I mean, like you look at sort of, you watch Star Wars, and it's like R2-D2 and C3PO, I
[00:52:07.880 --> 00:52:16.500]   love those guys, you know, they're awesome, and their personality, and I mean, all R2
[00:52:16.500 --> 00:52:24.020]   could do is just beef at you, can't speak English, and C3PO to translate the beefs.
[00:52:24.020 --> 00:52:28.420]   So you're in year two of that, if you did two or three years per iteration or something,
[00:52:28.420 --> 00:52:32.420]   it's a decade-long journey for this to hit some sort of scale?
[00:52:32.420 --> 00:52:38.980]   I would say the major iterations are less than two years, so it's probably on the order
[00:52:38.980 --> 00:52:46.540]   of five years, maybe six to get to a million units a year.
[00:52:46.540 --> 00:52:49.860]   And at that price point, everybody can afford one, on planet Earth.
[00:52:49.860 --> 00:52:53.420]   I mean, it's going to be that one-to-one, two-to-one, what do you think ultimately,
[00:52:53.420 --> 00:52:58.620]   if we're sitting here in 30 years, the number of robots on the planet versus humans?
[00:52:58.620 --> 00:53:03.060]   Yeah, I think the number of robots will vastly exceed the number of humans.
[00:53:03.060 --> 00:53:04.060]   Vastly, yeah.
[00:53:04.060 --> 00:53:05.060]   Vastly exceed.
[00:53:05.060 --> 00:53:08.140]   I mean, you have to say, who would not want their robot buddy?
[00:53:08.140 --> 00:53:11.020]   Everyone wants a robot buddy.
[00:53:11.020 --> 00:53:12.020]   Totally.
[00:53:12.020 --> 00:53:19.740]   You know, this is like, especially if it can, you know, it can take care of your, take your
[00:53:19.740 --> 00:53:26.940]   dog for a walk, it could mow the lawn, it could watch your kids, it could, you know,
[00:53:26.940 --> 00:53:29.180]   like, it could teach your kids, it could...
[00:53:29.180 --> 00:53:31.020]   We could also send it to Mars.
[00:53:31.020 --> 00:53:32.020]   Yeah, absolutely.
[00:53:32.020 --> 00:53:37.180]   We could send a lot of robots to Mars to do the work needed to make it a colonized planet
[00:53:37.180 --> 00:53:38.180]   for humans.
[00:53:38.180 --> 00:53:39.180]   Mars is already a robot planet.
[00:53:39.180 --> 00:53:42.100]   There's like a whole bunch of, you know, robots, like rovers and...
[00:53:42.100 --> 00:53:43.100]   Only robots.
[00:53:43.100 --> 00:53:44.100]   Helicopter.
[00:53:44.100 --> 00:53:45.540]   Yes, only robots.
[00:53:45.540 --> 00:53:54.860]   So yeah, no, I think the sort of useful humanoid robot opportunity is the single biggest opportunity
[00:53:54.860 --> 00:54:01.300]   ever.
[00:54:01.300 --> 00:54:06.460]   Because if you assume like, I mean, the ratio of humanoid robots to humans is going to be
[00:54:06.460 --> 00:54:11.340]   at least two to one, maybe three to one, because everybody will want one, and then there'll
[00:54:11.340 --> 00:54:14.220]   be a bunch of robots that you don't see that are making goods and services.
[00:54:14.220 --> 00:54:18.180]   And you think it's a general, one generalized robot that then learns how to do different
[00:54:18.180 --> 00:54:19.180]   tasks or...
[00:54:19.180 --> 00:54:20.180]   Yeah.
[00:54:20.180 --> 00:54:22.740]   I mean, we are a generalized robot.
[00:54:22.740 --> 00:54:24.940]   Yeah, we're a generalized, non-robot.
[00:54:24.940 --> 00:54:25.940]   We're just made of meat.
[00:54:25.940 --> 00:54:26.940]   Yeah, exactly.
[00:54:26.940 --> 00:54:27.940]   We're a meatball.
[00:54:27.940 --> 00:54:28.940]   We're a generalized meatball.
[00:54:28.940 --> 00:54:33.220]   Yeah, I mean, I'm operating my meat puppet, you know.
[00:54:33.220 --> 00:54:35.660]   So yeah, we are actually...
[00:54:35.660 --> 00:54:40.780]   And by the way, it turns out like, as we're designing Optimus, we sort of learn more and
[00:54:40.780 --> 00:54:45.380]   more about why humans are shaped the way they're shaped.
[00:54:45.380 --> 00:54:50.020]   And you know, and why we have five fingers and why your little finger is smaller than
[00:54:50.020 --> 00:54:58.340]   your index finger, obviously why you have opposable thumbs, but also why, for example,
[00:54:58.340 --> 00:55:04.340]   the muscles, the major muscles that operate your hand are actually in your forearm.
[00:55:04.340 --> 00:55:09.900]   And your fingers are primarily operated, like...
[00:55:09.900 --> 00:55:15.740]   The muscles that actuate your fingers are located, the vast majority of your finger
[00:55:15.740 --> 00:55:21.340]   strength is actually coming from your forearm, and your fingers are being operated by tendons,
[00:55:21.340 --> 00:55:24.900]   little strings.
[00:55:24.900 --> 00:55:31.540]   And so the current version of the Optimus hand has the actuators in the hand and has
[00:55:31.540 --> 00:55:36.940]   only 11 degrees of freedom, so it doesn't have all the degrees of freedom of human hand,
[00:55:36.940 --> 00:55:45.040]   which has, depending on how you count it, roughly 25 degrees of freedom.
[00:55:45.040 --> 00:55:49.660]   And it's also like, not strong enough in certain ways, because the actuators have to fit in
[00:55:49.660 --> 00:55:51.460]   the hand.
[00:55:51.460 --> 00:55:57.740]   So the next generation Optimus hand, which we have in prototype form, the actuators have
[00:55:57.740 --> 00:56:03.220]   moved to the forearm, just like a human, and they operate the fingers through cables, just
[00:56:03.220 --> 00:56:05.300]   like a human hand.
[00:56:05.300 --> 00:56:13.220]   And then the next generation hand has 22 degrees of freedom, which we think is enough to do
[00:56:13.220 --> 00:56:18.300]   almost anything that a human can do.
[00:56:18.300 --> 00:56:25.420]   And presumably, I think it was written that X and Tesla may work together and provide
[00:56:25.420 --> 00:56:28.940]   services, but my immediate thought went to, "Oh, if you just provide a grok to the robot,
[00:56:28.940 --> 00:56:34.260]   then the robot has a personality and can process voice and video and images and all of that
[00:56:34.260 --> 00:56:35.260]   stuff."
[00:56:35.260 --> 00:56:36.260]   It's the UI.
[00:56:36.260 --> 00:56:37.940]   That's where we wrap here.
[00:56:37.940 --> 00:56:44.020]   I think everybody talks about all the projects you're working on, but people don't know you
[00:56:44.020 --> 00:56:45.020]   have a great sense of humor.
[00:56:45.020 --> 00:56:46.020]   That's not true.
[00:56:46.020 --> 00:56:47.020]   Oh, you do.
[00:56:47.020 --> 00:56:48.020]   You do.
[00:56:48.020 --> 00:56:52.740]   People don't see it, but I would say, I know for me, the funniest week of my life, or one
[00:56:52.740 --> 00:56:57.940]   of the funniest, was when you did SNL and I got to tag along.
[00:56:57.940 --> 00:57:00.380]   Maybe you saw it.
[00:57:00.380 --> 00:57:08.180]   Maybe behind the scenes, some of your funniest recollections of that chaotic, insane week
[00:57:08.180 --> 00:57:10.180]   when we laughed for 12 hours a day.
[00:57:10.180 --> 00:57:13.100]   It was a little terrorizing on the first couple of days.
[00:57:13.100 --> 00:57:19.020]   Yeah, I was a bit worried at the beginning there because frankly, nothing was funny.
[00:57:19.020 --> 00:57:22.260]   Day one was rough.
[00:57:22.260 --> 00:57:23.260]   Rough.
[00:57:23.260 --> 00:57:24.260]   Yeah.
[00:57:24.260 --> 00:57:28.420]   Well, it's like a rule, but can't you guys just say it?
[00:57:28.420 --> 00:57:32.500]   Just say the stuff that got on the cut.
[00:57:32.500 --> 00:57:34.220]   The funniest skits were the ones they didn't let you do.
[00:57:34.220 --> 00:57:35.220]   That's what I'm saying.
[00:57:35.220 --> 00:57:36.220]   Can you just say it?
[00:57:36.220 --> 00:57:37.220]   There were a couple of funny ones, yeah, that they didn't let you do.
[00:57:37.220 --> 00:57:40.780]   You can say it, so that he doesn't get ... I mean, how much time do we have here?
[00:57:40.780 --> 00:57:46.100]   Well, we should just give him one or two because it was ... In your mind, which one do we regret
[00:57:46.100 --> 00:57:50.780]   most not getting on air?
[00:57:50.780 --> 00:57:51.780]   You really want to hear that?
[00:57:51.780 --> 00:57:52.780]   I mean ... I mean, it was a little spicy.
[00:57:52.780 --> 00:57:53.780]   It was a little funny.
[00:57:53.780 --> 00:57:54.780]   Okay.
[00:57:54.780 --> 00:57:55.780]   Here we go.
[00:57:55.780 --> 00:57:58.780]   All right, here we go, guys.
[00:57:58.780 --> 00:57:59.780]   All right.
[00:57:59.780 --> 00:58:13.540]   So, one of the things that I think everyone's been sort of wondering this whole time is,
[00:58:13.540 --> 00:58:18.100]   is Saturday Night Live actually live?
[00:58:18.100 --> 00:58:19.100]   Like live.
[00:58:19.100 --> 00:58:20.100]   Live, live, live.
[00:58:20.100 --> 00:58:25.940]   Do they have like a delay or like just in case there's a wardrobe malfunction or something
[00:58:25.940 --> 00:58:26.940]   like that?
[00:58:26.940 --> 00:58:27.940]   Right.
[00:58:27.940 --> 00:58:28.940]   Is it like a-
[00:58:28.940 --> 00:58:29.940]   Truly live.
[00:58:29.940 --> 00:58:30.940]   Five second delay?
[00:58:30.940 --> 00:58:31.940]   What's really going on?
[00:58:31.940 --> 00:58:34.100]   But there's a way to test this.
[00:58:34.100 --> 00:58:35.100]   Right.
[00:58:35.100 --> 00:58:36.100]   We came up with a way.
[00:58:36.100 --> 00:58:41.900]   There's a way to test this, which is we don't tell them what's going on.
[00:58:41.900 --> 00:58:46.140]   I walk on and say, "This is the script I'll throw on the ground.
[00:58:46.140 --> 00:58:48.740]   We're going to find out tonight, right now."
[00:58:48.740 --> 00:58:49.740]   If Saturday Night Live.
[00:58:49.740 --> 00:58:54.220]   Saturday Night Live is actually live.
[00:58:54.220 --> 00:58:59.960]   And the way that we're going to do this is I'm going to take my cock out.
[00:58:59.960 --> 00:59:08.420]   This is the greatest pitch ever.
[00:59:08.420 --> 00:59:15.620]   If you see my cock, you know it's true.
[00:59:15.620 --> 00:59:17.580]   And if you don't, it's been a lie.
[00:59:17.580 --> 00:59:19.380]   It's been a lie all these years.
[00:59:19.380 --> 00:59:20.380]   All these years.
[00:59:20.380 --> 00:59:23.760]   We're going to bust them right now.
[00:59:23.760 --> 00:59:24.760]   We're pitching this.
[00:59:24.760 --> 00:59:25.760]   Yeah.
[00:59:25.760 --> 00:59:26.760]   Yeah.
[00:59:26.760 --> 00:59:27.760]   So we're pitching this.
[00:59:27.760 --> 00:59:28.760]   On Zoom.
[00:59:28.760 --> 00:59:29.760]   Yeah.
[00:59:29.760 --> 00:59:30.760]   We're pitching this on Zoom on like a Monday afternoon.
[00:59:30.760 --> 00:59:31.760]   Because it's COVID.
[00:59:31.760 --> 00:59:32.760]   Yeah.
[00:59:32.760 --> 00:59:33.760]   We're like kind of hungover from the weekend.
[00:59:33.760 --> 00:59:34.760]   We're pitching this at noon.
[00:59:34.760 --> 00:59:35.760]   We're in Miami.
[00:59:35.760 --> 00:59:36.760]   Yeah.
[00:59:36.760 --> 00:59:37.760]   And Jason's on.
[00:59:37.760 --> 00:59:38.760]   Mike and you.
[00:59:38.760 --> 00:59:39.760]   Yeah.
[00:59:39.760 --> 00:59:40.760]   And Mike.
[00:59:40.760 --> 00:59:48.980]   My friends who I think are sort of quite funny, Jason's quite funny.
[00:59:48.980 --> 00:59:52.980]   I think Jason's the closest thing to Cartman that exists in real life.
[00:59:52.980 --> 00:59:53.980]   Yes.
[00:59:53.980 --> 01:00:00.100]   We have a joke going that he's Butters and I'm Cartman.
[01:00:00.100 --> 01:00:01.100]   Yeah.
[01:00:01.100 --> 01:00:05.620]   And my friend Mike's pretty funny too.
[01:00:05.620 --> 01:00:11.500]   So we come in like just like guns blazing with like ideas.
[01:00:11.500 --> 01:00:17.720]   And we didn't realize actually that's not how it works and that's normally like actors
[01:00:17.720 --> 01:00:22.940]   and they just get told what to do and like, oh, you mean we can't just like do funny things
[01:00:22.940 --> 01:00:23.940]   that we thought of?
[01:00:23.940 --> 01:00:24.940]   What?
[01:00:24.940 --> 01:00:25.940]   They're watching this.
[01:00:25.940 --> 01:00:29.140]   And on the Zoom, they're aghast at Elon's pitch.
[01:00:29.140 --> 01:00:30.140]   Yeah.
[01:00:30.140 --> 01:00:31.140]   It's silence.
[01:00:31.140 --> 01:00:34.140]   And I'm like, is this thing working?
[01:00:34.140 --> 01:00:35.140]   Is this?
[01:00:35.140 --> 01:00:36.140]   Are we muted?
[01:00:36.140 --> 01:00:37.140]   Is our mic on?
[01:00:37.140 --> 01:00:38.140]   And they're like, we hear you.
[01:00:38.140 --> 01:00:39.140]   Yeah.
[01:00:39.140 --> 01:00:43.060]   And then after a long silence, like Mike just says the word "crickets."
[01:00:43.060 --> 01:00:45.700]   And they're not laughing.
[01:00:45.700 --> 01:00:47.860]   Not even a chuckle.
[01:00:47.860 --> 01:00:48.860]   What's going on here?
[01:00:48.860 --> 01:00:51.340]   And then Elon explains the punchline, which is.
[01:00:51.340 --> 01:00:52.340]   Exactly.
[01:00:52.340 --> 01:00:57.460]   So there's more to it, okay.
[01:00:57.460 --> 01:00:59.740]   That's just the beginning.
[01:00:59.740 --> 01:01:00.740]   So Elon says.
[01:01:00.740 --> 01:01:10.700]   So then I'm like, so, so, so, so I said like, I'm, I'm, I'm gonna, I'm going to reach down
[01:01:10.700 --> 01:01:15.540]   into my pants and I stuck my hand on my pants and I'm going to, and I'm, and I want to pull
[01:01:15.540 --> 01:01:22.340]   my cock out and I tell this to the audience and the audience is going to be like, what?
[01:01:22.340 --> 01:01:31.060]   And then, and then, and then, and then I pull out a baby rooster, you know?
[01:01:31.060 --> 01:01:36.660]   And it's like, okay, this is kind of PG, you know, it's like, not that bad.
[01:01:36.660 --> 01:01:38.340]   This is my tiny cock.
[01:01:38.340 --> 01:01:44.820]   And, and it's like, what do you think?
[01:01:44.820 --> 01:01:47.820]   And so then, and do you think it's a nice cock?
[01:01:47.820 --> 01:01:48.820]   I mean, I like it.
[01:01:48.820 --> 01:01:52.020]   And I pitch, I'm like, and then Kate McKinnon walks out.
[01:01:52.020 --> 01:01:53.020]   Yeah, exactly.
[01:01:53.020 --> 01:01:56.340]   And I'm like, oh no, but you haven't heard half of it, so Kate McKinnon comes out and
[01:01:56.340 --> 01:02:00.940]   she says, Elon, I expected you would have a bigger cock.
[01:02:00.940 --> 01:02:01.940]   Yeah.
[01:02:01.940 --> 01:02:06.060]   I was like, I don't mean to disappoint you, Kate, but yeah.
[01:02:06.060 --> 01:02:10.260]   But I hope you like it anyway.
[01:02:10.260 --> 01:02:12.780]   Kate's got to come out with, with, with her cat.
[01:02:12.780 --> 01:02:13.780]   Okay.
[01:02:13.780 --> 01:02:14.780]   Right.
[01:02:14.780 --> 01:02:18.380]   So you can see where this is going.
[01:02:18.380 --> 01:02:19.380]   And I say nice.
[01:02:19.380 --> 01:02:20.380]   Wow.
[01:02:20.380 --> 01:02:24.820]   That's, that's a, that's a, that's a nice pussy you've got there, Kate.
[01:02:24.820 --> 01:02:25.820]   Wow.
[01:02:25.820 --> 01:02:27.740]   That's amazing.
[01:02:27.740 --> 01:02:28.740]   It looks a little wet.
[01:02:28.740 --> 01:02:29.740]   Was it raining outside?
[01:02:29.740 --> 01:02:35.980]   Do you mind if I stroke your pussy?
[01:02:35.980 --> 01:02:36.980]   Is that cool?
[01:02:36.980 --> 01:02:41.940]   It's like, oh no, Elon, actually, can I hold your cock?
[01:02:41.940 --> 01:02:45.740]   Of course, Kate, you definitely hold my cock.
[01:02:45.740 --> 01:02:48.860]   And then, you know, we exchanged and I think just the audio version of this was pretty
[01:02:48.860 --> 01:02:49.860]   good.
[01:02:49.860 --> 01:02:50.860]   Right.
[01:02:50.860 --> 01:02:57.580]   And, and, and, you know, it's just like, wow, you, I really like stroking your cock.
[01:02:57.580 --> 01:03:05.460]   And I was like, I'm really enjoying stroking your pussy.
[01:03:05.460 --> 01:03:07.020]   Yes, of course.
[01:03:07.020 --> 01:03:13.660]   And yeah, so, you know, they're looking at us like, oh my God, what have we done inviting
[01:03:13.660 --> 01:03:15.660]   these lunatics on the program?
[01:03:15.660 --> 01:03:16.660]   Yeah.
[01:03:16.660 --> 01:03:21.420]   And then they said, they said like, well, um, it is, uh, it is Mother's Day.
[01:03:21.420 --> 01:03:26.780]   It's Mother's Day, we might not want to go with this.
[01:03:26.780 --> 01:03:30.540]   A lot of moms in the audience, and I'm like, well, that's a good point.
[01:03:30.540 --> 01:03:31.540]   Fair, fair.
[01:03:31.540 --> 01:03:34.460]   It might be a bit uncomfortable for all the moms in the audience, maybe, I don't know.
[01:03:34.460 --> 01:03:35.460]   I don't know.
[01:03:35.460 --> 01:03:36.460]   Maybe they'll dig it.
[01:03:36.460 --> 01:03:37.460]   Maybe they'll like it.
[01:03:37.460 --> 01:03:45.860]   Uh, so, uh, yeah, that was, that's the, um, that's the, that's the, that's the, um, cold
[01:03:45.860 --> 01:03:47.140]   open that didn't make it.
[01:03:47.140 --> 01:03:53.020]   We didn't get that on the air, um, but, uh, we did fight for Doge.
[01:03:53.020 --> 01:03:54.020]   Yes.
[01:03:54.020 --> 01:03:55.020]   And we got Doge on the air.
[01:03:55.020 --> 01:03:57.180]   Well, I mean, there's a bunch of things that I said that were just not on the script.
[01:03:57.180 --> 01:03:59.860]   Like if they have these like cue cards for what you're supposed to say, and I just didn't
[01:03:59.860 --> 01:04:00.860]   say it.
[01:04:00.860 --> 01:04:01.860]   I just went off the rails.
[01:04:01.860 --> 01:04:02.860]   Yeah.
[01:04:02.860 --> 01:04:03.860]   They didn't see that coming.
[01:04:03.860 --> 01:04:04.860]   Yeah, it's live.
[01:04:04.860 --> 01:04:05.860]   It's live.
[01:04:05.860 --> 01:04:13.940]   And, uh, so the, Elon wanted to do Doge.
[01:04:13.940 --> 01:04:14.940]   This is the other one.
[01:04:14.940 --> 01:04:18.540]   Elon wanted to do Doge on late night and he says, um, hey, Jake, how can you, um, make
[01:04:18.540 --> 01:04:19.540]   sure?
[01:04:19.540 --> 01:04:20.540]   Oh yeah.
[01:04:20.540 --> 01:04:21.540]   I wanted to do the Doge father.
[01:04:21.540 --> 01:04:25.460]   Like you sort of redo the, you know, that scene from, uh, the, the, the Godfather.
[01:04:25.460 --> 01:04:28.020]   I mean, you kind of need the music to cue things up.
[01:04:28.020 --> 01:04:36.380]   You bring me on my daughter's wedding and you ask for Doge.
[01:04:36.380 --> 01:04:37.380]   Yeah.
[01:04:37.380 --> 01:04:38.380]   You got Marlon Brando.
[01:04:38.380 --> 01:04:40.300]   I give you Bitcoin, but you want Doge.
[01:04:40.300 --> 01:04:41.300]   Exactly.
[01:04:41.300 --> 01:04:42.300]   You really got to set the mood.
[01:04:42.300 --> 01:04:46.260]   You got to have the tuxedo and the sort of job office and the, you know, and you're gonna
[01:04:46.260 --> 01:04:57.220]   have like Marlon Brando and I said, you come to me on this day of my Doge's wedding and
[01:04:57.220 --> 01:05:02.420]   you ask me for your private keys.
[01:05:02.420 --> 01:05:04.540]   Are you even a friend?
[01:05:04.540 --> 01:05:08.280]   You call me the Doge father.
[01:05:08.280 --> 01:05:14.500]   So that's potential, that great potential.
[01:05:14.500 --> 01:05:19.940]   So they come to me and I'm, I'm talking to Colin, um, and Joe's who's got a great sense
[01:05:19.940 --> 01:05:20.940]   of humor and he's amazing.
[01:05:20.940 --> 01:05:24.820]   He loves Elon and he's like, we can't do it because of the law and stuff like that.
[01:05:24.820 --> 01:05:27.660]   And the law, the law and liability.
[01:05:27.660 --> 01:05:29.220]   So I said, it's okay.
[01:05:29.220 --> 01:05:37.640]   Elon called Comcast and he put in an offer and they just accepted it, NBC.
[01:05:37.640 --> 01:05:38.640]   So it's fine.
[01:05:38.640 --> 01:05:39.640]   Yeah.
[01:05:39.640 --> 01:05:45.560]   And Colin Jones looks at me and I sold it so good and he's like, you're serious.
[01:05:45.560 --> 01:05:52.760]   I'm like, yep, we own NBC now and he's like, okay, well that kind of changes things, doesn't
[01:05:52.760 --> 01:05:53.760]   it?
[01:05:53.760 --> 01:05:54.760]   I'm like, absolutely.
[01:05:54.760 --> 01:05:56.880]   We're a go on, on Doge.
[01:05:56.880 --> 01:06:01.720]   And then he's like, you're fucking with me and I'm like, I'm fucking with you.
[01:06:01.720 --> 01:06:02.720]   Or are we?
[01:06:02.720 --> 01:06:03.720]   Or are we?
[01:06:03.720 --> 01:06:09.080]   It was the greatest week of, and that like is like two of 10 stories.
[01:06:09.080 --> 01:06:12.120]   We'll save the other eight.
[01:06:12.120 --> 01:06:18.600]   But it was, and I was just so happy for you to see you have a great week of just joy and
[01:06:18.600 --> 01:06:19.600]   fun and letting go.
[01:06:19.600 --> 01:06:23.720]   Cause you were launching rockets, you're dealing with so much bullshit in your life to have
[01:06:23.720 --> 01:06:27.480]   those moments, to share them and just laugh.
[01:06:27.480 --> 01:06:28.480]   It was just so great.
[01:06:28.480 --> 01:06:29.480]   Yeah.
[01:06:29.480 --> 01:06:30.480]   More of those moments.
[01:06:30.480 --> 01:06:31.480]   I think we gotta, we gotta get you back on SNL.
[01:06:31.480 --> 01:06:32.480]   Who wants him back on SNL one more time?
[01:06:32.480 --> 01:06:37.480]   All right, ladies and gentlemen, our bestie, Elon Musk.
[01:06:37.480 --> 01:06:38.480]   Elon Musk.
[01:06:38.480 --> 01:06:38.480]   Elon Musk.
[01:06:38.480 --> 01:06:43.480]   Thank you.
[01:06:43.480 --> 01:06:48.480]   Thank you.
[01:06:48.480 --> 01:06:50.720]   (applause)
[01:06:50.720 --> 01:07:00.720]   [BLANK_AUDIO]

