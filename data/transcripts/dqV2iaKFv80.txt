
[00:00:00.000 --> 00:00:05.600]   Hello, my name is Zephania Shukla and this is my co-host Karen Maruni and you're listening
[00:00:05.600 --> 00:00:07.720]   to Greed and Dissent Business.
[00:00:07.720 --> 00:00:10.880]   Welcome to the show.
[00:00:10.880 --> 00:00:11.880]   Welcome everyone.
[00:00:11.880 --> 00:00:14.360]   Our guest today is Chris Van Pelt.
[00:00:14.360 --> 00:00:19.000]   Chris is the co-founder of Weitz and Beises, which a lot of you might know already.
[00:00:19.000 --> 00:00:23.640]   Before Weitz and Beises, Chris founded Figure Eight and Chris has been doing machine learning
[00:00:23.640 --> 00:00:24.840]   for over 15 years.
[00:00:24.840 --> 00:00:30.200]   So he was doing ML before ML was cool, before it was called deep learning.
[00:00:30.200 --> 00:00:33.840]   I've personally worked with Chris for more than four years and I've learned a lot from
[00:00:33.840 --> 00:00:36.440]   him and I'm super excited to talk to him.
[00:00:36.440 --> 00:00:39.520]   Chris, welcome to the show.
[00:00:39.520 --> 00:00:40.520]   Thanks for having me guys.
[00:00:40.520 --> 00:00:41.880]   Excited to be here.
[00:00:41.880 --> 00:00:43.280]   Thank you.
[00:00:43.280 --> 00:00:48.960]   So Chris, in this show, we're taking a little bit of a different approach from Lucas and
[00:00:48.960 --> 00:00:54.120]   we're trying to understand, like Karen said, the people behind and the stories behind the
[00:00:54.120 --> 00:00:55.120]   product.
[00:00:55.120 --> 00:00:59.720]   So I have a lot of, we all have a lot of friends who are in that state where they are showing
[00:00:59.720 --> 00:01:05.000]   their product to their first few users and it's easy to get lost in like chase down
[00:01:05.000 --> 00:01:09.280]   wrong rabbit holes or like not know what the right first user is.
[00:01:09.280 --> 00:01:13.440]   So like, how do you identify them and like make sure you don't like customize the product
[00:01:13.440 --> 00:01:14.440]   too much for them.
[00:01:14.440 --> 00:01:18.860]   And then how do you keep going with the current idea when you're hearing no, when do you know
[00:01:18.860 --> 00:01:21.200]   that you actually should pivot?
[00:01:21.200 --> 00:01:29.840]   Yeah, I mean, I think early for us, one of the things we did right was to partner with,
[00:01:29.840 --> 00:01:32.200]   you know, people that were actually doing machine learning.
[00:01:32.200 --> 00:01:35.960]   Like we knew when we started the company, we want to help people do machine learning.
[00:01:35.960 --> 00:01:40.800]   That was, that was like very clear.
[00:01:40.800 --> 00:01:44.080]   There's a number of different personas within that, in that space.
[00:01:44.080 --> 00:01:49.000]   And we were fortunate enough through some relationships that we had built at Crowdflower
[00:01:49.000 --> 00:01:53.800]   Figure Eight to be able to like talk to a team like Toyota Research Institute.
[00:01:53.800 --> 00:01:58.080]   And in those discussions, they were very open-ended, right?
[00:01:58.080 --> 00:02:00.420]   We barely had a product.
[00:02:00.420 --> 00:02:05.600]   So we would be going in and kind of talking to them about the kinds of challenges that
[00:02:05.600 --> 00:02:06.600]   they were having.
[00:02:06.600 --> 00:02:12.720]   You know, it was like Luke Shonron would drive down to Silicon Valley, have one of these
[00:02:12.720 --> 00:02:16.360]   meetings and then on our drive back up, we'd be processing it.
[00:02:16.360 --> 00:02:18.840]   And we'd be like, hey, you know, that person said they really wanted that.
[00:02:18.840 --> 00:02:22.560]   Should we maybe, you know, look into building something there?
[00:02:22.560 --> 00:02:30.760]   How can this all tie in to make something useful?
[00:02:30.760 --> 00:02:35.240]   So I mean, I'm already forgetting your original question, Lavanya, but I think the advice
[00:02:35.240 --> 00:02:41.920]   would be, you know, find folks that you enjoy being around.
[00:02:41.920 --> 00:02:47.520]   Like we really enjoyed going down and talking about their issues and what they were working
[00:02:47.520 --> 00:02:51.560]   on and thought what they were doing with like autonomous vehicles and making machines able
[00:02:51.560 --> 00:02:53.360]   to like navigate the real world.
[00:02:53.360 --> 00:02:55.820]   And they had like sick robots, like interior.
[00:02:55.820 --> 00:02:59.440]   It was just a joy to go to that place.
[00:02:59.440 --> 00:03:03.060]   And we got along and because of that, they were more open and sharing more.
[00:03:03.060 --> 00:03:08.420]   And that allowed us to kind of process and decide what was important and what direction
[00:03:08.420 --> 00:03:10.760]   we would take the product in early on.
[00:03:10.760 --> 00:03:15.000]   Without those partners, I think we likely would have made a lot of the wrong things
[00:03:15.000 --> 00:03:16.000]   for sure.
[00:03:16.000 --> 00:03:19.920]   So that's really good advice of find customers that excite you.
[00:03:19.920 --> 00:03:24.080]   And I think that's good advice in general when you're picking co-founders, the people
[00:03:24.080 --> 00:03:29.440]   that like you're working with at the beginning, like if they inspire you and excite you, that's
[00:03:29.440 --> 00:03:32.040]   probably good things are going to happen there.
[00:03:32.040 --> 00:03:33.040]   Yeah.
[00:03:33.040 --> 00:03:40.000]   I mean, I've heard you speak before and you get this real love for ML practitioners.
[00:03:40.000 --> 00:03:46.240]   Like it comes through and it's a joy to like watch and see.
[00:03:46.240 --> 00:03:47.840]   How did you find this love of tools?
[00:03:47.840 --> 00:03:51.560]   So you were like, I want to build tools.
[00:03:51.560 --> 00:03:52.560]   Like why?
[00:03:52.560 --> 00:03:54.560]   What drew that?
[00:03:54.560 --> 00:04:03.040]   Well, like as someone, you know, I mentioned I studied art and computer science in college
[00:04:03.040 --> 00:04:09.320]   and like as an artist, tools are like fundamental, right?
[00:04:09.320 --> 00:04:10.880]   I like need the paper.
[00:04:10.880 --> 00:04:18.320]   I need the paint, the pencils and whatever other interesting tool I might be using in
[00:04:18.320 --> 00:04:21.120]   my art media.
[00:04:21.120 --> 00:04:25.120]   I mean, literally I did a lot of sculpture in college and would use like welders and
[00:04:25.120 --> 00:04:30.560]   saws and what we would think of as like things in a tool shop.
[00:04:30.560 --> 00:04:41.660]   As a developer, tools make me feel like pretty intensely and they can make me feel either
[00:04:41.660 --> 00:04:48.500]   very elated or incredibly frustrated and angry.
[00:04:48.500 --> 00:04:56.700]   We care about our tools a lot as a developer and we spend a lot of time with them.
[00:04:56.700 --> 00:05:01.240]   So I think, you know, between art and the development I've done over the years and
[00:05:01.240 --> 00:05:07.600]   the discovery of like really good tools and the suffering through like really bad tools,
[00:05:07.600 --> 00:05:11.400]   it felt obvious to work on tools.
[00:05:11.400 --> 00:05:15.600]   Yeah, I get that.
[00:05:15.600 --> 00:05:20.420]   It's just interesting because a lot of people just aren't drawn to the thing that's most
[00:05:20.420 --> 00:05:21.420]   useful to others.
[00:05:21.420 --> 00:05:29.880]   I want to go back to something, you are a technical founder, you know, and yet you get
[00:05:29.880 --> 00:05:35.960]   on the airplane and you talk a lot to customers and you sell.
[00:05:35.960 --> 00:05:40.000]   And I know a lot of technical founders are like, I would like to outsource the selling
[00:05:40.000 --> 00:05:41.640]   part.
[00:05:41.640 --> 00:05:44.440]   What do you say and how do you think about that?
[00:05:45.340 --> 00:05:46.660]   Yeah, I mean, I think.
[00:05:46.660 --> 00:05:53.620]   I mean, not everyone's cut out for selling for for sure, and I.
[00:05:53.620 --> 00:06:00.220]   I don't remember a time where I was like, oh, I really need to hone my selling skills.
[00:06:00.220 --> 00:06:01.300]   It came.
[00:06:01.300 --> 00:06:07.660]   You know, somewhat naturally to me, and by no means do I, you know, believe that I'm
[00:06:07.660 --> 00:06:13.580]   I would agree with that, just like a suck your sleeve seller, but it might just be your
[00:06:13.580 --> 00:06:15.980]   curiosity or your interest in the people.
[00:06:15.980 --> 00:06:20.220]   That's sort of what's coming across versus like what they're doing, you know, the people
[00:06:20.220 --> 00:06:21.540]   and like what problem are you solving?
[00:06:21.540 --> 00:06:25.340]   How can I help you? That's like how Chris approaches like these selling conversations.
[00:06:25.340 --> 00:06:29.820]   I guess, you know, I don't know.
[00:06:29.820 --> 00:06:35.580]   Like sometimes I feel pretty introverted and sometimes I feel pretty extroverted, right,
[00:06:35.580 --> 00:06:40.580]   there's not I wouldn't say I'm necessarily, you know, really one or the other, but I do
[00:06:40.580 --> 00:06:48.020]   enjoy group settings and like people laughing and and connection and the feeling of energy
[00:06:48.020 --> 00:06:50.940]   you get when you.
[00:06:50.940 --> 00:06:56.220]   You know, you feel seen, heard and understood like that, that's a that's a thing that I
[00:06:56.220 --> 00:07:02.300]   go after, and I think you definitely need to be wired that way to to go in and sell.
[00:07:02.300 --> 00:07:07.820]   It sounds like you can do both because when you're coding that's in your own and then
[00:07:07.820 --> 00:07:10.060]   you go and you get the energy from other people.
[00:07:10.540 --> 00:07:12.900]   So it sounds like you play in both.
[00:07:12.900 --> 00:07:14.780]   LaVanya, are you an introvert or an extrovert?
[00:07:14.780 --> 00:07:16.500]   I'm going to guess, but I don't know.
[00:07:16.500 --> 00:07:17.660]   What do you what would you say?
[00:07:17.660 --> 00:07:21.740]   I feel like people would think I'm an extrovert, but I am an introvert with a lot of
[00:07:21.740 --> 00:07:22.700]   energy, I think.
[00:07:22.700 --> 00:07:23.420]   What about you?
[00:07:23.420 --> 00:07:30.460]   Probably the same when I was at Facebook, I was going to write an introvert's guide to
[00:07:30.460 --> 00:07:33.740]   an offsite because I would find myself hiding in the bathroom.
[00:07:33.740 --> 00:07:37.340]   But I really do enjoy talking to people.
[00:07:37.340 --> 00:07:39.540]   So I didn't know how to square that.
[00:07:39.540 --> 00:07:43.980]   And Chris, that really resonated with me when you said that, because I like to work with
[00:07:43.980 --> 00:07:49.300]   people and I get energy from it, but then I need a place to go and think and retreat.
[00:07:49.300 --> 00:07:53.020]   So I don't know what we call ourselves, but it sounds like we're all a little similar.
[00:07:53.020 --> 00:07:58.780]   Yeah, I think one thing we all have in common is like if you find your people, then you
[00:07:58.780 --> 00:08:00.140]   can be extroverted all day long.
[00:08:00.140 --> 00:08:03.580]   Right. But like you got to find that energy that Chris was talking about earlier.
[00:08:04.220 --> 00:08:08.380]   This makes me think about Chris, you've talked about this thing called the T group.
[00:08:08.380 --> 00:08:13.100]   So you are watching you as a leader is so amazing because you are so empathetic.
[00:08:13.100 --> 00:08:17.580]   Like if there's anyone in the company who's going through stuff, you really feel for them
[00:08:17.580 --> 00:08:21.860]   and you really lead when people are going through a tough time from your heart.
[00:08:21.860 --> 00:08:24.700]   How much of that comes from the T group?
[00:08:24.700 --> 00:08:27.780]   And can you tell us about how that shaped your leadership style in general?
[00:08:30.380 --> 00:08:36.580]   Yeah, well, maybe we should level set and explain to listeners what or attempt to explain
[00:08:36.580 --> 00:08:37.580]   what T group is.
[00:08:37.580 --> 00:08:38.940]   Yeah, I don't know what it is.
[00:08:38.940 --> 00:08:40.900]   Yeah, it's new to me.
[00:08:40.900 --> 00:08:44.580]   Well, I'm actually I'm going to a T group tomorrow.
[00:08:44.580 --> 00:08:49.660]   I'm really excited because this will be my third T group.
[00:08:49.660 --> 00:08:54.740]   T groups, it's a difficult thing to describe.
[00:08:54.740 --> 00:09:01.500]   It's much better experience, but essentially like 10 random startup leaders are going to
[00:09:01.500 --> 00:09:06.260]   go on a little field trip Thursday and Friday this week.
[00:09:06.260 --> 00:09:16.780]   Our kind of guide and moderator, a woman named Anna Marie is going to be hosting the T group
[00:09:16.780 --> 00:09:21.780]   itself. There's a number of kind of it's I guess it's a little bit like it's not like
[00:09:21.780 --> 00:09:22.700]   Fight Club at all.
[00:09:22.700 --> 00:09:24.260]   There are rules about T group.
[00:09:24.260 --> 00:09:26.180]   And are you allowed to talk about it?
[00:09:26.180 --> 00:09:28.260]   Isn't the rule with Fight Club you can't talk about it?
[00:09:28.260 --> 00:09:30.140]   Yeah, no, that's not one of the rules of T group.
[00:09:30.140 --> 00:09:31.420]   OK, OK.
[00:09:31.420 --> 00:09:36.380]   But yeah, we try to stay in in like the moment in the here and now.
[00:09:36.380 --> 00:09:40.780]   So it's not like a group help session where we're talking about all the trials and
[00:09:40.780 --> 00:09:43.820]   tribulations that happened in the past or the things we're worried about in the future.
[00:09:43.820 --> 00:09:47.300]   It's really about like what's happening in this group.
[00:09:47.300 --> 00:09:48.980]   And we're like in a circle.
[00:09:49.140 --> 00:09:53.300]   So we're all like looking at each other and we spend like the entire day.
[00:09:53.300 --> 00:09:57.180]   How many just kind of in an open discussion, about 10, 10 folks.
[00:09:57.180 --> 00:09:59.100]   Why do you do it?
[00:09:59.100 --> 00:10:07.660]   So we learn a lot about how we're perceived by others in a in a leadership context, in a
[00:10:07.660 --> 00:10:08.820]   group context.
[00:10:08.820 --> 00:10:12.060]   We're also encouraged to kind of come into it with some goals.
[00:10:12.060 --> 00:10:18.900]   So maybe I want to participate more or I want to be able to receive feedback better or I
[00:10:18.900 --> 00:10:20.660]   want to be able to give feedback better.
[00:10:20.660 --> 00:10:24.700]   And it can get like intense.
[00:10:24.700 --> 00:10:31.300]   So people will be very frank about how they're feeling.
[00:10:31.300 --> 00:10:34.900]   I mean, we're taught to really sit there and think about what feelings are coming up.
[00:10:34.900 --> 00:10:36.500]   And often those feelings are good.
[00:10:36.500 --> 00:10:40.660]   And we're going to tell the other people in the group that they made us feel that way.
[00:10:40.660 --> 00:10:46.780]   And I don't know, I've come out of it just with this like afterglow of like, wow, it's
[00:10:46.780 --> 00:10:51.340]   almost like a meditation retreat or something, because you're sitting there in a pretty
[00:10:51.340 --> 00:10:52.340]   vulnerable place.
[00:10:52.340 --> 00:10:57.540]   And I've grown a lot because I was so cheap to as a leader.
[00:10:57.540 --> 00:11:04.060]   Well, I mean, I think the big one was really prior.
[00:11:04.060 --> 00:11:06.860]   I mean, I've always.
[00:11:06.860 --> 00:11:14.100]   Yeah, I don't know, but prior to T Group, I think in my mind, feelings were not important,
[00:11:14.100 --> 00:11:15.660]   especially in a work context.
[00:11:16.220 --> 00:11:23.900]   Feelings got in the way they were a nuisance and they would prevent me from being the
[00:11:23.900 --> 00:11:26.940]   like ice cold, logical executor that I needed to be.
[00:11:26.940 --> 00:11:33.780]   And going to T Group made me realize like, oh, wait, like these feelings are like
[00:11:33.780 --> 00:11:40.220]   critical to to how I relate to others in the company, to how I can lead and to how I'm
[00:11:40.220 --> 00:11:46.620]   able to know where my boundaries are, what what's like good for me in any given, you
[00:11:46.620 --> 00:11:48.100]   know, often very stressful.
[00:11:48.100 --> 00:11:55.420]   And it would put it in the icky feeling bucket that I don't let myself feel that.
[00:11:55.420 --> 00:11:58.620]   And then, you know, that's a recipe for stuckness.
[00:11:58.620 --> 00:12:05.420]   When you start, you're pretty much writing code 100 percent of your time and over time,
[00:12:05.700 --> 00:12:10.900]   like the percentage of time of what you spend it on between like talking to customers or
[00:12:10.900 --> 00:12:15.340]   internally or do you have a rough breakdown of today?
[00:12:15.340 --> 00:12:19.980]   I know it's going to change, but today, percentage of your time spent in different
[00:12:19.980 --> 00:12:20.540]   areas?
[00:12:20.540 --> 00:12:23.480]   Well.
[00:12:23.480 --> 00:12:29.220]   It really fluctuates, but I guess my my like ideal.
[00:12:31.820 --> 00:12:37.740]   Time partitioning would be like 50 percent of my time I'm I'm writing code in some
[00:12:37.740 --> 00:12:43.740]   capacity, whether that is is code for like a new product feature or to squash a bug or
[00:12:43.740 --> 00:12:46.460]   it's it's code to actually use the product.
[00:12:46.460 --> 00:12:50.940]   It's like doing some modeling or playing with some of the new LLM stuff I've heard.
[00:12:50.940 --> 00:12:52.180]   People are pretty excited about that.
[00:12:52.180 --> 00:12:54.300]   I hear it's.
[00:12:54.300 --> 00:12:54.800]   Yeah.
[00:12:56.900 --> 00:13:00.060]   And then 25 percent of my time will be.
[00:13:00.060 --> 00:13:07.020]   Like customer related things like I just I had a call with a customer this morning.
[00:13:07.020 --> 00:13:13.020]   If I can help, whether it be something like more technical and how things work in the
[00:13:13.020 --> 00:13:19.140]   infrastructure space or just as by bringing like a founder's perspective to a strategic
[00:13:19.140 --> 00:13:22.180]   conversation, I love that and try to do that as much as possible.
[00:13:23.060 --> 00:13:29.140]   And, you know, talks like there's a talk at Ray Summit next week and then going to be
[00:13:29.140 --> 00:13:30.860]   going to DockerCon, doing more of those.
[00:13:30.860 --> 00:13:31.500]   It's always fun.
[00:13:31.500 --> 00:13:37.100]   And then the last 25 percent would just be, you know, the day to day like.
[00:13:37.100 --> 00:13:41.460]   Work that needs to be done, right, it's like more meetings, the thing I miss about
[00:13:41.460 --> 00:13:46.020]   the very like in the very beginning, there's just no meetings, there's zero, right?
[00:13:46.020 --> 00:13:49.300]   That's the thing I miss the most is like no meetings.
[00:13:49.300 --> 00:13:49.800]   Yeah.
[00:13:50.740 --> 00:13:54.820]   But now, you know, at least like 25 percent of my time, it's just meetings, you know,
[00:13:54.820 --> 00:13:57.460]   and I'm trying to bring my best self to those meetings.
[00:13:57.460 --> 00:13:58.980]   You're lucky it's 25 percent.
[00:13:58.980 --> 00:14:00.500]   Some of us.
[00:14:00.500 --> 00:14:02.620]   I know some are very different.
[00:14:02.620 --> 00:14:06.340]   So here we're going to change the pace.
[00:14:06.340 --> 00:14:08.060]   I'm going to do a lightning round.
[00:14:08.060 --> 00:14:09.340]   Lavanya is going to jump into.
[00:14:09.340 --> 00:14:11.860]   So this is the first thing that comes to your head.
[00:14:11.860 --> 00:14:14.380]   Don't overthink it and just go.
[00:14:15.460 --> 00:14:16.100]   First one.
[00:14:16.100 --> 00:14:21.980]   Three adjectives that others would describe you as and you can't say curious
[00:14:21.980 --> 00:14:26.660]   because I've heard curious, so three others besides curious adjectives that
[00:14:26.660 --> 00:14:28.580]   people would describe you as go.
[00:14:28.580 --> 00:14:30.100]   Oh, boy.
[00:14:30.100 --> 00:14:32.420]   Can't use curious.
[00:14:32.420 --> 00:14:33.000]   Nope.
[00:14:33.000 --> 00:14:35.500]   No, I'm trying to find a synonym now.
[00:14:35.500 --> 00:14:36.460]   Curiosity.
[00:14:36.460 --> 00:14:38.340]   OK, you can use curious.
[00:14:38.340 --> 00:14:39.980]   Yeah, no.
[00:14:39.980 --> 00:14:43.620]   OK, nerdy.
[00:14:44.340 --> 00:14:46.100]   Weird.
[00:14:46.100 --> 00:14:48.980]   Yeah, actually, the time you said that.
[00:14:48.980 --> 00:14:50.540]   Nice.
[00:14:50.540 --> 00:14:51.040]   Good.
[00:14:51.040 --> 00:14:51.900]   I love weird.
[00:14:51.900 --> 00:14:54.340]   Weird is a good way.
[00:14:54.340 --> 00:14:55.280]   Yeah.
[00:14:55.280 --> 00:14:55.900]   Good. Weird.
[00:14:55.900 --> 00:14:56.380]   Good. Weird.
[00:14:56.380 --> 00:14:57.200]   Not bad.
[00:14:57.200 --> 00:15:01.460]   Chris's introduction to the company is called Weird Stuff at CVP.
[00:15:01.460 --> 00:15:02.420]   That's a one on one.
[00:15:02.420 --> 00:15:07.900]   And I asked other people this and so far, curious and weird both came up.
[00:15:07.900 --> 00:15:10.380]   So I'm I'm curious if you have a third.
[00:15:10.380 --> 00:15:12.780]   Maybe silly.
[00:15:13.100 --> 00:15:13.780]   Silly.
[00:15:13.780 --> 00:15:15.660]   I heard humble a bunch from other people.
[00:15:15.660 --> 00:15:17.180]   We we like ask.
[00:15:17.180 --> 00:15:17.700]   Oh, you can't.
[00:15:17.700 --> 00:15:18.220]   You can't.
[00:15:18.220 --> 00:15:20.020]   I know you can't say you're humble.
[00:15:20.020 --> 00:15:20.940]   That's like totally.
[00:15:20.940 --> 00:15:22.700]   I know that's a total humble.
[00:15:22.700 --> 00:15:27.420]   OK, so inside voice when you're.
[00:15:27.420 --> 00:15:32.380]   Maybe even now, what's in your inside voice that you like, tell yourself not
[00:15:32.380 --> 00:15:34.100]   to say out loud, don't say it out loud.
[00:15:34.100 --> 00:15:35.020]   What is that?
[00:15:35.020 --> 00:15:39.780]   Well, honestly, it's like it's usually very little.
[00:15:39.780 --> 00:15:42.180]   I tend to be like an open book.
[00:15:42.180 --> 00:15:44.180]   I've got I've gotten in trouble for saying.
[00:15:44.180 --> 00:15:47.740]   More than I more than I should.
[00:15:47.740 --> 00:15:49.860]   Hence the weird thing that people call you.
[00:15:49.860 --> 00:15:52.020]   Yeah, maybe that's a part of the weird thing.
[00:15:52.020 --> 00:15:54.500]   It's because your inside voice is your outside voice.
[00:15:54.500 --> 00:15:55.460]   Yeah, yeah.
[00:15:55.460 --> 00:15:58.020]   I mean, I am like a little.
[00:15:58.020 --> 00:15:58.820]   Go ahead.
[00:15:58.820 --> 00:16:01.220]   I was just going to say Chris is very genuine.
[00:16:01.220 --> 00:16:02.580]   It's not like in a bad way.
[00:16:02.580 --> 00:16:06.940]   Chris is like so himself at all times that whatever he thinks comes out.
[00:16:06.940 --> 00:16:08.420]   Go ahead.
[00:16:08.420 --> 00:16:10.860]   Thanks, Lavanya.
[00:16:11.460 --> 00:16:15.980]   OK, you know, I will say that I am fairly conflict diverse.
[00:16:15.980 --> 00:16:20.420]   Like if there's something that's like annoying me, I'll just kind of like
[00:16:20.420 --> 00:16:22.060]   grit my teeth and push through it.
[00:16:22.060 --> 00:16:25.380]   I'd rather just like deal with it than confront it.
[00:16:25.380 --> 00:16:31.340]   But Tea Group's teaching me to actually confront in the appropriate situation.
[00:16:31.340 --> 00:16:31.860]   So.
[00:16:31.860 --> 00:16:34.580]   You're doing a great job at it.
[00:16:34.580 --> 00:16:36.420]   I've seen you improve over the years.
[00:16:36.420 --> 00:16:38.340]   Next question.
[00:16:38.340 --> 00:16:39.740]   What's the first thing you ever built?
[00:16:39.980 --> 00:16:41.500]   Ronthebustnut.com.
[00:16:41.500 --> 00:16:42.020]   That's right.
[00:16:42.020 --> 00:16:47.180]   Specifically, this was in an era where flash introductions were a big deal.
[00:16:47.180 --> 00:16:52.060]   Like when you go to the website, you need like all of the moving stuff and the sound.
[00:16:52.060 --> 00:16:55.420]   I made a sick flash animation with like a bus.
[00:16:55.420 --> 00:17:00.420]   And I remember I used like the prodigy was my background music, which my dad
[00:17:00.420 --> 00:17:04.140]   didn't really like the prodigy, but too bad, you know, it's my website intro
[00:17:04.140 --> 00:17:05.420]   for Ronthebustnut.com.
[00:17:05.420 --> 00:17:05.980]   Yeah.
[00:17:05.980 --> 00:17:07.300]   I think you're next.
[00:17:07.340 --> 00:17:11.380]   In your next meeting at Weights and Biases, you guys need to, you know,
[00:17:11.380 --> 00:17:12.780]   we need a revival of this.
[00:17:12.780 --> 00:17:13.540]   Hell yeah.
[00:17:13.540 --> 00:17:15.420]   I've looked for it.
[00:17:15.420 --> 00:17:19.060]   It's the Wayback Machine has some, some early Ronthebustnut.com, but I
[00:17:19.060 --> 00:17:20.660]   haven't found the original flash.
[00:17:20.660 --> 00:17:22.380]   I need to do some more digging.
[00:17:22.380 --> 00:17:25.740]   If you could hack with someone dead or alive, who would you hack
[00:17:25.740 --> 00:17:27.020]   with and what would you hack on?
[00:17:27.020 --> 00:17:30.940]   Ooh, hack.
[00:17:30.940 --> 00:17:32.580]   Um.
[00:17:35.100 --> 00:17:40.020]   I probably Yukihiro Matsumoto, the author of Ruby.
[00:17:40.020 --> 00:17:48.020]   I haven't done, I haven't done like Ruby in a long time, but I love, uh,
[00:17:48.020 --> 00:17:51.620]   Japan and Japanese culture and Ruby is what got me into this.
[00:17:51.620 --> 00:17:57.820]   So like the thought of sitting down, like writing code with the, um, kind
[00:17:57.820 --> 00:18:01.020]   of author of such a impactful tool.
[00:18:01.700 --> 00:18:02.220]   Is exciting.
[00:18:02.220 --> 00:18:03.540]   I don't know what we, what we hack on.
[00:18:03.540 --> 00:18:04.220]   Something's sick.
[00:18:04.220 --> 00:18:04.900]   We figured it out.
[00:18:04.900 --> 00:18:05.940]   That doesn't matter.
[00:18:05.940 --> 00:18:10.180]   Chris, Chris is known for building insane things in a day.
[00:18:10.180 --> 00:18:12.380]   You just give him a day and he just goes off.
[00:18:12.380 --> 00:18:14.500]   And that's the, okay.
[00:18:14.500 --> 00:18:16.740]   What's the biggest mistake work-wise.
[00:18:16.740 --> 00:18:19.420]   Don't need to dive into your personal life too much that you've
[00:18:19.420 --> 00:18:20.900]   made in the last two years.
[00:18:20.900 --> 00:18:24.820]   Oh, two years, three years.
[00:18:24.820 --> 00:18:27.020]   Well, okay.
[00:18:27.020 --> 00:18:28.580]   I'll just, this isn't quite the same thing.
[00:18:29.620 --> 00:18:29.980]   Well, okay.
[00:18:29.980 --> 00:18:31.420]   I'll just, this isn't quite three years.
[00:18:31.420 --> 00:18:34.180]   This is probably like a little older, but, um, the team
[00:18:34.180 --> 00:18:35.500]   is like still griping about it.
[00:18:35.500 --> 00:18:41.100]   Uh, when you start a company, you need to like choose all the technologies
[00:18:41.100 --> 00:18:44.140]   and you know that you're going to be like stuck with these mostly.
[00:18:44.140 --> 00:18:50.220]   Um, and like one of the decisions I felt I was forced into, we, we use MySQL
[00:18:50.220 --> 00:18:54.500]   as the like data store for, for everything, because at the time we were
[00:18:54.500 --> 00:18:57.820]   on Google cloud and all they had was a managed MySQL offering, they didn't
[00:18:57.820 --> 00:19:05.460]   have managed Postgres, um, so yeah, the, the team is still, they would much
[00:19:05.460 --> 00:19:14.700]   rather be, uh, on team Postgres, but alas, uh, oh, yeah, for sure.
[00:19:14.700 --> 00:19:19.060]   Um, and, uh, I chose a UI framework.
[00:19:19.060 --> 00:19:20.740]   It's called like semantic UI.
[00:19:20.740 --> 00:19:23.700]   I probably, this, this is a failure mode of mine.
[00:19:23.700 --> 00:19:26.940]   I should have just gone like material UI or something like a little more mainstream.
[00:19:27.340 --> 00:19:29.500]   Semantic UI isn't really being updated anymore.
[00:19:29.500 --> 00:19:30.980]   Now it's like all over the code base.
[00:19:30.980 --> 00:19:33.740]   So the team's like creating a new UI framework, but it's this like long
[00:19:33.740 --> 00:19:35.420]   painful process to move things over.
[00:19:35.420 --> 00:19:37.820]   Like once that stuff's in there, it's really hard to rip out.
[00:19:37.820 --> 00:19:40.220]   Um, so yeah.
[00:19:40.220 --> 00:19:44.460]   Uh, what do, what do you think people get wrong about Weights and Biases?
[00:19:44.460 --> 00:19:52.500]   Uh, well, I think a lot of the market sees us as experiment tracking.
[00:19:52.540 --> 00:19:57.700]   Like all we do is just track experiments and, uh, you know, we've grown now.
[00:19:57.700 --> 00:19:58.820]   We're a company of 200 people.
[00:19:58.820 --> 00:20:01.820]   We have lots of engineers working on, um, lots of different
[00:20:01.820 --> 00:20:04.780]   aspects of the, the ML Ops pipeline.
[00:20:04.780 --> 00:20:11.660]   Uh, and especially now with, with the advent of, of Gen AI and, and, um, the
[00:20:11.660 --> 00:20:15.580]   expansion of our targetable persona base, like it used to be like, okay.
[00:20:15.580 --> 00:20:20.940]   ML researcher builds model understands, uh, back prop and, and all of the, you
[00:20:20.940 --> 00:20:22.580]   know, underpinnings of deep learning.
[00:20:22.580 --> 00:20:26.180]   Now we have just regular software developers that are able to leverage,
[00:20:26.180 --> 00:20:30.660]   um, AI and still need to care about the same things like evaluating how well
[00:20:30.660 --> 00:20:34.220]   the model is doing and monitoring that in, in production settings.
[00:20:34.220 --> 00:20:39.980]   Um, yeah, we're, we're working on lots of, uh, really powerful tooling
[00:20:39.980 --> 00:20:41.220]   and all of those different aspects.
[00:20:41.220 --> 00:20:45.300]   And, and, uh, I think a lot of times the market doesn't realize that we can be
[00:20:45.300 --> 00:20:48.220]   used beyond just, uh, tracking of experiments.
[00:20:48.220 --> 00:20:49.020]   Yeah.
[00:20:49.660 --> 00:20:54.940]   Question, something that we talked about, how do you know if you're in a cave or a
[00:20:54.940 --> 00:21:01.900]   tunnel and a tunnel is a hard time and a cave is a dead end and a real change needs
[00:21:01.900 --> 00:21:04.980]   to be made, I think of a tunnel is like, you need to be patient.
[00:21:04.980 --> 00:21:05.900]   It's a hard time.
[00:21:05.900 --> 00:21:08.940]   A cave is like going to have to figure out something else.
[00:21:08.940 --> 00:21:10.820]   It's a mental model that I have for myself.
[00:21:10.820 --> 00:21:13.820]   Like when I'm in a tough time, I'm like caver tunnel caver tunnel.
[00:21:13.820 --> 00:21:15.940]   Yeah, totally.
[00:21:16.220 --> 00:21:22.540]   Um, I mean, I immediately like think visually and I'm like, okay, I'm looking
[00:21:22.540 --> 00:21:27.780]   for like light, I'm looking for like a pinhole somewhere in the distance.
[00:21:27.780 --> 00:21:32.020]   Even, even if it's like, if it's not clear, that's actually way out.
[00:21:32.020 --> 00:21:34.140]   Maybe it's just like a little crack in the cave.
[00:21:34.140 --> 00:21:40.140]   And it's, it's, you know, um, if I have something I'm clearly marching towards,
[00:21:40.140 --> 00:21:43.980]   then I feel like, you know, I'm, I'm more in this, uh, tunnel situation.
[00:21:43.980 --> 00:21:49.780]   But so many times as a programmer, you have this where you're, you think you
[00:21:49.780 --> 00:21:54.260]   have this great idea and you go down this like rabbit hole and then you get a little
[00:21:54.260 --> 00:21:57.780]   sign that's like, man, this seems like maybe a cave you should maybe turn around.
[00:21:57.780 --> 00:21:59.700]   But you're like, oh, but I came this far.
[00:21:59.700 --> 00:22:00.740]   I want to keep going.
[00:22:00.740 --> 00:22:05.540]   Uh, yeah, I put all this time.
[00:22:05.540 --> 00:22:08.740]   I've learned, like, it can actually feel really good to throw away.
[00:22:09.540 --> 00:22:14.740]   Like days of, of cave walking, um, if you will.
[00:22:14.740 --> 00:22:17.300]   It's that's, that's a unique skill.
[00:22:17.300 --> 00:22:21.020]   I mean, to, to be like, all right, you know, let's go, let's go try another
[00:22:21.020 --> 00:22:23.380]   cave because it's, it's hard to know.
[00:22:23.380 --> 00:22:27.740]   And it's really, it reminds me of like this fallacy of one thing.
[00:22:27.740 --> 00:22:31.820]   And when you start a company and I've heard you talk about this, like, if you
[00:22:31.820 --> 00:22:37.300]   can just do one thing, if you can get your first customer, if you can get one
[00:22:37.300 --> 00:22:41.060]   more feature, if you can fix this bug, like as a founder, you're constantly
[00:22:41.060 --> 00:22:45.180]   thinking like this one thing will change everything when you guys did have open
[00:22:45.180 --> 00:22:49.580]   AI as your first customer, so you could argue that one thing was pretty meaningful.
[00:22:49.580 --> 00:22:54.420]   But did you have this one thing idea frame and how do you think about it now?
[00:22:54.420 --> 00:23:02.780]   Uh, yeah, I mean, it was always like, uh, how do we get people to
[00:23:02.780 --> 00:23:05.180]   use what we're, what we're making?
[00:23:05.180 --> 00:23:06.020]   Right.
[00:23:06.020 --> 00:23:12.980]   So, uh, if there was one thing, it was like, get something into the hands of,
[00:23:12.980 --> 00:23:17.340]   of someone and, and try to help them be successful.
[00:23:17.340 --> 00:23:24.340]   And, you know, we were fortunate to have a partner in open AI that, that did use
[00:23:24.340 --> 00:23:27.420]   this early on, but that also wasn't easy or clear, like I remember we went to open
[00:23:27.420 --> 00:23:31.140]   AI and presented to a lot of the researchers.
[00:23:31.140 --> 00:23:34.260]   I remember I was doing the demo, which, you know, I give great demos, right?
[00:23:34.260 --> 00:23:35.580]   Well, I didn't feel like it.
[00:23:35.580 --> 00:23:38.940]   Like they were all on their computers, just looking down.
[00:23:38.940 --> 00:23:40.500]   It felt like so deflating.
[00:23:40.500 --> 00:23:43.380]   This is a visual that really every founder needs to see.
[00:23:43.380 --> 00:23:45.220]   Oh yeah, totally.
[00:23:45.220 --> 00:23:48.660]   Just because when you're giving a speech or a demo, all you're really going to see is like.
[00:23:48.660 --> 00:23:50.460]   Yeah, it's rough.
[00:23:50.460 --> 00:23:51.820]   You got to make your own energy.
[00:23:51.820 --> 00:23:53.380]   So that happens.
[00:23:53.380 --> 00:23:56.300]   You come out of that and you come out of those meetings and you just think like,
[00:23:56.300 --> 00:23:58.180]   phew, that's rough.
[00:23:58.180 --> 00:24:00.300]   And you feel like we missed an opportunity.
[00:24:00.300 --> 00:24:02.460]   Like a part of that must've been my fault.
[00:24:02.460 --> 00:24:06.380]   Like I wasn't engaging enough to, to like pull them away from their, from their
[00:24:06.380 --> 00:24:07.100]   laptops.
[00:24:07.100 --> 00:24:14.340]   But, you know, it's a lot of, of small like baby steps and everything we do can
[00:24:14.340 --> 00:24:17.580]   potentially help some, you know, future big milestone.
[00:24:17.580 --> 00:24:21.500]   And, you know, ultimately because we had been talking to them and they were aware
[00:24:21.500 --> 00:24:24.500]   of us when they had a problem, they thought, Hey, maybe weights and biases
[00:24:24.500 --> 00:24:25.820]   could help us with this problem.
[00:24:25.820 --> 00:24:28.260]   And we jumped at the, at the opportunity.
[00:24:28.260 --> 00:24:30.820]   But yeah.
[00:24:31.620 --> 00:24:36.220]   You know, this phase that you're talking about, sometimes it can last only six
[00:24:36.220 --> 00:24:36.460]   months.
[00:24:36.460 --> 00:24:38.100]   Sometimes it can last 14 years.
[00:24:38.100 --> 00:24:41.700]   Like with crowdsourcing, how do you keep going?
[00:24:41.700 --> 00:24:45.780]   And like, when do you know it's time to, like Karen said, like, is it a cave or
[00:24:45.780 --> 00:24:46.500]   like a tunnel?
[00:24:46.500 --> 00:24:50.020]   But like also even if you know it's a tunnel, but it's a long tunnel, how do
[00:24:50.020 --> 00:24:50.580]   you keep going?
[00:24:50.580 --> 00:24:54.620]   And I think you're also really good at taking the rest of us and like lifting
[00:24:54.620 --> 00:24:57.620]   our spirits and like making sure we're going along and optimistic with you.
[00:24:59.860 --> 00:25:04.340]   Well, I mean, you, you find the things that, that are working because oftentimes,
[00:25:04.340 --> 00:25:09.700]   you know, a lot of things aren't working, but as long as you've got some things
[00:25:09.700 --> 00:25:12.460]   that are working, just remind yourself of those.
[00:25:12.460 --> 00:25:14.260]   That's important.
[00:25:14.260 --> 00:25:14.940]   Yeah.
[00:25:14.940 --> 00:25:20.420]   You know, even this week, like I was, I was talking to the sales folks and they
[00:25:20.420 --> 00:25:23.260]   were telling me about some conversations they've been having and it's like great
[00:25:23.260 --> 00:25:23.660]   news.
[00:25:23.660 --> 00:25:26.380]   It's like really exciting stuff that's happening.
[00:25:26.380 --> 00:25:29.740]   And sometimes when you're not in a lot of those sales meetings or you're grinding
[00:25:29.740 --> 00:25:34.260]   away on some, some feature that's now like late and you feel like it has to be
[00:25:34.260 --> 00:25:35.020]   out yesterday.
[00:25:35.020 --> 00:25:37.020]   And if we don't get it out, it's going to be really, really bad.
[00:25:37.020 --> 00:25:40.940]   You remind, okay, well, these things, these things are working.
[00:25:40.940 --> 00:25:41.660]   It's going to be okay.
[00:25:41.660 --> 00:25:42.700]   Let's keep pushing.
[00:25:42.700 --> 00:25:47.260]   The other thing, you know, one of our early like seed investors, this is such a
[00:25:47.260 --> 00:25:48.740]   cliche, but it's true.
[00:25:48.740 --> 00:25:51.660]   It was like, you know, work smarter, not harder.
[00:25:51.660 --> 00:25:57.140]   Like at some point there's so much stuff that you have to get done, but you're not
[00:25:57.140 --> 00:25:58.060]   going to be able to do it all.
[00:25:58.060 --> 00:26:01.620]   So step back and think like, well, what really matters or what's the thing that
[00:26:01.620 --> 00:26:07.060]   could matter the most with the most reasonable amount of effort in the, in
[00:26:07.060 --> 00:26:07.740]   the near term.
[00:26:07.740 --> 00:26:13.220]   And just, you know, take a, take a step back and anytime you're starting to feel
[00:26:13.220 --> 00:26:17.980]   like really overwhelmed and like down in the dumps, you can take a step back and
[00:26:17.980 --> 00:26:19.780]   march towards a near term goal.
[00:26:19.780 --> 00:26:20.740]   Crowflower, right.
[00:26:20.740 --> 00:26:26.260]   We'd have periods of growth and then periods of like flatlining or even, you
[00:26:26.260 --> 00:26:30.700]   know, like losing revenue during periods in the, in the company.
[00:26:30.700 --> 00:26:33.700]   And it's at the end of the day, it's like, or we're trying to get this like
[00:26:33.700 --> 00:26:34.820]   revenue number to go up.
[00:26:34.820 --> 00:26:41.060]   So, you know, at a long enough timescale, it might look a little sad, but right now
[00:26:41.060 --> 00:26:43.220]   I have like this month, can I make it go up this month?
[00:26:43.220 --> 00:26:45.060]   Can I make it go up like next month?
[00:26:45.060 --> 00:26:48.340]   You know, let's, let's just like stay in the here and now and keep pushing forward.
[00:26:48.340 --> 00:26:54.260]   I know it's cliche, but it's hard to be like, all right, am I, how do you not
[00:26:54.260 --> 00:26:56.220]   mistake motion for progress?
[00:26:56.660 --> 00:27:01.540]   Cause you can be really busy, but the progress isn't either happening or
[00:27:01.540 --> 00:27:05.460]   you're, you're doing this march and it actually isn't the thing that matters
[00:27:05.460 --> 00:27:05.940]   anymore.
[00:27:05.940 --> 00:27:08.300]   We're all in ML.
[00:27:08.300 --> 00:27:16.180]   It's changing so bleeping fast that getting the outside input with like the
[00:27:16.180 --> 00:27:21.100]   relentless march, how do you check yourself that you're, you know, not just
[00:27:21.100 --> 00:27:24.540]   motion, but progress and that, you know, how do you know when you need to change?
[00:27:26.140 --> 00:27:30.740]   Yeah, well, I mean, I think Lucas has really put good systems in place at
[00:27:30.740 --> 00:27:34.900]   Weights and Biases around asking those questions.
[00:27:34.900 --> 00:27:40.980]   So you know, every two weeks we sit down and say, okay, what are, what are like
[00:27:40.980 --> 00:27:44.660]   top two to three goals for these two weeks?
[00:27:44.660 --> 00:27:48.980]   And those goals are then shared with the company.
[00:27:49.060 --> 00:27:56.700]   So you're, you're kind of, you know, claiming, all right, this is what I think
[00:27:56.700 --> 00:27:57.860]   is, is most important.
[00:27:57.860 --> 00:28:04.500]   And maybe that changes after like a week and that's, that's fine.
[00:28:04.500 --> 00:28:08.300]   But at least we're, we're having like a, you're not, hopefully you're not getting
[00:28:08.300 --> 00:28:11.780]   stuck in this, like, all right, now you're just, you're working on like all the wrong
[00:28:11.780 --> 00:28:12.900]   things or it doesn't matter.
[00:28:12.900 --> 00:28:13.700]   You're not correcting.
[00:28:13.700 --> 00:28:13.940]   Right.
[00:28:13.940 --> 00:28:17.140]   Because we have an actual like process built in here.
[00:28:17.660 --> 00:28:22.340]   We roll that up into, you know, quarterly goals so that hopefully these things can
[00:28:22.340 --> 00:28:25.620]   kind of fit into the, the bigger picture.
[00:28:25.620 --> 00:28:32.220]   But I also think the, the other important thing is that your peers, right.
[00:28:32.220 --> 00:28:37.900]   Hopefully we made a culture where we can actually like share disagreement in a, in
[00:28:37.900 --> 00:28:39.620]   a way that people can, can hear.
[00:28:39.620 --> 00:28:44.540]   Like if, if I think some team's working on the wrong thing, we'll, we'll share that
[00:28:44.540 --> 00:28:48.020]   and have a discussion and decide if, if that, that is the case or not.
[00:28:48.020 --> 00:28:52.860]   And having like a good leadership team that feels safe enough to, to call each
[00:28:52.860 --> 00:28:56.780]   other on, you know, what, what the various initiatives are ensures everyone's kind of
[00:28:56.780 --> 00:29:00.660]   held accountable and that hopefully we're marching towards goals that are actually
[00:29:00.660 --> 00:29:01.820]   going to make a difference.
[00:29:01.820 --> 00:29:04.620]   You are known as Tech Jesus.
[00:29:04.620 --> 00:29:05.780]   Will you tell us why?
[00:29:05.780 --> 00:29:06.100]   What?
[00:29:06.100 --> 00:29:07.740]   You're known as what?
[00:29:07.740 --> 00:29:09.220]   Tech Jesus.
[00:29:09.220 --> 00:29:10.860]   I'm not known.
[00:29:10.860 --> 00:29:11.900]   I'm not known.
[00:29:11.900 --> 00:29:12.980]   I didn't know that.
[00:29:12.980 --> 00:29:13.740]   As Tech Jesus.
[00:29:13.940 --> 00:29:15.020]   Who calls him that?
[00:29:15.020 --> 00:29:16.860]   Does he, did you even know somebody?
[00:29:16.860 --> 00:29:17.740]   Really?
[00:29:17.740 --> 00:29:18.540]   Yeah.
[00:29:18.540 --> 00:29:18.780]   Okay.
[00:29:18.780 --> 00:29:20.580]   I didn't know that, but I can remember.
[00:29:20.580 --> 00:29:22.900]   It's the hair and the tech.
[00:29:22.900 --> 00:29:24.420]   It's the hair and the tech.
[00:29:24.420 --> 00:29:24.980]   I don't even know.
[00:29:24.980 --> 00:29:25.500]   Yeah.
[00:29:25.500 --> 00:29:26.380]   Hair and tech.
[00:29:26.380 --> 00:29:28.340]   That's definitely a part of it.
[00:29:28.340 --> 00:29:28.860]   Wait, there's a whole story.
[00:29:28.860 --> 00:29:29.420]   Tell us the story.
[00:29:29.420 --> 00:29:30.900]   Oh, there's a story?
[00:29:30.900 --> 00:29:32.620]   It's not, I mean, okay.
[00:29:32.620 --> 00:29:37.860]   At our previous company, we were working with the department of defense.
[00:29:37.980 --> 00:29:45.420]   So we had a meeting in the Pentagon, which is a fascinating place to go
[00:29:45.420 --> 00:29:50.260]   to and a little intimidating, but we had to get rid of all of our phones.
[00:29:50.260 --> 00:29:53.740]   And I thought it was cool.
[00:29:53.740 --> 00:29:55.380]   I was like, I felt honored to be there.
[00:29:55.380 --> 00:29:56.780]   Did they do a background check on you before you got going?
[00:29:56.780 --> 00:29:57.300]   Yeah.
[00:29:57.300 --> 00:29:58.580]   I think they didn't find anything.
[00:29:58.580 --> 00:29:59.140]   Thank God.
[00:29:59.140 --> 00:30:00.940]   Just saying, like, I think they really had to.
[00:30:00.940 --> 00:30:01.300]   Okay.
[00:30:01.300 --> 00:30:02.260]   Okay.
[00:30:02.260 --> 00:30:04.380]   So you had to get rid of your phones.
[00:30:04.420 --> 00:30:08.220]   You probably had to go through a bunch of screens.
[00:30:08.220 --> 00:30:11.780]   We had, you know, meeting, I'm like meeting with the generals and I'm,
[00:30:11.780 --> 00:30:12.820]   you know, I look like this.
[00:30:12.820 --> 00:30:15.980]   I didn't like really clean up that much, you know?
[00:30:15.980 --> 00:30:22.100]   So I think that was a bit jarring, but I guess my charisma and my
[00:30:22.100 --> 00:30:30.620]   savior-like, just like aura, you know, had them calling me Tech Jesus.
[00:30:30.620 --> 00:30:33.740]   Also Christopher, my name, it means Christ bearer.
[00:30:34.140 --> 00:30:34.780]   Okay.
[00:30:34.780 --> 00:30:38.380]   So the people at the Pentagon called you Christ Jesus?
[00:30:38.380 --> 00:30:39.980]   Yeah.
[00:30:39.980 --> 00:30:43.300]   The, I mean, I think maybe some of the lackeys, I'm not, I'm not sure the
[00:30:43.300 --> 00:30:45.060]   generals did, but yeah.
[00:30:45.060 --> 00:30:46.020]   Were they in uniform?
[00:30:46.020 --> 00:30:48.460]   Do people in the, I've never been in the Pentagon.
[00:30:48.460 --> 00:30:52.340]   When you walk into the Pentagon, are people in uniform or are they?
[00:30:52.340 --> 00:30:52.460]   Yeah.
[00:30:52.460 --> 00:30:53.540]   It's a, yeah, definitely.
[00:30:53.540 --> 00:30:54.100]   Really?
[00:30:54.100 --> 00:30:55.220]   It's a little, it's intense.
[00:30:55.220 --> 00:30:55.420]   Yeah.
[00:30:55.420 --> 00:30:55.940]   Interesting.
[00:30:55.940 --> 00:30:56.820]   Wow.
[00:30:56.820 --> 00:30:58.340]   They're all wearing their hats and everything too.
[00:30:58.340 --> 00:31:03.500]   I just, you don't really, you don't really run into people wearing hats that much.
[00:31:04.100 --> 00:31:05.100]   Unless you're at a baseball game.
[00:31:05.100 --> 00:31:06.820]   Then otherwise it just doesn't happen.
[00:31:06.820 --> 00:31:08.060]   I know.
[00:31:08.060 --> 00:31:14.620]   Or like a horse race, like in Southern California, Del Mar, the big hats, the
[00:31:14.620 --> 00:31:15.980]   fancy hats, the derby hats.
[00:31:15.980 --> 00:31:18.140]   Do you go to horse races in Southern California?
[00:31:18.140 --> 00:31:18.700]   No.
[00:31:18.700 --> 00:31:19.860]   I mean, I've been to one.
[00:31:19.860 --> 00:31:21.140]   I've been to one before.
[00:31:21.140 --> 00:31:21.300]   Yeah.
[00:31:21.300 --> 00:31:21.700]   It was neat.
[00:31:21.700 --> 00:31:22.340]   We gotta go.
[00:31:22.340 --> 00:31:23.340]   I'm not a regular.
[00:31:23.340 --> 00:31:23.540]   Yeah.
[00:31:23.540 --> 00:31:25.540]   I wore a really nice hat.
[00:31:25.540 --> 00:31:30.740]   What's the weirdest place you've ever done a customer meeting besides the Pentagon?
[00:31:31.220 --> 00:31:35.060]   It's not like it's sort of like, where's the weirdest place you've ever done a
[00:31:35.060 --> 00:31:35.860]   customer meeting?
[00:31:35.860 --> 00:31:39.060]   It's like that would be the question you were expecting.
[00:31:39.060 --> 00:31:42.860]   Uh, okay.
[00:31:42.860 --> 00:31:45.260]   This wasn't really a customer meeting, but I liked the story.
[00:31:45.260 --> 00:31:50.860]   We were driving back from, uh, it was Lucas and I driving back from like a
[00:31:50.860 --> 00:31:53.180]   meeting down in Menlo or something.
[00:31:53.180 --> 00:31:57.860]   And Luke, he didn't get his driver's license until he was like 28, 29.
[00:31:57.900 --> 00:32:03.860]   So this is shortly after he got his driver's license and he, he can make me
[00:32:03.860 --> 00:32:07.420]   like a little nervous, but I had to take a call.
[00:32:07.420 --> 00:32:07.740]   Yeah.
[00:32:07.740 --> 00:32:08.300]   He was driving.
[00:32:08.300 --> 00:32:09.340]   Cause I had to take a call.
[00:32:09.340 --> 00:32:14.660]   I was taking a call in the car and, and like talking to this person and I like
[00:32:14.660 --> 00:32:17.020]   had to, you know, push my feet down.
[00:32:17.020 --> 00:32:22.220]   I was, it was very, very nerve wracking, but, uh, the call I think went well.
[00:32:22.220 --> 00:32:27.180]   What's the weirdest thing about Lucas that people don't know?
[00:32:28.180 --> 00:32:34.260]   Uh, he is really into, uh, like the evolutionary tree.
[00:32:34.260 --> 00:32:35.740]   Oh, and prisms.
[00:32:35.740 --> 00:32:37.420]   He's like really into prisms.
[00:32:37.420 --> 00:32:37.820]   Yeah.
[00:32:37.820 --> 00:32:40.540]   And magnets and weird, weird, uh, weird metals.
[00:32:40.540 --> 00:32:41.540]   Is he into Scientology?
[00:32:41.540 --> 00:32:42.580]   No.
[00:32:42.580 --> 00:32:43.580]   I'm not into Church.
[00:32:43.580 --> 00:32:44.780]   He's in science.
[00:32:44.780 --> 00:32:45.660]   He's into science.
[00:32:45.660 --> 00:32:47.020]   The Church is science.
[00:32:47.020 --> 00:32:47.340]   Yeah.
[00:32:47.340 --> 00:32:49.740]   He's at the edges of science, you know.
[00:32:49.740 --> 00:32:50.500]   Okay.
[00:32:50.500 --> 00:32:55.500]   So if I were to ask Lucas the same question about you, what would he say?
[00:32:56.340 --> 00:32:58.740]   And we already said weird is one of your adjectives.
[00:32:58.740 --> 00:32:59.180]   Yeah.
[00:32:59.180 --> 00:33:00.740]   He would just say he's weird.
[00:33:00.740 --> 00:33:04.620]   Like he couldn't describe how it was weird, but weirdness, weirdness, like
[00:33:04.620 --> 00:33:08.540]   I've definitely made Luke feel uncomfortable, um, a number of times.
[00:33:08.540 --> 00:33:09.940]   And I like, I like that.
[00:33:09.940 --> 00:33:14.060]   That makes, that's like affirmation that I'm being weird enough, you know?
[00:33:14.060 --> 00:33:14.340]   Yeah.
[00:33:14.340 --> 00:33:19.940]   What relationships in your life, um, friends, co-founders, people at WNB have
[00:33:19.940 --> 00:33:24.420]   like made you a better human, a better leader, like, you know, what has shaped
[00:33:24.420 --> 00:33:31.660]   you? Uh, well, I mean, certainly the, the biggest relationship that shaped me is my,
[00:33:31.660 --> 00:33:34.260]   my life partner, my wife, Snezhana.
[00:33:34.260 --> 00:33:37.620]   Um, how has she shaped you?
[00:33:37.620 --> 00:33:42.380]   Well, we came from like very different worldviews.
[00:33:42.380 --> 00:33:50.380]   So, you know, I grew up in Iowa, um, and my wife grew up in, in Serbia, uh, in
[00:33:50.380 --> 00:33:54.940]   Eastern Europe in a, you know, a war torn environment.
[00:33:54.940 --> 00:34:00.700]   So it was, you know, a very different, um, experience of things and very different
[00:34:00.700 --> 00:34:04.460]   cultures, like just the, the culture of, of Serbia is like pretty different than
[00:34:04.460 --> 00:34:07.660]   the culture of like Iowa or San Francisco, California.
[00:34:07.660 --> 00:34:15.300]   Um, so, you know, through curiosity and, and, uh, what has been like real
[00:34:15.300 --> 00:34:19.940]   heartfelt connection with, with a lot of people in that culture, I've fallen in
[00:34:19.940 --> 00:34:21.900]   love with, um, that part of the world.
[00:34:21.900 --> 00:34:28.820]   And, uh, yeah, my wife pushes me, calls me on my, on my shit.
[00:34:28.820 --> 00:34:31.380]   She is not conflict diverse like myself.
[00:34:31.380 --> 00:34:37.780]   She is like ready to, to, um, confront anything that is, that she's feeling, which
[00:34:37.780 --> 00:34:40.860]   is definitely, um, something I can learn from.
[00:34:41.860 --> 00:34:47.060]   Uh, how have, uh, Lucas and Sean, uh, for those of you who don't know, those
[00:34:47.060 --> 00:34:49.900]   are our co-founders, how have they shaped you as a human?
[00:34:49.900 --> 00:34:53.820]   And how do they press your button?
[00:34:53.820 --> 00:34:54.340]   Uh,
[00:34:54.340 --> 00:34:59.820]   yeah, I mean, this is like a relationship.
[00:34:59.820 --> 00:35:03.820]   Um, I actually, I mean, Luke and I started our previous company about a
[00:35:03.820 --> 00:35:05.540]   year before my wife and I got married.
[00:35:05.540 --> 00:35:09.300]   So I've actually been with Lucas longer than, um, than I've known him.
[00:35:09.300 --> 00:35:13.380]   Um, I've been with him for a couple of years, but I've never met him in person.
[00:35:13.380 --> 00:35:14.660]   So I've never met him in person.
[00:35:14.660 --> 00:35:15.780]   So I've never met him in person.
[00:35:15.780 --> 00:35:16.900]   So I've never met him in person.
[00:35:16.900 --> 00:35:17.940]   So I've never met him in person.
[00:35:17.940 --> 00:35:18.940]   So I've never met him in person.
[00:35:18.940 --> 00:35:19.980]   So I've never met him in person.
[00:35:19.980 --> 00:35:20.980]   So I've never met him in person.
[00:35:20.980 --> 00:35:21.980]   So I've never met him in person.
[00:35:21.980 --> 00:35:22.980]   So I've never met him in person.
[00:35:22.980 --> 00:35:23.980]   So I've never met him in person.
[00:35:23.980 --> 00:35:24.980]   So I've never met him in person.
[00:35:24.980 --> 00:35:25.980]   So I've never met him in person.
[00:35:25.980 --> 00:35:26.980]   So I've never met him in person.
[00:35:26.980 --> 00:35:27.980]   So I've never met him in person.
[00:35:27.980 --> 00:35:28.980]   So I've never met him in person.
[00:35:28.980 --> 00:35:29.980]   So I've never met him in person.
[00:35:29.980 --> 00:35:30.980]   So I've never met him in person.
[00:35:30.980 --> 00:35:56.980]   So I've never met him in person.
[00:35:56.980 --> 00:36:03.980]   So I've never met him in person.
[00:36:03.980 --> 00:36:12.980]   So I've never met him in person.
[00:36:12.980 --> 00:36:34.980]   So I've never met him in person.
[00:36:34.980 --> 00:37:00.980]   So I've never met him in person.
[00:37:00.980 --> 00:37:10.980]   So I've never met him in person.
[00:37:10.980 --> 00:37:11.980]   So I've never met him in person.
[00:37:11.980 --> 00:37:12.980]   So I've never met him in person.
[00:37:12.980 --> 00:37:13.980]   So I've never met him in person.
[00:37:13.980 --> 00:37:14.980]   So I've never met him in person.
[00:37:14.980 --> 00:37:15.980]   So I've never met him in person.
[00:37:15.980 --> 00:37:16.980]   So I've never met him in person.
[00:37:16.980 --> 00:37:17.980]   So I've never met him in person.
[00:37:17.980 --> 00:37:18.980]   So I've never met him in person.
[00:37:18.980 --> 00:37:19.980]   So I've never met him in person.
[00:37:19.980 --> 00:37:20.980]   So I've never met him in person.
[00:37:20.980 --> 00:37:21.980]   So I've never met him in person.
[00:37:21.980 --> 00:37:22.980]   So I've never met him in person.
[00:37:22.980 --> 00:37:23.980]   So I've never met him in person.
[00:37:23.980 --> 00:37:24.980]   So I've never met him in person.
[00:37:24.980 --> 00:37:25.980]   So I've never met him in person.
[00:37:25.980 --> 00:37:26.980]   So I've never met him in person.
[00:37:26.980 --> 00:37:27.980]   So I've never met him in person.
[00:37:27.980 --> 00:37:28.980]   So I've never met him in person.
[00:37:28.980 --> 00:37:29.980]   So I've never met him in person.
[00:37:29.980 --> 00:37:30.980]   So I've never met him in person.
[00:37:30.980 --> 00:37:31.980]   So I've never met him in person.
[00:37:31.980 --> 00:37:32.980]   So I've never met him in person.
[00:37:32.980 --> 00:37:33.980]   So I've never met him in person.
[00:37:33.980 --> 00:37:34.980]   So I've never met him in person.
[00:37:34.980 --> 00:37:35.980]   So I've never met him in person.
[00:37:35.980 --> 00:37:36.980]   So I've never met him in person.
[00:37:36.980 --> 00:37:37.980]   So I've never met him in person.
[00:37:37.980 --> 00:37:38.980]   So I've never met him in person.
[00:37:38.980 --> 00:37:39.980]   So I've never met him in person.
[00:37:39.980 --> 00:37:40.980]   So I've never met him in person.
[00:37:40.980 --> 00:37:41.980]   So I've never met him in person.
[00:37:41.980 --> 00:37:42.980]   So I've never met him in person.
[00:37:42.980 --> 00:37:43.980]   So I've never met him in person.
[00:37:43.980 --> 00:37:44.980]   So I've never met him in person.
[00:37:44.980 --> 00:37:45.980]   So I've never met him in person.
[00:37:45.980 --> 00:37:46.980]   So I've never met him in person.
[00:37:46.980 --> 00:37:47.980]   So I've never met him in person.
[00:37:47.980 --> 00:37:48.980]   So I've never met him in person.
[00:37:48.980 --> 00:37:49.980]   So I've never met him in person.
[00:37:50.980 --> 00:37:51.980]   So I've never met him in person.
[00:37:51.980 --> 00:37:52.980]   So I've never met him in person.
[00:37:52.980 --> 00:37:53.980]   So I've never met him in person.
[00:37:53.980 --> 00:37:54.980]   So I've never met him in person.
[00:37:54.980 --> 00:37:55.980]   So I've never met him in person.
[00:37:55.980 --> 00:37:56.980]   So I've never met him in person.
[00:37:56.980 --> 00:37:57.980]   So I've never met him in person.
[00:37:57.980 --> 00:37:58.980]   So I've never met him in person.
[00:37:58.980 --> 00:37:59.980]   So I've never met him in person.
[00:37:59.980 --> 00:38:00.980]   So I've never met him in person.
[00:38:00.980 --> 00:38:01.980]   So I've never met him in person.
[00:38:01.980 --> 00:38:02.980]   So I've never met him in person.
[00:38:02.980 --> 00:38:03.980]   So I've never met him in person.
[00:38:03.980 --> 00:38:04.980]   So I've never met him in person.
[00:38:04.980 --> 00:38:05.980]   So I've never met him in person.
[00:38:05.980 --> 00:38:06.980]   So I've never met him in person.
[00:38:06.980 --> 00:38:07.980]   So I've never met him in person.
[00:38:07.980 --> 00:38:08.980]   So I've never met him in person.
[00:38:08.980 --> 00:38:09.980]   So I've never met him in person.
[00:38:09.980 --> 00:38:10.980]   So I've never met him in person.
[00:38:11.980 --> 00:38:12.980]   When you're hiring an engineer, we talked about this.
[00:38:12.980 --> 00:38:13.980]   We've done this in the past where it's like,
[00:38:13.980 --> 00:38:14.980]   we're not quite sure.
[00:38:14.980 --> 00:38:17.980]   All right, let's contract with them for a month
[00:38:17.980 --> 00:38:20.980]   and see how it works out.
[00:38:20.980 --> 00:38:25.980]   This is more like, let's go rent an Airbnb somewhere
[00:38:25.980 --> 00:38:27.980]   and spend a week together trying to solve problems
[00:38:27.980 --> 00:38:31.980]   and see how we feel coming out of that.
[00:38:31.980 --> 00:38:34.980]   And even that isn't going to be as good a signal
[00:38:34.980 --> 00:38:36.980]   as having worked with someone and been a friend
[00:38:36.980 --> 00:38:39.980]   with someone for months.
[00:38:40.980 --> 00:38:42.980]   - So different question.
[00:38:42.980 --> 00:38:45.980]   I've been a long time listener of Gradient Dissent
[00:38:45.980 --> 00:38:48.980]   and I often hear Lucas use LaVanya's name.
[00:38:48.980 --> 00:38:49.980]   Like LaVanya says,
[00:38:49.980 --> 00:38:51.980]   LaVanya doesn't want us to talk about this.
[00:38:51.980 --> 00:38:53.980]   LaVanya's yelling at me.
[00:38:53.980 --> 00:38:56.980]   So what are things about LaVanya
[00:38:56.980 --> 00:38:58.980]   that most people don't know?
[00:38:58.980 --> 00:39:01.980]   - Oh, man.
[00:39:01.980 --> 00:39:02.980]   - Don't be shy.
[00:39:03.980 --> 00:39:07.980]   - Well, LaVanya doesn't get burnout as far as I can tell.
[00:39:07.980 --> 00:39:10.980]   I've been worried that LaVanya is going to be like burnout
[00:39:10.980 --> 00:39:12.980]   for years now.
[00:39:12.980 --> 00:39:15.980]   And I tell her to like take more breaks
[00:39:15.980 --> 00:39:17.980]   and not work as hard,
[00:39:17.980 --> 00:39:19.980]   but apparently she's got some gene or something.
[00:39:19.980 --> 00:39:22.980]   We need to go to 23andMe, figure out what it is.
[00:39:22.980 --> 00:39:23.980]   - Exactly.
[00:39:23.980 --> 00:39:24.980]   There's an energy thing.
[00:39:24.980 --> 00:39:26.980]   She gets energy from these problems.
[00:39:26.980 --> 00:39:27.980]   We'll be talking, she's like,
[00:39:27.980 --> 00:39:28.980]   oh, we're going to do this.
[00:39:28.980 --> 00:39:29.980]   We're going to do this.
[00:39:29.980 --> 00:39:31.980]   She gets energy from these problems.
[00:39:31.980 --> 00:39:32.980]   We'll be talking, she's like,
[00:39:32.980 --> 00:39:34.980]   oh, we're going to do our first event.
[00:39:34.980 --> 00:39:38.980]   And the amount of energy she got from like a stupid deadline
[00:39:38.980 --> 00:39:40.980]   and a big idea was just like,
[00:39:40.980 --> 00:39:41.980]   I'm glad you're excited.
[00:39:41.980 --> 00:39:45.980]   - The more impossible or outrageous the task is,
[00:39:45.980 --> 00:39:49.980]   the more like stoked LaVanya is going to be to try to get it done.
[00:39:49.980 --> 00:39:50.980]   - She told me and I went,
[00:39:50.980 --> 00:39:52.980]   when are you doing this?
[00:39:52.980 --> 00:39:53.980]   Like it's already happened.
[00:39:53.980 --> 00:39:54.980]   It was great.
[00:39:54.980 --> 00:39:55.980]   It was amazing.
[00:39:55.980 --> 00:39:56.980]   But for those of you that went,
[00:39:56.980 --> 00:39:57.980]   the deadline was super short.
[00:39:57.980 --> 00:40:00.980]   It was comical from an outsider, but knocked yourself out.
[00:40:00.980 --> 00:40:03.980]   - I think it goes back to like the people, you know?
[00:40:03.980 --> 00:40:07.980]   Like I feel like Chris and Lucas and Sean,
[00:40:07.980 --> 00:40:12.980]   they're such great leaders and they take the people who are in their team
[00:40:12.980 --> 00:40:15.980]   and like they really like unblock them and they're like,
[00:40:15.980 --> 00:40:17.980]   you can achieve anything you want.
[00:40:17.980 --> 00:40:19.980]   And here's the resources, just go nuts.
[00:40:19.980 --> 00:40:22.980]   You know, like, and like, that is such a rare opportunity.
[00:40:22.980 --> 00:40:25.980]   You don't get that often to have someone who just believes in you.
[00:40:25.980 --> 00:40:27.980]   So like, I think, you know, that's where I keep going.
[00:40:27.980 --> 00:40:30.980]   Cause like they keep believing and then they keep thinking,
[00:40:30.980 --> 00:40:31.980]   yep, I can do it.
[00:40:31.980 --> 00:40:39.980]   I think what I have learned from Chris is Chris's ability to take on like
[00:40:39.980 --> 00:40:41.980]   the gnarliest problems.
[00:40:41.980 --> 00:40:45.980]   Like he always picks the hardest problem in the company
[00:40:45.980 --> 00:40:46.980]   and he's always working on it.
[00:40:46.980 --> 00:40:52.980]   And somehow he just doesn't give up and like, he doesn't get bitter.
[00:40:52.980 --> 00:40:57.980]   He doesn't, you know, he just figures it out every single time.
[00:40:57.980 --> 00:40:59.980]   And I try to take that.
[00:40:59.980 --> 00:41:00.980]   Yes.
[00:41:00.980 --> 00:41:02.980]   - It's like, yeah, no, I'm going to try to take that from you.
[00:41:02.980 --> 00:41:05.980]   Cause you said you were conflict averse, but you're not conflict work averse.
[00:41:05.980 --> 00:41:08.980]   Like big issues seem to be like, yay.
[00:41:08.980 --> 00:41:10.980]   So maybe just.
[00:41:10.980 --> 00:41:15.980]   - Oh, that's the best feeling when you can like go up against something
[00:41:15.980 --> 00:41:20.980]   that's gnarly and come out like having defeated it.
[00:41:20.980 --> 00:41:21.980]   - Yeah.
[00:41:21.980 --> 00:41:22.980]   - Like a dragon slayer.
[00:41:22.980 --> 00:41:25.980]   - It's super satisfying.
[00:41:25.980 --> 00:41:27.980]   I remember that like flash animation.
[00:41:27.980 --> 00:41:30.980]   I couldn't get that flash animation to work when I was a teenager and I went
[00:41:30.980 --> 00:41:32.980]   to bed going like, how do I get this to work?
[00:41:32.980 --> 00:41:35.980]   And I like woke up with a solution and got it to work.
[00:41:35.980 --> 00:41:38.980]   And I felt like a king, you know?
[00:41:38.980 --> 00:41:39.980]   - I'm dying to see it.
[00:41:39.980 --> 00:41:41.980]   I want to know our prodigy song.
[00:41:41.980 --> 00:41:45.980]   It was the, okay, I'm going to switch gears and ask about LLMs and AI.
[00:41:45.980 --> 00:41:53.980]   So AI, what is, what industry-wide are people getting wrong?
[00:41:53.980 --> 00:41:58.980]   Because you guys started this company way before this was a thing.
[00:41:58.980 --> 00:42:04.980]   And so I'm curious as what I consider is an, when I look at you,
[00:42:04.980 --> 00:42:07.980]   you were sort of pushing a boulder up a hill saying machine learning's
[00:42:07.980 --> 00:42:08.980]   very important.
[00:42:08.980 --> 00:42:11.980]   We're going to give you tools and people were sort of like, uh-huh.
[00:42:11.980 --> 00:42:15.980]   And now it's a very different mindset about this,
[00:42:15.980 --> 00:42:18.980]   but at this very moment in time,
[00:42:18.980 --> 00:42:22.980]   what do you think the AI industry is getting wrong?
[00:42:22.980 --> 00:42:29.980]   - Well, I think everything changed December last year when
[00:42:29.980 --> 00:42:31.980]   Chat GPT was released.
[00:42:31.980 --> 00:42:34.980]   Now you got to like think what really happened there.
[00:42:34.980 --> 00:42:39.980]   It wasn't like we didn't have these models.
[00:42:39.980 --> 00:42:44.980]   It's like the interface and the RLHF tuning to make it like a real joy to
[00:42:44.980 --> 00:42:49.980]   interact with suddenly unlocked people's ability to see like a lot of
[00:42:49.980 --> 00:42:51.980]   people's ability to see what was possible.
[00:42:51.980 --> 00:42:58.980]   And now we've got a lot of just application developers,
[00:42:58.980 --> 00:43:00.980]   a lot of entrepreneurs,
[00:43:00.980 --> 00:43:03.980]   like 70% of the YC class is doing some gen AI thing.
[00:43:03.980 --> 00:43:09.980]   They're coming in using these things,
[00:43:09.980 --> 00:43:15.980]   seeing it just as another API they can hit and use, but it's under the hood.
[00:43:15.980 --> 00:43:19.980]   It's this probabilistic modeling problem, something that, you know,
[00:43:19.980 --> 00:43:22.980]   we're very familiar with here at Ways and Biases and have been helping teams
[00:43:22.980 --> 00:43:27.980]   to create systematic ways in which they can validate this thing that is,
[00:43:27.980 --> 00:43:30.980]   is not going to be like red and green and CI.
[00:43:30.980 --> 00:43:34.980]   It's going to be, you know, some, some gradient in between,
[00:43:34.980 --> 00:43:37.980]   like you're going to have some percentage that is okay.
[00:43:37.980 --> 00:43:42.980]   And you need a way to actually measure that and then systematically see how
[00:43:42.980 --> 00:43:44.980]   that changes over time.
[00:43:44.980 --> 00:43:47.980]   And then as you're changing your prompts or trying one of these new models
[00:43:47.980 --> 00:43:52.980]   that came out, you have baselines and you can actually compare it.
[00:43:52.980 --> 00:43:58.980]   So I think probably one of the more common mistakes is like not putting a
[00:43:58.980 --> 00:44:02.980]   standard way to actually measure this stuff in a way that can be shared with
[00:44:02.980 --> 00:44:04.980]   folks, which is, I mean,
[00:44:04.980 --> 00:44:07.980]   that happens to be really good for Ways and Biases because we build a lot of
[00:44:07.980 --> 00:44:09.980]   tools to help do that.
[00:44:09.980 --> 00:44:13.980]   But I think we're going to find it being more and more critical as the space
[00:44:13.980 --> 00:44:18.980]   gets more complicated and there's more iteration.
[00:44:18.980 --> 00:44:22.980]   What do you think the AI tech stack looks like for LLMs and how is that
[00:44:22.980 --> 00:44:27.980]   different from the machine learning tech stack?
[00:44:27.980 --> 00:44:31.980]   Well, I mean, I think now, certainly at least in prototyping,
[00:44:31.980 --> 00:44:39.980]   like you're going to probably use OpenAI to kind of measure, you know,
[00:44:39.980 --> 00:44:42.980]   GPT-4, very expensive, probably not going to be something you can put into
[00:44:42.980 --> 00:44:45.980]   production, but certainly something you can use to kind of evaluate how good
[00:44:45.980 --> 00:44:47.980]   whatever other approach you're using.
[00:44:47.980 --> 00:44:51.980]   So suddenly now the AI stack involves like hitting third-party APIs,
[00:44:51.980 --> 00:44:56.980]   which I think historically didn't happen as much.
[00:44:56.980 --> 00:44:58.980]   It starts to look more like traditional web development.
[00:44:58.980 --> 00:45:00.980]   Now, if you deploy this as a server,
[00:45:00.980 --> 00:45:03.980]   you need to like deal with the same things we've been dealing with as web
[00:45:03.980 --> 00:45:07.980]   developers for years in terms of seeing if there's errors, rate limits,
[00:45:07.980 --> 00:45:11.980]   retrying, lost connections and whatnot.
[00:45:11.980 --> 00:45:16.980]   I think today still most of what I'm seeing is Python,
[00:45:16.980 --> 00:45:26.980]   but JavaScript TypeScript is the most commonly understood language.
[00:45:26.980 --> 00:45:29.980]   I'd imagine if you look at the YC group that came out,
[00:45:29.980 --> 00:45:36.980]   the majority of them are using JavaScript or TypeScript to interface with
[00:45:36.980 --> 00:45:37.980]   these things.
[00:45:37.980 --> 00:45:39.980]   So that's kind of, that's a big leap.
[00:45:39.980 --> 00:45:43.980]   Like before, if you were doing data science or ML, it's just Python.
[00:45:43.980 --> 00:45:45.980]   You don't need JavaScript.
[00:45:45.980 --> 00:45:48.980]   Now I think we'll have a fair number of folks that are more comfortable in
[00:45:48.980 --> 00:45:54.980]   TypeScript or JavaScript land and there needs to be better tools for them.
[00:45:54.980 --> 00:45:55.980]   And then the last area is like,
[00:45:55.980 --> 00:45:58.980]   I think there's really cool stuff happening in the embedded,
[00:45:58.980 --> 00:46:00.980]   like running LLMs anywhere,
[00:46:00.980 --> 00:46:05.980]   whether that's on my laptop or some Raspberry Pi or on my phone with
[00:46:05.980 --> 00:46:11.980]   projects like llama.cpp and whisper.cpp.
[00:46:11.980 --> 00:46:14.980]   You can also, you can like run it in the browser using WebGL.
[00:46:14.980 --> 00:46:17.980]   I think we'll start seeing more of this in more novel ways to make the
[00:46:17.980 --> 00:46:25.980]   performance better because there's a lot of really cool things you can do
[00:46:25.980 --> 00:46:28.980]   when the data is like not leaving a given environment,
[00:46:28.980 --> 00:46:31.980]   especially as the chief information security officer.
[00:46:31.980 --> 00:46:32.980]   I think that's pretty sweet.
[00:46:32.980 --> 00:46:35.980]   Cause then it literally couldn't touch any sensitive data.
[00:46:35.980 --> 00:46:37.980]   It's just all right there on the edge.
[00:46:37.980 --> 00:46:40.980]   Do you see any problems with taking a model,
[00:46:40.980 --> 00:46:45.980]   having it output something and then using it to evaluate its own output?
[00:46:45.980 --> 00:46:52.980]   Well, I mean, this, it sounds like, you know, when we train a model,
[00:46:52.980 --> 00:46:55.980]   we don't test it on anything that we we've shown it.
[00:46:55.980 --> 00:47:00.980]   So I would say the same should probably apply for a model that you're
[00:47:00.980 --> 00:47:05.980]   measuring. Like you can certainly use a model to measure, you know,
[00:47:05.980 --> 00:47:08.980]   you can use an LLM to measure an LLM, but I would,
[00:47:08.980 --> 00:47:12.980]   I would think you'd want to use a different LLM than the one like
[00:47:12.980 --> 00:47:16.980]   outputting stuff. So I'm using GPT 3.5.
[00:47:16.980 --> 00:47:20.980]   I could evaluate it with, with GPT four.
[00:47:20.980 --> 00:47:23.980]   And I would probably, even if I did that,
[00:47:23.980 --> 00:47:28.980]   I would want like a human labeling pass just to see if I'm getting more
[00:47:28.980 --> 00:47:35.980]   signal and noise from, from whatever problem I'm working on.
[00:47:35.980 --> 00:47:38.980]   When you are advising big enterprises, you know,
[00:47:38.980 --> 00:47:42.980]   the type of companies that have traditionally moved slow.
[00:47:42.980 --> 00:47:46.980]   And I think that there's a, I'll pick a bank, you know, or something,
[00:47:46.980 --> 00:47:49.980]   something big with all kinds of compliance issues.
[00:47:49.980 --> 00:47:51.980]   How would you advise they start?
[00:47:51.980 --> 00:47:53.980]   Because I think there's just,
[00:47:53.980 --> 00:47:55.980]   everybody's doing something and there's a ton of interest,
[00:47:55.980 --> 00:48:00.980]   but then I hear from, you know, they, they're actually also scared.
[00:48:00.980 --> 00:48:05.980]   It's this FOMO of being left behind fear of moving forward because of data
[00:48:05.980 --> 00:48:08.980]   issues.
[00:48:08.980 --> 00:48:10.980]   Yeah. Baby steps, like first thing,
[00:48:10.980 --> 00:48:17.980]   just like get away for your developers to actually try stuff on the data
[00:48:17.980 --> 00:48:22.980]   that you're, you're concerned about. Right. So that means either they can
[00:48:22.980 --> 00:48:28.980]   only run inference in the infrastructure where the sensitive data is using,
[00:48:28.980 --> 00:48:31.980]   you know, things like Lama or other open source models,
[00:48:31.980 --> 00:48:34.980]   or you just get that enterprise contract with,
[00:48:34.980 --> 00:48:41.980]   with Microsoft to have like a private instance of GPT and then let the team
[00:48:41.980 --> 00:48:43.980]   try. And then while they're trying stuff,
[00:48:43.980 --> 00:48:45.980]   you can start to navigate how you can,
[00:48:45.980 --> 00:48:50.980]   how you stay compliant if this is released to actual users and goes beyond
[00:48:50.980 --> 00:48:56.980]   that. But what you need is like strong cases for,
[00:48:56.980 --> 00:48:59.980]   for how you could actually deploy these things within your organization.
[00:48:59.980 --> 00:49:03.980]   And any blocker to the developer is just, you know, it's deflating.
[00:49:03.980 --> 00:49:07.980]   We just want to make stuff. So when we have to jump through a bunch of
[00:49:07.980 --> 00:49:14.980]   hoops, let them at least prototype you said, like let your developer try.
[00:49:14.980 --> 00:49:15.980]   Yeah.
[00:49:15.980 --> 00:49:20.980]   You said like figure out how to use these elements in your org.
[00:49:20.980 --> 00:49:22.980]   Can you help us think big?
[00:49:22.980 --> 00:49:26.980]   Like I still don't think people understand the impact these elements can have.
[00:49:26.980 --> 00:49:30.980]   Like you can have a natural interface for anything that your product does.
[00:49:30.980 --> 00:49:32.980]   And there's other examples like that.
[00:49:32.980 --> 00:49:39.980]   So what in your mind is the craziest thing these elements can do?
[00:49:39.980 --> 00:49:42.980]   Well, I mean, for me, I think like as a developer,
[00:49:42.980 --> 00:49:46.980]   we immediately got the magic of these things because they're like remarkable
[00:49:46.980 --> 00:49:50.980]   at writing code. An example, recently, one of our designers,
[00:49:50.980 --> 00:49:53.980]   I needed to make a loading icon on one of these pages.
[00:49:53.980 --> 00:49:55.980]   And one of our designers, you know,
[00:49:55.980 --> 00:49:57.980]   gave me an English what that loading icon should do.
[00:49:57.980 --> 00:50:00.980]   It should rotate to the left 10%, zoom out 1%,
[00:50:00.980 --> 00:50:03.980]   change the opacity to 0.9, rotate right 30%.
[00:50:03.980 --> 00:50:06.980]   I'm reading this and I'm kind of thinking like, oh man,
[00:50:06.980 --> 00:50:09.980]   I haven't like written CSS in a while.
[00:50:09.980 --> 00:50:11.980]   I'm going to have to like look up how to do all this.
[00:50:11.980 --> 00:50:12.980]   And I was like, wait,
[00:50:12.980 --> 00:50:17.980]   I could just ask chat GPT and it perfectly translated that English into CSS
[00:50:17.980 --> 00:50:20.980]   three transforms. It's like, so cool. That's like, it's magical.
[00:50:20.980 --> 00:50:24.980]   I just got goosebumps. I know it can do this,
[00:50:24.980 --> 00:50:28.980]   but for some reason hearing that with like, oh, what a nightmare.
[00:50:28.980 --> 00:50:32.980]   And such a, you know, it's so cool. It's so cool.
[00:50:32.980 --> 00:50:36.980]   And it's such a, yeah. The, that like interface of just like,
[00:50:36.980 --> 00:50:39.980]   use your words and it gets you.
[00:50:39.980 --> 00:50:41.980]   For those of us with kids. Yeah.
[00:50:41.980 --> 00:50:46.980]   I think the other, you know,
[00:50:46.980 --> 00:50:50.980]   the use cases that I think are especially interesting for enterprise in
[00:50:50.980 --> 00:50:54.980]   terms of like internal business intelligence and,
[00:50:54.980 --> 00:50:57.980]   and automation, like really cool stuff.
[00:50:57.980 --> 00:51:00.980]   Around just in English asking for some.
[00:51:00.980 --> 00:51:04.980]   Business metrics.
[00:51:04.980 --> 00:51:06.980]   And if you've set things up right and,
[00:51:06.980 --> 00:51:10.980]   and given the LLM access to the schema and where all the data is,
[00:51:10.980 --> 00:51:14.980]   it can generate and then run, you know, SQL and actually give you,
[00:51:14.980 --> 00:51:17.980]   give you charts or information about things.
[00:51:17.980 --> 00:51:21.980]   What do you guys think makes this time different than all of the other
[00:51:21.980 --> 00:51:23.980]   cycles we've had?
[00:51:23.980 --> 00:51:24.980]   I mean, for me, I was like, well,
[00:51:24.980 --> 00:51:28.980]   why didn't I get it when I played with GPT too? Like, I thought it was cool,
[00:51:28.980 --> 00:51:30.980]   but I also thought like, all right, well, what can you do with this?
[00:51:30.980 --> 00:51:35.980]   Like it felt a bit toyish.
[00:51:35.980 --> 00:51:38.980]   But to see that, oh, whoa.
[00:51:38.980 --> 00:51:43.980]   Language turns out great interface and we can have it write code and we
[00:51:43.980 --> 00:51:47.980]   can specify the format of, of things. It becomes, you know,
[00:51:47.980 --> 00:51:49.980]   a really powerful interface.
[00:51:49.980 --> 00:51:53.980]   I think much like the transformer architecture became a really powerful way
[00:51:53.980 --> 00:51:56.980]   to parallelize a bigger and bigger compute.
[00:51:56.980 --> 00:52:01.980]   So I think things are just like all aligning and there's probably a fair
[00:52:01.980 --> 00:52:05.980]   bit of, of innovation still to come and yeah.
[00:52:05.980 --> 00:52:08.980]   Exciting times. Thank you, Chris. And so do I. Thank you.
[00:52:08.980 --> 00:52:12.980]   This was a lot of fun guys. Thanks so much.
[00:52:12.980 --> 00:52:15.980]   Thank you for listening to that episode of creating the scent.
[00:52:15.980 --> 00:52:16.980]   If you liked what you saw,
[00:52:16.980 --> 00:52:19.980]   don't forget to subscribe to us and we'll see you in the next episode.
[00:52:19.980 --> 00:52:25.980]   [Inaudible].
[00:52:25.980 --> 00:52:28.040]   you

