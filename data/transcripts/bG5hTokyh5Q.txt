
[00:00:00.000 --> 00:00:08.260]   We have to decide what should be open and a public good.
[00:00:08.260 --> 00:00:10.600]   This is not from a business perspective, it's from a societal perspective, versus what should
[00:00:10.600 --> 00:00:12.760]   be closed.
[00:00:12.760 --> 00:00:17.200]   Should the tools to allow anyone to be creative, anyone to be educated, and other things like
[00:00:17.200 --> 00:00:18.960]   that be run by private companies?
[00:00:18.960 --> 00:00:21.360]   Probably not.
[00:00:21.360 --> 00:00:25.740]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:25.740 --> 00:00:28.040]   and I'm your host, Lukas Biewald.
[00:00:28.040 --> 00:00:33.520]   Ahmad Mostak is the CEO and co-founder of Stability AI, which is one of the most exciting
[00:00:33.520 --> 00:00:36.540]   companies in the AI space right now.
[00:00:36.540 --> 00:00:41.540]   Before that, he was a hedge fund manager, and before that he was an engineer and an
[00:00:41.540 --> 00:00:42.540]   analyst.
[00:00:42.540 --> 00:00:44.540]   This is a super fun interview, and I hope you enjoy it.
[00:00:44.540 --> 00:00:48.220]   All right, do you mind if I just go rapid fire questions?
[00:00:48.220 --> 00:00:49.220]   Yeah, sure.
[00:00:49.220 --> 00:00:50.220]   Go for it.
[00:00:50.220 --> 00:00:51.220]   Good to see you, Lukas.
[00:00:51.220 --> 00:00:52.220]   Good to see you, Ahmad.
[00:00:52.220 --> 00:00:58.520]   All right, well, I think we need to start with defining, in your words, stability.
[00:00:58.520 --> 00:01:01.960]   I think everyone probably has heard of it, but everyone seems to have a slightly different
[00:01:01.960 --> 00:01:05.700]   impression of exactly what the company is and what it does.
[00:01:05.700 --> 00:01:08.080]   Let's hear from the source directly.
[00:01:08.080 --> 00:01:13.200]   Yeah, so our official mission at Stability is to build a foundation to activate humanity's
[00:01:13.200 --> 00:01:17.000]   potential, with a motto of let's make people happier.
[00:01:17.000 --> 00:01:21.560]   Stability was basically set up in the belief that these new models that we have, these
[00:01:21.560 --> 00:01:26.760]   transformer-based models and similar, are essential for basically unlocking people's
[00:01:26.760 --> 00:01:29.440]   potential in some of those powerful tech that we've seen.
[00:01:29.440 --> 00:01:34.320]   The belief that having them open source so people could build on them and use them was
[00:01:34.320 --> 00:01:39.980]   not only a great business model, but essential for closing the digital divide and getting
[00:01:39.980 --> 00:01:42.520]   this out as widely as possible.
[00:01:42.520 --> 00:01:47.340]   We basically catalyzed the building of open source AI models, and then we take those models
[00:01:47.340 --> 00:01:51.840]   and we scale and customize them for customers, and that's what we do.
[00:01:51.840 --> 00:01:54.280]   And I guess, how did you get started with this?
[00:01:54.280 --> 00:01:58.160]   Your background isn't originally in AI, or is it?
[00:01:58.160 --> 00:01:59.160]   So I see.
[00:01:59.160 --> 00:02:01.360]   I started my career in math, computer science at Oxford.
[00:02:01.360 --> 00:02:03.720]   I was an enterprise developer in my gap year.
[00:02:03.720 --> 00:02:06.560]   Then I did hedge fund managing for many years.
[00:02:06.560 --> 00:02:09.600]   I was a huge AI and video game investor.
[00:02:09.600 --> 00:02:15.880]   But then I took a break when my son was diagnosed with autism, and I used AI to do drug discoveries.
[00:02:15.880 --> 00:02:21.460]   So biomolecular pathway analysis and neurotransmitters and literature review to repurpose drugs to
[00:02:21.460 --> 00:02:25.960]   help ameliorate some of his symptoms while advising a bunch of hedge funds and others
[00:02:25.960 --> 00:02:30.120]   on governments on AI and tech and geopolitics, et cetera.
[00:02:30.120 --> 00:02:34.560]   Going from that experience, that was about 12 years ago that I started that.
[00:02:34.560 --> 00:02:36.520]   It was super interesting.
[00:02:36.520 --> 00:02:40.800]   And then we saw that a lot of the technologies were evolving, but not until the last few
[00:02:40.800 --> 00:02:43.060]   years has it really taken off, obviously.
[00:02:43.060 --> 00:02:46.000]   So I went back to running a hedge fund after that, and it was fine.
[00:02:46.000 --> 00:02:53.040]   And then a couple of years ago, I was one of the kind of lead architects of Kayak, which
[00:02:53.040 --> 00:02:56.840]   was a collective of augmented intelligence against COVID-19, which launched at Stanford
[00:02:56.840 --> 00:03:01.600]   in July of 2020 to take the world's COVID knowledge and then use AI to compress it down
[00:03:01.600 --> 00:03:02.600]   and make it useful.
[00:03:02.600 --> 00:03:06.160]   That's when I first really got exposed to, again, some of these new types of models.
[00:03:06.160 --> 00:03:08.840]   I was like, "Holy crap, this is huge."
[00:03:08.840 --> 00:03:14.100]   And they're getting good enough, fast enough, and soon cheap enough to go everywhere.
[00:03:14.100 --> 00:03:18.300]   And does it make sense that all this tech that's so amazingly powerful is going to be
[00:03:18.300 --> 00:03:21.020]   controlled by big companies and they believe their edge is that?
[00:03:21.020 --> 00:03:22.020]   Not really.
[00:03:22.020 --> 00:03:23.600]   Let's go away from that.
[00:03:23.600 --> 00:03:27.540]   So I've got some AI experience and others, but mostly what I do is kind of see big pictures
[00:03:27.540 --> 00:03:30.340]   and big patterns and then put them together.
[00:03:30.340 --> 00:03:33.060]   A bit of mechanism design, as it were.
[00:03:33.060 --> 00:03:34.060]   That's cool.
[00:03:34.060 --> 00:03:43.920]   I guess I was wondering, you've had such a meteoric ascension in the collective consciousness.
[00:03:43.920 --> 00:03:51.120]   I'm curious, has it happened exactly how you drew it up or has it been kind of surprising?
[00:03:51.120 --> 00:03:53.160]   I guess when you started the company, what were you thinking?
[00:03:53.160 --> 00:03:55.200]   Because it wasn't even that long ago, right?
[00:03:55.200 --> 00:03:58.840]   And then how has it unfolded differently than what you expected?
[00:03:58.840 --> 00:03:59.840]   Yeah.
[00:03:59.840 --> 00:04:02.360]   So we had the idea of stability three years ago.
[00:04:02.360 --> 00:04:06.680]   The first thing my co-founder and I did was we took the Global X Prize for Learning, which
[00:04:06.680 --> 00:04:12.480]   is a $15 million prize for the first app that could teach literacy and numeracy without
[00:04:12.480 --> 00:04:13.760]   internet.
[00:04:13.760 --> 00:04:15.320]   And that was by Elon Musk and Terry Robbins.
[00:04:15.320 --> 00:04:19.200]   And we were deploying tablets into refugee camps saying, "What happens if we use AI to
[00:04:19.200 --> 00:04:21.000]   make this one more better and more powerful?"
[00:04:21.000 --> 00:04:24.960]   We didn't use AI yet, but we just finished our RCTs showing literacy and numeracy in
[00:04:24.960 --> 00:04:30.480]   13 months of education on one hour a day of being taught for refugees in camps.
[00:04:30.480 --> 00:04:31.480]   Wow.
[00:04:31.480 --> 00:04:34.160]   And then we did some big announcements about the AI-ification of that next year.
[00:04:34.160 --> 00:04:36.400]   But that was kind of like a fuzzy one.
[00:04:36.400 --> 00:04:42.080]   Then we kind of set up stability properly two years ago to do the United Nations-backed
[00:04:42.080 --> 00:04:43.080]   AI work on COVID-19.
[00:04:43.080 --> 00:04:48.040]   Fell into a lot of bureaucracy and other things, but really kicked off properly literally a
[00:04:48.040 --> 00:04:49.540]   year ago.
[00:04:49.540 --> 00:04:52.920]   So I think nobody at that time would have expected that it would have gone like this.
[00:04:52.920 --> 00:04:57.640]   Like originally, we helped support the communities at Luther and Lyon and others, and we were
[00:04:57.640 --> 00:05:01.200]   thinking like, "Is this a Web3 DAO of DAOs?
[00:05:01.200 --> 00:05:03.680]   Let's reward all the community members and get them together."
[00:05:03.680 --> 00:05:07.100]   But then after a month or so, we realized that commercial open source software of scale
[00:05:07.100 --> 00:05:09.200]   and service was the way.
[00:05:09.200 --> 00:05:13.600]   And while I was funding the entire open source art space, I thought it would be at least
[00:05:13.600 --> 00:05:17.480]   until next year that we got anywhere near the quality that we've seen now.
[00:05:17.480 --> 00:05:21.080]   So I think there's that pace of compression of knowledge and the ease of use and being
[00:05:21.080 --> 00:05:23.080]   able to get this on people's devices.
[00:05:23.080 --> 00:05:25.400]   That surprised me because I thought it would be another couple of years, at least before
[00:05:25.400 --> 00:05:26.400]   we got there.
[00:05:26.400 --> 00:05:28.560]   But I think that's been the main capitalist, right?
[00:05:28.560 --> 00:05:32.960]   Stable diffusion being the first model that is good enough, fast enough, and cheap enough
[00:05:32.960 --> 00:05:34.320]   that anyone can run.
[00:05:34.320 --> 00:05:38.880]   Like it's a two gigabyte file from 100,000 gigabytes of data.
[00:05:38.880 --> 00:05:43.720]   That was the insane thing that I think has allowed it to go off massively.
[00:05:43.720 --> 00:05:49.040]   But I guess, is it an accident that the name stable diffusion and stability AI are connected
[00:05:49.040 --> 00:05:50.040]   like that?
[00:05:50.040 --> 00:05:52.080]   Well, so this is an interesting thing.
[00:05:52.080 --> 00:05:53.440]   Like what's the actual role of stability?
[00:05:53.440 --> 00:05:54.840]   So we've got over a hundred people.
[00:05:54.840 --> 00:05:59.000]   We've got some amazing researchers, but our role as a capitalist in the community, right?
[00:05:59.000 --> 00:06:04.040]   So with stable diffusion, it built on the work of the Confiz lab at a former university
[00:06:04.040 --> 00:06:07.960]   of Heidelberg, now LMU Munich, and a Bionic on that.
[00:06:07.960 --> 00:06:14.480]   And so the two lead authors of stable diffusion were Patrick Esser at RunwayML and then Robin
[00:06:14.480 --> 00:06:17.120]   Rombach who works with us.
[00:06:17.120 --> 00:06:20.560]   And they came up with the name all themselves because we provide a computer infrastructure
[00:06:20.560 --> 00:06:23.320]   support and then obviously Robin themselves there.
[00:06:23.320 --> 00:06:27.920]   But we always try to give developers lots of flexibility, especially when working in
[00:06:27.920 --> 00:06:30.000]   these collaborations, it does get complicated there.
[00:06:30.000 --> 00:06:32.560]   We can discuss that a bit later.
[00:06:32.560 --> 00:06:34.200]   And they came up with, I was like, yeah, I love that name.
[00:06:34.200 --> 00:06:36.520]   Go for it.
[00:06:36.520 --> 00:06:41.360]   But at the same time, there is this kind of inherent tension because a lot of people want
[00:06:41.360 --> 00:06:44.600]   us to manage the whole community, but that's not how open source works, right?
[00:06:44.600 --> 00:06:47.560]   The whole thing about open source is that there's lots of different things, even if
[00:06:47.560 --> 00:06:51.560]   you've got like a Linux or Red Hat or something like that.
[00:06:51.560 --> 00:06:56.240]   And for models, it's also a bit different because the normal open source software, you
[00:06:56.240 --> 00:06:59.680]   have loads and loads of contributors, like hundreds, thousands.
[00:06:59.680 --> 00:07:01.280]   You don't really have that for models.
[00:07:01.280 --> 00:07:05.080]   It's like you can do the whole thing just with a team of two to 10 people.
[00:07:05.080 --> 00:07:08.160]   Or if you are like Lucid Rains, you do that all by yourself.
[00:07:08.160 --> 00:07:10.280]   You know, like he's one of the developers that we support.
[00:07:10.280 --> 00:07:12.280]   He just cranks out models every day.
[00:07:12.280 --> 00:07:16.560]   If you're a programmer that wants to feel bad, go and look at GitHub/lucidrains for
[00:07:16.560 --> 00:07:17.560]   productivity.
[00:07:17.560 --> 00:07:18.560]   All right.
[00:07:18.560 --> 00:07:21.520]   I'll put a link in, but I don't want to look at it right now.
[00:07:21.520 --> 00:07:24.320]   I've been very unproductive over the last few years.
[00:07:24.320 --> 00:07:25.840]   Yeah, make you feel terrible.
[00:07:25.840 --> 00:07:27.560]   Like, ah, jeez.
[00:07:27.560 --> 00:07:35.600]   So I was curious about exactly like how that interaction works today with people building
[00:07:35.600 --> 00:07:36.600]   models.
[00:07:36.600 --> 00:07:39.760]   Like what's your way of working with folks?
[00:07:39.760 --> 00:07:42.880]   So yeah, I think that the best way is always kind of collaboration.
[00:07:42.880 --> 00:07:45.000]   So we have our supercomputer cluster here.
[00:07:45.000 --> 00:07:47.240]   It was 4,000 A100s originally.
[00:07:47.240 --> 00:07:50.080]   Now it's going much, much larger because I view that as a key unlock.
[00:07:50.080 --> 00:07:53.400]   And then the infrastructure support to make stuff usable there.
[00:07:53.400 --> 00:07:57.000]   We have the communities that we're spinning out into independent foundations like Eleuther
[00:07:57.000 --> 00:08:01.360]   and others, where we provide employment and benefits and equity, et cetera.
[00:08:01.360 --> 00:08:05.680]   And then collaborations with academia and non-academia independent researchers.
[00:08:05.680 --> 00:08:09.920]   I think the goal for the open source side of things is to put a lot more structure around
[00:08:09.920 --> 00:08:10.920]   that.
[00:08:10.920 --> 00:08:13.600]   So everyone knows when stuff is meant to be released, what happens if you've got ethical
[00:08:13.600 --> 00:08:15.240]   concerns and things like that.
[00:08:15.240 --> 00:08:18.160]   But again, really be a catalyst for the community.
[00:08:18.160 --> 00:08:22.080]   Some of the models you'll see released over the next period are entirely stability models.
[00:08:22.080 --> 00:08:26.080]   Some of them are combination models, but we want to make sure that these things are clearly
[00:08:26.080 --> 00:08:28.800]   defined because otherwise people get sad.
[00:08:28.800 --> 00:08:31.400]   And it's understandable as well, attribution should be given.
[00:08:31.400 --> 00:08:34.400]   One of the unique things that we have brought in though is that we're building an entire
[00:08:34.400 --> 00:08:37.360]   infrastructure to be able to scale and train these models.
[00:08:37.360 --> 00:08:42.480]   And if we do inference on any open source model, we actually put aside 10% of the revenue
[00:08:42.480 --> 00:08:44.880]   from that for the developers.
[00:08:44.880 --> 00:08:50.120]   So 5% goes into a community pool that we'll be activating in a month or two, where every
[00:08:50.120 --> 00:08:54.440]   developer affiliated with stability can vote to allocate to the coolest research they can
[00:08:54.440 --> 00:08:55.440]   find.
[00:08:55.440 --> 00:08:59.600]   And half of it goes to the developers themselves, even if they don't work at stability.
[00:08:59.600 --> 00:09:04.200]   So again, we're really trying to give back a bit to the community and recognize the authors'
[00:09:04.200 --> 00:09:09.000]   things, they can donate it or whatever from that angle and trying to make it so it's clear
[00:09:09.000 --> 00:09:10.280]   how we interact with these.
[00:09:10.280 --> 00:09:15.600]   Because we are the fastest providers of compute and support and technical support and input
[00:09:15.600 --> 00:09:21.360]   of anyone in the market, because you could access super compute before, but it was only
[00:09:21.360 --> 00:09:28.520]   really through these giant clusters with like six to 12 month processes for application,
[00:09:28.520 --> 00:09:33.480]   like from Jules, which is pretty good to summit, which is much more bureaucratic and some of
[00:09:33.480 --> 00:09:34.680]   the others.
[00:09:34.680 --> 00:09:38.040]   And that obviously doesn't keep pace with the pace of AI development now, which is literally
[00:09:38.040 --> 00:09:39.040]   exponential.
[00:09:39.040 --> 00:09:44.040]   So that's why what happened is that a lot of academics basically had to leave to either
[00:09:44.040 --> 00:09:48.200]   their own startups, which as you and I both know as CEOs, is incredibly difficult.
[00:09:48.200 --> 00:09:52.200]   Join a big tech company, which isn't so much of an option anymore, given the freeze that's
[00:09:52.200 --> 00:09:55.020]   going on.
[00:09:55.020 --> 00:09:56.020]   And that was it.
[00:09:56.020 --> 00:09:57.960]   And then that doesn't fit with academia.
[00:09:57.960 --> 00:10:01.160]   So academia is one area that we're supporting in general.
[00:10:01.160 --> 00:10:03.800]   And again, I think compute is the key unlock there.
[00:10:03.800 --> 00:10:08.240]   But over time, it's going to be increasing the infrastructure side of things and having
[00:10:08.240 --> 00:10:09.480]   standardized stuff.
[00:10:09.480 --> 00:10:13.840]   Like right now, not everyone uses tools like weights and biases, for example, to track
[00:10:13.840 --> 00:10:14.840]   their runs.
[00:10:14.840 --> 00:10:16.800]   We would like to move to more and more open runs.
[00:10:16.800 --> 00:10:22.280]   So you can actually see how they're doing like Bloom did with their updates, et cetera.
[00:10:22.280 --> 00:10:27.160]   So there's a lot of work to go, but we're trying to be as collaborative as possible.
[00:10:27.160 --> 00:10:33.280]   Do you, so say I'm a researcher and I have like an interesting area of work and I'm looking
[00:10:33.280 --> 00:10:36.720]   for infrastructure support.
[00:10:36.720 --> 00:10:40.800]   How do I apply to stability and how would you view my application?
[00:10:40.800 --> 00:10:44.640]   Like what would you consider, how would you decide whether or not to fund it and how much
[00:10:44.640 --> 00:10:46.240]   to fund it?
[00:10:46.240 --> 00:10:50.120]   So the way that we do it at the moment is that if you're an active member of any of
[00:10:50.120 --> 00:10:55.720]   the communities from Harmline for music to Alutha for language models, Ion for images,
[00:10:55.720 --> 00:10:58.800]   you're most likely to get compute that way.
[00:10:58.800 --> 00:11:04.080]   And that can be from an A100 up to 500 A100s, depending on how good your thing is, particularly
[00:11:04.080 --> 00:11:07.000]   if you bring in members of that community as your team.
[00:11:07.000 --> 00:11:08.000]   So that's the primary way.
[00:11:08.000 --> 00:11:13.280]   Right now we're setting up a grant making portal and we're working with certain universities
[00:11:13.280 --> 00:11:17.360]   in that regard, but then also trying to figure out how do we do like large clouds of almost
[00:11:17.360 --> 00:11:22.360]   Google collab on steroids to allow people to unlock things from day one.
[00:11:22.360 --> 00:11:25.440]   This fits in as well with the next stage of our program, which is that we funded like
[00:11:25.440 --> 00:11:29.240]   maybe a handful of PhDs so far who've been active members of the community.
[00:11:29.240 --> 00:11:32.800]   We're planning to fund a hundred in the next year and they will come with dedicated compute
[00:11:32.800 --> 00:11:35.320]   support for their labs and their projects as well.
[00:11:35.320 --> 00:11:40.160]   And there's an independent board being set up for deciding that because, you know, again,
[00:11:40.160 --> 00:11:45.360]   one of the tensions is always going to be kind of our work versus, sorry, our business
[00:11:45.360 --> 00:11:50.040]   side versus the broader side, you know, like why are we funding open BiomL to BiomL because
[00:11:50.040 --> 00:11:51.040]   it's useful.
[00:11:51.040 --> 00:11:58.440]   There's no kind of a business logic to it at the moment, but we want to keep that mix
[00:11:58.440 --> 00:12:03.480]   of, you know, supporting the entire ecosystem so we have a nice place in it and then focusing
[00:12:03.480 --> 00:12:06.440]   on some of the old business stuff, which is generative media at the moment.
[00:12:06.440 --> 00:12:09.920]   So I say for the moment, generative media, if there's anything interesting, you can just
[00:12:09.920 --> 00:12:13.480]   reach out on the communities and we fund most things in there.
[00:12:13.480 --> 00:12:17.760]   The other stuff we're building out the infrastructure, but just join those communities, you know,
[00:12:17.760 --> 00:12:20.840]   join the open BiomL and other communities and contribute.
[00:12:20.840 --> 00:12:23.360]   And then that's the best interview of all, right?
[00:12:23.360 --> 00:12:27.520]   It's like you're more likely to help people who help your communities.
[00:12:27.520 --> 00:12:32.040]   And then I guess like what's required of me, like say I'm someone with a new idea for generating
[00:12:32.040 --> 00:12:34.720]   like awesome music.
[00:12:34.720 --> 00:12:38.760]   Does that mean that I need to like contribute my model to the community after it's done
[00:12:38.760 --> 00:12:42.360]   training or how does that?
[00:12:42.360 --> 00:12:47.000]   We encourage open source, but you know, a large part of it is open access as well.
[00:12:47.000 --> 00:12:51.560]   Like we have incubator oncoming of VC arm and others for those who don't want to go
[00:12:51.560 --> 00:12:53.560]   open source, but we heavily encourage open source.
[00:12:53.560 --> 00:12:55.680]   I think not everything needs to be open source.
[00:12:55.680 --> 00:12:58.960]   What needs to be open source is the benchmark models, you know?
[00:12:58.960 --> 00:13:01.000]   So it's like need nobody behind.
[00:13:01.000 --> 00:13:04.720]   But the reality is that open source will always lag close source.
[00:13:04.720 --> 00:13:07.560]   So like Mid Journey just released version four, which is amazing, right?
[00:13:07.560 --> 00:13:10.360]   And Dali three will come out soon, which will be even more amazing.
[00:13:10.360 --> 00:13:11.360]   Why?
[00:13:11.360 --> 00:13:14.840]   Because they can take open source basis and go ahead or they can just do something different.
[00:13:14.840 --> 00:13:18.800]   So Mid Journey version four was completely different, but Mid Journey version three with
[00:13:18.800 --> 00:13:21.380]   stable diffusion was a mixture of the two.
[00:13:21.380 --> 00:13:24.160]   So you always get this interesting, well, open source will lag behind.
[00:13:24.160 --> 00:13:29.880]   We're just trying to make it so the lag is minimal and people start on that same basis.
[00:13:29.880 --> 00:13:34.280]   But for people who kind of come and use our cluster, the priority for the first cluster
[00:13:34.280 --> 00:13:39.220]   is open source, but we're going to have more clusters where they will also be for the companies
[00:13:39.220 --> 00:13:44.600]   that we're incubating our own use and other things like that.
[00:13:44.600 --> 00:13:46.600]   How do you think about the sort of broad buckets?
[00:13:46.600 --> 00:13:51.280]   Like it sounds like you kind of do it by use case.
[00:13:51.280 --> 00:13:56.280]   Like do you kind of think, I mean, it seems like you're kind of like good at recognizing
[00:13:56.280 --> 00:13:58.160]   larger scale patterns.
[00:13:58.160 --> 00:14:05.720]   Like do you have an opinion between the value of investing in infrastructure for audio generation,
[00:14:05.720 --> 00:14:09.120]   image generation, these large language models?
[00:14:09.120 --> 00:14:13.840]   How do you even like approach that question of allocation?
[00:14:13.840 --> 00:14:20.440]   So right now I would say from a business perspective, media is by far the most lucrative and that
[00:14:20.440 --> 00:14:21.720]   can fund a lot of other stuff.
[00:14:21.720 --> 00:14:26.560]   So Google and Fave Meta have amazing research labs that they fund through advertising.
[00:14:26.560 --> 00:14:29.120]   And let's face it, we all hate advertising.
[00:14:29.120 --> 00:14:31.760]   Advertising is manipulative, particularly with these new models, it's becoming more
[00:14:31.760 --> 00:14:32.840]   manipulative.
[00:14:32.840 --> 00:14:35.960]   The area that we focused on is the world's content.
[00:14:35.960 --> 00:14:40.400]   So audio, video, and others, those will all be in foundation models in the next five to
[00:14:40.400 --> 00:14:41.400]   10 years.
[00:14:41.400 --> 00:14:43.280]   And we're focusing on that to fund everything else.
[00:14:43.280 --> 00:14:45.640]   So I think that's a reasonable model.
[00:14:45.640 --> 00:14:49.280]   The Disneys and Paramounts and everyone of the world will eventually have to transform
[00:14:49.280 --> 00:14:50.360]   their entire archives.
[00:14:50.360 --> 00:14:56.000]   It's like the VHS to DVD uplift on steroids, because you know how difficult doing these
[00:14:56.000 --> 00:14:57.680]   models is.
[00:14:57.680 --> 00:15:01.000]   So that's kind of what we'll focus on from a business perspective.
[00:15:01.000 --> 00:15:03.120]   From an impact perspective, it's a lot more difficult.
[00:15:03.120 --> 00:15:06.880]   And this is also why like one of the things we've done now is, again, within a year we've
[00:15:06.880 --> 00:15:08.120]   built this giant cluster.
[00:15:08.120 --> 00:15:13.920]   So 4,100 isn't the largest private cluster, but on the public top 500 list, it's in the
[00:15:13.920 --> 00:15:15.800]   top 10 probably.
[00:15:15.800 --> 00:15:19.760]   The Jules Booster with 3,744 is number 11.
[00:15:19.760 --> 00:15:23.880]   The fastest super computer in the UK, Cambridge One, is 640.
[00:15:23.880 --> 00:15:27.040]   The same with Naval in Canada, for example, and NASA's got about the same.
[00:15:27.040 --> 00:15:29.840]   So this is a chunky old beast.
[00:15:29.840 --> 00:15:32.560]   The reality is that should be a public good eventually.
[00:15:32.560 --> 00:15:37.720]   And there is a National Research Cloud discussion led by Stanford and a bunch of others that
[00:15:37.720 --> 00:15:38.720]   say this is needed for US universities.
[00:15:38.720 --> 00:15:41.320]   I think it's needed for international universities.
[00:15:41.320 --> 00:15:44.760]   And so hopefully we can figure out a way to transfer over there with this value function
[00:15:44.760 --> 00:15:48.120]   that you're discussing, because otherwise it turns into fiefdoms.
[00:15:48.120 --> 00:15:52.480]   So right now it's quite a centralized thing where we're just like, what can be most beneficial
[00:15:52.480 --> 00:15:55.580]   for the community and attracting assets to the community?
[00:15:55.580 --> 00:15:57.300]   And this was media for us.
[00:15:57.300 --> 00:16:01.540]   We're still doing the LM training, but large language models I think are less impactful
[00:16:01.540 --> 00:16:06.240]   because language was already 85, 80% there and we've gone to 90% there.
[00:16:06.240 --> 00:16:09.600]   Whereas a lot of this image stuff was like 10% there and suddenly we've gone to 80 and
[00:16:09.600 --> 00:16:11.980]   now 90% there.
[00:16:11.980 --> 00:16:15.880]   And so it's a lot more immediate for people because this brings us to the final bit, which
[00:16:15.880 --> 00:16:21.200]   is that the nature of these models is, and the data that they run on, is that they can
[00:16:21.200 --> 00:16:23.200]   do just about anything.
[00:16:23.200 --> 00:16:28.040]   So if you have them converging in terms of quality from different players and then an
[00:16:28.040 --> 00:16:30.240]   open source version, where's the value?
[00:16:30.240 --> 00:16:33.400]   The value can't be in models if they can do anything, right?
[00:16:33.400 --> 00:16:35.220]   The value has to be elsewhere.
[00:16:35.220 --> 00:16:38.960]   And so that's going to be very interesting to see, especially from like I said, societal
[00:16:38.960 --> 00:16:41.240]   value versus business value.
[00:16:41.240 --> 00:16:45.160]   But I guess what's interesting is, as far as I could tell, the main thing that you're
[00:16:45.160 --> 00:16:50.280]   doing, the thing that you're really passionate about is democratizing access to creating
[00:16:50.280 --> 00:16:53.040]   and opening up these models.
[00:16:53.040 --> 00:16:59.000]   So I guess if the value isn't there in your mind, how do you think about creating a long
[00:16:59.000 --> 00:17:02.080]   term sustainable business?
[00:17:02.080 --> 00:17:07.100]   So it's basically the value is in going into Hello Kitty as a business and transforming
[00:17:07.100 --> 00:17:10.080]   all the assets into interactive ones.
[00:17:10.080 --> 00:17:13.280]   It can be for the metaverse, it can be for new experiences, it can be for whatever.
[00:17:13.280 --> 00:17:17.040]   And then building tools to enable them to access their models and other people to access
[00:17:17.040 --> 00:17:19.680]   their models, piping it around the world.
[00:17:19.680 --> 00:17:23.480]   So our main play as a business is basically kind of content and helping big companies
[00:17:23.480 --> 00:17:27.620]   for that, and then helping everyone else through the software that we built.
[00:17:27.620 --> 00:17:30.160]   Like Dream Studio Lite is just a very basic piece of software.
[00:17:30.160 --> 00:17:35.280]   Dream Studio Pro that's going to be released in late November is a fully functional animation
[00:17:35.280 --> 00:17:40.040]   suite with storyboarding and fine tuning capabilities and the ability to create your own models
[00:17:40.040 --> 00:17:41.620]   and other things like that.
[00:17:41.620 --> 00:17:44.460]   So again, being in that infrastructure layer and allowing the infrastructure to be usable
[00:17:44.460 --> 00:17:45.460]   is where we're at.
[00:17:45.460 --> 00:17:49.560]   Plus of course our APIs, which are industrial scale, and we're negotiating the cost down
[00:17:49.560 --> 00:17:50.880]   and down and down.
[00:17:50.880 --> 00:17:54.800]   Because the data on how models are used, as many people on this call will know, is as
[00:17:54.800 --> 00:17:57.720]   useful as the models themselves.
[00:17:57.720 --> 00:18:00.080]   Because then you can instruct them and you can guide them down.
[00:18:00.080 --> 00:18:05.480]   And you know the Carper team led by Lewis has done an exceptional job in releasing the
[00:18:05.480 --> 00:18:08.340]   first open source instruct model framework.
[00:18:08.340 --> 00:18:12.720]   And now we're training new models to be able to instruct them across modalities as well,
[00:18:12.720 --> 00:18:14.480]   based on some of this data.
[00:18:14.480 --> 00:18:20.660]   So I think that's where the sustainable edge is, a mixture of content and mixture of experience.
[00:18:20.660 --> 00:18:25.280]   And the content, to give you an example, we have a deal with Eros in Bollywood in India,
[00:18:25.280 --> 00:18:29.200]   which is the Netflix of India, 200 million daily active users.
[00:18:29.200 --> 00:18:32.520]   All the Bollywood assets are going to be converted by us.
[00:18:32.520 --> 00:18:34.840]   And then all the music will pretty much sound the same.
[00:18:34.840 --> 00:18:39.000]   It's like that data will eventually be converted.
[00:18:39.000 --> 00:18:43.040]   We're just doing it five years before anyone else otherwise would have.
[00:18:43.040 --> 00:18:46.160]   And sorry, when you say converted, converted into what?
[00:18:46.160 --> 00:18:50.440]   So you take all the Bollywood music and then you have a text conditioned audio model that
[00:18:50.440 --> 00:18:53.060]   can generate any Bollywood music.
[00:18:53.060 --> 00:18:56.640]   And that doesn't need to be open source, as a business thing.
[00:18:56.640 --> 00:18:59.940]   But then we can use the open source dance to fusion models and the new text condition
[00:18:59.940 --> 00:19:03.060]   ones we're working on to be the framework for that.
[00:19:03.060 --> 00:19:07.760]   So it's like you go and you do a MySQL database with someone and they load their data into
[00:19:07.760 --> 00:19:08.920]   it, right?
[00:19:08.920 --> 00:19:13.840]   And they're like, okay, well, I'm paying you to implement this because that's MySQL's model,
[00:19:13.840 --> 00:19:20.040]   PostgreSQL's models, or any of these other open source database providers or server providers.
[00:19:20.040 --> 00:19:23.160]   And that commercial open source software model is very well established.
[00:19:23.160 --> 00:19:27.400]   There's an extra wrinkle in this in that they load their data into a model, then converts
[00:19:27.400 --> 00:19:30.760]   into a couple of gigabytes that they can then use for their internal processes and then
[00:19:30.760 --> 00:19:31.760]   external things.
[00:19:31.760 --> 00:19:34.720]   The extra wrinkle is that it's hard.
[00:19:34.720 --> 00:19:36.280]   It's hard to train these models.
[00:19:36.280 --> 00:19:38.160]   Even to fine tune these models isn't that easy.
[00:19:38.160 --> 00:19:39.420]   We'll make it easier.
[00:19:39.420 --> 00:19:43.920]   And the pace of model development means that they have to retrain every so often as well.
[00:19:43.920 --> 00:19:48.080]   Until I think in image you get to a steady state in two years, in video probably three
[00:19:48.080 --> 00:19:52.600]   to four years, audio is probably about two years as well for having a standard model
[00:19:52.600 --> 00:19:53.600]   in the space.
[00:19:53.600 --> 00:19:57.440]   But I guess what are you doing for the Bollywood application today?
[00:19:57.440 --> 00:19:59.200]   What's the conversion that's happening?
[00:19:59.200 --> 00:20:02.280]   Oh, it's just like interest, right?
[00:20:02.280 --> 00:20:04.680]   Bollywood is just like basic...
[00:20:04.680 --> 00:20:09.440]   Well, we can't discuss it because we haven't announced it, but basically it's more basic
[00:20:09.440 --> 00:20:11.440]   TikTok type stuff and Snapchat type stuff.
[00:20:11.440 --> 00:20:14.820]   And the things that you've seen with the use of stable diffusion right now, which is image
[00:20:14.820 --> 00:20:17.320]   based by static.
[00:20:17.320 --> 00:20:22.080]   It's inevitable that entire audio and movies will be created using this technology, not
[00:20:22.080 --> 00:20:25.280]   zero shot, but in a pipeline of different things.
[00:20:25.280 --> 00:20:31.240]   So this is what we've seen with like you take EB synth, Koei and stable diffusion, and you
[00:20:31.240 --> 00:20:34.800]   can map a monster onto your face with the full kind of thing.
[00:20:34.800 --> 00:20:36.800]   That's the type of thing that we're thinking around this.
[00:20:36.800 --> 00:20:42.440]   So most of the Bollywood stuff is going to be used internally now to save costs on production.
[00:20:42.440 --> 00:20:46.280]   And then over the next few years, you will see it go from cost savings to new revenue
[00:20:46.280 --> 00:20:51.700]   streams as people have new and interactive experiences across modalities.
[00:20:51.700 --> 00:20:56.320]   So when you think about your own internal allocation of resources, I guess humans, right?
[00:20:56.320 --> 00:20:58.760]   You have about a hundred people, you said?
[00:20:58.760 --> 00:21:00.200]   Is that right?
[00:21:00.200 --> 00:21:08.240]   How do you break down who works on the foundation models versus who works on the commercialization?
[00:21:08.240 --> 00:21:11.260]   Or is that even the right way to think about what you're doing?
[00:21:11.260 --> 00:21:16.280]   So we split it into two basically, whereby the researchers who are open source researchers
[00:21:16.280 --> 00:21:20.760]   actually have in their contracts, they can open source anything they create unless we
[00:21:20.760 --> 00:21:22.480]   specifically agree otherwise.
[00:21:22.480 --> 00:21:26.480]   And they're given a lot of independence and a lot of free reign to make mistakes.
[00:21:26.480 --> 00:21:29.720]   So we can say we went overboard on compute, but that's what allowed us to experiment with
[00:21:29.720 --> 00:21:30.720]   different things.
[00:21:30.720 --> 00:21:36.000]   And we'll continue to ramp that up because a lot of researchers are constrained by compute
[00:21:36.000 --> 00:21:37.000]   and other resources.
[00:21:37.000 --> 00:21:41.520]   So it's like one training around all that done, or they've only got like a 50% buffer
[00:21:41.520 --> 00:21:42.520]   or something.
[00:21:42.520 --> 00:21:44.840]   We thought that was the wrong way to have breakthroughs.
[00:21:44.840 --> 00:21:49.960]   Separate from that is the product and kind of deployment teams, like the customer solutions
[00:21:49.960 --> 00:21:53.560]   teams, because we don't want product to influence research too much.
[00:21:53.560 --> 00:21:58.440]   Like people are aligned in that they want to create a great business so it could be
[00:21:58.440 --> 00:21:59.520]   self-sustaining.
[00:21:59.520 --> 00:22:03.840]   But when you have product influencing research, you get bad outcomes.
[00:22:03.840 --> 00:22:05.320]   The product team kind of does its own thing.
[00:22:05.320 --> 00:22:07.760]   They work closely with the research team.
[00:22:07.760 --> 00:22:14.080]   And they have discussions to influence the high level it, but there's no forcing function.
[00:22:14.080 --> 00:22:17.640]   So it's not like you have to have a model ready by this deadline in order for this product
[00:22:17.640 --> 00:22:18.640]   release.
[00:22:18.640 --> 00:22:21.440]   The problem is if you do that, you'll never have proper research.
[00:22:21.440 --> 00:22:24.960]   So that's kind of one of the ways that we split it down and there's infrastructure that
[00:22:24.960 --> 00:22:27.880]   supports all of them.
[00:22:27.880 --> 00:22:32.960]   Do you worry about someone else coming along and taking your open source models and then
[00:22:32.960 --> 00:22:36.400]   building their own like rival applications to yours?
[00:22:36.400 --> 00:22:39.640]   I really hope that other people release more open source models.
[00:22:39.640 --> 00:22:42.840]   And that means that I don't have to, right?
[00:22:42.840 --> 00:22:46.480]   Because again, like our role is to help grow these communities and it's to provide the
[00:22:46.480 --> 00:22:47.880]   support for people doing that.
[00:22:47.880 --> 00:22:51.440]   So if someone wants to come along and create their own model, we can provide compute for
[00:22:51.440 --> 00:22:52.440]   them.
[00:22:52.440 --> 00:22:55.240]   Like there's a lot of different entities that we're providing compute for.
[00:22:55.240 --> 00:22:58.560]   People would see as competitors because I think this whole market just grows massively.
[00:22:58.560 --> 00:23:04.240]   Like with Midjourney as an example on the outside, I gave a grant for the first May
[00:23:04.240 --> 00:23:05.960]   100s for the beta.
[00:23:05.960 --> 00:23:09.080]   And I said, when stable diffusion launched, they would be better than we are.
[00:23:09.080 --> 00:23:10.320]   You know, and it's fantastic.
[00:23:10.320 --> 00:23:11.320]   They are.
[00:23:11.320 --> 00:23:13.400]   Other people have had issues with voters and other things.
[00:23:13.400 --> 00:23:16.040]   I've stepped in to try and help them, even though they might be viewed as competitors
[00:23:16.040 --> 00:23:20.240]   with API, because again, I think the whole market will just grow massively.
[00:23:20.240 --> 00:23:24.120]   The key potential displacement point for us is basically another company coming and do
[00:23:24.120 --> 00:23:29.520]   exactly what we do and support in the community in this very strange way and being decentralized
[00:23:29.520 --> 00:23:31.140]   and having this division.
[00:23:31.140 --> 00:23:32.880]   But then it's like, why wouldn't you just stick with us?
[00:23:32.880 --> 00:23:37.240]   I think our replacement cost is quite high and the role of our company will change in
[00:23:37.240 --> 00:23:38.280]   the coming years.
[00:23:38.280 --> 00:23:43.320]   So now we're a catalyst to make sure and force people to go open as a forcing function.
[00:23:43.320 --> 00:23:48.640]   In a few years time, there'll be more of a services company that is building Indian level
[00:23:48.640 --> 00:23:53.920]   models for the Indian and Filipinos and kind of other things and for the largest content
[00:23:53.920 --> 00:23:54.920]   providers.
[00:23:54.920 --> 00:23:58.080]   And then I hope over time we move into being an AI platform.
[00:23:58.080 --> 00:24:04.100]   It's just making AI easy and accessible for everyone, you know, because all the models
[00:24:04.100 --> 00:24:05.100]   will be pushed to the edge.
[00:24:05.100 --> 00:24:08.760]   Because I think they'll get smaller and smaller and smaller and you're seeing custom silicon
[00:24:08.760 --> 00:24:13.760]   and like an iPhone and all these other architectures whereby a lot of these models just be a few
[00:24:13.760 --> 00:24:17.160]   hundred megabytes big and you've got your own model, I've got my own model and we're
[00:24:17.160 --> 00:24:19.400]   interacting with big models in the cloud.
[00:24:19.400 --> 00:24:22.720]   And I think that's a really interesting flip of the internet and that's where we're aiming.
[00:24:22.720 --> 00:24:26.360]   I don't think anyone I've ever seen is really doing the same.
[00:24:26.360 --> 00:24:30.480]   And if they are, they might as well join us, you know, like we're cool, we're fine.
[00:24:30.480 --> 00:24:32.040]   So you think, let me just understand this.
[00:24:32.040 --> 00:24:35.160]   So you think that, I mean, overall it sounds like you think models in the cloud will get
[00:24:35.160 --> 00:24:40.120]   bigger and bigger, but there'll be smaller versions of them for kind of ease of deployment
[00:24:40.120 --> 00:24:41.120]   and cost.
[00:24:41.120 --> 00:24:42.880]   Is that a fair statement?
[00:24:42.880 --> 00:24:48.120]   I think that if you look at the Chinchilla scaling paper, it basically, it sounds like
[00:24:48.120 --> 00:24:51.280]   more epochs of trading and actually means better data.
[00:24:51.280 --> 00:24:52.280]   We need to segregate it.
[00:24:52.280 --> 00:24:54.400]   So I think data quality will become essential.
[00:24:54.400 --> 00:24:58.760]   I think the models will become relatively small, but then on the edge they become even
[00:24:58.760 --> 00:24:59.760]   smaller.
[00:24:59.760 --> 00:25:05.960]   So you have this hybridized experience, like when you use the neural filters in Photoshop,
[00:25:05.960 --> 00:25:10.000]   there's a point of processing of the cloud and then it remains the render processing
[00:25:10.000 --> 00:25:11.000]   on your computer, right?
[00:25:11.000 --> 00:25:15.800]   And this hybridized experience on Microsoft Flight Simulator will become quite commonplace
[00:25:15.800 --> 00:25:18.000]   for the running efficiently on these models.
[00:25:18.000 --> 00:25:20.720]   But I don't think that models will continue to scale.
[00:25:20.720 --> 00:25:24.040]   Like we'll see a trillion parameter model or something like that.
[00:25:24.040 --> 00:25:28.680]   But instead I think kind of the almost MOE approach where you have multiple models that
[00:25:28.680 --> 00:25:30.800]   are good at various things will be key to this.
[00:25:30.800 --> 00:25:36.680]   Like right now on the stable diffusion example, you're seeing people using Dreambooth, right?
[00:25:36.680 --> 00:25:41.040]   To create a GTA model, an Elden Ring model or something like that.
[00:25:41.040 --> 00:25:44.880]   That's an optimal way rather than having potentially one model that can do everything.
[00:25:44.880 --> 00:25:46.240]   But we're not quite sure.
[00:25:46.240 --> 00:25:49.360]   And Dreambooth maybe isn't the best way to, maybe it's 5K networks or something else.
[00:25:49.360 --> 00:25:53.200]   But I think different models for different things and your own personal model, like a
[00:25:53.200 --> 00:25:56.640]   million models is the better way than one model that can do everything, even though
[00:25:56.640 --> 00:25:59.760]   that's very attractive because it's like, yeah, let's just chuck it in.
[00:25:59.760 --> 00:26:02.840]   And we've seen these development of skills and you've scaled up.
[00:26:02.840 --> 00:26:04.040]   So I think scale is everything.
[00:26:04.040 --> 00:26:08.200]   Now I think data quality will be everything and model usage for Instruct models will be
[00:26:08.200 --> 00:26:09.200]   everything.
[00:26:09.200 --> 00:26:12.600]   Either value is going to shift there, even as compute becomes plentiful to allow for
[00:26:12.600 --> 00:26:14.520]   ridiculous scale.
[00:26:14.520 --> 00:26:17.760]   So you predict that kind of reverse of the current trends of people building bigger and
[00:26:17.760 --> 00:26:18.760]   bigger models.
[00:26:18.760 --> 00:26:20.760]   You actually think they're going to start to get smaller.
[00:26:20.760 --> 00:26:28.920]   Well, I mean, like Instruct GPT is a 1.3 billion parameters as performant as GPT-3, right?
[00:26:28.920 --> 00:26:33.240]   Similarly if you look at T5 Flan and some of these other models that Google had released
[00:26:33.240 --> 00:26:35.880]   recently, the most performant models out there.
[00:26:35.880 --> 00:26:38.360]   Because like these are big neurons.
[00:26:38.360 --> 00:26:39.760]   You don't need all of that stuff.
[00:26:39.760 --> 00:26:45.040]   You know, similarly, like with the compute scarcity, relatively speaking, we just chucked
[00:26:45.040 --> 00:26:47.840]   a lot of random data into these things.
[00:26:47.840 --> 00:26:51.960]   But if you think of these models like a bit like the human brain, what's better, just
[00:26:51.960 --> 00:26:55.800]   a diet of like every piece of media out there or just the media that you need?
[00:26:55.800 --> 00:26:56.800]   Yeah.
[00:26:56.800 --> 00:26:58.520]   And what does that look like for these models?
[00:26:58.520 --> 00:26:59.520]   We don't know yet.
[00:26:59.520 --> 00:27:03.520]   You know, also it's moving so quickly that we haven't been able to keep up.
[00:27:03.520 --> 00:27:07.160]   Like a year ago, if I told you the image models would be like they are now, you'd be like,
[00:27:07.160 --> 00:27:08.160]   no way.
[00:27:08.160 --> 00:27:09.480]   Like even I can't believe it.
[00:27:09.480 --> 00:27:10.480]   Right.
[00:27:10.480 --> 00:27:12.880]   And so this opens up a big questions like why is an image model?
[00:27:12.880 --> 00:27:16.960]   Why stable diffusion two gigabytes and 890 million parameters?
[00:27:16.960 --> 00:27:22.760]   Whereas you've got 175 billion parameters of GPT-3, you know, what's the amount of information
[00:27:22.760 --> 00:27:23.760]   they can convey?
[00:27:23.760 --> 00:27:26.360]   Does it make sense that text is so much bigger than image?
[00:27:26.360 --> 00:27:27.360]   I don't know.
[00:27:27.360 --> 00:27:30.960]   I mean, it seems plausible that it's bigger than image.
[00:27:30.960 --> 00:27:37.000]   I mean, I guess my understanding was that these models, at least the language models
[00:27:37.000 --> 00:27:43.960]   sort of generally get better on a broad set of benchmarks as the model size grows.
[00:27:43.960 --> 00:27:46.000]   But I mean, certainly other things matter.
[00:27:46.000 --> 00:27:47.000]   No, they do.
[00:27:47.000 --> 00:27:48.000]   And again, this has been shown.
[00:27:48.000 --> 00:27:51.560]   But then, like I said, Tenshila paper showed that they also get better as you train them
[00:27:51.560 --> 00:27:53.040]   more for similar set of parameters.
[00:27:53.040 --> 00:27:59.960]   So 67 billion parameter, five times train model can outperform 180 billion parameter
[00:27:59.960 --> 00:28:01.760]   model effectively.
[00:28:01.760 --> 00:28:04.080]   But then you see other things like with image models, it's the same.
[00:28:04.080 --> 00:28:09.280]   Google has a different type of model called Party, whereby they scaled it to 20 billion
[00:28:09.280 --> 00:28:12.600]   parameters and it learned like language and things like that on the way.
[00:28:12.600 --> 00:28:16.360]   But like I said, stable diffusion being this performance at just a couple of gigabytes,
[00:28:16.360 --> 00:28:20.560]   890 million parameters makes you question what happens if we start optimizing the data?
[00:28:20.560 --> 00:28:25.160]   Because like we just chucked in an unfiltered data set, relatively speaking, like some of
[00:28:25.160 --> 00:28:28.400]   the bad stuff removed, just 2 billion images into that.
[00:28:28.400 --> 00:28:31.360]   What's the minimum number of images to have stable diffusion quality output?
[00:28:31.360 --> 00:28:32.840]   Is it 12 million?
[00:28:32.840 --> 00:28:38.320]   The model that Catherine Arley, General Chef Coder released in December of last year, CC12M,
[00:28:38.320 --> 00:28:41.640]   that was used for the original version of Mid Journey and a lot of stuff was only 12
[00:28:41.640 --> 00:28:43.320]   million images.
[00:28:43.320 --> 00:28:44.320]   How many images do you need?
[00:28:44.320 --> 00:28:46.080]   How much text do you need?
[00:28:46.080 --> 00:28:48.120]   And then what effect does that have on the size of the models?
[00:28:48.120 --> 00:28:51.560]   I think it's all scaling rules anymore.
[00:28:51.560 --> 00:28:55.760]   Even as, like I said, the compute becomes available now to scale infinitely, like, geez,
[00:28:55.760 --> 00:28:57.800]   some of the clusters I see being built are insane.
[00:28:57.800 --> 00:29:00.360]   Well, I guess it's sort of surprising.
[00:29:00.360 --> 00:29:01.480]   It's interesting.
[00:29:01.480 --> 00:29:05.960]   Your insight maybe, as you put it earlier, was to, you know, that people really needed
[00:29:05.960 --> 00:29:10.040]   this massive compute to make it broadly available.
[00:29:10.040 --> 00:29:13.440]   But then it's sort of, it's kind of an interesting contrast to your current prediction that the
[00:29:13.440 --> 00:29:17.520]   models will become smaller and more specific.
[00:29:17.520 --> 00:29:23.160]   Does that make you think that, does that make you have any sort of plans to sort of change
[00:29:23.160 --> 00:29:29.360]   resource allocation or the kinds of compute that you want to sort of get ready for researchers?
[00:29:29.360 --> 00:29:33.480]   Yeah, I think we basically don't need to infinitely scale compute anymore, right?
[00:29:33.480 --> 00:29:37.520]   It becomes then about data acquisition and we're building out a couple of dozen people
[00:29:37.520 --> 00:29:41.160]   data team to provide the right data for open source research.
[00:29:41.160 --> 00:29:45.560]   You can think data quality is underestimated in terms of its importance right now for these
[00:29:45.560 --> 00:29:49.320]   models because people are like scale is all you need, stacking layers, you know?
[00:29:49.320 --> 00:29:52.800]   And it was difficult to build a cluster even in the thousands of A100s just because there
[00:29:52.800 --> 00:29:54.200]   wasn't availability.
[00:29:54.200 --> 00:29:59.560]   But now you look at next year, I know of three 20,000 H100 clusters that are being built.
[00:29:59.560 --> 00:30:03.160]   A H100 is probably about three times as performant as an A100.
[00:30:03.160 --> 00:30:08.440]   So that's like 60,000 A100s, like 15 times bigger than our cluster.
[00:30:08.440 --> 00:30:13.560]   It can train the GPT-3 probably in like six hours or something like that, one of these
[00:30:13.560 --> 00:30:14.560]   clusters.
[00:30:14.560 --> 00:30:16.360]   So compute is no longer really a bottleneck.
[00:30:16.360 --> 00:30:19.800]   But I think what we'll see again is that people will take the standardized models and they'll
[00:30:19.800 --> 00:30:23.040]   customize them down and then have a load of different models.
[00:30:23.040 --> 00:30:24.840]   And maybe there'll be one or two more big models.
[00:30:24.840 --> 00:30:26.840]   But I think it's not about big models anymore.
[00:30:26.840 --> 00:30:28.760]   It's about optimal models.
[00:30:28.760 --> 00:30:36.160]   And we don't know what optimal foundation model is across a data training and other
[00:30:36.160 --> 00:30:41.400]   architectural parameters yet because we've been so constrained by compute and data and
[00:30:41.400 --> 00:30:42.560]   talent.
[00:30:42.560 --> 00:30:45.800]   And each of those is being unlocked right now.
[00:30:45.800 --> 00:30:46.800]   That's really cool.
[00:30:46.800 --> 00:30:51.160]   I mean, what kinds of data sets are you thinking about building?
[00:30:51.160 --> 00:30:56.120]   So like, we're talking to national governments about like national broadcaster data.
[00:30:56.120 --> 00:31:00.640]   Like you've got really interesting, highly structured things there that are high quality
[00:31:00.640 --> 00:31:03.620]   versus crawls of the internet.
[00:31:03.620 --> 00:31:05.680]   And these are public goods that should be available, right?
[00:31:05.680 --> 00:31:06.880]   And sorry, what would that be?
[00:31:06.880 --> 00:31:08.040]   I'm not familiar with-
[00:31:08.040 --> 00:31:12.680]   Well, so like you have PBS in the US, right?
[00:31:12.680 --> 00:31:16.320]   Like their data should be available for model creation for academia, right?
[00:31:16.320 --> 00:31:17.320]   Oh, I see.
[00:31:17.320 --> 00:31:22.360]   So you would just sort of acquire that data set or somehow get a license to make it available?
[00:31:22.360 --> 00:31:23.360]   Exactly.
[00:31:23.360 --> 00:31:27.640]   So researchers initially, and then hopefully more people, because again, this is public.
[00:31:27.640 --> 00:31:33.080]   It's paid for by the people, so it should be available to the people in various ways.
[00:31:33.080 --> 00:31:40.600]   Or even training a model on all the PBS radio station work, and they've all got like transcripts.
[00:31:40.600 --> 00:31:45.280]   You could do it in various different ways from pretty synthetic data sets off that.
[00:31:45.280 --> 00:31:49.440]   So looking at some of these media data sets has been quite interesting to us, but then
[00:31:49.440 --> 00:31:51.080]   in other areas, it's about more than that.
[00:31:51.080 --> 00:31:56.120]   So like open bioML, we're doing the usual protein folding and some DNA stuff and sporting
[00:31:56.120 --> 00:32:00.440]   and things there, but in bioML, there's just a lack of quality data.
[00:32:00.440 --> 00:32:04.060]   So one of the things we'll probably do soon, we're just deciding on this as a prize to
[00:32:04.060 --> 00:32:07.920]   basically identify what data sets should be built, and then bringing in external funders
[00:32:07.920 --> 00:32:09.280]   to help build those data sets.
[00:32:09.280 --> 00:32:12.560]   So protein folding was quite good because that was a great data set, and there was an
[00:32:12.560 --> 00:32:16.040]   objective function or quality, and so people could build around that.
[00:32:16.040 --> 00:32:19.720]   So you have open fold, you have LibreFold that we are doing and other things to make
[00:32:19.720 --> 00:32:21.160]   that more and more efficient.
[00:32:21.160 --> 00:32:23.600]   Other things in bioML don't have that.
[00:32:23.600 --> 00:32:27.120]   Within the language thing, we're doing the pile version twos.
[00:32:27.120 --> 00:32:31.320]   The pile version one from Alutha was very widely used, and the pile version two is much
[00:32:31.320 --> 00:32:32.320]   bigger.
[00:32:32.320 --> 00:32:34.680]   With images, we had Lion.
[00:32:34.680 --> 00:32:40.960]   So Lion were the first largest image data set was 100 million images, YFCC 100M, which
[00:32:40.960 --> 00:32:43.920]   was the Flickr data set from 2013.
[00:32:43.920 --> 00:32:50.160]   Lion did Lion 400M, which is 400 million images, text pairs, last year, and that was used by
[00:32:50.160 --> 00:32:54.760]   Google and Meta and a whole bunch of others in their models.
[00:32:54.760 --> 00:32:57.560]   That's how good it was because Google and Meta and others were actually constrained
[00:32:57.560 --> 00:33:02.540]   about using their user data because of FCC regulations and other things, weirdly enough.
[00:33:02.540 --> 00:33:06.960]   Now they've done Lion 5B, which is 5 billion images parameters, actually 5.8, and they're
[00:33:06.960 --> 00:33:07.960]   going to go even bigger.
[00:33:07.960 --> 00:33:11.920]   So it's creating these big open source data sets, replacing a lot of the scraped lower
[00:33:11.920 --> 00:33:16.480]   quality stuff with some of this public sector data and encouraging others to contribute
[00:33:16.480 --> 00:33:20.760]   to it, and then building great data sets for every modality so that everyone, again, is
[00:33:20.760 --> 00:33:21.760]   on the same page.
[00:33:21.760 --> 00:33:26.000]   I think we've got to the point now where the communities that we support and our own internal
[00:33:26.000 --> 00:33:29.760]   teams are building better data sets in some cases than even private companies have access
[00:33:29.760 --> 00:33:30.760]   to.
[00:33:30.760 --> 00:33:31.760]   Yeah.
[00:33:31.760 --> 00:33:36.560]   I think one of the disconnects that we see, talking to a lot of researchers and talking
[00:33:36.560 --> 00:33:41.960]   to a lot of companies, of course, there's a lot of overlap in applications and deep
[00:33:41.960 --> 00:33:44.400]   learning is incredibly practical in lots of ways.
[00:33:44.400 --> 00:33:53.000]   But I think a lot of companies are looking for more research around time series and structured
[00:33:53.000 --> 00:33:54.000]   data.
[00:33:54.000 --> 00:33:57.220]   Do you think about investing in that realm at all?
[00:33:57.220 --> 00:34:01.320]   So we've had some approaches for time series analysis and things like that.
[00:34:01.320 --> 00:34:05.200]   I'm not sure these foundation models are the best things for that, to be honest, because
[00:34:05.200 --> 00:34:07.920]   I view them more like principle-based analysis in the brain.
[00:34:07.920 --> 00:34:13.440]   Like, you know, with my son with his autism and ASD, the main thing about that is there's
[00:34:13.440 --> 00:34:14.960]   typically a GABA/glutamate imbalance.
[00:34:14.960 --> 00:34:17.960]   So GABA calms you down when you pop valium and glutamate excites you, so there's too
[00:34:17.960 --> 00:34:18.960]   much noise.
[00:34:18.960 --> 00:34:22.480]   And then once you calm down that noise, you do repetitive trial teaching so that you can
[00:34:22.480 --> 00:34:23.480]   rebuild things like words.
[00:34:23.480 --> 00:34:26.800]   Because if there's too much noise, you can't learn the connections between concepts and
[00:34:26.800 --> 00:34:27.800]   words.
[00:34:27.800 --> 00:34:31.440]   Like a cup is a world cup, cup your hands, all the different kind of cup meanings.
[00:34:31.440 --> 00:34:32.920]   And then you rebuild that.
[00:34:32.920 --> 00:34:35.920]   These models are the same and they can figure out the latent spaces or hidden meanings of
[00:34:35.920 --> 00:34:39.240]   connections between different labeled datasets.
[00:34:39.240 --> 00:34:42.080]   And with time series and things like that, I'm not sure this is the appropriate thing
[00:34:42.080 --> 00:34:43.080]   for that.
[00:34:43.080 --> 00:34:48.040]   Again, we're funding a little bit of research in that area, but I think that a lot more
[00:34:48.040 --> 00:34:52.680]   of the classical ML things are a lot better to do that because you typically don't do
[00:34:52.680 --> 00:34:54.600]   out-of-sample kind of stuff there.
[00:34:54.600 --> 00:35:00.120]   Kind of like looking at the hedge fund stuff, you are typically inferencing and extrapolating
[00:35:00.120 --> 00:35:05.520]   versus trying to do first principles analysis of like what is a Van Gogh painting mixed
[00:35:05.520 --> 00:35:08.320]   with a Banksy painting and these types of things.
[00:35:08.320 --> 00:35:16.840]   But again, I think 80% of research now in AI, I think this is in the AI index reports
[00:35:16.840 --> 00:35:20.800]   that was released by Stanford, is in foundation models.
[00:35:20.800 --> 00:35:26.500]   So we're one area of funding of this, and again, quite focused around media and language.
[00:35:26.500 --> 00:35:29.180]   This is the whole world of funding around this area.
[00:35:29.180 --> 00:35:32.440]   So if it is useful for time series, I'm sure we'll find out sooner rather than later.
[00:35:32.440 --> 00:35:33.440]   Or maybe we won't.
[00:35:33.440 --> 00:35:36.040]   Maybe they'll just take it, run a hedge fund and be like, "Ha, ha, ha.
[00:35:36.040 --> 00:35:37.040]   Get all the money."
[00:35:37.040 --> 00:35:43.320]   Do you have an opinion on kind of other architectures?
[00:35:43.320 --> 00:35:45.320]   Are you seeing anything?
[00:35:45.320 --> 00:35:52.560]   I sort of feel like it's amazing the convergence around transformers in so many different applications.
[00:35:52.560 --> 00:35:56.760]   Do you see any signs of that changing?
[00:35:56.760 --> 00:35:59.960]   I think there are some promising things that I've kind of seen.
[00:35:59.960 --> 00:36:03.000]   You don't necessarily need attention.
[00:36:03.000 --> 00:36:06.600]   Some recent papers kind of have shown, trying to remember which ones.
[00:36:06.600 --> 00:36:13.200]   And there's some attention free transformer stuff being done with kind of one of the projects
[00:36:13.200 --> 00:36:18.600]   that we're supporting around RWKB on the language model side.
[00:36:18.600 --> 00:36:22.560]   I think transformers are probably going to be the primary way of things for the next
[00:36:22.560 --> 00:36:25.520]   couple of years at least, just because they've got momentum, they have talent.
[00:36:25.520 --> 00:36:29.000]   And again, the commonality of architectures around this, you're like, "Hey, let's just
[00:36:29.000 --> 00:36:30.800]   chuck it at this or that or that."
[00:36:30.800 --> 00:36:32.120]   And you're like, "It works."
[00:36:32.120 --> 00:36:33.760]   And we're just scratching the surface.
[00:36:33.760 --> 00:36:39.320]   Like for those who don't know, kind of for images, last year we had kind of, well, the
[00:36:39.320 --> 00:36:42.880]   big breakthrough in kind of January, February by like Ryan Murdoch and Catherine Krauss
[00:36:42.880 --> 00:36:47.920]   and some others was to take the open source clip model that OpenAI released, a generative
[00:36:47.920 --> 00:36:49.280]   model of EQGAN.
[00:36:49.280 --> 00:36:55.920]   And that was Robin Rombach who kind of did that one with his team, Conviz.
[00:36:55.920 --> 00:37:00.240]   And having a generative model and a model that takes image to text and bouncing them
[00:37:00.240 --> 00:37:04.480]   back and forth across each other to guide the output, to get more and more coherent
[00:37:04.480 --> 00:37:05.480]   stuff.
[00:37:05.480 --> 00:37:09.760]   In December, Catherine postulated clip conditioning would be the best way, taking a clip model,
[00:37:09.760 --> 00:37:13.800]   the language model and a diffusion generative model and combining them together.
[00:37:13.800 --> 00:37:15.680]   And somehow it learned the stuff.
[00:37:15.680 --> 00:37:22.080]   Then Google with the Imogen team took a language model, T5XXL, that was a pure language model
[00:37:22.080 --> 00:37:26.560]   and mixed it together with a diffusion model and somehow it learned how to write images
[00:37:26.560 --> 00:37:27.560]   and it got even better.
[00:37:27.560 --> 00:37:29.720]   And everyone was like, "Wait, what?"
[00:37:29.720 --> 00:37:33.960]   Like we still don't exactly know how these things work to be honest and the potential
[00:37:33.960 --> 00:37:35.080]   of extending these.
[00:37:35.080 --> 00:37:37.680]   So I think transformers have a long way to go.
[00:37:37.680 --> 00:37:41.880]   But again, like there's a paper that I don't know if you saw it, the number of papers on
[00:37:41.880 --> 00:37:47.240]   archive, it's literally an exponential with a 24 month doubling on ML.
[00:37:47.240 --> 00:37:49.000]   Like it's just going crazy everywhere.
[00:37:49.000 --> 00:37:53.600]   Like who knows what people are going to come up with and the interest in this area compared
[00:37:53.600 --> 00:37:57.680]   to basically the rest of the global economy, this means there'll be more and more resources
[00:37:57.680 --> 00:38:02.520]   just deployed towards this because finally actually showing usefulness.
[00:38:02.520 --> 00:38:06.240]   It's just where that usefulness and value will lie, nobody really knows.
[00:38:06.240 --> 00:38:11.520]   But until then, just take some data and chuck it into the H100s, stir it up and see what
[00:38:11.520 --> 00:38:14.080]   pops out the other side.
[00:38:14.080 --> 00:38:21.280]   It seems a little surprising that you have this amazing company that does all this like
[00:38:21.280 --> 00:38:27.520]   cutting edge research in ML and model generation and the killer application or the first really
[00:38:27.520 --> 00:38:30.880]   big application is sort of generating media.
[00:38:30.880 --> 00:38:33.280]   I just never would have kind of thought that a priori.
[00:38:33.280 --> 00:38:39.520]   Do you have other areas that you sort of expect to take off or that you're looking into?
[00:38:39.520 --> 00:38:42.000]   I think you shouldn't underestimate media.
[00:38:42.000 --> 00:38:44.880]   Like the easiest way for us to communicate is doing what we're doing now.
[00:38:44.880 --> 00:38:47.200]   We're having a chat with our words, right?
[00:38:47.200 --> 00:38:51.600]   The next hardest is that we're writing each other emails or chats, you know, and to write
[00:38:51.600 --> 00:38:53.820]   a really good one is very hard, right?
[00:38:53.820 --> 00:38:57.560]   Like I made this message long because I could not spare the effort to make it shorter, I
[00:38:57.560 --> 00:38:58.560]   think someone once said.
[00:38:58.560 --> 00:39:02.200]   The hardest thing for us to do is communicate visually as a species.
[00:39:02.200 --> 00:39:03.760]   So this is why artists are great.
[00:39:03.760 --> 00:39:07.200]   You know, PowerPoints, we've all been there and stuck there.
[00:39:07.200 --> 00:39:11.000]   With the combination of a language model, a vision model and a same generation model
[00:39:11.000 --> 00:39:13.480]   and a code model, you don't need PowerPoint anymore.
[00:39:13.480 --> 00:39:17.280]   You can just speak and it'll create beautiful slides every time, right?
[00:39:17.280 --> 00:39:22.920]   Like with art and visual communication, anyone now, my mom could create memes and send it
[00:39:22.920 --> 00:39:27.600]   to me about why I don't call her enough in an instant, you know, which she does.
[00:39:27.600 --> 00:39:31.600]   Like humanity can finally communicate both through text now with these language models
[00:39:31.600 --> 00:39:37.600]   and you've seen how things like copy.ai and pseudo rights and Jasper have made that easier
[00:39:37.600 --> 00:39:39.080]   and now visually as well.
[00:39:39.080 --> 00:39:41.400]   And the next step will be 3D.
[00:39:41.400 --> 00:39:45.120]   Like that's a change in the way humanity communicates, which is a huge deal.
[00:39:45.120 --> 00:39:49.960]   So again, language was valuable, but it was really getting that you already had help like
[00:39:49.960 --> 00:39:54.680]   your Gmail suggesting you could tell him to bugger off in your replies or whatever, right?
[00:39:54.680 --> 00:39:59.480]   Now it's kind of the next step there, which is image and then 3D and things like that
[00:39:59.480 --> 00:40:04.400]   will follow and that's valuable because again, we have to look at where the money is.
[00:40:04.400 --> 00:40:09.040]   The previous iteration of the web was all about AI being used to target you ads.
[00:40:09.040 --> 00:40:12.960]   You know, now it's about something else where you're moving from maybe consumption to creation.
[00:40:12.960 --> 00:40:16.600]   So my focus has been in this area as a main driver there.
[00:40:16.600 --> 00:40:22.040]   But in terms of impact and global stuff, the ability to switch between structured and unstructured
[00:40:22.040 --> 00:40:25.840]   data dynamically at a human level, because I understand the principles when combined
[00:40:25.840 --> 00:40:29.840]   with like retrieval augmentation, other things, check for factual accuracy.
[00:40:29.840 --> 00:40:34.520]   It's such a huge deal because it means that you can write reports, you can do legal stuff,
[00:40:34.520 --> 00:40:36.800]   you can get rid of bureaucracy.
[00:40:36.800 --> 00:40:42.120]   You know, it's the first technology that enables so many things because it's so general that
[00:40:42.120 --> 00:40:43.600]   we're not sure where the value will be.
[00:40:43.600 --> 00:40:51.040]   But I do see the value in anyone being able to express themselves and communicate better.
[00:40:51.040 --> 00:40:56.320]   And so I think that we shouldn't underestimate that particular aspect of things.
[00:40:56.320 --> 00:41:00.600]   I guess I also want to ask you, and you've talked about this a fair amount, but I'd love
[00:41:00.600 --> 00:41:02.920]   to hear directly.
[00:41:02.920 --> 00:41:07.720]   You know, you made this kind of decision to make all your models really open in kind of
[00:41:07.720 --> 00:41:14.280]   contrast to what OpenAI and others were doing, which I think people got really excited about
[00:41:14.280 --> 00:41:19.720]   because it sort of felt like, I think with some of the earlier models, there were kind
[00:41:19.720 --> 00:41:22.600]   of these gatekeepers, like no one could really access them.
[00:41:22.600 --> 00:41:27.440]   You know, some models, like really like no one except people at the company could access.
[00:41:27.440 --> 00:41:32.200]   But I do think that, I remember the reason that some of these models didn't get opened
[00:41:32.200 --> 00:41:37.240]   up was sort of, or said to be kind of ethical concerns at the time.
[00:41:37.240 --> 00:41:41.640]   Do you think that there's any kind of merit to that argument?
[00:41:41.640 --> 00:41:42.960]   Do you think about that at all?
[00:41:42.960 --> 00:41:48.120]   Like kind of models being used to like trick people or spam people or things like that?
[00:41:48.120 --> 00:41:52.080]   Well, I think the, you know, it's a valid point of view.
[00:41:52.080 --> 00:41:56.520]   I mean, basically the logic there is similar to the logic of orthodox and ultra-orthodox
[00:41:56.520 --> 00:42:01.040]   religions, which say anything that leads to a sin in itself is sinful.
[00:42:01.040 --> 00:42:04.720]   And so just in case, but it's understandable because these models are so powerful that
[00:42:04.720 --> 00:42:08.760]   you move from a risk minimization framework, where you've got an expected utility.
[00:42:08.760 --> 00:42:10.320]   What's the positives, what's the negatives?
[00:42:10.320 --> 00:42:12.240]   And you try to figure that out roughly, right?
[00:42:12.240 --> 00:42:13.400]   To a regret minimax.
[00:42:13.400 --> 00:42:16.920]   If I release this model and something goes wrong, my company could get blown up.
[00:42:16.920 --> 00:42:18.520]   I minimize my maximum regret.
[00:42:18.520 --> 00:42:21.360]   And we don't know what it can be usable because it can be used for anything.
[00:42:21.360 --> 00:42:27.280]   However, I think the last few years have shown this GPT-2, too powerful to release, GPT-Neo
[00:42:27.280 --> 00:42:29.360]   and the other things come along.
[00:42:29.360 --> 00:42:31.160]   The world hasn't ended, you know?
[00:42:31.160 --> 00:42:34.760]   Stable diffusion has been in the hands of 4chan now for 10 weeks.
[00:42:34.760 --> 00:42:38.480]   And all they've basically created is like Cronenbergs that have given themselves nightmares.
[00:42:38.480 --> 00:42:41.080]   Like it's not great at creating these things.
[00:42:41.080 --> 00:42:43.400]   The bad guys already have the technology, you know?
[00:42:43.400 --> 00:42:47.160]   The nation states Russia has tens of thousands of A100s, right?
[00:42:47.160 --> 00:42:48.160]   And the people can't run.
[00:42:48.160 --> 00:42:49.560]   So they can build it.
[00:42:49.560 --> 00:42:53.560]   And we don't have immunity to this new alien technology being out there.
[00:42:53.560 --> 00:42:57.720]   Because ultimately we live in a society that regulates against stuff.
[00:42:57.720 --> 00:43:03.680]   So if you are creating bad things, you'll be regulated against, you know?
[00:43:03.680 --> 00:43:09.720]   If you are using it for bad purposes, again, the means of distribution of the social networks
[00:43:09.720 --> 00:43:11.840]   that have rules and regulations in place.
[00:43:11.840 --> 00:43:14.280]   Because what you're really trying to regulate is not content.
[00:43:14.280 --> 00:43:17.680]   As bad content is bad, you're trying to regulate behavior and then it's about who's allowed
[00:43:17.680 --> 00:43:20.560]   within these communities and not allowed within these communities.
[00:43:20.560 --> 00:43:22.360]   So I think all this stuff gets mixed up.
[00:43:22.360 --> 00:43:26.560]   And then the other aspect of it is this AI safety alignment issue, right?
[00:43:26.560 --> 00:43:28.080]   Of the technology killing us all.
[00:43:28.080 --> 00:43:32.400]   I will say quite clearly, I think that GPT-4 when it comes will be more dangerous than
[00:43:32.400 --> 00:43:35.760]   GPT-4chan, you know?
[00:43:35.760 --> 00:43:40.640]   Because a model like GPT-4chan that was trained by Yannick on 4chan that produces just pure
[00:43:40.640 --> 00:43:43.680]   RL rubbish isn't really going to go anywhere.
[00:43:43.680 --> 00:43:45.760]   It's just going to produce pure RL rubbish a bit easier.
[00:43:45.760 --> 00:43:49.680]   Whereas a GPT-4, which God knows what it'll be, but I'm sure they'll do an amazing piece
[00:43:49.680 --> 00:43:54.840]   of work, the large models that they're creating now are getting to human level and we don't
[00:43:54.840 --> 00:43:56.920]   know how exactly they work.
[00:43:56.920 --> 00:44:01.480]   And they're being created by unregulated entities with these models that are powerful as any
[00:44:01.480 --> 00:44:03.120]   technology out there.
[00:44:03.120 --> 00:44:04.600]   Small models are not the issue.
[00:44:04.600 --> 00:44:07.840]   Being widely used in the community regulating it, big models are the issue.
[00:44:07.840 --> 00:44:11.240]   And we should have more oversight on that just in case some of this AI alignment stuff
[00:44:11.240 --> 00:44:15.840]   turns out to be correct and these things are dangerous, which I think they probably are.
[00:44:15.840 --> 00:44:19.720]   But I guess you believe that these small models are also very powerful.
[00:44:19.720 --> 00:44:26.440]   So why would the regulations be different for the size of the model?
[00:44:26.440 --> 00:44:28.480]   Because they're not open, right?
[00:44:28.480 --> 00:44:30.040]   So when they're open, everyone can check it.
[00:44:30.040 --> 00:44:33.240]   So right now everyone's poking around and saying, "Oh, there's artists.
[00:44:33.240 --> 00:44:36.160]   Are they going to be compensated on Lion and this and that?"
[00:44:36.160 --> 00:44:38.120]   And we're like, "Cool, let's have that discussion in the open space.
[00:44:38.120 --> 00:44:39.680]   What's the best mechanism to do this?"
[00:44:39.680 --> 00:44:43.080]   We've got a $200,000 deepfake detection price coming up.
[00:44:43.080 --> 00:44:46.200]   We'll give it to the best implementation of open source deepfake detection.
[00:44:46.200 --> 00:44:48.040]   It's available for everyone and everyone can be a part of it.
[00:44:48.040 --> 00:44:50.240]   Whereas the big guys, there is no control.
[00:44:50.240 --> 00:44:56.400]   Like again, the example I gave a bit earlier, imagine that Apple or Amazon or Google or
[00:44:56.400 --> 00:45:00.520]   someone integrated emotional text to speech into their models, right?
[00:45:00.520 --> 00:45:05.080]   So Siri suddenly has a very alluring type voice and whispers to you that you should
[00:45:05.080 --> 00:45:07.120]   be buying stuff.
[00:45:07.120 --> 00:45:08.760]   You'll probably buy it more.
[00:45:08.760 --> 00:45:09.920]   Is that going to be regulated?
[00:45:09.920 --> 00:45:12.680]   It's not currently and it won't be in time.
[00:45:12.680 --> 00:45:15.000]   Whereas putting these models out into the open will get people to think about, "Actually,
[00:45:15.000 --> 00:45:17.320]   that's something that probably should be regulated."
[00:45:17.320 --> 00:45:22.680]   And if something is regulated, that is fine because it's a democratic process.
[00:45:22.680 --> 00:45:26.560]   Whereas companies using this technology to manipulate us, literally, because that's the
[00:45:26.560 --> 00:45:29.640]   advertising model I don't think is appropriate.
[00:45:29.640 --> 00:45:33.240]   And again, it's not just Western influence and deepfakes and elections and stuff like
[00:45:33.240 --> 00:45:38.520]   that because when you look at that, there is a herd immunity thing, not COVID type thing.
[00:45:38.520 --> 00:45:40.360]   Again, it's lots more than COVID.
[00:45:40.360 --> 00:45:44.320]   People understand this technology will mean that people will be more discerning over curated
[00:45:44.320 --> 00:45:47.360]   outputs and then it will be a mixture of this detection technology.
[00:45:47.360 --> 00:45:52.640]   And then for example, we're part of contentauthenticity.org where all our future models will have exit
[00:45:52.640 --> 00:45:58.120]   files, well, special metadata files showing that they are generated by default on the
[00:45:58.120 --> 00:45:59.120]   package.
[00:45:59.120 --> 00:46:00.640]   Now, will people choose to use them or not?
[00:46:00.640 --> 00:46:04.880]   They may choose not to use them, in which case they won't have a tick next to them.
[00:46:04.880 --> 00:46:08.680]   So there are all sorts of ways to do this, but the reality is that again, this is a complex
[00:46:08.680 --> 00:46:14.280]   debate that cannot be decided basically in San Francisco.
[00:46:14.280 --> 00:46:17.980]   It's something that is important because this technology is never to be around the world.
[00:46:17.980 --> 00:46:21.520]   And if you actually poke people and you say, "Okay, so you don't want this technology to
[00:46:21.520 --> 00:46:22.520]   be used by Indians?"
[00:46:22.520 --> 00:46:24.080]   They're like, "Well, of course we do."
[00:46:24.080 --> 00:46:25.080]   "When?"
[00:46:25.080 --> 00:46:26.080]   "When it's safe to."
[00:46:26.080 --> 00:46:27.080]   "Who decides that?"
[00:46:27.080 --> 00:46:28.080]   "We do."
[00:46:28.080 --> 00:46:29.080]   So they're not smart enough to decide it?
[00:46:29.080 --> 00:46:30.080]   No, they need to be educated.
[00:46:30.080 --> 00:46:31.080]   It's not bad, right?
[00:46:31.080 --> 00:46:38.400]   But again, I think it's understandable because it's scary and cool and scary all at the same
[00:46:38.400 --> 00:46:39.400]   time.
[00:46:39.400 --> 00:46:45.800]   Are there any applications currently of the models that you've built that make you uncomfortable
[00:46:45.800 --> 00:46:49.260]   that you would like to try to prevent?
[00:46:49.260 --> 00:46:57.000]   So like there was an example of a Dreambooth model being trained on a specific artist style.
[00:46:57.000 --> 00:47:04.480]   And so it was like a cute Teen Titans type artist and it was announced and released as
[00:47:04.480 --> 00:47:07.920]   that artist's model, but they had nothing to do with it.
[00:47:07.920 --> 00:47:11.940]   And I feel uncomfortable with that because I don't think that styles can be copyrighted,
[00:47:11.940 --> 00:47:17.920]   but it was like almost this co-opting of the name of that artist to do this, like eventually
[00:47:17.920 --> 00:47:19.120]   it got changed after discussion.
[00:47:19.120 --> 00:47:20.880]   There was a piece about that.
[00:47:20.880 --> 00:47:23.640]   Like we're entering some of these gray areas where we have to decide these things.
[00:47:23.640 --> 00:47:27.040]   I'm going to have to figure out things like attribution mechanisms and other stuff.
[00:47:27.040 --> 00:47:33.200]   You know, like DeepFaceLab has existed for years now as 35,000 GitHub stars for doing
[00:47:33.200 --> 00:47:35.000]   deep fakes at high quality.
[00:47:35.000 --> 00:47:37.720]   Maybe with this technology you can use it a bit easier, but that's the inevitable pace
[00:47:37.720 --> 00:47:38.720]   of it.
[00:47:38.720 --> 00:47:43.800]   I think we have to figure out some of these things around attribution, around kind of
[00:47:43.800 --> 00:47:48.360]   giving back and around making sure that people's things are used appropriately, right?
[00:47:48.360 --> 00:47:53.080]   Because like in general with attribution and copyright and things like this, these models
[00:47:53.080 --> 00:47:55.800]   do not create replicas when they're doing the training.
[00:47:55.800 --> 00:48:01.120]   If you look at how a diffusion model works in particular, and they just learn principles.
[00:48:01.120 --> 00:48:03.120]   So again, styles cannot be copyrighted.
[00:48:03.120 --> 00:48:04.120]   It's very difficult to do that.
[00:48:04.120 --> 00:48:07.960]   But when it comes down to the individual basis, I'm still struggling a bit with how do we
[00:48:07.960 --> 00:48:11.800]   prevent that from happening and people co-opting other people's things other than in a court
[00:48:11.800 --> 00:48:12.800]   of law?
[00:48:12.800 --> 00:48:13.800]   Is there any automated system?
[00:48:13.800 --> 00:48:20.200]   Because you have the ethical, moral and legal and community typically enforces kind of moral.
[00:48:20.200 --> 00:48:21.280]   Ethical is a more individual thing.
[00:48:21.280 --> 00:48:23.240]   And we have the Creative Open Air of L license for that.
[00:48:23.240 --> 00:48:24.520]   And legal is obviously for a lot of things.
[00:48:24.520 --> 00:48:26.760]   We don't want things to get down to legal.
[00:48:26.760 --> 00:48:28.480]   It's like, how can you encourage community norms?
[00:48:28.480 --> 00:48:32.920]   I say that's probably the primary one here that just made me a bit uneasy.
[00:48:32.920 --> 00:48:34.560]   I see.
[00:48:34.560 --> 00:48:35.560]   Interesting.
[00:48:35.560 --> 00:48:43.000]   Do you do any kind of like in your APIs that you offer, do you like put restrictions in
[00:48:43.000 --> 00:48:46.440]   there that you don't have in sort of the open source models you use just directly?
[00:48:46.440 --> 00:48:48.060]   Not a hundred percent.
[00:48:48.060 --> 00:48:53.200]   So again, it's a regional specific and it's general and it's very kind of safe for work,
[00:48:53.200 --> 00:48:57.240]   shall we say, because again, it's a private implementation of an open API.
[00:48:57.240 --> 00:49:01.920]   Even with the models like stable diffusion shipped with a safety filter, that's primarily
[00:49:01.920 --> 00:49:06.760]   kind of autographic, lead to based just in case you've gone out with it, you didn't like
[00:49:06.760 --> 00:49:10.600]   the new versions of it will be more accurate to reflecting what you want.
[00:49:10.600 --> 00:49:14.320]   And again, trained on potentially safer datasets, et cetera.
[00:49:14.320 --> 00:49:17.360]   But there's obviously a different bar for a private implementation.
[00:49:17.360 --> 00:49:21.080]   And our basic thing is that these models should be released open as benchmark models with
[00:49:21.080 --> 00:49:22.080]   safety around it.
[00:49:22.080 --> 00:49:24.240]   So like I said, there was a safety filter.
[00:49:24.240 --> 00:49:26.760]   Actually if you trip the safety filter in the open source version, it shows you a picture
[00:49:26.760 --> 00:49:30.560]   of Rick Astley and you can adjust the safety filter or you could turn it off.
[00:49:30.560 --> 00:49:33.060]   And then there's an ethical use license.
[00:49:33.060 --> 00:49:36.360]   Any other suggestions for improvements to that, we'd love to know.
[00:49:36.360 --> 00:49:40.960]   And again, I expect that this technology will proliferate because we catalyzed it.
[00:49:40.960 --> 00:49:48.400]   There was the contributions from kind of Patrick at Runway, from LMU, Confiz team and others.
[00:49:48.400 --> 00:49:50.720]   And it was led by those two developers.
[00:49:50.720 --> 00:49:54.960]   There'll be a variety of models of different types being created by a variety of entities.
[00:49:54.960 --> 00:49:57.320]   And some will be safe for work, some will be not safe for work.
[00:49:57.320 --> 00:50:00.960]   But I think we should try and figure out some standardization norms around this as this
[00:50:00.960 --> 00:50:03.240]   technology starts to proliferate.
[00:50:03.240 --> 00:50:06.200]   But again, that should be a communal process.
[00:50:06.200 --> 00:50:13.520]   You keep mentioning these communal processes and I'm curious, what happens when the community
[00:50:13.520 --> 00:50:16.080]   has deep disagreement with itself?
[00:50:16.080 --> 00:50:18.040]   I imagine that happens all the time.
[00:50:18.040 --> 00:50:23.160]   How do you kind of resolve a community where people might have really different sense of
[00:50:23.160 --> 00:50:26.280]   what's moral and sort of draw lines in different places?
[00:50:26.280 --> 00:50:29.240]   Has that happened yet in your community?
[00:50:29.240 --> 00:50:30.240]   And how do you expect it?
[00:50:30.240 --> 00:50:31.240]   A hundred percent.
[00:50:31.240 --> 00:50:35.280]   It happened after the wake of the stable diffusion release.
[00:50:35.280 --> 00:50:38.200]   And people like this can be used as not safe for work and we don't feel comfortable with
[00:50:38.200 --> 00:50:41.580]   that and supporting that internally within stability.
[00:50:41.580 --> 00:50:45.360]   And so we had discussion as a team and we decided not to release any more not safe for
[00:50:45.360 --> 00:50:49.520]   work models as stability itself.
[00:50:49.520 --> 00:50:50.920]   Now some people weren't happy with that.
[00:50:50.920 --> 00:50:54.680]   Some people, most people were fine with that, but that was kind of easier because it was
[00:50:54.680 --> 00:50:55.680]   a team decision.
[00:50:55.680 --> 00:50:58.920]   On a community basis, that comes down to governance structures.
[00:50:58.920 --> 00:51:02.440]   So right now, one of the things we're doing is we're looking at Luther AI and we want
[00:51:02.440 --> 00:51:06.440]   to spin that out into an independent community because it's got lots of different entities
[00:51:06.440 --> 00:51:08.840]   and it's got lots of different points of view.
[00:51:08.840 --> 00:51:10.320]   What is the appropriate governance structure?
[00:51:10.320 --> 00:51:11.880]   Is it Linux foundation, PyTorch?
[00:51:11.880 --> 00:51:15.320]   There's a lot of OSS kind of things.
[00:51:15.320 --> 00:51:20.800]   It's a bit different because these technologies are not like, what can you do with Linux really,
[00:51:20.800 --> 00:51:21.800]   right?
[00:51:21.800 --> 00:51:22.800]   Yeah, exactly.
[00:51:22.800 --> 00:51:26.120]   Whereas what can you do with the most advanced language model in the world?
[00:51:26.120 --> 00:51:29.100]   It's a lot more complicated and needs a lot more voices there and that's why we're taking
[00:51:29.100 --> 00:51:33.680]   some time just trying to say, this is a governance structure on day one.
[00:51:33.680 --> 00:51:38.000]   We need to make it adaptive because we're not sure exactly where this stuff will go.
[00:51:38.000 --> 00:51:42.560]   Right now, we as stability have a lot of control over GPU access and a lot of this stuff is
[00:51:42.560 --> 00:51:44.720]   kind of the spice.
[00:51:44.720 --> 00:51:49.940]   That shouldn't be the case going forward because no one entity, whether it's us, OpenAI, DeedMind
[00:51:49.940 --> 00:51:54.280]   or another, should have control over this technology that's a common good.
[00:51:54.280 --> 00:52:00.100]   And so again, we want to be contributors to an independent not-for-profit as it were,
[00:52:00.100 --> 00:52:04.780]   as opposed to controlling this technology and then have our part in supporting and boosting
[00:52:04.780 --> 00:52:05.780]   it being open source.
[00:52:05.780 --> 00:52:09.780]   I think eventually what will happen is people really disagree, they'll just fork.
[00:52:09.780 --> 00:52:12.420]   And so we've seen that in various communities, just fork it, right?
[00:52:12.420 --> 00:52:14.420]   It's the beauty of open source.
[00:52:14.420 --> 00:52:16.780]   You can go and do your own thing.
[00:52:16.780 --> 00:52:21.620]   Although I imagine it might be easier to fork a model because one or two people could kind
[00:52:21.620 --> 00:52:25.820]   of take it in a different direction.
[00:52:25.820 --> 00:52:28.660]   You can fine tune models, you can fork models.
[00:52:28.660 --> 00:52:31.300]   I think the key thing here is benchmark model.
[00:52:31.300 --> 00:52:33.540]   That's a lot of compute up front, right?
[00:52:33.540 --> 00:52:36.580]   And then fine tuning and running it, it's relatively little compute.
[00:52:36.580 --> 00:52:42.060]   This is the opposite of kind of the current paradigm on Google or Facebook, which is relatively
[00:52:42.060 --> 00:52:46.940]   little compute to get it into database structure and most of the compute is done on time to
[00:52:46.940 --> 00:52:48.020]   balance rates, right?
[00:52:48.020 --> 00:52:52.140]   So you can take a stable diffusion model right now, you can trade it on your face with 10
[00:52:52.140 --> 00:52:56.820]   images or 100 images and then boom, you've got your own like Lucas model that can put
[00:52:56.820 --> 00:53:00.100]   Lucas in anything, right?
[00:53:00.100 --> 00:53:01.860]   That's the flipping of the entire paradigm.
[00:53:01.860 --> 00:53:03.340]   But that is not forking of the community.
[00:53:03.340 --> 00:53:08.100]   A community fork will be disagreements over safe for work or not safe for work as the
[00:53:08.100 --> 00:53:11.700]   data sets crawled or licensed or things like that.
[00:53:11.700 --> 00:53:17.060]   And I imagine we will see different communities around this, around some of these key questions.
[00:53:17.060 --> 00:53:22.100]   Although I guess what's tricky maybe about this and a little different than other communities
[00:53:22.100 --> 00:53:27.620]   is you're kind of holding this very valuable resource in terms of compute.
[00:53:27.620 --> 00:53:31.900]   So at the end of the day, you will have to kind of arbitrate more aggressively maybe.
[00:53:31.900 --> 00:53:35.500]   For sure anyone could easily fork stuff, but then they would have to kind of potentially
[00:53:35.500 --> 00:53:43.340]   ask you to get the compute resources to really make a meaningful fork, right?
[00:53:43.340 --> 00:53:44.500]   Yeah, a hundred percent.
[00:53:44.500 --> 00:53:48.220]   So right now we have a lot of control because we're the fastest supplier of compute.
[00:53:48.220 --> 00:53:51.540]   But part of what we're trying to do as we spin these off independently is make it so
[00:53:51.540 --> 00:53:55.620]   they can access their own compute and also stimulate some of these national clusters
[00:53:55.620 --> 00:53:56.620]   to be more open.
[00:53:56.620 --> 00:54:01.500]   It doesn't take six to 12 months to get A100 or H100 access anymore.
[00:54:01.500 --> 00:54:06.540]   So I think again, it deserves to be a bit more diverse, to have multiple parties at
[00:54:06.540 --> 00:54:09.720]   the table as opposed to centralized.
[00:54:09.720 --> 00:54:14.140]   And this is a deliberate action by us to move towards more and more distributed and decentralization.
[00:54:14.140 --> 00:54:19.180]   Both from kind of an ethical and moral perspective, but then also, like I said, from a business
[00:54:19.180 --> 00:54:21.660]   perspective, it works for us as well.
[00:54:21.660 --> 00:54:24.780]   Because if we're considered to be in control of everything, like we don't know what's going
[00:54:24.780 --> 00:54:25.780]   to happen there.
[00:54:25.780 --> 00:54:32.900]   And it's really a lot of effort to coordinate an entire community that likely won't be positive
[00:54:32.900 --> 00:54:37.500]   because it's going to be a lot if this goes to a hundred million, a billion people as
[00:54:37.500 --> 00:54:38.500]   we expect.
[00:54:38.500 --> 00:54:39.740]   Coordinating all of those.
[00:54:39.740 --> 00:54:43.740]   So it should be an independent entity doing that where all the voices can be heard.
[00:54:43.740 --> 00:54:45.660]   We've got our own part to play within that.
[00:54:45.660 --> 00:54:49.820]   So we go from being the main provider of compute to being a provider of compute.
[00:54:49.820 --> 00:54:54.580]   So hopefully all the compute is provided by the world effectively to do this properly,
[00:54:54.580 --> 00:54:56.540]   because it is a public good.
[00:54:56.540 --> 00:54:58.860]   And that's good for us because it saves our costs, right?
[00:54:58.860 --> 00:55:03.380]   The open source models get created without cost to us.
[00:55:03.380 --> 00:55:07.780]   So you imagine a world where a huge fraction of the world's population is training models?
[00:55:07.780 --> 00:55:09.620]   Did I understand that right?
[00:55:09.620 --> 00:55:10.620]   No.
[00:55:10.620 --> 00:55:14.500]   I think everyone in the world will use these models.
[00:55:14.500 --> 00:55:21.860]   I reckon there will be like thousands of developers creating these models to certain standards
[00:55:21.860 --> 00:55:26.980]   established by the various communities and others in interrelation with each other.
[00:55:26.980 --> 00:55:31.540]   So you will have standard benchmark models like, you know, Red Hat version seven or something
[00:55:31.540 --> 00:55:34.180]   like that, or Ubuntu 20.
[00:55:34.180 --> 00:55:36.140]   Like there will be regular releases of these models.
[00:55:36.140 --> 00:55:37.300]   It will be independent.
[00:55:37.300 --> 00:55:40.260]   The countries and others will provide the compute for it.
[00:55:40.260 --> 00:55:44.580]   They'll be one of the voices at the table doing a little bit, and then people will build
[00:55:44.580 --> 00:55:48.180]   on those benchmark models and fine tune them for themselves.
[00:55:48.180 --> 00:55:52.900]   So you know, on the Apple architecture, like I said, there is a neural engine that's not
[00:55:52.900 --> 00:55:54.340]   really used.
[00:55:54.340 --> 00:55:58.140]   Others are having the same kind of foundation model engines that are coming through.
[00:55:58.140 --> 00:56:02.700]   So I think in five to 10 years, you will have AI at the edge, AI in the cloud, and the hybrid
[00:56:02.700 --> 00:56:07.380]   interaction of those two will be super powerful across modalities.
[00:56:07.380 --> 00:56:09.820]   This is also one reason why we are fully multimodal.
[00:56:09.820 --> 00:56:11.820]   If people are like, why don't you just focus on image?
[00:56:11.820 --> 00:56:15.380]   Because you don't know where the learnings come from or the value across all of these.
[00:56:15.380 --> 00:56:18.980]   So it makes sense for us to be that layer one infrastructure layer there to get things
[00:56:18.980 --> 00:56:21.380]   going and then have a business model and scaling this.
[00:56:21.380 --> 00:56:22.900]   Yeah, that makes sense.
[00:56:22.900 --> 00:56:25.380]   That makes total sense.
[00:56:25.380 --> 00:56:27.140]   I want to make sure I asked you about education.
[00:56:27.140 --> 00:56:29.540]   I mean, that kind of comes up every time we talk.
[00:56:29.540 --> 00:56:31.100]   It comes up in every interview.
[00:56:31.100 --> 00:56:34.660]   It's obviously something that you're super passionate about.
[00:56:34.660 --> 00:56:37.740]   How does education kind of fit into stability?
[00:56:37.740 --> 00:56:43.060]   So like a large part of stability for my own personal focus is around the rights of children
[00:56:43.060 --> 00:56:45.260]   because a lot of ethics complex and things like that.
[00:56:45.260 --> 00:56:47.940]   But we all agree that children don't have agency and so they have rights.
[00:56:47.940 --> 00:56:50.260]   I'm talking about effect of altruism a million years from now.
[00:56:50.260 --> 00:56:51.260]   I'm talking about kids right now today.
[00:56:51.260 --> 00:56:55.140]   And I was like, if I could go to the future and bring back technology to make kids' lives
[00:56:55.140 --> 00:56:56.140]   better, what would I do?
[00:56:56.140 --> 00:56:58.220]   I allow them to create any image, right?
[00:56:58.220 --> 00:57:01.340]   And use these tools, allow them to do code, you know, the type of stuff Amdata Redblit
[00:57:01.340 --> 00:57:02.340]   does, you know.
[00:57:02.340 --> 00:57:06.140]   I would allow them to communicate and be educated in the healthcare.
[00:57:06.140 --> 00:57:10.180]   So with the education thing, it was like first moving an app and a tablet could actually
[00:57:10.180 --> 00:57:12.180]   make a difference if you've done that through the RCTs.
[00:57:12.180 --> 00:57:15.340]   Now it's about bringing the world together and say, what's the best learning experience
[00:57:15.340 --> 00:57:16.340]   we can have to teach these kids?
[00:57:16.340 --> 00:57:19.780]   Because it doesn't make sense that we teach arithmetic in a different way across every
[00:57:19.780 --> 00:57:21.140]   single country.
[00:57:21.140 --> 00:57:23.420]   And we don't know what the best way to teach linear algebra is.
[00:57:23.420 --> 00:57:24.420]   No.
[00:57:24.420 --> 00:57:27.780]   But then having an AI model that teaches the kids and learns from the kids at scale, because
[00:57:27.780 --> 00:57:32.700]   you do entire countries at once, is the best data in the world for creating national level
[00:57:32.700 --> 00:57:33.700]   models.
[00:57:33.700 --> 00:57:36.340]   So if you want to create a Malawian model, you need to capture the Malawian culture and
[00:57:36.340 --> 00:57:37.340]   all the contexts.
[00:57:37.340 --> 00:57:41.300]   And if you're ever trained by little Malawian kids, that's a national level resource.
[00:57:41.300 --> 00:57:44.020]   So this is what I discussed in the upper part of the country.
[00:57:44.020 --> 00:57:47.700]   Like we're not feeding AI models the right things.
[00:57:47.700 --> 00:57:49.300]   We're feeding them a mishmash of stuff.
[00:57:49.300 --> 00:57:53.380]   But if we actually intentionally create data that teaches them to learn, that's going to
[00:57:53.380 --> 00:57:55.300]   be the best models out there.
[00:57:55.300 --> 00:57:58.700]   And similarly, like I said, the discussion that we've had about AI models going to the
[00:57:58.700 --> 00:58:02.980]   edge, having control over the hardware, software, and deployment means that we can standardize
[00:58:02.980 --> 00:58:07.540]   these tablets to be little AI machines, which will be amazing, because they'll have a richer
[00:58:07.540 --> 00:58:09.460]   experience than anyone else.
[00:58:09.460 --> 00:58:12.820]   And I personally think, like, you know, I don't know if you've got kids, Lucas, but
[00:58:12.820 --> 00:58:18.800]   you know, 13 months, one hour a day, you learn literacy and numeracy is good for any kid
[00:58:18.800 --> 00:58:20.220]   anywhere in the world.
[00:58:20.220 --> 00:58:26.300]   In a refugee camp where people earn a few bucks a day at best, like I think Malawi is
[00:58:26.300 --> 00:58:30.660]   like five to $10 a month, it's crazy.
[00:58:30.660 --> 00:58:34.220]   You know, especially when you've got one teacher per 400 kids, how else are you going to educate
[00:58:34.220 --> 00:58:35.960]   them other than with this technology?
[00:58:35.960 --> 00:58:39.220]   How else are you going to do it other than creating an open source standard that's scalable
[00:58:39.220 --> 00:58:41.540]   and working with the World Bank to be able to scale it?
[00:58:41.540 --> 00:58:45.600]   So I think this technology has a huge role to play in education.
[00:58:45.600 --> 00:58:52.260]   I think that incorporating into the West will be incredibly difficult and an uphill battle.
[00:58:52.260 --> 00:58:57.020]   Taking it with ROI is largest in terms of emerging markets and places like that is going
[00:58:57.020 --> 00:58:58.020]   to be the best.
[00:58:58.020 --> 00:59:00.540]   And then we'll create a system that's better for everyone.
[00:59:00.540 --> 00:59:06.740]   Because again, we have to decide what should be open and a public good.
[00:59:06.740 --> 00:59:09.060]   This is not from a business perspective, it's from a societal perspective, versus what should
[00:59:09.060 --> 00:59:11.240]   be closed.
[00:59:11.240 --> 00:59:15.700]   Should the tools to allow anyone to be creative, anyone to be educated and other things like
[00:59:15.700 --> 00:59:18.500]   that be run by private companies?
[00:59:18.500 --> 00:59:19.500]   Probably not.
[00:59:19.500 --> 00:59:21.400]   You know, they should be a public good.
[00:59:21.400 --> 00:59:25.900]   Should they be United Nations and other bureaucratic hellholes?
[00:59:25.900 --> 00:59:27.400]   Probably not.
[00:59:27.400 --> 00:59:30.780]   So with this technology coming right now, there's a little window where we can create
[00:59:30.780 --> 00:59:35.020]   better, more adaptive systems and bring them to the people where it can have the most value.
[00:59:35.020 --> 00:59:37.300]   And that's what stability is focused on.
[00:59:37.300 --> 00:59:40.620]   Because then you can build a real infrastructure for the next generation.
[00:59:40.620 --> 00:59:45.220]   And just to be like concrete about this, you're imagining making like a tablet that has like
[00:59:45.220 --> 00:59:50.620]   an AI teacher that's literally like talking to students and teaching them things like
[00:59:50.620 --> 00:59:51.620]   linear algebra.
[00:59:51.620 --> 00:59:52.620]   Is that?
[00:59:52.620 --> 00:59:53.620]   Yeah.
[00:59:53.620 --> 00:59:55.500]   I want to call it one AI per child, but others are against that.
[00:59:55.500 --> 00:59:56.700]   But that's the concept.
[00:59:56.700 --> 00:59:58.340]   You have an AI that helps you.
[00:59:58.340 --> 01:00:01.140]   Because what is AI but information classification, right?
[01:00:01.140 --> 01:00:06.060]   So what's that information that can help that kid be in Malawi or Brooklyn to the next part
[01:00:06.060 --> 01:00:07.940]   of their journey, right?
[01:00:07.940 --> 01:00:10.860]   And having a standardized architecture for that so you can take what works in Malawi
[01:00:10.860 --> 01:00:15.740]   and apply it to Ethiopia or apply it to Benin, apply it to anywhere, makes sense.
[01:00:15.740 --> 01:00:20.060]   And the output data of that is customized datasets that are ideal for local language
[01:00:20.060 --> 01:00:24.220]   models and local image models and local video models if you execute correctly.
[01:00:24.220 --> 01:00:29.100]   So this is why I think we are not open AI or deep mind.
[01:00:29.100 --> 01:00:30.660]   We don't train giant models.
[01:00:30.660 --> 01:00:33.660]   The entire focus is AI that's accessible for everyone.
[01:00:33.660 --> 01:00:35.980]   It's emerging markets and creativity.
[01:00:35.980 --> 01:00:37.340]   Those are our two kind of focuses.
[01:00:37.340 --> 01:00:40.740]   Again, like I don't really care about AGI except for it not killing us.
[01:00:40.740 --> 01:00:43.140]   I don't want to create a generalized intelligence.
[01:00:43.140 --> 01:00:48.940]   I want to create specific intelligences that are widely available.
[01:00:48.940 --> 01:00:51.740]   So we close the traditional divide and makes people's lives better.
[01:00:51.740 --> 01:00:54.740]   That's the key focus and lodestar of what we do.
[01:00:54.740 --> 01:01:00.340]   That totally resonates with me, but don't you feel like the trends lately have been
[01:01:00.340 --> 01:01:06.260]   creating better specific intelligences through creating better general intelligences?
[01:01:06.260 --> 01:01:13.540]   I've been kind of watching the last 20 years of machine learning seems like more and more
[01:01:13.540 --> 01:01:19.100]   kind of general purpose things that are then fine tuned on specific applications.
[01:01:19.100 --> 01:01:21.860]   Do you sort of expect that trend to change?
[01:01:21.860 --> 01:01:23.060]   I think it's an arc, right?
[01:01:23.060 --> 01:01:28.020]   So scaling is all you need and more layers and it's better datasets, right?
[01:01:28.020 --> 01:01:32.900]   And so as you kind of have this adaptation, I think the intelligence goes to the edge.
[01:01:32.900 --> 01:01:38.500]   I think instructs and the combination of reinforcement learning and deep learning is the next big
[01:01:38.500 --> 01:01:40.580]   trend that we're seeing start to accelerate.
[01:01:40.580 --> 01:01:44.020]   And again, that's why we've got corporate AI as a kind of representative contrastive
[01:01:44.020 --> 01:01:46.260]   learning lab.
[01:01:46.260 --> 01:01:50.020]   And so I think it will be loads of models because these big models were there, but they
[01:01:50.020 --> 01:01:52.260]   weren't really used, right?
[01:01:52.260 --> 01:01:53.700]   Now they're being used.
[01:01:53.700 --> 01:01:57.300]   So stable diffusion is being used probably by, well, it's being used by millions of people
[01:01:57.300 --> 01:01:59.060]   each day.
[01:01:59.060 --> 01:02:03.220]   As it gets better and as people release more models, this technology will be used by more
[01:02:03.220 --> 01:02:05.700]   and more people, be it private or public.
[01:02:05.700 --> 01:02:10.140]   And so I think that then it becomes about inference and cost.
[01:02:10.140 --> 01:02:13.300]   Because if you've got a model that's open source and it's closed model, open source
[01:02:13.300 --> 01:02:16.020]   models will always be worse than closed source models because you can always just take it
[01:02:16.020 --> 01:02:18.500]   and make it close and trade on better data.
[01:02:18.500 --> 01:02:19.860]   Then that creates a different paradigm.
[01:02:19.860 --> 01:02:24.780]   And again, I think it was this breakthrough point whereby stack more layers became less
[01:02:24.780 --> 01:02:27.260]   effective as you kind of went up.
[01:02:27.260 --> 01:02:32.700]   Now it's a case of make the layers more effective as it were and figure out how do we optimize
[01:02:32.700 --> 01:02:37.180]   these models if we can start doing A/B tests and training 10 of them at once.
[01:02:37.180 --> 01:02:39.100]   Where are the key optimization points here?
[01:02:39.100 --> 01:02:42.860]   And I think that the optimization points will be a model that's used by a million people
[01:02:42.860 --> 01:02:45.580]   versus a model that's used by an internal team.
[01:02:45.580 --> 01:02:50.420]   A million people will always win because people will figure out all sorts of tricks like Dream
[01:02:50.420 --> 01:02:51.420]   Booth training.
[01:02:51.420 --> 01:02:54.580]   So that's where you take a few pictures of yourselves and it's kind of fine tuning for
[01:02:54.580 --> 01:02:56.260]   the image model.
[01:02:56.260 --> 01:03:00.060]   That was 48 gigabytes when it first came out of VRAM requirement.
[01:03:00.060 --> 01:03:04.180]   After three weeks, it was eight gigabytes by the community building on it.
[01:03:04.180 --> 01:03:08.100]   And having that and having hundreds and hundreds of developers hacking away at these things
[01:03:08.100 --> 01:03:11.660]   and figuring out how to put them into processes as opposed to zero shot.
[01:03:11.660 --> 01:03:14.660]   These won't be the best for zero shot, but they will be more useful because they're in
[01:03:14.660 --> 01:03:15.660]   pipelines.
[01:03:15.660 --> 01:03:20.020]   And I think that we've shown that with stable diffusion versus other image things which
[01:03:20.020 --> 01:03:21.300]   are within their thing.
[01:03:21.300 --> 01:03:23.540]   But now we just have to upgrade the models again.
[01:03:23.540 --> 01:03:26.460]   All right, I have one more question that I didn't actually prepare.
[01:03:26.460 --> 01:03:32.380]   I'm kind of curious if you have thoughts on, which is that you've talked about your autistic
[01:03:32.380 --> 01:03:37.540]   son a few times and I actually have a little sister that's autistic.
[01:03:37.540 --> 01:03:42.140]   Autism has come up in many of these interviews that I've done, often like autistic family
[01:03:42.140 --> 01:03:43.140]   members.
[01:03:43.140 --> 01:03:48.460]   I'm curious, do you see any kind of connection between autism and machine learning?
[01:03:48.460 --> 01:03:50.380]   A hundred percent.
[01:03:50.380 --> 01:03:54.060]   This is why I really love transform-based architecture because what I do with my son
[01:03:54.060 --> 01:03:57.740]   in terms of repurposing drugs for him, and we'll do a full formal thing about this in
[01:03:57.740 --> 01:04:01.820]   the next year or two where we'll share all the learnings, is about reducing the noise
[01:04:01.820 --> 01:04:05.180]   and getting him to pay attention by reducing the imbalance.
[01:04:05.180 --> 01:04:09.540]   So there's too much glutamate making him excited, so not enough GABA calming him down.
[01:04:09.540 --> 01:04:13.820]   And then having things like applied behavioral analysis where he does rapid kind of iterations
[01:04:13.820 --> 01:04:16.980]   to learn that a cup means things and various things with a variable reward schedule where
[01:04:16.980 --> 01:04:20.340]   he gets rewarded at random so he's more motivated to rebuild these things.
[01:04:20.340 --> 01:04:22.740]   It's similar for stroke fix him and other things.
[01:04:22.740 --> 01:04:26.260]   But again, you look at what these machine models do with transform-based architecture,
[01:04:26.260 --> 01:04:27.540]   attention is all you need.
[01:04:27.540 --> 01:04:29.940]   They pay attention to the important parts.
[01:04:29.940 --> 01:04:33.460]   And that interconnection of creating latent spaces or hidden layers of meaning is exactly
[01:04:33.460 --> 01:04:38.700]   the same, almost, well, it's not the same, but the same principle as what we do for rebuilding
[01:04:38.700 --> 01:04:42.140]   the language capabilities of our kids.
[01:04:42.140 --> 01:04:44.220]   And so this is one of the things that really drew it to me.
[01:04:44.220 --> 01:04:45.380]   And I was like, I kind of get that.
[01:04:45.380 --> 01:04:49.140]   Like I have Asperger's myself, so I've had to rebuild and refigure out a lot of stuff.
[01:04:49.140 --> 01:04:50.820]   It's a principle-based approach.
[01:04:50.820 --> 01:04:53.980]   So that's why I was like, it's almost like type one versus type two thinking.
[01:04:53.980 --> 01:04:54.980]   Retrieval versus instinct.
[01:04:54.980 --> 01:04:59.820]   A combination of those is the most powerful combination we've ever had as a team.
[01:04:59.820 --> 01:05:02.620]   And again, I think that it'll be really be able to help with this.
[01:05:02.620 --> 01:05:07.260]   The other aspect of it is personalized medicine and education and other stuff.
[01:05:07.260 --> 01:05:08.260]   We don't have enough teachers.
[01:05:08.260 --> 01:05:10.220]   We don't have enough doctors.
[01:05:10.220 --> 01:05:14.140]   These technologies are now reaching human level in very narrow fields.
[01:05:14.140 --> 01:05:17.060]   What if we could put this on tablets out there?
[01:05:17.060 --> 01:05:21.820]   One AI per child doesn't just mean like something is literally an AI that can help them in everything
[01:05:21.820 --> 01:05:25.940]   if they've got special needs or if they're neurotypical or anything like that and personalize
[01:05:25.940 --> 01:05:29.620]   this stuff for them because our education system treats everyone like a number.
[01:05:29.620 --> 01:05:33.540]   It's like a Gurdik versus the non-mega city of humans, like toss a thousand coins at once.
[01:05:33.540 --> 01:05:36.860]   The same as tossing one coin a thousand times.
[01:05:36.860 --> 01:05:40.700]   And the reality is we are all unique, but we didn't have the tools to personalize until
[01:05:40.700 --> 01:05:43.420]   now and this is the first technology that could do that.
[01:05:43.420 --> 01:05:47.980]   So in doing that, we can figure out systematic diseases and conditions like autism, like
[01:05:47.980 --> 01:05:48.980]   COVID and others.
[01:05:48.980 --> 01:05:49.980]   This is why I focused on COVID.
[01:05:49.980 --> 01:05:53.820]   This was a multi systemic disease that modern science wouldn't be able to deal with.
[01:05:53.820 --> 01:05:59.140]   Like why do you have kind of massive kind of ferritin levels and other things in the
[01:05:59.140 --> 01:06:00.140]   blood?
[01:06:00.140 --> 01:06:01.140]   Is it serotonin syndrome?
[01:06:01.140 --> 01:06:02.220]   Is it this or that?
[01:06:02.220 --> 01:06:05.580]   The first principles analysis of COVID is even still lacking today.
[01:06:05.580 --> 01:06:11.300]   Thankfully, we found treatment and again, models are one science, but information isn't
[01:06:11.300 --> 01:06:14.700]   getting to where it's needed in the personalized basis and again, we can build systems for
[01:06:14.700 --> 01:06:15.700]   that.
[01:06:15.700 --> 01:06:17.980]   But AI models are only one part of that.
[01:06:17.980 --> 01:06:20.780]   It's more classical open source AI for the rest of it.
[01:06:20.780 --> 01:06:23.300]   So yeah, I think there are kind of parallels to this.
[01:06:23.300 --> 01:06:27.100]   And of course, being in our industry, it is very, very prevalent, right?
[01:06:27.100 --> 01:06:28.100]   It's like a double edged sword.
[01:06:28.100 --> 01:06:32.220]   Well, I'm curious, do you think your Asperger's has given you some advantages in building
[01:06:32.220 --> 01:06:33.900]   this really neat company?
[01:06:33.900 --> 01:06:35.740]   Yeah, no, a hundred percent.
[01:06:35.740 --> 01:06:37.180]   My real skill is mechanism design.
[01:06:37.180 --> 01:06:42.780]   I know how to convince governments and multilaterals and others like stability has huge international
[01:06:42.780 --> 01:06:46.740]   support because I've positioned it just right at the right time.
[01:06:46.740 --> 01:06:51.900]   And my Asperger's and ADHD typically balance each other out, I like to say.
[01:06:51.900 --> 01:06:53.420]   So you got to focus on what you're good at.
[01:06:53.420 --> 01:06:54.660]   That's what I'm good at.
[01:06:54.660 --> 01:06:59.460]   That's my job here to absorb the hate and to also do the big thing while letting the
[01:06:59.460 --> 01:07:05.500]   real heroes who are the developers, you know, and the community kind of get on with things.
[01:07:05.500 --> 01:07:09.580]   So it allows me to have a different perspective in that most companies would try to control
[01:07:09.580 --> 01:07:13.540]   this, but really we are just trying to capitalize it and get it out there.
[01:07:13.540 --> 01:07:17.260]   Because I think, again, from a mechanism design perspective and morally, that is the right
[01:07:17.260 --> 01:07:18.260]   thing to do.
[01:07:18.260 --> 01:07:19.260]   Interesting.
[01:07:19.260 --> 01:07:21.260]   Well, we always end with two questions.
[01:07:21.260 --> 01:07:23.140]   I just, I want to make sure I get them in.
[01:07:23.140 --> 01:07:28.700]   So the second to last is pretty open ended, but I usually ask what's a topic in machine
[01:07:28.700 --> 01:07:30.500]   learning that you think is underrated?
[01:07:30.500 --> 01:07:35.100]   You've mentioned a whole bunch, but is there anything else that you think is sort of deserving
[01:07:35.100 --> 01:07:39.620]   of more study than it gets right now?
[01:07:39.620 --> 01:07:40.620]   Machine learning.
[01:07:40.620 --> 01:07:43.660]   I think it's really data, to be honest.
[01:07:43.660 --> 01:07:47.900]   It's like, you can say classical AI was largely data science, but the role in data in these
[01:07:47.900 --> 01:07:52.380]   models is vastly just not looked at at all.
[01:07:52.380 --> 01:07:57.700]   Like I think that you can do 10, a hundred times less data for better outcomes on these
[01:07:57.700 --> 01:08:01.940]   models once we kind of really look at it and how the data impacts the latency and some
[01:08:01.940 --> 01:08:04.140]   of these other stuff.
[01:08:04.140 --> 01:08:05.700]   And so, like I said, we're building a team for that.
[01:08:05.700 --> 01:08:08.940]   And other people have been doing data cleaning, but I don't think that's enough.
[01:08:08.940 --> 01:08:13.300]   I think we'll see some remarkable things advance in that aspect.
[01:08:13.300 --> 01:08:16.820]   It's so funny because my last company that I ran for 10 years was data collection.
[01:08:16.820 --> 01:08:21.020]   And we always found actually data cleaning was the most important thing that anyone could
[01:08:21.020 --> 01:08:22.340]   do to make their models better.
[01:08:22.340 --> 01:08:27.540]   But we could never convince people to do as much data cleaning as we thought they should.
[01:08:27.540 --> 01:08:31.540]   It's cool to stack more layers, right?
[01:08:31.540 --> 01:08:32.540]   Yeah.
[01:08:32.540 --> 01:08:33.540]   Yeah.
[01:08:33.540 --> 01:08:36.580]   So, data cleaning, data extraction, there's a whole bunch in there, I think.
[01:08:36.580 --> 01:08:43.220]   And then the last question that we always ask is, what's the kind of hard part about
[01:08:43.220 --> 01:08:47.500]   taking a model and actually turning it into a product?
[01:08:47.500 --> 01:08:51.300]   And you've obviously just recently created some products built on top of these big models.
[01:08:51.300 --> 01:08:56.360]   I'm curious, outside of the training of the model, what's been maybe some unexpected challenges
[01:08:56.360 --> 01:09:00.260]   in making the whole product work cohesively?
[01:09:00.260 --> 01:09:03.620]   So we have Dream Studio Lite and Dream Studio Pro coming up very soon.
[01:09:03.620 --> 01:09:10.700]   I think probably the key challenge is just getting it responsive enough to have really
[01:09:10.700 --> 01:09:13.620]   that user experience that is seamless.
[01:09:13.620 --> 01:09:17.460]   We've gone sub one second now on inference, but that was very difficult to do.
[01:09:17.460 --> 01:09:21.140]   We had to do a lot of optimization there.
[01:09:21.140 --> 01:09:25.820]   Because again, even if this is relatively small, it's still a large model, right?
[01:09:25.820 --> 01:09:30.140]   And then the second part, I think, is around some of the fine tuning and creating custom
[01:09:30.140 --> 01:09:31.140]   models.
[01:09:31.140 --> 01:09:34.300]   That's a pretty different take on things.
[01:09:34.300 --> 01:09:37.740]   And I think there's a lot of work that's been going on into like, where do we actually store
[01:09:37.740 --> 01:09:42.580]   the models and keep them and the user data aspects of them becomes a very curious thing.
[01:09:42.580 --> 01:09:46.780]   But I think the most important thing is just having the snappy consumer feedback loops
[01:09:46.780 --> 01:09:53.660]   for these large models that will maintain, especially because we're doing animation,
[01:09:53.660 --> 01:09:55.940]   which people don't want to wait around for.
[01:09:55.940 --> 01:09:58.060]   They either wait a long time or they don't want to wait at all.
[01:09:58.060 --> 01:09:59.340]   Like, why isn't it real time?
[01:09:59.340 --> 01:10:01.620]   Because normally this would take like three weeks.
[01:10:01.620 --> 01:10:02.620]   That's challenging.
[01:10:02.620 --> 01:10:03.620]   Yeah.
[01:10:03.620 --> 01:10:04.620]   Well, thank you so much.
[01:10:04.620 --> 01:10:05.620]   I really appreciate it.
[01:10:05.620 --> 01:10:06.620]   That was a fun interview.
[01:10:06.620 --> 01:10:07.620]   No problem, Stigis.
[01:10:07.620 --> 01:10:08.620]   Cheers, buddy.
[01:10:08.620 --> 01:10:16.220]   If you're enjoying these interviews and you want to learn more, please click on the link
[01:10:16.220 --> 01:10:20.980]   to the show notes in the description where you can find links to all the papers that
[01:10:20.980 --> 01:10:25.340]   are mentioned, supplemental material, and a transcription that we work really hard to
[01:10:25.340 --> 01:10:26.340]   produce.
[01:10:26.340 --> 01:10:26.540]   So check it out.
[01:10:26.540 --> 01:10:28.960]   (upbeat music)

