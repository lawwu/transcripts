
[00:00:00.000 --> 00:00:06.960]   The whole evolution in NLP started with a seminal paper called Attention is All You Need,
[00:00:06.960 --> 00:00:16.000]   which was introducing this new architecture for NLP models based on transfer learning.
[00:00:16.000 --> 00:00:25.280]   BERT was the first, most popular of these new generation of models. The way they work is
[00:00:26.000 --> 00:00:34.960]   in a simplistic way without getting too technical, is that you pre-train a model on a large
[00:00:34.960 --> 00:00:42.080]   of text on one task. So for BERT, for example, it's mask filling. You give it sentences,
[00:00:42.080 --> 00:00:48.000]   you remove a word in the middle of the sentence, for example, and then you train the model on
[00:00:48.000 --> 00:00:54.800]   predicting this missing word. And then you do that on a very large group of text, usually
[00:00:55.600 --> 00:01:03.360]   slice of the web. And then you get a model, a pre-trained model that has some understanding
[00:01:03.360 --> 00:01:11.920]   of text that you can then fine tune. Hence the name transfer learning, because you can go from
[00:01:11.920 --> 00:01:19.920]   one pre-training task to other fine tuning tasks. You can fine tune this model, for example,
[00:01:19.920 --> 00:01:28.160]   on classification, by giving it a couple of thousands of examples of a text and classification
[00:01:28.160 --> 00:01:33.680]   for customer support emails that I was talking about. Classification, urgent and not urgent.
[00:01:33.680 --> 00:01:43.520]   And after that, the model is surprisingly good at classifying a new text that you give it based
[00:01:43.520 --> 00:01:49.520]   on urgency. And it's going to tell you, okay, this message, there's 90% chance it's urgent
[00:01:49.520 --> 00:01:53.680]   based on what I've learned in the pre-training and in the fine tuning.
[00:01:53.680 --> 00:02:01.360]   Lukas: And so for example, with BERT, I guess you have a model that can fill in missing words.
[00:02:01.360 --> 00:02:05.440]   How do you actually turn that into a model that say classifies customer support messages?
[00:02:05.440 --> 00:02:12.080]   Vincent: Yeah, with fine tuning, you fine tune by adding a layer. You fine tune this model
[00:02:12.640 --> 00:02:22.400]   to perform on your specific task. And that's kind of like a long-term way. I think that's a very
[00:02:22.400 --> 00:02:32.080]   interesting way of doing machine learning because intuitively you almost feel like it's the right
[00:02:32.080 --> 00:02:40.160]   way to do machine learning in the sense that what we've seen in the past with machine learning,
[00:02:40.160 --> 00:02:46.320]   and especially for startups, a lot of them have sold this dream of doing machine learning and
[00:02:46.320 --> 00:02:52.720]   doing some sort of data network effect on machine learning, because there's this assumption that
[00:02:52.720 --> 00:02:56.400]   you're going to give more data to the model and it's going to perform better.
[00:02:56.400 --> 00:03:03.760]   And I think that's true, but the challenge has always been that you have more data and so your
[00:03:03.760 --> 00:03:12.720]   model performs incrementally better, but only on what you're able to do already. So if you're doing
[00:03:12.720 --> 00:03:20.640]   time series prediction, maybe you have like 1 billion data points and your model performs at
[00:03:20.640 --> 00:03:28.800]   90% accuracy. You add maybe 9 billion, 10 billion additional data points and your model is going
[00:03:28.800 --> 00:03:37.680]   to perform at 90.5% accuracy. And that's great. That's good improvement. That's something you
[00:03:37.680 --> 00:03:46.320]   need, but it doesn't give the kind of increased performance that you're really expecting from a
[00:03:46.320 --> 00:03:54.640]   typical network effect in the sense that it doesn't make your result like 100x, 10x, 100x better than
[00:03:54.640 --> 00:04:03.840]   without it. With transfer learning, it's a bit different because you not only improve incrementally
[00:04:03.840 --> 00:04:14.000]   the accuracy on one task, you give it more ability to solve other tasks. And so you actually not only
[00:04:14.000 --> 00:04:21.360]   increase the accuracy, but you increase the capabilities of what your model is able to do.
[00:04:21.360 --> 00:04:31.440]   And so I won't go into the crazy Musk type prediction, but if you take actually Elon Musk
[00:04:31.440 --> 00:04:41.040]   open AI founding story where he's saying, "We need to bring the whole community together
[00:04:41.040 --> 00:04:47.520]   to contribute to something open source for everyone." Intuitively, you could think that
[00:04:47.520 --> 00:04:54.240]   could come with actually transfer learning in the sense that you could envision a world where
[00:04:54.240 --> 00:05:03.840]   every single company is contributing with their data sets, with their compute, with their weights,
[00:05:03.840 --> 00:05:13.360]   their machine learning model weights to build this giant kind of open source models that would
[00:05:13.360 --> 00:05:23.280]   be able to do 100x more things than what each of these companies could do alone. I don't know if
[00:05:23.280 --> 00:05:29.520]   we're going to get there in the foreseeable future, but I feel like that's in terms of concepts,
[00:05:29.520 --> 00:05:34.320]   that's something interesting to look at when you think about transfer learning as opposed to
[00:05:34.320 --> 00:05:37.280]   the other techniques of machine learning.
[00:05:37.280 --> 00:05:42.880]   Thanks for watching this clip. You can see the full episode on our YouTube channel,
[00:05:42.880 --> 00:05:49.360]   and you can join our friendly Slack community with over 4,000 ML engineers to participate in
[00:05:49.360 --> 00:05:54.240]   paper reading groups, AMAs, and other fun events.

