
[00:00:00.000 --> 00:00:03.640]   So Robert's got a nice tight schedule right now.
[00:00:03.640 --> 00:00:05.360]   So I think we'll actually just jump right in.
[00:00:05.360 --> 00:00:08.360]   So Robert, by way of introduction,
[00:00:08.360 --> 00:00:12.320]   Robert is a graduate of my alma mater, UC Berkeley,
[00:00:12.320 --> 00:00:13.840]   where he worked in the RISE Lab,
[00:00:13.840 --> 00:00:17.120]   I believe with Michael Jordan as his primary advisor.
[00:00:17.120 --> 00:00:17.960]   - That's correct.
[00:00:17.960 --> 00:00:22.960]   - And yeah, now is the co-founder and co-creator of any,
[00:00:22.960 --> 00:00:26.840]   co-founder of any scale, co-creator of Ray,
[00:00:26.840 --> 00:00:29.560]   an open source library for doing scalable machine learning.
[00:00:29.560 --> 00:00:32.040]   So lots of folks in the community are really interested
[00:00:32.040 --> 00:00:35.360]   in taking their code and scaling it up to big clusters,
[00:00:35.360 --> 00:00:38.000]   doing big jobs, training big models.
[00:00:38.000 --> 00:00:39.920]   So I'm really excited to hear from him.
[00:00:39.920 --> 00:00:42.200]   So Robert, go ahead and take it away.
[00:00:42.200 --> 00:00:43.440]   - I'll share my screen.
[00:00:43.440 --> 00:00:44.600]   Thanks a lot.
[00:00:44.600 --> 00:00:47.440]   And yeah, thanks a ton for inviting me.
[00:00:47.440 --> 00:00:50.320]   So I'm gonna tell you a bit about Ray,
[00:00:50.320 --> 00:00:52.900]   which it's an open source library.
[00:00:52.900 --> 00:00:55.400]   It's a Python library for scaling up,
[00:00:55.400 --> 00:00:58.480]   doing parallel and distributed Python.
[00:00:58.480 --> 00:01:02.080]   And it started out as a research project at UC Berkeley
[00:01:02.080 --> 00:01:03.840]   when I was a grad student.
[00:01:03.840 --> 00:01:06.920]   And now any scale is the company behind the project.
[00:01:06.920 --> 00:01:09.120]   Of course, we're doing a bunch of development on Ray
[00:01:09.120 --> 00:01:12.120]   at any scale, but it's an open source project.
[00:01:12.120 --> 00:01:14.400]   And there are serious contributions
[00:01:14.400 --> 00:01:16.500]   from a number of different companies.
[00:01:16.500 --> 00:01:20.920]   So before I dive into the details, at a high level,
[00:01:20.920 --> 00:01:22.960]   Ray consists of three parts.
[00:01:22.960 --> 00:01:27.080]   The core system consists of tools
[00:01:27.080 --> 00:01:28.840]   for scaling up Python applications.
[00:01:28.840 --> 00:01:31.620]   So essentially taking arbitrary Python applications
[00:01:31.620 --> 00:01:35.120]   and running those, parallelizing those across multiple cores
[00:01:35.120 --> 00:01:36.760]   or multiple machines.
[00:01:36.760 --> 00:01:39.920]   And then on top of those primitives,
[00:01:39.920 --> 00:01:42.040]   there's an ecosystem of libraries.
[00:01:42.040 --> 00:01:44.120]   So some of these are libraries that are developed
[00:01:44.120 --> 00:01:45.700]   natively as part of Ray.
[00:01:45.700 --> 00:01:49.280]   Some of those are libraries that are third-party libraries
[00:01:49.280 --> 00:01:51.480]   that run on top of Ray, like you may be familiar
[00:01:51.480 --> 00:01:54.640]   with Horovod or Hugging Face or Spacey
[00:01:54.640 --> 00:01:57.840]   or a number of different libraries
[00:01:57.840 --> 00:01:59.960]   in the machine learning ecosystem.
[00:01:59.960 --> 00:02:03.000]   And then lastly, Ray consists of tools
[00:02:03.000 --> 00:02:05.640]   for launching clusters on any cloud provider,
[00:02:05.640 --> 00:02:09.820]   running your Ray cluster on Kubernetes or AWS or GCP
[00:02:09.820 --> 00:02:13.480]   or Azure, and really trying to take a lot of the friction
[00:02:13.480 --> 00:02:18.140]   out of running scalable experiments or applications.
[00:02:18.140 --> 00:02:23.280]   So a natural question is, why are we doing this?
[00:02:23.280 --> 00:02:26.300]   The reason we started Ray and that we started AnyScale
[00:02:26.300 --> 00:02:29.100]   is that we think looking at hardware trends,
[00:02:29.100 --> 00:02:31.640]   that the future of computing is distributed.
[00:02:31.640 --> 00:02:33.980]   Basically that more and more applications
[00:02:33.980 --> 00:02:37.600]   will need to run on clusters.
[00:02:37.600 --> 00:02:38.880]   And there are a number of reasons for that.
[00:02:38.880 --> 00:02:42.000]   One is hardware trends looking at the amount of compute
[00:02:42.000 --> 00:02:45.600]   or the amount of memory or computation required
[00:02:45.600 --> 00:02:46.840]   to do machine learning
[00:02:46.840 --> 00:02:49.320]   and to build machine learning applications.
[00:02:49.320 --> 00:02:51.160]   Another has to do with the nature
[00:02:51.160 --> 00:02:52.940]   of machine learning applications
[00:02:52.940 --> 00:02:57.160]   and how AI and machine learning are being integrated
[00:02:57.160 --> 00:02:59.680]   into more and more applications.
[00:02:59.680 --> 00:03:04.040]   So to give you an example of what's changed,
[00:03:04.040 --> 00:03:06.120]   distributed systems are not new.
[00:03:06.120 --> 00:03:09.840]   So you may ask, look, why build a new distributed system?
[00:03:09.840 --> 00:03:13.080]   So going back to distributed computing
[00:03:13.080 --> 00:03:14.280]   has been around for quite a long time,
[00:03:14.280 --> 00:03:15.240]   at least since the '80s,
[00:03:15.240 --> 00:03:17.840]   you have high-performance computing,
[00:03:17.840 --> 00:03:20.120]   tools for simulating molecular dynamics
[00:03:20.120 --> 00:03:25.000]   or simulating the weather and climate.
[00:03:25.000 --> 00:03:26.400]   You have more recently in the '90s,
[00:03:26.400 --> 00:03:28.720]   you have the emergence of the internet
[00:03:28.720 --> 00:03:31.820]   and distributed systems to support large-scale web traffic.
[00:03:31.820 --> 00:03:34.560]   More recently in the 2000s,
[00:03:34.560 --> 00:03:36.980]   you have big data and the emergence of systems
[00:03:36.980 --> 00:03:38.800]   like MapReduce and Hadoop,
[00:03:38.800 --> 00:03:41.220]   and then more recently Spark.
[00:03:41.220 --> 00:03:42.880]   And then even more recently,
[00:03:42.880 --> 00:03:44.500]   you have the rise of deep learning
[00:03:44.500 --> 00:03:48.240]   and tools like systems like TensorFlow and PyTorch
[00:03:48.240 --> 00:03:51.600]   for scaling deep learning applications.
[00:03:51.600 --> 00:03:53.760]   So you have all these different workloads
[00:03:53.760 --> 00:03:55.120]   that need to be distributed
[00:03:55.120 --> 00:03:57.640]   and you have systems for accomplishing that
[00:03:57.640 --> 00:03:58.480]   for each of them.
[00:03:58.480 --> 00:04:03.020]   And, sorry about that.
[00:04:03.020 --> 00:04:05.920]   And let's see.
[00:04:05.920 --> 00:04:12.300]   Sorry, I just needed to turn off this.
[00:04:12.300 --> 00:04:15.440]   I think it's gone.
[00:04:15.440 --> 00:04:16.860]   Okay, apologies.
[00:04:17.880 --> 00:04:21.060]   Okay, so you have all these different distributed systems
[00:04:21.060 --> 00:04:23.480]   for different kinds of workloads.
[00:04:23.480 --> 00:04:25.860]   So what's changed, what's different now
[00:04:25.860 --> 00:04:27.440]   is that we no longer have,
[00:04:27.440 --> 00:04:29.360]   they're no longer isolated workloads
[00:04:29.360 --> 00:04:31.840]   that each exist separately.
[00:04:31.840 --> 00:04:33.480]   So whereas historically,
[00:04:33.480 --> 00:04:35.300]   you have all these separate workloads
[00:04:35.300 --> 00:04:36.480]   and you have separate systems
[00:04:36.480 --> 00:04:38.160]   for each computational pattern,
[00:04:38.160 --> 00:04:41.040]   like you have TensorFlow and PyTorch for deep learning,
[00:04:41.040 --> 00:04:44.640]   you have different systems for microservices and serving,
[00:04:44.640 --> 00:04:46.600]   you have big data systems like Spark
[00:04:46.600 --> 00:04:49.720]   and streaming systems like Kafka and Flink,
[00:04:49.720 --> 00:04:53.440]   and you have different tools for high performance computing.
[00:04:53.440 --> 00:04:56.000]   What's changed now is that you're starting
[00:04:56.000 --> 00:04:58.020]   to see these workloads overlap.
[00:04:58.020 --> 00:04:59.200]   So if you look at applications
[00:04:59.200 --> 00:05:01.180]   like reinforcement learning, for example,
[00:05:01.180 --> 00:05:05.200]   that requires training as well as large scale simulations.
[00:05:05.200 --> 00:05:06.840]   So you have high performance computing
[00:05:06.840 --> 00:05:09.360]   and deep learning overlapping.
[00:05:09.360 --> 00:05:11.800]   If you look at applications in like online learning,
[00:05:11.800 --> 00:05:13.200]   where you have machine learning models
[00:05:13.200 --> 00:05:15.200]   that are interacting with the real world
[00:05:15.200 --> 00:05:17.000]   and learning from that experience,
[00:05:17.000 --> 00:05:19.420]   you have training, of course, but also serving
[00:05:19.420 --> 00:05:21.960]   and all sorts of other application logic.
[00:05:21.960 --> 00:05:25.000]   And big data interacts with all of these.
[00:05:25.000 --> 00:05:27.560]   You use data to power your deep learning,
[00:05:27.560 --> 00:05:30.400]   you use it to power your simulations,
[00:05:30.400 --> 00:05:33.200]   the serving feeds into the data collection.
[00:05:33.200 --> 00:05:35.480]   And so you start to have applications
[00:05:35.480 --> 00:05:37.340]   that touch all of these different kinds
[00:05:37.340 --> 00:05:38.880]   of computational patterns.
[00:05:38.880 --> 00:05:40.680]   And so each of these systems individually,
[00:05:40.680 --> 00:05:43.220]   whether it's TensorFlow or Flink,
[00:05:43.220 --> 00:05:45.980]   has trouble supporting the whole,
[00:05:45.980 --> 00:05:48.800]   these more hybrid applications.
[00:05:48.800 --> 00:05:52.000]   And so there's a clear need for a system,
[00:05:52.000 --> 00:05:54.320]   a general purpose system that can support
[00:05:54.320 --> 00:05:55.960]   all of these different workloads.
[00:05:55.960 --> 00:05:58.520]   And the natural question is,
[00:05:58.520 --> 00:06:00.360]   what should that system look like?
[00:06:00.360 --> 00:06:02.680]   What are the requirements?
[00:06:02.680 --> 00:06:05.880]   And that's exactly the question that we posed for ourselves
[00:06:05.880 --> 00:06:07.480]   when we began working on Ray.
[00:06:07.480 --> 00:06:11.680]   So essentially, that's why we started working on Ray,
[00:06:11.680 --> 00:06:14.000]   why we started AnyScale.
[00:06:14.000 --> 00:06:15.660]   The goal with Ray is to build
[00:06:15.660 --> 00:06:17.920]   this general purpose distributed system
[00:06:17.920 --> 00:06:22.620]   that can really support this whole diversity
[00:06:22.620 --> 00:06:26.100]   of different kinds of applications on top.
[00:06:26.100 --> 00:06:30.800]   So to give you an example of what that looks like,
[00:06:30.800 --> 00:06:32.400]   or an example of an application
[00:06:32.400 --> 00:06:35.220]   that requires this kind of generality,
[00:06:35.220 --> 00:06:36.860]   I'll give you an example from a Ray user.
[00:06:36.860 --> 00:06:38.960]   So this is a large FinTech company
[00:06:38.960 --> 00:06:41.520]   that wants to build an online learning
[00:06:41.520 --> 00:06:42.760]   recommendation system.
[00:06:42.760 --> 00:06:47.840]   So the anatomy of this application, they have users,
[00:06:47.840 --> 00:06:50.160]   there's data streaming in from their users.
[00:06:50.160 --> 00:06:52.720]   They're processing that data in a streaming fashion
[00:06:52.720 --> 00:06:54.320]   to extract features.
[00:06:54.320 --> 00:06:56.240]   And then they're taking those features
[00:06:56.240 --> 00:06:57.880]   and they're feeding them into training.
[00:06:57.880 --> 00:07:01.240]   So they're incrementally training and updating
[00:07:01.240 --> 00:07:04.360]   their recommendation models using those features.
[00:07:04.360 --> 00:07:06.520]   And then they're taking those recommendation models
[00:07:06.520 --> 00:07:09.240]   and serving recommendations back to their users.
[00:07:09.240 --> 00:07:12.120]   And then that feeds back into the data collection.
[00:07:12.120 --> 00:07:14.040]   So you have all these different,
[00:07:14.040 --> 00:07:15.800]   to build a single application,
[00:07:15.800 --> 00:07:17.880]   you have a number of different moving parts
[00:07:17.880 --> 00:07:19.860]   and computational patterns.
[00:07:19.860 --> 00:07:22.040]   And one question they were asking themselves
[00:07:22.040 --> 00:07:23.600]   because they were doing online learning
[00:07:23.600 --> 00:07:25.360]   was how quickly can they update the model?
[00:07:25.360 --> 00:07:28.200]   How frequently can they push new models into production?
[00:07:28.200 --> 00:07:32.160]   And what is the benefit of doing that faster?
[00:07:32.160 --> 00:07:34.080]   And so they found when they initially
[00:07:34.080 --> 00:07:36.760]   implemented this system, it took about a day.
[00:07:36.760 --> 00:07:39.340]   They were updating the model about once a day.
[00:07:39.340 --> 00:07:41.120]   With a lot of optimizations,
[00:07:41.120 --> 00:07:44.640]   they were able to get it down to about once an hour.
[00:07:44.640 --> 00:07:48.440]   And that was, and when they did that optimization
[00:07:48.440 --> 00:07:51.680]   going from one day to one hour for pushing out new models,
[00:07:51.680 --> 00:07:54.080]   that led to a 5% increase in click-through rate,
[00:07:54.080 --> 00:07:55.400]   which was huge.
[00:07:55.400 --> 00:07:59.320]   So the natural question was, could they go further?
[00:07:59.320 --> 00:08:01.740]   And they wanted to, but given the way
[00:08:01.740 --> 00:08:04.400]   they had architected their application,
[00:08:04.400 --> 00:08:06.480]   they couldn't realistically do it.
[00:08:06.480 --> 00:08:10.320]   So, and to understand what the bottlenecks were there,
[00:08:10.320 --> 00:08:11.840]   what the limitations were,
[00:08:11.840 --> 00:08:13.640]   we have to talk a little bit about
[00:08:13.640 --> 00:08:16.120]   how they had architected their application.
[00:08:16.120 --> 00:08:18.120]   The way they were doing this, and in fact,
[00:08:18.120 --> 00:08:20.600]   one of the most common ways of building
[00:08:20.600 --> 00:08:23.080]   this kind of application is to essentially take
[00:08:23.080 --> 00:08:25.120]   each components of the pipeline,
[00:08:25.120 --> 00:08:27.480]   each different computational pattern
[00:08:27.480 --> 00:08:30.240]   and use the state-of-the-art off-the-shelf
[00:08:30.240 --> 00:08:32.060]   distributed system for that component.
[00:08:32.060 --> 00:08:35.820]   So you can use Spark or Flink for your stream processing.
[00:08:35.820 --> 00:08:39.660]   You can use TensorFlow or Horvod or PyTorch for training,
[00:08:39.660 --> 00:08:44.660]   and you can use a model serving system for the serving part.
[00:08:44.660 --> 00:08:45.900]   And then you essentially take
[00:08:45.900 --> 00:08:47.700]   all these different distributed systems
[00:08:47.700 --> 00:08:50.460]   and you have to glue them together and stitch them together.
[00:08:50.460 --> 00:08:53.660]   And this, it turns out this is a really difficult way
[00:08:53.660 --> 00:08:55.940]   to develop applications.
[00:08:55.940 --> 00:08:59.460]   You have to learn how to use all these different systems.
[00:08:59.460 --> 00:09:01.620]   You have to have potentially have infrastructure teams
[00:09:01.620 --> 00:09:03.780]   that are managing these different systems
[00:09:03.780 --> 00:09:05.120]   and maintaining them.
[00:09:05.120 --> 00:09:08.180]   You have to have, for the application itself,
[00:09:08.180 --> 00:09:09.920]   it can be expensive because you're moving data
[00:09:09.920 --> 00:09:11.880]   between different systems.
[00:09:11.880 --> 00:09:15.400]   If you're trying to reason about handling failures,
[00:09:15.400 --> 00:09:17.740]   it's quite complex because each of these systems
[00:09:17.740 --> 00:09:19.980]   has a different approach to handling failures.
[00:09:19.980 --> 00:09:22.840]   And some of them don't handle failures at all.
[00:09:22.840 --> 00:09:25.820]   And so, and of course, if you have,
[00:09:25.820 --> 00:09:28.460]   if the engineers are specializing in different systems,
[00:09:28.460 --> 00:09:30.260]   then it becomes harder for people working
[00:09:30.260 --> 00:09:32.460]   on one part of the application to help out with another.
[00:09:32.460 --> 00:09:35.500]   And this is all just to build a single application.
[00:09:35.500 --> 00:09:38.420]   So what they were able to do here,
[00:09:38.420 --> 00:09:39.940]   going back to this picture,
[00:09:39.940 --> 00:09:43.860]   they were able to take this whole application
[00:09:43.860 --> 00:09:45.820]   and build it on top of Ray.
[00:09:45.820 --> 00:09:48.740]   And there, instead of using a stream processing system
[00:09:48.740 --> 00:09:50.900]   and a training system and a serving system,
[00:09:50.900 --> 00:09:53.740]   they were able to use libraries
[00:09:53.740 --> 00:09:55.420]   that are part of the Ray ecosystem.
[00:09:55.420 --> 00:09:58.860]   So for a stream processing or training or serving.
[00:09:58.860 --> 00:10:01.740]   And then when they did that,
[00:10:01.740 --> 00:10:04.780]   the application got simpler, got faster,
[00:10:04.780 --> 00:10:08.660]   and of course led to a boost in click-through rate.
[00:10:08.660 --> 00:10:10.760]   So this was a huge win for them.
[00:10:10.760 --> 00:10:13.260]   And this is the kind of example of an application
[00:10:13.260 --> 00:10:17.820]   that it would be very, it's quite difficult to do without,
[00:10:17.820 --> 00:10:19.740]   when you're combining specialized systems together.
[00:10:19.740 --> 00:10:24.100]   The typical approach is either to stitch a bunch
[00:10:24.100 --> 00:10:26.100]   of existing systems together
[00:10:26.100 --> 00:10:28.860]   or to build a new system from scratch.
[00:10:28.860 --> 00:10:32.940]   Okay, so that's a little bit about the kind of application
[00:10:32.940 --> 00:10:35.340]   where we think Ray can really excel.
[00:10:35.340 --> 00:10:39.380]   So now I'm gonna tell you a little bit more about Ray.
[00:10:39.380 --> 00:10:44.380]   So I wanna emphasize just a few key ideas.
[00:10:44.380 --> 00:10:49.740]   So one is the API, the concepts that Ray introduces.
[00:10:49.740 --> 00:10:51.420]   With distributed systems,
[00:10:52.360 --> 00:10:54.320]   the kinds of applications that you can support
[00:10:54.320 --> 00:10:58.640]   on top of a system are really affected
[00:10:58.640 --> 00:11:01.400]   by the concepts that that system is built around.
[00:11:01.400 --> 00:11:03.080]   So with something like Spark,
[00:11:03.080 --> 00:11:05.820]   the core concept is a dataset or a data frame,
[00:11:05.820 --> 00:11:07.640]   and then the API reflects that.
[00:11:07.640 --> 00:11:10.480]   With TensorFlow, it's about neural networks
[00:11:10.480 --> 00:11:11.360]   or computation graphs.
[00:11:11.360 --> 00:11:13.080]   So that's the core abstraction.
[00:11:13.080 --> 00:11:15.620]   And then all the APIs are built around that.
[00:11:15.620 --> 00:11:17.240]   With Ray, and of course,
[00:11:17.240 --> 00:11:19.000]   when you have these higher level concepts
[00:11:19.000 --> 00:11:20.760]   like a dataset or a neural network,
[00:11:20.760 --> 00:11:25.760]   that makes it easy to build applications
[00:11:25.760 --> 00:11:29.760]   that are like based on data processing
[00:11:29.760 --> 00:11:31.220]   or based on neural networks.
[00:11:31.220 --> 00:11:32.600]   But if you're trying to build something
[00:11:32.600 --> 00:11:35.200]   that doesn't fit neatly into that abstraction,
[00:11:35.200 --> 00:11:38.640]   then you end up having to do a lot of things
[00:11:38.640 --> 00:11:39.800]   to work around it.
[00:11:39.800 --> 00:11:41.000]   And so with Ray,
[00:11:41.000 --> 00:11:43.140]   the way we're trying to achieve generality here
[00:11:43.140 --> 00:11:45.680]   is by not really introducing new concepts.
[00:11:45.680 --> 00:11:47.960]   So not introducing a higher level concept
[00:11:47.960 --> 00:11:49.680]   like a dataset or neural network,
[00:11:49.680 --> 00:11:52.760]   but rather taking the existing concepts
[00:11:52.760 --> 00:11:53.860]   of functions and classes,
[00:11:53.860 --> 00:11:55.440]   which we know are general enough
[00:11:55.440 --> 00:11:58.880]   to express all kinds of workloads,
[00:11:58.880 --> 00:12:01.540]   and then translating those into the distributed setting
[00:12:01.540 --> 00:12:02.640]   as tasks and actors.
[00:12:02.640 --> 00:12:05.660]   And I'll give you a code example in a few slides.
[00:12:05.660 --> 00:12:09.820]   The second idea is about enabling parallelism
[00:12:09.820 --> 00:12:12.920]   through futures, through asynchronous computation.
[00:12:12.920 --> 00:12:14.920]   I'll show you that in the API.
[00:12:14.920 --> 00:12:16.280]   And then the third,
[00:12:16.280 --> 00:12:18.400]   something we care a lot about is performance,
[00:12:18.400 --> 00:12:22.480]   because if you're trying to build a general purpose system
[00:12:22.480 --> 00:12:24.600]   that can support lots of different workloads,
[00:12:24.600 --> 00:12:28.440]   then you inherit all the performance requirements
[00:12:28.440 --> 00:12:29.440]   of all those different workloads.
[00:12:29.440 --> 00:12:31.160]   So to be more general,
[00:12:31.160 --> 00:12:33.800]   you actually have to often be a lot more performance.
[00:12:33.800 --> 00:12:39.280]   And one way that sort of manifests itself here
[00:12:39.280 --> 00:12:42.120]   is with our distributed object store.
[00:12:42.120 --> 00:12:44.000]   And I'll talk more about that as well.
[00:12:44.000 --> 00:12:48.260]   Okay, so I'll start with the API.
[00:12:48.260 --> 00:12:51.380]   So this is a couple of Python functions,
[00:12:51.380 --> 00:12:52.880]   just regular Python code,
[00:12:52.880 --> 00:12:54.920]   reading a couple of arrays from a file
[00:12:54.920 --> 00:12:57.880]   and adding them together to get the sum.
[00:12:57.880 --> 00:12:59.600]   And here's a Python class,
[00:12:59.600 --> 00:13:00.800]   which we instantiate,
[00:13:00.800 --> 00:13:04.120]   and then it just has a counter that can get incremented,
[00:13:04.120 --> 00:13:05.760]   and we increment the counter twice.
[00:13:05.760 --> 00:13:09.020]   So just kind of a toy example.
[00:13:09.020 --> 00:13:11.120]   So what we can do with Ray is, like I said,
[00:13:11.120 --> 00:13:13.240]   we translate the functions and classes
[00:13:13.240 --> 00:13:14.360]   into the distributed setting.
[00:13:14.360 --> 00:13:17.640]   We do that by adding this Ray.remote decorator.
[00:13:17.640 --> 00:13:20.760]   So that turns a regular Python function
[00:13:20.760 --> 00:13:23.640]   into a Python function that can be executed
[00:13:23.640 --> 00:13:29.320]   asynchronously and execute somewhere in the cluster.
[00:13:29.320 --> 00:13:32.840]   And now when we actually wanna execute that Python function,
[00:13:32.840 --> 00:13:35.600]   we do it by appending the .remote suffix
[00:13:35.600 --> 00:13:37.200]   to the function name.
[00:13:37.200 --> 00:13:41.600]   Similarly, when we instantiate the counter,
[00:13:41.600 --> 00:13:43.940]   because it now has the Ray.remote decorator,
[00:13:45.000 --> 00:13:47.680]   what it does is it creates a copy of that counter object
[00:13:47.680 --> 00:13:50.400]   as a service or a little microservice or actor
[00:13:50.400 --> 00:13:51.600]   somewhere in the cluster.
[00:13:51.600 --> 00:13:56.100]   And now that other actors or other tasks
[00:13:56.100 --> 00:13:57.840]   can invoke methods on that actor,
[00:13:57.840 --> 00:14:00.860]   which are essentially send messages to that actor
[00:14:00.860 --> 00:14:03.360]   to execute its methods.
[00:14:03.360 --> 00:14:06.200]   And of course, when we invoke these methods
[00:14:06.200 --> 00:14:08.280]   or these remote functions,
[00:14:08.280 --> 00:14:11.600]   what gets returned is essentially a reference to the output.
[00:14:11.600 --> 00:14:13.360]   It's not the actual output of the computation.
[00:14:13.360 --> 00:14:15.880]   It's just like a future.
[00:14:15.880 --> 00:14:18.760]   And then if we wanna retrieve the actual value
[00:14:18.760 --> 00:14:21.680]   from the computation, we can call Ray.get.
[00:14:21.680 --> 00:14:23.200]   So this is the API in a nutshell.
[00:14:23.200 --> 00:14:26.960]   It's just a few, it's not a huge API.
[00:14:26.960 --> 00:14:29.200]   It's just a few methods,
[00:14:29.200 --> 00:14:30.480]   but it's general enough to express
[00:14:30.480 --> 00:14:32.300]   all kinds of computations on top.
[00:14:32.300 --> 00:14:38.600]   Okay, so I will walk through what happens
[00:14:38.600 --> 00:14:40.960]   if you actually call this.
[00:14:40.960 --> 00:14:44.880]   So if you call the first function, readArray.remote,
[00:14:44.880 --> 00:14:47.600]   what happens is that a task gets scheduled
[00:14:47.600 --> 00:14:48.720]   somewhere in the cluster,
[00:14:48.720 --> 00:14:50.600]   running on one of these machines.
[00:14:50.600 --> 00:14:53.160]   It immediately returns a future.
[00:14:53.160 --> 00:14:56.600]   So you can call, invoke the second copy of the function
[00:14:56.600 --> 00:14:58.800]   before the first one has finished executing.
[00:14:58.800 --> 00:15:02.720]   We can take the two futures that were passed,
[00:15:02.720 --> 00:15:04.840]   that were returned by the first function calls,
[00:15:04.840 --> 00:15:07.680]   pass those into a third add task.
[00:15:07.680 --> 00:15:10.220]   And then that gets scheduled somewhere in the cluster.
[00:15:10.220 --> 00:15:12.720]   That third task is not gonna run,
[00:15:12.720 --> 00:15:14.920]   until the first two tasks have finished running
[00:15:14.920 --> 00:15:16.560]   and then their outputs will have,
[00:15:16.560 --> 00:15:18.480]   the outputs of the first two readArray tasks
[00:15:18.480 --> 00:15:20.240]   will be shipped under the hood
[00:15:20.240 --> 00:15:23.800]   to wherever the add task is getting executed.
[00:15:23.800 --> 00:15:25.640]   And then once those inputs have arrived,
[00:15:25.640 --> 00:15:27.680]   the add task will execute.
[00:15:27.680 --> 00:15:30.800]   And then lastly, if you wanna retrieve the results,
[00:15:30.800 --> 00:15:34.360]   you can call Ray.get to actually fetch the value.
[00:15:34.360 --> 00:15:36.820]   So that's what it looks like under the hood.
[00:15:39.440 --> 00:15:43.760]   So to say just quickly about the distributed object store,
[00:15:43.760 --> 00:15:48.200]   we have, when we actually execute one of these methods,
[00:15:48.200 --> 00:15:50.240]   it gets invoked on some machine
[00:15:50.240 --> 00:15:52.480]   and it gets executed on that machine.
[00:15:52.480 --> 00:15:56.020]   And then that machine runs the actual Python function,
[00:15:56.020 --> 00:15:57.560]   which produces the output.
[00:15:57.560 --> 00:15:59.160]   And then when that happens,
[00:15:59.160 --> 00:16:01.320]   that output stays on that machine.
[00:16:01.320 --> 00:16:04.760]   It doesn't get shipped back to the original node.
[00:16:04.760 --> 00:16:08.840]   And what that means is that you can then pass this IDX
[00:16:08.840 --> 00:16:13.080]   into a second function, which can then run somewhere else.
[00:16:13.080 --> 00:16:16.120]   And if that second function uses the output
[00:16:16.120 --> 00:16:18.700]   of the first function, that output will already be there.
[00:16:18.700 --> 00:16:20.280]   So it doesn't need to get shipped back and forth
[00:16:20.280 --> 00:16:24.400]   unnecessarily, or go through node one
[00:16:24.400 --> 00:16:26.040]   if it doesn't have to.
[00:16:26.040 --> 00:16:29.160]   And so the benefit there is that,
[00:16:29.160 --> 00:16:32.300]   especially if we're using, Ray makes use of shared memory,
[00:16:32.300 --> 00:16:34.080]   so that X can actually live in shared memory.
[00:16:34.080 --> 00:16:38.020]   We don't even need to copy X from one worker process
[00:16:38.020 --> 00:16:40.000]   to another worker process,
[00:16:40.000 --> 00:16:42.960]   especially with a lot of machine learning workloads using,
[00:16:42.960 --> 00:16:47.360]   a lot of computation is very heavy in numerical data,
[00:16:47.360 --> 00:16:49.440]   or neural network weights, or things like that.
[00:16:49.440 --> 00:16:52.720]   Having automatically storing this stuff in shared memory
[00:16:52.720 --> 00:16:54.800]   so it can be accessed by a bunch of different
[00:16:54.800 --> 00:16:56.920]   worker processes without creating copies,
[00:16:56.920 --> 00:16:59.840]   or without doing expensive deserialization
[00:16:59.840 --> 00:17:01.880]   can be a big win.
[00:17:01.880 --> 00:17:04.300]   So that's what I wanna say about performance.
[00:17:05.480 --> 00:17:10.320]   The architecture is similar to some other
[00:17:10.320 --> 00:17:11.680]   distributed systems.
[00:17:11.680 --> 00:17:13.800]   There are worker processes on each machine
[00:17:13.800 --> 00:17:15.880]   that actually execute the computation.
[00:17:15.880 --> 00:17:17.960]   There's a shared memory object store on each machine
[00:17:17.960 --> 00:17:19.840]   that holds objects in shared memory,
[00:17:19.840 --> 00:17:23.120]   so that we don't have to create copies.
[00:17:23.120 --> 00:17:25.500]   There's a scheduler on each machine.
[00:17:25.500 --> 00:17:28.400]   This is important for having this decentralized
[00:17:28.400 --> 00:17:30.440]   or distributed scheduler is important
[00:17:30.440 --> 00:17:33.440]   for avoiding centralized bottlenecks.
[00:17:33.440 --> 00:17:36.920]   So you can have a very high task throughput,
[00:17:36.920 --> 00:17:40.480]   which is important for lots of kinds of workloads,
[00:17:40.480 --> 00:17:43.360]   especially model serving where you may have
[00:17:43.360 --> 00:17:45.660]   tons and tons of different requests coming in
[00:17:45.660 --> 00:17:47.440]   that you're responding to.
[00:17:47.440 --> 00:17:51.480]   And then lastly, I wanna say there's a,
[00:17:51.480 --> 00:17:52.800]   this global control store,
[00:17:52.800 --> 00:17:55.240]   which stores important system metadata,
[00:17:55.240 --> 00:17:57.540]   which can be used for recovering from failures.
[00:17:57.540 --> 00:18:01.080]   Okay.
[00:18:01.080 --> 00:18:03.160]   And that'll just show one performance slide,
[00:18:03.160 --> 00:18:05.960]   which is about task throughput.
[00:18:05.960 --> 00:18:08.800]   So this is a difference from going from Ray 0.7
[00:18:08.800 --> 00:18:12.140]   to Ray 0.8 when we introduced direct actor calls,
[00:18:12.140 --> 00:18:15.120]   which basically a way for workers to talk directly
[00:18:15.120 --> 00:18:17.200]   from one worker to another
[00:18:17.200 --> 00:18:19.540]   without going through the scheduler.
[00:18:19.540 --> 00:18:22.920]   So this made a huge difference in terms of task throughput,
[00:18:22.920 --> 00:18:25.880]   scaling to hundreds of thousands of tasks per second
[00:18:25.880 --> 00:18:30.700]   on a small cluster with a couple hundred CPU cores.
[00:18:30.700 --> 00:18:33.640]   And of course you can scale much higher
[00:18:33.640 --> 00:18:35.660]   given larger clusters.
[00:18:35.660 --> 00:18:38.520]   Okay.
[00:18:38.520 --> 00:18:42.480]   So one thing I wanna say quickly is that Ray comes
[00:18:42.480 --> 00:18:44.320]   with a bunch of different libraries.
[00:18:44.320 --> 00:18:47.100]   They're native libraries that are developed as part of Ray.
[00:18:47.100 --> 00:18:49.640]   So RLib is one of the first we started building
[00:18:49.640 --> 00:18:50.980]   for reinforcement learning.
[00:18:50.980 --> 00:18:55.260]   Tune is one that actually integrates with weights and biases
[00:18:55.260 --> 00:18:58.240]   and it's used for hyperparameter tuning.
[00:18:58.240 --> 00:18:59.740]   We started a library recently
[00:18:59.740 --> 00:19:03.060]   for deploying machine learning models in production.
[00:19:03.060 --> 00:19:04.860]   This is at a much earlier stage,
[00:19:04.860 --> 00:19:08.180]   but it's seeing a lot of use.
[00:19:08.180 --> 00:19:10.620]   And we recently started a library
[00:19:10.620 --> 00:19:15.620]   for elastic distributed training of machine learning models
[00:19:15.620 --> 00:19:18.160]   using PyTorch and TensorFlow under the hood.
[00:19:18.160 --> 00:19:22.420]   But perhaps the even more exciting part
[00:19:22.420 --> 00:19:24.720]   is all the libraries in the ecosystem
[00:19:24.720 --> 00:19:26.140]   that are integrating with Ray.
[00:19:26.140 --> 00:19:29.800]   So you may be familiar with Optuna and Hyperopt
[00:19:29.800 --> 00:19:32.240]   which are popular hyperparameter tuning libraries.
[00:19:32.240 --> 00:19:33.560]   Horovod for training.
[00:19:33.560 --> 00:19:35.180]   These all are integrating with Ray.
[00:19:35.180 --> 00:19:37.360]   Tune for hyperparameter tuning.
[00:19:37.360 --> 00:19:38.880]   Hugging Face and Spacey,
[00:19:38.880 --> 00:19:41.600]   two of the most popular NLP libraries
[00:19:41.600 --> 00:19:43.000]   integrate with Ray as well.
[00:19:43.000 --> 00:19:46.920]   A lot of machine learning platforms
[00:19:46.920 --> 00:19:50.360]   or cloud machine learning platforms like AWS SageMaker,
[00:19:50.360 --> 00:19:53.140]   Azure Machine Learning are starting to use RLib
[00:19:53.140 --> 00:19:54.080]   and Tune as well.
[00:19:55.120 --> 00:19:57.900]   Dask is a popular open source library
[00:19:57.900 --> 00:20:00.300]   for distributed Python
[00:20:00.300 --> 00:20:02.940]   and it has a popular data frames library.
[00:20:02.940 --> 00:20:06.300]   And you can run Dask on top of Ray to scale more.
[00:20:06.300 --> 00:20:09.080]   We integrate with Weights and Biases
[00:20:09.080 --> 00:20:11.040]   for tracking your experiments.
[00:20:11.040 --> 00:20:15.560]   And there's other libraries from Intel as well.
[00:20:15.560 --> 00:20:18.300]   And lastly, there are a number of other libraries
[00:20:18.300 --> 00:20:19.580]   I don't have time to mention,
[00:20:19.580 --> 00:20:21.660]   but that are really,
[00:20:21.660 --> 00:20:24.420]   that are, form part of this ecosystem.
[00:20:24.420 --> 00:20:26.340]   And the cool thing here is not just that,
[00:20:26.340 --> 00:20:29.040]   you can run one of these libraries on top of Ray,
[00:20:29.040 --> 00:20:30.080]   but rather that you can,
[00:20:30.080 --> 00:20:31.960]   when you're building your applications,
[00:20:31.960 --> 00:20:34.460]   you can pick and choose the state of the art libraries
[00:20:34.460 --> 00:20:36.080]   that you wanna use off the shelf
[00:20:36.080 --> 00:20:38.880]   and integrate those to build a single application.
[00:20:38.880 --> 00:20:41.480]   Kind of like how on your laptop
[00:20:41.480 --> 00:20:42.720]   to develop your application,
[00:20:42.720 --> 00:20:45.180]   you can import NumPy and Pandas
[00:20:45.180 --> 00:20:47.540]   and use them all together to build your application.
[00:20:47.540 --> 00:20:49.100]   Right now in the distributed setting,
[00:20:49.100 --> 00:20:50.260]   there's nothing like that.
[00:20:50.260 --> 00:20:52.820]   You have standalone distributed systems
[00:20:52.820 --> 00:20:54.640]   and sort of that's the value we see
[00:20:54.640 --> 00:20:56.000]   with building an ecosystem.
[00:20:56.000 --> 00:20:59.480]   So if you're interested in learning more about Ray,
[00:20:59.480 --> 00:21:02.800]   and I think we can send a link in the chat,
[00:21:02.800 --> 00:21:06.060]   we're going to, we're hosting a summit
[00:21:06.060 --> 00:21:07.320]   at the end of this month.
[00:21:07.320 --> 00:21:10.400]   So we'll be showcasing a lot of interesting use cases,
[00:21:10.400 --> 00:21:12.140]   how companies are using Ray in production
[00:21:12.140 --> 00:21:14.960]   to scale their applications.
[00:21:14.960 --> 00:21:17.440]   And we'll also be,
[00:21:17.440 --> 00:21:18.840]   there'll be talks from Weights and Biases
[00:21:18.840 --> 00:21:20.320]   and Hugging Face and Spacey
[00:21:20.320 --> 00:21:23.160]   and a number of different libraries that are using Ray.
[00:21:23.160 --> 00:21:26.320]   And so, highly encourage you to check it out.
[00:21:26.320 --> 00:21:30.160]   And yeah, so I just wanna leave it at that
[00:21:30.160 --> 00:21:32.400]   and say Ray is a universal framework
[00:21:32.400 --> 00:21:33.780]   for distributed computing
[00:21:33.780 --> 00:21:36.560]   and we're building this rich ecosystem on top of it.
[00:21:36.560 --> 00:21:37.920]   So if you're a library developer
[00:21:37.920 --> 00:21:39.360]   and are working to,
[00:21:39.360 --> 00:21:40.880]   are interested in scaling your library
[00:21:40.880 --> 00:21:42.180]   or making it easy to,
[00:21:42.180 --> 00:21:45.840]   for people to run your library in the distributed setting,
[00:21:45.840 --> 00:21:48.080]   we'd love to, Ray may be a good fit for that
[00:21:48.080 --> 00:21:49.880]   and we'd love to help out with that.
[00:21:50.360 --> 00:21:51.640]   So thanks a lot.
[00:21:51.640 --> 00:21:59.440]   - Great, thanks Robert for a really great presentation.
[00:21:59.440 --> 00:22:00.920]   It's very exciting.
[00:22:00.920 --> 00:22:03.360]   I guess I'd never seen the sort of like high level vision
[00:22:03.360 --> 00:22:04.440]   behind Ray.
[00:22:04.440 --> 00:22:05.440]   Like I just heard, oh, you know,
[00:22:05.440 --> 00:22:08.000]   Ray helps you run at any scale, you know,
[00:22:08.000 --> 00:22:10.160]   Ray helps you run, you know,
[00:22:10.160 --> 00:22:12.560]   on your computer all the way up to a thousand GPUs.
[00:22:12.560 --> 00:22:15.120]   And this idea that you said at the end about, you know,
[00:22:15.120 --> 00:22:16.740]   it's sort of like how on your laptop,
[00:22:16.740 --> 00:22:19.160]   you're not stuck, you know, in any, you know,
[00:22:19.160 --> 00:22:20.640]   building these monolithic services,
[00:22:20.640 --> 00:22:22.400]   but you can compose an app together.
[00:22:22.400 --> 00:22:23.640]   That's a really cool vision
[00:22:23.640 --> 00:22:25.640]   and thanks for sharing it with us.
[00:22:25.640 --> 00:22:26.480]   - Thank you.
[00:22:26.480 --> 00:22:30.280]   - I have a couple of questions that I just wrote down
[00:22:30.280 --> 00:22:31.120]   while we were going
[00:22:31.120 --> 00:22:34.520]   and we'll get more from folks on YouTube and Zoom,
[00:22:34.520 --> 00:22:35.820]   but just to start off.
[00:22:35.820 --> 00:22:36.920]   So you mentioned, you know,
[00:22:36.920 --> 00:22:37.920]   that one of the motivations here
[00:22:37.920 --> 00:22:40.120]   was that the future is distributed computing
[00:22:40.120 --> 00:22:43.340]   and you sort of motivated it by the particular workloads
[00:22:43.340 --> 00:22:45.000]   that people are doing now that it's, you know,
[00:22:45.000 --> 00:22:46.800]   machine learning is a data heavy workflow
[00:22:46.800 --> 00:22:48.600]   that like kind of everybody wants to use.
[00:22:48.600 --> 00:22:50.000]   The internet is a famous example
[00:22:50.000 --> 00:22:52.400]   of successful distributed computing.
[00:22:52.400 --> 00:22:54.960]   Do you think that that's driven in part
[00:22:54.960 --> 00:22:57.960]   by the sort of like bending away from the Moore's law curve
[00:22:57.960 --> 00:22:59.280]   that a lot of people have talked about,
[00:22:59.280 --> 00:23:01.720]   or do you think that those are two separate phenomena?
[00:23:01.720 --> 00:23:04.480]   - You know, that's a really good point.
[00:23:04.480 --> 00:23:06.360]   So there are a couple of things going on here.
[00:23:06.360 --> 00:23:11.360]   One is, so yeah, so we know Moore's law is,
[00:23:11.360 --> 00:23:13.000]   you know, has kind of flattened out, right?
[00:23:13.000 --> 00:23:17.400]   And it's not keeping up with what it used to be doing.
[00:23:18.320 --> 00:23:22.560]   Now, even if it were continuing at its original pace,
[00:23:22.560 --> 00:23:24.240]   it still wouldn't be enough,
[00:23:24.240 --> 00:23:26.560]   because if you look at the amount of compute required
[00:23:26.560 --> 00:23:29.380]   to do machine learning, you know, going,
[00:23:29.380 --> 00:23:33.760]   whether that's like from AlexNet a while back
[00:23:33.760 --> 00:23:37.880]   to things like AlphaGo and GPT-3 now,
[00:23:37.880 --> 00:23:42.040]   to do that, that's growing way, way faster than Moore's law.
[00:23:42.040 --> 00:23:44.120]   And so even if Moore's law had continued,
[00:23:44.120 --> 00:23:45.960]   there's still a huge gap.
[00:23:45.960 --> 00:23:50.960]   And so, and it's not just compute, it's also memory.
[00:23:50.960 --> 00:23:55.520]   You know, the model sizes are increasing a huge amount.
[00:23:55.520 --> 00:23:57.320]   And this, you might ask like,
[00:23:57.320 --> 00:23:59.360]   what about specialized hardware,
[00:23:59.360 --> 00:24:02.520]   like accelerators, GPUs or TPUs?
[00:24:02.520 --> 00:24:05.840]   You know, when you have these different architectures,
[00:24:05.840 --> 00:24:07.440]   right, like a GPU or a TPU,
[00:24:07.440 --> 00:24:11.040]   you do get a huge, you know, big boost in terms of compute.
[00:24:11.040 --> 00:24:13.760]   But then when you fix the architecture,
[00:24:13.760 --> 00:24:16.800]   it still increases, you know, it still grows.
[00:24:16.800 --> 00:24:18.880]   The performance improvements for GPUs or TPUs
[00:24:18.880 --> 00:24:22.120]   are still growing in a Moore's law like fashion.
[00:24:22.120 --> 00:24:24.240]   So while it is giving you a boost,
[00:24:24.240 --> 00:24:27.840]   it's not closing the gap.
[00:24:27.840 --> 00:24:30.600]   So, and when you're scaling, you know,
[00:24:30.600 --> 00:24:34.840]   subsequent generations of TPUs typically are actually
[00:24:34.840 --> 00:24:39.920]   essentially embedded, you know, distributed themselves.
[00:24:39.920 --> 00:24:43.080]   They're putting multiple chips in a single TPU
[00:24:43.080 --> 00:24:44.480]   and things like that.
[00:24:44.480 --> 00:24:48.680]   So we have, so that's a big factor, right?
[00:24:48.680 --> 00:24:53.680]   Just the, yes, the fact that the Moore's law is over,
[00:24:53.680 --> 00:24:56.080]   the fact that the amount of compute required
[00:24:56.080 --> 00:24:58.560]   to do machine learning is growing a ton.
[00:24:58.560 --> 00:25:00.440]   And then furthermore, and the point
[00:25:00.440 --> 00:25:01.840]   I was trying to emphasize,
[00:25:01.840 --> 00:25:04.400]   that machine learning is getting embedded
[00:25:04.400 --> 00:25:06.880]   in more and more different kinds of applications.
[00:25:06.880 --> 00:25:10.800]   So it's not just, you know, you could say,
[00:25:10.800 --> 00:25:13.280]   oh, it's just if you wanna do machine learning training,
[00:25:13.280 --> 00:25:15.280]   then you need to use all this compute.
[00:25:15.280 --> 00:25:16.960]   That's not the case.
[00:25:16.960 --> 00:25:19.600]   Machine learning, it's not really this isolated workload
[00:25:19.600 --> 00:25:21.560]   where you just go and train a model, right?
[00:25:21.560 --> 00:25:23.920]   It's being integrated into more and more
[00:25:23.920 --> 00:25:26.360]   different kinds of applications,
[00:25:26.360 --> 00:25:28.360]   whether that's offline or online.
[00:25:28.360 --> 00:25:32.160]   And that's, you know, combined with all other sorts
[00:25:32.160 --> 00:25:33.160]   of business logic.
[00:25:33.160 --> 00:25:37.000]   - Yeah, that's actually, it's interesting you bring that up
[00:25:37.000 --> 00:25:39.280]   'cause that was a major point of a recent presentation
[00:25:39.280 --> 00:25:40.760]   in the salon by Andy Frilich,
[00:25:40.760 --> 00:25:43.400]   who's at Google Developer Program,
[00:25:43.400 --> 00:25:44.800]   who's big on this idea that, you know,
[00:25:44.800 --> 00:25:47.640]   now the main like use for machine learning models
[00:25:47.640 --> 00:25:49.960]   is to integrate multiple models together,
[00:25:49.960 --> 00:25:52.360]   not to do sort of like the end-to-end training
[00:25:52.360 --> 00:25:55.880]   that people initially thought was the main utility
[00:25:55.880 --> 00:25:59.160]   or the main sort of workflow for a machine learning model.
[00:25:59.160 --> 00:26:04.160]   So it's good to see that this idea is more broadly spread.
[00:26:04.160 --> 00:26:06.320]   - And a lot of people, you know, I mean,
[00:26:07.240 --> 00:26:09.600]   even if you wanna do something as simple as
[00:26:09.600 --> 00:26:14.000]   using TensorFlow to train your neural networks
[00:26:14.000 --> 00:26:17.320]   and then using Spark to do your data processing,
[00:26:17.320 --> 00:26:19.520]   and then of course you wanna be able to do both, right?
[00:26:19.520 --> 00:26:20.840]   You wanna be able to process your data
[00:26:20.840 --> 00:26:22.640]   and then feed it into training.
[00:26:22.640 --> 00:26:25.480]   Even that, there's aren't great ways
[00:26:25.480 --> 00:26:26.720]   to do that off the shelf.
[00:26:26.720 --> 00:26:28.960]   - Definitely.
[00:26:28.960 --> 00:26:34.040]   Yeah, I wanna make sure we actually get to these
[00:26:34.040 --> 00:26:36.360]   a couple of questions that came in.
[00:26:36.360 --> 00:26:38.320]   They're mostly actually about Tune
[00:26:38.320 --> 00:26:40.720]   and they're sort of more technical questions
[00:26:40.720 --> 00:26:41.560]   about optimization.
[00:26:41.560 --> 00:26:45.160]   So I hope you brought your optimization hat here.
[00:26:45.160 --> 00:26:48.200]   It's basically, there's some curiosity about, you know,
[00:26:48.200 --> 00:26:49.400]   what's going on under the hood
[00:26:49.400 --> 00:26:51.640]   to sort of terminate bad hyperparameter values
[00:26:51.640 --> 00:26:52.680]   during a search.
[00:26:52.680 --> 00:26:53.920]   And then relatedly, you know,
[00:26:53.920 --> 00:26:56.360]   which search methods would you recommend
[00:26:56.360 --> 00:26:59.120]   for folks who have limited compute
[00:26:59.120 --> 00:27:01.760]   and maybe are like very aggressively trying to cut down
[00:27:01.760 --> 00:27:04.480]   on their compute budgets, you know,
[00:27:04.480 --> 00:27:07.560]   between like Bayesian optimization with hyperband
[00:27:07.560 --> 00:27:10.960]   or population-based training or, you know, something else.
[00:27:10.960 --> 00:27:14.440]   - Yeah, so I'll say the answer that with the caveat
[00:27:14.440 --> 00:27:16.440]   that I might say the wrong thing.
[00:27:16.440 --> 00:27:19.360]   And I'm not 100%,
[00:27:19.360 --> 00:27:21.960]   I'm not actually the best person to answer the question.
[00:27:21.960 --> 00:27:24.760]   So what's going on under the hood
[00:27:24.760 --> 00:27:27.280]   when it comes to terminating bad trials?
[00:27:27.280 --> 00:27:31.120]   So there are two aspects of that, right?
[00:27:31.120 --> 00:27:32.600]   One is like, what is the mechanism
[00:27:32.600 --> 00:27:34.080]   for terminating bad trials?
[00:27:34.080 --> 00:27:36.000]   And then the other is how is the decision made
[00:27:36.000 --> 00:27:38.160]   to terminate a bad trial?
[00:27:38.160 --> 00:27:41.880]   I think for making the decision,
[00:27:41.880 --> 00:27:43.760]   there are certain, you know,
[00:27:43.760 --> 00:27:46.320]   metrics that you can report when you're using Tune and Track.
[00:27:46.320 --> 00:27:50.480]   And then if those fall below, you know, a certain amount
[00:27:50.480 --> 00:27:55.480]   or if those are much lower than other, you know,
[00:27:55.480 --> 00:27:56.960]   the performance of other trials,
[00:27:56.960 --> 00:27:59.760]   then that's how we choose to, which ones to terminate.
[00:28:00.960 --> 00:28:04.600]   And the other aspect of like how we actually terminate it,
[00:28:04.600 --> 00:28:08.600]   that's, so Tune is implemented using Ray Actors,
[00:28:08.600 --> 00:28:11.200]   which is one of the primitives that we showed here.
[00:28:11.200 --> 00:28:13.400]   And I think the way it's done
[00:28:13.400 --> 00:28:16.360]   is by essentially terminating the Actors,
[00:28:16.360 --> 00:28:19.680]   although it's also possible to pause some of the Actors
[00:28:19.680 --> 00:28:23.000]   and then resume the training that they're doing later.
[00:28:23.000 --> 00:28:27.160]   When it comes to what would you use
[00:28:27.160 --> 00:28:30.920]   if you're trying to not spend all your money on compute?
[00:28:30.920 --> 00:28:31.840]   A couple of things there.
[00:28:31.840 --> 00:28:33.960]   So one, which is not part of the question,
[00:28:33.960 --> 00:28:37.440]   but spot, you know, you can use spot instances,
[00:28:37.440 --> 00:28:40.360]   and this is a great case where you,
[00:28:40.360 --> 00:28:42.240]   if you have a system like Ray or Tune,
[00:28:42.240 --> 00:28:46.040]   which is resilient to failures and preemption of instances,
[00:28:46.040 --> 00:28:48.560]   then you can actually use cheaper spot instances.
[00:28:48.560 --> 00:28:52.800]   And then the second thing is, as for the actual,
[00:28:52.800 --> 00:28:54.640]   the answer, I think that the answer
[00:28:54.640 --> 00:28:57.680]   is not gonna be grid search.
[00:28:57.680 --> 00:28:59.000]   And as a simple baseline,
[00:28:59.000 --> 00:29:00.800]   random search is probably pretty good,
[00:29:00.800 --> 00:29:05.080]   but I've seen good results from population-based training.
[00:29:05.080 --> 00:29:09.320]   - Yeah, I think, you know, I would agree,
[00:29:09.320 --> 00:29:12.240]   I think with your assessment that random search is always,
[00:29:12.240 --> 00:29:13.600]   you know, it's a great baseline.
[00:29:13.600 --> 00:29:17.520]   It's a lot simpler than Bayesian methods often.
[00:29:17.520 --> 00:29:18.800]   And the addition of, you know,
[00:29:18.800 --> 00:29:21.560]   something like Hyperband to cut off trials early
[00:29:21.560 --> 00:29:26.000]   is usually enough to, you know, make that easy use.
[00:29:26.000 --> 00:29:27.480]   And that's something that's built into tools
[00:29:27.480 --> 00:29:29.360]   like Weights and Biases and Ray Tune
[00:29:29.360 --> 00:29:31.160]   and all kinds of other libraries.
[00:29:31.160 --> 00:29:33.040]   So you don't have to reinvent the wheel on that.
[00:29:33.040 --> 00:29:34.760]   - Absolutely, yeah.
[00:29:34.760 --> 00:29:36.600]   And one, you know, one thing that's nice about,
[00:29:36.600 --> 00:29:39.520]   well, about random search is just that
[00:29:39.520 --> 00:29:43.360]   it's hard to implement it incorrectly.
[00:29:43.360 --> 00:29:44.840]   - There you go, yeah.
[00:29:44.840 --> 00:29:47.920]   It's stateless, which is really nice, yeah.
[00:29:47.920 --> 00:29:51.560]   So actually, you know, I hate to cut you off,
[00:29:51.560 --> 00:29:54.160]   but you do have your 5.30 deadline.
[00:29:54.160 --> 00:29:55.960]   So I don't wanna make you late to anything.
[00:29:55.960 --> 00:29:57.160]   - Cool, thank you so much.
[00:29:57.160 --> 00:29:58.680]   It's really a lot of fun.
[00:29:58.680 --> 00:29:59.720]   - Yeah, thanks for coming, Robert.
[00:29:59.720 --> 00:30:00.560]   Take care.
[00:30:00.560 --> 00:30:08.160]   All right, so before we switch over to our second speaker,
[00:30:08.160 --> 00:30:12.160]   I just wanted to, I am going to do a quick little
[00:30:12.160 --> 00:30:17.600]   share with you all what's coming up at the next salon.
[00:30:17.600 --> 00:30:21.280]   So let me do a quick share screen.
[00:30:21.280 --> 00:30:22.560]   Great, okay.
[00:30:22.560 --> 00:30:25.920]   So what we have coming up at our next Deep Learning Salon,
[00:30:25.920 --> 00:30:28.640]   which is two weeks from now at the, you know,
[00:30:28.640 --> 00:30:31.680]   same bat time, same bat channel,
[00:30:31.680 --> 00:30:35.720]   we'll have one research talk and one talk
[00:30:35.720 --> 00:30:38.400]   that's a little bit more application and engineering driven.
[00:30:38.400 --> 00:30:40.920]   So the first talk is from Sarah Hooker,
[00:30:40.920 --> 00:30:43.760]   who is a researcher at Google Brain.
[00:30:43.760 --> 00:30:45.160]   She's done some really excellent research.
[00:30:45.160 --> 00:30:46.920]   I've followed her for a while now,
[00:30:46.920 --> 00:30:49.160]   since she did some work on ML explainability
[00:30:49.160 --> 00:30:50.440]   and interpretability.
[00:30:50.440 --> 00:30:53.280]   The work she'll be presenting now is a new paper
[00:30:53.280 --> 00:30:56.800]   on using the variance of gradients to detect
[00:30:56.800 --> 00:30:58.160]   when something is an outlier
[00:30:58.160 --> 00:30:59.520]   or maybe a really difficult example.
[00:30:59.520 --> 00:31:02.520]   So with applications in outlier detection, obviously,
[00:31:02.520 --> 00:31:04.480]   but also in curriculum learning.
[00:31:04.480 --> 00:31:06.760]   So really great work, very, you know,
[00:31:06.760 --> 00:31:10.120]   sort of fundamental and very interesting work
[00:31:10.120 --> 00:31:12.520]   that I'm excited to hear her talk about
[00:31:12.520 --> 00:31:14.760]   and to get a chance to ask her some questions about.
[00:31:14.760 --> 00:31:17.720]   And then we'll also have Hannes Hopke,
[00:31:17.720 --> 00:31:21.400]   who is a senior ML engineer at SAP.
[00:31:21.400 --> 00:31:25.240]   He is the author of a book on how to build ML pipelines.
[00:31:25.240 --> 00:31:26.640]   So what Robert talked about,
[00:31:26.640 --> 00:31:29.520]   about integrating, you know, the data into the,
[00:31:29.520 --> 00:31:32.840]   into the data loading part into your model
[00:31:32.840 --> 00:31:35.200]   and serving your model and then swapping out models,
[00:31:35.200 --> 00:31:36.520]   all these pieces of, you know,
[00:31:36.520 --> 00:31:38.640]   building a real application with machine learning.
[00:31:38.640 --> 00:31:41.640]   Hannes Hopke quite literally wrote the book on it.
[00:31:41.640 --> 00:31:42.840]   And so I'm really excited to see that.
[00:31:42.840 --> 00:31:46.040]   I think his preferred toolkit is TensorFlow extended.
[00:31:46.040 --> 00:31:47.640]   So that'll probably be the main focus,
[00:31:47.640 --> 00:31:49.360]   but I'm sure there'll be lots of lessons
[00:31:49.360 --> 00:31:51.960]   that are useful to people using any framework
[00:31:51.960 --> 00:31:53.480]   to build ML applications.
[00:31:53.480 --> 00:31:56.480]   So by the way, you know,
[00:31:56.480 --> 00:31:58.840]   I mentioned some of the stuff from previous salons
[00:31:58.840 --> 00:32:01.960]   that touched on the things that Robert talked about.
[00:32:01.960 --> 00:32:03.320]   If you want to catch salons you missed,
[00:32:03.320 --> 00:32:06.120]   you can find them on our YouTube channel.
[00:32:06.120 --> 00:32:08.480]   So we've got a playlist of all of our past salons.
[00:32:08.480 --> 00:32:11.000]   So go ahead and head there.
[00:32:11.000 --> 00:32:14.160]   If you want more machine learning content,
[00:32:14.160 --> 00:32:16.600]   more conversations, come to the,
[00:32:16.600 --> 00:32:17.480]   come to our Slack forum.
[00:32:17.480 --> 00:32:21.640]   We just had, or yeah, we had the CEO of Kaggle,
[00:32:21.640 --> 00:32:24.320]   Anthony Goldblum, it looks like that was back in August.
[00:32:24.320 --> 00:32:25.680]   We also had the folks from Raytune
[00:32:25.680 --> 00:32:27.200]   who answered a bunch of those questions
[00:32:27.200 --> 00:32:29.000]   about hyperparameter optimization
[00:32:29.000 --> 00:32:31.480]   that folks were asking in the chat.
[00:32:31.480 --> 00:32:35.400]   Those were questions that got answered by the Raytune folks.
[00:32:35.400 --> 00:32:37.520]   We also do lots of other regular projects.
[00:32:37.520 --> 00:32:42.360]   So the link to join that is bit.ly/slack-forum.
[00:32:42.360 --> 00:32:45.240]   So that's all I have to say about
[00:32:45.240 --> 00:32:48.280]   our wonderful machine learning community
[00:32:48.280 --> 00:32:49.880]   and our wonderful podcast.
[00:32:49.880 --> 00:32:52.400]   So it's now time in the world of wonderful things
[00:32:52.400 --> 00:32:55.080]   to introduce our wonderful next guest,
[00:32:55.080 --> 00:33:00.080]   Rok Novosel, who is joining us from the,
[00:33:00.080 --> 00:33:03.320]   from Slovenia, from Ljubljana,
[00:33:03.320 --> 00:33:07.600]   where he works at, he works as a full stack engineer,
[00:33:07.600 --> 00:33:11.160]   primarily not on machine learning and deep learning,
[00:33:11.160 --> 00:33:14.280]   but he has built a really cool tool
[00:33:14.280 --> 00:33:17.920]   as part of the CodeSearchNet challenge
[00:33:17.920 --> 00:33:20.480]   that Weights & Biases did with GitHub.
[00:33:20.480 --> 00:33:23.600]   And it's a really impressive project.
[00:33:23.600 --> 00:33:25.200]   It's got some really clear uses,
[00:33:25.200 --> 00:33:30.200]   and I'm really excited to hear from him about that.
[00:33:30.200 --> 00:33:32.400]   So Rok, go ahead and take it away.
[00:33:32.400 --> 00:33:36.720]   - Okay, thank you for the introduction.
[00:33:36.720 --> 00:33:38.040]   Hi, my name is Rok,
[00:33:38.040 --> 00:33:41.080]   and I would like to present CodeSnippet Search.
[00:33:41.080 --> 00:33:44.800]   It is a web application and a web extension
[00:33:44.800 --> 00:33:47.240]   that allows you to search GitHub repositories
[00:33:47.240 --> 00:33:51.520]   using natural language queries and code itself, actually.
[00:33:51.520 --> 00:33:54.720]   It uses PyTorch to power deep neural networks,
[00:33:54.720 --> 00:33:57.400]   which embed natural language queries
[00:33:57.400 --> 00:33:59.880]   and code snippets into vectors.
[00:33:59.880 --> 00:34:01.880]   So using nearest neighbor search,
[00:34:01.880 --> 00:34:03.680]   we can determine which code snippets
[00:34:03.680 --> 00:34:07.520]   are most similar to a given query or another code snippet.
[00:34:08.520 --> 00:34:11.400]   So why neural code search?
[00:34:11.400 --> 00:34:13.240]   Code search, for me at least,
[00:34:13.240 --> 00:34:16.520]   is one of the most important tools during coding.
[00:34:16.520 --> 00:34:19.440]   I noticed that searching and navigating through code
[00:34:19.440 --> 00:34:22.480]   heavily outweighs the actual coding.
[00:34:22.480 --> 00:34:25.440]   So when I'm already familiar with the code base,
[00:34:25.440 --> 00:34:28.440]   I find that exact search is what I need.
[00:34:28.440 --> 00:34:30.240]   For example, regular expressions
[00:34:30.240 --> 00:34:32.240]   and case-insensitive search.
[00:34:32.240 --> 00:34:35.320]   That is because I'm already familiar with the naming scheme,
[00:34:35.320 --> 00:34:36.840]   and I can reasonably predict
[00:34:36.840 --> 00:34:39.360]   how to formulate a search query.
[00:34:39.360 --> 00:34:41.960]   And basically every IDE and code editor
[00:34:41.960 --> 00:34:44.240]   is already equipped with it.
[00:34:44.240 --> 00:34:46.720]   But when I'm not familiar with the code base,
[00:34:46.720 --> 00:34:50.080]   or even a new folder within an existing large project,
[00:34:50.080 --> 00:34:53.520]   for example, it makes searching more difficult
[00:34:53.520 --> 00:34:55.480]   because it's more trial and error.
[00:34:55.480 --> 00:34:57.480]   A tool like CodeSnippet Search
[00:34:57.480 --> 00:35:01.080]   would allow me to easily explore unfamiliar code,
[00:35:01.080 --> 00:35:03.040]   focusing on the semantics
[00:35:03.040 --> 00:35:06.080]   without getting bogged down in the syntax.
[00:35:06.080 --> 00:35:07.880]   For example, this is especially useful
[00:35:07.880 --> 00:35:11.080]   when onboarding a new developer onto a project,
[00:35:11.080 --> 00:35:14.840]   because it can be a significant boost to their productivity.
[00:35:14.840 --> 00:35:18.160]   And here I listed some example queries
[00:35:18.160 --> 00:35:20.520]   a new developer could potentially enter.
[00:35:20.520 --> 00:35:22.760]   So if you're a new developer
[00:35:22.760 --> 00:35:25.760]   and you're working on a product
[00:35:25.760 --> 00:35:27.960]   that's using shipping or products
[00:35:27.960 --> 00:35:31.440]   or authenticating the user or generating QR code,
[00:35:31.440 --> 00:35:33.840]   so you don't exactly know
[00:35:33.840 --> 00:35:36.920]   how the authentication is implemented,
[00:35:36.920 --> 00:35:39.280]   is it a single method or is it in a class?
[00:35:39.280 --> 00:35:41.920]   So basically instead of trial and error,
[00:35:41.920 --> 00:35:44.640]   you just type in authenticate user
[00:35:44.640 --> 00:35:47.680]   and it would return either the function
[00:35:47.680 --> 00:35:51.720]   that implements it or the class or some kind of service.
[00:35:51.720 --> 00:35:56.600]   So that's where I see the potential behind these methods
[00:35:56.600 --> 00:36:00.040]   or behind the CodeSnippet Search.
[00:36:00.040 --> 00:36:02.800]   So outside of a work environment,
[00:36:02.800 --> 00:36:04.880]   we encounter unfamiliar code
[00:36:04.880 --> 00:36:08.680]   in the form of GitHub repositories basically every day.
[00:36:08.680 --> 00:36:11.760]   So semantic search tools would provide the faster way
[00:36:11.760 --> 00:36:15.000]   for users to find answers to their issues
[00:36:15.000 --> 00:36:16.440]   directly in the code.
[00:36:16.440 --> 00:36:19.360]   Consequently, it could lessen the burden
[00:36:19.360 --> 00:36:21.960]   on maintainers to provide these answers.
[00:36:21.960 --> 00:36:24.960]   That's why I think it's important for GitHub
[00:36:24.960 --> 00:36:26.760]   to explore this option
[00:36:26.760 --> 00:36:30.520]   and potentially maybe one day even implement it
[00:36:30.520 --> 00:36:33.160]   in their product.
[00:36:33.160 --> 00:36:36.760]   So as mentioned, the main data source
[00:36:36.760 --> 00:36:40.440]   and inspiration for CodeSnippet Search
[00:36:40.440 --> 00:36:42.800]   is GitHub's CodeSearchNet project.
[00:36:42.800 --> 00:36:46.200]   The CodeSearchNet corpus contains approximately
[00:36:46.200 --> 00:36:49.520]   6 million functions from six programming languages.
[00:36:49.520 --> 00:36:54.040]   These are Go, Python, PHP, Java, Ruby, and JavaScript,
[00:36:54.040 --> 00:36:56.560]   where 2 million functions are annotated
[00:36:56.560 --> 00:36:58.800]   with a doc string or a com.
[00:36:58.800 --> 00:37:01.520]   So CodeSearchNet, along with Weights and Biases,
[00:37:01.520 --> 00:37:03.720]   also hosts the challenge where the goal
[00:37:03.720 --> 00:37:06.560]   is to answer 99 queries as best as possible
[00:37:06.560 --> 00:37:08.720]   with functions from the corpus.
[00:37:08.720 --> 00:37:11.240]   So this challenge is what got me
[00:37:11.240 --> 00:37:13.520]   into neural code search in the first place
[00:37:13.520 --> 00:37:15.720]   and also deep learning in general.
[00:37:15.720 --> 00:37:20.920]   CodeSearchNet also provides various baseline implementations
[00:37:20.920 --> 00:37:23.600]   of neural code search and TensorFlow.
[00:37:23.600 --> 00:37:26.840]   And my implementation for CodeSnippet Search
[00:37:26.840 --> 00:37:30.760]   was inspired by their neural bag of words implementation
[00:37:30.760 --> 00:37:33.120]   because it was performing the best overall
[00:37:33.120 --> 00:37:35.280]   in the published results.
[00:37:35.280 --> 00:37:39.200]   So initially I had written CodeSnippet Search in Keras
[00:37:39.200 --> 00:37:42.720]   and it was able to search through the CodeSearchNet corpus,
[00:37:42.720 --> 00:37:45.080]   but due to difficulties when developing
[00:37:45.080 --> 00:37:46.520]   and deploying the models,
[00:37:46.520 --> 00:37:48.920]   I decided to switch to PyTorch
[00:37:48.920 --> 00:37:50.720]   when I wanted to add support
[00:37:50.720 --> 00:37:53.640]   for searching GitHub repositories.
[00:37:55.920 --> 00:38:00.920]   Okay, now since I think it's a good time
[00:38:00.920 --> 00:38:02.960]   to kind of show you the demo.
[00:38:02.960 --> 00:38:07.960]   So this is the demo of totally not cherry-picked examples
[00:38:07.960 --> 00:38:13.360]   I wanted to show you.
[00:38:13.360 --> 00:38:16.120]   So this is the CodeSnippet Search application,
[00:38:16.120 --> 00:38:18.680]   which is available at CodeSnippetSearch.net.
[00:38:18.680 --> 00:38:22.440]   Currently I support 31 code repositories
[00:38:22.440 --> 00:38:26.800]   because the instance I'm hosting on it is not that large.
[00:38:26.800 --> 00:38:29.880]   So that's kind of the limit of what I can get it on.
[00:38:29.880 --> 00:38:32.680]   So on the first page,
[00:38:32.680 --> 00:38:35.560]   we see the list of supported repositories,
[00:38:35.560 --> 00:38:38.680]   their supported languages and their descriptions.
[00:38:38.680 --> 00:38:41.800]   So for the example, I decided to use Django
[00:38:41.800 --> 00:38:46.800]   because Django powers the backend of this application.
[00:38:46.800 --> 00:38:49.840]   So for the first example,
[00:38:49.840 --> 00:38:54.640]   let's try something simple, for example, sendMail,
[00:38:54.640 --> 00:38:59.160]   which of course returns back a couple of functions
[00:38:59.160 --> 00:39:01.760]   that implement sendMail.
[00:39:01.760 --> 00:39:06.760]   I don't know, email user, mail admins, sendEmail message.
[00:39:06.760 --> 00:39:11.000]   And as you can see, it returns the link to the GitHub file.
[00:39:11.000 --> 00:39:16.000]   So you can click and it returns the highlighted function.
[00:39:16.000 --> 00:39:18.880]   Then you also get the match rating,
[00:39:18.880 --> 00:39:23.880]   which is kind of a pseudo match rating,
[00:39:23.880 --> 00:39:26.360]   which is calculated from the distance
[00:39:26.360 --> 00:39:29.440]   between the query and the code itself.
[00:39:29.440 --> 00:39:31.360]   And then we get the similar code snippets,
[00:39:31.360 --> 00:39:33.440]   which I'll show in a minute.
[00:39:33.440 --> 00:39:39.080]   So we can try a slightly more complex query,
[00:39:39.080 --> 00:39:44.080]   for example, how to get database table name in Django.
[00:39:44.080 --> 00:39:48.560]   So here we get getTableList,
[00:39:48.560 --> 00:39:51.320]   which is kind of what we're looking for.
[00:39:51.320 --> 00:39:53.080]   So not the exact answers,
[00:39:53.080 --> 00:39:57.960]   but I guess any skilled developer could take this function
[00:39:57.960 --> 00:40:00.400]   and extract what it needs from it.
[00:40:00.400 --> 00:40:03.080]   So to get just the one table name
[00:40:03.080 --> 00:40:05.600]   or the table name he's looking for.
[00:40:05.600 --> 00:40:07.920]   And to showcase the previously mentioned
[00:40:07.920 --> 00:40:10.040]   similar code snippets option,
[00:40:10.040 --> 00:40:15.320]   let's say we wanted to look for how to convert
[00:40:16.240 --> 00:40:20.200]   time zone to local date time.
[00:40:20.200 --> 00:40:25.680]   Okay, and we basically get functions
[00:40:25.680 --> 00:40:30.160]   that are able to do the conversion from time zone,
[00:40:30.160 --> 00:40:33.040]   from a specific time zone to a local date time.
[00:40:33.040 --> 00:40:36.320]   And if we click on the similar code snippets,
[00:40:36.320 --> 00:40:40.000]   so this basically takes this code snippet
[00:40:40.000 --> 00:40:45.000]   and basically searches for similar implementations,
[00:40:45.040 --> 00:40:50.040]   similar functions that implement the same functionality.
[00:40:50.040 --> 00:40:53.000]   And here we get, as expected,
[00:40:53.000 --> 00:40:58.000]   a bunch of time zone converting functions as well.
[00:40:58.000 --> 00:41:01.720]   So I mentioned that code snippet search
[00:41:01.720 --> 00:41:04.560]   also comes in the web extension variety.
[00:41:04.560 --> 00:41:08.040]   So if we switch to a GitHub repository,
[00:41:08.040 --> 00:41:13.040]   we can open the sidebar by pressing Alt Shift and S.
[00:41:13.160 --> 00:41:15.840]   And here we get the sidebar
[00:41:15.840 --> 00:41:19.720]   where you can enter the query the same as before.
[00:41:19.720 --> 00:41:23.480]   Let's say, since we're on the PyTorch GitHub repository,
[00:41:23.480 --> 00:41:24.520]   let's say we wanted to look for
[00:41:24.520 --> 00:41:27.540]   one dimensional convolutions.
[00:41:27.540 --> 00:41:34.480]   And as expected, we get back the one dimensional convolution
[00:41:34.480 --> 00:41:36.880]   implemented in PyTorch.
[00:41:36.880 --> 00:41:38.240]   That's for fungis.
[00:41:42.400 --> 00:41:44.920]   Search this in the GitHub search
[00:41:44.920 --> 00:41:46.720]   and let's see what we get back.
[00:41:46.720 --> 00:41:53.160]   As we see, code snippet searches is better in every way.
[00:41:53.160 --> 00:41:58.520]   So don't use the GitHub search anymore.
[00:41:58.520 --> 00:42:03.280]   Of course not.
[00:42:03.280 --> 00:42:05.840]   And another feature I would like to show you
[00:42:05.840 --> 00:42:08.080]   is the search by code feature,
[00:42:08.080 --> 00:42:12.440]   which is basically you select any arbitrary code snippet,
[00:42:12.440 --> 00:42:17.200]   you press the right mouse click
[00:42:17.200 --> 00:42:19.800]   and you can select search by code.
[00:42:19.800 --> 00:42:24.800]   This basically searches for similar code snippets.
[00:42:24.800 --> 00:42:28.400]   And here we get various implementations
[00:42:28.400 --> 00:42:32.640]   of the ReLU function from various modules.
[00:42:32.640 --> 00:42:36.080]   And here, luckily we also get,
[00:42:36.080 --> 00:42:38.160]   let's say the leaky ReLU function,
[00:42:38.160 --> 00:42:39.680]   which is also implemented.
[00:42:39.680 --> 00:42:43.520]   And it's kind of, let's say a kind of expected result.
[00:42:43.520 --> 00:42:45.420]   If you're looking for the ReLU function,
[00:42:45.420 --> 00:42:50.000]   you might expect to also see the leaky ReLU function.
[00:42:50.000 --> 00:42:54.640]   So this was kind of a quick demo of the function
[00:42:54.640 --> 00:42:58.800]   of the web application and the web extension.
[00:42:58.800 --> 00:43:01.080]   So,
[00:43:04.640 --> 00:43:08.720]   let's restart the presentation.
[00:43:08.720 --> 00:43:12.880]   Okay, so now we can go take a closer look behind the scene.
[00:43:12.880 --> 00:43:14.680]   As I've mentioned before,
[00:43:14.680 --> 00:43:18.080]   code snippet search works by using joint embeddings
[00:43:18.080 --> 00:43:22.640]   of code and queries to implement the neural search system.
[00:43:22.640 --> 00:43:25.440]   The training objective is to map code
[00:43:25.440 --> 00:43:28.400]   and corresponding queries onto vectors
[00:43:28.400 --> 00:43:29.880]   that are close to each other.
[00:43:29.880 --> 00:43:32.960]   With this, I can embed the natural language query
[00:43:32.960 --> 00:43:35.320]   and then use nearest neighbor search
[00:43:35.320 --> 00:43:39.880]   to return a set of closest code snippets.
[00:43:39.880 --> 00:43:42.560]   During training, I use function docstrings
[00:43:42.560 --> 00:43:46.920]   or comments as substitutes for natural language queries.
[00:43:46.920 --> 00:43:50.880]   So in this image, I have the entire model
[00:43:50.880 --> 00:43:52.080]   split up into layers.
[00:43:52.080 --> 00:43:55.020]   So first is of course the input layer.
[00:43:55.020 --> 00:43:58.960]   One input we have for the queries or docstrings
[00:43:58.960 --> 00:44:01.680]   and one input each for languages.
[00:44:02.640 --> 00:44:06.800]   So inputs are then forwarded into embedding layers
[00:44:06.800 --> 00:44:09.840]   with a job of the type afterward.
[00:44:09.840 --> 00:44:11.880]   And basically the magic happens
[00:44:11.880 --> 00:44:14.680]   in the encoding and coding layer.
[00:44:14.680 --> 00:44:16.080]   Here I take the samples,
[00:44:16.080 --> 00:44:18.240]   but in this case, the most effective way
[00:44:18.240 --> 00:44:22.360]   of encoding the tokens, which is using a weighted mean.
[00:44:22.360 --> 00:44:24.960]   The weights we use for weighting are learned
[00:44:24.960 --> 00:44:28.360]   in a separate layer, in a separate trainable layer.
[00:44:28.360 --> 00:44:32.320]   Since we split up the languages at the input,
[00:44:32.320 --> 00:44:34.760]   we have to concatenate them back
[00:44:34.760 --> 00:44:37.760]   in the same order as they appeared in the queries.
[00:44:37.760 --> 00:44:40.320]   And finally, I normalize the rows
[00:44:40.320 --> 00:44:42.440]   in the concatenated matrices
[00:44:42.440 --> 00:44:46.720]   and multiply them to get cosine similarities.
[00:44:46.720 --> 00:44:49.000]   So in the last functions,
[00:44:49.000 --> 00:44:50.600]   the code search net implemented
[00:44:50.600 --> 00:44:52.720]   a couple of different loss functions,
[00:44:52.720 --> 00:44:56.200]   but in general, the last function
[00:44:56.200 --> 00:44:58.600]   can be intuitively explained
[00:44:58.600 --> 00:45:00.680]   as maximizing the similarities
[00:45:00.680 --> 00:45:03.800]   between the corresponding code and query pairs
[00:45:03.800 --> 00:45:06.240]   while minimizing the similarities
[00:45:06.240 --> 00:45:08.240]   between non-corresponding pairs.
[00:45:08.240 --> 00:45:13.240]   So in this case, the similarity is the cosine similarity.
[00:45:13.240 --> 00:45:18.360]   So, and how is the actual searching performed?
[00:45:18.360 --> 00:45:20.320]   I take all of the existing code snippets,
[00:45:20.320 --> 00:45:22.200]   I encode them and store them
[00:45:22.200 --> 00:45:25.120]   in a nearest neighbors search index.
[00:45:25.120 --> 00:45:28.680]   So when I want to look up nearest code snippets
[00:45:28.680 --> 00:45:31.520]   using a query, I encode the query,
[00:45:31.520 --> 00:45:32.880]   look up the nearest neighbors
[00:45:32.880 --> 00:45:35.440]   and return the most similar ones.
[00:45:35.440 --> 00:45:38.600]   I'm using the Annoy Index nearest neighbors library
[00:45:38.600 --> 00:45:42.120]   because of the lookup speed and it's very fast.
[00:45:42.120 --> 00:45:45.680]   And since the data set was really large,
[00:45:45.680 --> 00:45:48.720]   building the index was time consuming.
[00:45:48.720 --> 00:45:51.360]   That forced me to do some open source work
[00:45:51.360 --> 00:45:53.600]   and contribute to the Annoy Index project
[00:45:53.600 --> 00:45:56.680]   to implement a multi-threaded index build.
[00:45:56.680 --> 00:45:58.400]   So if you're using Annoy Index,
[00:45:58.400 --> 00:46:00.360]   it should be available.
[00:46:00.360 --> 00:46:02.200]   And the Annoy Index was responsible
[00:46:02.200 --> 00:46:04.480]   for another little anecdote that happened
[00:46:04.480 --> 00:46:07.720]   while working on the code search challenge.
[00:46:07.720 --> 00:46:11.560]   I've mentioned that I've re-implemented their baseline,
[00:46:11.560 --> 00:46:13.840]   including the searching part.
[00:46:13.840 --> 00:46:15.600]   In my re-implementation,
[00:46:15.600 --> 00:46:19.080]   I used the Scikit nearest neighbors search class
[00:46:19.080 --> 00:46:22.080]   instead of Annoy Index because then I wasn't familiar
[00:46:22.080 --> 00:46:23.640]   with it.
[00:46:23.640 --> 00:46:26.480]   I wasn't expecting a good result on the challenge
[00:46:26.480 --> 00:46:28.520]   since I wasn't doing anything special.
[00:46:28.520 --> 00:46:33.200]   But it turned out I was at the top of the leaderboard,
[00:46:33.200 --> 00:46:36.400]   almost doubling the previous high score.
[00:46:36.400 --> 00:46:40.480]   So this kind of puzzled me and the organizers as well.
[00:46:40.480 --> 00:46:42.040]   So I did some digging.
[00:46:42.040 --> 00:46:44.760]   I swapped out the Scikit versions of my version
[00:46:44.760 --> 00:46:47.960]   with Annoy Index and my score instantly dropped back.
[00:46:47.960 --> 00:46:51.920]   It turned out the provided code search evaluation code
[00:46:51.920 --> 00:46:54.760]   didn't update the size of the Annoy Index.
[00:46:54.760 --> 00:46:57.320]   And so it returned terrible results.
[00:46:57.320 --> 00:47:01.600]   So the lesson is always benchmark the index size
[00:47:01.600 --> 00:47:03.440]   if you're using Annoy Index.
[00:47:03.440 --> 00:47:07.760]   Okay, back to code snippet search.
[00:47:07.760 --> 00:47:10.480]   The training procedure is basically split
[00:47:10.480 --> 00:47:12.080]   into two major parts,
[00:47:12.080 --> 00:47:14.120]   training the base language models
[00:47:14.120 --> 00:47:17.400]   and then training the repository language models
[00:47:17.400 --> 00:47:20.400]   with the help of the base language models.
[00:47:20.400 --> 00:47:23.200]   First, I trained the base language models
[00:47:23.200 --> 00:47:25.560]   using the code search net corpus.
[00:47:25.560 --> 00:47:28.960]   And the main goal of the base language model
[00:47:28.960 --> 00:47:32.560]   is to train the word and code token embeddings
[00:47:32.560 --> 00:47:37.120]   that can then be transferred to repository models.
[00:47:37.120 --> 00:47:38.600]   Basically with this, I'm hoping
[00:47:38.600 --> 00:47:40.920]   that the base language models learn
[00:47:40.920 --> 00:47:43.520]   some general programming language features
[00:47:43.520 --> 00:47:46.640]   that can then be fine tuned by the repository data.
[00:47:46.640 --> 00:47:50.640]   With repositories, I was kind of on my own
[00:47:50.640 --> 00:47:52.240]   to extract the corpus.
[00:47:52.240 --> 00:47:57.160]   So I'm using another GitHub project called TreeSitter
[00:47:57.160 --> 00:48:01.680]   to find functions and then tokenize them.
[00:48:01.680 --> 00:48:05.720]   So repository models have the exact same structure.
[00:48:05.720 --> 00:48:07.920]   I just initialize all embedding layers
[00:48:07.920 --> 00:48:10.560]   with base language embeddings.
[00:48:10.560 --> 00:48:13.520]   So I just use that as an initial value
[00:48:13.520 --> 00:48:15.800]   and the embeddings are then fine tuned
[00:48:15.800 --> 00:48:19.440]   by training the model with repository code snippets.
[00:48:20.280 --> 00:48:23.760]   Okay, and there are of course,
[00:48:23.760 --> 00:48:27.760]   plenty of warnings attached to code snippet search.
[00:48:27.760 --> 00:48:29.680]   You need a reasonably large,
[00:48:29.680 --> 00:48:32.720]   well-documented repository to train the model.
[00:48:32.720 --> 00:48:35.280]   And even then it only works on functions.
[00:48:35.280 --> 00:48:38.920]   So the first logical step would be to figure out
[00:48:38.920 --> 00:48:41.120]   how to work on smaller chunks of code.
[00:48:41.120 --> 00:48:44.480]   So a couple of lines of code that I comment about.
[00:48:44.480 --> 00:48:46.400]   And from a model perspective,
[00:48:46.400 --> 00:48:49.080]   I would like to experiment with tree-based models
[00:48:49.080 --> 00:48:52.320]   that could potentially capture the information
[00:48:52.320 --> 00:48:55.400]   hidden in the abstract syntax tree.
[00:48:55.400 --> 00:48:57.320]   As you've seen in the model image,
[00:48:57.320 --> 00:48:59.920]   we're now kind of just ignoring,
[00:48:59.920 --> 00:49:01.440]   even ignoring the order
[00:49:01.440 --> 00:49:03.800]   because that seems to work the best
[00:49:03.800 --> 00:49:08.800]   when encoding the code and queries.
[00:49:08.800 --> 00:49:11.800]   So, but potentially for that,
[00:49:11.800 --> 00:49:13.800]   we would need to gather a lot more samples
[00:49:13.800 --> 00:49:18.800]   because even using RNNs or Python,
[00:49:18.800 --> 00:49:23.320]   or 1D convolutions on these sequences of tokens,
[00:49:23.320 --> 00:49:28.320]   they don't work as good as a simple Daggerboard model.
[00:49:28.320 --> 00:49:32.720]   So potentially we would have to gather a lot more samples.
[00:49:32.720 --> 00:49:37.080]   So that would be all from me.
[00:49:37.080 --> 00:49:38.360]   I hope you like the project.
[00:49:38.360 --> 00:49:41.440]   I hope it's potentially useful
[00:49:41.440 --> 00:49:43.400]   and thank you for listening.
[00:49:48.360 --> 00:49:49.240]   - Great.
[00:49:49.240 --> 00:49:51.600]   Yeah, thanks for sharing, Rock.
[00:49:51.600 --> 00:49:52.920]   I've got a lot of questions
[00:49:52.920 --> 00:49:56.600]   and I hope, I encourage folks in the audience
[00:49:56.600 --> 00:49:58.320]   to submit their own questions as well,
[00:49:58.320 --> 00:49:59.640]   but I'll kick us off here.
[00:49:59.640 --> 00:50:05.480]   One is, so based off of your experience with the tool
[00:50:05.480 --> 00:50:07.200]   or maybe the way that you tokenize things,
[00:50:07.200 --> 00:50:09.600]   how robust is this to typos?
[00:50:09.600 --> 00:50:12.240]   You know, like, so if I typed, you know, convolutions
[00:50:12.240 --> 00:50:14.760]   as you did a couple of times instead of convolutions,
[00:50:14.760 --> 00:50:16.520]   is that going to throw it off
[00:50:16.520 --> 00:50:18.720]   or is it robust to that?
[00:50:18.720 --> 00:50:22.760]   - I guess the answer is kind of,
[00:50:22.760 --> 00:50:26.280]   because we're not, we're kind of using,
[00:50:26.280 --> 00:50:27.600]   for tokenizing, for tokens,
[00:50:27.600 --> 00:50:31.760]   we're using half of it as just the regular vocabulary
[00:50:31.760 --> 00:50:35.400]   and half of it with the byte pair encoding.
[00:50:35.400 --> 00:50:40.400]   So we're using kind of the subset of the token.
[00:50:40.400 --> 00:50:45.680]   So I guess if you misspell the last part of the word,
[00:50:45.840 --> 00:50:50.720]   so the CO and V, I guess it could capture that
[00:50:50.720 --> 00:50:53.680]   and still find the appropriate function.
[00:50:53.680 --> 00:50:55.360]   So it's kind of robust,
[00:50:55.360 --> 00:51:00.360]   but I guess the answer is kind of,
[00:51:00.360 --> 00:51:06.200]   as I've said, these were kind of cherry-picked examples.
[00:51:06.200 --> 00:51:10.120]   So if you provide a weird input,
[00:51:10.120 --> 00:51:12.720]   it's going to provide a weird output for now,
[00:51:13.760 --> 00:51:16.480]   but I'm trying to work on the,
[00:51:16.480 --> 00:51:20.880]   minimizing the amount of times that happens.
[00:51:20.880 --> 00:51:24.200]   - I mean, yeah, I mean,
[00:51:24.200 --> 00:51:26.440]   I would imagine a byte pair encoding.
[00:51:26.440 --> 00:51:28.440]   So this is for folks who aren't as familiar
[00:51:28.440 --> 00:51:29.880]   with natural language byte pair encoding,
[00:51:29.880 --> 00:51:33.480]   tries to find good ways to break words down into sub words
[00:51:33.480 --> 00:51:36.000]   and use those and embed those.
[00:51:36.000 --> 00:51:38.840]   So it's what's used in the general purpose transform
[00:51:38.840 --> 00:51:41.640]   or not the generative pre-trained transformer models,
[00:51:41.640 --> 00:51:44.000]   the GPT models at OpenAI.
[00:51:44.000 --> 00:51:44.840]   That's what they use
[00:51:44.840 --> 00:51:46.400]   for their natural language processing stuff.
[00:51:46.400 --> 00:51:49.320]   I'm actually, so you trained this byte pair encoding,
[00:51:49.320 --> 00:51:51.480]   it's specialized to code, right?
[00:51:51.480 --> 00:51:54.760]   It's not like a pre-trained thing from, yeah.
[00:51:54.760 --> 00:51:57.240]   So it's actually, it would be cool actually
[00:51:57.240 --> 00:52:00.840]   to see what are the byte pairs that show up in code.
[00:52:00.840 --> 00:52:02.960]   Are they different from the ones that show up
[00:52:02.960 --> 00:52:04.160]   in natural language?
[00:52:04.160 --> 00:52:07.440]   Like maybe underscores become really important
[00:52:07.440 --> 00:52:09.040]   or something like that.
[00:52:09.040 --> 00:52:12.000]   - Yeah, the, basically all of the punctuation
[00:52:12.000 --> 00:52:15.080]   in the code becomes really important.
[00:52:15.080 --> 00:52:21.200]   Yeah, I think basically I looked inside the vocabularies.
[00:52:21.200 --> 00:52:24.720]   I think the first 10, 15 are kind of the combinations
[00:52:24.720 --> 00:52:27.400]   of basically of the punctuation.
[00:52:27.400 --> 00:52:31.040]   So semicolons, colons, stuff like that.
[00:52:31.040 --> 00:52:33.040]   - Nice, and yeah, and actually that's an important detail
[00:52:33.040 --> 00:52:34.960]   of the byte pair encoding algorithm, right?
[00:52:34.960 --> 00:52:38.000]   It starts with the most important chunks.
[00:52:38.000 --> 00:52:40.720]   So you actually do get kind of an ordering of importance
[00:52:40.720 --> 00:52:42.840]   of how like, how soon were these learned
[00:52:42.840 --> 00:52:45.680]   as to be a useful thing to encode.
[00:52:45.680 --> 00:52:47.520]   So it's cool that it's actually all punctuation.
[00:52:47.520 --> 00:52:49.520]   That's a neat, that's a neat result.
[00:52:49.520 --> 00:52:51.280]   I don't know if anybody's ever published
[00:52:51.280 --> 00:52:53.640]   on what the encodings look like for.
[00:52:53.640 --> 00:53:00.920]   You mentioned that you switched over
[00:53:00.920 --> 00:53:05.400]   from TensorFlow Keras to PyTorch.
[00:53:05.400 --> 00:53:07.520]   So I was wondering like a lot of our audience actually,
[00:53:07.520 --> 00:53:10.160]   you know, works in both frameworks sometimes.
[00:53:10.160 --> 00:53:11.560]   It's sort of like jumps between them.
[00:53:11.560 --> 00:53:14.200]   I often get questions from folks in our community,
[00:53:14.200 --> 00:53:16.240]   you know, should I do this project in Keras?
[00:53:16.240 --> 00:53:17.120]   Should I do it in PyTorch?
[00:53:17.120 --> 00:53:18.640]   What's your recommendation?
[00:53:18.640 --> 00:53:20.640]   What do you think are the important things
[00:53:20.640 --> 00:53:22.280]   to help make that decision?
[00:53:22.280 --> 00:53:28.520]   - Just looking at the image of my model, right?
[00:53:28.520 --> 00:53:30.800]   I was having a lot of trouble fitting this kind
[00:53:30.800 --> 00:53:34.440]   of structure into, basically into Keras.
[00:53:34.440 --> 00:53:38.360]   Basically I had to, there is no way in,
[00:53:38.360 --> 00:53:42.280]   at least I know in Keras to encode,
[00:53:42.280 --> 00:53:44.880]   or basically to have inputs of different sizes, right?
[00:53:44.880 --> 00:53:49.880]   So potentially you would have three Python samples,
[00:53:49.880 --> 00:53:53.720]   10 Go samples, 15 Ruby samples.
[00:53:53.720 --> 00:53:58.080]   And as far as I could experiment,
[00:53:58.080 --> 00:54:00.840]   I couldn't come up with a way to make,
[00:54:00.840 --> 00:54:04.000]   for Keras to make sense of that.
[00:54:04.000 --> 00:54:07.880]   So basically with Keras, I had to split up the models.
[00:54:07.880 --> 00:54:12.880]   Basically there were six models, one for each language,
[00:54:12.880 --> 00:54:17.320]   basically one for each query and programming language pair,
[00:54:17.320 --> 00:54:22.320]   because that was the only way to have multiple languages.
[00:54:22.320 --> 00:54:26.240]   But that kind of loses the,
[00:54:26.240 --> 00:54:31.520]   that kind of loses the point where you have
[00:54:31.520 --> 00:54:34.840]   the same queries for all of the languages.
[00:54:34.840 --> 00:54:39.840]   And so you kind of lose on the, when training.
[00:54:39.840 --> 00:54:43.480]   And that was kind of the first problem.
[00:54:43.480 --> 00:54:45.480]   The Keras model looked really weird
[00:54:45.480 --> 00:54:49.840]   and it was kind of hard to implement it, at least for me.
[00:54:49.840 --> 00:54:54.640]   So I, and then the second problem was basically
[00:54:54.640 --> 00:54:57.800]   just the development problem with the Django.
[00:54:57.800 --> 00:55:01.200]   When locally developing Keras and Django,
[00:55:01.200 --> 00:55:04.720]   there were some weird threading issues
[00:55:04.720 --> 00:55:07.640]   I couldn't figure out because
[00:55:07.640 --> 00:55:13.400]   I don't exactly remember what it was,
[00:55:13.400 --> 00:55:16.760]   but basically when I ran a local Django server
[00:55:16.760 --> 00:55:19.320]   and it called the model, the TensorFlow model
[00:55:19.320 --> 00:55:23.320]   or the Keras model, it just segfaulted
[00:55:23.320 --> 00:55:25.680]   or something like that, something really weird,
[00:55:25.680 --> 00:55:29.120]   which basically there was some threading issue
[00:55:29.120 --> 00:55:33.680]   that required me to run Djangos
[00:55:33.680 --> 00:55:36.480]   in a single thread somewhere else or something like that.
[00:55:36.480 --> 00:55:41.480]   So that kind of culminated in me just looking into PyTorch
[00:55:41.480 --> 00:55:45.800]   and seeing that basically it was really simple
[00:55:45.800 --> 00:55:48.520]   to transfer all of the codes,
[00:55:48.520 --> 00:55:53.560]   all of the codes into PyTorch.
[00:55:53.560 --> 00:55:57.080]   And the model itself is actually really simple
[00:55:57.080 --> 00:56:00.760]   to implement in PyTorch.
[00:56:00.760 --> 00:56:05.440]   In Keras, it was actually pretty long.
[00:56:05.440 --> 00:56:08.440]   This is the model.
[00:56:08.440 --> 00:56:10.980]   Basically, these are all helper functions.
[00:56:10.980 --> 00:56:17.040]   And this is the forward, basically the forward function
[00:56:17.040 --> 00:56:18.520]   for training the model, right?
[00:56:18.520 --> 00:56:19.600]   This is it.
[00:56:19.600 --> 00:56:22.000]   This is basically, this does everything
[00:56:22.000 --> 00:56:25.080]   and here are the encoding stuff.
[00:56:25.080 --> 00:56:28.280]   And I don't know, it just seems simple.
[00:56:28.280 --> 00:56:29.840]   It was easy to deploy.
[00:56:29.840 --> 00:56:35.240]   The training was marginally faster with PyTorch
[00:56:35.240 --> 00:56:41.320]   and because I didn't have to train six separate models
[00:56:41.320 --> 00:56:44.560]   that also helped a lot with the development speed
[00:56:44.560 --> 00:56:47.820]   and I was able to kind of iterate faster.
[00:56:47.820 --> 00:56:51.240]   And the train and the model, of course, got better
[00:56:51.240 --> 00:56:54.160]   because queries were able to be shared
[00:56:54.160 --> 00:56:56.080]   between separate languages.
[00:56:56.080 --> 00:56:58.840]   - That's interesting.
[00:56:58.840 --> 00:57:00.840]   Yeah, I definitely have heard from folks
[00:57:00.840 --> 00:57:05.140]   that when it comes time to build a complicated model
[00:57:05.140 --> 00:57:07.760]   in Keras, it's not as clear how to do that.
[00:57:07.760 --> 00:57:11.240]   I think there are folks in the TensorFlow community
[00:57:11.240 --> 00:57:12.480]   who do build these kinds of models
[00:57:12.480 --> 00:57:13.860]   and they know the right way to do it.
[00:57:13.860 --> 00:57:15.520]   But I think what people's experience has been
[00:57:15.520 --> 00:57:17.320]   is that it's just a little bit harder to figure out
[00:57:17.320 --> 00:57:19.400]   how to do that than in PyTorch.
[00:57:19.400 --> 00:57:21.840]   So hopefully, I think they're both great frameworks
[00:57:21.840 --> 00:57:23.760]   and I do work in both of them.
[00:57:23.760 --> 00:57:26.840]   So hopefully that gets surfaced better
[00:57:26.840 --> 00:57:28.960]   so that we have more flexibility
[00:57:28.960 --> 00:57:32.040]   in choosing which framework to use.
[00:57:32.040 --> 00:57:35.920]   Let's see.
[00:57:35.920 --> 00:57:39.920]   Oh, another question, a little bit broader.
[00:57:39.920 --> 00:57:43.120]   Why do you think the bag of words methods
[00:57:43.120 --> 00:57:44.720]   are so successful here?
[00:57:44.720 --> 00:57:46.080]   You mentioned, I think a couple of times
[00:57:46.080 --> 00:57:49.340]   that people have tried 1D convolutions
[00:57:49.340 --> 00:57:51.160]   and recurrent networks of different kinds
[00:57:51.160 --> 00:57:53.400]   and not found great performance.
[00:57:53.400 --> 00:57:55.960]   Do you have any intuition for why that should be the case
[00:57:55.960 --> 00:57:57.520]   that these would work so poorly?
[00:57:57.520 --> 00:58:03.120]   - My first guess would be that there just isn't a lot of,
[00:58:03.120 --> 00:58:04.920]   that there isn't enough samples,
[00:58:04.920 --> 00:58:08.280]   like probably 6 million functions are not enough to,
[00:58:08.280 --> 00:58:13.880]   are not enough to train basically a recurrent model
[00:58:13.880 --> 00:58:17.200]   or there just isn't a lot of information to get out.
[00:58:17.200 --> 00:58:22.200]   But basically, strictly from the challenge perspective,
[00:58:22.880 --> 00:58:27.880]   I think that a lot of the queries can be answered
[00:58:27.880 --> 00:58:32.280]   just by using a simple token search or something like that,
[00:58:32.280 --> 00:58:35.240]   which is basically what bag of words is.
[00:58:35.240 --> 00:58:38.600]   I think the queries itself,
[00:58:38.600 --> 00:58:42.040]   the challenge queries are not that complicated.
[00:58:42.040 --> 00:58:46.160]   So they probably don't need any deeper understanding
[00:58:46.160 --> 00:58:49.040]   than just basically looking at tokens from the query,
[00:58:49.040 --> 00:58:50.640]   tokens from the code,
[00:58:50.640 --> 00:58:52.760]   and just kind of matching,
[00:58:52.760 --> 00:58:55.040]   kind of fuzzy matching them together
[00:58:55.040 --> 00:58:57.480]   and get a pretty good result.
[00:58:57.480 --> 00:59:02.200]   I think the, now with the updated results,
[00:59:02.200 --> 00:59:05.240]   the self-attention model was really close
[00:59:05.240 --> 00:59:07.280]   to the neural bag of words model.
[00:59:07.280 --> 00:59:12.280]   So I think that with some tweaking
[00:59:12.280 --> 00:59:13.880]   the self-attention model,
[00:59:13.880 --> 00:59:16.000]   which I didn't look too much into,
[00:59:16.000 --> 00:59:18.520]   but I think it could be better
[00:59:18.520 --> 00:59:21.920]   than the neural bag of words model.
[00:59:21.920 --> 00:59:24.640]   I kind of had to tweak the original implementations
[00:59:24.640 --> 00:59:27.240]   so I could get it just right.
[00:59:27.240 --> 00:59:31.760]   But I think the main thing is just to get more samples.
[00:59:31.760 --> 00:59:35.480]   Basically only 2 million functions are trainable,
[00:59:35.480 --> 00:59:37.360]   which have a docstring.
[00:59:37.360 --> 00:59:42.360]   So probably just more data, I guess.
[00:59:42.360 --> 00:59:43.840]   - Gotcha.
[00:59:43.840 --> 00:59:46.880]   Yeah, I know that to get those big transformer
[00:59:46.880 --> 00:59:49.280]   self-attention models working on natural language,
[00:59:49.280 --> 00:59:50.440]   as opposed to code,
[00:59:50.440 --> 00:59:52.560]   people have found that it's pre-training, right?
[00:59:52.560 --> 00:59:54.320]   It's being able to get a dataset
[00:59:54.320 --> 00:59:57.240]   that's like orders of magnitude larger
[00:59:57.240 --> 00:59:58.840]   than your supervised dataset,
[00:59:58.840 --> 01:00:00.920]   because there's no human in the loop.
[01:00:00.920 --> 01:00:05.920]   So if you were able to code crawl all of GitHub,
[01:00:05.920 --> 01:00:07.960]   all of its public repositories,
[01:00:07.960 --> 01:00:10.440]   and just learn how to generate code,
[01:00:10.440 --> 01:00:12.840]   that might be helpful.
[01:00:12.840 --> 01:00:14.320]   They probably already did some pre-training
[01:00:14.320 --> 01:00:16.320]   in the self-attention one.
[01:00:17.000 --> 01:00:20.440]   - Probably, I guess, but yeah,
[01:00:20.440 --> 01:00:23.680]   my guess would be just more data.
[01:00:23.680 --> 01:00:27.240]   And with the neural bag of words model,
[01:00:27.240 --> 01:00:29.920]   there's a funny little issue where you don't,
[01:00:29.920 --> 01:00:34.480]   the model doesn't differentiate between,
[01:00:34.480 --> 01:00:37.720]   convert int to string or string to int.
[01:00:37.720 --> 01:00:41.240]   Basically that's the same thing to the model
[01:00:41.240 --> 01:00:43.240]   and it returns same results.
[01:00:46.240 --> 01:00:48.280]   And a lot of the challenge queries
[01:00:48.280 --> 01:00:50.440]   are convert something to that,
[01:00:50.440 --> 01:00:55.240]   JSON to XML and to get it working on that,
[01:00:55.240 --> 01:01:00.240]   or basically maybe just using a sequence model
[01:01:00.240 --> 01:01:03.120]   on the queries and then just the neural bag of words
[01:01:03.120 --> 01:01:08.120]   on the code might help to encode this kind of dependencies.
[01:01:08.120 --> 01:01:13.200]   - Gotcha, yeah, that makes sense.
[01:01:14.160 --> 01:01:18.880]   Great, well, I think that's about all the time that we have.
[01:01:18.880 --> 01:01:22.080]   So thank you so much for staying up late
[01:01:22.080 --> 01:01:25.360]   to be able to present your work to this community.
[01:01:25.360 --> 01:01:27.280]   And thanks for answering my questions.
[01:01:27.280 --> 01:01:29.160]   It's really cool work.
[01:01:29.160 --> 01:01:30.880]   Really, again, like very impressive.
[01:01:30.880 --> 01:01:32.880]   Oh, I did have one question that I wanted to make sure
[01:01:32.880 --> 01:01:35.040]   I didn't forget to ask you,
[01:01:35.040 --> 01:01:37.960]   which is, I think a lot of people in our community
[01:01:37.960 --> 01:01:41.440]   are on sort of like a similar track to you
[01:01:41.440 --> 01:01:43.000]   where you're a full stack developer,
[01:01:43.000 --> 01:01:46.320]   you do, your day job is not machine learning
[01:01:46.320 --> 01:01:47.160]   and deep learning.
[01:01:47.160 --> 01:01:48.120]   And this is something that you've,
[01:01:48.120 --> 01:01:50.400]   like a skill that you've added to your toolkit.
[01:01:50.400 --> 01:01:51.880]   So could you just comment on maybe
[01:01:51.880 --> 01:01:54.680]   the most important resources that you think are out there
[01:01:54.680 --> 01:01:57.080]   for following that journey?
[01:01:57.080 --> 01:01:59.520]   What was most important to you along the way?
[01:01:59.520 --> 01:02:01.520]   Like the kind of, the advice that you would have liked
[01:02:01.520 --> 01:02:04.320]   to have gotten maybe when you were just getting started.
[01:02:04.320 --> 01:02:11.000]   - I guess from an engineer's perspective, right?
[01:02:11.040 --> 01:02:16.040]   Just don't care about math too much at the start, right?
[01:02:16.040 --> 01:02:21.240]   Just kind of, I would think it would be better
[01:02:21.240 --> 01:02:25.040]   because I was worried that if I didn't understand
[01:02:25.040 --> 01:02:27.000]   everything behind that was happening,
[01:02:27.000 --> 01:02:30.360]   like the math, the back propagations, all of that stuff,
[01:02:30.360 --> 01:02:34.640]   that I wouldn't be able to come up with models on my own.
[01:02:34.640 --> 01:02:38.360]   So I think it would be great advice
[01:02:38.360 --> 01:02:42.480]   if someone could tell me back then
[01:02:42.480 --> 01:02:47.080]   that you can just start coding, start producing models,
[01:02:47.080 --> 01:02:50.040]   throw some data at it and see if it works.
[01:02:50.040 --> 01:02:53.520]   And if it doesn't, then you can go basically explore
[01:02:53.520 --> 01:02:54.360]   why it doesn't work.
[01:02:54.360 --> 01:02:58.200]   And then by basically using that, you can learn.
[01:02:58.200 --> 01:03:03.200]   But just kind of as a general resource,
[01:03:06.920 --> 01:03:11.920]   I think the PyTorch documentation is really good.
[01:03:11.920 --> 01:03:14.680]   The Keras documentation is really good.
[01:03:14.680 --> 01:03:18.560]   Basically for beginners, Keras is great.
[01:03:18.560 --> 01:03:20.360]   The tutorials are great.
[01:03:20.360 --> 01:03:26.440]   And also the Kaggle competitions or some of the,
[01:03:26.440 --> 01:03:32.200]   not the competitions, but the stuff that is just like there
[01:03:32.200 --> 01:03:33.480]   for beginners to learn.
[01:03:33.480 --> 01:03:35.160]   There are a lot of great notebooks
[01:03:35.160 --> 01:03:39.640]   that basically implement either a neural network
[01:03:39.640 --> 01:03:41.880]   from scratch or they're using Keras
[01:03:41.880 --> 01:03:46.760]   to basically train a simple model to predict something.
[01:03:46.760 --> 01:03:49.040]   I learned actually a lot from those notebooks
[01:03:49.040 --> 01:03:52.400]   because I could actually see the code
[01:03:52.400 --> 01:03:55.920]   and not just go through tutorials over and over again.
[01:03:55.920 --> 01:03:59.520]   I could just download the, I think the iPython notebook
[01:03:59.520 --> 01:04:02.200]   and kind of try on my own.
[01:04:03.200 --> 01:04:05.440]   Basically just trying stuff on your own
[01:04:05.440 --> 01:04:09.840]   is the most important thing to get started.
[01:04:09.840 --> 01:04:14.840]   And yeah, just don't get stuck in the math at first.
[01:04:14.840 --> 01:04:19.120]   You can get stuck later, but it's not at first.
[01:04:19.120 --> 01:04:23.000]   At least for non-machine learning practitioners,
[01:04:23.000 --> 01:04:27.000]   basically just people are coming in from, I don't know,
[01:04:27.000 --> 01:04:29.800]   front end, back end, full stack developers.
[01:04:29.800 --> 01:04:32.000]   - Yeah, I think that makes sense.
[01:04:32.000 --> 01:04:34.800]   It's motivating, it's energizing when you make something,
[01:04:34.800 --> 01:04:36.400]   even if it's not perfect,
[01:04:36.400 --> 01:04:39.320]   if you make a cool tool for yourself,
[01:04:39.320 --> 01:04:41.200]   that's a nice way to get started
[01:04:41.200 --> 01:04:43.360]   rather than just slogging through textbooks.
[01:04:43.360 --> 01:04:47.000]   - Exactly, that would be my advice.
[01:04:47.000 --> 01:04:48.040]   - Yeah, yeah.
[01:04:48.040 --> 01:04:50.800]   So it's, we sort of took, I think, two very different paths.
[01:04:50.800 --> 01:04:54.080]   I went in, did a PhD and had the chance
[01:04:54.080 --> 01:04:55.880]   to just get lost in math for a couple of years
[01:04:55.880 --> 01:04:58.760]   while learning my way forward.
[01:04:58.760 --> 01:05:00.880]   But it's not a path I would recommend necessarily
[01:05:00.880 --> 01:05:04.800]   to anybody or at least to everybody.
[01:05:04.800 --> 01:05:06.960]   So yeah, that's great advice, Ra.
[01:05:06.960 --> 01:05:09.640]   - Yeah, depending on the person, yeah.
[01:05:09.640 --> 01:05:11.800]   - Yeah, well, so thank you so much for joining us.
[01:05:11.800 --> 01:05:13.360]   I'll let you go now.
[01:05:13.360 --> 01:05:18.480]   And yeah, so thanks to everybody for tuning in.
[01:05:18.480 --> 01:05:23.480]   And I look forward to seeing you all again in two weeks
[01:05:23.480 --> 01:05:28.760]   for the next Salon, where we'll have Sarah Hooger
[01:05:29.040 --> 01:05:31.640]   from Google Brain presenting her research
[01:05:31.640 --> 01:05:35.960]   on variance and gradients, and Hannes Hapke from SAP,
[01:05:35.960 --> 01:05:38.880]   who will be talking about building ML pipelines
[01:05:38.880 --> 01:05:41.160]   with ML metadata and TensorFlow Extended.
[01:05:41.160 --> 01:05:45.160]   So until then, happy learning, catch you around.
[01:05:45.160 --> 01:05:47.160]   (Subs by Andrew J)

