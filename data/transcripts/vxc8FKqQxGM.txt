
[00:00:00.000 --> 00:00:05.660]   I don't know if you've seen that account, like Weird Dali.
[00:00:05.660 --> 00:00:12.780]   They have crazy images that they put, like the Demogorgon from Stranger Things holding
[00:00:12.780 --> 00:00:13.780]   a basketball.
[00:00:13.780 --> 00:00:14.780]   That is insane.
[00:00:14.780 --> 00:00:15.780]   That is so cool.
[00:00:15.780 --> 00:00:22.340]   And you go through it and there's amazing things that are so fun.
[00:00:22.340 --> 00:00:26.700]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:26.700 --> 00:00:28.740]   and I'm your host, Lukas Biewald.
[00:00:28.740 --> 00:00:34.220]   Boris Dehme is a machine learning consultant and a long time Weights & Biases user.
[00:00:34.220 --> 00:00:39.660]   In fact, he started using us in such early days when I knew all of the users.
[00:00:39.660 --> 00:00:46.900]   Boris has gone on to build a model called Dali Mini, which is a model inspired by OpenAI's
[00:00:46.900 --> 00:00:49.220]   famous Dali project.
[00:00:49.220 --> 00:00:53.180]   And somehow Dali Mini has just captured the imagination of the world to the point where
[00:00:53.180 --> 00:00:57.260]   I've seen it in New Yorker cartoons, I see it all over my Facebook feed.
[00:00:57.260 --> 00:01:02.740]   It's just an amazing piece of work and I'm really excited to talk to him about it today.
[00:01:02.740 --> 00:01:06.980]   So maybe taking a step back for people who maybe aren't familiar with Dali at all, could
[00:01:06.980 --> 00:01:12.020]   you talk about what Dali is and what Dali Mini is and Dali Mega and how you've been
[00:01:12.020 --> 00:01:13.020]   working on this?
[00:01:13.020 --> 00:01:14.020]   Yeah, yeah.
[00:01:14.020 --> 00:01:21.260]   So Dali, it came from that paper from OpenAI of beginning of last year.
[00:01:21.260 --> 00:01:22.260]   And it was really amazing.
[00:01:22.260 --> 00:01:26.980]   And actually, the first time I saw it was a tweet you posted about it.
[00:01:26.980 --> 00:01:30.580]   And I replied to that tweet and I was like, "I'm going to build that."
[00:01:30.580 --> 00:01:31.580]   And I was like, "This is so cool."
[00:01:31.580 --> 00:01:32.780]   And I was like, "I'm going to build that.
[00:01:32.780 --> 00:01:33.780]   I want to build that."
[00:01:33.780 --> 00:01:42.340]   And it was basically OpenAI had done the first impressive image generation model that you
[00:01:42.340 --> 00:01:45.940]   would type any prompt and you would do something that looked actually cool.
[00:01:45.940 --> 00:01:50.500]   Because before you had image GPT or whatever, which would do something very tiny.
[00:01:50.500 --> 00:01:54.820]   You have a bit the idea, but it would do something more complex like the avocado armchair, which
[00:01:54.820 --> 00:01:55.820]   was cool at the time.
[00:01:55.820 --> 00:01:59.340]   Nowadays, the avocado armchair is just something simple.
[00:01:59.340 --> 00:02:01.580]   It's nothing impressive anymore.
[00:02:01.580 --> 00:02:02.580]   It's crazy.
[00:02:02.580 --> 00:02:07.380]   A few months ago, I was still very happy when I had a good avocado armchair.
[00:02:07.380 --> 00:02:11.660]   Yeah, so that's where it came from.
[00:02:11.660 --> 00:02:16.100]   And basically, in July of last year, I wanted to build it.
[00:02:16.100 --> 00:02:19.820]   I didn't do anything for six months.
[00:02:19.820 --> 00:02:21.900]   I read the paper quite a few times.
[00:02:21.900 --> 00:02:23.420]   I don't know how many times I read it.
[00:02:23.420 --> 00:02:29.700]   I read it a ton of times and I didn't understand much to it.
[00:02:29.700 --> 00:02:35.380]   And at some point, Hugging Face and Google, they organized a hackathon where you had to
[00:02:35.380 --> 00:02:41.540]   build something cool in JAX, which is a programming framework from Google.
[00:02:41.540 --> 00:02:46.540]   And you would have those cool computers from Google, those TPU, VM.
[00:02:46.540 --> 00:02:49.060]   And I was like, "Okay, that's an opportunity to do something cool."
[00:02:49.060 --> 00:02:55.820]   So I was like, "I'm going to try to build the replication of DALI in terms of the results."
[00:02:55.820 --> 00:02:59.540]   And it turns out that it worked pretty well.
[00:02:59.540 --> 00:03:01.220]   I researched a lot of the paper.
[00:03:01.220 --> 00:03:03.460]   Some people joined in the team too.
[00:03:03.460 --> 00:03:10.380]   And somehow, the program had pretty cool results for such a short time frame.
[00:03:10.380 --> 00:03:11.380]   Then I continued.
[00:03:11.380 --> 00:03:12.660]   Can you describe how the program works?
[00:03:12.660 --> 00:03:15.460]   I mean, it really feels like magic.
[00:03:15.460 --> 00:03:16.460]   How is it actually set up?
[00:03:16.460 --> 00:03:17.460]   I know, I know.
[00:03:17.460 --> 00:03:20.220]   It felt like magic for me for so long.
[00:03:20.220 --> 00:03:26.180]   And I think even if I read the paper again now, each time I read it, I learn new things.
[00:03:26.180 --> 00:03:35.260]   So basically, the way it works is you have a good model right now for NLP, which transform
[00:03:35.260 --> 00:03:36.860]   text into text.
[00:03:36.860 --> 00:03:43.540]   So for example, let's say for summarization or let's say for translation, going from English
[00:03:43.540 --> 00:03:44.980]   to French.
[00:03:44.980 --> 00:03:49.460]   And you are trying to do kind of the same thing, except instead of going from English
[00:03:49.460 --> 00:03:52.900]   to French, we want to go from English to an image.
[00:03:52.900 --> 00:03:54.620]   But it's almost like the same.
[00:03:54.620 --> 00:03:57.780]   It's just a translation.
[00:03:57.780 --> 00:04:05.340]   So the way you do it, when you do text to text, you encode each text, become a token.
[00:04:05.340 --> 00:04:07.420]   It's just a number, a unique number.
[00:04:07.420 --> 00:04:09.380]   And you try to predict that sequence of number.
[00:04:09.380 --> 00:04:12.340]   And each number corresponds to some text.
[00:04:12.340 --> 00:04:16.020]   And we try to do the exact same thing, except that each number corresponds to a patch of
[00:04:16.020 --> 00:04:17.180]   an image.
[00:04:17.180 --> 00:04:18.980]   So that's all it is.
[00:04:18.980 --> 00:04:23.940]   So first you need to try to create an encoder that's going to transform that image into
[00:04:23.940 --> 00:04:25.700]   a sequence of numbers.
[00:04:25.700 --> 00:04:28.740]   The same that the tokenizer for text would do.
[00:04:28.740 --> 00:04:29.740]   And once you have that-
[00:04:29.740 --> 00:04:31.180]   Sorry, can you slow down a little bit for me?
[00:04:31.180 --> 00:04:32.180]   Oh, sorry.
[00:04:32.180 --> 00:04:35.380]   So you have an encoder that's taking text, right?
[00:04:35.380 --> 00:04:37.300]   And turning it into some kind of encoded vector.
[00:04:37.300 --> 00:04:38.300]   Yeah, that's right.
[00:04:38.300 --> 00:04:41.700]   And then the vector goes into a decoder that creates the image?
[00:04:41.700 --> 00:04:42.700]   That's right.
[00:04:42.700 --> 00:04:44.500]   But what did you say about the batches?
[00:04:44.500 --> 00:04:47.620]   Yeah, no, that's exactly right.
[00:04:47.620 --> 00:04:52.300]   Except typically when you have the decoder, it would create a number that corresponds
[00:04:52.300 --> 00:04:53.900]   again to text.
[00:04:53.900 --> 00:04:58.780]   Now you want that number to correspond to some kind of image.
[00:04:58.780 --> 00:05:02.420]   So you could try to do RGB, like the pixel values.
[00:05:02.420 --> 00:05:05.580]   The problem is there would be too many, so it wouldn't be very efficient.
[00:05:05.580 --> 00:05:07.500]   That's what ImageGPT was doing.
[00:05:07.500 --> 00:05:11.140]   And that's why they had like, maybe it was 16 by 16 squares.
[00:05:11.140 --> 00:05:14.740]   So it would be very, very small and it's very limited.
[00:05:14.740 --> 00:05:20.420]   But instead what OpenAI did, each number corresponded to a patch.
[00:05:20.420 --> 00:05:23.560]   So for example, one number can correspond to a green patch.
[00:05:23.560 --> 00:05:28.020]   One number can correspond to a blue patch with a yellow dot in the center.
[00:05:28.020 --> 00:05:30.460]   They all correspond to something more complex.
[00:05:30.460 --> 00:05:32.120]   I see.
[00:05:32.120 --> 00:05:35.980]   So you train basically a separate model that's completely independent, and that's trained
[00:05:35.980 --> 00:05:40.580]   separately and frozen later, that learns how to create those patches.
[00:05:40.580 --> 00:05:44.740]   And the goal is you want to, because you're limited in vocabulary, maybe you can create
[00:05:44.740 --> 00:05:49.540]   only 10,000 different patches, 16,000 or something like that.
[00:05:49.540 --> 00:05:52.460]   I think it's what is used commonly.
[00:05:52.460 --> 00:05:56.820]   So what you do, you want to create the patches that are going to be used the most often,
[00:05:56.820 --> 00:05:58.060]   the ones that are the most relevant.
[00:05:58.060 --> 00:06:01.300]   So you have a model that basically is trained to find those patches.
[00:06:01.300 --> 00:06:07.900]   It looks at a lot of images and tries to encode it into a code book where in the end, when
[00:06:07.900 --> 00:06:12.680]   you reconstitute the image, it's as close as possible to the original ones.
[00:06:12.680 --> 00:06:17.700]   Once you build that, once you're able to go from image patch to a number, it's the exact
[00:06:17.700 --> 00:06:21.540]   same thing as doing a translation.
[00:06:21.540 --> 00:06:24.900]   And are the patches in a grid, like a two by two grid?
[00:06:24.900 --> 00:06:25.900]   That's right.
[00:06:25.900 --> 00:06:26.900]   They're in a grid.
[00:06:26.900 --> 00:06:33.220]   So for me, the picture is divided, each patch is about like a 16 by 16.
[00:06:33.220 --> 00:06:34.220]   Oh, wow.
[00:06:34.220 --> 00:06:35.740]   It's amazing you don't notice that.
[00:06:35.740 --> 00:06:37.500]   Yeah, those are big patches.
[00:06:37.500 --> 00:06:42.140]   So it's not completely independent patches because it's a unit.
[00:06:42.140 --> 00:06:47.220]   So with the convolution, there's a bit some overlap in the middle, which makes sure that
[00:06:47.220 --> 00:06:49.140]   you don't see those patches.
[00:06:49.140 --> 00:06:50.140]   I see.
[00:06:50.140 --> 00:06:51.140]   Yeah.
[00:06:51.140 --> 00:06:52.740]   So they're kind of blended together between...
[00:06:52.740 --> 00:06:53.740]   Exactly.
[00:06:53.740 --> 00:06:54.740]   I see.
[00:06:54.740 --> 00:06:55.740]   Yeah.
[00:06:55.740 --> 00:06:56.740]   Interesting.
[00:06:56.740 --> 00:07:06.900]   And so it's the same as from the attention is all you need paper with the attention encoder.
[00:07:06.900 --> 00:07:09.060]   It's kind of identical to that translation model.
[00:07:09.060 --> 00:07:10.060]   Is that right?
[00:07:10.060 --> 00:07:11.060]   Oh, yeah.
[00:07:11.060 --> 00:07:12.060]   It's very similar.
[00:07:12.060 --> 00:07:13.060]   So daily what they do, it's just GPT.
[00:07:13.060 --> 00:07:16.540]   So you have GPT, it reads the text.
[00:07:16.540 --> 00:07:18.700]   Then there's like a special token maybe.
[00:07:18.700 --> 00:07:23.820]   I mean, I don't have exactly the details because the code is not released on the paper, but
[00:07:23.820 --> 00:07:28.500]   the paper is very detailed, but it's missing a few of the small details.
[00:07:28.500 --> 00:07:32.980]   So it's like you have all the text and then you have the encoding for the image at some
[00:07:32.980 --> 00:07:33.980]   point.
[00:07:33.980 --> 00:07:34.980]   And so it predicts.
[00:07:34.980 --> 00:07:38.340]   And at some point there must be a special token that says, "Hey, now it's an image."
[00:07:38.340 --> 00:07:40.660]   So it's switched to another modality.
[00:07:40.660 --> 00:07:45.980]   Me, my model is slightly different in the sense that I have the encoder.
[00:07:45.980 --> 00:07:50.060]   It's like a translation model where the encoder and decoder is separate.
[00:07:50.060 --> 00:07:55.220]   So the encoder will read the caption and then the decoder will do the image and will be
[00:07:55.220 --> 00:07:56.580]   causal.
[00:07:56.580 --> 00:08:01.580]   And the idea was that it would be maybe more efficient.
[00:08:01.580 --> 00:08:08.340]   And now with like a 1D string of text, I kind of understand how you feed back each individual
[00:08:08.340 --> 00:08:12.940]   one into the decoder and it looks at the previous words.
[00:08:12.940 --> 00:08:15.620]   But how does that generalize to like a 2D image?
[00:08:15.620 --> 00:08:16.840]   How do you do that?
[00:08:16.840 --> 00:08:20.840]   So the 2D image, actually you just put the patches next to each other.
[00:08:20.840 --> 00:08:22.220]   You don't consider the 2D.
[00:08:22.220 --> 00:08:26.860]   So it's actually an issue too, because basically when you predict the image, it's almost you
[00:08:26.860 --> 00:08:30.580]   predict from top left to right, to bottom right.
[00:08:30.580 --> 00:08:31.780]   Oh, is that right?
[00:08:31.780 --> 00:08:33.180]   Just like a line by line?
[00:08:33.180 --> 00:08:34.180]   Yeah, yeah.
[00:08:34.180 --> 00:08:35.220]   That's what is done.
[00:08:35.220 --> 00:08:41.020]   So that's a problem because if you mess up at some point, it influences the entire rest.
[00:08:41.020 --> 00:08:44.460]   That's a limitation that diffusion models don't have, for example.
[00:08:44.460 --> 00:08:46.400]   And now, how do you actually train this?
[00:08:46.400 --> 00:08:51.160]   Because I would think it would generate images that would typically be just totally different.
[00:08:51.160 --> 00:08:54.760]   Each image actually is quite different from an RGB value.
[00:08:54.760 --> 00:08:56.640]   So how does the training work?
[00:08:56.640 --> 00:09:03.040]   So the training, the way it works is like the image you will first encode it into a
[00:09:03.040 --> 00:09:05.400]   sequence of numbers.
[00:09:05.400 --> 00:09:11.160]   And then your text will be also a sequence of numbers, typically when you have your tokenizer.
[00:09:11.160 --> 00:09:15.120]   And your input basically is going to be those numbers for the text, and the output is going
[00:09:15.120 --> 00:09:18.480]   to be those numbers for the image.
[00:09:18.480 --> 00:09:22.920]   So the image encoder, it's frozen at that point.
[00:09:22.920 --> 00:09:24.200]   You don't train it.
[00:09:24.200 --> 00:09:29.280]   So you just go from numbers, a sequence of numbers, to another sequence of numbers.
[00:09:29.280 --> 00:09:33.000]   So then what you do, you have that decoder.
[00:09:33.000 --> 00:09:34.000]   It will predict the logits.
[00:09:34.000 --> 00:09:37.160]   It will predict what are the numbers for that image.
[00:09:37.160 --> 00:09:42.840]   And it's a decoder that goes from one to one, and the attention looks only at the previous
[00:09:42.840 --> 00:09:43.840]   patches.
[00:09:43.840 --> 00:09:47.200]   And it will predict it.
[00:09:47.200 --> 00:09:50.440]   And then you have the ground truth, because you know what is the real image.
[00:09:50.440 --> 00:09:53.000]   So then you're going to calculate cross-entropy.
[00:09:53.000 --> 00:09:56.560]   You're going to calculate the loss, the same that you would do with another program, to
[00:09:56.560 --> 00:09:57.680]   see how wrong you were.
[00:09:57.680 --> 00:10:02.560]   So basically, it's kind of strange, because you have a caption that would say, "A cat
[00:10:02.560 --> 00:10:04.200]   in the field."
[00:10:04.200 --> 00:10:06.360]   And then you have a ground truth image.
[00:10:06.360 --> 00:10:10.680]   And basically, the goal is to predict that ground truth image.
[00:10:10.680 --> 00:10:15.240]   It's a bit counterintuitive that it works, because there's so many possible cats in the
[00:10:15.240 --> 00:10:16.240]   field.
[00:10:16.240 --> 00:10:18.280]   And it could be right.
[00:10:18.280 --> 00:10:22.320]   And you try to predict only one specific one, which is that specific image you're looking
[00:10:22.320 --> 00:10:24.640]   at right now.
[00:10:24.640 --> 00:10:25.640]   But somehow it works.
[00:10:25.640 --> 00:10:32.400]   Somehow it learns to eventually associate concepts, to minimize the probability.
[00:10:32.400 --> 00:10:38.080]   The simple way to think about it is, let's say, if you want to predict a view of something
[00:10:38.080 --> 00:10:45.080]   by night, or the view of something on the beach during the day, the model will know
[00:10:45.080 --> 00:10:49.080]   that, OK, it should probably be dark images at the top.
[00:10:49.080 --> 00:10:52.000]   It's going to be black, because it's the night.
[00:10:52.000 --> 00:10:54.160]   Or it's going to be very dark blue.
[00:10:54.160 --> 00:10:57.520]   So I'm going to just increase the probability for those tokens.
[00:10:57.520 --> 00:11:00.520]   And if you say during the day, it's probably going to be blue.
[00:11:00.520 --> 00:11:03.440]   Maybe there's some sky up there.
[00:11:03.440 --> 00:11:07.240]   And then when it outputs the next tokens, it already knows the previous ones.
[00:11:07.240 --> 00:11:11.120]   So you have part of the image, you need to predict the rest.
[00:11:11.120 --> 00:11:13.640]   It's easier to predict.
[00:11:13.640 --> 00:11:18.440]   Oh, so when you're doing the prediction, you're feeding in part of the ground truth image
[00:11:18.440 --> 00:11:20.360]   and then trying to complete it?
[00:11:20.360 --> 00:11:26.840]   So you're feeding the entire image, but basically each token sees what is before itself.
[00:11:26.840 --> 00:11:30.300]   So for example, the first patch doesn't see anything.
[00:11:30.300 --> 00:11:32.960]   But the second patch sees the one that's before.
[00:11:32.960 --> 00:11:33.960]   Not its prediction.
[00:11:33.960 --> 00:11:37.360]   It's the one that actually was before.
[00:11:37.360 --> 00:11:45.240]   The second patch token sees the ground truth patch, not the first predicted patch.
[00:11:45.240 --> 00:11:46.240]   You're right.
[00:11:46.240 --> 00:11:50.000]   The second patch sees the ground truth of the first patch.
[00:11:50.000 --> 00:11:55.200]   Let's say when you predict the second bottom half, it already knows the exact ground truth
[00:11:55.200 --> 00:11:57.040]   of the first top.
[00:11:57.040 --> 00:11:58.700]   And it also sees the prompt.
[00:11:58.700 --> 00:12:03.040]   So the prompt is supposed to help.
[00:12:03.040 --> 00:12:09.680]   But I guess I would think if you were just, say a cat at night or something, if I was
[00:12:09.680 --> 00:12:15.800]   going to guess just a whole image of that, that I want to be closer from an RGB space,
[00:12:15.800 --> 00:12:21.080]   I could imagine it might be better to just predict gray than drawing a specific cat at
[00:12:21.080 --> 00:12:23.600]   a specific point.
[00:12:23.600 --> 00:12:26.200]   That's actually a huge problem.
[00:12:26.200 --> 00:12:28.600]   Remember I was playing with colorization before.
[00:12:28.600 --> 00:12:35.160]   And yeah, you predict gray is like the best, the most chance if you don't know anything,
[00:12:35.160 --> 00:12:38.840]   just output a gray image.
[00:12:38.840 --> 00:12:42.380]   But in that case, the loss is cross entropy on tokens.
[00:12:42.380 --> 00:12:47.240]   So you don't have an advantage in predicting gray because you have the same loss for being
[00:12:47.240 --> 00:12:51.400]   correct or for being wrong, depending on the color.
[00:12:51.400 --> 00:12:55.800]   So saying gray or blue while it's red, you have the same loss there.
[00:12:55.800 --> 00:13:00.200]   Oh, so you're predicting the probability of the next token and there's only a set of...
[00:13:00.200 --> 00:13:01.200]   I see.
[00:13:01.200 --> 00:13:02.200]   I see.
[00:13:02.200 --> 00:13:09.300]   So you don't have the problem that it's going to show black and white images.
[00:13:09.300 --> 00:13:13.600]   It has to predict something and a color is to me as well.
[00:13:13.600 --> 00:13:14.920]   I see.
[00:13:14.920 --> 00:13:16.640]   What was it like to build this?
[00:13:16.640 --> 00:13:18.920]   What kinds of issues did you run into?
[00:13:18.920 --> 00:13:22.560]   I would think this would be one of those things where it's just incredibly hard to debug when
[00:13:22.560 --> 00:13:24.080]   the program's not working.
[00:13:24.080 --> 00:13:31.200]   Yeah, it is quite hard because you don't know why it's not running and what's happening.
[00:13:31.200 --> 00:13:38.120]   I think what was good is the first version, the Dalai Mini, maybe we were lucky.
[00:13:38.120 --> 00:13:40.920]   Somehow it worked well pretty fast.
[00:13:40.920 --> 00:13:45.520]   And we thought a bit of those details, like seeing the last token.
[00:13:45.520 --> 00:13:49.560]   You can have a little mistake, like you have an offset of one token or you have little
[00:13:49.560 --> 00:13:52.200]   primes and it doesn't work.
[00:13:52.200 --> 00:13:54.200]   And we used a pre-trained model.
[00:13:54.200 --> 00:13:59.720]   So we used just a summarization model and we decided, okay, the decoder returns from
[00:13:59.720 --> 00:14:00.720]   scratch.
[00:14:00.720 --> 00:14:02.680]   Now it needs to predict the images.
[00:14:02.680 --> 00:14:07.200]   And what was good is it actually worked pretty well fast.
[00:14:07.200 --> 00:14:14.560]   So when I worked later more on the larger model, you have always a baseline.
[00:14:14.560 --> 00:14:16.520]   Is it getting better or worse?
[00:14:16.520 --> 00:14:21.760]   But there were some bugs that I spent two weeks or more to understand what was happening,
[00:14:21.760 --> 00:14:22.760]   why it doesn't work.
[00:14:22.760 --> 00:14:25.360]   Can you tell me about one of them?
[00:14:25.360 --> 00:14:34.080]   One difficult one was like, let me try to think of an interesting one.
[00:14:34.080 --> 00:14:40.640]   An interesting one was like, I was trying to use Alibi, which is like the way you encode
[00:14:40.640 --> 00:14:42.200]   the position embeddings.
[00:14:42.200 --> 00:14:44.800]   No, no, sorry, I messed up.
[00:14:44.800 --> 00:14:47.080]   I was trying to use SyncFormer.
[00:14:47.080 --> 00:14:53.760]   So SyncFormer is a certain way of adapting the transformer where you normalize the token
[00:14:53.760 --> 00:14:54.960]   and all.
[00:14:54.960 --> 00:15:00.520]   But it's used for encoder models, but for decoder basically, you don't realize it, but
[00:15:00.520 --> 00:15:04.080]   you have some information that leaks through the next token.
[00:15:04.080 --> 00:15:09.960]   So I would have my loss that was going very well, very fast to close to zero.
[00:15:09.960 --> 00:15:11.080]   And I had no clue why.
[00:15:11.080 --> 00:15:15.240]   And it's just when it normalizes, it gets some information of the future.
[00:15:15.240 --> 00:15:20.120]   And that was really hard to understand why that type of model didn't work.
[00:15:20.120 --> 00:15:25.360]   But the biggest challenges were actually, when you make it larger, training those large
[00:15:25.360 --> 00:15:27.240]   models is very, very hard.
[00:15:27.240 --> 00:15:32.200]   And when I first had the small model, I was like, it's going to be easy to have a large
[00:15:32.200 --> 00:15:33.200]   model.
[00:15:33.200 --> 00:15:38.720]   I'm just going to add more layers and train it on bigger computers for longer, maybe a
[00:15:38.720 --> 00:15:42.560]   bit more data, and it's just going to run.
[00:15:42.560 --> 00:15:46.200]   But unfortunately, it didn't work.
[00:15:46.200 --> 00:15:48.400]   And that was a bit sad.
[00:15:48.400 --> 00:15:54.960]   And first, you have to be able to split the memory well across all the devices.
[00:15:54.960 --> 00:16:00.080]   It's not very easy because one model doesn't necessarily fit on one device.
[00:16:00.080 --> 00:16:03.760]   So you need to spread the weights across the other devices.
[00:16:03.760 --> 00:16:06.080]   Jaxx has cool features to do that, by the way.
[00:16:06.080 --> 00:16:09.800]   That was very, very helpful to do that.
[00:16:09.800 --> 00:16:12.400]   But then you have the model that becomes unstable.
[00:16:12.400 --> 00:16:14.800]   You have peaks that happen randomly.
[00:16:14.800 --> 00:16:16.160]   It starts well the first hours.
[00:16:16.160 --> 00:16:18.320]   You have your loss that goes on and you're so happy.
[00:16:18.320 --> 00:16:22.560]   And then suddenly you have big peaks and like, oh, OK, fine, I'm going to restart before.
[00:16:22.560 --> 00:16:23.760]   And it's like, OK, now it went through.
[00:16:23.760 --> 00:16:27.000]   But five minutes later, another peak.
[00:16:27.000 --> 00:16:29.520]   It was really, really hard.
[00:16:29.520 --> 00:16:34.920]   On that level, something that was cool is as I was training it, I was making my reports
[00:16:34.920 --> 00:16:39.080]   on Weights and Bases and I was sharing them online all the time since I was working on
[00:16:39.080 --> 00:16:40.080]   that.
[00:16:40.080 --> 00:16:45.640]   And what's cool is the community on Twitter and all got very engaged with it.
[00:16:45.640 --> 00:16:46.640]   And they were so helpful.
[00:16:46.640 --> 00:16:52.200]   Actually, I don't know if I would have been able to build it with that success without
[00:16:52.200 --> 00:16:57.960]   having shared all the journey because I had a few key elements that helped make the model
[00:16:57.960 --> 00:17:01.120]   better and that were shared by replies on Twitter.
[00:17:01.120 --> 00:17:05.840]   Like, oh, maybe you could try that, like the optimizer we use, like distributed shampoo.
[00:17:05.840 --> 00:17:10.760]   And it just came kind of randomly through Twitter or super conditioning from Catherine
[00:17:10.760 --> 00:17:11.760]   Cross.
[00:17:11.760 --> 00:17:16.640]   And it was like a bit random things that I discovered along the way just by sharing that
[00:17:16.640 --> 00:17:18.640]   training publicly.
[00:17:18.640 --> 00:17:20.520]   So it was very beneficial for me.
[00:17:20.520 --> 00:17:21.520]   That's amazing.
[00:17:21.520 --> 00:17:24.240]   So you were sharing Weights and Bases reports on Twitter and getting feedback?
[00:17:24.240 --> 00:17:25.240]   Oh, yeah, yeah.
[00:17:25.240 --> 00:17:26.240]   I was getting feedback.
[00:17:26.240 --> 00:17:28.560]   And I think little by little there was interest.
[00:17:28.560 --> 00:17:31.180]   I was showing the new predictions.
[00:17:31.180 --> 00:17:36.240]   And then I was like, OK, I wanted that little demo online so people can play with it a bit.
[00:17:36.240 --> 00:17:39.360]   And some people were already engaged, like, oh, yeah, I see it's better at that or it's
[00:17:39.360 --> 00:17:40.920]   better at this.
[00:17:40.920 --> 00:17:43.120]   And it would give me an idea on what to correct.
[00:17:43.120 --> 00:17:49.200]   And having it open helped me a lot because in the end, it's almost like it feels like
[00:17:49.200 --> 00:17:52.240]   it was not just my work.
[00:17:52.240 --> 00:17:55.320]   I got like free advice from everybody too.
[00:17:55.320 --> 00:17:57.640]   So it was really good.
[00:17:57.640 --> 00:18:01.400]   It would be fun to see all the Weights and Bases reports.
[00:18:01.400 --> 00:18:04.320]   I wonder if you could make a collection of the history reports.
[00:18:04.320 --> 00:18:05.320]   I did.
[00:18:05.320 --> 00:18:06.320]   Cool.
[00:18:06.320 --> 00:18:10.600]   Yeah, if you open up a main report, basically, and I link to all those reports.
[00:18:10.600 --> 00:18:12.160]   I link to all the main ones.
[00:18:12.160 --> 00:18:17.200]   If I had to link to all of them, there would probably be like 50 reports or more.
[00:18:17.200 --> 00:18:21.760]   And there's some I did just for myself that I wouldn't share because they were a mess.
[00:18:21.760 --> 00:18:25.760]   The things like sometimes like, oh, yeah, you have a conclusion, you do some tests and
[00:18:25.760 --> 00:18:31.520]   like, okay, it's important, maybe choose Dropout or not to use or weight decay has no effect.
[00:18:31.520 --> 00:18:34.560]   But you forget it when you do so many experiments.
[00:18:34.560 --> 00:18:37.560]   You forget why you don't use it.
[00:18:37.560 --> 00:18:39.760]   And so I would always go back to my report.
[00:18:39.760 --> 00:18:42.020]   I know I had those experiments somewhere.
[00:18:42.020 --> 00:18:44.720]   And I would type a line like, oh, here, this is the runs.
[00:18:44.720 --> 00:18:47.440]   Those are the runs that show that you shouldn't use that.
[00:18:47.440 --> 00:18:52.280]   And it was actually very convenient for me to see why I take some decisions or why the
[00:18:52.280 --> 00:18:57.800]   previous code run, but not now, what was the difference between those two runs.
[00:18:57.800 --> 00:19:00.400]   It was major help.
[00:19:00.400 --> 00:19:01.400]   That's really cool.
[00:19:01.400 --> 00:19:06.720]   Do you feel like there is enough in the DALI paper to really reproduce it?
[00:19:06.720 --> 00:19:10.440]   Or did you feel like there are key things you had to learn along the way to really get
[00:19:10.440 --> 00:19:12.520]   the thing to work?
[00:19:12.520 --> 00:19:18.200]   So the DALI paper actually provides pretty clearly the main ideas.
[00:19:18.200 --> 00:19:21.400]   And the main ideas, like there's the last model we didn't talk about, but there's one
[00:19:21.400 --> 00:19:23.640]   model that's going to encode the image.
[00:19:23.640 --> 00:19:25.480]   The one I use is not the same as the DALI.
[00:19:25.480 --> 00:19:28.600]   It's actually one from a timing transformers.
[00:19:28.600 --> 00:19:34.120]   Some other people added it and added some GAN loss in there, some perceptual loss to
[00:19:34.120 --> 00:19:35.720]   make it a bit better.
[00:19:35.720 --> 00:19:40.800]   So the premise creates those weird artifacts that we have sometimes on the faces and all,
[00:19:40.800 --> 00:19:42.560]   versus the original one.
[00:19:42.560 --> 00:19:46.600]   What it would do, it would do something blurry.
[00:19:46.600 --> 00:19:50.080]   So and then there's that model that needs to predict the next tokens.
[00:19:50.080 --> 00:19:53.960]   So for OpenAI, it's like a kind of a GPT model.
[00:19:53.960 --> 00:19:59.960]   It's more similar to BART, which is encoder decoder, because I thought it could be maybe
[00:19:59.960 --> 00:20:01.080]   more efficient.
[00:20:01.080 --> 00:20:04.480]   And then they have that CLIP model that actually was released.
[00:20:04.480 --> 00:20:09.400]   They released little by little a model larger and larger.
[00:20:09.400 --> 00:20:19.160]   But that's a model that has revolutionized a bit a lot of the research in multiple modalities.
[00:20:19.160 --> 00:20:22.000]   And that includes text and images, and now audio.
[00:20:22.000 --> 00:20:24.360]   People are adapting it for 3D.
[00:20:24.360 --> 00:20:30.200]   But it's that model that basically it will tell you how well-- it learns how well a text
[00:20:30.200 --> 00:20:32.120]   and an image match.
[00:20:32.120 --> 00:20:33.120]   So it will give a score.
[00:20:33.120 --> 00:20:38.040]   You give a text, you give an image, and it will give a score based on how well it believes
[00:20:38.040 --> 00:20:39.040]   they match.
[00:20:39.040 --> 00:20:43.960]   So for example, when you use the demo, we output more images than the nine that you
[00:20:43.960 --> 00:20:44.960]   see.
[00:20:44.960 --> 00:20:51.480]   So maybe 16, maybe more if there's not too much traffic, which never happens nowadays.
[00:20:51.480 --> 00:20:55.920]   But then over the 16, we have CLIP look at them, and it chooses the nine that it thinks
[00:20:55.920 --> 00:20:56.920]   are the best.
[00:20:56.920 --> 00:20:59.240]   And it actually improves the quality quite a bit.
[00:20:59.240 --> 00:21:00.240]   Oh, interesting.
[00:21:00.240 --> 00:21:02.360]   So there's outliers that you don't get to see.
[00:21:02.360 --> 00:21:03.360]   Yeah.
[00:21:03.360 --> 00:21:07.800]   The ones that are really bad, typically it will find them like, "No, don't show them.
[00:21:07.800 --> 00:21:10.520]   It's not worth it."
[00:21:10.520 --> 00:21:14.880]   So the paper actually has those ideas, which are the essential ideas.
[00:21:14.880 --> 00:21:17.920]   Then it's missing some details on how it's trained and all.
[00:21:17.920 --> 00:21:18.920]   There's a lot of details.
[00:21:18.920 --> 00:21:22.880]   There's some things that are missing.
[00:21:22.880 --> 00:21:26.720]   But overall, it's a good base enough to build something.
[00:21:26.720 --> 00:21:30.960]   I wish I could have just run the code immediately and train it.
[00:21:30.960 --> 00:21:36.520]   Actually, in a way, I'm quite happy.
[00:21:36.520 --> 00:21:37.520]   How to say it?
[00:21:37.520 --> 00:21:43.920]   In a way, the fact that it was not released motivated me to learn how to build it.
[00:21:43.920 --> 00:21:48.800]   I don't think I would have learned how it was built if I didn't have to try to build
[00:21:48.800 --> 00:21:49.800]   it myself.
[00:21:49.800 --> 00:21:50.800]   That's cool.
[00:21:50.800 --> 00:21:58.400]   I mean, how sensitive is the performance to the details of how the model works, in your
[00:21:58.400 --> 00:21:59.400]   opinion?
[00:21:59.400 --> 00:22:00.400]   I always wonder this.
[00:22:00.400 --> 00:22:05.080]   I don't know if you have a thought here, but when I look at the attention mechanism, we
[00:22:05.080 --> 00:22:09.440]   tell ourselves a story about the three vectors that get generated and how they're multiplied
[00:22:09.440 --> 00:22:10.440]   together.
[00:22:10.440 --> 00:22:11.440]   Oh, yeah.
[00:22:11.440 --> 00:22:17.760]   I always wonder, how much do you think that the specifics of that really matters?
[00:22:17.760 --> 00:22:21.900]   I think there's some details that whatever you put, it works.
[00:22:21.900 --> 00:22:26.440]   When I started to do machine learning, I know you have to do a convenient to detect cats
[00:22:26.440 --> 00:22:27.440]   and dogs.
[00:22:27.440 --> 00:22:30.840]   I remember I was like, "Oh, I'm going to try to build my own model."
[00:22:30.840 --> 00:22:33.520]   And then you're like, "What depth should I put?
[00:22:33.520 --> 00:22:34.520]   How many layers?"
[00:22:34.520 --> 00:22:38.520]   I'm like, "I'm going to put 12 here, and then I'm going to put 36, and then I'm going to
[00:22:38.520 --> 00:22:39.520]   put less and more."
[00:22:39.520 --> 00:22:41.760]   And then I would put random things for no reason.
[00:22:41.760 --> 00:22:42.760]   And whatever you do, it works.
[00:22:42.760 --> 00:22:48.800]   Like a simple model, whatever you do, in the end, it kind of works.
[00:22:48.800 --> 00:22:53.760]   I think there's so many configurations where it would work, and you actually don't need
[00:22:53.760 --> 00:22:55.440]   to bother too much.
[00:22:55.440 --> 00:22:59.640]   On the larger model, there's some scaling laws and there's a bit of research, and it's
[00:22:59.640 --> 00:23:02.960]   kind of hard to know what works, what doesn't work.
[00:23:02.960 --> 00:23:06.720]   But I try to follow them a bit because I'm like, "Okay, some people tried a few things,
[00:23:06.720 --> 00:23:12.080]   and I'm going to try to use the same ratio they have of width versus depth."
[00:23:12.080 --> 00:23:14.200]   And then I have a report where I tried a lot of variants.
[00:23:14.200 --> 00:23:17.960]   Like transformers, there's like 100 different variants.
[00:23:17.960 --> 00:23:19.840]   So I tried a lot of them.
[00:23:19.840 --> 00:23:22.440]   And actually, some converge.
[00:23:22.440 --> 00:23:26.940]   Some not necessarily converge better, but we're more stable for some reason.
[00:23:26.940 --> 00:23:31.760]   So I tried a bunch of them, and I picked the one that worked the best.
[00:23:31.760 --> 00:23:32.760]   Interesting.
[00:23:32.760 --> 00:23:33.760]   Yeah.
[00:23:33.760 --> 00:23:34.760]   Same for the activation.
[00:23:34.800 --> 00:23:37.320]   The activation function, I don't think it matters a lot.
[00:23:37.320 --> 00:23:38.720]   I tried different...
[00:23:38.720 --> 00:23:42.960]   Initially, when you tried hyperparameters, like, "Oh, let me try different activation
[00:23:42.960 --> 00:23:43.960]   functions."
[00:23:43.960 --> 00:23:47.020]   It barely matters overall.
[00:23:47.020 --> 00:23:50.600]   Whatever you purchase, it's okay.
[00:23:50.600 --> 00:23:52.360]   But maybe there's some advantages.
[00:23:52.360 --> 00:23:55.320]   I had one, maybe it had some noise, and I was like, "Okay, I'm going to take the one
[00:23:55.320 --> 00:23:57.560]   that was stable."
[00:23:57.560 --> 00:24:02.280]   But maybe if I had just taken another seed, I would have had different results.
[00:24:02.280 --> 00:24:06.080]   So I don't know how much I can rely on some of the conclusions.
[00:24:06.080 --> 00:24:07.080]   Cool.
[00:24:07.080 --> 00:24:16.400]   What do you attribute to the model's massive increase in popularity?
[00:24:16.400 --> 00:24:20.360]   We kind of noticed that our metrics for reports are getting messed up because there's so much
[00:24:20.360 --> 00:24:22.000]   traffic to your report.
[00:24:22.000 --> 00:24:25.760]   What do you think is going on?
[00:24:25.760 --> 00:24:26.760]   Yeah.
[00:24:26.760 --> 00:24:31.220]   So I think, somehow, people think that that model is new.
[00:24:31.220 --> 00:24:34.720]   But that model has been there for a year already, almost.
[00:24:34.720 --> 00:24:39.880]   It's just like, over time, I worked on it, and little by little, it became better.
[00:24:39.880 --> 00:24:45.600]   And actually, the traffic, the people using the model, people involved in the forums and
[00:24:45.600 --> 00:24:49.160]   talking about it, actually increased a bit over time.
[00:24:49.160 --> 00:24:54.760]   But I think when I trained a larger model, it reached maybe a critical stage where suddenly
[00:24:54.760 --> 00:25:00.080]   it became good enough for virality.
[00:25:00.080 --> 00:25:07.560]   I think some YouTubers tried, maybe for fun, they would put their name in different situations,
[00:25:07.560 --> 00:25:12.680]   like in the golf cart or whatever, and suddenly they would see their face and they would see
[00:25:12.680 --> 00:25:14.400]   something that looks like them.
[00:25:14.400 --> 00:25:16.760]   That was not very good, but it kind of looks like them.
[00:25:16.760 --> 00:25:20.680]   And they would put themselves in the craziest situations.
[00:25:20.680 --> 00:25:26.200]   And I think they got excited about it, and little by little, it amplified.
[00:25:26.200 --> 00:25:29.160]   But yeah, it's just reached a threshold where it was good enough.
[00:25:29.160 --> 00:25:32.320]   What's fun is the model is actually still training a little bit.
[00:25:32.320 --> 00:25:35.320]   So I'm curious, is it going to be much better?
[00:25:35.320 --> 00:25:36.720]   And there's still stuff to improve.
[00:25:36.720 --> 00:25:41.440]   So it's interesting, it's already reached a threshold that interests people.
[00:25:41.440 --> 00:25:44.720]   And there's easy ways to make it still better.
[00:25:44.720 --> 00:25:47.760]   How much data did you train this model on?
[00:25:47.760 --> 00:25:53.520]   So it's probably, I would say, around maybe 400 million.
[00:25:53.520 --> 00:25:54.520]   Wow.
[00:25:54.520 --> 00:25:55.920]   400, 500 million.
[00:25:55.920 --> 00:26:01.080]   But the data is actually very important.
[00:26:01.080 --> 00:26:07.120]   And there's some tricks here and there to try to make it work.
[00:26:07.120 --> 00:26:10.800]   When we train the model at first, and when you look at a lot of open source models that
[00:26:10.800 --> 00:26:16.920]   exist that you can play with, some problems that would happen a lot is you would say,
[00:26:16.920 --> 00:26:20.800]   "Okay, I want a view of the snowy mountains."
[00:26:20.800 --> 00:26:25.440]   And you would draw maybe well the snowy mountains, and then on top of it, you would write "Shutterstock."
[00:26:25.440 --> 00:26:27.600]   Or you would write "Alameinu."
[00:26:27.600 --> 00:26:31.560]   And the model had learned that, okay, an image typically needs to have a Shutterstock watermark
[00:26:31.560 --> 00:26:37.760]   or Alameinu, which is kind of horrible, because that image was completely new, and you would
[00:26:37.760 --> 00:26:40.480]   have that horrible watermark on it.
[00:26:40.480 --> 00:26:44.320]   So one of the first things I did was, "Okay, I don't want any of those images."
[00:26:44.320 --> 00:26:46.920]   I was like, "How to avoid that?"
[00:26:46.920 --> 00:26:49.800]   So actually, it was a little problem.
[00:26:49.800 --> 00:26:52.120]   But for example, how did I solve that problem?
[00:26:52.120 --> 00:26:57.920]   Initially, I looked online here how to detect if an image has a watermark or not.
[00:26:57.920 --> 00:27:01.680]   There were some things here and there, but it was not that great.
[00:27:01.680 --> 00:27:09.120]   And then some people were trying to generate data sets with fake watermarks to try to detect
[00:27:09.120 --> 00:27:10.120]   it.
[00:27:10.120 --> 00:27:12.880]   And it was already a big challenge to do that.
[00:27:12.880 --> 00:27:17.200]   And then I realized, "Okay, I can just remove all the images that have Shutterstock in the
[00:27:17.200 --> 00:27:19.600]   URL, and that problem is solved."
[00:27:19.600 --> 00:27:25.080]   And actually, that's the solution I took, and it works quite well because you never
[00:27:25.080 --> 00:27:26.960]   see a watermark.
[00:27:26.960 --> 00:27:28.040]   Interesting.
[00:27:28.040 --> 00:27:34.760]   And I guess, how long does it take to train on 400 million images?
[00:27:34.760 --> 00:27:45.240]   So I think what matters the most, when we didn't have a lot of data initially, after
[00:27:45.240 --> 00:27:49.000]   a while, the motor overfits.
[00:27:49.000 --> 00:27:51.760]   And even more, it's a bit smaller.
[00:27:51.760 --> 00:27:57.000]   The smaller model was 400 million parameters, which is already quite big.
[00:27:57.000 --> 00:28:03.680]   But after a while, you overfit maybe, I think, after five or six epochs, I would overfit.
[00:28:03.680 --> 00:28:12.280]   And typically, that was the equivalent of maybe two weeks on one single TPU VM.
[00:28:12.280 --> 00:28:19.320]   Now when you have so much data, maybe more than what your model size can handle, I don't
[00:28:19.320 --> 00:28:22.200]   know if you overfit that easily.
[00:28:22.200 --> 00:28:26.320]   Maybe you can, but that's where you need to be very careful of having a good validation
[00:28:26.320 --> 00:28:32.320]   loss because there's that cool model that's called RearDally, the Russian Dally, which
[00:28:32.320 --> 00:28:34.040]   is really, really nice.
[00:28:34.040 --> 00:28:38.720]   But when you use it, it looks like it overfits.
[00:28:38.720 --> 00:28:42.800]   I think it's not as good at composition, but it will make really nice images.
[00:28:42.800 --> 00:28:49.520]   It encodes the images also in a higher resolution than me, so they have less of those artifacts.
[00:28:49.520 --> 00:28:53.840]   But sometimes you realize that you will type a description and it will show an image that
[00:28:53.840 --> 00:28:55.640]   it has seen before.
[00:28:55.640 --> 00:28:58.440]   So there's a lot of memory in that.
[00:28:58.440 --> 00:29:03.080]   So they overfit, and I think maybe they didn't have the right validation set.
[00:29:03.080 --> 00:29:08.560]   And there's a problem in that validation set too, which is if you just try to take random
[00:29:08.560 --> 00:29:13.240]   set for your training data and you call it the validation set, it doesn't work.
[00:29:13.240 --> 00:29:18.080]   And the reason it doesn't work is typically, let's say the Google logo is present on a
[00:29:18.080 --> 00:29:19.800]   ton of websites.
[00:29:19.800 --> 00:29:25.680]   So you have a ton of URL that will be unique, that will have that Google logo, but maybe
[00:29:25.680 --> 00:29:26.840]   the caption is different.
[00:29:26.840 --> 00:29:32.120]   So maybe if you try to be unique by image and caption, because it's okay to have a same
[00:29:32.120 --> 00:29:39.520]   image with multiple descriptions, but it's not good for your validation set because remember
[00:29:39.520 --> 00:29:42.560]   when you train it, the pixels see the previous pixels.
[00:29:42.560 --> 00:29:46.320]   So it will recognize the image, it may ignore the caption and you will see your validation
[00:29:46.320 --> 00:29:47.320]   loss going down.
[00:29:47.320 --> 00:29:50.200]   But it's just because it doesn't care about the prompt.
[00:29:50.200 --> 00:29:53.120]   It just recognizes the images and just predicts it.
[00:29:53.120 --> 00:29:54.120]   I see.
[00:29:54.120 --> 00:29:55.240]   That makes sense.
[00:29:55.240 --> 00:30:01.360]   So you're really training on just captions on images that you crawled on the web?
[00:30:01.360 --> 00:30:02.360]   That's right.
[00:30:02.360 --> 00:30:05.720]   I would imagine that would introduce all kinds of crazy artifacts.
[00:30:05.720 --> 00:30:09.480]   Is it easy to generate cool logos of companies?
[00:30:09.480 --> 00:30:11.200]   Yeah, it can.
[00:30:11.200 --> 00:30:14.880]   So actually, it's good at creating logos.
[00:30:14.880 --> 00:30:18.200]   It's funny because it's something that made me happy a while back.
[00:30:18.200 --> 00:30:21.960]   So some person reached out to me like, "Hey, my mom started a new business.
[00:30:21.960 --> 00:30:24.400]   She couldn't afford to have a graphic designer.
[00:30:24.400 --> 00:30:25.400]   I just used Aliminia.
[00:30:25.400 --> 00:30:26.400]   I gave her a logo.
[00:30:26.400 --> 00:30:27.400]   It was good enough for her business."
[00:30:27.400 --> 00:30:28.400]   Amazing.
[00:30:28.440 --> 00:30:32.400]   I was so happy that it was helpful in that way.
[00:30:32.400 --> 00:30:35.920]   So it can do surprising things.
[00:30:35.920 --> 00:30:39.400]   Some things that I didn't realize would be possible and the fact that it's open and that
[00:30:39.400 --> 00:30:44.280]   so many people can use it, that's why I'm learning so much more.
[00:30:44.280 --> 00:30:47.000]   I realized I was barely testing the model before.
[00:30:47.000 --> 00:30:52.560]   I was putting a cat on a skateboard while people have those crazy prompts.
[00:30:52.560 --> 00:30:55.960]   My prompt, I thought when I was putting the Eiffel Tower on the moon, I thought I was
[00:30:55.960 --> 00:30:56.960]   being creative.
[00:30:56.960 --> 00:31:03.000]   But it's ridiculous in comparison to what other people do.
[00:31:03.000 --> 00:31:04.000]   That's awesome.
[00:31:04.000 --> 00:31:06.720]   Do you have plans for what you want to do next?
[00:31:06.720 --> 00:31:10.040]   Yeah, I think I have a lot of ideas.
[00:31:10.040 --> 00:31:12.640]   I don't know where I would go next.
[00:31:12.640 --> 00:31:18.240]   Obviously, the models that use diffusion are very attractive because they do very impressive
[00:31:18.240 --> 00:31:19.240]   images.
[00:31:19.240 --> 00:31:21.680]   So it's definitely something I want to look at.
[00:31:21.680 --> 00:31:22.680]   How does that work?
[00:31:22.680 --> 00:31:24.680]   I'm not for models that do diffusion.
[00:31:24.680 --> 00:31:30.040]   So diffusion, the way it works, instead of predicting the image in one shot, because
[00:31:30.040 --> 00:31:36.120]   here you predict a patch and you just predict in a way in one shot, you iterate many times
[00:31:36.120 --> 00:31:39.880]   and you have an image that's initially just noise, random noise.
[00:31:39.880 --> 00:31:46.480]   So imagine random pixels, random colors, and you try to remove that noise little by little.
[00:31:46.480 --> 00:31:51.560]   And you go through it like a hundred times or maybe a thousand times, a high number of
[00:31:51.560 --> 00:31:52.560]   steps.
[00:31:52.560 --> 00:31:56.920]   And you try to go through it a bit less times, but you go through the same model many times.
[00:31:56.920 --> 00:31:59.640]   Each time it removes a little bit of noise.
[00:31:59.640 --> 00:32:02.840]   And at the end, it turns up to an image that's actually cool.
[00:32:02.840 --> 00:32:08.600]   So it's almost like I compare to me almost if it was like a recurrent model where you
[00:32:08.600 --> 00:32:13.400]   know you go through it, but the fact that you just remove the noise little by little,
[00:32:13.400 --> 00:32:18.480]   it guides it to a loss that's very friendly to train.
[00:32:18.480 --> 00:32:20.720]   So it's like super promising.
[00:32:20.720 --> 00:32:27.760]   And it's already proved to work very well with Dali too.
[00:32:27.760 --> 00:32:28.760]   So that's something cool.
[00:32:28.760 --> 00:32:35.640]   The problem with those models is they're a bit more computationally expensive.
[00:32:35.640 --> 00:32:40.800]   So we still need a bit of research on how to make it more efficient, which I think is
[00:32:40.800 --> 00:32:44.300]   something interesting.
[00:32:44.300 --> 00:32:45.960]   So I'll probably look at that.
[00:32:45.960 --> 00:32:50.720]   But there's also my current model as many ways it can also be improved in cheap ways
[00:32:50.720 --> 00:32:52.260]   and fast ways.
[00:32:52.260 --> 00:32:58.880]   So maybe I try to do that a bit or fine tuning it on your own art or your own data.
[00:32:58.880 --> 00:33:01.960]   I think that could be pretty cool.
[00:33:01.960 --> 00:33:06.520]   And then there's people you can use the same model to maybe generate sound or music or
[00:33:06.520 --> 00:33:07.520]   video.
[00:33:07.520 --> 00:33:10.740]   You can do so much with the same type of model.
[00:33:10.740 --> 00:33:13.920]   So it's pretty exciting where it's going.
[00:33:13.920 --> 00:33:16.160]   I think it's going to go very fast too.
[00:33:16.160 --> 00:33:23.160]   Is there any feature we can add to Weights and Biases to help you with your work?
[00:33:23.160 --> 00:33:27.440]   To be fair, I feel like I've been using all the features with that project.
[00:33:27.440 --> 00:33:31.720]   You know, I have the pipeline, the model is trained, I have the checkpoints, I resume
[00:33:31.720 --> 00:33:32.820]   from those.
[00:33:32.820 --> 00:33:33.820]   It's all tracked.
[00:33:33.820 --> 00:33:39.800]   When I do inference, I cannot do inference during training because it would be too expensive.
[00:33:39.800 --> 00:33:42.600]   I need to load that image decoder model.
[00:33:42.600 --> 00:33:43.780]   So you know, it would be inefficient.
[00:33:43.780 --> 00:33:49.040]   So I have another machine that does it, that's linked to the checkpoint.
[00:33:49.040 --> 00:33:54.960]   So I have this entire pipeline that's set up and that does regularly some inference.
[00:33:54.960 --> 00:33:55.960]   What could be added?
[00:33:55.960 --> 00:33:59.680]   Are you using alerts for when your model starts to go badly?
[00:33:59.680 --> 00:34:05.000]   I should because I look at the training way too often on my phone.
[00:34:05.000 --> 00:34:08.160]   You have a little pause or I go somewhere, I'm working and I'm checking quickly, is it
[00:34:08.160 --> 00:34:12.240]   still training or did the TPU crash or not?
[00:34:12.240 --> 00:34:14.360]   Is the loss going high?
[00:34:14.360 --> 00:34:20.520]   Maybe alerts would make me feel more relaxed.
[00:34:20.520 --> 00:34:24.340]   Well I have to tell you, it's been really fun to watch my friends who aren't in machine
[00:34:24.340 --> 00:34:29.560]   learning talking about Dolly Mini and being like, "I know Boris.
[00:34:29.560 --> 00:34:30.560]   I know the guy that made it."
[00:34:30.560 --> 00:34:31.640]   And they're impressed.
[00:34:31.640 --> 00:34:35.840]   So congratulations on such a successful model.
[00:34:35.840 --> 00:34:37.960]   It's really captured everybody's imagination.
[00:34:37.960 --> 00:34:38.960]   Thank you.
[00:34:38.960 --> 00:34:39.960]   That's fun.
[00:34:39.960 --> 00:34:42.360]   I think it's cool to see so many people using it.
[00:34:42.360 --> 00:34:49.920]   I was a bit scared because you could see it in negative ways too or it's creating images.
[00:34:49.920 --> 00:34:52.920]   But overall, the reaction has been pretty positive.
[00:34:52.920 --> 00:34:58.200]   People are happy that they can see through that model what are the limitations and biases
[00:34:58.200 --> 00:35:00.120]   and what can it be used for.
[00:35:00.120 --> 00:35:01.440]   And they can test it out themselves.
[00:35:01.440 --> 00:35:06.040]   And if I had to figure out the limitation and biases myself, it would be impossible.
[00:35:06.040 --> 00:35:08.640]   So I think it's actually really cool.
[00:35:08.640 --> 00:35:12.960]   And I like that it's used for people who cannot draw at all like me.
[00:35:12.960 --> 00:35:17.480]   It's kind of cool because even if the image is not that pretty, it's still so much better
[00:35:17.480 --> 00:35:19.400]   than what I could do.
[00:35:19.400 --> 00:35:24.160]   But for people who are actually talented, I'm happy to see that some people, they use
[00:35:24.160 --> 00:35:25.160]   it as inspiration.
[00:35:25.160 --> 00:35:27.960]   They take, "Hey, this was the output from Dolly Mini."
[00:35:27.960 --> 00:35:31.400]   And then they use Photoshop and they do something crazy out of it.
[00:35:31.400 --> 00:35:35.280]   And I think it's really nice to see that it can also be used that way.
[00:35:35.280 --> 00:35:36.280]   Awesome.
[00:35:36.280 --> 00:35:37.280]   Well, thanks so much, Boris.
[00:35:37.280 --> 00:35:38.280]   Great chat.
[00:35:38.280 --> 00:35:39.280]   Thanks.
[00:35:39.280 --> 00:35:40.280]   Awesome chatting with you.
[00:35:40.280 --> 00:35:45.560]   If you're enjoying these interviews and you want to learn more, please click on the link
[00:35:45.560 --> 00:35:50.280]   to the show notes in the description where you can find links to all the papers that
[00:35:50.280 --> 00:35:54.520]   are mentioned, supplemental material, and a transcription that we work really hard to
[00:35:54.520 --> 00:35:55.520]   produce.
[00:35:55.520 --> 00:35:55.880]   So check it out.
[00:35:55.880 --> 00:35:57.880]   [music fades out]

