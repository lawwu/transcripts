
[00:00:00.000 --> 00:00:04.880]   say, okay, I think we would also like to get to the demos. We can
[00:00:04.880 --> 00:00:07.200]   talk about fastA all day but like it's such an interesting
[00:00:07.200 --> 00:00:10.440]   time. Let's come back to the demos because we do have
[00:00:10.440 --> 00:00:13.040]   interesting submissions and we'll just be wrapping up
[00:00:13.040 --> 00:00:17.200]   today. So today was supposed to be a few interesting submissions
[00:00:17.200 --> 00:00:20.160]   and we do have winners. So I let Wade announce them since he's
[00:00:20.160 --> 00:00:24.240]   been leading the study group. And since we do have the winners
[00:00:24.240 --> 00:00:27.160]   in the poll, we'll invite them to present otherwise either Wade
[00:00:27.160 --> 00:00:28.520]   or I will walk through.
[00:00:29.720 --> 00:00:32.840]   Yeah, awesome. All right, I'm going to share my screen just
[00:00:32.840 --> 00:00:36.680]   briefly, then I'm going to hand it around to folks that have
[00:00:36.680 --> 00:00:40.320]   submitted their work and let them present and demo and answer
[00:00:40.320 --> 00:00:43.440]   all the tough questions related to it. So
[00:00:43.440 --> 00:00:47.200]   Sorry to interrupt the fastA chat. I know we're like so
[00:00:47.200 --> 00:00:48.000]   passionate about it.
[00:00:48.000 --> 00:00:52.480]   Can everybody see the presentation screen?
[00:00:52.480 --> 00:00:53.480]   Yes.
[00:00:53.480 --> 00:00:56.680]   All right. I got like three monitors going on here. And it's
[00:00:56.680 --> 00:00:59.520]   never clear to me which one's got the focus. And I'm always
[00:00:59.520 --> 00:01:03.600]   hoping like, it's the right one. Because I got all kinds of other
[00:01:03.600 --> 00:01:06.240]   stuff going on. And I'm like, hopefully people are seeing the
[00:01:06.240 --> 00:01:07.040]   presentation.
[00:01:07.040 --> 00:01:12.040]   I think trying to get me to a wait, wait is that what's going
[00:01:12.040 --> 00:01:12.240]   on?
[00:01:12.240 --> 00:01:16.560]   Yeah, it's always good. Like, like, sometimes I don't get any
[00:01:16.560 --> 00:01:19.000]   responses. But usually, Sanyin, you're pretty good. You go like,
[00:01:19.000 --> 00:01:21.840]   Okay, yeah, I see it. Other people are like, maybe just
[00:01:21.840 --> 00:01:26.720]   messing with me. So yeah, so this is a session five. And
[00:01:27.480 --> 00:01:31.200]   first, thank you so much for being here at the study groups.
[00:01:31.200 --> 00:01:35.120]   Thanks to Weights and Biases. Thanks to Sanyin, Andrea, Zach,
[00:01:35.120 --> 00:01:41.600]   Morgan, Arto, and anybody else I'm missing. And then, and
[00:01:41.600 --> 00:01:46.720]   thanks to everybody who's stuck along for the whole four, five
[00:01:46.720 --> 00:01:49.880]   weeks of this study group. It's been a lot of fun for I think
[00:01:49.880 --> 00:01:50.480]   everybody.
[00:01:50.480 --> 00:01:55.560]   I'm just throwing this up here. This will be part of the slide
[00:01:55.560 --> 00:01:59.080]   deck. It's the same resources you've seen before. Hopefully
[00:01:59.080 --> 00:02:03.640]   you all will continue with Fast.ai and Hugging Face. And
[00:02:03.640 --> 00:02:07.480]   and then Sanyin, you still gonna keep up with the Chai Time Data
[00:02:07.480 --> 00:02:10.800]   Science podcast? Does that continue at Weights and Biases?
[00:02:10.800 --> 00:02:13.080]   Yeah, it will.
[00:02:13.080 --> 00:02:19.520]   Okay, good. Good, good, good. So anyways, keep keep up with all
[00:02:19.520 --> 00:02:23.560]   that stuff. And again, if you got questions or want to help
[00:02:23.560 --> 00:02:29.600]   others who are on the same path that you've already walked, be a
[00:02:29.600 --> 00:02:33.800]   part active part of the discord. And, and we'll keep working
[00:02:33.800 --> 00:02:38.000]   through Fast.ai and Hugging Face stuff there. So the agenda
[00:02:38.000 --> 00:02:41.400]   today is pretty simple. This is the finale. We've been through
[00:02:41.400 --> 00:02:44.000]   the first part of the Hugging Face course, we looked at Fast.ai
[00:02:44.000 --> 00:02:49.160]   and how to train Hugging Face Transformers with Fast.ai. And
[00:02:49.160 --> 00:02:53.080]   so today, we'll be looking at some of the community
[00:02:53.080 --> 00:02:58.240]   submissions, blog posts, one competition notebook, and then
[00:02:58.240 --> 00:03:01.520]   we'll do some final Q&A. And I think that what we'll do is we
[00:03:01.520 --> 00:03:05.280]   go through and look at the winners of the competitions.
[00:03:05.280 --> 00:03:10.640]   After they present, maybe we'll do a little Q&A for each
[00:03:10.640 --> 00:03:15.480]   presenter. And Sanyin, maybe you can manage all the questions and
[00:03:15.480 --> 00:03:19.240]   answers, as you've done previously. So we'll do that.
[00:03:19.240 --> 00:03:22.360]   And then at the end, if there's some other Q&A or anything else
[00:03:22.360 --> 00:03:26.800]   that folks want to talk about, we can do that. So let's get to
[00:03:26.800 --> 00:03:33.280]   it. The awards go to for best blog posts on training
[00:03:33.280 --> 00:03:37.480]   Transformers with Fast.ai. We had two really excellent
[00:03:38.320 --> 00:03:48.840]   submissions. Tasmiya, do you go by Tasin or Tasmiya? You're on
[00:03:48.840 --> 00:03:49.200]   mute.
[00:03:49.200 --> 00:03:54.160]   Just call me Maisha. Bangla naming kind of works differently
[00:03:54.160 --> 00:03:57.520]   from English. So that would work or Tasin too.
[00:03:57.520 --> 00:04:02.520]   Okay, okay. I see Tasin on Zoom. And then up here, when you sent
[00:04:02.520 --> 00:04:05.680]   the email, it said Tasmiya. So I was like, okay, let me figure
[00:04:05.680 --> 00:04:08.680]   out which one's the right one here. So I'll go with Tasin. I
[00:04:08.680 --> 00:04:14.880]   think I've heard you referred to that way on the forums. But you
[00:04:14.880 --> 00:04:19.160]   actually really submitted like a really impressive blog post and
[00:04:19.160 --> 00:04:22.720]   the work on classifying Bangla fake news with Hugging Face
[00:04:22.720 --> 00:04:27.440]   Transformers and Fast.ai. And then Ravi actually has an entire
[00:04:27.440 --> 00:04:31.240]   series. So he submitted his third part, training
[00:04:31.240 --> 00:04:34.840]   Transformers with Fast.ai. But I also have linked his entire
[00:04:35.280 --> 00:04:39.400]   website in the in the slide deck, because I think you have
[00:04:39.400 --> 00:04:43.640]   three articles for the study session. And so they're all
[00:04:43.640 --> 00:04:47.080]   great. And there's also a bunch of other stuff with the Fast.ai
[00:04:47.080 --> 00:04:50.080]   study group and whatnot. So I wanted to make sure folks were
[00:04:50.080 --> 00:04:54.760]   aware of that. So Tasin, let's start with you. And I'm going to
[00:04:54.760 --> 00:04:59.120]   unshare my screen and let you share. And you can go over.
[00:05:00.160 --> 00:05:06.240]   Oops, hold on. Clicking the wrong things. And if you'd like
[00:05:06.240 --> 00:05:09.280]   to share your work, that'd be great. And then we'll go to
[00:05:09.280 --> 00:05:10.440]   Ravi afterwards.
[00:05:10.440 --> 00:05:20.960]   Do I have to share screen right now?
[00:05:20.960 --> 00:05:25.480]   Yeah, just I think you should be able to use it or be able to
[00:05:25.480 --> 00:05:26.240]   share right now.
[00:05:27.240 --> 00:05:28.240]   Oh, no.
[00:05:28.240 --> 00:05:35.560]   Once again,
[00:05:35.560 --> 00:05:41.000]   meanwhile, people in the call and watching YouTube live, please
[00:05:41.000 --> 00:05:43.760]   drop some clap emojis for our winners.
[00:05:43.760 --> 00:05:45.640]   Join me for that.
[00:05:45.640 --> 00:05:47.160]   It's been a great
[00:05:47.160 --> 00:05:50.600]   Is it working properly?
[00:05:50.600 --> 00:05:52.240]   Yeah, I see it.
[00:05:54.360 --> 00:06:00.040]   Okay, so before we go into it, first, I will go over how I
[00:06:00.040 --> 00:06:00.960]   started
[00:06:00.960 --> 00:06:08.920]   on this project. So initially, the HuggingFest recently launched
[00:06:08.920 --> 00:06:13.880]   like Jack's Community Week, and we were getting like, Bangla
[00:06:13.880 --> 00:06:17.920]   GPT-2 and T5 models. And there I started working with big news
[00:06:17.920 --> 00:06:21.680]   data for like, evaluating the language models on downstream
[00:06:21.680 --> 00:06:26.200]   tasks. So I kind of had some of the code prepared. And after
[00:06:26.200 --> 00:06:30.280]   that, so the real issue with Bangla fake news is the data
[00:06:30.280 --> 00:06:33.760]   set is are totally not available. There's probably only
[00:06:33.760 --> 00:06:38.720]   two papers on right now, which are really good. One of them is
[00:06:38.720 --> 00:06:42.800]   this one and probably the most prominent one. So here I was
[00:06:42.800 --> 00:06:47.920]   trying to reproduce the work done by them. So they also
[00:06:47.920 --> 00:06:51.560]   trained a multilingual bird on the same data set, but they had
[00:06:51.680 --> 00:06:55.080]   a much more like computational resources. So they kind of
[00:06:55.080 --> 00:06:58.040]   co-trained the bird for like 50 approaches without hanging.
[00:06:58.040 --> 00:07:04.240]   That is like a huge thing. So I will go over it briefly. So,
[00:07:04.240 --> 00:07:07.960]   okay, my work is as well, I have to post some stock images there
[00:07:07.960 --> 00:07:12.080]   when I'm feeling good. So in this post, we cover finetuning a
[00:07:12.080 --> 00:07:16.280]   multilingual bird model from HuggingFest Transformers
[00:07:16.280 --> 00:07:19.680]   library on Bangla fake news data set. It was released in
[00:07:19.680 --> 00:07:25.960]   AEREC 2020. It has a lot of low resource papers. So English fake
[00:07:25.960 --> 00:07:30.560]   news has a lot of coverage in social media. But same sort of
[00:07:30.560 --> 00:07:34.520]   effort in Bangla is non-existent, as always. So
[00:07:34.520 --> 00:07:38.160]   Bangla, this ban fake news data set contains like 50k annotated
[00:07:38.160 --> 00:07:43.640]   articles, but the problem is extremely imbalanced data. At
[00:07:43.640 --> 00:07:48.640]   least 48k articles are like authentic and only maybe 1200
[00:07:48.640 --> 00:07:54.320]   articles are fake. So as far as I know that I have worked with
[00:07:54.320 --> 00:07:58.000]   transformers, but I'm not really that experienced. So I will go
[00:07:58.000 --> 00:08:02.680]   with the way I can say it. So this post is like using the
[00:08:02.680 --> 00:08:07.400]   materials from this course. And so I initially was debating
[00:08:07.400 --> 00:08:11.560]   between which library to use like BLUR or FastHash or AdaptNLP.
[00:08:11.560 --> 00:08:15.960]   So I, the FastHash one, I'm really sorry, R2, but it's not
[00:08:15.960 --> 00:08:20.600]   really documented that well yet. And so I kind of like choose
[00:08:20.600 --> 00:08:24.160]   between BLUR and AdaptNLP. And after that, I went with BLUR
[00:08:24.160 --> 00:08:27.600]   because the high level API seemed easier to use like for
[00:08:27.600 --> 00:08:31.120]   people like me who are kind of new at both FastAI and
[00:08:31.120 --> 00:08:34.560]   HoneyBase. Like I'm not totally new in FastAI definitely, but I
[00:08:34.560 --> 00:08:38.720]   have never really used it in a way, especially with the v2
[00:08:38.720 --> 00:08:45.200]   release. So after that, so here are all the imports. So this
[00:08:45.200 --> 00:08:49.120]   data set is available in Kaggle. And this is like one sample, we
[00:08:49.120 --> 00:08:52.680]   have the article and the headline too. So in a news
[00:08:52.680 --> 00:08:55.720]   article, I was trying a sentiment classification on news
[00:08:55.720 --> 00:09:00.120]   in our times, Kaggle competition once and I saw that if I drop
[00:09:00.120 --> 00:09:03.920]   the headline, the scores were going down in general. So in
[00:09:03.920 --> 00:09:06.560]   this case, I decided to kind of concatenate the headline with
[00:09:06.560 --> 00:09:11.280]   the article before feeding into the model. And so they actually
[00:09:11.320 --> 00:09:15.600]   did a lot of categorization here. And there's instead of
[00:09:15.600 --> 00:09:19.120]   two, there's four data set available. So I actually framed
[00:09:19.120 --> 00:09:24.040]   this problem as a general binary classification problem. And so
[00:09:24.040 --> 00:09:27.560]   after that, this is kind of a news article in Bangla. And it's
[00:09:27.560 --> 00:09:33.640]   always written in a very heavy way because it's news. So here
[00:09:33.640 --> 00:09:38.720]   I'm kind of doing the trend split. This data set had one
[00:09:38.720 --> 00:09:43.920]   issue that the big and authentic articles but in totally
[00:09:43.920 --> 00:09:47.200]   different data sets. So there was no like random sample
[00:09:47.200 --> 00:09:51.160]   provided. So we kind of had to work with it. And the real
[00:09:51.160 --> 00:09:55.960]   problem I have always been with like, I'm kind of new. So I had
[00:09:55.960 --> 00:10:01.760]   trouble like setting up the data loaders in PyTorch so far. And I
[00:10:01.760 --> 00:10:04.360]   understood that if I have to get better than I have planned like
[00:10:04.360 --> 00:10:08.280]   how to take different sort of data set and manage it into a
[00:10:08.280 --> 00:10:12.320]   data loaders, but I'm still not good at it. So here I initially
[00:10:12.320 --> 00:10:16.560]   was using the low level API. But then I figured let's just go
[00:10:16.560 --> 00:10:20.560]   from like a top down approach as Jeremy always says, for this
[00:10:20.560 --> 00:10:23.520]   blog post, I also wanted to kind of design it in a way so that
[00:10:23.520 --> 00:10:27.200]   people can just read it and go over like the most important
[00:10:27.200 --> 00:10:30.120]   parts in the like, really like what do I need to learn from
[00:10:30.120 --> 00:10:34.000]   first AI and what do I need to learn from having this to go
[00:10:34.000 --> 00:10:38.800]   over like to make something. So I was watching all the lectures
[00:10:38.800 --> 00:10:42.000]   and there's like four video lectures. And the information in
[00:10:42.000 --> 00:10:44.920]   them is actually pretty heavy. If I start writing on it, it
[00:10:44.920 --> 00:10:49.040]   would at least take at least three chapters of a book. Even
[00:10:49.040 --> 00:10:53.200]   if it looks small, one hour videos, but the content is pretty
[00:10:53.200 --> 00:10:57.160]   large. So after that, I understood for like, working
[00:10:57.160 --> 00:11:00.120]   with this project, I need to kind of first understand two
[00:11:00.120 --> 00:11:04.160]   topics properly. From first AI dimension, the topics are
[00:11:04.160 --> 00:11:08.840]   callbacks and data blocks. And for Hugging Quiz dimension, I
[00:11:08.840 --> 00:11:11.560]   need to understand how to download the pre-trained weights
[00:11:11.560 --> 00:11:15.160]   and configuration part. Initially, I was like, why do I
[00:11:15.160 --> 00:11:19.600]   have this model configuration part separated from the like,
[00:11:19.600 --> 00:11:23.720]   like, this is very modular, right? When I was initially
[00:11:23.720 --> 00:11:29.480]   using like, first AI course two or three, you kind of have that
[00:11:29.520 --> 00:11:32.640]   this notebook, then you write down all the code for CNN or
[00:11:32.640 --> 00:11:35.400]   anything, or it is probably wasted and then you train it.
[00:11:35.400 --> 00:11:39.120]   But right now, all of the models are like pre-trained. I work
[00:11:39.120 --> 00:11:42.080]   with TensorFlow Hub 2.0. And they also focused a lot on
[00:11:42.080 --> 00:11:46.480]   releasing like components of model to public people, like
[00:11:46.480 --> 00:11:50.080]   blocks people can mix and match. But we can kind of see that now
[00:11:50.080 --> 00:11:54.000]   that the data set also, part is also kind of being thought of
[00:11:54.000 --> 00:11:57.160]   like that, that TensorFlow's data sets library and here
[00:11:57.160 --> 00:12:01.240]   PyTorch's data loader and first AI's data block. So I went over
[00:12:01.240 --> 00:12:04.760]   the paper first, like how they're thinking about it, this
[00:12:04.760 --> 00:12:10.080]   problem of like, how to take this data to the models. And I'm
[00:12:10.080 --> 00:12:13.400]   even probably with it. And honestly speaking, I'm kind of
[00:12:13.400 --> 00:12:17.200]   lazy. So I don't really want to work too hard on anything. So I
[00:12:17.200 --> 00:12:22.000]   figured, let's just go over the paper first. So from that, I saw
[00:12:22.000 --> 00:12:26.560]   that the data set part, like the data block design is the high
[00:12:26.560 --> 00:12:31.400]   level API for first AI. And there's the idea is like, going
[00:12:31.400 --> 00:12:37.920]   over the like, raw specifications for how I want to
[00:12:37.920 --> 00:12:42.200]   pre-process the data for like pre-defined problem domains. And
[00:12:42.200 --> 00:12:44.280]   in this case, it's text classification. So it's like a
[00:12:44.280 --> 00:12:48.440]   really easy problem domain. So we just use this burner for
[00:12:48.440 --> 00:12:52.080]   sequence classification. And pretty much the modeling part is
[00:12:52.080 --> 00:12:57.600]   here actually. We can just go over the documentation and the
[00:12:57.600 --> 00:13:04.360]   IMDB, like the thing there, like that demo that was that works
[00:13:04.360 --> 00:13:08.400]   pretty well. So I just have to, technically speaking, this is
[00:13:08.400 --> 00:13:11.680]   not that hard, but I wanted to like create a blog post that
[00:13:11.680 --> 00:13:14.880]   goes over the entire process, get the data, use the data block
[00:13:14.880 --> 00:13:19.880]   API, integrate with a browser, then push the model to hub and
[00:13:20.120 --> 00:13:24.320]   also use 1DB in the process. I didn't use OS and BIOS so far,
[00:13:24.320 --> 00:13:27.640]   it seemed too hard, but it's really cool. So this time I used
[00:13:27.640 --> 00:13:30.200]   it and my system was really impressed. Okay, what is this
[00:13:30.200 --> 00:13:36.600]   thing? And then, so here, first I will go over the data block
[00:13:36.600 --> 00:13:42.840]   API, some parts. So here we're doing the data split. And now,
[00:13:42.840 --> 00:13:47.760]   the main idea is to kind of like just take this data set and take
[00:13:47.760 --> 00:13:52.720]   it to the burner for sequence classification API. So I saw
[00:13:52.720 --> 00:14:00.000]   that the burner API actually is using like fastAI's data blocks,
[00:14:00.000 --> 00:14:04.640]   so like the ideas really well, the splitter methods and the,
[00:14:04.640 --> 00:14:07.800]   like there are many splitting methods for like just splitting
[00:14:07.800 --> 00:14:11.720]   the data set, right? There is a random splitting method and also
[00:14:11.720 --> 00:14:17.480]   this one where we're just kind of showing which one is from the
[00:14:17.480 --> 00:14:20.920]   training set and which one is from the evaluation set here.
[00:14:20.920 --> 00:14:27.120]   So after that, there was this model choosing part. So for this
[00:14:27.120 --> 00:14:30.840]   project, my initial aim was to use, there is a Bangabart model
[00:14:30.840 --> 00:14:35.160]   available in Hynkbe's hub now using that and comparing with
[00:14:35.160 --> 00:14:38.320]   the multilingual part. But the way it worked out, actually, I
[00:14:38.320 --> 00:14:41.240]   could train the multilingual part, but when I tried that
[00:14:41.240 --> 00:14:45.440]   Bangabart model, it started giving some errors with some
[00:14:45.440 --> 00:14:48.640]   sort of, I'm not really sure why it happened, I would have to
[00:14:48.640 --> 00:14:52.280]   see it later. So here, I actually had to kind of
[00:14:52.280 --> 00:14:57.280]   experiment with the learning data a lot. Because I saw that
[00:14:57.280 --> 00:15:01.680]   in Colab, this data set, it has only 50 articles in total, but
[00:15:01.680 --> 00:15:05.520]   it takes at least one and a half hour for running in my network
[00:15:05.520 --> 00:15:09.480]   at least, or repo. And waiting that much for a blog post is
[00:15:09.480 --> 00:15:12.640]   like really hard. So I kind of showed to myself, I'm never
[00:15:12.640 --> 00:15:16.240]   going to take really large data set for blog post again. This
[00:15:16.240 --> 00:15:21.760]   is the last time. And so here are some results. The actual
[00:15:21.760 --> 00:15:25.840]   paper people did in Bangabart for like 50 books, the writing
[00:15:25.840 --> 00:15:29.520]   rate was this, and the optimizer was around, batch size was 32.
[00:15:29.520 --> 00:15:33.240]   I tried to really increase the batch size because in this sort
[00:15:33.240 --> 00:15:36.280]   of cases, the batch size is really important. As far as I
[00:15:36.280 --> 00:15:39.160]   know, larger batch sizes generally give better results.
[00:15:39.160 --> 00:15:43.920]   But here I couldn't really go over for the laptop hangs. And
[00:15:43.920 --> 00:15:49.040]   so the iPhone score for them was like the tech class was 0.68.
[00:15:49.040 --> 00:15:54.240]   And overall was 0.99. So I actually checked out multilingual
[00:15:54.240 --> 00:15:58.520]   barter. I understood that this has been trained on like
[00:15:58.520 --> 00:16:02.520]   multiple languages with a really large corpora. It has also
[00:16:02.520 --> 00:16:07.160]   Bangla, the model is in support of an organization like BP
[00:16:07.200 --> 00:16:10.840]   probably, these are the information for the model. And
[00:16:10.840 --> 00:16:15.440]   here is like the one DB part where I seriously had to figure
[00:16:15.440 --> 00:16:18.560]   out like I think like one DB is providing too much information,
[00:16:18.560 --> 00:16:23.960]   to be really honest. And for me, I would have to learn like how
[00:16:23.960 --> 00:16:29.960]   to kind of figure out like, which it wants to keep. I know
[00:16:29.960 --> 00:16:32.680]   for a team working on like infrastructure, you don't really
[00:16:32.680 --> 00:16:36.520]   get to have a lot of system related knowledge. So here you
[00:16:36.520 --> 00:16:40.360]   can see the loss is going down. And here I will actually go
[00:16:40.360 --> 00:16:46.480]   over the how to understand like that or any other, any of the
[00:16:46.480 --> 00:16:49.840]   libraries. So I've been like, so these are the four components
[00:16:49.840 --> 00:16:54.480]   like really important. So I'm considering how to use this data
[00:16:54.480 --> 00:16:58.120]   block or the philosophy behind it, to be honest, like I really
[00:16:58.120 --> 00:17:01.560]   like that data blocks idea of composing like transformations
[00:17:01.560 --> 00:17:05.960]   together to kind of achieve a particular task. It totally goes
[00:17:05.960 --> 00:17:09.520]   with my philosophy to like make things as easy as possible. And
[00:17:09.520 --> 00:17:13.880]   so I also like the learning rate finder. But one problem I
[00:17:13.880 --> 00:17:17.840]   think like that focusing really a lot on learning rate finding
[00:17:17.840 --> 00:17:21.080]   and like making the model converge as fast as possible
[00:17:21.080 --> 00:17:24.640]   doesn't essentially translate into like the business bid. I
[00:17:24.640 --> 00:17:27.640]   mean, we're not a competition for like, fastest model
[00:17:27.640 --> 00:17:31.480]   possible in general. And the model performance matters a lot
[00:17:31.480 --> 00:17:34.640]   too. So I was really experimenting a lot with how to
[00:17:34.640 --> 00:17:39.680]   use this learning rate finder, but I was not sure how to like
[00:17:39.680 --> 00:17:44.760]   use it in a really like, proper way of business term, I guess.
[00:17:44.760 --> 00:17:50.720]   So after that, so here is my understanding of how the data
[00:17:50.720 --> 00:17:54.600]   block API works. And here is where I am totally heading over
[00:17:54.600 --> 00:17:59.000]   to work with me again, this is the paper link. And so this is
[00:17:59.000 --> 00:18:02.440]   the low level API, I actually went to the low level API for
[00:18:02.440 --> 00:18:06.760]   Brad to see how the like the high level API is using the low
[00:18:06.760 --> 00:18:12.000]   level API. And it was really cool. I saw here, one part I
[00:18:12.000 --> 00:18:15.560]   couldn't really highlight in the blog post is how Brian is kind
[00:18:15.560 --> 00:18:18.680]   of taking the like hugging this models and the architecture
[00:18:18.680 --> 00:18:22.400]   components and integrating it because this part is like in the
[00:18:22.400 --> 00:18:27.520]   video and that part is really large. So here we are kind of
[00:18:27.520 --> 00:18:30.160]   sending the data frame, we're sending the pre trained model
[00:18:30.160 --> 00:18:33.680]   name, the model name, and then the banner for sequence
[00:18:33.680 --> 00:18:36.560]   classification, this high level API actually goes over like
[00:18:36.560 --> 00:18:41.240]   fetching the hugging this objects and for hugging this
[00:18:41.240 --> 00:18:44.480]   objects, we mean the model configuration and architecture.
[00:18:44.480 --> 00:18:49.760]   So like those parts I felt like it was important to understand
[00:18:49.760 --> 00:18:52.680]   and highlight, but I didn't do it. I tried to use the learning
[00:18:52.680 --> 00:18:57.480]   rate finder. So I saw in the past API post that so like this
[00:18:57.480 --> 00:19:01.080]   is 10 inverse 2 is the minimum one. And the suggested one is
[00:19:01.080 --> 00:19:04.920]   like in general, one order of magnitude more than this. So I
[00:19:04.920 --> 00:19:08.000]   then went with the 10 inverse 3. And the interesting part was
[00:19:08.000 --> 00:19:11.600]   like the actual paper also used this one like 580 inverse 2 as
[00:19:11.600 --> 00:19:15.080]   a learning rate. I thought that was interesting. So I tried this
[00:19:15.080 --> 00:19:19.640]   with one cycle, I did one mistake here. So I was initially
[00:19:19.640 --> 00:19:22.440]   running over the full dataset, but it was taking a lot of time.
[00:19:22.440 --> 00:19:25.720]   After that, I had to really decide that this is the time I'm
[00:19:25.720 --> 00:19:28.840]   going to take half of the dataset. So instead of 48,000,
[00:19:28.840 --> 00:19:32.480]   I took like 20,000 articles for Bangla, authentic news and the
[00:19:32.480 --> 00:19:35.880]   rest were fake. So you can see kind of the F1 score is going
[00:19:35.880 --> 00:19:41.040]   to like 0.79. But that one, that the actual benchmark model
[00:19:41.040 --> 00:19:43.680]   trained for 50 books, and this one is training for 500 books.
[00:19:43.680 --> 00:19:47.800]   So I'm going to hook with my performance here. And so these
[00:19:47.800 --> 00:19:51.960]   are some of the results. And honestly, this is the fake news
[00:19:52.000 --> 00:19:57.120]   in from news. But it's kind of like it's in the Bangla fake
[00:19:57.120 --> 00:20:00.640]   news data collection, the news resources were collected from
[00:20:00.640 --> 00:20:03.800]   the really reputed trusted sources. So the fake news were
[00:20:03.800 --> 00:20:08.880]   like maybe 1200-1500. I found this part of how to annotate the
[00:20:08.880 --> 00:20:11.920]   fake news part really interesting from that actual
[00:20:11.920 --> 00:20:17.240]   paper. So here is the run summaries, run history. I really
[00:20:17.240 --> 00:20:21.160]   like this results from one review and report. And so this
[00:20:21.160 --> 00:20:24.400]   is the part where we model share. So one problem I found
[00:20:24.400 --> 00:20:28.840]   here is like, so for sending something to HangingFace hub,
[00:20:28.840 --> 00:20:32.680]   we have to kind of use this Git LFS because the file size are
[00:20:32.680 --> 00:20:37.320]   really big for the vocabulary, like the tokenizer and the
[00:20:37.320 --> 00:20:41.480]   model weights. So this is the one where we are pushing to the
[00:20:41.480 --> 00:20:45.600]   hub. HangingFace is a really good like API for like pushing
[00:20:45.600 --> 00:20:50.200]   to the push to hub like this is the highest level and there can
[00:20:50.200 --> 00:20:53.400]   also be like some more custom optimizations on how we want to
[00:20:53.400 --> 00:20:56.640]   send things to hub. And I thought this is like a blur
[00:20:56.640 --> 00:21:00.240]   transformer from this, this part I'm not sure this is not in
[00:21:00.240 --> 00:21:04.000]   documentation yet that much like this is part where I'm really
[00:21:04.000 --> 00:21:07.480]   using the code from that is given. But here I want to kind
[00:21:07.480 --> 00:21:11.040]   of expand over how to explain predictions part. So I was
[00:21:11.040 --> 00:21:15.680]   really trying to like how to visualize like the model
[00:21:15.680 --> 00:21:18.720]   explanations more. I used in Bangla, I worked on Bangla
[00:21:18.720 --> 00:21:21.520]   question answering before and but back then there was no
[00:21:21.520 --> 00:21:24.400]   interpretability libraries for transformers. And this is the
[00:21:24.400 --> 00:21:29.040]   first time I found it. So here I saw this uses something called
[00:21:29.040 --> 00:21:35.840]   layered integrated gradients for like scoring which tokens are
[00:21:35.840 --> 00:21:39.840]   contributing to the models final predictions more. So here is
[00:21:39.840 --> 00:21:42.840]   like the visualization. One problem I found is like this
[00:21:42.840 --> 00:21:47.640]   level zero part. So when we are kind of using the guard API, it
[00:21:47.680 --> 00:21:51.040]   automatically takes care of tracking which level is which.
[00:21:51.040 --> 00:21:56.720]   But after I have sent it to hub and added down from the hub with
[00:21:56.720 --> 00:22:00.120]   the pipeline API is from the level zero and one. So it's kind
[00:22:00.120 --> 00:22:03.600]   of hard to interpret. And I feel like this is a very subtle bug.
[00:22:03.600 --> 00:22:07.240]   And another thing I noticed is that like when we're trying to
[00:22:07.240 --> 00:22:10.480]   like predict a sequence, this one is probably predicting is
[00:22:10.480 --> 00:22:15.560]   that authentic, but these are sub word tokens and sub word
[00:22:15.560 --> 00:22:18.600]   tokens are like inherently less interpretable than the word
[00:22:18.600 --> 00:22:21.200]   tokens as far as I understand, because these are breaking into
[00:22:21.200 --> 00:22:26.400]   like the smaller bits. So I would have to like study more
[00:22:26.400 --> 00:22:29.800]   over how it's working. But I would like to show how this
[00:22:29.800 --> 00:22:34.160]   transformers interpret work here and let's just plug in for the
[00:22:34.160 --> 00:22:38.640]   library people. So here is how the pipeline part works. So
[00:22:38.640 --> 00:22:42.320]   having this is really a good idea here. But this is a bit
[00:22:42.320 --> 00:22:44.800]   like scikit learns pipeline where we're kind of composing
[00:22:44.800 --> 00:22:47.560]   all the models and the transformation to achieve a
[00:22:47.560 --> 00:22:51.040]   particular task. But here we're just doing it for like
[00:22:51.040 --> 00:22:54.600]   transformer models on LMP tasks. So this is like a model for
[00:22:54.600 --> 00:22:57.560]   sequence classification and auto tokenizer. It picks out which
[00:22:57.560 --> 00:23:02.680]   tokenizer to use with like which model. So this is where this is
[00:23:02.680 --> 00:23:06.120]   my first model sent to hub at last after like failing for at
[00:23:06.120 --> 00:23:09.800]   least three times during the Jax week. So I was totally proud of
[00:23:09.800 --> 00:23:13.680]   it. And I also kind of did another mistake, leave hard,
[00:23:13.680 --> 00:23:18.280]   if I like to that it is time to collab after like cleaning a
[00:23:18.280 --> 00:23:21.640]   whole model for five hours. This is like my like better than
[00:23:21.640 --> 00:23:27.800]   knowledge here. So this is like the random text. And so what
[00:23:27.800 --> 00:23:31.440]   they use like right now this library is supporting question
[00:23:31.440 --> 00:23:34.560]   answering translation and probably sequence classification
[00:23:34.560 --> 00:23:39.880]   task but a node like support for like NAR and token
[00:23:39.880 --> 00:23:45.240]   classification support are not provided yet. So here we kind
[00:23:45.240 --> 00:23:47.520]   of use the sequence classification explainer and
[00:23:47.520 --> 00:23:50.520]   we send in the model and the tokenizer and it sports the
[00:23:50.520 --> 00:23:54.120]   tokens according to how much attention the model is giving
[00:23:54.120 --> 00:23:56.960]   to each of the like tokens. Here the tokens are actually
[00:23:56.960 --> 00:24:01.080]   somewhat tokens instead of the real ones and we can kind of
[00:24:01.080 --> 00:24:07.320]   see like which one is getting more importance. And but
[00:24:07.320 --> 00:24:12.360]   honestly, this is not that interpretable. If I try to show
[00:24:12.360 --> 00:24:15.960]   it like this is very important, but this is at least it's
[00:24:15.960 --> 00:24:19.400]   actually more interpretable than zero. And we will transform
[00:24:19.400 --> 00:24:22.960]   our models are more like a black box and really hard to
[00:24:22.960 --> 00:24:26.720]   understand who with all the importer stats. So here are the
[00:24:26.720 --> 00:24:31.160]   resources I think people who want to like go over like to
[00:24:31.160 --> 00:24:35.080]   use this like integrated libraries which are kind of
[00:24:35.080 --> 00:24:39.560]   composing and making two or three more frameworks together
[00:24:39.560 --> 00:24:44.440]   like that are like fast hubs and I didn't really see this
[00:24:44.440 --> 00:24:49.800]   trade that much until now. Like using totally different like
[00:24:49.800 --> 00:24:54.160]   libraries to totally work with our honestly, I feel like the
[00:24:54.160 --> 00:24:57.520]   Pytorch ecosystem is much better in this respect compared to
[00:24:57.520 --> 00:25:01.800]   having this and so I'm sorry, compared to TensorFlow. So here
[00:25:01.800 --> 00:25:04.720]   are all the resources I would totally recommend to go over
[00:25:04.720 --> 00:25:09.600]   this first time paper first to kind of have that idea and I
[00:25:09.600 --> 00:25:13.760]   thought the documentation was really good too. I really like
[00:25:13.760 --> 00:25:20.040]   this collab and here are some model specific parts and I also
[00:25:20.040 --> 00:25:24.440]   had to learn how to use callbacks. So that was a that's
[00:25:24.440 --> 00:25:30.360]   a very large part of the library. So I have to understudy
[00:25:30.360 --> 00:25:35.000]   it and this part, this blog post I really like to like how to
[00:25:35.000 --> 00:25:40.360]   use the learning rate finder technique. So the real benefit
[00:25:40.360 --> 00:25:45.160]   for the first AI integration with HuggingFace comes from this
[00:25:45.160 --> 00:25:49.040]   idea that we can kind of take the different parameters into
[00:25:49.040 --> 00:25:52.600]   groups and use different learning rates for each of the
[00:25:52.600 --> 00:25:56.040]   groups, right? So without understanding this, how to like
[00:25:56.040 --> 00:25:58.600]   use this learning rate finder technique or this discriminating
[00:25:58.600 --> 00:26:02.040]   fine tuning how this sort of things are working is hard to
[00:26:02.040 --> 00:26:07.840]   like, kind of get an idea of how to use it. So here is, this is
[00:26:07.840 --> 00:26:13.400]   pretty much my blog post. It's very like small, but that's the
[00:26:13.400 --> 00:26:18.040]   like done by purpose. I wanted to like highlight extremely
[00:26:18.040 --> 00:26:21.800]   important parts of me and nothing else. So thanks a lot
[00:26:21.800 --> 00:26:29.200]   for considering it as a winning post. If anyone wants to ask
[00:26:29.200 --> 00:26:31.200]   anything as a QA partner.
[00:26:31.200 --> 00:26:38.360]   I just want to say before probably Sanyam steps in with
[00:26:38.360 --> 00:26:42.720]   some questions. This is like really blew me away like how
[00:26:42.720 --> 00:26:47.040]   great the article was, and how it was presented. I was so
[00:26:47.040 --> 00:26:50.120]   jealous because I blog, you know, a little bit and I was
[00:26:50.120 --> 00:26:52.960]   like, I don't think anything I've ever done looks as nice as
[00:26:52.960 --> 00:26:55.880]   your work. So I want to ask the first question.
[00:26:55.880 --> 00:26:59.960]   Yeah, we were discussing by letting you were judging this,
[00:26:59.960 --> 00:27:02.680]   we were just talking about how extensive and like thought of
[00:27:02.680 --> 00:27:05.920]   this blog is so my side is not it's not short, like we at
[00:27:05.920 --> 00:27:08.560]   least a few of us found it like very detailed.
[00:27:08.560 --> 00:27:12.520]   Yeah, it was, it was awesome. And just that you applied also
[00:27:12.520 --> 00:27:16.080]   this stuff to a different language besides English. And I
[00:27:16.080 --> 00:27:19.680]   love the interpretation stuff at the end, like I didn't even
[00:27:19.680 --> 00:27:25.280]   know that existed. But how did you get so good at blogging and
[00:27:25.280 --> 00:27:29.480]   being able to articulate your work in a short blog post like
[00:27:29.480 --> 00:27:31.080]   this? Like, it's really phenomenal.
[00:27:31.080 --> 00:27:36.760]   Oh, so I was so I'm kind of more like an internet style like
[00:27:36.760 --> 00:27:39.800]   Sanyam will understand it, right? Like there are many
[00:27:39.800 --> 00:27:43.400]   people from India who are like, kind of we are not really
[00:27:43.400 --> 00:27:47.080]   fitting in that much with the people around us. And so we were
[00:27:47.120 --> 00:27:50.640]   I was hanging out in Cora all the time. When I was younger, I
[00:27:50.640 --> 00:27:54.080]   was in Canada and Waterloo totally alone. I totally hung
[00:27:54.080 --> 00:27:57.840]   up with the Cora top writer group all the time. So back
[00:27:57.840 --> 00:28:01.800]   then I was really volatile, I guess, moody. And I was a
[00:28:01.800 --> 00:28:04.600]   teenager, but the people were really great. They were great
[00:28:04.600 --> 00:28:07.440]   writers, they were giving me feedback on my writing all the
[00:28:07.440 --> 00:28:11.640]   time. So for five years, I was like a Cora top writer from
[00:28:11.680 --> 00:28:17.200]   straight from the like year I started writing. And I got a
[00:28:17.200 --> 00:28:20.880]   lot of feedback on my writing. But I was writing on pop
[00:28:20.880 --> 00:28:25.680]   culture, like the movie and the wiki and the fandom. And I was
[00:28:25.680 --> 00:28:29.480]   totally in the fan fiction stuff too. So I kind of brought that
[00:28:29.480 --> 00:28:32.480]   and then I got interested in machine learning in actually
[00:28:32.480 --> 00:28:37.480]   2017. And I went to Uda, City and Uda City also has a really
[00:28:37.480 --> 00:28:42.000]   good career section in their website and also in their
[00:28:42.000 --> 00:28:44.880]   channel. They encourage a lot of people to blog about their
[00:28:44.880 --> 00:28:49.240]   experience. And then I met Luis Serrano. So Luis Serrano was
[00:28:49.240 --> 00:28:54.040]   the like the head of like, machine learning like the
[00:28:54.040 --> 00:28:57.600]   nanodiggy. Like he was doing something typical and I'm very
[00:28:57.600 --> 00:29:03.000]   sure about it. So like he was encouraging a lot to me to blog
[00:29:03.000 --> 00:29:05.960]   too when I showed my capsule project there, he told me it
[00:29:05.960 --> 00:29:09.360]   looks like a story. So that totally went to my head, okay,
[00:29:09.360 --> 00:29:12.560]   my work looks like a story. I want to use my storytelling
[00:29:12.560 --> 00:29:16.160]   abilities of my total like fandom of pop culture to
[00:29:16.160 --> 00:29:20.920]   machine learning blogging. And then I started like, in 2019
[00:29:20.920 --> 00:29:24.680]   first, I actually started working with Bangla data. So I
[00:29:24.680 --> 00:29:29.120]   was doing an NLP course in my university. And I initially did
[00:29:29.120 --> 00:29:32.560]   a machine translation project. And then I had to do my thesis
[00:29:32.560 --> 00:29:37.160]   project. So there too, I didn't want to work. So my supervisor
[00:29:37.160 --> 00:29:39.760]   told me, you know, Maisha, you should actually put some effort
[00:29:39.760 --> 00:29:43.800]   in your thesis project and do a research paper on it. I was
[00:29:43.800 --> 00:29:46.920]   like, no, I don't want to do it. So after a lot of back and
[00:29:46.920 --> 00:29:49.280]   forth like that, I told okay, I'm going to work on Bangla
[00:29:49.280 --> 00:29:52.280]   question answering. And I was more immature back then, like
[00:29:52.280 --> 00:29:56.360]   even one year ago, I should have done it. But my professor was a
[00:29:56.360 --> 00:30:00.640]   very patient guy. Like he also kind of gave me at least five
[00:30:00.640 --> 00:30:04.720]   iterations of feedback on my paper draft writing. And I
[00:30:04.720 --> 00:30:10.280]   found writing like related work sections really are like, if
[00:30:10.280 --> 00:30:13.560]   anyone has seen like the university academic writing
[00:30:13.560 --> 00:30:16.880]   sections, they put a lot of effort on how to do literature
[00:30:16.880 --> 00:30:21.120]   search. So after literature search also has in HCI
[00:30:21.120 --> 00:30:25.440]   especially, like how to do literature synthesis. So, so
[00:30:25.440 --> 00:30:28.400]   when we're like, kind of taking multiple sources and people
[00:30:28.400 --> 00:30:32.080]   also try to synthesize it. In academia, I think that writing
[00:30:32.080 --> 00:30:37.240]   and reading is like a sport. So people really try to go over
[00:30:37.240 --> 00:30:41.520]   lots of ideas on how to do it efficiently. I ended up
[00:30:41.520 --> 00:30:45.560]   downloading a bunch of like, tools for literature studying.
[00:30:45.560 --> 00:30:49.680]   That is the Mendeli and there is also Zotero and some more. So
[00:30:49.680 --> 00:30:52.600]   after going through all of that, and then this review with
[00:30:52.600 --> 00:30:57.520]   writing technique, and also a lot of anime. So, and I was
[00:30:57.520 --> 00:31:02.160]   also, yeah, Luis did have an interview. I really like his
[00:31:02.160 --> 00:31:08.840]   content. He has, sorry, I was reading the chat. So, after,
[00:31:08.840 --> 00:31:11.960]   like, you kind of have this all of the component and I'm also
[00:31:11.960 --> 00:31:16.200]   having kind of thinking how to really kind of manage good
[00:31:16.200 --> 00:31:20.360]   blog posts. In like, machine learning community, the blog
[00:31:20.360 --> 00:31:24.520]   posts are generally very, very technical. And they're also
[00:31:24.520 --> 00:31:27.640]   encouraged to be technical by the people who are like, reading
[00:31:27.640 --> 00:31:30.320]   often, like the master handlers don't want to spend their time
[00:31:30.320 --> 00:31:34.440]   over reading something like that is kind of considered as easy
[00:31:34.440 --> 00:31:38.040]   by them. But for me, when I'm a beginner, then I can't really
[00:31:38.040 --> 00:31:41.240]   study their notebooks properly and actually understand it. So
[00:31:41.240 --> 00:31:44.840]   there is like that information discrepancy part always. And I
[00:31:44.840 --> 00:31:47.840]   don't really like it. I feel like reading is like a very
[00:31:47.840 --> 00:31:50.960]   universal thing. And people, whatever people did, they should
[00:31:50.960 --> 00:31:56.040]   be able to understand it. And so and econo, and also another
[00:31:56.040 --> 00:32:01.360]   thing is like, HCI and social sciences, often their papers
[00:32:01.360 --> 00:32:03.960]   totally read like stories. They're also representing
[00:32:03.960 --> 00:32:08.720]   information, but in a way that looks very, I mean, they're
[00:32:08.720 --> 00:32:12.520]   totally doing a storytelling on like qualitative research. And
[00:32:12.520 --> 00:32:15.960]   I also went to like many of the lectures in the qualitative
[00:32:15.960 --> 00:32:18.920]   research community because one of my teachers works there and I
[00:32:18.920 --> 00:32:22.520]   kind of had to sit with her for like, seminars, I didn't want
[00:32:22.520 --> 00:32:27.880]   to, but she really insisted it. And then another thing is the
[00:32:27.880 --> 00:32:31.760]   rationalist community. So I have a friend, Kevin Simner, like,
[00:32:31.760 --> 00:32:36.160]   so he's like a longtime blogger, he runs a blog called the
[00:32:36.160 --> 00:32:41.040]   Melting Asphalt. And he has the really like, blogging community.
[00:32:41.040 --> 00:32:44.520]   So there are people from Les Rong, then there is a ribbon
[00:32:44.520 --> 00:32:48.400]   farm from like the Venkatesh. So those people already blog like
[00:32:48.400 --> 00:32:52.120]   a pro. And I have been reading their blogs from when I was a
[00:32:52.120 --> 00:32:55.760]   child. So I always wanted to do what they do, like they're
[00:32:55.760 --> 00:32:58.520]   actually kind of, they're coming from the philosophy community
[00:32:58.520 --> 00:33:02.600]   actually. So that philosophy community really focuses on
[00:33:02.600 --> 00:33:05.400]   representing ideas, but the machine learning community
[00:33:05.400 --> 00:33:09.120]   really focuses on representing systems. And I'm not sure how to
[00:33:09.120 --> 00:33:13.080]   kind of like combine those two yet. I really like the way they
[00:33:13.080 --> 00:33:17.200]   do series like the blog post series, Rafi Chandra has a, has
[00:33:17.200 --> 00:33:20.000]   one and I'm really interested in seeing it like how to plan out
[00:33:20.000 --> 00:33:23.200]   the content production for series. But like, that's the
[00:33:23.200 --> 00:33:28.160]   thing, like, for good blogging, I felt like the main ingredient
[00:33:28.160 --> 00:33:33.640]   is like coherence first, and also kind of like, writing is
[00:33:33.640 --> 00:33:36.760]   like putting a lens, right? So like, we kind of have to pick
[00:33:36.760 --> 00:33:39.520]   and choose on this element should be emphasized and this
[00:33:39.520 --> 00:33:44.120]   element should be like reduced. Always. And it's true for
[00:33:44.120 --> 00:33:47.320]   academic writing too. And my formal writing is really bad.
[00:33:47.320 --> 00:33:51.640]   Like, for here, I was writing a blog for a while, and then I had
[00:33:51.640 --> 00:33:54.640]   to switch to formal writing. And now I'm switching to blogging
[00:33:54.640 --> 00:33:57.920]   again. And so this back and forth kind of makes writing
[00:33:57.920 --> 00:34:02.520]   really hard. I know Sebastian Bludar, he really pokes it up
[00:34:02.520 --> 00:34:05.840]   really well. When I started writing NLP papers first, I
[00:34:05.840 --> 00:34:08.880]   totally went to his blog, tried to copy his writing style,
[00:34:08.880 --> 00:34:13.960]   literally wrote it a lot like in his style. But it's really
[00:34:13.960 --> 00:34:18.480]   like I'm that book. I don't know even if I would be but that's
[00:34:18.480 --> 00:34:23.760]   kind of the idea. Like, you kind of have to work like a creative
[00:34:23.760 --> 00:34:28.160]   person in Hollywood more like a blogger instead of a scientist.
[00:34:28.160 --> 00:34:31.000]   Like, we can steal other people's techniques, kind of use
[00:34:31.000 --> 00:34:35.760]   it and do an experiment with it. That's how I like to work.
[00:34:35.760 --> 00:34:40.400]   It was awesome. Like, you have a real gift for articulating
[00:34:40.400 --> 00:34:43.640]   technical concepts that are you make it easy to understand.
[00:34:43.640 --> 00:34:49.240]   So I hope you keep up doing this. It was really great. Like,
[00:34:49.240 --> 00:34:52.040]   I was just like, Oh my gosh, like he did such a great job.
[00:34:52.040 --> 00:34:55.360]   Like, these are difficult concepts. And so like, you did a
[00:34:55.360 --> 00:34:59.600]   great job presenting them. And it was super straightforward,
[00:34:59.600 --> 00:35:02.680]   easy to read and understand. So like, really awesome.
[00:35:02.680 --> 00:35:05.600]   And the philosophy was if I don't understand something, then
[00:35:05.600 --> 00:35:06.560]   I'm not going to write it.
[00:35:06.560 --> 00:35:09.560]   Yeah, this is something like Sonia talks about a lot, you
[00:35:09.560 --> 00:35:13.760]   know, blogging and doing articles. And I think just even
[00:35:13.760 --> 00:35:15.760]   from what you're saying, like the more you do it, the better
[00:35:15.760 --> 00:35:18.440]   you get. And if you want to get good at it, you got to just get
[00:35:18.440 --> 00:35:22.720]   in there and just do it. But really fabulous. So I'll let
[00:35:22.720 --> 00:35:25.160]   Sonia ask any other questions that popped up.
[00:35:25.160 --> 00:35:29.880]   Thanks. Thanks. Thanks, Manisha. I would just quickly say I
[00:35:29.880 --> 00:35:33.160]   would request moving the questions to the chat. So if you
[00:35:33.160 --> 00:35:37.680]   could. But I would also like to point out that many people have
[00:35:37.680 --> 00:35:40.960]   been applauding your work on the YouTube chat and also in the
[00:35:40.960 --> 00:35:44.520]   Zoom chat. Guys, this is like an online study group. So I would
[00:35:44.520 --> 00:35:46.960]   request you to at least drop some clap emojis. This was an
[00:35:46.960 --> 00:35:50.160]   incredible work. And I know you mentioned that this was your
[00:35:50.160 --> 00:35:53.360]   first turn with Blur. So that's really cool to see that you've
[00:35:53.360 --> 00:35:56.880]   done such an interesting work. And I think also, it's really
[00:35:56.880 --> 00:35:59.640]   fascinating, at least for me to understand like the weights and
[00:35:59.640 --> 00:36:02.240]   biases, you just need to add one command and we take care of
[00:36:02.240 --> 00:36:02.640]   everything.
[00:36:02.640 --> 00:36:06.200]   Yeah, that was one of the like craziest thing ever, right? I
[00:36:06.200 --> 00:36:09.320]   mean, the loading techniques, we didn't really have it in like
[00:36:09.320 --> 00:36:11.760]   even one year ago or two year ago.
[00:36:11.760 --> 00:36:15.200]   Yeah, I'm sad to admit, I actually did a conference call
[00:36:15.200 --> 00:36:17.360]   with Morgan because I'm like, hey, how do I use weights and
[00:36:17.360 --> 00:36:21.760]   biases in fast.ai? And literally, we had like a half an
[00:36:21.760 --> 00:36:25.680]   hour block and the call lasted 30 seconds. And I was like,
[00:36:25.680 --> 00:36:28.600]   that's it. And he goes, Yeah, that's it. I was just like, oh,
[00:36:28.600 --> 00:36:32.400]   my gosh, like, that's amazing right there. And yeah,
[00:36:33.680 --> 00:36:36.840]   but at least for the framework that I integrated, a lot of
[00:36:36.840 --> 00:36:40.560]   them are with just one line, we capture every possible detail. I
[00:36:40.560 --> 00:36:43.200]   know you mentioned like it's a lot of details, but also we're
[00:36:43.200 --> 00:36:46.760]   an easy person like you just add that one line, you're training
[00:36:46.760 --> 00:36:49.600]   in Cola, you're training on your model, your model on your
[00:36:49.600 --> 00:36:53.040]   machine, you're training in AWS, you don't have to worry about
[00:36:53.040 --> 00:36:57.520]   it. So thanks for trying that out as well. Now let's move to
[00:36:57.520 --> 00:36:59.880]   conversation. I think Ravi is next.
[00:37:00.000 --> 00:37:03.120]   Yeah, Ravi. Share away.
[00:37:03.120 --> 00:37:08.000]   Yep. Thank you so much for selecting the blog post. My
[00:37:08.000 --> 00:37:11.920]   motivations are really different. Writing the blog
[00:37:11.920 --> 00:37:17.480]   post. Let me share my screen. One second. I think Zoom requires
[00:37:17.480 --> 00:37:25.520]   some permissions. That's a different on Mac. Okay. Well, it
[00:37:25.520 --> 00:37:32.840]   looks like I have to rejoin the call. Is it okay? Okay. Yeah,
[00:37:32.840 --> 00:37:35.040]   just let me rejoin the call.
[00:37:35.040 --> 00:37:41.520]   Give like a permission and then they will simply start.
[00:37:41.520 --> 00:37:43.520]   Yeah, that's the problem. Yeah, let me come back.
[00:37:43.520 --> 00:37:49.200]   Hey, Tosin, what did you think about Jax versus PyTorch since
[00:37:49.200 --> 00:37:50.600]   you did the Jax week?
[00:37:51.480 --> 00:37:55.680]   Jax makes parallelization really easy. There is like a P map and
[00:37:55.680 --> 00:38:00.800]   V map for like parallelizing batches. And like, there's also
[00:38:00.800 --> 00:38:05.960]   function for vectorizing like a function. So I thought that was
[00:38:05.960 --> 00:38:09.240]   really cool, but it's really hard to understand and I need to
[00:38:09.240 --> 00:38:13.280]   like study it. And three weeks is not enough. Like there has
[00:38:13.280 --> 00:38:16.960]   been three lectures on Jax there, but this is definitely
[00:38:16.960 --> 00:38:19.520]   not enough for people like me when it's hard to understand it.
[00:38:19.760 --> 00:38:22.680]   So I'm going to like go through their forums and like really
[00:38:22.680 --> 00:38:23.240]   understand.
[00:38:23.240 --> 00:38:26.480]   In three weeks you can decide if you want to dive into a free
[00:38:26.480 --> 00:38:26.480]   module.
[00:38:26.480 --> 00:38:28.160]   No, like three days is the content.
[00:38:28.160 --> 00:38:33.600]   Okay. So can I go ahead?
[00:38:33.600 --> 00:38:35.600]   Please, please. Yeah, go for it.
[00:38:35.600 --> 00:38:39.000]   Okay, I'm able to share my screen. Okay, my motivations
[00:38:39.000 --> 00:38:42.800]   were different when I'm writing the blog post. So I submitted
[00:38:42.800 --> 00:38:47.080]   one notebook, which is MRPC on the dataset compilation thing.
[00:38:47.480 --> 00:38:52.280]   And I use the bird to bird uncaged for, for, for defining
[00:38:52.280 --> 00:38:56.720]   for using this particular particular competition. But then
[00:38:56.720 --> 00:39:00.840]   what I encountered in the while doing this one is that since I'm
[00:39:00.840 --> 00:39:06.680]   very new to the new to this fast AI world, I have been roughly
[00:39:06.680 --> 00:39:10.680]   about 10 weeks, 11 weeks, I guess. So I was trying to
[00:39:10.680 --> 00:39:13.320]   understand all the fast AI library and then I for
[00:39:13.320 --> 00:39:16.640]   initially I felt very old by what's PyTorch and what's fast
[00:39:16.640 --> 00:39:19.840]   AI and all those things. But then once I tried understanding
[00:39:19.840 --> 00:39:23.760]   his callback system and how it is at low level, it's how it is
[00:39:23.760 --> 00:39:27.800]   a just a PyTorch. And then I how I saw and how I use those
[00:39:27.800 --> 00:39:31.800]   concepts from the fast book, then I understood that that was
[00:39:31.800 --> 00:39:35.400]   very easy in terms of creating models and creating all of the
[00:39:35.400 --> 00:39:40.720]   stuff. And at that point, when I was trying to understand all of
[00:39:40.720 --> 00:39:44.880]   these things, I wished I had some kind of blog post come
[00:39:44.880 --> 00:39:47.880]   tutorials, which will help me understand it in a better way
[00:39:47.880 --> 00:39:51.600]   and easy way. The same thing I encountered while I was trying
[00:39:51.600 --> 00:39:54.480]   to do this competition thing. So I had this blurry API, I had
[00:39:54.480 --> 00:39:58.000]   this adapt adapt an LP and that fast text. And I'm new to three
[00:39:58.000 --> 00:40:00.400]   of those. So I started understanding three of those.
[00:40:00.400 --> 00:40:03.640]   And then again, we have one layer PyTorch, we have fast AI.
[00:40:03.640 --> 00:40:06.480]   Now we have integration with another one, the blurry I, the
[00:40:06.480 --> 00:40:13.680]   set of blur API. So I had done a blog post on that, which is
[00:40:14.600 --> 00:40:19.600]   kind of let me share that right there, right here. Okay, so this
[00:40:19.600 --> 00:40:24.600]   blog post here, which is kind of how do you start with during
[00:40:24.600 --> 00:40:27.080]   the transform with the fast AI? How do you set up your
[00:40:27.080 --> 00:40:30.440]   environment? How do you have your necessary libraries
[00:40:30.440 --> 00:40:35.440]   necessary imports. And then to hear my my style of my I was
[00:40:35.440 --> 00:40:38.640]   thinking about writing just the blurry API thing, how does a
[00:40:38.640 --> 00:40:42.920]   blurry API work? And how does the functions work, but then I
[00:40:42.920 --> 00:40:46.440]   quickly realized if I am a new person to this particular blog
[00:40:46.440 --> 00:40:49.760]   portion and come back here, and let's say I'm going from fast AI
[00:40:49.760 --> 00:40:54.360]   environment, I don't understand what is the API? What How does
[00:40:54.360 --> 00:40:57.040]   it act on top of the fast AI? So I started this writing this
[00:40:57.040 --> 00:40:59.560]   blog post in a different way. So from here, the context went
[00:40:59.560 --> 00:41:03.560]   differently. So I wanted to highlight what's transformers,
[00:41:03.560 --> 00:41:07.480]   how do we use them, then how what is fast AI? How do how do
[00:41:07.480 --> 00:41:10.400]   I use it? If I'm using the blurry API on top of it, what
[00:41:10.400 --> 00:41:13.640]   steps I'm going to change. So when you see the when you see
[00:41:13.640 --> 00:41:17.000]   that, when you see the application, we see a mix of
[00:41:17.000 --> 00:41:20.360]   fast API and the blurry and the blurry API concepts because
[00:41:20.360 --> 00:41:23.840]   blurry page just one of the it's a it's a cream on top of the
[00:41:23.840 --> 00:41:27.520]   fast AI kind of thing, but it's really great in giving all that
[00:41:27.520 --> 00:41:33.120]   necessary things to us. So I started writing on. So about
[00:41:33.120 --> 00:41:37.000]   this one, so how does we take the highlights of fast AI
[00:41:37.000 --> 00:41:40.040]   understand hugging face differently? And then how do we
[00:41:40.040 --> 00:41:44.840]   have the same steps, we want to use it in the fast AI. And then
[00:41:44.840 --> 00:41:47.400]   if you're using a blurry API on top of it, how do I actually
[00:41:47.400 --> 00:41:51.280]   change that particular thing and how both of them differ. So I
[00:41:51.280 --> 00:41:57.040]   had each step. No, no explaining about what happens in blurry API
[00:41:57.040 --> 00:42:01.920]   what happens in the fast API, and then what is the similarity
[00:42:01.920 --> 00:42:05.120]   in a fast API, if I want to create a create a block, how do
[00:42:05.120 --> 00:42:07.920]   I do it? If I want to create a learner in blurry API, how do I
[00:42:07.920 --> 00:42:11.720]   do it? And then what steps are common to the fast API, even
[00:42:11.720 --> 00:42:15.120]   when I'm doing that, so not to just confused. So I had each
[00:42:15.120 --> 00:42:19.680]   step describing how do I do it and, and I linked all of those
[00:42:19.680 --> 00:42:22.800]   things, the walk with fast AI was a great resource. And I was
[00:42:22.800 --> 00:42:25.000]   understanding that particular competition thing, the blurry
[00:42:25.000 --> 00:42:27.800]   API page was really great. The documentation is really nice.
[00:42:27.800 --> 00:42:32.880]   And the chance to go through adapt NLP and fast x as well. I
[00:42:32.880 --> 00:42:35.680]   had another blog post I started writing on adapt NLP, but it's
[00:42:35.680 --> 00:42:38.800]   not incomplete. So I did not share that. But till here, it
[00:42:38.800 --> 00:42:42.240]   was a really great, amazing experience understanding both
[00:42:42.240 --> 00:42:45.960]   the API's and this competition really gave me this is the first
[00:42:45.960 --> 00:42:48.000]   competition participating and this competition really gave me
[00:42:48.000 --> 00:42:51.160]   insights into how do we understand things? How do we,
[00:42:51.160 --> 00:42:54.360]   you know, work on different kinds of things, because I
[00:42:54.360 --> 00:42:58.360]   submitted MRPC data set here, but I really had another one,
[00:42:58.360 --> 00:43:02.120]   which I was working on sst2. But it was lots of errors, I'm
[00:43:02.120 --> 00:43:04.640]   still debugging through it. So I'm trying to understand how to
[00:43:04.640 --> 00:43:08.640]   do it. So I will mix and matching the blurry API, also
[00:43:08.640 --> 00:43:13.160]   the walk with fast AI resources, we have some great resources
[00:43:13.160 --> 00:43:16.800]   there, fast API. So I was asking questions, and Zach was kind
[00:43:16.800 --> 00:43:21.760]   enough to answer in the in that particular discord. So it was a
[00:43:21.760 --> 00:43:24.720]   great experience. And thank you so much for selecting this blog
[00:43:24.720 --> 00:43:25.080]   post.
[00:43:25.080 --> 00:43:31.520]   Yeah, that's awesome. I mean, you have another gift for like
[00:43:31.520 --> 00:43:34.920]   writing blog posts, like they're super clear. And I think I put
[00:43:34.920 --> 00:43:38.720]   this in the chat. This is like a great resource for folks that
[00:43:38.720 --> 00:43:41.200]   are starting out with the SETI course and starting out with
[00:43:41.200 --> 00:43:44.240]   fast AI and Hugging Face, I would definitely send them to
[00:43:44.240 --> 00:43:48.600]   your blog, it summarize everything I tried to say over
[00:43:48.600 --> 00:43:53.560]   four hours, I guess in four weeks, and really succinct blog
[00:43:53.560 --> 00:43:57.680]   posts with good examples. So really well done. Thank you.
[00:43:58.680 --> 00:44:02.280]   What's a super consistent like, he has been studying and also
[00:44:02.280 --> 00:44:06.320]   sharing it every single week. Yeah, that's, that's really
[00:44:06.320 --> 00:44:09.160]   awesome. And it's great to see that he's been learning and also
[00:44:09.160 --> 00:44:09.680]   sharing it.
[00:44:09.680 --> 00:44:15.720]   And you know, what's awesome is that if you look at folks who
[00:44:15.720 --> 00:44:19.520]   share what they know, like, it's amazing how they just blow up in
[00:44:19.520 --> 00:44:23.440]   the community. I mean, Jeremy, why is he so like, well known
[00:44:23.440 --> 00:44:27.680]   is because he shares everything for free. Sanyam, where did he
[00:44:27.680 --> 00:44:29.920]   come from? Well, he shares everything. He's doing the he's
[00:44:29.920 --> 00:44:32.880]   doing the podcast, the blog posts, same as Zach, you know,
[00:44:32.880 --> 00:44:36.240]   he's real active on the forums and the walk with fast AI. So
[00:44:36.240 --> 00:44:39.200]   like, when you do these things, this is how you kind of like,
[00:44:39.200 --> 00:44:45.400]   blow up in a good way. And you get it creates opportunities for
[00:44:45.400 --> 00:44:49.720]   you know, in your career. It's fun to be able to help people
[00:44:49.720 --> 00:44:51.880]   and see other people come, you know, learn from you and
[00:44:51.880 --> 00:44:55.360]   actually develop their own skill sets. So it's really
[00:44:55.360 --> 00:44:58.240]   impressive. Yeah, really enjoyed it. Every week. I always saw
[00:44:58.240 --> 00:45:00.800]   your blog post and always made me feel good.
[00:45:00.800 --> 00:45:03.600]   Thank you. Thank you. Thank you. Thank you.
[00:45:03.600 --> 00:45:06.400]   I think
[00:45:06.400 --> 00:45:09.440]   was
[00:45:09.440 --> 00:45:15.600]   Oh, yeah, I I asked it, which was so having gone through the
[00:45:15.600 --> 00:45:18.520]   course, I know you're also going through the fast AI course, what
[00:45:18.520 --> 00:45:21.880]   would you say, like, is the absolute most important things
[00:45:21.880 --> 00:45:25.120]   like someone just starting should know about and like focus
[00:45:25.120 --> 00:45:27.080]   on just starting out?
[00:45:27.080 --> 00:45:31.320]   Okay, one thing I really
[00:45:31.320 --> 00:45:38.360]   was helpful for me is trying to understand one single thing at
[00:45:38.360 --> 00:45:41.880]   one single point of time not getting very overwhelmed. And I
[00:45:41.880 --> 00:45:45.240]   really like a line from Sanyam when we were on a call, we're in
[00:45:45.240 --> 00:45:47.280]   a different context talking about some computer person
[00:45:47.280 --> 00:45:50.200]   computer, but then he told me that you should avoid really
[00:45:50.200 --> 00:45:53.920]   avoid the rabbit holes because what I'm a software engineer, by
[00:45:53.920 --> 00:45:57.080]   nature is what I was trying to do was, I was trying to
[00:45:57.080 --> 00:45:59.680]   understand every single method going into the library in the
[00:45:59.680 --> 00:46:03.360]   fast AI, which like first two weeks, it tripped off me, I
[00:46:03.360 --> 00:46:06.320]   cannot get anything out of it. But then he told me that you
[00:46:06.320 --> 00:46:08.920]   should avoid the rabbit hole, keep the and I asked him the
[00:46:08.920 --> 00:46:11.600]   second question was, should I read papers? He said, not right
[00:46:11.600 --> 00:46:14.600]   now, it's not the time, it's your time to understand. So I was
[00:46:14.600 --> 00:46:17.280]   taking one single concept trying to understand how does it work
[00:46:17.280 --> 00:46:20.960]   the different variations. And then I try to experiment with it
[00:46:21.000 --> 00:46:24.240]   what works, what doesn't work, what would break with it, that
[00:46:24.240 --> 00:46:27.840]   really helped me and then writing about it made me more
[00:46:27.840 --> 00:46:31.320]   more concepts very, very clear for me that really helped me.
[00:46:31.320 --> 00:46:35.640]   And then understanding now, different things like hugging
[00:46:35.640 --> 00:46:39.800]   face fast AI, blur, adapt and repeat every single all at one
[00:46:39.800 --> 00:46:43.240]   time. Again, it's again, it's a big, you know, the big mixture
[00:46:43.240 --> 00:46:46.120]   of all those things. So again, it's overwhelming. So again, at
[00:46:46.120 --> 00:46:48.560]   the same time, I was also trying to understand one single thing
[00:46:48.600 --> 00:46:50.880]   at a time. That really helped me a lot.
[00:46:50.880 --> 00:46:53.800]   That's awesome. That's really great advice.
[00:46:53.800 --> 00:46:58.200]   Someone told me I'm not that smart. I was just passing.
[00:46:58.200 --> 00:47:04.160]   Yeah, I think I think Jeremy has said that a few times, right? In
[00:47:04.160 --> 00:47:06.640]   terms of their teaching approach. That that, by the way,
[00:47:06.640 --> 00:47:09.520]   is one thing that's drawn me to fast and why I recommend it to
[00:47:09.520 --> 00:47:13.400]   everybody is that having gone through the Andrew Ning course,
[00:47:13.400 --> 00:47:16.400]   at the end, I was like, Okay, now what do I do with all this
[00:47:16.400 --> 00:47:19.000]   stuff? I've learned how to calculate partial derivatives
[00:47:19.000 --> 00:47:22.280]   and do a real simple neural network. And then I found
[00:47:22.280 --> 00:47:26.720]   Jeremy's course and I was like, Oh my gosh, after the first hour
[00:47:26.720 --> 00:47:31.160]   I already have a CNN and I can do some image classification.
[00:47:31.160 --> 00:47:34.720]   I'm like, I could see how to actually use it. And so yeah,
[00:47:34.720 --> 00:47:39.320]   that's really good advice. Hard, hard for software engineers to
[00:47:39.320 --> 00:47:40.320]   do that, by the way.
[00:47:43.880 --> 00:47:48.880]   I was up till 4am. I think two nights ago trying to deploy a
[00:47:48.880 --> 00:47:52.920]   Docker multiple Docker containers to AWS. And I was
[00:47:52.920 --> 00:47:55.960]   like, I should just go to bed. But I felt so close. I just kept
[00:47:55.960 --> 00:48:00.400]   at it. And finally, at 4am it worked. I was exhausted. And I'm
[00:48:00.400 --> 00:48:03.080]   still recovering. So I know how that is.
[00:48:03.080 --> 00:48:06.560]   We've all been there when we find the bug and it's this
[00:48:06.560 --> 00:48:09.520]   evening and then you look out the window, the sun's rising.
[00:48:09.520 --> 00:48:10.360]   And you're like,
[00:48:10.360 --> 00:48:18.800]   So anyways, congratulations. We're going to move to the, the
[00:48:18.800 --> 00:48:24.200]   next competition. And the only one who submitted and actually
[00:48:24.200 --> 00:48:28.120]   got great results, even though it was on the MRP, MRPC data
[00:48:28.120 --> 00:48:31.800]   set, it was still really great. So I forgave you on doing the
[00:48:31.800 --> 00:48:35.120]   other data set. And I'm counting on you doing the SST2
[00:48:35.120 --> 00:48:37.520]   eventually, and you have to blog about it.
[00:48:37.680 --> 00:48:40.640]   But definitely. Thank you. Thank you so much.
[00:48:40.640 --> 00:48:46.200]   Congratulations. And thanks for submitting that. And then our
[00:48:46.200 --> 00:48:50.480]   final competition, most interesting application. Tassin,
[00:48:50.480 --> 00:48:53.400]   I put you down for this one for two prizes, because not only
[00:48:53.400 --> 00:48:58.840]   was it a beautiful article, but the application to Bengali is
[00:48:58.840 --> 00:49:01.560]   something like I thought was just amazing. And just the
[00:49:01.560 --> 00:49:05.200]   entire work that the interpretation work at the end,
[00:49:05.400 --> 00:49:09.440]   just really fantastic. So I hope you're okay with getting two
[00:49:09.440 --> 00:49:10.160]   prizes.
[00:49:10.160 --> 00:49:12.080]   Oh, totally.
[00:49:12.080 --> 00:49:15.840]   Great. And then also, oh, go ahead.
[00:49:15.840 --> 00:49:23.520]   Also, we have Hernan who actually this is a work in
[00:49:23.520 --> 00:49:28.320]   progress. But I'd love for him to spend some time presenting
[00:49:28.320 --> 00:49:31.360]   and talking about what he's working on. I have a link to his
[00:49:32.880 --> 00:49:35.960]   GitHub, but he's actually doing some really interesting stuff
[00:49:35.960 --> 00:49:38.640]   with applying fast AI to training transformers for
[00:49:38.640 --> 00:49:42.640]   Spanish. So I'm going to hand it over to him. I think he's on
[00:49:42.640 --> 00:49:47.600]   the call. So let me figure out how to stop my share.
[00:49:47.600 --> 00:49:52.680]   Hi.
[00:49:52.680 --> 00:49:54.400]   Hey, how's it going?
[00:49:54.400 --> 00:50:02.480]   Anyways, thank you for submitting your work. And yeah,
[00:50:02.480 --> 00:50:05.360]   that'd be great if you can maybe share what you're working on
[00:50:05.360 --> 00:50:08.960]   and and kind of like your overall like direction and
[00:50:08.960 --> 00:50:10.880]   objectives with your work.
[00:50:10.880 --> 00:50:21.520]   Yes, actually, at the very beginning. Okay. Let's see.
[00:50:22.960 --> 00:50:39.400]   Can share screen here. Okay. Can you see the screen?
[00:50:39.400 --> 00:50:41.680]   Yes, I can see it.
[00:50:49.520 --> 00:50:55.000]   Okay, well, my idea is just to, well, at first, I was a bit
[00:50:55.000 --> 00:51:00.120]   ambitious about the project. I knew that I couldn't be able to
[00:51:00.120 --> 00:51:07.960]   finish something that's a bit tough. So actually, reduce it.
[00:51:07.960 --> 00:51:16.240]   Okay, and I, my idea is that try to think about how people would
[00:51:16.680 --> 00:51:26.400]   try to figure NLP problems. So maybe with approach, trying to
[00:51:26.400 --> 00:51:34.880]   download a corpus and for instance, apply probably not
[00:51:34.880 --> 00:51:42.760]   classification, but maybe would look like a nomenclature of
[00:51:42.760 --> 00:51:54.600]   missions or even sentimentally analysis. So I started not by
[00:51:54.600 --> 00:52:01.000]   using the data sets that are in the hub, in, again, this hub,
[00:52:01.000 --> 00:52:06.800]   but another corpus that well, actually, that's an example
[00:52:06.800 --> 00:52:18.280]   that's a university in Spain. Using the same strategy, like
[00:52:18.280 --> 00:52:27.200]   FASTA-AI, that for the very beginning, start to see and work
[00:52:27.200 --> 00:52:34.240]   end to end, try to solve the problem end to end. And that's
[00:52:34.720 --> 00:52:39.840]   what I'm trying to do here. So the first thing is just build
[00:52:39.840 --> 00:52:52.360]   the data set, which is the same, the first notebook. Just to get
[00:52:52.360 --> 00:53:00.000]   a data set, which in this case, it's a multi-label, no, sorry,
[00:53:00.160 --> 00:53:12.640]   multi-class classification. And to be able just to download and
[00:53:12.640 --> 00:53:20.960]   to format the data set, the corpus, the next, the next
[00:53:20.960 --> 00:53:25.320]   notebook, it's just another one that it's just a simple one
[00:53:26.000 --> 00:53:38.080]   using the most, sorry, this one, the most simple, Blur, HiAPA,
[00:53:38.080 --> 00:53:49.400]   which is extremely easy to use. And using a pre-trained model in
[00:53:49.400 --> 00:53:58.120]   English, using this here, Coberta Bayes, and while using
[00:53:58.120 --> 00:54:09.160]   the Bilerner for sequence classification. So, well, it
[00:54:09.160 --> 00:54:18.320]   works quite well. Even though the model, it was pre-trained
[00:54:18.360 --> 00:54:32.240]   in English. Well, here is the, the, the results, the X score,
[00:54:32.240 --> 00:54:43.680]   this one score. And then I found another one model, pre-trained
[00:54:43.680 --> 00:54:51.840]   model. This one is an LPTown, which is multilingual. It's a
[00:54:51.840 --> 00:55:00.720]   bigger model, kind of twice in size, but it also is, but it was
[00:55:00.720 --> 00:55:10.080]   pre-trained in Spanish language. So, and well, training with
[00:55:10.320 --> 00:55:19.720]   three cycles. And here are the S1, the one scores. Ideally,
[00:55:19.720 --> 00:55:25.280]   what I wanted is to write notebooks that are
[00:55:25.280 --> 00:55:35.280]   incrementally more deeper. So, this one is using the Blur API,
[00:55:35.320 --> 00:55:43.120]   the high level API, and then use the Bilerner API. And the last
[00:55:43.120 --> 00:55:51.720]   one is using the FATTIR learner API, the simplest, the standard
[00:55:51.720 --> 00:56:01.160]   that we know, or the FATTIR course, which is this one. Well,
[00:56:01.160 --> 00:56:06.720]   that was my idea that I need to keep working on it.
[00:56:06.720 --> 00:56:12.160]   Yeah, that's awesome. You know what I like about your, your
[00:56:12.160 --> 00:56:15.160]   notebooks, Hernan, is that you really start at the very
[00:56:15.160 --> 00:56:18.040]   beginning, which is, we typically, you know, just
[00:56:18.040 --> 00:56:20.920]   download a data set. And that's a really challenging thing to
[00:56:20.920 --> 00:56:24.000]   actually create a data set, something that you can actually
[00:56:24.000 --> 00:56:27.880]   use from something where you can't just download it from
[00:56:27.880 --> 00:56:31.200]   Hugging Face or from, you know, somewhere else. So, I love the
[00:56:31.200 --> 00:56:34.760]   whole process that you start with the data set creation, and
[00:56:34.760 --> 00:56:38.640]   then show how to work with it with both the low level, mid
[00:56:38.640 --> 00:56:42.880]   level API, as well as the high level API. The high level API
[00:56:42.880 --> 00:56:45.760]   makes it simple to do these things. But you really start to
[00:56:45.760 --> 00:56:49.800]   understand what's happening when you start, you know, diving down
[00:56:49.800 --> 00:56:54.080]   to the lower bits and seeing what's going on. And it's
[00:56:54.080 --> 00:56:56.560]   amazing, because you start seeing how concepts of like
[00:56:56.560 --> 00:56:59.720]   callbacks and like that, how the learner works and the learning
[00:56:59.720 --> 00:57:02.920]   rate finder. Once you understand those concepts, all of a sudden
[00:57:02.920 --> 00:57:05.920]   everything starts clicking. And you can apply that to things
[00:57:05.920 --> 00:57:08.560]   beyond NLP, like working with vision, you're like, oh, yeah,
[00:57:08.560 --> 00:57:10.720]   there's callbacks right there. I see what's happening. I see how
[00:57:10.720 --> 00:57:15.320]   that's altering the tensors and whatnot. So I was really
[00:57:15.320 --> 00:57:19.320]   I hope you keep it on your progress.
[00:57:20.000 --> 00:57:24.360]   Yeah, yes, I try. Yeah.
[00:57:24.360 --> 00:57:30.440]   Thanks. Thanks for sharing this. I think Jeremy said this or
[00:57:30.440 --> 00:57:34.000]   someone else said this, that with languages that most of the
[00:57:34.000 --> 00:57:37.400]   researchers aren't looking for, and a language that you speak
[00:57:37.400 --> 00:57:42.000]   without any technique chance that are state of the art. So I
[00:57:42.000 --> 00:57:44.680]   know a lot of interesting work will come from all of you. So I
[00:57:44.680 --> 00:57:47.960]   really look forward to that. It was like really interesting.
[00:57:47.960 --> 00:57:50.600]   It's a collection of all of the notes. So thanks. Thanks for
[00:57:50.600 --> 00:57:51.480]   putting that together.
[00:57:51.480 --> 00:57:53.600]   Awesome.
[00:57:53.600 --> 00:58:03.760]   All right, I'm gonna share my screen for some final words
[00:58:03.760 --> 00:58:07.440]   here. Find it. There we go.
[00:58:07.440 --> 00:58:13.320]   Anyways, thank you so much to everybody who submitted their
[00:58:13.320 --> 00:58:18.000]   work and for being willing to come online and present really
[00:58:18.000 --> 00:58:22.320]   awesome stuff. And really encourage folks to go back and
[00:58:22.320 --> 00:58:29.400]   read those posts and ask questions and get involved. So
[00:58:29.400 --> 00:58:34.400]   as we end with this study group, and others will probably do
[00:58:34.400 --> 00:58:39.040]   another one if folks want to once the second series from
[00:58:39.040 --> 00:58:42.680]   hugging face comes out, but there's always homework. So
[00:58:42.680 --> 00:58:46.360]   really encourage folks to watch the official course videos and
[00:58:46.360 --> 00:58:48.960]   toss in one thing like member in the interpretation where you saw
[00:58:48.960 --> 00:58:52.640]   that label zero and label underscore one. There's
[00:58:52.640 --> 00:58:56.120]   actually a way to fix that. And I think Sylvan covers that in
[00:58:56.120 --> 00:59:01.080]   session four, you can actually update the convict file. So so
[00:59:01.080 --> 00:59:03.400]   take a look at that. And I think that will you'll be able to fix
[00:59:03.400 --> 00:59:08.200]   that issue. So you'll see the actual labels. So yeah,
[00:59:08.200 --> 00:59:09.880]   definitely watch the course videos. There's a lot of good
[00:59:09.880 --> 00:59:12.720]   content over there. And they're fun to watch, because they're
[00:59:12.720 --> 00:59:16.160]   very succinct, like three to five minutes, mostly so really
[00:59:16.160 --> 00:59:20.000]   easy to, to go through. And, and of course, those are the folks
[00:59:20.000 --> 00:59:22.680]   that are working at hugging face. So good people to learn
[00:59:22.680 --> 00:59:27.560]   from. The other thing, as I think was demonstrated today is
[00:59:27.560 --> 00:59:33.080]   blogging. Try to be Ravi, see if you can go ahead and surpass
[00:59:33.080 --> 00:59:36.320]   him on blogging. He's pretty active. So that's my challenge
[00:59:36.320 --> 00:59:40.840]   to all of you. But blog about things in particular questions,
[00:59:40.840 --> 00:59:43.120]   difficulties that you've had along the way, things that were
[00:59:43.120 --> 00:59:46.000]   hard to understand, and spend some time really understanding
[00:59:46.000 --> 00:59:48.960]   it and explaining it to others. Because once you're able to
[00:59:48.960 --> 00:59:51.640]   explain those things to others, then you know that you really
[00:59:51.640 --> 00:59:54.360]   understand them. And you're going to help people maybe save
[00:59:54.360 --> 00:59:58.600]   hours, you know, struggling with things that that you had to
[00:59:58.600 --> 01:00:01.440]   struggle with. So definitely encourage folks to keep
[01:00:01.440 --> 01:00:05.760]   blogging. Enter a Kaggle competition. There's always NLP
[01:00:05.760 --> 01:00:08.120]   ones popping up. And there's I think there's ones in the
[01:00:08.120 --> 01:00:13.400]   playground. Try taking some of this stuff use adapt NLP, fast
[01:00:13.400 --> 01:00:17.040]   tugs, blur, use all of them or one of them and kind of see how
[01:00:17.040 --> 01:00:21.280]   that goes. You learn a lot by competing and doing that. And
[01:00:21.280 --> 01:00:23.920]   then help out folks on the discord forums. We're going to
[01:00:23.920 --> 01:00:27.920]   keep that open. So I hope folks will be active. And along the
[01:00:27.920 --> 01:00:31.160]   way read a paper or two and a toss and recommended the fast AI
[01:00:31.160 --> 01:00:35.040]   paper, which is a good one easy to read and really explains how
[01:00:35.040 --> 01:00:38.960]   fast they add like the the design objectives, and how
[01:00:38.960 --> 01:00:41.960]   things work from the low level to the high level API. So that
[01:00:41.960 --> 01:00:45.200]   might be a good one to start with. But yeah, try to read some
[01:00:45.200 --> 01:00:49.960]   of the papers. And I think that is it. And thank you for joining
[01:00:49.960 --> 01:00:53.160]   I've had a lot of fun. Appreciate the weights and biases folks
[01:00:53.160 --> 01:00:58.080]   host hosting all this and Sonia for his expert hosting and
[01:00:58.080 --> 01:01:02.720]   handling of the q&a and the zoom bombing. So really appreciate
[01:01:02.720 --> 01:01:06.800]   everybody. And if there's any final q&a or things folks want
[01:01:06.800 --> 01:01:09.640]   to talk about. I'll go ahead and open it up.
[01:01:09.640 --> 01:01:15.800]   Thank you. Thank you so much for this opportunity. Everyone who
[01:01:15.800 --> 01:01:21.160]   really helped us on the I know Sonia, it's, I think 1135pm for
[01:01:21.160 --> 01:01:24.640]   him. It's great to see all of these guys joining here. Thanks
[01:01:24.640 --> 01:01:27.200]   very much. I think we're encouragement and thank you all
[01:01:27.200 --> 01:01:32.920]   the folks from Andrea Morgan, everyone from Zack. Otto, Otto
[01:01:32.920 --> 01:01:35.600]   projects. Thank you so much, everyone.
[01:01:35.600 --> 01:01:37.480]   Yeah, awesome. Thanks.
[01:01:37.480 --> 01:01:42.880]   I have three things to say. So let's please join me in
[01:01:42.880 --> 01:01:46.000]   thanking Andrea. I know like most of us at least we are the
[01:01:46.000 --> 01:01:49.880]   guys talking on zoom but really Andrea is making sure that and I
[01:01:49.880 --> 01:01:52.720]   know you all see her on the zoom call. She's like always behind
[01:01:52.720 --> 01:01:55.200]   the scenes, but she's making sure that nothing goes out. So
[01:01:55.480 --> 01:01:58.080]   we really need to thank her. We also need to thank you. I'm just
[01:01:58.080 --> 01:02:02.760]   like helping coordinate q&a not doing a lot. It's been the start
[01:02:02.760 --> 01:02:06.200]   of the show. So all the thanks to me. And the other other thing
[01:02:06.200 --> 01:02:09.440]   I really want to highlight is that this has been a cycle of
[01:02:09.440 --> 01:02:13.120]   learning. I remember being was leading the previous groups
[01:02:13.120 --> 01:02:16.920]   where we were just like discussing the lectures. Now
[01:02:16.920 --> 01:02:19.240]   we're at the point where we should be building stuff. So
[01:02:19.240 --> 01:02:22.120]   it's like really interesting to see as a group also, we're not
[01:02:22.120 --> 01:02:24.760]   just talking about daily was the lecture, do you have questions
[01:02:24.760 --> 01:02:28.240]   from the lecture? No, no, no, no. What did you build? Show me
[01:02:28.240 --> 01:02:31.240]   that I'll ask you questions on that. And I'll make sure others
[01:02:31.240 --> 01:02:34.680]   do it. So that's, that's really cool to see. And like, I really
[01:02:34.680 --> 01:02:37.640]   want to be a part of those study groups and help those. So the
[01:02:37.640 --> 01:02:41.240]   third announcement I would have for everyone I've joined by
[01:02:41.240 --> 01:02:44.400]   since and my responsibility, literally my job
[01:02:44.400 --> 01:02:47.840]   responsibility is make sure more of this happens. So please keep
[01:02:47.840 --> 01:02:51.760]   an eye out on our Twitter like, I'll probably start a few more
[01:02:51.760 --> 01:02:55.880]   study groups around these topics. The goal is only that
[01:02:55.880 --> 01:03:00.880]   all of us know that there's no other world. So just to see
[01:03:00.880 --> 01:03:04.440]   comments, and of course, thanks everyone for sharing. It's been
[01:03:04.440 --> 01:03:07.200]   awesome to just be in this Google class.
[01:03:07.200 --> 01:03:11.520]   And this is great. This is one of the nice things with the
[01:03:11.520 --> 01:03:15.000]   pandemic. We're used to on zoom. I mean, these things wouldn't be
[01:03:15.000 --> 01:03:17.880]   in would be impossible in person. We got people from all
[01:03:17.880 --> 01:03:21.200]   over the world, which is really fascinating. I tell my wife, I
[01:03:21.200 --> 01:03:25.800]   go, yeah, I'm talking to some people in India, Spain, you
[01:03:25.800 --> 01:03:28.760]   know, United States down there, South America. She's like, what
[01:03:28.760 --> 01:03:32.000]   I go, yeah, I said, there are people from all over the world
[01:03:32.000 --> 01:03:35.320]   that are involved in the study group. And it makes it like just
[01:03:35.320 --> 01:03:38.080]   so much more exciting. So yeah, thank you. It's been a lot of
[01:03:38.080 --> 01:03:41.800]   fun for me. I've learned a lot. And hopefully, hopefully I'll be
[01:03:41.800 --> 01:03:45.360]   back for the second part. So we'll stay tuned.
[01:03:45.360 --> 01:03:52.160]   Awesome.
[01:03:52.160 --> 01:03:57.040]   Let us hear from us, our states and buses. In case you haven't
[01:03:57.040 --> 01:04:01.920]   booked. Thanks, thanks everyone for joining. We have more study
[01:04:01.920 --> 01:04:04.200]   groups. I won't say bye. But I'll see you soon. That's what
[01:04:04.200 --> 01:04:04.560]   I'm saying.
[01:04:04.560 --> 01:04:07.760]   All right. Thank you.

