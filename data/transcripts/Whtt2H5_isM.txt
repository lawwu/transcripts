
[00:00:00.000 --> 00:00:03.040]   The following is a conversation with David Ferrucci.
[00:00:03.040 --> 00:00:05.200]   He led the team that built Watson,
[00:00:05.200 --> 00:00:07.040]   the IBM question answering system
[00:00:07.040 --> 00:00:09.080]   that beat the top humans in the world
[00:00:09.080 --> 00:00:11.160]   at the game of Jeopardy.
[00:00:11.160 --> 00:00:12.920]   For spending a couple hours with David,
[00:00:12.920 --> 00:00:14.960]   I saw a genuine passion,
[00:00:14.960 --> 00:00:17.740]   not only for abstract understanding of intelligence,
[00:00:17.740 --> 00:00:21.240]   but for engineering it to solve real world problems
[00:00:21.240 --> 00:00:24.800]   under real world deadlines and resource constraints.
[00:00:24.800 --> 00:00:26.520]   Where science meets engineering
[00:00:26.520 --> 00:00:29.960]   is where brilliant, simple ingenuity emerges.
[00:00:29.960 --> 00:00:32.120]   People who work at joining it to
[00:00:32.120 --> 00:00:33.840]   have a lot of wisdom earned
[00:00:33.840 --> 00:00:37.000]   through failures and eventual success.
[00:00:37.000 --> 00:00:39.100]   David is also the founder, CEO,
[00:00:39.100 --> 00:00:41.680]   and chief scientist of Elemental Cognition,
[00:00:41.680 --> 00:00:44.500]   a company working to engineer AI systems
[00:00:44.500 --> 00:00:47.440]   that understand the world the way people do.
[00:00:47.440 --> 00:00:50.300]   This is the Artificial Intelligence Podcast.
[00:00:50.300 --> 00:00:52.740]   If you enjoy it, subscribe on YouTube,
[00:00:52.740 --> 00:00:54.460]   give it five stars on iTunes,
[00:00:54.460 --> 00:00:55.880]   support it on Patreon,
[00:00:55.880 --> 00:00:57.960]   or simply connect with me on Twitter
[00:00:57.960 --> 00:01:01.400]   at Lex Friedman, spelled F-R-I-D-M-A-N.
[00:01:01.400 --> 00:01:05.200]   And now, here's my conversation with David Ferrucci.
[00:01:05.200 --> 00:01:08.040]   Your undergrad was in biology
[00:01:08.040 --> 00:01:11.320]   with an eye toward medical school
[00:01:11.320 --> 00:01:14.360]   before you went on for the PhD in computer science.
[00:01:14.360 --> 00:01:16.840]   So let me ask you an easy question.
[00:01:16.840 --> 00:01:20.560]   What is the difference between biological systems
[00:01:20.560 --> 00:01:21.880]   and computer systems?
[00:01:21.880 --> 00:01:26.320]   When you sit back, look at the stars,
[00:01:26.320 --> 00:01:27.620]   and think philosophically.
[00:01:28.580 --> 00:01:30.820]   - I often wonder whether or not
[00:01:30.820 --> 00:01:32.900]   there is a substantive difference.
[00:01:32.900 --> 00:01:34.480]   I mean, I think the thing that got me
[00:01:34.480 --> 00:01:35.980]   into computer science,
[00:01:35.980 --> 00:01:37.220]   into artificial intelligence,
[00:01:37.220 --> 00:01:40.980]   was exactly this presupposition that
[00:01:40.980 --> 00:01:44.340]   if we can get machines to think,
[00:01:44.340 --> 00:01:45.620]   or I should say this question,
[00:01:45.620 --> 00:01:47.460]   this philosophical question,
[00:01:47.460 --> 00:01:50.580]   if we can get machines to think,
[00:01:50.580 --> 00:01:54.800]   to understand, to process information the way we do,
[00:01:54.800 --> 00:01:56.460]   so if we can describe a procedure,
[00:01:56.460 --> 00:01:57.960]   describe a process,
[00:01:57.960 --> 00:02:02.500]   even if that process were the intelligence process itself,
[00:02:02.500 --> 00:02:05.280]   then what would be the difference?
[00:02:05.280 --> 00:02:07.680]   So from a philosophical standpoint,
[00:02:07.680 --> 00:02:11.660]   I'm not sure I'm convinced that there is.
[00:02:11.660 --> 00:02:14.960]   I mean, you can go in the direction of spirituality,
[00:02:14.960 --> 00:02:16.660]   you can go in the direction of the soul,
[00:02:16.660 --> 00:02:21.660]   but in terms of what we can experience
[00:02:22.040 --> 00:02:26.000]   from an intellectual and physical perspective,
[00:02:26.000 --> 00:02:27.500]   I'm not sure there is.
[00:02:27.500 --> 00:02:31.120]   Clearly, there are different implementations,
[00:02:31.120 --> 00:02:33.240]   but if you were to say,
[00:02:33.240 --> 00:02:36.200]   is a biological information processing system
[00:02:36.200 --> 00:02:38.460]   fundamentally more capable
[00:02:38.460 --> 00:02:41.040]   than one we might be able to build out of silicon
[00:02:41.040 --> 00:02:43.900]   or some other substrate,
[00:02:43.900 --> 00:02:46.560]   I don't know that there is.
[00:02:46.560 --> 00:02:50.600]   - How distant do you think is the biological implementation?
[00:02:50.600 --> 00:02:53.840]   So fundamentally, they may have the same capabilities,
[00:02:53.840 --> 00:02:58.300]   but is it really a far mystery
[00:02:58.300 --> 00:03:00.720]   where a huge number of breakthroughs are needed
[00:03:00.720 --> 00:03:02.720]   to be able to understand it,
[00:03:02.720 --> 00:03:06.320]   or is it something that, for the most part,
[00:03:06.320 --> 00:03:08.680]   in the important aspects,
[00:03:08.680 --> 00:03:11.160]   echoes of the same kind of characteristics?
[00:03:11.160 --> 00:03:12.840]   - Yeah, that's interesting.
[00:03:12.840 --> 00:03:16.560]   So your question presupposes that there's this goal
[00:03:16.560 --> 00:03:20.920]   to recreate what we perceive as biological intelligence.
[00:03:20.920 --> 00:03:25.920]   I'm not sure that's how I would state the goal.
[00:03:25.920 --> 00:03:27.440]   I mean, I think that's--
[00:03:27.440 --> 00:03:29.240]   - What is the goal?
[00:03:29.240 --> 00:03:32.200]   - Good, so I think there are a few goals.
[00:03:32.200 --> 00:03:35.760]   I think that understanding the human brain
[00:03:35.760 --> 00:03:38.560]   and how it works is important
[00:03:38.560 --> 00:03:43.560]   for us to be able to diagnose and treat issues
[00:03:44.760 --> 00:03:48.240]   for us to understand our own strengths and weaknesses,
[00:03:48.240 --> 00:03:52.440]   both intellectual, psychological, and physical.
[00:03:52.440 --> 00:03:55.000]   So neuroscience and understanding the brain
[00:03:55.000 --> 00:03:59.560]   from that perspective, there's a clear goal there.
[00:03:59.560 --> 00:04:01.160]   From the perspective of saying,
[00:04:01.160 --> 00:04:04.800]   I wanna mimic human intelligence,
[00:04:04.800 --> 00:04:06.440]   that one's a little bit more interesting.
[00:04:06.440 --> 00:04:10.480]   Human intelligence certainly has a lot of things we envy.
[00:04:10.480 --> 00:04:12.880]   It's also got a lot of problems, too.
[00:04:12.880 --> 00:04:16.680]   So I think we're capable of sort of stepping back
[00:04:16.680 --> 00:04:21.680]   and saying, what do we want out of an intelligence?
[00:04:21.680 --> 00:04:24.400]   How do we wanna communicate with that intelligence?
[00:04:24.400 --> 00:04:25.560]   How do we want it to behave?
[00:04:25.560 --> 00:04:27.460]   How do we want it to perform?
[00:04:27.460 --> 00:04:30.360]   Now, of course, it's somewhat of an interesting argument
[00:04:30.360 --> 00:04:33.940]   because I'm sitting here as a human with a biological brain
[00:04:33.940 --> 00:04:36.440]   and I'm critiquing the strengths and weaknesses
[00:04:36.440 --> 00:04:39.080]   of human intelligence and saying that we have
[00:04:39.080 --> 00:04:42.560]   the capacity to step back and say,
[00:04:42.560 --> 00:04:45.120]   gee, what is intelligence and what do we really want
[00:04:45.120 --> 00:04:48.100]   out of it, and that in and of itself suggests
[00:04:48.100 --> 00:04:52.100]   that human intelligence is something quite enviable,
[00:04:52.100 --> 00:04:57.100]   that it can introspect that way.
[00:04:57.100 --> 00:05:00.320]   - And the flaws, you mentioned the flaws.
[00:05:00.320 --> 00:05:01.160]   So humans have flaws.
[00:05:01.160 --> 00:05:04.720]   - Yeah, I think that flaws that human intelligence has
[00:05:04.720 --> 00:05:08.440]   is extremely prejudicial and biased
[00:05:08.440 --> 00:05:10.480]   in the way it draws many inferences.
[00:05:10.480 --> 00:05:12.040]   - Do you think those are, sorry to interrupt,
[00:05:12.040 --> 00:05:14.400]   do you think those are features or are those bugs?
[00:05:14.400 --> 00:05:19.400]   Do you think the prejudice, the forgetfulness, the fear,
[00:05:19.400 --> 00:05:22.900]   what are the flaws?
[00:05:22.900 --> 00:05:23.740]   List them all.
[00:05:23.740 --> 00:05:24.560]   What, love?
[00:05:24.560 --> 00:05:25.600]   Maybe that's a flaw.
[00:05:25.600 --> 00:05:28.160]   You think those are all things that can be,
[00:05:28.160 --> 00:05:30.820]   get in the way of intelligence
[00:05:30.820 --> 00:05:33.440]   or the essential components of intelligence?
[00:05:33.440 --> 00:05:36.200]   - Well, again, if you go back and you define intelligence
[00:05:36.200 --> 00:05:41.200]   as being able to sort of accurately, precisely,
[00:05:41.240 --> 00:05:43.840]   rigorously reason, develop answers,
[00:05:43.840 --> 00:05:46.640]   and justify those answers in an objective way,
[00:05:46.640 --> 00:05:49.720]   yeah, then human intelligence has these flaws
[00:05:49.720 --> 00:05:52.880]   in that it tends to be more influenced
[00:05:52.880 --> 00:05:55.180]   by some of the things you said.
[00:05:55.180 --> 00:05:59.740]   And it's largely an inductive process,
[00:05:59.740 --> 00:06:03.560]   meaning it takes past data, uses that to predict the future,
[00:06:03.560 --> 00:06:06.000]   very advantageous in some cases,
[00:06:06.000 --> 00:06:09.280]   but fundamentally biased and prejudicial in other cases,
[00:06:09.280 --> 00:06:10.760]   'cause it's gonna be strongly influenced
[00:06:10.760 --> 00:06:13.880]   by its priors, whether they're right or wrong
[00:06:13.880 --> 00:06:17.420]   from some objective reasoning perspective,
[00:06:17.420 --> 00:06:20.520]   you're gonna favor them because those are the decisions
[00:06:20.520 --> 00:06:24.240]   or those are the paths that succeeded in the past.
[00:06:24.240 --> 00:06:29.240]   And I think that mode of intelligence makes a lot of sense
[00:06:29.240 --> 00:06:33.680]   for when your primary goal is to act quickly
[00:06:33.680 --> 00:06:37.240]   and survive and make fast decisions.
[00:06:37.240 --> 00:06:39.400]   And I think those create problems
[00:06:40.360 --> 00:06:42.080]   when you wanna think more deeply
[00:06:42.080 --> 00:06:45.160]   and make more objective and reasoned decisions.
[00:06:45.160 --> 00:06:48.400]   Of course, human's capable of doing both.
[00:06:48.400 --> 00:06:51.120]   They do sort of one more naturally than they do the other,
[00:06:51.120 --> 00:06:53.320]   but they're capable of doing both.
[00:06:53.320 --> 00:06:54.560]   - You're saying they do the one
[00:06:54.560 --> 00:06:56.520]   that responds quickly more naturally.
[00:06:56.520 --> 00:06:57.360]   - Right.
[00:06:57.360 --> 00:06:58.460]   - 'Cause that's the thing we kinda need
[00:06:58.460 --> 00:07:02.760]   to not be eaten by the predators in the world.
[00:07:02.760 --> 00:07:07.760]   - For example, but then we've learned to reason
[00:07:08.960 --> 00:07:11.240]   through logic, we've developed science,
[00:07:11.240 --> 00:07:13.020]   we've trained people to do that.
[00:07:13.020 --> 00:07:17.000]   I think that's harder for the individual to do.
[00:07:17.000 --> 00:07:21.000]   I think it requires training and teaching.
[00:07:21.000 --> 00:07:24.220]   I think we are, the human mind certainly is capable of it,
[00:07:24.220 --> 00:07:25.320]   but we find it more difficult.
[00:07:25.320 --> 00:07:27.720]   And then there are other weaknesses, if you will,
[00:07:27.720 --> 00:07:30.660]   as you mentioned earlier, just memory capacity
[00:07:30.660 --> 00:07:35.660]   and how many chains of inference can you actually go through
[00:07:35.780 --> 00:07:39.260]   without losing your way, so just focus.
[00:07:39.260 --> 00:07:43.300]   - So the way you think about intelligence,
[00:07:43.300 --> 00:07:45.100]   and we're really sort of floating
[00:07:45.100 --> 00:07:47.260]   in this philosophical space,
[00:07:47.260 --> 00:07:51.140]   but I think you're the perfect person to talk about this
[00:07:51.140 --> 00:07:55.700]   because we'll get to Jeopardy and beyond.
[00:07:55.700 --> 00:07:59.500]   That's one of the most incredible accomplishments in AI,
[00:07:59.500 --> 00:08:00.940]   in the history of AI,
[00:08:00.940 --> 00:08:03.440]   but hence the philosophical discussion.
[00:08:03.440 --> 00:08:06.300]   So let me ask, you've kind of alluded to it,
[00:08:06.300 --> 00:08:09.420]   but let me ask again, what is intelligence?
[00:08:09.420 --> 00:08:12.500]   Underlying the discussion we'll have
[00:08:12.500 --> 00:08:15.540]   with Jeopardy and beyond.
[00:08:15.540 --> 00:08:17.140]   How do you think about intelligence?
[00:08:17.140 --> 00:08:19.820]   Is it a sufficiently complicated problem,
[00:08:19.820 --> 00:08:22.500]   being able to reason your way through solving that problem?
[00:08:22.500 --> 00:08:23.820]   Is that kind of how you think about
[00:08:23.820 --> 00:08:25.460]   what it means to be intelligent?
[00:08:25.460 --> 00:08:29.700]   - So I think of intelligence primarily two ways.
[00:08:29.700 --> 00:08:33.320]   One is the ability to predict.
[00:08:33.320 --> 00:08:35.820]   So in other words, if I have a problem,
[00:08:35.820 --> 00:08:37.620]   can I predict what's gonna happen next,
[00:08:37.620 --> 00:08:40.900]   whether it's to predict the answer of a question
[00:08:40.900 --> 00:08:43.900]   or to say, look, I'm looking at all the market dynamics
[00:08:43.900 --> 00:08:46.160]   and I'm gonna tell you what's gonna happen next,
[00:08:46.160 --> 00:08:49.380]   or you're in a room and somebody walks in
[00:08:49.380 --> 00:08:51.340]   and you're gonna predict what they're gonna do next
[00:08:51.340 --> 00:08:52.900]   or what they're gonna say next.
[00:08:52.900 --> 00:08:56.540]   - In a highly dynamic environment full of uncertainty,
[00:08:56.540 --> 00:08:58.620]   be able to predict.
[00:08:58.620 --> 00:09:01.500]   - The more variables, the more complex.
[00:09:01.500 --> 00:09:04.080]   The more possibilities, the more complex.
[00:09:04.080 --> 00:09:07.720]   But can I take a small amount of prior data
[00:09:07.720 --> 00:09:09.880]   and learn the pattern and then predict
[00:09:09.880 --> 00:09:13.000]   what's gonna happen next accurately and consistently?
[00:09:13.000 --> 00:09:16.960]   That's certainly a form of intelligence.
[00:09:16.960 --> 00:09:18.320]   - What do you need for that, by the way?
[00:09:18.320 --> 00:09:22.880]   You need to have an understanding of the way the world works
[00:09:22.880 --> 00:09:25.320]   in order to be able to unroll it into the future.
[00:09:25.320 --> 00:09:28.040]   What do you think is needed to predict?
[00:09:28.040 --> 00:09:29.480]   - Depends what you mean by understanding.
[00:09:30.380 --> 00:09:32.260]   I need to be able to find that function.
[00:09:32.260 --> 00:09:35.140]   This is very much what deep learning does,
[00:09:35.140 --> 00:09:39.000]   machine learning does, is if you give me enough prior data
[00:09:39.000 --> 00:09:41.980]   and you tell me what the output variable is that matters,
[00:09:41.980 --> 00:09:44.500]   I'm gonna sit there and be able to predict it.
[00:09:44.500 --> 00:09:47.300]   And if I can predict it accurately
[00:09:47.300 --> 00:09:50.980]   so that I can get it right more often than not, I'm smart.
[00:09:50.980 --> 00:09:54.820]   If I can do that with less data and less training time,
[00:09:54.820 --> 00:09:56.020]   I'm even smarter.
[00:09:58.020 --> 00:10:00.620]   If I can figure out what's even worth predicting,
[00:10:00.620 --> 00:10:03.920]   I'm smarter, meaning I'm figuring out
[00:10:03.920 --> 00:10:06.380]   what path is gonna get me toward a goal.
[00:10:06.380 --> 00:10:07.580]   - What about picking a goal?
[00:10:07.580 --> 00:10:08.500]   Sorry, you're up again.
[00:10:08.500 --> 00:10:10.100]   - Well, that's interesting about picking a goal.
[00:10:10.100 --> 00:10:11.060]   Sort of an interesting thing.
[00:10:11.060 --> 00:10:13.220]   I think that's where you bring in
[00:10:13.220 --> 00:10:15.020]   what are you pre-programmed to do?
[00:10:15.020 --> 00:10:17.020]   We talk about humans and, well,
[00:10:17.020 --> 00:10:19.380]   humans are pre-programmed to survive.
[00:10:19.380 --> 00:10:23.320]   So sort of their primary driving goal,
[00:10:23.320 --> 00:10:24.700]   what do they have to do to do that?
[00:10:24.700 --> 00:10:27.380]   And that can be very complex, right?
[00:10:27.380 --> 00:10:30.820]   So it's not just figuring out
[00:10:30.820 --> 00:10:33.640]   that you need to run away from the ferocious tiger,
[00:10:33.640 --> 00:10:38.640]   but we survive in a social context as an example.
[00:10:38.640 --> 00:10:42.300]   So understanding the subtleties of social dynamics
[00:10:42.300 --> 00:10:45.420]   becomes something that's important for surviving,
[00:10:45.420 --> 00:10:47.180]   finding a mate, reproducing, right?
[00:10:47.180 --> 00:10:49.360]   So we're continually challenged
[00:10:49.360 --> 00:10:53.760]   with complex sets of variables, complex constraints,
[00:10:53.760 --> 00:10:56.880]   rules, if you will, or patterns.
[00:10:56.880 --> 00:10:59.340]   And we learn how to find the functions
[00:10:59.340 --> 00:11:00.700]   and predict the things.
[00:11:00.700 --> 00:11:03.580]   In other words, represent those patterns efficiently
[00:11:03.580 --> 00:11:04.940]   and be able to predict what's gonna happen.
[00:11:04.940 --> 00:11:06.060]   And that's a form of intelligence.
[00:11:06.060 --> 00:11:11.060]   That doesn't really require anything specific
[00:11:11.060 --> 00:11:13.380]   other than the ability to find that function
[00:11:13.380 --> 00:11:15.860]   and predict that right answer.
[00:11:15.860 --> 00:11:18.460]   It's certainly a form of intelligence.
[00:11:18.460 --> 00:11:23.300]   But then when we say, well, do we understand each other?
[00:11:23.300 --> 00:11:28.300]   In other words, would you perceive me as intelligent
[00:11:28.300 --> 00:11:31.040]   beyond that ability to predict?
[00:11:31.040 --> 00:11:35.200]   So now I can predict, but I can't really articulate
[00:11:35.200 --> 00:11:37.840]   how I'm going through that process,
[00:11:37.840 --> 00:11:41.240]   what my underlying theory is for predicting.
[00:11:41.240 --> 00:11:43.680]   And I can't get you to understand what I'm doing
[00:11:43.680 --> 00:11:45.660]   so that you can follow,
[00:11:45.660 --> 00:11:48.080]   you can figure out how to do this yourself
[00:11:48.080 --> 00:11:50.800]   if you did not have, for example,
[00:11:50.800 --> 00:11:53.860]   the right pattern-managing machinery that I did.
[00:11:53.860 --> 00:11:55.780]   And now we potentially have this breakdown
[00:11:55.780 --> 00:11:59.120]   where in effect, I'm intelligent,
[00:11:59.120 --> 00:12:02.660]   but I'm sort of an alien intelligence relative to you.
[00:12:02.660 --> 00:12:05.060]   - You're intelligent, but nobody knows about it.
[00:12:05.060 --> 00:12:08.660]   Or I can't-- - Well, I can see the output.
[00:12:08.660 --> 00:12:11.700]   - So you're saying, let's sort of separate the two things.
[00:12:11.700 --> 00:12:15.940]   One is you explaining why you were able
[00:12:15.940 --> 00:12:17.420]   to predict the future.
[00:12:17.420 --> 00:12:22.420]   And the second is me being able to,
[00:12:22.420 --> 00:12:25.560]   impressing me that you're intelligent.
[00:12:25.560 --> 00:12:26.560]   Me being able to know
[00:12:26.560 --> 00:12:28.720]   that you successfully predicted the future.
[00:12:28.720 --> 00:12:29.640]   Do you think that's--
[00:12:29.640 --> 00:12:31.400]   - Well, it's not impressing you that I'm intelligent.
[00:12:31.400 --> 00:12:33.680]   In other words, you may be convinced
[00:12:33.680 --> 00:12:35.920]   that I'm intelligent in some form.
[00:12:35.920 --> 00:12:38.920]   Because of my ability to predict.
[00:12:38.920 --> 00:12:39.760]   - So I would look at the metrics.
[00:12:39.760 --> 00:12:43.940]   - When you pass, I say, wow, you're right more times
[00:12:43.940 --> 00:12:46.360]   than I am, you're doing something interesting.
[00:12:46.360 --> 00:12:49.160]   That's a form of intelligence.
[00:12:49.160 --> 00:12:53.440]   But then what happens is, if I say, how are you doing that?
[00:12:53.440 --> 00:12:55.320]   And you can't communicate with me,
[00:12:55.320 --> 00:12:57.760]   and you can't describe that to me,
[00:12:57.760 --> 00:13:00.760]   now I may label you a savant.
[00:13:00.760 --> 00:13:03.280]   I may say, well, you're doing something weird,
[00:13:03.280 --> 00:13:06.420]   and it's just not very interesting to me,
[00:13:06.420 --> 00:13:09.400]   because you and I can't really communicate.
[00:13:09.400 --> 00:13:12.400]   And so now, so this is interesting, right?
[00:13:12.400 --> 00:13:15.160]   Because now this is, you're in this weird place
[00:13:15.160 --> 00:13:19.360]   where for you to be recognized as intelligent
[00:13:19.360 --> 00:13:22.040]   the way I'm intelligent, then you and I
[00:13:22.040 --> 00:13:24.320]   sort of have to be able to communicate.
[00:13:24.320 --> 00:13:28.560]   And then we start to understand each other,
[00:13:28.560 --> 00:13:33.520]   and then my respect and my appreciation,
[00:13:33.520 --> 00:13:36.800]   my ability to relate to you starts to change.
[00:13:36.800 --> 00:13:39.120]   So now you're not an alien intelligence anymore,
[00:13:39.120 --> 00:13:41.100]   you're a human intelligence now,
[00:13:41.100 --> 00:13:43.920]   because you and I can communicate.
[00:13:43.920 --> 00:13:48.120]   And so I think when we look at animals, for example,
[00:13:48.120 --> 00:13:50.720]   animals can do things we can't quite comprehend,
[00:13:50.720 --> 00:13:51.800]   we don't quite know how they do them,
[00:13:51.800 --> 00:13:54.420]   but they can't really communicate with us.
[00:13:54.420 --> 00:13:58.360]   They can't put what they're going through in our terms.
[00:13:58.360 --> 00:13:59.720]   And so we think of them as sort of,
[00:13:59.720 --> 00:14:01.520]   well, they're these alien intelligences,
[00:14:01.520 --> 00:14:03.600]   and they're not really worth necessarily what we're worth,
[00:14:03.600 --> 00:14:06.360]   we don't treat them the same way as a result of that.
[00:14:06.360 --> 00:14:11.360]   But it's hard, because who knows what's going on.
[00:14:11.640 --> 00:14:15.640]   - So just a quick elaboration on that,
[00:14:15.640 --> 00:14:17.960]   the explaining that you're intelligent,
[00:14:17.960 --> 00:14:22.280]   explaining the reasoning that went into the prediction
[00:14:22.280 --> 00:14:27.080]   is not some kind of mathematical proof.
[00:14:27.080 --> 00:14:30.240]   If we look at humans, look at political debates
[00:14:30.240 --> 00:14:35.240]   and discourse on Twitter, it's mostly just telling stories.
[00:14:35.240 --> 00:14:40.340]   So your task is, sorry, your task is not to tell
[00:14:41.080 --> 00:14:45.160]   an accurate depiction of how you reason,
[00:14:45.160 --> 00:14:48.440]   but to tell a story, real or not,
[00:14:48.440 --> 00:14:51.120]   that convinces me that there was a mechanism
[00:14:51.120 --> 00:14:51.960]   by which you--
[00:14:51.960 --> 00:14:53.640]   - Ultimately, that's what a proof is.
[00:14:53.640 --> 00:14:56.240]   I mean, even a mathematical proof is that.
[00:14:56.240 --> 00:14:58.220]   Because ultimately, the other mathematicians
[00:14:58.220 --> 00:15:00.020]   have to be convinced by your proof.
[00:15:00.020 --> 00:15:03.000]   Otherwise, in fact, there have been--
[00:15:03.000 --> 00:15:04.440]   - That's the metric of success, yeah.
[00:15:04.440 --> 00:15:06.060]   - Yeah, there have been several proofs out there
[00:15:06.060 --> 00:15:08.040]   where mathematicians would study for a long time
[00:15:08.040 --> 00:15:08.880]   before they were convinced
[00:15:08.880 --> 00:15:10.880]   that it actually proved anything, right?
[00:15:10.880 --> 00:15:12.200]   You never know if it proved anything
[00:15:12.200 --> 00:15:14.840]   until the community mathematicians decided that it did.
[00:15:14.840 --> 00:15:18.640]   So, I mean, but it's a real thing, right?
[00:15:18.640 --> 00:15:20.920]   And that's sort of the point, right?
[00:15:20.920 --> 00:15:24.600]   Is that ultimately, this notion of understanding us,
[00:15:24.600 --> 00:15:28.200]   understanding something is ultimately a social concept.
[00:15:28.200 --> 00:15:30.700]   In other words, I have to convince enough people
[00:15:30.700 --> 00:15:33.720]   that I did this in a reasonable way.
[00:15:33.720 --> 00:15:36.360]   I did this in a way that other people can understand
[00:15:36.360 --> 00:15:39.800]   and replicate and that it makes sense to them.
[00:15:39.800 --> 00:15:44.800]   So, human intelligence is bound together in that way.
[00:15:44.800 --> 00:15:47.440]   We're bound up in that sense.
[00:15:47.440 --> 00:15:49.520]   We sort of never really get away with it
[00:15:49.520 --> 00:15:52.560]   until we can sort of convince others
[00:15:52.560 --> 00:15:55.840]   that our thinking process makes sense.
[00:15:55.840 --> 00:15:59.080]   - Did you think the general question of intelligence
[00:15:59.080 --> 00:16:00.960]   is then also a social construct?
[00:16:00.960 --> 00:16:05.280]   So, if we ask questions
[00:16:05.280 --> 00:16:06.680]   of an artificial intelligence system,
[00:16:06.680 --> 00:16:08.600]   is this system intelligent?
[00:16:08.600 --> 00:16:13.600]   The answer will ultimately be a socially constructed concept.
[00:16:13.600 --> 00:16:16.040]   - So, I think, I'm making two statements.
[00:16:16.040 --> 00:16:18.000]   I'm saying we can try to define intelligence
[00:16:18.000 --> 00:16:21.360]   in this super objective way that says,
[00:16:21.360 --> 00:16:25.740]   here's this data, I wanna predict this type of thing,
[00:16:25.740 --> 00:16:28.820]   learn this function, and then if you get it right,
[00:16:28.820 --> 00:16:32.080]   often enough, we consider you intelligent.
[00:16:32.080 --> 00:16:34.440]   - But that's more like a savant.
[00:16:34.440 --> 00:16:35.720]   - I think it is.
[00:16:35.720 --> 00:16:37.120]   It doesn't mean it's not useful.
[00:16:37.560 --> 00:16:38.640]   It could be incredibly useful.
[00:16:38.640 --> 00:16:41.480]   It could be solving a problem we can't otherwise solve
[00:16:41.480 --> 00:16:44.520]   and can solve it more reliably than we can.
[00:16:44.520 --> 00:16:46.960]   But then there's this notion of,
[00:16:46.960 --> 00:16:50.440]   can humans take responsibility
[00:16:50.440 --> 00:16:53.680]   for the decision that you're making?
[00:16:53.680 --> 00:16:56.120]   Can we make those decisions ourselves?
[00:16:56.120 --> 00:16:58.840]   Can we relate to the process that you're going through?
[00:16:58.840 --> 00:17:01.160]   And now, you as an agent,
[00:17:01.160 --> 00:17:04.540]   whether you're a machine or another human, frankly,
[00:17:04.540 --> 00:17:08.680]   are now obliged to make me understand
[00:17:08.680 --> 00:17:10.880]   how it is that you're arriving at that answer
[00:17:10.880 --> 00:17:13.880]   and allow me, me or obviously a community
[00:17:13.880 --> 00:17:16.840]   or a judge of people to decide whether or not
[00:17:16.840 --> 00:17:17.680]   that makes sense.
[00:17:17.680 --> 00:17:20.240]   And by the way, that happens with humans as well.
[00:17:20.240 --> 00:17:22.080]   You're sitting down with your staff, for example,
[00:17:22.080 --> 00:17:25.560]   and you ask for suggestions about what to do next,
[00:17:25.560 --> 00:17:28.680]   and someone says, oh, I think you should buy,
[00:17:28.680 --> 00:17:30.600]   and I think you should buy this much,
[00:17:30.600 --> 00:17:33.200]   or whatever, or sell, or whatever it is,
[00:17:33.200 --> 00:17:35.740]   or I think you should launch the product today or tomorrow,
[00:17:35.740 --> 00:17:37.140]   or launch this product versus that product,
[00:17:37.140 --> 00:17:39.860]   whatever the decision may be, and you ask why,
[00:17:39.860 --> 00:17:42.820]   and the person says, I just have a good feeling about it.
[00:17:42.820 --> 00:17:44.420]   And you're not very satisfied.
[00:17:44.420 --> 00:17:48.380]   Now, that person could be, you might say,
[00:17:48.380 --> 00:17:50.920]   well, you've been right before,
[00:17:50.920 --> 00:17:54.180]   but I'm gonna put the company on the line.
[00:17:54.180 --> 00:17:56.820]   Can you explain to me why I should believe this?
[00:17:56.820 --> 00:18:00.060]   - And that explanation may have nothing to do
[00:18:00.060 --> 00:18:00.900]   with the truth.
[00:18:01.860 --> 00:18:03.520]   - You just gotta convince the other person.
[00:18:03.520 --> 00:18:04.360]   It could still be wrong.
[00:18:04.360 --> 00:18:05.320]   It could still be wrong.
[00:18:05.320 --> 00:18:06.320]   - It's just gotta be convincing.
[00:18:06.320 --> 00:18:07.880]   - But it's ultimately gotta be convincing.
[00:18:07.880 --> 00:18:12.200]   And that's why I'm saying we're bound together.
[00:18:12.200 --> 00:18:14.200]   Our intelligences are bound together in that sense.
[00:18:14.200 --> 00:18:15.400]   We have to understand each other.
[00:18:15.400 --> 00:18:18.960]   And if, for example, you're giving me an explanation,
[00:18:18.960 --> 00:18:21.060]   I mean, this is a very important point,
[00:18:21.060 --> 00:18:23.120]   you're giving me an explanation,
[00:18:23.120 --> 00:18:28.320]   and I'm not good,
[00:18:29.420 --> 00:18:33.560]   and then I'm not good at reasoning well
[00:18:33.560 --> 00:18:38.080]   and being objective and following logical paths
[00:18:38.080 --> 00:18:39.200]   and consistent paths,
[00:18:39.200 --> 00:18:41.440]   and I'm not good at measuring
[00:18:41.440 --> 00:18:45.560]   and sort of computing probabilities across those paths,
[00:18:45.560 --> 00:18:47.240]   what happens is collectively,
[00:18:47.240 --> 00:18:50.160]   we're not gonna do well.
[00:18:50.160 --> 00:18:53.200]   - How hard is that problem, the second one?
[00:18:53.200 --> 00:18:58.000]   So I think we'll talk quite a bit about the first
[00:18:58.000 --> 00:19:03.000]   on a specific objective metric benchmark performing well.
[00:19:03.000 --> 00:19:08.860]   But being able to explain the steps, the reasoning,
[00:19:08.860 --> 00:19:10.580]   how hard is that problem?
[00:19:10.580 --> 00:19:11.820]   - I think that's very hard.
[00:19:11.820 --> 00:19:13.300]   I mean, I think that that's,
[00:19:13.300 --> 00:19:18.180]   well, it's hard for humans.
[00:19:18.180 --> 00:19:20.980]   - The thing that's hard for humans, as you know,
[00:19:20.980 --> 00:19:22.940]   may not necessarily be hard for computers
[00:19:22.940 --> 00:19:24.460]   and vice versa.
[00:19:24.460 --> 00:19:29.460]   So, sorry, so how hard is that problem for computers?
[00:19:29.460 --> 00:19:32.660]   - I think it's hard for computers,
[00:19:32.660 --> 00:19:34.620]   and the reason why I related to,
[00:19:34.620 --> 00:19:36.440]   or saying that it's also hard for humans
[00:19:36.440 --> 00:19:38.360]   is because I think when we step back
[00:19:38.360 --> 00:19:41.960]   and we say we wanna design computers to do that,
[00:19:41.960 --> 00:19:46.480]   one of the things we have to recognize
[00:19:46.480 --> 00:19:50.520]   is we're not sure how to do it well.
[00:19:50.520 --> 00:19:52.960]   I'm not sure we have a recipe for that,
[00:19:52.960 --> 00:19:55.340]   and even if you wanted to learn it,
[00:19:55.340 --> 00:19:58.420]   it's not clear exactly what data we use
[00:19:58.420 --> 00:20:03.720]   and what judgments we use to learn that well.
[00:20:03.720 --> 00:20:05.460]   And so what I mean by that is,
[00:20:05.460 --> 00:20:09.500]   if you look at the entire enterprise of science,
[00:20:09.500 --> 00:20:11.620]   science is supposed to be about
[00:20:11.620 --> 00:20:13.700]   objective reasoning reason, right?
[00:20:13.700 --> 00:20:17.660]   So we think about, gee, who's the most intelligent person
[00:20:17.660 --> 00:20:20.500]   or group of people in the world?
[00:20:20.500 --> 00:20:24.080]   Do we think about the savants who can close their eyes
[00:20:24.080 --> 00:20:25.540]   and give you a number?
[00:20:25.540 --> 00:20:27.680]   We think about the think tanks,
[00:20:27.680 --> 00:20:29.500]   or the scientists or the philosophers
[00:20:29.500 --> 00:20:33.820]   who kinda work through the details and write the papers
[00:20:33.820 --> 00:20:37.100]   and come up with the thoughtful, logical proofs
[00:20:37.100 --> 00:20:38.620]   and use the scientific method,
[00:20:38.620 --> 00:20:40.680]   and I think it's the latter.
[00:20:40.680 --> 00:20:45.780]   And my point is that, how do you train someone to do that?
[00:20:45.780 --> 00:20:47.600]   And that's what I mean by it's hard.
[00:20:47.600 --> 00:20:50.800]   What's the process of training people to do that well?
[00:20:50.800 --> 00:20:52.400]   That's a hard process.
[00:20:52.400 --> 00:20:56.040]   We work, as a society, we work pretty hard
[00:20:56.040 --> 00:20:59.240]   to get other people to understand our thinking
[00:20:59.240 --> 00:21:02.220]   and to convince them of things.
[00:21:02.220 --> 00:21:04.040]   Now we could persuade them,
[00:21:04.040 --> 00:21:05.300]   obviously we talked about this,
[00:21:05.300 --> 00:21:07.500]   like human flaws or weaknesses,
[00:21:07.500 --> 00:21:12.180]   we can persuade them through emotional means,
[00:21:12.180 --> 00:21:16.140]   but to get them to understand and connect to
[00:21:16.140 --> 00:21:19.960]   and follow a logical argument is difficult.
[00:21:19.960 --> 00:21:22.440]   We try it, we do it as scientists,
[00:21:22.440 --> 00:21:24.200]   we try to do it as journalists,
[00:21:24.200 --> 00:21:27.280]   we try to do it as even artists in many forms,
[00:21:27.280 --> 00:21:29.760]   as writers, as teachers.
[00:21:29.760 --> 00:21:32.920]   We go through a fairly significant training process
[00:21:32.920 --> 00:21:35.220]   to do that, and then we could ask,
[00:21:35.220 --> 00:21:37.920]   well, why is that so hard?
[00:21:37.920 --> 00:21:42.940]   But it's hard, and for humans, it takes a lot of work.
[00:21:42.940 --> 00:21:45.960]   And when we step back and say,
[00:21:45.960 --> 00:21:49.160]   well, how do we get a machine to do that?
[00:21:49.160 --> 00:21:50.620]   It's a vexing question.
[00:21:50.620 --> 00:21:55.240]   - How would you begin to try to solve that?
[00:21:55.240 --> 00:21:57.400]   And maybe just a quick pause,
[00:21:57.400 --> 00:21:59.840]   because there's an optimistic notion
[00:21:59.840 --> 00:22:01.040]   in the things you're describing,
[00:22:01.040 --> 00:22:05.100]   which is being able to explain something through reason.
[00:22:05.100 --> 00:22:08.660]   But if you look at algorithms that recommend things
[00:22:08.660 --> 00:22:09.780]   that we'll look at next,
[00:22:09.780 --> 00:22:11.800]   whether it's Facebook, Google,
[00:22:11.800 --> 00:22:13.440]   advertisement-based companies,
[00:22:14.600 --> 00:22:19.400]   you know, their goal is to convince you to buy things
[00:22:19.400 --> 00:22:22.400]   based on anything.
[00:22:22.400 --> 00:22:25.480]   So that could be reason,
[00:22:25.480 --> 00:22:27.180]   'cause the best of advertisement
[00:22:27.180 --> 00:22:29.640]   is showing you things that you really do need
[00:22:29.640 --> 00:22:32.020]   and explain why you need it.
[00:22:32.020 --> 00:22:35.700]   But it could also be through emotional manipulation.
[00:22:35.700 --> 00:22:40.840]   The algorithm that describes why a certain reason,
[00:22:40.840 --> 00:22:43.800]   a certain decision was made,
[00:22:43.800 --> 00:22:48.200]   how hard is it to do it through emotional manipulation?
[00:22:48.200 --> 00:22:51.960]   And why is that a good or a bad thing?
[00:22:51.960 --> 00:22:56.920]   So you've kind of focused on reason, logic,
[00:22:56.920 --> 00:23:01.500]   really showing in a clear way why something is good.
[00:23:01.500 --> 00:23:05.960]   One, is that even a thing that us humans do?
[00:23:05.960 --> 00:23:09.920]   And two, how do you think of the difference
[00:23:09.920 --> 00:23:13.420]   in the reasoning aspect and the emotional manipulation?
[00:23:13.420 --> 00:23:17.320]   - So you call it emotional manipulation,
[00:23:17.320 --> 00:23:20.160]   but more objectively, it's essentially saying,
[00:23:20.160 --> 00:23:22.600]   there are certain features of things
[00:23:22.600 --> 00:23:24.400]   that seem to attract your attention.
[00:23:24.400 --> 00:23:26.760]   I mean, it kind of give you more of that stuff.
[00:23:26.760 --> 00:23:28.240]   - Manipulation is a bad word.
[00:23:28.240 --> 00:23:31.120]   - Yeah, I mean, I'm not saying it's right or wrong.
[00:23:31.120 --> 00:23:32.960]   It works to get your attention,
[00:23:32.960 --> 00:23:34.400]   and it works to get you to buy stuff.
[00:23:34.400 --> 00:23:35.940]   And when you think about algorithms
[00:23:35.940 --> 00:23:39.980]   that look at the patterns of features
[00:23:39.980 --> 00:23:41.880]   that you seem to be spending your money on,
[00:23:41.880 --> 00:23:43.220]   and say, I'm gonna give you something
[00:23:43.220 --> 00:23:44.780]   with a similar pattern,
[00:23:44.780 --> 00:23:46.060]   so I'm gonna learn that function,
[00:23:46.060 --> 00:23:48.180]   because the objective is to get you to click on it
[00:23:48.180 --> 00:23:50.180]   or get you to buy it or whatever it is.
[00:23:50.180 --> 00:23:53.340]   I don't know, I mean, it is what it is.
[00:23:53.340 --> 00:23:55.800]   I mean, that's what the algorithm does.
[00:23:55.800 --> 00:23:57.420]   You can argue whether it's good or bad.
[00:23:57.420 --> 00:24:00.380]   It depends what your goal is.
[00:24:00.380 --> 00:24:04.140]   - I guess this seems to be very useful for convincing,
[00:24:04.140 --> 00:24:04.980]   for telling a story.
[00:24:04.980 --> 00:24:07.660]   - For convincing humans, it's good,
[00:24:07.660 --> 00:24:09.580]   because again, this goes back to,
[00:24:09.580 --> 00:24:12.060]   what is the human behavior like?
[00:24:12.060 --> 00:24:16.980]   How does the human brain respond to things?
[00:24:16.980 --> 00:24:19.340]   I think there's a more optimistic view of that, too,
[00:24:19.340 --> 00:24:21.980]   which is that if you're searching
[00:24:21.980 --> 00:24:23.100]   for certain kinds of things,
[00:24:23.100 --> 00:24:26.140]   you've already reasoned that you need them.
[00:24:26.140 --> 00:24:30.020]   And these algorithms are saying, look, that's up to you
[00:24:30.020 --> 00:24:32.160]   to reason whether you need something or not.
[00:24:32.160 --> 00:24:33.640]   That's your job.
[00:24:33.640 --> 00:24:36.900]   You may have an unhealthy addiction to this stuff,
[00:24:36.900 --> 00:24:41.900]   or you may have a reasoned and thoughtful explanation
[00:24:41.900 --> 00:24:44.500]   for why it's important to you,
[00:24:44.500 --> 00:24:47.220]   and the algorithms are saying, hey, that's whatever.
[00:24:47.220 --> 00:24:48.060]   That's your problem.
[00:24:48.060 --> 00:24:50.580]   All I know is you're buying stuff like that,
[00:24:50.580 --> 00:24:51.900]   you're interested in stuff like that.
[00:24:51.900 --> 00:24:53.900]   Could be a bad reason, could be a good reason.
[00:24:53.900 --> 00:24:55.020]   That's up to you.
[00:24:55.020 --> 00:24:57.060]   I'm gonna show you more of that stuff.
[00:24:57.060 --> 00:25:02.220]   And I think that it's not good or bad.
[00:25:02.220 --> 00:25:03.540]   It's not reasoned or not reasoned.
[00:25:03.540 --> 00:25:04.920]   The algorithm is doing what it does,
[00:25:04.920 --> 00:25:06.920]   which is saying, you seem to be interested in this.
[00:25:06.920 --> 00:25:09.340]   I'm gonna show you more of that stuff.
[00:25:09.340 --> 00:25:11.200]   And I think we're seeing this not just in buying stuff,
[00:25:11.200 --> 00:25:12.160]   but even in social media.
[00:25:12.160 --> 00:25:13.980]   You're reading this kind of stuff.
[00:25:13.980 --> 00:25:15.740]   I'm not judging on whether it's good or bad.
[00:25:15.740 --> 00:25:16.940]   I'm not reasoning at all.
[00:25:16.940 --> 00:25:19.200]   I'm just saying, I'm gonna show you other stuff
[00:25:19.200 --> 00:25:20.820]   with similar features.
[00:25:20.820 --> 00:25:23.540]   And that's it, and I wash my hands from it,
[00:25:23.540 --> 00:25:25.920]   and I say, that's all that's going on.
[00:25:25.920 --> 00:25:31.920]   - People are so harsh on AI systems.
[00:25:31.920 --> 00:25:34.940]   So one, the bar of performance is extremely high,
[00:25:34.940 --> 00:25:39.580]   and yet we also ask them to, in the case of social media,
[00:25:39.580 --> 00:25:42.940]   to help find the better angels of our nature
[00:25:42.940 --> 00:25:45.980]   and help make a better society.
[00:25:45.980 --> 00:25:47.900]   So what do you think about the role of AI?
[00:25:47.900 --> 00:25:48.980]   - So that, I agree with you.
[00:25:48.980 --> 00:25:51.580]   That's the interesting dichotomy, right?
[00:25:51.580 --> 00:25:54.180]   Because on one hand, we're sitting there
[00:25:54.180 --> 00:25:55.900]   and we're sort of doing the easy part,
[00:25:55.900 --> 00:25:57.980]   which is finding the patterns.
[00:25:57.980 --> 00:26:01.820]   We're not building, the system's not building a theory
[00:26:01.820 --> 00:26:04.200]   that is consumable and understandable by other humans
[00:26:04.200 --> 00:26:06.360]   that can be explained and justified.
[00:26:06.360 --> 00:26:11.360]   And so on one hand to say, oh, AI is doing this.
[00:26:11.360 --> 00:26:13.700]   Why isn't it doing this other thing?
[00:26:13.700 --> 00:26:16.280]   Well, this other thing's a lot harder.
[00:26:16.280 --> 00:26:20.160]   And it's interesting to think about why it's harder.
[00:26:20.160 --> 00:26:23.960]   And because you're interpreting the data
[00:26:23.960 --> 00:26:26.260]   in the context of prior models.
[00:26:26.260 --> 00:26:28.920]   In other words, understandings of what's important
[00:26:28.920 --> 00:26:30.220]   in the world, what's not important.
[00:26:30.220 --> 00:26:32.040]   What are all the other abstract features
[00:26:32.040 --> 00:26:35.380]   that drive our decision-making?
[00:26:35.380 --> 00:26:37.440]   What's sensible, what's not sensible, what's good,
[00:26:37.440 --> 00:26:40.020]   what's bad, what's moral, what's valuable, what isn't?
[00:26:40.020 --> 00:26:41.120]   Where is that stuff?
[00:26:41.120 --> 00:26:43.240]   No one's applying the interpretation.
[00:26:43.240 --> 00:26:46.640]   So when I see you clicking on a bunch of stuff
[00:26:46.640 --> 00:26:49.760]   and I look at these simple features, the raw features,
[00:26:49.760 --> 00:26:51.080]   the features that are there in the data,
[00:26:51.080 --> 00:26:53.340]   like what words are being used,
[00:26:53.340 --> 00:26:57.680]   or how long the material is,
[00:26:57.680 --> 00:27:00.620]   or other very superficial features,
[00:27:00.620 --> 00:27:02.540]   what colors are being used in the material.
[00:27:02.540 --> 00:27:03.900]   Like I don't know why you're clicking
[00:27:03.900 --> 00:27:04.980]   on the stuff you're looking.
[00:27:04.980 --> 00:27:07.620]   Or if it's products, what the price is,
[00:27:07.620 --> 00:27:09.580]   or what the category is, and stuff like that.
[00:27:09.580 --> 00:27:11.560]   And I just feed you more of the same stuff.
[00:27:11.560 --> 00:27:13.780]   That's very different than kind of getting in there
[00:27:13.780 --> 00:27:16.000]   and saying, what does this mean?
[00:27:16.000 --> 00:27:20.380]   The stuff you're reading, like why are you reading it?
[00:27:20.380 --> 00:27:23.940]   What assumptions are you bringing to the table?
[00:27:23.940 --> 00:27:26.400]   Are those assumptions sensible?
[00:27:26.400 --> 00:27:29.020]   Does the material make any sense?
[00:27:29.020 --> 00:27:34.020]   Does it lead you to thoughtful, good conclusions?
[00:27:34.020 --> 00:27:37.460]   Again, there's interpretation and judgment involved
[00:27:37.460 --> 00:27:42.460]   in that process that isn't really happening in the AI today.
[00:27:42.460 --> 00:27:47.220]   That's harder because you have to start getting
[00:27:47.220 --> 00:27:52.040]   at the meaning of the stuff, of the content.
[00:27:52.040 --> 00:27:55.780]   You have to get at how humans interpret the content
[00:27:55.780 --> 00:27:58.720]   relative to their value system
[00:27:58.720 --> 00:28:00.600]   and deeper thought processes.
[00:28:00.600 --> 00:28:02.140]   - So that's what meaning means,
[00:28:02.140 --> 00:28:07.140]   is not just some kind of deep, timeless, semantic thing
[00:28:07.140 --> 00:28:10.960]   that the statement represents,
[00:28:10.960 --> 00:28:13.400]   but also how a large number of people
[00:28:13.400 --> 00:28:15.240]   are likely to interpret.
[00:28:15.240 --> 00:28:19.120]   So it's again, even meaning is a social construct.
[00:28:19.120 --> 00:28:22.800]   So you have to try to predict how most people
[00:28:22.800 --> 00:28:24.520]   would understand this kind of statement.
[00:28:24.520 --> 00:28:27.280]   - Yeah, meaning is often relative,
[00:28:27.280 --> 00:28:29.860]   but meaning implies that the connections
[00:28:29.860 --> 00:28:31.840]   go beneath the surface of the artifacts.
[00:28:31.840 --> 00:28:35.480]   If I show you a painting, it's a bunch of colors on a canvas,
[00:28:35.480 --> 00:28:37.140]   what does it mean to you?
[00:28:37.140 --> 00:28:39.420]   And it may mean different things to different people
[00:28:39.420 --> 00:28:42.240]   because of their different experiences.
[00:28:42.240 --> 00:28:44.720]   It may mean something even different
[00:28:44.720 --> 00:28:46.460]   to the artist who painted it.
[00:28:46.460 --> 00:28:50.720]   As we try to get more rigorous with our communication,
[00:28:50.720 --> 00:28:53.280]   we try to really nail down that meaning.
[00:28:53.280 --> 00:28:58.280]   So we go from abstract art to precise mathematics,
[00:28:58.280 --> 00:29:01.520]   precise engineering drawings and things like that.
[00:29:01.520 --> 00:29:03.440]   We're really trying to say,
[00:29:03.440 --> 00:29:08.280]   I wanna narrow that space of possible interpretations
[00:29:08.280 --> 00:29:10.720]   because the precision of the communication
[00:29:10.720 --> 00:29:13.400]   ends up becoming more and more important.
[00:29:13.400 --> 00:29:17.920]   And so that means that I have to specify,
[00:29:17.920 --> 00:29:21.400]   and I think that's why this becomes really hard,
[00:29:21.400 --> 00:29:24.200]   because if I'm just showing you an artifact
[00:29:24.200 --> 00:29:26.000]   and you're looking at it superficially,
[00:29:26.000 --> 00:29:28.240]   whether it's a bunch of words on a page
[00:29:28.240 --> 00:29:31.940]   or whether it's brushstrokes on a canvas
[00:29:31.940 --> 00:29:33.600]   or pixels in a photograph,
[00:29:33.600 --> 00:29:35.080]   you can sit there and you can interpret
[00:29:35.080 --> 00:29:37.840]   lots of different ways at many, many different levels.
[00:29:37.840 --> 00:29:44.960]   But when I wanna align our understanding of that,
[00:29:44.960 --> 00:29:48.400]   I have to specify a lot more stuff
[00:29:48.400 --> 00:29:52.320]   that's actually not directly in the artifact.
[00:29:52.320 --> 00:29:56.000]   Now I have to say, well, how are you interpreting
[00:29:56.000 --> 00:29:57.280]   this image and that image?
[00:29:57.280 --> 00:29:59.400]   And what about the colors and what do they mean to you?
[00:29:59.400 --> 00:30:02.600]   What perspective are you bringing to the table?
[00:30:02.600 --> 00:30:05.640]   What are your prior experiences with those artifacts?
[00:30:05.640 --> 00:30:08.840]   What are your fundamental assumptions and values?
[00:30:08.840 --> 00:30:10.880]   What is your ability to kind of reason
[00:30:10.880 --> 00:30:13.680]   to chain together logical implication
[00:30:13.680 --> 00:30:14.520]   as you're sitting there and saying,
[00:30:14.520 --> 00:30:16.560]   well, if this is the case, then I would conclude this.
[00:30:16.560 --> 00:30:19.120]   If that's the case, then I would conclude that.
[00:30:19.120 --> 00:30:22.520]   So your reasoning processes and how they work,
[00:30:22.520 --> 00:30:25.360]   your prior models and what they are,
[00:30:25.360 --> 00:30:27.200]   your values and your assumptions,
[00:30:27.200 --> 00:30:30.640]   all those things now come together into the interpretation.
[00:30:30.640 --> 00:30:33.800]   Getting in sync of that is hard.
[00:30:33.800 --> 00:30:37.640]   - And yet humans are able to intuit some of that
[00:30:37.640 --> 00:30:39.600]   without any pre--
[00:30:39.600 --> 00:30:41.560]   - Because they have the shared experience.
[00:30:41.560 --> 00:30:42.960]   - And we're not talking about shared,
[00:30:42.960 --> 00:30:44.440]   two people having a shared experience.
[00:30:44.440 --> 00:30:46.440]   I mean, as a society-- - That's correct.
[00:30:46.600 --> 00:30:51.240]   - We have the shared experience and we have similar brains.
[00:30:51.240 --> 00:30:54.120]   So we tend to, in other words,
[00:30:54.120 --> 00:30:55.160]   part of our shared experience
[00:30:55.160 --> 00:30:56.520]   is our shared local experience.
[00:30:56.520 --> 00:30:57.920]   Like we may live in the same culture,
[00:30:57.920 --> 00:30:59.120]   we may live in the same society,
[00:30:59.120 --> 00:31:02.080]   and therefore we have similar educations.
[00:31:02.080 --> 00:31:04.160]   We have similar, what we like to call prior models
[00:31:04.160 --> 00:31:05.920]   about the prior experiences.
[00:31:05.920 --> 00:31:07.400]   And we use that as a,
[00:31:07.400 --> 00:31:11.000]   think of it as a wide collection of interrelated variables
[00:31:11.000 --> 00:31:12.840]   and they're all bound to similar things.
[00:31:12.840 --> 00:31:15.120]   And so we take that as our background
[00:31:15.120 --> 00:31:17.560]   and we start interpreting things similarly.
[00:31:17.560 --> 00:31:21.840]   But as humans, we have a lot of shared experience.
[00:31:21.840 --> 00:31:24.960]   We do have similar brains, similar goals,
[00:31:24.960 --> 00:31:28.080]   similar emotions under similar circumstances
[00:31:28.080 --> 00:31:29.040]   because we're both humans.
[00:31:29.040 --> 00:31:31.400]   So now one of the early questions you asked,
[00:31:31.400 --> 00:31:36.400]   how is biological and computer information systems
[00:31:36.400 --> 00:31:38.000]   fundamentally different?
[00:31:38.000 --> 00:31:43.000]   Well, one is humans come with a lot of pre-programmed stuff,
[00:31:43.840 --> 00:31:45.960]   a ton of program stuff,
[00:31:45.960 --> 00:31:47.240]   and they're able to communicate
[00:31:47.240 --> 00:31:48.360]   because they have a lot of,
[00:31:48.360 --> 00:31:50.360]   because they share that stuff.
[00:31:50.360 --> 00:31:52.700]   - Do you think that shared knowledge,
[00:31:52.700 --> 00:31:57.560]   if we can maybe escape the hardware question,
[00:31:57.560 --> 00:31:59.440]   how much is encoded in the hardware,
[00:31:59.440 --> 00:32:01.200]   just the shared knowledge in the software,
[00:32:01.200 --> 00:32:05.280]   the history, the many centuries of wars and so on
[00:32:05.280 --> 00:32:07.960]   that came to today, that shared knowledge,
[00:32:09.800 --> 00:32:13.400]   how hard is it to encode?
[00:32:13.400 --> 00:32:15.840]   Do you have a hope?
[00:32:15.840 --> 00:32:19.360]   Can you speak to how hard is it to encode that knowledge
[00:32:19.360 --> 00:32:22.800]   systematically in a way that could be used by a computer?
[00:32:22.800 --> 00:32:26.340]   - So I think it is possible to learn for a machine,
[00:32:26.340 --> 00:32:29.600]   to program a machine to acquire that knowledge
[00:32:29.600 --> 00:32:31.440]   with a similar foundation.
[00:32:31.440 --> 00:32:36.120]   In other words, a similar interpretive foundation
[00:32:36.120 --> 00:32:38.040]   for processing that knowledge.
[00:32:38.040 --> 00:32:39.080]   - What do you mean by that?
[00:32:39.080 --> 00:32:40.800]   - So in other words,
[00:32:40.800 --> 00:32:44.360]   we view the world in a particular way.
[00:32:44.360 --> 00:32:48.800]   And so in other words, we have, if you will,
[00:32:48.800 --> 00:32:50.080]   as humans, we have a framework
[00:32:50.080 --> 00:32:52.200]   for interpreting the world around us.
[00:32:52.200 --> 00:32:54.720]   So we have multiple frameworks
[00:32:54.720 --> 00:32:55.920]   for interpreting the world around us.
[00:32:55.920 --> 00:32:59.720]   But if you're interpreting, for example,
[00:32:59.720 --> 00:33:01.320]   sociopolitical interactions,
[00:33:01.320 --> 00:33:03.080]   you're thinking about where there's people,
[00:33:03.080 --> 00:33:05.520]   there's collections and groups of people,
[00:33:05.520 --> 00:33:08.400]   they have goals, goals are largely built around survival
[00:33:08.400 --> 00:33:10.040]   and quality of life.
[00:33:10.040 --> 00:33:13.280]   There are fundamental economics
[00:33:13.280 --> 00:33:16.600]   around scarcity of resources.
[00:33:16.600 --> 00:33:19.600]   And when humans come and start interpreting
[00:33:19.600 --> 00:33:20.640]   a situation like that,
[00:33:20.640 --> 00:33:23.560]   because you brought up like historical events,
[00:33:23.560 --> 00:33:25.480]   they start interpreting situations like that.
[00:33:25.480 --> 00:33:29.480]   They apply a lot of this fundamental framework
[00:33:29.480 --> 00:33:30.720]   for interpreting that.
[00:33:30.720 --> 00:33:32.200]   Well, who are the people?
[00:33:32.200 --> 00:33:33.280]   What were their goals?
[00:33:33.280 --> 00:33:35.000]   What resources did they have?
[00:33:35.000 --> 00:33:37.000]   How much power or influence did they have over the other?
[00:33:37.000 --> 00:33:40.520]   Like this fundamental substrate, if you will,
[00:33:40.520 --> 00:33:42.760]   for interpreting and reasoning about that.
[00:33:42.760 --> 00:33:46.880]   So I think it is possible to imbue a computer
[00:33:46.880 --> 00:33:50.600]   with that stuff that humans like take for granted
[00:33:50.600 --> 00:33:54.280]   when they go and sit down and try to interpret things.
[00:33:54.280 --> 00:33:58.800]   And then with that foundation, they acquire,
[00:33:58.800 --> 00:34:00.280]   they start acquiring the details,
[00:34:00.280 --> 00:34:02.800]   the specifics in a given situation,
[00:34:02.800 --> 00:34:05.680]   are then able to interpret it with regard to that framework.
[00:34:05.680 --> 00:34:08.680]   And then given that interpretation, they can do what?
[00:34:08.680 --> 00:34:10.320]   They can predict.
[00:34:10.320 --> 00:34:12.200]   But not only can they predict,
[00:34:12.200 --> 00:34:14.800]   they can predict now with an explanation
[00:34:14.800 --> 00:34:17.920]   that can be given in those terms,
[00:34:17.920 --> 00:34:20.200]   in the terms of that underlying framework
[00:34:20.200 --> 00:34:22.320]   that most humans share.
[00:34:22.320 --> 00:34:24.640]   Now you can find humans that come and interpret events
[00:34:24.640 --> 00:34:26.320]   very differently than other humans,
[00:34:26.320 --> 00:34:30.160]   because they're like using a different framework.
[00:34:30.160 --> 00:34:32.480]   You know, the movie "Matrix" comes to mind,
[00:34:32.480 --> 00:34:36.360]   where they decided humans were really just batteries,
[00:34:36.360 --> 00:34:39.880]   and that's how they interpreted the value of humans
[00:34:39.880 --> 00:34:41.600]   as a source of electrical energy.
[00:34:41.600 --> 00:34:45.400]   So, but I think that for the most part,
[00:34:45.400 --> 00:34:50.400]   we have a way of interpreting the events,
[00:34:50.400 --> 00:34:52.240]   or the social events around us,
[00:34:52.240 --> 00:34:54.120]   because we have this shared framework.
[00:34:54.120 --> 00:34:58.680]   It comes from, again, the fact that we're similar beings
[00:34:58.680 --> 00:35:01.040]   that have similar goals, similar emotions,
[00:35:01.040 --> 00:35:02.880]   and we can make sense out of these.
[00:35:02.880 --> 00:35:04.960]   These frameworks make sense to us.
[00:35:04.960 --> 00:35:08.040]   - So how much knowledge is there, do you think?
[00:35:08.040 --> 00:35:09.560]   So you said it's possible.
[00:35:09.560 --> 00:35:10.600]   - Well, there's a tremendous amount
[00:35:10.600 --> 00:35:12.840]   of detailed knowledge in the world.
[00:35:12.840 --> 00:35:17.600]   You can imagine effectively infinite number
[00:35:17.600 --> 00:35:20.840]   of unique situations and unique configurations
[00:35:20.840 --> 00:35:22.120]   of these things.
[00:35:22.120 --> 00:35:25.120]   But the knowledge that you need,
[00:35:25.120 --> 00:35:27.600]   what I refer to as like the frameworks,
[00:35:27.600 --> 00:35:29.560]   for you need for interpreting them, I don't think.
[00:35:29.560 --> 00:35:31.480]   I think those are finite.
[00:35:31.480 --> 00:35:35.000]   - You think the frameworks are more important
[00:35:35.000 --> 00:35:36.760]   than the bulk of the knowledge?
[00:35:36.760 --> 00:35:37.760]   So like framing--
[00:35:37.760 --> 00:35:39.200]   - Yeah, because what the frameworks do
[00:35:39.200 --> 00:35:41.560]   is they give you now the ability to interpret and reason,
[00:35:41.560 --> 00:35:46.560]   and to interpret and reason over the specifics
[00:35:46.560 --> 00:35:49.240]   in ways that other humans would understand.
[00:35:49.240 --> 00:35:51.240]   - What about the specifics?
[00:35:51.240 --> 00:35:53.960]   - Well, you acquire the specifics by reading
[00:35:53.960 --> 00:35:55.560]   and by talking to other people.
[00:35:55.560 --> 00:35:57.720]   - So I'll mostly, actually, just even,
[00:35:57.720 --> 00:36:00.240]   if we can focus on even the beginning,
[00:36:00.240 --> 00:36:01.480]   the common sense stuff,
[00:36:01.480 --> 00:36:03.400]   the stuff that doesn't even require reading,
[00:36:03.400 --> 00:36:06.280]   or it almost requires playing around
[00:36:06.280 --> 00:36:07.720]   with the world or something.
[00:36:07.720 --> 00:36:10.800]   Just being able to sort of manipulate objects,
[00:36:10.800 --> 00:36:13.880]   drink water and so on, all of that.
[00:36:13.880 --> 00:36:16.120]   Every time we try to do that kind of thing
[00:36:16.120 --> 00:36:21.040]   in robotics or AI, it seems to be like an onion.
[00:36:21.040 --> 00:36:23.240]   You seem to realize how much knowledge
[00:36:23.240 --> 00:36:24.620]   is really required to perform
[00:36:24.620 --> 00:36:27.040]   even some of these basic tasks.
[00:36:27.040 --> 00:36:30.320]   Do you have that sense as well?
[00:36:30.320 --> 00:36:33.800]   And if so, how do we get all those details?
[00:36:33.800 --> 00:36:35.680]   Are they written down somewhere?
[00:36:35.680 --> 00:36:39.200]   Do they have to be learned through experience?
[00:36:39.200 --> 00:36:41.320]   - So I think when, like if you're talking about
[00:36:41.320 --> 00:36:44.680]   sort of the physics, the basic physics around us,
[00:36:44.680 --> 00:36:46.600]   for example, acquiring information about,
[00:36:46.600 --> 00:36:48.120]   acquiring how that works.
[00:36:48.120 --> 00:36:52.200]   Yeah, I mean, I think there's a combination of things going,
[00:36:52.200 --> 00:36:54.600]   I think there's a combination of things going on.
[00:36:54.600 --> 00:36:57.800]   I think there is like fundamental pattern matching,
[00:36:57.800 --> 00:36:59.680]   like what we were talking about before,
[00:36:59.680 --> 00:37:01.040]   where you see enough examples,
[00:37:01.040 --> 00:37:02.160]   enough data about something,
[00:37:02.160 --> 00:37:03.840]   you just start assuming that,
[00:37:03.840 --> 00:37:07.720]   and with similar input, I'm gonna predict similar outputs.
[00:37:07.720 --> 00:37:10.120]   You can't necessarily explain it at all.
[00:37:10.120 --> 00:37:13.620]   You may learn very quickly that when you let something go,
[00:37:13.620 --> 00:37:15.920]   it falls to the ground.
[00:37:15.920 --> 00:37:17.840]   - That's such a--
[00:37:17.840 --> 00:37:19.760]   - But you can't necessarily explain that.
[00:37:19.760 --> 00:37:22.320]   - But that's such a deep idea,
[00:37:22.320 --> 00:37:23.880]   that if you let something go,
[00:37:23.880 --> 00:37:25.200]   like the idea of gravity.
[00:37:25.200 --> 00:37:27.920]   - I mean, people are letting things go
[00:37:27.920 --> 00:37:29.080]   and counting on them falling
[00:37:29.080 --> 00:37:30.760]   well before they understood gravity.
[00:37:30.760 --> 00:37:33.840]   - But that seems to be, that's exactly what I mean,
[00:37:33.840 --> 00:37:36.080]   is before you take a physics class
[00:37:36.080 --> 00:37:39.560]   or study anything about Newton,
[00:37:39.560 --> 00:37:42.520]   just the idea that stuff falls to the ground
[00:37:42.520 --> 00:37:45.320]   and then you be able to generalize
[00:37:45.320 --> 00:37:48.540]   that all kinds of stuff falls to the ground,
[00:37:48.540 --> 00:37:52.200]   it just seems like a non,
[00:37:52.200 --> 00:37:55.200]   without encoding it, like hard coding it in,
[00:37:55.200 --> 00:37:57.400]   it seems like a difficult thing to pick up.
[00:37:57.400 --> 00:38:01.380]   It seems like you have to have a lot of different knowledge
[00:38:01.380 --> 00:38:05.360]   to be able to integrate that into the framework,
[00:38:05.360 --> 00:38:07.760]   sort of into everything else.
[00:38:07.760 --> 00:38:10.360]   So both know that stuff falls to the ground
[00:38:10.360 --> 00:38:15.360]   and start to reason about sociopolitical discourse.
[00:38:15.360 --> 00:38:18.600]   So both, like the very basic
[00:38:18.600 --> 00:38:22.560]   and the high level reasoning decision-making.
[00:38:22.560 --> 00:38:24.960]   I guess my question is how hard is this problem?
[00:38:24.960 --> 00:38:29.040]   Sorry to linger on it because again,
[00:38:29.040 --> 00:38:31.120]   and we'll get to it for sure,
[00:38:31.120 --> 00:38:33.000]   as what Watson with Jeopardy did,
[00:38:33.000 --> 00:38:35.480]   is take on a problem that's much more constrained
[00:38:35.480 --> 00:38:38.240]   but has the same hugeness of scale,
[00:38:38.240 --> 00:38:40.640]   at least from the outsider's perspective.
[00:38:40.640 --> 00:38:42.880]   So I'm asking the general life question
[00:38:42.880 --> 00:38:45.600]   of to be able to be an intelligent being
[00:38:45.600 --> 00:38:47.520]   and reasoning in the world
[00:38:47.520 --> 00:38:50.880]   about both gravity and politics,
[00:38:50.880 --> 00:38:52.120]   how hard is that problem?
[00:38:52.120 --> 00:38:56.180]   - So I think it's solvable.
[00:38:56.180 --> 00:39:00.680]   - Okay, now beautiful.
[00:39:00.680 --> 00:39:04.320]   So what about time travel?
[00:39:04.320 --> 00:39:07.960]   Okay, on that topic, I'm just,
[00:39:07.960 --> 00:39:09.680]   not the same answer? - Not as convinced.
[00:39:09.680 --> 00:39:11.040]   - Not as convinced yet, okay.
[00:39:11.040 --> 00:39:14.240]   - No, I think it is, I think it is solvable.
[00:39:14.240 --> 00:39:16.440]   I mean, I think that it's,
[00:39:16.440 --> 00:39:18.400]   first of all, it's about getting machines to learn.
[00:39:18.400 --> 00:39:21.360]   Learning is fundamental.
[00:39:21.360 --> 00:39:24.400]   And I think we're already in a place that we understand,
[00:39:24.400 --> 00:39:28.560]   for example, how machines can learn in various ways.
[00:39:28.560 --> 00:39:32.420]   Right now, our learning stuff is sort of primitive
[00:39:32.420 --> 00:39:37.420]   in that we haven't sort of taught machines
[00:39:37.420 --> 00:39:39.200]   to learn the frameworks.
[00:39:39.200 --> 00:39:41.120]   We don't communicate our frameworks
[00:39:41.120 --> 00:39:42.820]   because of how shared, in some cases we do,
[00:39:42.820 --> 00:39:46.360]   but we don't annotate, if you will,
[00:39:46.360 --> 00:39:48.920]   all the data in the world with the frameworks
[00:39:48.920 --> 00:39:53.080]   that are inherent or underlying our understanding.
[00:39:53.080 --> 00:39:56.140]   Instead, we just operate with the data.
[00:39:56.140 --> 00:39:59.060]   So if we wanna be able to reason over the data
[00:39:59.060 --> 00:40:02.280]   in similar terms in the common frameworks,
[00:40:02.280 --> 00:40:03.720]   we need to be able to teach the computer,
[00:40:03.720 --> 00:40:07.560]   or at least we need to program the computer to acquire,
[00:40:07.560 --> 00:40:12.560]   to have access to and acquire, learn the frameworks as well
[00:40:12.560 --> 00:40:15.700]   and connect the frameworks to the data.
[00:40:15.700 --> 00:40:18.400]   I think this can be done.
[00:40:18.400 --> 00:40:22.920]   I think we can start, I think machine learning,
[00:40:22.920 --> 00:40:26.040]   for example, with enough examples,
[00:40:26.040 --> 00:40:28.880]   can start to learn these basic dynamics.
[00:40:28.880 --> 00:40:32.200]   Will they relate them necessarily to gravity?
[00:40:32.200 --> 00:40:37.080]   Not unless they can also acquire those theories as well
[00:40:37.080 --> 00:40:40.880]   and put the experiential knowledge
[00:40:40.880 --> 00:40:43.360]   and connect it back to the theoretical knowledge.
[00:40:43.360 --> 00:40:47.160]   I think if we think in terms of these class of architectures
[00:40:47.160 --> 00:40:51.000]   that are designed to both learn the specifics,
[00:40:51.000 --> 00:40:54.180]   find the patterns, but also acquire the frameworks
[00:40:54.180 --> 00:40:56.300]   and connect the data to the frameworks,
[00:40:56.300 --> 00:40:59.660]   if we think in terms of robust architectures like this,
[00:40:59.660 --> 00:41:03.380]   I think there is a path toward getting there.
[00:41:03.380 --> 00:41:06.180]   - In terms of encoding architectures like that,
[00:41:06.180 --> 00:41:09.180]   do you think systems that are able to do this
[00:41:09.180 --> 00:41:12.020]   will look like neural networks
[00:41:12.020 --> 00:41:17.020]   or representing, if you look back to the '80s and '90s,
[00:41:17.020 --> 00:41:18.680]   with the expert systems,
[00:41:18.680 --> 00:41:23.540]   so more like graphs, systems that are based in logic,
[00:41:23.540 --> 00:41:26.500]   able to contain a large amount of knowledge
[00:41:26.500 --> 00:41:28.500]   where the challenge was the automated acquisition
[00:41:28.500 --> 00:41:29.860]   of that knowledge.
[00:41:29.860 --> 00:41:31.840]   I guess the question is,
[00:41:31.840 --> 00:41:33.820]   when you collect both the frameworks
[00:41:33.820 --> 00:41:35.300]   and the knowledge from the data,
[00:41:35.300 --> 00:41:37.260]   what do you think that thing will look like?
[00:41:37.260 --> 00:41:39.340]   - Yeah, so I mean, I think asking the question
[00:41:39.340 --> 00:41:41.260]   do they look like neural networks is a bit of a red herring.
[00:41:41.260 --> 00:41:45.180]   I mean, I think that they will certainly do inductive
[00:41:45.180 --> 00:41:46.740]   or pattern-matched-based reasoning.
[00:41:46.740 --> 00:41:49.020]   And I've already experimented with architectures
[00:41:49.020 --> 00:41:52.740]   that combine both, that use machine learning
[00:41:52.740 --> 00:41:55.380]   and neural networks to learn certain classes of knowledge,
[00:41:55.380 --> 00:41:57.340]   in other words, to find repeated patterns
[00:41:57.340 --> 00:42:01.540]   in order for it to make good inductive guesses,
[00:42:01.540 --> 00:42:05.300]   but then ultimately to try to take those learnings
[00:42:05.300 --> 00:42:09.560]   and marry them, in other words, connect them to frameworks
[00:42:09.560 --> 00:42:11.520]   so that it can then reason over that
[00:42:11.520 --> 00:42:13.660]   in terms other humans understand.
[00:42:13.660 --> 00:42:16.140]   So for example, at Elemental Cognition, we do both.
[00:42:16.140 --> 00:42:19.860]   We have architectures that do both, but both those things,
[00:42:19.860 --> 00:42:21.700]   but also have a learning method
[00:42:21.700 --> 00:42:24.380]   for acquiring the frameworks themselves and saying,
[00:42:24.380 --> 00:42:27.260]   "Look, ultimately I need to take this data.
[00:42:27.260 --> 00:42:29.980]   "I need to interpret it in the form of these frameworks
[00:42:29.980 --> 00:42:30.900]   "so they can reason over it."
[00:42:30.900 --> 00:42:33.380]   So there is a fundamental knowledge representation,
[00:42:33.380 --> 00:42:35.820]   like what you're saying, like these graphs of logic,
[00:42:35.820 --> 00:42:36.820]   if you will.
[00:42:36.820 --> 00:42:39.320]   There are also neural networks
[00:42:39.320 --> 00:42:41.660]   that acquire a certain class of information.
[00:42:41.660 --> 00:42:45.960]   Then they align them with these frameworks,
[00:42:45.960 --> 00:42:47.200]   but there's also a mechanism
[00:42:47.200 --> 00:42:49.240]   to acquire the frameworks themselves.
[00:42:49.240 --> 00:42:52.600]   - Yeah, so it seems like the idea of frameworks
[00:42:52.600 --> 00:42:55.440]   requires some kind of collaboration with humans.
[00:42:55.440 --> 00:42:56.360]   - Absolutely.
[00:42:56.360 --> 00:42:59.360]   - So do you think of that collaboration as direct?
[00:42:59.360 --> 00:43:01.960]   - Well, and let's be clear.
[00:43:01.960 --> 00:43:04.400]   Only for the express purpose
[00:43:04.400 --> 00:43:09.400]   that you're designing an intelligence
[00:43:09.400 --> 00:43:12.060]   that can ultimately communicate with humans
[00:43:12.060 --> 00:43:15.980]   in the terms of frameworks that help them understand things.
[00:43:15.980 --> 00:43:22.500]   So to be really clear, you can independently create
[00:43:22.500 --> 00:43:26.440]   a machine learning system, an intelligence
[00:43:26.440 --> 00:43:28.500]   that I might call an alien intelligence
[00:43:28.500 --> 00:43:31.180]   that does a better job than you with some things,
[00:43:31.180 --> 00:43:33.540]   but can't explain the framework to you.
[00:43:33.540 --> 00:43:36.760]   That doesn't mean it might be better than you at the thing.
[00:43:36.760 --> 00:43:39.540]   It might be that you cannot comprehend the framework
[00:43:39.540 --> 00:43:41.400]   that it may have created for itself
[00:43:41.400 --> 00:43:43.960]   that is inexplicable to you.
[00:43:43.960 --> 00:43:45.320]   That's a reality.
[00:43:45.320 --> 00:43:48.820]   - But you're more interested in a case where you can.
[00:43:48.820 --> 00:43:51.080]   - I am, yeah.
[00:43:51.080 --> 00:43:53.120]   My sort of approach to AI
[00:43:53.120 --> 00:43:55.920]   is because I've set the goal for myself.
[00:43:55.920 --> 00:43:58.800]   I want machines to be able to ultimately communicate
[00:43:58.800 --> 00:44:01.440]   understanding with humans.
[00:44:01.440 --> 00:44:03.520]   I want them to be able to acquire and communicate,
[00:44:03.520 --> 00:44:04.780]   acquire knowledge from humans
[00:44:04.780 --> 00:44:07.060]   and communicate knowledge to humans.
[00:44:07.060 --> 00:44:10.340]   They should be using what, you know,
[00:44:10.340 --> 00:44:13.760]   inductive machine learning techniques are good at,
[00:44:13.760 --> 00:44:16.860]   which is to observe patterns of data,
[00:44:16.860 --> 00:44:19.340]   whether it be in language or whether it be in images
[00:44:19.340 --> 00:44:24.340]   or videos or whatever, to acquire these patterns,
[00:44:24.340 --> 00:44:29.420]   to induce the generalizations from those patterns,
[00:44:29.420 --> 00:44:31.300]   but then ultimately work with humans
[00:44:31.300 --> 00:44:34.680]   to connect them to frameworks, interpretations, if you will,
[00:44:34.680 --> 00:44:36.720]   that ultimately make sense to humans.
[00:44:36.720 --> 00:44:38.460]   Of course, the machine is gonna have the strength
[00:44:38.460 --> 00:44:41.460]   that it has, the richer, longer memory,
[00:44:41.460 --> 00:44:44.340]   but that, you know, it has the more rigorous
[00:44:44.340 --> 00:44:47.080]   reasoning abilities, the deeper reasoning abilities.
[00:44:47.080 --> 00:44:49.220]   So it'll be an interesting, you know,
[00:44:49.220 --> 00:44:53.220]   complementary relationship between the human and the machine.
[00:44:53.220 --> 00:44:55.140]   - Do you think that ultimately needs explainability
[00:44:55.140 --> 00:44:56.020]   like a machine?
[00:44:56.020 --> 00:44:57.900]   So if we look, we study, for example,
[00:44:57.900 --> 00:45:00.860]   Tesla autopilot a lot, where humans,
[00:45:00.860 --> 00:45:02.100]   I don't know if you've driven the vehicle,
[00:45:02.100 --> 00:45:06.460]   are aware of what, so you're basically,
[00:45:06.460 --> 00:45:10.340]   the human and machine are working together there,
[00:45:10.340 --> 00:45:12.500]   and the human is responsible for their own life
[00:45:12.500 --> 00:45:15.340]   to monitor the system, and you know,
[00:45:15.340 --> 00:45:18.420]   the system fails every few miles.
[00:45:18.420 --> 00:45:21.100]   And so there's hundreds, there's millions
[00:45:21.100 --> 00:45:23.660]   of those failures a day.
[00:45:23.660 --> 00:45:25.780]   And so that's like a moment of interaction.
[00:45:25.780 --> 00:45:26.620]   Do you see--
[00:45:26.620 --> 00:45:27.900]   - Yeah, no, that's exactly right.
[00:45:27.900 --> 00:45:31.980]   That's a moment of interaction where, you know,
[00:45:31.980 --> 00:45:33.900]   the machine has learned some stuff,
[00:45:33.900 --> 00:45:38.740]   it has a failure, somehow the failure's communicated,
[00:45:38.740 --> 00:45:41.900]   the human is now filling in the mistake, if you will,
[00:45:41.900 --> 00:45:43.640]   or maybe correcting or doing something
[00:45:43.640 --> 00:45:45.880]   that is more successful in that case,
[00:45:45.880 --> 00:45:47.900]   the computer takes that learning.
[00:45:47.900 --> 00:45:50.260]   So I believe that the collaboration
[00:45:50.260 --> 00:45:52.540]   between human and machine, I mean,
[00:45:52.540 --> 00:45:53.900]   that's sort of a primitive example
[00:45:53.900 --> 00:45:57.820]   and sort of a more, another example
[00:45:57.820 --> 00:45:59.500]   is where the machine's literally talking to you
[00:45:59.500 --> 00:46:02.700]   and saying, "Look, I'm reading this thing.
[00:46:02.700 --> 00:46:06.520]   "I know that like the next word might be this or that,
[00:46:06.520 --> 00:46:08.840]   "but I don't really understand why.
[00:46:08.840 --> 00:46:11.000]   "I have my guess, can you help me understand
[00:46:11.000 --> 00:46:14.100]   "the framework that supports this?"
[00:46:14.100 --> 00:46:16.080]   And then can kind of acquire that,
[00:46:16.080 --> 00:46:18.180]   take that and reason about it and reuse it
[00:46:18.180 --> 00:46:20.540]   the next time it's reading to try to understand something.
[00:46:20.540 --> 00:46:24.780]   Not unlike a human student might do.
[00:46:24.780 --> 00:46:27.500]   I mean, I remember when my daughter was in first grade
[00:46:27.500 --> 00:46:31.180]   and she had a reading assignment about electricity
[00:46:31.180 --> 00:46:35.540]   and somewhere in the text it says,
[00:46:35.540 --> 00:46:38.660]   "And electricity is produced by water flowing over turbines,"
[00:46:38.660 --> 00:46:39.780]   or something like that.
[00:46:39.780 --> 00:46:41.180]   And then there's a question that says,
[00:46:41.180 --> 00:46:43.180]   "Well, how is electricity created?"
[00:46:43.180 --> 00:46:45.140]   And so my daughter comes to me and says,
[00:46:45.140 --> 00:46:47.400]   "I mean, I could, you know, created and produced
[00:46:47.400 --> 00:46:49.140]   "are kind of synonyms in this case,
[00:46:49.140 --> 00:46:51.780]   "so I can go back to the text and I can copy
[00:46:51.780 --> 00:46:53.700]   "by water flowing over turbines."
[00:46:53.700 --> 00:46:56.140]   But I have no idea what that means.
[00:46:56.140 --> 00:46:57.640]   Like, I don't know how to interpret
[00:46:57.640 --> 00:47:00.400]   water flowing over turbines and what electricity even is.
[00:47:00.400 --> 00:47:04.020]   I mean, I can get the answer right by matching the text,
[00:47:04.020 --> 00:47:06.160]   but I don't have any framework for understanding
[00:47:06.160 --> 00:47:07.900]   what this means at all.
[00:47:07.900 --> 00:47:10.540]   - And framework really is, I mean, it's a set of,
[00:47:10.540 --> 00:47:14.180]   not to be mathematical, but axioms of ideas
[00:47:14.180 --> 00:47:16.380]   that you bring to the table and interpreting stuff
[00:47:16.380 --> 00:47:18.300]   and then you build those up somehow.
[00:47:18.300 --> 00:47:20.460]   You build them up with the expectation
[00:47:20.460 --> 00:47:23.780]   that there's a shared understanding of what they are.
[00:47:23.780 --> 00:47:27.800]   - Sure, yeah, it's the social, us humans.
[00:47:27.800 --> 00:47:32.060]   Do you have a sense that humans on Earth in general
[00:47:32.060 --> 00:47:36.500]   share a set of, like how many frameworks are there?
[00:47:36.500 --> 00:47:38.200]   - I mean, it depends on how you bound them, right?
[00:47:38.200 --> 00:47:39.900]   So in other words, how big or small
[00:47:39.900 --> 00:47:41.780]   like their individual scope.
[00:47:41.780 --> 00:47:44.220]   But there's lots and there are new ones.
[00:47:44.220 --> 00:47:47.620]   I think the way I think about it is kind of in a layer.
[00:47:47.620 --> 00:47:50.060]   I think of the architectures as being layered in that.
[00:47:50.060 --> 00:47:53.580]   There's a small set of primitives
[00:47:53.580 --> 00:47:56.300]   that allow you the foundation to build frameworks.
[00:47:56.300 --> 00:47:58.400]   And then there may be many frameworks,
[00:47:58.400 --> 00:48:00.620]   but you have the ability to acquire them.
[00:48:00.620 --> 00:48:03.060]   And then you have the ability to reuse them.
[00:48:03.060 --> 00:48:05.340]   I mean, one of the most compelling ways of thinking
[00:48:05.340 --> 00:48:07.820]   about this is a reasoning by analogy where I can say,
[00:48:07.820 --> 00:48:10.060]   oh, wow, I've learned something very similar.
[00:48:10.060 --> 00:48:15.280]   I never heard of this game, soccer,
[00:48:15.280 --> 00:48:17.860]   but if it's like basketball in the sense
[00:48:17.860 --> 00:48:19.620]   that the goal's like the hoop
[00:48:19.620 --> 00:48:21.020]   and I have to get the ball in the hoop
[00:48:21.020 --> 00:48:23.520]   and I have guards and I have this and I have that,
[00:48:23.520 --> 00:48:27.860]   like where are the similarities and where are the differences
[00:48:27.860 --> 00:48:29.140]   and I have a foundation now
[00:48:29.140 --> 00:48:31.380]   for interpreting this new information.
[00:48:31.380 --> 00:48:33.300]   - And then the different groups,
[00:48:33.300 --> 00:48:35.760]   like the millennials will have a framework.
[00:48:35.760 --> 00:48:39.700]   - Yeah, well, like that--
[00:48:39.700 --> 00:48:41.580]   - The Democrats and Republicans.
[00:48:41.580 --> 00:48:43.860]   Millennials, nobody wants that framework.
[00:48:43.860 --> 00:48:44.700]   - Well, I mean, I think--
[00:48:44.700 --> 00:48:45.580]   - Nobody understands it.
[00:48:45.580 --> 00:48:47.100]   - Right, I mean, they're talking about political
[00:48:47.100 --> 00:48:49.860]   and social ways of interpreting the world around them.
[00:48:49.860 --> 00:48:51.060]   And I think these frameworks
[00:48:51.060 --> 00:48:52.700]   are still largely, largely similar.
[00:48:52.700 --> 00:48:55.420]   I think they differ in maybe what some fundamental
[00:48:55.420 --> 00:48:57.340]   assumptions and values are.
[00:48:57.340 --> 00:48:59.820]   Now, from a reasoning perspective,
[00:48:59.820 --> 00:49:02.500]   like the ability to process the framework
[00:49:02.500 --> 00:49:04.120]   might not be that different.
[00:49:04.120 --> 00:49:06.520]   The implications of different fundamental values
[00:49:06.520 --> 00:49:09.420]   or fundamental assumptions in those frameworks
[00:49:09.420 --> 00:49:12.120]   may reach very different conclusions.
[00:49:12.120 --> 00:49:14.760]   So from a social perspective,
[00:49:14.760 --> 00:49:16.860]   the conclusions may be very different.
[00:49:16.860 --> 00:49:18.360]   From an intelligence perspective,
[00:49:18.360 --> 00:49:21.580]   I just followed where my assumptions took me.
[00:49:21.580 --> 00:49:23.400]   - Yeah, the process itself will look similar,
[00:49:23.400 --> 00:49:25.540]   but that's a fascinating idea
[00:49:25.540 --> 00:49:30.540]   that frameworks really help carve
[00:49:30.540 --> 00:49:33.700]   how a statement will be interpreted.
[00:49:33.700 --> 00:49:38.700]   I mean, having a Democrat and a Republican framework
[00:49:40.320 --> 00:49:42.140]   and then read the exact same statement
[00:49:42.140 --> 00:49:44.160]   and the conclusions that you derive
[00:49:44.160 --> 00:49:46.540]   will be totally different from an AI perspective
[00:49:46.540 --> 00:49:47.580]   is fascinating.
[00:49:47.580 --> 00:49:49.420]   - What we would want out of the AI
[00:49:49.420 --> 00:49:52.500]   is to be able to tell you that this perspective,
[00:49:52.500 --> 00:49:54.700]   one perspective, one set of assumptions
[00:49:54.700 --> 00:49:55.540]   is gonna lead you here,
[00:49:55.540 --> 00:49:58.660]   another set of assumptions is gonna lead you there.
[00:49:58.660 --> 00:50:01.380]   And in fact, to help people reason and say,
[00:50:01.380 --> 00:50:05.060]   oh, I see where our differences lie.
[00:50:05.060 --> 00:50:06.900]   You know, I have this fundamental belief about that,
[00:50:06.900 --> 00:50:09.160]   I have this fundamental belief about that.
[00:50:09.160 --> 00:50:10.060]   - Yeah, that's quite brilliant.
[00:50:10.060 --> 00:50:12.620]   - From my perspective, NLP,
[00:50:12.620 --> 00:50:14.140]   there's this idea that there's one way
[00:50:14.140 --> 00:50:16.100]   to really understand a statement,
[00:50:16.100 --> 00:50:18.780]   but there probably isn't.
[00:50:18.780 --> 00:50:20.140]   There's probably an infinite number of ways
[00:50:20.140 --> 00:50:21.900]   to understand a statement, depending on the framework.
[00:50:21.900 --> 00:50:23.460]   - There's lots of different interpretations
[00:50:23.460 --> 00:50:28.460]   and the broader the content, the richer it is.
[00:50:28.460 --> 00:50:35.300]   And so, you and I can have very different experiences
[00:50:35.300 --> 00:50:37.460]   with the same text, obviously.
[00:50:37.460 --> 00:50:41.360]   And if we're committed to understanding each other,
[00:50:41.360 --> 00:50:45.300]   we start, and that's the other important point,
[00:50:45.300 --> 00:50:47.780]   if we're committed to understanding each other,
[00:50:47.780 --> 00:50:51.900]   we start decomposing and breaking down our interpretation
[00:50:51.900 --> 00:50:54.060]   to its more and more primitive components
[00:50:54.060 --> 00:50:55.940]   until we get to that point where we say,
[00:50:55.940 --> 00:50:58.300]   oh, I see why we disagree.
[00:50:58.300 --> 00:51:00.540]   And we try to understand how fundamental
[00:51:00.540 --> 00:51:02.260]   that disagreement really is.
[00:51:02.260 --> 00:51:04.620]   But that requires a commitment
[00:51:04.620 --> 00:51:06.580]   to breaking down that interpretation
[00:51:06.580 --> 00:51:08.980]   in terms of that framework in a logical way.
[00:51:08.980 --> 00:51:12.300]   Otherwise, and this is why I think of AI
[00:51:12.300 --> 00:51:16.040]   as really complementing and helping human intelligence
[00:51:16.040 --> 00:51:19.900]   to overcome some of its biases and its predisposition
[00:51:19.900 --> 00:51:24.900]   to be persuaded by more shallow reasoning,
[00:51:24.900 --> 00:51:27.020]   in the sense that we get over this idea,
[00:51:27.020 --> 00:51:30.020]   well, I'm right because I'm Republican,
[00:51:30.020 --> 00:51:31.420]   or I'm right because I'm Democratic,
[00:51:31.420 --> 00:51:33.380]   and someone labeled this as a Democratic point of view,
[00:51:33.380 --> 00:51:35.420]   or it has the following keywords in it.
[00:51:35.420 --> 00:51:38.500]   And if the machine can help us break that argument down
[00:51:38.500 --> 00:51:40.540]   and say, wait a second,
[00:51:40.540 --> 00:51:42.260]   what do you really think about this?
[00:51:42.260 --> 00:51:45.460]   So essentially holding us accountable
[00:51:45.460 --> 00:51:47.540]   to doing more critical thinking.
[00:51:47.540 --> 00:51:50.300]   - We're gonna have to sit and think about that.
[00:51:50.300 --> 00:51:51.140]   I love that.
[00:51:51.140 --> 00:51:53.580]   I think that's really empowering use of AI
[00:51:53.580 --> 00:51:54.860]   for the public discourse
[00:51:54.860 --> 00:51:58.580]   that's completely disintegrating currently
[00:51:58.580 --> 00:52:00.420]   as we learn how to do it on social media.
[00:52:00.420 --> 00:52:01.260]   - That's right.
[00:52:02.460 --> 00:52:05.860]   - So one of the greatest accomplishments
[00:52:05.860 --> 00:52:10.860]   in the history of AI is Watson
[00:52:10.860 --> 00:52:13.820]   competing in the game of Jeopardy against humans.
[00:52:13.820 --> 00:52:18.940]   And you were a lead in that, a critical part of that.
[00:52:18.940 --> 00:52:20.580]   Let's start at the very basics.
[00:52:20.580 --> 00:52:22.820]   What is the game of Jeopardy?
[00:52:22.820 --> 00:52:25.820]   The game for us humans, human versus human.
[00:52:25.820 --> 00:52:30.820]   - Right, so it's to take a question
[00:52:30.840 --> 00:52:32.180]   and answer it.
[00:52:32.180 --> 00:52:34.700]   The game of Jeopardy.
[00:52:34.700 --> 00:52:35.540]   - It's just the opposite.
[00:52:35.540 --> 00:52:38.780]   - Actually, well, no, but it's not, right?
[00:52:38.780 --> 00:52:39.620]   It's really not.
[00:52:39.620 --> 00:52:41.820]   It's really to get a question and answer,
[00:52:41.820 --> 00:52:43.900]   but it's what we call a factoid question.
[00:52:43.900 --> 00:52:45.460]   So this notion of like,
[00:52:45.460 --> 00:52:46.820]   it really relates to some fact
[00:52:46.820 --> 00:52:49.260]   that few people would argue
[00:52:49.260 --> 00:52:50.540]   whether the facts are true or not.
[00:52:50.540 --> 00:52:51.580]   In fact, most people wouldn't.
[00:52:51.580 --> 00:52:53.020]   Jeopardy kind of counts on the idea
[00:52:53.020 --> 00:52:57.660]   that these statements have factual answers.
[00:52:57.660 --> 00:53:02.000]   And the idea is to, first of all,
[00:53:02.000 --> 00:53:03.760]   determine whether or not you know the answer,
[00:53:03.760 --> 00:53:06.080]   which is sort of an interesting twist.
[00:53:06.080 --> 00:53:07.880]   - So first of all, understand the question.
[00:53:07.880 --> 00:53:08.840]   - You have to understand the question.
[00:53:08.840 --> 00:53:09.860]   What is it asking?
[00:53:09.860 --> 00:53:10.760]   And that's a good point
[00:53:10.760 --> 00:53:14.440]   because the questions are not asked directly, right?
[00:53:14.440 --> 00:53:16.880]   - They're all like, the way the questions are asked
[00:53:16.880 --> 00:53:18.340]   is nonlinear.
[00:53:18.340 --> 00:53:20.680]   It's like, it's a little bit witty.
[00:53:20.680 --> 00:53:22.480]   It's a little bit playful sometimes.
[00:53:22.480 --> 00:53:25.960]   It's a little bit tricky.
[00:53:25.960 --> 00:53:27.940]   - Yeah, they're asked in, exactly,
[00:53:27.940 --> 00:53:30.620]   in numerous, witty, tricky ways.
[00:53:30.620 --> 00:53:32.580]   Exactly what they're asking is not obvious.
[00:53:32.580 --> 00:53:35.380]   It takes inexperienced humans a while to go,
[00:53:35.380 --> 00:53:36.940]   what is it even asking?
[00:53:36.940 --> 00:53:39.140]   And that's sort of an interesting realization
[00:53:39.140 --> 00:53:40.180]   that you have when somebody says,
[00:53:40.180 --> 00:53:42.460]   oh, Jeopardy is a question answering show.
[00:53:42.460 --> 00:53:43.900]   And then it's like, oh, I know a lot.
[00:53:43.900 --> 00:53:44.740]   And then you read it,
[00:53:44.740 --> 00:53:47.020]   and you're still trying to process the question,
[00:53:47.020 --> 00:53:49.140]   and the champions have answered and moved on.
[00:53:49.140 --> 00:53:51.180]   There are three questions ahead
[00:53:51.180 --> 00:53:52.740]   by the time you've figured out
[00:53:52.740 --> 00:53:54.100]   what the question even meant.
[00:53:54.100 --> 00:53:56.480]   So there's definitely an ability there
[00:53:56.480 --> 00:53:59.520]   to just parse out what the question even is.
[00:53:59.520 --> 00:54:00.860]   So that was certainly challenging.
[00:54:00.860 --> 00:54:02.240]   It's interesting, historically, though,
[00:54:02.240 --> 00:54:06.200]   if you look back at the Jeopardy games much earlier.
[00:54:06.200 --> 00:54:08.160]   - Like '60s, '70s, that kind of thing?
[00:54:08.160 --> 00:54:10.200]   - The questions were much more direct.
[00:54:10.200 --> 00:54:11.340]   They weren't quite like that.
[00:54:11.340 --> 00:54:13.720]   They got sort of more and more interesting.
[00:54:13.720 --> 00:54:14.700]   The way they asked them,
[00:54:14.700 --> 00:54:16.480]   that sort of got more and more interesting,
[00:54:16.480 --> 00:54:20.840]   and subtle, and nuanced, and humorous, and witty over time,
[00:54:20.840 --> 00:54:22.560]   which really required the human
[00:54:22.560 --> 00:54:24.280]   to kind of make the right connections
[00:54:24.280 --> 00:54:26.880]   in figuring out what the question was even asking.
[00:54:26.880 --> 00:54:29.960]   So yeah, you have to figure out the questions even asking.
[00:54:29.960 --> 00:54:32.180]   Then you have to determine whether or not
[00:54:32.180 --> 00:54:34.520]   you think you know the answer.
[00:54:34.520 --> 00:54:37.380]   And because you have to buzz in really quickly,
[00:54:37.380 --> 00:54:39.800]   you sort of have to make that determination
[00:54:39.800 --> 00:54:41.200]   as quickly as you possibly can.
[00:54:41.200 --> 00:54:43.440]   Otherwise, you lose the opportunity to buzz in.
[00:54:43.440 --> 00:54:46.520]   - Even before you really know if you know the answer.
[00:54:46.520 --> 00:54:48.640]   - I think a lot of humans will assume
[00:54:49.080 --> 00:54:53.000]   they'll look at it, process it very superficially.
[00:54:53.000 --> 00:54:56.000]   In other words, what's the topic, what are some keywords,
[00:54:56.000 --> 00:54:58.680]   and just say, do I know this area or not
[00:54:58.680 --> 00:55:00.840]   before they actually know the answer?
[00:55:00.840 --> 00:55:03.240]   Then they'll buzz in and think about it.
[00:55:03.240 --> 00:55:04.680]   So it's interesting what humans do.
[00:55:04.680 --> 00:55:06.960]   Now, some people who know all things,
[00:55:06.960 --> 00:55:08.480]   like Ken Jennings or something,
[00:55:08.480 --> 00:55:10.440]   or the more recent Big Jeopardy player,
[00:55:10.440 --> 00:55:12.440]   I mean, they'll just buzz in.
[00:55:12.440 --> 00:55:14.120]   They'll just assume they know all about Jeopardy,
[00:55:14.120 --> 00:55:15.920]   and they'll just buzz in.
[00:55:15.920 --> 00:55:18.360]   Watson, interestingly, didn't even come close
[00:55:18.360 --> 00:55:20.120]   to knowing all of Jeopardy, right?
[00:55:20.120 --> 00:55:21.800]   Watson really-- - Even at the peak,
[00:55:21.800 --> 00:55:23.800]   even at its best. - Yeah, so for example,
[00:55:23.800 --> 00:55:25.960]   I mean, we had this thing called recall,
[00:55:25.960 --> 00:55:29.400]   which is like how many of all the Jeopardy questions,
[00:55:29.400 --> 00:55:33.440]   how many could we even find the right answer for,
[00:55:33.440 --> 00:55:34.440]   like anywhere?
[00:55:34.440 --> 00:55:38.080]   Like, could we come up with, if we had a big body of knowledge
[00:55:38.080 --> 00:55:39.800]   of some of the order of several terabytes,
[00:55:39.800 --> 00:55:42.920]   I mean, from a web scale, it was actually very small.
[00:55:42.920 --> 00:55:44.360]   But from a book scale,
[00:55:44.360 --> 00:55:46.320]   I was talking about millions of books, right?
[00:55:46.320 --> 00:55:48.320]   So they're calling millions of books,
[00:55:48.320 --> 00:55:50.040]   encyclopedias, dictionaries, books,
[00:55:50.040 --> 00:55:52.280]   so it's still a ton of information.
[00:55:52.280 --> 00:55:55.200]   And for, I think it was only 85%
[00:55:55.200 --> 00:55:57.600]   was the answer anywhere to be found.
[00:55:57.600 --> 00:56:00.360]   So you're already down at that level
[00:56:00.360 --> 00:56:02.080]   just to get started, right?
[00:56:02.080 --> 00:56:07.080]   So, and so it was important to get a very quick sense of,
[00:56:07.080 --> 00:56:10.080]   do you think you know the right answer to this question?
[00:56:10.080 --> 00:56:12.200]   So we had to compute that confidence
[00:56:12.200 --> 00:56:14.320]   as quickly as we possibly could.
[00:56:14.320 --> 00:56:16.480]   So in effect, we had to answer it
[00:56:16.480 --> 00:56:21.480]   and at least spend some time essentially answering it
[00:56:21.480 --> 00:56:23.760]   and then judging the confidence
[00:56:23.760 --> 00:56:26.680]   that our answer was right
[00:56:26.680 --> 00:56:28.080]   and then deciding whether or not
[00:56:28.080 --> 00:56:30.040]   we were confident enough to buzz in.
[00:56:30.040 --> 00:56:31.920]   And that would depend on what else was going on in the game
[00:56:31.920 --> 00:56:33.440]   because there was a risk.
[00:56:33.440 --> 00:56:35.120]   So like, if you're really in a situation
[00:56:35.120 --> 00:56:38.380]   where I have to take a guess, I have very little to lose,
[00:56:38.380 --> 00:56:40.280]   then you'll buzz in with less confidence.
[00:56:40.280 --> 00:56:42.960]   - So that was accounted for the financial standings
[00:56:42.960 --> 00:56:44.320]   of the different competitors.
[00:56:44.320 --> 00:56:45.160]   - Correct.
[00:56:45.160 --> 00:56:46.640]   How much of the game was laughed,
[00:56:46.640 --> 00:56:48.280]   how much time was laughed,
[00:56:48.280 --> 00:56:50.800]   where you were in the standing and things like that.
[00:56:50.800 --> 00:56:52.880]   - How many hundreds of milliseconds
[00:56:52.880 --> 00:56:53.960]   that we're talking about here?
[00:56:53.960 --> 00:56:56.000]   Do you have a sense of what is,
[00:56:56.000 --> 00:56:58.440]   like if it's, what's the target?
[00:56:58.440 --> 00:57:03.440]   - So, I mean, we targeted answering in under three seconds
[00:57:03.440 --> 00:57:04.720]   and--
[00:57:04.720 --> 00:57:08.400]   - Buzzing in, so the decision to buzz in
[00:57:08.400 --> 00:57:10.000]   and then the actual answering,
[00:57:10.000 --> 00:57:10.840]   are those two different stages?
[00:57:10.840 --> 00:57:12.680]   - Yeah, they were two different things.
[00:57:12.680 --> 00:57:14.520]   In fact, we had multiple stages,
[00:57:14.520 --> 00:57:17.400]   whereas like we would say, let's estimate our confidence,
[00:57:17.400 --> 00:57:21.040]   which was sort of a shallow answering process.
[00:57:21.040 --> 00:57:23.840]   And then ultimately decide to buzz in,
[00:57:23.840 --> 00:57:26.360]   and then we may take another second or something
[00:57:26.360 --> 00:57:30.880]   to kind of go in there and do that.
[00:57:30.880 --> 00:57:33.920]   But by and large, we're saying like, we can't play the game.
[00:57:33.920 --> 00:57:37.600]   We can't even compete if we can't, on average,
[00:57:37.600 --> 00:57:40.360]   answer these questions in around three seconds or less.
[00:57:40.360 --> 00:57:41.720]   - So you stepped in,
[00:57:41.720 --> 00:57:45.320]   so there's these three humans playing a game,
[00:57:45.320 --> 00:57:48.000]   and you stepped in with the idea that IBM Watson
[00:57:48.000 --> 00:57:49.960]   would be one of, replace one of the humans
[00:57:49.960 --> 00:57:52.000]   and compete against two.
[00:57:52.000 --> 00:57:56.760]   Can you tell the story of Watson taking on this game?
[00:57:56.760 --> 00:57:57.600]   - Sure.
[00:57:57.600 --> 00:57:58.720]   - It seems exceptionally difficult.
[00:57:58.720 --> 00:58:03.520]   - Yeah, so the story was that it was coming up,
[00:58:03.520 --> 00:58:06.960]   I think, to the 10-year anniversary of Big Blue.
[00:58:06.960 --> 00:58:08.800]   Not Big Blue, Deep Blue.
[00:58:08.800 --> 00:58:11.960]   IBM wanted to do sort of another kind of really,
[00:58:11.960 --> 00:58:14.200]   fun challenge, public challenge
[00:58:14.200 --> 00:58:16.400]   that can bring attention to IBM research
[00:58:16.400 --> 00:58:18.640]   and the kind of the cool stuff that we were doing.
[00:58:18.640 --> 00:58:23.760]   I had been working in AI at IBM for some time.
[00:58:23.760 --> 00:58:26.520]   I had a team doing what's called
[00:58:26.520 --> 00:58:28.640]   open domain factoid question answering,
[00:58:28.640 --> 00:58:31.040]   which is, we're not gonna tell you what the questions are,
[00:58:31.040 --> 00:58:33.120]   we're not even gonna tell you what they're about.
[00:58:33.120 --> 00:58:36.840]   Can you go off and get accurate answers to these questions?
[00:58:36.840 --> 00:58:41.400]   And it was an area of AI research that I was involved in.
[00:58:41.400 --> 00:58:44.280]   And so it was a very specific passion of mine.
[00:58:44.280 --> 00:58:47.080]   Language understanding had always been a passion of mine.
[00:58:47.080 --> 00:58:49.600]   One sort of narrow slice on whether or not
[00:58:49.600 --> 00:58:50.960]   you could do anything with language
[00:58:50.960 --> 00:58:52.560]   was this notion of open domain,
[00:58:52.560 --> 00:58:54.560]   meaning I could ask anything about anything.
[00:58:54.560 --> 00:58:57.840]   Factoid, meaning it essentially had an answer,
[00:58:57.840 --> 00:59:00.920]   and being able to do that accurately and quickly.
[00:59:00.920 --> 00:59:03.960]   So that was a research area that my team had already been in.
[00:59:03.960 --> 00:59:06.320]   And so completely independently,
[00:59:06.320 --> 00:59:09.080]   several IBM executives were like, what are we gonna do?
[00:59:09.080 --> 00:59:11.060]   What's the next cool thing to do?
[00:59:11.060 --> 00:59:13.920]   And Ken Jennings was on his winning streak.
[00:59:13.920 --> 00:59:16.640]   This was like, whatever it was, 2004, I think,
[00:59:16.640 --> 00:59:18.760]   was on his winning streak.
[00:59:18.760 --> 00:59:20.920]   And someone thought, hey, that would be really cool
[00:59:20.920 --> 00:59:23.960]   if the computer can play Jeopardy.
[00:59:23.960 --> 00:59:25.760]   And so this was like in 2004,
[00:59:25.760 --> 00:59:28.040]   they were shopping this thing around.
[00:59:28.040 --> 00:59:33.040]   And everyone was telling the research execs, no way.
[00:59:33.560 --> 00:59:35.200]   Like, this is crazy.
[00:59:35.200 --> 00:59:37.080]   And we had some pretty senior people in the field
[00:59:37.080 --> 00:59:38.200]   and they're saying, no, this is crazy.
[00:59:38.200 --> 00:59:40.240]   And it would come across my desk and I was like,
[00:59:40.240 --> 00:59:43.140]   but that's kind of what I'm really interested in doing.
[00:59:43.140 --> 00:59:46.800]   But there was such this prevailing sense of,
[00:59:46.800 --> 00:59:49.440]   this is nuts, we're not gonna risk IBM's reputation on this,
[00:59:49.440 --> 00:59:50.280]   we're just not doing it.
[00:59:50.280 --> 00:59:53.200]   And this happened in 2004, it happened in 2005.
[00:59:53.200 --> 00:59:58.200]   At the end of 2006, it was coming around again.
[00:59:58.200 --> 01:00:01.120]   And I was coming off of a,
[01:00:01.120 --> 01:00:03.120]   I was doing the open domain question answering stuff,
[01:00:03.120 --> 01:00:06.000]   but I was coming off a couple other projects.
[01:00:06.000 --> 01:00:08.080]   I had a lot more time to put into this.
[01:00:08.080 --> 01:00:10.240]   And I argued that it could be done.
[01:00:10.240 --> 01:00:12.800]   And I argued it would be crazy not to do this.
[01:00:12.800 --> 01:00:15.880]   - Can I, you can be honest at this point.
[01:00:15.880 --> 01:00:17.640]   So even though you argued for it,
[01:00:17.640 --> 01:00:21.560]   what's the confidence that you had yourself, privately,
[01:00:21.560 --> 01:00:22.800]   that this could be done?
[01:00:22.800 --> 01:00:25.680]   Was, we just told the story,
[01:00:25.680 --> 01:00:27.800]   how you tell stories to convince others.
[01:00:27.800 --> 01:00:29.000]   How confident were you?
[01:00:29.000 --> 01:00:32.720]   What was your estimation of the problem at that time?
[01:00:32.720 --> 01:00:34.360]   - So I thought it was possible.
[01:00:34.360 --> 01:00:36.360]   And a lot of people thought it was impossible.
[01:00:36.360 --> 01:00:37.920]   I thought it was possible.
[01:00:37.920 --> 01:00:39.200]   A reason why I thought it was possible
[01:00:39.200 --> 01:00:41.560]   was because I did some brief experimentation.
[01:00:41.560 --> 01:00:43.520]   I knew a lot about how we were approaching
[01:00:43.520 --> 01:00:46.000]   open domain factoid question answering.
[01:00:46.000 --> 01:00:47.680]   We've been doing it for some years.
[01:00:47.680 --> 01:00:49.400]   I looked at the Jeopardy stuff.
[01:00:49.400 --> 01:00:50.960]   I said, this is gonna be hard
[01:00:50.960 --> 01:00:54.200]   for a lot of the points that we mentioned earlier.
[01:00:54.200 --> 01:00:55.760]   Hard to interpret the question,
[01:00:55.760 --> 01:00:58.960]   hard to do it quickly enough,
[01:00:58.960 --> 01:01:00.560]   hard to compute an accurate confidence.
[01:01:00.560 --> 01:01:03.120]   None of this stuff had been done well enough before.
[01:01:03.120 --> 01:01:04.720]   But a lot of the technologies we're building
[01:01:04.720 --> 01:01:07.520]   were the kinds of technologies that should work.
[01:01:07.520 --> 01:01:10.880]   But more to the point, what was driving me was,
[01:01:10.880 --> 01:01:12.840]   I was in IBM Research.
[01:01:12.840 --> 01:01:14.920]   I was a senior leader in IBM Research.
[01:01:14.920 --> 01:01:17.160]   And this is the kind of stuff we were supposed to do.
[01:01:17.160 --> 01:01:18.720]   In other words, we were basically supposed to--
[01:01:18.720 --> 01:01:19.720]   - This is the moonshot.
[01:01:19.720 --> 01:01:20.560]   This is the--
[01:01:20.560 --> 01:01:21.960]   - I mean, we were supposed to take things and say,
[01:01:21.960 --> 01:01:24.120]   this is an active research area.
[01:01:24.120 --> 01:01:27.560]   It's our obligation to kind of,
[01:01:27.560 --> 01:01:30.160]   if we have the opportunity, to push it to the limits.
[01:01:30.160 --> 01:01:32.840]   And if it doesn't work, to understand more deeply
[01:01:32.840 --> 01:01:34.800]   why we can't do it.
[01:01:34.800 --> 01:01:37.440]   And so I was very committed to that notion,
[01:01:37.440 --> 01:01:40.080]   saying, folks, this is what we do.
[01:01:40.080 --> 01:01:42.200]   It's crazy not to do this.
[01:01:42.200 --> 01:01:43.760]   This is an active research area.
[01:01:43.760 --> 01:01:45.000]   We've been in this for years.
[01:01:45.000 --> 01:01:47.480]   Why wouldn't we take this grand challenge
[01:01:47.480 --> 01:01:50.720]   and push it as hard as we can?
[01:01:50.720 --> 01:01:53.240]   At the very least, we'd be able to come out and say,
[01:01:53.240 --> 01:01:57.080]   here's why this problem is way hard.
[01:01:57.080 --> 01:01:58.700]   Here's what we've tried and here's how we failed.
[01:01:58.700 --> 01:02:03.700]   So I was very driven as a scientist from that perspective.
[01:02:03.700 --> 01:02:06.640]   And then I also argued,
[01:02:06.640 --> 01:02:08.640]   based on what we did a feasibility study,
[01:02:08.640 --> 01:02:10.960]   of why I thought it was hard but possible.
[01:02:10.960 --> 01:02:14.200]   And I showed examples of where it succeeded,
[01:02:14.200 --> 01:02:16.120]   where it failed, why it failed,
[01:02:16.120 --> 01:02:18.160]   and sort of a high-level architectural approach
[01:02:18.160 --> 01:02:19.520]   for why we should do it.
[01:02:19.520 --> 01:02:22.280]   But for the most part, at that point,
[01:02:22.280 --> 01:02:23.640]   the execs really were just looking
[01:02:23.640 --> 01:02:25.880]   for someone crazy enough to say yes,
[01:02:25.880 --> 01:02:27.880]   because for several years at that point,
[01:02:27.880 --> 01:02:29.640]   everyone had said no.
[01:02:29.640 --> 01:02:32.280]   I'm not willing to risk my reputation
[01:02:32.280 --> 01:02:34.840]   and my career on this thing.
[01:02:34.840 --> 01:02:36.720]   - Clearly, you did not have such fears.
[01:02:36.720 --> 01:02:38.000]   - I did not.
[01:02:38.000 --> 01:02:41.220]   - So you dived right in, and yet,
[01:02:41.220 --> 01:02:42.840]   from what I understand,
[01:02:42.840 --> 01:02:46.340]   it was performing very poorly in the beginning.
[01:02:46.340 --> 01:02:49.780]   So what were the initial approaches and why did they fail?
[01:02:49.780 --> 01:02:54.860]   - Well, there were lots of hard aspects to it.
[01:02:54.860 --> 01:02:57.740]   I mean, one of the reasons why prior approaches
[01:02:57.740 --> 01:03:02.400]   that we had worked on in the past failed
[01:03:02.400 --> 01:03:07.400]   was because the questions were difficult to interpret.
[01:03:07.400 --> 01:03:10.120]   Like, what are you even asking for?
[01:03:10.120 --> 01:03:12.520]   Very often, if the question was very direct,
[01:03:12.520 --> 01:03:16.640]   like what city, or what, even then it could be tricky,
[01:03:16.640 --> 01:03:21.640]   but what city or what person,
[01:03:21.640 --> 01:03:24.240]   often when it would name it very clearly,
[01:03:24.240 --> 01:03:25.440]   you would know that.
[01:03:26.020 --> 01:03:28.140]   And if there were just a small set of them,
[01:03:28.140 --> 01:03:31.580]   in other words, we're gonna ask about these five types.
[01:03:31.580 --> 01:03:33.580]   Like, it's gonna be an answer,
[01:03:33.580 --> 01:03:36.840]   and the answer will be a city in this state,
[01:03:36.840 --> 01:03:37.820]   or a city in this country.
[01:03:37.820 --> 01:03:41.060]   The answer will be a person of this type, right?
[01:03:41.060 --> 01:03:42.780]   Like an actor or whatever it is.
[01:03:42.780 --> 01:03:44.400]   But it turns out that in "Jeopardy!"
[01:03:44.400 --> 01:03:47.640]   there were like tens of thousands of these things,
[01:03:47.640 --> 01:03:49.600]   and it was a very, very long tale,
[01:03:49.600 --> 01:03:52.540]   meaning it just went on and on.
[01:03:52.540 --> 01:03:56.920]   And so even if you focused on trying to encode the types
[01:03:56.920 --> 01:03:59.880]   at the very top, like there's five that were the most,
[01:03:59.880 --> 01:04:01.620]   let's say five of the most frequent,
[01:04:01.620 --> 01:04:04.200]   you still cover a very small percentage of the data.
[01:04:04.200 --> 01:04:07.180]   So you couldn't take that approach of saying,
[01:04:07.180 --> 01:04:09.820]   I'm just going to try to collect facts
[01:04:09.820 --> 01:04:12.040]   about these five or 10 types,
[01:04:12.040 --> 01:04:14.440]   or 20 types, or 50 types, or whatever.
[01:04:14.440 --> 01:04:16.980]   So that was like one of the first things,
[01:04:16.980 --> 01:04:18.240]   like what do you do about that?
[01:04:18.240 --> 01:04:21.540]   And so we came up with an approach toward that.
[01:04:21.540 --> 01:04:23.520]   And the approach looked promising.
[01:04:23.520 --> 01:04:26.000]   And we continued to improve our ability
[01:04:26.000 --> 01:04:29.560]   to handle that problem throughout the project.
[01:04:29.560 --> 01:04:32.440]   The other issue was that right from the outset,
[01:04:32.440 --> 01:04:34.620]   I said, we're not going to,
[01:04:34.620 --> 01:04:37.660]   I committed to doing this in three to five years.
[01:04:37.660 --> 01:04:39.120]   So we did it in four.
[01:04:39.120 --> 01:04:41.000]   So I got lucky.
[01:04:41.000 --> 01:04:42.420]   But one of the things that that,
[01:04:42.420 --> 01:04:45.180]   putting that stake in the ground,
[01:04:45.180 --> 01:04:46.600]   was I, and I knew how hard
[01:04:46.600 --> 01:04:47.820]   the language understanding problem was.
[01:04:47.820 --> 01:04:51.680]   I said, we're not going to actually understand language
[01:04:51.680 --> 01:04:52.780]   to solve this problem.
[01:04:52.780 --> 01:04:57.480]   We are not going to interpret the question
[01:04:57.480 --> 01:05:00.200]   and the domain of knowledge that the question refers to
[01:05:00.200 --> 01:05:02.440]   in reason over that to answer these questions.
[01:05:02.440 --> 01:05:04.200]   Obviously, we're not going to be doing that.
[01:05:04.200 --> 01:05:08.280]   At the same time, simple search wasn't good enough
[01:05:08.280 --> 01:05:13.040]   to confidently answer with a single correct answer.
[01:05:13.040 --> 01:05:14.280]   - First of all, that's like brilliant.
[01:05:14.280 --> 01:05:16.160]   That's such a great mix of innovation
[01:05:16.160 --> 01:05:18.660]   and practical engineering, three, four, eight.
[01:05:18.660 --> 01:05:21.800]   So you're not trying to solve the general NLU problem.
[01:05:21.800 --> 01:05:25.300]   You're saying, let's solve this in any way possible.
[01:05:25.300 --> 01:05:27.860]   - Oh, yeah, no, I was committed to saying,
[01:05:27.860 --> 01:05:28.700]   look, we're just solving
[01:05:28.700 --> 01:05:31.020]   the open domain question answering problem.
[01:05:31.020 --> 01:05:33.620]   We're using Jeopardy as a driver for that.
[01:05:33.620 --> 01:05:34.460]   - Big benchmark.
[01:05:34.460 --> 01:05:36.540]   - Hard enough, big benchmark, exactly.
[01:05:36.540 --> 01:05:38.220]   And now we're--
[01:05:38.220 --> 01:05:39.060]   - How do we do it?
[01:05:39.060 --> 01:05:39.980]   - We could just like, whatever,
[01:05:39.980 --> 01:05:41.180]   like just figure out what works,
[01:05:41.180 --> 01:05:42.380]   because I want to be able to go back
[01:05:42.380 --> 01:05:44.780]   to the academic science community and say,
[01:05:44.780 --> 01:05:46.020]   here's what we tried.
[01:05:46.020 --> 01:05:46.860]   Here's what worked.
[01:05:46.860 --> 01:05:48.200]   Here's what didn't work.
[01:05:48.200 --> 01:05:50.300]   I don't want to go in and say,
[01:05:50.300 --> 01:05:52.020]   oh, I only have one technology.
[01:05:52.020 --> 01:05:52.860]   I have a hammer.
[01:05:52.860 --> 01:05:53.680]   I'm only going to use this.
[01:05:53.680 --> 01:05:54.740]   I'm going to do whatever it takes.
[01:05:54.740 --> 01:05:55.980]   I'm like, I'm going to think out of the box
[01:05:55.980 --> 01:05:57.180]   and do whatever it takes.
[01:05:57.180 --> 01:06:00.580]   And I also, there was another thing I believed.
[01:06:00.580 --> 01:06:04.620]   I believed that the fundamental NLP technologies
[01:06:04.620 --> 01:06:08.800]   and machine learning technologies would be adequate.
[01:06:08.800 --> 01:06:11.980]   And this was an issue of how do we enhance them?
[01:06:11.980 --> 01:06:13.680]   How do we integrate them?
[01:06:13.680 --> 01:06:15.340]   How do we advance them?
[01:06:15.340 --> 01:06:17.220]   So I had one researcher who came to me
[01:06:17.220 --> 01:06:18.660]   who had been working on question answering
[01:06:18.660 --> 01:06:20.180]   with me for a very long time,
[01:06:20.180 --> 01:06:24.300]   who had said, we're going to need Maxwell's equations
[01:06:24.300 --> 01:06:25.700]   for question answering.
[01:06:25.700 --> 01:06:28.740]   And I said, if we need some fundamental formula
[01:06:28.740 --> 01:06:31.860]   that breaks new ground in how we understand language,
[01:06:31.860 --> 01:06:33.100]   we're screwed.
[01:06:33.100 --> 01:06:34.420]   We're not going to get there from here.
[01:06:34.420 --> 01:06:38.060]   Like, I am not counting.
[01:06:38.060 --> 01:06:39.700]   My assumption is I'm not counting
[01:06:39.700 --> 01:06:42.420]   on some brand new invention.
[01:06:42.420 --> 01:06:46.620]   What I'm counting on is the ability to take everything
[01:06:46.620 --> 01:06:50.300]   that has done before to figure out an architecture
[01:06:50.300 --> 01:06:54.340]   on how to integrate it well, and then see where it breaks
[01:06:54.340 --> 01:06:57.260]   and make the necessary advances we need to make
[01:06:57.260 --> 01:06:58.580]   until this thing works.
[01:06:58.580 --> 01:07:00.460]   - Yeah, push it hard to see where it breaks
[01:07:00.460 --> 01:07:01.660]   and then patch it up.
[01:07:01.660 --> 01:07:03.260]   I mean, that's how people change the world.
[01:07:03.260 --> 01:07:05.980]   I mean, that's the Elon Musk approach with rockets,
[01:07:05.980 --> 01:07:08.860]   SpaceX, that's the Henry Ford and so on.
[01:07:08.860 --> 01:07:09.700]   I love it.
[01:07:09.700 --> 01:07:11.980]   - And I happen to be, in this case, I happen to be right,
[01:07:11.980 --> 01:07:14.380]   but we didn't know.
[01:07:14.380 --> 01:07:15.860]   But you kind of have to put a stake in it
[01:07:15.860 --> 01:07:17.420]   as to how you're going to run the project.
[01:07:17.420 --> 01:07:20.380]   - So, yeah, and backtracking to search.
[01:07:20.380 --> 01:07:24.340]   So, if you were to do, what's the brute force solution?
[01:07:24.340 --> 01:07:26.140]   What would you search over?
[01:07:26.140 --> 01:07:29.260]   So you have a question, how would you search
[01:07:29.260 --> 01:07:31.380]   the possible space of answers?
[01:07:31.380 --> 01:07:34.860]   - Look, web search has come a long way, even since then.
[01:07:34.860 --> 01:07:38.020]   But at the time, first of all, I mean,
[01:07:38.020 --> 01:07:40.060]   there are a couple other constraints around the problem,
[01:07:40.060 --> 01:07:40.980]   which is interesting.
[01:07:40.980 --> 01:07:43.140]   So you couldn't go out to the web,
[01:07:43.140 --> 01:07:45.020]   you couldn't search the internet.
[01:07:45.020 --> 01:07:47.620]   In other words, the AI experiment was,
[01:07:47.620 --> 01:07:50.460]   we want a self-contained device.
[01:07:50.460 --> 01:07:52.620]   The device, if the device is as big as a room,
[01:07:52.620 --> 01:07:53.900]   fine, it's as big as a room,
[01:07:53.900 --> 01:07:57.980]   but we want a self-contained device.
[01:07:57.980 --> 01:07:59.260]   You're not going out to the internet,
[01:07:59.260 --> 01:08:01.580]   you don't have a lifeline to anything.
[01:08:01.580 --> 01:08:04.280]   So it had to kind of fit in a shoebox, if you will,
[01:08:04.280 --> 01:08:06.600]   or at least the size of a few refrigerators,
[01:08:06.600 --> 01:08:08.060]   whatever it might be.
[01:08:08.060 --> 01:08:10.440]   So, but also you couldn't just get out there.
[01:08:10.440 --> 01:08:13.060]   You couldn't go off network, right, to kind of go.
[01:08:13.060 --> 01:08:14.920]   So there was that limitation.
[01:08:14.920 --> 01:08:16.900]   But then we did, but the basic thing was,
[01:08:16.900 --> 01:08:19.340]   go do a web search.
[01:08:19.340 --> 01:08:22.940]   The problem was, even when we went and did a web search,
[01:08:22.940 --> 01:08:24.540]   I don't remember exactly the numbers,
[01:08:24.540 --> 01:08:27.580]   but somewhere in the order of 65% of the time,
[01:08:27.580 --> 01:08:30.020]   the answer would be somewhere,
[01:08:30.020 --> 01:08:32.900]   you know, in the top 10 or 20 documents.
[01:08:32.900 --> 01:08:34.420]   So first of all, that's not even good enough
[01:08:34.420 --> 01:08:35.440]   to play Jeopardy.
[01:08:35.440 --> 01:08:38.180]   You know, in other words, even if you could pull the,
[01:08:38.180 --> 01:08:40.240]   even if you could perfectly pull the answer
[01:08:40.240 --> 01:08:42.200]   out of the top 20 documents,
[01:08:42.200 --> 01:08:43.960]   top 10 documents, whatever it was,
[01:08:43.960 --> 01:08:45.240]   which we didn't know how to do,
[01:08:45.240 --> 01:08:46.960]   but even if you could do that,
[01:08:46.960 --> 01:08:49.120]   you'd be, and you knew it was right,
[01:08:49.120 --> 01:08:50.720]   unless you had enough confidence in it, right?
[01:08:50.720 --> 01:08:52.480]   So you'd have to pull out the right answer.
[01:08:52.480 --> 01:08:54.840]   You'd have to have confidence it was the right answer.
[01:08:54.840 --> 01:08:56.440]   And then you'd have to do that fast enough
[01:08:56.440 --> 01:08:58.120]   to now go buzz in,
[01:08:58.120 --> 01:09:00.320]   and you'd still only get 65% of them right,
[01:09:00.320 --> 01:09:02.640]   which doesn't even put you in the winner's circle.
[01:09:02.640 --> 01:09:05.080]   Winner's circle, you have to be up over 70,
[01:09:05.080 --> 01:09:06.040]   and you have to do it really quick,
[01:09:06.040 --> 01:09:08.040]   and you have to do it really quickly.
[01:09:08.040 --> 01:09:10.100]   But now the problem is, well,
[01:09:10.100 --> 01:09:12.500]   even if I had somewhere in the top 10 documents,
[01:09:12.500 --> 01:09:14.980]   how do I figure out where in the top 10 documents
[01:09:14.980 --> 01:09:18.040]   that answer is, and how do I compute a confidence
[01:09:18.040 --> 01:09:19.740]   of all the possible candidates,
[01:09:19.740 --> 01:09:21.780]   so it's not like I go in knowing the right answer
[01:09:21.780 --> 01:09:22.620]   and have to pick it.
[01:09:22.620 --> 01:09:23.940]   I don't know the right answer.
[01:09:23.940 --> 01:09:25.580]   I have a bunch of documents,
[01:09:25.580 --> 01:09:27.100]   somewhere in there's the right answer.
[01:09:27.100 --> 01:09:29.060]   How do I, as a machine, go out and figure out
[01:09:29.060 --> 01:09:31.460]   which one's right, and then how do I score it?
[01:09:31.460 --> 01:09:35.300]   So, and now how do I deal with the fact
[01:09:35.300 --> 01:09:37.320]   that I can't actually go out to the web?
[01:09:37.320 --> 01:09:40.020]   - First of all, if you pause on that, just think about it.
[01:09:40.020 --> 01:09:42.180]   If you could go to the web,
[01:09:42.180 --> 01:09:44.260]   do you think that problem is solvable,
[01:09:44.260 --> 01:09:45.540]   if you just pause on it?
[01:09:45.540 --> 01:09:48.320]   Just thinking even beyond Jeopardy,
[01:09:48.320 --> 01:09:51.340]   do you think the problem of reading text
[01:09:51.340 --> 01:09:53.660]   to find where the answer is?
[01:09:53.660 --> 01:09:56.700]   - Well, we solved that in some definition of solved,
[01:09:56.700 --> 01:09:57.980]   given the Jeopardy challenge.
[01:09:57.980 --> 01:09:59.020]   - How did you do it for Jeopardy?
[01:09:59.020 --> 01:10:03.260]   So how do you take a body of work on a particular topic
[01:10:03.260 --> 01:10:05.940]   and extract the key pieces of information?
[01:10:05.940 --> 01:10:09.120]   - So, now forgetting about the huge volumes
[01:10:09.120 --> 01:10:10.080]   that are on the web, right?
[01:10:10.080 --> 01:10:11.240]   So now we have to figure out,
[01:10:11.240 --> 01:10:12.720]   we did a lot of source research.
[01:10:12.720 --> 01:10:15.720]   In other words, what body of knowledge
[01:10:15.720 --> 01:10:18.760]   is gonna be small enough, but broad enough,
[01:10:18.760 --> 01:10:19.840]   to answer Jeopardy?
[01:10:19.840 --> 01:10:21.960]   And we ultimately did find the body of knowledge
[01:10:21.960 --> 01:10:23.760]   that did that, I mean, it included Wikipedia
[01:10:23.760 --> 01:10:25.160]   and a bunch of other stuff.
[01:10:25.160 --> 01:10:26.720]   - So like encyclopedia type of stuff,
[01:10:26.720 --> 01:10:27.560]   I don't know if you can speak to--
[01:10:27.560 --> 01:10:28.560]   - Encyclopedia, dictionaries,
[01:10:28.560 --> 01:10:31.000]   different types of semantic resources,
[01:10:31.000 --> 01:10:33.200]   like WordNet and other types of semantic resources,
[01:10:33.200 --> 01:10:36.100]   like that, as well as like some web crawls.
[01:10:36.100 --> 01:10:39.100]   In other words, where we went out and took that content
[01:10:39.100 --> 01:10:42.500]   and then expanded it based on producing statistical,
[01:10:42.500 --> 01:10:44.660]   you know, statistically producing seeds,
[01:10:44.660 --> 01:10:47.580]   using those seeds for other searches,
[01:10:47.580 --> 01:10:48.780]   and then expanding that.
[01:10:48.780 --> 01:10:51.540]   So using these like expansion techniques,
[01:10:51.540 --> 01:10:53.660]   we went out and found enough content
[01:10:53.660 --> 01:10:54.660]   and we're like, okay, this is good.
[01:10:54.660 --> 01:10:57.020]   And even up until the end, you know,
[01:10:57.020 --> 01:10:58.420]   we had a thread of research,
[01:10:58.420 --> 01:10:59.820]   it was always trying to figure out
[01:10:59.820 --> 01:11:02.260]   what content could we efficiently include.
[01:11:02.260 --> 01:11:03.440]   - I mean, there's a lot of popular,
[01:11:03.440 --> 01:11:05.440]   like what is the church lady?
[01:11:05.440 --> 01:11:08.060]   Well, I think was one of the, like what,
[01:11:08.060 --> 01:11:12.440]   where do you, I guess that's probably an encyclopedia, so.
[01:11:12.440 --> 01:11:15.640]   - So that's an encyclopedia, but then we would take
[01:11:15.640 --> 01:11:17.800]   that stuff and we would go out and we would expand.
[01:11:17.800 --> 01:11:20.180]   In other words, we go find other content
[01:11:20.180 --> 01:11:23.000]   that wasn't in the core resources and expand it.
[01:11:23.000 --> 01:11:24.280]   You know, the amount of content,
[01:11:24.280 --> 01:11:26.200]   we grew it by an order of magnitude,
[01:11:26.200 --> 01:11:28.600]   but still, again, from a web scale perspective,
[01:11:28.600 --> 01:11:30.560]   this is very small amount of content.
[01:11:30.560 --> 01:11:31.400]   - It's very select.
[01:11:31.400 --> 01:11:33.080]   - And then we then took all that content
[01:11:33.080 --> 01:11:35.240]   and we pre-analyzed the crap out of it,
[01:11:35.240 --> 01:11:38.480]   meaning we parsed it, you know,
[01:11:38.480 --> 01:11:40.720]   broke it down into all those individual words
[01:11:40.720 --> 01:11:42.200]   and then we did semantic,
[01:11:42.200 --> 01:11:44.320]   static and semantic parses on it,
[01:11:44.320 --> 01:11:47.000]   you know, had computer algorithms that annotated it
[01:11:47.000 --> 01:11:52.000]   and we indexed that in a very rich and very fast index.
[01:11:52.000 --> 01:11:55.240]   So we have a relatively huge amount of,
[01:11:55.240 --> 01:11:57.420]   let's say the equivalent of, for the sake of argument,
[01:11:57.420 --> 01:11:59.000]   two to five million bucks.
[01:11:59.000 --> 01:12:01.840]   We've now analyzed all that, blowing up its size even more
[01:12:01.840 --> 01:12:03.600]   because now we have all this metadata
[01:12:03.600 --> 01:12:05.640]   and then we richly indexed all of that
[01:12:05.640 --> 01:12:08.960]   and by way in a giant in-memory cache.
[01:12:08.960 --> 01:12:11.980]   So Watson did not go to disk.
[01:12:11.980 --> 01:12:13.680]   - So the infrastructure component there,
[01:12:13.680 --> 01:12:15.840]   if you could just speak to it, how tough it,
[01:12:15.840 --> 01:12:20.780]   I mean, I know 2000, maybe this is 2008, nine,
[01:12:20.780 --> 01:12:24.680]   you know, that's kind of a long time ago.
[01:12:24.680 --> 01:12:25.920]   - Right.
[01:12:25.920 --> 01:12:27.880]   - How hard is it to use multiple machines?
[01:12:27.880 --> 01:12:29.880]   Like how hard is the infrastructure component,
[01:12:29.880 --> 01:12:31.640]   the hardware component?
[01:12:31.640 --> 01:12:33.840]   - So we used IBM hardware.
[01:12:33.840 --> 01:12:36.080]   We had something like, I forget exactly,
[01:12:36.080 --> 01:12:40.760]   but close to 3000 cores completely connected.
[01:12:40.760 --> 01:12:42.080]   So you had a switch where, you know,
[01:12:42.080 --> 01:12:43.840]   every CPU was connected to every other CPU.
[01:12:43.840 --> 01:12:46.120]   - And they were sharing memory in some kind of way.
[01:12:46.120 --> 01:12:48.040]   - Large shared memory, right?
[01:12:48.040 --> 01:12:50.800]   And all this data was pre-analyzed
[01:12:50.800 --> 01:12:54.920]   and put into a very fast indexing structure
[01:12:54.920 --> 01:12:58.320]   that was all in memory.
[01:12:58.320 --> 01:13:01.420]   And then we took that question,
[01:13:01.420 --> 01:13:04.440]   we would analyze the question.
[01:13:04.440 --> 01:13:07.240]   So all the content was now pre-analyzed.
[01:13:07.240 --> 01:13:10.840]   So if I went and tried to find a piece of content,
[01:13:10.840 --> 01:13:12.600]   it would come back with all the metadata
[01:13:12.600 --> 01:13:14.600]   that we had pre-computed.
[01:13:14.600 --> 01:13:16.960]   - How do you shove that question?
[01:13:16.960 --> 01:13:20.040]   How do you connect the big stuff,
[01:13:20.040 --> 01:13:21.560]   the big knowledge base of the metadata
[01:13:21.880 --> 01:13:25.080]   that's indexed to the simple little witty,
[01:13:25.080 --> 01:13:27.000]   confusing question?
[01:13:27.000 --> 01:13:27.840]   - Right.
[01:13:27.840 --> 01:13:31.360]   So therein lies, you know, the Watson architecture.
[01:13:31.360 --> 01:13:33.000]   So we would take the question,
[01:13:33.000 --> 01:13:34.760]   we would analyze the question.
[01:13:34.760 --> 01:13:37.080]   So which means that we would parse it
[01:13:37.080 --> 01:13:38.800]   and interpret it a bunch of different ways.
[01:13:38.800 --> 01:13:40.880]   We'd try to figure out what is it asking about?
[01:13:40.880 --> 01:13:44.440]   So we would come, we had multiple strategies
[01:13:44.440 --> 01:13:47.240]   to kind of determine what was it asking for.
[01:13:47.240 --> 01:13:49.520]   That might be represented as a simple string,
[01:13:49.520 --> 01:13:50.640]   a character string,
[01:13:51.440 --> 01:13:53.160]   or something we would connect back
[01:13:53.160 --> 01:13:54.840]   to different semantic types
[01:13:54.840 --> 01:13:56.160]   that were from existing resources.
[01:13:56.160 --> 01:13:58.160]   So anyway, the bottom line is we would do
[01:13:58.160 --> 01:14:00.440]   a bunch of analysis in the question.
[01:14:00.440 --> 01:14:02.160]   And question analysis had to finish,
[01:14:02.160 --> 01:14:04.240]   and it had to finish fast.
[01:14:04.240 --> 01:14:05.360]   So we do the question analysis
[01:14:05.360 --> 01:14:07.960]   because then from the question analysis,
[01:14:07.960 --> 01:14:09.840]   we would now produce searches.
[01:14:09.840 --> 01:14:12.720]   So we would, and we had built,
[01:14:12.720 --> 01:14:16.160]   using open source search engines, we modified them.
[01:14:16.160 --> 01:14:19.060]   We had a number of different search engines we would use
[01:14:19.060 --> 01:14:20.760]   that had different characteristics.
[01:14:20.760 --> 01:14:22.560]   We went in there and engineered
[01:14:22.560 --> 01:14:24.520]   and modified those search engines,
[01:14:24.520 --> 01:14:28.520]   ultimately to now take our question analysis,
[01:14:28.520 --> 01:14:32.120]   produce multiple queries based on different interpretations
[01:14:32.120 --> 01:14:33.320]   of the question,
[01:14:33.320 --> 01:14:36.480]   and fire out a whole bunch of searches in parallel.
[01:14:36.480 --> 01:14:41.080]   And they would come back with passages.
[01:14:41.080 --> 01:14:43.040]   So these are passive search algorithms.
[01:14:43.040 --> 01:14:44.840]   They would come back with passages.
[01:14:44.840 --> 01:14:48.200]   And so now let's say you had a thousand passages.
[01:14:48.200 --> 01:14:51.800]   Now for each passage, you parallelize again.
[01:14:51.800 --> 01:14:56.280]   So you went out and you parallelized the search.
[01:14:56.280 --> 01:14:57.480]   Each search would now come back
[01:14:57.480 --> 01:14:59.720]   with a whole bunch of passages.
[01:14:59.720 --> 01:15:01.440]   Maybe you had a total of a thousand
[01:15:01.440 --> 01:15:03.280]   or 5,000, whatever passages.
[01:15:03.280 --> 01:15:05.800]   For each passage now, you'd go and figure out
[01:15:05.800 --> 01:15:07.160]   whether or not there was a candidate,
[01:15:07.160 --> 01:15:09.180]   we'd call a candidate answer in there.
[01:15:09.180 --> 01:15:12.160]   So you had a whole bunch of other algorithms
[01:15:12.160 --> 01:15:13.840]   that would find candidate answers,
[01:15:13.840 --> 01:15:16.220]   possible answers to the question.
[01:15:16.220 --> 01:15:18.240]   And so you had candidate answers,
[01:15:18.240 --> 01:15:20.160]   called candidate answers generators,
[01:15:20.160 --> 01:15:21.360]   a whole bunch of those.
[01:15:21.360 --> 01:15:23.680]   So for every one of these components,
[01:15:23.680 --> 01:15:25.480]   the team was constantly doing research,
[01:15:25.480 --> 01:15:27.920]   coming up better ways to generate search queries
[01:15:27.920 --> 01:15:28.800]   from the questions,
[01:15:28.800 --> 01:15:30.520]   better ways to analyze the question,
[01:15:30.520 --> 01:15:31.960]   better ways to generate candidates.
[01:15:31.960 --> 01:15:35.880]   - And speed, so better is accuracy and speed.
[01:15:35.880 --> 01:15:38.600]   - Correct, so right, speed and accuracy,
[01:15:38.600 --> 01:15:40.960]   for the most part, were separated.
[01:15:40.960 --> 01:15:42.640]   We handled that sort of in separate ways.
[01:15:42.640 --> 01:15:45.640]   Like I focus purely on accuracy and inaccuracy,
[01:15:45.640 --> 01:15:47.360]   are we ultimately getting more questions
[01:15:47.360 --> 01:15:49.200]   and producing more accurate confidences?
[01:15:49.200 --> 01:15:50.620]   And then a whole nother team
[01:15:50.620 --> 01:15:52.880]   that was constantly analyzing the workflow
[01:15:52.880 --> 01:15:54.240]   to find the bottlenecks,
[01:15:54.240 --> 01:15:56.200]   and then figuring out how to both parallelize
[01:15:56.200 --> 01:15:58.520]   and drive the algorithm speed.
[01:15:58.520 --> 01:16:00.400]   But anyway, so now think of it like,
[01:16:00.400 --> 01:16:02.160]   you have this big fan out now, right?
[01:16:02.160 --> 01:16:04.040]   Because you had multiple queries,
[01:16:04.040 --> 01:16:07.360]   now you have thousands of candidate answers.
[01:16:07.360 --> 01:16:10.400]   For each candidate answer, you're gonna score it.
[01:16:10.400 --> 01:16:12.840]   So you're gonna use all the data that built up.
[01:16:12.840 --> 01:16:15.880]   You're gonna use the question analysis.
[01:16:15.880 --> 01:16:17.960]   You're gonna use how the query was generated.
[01:16:17.960 --> 01:16:20.240]   You're gonna use the passage itself.
[01:16:20.240 --> 01:16:22.000]   And you're gonna use the candidate answer
[01:16:22.000 --> 01:16:23.280]   that was generated.
[01:16:23.280 --> 01:16:25.840]   And you're gonna score that.
[01:16:25.840 --> 01:16:28.400]   So now we have a group of researchers
[01:16:28.400 --> 01:16:30.120]   coming up with scorers.
[01:16:30.120 --> 01:16:32.280]   There are hundreds of different scorers.
[01:16:32.280 --> 01:16:34.600]   So now you're getting a fan out of it again,
[01:16:34.600 --> 01:16:37.400]   from however many candidate answers you have,
[01:16:37.400 --> 01:16:39.280]   to all the different scorers.
[01:16:39.280 --> 01:16:41.240]   So if you have 200 different scorers
[01:16:41.240 --> 01:16:42.440]   and you have a thousand candidates,
[01:16:42.440 --> 01:16:45.200]   now you have 200,000 scores.
[01:16:45.200 --> 01:16:47.160]   And so now you gotta figure out,
[01:16:47.160 --> 01:16:52.400]   how do I now rank these answers
[01:16:52.400 --> 01:16:54.520]   based on the scores that came back?
[01:16:54.520 --> 01:16:56.360]   And I wanna rank them based on the likelihood
[01:16:56.360 --> 01:16:58.680]   that they're a correct answer to the question.
[01:16:58.680 --> 01:17:01.400]   So every scorer was its own research project.
[01:17:01.400 --> 01:17:02.360]   - What do you mean by scorer?
[01:17:02.360 --> 01:17:04.080]   So is that the annotation process
[01:17:04.080 --> 01:17:06.520]   of basically a human being saying
[01:17:06.520 --> 01:17:09.360]   that this answer has a quality of--
[01:17:09.360 --> 01:17:10.800]   - Think of it, if you wanna think of it,
[01:17:10.800 --> 01:17:13.240]   what you're doing, if you wanna think about
[01:17:13.240 --> 01:17:14.080]   what a human would be doing,
[01:17:14.080 --> 01:17:17.080]   a human would be looking at a possible answer.
[01:17:17.080 --> 01:17:20.920]   They'd be reading the, Emily Dickinson,
[01:17:20.920 --> 01:17:23.560]   they'd be reading the passage in which that occurred.
[01:17:23.560 --> 01:17:25.400]   They'd be looking at the question
[01:17:25.400 --> 01:17:28.400]   and they'd be making a decision of how likely it is
[01:17:28.400 --> 01:17:32.320]   that Emily Dickinson, given this evidence in this passage,
[01:17:32.320 --> 01:17:33.960]   is the right answer to that question.
[01:17:33.960 --> 01:17:34.800]   - Got it.
[01:17:34.800 --> 01:17:36.240]   So that's the annotation task.
[01:17:36.240 --> 01:17:37.080]   That's the annotation process.
[01:17:37.080 --> 01:17:38.840]   - That's the scoring task.
[01:17:38.840 --> 01:17:41.320]   - But scoring implies zero to one kind of continuous--
[01:17:41.320 --> 01:17:43.000]   - That's right, you give it a zero to one score.
[01:17:43.000 --> 01:17:44.360]   - So it's not a binary--
[01:17:44.360 --> 01:17:46.000]   - No, you give it a score.
[01:17:46.000 --> 01:17:48.440]   Give it a zero, yeah, exactly, a zero to one score.
[01:17:48.440 --> 01:17:50.520]   - So but humans give different scores,
[01:17:50.520 --> 01:17:52.960]   so you have to somehow normalize and all that kind of stuff
[01:17:52.960 --> 01:17:54.240]   that deal with all that complexity.
[01:17:54.240 --> 01:17:55.760]   - Depends on what your strategy is.
[01:17:55.760 --> 01:17:57.080]   We both, we--
[01:17:57.080 --> 01:17:58.040]   - Could be relative, too.
[01:17:58.040 --> 01:17:59.440]   It could be--
[01:17:59.440 --> 01:18:01.960]   - We actually looked at the raw scores as well,
[01:18:01.960 --> 01:18:04.980]   standardized scores, because humans are not involved in this.
[01:18:04.980 --> 01:18:05.920]   Humans are not involved.
[01:18:05.920 --> 01:18:08.680]   - Sorry, so I'm misunderstanding the process here.
[01:18:08.680 --> 01:18:10.440]   There's passages.
[01:18:10.440 --> 01:18:13.320]   Where is the ground truth coming from?
[01:18:13.320 --> 01:18:15.920]   - Ground truth is only the answers to the questions.
[01:18:15.920 --> 01:18:17.920]   - So it's end to end.
[01:18:17.920 --> 01:18:19.000]   - It's end to end.
[01:18:19.000 --> 01:18:22.360]   So I was always driving end to end performance.
[01:18:22.360 --> 01:18:27.360]   It was a very interesting engineering approach,
[01:18:27.360 --> 01:18:30.040]   and ultimately scientific and research approach,
[01:18:30.040 --> 01:18:31.200]   always driving end to end.
[01:18:31.200 --> 01:18:36.200]   Now, that's not to say we wouldn't make hypotheses
[01:18:38.560 --> 01:18:42.120]   that individual component performance
[01:18:42.120 --> 01:18:44.400]   was related in some way to end to end performance.
[01:18:44.400 --> 01:18:47.040]   Of course we would, because people would have to
[01:18:47.040 --> 01:18:48.860]   build individual components.
[01:18:48.860 --> 01:18:50.840]   But ultimately, to get your component
[01:18:50.840 --> 01:18:52.340]   integrated into the system,
[01:18:52.340 --> 01:18:54.960]   you had to show impact on end to end performance,
[01:18:54.960 --> 01:18:56.320]   question answering performance.
[01:18:56.320 --> 01:18:58.360]   - There's many very smart people working on this,
[01:18:58.360 --> 01:19:01.520]   and they're basically trying to sell their ideas
[01:19:01.520 --> 01:19:03.400]   as a component that should be part of the system.
[01:19:03.400 --> 01:19:04.560]   - That's right.
[01:19:04.560 --> 01:19:07.320]   And they would do research on their component,
[01:19:07.320 --> 01:19:09.720]   and they would say things like,
[01:19:09.720 --> 01:19:13.120]   I'm gonna improve this as a candidate generator,
[01:19:13.120 --> 01:19:15.840]   or I'm gonna improve this as a question score,
[01:19:15.840 --> 01:19:19.040]   or as a passage score, I'm gonna improve this,
[01:19:19.040 --> 01:19:23.920]   or as a parser, and I can improve it by 2%
[01:19:23.920 --> 01:19:26.720]   on its component metric, like a better parse,
[01:19:26.720 --> 01:19:29.200]   or a better candidate, or a better type estimation,
[01:19:29.200 --> 01:19:30.200]   or whatever it is.
[01:19:30.200 --> 01:19:32.600]   And then I would say, I need to understand
[01:19:32.600 --> 01:19:35.320]   how the improvement on that component metric
[01:19:35.320 --> 01:19:37.720]   is gonna affect the end to end performance.
[01:19:37.720 --> 01:19:40.520]   If you can't estimate that, and can't do experiments
[01:19:40.520 --> 01:19:43.360]   to demonstrate that, it doesn't get in.
[01:19:43.360 --> 01:19:47.520]   - That's like the best run AI project I've ever heard.
[01:19:47.520 --> 01:19:49.040]   That's awesome, okay.
[01:19:49.040 --> 01:19:51.800]   What breakthrough would you say,
[01:19:51.800 --> 01:19:54.240]   like I'm sure there's a lot of day to day breakthroughs,
[01:19:54.240 --> 01:19:55.600]   but was there like a breakthrough
[01:19:55.600 --> 01:19:57.880]   that really helped improve performance?
[01:19:57.880 --> 01:20:00.180]   Like where people began to believe?
[01:20:00.180 --> 01:20:02.480]   Or is it just a gradual process?
[01:20:02.480 --> 01:20:04.520]   - Well, I think it was a gradual process,
[01:20:04.520 --> 01:20:08.960]   but one of the things that I think gave people confidence
[01:20:08.960 --> 01:20:13.960]   that we can get there was that as we follow this procedure
[01:20:13.960 --> 01:20:19.160]   of different ideas, build different components,
[01:20:19.160 --> 01:20:21.200]   plug them into the architecture, run the system,
[01:20:21.200 --> 01:20:24.680]   see how we do, do the error analysis,
[01:20:24.680 --> 01:20:28.120]   start off new research projects to improve things,
[01:20:28.120 --> 01:20:33.120]   and the very important idea that the individual component
[01:20:33.640 --> 01:20:38.640]   work did not have to deeply understand everything
[01:20:38.640 --> 01:20:42.240]   that was going on with every other component.
[01:20:42.240 --> 01:20:45.080]   And this is where we leverage machine learning
[01:20:45.080 --> 01:20:47.400]   in a very important way.
[01:20:47.400 --> 01:20:50.360]   So while individual components could be statistically driven
[01:20:50.360 --> 01:20:52.760]   machine learning components, some of them were heuristic,
[01:20:52.760 --> 01:20:54.640]   some of them were machine learning components,
[01:20:54.640 --> 01:20:58.120]   the system has a whole combined all the scores
[01:20:58.120 --> 01:20:59.280]   using machine learning.
[01:21:00.560 --> 01:21:03.440]   This was critical because that way you can divide
[01:21:03.440 --> 01:21:04.400]   and conquer.
[01:21:04.400 --> 01:21:07.520]   So you can say, okay, you work on your candidate generator,
[01:21:07.520 --> 01:21:09.780]   or you work on this approach to answer scoring,
[01:21:09.780 --> 01:21:11.800]   you work on this approach to type scoring,
[01:21:11.800 --> 01:21:14.520]   you work on this approach to passage search
[01:21:14.520 --> 01:21:16.360]   or to passage selection and so forth.
[01:21:16.360 --> 01:21:19.600]   But when we just plug it in,
[01:21:19.600 --> 01:21:22.080]   and we had enough training data to say,
[01:21:22.080 --> 01:21:26.560]   now we can train and figure out how do we weigh
[01:21:26.560 --> 01:21:29.360]   all the scores relative to each other
[01:21:29.360 --> 01:21:31.920]   based on predicting the outcome,
[01:21:31.920 --> 01:21:33.960]   which is right or wrong on Jeopardy.
[01:21:33.960 --> 01:21:36.800]   And we had enough training data to do that.
[01:21:36.800 --> 01:21:40.680]   So this enabled people to work independently
[01:21:40.680 --> 01:21:43.400]   and to let the machine learning do the integration.
[01:21:43.400 --> 01:21:45.160]   - Beautiful, so yeah, the machine learning
[01:21:45.160 --> 01:21:48.280]   is doing the fusion, and then it's a human orchestrated
[01:21:48.280 --> 01:21:50.520]   ensemble of different approaches.
[01:21:50.520 --> 01:21:51.980]   That's great.
[01:21:51.980 --> 01:21:55.600]   Still impressive that you're able to get it done
[01:21:55.600 --> 01:21:56.540]   in a few years.
[01:21:57.680 --> 01:22:00.440]   That's not obvious to me that it's doable
[01:22:00.440 --> 01:22:03.400]   if I just put myself in that mindset.
[01:22:03.400 --> 01:22:05.920]   But when you look back at the Jeopardy challenge,
[01:22:05.920 --> 01:22:10.280]   again, when you're looking up at the stars,
[01:22:10.280 --> 01:22:11.760]   what are you most proud of?
[01:22:11.760 --> 01:22:15.440]   Just looking back at those days.
[01:22:15.440 --> 01:22:21.760]   - I'm most proud of my...
[01:22:21.760 --> 01:22:24.520]   (mouse clicking)
[01:22:24.520 --> 01:22:30.920]   My commitment and my team's commitment
[01:22:30.920 --> 01:22:33.920]   to be true to the science,
[01:22:33.920 --> 01:22:38.100]   to not be afraid to fail.
[01:22:38.100 --> 01:22:41.600]   - That's beautiful because there's so much pressure
[01:22:41.600 --> 01:22:44.440]   because it is a public event, it is a public show,
[01:22:44.440 --> 01:22:47.040]   that you were dedicated to the idea.
[01:22:47.040 --> 01:22:47.880]   - That's right.
[01:22:50.520 --> 01:22:53.200]   - Do you think it was a success?
[01:22:53.200 --> 01:22:55.360]   In the eyes of the world, it was a success.
[01:22:55.360 --> 01:22:59.720]   By your, I'm sure, exceptionally high standards,
[01:22:59.720 --> 01:23:03.740]   is there something you regret you would do differently?
[01:23:03.740 --> 01:23:07.920]   - It was a success.
[01:23:07.920 --> 01:23:10.160]   It was a success for our goal.
[01:23:10.160 --> 01:23:13.400]   Our goal was to build the most advanced
[01:23:13.400 --> 01:23:15.400]   open domain question answering system.
[01:23:15.400 --> 01:23:19.520]   We went back to the old problems that we used to try
[01:23:19.520 --> 01:23:23.160]   to solve and we did dramatically better on all of them,
[01:23:23.160 --> 01:23:26.120]   as well as we beat Jeopardy.
[01:23:26.120 --> 01:23:27.760]   So we won at Jeopardy.
[01:23:27.760 --> 01:23:30.160]   So it was a success.
[01:23:30.160 --> 01:23:36.160]   I worry that the world would not understand it as a success
[01:23:36.160 --> 01:23:38.720]   because it came down to only one game.
[01:23:38.720 --> 01:23:40.440]   And I knew statistically speaking,
[01:23:40.440 --> 01:23:42.280]   this can be a huge technical success
[01:23:42.280 --> 01:23:43.880]   and we could still lose that one game.
[01:23:43.880 --> 01:23:47.260]   And that's a whole nother theme of the journey.
[01:23:47.260 --> 01:23:50.280]   But it was a success.
[01:23:50.280 --> 01:23:53.640]   It was not a success in natural language understanding,
[01:23:53.640 --> 01:23:54.940]   but that was not the goal.
[01:23:54.940 --> 01:23:59.840]   - Yeah, that was, but I would argue,
[01:23:59.840 --> 01:24:04.120]   I understand what you're saying in terms of the science,
[01:24:04.120 --> 01:24:07.640]   but I would argue that the inspiration of it, right,
[01:24:07.640 --> 01:24:11.200]   not a success in terms of solving
[01:24:11.200 --> 01:24:12.800]   natural language understanding,
[01:24:12.800 --> 01:24:15.400]   but it was a success of being an inspiration
[01:24:16.280 --> 01:24:17.860]   to future challenges.
[01:24:17.860 --> 01:24:18.820]   - Absolutely.
[01:24:18.820 --> 01:24:21.140]   - That drive future efforts.
[01:24:21.140 --> 01:24:23.740]   What's the difference between how human being
[01:24:23.740 --> 01:24:26.860]   compete in Jeopardy and how Watson does it?
[01:24:26.860 --> 01:24:28.740]   That's important in terms of intelligence.
[01:24:28.740 --> 01:24:31.380]   - Yeah, so that actually came up very early on
[01:24:31.380 --> 01:24:32.600]   in the project also.
[01:24:32.600 --> 01:24:35.140]   In fact, I had people who wanted to be on the project
[01:24:35.140 --> 01:24:39.060]   who were early on, who sort of approached me
[01:24:39.060 --> 01:24:40.820]   once I committed to do it,
[01:24:40.820 --> 01:24:44.300]   that wanted to think about how humans do it.
[01:24:44.300 --> 01:24:47.080]   And they were, you know, from a cognition perspective,
[01:24:47.080 --> 01:24:49.920]   like human cognition and how that should play.
[01:24:49.920 --> 01:24:52.200]   And I would not take them on the project
[01:24:52.200 --> 01:24:55.800]   because another assumption or another stake
[01:24:55.800 --> 01:24:58.480]   I put in the ground was I don't really care
[01:24:58.480 --> 01:25:00.000]   how humans do this.
[01:25:00.000 --> 01:25:01.480]   - At least in the context of this project.
[01:25:01.480 --> 01:25:03.880]   - I need to build, in the context of this project,
[01:25:03.880 --> 01:25:07.000]   in NLU and in building an AI that understands
[01:25:07.000 --> 01:25:09.660]   how it needs to ultimately communicate with humans,
[01:25:09.660 --> 01:25:11.280]   I very much care.
[01:25:11.280 --> 01:25:16.280]   So it wasn't that I didn't care in general.
[01:25:16.280 --> 01:25:20.780]   In fact, as an AI scientist, I care a lot about that,
[01:25:20.780 --> 01:25:22.580]   but I'm also a practical engineer
[01:25:22.580 --> 01:25:25.540]   and I committed to getting this thing done
[01:25:25.540 --> 01:25:27.500]   and I wasn't gonna get distracted.
[01:25:27.500 --> 01:25:30.780]   I had to kind of say, like, if I'm gonna get this done,
[01:25:30.780 --> 01:25:33.300]   I'm gonna chart this path and this path says,
[01:25:33.300 --> 01:25:34.940]   we're gonna engineer a machine
[01:25:34.940 --> 01:25:37.580]   that's gonna get this thing done.
[01:25:37.640 --> 01:25:41.560]   And we know what search and NLP can do.
[01:25:41.560 --> 01:25:44.200]   We have to build on that foundation.
[01:25:44.200 --> 01:25:46.320]   If I come in and take a different approach
[01:25:46.320 --> 01:25:48.100]   and start wondering about how the human mind
[01:25:48.100 --> 01:25:49.760]   might or might not do this,
[01:25:49.760 --> 01:25:54.400]   I'm not gonna get there from here in the time frame.
[01:25:54.400 --> 01:25:56.680]   - I think that's a great way to lead the team.
[01:25:56.680 --> 01:25:59.240]   But now that it's done and then one,
[01:25:59.240 --> 01:26:02.560]   when you look back, analyze what's the difference actually.
[01:26:02.560 --> 01:26:05.520]   - Right, so I was a little bit surprised actually
[01:26:05.520 --> 01:26:09.060]   to discover over time, as this would come up
[01:26:09.060 --> 01:26:11.160]   from time to time and we'd reflect on it,
[01:26:11.160 --> 01:26:15.020]   and talking to Ken Jennings a little bit
[01:26:15.020 --> 01:26:17.060]   and hearing Ken Jennings talk about
[01:26:17.060 --> 01:26:18.900]   how he answered questions,
[01:26:18.900 --> 01:26:21.300]   that it might have been closer to the way humans
[01:26:21.300 --> 01:26:24.740]   answer questions than I might have imagined previously.
[01:26:24.740 --> 01:26:27.900]   - 'Cause humans are probably in the game of Jeopardy
[01:26:27.900 --> 01:26:29.620]   at the level of Ken Jennings,
[01:26:29.620 --> 01:26:34.620]   probably also cheating their way to winning, right?
[01:26:34.980 --> 01:26:36.660]   - Well, they're doing shallow analysis.
[01:26:36.660 --> 01:26:39.380]   - Shallow, the fastest possible.
[01:26:39.380 --> 01:26:40.900]   - They're doing shallow analysis.
[01:26:40.900 --> 01:26:44.900]   So they are very quickly analyzing the question
[01:26:44.900 --> 01:26:49.900]   and coming up with some key vectors or cues, if you will.
[01:26:49.900 --> 01:26:51.100]   And they're taking those cues
[01:26:51.100 --> 01:26:52.900]   and they're very quickly going through
[01:26:52.900 --> 01:26:54.920]   their library of stuff,
[01:26:54.920 --> 01:26:57.740]   not deeply reasoning about what's going on.
[01:26:57.740 --> 01:27:00.620]   And then sort of like a lots of different,
[01:27:00.620 --> 01:27:03.220]   like what we call these scores,
[01:27:03.220 --> 01:27:06.100]   would kind of score that in a very shallow way
[01:27:06.100 --> 01:27:08.940]   and then say, oh, boom, that's what it is.
[01:27:08.940 --> 01:27:12.460]   And so it's interesting as we reflected on that,
[01:27:12.460 --> 01:27:16.060]   so we may be doing something that's not too far off
[01:27:16.060 --> 01:27:17.260]   from the way humans do it,
[01:27:17.260 --> 01:27:21.460]   but we certainly didn't approach it by saying,
[01:27:21.460 --> 01:27:22.700]   how would a human do this?
[01:27:22.700 --> 01:27:24.660]   Now in elemental cognition,
[01:27:24.660 --> 01:27:27.340]   like the project I'm leading now,
[01:27:27.340 --> 01:27:28.780]   we ask those questions all the time,
[01:27:28.780 --> 01:27:31.700]   because ultimately we're trying to do something
[01:27:31.700 --> 01:27:35.100]   that is to make the intelligence of the machine
[01:27:35.100 --> 01:27:37.780]   and the intelligence of the human very compatible.
[01:27:37.780 --> 01:27:39.500]   Well, compatible in the sense they can communicate
[01:27:39.500 --> 01:27:42.580]   with one another and they can reason
[01:27:42.580 --> 01:27:44.540]   with this shared understanding.
[01:27:44.540 --> 01:27:48.060]   So how they think about things and how they build answers,
[01:27:48.060 --> 01:27:49.780]   how they build explanations
[01:27:49.780 --> 01:27:52.140]   becomes a very important question to consider.
[01:27:52.140 --> 01:27:56.920]   - So what's the difference between this open domain,
[01:27:56.920 --> 01:28:01.920]   but cold constructed question answering of Jeopardy
[01:28:01.920 --> 01:28:07.360]   and more something that requires understanding
[01:28:07.360 --> 01:28:10.280]   for shared communication with humans and machines?
[01:28:10.280 --> 01:28:13.360]   - Yeah, well, this goes back to the interpretation
[01:28:13.360 --> 01:28:14.800]   of what we were talking about before.
[01:28:14.800 --> 01:28:15.640]   - Framework.
[01:28:15.640 --> 01:28:18.560]   - Jeopardy, the system's not trying to interpret
[01:28:18.560 --> 01:28:20.680]   the question and it's not interpreting the content
[01:28:20.680 --> 01:28:23.880]   that's reusing with regard to any particular framework.
[01:28:23.880 --> 01:28:26.880]   I mean, it is parsing it and parsing the content
[01:28:26.880 --> 01:28:29.440]   and using grammatical cues and stuff like that.
[01:28:29.440 --> 01:28:31.680]   So if you think of grammar as a human framework,
[01:28:31.680 --> 01:28:33.400]   in some sense it has that.
[01:28:33.400 --> 01:28:36.880]   But when you get into the richer semantic frameworks,
[01:28:36.880 --> 01:28:40.080]   what are people, how do they think, what motivates them?
[01:28:40.080 --> 01:28:41.640]   What are the events that are occurring
[01:28:41.640 --> 01:28:43.280]   and why are they occurring and what causes
[01:28:43.280 --> 01:28:47.440]   what else to happen and where are things in time and space?
[01:28:47.440 --> 01:28:51.280]   And like when you start thinking about how humans formulate
[01:28:51.280 --> 01:28:54.000]   and structure the knowledge that they acquire in their head,
[01:28:54.000 --> 01:28:55.400]   it wasn't doing any of that.
[01:28:56.400 --> 01:29:01.400]   - What do you think are the essential challenges
[01:29:01.400 --> 01:29:05.840]   of free-flowing communication, free-flowing dialogue
[01:29:05.840 --> 01:29:09.040]   versus question answering even with a framework
[01:29:09.040 --> 01:29:10.560]   of the interpretation?
[01:29:10.560 --> 01:29:11.400]   Dialogue.
[01:29:11.400 --> 01:29:12.320]   - Yep.
[01:29:12.320 --> 01:29:14.960]   - Do you see free-flowing dialogue
[01:29:14.960 --> 01:29:19.960]   as fundamentally more difficult than question answering
[01:29:19.960 --> 01:29:23.560]   even with shared interpretation?
[01:29:23.560 --> 01:29:26.640]   - So dialogue is important in a number of different ways.
[01:29:26.640 --> 01:29:27.480]   I mean, it's a challenge.
[01:29:27.480 --> 01:29:30.520]   So first of all, when I think about the machine that,
[01:29:30.520 --> 01:29:33.280]   when I think about a machine that understands language
[01:29:33.280 --> 01:29:36.760]   and ultimately can reason in an objective way
[01:29:36.760 --> 01:29:40.560]   that can take the information that it perceives
[01:29:40.560 --> 01:29:42.200]   through language or other means
[01:29:42.200 --> 01:29:44.520]   and connect it back to these frameworks,
[01:29:44.520 --> 01:29:46.200]   reason and explain itself,
[01:29:46.200 --> 01:29:50.680]   that system ultimately needs to be able to talk to humans
[01:29:50.680 --> 01:29:52.880]   or it needs to be able to interact with humans.
[01:29:52.880 --> 01:29:55.120]   So in some sense it needs to dialogue.
[01:29:55.120 --> 01:29:57.560]   That doesn't mean that it,
[01:29:57.560 --> 01:30:01.800]   sometimes people talk about dialogue and they think,
[01:30:01.800 --> 01:30:05.080]   you know, how do humans talk to each other
[01:30:05.080 --> 01:30:07.680]   in a casual conversation?
[01:30:07.680 --> 01:30:09.880]   And you can mimic casual conversations.
[01:30:09.880 --> 01:30:14.320]   We're not trying to mimic casual conversations.
[01:30:14.320 --> 01:30:17.520]   We're really trying to produce a machine
[01:30:17.520 --> 01:30:20.200]   whose goal is to help you think
[01:30:20.200 --> 01:30:22.200]   and help you reason about your answers
[01:30:22.200 --> 01:30:23.600]   and explain why.
[01:30:23.600 --> 01:30:26.560]   So instead of like talking to your friend down the street
[01:30:26.560 --> 01:30:28.880]   about having a small talk conversation
[01:30:28.880 --> 01:30:30.480]   with your friend down the street,
[01:30:30.480 --> 01:30:32.360]   this is more about like you would be communicating
[01:30:32.360 --> 01:30:35.120]   to the computer on Star Trek where,
[01:30:35.120 --> 01:30:36.760]   like what do you wanna think about?
[01:30:36.760 --> 01:30:37.600]   Like what do you wanna reason about?
[01:30:37.600 --> 01:30:38.760]   I'm gonna tell you the information I have.
[01:30:38.760 --> 01:30:39.840]   I'm gonna have to summarize it.
[01:30:39.840 --> 01:30:41.040]   I'm gonna ask you questions.
[01:30:41.040 --> 01:30:42.680]   You're gonna answer those questions.
[01:30:42.680 --> 01:30:44.240]   I'm gonna go back and forth with you.
[01:30:44.240 --> 01:30:46.600]   I'm gonna figure out what your mental model is.
[01:30:46.600 --> 01:30:50.080]   I'm gonna now relate that to the information I have
[01:30:50.080 --> 01:30:53.040]   and present it to you in a way that you can understand it
[01:30:53.040 --> 01:30:54.920]   and then we could ask follow-up questions.
[01:30:54.920 --> 01:30:58.280]   So it's that type of dialogue that you wanna construct.
[01:30:58.280 --> 01:31:00.400]   It's more structured.
[01:31:00.400 --> 01:31:04.840]   It's more goal-oriented, but it needs to be fluid.
[01:31:04.840 --> 01:31:06.880]   In other words, it can't, it can't,
[01:31:06.880 --> 01:31:09.240]   it has to be engaging and fluid.
[01:31:09.240 --> 01:31:13.080]   It has to be productive and not distracting.
[01:31:13.080 --> 01:31:15.720]   So there has to be a model of,
[01:31:15.720 --> 01:31:17.580]   in other words, the machine has to have a model
[01:31:17.580 --> 01:31:22.580]   of how humans think through things and discuss them.
[01:31:22.580 --> 01:31:27.000]   - So basically a productive, rich conversation,
[01:31:27.000 --> 01:31:30.120]   unlike this podcast.
[01:31:30.120 --> 01:31:34.960]   - I'd like to think it's more similar to this podcast.
[01:31:34.960 --> 01:31:36.120]   - I was just joking.
[01:31:36.120 --> 01:31:39.760]   I'll ask you about humor as well, actually.
[01:31:39.760 --> 01:31:43.280]   But what's the hardest part of that?
[01:31:43.280 --> 01:31:45.320]   Because it seems we're quite far away
[01:31:46.600 --> 01:31:49.800]   as a community from that still to be able to,
[01:31:49.800 --> 01:31:53.000]   so one is having a shared understanding.
[01:31:53.000 --> 01:31:54.880]   That's, I think, a lot of the stuff you said
[01:31:54.880 --> 01:31:57.120]   with frameworks is quite brilliant.
[01:31:57.120 --> 01:32:01.480]   But just creating a smooth discourse.
[01:32:01.480 --> 01:32:05.280]   Yeah, it feels clunky right now.
[01:32:05.280 --> 01:32:10.040]   Which aspects of this whole problem that you specified
[01:32:10.040 --> 01:32:12.760]   of having a productive conversation is the hardest?
[01:32:14.600 --> 01:32:18.600]   Or maybe any aspect of it you can comment on
[01:32:18.600 --> 01:32:20.780]   'cause it's so shrouded in mystery.
[01:32:20.780 --> 01:32:24.280]   - So I think to do this, you kind of have to be creative
[01:32:24.280 --> 01:32:25.880]   in the following sense.
[01:32:25.880 --> 01:32:29.800]   If I were to do this as purely a machine learning approach
[01:32:29.800 --> 01:32:32.840]   and someone said, "Learn how to have a good,
[01:32:32.840 --> 01:32:37.360]   "fluent, structured knowledge acquisition conversation,"
[01:32:37.360 --> 01:32:40.080]   I'd go out and say, "Okay, I have to collect a bunch
[01:32:40.080 --> 01:32:44.400]   "of data of people doing that, people reasoning well
[01:32:44.720 --> 01:32:47.680]   "having a good, structured conversation
[01:32:47.680 --> 01:32:50.220]   "that both acquires knowledge efficiently
[01:32:50.220 --> 01:32:52.320]   "as well as produces answers and explanations
[01:32:52.320 --> 01:32:54.600]   "as part of the process."
[01:32:54.600 --> 01:32:57.340]   And you struggle.
[01:32:57.340 --> 01:32:58.520]   I don't know-- - To collect the data.
[01:32:58.520 --> 01:33:00.680]   - To collect the data because I don't know
[01:33:00.680 --> 01:33:03.120]   how much data is like that.
[01:33:03.120 --> 01:33:06.160]   - Okay, okay, this one, there's a humorous commenter
[01:33:06.160 --> 01:33:08.560]   on the lack of rational discourse.
[01:33:08.560 --> 01:33:12.720]   But also, even if it's out there, say it was out there,
[01:33:12.720 --> 01:33:14.800]   how do you actually-- - Yeah, how--
[01:33:14.800 --> 01:33:17.200]   - Like how do you collect successful examples?
[01:33:17.200 --> 01:33:19.240]   - Right, so I think any problem like this
[01:33:19.240 --> 01:33:23.200]   where you don't have enough data to represent
[01:33:23.200 --> 01:33:25.960]   the phenomenon you wanna learn, in other words,
[01:33:25.960 --> 01:33:27.440]   if you have enough data, you could potentially
[01:33:27.440 --> 01:33:28.560]   learn the pattern.
[01:33:28.560 --> 01:33:30.360]   In an example like this, it's hard to do.
[01:33:30.360 --> 01:33:34.400]   It's sort of a human sort of thing to do.
[01:33:34.400 --> 01:33:36.960]   What recently came out at IBM was the debater project,
[01:33:36.960 --> 01:33:39.440]   sort of interesting, right, because now you do have
[01:33:39.440 --> 01:33:42.560]   these structured dialogues, these debate things,
[01:33:42.560 --> 01:33:44.680]   where they did use machine learning techniques
[01:33:44.680 --> 01:33:46.980]   to generate these debates.
[01:33:46.980 --> 01:33:52.440]   Dialogues are a little bit tougher, in my opinion,
[01:33:52.440 --> 01:33:56.080]   than generating a structured argument
[01:33:56.080 --> 01:33:58.000]   where you have lots of other structured arguments like this.
[01:33:58.000 --> 01:33:59.520]   You could potentially annotate that data
[01:33:59.520 --> 01:34:00.800]   and you could say this is a good response,
[01:34:00.800 --> 01:34:03.240]   this is a bad response in a particular domain.
[01:34:03.240 --> 01:34:08.240]   Here, I have to be responsive and I have to be opportunistic
[01:34:08.240 --> 01:34:11.840]   with regard to what is the human saying.
[01:34:11.840 --> 01:34:14.960]   So I'm goal-oriented in saying I wanna solve the problem,
[01:34:14.960 --> 01:34:16.640]   I wanna acquire the knowledge necessary,
[01:34:16.640 --> 01:34:19.200]   but I also have to be opportunistic and responsive
[01:34:19.200 --> 01:34:21.080]   to what the human is saying.
[01:34:21.080 --> 01:34:24.120]   So I think that it's not clear that we could just train
[01:34:24.120 --> 01:34:28.040]   on the body of data to do this, but we could bootstrap it.
[01:34:28.040 --> 01:34:29.960]   In other words, we can be creative
[01:34:29.960 --> 01:34:31.480]   and we could say, what do we think?
[01:34:31.480 --> 01:34:34.080]   What do we think the structure of a good dialogue is
[01:34:34.080 --> 01:34:35.880]   that does this well?
[01:34:35.880 --> 01:34:37.880]   And we can start to create that.
[01:34:38.520 --> 01:34:42.080]   If we can create that more programmatically,
[01:34:42.080 --> 01:34:44.720]   at least to get this process started,
[01:34:44.720 --> 01:34:48.000]   and I can create a tool that now engages humans effectively,
[01:34:48.000 --> 01:34:51.320]   I could start both, I could start generating data,
[01:34:51.320 --> 01:34:53.040]   I could start with the human learning process
[01:34:53.040 --> 01:34:55.080]   and I can update my machine,
[01:34:55.080 --> 01:34:55.920]   but I could also start
[01:34:55.920 --> 01:34:58.600]   the automatic learning process as well.
[01:34:58.600 --> 01:35:01.880]   But I have to understand what features to even learn over.
[01:35:01.880 --> 01:35:04.760]   So I have to bootstrap the process a little bit first.
[01:35:04.760 --> 01:35:07.760]   And that's a creative design task
[01:35:07.760 --> 01:35:11.040]   that I could then use as input
[01:35:11.040 --> 01:35:13.400]   into a more automatic learning task.
[01:35:13.400 --> 01:35:16.720]   - So some creativity and yeah, and bootstrapping.
[01:35:16.720 --> 01:35:18.920]   What elements of a conversation do you think
[01:35:18.920 --> 01:35:21.120]   you would like to see?
[01:35:21.120 --> 01:35:25.600]   So one of the benchmarks for me is humor, right?
[01:35:25.600 --> 01:35:27.800]   That seems to be one of the hardest.
[01:35:27.800 --> 01:35:30.340]   And to me, the biggest contrast is Watson.
[01:35:30.340 --> 01:35:35.280]   So one of the greatest comedy sketches of all time, right,
[01:35:35.280 --> 01:35:38.600]   is the SNL celebrity Jeopardy!
[01:35:38.600 --> 01:35:42.120]   With Alex Trebek and Sean Connery
[01:35:42.120 --> 01:35:44.120]   and Burt Reynolds and so on.
[01:35:44.120 --> 01:35:48.240]   With Sean Connery commentating on Alex Trebek's mother
[01:35:48.240 --> 01:35:49.440]   a lot.
[01:35:49.440 --> 01:35:52.920]   And I think all of them are in the negative points wise.
[01:35:52.920 --> 01:35:55.160]   So they're clearly all losing
[01:35:55.160 --> 01:35:56.360]   in terms of the game of Jeopardy!,
[01:35:56.360 --> 01:35:58.360]   but they're winning in terms of comedy.
[01:35:58.360 --> 01:36:03.360]   So what do you think about humor in this whole interaction
[01:36:03.800 --> 01:36:06.520]   in the dialogue that's productive?
[01:36:06.520 --> 01:36:10.200]   Or even just whatever, what humor represents to me is
[01:36:10.200 --> 01:36:15.400]   the same idea that you're saying about framework,
[01:36:15.400 --> 01:36:16.400]   'cause humor only exists
[01:36:16.400 --> 01:36:18.320]   within a particular human framework.
[01:36:18.320 --> 01:36:19.560]   So what do you think about humor?
[01:36:19.560 --> 01:36:21.520]   What do you think about things like humor
[01:36:21.520 --> 01:36:23.320]   that connect to the kind of creativity
[01:36:23.320 --> 01:36:25.120]   you mentioned that's needed?
[01:36:25.120 --> 01:36:26.400]   - I think there's a couple of things going on there.
[01:36:26.400 --> 01:36:29.520]   So I sort of feel like,
[01:36:29.520 --> 01:36:31.800]   and I might be too optimistic this way,
[01:36:31.800 --> 01:36:34.720]   but I think that there are,
[01:36:34.720 --> 01:36:39.000]   we did a little bit about with puns in Jeopardy!.
[01:36:39.000 --> 01:36:40.720]   We literally sat down and said,
[01:36:40.720 --> 01:36:43.160]   how do puns work?
[01:36:43.160 --> 01:36:44.800]   And it's like wordplay,
[01:36:44.800 --> 01:36:46.120]   and you could formalize these things.
[01:36:46.120 --> 01:36:48.240]   So I think there's a lot aspects of humor
[01:36:48.240 --> 01:36:50.200]   that you could formalize.
[01:36:50.200 --> 01:36:51.600]   You could also learn humor.
[01:36:51.600 --> 01:36:53.480]   You could just say, what do people laugh at?
[01:36:53.480 --> 01:36:54.880]   And if you have enough, again,
[01:36:54.880 --> 01:36:56.880]   if you have enough data to represent that phenomenon,
[01:36:56.880 --> 01:36:59.440]   you might be able to weigh the features
[01:36:59.440 --> 01:37:01.280]   and figure out what humans find funny
[01:37:01.280 --> 01:37:02.720]   and what they don't find funny.
[01:37:02.720 --> 01:37:05.200]   The machine might not be able to explain
[01:37:05.200 --> 01:37:06.680]   why the human find it funny,
[01:37:06.680 --> 01:37:10.200]   unless we sit back and think about that more formally.
[01:37:10.200 --> 01:37:12.440]   I think, again, I think you do a combination of both.
[01:37:12.440 --> 01:37:13.920]   And I'm always a big proponent of that.
[01:37:13.920 --> 01:37:16.720]   I think robust architectures and approaches
[01:37:16.720 --> 01:37:19.680]   are always a little bit combination of us reflecting
[01:37:19.680 --> 01:37:22.520]   and being creative about how things are structured,
[01:37:22.520 --> 01:37:23.800]   how to formalize them,
[01:37:23.800 --> 01:37:25.560]   and then taking advantage of large data
[01:37:25.560 --> 01:37:26.400]   and doing learning
[01:37:26.400 --> 01:37:29.120]   and figuring out how to combine these two approaches.
[01:37:29.120 --> 01:37:31.440]   I think there's another aspect to humor though,
[01:37:31.440 --> 01:37:34.360]   which goes to the idea that I feel like I can relate
[01:37:34.360 --> 01:37:36.040]   to the person telling the story.
[01:37:36.040 --> 01:37:42.160]   And I think that's an interesting theme
[01:37:42.160 --> 01:37:44.200]   in the whole AI theme, which is,
[01:37:44.200 --> 01:37:47.680]   do I feel differently when I know it's a robot?
[01:37:47.680 --> 01:37:51.480]   And when I know, when I imagine
[01:37:51.480 --> 01:37:54.200]   that the robot is not conscious the way I'm conscious,
[01:37:54.200 --> 01:37:56.320]   when I imagine the robot does not actually
[01:37:56.320 --> 01:37:58.720]   have the experiences that I experience,
[01:37:58.720 --> 01:38:00.960]   do I find it funny?
[01:38:00.960 --> 01:38:03.040]   Or do, because it's not as related,
[01:38:03.040 --> 01:38:06.560]   I don't imagine that the person's relating it to it
[01:38:06.560 --> 01:38:07.840]   the way I relate to it.
[01:38:07.840 --> 01:38:11.360]   I think this also, you see this in the arts
[01:38:11.360 --> 01:38:13.760]   and in entertainment where,
[01:38:13.760 --> 01:38:15.720]   like sometimes you have savants
[01:38:15.720 --> 01:38:17.400]   who are remarkable at a thing,
[01:38:17.400 --> 01:38:19.800]   whether it's sculpture, it's music or whatever,
[01:38:19.800 --> 01:38:21.320]   but the people who get the most attention
[01:38:21.320 --> 01:38:26.320]   are the people who can evoke a similar emotional response
[01:38:26.680 --> 01:38:31.680]   who can get you to emote, right, about the way they are.
[01:38:31.680 --> 01:38:34.440]   In other words, who can basically make the connection
[01:38:34.440 --> 01:38:37.000]   from the artifact, from the music or the painting
[01:38:37.000 --> 01:38:39.800]   or the sculpture to the emotion
[01:38:39.800 --> 01:38:42.360]   and get you to share that emotion with them.
[01:38:42.360 --> 01:38:44.700]   And then, and that's when it becomes compelling.
[01:38:44.700 --> 01:38:46.960]   So they're communicating at a whole different level.
[01:38:46.960 --> 01:38:49.340]   They're just not communicating the artifact.
[01:38:49.340 --> 01:38:50.960]   They're communicating their emotional response
[01:38:50.960 --> 01:38:51.800]   to the artifact.
[01:38:51.800 --> 01:38:53.360]   And then you feel like, oh, wow,
[01:38:53.360 --> 01:38:54.520]   I can relate to that person.
[01:38:54.520 --> 01:38:57.080]   I can connect to that person.
[01:38:57.080 --> 01:39:00.640]   So I think humor has that aspect as well.
[01:39:00.640 --> 01:39:04.760]   - So the idea that you can connect to that person,
[01:39:04.760 --> 01:39:06.360]   person being the critical thing,
[01:39:06.360 --> 01:39:12.240]   but we're also able to anthropomorphize objects pretty,
[01:39:12.240 --> 01:39:15.140]   robots and AI systems pretty well.
[01:39:15.140 --> 01:39:18.720]   So we're almost looking to make them human.
[01:39:18.720 --> 01:39:20.760]   Then maybe from your experience with Watson,
[01:39:20.760 --> 01:39:22.880]   maybe you can comment on,
[01:39:22.880 --> 01:39:24.920]   did you consider that as part,
[01:39:24.920 --> 01:39:26.960]   well, obviously the problem of Jeopardy
[01:39:26.960 --> 01:39:30.480]   doesn't require anthropomorphization, but nevertheless--
[01:39:30.480 --> 01:39:32.240]   - Well, there was some interest in doing that.
[01:39:32.240 --> 01:39:34.960]   And that's another thing I didn't wanna do
[01:39:34.960 --> 01:39:36.200]   'cause I didn't wanna distract
[01:39:36.200 --> 01:39:38.720]   from the actual scientific task.
[01:39:38.720 --> 01:39:39.600]   But you're absolutely right.
[01:39:39.600 --> 01:39:42.880]   I mean, humans do anthropomorphize
[01:39:42.880 --> 01:39:45.840]   and without necessarily a lot of work.
[01:39:45.840 --> 01:39:47.040]   I mean, you just put some eyes
[01:39:47.040 --> 01:39:49.200]   and a couple of eyebrow movements
[01:39:49.200 --> 01:39:51.800]   and you're getting humans to react emotionally.
[01:39:51.800 --> 01:39:53.520]   And I think you can do that.
[01:39:53.520 --> 01:39:56.800]   So I didn't mean to suggest that,
[01:39:56.800 --> 01:40:00.620]   that that connection cannot be mimicked.
[01:40:00.620 --> 01:40:02.240]   I think that connection can be mimicked
[01:40:02.240 --> 01:40:03.600]   and can get you to,
[01:40:03.600 --> 01:40:07.280]   can produce that emotional response.
[01:40:07.280 --> 01:40:08.720]   I just wonder though,
[01:40:08.720 --> 01:40:13.000]   if you're told what's really going on,
[01:40:13.000 --> 01:40:17.200]   if you know that the machine is not conscious,
[01:40:17.200 --> 01:40:20.760]   not having the same richness of emotional reactions
[01:40:20.760 --> 01:40:22.200]   and understanding that it doesn't really share
[01:40:22.200 --> 01:40:24.000]   the understanding, but it's essentially
[01:40:24.000 --> 01:40:26.400]   just moving its eyebrow or drooping its eyes
[01:40:26.400 --> 01:40:28.160]   or making them bigger, whatever it's doing,
[01:40:28.160 --> 01:40:30.180]   just getting the emotional response,
[01:40:30.180 --> 01:40:31.600]   will you still feel it?
[01:40:31.600 --> 01:40:34.380]   Interesting, I think you probably would for a while.
[01:40:34.380 --> 01:40:35.880]   And then when it becomes more important
[01:40:35.880 --> 01:40:38.680]   that there's a deeper shared understanding,
[01:40:38.680 --> 01:40:40.080]   it may run flat, but I don't know.
[01:40:40.080 --> 01:40:45.080]   - No, I'm pretty confident that majority of the world,
[01:40:45.080 --> 01:40:46.680]   even if you tell them how it works--
[01:40:46.680 --> 01:40:47.520]   - Won't matter.
[01:40:47.520 --> 01:40:49.120]   - Well, it will not matter,
[01:40:49.120 --> 01:40:53.080]   especially if the machine herself says
[01:40:53.080 --> 01:40:55.400]   that she is conscious.
[01:40:55.400 --> 01:40:56.240]   - That's very possible.
[01:40:56.240 --> 01:40:58.560]   - So you, the scientist that made the machine,
[01:40:58.560 --> 01:41:02.820]   is saying that this is how the algorithm works.
[01:41:02.820 --> 01:41:04.400]   Everybody will just assume you're lying
[01:41:04.400 --> 01:41:06.080]   and that there's a conscious being there.
[01:41:06.080 --> 01:41:09.200]   - So you're deep into the science fiction genre now,
[01:41:09.200 --> 01:41:10.040]   but yeah, I know--
[01:41:10.040 --> 01:41:12.000]   - I don't think it's, it's actually psychology.
[01:41:12.000 --> 01:41:13.720]   I think it's not science fiction.
[01:41:13.720 --> 01:41:14.880]   I think it's reality.
[01:41:14.880 --> 01:41:16.780]   I think it's a really powerful one
[01:41:16.780 --> 01:41:20.000]   that we'll have to be exploring in the next few decades.
[01:41:20.000 --> 01:41:20.840]   - I agree.
[01:41:20.840 --> 01:41:23.560]   - It's a very interesting element of intelligence.
[01:41:23.560 --> 01:41:25.200]   So what do you think,
[01:41:25.200 --> 01:41:28.520]   we've talked about social constructs of intelligence
[01:41:28.520 --> 01:41:31.160]   and frameworks and the way humans
[01:41:31.160 --> 01:41:33.960]   kind of interpret information.
[01:41:33.960 --> 01:41:35.720]   What do you think is a good test of intelligence
[01:41:35.720 --> 01:41:36.540]   in your view?
[01:41:36.540 --> 01:41:41.320]   So there's the Alan Turing with the Turing test.
[01:41:41.320 --> 01:41:44.920]   Watson accomplished something very impressive with Jeopardy.
[01:41:44.920 --> 01:41:46.760]   What do you think is a test
[01:41:46.760 --> 01:41:49.720]   that would impress the heck out of you,
[01:41:49.720 --> 01:41:52.940]   that you saw that a computer could do?
[01:41:52.940 --> 01:41:57.240]   They would say, this is crossing a kind of threshold
[01:41:57.240 --> 01:42:00.620]   that gives me pause in a good way.
[01:42:00.620 --> 01:42:06.060]   - My expectations for AI are generally high.
[01:42:06.060 --> 01:42:07.360]   - What does high look like, by the way?
[01:42:07.360 --> 01:42:10.360]   So not the threshold, test is a threshold.
[01:42:10.360 --> 01:42:12.480]   What do you think is the destination?
[01:42:12.480 --> 01:42:14.520]   What do you think is the ceiling?
[01:42:15.520 --> 01:42:18.520]   - I think machines will, in many measures,
[01:42:18.520 --> 01:42:21.680]   will be better than us, will become more effective.
[01:42:21.680 --> 01:42:25.160]   In other words, better predictors about a lot of things
[01:42:25.160 --> 01:42:28.580]   than ultimately we can do.
[01:42:28.580 --> 01:42:30.840]   I think where they're gonna struggle
[01:42:30.840 --> 01:42:32.280]   is what we've talked about before,
[01:42:32.280 --> 01:42:36.580]   which is relating to, communicating with,
[01:42:36.580 --> 01:42:40.620]   and understanding humans in deeper ways.
[01:42:40.620 --> 01:42:42.480]   And so I think that's a key point.
[01:42:42.480 --> 01:42:44.840]   Like, we can create the super parrot.
[01:42:44.840 --> 01:42:46.760]   What I mean by the super parrot is,
[01:42:46.760 --> 01:42:48.640]   given enough data, a machine can mimic
[01:42:48.640 --> 01:42:51.480]   your emotional response, can even generate language
[01:42:51.480 --> 01:42:54.600]   that will sound smart, and what someone else might say
[01:42:54.600 --> 01:42:56.420]   under similar circumstances.
[01:42:56.420 --> 01:42:58.960]   Like, I would just pause on that.
[01:42:58.960 --> 01:43:01.240]   Like, that's the super parrot, right?
[01:43:01.240 --> 01:43:03.700]   So, given similar circumstances,
[01:43:03.700 --> 01:43:06.960]   moves its faces in similar ways,
[01:43:06.960 --> 01:43:09.480]   changes its tone of voice in similar ways,
[01:43:09.480 --> 01:43:11.860]   produces strings of language that, you know,
[01:43:11.860 --> 01:43:14.320]   would similar that a human might say,
[01:43:14.320 --> 01:43:16.760]   not necessarily being able to produce
[01:43:16.760 --> 01:43:19.440]   a logical interpretation or understanding
[01:43:19.440 --> 01:43:25.320]   that would ultimately satisfy a critical interrogation
[01:43:25.320 --> 01:43:26.780]   or a critical understanding.
[01:43:26.780 --> 01:43:30.440]   - I think you just described me in a nutshell.
[01:43:30.440 --> 01:43:34.400]   So I think philosophically speaking,
[01:43:34.400 --> 01:43:36.560]   you could argue that that's all we're doing
[01:43:36.560 --> 01:43:37.520]   as human beings, too.
[01:43:37.520 --> 01:43:39.080]   We're super parrots. - So I was gonna say,
[01:43:39.080 --> 01:43:40.460]   it's very possible, you know,
[01:43:40.460 --> 01:43:42.580]   humans do behave that way, too.
[01:43:42.580 --> 01:43:45.820]   And so upon deeper probing and deeper interrogation,
[01:43:45.820 --> 01:43:48.900]   you may find out that there isn't a shared understanding,
[01:43:48.900 --> 01:43:50.300]   because I think humans do both.
[01:43:50.300 --> 01:43:53.200]   Like, humans are statistical language model machines,
[01:43:53.200 --> 01:43:57.620]   and they are capable reasoners.
[01:43:57.620 --> 01:43:59.860]   You know, they're both.
[01:43:59.860 --> 01:44:02.860]   And you don't know which is going on, right?
[01:44:02.860 --> 01:44:07.860]   So, and I think it's an interesting problem,
[01:44:08.840 --> 01:44:10.520]   we talked earlier about, like,
[01:44:10.520 --> 01:44:14.720]   where we are in our social and political landscape.
[01:44:14.720 --> 01:44:19.560]   Can you distinguish someone who can string words together
[01:44:19.560 --> 01:44:21.800]   and sound like they know what they're talking about
[01:44:21.800 --> 01:44:24.040]   from someone who actually does?
[01:44:24.040 --> 01:44:25.640]   Can you do that without dialogue,
[01:44:25.640 --> 01:44:27.740]   without interrogative or probing dialogue?
[01:44:27.740 --> 01:44:32.400]   So it's interesting, because humans are really good
[01:44:32.400 --> 01:44:35.000]   at, in their own mind, justifying or explaining
[01:44:35.000 --> 01:44:37.400]   what they hear, because they project
[01:44:37.400 --> 01:44:39.880]   their understanding onto yours.
[01:44:39.880 --> 01:44:42.960]   So you could say, you could put together a string of words,
[01:44:42.960 --> 01:44:45.120]   and someone will sit there and interpret it
[01:44:45.120 --> 01:44:47.080]   in a way that's extremely biased
[01:44:47.080 --> 01:44:48.200]   to the way they want to interpret it.
[01:44:48.200 --> 01:44:49.440]   They want to assume that you're an idiot,
[01:44:49.440 --> 01:44:51.080]   and they'll interpret it one way.
[01:44:51.080 --> 01:44:52.440]   They will assume you're a genius,
[01:44:52.440 --> 01:44:55.400]   and they'll interpret it another way that suits their needs.
[01:44:55.400 --> 01:44:57.920]   So this is tricky business.
[01:44:57.920 --> 01:45:00.580]   So I think to answer your question,
[01:45:00.580 --> 01:45:03.560]   as AI gets better and better at better and better mimic,
[01:45:03.560 --> 01:45:05.040]   we create the super parrots,
[01:45:05.900 --> 01:45:08.020]   we're challenged, just as we are with,
[01:45:08.020 --> 01:45:09.580]   we're challenged with humans.
[01:45:09.580 --> 01:45:12.100]   Do you really know what you're talking about?
[01:45:12.100 --> 01:45:16.420]   Do you have a meaningful interpretation,
[01:45:16.420 --> 01:45:19.420]   a powerful framework that you could reason over
[01:45:19.420 --> 01:45:24.420]   and justify your answers, justify your predictions
[01:45:24.420 --> 01:45:27.140]   and your beliefs, why you think they make sense?
[01:45:27.140 --> 01:45:29.500]   Can you convince me what the implications are?
[01:45:29.500 --> 01:45:33.620]   You know, can you, so can you reason intelligently
[01:45:33.620 --> 01:45:36.160]   and make me believe that those,
[01:45:36.160 --> 01:45:41.380]   the implications of your prediction and so forth?
[01:45:41.380 --> 01:45:44.180]   So what happens is it becomes reflective.
[01:45:44.180 --> 01:45:47.500]   My standard for judging your intelligence
[01:45:47.500 --> 01:45:48.800]   depends a lot on mine.
[01:45:48.800 --> 01:45:53.800]   - But you're saying that there should be
[01:45:53.800 --> 01:45:56.380]   a large group of people with a certain standard
[01:45:56.380 --> 01:45:58.620]   of intelligence that would be convinced
[01:45:58.620 --> 01:46:01.640]   by this particular AI system,
[01:46:02.580 --> 01:46:03.540]   then it would pass.
[01:46:03.540 --> 01:46:05.380]   - There should be, but I think one of the,
[01:46:05.380 --> 01:46:07.660]   depending on the content,
[01:46:07.660 --> 01:46:09.900]   one of the problems we have there is that
[01:46:09.900 --> 01:46:14.220]   if that large community of people are not judging it
[01:46:14.220 --> 01:46:16.600]   with regard to a rigorous standard
[01:46:16.600 --> 01:46:19.500]   of objective logic and reason, you still have a problem.
[01:46:19.500 --> 01:46:23.780]   Like masses of people can be persuaded.
[01:46:23.780 --> 01:46:24.980]   - The millennials, yeah.
[01:46:24.980 --> 01:46:27.640]   - To turn their brains off.
[01:46:27.640 --> 01:46:29.940]   - Right, okay.
[01:46:31.980 --> 01:46:32.820]   - Sorry.
[01:46:32.820 --> 01:46:33.660]   - By the way, I have nothing against the one.
[01:46:33.660 --> 01:46:36.060]   - No, I don't know, I'm just,
[01:46:36.060 --> 01:46:40.980]   so you're a part of one of the great benchmarks,
[01:46:40.980 --> 01:46:43.260]   challenges of AI history.
[01:46:43.260 --> 01:46:47.220]   What do you think about AlphaZero, OpenAI5,
[01:46:47.220 --> 01:46:50.740]   AlphaStar accomplishments on video games recently,
[01:46:50.740 --> 01:46:55.300]   which are also, I think, at least in the case of Go,
[01:46:55.300 --> 01:46:57.180]   with AlphaGo and AlphaZero playing Go
[01:46:57.180 --> 01:46:59.700]   was a monumental accomplishment as well.
[01:46:59.700 --> 01:47:01.740]   What are your thoughts about that challenge?
[01:47:01.740 --> 01:47:03.460]   - I think it was a giant landmark for AI.
[01:47:03.460 --> 01:47:04.500]   I think it was phenomenal.
[01:47:04.500 --> 01:47:06.020]   I mean, it was one of those other things
[01:47:06.020 --> 01:47:08.540]   nobody thought like solving Go was gonna be easy,
[01:47:08.540 --> 01:47:10.460]   particularly 'cause it's hard for,
[01:47:10.460 --> 01:47:12.700]   particularly hard for humans,
[01:47:12.700 --> 01:47:15.540]   hard for humans to learn, hard for humans to excel at.
[01:47:15.540 --> 01:47:19.600]   And so it was another measure of intelligence.
[01:47:19.600 --> 01:47:22.500]   It's very cool.
[01:47:22.500 --> 01:47:24.980]   I mean, it's very interesting what they did.
[01:47:24.980 --> 01:47:27.940]   I mean, and I loved how they solved the data problem,
[01:47:27.940 --> 01:47:29.180]   which is, again, they bootstrapped it
[01:47:29.180 --> 01:47:30.420]   and got the machine to play itself
[01:47:30.420 --> 01:47:32.740]   to generate enough data to learn from.
[01:47:32.740 --> 01:47:33.860]   I think that was brilliant.
[01:47:33.860 --> 01:47:35.700]   I think that was great.
[01:47:35.700 --> 01:47:38.940]   And of course the result speaks for itself.
[01:47:38.940 --> 01:47:41.900]   I think it makes us think about, again,
[01:47:41.900 --> 01:47:42.980]   okay, what's intelligence?
[01:47:42.980 --> 01:47:45.580]   What aspects of intelligence are important?
[01:47:45.580 --> 01:47:49.380]   Can the Go machine help me make me a better Go player?
[01:47:49.380 --> 01:47:51.700]   Is it an alien intelligence?
[01:47:51.700 --> 01:47:54.380]   Am I even capable of, like, again,
[01:47:54.380 --> 01:47:57.460]   if we put in very simple terms, it found the function.
[01:47:57.460 --> 01:47:59.220]   It found the Go function.
[01:47:59.220 --> 01:48:00.860]   Can I even comprehend the Go function?
[01:48:00.860 --> 01:48:02.300]   Can I talk about the Go function?
[01:48:02.300 --> 01:48:03.940]   Can I conceptualize the Go function,
[01:48:03.940 --> 01:48:05.540]   like whatever it might be?
[01:48:05.540 --> 01:48:08.080]   - So one of the interesting ideas of that system
[01:48:08.080 --> 01:48:10.100]   is that it plays against itself, right?
[01:48:10.100 --> 01:48:12.700]   But there's no human in the loop there.
[01:48:12.700 --> 01:48:16.520]   So like you're saying, it could have, by itself,
[01:48:16.520 --> 01:48:18.480]   created an alien intelligence.
[01:48:18.480 --> 01:48:21.860]   - Toward a goal, like, imagine you're sentencing,
[01:48:21.860 --> 01:48:24.740]   you're a judge and you're sentencing people,
[01:48:24.740 --> 01:48:26.460]   or you're setting policy,
[01:48:26.460 --> 01:48:31.220]   or you're making medical decisions,
[01:48:31.220 --> 01:48:33.380]   and you can't explain.
[01:48:33.380 --> 01:48:34.940]   You can't get anybody to understand
[01:48:34.940 --> 01:48:36.220]   what you're doing or why.
[01:48:36.220 --> 01:48:42.340]   So it's an interesting dilemma for the applications of AI.
[01:48:42.340 --> 01:48:48.060]   Do we hold AI to this accountability that says,
[01:48:48.060 --> 01:48:52.900]   you know, humans have to be able to take responsibility
[01:48:52.900 --> 01:48:56.380]   for the decision.
[01:48:56.380 --> 01:48:58.780]   In other words, can you explain why you would do the thing?
[01:48:58.780 --> 01:49:02.040]   Will you get up and speak to other humans
[01:49:02.040 --> 01:49:04.660]   and convince them that this was a smart decision?
[01:49:04.660 --> 01:49:07.180]   Is the AI enabling you to do that?
[01:49:07.180 --> 01:49:10.220]   Can you get behind the logic that was made there?
[01:49:10.220 --> 01:49:13.420]   - Do you think, sorry to linger on this point,
[01:49:13.420 --> 01:49:15.420]   'cause it's a fascinating one.
[01:49:15.420 --> 01:49:17.540]   It's a great goal for AI.
[01:49:17.540 --> 01:49:21.460]   Do you think it's achievable in many cases?
[01:49:21.460 --> 01:49:23.880]   Or, okay, there's two possible worlds
[01:49:23.880 --> 01:49:25.820]   that we have in the future.
[01:49:25.820 --> 01:49:28.940]   One is where AI systems do like medical diagnosis
[01:49:28.940 --> 01:49:32.420]   or things like that, or drive a car,
[01:49:32.420 --> 01:49:36.600]   without ever explaining to you why it fails when it does.
[01:49:36.600 --> 01:49:40.380]   That's one possible world, and we're okay with it.
[01:49:40.380 --> 01:49:42.980]   Or the other, where we are not okay with it,
[01:49:42.980 --> 01:49:45.380]   and we really hold back the technology
[01:49:45.380 --> 01:49:48.780]   from getting too good before it gets able to explain.
[01:49:48.780 --> 01:49:50.780]   Which of those worlds are more likely, do you think,
[01:49:50.780 --> 01:49:53.500]   and which are concerning to you or not?
[01:49:53.500 --> 01:49:55.940]   - I think the reality is it's gonna be a mix.
[01:49:55.940 --> 01:49:57.460]   I'm not sure I have a problem with that.
[01:49:57.460 --> 01:50:00.460]   I mean, I think there are tasks that I'm perfectly fine with
[01:50:00.460 --> 01:50:03.980]   machines show a certain level of performance,
[01:50:03.980 --> 01:50:07.740]   and that level of performance is already better than humans.
[01:50:07.740 --> 01:50:10.100]   So, for example, I don't know that I,
[01:50:10.100 --> 01:50:11.300]   take driverless cars.
[01:50:11.300 --> 01:50:14.340]   If driverless cars learn how to be more effective drivers
[01:50:14.340 --> 01:50:16.920]   than humans, but can't explain what they're doing,
[01:50:16.920 --> 01:50:19.060]   but bottom line, statistically speaking,
[01:50:19.060 --> 01:50:22.420]   they're 10 times safer than humans,
[01:50:22.420 --> 01:50:24.980]   I don't know that I care.
[01:50:24.980 --> 01:50:27.580]   I think when we have these edge cases,
[01:50:27.580 --> 01:50:29.740]   when something bad happens and we wanna decide
[01:50:29.740 --> 01:50:32.580]   who's liable for that thing, and who made that mistake,
[01:50:32.580 --> 01:50:33.540]   and what do we do about that?
[01:50:33.540 --> 01:50:36.740]   And I think those edge cases are interesting cases.
[01:50:36.740 --> 01:50:38.940]   And now do we go to designers of the AI,
[01:50:38.940 --> 01:50:39.780]   and the AI says, I don't know,
[01:50:39.780 --> 01:50:41.060]   if that's what it learned to do?
[01:50:41.060 --> 01:50:43.620]   And it says, well, you didn't train it properly.
[01:50:43.620 --> 01:50:46.740]   You know, you were negligent in the training data
[01:50:46.740 --> 01:50:47.820]   that you gave that machine.
[01:50:47.820 --> 01:50:49.420]   Like, how do we drive down the reliability?
[01:50:49.420 --> 01:50:52.080]   So I think those are interesting questions.
[01:50:52.080 --> 01:50:55.300]   - So the optimization problem there, sorry,
[01:50:55.300 --> 01:50:56.900]   is to create an AI system that's able
[01:50:56.900 --> 01:50:58.820]   to explain the lawyers away.
[01:50:58.820 --> 01:51:01.620]   - Yeah, there you go.
[01:51:01.620 --> 01:51:04.020]   I think that, I think it's gonna be interesting.
[01:51:04.020 --> 01:51:05.820]   I mean, I think this is where technology
[01:51:05.820 --> 01:51:09.500]   and social discourse are gonna get deeply intertwined
[01:51:09.500 --> 01:51:11.900]   in how we start thinking about problems,
[01:51:11.900 --> 01:51:13.500]   decisions, and problems like that.
[01:51:13.500 --> 01:51:15.860]   I think in other cases, it becomes more obvious
[01:51:15.860 --> 01:51:18.120]   where, you know, it's like,
[01:51:18.120 --> 01:51:21.180]   like, why did you decide to give that person,
[01:51:21.180 --> 01:51:26.060]   you know, a longer sentence, or deny them parole?
[01:51:26.060 --> 01:51:30.540]   Again, policy decisions, or why did you pick that treatment?
[01:51:30.540 --> 01:51:32.260]   Like, that treatment ended up killing that guy.
[01:51:32.260 --> 01:51:35.060]   Like, why was that a reasonable choice to make?
[01:51:35.060 --> 01:51:40.060]   So, and people are gonna demand explanations.
[01:51:40.060 --> 01:51:41.900]   Now, there's a reality, though, here.
[01:51:43.460 --> 01:51:45.940]   And the reality is that it's not,
[01:51:45.940 --> 01:51:48.580]   I'm not sure humans are making reasonable choices
[01:51:48.580 --> 01:51:49.900]   when they do these things.
[01:51:49.900 --> 01:51:54.740]   They are using statistical hunches, biases,
[01:51:54.740 --> 01:51:58.460]   or even systematically using statistical averages
[01:51:58.460 --> 01:51:59.300]   to make calls.
[01:51:59.300 --> 01:52:00.340]   I mean, this is what happened to my dad,
[01:52:00.340 --> 01:52:01.900]   and if you saw the talk I gave about that.
[01:52:01.900 --> 01:52:04.940]   But, you know, I mean, they decided
[01:52:04.940 --> 01:52:07.260]   that my father was brain dead.
[01:52:07.260 --> 01:52:09.300]   He had went into cardiac arrest,
[01:52:09.300 --> 01:52:12.380]   and it took a long time for the ambulance to get there,
[01:52:12.380 --> 01:52:14.540]   and he was not resuscitated right away, and so forth.
[01:52:14.540 --> 01:52:16.900]   And they came, they told me he was brain dead.
[01:52:16.900 --> 01:52:17.860]   And why was he brain dead?
[01:52:17.860 --> 01:52:19.060]   Because essentially, they gave me
[01:52:19.060 --> 01:52:21.060]   a purely statistical argument.
[01:52:21.060 --> 01:52:23.820]   Under these conditions, with these four features,
[01:52:23.820 --> 01:52:25.300]   98% chance he's brain dead.
[01:52:25.300 --> 01:52:27.740]   And I said, but can you just tell me,
[01:52:27.740 --> 01:52:29.660]   not inductively, but deductively,
[01:52:29.660 --> 01:52:31.380]   go there and tell me his brain's not functioning
[01:52:31.380 --> 01:52:32.820]   as the way for you to do that?
[01:52:32.820 --> 01:52:35.980]   And the protocol and response was,
[01:52:35.980 --> 01:52:37.980]   no, this is how we make this decision.
[01:52:37.980 --> 01:52:39.740]   I said, this is inadequate for me.
[01:52:39.740 --> 01:52:41.140]   I understand the statistics,
[01:52:41.140 --> 01:52:43.100]   and I don't know how, you know,
[01:52:43.100 --> 01:52:44.540]   there's a 2% chance he's still alive.
[01:52:44.540 --> 01:52:46.500]   Like, I just don't know the specifics.
[01:52:46.500 --> 01:52:49.380]   I need the specifics of this case,
[01:52:49.380 --> 01:52:51.420]   and I want the deductive, logical argument
[01:52:51.420 --> 01:52:53.580]   about why you actually know he's brain dead.
[01:52:53.580 --> 01:52:55.980]   So I wouldn't sign the do not resuscitate.
[01:52:55.980 --> 01:52:57.260]   And, I don't know, it was like,
[01:52:57.260 --> 01:52:58.700]   they went through lots of procedures.
[01:52:58.700 --> 01:53:00.020]   It was a big, long story.
[01:53:00.020 --> 01:53:02.060]   But the bottom, a fascinating story, by the way,
[01:53:02.060 --> 01:53:04.340]   about how I reasoned, and how the doctors reasoned
[01:53:04.340 --> 01:53:05.980]   through this whole process.
[01:53:05.980 --> 01:53:07.900]   But I don't know, somewhere around 24 hours later
[01:53:07.900 --> 01:53:09.460]   or something, he was sitting up in bed
[01:53:09.460 --> 01:53:11.020]   with zero brain damage.
[01:53:11.020 --> 01:53:18.020]   - I mean, what lessons do you draw from that story,
[01:53:18.020 --> 01:53:19.500]   that experience?
[01:53:19.500 --> 01:53:22.700]   - That the data that's being used
[01:53:22.700 --> 01:53:24.100]   to make statistical inferences
[01:53:24.100 --> 01:53:26.460]   doesn't adequately reflect the phenomenon.
[01:53:26.460 --> 01:53:29.180]   So in other words, you're getting shit wrong, sorry.
[01:53:29.180 --> 01:53:31.740]   You're getting stuff wrong
[01:53:31.740 --> 01:53:35.260]   because your model's not robust enough,
[01:53:35.260 --> 01:53:38.580]   and you might be better off
[01:53:38.580 --> 01:53:41.340]   not using statistical inference
[01:53:41.340 --> 01:53:43.080]   and statistical averages in certain cases
[01:53:43.080 --> 01:53:45.220]   when you know the model's insufficient,
[01:53:45.220 --> 01:53:48.460]   and that you should be reasoning it about the specific case
[01:53:48.460 --> 01:53:51.060]   more logically and more deductively,
[01:53:51.060 --> 01:53:52.500]   and hold yourself responsible,
[01:53:52.500 --> 01:53:54.500]   hold yourself accountable to doing that.
[01:53:54.500 --> 01:53:58.700]   - And perhaps AI has a role to say
[01:53:58.700 --> 01:54:00.740]   the exact thing we just said,
[01:54:00.740 --> 01:54:03.020]   which is, perhaps this is a case
[01:54:03.020 --> 01:54:05.460]   you should think for yourself.
[01:54:05.460 --> 01:54:07.220]   You should reason deductively.
[01:54:08.140 --> 01:54:13.140]   - Well, so it's hard because it's hard to know that.
[01:54:13.140 --> 01:54:15.740]   You'd have to go back
[01:54:15.740 --> 01:54:18.260]   and you'd have to have enough data to essentially say,
[01:54:18.260 --> 01:54:22.060]   and this goes back to the case of how do we decide
[01:54:22.060 --> 01:54:24.560]   whether AI is good enough to do a particular task?
[01:54:24.560 --> 01:54:27.340]   And regardless of whether or not
[01:54:27.340 --> 01:54:28.700]   it produces an explanation.
[01:54:28.700 --> 01:54:34.740]   So, and what standards do we hold, right, for that?
[01:54:35.020 --> 01:54:40.020]   So, if you look more broadly, for example,
[01:54:40.020 --> 01:54:45.980]   as my father, as a medical case,
[01:54:45.980 --> 01:54:49.860]   the medical system ultimately helped him
[01:54:49.860 --> 01:54:51.540]   a lot throughout his life.
[01:54:51.540 --> 01:54:54.720]   Without it, he probably would have died much sooner.
[01:54:54.720 --> 01:54:58.940]   So overall, it's sort of worked for him
[01:54:58.940 --> 01:55:00.800]   in sort of a net-net kind of way.
[01:55:00.800 --> 01:55:04.860]   Actually, I don't know that that's fair,
[01:55:04.860 --> 01:55:08.140]   but maybe not in that particular case, but overall.
[01:55:08.140 --> 01:55:10.980]   The medical system overall does more good than bad.
[01:55:10.980 --> 01:55:12.460]   - Yeah, the medical system overall
[01:55:12.460 --> 01:55:14.340]   was doing more good than bad.
[01:55:14.340 --> 01:55:16.580]   Now, there's another argument that suggests
[01:55:16.580 --> 01:55:17.420]   that that wasn't the case,
[01:55:17.420 --> 01:55:18.620]   but for the sake of argument,
[01:55:18.620 --> 01:55:21.060]   let's say that's a net positive.
[01:55:21.060 --> 01:55:22.260]   And I think you have to sit there
[01:55:22.260 --> 01:55:24.860]   and take that into consideration.
[01:55:24.860 --> 01:55:26.700]   Now you look at a particular use case,
[01:55:26.700 --> 01:55:28.920]   like for example, making this decision.
[01:55:28.920 --> 01:55:32.340]   Have you done enough studies to know
[01:55:33.440 --> 01:55:35.700]   how good that prediction really is?
[01:55:35.700 --> 01:55:40.120]   And have you done enough studies to compare it?
[01:55:40.120 --> 01:55:45.120]   To say, well, what if we dug in in a more direct,
[01:55:45.120 --> 01:55:48.040]   let's get the evidence, let's do the deductive thing
[01:55:48.040 --> 01:55:49.480]   and not use statistics here.
[01:55:49.480 --> 01:55:51.640]   How often would that have done better?
[01:55:51.640 --> 01:55:53.760]   So you have to do the studies
[01:55:53.760 --> 01:55:56.200]   to know how good the AI actually is.
[01:55:56.200 --> 01:55:57.600]   And it's complicated,
[01:55:57.600 --> 01:55:59.600]   because it depends how fast you have to make decision.
[01:55:59.600 --> 01:56:02.400]   So if you have to make decisions super fast,
[01:56:02.400 --> 01:56:03.360]   you have no choice.
[01:56:03.360 --> 01:56:06.800]   If you have more time,
[01:56:06.800 --> 01:56:09.080]   but if you're ready to pull the plug,
[01:56:09.080 --> 01:56:11.520]   and this is a lot of the argument that I had with a doctor,
[01:56:11.520 --> 01:56:13.260]   I said, what's he gonna do if you do it,
[01:56:13.260 --> 01:56:15.280]   what's gonna happen to him in that room
[01:56:15.280 --> 01:56:16.400]   if you do it my way?
[01:56:16.400 --> 01:56:18.800]   Well, he's gonna die anyway,
[01:56:18.800 --> 01:56:20.120]   so let's do it my way then.
[01:56:20.120 --> 01:56:22.880]   - I mean, it raises questions for our society
[01:56:22.880 --> 01:56:26.560]   to struggle with, as is the case with your father,
[01:56:26.560 --> 01:56:28.700]   but also when things like race and gender
[01:56:28.700 --> 01:56:29.880]   start coming into play,
[01:56:31.760 --> 01:56:35.640]   when judgments are made based on things
[01:56:35.640 --> 01:56:39.040]   that are complicated in our society,
[01:56:39.040 --> 01:56:40.120]   at least in the discourse.
[01:56:40.120 --> 01:56:44.000]   And it starts, I think I'm safe to say
[01:56:44.000 --> 01:56:47.340]   that most of the violent crime is committed by males.
[01:56:47.340 --> 01:56:51.080]   So if you discriminate based,
[01:56:51.080 --> 01:56:53.880]   it's a male versus female saying that
[01:56:53.880 --> 01:56:56.160]   if it's a male, more likely to commit the crime.
[01:56:56.160 --> 01:57:01.040]   - So this is one of my very positive and optimistic views
[01:57:01.040 --> 01:57:05.520]   of why the study of artificial intelligence,
[01:57:05.520 --> 01:57:08.000]   the process of thinking and reasoning,
[01:57:08.000 --> 01:57:10.520]   logically and statistically, and how to combine them
[01:57:10.520 --> 01:57:12.200]   is so important for the discourse today,
[01:57:12.200 --> 01:57:17.200]   because it's causing a, regardless of what state AI devices
[01:57:17.200 --> 01:57:22.240]   are or not, it's causing this dialogue to happen.
[01:57:22.240 --> 01:57:24.840]   This is one of the most important dialogues
[01:57:24.840 --> 01:57:28.200]   that, in my view, the human species can have right now,
[01:57:28.200 --> 01:57:33.200]   which is how to think well, how to reason well,
[01:57:33.200 --> 01:57:38.840]   how to understand our own cognitive biases
[01:57:38.840 --> 01:57:41.000]   and what to do about them.
[01:57:41.000 --> 01:57:43.640]   That has got to be one of the most important things
[01:57:43.640 --> 01:57:46.760]   we as a species can be doing, honestly.
[01:57:46.760 --> 01:57:51.200]   We've created an incredibly complex society.
[01:57:51.200 --> 01:57:55.840]   We've created amazing abilities to amplify noise
[01:57:55.840 --> 01:57:58.420]   faster than we can amplify signal.
[01:57:58.420 --> 01:58:01.280]   We are challenged.
[01:58:01.280 --> 01:58:03.680]   We are deeply, deeply challenged.
[01:58:03.680 --> 01:58:06.320]   We have big segments of the population
[01:58:06.320 --> 01:58:08.960]   getting hit with enormous amounts of information.
[01:58:08.960 --> 01:58:11.000]   Do they know how to do critical thinking?
[01:58:11.000 --> 01:58:14.240]   Do they know how to objectively reason?
[01:58:14.240 --> 01:58:17.000]   Do they understand what they are doing,
[01:58:17.000 --> 01:58:18.800]   nevermind what their AI is doing?
[01:58:18.800 --> 01:58:23.200]   This is such an important dialogue to be having.
[01:58:23.200 --> 01:58:27.960]   And we are fundamentally, our thinking can be
[01:58:27.960 --> 01:58:31.440]   and easily becomes fundamentally biased.
[01:58:31.440 --> 01:58:34.480]   And there are statistics, and we shouldn't blind ourselves,
[01:58:34.480 --> 01:58:37.320]   we shouldn't discard statistical inference,
[01:58:37.320 --> 01:58:40.920]   but we should understand the nature of statistical inference.
[01:58:40.920 --> 01:58:45.920]   As a society, we decide to reject statistical inference,
[01:58:48.240 --> 01:58:53.240]   to favor understanding and deciding on the individual.
[01:58:53.240 --> 01:59:00.680]   We consciously make that choice.
[01:59:00.680 --> 01:59:03.240]   So even if the statistics said,
[01:59:03.240 --> 01:59:07.720]   even if the statistics said males are more likely
[01:59:07.720 --> 01:59:09.720]   to be violent criminals,
[01:59:09.720 --> 01:59:12.800]   we still take each person as an individual,
[01:59:12.800 --> 01:59:16.000]   and we treat them based on the logic
[01:59:16.880 --> 01:59:20.320]   and the knowledge of that situation.
[01:59:20.320 --> 01:59:22.960]   We purposefully and intentionally
[01:59:22.960 --> 01:59:27.480]   reject the statistical inference.
[01:59:27.480 --> 01:59:31.280]   We do that out of respect for the individual.
[01:59:31.280 --> 01:59:32.200]   - For the individual, yeah,
[01:59:32.200 --> 01:59:35.200]   and that requires reasoning and thinking.
[01:59:35.200 --> 01:59:37.480]   Looking forward, what grand challenges
[01:59:37.480 --> 01:59:39.000]   would you like to see in the future?
[01:59:39.000 --> 01:59:44.000]   Because the Jeopardy challenge captivated the world,
[01:59:45.200 --> 01:59:48.080]   AlphaGo, AlphaZero captivated the world,
[01:59:48.080 --> 01:59:50.280]   Deep Blue certainly beating Kasparov,
[01:59:50.280 --> 01:59:55.720]   Gary's bitterness aside, captivated the world.
[01:59:55.720 --> 01:59:57.880]   What do you think, do you have ideas
[01:59:57.880 --> 02:00:01.400]   for next grand challenges for future challenges of that?
[02:00:01.400 --> 02:00:03.280]   - Look, I mean, I think there are lots
[02:00:03.280 --> 02:00:05.800]   of really great ideas for grand challenges.
[02:00:05.800 --> 02:00:08.480]   I'm particularly focused on one right now,
[02:00:08.480 --> 02:00:12.520]   which is can you demonstrate that they understand,
[02:00:12.520 --> 02:00:14.960]   that they could read and understand,
[02:00:14.960 --> 02:00:18.000]   that they can acquire these frameworks
[02:00:18.000 --> 02:00:21.160]   and reason and communicate with humans?
[02:00:21.160 --> 02:00:23.360]   So it is kind of like the Turing test,
[02:00:23.360 --> 02:00:26.560]   but it's a little bit more demanding than the Turing test.
[02:00:26.560 --> 02:00:31.280]   It's not enough to convince me that you might be human
[02:00:31.280 --> 02:00:34.920]   because you can parrot a conversation.
[02:00:34.920 --> 02:00:38.480]   I think the standard is a little bit higher.
[02:00:38.480 --> 02:00:43.400]   For example, can you, the standard is higher,
[02:00:43.400 --> 02:00:45.560]   and I think one of the challenges
[02:00:45.560 --> 02:00:48.280]   of devising this grand challenge
[02:00:48.280 --> 02:00:53.280]   is that we're not sure what intelligence is.
[02:00:53.280 --> 02:00:56.240]   We're not sure how to determine
[02:00:56.240 --> 02:00:59.160]   whether or not two people actually understand each other
[02:00:59.160 --> 02:01:01.120]   and in what depth they understand it,
[02:01:01.120 --> 02:01:04.400]   to what depth they understand each other.
[02:01:04.400 --> 02:01:07.440]   So the challenge becomes something along the lines
[02:01:07.440 --> 02:01:12.440]   of can you satisfy me that we have a shared purpose
[02:01:12.600 --> 02:01:14.800]   we have a shared understanding?
[02:01:14.800 --> 02:01:18.400]   So if I were to probe and probe and you probe me,
[02:01:18.400 --> 02:01:23.400]   can machines really act like thought partners
[02:01:23.400 --> 02:01:27.320]   where they can satisfy me that we have a shared,
[02:01:27.320 --> 02:01:29.400]   our understanding is shared enough
[02:01:29.400 --> 02:01:33.320]   that we can collaborate and produce answers together
[02:01:33.320 --> 02:01:36.760]   and that they can help me explain and justify those answers.
[02:01:36.760 --> 02:01:38.120]   - So maybe here's an idea.
[02:01:38.120 --> 02:01:43.120]   So we'll have AI system run for president and convince--
[02:01:43.120 --> 02:01:46.120]   - That's too easy.
[02:01:46.120 --> 02:01:46.960]   I'm sorry, go ahead.
[02:01:46.960 --> 02:01:51.600]   - You have to convince the voters that they should vote.
[02:01:51.600 --> 02:01:53.800]   So I guess what does winning look like?
[02:01:53.800 --> 02:01:55.920]   - Again, that's why I think this is such a challenge
[02:01:55.920 --> 02:02:00.040]   because we go back to the emotional persuasion.
[02:02:00.040 --> 02:02:05.040]   We go back to, now we're checking off an aspect
[02:02:06.080 --> 02:02:11.080]   of human cognition that is in many ways weak or flawed.
[02:02:11.080 --> 02:02:13.960]   We're so easily manipulated.
[02:02:13.960 --> 02:02:18.960]   Our minds are drawn for often the wrong reasons.
[02:02:18.960 --> 02:02:21.880]   Not the reasons that ultimately matter to us,
[02:02:21.880 --> 02:02:24.000]   but the reasons that can easily persuade us.
[02:02:24.000 --> 02:02:28.440]   I think we can be persuaded to believe one thing or another
[02:02:28.440 --> 02:02:31.360]   for reasons that ultimately don't serve us well
[02:02:31.360 --> 02:02:33.200]   in the long term.
[02:02:33.200 --> 02:02:38.200]   And a good benchmark should not play with those elements
[02:02:38.200 --> 02:02:40.800]   of emotional manipulation.
[02:02:40.800 --> 02:02:41.640]   - I don't think so.
[02:02:41.640 --> 02:02:44.360]   And I think that's where we have to set the higher standard
[02:02:44.360 --> 02:02:47.160]   for ourselves of what does it mean.
[02:02:47.160 --> 02:02:48.920]   This goes back to rationality
[02:02:48.920 --> 02:02:50.680]   and it goes back to objective thinking.
[02:02:50.680 --> 02:02:53.360]   Can you produce, can you acquire information
[02:02:53.360 --> 02:02:54.840]   and produce reasoned arguments?
[02:02:54.840 --> 02:02:56.360]   And to those reasoned arguments,
[02:02:56.360 --> 02:02:58.000]   pass a certain amount of muster.
[02:02:58.000 --> 02:03:02.600]   And can you acquire new knowledge?
[02:03:02.600 --> 02:03:06.280]   Can you, for example, can you reason,
[02:03:06.280 --> 02:03:07.480]   I have acquired new knowledge,
[02:03:07.480 --> 02:03:11.240]   can you identify where it's consistent or contradictory
[02:03:11.240 --> 02:03:12.920]   with other things you've learned?
[02:03:12.920 --> 02:03:14.080]   And can you explain that to me
[02:03:14.080 --> 02:03:15.600]   and get me to understand that?
[02:03:15.600 --> 02:03:18.520]   So I think another way to think about it perhaps
[02:03:18.520 --> 02:03:22.780]   is can a machine teach you?
[02:03:22.780 --> 02:03:28.640]   Can it help you-- - Oh, that's a really nice,
[02:03:28.640 --> 02:03:29.720]   nice and then nice way to put it.
[02:03:29.720 --> 02:03:30.560]   Can it help you understand--
[02:03:30.560 --> 02:03:32.440]   - Can it help you understand something
[02:03:32.440 --> 02:03:34.920]   that you didn't really understand before?
[02:03:34.920 --> 02:03:35.760]   - Oh, that's a beautiful way to put it.
[02:03:35.760 --> 02:03:39.120]   - Where it's taking you, so you're not,
[02:03:39.120 --> 02:03:41.360]   again, it's almost like, can it teach you?
[02:03:41.360 --> 02:03:43.480]   Can it help you learn?
[02:03:43.480 --> 02:03:47.040]   And in an arbitrary space,
[02:03:47.040 --> 02:03:49.000]   so it can open those domain space.
[02:03:49.000 --> 02:03:50.120]   So can you tell the machine,
[02:03:50.120 --> 02:03:52.840]   and again, this borrows from some science fictions,
[02:03:52.840 --> 02:03:55.820]   but can you go off and learn about this topic
[02:03:55.820 --> 02:03:57.520]   that I'd like to understand better
[02:03:58.440 --> 02:04:00.840]   and then work with me to help me understand it?
[02:04:00.840 --> 02:04:03.640]   - That's quite brilliant.
[02:04:03.640 --> 02:04:06.960]   Well, a machine that passes that kind of test,
[02:04:06.960 --> 02:04:11.560]   do you think it would need to have self-awareness
[02:04:11.560 --> 02:04:13.160]   or even consciousness?
[02:04:13.160 --> 02:04:16.160]   What do you think about consciousness
[02:04:16.160 --> 02:04:17.840]   and the importance of it,
[02:04:17.840 --> 02:04:21.120]   maybe in relation to having a body,
[02:04:21.120 --> 02:04:24.760]   having a presence, an entity?
[02:04:24.760 --> 02:04:26.760]   Do you think that's important?
[02:04:26.960 --> 02:04:28.760]   - People used to ask me if Watson was conscious,
[02:04:28.760 --> 02:04:32.280]   and I used to say, conscious of what exactly?
[02:04:32.280 --> 02:04:34.280]   I mean, I think-- - Of self.
[02:04:34.280 --> 02:04:36.080]   - It depends what it is that you're conscious of.
[02:04:36.080 --> 02:04:38.520]   I mean, so did it,
[02:04:38.520 --> 02:04:42.800]   it's certainly easy for it to answer questions about,
[02:04:42.800 --> 02:04:44.800]   it would be trivial to program it
[02:04:44.800 --> 02:04:46.120]   so that it'd answer questions about
[02:04:46.120 --> 02:04:47.580]   whether or not it was playing Jeopardy.
[02:04:47.580 --> 02:04:49.040]   I mean, it could certainly answer questions
[02:04:49.040 --> 02:04:51.280]   that would imply that it was aware of things.
[02:04:51.280 --> 02:04:52.680]   Exactly, what does it mean to be aware
[02:04:52.680 --> 02:04:53.520]   and what does it mean to consciousness?
[02:04:53.520 --> 02:04:54.440]   It's sort of interesting.
[02:04:54.440 --> 02:04:58.000]   I mean, I think that we differ from one another
[02:04:58.000 --> 02:04:59.840]   based on what we're conscious of.
[02:04:59.840 --> 02:05:02.680]   - But wait, wait, yes, for sure.
[02:05:02.680 --> 02:05:05.320]   There's degrees of consciousness in there, so--
[02:05:05.320 --> 02:05:06.960]   - Well, and there's just areas.
[02:05:06.960 --> 02:05:08.440]   Like, it's not just degrees.
[02:05:08.440 --> 02:05:10.160]   What are you aware of?
[02:05:10.160 --> 02:05:11.160]   Like, what are you not aware of?
[02:05:11.160 --> 02:05:13.440]   - But nevertheless, there's a very subjective element
[02:05:13.440 --> 02:05:14.740]   to our experience.
[02:05:14.740 --> 02:05:18.400]   Let me even not talk about consciousness.
[02:05:18.400 --> 02:05:21.600]   Let me talk about another, to me,
[02:05:21.600 --> 02:05:23.640]   really interesting topic of mortality,
[02:05:23.640 --> 02:05:25.600]   fear of mortality.
[02:05:25.600 --> 02:05:29.320]   Watson, as far as I could tell,
[02:05:29.320 --> 02:05:30.980]   did not have a fear of death.
[02:05:30.980 --> 02:05:33.040]   - Certainly not.
[02:05:33.040 --> 02:05:35.920]   - Most humans do.
[02:05:35.920 --> 02:05:39.080]   - Wasn't conscious of death.
[02:05:39.080 --> 02:05:42.880]   - He wasn't, yeah, so there's an element of finiteness
[02:05:42.880 --> 02:05:46.160]   to our existence that I think, like you mentioned,
[02:05:46.160 --> 02:05:49.040]   survival, that adds to the whole thing.
[02:05:49.040 --> 02:05:50.880]   I mean, consciousness is tied up with that,
[02:05:50.880 --> 02:05:52.880]   that we are a thing.
[02:05:52.880 --> 02:05:56.200]   It's a subjective thing that ends,
[02:05:56.200 --> 02:05:59.000]   and that seems to add a color and flavor
[02:05:59.000 --> 02:06:01.640]   to our motivations in a way that seems
[02:06:01.640 --> 02:06:05.960]   to be fundamentally important for intelligence,
[02:06:05.960 --> 02:06:07.880]   or at least the kind of human intelligence.
[02:06:07.880 --> 02:06:09.800]   - Well, I think for generating goals.
[02:06:09.800 --> 02:06:14.520]   Again, I think you could have an intelligence capability
[02:06:14.520 --> 02:06:18.520]   and a capability to learn, a capability to predict,
[02:06:18.520 --> 02:06:23.520]   but I think without, I mean, again, you get a fear,
[02:06:23.520 --> 02:06:27.000]   but essentially without the goal to survive.
[02:06:27.000 --> 02:06:29.080]   - So you think you can just encode that
[02:06:29.080 --> 02:06:29.920]   without having to really--
[02:06:29.920 --> 02:06:30.920]   - I think you can encode that.
[02:06:30.920 --> 02:06:32.800]   I mean, you can create a robot now,
[02:06:32.800 --> 02:06:36.000]   and you could say, plug it in,
[02:06:36.000 --> 02:06:38.520]   and say, protect your power source,
[02:06:38.520 --> 02:06:39.720]   and give it some capabilities,
[02:06:39.720 --> 02:06:40.880]   and it'll sit there and operate
[02:06:40.880 --> 02:06:42.760]   to try to protect its power source and survive.
[02:06:42.760 --> 02:06:45.680]   I mean, so I don't know that that's philosophically
[02:06:45.680 --> 02:06:46.680]   a hard thing to demonstrate.
[02:06:46.680 --> 02:06:49.000]   It sounds like a fairly easy thing to demonstrate
[02:06:49.000 --> 02:06:50.080]   that you can give it that goal.
[02:06:50.080 --> 02:06:52.400]   Well, it'll come up with that goal by itself,
[02:06:52.400 --> 02:06:54.560]   and I think you have to program that goal in.
[02:06:54.560 --> 02:06:57.840]   - But there's something, because I think,
[02:06:57.840 --> 02:06:59.680]   as we touched on, intelligence is kind of
[02:06:59.680 --> 02:07:01.520]   like a social construct.
[02:07:01.520 --> 02:07:06.280]   The fact that a robot will be protecting its power source
[02:07:06.280 --> 02:07:12.560]   would add depth and grounding to its intelligence
[02:07:12.560 --> 02:07:15.840]   in terms of us being able to respect it.
[02:07:15.840 --> 02:07:18.280]   I mean, ultimately, it boils down to us
[02:07:18.280 --> 02:07:20.680]   acknowledging that it's intelligent,
[02:07:20.680 --> 02:07:23.520]   and the fact that it can die,
[02:07:23.520 --> 02:07:26.120]   I think, is an important part of that.
[02:07:26.120 --> 02:07:27.840]   - The interesting thing to reflect on
[02:07:27.840 --> 02:07:29.520]   is how trivial that would be,
[02:07:29.520 --> 02:07:32.120]   and I don't think if you knew how trivial that was,
[02:07:32.120 --> 02:07:35.400]   you would associate that with being intelligence.
[02:07:35.400 --> 02:07:37.480]   I mean, I literally put in a statement of code
[02:07:37.480 --> 02:07:40.440]   that says you have the following actions you can take.
[02:07:40.440 --> 02:07:41.640]   You give it a bunch of actions,
[02:07:41.640 --> 02:07:44.000]   like maybe you mount a laser gun on it,
[02:07:44.000 --> 02:07:48.960]   or you have the ability to scream or screech or whatever,
[02:07:48.960 --> 02:07:52.720]   and you say, if you see your power source threatened,
[02:07:52.720 --> 02:07:53.920]   then you could program that in,
[02:07:53.920 --> 02:07:58.060]   and you're gonna take these actions to protect it.
[02:07:58.060 --> 02:08:02.200]   You could train it on a bunch of things.
[02:08:02.200 --> 02:08:03.800]   So, and now you're gonna look at that,
[02:08:03.800 --> 02:08:05.280]   and you're gonna say, well, that's intelligence,
[02:08:05.280 --> 02:08:06.840]   because it's protecting its power source.
[02:08:06.840 --> 02:08:10.220]   Maybe, but that's, again, this human bias that says,
[02:08:10.220 --> 02:08:14.580]   the thing I, I identify my intelligence and my conscience
[02:08:14.580 --> 02:08:16.740]   so fundamentally with the desire,
[02:08:16.740 --> 02:08:18.660]   or at least the behaviors associated
[02:08:18.660 --> 02:08:20.400]   with the desire to survive,
[02:08:20.400 --> 02:08:23.860]   that if I see another thing doing that,
[02:08:23.860 --> 02:08:27.280]   I'm going to assume it's intelligence.
[02:08:27.280 --> 02:08:31.080]   - What timeline year will society have a,
[02:08:31.080 --> 02:08:36.020]   something that would, that you would be comfortable calling
[02:08:36.020 --> 02:08:38.120]   an artificial general intelligence system?
[02:08:39.560 --> 02:08:41.060]   What's your intuition?
[02:08:41.060 --> 02:08:42.460]   Nobody can predict the future,
[02:08:42.460 --> 02:08:46.460]   certainly not the next few months or 20 years away,
[02:08:46.460 --> 02:08:47.620]   but what's your intuition?
[02:08:47.620 --> 02:08:48.900]   How far away are we?
[02:08:48.900 --> 02:08:50.940]   - I don't know.
[02:08:50.940 --> 02:08:52.100]   It's hard to make these predictions.
[02:08:52.100 --> 02:08:54.740]   I mean, I would be, you know, I would be guessing,
[02:08:54.740 --> 02:08:57.000]   and there's so many different variables,
[02:08:57.000 --> 02:08:59.100]   including just how much we want to invest in it,
[02:08:59.100 --> 02:09:00.380]   and how important it, you know,
[02:09:00.380 --> 02:09:02.080]   and how important we think it is,
[02:09:02.080 --> 02:09:06.140]   what kind of investment we're willing to make in it,
[02:09:06.140 --> 02:09:07.860]   what kind of talent we end up bringing to the table,
[02:09:07.860 --> 02:09:09.200]   all, you know, the incentive structure,
[02:09:09.200 --> 02:09:10.140]   all these things.
[02:09:10.140 --> 02:09:15.140]   So I think it is possible to do this sort of thing.
[02:09:15.140 --> 02:09:20.180]   I think it's, I think trying to sort of ignore many
[02:09:20.180 --> 02:09:23.000]   of the variables and things like that,
[02:09:23.000 --> 02:09:24.020]   is it a 10-year thing?
[02:09:24.020 --> 02:09:25.380]   Is it a 20-year?
[02:09:25.380 --> 02:09:27.860]   It's probably closer to a 20-year thing, I guess.
[02:09:27.860 --> 02:09:29.680]   - But not several hundred years.
[02:09:29.680 --> 02:09:32.060]   - No, I don't think it's several hundred years.
[02:09:32.060 --> 02:09:33.620]   I don't think it's several hundred years,
[02:09:33.620 --> 02:09:38.620]   but again, so much depends on how committed we are
[02:09:38.820 --> 02:09:43.080]   to investing and incentivizing this type of work.
[02:09:43.080 --> 02:09:45.160]   And it's sort of interesting,
[02:09:45.160 --> 02:09:50.160]   like I don't think it's obvious how incentivized we are.
[02:09:50.160 --> 02:09:54.380]   I think from a task perspective, you know,
[02:09:54.380 --> 02:09:57.840]   if we see business opportunities to take this technique
[02:09:57.840 --> 02:09:59.120]   or that technique to solve that problem,
[02:09:59.120 --> 02:10:03.220]   I think that's the main driver for many of these things.
[02:10:03.220 --> 02:10:05.520]   From a general intelligence,
[02:10:05.520 --> 02:10:06.920]   it's kind of an interesting question.
[02:10:06.920 --> 02:10:09.560]   Are we really motivated to do that?
[02:10:09.560 --> 02:10:12.520]   And like we just struggled ourselves right now
[02:10:12.520 --> 02:10:14.760]   to even define what it is.
[02:10:14.760 --> 02:10:16.920]   So it's hard to incentivize when we don't even know
[02:10:16.920 --> 02:10:18.800]   what it is we're incentivized to create.
[02:10:18.800 --> 02:10:21.320]   And if you said mimic a human intelligence,
[02:10:21.320 --> 02:10:25.520]   I just think there are so many challenges
[02:10:25.520 --> 02:10:27.720]   with the significance and meaning of that,
[02:10:27.720 --> 02:10:29.640]   that there's not a clear directive.
[02:10:29.640 --> 02:10:32.280]   There's no clear directive to do precisely that thing.
[02:10:32.280 --> 02:10:36.480]   - So assistance in a larger and larger number of tasks.
[02:10:36.480 --> 02:10:39.600]   So being able to, a system that's particularly able
[02:10:39.600 --> 02:10:42.640]   to operate my microwave and making a grilled cheese sandwich,
[02:10:42.640 --> 02:10:45.000]   I don't even know how to make one of those.
[02:10:45.000 --> 02:10:48.040]   And then the same system would be doing the vacuum cleaning.
[02:10:48.040 --> 02:10:51.680]   And then the same system would be teaching
[02:10:51.680 --> 02:10:56.300]   my kids that I don't have math.
[02:10:56.300 --> 02:11:00.760]   - I think that when you get into a general intelligence
[02:11:00.760 --> 02:11:04.280]   for learning physical tasks,
[02:11:04.280 --> 02:11:06.080]   and again, I wanna go back to your body question,
[02:11:06.080 --> 02:11:07.320]   'cause I think your body question was interesting,
[02:11:07.320 --> 02:11:11.120]   but you wanna go back to learning the abilities
[02:11:11.120 --> 02:11:11.960]   to do physical tasks.
[02:11:11.960 --> 02:11:14.480]   You might have, we might get,
[02:11:14.480 --> 02:11:16.080]   I imagine in that timeframe,
[02:11:16.080 --> 02:11:17.480]   we will get better and better
[02:11:17.480 --> 02:11:19.040]   at learning these kinds of tasks,
[02:11:19.040 --> 02:11:21.280]   whether it's mowing your lawn or driving a car
[02:11:21.280 --> 02:11:22.760]   or whatever it is.
[02:11:22.760 --> 02:11:24.480]   I think we will get better and better at that
[02:11:24.480 --> 02:11:25.880]   where it's learning how to make predictions
[02:11:25.880 --> 02:11:27.040]   over large bodies of data.
[02:11:27.040 --> 02:11:27.880]   I think we're gonna continue
[02:11:27.880 --> 02:11:29.480]   to get better and better at that.
[02:11:29.480 --> 02:11:33.560]   And machines will outpace humans
[02:11:33.560 --> 02:11:35.600]   in a variety of those things.
[02:11:35.600 --> 02:11:40.600]   The underlying mechanisms for doing that may be the same,
[02:11:40.600 --> 02:11:43.700]   meaning that maybe these are deep nets,
[02:11:43.700 --> 02:11:46.280]   there's infrastructure to train them,
[02:11:46.280 --> 02:11:48.120]   reusable components to get them
[02:11:48.120 --> 02:11:50.960]   to do different classes of tasks,
[02:11:50.960 --> 02:11:51.920]   and we get better and better
[02:11:51.920 --> 02:11:53.980]   at building these kinds of machines.
[02:11:53.980 --> 02:11:55.040]   You could still argue
[02:11:55.040 --> 02:11:57.040]   that the general learning infrastructure in there
[02:11:57.040 --> 02:12:01.040]   is a form of a general type of intelligence.
[02:12:01.040 --> 02:12:03.280]   I think what starts getting harder
[02:12:03.280 --> 02:12:04.720]   is this notion of,
[02:12:04.720 --> 02:12:09.120]   can we effectively communicate and understand
[02:12:09.120 --> 02:12:10.840]   and build that shared understanding
[02:12:10.840 --> 02:12:12.640]   because of the layers of interpretation
[02:12:12.640 --> 02:12:14.300]   that are required to do that,
[02:12:14.300 --> 02:12:15.640]   and the need for the machine
[02:12:15.640 --> 02:12:18.200]   to be engaged with humans at that level
[02:12:18.200 --> 02:12:20.320]   in a continuous basis.
[02:12:20.320 --> 02:12:21.440]   So how do you get in there?
[02:12:21.440 --> 02:12:23.480]   How do you get the machine in the game?
[02:12:23.480 --> 02:12:26.600]   How do you get the machine in the intellectual game?
[02:12:26.600 --> 02:12:29.120]   - Yeah, and to solve AGI,
[02:12:29.120 --> 02:12:31.000]   you probably have to solve that problem.
[02:12:31.000 --> 02:12:31.960]   - You have to get the machine.
[02:12:31.960 --> 02:12:33.800]   So it's a little bit of a bootstrapping thing.
[02:12:33.800 --> 02:12:38.040]   Can we get the machine engaged in the intellectual,
[02:12:38.040 --> 02:12:39.160]   I'm calling it a game,
[02:12:39.160 --> 02:12:42.360]   but in the intellectual dialogue with the humans?
[02:12:42.360 --> 02:12:43.960]   Are the humans sufficiently
[02:12:43.960 --> 02:12:45.600]   in intellectual dialogue with each other
[02:12:45.600 --> 02:12:49.640]   to generate enough data in this context?
[02:12:49.640 --> 02:12:51.020]   And how do you bootstrap that?
[02:12:51.020 --> 02:12:54.080]   Because every one of those conversations,
[02:12:54.080 --> 02:12:55.760]   every one of those conversations,
[02:12:55.760 --> 02:12:58.040]   those intelligent interactions
[02:12:58.040 --> 02:12:59.680]   require so much prior knowledge
[02:12:59.680 --> 02:13:01.660]   that it's a challenge to bootstrap it.
[02:13:01.660 --> 02:13:05.840]   So the question is, and how committed?
[02:13:05.840 --> 02:13:07.300]   So I think that's possible,
[02:13:07.300 --> 02:13:10.880]   but when I go back to, are we incentivized to do that?
[02:13:10.880 --> 02:13:13.160]   I know we're incentivized to do the former.
[02:13:13.160 --> 02:13:15.880]   Are we incentivized to do the latter significantly enough?
[02:13:15.880 --> 02:13:16.720]   Do people understand
[02:13:16.720 --> 02:13:18.460]   what the latter really is well enough?
[02:13:18.460 --> 02:13:20.860]   Part of the elemental cognition mission
[02:13:20.860 --> 02:13:23.520]   is to try to articulate that better and better
[02:13:23.520 --> 02:13:24.560]   through demonstrations
[02:13:24.560 --> 02:13:26.960]   and through trying to craft these grand challenges
[02:13:26.960 --> 02:13:27.920]   and get people to say,
[02:13:27.920 --> 02:13:30.440]   look, this is a class of intelligence.
[02:13:30.440 --> 02:13:31.840]   This is a class of AI.
[02:13:31.840 --> 02:13:33.420]   Do we want this?
[02:13:33.420 --> 02:13:35.820]   What is the potential of this?
[02:13:35.820 --> 02:13:37.840]   What's the business potential?
[02:13:37.840 --> 02:13:40.120]   What's the societal potential to that?
[02:13:40.120 --> 02:13:45.080]   And to build up that incentive system around that.
[02:13:45.080 --> 02:13:46.840]   - Yeah, I think if people don't understand yet,
[02:13:46.840 --> 02:13:47.680]   I think they will.
[02:13:47.680 --> 02:13:49.600]   I think there's a huge business potential here.
[02:13:49.600 --> 02:13:52.020]   So it's exciting that you're working on it.
[02:13:52.020 --> 02:13:54.960]   We kind of skipped over,
[02:13:54.960 --> 02:13:59.560]   but I'm a huge fan of physical presence of things.
[02:13:59.560 --> 02:14:03.360]   Do you think, you know, Watson had a body.
[02:14:03.360 --> 02:14:08.360]   Do you think having a body adds to the interactive element
[02:14:08.360 --> 02:14:11.720]   between the AI system and a human,
[02:14:11.720 --> 02:14:13.540]   or just in general to intelligence?
[02:14:13.540 --> 02:14:19.680]   - So I think going back to that shared understanding bit,
[02:14:19.680 --> 02:14:21.680]   humans are very connected to their bodies.
[02:14:21.680 --> 02:14:23.520]   I mean, one of the reasons,
[02:14:23.520 --> 02:14:26.360]   one of the challenges in getting an AI
[02:14:26.360 --> 02:14:29.200]   to kind of be a compatible human intelligence
[02:14:29.200 --> 02:14:33.720]   is that our physical bodies are generating a lot of features
[02:14:33.720 --> 02:14:37.760]   that make up the input.
[02:14:37.760 --> 02:14:39.480]   So in other words, where our bodies are,
[02:14:39.480 --> 02:14:42.760]   are the tool we use to affect output,
[02:14:42.760 --> 02:14:46.400]   but they also generate a lot of input for our brains.
[02:14:46.400 --> 02:14:49.640]   So we generate emotion, we generate all these feelings,
[02:14:49.640 --> 02:14:52.800]   we generate all these signals that machines don't have.
[02:14:52.800 --> 02:14:55.300]   So the machines that have this is the input data,
[02:14:55.300 --> 02:14:58.840]   and they don't have the feedback that says,
[02:14:58.840 --> 02:15:01.240]   okay, I've gotten this, I've gotten this emotion,
[02:15:01.240 --> 02:15:04.360]   or I've gotten this idea, I now wanna process it,
[02:15:04.360 --> 02:15:09.000]   and then I can, it then affects me as a physical being,
[02:15:09.000 --> 02:15:12.240]   and then I can play that out.
[02:15:12.240 --> 02:15:14.080]   In other words, I could realize the implications of that,
[02:15:14.080 --> 02:15:17.560]   'cause the implications, again, on my mind-body complex,
[02:15:17.560 --> 02:15:20.000]   I then process that, and the implications, again,
[02:15:20.000 --> 02:15:23.660]   are internal features are generated, I learn from them,
[02:15:23.660 --> 02:15:26.800]   they have an effect on my mind-body complex.
[02:15:26.800 --> 02:15:28.920]   So it's interesting when we think,
[02:15:28.920 --> 02:15:30.480]   do we want a human intelligence?
[02:15:30.480 --> 02:15:33.240]   Well, if we want a human-compatible intelligence,
[02:15:33.240 --> 02:15:34.360]   probably the best thing to do
[02:15:34.360 --> 02:15:36.800]   is to embed it in a human body.
[02:15:36.800 --> 02:15:39.980]   - Just to clarify, and both concepts are beautiful,
[02:15:39.980 --> 02:15:44.980]   is humanoid robots, so robots that look like humans is one,
[02:15:44.980 --> 02:15:50.480]   or did you mean actually sort of what Elon Musk
[02:15:50.480 --> 02:15:53.000]   is working with Neuralink,
[02:15:53.000 --> 02:15:55.840]   really embedding intelligence systems
[02:15:55.840 --> 02:15:59.840]   to ride along human bodies?
[02:15:59.840 --> 02:16:01.880]   - No, I mean, riding along is different.
[02:16:01.880 --> 02:16:05.880]   I meant like if you wanna create an intelligence
[02:16:05.880 --> 02:16:08.760]   that is human-compatible,
[02:16:08.760 --> 02:16:10.880]   meaning that it can learn and develop
[02:16:10.880 --> 02:16:13.080]   a shared understanding of the world around it,
[02:16:13.080 --> 02:16:15.160]   you have to give it a lot of the same substrate.
[02:16:15.160 --> 02:16:18.240]   Part of that substrate is the idea
[02:16:18.240 --> 02:16:21.160]   that it generates these kinds of internal features,
[02:16:21.160 --> 02:16:24.040]   the sort of emotional stuff, it has similar senses,
[02:16:24.040 --> 02:16:25.680]   it has to do a lot of the same things
[02:16:25.680 --> 02:16:28.240]   with those same senses, right?
[02:16:28.240 --> 02:16:29.800]   So I think if you want that,
[02:16:29.800 --> 02:16:32.080]   again, I don't know that you want that.
[02:16:32.080 --> 02:16:34.280]   Like, that's not my specific goal.
[02:16:34.280 --> 02:16:35.840]   I think that's a fascinating scientific goal.
[02:16:35.840 --> 02:16:37.860]   I think it has all kinds of other implications.
[02:16:37.860 --> 02:16:39.280]   That's sort of not the goal.
[02:16:39.280 --> 02:16:41.600]   Like, I wanna create, I think of it
[02:16:41.600 --> 02:16:44.160]   as I create intellectual thought partners for humans,
[02:16:44.160 --> 02:16:46.300]   so that kind of intelligence.
[02:16:46.300 --> 02:16:48.560]   I know there are other companies
[02:16:48.560 --> 02:16:50.160]   that are creating physical thought partners.
[02:16:50.160 --> 02:16:52.440]   Physical partners for humans.
[02:16:52.440 --> 02:16:56.120]   But that's kind of not where I'm at.
[02:16:56.120 --> 02:17:00.760]   But the important point is that a big part
[02:17:00.760 --> 02:17:05.760]   of what we process is that physical experience
[02:17:05.760 --> 02:17:08.080]   of the world around us.
[02:17:08.080 --> 02:17:10.520]   - On the point of thought partners,
[02:17:10.520 --> 02:17:13.920]   what role does an emotional connection,
[02:17:13.920 --> 02:17:17.820]   or forgive me, love, have to play
[02:17:17.820 --> 02:17:19.840]   in that thought partnership?
[02:17:19.840 --> 02:17:22.000]   Is that something you're interested in,
[02:17:22.000 --> 02:17:25.440]   put another way, sort of having a deep connection
[02:17:25.440 --> 02:17:29.280]   beyond intellectual?
[02:17:29.280 --> 02:17:30.200]   - With the AI?
[02:17:30.200 --> 02:17:32.720]   - Yeah, with the AI, between human and AI.
[02:17:32.720 --> 02:17:34.440]   Is that something that gets in the way
[02:17:34.440 --> 02:17:37.560]   of the rational discourse?
[02:17:37.560 --> 02:17:39.240]   Is that something that's useful?
[02:17:39.240 --> 02:17:41.920]   - I worry about biases, you know, obviously.
[02:17:41.920 --> 02:17:44.280]   So in other words, if you develop an emotional relationship
[02:17:44.280 --> 02:17:46.640]   with a machine, all of a sudden you start,
[02:17:46.640 --> 02:17:48.320]   are more likely to believe what it's saying,
[02:17:48.320 --> 02:17:50.240]   even if it doesn't make any sense.
[02:17:50.240 --> 02:17:53.640]   So I worry about that.
[02:17:53.640 --> 02:17:55.680]   But at the same time, I think the opportunity
[02:17:55.680 --> 02:17:57.760]   to use machines to provide human companionship
[02:17:57.760 --> 02:17:59.060]   is actually not crazy.
[02:17:59.060 --> 02:18:05.240]   Intellectual and social companionship is not a crazy idea.
[02:18:05.240 --> 02:18:09.960]   - Do you have concerns, as a few people do,
[02:18:09.960 --> 02:18:13.880]   Elon Musk, Sam Harris, about long-term existential threats
[02:18:13.880 --> 02:18:18.720]   of AI and perhaps short-term threats of AI?
[02:18:18.720 --> 02:18:21.080]   We talked about bias, we talked about different misuses,
[02:18:21.080 --> 02:18:25.640]   but do you have concerns about thought partners,
[02:18:25.640 --> 02:18:28.560]   systems that are able to help us make decisions
[02:18:28.560 --> 02:18:31.200]   together with humans, somehow having a significant
[02:18:31.200 --> 02:18:33.720]   negative impact on society in the long-term?
[02:18:33.720 --> 02:18:35.300]   - I think there are things to worry about.
[02:18:35.300 --> 02:18:40.300]   I think giving machines too much leverage is a problem.
[02:18:41.480 --> 02:18:45.640]   And what I mean by leverage is too much control
[02:18:45.640 --> 02:18:48.880]   over things that can hurt us, whether it's socially,
[02:18:48.880 --> 02:18:51.640]   psychologically, intellectually, or physically.
[02:18:51.640 --> 02:18:53.500]   And if you give the machines too much control,
[02:18:53.500 --> 02:18:54.800]   I think that's a concern.
[02:18:54.800 --> 02:18:57.260]   You forget about the AI, just when you give them
[02:18:57.260 --> 02:19:00.360]   too much control, human bad actors can hack them
[02:19:00.360 --> 02:19:02.640]   and produce havoc.
[02:19:02.640 --> 02:19:07.000]   So that's a problem.
[02:19:07.000 --> 02:19:10.040]   And you can imagine hackers taking over
[02:19:10.040 --> 02:19:15.040]   the driverless car network and creating all kinds of havoc.
[02:19:15.040 --> 02:19:20.200]   But you could also imagine, given the ease at which
[02:19:20.200 --> 02:19:22.800]   humans could be persuaded one way or the other,
[02:19:22.800 --> 02:19:25.840]   and now we have algorithms that can easily take control
[02:19:25.840 --> 02:19:30.600]   over that and amplify noise and move people
[02:19:30.600 --> 02:19:32.000]   one direction or another.
[02:19:32.000 --> 02:19:34.140]   I mean, humans do that to other humans all the time.
[02:19:34.140 --> 02:19:37.120]   And we have marketing campaigns, we have political campaigns
[02:19:37.120 --> 02:19:41.640]   that take advantage of our emotions or our fears.
[02:19:41.640 --> 02:19:44.180]   And this is done all the time.
[02:19:44.180 --> 02:19:47.740]   But with machines, machines are like giant megaphones, right?
[02:19:47.740 --> 02:19:50.680]   We can amplify this in orders of magnitude
[02:19:50.680 --> 02:19:54.840]   and fine tune its control so we can tailor the message.
[02:19:54.840 --> 02:19:58.600]   We can now very rapidly and efficiently tailor the message
[02:19:58.600 --> 02:20:03.600]   to the audience, taking advantage of their biases
[02:20:03.600 --> 02:20:06.640]   and amplifying them and using them to persuade them
[02:20:06.640 --> 02:20:10.760]   in one direction or another in ways that are not fair,
[02:20:10.760 --> 02:20:13.440]   not logical, not objective, not meaningful.
[02:20:13.440 --> 02:20:17.040]   And machines empower that.
[02:20:17.040 --> 02:20:18.920]   So that's what I mean by leverage.
[02:20:18.920 --> 02:20:22.840]   Like, it's not new, but wow, it's powerful
[02:20:22.840 --> 02:20:24.400]   because machines can do it more effectively,
[02:20:24.400 --> 02:20:27.720]   more quickly, and we see that already going on
[02:20:27.720 --> 02:20:30.480]   in social media and other places.
[02:20:30.480 --> 02:20:33.120]   That's scary.
[02:20:33.120 --> 02:20:38.120]   And that's why I go back to saying,
[02:20:38.120 --> 02:20:43.760]   one of the most important public dialogues
[02:20:43.760 --> 02:20:47.960]   we could be having is about the nature of intelligence
[02:20:47.960 --> 02:20:52.160]   and the nature of inference and logic
[02:20:52.160 --> 02:20:53.940]   and reason and rationality,
[02:20:53.940 --> 02:20:58.000]   and us understanding our own biases,
[02:20:58.000 --> 02:20:59.800]   us understanding our own cognitive biases
[02:20:59.800 --> 02:21:03.160]   and how they work and then how machines work
[02:21:03.160 --> 02:21:06.040]   and how do we use them to complement it basically
[02:21:06.040 --> 02:21:09.680]   so that in the end we have a stronger overall system.
[02:21:09.680 --> 02:21:11.400]   That's just incredibly important.
[02:21:11.400 --> 02:21:15.800]   I don't think most people understand that.
[02:21:15.800 --> 02:21:19.780]   So like telling your kids or telling your students,
[02:21:19.780 --> 02:21:22.560]   this goes back to the cognition.
[02:21:22.560 --> 02:21:24.480]   Here's how your brain works.
[02:21:24.480 --> 02:21:28.080]   Here's how easy it is to trick your brain, right?
[02:21:28.080 --> 02:21:29.480]   There are fundamental cognitive,
[02:21:29.480 --> 02:21:34.040]   you should appreciate the different types of thinking
[02:21:34.040 --> 02:21:36.800]   and how they work and what you're prone to
[02:21:36.800 --> 02:21:41.680]   and what do you prefer and under what conditions
[02:21:41.680 --> 02:21:43.640]   does this make sense versus that make sense?
[02:21:43.640 --> 02:21:46.440]   And then say, here's what AI can do.
[02:21:46.440 --> 02:21:48.640]   Here's how it can make this worse
[02:21:48.640 --> 02:21:51.040]   and here's how it can make this better.
[02:21:51.040 --> 02:21:52.720]   - And that's where the AI has a role
[02:21:52.720 --> 02:21:55.620]   is to reveal that trade-off.
[02:21:56.640 --> 02:22:00.760]   So if you imagine a system that is able to,
[02:22:00.760 --> 02:22:06.960]   beyond any definition of the Turing test or the benchmark,
[02:22:06.960 --> 02:22:10.240]   really an AGI system as a thought partner
[02:22:10.240 --> 02:22:12.980]   that you one day will create,
[02:22:12.980 --> 02:22:19.320]   what question, what topic of discussion
[02:22:19.320 --> 02:22:23.120]   if you get to pick one, would you have with that system?
[02:22:23.960 --> 02:22:28.240]   What would you ask and you get to find out
[02:22:28.240 --> 02:22:30.360]   the truth together?
[02:22:30.360 --> 02:22:35.280]   - So you threw me a little bit
[02:22:35.280 --> 02:22:36.880]   with finding the truth at the end,
[02:22:36.880 --> 02:22:38.840]   (laughing)
[02:22:38.840 --> 02:22:41.000]   'cause the truth is a whole nother topic.
[02:22:41.000 --> 02:22:43.560]   But I think the beauty of it,
[02:22:43.560 --> 02:22:46.040]   I think what excites me is the beauty of it is
[02:22:46.040 --> 02:22:48.700]   if I really have that system, I don't have to pick.
[02:22:48.700 --> 02:22:51.600]   So in other words, I can go to it and say,
[02:22:51.600 --> 02:22:54.080]   this is what I care about today.
[02:22:54.080 --> 02:22:55.960]   And that's what we mean by it,
[02:22:55.960 --> 02:22:57.200]   like this general capability.
[02:22:57.200 --> 02:23:00.520]   Go out, read this stuff in the next three milliseconds.
[02:23:00.520 --> 02:23:02.480]   And I wanna talk to you about it.
[02:23:02.480 --> 02:23:04.040]   I wanna draw analogies.
[02:23:04.040 --> 02:23:06.000]   I wanna understand how this affects
[02:23:06.000 --> 02:23:08.040]   this decision or that decision.
[02:23:08.040 --> 02:23:09.160]   What if this were true?
[02:23:09.160 --> 02:23:10.680]   What if that were true?
[02:23:10.680 --> 02:23:13.160]   What knowledge should I be aware of
[02:23:13.160 --> 02:23:15.960]   that could impact my decision?
[02:23:15.960 --> 02:23:18.920]   Here's what I'm thinking is the main implication.
[02:23:18.920 --> 02:23:21.060]   Can you find, can you prove that out?
[02:23:21.060 --> 02:23:23.260]   Can you give me the evidence that supports that?
[02:23:23.260 --> 02:23:25.560]   Can you give me evidence that supports this other thing?
[02:23:25.560 --> 02:23:27.360]   Boy, would that be incredible.
[02:23:27.360 --> 02:23:28.520]   Would that be just incredible.
[02:23:28.520 --> 02:23:30.360]   - Just a long discourse.
[02:23:30.360 --> 02:23:33.300]   - Just to be part of, whether it's a medical diagnosis
[02:23:33.300 --> 02:23:35.840]   or whether it's the various treatment options
[02:23:35.840 --> 02:23:38.360]   or whether it's a legal case
[02:23:38.360 --> 02:23:41.600]   or whether it's a social problem that people are discussing,
[02:23:41.600 --> 02:23:46.600]   be part of the dialogue, one that holds itself and us
[02:23:48.240 --> 02:23:51.540]   accountable to reasons and objective dialogue.
[02:23:51.540 --> 02:23:54.140]   I get goosebumps talking about it.
[02:23:54.140 --> 02:23:56.140]   It's like, this is what I want.
[02:23:56.140 --> 02:24:00.980]   - So when you create it, please come back on the podcast
[02:24:00.980 --> 02:24:03.500]   so we can have a discussion together
[02:24:03.500 --> 02:24:04.780]   and make it even longer.
[02:24:04.780 --> 02:24:07.460]   This is a record for the longest conversation ever.
[02:24:07.460 --> 02:24:08.460]   It was an honor.
[02:24:08.460 --> 02:24:09.380]   It was a pleasure, David.
[02:24:09.380 --> 02:24:10.220]   Thank you so much for talking to me.
[02:24:10.220 --> 02:24:11.780]   - Thanks so much, a lot of fun.
[02:24:11.780 --> 02:24:14.360]   (upbeat music)
[02:24:14.360 --> 02:24:16.940]   (upbeat music)
[02:24:16.940 --> 02:24:19.520]   (upbeat music)
[02:24:19.520 --> 02:24:22.100]   (upbeat music)
[02:24:22.100 --> 02:24:24.680]   (upbeat music)
[02:24:24.680 --> 02:24:27.260]   (upbeat music)
[02:24:27.260 --> 02:24:37.260]   [BLANK_AUDIO]

