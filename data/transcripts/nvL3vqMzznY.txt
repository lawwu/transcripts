
[00:00:00.000 --> 00:00:04.080]   You start with this idea that democracy is green and we should have tons and tons of people participating.
[00:00:04.080 --> 00:00:09.540]   Tons of people participate and then it turns out that most participation is actually just noise and not that useful.
[00:00:09.540 --> 00:00:15.960]   That really squarely puts SPF into the finance crowd, much more so than startups or crypto.
[00:00:15.960 --> 00:00:19.840]   Founders will always talk about building and startups are so important or whatever.
[00:00:19.840 --> 00:00:21.340]   And what are all of them doing in their spare time?
[00:00:21.340 --> 00:00:26.480]   They're reading books, they're reading essays, and then those books and essays influence how they think about stuff.
[00:00:26.480 --> 00:00:32.260]   Okay, today I have the pleasure of talking with Nadia Asperova.
[00:00:32.260 --> 00:00:38.440]   She is previously the author of Working in Public, the Making and Maintenance of Open Source Software,
[00:00:38.440 --> 00:00:43.140]   and she is currently researching what the new tech elite will look like.
[00:00:43.140 --> 00:00:45.040]   Nadia, welcome to the podcast.
[00:00:45.040 --> 00:00:46.440]   Thanks for having me.
[00:00:46.440 --> 00:00:51.560]   Yeah, okay, so this is perfect timing, obviously, given what's been happening with SPF.
[00:00:51.560 --> 00:00:55.400]   How much do you think SPF was motivated by effective altruism?
[00:00:55.400 --> 00:01:02.920]   Where do you place him in the whole dimensionality of idea machines and motivations?
[00:01:02.920 --> 00:01:07.720]   Yeah, I mean, I know there's sort of like conflicting accounts going around.
[00:01:07.720 --> 00:01:14.200]   Like, I mean, just from my sort of like character study or looking at SPF, it seems pretty clear to me that he is
[00:01:14.200 --> 00:01:20.520]   sort of inextricably tied to the concepts of utilitarianism that then motivate effective altruism.
[00:01:20.520 --> 00:01:27.680]   The difference for me in sort of like where I characterize effective altruism is I think it's much closer to sort of like
[00:01:27.680 --> 00:01:35.160]   finance Wall Street elite mindset than it is to startup mindset, even though a lot of people associate effective altruism with tech people.
[00:01:35.160 --> 00:01:42.480]   So, yeah, to me, like that really squarely puts SPF in sort of like the finance crowd, much more so than startups or crypto.
[00:01:42.480 --> 00:01:45.280]   And I think that's something that gets really misunderstood about him.
[00:01:45.280 --> 00:01:54.160]   Interesting. Yeah, I find that interesting because if you think of Jeff Bezos, when he started Amazon, he wasn't somebody like John Perry Barlow,
[00:01:54.160 --> 00:01:57.360]   who was just motivated by the free philosophy of the Internet.
[00:01:57.360 --> 00:02:02.240]   You know, he saw a graph of Internet usage going up into the right and he's like, I should build a business on top of this.
[00:02:02.240 --> 00:02:11.080]   And in a sort of loopholy way, try to figure out, like, what is the thing that is that is the first thing you would want to put a SQL database on top of to ship and produce?
[00:02:11.080 --> 00:02:15.480]   And books was the answer. So and obviously he also came from a hedge fund. Right.
[00:02:15.480 --> 00:02:22.000]   Would you play somebody like him also in the old finance crowd rather than as a startup founder?
[00:02:22.000 --> 00:02:32.360]   Yeah, it's kind of a weird one because he's both associated with the early computing revolution, but then also AWS was sort of like what kicked off all of the 2010s sort of startup.
[00:02:32.360 --> 00:02:40.880]   And I think in the way that he's started thinking about his public legacy and just from sort of his public behavior, I think he fits much more squarely now in that sort of tech startup elite
[00:02:40.880 --> 00:02:48.240]   mindset of the 2010s crowd, more so than the Davos elite crowd of the 2000s.
[00:02:48.240 --> 00:02:50.760]   What in specific are you referring to?
[00:02:50.760 --> 00:02:57.880]   Well, he's come out and been like sort of openly critical about a lot of like Davos type institutions.
[00:02:57.880 --> 00:03:03.800]   He kind of pokes fun at mainstream media and for not believing in him, not believing in AWS.
[00:03:03.800 --> 00:03:17.000]   And I think he's because he sort of like spans across like both of these generations, he's been able to see the evolution of like how maybe like his earlier peers function versus the sort of second cohort of peers that he came across.
[00:03:17.000 --> 00:03:39.640]   But to me, he seems much more like much, much more of the sort of like startup elite mindset. And I can kind of back up a little bit there, but what I associate with the Davos Wall Street kind of crowd is much more of this focus on quantitative thinking, measuring efficiency, and then also this like globalist mindset.
[00:03:39.640 --> 00:03:48.800]   Like I think that the vision that they want to ensure for the world is this idea of like a very interconnected world where we, you know, sort of like the United Nations kind of mindset.
[00:03:48.800 --> 00:04:08.000]   And that is really like literally what the Davos gathering is, whereas Bezos from his actions today feels much closer to the startup like Y Combinator post AWS kind of mindset of founders that were really made their money by taking these non-obvious bets on talented people.
[00:04:08.000 --> 00:04:20.680]   So they were much less focused on credentialism. They were much more into this idea of meritocracy. I think we sort of forget like how commonplace this trope is of like, you know, the young founder in a dorm room.
[00:04:20.680 --> 00:04:32.560]   And that was really popularized by the 2010s cohort of the startup elite of being someone that may have like absolutely no skills, no background in industry, but can somehow sort of like turn the entire industry over on its head.
[00:04:32.560 --> 00:04:52.080]   And I think that was sort of like the unique insight of the tech startup crowd. And yeah, when I think about just sort of like some of the things that Bezos is doing now, it feels like she identifies with that much more strongly of being the sort of like lone cowboy or having this like one, one talented person with really great ideas who can sort of change the world.
[00:04:52.080 --> 00:05:07.880]   I think about the, what is it called? The Altos Institute or the new like science initiative that he put out where he was recruiting these like scientists from academic institutions and paying them really high salaries just to attract like the very best top scientists from around the world.
[00:05:07.880 --> 00:05:17.240]   That's much more of that kind of mindset than it is about like putting faith in sort of like existing institutions, which is what we would see from more of like a Davos kind of mindset.
[00:05:17.240 --> 00:05:30.840]   Interesting. Do you think that in the future, like the kids of today's tech billionaires will be future aristocrats, so effective altruism will be a sort of elite aristocratic philosophy? They'll be like tomorrow's Rockefellers?
[00:05:30.840 --> 00:05:42.400]   Yeah, I kind of worry about that, actually. I think of there as being like within the US, we were kind of lucky in that we have these two different types of elites. We have the aristocratic elites and we have meritocratic elites.
[00:05:42.400 --> 00:05:59.320]   Most other countries, I think, basically just have aristocratic elites, especially comparing like the US to Britain in this way. And so in the aristocratic model, your wealth and your power is sort of like conferred to you by previous generations, you just kind of like inherit it from your parents or your family or whomever.
[00:05:59.320 --> 00:06:13.080]   And the upside of that, if there is an upside, is that you get really socialized into this idea of what does it mean to be a public steward? What does it mean to think of yourself and like your responsibility to the rest of society as a sort of like privileged elite person?
[00:06:13.080 --> 00:06:25.760]   In the US, we have this really great thing where you can kind of just, you know, we have the American dream, right? So lots of people that didn't grow up with money can break into the elite ranks by doing something that makes them really successful.
[00:06:25.760 --> 00:06:37.120]   And that's like a really special thing about the US. So we have this whole class of like meritocratic elites who may not have aristocratic backgrounds, but ended up doing something within their lifetimes that made them successful.
[00:06:37.120 --> 00:06:46.960]   And so, yeah, I think it's a really cool thing. The downside of that being that you don't really get like socialized into what does it mean to like have this fortune and do something interesting with your money?
[00:06:46.960 --> 00:07:01.200]   You don't have this sort of like generational benefit that the aristocratic elites have of sort of presiding over your land or whatever you want to call it, where you're sort of like learning how to think about yourself in relation to the rest of society.
[00:07:01.200 --> 00:07:10.480]   And so it's much easier to just kind of like hoard your wealth or whatever. And so when you think about sort of like what are the next generations, the children of the meritocratic elites going to look like or what are they going to do?
[00:07:10.480 --> 00:07:24.640]   It's very easy to imagine kind of just becoming aristocratic elites in the sense of like, yeah, they're just going to like inherit the money from their families and they haven't also really been socialized into like how to think about their role in society.
[00:07:24.640 --> 00:07:39.920]   And so, yeah, all the meritocratic elites eventually turn into aristocratic elites, which is where I think you start seeing this trend now towards people wanting to sort of like spend down their fortunes within their lifetime or within a set number of decades after they die because they kind of see what happened in previous generations.
[00:07:39.920 --> 00:07:41.920]   And they're like, oh, I don't I don't want to do that.
[00:07:41.920 --> 00:07:52.800]   Yeah, well, it's interesting you mentioned that the aristocratic elites feel they have the responsibility to give back, I guess, more so than the meritocratic elites.
[00:07:52.800 --> 00:08:03.120]   But I believe that in the U.S., the amount of people who give to philanthropy and the total amount they give is higher than in Europe, right, where they probably have a higher ratio of aristocratic elites.
[00:08:03.120 --> 00:08:10.000]   Wouldn't you expect the opposite if the aristocratic elites are the ones that are, you know, inculcated to give back?
[00:08:10.000 --> 00:08:20.400]   Well, I assume like most of the people that are the figures about sort of like Americans giving back is spread across like all Americans, not just the wealthiest.
[00:08:20.400 --> 00:08:28.880]   Yeah. So you would predict that among the top 10 percent of Americans, there's less philanthropy than the top 10 percent of Europeans?
[00:08:28.880 --> 00:08:35.200]   There's sorry, I'm not sure I understand the question.
[00:08:35.200 --> 00:08:45.440]   I guess, does the ratio of meritocratic to aristocratic elites change how much philanthropy there is among the elites?
[00:08:45.440 --> 00:08:57.360]   Yeah, I mean, like here we have much more of a culture of like even among aristocratic elites, this idea of like institution building or like large donations to like build institutions.
[00:08:57.360 --> 00:09:00.720]   Whereas in Europe, a lot of the public institutions are created by government.
[00:09:00.720 --> 00:09:06.160]   And there's sort of this mentality of like private citizens don't experiment with public institutions.
[00:09:06.160 --> 00:09:11.040]   That's the government's job. And you see that sort of like pervasively throughout all of like European cultures.
[00:09:11.040 --> 00:09:16.400]   Like when we want something to change in public society, we look to government to like regulate or change it.
[00:09:16.400 --> 00:09:19.520]   Whereas in the U.S., it's kind of much more like choose your own adventure.
[00:09:19.520 --> 00:09:26.080]   And we don't really see the government as like the sole provider or shaper of public institutions.
[00:09:26.080 --> 00:09:36.640]   We also look at private citizens and like there's so many things that like public institutions that we have now that were not started by government, but were started by private philanthropists.
[00:09:36.640 --> 00:09:39.040]   And that's like a really unusual thing about about the U.S.
[00:09:39.040 --> 00:09:54.240]   So there's this common pattern in philanthropy where a guy will become a billionaire and then his wife will be heavily involved with or even potentially in charge of, you know, the family's philanthropic efforts.
[00:09:54.240 --> 00:10:02.240]   And there's many examples of this, right, like Bill and Melinda Gates, you know, Mark Zuckerberg and yeah, yeah, exactly.
[00:10:02.240 --> 00:10:05.840]   And Dustin Moskovitz. And yeah.
[00:10:05.840 --> 00:10:08.960]   So what is the consequence of this?
[00:10:08.960 --> 00:10:16.400]   How is philanthropy, the causes and the foundations, how are they different because of this pattern?
[00:10:16.400 --> 00:10:25.520]   Well, I mean, I feel like we see that pattern like the problems that what what even is philanthropy is changing very quickly.
[00:10:25.520 --> 00:10:28.960]   So we can say historically that.
[00:10:28.960 --> 00:10:43.520]   Not even historically, in recent history, in recent decades, that has probably been true, that wasn't true in, say, like late 1800s, early 1900s, it was, you know, Carnegie and Rockefeller were the ones that were actually doing their own philanthropy, not their spouses.
[00:10:43.520 --> 00:10:46.880]   So I'd say it's a more recent trend.
[00:10:46.880 --> 00:10:56.720]   But now I think we're also seeing this thing where like a lot of wealthy people are not necessarily doing their philanthropic activities through foundations anymore.
[00:10:56.720 --> 00:11:12.880]   And that's true both within like traditional philanthropy sector and sort of like the looser definition of what we might consider to be philanthropy, depending on how you define it, which I kind of more broadly want to define as like the actions of elites that are sort of like, you know, public facing activities.
[00:11:12.880 --> 00:11:24.880]   But like, even within sort of traditional philanthropy circles, we have like, you know, the 51C3 nonprofit, which is, you know, traditionally how people, you know, house all their money in a foundation, and then they do their philanthropic activities out of that.
[00:11:24.880 --> 00:11:29.600]   But in more recent years, we've seen this trend towards like LLCs.
[00:11:29.600 --> 00:11:33.280]   So Emerson Collective, I think, might have been maybe the first one to do it.
[00:11:33.280 --> 00:11:37.200]   And that was Steve Jobs' Philanthropic Foundation.
[00:11:37.200 --> 00:11:41.520]   And then Mark Zuckerberg with Chan Zuckerberg Initiative also used an LLC.
[00:11:41.520 --> 00:11:52.400]   And then since then, a lot of other, especially within sort of like tech wealth, we've seen that move towards people using LLCs instead of 51C3s, because it just gives you a lot more flexibility in the kinds of things you can fund.
[00:11:52.400 --> 00:11:55.040]   You don't just have to fund other nonprofits.
[00:11:55.040 --> 00:11:57.520]   And then you also see donor advised funds.
[00:11:57.520 --> 00:12:01.920]   So DAFs, which are sort of this like hacky workaround to foundations as well.
[00:12:01.920 --> 00:12:15.440]   So I guess point being that like, this sort of mental model of like, you know, one person makes a ton of money, and then their spouse kind of directs these like, nice, feel good, like philanthropic activities, I think is like, may not be the model that we continue to move forward on.
[00:12:15.440 --> 00:12:43.360]   And I'm kind of hopeful or curious to see, like, what does a return to like, because we've had so many new people making a ton of money in the last 10 years or so, we might see this return to sort of like, the Gilded Age style of philanthropy, where people are not necessarily just like, forming a philanthropic foundation and looking for the nicest causes to fund, but are actually just like thinking a little bit more holistically about like, how do I help build and create like a movement around a thing that I really care about?
[00:12:43.360 --> 00:13:08.800]   How do I think more broadly around like funding companies and nonprofits and individuals and like doing lots of different kinds of activities, because I think like the broader goal that like, motivates at least, like, the new sort of elite classes to want to do any of this stuff at all, like, I don't really think philanthropy is about altruism, I just, I think like the term philanthropy is just totally fraud and like refers to too many different things, and it's not very helpful.
[00:13:08.800 --> 00:13:29.040]   But I think like the part that I'm interested in, at least is sort of like what motivates elites to go from just sort of like making a lot of money and then like thinking about themselves to them thinking about sort of like their place in broader public society, and I think that starts with thinking about how do I control like media, academia, government are sort of like the three like arms of the public sector.
[00:13:29.040 --> 00:13:48.720]   And we think of it in that way, a little bit more broadly, where it's really much more about sort of like maintaining control over your own power, more so than sort of like this like altruistic kind of, you know, whitewashed, then it becomes like, you know, there's so many other like creative ways to think about like how that might happen.
[00:13:48.720 --> 00:14:18.080]   That's, that's, that's really interesting. That's, yeah, that's a really interesting way of thinking about what it is you're doing with philanthropy. Isn't the word noble descended from a word that basically means to give alms to people like if you're in charge of them, you give alms to them. And in a way, I mean, it might have been another word I'm thinking of, but in a way, yeah, a part of what motivates altruism, not obviously all of it, but part of it is that, yeah, you influence and
[00:14:18.080 --> 00:14:30.240]   give people power, not even in a necessarily negative connotation. But that's definitely what motivates altruism. So having that put square front and center is refreshing and honest, actually.
[00:14:30.240 --> 00:14:46.640]   Yeah, I don't I really don't see it as like a negative thing at all. And I think most of the like, you know, writing and journalism and academia that focuses on philanthropy tends to be very wealth critical. I'm not at all like I personally don't feel wealth critical at all.
[00:14:46.640 --> 00:15:00.560]   I think like, again, sort of returning to this like mental model of like aristocratic and meritocratic elites, aristocratic elites are able to still like pass down, like encode what they're supposed to be doing in each generation because they have this kind of like familial ties.
[00:15:00.560 --> 00:15:15.760]   And I think like on the meritocratic side, like if you didn't have any sort of language around altruism or public stewardship, then like, it's like, you need to kind of create that narrative for the meritocratic elite or else, you know, there's just like nothing to hold on to.
[00:15:15.760 --> 00:15:29.360]   And I think like, it makes sense to talk in those terms. Andrew Carnegie being sort of the father of modern philanthropy in the US like wrote these series of essays about wealth that were like very influential and where he sort of talks about this like moral obligation.
[00:15:29.360 --> 00:15:51.040]   And I think like, really, it was kind of this like, a quiet way for him to even though it was ostensibly about sort of like giving back or, you know, helping lift up the next generation of people, the next generation of entrepreneurs, like I think it really was much more of a protective stance of saying like, if he doesn't frame it in this way, then people are just going to knock down the concept of wealth altogether.
[00:15:51.040 --> 00:16:20.000]   Yeah, yeah, yeah, no, that's really interesting. And it's interesting, in which cases this kind of influence has been successful and where it's not. When Jeff Bezos bought the Washington Post, it's has there been any in any counterfactual impact on how the Washington Post has run as a result? I doubt it. But you know, when must takes over Twitter, I guess it's a much more expensive purchase. We'll see what the influence is negative or positive, but it's certainly different than what Twitter otherwise would have been. So control over media, it's, I guess it's a bigger meme now.
[00:16:20.000 --> 00:16:48.880]   Let me just take a digression and ask about open source for a second. So based on your experience, studying these open source projects, do you find the theory that Homer and Shakespeare were basically container words for these open source repositories that stretched out through centuries? Do you find that more plausible now, rather than that being individuals, of course? Do you find that more plausible now, given your given your study of open source?
[00:16:48.880 --> 00:17:02.880]   Sorry, what did the idea is that they weren't just one person. It was just like a whole bunch of people throughout a bunch of centuries who composed different parts of each story or composed different stories.
[00:17:02.880 --> 00:17:32.480]   The Nicholas Berbaki model, same concept of, you know, a single mathematician who's actually comprised of like lots of different. I think it's actually the opposite would be sort of my conclusion. We think of open source as this very like, collective volunteer effort. And I think, use that as an excuse to not really contribute back to open source or not really think about like how open source projects are maintained, because we were like, you know, you kind of have this bystander effect where you're like, well, you know, someone's taking care of it.
[00:17:32.480 --> 00:17:52.160]   It's volunteer oriented, like, of course, there's someone out there taking care of it. But in reality, it actually turns out it is just one person. So maybe it's a little bit more like a Wizard of Oz type model. It's actually just like one person behind the curtain that's like, you know, doing everything and you see this huge, you know, grandeur and you think, there must be so many people that are just one person.
[00:17:52.160 --> 00:18:22.080]   Yeah, and I think that's sort of undervalued. I think a lot of the rhetoric that we have about open source is rooted in sort of like early 2000s kind of starry eyed idea about like the power of the internet and the idea of like crowdsourcing and Wikipedia and all this stuff. And then like, in reality, like we kind of see this convergence from like very broad based collaborative volunteer efforts to like, narrowing down to kind of like single creators. And I think a lot of like, you know, single creators are the people that are really driving a lot of the internet today, and a lot of cultural production.
[00:18:22.080 --> 00:18:41.720]   Oh, that's that's super fascinating. Does that in general make you more sympathetic towards the lone genius view of accomplishments in history? Not just in literature, I guess. But just like when you think back to how likely is it that, you know, Newton came up with all that stuff on his own versus how much was fed into him by, you know, the others around him?
[00:18:41.720 --> 00:19:11.640]   Yeah, I think so. I feel I've never been like a big, like, you know, great founder theory kind of person. I think I'm like, my true theory is, I guess that ideas are maybe some sort of like sentient, like, concept or virus that operates outside of us. And we are just sort of like the vessels through which like ideas flow. So in that sense, you know, it's not really about any one person. But I do think I think I tend
[00:19:11.640 --> 00:19:41.240]   to lean, like, in terms of sort of like, where does creative, like, creative effort come from, I do think a lot of it comes much more from like, a single individual than it does from with some of the crowds. But everything just serves like different purposes, right? Like, because I think like, with an open source, it's like, not all of open source maintenance work is creative. In fact, most of it is pretty boring and dredgerous. And that's the stuff that no one wants to do. And that, like, one person kind of gets stuck with doing and that's really different from like, who created a certain open source projects, which is a little
[00:19:41.240 --> 00:19:43.120]   bit more of that, like, creative mindset.
[00:19:43.120 --> 00:20:08.360]   Yeah, yeah, that's really interesting. Do you think more projects in open source, so just take a popular repository? On average, do you think that these repositories would be better off if, let's say a larger percentage of them where pull requests were closed and feature requests were closed? You can look at the code, but you can't interact with it or its creators anyway? Should more repositories have this model?
[00:20:09.360 --> 00:20:10.400]   Yeah, definitely. I think so.
[00:20:10.400 --> 00:20:24.480]   Yeah, yeah. I mean, it's interesting to think about the implications of this for other areas outside of code, right, which is where it gets really interesting. I mean, in general, there's like a discussion. Sorry, go ahead. Yeah.
[00:20:24.880 --> 00:20:51.440]   Oh, that's good. I mean, that's basically what's for the writing of my book, because I was like, okay, I feel like whatever's happening open source right now, you start with this idea that like democracy is green, and like, we should have tons and tons of people participating, tons of people participate, and then it turns out that like, most participation is actually just noise and not that useful. And then it ends up like scaring everyone away. And in the end, you just have like, you know, one or a small handful of people that are actually doing all the work while everyone else is kind of like screaming around them. And this becomes like a really great metaphor for what happens in social media.
[00:20:51.720 --> 00:21:13.960]   And the reason I after I wrote the book, I went and worked at Substack. And, you know, part of it was because I was like, I think the model is kind of converging from like, you know, Twitter being this big open space to like, suddenly everyone is retreating, like, the public space is so hostile that everyone must retreat into like, smaller private spaces. So then, you know, chats became a thing, Substack became a thing. And yeah, I just feel sort of like realistic, right?
[00:21:13.960 --> 00:21:18.920]   Yeah, no, that's really fascinating. Yeah, the Straussian message in that book is very strong.
[00:21:21.600 --> 00:21:47.200]   But in general, there's, when you're thinking about something like corporate governance, right? There's a big question. And I guess even more interestingly, when you think if you think DAOs are going to be a thing, and you think that we will have to reinvent corporate governance from the ground up, there's a question of, should these be run like monarchy? Should they be sort of oligarchies where the board is in control? Should they be just complete democracies where everybody gets one vote on what you do at the next, you know, shareholder meeting or something?
[00:21:48.080 --> 00:21:59.000]   And this book and that analysis is actually pretty interesting to think about. Like, how should corporations be run differently, if at all? What does it inform how you think the average corporation should be run?
[00:21:59.720 --> 00:22:27.120]   Yeah, definitely. I mean, I think we are seeing a little bit, I'm not a corporate governance expert, but I do feel like we're seeing a little of this, like, backlash against, like, you know, shareholder activism, and like, extreme focus on sort of like DEI and boards and things like that. And like, I think we're seeing a little of people starting to like take the reins and take control again, because they're like, ah, that doesn't really work so well, it turns out. I think DAOs are going to learn this hard lesson as well.
[00:22:27.760 --> 00:22:57.640]   It's still maybe just too early to say what is happening in DAOs right now. But at least the ones that I've looked at, it feels like there is a very common failure mode of people saying, you know, like, let's just have like, let's have this be super democratic and like, leave it to the crowd to kind of like run this thing and figure out how it works. And it turns out you actually do need a strong leader, even the beginning. And this, this is something I learned just from looking at open source projects where it's like, you know, very rarely, or if at all, do you have a project that starts sort of like leaderless
[00:22:57.640 --> 00:23:03.840]   and faceless. And then, you know, usually there is some strong creator, leader or influential
[00:23:03.840 --> 00:23:07.720]   figure that is like driving the project forward for a certain period of time. And then you
[00:23:07.720 --> 00:23:10.800]   can kind of get to the point when you have enough of an active community that maybe that
[00:23:10.800 --> 00:23:14.320]   leader takes a step back and lets other people take over. But it's not like you can do that
[00:23:14.320 --> 00:23:19.540]   off day one. And that's sort of this open question that I have for crypto as an industry
[00:23:19.540 --> 00:23:23.000]   more broadly, because I think like if I think about sort of like what is defining each of
[00:23:23.000 --> 00:23:28.480]   these generations of people that are pushing forward new technological paradigms. I mentioned
[00:23:28.480 --> 00:23:33.440]   that like Wall Street finance mindset is very focused on like globalism and on this sort
[00:23:33.440 --> 00:23:38.840]   of like efficiency, quantitative mindset. You have the tech Silicon Valley, Y Combinator
[00:23:38.840 --> 00:23:43.280]   kind of generation that is really focused on top talent and the idea of this sort of
[00:23:43.280 --> 00:23:47.880]   like, you know, founder mindset, the power of like individuals breaking institutions.
[00:23:47.880 --> 00:23:52.440]   And then you have like the crypto mindset, which is this sort of like faceless leaderless,
[00:23:52.440 --> 00:23:59.960]   like governed by protocol and by code mindset, which is like intriguing to me. But I have
[00:23:59.960 --> 00:24:05.640]   a really hard time squaring it with seeing like in some sense, open source was the experiment
[00:24:05.640 --> 00:24:09.960]   that started playing out, you know, 20 years before then. And some things are obviously
[00:24:09.960 --> 00:24:15.240]   different in crypto because tokenization completely changes the incentive system for contributing
[00:24:15.240 --> 00:24:19.800]   and maintaining crypto projects versus like traditional open source projects. But in the
[00:24:19.800 --> 00:24:22.720]   end, also like humans are humans. And like, I feel like there are a lot of lessons to
[00:24:22.720 --> 00:24:27.120]   be learned from open source of like, you know, they also started out early on as being very
[00:24:27.120 --> 00:24:33.600]   starry eyed about the power of like hyperdemocratic regimes. And it turned out like that just
[00:24:33.600 --> 00:24:38.960]   like doesn't work in practice. And so like, how is Cryptogaster like square that? I'm
[00:24:38.960 --> 00:24:41.200]   just very curious to see what happens.
[00:24:41.200 --> 00:24:44.920]   Yeah, that's super fascinating. That raises an interesting question, by the way, you've
[00:24:44.920 --> 00:24:48.760]   written about idea machines, and you can explain that concept while you answer this question.
[00:24:48.760 --> 00:24:55.480]   But do you think that movements can survive without a charismatic founder who is both
[00:24:55.480 --> 00:25:02.040]   alive and engaged? So once Will McCaskill dies, would you be shorting effective altruism?
[00:25:02.040 --> 00:25:07.120]   Or if like Tyler Cowen dies, would you be short progress studies? Or do you think that,
[00:25:07.120 --> 00:25:09.560]   you know, once you get a movement off the ground, it can survive on its own?
[00:25:09.560 --> 00:25:14.400]   That's a good question. I mean, like, I don't think there's some perfect template, like
[00:25:14.400 --> 00:25:18.680]   each of these kind of has its own sort of unique quirks and characteristics to them.
[00:25:18.680 --> 00:25:24.480]   I guess, yeah, back up a little bit. Idea machines is this concept to have around what
[00:25:24.480 --> 00:25:28.280]   the transition from we were talking before about so like traditional 5.1c3 foundations
[00:25:28.280 --> 00:25:31.480]   as vehicles for philanthropy, what does the modern version of that look like that is not
[00:25:31.480 --> 00:25:36.600]   necessarily encoded in institution. And so I had this term idea machines, which is sort
[00:25:36.600 --> 00:25:40.440]   of this different way of thinking about like turning ideas into outcomes where you have
[00:25:40.440 --> 00:25:44.720]   a community that forms around a shared set of values and ideas. So yeah, you mentioned
[00:25:44.720 --> 00:25:48.680]   like progress studies is an example of that or effective altruism example. Eventually
[00:25:48.680 --> 00:25:54.640]   that community gets capitalized by some funders. And then it starts to be able to develop an
[00:25:54.640 --> 00:25:59.700]   agenda and then like actually start building like, you know, operational outcomes and like
[00:25:59.700 --> 00:26:07.080]   turning those ideas into real world initiatives. And remind me of your question again.
[00:26:07.080 --> 00:26:14.360]   So once the charismatic founder dies of a movement, is a movement basically handicapped
[00:26:14.360 --> 00:26:17.720]   in some way? Like maybe it'll still be a thing, but it's never going to reach the heights
[00:26:17.720 --> 00:26:21.240]   it could have reached if that main guy had been around.
[00:26:21.240 --> 00:26:26.000]   I think there are just like different shapes and classifications of like different types
[00:26:26.000 --> 00:26:29.480]   of communities here. So like, and I'm just thinking back again to sort of like different
[00:26:29.480 --> 00:26:32.640]   types of open source projects where it's not like they're like one model that fits perfectly
[00:26:32.640 --> 00:26:35.720]   for all of them. So I think there are some communities where it's like, yeah, I mean,
[00:26:35.720 --> 00:26:39.440]   I think effective altruism is maybe a good example of that where like the community has
[00:26:39.440 --> 00:26:44.320]   grown so much that I like, if all their leaders were to, you know, knock on wood, disappear
[00:26:44.320 --> 00:26:47.780]   tomorrow or something like that. Like, I think the movement would still keep going. There
[00:26:47.780 --> 00:26:52.400]   are enough true believers, like even within the next order of that community that like,
[00:26:52.400 --> 00:26:56.760]   I think that would just continue to grow. Whereas you have like, yeah, maybe certain
[00:26:56.760 --> 00:27:00.320]   like smaller or more nascent communities that are like, or just like communities that are
[00:27:00.320 --> 00:27:06.680]   much more like oriented around like a charismatic founder. That's just like a different type
[00:27:06.680 --> 00:27:10.560]   where if you lose that leader, then suddenly, you know, the whole thing falls apart because
[00:27:10.560 --> 00:27:14.720]   they're much more like these like cults or religions. And I don't think it makes one
[00:27:14.720 --> 00:27:19.440]   better or worse. It's like the right way to do it is probably like Bitcoin where you have
[00:27:19.440 --> 00:27:25.320]   a charismatic leader for life because that leader is not necessarily, can't go away,
[00:27:25.320 --> 00:27:29.480]   never die, but you still have the like, you know, North stars and like that.
[00:27:29.480 --> 00:27:34.880]   Yeah, it is funny. I mean, a lot of prophets have this property of you're not really sure
[00:27:34.880 --> 00:27:39.980]   what they believed in. So people with different temperaments can project their own preferences
[00:27:39.980 --> 00:27:44.880]   onto him. Somebody like Jesus, right? It's, you know, you can be like a super left winger
[00:27:44.880 --> 00:27:48.000]   and believe Jesus did for everything you believe in. You can be a super right winger and believe
[00:27:48.000 --> 00:27:55.120]   the same. Go ahead. I think there's value in like writing cryptically
[00:27:55.120 --> 00:27:59.280]   more. Like I think about like, I think Curtis Yarvin has done a really good job of this
[00:27:59.280 --> 00:28:03.580]   where, you know, intentionally or not, but because like his writing is so cryptic and
[00:28:03.580 --> 00:28:07.680]   long winded and like, it's like the Bible where you can just kind of like pour over
[00:28:07.680 --> 00:28:11.560]   endlessly being like, what does this mean? What does this mean? And in a weird, you know,
[00:28:11.560 --> 00:28:14.280]   you're always told to write very clearly. You're told to write succinctly, but like
[00:28:14.280 --> 00:28:17.640]   sometimes actually in a weird way, you can be much more effective by being very long
[00:28:17.640 --> 00:28:21.880]   winded and not obvious on what you're saying. Yes, which actually raises an interesting
[00:28:21.880 --> 00:28:25.760]   question that I've been wondering about. There have been movements, I guess, effective altruism
[00:28:25.760 --> 00:28:31.480]   is a good example that have been focused on community building in a sort of like explicit
[00:28:31.480 --> 00:28:37.480]   way. And then there's other movements where they have a charismatic founder. And moreover,
[00:28:37.480 --> 00:28:41.240]   this guy, he doesn't really try to recruit people. I'm thinking of somebody like Peter
[00:28:41.240 --> 00:28:46.160]   Thiel, for example, right? He goes on like once every year or two, he'll go on a podcast
[00:28:46.160 --> 00:28:50.600]   and have this like really cryptic back and forth. And then just kind of go away in a
[00:28:50.600 --> 00:28:56.720]   hole for a few months or a few years. And I'm curious, which one you think is more effective
[00:28:56.720 --> 00:29:00.360]   given the fact that you're not really competing for votes. So absolute number of people is
[00:29:00.360 --> 00:29:04.240]   not what you care about. It's not clear what you care about, but you do want to have more
[00:29:04.240 --> 00:29:12.320]   influence among the elites who matter in like politics and tech as well. So just your thoughts
[00:29:12.320 --> 00:29:16.320]   on those kinds of strategies, explicitly trying to community build versus just kind of projecting
[00:29:16.320 --> 00:29:21.480]   out there in a sort of cryptic way. Yeah, I mean, I definitely being somewhat cryptic
[00:29:21.480 --> 00:29:27.000]   myself. I favor the cryptic methodology. But I mean, yeah, I mean, you mentioned Peter
[00:29:27.000 --> 00:29:31.240]   Thiel. I think like the Thiel versus probably like the most like one of the most influential
[00:29:31.240 --> 00:29:36.120]   things. In fact, it is partly so effective because it is hard to even define what it
[00:29:36.120 --> 00:29:39.880]   is or wrap your head around that you just know that sort of like every interesting person
[00:29:39.880 --> 00:29:46.520]   you meet somehow has some weird connection to Peter Thiel. And it's funny. But I think
[00:29:46.520 --> 00:29:52.640]   this is sort of that evolution from the 51C3 foundation to the like idea machine implicit
[00:29:52.640 --> 00:29:58.120]   in that is this switch from, you know, you used to start the, you know, Nadia Asparova
[00:29:58.120 --> 00:30:01.600]   foundation or whatever. And it was like, you know, had your name on it. And it was all
[00:30:01.600 --> 00:30:06.080]   about like, what do I as a funder want to do in the world? Right. And you spend all
[00:30:06.080 --> 00:30:10.280]   this time doing this sort of like classical, you know, research, going out into the field,
[00:30:10.280 --> 00:30:13.360]   talking to people and you sit and you think, okay, like, here's a strategy I'm going to
[00:30:13.360 --> 00:30:18.960]   pursue. But like, ultimately, it's like, very, very donor centric in this very explicit way.
[00:30:18.960 --> 00:30:22.480]   And so within traditional philanthropy, you're seeing this sort of like backlash against
[00:30:22.480 --> 00:30:27.440]   that in like, you know, straight up like nonprofit land where now you're seeing the locus of
[00:30:27.440 --> 00:30:31.920]   power moving from being very donor centric to being sort of like community centric and
[00:30:31.920 --> 00:30:34.480]   people saying like, well, we don't really want the donors telling us what to do, even
[00:30:34.480 --> 00:30:38.760]   though it's also their money. And like, you know, instead, let's have this be driven by
[00:30:38.760 --> 00:30:45.960]   the community from the ground up. That's maybe like one very literal reaction against that,
[00:30:45.960 --> 00:30:49.360]   like having the donor as sort of the central power figure. But I think idea machines are
[00:30:49.360 --> 00:30:56.200]   kind of like the like, maybe like the more realistic or effective answer in that like,
[00:30:56.200 --> 00:30:59.960]   the donor is still like without the presence of a funder, like a community is just a community.
[00:30:59.960 --> 00:31:03.280]   They're just sitting around and talking about ideas of like, what could possibly happen?
[00:31:03.280 --> 00:31:07.800]   Like, they don't have any money to make anything happen. But like, I think like really effective
[00:31:07.800 --> 00:31:13.120]   funders are good at being sort of like subtle and thoughtful about like, like, you know,
[00:31:13.120 --> 00:31:17.760]   no one wants to see like the Peter Thiel Foundation necessarily, that's just like, it's so like
[00:31:17.760 --> 00:31:22.040]   not the style of how it works. But you know, you meet so many people that are being funded
[00:31:22.040 --> 00:31:25.840]   by the same person, like just going out and sort of aggressively like arming the rebels
[00:31:25.840 --> 00:31:31.240]   is a more sort of like, yeah, just like distributed, decentralized way of thinking about like spreading
[00:31:31.240 --> 00:31:34.680]   one's power, instead of just starting a foundation.
[00:31:34.680 --> 00:31:42.320]   Yeah, even if you look at the life of influential politicians, somebody like LBJ, or Robert
[00:31:42.320 --> 00:31:47.680]   Moses, it's how much of it was like calculated and how much of it is just like decades of
[00:31:47.680 --> 00:31:53.920]   building up favors and building up connections in a way that had no definite and clear plan.
[00:31:53.920 --> 00:31:57.480]   But it just you're hoping that someday you can call upon them and sort of like Godfather
[00:31:57.480 --> 00:32:04.120]   way. Yeah, that's interesting. And by the way, that this is also where your work on
[00:32:04.120 --> 00:32:08.040]   open source comes in, right? Like, there's this idea that in the movement, you know,
[00:32:08.040 --> 00:32:12.600]   everybody will come in with their ideas, and you can community build your way towards,
[00:32:12.600 --> 00:32:17.040]   you know, what should be funded. And, yeah, I'm inclined to believe that it's probably
[00:32:17.040 --> 00:32:21.440]   like a few people who have these ideas about what should be funded. And the rest of it
[00:32:21.440 --> 00:32:29.640]   is either just a way of like building up engagement and building up hype. Or, I don't know, or
[00:32:29.640 --> 00:32:33.240]   maybe just useless. But what are your thoughts on that?
[00:32:33.240 --> 00:32:39.000]   I was like, I am like, really very much a tech startup person and not a crypto person,
[00:32:39.000 --> 00:32:42.240]   even though I would very much like to be fun because I'm like, ah, this is the future.
[00:32:42.240 --> 00:32:46.040]   And there's so many interesting things happening. And I'm like, for the record, not at all like
[00:32:46.040 --> 00:32:50.800]   down in crypto. I think it is like the next big sort of movement of things that are happening.
[00:32:50.800 --> 00:32:55.600]   But when I really come down to like the mindset, it's like, I am so in that sort of like, top
[00:32:55.600 --> 00:32:59.720]   talent founder, like power of the individual to break institutions mindset, like that just
[00:32:59.720 --> 00:33:05.800]   resonates with me so much more than the like, leaderless, faceless, like, highly participatory
[00:33:05.800 --> 00:33:11.680]   kind of thing. And again, like, I am very open to that being true. Like, I maybe I'm
[00:33:11.680 --> 00:33:16.240]   so wrong on that. I just like, I have not yet seen evidence that that works in the world.
[00:33:16.240 --> 00:33:20.400]   I see a lot of rhetoric about how that could work or should work. We have this sort of
[00:33:20.400 --> 00:33:25.360]   like implicit belief that like, direct democracy is somehow like the greatest thing to aspire
[00:33:25.360 --> 00:33:30.680]   towards. But like, over and over, we see evidence that like that doesn't that just like doesn't
[00:33:30.680 --> 00:33:35.080]   really work. It doesn't even have to throw out the underlying principles or values behind
[00:33:35.080 --> 00:33:40.760]   that. Like, I still really believe in meritocracy. I really believe in like, access to opportunity.
[00:33:40.760 --> 00:33:43.760]   I really believe in like, pursuit of happiness. Like, to me, those are all like, very like
[00:33:43.760 --> 00:33:50.000]   American values. But like, I think that where that breaks is the idea that like, that has
[00:33:50.000 --> 00:33:54.680]   to happen through these like, highly participatory methods. I just like, yeah, I haven't seen
[00:33:54.680 --> 00:33:57.820]   really great evidence of that being that working.
[00:33:57.820 --> 00:34:03.360]   What does that imply about how you think about politics or at least political structures?
[00:34:03.360 --> 00:34:07.440]   You think it would you you elect a mayor, but like, just forget, no, no participation,
[00:34:07.440 --> 00:34:10.120]   he gets to do everything he wants to do for four years, and you can get rid of him in
[00:34:10.120 --> 00:34:15.320]   four years. But until then, no community meetings. Well, what does that imply about how you think
[00:34:15.320 --> 00:34:18.960]   cities and states and countries should be run?
[00:34:18.960 --> 00:34:30.600]   I have some very complicated thoughts on that. I mean, I, I think it's also like, everyone
[00:34:30.600 --> 00:34:35.120]   has the fantasy of when it'd be so nice if there were just one person in charge. I hate
[00:34:35.120 --> 00:34:39.080]   all this squabbling. It would just be so great if we could just, you know, have one person
[00:34:39.080 --> 00:34:45.000]   just who has exactly the views that I have and just put them in charge and let them run
[00:34:45.000 --> 00:34:50.000]   things. That would be very nice. I just, I do also think it's unrealistic. Like, I don't
[00:34:50.000 --> 00:34:57.760]   think I'm, you know, maybe like, monarchy sounds great in theory, but in practice just
[00:34:57.760 --> 00:35:04.840]   doesn't, like, I really embrace and I, and I think like, there is no perfect governance
[00:35:04.840 --> 00:35:08.640]   design either in the same way that there's no perfect open source project design or whatever
[00:35:08.640 --> 00:35:16.080]   else we're talking about. Like, yeah, it really just depends like, what is, like, what is
[00:35:16.080 --> 00:35:20.200]   your population comprised of? There are some very small homogenous populations that can
[00:35:20.200 --> 00:35:25.800]   be very easily governed by like, you know, a small government or one person or whatever,
[00:35:25.800 --> 00:35:28.880]   because there just isn't that much dissent or difference. Everyone is sort of on the
[00:35:28.880 --> 00:35:33.040]   same page. America is the extreme opposite in that angle. And I'm always thinking about
[00:35:33.040 --> 00:35:38.000]   America, because like, I don't know, I'm American and I love America. But like, you know, everyone
[00:35:38.000 --> 00:35:41.440]   is trying to solve the governance question for America. And I think like, yeah, I don't
[00:35:41.440 --> 00:35:45.320]   know. I mean, we're an extremely heterogeneous population. There are a lot of competing world
[00:35:45.320 --> 00:35:50.680]   views. I may not agree with all the views of everyone in America, but like, I also,
[00:35:50.680 --> 00:35:54.440]   like I don't want just one person that represents my personal views. I think like, I would focus
[00:35:54.440 --> 00:36:00.440]   more like effectiveness in governance than I would like having like, you know, just one
[00:36:00.440 --> 00:36:05.200]   person in charge or something that, like, I don't mind if someone disagrees with my
[00:36:05.200 --> 00:36:10.320]   views as long as they're good at what they do, if that makes sense. And so I think the
[00:36:10.320 --> 00:36:15.040]   questions are like, how do we improve the speed at which, like, our government works
[00:36:15.040 --> 00:36:20.160]   and the efficacy with which it works? Like, I think there's so much room to be, room for
[00:36:20.160 --> 00:36:25.120]   improvement there versus like, I don't know how much, like, I really care about, like,
[00:36:25.120 --> 00:36:28.240]   changing the actual structure of our government.
[00:36:28.240 --> 00:36:33.920]   Interesting. Going back to open source for a second. Why do these companies release so
[00:36:33.920 --> 00:36:39.840]   much stuff in open source for free? And it's probably literally worth trillions of dollars
[00:36:39.840 --> 00:36:43.800]   of value in total. And they just release it out and free and many of them are developer
[00:36:43.800 --> 00:36:48.640]   tools that other developers use to build competitors for these big tech companies that are releasing
[00:36:48.640 --> 00:36:52.560]   these open source tools. Why did they do it? What explains it?
[00:36:52.560 --> 00:36:58.160]   I mean, I think it depends on the specific project, but like a lot of times these are
[00:36:58.160 --> 00:37:02.600]   projects that were developed internally. It's the same reason of like, I think code and
[00:37:02.600 --> 00:37:06.040]   writing are not that dissimilar in this way of like, why do people spend all this time
[00:37:06.040 --> 00:37:10.640]   writing like long posts or papers or whatever and then just release them for free? Like,
[00:37:10.640 --> 00:37:13.760]   why not put everything behind a paywall? And I think the answer is probably still in both
[00:37:13.760 --> 00:37:19.060]   cases where like mind share is a lot more interesting than, you know, your literal IP.
[00:37:19.060 --> 00:37:23.000]   And so, you know, you put out, you write these like long reports or you tweet or whatever,
[00:37:23.000 --> 00:37:25.640]   like you spend all this time creating content for free and putting it out there because
[00:37:25.640 --> 00:37:29.160]   you're trying to capture mind share. Same thing with companies releasing open source
[00:37:29.160 --> 00:37:33.240]   projects. Like a lot of times they really want like other developers to come in and
[00:37:33.240 --> 00:37:37.120]   contribute to them. They want to increase their status as like an open source friendly
[00:37:37.120 --> 00:37:40.960]   kind of company or company or show like, you know, here's the type of code that we write
[00:37:40.960 --> 00:37:44.680]   internally and showing that externally they want to like recruiting is, you know, the
[00:37:44.680 --> 00:37:48.640]   hardest thing for any company. Right. And so being able to attract the right kinds of
[00:37:48.640 --> 00:37:52.560]   developers or people that, you know, might fit really well into their developer culture
[00:37:52.560 --> 00:37:55.480]   just matters a lot more. And they're just doing that instead of with words or doing
[00:37:55.480 --> 00:38:00.280]   that with code. You've talked about the need for more idea
[00:38:00.280 --> 00:38:03.680]   machines. You're like dissatisfied with the fact that Effective Altruism is a big game
[00:38:03.680 --> 00:38:10.960]   in town. Is there some idea or nascent movement where I mean, other than progress ideas, but
[00:38:10.960 --> 00:38:16.120]   like something where you feel like this could be a thing, but it just needs some like charismatic
[00:38:16.120 --> 00:38:20.680]   founder to take it to the next level or even if it doesn't exist yet, it just like a set
[00:38:20.680 --> 00:38:24.480]   of ideas around this vein is like clearly something there is going to exist. You know
[00:38:24.480 --> 00:38:28.040]   what I mean? Is there anything like that that you notice?
[00:38:28.040 --> 00:38:32.520]   I only had a couple of different possibilities in that post. Yeah, I think like the progress
[00:38:32.520 --> 00:38:37.840]   sort of meme is probably the largest growing contender that I would see right now. I think
[00:38:37.840 --> 00:38:42.960]   there's another one right now around sort of like the new right. That's not even like
[00:38:42.960 --> 00:38:47.000]   the best term necessarily for it, but there's sort of like a shared set of values there
[00:38:47.000 --> 00:38:52.480]   that are maybe starting with like politics, but like ideally spreading to like other areas
[00:38:52.480 --> 00:38:56.600]   of public influence. So I think like those are a couple of like the bigger movements
[00:38:56.600 --> 00:39:00.120]   that I see right now, but there's like smaller stuff too. Like I mentioned like tools for
[00:39:00.120 --> 00:39:05.480]   thought in that post where like that's never going to be a huge idea machine, but it's
[00:39:05.480 --> 00:39:09.000]   one where you have a lot of like interesting, talented people that are thinking about sort
[00:39:09.000 --> 00:39:14.160]   of like future of computing. But like until maybe more recently, like there just hasn't
[00:39:14.160 --> 00:39:18.320]   been a lot of funding available and the funding is always really uneven and unpredictable.
[00:39:18.320 --> 00:39:23.160]   And so that's to me an example of like a smaller community that like just needs that sort of
[00:39:23.160 --> 00:39:29.720]   like extra influx to turn a bunch of abstract ideas into practice. But yeah, I mean, I think
[00:39:29.720 --> 00:39:33.680]   like, yeah, those are some of like the bigger ones that I see right now. I think there is
[00:39:33.680 --> 00:39:38.840]   just so much more potential to do more, but I wish people would just think a little bit
[00:39:38.840 --> 00:39:43.440]   more creatively because yeah, I really do think like effective intrusive kind of becomes
[00:39:43.440 --> 00:39:46.400]   like the default option for a lot of people. Then they're kind of vaguely dissatisfied
[00:39:46.400 --> 00:39:50.920]   with it and they don't like think about like, well, what do I actually really care about
[00:39:50.920 --> 00:39:53.680]   in the world and how do I want to put that forward?
[00:39:53.680 --> 00:40:00.320]   Yeah. There's also the fact that effective altruism has this like very fit meme plex
[00:40:00.320 --> 00:40:06.160]   in the sense that it's like a polytheistic religion where if you have a cause area, then
[00:40:06.160 --> 00:40:10.400]   you don't have your own movement. You just have a cause area within our broader movement,
[00:40:10.400 --> 00:40:13.480]   right? It just like adopts your gods into our movement.
[00:40:13.480 --> 00:40:19.120]   Yeah, that's the same thing I see like people trying to lobby for effective altruism to
[00:40:19.120 --> 00:40:24.360]   care about their cause area. But then it's like, you could just start a separate, like
[00:40:24.360 --> 00:40:27.760]   if you can't get EA to care about, then why not just like start another one somewhere
[00:40:27.760 --> 00:40:28.760]   else?
[00:40:28.760 --> 00:40:36.520]   Yeah. So, you know, it's interesting to me that the wealth boom in Silicon Valley and
[00:40:36.520 --> 00:40:40.640]   in TechSphere has led to this outgrowth of philanthropy, but that hasn't always been
[00:40:40.640 --> 00:40:45.520]   the case. Even in America, like a lot of people became billionaires after energy markets were
[00:40:45.520 --> 00:40:51.400]   deregulated in the 80s and the 90s. And then there wasn't, and obviously the hub of that
[00:40:51.400 --> 00:40:56.880]   was like the Texas area or, you know, and there, as far as I'm aware, there wasn't like
[00:40:56.880 --> 00:41:03.800]   a boom of philanthropy motivated by the ideas that people in that region had. What's different
[00:41:03.800 --> 00:41:08.480]   about Silicon Valley? Why are they, or do you actually think that these other places
[00:41:08.480 --> 00:41:11.120]   have also had their own booms of philanthropic giving?
[00:41:11.120 --> 00:41:16.720]   No, I think you're right. Yeah. I would make a distinction between like being wealthy is
[00:41:16.720 --> 00:41:21.480]   not the same as being elite or whatever other term you want to use there. And so, yeah,
[00:41:21.480 --> 00:41:25.480]   there are definitely like pockets of what's called like more like local markets of wealth,
[00:41:25.480 --> 00:41:31.680]   like yeah, Texas oil or energy billionaires that tend to operate kind of just more in
[00:41:31.680 --> 00:41:36.440]   their own sphere. And a lot of, if you look at any philanthropic, like a lot of them will
[00:41:36.440 --> 00:41:42.320]   be philanthropically active, but they only really focus on their geographic area. But
[00:41:42.320 --> 00:41:49.080]   there's sort of this difference. And I think this is part of where it comes from the question
[00:41:49.080 --> 00:41:54.560]   of like, you know, like what forces someone to actually like do something more public
[00:41:54.560 --> 00:42:00.080]   facing with their power. And I think that comes from your power being sort of like threatened.
[00:42:00.080 --> 00:42:03.960]   That's like one aspect I would say of that. So like tech has only really become a lot
[00:42:03.960 --> 00:42:08.400]   more active in the public sphere outside of startups after the tech backlash of the mid
[00:42:08.400 --> 00:42:14.380]   2010s. And you can say a similar thing kind of happened with the Davos elite as well.
[00:42:14.380 --> 00:42:21.320]   And also for the Gilded Age cohort of wealth. And so, yeah, when you have sort of, you're
[00:42:21.320 --> 00:42:25.160]   kind of like, you know, building in your own little world and like, you know, we had literally
[00:42:25.160 --> 00:42:28.360]   like Silicon Valley where everyone was kind of like sequestered off and just thinking
[00:42:28.360 --> 00:42:32.520]   about startups and thinking themselves of like tech is essentially like an industry
[00:42:32.520 --> 00:42:36.120]   just like any other sort of, you know, entertainment or whatever. And we're just kind of happy
[00:42:36.120 --> 00:42:40.060]   building over here. And then it was only when sort of like the Panopticon like turned its
[00:42:40.060 --> 00:42:48.560]   head towards tech and started and they had this sort of like onslaught of critiques coming
[00:42:48.560 --> 00:42:52.680]   from sort of like mainstream discourse where they went, oh, like what is my place in this
[00:42:52.680 --> 00:42:57.880]   world? And, you know, if I don't try to like defend that, then I'm going to just kind of,
[00:42:57.880 --> 00:43:01.480]   yeah, we're going to lose all that power. So I think that that need to sort of like
[00:43:01.480 --> 00:43:05.640]   defend one's power can kind of like prompt that sort of action. The other aspect I'd
[00:43:05.640 --> 00:43:12.000]   highlight is just like, I think a lot of elites are driven by these like technological paradigm
[00:43:12.000 --> 00:43:17.120]   shifts. So there's this scholar, Carlotta Perrins, who writes about technological revolutions
[00:43:17.120 --> 00:43:21.960]   and financial capital. And she identifies like a few different technological revolutions
[00:43:21.960 --> 00:43:29.840]   over the last, whatever, 100 plus years that like drove this cycle of, you know, a new
[00:43:29.840 --> 00:43:33.840]   technology is invented. It's people are kind of like working on it in this smaller industry
[00:43:33.840 --> 00:43:40.080]   sort of way. There is some kind of like crazy like public frenzy and then like a backlash.
[00:43:40.080 --> 00:43:44.800]   And then from after that, then you have this sort of like focus on public institution building.
[00:43:44.800 --> 00:43:49.160]   But she really points out that like not all technology fits into that. Like not all technology
[00:43:49.160 --> 00:43:54.600]   is a paradigm shift. Sometimes technology is just technology. And so, yeah, I think
[00:43:54.600 --> 00:43:58.120]   like a lot of wealth might just fall into that category. My third example, by the way,
[00:43:58.120 --> 00:44:04.280]   is the Koch family because you had, you know, the Koch brothers, but then like their father
[00:44:04.280 --> 00:44:09.760]   was actually the one who like kind of initially made their wealth, but was like very localized
[00:44:09.760 --> 00:44:13.480]   in sort of like how he thought about philanthropy. He had his own like, you know, family foundation
[00:44:13.480 --> 00:44:17.320]   was just sort of like doing that sort of like, you know, Texas billionaire mindset that we're
[00:44:17.320 --> 00:44:20.680]   talking about of, you know, I made a bunch of money. I'm going to just sort of like,
[00:44:20.680 --> 00:44:26.120]   yeah, do my local philanthropic activity. It was only the next generation of his children
[00:44:26.120 --> 00:44:30.640]   that then like took that wealth and started thinking about like how do we actually like
[00:44:30.640 --> 00:44:36.480]   move that on to like a more elite stage and thinking about like their impact in the media.
[00:44:36.480 --> 00:44:40.040]   But like you can see there was like two clear generations within the same family. Like one
[00:44:40.040 --> 00:44:43.360]   has this sort of like local wealth mindset and one of them has the more like elite wealth
[00:44:43.360 --> 00:44:47.400]   mindset. And yeah, you can kind of like ask yourself, why did that switch happen? But
[00:44:47.400 --> 00:44:51.480]   yeah, it's clearly about more than just money. It's also about intention.
[00:44:51.480 --> 00:44:57.320]   Yeah, that's really interesting. Well, it's interesting because there's, if you identify
[00:44:57.320 --> 00:45:03.240]   the current mainstream media as affiliated with like that Davos aristocratic elite or
[00:45:03.240 --> 00:45:10.880]   maybe not aristocratic, but like the Davos group, yeah, exactly. There is a growing field
[00:45:10.880 --> 00:45:17.120]   of independent media, but you would not identify somebody like Joe Rogan as in the Silicon
[00:45:17.120 --> 00:45:23.760]   Valley sphere. Right. So there is a new media. I just I guess the startup people don't have
[00:45:23.760 --> 00:45:28.880]   that much influence over them yet. And they feel like, yeah, I think they're trying to
[00:45:28.880 --> 00:45:34.120]   like take that strategy. Right. So you have like a bunch of founders like Palmer Luckey
[00:45:34.120 --> 00:45:39.800]   and Mark Zuckerberg and Brian Armstrong and whoever else that like will not really talk
[00:45:39.800 --> 00:45:44.280]   to mainstream media anymore. They will not get an interview to the New York Times, but
[00:45:44.280 --> 00:45:49.400]   they will go to like an individual influencer or an individual creator and they'll do an
[00:45:49.400 --> 00:45:54.280]   interview with them. So like when Mark Zuckerberg announced Meta, like he did not get grand
[00:45:54.280 --> 00:45:57.560]   interviews to mainstream publications, but he went and talked to like Ben Thompson at
[00:45:57.560 --> 00:46:04.920]   Strategory. And so I think there is like it fits really well with that mindset of like,
[00:46:04.920 --> 00:46:09.360]   we're not necessarily institution building. We're going to like focus on power of individuals
[00:46:09.360 --> 00:46:13.520]   who sort of like defy institutions. And that is kind of like an open question that I have
[00:46:13.520 --> 00:46:17.960]   about like what will the long term influence of the tech elite look like? Because like,
[00:46:17.960 --> 00:46:25.520]   you know, the human history tells us that eventually all individual behaviors kind of
[00:46:25.520 --> 00:46:30.080]   get codified into institutions. Right. But we're obviously living in a very different
[00:46:30.080 --> 00:46:36.280]   time now. And I think like the way that the Davos elite managed to like really codify
[00:46:36.280 --> 00:46:40.640]   and extend their influence across all these different sectors was by taking that institutional
[00:46:40.640 --> 00:46:46.560]   mindset and, you know, like thinking about sort of like academic institutions and media
[00:46:46.560 --> 00:46:52.960]   institutions, all that stuff. If the startup mindset is really inherently like anti-institution
[00:46:52.960 --> 00:46:56.800]   and sort of like we don't want to build the next Harvard necessarily. We just want to
[00:46:56.800 --> 00:47:01.920]   like blow apart the concept of universities whatsoever. Or, you know, we don't want to
[00:47:01.920 --> 00:47:08.640]   create a new CNN or a new Fox News. We want to just like fund like individual creators
[00:47:08.640 --> 00:47:14.400]   to do that same sort of work, but in this very decentralized way. Like will that work
[00:47:14.400 --> 00:47:17.320]   long term? I don't know. Like, is that just sort of like a temporary state that we're
[00:47:17.320 --> 00:47:21.080]   in right now where no one really knows what the next institutions will look like? Or is
[00:47:21.080 --> 00:47:25.400]   that really like an important part of this generation where like we shouldn't be asking
[00:47:25.400 --> 00:47:28.120]   the question of like, how do you build a new media network? We should just be saying like
[00:47:28.120 --> 00:47:32.000]   the answer is there is no media network. We just go to like all these individuals instead.
[00:47:32.000 --> 00:47:41.920]   Yeah, that's interesting. What do you make of this idea that I think let's say that these
[00:47:41.920 --> 00:47:47.280]   idea machines might be limited by the fact that if you're going to start some sort of
[00:47:47.280 --> 00:47:54.540]   organization in them, you're very much depending on somebody who has made a lot of money independently
[00:47:54.540 --> 00:48:01.200]   to fund you and to grant you approval. And I just have a hard time seeing somebody who
[00:48:01.200 --> 00:48:09.360]   is like a Napoleon like figure being willing long term to live under that arrangement.
[00:48:09.360 --> 00:48:15.320]   And that so there'll just be the people who are just have this desire to dominate and
[00:48:15.320 --> 00:48:19.280]   be recognized who are probably pretty important to any movement you want to create. They'll
[00:48:19.280 --> 00:48:23.200]   just want to go off and just like build the company or something that gives them an independent
[00:48:23.200 --> 00:48:28.520]   footing first. And they just won't fall under any umbrella. You know what I mean?
[00:48:28.520 --> 00:48:34.040]   Yeah. I mean, like Dustin Moskovitz, for example, has been funding EA for a really long time
[00:48:34.040 --> 00:48:39.160]   and hasn't hasn't walked away necessarily. Yeah. I mean, on the flip side, you can see
[00:48:39.160 --> 00:48:45.080]   like SPF carried a lot of a lot of risk because it's your point, I guess, like, you know,
[00:48:45.080 --> 00:48:48.320]   you end up relying on this one funder. The one funder disappears and everything else
[00:48:48.320 --> 00:48:56.000]   kind of falls apart. I mean, I think like I don't have any sort of like preciousness
[00:48:56.000 --> 00:49:00.840]   attached to the idea of like communities, you know, lasting forever. I think this is
[00:49:00.840 --> 00:49:04.240]   like, again, if we're trying to solve for the problem of like what did not work well
[00:49:04.240 --> 00:49:10.720]   about 5.1c3 foundations for most of recent history, like part of it was that they're,
[00:49:10.720 --> 00:49:16.280]   you know, just meant to live on into perpetuity. Like why why do we still have like, you know,
[00:49:16.280 --> 00:49:19.040]   Rockefeller Foundation, there are now actually many different Rockefeller foundations, but
[00:49:19.040 --> 00:49:23.080]   like, why does that even exist? Like, why did that money not just get spent down? And
[00:49:23.080 --> 00:49:27.800]   actually, when John D. Rockefeller was first proposing the idea of foundations, he wanted
[00:49:27.800 --> 00:49:32.720]   them to be like to have like a finite end state. So he wanted them to last only like
[00:49:32.720 --> 00:49:36.280]   50 years or 100 years when he was proposing this like federal charter, but that federal
[00:49:36.280 --> 00:49:40.840]   charter failed. And so now we have these like state charters and foundations can just exist
[00:49:40.840 --> 00:49:44.600]   forever. But like, I think if we want to like improve upon this idea of like how do we prevent
[00:49:44.600 --> 00:49:49.000]   like meritocratic elites from turning into aristocratic elites? How do we like, yeah,
[00:49:49.000 --> 00:49:53.000]   how do we actually just like try to do a lot of really interesting stuff in our lifetimes?
[00:49:53.000 --> 00:49:57.360]   It's like a very, it's very counterintuitive because you think about like, leaving a legacy
[00:49:57.360 --> 00:50:01.680]   must mean like creating institutions or creating a foundation that lasts forever. And, you
[00:50:01.680 --> 00:50:04.960]   know, 200 years from now, there's still like the Nadia Asperova Foundation out there. But
[00:50:04.960 --> 00:50:08.720]   like, if I really think of it, it's like I would almost rather just do really, really,
[00:50:08.720 --> 00:50:14.120]   really good, interesting work in like 50 years or 20 years or 10 years and have that be the
[00:50:14.120 --> 00:50:19.440]   legacy versus your name kind of getting, you know, submerged over a century of institutional
[00:50:19.440 --> 00:50:25.520]   decay and decline. So yeah, I don't like if, you know, you have a community that lasts
[00:50:25.520 --> 00:50:30.240]   for maybe only last 10 years or something like that, and it's funded for that amount
[00:50:30.240 --> 00:50:34.440]   of time, and then it kind of elbows its usefulness and it winds down or it becomes less relevant.
[00:50:34.440 --> 00:50:38.480]   Like I don't necessarily see it as a bad thing. Of course, like in practice, you know, nothing
[00:50:38.480 --> 00:50:45.040]   ever ends that neatly and that quietly. But yeah, I don't think that's a bad thing.
[00:50:45.040 --> 00:50:50.760]   Yeah, yeah. Who are some ethnographers or sociologists from a previous era that have
[00:50:50.760 --> 00:50:54.240]   influenced your work? So was there somebody writing about, you know, what it was like
[00:50:54.240 --> 00:50:58.360]   to be in a Roman legion? Or what it was like to work in a factory floor? And you're like,
[00:50:58.360 --> 00:51:01.440]   you know what, I want to do that for open source? Or I want to do that for the new tech
[00:51:01.440 --> 00:51:03.360]   elite?
[00:51:03.360 --> 00:51:08.440]   For open source, I was definitely really influenced by Jane Jacobs and Eleanor Ostrom, who I think
[00:51:08.440 --> 00:51:16.280]   both had this quality of, so yeah, Eleanor Ostrom was looking at examples of common pool
[00:51:16.280 --> 00:51:21.240]   resources like fisheries or forests or whatever, and just like going and visiting them and
[00:51:21.240 --> 00:51:24.520]   spending a lot of time with them and then saying like, actually, I don't think tragedy
[00:51:24.520 --> 00:51:28.400]   of the commons is like a real thing, or it's not the only outcome that we can possibly
[00:51:28.400 --> 00:51:32.880]   have. Sometimes commons can be managed like perfectly sustainably, and it's not necessarily
[00:51:32.880 --> 00:51:36.800]   true that everyone just like treats them very extractively, and just like wrote about what
[00:51:36.800 --> 00:51:43.120]   she saw. And same with Jane Jacobs, sort of looking at cities as someone who lives in
[00:51:43.120 --> 00:51:45.600]   one, right? Like she didn't have any fancy credentials or anything like that. She was
[00:51:45.600 --> 00:51:50.040]   just like, I live in the city, and I'm looking around, and this idea of like top-down urban
[00:51:50.040 --> 00:51:55.160]   planning where you have like someone trying to design this perfect city that like doesn't
[00:51:55.160 --> 00:52:00.040]   change and doesn't yield to its people just seems completely unrealistic. And the style
[00:52:00.040 --> 00:52:04.840]   that both of them take in their writing is very, it just, it starts from them just like
[00:52:04.840 --> 00:52:09.840]   observing what they see and then like trying to write about it. And I, I just, yeah, that's,
[00:52:09.840 --> 00:52:12.160]   that's the style that I really want to emulate.
[00:52:12.160 --> 00:52:13.160]   Interesting.
[00:52:13.160 --> 00:52:17.600]   Yeah. I think for people to just be talking to like, I don't know, like Chris, just like
[00:52:17.600 --> 00:52:20.640]   just talking to like open source developers, it turns out you can learn a lot more from
[00:52:20.640 --> 00:52:24.120]   that than just sitting around, like thinking about what open source developers might be
[00:52:24.120 --> 00:52:26.080]   thinking about, but.
[00:52:26.080 --> 00:52:30.640]   I have this, I have had this idea of not even for like writing it out loud, but just to
[00:52:30.640 --> 00:52:36.840]   understand how the world works, just like shadowing people who are in just like a random
[00:52:36.840 --> 00:52:40.600]   position. They don't have to be a lead in any way, but just like a person who's the
[00:52:40.600 --> 00:52:44.960]   personal assistant to somebody influential, how to decide whose emails they forward, how
[00:52:44.960 --> 00:52:49.680]   they decide what's the priority or somebody who's just like an accountant for a big company,
[00:52:49.680 --> 00:52:55.000]   right? It just like, what is involved there? Like what kinds of, you know what I mean?
[00:52:55.000 --> 00:52:59.560]   Just like random people, the line manager at the local factory. I just like have no
[00:52:59.560 --> 00:53:03.280]   idea how these parts of the world work. And I just want to like, yeah, just shadow them
[00:53:03.280 --> 00:53:05.680]   for a day and see like what happens there.
[00:53:05.680 --> 00:53:11.280]   This is really interesting because everyone else focuses on sort of like, you know, the
[00:53:11.280 --> 00:53:15.640]   big name figure or whatever, but you know, who's the actual gatekeeper there. But yeah,
[00:53:15.640 --> 00:53:19.400]   I mean, I've definitely found like if you just start cold emailing people and talking
[00:53:19.400 --> 00:53:24.840]   to them, people are often like surprisingly very, very open to being talked to because
[00:53:24.840 --> 00:53:28.760]   I don't know, like most people do not get asked questions about what they do and how
[00:53:28.760 --> 00:53:34.440]   they think and stuff. So, you know, if you want to realize that dream.
[00:53:34.440 --> 00:53:38.480]   So maybe I'm not like John Rockefeller in that I only want my organization to last for
[00:53:38.480 --> 00:53:42.080]   50 years. I'm sure you've come across these people who have this idea that, you know,
[00:53:42.080 --> 00:53:45.320]   I'll let my money compound for like 200 years. And if it just compounds at some reasonable
[00:53:45.320 --> 00:53:48.520]   rate, I'll be, it'll be like the most wealthy institution in the world, unless somebody
[00:53:48.520 --> 00:53:54.320]   else has the same exact idea. If somebody wanted to do that, but they wanted to hedge
[00:53:54.320 --> 00:53:58.720]   for the possibility that there's a war or there's a revolt or there's some sort of
[00:53:58.720 --> 00:54:05.320]   change in law that draws down this wealth. How would you set up a thousand year endowment
[00:54:05.320 --> 00:54:09.400]   basically is what I'm asking, or like a 500 year endowment. Would you just put it in like
[00:54:09.400 --> 00:54:13.680]   a crypto wallet with us and just, you know what I mean? Like how, how would you go about
[00:54:13.680 --> 00:54:16.920]   that organizationally? How would you, like, that's your goal. I want to have the most
[00:54:16.920 --> 00:54:20.480]   influence in a hundred years. Well, I'd worry much less. The question for
[00:54:20.480 --> 00:54:25.240]   me is not about how do I make sure that there are assets available to distribute in a thousand
[00:54:25.240 --> 00:54:29.520]   years, because I don't know, just put on the stock market or something, like you do some
[00:54:29.520 --> 00:54:35.640]   pretty boring things to just like, you know, ensure your assets grow over time. The more
[00:54:35.640 --> 00:54:40.520]   difficult question is how do you ensure that whoever is deciding how to distribute the
[00:54:40.520 --> 00:54:46.960]   funds distributes them in a way that you personally want them to be spent. So Ford foundation
[00:54:46.960 --> 00:54:52.280]   is a really interesting example of this where Henry Ford like created a Ford foundation
[00:54:52.280 --> 00:54:58.240]   like shortly before he died and just pledged a lot of Ford stock to create this foundation
[00:54:58.240 --> 00:55:04.000]   and was doing it basically for tax reasons, had no philanthropic. It's just like, this
[00:55:04.000 --> 00:55:07.600]   is what we're doing to like house this wealth over here. And then, you know, passed away,
[00:55:07.600 --> 00:55:12.880]   son passed away and grandson ended up being on the board. And, but the board ended up
[00:55:12.880 --> 00:55:17.200]   being basically like, you know, a bunch of people that Henry Ford certainly would not
[00:55:17.200 --> 00:55:22.960]   have ever wanted to be on his board. And, and so, you know, and you end up seeing like
[00:55:22.960 --> 00:55:26.680]   the Ford Foundation ended up becoming huge influential. I like, I have received money
[00:55:26.680 --> 00:55:33.280]   from them. So it's not at all an indictment of, of sort of like their, their views or
[00:55:33.280 --> 00:55:37.000]   anything like that. It's just much more of like, you know, you had the intent of the
[00:55:37.000 --> 00:55:41.560]   original donor and then you had like, who are all these people that like suddenly just
[00:55:41.560 --> 00:55:45.000]   ended up with a giant pool of capital and then like decided to spend it however they
[00:55:45.000 --> 00:55:49.080]   felt like spending it. And the grandson at the time sort of like famously resigned because
[00:55:49.080 --> 00:55:52.560]   he was like really frustrated and was just like, this is not at all what my family wanted.
[00:55:52.560 --> 00:55:56.480]   And like basically getting like kicked off the board. And so anyway, so that, that is
[00:55:56.480 --> 00:56:00.320]   the question that I would like figure out if I had a thousand year endowment is like,
[00:56:00.320 --> 00:56:06.440]   how do I make sure that whomever manages that endowment actually shares my views? One shares
[00:56:06.440 --> 00:56:11.320]   my views, but then also like, how do I even know what we need to care about in a thousand
[00:56:11.320 --> 00:56:15.000]   years? Because like, I don't even know what the problems are in a thousand years. And
[00:56:15.000 --> 00:56:18.080]   this is why, like, I think like very long-term thinking can be a little bit dangerous in
[00:56:18.080 --> 00:56:22.560]   this way because you're sort of like presuming that you know what even matters then. Whereas
[00:56:22.560 --> 00:56:26.120]   I think like figure out the most impactful things to do is just like so contextually
[00:56:26.120 --> 00:56:31.160]   dependent on like what is going on at the time. So I can't, I don't know. And there
[00:56:31.160 --> 00:56:34.480]   are also foundations where, you know, the donor like writes in the charter, like this
[00:56:34.480 --> 00:56:38.280]   money can only be spent on, you know, X cause or whatever, but then it just becomes really
[00:56:38.280 --> 00:56:42.240]   awkward over time. Cause it's like, I don't know, they're spending money on like lighthouse
[00:56:42.240 --> 00:56:46.120]   keepers or something like that. And it's like, you know, like this is just like not a thing
[00:56:46.120 --> 00:56:51.080]   that actually really like, you know, should be the main focus anymore. So yeah, I don't
[00:56:51.080 --> 00:56:56.240]   know. I think I would probably try to figure out a way to like select for like thoughtful,
[00:56:56.240 --> 00:57:02.040]   somehow select for like thoughtful people. But like how to determine, like, I wonder
[00:57:02.040 --> 00:57:05.840]   if there's like a committee that like short appointment terms and then like there's some
[00:57:05.840 --> 00:57:12.320]   committee that can like run a contest or something to determine like who gets to run this money
[00:57:12.320 --> 00:57:15.680]   or distribute this money every generation or something like that. I don't know. I'd
[00:57:15.680 --> 00:57:20.680]   have to call it something pretty crazy like that, but wow. But yeah, yeah, that that would
[00:57:20.680 --> 00:57:24.680]   be the biggest challenge I think. Yeah. I just started reading the foundation, the book
[00:57:24.680 --> 00:57:29.200]   about the Ford foundation. I haven't got that far. It's so fascinating. But you know, that
[00:57:29.200 --> 00:57:33.400]   raises an interesting question. There is the problem of value drift in charities, but it's
[00:57:33.400 --> 00:57:37.200]   a very particular kind of value drift, right? So there's famously conquest second law that
[00:57:37.200 --> 00:57:43.440]   any institution that is not constitutionally and explicitly right wing becomes left wing
[00:57:43.440 --> 00:57:48.880]   over time. And this seems especially true of NGOs and charitable organizations. What's
[00:57:48.880 --> 00:57:54.840]   the explanation? Why is conquest second law seem true in this arena? I have to ask Curtis
[00:57:54.840 --> 00:58:02.760]   that. I mean, I don't I don't know that I have. I think we can observe that that is
[00:58:02.760 --> 00:58:09.760]   maybe what is happening. I don't think I have an amazing answer to that. I think. I mean,
[00:58:09.760 --> 00:58:18.000]   my best guess if I had to come to an answer is I think that the values of. Like democracy
[00:58:18.000 --> 00:58:26.240]   and peace and freedom and whatever, like there's a set of sort of like pacifying social values
[00:58:26.240 --> 00:58:31.640]   that are very hard to disagree with. And so there's always this sort of like natural drift
[00:58:31.640 --> 00:58:39.400]   towards that. I do find that like I think the most thoughtful people I know are often
[00:58:39.400 --> 00:58:49.560]   like there is a strong like intellectual conservative movement. But I think people that love nuance
[00:58:49.560 --> 00:58:55.200]   where, you know, where there is no like there is no mindless playbook that you can use to
[00:58:55.200 --> 00:59:02.440]   just sort of like the answer is not always like direct democracy or peace or whatever.
[00:59:02.440 --> 00:59:06.600]   If that's not your like guiding star and you are actually interested in like a fair bit
[00:59:06.600 --> 00:59:10.960]   of nuance, like you're not going to really run institutions. And I say that as someone
[00:59:10.960 --> 00:59:17.480]   who is like I am much more on the like nuance side. But like I like I think the tradeoff
[00:59:17.480 --> 00:59:21.560]   of that is like it just doesn't necessarily have mainstream appeal always because you
[00:59:21.560 --> 00:59:25.480]   don't have these really simplified messages. So, yeah, if you think about sort of like
[00:59:25.480 --> 00:59:29.800]   institutions are need to have like simplified messages that they pass on to people and those
[00:59:29.800 --> 00:59:34.480]   simplified messages work much better when they're things that make people feel good
[00:59:34.480 --> 00:59:37.520]   about themselves. And you're always going to have that kind of more or less word word
[00:59:37.520 --> 00:59:44.280]   drift. Yeah, yeah, yeah. It raises a question of how you would set up. I mean, it's like
[00:59:44.280 --> 00:59:49.600]   the two monarchs problem of like, you know, like you need somebody who's like a good a
[00:59:49.600 --> 00:59:53.800]   good director, but then you also need him to be able to appoint somebody who's a good
[00:59:53.800 --> 01:00:04.520]   director. Yeah, yeah. Yeah, it's really interesting. Let's talk about like, I guess what you and
[01:00:04.520 --> 01:00:08.600]   I do or no before that, actually. All right. I'm just gonna make a note for my editor right
[01:00:08.600 --> 01:00:17.280]   here. But so the next question is, do you think this new funding for science and thinkers,
[01:00:17.280 --> 01:00:23.520]   is that going to lead to a resurgence of the gentleman's collar category or has the nature
[01:00:23.520 --> 01:00:28.160]   of science just become too different and science has just gotten much more specialized now
[01:00:28.160 --> 01:00:36.800]   that that's no longer possible? Yeah, I mean, I think within within the realm of science
[01:00:36.800 --> 01:00:44.520]   specifically the sort of gentleman scientist era, you know, the Charles Darwin type era,
[01:00:44.520 --> 01:00:49.920]   it feels a little bit bygone in the sense of, yeah, I don't know, it feels like there's
[01:00:49.920 --> 01:00:55.020]   a lot of low-hanging fruit then that maybe like science is just so much bigger. It is
[01:00:55.020 --> 01:00:57.880]   funded in a completely different way that is sort of unrecognizable from where it was
[01:00:57.880 --> 01:01:02.840]   before. I think when people talk about problems in science, they like to romanticize the past
[01:01:02.840 --> 01:01:07.800]   or that's probably true for any sort of institutional problem of just, you know, why can't we just
[01:01:07.800 --> 01:01:11.200]   have it the way that it was like 100 years ago or whatever? And, you know, there's usually
[01:01:11.200 --> 01:01:17.400]   good reasons why things don't run the way they did before. And I always try to think
[01:01:17.400 --> 01:01:21.640]   about how do we actually take the conditions that we're in right now and come up with something
[01:01:21.640 --> 01:01:30.360]   new? That being said, even if we don't have sort of a return to the literal gentleman
[01:01:30.360 --> 01:01:36.840]   scientist as default way of doing things in science, there's a ton of room to go from
[01:01:36.840 --> 01:01:40.800]   the current model of how science is funded and the sort of like extremely constrained
[01:01:40.800 --> 01:01:44.040]   environments that people have to work in to like giving people a little bit more academic
[01:01:44.040 --> 01:01:51.440]   freedom, a little bit more creative freedom to experiment. So, but I think like, yeah,
[01:01:51.440 --> 01:01:55.200]   science doesn't really have any easy answers. I spent a bunch of time trying to understand
[01:01:55.200 --> 01:02:02.840]   it this summer and yeah, it's I think because like government funding of science became
[01:02:02.840 --> 01:02:10.600]   a thing right around like the middle of the 20th century after sort of like World War
[01:02:10.600 --> 01:02:15.520]   II. And like the way that science ran before then where there was very little government
[01:02:15.520 --> 01:02:19.640]   funding and very little involvement to where now like the fact of the matter is that like
[01:02:19.640 --> 01:02:22.840]   a lot of it is government funded or most of it is government funded just means that it's
[01:02:22.840 --> 01:02:29.000]   a completely different kind of ball game. Yeah. But I guess then for public intellectuals,
[01:02:29.000 --> 01:02:34.640]   there's a change in, especially if you're making content that is tech adjacent or something,
[01:02:34.640 --> 01:02:40.360]   there's a change in funding from it's no longer, you know, Kevin Kelly's 10,000 true fans,
[01:02:40.360 --> 01:02:44.240]   but more like one tech billionaire who likes your work and will, you know, write you a
[01:02:44.240 --> 01:02:50.640]   check to investigate it for a year. What is the consequence of that kind of change? And
[01:02:50.640 --> 01:02:57.500]   you have much more concentrated sources of funding in terms of what areas one can focus
[01:02:57.500 --> 01:03:01.240]   on and one does focus on and like the ways in which they engage with their audience and
[01:03:01.240 --> 01:03:05.440]   publish their content. Yeah. What impact does that have?
[01:03:05.440 --> 01:03:09.440]   That I'm pretty excited about and like can only really speak within my relatively narrow
[01:03:09.440 --> 01:03:14.720]   sort of like tech and tech adjacent creator world. But I definitely have noticed as someone
[01:03:14.720 --> 01:03:21.440]   who's been sort of independently or weirdly funded in a lot of ways for a while now, like
[01:03:21.440 --> 01:03:27.200]   it feels like that was extremely uncommon when I started. And now I meet a lot of people
[01:03:27.200 --> 01:03:31.720]   that are like me. I don't know if that's just because I'm meeting more people like me or
[01:03:31.720 --> 01:03:36.240]   if that's really a shift, but I thought like, yeah, you know, five years ago even it was
[01:03:36.240 --> 01:03:41.240]   like hard to identify a lot of people with that kind of situation. And so, yeah, I think
[01:03:41.240 --> 01:03:46.320]   it's a really cool, like people talk about like, you know, how do we bring back the Medici's?
[01:03:46.320 --> 01:03:50.480]   How do you bring back this like model of patronage? Like it's already happening I think in a lot
[01:03:50.480 --> 01:03:54.800]   of ways. It's just that people don't talk about it. People don't, you know, unless you're
[01:03:54.800 --> 01:03:58.440]   being funded on Patreon or you have subside subscriptions or there's something some very
[01:03:58.440 --> 01:04:02.560]   legible way to point out like how you're making money. Like there are so many people that
[01:04:02.560 --> 01:04:08.320]   are just being quietly funded that just don't talk about it. I do actually think like the
[01:04:08.320 --> 01:04:12.440]   model of patronage is very alive and well right now. It's just not super obvious.
[01:04:12.440 --> 01:04:18.240]   Yeah, yeah. And how do you think about the value of like, I guess what you and I, obviously
[01:04:18.240 --> 01:04:22.720]   we do different things, but in terms of like doing podcasts or writing essays and how do
[01:04:22.720 --> 01:04:26.400]   you think about the value of that? Like should we just like be writing code and digging dishes
[01:04:26.400 --> 01:04:31.720]   and doing something else that is more legibly useful to society? Like what is the, you know
[01:04:31.720 --> 01:04:34.720]   what I mean? Like how do you think about what is the value of this?
[01:04:34.720 --> 01:04:40.480]   Yeah, you know, I'm like very like, I only know how to do a handful of things in this
[01:04:40.480 --> 01:04:46.320]   world and it's more like I feel like I should be doing the thing that I cannot help myself
[01:04:46.320 --> 01:04:53.480]   but I have to do all the time. Like I don't really think, I don't have a very like rosy
[01:04:53.480 --> 01:04:57.320]   relationship with writing, to be perfectly honest. I hate writing. Writing makes me crazy.
[01:04:57.320 --> 01:05:02.240]   Like it's like I don't find it to be enjoyable. It's always enjoyable once it's over, but
[01:05:02.240 --> 01:05:08.000]   like the acroplastic is a little miserable. But like, you know, you would think like why
[01:05:08.000 --> 01:05:10.960]   do you do this thing that makes you miserable? But like it's just like it's the thing I know
[01:05:10.960 --> 01:05:14.080]   how to do and I don't think there's anything glamorous about it. I don't think there's
[01:05:14.080 --> 01:05:17.080]   anything special. It might not be the best thing to do. Like there's probably more impactful
[01:05:17.080 --> 01:05:20.600]   things I could be doing with my time. But like it's the thing I like have to do. And
[01:05:20.600 --> 01:05:24.120]   I think everyone should just be doing the thing that they like absolutely have to do,
[01:05:24.120 --> 01:05:27.680]   whatever that is. That would make me happy in the world is if everyone was just like,
[01:05:27.680 --> 01:05:33.880]   yeah, leaning into their obsession. So that's my obsession. I do think like when I think
[01:05:33.880 --> 01:05:39.480]   about my own impact, I don't know how you think about it, but like I think about I want
[01:05:39.480 --> 01:05:48.480]   to, I want my ideas to be heard by people that I think can do something about them.
[01:05:48.480 --> 01:05:53.680]   So in other words, like I care much more about like quality than quantity. I don't, I'm not
[01:05:53.680 --> 01:05:59.320]   very active on Twitter. I don't really focus on like needing to reach some kind of like
[01:05:59.320 --> 01:06:05.400]   mass mainstream audience. When I published my book, like I told myself, I really like
[01:06:05.400 --> 01:06:09.040]   the people that need to hear about how open source works are people that work at tech
[01:06:09.040 --> 01:06:12.520]   companies, software developers that use open source software. Like it mattered less to
[01:06:12.520 --> 01:06:15.480]   me that there's like, this is not the kind of book that needs to be in like an airport
[01:06:15.480 --> 01:06:20.000]   bookstore would pick. It's almost like essays and stuff. Like I think it's much more important
[01:06:20.000 --> 01:06:25.360]   to me that people whose opinions I care about read it. And, and hopefully, you know, and
[01:06:25.360 --> 01:06:30.120]   I make my essays public because I hope everyone reads them. But like what I think about sort
[01:06:30.120 --> 01:06:34.960]   of how do I measure my impact? It's not like how many page views that I get on an essay.
[01:06:34.960 --> 01:06:39.440]   It's more of like who ended up talking about it. And are those the people that I wanted
[01:06:39.440 --> 01:06:46.120]   to talk about it? Yeah. Yeah. Yeah. I think like people really undervalue, like this,
[01:06:46.120 --> 01:06:50.080]   like my, my, my, my sort of like personal pet peeve is like founders will always talk
[01:06:50.080 --> 01:06:53.520]   about like building and like startups is like so important or whatever. And like, what are
[01:06:53.520 --> 01:06:56.120]   all of them doing in their spare time? They're like reading books, they're reading essays
[01:06:56.120 --> 01:06:59.420]   and like, and then there's like books and essays influence how they think about stuff.
[01:06:59.420 --> 01:07:05.440]   And so it is very like indirect sort of influence or in, in, yeah, like, but like you can't
[01:07:05.440 --> 01:07:08.920]   like, I just feel like, you know, you can't sort of have out of one mouth saying like
[01:07:08.920 --> 01:07:13.320]   the only important thing in the world is like starting startups. And then at the same time,
[01:07:13.320 --> 01:07:16.040]   talk about like the cool new book you read at a cocktail party. Like both those things
[01:07:16.040 --> 01:07:20.640]   are important in different ways. Right? Yeah, no, I totally agree. And I don't want to repeat
[01:07:20.640 --> 01:07:24.320]   myself because I, I talked about this on my burn episode, but one of the things we were
[01:07:24.320 --> 01:07:30.480]   talking about, you know, Carol's books, Robert Carol's books. And one interesting thing is,
[01:07:30.480 --> 01:07:36.400]   you know, this guy was nominated for a Pulitzer prize before he wrote the power broker. So
[01:07:36.400 --> 01:07:43.720]   he was like a top tier investigative journalist. And can you imagine you crunching the numbers
[01:07:43.720 --> 01:07:47.660]   as a top tier investigative journalist at the peak of your career? And you're like,
[01:07:47.660 --> 01:07:51.000]   you know, what would be a good use of my time, I'm going to spend the next seven years in
[01:07:51.000 --> 01:07:56.800]   almost poverty writing about this one guy who had a lot of influence in New York. And
[01:07:56.800 --> 01:08:01.640]   I'm going to talk to any person who had conceivably even been in the same room as him or had been
[01:08:01.640 --> 01:08:05.280]   indirectly affected by his policies in any way. I'm going to do that obsessively for
[01:08:05.280 --> 01:08:08.600]   the next seven years. Yeah, there's no way the number crunching would get you there.
[01:08:08.600 --> 01:08:13.200]   But it's probably been one of the most influential books in terms of how urban governance is
[01:08:13.200 --> 01:08:16.880]   done. I mean, like presidents have praised and read the book and said it like changed
[01:08:16.880 --> 01:08:20.840]   how they think about politics. So you know, like, it is the kind of thing where you wouldn't
[01:08:20.840 --> 01:08:25.600]   have gone to that conclusion just from Yeah, yeah, thinking about it beforehand. And like,
[01:08:25.600 --> 01:08:32.340]   this is the most effective thing I could do. Yeah, totally. Yeah. But okay, you had this
[01:08:32.340 --> 01:08:37.520]   recent post about, you know, climate tribes. I thought was really interesting, especially
[01:08:37.520 --> 01:08:43.400]   the addendum. And by the way, I have noticed this, this tendency of writers to hide the
[01:08:43.400 --> 01:08:47.680]   most interesting thoughts, it footnotes and addendums. And I'm curious why that is what
[01:08:47.680 --> 01:08:50.960]   I think it might be because your most interesting thoughts are digressions that you feel like
[01:08:50.960 --> 01:08:58.820]   you have to take out of the main text. But anyways, what I thought was interesting. You're
[01:08:58.820 --> 01:09:06.160]   comparing climate dumerism to other kinds of dumerism that are yet to become fully mature.
[01:09:06.160 --> 01:09:13.120]   And I am wondering what is your predictions about the different tribes that will emerge
[01:09:13.120 --> 01:09:19.800]   when thinking about AI as both capabilities grow and as public awareness of those capabilities
[01:09:19.800 --> 01:09:23.720]   grows? Oh, gosh, I think it's definitely just too
[01:09:23.720 --> 01:09:27.520]   early to say. And I know that sounds like a cop out, but I don't want to say things
[01:09:27.520 --> 01:09:32.560]   that I don't feel confident about. I think it's too early to say even within like AI
[01:09:32.560 --> 01:09:36.400]   though, like if you think about say I had these sort of like, different tribes that
[01:09:36.400 --> 01:09:42.240]   are influencing the climate discourse today. There's, there's some parallel version of
[01:09:42.240 --> 01:09:50.040]   that for AI, for AI more broadly, I think where, because right now I feel like AI safety
[01:09:50.040 --> 01:09:54.560]   gets really constrained to sort of like, I don't know, like Miri or something like very,
[01:09:54.560 --> 01:10:01.400]   very specific. I imagine like as AI becomes more widespread and more like more people
[01:10:01.400 --> 01:10:05.800]   have experiences with it and have opinions on it, then that might sort of like lead to
[01:10:05.800 --> 01:10:11.760]   other, you know, philosophies kind of forming around that. And then we'll kind of see this
[01:10:11.760 --> 01:10:17.760]   one very narrow view of like, I think the sort of like Miri mindset is equivalent to
[01:10:17.760 --> 01:10:21.720]   like the Doomer tribe that I identified in climate where it's like, that is one specific
[01:10:21.720 --> 01:10:24.160]   tribe, but there are a lot of other people that are really interested in climate that
[01:10:24.160 --> 01:10:30.080]   like don't feel Doomer-y at all. Even though that's sort of like the most flashy, like
[01:10:30.080 --> 01:10:36.560]   media friendly kind of version of it. So yeah, I mean, other than saying like, as more people
[01:10:36.560 --> 01:10:42.200]   interact with AI, I imagine there will be more philosophies emerging there. I think
[01:10:42.200 --> 01:10:46.080]   it's still too early to say what that will be. AI is still kind of like a big mystery
[01:10:46.080 --> 01:10:52.560]   box to me right now. So it's there, but I don't really know what's inside.
[01:10:52.560 --> 01:10:58.160]   Having studied these different sorts of Doomerisms, whether they're right or not, by the way,
[01:10:58.160 --> 01:11:02.920]   it's like a separate question, but just in terms of the sociology of them, has it always
[01:11:02.920 --> 01:11:08.500]   been true that smart, talented people tend to get a lot of meaning by working on things
[01:11:08.500 --> 01:11:14.360]   that are seen as existential catastrophes? Or is that a property of, you know, tech adjacent
[01:11:14.360 --> 01:11:21.200]   areas or modern tech adjacent areas? Like how unique is this sort of sociological phenomenon?
[01:11:21.200 --> 01:11:26.240]   Yeah, I think it is pretty new. And that's why it's kind of gnawing at my brain a little
[01:11:26.240 --> 01:11:34.920]   bit. I think it's really new, like last five years ish. Yeah, well, and so I tried to sort
[01:11:34.920 --> 01:11:40.200]   of track this a little bit, and I'm not super high confidence in all this, but there's this
[01:11:40.200 --> 01:11:46.120]   one sort of theory around, okay, we used to have sort of like shared, broader narratives
[01:11:46.120 --> 01:11:50.140]   that were actually Doomer-esque. So we had world wars, we had the Cold War, whatever.
[01:11:50.140 --> 01:11:54.920]   And so like, super smart, talented people that need to be pointed in a direction somewhere,
[01:11:54.920 --> 01:11:58.380]   they're going to go work on those kinds of problems. And like, there's a shared understanding
[01:11:58.380 --> 01:12:03.120]   that like, we really are like, you know, saving our country or protecting our country or whatever,
[01:12:03.120 --> 01:12:07.840]   by working on these different things. And so yeah, I don't know, like stem talent in
[01:12:07.840 --> 01:12:11.760]   the Cold War or whatever. And then you see, okay, after the Cold War, now we're not so
[01:12:11.760 --> 01:12:15.080]   like, you know, we don't have these like deep existential threats anymore. So we have to
[01:12:15.080 --> 01:12:19.180]   find them somewhere else. And that's where like environmentalism kind of became much
[01:12:19.180 --> 01:12:24.000]   more like alarmist, whereas in the past, it was kind of like this niche social cause,
[01:12:24.000 --> 01:12:29.480]   it became much more like we need to save the planet. And coming out of sort of like World
[01:12:29.480 --> 01:12:36.760]   War Two, and yeah, just sort of like manufacturing chemicals, like, whatever, suddenly, like
[01:12:36.760 --> 01:12:41.360]   people are just grappling with the aftereffects of that. But like, that doesn't explain sort
[01:12:41.360 --> 01:12:47.160]   of like, in the last five years or so where it's like, it's not like a weird activisty
[01:12:47.160 --> 01:12:51.160]   thing to work in climate, it can even be like a very boring thing for people to work in
[01:12:51.160 --> 01:12:55.920]   climate. But it's like all connected to this idea of like, this is the most important thing
[01:12:55.920 --> 01:12:59.940]   I need to be working on. And so I think like, maybe in the absence of having some like bigger
[01:12:59.940 --> 01:13:03.880]   narrative that is like, all consuming for everyone, you kind of have to make your own
[01:13:03.880 --> 01:13:08.520]   meaning somewhere. But it is funny to me that like, we don't just say, like, again, I mean,
[01:13:08.520 --> 01:13:12.400]   you go back to, you know, we talked about like writing, it's like, I have no defense
[01:13:12.400 --> 01:13:16.520]   as to like, why I write all day. Like, it's just like what I have to do. Like, I cannot
[01:13:16.520 --> 01:13:22.520]   defend it as like, the most like, you know, needle moving thing in the world or whatever.
[01:13:22.520 --> 01:13:26.520]   I don't really relate to this sort of need to have like, a like, doomer narrative. There
[01:13:26.520 --> 01:13:29.960]   is no doomer narrative attached to why I write, I just write because like, I think it's important.
[01:13:29.960 --> 01:13:36.640]   And I think I have like, ideas that are like, questions I want to answer. And to me, that
[01:13:36.640 --> 01:13:40.040]   is how I define impact, though, like to me, like that matches my model of like, what I
[01:13:40.040 --> 01:13:43.080]   think is the most impactful thing in the world. If I didn't have that model, then yeah, maybe
[01:13:43.080 --> 01:13:46.240]   I would try to say, okay, like, what is the most impactful thing to be doing in the world
[01:13:46.240 --> 01:13:50.200]   that is like, sort of external to my own personal curiosity or whatever. And I think that's
[01:13:50.200 --> 01:13:54.640]   where those sort of like, doomer narratives come from. I am, I did bury in another footnote
[01:13:54.640 --> 01:14:00.120]   at the end of that post, this question about like, okay, like, if we think maybe like,
[01:14:00.120 --> 01:14:06.200]   early 2010s or something, I feel like there was, there's always like, there's this other
[01:14:06.200 --> 01:14:09.440]   grouping of industries that's not doomer-esque, but also attracts smart, talented people.
[01:14:09.440 --> 01:14:14.400]   So you have like, advertising and trading, and playing video games. I don't really know
[01:14:14.400 --> 01:14:18.200]   if that's like an industry, but like, there's that mindset of like, those people end up
[01:14:18.200 --> 01:14:25.280]   like, like, that's some shared set of skills across all those different industries or practices
[01:14:25.280 --> 01:14:27.960]   that attracts like, smart, talented people. Like, why do so many smart, talented people
[01:14:27.960 --> 01:14:33.560]   just go into like, trading? And I wonder if there's some other sort of similar gravity
[01:14:33.560 --> 01:14:38.560]   well effect there that is also attracting smart, talented people into like, climate,
[01:14:38.560 --> 01:14:43.000]   maybe from like, a different crowd or whatever. But so I wonder maybe if like, before the
[01:14:43.000 --> 01:14:49.120]   last five years or whatever, like, maybe there was, maybe that was where everyone was dumping
[01:14:49.120 --> 01:14:52.260]   into. I don't really know. Yeah.
[01:14:52.260 --> 01:14:57.960]   Do you have some general theory of what these gravity wells for talent are? Like, what connects
[01:14:57.960 --> 01:15:00.000]   to trading to climate?
[01:15:00.000 --> 01:15:04.960]   I don't know. And maybe they're different crowds. Like I, yeah, and that's why I just
[01:15:04.960 --> 01:15:07.800]   like, stuck it in a footnote because I was lazy. And I was like, I don't know what to
[01:15:07.800 --> 01:15:18.440]   call it. I need to put it somewhere. But it just seems like it like, yeah, I don't. There's
[01:15:18.440 --> 01:15:22.480]   just something about these, like, all these sorts of industries where it's like, if they
[01:15:22.480 --> 01:15:25.880]   were starting with a blank slate, they could be doing like, anything. And for some reason,
[01:15:25.880 --> 01:15:31.200]   they just all end up in these sort of like, non obvious places. Like, why is, like, why
[01:15:31.200 --> 01:15:35.440]   are there so many people that work that end up in trading? Like, it's just like, so specific
[01:15:35.440 --> 01:15:40.800]   when you really think about it. And then like, same with like, climate where, I mean, depending
[01:15:40.800 --> 01:15:44.960]   on how literally you take sort of like, climate, do more predictions or whatever. But if you
[01:15:44.960 --> 01:15:48.520]   don't think the world is going to end in 30 years, like, then why is everyone so focused
[01:15:48.520 --> 01:15:52.120]   on this one specific thing when they could be working on like, lots of different things?
[01:15:52.120 --> 01:15:55.280]   And so, yeah, in both cases, it feels like they kind of like, flop in there somehow.
[01:15:55.280 --> 01:15:58.920]   I did put in, yeah, in that addendum and just sort of thinking about like, what is the shape
[01:15:58.920 --> 01:16:03.360]   of a doomer industry? Like, I think one of the under discussed aspects of it is that
[01:16:03.360 --> 01:16:07.080]   it is like, adjacent to some kind of like, commercial opportunity. So, like, the reason
[01:16:07.080 --> 01:16:10.240]   why everyone doesn't just go off and work on like, global poverty is because there's
[01:16:10.240 --> 01:16:14.640]   like, no money to be made in working on that. But like, if you think about like, misinformation
[01:16:14.640 --> 01:16:19.120]   and the threat of like, deep fakes or something upending democracy, or you think about AI
[01:16:19.120 --> 01:16:21.800]   safety or climate or whatever, like, they are adjacent to commercial industries where
[01:16:21.800 --> 01:16:26.640]   you can actually like, make a real salary and feel like, relevant to the business world
[01:16:26.640 --> 01:16:32.120]   or whatever, or to like, all your peers, while still also working on this social cause area.
[01:16:32.120 --> 01:16:37.520]   So, yeah, I don't know if that helps at least somewhat. And that's probably the simplest,
[01:16:37.520 --> 01:16:42.800]   you know, like, sort of like, non-overthinking answer for like, why it is like, advertising
[01:16:42.800 --> 01:16:45.840]   and trading attracts so many people is just because you can make a lot of money in it.
[01:16:45.840 --> 01:16:46.840]   And that's that simple.
[01:16:46.840 --> 01:16:51.560]   Oh, I have one theory about trading and video games that connects them.
[01:16:51.560 --> 01:16:52.560]   Yeah.
[01:16:52.560 --> 01:16:57.320]   Byrne Hobart, funnily enough, another footnote. Byrne Hobart, he was actually a like, a blog
[01:16:57.320 --> 01:17:01.400]   post about SaaS products or something. And one of the footnotes was, you know, one of
[01:17:01.400 --> 01:17:09.600]   the positive things about finance might be that it gives a sort of venting for talented
[01:17:09.600 --> 01:17:16.320]   people who just like to play zero-sum or negative-sum games. And that otherwise, it would have been
[01:17:16.320 --> 01:17:20.000]   used up in a war, but since there's no war, they would be doing something else destructive
[01:17:20.000 --> 01:17:23.520]   in our world. And if we can just put them in front of a trading screen and make them,
[01:17:23.520 --> 01:17:30.040]   you know, get the microsecond efficiency of some like, equity market better, it's like,
[01:17:30.040 --> 01:17:32.640]   better than anything else I could be doing with that mentality.
[01:17:32.640 --> 01:17:40.720]   I kind of like that. There's some parallel with this for like, content creators, too.
[01:17:40.720 --> 01:17:46.120]   And I will cringingly put myself in that category. But it's weird to just like, where it's also
[01:17:46.120 --> 01:17:49.760]   this grouping of people where it's like, I don't really know like, what, like, they are
[01:17:49.760 --> 01:17:54.280]   kind of just like bodies in a room, again, myself included. That's what I'd be doing
[01:17:54.280 --> 01:17:57.520]   if not this, at least there's a way to like, kind of make money in this. But it's like
[01:17:57.520 --> 01:18:04.640]   a much more amorphous and yeah, non-coherent industry than like, trading. But it's a different
[01:18:04.640 --> 01:18:07.040]   set of people.
[01:18:07.040 --> 01:18:12.420]   One interesting area that is not included, one, I guess, philosophy that is not in any
[01:18:12.420 --> 01:18:17.680]   of these influential IGM machines in Silicon Valley is religion. I mean, you know, they've
[01:18:17.680 --> 01:18:24.320]   been like some of the most important ideas in history. And yet somehow they're, so far,
[01:18:24.320 --> 01:18:28.560]   they've had like very little influence in terms of what kinds of things the new elite
[01:18:28.560 --> 01:18:32.680]   is funding and paying attention to. Do you think that will change or have we just had
[01:18:32.680 --> 01:18:36.800]   a complete change in terms of what kinds of ideas get promoted?
[01:18:36.800 --> 01:18:46.280]   Yeah, TBD. I think like the new right is bringing at least some of those underlying like Christian
[01:18:46.280 --> 01:18:52.480]   religious values back. Maybe they're not like literally funding churches or something. But
[01:18:52.480 --> 01:18:57.280]   like how do we sort of like, yeah, bring Christian values back to public society? I think there's
[01:18:57.280 --> 01:19:03.040]   a lot, there is a lot going on there, even if it's not explicitly called religion. And
[01:19:03.040 --> 01:19:08.320]   then, so yeah, I mean, that's one question of like, among elites that are, you know,
[01:19:08.320 --> 01:19:13.880]   explicitly religious, how are they sort of like encoding those values into public institutions?
[01:19:13.880 --> 01:19:19.080]   I think that is sort of happening on that front. I also just think like, if we take
[01:19:19.080 --> 01:19:24.320]   a broader view of like, how does religion factor into our day-to-day life? I feel like
[01:19:24.320 --> 01:19:29.640]   it's a weirdly, like people ask this question more than they need to ask it or something,
[01:19:29.640 --> 01:19:33.280]   like where like, you know, everyone just sort of says, oh, like, you know, people aren't
[01:19:33.280 --> 01:19:36.320]   religious anymore. They're not going to church. They need to find meaning. Like, how do we
[01:19:36.320 --> 01:19:41.760]   like create new religions today? And I just, I feel like people are religious. They're
[01:19:41.760 --> 01:19:45.360]   just religious about different things. And that was sort of one of my conclusions with
[01:19:45.360 --> 01:19:53.440]   climate is like, in the early 2000s, you had Michael Crichton criticizing environmentalism
[01:19:53.440 --> 01:19:59.920]   as a religion and saying that, you know, it's distracting people from the science and maybe
[01:19:59.920 --> 01:20:04.760]   we shouldn't treat environmentalism as religion. We should really get back to the science.
[01:20:04.760 --> 01:20:08.640]   Whereas I think like, you know, 20 years later, it's like, I think it just is a religion for
[01:20:08.640 --> 01:20:12.840]   a lot of people. And why not just lean into that and say, like, I don't, like that is
[01:20:12.840 --> 01:20:18.240]   how people are finding meaning. That is how people are finding community. So I don't know.
[01:20:18.240 --> 01:20:23.920]   Like, there it is. There's a religion. Like we may not literally have like actively practicing
[01:20:23.920 --> 01:20:28.040]   Christians in America or something. And, you know, there's the question of what else does
[01:20:28.040 --> 01:20:31.680]   religion need to fulfill that it's not through something like climate. But I also think that
[01:20:31.680 --> 01:20:37.840]   religious practice is very like active and around us everywhere. And like, I don't think
[01:20:37.840 --> 01:20:41.480]   that's like sad or bad. I think that's just like how it's evolved.
[01:20:41.480 --> 01:20:46.920]   Yeah. Yeah. Interesting. All right. Final question. You have a great blog post about
[01:20:46.920 --> 01:20:52.720]   shamelessness as a strategy. What are you most shameless about in public? What is your
[01:20:52.720 --> 01:21:01.160]   most shameful strategy? I don't know that I. Oh, man. I wish I had a really good juicy
[01:21:01.160 --> 01:21:07.720]   answer. I mean, you have to ask someone else that knows me. What am I most shameless about
[01:21:07.720 --> 01:21:12.840]   in public? It's one property of being shameless. It's like you don't even realize it's shameful.
[01:21:12.840 --> 01:21:18.640]   You're so shameless. You know, it's like you don't even keep track of it that way. Just
[01:21:18.640 --> 01:21:28.800]   the fact. Yeah. Yeah. I'm sure I have something. I think I'm a very shameful person. I don't
[01:21:28.800 --> 01:21:36.600]   know that I have. Yeah, I'm not. I'm not the best at being shameless. Yeah. You're gonna
[01:21:36.600 --> 01:21:41.160]   have to ask. You're gonna have to ask a friend to tell you what I'm most shameless about.
[01:21:41.160 --> 01:21:48.080]   It sounds good. Sounds good. OK, Nadia, thank you so much for being on the podcast. So tell
[01:21:48.080 --> 01:21:53.880]   people where they can find your your blog, your Twitter, anywhere else that they should
[01:21:53.880 --> 01:22:02.760]   look for you. Blog is at Nadia, N-A-D-I-A dot X, Y-Z. And I'm on Twitter at N-I-A-F-I-A
[01:22:02.760 --> 01:22:08.000]   N-A-Y-A-F-I-A. Awesome. OK. Awesome, Nadia. This is a lot of fun. Thanks for coming on.
[01:22:08.000 --> 01:22:09.080]   Yeah. Thanks, Chen.
[01:22:09.080 --> 01:22:19.080]   [BLANK_AUDIO]

