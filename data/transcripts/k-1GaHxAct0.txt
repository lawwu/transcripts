
[00:00:00.000 --> 00:00:01.600]   - Cover today.
[00:00:01.600 --> 00:00:04.560]   Just quick overview of what to expect from today.
[00:00:04.560 --> 00:00:07.480]   First, I'm gonna do a very, very quick recap
[00:00:07.480 --> 00:00:11.400]   of the lecture that you all watched from Sergey
[00:00:11.400 --> 00:00:13.480]   on testing and deployment.
[00:00:13.480 --> 00:00:15.200]   And it's gonna go by very, very fast,
[00:00:15.200 --> 00:00:17.120]   so if you did not do your homework,
[00:00:17.120 --> 00:00:21.960]   then I'm not sure that this will bring you back up to speed,
[00:00:21.960 --> 00:00:23.600]   but that's the intention.
[00:00:23.600 --> 00:00:26.780]   And then I wanna just check in briefly on projects
[00:00:26.780 --> 00:00:28.480]   and just kind of see how everyone is feeling
[00:00:28.480 --> 00:00:29.560]   about where they're at.
[00:00:29.560 --> 00:00:31.440]   We have a week left in the class,
[00:00:31.440 --> 00:00:34.480]   and so I wanna get a sense of how close to being done
[00:00:34.480 --> 00:00:35.560]   everyone feels.
[00:00:35.560 --> 00:00:37.880]   And then we have two amazing guest speakers
[00:00:37.880 --> 00:00:40.680]   to kind of conclude this part of the class.
[00:00:40.680 --> 00:00:42.980]   All right, any questions before I get started?
[00:00:42.980 --> 00:00:47.140]   Great.
[00:00:47.140 --> 00:00:51.520]   So just to briefly review the lecture
[00:00:51.520 --> 00:00:52.960]   that you watched from Sergey,
[00:00:52.960 --> 00:00:55.960]   he started by covering some concepts
[00:00:55.960 --> 00:00:57.780]   of testing and deployment.
[00:00:57.780 --> 00:00:59.800]   And so these concepts were kind of
[00:00:59.800 --> 00:01:02.360]   how to think about the entire structure
[00:01:02.360 --> 00:01:03.760]   of your machine learning projects
[00:01:03.760 --> 00:01:05.820]   and how different tests fit into that.
[00:01:05.820 --> 00:01:08.740]   And then he covered this idea of the ML test score,
[00:01:08.740 --> 00:01:10.740]   which is a rubric from Google of how to think about
[00:01:10.740 --> 00:01:13.900]   how production-ready your machine learning code base is.
[00:01:13.900 --> 00:01:15.880]   And then he talked about some of the infrastructure
[00:01:15.880 --> 00:01:18.060]   and tooling around this sort of part
[00:01:18.060 --> 00:01:20.200]   of machine learning projects.
[00:01:20.200 --> 00:01:24.000]   So we talked about continuous integration and testing,
[00:01:24.000 --> 00:01:26.360]   covered Docker in some depth,
[00:01:26.360 --> 00:01:28.340]   some ideas for deploying to the web,
[00:01:28.340 --> 00:01:32.160]   monitoring prediction systems once they've been deployed,
[00:01:32.160 --> 00:01:34.260]   and then a little bit on kind of how to think
[00:01:34.260 --> 00:01:35.980]   about deploying not to the web,
[00:01:35.980 --> 00:01:37.480]   but to hardware or to mobile.
[00:01:37.480 --> 00:01:41.880]   And so I'll talk through a couple of the key slides
[00:01:41.880 --> 00:01:42.720]   from the talk.
[00:01:42.720 --> 00:01:47.500]   The first that I really like is this kind of overview
[00:01:47.500 --> 00:01:49.140]   of machine learning systems.
[00:01:49.140 --> 00:01:50.960]   So you have your training system,
[00:01:50.960 --> 00:01:53.560]   and your training system is combined
[00:01:53.560 --> 00:01:55.220]   with your training and validation data
[00:01:55.220 --> 00:01:56.660]   to create a prediction system,
[00:01:56.660 --> 00:01:59.580]   which is then served into production.
[00:01:59.580 --> 00:02:02.600]   And the key concept here is the different types of tests
[00:02:02.600 --> 00:02:04.380]   that you might have for the different stages
[00:02:04.380 --> 00:02:07.260]   of this code base.
[00:02:07.260 --> 00:02:10.100]   So you have tests on your training system,
[00:02:10.100 --> 00:02:11.640]   and this is things like,
[00:02:11.640 --> 00:02:13.340]   if you push some update to your code,
[00:02:13.340 --> 00:02:16.460]   is it breaking your ability to achieve a certain score
[00:02:16.460 --> 00:02:17.300]   on your training set?
[00:02:17.300 --> 00:02:19.380]   And these are kind of longer tests that take,
[00:02:19.380 --> 00:02:21.260]   maybe up to a day to run.
[00:02:21.260 --> 00:02:24.380]   Then on your prediction system, you have validation sets,
[00:02:24.380 --> 00:02:28.380]   and these are testing regressions to your model itself.
[00:02:28.380 --> 00:02:30.440]   So if you push an update to your model,
[00:02:30.440 --> 00:02:31.820]   you wanna make sure that it's still performing
[00:02:31.820 --> 00:02:33.340]   as well as it did before.
[00:02:33.340 --> 00:02:35.840]   And then functionality tests, which are quicker tests
[00:02:35.840 --> 00:02:38.140]   that can catch kind of making sure that you perform well
[00:02:38.140 --> 00:02:40.240]   on really important examples or edge cases.
[00:02:40.240 --> 00:02:43.360]   And then finally, once you've deployed the system
[00:02:43.360 --> 00:02:44.800]   into production, you wanna monitor it.
[00:02:44.800 --> 00:02:47.380]   So you wanna make sure that it doesn't go down,
[00:02:47.380 --> 00:02:49.540]   you don't have data shifts,
[00:02:49.540 --> 00:02:50.880]   and you don't have more errors
[00:02:50.880 --> 00:02:52.340]   than you're expecting to have.
[00:02:53.020 --> 00:02:57.940]   This is a slide that covers some of what was talked about
[00:02:57.940 --> 00:03:00.100]   in the ML readiness score.
[00:03:00.100 --> 00:03:02.140]   And so it just talks about some of the different types
[00:03:02.140 --> 00:03:03.940]   of tests that you might wanna have for your data set,
[00:03:03.940 --> 00:03:05.580]   your model, your infrastructure,
[00:03:05.580 --> 00:03:07.420]   and then monitoring and production.
[00:03:07.420 --> 00:03:09.740]   I won't go through all of these now,
[00:03:09.740 --> 00:03:11.500]   but these are just some things to think about
[00:03:11.500 --> 00:03:15.840]   as you're writing tests for your code base for the project.
[00:03:15.840 --> 00:03:20.440]   And then diving into testing and continuous integration.
[00:03:20.440 --> 00:03:21.540]   There's a few concepts here.
[00:03:21.540 --> 00:03:23.620]   There's unit and integration tests.
[00:03:23.620 --> 00:03:27.620]   And so this is testing individual parts of your code base
[00:03:27.620 --> 00:03:29.100]   to make sure that they continue to function
[00:03:29.100 --> 00:03:30.380]   when you change your code,
[00:03:30.380 --> 00:03:34.060]   and possibly testing your entire system.
[00:03:34.060 --> 00:03:36.540]   And then there's a concept of continuous integration.
[00:03:36.540 --> 00:03:38.580]   And all this means is that every time
[00:03:38.580 --> 00:03:41.380]   that you push new code to your repo, let's say,
[00:03:41.380 --> 00:03:45.060]   before you deploy a new model into production,
[00:03:45.060 --> 00:03:46.620]   or sometimes even in some organizations
[00:03:46.620 --> 00:03:48.580]   before you merge that code into master,
[00:03:48.580 --> 00:03:51.380]   you wanna run some tests to make sure
[00:03:51.380 --> 00:03:54.720]   that that code is not broken what you were able to do before.
[00:03:54.720 --> 00:03:58.540]   There are a bunch of software as a service tools
[00:03:58.540 --> 00:04:00.120]   for continuous integration.
[00:04:00.120 --> 00:04:01.040]   Most of these are not.
[00:04:01.040 --> 00:04:02.620]   In fact, I think all of these are not specific
[00:04:02.620 --> 00:04:05.060]   to machine learning, but these are just some of the tools
[00:04:05.060 --> 00:04:06.620]   that you might experiment with.
[00:04:06.620 --> 00:04:09.340]   And then another core idea that Sergey talked about
[00:04:09.340 --> 00:04:11.540]   was containerization.
[00:04:11.540 --> 00:04:13.860]   And so this is a way of managing the dependencies
[00:04:13.860 --> 00:04:16.500]   of your code when you run it in a continuous integration
[00:04:16.500 --> 00:04:17.600]   or deployment setting.
[00:04:19.820 --> 00:04:23.260]   So that's kind of what was covered in testing.
[00:04:23.260 --> 00:04:26.480]   For deployment, a few concepts here.
[00:04:26.480 --> 00:04:27.820]   The first is a REST API.
[00:04:27.820 --> 00:04:32.820]   And so this is just a general API for HTTP systems.
[00:04:32.820 --> 00:04:36.660]   And so one way to think about deploying
[00:04:36.660 --> 00:04:39.060]   machine learning system is just treating them
[00:04:39.060 --> 00:04:42.060]   as kind of a black box that's called by a web server.
[00:04:42.060 --> 00:04:45.420]   You have a bunch of different options
[00:04:45.420 --> 00:04:48.540]   for deploying machine learning code into production.
[00:04:48.540 --> 00:04:51.020]   You can put the code into a virtual machine
[00:04:51.020 --> 00:04:54.120]   like a Docker container, and then you can scale it up
[00:04:54.120 --> 00:04:57.700]   to more users by adding instances to your system.
[00:04:57.700 --> 00:05:01.140]   Or you can do it via orchestration.
[00:05:01.140 --> 00:05:03.300]   And then the last concept that Sergey talked about
[00:05:03.300 --> 00:05:05.100]   that he really likes and I think is really exciting
[00:05:05.100 --> 00:05:06.940]   is serverless functions, where you don't actually
[00:05:06.940 --> 00:05:09.960]   have to manage your own infrastructure at all.
[00:05:09.960 --> 00:05:15.300]   And the takeaways here were, if you're doing inference
[00:05:15.300 --> 00:05:20.300]   on a CPU, then you can get away by scaling up
[00:05:20.300 --> 00:05:23.940]   by launching more and more servers or by going serverless.
[00:05:23.940 --> 00:05:26.700]   And you don't really have to do anything too crazy here.
[00:05:26.700 --> 00:05:30.220]   Sergey's dream, which I think would be really cool,
[00:05:30.220 --> 00:05:33.540]   is just deploying Docker as easily as deploying Lambda.
[00:05:33.540 --> 00:05:37.620]   But the next best thing is either using Lambda
[00:05:37.620 --> 00:05:40.400]   and dealing with the fact that the form
[00:05:40.400 --> 00:05:42.740]   that you have to get your model into is much trickier,
[00:05:42.740 --> 00:05:45.380]   or using Docker, and it just kind of depends
[00:05:45.380 --> 00:05:48.540]   on what you need for your model and what your priorities are,
[00:05:48.540 --> 00:05:50.540]   which side of the trade-off you land on.
[00:05:50.540 --> 00:05:52.940]   If you're doing GPU inference,
[00:05:52.940 --> 00:05:54.460]   then this becomes more tricky,
[00:05:54.460 --> 00:05:57.820]   and there are more specialized tools like TF Serving
[00:05:57.820 --> 00:05:59.120]   that you should look into.
[00:05:59.120 --> 00:06:03.300]   All right, so that's deploying on the web,
[00:06:03.300 --> 00:06:05.780]   and what about deploying into hardware?
[00:06:05.780 --> 00:06:08.220]   So the core challenge here is that your cell phone
[00:06:08.220 --> 00:06:10.240]   does not have the same amount of processing power
[00:06:10.240 --> 00:06:12.140]   that you can get on a server.
[00:06:12.140 --> 00:06:14.940]   And so you often have to use a bunch of tricks,
[00:06:14.940 --> 00:06:16.220]   and Sergey talked about some of them,
[00:06:16.220 --> 00:06:18.460]   to reduce the size of your network
[00:06:18.460 --> 00:06:20.180]   and maybe quantize the weights.
[00:06:20.180 --> 00:06:23.540]   Another challenge is that the frameworks
[00:06:23.540 --> 00:06:25.900]   that people use on mobile are actually less full-featured,
[00:06:25.900 --> 00:06:28.100]   and so you might need to choose your model architecture
[00:06:28.100 --> 00:06:31.040]   specifically to be one that can run on mobile.
[00:06:31.040 --> 00:06:35.740]   There are a few options for doing this in TensorFlow.
[00:06:35.740 --> 00:06:39.500]   There's TensorFlow Lite and TensorFlow Mobile.
[00:06:39.500 --> 00:06:43.340]   And there are also a few that are kind of more specific
[00:06:43.340 --> 00:06:44.460]   to different hardware platforms.
[00:06:44.460 --> 00:06:47.060]   So Apple has a platform, Google has a platform,
[00:06:47.060 --> 00:06:49.220]   and then there's this Fritz option
[00:06:49.220 --> 00:06:52.240]   that claims to be able to work well with both.
[00:06:52.240 --> 00:06:58.460]   Okay, great, that was the lightning five-minute overview
[00:06:58.460 --> 00:07:01.300]   of Sergey's 90-minute lecture.
[00:07:01.300 --> 00:07:04.420]   So I'm curious, were there any questions about the lecture,
[00:07:04.420 --> 00:07:07.920]   concepts that he covered that you would like to talk about?
[00:07:07.920 --> 00:07:11.800]   (audience member speaking faintly)
[00:07:11.800 --> 00:07:12.640]   Yeah.
[00:07:12.640 --> 00:07:16.360]   - Yeah, so Sergey mentioned how he goes all the way
[00:07:16.360 --> 00:07:19.600]   with Docker, where he containerizes each component
[00:07:19.600 --> 00:07:21.160]   in his ML code base.
[00:07:21.160 --> 00:07:23.760]   I was just wondering what benefit does that provide
[00:07:23.760 --> 00:07:25.800]   over just containerizing the whole thing?
[00:07:25.800 --> 00:07:28.440]   - Yeah, I think if you have different components
[00:07:28.440 --> 00:07:31.080]   of a larger system that need to interact with each other,
[00:07:31.080 --> 00:07:33.080]   then it could be helpful to just isolate each of them.
[00:07:33.080 --> 00:07:38.080]   So they have sort of a very small surface area of their API,
[00:07:38.080 --> 00:07:41.000]   and so you kind of know what to expect
[00:07:41.000 --> 00:07:42.360]   from the different components of the system
[00:07:42.360 --> 00:07:44.080]   interacting with each other.
[00:07:44.080 --> 00:07:45.520]   It could make it easier to test,
[00:07:45.520 --> 00:07:48.160]   could make it easier for multiple people on a larger team
[00:07:48.160 --> 00:07:50.120]   to work on different components together
[00:07:50.120 --> 00:07:52.360]   and have kind of a spec
[00:07:52.360 --> 00:07:54.200]   that they need to meet for each other.
[00:07:54.200 --> 00:07:58.880]   Other questions?
[00:07:58.880 --> 00:08:02.840]   Yeah.
[00:08:02.840 --> 00:08:05.000]   - I find that you just showed from training
[00:08:05.000 --> 00:08:08.080]   to interview friends to serving,
[00:08:08.080 --> 00:08:11.280]   which part has been mostly overlooked
[00:08:11.280 --> 00:08:12.600]   and might not take into account
[00:08:12.600 --> 00:08:14.760]   and later on became like a problematic thing
[00:08:14.760 --> 00:08:17.200]   from people that you're talking to?
[00:08:17.200 --> 00:08:22.200]   - Yeah, I think for us at OpenAI, I would say,
[00:08:22.200 --> 00:08:26.040]   and I'm curious what Peter thinks about this as well,
[00:08:26.040 --> 00:08:28.080]   but I feel like training is the one
[00:08:28.080 --> 00:08:29.640]   that was overlooked for a long time.
[00:08:29.640 --> 00:08:31.280]   Like we would often have,
[00:08:31.280 --> 00:08:32.600]   we would really, really push one part
[00:08:32.600 --> 00:08:33.760]   of the code base forward,
[00:08:33.760 --> 00:08:35.280]   and then we would go back to like some model
[00:08:35.280 --> 00:08:37.080]   that we got working months ago
[00:08:37.080 --> 00:08:40.080]   and find out that we could no longer train that model.
[00:08:40.080 --> 00:08:42.000]   So I think that's one thing
[00:08:42.000 --> 00:08:43.720]   that's like really easy to overlook.
[00:08:43.720 --> 00:08:54.440]   Other questions on this?
[00:08:54.440 --> 00:08:55.280]   Yeah.
[00:08:55.280 --> 00:08:56.240]   - Actually, on what you just said,
[00:08:56.240 --> 00:09:00.120]   like when you said you made progress on other part,
[00:09:00.120 --> 00:09:02.120]   you're trying to retrain something,
[00:09:02.120 --> 00:09:05.720]   are you saying you trained another model forward
[00:09:05.720 --> 00:09:09.160]   and then you tried to go back to a different model
[00:09:09.160 --> 00:09:10.280]   in the same code base?
[00:09:10.280 --> 00:09:14.840]   - Yeah, so say you have two or three tasks
[00:09:14.840 --> 00:09:16.240]   that you're working on as a team
[00:09:16.240 --> 00:09:17.560]   and you have like a mono repo
[00:09:17.560 --> 00:09:18.960]   that you're using for all of them.
[00:09:18.960 --> 00:09:20.680]   And you solve the first two of them
[00:09:20.680 --> 00:09:22.360]   and your model works really great.
[00:09:22.360 --> 00:09:26.520]   And you have some weights for those models
[00:09:26.520 --> 00:09:27.640]   that you're happy with.
[00:09:27.640 --> 00:09:29.240]   And then most of the team goes
[00:09:29.240 --> 00:09:31.520]   and works on the third component.
[00:09:31.520 --> 00:09:33.640]   One challenge there is then,
[00:09:33.640 --> 00:09:35.480]   they could push on that third component
[00:09:35.480 --> 00:09:36.600]   for two or three months
[00:09:36.600 --> 00:09:37.920]   and make some breaking changes
[00:09:37.920 --> 00:09:40.960]   to the training part of the pipeline
[00:09:40.960 --> 00:09:44.200]   for the first two components without really realizing it.
[00:09:44.200 --> 00:09:46.960]   And even if you're deploying those,
[00:09:46.960 --> 00:09:48.520]   the first two components into production
[00:09:48.520 --> 00:09:49.360]   or something like that,
[00:09:49.360 --> 00:09:50.200]   you still might not notice
[00:09:50.200 --> 00:09:52.120]   because you might,
[00:09:52.120 --> 00:09:54.680]   the pre-trained model might still be working,
[00:09:54.680 --> 00:09:57.280]   but your ability to actually get the loss down
[00:09:57.280 --> 00:10:00.000]   on that model might have disappeared.
[00:10:00.000 --> 00:10:04.320]   And I think more generally,
[00:10:04.320 --> 00:10:06.280]   outside of the OpenAI context,
[00:10:06.280 --> 00:10:07.960]   something that's less of a problem for us,
[00:10:07.960 --> 00:10:10.320]   but a lot of people complain about
[00:10:10.320 --> 00:10:14.240]   when I talk to them is production monitoring.
[00:10:14.240 --> 00:10:18.120]   Because it's really, really easy
[00:10:18.120 --> 00:10:20.040]   to have data drift
[00:10:20.040 --> 00:10:24.360]   or to have some sort of weird input
[00:10:24.360 --> 00:10:25.440]   go into your pipeline
[00:10:25.440 --> 00:10:27.080]   that you weren't accounting for.
[00:10:27.600 --> 00:10:29.440]   And just break things without even realizing
[00:10:29.440 --> 00:10:31.320]   that they're breaking.
[00:10:31.320 --> 00:10:32.920]   And so I think a lot of teams put a lot of effort
[00:10:32.920 --> 00:10:35.840]   into figuring out how to do that really well.
[00:10:35.840 --> 00:10:36.680]   Yeah.
[00:10:36.680 --> 00:10:37.520]   - Yeah, I was gonna ask,
[00:10:37.520 --> 00:10:39.160]   so when it comes to monitoring,
[00:10:39.160 --> 00:10:43.120]   so let's say I would like to know
[00:10:43.120 --> 00:10:45.160]   if the distribution is skewed
[00:10:45.160 --> 00:10:47.560]   where I'm used to one type of images
[00:10:47.560 --> 00:10:49.960]   and the users that are requesting
[00:10:49.960 --> 00:10:53.400]   are getting another source for the images.
[00:10:53.400 --> 00:10:55.480]   What sort of metrics can I use?
[00:10:56.640 --> 00:10:58.240]   - Yeah, I think Sergey talked about
[00:10:58.240 --> 00:10:59.520]   a couple of them in the lecture.
[00:10:59.520 --> 00:11:00.720]   And this is sort of something
[00:11:00.720 --> 00:11:03.360]   that I have done less of personally.
[00:11:03.360 --> 00:11:04.680]   But I think you would just wanna look at stuff
[00:11:04.680 --> 00:11:07.040]   like the statistics of the inputs and outputs.
[00:11:07.040 --> 00:11:08.880]   - Yeah, you mentioned pixel intensity,
[00:11:08.880 --> 00:11:11.000]   but I wasn't sure what that was.
[00:11:11.000 --> 00:11:13.000]   - Yeah, so just the value of the pixels.
[00:11:13.000 --> 00:11:16.160]   And so, I mean, you could imagine doing trickier things too.
[00:11:16.160 --> 00:11:18.440]   Like you could look at the output distributions
[00:11:18.440 --> 00:11:19.280]   of your model.
[00:11:19.280 --> 00:11:21.480]   And you could,
[00:11:21.480 --> 00:11:23.880]   maybe most of the time your model
[00:11:23.880 --> 00:11:25.440]   produces pretty confident predictions.
[00:11:25.440 --> 00:11:28.000]   And if the confidence of your model starts degrading,
[00:11:28.000 --> 00:11:29.200]   that might be an early warning
[00:11:29.200 --> 00:11:31.160]   that the data distribution is shifting.
[00:11:31.160 --> 00:11:34.320]   Yeah.
[00:11:34.320 --> 00:11:39.880]   But I think like input and output statistics.
[00:11:39.880 --> 00:11:43.600]   So just like over the last N images,
[00:11:43.600 --> 00:11:47.360]   what have been the sort of the average values of things?
[00:11:47.360 --> 00:11:51.920]   And over the last N classifications,
[00:11:51.920 --> 00:11:53.160]   what have been the classes?
[00:11:53.160 --> 00:11:54.320]   What's the distribution of classes
[00:11:54.320 --> 00:11:55.600]   that we've looked at?
[00:11:55.600 --> 00:11:57.320]   That seems to be the most common thing
[00:11:57.320 --> 00:11:59.240]   that people do from talking to people.
[00:11:59.240 --> 00:12:02.800]   Yeah.
[00:12:02.800 --> 00:12:06.880]   - For the context of like NLB distribution,
[00:12:06.880 --> 00:12:09.320]   would it be like vocabulary or?
[00:12:09.320 --> 00:12:13.880]   - Yeah, I mean, I think,
[00:12:13.880 --> 00:12:15.040]   so like if you have,
[00:12:15.040 --> 00:12:20.760]   if you have like a character level model, let's say,
[00:12:20.760 --> 00:12:22.640]   then you could just look at the distribution of characters.
[00:12:22.640 --> 00:12:24.960]   And if that shifts pretty wildly,
[00:12:24.960 --> 00:12:26.520]   then you know that like,
[00:12:26.520 --> 00:12:29.000]   maybe you're getting sort of a weird type of input
[00:12:29.000 --> 00:12:29.840]   into your model.
[00:12:29.840 --> 00:12:31.880]   And you should just make sure that the distribution
[00:12:31.880 --> 00:12:34.280]   isn't vastly different than what you trained on.
[00:12:34.280 --> 00:12:39.280]   Yeah.
[00:12:39.280 --> 00:12:42.560]   - Any tips and tricks on how to shrink down
[00:12:42.560 --> 00:12:44.520]   the package size for the dump?
[00:12:44.520 --> 00:12:49.520]   'Cause it gets pretty big once you get other stuff in there.
[00:12:49.520 --> 00:12:51.640]   Package is too big for the dump.
[00:12:51.640 --> 00:12:56.120]   And then for best practice,
[00:12:56.120 --> 00:12:58.080]   do you have the model weight
[00:12:58.080 --> 00:13:00.240]   sitting somewhere outside of lambda?
[00:13:00.240 --> 00:13:03.160]   And just have the, I guess the processing of lambda
[00:13:03.160 --> 00:13:06.400]   or you have the model weight inside of lambda?
[00:13:06.400 --> 00:13:07.320]   - Yeah, I think,
[00:13:07.320 --> 00:13:09.840]   and someone else who has sort of done more of this
[00:13:09.840 --> 00:13:11.080]   should chime in.
[00:13:11.080 --> 00:13:13.320]   But I think what you do,
[00:13:13.320 --> 00:13:15.560]   I think you definitely wanna have your weights
[00:13:15.560 --> 00:13:16.640]   inside of lambda.
[00:13:16.640 --> 00:13:18.320]   Otherwise you're gonna have to call out to something else
[00:13:18.320 --> 00:13:20.280]   and it's gonna be really slow.
[00:13:20.280 --> 00:13:22.080]   I think like, so there's one bag of tricks,
[00:13:22.080 --> 00:13:23.880]   which is like all these,
[00:13:23.880 --> 00:13:27.040]   model compression, quantization type things.
[00:13:27.040 --> 00:13:29.800]   So that's like how to actually get reduced the size,
[00:13:29.800 --> 00:13:31.800]   like the number of parameters
[00:13:31.800 --> 00:13:34.540]   and the size of the parameter,
[00:13:34.540 --> 00:13:37.100]   the parameter matrices in your model.
[00:13:37.100 --> 00:13:39.520]   And then, aside from that,
[00:13:39.520 --> 00:13:41.400]   it's like just about minimizing the dependencies
[00:13:41.400 --> 00:13:42.560]   that you have, I think.
[00:13:42.560 --> 00:13:45.080]   Any comments on that?
[00:13:45.080 --> 00:13:49.120]   Like any other tips from people who have deployed to lambda?
[00:13:50.120 --> 00:13:52.800]   - So Sergey mentioned at the end
[00:13:52.800 --> 00:13:56.080]   where he removed components features from TensorFlow
[00:13:56.080 --> 00:13:59.280]   that you didn't use like TensorBoard and Comtrip.
[00:13:59.280 --> 00:14:01.000]   So trying to like have a model
[00:14:01.000 --> 00:14:02.240]   that's not using those features
[00:14:02.240 --> 00:14:04.240]   and then getting rid of them.
[00:14:04.240 --> 00:14:05.520]   - Yeah.
[00:14:05.520 --> 00:14:07.440]   Yeah, you can use a more stripped down version
[00:14:07.440 --> 00:14:08.280]   of TensorFlow.
[00:14:08.280 --> 00:14:10.320]   I think that's a really good thing to do as well.
[00:14:10.320 --> 00:14:15.800]   (man speaking off mic)
[00:14:15.800 --> 00:14:16.640]   Yeah.
[00:14:16.640 --> 00:14:18.600]   - I'm monitoring business metrics
[00:14:18.600 --> 00:14:20.080]   as well as user feedback.
[00:14:20.080 --> 00:14:23.320]   (man speaking off mic)
[00:14:23.320 --> 00:14:29.160]   - So what are some examples of those that you might monitor?
[00:14:29.160 --> 00:14:31.440]   - So for example, if you're predicting,
[00:14:31.440 --> 00:14:34.120]   like if you're doing ads, for example,
[00:14:34.120 --> 00:14:35.200]   that click through rates,
[00:14:35.200 --> 00:14:37.680]   or if you're predicting text,
[00:14:37.680 --> 00:14:40.320]   generating text when the user has to accept or not,
[00:14:40.320 --> 00:14:44.400]   user interaction feedback,
[00:14:44.400 --> 00:14:47.320]   are they buying something that you're recommending to them?
[00:14:47.320 --> 00:14:50.600]   (man speaking off mic)
[00:14:50.600 --> 00:14:51.920]   Yeah, I like that a lot.
[00:14:51.920 --> 00:14:55.160]   (man speaking off mic)
[00:14:55.160 --> 00:15:15.800]   So you should look into some of the tools
[00:15:15.800 --> 00:15:19.400]   that Sergey talked about around deploying,
[00:15:19.400 --> 00:15:22.720]   let's see which slide this was.
[00:15:22.720 --> 00:15:31.080]   Yeah, so things like TF serving and Clipper,
[00:15:31.080 --> 00:15:32.640]   if you need to deploy a model that,
[00:15:32.640 --> 00:15:35.000]   where you need to do inference on GPU.
[00:15:35.000 --> 00:15:36.360]   Is that getting your question?
[00:15:36.360 --> 00:15:39.600]   (man speaking off mic)
[00:15:44.320 --> 00:15:45.160]   Like press a button
[00:15:45.160 --> 00:15:47.520]   and have your model automatically get deployed?
[00:15:47.520 --> 00:15:49.880]   Yeah, it's a great question.
[00:15:49.880 --> 00:15:51.440]   It seems like something like that should exist.
[00:15:51.440 --> 00:15:52.280]   I don't know.
[00:15:52.280 --> 00:15:54.520]   Actually, I'm curious, Lucas,
[00:15:54.520 --> 00:15:55.960]   have you seen anything like that?
[00:15:55.960 --> 00:15:59.200]   (man speaking off mic)
[00:15:59.200 --> 00:16:03.120]   That has this claim, but I haven't seen it myself.
[00:16:03.120 --> 00:16:04.600]   But I also wonder,
[00:16:04.600 --> 00:16:08.320]   I mean, I think the big deployment problems,
[00:16:08.320 --> 00:16:10.000]   I think are more,
[00:16:10.000 --> 00:16:12.200]   that we see is more of what Josh was talking about,
[00:16:12.200 --> 00:16:16.840]   versus actually practically moving the files around.
[00:16:16.840 --> 00:16:20.800]   It's so easy to kind of quietly shoot yourself in the foot,
[00:16:20.800 --> 00:16:23.040]   that I think most people aren't,
[00:16:23.040 --> 00:16:25.720]   most people aren't super worried about
[00:16:25.720 --> 00:16:28.920]   how do I more easily copy my file,
[00:16:28.920 --> 00:16:29.760]   and more kind of like,
[00:16:29.760 --> 00:16:32.040]   they want actually kind of more safeguards
[00:16:32.040 --> 00:16:33.360]   than rails around it.
[00:16:33.360 --> 00:16:34.200]   - I see.
[00:16:34.200 --> 00:16:35.480]   So there's this logic around the model
[00:16:35.480 --> 00:16:36.760]   that acts on it.
[00:16:36.760 --> 00:16:37.600]   - Well, just because it's like,
[00:16:37.600 --> 00:16:38.760]   I mean, like Josh was saying,
[00:16:38.760 --> 00:16:42.200]   the problem is the models won't give you an error message.
[00:16:42.200 --> 00:16:44.520]   They'll just start behaving badly.
[00:16:44.520 --> 00:16:46.920]   Every company that I remember worked at,
[00:16:46.920 --> 00:16:49.480]   or worked with, has a horror story,
[00:16:49.480 --> 00:16:51.720]   where they, 'cause you might not realize,
[00:16:51.720 --> 00:16:53.360]   like just if you haven't done it,
[00:16:53.360 --> 00:16:56.120]   like you really do end up with a lot of models
[00:16:56.120 --> 00:16:58.040]   that have upstream models feeding in the data,
[00:16:58.040 --> 00:16:59.480]   and you're like, oh yeah, this failed model
[00:16:59.480 --> 00:17:01.880]   would be like so great for my like relevance model.
[00:17:01.880 --> 00:17:03.640]   Let me just feed that data in.
[00:17:03.640 --> 00:17:04.480]   And then before you know it,
[00:17:04.480 --> 00:17:06.000]   you actually have like 100 other models
[00:17:06.000 --> 00:17:09.960]   that are feeding into some like critical model.
[00:17:09.960 --> 00:17:12.520]   And it's really where you end up every single time, right?
[00:17:12.520 --> 00:17:16.000]   And then, you know, if anybody changes one of those,
[00:17:16.000 --> 00:17:17.560]   it can mess up the result.
[00:17:17.560 --> 00:17:18.680]   I mean, I remember actually,
[00:17:18.680 --> 00:17:20.160]   this is like years ago, so I feel like I can say,
[00:17:20.160 --> 00:17:21.960]   but I actually remember in,
[00:17:21.960 --> 00:17:23.320]   I was working at Yahoo search,
[00:17:23.320 --> 00:17:25.400]   which we were deploying machine learning models.
[00:17:25.400 --> 00:17:27.720]   And at one point we actually deployed a relevance model
[00:17:27.720 --> 00:17:30.080]   that had the relevance reversed.
[00:17:30.080 --> 00:17:32.680]   Like, and it was in a country
[00:17:32.680 --> 00:17:33.960]   that the language was like rare enough
[00:17:33.960 --> 00:17:36.240]   that like, you know, there wasn't like a lot of QA,
[00:17:36.240 --> 00:17:37.080]   and like it was really hard to,
[00:17:37.080 --> 00:17:38.440]   it was actually like, I mean,
[00:17:38.440 --> 00:17:40.960]   you might think that's like so blindingly like stupid,
[00:17:40.960 --> 00:17:41.960]   but it's actually, you know,
[00:17:41.960 --> 00:17:44.720]   there's interactions like an hour before,
[00:17:44.720 --> 00:17:46.400]   you know, the quick metrics cause people to notice
[00:17:46.400 --> 00:17:47.840]   that they should really take it out.
[00:17:47.840 --> 00:17:49.520]   So it really is like, it might seem dumb
[00:17:49.520 --> 00:17:51.680]   if you are like really thinking about it,
[00:17:51.680 --> 00:17:54.720]   but you can really get lulled into complacency.
[00:17:54.720 --> 00:17:55.560]   - Yeah.
[00:17:56.640 --> 00:17:59.000]   Yeah, another thing that people have talked a lot about
[00:17:59.000 --> 00:18:02.280]   that seems to really help here is like, don't, you know,
[00:18:02.280 --> 00:18:03.760]   even if you are pretty confident
[00:18:03.760 --> 00:18:05.840]   that your model is gonna work well,
[00:18:05.840 --> 00:18:07.400]   don't just deploy it into production
[00:18:07.400 --> 00:18:09.040]   to like start serving predictions right away.
[00:18:09.040 --> 00:18:11.320]   Like what a lot of companies do is deploy it
[00:18:11.320 --> 00:18:14.840]   alongside the existing model that they know works okay.
[00:18:14.840 --> 00:18:16.320]   And then just sort of keep track
[00:18:16.320 --> 00:18:18.000]   of all of that stuff for a while,
[00:18:18.000 --> 00:18:20.040]   or maybe even like serve predictions to, you know,
[00:18:20.040 --> 00:18:22.720]   1% of their customers or 10% or something like that.
[00:18:22.720 --> 00:18:24.480]   And then sort of only over time,
[00:18:24.480 --> 00:18:26.880]   when you build more confidence that the model
[00:18:26.880 --> 00:18:28.320]   is gonna work in the real world,
[00:18:28.320 --> 00:18:31.320]   then actually make that sort of your main production model.
[00:18:31.320 --> 00:18:38.680]   - One thing that I suggest,
[00:18:38.680 --> 00:18:39.640]   I don't know if you looked at it,
[00:18:39.640 --> 00:18:42.240]   it's core machine learning engine with,
[00:18:42.240 --> 00:18:44.840]   with Google.
[00:18:44.840 --> 00:18:50.600]   And if you just conform to the TF estimator API,
[00:18:50.600 --> 00:18:52.200]   then you can, it, it,
[00:18:52.200 --> 00:18:53.760]   you try to make it a one click.
[00:18:53.760 --> 00:18:56.680]   - Do you have to use it?
[00:18:56.680 --> 00:18:57.600]   Do you use that?
[00:18:57.600 --> 00:19:01.320]   - I've gone through like the workshops.
[00:19:01.320 --> 00:19:02.160]   - Yeah.
[00:19:02.160 --> 00:19:03.200]   - Yeah, it's pretty nice.
[00:19:03.200 --> 00:19:09.680]   - Cool, any other thoughts on testing deployment?
[00:19:09.680 --> 00:19:16.080]   How many feel like you're gonna be able to like
[00:19:16.080 --> 00:19:18.840]   actually write some unit tests or, you know,
[00:19:18.840 --> 00:19:22.120]   regression tests around your code base for your project
[00:19:22.120 --> 00:19:23.040]   before next week?
[00:19:23.040 --> 00:19:25.720]   One, two?
[00:19:25.720 --> 00:19:27.720]   That's pretty good, yeah.
[00:19:27.720 --> 00:19:28.560]   It's a--
[00:19:28.560 --> 00:19:29.960]   (man speaking off mic)
[00:19:29.960 --> 00:19:30.800]   What's that?
[00:19:30.800 --> 00:19:32.160]   (man speaking off mic)
[00:19:32.160 --> 00:19:33.240]   I don't know, we have a lot.
[00:19:33.240 --> 00:19:34.080]   What do you think, Peter?
[00:19:34.080 --> 00:19:34.920]   Like?
[00:19:34.920 --> 00:19:37.720]   - I would say we probably have,
[00:19:37.720 --> 00:19:40.600]   we're trying to, we just started measuring our coverage
[00:19:40.600 --> 00:19:43.040]   like a few weeks back.
[00:19:43.040 --> 00:19:46.560]   I think we're at like 70%, it's not,
[00:19:46.560 --> 00:19:49.240]   which is so on the order of hundreds
[00:19:49.240 --> 00:19:50.560]   at this point probably.
[00:19:50.560 --> 00:19:52.000]   - Yeah.
[00:19:52.000 --> 00:19:54.400]   - The other thing I'll say about this is that like,
[00:19:54.400 --> 00:19:57.840]   you know, I think like when I'm writing research code,
[00:19:57.840 --> 00:20:00.480]   I usually will start by not writing any tests.
[00:20:00.480 --> 00:20:02.600]   But I think that one mistake people make is like
[00:20:02.600 --> 00:20:04.040]   when you're transitioning from research code
[00:20:04.040 --> 00:20:05.880]   to code that's like gonna be used by a lot of people
[00:20:05.880 --> 00:20:07.360]   or deployed into production,
[00:20:07.360 --> 00:20:10.040]   is like waiting too long to start writing tests.
[00:20:10.040 --> 00:20:13.280]   Because, you know, like the longer time you spend
[00:20:13.280 --> 00:20:14.920]   between when you introduce a bug
[00:20:14.920 --> 00:20:16.280]   and then when it gets caught,
[00:20:16.280 --> 00:20:17.920]   the harder it's gonna be to like to go back
[00:20:17.920 --> 00:20:19.960]   and find where that bug was and fix it.
[00:20:19.960 --> 00:20:22.080]   And this is especially true if there's multiple people
[00:20:22.080 --> 00:20:23.320]   working on the code base.
[00:20:23.320 --> 00:20:25.160]   Yeah.
[00:20:25.160 --> 00:20:26.080]   - So the open AI unit test,
[00:20:26.080 --> 00:20:29.160]   are those actually unit testing the modules
[00:20:29.160 --> 00:20:32.840]   or are they actually unit testing the networks as well?
[00:20:32.840 --> 00:20:33.880]   - Both actually.
[00:20:33.880 --> 00:20:37.880]   So in some cases, we, you know,
[00:20:37.880 --> 00:20:40.320]   we would initialize models and stuff like that
[00:20:40.320 --> 00:20:42.360]   with random weights and test them in various ways.
[00:20:42.360 --> 00:20:45.760]   We also load existing models and stuff like that.
[00:20:45.760 --> 00:20:48.560]   So we tried, I think,
[00:20:49.560 --> 00:20:52.360]   it used to be the case where our unit tests
[00:20:52.360 --> 00:20:54.040]   were kind of integration tests,
[00:20:54.040 --> 00:20:56.360]   kind of really, they were really integration tests,
[00:20:56.360 --> 00:20:58.800]   like, you know, pretending,
[00:20:58.800 --> 00:21:01.400]   they were unit tests pretending to be integration tests.
[00:21:01.400 --> 00:21:05.320]   And it would be like, take a really long time.
[00:21:05.320 --> 00:21:07.080]   Like for example, they would actually train a little bit
[00:21:07.080 --> 00:21:09.160]   and stuff like that, which is really not a good idea
[00:21:09.160 --> 00:21:11.480]   because it turns out if you have hundreds of unit tests
[00:21:11.480 --> 00:21:13.360]   then you have to wait for like an hour
[00:21:13.360 --> 00:21:14.840]   for all the unit tests to run.
[00:21:14.840 --> 00:21:16.800]   So you have to be pretty careful about that.
[00:21:16.800 --> 00:21:18.880]   But like ultimately now we're much more in a state
[00:21:18.880 --> 00:21:21.680]   of testing modules and testing the networks
[00:21:21.680 --> 00:21:24.180]   that are behaving in the way we expect and stuff.
[00:21:24.180 --> 00:21:28.360]   - Great.
[00:21:28.360 --> 00:21:30.520]   Okay, the next thing I wanna do
[00:21:30.520 --> 00:21:33.400]   is just sort of briefly check in on projects.
[00:21:33.400 --> 00:21:34.280]   You know, we have a week left.
[00:21:34.280 --> 00:21:38.000]   The intention is for everyone to kind of present
[00:21:38.000 --> 00:21:40.720]   their projects next week and kind of, you know,
[00:21:40.720 --> 00:21:44.360]   just, you know, what you try to do, where you're at,
[00:21:44.360 --> 00:21:46.240]   and what some of the challenges were.
[00:21:46.240 --> 00:21:47.960]   So I just wanna get a quick sense, like,
[00:21:47.960 --> 00:21:51.200]   how many feel like you have kind of a model
[00:21:51.200 --> 00:21:53.040]   for your project that, you know,
[00:21:53.040 --> 00:21:55.080]   is producing like reasonably good results
[00:21:55.080 --> 00:21:58.040]   and will be, you know, can be like kind of a nice graph
[00:21:58.040 --> 00:22:01.360]   or some nice outputs to talk about next week.
[00:22:01.360 --> 00:22:06.280]   Okay, relatively small number.
[00:22:06.280 --> 00:22:08.560]   For those of you that are not quite at that stage,
[00:22:08.560 --> 00:22:09.500]   what's the main blocker?
[00:22:09.500 --> 00:22:12.600]   Is it, you know, you just, you have sort of a model training
[00:22:12.600 --> 00:22:13.520]   and it's just not good enough,
[00:22:13.520 --> 00:22:16.560]   or is it like you're still not able to get data?
[00:22:16.560 --> 00:22:20.480]   - Training is not good enough.
[00:22:20.480 --> 00:22:22.200]   - Training and it's not good enough.
[00:22:22.200 --> 00:22:24.200]   - It's not going to work.
[00:22:24.200 --> 00:22:25.040]   - Mm-hmm.
[00:22:25.040 --> 00:22:28.600]   - I feel like our data quality wasn't good enough.
[00:22:28.600 --> 00:22:30.880]   - Yeah, data quality wasn't good enough, yep.
[00:22:30.880 --> 00:22:32.180]   That can be a big problem.
[00:22:32.180 --> 00:22:36.120]   Other sort of like big challenges
[00:22:36.120 --> 00:22:37.420]   that people have run into?
[00:22:40.280 --> 00:22:44.200]   (man speaking off mic)
[00:22:44.200 --> 00:22:47.560]   - Something really slim and simple,
[00:22:47.560 --> 00:22:50.520]   like a bare bones TensorFlow could be really employed
[00:22:50.520 --> 00:22:55.160]   to kind of use, but it doesn't work.
[00:22:55.160 --> 00:22:57.040]   So something that is more advanced
[00:22:57.040 --> 00:23:00.440]   and takes advantage of like Keras, you know,
[00:23:00.440 --> 00:23:04.600]   from down in Keras, pre-trained ResNet
[00:23:04.600 --> 00:23:08.000]   and things like that, works way better.
[00:23:08.000 --> 00:23:11.560]   - But it's such a pain in the rear to drag to Lambda.
[00:23:11.560 --> 00:23:12.400]   - Yeah.
[00:23:12.400 --> 00:23:14.640]   - Just as you might mention,
[00:23:14.640 --> 00:23:19.640]   just trimming down the freaking image size is just awful.
[00:23:19.640 --> 00:23:23.480]   - Yeah, okay, so like model's working pretty well,
[00:23:23.480 --> 00:23:25.600]   but deployment is also a challenge.
[00:23:25.600 --> 00:23:27.640]   - Yeah, yeah, so technical things.
[00:23:27.640 --> 00:23:28.480]   - Uh-huh.
[00:23:28.480 --> 00:23:31.920]   - I have the exact same thing for GPT functions.
[00:23:31.920 --> 00:23:33.080]   I didn't know how to deploy this.
[00:23:33.080 --> 00:23:34.640]   - Oh yeah, don't worry.
[00:23:34.640 --> 00:23:35.480]   - Yeah.
[00:23:35.480 --> 00:23:36.320]   - Interesting, okay.
[00:23:36.320 --> 00:23:38.840]   And then this is related to my question earlier.
[00:23:38.840 --> 00:23:39.680]   - Yeah.
[00:23:39.680 --> 00:23:40.600]   - I would imagine that there is
[00:23:40.600 --> 00:23:43.600]   standard procedures for these things.
[00:23:43.600 --> 00:23:47.180]   - Other sort of big problems that people ran into?
[00:23:47.180 --> 00:23:49.440]   Yeah.
[00:23:49.440 --> 00:23:52.240]   - We were starting from an existing research code base.
[00:23:52.240 --> 00:23:53.080]   - Mm-hmm.
[00:23:53.080 --> 00:23:56.400]   - And I think we thought that we could figure out
[00:23:56.400 --> 00:23:57.880]   where they were defining their architecture
[00:23:57.880 --> 00:23:59.560]   and products and different architectures and try it,
[00:23:59.560 --> 00:24:03.480]   and it turned out to be a not working strategy
[00:24:03.480 --> 00:24:05.160]   in this particular code base.
[00:24:05.160 --> 00:24:06.520]   - Interesting, why was that?
[00:24:06.520 --> 00:24:07.360]   Just because the--
[00:24:07.360 --> 00:24:10.680]   - I think it might just be like,
[00:24:10.680 --> 00:24:16.600]   just sort of like the overall complexity of it,
[00:24:16.600 --> 00:24:19.120]   and it's like not written in a way
[00:24:19.120 --> 00:24:21.360]   that's particularly modular,
[00:24:21.360 --> 00:24:24.520]   especially like there's like a feedback
[00:24:24.520 --> 00:24:26.400]   because of the particular type of model
[00:24:26.400 --> 00:24:31.400]   that's like the computing loss depends on more things.
[00:24:31.400 --> 00:24:33.560]   - Yeah, so just like--
[00:24:33.560 --> 00:24:34.720]   - Like the input output thing.
[00:24:34.720 --> 00:24:35.560]   - Yeah.
[00:24:35.560 --> 00:24:37.600]   - So the short version is that like I sort of like
[00:24:37.600 --> 00:24:41.560]   change the goal to just get like an implementation
[00:24:41.560 --> 00:24:46.560]   from scratch working, and so I'm not so much concerned
[00:24:46.560 --> 00:24:49.920]   about whether it performs as well as that code base.
[00:24:49.920 --> 00:24:51.000]   - Yeah.
[00:24:51.000 --> 00:24:52.240]   - But just have something running.
[00:24:52.240 --> 00:24:54.160]   - Mm-hmm, cool.
[00:24:54.160 --> 00:24:56.360]   Yeah, so like working with existing code bases
[00:24:56.360 --> 00:24:59.000]   can be really challenging because, you know,
[00:24:59.000 --> 00:25:01.600]   as much as we wish this were not true,
[00:25:01.600 --> 00:25:03.240]   a lot of times machine learning researchers,
[00:25:03.240 --> 00:25:06.080]   you know, do not write the best, most extendable code.
[00:25:06.080 --> 00:25:08.440]   How many people actually like ran into,
[00:25:08.440 --> 00:25:11.480]   like found bugs in existing code bases that you tried to use?
[00:25:11.480 --> 00:25:12.800]   Okay, yeah.
[00:25:12.800 --> 00:25:15.040]   How many people like used an existing code base
[00:25:15.040 --> 00:25:16.200]   and did not find a bug?
[00:25:16.200 --> 00:25:18.520]   All right, zero, all right, yeah.
[00:25:18.520 --> 00:25:19.760]   This has been my experience as well.
[00:25:19.760 --> 00:25:23.320]   Like I usually, I mean, it's great to be able to start
[00:25:23.320 --> 00:25:24.460]   with an existing code base,
[00:25:24.460 --> 00:25:27.440]   but I'll almost always end up re-implementing it
[00:25:27.440 --> 00:25:29.720]   from scratch myself just because, you know,
[00:25:29.720 --> 00:25:32.440]   I don't generally don't trust random machine learning code
[00:25:32.440 --> 00:25:33.840]   that I find on the internet.
[00:25:33.840 --> 00:25:38.880]   Cool, okay, so I think like the right way
[00:25:38.880 --> 00:25:41.360]   for us to do this next week is, you know,
[00:25:41.360 --> 00:25:44.360]   we'll have sort of a more detailed kind of suggested template
[00:25:44.360 --> 00:25:49.240]   for your presentations, but the, like the attitude
[00:25:49.240 --> 00:25:52.900]   that I want us to have is like, let's be, you know,
[00:25:52.900 --> 00:25:55.200]   talk about like the progress that you made
[00:25:55.200 --> 00:25:56.880]   and sort of how well your thing is working,
[00:25:56.880 --> 00:25:58.080]   but I wanna be like really upfront
[00:25:58.080 --> 00:25:59.960]   about what the big challenges were
[00:25:59.960 --> 00:26:02.000]   and try to spend a little bit of time thinking about,
[00:26:02.000 --> 00:26:03.920]   like if you were gonna start this project over again
[00:26:03.920 --> 00:26:07.320]   from scratch, you know, six, seven weeks ago,
[00:26:07.320 --> 00:26:09.900]   what would you have done differently now that you know
[00:26:09.900 --> 00:26:12.160]   what the challenges that you faced were?
[00:26:12.160 --> 00:26:13.440]   You know, choosing a different project,
[00:26:13.440 --> 00:26:16.000]   choosing a different dataset, different model,
[00:26:16.000 --> 00:26:17.500]   'cause I think that'll be really helpful for people
[00:26:17.500 --> 00:26:19.600]   to just kind of learn from your experience
[00:26:19.600 --> 00:26:22.200]   of what ended up being difficult in your projects.
[00:26:22.200 --> 00:26:25.680]   Great, and Stephanie, did you wanna say anything else
[00:26:25.680 --> 00:26:27.800]   about the project presentations next week?
[00:26:28.920 --> 00:26:31.920]   - Yeah, I think we covered the key points.
[00:26:31.920 --> 00:26:33.360]   Hopefully you guys can give me about,
[00:26:33.360 --> 00:26:35.240]   'cause you know I'm not having enough for next week,
[00:26:35.240 --> 00:26:37.080]   we're not in Brazil.
[00:26:37.080 --> 00:26:40.360]   I just sent something into the Applied Deep Learning channel.
[00:26:40.360 --> 00:26:45.320]   It's just an outline and I'll help you guys gain clarity
[00:26:45.320 --> 00:26:48.280]   around what to present and how to present next week.
[00:26:48.280 --> 00:26:49.520]   It wasn't just a good reflection,
[00:26:49.520 --> 00:26:52.520]   'cause surprise, it's week seven already for the group,
[00:26:52.520 --> 00:26:56.200]   but yeah, if you have any questions,
[00:26:56.200 --> 00:26:58.760]   reach out to Leo, Lucas, or Josh.
[00:26:58.760 --> 00:27:02.400]   And also, if you answer the questions in,
[00:27:02.400 --> 00:27:03.560]   you know, like, have the questions over
[00:27:03.560 --> 00:27:05.280]   and then answer them in another talk,
[00:27:05.280 --> 00:27:08.040]   you can go in and ask Justin to make comments.
[00:27:08.040 --> 00:27:12.120]   Yeah, I think a big part of doing all this work
[00:27:12.120 --> 00:27:15.160]   is to be able to communicate with people and talk about it.
[00:27:15.160 --> 00:27:17.920]   So this will also be good practice.
[00:27:17.920 --> 00:27:19.920]   - I just have one more thing to add to that.
[00:27:19.920 --> 00:27:22.520]   If you plan on projecting your presentation
[00:27:22.520 --> 00:27:25.680]   or using the projector, it's easiest if we're presenting
[00:27:25.680 --> 00:27:28.120]   from a Mac, just 'cause we use AirPlay
[00:27:28.120 --> 00:27:30.000]   to pipe into these monitors.
[00:27:30.000 --> 00:27:33.120]   So if that is an issue, just let one of us know
[00:27:33.120 --> 00:27:36.720]   and we can try and plan some other way of projecting those.
[00:27:36.720 --> 00:27:38.120]   - Yeah, that's a good point.
[00:27:38.120 --> 00:27:40.480]   So slides are probably a good format to do it in.
[00:27:40.480 --> 00:27:43.880]   Maybe just send all of them to,
[00:27:43.880 --> 00:27:45.200]   if you make slides, send them all to me.
[00:27:45.200 --> 00:27:46.600]   We can just use this laptop.
[00:27:46.600 --> 00:27:48.760]   Yeah. - How much time
[00:27:48.760 --> 00:27:49.600]   do we have?
[00:27:49.600 --> 00:27:52.160]   - Five to seven minutes.
[00:27:53.280 --> 00:27:57.000]   - Yeah, I think some of the groups have a live demo, right?
[00:27:57.000 --> 00:27:57.840]   Where's Katie?
[00:27:57.840 --> 00:27:59.520]   We're gonna see.
[00:27:59.520 --> 00:28:00.360]   - Maybe.
[00:28:00.360 --> 00:28:01.200]   (audience laughing)
[00:28:01.200 --> 00:28:03.200]   - No, yeah.
[00:28:03.200 --> 00:28:04.720]   So I know about that group.
[00:28:04.720 --> 00:28:06.280]   That group will get like 15 minutes
[00:28:06.280 --> 00:28:09.280]   or like 30 minutes, depending on how much fun we're having.
[00:28:09.280 --> 00:28:14.280]   But I think we should plan for like seven minutes per group.
[00:28:14.280 --> 00:28:18.520]   If you need more, or let us know.
[00:28:18.520 --> 00:28:19.920]   But next week should be fun.
[00:28:19.920 --> 00:28:22.720]   - Great.
[00:28:22.720 --> 00:28:23.720]   Yeah, that's all for me.
[00:28:23.720 --> 00:28:25.220]   It's time for our guest speakers.

