
[00:00:00.000 --> 00:00:06.400]   Hello guys, welcome to my review of the lip-sync expert is all you need for speech to lip generation in the wild
[00:00:06.400 --> 00:00:08.480]   which is a very long title to say that
[00:00:08.480 --> 00:00:14.720]   Basically, we are talking about a model that can allow you to generate lip-synced videos
[00:00:14.720 --> 00:00:21.920]   Using arbitrary videos and arbitrary audios. Let's see an example directly from the official website of the paper
[00:00:21.920 --> 00:00:28.400]   Here in the example tree proposed we can see that there is a video without any audio
[00:00:29.120 --> 00:00:31.120]   And the person here is not talking
[00:00:31.120 --> 00:00:34.640]   Then we have an audio of three seconds. Let's listen to it
[00:00:34.640 --> 00:00:37.680]   I'll go around wait for my call
[00:00:37.680 --> 00:00:45.760]   Then we press this button sync this pair and the output video which is here. I already generated it before we can play it
[00:00:45.760 --> 00:00:49.280]   I'll go around wait for my call
[00:00:49.280 --> 00:00:52.960]   As you can see the results are remarkable. We can see that the person
[00:00:53.520 --> 00:00:59.140]   Before was not not talking at all. And now actually the his lip has been automatically generated
[00:00:59.140 --> 00:01:03.680]   And the what the movement of his lip actually match what he's saying
[00:01:03.680 --> 00:01:09.600]   I don't see any big difference actually from what I would expect from any other person
[00:01:09.600 --> 00:01:13.120]   Let's go into the details of how this all works
[00:01:13.120 --> 00:01:15.840]   so
[00:01:15.840 --> 00:01:23.200]   The the system can be used for many applications for example for dubbing videos in multiple languages
[00:01:23.360 --> 00:01:26.080]   educational videos or any other thing that you like
[00:01:26.080 --> 00:01:31.120]   And let's go to the architecture of the video of the model
[00:01:31.120 --> 00:01:34.560]   We can see that we have two streams. One is an audio stream
[00:01:34.560 --> 00:01:41.600]   And one way one is a video stream. They are both down sampled using a convolutional neural networks
[00:01:41.600 --> 00:01:47.680]   Combined together and then up sampled again with skip connections from the video stream
[00:01:47.680 --> 00:01:51.600]   And this is the generator. So we are talking about a GAN network
[00:01:52.300 --> 00:01:55.420]   Actually more or less a GAN we will see why it's different
[00:01:55.420 --> 00:02:01.420]   And then the generator frame so we have a sequence of frames
[00:02:01.420 --> 00:02:03.580]   are
[00:02:03.580 --> 00:02:08.860]   Compared with what is the ground truth and this is the reconstruction loss of the image
[00:02:08.860 --> 00:02:17.180]   Actually, the authors claim that this the reconstruction loss is not enough to generate a good image
[00:02:18.380 --> 00:02:24.540]   And which is basically also the technique used by previous models. So before the Wav2Lip was
[00:02:24.540 --> 00:02:28.860]   introduced
[00:02:28.860 --> 00:02:31.820]   Because as the author claims
[00:02:31.820 --> 00:02:34.620]   in the previous
[00:02:34.620 --> 00:02:35.740]   here in 3.1
[00:02:35.740 --> 00:02:39.820]   we can see that the pixel level reconstruction loss is a weak judge of lip-sync why because
[00:02:39.820 --> 00:02:45.660]   The the system tries to the model tries to generate the image and try to make it look like the original
[00:02:45.740 --> 00:02:50.140]   however, the model doesn't concentrate on the lip area only which is
[00:02:50.140 --> 00:02:55.260]   What is what we want is one of the most important thing that we want to judge in this model, right?
[00:02:55.260 --> 00:03:00.540]   But they say that the lip area actually correspond to less than four percent of the total reconstruction loss
[00:03:00.540 --> 00:03:05.340]   So it is um, we can we should find a way to concentrate on that
[00:03:05.340 --> 00:03:10.380]   To generate a better lip area, of course while preserving the original image
[00:03:10.380 --> 00:03:14.460]   So we don't want the background to change. We don't want the pose of the person to change etc
[00:03:15.020 --> 00:03:19.420]   So what the authors do they introduce a sync net sync net is a model
[00:03:19.420 --> 00:03:21.980]   that allows
[00:03:21.980 --> 00:03:23.820]   was introduced previously
[00:03:23.820 --> 00:03:28.620]   Allows to check how much a video and audio are synced together
[00:03:28.620 --> 00:03:31.740]   And if they are not synced by how much they are out of sync
[00:03:31.740 --> 00:03:34.940]   The authors they call it a lip-sync expert
[00:03:34.940 --> 00:03:37.820]   They retrain the sync net
[00:03:37.820 --> 00:03:39.740]   from the ground
[00:03:39.740 --> 00:03:45.900]   Using little variations. For example, the original sync net was trained using black and white images
[00:03:45.900 --> 00:03:51.280]   Now they use color images and secondly, they change the loss function to cosine similarity
[00:03:51.280 --> 00:03:56.560]   So the generator actually the loss function of the generator
[00:03:56.560 --> 00:04:03.500]   Is a combination of the L1 construction loss the GAN loss and the sync loss
[00:04:03.500 --> 00:04:07.900]   We can find it here in the equation number six
[00:04:08.060 --> 00:04:15.100]   So actually the L total is the total loss of the generator, which is a combination of this loss of this loss and this loss
[00:04:15.100 --> 00:04:21.020]   And there are some weights to choose how much emphasis to give to each loss
[00:04:21.020 --> 00:04:27.520]   The the system has been trained using an adapt optimizer, these are the parameters
[00:04:27.520 --> 00:04:30.940]   But let's go to check the results
[00:04:30.940 --> 00:04:32.780]   now
[00:04:32.780 --> 00:04:40.300]   The authors compare the current model with the previous models and using three different data sets
[00:04:40.300 --> 00:04:46.780]   The first one is dubbed. So we have a video and the audio that is dubbed taken from the internet, I guess
[00:04:46.780 --> 00:04:49.340]   and
[00:04:49.340 --> 00:04:52.220]   Where the audio and the video are not in sync
[00:04:52.220 --> 00:04:57.980]   And they try to sync it using Guav2Lip and also the two baseline models Speech2Read and LipGAN
[00:04:57.980 --> 00:05:02.220]   We can see that according to human evaluators. So these are all
[00:05:03.180 --> 00:05:10.220]   Evaluations made by humans. The Guav2Lip actually is preferred. The method of evaluation is written here in
[00:05:10.220 --> 00:05:12.760]   4.4.2
[00:05:12.760 --> 00:05:14.700]   And
[00:05:14.700 --> 00:05:18.060]   Secondly, we have a random data set with that
[00:05:18.060 --> 00:05:23.820]   data set of random videos with random audios and the Guav2Lip is trained to
[00:05:23.820 --> 00:05:30.460]   Sync them and finally we have TTS in which the audio is generated from a TTS system
[00:05:31.100 --> 00:05:36.540]   As we can see overall the Guav2Lip is performing much better. We will see some example later
[00:05:36.540 --> 00:05:39.580]   and according to human evaluators
[00:05:39.580 --> 00:05:42.220]   and
[00:05:42.220 --> 00:05:49.020]   We see that here in the authors right finally it is worth noting that our lip-sync videos are preferred over existing methods
[00:05:49.020 --> 00:05:56.940]   Or even the actual unsynced videos over 90% of the time so it means that also the visual quality is not bad
[00:05:58.540 --> 00:06:01.420]   Here are some examples for example, we can see the red
[00:06:01.420 --> 00:06:08.860]   frames are from the previous models and we can see the quality of the face here of
[00:06:08.860 --> 00:06:11.500]   the german chancellor
[00:06:11.500 --> 00:06:16.140]   is not so good, but here with the syncnet with the
[00:06:16.140 --> 00:06:21.660]   Guav2Lip the reconstruction image is quite good
[00:06:21.660 --> 00:06:26.300]   The authors actually train two models one with
[00:06:27.180 --> 00:06:30.540]   GAN and one without the GAN loss and we can see that
[00:06:30.540 --> 00:06:37.820]   Without the GAN is performing better on some metrics and a little worse on some other metrics
[00:06:37.820 --> 00:06:46.860]   And so actually I think this model have a lot of potential for generating talking avatars or for dubbing videos generating educational videos
[00:06:46.860 --> 00:06:51.820]   Maybe in the future. We don't need to record the three times the same video in multiple language
[00:06:51.820 --> 00:06:55.520]   We can just generate it once and let the AI do the rest. Thank you for listening
[00:06:55.520 --> 00:07:05.520]   [BLANK_AUDIO]

