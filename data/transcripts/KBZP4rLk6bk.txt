
[00:00:00.000 --> 00:00:02.080]   we can actually figure out where are the aliens
[00:00:02.080 --> 00:00:03.840]   out there in space time by being clever
[00:00:03.840 --> 00:00:05.280]   about the few things we can see,
[00:00:05.280 --> 00:00:07.120]   one of which is our current date.
[00:00:07.120 --> 00:00:10.080]   And so now that you have this living cosmology,
[00:00:10.080 --> 00:00:12.960]   we can tell the story that the universe starts out empty
[00:00:12.960 --> 00:00:15.520]   and then at some point, things like us appear,
[00:00:15.520 --> 00:00:18.240]   very primitive, and then some of those
[00:00:18.240 --> 00:00:20.440]   stop being quiet and expand.
[00:00:20.440 --> 00:00:22.360]   And then for a few billion years, they expand
[00:00:22.360 --> 00:00:23.580]   and then they meet each other.
[00:00:23.580 --> 00:00:26.080]   And then for the next hundred billion years,
[00:00:26.080 --> 00:00:28.300]   they commune with each other.
[00:00:28.300 --> 00:00:30.560]   That is the usual models of cosmology say
[00:00:30.560 --> 00:00:34.540]   that in roughly 100, 150 billion years,
[00:00:34.540 --> 00:00:36.560]   the expansion of the universe will happen so much
[00:00:36.560 --> 00:00:39.600]   that all you'll have left is some galaxy clusters
[00:00:39.600 --> 00:00:41.780]   and that are sort of disconnected from each other.
[00:00:41.780 --> 00:00:44.500]   But before then, they will interact.
[00:00:44.500 --> 00:00:45.780]   There will be this community
[00:00:45.780 --> 00:00:48.000]   of all the grabby alien civilizations
[00:00:48.000 --> 00:00:49.340]   and each one of them will hear about
[00:00:49.340 --> 00:00:51.860]   and even meet thousands of others.
[00:00:51.860 --> 00:00:55.120]   And we might hope to join them someday
[00:00:55.120 --> 00:00:56.820]   and become part of that community.
[00:00:57.000 --> 00:00:59.000]   (air whooshing)
[00:00:59.000 --> 00:01:01.680]   The following is a conversation with Robin Hansen,
[00:01:01.680 --> 00:01:04.160]   an economist at George Mason University
[00:01:04.160 --> 00:01:06.760]   and one of the most fascinating, wild, fearless,
[00:01:06.760 --> 00:01:09.320]   and fun minds I've ever gotten a chance to accompany
[00:01:09.320 --> 00:01:12.760]   for a time in exploring questions of human nature,
[00:01:12.760 --> 00:01:16.560]   human civilization, and alien life out there
[00:01:16.560 --> 00:01:19.240]   in our impossibly big universe.
[00:01:19.240 --> 00:01:21.680]   He is the co-author of a book titled
[00:01:21.680 --> 00:01:25.280]   "The Elephant in the Brain, Hidden Motives in Everyday Life,
[00:01:25.280 --> 00:01:28.040]   "The Age of M, Work, Love, and Life
[00:01:28.040 --> 00:01:29.800]   "When Robots Rule the Earth,"
[00:01:29.800 --> 00:01:33.040]   and a fascinating recent paper I recommend
[00:01:33.040 --> 00:01:35.840]   on quote, "Grabby Aliens,"
[00:01:35.840 --> 00:01:39.260]   titled "If Loud Aliens Explain Human Earliness,
[00:01:39.260 --> 00:01:41.680]   "Quiet Aliens Are Also Rare."
[00:01:41.680 --> 00:01:44.480]   This is the Lex Friedman Podcast.
[00:01:44.480 --> 00:01:46.580]   To support it, please check out our sponsors
[00:01:46.580 --> 00:01:47.960]   in the description.
[00:01:47.960 --> 00:01:51.340]   And now, dear friends, here's Robin Hansen.
[00:01:52.420 --> 00:01:56.800]   You are working on a book about quote, "Grabby Aliens."
[00:01:56.800 --> 00:01:59.800]   This is a technical term, like the Big Bang.
[00:01:59.800 --> 00:02:02.200]   So what are grabby aliens?
[00:02:02.200 --> 00:02:07.160]   - Grabby aliens expand fast into the universe
[00:02:07.160 --> 00:02:08.420]   and they change stuff.
[00:02:08.420 --> 00:02:10.580]   That's the key concept.
[00:02:10.580 --> 00:02:14.040]   So if they were out there, we would notice.
[00:02:14.040 --> 00:02:15.840]   That's the key idea.
[00:02:15.840 --> 00:02:19.160]   So the question is, where are the grabby aliens?
[00:02:19.160 --> 00:02:22.000]   So Fermi's question is, where are the aliens?
[00:02:22.000 --> 00:02:24.580]   And we could vary that in two terms, right?
[00:02:24.580 --> 00:02:26.940]   Where are the quiet, hard to see aliens
[00:02:26.940 --> 00:02:30.240]   and where are the big, loud grabby aliens?
[00:02:30.240 --> 00:02:32.680]   So it's actually hard to say
[00:02:32.680 --> 00:02:34.920]   where all the quiet ones are, right?
[00:02:34.920 --> 00:02:37.000]   There could be a lot of them out there
[00:02:37.000 --> 00:02:37.880]   'cause they're not doing much.
[00:02:37.880 --> 00:02:39.760]   They're not making a big difference in the world.
[00:02:39.760 --> 00:02:42.240]   But the grabby aliens, by definition,
[00:02:42.240 --> 00:02:44.100]   are the ones you would see.
[00:02:44.100 --> 00:02:47.240]   We don't know exactly what they do with where they went,
[00:02:47.240 --> 00:02:50.000]   but the idea is they're in some sort of competitive world
[00:02:50.000 --> 00:02:54.160]   where each part of them is trying to grab more stuff
[00:02:54.160 --> 00:02:56.160]   and do something with it.
[00:02:56.160 --> 00:03:00.600]   And almost surely, whatever is the most competitive thing
[00:03:00.600 --> 00:03:03.320]   to do with all the stuff they grab
[00:03:03.320 --> 00:03:06.720]   isn't to leave it alone the way it started, right?
[00:03:06.720 --> 00:03:08.640]   So we humans, when we go around the Earth
[00:03:08.640 --> 00:03:10.000]   and use stuff, we change it.
[00:03:10.000 --> 00:03:14.800]   We turn a forest into a farmland, turn a harbor into a city.
[00:03:14.800 --> 00:03:18.100]   So the idea is aliens would do something with it
[00:03:18.100 --> 00:03:20.500]   and so we're not exactly sure what it would look like,
[00:03:20.500 --> 00:03:21.340]   but it would look different.
[00:03:21.340 --> 00:03:24.460]   So somewhere in the sky, we would see big spheres
[00:03:24.460 --> 00:03:27.300]   of different activity where things had been changed
[00:03:27.300 --> 00:03:29.020]   because they had been there.
[00:03:29.020 --> 00:03:30.880]   - Expanding spheres. - Right.
[00:03:30.880 --> 00:03:33.180]   - So as you expand, you aggressively interact
[00:03:33.180 --> 00:03:34.540]   and change the environment.
[00:03:34.540 --> 00:03:37.700]   So the word grabby versus loud,
[00:03:37.700 --> 00:03:40.740]   you're using them sometimes synonymously, sometimes not.
[00:03:40.740 --> 00:03:45.740]   Grabby to me is a little bit more aggressive.
[00:03:45.740 --> 00:03:47.620]   What does it mean to be loud?
[00:03:47.620 --> 00:03:48.940]   What does it mean to be grabby?
[00:03:48.940 --> 00:03:49.900]   What's the difference?
[00:03:49.900 --> 00:03:51.340]   And loud in what way?
[00:03:51.340 --> 00:03:54.460]   Is it visual, is it sound, is it some other
[00:03:54.460 --> 00:03:57.180]   physical phenomenon like gravitational waves?
[00:03:57.180 --> 00:03:58.900]   What, are you using this kind of
[00:03:58.900 --> 00:04:01.020]   in a broad philosophical sense?
[00:04:01.020 --> 00:04:04.440]   Or there's a specific thing that it means
[00:04:04.440 --> 00:04:07.380]   to be loud in this universe of ours?
[00:04:07.380 --> 00:04:09.920]   - My co-authors and I put together a paper
[00:04:09.920 --> 00:04:12.980]   with a particular mathematical model.
[00:04:12.980 --> 00:04:15.500]   And so we used the term grabby aliens
[00:04:15.500 --> 00:04:17.700]   to describe that more particular model.
[00:04:17.700 --> 00:04:19.940]   And the idea is it's a more particular model
[00:04:19.940 --> 00:04:21.940]   of the general concept of loud.
[00:04:21.940 --> 00:04:23.780]   So loud would just be the general idea
[00:04:23.780 --> 00:04:25.900]   that they would be really obvious.
[00:04:25.900 --> 00:04:27.580]   - So grabby is the technical term,
[00:04:27.580 --> 00:04:29.080]   is it in the title of the paper?
[00:04:29.080 --> 00:04:30.080]   - It's in the body.
[00:04:30.080 --> 00:04:33.260]   The title is actually about loud and quiet.
[00:04:33.260 --> 00:04:34.100]   - Right, loud like that.
[00:04:34.100 --> 00:04:35.180]   - So the idea is there's, you know,
[00:04:35.180 --> 00:04:37.460]   you wanna distinguish your particular model of things
[00:04:37.460 --> 00:04:39.000]   from the general category of things
[00:04:39.000 --> 00:04:40.180]   everybody else might talk about.
[00:04:40.180 --> 00:04:41.540]   So that's how we distinguish.
[00:04:41.540 --> 00:04:42.380]   - The paper title is,
[00:04:42.380 --> 00:04:45.180]   if loud aliens explain human earliness,
[00:04:45.180 --> 00:04:47.400]   quiet aliens are also rare.
[00:04:47.400 --> 00:04:50.540]   If life on Earth, God, this is such a good abstract.
[00:04:50.540 --> 00:04:53.460]   If life on Earth had to achieve--
[00:04:53.460 --> 00:04:54.300]   - N hard.
[00:04:54.300 --> 00:04:57.300]   - N hard steps to reach humanity's level,
[00:04:57.300 --> 00:04:59.860]   then the chance of this event rose as time
[00:04:59.860 --> 00:05:00.920]   to the Nth power.
[00:05:00.920 --> 00:05:02.220]   So we'll talk about power,
[00:05:02.220 --> 00:05:03.940]   we'll talk about linear increase.
[00:05:03.940 --> 00:05:08.640]   So what is the technical definition of grabby?
[00:05:10.120 --> 00:05:12.640]   How do you envision grabbiness?
[00:05:12.640 --> 00:05:16.760]   And why are, in contrast with humans,
[00:05:16.760 --> 00:05:18.160]   why aren't humans grabby?
[00:05:18.160 --> 00:05:20.040]   So like where's that line?
[00:05:20.040 --> 00:05:21.400]   Is it well definable?
[00:05:21.400 --> 00:05:23.880]   What is grabby, what is not grabby?
[00:05:23.880 --> 00:05:26.880]   - We have a mathematical model of the distribution
[00:05:26.880 --> 00:05:31.240]   of advanced civilizations, i.e. aliens, in space and time.
[00:05:31.240 --> 00:05:34.280]   That model has three parameters,
[00:05:34.280 --> 00:05:37.400]   and we can set each one of those parameters from data,
[00:05:37.400 --> 00:05:41.240]   and therefore we claim this is actually what we know about
[00:05:41.240 --> 00:05:43.040]   where they are in space-time.
[00:05:43.040 --> 00:05:46.500]   So the key idea is they appear at some point in space-time,
[00:05:46.500 --> 00:05:51.280]   and then after some short delay, they start expanding,
[00:05:51.280 --> 00:05:53.460]   and they expand at some speed.
[00:05:53.460 --> 00:05:55.400]   And the speed is one of those parameters.
[00:05:55.400 --> 00:05:56.760]   That's one of the three.
[00:05:56.760 --> 00:05:58.960]   And the other two parameters are about
[00:05:58.960 --> 00:06:00.200]   how they appear in time.
[00:06:00.200 --> 00:06:02.780]   That is, they appear at random places,
[00:06:02.780 --> 00:06:05.800]   and they appear in time according to a power law,
[00:06:05.800 --> 00:06:07.600]   and that power law has two parameters,
[00:06:07.600 --> 00:06:10.120]   and we can fit each of those parameters to data.
[00:06:10.120 --> 00:06:13.000]   And so then we can say, now we know.
[00:06:13.000 --> 00:06:15.360]   We know the distribution of advanced civilizations
[00:06:15.360 --> 00:06:16.180]   in space and time.
[00:06:16.180 --> 00:06:19.380]   So we are right now a new civilization,
[00:06:19.380 --> 00:06:21.680]   and we have not yet started to expand.
[00:06:21.680 --> 00:06:23.600]   But plausibly, we would start to do that
[00:06:23.600 --> 00:06:26.880]   within, say, 10 million years of the current moment.
[00:06:26.880 --> 00:06:27.980]   That's plenty of time.
[00:06:27.980 --> 00:06:30.360]   And 10 million years is a really short duration
[00:06:30.360 --> 00:06:31.440]   in the history of the universe.
[00:06:31.440 --> 00:06:36.040]   So we are, at the moment, a sort of random sample
[00:06:36.040 --> 00:06:38.560]   of the kind of times at which an advanced civilization
[00:06:38.560 --> 00:06:41.280]   might appear, because we may or may not become grabby,
[00:06:41.280 --> 00:06:42.880]   but if we do, we'll do it soon.
[00:06:42.880 --> 00:06:45.080]   And so our current date is a sample,
[00:06:45.080 --> 00:06:47.280]   and that gives us one of the other parameters.
[00:06:47.280 --> 00:06:48.960]   The second parameter is the constant
[00:06:48.960 --> 00:06:50.020]   in front of the power law,
[00:06:50.020 --> 00:06:52.760]   and that's arrived from our current date.
[00:06:52.760 --> 00:06:58.600]   - So power law, what is the N in the power law?
[00:06:58.600 --> 00:06:59.880]   What is the constant?
[00:06:59.880 --> 00:07:02.000]   - That's a complicated thing to explain.
[00:07:02.000 --> 00:07:04.560]   Advanced life appeared by going through
[00:07:04.560 --> 00:07:07.400]   a sequence of hard steps.
[00:07:07.400 --> 00:07:09.360]   So starting with very simple life,
[00:07:09.360 --> 00:07:11.240]   and here we are at the end of this process
[00:07:11.240 --> 00:07:12.360]   at pretty advanced life,
[00:07:12.360 --> 00:07:14.760]   and so we had to go through some intermediate steps,
[00:07:14.760 --> 00:07:18.760]   such as sexual selection, photosynthesis,
[00:07:18.760 --> 00:07:22.280]   multicellular animals, and the idea is that
[00:07:22.280 --> 00:07:23.740]   each of those steps was hard.
[00:07:23.740 --> 00:07:26.640]   Evolution just took a long time searching
[00:07:26.640 --> 00:07:28.200]   in a big space of possibilities
[00:07:28.200 --> 00:07:30.880]   to find each of those steps,
[00:07:30.880 --> 00:07:33.720]   and the challenge was to achieve all of those steps
[00:07:33.720 --> 00:07:36.560]   by a deadline of when the planets
[00:07:36.560 --> 00:07:39.400]   would no longer host simple life.
[00:07:39.400 --> 00:07:41.920]   And so Earth has been really lucky
[00:07:41.920 --> 00:07:45.040]   compared to all the other billions of planets out there,
[00:07:45.040 --> 00:07:47.200]   and that we managed to achieve all these steps
[00:07:47.200 --> 00:07:50.920]   in the short time of the five billion years
[00:07:50.920 --> 00:07:53.960]   that Earth can support simple life.
[00:07:53.960 --> 00:07:55.800]   - So not all steps, but a lot of them,
[00:07:55.800 --> 00:07:57.480]   'cause we don't know how many steps there are
[00:07:57.480 --> 00:07:58.800]   before you start the expansion.
[00:07:58.800 --> 00:08:02.320]   So these are all the steps from the birth of life
[00:08:02.320 --> 00:08:05.640]   to the initiation of major expansion.
[00:08:05.640 --> 00:08:07.120]   - Right, so we're pretty sure that
[00:08:07.120 --> 00:08:10.160]   it would happen really soon so that it couldn't be
[00:08:10.160 --> 00:08:12.080]   the same sort of a hard step as the last one,
[00:08:12.080 --> 00:08:13.400]   so in terms of taking a long time.
[00:08:13.400 --> 00:08:16.920]   So when we look at the history of Earth,
[00:08:16.920 --> 00:08:19.160]   we look at the durations of the major things
[00:08:19.160 --> 00:08:23.120]   that have happened, that suggests that there's roughly,
[00:08:23.120 --> 00:08:25.400]   say, six hard steps that happened,
[00:08:25.400 --> 00:08:27.720]   say, between three and 12,
[00:08:27.720 --> 00:08:30.360]   and that we have just achieved the last one
[00:08:30.360 --> 00:08:32.360]   that would take a long time.
[00:08:32.360 --> 00:08:33.960]   - Which is?
[00:08:33.960 --> 00:08:35.280]   - Well, we don't know.
[00:08:35.280 --> 00:08:36.120]   - Oh, okay.
[00:08:36.120 --> 00:08:38.560]   - But whatever it is, we've just achieved the last one.
[00:08:38.560 --> 00:08:40.400]   - Are we talking about humans or aliens here?
[00:08:40.400 --> 00:08:42.280]   So let's talk about some of these steps.
[00:08:42.280 --> 00:08:44.760]   So Earth is really special in some way.
[00:08:44.760 --> 00:08:47.760]   We don't exactly know the level of specialness,
[00:08:47.760 --> 00:08:50.920]   we don't really know which steps were the hardest or not,
[00:08:50.920 --> 00:08:53.000]   because we just have a sample of one,
[00:08:53.000 --> 00:08:55.720]   but you're saying that there's three to 12 steps
[00:08:55.720 --> 00:08:58.280]   that we have to go through to get to where we are
[00:08:58.280 --> 00:09:00.440]   that are hard steps, hard to find by something
[00:09:00.440 --> 00:09:05.440]   that took a long time and is unlikely.
[00:09:05.440 --> 00:09:07.880]   There's a lot of ways to fail.
[00:09:07.880 --> 00:09:10.880]   There's a lot more ways to fail than to succeed.
[00:09:10.880 --> 00:09:13.200]   - The first step would be sort of the very simplest
[00:09:13.200 --> 00:09:14.740]   form of life of any sort,
[00:09:14.740 --> 00:09:19.360]   and then we don't know whether that first sort
[00:09:19.360 --> 00:09:21.640]   is the first sort that we see in the historical record
[00:09:21.640 --> 00:09:24.200]   or not, but then some other steps are, say,
[00:09:24.200 --> 00:09:26.240]   the development of photosynthesis,
[00:09:26.240 --> 00:09:29.480]   the development of sexual reproduction,
[00:09:29.480 --> 00:09:32.160]   there's the development of eukaryotic cells,
[00:09:32.160 --> 00:09:34.040]   which are certain kind of complicated cell
[00:09:34.040 --> 00:09:36.800]   that seems to have only appeared once,
[00:09:36.800 --> 00:09:38.840]   and then there's multicellularity,
[00:09:38.840 --> 00:09:40.680]   that is multiple cells coming together
[00:09:40.680 --> 00:09:42.340]   to large organisms like us.
[00:09:42.340 --> 00:09:46.640]   And in this statistical model of trying to fit
[00:09:46.640 --> 00:09:49.380]   all these steps into a finite window,
[00:09:49.380 --> 00:09:51.620]   the model actually predicts that these steps
[00:09:51.620 --> 00:09:53.260]   could be of varying difficulties,
[00:09:53.260 --> 00:09:55.180]   that is they could each take different amounts
[00:09:55.180 --> 00:09:57.940]   of time on average, but if you're lucky enough
[00:09:57.940 --> 00:10:00.340]   that they all appear at a very short time,
[00:10:00.340 --> 00:10:03.600]   then the durations between them will be roughly equal,
[00:10:03.600 --> 00:10:06.460]   and the time remaining left over in the rest
[00:10:06.460 --> 00:10:08.420]   of the window will also be the same length.
[00:10:08.420 --> 00:10:11.220]   So we at the moment have roughly a billion years
[00:10:11.220 --> 00:10:14.420]   left on Earth until simple life like us
[00:10:14.420 --> 00:10:16.420]   would no longer be possible.
[00:10:16.420 --> 00:10:18.820]   Life appeared roughly 400 million years
[00:10:18.820 --> 00:10:20.860]   after the very first time when life was possible
[00:10:20.860 --> 00:10:24.260]   at the very beginning, so those two numbers right there
[00:10:24.260 --> 00:10:26.940]   give you the rough estimate of six hard steps.
[00:10:26.940 --> 00:10:28.480]   - Just to build up an intuition here,
[00:10:28.480 --> 00:10:32.700]   so we're trying to create a simple mathematical model
[00:10:32.700 --> 00:10:37.700]   of how life emerges and expands in the universe.
[00:10:37.700 --> 00:10:39.780]   And there's a section in this paper,
[00:10:39.780 --> 00:10:41.780]   how many hard steps, question mark.
[00:10:41.780 --> 00:10:43.280]   - Right.
[00:10:43.280 --> 00:10:45.820]   - The two most plausibly diagnostic Earth durations
[00:10:45.820 --> 00:10:47.820]   seem to be the one remaining after now
[00:10:47.820 --> 00:10:51.080]   before Earth becomes uninhabitable for complex life.
[00:10:51.080 --> 00:10:54.380]   So you estimate how long Earth lasts,
[00:10:54.380 --> 00:10:58.380]   how many hard steps, there's windows
[00:10:58.380 --> 00:11:00.780]   for doing different hard steps,
[00:11:00.780 --> 00:11:03.460]   and you can sort of like queuing theory,
[00:11:03.460 --> 00:11:08.460]   mathematically estimate of like the solution
[00:11:08.460 --> 00:11:11.420]   or the passing of the hard steps
[00:11:11.420 --> 00:11:13.220]   or the taking of the hard steps.
[00:11:13.220 --> 00:11:17.020]   Sort of like coldly mathematical look.
[00:11:17.020 --> 00:11:22.020]   If life, pre-expansionary life requires a number of steps,
[00:11:22.020 --> 00:11:26.300]   what is the probability of taking those steps
[00:11:26.300 --> 00:11:28.300]   on an Earth that lasts a billion years
[00:11:28.300 --> 00:11:30.140]   or two billion years or five billion years
[00:11:30.140 --> 00:11:31.740]   or 10 billion years?
[00:11:31.740 --> 00:11:36.140]   And you say solving for E using the observed durations
[00:11:36.140 --> 00:11:41.140]   of 1.1 and 0.4 then gives E values of 3.9 and 12.5,
[00:11:41.140 --> 00:11:46.020]   range 5.7 to 26, suggesting a middle estimate
[00:11:46.020 --> 00:11:49.980]   of at least six, that's where you said six hard steps.
[00:11:49.980 --> 00:11:50.820]   - Right.
[00:11:50.820 --> 00:11:53.140]   - Just to get to where we are.
[00:11:53.140 --> 00:11:53.980]   - Right.
[00:11:53.980 --> 00:11:55.420]   - We started at the bottom, now we're here,
[00:11:55.420 --> 00:11:57.780]   and that took six steps on average.
[00:11:57.780 --> 00:12:00.420]   The key point is on average,
[00:12:00.420 --> 00:12:02.500]   these things on any one random planet
[00:12:02.500 --> 00:12:06.300]   would take trillions or trillions of years,
[00:12:06.300 --> 00:12:07.900]   just a really long time.
[00:12:07.900 --> 00:12:10.100]   And so we're really lucky that they all happened
[00:12:10.100 --> 00:12:14.020]   really fast in a short time before our window closed.
[00:12:14.020 --> 00:12:17.460]   And the chance of that happening in that short window
[00:12:17.460 --> 00:12:19.760]   goes as that time period to the power
[00:12:19.760 --> 00:12:20.740]   of the number of steps.
[00:12:20.740 --> 00:12:22.780]   And so that was where the power we talked about
[00:12:22.780 --> 00:12:24.300]   before it came from.
[00:12:24.300 --> 00:12:27.500]   And so that means in the history of the universe,
[00:12:27.500 --> 00:12:30.780]   we should overall roughly expect advanced life to appear
[00:12:30.780 --> 00:12:32.480]   as a power law in time.
[00:12:32.480 --> 00:12:35.900]   So that very early on, there was very little chance
[00:12:35.900 --> 00:12:38.540]   of anything appearing, and then later on as things appear,
[00:12:38.540 --> 00:12:41.780]   other things are appearing somewhat closer to them in time
[00:12:41.780 --> 00:12:44.260]   because they're all going as this power law.
[00:12:44.260 --> 00:12:45.860]   - What is a power law?
[00:12:45.860 --> 00:12:47.340]   Can we, for people who are not--
[00:12:47.340 --> 00:12:48.180]   - Sure.
[00:12:48.180 --> 00:12:50.020]   - Math inclined, can you describe what a power law is?
[00:12:50.020 --> 00:12:53.220]   - So, say the function x is linear,
[00:12:53.220 --> 00:12:57.220]   and x squared is quadratic, so it's the power of two.
[00:12:57.220 --> 00:13:00.100]   If we make x to the three, that's cubic,
[00:13:00.100 --> 00:13:01.420]   or the power of three.
[00:13:01.420 --> 00:13:05.020]   And so x to the sixth is the power of six.
[00:13:05.020 --> 00:13:08.720]   And so we'd say life appears in the universe
[00:13:08.720 --> 00:13:12.160]   on a planet like Earth in that proportion to the time
[00:13:12.160 --> 00:13:17.160]   that it's been ready for life to appear.
[00:13:17.160 --> 00:13:21.120]   And that over the universe in general,
[00:13:21.120 --> 00:13:24.120]   it'll appear at roughly a power law like that.
[00:13:24.120 --> 00:13:25.780]   - What is the x, what is n?
[00:13:25.780 --> 00:13:28.160]   Is it the number of hard steps?
[00:13:28.160 --> 00:13:29.360]   - Yes, the number of hard steps.
[00:13:29.360 --> 00:13:30.200]   So that's the idea.
[00:13:30.200 --> 00:13:32.720]   - Okay, so it's like if you're gambling
[00:13:32.720 --> 00:13:34.640]   and you're doubling up every time,
[00:13:34.640 --> 00:13:37.240]   this is the probability you just keep winning.
[00:13:37.240 --> 00:13:39.000]   (laughing)
[00:13:39.000 --> 00:13:42.920]   - So it gets very unlikely very quickly.
[00:13:42.920 --> 00:13:46.240]   And so we're the result of this unlikely chain of successes.
[00:13:46.240 --> 00:13:47.760]   - It's actually a lot like cancer.
[00:13:47.760 --> 00:13:51.080]   So the dominant model of cancer in an organism
[00:13:51.080 --> 00:13:53.840]   like each of us is that we have all these cells,
[00:13:53.840 --> 00:13:55.600]   and in order to become cancerous,
[00:13:55.600 --> 00:13:58.980]   a single cell has to go through a number of mutations.
[00:13:58.980 --> 00:14:00.800]   And these are very unlikely mutations,
[00:14:00.800 --> 00:14:02.560]   and so any one cell is very unlikely
[00:14:02.560 --> 00:14:04.680]   to have all these mutations happen
[00:14:04.680 --> 00:14:06.880]   by the time your lifespan's over.
[00:14:06.880 --> 00:14:09.120]   But we have enough cells in our body
[00:14:09.120 --> 00:14:11.760]   that the chance of any one cell producing cancer
[00:14:11.760 --> 00:14:13.420]   by the end of your life is actually pretty high,
[00:14:13.420 --> 00:14:15.360]   more like 40%.
[00:14:15.360 --> 00:14:18.360]   And so the chance of cancer appearing in your lifetime
[00:14:18.360 --> 00:14:19.520]   also goes as a power law,
[00:14:19.520 --> 00:14:22.400]   this power of the number of mutations that's required
[00:14:22.400 --> 00:14:24.600]   for any one cell in your body to become cancerous.
[00:14:24.600 --> 00:14:26.320]   - So the longer you live,
[00:14:26.320 --> 00:14:29.240]   the likely you are to have cancer.
[00:14:29.240 --> 00:14:30.980]   - And the power is also roughly six.
[00:14:30.980 --> 00:14:33.680]   That is, the chance of you getting cancer
[00:14:33.680 --> 00:14:37.120]   is roughly the power of six of the time you've been
[00:14:37.120 --> 00:14:37.960]   since you were born.
[00:14:37.960 --> 00:14:40.640]   - It is perhaps not lost on people
[00:14:40.640 --> 00:14:45.520]   that you're comparing power laws of the survival
[00:14:45.520 --> 00:14:50.360]   or the arrival of the human species to cancerous cells.
[00:14:50.360 --> 00:14:51.700]   The same mathematical model,
[00:14:51.700 --> 00:14:55.160]   but of course we might have a different value assumption
[00:14:55.160 --> 00:14:56.520]   about the two outcomes.
[00:14:56.520 --> 00:14:58.720]   But of course, from the point of view of cancer,
[00:14:58.720 --> 00:15:00.000]   (both laughing)
[00:15:00.000 --> 00:15:01.800]   it's more similar.
[00:15:01.800 --> 00:15:04.040]   From the point of view of cancer, it's a win-win.
[00:15:04.040 --> 00:15:08.000]   We both get to thrive, I suppose.
[00:15:08.000 --> 00:15:11.240]   It is interesting to take the point of view
[00:15:11.240 --> 00:15:13.400]   of all kinds of life forms on earth,
[00:15:13.400 --> 00:15:15.280]   of viruses, of bacteria.
[00:15:15.280 --> 00:15:17.020]   They have a very different view.
[00:15:17.020 --> 00:15:22.600]   It's like the Instagram channel, Nature is Metal.
[00:15:22.600 --> 00:15:25.120]   The ethic under which nature operates
[00:15:25.120 --> 00:15:30.120]   doesn't often correlate with human morals.
[00:15:31.520 --> 00:15:36.120]   It seems cold and machine-like
[00:15:36.120 --> 00:15:38.420]   in the selection process that it performs.
[00:15:38.420 --> 00:15:42.400]   I am an analyst, I'm a scholar, an intellectual,
[00:15:42.400 --> 00:15:45.640]   and I feel I should carefully distinguish
[00:15:45.640 --> 00:15:48.240]   predicting what's likely to happen
[00:15:48.240 --> 00:15:50.560]   and then evaluating or judging
[00:15:50.560 --> 00:15:52.880]   what I think would be better to happen.
[00:15:52.880 --> 00:15:55.620]   And it's a little dangerous to mix those up too closely
[00:15:55.620 --> 00:15:58.800]   because then we can have wishful thinking.
[00:15:58.800 --> 00:16:01.120]   And so I try typically to just analyze
[00:16:01.120 --> 00:16:02.440]   what seems likely to happen,
[00:16:02.440 --> 00:16:04.280]   regardless of whether I like it
[00:16:04.280 --> 00:16:05.880]   or whether we do anything about it.
[00:16:05.880 --> 00:16:08.120]   And then once you see a rough picture
[00:16:08.120 --> 00:16:10.540]   of what's likely to happen if we do nothing,
[00:16:10.540 --> 00:16:12.960]   then we can ask, well, what might we prefer?
[00:16:12.960 --> 00:16:14.720]   And ask where could the levers be
[00:16:14.720 --> 00:16:17.920]   to move it at least a little toward what we might prefer.
[00:16:17.920 --> 00:16:19.480]   - It's good. - And that's a useful,
[00:16:19.480 --> 00:16:21.680]   but often doing that just analysis
[00:16:21.680 --> 00:16:24.320]   of what's likely to happen if we do nothing
[00:16:24.320 --> 00:16:25.960]   offends many people.
[00:16:25.960 --> 00:16:30.740]   They find that dehumanizing or cold or metal, as you say,
[00:16:30.740 --> 00:16:33.280]   to just say, well, this is what's likely to happen
[00:16:33.280 --> 00:16:36.360]   and it's not your favorite, sorry,
[00:16:36.360 --> 00:16:38.800]   but maybe we can do something,
[00:16:38.800 --> 00:16:41.080]   but maybe we can't do that much.
[00:16:41.080 --> 00:16:45.480]   - This is very interesting, that the cold analysis,
[00:16:45.480 --> 00:16:50.520]   whether it's geopolitics, whether it's medicine,
[00:16:50.520 --> 00:16:54.240]   whether it's economics, sometimes misses
[00:16:54.240 --> 00:16:59.240]   some very specific aspect of human condition.
[00:16:59.860 --> 00:17:04.860]   Like for example, when you look at a doctor
[00:17:04.860 --> 00:17:09.140]   and the act of a doctor helping a single patient,
[00:17:09.140 --> 00:17:12.460]   if you do the analysis of that doctor's time
[00:17:12.460 --> 00:17:14.940]   and cost of the medicine or the surgery
[00:17:14.940 --> 00:17:17.160]   or the transportation of the patient,
[00:17:17.160 --> 00:17:18.860]   this is the Paul Farmer question,
[00:17:18.860 --> 00:17:25.060]   is it worth spending 10, 20, $30,000 on this one patient?
[00:17:25.060 --> 00:17:26.460]   When you look at all the people
[00:17:26.460 --> 00:17:27.500]   that are suffering in the world,
[00:17:27.500 --> 00:17:29.760]   that money could be spent so much better.
[00:17:29.760 --> 00:17:33.300]   And yet, there's something about human nature
[00:17:33.300 --> 00:17:37.680]   that wants to help the person in front of you,
[00:17:37.680 --> 00:17:40.600]   and that is actually the right thing to do,
[00:17:40.600 --> 00:17:42.880]   despite the analysis.
[00:17:42.880 --> 00:17:45.040]   And sometimes when you do the analysis,
[00:17:45.040 --> 00:17:47.840]   there's something about the human mind
[00:17:47.840 --> 00:17:50.320]   that allows you to not take that leap,
[00:17:50.320 --> 00:17:55.040]   that irrational leap to act in this way,
[00:17:55.040 --> 00:17:57.200]   that the analysis explains it away.
[00:17:57.200 --> 00:18:01.180]   Well, it's like, for example, the US government,
[00:18:01.180 --> 00:18:04.380]   the DOT, Department of Transportation,
[00:18:04.380 --> 00:18:09.280]   puts a value of, I think, like $9 million on a human life.
[00:18:09.280 --> 00:18:11.700]   And the moment you put that number on a human life,
[00:18:11.700 --> 00:18:13.520]   you can start thinking, well, okay,
[00:18:13.520 --> 00:18:16.480]   I can start making decisions about this or that
[00:18:16.480 --> 00:18:20.380]   and with a sort of cold economic perspective,
[00:18:20.380 --> 00:18:24.060]   and then you might lose, you might deviate
[00:18:24.060 --> 00:18:28.260]   from a deeper truth of what it means to be human somehow.
[00:18:28.260 --> 00:18:32.300]   So you have to dance, because then if you put too much weight
[00:18:32.300 --> 00:18:36.880]   on the anecdotal evidence on these kinds of human emotions,
[00:18:36.880 --> 00:18:39.140]   then you're going to lose,
[00:18:39.140 --> 00:18:42.980]   you could also probably more likely deviate from truth.
[00:18:42.980 --> 00:18:45.420]   But there's something about that cold analysis.
[00:18:45.420 --> 00:18:47.160]   Like I've been listening to a lot of people
[00:18:47.160 --> 00:18:52.160]   coldly analyze wars, war in Yemen, war in Syria,
[00:18:52.820 --> 00:18:56.420]   Israel-Palestine, war in Ukraine,
[00:18:56.420 --> 00:18:59.100]   and there's something lost when you do a cold analysis
[00:18:59.100 --> 00:19:00.740]   of why something happened.
[00:19:00.740 --> 00:19:02.320]   When you talk about energy,
[00:19:02.320 --> 00:19:07.660]   talking about sort of conflict, competition over resources,
[00:19:07.660 --> 00:19:11.380]   when you talk about geopolitics,
[00:19:11.380 --> 00:19:14.540]   sort of models of geopolitics and why a certain war happened,
[00:19:14.540 --> 00:19:18.120]   you lose something about the suffering that happens.
[00:19:18.120 --> 00:19:19.120]   I don't know.
[00:19:19.120 --> 00:19:20.860]   It's an interesting thing because you're both,
[00:19:20.860 --> 00:19:25.860]   you're exceptionally good at models in all domains,
[00:19:25.860 --> 00:19:31.220]   literally, but also there's a humanity to you.
[00:19:31.220 --> 00:19:32.180]   So it's an interesting dance.
[00:19:32.180 --> 00:19:33.980]   I don't know if you can comment on that dance.
[00:19:33.980 --> 00:19:35.140]   - Sure.
[00:19:35.140 --> 00:19:39.340]   It's definitely true as you say that for many people,
[00:19:39.340 --> 00:19:42.180]   if you are accurate in your judgment of say,
[00:19:42.180 --> 00:19:43.980]   for a medical patient, right?
[00:19:43.980 --> 00:19:47.700]   What's the chance that this treatment might help?
[00:19:47.700 --> 00:19:49.780]   And what's the cost?
[00:19:49.780 --> 00:19:51.600]   And compare those to each other.
[00:19:51.600 --> 00:19:52.720]   And you might say,
[00:19:52.720 --> 00:19:57.660]   this looks like a lot of cost for a small medical gain.
[00:19:57.660 --> 00:20:01.580]   And at that point, knowing that fact,
[00:20:01.580 --> 00:20:05.720]   that might take the air out of your sails.
[00:20:05.720 --> 00:20:08.500]   You might not be willing to do the thing
[00:20:08.500 --> 00:20:10.660]   that maybe you feel is right anyway,
[00:20:10.660 --> 00:20:12.300]   which is still to pay for it.
[00:20:12.300 --> 00:20:15.980]   And then somebody knowing that
[00:20:15.980 --> 00:20:17.540]   might wanna keep that news from you,
[00:20:17.540 --> 00:20:20.060]   not tell you about the low chance of success
[00:20:20.060 --> 00:20:23.540]   or the high cost in order to save you this tension,
[00:20:23.540 --> 00:20:26.420]   this awkward moment where you might fail
[00:20:26.420 --> 00:20:28.580]   to do what they and you think is right.
[00:20:28.580 --> 00:20:32.540]   But I think the higher calling,
[00:20:32.540 --> 00:20:34.900]   the higher standard to hold you to,
[00:20:34.900 --> 00:20:38.660]   which many people can be held to is to say,
[00:20:38.660 --> 00:20:41.380]   I will look at things accurately, I will know the truth,
[00:20:41.380 --> 00:20:44.980]   and then I will also do the right thing with it.
[00:20:44.980 --> 00:20:46.980]   I will be at peace with my judgment
[00:20:46.980 --> 00:20:49.200]   about what the right thing is in terms of the truth.
[00:20:49.200 --> 00:20:50.500]   I don't need to be lied to
[00:20:50.500 --> 00:20:54.380]   in order to figure out what the right thing to do is.
[00:20:54.380 --> 00:20:56.620]   And I think if you do think you need to be lied to
[00:20:56.620 --> 00:20:59.660]   in order to figure out what the right thing to do is,
[00:20:59.660 --> 00:21:00.960]   you're at a great disadvantage
[00:21:00.960 --> 00:21:03.500]   because then people will be lying to you,
[00:21:03.500 --> 00:21:04.860]   you will be lying to yourself,
[00:21:04.860 --> 00:21:08.020]   and you won't be as effective
[00:21:08.020 --> 00:21:11.460]   achieving whatever good you were trying to achieve.
[00:21:11.460 --> 00:21:14.780]   - But getting the data, getting the facts is step one,
[00:21:14.780 --> 00:21:16.900]   not the final step. - Absolutely.
[00:21:16.900 --> 00:21:20.020]   So I would say having a good model,
[00:21:20.020 --> 00:21:23.780]   getting the good data is step one, and it's a burden.
[00:21:23.780 --> 00:21:28.340]   Because you can't just use that data
[00:21:28.340 --> 00:21:33.340]   to arrive at sort of the easy, convenient thing.
[00:21:33.340 --> 00:21:34.900]   You have to really deeply think
[00:21:34.900 --> 00:21:36.980]   about what is the right thing.
[00:21:36.980 --> 00:21:40.820]   You can't use, so the dark aspect of data,
[00:21:40.820 --> 00:21:45.500]   of models, is you can use it
[00:21:45.500 --> 00:21:49.460]   to excuse away actions that aren't ethical.
[00:21:49.460 --> 00:21:52.260]   You can use data to basically excuse away anything.
[00:21:52.260 --> 00:21:54.660]   - But not looking at data lets you--
[00:21:54.660 --> 00:21:57.260]   - Excuse yourself to pretend and think
[00:21:57.260 --> 00:21:59.060]   that you're doing good when you're not.
[00:21:59.060 --> 00:22:00.540]   - Exactly.
[00:22:00.540 --> 00:22:01.780]   But it is a burden.
[00:22:01.780 --> 00:22:04.980]   It doesn't excuse you from still being human
[00:22:04.980 --> 00:22:07.540]   and deeply thinking about what is right.
[00:22:07.540 --> 00:22:11.140]   That very kind of gray area, that very subjective area.
[00:22:11.140 --> 00:22:15.260]   That's part of the human condition.
[00:22:15.260 --> 00:22:18.340]   But let us return for a time to aliens.
[00:22:18.340 --> 00:22:21.620]   So you started to define sort of the model,
[00:22:21.620 --> 00:22:25.260]   the parameters of grabbiness.
[00:22:25.260 --> 00:22:26.100]   - Right.
[00:22:26.100 --> 00:22:28.300]   - Or the, as we approach grabbiness.
[00:22:28.300 --> 00:22:29.420]   So what happens?
[00:22:29.420 --> 00:22:31.140]   - So again, there was three parameters.
[00:22:31.140 --> 00:22:32.060]   - Yes.
[00:22:32.060 --> 00:22:34.320]   - There's the speed at which they expand.
[00:22:34.320 --> 00:22:37.420]   There's the rate at which they appear in time.
[00:22:37.420 --> 00:22:39.840]   And that rate has a constant and a power.
[00:22:39.840 --> 00:22:41.700]   So we've talked about the history of life on Earth
[00:22:41.700 --> 00:22:45.540]   suggests that power is around six, but maybe three to 12.
[00:22:45.540 --> 00:22:48.420]   We can say that constant comes from our current date,
[00:22:48.420 --> 00:22:50.540]   sort of sets the overall rate.
[00:22:50.540 --> 00:22:53.260]   And the speed, which is the last parameter,
[00:22:53.260 --> 00:22:55.220]   comes from the fact that when we look in the sky,
[00:22:55.220 --> 00:22:56.400]   we don't see them.
[00:22:56.400 --> 00:22:58.380]   So the model predicts very strongly
[00:22:58.380 --> 00:22:59.900]   that if they were expanding slowly,
[00:22:59.900 --> 00:23:02.020]   say 1% of the speed of light,
[00:23:02.020 --> 00:23:04.820]   our sky would be full of vast spheres
[00:23:04.820 --> 00:23:06.660]   that were full of activity.
[00:23:06.660 --> 00:23:09.900]   That is, at a random time when a civilization
[00:23:09.900 --> 00:23:12.420]   is first appearing, if it looks out into its sky,
[00:23:12.420 --> 00:23:13.820]   it would see many other
[00:23:13.820 --> 00:23:15.500]   grabby alien civilizations in the sky.
[00:23:15.500 --> 00:23:16.900]   And they would be much bigger than the full moon.
[00:23:16.900 --> 00:23:18.580]   They'd be huge spheres in the sky.
[00:23:18.580 --> 00:23:20.460]   And they would be visibly different.
[00:23:20.460 --> 00:23:21.300]   We don't see them.
[00:23:21.300 --> 00:23:22.460]   - Can we pause for a second?
[00:23:22.460 --> 00:23:23.300]   - Okay.
[00:23:23.300 --> 00:23:27.680]   - There's a bunch of hard steps that Earth had to pass
[00:23:27.680 --> 00:23:30.220]   to arrive at this place we are currently,
[00:23:30.220 --> 00:23:33.180]   which we're starting to launch rockets out into space.
[00:23:33.180 --> 00:23:34.820]   We're kind of starting to expand.
[00:23:34.820 --> 00:23:35.660]   - A bit.
[00:23:35.660 --> 00:23:36.620]   - Very slowly.
[00:23:36.620 --> 00:23:37.660]   - Okay.
[00:23:37.660 --> 00:23:39.140]   - But this is like the birth.
[00:23:39.140 --> 00:23:43.180]   If you look at the entirety of the history of Earth,
[00:23:43.180 --> 00:23:46.660]   we're now at this precipice of expansion.
[00:23:46.660 --> 00:23:47.500]   - We could.
[00:23:47.500 --> 00:23:48.320]   We might not choose to.
[00:23:48.320 --> 00:23:51.820]   But if we do, we will do it in the next 10 million years.
[00:23:51.820 --> 00:23:53.460]   - 10 million, wow.
[00:23:53.460 --> 00:23:55.620]   Time flies when you're having fun.
[00:23:55.620 --> 00:23:56.460]   - I was thinking more like a--
[00:23:56.460 --> 00:23:58.400]   - 10 million is a short time on the cosmological scale.
[00:23:58.400 --> 00:24:00.100]   So that is, it might be only 1,000.
[00:24:00.100 --> 00:24:02.220]   But the point is, even if it's up to 10 million,
[00:24:02.220 --> 00:24:03.860]   that hardly makes any difference to the model.
[00:24:03.860 --> 00:24:05.500]   So I might as well give you 10 million.
[00:24:05.500 --> 00:24:08.020]   - This makes me feel,
[00:24:08.020 --> 00:24:10.580]   I was so stressed about planning what I'm gonna do today.
[00:24:10.580 --> 00:24:11.420]   And now--
[00:24:11.420 --> 00:24:12.260]   - Right, you've got plenty of time.
[00:24:12.260 --> 00:24:13.480]   - Plenty of time.
[00:24:13.480 --> 00:24:17.260]   I just need to be generating some offspring quickly here.
[00:24:17.260 --> 00:24:18.100]   Okay.
[00:24:18.100 --> 00:24:21.540]   So, and there's this moment.
[00:24:21.540 --> 00:24:28.180]   This 10 million year gap or window when we start expanding.
[00:24:28.180 --> 00:24:29.480]   And you're saying, okay,
[00:24:29.480 --> 00:24:31.540]   so this is an interesting moment
[00:24:31.540 --> 00:24:33.900]   where there's a bunch of other alien civilizations
[00:24:33.900 --> 00:24:36.380]   that might, at some history of the universe,
[00:24:36.380 --> 00:24:38.180]   arrived at this moment, we're here.
[00:24:38.180 --> 00:24:39.740]   They passed all the hard steps.
[00:24:39.740 --> 00:24:44.140]   There's a model for how likely it is that that happens.
[00:24:44.140 --> 00:24:45.660]   And then they start expanding.
[00:24:45.660 --> 00:24:48.740]   And you think of an expansion as almost like a sphere.
[00:24:48.740 --> 00:24:49.580]   - Right.
[00:24:49.580 --> 00:24:50.860]   - When you say speed,
[00:24:50.860 --> 00:24:53.860]   we're talking about the speed of the radius growth.
[00:24:53.860 --> 00:24:56.540]   - Exactly, the surface, how fast the surface expands.
[00:24:56.540 --> 00:24:59.580]   - Okay, and so you're saying that there is some speed
[00:24:59.580 --> 00:25:01.700]   for that expansion, average speed.
[00:25:01.700 --> 00:25:04.460]   And then we can play with that parameter.
[00:25:04.460 --> 00:25:07.260]   And if that speed is super slow,
[00:25:07.260 --> 00:25:10.660]   then maybe that explains why we haven't seen anything.
[00:25:10.660 --> 00:25:11.700]   If it's super fast--
[00:25:11.700 --> 00:25:14.220]   - Well, if the slow would create the puzzle,
[00:25:14.220 --> 00:25:16.700]   if slow predicts we would see them, but we don't see them.
[00:25:16.700 --> 00:25:17.540]   - Okay.
[00:25:17.540 --> 00:25:19.100]   - And so the way to explain that is that they're fast.
[00:25:19.100 --> 00:25:21.960]   So the idea is if they're moving really fast,
[00:25:21.960 --> 00:25:25.100]   then we don't see them until they're almost here.
[00:25:25.100 --> 00:25:26.580]   - And okay, this is counterintuitive.
[00:25:26.580 --> 00:25:27.700]   All right, hold on a second.
[00:25:27.700 --> 00:25:29.380]   So I think this works best
[00:25:29.380 --> 00:25:31.100]   when I say a bunch of dumb things.
[00:25:31.100 --> 00:25:31.940]   - Okay.
[00:25:33.180 --> 00:25:37.660]   - And then you elucidate the full complexity
[00:25:37.660 --> 00:25:39.180]   and the beauty of the dumbness.
[00:25:39.180 --> 00:25:44.180]   Okay, so there's these spheres out there in the universe
[00:25:44.180 --> 00:25:47.180]   that are made visible because they're sort of
[00:25:47.180 --> 00:25:48.580]   using a lot of energy.
[00:25:48.580 --> 00:25:49.860]   So they're generating a lot of light.
[00:25:49.860 --> 00:25:51.460]   - Doing stuff, they're changing things.
[00:25:51.460 --> 00:25:52.300]   - They're changing things.
[00:25:52.300 --> 00:25:56.220]   And change would be visible a long way off.
[00:25:56.220 --> 00:25:57.060]   - Yes.
[00:25:57.060 --> 00:25:58.860]   - They would take apart stars, rearrange them,
[00:25:58.860 --> 00:26:00.540]   restructure galaxies, they would just--
[00:26:00.540 --> 00:26:02.580]   - All kinds of fun. - Big, huge stuff.
[00:26:02.580 --> 00:26:06.060]   - Okay, if they're expanding slowly,
[00:26:06.060 --> 00:26:09.860]   we would see a lot of them because the universe is old.
[00:26:09.860 --> 00:26:12.500]   Is old enough to where we would see--
[00:26:12.500 --> 00:26:15.100]   - That is, we're assuming we're just typical,
[00:26:15.100 --> 00:26:16.820]   maybe at the 50th percentile of them.
[00:26:16.820 --> 00:26:18.820]   So like half of them have appeared so far,
[00:26:18.820 --> 00:26:20.780]   the other half will still appear later.
[00:26:20.780 --> 00:26:26.300]   And the math of our best estimate is that
[00:26:26.300 --> 00:26:29.000]   they appear roughly once per million galaxies.
[00:26:30.140 --> 00:26:33.180]   And we would meet them in roughly a billion years
[00:26:33.180 --> 00:26:35.620]   if we expanded out to meet them.
[00:26:35.620 --> 00:26:39.140]   - So we're looking at a Grabby Aliens model, 3D sim.
[00:26:39.140 --> 00:26:39.980]   - Right.
[00:26:39.980 --> 00:26:42.580]   - What's, that's the actual name of the video.
[00:26:42.580 --> 00:26:47.580]   What, by the time we get to 13.8 billion years,
[00:26:47.580 --> 00:26:49.860]   the fun begins.
[00:26:49.860 --> 00:26:54.860]   Okay, so this is, we're watching a three-dimensional sphere
[00:26:54.860 --> 00:26:57.500]   rotating, I presume that's the universe,
[00:26:57.500 --> 00:26:59.660]   and then Grabby Aliens are expanding
[00:26:59.660 --> 00:27:01.020]   and filling that universe--
[00:27:01.020 --> 00:27:01.860]   - Exactly.
[00:27:01.860 --> 00:27:04.140]   - With all kinds of fun.
[00:27:04.140 --> 00:27:05.540]   - Pretty soon it's all full.
[00:27:05.540 --> 00:27:06.360]   - It's full.
[00:27:06.360 --> 00:27:11.360]   So that's how the Grabby Aliens come in contact,
[00:27:11.360 --> 00:27:13.340]   first of all, with other aliens,
[00:27:13.340 --> 00:27:16.380]   and then with us, humans.
[00:27:16.380 --> 00:27:18.740]   The following is a simulation of the Grabby Aliens model
[00:27:18.740 --> 00:27:20.260]   of alien civilizations.
[00:27:20.260 --> 00:27:22.220]   Civilizations are born,
[00:27:22.220 --> 00:27:24.620]   they expand outwards at constant speed.
[00:27:24.620 --> 00:27:27.040]   A spherical region of space is shown.
[00:27:27.040 --> 00:27:30.180]   By the time we get to 13.8 billion years,
[00:27:30.180 --> 00:27:33.820]   this sphere will be about 3,000 times as wide
[00:27:33.820 --> 00:27:37.520]   as the distance from the Milky Way to Andromeda.
[00:27:37.520 --> 00:27:39.000]   Okay, this is fun.
[00:27:39.000 --> 00:27:39.840]   - It's huge.
[00:27:39.840 --> 00:27:41.540]   - Okay, it's huge.
[00:27:41.540 --> 00:27:45.560]   All right, so why don't we see,
[00:27:45.560 --> 00:27:49.180]   we're one little tiny, tiny, tiny, tiny dot
[00:27:49.180 --> 00:27:50.820]   in that giant, giant sphere.
[00:27:50.820 --> 00:27:51.660]   - Right.
[00:27:51.660 --> 00:27:54.520]   - Why don't we see any of the Grabby Aliens?
[00:27:55.400 --> 00:27:57.520]   - Depends on how fast they expand.
[00:27:57.520 --> 00:27:59.220]   So you could see that if they expanded
[00:27:59.220 --> 00:28:00.620]   at the speed of light,
[00:28:00.620 --> 00:28:03.260]   you wouldn't see them until they were here.
[00:28:03.260 --> 00:28:04.100]   So like out there,
[00:28:04.100 --> 00:28:05.480]   if somebody is destroying the universe
[00:28:05.480 --> 00:28:07.760]   with a vacuum decay,
[00:28:07.760 --> 00:28:10.760]   there's this doomsday scenario
[00:28:10.760 --> 00:28:13.040]   where somebody somewhere could change
[00:28:13.040 --> 00:28:14.480]   the vacuum of the universe,
[00:28:14.480 --> 00:28:15.840]   and that would expand at the speed of light
[00:28:15.840 --> 00:28:17.260]   and basically destroy everything it hit.
[00:28:17.260 --> 00:28:19.320]   But you'd never see that until it got here,
[00:28:19.320 --> 00:28:21.520]   'cause it's expanding at the speed of light.
[00:28:21.520 --> 00:28:22.680]   If you're expanding really slow,
[00:28:22.680 --> 00:28:24.420]   then you see it from a long way off.
[00:28:24.420 --> 00:28:26.720]   So the fact we don't see anything in the sky
[00:28:26.720 --> 00:28:28.800]   tells us they're expanding fast,
[00:28:28.800 --> 00:28:30.760]   say over a third the speed of light,
[00:28:30.760 --> 00:28:32.520]   and that's really, really fast.
[00:28:32.520 --> 00:28:35.140]   But that's what you have to believe
[00:28:35.140 --> 00:28:37.680]   if you look out and you don't see anything.
[00:28:37.680 --> 00:28:38.520]   Now you might say,
[00:28:38.520 --> 00:28:40.560]   well, maybe I just don't wanna believe this whole model.
[00:28:40.560 --> 00:28:42.560]   Why should I believe this whole model at all?
[00:28:42.560 --> 00:28:46.000]   And our best evidence why you should believe this model
[00:28:46.000 --> 00:28:47.280]   is our early date.
[00:28:47.280 --> 00:28:50.120]   We are right now,
[00:28:50.120 --> 00:28:52.980]   almost 14 million years into the universe,
[00:28:52.980 --> 00:28:55.240]   on a planet around a star
[00:28:55.240 --> 00:28:57.280]   that's roughly five billion years old.
[00:28:57.280 --> 00:29:00.840]   But the average star out there
[00:29:00.840 --> 00:29:04.040]   will last roughly five trillion years.
[00:29:04.040 --> 00:29:06.960]   That is 1,000 times longer.
[00:29:06.960 --> 00:29:08.600]   And remember that power law.
[00:29:08.600 --> 00:29:10.520]   It says that the chance of advanced life
[00:29:10.520 --> 00:29:13.900]   appearing on a planet goes as the power of sixth of the time.
[00:29:13.900 --> 00:29:17.160]   So if a planet lasts 1,000 times longer,
[00:29:17.160 --> 00:29:20.120]   then the chance of it appearing on that planet,
[00:29:20.120 --> 00:29:21.760]   if everything would stay empty at least,
[00:29:21.760 --> 00:29:25.220]   is 1,000 to the sixth power, or 10 to the 18.
[00:29:25.220 --> 00:29:28.900]   So enormous, overwhelming chance
[00:29:28.900 --> 00:29:30.460]   that if the universe would just say,
[00:29:30.460 --> 00:29:33.020]   sit and empty and waiting for advanced life to appear,
[00:29:33.020 --> 00:29:36.500]   when it would appear would be way at the end
[00:29:36.500 --> 00:29:39.180]   of all these planet lifetimes.
[00:29:39.180 --> 00:29:42.220]   That is the long planets near the end of the lifetime,
[00:29:42.220 --> 00:29:44.460]   trillions of years into the future.
[00:29:44.460 --> 00:29:46.460]   But we're really early compared to that.
[00:29:46.460 --> 00:29:47.820]   And our explanation is,
[00:29:47.820 --> 00:29:49.460]   at the moment, as you saw in the video,
[00:29:49.460 --> 00:29:51.840]   the universe is filling up in roughly a billion years.
[00:29:51.840 --> 00:29:52.880]   It'll all be full.
[00:29:52.880 --> 00:29:54.320]   And at that point, it's too late
[00:29:54.320 --> 00:29:55.760]   for advanced life to show up.
[00:29:55.760 --> 00:29:57.960]   So you had to show up now before that deadline.
[00:29:57.960 --> 00:30:00.240]   - Okay, can we break that apart a little bit?
[00:30:00.240 --> 00:30:03.160]   Okay, or linger on some of the things you said.
[00:30:03.160 --> 00:30:06.120]   So with the power law, the things we've done on Earth,
[00:30:06.120 --> 00:30:10.480]   the model you have says that it's very unlikely.
[00:30:10.480 --> 00:30:13.240]   Like we're lucky SOBs.
[00:30:13.240 --> 00:30:15.400]   Is that mathematically correct to say?
[00:30:15.400 --> 00:30:18.440]   - We're crazy early.
[00:30:18.440 --> 00:30:20.340]   - That is. - When early means like--
[00:30:20.340 --> 00:30:21.700]   - In the history of the universe.
[00:30:21.700 --> 00:30:26.420]   - In the history, okay, so given this model,
[00:30:26.420 --> 00:30:28.780]   how do we make sense of that?
[00:30:28.780 --> 00:30:31.740]   If we're super, can we just be the lucky ones?
[00:30:31.740 --> 00:30:34.020]   - Well, 10 to the 18 lucky, you know?
[00:30:34.020 --> 00:30:35.100]   How lucky do you feel?
[00:30:35.100 --> 00:30:38.020]   So, you know. (laughs)
[00:30:38.020 --> 00:30:39.860]   That's pretty lucky, right?
[00:30:39.860 --> 00:30:41.720]   10 to the 18 is a billion billion.
[00:30:41.720 --> 00:30:46.100]   So then if you were just being honest and humble,
[00:30:46.100 --> 00:30:49.020]   that that means, what does that mean?
[00:30:49.020 --> 00:30:50.740]   - Means one of the assumptions that calculated
[00:30:50.740 --> 00:30:52.860]   this crazy early must be wrong.
[00:30:52.860 --> 00:30:53.700]   That's what it means.
[00:30:53.700 --> 00:30:55.500]   So the key assumption we suggest is
[00:30:55.500 --> 00:30:57.840]   that the universe would stay empty.
[00:30:57.840 --> 00:31:02.060]   So most life would appear like 1,000 times longer
[00:31:02.060 --> 00:31:04.780]   later than now if everything would stay empty
[00:31:04.780 --> 00:31:06.480]   waiting for it to appear.
[00:31:06.480 --> 00:31:08.300]   - So what does non-empty mean?
[00:31:08.300 --> 00:31:10.660]   - So the gravity aliens are filling the universe right now.
[00:31:10.660 --> 00:31:13.120]   Roughly at the moment they've filled half of the universe
[00:31:13.120 --> 00:31:14.300]   and they've changed it.
[00:31:14.300 --> 00:31:15.620]   And when they fill everything,
[00:31:15.620 --> 00:31:17.380]   it's too late for stuff like us to appear.
[00:31:17.380 --> 00:31:18.820]   - But wait, hold on a second.
[00:31:18.820 --> 00:31:23.040]   Did anyone help us get lucky?
[00:31:23.040 --> 00:31:26.340]   If it's so difficult, how do, like--
[00:31:26.340 --> 00:31:28.260]   - So it's like cancer, right?
[00:31:28.260 --> 00:31:29.940]   There's all these cells, each of which
[00:31:29.940 --> 00:31:32.340]   randomly does or doesn't get cancer.
[00:31:32.340 --> 00:31:34.380]   And eventually some cell gets cancer
[00:31:34.380 --> 00:31:36.440]   and, you know, we were one of those.
[00:31:36.440 --> 00:31:38.940]   - But hold on a second.
[00:31:38.940 --> 00:31:40.020]   Okay.
[00:31:40.020 --> 00:31:41.540]   But we got it early.
[00:31:41.540 --> 00:31:42.900]   We got it-- - Early compared to
[00:31:42.900 --> 00:31:46.260]   the prediction with an assumption that's wrong.
[00:31:46.260 --> 00:31:48.180]   So that's how we do a lot of, you know,
[00:31:48.180 --> 00:31:49.540]   theoretical analysis.
[00:31:49.540 --> 00:31:51.420]   You have a model that makes a prediction that's wrong,
[00:31:51.420 --> 00:31:53.300]   then that helps you reject that model.
[00:31:53.300 --> 00:31:54.120]   - Okay.
[00:31:54.120 --> 00:31:56.460]   Let's try to understand exactly where the wrong is.
[00:31:56.460 --> 00:31:59.620]   So the assumption is that the universe is empty.
[00:31:59.620 --> 00:32:01.380]   - Stays empty. - Stays empty.
[00:32:01.380 --> 00:32:03.780]   - And waits until this advanced life
[00:32:03.780 --> 00:32:05.820]   appears in trillions of years.
[00:32:05.820 --> 00:32:07.480]   That is, if the universe would just stay empty,
[00:32:07.480 --> 00:32:10.820]   if there was just, you know, nobody else out there,
[00:32:10.820 --> 00:32:14.180]   then when you should expect advanced life to appear,
[00:32:14.180 --> 00:32:15.300]   if you're the only one in the universe,
[00:32:15.300 --> 00:32:16.580]   when should you expect to appear?
[00:32:16.580 --> 00:32:18.380]   You should expect to appear trillions of years
[00:32:18.380 --> 00:32:19.620]   in the future.
[00:32:19.620 --> 00:32:20.460]   - I see.
[00:32:20.460 --> 00:32:21.280]   Right, right.
[00:32:21.280 --> 00:32:25.140]   So this is a very sort of nuanced mathematical assumption.
[00:32:25.140 --> 00:32:29.420]   I don't think we can intuit it cleanly with words.
[00:32:29.420 --> 00:32:32.740]   But if you assume that you're just,
[00:32:32.740 --> 00:32:35.220]   the universe stays empty and you're waiting
[00:32:35.220 --> 00:32:40.220]   for one life civilization to pop up,
[00:32:40.700 --> 00:32:44.940]   then it should happen very late, much later than now.
[00:32:44.940 --> 00:32:49.260]   And if you look at Earth, the way things happen on Earth,
[00:32:49.260 --> 00:32:51.740]   it happened much, much, much, much, much earlier
[00:32:51.740 --> 00:32:53.540]   than it was supposed to according to this model
[00:32:53.540 --> 00:32:55.360]   if you take the initial assumption.
[00:32:55.360 --> 00:32:58.100]   Therefore, you can say, well, the initial assumption
[00:32:58.100 --> 00:33:00.740]   of the universe staying empty is very unlikely.
[00:33:00.740 --> 00:33:01.580]   - Right.
[00:33:01.580 --> 00:33:02.420]   - Okay.
[00:33:02.420 --> 00:33:04.900]   - And the other alternative theory is the universe
[00:33:04.900 --> 00:33:07.140]   is filling up and will fill up soon.
[00:33:07.140 --> 00:33:10.020]   And so we are typical for the origin data
[00:33:10.020 --> 00:33:12.380]   of things that can appear before the deadline.
[00:33:12.380 --> 00:33:13.220]   - Before the deadline.
[00:33:13.220 --> 00:33:15.460]   Okay, it's filling up, so why don't we see anything
[00:33:15.460 --> 00:33:16.420]   if it's filling up?
[00:33:16.420 --> 00:33:19.060]   - Because they're expanding really fast.
[00:33:19.060 --> 00:33:19.980]   - Close to the speed of light.
[00:33:19.980 --> 00:33:20.820]   - Exactly.
[00:33:20.820 --> 00:33:22.420]   - So we will only see it when it's here.
[00:33:22.420 --> 00:33:23.820]   - Almost here.
[00:33:23.820 --> 00:33:24.660]   - Okay.
[00:33:24.660 --> 00:33:29.940]   What are the ways in which we might see a quickly expanding?
[00:33:29.940 --> 00:33:32.860]   - This is both exciting and terrifying.
[00:33:32.860 --> 00:33:33.700]   - It is terrifying.
[00:33:33.700 --> 00:33:36.380]   - It's like watching a truck driving at you
[00:33:36.380 --> 00:33:37.820]   at 100 miles an hour.
[00:33:39.220 --> 00:33:41.780]   - So we would see spheres in the sky,
[00:33:41.780 --> 00:33:44.820]   at least one sphere in the sky, growing very rapidly.
[00:33:44.820 --> 00:33:48.300]   - Like very rapidly.
[00:33:48.300 --> 00:33:50.440]   - Right, yes, very rapidly.
[00:33:50.440 --> 00:33:55.180]   - So there's different, 'cause we were just talking
[00:33:55.180 --> 00:33:57.740]   about 10 million years, this would be--
[00:33:57.740 --> 00:34:00.580]   - You might see it 10 million years in advance coming.
[00:34:00.580 --> 00:34:02.660]   I mean, you still might have a long warning.
[00:34:02.660 --> 00:34:06.100]   Again, the universe is 14 million years old.
[00:34:06.100 --> 00:34:08.300]   The typical origin times of these things
[00:34:08.300 --> 00:34:10.060]   are spread over several billion years.
[00:34:10.060 --> 00:34:13.880]   So the chance of one originating very close to you in time
[00:34:13.880 --> 00:34:14.820]   is very low.
[00:34:14.820 --> 00:34:18.460]   So it still might take millions of years
[00:34:18.460 --> 00:34:21.540]   from the time you see it, from the time it gets here.
[00:34:21.540 --> 00:34:23.620]   You've got a million years to be terrified
[00:34:23.620 --> 00:34:25.540]   of this fast sphere coming at you.
[00:34:25.540 --> 00:34:27.300]   - But coming at you very fast,
[00:34:27.300 --> 00:34:29.380]   so if they're traveling close to the speed of light--
[00:34:29.380 --> 00:34:31.380]   - But they're coming from a long way away.
[00:34:31.380 --> 00:34:33.620]   So remember, the rate at which they appear
[00:34:33.620 --> 00:34:36.080]   is one per million galaxies.
[00:34:36.080 --> 00:34:36.920]   - Right.
[00:34:36.920 --> 00:34:40.160]   - So they're roughly 100 galaxies away.
[00:34:40.160 --> 00:34:43.660]   - I see, so the delta between the speed of light
[00:34:43.660 --> 00:34:46.740]   and their actual travel speed is very important?
[00:34:46.740 --> 00:34:48.060]   - Right, so if they're going at, say,
[00:34:48.060 --> 00:34:49.500]   half the speed of light--
[00:34:49.500 --> 00:34:50.940]   - We'll have a long time.
[00:34:50.940 --> 00:34:52.000]   - Then-- - Yeah.
[00:34:52.000 --> 00:34:54.740]   But what if they're traveling exactly at a speed of light?
[00:34:54.740 --> 00:34:56.220]   Then we see 'em like--
[00:34:56.220 --> 00:34:57.260]   - Then we wouldn't have much warning,
[00:34:57.260 --> 00:34:58.500]   but that's less likely.
[00:34:58.500 --> 00:34:59.820]   Well, we can't exclude it.
[00:34:59.820 --> 00:35:03.060]   - And they could also be somehow traveling fast
[00:35:03.060 --> 00:35:04.860]   in the speed of light.
[00:35:04.860 --> 00:35:06.380]   - Well, I think we can exclude,
[00:35:06.380 --> 00:35:08.480]   because if they could go faster than the speed of light,
[00:35:08.480 --> 00:35:11.020]   then they would just already be everywhere.
[00:35:11.020 --> 00:35:13.140]   So in a universe where you can travel faster
[00:35:13.140 --> 00:35:15.660]   than the speed of light, you can go backwards in space time.
[00:35:15.660 --> 00:35:17.820]   So any time you appeared anywhere in space time,
[00:35:17.820 --> 00:35:19.620]   you could just fill up everything.
[00:35:19.620 --> 00:35:21.300]   - Yeah, and--
[00:35:21.300 --> 00:35:23.040]   - So anybody in the future, whoever appeared,
[00:35:23.040 --> 00:35:24.780]   they would have been here by now.
[00:35:24.780 --> 00:35:26.300]   - Can you exclude the possibility
[00:35:26.300 --> 00:35:28.660]   that those kinds of aliens aren't already here?
[00:35:28.660 --> 00:35:33.020]   - Well, we should have a different discussion of that.
[00:35:33.020 --> 00:35:33.860]   - Right, okay.
[00:35:33.860 --> 00:35:36.140]   Well, let's actually leave that discussion aside
[00:35:36.140 --> 00:35:39.700]   just to linger and understand the Grabby alien expansion,
[00:35:39.700 --> 00:35:42.220]   which is beautiful and fascinating.
[00:35:42.220 --> 00:35:43.060]   Okay.
[00:35:43.060 --> 00:35:46.660]   So there's these giant expanding--
[00:35:46.660 --> 00:35:47.500]   - Spheres.
[00:35:47.500 --> 00:35:49.900]   - Spheres of alien civilizations.
[00:35:49.900 --> 00:35:54.900]   Now, when those spheres collide,
[00:35:54.900 --> 00:36:01.020]   mathematically, it's very likely
[00:36:01.020 --> 00:36:04.020]   that we're not the first collision
[00:36:04.020 --> 00:36:07.380]   of Grabby alien civilizations,
[00:36:07.380 --> 00:36:09.540]   I suppose is one way to say it.
[00:36:09.540 --> 00:36:12.620]   So there's, like, the first time the spheres touch each other
[00:36:12.620 --> 00:36:14.620]   and recognize each other, they meet.
[00:36:14.620 --> 00:36:19.060]   They recognize each other first before they meet.
[00:36:19.060 --> 00:36:20.940]   - They see each other coming.
[00:36:20.940 --> 00:36:21.900]   - They see each other coming.
[00:36:21.900 --> 00:36:23.780]   And then, so there's a bunch of them,
[00:36:23.780 --> 00:36:25.300]   there's a combinatorial thing
[00:36:25.300 --> 00:36:26.860]   where they start seeing each other coming,
[00:36:26.860 --> 00:36:28.220]   and then there's a third neighbor,
[00:36:28.220 --> 00:36:29.140]   it's like, what the hell?
[00:36:29.140 --> 00:36:30.420]   And then there's a fourth one.
[00:36:30.420 --> 00:36:33.580]   Okay, so what does that, you think, look like?
[00:36:33.580 --> 00:36:37.300]   What lessons, from human nature,
[00:36:37.300 --> 00:36:39.140]   that's the only data we have?
[00:36:39.140 --> 00:36:41.060]   What can you draw--
[00:36:41.060 --> 00:36:43.660]   - So the story of the history of the universe here
[00:36:43.660 --> 00:36:45.700]   is what I would call a living cosmology.
[00:36:45.700 --> 00:36:49.420]   So what I'm excited about, in part, by this model,
[00:36:49.420 --> 00:36:52.020]   is that it lets us tell a story of cosmology
[00:36:52.020 --> 00:36:54.820]   where there are actors who have agendas.
[00:36:54.820 --> 00:36:57.860]   So most ancient peoples, they had cosmologies,
[00:36:57.860 --> 00:36:59.940]   the stories they told about where the universe came from
[00:36:59.940 --> 00:37:01.540]   and where it's going and what's happening out there.
[00:37:01.540 --> 00:37:03.340]   And their stories, they like to have agents
[00:37:03.340 --> 00:37:05.740]   and actors, gods or something, out there doing things.
[00:37:05.740 --> 00:37:10.740]   And lately, our favorite cosmology is dead, kind of boring.
[00:37:10.740 --> 00:37:13.340]   We're the only activity we know about our sea
[00:37:13.340 --> 00:37:15.780]   and everything else just looks dead and empty.
[00:37:15.780 --> 00:37:19.820]   But this is now telling us, no, that's not quite right.
[00:37:19.820 --> 00:37:21.300]   At the moment, the universe is filling up,
[00:37:21.300 --> 00:37:24.820]   and in a few billion years, it'll be all full.
[00:37:24.820 --> 00:37:26.820]   And from then on, the history of the universe
[00:37:26.820 --> 00:37:29.980]   will be the universe full of aliens.
[00:37:29.980 --> 00:37:32.620]   - Yeah, so that's a really good reminder,
[00:37:32.620 --> 00:37:35.300]   a really good way to think about cosmologies.
[00:37:35.300 --> 00:37:37.660]   We're surrounded by a vast darkness,
[00:37:37.660 --> 00:37:40.740]   and we don't know what's going on in that darkness
[00:37:40.740 --> 00:37:45.660]   until the light from whatever generate lights arrives here.
[00:37:45.660 --> 00:37:48.220]   So we kind of, yeah, we look up at the sky,
[00:37:48.220 --> 00:37:50.260]   okay, there's stars, oh, they're pretty.
[00:37:50.260 --> 00:37:55.260]   But you don't think about the giant expanding spheres
[00:37:55.260 --> 00:37:56.460]   of aliens. (laughs)
[00:37:56.460 --> 00:37:57.980]   - Right, 'cause you don't see them.
[00:37:57.980 --> 00:38:00.820]   But now our date, looking at the clock,
[00:38:00.820 --> 00:38:02.180]   if you're clever, the clock tells you.
[00:38:02.180 --> 00:38:04.020]   - So I like the analogy with the ancient Greeks.
[00:38:04.020 --> 00:38:07.180]   So you might think that an ancient Greek
[00:38:07.180 --> 00:38:09.540]   staring at the universe couldn't possibly tell
[00:38:09.540 --> 00:38:12.320]   how far away the sun was, or how far away the moon is,
[00:38:12.320 --> 00:38:13.900]   or how big the earth is.
[00:38:13.900 --> 00:38:16.300]   That all you can see is just big things in the sky
[00:38:16.300 --> 00:38:17.140]   you can't tell.
[00:38:17.140 --> 00:38:18.220]   But they were clever enough, actually,
[00:38:18.220 --> 00:38:20.540]   to be able to figure out the size of the earth
[00:38:20.540 --> 00:38:22.020]   and the distance to the moon and the sun
[00:38:22.020 --> 00:38:24.580]   and the size of the moon and sun.
[00:38:24.580 --> 00:38:26.820]   That is, they could figure those things out, actually,
[00:38:26.820 --> 00:38:27.820]   by being clever enough.
[00:38:27.820 --> 00:38:29.860]   And so similarly, we can actually figure out
[00:38:29.860 --> 00:38:31.700]   where are the aliens out there in space-time
[00:38:31.700 --> 00:38:33.900]   by being clever about the few things we can see,
[00:38:33.900 --> 00:38:35.780]   one of which is our current date.
[00:38:35.780 --> 00:38:38.740]   And so now that you have this living cosmology,
[00:38:38.740 --> 00:38:41.600]   we can tell the story that the universe starts out empty,
[00:38:41.600 --> 00:38:44.140]   and then at some point, things like us appear,
[00:38:44.140 --> 00:38:46.860]   very primitive, and then some of those
[00:38:46.860 --> 00:38:49.100]   stop being quiet and expand.
[00:38:49.100 --> 00:38:51.000]   And then for a few billion years, they expand,
[00:38:51.000 --> 00:38:52.220]   and then they meet each other.
[00:38:52.220 --> 00:38:54.700]   And then for the next 100 billion years,
[00:38:54.700 --> 00:38:56.940]   they commune with each other.
[00:38:56.940 --> 00:38:59.180]   That is, the usual models of cosmology say
[00:38:59.180 --> 00:39:03.180]   that in roughly 100, 150 billion years,
[00:39:03.180 --> 00:39:05.200]   the expansion of the universe will happen so much
[00:39:05.200 --> 00:39:08.260]   that all you'll have left is some galaxy clusters
[00:39:08.260 --> 00:39:10.420]   and that are sort of disconnected from each other.
[00:39:10.420 --> 00:39:15.420]   But before then, for the next 100 billion years,
[00:39:15.420 --> 00:39:17.620]   they will interact.
[00:39:17.620 --> 00:39:18.880]   There will be this community
[00:39:18.880 --> 00:39:21.100]   of all the grabby alien civilizations,
[00:39:21.100 --> 00:39:22.460]   and each one of them will hear about
[00:39:22.460 --> 00:39:24.980]   and even meet thousands of others.
[00:39:24.980 --> 00:39:28.220]   And we might hope to join them someday
[00:39:28.220 --> 00:39:29.720]   and become part of that community.
[00:39:29.720 --> 00:39:32.100]   That's an interesting thing to aspire to.
[00:39:32.100 --> 00:39:35.960]   - Yes, interesting is an interesting word.
[00:39:35.960 --> 00:39:40.960]   Is the universe of alien civilizations defined by war
[00:39:40.960 --> 00:39:45.960]   as much or more than war-defined human history?
[00:39:45.960 --> 00:39:52.180]   I would say it's defined by competition,
[00:39:52.180 --> 00:39:56.260]   and then the question is how much competition implies war.
[00:39:57.100 --> 00:40:02.100]   So up until recently, competition defined life on Earth.
[00:40:02.100 --> 00:40:08.580]   Competition between species and organisms and among humans,
[00:40:08.580 --> 00:40:11.580]   competitions among individuals and communities,
[00:40:11.580 --> 00:40:13.640]   and that competition often took the form of war
[00:40:13.640 --> 00:40:15.200]   in the last 10,000 years.
[00:40:15.200 --> 00:40:20.380]   Many people now are hoping or even expecting
[00:40:20.380 --> 00:40:23.880]   to sort of suppress and end competition in human affairs.
[00:40:25.340 --> 00:40:27.340]   They regulate business competition,
[00:40:27.340 --> 00:40:30.460]   they prevent military competition,
[00:40:30.460 --> 00:40:33.260]   and that's a future I think a lot of people
[00:40:33.260 --> 00:40:35.420]   will like to continue and strengthen.
[00:40:35.420 --> 00:40:37.020]   People will like to have something close
[00:40:37.020 --> 00:40:39.420]   to world government or world governance
[00:40:39.420 --> 00:40:40.920]   or at least a world community,
[00:40:40.920 --> 00:40:42.780]   and they will like to suppress war
[00:40:42.780 --> 00:40:45.680]   and any forms of business and personal competition
[00:40:45.680 --> 00:40:46.980]   over the coming centuries.
[00:40:46.980 --> 00:40:50.860]   And they may like that so much
[00:40:50.860 --> 00:40:53.180]   that they prevent interstellar colonization,
[00:40:53.180 --> 00:40:55.160]   which would become the end of that era.
[00:40:55.160 --> 00:40:56.780]   That is, interstellar colonization
[00:40:56.780 --> 00:40:59.460]   would just return severe competition
[00:40:59.460 --> 00:41:01.580]   to human or our descendant affairs.
[00:41:01.580 --> 00:41:04.060]   And many civilizations may prefer that,
[00:41:04.060 --> 00:41:05.460]   and ours may prefer that.
[00:41:05.460 --> 00:41:09.100]   But if they choose to allow interstellar colonization,
[00:41:09.100 --> 00:41:11.640]   they will have chosen to allow competition
[00:41:11.640 --> 00:41:12.900]   to return with great force.
[00:41:12.900 --> 00:41:14.700]   That is, there's really not much of a way
[00:41:14.700 --> 00:41:17.380]   to centrally govern a rapidly expanding
[00:41:17.380 --> 00:41:19.220]   sphere of civilization.
[00:41:19.220 --> 00:41:22.600]   And so I think one of the most solid things
[00:41:22.600 --> 00:41:23.940]   we can predict about Gravelians
[00:41:23.940 --> 00:41:26.700]   is they have accepted competition,
[00:41:26.700 --> 00:41:28.900]   and they have internal competition,
[00:41:28.900 --> 00:41:31.840]   and therefore they have the potential for competition
[00:41:31.840 --> 00:41:33.360]   when they meet each other at the borders.
[00:41:33.360 --> 00:41:36.300]   But whether that's military competition
[00:41:36.300 --> 00:41:38.160]   is more of an open question.
[00:41:38.160 --> 00:41:41.440]   - So military meaning physically destructive.
[00:41:41.440 --> 00:41:42.280]   - Right.
[00:41:42.280 --> 00:41:47.700]   - So there's a lot to say there.
[00:41:47.700 --> 00:41:51.040]   So one idea that you kind of proposed
[00:41:51.040 --> 00:41:56.040]   is progress might be maximized through competition,
[00:41:56.040 --> 00:42:00.640]   through some kind of healthy competition,
[00:42:00.640 --> 00:42:02.360]   some definition of healthy.
[00:42:02.360 --> 00:42:06.480]   So like constructive, not destructive competition.
[00:42:06.480 --> 00:42:09.160]   So like we would likely,
[00:42:09.160 --> 00:42:12.720]   Graby alien civilizations would be likely defined
[00:42:12.720 --> 00:42:15.200]   by competition 'cause they can expand faster.
[00:42:15.200 --> 00:42:18.380]   Because competition allows innovation
[00:42:18.380 --> 00:42:19.840]   and sort of the battle of ideas.
[00:42:19.840 --> 00:42:22.080]   - The way I would take the logic is to say,
[00:42:22.080 --> 00:42:27.520]   competition just happens if you can't coordinate to stop it.
[00:42:27.520 --> 00:42:30.080]   And you probably can't coordinate to stop it
[00:42:30.080 --> 00:42:32.320]   in an expanding interstellar wave.
[00:42:32.320 --> 00:42:37.320]   So competition is a fundamental force in the universe.
[00:42:37.320 --> 00:42:39.100]   - It has been so far,
[00:42:39.100 --> 00:42:42.080]   and it would be within an expanding
[00:42:42.080 --> 00:42:43.260]   Graby alien civilization.
[00:42:43.260 --> 00:42:47.240]   But we today have the chance, many people think and hope,
[00:42:47.240 --> 00:42:50.120]   of greatly controlling and limiting competition
[00:42:50.120 --> 00:42:52.880]   within our civilization for a while.
[00:42:52.880 --> 00:42:55.640]   And that's an interesting choice.
[00:42:55.640 --> 00:42:59.680]   Whether to allow competition to sort of regain
[00:42:59.680 --> 00:43:03.940]   its full force or whether to suppress and manage it.
[00:43:03.940 --> 00:43:08.940]   - Well, one of the open questions that has been raised
[00:43:08.940 --> 00:43:13.460]   in the past less than 100 years
[00:43:13.460 --> 00:43:18.380]   is whether our desire to lessen the destructive nature
[00:43:18.380 --> 00:43:22.300]   of competition or the destructive kind of competition
[00:43:22.300 --> 00:43:27.200]   will be outpaced by the destructive power of our weapons.
[00:43:27.200 --> 00:43:33.720]   Sort of if nuclear weapons and weapons of that kind
[00:43:33.720 --> 00:43:40.660]   become more destructive than our desire for peace,
[00:43:40.660 --> 00:43:43.340]   then all it takes is one asshole at the party
[00:43:43.340 --> 00:43:45.580]   to ruin the party.
[00:43:45.580 --> 00:43:48.140]   - It takes one asshole to make a delay,
[00:43:48.140 --> 00:43:49.860]   but not that much of a delay
[00:43:49.860 --> 00:43:52.500]   on the cosmological scales we're talking about.
[00:43:52.500 --> 00:43:56.200]   So even a vast nuclear war,
[00:43:56.200 --> 00:43:58.620]   if it happened here right now on Earth,
[00:43:58.620 --> 00:44:01.880]   it would not kill all humans.
[00:44:01.880 --> 00:44:05.380]   It certainly wouldn't kill all life.
[00:44:05.380 --> 00:44:07.980]   And so human civilization would return
[00:44:07.980 --> 00:44:10.220]   within 100,000 years.
[00:44:10.220 --> 00:44:14.260]   So all the history of atrocities,
[00:44:14.260 --> 00:44:18.260]   and if you look at the Black Plague,
[00:44:18.260 --> 00:44:26.540]   which is not human-caused atrocities or whatever.
[00:44:26.540 --> 00:44:29.300]   - There are a lot of military atrocities in history,
[00:44:29.300 --> 00:44:30.660]   absolutely. - In the 20th century.
[00:44:30.660 --> 00:44:35.660]   Those are, those challenges to think about human nature,
[00:44:36.620 --> 00:44:40.380]   but the cosmic scale of time and space,
[00:44:40.380 --> 00:44:44.580]   they do not stop the human spirit, essentially.
[00:44:44.580 --> 00:44:46.980]   The humanity goes on.
[00:44:46.980 --> 00:44:49.500]   Through all the atrocities, it goes on.
[00:44:49.500 --> 00:44:50.640]   Life goes on. - Most likely.
[00:44:50.640 --> 00:44:53.900]   So even a nuclear war isn't enough to destroy us
[00:44:53.900 --> 00:44:57.340]   or to stop our potential from expanding,
[00:44:57.340 --> 00:45:02.340]   but we could institute a regime of global governance
[00:45:02.340 --> 00:45:04.060]   that limited competition,
[00:45:04.060 --> 00:45:06.940]   including military and business competition of sorts,
[00:45:06.940 --> 00:45:09.660]   and that could prevent our expansion.
[00:45:09.660 --> 00:45:12.660]   Of course, to play devil's advocate,
[00:45:12.660 --> 00:45:20.380]   global governance is centralized power,
[00:45:20.380 --> 00:45:25.540]   and power corrupts, and absolute power corrupts absolutely.
[00:45:25.540 --> 00:45:27.980]   One of the aspects of competition
[00:45:27.980 --> 00:45:30.020]   that's been very productive
[00:45:30.020 --> 00:45:35.020]   is not letting any one person, any one country,
[00:45:35.020 --> 00:45:39.980]   any one center of power become absolutely powerful,
[00:45:39.980 --> 00:45:43.380]   because that's another lesson, is it seems to corrupt.
[00:45:43.380 --> 00:45:45.540]   There's something about ego and the human mind
[00:45:45.540 --> 00:45:47.660]   that seems to be corrupted by power,
[00:45:47.660 --> 00:45:50.360]   so when you say global governance,
[00:45:50.360 --> 00:45:55.360]   that terrifies me more than the possibility of war,
[00:45:55.360 --> 00:45:57.980]   because it's--
[00:45:57.980 --> 00:45:59.940]   - I think people will be less terrified
[00:45:59.940 --> 00:46:01.380]   than you are right now,
[00:46:01.380 --> 00:46:04.300]   and let me try to paint the picture from their point of view.
[00:46:04.300 --> 00:46:05.540]   This isn't my point of view,
[00:46:05.540 --> 00:46:08.100]   but I think it's going to be a widely shared point of view.
[00:46:08.100 --> 00:46:11.020]   - Yes, this is two devil's advocates arguing, two devils.
[00:46:11.020 --> 00:46:15.060]   - Okay, so for the last half century
[00:46:15.060 --> 00:46:16.900]   and into the continuing future,
[00:46:16.900 --> 00:46:21.900]   we actually have had a strong elite global community
[00:46:21.900 --> 00:46:24.980]   that shares a lot of values and beliefs
[00:46:24.980 --> 00:46:29.940]   and has created a lot of convergence in global policy.
[00:46:29.940 --> 00:46:32.180]   So if you look at electromagnetic spectrum
[00:46:32.180 --> 00:46:36.020]   or medical experiments or pandemic policy
[00:46:36.020 --> 00:46:39.980]   or nuclear power energy or regulating airplanes
[00:46:39.980 --> 00:46:41.980]   or just in a wide range of area,
[00:46:41.980 --> 00:46:45.460]   in fact, the world has very similar regulations
[00:46:45.460 --> 00:46:46.940]   and rules everywhere,
[00:46:46.940 --> 00:46:48.220]   and it's not a coincidence
[00:46:48.220 --> 00:46:50.820]   because they are part of a world community
[00:46:50.820 --> 00:46:53.540]   where people get together at places like Davos, et cetera,
[00:46:53.540 --> 00:46:57.260]   where world elites want to be respected
[00:46:57.260 --> 00:46:58.580]   by other world elites,
[00:46:58.580 --> 00:47:02.380]   and they have a convergence of opinion,
[00:47:02.380 --> 00:47:06.180]   and that produces something like global governance,
[00:47:06.180 --> 00:47:08.140]   but without a global center.
[00:47:08.140 --> 00:47:10.020]   And this is sort of what human mobs
[00:47:10.020 --> 00:47:11.780]   or communities have done for a long time.
[00:47:11.780 --> 00:47:14.740]   That is, humans can coordinate together on shared behavior
[00:47:14.740 --> 00:47:18.780]   without a center by having gossip and reputation
[00:47:18.780 --> 00:47:21.020]   within a community of elites.
[00:47:21.020 --> 00:47:22.740]   And that is what we have been doing
[00:47:22.740 --> 00:47:24.860]   and are likely to do a lot more of.
[00:47:24.860 --> 00:47:27.980]   So for example, one of the things that's happening,
[00:47:27.980 --> 00:47:29.180]   say, with the war in Ukraine
[00:47:29.180 --> 00:47:31.460]   is that this world community of elites
[00:47:31.460 --> 00:47:35.020]   has decided that they disapprove of the Russian invasion
[00:47:35.020 --> 00:47:38.180]   and they are coordinating to pull resources together
[00:47:38.180 --> 00:47:40.540]   from all around the world in order to oppose it,
[00:47:40.540 --> 00:47:42.140]   and they are proud of that,
[00:47:42.140 --> 00:47:45.020]   sharing that opinion in there,
[00:47:45.020 --> 00:47:47.700]   and they feel that they are morally justified
[00:47:47.700 --> 00:47:49.900]   in their stance there.
[00:47:49.900 --> 00:47:53.020]   And that's the kind of event
[00:47:53.020 --> 00:47:55.500]   that actually brings world elite communities together,
[00:47:55.500 --> 00:47:59.660]   where they come together and they push a particular policy
[00:47:59.660 --> 00:48:02.580]   and position that they share and that they achieve successes.
[00:48:02.580 --> 00:48:05.500]   And the same sort of passion animates global elites
[00:48:05.500 --> 00:48:07.620]   with respect to, say, global warming,
[00:48:07.620 --> 00:48:09.820]   or global poverty, and other sorts of things.
[00:48:09.820 --> 00:48:12.420]   And they are, in fact, making progress
[00:48:12.420 --> 00:48:13.660]   on those sorts of things
[00:48:13.660 --> 00:48:17.380]   through shared global community of elites.
[00:48:18.500 --> 00:48:19.700]   And in some sense,
[00:48:19.700 --> 00:48:22.220]   they are slowly walking toward global governance,
[00:48:22.220 --> 00:48:25.180]   slowly strengthening various world institutions
[00:48:25.180 --> 00:48:27.980]   of governance, but cautiously, carefully,
[00:48:27.980 --> 00:48:29.700]   watching out for the possibility
[00:48:29.700 --> 00:48:32.000]   of a single power that might corrupt it.
[00:48:32.000 --> 00:48:35.180]   I think a lot of people over the coming centuries
[00:48:35.180 --> 00:48:37.080]   will look at that history and like it.
[00:48:37.080 --> 00:48:41.260]   - It's an interesting thought,
[00:48:41.260 --> 00:48:44.360]   and thank you for playing that devil's advocate there.
[00:48:45.820 --> 00:48:50.100]   But I think the elites too easily lose touch
[00:48:50.100 --> 00:48:55.900]   of the morals that the best of human nature
[00:48:55.900 --> 00:48:57.260]   and power corrupts.
[00:48:57.260 --> 00:48:58.100]   - Sure, but-- - And everything you just said.
[00:48:58.100 --> 00:49:01.140]   - If their view is the one that determines what happens,
[00:49:01.140 --> 00:49:04.220]   their view may still end up there,
[00:49:04.220 --> 00:49:06.500]   even if you or I might criticize it
[00:49:06.500 --> 00:49:07.780]   from that point of view, so.
[00:49:07.780 --> 00:49:11.020]   - From a perspective of minimizing human suffering,
[00:49:11.020 --> 00:49:15.820]   elites can use topics of the war in Ukraine
[00:49:15.820 --> 00:49:19.700]   and climate change and all of those things
[00:49:19.700 --> 00:49:23.820]   to sell an idea to the world
[00:49:23.820 --> 00:49:30.000]   and with disregard to the amount of suffering it causes,
[00:49:30.000 --> 00:49:32.220]   their actual actions.
[00:49:32.220 --> 00:49:34.500]   So like you can tell all kinds of narratives,
[00:49:34.500 --> 00:49:36.540]   that's the way propaganda works.
[00:49:36.540 --> 00:49:39.960]   Hitler really sold the idea
[00:49:39.960 --> 00:49:42.300]   that everything Germany is doing is either,
[00:49:42.300 --> 00:49:44.420]   it's the victim, is defending itself
[00:49:44.420 --> 00:49:46.320]   against the cruelty of the world,
[00:49:46.320 --> 00:49:50.320]   and it's actually trying to bring about a better world.
[00:49:50.320 --> 00:49:54.420]   So every power center thinks they're doing good.
[00:49:54.420 --> 00:49:59.420]   And so this is the positive of competition,
[00:49:59.420 --> 00:50:02.260]   of having multiple power centers.
[00:50:02.260 --> 00:50:04.380]   This kind of gathering of elites
[00:50:04.380 --> 00:50:08.520]   makes me very, very, very nervous.
[00:50:08.520 --> 00:50:13.520]   The dinners, the meetings in the closed rooms.
[00:50:13.520 --> 00:50:14.820]   I don't know.
[00:50:14.820 --> 00:50:19.880]   But remember we talked about separating our cold analysis
[00:50:19.880 --> 00:50:22.580]   of what's likely or possible from what we prefer,
[00:50:22.580 --> 00:50:25.020]   and so this isn't exactly enough time for that.
[00:50:25.020 --> 00:50:28.980]   We might say, I would recommend we don't go this route
[00:50:28.980 --> 00:50:30.900]   of a strong world governance,
[00:50:30.900 --> 00:50:35.100]   and because I would say it'll preclude this possibility
[00:50:35.100 --> 00:50:36.360]   of becoming grabby aliens,
[00:50:36.360 --> 00:50:39.500]   of filling the nearest million galaxies
[00:50:39.500 --> 00:50:43.780]   for the next billion years with vast amounts of activity,
[00:50:43.780 --> 00:50:47.080]   and interest, and value of life out there.
[00:50:47.080 --> 00:50:49.020]   That's the thing we would lose
[00:50:49.020 --> 00:50:51.300]   by deciding that we wouldn't expand,
[00:50:51.300 --> 00:50:52.480]   that we would stay here
[00:50:52.480 --> 00:50:55.980]   and keep our comfortable shared governance.
[00:50:55.980 --> 00:51:00.980]   - So you, wait, you think that global governance
[00:51:04.900 --> 00:51:07.060]   makes it more likely or less likely
[00:51:07.060 --> 00:51:08.780]   that we expand out into the universe?
[00:51:08.780 --> 00:51:10.140]   - Less.
[00:51:10.140 --> 00:51:10.980]   - So okay.
[00:51:10.980 --> 00:51:12.500]   - This is the key point.
[00:51:12.500 --> 00:51:15.560]   - Great, right, so screw the elites.
[00:51:15.560 --> 00:51:16.400]   (laughing)
[00:51:16.400 --> 00:51:17.500]   - Right. - So if we want to,
[00:51:17.500 --> 00:51:19.400]   wait, do we want to expand?
[00:51:19.400 --> 00:51:23.360]   - So again, I want to separate my neutral analysis
[00:51:23.360 --> 00:51:25.940]   from my evaluation and say,
[00:51:25.940 --> 00:51:28.260]   first of all, I have an analysis that tells us
[00:51:28.260 --> 00:51:30.340]   this is a key choice that we will face,
[00:51:30.340 --> 00:51:31.260]   and that it's a key choice
[00:51:31.260 --> 00:51:33.220]   other aliens have faced out there.
[00:51:33.220 --> 00:51:34.660]   And it could be that only one in 10
[00:51:34.660 --> 00:51:37.660]   or one in 100 civilizations chooses to expand,
[00:51:37.660 --> 00:51:39.420]   and the rest of them stay quiet.
[00:51:39.420 --> 00:51:40.860]   And that's how it goes out there.
[00:51:40.860 --> 00:51:42.720]   And we face that choice too.
[00:51:42.720 --> 00:51:46.740]   And it'll happen sometime in the next 10 million years,
[00:51:46.740 --> 00:51:48.180]   maybe the next thousand.
[00:51:48.180 --> 00:51:50.660]   But the key thing to notice from our point of view
[00:51:50.660 --> 00:51:54.380]   is that even though you might like our global governance,
[00:51:54.380 --> 00:51:56.140]   you might like the fact that we've come together,
[00:51:56.140 --> 00:51:58.040]   we no longer have massive wars,
[00:51:58.040 --> 00:52:00.460]   and we no longer have destructive competition,
[00:52:00.460 --> 00:52:04.200]   and that we could continue that.
[00:52:04.200 --> 00:52:05.820]   The cost of continuing that would be
[00:52:05.820 --> 00:52:07.980]   to prevent interstellar colonization.
[00:52:07.980 --> 00:52:10.300]   That is, once you allow interstellar colonization,
[00:52:10.300 --> 00:52:12.620]   then you've lost control of those colonies,
[00:52:12.620 --> 00:52:14.420]   and whatever they change into,
[00:52:14.420 --> 00:52:18.040]   they could come back here and compete with you back here
[00:52:18.040 --> 00:52:19.980]   as a result of having lost control.
[00:52:19.980 --> 00:52:23.700]   And I think if people value that global governance
[00:52:23.700 --> 00:52:26.420]   and global community and regulation
[00:52:26.420 --> 00:52:28.120]   and all the things it can do enough,
[00:52:28.120 --> 00:52:31.660]   they would then want to prevent interstellar colonization.
[00:52:31.660 --> 00:52:33.860]   - I want to have a conversation with those people.
[00:52:33.860 --> 00:52:38.860]   I believe that both for humanity, for the good of humanity,
[00:52:38.860 --> 00:52:41.060]   for what I believe is good in humanity,
[00:52:41.060 --> 00:52:46.060]   and for expansion, exploration, innovation,
[00:52:46.060 --> 00:52:50.000]   distributing the centers of power is very beneficial.
[00:52:50.000 --> 00:52:51.420]   So this whole meeting of elites,
[00:52:51.420 --> 00:52:55.040]   and I've been very fortunate to meet
[00:52:55.040 --> 00:52:56.780]   quite a large number of elites,
[00:52:56.780 --> 00:53:00.200]   they make me nervous.
[00:53:00.200 --> 00:53:05.200]   Because it's easy to lose touch of reality.
[00:53:05.200 --> 00:53:09.220]   I'm nervous about that in myself,
[00:53:09.220 --> 00:53:11.680]   to make sure that you never lose touch
[00:53:11.680 --> 00:53:17.600]   as you get sort of older, wiser,
[00:53:17.600 --> 00:53:20.600]   you know how you generally get disrespectful of kids,
[00:53:20.600 --> 00:53:21.920]   kids these days.
[00:53:21.920 --> 00:53:23.240]   No, the kids are--
[00:53:23.240 --> 00:53:25.920]   - Okay, but I think you should hear a stronger case
[00:53:25.920 --> 00:53:27.880]   for their position, so I'm gonna play that.
[00:53:27.880 --> 00:53:29.120]   - For the elites.
[00:53:29.120 --> 00:53:33.680]   - Yes, well, for the limiting of expansion,
[00:53:33.680 --> 00:53:37.040]   and for the regulation of behavior.
[00:53:37.040 --> 00:53:39.240]   - Just, okay, can I linger on that?
[00:53:39.240 --> 00:53:41.740]   So you're saying those two are connected.
[00:53:41.740 --> 00:53:45.040]   So the human civilization and alien civilizations
[00:53:45.040 --> 00:53:48.840]   come to a crossroads, they have to decide,
[00:53:48.840 --> 00:53:51.240]   do we want to expand or not?
[00:53:51.240 --> 00:53:54.880]   And connected to that, do we want to give a lot of power
[00:53:54.880 --> 00:53:57.680]   to a central elite, or do we want to
[00:53:58.680 --> 00:54:02.240]   distribute the power centers,
[00:54:02.240 --> 00:54:05.600]   which is naturally connected to the expansion?
[00:54:05.600 --> 00:54:08.100]   When you expand, you distribute the power.
[00:54:08.100 --> 00:54:11.280]   - If, say, over the next thousand years,
[00:54:11.280 --> 00:54:13.200]   we fill up the solar system, right?
[00:54:13.200 --> 00:54:15.440]   We go out from Earth and we colonize Mars
[00:54:15.440 --> 00:54:17.440]   and we change a lot of things.
[00:54:17.440 --> 00:54:20.320]   Within a solar system, still, everything is within reach.
[00:54:20.320 --> 00:54:22.800]   That is, if there's a rebellious colony around Neptune,
[00:54:22.800 --> 00:54:24.480]   you can throw rocks at it and smash it,
[00:54:24.480 --> 00:54:26.500]   and teach them discipline, okay?
[00:54:27.480 --> 00:54:29.080]   - How does that work for the British?
[00:54:29.080 --> 00:54:32.100]   - A central control over the solar system is feasible.
[00:54:32.100 --> 00:54:35.100]   But once you let it escape the solar system,
[00:54:35.100 --> 00:54:36.040]   it's no longer feasible.
[00:54:36.040 --> 00:54:37.760]   But if you have a solar system
[00:54:37.760 --> 00:54:39.120]   that doesn't have a central control,
[00:54:39.120 --> 00:54:41.720]   maybe broken into a thousand different political units
[00:54:41.720 --> 00:54:45.400]   in the solar system, then any one part of that
[00:54:45.400 --> 00:54:48.000]   that allows interstellar colonization, and it happens.
[00:54:48.000 --> 00:54:50.720]   That is, interstellar colonization happens
[00:54:50.720 --> 00:54:53.160]   when only one party chooses to do it,
[00:54:53.160 --> 00:54:55.920]   and is able to do it, and that's what it, therefore.
[00:54:55.920 --> 00:54:58.760]   So we can just say, in a world of competition,
[00:54:58.760 --> 00:55:01.320]   if interstellar colonization is possible, it will happen,
[00:55:01.320 --> 00:55:02.800]   and then competition will continue.
[00:55:02.800 --> 00:55:04.720]   And that will sort of ensure the continuation
[00:55:04.720 --> 00:55:07.680]   of competition into the indefinite future.
[00:55:07.680 --> 00:55:10.080]   - And competition, we don't know,
[00:55:10.080 --> 00:55:11.960]   but competition can take violent forms,
[00:55:11.960 --> 00:55:13.480]   or can take productive forms. - In many forms.
[00:55:13.480 --> 00:55:15.440]   And the case I was going to make is that,
[00:55:15.440 --> 00:55:17.400]   I think one of the things that most scares people
[00:55:17.400 --> 00:55:19.800]   about competition is not just that it creates
[00:55:19.800 --> 00:55:22.880]   holocausts and death on massive scales,
[00:55:22.880 --> 00:55:27.320]   is that it's likely to change who we are,
[00:55:27.320 --> 00:55:29.880]   and what we value. - Yes.
[00:55:29.880 --> 00:55:32.800]   So this is the other thing with power.
[00:55:32.800 --> 00:55:37.280]   As we grow, as human civilization grows,
[00:55:37.280 --> 00:55:41.600]   becomes multi-planetary, multi-solar system, potentially,
[00:55:41.600 --> 00:55:43.600]   how does that change us, do you think?
[00:55:43.600 --> 00:55:45.800]   - I think the more you think about it,
[00:55:45.800 --> 00:55:48.040]   the more you realize it can change us a lot.
[00:55:48.040 --> 00:55:49.480]   So, first of all, I would say--
[00:55:49.480 --> 00:55:50.800]   - This is pretty dark, by the way.
[00:55:50.800 --> 00:55:53.640]   - Well, it's-- - It's just honest.
[00:55:53.640 --> 00:55:55.040]   - Right, well, I'm trying to get you there.
[00:55:55.040 --> 00:55:56.040]   I think the first thing you should say,
[00:55:56.040 --> 00:55:58.040]   if you look at history, just human history
[00:55:58.040 --> 00:55:59.920]   over the last 10,000 years,
[00:55:59.920 --> 00:56:02.320]   if you really understood what people were like
[00:56:02.320 --> 00:56:03.640]   a long time ago, you'd realize
[00:56:03.640 --> 00:56:05.440]   they were really quite different.
[00:56:05.440 --> 00:56:08.000]   Ancient cultures created people
[00:56:08.000 --> 00:56:08.960]   who were really quite different.
[00:56:08.960 --> 00:56:11.680]   Most historical fiction lies to you about that.
[00:56:11.680 --> 00:56:14.640]   It often offers you modern characters in an ancient world.
[00:56:14.640 --> 00:56:16.960]   But if you actually study history,
[00:56:16.960 --> 00:56:18.560]   you will see just how different they were,
[00:56:18.560 --> 00:56:20.640]   and how differently they thought.
[00:56:20.640 --> 00:56:23.600]   And they've changed a lot, many times,
[00:56:23.600 --> 00:56:25.240]   and they've changed a lot across time.
[00:56:25.240 --> 00:56:28.160]   So I think the most obvious prediction about the future is,
[00:56:28.160 --> 00:56:30.440]   even if you only have the mechanisms of change
[00:56:30.440 --> 00:56:32.360]   we've seen in the past, you should still expect
[00:56:32.360 --> 00:56:33.960]   a lot of change in the future.
[00:56:33.960 --> 00:56:36.480]   But we have a lot bigger mechanisms for change
[00:56:36.480 --> 00:56:39.040]   in the future than we had in the past.
[00:56:39.040 --> 00:56:42.560]   So, I have this book called "The Age of M,"
[00:56:42.560 --> 00:56:44.840]   "Work, Love, and Life," and "Robots Rule the Earth,"
[00:56:44.840 --> 00:56:46.160]   and it's about what happens
[00:56:46.160 --> 00:56:48.160]   if brain emulations become possible.
[00:56:48.160 --> 00:56:49.840]   So a brain emulation is where you take
[00:56:49.840 --> 00:56:52.160]   a actual human brain and you scan it
[00:56:52.160 --> 00:56:53.880]   and find spatial and chemical detail
[00:56:53.880 --> 00:56:57.560]   to create a computer simulation of that brain.
[00:56:57.560 --> 00:57:00.320]   And then those computer simulations of brains
[00:57:00.320 --> 00:57:02.280]   are basically citizens in a new world.
[00:57:02.280 --> 00:57:04.720]   They work and they vote and they fall in love
[00:57:04.720 --> 00:57:06.960]   and they get mad and they lie to each other.
[00:57:06.960 --> 00:57:08.200]   And this is a whole new world.
[00:57:08.200 --> 00:57:10.480]   And my book is about analyzing how that world
[00:57:10.480 --> 00:57:12.760]   is different than our world,
[00:57:12.760 --> 00:57:15.800]   basically using competition as my key lever of analysis.
[00:57:15.800 --> 00:57:18.040]   That is, if that world remains competitive,
[00:57:18.040 --> 00:57:20.600]   then I can figure out how they change in that world,
[00:57:20.600 --> 00:57:22.400]   what they do differently than we do.
[00:57:22.400 --> 00:57:24.920]   And it's very different.
[00:57:24.920 --> 00:57:28.560]   And it's different in ways that are shocking sometimes
[00:57:28.560 --> 00:57:31.600]   to many people and ways some people don't like.
[00:57:31.600 --> 00:57:32.760]   I think it's an okay world,
[00:57:32.760 --> 00:57:34.720]   but I have to admit it's quite different.
[00:57:34.720 --> 00:57:38.080]   And that's just one technology.
[00:57:38.080 --> 00:57:41.120]   If we add dozens more technologies,
[00:57:41.120 --> 00:57:44.600]   changes into the future, we should just expect
[00:57:44.600 --> 00:57:47.520]   it's possible to become very different than who we are.
[00:57:47.520 --> 00:57:49.880]   I mean, in the space of all possible minds,
[00:57:49.880 --> 00:57:52.000]   our minds are a particular architecture,
[00:57:52.000 --> 00:57:55.040]   a particular structure, a particular set of habits,
[00:57:55.040 --> 00:57:59.240]   and they are only one piece in a vast space of possibilities.
[00:57:59.240 --> 00:58:01.960]   The space of possible minds is really huge.
[00:58:01.960 --> 00:58:05.680]   - So yeah, let's linger on the space of possible minds
[00:58:05.680 --> 00:58:09.120]   for a moment just to sort of humble ourselves
[00:58:09.120 --> 00:58:15.640]   how peculiar our peculiarities are.
[00:58:15.640 --> 00:58:20.040]   Like the fact that we like a particular kind of sex
[00:58:20.040 --> 00:58:23.600]   and the fact that we eat food through one hole
[00:58:23.600 --> 00:58:26.920]   and poop through another hole.
[00:58:26.920 --> 00:58:29.960]   And that seems to be a fundamental aspect of life,
[00:58:29.960 --> 00:58:31.480]   is very important to us.
[00:58:31.480 --> 00:58:36.880]   And that life is finite in a certain kind of way.
[00:58:36.880 --> 00:58:38.920]   We have a meat vehicle.
[00:58:38.920 --> 00:58:41.000]   So death is very important to us.
[00:58:41.000 --> 00:58:43.320]   I wonder which aspects are fundamental
[00:58:43.320 --> 00:58:46.320]   or would be common throughout human history
[00:58:46.320 --> 00:58:48.360]   and also throughout, sorry,
[00:58:48.360 --> 00:58:50.520]   throughout history of life on Earth
[00:58:50.520 --> 00:58:53.160]   and throughout other kinds of lives.
[00:58:53.160 --> 00:58:55.040]   Like what is really useful?
[00:58:55.040 --> 00:58:56.200]   You mentioned competition,
[00:58:56.200 --> 00:58:57.800]   seems to be a one fundamental thing.
[00:58:57.800 --> 00:58:59.800]   - I've tried to do analysis
[00:58:59.800 --> 00:59:02.560]   of where our distant descendants might go
[00:59:02.560 --> 00:59:04.200]   in terms of what are robust features
[00:59:04.200 --> 00:59:06.120]   we could predict about our descendants.
[00:59:06.120 --> 00:59:07.800]   So again, I have this analysis
[00:59:07.800 --> 00:59:09.720]   of sort of the next generation,
[00:59:09.720 --> 00:59:11.400]   so the next era after ours.
[00:59:11.400 --> 00:59:12.400]   If you think of human history
[00:59:12.400 --> 00:59:14.720]   as having three eras so far, right?
[00:59:14.720 --> 00:59:15.960]   There was the forager era,
[00:59:15.960 --> 00:59:18.080]   the farmer era and the industry era.
[00:59:18.080 --> 00:59:19.640]   Then my attempt in age of M
[00:59:19.640 --> 00:59:21.360]   is to analyze the next era after that.
[00:59:21.360 --> 00:59:22.280]   And it's very different,
[00:59:22.280 --> 00:59:23.520]   but of course there could be
[00:59:23.520 --> 00:59:25.320]   more and more eras after that.
[00:59:25.320 --> 00:59:27.880]   So, analyzing a particular scenario
[00:59:27.880 --> 00:59:29.200]   and thinking it through is one way
[00:59:29.200 --> 00:59:31.920]   to try to see how different the future could be,
[00:59:31.920 --> 00:59:34.320]   but that doesn't give you some sort of like sense
[00:59:34.320 --> 00:59:35.240]   of what's typical.
[00:59:35.240 --> 00:59:39.120]   But I have tried to analyze what's typical.
[00:59:39.120 --> 00:59:40.840]   And so I have two predictions,
[00:59:40.840 --> 00:59:42.960]   I think I can make pretty solidly.
[00:59:42.960 --> 00:59:45.840]   One thing is that we know at the moment
[00:59:45.840 --> 00:59:49.040]   that humans discount the future rapidly.
[00:59:49.040 --> 00:59:51.280]   So, we discount the future
[00:59:51.280 --> 00:59:53.200]   in terms of caring about consequences
[00:59:53.200 --> 00:59:55.280]   roughly a factor of two per generation.
[00:59:55.280 --> 00:59:57.680]   And there's a solid evolutionary analysis
[00:59:57.680 --> 00:59:59.920]   why sexual creatures would do that.
[00:59:59.920 --> 01:00:01.480]   'Cause basically your descendants
[01:00:01.480 --> 01:00:03.240]   only share half of your genes
[01:00:03.240 --> 01:00:05.280]   and your descendants are a generation away.
[01:00:05.280 --> 01:00:07.560]   - So we only care about our grandchildren.
[01:00:08.560 --> 01:00:11.040]   - Basically that's a factor of four later
[01:00:11.040 --> 01:00:13.360]   because it's later.
[01:00:13.360 --> 01:00:16.200]   So, this actually explains typical interest rates
[01:00:16.200 --> 01:00:17.040]   in the economy,
[01:00:17.040 --> 01:00:19.360]   that is interest rates are greatly influenced
[01:00:19.360 --> 01:00:20.440]   by our discount rates.
[01:00:20.440 --> 01:00:22.680]   And we basically discount the future
[01:00:22.680 --> 01:00:24.360]   by a factor of two per generation.
[01:00:24.360 --> 01:00:29.480]   But that's a side effect of the way
[01:00:29.480 --> 01:00:33.640]   our preferences evolved as sexually selected creatures.
[01:00:33.640 --> 01:00:35.920]   We should expect that in the longer run
[01:00:35.920 --> 01:00:39.320]   creatures will evolve who don't discount the future.
[01:00:39.320 --> 01:00:41.440]   They will care about the long run
[01:00:41.440 --> 01:00:43.560]   and they will therefore not neglect the wrong.
[01:00:43.560 --> 01:00:45.240]   So for example, for things like global warming
[01:00:45.240 --> 01:00:46.840]   or things like that,
[01:00:46.840 --> 01:00:49.520]   at the moment many commenters are sad
[01:00:49.520 --> 01:00:51.520]   that basically ordinary people don't seem to care much,
[01:00:51.520 --> 01:00:53.080]   market prices don't seem to care much
[01:00:53.080 --> 01:00:54.320]   and more ordinary people,
[01:00:54.320 --> 01:00:55.600]   it doesn't really impact them much
[01:00:55.600 --> 01:00:59.240]   because humans don't care much about the long-term future.
[01:00:59.240 --> 01:01:03.360]   But, and futurists find it hard to motivate people
[01:01:03.360 --> 01:01:05.200]   and to engage people about the long-term future
[01:01:05.200 --> 01:01:07.280]   because they just don't care that much.
[01:01:07.280 --> 01:01:09.720]   But that's a side effect of this particular way
[01:01:09.720 --> 01:01:12.920]   that our preferences evolved about the future.
[01:01:12.920 --> 01:01:15.960]   And so in the future, they will neglect the future less.
[01:01:15.960 --> 01:01:17.560]   And that's an interesting thing
[01:01:17.560 --> 01:01:18.800]   that we can predict robustly.
[01:01:18.800 --> 01:01:22.480]   Eventually, maybe a few centuries, maybe longer,
[01:01:22.480 --> 01:01:25.920]   eventually our descendants will care about the future.
[01:01:25.920 --> 01:01:27.960]   - Can you speak to the intuition behind that?
[01:01:27.960 --> 01:01:32.160]   Is it useful to think more about the future?
[01:01:32.160 --> 01:01:35.000]   - Right, if evolution rewards creatures
[01:01:35.000 --> 01:01:37.000]   for having many descendants,
[01:01:37.000 --> 01:01:39.720]   then if you have decisions that influence
[01:01:39.720 --> 01:01:41.480]   how many descendants you have,
[01:01:41.480 --> 01:01:43.640]   then that would be good if you made those decisions.
[01:01:43.640 --> 01:01:45.840]   But in order to do that, you'll have to care about them.
[01:01:45.840 --> 01:01:47.440]   You'll have to care about that future.
[01:01:47.440 --> 01:01:48.760]   - So to push back,
[01:01:48.760 --> 01:01:52.040]   that's if you're trying to maximize the number of descendants
[01:01:52.040 --> 01:01:54.680]   but the nice thing about not caring too much
[01:01:54.680 --> 01:01:56.520]   about the long-term future
[01:01:56.520 --> 01:01:58.640]   is you're more likely to take big risks
[01:01:58.640 --> 01:02:01.000]   or you're less risk-averse.
[01:02:01.000 --> 01:02:04.640]   And it's possible that both evolution
[01:02:04.640 --> 01:02:07.640]   and just life in the universe
[01:02:07.640 --> 01:02:11.560]   rewards the risk-takers.
[01:02:11.560 --> 01:02:13.160]   - Well, we actually have analysis
[01:02:13.160 --> 01:02:16.240]   of the ideal risk preferences too.
[01:02:16.240 --> 01:02:19.760]   So there's literature on ideal preferences
[01:02:19.760 --> 01:02:21.440]   that evolution should promote.
[01:02:21.440 --> 01:02:22.800]   And for example, there's literature
[01:02:22.800 --> 01:02:24.600]   on competing investment funds
[01:02:24.600 --> 01:02:27.320]   and what the managers of those funds should care about
[01:02:27.320 --> 01:02:29.680]   in terms of various kinds of risks
[01:02:29.680 --> 01:02:30.960]   and in terms of discounting.
[01:02:30.960 --> 01:02:33.440]   And so managers of investment funds
[01:02:33.440 --> 01:02:37.440]   should basically have logarithmic risk,
[01:02:37.440 --> 01:02:41.680]   i.e. in shared risk, in correlated risk,
[01:02:41.680 --> 01:02:46.040]   but be very risk-neutral with respect to uncorrelated risk.
[01:02:46.040 --> 01:02:50.400]   So that's a feature that's predicted to happen
[01:02:50.400 --> 01:02:54.240]   about individual personal choices in biology
[01:02:54.240 --> 01:02:55.280]   and also for investment funds.
[01:02:55.280 --> 01:02:56.120]   So that's other things.
[01:02:56.120 --> 01:02:58.240]   That's also something we can say about the long run.
[01:02:58.240 --> 01:03:00.840]   - What's correlated and uncorrelated risk?
[01:03:00.840 --> 01:03:03.480]   - If there's something that would affect
[01:03:03.480 --> 01:03:04.840]   all of your descendants,
[01:03:04.840 --> 01:03:08.440]   then if you take that risk,
[01:03:08.440 --> 01:03:09.920]   you might have more descendants,
[01:03:09.920 --> 01:03:11.560]   but you might have zero.
[01:03:11.560 --> 01:03:14.920]   And that's just really bad to have zero descendants.
[01:03:14.920 --> 01:03:16.840]   But an uncorrelated risk would be a risk
[01:03:16.840 --> 01:03:18.520]   that some of your descendants would suffer,
[01:03:18.520 --> 01:03:19.600]   but others wouldn't.
[01:03:19.600 --> 01:03:22.040]   And then you have a portfolio of descendants.
[01:03:22.040 --> 01:03:25.840]   And so that portfolio ensures you against problems
[01:03:25.840 --> 01:03:26.680]   with any one of them.
[01:03:26.680 --> 01:03:28.640]   - I like the idea of portfolio of descendants.
[01:03:28.640 --> 01:03:31.440]   And we'll talk about portfolios with your idea
[01:03:31.440 --> 01:03:32.840]   of you briefly mentioned.
[01:03:32.840 --> 01:03:36.600]   We'll return there with M, E-M, the age of E-M.
[01:03:36.600 --> 01:03:39.600]   Work, love, and life when robots rule the earth.
[01:03:39.600 --> 01:03:41.680]   E-M, by the way, is emulated minds.
[01:03:41.680 --> 01:03:44.360]   So this, one of the--
[01:03:44.360 --> 01:03:46.320]   - M is short for emulations.
[01:03:46.320 --> 01:03:47.720]   - M is short for emulations,
[01:03:47.720 --> 01:03:50.160]   and it's kind of an idea of how we might
[01:03:50.160 --> 01:03:53.480]   create artificial minds, artificial copies of minds,
[01:03:53.480 --> 01:03:57.040]   or human-like intelligences.
[01:03:57.040 --> 01:03:59.320]   - I have another dramatic prediction I can make
[01:03:59.320 --> 01:04:00.720]   about long-term preferences.
[01:04:00.720 --> 01:04:01.560]   - Yes.
[01:04:01.560 --> 01:04:05.080]   - Which is at the moment, we reproduce as the result
[01:04:05.080 --> 01:04:07.000]   of a hodgepodge of preferences
[01:04:07.000 --> 01:04:09.120]   that aren't very well integrated,
[01:04:09.120 --> 01:04:11.040]   but sort of in our ancestral environment
[01:04:11.040 --> 01:04:12.360]   induced us to reproduce.
[01:04:12.360 --> 01:04:14.960]   So we have preferences over being sleepy,
[01:04:14.960 --> 01:04:17.560]   and hungry, and thirsty, and wanting to have sex,
[01:04:17.560 --> 01:04:21.160]   and wanting to be excitement, et cetera, right?
[01:04:21.160 --> 01:04:23.120]   And so in our ancestral environment,
[01:04:23.120 --> 01:04:25.920]   the packages of preferences that we evolved to have
[01:04:25.920 --> 01:04:29.240]   did induce us to have more descendants.
[01:04:29.240 --> 01:04:31.160]   That's why we're here.
[01:04:31.160 --> 01:04:35.000]   But those packages of preferences are not a robust way
[01:04:35.000 --> 01:04:36.640]   to promote having more descendants.
[01:04:36.640 --> 01:04:39.280]   They were tied to our ancestral environment,
[01:04:39.280 --> 01:04:40.120]   which is no longer true.
[01:04:40.120 --> 01:04:41.780]   So that's one of the reasons we are now having
[01:04:41.780 --> 01:04:43.680]   a big fertility decline,
[01:04:43.680 --> 01:04:45.480]   because in our current environment,
[01:04:45.480 --> 01:04:47.760]   our ancestral preferences are not inducing us
[01:04:47.760 --> 01:04:48.840]   to have a lot of kids,
[01:04:48.840 --> 01:04:52.440]   which is, from evolution's point of view, a big mistake.
[01:04:52.440 --> 01:04:55.020]   We can predict that in the longer run,
[01:04:55.020 --> 01:04:58.600]   there will arise creatures who just abstractly know
[01:04:58.600 --> 01:05:00.920]   that what they want is more descendants.
[01:05:00.920 --> 01:05:04.040]   That's a very robust way to have more descendants
[01:05:04.040 --> 01:05:05.920]   is to have that as your direct preference.
[01:05:05.920 --> 01:05:08.480]   First of all, you're thinking is so clear.
[01:05:08.480 --> 01:05:09.480]   I love it.
[01:05:09.480 --> 01:05:14.480]   So mathematical, and thank you for thinking so clear with me
[01:05:14.480 --> 01:05:16.360]   and bearing with my interruptions
[01:05:16.360 --> 01:05:20.560]   and going on the tangents when we go there.
[01:05:20.560 --> 01:05:23.800]   So you're just clearly saying that successful,
[01:05:23.800 --> 01:05:28.800]   long-term civilizations will prefer to have descendants,
[01:05:28.800 --> 01:05:30.160]   more descendants.
[01:05:30.160 --> 01:05:33.500]   - Not just prefer, consciously and abstractly prefer.
[01:05:33.500 --> 01:05:36.480]   That is, it won't be the indirect consequence
[01:05:36.480 --> 01:05:37.320]   of other preference.
[01:05:37.320 --> 01:05:40.040]   It will just be the thing they know they want.
[01:05:40.040 --> 01:05:42.320]   - There'll be a president in the future that says,
[01:05:42.320 --> 01:05:44.800]   "We must have more sex."
[01:05:44.800 --> 01:05:45.920]   - We must have more descendants
[01:05:45.920 --> 01:05:47.720]   and do whatever it takes to do that.
[01:05:47.720 --> 01:05:48.960]   - Whatever.
[01:05:48.960 --> 01:05:51.160]   - We must go to the moon and do the other things.
[01:05:51.160 --> 01:05:52.000]   - Right.
[01:05:52.000 --> 01:05:53.760]   - Not because they're easy, but because they're hard.
[01:05:53.760 --> 01:05:55.620]   But instead of the moon, let's have lots of sex.
[01:05:55.620 --> 01:05:59.040]   Okay, but there's a lot of ways to have descendants, right?
[01:05:59.040 --> 01:06:00.280]   - Right, but so that's the whole point.
[01:06:00.280 --> 01:06:02.120]   When the world gets more complicated
[01:06:02.120 --> 01:06:03.760]   and there are many possible strategies,
[01:06:03.760 --> 01:06:05.720]   it's having that as your abstract preference
[01:06:05.720 --> 01:06:08.440]   that will force you to think through those possibilities
[01:06:08.440 --> 01:06:10.040]   and pick the one that's most effective.
[01:06:10.040 --> 01:06:14.400]   - So just to clarify, descendants doesn't necessarily mean
[01:06:14.400 --> 01:06:16.140]   the narrow definition of descendants,
[01:06:16.140 --> 01:06:18.640]   meaning humans having sex and then having babies.
[01:06:18.640 --> 01:06:19.460]   - Exactly.
[01:06:19.460 --> 01:06:21.600]   - You can have artificial intelligence systems
[01:06:21.600 --> 01:06:26.600]   that in whom you instill some capability of cognition
[01:06:26.600 --> 01:06:29.280]   and perhaps even consciousness.
[01:06:29.280 --> 01:06:31.560]   You can also create through genetics and biology
[01:06:31.560 --> 01:06:35.680]   clones of yourself or slightly modified clones,
[01:06:35.680 --> 01:06:36.760]   thousands of them.
[01:06:36.760 --> 01:06:38.400]   - Right.
[01:06:38.400 --> 01:06:40.080]   - So all kinds of descendants.
[01:06:40.080 --> 01:06:43.320]   It could be descendants in the space of ideas too,
[01:06:43.320 --> 01:06:46.720]   for somehow we no longer exist in this meat vehicle.
[01:06:46.720 --> 01:06:51.600]   It's now just like whatever the definition of a life form is,
[01:06:51.600 --> 01:06:54.420]   you have descendants of those life forms.
[01:06:54.420 --> 01:06:56.680]   - Yes, and they will be thoughtful about that.
[01:06:56.680 --> 01:06:59.760]   They will have thought about what counts as a descendant
[01:06:59.760 --> 01:07:02.360]   and that'll be important to them to have the right concept.
[01:07:02.360 --> 01:07:06.080]   - So the they there is very interesting, who the they are.
[01:07:06.080 --> 01:07:08.320]   - But the key thing is we're making predictions
[01:07:08.320 --> 01:07:09.560]   that I think are somewhat robust
[01:07:09.560 --> 01:07:11.800]   about what our distant descendants will be like.
[01:07:11.800 --> 01:07:14.020]   Another thing I think you would automatically accept
[01:07:14.020 --> 01:07:16.480]   is they will almost entirely be artificial.
[01:07:16.480 --> 01:07:18.000]   And I think that would be the obvious prediction
[01:07:18.000 --> 01:07:19.520]   about any aliens we would meet.
[01:07:19.520 --> 01:07:22.400]   That is they would long sense have given up
[01:07:22.400 --> 01:07:24.220]   reproducing biologically.
[01:07:24.220 --> 01:07:27.220]   - Well, it's all, it's like organic or something.
[01:07:27.220 --> 01:07:28.800]   It's all real and it's--
[01:07:28.800 --> 01:07:31.360]   - It might be squishy and made out of hydrocarbons,
[01:07:31.360 --> 01:07:33.920]   but it would be artificial in the sense of made in factories
[01:07:33.920 --> 01:07:36.040]   with designs on CAD things, right?
[01:07:36.040 --> 01:07:37.240]   Factories with scale economies.
[01:07:37.240 --> 01:07:39.400]   So the factories we have made on Earth today
[01:07:39.400 --> 01:07:40.960]   have much larger scale economies
[01:07:40.960 --> 01:07:42.160]   than the factories in our cells.
[01:07:42.160 --> 01:07:44.920]   So the factories in our cells are, there are marvels,
[01:07:44.920 --> 01:07:46.840]   but they don't achieve very many scale economies.
[01:07:46.840 --> 01:07:48.120]   They're tiny little factories.
[01:07:48.120 --> 01:07:49.160]   - But they're all factories.
[01:07:49.160 --> 01:07:50.000]   - Yes.
[01:07:50.000 --> 01:07:50.820]   - Factories on top of factories.
[01:07:50.820 --> 01:07:53.320]   So everything, the factories on top--
[01:07:53.320 --> 01:07:54.520]   - But the factories that are designed
[01:07:54.520 --> 01:07:58.000]   is different than sort of the factories that have evolved.
[01:07:58.000 --> 01:08:00.160]   - I think the nature of the word design
[01:08:00.160 --> 01:08:02.320]   is very interesting to uncover there.
[01:08:02.320 --> 01:08:05.280]   But let me, in terms of aliens,
[01:08:05.280 --> 01:08:09.500]   let me go, let me analyze your Twitter like it's Shakespeare.
[01:08:09.500 --> 01:08:10.340]   - Okay.
[01:08:10.340 --> 01:08:12.360]   - There's a tweet that says,
[01:08:12.360 --> 01:08:15.520]   define "hello" in quotes, alien civilizations
[01:08:15.520 --> 01:08:17.960]   as one that might, in the next million years,
[01:08:17.960 --> 01:08:21.020]   identify humans as intelligent and civilized,
[01:08:21.020 --> 01:08:24.140]   travel to Earth and say, "Hello,"
[01:08:24.140 --> 01:08:27.560]   by making their presence and advanced abilities known to us.
[01:08:27.560 --> 01:08:30.480]   The next 15 polls, this is a Twitter thread,
[01:08:30.480 --> 01:08:34.320]   the next 15 polls ask about such "hello" aliens.
[01:08:34.320 --> 01:08:38.320]   And what these polls ask is your Twitter followers,
[01:08:38.320 --> 01:08:41.720]   what they think those aliens will be like.
[01:08:41.720 --> 01:08:43.640]   Certain particular qualities.
[01:08:43.640 --> 01:08:47.840]   So poll number one is, what percent of "hello" aliens
[01:08:47.840 --> 01:08:51.320]   evolved from biological species with two main genders?
[01:08:51.320 --> 01:08:56.320]   And, you know, the popular vote is above 80%.
[01:08:56.320 --> 01:08:58.640]   So most of them have two genders.
[01:08:58.640 --> 01:08:59.760]   What do you think about that?
[01:08:59.760 --> 01:09:00.760]   I'll ask you about some of these
[01:09:00.760 --> 01:09:01.600]   'cause they're so interesting.
[01:09:01.600 --> 01:09:02.440]   It's such an interesting question.
[01:09:02.440 --> 01:09:03.640]   - It is a fun set of questions.
[01:09:03.640 --> 01:09:04.900]   - Yes, a fun set of questions.
[01:09:04.900 --> 01:09:08.040]   So the genders as we look through evolutionary history,
[01:09:08.040 --> 01:09:09.560]   what's the usefulness of that?
[01:09:09.560 --> 01:09:13.680]   As opposed to having just one or like millions.
[01:09:13.680 --> 01:09:16.520]   - So there's a question in evolution of life on Earth,
[01:09:16.520 --> 01:09:19.840]   there are very few species that have more than two genders.
[01:09:19.840 --> 01:09:22.240]   There are some, but they aren't very many.
[01:09:22.240 --> 01:09:24.200]   But there's an enormous number of species
[01:09:24.200 --> 01:09:27.040]   that do have two genders, much more than one.
[01:09:27.040 --> 01:09:32.040]   And so there's literature on why did multiple genders evolve
[01:09:32.040 --> 01:09:34.640]   and then sort of what's the point of having males
[01:09:34.640 --> 01:09:36.440]   and females versus hermaphrodites.
[01:09:36.440 --> 01:09:38.960]   So most plants are hermaphrodites.
[01:09:38.960 --> 01:09:42.840]   That is they have, they would mate male, female,
[01:09:42.840 --> 01:09:45.840]   but each plant can be either role.
[01:09:45.840 --> 01:09:48.600]   And then most animals have chosen to split
[01:09:48.600 --> 01:09:50.440]   into males and females.
[01:09:50.440 --> 01:09:53.000]   And then they're differentiating the two genders.
[01:09:53.000 --> 01:09:55.440]   And there's an interesting set of questions
[01:09:55.440 --> 01:09:56.560]   about why that happens.
[01:09:56.560 --> 01:09:58.520]   - 'Cause you can do selection.
[01:09:58.520 --> 01:10:03.040]   You basically have like one gender competes
[01:10:03.040 --> 01:10:06.560]   for the affection of other and there's sexual partnership
[01:10:06.560 --> 01:10:07.640]   that creates the offspring.
[01:10:07.640 --> 01:10:08.880]   So there's sexual selection.
[01:10:08.880 --> 01:10:12.440]   It's nice to have like to a party,
[01:10:12.440 --> 01:10:13.880]   it's nice to have dance partners.
[01:10:13.880 --> 01:10:15.880]   And then each one get to choose
[01:10:15.880 --> 01:10:17.440]   based on certain characteristics.
[01:10:17.440 --> 01:10:19.960]   And that's an efficient mechanism
[01:10:19.960 --> 01:10:21.400]   for adapting to the environment,
[01:10:21.400 --> 01:10:24.440]   being successfully adapted to the environment.
[01:10:24.440 --> 01:10:27.080]   - It does look like there's an advantage.
[01:10:27.080 --> 01:10:30.680]   If you have males, then the males can take higher variance.
[01:10:30.680 --> 01:10:32.800]   And so there can be stronger selection among the males
[01:10:32.800 --> 01:10:34.960]   in terms of weeding out genetic mutations
[01:10:34.960 --> 01:10:37.400]   because the males have higher variance
[01:10:37.400 --> 01:10:39.120]   in their mating success.
[01:10:39.120 --> 01:10:40.640]   - Sure, okay.
[01:10:40.640 --> 01:10:43.760]   Question number two, what percent of hello aliens
[01:10:43.760 --> 01:10:46.360]   evolved from land animals as opposed to plants
[01:10:46.360 --> 01:10:51.280]   or ocean/air organisms?
[01:10:51.280 --> 01:10:55.400]   By the way, I did recently see that
[01:10:55.400 --> 01:11:00.400]   there's only 10% of species on earth are in the ocean.
[01:11:00.400 --> 01:11:04.480]   So there's a lot more variety on land.
[01:11:04.480 --> 01:11:05.600]   - There is.
[01:11:05.600 --> 01:11:06.520]   - It's interesting.
[01:11:06.520 --> 01:11:07.440]   So why is that?
[01:11:07.440 --> 01:11:11.040]   I don't even, I can't even intuit exactly why that would be.
[01:11:11.040 --> 01:11:13.720]   Maybe survival on land is harder,
[01:11:13.720 --> 01:11:14.560]   and so you get a lot--
[01:11:14.560 --> 01:11:18.120]   - So the story that I understand is it's about small niches.
[01:11:18.120 --> 01:11:21.240]   So speciation can be promoted
[01:11:21.240 --> 01:11:23.240]   by having multiple different species.
[01:11:23.240 --> 01:11:25.600]   So in the ocean, species are larger.
[01:11:25.600 --> 01:11:28.720]   That is, there are more creatures in each species
[01:11:28.720 --> 01:11:31.320]   because the ocean environments don't vary as much.
[01:11:31.320 --> 01:11:32.500]   So if you're good in one place,
[01:11:32.500 --> 01:11:34.400]   you're good in many other places.
[01:11:34.400 --> 01:11:36.480]   But on land, and especially in rivers,
[01:11:36.480 --> 01:11:38.360]   rivers contain an enormous percentage
[01:11:38.360 --> 01:11:42.640]   of the kinds of species on land, you see,
[01:11:42.640 --> 01:11:46.480]   because they vary so much from place to place.
[01:11:46.480 --> 01:11:48.560]   And so a species can be good in one place,
[01:11:48.560 --> 01:11:50.400]   and then other species can't really compete
[01:11:50.400 --> 01:11:52.400]   because they came from a different place
[01:11:52.400 --> 01:11:53.240]   where things are different.
[01:11:53.240 --> 01:11:57.200]   So it's a remarkable fact, actually,
[01:11:57.200 --> 01:12:00.360]   that speciation promotes evolution in the long run.
[01:12:00.360 --> 01:12:02.700]   That is, more evolution has happened on land
[01:12:02.700 --> 01:12:05.200]   because there have been more species on land
[01:12:05.200 --> 01:12:07.700]   because each species has been smaller.
[01:12:07.700 --> 01:12:10.920]   And that's actually a warning about something called rot
[01:12:10.920 --> 01:12:12.200]   that I've thought a lot about,
[01:12:12.200 --> 01:12:15.080]   which is one of the problems with even a world government,
[01:12:15.080 --> 01:12:17.240]   which is large systems of software today
[01:12:17.240 --> 01:12:19.720]   just consistently rot and decay with time
[01:12:19.720 --> 01:12:20.720]   and have to be replaced.
[01:12:20.720 --> 01:12:23.000]   And that plausibly also is a problem
[01:12:23.000 --> 01:12:25.360]   for other large systems, including biological systems,
[01:12:25.360 --> 01:12:27.760]   legal systems, regulatory systems.
[01:12:27.760 --> 01:12:30.880]   And it seems like large species
[01:12:30.880 --> 01:12:34.280]   actually don't evolve as effectively as small ones do.
[01:12:34.280 --> 01:12:38.040]   And that's an important thing to notice about.
[01:12:38.040 --> 01:12:40.120]   And that's actually, that's different
[01:12:40.120 --> 01:12:44.760]   from ordinary sort of evolution in economies on Earth
[01:12:44.760 --> 01:12:46.460]   in the last few centuries, say.
[01:12:46.460 --> 01:12:50.040]   On Earth, the more technical evolution
[01:12:50.040 --> 01:12:51.120]   and economic growth happens
[01:12:51.120 --> 01:12:54.520]   in larger integrated cities and nations.
[01:12:54.520 --> 01:12:56.240]   But in biology, it's the other way around.
[01:12:56.240 --> 01:12:59.560]   More evolution happened in the fragmented species.
[01:12:59.560 --> 01:13:01.840]   - Yeah, it's such a nuanced discussion
[01:13:01.840 --> 01:13:04.480]   'cause you can also push back in terms of nations
[01:13:04.480 --> 01:13:06.400]   and at least companies.
[01:13:06.400 --> 01:13:10.560]   It's like large companies seems to evolve less effectively.
[01:13:10.560 --> 01:13:13.400]   There is something that, you know,
[01:13:13.400 --> 01:13:15.860]   they have more resources, more,
[01:13:15.860 --> 01:13:18.720]   they don't even have better resilience.
[01:13:18.720 --> 01:13:22.520]   And when you look at the scale of decades and centuries,
[01:13:22.520 --> 01:13:25.480]   it seems like a lot of large companies die.
[01:13:25.480 --> 01:13:27.480]   - But still large economies do better.
[01:13:27.480 --> 01:13:30.400]   Like large cities grow better than small cities.
[01:13:30.400 --> 01:13:32.440]   Large integrated economies like the United States
[01:13:32.440 --> 01:13:35.320]   or the European Union do better than small fragmented ones.
[01:13:35.320 --> 01:13:37.240]   So even-- - Yeah, sure.
[01:13:37.240 --> 01:13:40.320]   That's a very interesting long discussion.
[01:13:40.320 --> 01:13:43.720]   But so most of the people, and obviously votes on Twitter
[01:13:43.720 --> 01:13:48.440]   represent the absolute objective truth of things.
[01:13:48.440 --> 01:13:51.200]   So most, but an interesting question about oceans is that,
[01:13:51.200 --> 01:13:53.320]   okay, remember I told you about how most planets
[01:13:53.320 --> 01:13:56.640]   would last for trillions of years and be later, right?
[01:13:56.640 --> 01:13:59.560]   So people have tried to explain why life appeared on earth
[01:13:59.560 --> 01:14:02.440]   by saying, oh, all those planets are gonna be unqualified
[01:14:02.440 --> 01:14:04.040]   for life because of various problems.
[01:14:04.040 --> 01:14:06.400]   That is, they're around smaller stars, which lasts longer,
[01:14:06.400 --> 01:14:09.280]   and smaller stars have some things like more solar flares,
[01:14:09.280 --> 01:14:10.640]   maybe more tidal locking.
[01:14:10.640 --> 01:14:14.640]   But almost all of these problems with longer lived planets
[01:14:14.640 --> 01:14:16.760]   aren't problems for ocean worlds.
[01:14:16.760 --> 01:14:20.320]   And a large fraction of planets out there are ocean worlds.
[01:14:20.320 --> 01:14:22.560]   So if life can appear on an ocean world,
[01:14:22.560 --> 01:14:25.120]   then that pretty much ensures
[01:14:25.120 --> 01:14:29.280]   that these planets that last a very long time
[01:14:29.280 --> 01:14:31.640]   could have advanced life because most,
[01:14:31.640 --> 01:14:33.080]   you know, there's a huge fraction of ocean worlds.
[01:14:33.080 --> 01:14:34.560]   - So that's actually an open question.
[01:14:34.560 --> 01:14:38.800]   So when you say, sorry, when you say life appear,
[01:14:38.800 --> 01:14:42.000]   you're kind of saying life and intelligent life.
[01:14:42.000 --> 01:14:46.360]   So like, so that's an open question.
[01:14:46.360 --> 01:14:48.960]   Is land, and that's I suppose the question
[01:14:48.960 --> 01:14:52.440]   behind the Twitter poll,
[01:14:52.440 --> 01:14:55.280]   which is a grabby alien civilization
[01:14:55.280 --> 01:14:57.440]   that comes to say hello.
[01:14:57.440 --> 01:15:02.120]   What's the chance that they first began their early steps,
[01:15:02.120 --> 01:15:04.780]   the difficult steps they took on land?
[01:15:04.780 --> 01:15:07.920]   What do you think?
[01:15:07.920 --> 01:15:14.040]   80%, most people on Twitter think it's very likely.
[01:15:14.040 --> 01:15:15.000]   - Right. - What do you think?
[01:15:15.000 --> 01:15:18.040]   - I think people are discounting ocean worlds too much.
[01:15:18.040 --> 01:15:20.080]   That is, I think people tend to assume
[01:15:20.080 --> 01:15:22.360]   that whatever we did must be the only way
[01:15:22.360 --> 01:15:24.320]   it's possible, and I think people aren't giving enough
[01:15:24.320 --> 01:15:26.320]   credit for other possible paths, but.
[01:15:26.320 --> 01:15:28.840]   - Dolphins, water world, by the way,
[01:15:28.840 --> 01:15:30.160]   people criticize that movie.
[01:15:30.160 --> 01:15:31.040]   I love that movie.
[01:15:31.040 --> 01:15:33.080]   Kevin Costner can do me no wrong.
[01:15:33.080 --> 01:15:34.440]   Okay, next question.
[01:15:34.440 --> 01:15:38.760]   What percent of hello aliens once had a nuclear war
[01:15:38.760 --> 01:15:43.380]   with greater than 10 nukes fired in anger?
[01:15:43.380 --> 01:15:46.560]   So not in incompetence and as an accident.
[01:15:47.760 --> 01:15:49.860]   - Intentional firing of nukes,
[01:15:49.860 --> 01:15:54.360]   and less than 20% was the most popular vote.
[01:15:54.360 --> 01:15:56.420]   - That just seems wrong to me.
[01:15:56.420 --> 01:15:59.920]   - So like, I wonder what, so most people think
[01:15:59.920 --> 01:16:02.560]   once you get nukes, we're not gonna fire them.
[01:16:02.560 --> 01:16:05.880]   They believe in the power of the game theory.
[01:16:05.880 --> 01:16:07.640]   - I think they're assuming that if you had a nuclear war,
[01:16:07.640 --> 01:16:09.540]   then that would just end civilization for good.
[01:16:09.540 --> 01:16:10.840]   I think that's the thinking.
[01:16:10.840 --> 01:16:11.680]   - That's the main thing.
[01:16:11.680 --> 01:16:12.880]   - Right, and I think that's just wrong.
[01:16:12.880 --> 01:16:15.320]   I think you could rise again after a nuclear war.
[01:16:15.320 --> 01:16:17.640]   It might take 10,000 years or 100,000 years,
[01:16:17.640 --> 01:16:19.040]   but it could rise again.
[01:16:19.040 --> 01:16:21.720]   - So what do you think about mutually assured destruction
[01:16:21.720 --> 01:16:25.880]   as a force to prevent people from firing nuclear weapons?
[01:16:25.880 --> 01:16:29.840]   That's a question that I knew to a terrifying degree
[01:16:29.840 --> 01:16:32.180]   has been raised now and what's going on.
[01:16:32.180 --> 01:16:34.760]   - Well, I mean, clearly it has had an effect.
[01:16:34.760 --> 01:16:37.880]   The question is just how strong an effect for how long?
[01:16:37.880 --> 01:16:41.680]   I mean, clearly we have not gone wild with nuclear war,
[01:16:41.680 --> 01:16:44.360]   and clearly the devastation that you would get
[01:16:44.360 --> 01:16:46.440]   if you initiated a nuclear war is part of the reasons
[01:16:46.440 --> 01:16:48.120]   people have been reluctant to start a war.
[01:16:48.120 --> 01:16:50.800]   The question is just how reliably
[01:16:50.800 --> 01:16:52.960]   will that ensure the absence of a war?
[01:16:52.960 --> 01:16:54.560]   - Yeah, the knight is still young.
[01:16:54.560 --> 01:16:55.400]   - Exactly.
[01:16:55.400 --> 01:16:57.760]   - So it's been 70 years or whatever it's been.
[01:16:57.760 --> 01:17:02.120]   I mean, but what do you think?
[01:17:02.120 --> 01:17:07.040]   Do you think we'll see nuclear war in the century?
[01:17:07.040 --> 01:17:07.960]   - I don't know if in the century,
[01:17:07.960 --> 01:17:10.560]   but it's the sort of thing
[01:17:10.560 --> 01:17:12.960]   that's likely to happen eventually.
[01:17:12.960 --> 01:17:14.400]   - That's a very loose statement.
[01:17:14.400 --> 01:17:15.600]   Okay, I understand.
[01:17:15.600 --> 01:17:18.560]   Now this is where I pull you out of your mathematical model
[01:17:18.560 --> 01:17:20.440]   and ask a human question.
[01:17:20.440 --> 01:17:22.920]   Do you think, this particular human question--
[01:17:22.920 --> 01:17:25.460]   - I think we've been lucky that it hasn't happened so far.
[01:17:25.460 --> 01:17:27.160]   - But what is the nature of nuclear war?
[01:17:27.160 --> 01:17:29.040]   Let's think about this.
[01:17:29.040 --> 01:17:32.960]   There is dictators, there's democracies,
[01:17:32.960 --> 01:17:39.320]   miscommunication, how did war start?
[01:17:39.320 --> 01:17:41.080]   World War I, World War II.
[01:17:41.080 --> 01:17:43.600]   - So the biggest datum here is that we've had
[01:17:43.600 --> 01:17:46.960]   an enormous decline in major war over the last century.
[01:17:46.960 --> 01:17:48.640]   So that has to be taken into account.
[01:17:48.640 --> 01:17:51.720]   Now, so the problem is,
[01:17:51.720 --> 01:17:54.760]   war is a process that has a very long tail.
[01:17:54.760 --> 01:17:58.180]   That is, there are rare, very large wars.
[01:17:58.180 --> 01:18:02.720]   So the average war is much worse than the median war
[01:18:02.720 --> 01:18:04.560]   because of this long tail.
[01:18:04.560 --> 01:18:07.900]   And that makes it hard to identify trends over time.
[01:18:07.900 --> 01:18:10.480]   So the median war has clearly gone way down
[01:18:10.480 --> 01:18:12.240]   in the last century, that a median rate of war.
[01:18:12.240 --> 01:18:15.440]   But it could be that's because the tail has gotten thicker.
[01:18:15.440 --> 01:18:17.400]   And in fact, the average war is just as bad.
[01:18:17.400 --> 01:18:19.760]   But most wars are gonna be big wars.
[01:18:19.760 --> 01:18:21.640]   So that's the thing we're not so sure about.
[01:18:21.640 --> 01:18:26.640]   - There's no strong data on wars with one,
[01:18:26.640 --> 01:18:31.680]   because of the destructive nature of the weapons,
[01:18:31.680 --> 01:18:33.840]   kill hundreds of millions of people.
[01:18:33.840 --> 01:18:35.440]   There's no data on this.
[01:18:35.440 --> 01:18:36.280]   - Right.
[01:18:36.280 --> 01:18:37.560]   - But we can start intuiting.
[01:18:37.560 --> 01:18:39.320]   - But we can see that the power law,
[01:18:39.320 --> 01:18:41.180]   we can do a power law fit to the rate of wars.
[01:18:41.180 --> 01:18:43.680]   And it's a power law with a thick tail.
[01:18:43.680 --> 01:18:45.920]   So it's one of those things that you should expect
[01:18:45.920 --> 01:18:48.160]   most of the damage to be in the few biggest ones.
[01:18:48.160 --> 01:18:49.960]   So that's also true for pandemics
[01:18:49.960 --> 01:18:51.180]   and some a few other things.
[01:18:51.180 --> 01:18:52.480]   For pandemics, most of the damage
[01:18:52.480 --> 01:18:53.560]   is in the few biggest ones.
[01:18:53.560 --> 01:18:56.460]   So the median pandemics of ours less than the average
[01:18:56.460 --> 01:18:57.920]   that you should expect in the future.
[01:18:57.920 --> 01:19:02.920]   - But those, that fitting of data is very questionable
[01:19:02.920 --> 01:19:06.520]   because everything you said is correct.
[01:19:06.520 --> 01:19:08.800]   The question is like, what can we infer
[01:19:08.800 --> 01:19:13.800]   about the future of civilization threatening pandemics
[01:19:13.800 --> 01:19:19.380]   or nuclear war from studying the history
[01:19:19.380 --> 01:19:21.220]   of the 20th century?
[01:19:21.220 --> 01:19:23.740]   So like, you can't just fit it to the data,
[01:19:23.740 --> 01:19:25.720]   the rate of wars and the destructive nature.
[01:19:25.720 --> 01:19:28.820]   Like that's not how nuclear war will happen.
[01:19:28.820 --> 01:19:33.660]   Nuclear war happens with two assholes or idiots
[01:19:33.660 --> 01:19:35.180]   that have access to a button.
[01:19:35.180 --> 01:19:37.060]   - Small wars happen that way too.
[01:19:37.060 --> 01:19:37.940]   - No, I understand that.
[01:19:37.940 --> 01:19:41.080]   But that's, it's very important, small wars aside,
[01:19:41.080 --> 01:19:42.940]   it's very important to understand the dynamics,
[01:19:42.940 --> 01:19:45.160]   the human dynamics and the geopolitics
[01:19:45.160 --> 01:19:47.560]   of the way nuclear war happens
[01:19:47.560 --> 01:19:52.560]   in order to predict how we can minimize the chance of--
[01:19:52.560 --> 01:19:55.420]   - It is a common and useful intellectual strategy
[01:19:55.420 --> 01:19:57.900]   to take something that could be really big
[01:19:57.900 --> 01:19:59.500]   or but is often very small
[01:19:59.500 --> 01:20:01.340]   and fit the distribution of the data,
[01:20:01.340 --> 01:20:02.720]   small things which you have a lot of them
[01:20:02.720 --> 01:20:04.900]   and then ask, do I believe the big things
[01:20:04.900 --> 01:20:05.900]   are really that different?
[01:20:05.900 --> 01:20:06.740]   - Right, I see.
[01:20:06.740 --> 01:20:08.540]   - So sometimes it's reasonable to say like,
[01:20:08.540 --> 01:20:11.220]   say with tornadoes or even pandemics or something,
[01:20:11.220 --> 01:20:14.620]   the underlying process might not be that different
[01:20:14.620 --> 01:20:15.940]   for the high and small ones.
[01:20:15.940 --> 01:20:16.900]   It might not be.
[01:20:16.900 --> 01:20:21.520]   The fact that mutual assured destruction
[01:20:21.520 --> 01:20:23.660]   seems to work to some degree
[01:20:23.660 --> 01:20:26.300]   shows you that to some degree it's different
[01:20:26.300 --> 01:20:27.440]   than the small wars.
[01:20:27.440 --> 01:20:36.140]   So it's a really important question to understand
[01:20:36.500 --> 01:20:40.460]   is are humans capable, one human,
[01:20:40.460 --> 01:20:42.760]   like how many humans on earth,
[01:20:42.760 --> 01:20:44.660]   if I give them a button now,
[01:20:44.660 --> 01:20:48.020]   say you pressing this button will kill everyone on earth,
[01:20:48.020 --> 01:20:49.060]   everyone, right?
[01:20:49.060 --> 01:20:51.900]   How many humans will press that button?
[01:20:51.900 --> 01:20:53.700]   I wanna know those numbers,
[01:20:53.700 --> 01:20:55.860]   like day to day, minute to minute,
[01:20:55.860 --> 01:20:58.940]   how many people have that much irresponsibility,
[01:20:58.940 --> 01:21:03.260]   evil, incompetence, ignorance,
[01:21:03.260 --> 01:21:04.540]   whatever word you wanna assign,
[01:21:04.540 --> 01:21:06.860]   there's a lot of dynamics in the psychology
[01:21:06.860 --> 01:21:09.180]   that leads you to press that button, but how many?
[01:21:09.180 --> 01:21:11.020]   My intuition is the number,
[01:21:11.020 --> 01:21:14.740]   the more destructive that press of a button,
[01:21:14.740 --> 01:21:16.460]   the fewer humans you find.
[01:21:16.460 --> 01:21:19.380]   That number gets very close to zero very quickly,
[01:21:19.380 --> 01:21:22.940]   especially people have access to such a button.
[01:21:22.940 --> 01:21:26.340]   But that's perhaps a hope than a reality.
[01:21:26.340 --> 01:21:28.840]   And unfortunately we don't have good data on this,
[01:21:30.980 --> 01:21:34.820]   which is like how destructive are humans willing to be?
[01:21:34.820 --> 01:21:38.340]   - So I think part of this just has to think about,
[01:21:38.340 --> 01:21:40.740]   ask what your time scales you're looking at, right?
[01:21:40.740 --> 01:21:43.380]   So if you say, if you look at the history of war,
[01:21:43.380 --> 01:21:45.660]   we've had a lot of wars pretty consistently
[01:21:45.660 --> 01:21:47.400]   over many centuries.
[01:21:47.400 --> 01:21:49.900]   So if you ask, will we have a nuclear war
[01:21:49.900 --> 01:21:52.580]   in the next 50 years, I might say, well, probably not.
[01:21:52.580 --> 01:21:55.460]   If I say 500 or 5,000 years,
[01:21:55.460 --> 01:21:57.620]   like if the same sort of risks are underlying
[01:21:57.620 --> 01:21:58.480]   and they just continue,
[01:21:58.480 --> 01:22:00.260]   then you have to add that up over time
[01:22:00.260 --> 01:22:02.540]   and think the risk is getting a lot larger
[01:22:02.540 --> 01:22:04.540]   the longer a time scale we're looking at.
[01:22:04.540 --> 01:22:07.100]   - But, okay, let's generalize nuclear war
[01:22:07.100 --> 01:22:09.020]   because what I was more referring to
[01:22:09.020 --> 01:22:12.180]   is something that kills more than
[01:22:12.180 --> 01:22:18.180]   20% of humans on earth
[01:22:18.180 --> 01:22:21.300]   and injures or makes
[01:22:21.300 --> 01:22:28.580]   the other 80% suffer horribly,
[01:22:28.580 --> 01:22:30.120]   survive but suffer.
[01:22:30.120 --> 01:22:31.060]   That's what I was referring to.
[01:22:31.060 --> 01:22:32.800]   So when you look at 500 years from now,
[01:22:32.800 --> 01:22:33.980]   there might not be nuclear war,
[01:22:33.980 --> 01:22:35.740]   there might be something else
[01:22:35.740 --> 01:22:38.740]   that has that destructive effect.
[01:22:38.740 --> 01:22:43.740]   And I don't know, these feel like novel questions
[01:22:43.740 --> 01:22:46.060]   in the history of humanity.
[01:22:46.060 --> 01:22:47.660]   I just don't know.
[01:22:47.660 --> 01:22:49.820]   I think since nuclear weapons,
[01:22:49.820 --> 01:22:55.620]   this has been engineering pandemics, for example,
[01:22:55.620 --> 01:22:58.080]   robotics, so nanobots,
[01:22:59.080 --> 01:23:01.260]   here's how I phrase the question.
[01:23:01.260 --> 01:23:03.340]   - It just seems like a real new possibility
[01:23:03.340 --> 01:23:04.740]   that we have to contend with
[01:23:04.740 --> 01:23:06.260]   and we don't have good models,
[01:23:06.260 --> 01:23:08.300]   or from my perspective.
[01:23:08.300 --> 01:23:10.860]   - So if you look on, say, the last 1,000 years
[01:23:10.860 --> 01:23:13.700]   or 10,000 years, we could say we've seen a certain rate
[01:23:13.700 --> 01:23:16.340]   at which people are willing to make big destruction
[01:23:16.340 --> 01:23:17.420]   in terms of war.
[01:23:17.420 --> 01:23:18.260]   - Yes.
[01:23:18.260 --> 01:23:21.500]   - Okay, and if you're willing to project that data forward,
[01:23:21.500 --> 01:23:24.460]   then I think if you wanna ask over periods of thousands
[01:23:24.460 --> 01:23:25.620]   or tens of thousands of years,
[01:23:25.620 --> 01:23:27.500]   you would have a reasonable data set.
[01:23:27.500 --> 01:23:30.300]   So the key question is what's changed lately?
[01:23:30.300 --> 01:23:31.140]   - Yes.
[01:23:31.140 --> 01:23:33.620]   - Okay, and so a big question
[01:23:33.620 --> 01:23:35.340]   of which I've given a lot of thought to,
[01:23:35.340 --> 01:23:37.820]   what are the major changes that seem to have happened
[01:23:37.820 --> 01:23:40.860]   in culture and human attitudes over the last few centuries
[01:23:40.860 --> 01:23:42.700]   and what's our best explanation for those
[01:23:42.700 --> 01:23:45.660]   so that we can project them forward into the future?
[01:23:45.660 --> 01:23:48.660]   And I have a story about that,
[01:23:48.660 --> 01:23:51.180]   which is the story that we have been drifting back
[01:23:51.180 --> 01:23:54.820]   toward forager attitudes in the last few centuries
[01:23:54.820 --> 01:23:55.860]   as we get rich.
[01:23:55.860 --> 01:23:59.460]   So the idea is we spent a million years being a forager,
[01:23:59.460 --> 01:24:02.820]   and that was a very sort of standard lifestyle
[01:24:02.820 --> 01:24:04.540]   that we know a lot about.
[01:24:04.540 --> 01:24:06.580]   Foragers sort of live in small bands,
[01:24:06.580 --> 01:24:09.780]   they make decisions cooperatively, they share food,
[01:24:09.780 --> 01:24:13.940]   they don't have much property, et cetera.
[01:24:13.940 --> 01:24:15.580]   And humans liked that.
[01:24:15.580 --> 01:24:18.220]   And then 10,000 years ago, farming became possible,
[01:24:18.220 --> 01:24:20.620]   but it was only possible because we were plastic enough
[01:24:20.620 --> 01:24:21.900]   to really change our culture.
[01:24:21.900 --> 01:24:24.540]   Farming styles and cultures are very different.
[01:24:24.540 --> 01:24:26.780]   They have slavery, they have war, they have property,
[01:24:26.780 --> 01:24:29.340]   they have inequality, they have kings,
[01:24:29.340 --> 01:24:31.580]   they stay in one place instead of wandering,
[01:24:31.580 --> 01:24:34.780]   they don't have as much diversity of experience or food,
[01:24:34.780 --> 01:24:36.460]   they have more disease.
[01:24:36.460 --> 01:24:37.980]   Farming life is just very different.
[01:24:37.980 --> 01:24:41.700]   But humans were able to sort of introduce conformity
[01:24:41.700 --> 01:24:42.980]   and religion and all sorts of things
[01:24:42.980 --> 01:24:45.380]   to become just a very different kind of creature as farmers.
[01:24:45.380 --> 01:24:47.140]   Farmers are just really different than foragers
[01:24:47.140 --> 01:24:49.180]   in terms of their values and their lives.
[01:24:49.180 --> 01:24:52.660]   But the pressures that made foragers into farmers
[01:24:52.660 --> 01:24:54.420]   were a part mediated by poverty.
[01:24:54.420 --> 01:24:57.340]   Farmers are poor, and if they deviated
[01:24:57.340 --> 01:25:00.100]   from the farming norms that people around them supported,
[01:25:00.100 --> 01:25:02.340]   they were quite at risk of starving to death.
[01:25:02.340 --> 01:25:06.660]   And then in the last few centuries, we've gotten rich.
[01:25:06.660 --> 01:25:10.020]   And as we've gotten rich, the social pressures
[01:25:10.020 --> 01:25:12.060]   that turned foragers into farmers
[01:25:12.060 --> 01:25:15.740]   have become less persuasive to us.
[01:25:15.740 --> 01:25:18.100]   So for example, a farming young woman who was told,
[01:25:18.100 --> 01:25:19.660]   "If you have a child out of wedlock,
[01:25:19.660 --> 01:25:21.420]   "you and your child may starve,"
[01:25:21.420 --> 01:25:22.700]   that was a credible threat.
[01:25:22.700 --> 01:25:25.340]   She would see actual examples around her
[01:25:25.340 --> 01:25:27.620]   to make that a believable threat.
[01:25:27.620 --> 01:25:29.340]   Today, if you say to a young woman,
[01:25:29.340 --> 01:25:30.900]   "You shouldn't have a child out of wedlock,"
[01:25:30.900 --> 01:25:32.580]   she will see other young women around her
[01:25:32.580 --> 01:25:33.980]   doing okay that way.
[01:25:33.980 --> 01:25:35.860]   We're all rich enough to be able to afford
[01:25:35.860 --> 01:25:38.060]   that sort of a thing, and therefore,
[01:25:38.060 --> 01:25:41.860]   she's more inclined often to go with her inclinations,
[01:25:41.860 --> 01:25:44.140]   her sort of more natural inclinations about such things
[01:25:44.140 --> 01:25:46.380]   rather than to be pressured to follow
[01:25:46.380 --> 01:25:48.420]   the official farming norms
[01:25:48.420 --> 01:25:49.660]   of that you shouldn't do that sort of thing.
[01:25:49.660 --> 01:25:51.180]   And all through our lives,
[01:25:51.180 --> 01:25:54.580]   we have been drifting back toward forager attitudes
[01:25:54.580 --> 01:25:56.180]   because we've been getting rich.
[01:25:56.180 --> 01:25:59.300]   And so aside from at work, which is an exception,
[01:25:59.300 --> 01:26:02.620]   but elsewhere, I think this explains trends
[01:26:02.620 --> 01:26:06.260]   toward less slavery, more democracy, less religion,
[01:26:06.260 --> 01:26:09.540]   less fertility, more promiscuity, more travel,
[01:26:09.540 --> 01:26:14.220]   more art, more leisure, fewer work hours.
[01:26:14.220 --> 01:26:16.420]   All of these trends are basically explained
[01:26:16.420 --> 01:26:19.100]   by becoming more forager-like.
[01:26:19.100 --> 01:26:21.180]   And much science fiction celebrates this.
[01:26:21.180 --> 01:26:22.740]   Star Trek or the culture novels,
[01:26:22.740 --> 01:26:25.060]   people like this image that we are moving
[01:26:25.060 --> 01:26:27.380]   toward this world where basically like foragers,
[01:26:27.380 --> 01:26:30.460]   we're peaceful, we share, we make decisions collectively,
[01:26:30.460 --> 01:26:33.940]   we have a lot of free time, we are into art.
[01:26:33.940 --> 01:26:38.860]   So forager, you know, forager is a word,
[01:26:38.860 --> 01:26:42.180]   and it's a loaded word because it's connected
[01:26:42.180 --> 01:26:47.180]   to the actual, what life was actually like at that time.
[01:26:47.580 --> 01:26:50.100]   As you mentioned, we sometimes don't do a good job
[01:26:50.100 --> 01:26:53.260]   of telling accurately what life was like back then.
[01:26:53.260 --> 01:26:55.940]   But you're saying if it's not exactly like foragers,
[01:26:55.940 --> 01:26:58.340]   it rhymes in some fundamental way.
[01:26:58.340 --> 01:27:00.820]   - Right. - 'Cause you also said peaceful.
[01:27:00.820 --> 01:27:03.900]   Is it obvious that a forager with a nuclear weapon
[01:27:03.900 --> 01:27:07.740]   would be peaceful?
[01:27:07.740 --> 01:27:09.540]   I don't know if that's 100% obvious.
[01:27:09.540 --> 01:27:11.380]   - So we know, again, we know a fair bit
[01:27:11.380 --> 01:27:13.860]   about what foragers' lives were like.
[01:27:13.860 --> 01:27:16.900]   The main sort of violence they had would be sexual jealousy.
[01:27:16.900 --> 01:27:18.380]   They were relatively promiscuous,
[01:27:18.380 --> 01:27:19.720]   and so there'd be a lot of jealousy.
[01:27:19.720 --> 01:27:22.940]   But they did not have organized wars with each other.
[01:27:22.940 --> 01:27:24.300]   That is, they were at peace
[01:27:24.300 --> 01:27:25.980]   with their neighboring forager bands.
[01:27:25.980 --> 01:27:28.540]   They didn't have property in land or even in people.
[01:27:28.540 --> 01:27:30.180]   They didn't really have marriage.
[01:27:30.180 --> 01:27:35.060]   And so they were, in fact, peaceful.
[01:27:35.060 --> 01:27:37.580]   - And when you think about large-scale wars,
[01:27:37.580 --> 01:27:38.420]   they don't start large-scale wars.
[01:27:38.420 --> 01:27:40.500]   - Right, they didn't have coordinated large-scale wars
[01:27:40.500 --> 01:27:41.660]   in the way chimpanzees do.
[01:27:41.660 --> 01:27:43.560]   Now, chimpanzees do have wars
[01:27:43.560 --> 01:27:45.420]   between one tribe of chimpanzees and others,
[01:27:45.420 --> 01:27:46.860]   but human foragers did not.
[01:27:46.860 --> 01:27:48.640]   Farmers returned to that, of course,
[01:27:48.640 --> 01:27:50.500]   the more chimpanzee-like styles.
[01:27:50.500 --> 01:27:52.660]   - Well, that's a hopeful message.
[01:27:52.660 --> 01:27:54.340]   If we could return real quick
[01:27:54.340 --> 01:27:59.180]   to the Hello Aliens Twitter thread,
[01:27:59.180 --> 01:28:01.060]   one of them is really interesting about language.
[01:28:01.060 --> 01:28:02.460]   What percent of Hello Aliens
[01:28:02.460 --> 01:28:05.460]   would be able to talk to us in our language?
[01:28:05.460 --> 01:28:08.020]   So this is the question of communication.
[01:28:08.020 --> 01:28:10.460]   It actually gets to the nature of language.
[01:28:10.460 --> 01:28:13.180]   - It also gets to the nature
[01:28:13.180 --> 01:28:16.260]   of how advanced you expect them to be.
[01:28:16.260 --> 01:28:21.100]   So I think some people see that we have advanced
[01:28:21.100 --> 01:28:22.980]   over the last thousands of years
[01:28:22.980 --> 01:28:25.240]   and we aren't reaching any sort of limit,
[01:28:25.240 --> 01:28:28.380]   and so they tend to assume it could go on forever.
[01:28:28.380 --> 01:28:30.980]   And I actually tend to think that within, say,
[01:28:30.980 --> 01:28:35.100]   10 million years, we will sort of max out on technology.
[01:28:35.100 --> 01:28:38.780]   We will sort of learn everything that's feasible to know,
[01:28:38.780 --> 01:28:39.940]   for the most part.
[01:28:39.940 --> 01:28:42.280]   And then obstacles to understanding
[01:28:42.280 --> 01:28:44.840]   would more be about cultural differences,
[01:28:44.840 --> 01:28:46.540]   like ways in which different places
[01:28:46.540 --> 01:28:49.140]   have just chosen to do things differently.
[01:28:49.140 --> 01:28:51.900]   And so then the question is,
[01:28:51.900 --> 01:28:54.540]   is it even possible to communicate
[01:28:54.540 --> 01:28:57.180]   across some cultural distances?
[01:28:57.180 --> 01:28:58.340]   And I might think, yeah,
[01:28:58.340 --> 01:29:00.280]   I could imagine some maybe advanced aliens
[01:29:00.280 --> 01:29:02.420]   who've become so weird and different from each other
[01:29:02.420 --> 01:29:03.820]   they can't communicate with each other,
[01:29:03.820 --> 01:29:07.540]   but we're probably pretty simple compared to them.
[01:29:07.540 --> 01:29:10.860]   So I would think, sure, if they wanted to,
[01:29:10.860 --> 01:29:12.600]   they could communicate with us.
[01:29:12.600 --> 01:29:15.020]   - So it's the simplicity of the recipient.
[01:29:15.020 --> 01:29:18.480]   I tend to, just to push back,
[01:29:18.480 --> 01:29:22.560]   let's explore the possibility where that's not the case.
[01:29:22.560 --> 01:29:24.960]   Can we communicate with ants?
[01:29:24.960 --> 01:29:30.560]   I find that, like this idea that--
[01:29:30.560 --> 01:29:33.400]   - We're not very good at communicating in general.
[01:29:33.400 --> 01:29:36.240]   - Oh, you're saying, all right, I see.
[01:29:36.240 --> 01:29:38.520]   You're saying once you get orders of magnitude better
[01:29:38.520 --> 01:29:39.880]   at communicating.
[01:29:39.880 --> 01:29:43.080]   - Once they had maxed out on all communication technology
[01:29:43.080 --> 01:29:45.320]   in general, and they just understood in general
[01:29:45.320 --> 01:29:46.920]   how to communicate with lots of things,
[01:29:46.920 --> 01:29:48.680]   and had done that for millions of years.
[01:29:48.680 --> 01:29:51.240]   - But you have to be able to, this is so interesting,
[01:29:51.240 --> 01:29:53.040]   as somebody who cares a lot about empathy
[01:29:53.040 --> 01:29:55.220]   and imagining how other people feel,
[01:29:55.220 --> 01:30:00.240]   communication requires empathy,
[01:30:00.240 --> 01:30:03.520]   meaning you have to truly understand
[01:30:03.520 --> 01:30:08.520]   how the other person, the other organism sees the world.
[01:30:08.880 --> 01:30:11.800]   It's like a four-dimensional species
[01:30:11.800 --> 01:30:13.440]   talking to a two-dimensional species.
[01:30:13.440 --> 01:30:16.640]   It's not as trivial as, to me at least,
[01:30:16.640 --> 01:30:18.320]   as it might at first seem.
[01:30:18.320 --> 01:30:20.840]   - So let me reverse my position a little,
[01:30:20.840 --> 01:30:24.480]   because I'll say, well, the hello aliens question
[01:30:24.480 --> 01:30:28.200]   really combines two different scenarios
[01:30:28.200 --> 01:30:30.400]   that we're slipping over.
[01:30:30.400 --> 01:30:33.700]   So one scenario would be that the hello aliens
[01:30:33.700 --> 01:30:35.160]   would be like grabby aliens.
[01:30:35.160 --> 01:30:36.880]   They would be just fully advanced.
[01:30:36.880 --> 01:30:38.560]   They would have been expanding for millions of years.
[01:30:38.560 --> 01:30:40.880]   They would have a very advanced civilization,
[01:30:40.880 --> 01:30:43.280]   and then they would finally be arriving here
[01:30:43.280 --> 01:30:45.600]   after a billion years, perhaps, of expanding,
[01:30:45.600 --> 01:30:47.720]   in which case they're gonna be crazy advanced
[01:30:47.720 --> 01:30:49.000]   at some maximum level.
[01:30:49.000 --> 01:30:54.000]   But the hello aliens about aliens we might meet soon,
[01:30:54.000 --> 01:30:56.160]   which might be sort of UFO aliens,
[01:30:56.160 --> 01:31:00.460]   and UFO aliens probably are not grabby aliens.
[01:31:00.460 --> 01:31:04.500]   - How do you get here if you're not a grabby alien?
[01:31:04.500 --> 01:31:06.760]   - Well, they would have to be able to travel.
[01:31:08.400 --> 01:31:11.640]   But they would not be expansive.
[01:31:11.640 --> 01:31:14.960]   So if it's a road trip, it doesn't count as grabby.
[01:31:14.960 --> 01:31:19.640]   So we're talking about expanding the comfortable colony.
[01:31:19.640 --> 01:31:24.180]   - The question is, if UFOs, some of them are aliens,
[01:31:24.180 --> 01:31:26.560]   what kind of aliens would they be?
[01:31:26.560 --> 01:31:28.600]   This is sort of the key question you have to ask
[01:31:28.600 --> 01:31:30.800]   in order to try to interpret that scenario.
[01:31:30.800 --> 01:31:36.140]   The key fact we would know is that they are here right now,
[01:31:36.140 --> 01:31:39.080]   but the universe around us is not full
[01:31:39.080 --> 01:31:41.240]   of an alien civilization.
[01:31:41.240 --> 01:31:45.800]   So that says right off the bat that they chose not
[01:31:45.800 --> 01:31:50.800]   to allow massive expansion of a grabby civilization.
[01:31:50.800 --> 01:31:53.160]   - Is it possible that they chose it,
[01:31:53.160 --> 01:31:55.120]   but we just don't see them yet?
[01:31:55.120 --> 01:31:58.200]   These are the stragglers, the journeymen, the--
[01:31:58.200 --> 01:32:00.440]   - So the timing coincidence is,
[01:32:00.440 --> 01:32:02.920]   it's almost surely if they are here now,
[01:32:02.920 --> 01:32:04.160]   they are much older than us.
[01:32:04.160 --> 01:32:07.340]   They are many millions of years older than us.
[01:32:07.340 --> 01:32:09.800]   And so they could have filled the galaxy
[01:32:09.800 --> 01:32:12.400]   in that last millions of years if they had wanted to.
[01:32:12.400 --> 01:32:15.900]   That is, they couldn't just be right at the edge.
[01:32:15.900 --> 01:32:17.060]   Very unlikely.
[01:32:17.060 --> 01:32:19.540]   Most likely they would have been around waiting for us
[01:32:19.540 --> 01:32:20.380]   for a long time.
[01:32:20.380 --> 01:32:21.520]   They could have come here anytime
[01:32:21.520 --> 01:32:22.820]   in the last millions of years,
[01:32:22.820 --> 01:32:24.940]   and they just chosen, they've been waiting around for this,
[01:32:24.940 --> 01:32:26.860]   or they just chose to come recently.
[01:32:26.860 --> 01:32:30.100]   But the timing coincidence, it would be crazy unlikely
[01:32:30.100 --> 01:32:32.140]   that they just happened to be able to get here,
[01:32:32.140 --> 01:32:34.180]   say in the last 100 years.
[01:32:34.180 --> 01:32:36.500]   They would no doubt have been able to get here
[01:32:36.500 --> 01:32:37.860]   far earlier than that.
[01:32:37.860 --> 01:32:39.100]   - Again, we don't know.
[01:32:39.100 --> 01:32:41.580]   So this is a friend like UFO sightings on Earth.
[01:32:41.580 --> 01:32:44.580]   We don't know if this kind of increase in sightings
[01:32:44.580 --> 01:32:46.580]   have anything to do with actual visitation.
[01:32:46.580 --> 01:32:48.340]   - I'm just talking about the timing.
[01:32:48.340 --> 01:32:50.980]   They arose at some point in space time.
[01:32:50.980 --> 01:32:55.260]   And it's very unlikely that that was just to the point
[01:32:55.260 --> 01:32:57.680]   that they could just barely get here recently.
[01:32:57.680 --> 01:32:59.500]   Almost surely they would have--
[01:32:59.500 --> 01:33:00.340]   - But they might have been here.
[01:33:00.340 --> 01:33:01.900]   - They could have gotten here much earlier.
[01:33:01.900 --> 01:33:04.580]   - And well, throughout the stretch of several billion years
[01:33:04.580 --> 01:33:06.380]   that Earth existed, they could have been here often.
[01:33:06.380 --> 01:33:10.180]   - Exactly, so they could have therefore filled the galaxy
[01:33:10.180 --> 01:33:11.980]   long time ago if they had wanted to.
[01:33:11.980 --> 01:33:13.940]   - Let's push back on that.
[01:33:13.940 --> 01:33:16.580]   The question to me is, isn't it possible
[01:33:16.580 --> 01:33:19.680]   that the expansion of a civilization
[01:33:19.680 --> 01:33:23.700]   is much harder than the travel,
[01:33:23.700 --> 01:33:28.780]   the sphere of the reachable is different
[01:33:28.780 --> 01:33:31.500]   than the sphere of the colonized.
[01:33:31.500 --> 01:33:36.340]   So isn't it possible that the sphere of places
[01:33:36.340 --> 01:33:38.980]   where the stragglers go, the different people
[01:33:38.980 --> 01:33:42.100]   that journey out, the explorers, is much, much larger
[01:33:42.100 --> 01:33:46.460]   and grows much faster than the civilization?
[01:33:46.460 --> 01:33:49.560]   So in which case, they would visit us.
[01:33:49.560 --> 01:33:50.700]   There's a lot of visitors,
[01:33:50.700 --> 01:33:53.080]   the grad students of the civilization.
[01:33:53.080 --> 01:33:55.780]   They're exploring, they're collecting the data,
[01:33:55.780 --> 01:33:58.660]   but we're not yet going to see them.
[01:33:58.660 --> 01:34:01.640]   And by yet, I mean across millions of years.
[01:34:01.640 --> 01:34:07.540]   - The time delay between when the first thing might arrive
[01:34:07.540 --> 01:34:11.140]   and then when colonists could arrive in mass
[01:34:11.140 --> 01:34:14.540]   and do a mass amount of work is cosmologically short.
[01:34:14.540 --> 01:34:16.420]   In human history, of course, sure,
[01:34:16.420 --> 01:34:18.700]   there might be a century between that,
[01:34:18.700 --> 01:34:21.520]   but a century is just a tiny amount of time
[01:34:21.520 --> 01:34:23.100]   on the scales we're talking about.
[01:34:23.100 --> 01:34:25.460]   - So this is, in computer science,
[01:34:25.460 --> 01:34:27.020]   there's ant colony optimization.
[01:34:27.020 --> 01:34:28.500]   It's true for ants.
[01:34:28.500 --> 01:34:30.340]   So it's like when the first ant shows up,
[01:34:30.340 --> 01:34:33.260]   it's likely, and if there's anything of value,
[01:34:33.260 --> 01:34:36.700]   it's likely the other ants will follow quickly.
[01:34:36.700 --> 01:34:37.580]   Yeah. - Relatively short.
[01:34:37.580 --> 01:34:41.620]   It's also true that traveling over very long distances,
[01:34:41.620 --> 01:34:44.500]   probably one of the main ways to make that feasible
[01:34:44.500 --> 01:34:47.460]   is that you land somewhere, you colonize a bit,
[01:34:47.460 --> 01:34:49.940]   you create new resources that can then allow you
[01:34:49.940 --> 01:34:51.620]   to go farther. - Many short hops
[01:34:51.620 --> 01:34:53.260]   as opposed to a giant, long journey.
[01:34:53.260 --> 01:34:56.180]   - Exactly, but those hops require that you are able
[01:34:56.180 --> 01:34:59.460]   to start a colonization of sorts along those hops, right?
[01:34:59.460 --> 01:35:01.180]   You have to be able to stop somewhere,
[01:35:01.180 --> 01:35:04.460]   make it into a way station such that you can then
[01:35:04.460 --> 01:35:05.620]   support you moving farther.
[01:35:05.620 --> 01:35:07.460]   - So what do you think of,
[01:35:07.460 --> 01:35:09.880]   there's been a lot of UFO sightings,
[01:35:09.880 --> 01:35:12.580]   what do you think about those UFO sightings
[01:35:12.580 --> 01:35:16.460]   and what do you think if any of them are
[01:35:16.460 --> 01:35:21.820]   of extraterrestrial origin and we don't see
[01:35:21.820 --> 01:35:25.220]   giant civilizations out in the sky,
[01:35:25.220 --> 01:35:27.620]   how do you make sense of that then?
[01:35:27.620 --> 01:35:29.900]   - I wanna do some clearing of throats,
[01:35:29.900 --> 01:35:33.020]   which is people like to do on this topic, right?
[01:35:33.020 --> 01:35:34.260]   They wanna make sure you understand
[01:35:34.260 --> 01:35:36.060]   they're saying this and not that, right?
[01:35:36.060 --> 01:35:40.140]   So I would say the analysis needs both
[01:35:40.140 --> 01:35:41.780]   a prior and a likelihood.
[01:35:41.780 --> 01:35:45.960]   So the prior is what are the scenarios
[01:35:45.960 --> 01:35:48.740]   that are at all plausible in terms of what we know
[01:35:48.740 --> 01:35:50.820]   about the universe and then the likelihood
[01:35:50.820 --> 01:35:53.300]   is the particular actual sightings,
[01:35:53.300 --> 01:35:56.740]   like how hard are those to explain through various means.
[01:35:56.740 --> 01:36:00.340]   I will establish myself as someone of an expert
[01:36:00.340 --> 01:36:03.060]   on the prior, I would say my studies
[01:36:03.060 --> 01:36:05.060]   and the things I've studied make me an expert
[01:36:05.060 --> 01:36:07.080]   and I should stand up and have an opinion on that
[01:36:07.080 --> 01:36:09.120]   and be able to explain it.
[01:36:09.120 --> 01:36:11.980]   The likelihood, however, is not my area of expertise.
[01:36:11.980 --> 01:36:16.300]   That is, I'm not a pilot, I don't do atmospheric studies
[01:36:16.300 --> 01:36:18.140]   of things I haven't studied in detail,
[01:36:18.140 --> 01:36:20.740]   the various kinds of atmospheric phenomena or whatever
[01:36:20.740 --> 01:36:23.260]   that might be used to explain the particular sightings.
[01:36:23.260 --> 01:36:25.260]   I can just say from my amateur stance,
[01:36:25.260 --> 01:36:27.860]   the sightings look damn puzzling.
[01:36:27.860 --> 01:36:30.620]   They do not look easy to dismiss,
[01:36:30.620 --> 01:36:33.060]   the attempts I've seen to easily dismiss them
[01:36:33.060 --> 01:36:36.180]   seem to me to fail, it seems like these are pretty puzzling,
[01:36:36.180 --> 01:36:40.500]   weird stuff that deserve an expert's attention
[01:36:40.500 --> 01:36:43.540]   in terms of considering, asking what the likelihood is.
[01:36:43.540 --> 01:36:46.320]   So analogy I would make is a murder trial, okay?
[01:36:46.320 --> 01:36:49.140]   On average, if we say what's the chance
[01:36:49.140 --> 01:36:51.100]   any one person murdered another person
[01:36:51.100 --> 01:36:53.660]   as a prior probability, maybe one in a thousand people
[01:36:53.660 --> 01:36:55.860]   get murdered, maybe each person has a thousand people
[01:36:55.860 --> 01:36:57.440]   around them who could plausibly have done it,
[01:36:57.440 --> 01:37:00.740]   so the prior probability of a murder is one in a million.
[01:37:00.740 --> 01:37:03.740]   But we allow murder trials because often evidence
[01:37:03.740 --> 01:37:06.660]   is sufficient to overcome a one in a million prior
[01:37:06.660 --> 01:37:10.020]   because the evidence is often strong enough, right?
[01:37:10.020 --> 01:37:14.460]   My guess, rough guess for the UFOs as aliens scenario,
[01:37:14.460 --> 01:37:15.900]   at least some of them, is the prior is roughly
[01:37:15.900 --> 01:37:19.860]   one in a thousand, much higher than the usual murder trial,
[01:37:19.860 --> 01:37:23.620]   plenty high enough that strong physical evidence
[01:37:23.620 --> 01:37:25.460]   could put you over the top to think
[01:37:25.460 --> 01:37:27.180]   it's more likely than not.
[01:37:27.180 --> 01:37:29.300]   But I'm not an expert on that physical evidence,
[01:37:29.300 --> 01:37:31.060]   I'm gonna leave that part to someone else.
[01:37:31.060 --> 01:37:33.540]   I'm gonna say the prior is pretty high,
[01:37:33.540 --> 01:37:34.900]   this isn't a crazy scenario.
[01:37:34.900 --> 01:37:38.060]   So then I can elaborate on where my prior comes from.
[01:37:38.060 --> 01:37:41.300]   What scenario could make most sense of this data?
[01:37:41.300 --> 01:37:46.500]   My scenario to make sense has two main parts.
[01:37:46.500 --> 01:37:49.820]   First is panspermia siblings.
[01:37:49.820 --> 01:37:53.260]   So panspermia is the hypothesized process
[01:37:53.260 --> 01:37:56.460]   by which life might have arrived on Earth from elsewhere.
[01:37:56.460 --> 01:37:59.700]   And a plausible time for that, I mean,
[01:37:59.700 --> 01:38:01.780]   it would have to happen very early in Earth's history
[01:38:01.780 --> 01:38:03.460]   'cause we see life early in history.
[01:38:03.460 --> 01:38:05.340]   And a plausible time could have been
[01:38:05.340 --> 01:38:08.580]   during the stellar nursery where the sun was born
[01:38:08.580 --> 01:38:12.580]   with many other stars in the same close proximity
[01:38:12.580 --> 01:38:14.180]   with lots of rocks flying around,
[01:38:14.180 --> 01:38:17.020]   able to move things from one place to another.
[01:38:18.340 --> 01:38:22.580]   If a rock with life on it from some rock with planet
[01:38:22.580 --> 01:38:24.620]   with life came into that stellar nursery,
[01:38:24.620 --> 01:38:27.980]   it plausibly could have seeded many planets
[01:38:27.980 --> 01:38:30.140]   in that stellar nursery all at the same time.
[01:38:30.140 --> 01:38:31.700]   They're all born at the same time in the same place,
[01:38:31.700 --> 01:38:35.140]   pretty close to each other, lots of rocks flying around.
[01:38:35.140 --> 01:38:38.580]   So a panspermia scenario would then create siblings,
[01:38:38.580 --> 01:38:42.220]   i.e. there would be say a few thousand
[01:38:42.220 --> 01:38:44.740]   other planets out there.
[01:38:44.740 --> 01:38:47.380]   So after the nursery forms, it drifts, it separates,
[01:38:47.380 --> 01:38:48.420]   they drift apart.
[01:38:48.420 --> 01:38:50.020]   And so out there in the galaxy,
[01:38:50.020 --> 01:38:51.700]   there would now be a bunch of other stars
[01:38:51.700 --> 01:38:52.940]   all formed at the same time,
[01:38:52.940 --> 01:38:55.780]   and we can actually spot them in terms of their spectrum.
[01:38:55.780 --> 01:38:59.840]   And they would have then started on the same path of life
[01:38:59.840 --> 01:39:02.020]   as we did with that life being seeded,
[01:39:02.020 --> 01:39:04.180]   but they would move at different rates.
[01:39:04.180 --> 01:39:08.780]   And most likely, most of them would never
[01:39:08.780 --> 01:39:10.580]   reach an advanced level before the deadline,
[01:39:10.580 --> 01:39:15.300]   but maybe one other did, and maybe it did before us.
[01:39:16.340 --> 01:39:19.020]   So if they did, they could know all of this,
[01:39:19.020 --> 01:39:20.740]   and they could go searching for their siblings.
[01:39:20.740 --> 01:39:23.060]   That is, they could look in the sky for the other stars
[01:39:23.060 --> 01:39:25.740]   that match the spectrum that matches the spectrum
[01:39:25.740 --> 01:39:27.060]   that came from this nursery.
[01:39:27.060 --> 01:39:30.260]   They could identify their sibling stars in the galaxy,
[01:39:30.260 --> 01:39:31.940]   the thousand of them,
[01:39:31.940 --> 01:39:33.780]   and those would be of special interest to them
[01:39:33.780 --> 01:39:36.480]   'cause they would think, well, life might be on those.
[01:39:36.480 --> 01:39:39.580]   And they could go looking for them.
[01:39:39.580 --> 01:39:43.180]   - You're just such a brilliant mathematical,
[01:39:43.180 --> 01:39:48.180]   philosophical, physical, biological idea
[01:39:48.180 --> 01:39:51.500]   of panspermia siblings
[01:39:51.500 --> 01:39:54.060]   because we all kind of started at similar time
[01:39:54.060 --> 01:39:59.140]   in this local pocket of the universe.
[01:39:59.140 --> 01:40:02.980]   And so that changes a lot of the math.
[01:40:02.980 --> 01:40:04.580]   - So that would create this correlation
[01:40:04.580 --> 01:40:06.140]   between when advanced life might appear.
[01:40:06.140 --> 01:40:09.240]   No longer just random independent spaces in space-time.
[01:40:09.240 --> 01:40:10.900]   There'd be this cluster, perhaps.
[01:40:10.900 --> 01:40:13.500]   And that allows interaction between--
[01:40:13.500 --> 01:40:15.020]   - The elements of the cluster, yes.
[01:40:15.020 --> 01:40:17.340]   - Non-grabby alien civilizations,
[01:40:17.340 --> 01:40:21.660]   like kind of primitive alien civilizations
[01:40:21.660 --> 01:40:23.700]   like us with others,
[01:40:23.700 --> 01:40:25.500]   and they might be a little bit ahead.
[01:40:25.500 --> 01:40:26.980]   That's so fascinating.
[01:40:26.980 --> 01:40:28.420]   - They would probably be a lot ahead.
[01:40:28.420 --> 01:40:30.940]   So the puzzle is-- - Sure, sure.
[01:40:30.940 --> 01:40:33.780]   - If they happen before us,
[01:40:33.780 --> 01:40:35.780]   they probably happened hundreds of millions
[01:40:35.780 --> 01:40:37.180]   of years before us.
[01:40:37.180 --> 01:40:38.700]   - But less than a billion.
[01:40:38.700 --> 01:40:41.300]   - Less than a billion, but still plenty of time
[01:40:41.300 --> 01:40:42.700]   that they could have become grabby
[01:40:42.700 --> 01:40:45.580]   and filled the galaxy and gone beyond.
[01:40:45.580 --> 01:40:46.460]   So there'd be plenty,
[01:40:46.460 --> 01:40:49.080]   so the fact is they chose not to become grabby.
[01:40:49.080 --> 01:40:51.220]   That would have to be the interpretation.
[01:40:51.220 --> 01:40:52.060]   If we have panspermia siblings--
[01:40:52.060 --> 01:40:54.260]   - Plenty of time to become grabby, you said.
[01:40:54.260 --> 01:40:55.860]   So it should be fine. - Yes, they had plenty of time
[01:40:55.860 --> 01:40:56.980]   and they chose not to.
[01:40:56.980 --> 01:40:59.300]   - Are we sure about this?
[01:40:59.300 --> 01:41:01.900]   So 100 million years is enough?
[01:41:01.900 --> 01:41:04.660]   - 100 million, so I told you before that I said
[01:41:04.660 --> 01:41:06.300]   within 10 million years,
[01:41:06.300 --> 01:41:09.020]   our descendants will become grabby or not.
[01:41:09.020 --> 01:41:11.020]   - And they'll have that choice, okay.
[01:41:11.020 --> 01:41:13.740]   - And so they clearly more than 10 million years
[01:41:13.740 --> 01:41:16.380]   earlier than us, so they chose not to.
[01:41:16.380 --> 01:41:19.500]   - But still go on vacation, look around,
[01:41:19.500 --> 01:41:20.460]   so it's not grabby.
[01:41:20.460 --> 01:41:22.860]   - If they chose not to expand,
[01:41:22.860 --> 01:41:24.540]   that's going to have to be a rule they set
[01:41:24.540 --> 01:41:26.500]   to not allow any part of themselves to do it.
[01:41:26.500 --> 01:41:30.820]   If they let any little ship fly away
[01:41:30.820 --> 01:41:33.140]   with the ability to create a colony,
[01:41:33.140 --> 01:41:33.980]   the game's over.
[01:41:33.980 --> 01:41:36.780]   Then they have prevented, then the universe
[01:41:36.780 --> 01:41:39.260]   becomes grabby from their origin with this one colony.
[01:41:39.260 --> 01:41:42.340]   So in order to prevent their civilization being grabby,
[01:41:42.340 --> 01:41:44.540]   they have to have a rule they enforce pretty strongly
[01:41:44.540 --> 01:41:47.100]   that no part of them can ever try to do that.
[01:41:47.100 --> 01:41:49.980]   - Through a global authoritarian regime
[01:41:49.980 --> 01:41:52.900]   or through something that's internal to them,
[01:41:52.900 --> 01:41:55.940]   meaning it's part of the nature of life
[01:41:55.940 --> 01:41:58.300]   that it doesn't want, as become advanced--
[01:41:58.300 --> 01:42:00.540]   - Like a political officer in the brain or whatever.
[01:42:00.540 --> 01:42:04.620]   - Yes, there's something in human nature
[01:42:04.620 --> 01:42:08.780]   that prevents you from, or like alien nature,
[01:42:08.780 --> 01:42:10.620]   that as you get more advanced,
[01:42:10.620 --> 01:42:12.380]   you become lazier and lazier
[01:42:12.380 --> 01:42:14.700]   in terms of exploration and expansion.
[01:42:14.700 --> 01:42:17.540]   - So I would say they would have to have enforced
[01:42:17.540 --> 01:42:20.220]   a rule against expanding, and that rule
[01:42:20.220 --> 01:42:22.000]   would probably make them reluctant
[01:42:22.000 --> 01:42:24.380]   to let people leave very far.
[01:42:24.380 --> 01:42:27.260]   Any one vacation trip far away could risk
[01:42:27.260 --> 01:42:29.180]   an expansion from this vacation trip.
[01:42:29.180 --> 01:42:31.060]   So they would probably have a pretty tight lid
[01:42:31.060 --> 01:42:34.220]   on just allowing any travel out from their origin
[01:42:34.220 --> 01:42:35.840]   in order to enforce this rule.
[01:42:35.840 --> 01:42:38.900]   - Interesting. - But then we also know,
[01:42:38.900 --> 01:42:40.740]   well, they would have chosen to come here.
[01:42:40.740 --> 01:42:43.900]   So clearly they made an exception from their general rule
[01:42:43.900 --> 01:42:46.900]   to say, okay, but an expedition to Earth,
[01:42:46.900 --> 01:42:48.260]   that should be allowed.
[01:42:48.260 --> 01:42:50.340]   - It could be intentional exception
[01:42:50.340 --> 01:42:52.780]   or incompetent exception.
[01:42:52.780 --> 01:42:54.840]   - But if incompetent, then they couldn't maintain
[01:42:54.840 --> 01:42:56.700]   this over 100 million years,
[01:42:56.700 --> 01:42:58.860]   this policy of not allowing any expansion.
[01:42:58.860 --> 01:43:01.180]   So we have to see they have successfully,
[01:43:01.180 --> 01:43:02.820]   they not just had a policy to try,
[01:43:02.820 --> 01:43:05.340]   they succeeded over 100 million years
[01:43:05.340 --> 01:43:07.700]   in preventing the expansion.
[01:43:07.700 --> 01:43:09.780]   That's a substantial competence.
[01:43:09.780 --> 01:43:11.540]   - Let me think about this.
[01:43:11.540 --> 01:43:13.300]   So you don't think there could be a barrier
[01:43:13.300 --> 01:43:15.100]   in 100 million years, you don't think
[01:43:15.100 --> 01:43:19.860]   there could be a barrier to, like,
[01:43:19.860 --> 01:43:24.540]   technological barrier to becoming expansionary?
[01:43:24.540 --> 01:43:28.220]   - Imagine the Europeans that tried to prevent anybody
[01:43:28.220 --> 01:43:31.100]   from leaving Europe to go to the New World.
[01:43:31.100 --> 01:43:33.020]   And imagine what it would have taken
[01:43:33.020 --> 01:43:36.260]   to make that happen over 100 million years.
[01:43:36.260 --> 01:43:38.020]   - Yeah, it's impossible.
[01:43:38.020 --> 01:43:40.760]   - They would have had to have very strict, you know,
[01:43:40.760 --> 01:43:43.060]   guards at the borders, at the borders saying,
[01:43:43.060 --> 01:43:44.260]   "No, you can't go."
[01:43:44.260 --> 01:43:47.380]   - But just to clarify, you're not suggesting
[01:43:47.380 --> 01:43:49.040]   that's actually possible.
[01:43:49.040 --> 01:43:51.340]   - I am suggesting it's possible.
[01:43:51.340 --> 01:43:56.020]   - I don't know how you keep, in my silly human brain,
[01:43:56.020 --> 01:43:57.780]   maybe it's a brain that values freedom,
[01:43:57.780 --> 01:43:59.700]   but I don't know how you can keep,
[01:43:59.700 --> 01:44:01.820]   no matter how much force,
[01:44:01.820 --> 01:44:05.140]   no matter how much censorship or control or so on,
[01:44:05.140 --> 01:44:08.100]   I just don't know how you can keep people
[01:44:08.100 --> 01:44:11.780]   from exploring into the mysterious, into the unknown.
[01:44:11.780 --> 01:44:13.580]   - You're thinking of people, we're talking aliens.
[01:44:13.580 --> 01:44:14.940]   So remember, there's a vast space
[01:44:14.940 --> 01:44:16.400]   of different possible social creatures
[01:44:16.400 --> 01:44:17.520]   they could have evolved from,
[01:44:17.520 --> 01:44:19.580]   different kinds of cultures they could be in,
[01:44:19.580 --> 01:44:21.500]   different kinds of threats.
[01:44:21.500 --> 01:44:23.000]   I mean, there are many things, as you talked about,
[01:44:23.000 --> 01:44:25.820]   that most of us would feel very reluctant to do.
[01:44:25.820 --> 01:44:27.100]   This isn't one of those, but--
[01:44:27.100 --> 01:44:29.740]   - Okay, so how, if the UFO sightings
[01:44:29.740 --> 01:44:32.300]   represent alien visitors,
[01:44:32.300 --> 01:44:33.940]   how the heck are they getting here
[01:44:33.940 --> 01:44:36.380]   under the Panspermia siblings?
[01:44:36.380 --> 01:44:39.100]   - So Panspermia siblings is one part of the scenario,
[01:44:39.100 --> 01:44:41.500]   which is that's where they came from.
[01:44:41.500 --> 01:44:42.940]   And from that, we can conclude
[01:44:42.940 --> 01:44:44.520]   they had this rule against expansion,
[01:44:44.520 --> 01:44:46.980]   and they've successfully enforced that.
[01:44:46.980 --> 01:44:49.380]   That also creates a plausible agenda
[01:44:49.380 --> 01:44:50.820]   for why they would be here,
[01:44:50.820 --> 01:44:52.740]   that is, to enforce that rule on us.
[01:44:52.740 --> 01:44:54.860]   That is, if we go out and expanding,
[01:44:54.860 --> 01:44:58.100]   then we have defeated the purpose of this rule they set up.
[01:44:58.100 --> 01:44:58.940]   - Interesting.
[01:44:58.940 --> 01:45:02.660]   - Right, so they would be here to convince us
[01:45:02.660 --> 01:45:03.740]   to not expand.
[01:45:03.740 --> 01:45:05.260]   - Convince in quotes.
[01:45:05.260 --> 01:45:06.860]   - Right, through various mechanisms.
[01:45:06.860 --> 01:45:08.620]   So obviously, one thing we conclude
[01:45:08.620 --> 01:45:10.040]   is they didn't just destroy us.
[01:45:10.040 --> 01:45:12.320]   That would have been completely possible, right?
[01:45:12.320 --> 01:45:14.980]   So the fact that they're here and we are not destroyed
[01:45:14.980 --> 01:45:17.900]   means that they chose not to destroy us.
[01:45:17.900 --> 01:45:19.620]   They have some degree of empathy
[01:45:19.620 --> 01:45:22.060]   or whatever their morals are
[01:45:22.060 --> 01:45:24.460]   that would make them reluctant to just destroy us.
[01:45:24.460 --> 01:45:25.900]   They would rather persuade us.
[01:45:25.900 --> 01:45:27.820]   - They're, destroy their brethren.
[01:45:27.820 --> 01:45:29.500]   And so they may have been,
[01:45:29.500 --> 01:45:32.460]   there's a difference between arrival and observation.
[01:45:32.460 --> 01:45:34.580]   They may have been observing for a very long time.
[01:45:34.580 --> 01:45:35.420]   - Exactly.
[01:45:35.420 --> 01:45:40.420]   - And they arrived to try to, not to try,
[01:45:40.420 --> 01:45:42.380]   I don't think, to try. - To ensure.
[01:45:42.380 --> 01:45:46.780]   - To ensure that we don't become grabby.
[01:45:46.780 --> 01:45:49.220]   - Which is because that's, we can see that they did not,
[01:45:49.220 --> 01:45:51.340]   they must have enforced a ruling against that,
[01:45:51.340 --> 01:45:53.700]   and they are therefore here to,
[01:45:53.700 --> 01:45:55.300]   that's a plausible interpretation
[01:45:55.300 --> 01:45:56.860]   why they would risk this expedition
[01:45:56.860 --> 01:45:59.060]   when they clearly don't risk very many expeditions
[01:45:59.060 --> 01:46:01.420]   over this long period, to allow this one exception,
[01:46:01.420 --> 01:46:04.900]   because otherwise, if they don't, we may become grabby.
[01:46:04.900 --> 01:46:06.500]   And they could have just destroyed us, but they didn't.
[01:46:06.500 --> 01:46:07.660]   - And they're closely monitoring
[01:46:07.660 --> 01:46:10.900]   the technological advancing of civilization,
[01:46:10.900 --> 01:46:12.740]   like what nuclear weapons is one thing,
[01:46:12.740 --> 01:46:13.940]   is that, all right, cool,
[01:46:13.940 --> 01:46:16.340]   that might have less to do with nuclear weapons
[01:46:16.340 --> 01:46:18.140]   and more with nuclear energy.
[01:46:18.140 --> 01:46:20.860]   Maybe they're monitoring fusion closely.
[01:46:20.860 --> 01:46:23.460]   Like, how clever are these apes getting?
[01:46:23.460 --> 01:46:25.420]   So no doubt, they have a button
[01:46:25.420 --> 01:46:27.860]   that if we get too uppity or risky,
[01:46:27.860 --> 01:46:30.940]   they can push the button and ensure that we don't expand,
[01:46:30.940 --> 01:46:32.540]   but they'd rather do it some other way.
[01:46:32.540 --> 01:46:35.740]   So now, that explains why they're here
[01:46:35.740 --> 01:46:36.580]   and why they aren't out there.
[01:46:36.580 --> 01:46:38.180]   But there's another thing that we need to explain.
[01:46:38.180 --> 01:46:40.500]   There's another key data we need to explain about UFOs
[01:46:40.500 --> 01:46:42.460]   if we're gonna have a hypothesis that explains them.
[01:46:42.460 --> 01:46:44.740]   And this is something many people have noticed,
[01:46:44.740 --> 01:46:48.860]   which is they had two extreme options
[01:46:48.860 --> 01:46:50.980]   they could have chosen and didn't choose.
[01:46:51.020 --> 01:46:53.980]   They could have either just remained completely invisible.
[01:46:53.980 --> 01:46:55.340]   Clearly, an advanced civilization
[01:46:55.340 --> 01:46:56.620]   could have been completely invisible.
[01:46:56.620 --> 01:46:59.180]   There's no reason they need to fly around and be noticed.
[01:46:59.180 --> 01:47:01.580]   They could just be in orbit in dark satellites
[01:47:01.580 --> 01:47:03.220]   that are completely invisible to us,
[01:47:03.220 --> 01:47:04.340]   watching whatever they wanna watch.
[01:47:04.340 --> 01:47:06.700]   That would be well within their abilities.
[01:47:06.700 --> 01:47:07.820]   That's one thing they could have done.
[01:47:07.820 --> 01:47:10.020]   The other thing they could do is just show up
[01:47:10.020 --> 01:47:12.260]   and land on the White House lawn, as they say,
[01:47:12.260 --> 01:47:15.180]   and shake hands, like make themselves really obvious.
[01:47:15.180 --> 01:47:16.980]   They could have done either of those,
[01:47:16.980 --> 01:47:18.260]   and they didn't do either of those.
[01:47:18.260 --> 01:47:20.380]   That's the next thing you need to explain
[01:47:20.380 --> 01:47:21.420]   about UFOs as aliens.
[01:47:21.420 --> 01:47:23.500]   Why would they take this intermediate approach,
[01:47:23.500 --> 01:47:26.300]   hanging out near the edge of visibility
[01:47:26.300 --> 01:47:27.860]   with somewhat impressive mechanisms,
[01:47:27.860 --> 01:47:29.740]   but not walking up and introducing themselves,
[01:47:29.740 --> 01:47:31.580]   nor just being completely invisible?
[01:47:31.580 --> 01:47:33.820]   - So, okay, a lot of questions there.
[01:47:33.820 --> 01:47:36.820]   So one, do you think it's obvious
[01:47:36.820 --> 01:47:39.860]   where the White House is, or the White House lawn--
[01:47:39.860 --> 01:47:41.340]   - Well, it's obvious where there are concentrations
[01:47:41.340 --> 01:47:42.580]   of humans that you could go up and introduce.
[01:47:42.580 --> 01:47:46.100]   - But is humans the most interesting thing about Earth?
[01:47:46.100 --> 01:47:47.900]   - Yeah, are you sure about this?
[01:47:47.900 --> 01:47:49.580]   Because-- - If they're worried
[01:47:49.580 --> 01:47:52.420]   about an expansion, then they would be worried
[01:47:52.420 --> 01:47:54.660]   about a civilization that could be capable of expansion.
[01:47:54.660 --> 01:47:56.660]   Obviously, humans are the civilization on Earth
[01:47:56.660 --> 01:47:59.820]   that's by far the closest to being able to expand.
[01:47:59.820 --> 01:48:03.880]   - I just don't know if aliens obviously see,
[01:48:03.880 --> 01:48:11.500]   obviously see humans, like the individual humans,
[01:48:11.500 --> 01:48:16.420]   like the meat vehicles, as the center of focus
[01:48:16.420 --> 01:48:19.700]   for observing a life on a planet.
[01:48:19.700 --> 01:48:21.460]   - They're supposed to be really smart and advanced.
[01:48:21.460 --> 01:48:23.740]   Like, this shouldn't be that hard for them.
[01:48:23.740 --> 01:48:26.260]   - But I think we're actually the dumb ones,
[01:48:26.260 --> 01:48:28.220]   because we think humans are the important things,
[01:48:28.220 --> 01:48:30.720]   but it could be our ideas,
[01:48:30.720 --> 01:48:32.940]   it could be something about our technologies.
[01:48:32.940 --> 01:48:34.620]   - But that's mediated with us, it's correlated with us.
[01:48:34.620 --> 01:48:39.620]   - No, we make it seem like it's mediated by us humans,
[01:48:39.620 --> 01:48:43.380]   but the focus for alien civilizations
[01:48:43.380 --> 01:48:47.300]   might be the AI systems or the technologies themselves.
[01:48:47.300 --> 01:48:48.880]   That might be the organism.
[01:48:48.880 --> 01:48:56.200]   Human is the food, the source of the organism
[01:48:56.200 --> 01:48:59.660]   that's under observation, versus like--
[01:48:59.660 --> 01:49:01.500]   - So what they wanted to have close contact with
[01:49:01.500 --> 01:49:03.460]   was something that was closely near humans,
[01:49:03.460 --> 01:49:05.620]   then they would be contacting those,
[01:49:05.620 --> 01:49:08.180]   and we would just incidentally see, but we would still see.
[01:49:08.180 --> 01:49:11.380]   - But don't you think, isn't it possible,
[01:49:11.380 --> 01:49:13.780]   taking their perspective, isn't it possible
[01:49:13.780 --> 01:49:15.140]   that they would want to interact
[01:49:15.140 --> 01:49:18.540]   with some fundamental aspect that they're interested in
[01:49:18.540 --> 01:49:21.060]   without interfering with it?
[01:49:21.060 --> 01:49:24.380]   And that's actually a very, no matter how advanced you are,
[01:49:24.380 --> 01:49:25.580]   it's very difficult to do, I think.
[01:49:25.580 --> 01:49:26.820]   - But that's puzzling.
[01:49:26.820 --> 01:49:30.820]   So, I mean, the prototypical UFO observation
[01:49:30.820 --> 01:49:35.620]   is a shiny, big object in the sky
[01:49:35.620 --> 01:49:38.340]   that has very rapid acceleration
[01:49:38.340 --> 01:49:42.700]   and no apparent surfaces for using air
[01:49:42.700 --> 01:49:44.600]   to manipulate its speed.
[01:49:44.600 --> 01:49:50.180]   And the question is, why that, right?
[01:49:50.180 --> 01:49:52.020]   Again, if they just, for example,
[01:49:52.020 --> 01:49:53.780]   if they just wanted to talk to our computer systems,
[01:49:53.780 --> 01:49:56.540]   they could move some sort of a little probe
[01:49:56.540 --> 01:50:00.580]   that connects to a wire and reads and sends bits there.
[01:50:00.580 --> 01:50:02.980]   They don't need a shiny thing flying in the sky.
[01:50:02.980 --> 01:50:05.940]   - But don't you think they would be,
[01:50:05.940 --> 01:50:09.060]   they are, would be looking for the right way to communicate,
[01:50:09.060 --> 01:50:11.100]   the right language to communicate?
[01:50:11.100 --> 01:50:14.380]   Everything you just said, looking at the computer systems,
[01:50:14.380 --> 01:50:16.900]   I mean, that's not a trivial thing.
[01:50:16.900 --> 01:50:19.880]   Coming up with a signal that us humans
[01:50:19.880 --> 01:50:22.760]   would not freak out too much about,
[01:50:22.760 --> 01:50:25.340]   but also understand, might not be that trivial.
[01:50:25.340 --> 01:50:26.500]   How would you talk to things? - Well, so not freak out
[01:50:26.500 --> 01:50:28.580]   a part is another interesting constraint.
[01:50:28.580 --> 01:50:30.740]   So again, I said, like the two obvious strategies
[01:50:30.740 --> 01:50:33.100]   are just to remain completely invisible and watch,
[01:50:33.100 --> 01:50:34.340]   which would be quite feasible,
[01:50:34.340 --> 01:50:36.780]   or to just directly interact,
[01:50:36.780 --> 01:50:39.420]   let's come out and be really very direct, right?
[01:50:39.420 --> 01:50:41.580]   I mean, there's big things that you can see around.
[01:50:41.580 --> 01:50:44.220]   There's big cities, there's aircraft carriers.
[01:50:44.220 --> 01:50:46.300]   There's lots of, if you want to just find a big thing
[01:50:46.300 --> 01:50:48.940]   and come right up to it and like tap it on the shoulder
[01:50:48.940 --> 01:50:50.420]   or whatever, that would be quite feasible.
[01:50:50.420 --> 01:50:52.080]   Then they're not doing that.
[01:50:52.080 --> 01:50:57.080]   So my hypothesis is that one of the other questions there
[01:50:57.080 --> 01:50:59.540]   was, do they have a status hierarchy?
[01:50:59.540 --> 01:51:02.380]   And I think most animals on earth
[01:51:02.380 --> 01:51:05.060]   who are social animals have status hierarchy,
[01:51:05.060 --> 01:51:06.740]   and they would reasonably presume
[01:51:06.740 --> 01:51:08.340]   that we have a status hierarchy.
[01:51:08.340 --> 01:51:11.420]   - Take me to your leader.
[01:51:11.420 --> 01:51:15.340]   - Well, I would say their strategy is to be impressive
[01:51:15.340 --> 01:51:17.280]   and sort of get us to see them
[01:51:17.280 --> 01:51:19.280]   at the top of our status hierarchy.
[01:51:19.280 --> 01:51:23.660]   Just to, you know, that's how, for example,
[01:51:23.660 --> 01:51:25.660]   we domesticate dogs, right?
[01:51:25.660 --> 01:51:29.100]   We convince dogs we're the leader of their pack, right?
[01:51:29.100 --> 01:51:30.800]   And we domesticate many animals that way,
[01:51:30.800 --> 01:51:34.140]   but as we just swap into the top of their status hierarchy
[01:51:34.140 --> 01:51:36.800]   and we say, we're your top status animal,
[01:51:36.800 --> 01:51:39.600]   so you should do what we say, you should follow our lead.
[01:51:39.600 --> 01:51:42.120]   So the idea that would be,
[01:51:42.120 --> 01:51:44.400]   they are going to get us to do what they want
[01:51:44.400 --> 01:51:47.760]   by being top status.
[01:51:47.760 --> 01:51:49.640]   You know, all through history,
[01:51:49.640 --> 01:51:51.080]   kings and emperors, et cetera,
[01:51:51.080 --> 01:51:53.340]   have tried to impress their citizens and other people
[01:51:53.340 --> 01:51:55.400]   by having the bigger palace, the bigger parade,
[01:51:55.400 --> 01:51:57.560]   the bigger crown and diamonds, right?
[01:51:57.560 --> 01:52:00.120]   Whatever, maybe building a bigger pyramid, et cetera.
[01:52:00.120 --> 01:52:02.440]   Just, it's a very well-established trend
[01:52:02.440 --> 01:52:04.000]   that just be high status
[01:52:04.000 --> 01:52:05.840]   by being more impressive than the rest.
[01:52:05.840 --> 01:52:08.520]   - To push back, when there's an order of,
[01:52:08.520 --> 01:52:11.680]   several orders of magnitude of power differential,
[01:52:11.680 --> 01:52:13.280]   asymmetry of power,
[01:52:13.280 --> 01:52:15.840]   I feel like that status hierarchy no longer applies.
[01:52:15.840 --> 01:52:17.460]   It's like mimetic theory.
[01:52:17.460 --> 01:52:18.300]   It's like-
[01:52:18.300 --> 01:52:20.280]   - Most emperors are several orders of magnitude
[01:52:20.280 --> 01:52:23.080]   more powerful than any one member of their empire.
[01:52:23.080 --> 01:52:25.880]   - Let's increase that by even more.
[01:52:25.880 --> 01:52:29.760]   So like, if I'm interacting with ants, right?
[01:52:29.760 --> 01:52:32.640]   I no longer feel like I need to establish
[01:52:32.640 --> 01:52:34.120]   my power with ants.
[01:52:34.120 --> 01:52:36.640]   I actually want to lessen,
[01:52:36.640 --> 01:52:39.720]   I want to lower myself to the ants.
[01:52:39.720 --> 01:52:42.440]   I want to become the lowest possible ant
[01:52:42.440 --> 01:52:44.560]   so that they would welcome me.
[01:52:44.560 --> 01:52:46.980]   So I'm less concerned about them worshiping me.
[01:52:46.980 --> 01:52:49.240]   I'm more concerned about them welcoming me,
[01:52:49.240 --> 01:52:50.080]   integrating me into their world.
[01:52:50.080 --> 01:52:51.720]   - But it is important that you be non-threatening
[01:52:51.720 --> 01:52:52.640]   and that you be local.
[01:52:52.640 --> 01:52:53.480]   So I think, for example,
[01:52:53.480 --> 01:52:55.780]   if the aliens had done something really big in the sky,
[01:52:55.780 --> 01:52:57.780]   you know, a hundred light years away,
[01:52:57.780 --> 01:52:59.360]   that would be there, not here.
[01:52:59.360 --> 01:53:00.200]   - Yes.
[01:53:00.200 --> 01:53:01.280]   - And that could seem threatening.
[01:53:01.280 --> 01:53:03.720]   So I think their strategy to be the high status
[01:53:03.720 --> 01:53:05.140]   would have to be to be visible,
[01:53:05.140 --> 01:53:06.640]   but to be here and non-threatening.
[01:53:06.640 --> 01:53:09.440]   - I just don't know if it's obvious how to do that.
[01:53:09.440 --> 01:53:10.920]   Like, take your own perspective.
[01:53:10.920 --> 01:53:15.820]   You see a planet with relatively intelligent,
[01:53:15.820 --> 01:53:18.360]   like complex structures being formed,
[01:53:18.360 --> 01:53:20.200]   like, yeah, life forms.
[01:53:20.200 --> 01:53:23.800]   We could see this under, in Titan or something like that.
[01:53:23.800 --> 01:53:26.360]   The moon, you know, Europa.
[01:53:27.320 --> 01:53:29.980]   You start to see not just primitive bacterial life,
[01:53:29.980 --> 01:53:31.300]   but multicellular life,
[01:53:31.300 --> 01:53:33.620]   and it seems to form some very complicated
[01:53:33.620 --> 01:53:38.620]   cellular colonies, structures that they're dynamic.
[01:53:38.620 --> 01:53:40.360]   There's a lot of stuff going on.
[01:53:40.360 --> 01:53:45.360]   Some gigantic cellular automata type of construct.
[01:53:45.360 --> 01:53:50.100]   How do you make yourself known to them
[01:53:50.100 --> 01:53:54.640]   in an impressive fashion without destroying it?
[01:53:54.640 --> 01:53:56.920]   Like, we know how to destroy, potentially.
[01:53:56.920 --> 01:53:59.680]   - Right, so if you go touch stuff,
[01:53:59.680 --> 01:54:01.160]   you're likely to hurt it, right?
[01:54:01.160 --> 01:54:02.560]   There's a good risk of hurting something
[01:54:02.560 --> 01:54:05.080]   by getting too close and touching it and interacting, right?
[01:54:05.080 --> 01:54:06.940]   - Yeah, like landing on a White House lawn.
[01:54:06.940 --> 01:54:11.400]   - Right, so the claim is that their current strategy
[01:54:11.400 --> 01:54:13.740]   of hanging out at the periphery of our vision
[01:54:13.740 --> 01:54:16.240]   and just being very clearly physically impressive
[01:54:16.240 --> 01:54:19.360]   with very clear physically impressive abilities
[01:54:19.360 --> 01:54:23.060]   is at least a plausible strategy they might use
[01:54:23.060 --> 01:54:25.240]   to impress us and convince us
[01:54:25.240 --> 01:54:28.160]   that we're at the top of their status hierarchy.
[01:54:28.160 --> 01:54:30.560]   And I would say, if they came closer,
[01:54:30.560 --> 01:54:31.960]   not only would they risk hurting us
[01:54:31.960 --> 01:54:33.840]   in ways that they couldn't really understand,
[01:54:33.840 --> 01:54:36.440]   but more plausibly, they would reveal things
[01:54:36.440 --> 01:54:37.800]   about themselves we would hate.
[01:54:37.800 --> 01:54:41.120]   So if you look at how we treat other civilizations
[01:54:41.120 --> 01:54:42.920]   on Earth and other people,
[01:54:42.920 --> 01:54:46.080]   we are generally interested in foreigners
[01:54:46.080 --> 01:54:47.840]   and people from other plant lands,
[01:54:47.840 --> 01:54:49.240]   and we were generally interested
[01:54:49.240 --> 01:54:51.240]   in their varying cult customs, et cetera,
[01:54:51.240 --> 01:54:53.260]   until we find out that they do something
[01:54:53.260 --> 01:54:55.920]   that violates our moral norms, and then we hate them.
[01:54:55.920 --> 01:54:56.760]   (laughs)
[01:54:56.760 --> 01:54:59.280]   And these are aliens, for God's sakes, right?
[01:54:59.280 --> 01:55:00.360]   - Yeah. - There's just gonna be
[01:55:00.360 --> 01:55:01.760]   something about them that we hate.
[01:55:01.760 --> 01:55:04.520]   They eat babies, who knows what it is.
[01:55:04.520 --> 01:55:05.920]   But something they don't think is offensive,
[01:55:05.920 --> 01:55:07.840]   but that they think we might find.
[01:55:07.840 --> 01:55:10.040]   And so they would be risking a lot
[01:55:10.040 --> 01:55:11.720]   by revealing a lot about themselves.
[01:55:11.720 --> 01:55:13.720]   We would find something we hated.
[01:55:13.720 --> 01:55:16.200]   - Interesting, but do you resonate at all
[01:55:16.200 --> 01:55:20.040]   with mimetic theory where we only feel this way
[01:55:20.040 --> 01:55:21.920]   about things that are very close to us?
[01:55:21.920 --> 01:55:23.360]   So aliens are sufficiently different
[01:55:23.360 --> 01:55:25.940]   to where we'll be like fascinated,
[01:55:25.940 --> 01:55:27.880]   terrified or fascinated, but not like--
[01:55:27.880 --> 01:55:29.260]   - Right, but if they wanna be at the top
[01:55:29.260 --> 01:55:31.500]   of our status hierarchy to get us to follow them,
[01:55:31.500 --> 01:55:32.800]   they can't be too distant.
[01:55:32.800 --> 01:55:36.100]   They have to be close enough that we would see them that way.
[01:55:36.100 --> 01:55:38.180]   - But pretend to be close enough, right,
[01:55:38.180 --> 01:55:40.300]   and not reveal much, that mystery,
[01:55:40.300 --> 01:55:43.180]   that old Clint Eastwood cowboy say less.
[01:55:43.180 --> 01:55:45.420]   - I mean, the point is we're clever enough
[01:55:45.420 --> 01:55:46.820]   that we can figure out their agenda.
[01:55:46.820 --> 01:55:48.380]   That is just from the fact that we're here.
[01:55:48.380 --> 01:55:49.980]   If we see that they're here, we can figure out,
[01:55:49.980 --> 01:55:51.620]   oh, they want us not to expand.
[01:55:51.620 --> 01:55:53.780]   And look, they are this huge power
[01:55:53.780 --> 01:55:54.620]   and they're very impressive,
[01:55:54.620 --> 01:55:56.660]   and a lot of us don't wanna expand,
[01:55:56.660 --> 01:55:58.780]   so that could easily tip us over the edge
[01:55:58.780 --> 01:56:01.980]   toward we already wanted to not expand.
[01:56:01.980 --> 01:56:03.820]   We already wanted to be able to regulate
[01:56:03.820 --> 01:56:05.560]   and have a central community,
[01:56:05.560 --> 01:56:08.020]   and here are these very advanced, smart aliens
[01:56:08.020 --> 01:56:11.060]   who have survived for 100 million years,
[01:56:11.060 --> 01:56:13.260]   and they're telling us not to expand either.
[01:56:13.260 --> 01:56:16.100]   - (laughs) This is brilliant.
[01:56:16.100 --> 01:56:17.200]   I love this so much.
[01:56:17.200 --> 01:56:21.540]   Returning to panspermia siblings,
[01:56:21.540 --> 01:56:22.900]   just to clarify one thing,
[01:56:22.900 --> 01:56:28.800]   in that framework, who originated, who planted it?
[01:56:28.800 --> 01:56:33.060]   Would it be a grabby alien civilization
[01:56:33.060 --> 01:56:35.820]   that planted the siblings or no?
[01:56:35.820 --> 01:56:38.760]   - The simple scenario is that life started
[01:56:38.760 --> 01:56:42.460]   on some other planet billions of years ago,
[01:56:42.460 --> 01:56:45.220]   and it went through part of the stages of evolution
[01:56:45.220 --> 01:56:48.100]   to advanced life, but not all the way to advanced life,
[01:56:48.100 --> 01:56:50.980]   and then some rock hit it, grabbed a piece of it
[01:56:50.980 --> 01:56:52.660]   on the rock, and that rock drifted
[01:56:52.660 --> 01:56:55.260]   for maybe a million years until it happened
[01:56:55.260 --> 01:56:57.060]   to prawn the stellar nursery,
[01:56:57.060 --> 01:56:59.220]   where it then seeded many stars.
[01:56:59.220 --> 01:57:01.340]   - And something about that life,
[01:57:01.340 --> 01:57:02.860]   without being super advanced,
[01:57:02.860 --> 01:57:04.580]   it was nevertheless resilient
[01:57:04.580 --> 01:57:06.560]   to the harsh conditions of space.
[01:57:06.560 --> 01:57:08.860]   - There's some graphs that I've been impressed by
[01:57:08.860 --> 01:57:12.360]   that show sort of the level of genetic information
[01:57:12.360 --> 01:57:15.100]   in various kinds of life on the history of Earth,
[01:57:15.100 --> 01:57:17.780]   and basically, we are now more complex
[01:57:17.780 --> 01:57:19.620]   than the earlier life, but the earlier life
[01:57:19.620 --> 01:57:21.660]   was still pretty damn complex.
[01:57:21.660 --> 01:57:24.740]   And so if you actually project this log graph in history,
[01:57:24.740 --> 01:57:27.420]   it looks like it was many billions of years ago
[01:57:27.420 --> 01:57:29.620]   when you get down to zero, so plausibly,
[01:57:29.620 --> 01:57:31.500]   you could say there was just a lot of evolution
[01:57:31.500 --> 01:57:33.240]   that had to happen before you could get
[01:57:33.240 --> 01:57:35.180]   to the simplest life we've ever seen in history
[01:57:35.180 --> 01:57:37.540]   of life on Earth, was still pretty damn complicated.
[01:57:37.540 --> 01:57:40.660]   Okay, and so that's always been this puzzle.
[01:57:40.660 --> 01:57:43.860]   How could life get to this enormously complicated level
[01:57:43.860 --> 01:57:46.420]   in the short period it seems to
[01:57:46.420 --> 01:57:48.300]   at the beginning of Earth history?
[01:57:48.300 --> 01:57:51.780]   So where, you know, it's only 300 million years
[01:57:51.780 --> 01:57:55.180]   at most it appeared, and then it was really complicated
[01:57:55.180 --> 01:57:57.860]   at that point, so panspermia allows you
[01:57:57.860 --> 01:57:59.220]   to explain that complexity by saying,
[01:57:59.220 --> 01:58:01.860]   well, it's been another five billion years
[01:58:01.860 --> 01:58:04.700]   on another planet, going through lots of earlier stages
[01:58:04.700 --> 01:58:06.780]   where it was working its way up to the level
[01:58:06.780 --> 01:58:09.380]   of complexity you see at the beginning of Earth.
[01:58:09.380 --> 01:58:12.060]   - We'll try to talk about other ideas
[01:58:12.060 --> 01:58:16.080]   of the origin of life, but let me return to UFO sightings.
[01:58:16.080 --> 01:58:18.620]   Is there other explanations that are possible
[01:58:18.620 --> 01:58:22.180]   outside of panspermia siblings that can explain
[01:58:22.180 --> 01:58:24.700]   no grabby aliens in the sky,
[01:58:24.700 --> 01:58:27.340]   and yet alien arrival on Earth?
[01:58:27.340 --> 01:58:31.200]   - Well, the other categories of explanations
[01:58:31.200 --> 01:58:33.820]   that most people will use is, well, first of all,
[01:58:33.820 --> 01:58:37.020]   just mistakes, like, you know, you're confusing
[01:58:37.020 --> 01:58:40.780]   something ordinary for something mysterious, right?
[01:58:40.780 --> 01:58:43.380]   Or some sort of secret organization,
[01:58:43.380 --> 01:58:45.940]   like our government is secretly messing with us,
[01:58:45.940 --> 01:58:50.100]   and trying to do a false flag ops or whatever, right?
[01:58:50.100 --> 01:58:52.180]   They're trying to convince the Russians or the Chinese
[01:58:52.180 --> 01:58:53.820]   that there might be aliens and scare them
[01:58:53.820 --> 01:58:55.800]   into not attacking or something, right?
[01:58:55.800 --> 01:58:58.500]   'Cause if you know the history of World War II,
[01:58:58.500 --> 01:59:02.700]   say the US government did all these big fake operations
[01:59:02.700 --> 01:59:04.580]   where they were faking a lot of big things
[01:59:04.580 --> 01:59:07.060]   in order to mess with people, so that's a possibility.
[01:59:07.060 --> 01:59:09.740]   The government's been lying and faking things
[01:59:09.740 --> 01:59:14.060]   and paying people to lie about what they saw, et cetera.
[01:59:14.060 --> 01:59:16.380]   That's a plausible set of explanations
[01:59:16.380 --> 01:59:18.980]   for the range of sightings seen.
[01:59:18.980 --> 01:59:20.540]   And another explanation people offer
[01:59:20.540 --> 01:59:22.660]   is some other hidden organization on Earth,
[01:59:22.660 --> 01:59:25.340]   or some secret organization somewhere
[01:59:25.340 --> 01:59:27.220]   that has much more advanced capabilities
[01:59:27.220 --> 01:59:28.860]   than anybody's given it credit for.
[01:59:28.860 --> 01:59:31.020]   For some reason, it's been keeping secret.
[01:59:31.020 --> 01:59:34.060]   I mean, they all sound somewhat implausible,
[01:59:34.060 --> 01:59:35.460]   but again, we're looking for maybe
[01:59:35.460 --> 01:59:37.780]   one in a thousand sort of priors.
[01:59:37.780 --> 01:59:40.540]   Question is, you know, could they be
[01:59:40.540 --> 01:59:42.860]   in that level of plausibility?
[01:59:42.860 --> 01:59:44.060]   - Can we just linger on this?
[01:59:44.060 --> 01:59:47.460]   So you, first of all, you've written,
[01:59:47.460 --> 01:59:51.260]   talked about, thought about so many different topics.
[01:59:51.260 --> 01:59:52.940]   You're an incredible mind,
[01:59:52.940 --> 01:59:55.940]   and I just thank you for sitting down today.
[01:59:55.940 --> 01:59:59.740]   I'm almost like at a loss of which place we explore,
[01:59:59.740 --> 02:00:03.660]   but let me, on this topic, ask about conspiracy theories.
[02:00:03.660 --> 02:00:07.740]   'Cause you've written about institutions, authorities.
[02:00:11.100 --> 02:00:14.180]   What, this is a bit of a therapy session,
[02:00:14.180 --> 02:00:19.180]   but what do we make of conspiracy theories?
[02:00:19.180 --> 02:00:23.260]   - The phrase itself is pushing you in a direction, right?
[02:00:23.260 --> 02:00:26.020]   So clearly, in history, we have had
[02:00:26.020 --> 02:00:29.140]   many large coordinated keepings of secrets, right?
[02:00:29.140 --> 02:00:30.580]   Say the Manhattan Project, right?
[02:00:30.580 --> 02:00:32.020]   And there was, what, hundreds of thousands
[02:00:32.020 --> 02:00:34.420]   of people working on that over many years.
[02:00:34.420 --> 02:00:36.180]   But they kept it a secret, right?
[02:00:36.180 --> 02:00:38.460]   Clearly, many large military operations
[02:00:38.460 --> 02:00:43.300]   have kept things secret over even decades
[02:00:43.300 --> 02:00:45.380]   with many thousands of people involved.
[02:00:45.380 --> 02:00:48.620]   So clearly, it's possible to keep something secret
[02:00:48.620 --> 02:00:53.460]   over time periods, but the more people you involve
[02:00:53.460 --> 02:00:55.300]   and the more time you're assuming,
[02:00:55.300 --> 02:00:58.820]   and the more, the less centralized an organization
[02:00:58.820 --> 02:00:59.980]   or the less discipline they have,
[02:00:59.980 --> 02:01:01.420]   the harder it gets to believe.
[02:01:01.420 --> 02:01:04.620]   But we're just trying to calibrate, basically, in our minds
[02:01:04.620 --> 02:01:06.820]   which kind of secrets can be kept by which groups
[02:01:06.820 --> 02:01:08.860]   over what time periods for what purposes, right?
[02:01:08.860 --> 02:01:11.940]   - But let me, I don't have enough data.
[02:01:11.940 --> 02:01:16.580]   So I'm somebody, I hang out with people,
[02:01:16.580 --> 02:01:19.620]   and I love people, I love all things, really.
[02:01:19.620 --> 02:01:22.980]   And I just, I think that most people,
[02:01:22.980 --> 02:01:25.900]   even the assholes, have the capacity to be good,
[02:01:25.900 --> 02:01:28.500]   and they're beautiful, and I enjoy them.
[02:01:28.500 --> 02:01:30.860]   So the kind of data my brain,
[02:01:30.860 --> 02:01:32.400]   whatever the chemistry of my brain is
[02:01:32.400 --> 02:01:34.500]   that sees the beautiful in things,
[02:01:34.500 --> 02:01:37.860]   is maybe collecting a subset of data
[02:01:37.860 --> 02:01:42.460]   that doesn't allow me to intuit the competence
[02:01:42.460 --> 02:01:46.860]   that humans are able to achieve
[02:01:46.860 --> 02:01:49.460]   in constructing a conspiracy theory.
[02:01:49.460 --> 02:01:52.060]   So for example, one thing that people often talk about
[02:01:52.060 --> 02:01:55.340]   is intelligence agencies, this broad thing they say,
[02:01:55.340 --> 02:01:58.900]   the CIA, the FSB, the different, the British intelligence.
[02:01:58.900 --> 02:02:01.980]   I've, fortunate or unfortunate enough,
[02:02:01.980 --> 02:02:04.740]   never gotten a chance that I know of
[02:02:04.740 --> 02:02:07.440]   to talk to any member of those intelligence agencies.
[02:02:07.440 --> 02:02:13.340]   Nor take a peek behind the curtain,
[02:02:13.340 --> 02:02:15.380]   or the first curtain, I don't know how many levels
[02:02:15.380 --> 02:02:18.580]   of curtains there are, and so I can't intuit.
[02:02:18.580 --> 02:02:20.060]   But my interactions with government,
[02:02:20.060 --> 02:02:24.420]   I was funded by DOD and DARPA, and I've interacted,
[02:02:24.420 --> 02:02:25.660]   been to the Pentagon.
[02:02:25.660 --> 02:02:30.720]   With all due respect to my friends,
[02:02:30.720 --> 02:02:32.140]   lovely friends in government,
[02:02:32.140 --> 02:02:34.100]   and there are a lot of incredible people,
[02:02:34.100 --> 02:02:37.140]   but there is a very giant bureaucracy
[02:02:37.140 --> 02:02:41.620]   that sometimes suffocates the ingenuity of the human spirit,
[02:02:41.620 --> 02:02:43.140]   is one way I can put it.
[02:02:43.140 --> 02:02:46.940]   Meaning, they are, I just, it's difficult for me
[02:02:46.940 --> 02:02:50.100]   to imagine extreme competence at a scale
[02:02:50.100 --> 02:02:52.520]   of hundreds or thousands of human beings.
[02:02:52.520 --> 02:02:55.620]   Now, that doesn't mean, that's my very anecdotal data
[02:02:55.620 --> 02:02:56.900]   of the situation.
[02:02:56.900 --> 02:02:59.580]   And so I try to build up my intuition
[02:03:00.700 --> 02:03:04.880]   about centralized system of government,
[02:03:04.880 --> 02:03:07.900]   how much conspiracy is possible,
[02:03:07.900 --> 02:03:10.720]   how much the intelligence agencies,
[02:03:10.720 --> 02:03:14.100]   or some other source can generate
[02:03:14.100 --> 02:03:18.420]   sufficiently robust propaganda that controls the populace.
[02:03:18.420 --> 02:03:21.780]   If you look at World War II, as you mentioned,
[02:03:21.780 --> 02:03:25.380]   there have been extremely powerful propaganda machines
[02:03:25.380 --> 02:03:30.260]   on the side of Nazi Germany, on the side of the Soviet Union,
[02:03:30.260 --> 02:03:32.340]   on the side of the United States,
[02:03:32.340 --> 02:03:35.780]   and all these different mechanisms.
[02:03:35.780 --> 02:03:38.660]   Sometimes they control the free press
[02:03:38.660 --> 02:03:40.220]   through social pressures.
[02:03:40.220 --> 02:03:42.940]   Sometimes they control the press
[02:03:42.940 --> 02:03:45.980]   through the threat of violence,
[02:03:45.980 --> 02:03:48.100]   as you do in authoritarian regimes.
[02:03:48.100 --> 02:03:49.500]   Sometimes it's like deliberately,
[02:03:49.500 --> 02:03:52.420]   the dictator writing the news,
[02:03:52.420 --> 02:03:54.460]   the headlines and literally announcing it.
[02:03:54.460 --> 02:03:59.020]   And something about human psychology
[02:03:59.020 --> 02:04:03.580]   forces you to embrace the narrative
[02:04:03.580 --> 02:04:05.020]   and believe the narrative.
[02:04:05.020 --> 02:04:07.620]   And at scale, that becomes reality
[02:04:07.620 --> 02:04:11.360]   when the initial spark was just a propaganda thought
[02:04:11.360 --> 02:04:13.300]   in a single individual's mind.
[02:04:13.300 --> 02:04:18.300]   So I can't necessarily intuit of what's possible,
[02:04:18.300 --> 02:04:23.380]   but I'm skeptical of the power of human institutions
[02:04:23.380 --> 02:04:26.480]   to construct conspiracy theories
[02:04:26.480 --> 02:04:28.860]   that cause suffering at scale.
[02:04:28.860 --> 02:04:30.780]   Especially in this modern age,
[02:04:30.780 --> 02:04:33.340]   when information is becoming more and more accessible
[02:04:33.340 --> 02:04:34.420]   by the populace.
[02:04:34.420 --> 02:04:37.700]   Anyway, I don't know if you can elucidate--
[02:04:37.700 --> 02:04:39.020]   - You said it's cause suffering at scale,
[02:04:39.020 --> 02:04:40.660]   but of course, say during wartime,
[02:04:40.660 --> 02:04:43.300]   the people who are managing the various conspiracies
[02:04:43.300 --> 02:04:45.300]   like D-Day or Manhattan Project,
[02:04:45.300 --> 02:04:49.260]   they thought that their conspiracy was avoiding harm
[02:04:49.260 --> 02:04:51.040]   rather than causing harm.
[02:04:51.040 --> 02:04:52.560]   So if you can get a lot of people to think
[02:04:52.560 --> 02:04:56.340]   that supporting the conspiracy is helpful,
[02:04:56.340 --> 02:04:58.820]   then a lot more might do that.
[02:04:58.820 --> 02:05:00.300]   And there's just a lot of things
[02:05:00.300 --> 02:05:02.980]   that people just don't wanna see.
[02:05:02.980 --> 02:05:04.980]   So if you can make your conspiracy the sort of thing
[02:05:04.980 --> 02:05:06.860]   that people wouldn't wanna talk about anyway,
[02:05:06.860 --> 02:05:08.620]   even if they knew about it,
[02:05:08.620 --> 02:05:10.740]   you're most of the way there.
[02:05:10.740 --> 02:05:13.020]   So I have learned over the years,
[02:05:13.020 --> 02:05:15.280]   many things that most ordinary people
[02:05:15.280 --> 02:05:17.320]   should be interested in, but somehow don't know,
[02:05:17.320 --> 02:05:19.620]   even though the data has been very widespread.
[02:05:19.620 --> 02:05:21.740]   So I have this book, "The Elephant and the Brain,"
[02:05:21.740 --> 02:05:23.940]   and one of the chapters is there on medicine.
[02:05:23.940 --> 02:05:27.140]   And basically, most people seem ignorant
[02:05:27.140 --> 02:05:29.860]   of the very basic fact that when we do randomized trials
[02:05:29.860 --> 02:05:32.540]   where we give some people more medicine than others,
[02:05:32.540 --> 02:05:35.100]   the people who get more medicine are not healthier.
[02:05:35.100 --> 02:05:38.940]   Just overall, in general, just like induce somebody
[02:05:38.940 --> 02:05:40.580]   to get more medicine because you just give them
[02:05:40.580 --> 02:05:42.860]   more budget to buy medicine, say.
[02:05:42.860 --> 02:05:45.820]   Not a specific medicine, just the whole category.
[02:05:45.820 --> 02:05:47.360]   And you would think that would be something
[02:05:47.360 --> 02:05:49.100]   most people should know about medicine.
[02:05:49.100 --> 02:05:51.940]   You might even think that would be a conspiracy theory
[02:05:51.940 --> 02:05:53.100]   to think that would be hidden,
[02:05:53.100 --> 02:05:55.860]   but in fact, most people never learn that fact.
[02:05:55.860 --> 02:06:00.860]   - So just to clarify, just a general high-level statement,
[02:06:00.860 --> 02:06:03.580]   the more medicine you take, the less healthy you are.
[02:06:03.580 --> 02:06:07.340]   - Randomized experiments don't find that fact.
[02:06:07.340 --> 02:06:10.220]   Do not find that more medicine makes you more healthy.
[02:06:10.220 --> 02:06:11.520]   They're just no connection.
[02:06:11.520 --> 02:06:15.060]   In randomized experiments, there's no relationship
[02:06:15.060 --> 02:06:16.540]   between more medicine and being healthier.
[02:06:16.540 --> 02:06:18.300]   - So it's not a negative relationship,
[02:06:18.300 --> 02:06:19.740]   but it's just no relationship.
[02:06:19.740 --> 02:06:21.020]   - Right.
[02:06:21.020 --> 02:06:25.080]   - And so the conspiracy theory would say
[02:06:25.080 --> 02:06:27.100]   that the businesses that sell you medicine
[02:06:27.100 --> 02:06:28.820]   don't want you to know that fact.
[02:06:28.820 --> 02:06:32.100]   And then you're saying that there's also part of this
[02:06:32.100 --> 02:06:33.860]   is that people just don't wanna know.
[02:06:33.860 --> 02:06:35.300]   - They just don't wanna know.
[02:06:35.300 --> 02:06:36.300]   And so they don't learn this.
[02:06:36.300 --> 02:06:38.620]   So I've lived in the Washington area
[02:06:38.620 --> 02:06:39.940]   for several decades now,
[02:06:39.940 --> 02:06:41.660]   reading the Washington Post regularly.
[02:06:41.660 --> 02:06:44.940]   Every week there was a special section
[02:06:44.940 --> 02:06:45.840]   on health and medicine.
[02:06:45.840 --> 02:06:48.500]   It never was mentioned in that section of the paper
[02:06:48.500 --> 02:06:50.740]   in all the 20 years I read that.
[02:06:50.740 --> 02:06:52.060]   - So do you think there is some truth
[02:06:52.080 --> 02:06:55.320]   to this caricatured blue pill, red pill,
[02:06:55.320 --> 02:06:58.160]   where most people don't want to know the truth?
[02:06:58.160 --> 02:07:00.560]   - There are many things about which people
[02:07:00.560 --> 02:07:03.160]   don't want to know certain kinds of truths.
[02:07:03.160 --> 02:07:04.000]   - Yeah.
[02:07:04.000 --> 02:07:06.640]   - That is bad looking truths, truths that discouraging,
[02:07:06.640 --> 02:07:09.040]   truths that sort of take away the justification
[02:07:09.040 --> 02:07:11.320]   for things they feel passionate about.
[02:07:11.320 --> 02:07:14.200]   - Do you think that's a bad aspect of human nature?
[02:07:14.200 --> 02:07:16.360]   That's something we should try to overcome?
[02:07:16.360 --> 02:07:20.060]   - Well, as we discussed, my first priority
[02:07:20.060 --> 02:07:21.620]   is to just tell people about it,
[02:07:21.620 --> 02:07:23.600]   to do the analysis and the cold facts
[02:07:23.600 --> 02:07:24.680]   of what's actually happening,
[02:07:24.680 --> 02:07:27.260]   and then to try to be careful about how we can improve.
[02:07:27.260 --> 02:07:28.960]   So our book, "The Elephant in the Brain,"
[02:07:28.960 --> 02:07:30.280]   coauthored with Kevin Simler,
[02:07:30.280 --> 02:07:33.360]   is about hidden motives in everyday life.
[02:07:33.360 --> 02:07:36.080]   And our first priority there is just to explain to you
[02:07:36.080 --> 02:07:38.360]   what are the things that you are not looking at
[02:07:38.360 --> 02:07:40.540]   that you are reluctant to look at.
[02:07:40.540 --> 02:07:42.200]   And many people try to take that book
[02:07:42.200 --> 02:07:43.760]   as a self-help book where they're trying
[02:07:43.760 --> 02:07:45.520]   to improve themselves and make sure
[02:07:45.520 --> 02:07:46.840]   they look at more things.
[02:07:46.840 --> 02:07:49.300]   And that often goes badly because it's harder
[02:07:49.300 --> 02:07:51.240]   to actually do that than you think.
[02:07:51.240 --> 02:07:52.080]   - Yeah.
[02:07:52.080 --> 02:07:54.000]   - And so, but we at least want you to know
[02:07:54.000 --> 02:07:57.160]   that this truth is available if you want to learn about it.
[02:07:57.160 --> 02:07:59.440]   - It's the Nietzsche, "If you gaze long into the abyss,
[02:07:59.440 --> 02:08:01.280]   the abyss gazes into you."
[02:08:01.280 --> 02:08:03.580]   Let's talk about this "Elephant in the Brain."
[02:08:03.580 --> 02:08:05.960]   Amazing book.
[02:08:05.960 --> 02:08:08.440]   "The Elephant in the Room" is, quote,
[02:08:08.440 --> 02:08:10.240]   "An important issue that people are reluctant
[02:08:10.240 --> 02:08:12.920]   to acknowledge or address a social taboo.
[02:08:12.920 --> 02:08:15.400]   The elephant in the brain is an important
[02:08:15.400 --> 02:08:18.440]   but unacknowledged feature of how our mind works,
[02:08:18.440 --> 02:08:21.360]   an introspective taboo.
[02:08:21.360 --> 02:08:25.360]   You describe selfishness and self-deception
[02:08:25.360 --> 02:08:29.760]   as the core or some of the core elephants,
[02:08:29.760 --> 02:08:33.480]   as some of the elephant offspring in the brain.
[02:08:33.480 --> 02:08:35.720]   Selfishness and self-deception."
[02:08:35.720 --> 02:08:38.200]   All right.
[02:08:38.200 --> 02:08:39.240]   Can you explain?
[02:08:39.240 --> 02:08:43.240]   Can you explain why these are the taboos
[02:08:43.240 --> 02:08:48.240]   in our brain that we don't want to acknowledge to ourselves?
[02:08:48.240 --> 02:08:51.040]   - In your conscious mind, the one that's listening to me
[02:08:51.040 --> 02:08:53.000]   that I'm talking to at the moment,
[02:08:53.000 --> 02:08:55.360]   you like to think of yourself as the president
[02:08:55.360 --> 02:08:59.080]   or king of your mind, ruling over all that you see,
[02:08:59.080 --> 02:09:01.440]   issuing commands that immediately obeyed.
[02:09:01.440 --> 02:09:04.720]   You are instead better understood
[02:09:04.720 --> 02:09:07.680]   as the press secretary of your brain.
[02:09:07.680 --> 02:09:11.600]   You don't make decisions, you justify them to an audience.
[02:09:11.600 --> 02:09:14.520]   That's what your conscious mind is for.
[02:09:15.960 --> 02:09:19.440]   You watch what you're doing and you try to come up
[02:09:19.440 --> 02:09:21.440]   with stories that explain what you're doing
[02:09:21.440 --> 02:09:24.800]   so that you can avoid accusations of violating norms.
[02:09:24.800 --> 02:09:28.280]   So humans compared to most other animals have norms
[02:09:28.280 --> 02:09:30.640]   and this allows us to manage larger groups
[02:09:30.640 --> 02:09:33.560]   with our morals and norms about what we should
[02:09:33.560 --> 02:09:35.000]   or shouldn't be doing.
[02:09:35.000 --> 02:09:38.040]   This is so important to us that we needed
[02:09:38.040 --> 02:09:40.000]   to be constantly watching what we were doing
[02:09:40.000 --> 02:09:42.560]   in order to make sure we had a good story
[02:09:42.560 --> 02:09:44.080]   to avoid norm violations.
[02:09:44.080 --> 02:09:45.960]   So many norms are about motives.
[02:09:45.960 --> 02:09:48.720]   So if I hit you on purpose, that's a big violation.
[02:09:48.720 --> 02:09:50.880]   If I hit you accidentally, that's okay.
[02:09:50.880 --> 02:09:53.120]   I need to be able to explain why it was an accident
[02:09:53.120 --> 02:09:54.160]   and not on purpose.
[02:09:54.160 --> 02:09:57.080]   - So where does that need come from
[02:09:57.080 --> 02:09:59.000]   for your own self-preservation?
[02:09:59.000 --> 02:10:01.280]   - Right, so humans have norms and we have the norm
[02:10:01.280 --> 02:10:03.200]   that if we see anybody violating a norm,
[02:10:03.200 --> 02:10:05.280]   we need to tell other people and then coordinate
[02:10:05.280 --> 02:10:09.280]   to make them stop and punish them for violating.
[02:10:09.280 --> 02:10:12.240]   So such benefits are strong enough and severe enough
[02:10:12.240 --> 02:10:15.920]   that we each want to avoid being successfully accused
[02:10:15.920 --> 02:10:16.880]   of violating norms.
[02:10:16.880 --> 02:10:20.600]   So for example, hitting someone on purpose
[02:10:20.600 --> 02:10:22.280]   is a big clear norm violation.
[02:10:22.280 --> 02:10:24.760]   If we do it consistently, we may be thrown out of the group
[02:10:24.760 --> 02:10:26.640]   and that would mean we would die.
[02:10:26.640 --> 02:10:29.440]   Okay, so we need to be able to convince people
[02:10:29.440 --> 02:10:32.520]   we are not going around hitting people on purpose.
[02:10:32.520 --> 02:10:35.360]   If somebody happens to be at the other end of our fist
[02:10:35.360 --> 02:10:38.240]   and their face connects, that was an accident
[02:10:38.240 --> 02:10:40.200]   and we need to be able to explain that.
[02:10:41.780 --> 02:10:44.720]   And similarly for many other norms humans have,
[02:10:44.720 --> 02:10:46.200]   we are serious about these norms
[02:10:46.200 --> 02:10:48.120]   and we don't want people to violate.
[02:10:48.120 --> 02:10:49.920]   We find them violating, we're going to accuse them.
[02:10:49.920 --> 02:10:52.400]   But many norms have a motive component
[02:10:52.400 --> 02:10:55.320]   and so we are trying to explain ourselves
[02:10:55.320 --> 02:10:57.280]   and make sure we have a good motive story
[02:10:57.280 --> 02:11:01.000]   about everything we do, which is why we're constantly
[02:11:01.000 --> 02:11:02.240]   trying to explain what we're doing
[02:11:02.240 --> 02:11:04.240]   and that's what your conscious mind is doing.
[02:11:04.240 --> 02:11:07.480]   It is trying to make sure you've got a good motive story
[02:11:07.480 --> 02:11:08.720]   for everything you're doing.
[02:11:08.720 --> 02:11:11.240]   And that's why you don't know why you really do things.
[02:11:11.240 --> 02:11:13.880]   What you know is what the good story is
[02:11:13.880 --> 02:11:15.480]   about why you've been doing things.
[02:11:15.480 --> 02:11:17.320]   - And that's the self-deception.
[02:11:17.320 --> 02:11:19.380]   And you're saying that there is a machine,
[02:11:19.380 --> 02:11:22.840]   the actual dictator is selfish.
[02:11:22.840 --> 02:11:24.520]   And then you're just the press secretary
[02:11:24.520 --> 02:11:26.680]   who's desperately doesn't want to get fired
[02:11:26.680 --> 02:11:30.240]   and is justifying all of the decisions of the dictator.
[02:11:30.240 --> 02:11:32.240]   And that's the self-deception.
[02:11:32.240 --> 02:11:35.300]   - Right, now most people actually are willing to believe
[02:11:35.300 --> 02:11:36.720]   that this is true in the abstract.
[02:11:36.720 --> 02:11:39.200]   So our book has been classified as psychology
[02:11:39.200 --> 02:11:40.880]   and it was reviewed by psychologists
[02:11:40.880 --> 02:11:43.840]   and the basic way that psychology referees
[02:11:43.840 --> 02:11:47.080]   and reviewers responded is to say this is well known.
[02:11:47.080 --> 02:11:48.840]   Most people accept that there's a fair bit
[02:11:48.840 --> 02:11:49.720]   of self-deception.
[02:11:49.720 --> 02:11:52.080]   - But they don't want to accept it about themselves.
[02:11:52.080 --> 02:11:52.920]   - Well, they don't want to accept it
[02:11:52.920 --> 02:11:55.180]   about the particular topics that we talk about.
[02:11:55.180 --> 02:11:57.800]   So people accept the idea in the abstract
[02:11:57.800 --> 02:11:59.160]   that they might be self-deceived
[02:11:59.160 --> 02:12:01.920]   or that they might not be honest about various things.
[02:12:01.920 --> 02:12:04.200]   But that hasn't penetrated into the literatures
[02:12:04.200 --> 02:12:06.000]   where people are explaining particular things
[02:12:06.000 --> 02:12:08.440]   like why we go to school, why we go to the doctor,
[02:12:08.440 --> 02:12:10.120]   why we vote, et cetera.
[02:12:10.120 --> 02:12:13.080]   So our book is mainly about 10 areas of life
[02:12:13.080 --> 02:12:15.160]   and explaining about in each area
[02:12:15.160 --> 02:12:17.760]   what our actual motives there are.
[02:12:17.760 --> 02:12:21.640]   And people who study those things have not admitted
[02:12:21.640 --> 02:12:25.480]   that hidden motives are explaining those particular areas.
[02:12:25.480 --> 02:12:28.040]   - So they haven't taken the leap from theoretical psychology
[02:12:28.040 --> 02:12:30.240]   to actual public policy.
[02:12:30.240 --> 02:12:31.080]   - Exactly.
[02:12:31.080 --> 02:12:32.720]   - And economics and all that kind of stuff.
[02:12:32.720 --> 02:12:34.680]   Well, let me just linger on this
[02:12:34.680 --> 02:12:39.680]   and bring up my old friends Zingman Freud
[02:12:39.960 --> 02:12:41.440]   and Carl Jung.
[02:12:41.440 --> 02:12:46.440]   So how vast is this landscape of the unconscious mind,
[02:12:46.440 --> 02:12:50.860]   the power and the scope of the dictator?
[02:12:50.860 --> 02:12:53.680]   Is it only dark there?
[02:12:53.680 --> 02:12:55.720]   Is it some light?
[02:12:55.720 --> 02:12:56.880]   Is there some love?
[02:12:56.880 --> 02:12:59.480]   - The vast majority of what's happening in your head
[02:12:59.480 --> 02:13:00.660]   you are unaware of.
[02:13:00.660 --> 02:13:03.860]   So in a literal sense, the unconscious,
[02:13:03.860 --> 02:13:06.760]   the aspects of your mind that you're not conscious of
[02:13:06.760 --> 02:13:09.160]   is the overwhelming majority.
[02:13:09.160 --> 02:13:11.960]   But that's just true in a literal engineering sense.
[02:13:11.960 --> 02:13:13.920]   Your mind is doing lots of low-level things
[02:13:13.920 --> 02:13:15.600]   and you just can't be consciously aware
[02:13:15.600 --> 02:13:16.840]   of all that low-level stuff.
[02:13:16.840 --> 02:13:18.120]   But there's plenty of room there
[02:13:18.120 --> 02:13:20.080]   for lots of things you're not aware of.
[02:13:20.080 --> 02:13:22.920]   - But can we try to shine a light
[02:13:22.920 --> 02:13:25.200]   at the things we're unaware of,
[02:13:25.200 --> 02:13:26.600]   specifically, now again,
[02:13:26.600 --> 02:13:29.920]   staying with the philosophical psychology side for a moment.
[02:13:29.920 --> 02:13:32.800]   You know, can you shine a light in the Jungian shadow?
[02:13:32.800 --> 02:13:35.600]   Can you, what's going on there?
[02:13:35.600 --> 02:13:37.360]   What is this machine like?
[02:13:37.360 --> 02:13:40.440]   Like what level of thoughts are happening there?
[02:13:40.440 --> 02:13:43.320]   Is it something that we can even interpret?
[02:13:43.320 --> 02:13:46.000]   If we somehow could visualize it,
[02:13:46.000 --> 02:13:47.680]   is it something that's human interpretable?
[02:13:47.680 --> 02:13:49.320]   Or is it just a kind of chaos
[02:13:49.320 --> 02:13:52.300]   of like monitoring different systems in the body,
[02:13:52.300 --> 02:13:56.100]   making sure you're happy, making sure you're fed,
[02:13:56.100 --> 02:13:57.640]   all those kind of basic forces
[02:13:57.640 --> 02:13:59.880]   that form abstractions on top of each other
[02:13:59.880 --> 02:14:01.800]   and they're not introspective at all?
[02:14:01.800 --> 02:14:03.960]   - We humans are social creatures.
[02:14:03.960 --> 02:14:05.980]   Plausibly being social is the main reason
[02:14:05.980 --> 02:14:07.880]   we have these unusually large brains.
[02:14:07.880 --> 02:14:10.940]   Therefore, most of our brain is devoted to being social.
[02:14:10.940 --> 02:14:14.360]   And so the things we are very obsessed with
[02:14:14.360 --> 02:14:16.680]   and constantly paying attention to are,
[02:14:16.680 --> 02:14:18.880]   how do I look to others?
[02:14:18.880 --> 02:14:20.480]   What would others think of me
[02:14:20.480 --> 02:14:22.120]   if they knew these various things
[02:14:22.120 --> 02:14:23.240]   they might learn about me?
[02:14:23.240 --> 02:14:25.080]   - So that's close to being fundamental
[02:14:25.080 --> 02:14:26.560]   to what it means to be human,
[02:14:26.560 --> 02:14:28.040]   is caring what others think.
[02:14:28.040 --> 02:14:31.400]   - Right, to be trying to present a story
[02:14:31.400 --> 02:14:33.240]   that would be okay for what other things,
[02:14:33.240 --> 02:14:35.320]   but we're very constantly thinking,
[02:14:35.320 --> 02:14:36.820]   what do other people think?
[02:14:36.820 --> 02:14:39.060]   - So let me ask you this question then
[02:14:39.060 --> 02:14:44.060]   about you, Robin Hanson, who many places,
[02:14:44.060 --> 02:14:48.180]   sometimes for fun, sometimes as a basic statement
[02:14:48.180 --> 02:14:50.540]   of principle, likes to disagree
[02:14:50.540 --> 02:14:53.220]   with what the majority of people think.
[02:14:53.220 --> 02:14:57.220]   So how do you explain,
[02:14:57.220 --> 02:15:01.220]   how are you self-deceiving yourself in this task
[02:15:01.220 --> 02:15:03.140]   and how are you being self,
[02:15:03.140 --> 02:15:06.540]   how's your, like why is the dictator manipulating you
[02:15:06.540 --> 02:15:08.700]   inside your head to be self-critical?
[02:15:08.700 --> 02:15:13.560]   Like there's norms, why do you wanna stand out in this way?
[02:15:13.560 --> 02:15:15.560]   Why do you want to challenge the norms in this way?
[02:15:15.560 --> 02:15:17.480]   - Almost by definition, I can't tell you
[02:15:17.480 --> 02:15:19.680]   what I'm deceiving myself about,
[02:15:19.680 --> 02:15:22.400]   but the more practical strategy that's quite feasible
[02:15:22.400 --> 02:15:24.760]   is to ask about what are typical things
[02:15:24.760 --> 02:15:26.720]   that most people deceive themselves about
[02:15:26.720 --> 02:15:29.360]   and then to own up to those particular things.
[02:15:29.360 --> 02:15:31.960]   - Sure, what's a good one?
[02:15:31.960 --> 02:15:35.360]   - So for example, I can very much acknowledge
[02:15:35.360 --> 02:15:38.560]   that I would like to be well thought of,
[02:15:38.560 --> 02:15:43.360]   that I would be seeking attention and glory
[02:15:43.360 --> 02:15:47.480]   and praise from my intellectual work
[02:15:47.480 --> 02:15:49.380]   and that that would be a major agenda
[02:15:49.380 --> 02:15:51.880]   driving my intellectual attempts.
[02:15:51.880 --> 02:15:55.980]   So if there were topics that other people
[02:15:55.980 --> 02:15:57.420]   would find less interesting,
[02:15:57.420 --> 02:15:59.240]   I might be less interested in those
[02:15:59.240 --> 02:16:00.680]   for that reason, for example.
[02:16:00.680 --> 02:16:03.080]   I might want to find topics where other people are interested
[02:16:03.080 --> 02:16:06.680]   and I might want to go for the glory
[02:16:06.680 --> 02:16:10.600]   of finding a big insight rather than a small one
[02:16:10.600 --> 02:16:15.520]   and maybe one that was especially surprising.
[02:16:15.520 --> 02:16:16.880]   That's also of course consistent
[02:16:16.880 --> 02:16:18.920]   with some more ideal concept
[02:16:18.920 --> 02:16:20.680]   of what an intellectual should be,
[02:16:20.680 --> 02:16:25.120]   but most intellectuals are relatively risk-averse.
[02:16:25.120 --> 02:16:28.540]   They are in some local intellectual tradition
[02:16:28.540 --> 02:16:30.160]   and they are adding to that
[02:16:30.160 --> 02:16:32.160]   and they are staying conforming
[02:16:32.160 --> 02:16:33.780]   to the sort of usual assumptions
[02:16:33.780 --> 02:16:36.320]   and usual accepted beliefs and practices
[02:16:36.320 --> 02:16:38.040]   of a particular area
[02:16:38.040 --> 02:16:40.540]   so that they can be accepted in that area
[02:16:40.540 --> 02:16:43.280]   and treated as part of the community.
[02:16:43.280 --> 02:16:47.380]   But you might think for the purpose
[02:16:47.380 --> 02:16:49.120]   of the larger intellectual project
[02:16:49.120 --> 02:16:51.160]   of understanding the world better,
[02:16:51.160 --> 02:16:53.200]   people should be less eager
[02:16:53.200 --> 02:16:55.480]   to just add a little bit to some tradition
[02:16:55.480 --> 02:16:57.240]   and they should be looking for what's neglected
[02:16:57.240 --> 02:16:59.400]   between the major traditions and major questions.
[02:16:59.400 --> 02:17:00.840]   They should be looking for assumptions
[02:17:00.840 --> 02:17:02.520]   maybe we're making that are wrong.
[02:17:02.520 --> 02:17:04.400]   They should be looking at ways,
[02:17:04.400 --> 02:17:05.880]   things that are very surprising,
[02:17:05.880 --> 02:17:07.240]   like things that would be,
[02:17:07.240 --> 02:17:10.120]   you would have thought a priori unlikely
[02:17:10.120 --> 02:17:11.640]   that once you are convinced of it,
[02:17:11.640 --> 02:17:13.480]   you find that to be very important
[02:17:13.480 --> 02:17:16.200]   and a big update, right?
[02:17:16.200 --> 02:17:21.200]   So you could say that one motivation I might have
[02:17:21.200 --> 02:17:26.480]   is less motivated to be sort of comfortably accepted
[02:17:26.480 --> 02:17:28.340]   into some particular intellectual community
[02:17:28.340 --> 02:17:30.400]   and more willing to just go for these
[02:17:30.400 --> 02:17:33.480]   more fundamental long shots
[02:17:33.480 --> 02:17:36.760]   that should be very important if you could find them.
[02:17:36.760 --> 02:17:39.680]   - Which would, if you can find them,
[02:17:39.680 --> 02:17:43.680]   would get you appreciated-- - Attention, respect.
[02:17:43.680 --> 02:17:45.960]   - Across a larger number of people
[02:17:45.960 --> 02:17:49.000]   across the longer time span of history.
[02:17:49.000 --> 02:17:49.840]   - Right.
[02:17:49.840 --> 02:17:52.840]   - So like maybe the small local community
[02:17:52.840 --> 02:17:54.640]   will say you suck.
[02:17:54.640 --> 02:17:55.480]   - Right.
[02:17:55.480 --> 02:17:56.720]   - You must conform,
[02:17:56.720 --> 02:18:00.300]   but the larger community will see the brilliance
[02:18:00.300 --> 02:18:03.540]   of you breaking out of the cage of the small conformity
[02:18:03.540 --> 02:18:05.340]   into a larger cage.
[02:18:05.340 --> 02:18:07.580]   It's always a bigger, there's always a bigger cage,
[02:18:07.580 --> 02:18:10.140]   and then you'll be remembered by more.
[02:18:10.140 --> 02:18:14.300]   Yeah, also that explains your choice of colorful shirt
[02:18:14.300 --> 02:18:15.860]   that looks great in a black background,
[02:18:15.860 --> 02:18:17.940]   so you definitely stand out.
[02:18:17.940 --> 02:18:20.860]   - Right, now of course, you could say,
[02:18:20.860 --> 02:18:22.500]   well, you could get all this attention
[02:18:22.500 --> 02:18:26.100]   by making false claims of dramatic improvement.
[02:18:26.100 --> 02:18:26.940]   - Sure.
[02:18:26.940 --> 02:18:28.460]   - And then wouldn't that be much easier
[02:18:28.460 --> 02:18:30.180]   than actually working through all the details--
[02:18:30.180 --> 02:18:31.020]   - Why not?
[02:18:31.020 --> 02:18:31.840]   - To make true claims, right?
[02:18:31.840 --> 02:18:34.860]   - Let me ask the press secretary, why not?
[02:18:34.860 --> 02:18:37.680]   Why, so of course, you spoke several times
[02:18:37.680 --> 02:18:40.640]   about how much you value truth and the pursuit of truth.
[02:18:40.640 --> 02:18:42.240]   That's a very nice narrative.
[02:18:42.240 --> 02:18:43.080]   - Right.
[02:18:43.080 --> 02:18:46.460]   - Hitler and Stalin also talked about the value of truth.
[02:18:46.460 --> 02:18:49.440]   Do you worry when you introspect,
[02:18:49.440 --> 02:18:52.860]   as broadly as all humans might,
[02:18:52.860 --> 02:18:54.800]   that it becomes a drug?
[02:18:55.800 --> 02:19:00.360]   This being a martyr,
[02:19:00.360 --> 02:19:03.620]   being the person who points out
[02:19:03.620 --> 02:19:05.740]   that the emperor wears no clothes,
[02:19:05.740 --> 02:19:09.280]   even when the emperor is obviously dressed,
[02:19:09.280 --> 02:19:12.360]   just to be the person who points out
[02:19:12.360 --> 02:19:14.760]   that the emperor is wearing no clothes.
[02:19:14.760 --> 02:19:15.960]   Do you think about that?
[02:19:15.960 --> 02:19:22.720]   - So I think the standards you hold yourself to
[02:19:22.960 --> 02:19:25.360]   are dependent on the audience you have in mind.
[02:19:25.360 --> 02:19:28.120]   So if you think of your audience
[02:19:28.120 --> 02:19:32.500]   as relatively easily fooled or relatively gullible,
[02:19:32.500 --> 02:19:35.400]   then you won't bother to generate more complicated,
[02:19:35.400 --> 02:19:39.240]   deep arguments and structures and evidence
[02:19:39.240 --> 02:19:42.280]   to persuade somebody who has higher standards
[02:19:42.280 --> 02:19:43.520]   because why bother?
[02:19:43.520 --> 02:19:46.340]   You can get away with something much easier.
[02:19:46.340 --> 02:19:49.200]   And of course, if you are, say, a salesperson,
[02:19:49.200 --> 02:19:50.440]   or you make money on sales,
[02:19:50.440 --> 02:19:53.480]   then you don't need to convince the top few percent
[02:19:53.480 --> 02:19:54.700]   of the most sharp customers.
[02:19:54.700 --> 02:19:57.440]   You can just go for the bottom 60%
[02:19:57.440 --> 02:19:58.640]   of the most gullible customers
[02:19:58.640 --> 02:20:01.040]   and make plenty of sales, right?
[02:20:01.040 --> 02:20:05.280]   So I think intellectuals have to vary.
[02:20:05.280 --> 02:20:06.680]   One of the main ways intellectuals vary
[02:20:06.680 --> 02:20:08.440]   is in who is their audience in their mind.
[02:20:08.440 --> 02:20:10.840]   Who are they trying to impress?
[02:20:10.840 --> 02:20:12.440]   Is it the people down the hall?
[02:20:12.440 --> 02:20:14.840]   Is it the people who are reading their Twitter feed?
[02:20:14.840 --> 02:20:16.340]   Is it their parents?
[02:20:16.340 --> 02:20:18.000]   Is it their high school teacher?
[02:20:19.000 --> 02:20:23.380]   Or is it Einstein and Freud and Socrates, right?
[02:20:23.380 --> 02:20:27.560]   So I think those of us who are especially arrogant,
[02:20:27.560 --> 02:20:31.040]   especially think that we're really big shot
[02:20:31.040 --> 02:20:33.120]   or have a chance at being a really big shot,
[02:20:33.120 --> 02:20:35.880]   we were naturally gonna pick the big shot audience
[02:20:35.880 --> 02:20:36.720]   that we can.
[02:20:36.720 --> 02:20:39.520]   We're gonna be trying to impress Socrates and Einstein.
[02:20:39.520 --> 02:20:41.680]   - Is that why you hang out with Tyler Cohen a lot?
[02:20:41.680 --> 02:20:42.760]   - Sure, I mean.
[02:20:42.760 --> 02:20:44.040]   - Try to convince him and stuff.
[02:20:44.040 --> 02:20:45.640]   - You know, and you might think,
[02:20:45.640 --> 02:20:47.480]   from the point of view of just making money
[02:20:47.480 --> 02:20:49.360]   or having sex or other sorts of things,
[02:20:49.360 --> 02:20:52.560]   this is misdirected energy, right?
[02:20:52.560 --> 02:20:56.760]   Trying to impress the very most highest quality minds,
[02:20:56.760 --> 02:20:57.920]   that's such a small sample
[02:20:57.920 --> 02:21:00.280]   and they can't do that much for you anyway.
[02:21:00.280 --> 02:21:04.640]   So I might well have had more ordinary success in life,
[02:21:04.640 --> 02:21:07.320]   be more popular, invited to more parties, make more money
[02:21:07.320 --> 02:21:12.320]   if I had targeted a lower tier set of intellectuals
[02:21:12.320 --> 02:21:14.560]   with the standards they have.
[02:21:14.560 --> 02:21:17.160]   But for some reason, I decided early on
[02:21:17.160 --> 02:21:20.640]   that Einstein was my audience, or people like him,
[02:21:20.640 --> 02:21:23.280]   and I was gonna impress them.
[02:21:23.280 --> 02:21:26.800]   - Yeah, I mean, you pick your set of motivations.
[02:21:26.800 --> 02:21:28.880]   You know, convincing, impressing Tyler Cohen
[02:21:28.880 --> 02:21:32.360]   is not gonna help you get laid, trust me, I tried.
[02:21:32.360 --> 02:21:33.200]   All right.
[02:21:33.200 --> 02:21:39.760]   What are some notable sort of effects
[02:21:39.760 --> 02:21:43.560]   of the elephant in the brain in everyday life?
[02:21:43.560 --> 02:21:46.840]   So you mentioned when we tried to apply that
[02:21:46.840 --> 02:21:49.160]   to economics, to public policy,
[02:21:49.160 --> 02:21:51.080]   so when we think about medicine, education,
[02:21:51.080 --> 02:21:53.200]   all those kinds of things, what are some things
[02:21:53.200 --> 02:21:54.200]   that we just-- - Well, the key thing is,
[02:21:54.200 --> 02:21:57.120]   medicine is much less useful health-wise than you think.
[02:21:57.120 --> 02:22:01.280]   So, you know, if you were focused on your health,
[02:22:01.280 --> 02:22:03.400]   you would care a lot less about it.
[02:22:03.400 --> 02:22:05.160]   And if you were focused on other people's health,
[02:22:05.160 --> 02:22:06.720]   you would also care a lot less about it.
[02:22:06.720 --> 02:22:09.420]   But if medicine is, as we suggest,
[02:22:09.420 --> 02:22:10.560]   more about showing that you care
[02:22:10.560 --> 02:22:13.160]   and let other people showing that they care about you,
[02:22:13.160 --> 02:22:16.120]   then a lot of priority on medicine can make sense.
[02:22:16.120 --> 02:22:19.280]   So that was our very earliest discussion in the podcast.
[02:22:19.280 --> 02:22:21.000]   You were talking about, you know,
[02:22:21.000 --> 02:22:22.840]   should you give people a lot of medicine
[02:22:22.840 --> 02:22:24.120]   when it's not very effective?
[02:22:24.120 --> 02:22:25.960]   And then the answer then is, well,
[02:22:25.960 --> 02:22:28.560]   if that's the way that you show that you care about them
[02:22:28.560 --> 02:22:31.160]   and you really want them to know you care,
[02:22:31.160 --> 02:22:33.220]   then maybe that's what you need to do
[02:22:33.220 --> 02:22:36.200]   if you can't find a cheaper, more effective substitute.
[02:22:36.200 --> 02:22:39.500]   - So if we actually just pause on that for a little bit,
[02:22:39.500 --> 02:22:43.200]   how do we start to untangle the full set
[02:22:43.200 --> 02:22:46.720]   of self-deception happening in the space of medicine?
[02:22:46.720 --> 02:22:48.800]   - So we have a method that we use in our book
[02:22:48.800 --> 02:22:50.600]   that is what I recommend for people
[02:22:50.600 --> 02:22:52.200]   to use in all these sorts of topics.
[02:22:52.200 --> 02:22:53.360]   The straightforward method is,
[02:22:53.360 --> 02:22:55.800]   first, don't look at yourself.
[02:22:55.800 --> 02:22:57.420]   Look at other people.
[02:22:57.420 --> 02:23:00.560]   Look at broad patterns of behavior in other people.
[02:23:00.560 --> 02:23:03.720]   And then ask, what are the various theories we could have
[02:23:03.720 --> 02:23:05.700]   to explain these patterns of behavior?
[02:23:05.700 --> 02:23:07.240]   And then just do the simple matching.
[02:23:07.240 --> 02:23:10.920]   Which theory better matches the behavior they have?
[02:23:10.920 --> 02:23:13.140]   And the last step is to assume that's true of you
[02:23:13.140 --> 02:23:13.980]   too.
[02:23:13.980 --> 02:23:17.120]   Don't assume you're an exception.
[02:23:17.120 --> 02:23:19.000]   If you happen to be an exception, that won't go so well,
[02:23:19.000 --> 02:23:21.680]   but nevertheless, on average, you aren't very well
[02:23:21.680 --> 02:23:23.220]   positioned to judge if you're an exception.
[02:23:23.220 --> 02:23:26.200]   So look at what other people do,
[02:23:26.200 --> 02:23:29.220]   explain what other people do, and assume that's you too.
[02:23:29.220 --> 02:23:31.080]   - But also, in the case of medicine,
[02:23:31.080 --> 02:23:34.400]   there's several parties to consider.
[02:23:34.400 --> 02:23:35.960]   So there's the individual person
[02:23:35.960 --> 02:23:37.480]   that's receiving the medicine.
[02:23:37.480 --> 02:23:40.080]   There's the doctors that are prescribing the medicine.
[02:23:40.080 --> 02:23:44.380]   There's drug companies that are selling drugs.
[02:23:44.380 --> 02:23:46.060]   There are governments that have regulations
[02:23:46.060 --> 02:23:46.940]   that are lobbyists.
[02:23:46.940 --> 02:23:50.980]   So you can build up a network of categories of humans
[02:23:50.980 --> 02:23:53.900]   in this, and they each play their role.
[02:23:53.900 --> 02:23:56.140]   So how do you introspect,
[02:23:56.140 --> 02:24:01.340]   sort of analyze the system at a system scale
[02:24:01.340 --> 02:24:03.820]   versus at the individual scale?
[02:24:03.820 --> 02:24:06.100]   - So it turns out that in general,
[02:24:06.100 --> 02:24:09.700]   it's usually much easier to explain producer behavior
[02:24:09.700 --> 02:24:11.340]   than consumer behavior.
[02:24:11.340 --> 02:24:14.280]   That is, the drug companies or the doctors
[02:24:14.280 --> 02:24:16.100]   have relatively clear incentives
[02:24:16.100 --> 02:24:18.240]   to give the customers whatever they want.
[02:24:18.240 --> 02:24:22.020]   And similarly say, governments in democratic countries
[02:24:22.020 --> 02:24:24.900]   have the incentive to give the voters what they want.
[02:24:24.900 --> 02:24:29.020]   So that focuses your attention on the patient
[02:24:29.020 --> 02:24:31.260]   and the voter in this equation,
[02:24:31.260 --> 02:24:33.020]   and saying, what do they want?
[02:24:33.020 --> 02:24:36.040]   They would be driving the rest of the system.
[02:24:36.040 --> 02:24:37.900]   Whatever they want, the other parties
[02:24:37.900 --> 02:24:40.560]   are willing to give them in order to get paid.
[02:24:40.560 --> 02:24:43.460]   So now we're looking for puzzles
[02:24:43.460 --> 02:24:46.020]   in patient and voter behavior.
[02:24:46.020 --> 02:24:49.260]   What are they choosing and why do they choose that?
[02:24:49.260 --> 02:24:52.780]   - And how much exactly?
[02:24:52.780 --> 02:24:54.500]   And then we can explain that potentially,
[02:24:54.500 --> 02:24:56.380]   again, returning to the producer,
[02:24:56.380 --> 02:24:59.220]   by the producer being incentivized to manipulate
[02:24:59.220 --> 02:25:01.420]   the decision-making processes of the voter
[02:25:01.420 --> 02:25:02.240]   and the consumer.
[02:25:02.240 --> 02:25:04.340]   - Well now, in almost every industry,
[02:25:04.340 --> 02:25:07.340]   producers are, in general, happy to lie
[02:25:07.340 --> 02:25:09.700]   and exaggerate in order to get more customers.
[02:25:09.700 --> 02:25:10.900]   This is true of auto repair
[02:25:10.900 --> 02:25:13.560]   as much as human body repair in medicine.
[02:25:13.560 --> 02:25:15.460]   So the differences between these industries
[02:25:15.460 --> 02:25:18.500]   can't be explained by the willingness of the producers
[02:25:18.500 --> 02:25:20.220]   to give customers what they want
[02:25:20.220 --> 02:25:22.540]   or to do various things that we have to, again,
[02:25:22.540 --> 02:25:23.500]   go to the customers.
[02:25:23.500 --> 02:25:26.420]   Why are customers treating body repair
[02:25:26.420 --> 02:25:27.780]   different than auto repair?
[02:25:31.100 --> 02:25:34.500]   - Yeah, and that potentially requires a lot of thinking,
[02:25:34.500 --> 02:25:35.780]   a lot of data collection,
[02:25:35.780 --> 02:25:38.060]   and potentially looking at historical data too
[02:25:38.060 --> 02:25:40.340]   'cause things don't just happen overnight.
[02:25:40.340 --> 02:25:41.180]   Over time, there's trends.
[02:25:41.180 --> 02:25:42.700]   - In principle, it does, but actually,
[02:25:42.700 --> 02:25:45.500]   it's a lot, actually, easier than you might think.
[02:25:45.500 --> 02:25:47.940]   I think the biggest limitation is just the willingness
[02:25:47.940 --> 02:25:49.860]   to consider alternative hypotheses.
[02:25:49.860 --> 02:25:52.860]   So many of the patterns that you need to rely on
[02:25:52.860 --> 02:25:54.780]   are actually pretty obvious, simple patterns.
[02:25:54.780 --> 02:25:57.220]   You just have to notice them and ask yourself,
[02:25:57.220 --> 02:25:59.300]   how can I explain those?
[02:25:59.300 --> 02:26:01.340]   Often, you don't need to look at the most subtle,
[02:26:01.340 --> 02:26:04.420]   most difficult statistical evidence
[02:26:04.420 --> 02:26:05.540]   that might be out there.
[02:26:05.540 --> 02:26:08.620]   The simplest patterns are often enough.
[02:26:08.620 --> 02:26:10.780]   - All right, so there's a fundamental statement
[02:26:10.780 --> 02:26:12.740]   about self-deception in the book.
[02:26:12.740 --> 02:26:14.060]   There's the application of that,
[02:26:14.060 --> 02:26:15.920]   like we just did in medicine.
[02:26:15.920 --> 02:26:18.740]   Can you steel man the argument
[02:26:18.740 --> 02:26:23.220]   that many of the foundational ideas in the book are wrong?
[02:26:25.260 --> 02:26:29.540]   Meaning there's two that you just made,
[02:26:29.540 --> 02:26:33.100]   which is it can be a lot simpler than it looks.
[02:26:33.100 --> 02:26:35.780]   Can you steel man the case that it's,
[02:26:35.780 --> 02:26:38.900]   case by case, it's always super complicated.
[02:26:38.900 --> 02:26:39.980]   Like it's a complex system.
[02:26:39.980 --> 02:26:42.860]   It's very difficult to have a simple model about.
[02:26:42.860 --> 02:26:44.140]   It's very difficult to introspect.
[02:26:44.140 --> 02:26:46.500]   And the other one is that the human brain
[02:26:46.500 --> 02:26:50.140]   isn't not just about self-deception,
[02:26:51.340 --> 02:26:56.340]   that there's a lot of motivations at play.
[02:26:56.340 --> 02:26:59.700]   And we are able to really introspect our own mind.
[02:26:59.700 --> 02:27:02.340]   And like what's on the surface of the conscious
[02:27:02.340 --> 02:27:04.660]   is actually quite a good representation
[02:27:04.660 --> 02:27:06.500]   of what's going on in the brain.
[02:27:06.500 --> 02:27:07.900]   And you're not deceiving yourself.
[02:27:07.900 --> 02:27:11.060]   You're able to actually arrive to deeply think
[02:27:11.060 --> 02:27:12.560]   about where your mind stands
[02:27:12.560 --> 02:27:14.180]   and what you think about the world.
[02:27:14.180 --> 02:27:15.980]   And it's less about impressing people
[02:27:15.980 --> 02:27:19.500]   and more about being a free thinking individual.
[02:27:19.500 --> 02:27:23.820]   - So when a child tries to explain
[02:27:23.820 --> 02:27:26.500]   why they don't have their homework assignment,
[02:27:26.500 --> 02:27:28.580]   they are sometimes inclined to say,
[02:27:28.580 --> 02:27:30.400]   the dog ate my homework.
[02:27:30.400 --> 02:27:33.080]   They almost never say the dragon ate my homework.
[02:27:33.080 --> 02:27:35.740]   The reason is the dragon
[02:27:35.740 --> 02:27:39.060]   is a completely implausible explanation.
[02:27:39.060 --> 02:27:41.700]   Almost always when we make excuses for things,
[02:27:41.700 --> 02:27:45.200]   we choose things that are at least in some degree plausible.
[02:27:45.200 --> 02:27:46.820]   It could perhaps have happened.
[02:27:47.980 --> 02:27:52.140]   That's an obstacle for any explanation of a hidden motive
[02:27:52.140 --> 02:27:54.660]   or a hidden feature of human behavior.
[02:27:54.660 --> 02:27:57.260]   If people are pretending one thing
[02:27:57.260 --> 02:27:58.500]   while really doing another,
[02:27:58.500 --> 02:28:00.340]   they're usually gonna pick as a pretense
[02:28:00.340 --> 02:28:02.100]   something that's somewhat plausible.
[02:28:02.100 --> 02:28:07.900]   That's gonna be an obstacle to proving that hypothesis
[02:28:07.900 --> 02:28:10.260]   if you are focused on sort of the local data
[02:28:10.260 --> 02:28:13.060]   that a person would typically have if they were challenged.
[02:28:13.060 --> 02:28:14.460]   So if you're just looking at one kid
[02:28:14.460 --> 02:28:15.660]   and his lack of homework,
[02:28:17.020 --> 02:28:19.780]   maybe you can't tell whether his dog ate his homework or not.
[02:28:19.780 --> 02:28:21.980]   If you happen to know he doesn't have a dog,
[02:28:21.980 --> 02:28:24.660]   you might have more confidence, right?
[02:28:24.660 --> 02:28:26.780]   You will need to have a wider range of evidence
[02:28:26.780 --> 02:28:28.340]   than a typical person would
[02:28:28.340 --> 02:28:30.420]   when they're encountering that actual excuse
[02:28:30.420 --> 02:28:32.340]   in order to see past the excuse.
[02:28:32.340 --> 02:28:35.540]   That will just be a general feature of it.
[02:28:35.540 --> 02:28:38.860]   So if I say, there's this usual story
[02:28:38.860 --> 02:28:39.940]   about why we go to the doctor
[02:28:39.940 --> 02:28:42.660]   and then there's this other explanation,
[02:28:42.660 --> 02:28:44.940]   it'll be true that you'll have to look at wider data
[02:28:44.940 --> 02:28:46.100]   in order to see that
[02:28:46.140 --> 02:28:49.860]   because people don't usually offer excuses
[02:28:49.860 --> 02:28:52.500]   unless in the local context of their excuse,
[02:28:52.500 --> 02:28:54.060]   they can get away with it.
[02:28:54.060 --> 02:28:55.780]   That is, it's hard to tell, right?
[02:28:55.780 --> 02:28:58.340]   So in the case of medicine,
[02:28:58.340 --> 02:29:00.900]   I have to point you to sort of larger sets of data,
[02:29:00.900 --> 02:29:06.860]   but in many areas of academia, including health economics,
[02:29:06.860 --> 02:29:09.100]   the researchers there also
[02:29:09.100 --> 02:29:11.820]   want to support the usual points of view.
[02:29:11.820 --> 02:29:14.260]   And so they will have selection effects
[02:29:14.260 --> 02:29:16.260]   in their publications and their analysis
[02:29:16.260 --> 02:29:19.460]   whereby if they're getting a result too much contrary
[02:29:19.460 --> 02:29:21.420]   to the usual point of view everybody wants to have,
[02:29:21.420 --> 02:29:24.100]   they will file drawer that paper
[02:29:24.100 --> 02:29:26.500]   or redo the analysis until they get an answer
[02:29:26.500 --> 02:29:28.220]   that's more to people's liking.
[02:29:28.220 --> 02:29:31.620]   So that means in the health economics literature,
[02:29:31.620 --> 02:29:33.780]   there are plenty of people who will claim
[02:29:33.780 --> 02:29:37.140]   that in fact, we have evidence that medicine is effective.
[02:29:37.140 --> 02:29:39.380]   And when I respond,
[02:29:39.380 --> 02:29:42.740]   I will have to point you to our most reliable evidence
[02:29:43.940 --> 02:29:45.940]   and ask you to consider the possibility
[02:29:45.940 --> 02:29:47.540]   that the literature is biased
[02:29:47.540 --> 02:29:50.220]   in that when the evidence isn't as reliable,
[02:29:50.220 --> 02:29:51.660]   when they have more degrees of freedom
[02:29:51.660 --> 02:29:53.260]   in order to get the answer they want,
[02:29:53.260 --> 02:29:55.020]   they do tend to get the answer they want.
[02:29:55.020 --> 02:29:57.460]   But when we get to the kind of evidence
[02:29:57.460 --> 02:29:59.580]   that's much harder to mess with,
[02:29:59.580 --> 02:30:03.420]   that's where we will see the truth be more revealed.
[02:30:03.420 --> 02:30:05.620]   So with respect to medicine,
[02:30:05.620 --> 02:30:08.460]   we have millions of papers published in medicine
[02:30:08.460 --> 02:30:09.300]   over the years,
[02:30:09.300 --> 02:30:12.540]   most of which give the impression that medicine is useful.
[02:30:13.660 --> 02:30:16.980]   There's a small literature on randomized experiments
[02:30:16.980 --> 02:30:19.220]   of the aggregate effects of medicine,
[02:30:19.220 --> 02:30:22.980]   where there's maybe a few half dozen or so papers,
[02:30:22.980 --> 02:30:26.460]   where it would be the hardest to hide it
[02:30:26.460 --> 02:30:29.580]   because it's such a straightforward experiment
[02:30:29.580 --> 02:30:32.420]   done in a straightforward way
[02:30:32.420 --> 02:30:36.060]   that it's hard to manipulate.
[02:30:36.060 --> 02:30:37.740]   And that's where I will point you to,
[02:30:37.740 --> 02:30:40.100]   to show you that there's relatively little correlation
[02:30:40.100 --> 02:30:40.940]   between health and medicine.
[02:30:40.940 --> 02:30:44.500]   But even then, people could try to save the phenomenon
[02:30:44.500 --> 02:30:46.500]   and say, "Well, it's not hidden motives, it's just ignorance."
[02:30:46.500 --> 02:30:48.020]   They could say, for example,
[02:30:48.020 --> 02:30:50.700]   medicine's complicated,
[02:30:50.700 --> 02:30:53.300]   most people don't know the literature,
[02:30:53.300 --> 02:30:56.820]   therefore they can be excused for ignorance.
[02:30:56.820 --> 02:30:59.700]   They are just ignorantly assuming that medicine is effective.
[02:30:59.700 --> 02:31:01.380]   It's not that they have some other motive
[02:31:01.380 --> 02:31:02.940]   that they're trying to achieve.
[02:31:02.940 --> 02:31:06.020]   And then I will have to do,
[02:31:06.020 --> 02:31:08.180]   as with a conspiracy theory analysis,
[02:31:08.180 --> 02:31:11.780]   I'm saying, "Well, how long has this misperception
[02:31:11.780 --> 02:31:12.620]   been going on?
[02:31:12.620 --> 02:31:15.140]   How consistently has it happened around the world
[02:31:15.140 --> 02:31:16.420]   and across time?"
[02:31:16.420 --> 02:31:20.140]   And I would have to say, "Look, if we're talking about,
[02:31:20.140 --> 02:31:24.220]   say, a recent new product, like Segway scooters
[02:31:24.220 --> 02:31:26.700]   or something, I could say not so many people
[02:31:26.700 --> 02:31:27.740]   have seen them or used them.
[02:31:27.740 --> 02:31:29.820]   Maybe they could be confused about their value.
[02:31:29.820 --> 02:31:31.660]   If we're talking about a product that's been around
[02:31:31.660 --> 02:31:34.260]   for thousands of years, used in roughly the same way
[02:31:34.260 --> 02:31:36.700]   all across the world, and we see the same pattern
[02:31:36.700 --> 02:31:40.300]   over and over again, this sort of ignorance mistake
[02:31:40.300 --> 02:31:41.860]   just doesn't work so well."
[02:31:41.860 --> 02:31:46.860]   - It's also is a question of how much of the self-deception
[02:31:46.860 --> 02:31:50.580]   is prevalent versus foundational?
[02:31:50.580 --> 02:31:52.500]   Because there's a kind of implied thing
[02:31:52.500 --> 02:31:54.980]   where it's foundational to human nature
[02:31:54.980 --> 02:31:58.700]   versus just a common pitfall.
[02:31:58.700 --> 02:32:01.780]   This is a question I have.
[02:32:01.780 --> 02:32:05.740]   So maybe human progress is made by people
[02:32:05.740 --> 02:32:09.220]   who don't fall into the self-deception.
[02:32:09.220 --> 02:32:11.740]   It's like a baser aspect of human nature,
[02:32:11.740 --> 02:32:14.820]   but then you escape it easily if you're motivated.
[02:32:14.820 --> 02:32:19.220]   - The motivational hypotheses about the self-deceptions
[02:32:19.220 --> 02:32:21.180]   are in terms of how it makes you look
[02:32:21.180 --> 02:32:22.020]   to the people around you.
[02:32:22.020 --> 02:32:24.100]   Again, the press secretary.
[02:32:24.100 --> 02:32:27.340]   So the story would be most people want to look good
[02:32:27.340 --> 02:32:28.660]   to the people around them.
[02:32:28.660 --> 02:32:31.860]   Therefore, most people present themselves in ways
[02:32:31.860 --> 02:32:34.840]   that help them look good to the people around them.
[02:32:35.840 --> 02:32:39.000]   That's sufficient to say there would be a lot of it.
[02:32:39.000 --> 02:32:41.400]   It doesn't need to be 100%, right?
[02:32:41.400 --> 02:32:44.000]   There's enough variety in people and in circumstances
[02:32:44.000 --> 02:32:46.120]   that sometimes taking a contrarian strategy
[02:32:46.120 --> 02:32:49.620]   can be in the interest of some minority of the people.
[02:32:49.620 --> 02:32:51.520]   So I might, for example, say that
[02:32:51.520 --> 02:32:53.640]   that's a strategy I've taken.
[02:32:53.640 --> 02:32:57.860]   I've decided that being contrarian on these things
[02:32:57.860 --> 02:33:01.040]   could be winning for me in that there's a room
[02:33:01.040 --> 02:33:02.880]   for a small number of people like me
[02:33:02.880 --> 02:33:05.440]   who have these sort of messages
[02:33:05.440 --> 02:33:07.080]   who can then get more attention,
[02:33:07.080 --> 02:33:09.680]   even if there's not room for most people to do that.
[02:33:09.680 --> 02:33:14.320]   And that can be explaining sort of the variety, right?
[02:33:14.320 --> 02:33:16.280]   Similarly, you might say, look,
[02:33:16.280 --> 02:33:17.580]   just look at the most obvious things.
[02:33:17.580 --> 02:33:19.260]   Most people would like to look good, right,
[02:33:19.260 --> 02:33:20.100]   in the sense of physically.
[02:33:20.100 --> 02:33:21.420]   Just you look good right now.
[02:33:21.420 --> 02:33:22.260]   You're wearing a nice suit.
[02:33:22.260 --> 02:33:23.100]   You have a haircut.
[02:33:23.100 --> 02:33:24.560]   You shaved, right?
[02:33:24.560 --> 02:33:25.400]   So, and we--
[02:33:25.400 --> 02:33:26.240]   - I cut my own hair, by the way.
[02:33:26.240 --> 02:33:28.120]   - Okay, well, then, all the more impressive.
[02:33:28.120 --> 02:33:31.840]   - That's a counter argument for your claim
[02:33:31.840 --> 02:33:33.080]   that most people wanna look good.
[02:33:33.080 --> 02:33:34.280]   - Clearly, if we look at most people
[02:33:34.280 --> 02:33:35.200]   and their physical appearance,
[02:33:35.200 --> 02:33:37.880]   clearly most people are trying to look somewhat nice, right?
[02:33:37.880 --> 02:33:41.080]   They shower, they shave, they comb their hair,
[02:33:41.080 --> 02:33:42.760]   but we certainly see some people around
[02:33:42.760 --> 02:33:44.440]   who are not trying to look so nice, right?
[02:33:44.440 --> 02:33:46.120]   Is that a big challenge,
[02:33:46.120 --> 02:33:48.040]   the hypothesis that people wanna look nice?
[02:33:48.040 --> 02:33:49.640]   Not that much, right?
[02:33:49.640 --> 02:33:53.800]   We can see in those particular people's context
[02:33:53.800 --> 02:33:55.880]   more particular reasons why they've chosen
[02:33:55.880 --> 02:33:58.720]   to be an exception to the more general rule.
[02:33:58.720 --> 02:34:01.720]   - So the general rule does reveal something foundational.
[02:34:01.720 --> 02:34:02.680]   - In general, really.
[02:34:02.680 --> 02:34:04.720]   - Right.
[02:34:04.720 --> 02:34:05.840]   - That's the way things work.
[02:34:05.840 --> 02:34:07.840]   Let me ask you, you wrote a blog post
[02:34:07.840 --> 02:34:09.520]   about the accuracy of authorities,
[02:34:09.520 --> 02:34:12.520]   since we're talking about this, especially in medicine.
[02:34:12.520 --> 02:34:16.720]   Just looking around us,
[02:34:16.720 --> 02:34:18.960]   especially during this time of the pandemic,
[02:34:18.960 --> 02:34:22.000]   there's been a growing distrust of authorities,
[02:34:22.000 --> 02:34:26.420]   of institutions, even the institution of science itself.
[02:34:27.760 --> 02:34:32.520]   What are the pros and cons of authorities, would you say?
[02:34:32.520 --> 02:34:35.880]   So what's nice about authorities?
[02:34:35.880 --> 02:34:37.920]   What's nice about institutions?
[02:34:37.920 --> 02:34:39.560]   And what are their pitfalls?
[02:34:39.560 --> 02:34:43.080]   - One standard function of authority
[02:34:43.080 --> 02:34:45.800]   is as something you can defer to respectively
[02:34:45.800 --> 02:34:48.560]   without needing to seem too submissive
[02:34:48.560 --> 02:34:53.560]   or ignorant or gullible.
[02:34:54.680 --> 02:34:59.680]   That is, when you're asking, what should I act on
[02:34:59.680 --> 02:35:02.880]   or what belief should I act on?
[02:35:02.880 --> 02:35:06.440]   You might be worried if I chose something too contrarian,
[02:35:06.440 --> 02:35:11.200]   too weird, too speculative, that that would make me look bad
[02:35:11.200 --> 02:35:14.340]   so I would just choose something very conservative.
[02:35:14.340 --> 02:35:17.000]   So maybe an authority lets you choose something
[02:35:17.000 --> 02:35:18.160]   a little less conservative
[02:35:18.160 --> 02:35:21.120]   because the authority is your authorization.
[02:35:21.120 --> 02:35:23.320]   The authority will let you do it.
[02:35:23.320 --> 02:35:25.640]   And somebody says, why did you do that thing?
[02:35:25.640 --> 02:35:27.680]   And they say, the authority authorized it.
[02:35:27.680 --> 02:35:29.960]   The authority tells me I should do this.
[02:35:29.960 --> 02:35:31.520]   Why aren't you doing it?
[02:35:31.520 --> 02:35:34.600]   - So the authority is often pushing for the conservative.
[02:35:34.600 --> 02:35:36.760]   - Well, no, the authority can do more.
[02:35:36.760 --> 02:35:39.000]   I mean, so for example, we just think about,
[02:35:39.000 --> 02:35:41.400]   I don't know, in a pandemic even,
[02:35:41.400 --> 02:35:43.040]   you could just think, oh, I'll just stay home
[02:35:43.040 --> 02:35:44.800]   and close all the doors or I'll just ignore it.
[02:35:44.800 --> 02:35:47.000]   You could just think of just some very simple strategy
[02:35:47.000 --> 02:35:49.840]   that might be defensible if there were no authorities.
[02:35:50.960 --> 02:35:54.080]   But authorities might be able to know more than that.
[02:35:54.080 --> 02:35:55.820]   They might be able to look at some evidence,
[02:35:55.820 --> 02:35:58.280]   draw a more context-dependent conclusion,
[02:35:58.280 --> 02:36:00.520]   declare it as the authority's opinion,
[02:36:00.520 --> 02:36:01.760]   and then other people might follow that,
[02:36:01.760 --> 02:36:03.640]   and that could be better than doing nothing.
[02:36:03.640 --> 02:36:06.080]   - So you mentioned WHO,
[02:36:06.080 --> 02:36:08.640]   the world's most beloved organization.
[02:36:08.640 --> 02:36:13.640]   So this is me speaking in general,
[02:36:13.640 --> 02:36:17.760]   WHO and CDC has been kind of,
[02:36:17.760 --> 02:36:19.880]   depending on degrees.
[02:36:19.880 --> 02:36:20.720]   - Right.
[02:36:20.720 --> 02:36:27.720]   - Details, just not behaving as I would have imagined
[02:36:27.720 --> 02:36:31.600]   in the best possible evolution of human civilization,
[02:36:31.600 --> 02:36:33.200]   authorities should act.
[02:36:33.200 --> 02:36:36.920]   They seem to have failed in some fundamental way
[02:36:36.920 --> 02:36:40.880]   in terms of leadership in a difficult time for our society.
[02:36:40.880 --> 02:36:43.200]   Can you say what are the pros and cons
[02:36:43.200 --> 02:36:45.600]   of this particular authority?
[02:36:45.600 --> 02:36:49.080]   - So again, if there were no authorities whatsoever,
[02:36:49.080 --> 02:36:51.080]   no accepted authorities,
[02:36:51.080 --> 02:36:54.760]   then people would have to sort of randomly pick
[02:36:54.760 --> 02:36:56.360]   different local authorities
[02:36:56.360 --> 02:36:57.560]   who would conflict with each other,
[02:36:57.560 --> 02:36:59.400]   and then they'd be fighting each other about that,
[02:36:59.400 --> 02:37:01.000]   or just not believe anybody
[02:37:01.000 --> 02:37:02.920]   and just do some initial default action
[02:37:02.920 --> 02:37:05.640]   that you would always do without responding to context.
[02:37:05.640 --> 02:37:08.600]   So the potential gain of an authority
[02:37:08.600 --> 02:37:11.600]   is that they could know more than just basic ignorance,
[02:37:11.600 --> 02:37:13.600]   and if people followed them,
[02:37:13.600 --> 02:37:16.280]   they could both be more informed than ignorance
[02:37:16.280 --> 02:37:17.900]   and all doing the same thing,
[02:37:17.900 --> 02:37:20.300]   so they're each protected from being accused
[02:37:20.300 --> 02:37:21.200]   or complained about.
[02:37:21.200 --> 02:37:23.200]   That's the idea of an authority.
[02:37:23.200 --> 02:37:24.720]   That would be the good--
[02:37:24.720 --> 02:37:26.760]   - What's the con of that?
[02:37:26.760 --> 02:37:27.600]   - Okay.
[02:37:27.600 --> 02:37:28.440]   - What's the negative?
[02:37:28.440 --> 02:37:29.260]   How does that go wrong?
[02:37:29.260 --> 02:37:31.900]   - So the con is that if you think of yourself
[02:37:31.900 --> 02:37:33.200]   as the authority and asking,
[02:37:33.200 --> 02:37:35.920]   "What's my best strategy as an authority?"
[02:37:35.920 --> 02:37:39.100]   it's unfortunately not to be maximally informative.
[02:37:39.100 --> 02:37:42.320]   So you might think the ideal authority
[02:37:42.320 --> 02:37:44.760]   would not just tell you more than ignorance,
[02:37:44.760 --> 02:37:46.660]   it would tell you as much as possible.
[02:37:47.560 --> 02:37:49.800]   Okay, it would give you as much detail
[02:37:49.800 --> 02:37:53.460]   as you could possibly listen to and manage to assimilate,
[02:37:53.460 --> 02:37:56.260]   and it would update that as frequently as possible
[02:37:56.260 --> 02:37:58.880]   or as frequently as you were able to listen and assimilate,
[02:37:58.880 --> 02:38:02.060]   and that would be the maximally informative authority.
[02:38:02.060 --> 02:38:04.960]   The problem is there's a conflict
[02:38:04.960 --> 02:38:08.800]   between being an authority or being seen as an authority
[02:38:08.800 --> 02:38:10.980]   and being maximally informative.
[02:38:10.980 --> 02:38:12.840]   That was the point of my blog post
[02:38:12.840 --> 02:38:15.460]   that you're pointing out to here.
[02:38:15.460 --> 02:38:18.900]   That is, if you look at it from their point of view,
[02:38:18.900 --> 02:38:22.860]   they won't long remain the perceived authority
[02:38:22.860 --> 02:38:27.860]   if they are too incautious about how they use that authority,
[02:38:27.860 --> 02:38:31.780]   and one of the ways to be incautious
[02:38:31.780 --> 02:38:34.740]   would be to be too informative.
[02:38:34.740 --> 02:38:37.140]   - Okay, that's still in the pro column for me
[02:38:37.140 --> 02:38:39.100]   'cause you're talking about the tensions
[02:38:39.100 --> 02:38:42.660]   that are very data-driven and very honest,
[02:38:42.660 --> 02:38:46.140]   and I would hope that authorities struggle with that,
[02:38:46.140 --> 02:38:49.140]   how much information to provide to people
[02:38:49.140 --> 02:38:53.800]   to maximize outcomes.
[02:38:53.800 --> 02:38:54.940]   Now, I'm generally somebody
[02:38:54.940 --> 02:38:56.500]   that believes more information is better
[02:38:56.500 --> 02:38:58.500]   'cause I trust in the intelligence of people,
[02:38:58.500 --> 02:39:02.940]   but I'd like to mention a bigger con on authorities,
[02:39:02.940 --> 02:39:04.220]   which is the human question.
[02:39:04.220 --> 02:39:07.820]   This comes back to global government and so on,
[02:39:07.820 --> 02:39:12.820]   is that there's humans that sit in chairs during meetings
[02:39:12.820 --> 02:39:16.460]   and those authorities, they have different titles,
[02:39:16.460 --> 02:39:18.220]   humans form hierarchies,
[02:39:18.220 --> 02:39:20.780]   and sometimes those titles get to your head a little bit,
[02:39:20.780 --> 02:39:23.060]   and you start to want to think,
[02:39:23.060 --> 02:39:26.380]   how do I preserve my control over this authority,
[02:39:26.380 --> 02:39:28.540]   as opposed to thinking through
[02:39:28.540 --> 02:39:30.940]   what is the mission of the authority,
[02:39:30.940 --> 02:39:34.300]   what is the mission of WHO and the other such organization,
[02:39:34.300 --> 02:39:37.740]   and how do I maximize the implementation of that mission,
[02:39:37.740 --> 02:39:40.420]   you start to think, well, I kind of like sitting
[02:39:40.420 --> 02:39:42.620]   in this big chair at the head of the table,
[02:39:42.620 --> 02:39:45.220]   I'd like to sit there for another few years,
[02:39:45.220 --> 02:39:47.980]   or better yet, I want to be remembered
[02:39:47.980 --> 02:39:50.300]   as the person who in a time of crisis
[02:39:50.300 --> 02:39:52.660]   was at the head of this authority
[02:39:52.660 --> 02:39:54.420]   and did a lot of good things.
[02:39:54.420 --> 02:39:58.780]   So you stop trying to do good
[02:39:58.780 --> 02:40:02.100]   under what good means, given the mission of the authority,
[02:40:02.100 --> 02:40:05.600]   and you start to try to carve a narrative,
[02:40:05.600 --> 02:40:06.780]   to manipulate the narrative.
[02:40:06.780 --> 02:40:09.900]   First, in the meeting room, everybody around you,
[02:40:09.900 --> 02:40:12.380]   just a small little story you tell yourself,
[02:40:12.380 --> 02:40:15.420]   the new interns, the managers,
[02:40:15.420 --> 02:40:17.540]   throughout the whole hierarchy of the company.
[02:40:17.540 --> 02:40:19.780]   Okay, once everybody in the company,
[02:40:19.780 --> 02:40:22.680]   or in the organization believes this narrative,
[02:40:22.680 --> 02:40:27.680]   now you start to control the release of information,
[02:40:27.680 --> 02:40:30.060]   not because you're trying to maximize outcomes,
[02:40:30.060 --> 02:40:32.340]   but because you're trying to maximize
[02:40:32.340 --> 02:40:34.220]   the effectiveness of the narrative,
[02:40:34.220 --> 02:40:38.020]   that you are truly a great representative
[02:40:38.020 --> 02:40:40.220]   of this authority in human history.
[02:40:40.220 --> 02:40:42.900]   And I just feel like those human forces,
[02:40:42.900 --> 02:40:45.620]   whenever you have an authority,
[02:40:45.620 --> 02:40:48.040]   it starts getting to people's heads.
[02:40:48.040 --> 02:40:50.180]   One of the most, me as a scientist,
[02:40:50.180 --> 02:40:52.100]   one of the most disappointing things to see
[02:40:52.100 --> 02:40:56.480]   during the pandemic is the use of authority
[02:40:56.480 --> 02:41:01.480]   from colleagues of mine to roll their eyes,
[02:41:01.580 --> 02:41:05.340]   to dismiss other human beings,
[02:41:05.340 --> 02:41:07.940]   just because they got a PhD,
[02:41:07.940 --> 02:41:11.860]   just because they're an assistant associate, full faculty,
[02:41:11.860 --> 02:41:16.860]   just because they are deputy head of X organization,
[02:41:16.860 --> 02:41:20.740]   NIH, whatever the heck the organization is,
[02:41:20.740 --> 02:41:22.980]   just because they got an award of some kind,
[02:41:22.980 --> 02:41:24.900]   and at a conference,
[02:41:24.900 --> 02:41:28.300]   they won a Best Paper award seven years ago,
[02:41:28.300 --> 02:41:31.140]   and then somebody shook their hand and gave them a medal,
[02:41:31.140 --> 02:41:32.460]   maybe it was a president,
[02:41:32.460 --> 02:41:34.980]   and it's been 20, 30 years
[02:41:34.980 --> 02:41:36.820]   that people have been patting them on the back,
[02:41:36.820 --> 02:41:38.860]   saying how special they are,
[02:41:38.860 --> 02:41:40.700]   especially when they're controlling money
[02:41:40.700 --> 02:41:44.040]   and getting sucked up from other scientists
[02:41:44.040 --> 02:41:46.780]   who really want the money in a self-deception kind of way,
[02:41:46.780 --> 02:41:48.900]   they don't actually really care about your performance,
[02:41:48.900 --> 02:41:50.540]   and all of that gets to your head,
[02:41:50.540 --> 02:41:52.700]   and no longer are you the authority
[02:41:52.700 --> 02:41:54.140]   that's trying to do good
[02:41:54.140 --> 02:41:55.820]   and lessen the suffering in the world,
[02:41:55.820 --> 02:41:59.540]   you become an authority that just wants to maximize
[02:42:00.460 --> 02:42:05.460]   self-preserve yourself in a sitting on a throne of power.
[02:42:05.460 --> 02:42:08.860]   - So this is core to sort of what it is to be an economist,
[02:42:08.860 --> 02:42:11.740]   I'm a professor of economics.
[02:42:11.740 --> 02:42:13.380]   - There you go, with the authority again.
[02:42:13.380 --> 02:42:15.220]   - No, so it's about saying--
[02:42:15.220 --> 02:42:16.340]   - Just joking, yes.
[02:42:16.340 --> 02:42:20.820]   - We often have a situation where we see a world of behavior,
[02:42:20.820 --> 02:42:23.700]   and then we see ways in which particular behaviors
[02:42:23.700 --> 02:42:27.100]   are not sort of maximally socially useful.
[02:42:27.100 --> 02:42:28.100]   - Yes.
[02:42:28.140 --> 02:42:31.780]   - And we have a variety of reactions to that,
[02:42:31.780 --> 02:42:34.640]   so one kind of reaction is to sort of morally blame
[02:42:34.640 --> 02:42:36.900]   each individual for not doing
[02:42:36.900 --> 02:42:38.800]   the maximally socially useful thing,
[02:42:38.800 --> 02:42:43.740]   under perhaps the idea that people could be identified
[02:42:43.740 --> 02:42:44.640]   and shamed for that,
[02:42:44.640 --> 02:42:46.660]   and maybe induced into doing the better thing
[02:42:46.660 --> 02:42:50.080]   if only enough people were calling them out on it, right?
[02:42:50.080 --> 02:42:52.460]   But another way to think about it
[02:42:52.460 --> 02:42:55.180]   is to think that people sit in institutions
[02:42:55.180 --> 02:42:58.100]   with certain stable institutional structures,
[02:42:58.100 --> 02:43:00.740]   and that institutions create particular incentives
[02:43:00.740 --> 02:43:04.220]   for individuals, and that individuals are typically
[02:43:04.220 --> 02:43:07.280]   doing whatever is in their local interest
[02:43:07.280 --> 02:43:09.080]   in the context of that institution,
[02:43:09.080 --> 02:43:13.340]   and then perhaps to less blame individuals
[02:43:13.340 --> 02:43:15.940]   for winning their local institutional game,
[02:43:15.940 --> 02:43:19.380]   and more blaming the world for having the wrong institutions.
[02:43:19.380 --> 02:43:21.420]   So economists are often like wondering
[02:43:21.420 --> 02:43:22.980]   what other institutions we could have
[02:43:22.980 --> 02:43:24.340]   instead of the ones we have,
[02:43:24.340 --> 02:43:26.140]   and which of them might promote better behavior,
[02:43:26.140 --> 02:43:29.660]   and this is a common thing we do all across human behavior,
[02:43:29.660 --> 02:43:32.060]   is to think of what are the institutions we're in,
[02:43:32.060 --> 02:43:34.500]   and what are the alternative variations we could imagine,
[02:43:34.500 --> 02:43:35.980]   and then to say which institutions
[02:43:35.980 --> 02:43:37.220]   would be most productive.
[02:43:37.220 --> 02:43:42.080]   I would agree with you that our information institutions,
[02:43:42.080 --> 02:43:45.060]   that is the institutions by which we collect information
[02:43:45.060 --> 02:43:47.260]   and aggregate it and share it with people,
[02:43:47.260 --> 02:43:52.020]   are especially broken in the sense of far from the ideal
[02:43:52.020 --> 02:43:54.540]   of what would be the most cost-effective way
[02:43:54.540 --> 02:43:56.740]   to collect and share information,
[02:43:56.740 --> 02:43:58.100]   but then the challenge is to try
[02:43:58.100 --> 02:44:00.980]   to produce better institutions.
[02:44:00.980 --> 02:44:03.820]   And as an academic, I'm aware that academia
[02:44:03.820 --> 02:44:06.860]   is particularly broken in the sense
[02:44:06.860 --> 02:44:09.700]   that we give people incentives to do research
[02:44:09.700 --> 02:44:11.980]   that's not very interesting or important,
[02:44:11.980 --> 02:44:14.300]   because basically they're being impressive,
[02:44:14.300 --> 02:44:16.380]   and we actually care more about whether academics
[02:44:16.380 --> 02:44:19.260]   are impressive than whether they're interesting or useful.
[02:44:20.260 --> 02:44:23.580]   And I'm happy to go into detail
[02:44:23.580 --> 02:44:25.380]   with lots of different known institutions
[02:44:25.380 --> 02:44:27.100]   and their known institutional failings,
[02:44:27.100 --> 02:44:30.820]   ways in which those institutions produce incentives
[02:44:30.820 --> 02:44:32.940]   that are mistaken, and that was the point of the post
[02:44:32.940 --> 02:44:34.740]   we started with talking about the authorities.
[02:44:34.740 --> 02:44:37.620]   If I need to be seen as an authority,
[02:44:37.620 --> 02:44:40.580]   that's at odds with my being informative,
[02:44:40.580 --> 02:44:42.860]   and I might choose to be the authority
[02:44:42.860 --> 02:44:43.860]   instead of being informative,
[02:44:43.860 --> 02:44:46.300]   'cause that's my institutional incentives.
[02:44:46.300 --> 02:44:48.220]   - And if I may, I'd like to,
[02:44:48.220 --> 02:44:53.100]   given that beautiful picture of incentives
[02:44:53.100 --> 02:44:55.980]   and individuals that you just painted,
[02:44:55.980 --> 02:44:58.660]   let me just apologize for a couple of things.
[02:44:58.660 --> 02:45:03.660]   One, I often put too much blame on leaders of institutions
[02:45:03.660 --> 02:45:09.620]   versus the incentives that govern those institutions.
[02:45:09.620 --> 02:45:12.060]   And as a result of that, I've been,
[02:45:12.060 --> 02:45:17.500]   I believe, too critical of Anthony Fauci,
[02:45:17.500 --> 02:45:20.940]   too emotional about my criticism of Anthony Fauci,
[02:45:20.940 --> 02:45:22.820]   and I'd like to apologize for that,
[02:45:22.820 --> 02:45:27.460]   because I think there's deeper truths to think about,
[02:45:27.460 --> 02:45:29.620]   there's deeper incentives to think about.
[02:45:29.620 --> 02:45:34.300]   That said, I do sort of, I'm a romantic creature by nature.
[02:45:34.300 --> 02:45:37.220]   I romanticize Winston Churchill,
[02:45:37.220 --> 02:45:42.220]   and when I think about Nazi Germany,
[02:45:42.220 --> 02:45:44.500]   I think about Hitler more than I do
[02:45:44.500 --> 02:45:46.900]   about the individual people of Nazi Germany.
[02:45:46.900 --> 02:45:49.380]   You think about leaders, you think about individuals,
[02:45:49.380 --> 02:45:51.020]   not necessarily the parameters,
[02:45:51.020 --> 02:45:53.380]   the incentives that govern the system,
[02:45:53.380 --> 02:45:55.620]   'cause it's harder.
[02:45:55.620 --> 02:45:58.900]   It's harder to think through deeply about the models
[02:45:58.900 --> 02:46:00.660]   from which those individuals arise,
[02:46:00.660 --> 02:46:03.300]   but that's the right thing to do.
[02:46:03.300 --> 02:46:07.860]   But also, I don't apologize for being emotional sometimes,
[02:46:07.860 --> 02:46:08.700]   and being--
[02:46:08.700 --> 02:46:11.060]   - I'm happy to blame the individual leaders
[02:46:11.060 --> 02:46:12.940]   in the sense that I might say,
[02:46:12.940 --> 02:46:15.180]   well, you should be trying to reform these institutions
[02:46:15.180 --> 02:46:17.420]   if you're just there to get promoted
[02:46:17.420 --> 02:46:19.100]   and look good at being at the top,
[02:46:19.100 --> 02:46:21.060]   but maybe I can blame you for your motives
[02:46:21.060 --> 02:46:22.660]   and your priorities in there.
[02:46:22.660 --> 02:46:24.780]   But I can understand why the people at the top
[02:46:24.780 --> 02:46:26.180]   would be the people who are selected
[02:46:26.180 --> 02:46:27.940]   for having the priority of primarily
[02:46:27.940 --> 02:46:29.180]   trying to get to the top.
[02:46:29.180 --> 02:46:30.300]   I get that.
[02:46:30.300 --> 02:46:33.220]   - Can I maybe ask you about particular universities?
[02:46:33.220 --> 02:46:37.340]   They've received, like science has received
[02:46:37.340 --> 02:46:40.740]   an increase in distrust overall as an institution,
[02:46:40.740 --> 02:46:42.580]   which breaks my heart,
[02:46:42.580 --> 02:46:45.260]   because I think science is beautiful as a,
[02:46:45.260 --> 02:46:47.220]   not maybe not as an institution,
[02:46:47.220 --> 02:46:50.420]   but as one of the things,
[02:46:50.420 --> 02:46:54.540]   one of the journeys that humans have taken on.
[02:46:54.540 --> 02:46:55.820]   The other one is university.
[02:46:55.820 --> 02:46:58.580]   I think university is actually a place,
[02:46:58.580 --> 02:47:01.020]   for me at least, in the way I see it,
[02:47:01.020 --> 02:47:05.740]   is a place of freedom of exploring ideas,
[02:47:05.740 --> 02:47:08.160]   scientific ideas, engineering ideas,
[02:47:10.100 --> 02:47:13.820]   more than a corporate, more than a company,
[02:47:13.820 --> 02:47:15.620]   more than a lot of domains in life.
[02:47:15.620 --> 02:47:18.740]   It's not just in its ideal,
[02:47:18.740 --> 02:47:21.300]   but it's in its implementation,
[02:47:21.300 --> 02:47:24.500]   a place where you can be a kid for your whole life
[02:47:24.500 --> 02:47:25.900]   and play with ideas.
[02:47:25.900 --> 02:47:28.060]   And I think with all the criticism
[02:47:28.060 --> 02:47:32.140]   that universities still not currently receive,
[02:47:32.140 --> 02:47:34.500]   I think they, I don't think that criticism
[02:47:34.500 --> 02:47:36.340]   is representative of universities.
[02:47:36.340 --> 02:47:38.420]   They focus on very anecdotal evidence
[02:47:38.420 --> 02:47:40.740]   of particular departments, particular people.
[02:47:40.740 --> 02:47:44.180]   But I still feel like there's a lot of place
[02:47:44.180 --> 02:47:46.580]   for freedom of thought,
[02:47:46.580 --> 02:47:50.060]   at least at MIT,
[02:47:50.060 --> 02:47:52.900]   at least in the fields I care about,
[02:47:52.900 --> 02:47:56.060]   in particular kind of science,
[02:47:56.060 --> 02:47:57.940]   particular kind of technical fields,
[02:47:57.940 --> 02:48:01.140]   mathematics, computer science, physics,
[02:48:01.140 --> 02:48:04.460]   engineering, so robotics, artificial intelligence.
[02:48:04.460 --> 02:48:06.620]   This is a place where you get to be a kid.
[02:48:06.620 --> 02:48:11.620]   Yet there is bureaucracy that's rising up.
[02:48:11.620 --> 02:48:16.100]   There's like more rules, there's more meetings,
[02:48:16.100 --> 02:48:18.000]   and there's more administration,
[02:48:18.000 --> 02:48:21.420]   having like PowerPoint presentations,
[02:48:21.420 --> 02:48:25.100]   which to me, you should like
[02:48:25.100 --> 02:48:30.260]   be more of a renegade explorer of ideas.
[02:48:30.260 --> 02:48:34.500]   And meetings destroy, they suffocate that radical thought
[02:48:34.500 --> 02:48:36.420]   that happens when you're an undergraduate student
[02:48:36.420 --> 02:48:37.860]   and you can do all kinds of wild things
[02:48:37.860 --> 02:48:39.340]   when you're a graduate student.
[02:48:39.340 --> 02:48:40.700]   Anyway, all that to say,
[02:48:40.700 --> 02:48:42.220]   you've thought about this aspect too.
[02:48:42.220 --> 02:48:46.460]   Is there something positive, insightful you could say
[02:48:46.460 --> 02:48:50.340]   about how we can make for better universities
[02:48:50.340 --> 02:48:53.380]   in the decades to come, this particular institution?
[02:48:53.380 --> 02:48:55.500]   How can we improve them?
[02:48:55.500 --> 02:48:58.100]   - I hear that centuries ago,
[02:48:58.100 --> 02:49:01.460]   many scientists and intellectuals were aristocrats.
[02:49:01.540 --> 02:49:06.460]   They had time and could, if they chose,
[02:49:06.460 --> 02:49:07.820]   choose to be intellectuals.
[02:49:07.820 --> 02:49:12.140]   That's a feature of the combination
[02:49:12.140 --> 02:49:14.380]   that they had some source of resources
[02:49:14.380 --> 02:49:16.280]   that allowed them leisure,
[02:49:16.280 --> 02:49:19.260]   and that the kind of competition they were faced in
[02:49:19.260 --> 02:49:23.860]   among aristocrats allowed that sort of a self-indulgence
[02:49:23.860 --> 02:49:27.020]   or self-pursuit at least at some point in their lives.
[02:49:28.920 --> 02:49:33.920]   So the analogous observation is that university professors
[02:49:33.920 --> 02:49:37.080]   often have sort of the freedom and space
[02:49:37.080 --> 02:49:39.160]   to do a wide range of things.
[02:49:39.160 --> 02:49:41.960]   And I am certainly enjoying that as a tenured professor.
[02:49:41.960 --> 02:49:44.800]   - You're a really, sorry to interrupt,
[02:49:44.800 --> 02:49:47.080]   a really good representative of that.
[02:49:47.080 --> 02:49:50.120]   Just the exploration you're doing, the depth of thought,
[02:49:50.120 --> 02:49:53.680]   like most people are afraid to do the kind of
[02:49:53.680 --> 02:49:56.000]   broad thinking that you're doing, which is great.
[02:49:56.000 --> 02:49:57.880]   - The fact that that can happen
[02:49:57.880 --> 02:50:00.080]   is the combination of these two things analogously.
[02:50:00.080 --> 02:50:02.520]   One is that we have fierce competition
[02:50:02.520 --> 02:50:03.800]   to become a tenured professor,
[02:50:03.800 --> 02:50:05.160]   but then once you become tenured,
[02:50:05.160 --> 02:50:08.560]   we give you the freedom to do what you like.
[02:50:08.560 --> 02:50:10.720]   And that's a happenstance.
[02:50:10.720 --> 02:50:12.640]   It didn't have to be that way.
[02:50:12.640 --> 02:50:14.400]   And in many other walks of life,
[02:50:14.400 --> 02:50:17.120]   even though people have a lot of resources, et cetera,
[02:50:17.120 --> 02:50:18.600]   they don't have that kind of freedom set up.
[02:50:18.600 --> 02:50:23.280]   So I think I'm kind of lucky that tenure exists
[02:50:23.280 --> 02:50:24.560]   and that I'm enjoying it.
[02:50:25.440 --> 02:50:28.480]   But I can't be too enthusiastic about this
[02:50:28.480 --> 02:50:30.280]   unless I can approve of sort of the source
[02:50:30.280 --> 02:50:32.480]   of the resources that's paying for all this.
[02:50:32.480 --> 02:50:33.760]   So for the aristocrat, if you thought
[02:50:33.760 --> 02:50:36.800]   they stole it in war or something,
[02:50:36.800 --> 02:50:38.000]   you wouldn't be so pleased,
[02:50:38.000 --> 02:50:39.560]   whereas if you thought they had earned it
[02:50:39.560 --> 02:50:41.920]   or their ancestors had earned this money
[02:50:41.920 --> 02:50:43.480]   that they were spending as an aristocrat,
[02:50:43.480 --> 02:50:45.340]   then you could be more okay with that.
[02:50:45.340 --> 02:50:48.920]   So for universities, I have to ask,
[02:50:48.920 --> 02:50:50.640]   where are the main sources of resources
[02:50:50.640 --> 02:50:52.180]   that are going to the universities
[02:50:52.180 --> 02:50:54.240]   and are they getting their money's worth?
[02:50:54.240 --> 02:50:56.960]   Or are they getting a good value for that payment?
[02:50:56.960 --> 02:51:00.820]   So first of all, they're students.
[02:51:00.820 --> 02:51:03.920]   And the question is, are students getting good value
[02:51:03.920 --> 02:51:05.040]   for their education?
[02:51:05.040 --> 02:51:08.340]   And each person is getting value in the sense
[02:51:08.340 --> 02:51:10.720]   that they are identified and shown
[02:51:10.720 --> 02:51:12.560]   to be a more capable person,
[02:51:12.560 --> 02:51:16.180]   which is then worth more salary as an employee later.
[02:51:16.180 --> 02:51:17.560]   But there is a case for saying
[02:51:17.560 --> 02:51:19.040]   there's a big waste of the system
[02:51:19.040 --> 02:51:22.520]   because we aren't actually changing the students
[02:51:22.520 --> 02:51:26.880]   or educating them, we're more sorting them or labeling them.
[02:51:26.880 --> 02:51:30.280]   And that's a very expensive process to produce that outcome.
[02:51:30.280 --> 02:51:33.720]   And part of the expense is the freedom from tenure, I guess.
[02:51:33.720 --> 02:51:36.680]   So I feel like I can't be too proud of that
[02:51:36.680 --> 02:51:39.600]   because it's basically a tax on all these young students
[02:51:39.600 --> 02:51:41.280]   to pay this enormous amount of money
[02:51:41.280 --> 02:51:42.840]   in order to be labeled as better,
[02:51:42.840 --> 02:51:44.600]   whereas I feel like we should be able
[02:51:44.600 --> 02:51:47.120]   to find cheaper ways of doing that.
[02:51:47.120 --> 02:51:50.640]   The other main customer is researcher patrons
[02:51:50.640 --> 02:51:53.360]   like the government or other foundations.
[02:51:53.360 --> 02:51:55.880]   And then the question is, are they getting their money worth
[02:51:55.880 --> 02:51:58.800]   out of the money they're paying for research to happen?
[02:51:58.800 --> 02:52:03.440]   And my analysis is they don't actually care
[02:52:03.440 --> 02:52:04.660]   about the research progress.
[02:52:04.660 --> 02:52:06.520]   They are mainly buying an affiliation
[02:52:06.520 --> 02:52:07.880]   with credentialed impressiveness
[02:52:07.880 --> 02:52:09.620]   on the part of the researchers.
[02:52:09.620 --> 02:52:12.680]   They mainly pay money to researchers who are impressive
[02:52:12.680 --> 02:52:15.240]   and have impressive affiliations,
[02:52:15.240 --> 02:52:16.200]   and they don't really much care
[02:52:16.200 --> 02:52:19.000]   what research project happens as a result.
[02:52:19.000 --> 02:52:21.160]   - Is that a cynical?
[02:52:21.160 --> 02:52:25.560]   So there's a deep truth to that cynical perspective.
[02:52:25.560 --> 02:52:29.920]   Is there a less cynical perspective that they do care
[02:52:29.920 --> 02:52:31.760]   about the long-term investment
[02:52:31.760 --> 02:52:34.480]   into the progress of science and humanity?
[02:52:34.480 --> 02:52:35.780]   - They might personally care,
[02:52:35.780 --> 02:52:37.680]   but they're stuck in an equilibrium.
[02:52:37.680 --> 02:52:38.520]   - Sure.
[02:52:38.520 --> 02:52:40.680]   - Wherein they basically most foundations
[02:52:40.680 --> 02:52:42.520]   like governments or research,
[02:52:42.520 --> 02:52:44.820]   or like the Ford Foundation,
[02:52:46.520 --> 02:52:49.780]   the individuals there are rated based on the prestige
[02:52:49.780 --> 02:52:51.720]   they bring to that organization.
[02:52:51.720 --> 02:52:52.560]   - Yeah.
[02:52:52.560 --> 02:52:54.000]   - And even if they might personally want
[02:52:54.000 --> 02:52:55.440]   to produce more intellectual progress,
[02:52:55.440 --> 02:52:58.520]   they are in a competitive game where they don't have tenure
[02:52:58.520 --> 02:53:01.160]   and they need to produce this prestige.
[02:53:01.160 --> 02:53:04.000]   And so once they give grant money to prestigious people,
[02:53:04.000 --> 02:53:06.100]   that is the thing that shows that they have achieved
[02:53:06.100 --> 02:53:07.520]   prestige for the organization,
[02:53:07.520 --> 02:53:08.840]   and that's what they need to do
[02:53:08.840 --> 02:53:11.080]   in order to retain their position.
[02:53:11.080 --> 02:53:13.320]   - And you do hope that there's a correlation
[02:53:13.320 --> 02:53:17.000]   between prestige and actual competence.
[02:53:17.000 --> 02:53:18.320]   - Of course there is a correlation.
[02:53:18.320 --> 02:53:19.240]   The question is just,
[02:53:19.240 --> 02:53:21.160]   could we do this better some other way?
[02:53:21.160 --> 02:53:22.000]   - Yes.
[02:53:22.000 --> 02:53:24.700]   - I think it's pretty clear we could.
[02:53:24.700 --> 02:53:27.240]   What it's harder to do is move the world
[02:53:27.240 --> 02:53:29.720]   to a new equilibrium where we do that instead.
[02:53:29.720 --> 02:53:33.440]   - What are the components of the better ways to do it?
[02:53:33.440 --> 02:53:36.820]   Is it money?
[02:53:36.820 --> 02:53:40.620]   So the sources of money and how the money is allocated
[02:53:40.620 --> 02:53:44.500]   to give the individual researchers freedom?
[02:53:44.500 --> 02:53:47.440]   - Years ago, I started studying this topic exactly
[02:53:47.440 --> 02:53:48.760]   because this was my issue
[02:53:48.760 --> 02:53:50.680]   and this was many decades ago now,
[02:53:50.680 --> 02:53:51.720]   and I spent a long time,
[02:53:51.720 --> 02:53:55.340]   and my best guess still is prediction markets,
[02:53:55.340 --> 02:53:56.180]   betting markets.
[02:53:56.180 --> 02:53:59.920]   So if you as a research patron want to know
[02:53:59.920 --> 02:54:01.840]   the answer to a particular question,
[02:54:01.840 --> 02:54:04.280]   like what's the mass of the electron neutrino,
[02:54:04.280 --> 02:54:07.760]   then what you can do is just subsidize a betting market
[02:54:07.760 --> 02:54:11.180]   in that question, and that will induce more research
[02:54:11.180 --> 02:54:12.340]   into answering that question
[02:54:12.340 --> 02:54:14.320]   because the people who then answer that question
[02:54:14.320 --> 02:54:15.940]   can then make money in that betting market
[02:54:15.940 --> 02:54:17.900]   with the new information they gain.
[02:54:17.900 --> 02:54:21.420]   So that's a robust way to induce more information
[02:54:21.420 --> 02:54:22.260]   on a topic.
[02:54:22.260 --> 02:54:23.780]   If you want to induce an accomplishment,
[02:54:23.780 --> 02:54:25.040]   you can create prizes,
[02:54:25.040 --> 02:54:26.820]   and there's of course a long history
[02:54:26.820 --> 02:54:29.660]   of prizes to induce accomplishments.
[02:54:29.660 --> 02:54:32.220]   And we moved away from prizes,
[02:54:32.220 --> 02:54:34.980]   even though we once used them far more often
[02:54:34.980 --> 02:54:35.980]   than we did today.
[02:54:36.940 --> 02:54:38.480]   And there's a history to that.
[02:54:38.480 --> 02:54:42.860]   And for the customers who want to be affiliated
[02:54:42.860 --> 02:54:44.740]   with impressive academics,
[02:54:44.740 --> 02:54:46.320]   which is what most of the customers want,
[02:54:46.320 --> 02:54:48.460]   students, journalists, and patrons,
[02:54:48.460 --> 02:54:50.080]   I think there's a better way of doing that,
[02:54:50.080 --> 02:54:54.640]   which I just wrote about in my second most recent blog post.
[02:54:54.640 --> 02:54:55.520]   - Can you explain?
[02:54:55.520 --> 02:54:56.360]   - Sure.
[02:54:56.360 --> 02:54:58.960]   What we do today is we take sort of acceptance
[02:54:58.960 --> 02:55:02.560]   by other academics recently as our best indication
[02:55:02.560 --> 02:55:04.240]   of their deserved prestige.
[02:55:04.260 --> 02:55:08.820]   That is recent publications, recent job affiliation,
[02:55:08.820 --> 02:55:12.900]   institutional affiliations, recent invitations to speak,
[02:55:12.900 --> 02:55:14.780]   recent grants.
[02:55:14.780 --> 02:55:19.060]   We are today taking other impressive academics'
[02:55:19.060 --> 02:55:21.740]   recent choices to affiliate with them
[02:55:21.740 --> 02:55:24.020]   as our best guesstimate of their prestige.
[02:55:24.020 --> 02:55:28.300]   I would say we could do better by creating betting markets
[02:55:28.300 --> 02:55:30.700]   in what the distant future will judge
[02:55:30.700 --> 02:55:32.900]   to have been their deserved prestige,
[02:55:32.900 --> 02:55:34.480]   looking back on them.
[02:55:34.480 --> 02:55:37.280]   I think most intellectuals, for example,
[02:55:37.280 --> 02:55:39.880]   think that if we looked back two centuries,
[02:55:39.880 --> 02:55:42.800]   say to intellectuals from two centuries ago,
[02:55:42.800 --> 02:55:45.680]   and tried to look in detail at their research
[02:55:45.680 --> 02:55:47.460]   and how it influenced future research
[02:55:47.460 --> 02:55:49.240]   and which path it was on,
[02:55:49.240 --> 02:55:52.640]   we could much more accurately judge
[02:55:52.640 --> 02:55:55.240]   their actual deserved prestige.
[02:55:55.240 --> 02:55:57.040]   That is who was actually on the right track,
[02:55:57.040 --> 02:55:58.560]   who actually helped,
[02:55:58.560 --> 02:56:01.200]   which will be different than what people at the time judged
[02:56:01.200 --> 02:56:03.740]   using the immediate indications of the time
[02:56:03.740 --> 02:56:04.860]   or which position they had
[02:56:04.860 --> 02:56:07.600]   or which publications they had or things like that.
[02:56:07.600 --> 02:56:10.780]   - In this way, if you think from the perspective
[02:56:10.780 --> 02:56:12.700]   of multiple centuries,
[02:56:12.700 --> 02:56:17.380]   you would higher prioritize true novelty,
[02:56:17.380 --> 02:56:19.980]   you would disregard the temporal proximity,
[02:56:19.980 --> 02:56:22.220]   like how recent the thing is,
[02:56:22.220 --> 02:56:25.700]   and you would think like what is the brave, the bold,
[02:56:25.700 --> 02:56:28.420]   the big, novel idea that this,
[02:56:28.420 --> 02:56:29.260]   and you would actually--
[02:56:29.260 --> 02:56:31.060]   - You would be able to rate that
[02:56:31.060 --> 02:56:33.420]   'cause you could see the path with which ideas took,
[02:56:33.420 --> 02:56:34.500]   which things had dead ends,
[02:56:34.500 --> 02:56:36.020]   which led to what other followings.
[02:56:36.020 --> 02:56:39.120]   You could, looking back centuries later,
[02:56:39.120 --> 02:56:41.760]   have a much better estimate of who actually had
[02:56:41.760 --> 02:56:44.100]   what long-term effects on intellectual progress.
[02:56:44.100 --> 02:56:47.140]   So my proposal is we actually pay people
[02:56:47.140 --> 02:56:49.500]   in several centuries to do this historical analysis
[02:56:49.500 --> 02:56:52.340]   and we have prediction markets today
[02:56:52.340 --> 02:56:55.300]   where we buy and sell assets which will later off pay off
[02:56:55.300 --> 02:56:57.660]   in terms of those final evaluations.
[02:56:57.660 --> 02:56:59.520]   So now we'll be inducing people today
[02:56:59.520 --> 02:57:01.100]   to make their best estimate of those things
[02:57:01.100 --> 02:57:04.620]   by actually looking at the details of people
[02:57:04.620 --> 02:57:06.260]   and setting the prices according.
[02:57:06.260 --> 02:57:08.820]   So my proposal would be we rate people today
[02:57:08.820 --> 02:57:10.220]   on those prices today.
[02:57:10.220 --> 02:57:12.420]   So instead of looking at their list of publications
[02:57:12.420 --> 02:57:15.340]   or affiliations, you look at the actual price of assets
[02:57:15.340 --> 02:57:17.880]   that represent people's best guess
[02:57:17.880 --> 02:57:19.380]   of what the future will say about them.
[02:57:19.380 --> 02:57:20.220]   - That's brilliant.
[02:57:20.220 --> 02:57:23.540]   So this concept of idea futures,
[02:57:23.540 --> 02:57:26.660]   can you elaborate what this would entail?
[02:57:27.780 --> 02:57:30.240]   - I've been elaborating two versions of it here.
[02:57:30.240 --> 02:57:33.040]   So one is if there's a particular question,
[02:57:33.040 --> 02:57:35.520]   say the mass of the electron neutrino,
[02:57:35.520 --> 02:57:37.160]   and what you as a patron wanna do
[02:57:37.160 --> 02:57:39.760]   is get an answer to that question,
[02:57:39.760 --> 02:57:42.080]   then what you would do is subsidize a betting market
[02:57:42.080 --> 02:57:44.260]   in that question under the assumption
[02:57:44.260 --> 02:57:45.840]   that eventually we'll just know the answer
[02:57:45.840 --> 02:57:48.240]   and we can pay off the bets that way.
[02:57:48.240 --> 02:57:49.560]   And that is a plausible assumption
[02:57:49.560 --> 02:57:52.240]   for many kinds of concrete intellectual questions
[02:57:52.240 --> 02:57:53.880]   like what's the mass of the electron neutrino.
[02:57:53.880 --> 02:57:56.400]   - In this hypothetical world that you're constructing
[02:57:56.400 --> 02:57:58.160]   that may be a real world,
[02:57:58.160 --> 02:58:01.200]   do you mean literally financial?
[02:58:01.200 --> 02:58:03.520]   - Yes, literal, very literal.
[02:58:03.520 --> 02:58:07.760]   Very cash, very direct and literal, yes.
[02:58:07.760 --> 02:58:09.100]   - Or crypto.
[02:58:09.100 --> 02:58:10.760]   - Well, crypto is money.
[02:58:10.760 --> 02:58:11.600]   - Yes, true.
[02:58:11.600 --> 02:58:15.240]   - So the idea would be research labs would be for profit.
[02:58:15.240 --> 02:58:17.060]   They would have as their expense,
[02:58:17.060 --> 02:58:18.600]   paying researchers to study things,
[02:58:18.600 --> 02:58:21.200]   and then their profit would come from using the insights
[02:58:21.200 --> 02:58:24.100]   the researchers gains to trade in these financial markets.
[02:58:25.540 --> 02:58:27.400]   Just like hedge funds today make money
[02:58:27.400 --> 02:58:29.740]   by paying researchers to study firms
[02:58:29.740 --> 02:58:32.440]   and then making their profits by trading on those,
[02:58:32.440 --> 02:58:34.280]   that insight in the ordinary financial market.
[02:58:34.280 --> 02:58:37.800]   - And the market would, if it's efficient,
[02:58:37.800 --> 02:58:39.720]   would be able to become better and better
[02:58:39.720 --> 02:58:42.320]   predicting the powerful ideas
[02:58:42.320 --> 02:58:44.320]   that the individual is able to generate.
[02:58:44.320 --> 02:58:46.600]   - The variance around the mass of the electron neutrino
[02:58:46.600 --> 02:58:48.440]   would decrease with time as we learned
[02:58:48.440 --> 02:58:49.960]   that value of that parameter better
[02:58:49.960 --> 02:58:52.600]   and any other parameters that we wanted to estimate.
[02:58:52.600 --> 02:58:54.520]   - You don't think those markets would also respond
[02:58:54.520 --> 02:58:58.680]   to recency of prestige and all those kinds of things?
[02:58:58.680 --> 02:59:00.960]   - They would respond, but the question is,
[02:59:00.960 --> 02:59:02.680]   if they might respond incorrectly,
[02:59:02.680 --> 02:59:04.400]   but if you think they're doing it incorrectly,
[02:59:04.400 --> 02:59:05.240]   you have a profit opportunity.
[02:59:05.240 --> 02:59:06.760]   - There'll be a correction mechanism.
[02:59:06.760 --> 02:59:08.120]   - You can go fix it.
[02:59:08.120 --> 02:59:10.360]   So we'd be inviting everybody to ask
[02:59:10.360 --> 02:59:12.400]   whether they can find any biases or errors
[02:59:12.400 --> 02:59:14.200]   in the current ways in which people are estimating
[02:59:14.200 --> 02:59:15.760]   these things from whatever clues they have.
[02:59:15.760 --> 02:59:16.960]   - Right, there's a big incentive
[02:59:16.960 --> 02:59:18.640]   for the correction mechanism.
[02:59:18.640 --> 02:59:21.480]   In academia currently, there's not,
[02:59:22.400 --> 02:59:25.080]   it's the safe choice to go with the prestige.
[02:59:25.080 --> 02:59:25.920]   - Exactly.
[02:59:25.920 --> 02:59:26.840]   - And there's no--
[02:59:26.840 --> 02:59:28.800]   - Even if you privately think that the prestige
[02:59:28.800 --> 02:59:30.960]   is overrated.
[02:59:30.960 --> 02:59:34.200]   - Even if you privately think strongly that it's overrated.
[02:59:34.200 --> 02:59:36.680]   - Still, you don't have an incentive to defy that publicly.
[02:59:36.680 --> 02:59:38.520]   - You're going to lose a lot,
[02:59:38.520 --> 02:59:42.720]   unless you're a contrarian that writes brilliant blogs
[02:59:42.720 --> 02:59:45.560]   and you could talk about it on a podcast.
[02:59:45.560 --> 02:59:46.400]   - Right.
[02:59:46.400 --> 02:59:48.120]   I mean, initially, this was my initial concept
[02:59:48.120 --> 02:59:50.240]   of having these betting markets on these key parameters.
[02:59:50.240 --> 02:59:52.600]   What I then realized over time was that
[02:59:52.600 --> 02:59:54.520]   that's more what people pretend to care about.
[02:59:54.520 --> 02:59:58.200]   What they really mostly care about is just who's how good.
[02:59:58.200 --> 02:59:59.960]   And that's what most of the system is built on,
[02:59:59.960 --> 03:00:01.840]   is trying to rate people and rank them.
[03:00:01.840 --> 03:00:04.080]   And so I designed this other alternative
[03:00:04.080 --> 03:00:06.480]   based on historical evaluation centuries later,
[03:00:06.480 --> 03:00:08.600]   just about who's how good,
[03:00:08.600 --> 03:00:10.240]   because that's what I think most of the customers
[03:00:10.240 --> 03:00:11.120]   really care about.
[03:00:11.120 --> 03:00:15.960]   - Customers, I like the word customers here, humans.
[03:00:15.960 --> 03:00:18.440]   - Right, well, every major area of life,
[03:00:18.440 --> 03:00:21.080]   which has specialists who get paid to do that thing,
[03:00:21.080 --> 03:00:22.920]   must have some customers from elsewhere
[03:00:22.920 --> 03:00:24.520]   who are paying for it.
[03:00:24.520 --> 03:00:27.400]   - Well, who are the customers for the mass of the neutrino?
[03:00:27.400 --> 03:00:31.200]   Yes, I understand, in a sense,
[03:00:31.200 --> 03:00:35.480]   people who are willing to pay for a thing.
[03:00:35.480 --> 03:00:37.560]   - That's an important thing to understand about anything,
[03:00:37.560 --> 03:00:39.760]   who are the customers, and what's the product,
[03:00:39.760 --> 03:00:44.280]   like medicine, education, academia, military, et cetera.
[03:00:44.280 --> 03:00:45.920]   That's part of the hidden motives analysis.
[03:00:45.920 --> 03:00:47.440]   Often people have a thing they say
[03:00:47.440 --> 03:00:49.240]   about what the product is and who the customer is,
[03:00:49.240 --> 03:00:51.120]   and maybe you need to dig a little deeper
[03:00:51.120 --> 03:00:52.880]   to find out what's really going on.
[03:00:52.880 --> 03:00:54.960]   - Or a lot deeper.
[03:00:54.960 --> 03:00:59.960]   You've written that you seek out, quote, view quakes.
[03:00:59.960 --> 03:01:02.880]   You're able, as an intelligent black box
[03:01:02.880 --> 03:01:04.320]   word generating machine,
[03:01:04.320 --> 03:01:06.040]   you're able to generate a lot of sexy words.
[03:01:06.040 --> 03:01:07.440]   I like it, I love it.
[03:01:07.440 --> 03:01:10.520]   View quakes, which are insights
[03:01:10.520 --> 03:01:14.800]   which dramatically changed my worldview, your worldview.
[03:01:15.920 --> 03:01:18.880]   You write, I loved science fiction as a child,
[03:01:18.880 --> 03:01:21.000]   studied physics and artificial intelligence
[03:01:21.000 --> 03:01:22.680]   for a long time each,
[03:01:22.680 --> 03:01:25.600]   and now study economics and political science,
[03:01:25.600 --> 03:01:28.720]   all fields full of such insights.
[03:01:28.720 --> 03:01:31.940]   So, let me ask, what are some view quakes,
[03:01:31.940 --> 03:01:34.600]   or a beautiful, surprising idea to you
[03:01:34.600 --> 03:01:36.280]   from each of those fields,
[03:01:36.280 --> 03:01:38.920]   physics, AI, economics, political science?
[03:01:38.920 --> 03:01:40.320]   I know it's a tough question,
[03:01:40.320 --> 03:01:43.180]   something that springs to mind about physics, for example,
[03:01:43.180 --> 03:01:44.360]   that just is beautiful to me.
[03:01:44.360 --> 03:01:46.120]   Right from the beginning, say,
[03:01:46.120 --> 03:01:48.460]   special relativity was a big surprise.
[03:01:48.460 --> 03:01:51.720]   Most of us have a simple concept of time,
[03:01:51.720 --> 03:01:53.280]   and it seems perfectly adequate
[03:01:53.280 --> 03:01:55.080]   for everything we've ever seen.
[03:01:55.080 --> 03:01:56.560]   And to have it explained to you
[03:01:56.560 --> 03:01:58.800]   that you need to sort of have a mixture concept
[03:01:58.800 --> 03:01:59.700]   of time and space,
[03:01:59.700 --> 03:02:02.200]   where you put it into the space-time construct,
[03:02:02.200 --> 03:02:05.120]   how it looks different from different perspectives,
[03:02:05.120 --> 03:02:06.680]   that was quite a shock.
[03:02:06.680 --> 03:02:10.080]   And that was such a shock that it makes you think,
[03:02:10.080 --> 03:02:13.440]   what else do I know that isn't the way it seems?
[03:02:13.440 --> 03:02:15.200]   Certainly, quantum mechanics is certainly
[03:02:15.200 --> 03:02:17.520]   another enormous shock in terms of,
[03:02:17.520 --> 03:02:18.680]   from your point, you know,
[03:02:18.680 --> 03:02:20.580]   you have this idea that there's a space,
[03:02:20.580 --> 03:02:23.440]   and then there's particles at points,
[03:02:23.440 --> 03:02:25.140]   and maybe fields in between.
[03:02:25.140 --> 03:02:28.440]   And quantum mechanics is just
[03:02:28.440 --> 03:02:29.600]   a whole different representation.
[03:02:29.600 --> 03:02:32.000]   It looks nothing like what you would have thought
[03:02:32.000 --> 03:02:35.140]   as sort of the basic representation of the physical world.
[03:02:35.140 --> 03:02:36.880]   And that was quite a surprise.
[03:02:36.880 --> 03:02:39.020]   - What would you say is the catalyst
[03:02:39.020 --> 03:02:42.200]   for the view quake in theoretical physics
[03:02:42.200 --> 03:02:43.080]   in the 20th century?
[03:02:43.080 --> 03:02:44.560]   Where does that come from?
[03:02:44.560 --> 03:02:46.000]   So the interesting thing about Einstein,
[03:02:46.000 --> 03:02:47.240]   it seems like a lot of that came
[03:02:47.240 --> 03:02:49.200]   from like almost thought experiments.
[03:02:49.200 --> 03:02:51.860]   It wasn't almost experimentally driven.
[03:02:51.860 --> 03:02:56.480]   And with, actually, I don't know
[03:02:56.480 --> 03:02:58.240]   the full story of quantum mechanics,
[03:02:58.240 --> 03:02:59.960]   how much of it is experiment,
[03:02:59.960 --> 03:03:03.300]   like where, if you look at the full trace
[03:03:03.300 --> 03:03:05.280]   of idea generation there,
[03:03:05.280 --> 03:03:08.720]   of all the weird stuff that falls out of quantum mechanics,
[03:03:08.720 --> 03:03:10.640]   how much of that was the experimentalists,
[03:03:10.640 --> 03:03:12.160]   how much was it the theoreticians?
[03:03:12.160 --> 03:03:14.000]   But usually, in theoretical physics,
[03:03:14.000 --> 03:03:15.720]   the theories lead the way.
[03:03:15.720 --> 03:03:19.720]   So maybe can you elucidate,
[03:03:19.720 --> 03:03:22.420]   like what is the catalyst for these?
[03:03:22.420 --> 03:03:24.540]   - The remarkable thing about physics
[03:03:24.540 --> 03:03:28.000]   and about many other areas of academic intellectual life
[03:03:28.000 --> 03:03:30.680]   is that it just seems way over-determined.
[03:03:30.680 --> 03:03:34.080]   That is, if it hadn't been for Einstein
[03:03:34.080 --> 03:03:36.120]   or if it hadn't been for Heisenberg,
[03:03:36.120 --> 03:03:37.840]   certainly within a half a century,
[03:03:37.840 --> 03:03:39.440]   somebody else would have come up
[03:03:39.440 --> 03:03:42.120]   with essentially the same things.
[03:03:42.120 --> 03:03:43.320]   - Is that something you believe?
[03:03:43.320 --> 03:03:44.160]   - Yes. - Or is that something?
[03:03:44.160 --> 03:03:46.000]   - Yes, so I think when you look at
[03:03:46.000 --> 03:03:47.600]   sort of just the history of physics
[03:03:47.600 --> 03:03:49.480]   and the history of other areas,
[03:03:49.480 --> 03:03:50.360]   some areas like that,
[03:03:50.360 --> 03:03:52.400]   there's just this enormous convergence.
[03:03:52.400 --> 03:03:54.000]   That the different kind of evidence
[03:03:54.000 --> 03:03:57.600]   that was being collected was so redundant
[03:03:57.600 --> 03:03:59.480]   in the sense that so many different things
[03:03:59.480 --> 03:04:02.080]   revealed the same things that eventually
[03:04:02.080 --> 03:04:04.280]   you just kind of have to accept it
[03:04:04.280 --> 03:04:06.580]   because it just gets obvious.
[03:04:06.580 --> 03:04:09.180]   So if you look at the details, of course,
[03:04:09.180 --> 03:04:11.760]   Einstein did it before somebody else,
[03:04:11.760 --> 03:04:14.240]   and it's well worth celebrating Einstein for that.
[03:04:14.240 --> 03:04:17.520]   And we, by celebrating the particular people
[03:04:17.520 --> 03:04:19.960]   who did something first or came across something first,
[03:04:19.960 --> 03:04:23.320]   we are encouraging all the rest to move a little faster,
[03:04:23.320 --> 03:04:28.760]   to try to push us all a little faster, which is great,
[03:04:28.760 --> 03:04:31.920]   but I still think we would have gotten
[03:04:31.920 --> 03:04:34.520]   roughly to the same place within a half century.
[03:04:34.520 --> 03:04:37.040]   So sometimes people are special
[03:04:37.040 --> 03:04:39.400]   because of how much longer it would have taken.
[03:04:39.400 --> 03:04:41.000]   So some people say general relativity
[03:04:41.000 --> 03:04:43.880]   would have taken longer without Einstein than other things.
[03:04:43.880 --> 03:04:45.960]   I mean, Heisenberg quantum mechanics,
[03:04:45.960 --> 03:04:47.400]   I mean, there were several different formulations
[03:04:47.400 --> 03:04:50.320]   of quantum mechanics all around the same few years,
[03:04:50.320 --> 03:04:52.880]   means no one of them made that much of a difference.
[03:04:52.880 --> 03:04:54.640]   We would have had pretty much the same thing
[03:04:54.640 --> 03:04:57.400]   regardless of which of them did it exactly when.
[03:04:57.400 --> 03:05:00.040]   Nevertheless, I'm happy to celebrate them all.
[03:05:00.040 --> 03:05:02.440]   But this is a choice I make in my research.
[03:05:02.440 --> 03:05:03.280]   That is, when there's an area
[03:05:03.280 --> 03:05:05.960]   where there's lots of people working together,
[03:05:05.960 --> 03:05:08.080]   who are sort of scoping each other
[03:05:08.080 --> 03:05:10.720]   and getting a result just before somebody else does,
[03:05:10.720 --> 03:05:14.040]   you ask, well, how much of a difference would I make there?
[03:05:14.040 --> 03:05:15.640]   At most, I could make something happen
[03:05:15.640 --> 03:05:17.800]   a few months before somebody else.
[03:05:17.800 --> 03:05:20.600]   And so I'm less worried about them missing things.
[03:05:20.600 --> 03:05:23.080]   So when I'm trying to help the world, like doing research,
[03:05:23.080 --> 03:05:24.200]   I'm looking for neglected things.
[03:05:24.200 --> 03:05:26.200]   I'm looking for things that nobody's doing it.
[03:05:26.200 --> 03:05:28.200]   If I didn't do it, nobody would do it.
[03:05:28.200 --> 03:05:29.120]   - Nobody would do it.
[03:05:29.120 --> 03:05:29.960]   - Or at least for a long time.
[03:05:29.960 --> 03:05:31.160]   - In the next 10, 20 years kind of thing.
[03:05:31.160 --> 03:05:32.000]   - Exactly.
[03:05:32.000 --> 03:05:33.520]   - Same with general relativity, just, you know,
[03:05:33.520 --> 03:05:35.160]   the whole would do it.
[03:05:35.160 --> 03:05:37.520]   It might take another 10, 20, 30, 50 years.
[03:05:37.520 --> 03:05:38.360]   - So that's the place
[03:05:38.360 --> 03:05:39.760]   where you can have the biggest impact,
[03:05:39.760 --> 03:05:41.640]   is finding the things that nobody would do
[03:05:41.640 --> 03:05:43.400]   unless you did them.
[03:05:43.400 --> 03:05:46.200]   - And then that's when you get the big view quake,
[03:05:46.200 --> 03:05:47.240]   the insight.
[03:05:47.240 --> 03:05:49.440]   So what about artificial intelligence?
[03:05:49.440 --> 03:05:54.440]   Would it be the EMs, the emulated minds?
[03:05:54.440 --> 03:05:59.600]   What idea, whether that struck you in the shower one day,
[03:05:59.600 --> 03:06:03.440]   or are they you just observed?
[03:06:03.440 --> 03:06:05.120]   - Clearly, the biggest view quake
[03:06:05.120 --> 03:06:08.200]   in artificial intelligence is the realization
[03:06:08.200 --> 03:06:10.880]   of just how complicated our human minds are.
[03:06:10.880 --> 03:06:14.420]   So most people who come to artificial intelligence
[03:06:14.420 --> 03:06:17.820]   from other fields or from relative ignorance,
[03:06:17.820 --> 03:06:20.160]   a very common phenomenon, which you must be familiar with,
[03:06:20.160 --> 03:06:22.360]   is that they come up with some concept
[03:06:22.360 --> 03:06:24.640]   and then they think that must be it.
[03:06:24.640 --> 03:06:27.200]   Once we implement this new concept, we will have it.
[03:06:27.200 --> 03:06:28.880]   We will have full human level
[03:06:28.880 --> 03:06:30.560]   or higher artificial intelligence, right?
[03:06:30.560 --> 03:06:34.240]   And they're just not appreciating just how big the problem is,
[03:06:34.240 --> 03:06:36.720]   how long the road is, just how much is involved,
[03:06:36.720 --> 03:06:38.680]   because that's actually hard to appreciate.
[03:06:38.680 --> 03:06:41.600]   When we just think, it seems really simple.
[03:06:41.600 --> 03:06:43.720]   And studying artificial intelligence,
[03:06:43.720 --> 03:06:45.160]   going through many particular problems,
[03:06:45.160 --> 03:06:47.220]   looking in each problem, all the different things
[03:06:47.220 --> 03:06:49.960]   you need to be able to do to solve a problem like that
[03:06:49.960 --> 03:06:52.360]   makes you realize all the things your minds are doing
[03:06:52.360 --> 03:06:54.200]   that you are not aware of.
[03:06:54.200 --> 03:06:57.560]   That's that vast subconscious that you're not aware.
[03:06:57.560 --> 03:06:58.560]   That's the biggest view quake
[03:06:58.560 --> 03:07:00.180]   from artificial intelligence by far,
[03:07:00.180 --> 03:07:02.320]   for most people who study artificial intelligence,
[03:07:02.320 --> 03:07:05.080]   is to see just how hard it is.
[03:07:05.080 --> 03:07:07.160]   - I think that's a good point.
[03:07:07.160 --> 03:07:10.920]   But I think it's a very early view quake.
[03:07:10.920 --> 03:07:15.920]   It's when the stunning Kruger crashes hard.
[03:07:15.920 --> 03:07:18.840]   It's the first realization
[03:07:18.840 --> 03:07:21.160]   that humans are actually quite incredible.
[03:07:21.160 --> 03:07:23.240]   The human mind, the human body is quite incredible.
[03:07:23.240 --> 03:07:25.220]   - There's a lot of different parts to it.
[03:07:25.220 --> 03:07:29.460]   - But then, see, it's already been so long for me
[03:07:29.460 --> 03:07:31.080]   that I've experienced that view quake,
[03:07:31.080 --> 03:07:34.040]   that for me, I now experience the view quakes
[03:07:34.040 --> 03:07:37.400]   of holy shit, this little thing is actually quite powerful.
[03:07:37.400 --> 03:07:39.880]   Like neural networks, I'm amazed.
[03:07:39.880 --> 03:07:43.520]   'Cause you've become more cynical
[03:07:43.520 --> 03:07:48.000]   after that first view quake of like, this is so hard.
[03:07:48.000 --> 03:07:50.440]   Like evolution did some incredible work
[03:07:50.440 --> 03:07:52.240]   to create the human mind.
[03:07:52.240 --> 03:07:55.000]   But then you realize, Jessica, as you have,
[03:07:55.000 --> 03:07:57.280]   you've talked about a bunch of simple models,
[03:07:57.280 --> 03:08:01.080]   that simple things can actually be extremely powerful.
[03:08:01.080 --> 03:08:06.000]   That maybe emulating the human mind is extremely difficult,
[03:08:06.000 --> 03:08:09.000]   but you can go a long way with a large neural network.
[03:08:09.000 --> 03:08:11.000]   You can go a long way with a dumb solution.
[03:08:11.000 --> 03:08:12.140]   It's that Stuart Russell thing
[03:08:12.140 --> 03:08:14.120]   with the reinforcement learning.
[03:08:14.120 --> 03:08:17.480]   Holy crap, you can go quite a long way with a simple thing.
[03:08:17.480 --> 03:08:19.160]   - But we still have a very long road to go,
[03:08:19.160 --> 03:08:20.640]   but nevertheless.
[03:08:20.640 --> 03:08:23.400]   - I can't, I refuse to sort of know.
[03:08:23.400 --> 03:08:27.720]   The road is full of surprises.
[03:08:27.720 --> 03:08:30.520]   So long is an interesting, like you said,
[03:08:30.520 --> 03:08:34.080]   with the six hard steps that humans have to take
[03:08:34.080 --> 03:08:37.600]   to arrive at where we are from the origin of life on Earth.
[03:08:37.600 --> 03:08:42.040]   So it's long maybe in the statistical improbability
[03:08:42.040 --> 03:08:44.560]   of the steps that have to be taken.
[03:08:44.560 --> 03:08:48.000]   But in terms of how quickly those steps could be taken,
[03:08:48.000 --> 03:08:50.800]   I don't know if my intuition says
[03:08:50.800 --> 03:08:53.560]   it's if it's hundreds of years away,
[03:08:53.560 --> 03:08:57.480]   or if it's a couple of years away.
[03:08:57.480 --> 03:08:59.000]   I prefer to measure--
[03:08:59.000 --> 03:09:00.880]   - Pretty confident it's at least a decade.
[03:09:00.880 --> 03:09:03.400]   And mildly confident it's at least three decades.
[03:09:03.400 --> 03:09:05.480]   - I can steel man either direction.
[03:09:05.480 --> 03:09:08.760]   I prefer to measure that journey in Elon Musk's.
[03:09:08.760 --> 03:09:09.720]   That's a new--
[03:09:09.720 --> 03:09:11.080]   - We don't get Elon Musk very often,
[03:09:11.080 --> 03:09:12.680]   so that's a long timescale.
[03:09:12.680 --> 03:09:15.560]   - For now, I don't know, maybe you can clone,
[03:09:15.560 --> 03:09:18.880]   or maybe multiply, or I don't even know what Elon Musk,
[03:09:18.880 --> 03:09:21.080]   what that is, what is that?
[03:09:21.080 --> 03:09:23.000]   - That's a good question, exactly.
[03:09:23.000 --> 03:09:24.000]   Well, that's an excellent question.
[03:09:24.000 --> 03:09:28.080]   - How does that fit into the model of the three parameters
[03:09:28.080 --> 03:09:33.080]   that are required for becoming a grabby alien civilization?
[03:09:33.080 --> 03:09:35.880]   - That's the question of how much any individual makes
[03:09:35.880 --> 03:09:38.720]   in the long path of civilization over time.
[03:09:38.720 --> 03:09:41.320]   Yes, and it's a favorite topic of historians
[03:09:41.320 --> 03:09:43.960]   and people to try to focus on individuals
[03:09:43.960 --> 03:09:45.240]   and how much of a difference they make.
[03:09:45.240 --> 03:09:48.080]   And certainly some individuals make a substantial difference
[03:09:48.080 --> 03:09:50.400]   in the modest term, right?
[03:09:50.400 --> 03:09:53.880]   Like, certainly without Hitler being Hitler
[03:09:53.880 --> 03:09:55.440]   in the role he took,
[03:09:55.440 --> 03:09:57.440]   European history would have taken a different path
[03:09:57.440 --> 03:09:59.280]   for a while there.
[03:09:59.280 --> 03:10:02.520]   But if we're looking over many centuries longer term things,
[03:10:02.520 --> 03:10:05.800]   most individuals do fade in their individual influence.
[03:10:05.800 --> 03:10:08.360]   - So, I mean--
[03:10:08.360 --> 03:10:09.200]   - Even Einstein.
[03:10:09.200 --> 03:10:10.800]   - Even Einstein.
[03:10:10.800 --> 03:10:13.920]   No matter how sexy your hair is,
[03:10:13.920 --> 03:10:17.720]   you will also be forgotten in the long arc of history.
[03:10:17.720 --> 03:10:19.760]   So you said at least 10 years,
[03:10:19.760 --> 03:10:23.960]   so let's talk a little bit about this AI point
[03:10:26.320 --> 03:10:28.120]   of how we achieve,
[03:10:28.120 --> 03:10:31.120]   how hard is the problem of solving intelligence
[03:10:31.120 --> 03:10:35.440]   by engineering artificial intelligence
[03:10:35.440 --> 03:10:39.840]   that achieves human-level, human-like qualities
[03:10:39.840 --> 03:10:41.240]   that we associate with intelligence?
[03:10:41.240 --> 03:10:42.120]   How hard is this?
[03:10:42.120 --> 03:10:45.240]   What are the different trajectories that take us there?
[03:10:45.240 --> 03:10:48.600]   - One way to think about it is in terms of the scope
[03:10:48.600 --> 03:10:50.920]   of the technology space you're talking about.
[03:10:50.920 --> 03:10:53.480]   So let's take the biggest possible scope,
[03:10:53.480 --> 03:10:56.160]   all of human technology, right?
[03:10:56.160 --> 03:10:57.880]   The entire human economy.
[03:10:57.880 --> 03:11:02.320]   So the entire economy is composed of many industries,
[03:11:02.320 --> 03:11:03.680]   each of which have many products
[03:11:03.680 --> 03:11:06.320]   with many different technologies supporting each one.
[03:11:06.320 --> 03:11:10.560]   At that scale, I think we can accept
[03:11:10.560 --> 03:11:14.560]   that most innovations are a small fraction of the total.
[03:11:14.560 --> 03:11:18.200]   That is, usually you have relatively gradual overall progress
[03:11:18.200 --> 03:11:21.920]   and that individual innovations
[03:11:21.920 --> 03:11:23.400]   that have a substantial effect,
[03:11:23.400 --> 03:11:26.000]   that total are rare and their total effect
[03:11:26.000 --> 03:11:29.000]   is still a small percentage of the total economy, right?
[03:11:29.000 --> 03:11:30.800]   There's very few individual innovations
[03:11:30.800 --> 03:11:34.120]   that made a substantial difference to the whole economy.
[03:11:34.120 --> 03:11:36.080]   What are we talking, steam engine,
[03:11:36.080 --> 03:11:38.080]   shipping containers, a few things.
[03:11:38.080 --> 03:11:40.600]   - Shipping containers?
[03:11:40.600 --> 03:11:42.000]   - Shipping containers deserves to be up there
[03:11:42.000 --> 03:11:44.160]   with steam engines, honestly.
[03:11:44.160 --> 03:11:47.080]   - Can you say exactly why shipping containers?
[03:11:47.080 --> 03:11:49.800]   - Shipping containers revolutionized shipping.
[03:11:49.800 --> 03:11:51.240]   Shipping is very important.
[03:11:51.240 --> 03:11:55.480]   - But placing that at shipping containers,
[03:11:55.480 --> 03:11:57.640]   so you're saying you wouldn't have some of the magic
[03:11:57.640 --> 03:11:59.240]   of the supply chain and all that
[03:11:59.240 --> 03:12:00.880]   without shipping containers?
[03:12:00.880 --> 03:12:02.280]   - Made a big difference, absolutely.
[03:12:02.280 --> 03:12:05.200]   - Interesting, that's something we'll look into.
[03:12:05.200 --> 03:12:08.000]   We shouldn't take that tangent, although I'm tempted to.
[03:12:08.000 --> 03:12:11.040]   But anyway, so there's a few, just a few innovations.
[03:12:11.040 --> 03:12:14.040]   - Right, so at the scale of the whole economy, right?
[03:12:14.040 --> 03:12:17.480]   Now, as you move down to a much smaller scale,
[03:12:17.480 --> 03:12:19.560]   you will see individual innovations
[03:12:19.560 --> 03:12:21.280]   having a bigger effect, right?
[03:12:21.280 --> 03:12:24.960]   So if you look at, I don't know, lawnmowers or something,
[03:12:24.960 --> 03:12:26.160]   I don't know about the innovations of lawnmower,
[03:12:26.160 --> 03:12:28.800]   but there were probably like steps where you just had
[03:12:28.800 --> 03:12:31.640]   a new kind of lawnmower, and that made a big difference
[03:12:31.640 --> 03:12:35.800]   to mowing lawns, because you're focusing on a smaller part
[03:12:35.800 --> 03:12:38.200]   of the whole technology space, right?
[03:12:38.200 --> 03:12:42.480]   So, and you know, sometimes like military technology,
[03:12:42.480 --> 03:12:43.880]   there's a lot of military technologies,
[03:12:43.880 --> 03:12:45.280]   a lot of small ones, but every once in a while,
[03:12:45.280 --> 03:12:48.840]   a particular military weapon like makes a big difference.
[03:12:48.840 --> 03:12:51.360]   But still, even so, mostly overall,
[03:12:51.360 --> 03:12:54.480]   they're making modest differences to something
[03:12:54.480 --> 03:12:57.040]   that's increasing relatively, like US military
[03:12:57.040 --> 03:12:59.840]   is the strongest in the world consistently for a while.
[03:12:59.840 --> 03:13:03.320]   No one weapon in the last 70 years has like made
[03:13:03.320 --> 03:13:06.080]   a big difference in terms of the overall prominence
[03:13:06.080 --> 03:13:07.400]   of the US military, right?
[03:13:07.400 --> 03:13:09.760]   'Cause that's just saying, even though every once in a while,
[03:13:09.760 --> 03:13:13.360]   even the recent Soviet hyper missiles or whatever they are,
[03:13:13.360 --> 03:13:17.000]   they aren't changing the overall balance dramatically, right?
[03:13:17.000 --> 03:13:21.560]   So when we get to AI, now I can frame the question,
[03:13:21.560 --> 03:13:22.760]   how big is AI?
[03:13:23.680 --> 03:13:26.760]   Basically, if, so one way of thinking about AI
[03:13:26.760 --> 03:13:28.840]   is it's just all mental tasks.
[03:13:28.840 --> 03:13:30.960]   And then you ask what fraction of tasks are mental tasks?
[03:13:30.960 --> 03:13:32.320]   And then I go, a lot.
[03:13:32.320 --> 03:13:37.320]   And then if I think of AI as like half of everything,
[03:13:37.320 --> 03:13:41.240]   then I think, well, it's gotta be composed of lots of parts
[03:13:41.240 --> 03:13:44.480]   where any one innovation is only a small impact, right?
[03:13:44.480 --> 03:13:48.800]   Now, if you think, no, no, no, AI is like AGI.
[03:13:48.800 --> 03:13:52.840]   And then you think AGI is a small thing, right?
[03:13:52.840 --> 03:13:55.080]   There's only a small number of key innovations
[03:13:55.080 --> 03:13:56.640]   that will enable it.
[03:13:56.640 --> 03:14:00.760]   Now you're thinking there could be a bigger chunk
[03:14:00.760 --> 03:14:02.760]   that you might find that would have a bigger impact.
[03:14:02.760 --> 03:14:05.480]   So the way I would ask you to frame these things
[03:14:05.480 --> 03:14:09.840]   in terms of the chunkiness of different areas of technology,
[03:14:09.840 --> 03:14:11.240]   in part, in terms of how big they are.
[03:14:11.240 --> 03:14:14.480]   So if you take 10 chunky areas and you add them together,
[03:14:14.480 --> 03:14:16.080]   the total is less chunky.
[03:14:16.080 --> 03:14:19.840]   - Yeah, but don't you, are you able until you solve
[03:14:19.840 --> 03:14:22.680]   the fundamental core parts of the problem
[03:14:22.680 --> 03:14:25.440]   to estimate the chunkiness of that problem?
[03:14:25.440 --> 03:14:28.360]   - Well, if you have a history of prior chunkiness,
[03:14:28.360 --> 03:14:30.600]   that could be your best estimate for future chunkiness.
[03:14:30.600 --> 03:14:31.680]   So for example, I mean,
[03:14:31.680 --> 03:14:34.080]   even at the level of the world economy, right?
[03:14:34.080 --> 03:14:37.480]   We've had this, what, 10,000 years of civilization.
[03:14:37.480 --> 03:14:39.120]   Well, that's only a short time.
[03:14:39.120 --> 03:14:42.120]   You might say, oh, that doesn't predict future chunkiness.
[03:14:42.120 --> 03:14:46.280]   But it looks relatively steady and consistent.
[03:14:46.280 --> 03:14:48.600]   We can say, even in computer science,
[03:14:48.600 --> 03:14:50.800]   we've had seven years of computer science.
[03:14:50.800 --> 03:14:52.520]   We have enough data to look at chunkiness
[03:14:52.520 --> 03:14:54.040]   in computer science.
[03:14:54.040 --> 03:14:57.400]   Like, when were there algorithms or approaches
[03:14:57.400 --> 03:14:59.160]   that made a big, chunky difference?
[03:14:59.160 --> 03:15:03.720]   And how large a fraction of those that was that?
[03:15:03.720 --> 03:15:05.800]   And I'd say, mostly in computer science,
[03:15:05.800 --> 03:15:07.880]   most innovation has been relatively small chunks.
[03:15:07.880 --> 03:15:10.080]   The bigger chunks have been rare.
[03:15:10.080 --> 03:15:11.600]   - Well, this is the interesting thing.
[03:15:11.600 --> 03:15:14.480]   This is about AI and just algorithms in general,
[03:15:14.480 --> 03:15:17.760]   is page rank.
[03:15:17.760 --> 03:15:19.640]   So Google's, right?
[03:15:19.640 --> 03:15:23.600]   So sometimes it's a simple algorithm
[03:15:23.600 --> 03:15:28.320]   that by itself is not that useful, but the scale--
[03:15:28.320 --> 03:15:29.160]   - Context.
[03:15:29.160 --> 03:15:31.680]   - And in a context that's scalable,
[03:15:31.680 --> 03:15:34.280]   like depending on the, yeah, depending on the context,
[03:15:34.280 --> 03:15:36.480]   is all of a sudden the power is revealed.
[03:15:36.480 --> 03:15:37.480]   And there's something,
[03:15:37.480 --> 03:15:39.720]   I guess that's the nature of chunkiness,
[03:15:39.720 --> 03:15:42.440]   is that you could,
[03:15:42.440 --> 03:15:45.560]   things that can reach a lot of people simply
[03:15:45.560 --> 03:15:46.480]   can be quite chunky.
[03:15:46.480 --> 03:15:49.360]   - So one standard story about algorithms is to say,
[03:15:49.760 --> 03:15:54.400]   algorithms have a fixed cost plus a marginal cost.
[03:15:54.400 --> 03:15:57.000]   And so in history, when you had computers that were
[03:15:57.000 --> 03:15:58.800]   very small, you tried,
[03:15:58.800 --> 03:16:00.760]   all the algorithms had low fixed costs,
[03:16:00.760 --> 03:16:03.400]   and you look for the best of those.
[03:16:03.400 --> 03:16:04.840]   But over time, as computers got bigger,
[03:16:04.840 --> 03:16:07.760]   you could afford to do larger fixed costs and try those.
[03:16:07.760 --> 03:16:10.600]   And some of those had more effective algorithms
[03:16:10.600 --> 03:16:12.560]   in terms of their marginal cost.
[03:16:12.560 --> 03:16:15.720]   And that, in fact, that roughly explains
[03:16:15.720 --> 03:16:17.560]   the long-term history where, in fact,
[03:16:17.560 --> 03:16:19.760]   the rate of algorithmic improvement is about the same
[03:16:19.760 --> 03:16:22.040]   as the rate of hardware improvement,
[03:16:22.040 --> 03:16:23.800]   which is a remarkable coincidence.
[03:16:23.800 --> 03:16:26.440]   But it would be explained by saying,
[03:16:26.440 --> 03:16:29.840]   well, there's all these better algorithms you can't try
[03:16:29.840 --> 03:16:32.440]   until you have a big enough computer to pay the fixed cost
[03:16:32.440 --> 03:16:35.160]   of doing some trials to find out if that algorithm
[03:16:35.160 --> 03:16:37.320]   actually saves you on the marginal cost.
[03:16:37.320 --> 03:16:39.600]   And so that's an explanation
[03:16:39.600 --> 03:16:41.720]   for this relatively continuous history where,
[03:16:41.720 --> 03:16:43.480]   so we have a good story about why hardware
[03:16:43.480 --> 03:16:44.640]   is so continuous, right?
[03:16:44.640 --> 03:16:47.560]   And you might think, why would software be so continuous
[03:16:47.560 --> 03:16:48.400]   with the hardware?
[03:16:48.400 --> 03:16:49.960]   But if there's a distribution of algorithms
[03:16:49.960 --> 03:16:51.840]   in terms of their fixed costs,
[03:16:51.840 --> 03:16:55.280]   and it's, say, spread out in a wide log-normal distribution,
[03:16:55.280 --> 03:16:56.880]   then we could be sort of marching
[03:16:56.880 --> 03:16:58.520]   through that log-normal distribution,
[03:16:58.520 --> 03:17:00.840]   trying out algorithms with larger fixed costs
[03:17:00.840 --> 03:17:04.240]   and finding the ones that have lower marginal costs.
[03:17:04.240 --> 03:17:09.240]   - So would you say AGI, human-level, AI, even EM, M,
[03:17:14.440 --> 03:17:18.560]   emulated minds, is chunky?
[03:17:18.560 --> 03:17:20.040]   Like a few breakthroughs can take this?
[03:17:20.040 --> 03:17:23.080]   - So an M is by its nature chunky,
[03:17:23.080 --> 03:17:25.520]   in the sense that if you have an emulated brain
[03:17:25.520 --> 03:17:29.560]   and you're 25% effective at emulating it, that's crap.
[03:17:29.560 --> 03:17:30.400]   That's nothing.
[03:17:30.400 --> 03:17:35.160]   You pretty much need to emulate a full human brain.
[03:17:35.160 --> 03:17:36.600]   - Is that obvious?
[03:17:36.600 --> 03:17:38.760]   Is that obvious that the 25%- - I think it's pretty obvious.
[03:17:38.760 --> 03:17:40.840]   I'm talking about like, you know,
[03:17:40.840 --> 03:17:43.600]   so the key thing is you're emulating various brain cells,
[03:17:43.600 --> 03:17:44.560]   and so you have to emulate
[03:17:44.560 --> 03:17:46.520]   the input-output pattern of those cells.
[03:17:46.520 --> 03:17:49.040]   So if you get that pattern somewhat close,
[03:17:49.040 --> 03:17:51.400]   but not close enough, then the whole system
[03:17:51.400 --> 03:17:53.200]   just doesn't have the overall behavior
[03:17:53.200 --> 03:17:54.120]   you're looking for, right?
[03:17:54.120 --> 03:17:56.520]   - But it could have, functionally,
[03:17:56.520 --> 03:17:58.120]   some of the power of the overall system.
[03:17:58.120 --> 03:17:59.160]   - So there'll be some threshold.
[03:17:59.160 --> 03:18:00.880]   The point is, when you get close enough,
[03:18:00.880 --> 03:18:02.520]   then it goes over the threshold.
[03:18:02.520 --> 03:18:04.040]   It's like taking a computer chip
[03:18:04.040 --> 03:18:07.200]   and deleting every 1% of the gates, right?
[03:18:07.200 --> 03:18:09.520]   - No, that's very chunky.
[03:18:09.520 --> 03:18:12.840]   But the hope is that the emulating the human brain,
[03:18:12.840 --> 03:18:14.520]   I mean, the human brain itself is not-
[03:18:14.520 --> 03:18:16.680]   - Right, so it has a certain level of redundancy
[03:18:16.680 --> 03:18:17.800]   and a certain level of robustness.
[03:18:17.800 --> 03:18:18.800]   And so there's some threshold.
[03:18:18.800 --> 03:18:20.520]   When you get close to that level of redundancy
[03:18:20.520 --> 03:18:21.920]   and robustness, then it starts to work.
[03:18:21.920 --> 03:18:24.040]   But until you get to that level,
[03:18:24.040 --> 03:18:25.520]   it's just gonna be crap, right?
[03:18:25.520 --> 03:18:28.160]   It's gonna be just a big thing that isn't working well.
[03:18:28.160 --> 03:18:31.160]   So we can be pretty sure that emulations
[03:18:31.160 --> 03:18:34.320]   is a big chunk in an economic sense, right?
[03:18:34.320 --> 03:18:36.000]   At some point, you'll be able to make one
[03:18:36.000 --> 03:18:39.920]   that's actually effective in able substituting for humans,
[03:18:39.920 --> 03:18:43.040]   and then that will be this huge economic product
[03:18:43.040 --> 03:18:44.480]   that people will try to buy like crazy.
[03:18:44.480 --> 03:18:46.240]   - You'll bring a lot of value to people's lives,
[03:18:46.240 --> 03:18:48.960]   so they'll be willing to pay for it.
[03:18:48.960 --> 03:18:51.520]   - But it could be that the first emulation costs
[03:18:51.520 --> 03:18:53.600]   a billion dollars each, right?
[03:18:53.600 --> 03:18:55.760]   And then we have them, but we can't really use them,
[03:18:55.760 --> 03:18:56.600]   they're too expensive.
[03:18:56.600 --> 03:18:58.040]   And then the cost slowly comes down,
[03:18:58.040 --> 03:19:02.440]   and now we have less of a chunky adoption, right?
[03:19:02.440 --> 03:19:04.320]   That as the cost comes down,
[03:19:04.320 --> 03:19:07.360]   then we use more and more of them in more and more contexts.
[03:19:07.360 --> 03:19:09.120]   And that's a more continuous curve.
[03:19:10.120 --> 03:19:13.640]   So it's only if the first emulations are relatively cheap
[03:19:13.640 --> 03:19:17.600]   that you get a more sudden disruption to society.
[03:19:17.600 --> 03:19:19.960]   And that could happen if sort of the algorithm
[03:19:19.960 --> 03:19:21.840]   is the last thing you figure out how to do or something.
[03:19:21.840 --> 03:19:24.360]   - What about robots that capture some magic
[03:19:24.360 --> 03:19:28.680]   in terms of social connection?
[03:19:28.680 --> 03:19:30.920]   The robots, like we have a robot dog
[03:19:30.920 --> 03:19:32.240]   on the carpet right there,
[03:19:32.240 --> 03:19:36.160]   robots that are able to capture some magic
[03:19:36.160 --> 03:19:39.960]   of human connection as they interact with humans,
[03:19:39.960 --> 03:19:42.000]   but are not emulating the brain.
[03:19:42.000 --> 03:19:44.920]   What about those, how far away?
[03:19:44.920 --> 03:19:48.840]   - So we're thinking about chunkiness or distance now.
[03:19:48.840 --> 03:19:51.160]   So if you ask how chunky is the task
[03:19:51.160 --> 03:19:54.800]   of making a emulatable robot or something,
[03:19:54.800 --> 03:19:59.320]   - Which chunkiness and time are correlated.
[03:19:59.320 --> 03:20:02.040]   - Right, but it's about how far away it is
[03:20:02.040 --> 03:20:04.240]   or how suddenly it would happen.
[03:20:04.240 --> 03:20:06.160]   Chunkiness is how suddenly,
[03:20:06.160 --> 03:20:08.800]   and difficulty is just how far away it is.
[03:20:08.800 --> 03:20:10.720]   But it could be a continuous difficulty.
[03:20:10.720 --> 03:20:11.560]   It could just be far away,
[03:20:11.560 --> 03:20:13.080]   but we'll slowly steadily get there.
[03:20:13.080 --> 03:20:14.480]   Or there could be these thresholds
[03:20:14.480 --> 03:20:15.440]   where we reach a threshold
[03:20:15.440 --> 03:20:16.880]   and suddenly we can do a lot better.
[03:20:16.880 --> 03:20:19.280]   - Yeah, that's a good question for both.
[03:20:19.280 --> 03:20:23.160]   I tend to believe that all of it, not just the M,
[03:20:23.160 --> 03:20:25.840]   but AGI too is chunky.
[03:20:25.840 --> 03:20:30.840]   And human level intelligence embodied in robots
[03:20:30.840 --> 03:20:32.120]   is also chunky.
[03:20:32.120 --> 03:20:34.440]   The history of computer science and chunkiness so far
[03:20:34.440 --> 03:20:37.840]   seems to be my rough best guess for the chunkiness of AGI.
[03:20:37.840 --> 03:20:39.800]   That is, it is chunky.
[03:20:39.800 --> 03:20:42.440]   Modestly chunky, not that chunky.
[03:20:42.440 --> 03:20:43.280]   Right?
[03:20:43.280 --> 03:20:44.120]   (laughing)
[03:20:44.120 --> 03:20:46.920]   Our ability to use computers to do many things in the economy
[03:20:46.920 --> 03:20:48.600]   has been moving relatively steadily.
[03:20:48.600 --> 03:20:51.520]   Overall, in terms of our use of computers in society,
[03:20:51.520 --> 03:20:55.760]   they have been relatively steadily improving for 70 years.
[03:20:55.760 --> 03:20:57.440]   - No, but I would say that's hard.
[03:20:57.440 --> 03:20:58.880]   Well, yeah, okay.
[03:20:58.880 --> 03:21:00.560]   Okay, I would have to really think about that
[03:21:00.560 --> 03:21:03.880]   'cause neural networks are quite surprising.
[03:21:03.880 --> 03:21:04.960]   - Sure, but every once in a while
[03:21:04.960 --> 03:21:06.240]   we have a new thing that's surprising.
[03:21:06.240 --> 03:21:07.920]   But if you stand back,
[03:21:07.920 --> 03:21:10.360]   we see something like that every 10 years or so.
[03:21:10.360 --> 03:21:12.280]   Some new innovations-- - The progress is gradual.
[03:21:12.280 --> 03:21:13.640]   - That has a big effect.
[03:21:13.640 --> 03:21:16.840]   - So moderately chunky.
[03:21:16.840 --> 03:21:19.800]   Huh, yeah.
[03:21:19.800 --> 03:21:21.400]   - The history of the level of disruption
[03:21:21.400 --> 03:21:23.240]   we've seen in the past would be a rough estimate
[03:21:23.240 --> 03:21:24.720]   of the level of disruption in the future.
[03:21:24.720 --> 03:21:27.520]   Unless the future is we're gonna hit a chunky territory,
[03:21:27.520 --> 03:21:29.480]   much chunkier than we've seen in the past.
[03:21:29.480 --> 03:21:30.880]   - Well, I do think there's,
[03:21:30.880 --> 03:21:35.400]   it's like Kuhnian, like revolution type.
[03:21:35.400 --> 03:21:39.640]   It seems like the data, especially on AI,
[03:21:39.640 --> 03:21:44.640]   is difficult to reason with because it's so recent.
[03:21:44.640 --> 03:21:48.040]   It's such a recent field.
[03:21:48.040 --> 03:21:50.640]   - Well, I've been around for 50 years.
[03:21:50.640 --> 03:21:53.760]   - I mean, 50, 60, 70, 80 years being recent.
[03:21:53.760 --> 03:21:54.600]   - Okay.
[03:21:54.600 --> 03:21:56.240]   - That's how I'm--
[03:21:56.240 --> 03:21:58.880]   - It's enough time to see a lot of trends.
[03:21:58.880 --> 03:22:01.080]   - A few trends, a few trends.
[03:22:01.080 --> 03:22:04.840]   I think the internet, computing,
[03:22:04.840 --> 03:22:07.040]   there's really a lot of interesting stuff
[03:22:07.040 --> 03:22:09.360]   that's happened over the past 30 years
[03:22:09.360 --> 03:22:13.880]   that I think the possibility of revolutions
[03:22:13.880 --> 03:22:16.880]   is likelier than it was in the--
[03:22:16.880 --> 03:22:18.040]   - I think for the last 70 years,
[03:22:18.040 --> 03:22:19.600]   there have always been a lot of things
[03:22:19.600 --> 03:22:21.240]   that looked like they had a potential for revolution.
[03:22:21.240 --> 03:22:23.200]   - So we can't reason well about this.
[03:22:23.200 --> 03:22:24.600]   - I mean, we can reason well
[03:22:24.600 --> 03:22:26.000]   by looking at the past trends.
[03:22:26.000 --> 03:22:28.480]   I would say the past trend is roughly your best guess
[03:22:28.480 --> 03:22:29.320]   for the future.
[03:22:29.320 --> 03:22:32.520]   - No, but if I look back at the things
[03:22:32.520 --> 03:22:33.840]   that might have looked like revolutions
[03:22:33.840 --> 03:22:35.640]   in the '70s and '80s and '90s,
[03:22:35.640 --> 03:22:40.400]   they are less like the revolutions
[03:22:40.400 --> 03:22:42.240]   that appear to be happening now,
[03:22:42.240 --> 03:22:45.520]   or the capacity of revolution that appear to be there now.
[03:22:45.520 --> 03:22:48.920]   First of all, there's a lot more money to be made.
[03:22:48.920 --> 03:22:50.800]   So there's a lot more incentive for markets
[03:22:50.800 --> 03:22:52.360]   to do a lot of kind of innovation,
[03:22:52.360 --> 03:22:54.680]   it seems like, in the AI space.
[03:22:54.680 --> 03:22:57.160]   But then again, there's a history of winters
[03:22:57.160 --> 03:22:58.680]   and summers and so on.
[03:22:58.680 --> 03:23:01.080]   So maybe we're just like riding a nice wave right now.
[03:23:01.080 --> 03:23:03.240]   - One of the biggest issues is the difference
[03:23:03.240 --> 03:23:05.880]   between impressive demos and commercial value.
[03:23:05.880 --> 03:23:06.720]   - Yes.
[03:23:06.720 --> 03:23:08.320]   - So we often, through the history of AI,
[03:23:08.320 --> 03:23:10.360]   we saw very impressive demos
[03:23:10.360 --> 03:23:12.920]   that never really translated much into commercial value.
[03:23:12.920 --> 03:23:15.480]   - Somebody who works on and cares about autonomous
[03:23:15.480 --> 03:23:18.080]   and semi-autonomous vehicles, tell me about it.
[03:23:18.080 --> 03:23:21.440]   And there again, we return to the number
[03:23:21.440 --> 03:23:25.800]   of Elon Musks per Earth per year generated.
[03:23:27.040 --> 03:23:28.040]   That's the M.
[03:23:28.040 --> 03:23:30.800]   Coincidentally, same initials as the M.
[03:23:30.800 --> 03:23:31.640]   - Yeah.
[03:23:31.640 --> 03:23:33.240]   - Very suspicious, very suspicious.
[03:23:33.240 --> 03:23:35.120]   We're gonna have to look into that.
[03:23:35.120 --> 03:23:39.320]   All right, two more fields that I would like to force
[03:23:39.320 --> 03:23:40.520]   and twist your arm to.
[03:23:40.520 --> 03:23:41.360]   - All right.
[03:23:41.360 --> 03:23:44.280]   - To look for view quakes and for beautiful ideas, economics.
[03:23:44.280 --> 03:23:49.280]   What is a beautiful idea to you about economics?
[03:23:49.280 --> 03:23:53.160]   You've mentioned a lot of them already.
[03:23:53.160 --> 03:23:55.760]   - Sure, so as you said before,
[03:23:55.760 --> 03:23:58.600]   there's gonna be the first view quake most people encounter
[03:23:58.600 --> 03:24:01.040]   that makes the biggest difference on average in the world,
[03:24:01.040 --> 03:24:03.360]   'cause that's the only thing most people ever see
[03:24:03.360 --> 03:24:05.040]   is the first one.
[03:24:05.040 --> 03:24:09.360]   And so, with AI, the first one is just how big
[03:24:09.360 --> 03:24:11.760]   the problem is, but once you get past that,
[03:24:11.760 --> 03:24:12.600]   you'll find others.
[03:24:12.600 --> 03:24:16.080]   Certainly for economics, the first one is just
[03:24:16.080 --> 03:24:17.240]   the power of markets.
[03:24:17.240 --> 03:24:20.800]   You might've thought it was just really hard
[03:24:20.800 --> 03:24:24.160]   to figure out how to optimize in a big, complicated space,
[03:24:24.160 --> 03:24:27.640]   and markets just do a good first pass
[03:24:27.640 --> 03:24:29.000]   for an awful lot of stuff.
[03:24:29.000 --> 03:24:31.400]   And they are really quite robust and powerful.
[03:24:31.400 --> 03:24:35.080]   And that's just quite the view quake,
[03:24:35.080 --> 03:24:37.600]   where you just say, you know, just let a,
[03:24:37.600 --> 03:24:39.160]   if you wanna get in the ballpark,
[03:24:39.160 --> 03:24:42.120]   just let a market handle it and step back.
[03:24:42.120 --> 03:24:44.720]   And that's true for a wide range of things.
[03:24:44.720 --> 03:24:45.560]   It's not true for everything,
[03:24:45.560 --> 03:24:48.720]   but it's a very good first approximation.
[03:24:48.720 --> 03:24:50.720]   And most people's intuitions for how they should limit
[03:24:50.720 --> 03:24:53.560]   markets are actually messing them up.
[03:24:53.560 --> 03:24:55.160]   They're that good in sense, right?
[03:24:55.160 --> 03:24:57.200]   Most people, when you go, I don't know if we wanna trust
[03:24:57.200 --> 03:24:59.120]   that, well, you should be trusting that.
[03:24:59.120 --> 03:25:02.600]   - What about, what are markets?
[03:25:02.600 --> 03:25:05.520]   Like just a couple of words.
[03:25:05.520 --> 03:25:09.120]   - So the idea is if people want something,
[03:25:09.120 --> 03:25:12.520]   then let other companies form to try to supply that thing.
[03:25:12.520 --> 03:25:14.560]   Let those people pay for their cost
[03:25:14.560 --> 03:25:17.000]   of whatever they're making and try to offer that product
[03:25:17.000 --> 03:25:17.840]   to those people.
[03:25:17.840 --> 03:25:21.240]   Let many people, many such firms enter that industry
[03:25:21.240 --> 03:25:23.280]   and let the customers decide which ones they want.
[03:25:23.280 --> 03:25:25.760]   And if the firm goes out of business, let it go bankrupt
[03:25:25.760 --> 03:25:28.080]   and let other people invest in whichever ventures
[03:25:28.080 --> 03:25:29.960]   they wanna try to attract customers
[03:25:29.960 --> 03:25:31.840]   to their version of the product.
[03:25:31.840 --> 03:25:33.200]   And that just works for a wide range
[03:25:33.200 --> 03:25:34.440]   of products and services.
[03:25:34.440 --> 03:25:35.360]   - And through all of this,
[03:25:35.360 --> 03:25:37.800]   there's a free exchange of information too.
[03:25:37.800 --> 03:25:40.680]   There's a hope that there's no manipulation of information
[03:25:40.680 --> 03:25:43.400]   and so on, that there, you're making--
[03:25:43.400 --> 03:25:45.440]   - Even when those things happen,
[03:25:45.440 --> 03:25:48.120]   still just the simple market solution is usually better
[03:25:48.120 --> 03:25:49.840]   than the things you'll try to do to fix it.
[03:25:49.840 --> 03:25:51.000]   - Than the alternative.
[03:25:52.560 --> 03:25:53.760]   - That's a view, Craig.
[03:25:53.760 --> 03:25:54.600]   It's surprising.
[03:25:54.600 --> 03:25:57.120]   It's not what you would have initially thought.
[03:25:57.120 --> 03:25:59.800]   - That's one of the great, I guess, inventions
[03:25:59.800 --> 03:26:03.240]   of human civilization that trusts the markets.
[03:26:03.240 --> 03:26:06.680]   - Now, another view, Craig, that I learned in my research
[03:26:06.680 --> 03:26:09.200]   that's not all of economics but something more specialized
[03:26:09.200 --> 03:26:12.240]   is the rationality of disagreement.
[03:26:12.240 --> 03:26:14.720]   That is, basically, people who are trying to believe
[03:26:14.720 --> 03:26:17.560]   what's true in a complicated situation
[03:26:17.560 --> 03:26:18.960]   would not actually disagree.
[03:26:18.960 --> 03:26:22.280]   And of course, humans disagree all the time,
[03:26:22.280 --> 03:26:24.400]   so it was quite the striking fact for me to learn
[03:26:24.400 --> 03:26:26.560]   in grad school that actually,
[03:26:26.560 --> 03:26:28.800]   rational agents would not knowingly disagree.
[03:26:28.800 --> 03:26:32.200]   And so, that makes disagreement more puzzling
[03:26:32.200 --> 03:26:35.180]   and it makes you less willing to disagree.
[03:26:35.180 --> 03:26:42.520]   - Humans are, to some degree, rational and are able to--
[03:26:42.520 --> 03:26:44.560]   - Their priorities are different
[03:26:44.560 --> 03:26:46.260]   than just figuring out the truth.
[03:26:46.260 --> 03:26:51.280]   Which might not be the same as being irrational.
[03:26:52.280 --> 03:26:54.720]   - That's another tangent that could take an hour.
[03:26:54.720 --> 03:27:01.300]   In the space of human affairs, political science,
[03:27:01.300 --> 03:27:06.000]   what is a beautiful, foundational, interesting idea to you,
[03:27:06.000 --> 03:27:08.560]   a view, Craig, in the space of political science?
[03:27:08.560 --> 03:27:13.640]   - The main thing that goes wrong in politics
[03:27:13.640 --> 03:27:17.760]   is people not agreeing on what the best thing to do is.
[03:27:17.760 --> 03:27:20.720]   - That's a wrong thing.
[03:27:20.720 --> 03:27:22.400]   - So that's what goes wrong, that is when you say
[03:27:22.400 --> 03:27:25.340]   what's fundamentally behind most political failures,
[03:27:25.340 --> 03:27:27.400]   it's that people are ignorant
[03:27:27.400 --> 03:27:29.800]   of what the consequences of policy is.
[03:27:29.800 --> 03:27:33.280]   And that's surprising because it's actually feasible
[03:27:33.280 --> 03:27:35.800]   to solve that problem, which we aren't solving.
[03:27:35.800 --> 03:27:37.160]   - So it's a bug, not a feature,
[03:27:37.160 --> 03:27:42.160]   that there's an inability to arrive at a consensus.
[03:27:42.160 --> 03:27:44.800]   - So most political systems,
[03:27:44.800 --> 03:27:47.640]   if everybody looked to some authority, say, on a question,
[03:27:47.640 --> 03:27:49.320]   and that authority told them the answer,
[03:27:49.320 --> 03:27:51.040]   then most political systems are capable
[03:27:51.040 --> 03:27:52.280]   of just doing that thing.
[03:27:52.280 --> 03:27:57.840]   That is, and so it's the failure
[03:27:57.840 --> 03:28:00.120]   to have trustworthy authorities
[03:28:00.120 --> 03:28:02.720]   that is sort of the underlying failure
[03:28:02.720 --> 03:28:04.480]   behind most political failure.
[03:28:04.480 --> 03:28:07.120]   We failed, we have bad, we invade Iraq, say,
[03:28:07.120 --> 03:28:09.080]   when we don't have an authority to tell us
[03:28:09.080 --> 03:28:10.840]   that's a really stupid thing to do.
[03:28:10.840 --> 03:28:14.840]   And it is possible to create
[03:28:14.840 --> 03:28:17.760]   more informative, trustworthy authorities,
[03:28:17.760 --> 03:28:19.280]   that that's a remarkable fact
[03:28:19.280 --> 03:28:22.080]   about the world of institutions,
[03:28:22.080 --> 03:28:24.660]   that we could do that, but we aren't.
[03:28:24.660 --> 03:28:26.640]   - Yeah, that's surprising.
[03:28:26.640 --> 03:28:28.160]   We could and we aren't.
[03:28:28.160 --> 03:28:30.120]   - Right, another big view crick about politics
[03:28:30.120 --> 03:28:31.320]   is from the elephant in the brain,
[03:28:31.320 --> 03:28:33.360]   that most people, when they're interacting with politics,
[03:28:33.360 --> 03:28:35.920]   they say they want to make the world better,
[03:28:35.920 --> 03:28:37.640]   they make their city better, their country better,
[03:28:37.640 --> 03:28:39.360]   and that's not their priority.
[03:28:39.360 --> 03:28:40.200]   - What is it?
[03:28:40.200 --> 03:28:42.760]   - They want to show loyalty to their allies.
[03:28:42.760 --> 03:28:45.400]   They wanna show their people they're on their side, yes.
[03:28:45.400 --> 03:28:47.000]   Are there various tribes they're in?
[03:28:47.000 --> 03:28:51.480]   That's their primary priority, and they do accomplish that.
[03:28:51.480 --> 03:28:54.240]   - Yeah, and the tribes are usually color-coded,
[03:28:54.240 --> 03:28:55.540]   conveniently enough.
[03:28:55.540 --> 03:29:01.480]   What would you say, you know, it's the Churchill question,
[03:29:01.480 --> 03:29:04.360]   democracy's the crappiest form of government,
[03:29:04.360 --> 03:29:05.800]   but it's the best one we got.
[03:29:05.800 --> 03:29:08.280]   What's the best form of government
[03:29:08.280 --> 03:29:12.740]   for this, our, seven billion human civilization,
[03:29:12.740 --> 03:29:16.160]   and the, maybe, as we get farther and farther,
[03:29:16.160 --> 03:29:18.160]   you mentioned a lot of stuff that's fascinating
[03:29:18.160 --> 03:29:21.760]   about human history as we become more forager-like,
[03:29:21.760 --> 03:29:25.160]   and looking out beyond, what's the best form of government
[03:29:25.160 --> 03:29:26.680]   in the next 50, 100 years
[03:29:26.680 --> 03:29:28.520]   as we become a multi-planetary species?
[03:29:28.520 --> 03:29:32.520]   - So, the key failing is that
[03:29:32.520 --> 03:29:35.040]   we have existing political institutions
[03:29:35.040 --> 03:29:37.680]   and related institutions, like media institutions
[03:29:37.680 --> 03:29:39.300]   and other authority institutions,
[03:29:39.300 --> 03:29:42.600]   and these institutions sit in a vast space
[03:29:42.600 --> 03:29:44.240]   of possible institutions.
[03:29:44.240 --> 03:29:47.600]   And the key failing, we're just not exploring that space.
[03:29:47.600 --> 03:29:50.560]   So, I have made my proposals in that space,
[03:29:50.560 --> 03:29:53.600]   and I think I can identify many promising solutions,
[03:29:53.600 --> 03:29:54.720]   and many other people have made
[03:29:54.720 --> 03:29:57.080]   many other promising proposals in that space,
[03:29:57.080 --> 03:29:58.960]   but the key thing is we're just not pursuing
[03:29:58.960 --> 03:30:01.360]   those proposals, we're not trying them out on small scales,
[03:30:01.360 --> 03:30:04.240]   we're not doing tests, we're not exploring
[03:30:04.240 --> 03:30:05.760]   the space of these options.
[03:30:05.760 --> 03:30:08.040]   That is the key thing we're failing to do.
[03:30:08.040 --> 03:30:11.520]   And if we did that, I am confident we would find
[03:30:11.520 --> 03:30:13.920]   much better institutions than the one we're using now,
[03:30:13.920 --> 03:30:15.940]   but we would have to actually try.
[03:30:15.940 --> 03:30:17.400]   (Lex laughing)
[03:30:17.400 --> 03:30:19.960]   - So, a lot of those topics,
[03:30:19.960 --> 03:30:21.860]   I do hope we get a chance to talk again.
[03:30:21.860 --> 03:30:23.640]   You're a fascinating human being,
[03:30:23.640 --> 03:30:26.480]   so I'm skipping a lot of tangents on purpose
[03:30:26.480 --> 03:30:28.040]   that I would love to take.
[03:30:28.040 --> 03:30:29.280]   You're such a brilliant person
[03:30:29.280 --> 03:30:31.160]   on so many different topics.
[03:30:31.160 --> 03:30:35.240]   Let me take a stroll
[03:30:35.240 --> 03:30:39.820]   into the deep human psyche
[03:30:39.820 --> 03:30:42.960]   of Robin Hanson himself.
[03:30:42.960 --> 03:30:44.640]   So first-- - May not be that deep.
[03:30:44.640 --> 03:30:47.300]   (both laughing)
[03:30:47.300 --> 03:30:49.880]   I might just be all on the surface.
[03:30:49.880 --> 03:30:50.840]   What you see, what you get,
[03:30:50.840 --> 03:30:52.440]   there might not be much hiding behind it.
[03:30:52.440 --> 03:30:55.000]   - Some of the fun is on the surface.
[03:30:55.000 --> 03:30:57.880]   - I actually think this is true
[03:30:57.880 --> 03:30:59.800]   of many of the most successful,
[03:30:59.800 --> 03:31:02.080]   most interesting people you see in the world.
[03:31:02.080 --> 03:31:04.740]   That is, they have put so much effort
[03:31:04.740 --> 03:31:07.640]   into the surface that they've constructed,
[03:31:07.640 --> 03:31:09.600]   and that's where they put all their energy.
[03:31:09.600 --> 03:31:12.360]   So somebody might be a statesman
[03:31:12.360 --> 03:31:13.640]   or an actor or something else,
[03:31:13.640 --> 03:31:14.600]   and people wanna interview them,
[03:31:14.600 --> 03:31:16.760]   and they wanna say, "What are you behind the scenes?
[03:31:16.760 --> 03:31:17.960]   "What do you do in your free time?"
[03:31:17.960 --> 03:31:18.800]   You know what?
[03:31:18.800 --> 03:31:19.640]   Those people don't have free time.
[03:31:19.640 --> 03:31:21.920]   They don't have another life behind the scenes.
[03:31:21.920 --> 03:31:24.700]   They put all their energy into that surface,
[03:31:24.700 --> 03:31:27.340]   the one we admire, the one we're fascinated by,
[03:31:27.340 --> 03:31:29.740]   and they kinda have to make up the stuff behind the scenes
[03:31:29.740 --> 03:31:32.920]   to supply it for you, but it's not really there.
[03:31:32.920 --> 03:31:34.480]   - Well, there's several ways of phrasing this.
[03:31:34.480 --> 03:31:35.920]   So one of it is authenticity,
[03:31:35.920 --> 03:31:40.920]   which is if you become the thing you are on the surface,
[03:31:40.920 --> 03:31:45.440]   if the depths mirror the surface,
[03:31:45.440 --> 03:31:47.480]   then that's what authenticity is.
[03:31:47.480 --> 03:31:48.640]   You're not hiding something.
[03:31:48.640 --> 03:31:49.920]   You're not concealing something.
[03:31:49.920 --> 03:31:51.920]   To push back on the idea of actors,
[03:31:51.920 --> 03:31:55.840]   they actually have often a manufactured surface
[03:31:55.840 --> 03:31:58.880]   that they put on, and they try on different masks,
[03:31:58.880 --> 03:32:01.800]   and the depths are very different from the surface,
[03:32:01.800 --> 03:32:02.880]   and that's actually what makes them
[03:32:02.880 --> 03:32:04.800]   very not interesting to interview.
[03:32:04.800 --> 03:32:09.800]   If you're an actor who actually lives the role
[03:32:09.800 --> 03:32:13.440]   that you play, so like, I don't know,
[03:32:13.440 --> 03:32:14.720]   a Clint Eastwood-type character
[03:32:14.720 --> 03:32:17.760]   who clearly represents the cowboy,
[03:32:17.760 --> 03:32:21.480]   I mean, at least rhymes or echoes
[03:32:21.480 --> 03:32:24.560]   the person you play on the surface, that's authenticity.
[03:32:24.560 --> 03:32:25.880]   - Some people are typecasts,
[03:32:25.880 --> 03:32:27.580]   and they have basically one persona.
[03:32:27.580 --> 03:32:29.680]   They play in all of their movies and TV shows,
[03:32:29.680 --> 03:32:31.460]   and so those people, it probably is
[03:32:31.460 --> 03:32:34.200]   the actual persona that they are,
[03:32:34.200 --> 03:32:36.640]   or it has become that over time.
[03:32:36.640 --> 03:32:37.700]   Clint Eastwood would be one.
[03:32:37.700 --> 03:32:39.120]   I think of Tom Hanks as another.
[03:32:39.120 --> 03:32:41.000]   I think they just always play the same person.
[03:32:41.000 --> 03:32:44.600]   - And you and I are just both surface players.
[03:32:44.600 --> 03:32:47.960]   You're the fun, brilliant thinker,
[03:32:47.960 --> 03:32:52.960]   and I am the suit-wearing idiot full of silly questions.
[03:32:52.960 --> 03:32:59.800]   All right, that said, let's put on your wise sage hat
[03:33:02.520 --> 03:33:04.360]   and ask you what advice would you give
[03:33:04.360 --> 03:33:08.080]   to young people today in high school and college
[03:33:08.080 --> 03:33:12.880]   about life, about how to live a successful life
[03:33:12.880 --> 03:33:16.380]   in career or just in general that they can be proud of?
[03:33:16.380 --> 03:33:20.640]   - Most young people, when they actually ask you
[03:33:20.640 --> 03:33:22.560]   that question, what they usually mean
[03:33:22.560 --> 03:33:26.200]   is how can I be successful by usual standards.
[03:33:26.200 --> 03:33:27.020]   - Yeah.
[03:33:27.020 --> 03:33:28.500]   - I'm not very good at giving advice about that
[03:33:28.500 --> 03:33:30.840]   'cause that's not how I tried to live my life.
[03:33:31.680 --> 03:33:35.960]   So I would more flip it around and say,
[03:33:35.960 --> 03:33:40.080]   you live in a rich society, you will have a long life,
[03:33:40.080 --> 03:33:42.380]   you have many resources available to you.
[03:33:42.380 --> 03:33:46.580]   Whatever career you take, you'll have plenty of time
[03:33:46.580 --> 03:33:49.440]   to make progress on something else.
[03:33:49.440 --> 03:33:51.360]   Yes, it might be better if you find a way
[03:33:51.360 --> 03:33:53.760]   to combine your career and your interests
[03:33:53.760 --> 03:33:55.320]   in a way that gives you more time and energy,
[03:33:55.320 --> 03:33:58.560]   but there are often big compromises there as well.
[03:33:58.560 --> 03:34:01.040]   So if you have a passion about some topic
[03:34:01.040 --> 03:34:03.920]   or some thing that you think just is worth pursuing,
[03:34:03.920 --> 03:34:06.880]   you can just do it, you don't need other people's approval.
[03:34:06.880 --> 03:34:10.640]   And you can just start doing whatever it is
[03:34:10.640 --> 03:34:12.520]   you think is worth doing.
[03:34:12.520 --> 03:34:14.800]   It might take you decades, but decades are enough
[03:34:14.800 --> 03:34:18.520]   to make enormous progress on most all interesting things.
[03:34:18.520 --> 03:34:20.520]   - And don't worry about the commitment of it.
[03:34:20.520 --> 03:34:23.000]   I mean, that's a lot of what people worry about is,
[03:34:23.000 --> 03:34:25.540]   well, there's so many options, and if I choose a thing
[03:34:25.540 --> 03:34:28.280]   and I stick with it, I sacrifice all the other
[03:34:28.280 --> 03:34:29.360]   paths I could have taken.
[03:34:29.360 --> 03:34:32.320]   - But I mean, so I switched my career at the age of 34
[03:34:32.320 --> 03:34:35.040]   with two kids age zero and two, went back to grad school
[03:34:35.040 --> 03:34:37.720]   in social science after being a software,
[03:34:37.720 --> 03:34:40.640]   research software engineer.
[03:34:40.640 --> 03:34:43.860]   So it's quite possible to change your mind later in life.
[03:34:43.860 --> 03:34:46.980]   - How can you have an age of zero?
[03:34:46.980 --> 03:34:49.320]   - Shot, less than one.
[03:34:49.320 --> 03:34:55.080]   - Okay, so, oh, oh, you indexed with zero, I got it, okay.
[03:34:55.080 --> 03:34:57.720]   - Right, and you know, like people also ask what to read,
[03:34:57.720 --> 03:34:59.480]   and I say textbooks.
[03:34:59.480 --> 03:35:02.160]   And until you've read lots of textbooks,
[03:35:02.160 --> 03:35:04.840]   or maybe review articles, I'm not so sure you should
[03:35:04.840 --> 03:35:09.840]   be reading blog posts and Twitter feeds and even podcasts.
[03:35:09.840 --> 03:35:13.800]   I would say at the beginning, read the,
[03:35:13.800 --> 03:35:16.000]   this is our best, humanity's best summary
[03:35:16.000 --> 03:35:18.280]   of how to learn things is crammed into textbooks.
[03:35:18.280 --> 03:35:22.360]   - Yeah, especially the ones on introduction to biology.
[03:35:22.360 --> 03:35:23.640]   - Yeah, everything, introduction to everything.
[03:35:23.640 --> 03:35:25.420]   Just read all the textbooks. - Algorithms.
[03:35:25.420 --> 03:35:27.640]   - Read as many textbooks as you can stomach
[03:35:27.640 --> 03:35:29.600]   and then maybe if you wanna know more about a subject,
[03:35:29.600 --> 03:35:31.240]   find review articles.
[03:35:31.240 --> 03:35:33.600]   You don't need to read the latest stuff for most topics.
[03:35:33.600 --> 03:35:35.560]   - Yeah, and actually textbooks often have
[03:35:35.560 --> 03:35:37.760]   the prettiest pictures. - There you go.
[03:35:37.760 --> 03:35:40.120]   - And depending on the field, if it's technical,
[03:35:40.120 --> 03:35:42.600]   then doing the homework problems at the end
[03:35:42.600 --> 03:35:44.880]   is actually extremely, extremely useful.
[03:35:44.880 --> 03:35:47.440]   Extremely powerful way to understand something
[03:35:47.440 --> 03:35:48.880]   if you allow it.
[03:35:48.880 --> 03:35:52.520]   You know, I actually think of like high school and college,
[03:35:52.520 --> 03:35:54.720]   which you kind of remind me of.
[03:35:54.720 --> 03:35:56.280]   People don't often think of it that way,
[03:35:56.280 --> 03:36:01.280]   but you will almost not again get an opportunity
[03:36:01.280 --> 03:36:04.360]   to spend the time with a fundamental subject.
[03:36:04.360 --> 03:36:05.720]   - Bring up lots of stuff.
[03:36:05.720 --> 03:36:06.760]   - And like, no. - All the basics.
[03:36:06.760 --> 03:36:08.120]   - And everybody's forcing you,
[03:36:08.120 --> 03:36:10.400]   like everybody wants you to do it.
[03:36:10.400 --> 03:36:14.820]   And like you'll never get that chance again to sit there,
[03:36:14.820 --> 03:36:16.920]   even though it's outside of your interest, biology.
[03:36:16.920 --> 03:36:20.840]   Like in high school I took AP biology, AP chemistry.
[03:36:20.840 --> 03:36:24.840]   I'm thinking of subjects I never again
[03:36:24.840 --> 03:36:26.480]   really visited seriously.
[03:36:26.480 --> 03:36:31.480]   And it was so nice to be forced into anatomy and physiology,
[03:36:31.480 --> 03:36:35.200]   to be forced into that world, to stay with it,
[03:36:35.200 --> 03:36:36.840]   to look at the pretty pictures,
[03:36:36.840 --> 03:36:39.760]   to certain moments to actually for a moment
[03:36:39.760 --> 03:36:43.520]   enjoy the beauty of these, of like how a cell works
[03:36:43.520 --> 03:36:44.640]   and all those kinds of things.
[03:36:44.640 --> 03:36:48.740]   And somehow that stays, like the ripples of that fascination
[03:36:48.740 --> 03:36:51.120]   that stays with you even if you never do those,
[03:36:51.120 --> 03:36:56.040]   even if you never utilize those learnings
[03:36:56.040 --> 03:36:56.960]   in your actual work.
[03:36:56.960 --> 03:36:59.880]   - A common problem, at least of many young people I meet,
[03:36:59.880 --> 03:37:03.320]   is that they're like feeling idealistic and altruistic,
[03:37:03.320 --> 03:37:05.040]   but in a rush.
[03:37:05.040 --> 03:37:05.880]   - Yes.
[03:37:05.880 --> 03:37:09.640]   - So the usual human tradition that goes back
[03:37:09.640 --> 03:37:10.840]   hundreds of thousands of years
[03:37:10.840 --> 03:37:13.640]   is that people's productivity rises with time
[03:37:13.640 --> 03:37:16.440]   and maybe peaks around the age of 40 or 50.
[03:37:16.440 --> 03:37:18.120]   The age of 40, 50 is when you will be
[03:37:18.120 --> 03:37:21.080]   having the highest income, you'll have the most contacts,
[03:37:21.080 --> 03:37:23.640]   you will sort of be wise about how the world works.
[03:37:23.640 --> 03:37:27.720]   Expect to have your biggest impact then.
[03:37:27.720 --> 03:37:30.200]   Before then, you can have impacts,
[03:37:30.200 --> 03:37:32.060]   but you're also mainly building up
[03:37:32.060 --> 03:37:33.580]   your resources and abilities.
[03:37:33.580 --> 03:37:37.640]   That's the usual human trajectory,
[03:37:37.640 --> 03:37:39.640]   expect that to be true of you too.
[03:37:39.640 --> 03:37:41.640]   Don't be in such a rush to like accomplish
[03:37:41.640 --> 03:37:43.920]   enormous things at the age of 18 or whatever.
[03:37:43.920 --> 03:37:46.080]   I mean, you might as well practice trying to do things,
[03:37:46.080 --> 03:37:48.600]   but that's mostly about learning how to do things
[03:37:48.600 --> 03:37:49.440]   by practicing.
[03:37:49.440 --> 03:37:50.380]   There's a lot of things you can't do
[03:37:50.380 --> 03:37:51.980]   unless you just keep trying them.
[03:37:51.980 --> 03:37:54.720]   - And when all else fails,
[03:37:54.720 --> 03:37:56.600]   try to maximize the number of offspring
[03:37:56.600 --> 03:37:58.520]   however way you can.
[03:37:58.520 --> 03:38:00.280]   - That's certainly something I've neglected.
[03:38:00.280 --> 03:38:03.360]   I would tell my younger version of myself,
[03:38:03.360 --> 03:38:04.820]   try to have more descendants.
[03:38:04.820 --> 03:38:07.520]   Yes, absolutely.
[03:38:07.520 --> 03:38:10.420]   It matters more than I realized at the time.
[03:38:10.420 --> 03:38:15.960]   - Both in terms of making copies of yourself
[03:38:15.960 --> 03:38:20.960]   in mutated form and just the joy of raising them?
[03:38:20.960 --> 03:38:22.040]   - Sure.
[03:38:22.040 --> 03:38:23.960]   I mean, the meaning even.
[03:38:23.960 --> 03:38:29.680]   So in the literature on the value people get out of life,
[03:38:29.680 --> 03:38:32.480]   there's a key distinction between happiness and meaning.
[03:38:32.480 --> 03:38:36.440]   So happiness is how do you feel right now about right now,
[03:38:36.440 --> 03:38:39.040]   and meaning is how do you feel about your whole life?
[03:38:39.040 --> 03:38:43.340]   And many things that produce happiness
[03:38:43.340 --> 03:38:44.880]   don't produce meaning as reliably,
[03:38:44.880 --> 03:38:46.000]   and if you have to choose between them,
[03:38:46.000 --> 03:38:48.080]   you'd rather have meaning.
[03:38:48.080 --> 03:38:53.080]   And meaning goes along with sacrificing happiness sometimes.
[03:38:53.080 --> 03:38:56.480]   And children are an example of that.
[03:38:56.480 --> 03:38:58.880]   You get a lot more meaning out of children,
[03:38:58.880 --> 03:39:01.100]   even if they're a lot more work.
[03:39:01.100 --> 03:39:07.520]   - Why do you think kids, children are so magical,
[03:39:07.520 --> 03:39:08.960]   like raising kids?
[03:39:08.960 --> 03:39:12.120]   'Cause I would love to have kids,
[03:39:12.120 --> 03:39:16.080]   and whenever I work with robots,
[03:39:16.080 --> 03:39:17.340]   there's some of the same magic
[03:39:17.340 --> 03:39:19.640]   when there's an entity that comes to life.
[03:39:19.640 --> 03:39:23.860]   And in that case, I'm not trying to draw too many parallels,
[03:39:23.860 --> 03:39:27.360]   but there is some echo to it,
[03:39:27.360 --> 03:39:29.620]   which is when you program a robot,
[03:39:29.620 --> 03:39:32.160]   there's some aspect of your intellect
[03:39:32.160 --> 03:39:35.400]   that is now instilled in this other moving being
[03:39:35.400 --> 03:39:37.320]   that's kind of magical.
[03:39:37.320 --> 03:39:39.280]   Well, why do you think that's magical?
[03:39:39.280 --> 03:39:41.100]   And you said happiness and meaning.
[03:39:41.940 --> 03:39:44.820]   - Meaningful. - As opposed to a short,
[03:39:44.820 --> 03:39:46.320]   why is it meaningful?
[03:39:46.320 --> 03:39:49.380]   - Overdetermined, like I can give you
[03:39:49.380 --> 03:39:51.860]   several different reasons, all of which is sufficient.
[03:39:51.860 --> 03:39:52.900]   And so the question is,
[03:39:52.900 --> 03:39:54.820]   we don't know which ones are the correct reasons.
[03:39:54.820 --> 03:39:58.780]   - Such a technical, it's overdetermined, look it up.
[03:39:58.780 --> 03:40:01.460]   - So I meet a lot of people interested in the future,
[03:40:01.460 --> 03:40:02.940]   interested in thinking about the future.
[03:40:02.940 --> 03:40:05.220]   They're thinking about how can I influence the future?
[03:40:05.220 --> 03:40:08.540]   But overwhelmingly in history so far,
[03:40:08.540 --> 03:40:10.380]   the main way people have influenced the future
[03:40:10.380 --> 03:40:13.740]   is by having children, overwhelmingly.
[03:40:13.740 --> 03:40:16.820]   And that's just not an incidental fact.
[03:40:16.820 --> 03:40:18.820]   You are built for that.
[03:40:18.820 --> 03:40:22.580]   That is, you're the sequence of thousands of generations,
[03:40:22.580 --> 03:40:25.340]   each of which successfully had a descendant.
[03:40:25.340 --> 03:40:27.500]   And that affected who you are.
[03:40:27.500 --> 03:40:30.980]   You just have to expect, and it's true that who you are
[03:40:30.980 --> 03:40:34.900]   is built to be, expect to have a child,
[03:40:34.900 --> 03:40:37.220]   to want to have a child,
[03:40:37.220 --> 03:40:40.260]   to have that be a natural and meaningful interaction for you.
[03:40:40.260 --> 03:40:41.780]   And it's just true.
[03:40:41.780 --> 03:40:44.020]   It's just one of those things you just should have expected,
[03:40:44.020 --> 03:40:46.340]   and it's not a surprise.
[03:40:46.340 --> 03:40:49.140]   - Well, to push back and sort of,
[03:40:49.140 --> 03:40:51.980]   in terms of influencing the future,
[03:40:51.980 --> 03:40:54.180]   as we get more and more technology,
[03:40:54.180 --> 03:40:56.780]   more and more of us are able to influence the future
[03:40:56.780 --> 03:40:58.740]   in all kinds of other ways, right?
[03:40:58.740 --> 03:40:59.980]   Being a teacher, educator.
[03:40:59.980 --> 03:41:02.660]   - Even so, though, still most of our influence
[03:41:02.660 --> 03:41:05.300]   in the future has probably happened being kids,
[03:41:05.300 --> 03:41:08.780]   even though we've accumulated more other ways to do it.
[03:41:08.780 --> 03:41:09.900]   - You mean at scale.
[03:41:09.900 --> 03:41:11.940]   I guess the depth of influence,
[03:41:11.940 --> 03:41:14.180]   like really how much effort,
[03:41:14.180 --> 03:41:16.500]   how much of yourself you really put into another human being.
[03:41:16.500 --> 03:41:20.340]   Do you mean both the raising of a kid,
[03:41:20.340 --> 03:41:23.360]   or do you mean raw genetic information?
[03:41:23.360 --> 03:41:25.260]   - Well, both, but raw genetics
[03:41:25.260 --> 03:41:27.140]   is probably more than half of it.
[03:41:27.140 --> 03:41:28.280]   - More than half.
[03:41:28.280 --> 03:41:31.220]   More than half, even in this modern world?
[03:41:31.220 --> 03:41:32.860]   - Yeah.
[03:41:32.860 --> 03:41:33.960]   - Genetics.
[03:41:33.960 --> 03:41:39.520]   Let me ask some dark, difficult questions, if I might.
[03:41:39.520 --> 03:41:43.300]   Let's take a stroll into that place
[03:41:43.300 --> 03:41:46.900]   that may or may not exist, according to you.
[03:41:46.900 --> 03:41:48.940]   What's the darkest place you've ever gone to
[03:41:48.940 --> 03:41:50.940]   in your mind, in your life?
[03:41:50.940 --> 03:41:54.060]   A dark time, a challenging time in your life
[03:41:54.060 --> 03:41:55.320]   that you had to overcome?
[03:41:55.320 --> 03:42:02.620]   - Probably just feeling strongly rejected.
[03:42:02.620 --> 03:42:07.260]   And so I'm apparently somewhat emotionally scarred
[03:42:07.260 --> 03:42:09.700]   by just being very rejection-averse,
[03:42:09.700 --> 03:42:12.260]   which must have happened because some rejections
[03:42:12.260 --> 03:42:14.460]   were just very scarring.
[03:42:14.460 --> 03:42:17.820]   - At a scale, in what kinds of communities?
[03:42:17.820 --> 03:42:19.860]   On the individual scale?
[03:42:19.860 --> 03:42:22.380]   - I mean, lots of different scales, yeah.
[03:42:22.380 --> 03:42:24.540]   All the different, many different scales.
[03:42:24.540 --> 03:42:26.340]   Still, that rejection stings.
[03:42:26.340 --> 03:42:32.260]   - Hold on a second, but you're a contrarian thinker.
[03:42:32.260 --> 03:42:34.700]   You challenge the norms.
[03:42:34.700 --> 03:42:39.700]   Why, if you were scarred by rejection,
[03:42:39.700 --> 03:42:42.820]   why welcome it in so many ways
[03:42:42.820 --> 03:42:45.460]   at a much larger scale, constantly with your ideas?
[03:42:45.460 --> 03:42:47.620]   - It could be that I'm just stupid.
[03:42:47.620 --> 03:42:51.780]   Or that I've just categorized them differently
[03:42:51.780 --> 03:42:53.140]   than I should or something.
[03:42:53.140 --> 03:42:58.300]   Most rejection that I've faced hasn't been
[03:42:58.300 --> 03:42:59.940]   because of my intellectual ideas.
[03:42:59.940 --> 03:43:03.100]   So.
[03:43:03.100 --> 03:43:04.220]   - Oh, so that once--
[03:43:04.220 --> 03:43:06.140]   - The intellectual ideas haven't been the thing
[03:43:06.140 --> 03:43:07.500]   to risk the rejection.
[03:43:07.500 --> 03:43:12.500]   - The one that, the things that challenge your mind,
[03:43:12.500 --> 03:43:15.900]   taking you to a dark place,
[03:43:15.900 --> 03:43:18.100]   the more psychological rejections.
[03:43:18.100 --> 03:43:21.780]   - You just asked me what took me to a dark place.
[03:43:21.780 --> 03:43:23.500]   You didn't specify it as sort of
[03:43:23.500 --> 03:43:25.020]   an intellectual dark place, I guess.
[03:43:25.020 --> 03:43:27.260]   Yeah, I just meant like what--
[03:43:27.260 --> 03:43:30.980]   - So intellectual is disjoint, or at least
[03:43:30.980 --> 03:43:35.300]   at a more surface level than something emotional.
[03:43:35.300 --> 03:43:37.620]   - Yeah, I would just think there are times
[03:43:37.620 --> 03:43:40.020]   in your life when you're just in a dark place
[03:43:40.020 --> 03:43:42.060]   and that can have many different causes.
[03:43:42.060 --> 03:43:45.620]   Most intellectuals are still just people
[03:43:45.620 --> 03:43:47.460]   and most of the things that will affect them
[03:43:47.460 --> 03:43:49.260]   are the kinds of things that affect people.
[03:43:49.260 --> 03:43:51.300]   They aren't that different necessarily.
[03:43:51.300 --> 03:43:52.380]   I mean, that's gonna be true for,
[03:43:52.380 --> 03:43:54.100]   like I presume most basketball players
[03:43:54.100 --> 03:43:55.020]   are still just people.
[03:43:55.020 --> 03:43:56.620]   If you ask them what was the worst part of their life,
[03:43:56.620 --> 03:43:58.420]   it's gonna be this kind of thing
[03:43:58.420 --> 03:44:00.260]   that was the worst part of life for most people.
[03:44:00.260 --> 03:44:02.340]   - So rejection early in life?
[03:44:02.340 --> 03:44:05.820]   - Yeah, I mean, not in grade school probably,
[03:44:05.820 --> 03:44:08.340]   but yeah, sort of being a young nerdy guy
[03:44:08.340 --> 03:44:13.340]   and feeling not in much demand or interest
[03:44:13.340 --> 03:44:18.860]   or later on lots of different kinds of rejection.
[03:44:18.860 --> 03:44:21.460]   But yeah, but I think that's,
[03:44:21.460 --> 03:44:25.180]   most of us like to pretend we don't that much
[03:44:25.180 --> 03:44:27.420]   need other people, we don't care what they think.
[03:44:27.420 --> 03:44:29.660]   It's a common sort of stance if somebody rejects you
[03:44:29.660 --> 03:44:32.300]   and says, "Oh, I didn't care about them anyway."
[03:44:32.300 --> 03:44:35.100]   But I think to be honest, people really do care.
[03:44:35.100 --> 03:44:37.900]   - Yeah, we do seek that connection, that love.
[03:44:37.900 --> 03:44:39.540]   What do you think is the role of love
[03:44:39.540 --> 03:44:40.880]   in the human condition?
[03:44:40.880 --> 03:44:45.980]   - Opacity in part.
[03:44:45.980 --> 03:44:51.300]   That is, love is one of those things
[03:44:51.300 --> 03:44:54.220]   where we know at some level it's important to us,
[03:44:54.220 --> 03:44:56.940]   but it's not very clearly shown to us
[03:44:56.940 --> 03:44:59.580]   exactly how or why or in what ways.
[03:44:59.580 --> 03:45:01.740]   There are some kinds of things we want
[03:45:01.740 --> 03:45:03.380]   where we can just clearly see that we want it
[03:45:03.380 --> 03:45:04.300]   and why that we want it, right?
[03:45:04.300 --> 03:45:06.700]   We know when we're thirsty and we know why we were thirsty
[03:45:06.700 --> 03:45:08.500]   and we know what to do about being thirsty
[03:45:08.500 --> 03:45:11.340]   and we know when it's over that we're no longer thirsty.
[03:45:11.340 --> 03:45:13.800]   Love isn't like that.
[03:45:13.800 --> 03:45:16.620]   - It's like, what do we seek from this?
[03:45:16.620 --> 03:45:18.820]   We're drawn to it, but we do not understand
[03:45:18.820 --> 03:45:23.660]   why we're drawn exactly, because it's not just affection,
[03:45:23.660 --> 03:45:25.180]   because if it was just affection,
[03:45:25.180 --> 03:45:28.020]   we don't seem to be drawn to pure affection.
[03:45:28.020 --> 03:45:32.980]   We don't seem to be drawn to somebody who's like a servant.
[03:45:32.980 --> 03:45:35.200]   We don't seem to be necessarily drawn to somebody
[03:45:35.200 --> 03:45:38.260]   that satisfies all your needs or something like that.
[03:45:38.260 --> 03:45:41.540]   - So it's clearly something we want or need,
[03:45:41.540 --> 03:45:43.620]   but we're not exactly very clear about it,
[03:45:43.620 --> 03:45:45.620]   and that isn't kind of important to it.
[03:45:45.620 --> 03:45:48.220]   So I've also noticed there are some kinds of things
[03:45:48.220 --> 03:45:49.820]   you can't imagine very well.
[03:45:49.820 --> 03:45:51.820]   So if you imagine a situation,
[03:45:51.820 --> 03:45:53.180]   there's some aspects of the situation
[03:45:53.180 --> 03:45:55.060]   that you can clearly, you can imagine it being bright
[03:45:55.060 --> 03:45:57.100]   or dim, you can imagine it being windy,
[03:45:57.100 --> 03:45:59.860]   or you can imagine it being hot or cold,
[03:45:59.860 --> 03:46:02.380]   but there's some aspects about your emotional stance
[03:46:02.380 --> 03:46:05.560]   in a situation that's actually just hard to imagine
[03:46:05.560 --> 03:46:06.400]   or even remember.
[03:46:06.400 --> 03:46:08.820]   It's hard to like, you can often remember an emotion
[03:46:08.820 --> 03:46:11.540]   only when you're in a similar sort of emotion situation,
[03:46:11.540 --> 03:46:14.620]   and otherwise you just can't bring the emotion
[03:46:14.620 --> 03:46:17.700]   to your mind, and you can't even imagine it, right?
[03:46:17.700 --> 03:46:20.340]   So there's certain kinds of emotions you can have,
[03:46:20.340 --> 03:46:21.380]   and when you're in that emotion,
[03:46:21.380 --> 03:46:22.460]   you can know that you have it,
[03:46:22.460 --> 03:46:23.980]   and you can have a name and it's associated,
[03:46:23.980 --> 03:46:26.180]   but later on I tell you, you know,
[03:46:26.180 --> 03:46:29.020]   remember joy, and it doesn't come to mind.
[03:46:29.020 --> 03:46:30.740]   - Not able to replay it.
[03:46:30.740 --> 03:46:33.180]   - Right, and it's sort of a reason why we have,
[03:46:33.180 --> 03:46:35.540]   one of the reasons that pushes us to reconsume it
[03:46:35.540 --> 03:46:39.700]   and reproduce it is that we can't reimagine it.
[03:46:39.700 --> 03:46:41.340]   - Well, there's a, it's interesting,
[03:46:41.340 --> 03:46:44.540]   'cause there's a Daniel Kahneman type of thing
[03:46:44.540 --> 03:46:45.780]   of like reliving memories,
[03:46:45.780 --> 03:46:49.660]   'cause I'm able to summon some aspect of that emotion,
[03:46:49.660 --> 03:46:51.820]   again, by thinking of that situation
[03:46:51.820 --> 03:46:53.900]   from which that emotion came.
[03:46:53.900 --> 03:46:54.740]   - Right.
[03:46:54.740 --> 03:46:58.140]   - So like a certain song, you can listen to it,
[03:46:58.140 --> 03:46:59.820]   and you can feel the same way you felt
[03:46:59.820 --> 03:47:02.140]   the first time you remembered that song associated
[03:47:02.140 --> 03:47:02.980]   with a certain-- - Right, but you need
[03:47:02.980 --> 03:47:05.740]   to remember that situation in some sort of complete package.
[03:47:05.740 --> 03:47:06.780]   - Yes, and then-- - You can't just take
[03:47:06.780 --> 03:47:08.540]   one part off of it, and then if you get
[03:47:08.540 --> 03:47:11.100]   the whole package again, if you remember the whole feeling.
[03:47:11.100 --> 03:47:14.540]   - Yes, or some fundamental aspect of that whole experience
[03:47:14.540 --> 03:47:17.180]   that aroused, from which the feeling arose,
[03:47:17.180 --> 03:47:20.500]   and actually the feeling is probably different in some way.
[03:47:20.500 --> 03:47:22.220]   It could be more pleasant or less pleasant
[03:47:22.220 --> 03:47:24.060]   than the feeling you felt originally,
[03:47:24.060 --> 03:47:25.500]   and that morphs over time,
[03:47:25.500 --> 03:47:27.340]   every time you replay that memory.
[03:47:27.340 --> 03:47:28.660]   It is interesting, you're not able
[03:47:28.660 --> 03:47:31.540]   to replay the feeling perfectly.
[03:47:31.540 --> 03:47:32.700]   You don't remember the feeling,
[03:47:32.700 --> 03:47:34.500]   you remember the facts of the events.
[03:47:34.500 --> 03:47:36.100]   - So there's a sense in which, over time,
[03:47:36.100 --> 03:47:39.300]   we expand our vocabulary as a community of language,
[03:47:39.300 --> 03:47:41.540]   and that allows us to sort of have more feelings
[03:47:41.540 --> 03:47:43.700]   and know that we are feeling them.
[03:47:43.700 --> 03:47:45.580]   'Cause you can have a feeling, but not have a word for it,
[03:47:45.580 --> 03:47:47.660]   and then you don't know how to categorize it,
[03:47:47.660 --> 03:47:49.760]   or even what it is, and whether it's the same
[03:47:49.760 --> 03:47:52.260]   as something else, but once you have a word for it,
[03:47:52.260 --> 03:47:54.980]   you can sort of pull it together more easily.
[03:47:54.980 --> 03:47:57.160]   And so I think, over time, we are having
[03:47:57.160 --> 03:47:59.340]   a richer palette of feelings,
[03:47:59.340 --> 03:48:03.040]   'cause we have more words for them.
[03:48:03.040 --> 03:48:05.640]   - What has been a painful loss in your life?
[03:48:05.640 --> 03:48:09.820]   Maybe somebody or something that's no longer in your life,
[03:48:09.820 --> 03:48:12.720]   but played an important part in your life.
[03:48:12.720 --> 03:48:14.880]   - Youth? (laughs)
[03:48:14.880 --> 03:48:17.440]   - That's a concept, no, it has to be--
[03:48:17.440 --> 03:48:19.720]   But I was once younger, I had more health,
[03:48:19.720 --> 03:48:21.340]   and I had vitality, I was insomer,
[03:48:21.340 --> 03:48:23.080]   I mean, you know, I've lost that over time.
[03:48:23.080 --> 03:48:24.480]   - Do you see that as a different person?
[03:48:24.480 --> 03:48:26.080]   Maybe you've lost that person?
[03:48:26.080 --> 03:48:28.560]   - Certainly, yes, absolutely, I'm a different person
[03:48:28.560 --> 03:48:31.360]   than I was when I was younger, and I'm not who,
[03:48:31.360 --> 03:48:33.440]   I don't even remember exactly what he was.
[03:48:33.440 --> 03:48:35.880]   So I don't remember as many things from the past
[03:48:35.880 --> 03:48:37.360]   as many people do, so in some sense,
[03:48:37.360 --> 03:48:40.880]   I've just lost a lot of my history by not remembering it.
[03:48:40.880 --> 03:48:42.680]   - Does that-- - And I'm not that person
[03:48:42.680 --> 03:48:44.240]   anymore, that person's gone, and I don't have
[03:48:44.240 --> 03:48:45.200]   any of their abilities. - Is that a painful loss?
[03:48:45.200 --> 03:48:46.960]   Is it a painful loss, though?
[03:48:46.960 --> 03:48:48.680]   - Yeah. - Or is it a,
[03:48:48.680 --> 03:48:50.300]   why is it painful?
[03:48:50.300 --> 03:48:54.800]   'Cause you're wiser, you're, I mean,
[03:48:54.800 --> 03:48:57.120]   there's so many things that are beneficial
[03:48:57.120 --> 03:48:59.580]   to getting older. - Right, but--
[03:48:59.580 --> 03:49:02.800]   - Or you just call it-- - I just was this person,
[03:49:02.800 --> 03:49:06.520]   and I felt assured that I could continue to be that person.
[03:49:06.520 --> 03:49:07.840]   - And you're no longer that person.
[03:49:07.840 --> 03:49:10.280]   - And he's gone, and I'm not him anymore,
[03:49:10.280 --> 03:49:14.480]   and he died without fanfare or a funeral.
[03:49:14.480 --> 03:49:16.800]   - And that the person you are today, talking to me,
[03:49:16.800 --> 03:49:20.800]   that person will be changed, too.
[03:49:20.800 --> 03:49:24.440]   - Yes, and in 20 years, he won't be there anymore.
[03:49:24.440 --> 03:49:26.600]   - And a future person, you have to,
[03:49:26.600 --> 03:49:30.680]   we'll look back, a future version of you--
[03:49:30.680 --> 03:49:32.660]   - For M's, this'll be less of a problem.
[03:49:32.660 --> 03:49:35.160]   For M's, they would be able to save an archived copy
[03:49:35.160 --> 03:49:37.240]   of themselves at each different age,
[03:49:37.240 --> 03:49:39.080]   and they could turn it on periodically
[03:49:39.080 --> 03:49:40.120]   and go back and talk to it.
[03:49:40.120 --> 03:49:41.060]   - To replay.
[03:49:41.060 --> 03:49:43.640]   You think some of that will be,
[03:49:44.480 --> 03:49:47.380]   so with emulated minds, with M's,
[03:49:47.380 --> 03:49:54.000]   there's a digital cloning that happens,
[03:49:54.000 --> 03:49:58.240]   and do you think that makes your,
[03:49:58.240 --> 03:50:03.440]   you less special, if you're clonable?
[03:50:03.440 --> 03:50:08.440]   Like, does that make you the experience of life,
[03:50:08.440 --> 03:50:12.840]   the experience of a moment, the scarcity of that moment,
[03:50:12.840 --> 03:50:14.960]   the scarcity of that experience,
[03:50:14.960 --> 03:50:16.880]   isn't that a fundamental part of what makes
[03:50:16.880 --> 03:50:20.280]   that experience so delicious, so rich of feeling?
[03:50:20.280 --> 03:50:21.920]   - I think if you think of a song
[03:50:21.920 --> 03:50:23.440]   that lots of people listen to
[03:50:23.440 --> 03:50:25.040]   that are copies all over the world,
[03:50:25.040 --> 03:50:27.140]   we're gonna call that a more special song.
[03:50:27.140 --> 03:50:30.380]   - Yeah, yeah.
[03:50:30.380 --> 03:50:37.680]   So there's a perspective on copying and cloning
[03:50:37.680 --> 03:50:42.520]   where you're just scaling happiness versus degrading it.
[03:50:42.520 --> 03:50:44.720]   Each copy of a song is less special
[03:50:44.720 --> 03:50:45.680]   if there are many copies,
[03:50:45.680 --> 03:50:47.580]   but the song itself is more special
[03:50:47.580 --> 03:50:48.640]   if there are many copies.
[03:50:48.640 --> 03:50:52.960]   - And on mass, right, you're actually spreading
[03:50:52.960 --> 03:50:54.920]   the happiness even if it diminishes
[03:50:54.920 --> 03:50:56.800]   over a larger number of people at scale,
[03:50:56.800 --> 03:50:59.600]   and that increases the overall happiness in the world.
[03:50:59.600 --> 03:51:02.320]   And then you're able to do that with multiple songs.
[03:51:02.320 --> 03:51:05.600]   - Is a person who has an identical twin
[03:51:05.600 --> 03:51:07.880]   more or less special?
[03:51:07.880 --> 03:51:12.800]   - Well, the problem with identical twins
[03:51:12.800 --> 03:51:17.240]   is it's just two with Ms.
[03:51:17.240 --> 03:51:19.600]   - But two is different than one.
[03:51:19.600 --> 03:51:20.440]   - But there's a diminishing--
[03:51:20.440 --> 03:51:22.740]   - I think an identical twin's life is richer
[03:51:22.740 --> 03:51:24.400]   for having this other identical twin,
[03:51:24.400 --> 03:51:25.640]   somebody who understands them better
[03:51:25.640 --> 03:51:26.900]   than anybody else can.
[03:51:26.900 --> 03:51:30.040]   From the point of view of an identical twin,
[03:51:30.040 --> 03:51:31.480]   I think they have a richer life
[03:51:31.480 --> 03:51:33.720]   for being part of this couple,
[03:51:33.720 --> 03:51:34.680]   each of which is very similar.
[03:51:34.680 --> 03:51:37.120]   Now if you said, will the world,
[03:51:37.120 --> 03:51:38.880]   if we lose one of the identical twins,
[03:51:38.880 --> 03:51:40.320]   will the world miss it as much
[03:51:40.320 --> 03:51:41.360]   because you've got the other one
[03:51:41.360 --> 03:51:42.340]   and they're pretty similar?
[03:51:42.340 --> 03:51:44.000]   Maybe from the rest of the world's point of view,
[03:51:44.000 --> 03:51:46.400]   they suffer less of a loss
[03:51:46.400 --> 03:51:48.240]   when they lose one of the identical twins,
[03:51:48.240 --> 03:51:49.160]   but from the point of view
[03:51:49.160 --> 03:51:51.440]   of the identical twin themselves,
[03:51:51.440 --> 03:51:53.640]   their life is enriched by having a twin.
[03:51:53.640 --> 03:51:57.040]   - See, but the identical twin copying happens
[03:51:57.040 --> 03:51:58.580]   at the place of birth.
[03:51:58.580 --> 03:52:01.400]   It's different than copying
[03:52:01.400 --> 03:52:04.120]   after you've done some of the environment,
[03:52:04.120 --> 03:52:06.680]   like the nurture at the teenage
[03:52:06.680 --> 03:52:09.000]   or in the 20s after going to college.
[03:52:09.000 --> 03:52:10.040]   - Yes, that'll be an interesting thing
[03:52:10.040 --> 03:52:11.000]   for M's to find out,
[03:52:11.000 --> 03:52:11.960]   all the different ways
[03:52:11.960 --> 03:52:13.400]   that they can have different relationships
[03:52:13.400 --> 03:52:15.520]   to different people who have different degrees
[03:52:15.520 --> 03:52:17.760]   of similarity to them in time.
[03:52:17.760 --> 03:52:18.600]   - Yeah.
[03:52:18.600 --> 03:52:21.540]   Yeah, man.
[03:52:21.540 --> 03:52:25.760]   - But it seems like a rich space to explore
[03:52:25.760 --> 03:52:27.000]   and I don't feel sorry for them.
[03:52:27.000 --> 03:52:29.360]   This seems like interesting world to live in.
[03:52:29.360 --> 03:52:32.040]   - And there could be some ethical conundrums there.
[03:52:32.040 --> 03:52:33.920]   - There will be many new choices to make
[03:52:33.920 --> 03:52:36.200]   that they don't make now.
[03:52:36.200 --> 03:52:37.040]   We discussed it,
[03:52:37.040 --> 03:52:39.560]   and I discussed that in the book "Age of M."
[03:52:39.560 --> 03:52:41.040]   Say you have a lover
[03:52:41.040 --> 03:52:42.320]   and you make a copy of yourself,
[03:52:42.320 --> 03:52:43.560]   but the lover doesn't make a copy.
[03:52:43.560 --> 03:52:45.920]   Well, now, which one of you,
[03:52:45.920 --> 03:52:48.880]   or are both still related to the lover?
[03:52:48.880 --> 03:52:53.680]   - Socially entitled to show up.
[03:52:53.680 --> 03:52:56.380]   - Yes, so you'll have to make choices then
[03:52:56.380 --> 03:52:57.400]   when you split yourself.
[03:52:57.400 --> 03:53:00.280]   Which of you inherit which unique things?
[03:53:00.280 --> 03:53:03.040]   - Yeah, and of course,
[03:53:03.040 --> 03:53:08.040]   there'll be an equivalent increase in lawyers.
[03:53:08.040 --> 03:53:10.760]   Well, I guess you can clone the lawyers
[03:53:10.760 --> 03:53:14.800]   to help manage some of these negotiations
[03:53:14.800 --> 03:53:16.320]   of how to split property.
[03:53:16.320 --> 03:53:18.120]   The nature of owning, I mean,
[03:53:18.120 --> 03:53:22.240]   property is connected to individuals, right?
[03:53:22.240 --> 03:53:23.920]   - You only really need lawyers for this
[03:53:23.920 --> 03:53:25.720]   with an inefficient, awkward law
[03:53:25.720 --> 03:53:28.320]   that is not very transparent and able to do things.
[03:53:28.320 --> 03:53:31.980]   So, for example, an operating system of a computer
[03:53:31.980 --> 03:53:33.760]   is a law for that computer.
[03:53:33.760 --> 03:53:35.560]   When the operating system is simple and clean,
[03:53:35.560 --> 03:53:37.440]   you don't need to hire a lawyer
[03:53:37.440 --> 03:53:39.040]   to make a key choice with the operating system.
[03:53:39.040 --> 03:53:40.360]   - You don't need a human in the loop.
[03:53:40.360 --> 03:53:42.640]   - You just make a choice. - Qualify rules, yeah.
[03:53:42.640 --> 03:53:44.760]   - Right, so ideally, we want a legal system
[03:53:44.760 --> 03:53:47.920]   that makes the common choices easy
[03:53:47.920 --> 03:53:49.520]   and not require much overhead.
[03:53:49.520 --> 03:53:52.520]   - And that's what the digitization of things
[03:53:52.520 --> 03:53:54.500]   further enables that.
[03:53:54.500 --> 03:53:59.040]   So the loss of a younger self.
[03:53:59.040 --> 03:54:01.480]   What about the loss of your life overall?
[03:54:01.480 --> 03:54:03.960]   Do you ponder your death, your mortality?
[03:54:03.960 --> 03:54:05.240]   Are you afraid of it?
[03:54:05.240 --> 03:54:07.160]   - I am a cryonics customer.
[03:54:07.160 --> 03:54:09.720]   That's what this little tag around my deck says.
[03:54:09.720 --> 03:54:12.840]   It says that if you find me in a medical situation,
[03:54:12.840 --> 03:54:17.000]   you should call these people to enable the cryonics transfer.
[03:54:17.000 --> 03:54:20.400]   So I am taking a long-shot chance
[03:54:20.400 --> 03:54:22.640]   at living a much longer life.
[03:54:22.640 --> 03:54:25.600]   - Can you explain what cryonics is?
[03:54:25.600 --> 03:54:30.560]   - So when medical science gives up on me in this world,
[03:54:30.560 --> 03:54:33.800]   instead of burning me or letting worms eat me,
[03:54:33.800 --> 03:54:37.120]   they will freeze me, or at least freeze my head.
[03:54:37.120 --> 03:54:39.080]   And there's damage that happens
[03:54:39.080 --> 03:54:40.480]   in the process of freezing the head,
[03:54:40.480 --> 03:54:44.360]   but once it's frozen, it won't change for a very long time.
[03:54:44.360 --> 03:54:47.560]   Chemically, it'll just be completely exactly the same.
[03:54:47.560 --> 03:54:51.080]   So future technology might be able to revive me.
[03:54:51.080 --> 03:54:52.920]   And in fact, I would be mainly counting
[03:54:52.920 --> 03:54:54.880]   on the brain emulation scenario,
[03:54:54.880 --> 03:54:57.920]   which doesn't require reviving my entire biological body.
[03:54:57.920 --> 03:55:00.520]   It means I would be in a computer simulation.
[03:55:00.520 --> 03:55:06.560]   And so that's, I think I've got at least a 5% shot at that.
[03:55:06.560 --> 03:55:08.440]   And that's immortality.
[03:55:08.440 --> 03:55:10.920]   - Are you, can you still--
[03:55:10.920 --> 03:55:12.120]   - Most likely it won't happen,
[03:55:12.120 --> 03:55:15.040]   and therefore I'm sad that it won't happen.
[03:55:15.040 --> 03:55:17.960]   - Do you think immortality is something
[03:55:17.960 --> 03:55:19.360]   that you would like to have?
[03:55:19.360 --> 03:55:22.800]   - Well, I mean, just like infinity,
[03:55:22.800 --> 03:55:25.640]   I mean, you can't know until forever,
[03:55:25.640 --> 03:55:26.920]   which means never, right?
[03:55:26.920 --> 03:55:29.760]   So all you can really, the better choice is,
[03:55:29.760 --> 03:55:31.640]   at each moment, do you wanna keep going?
[03:55:31.640 --> 03:55:33.520]   So I would like at every moment
[03:55:33.520 --> 03:55:35.160]   to have the option to keep going.
[03:55:35.160 --> 03:55:40.080]   - The interesting thing about human experience
[03:55:40.080 --> 03:55:45.080]   is that the way you phrase it is exactly right.
[03:55:45.080 --> 03:55:48.840]   At every moment, I would like to keep going.
[03:55:48.840 --> 03:55:50.500]   But the thing that happens,
[03:55:50.500 --> 03:55:55.560]   you know, I'll leave them wanting more
[03:55:55.560 --> 03:55:59.360]   of whatever that phrase is.
[03:55:59.360 --> 03:56:01.200]   The thing that happens is over time,
[03:56:01.200 --> 03:56:05.680]   it's possible for certain experiences to become bland,
[03:56:05.680 --> 03:56:07.920]   and you become tired of them.
[03:56:07.920 --> 03:56:12.920]   And that actually makes life really unpleasant.
[03:56:12.920 --> 03:56:15.920]   Sorry, it makes that experience really unpleasant.
[03:56:15.920 --> 03:56:19.160]   And perhaps you can generalize that to life itself
[03:56:19.160 --> 03:56:21.440]   if you have a long enough horizon.
[03:56:21.440 --> 03:56:22.280]   And so--
[03:56:22.280 --> 03:56:24.760]   - Might happen, but might as well wait and find out.
[03:56:24.760 --> 03:56:28.240]   But then you're ending on suffering, you know?
[03:56:28.240 --> 03:56:31.980]   - So in the world of brain emulations, I have more options.
[03:56:31.980 --> 03:56:34.240]   - You can return yourself to--
[03:56:34.240 --> 03:56:36.920]   - That is, I can make copies of myself,
[03:56:36.920 --> 03:56:39.200]   archive copies at various ages.
[03:56:39.200 --> 03:56:40.940]   And at a later age, I could decide
[03:56:40.940 --> 03:56:42.520]   that I'd rather replace myself
[03:56:42.520 --> 03:56:44.800]   with a new copy from a younger age.
[03:56:44.800 --> 03:56:47.960]   - So does a brain emulation still operate
[03:56:47.960 --> 03:56:48.840]   in physical space?
[03:56:48.840 --> 03:56:51.440]   So can we do, what do you think about like the metaverse
[03:56:51.440 --> 03:56:53.520]   and operating in virtual reality?
[03:56:53.520 --> 03:56:56.000]   So we can conjure up, not just emulate,
[03:56:56.000 --> 03:56:59.440]   not just your own brain and body,
[03:56:59.440 --> 03:57:01.480]   but the entirety of the environment.
[03:57:01.480 --> 03:57:03.680]   - Most brain emulations will in fact
[03:57:03.680 --> 03:57:06.120]   spost most of their time in virtual reality.
[03:57:06.120 --> 03:57:08.640]   But they wouldn't think of it as virtual reality,
[03:57:08.640 --> 03:57:11.280]   they would just think of it as their usual reality.
[03:57:11.280 --> 03:57:13.520]   I mean, the thing to notice, I think, in our world,
[03:57:13.520 --> 03:57:16.440]   most of us spend most time indoors.
[03:57:16.440 --> 03:57:20.240]   And indoors, we are surrounded by walls covered with paint
[03:57:20.240 --> 03:57:23.520]   and floors covered with tile or rugs.
[03:57:23.520 --> 03:57:26.520]   Most of our environment is artificial.
[03:57:26.520 --> 03:57:28.640]   It's constructed to be convenient for us,
[03:57:28.640 --> 03:57:31.440]   it's not the natural world that was there before.
[03:57:31.440 --> 03:57:33.960]   A virtual reality is basically just like that.
[03:57:33.960 --> 03:57:35.920]   It is the environment that's comfortable
[03:57:35.920 --> 03:57:37.440]   and convenient for you.
[03:57:37.440 --> 03:57:40.540]   And, but when it's the right, that environment for you,
[03:57:40.540 --> 03:57:43.320]   it's real for you, just like the room you're in right now,
[03:57:43.320 --> 03:57:45.120]   most likely is very real for you.
[03:57:45.120 --> 03:57:47.240]   You're not focused on the fact that the paint
[03:57:47.240 --> 03:57:50.000]   is hiding the actual studs behind the wall
[03:57:50.000 --> 03:57:52.960]   and the actual wires and pipes and everything else.
[03:57:52.960 --> 03:57:54.320]   The fact that we're hiding that from you
[03:57:54.320 --> 03:57:56.280]   doesn't make it fake or unreal.
[03:57:56.280 --> 03:58:02.240]   - What are the chances that we're actually
[03:58:02.240 --> 03:58:04.540]   in the very kind of system that you're describing
[03:58:04.540 --> 03:58:07.400]   where the environment and the brain is being emulated
[03:58:07.400 --> 03:58:09.040]   and you're just replaying an experience
[03:58:09.040 --> 03:58:14.040]   when you were first did a podcast with Lex after,
[03:58:14.040 --> 03:58:17.980]   and now, you know, the person that originally launched this
[03:58:17.980 --> 03:58:19.880]   already did hundreds of podcasts with Lex.
[03:58:19.880 --> 03:58:21.360]   This is just the first time.
[03:58:21.360 --> 03:58:24.680]   And you like this time because there's so much uncertainty.
[03:58:24.680 --> 03:58:27.120]   There's nerves, it could have gone any direction.
[03:58:27.120 --> 03:58:30.640]   - At the moment, we don't have the technical ability
[03:58:30.640 --> 03:58:32.720]   to create that emulation.
[03:58:32.720 --> 03:58:35.560]   So we'd have to be postulating that in the future,
[03:58:35.560 --> 03:58:37.680]   we have that ability, and then they choose
[03:58:37.680 --> 03:58:40.480]   to evaluate this moment now, to simulate it.
[03:58:40.480 --> 03:58:43.840]   - Don't you think we could be in the simulation
[03:58:43.840 --> 03:58:45.440]   of that exact experience right now
[03:58:45.440 --> 03:58:47.080]   and we wouldn't be able to know?
[03:58:47.080 --> 03:58:51.180]   - So one scenario would be this never really happened.
[03:58:51.180 --> 03:58:54.440]   This only happens as a reconstruction later on.
[03:58:54.440 --> 03:58:55.280]   - Yeah.
[03:58:55.280 --> 03:58:56.200]   - That's different than the scenario
[03:58:56.200 --> 03:58:57.800]   that this did happen the first time
[03:58:57.800 --> 03:59:00.840]   and now it's happening again as a reconstruction.
[03:59:00.840 --> 03:59:03.880]   That second scenario is harder to put together
[03:59:03.880 --> 03:59:06.000]   because it requires this coincidence
[03:59:06.000 --> 03:59:09.220]   where between the two times we produce the ability to do it.
[03:59:09.220 --> 03:59:12.720]   - No, but don't you think replay of memories,
[03:59:13.880 --> 03:59:18.160]   poor replay of memories is something that--
[03:59:18.160 --> 03:59:19.640]   - That might be a possible thing in the future.
[03:59:19.640 --> 03:59:20.480]   - So you're saying it's harder
[03:59:20.480 --> 03:59:23.760]   than to conjure up things from scratch?
[03:59:23.760 --> 03:59:25.120]   - It's certainly possible.
[03:59:25.120 --> 03:59:26.840]   So the main way I would think about it
[03:59:26.840 --> 03:59:29.680]   is in terms of the demand for simulation
[03:59:29.680 --> 03:59:31.160]   versus other kinds of things.
[03:59:31.160 --> 03:59:32.720]   So I've given this a lot of thought
[03:59:32.720 --> 03:59:35.580]   because I first wrote about this long ago
[03:59:35.580 --> 03:59:37.400]   when Bostrom first wrote his papers
[03:59:37.400 --> 03:59:39.320]   about simulation argument and I wrote about
[03:59:39.320 --> 03:59:40.720]   how to live in a simulation.
[03:59:42.240 --> 03:59:47.240]   And so the key issue is the fraction of creatures
[03:59:47.240 --> 03:59:50.680]   in the universe that are really experiencing
[03:59:50.680 --> 03:59:52.560]   what you appear to be really experiencing
[03:59:52.560 --> 03:59:54.960]   relative to the fraction that are experiencing it
[03:59:54.960 --> 03:59:57.800]   in a simulation way, i.e. simulated.
[03:59:57.800 --> 04:00:02.800]   So then the key parameter is at any one moment in time,
[04:00:02.800 --> 04:00:06.920]   creatures at that time, many of them,
[04:00:06.920 --> 04:00:09.240]   most of them are presumably really experiencing
[04:00:09.240 --> 04:00:11.200]   what they're experiencing, but some fraction of them
[04:00:11.200 --> 04:00:14.600]   are experiencing some past time
[04:00:14.600 --> 04:00:17.280]   where that past time is being remembered
[04:00:17.280 --> 04:00:18.560]   via their simulation.
[04:00:18.560 --> 04:00:22.840]   So to figure out this ratio,
[04:00:22.840 --> 04:00:26.020]   what we need to think about is basically two functions.
[04:00:26.020 --> 04:00:30.360]   One is how fast in time does the number of creatures grow?
[04:00:30.360 --> 04:00:32.760]   And then how fast in time does the interest
[04:00:32.760 --> 04:00:33.960]   in the past decline?
[04:00:33.960 --> 04:00:38.000]   Because at any one time, people will be simulating
[04:00:38.000 --> 04:00:40.560]   different periods in the past with different emphasis
[04:00:40.560 --> 04:00:41.400]   based on-- - I love the way
[04:00:41.400 --> 04:00:43.000]   you think so much.
[04:00:43.000 --> 04:00:44.240]   That's exactly right, yeah.
[04:00:44.240 --> 04:00:48.120]   - So if the first function grows slower
[04:00:48.120 --> 04:00:50.440]   than the second one declines,
[04:00:50.440 --> 04:00:54.540]   then in fact, your chances of being simulated are low.
[04:00:54.540 --> 04:00:57.960]   So the key question is how fast does interest
[04:00:57.960 --> 04:01:00.440]   in the past decline relative to the rate
[04:01:00.440 --> 04:01:02.160]   at which the population grows with time?
[04:01:02.160 --> 04:01:04.360]   - Does this correlate to, you earlier suggested
[04:01:04.360 --> 04:01:07.520]   that the interest in the future increases over time.
[04:01:07.520 --> 04:01:09.640]   Are those correlated, interest in the future
[04:01:09.640 --> 04:01:11.000]   versus interest in the past?
[04:01:11.000 --> 04:01:13.480]   Like why are we interested in the past?
[04:01:13.480 --> 04:01:15.200]   - But the simple way to do it is, as you know,
[04:01:15.200 --> 04:01:18.240]   like Google Ngrams has a way to type in a word
[04:01:18.240 --> 04:01:21.480]   and see how interest in it declines or rises over time.
[04:01:21.480 --> 04:01:22.320]   Right? - Yeah, yeah.
[04:01:22.320 --> 04:01:24.840]   - You can just type in a year and get the answer for that.
[04:01:24.840 --> 04:01:29.160]   If you type in a particular year like 1900 or 1950,
[04:01:29.160 --> 04:01:32.720]   you can see with Google Ngram how interest in that year
[04:01:32.720 --> 04:01:36.160]   increased up until that date and decreased after it.
[04:01:36.160 --> 04:01:39.720]   And you can see that interest in a date declines faster
[04:01:39.720 --> 04:01:42.500]   than does the population grow with time.
[04:01:42.500 --> 04:01:45.480]   - That is brilliant.
[04:01:45.480 --> 04:01:46.760]   - And so-- - That is so interesting.
[04:01:46.760 --> 04:01:47.880]   - You have the answer.
[04:01:47.880 --> 04:01:50.720]   - Wow.
[04:01:50.720 --> 04:01:53.160]   And that was your argument against,
[04:01:53.160 --> 04:01:56.280]   not against, to this particular aspect of the simulation,
[04:01:56.280 --> 04:02:00.520]   how much past simulation there will be,
[04:02:00.520 --> 04:02:02.080]   replay of past memories.
[04:02:02.080 --> 04:02:04.120]   - First of all, if we assume that like simulation
[04:02:04.120 --> 04:02:06.560]   of the past is a small fraction of all the creatures
[04:02:06.560 --> 04:02:08.960]   at that moment, right? - Yes.
[04:02:08.960 --> 04:02:10.520]   - And then it's about how fast.
[04:02:10.520 --> 04:02:12.440]   Now, some people have argued plausibly
[04:02:12.440 --> 04:02:15.360]   that maybe most interest in the past
[04:02:15.360 --> 04:02:16.520]   falls with this fast function,
[04:02:16.520 --> 04:02:19.000]   but some unusual category of interest in the past
[04:02:19.000 --> 04:02:20.320]   won't fall that quickly,
[04:02:20.320 --> 04:02:22.200]   and then that eventually would dominate.
[04:02:22.200 --> 04:02:24.280]   So that's a other hypothesis.
[04:02:24.280 --> 04:02:25.600]   - Some category.
[04:02:25.600 --> 04:02:28.880]   So that very outlier specific kind of, yeah, okay.
[04:02:28.880 --> 04:02:29.720]   Yeah, yeah, yeah.
[04:02:29.720 --> 04:02:33.120]   Like really popular kinds of memories,
[04:02:33.120 --> 04:02:36.200]   but like probably sexual-- - In a trillion years,
[04:02:36.200 --> 04:02:38.560]   there's some small research institute
[04:02:38.560 --> 04:02:40.160]   that tries to randomly select
[04:02:40.160 --> 04:02:42.240]   from all possible people in history or something
[04:02:42.240 --> 04:02:43.440]   to simulate.
[04:02:43.440 --> 04:02:46.760]   - Yeah, yeah, yeah.
[04:02:46.760 --> 04:02:48.840]   - So the question is how big is this research institute
[04:02:48.840 --> 04:02:51.320]   and how big is the future in a trillion years, right?
[04:02:51.320 --> 04:02:52.840]   And that would be hard to say.
[04:02:52.840 --> 04:02:54.880]   But if we just look at the ordinary process
[04:02:54.880 --> 04:02:56.920]   by which people simulate recent,
[04:02:56.920 --> 04:02:59.120]   so if you look at,
[04:02:59.120 --> 04:03:02.120]   I think it's also true for movies and plays and video games,
[04:03:02.120 --> 04:03:04.920]   overwhelmingly they're interested in the recent past.
[04:03:04.920 --> 04:03:06.200]   There's very few video games
[04:03:06.200 --> 04:03:08.080]   where you play someone in the Roman Empire.
[04:03:08.080 --> 04:03:09.280]   - Right.
[04:03:09.280 --> 04:03:10.840]   - Even fewer where you play someone
[04:03:10.840 --> 04:03:12.600]   in the ancient Egyptian Empire.
[04:03:12.600 --> 04:03:15.520]   - Yeah, just different--
[04:03:15.520 --> 04:03:16.640]   - It's just declined very quickly.
[04:03:16.640 --> 04:03:19.360]   - But every once in a while, that's brought back.
[04:03:19.360 --> 04:03:21.960]   But yeah, you're right.
[04:03:21.960 --> 04:03:25.120]   I mean, just if you look at the mass of entertainment,
[04:03:25.120 --> 04:03:29.080]   movies and games, it's focusing on the present, recent past.
[04:03:29.080 --> 04:03:30.480]   And maybe some, I mean,
[04:03:30.480 --> 04:03:32.320]   where does science fiction fit into this?
[04:03:32.320 --> 04:03:35.200]   Because it's sort of,
[04:03:35.200 --> 04:03:39.080]   what is science fiction?
[04:03:39.080 --> 04:03:40.960]   I mean, it's a mix of the past and the present
[04:03:40.960 --> 04:03:43.080]   and some kind of manipulation of that
[04:03:43.080 --> 04:03:45.280]   to make it more efficient for us
[04:03:45.280 --> 04:03:49.000]   to ask deep philosophical questions about humanity.
[04:03:49.000 --> 04:03:51.600]   - The closest genre to science fiction is clearly fantasy.
[04:03:51.600 --> 04:03:53.400]   Fantasy and science fiction in many bookstores
[04:03:53.400 --> 04:03:55.240]   and even Netflix or whatever categories,
[04:03:55.240 --> 04:03:56.800]   they're just lumped together.
[04:03:56.800 --> 04:03:58.960]   So clearly they have a similar function.
[04:03:58.960 --> 04:04:01.800]   So the function of fantasy is more transparent
[04:04:01.800 --> 04:04:02.920]   than the function of science fiction.
[04:04:02.920 --> 04:04:04.960]   So use that as your guide.
[04:04:04.960 --> 04:04:05.880]   What's fantasy for?
[04:04:05.880 --> 04:04:08.240]   It's just to take away the constraints
[04:04:08.240 --> 04:04:09.080]   of the ordinary world
[04:04:09.080 --> 04:04:11.400]   and imagine stories with much fewer constraints.
[04:04:11.400 --> 04:04:12.520]   That's what fantasy is.
[04:04:12.520 --> 04:04:13.800]   You're much less constrained.
[04:04:13.800 --> 04:04:15.600]   - What's the purpose to remove constraints?
[04:04:15.600 --> 04:04:18.880]   Is it to escape from the harshness of the constraints
[04:04:18.880 --> 04:04:20.280]   of the real world?
[04:04:20.280 --> 04:04:22.080]   Or is it to just remove constraints
[04:04:22.080 --> 04:04:24.480]   in order to explore some,
[04:04:24.480 --> 04:04:26.920]   get a deeper understanding of our world?
[04:04:26.920 --> 04:04:27.760]   What is it?
[04:04:27.760 --> 04:04:28.920]   I mean, why do people read fantasy?
[04:04:28.920 --> 04:04:33.920]   - I'm not a cheap fantasy reading kind of person.
[04:04:33.920 --> 04:04:35.480]   So I need to...
[04:04:35.480 --> 04:04:38.200]   - One story that it sounds plausible to me
[04:04:38.200 --> 04:04:40.880]   is that there are sort of these deep story structures
[04:04:40.880 --> 04:04:43.920]   that we love and we want to realize,
[04:04:43.920 --> 04:04:46.640]   and then many details of the world get in their way.
[04:04:46.640 --> 04:04:48.680]   Fantasy takes all those obstacles out of the way
[04:04:48.680 --> 04:04:51.320]   and lets you tell the essential hero story
[04:04:51.320 --> 04:04:52.360]   or the essential love story,
[04:04:52.360 --> 04:04:54.760]   whatever essential story you want to tell.
[04:04:54.760 --> 04:04:57.120]   The reality and constraints are not in the way.
[04:04:57.960 --> 04:05:01.280]   - And so science fiction can be thought of as like fantasy,
[04:05:01.280 --> 04:05:04.320]   except you're not willing to admit that it can't be true.
[04:05:04.320 --> 04:05:06.200]   So the future gives the excuse of saying,
[04:05:06.200 --> 04:05:07.400]   "Well, it could happen."
[04:05:07.400 --> 04:05:11.040]   And you accept some more reality constraints
[04:05:11.040 --> 04:05:12.960]   for the illusion, at least,
[04:05:12.960 --> 04:05:14.720]   that maybe it could really happen.
[04:05:14.720 --> 04:05:18.000]   - Maybe it could happen,
[04:05:18.000 --> 04:05:19.960]   and that it stimulates the imagination.
[04:05:19.960 --> 04:05:23.000]   The imagination is something really interesting
[04:05:23.000 --> 04:05:24.840]   about human beings,
[04:05:24.840 --> 04:05:27.000]   and it seems also to be an important part
[04:05:27.000 --> 04:05:28.520]   of creating really special things,
[04:05:28.520 --> 04:05:30.920]   is to be able to first imagine them.
[04:05:30.920 --> 04:05:32.600]   With you and Nick Bostrom,
[04:05:32.600 --> 04:05:34.840]   where do you land on the simulation
[04:05:34.840 --> 04:05:38.560]   and all the mathematical ways of thinking it
[04:05:38.560 --> 04:05:41.240]   and just the thought experiment of it?
[04:05:41.240 --> 04:05:42.920]   Are we living in a simulation?
[04:05:42.920 --> 04:05:46.720]   - That was the discussion we just had.
[04:05:46.720 --> 04:05:49.280]   That is, you should grant the possibility
[04:05:49.280 --> 04:05:50.120]   of being in a simulation.
[04:05:50.120 --> 04:05:52.160]   You shouldn't be 100% confident that you're not.
[04:05:52.160 --> 04:05:54.320]   You should certainly grant a small probability.
[04:05:54.320 --> 04:05:56.280]   The question is, how large is that probability?
[04:05:56.280 --> 04:05:57.880]   - Are you saying we would be,
[04:05:57.880 --> 04:06:01.200]   I misunderstood because I thought our discussion
[04:06:01.200 --> 04:06:03.480]   was about replaying things that have already happened.
[04:06:03.480 --> 04:06:05.400]   - Right, but the whole question is,
[04:06:05.400 --> 04:06:08.200]   right now, is that what I am?
[04:06:08.200 --> 04:06:11.960]   Am I actually a replay from some distant future?
[04:06:11.960 --> 04:06:13.760]   - But it doesn't necessarily need to be a replay.
[04:06:13.760 --> 04:06:15.400]   It could be a totally new.
[04:06:15.400 --> 04:06:16.920]   You don't have to be an NPC.
[04:06:16.920 --> 04:06:19.200]   - Right, but clearly, I'm in a certain era
[04:06:19.200 --> 04:06:21.040]   with a certain kind of world around me, right?
[04:06:21.040 --> 04:06:22.600]   So either this is a complete fantasy
[04:06:22.600 --> 04:06:24.880]   or it's a past of somebody else in the future.
[04:06:25.720 --> 04:06:28.200]   But no, it could be a complete fantasy, though.
[04:06:28.200 --> 04:06:30.440]   - It could be, right, but then you have to talk about
[04:06:30.440 --> 04:06:33.840]   what's the frank fraction of complete fantasies, right?
[04:06:33.840 --> 04:06:35.840]   - I would say it's easier to generate a fantasy
[04:06:35.840 --> 04:06:37.920]   than to replay a memory, right?
[04:06:37.920 --> 04:06:40.000]   - Sure, but if we just look at the entire history,
[04:06:40.000 --> 04:06:41.760]   if we just look at the entire history of everything,
[04:06:41.760 --> 04:06:43.880]   we should say, sure, but most things are real.
[04:06:43.880 --> 04:06:45.360]   Most things aren't fantasies, right?
[04:06:45.360 --> 04:06:47.200]   Therefore, the chance that my thing is real, right?
[04:06:47.200 --> 04:06:49.680]   So the simulation argument works stronger
[04:06:49.680 --> 04:06:50.600]   about sort of the past.
[04:06:50.600 --> 04:06:52.560]   We say, ah, but there's more future people
[04:06:52.560 --> 04:06:53.840]   than there are today.
[04:06:53.840 --> 04:06:56.080]   So you being in the past or the future
[04:06:56.080 --> 04:06:57.720]   makes you special relative to them,
[04:06:57.720 --> 04:07:00.240]   which makes you more likely to be in a simulation, right?
[04:07:00.240 --> 04:07:02.720]   If we're just taking the full count and saying,
[04:07:02.720 --> 04:07:05.120]   in all creatures ever, what percentage are in simulations?
[04:07:05.120 --> 04:07:07.600]   Probably no more than 10%.
[04:07:07.600 --> 04:07:10.000]   - So what's a good argument for that?
[04:07:10.000 --> 04:07:11.680]   That most things are real?
[04:07:11.680 --> 04:07:12.520]   - Yeah.
[04:07:12.520 --> 04:07:15.000]   - 'Cause as Bostrom says the other way, right?
[04:07:15.000 --> 04:07:18.160]   - In a competitive world, in a world where people
[04:07:18.160 --> 04:07:20.960]   like have to work and have to get things done,
[04:07:21.040 --> 04:07:24.800]   then they have a limited budget for leisure.
[04:07:24.800 --> 04:07:28.240]   And so, you know, leisure things are less common
[04:07:28.240 --> 04:07:31.080]   than work things, like real things, right?
[04:07:31.080 --> 04:07:32.880]   That's just--
[04:07:32.880 --> 04:07:37.880]   - But if you look at the stretch of history in the universe,
[04:07:37.880 --> 04:07:41.980]   doesn't the ratio of leisure increase?
[04:07:41.980 --> 04:07:45.480]   Isn't that where we, isn't that the--
[04:07:45.480 --> 04:07:47.480]   - Right, but now we're looking at the fraction of leisure,
[04:07:47.480 --> 04:07:49.160]   which takes the form of something
[04:07:49.160 --> 04:07:52.120]   where the person doing the leisure doesn't realize it.
[04:07:52.120 --> 04:07:53.320]   Now there could be some fraction of it,
[04:07:53.320 --> 04:07:55.240]   but that's much smaller, right?
[04:07:55.240 --> 04:07:56.080]   - Yeah.
[04:07:56.080 --> 04:07:59.040]   Clueless foragers.
[04:07:59.040 --> 04:08:00.860]   - Or somebody is clueless in the process
[04:08:00.860 --> 04:08:02.800]   of supporting this leisure, right?
[04:08:02.800 --> 04:08:04.120]   It might not be the person leisuring,
[04:08:04.120 --> 04:08:05.760]   somebody, they're a supporting character or something,
[04:08:05.760 --> 04:08:07.120]   but still, that's gotta be a pretty small
[04:08:07.120 --> 04:08:08.120]   fraction of leisure.
[04:08:08.120 --> 04:08:11.900]   - What, you mentioned that children are one of the things
[04:08:11.900 --> 04:08:14.960]   that are a source of meaning, broadly speaking.
[04:08:14.960 --> 04:08:16.440]   And let me ask the big question,
[04:08:16.440 --> 04:08:18.400]   what's the meaning of this whole thing?
[04:08:19.160 --> 04:08:21.280]   - The-- - Robin, meaning of life.
[04:08:21.280 --> 04:08:23.200]   What is the meaning of life?
[04:08:23.200 --> 04:08:26.260]   We talked about alien civilizations,
[04:08:26.260 --> 04:08:27.680]   but this is the one we got.
[04:08:27.680 --> 04:08:28.920]   Where are the aliens?
[04:08:28.920 --> 04:08:30.960]   Where are the human?
[04:08:30.960 --> 04:08:35.160]   Seem to be conscious, be able to introspect.
[04:08:35.160 --> 04:08:37.400]   What's, why are we here?
[04:08:37.400 --> 04:08:39.220]   - This is the thing I told you before
[04:08:39.220 --> 04:08:41.560]   about how we can predict that future creatures
[04:08:41.560 --> 04:08:42.860]   will be different from us.
[04:08:42.860 --> 04:08:47.400]   We, our preferences are this amalgam
[04:08:47.400 --> 04:08:51.280]   of various sorts of random sort of patched together
[04:08:51.280 --> 04:08:55.200]   preferences about thirst and sex and sleep
[04:08:55.200 --> 04:08:57.420]   and attention and all these sorts of things.
[04:08:57.420 --> 04:08:59.760]   So we don't understand that very well.
[04:08:59.760 --> 04:09:03.300]   It's not very transparent and it's a mess, right?
[04:09:03.300 --> 04:09:06.600]   That is the source of our motivation.
[04:09:06.600 --> 04:09:10.280]   That is how we were made and how we are induced to do things.
[04:09:10.280 --> 04:09:12.400]   But we can't summarize it very well
[04:09:12.400 --> 04:09:14.440]   and we don't even understand it very well.
[04:09:14.440 --> 04:09:15.740]   That's who we are.
[04:09:15.740 --> 04:09:17.480]   And often we find ourselves in a situation
[04:09:17.480 --> 04:09:19.560]   where we don't feel very motivated, we don't know why.
[04:09:19.560 --> 04:09:21.740]   In other situations, we find ourselves very motivated
[04:09:21.740 --> 04:09:23.200]   and we don't know why either.
[04:09:23.200 --> 04:09:27.880]   And so that's the nature of being a human
[04:09:27.880 --> 04:09:28.960]   of the sort that we are,
[04:09:28.960 --> 04:09:31.400]   because even though we can think abstractly
[04:09:31.400 --> 04:09:33.760]   and reason abstractly, this package of motivations
[04:09:33.760 --> 04:09:35.960]   is just opaque and a mess.
[04:09:35.960 --> 04:09:38.500]   And that's what it means to be a human today
[04:09:38.500 --> 04:09:39.440]   and the motivation.
[04:09:39.440 --> 04:09:42.280]   We can't very well tell the meaning of our life.
[04:09:42.280 --> 04:09:43.600]   It is this mess.
[04:09:43.600 --> 04:09:44.920]   That our descendants will be different.
[04:09:44.920 --> 04:09:48.300]   They will actually know exactly what they want
[04:09:48.300 --> 04:09:51.080]   and it will be to have more descendants.
[04:09:51.080 --> 04:09:52.760]   That will be the meaning for them.
[04:09:52.760 --> 04:09:54.640]   - Well, it's funny that you have the certainty.
[04:09:54.640 --> 04:09:57.400]   You have more certainty, you have more transparency
[04:09:57.400 --> 04:10:01.960]   about our descendants than you do about your own self.
[04:10:01.960 --> 04:10:02.800]   - Right.
[04:10:02.800 --> 04:10:06.000]   - So it's really interesting to think,
[04:10:06.000 --> 04:10:08.040]   'cause you mentioned this about love,
[04:10:08.040 --> 04:10:12.200]   that something that's fundamental about love
[04:10:12.200 --> 04:10:13.820]   is this opaqueness, that we're not able
[04:10:13.820 --> 04:10:16.460]   to really introspect what the heck it is.
[04:10:16.460 --> 04:10:19.720]   Or all the feelings, the complex feelings involved.
[04:10:19.720 --> 04:10:21.520]   - And that's true about many of our motivations.
[04:10:21.520 --> 04:10:23.860]   - And that's what it means to be human
[04:10:23.860 --> 04:10:28.860]   of the 20th and the 21st century variety.
[04:10:28.860 --> 04:10:33.640]   Why is that not a feature that we want,
[04:10:33.640 --> 04:10:37.040]   will choose to persist in civilization then?
[04:10:37.040 --> 04:10:40.400]   This opaqueness, put another way, mystery.
[04:10:40.400 --> 04:10:42.440]   Maintaining a sense of mystery about ourselves
[04:10:42.440 --> 04:10:44.480]   and about those around us.
[04:10:44.480 --> 04:10:47.040]   Maybe that's a really nice thing to have, maybe.
[04:10:47.040 --> 04:10:50.140]   - So, I mean, this is the fundamental issue
[04:10:50.140 --> 04:10:53.780]   in analyzing the future, what will set the future?
[04:10:53.780 --> 04:10:55.440]   One theory about what will set the future
[04:10:55.440 --> 04:10:58.560]   is what do we want the future to be?
[04:10:58.560 --> 04:11:00.320]   So under that theory, we should sit and talk
[04:11:00.320 --> 04:11:01.480]   about what we want the future to be,
[04:11:01.480 --> 04:11:04.100]   have some conferences, have some conventions,
[04:11:04.100 --> 04:11:06.000]   discussion things, vote on it maybe,
[04:11:06.000 --> 04:11:08.200]   and then hand it off to the implementation people
[04:11:08.200 --> 04:11:11.000]   to make the future the way we've decided it should be.
[04:11:12.400 --> 04:11:15.080]   That's not the actual process that's changed the world
[04:11:15.080 --> 04:11:16.800]   over history up to this point.
[04:11:16.800 --> 04:11:19.280]   It has not been the result of us deciding
[04:11:19.280 --> 04:11:21.440]   what we want and making it happen.
[04:11:21.440 --> 04:11:23.400]   In our individual lives, we can do that.
[04:11:23.400 --> 04:11:24.880]   We might decide what career we want
[04:11:24.880 --> 04:11:26.900]   or where we want to live, who we want to live with.
[04:11:26.900 --> 04:11:29.640]   In our individual lives, we often do slowly
[04:11:29.640 --> 04:11:32.040]   make our lives better according to our plan and our things,
[04:11:32.040 --> 04:11:33.600]   but that's not the whole world.
[04:11:33.600 --> 04:11:37.280]   The whole world so far has mostly been a competitive world
[04:11:37.280 --> 04:11:40.300]   where things happen if anybody anywhere chooses
[04:11:40.300 --> 04:11:42.280]   to adopt them and they have an advantage.
[04:11:42.280 --> 04:11:43.920]   And then it spreads and other people are forced
[04:11:43.920 --> 04:11:46.360]   to adopt it by competitive pressures.
[04:11:46.360 --> 04:11:48.360]   So that's the kind of analysis I can use
[04:11:48.360 --> 04:11:50.260]   to predict the future, and I do use that
[04:11:50.260 --> 04:11:51.100]   to predict the future.
[04:11:51.100 --> 04:11:52.640]   It doesn't tell us it'll be a future we like.
[04:11:52.640 --> 04:11:54.840]   It just tells us what it'll be.
[04:11:54.840 --> 04:11:56.760]   - And it'll be one where we're trying to maximize
[04:11:56.760 --> 04:11:57.920]   the number of our descendants.
[04:11:57.920 --> 04:12:00.600]   - And we know that abstractly and directly,
[04:12:00.600 --> 04:12:01.680]   and it's not opaque.
[04:12:01.680 --> 04:12:04.380]   - With some probability that's non-zero
[04:12:04.380 --> 04:12:06.400]   that will lead us to become grabby
[04:12:06.400 --> 04:12:12.080]   in expanding aggressively out into the cosmos
[04:12:12.080 --> 04:12:13.800]   until we meet other aliens.
[04:12:13.800 --> 04:12:14.840]   - The timing isn't clear.
[04:12:14.840 --> 04:12:17.160]   We might become grabby and then this happens.
[04:12:17.160 --> 04:12:19.480]   These are, be grabbyness and this are both
[04:12:19.480 --> 04:12:21.680]   the results of competition, but it's less clear
[04:12:21.680 --> 04:12:22.680]   which happens first.
[04:12:22.680 --> 04:12:26.840]   - Does this future excite you, scare you?
[04:12:26.840 --> 04:12:27.840]   How do you feel about this whole thing?
[04:12:27.840 --> 04:12:30.000]   - Well, again, I told you compared to sort of
[04:12:30.000 --> 04:12:32.720]   a dead cosmology, at least it's energizing
[04:12:32.720 --> 04:12:34.920]   and having a living story with real actors
[04:12:34.920 --> 04:12:36.840]   and characters and agendas, right?
[04:12:36.840 --> 04:12:40.880]   - Yeah, and that's one hell of a fun universe to live in.
[04:12:40.880 --> 04:12:42.960]   - Robin, you're one of the most fascinating,
[04:12:42.960 --> 04:12:47.200]   fun people to talk to, brilliant, humble,
[04:12:47.200 --> 04:12:48.920]   systematic in your analysis.
[04:12:48.920 --> 04:12:50.000]   - Hold on to my wallet here.
[04:12:50.000 --> 04:12:50.840]   What's he looking for?
[04:12:50.840 --> 04:12:53.040]   - I already stole your wallet long ago.
[04:12:53.040 --> 04:12:54.560]   I really, really appreciate you spending
[04:12:54.560 --> 04:12:55.560]   your valuable time with me.
[04:12:55.560 --> 04:12:58.880]   I hope we get a chance to talk many more times
[04:12:58.880 --> 04:12:59.720]   in the future.
[04:12:59.720 --> 04:13:01.440]   Thank you so much for sitting down.
[04:13:01.440 --> 04:13:02.260]   - Thank you.
[04:13:02.260 --> 04:13:04.800]   - Thanks for listening to this conversation
[04:13:04.800 --> 04:13:06.080]   with Robin Hanson.
[04:13:06.080 --> 04:13:08.640]   To support this podcast, please check out our sponsors
[04:13:08.640 --> 04:13:10.040]   in the description.
[04:13:10.040 --> 04:13:12.040]   And now let me leave you with some words
[04:13:12.040 --> 04:13:13.840]   from Ray Bradbury.
[04:13:13.840 --> 04:13:18.840]   We are an impossibility in an impossible universe.
[04:13:18.840 --> 04:13:21.600]   Thank you for listening and hope to see you next time.
[04:13:21.600 --> 04:13:24.180]   (upbeat music)
[04:13:24.180 --> 04:13:26.760]   (upbeat music)
[04:13:26.760 --> 04:13:36.760]   [BLANK_AUDIO]

