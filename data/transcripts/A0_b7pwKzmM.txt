
[00:00:00.000 --> 00:00:05.400]   There's lots of genetic defect in everyone.
[00:00:05.400 --> 00:00:11.640]   We think the average healthy person has a couple of hundred genes with defective function
[00:00:11.640 --> 00:00:12.640]   in their genome.
[00:00:12.640 --> 00:00:18.340]   And there's probably lots of what we think of as subclinical symptoms wandering around
[00:00:18.340 --> 00:00:20.000]   across the whole population.
[00:00:20.000 --> 00:00:25.000]   So I think that's kind of the future vision of the company is everyone would benefit from
[00:00:25.000 --> 00:00:30.280]   having a full understanding of their genetic background.
[00:00:30.280 --> 00:00:35.000]   And it's a complicated problem that the doctors don't understand.
[00:00:35.000 --> 00:00:36.780]   The patients certainly don't understand.
[00:00:36.780 --> 00:00:40.040]   And they were really at the frontier of understanding.
[00:00:40.040 --> 00:00:44.400]   You're listening to Gradient Dissent, a show about machine learning in the real world.
[00:00:44.400 --> 00:00:46.960]   And I'm your host, Lucas Biewald.
[00:00:46.960 --> 00:00:51.880]   Matthew Davis is the head of AI at Vitae, a medical genetic testing company.
[00:00:51.880 --> 00:00:55.680]   He applies a really wide range of machine learning techniques to the genetic testing
[00:00:55.680 --> 00:00:59.920]   problem, which I think is one of the most interesting applications of ML today.
[00:00:59.920 --> 00:01:02.840]   I'm super excited to talk to him.
[00:01:02.840 --> 00:01:08.000]   In Vitae is actually a household name in my house because my wife runs a startup that
[00:01:08.000 --> 00:01:10.760]   sells to Vitae and I run a startup that also sells to Vitae.
[00:01:10.760 --> 00:01:13.560]   So you're one of the very few overlapping customers to us.
[00:01:13.560 --> 00:01:18.720]   So I feel like I know Vitae very well, but I was thinking if that wasn't the case, I
[00:01:18.720 --> 00:01:20.000]   would definitely not know In Vitae.
[00:01:20.000 --> 00:01:25.120]   So I was wondering if you could describe what In Vitae does and how I might interact with
[00:01:25.120 --> 00:01:27.280]   your products as a consumer.
[00:01:27.280 --> 00:01:28.400]   Yeah, sure.
[00:01:28.400 --> 00:01:33.400]   So for starters, we're a medical genetic diagnostics company.
[00:01:33.400 --> 00:01:39.200]   And I'm pretty sure by volume of tests, we're the biggest in the world.
[00:01:39.200 --> 00:01:43.520]   Which is amazing because you're fairly new for a public company, right?
[00:01:43.520 --> 00:01:46.280]   Didn't you start in 2010 or something like that?
[00:01:46.280 --> 00:01:47.280]   Yeah, that's about right.
[00:01:47.280 --> 00:01:50.320]   So the company itself is about a decade old.
[00:01:50.320 --> 00:01:56.640]   And it's not a coincidence because the availability of high throughput, low cost genome sequencing
[00:01:56.640 --> 00:02:04.520]   really came online in like 2008 with Illumina, like making a scalable platform.
[00:02:04.520 --> 00:02:10.240]   And at that point, it became clear that instead of analyzing one or two genes at a time, that
[00:02:10.240 --> 00:02:14.160]   you could be analyzing lots of genes for less money.
[00:02:14.160 --> 00:02:21.520]   And I think the strategy that was clear to the founders was, here's a market with, it's
[00:02:21.520 --> 00:02:26.800]   a very narrow market with a very high margin, that actually should be an adjustable market
[00:02:26.800 --> 00:02:29.920]   of everyone with access to modern medicine.
[00:02:29.920 --> 00:02:34.800]   And instead, the cost could be low and the volume could be high.
[00:02:34.800 --> 00:02:41.440]   And that if you pursued that strategy, there was actually way bigger benefits to like mankind,
[00:02:41.440 --> 00:02:45.960]   and also shareholders of a company, because you'd start to learn things about medical
[00:02:45.960 --> 00:02:48.360]   genetics and disease.
[00:02:48.360 --> 00:02:52.800]   And probably most importantly, relationships to treatments that you weren't going to learn
[00:02:52.800 --> 00:02:56.840]   if you took a small, small adjustable market strategy.
[00:02:56.840 --> 00:02:58.280]   So that was the vision.
[00:02:58.280 --> 00:03:02.960]   I think you live in this, but for someone who hasn't had a genetic test for a medical
[00:03:02.960 --> 00:03:06.520]   reason, what would be a scenario where you'd actually want that and what would it do for
[00:03:06.520 --> 00:03:07.520]   you?
[00:03:07.520 --> 00:03:08.520]   Yeah.
[00:03:08.520 --> 00:03:10.920]   So classically, diagnostics were not about genetics.
[00:03:10.920 --> 00:03:16.560]   They were about, your cholesterol is high or some other hormone is low or whatever.
[00:03:16.560 --> 00:03:23.200]   Genetic diagnostics were first kind of proven in mass at breast cancer, where we know there
[00:03:23.200 --> 00:03:28.020]   are genetic predispositions that would change your treatment strategy, where the risk is
[00:03:28.020 --> 00:03:33.960]   high enough that if your mother or your sister or your grandmother, your aunt had breast
[00:03:33.960 --> 00:03:37.720]   cancer at age 70, people get cancer when they're old.
[00:03:37.720 --> 00:03:41.240]   But if it's at age 35, that's way scarier.
[00:03:41.240 --> 00:03:46.560]   And when we added genetic analysis on top of that, we could further partition that to,
[00:03:46.560 --> 00:03:52.400]   "Oh, because they had this variant, they were early cancer patients."
[00:03:52.400 --> 00:03:57.480]   And then your doctor can help you make the best proactive decisions about how to avoid
[00:03:57.480 --> 00:03:58.480]   it.
[00:03:58.480 --> 00:04:01.360]   At the extreme level, that's like a proactive mastectomy.
[00:04:01.360 --> 00:04:04.680]   And but there are lots of other intermediates, including like, which type of drug would be
[00:04:04.680 --> 00:04:06.280]   most effective for you?
[00:04:06.280 --> 00:04:08.200]   Should you risk the downsides of chemo?
[00:04:08.200 --> 00:04:10.120]   Should you just wait?
[00:04:10.120 --> 00:04:13.600]   So it worked with breast cancer.
[00:04:13.600 --> 00:04:18.600]   And then as we started being able to analyze more and more genes, we started discovering
[00:04:18.600 --> 00:04:20.920]   more and more things that it works for.
[00:04:20.920 --> 00:04:26.240]   I think one thing to keep in mind is that human geneticists in general, geneticists,
[00:04:26.240 --> 00:04:29.360]   they historically study horrific things, right?
[00:04:29.360 --> 00:04:34.040]   Like whether you're studying fruit flies or mice or humans, it's big effects.
[00:04:34.040 --> 00:04:38.440]   There's this old saying, like big mutations have big effects.
[00:04:38.440 --> 00:04:42.880]   Meaning, we study things that give you a heart attack at an early age or make you grow tumors
[00:04:42.880 --> 00:04:45.680]   or have crippling nervous system diseases, right?
[00:04:45.680 --> 00:04:49.520]   And there are lots of people at risk for that who don't know it.
[00:04:49.520 --> 00:04:54.200]   But I think the real future is there's lots of genetic defect in everyone.
[00:04:54.200 --> 00:05:00.440]   We think the average healthy person has a couple of hundred genes with defective function
[00:05:00.440 --> 00:05:02.280]   in their genome.
[00:05:02.280 --> 00:05:07.120]   And there's probably lots of what we think of as subclinical symptoms wandering around
[00:05:07.120 --> 00:05:08.800]   across the whole population.
[00:05:08.800 --> 00:05:14.440]   So I think that's kind of the future vision of the company is everyone would benefit from
[00:05:14.440 --> 00:05:19.720]   having a full understanding of their genetic background.
[00:05:19.720 --> 00:05:24.440]   And it's a complicated problem that the doctors don't understand.
[00:05:24.440 --> 00:05:26.240]   The patients certainly don't understand.
[00:05:26.240 --> 00:05:28.640]   And they were really at the frontier of understanding.
[00:05:28.640 --> 00:05:33.040]   So that's a little bit of the history and a little bit of the future mission statement.
[00:05:33.040 --> 00:05:37.760]   And so I'd imagine that a lot of people listening to this have done one of the kind of consumer
[00:05:37.760 --> 00:05:40.640]   tests, like maybe like Ancestry or 23andMe.
[00:05:40.640 --> 00:05:43.200]   So how is what you do different from what happens there?
[00:05:43.200 --> 00:05:44.200]   Yeah.
[00:05:44.200 --> 00:05:45.360]   I mean, it's a great question.
[00:05:45.360 --> 00:05:49.080]   And it's one that pre-COVID riding on a plane, someone asks you what you do and they're like,
[00:05:49.080 --> 00:05:50.080]   "Oh, like 23andMe."
[00:05:50.080 --> 00:05:51.080]   And you're like, "Oh man, no."
[00:05:51.080 --> 00:05:57.360]   So I mean, the obvious difference to the interaction with our customers is that historically it
[00:05:57.360 --> 00:05:59.480]   goes through a doctor.
[00:05:59.480 --> 00:06:05.720]   It's a medical test and you want that provisioned and administered by a medical professional.
[00:06:05.720 --> 00:06:11.320]   And 23andMe is a fascinating company, but they've focused on things like whether or
[00:06:11.320 --> 00:06:16.440]   not you like cilantro, not whether or not you are at risk for a disease.
[00:06:16.440 --> 00:06:23.520]   And they have tried to move into a diagnostic space, but they're not built for that.
[00:06:23.520 --> 00:06:29.840]   And we finally acknowledged that a couple of years ago, after many years of not wanting
[00:06:29.840 --> 00:06:37.520]   to offer a medical diagnostic procedure directly to patients, because we didn't want people
[00:06:37.520 --> 00:06:43.520]   to go with information in hand, but not understanding an explanation that they could get from a
[00:06:43.520 --> 00:06:44.980]   medical caregiver.
[00:06:44.980 --> 00:06:48.680]   So that's the real difference is like we have medical caretakers in place, but we now have
[00:06:48.680 --> 00:06:53.360]   a strategy where we let patients initiate their orders.
[00:06:53.360 --> 00:06:56.640]   And that's really because there's a lot of the country that doesn't have access to one
[00:06:56.640 --> 00:07:00.000]   of the few thousand genetic counselors in the US.
[00:07:00.000 --> 00:07:02.160]   So there are places where it's a six month wait.
[00:07:02.160 --> 00:07:05.320]   There's places where you're just not going to go.
[00:07:05.320 --> 00:07:12.080]   And thanks to telemedicine, thanks to software engineering, it's easier now for us to let
[00:07:12.080 --> 00:07:17.000]   a patient who's mother had breast cancer start the process themselves.
[00:07:17.000 --> 00:07:22.200]   We refer them to a telemedicine genetic counselor who helps them by being their medical caregiver
[00:07:22.200 --> 00:07:26.440]   without them having to wait six months to go to a medical center that's a hundred miles
[00:07:26.440 --> 00:07:27.440]   away.
[00:07:27.440 --> 00:07:28.440]   And we think that's great.
[00:07:28.440 --> 00:07:32.720]   Does it lead to a little more confusion of like, well, we used to say we're not consumer
[00:07:32.720 --> 00:07:36.080]   facing and now we have patient facing border farms.
[00:07:36.080 --> 00:07:41.440]   But that's really the difference is like we are trying to help people with a complicated
[00:07:41.440 --> 00:07:45.200]   medical problem, not find out your ancestry.
[00:07:45.200 --> 00:07:49.320]   Unless of course your ancestry has direct bearing on the medical risk.
[00:07:49.320 --> 00:07:50.320]   Got it.
[00:07:50.320 --> 00:07:51.760]   That makes sense.
[00:07:51.760 --> 00:07:55.280]   And so, I mean, how does AI fit into this?
[00:07:55.280 --> 00:07:58.240]   Like you talk about a broad range.
[00:07:58.240 --> 00:08:02.560]   When I've talked to you in the past, I've been kind of shocked by the number of different
[00:08:02.560 --> 00:08:05.560]   ML kind of fields that you draw from.
[00:08:05.560 --> 00:08:10.440]   So I was thinking maybe you could give me an overview of the different problems where
[00:08:10.440 --> 00:08:13.920]   machine learning techniques can help with what you all are doing.
[00:08:13.920 --> 00:08:14.920]   Yeah.
[00:08:14.920 --> 00:08:22.040]   So, you know, we say AI and it was an easy to adopt term, especially for the last few
[00:08:22.040 --> 00:08:23.040]   years.
[00:08:23.040 --> 00:08:25.560]   But when I think of AI, I really think of like every chapter of a textbook of a field
[00:08:25.560 --> 00:08:29.040]   of computer science that's been around for many decades.
[00:08:29.040 --> 00:08:32.080]   And a lot of it, thankfully, is machine learning and some of it's not.
[00:08:32.080 --> 00:08:37.560]   So some of it is like optimization algorithms and robotic planning and so forth that has
[00:08:37.560 --> 00:08:42.360]   been around for a long time and it's still making rapid advance in those fields.
[00:08:42.360 --> 00:08:46.600]   But maybe it's a little less well known to machine learning folks.
[00:08:46.600 --> 00:08:51.680]   And then a bunch of it is machine learning approximations that can make a problem tractable
[00:08:51.680 --> 00:08:53.080]   that wasn't tractable before.
[00:08:53.080 --> 00:09:00.320]   So I mean, the actual applications, we have a key scaling problem in a lot of ways where
[00:09:00.320 --> 00:09:07.640]   like a manufacturing company and that our volume tends to almost double every year.
[00:09:07.640 --> 00:09:12.480]   And we have a laboratory that has to run assays with actual robots.
[00:09:12.480 --> 00:09:17.640]   We have rather complicated standard operating procedures and business process models that
[00:09:17.640 --> 00:09:23.120]   need careful execution, not to mention like audit logging and accounting stuff.
[00:09:23.120 --> 00:09:24.120]   That's the medical field.
[00:09:24.120 --> 00:09:28.680]   So you have to be compliant and follow not just like HIPAA laws, which are complex, but
[00:09:28.680 --> 00:09:32.920]   also like contractual obligations to insurance companies and things like that.
[00:09:32.920 --> 00:09:38.200]   So there's a lot of complicated process modeling and then there's a lot of knowledge worker
[00:09:38.200 --> 00:09:39.200]   problems.
[00:09:39.200 --> 00:09:47.960]   So we have on staff, dozens of PhD geneticists and biologists who have done this Herculean
[00:09:47.960 --> 00:09:54.240]   task of curating the medical genetic literature for any scrap of evidence that could inform
[00:09:54.240 --> 00:09:58.720]   whether this variant that seems to be breaking the function of a gene in a patient could
[00:09:58.720 --> 00:10:01.560]   actually be the causal factor that puts them at higher risk.
[00:10:01.560 --> 00:10:06.320]   Because we still don't know most of the variants that are analyzed in medical genetics.
[00:10:06.320 --> 00:10:08.640]   We're still uncertain what their eventual effect would be.
[00:10:08.640 --> 00:10:14.920]   That involves literature mining, all the most contemporary NLP methods for entity extraction,
[00:10:14.920 --> 00:10:19.040]   relationship modeling, linking to ontologies.
[00:10:19.040 --> 00:10:27.960]   We don't get into things like summarization because even the fanciest, most expensive
[00:10:27.960 --> 00:10:33.120]   model, it's not confident enough to write a medical report for someone.
[00:10:33.120 --> 00:10:37.160]   But the sort of language modeling that goes into something like GPT-3, we can use that
[00:10:37.160 --> 00:10:42.880]   for concept embeddings, for extraction, for classification, for recommendation engines.
[00:10:42.880 --> 00:10:48.240]   So we have a lot of that NLP work that a lot of the rest of the world thinks of.
[00:10:48.240 --> 00:10:52.640]   And then I've got a fair chunk of computer vision problems, whether they're things like
[00:10:52.640 --> 00:10:56.800]   document processing, computer vision, or they're computational biology problems.
[00:10:56.800 --> 00:11:03.040]   And about half of my team is devoted to more core research, like advancing future products,
[00:11:03.040 --> 00:11:05.160]   doing academic collaborations with folks.
[00:11:05.160 --> 00:11:11.080]   So they're really trying to struggle with that problem I stated before, which is geneticists
[00:11:11.080 --> 00:11:18.320]   traditionally focus on big diseases with big mutations, but there's a lot more subtle signal
[00:11:18.320 --> 00:11:22.440]   going on for almost everyone on the planet.
[00:11:22.440 --> 00:11:25.400]   And in a sense, it's a signal detection problem.
[00:11:25.400 --> 00:11:29.000]   And it's a high order of complexity.
[00:11:29.000 --> 00:11:32.360]   It's silly to think of it this way, but if you just imagine we have 25,000 genes working
[00:11:32.360 --> 00:11:34.600]   in combinations, right?
[00:11:34.600 --> 00:11:39.800]   How do you search a space of 25,000 factorial combinations, right?
[00:11:39.800 --> 00:11:44.800]   So the hope is that things that were completely intractable before by enumeration could be
[00:11:44.800 --> 00:11:47.040]   tractable by approximation.
[00:11:47.040 --> 00:11:52.800]   And so that's one of the great hopes for computational biology is that we can produce a search space
[00:11:52.800 --> 00:11:53.800]   with machine learning.
[00:11:53.800 --> 00:11:58.480]   And then, yeah, so we covered computational biology, knowledge and operations.
[00:11:58.480 --> 00:12:02.240]   That's a big breadth of stuff to worry about.
[00:12:02.240 --> 00:12:05.640]   And then on top of that, I think there are things like graph embeddings for heterogeneous
[00:12:05.640 --> 00:12:11.120]   networks where there's lots of reasons to believe that heterogeneous entities out in
[00:12:11.120 --> 00:12:15.880]   the literature shouldn't be just treated as like word tokens that you learn with a language
[00:12:15.880 --> 00:12:20.420]   model, but instead you can layer on causality and known relationships.
[00:12:20.420 --> 00:12:26.040]   Causality is this kind of fascinating field because if you really cared about Newtonian
[00:12:26.040 --> 00:12:31.040]   mechanics, then you probably don't need a neural network approximator to tell you how
[00:12:31.040 --> 00:12:34.160]   fast the ball is going to roll down the incline plane with a certain coefficient of friction
[00:12:34.160 --> 00:12:35.160]   and whatever, right?
[00:12:35.160 --> 00:12:38.120]   Because you can physically model it really accurately.
[00:12:38.120 --> 00:12:43.520]   And in biology, if you open a biology textbook, there are all these cartoons of this protein
[00:12:43.520 --> 00:12:47.960]   binds to this protein and they both bind to the DNA and then the RNA is made and whatever.
[00:12:47.960 --> 00:12:52.840]   They're not just cartoons that you memorize when you're a biology undergrad, they're actual
[00:12:52.840 --> 00:12:56.960]   physical models of a material process of the universe.
[00:12:56.960 --> 00:12:58.680]   But the uncertainty is way higher, right?
[00:12:58.680 --> 00:13:00.400]   They are rough drafts.
[00:13:00.400 --> 00:13:07.000]   And because it's a tiny little, some microscopic machines, historically, we don't just take
[00:13:07.000 --> 00:13:08.000]   the picture.
[00:13:08.000 --> 00:13:11.360]   I guess that's a lesson that's true because electron microscopy is now getting really
[00:13:11.360 --> 00:13:14.200]   good and X-ray crystallography in some ways is really good at that.
[00:13:14.200 --> 00:13:16.280]   But for the most part, you do it by inference, right?
[00:13:16.280 --> 00:13:19.960]   You do some experiment and the readout is like, you look at a different color of like
[00:13:19.960 --> 00:13:22.280]   jelly of agarose in a tray, right?
[00:13:22.280 --> 00:13:23.880]   And it's all by inference.
[00:13:23.880 --> 00:13:29.120]   So when you see one of those CSI TV shows and you're looking at the big bands of DNA,
[00:13:29.120 --> 00:13:35.060]   it's a very abstract version of the actual physical process.
[00:13:35.060 --> 00:13:41.600]   And that's where it's great for machine learning because there's enough structure to that cartoon
[00:13:41.600 --> 00:13:44.760]   that you don't have to imagine every possible force vector.
[00:13:44.760 --> 00:13:48.720]   You have some constraints, but it's uncertain enough that it's not Newtonian mechanics.
[00:13:48.720 --> 00:13:54.960]   So modeling it with uncertainty and then using those indirect observations to guide your
[00:13:54.960 --> 00:14:01.640]   search, in a lot of ways, it's a perfect field for using model-based machine learning.
[00:14:01.640 --> 00:14:02.640]   Well, okay.
[00:14:02.640 --> 00:14:07.400]   So I'm taking a mental note of all the different applications that you mentioned.
[00:14:07.400 --> 00:14:11.520]   I have so many questions on each one, but maybe we should start with the last one because
[00:14:11.520 --> 00:14:12.960]   it seems very intriguing, right?
[00:14:12.960 --> 00:14:22.640]   So why would a company like yours care about modeling the chemistry of molecules?
[00:14:22.640 --> 00:14:24.640]   What does that do for you?
[00:14:24.640 --> 00:14:25.640]   Yeah.
[00:14:25.640 --> 00:14:32.200]   So, I mean, we know that if you put a change in this DNA sequence, that there's a high
[00:14:32.200 --> 00:14:37.080]   likelihood that it's going to change what amino acid is put in the protein.
[00:14:37.080 --> 00:14:41.280]   Very predictably, we can predict that from basic molecular biology knowledge, but we
[00:14:41.280 --> 00:14:46.400]   don't necessarily know that's going to affect the function of the protein.
[00:14:46.400 --> 00:14:51.360]   And the easier ways historically to make computational estimates of that, where, you know, compare
[00:14:51.360 --> 00:14:58.080]   the sequence of that gene across a thousand related species or, you know, 10,000 humans
[00:14:58.080 --> 00:15:01.160]   and see like, is it always the same letter?
[00:15:01.160 --> 00:15:04.440]   Then it's probably important because if it's not important, then evolution will let it
[00:15:04.440 --> 00:15:05.440]   float around.
[00:15:05.440 --> 00:15:09.500]   But there's actually quite a lot of flexibility in those proteins where they're still functional.
[00:15:09.500 --> 00:15:12.560]   So there might be a subset of people where it's different and it doesn't actually matter.
[00:15:12.560 --> 00:15:17.440]   And if you're an actual biochemist, then you might go do experiments in the laboratory,
[00:15:17.440 --> 00:15:21.520]   like seeing how the proteins actually touch each other and discovering that the enzyme
[00:15:21.520 --> 00:15:23.560]   works better or doesn't work as well.
[00:15:23.560 --> 00:15:28.520]   And it's really expensive and time consuming to do that at that like slow process and scale.
[00:15:28.520 --> 00:15:35.880]   But if you had molecular models of those physical properties, then you could do in silico experiments
[00:15:35.880 --> 00:15:40.520]   and say like, well, I can't be sure that the enzyme is not going to be as efficient, but
[00:15:40.520 --> 00:15:41.520]   based on a whole lot of-
[00:15:41.520 --> 00:15:42.520]   You say in silico, even like in silicon?
[00:15:42.520 --> 00:15:43.520]   I don't know.
[00:15:43.520 --> 00:15:44.520]   That's a new term for me, but I love it.
[00:15:44.520 --> 00:15:45.520]   Yeah, yeah, yeah.
[00:15:45.520 --> 00:15:46.520]   Right.
[00:15:46.520 --> 00:15:47.520]   So, I mean, that's not me, right?
[00:15:47.520 --> 00:15:48.520]   That's a, you know, biology loves Latin.
[00:15:48.520 --> 00:15:49.520]   That's great.
[00:15:49.520 --> 00:15:50.520]   And yeah, so that's a well-tested phrase in computational biology for a long time.
[00:15:50.520 --> 00:15:51.520]   But yeah, that's the right answer.
[00:15:51.520 --> 00:15:52.520]   That's what it means.
[00:15:52.520 --> 00:16:05.180]   So you're doing the simulation and then you can say like, well, with some certainty based
[00:16:05.180 --> 00:16:09.280]   on the parameterization of those actual biochemical experiments that other people have done, this
[00:16:09.280 --> 00:16:13.080]   looks like a big change and therefore it's going to affect the function of the gene.
[00:16:13.080 --> 00:16:17.880]   And therefore we have more reason to believe in a very Bayesian sense.
[00:16:17.880 --> 00:16:21.840]   Our belief increases that this is the cause of someone's disease.
[00:16:21.840 --> 00:16:28.040]   And is this something that would be kind of something you'd really do in the future?
[00:16:28.040 --> 00:16:29.840]   Or is it like in use now?
[00:16:29.840 --> 00:16:34.800]   Is this like something that everyone would have to do to make a realistic model of...
[00:16:34.800 --> 00:16:41.720]   Like I guess how in use is this kind of modeling for deciding which genes to look at?
[00:16:41.720 --> 00:16:42.720]   This is something you do every day?
[00:16:42.720 --> 00:16:46.280]   I mean, this is definitely a thing that our company does.
[00:16:46.280 --> 00:16:48.280]   Like there's a team that does this.
[00:16:48.280 --> 00:16:52.840]   And I think it's also an interesting example where I think it's a case where industrial
[00:16:52.840 --> 00:17:00.760]   research has more potential than traditional academic research just because of the volume.
[00:17:00.760 --> 00:17:04.840]   The biggest academic collaborations for genome sequencing don't actually get to the same
[00:17:04.840 --> 00:17:10.000]   number of people as come through our samples.
[00:17:10.000 --> 00:17:12.160]   And they're not as enriched for people who actually have disease.
[00:17:12.160 --> 00:17:18.520]   Like the big population genome sequencing centers in China and the UK and in the US,
[00:17:18.520 --> 00:17:22.660]   they're not generally systematically going after people with disease.
[00:17:22.660 --> 00:17:27.760]   We have an ascertainment bias that's actually a benefit if we want to study disease because
[00:17:27.760 --> 00:17:31.280]   people with disease and their families come to the door.
[00:17:31.280 --> 00:17:35.560]   And that means that we can do stuff that you can't do if you're working at the Broad Institute
[00:17:35.560 --> 00:17:40.040]   at MIT and Harvard or at Cambridge with the European Bioinformatics Institute.
[00:17:40.040 --> 00:17:44.880]   We have access to data that you can use these methods on that no one else can.
[00:17:44.880 --> 00:17:47.560]   And how do you actually set up this problem?
[00:17:47.560 --> 00:17:48.560]   How do you formulate it?
[00:17:48.560 --> 00:17:49.840]   I'm trying to put this...
[00:17:49.840 --> 00:17:53.240]   My mind is just like, how would I set this up as a machine learning problem that I could
[00:17:53.240 --> 00:17:54.240]   actually train on?
[00:17:54.960 --> 00:17:57.080]   Is it standard what the loss function is?
[00:17:57.080 --> 00:18:01.680]   And what could you actually observe to put into this?
[00:18:01.680 --> 00:18:03.920]   Yeah, right.
[00:18:03.920 --> 00:18:07.640]   I don't think there's a canonically true answer to that question, but we can talk a little
[00:18:07.640 --> 00:18:10.560]   bit about the pros and cons of approaches.
[00:18:10.560 --> 00:18:15.920]   So one thing is it's not like a consumer recommender system where you recommend products and people
[00:18:15.920 --> 00:18:17.320]   click on them and buy them or they don't.
[00:18:17.320 --> 00:18:24.080]   In fact, that's diagnostics in general has this problem of no ground truth.
[00:18:24.080 --> 00:18:28.960]   People die of symptoms on hospital beds and their doctors don't actually know in some
[00:18:28.960 --> 00:18:35.880]   sort of like Plato, Aristotle sort of way, why they died.
[00:18:35.880 --> 00:18:38.920]   It's just that we have a stronger belief about the causality.
[00:18:38.920 --> 00:18:43.320]   So you can take a labeled dataset and say, these people were diagnosed and they had that
[00:18:43.320 --> 00:18:48.320]   variant and I'll make a model that can predict that outcome with a supervised learning method.
[00:18:48.320 --> 00:18:52.160]   But you're not actually dealing with ground truth because some of those people had the
[00:18:52.160 --> 00:18:55.960]   disease, they had the variant and they had the disease, but they actually had the disease
[00:18:55.960 --> 00:19:00.240]   because they smoked cigarettes for 50 years or because they were 90 years old or some
[00:19:00.240 --> 00:19:02.720]   other confounding factor was there.
[00:19:02.720 --> 00:19:08.680]   So if you want to try and think of it as belief, then you can go down the Bayesian probabilistic
[00:19:08.680 --> 00:19:16.900]   graphical model, causality, Judea pearl, explainable AI path that I think people are excited about
[00:19:16.900 --> 00:19:19.320]   talking about.
[00:19:19.320 --> 00:19:24.400]   But you have to know a lot of human knowledge goes into that.
[00:19:24.400 --> 00:19:28.360]   And it's not as simple as I have some labeled data and I'm going to train an arbitrarily
[00:19:28.360 --> 00:19:32.280]   deep neural network to approximate the softmax or something.
[00:19:32.280 --> 00:19:36.560]   So you end up working a lot with how do I take those physical models of what I think
[00:19:36.560 --> 00:19:42.840]   is going on in biology and you try and design the algorithm to do that, whether it's with
[00:19:42.840 --> 00:19:48.080]   causal graphical models or it's just knowing from these feature vectors, I can learn an
[00:19:48.080 --> 00:19:52.720]   auto-encoded representation that should in theory account for these factors we know from
[00:19:52.720 --> 00:19:54.680]   the physical model are important.
[00:19:54.680 --> 00:20:00.120]   And then I'm going to let the neural network set the weights by showing it what are the
[00:20:00.120 --> 00:20:04.240]   observations that are the closest to the ground truth that we'll ever have.
[00:20:04.240 --> 00:20:08.360]   It sounded like there's sort of sub problems here that people work on.
[00:20:08.360 --> 00:20:13.240]   You talked about people looking at proteins and mixing them together and kind of seeing
[00:20:13.240 --> 00:20:14.640]   what happens.
[00:20:14.640 --> 00:20:20.480]   Is that like a sub problem of this bigger problem where you could have different observations
[00:20:20.480 --> 00:20:22.480]   and build a different model around it?
[00:20:22.480 --> 00:20:23.480]   Yeah.
[00:20:23.480 --> 00:20:24.480]   Right.
[00:20:24.480 --> 00:20:28.280]   So I mean, we do have a wet lab team that is collecting basic molecular biology data.
[00:20:28.280 --> 00:20:32.560]   But one of the awesome things about biology is that lots of other people are doing that.
[00:20:32.560 --> 00:20:38.240]   So lots of professors and lots of universities and lots of their grad students are collecting
[00:20:38.240 --> 00:20:44.000]   and publishing data in a way that is ingestible for us to learn some of those things.
[00:20:44.000 --> 00:20:49.400]   But there are places where you may identify a key deficiency in that knowledge set.
[00:20:49.400 --> 00:20:52.960]   Like, well, it's worth it for us to do this experiment because it would really help us
[00:20:52.960 --> 00:20:56.360]   parameterize what we think is missing in this model.
[00:20:56.360 --> 00:21:00.920]   And so then you have from an industrial research perspective, you kind of have to think about
[00:21:00.920 --> 00:21:01.920]   the cost benefit.
[00:21:01.920 --> 00:21:08.800]   Like, is it worth spinning up a wet lab initiative to do stuff that's hard to do at scale to
[00:21:08.800 --> 00:21:10.240]   fill in that?
[00:21:10.240 --> 00:21:12.600]   You want that feature vector.
[00:21:12.600 --> 00:21:14.560]   It better be worth it because it's not cheap.
[00:21:14.560 --> 00:21:15.560]   Sure, sure.
[00:21:15.560 --> 00:21:21.000]   Although it sounds actually kind of, I don't know, it's evocatively similar to exploring
[00:21:21.000 --> 00:21:23.720]   a space of hyper parameters for-
[00:21:23.720 --> 00:21:30.000]   I mean, maybe it's more like if you had a product recommender and you knew everything
[00:21:30.000 --> 00:21:33.160]   that everyone had ever clicked on, but it just still doesn't seem to have that much
[00:21:33.160 --> 00:21:34.160]   accuracy.
[00:21:34.160 --> 00:21:37.680]   So you send out some design researchers to talk to your customers and sit in their house
[00:21:37.680 --> 00:21:38.680]   with them and talk to them.
[00:21:38.680 --> 00:21:39.680]   Oh, I see.
[00:21:39.680 --> 00:21:40.680]   I see.
[00:21:40.680 --> 00:21:41.680]   Like, "Oh, that's weird.
[00:21:41.680 --> 00:21:42.680]   I'm buying Adidas.
[00:21:42.680 --> 00:21:43.680]   I didn't notice that before.
[00:21:43.680 --> 00:21:47.400]   Is that in the model?
[00:21:47.400 --> 00:21:51.640]   Let's go find out all the shoes everyone buys and then see if that boosts the accuracy."
[00:21:51.640 --> 00:21:52.640]   Got it.
[00:21:52.640 --> 00:21:53.640]   That makes sense.
[00:21:53.640 --> 00:21:58.080]   But the thing is, if you had the whole life history of me as an individual and everything
[00:21:58.080 --> 00:22:02.360]   I'd ever done, then you might be able to start down the path of that modeling.
[00:22:02.360 --> 00:22:03.360]   But that's crazy.
[00:22:03.360 --> 00:22:04.360]   No one would do that.
[00:22:04.360 --> 00:22:08.360]   You just look at my ad click data and make some recommender that if it has 38% accuracy
[00:22:08.360 --> 00:22:11.480]   is going to make a bunch of money for a ad company.
[00:22:11.480 --> 00:22:14.960]   But if you're talking about someone's health and complex things like biology, then you
[00:22:14.960 --> 00:22:22.560]   want it to be higher accuracy and you got to go actually model stuff out deeper.
[00:22:22.560 --> 00:22:28.560]   So I guess another whole field that you talked about doing is sort of the, what do you call
[00:22:28.560 --> 00:22:34.480]   it, medical NLP or bioinformatics.
[00:22:34.480 --> 00:22:37.120]   And you talked about, I mean, this is one thing I've been curious about.
[00:22:37.120 --> 00:22:45.080]   You've seen a lot of progress, very visible progress in NLP, notably GPT-3, but also these
[00:22:45.080 --> 00:22:47.760]   word embeddings becoming super popular.
[00:22:47.760 --> 00:22:49.640]   Has that influenced bioinformatics?
[00:22:49.640 --> 00:22:51.320]   Does that directly apply?
[00:22:51.320 --> 00:22:56.200]   Can you fine tune these models on medical text domains?
[00:22:56.200 --> 00:22:57.200]   What's the state of the art there today?
[00:22:57.200 --> 00:22:58.200]   Yeah, right.
[00:22:58.200 --> 00:23:02.680]   So I mean, I think there's two big problems in the industry that people would love to
[00:23:02.680 --> 00:23:03.680]   solve.
[00:23:03.680 --> 00:23:07.440]   One is comprehending medical records and the other one is comprehending the medical
[00:23:07.440 --> 00:23:08.440]   literature.
[00:23:08.440 --> 00:23:12.320]   And when you state the problems, they sound the same, right?
[00:23:12.320 --> 00:23:16.040]   It's like I want to extract the entities, map their relationships, and then link them
[00:23:16.040 --> 00:23:20.680]   to ontologies so that I can structure the data and then make queries over it.
[00:23:20.680 --> 00:23:23.840]   And if you can do that, then, you know, like the challenge is the practical challenges
[00:23:23.840 --> 00:23:27.400]   are things like, can I show to one of our clinical scientists the right piece of literature
[00:23:27.400 --> 00:23:30.840]   at the right time to help them make the right insight about this genetic variant that's
[00:23:30.840 --> 00:23:33.800]   never been observed in someone before, right?
[00:23:33.800 --> 00:23:37.480]   And then if you look at the medical records, it's like, how do I take this allegedly structured,
[00:23:37.480 --> 00:23:43.040]   unstructured data and turn it into something that's actually structured so that we can
[00:23:43.040 --> 00:23:48.600]   make trajectories of people's disease progression or predict their risk?
[00:23:48.600 --> 00:23:53.520]   And so it turns out that training language models on Google Books and New York Times
[00:23:53.520 --> 00:23:57.080]   articles and Wikipedia does not actually help that much.
[00:23:57.080 --> 00:24:01.360]   But also kind of surprisingly, like several years ago, I did some experiments when I was
[00:24:01.360 --> 00:24:03.720]   still, I used to be at IBM Research where I had a research group.
[00:24:03.720 --> 00:24:08.320]   We did some experiments where we had domain specific corpuses and general corpuses.
[00:24:08.320 --> 00:24:13.520]   We would train the same models and kind of to my surprise, the bigger general corpus
[00:24:13.520 --> 00:24:16.480]   helped more than the specific corpus.
[00:24:16.480 --> 00:24:19.840]   And that was like an early transfer learning kind of insight.
[00:24:19.840 --> 00:24:24.640]   Like take the biggest corpus you can get and then transfer learn is a good idea.
[00:24:24.640 --> 00:24:31.720]   What's hard is the concepts are the same to humans, but when you look in a medical record,
[00:24:31.720 --> 00:24:39.080]   it says like MGM BRCA, and that means maternal grandmother had breast cancer.
[00:24:39.080 --> 00:24:43.840]   And you look in the medical literature that's published academically, it doesn't even talk
[00:24:43.840 --> 00:24:46.960]   about the relatives and it doesn't even say breast cancer.
[00:24:46.960 --> 00:24:51.120]   It says, you know, lignin neoplasm of the tissue.
[00:24:51.120 --> 00:24:54.080]   Like you don't even know it's talking about the same thing.
[00:24:54.080 --> 00:24:58.760]   So mapping the concepts across is tricky and just the syntax, right?
[00:24:58.760 --> 00:25:05.820]   Like the medical abbreviations and it's almost like it needs its own language model.
[00:25:05.820 --> 00:25:11.560]   So I mean, those are some of the hard problems for contemporary methods to actually work
[00:25:11.560 --> 00:25:13.520]   on, especially out of the box.
[00:25:13.520 --> 00:25:18.600]   But we do take things like, so we do take encoder decoder based sort of transformer
[00:25:18.600 --> 00:25:24.200]   models and adapt them pretty readily with supervised training.
[00:25:24.200 --> 00:25:27.920]   And it's definitely better than starting from scratch, but it still requires like, you know,
[00:25:27.920 --> 00:25:30.960]   domain experts labeling stuff to get there.
[00:25:30.960 --> 00:25:36.960]   Or it takes some like weak supervision data programming sort of methods where people are
[00:25:36.960 --> 00:25:40.320]   writing roles that make a lot of sense to weakly label the data.
[00:25:40.320 --> 00:25:44.760]   And you know, it's not as good as human labeled expert data, but that you can kind of bootstrap
[00:25:44.760 --> 00:25:48.120]   yourself into having a better data set to train on.
[00:25:48.120 --> 00:25:50.520]   So some of those methods work really well in biology.
[00:25:50.520 --> 00:25:51.520]   Yeah.
[00:25:51.520 --> 00:25:52.520]   I don't know if that ends that.
[00:25:52.520 --> 00:25:53.520]   No, that's really interesting.
[00:25:53.520 --> 00:25:55.200]   Do you have the sense that, well, I don't know.
[00:25:55.200 --> 00:25:59.680]   I have the sense that recently NLP methods have improved a lot.
[00:25:59.680 --> 00:26:04.560]   Like when I look at scores that I'm used to from like a decade or two ago, they just seem
[00:26:04.560 --> 00:26:07.000]   much better over the last couple of years.
[00:26:07.000 --> 00:26:11.800]   Has the same thing happened in the medical field?
[00:26:11.800 --> 00:26:12.800]   Kind of, right?
[00:26:12.800 --> 00:26:17.680]   Like if you take, you know, if you take like the scores on question, answer data sets,
[00:26:17.680 --> 00:26:21.280]   you know, like the model is better at answering Stanford question, answer questions than I
[00:26:21.280 --> 00:26:22.280]   am.
[00:26:22.280 --> 00:26:23.280]   Right.
[00:26:23.280 --> 00:26:24.280]   Super, very impressive.
[00:26:24.280 --> 00:26:25.280]   Right.
[00:26:25.280 --> 00:26:31.080]   Like, but, but I don't think you would expect the same thing to be true with medical question
[00:26:31.080 --> 00:26:37.000]   answer and a bunch of like specialists, doctors in whatever domain.
[00:26:37.000 --> 00:26:42.720]   So like no one expects a chat bot powered by a GPT-3 to be better at giving medical
[00:26:42.720 --> 00:26:48.600]   advice, but it doesn't mean, but that said, like the language model that's learned could
[00:26:48.600 --> 00:26:55.280]   be extremely useful for facilitating a human expert.
[00:26:55.280 --> 00:26:57.960]   And so I think that's where the hope is at this point.
[00:26:57.960 --> 00:27:05.600]   It's just kind of like AI assistant, you know, better information retrieval, better support
[00:27:05.600 --> 00:27:08.320]   for the expert is the current hope.
[00:27:08.320 --> 00:27:09.320]   Right.
[00:27:09.320 --> 00:27:10.320]   Got it.
[00:27:10.320 --> 00:27:14.880]   So I guess, you know, like a general problem that a lot of people ask me about, and I know
[00:27:14.880 --> 00:27:20.280]   a lot of people listening to this kind of wonder about is, how do you think about structuring
[00:27:20.280 --> 00:27:21.280]   your team?
[00:27:21.280 --> 00:27:25.040]   Like you talked about half the people doing kind of core research, but then also it seems
[00:27:25.040 --> 00:27:29.920]   like, you know, what you're doing is very connected to what the company is doing.
[00:27:29.920 --> 00:27:34.560]   Like do you try to like literally separate the people that are doing the sort of like
[00:27:34.560 --> 00:27:40.120]   applied stuff and research stuff, or do you separate it by the sort of field of work or
[00:27:40.120 --> 00:27:41.600]   how do you think about that?
[00:27:41.600 --> 00:27:44.360]   Yeah, it's a really good question.
[00:27:44.360 --> 00:27:48.800]   And I think, I suspect the answer that I have today will be different than the answer I
[00:27:48.800 --> 00:27:52.120]   have in a few years, which I know is different from the answer I had a few years ago.
[00:27:52.120 --> 00:27:56.280]   And it feels like one of those things that like we'll keep reinventing, like we'll keep
[00:27:56.280 --> 00:28:01.200]   reinventing, you know, how to deploy software and we'll keep reinventing how to provision
[00:28:01.200 --> 00:28:06.280]   infrastructure and we'll come back to the same basic principles that people thought
[00:28:06.280 --> 00:28:09.320]   of a few decades ago, but we'll keep refining it.
[00:28:09.320 --> 00:28:18.360]   And, you know, so right now, like the company, you know, the company was effectively, you
[00:28:18.360 --> 00:28:25.760]   know, the company still works like a startup with very clear product driven vertical teams.
[00:28:25.760 --> 00:28:31.200]   And the idea that we were going to imbue machine learning capabilities to the company, it's
[00:28:31.200 --> 00:28:34.120]   hard to figure that out.
[00:28:34.120 --> 00:28:39.560]   And it's a little different if you're at Google and like, well, you know, the company is built
[00:28:39.560 --> 00:28:42.040]   on machine learning based information retrieval.
[00:28:42.040 --> 00:28:45.240]   So we kind of expect everyone to take a machine learning approach to something.
[00:28:45.240 --> 00:28:49.080]   So, you know, I guess the direct answer is like, we have a team, we call it a functional
[00:28:49.080 --> 00:28:50.080]   team.
[00:28:50.080 --> 00:28:54.360]   Everyone goes to meetings together, hangs out together, checks in together, but people have
[00:28:54.360 --> 00:28:59.560]   different projects and, you know, it has definitely been hard for some of the team members, like
[00:28:59.560 --> 00:29:04.000]   a common source of feedback is like, I don't know what everyone else is doing because everyone
[00:29:04.000 --> 00:29:05.640]   is working on something else.
[00:29:05.640 --> 00:29:09.960]   I'm used to working with four people on a specific project and we talk every day in
[00:29:09.960 --> 00:29:15.200]   a standup meeting and in this team, like everyone's doing something different.
[00:29:15.200 --> 00:29:19.940]   And so, you know, for the people who are the people on the team who like went to grad school
[00:29:19.940 --> 00:29:25.680]   and experienced like what that's like to get a PhD where it's ultimately up to you to do
[00:29:25.680 --> 00:29:28.560]   your thing, they're more comfortable with it because they're like, yeah, of course we're
[00:29:28.560 --> 00:29:29.560]   all doing our own thing.
[00:29:29.560 --> 00:29:32.680]   In reality, like I really hope everyone's not doing their own thing, right?
[00:29:32.680 --> 00:29:36.680]   Like I hope that there is cross-fertilization and support and it's like kind of inherently
[00:29:36.680 --> 00:29:37.680]   matrixed.
[00:29:37.680 --> 00:29:44.040]   But the goal is, you know, we reserve some of the people's time for research because
[00:29:44.040 --> 00:29:48.960]   if you don't explicitly kind of set aside the commitment, then it'll be absorbed by
[00:29:48.960 --> 00:29:52.600]   whatever like demand of the product team in the short term.
[00:29:52.600 --> 00:29:57.200]   And then we set aside some people's time to develop platforms that are modular and reusable
[00:29:57.200 --> 00:30:01.240]   with the hopes that we continue to imbue that throughout the rest of the engineering teams.
[00:30:01.240 --> 00:30:05.360]   And then we set aside some people who are then like functionally assigned to specific
[00:30:05.360 --> 00:30:11.920]   engineering projects, whether it's to realize one of the research projects into production
[00:30:11.920 --> 00:30:18.360]   or it's to leverage one of the platforms for a problem, or maybe it's just like someone
[00:30:18.360 --> 00:30:21.720]   has a pretty straightforward problem and they need like a scikit-learn model and it's going
[00:30:21.720 --> 00:30:26.000]   to take someone like an afternoon to prototype it and three weeks to get it into production.
[00:30:26.000 --> 00:30:29.480]   So we stick someone in there for a sprint or two and make sure it happens.
[00:30:29.480 --> 00:30:34.520]   So I guess in some sense it's very zone defense kind of strategy.
[00:30:34.520 --> 00:30:36.560]   Like it has to be flexible.
[00:30:36.560 --> 00:30:38.440]   Right, right.
[00:30:38.440 --> 00:30:41.920]   And do you then hire people who have sort of like knowledge of like multiple topics?
[00:30:41.920 --> 00:30:45.440]   These seem like such kind of deep fields that are kind of different.
[00:30:45.440 --> 00:30:49.640]   Like is it possible to find someone that knows about multiple of these applications?
[00:30:49.640 --> 00:30:50.640]   Yeah.
[00:30:50.640 --> 00:30:53.160]   So we hire people with specific expertise for sure.
[00:30:53.160 --> 00:30:57.040]   I am actually just extremely fortunate.
[00:30:57.040 --> 00:31:01.680]   I was a software engineer who went to get a graduate degree in computational biology
[00:31:01.680 --> 00:31:07.240]   at a time when doing that probably also meant that you were going to do wet lab biology.
[00:31:07.240 --> 00:31:13.040]   And then I went and worked at IBM in a research division with just this huge diversity of
[00:31:13.040 --> 00:31:14.360]   industrial interests.
[00:31:14.360 --> 00:31:21.080]   So I was exposed to lots of different AI methods and that was not something that I
[00:31:21.080 --> 00:31:25.000]   knew was going to happen to me, but was really fortunate.
[00:31:25.000 --> 00:31:30.560]   And what that means is I met people in different industries at different conferences to understand
[00:31:30.560 --> 00:31:36.800]   like, oh, there's this kind of boutique thing that was kind of popular two decades ago,
[00:31:36.800 --> 00:31:40.720]   but continues to be a core technology for NASA or Toyota.
[00:31:40.720 --> 00:31:43.920]   And not a lot of people pay attention to it, but man, it can solve a lot of problems.
[00:31:43.920 --> 00:31:44.920]   Right.
[00:31:44.920 --> 00:31:47.040]   But you're just not going to find like a Coursera course on.
[00:31:47.040 --> 00:31:50.320]   So it's great because we can find people with that expertise.
[00:31:50.320 --> 00:31:57.040]   And if they're CS PhDs, kind of fortunately, right, so computer science PhDs are generally
[00:31:57.040 --> 00:31:58.800]   interested in stuff.
[00:31:58.800 --> 00:32:07.000]   Like if you practice NLP algorithms, you're probably still interested in computer vision.
[00:32:07.000 --> 00:32:09.880]   So I think that's a fortunate thing.
[00:32:09.880 --> 00:32:14.680]   I can find someone with the expertise in information retrieval and they can still make really meaningful
[00:32:14.680 --> 00:32:19.120]   contributions to other types of problems and other subject domains.
[00:32:19.120 --> 00:32:23.960]   One of the harder things is getting the biology knowledge solid enough that they can talk
[00:32:23.960 --> 00:32:28.720]   to the biologists and the other stakeholders and quickly understand the problem statement.
[00:32:28.720 --> 00:32:29.720]   That makes sense.
[00:32:29.720 --> 00:32:32.200]   And I guess like one of the things that you talk about a lot, I think is the importance
[00:32:32.200 --> 00:32:35.600]   of engineering to making all this stuff work.
[00:32:35.600 --> 00:32:39.920]   Do you hire just like kind of pure engineers on your team or do you rely on outside teams
[00:32:39.920 --> 00:32:40.920]   to provide that?
[00:32:40.920 --> 00:32:43.360]   Yeah, no, I mean, I think it's really important.
[00:32:43.360 --> 00:32:50.400]   On the research projects, it's really important to be able to prototype things because I hope
[00:32:50.400 --> 00:32:56.960]   your listeners find me eloquent, but my experience in life is I may have some beautiful complex
[00:32:56.960 --> 00:33:00.680]   system in my head and I have a very little ability to communicate it to other people's
[00:33:00.680 --> 00:33:04.480]   brains and building a prototype really helps.
[00:33:04.480 --> 00:33:08.520]   And you need a diversity of skills and even a small team to make that happen.
[00:33:08.520 --> 00:33:13.240]   It's just a waste of everyone's potential to ask the algorithms expert to write some
[00:33:13.240 --> 00:33:16.800]   React front end that you're going to throw away after you show it off to a stakeholder.
[00:33:16.800 --> 00:33:20.840]   So it's better to have a JavaScript programmer on hand for that.
[00:33:20.840 --> 00:33:23.040]   Do you have a ratio that you shoot for?
[00:33:23.040 --> 00:33:27.440]   I'm always kind of curious about this, sort of algorithms to implementers.
[00:33:27.440 --> 00:33:33.400]   I think it just depends, but we try to maintain a bench of depth so that we can recombine
[00:33:33.400 --> 00:33:34.680]   it.
[00:33:34.680 --> 00:33:42.040]   I think a lot of really high impact projects can be done with one algorithms person prototypes
[00:33:42.040 --> 00:33:47.120]   a thing, hands it off to one or two engineers who implement the thing.
[00:33:47.120 --> 00:33:52.320]   And then we further hand it off after it's implemented to an engineering team that's
[00:33:52.320 --> 00:33:56.080]   going to love and care for it in the long term and maybe come back to us if they need
[00:33:56.080 --> 00:33:57.080]   new features.
[00:33:57.080 --> 00:34:02.680]   But to them, it looks like software that could have come from anywhere.
[00:34:02.680 --> 00:34:09.320]   Other projects you need, we have some more challenging algorithmic problems where we
[00:34:09.320 --> 00:34:12.360]   like the approach of probabilistic programming.
[00:34:12.360 --> 00:34:17.840]   And there's not a lot of mature frameworks out there for that, like Google and Uber AI
[00:34:17.840 --> 00:34:24.840]   both popularize some, but you need some pretty heavy lifting on algorithm development, some
[00:34:24.840 --> 00:34:30.640]   kind of fearless backend engineering chops to make anything happen.
[00:34:30.640 --> 00:34:33.560]   And then once you have the ability to make anything happen, then you also want to layer
[00:34:33.560 --> 00:34:39.360]   in the computational biology expertise to make sure the right modeling steps that I
[00:34:39.360 --> 00:34:41.120]   described before is happening.
[00:34:41.120 --> 00:34:49.200]   So that could be a several person team just to make the prototype because it's complicated
[00:34:49.200 --> 00:34:54.160]   and the tooling requires help and it's not as simple as a web backend and a React frontend
[00:34:54.160 --> 00:34:55.160]   or something.
[00:34:55.160 --> 00:35:02.880]   One of the things that I've kind of been noticing, at my company, we've seen more and more interest
[00:35:02.880 --> 00:35:07.040]   in customers coming in from pharma and the kind of medical stuff.
[00:35:07.040 --> 00:35:12.600]   And it always feels to me like of all of our customers, it's the biggest kind of culture
[00:35:12.600 --> 00:35:13.600]   clash.
[00:35:13.600 --> 00:35:17.600]   Just basic stuff that I feel like I haven't discussed in a long time.
[00:35:17.600 --> 00:35:20.200]   They'll be suspicious of open source software.
[00:35:20.200 --> 00:35:23.800]   I'm just like, "Oh my God, what?"
[00:35:23.800 --> 00:35:30.440]   Since it's 1995, does that not happen at Invitae because it's sort of a newer company and more
[00:35:30.440 --> 00:35:34.360]   maybe CS focused or do you also kind of feel that kind of working with biologists?
[00:35:34.360 --> 00:35:37.360]   No, I don't think it's a problem here.
[00:35:37.360 --> 00:35:41.040]   And certainly I saw that problem with IBM customers at times.
[00:35:41.040 --> 00:35:46.760]   I was lucky at IBM, they were huge investors in Linux 20 years ago and it was clear to
[00:35:46.760 --> 00:35:50.280]   everyone why that continued to be the case.
[00:35:50.280 --> 00:35:55.520]   But I would see it from other companies who were like, "I would prefer the lower performance,
[00:35:55.520 --> 00:35:58.280]   more expensive proprietary thing.
[00:35:58.280 --> 00:35:59.280]   Thank you."
[00:35:59.280 --> 00:36:07.720]   I mean, I think one of the virtues of Invitae is it does have a Bay Area ethos and get there
[00:36:07.720 --> 00:36:12.880]   faster, get there cheaper is a good idea.
[00:36:12.880 --> 00:36:19.440]   So I don't think there's any skepticism there, but sometimes you collaborate with the insurance
[00:36:19.440 --> 00:36:27.520]   agencies or the insurance payers or Medicare and then you're into a whole ballpark of...
[00:36:27.520 --> 00:36:29.240]   It's not even individual skepticism.
[00:36:29.240 --> 00:36:34.840]   It's like, and it's not even institutional skepticism, it's codified in contracts, the
[00:36:34.840 --> 00:36:35.840]   skepticism.
[00:36:35.840 --> 00:36:36.840]   Right, right.
[00:36:36.840 --> 00:36:37.840]   Yeah.
[00:36:37.840 --> 00:36:38.840]   So I mean, for us, it's not a problem at all.
[00:36:38.840 --> 00:36:44.920]   I think I would imagine the bigger the company, the older the company.
[00:36:44.920 --> 00:36:47.920]   It's probably true in every sector, but a lot of the big old companies in technology
[00:36:47.920 --> 00:36:49.920]   got over it a long time ago.
[00:36:49.920 --> 00:36:50.920]   Right, right.
[00:36:50.920 --> 00:36:51.920]   Yeah, that makes sense.
[00:36:51.920 --> 00:36:54.120]   We always end with two questions and I want to make sure we have time for them.
[00:36:54.120 --> 00:36:57.160]   They're kind of broad and feel free to expand a little bit.
[00:36:57.160 --> 00:37:02.200]   But one thing we always ask people is when you look at what people are seeing or what
[00:37:02.200 --> 00:37:06.880]   people are doing in ML, what's a topic that you think people don't pay enough attention
[00:37:06.880 --> 00:37:07.880]   to?
[00:37:07.880 --> 00:37:10.800]   Maybe a skillset that you'd like to hire for, but nobody's studying or something that you'd
[00:37:10.800 --> 00:37:12.960]   like to spend more time on if you could.
[00:37:12.960 --> 00:37:13.960]   Yeah.
[00:37:13.960 --> 00:37:16.760]   I think it's just reasoning in general.
[00:37:16.760 --> 00:37:22.440]   And I think this happens, if you go to a general AI conference, whether it's one in recent
[00:37:22.440 --> 00:37:28.400]   favor like ICCML or NeurIPS, or it's like the older AAAI sort of standard conferences,
[00:37:28.400 --> 00:37:33.640]   lots of people, keynote speakers will talk about fast and slow AI or system one and system
[00:37:33.640 --> 00:37:35.120]   two or whatever.
[00:37:35.120 --> 00:37:39.600]   But I think no one ever actually wants to do reasoning because it's so hard.
[00:37:39.600 --> 00:37:40.600]   Right.
[00:37:40.600 --> 00:37:48.400]   And, but then you see communities of self-flagellating academics lamenting that they're only competing
[00:37:48.400 --> 00:37:53.640]   to get a higher F1 score on some published data set that's been around forever.
[00:37:53.640 --> 00:37:55.200]   And what's the actual use of it all?
[00:37:55.200 --> 00:37:59.160]   And I think this conversation is also often turned to, well, if we were doing some more
[00:37:59.160 --> 00:38:03.160]   complex reasoning thing, then it would be more valuable for mankind, but it's just hard.
[00:38:03.160 --> 00:38:04.160]   Right.
[00:38:04.160 --> 00:38:09.800]   So that's why I said earlier, we're into the probabilistic programming ideas because you
[00:38:09.800 --> 00:38:16.000]   can take a causal graphical model that can be highly explainable and you can not have
[00:38:16.000 --> 00:38:22.560]   to Monte Carlo sample it until the end of time, thanks to variational inference and
[00:38:22.560 --> 00:38:25.440]   frameworks like Edward and Pyro that make it easier.
[00:38:25.440 --> 00:38:30.760]   I think that's going to push our ability to reason about really complex things and bring
[00:38:30.760 --> 00:38:36.200]   human expertise in and let people help correct the models and do a lot of the things that
[00:38:36.200 --> 00:38:40.920]   we just frankly feel like we'll talk about doing, but are hard to do.
[00:38:40.920 --> 00:38:45.000]   I think there's also a bias against systems and academic conferences, right?
[00:38:45.000 --> 00:38:48.640]   Like no one wants to write a quote unquote systems paper in a workshop.
[00:38:48.640 --> 00:38:54.320]   They want to write an algorithms paper that's going to get cited 10,000 times, but that
[00:38:54.320 --> 00:38:55.840]   work is probably more important.
[00:38:55.840 --> 00:38:56.840]   Right.
[00:38:56.840 --> 00:39:01.080]   Like putting together a thing that solves a problem is really valuable.
[00:39:01.080 --> 00:39:08.320]   And I wish we trained grad students to think about that instead of to think about hyperparameter
[00:39:08.320 --> 00:39:09.320]   tuning effectively.
[00:39:09.320 --> 00:39:10.320]   Right.
[00:39:10.320 --> 00:39:15.360]   And if I can snap my fingers and change one thing about the field, I think that would
[00:39:15.360 --> 00:39:20.480]   be it is like pay attention to complicated systems because it'll help you build things
[00:39:20.480 --> 00:39:22.120]   like reasoning engines.
[00:39:22.120 --> 00:39:23.120]   It's really interesting.
[00:39:23.120 --> 00:39:27.480]   I guess I've not made a connection between reasoning engines and systems.
[00:39:27.480 --> 00:39:30.500]   Like those two both seem like kind of separate tracks.
[00:39:30.500 --> 00:39:35.280]   Is there something about making working systems in your experience that really requires reasoning?
[00:39:35.280 --> 00:39:36.280]   Well, it's good.
[00:39:36.280 --> 00:39:41.280]   I mean, so if you take an example like the word embeddings or graph embeddings, once
[00:39:41.280 --> 00:39:46.200]   you have the representation of similarity, you can rank documents and calculate a F1
[00:39:46.200 --> 00:39:51.900]   score, but you can also give it to an expert and say, "I found this thing for you.
[00:39:51.900 --> 00:39:53.520]   Do you think it's the right thing or not?"
[00:39:53.520 --> 00:39:57.680]   And if they say, "Yes," you can further process it and extract some more information out of
[00:39:57.680 --> 00:39:58.840]   it for a specific purpose.
[00:39:58.840 --> 00:40:03.680]   And if they say, "No," then you could ask them, "Why not?"
[00:40:03.680 --> 00:40:07.520]   And reason about the entities and relationships you extracted and actually auto refine your
[00:40:07.520 --> 00:40:10.100]   model from the feedback of the user.
[00:40:10.100 --> 00:40:12.320]   But that's an HCI problem.
[00:40:12.320 --> 00:40:16.560]   That's an interaction problem that you're not even going to start to touch unless you're
[00:40:16.560 --> 00:40:20.880]   open to the idea of building some boring system that ties together a user interface and some
[00:40:20.880 --> 00:40:23.800]   backend systems that are not all machine learning.
[00:40:23.800 --> 00:40:24.800]   Totally.
[00:40:25.760 --> 00:40:26.760]   Okay.
[00:40:26.760 --> 00:40:31.000]   So final question is basically when you look at in your career, kind of taking stuff from
[00:40:31.000 --> 00:40:35.320]   the sort of prototype version to like deployed in the real world and useful, where do you
[00:40:35.320 --> 00:40:41.080]   see the sort of biggest bottlenecks or biggest problems?
[00:40:41.080 --> 00:40:46.160]   I think the biggest fundamental problem is when you work in industry and you have an
[00:40:46.160 --> 00:40:51.320]   existing company, but probably also when you have a startup and you're trying to get funding
[00:40:51.320 --> 00:40:59.440]   for it to have buy-in from the product philosophy from the outset.
[00:40:59.440 --> 00:41:03.680]   And to have some willingness that the prototypes might not work.
[00:41:03.680 --> 00:41:12.720]   So you need a foundational, definitely go into work plan to make a product.
[00:41:12.720 --> 00:41:16.800]   But to have a, "I'm going to reserve 20% of the resources to try this crazier thing.
[00:41:16.800 --> 00:41:17.800]   We'll prototype it.
[00:41:17.800 --> 00:41:18.800]   And if it works, it'll be great."
[00:41:18.800 --> 00:41:23.600]   You've got to have the person who's going to take it to market care about that idea.
[00:41:23.600 --> 00:41:28.440]   When you have a bunch of researchers hanging out, making cool prototypes, and then they
[00:41:28.440 --> 00:41:32.280]   take it around like a toddler who made a thing and they're like, "Oh, look at this thing
[00:41:32.280 --> 00:41:33.280]   I made you.
[00:41:33.280 --> 00:41:34.280]   Don't you love me?"
[00:41:34.280 --> 00:41:40.720]   I mean, I think almost every researcher I've known in industry could identify with what
[00:41:40.720 --> 00:41:43.200]   I just said as the toddler.
[00:41:43.200 --> 00:41:45.800]   Because we all think we have some brilliant idea and we make a thing and we take it to
[00:41:45.800 --> 00:41:48.840]   people and they're like, "I'm sorry, I have a deadline right now.
[00:41:48.840 --> 00:41:51.080]   I don't understand why does this thing...
[00:41:51.080 --> 00:41:54.280]   I already have a thing that recommends papers.
[00:41:54.280 --> 00:41:56.680]   I think it uses a regular expression."
[00:41:56.680 --> 00:41:57.680]   Right?
[00:41:57.680 --> 00:42:00.560]   They don't care and they don't see the value.
[00:42:00.560 --> 00:42:05.600]   So you have to really get the buy-in at the beginning or you can spend a lot of time making
[00:42:05.600 --> 00:42:09.440]   a hard thing and probably an expensive thing happen.
[00:42:09.440 --> 00:42:11.840]   And then it doesn't actually go anywhere.
[00:42:11.840 --> 00:42:14.720]   And it's more emotional than strategic.
[00:42:14.720 --> 00:42:21.360]   You have to be open to the idea that they might not see the value in what you want to
[00:42:21.360 --> 00:42:22.360]   do.
[00:42:22.360 --> 00:42:26.240]   And that helps you prioritize what to do.
[00:42:26.240 --> 00:42:27.240]   Interesting.
[00:42:27.240 --> 00:42:29.800]   We've not heard that answer yet, but that really resonates.
[00:42:29.800 --> 00:42:32.880]   That makes a lot of sense.
[00:42:32.880 --> 00:42:33.880]   Thank you so much.
[00:42:33.880 --> 00:42:34.880]   This is a lot of fun.
[00:42:34.880 --> 00:42:35.880]   I really appreciate your openness.
[00:42:35.880 --> 00:42:36.880]   You're welcome.
[00:42:36.880 --> 00:42:37.880]   My pleasure.
[00:42:37.880 --> 00:42:39.360]   I really appreciate it.
[00:42:39.360 --> 00:42:42.600]   Thanks for listening to another episode of Gradient Dissent.
[00:42:42.600 --> 00:42:46.880]   Doing these interviews are a lot of fun and it's especially fun for me when I can actually
[00:42:46.880 --> 00:42:49.640]   hear from the people that are listening to these episodes.
[00:42:49.640 --> 00:42:53.720]   So if you wouldn't mind leaving a comment and telling me what you think or starting
[00:42:53.720 --> 00:42:57.660]   a conversation, that would make me inspired to do more of these episodes.
[00:42:57.660 --> 00:43:01.200]   And also if you wouldn't mind liking and subscribing, I'd appreciate that a lot.

