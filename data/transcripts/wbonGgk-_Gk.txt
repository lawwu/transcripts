
[00:00:00.000 --> 00:00:07.360]   I think a big mistake of research specifically in the area of computational creativities
[00:00:07.360 --> 00:00:09.840]   is that you can automate it entirely.
[00:00:09.840 --> 00:00:13.160]   So you see one-click-off solutions to do X, Y, or Z.
[00:00:13.160 --> 00:00:17.200]   I think that's missing the bigger picture of how most creative works should actually
[00:00:17.200 --> 00:00:18.200]   work.
[00:00:18.200 --> 00:00:21.800]   Or that probably means that you've never actually worked with an agency where the client was
[00:00:21.800 --> 00:00:26.920]   asking you to change things every single hour, make it bigger, make it smaller, right?
[00:00:26.920 --> 00:00:31.280]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:31.280 --> 00:00:34.160]   and I'm your host, Lukas Biewald.
[00:00:34.160 --> 00:00:41.000]   Chris Valenzuela is an artist and technologist and entrepreneur and CEO and founder of a
[00:00:41.000 --> 00:00:48.120]   company called Runway, which is a maker of ML-powered video editing software.
[00:00:48.120 --> 00:00:52.920]   But I feel like that description doesn't even do justice to how incredible and innovative
[00:00:52.920 --> 00:00:54.760]   his product is.
[00:00:54.760 --> 00:01:00.360]   This interview actually starts off with a live demo of his product, and I really recommend
[00:01:00.360 --> 00:01:05.160]   switching to video if you're listening to this on audio only because his demo is absolutely
[00:01:05.160 --> 00:01:06.160]   incredible.
[00:01:06.160 --> 00:01:11.240]   Well, all right, Chris, we don't normally do this, but I thought it would be fun to
[00:01:11.240 --> 00:01:13.800]   start with a product demo if you're down for it.
[00:01:13.800 --> 00:01:16.520]   You have such a cool, compelling product.
[00:01:16.520 --> 00:01:18.200]   Would you be up for that?
[00:01:18.200 --> 00:01:19.200]   Sure.
[00:01:19.200 --> 00:01:21.360]   What do you want me to demo?
[00:01:21.360 --> 00:01:22.360]   There's a lot I can demo.
[00:01:22.360 --> 00:01:25.640]   I just want to make sure I can focus on what you want to see.
[00:01:25.640 --> 00:01:35.280]   Well, this is an ML podcast, so I think people would probably be interested in the most flashy
[00:01:35.280 --> 00:01:36.280]   ML features.
[00:01:36.280 --> 00:01:38.040]   How about that?
[00:01:38.040 --> 00:01:39.040]   Okay.
[00:01:39.040 --> 00:01:45.280]   So, in short, Runway is a full video creation suite.
[00:01:45.280 --> 00:01:49.160]   It allows you to do things that you might be able to do in more traditional video editing
[00:01:49.160 --> 00:01:50.440]   software.
[00:01:50.440 --> 00:01:54.360]   The main difference is that everything that runs behind the scenes, so most of the core
[00:01:54.360 --> 00:01:57.480]   components of Runway, are ML-driven.
[00:01:57.480 --> 00:02:04.440]   And the reason for that, it has two main modes or uniqueness about making everything ML-based.
[00:02:04.440 --> 00:02:11.280]   One is it helps editors and content creators and video makers automate and simplify really
[00:02:11.280 --> 00:02:16.200]   time-consuming and expensive processes when making video or content.
[00:02:16.200 --> 00:02:20.920]   There are stuff that you're doing in traditional software that are very repetitive in nature,
[00:02:20.920 --> 00:02:23.160]   that are very time-consuming and expensive.
[00:02:23.160 --> 00:02:29.240]   So Runway aims basically to simplify and reduce the time of doing this stuff.
[00:02:29.240 --> 00:02:33.360]   If you have a video you want to edit, an idea you want to execute, spending the time and
[00:02:33.360 --> 00:02:38.120]   the minutes and the hours and sometimes days on this very boring stuff is not the thing
[00:02:38.120 --> 00:02:40.320]   that you really want to do.
[00:02:40.320 --> 00:02:47.280]   And so we build algorithms and systems that help you just do that in a very easy way.
[00:02:47.280 --> 00:02:51.040]   And then there's another aspect of Runway that it's not only about automation, but it's
[00:02:51.040 --> 00:02:52.040]   about generation.
[00:02:52.040 --> 00:02:58.400]   And so we build models and algorithms and systems that allow our users and customers
[00:02:58.400 --> 00:03:01.480]   to create content on demand.
[00:03:01.480 --> 00:03:06.120]   And everything, I guess, a baseline for us is that everything happens on the browser.
[00:03:06.120 --> 00:03:11.400]   And so it's web-based and cloud native, which means that you don't rely anymore on native
[00:03:11.400 --> 00:03:16.200]   computers or native applications or desktop compute.
[00:03:16.200 --> 00:03:21.740]   You have access to our GPU cluster on demand and you can render videos on 4K, 6K, pretty
[00:03:21.740 --> 00:03:23.320]   much in real time.
[00:03:23.320 --> 00:03:27.160]   Plus you can do all of this AI stuff also in real time as well.
[00:03:27.160 --> 00:03:32.960]   And so a lot of the folks are using Runway now like CDS, like the late night show with
[00:03:32.960 --> 00:03:39.920]   Colbert or the folks who edit Top Gear or sometimes like creators who do stuff for Alicia
[00:03:39.920 --> 00:03:42.960]   Keys or for just TikTok or movies.
[00:03:42.960 --> 00:03:47.280]   They're all leveraging this AI kind of like things behind the scene via this web-based,
[00:03:47.280 --> 00:03:50.200]   cloud-based editor.
[00:03:50.200 --> 00:03:54.680]   So I'd say I guess in a short, I guess five minute intro, what the product does and how
[00:03:54.680 --> 00:04:00.560]   ML or AI plays a role in the product itself.
[00:04:00.560 --> 00:04:03.680]   But I'm happy to kind of like now show you how everything kind of like goes together
[00:04:03.680 --> 00:04:06.760]   and like the experience of using it, if that makes sense.
[00:04:06.760 --> 00:04:08.320]   Please, yeah.
[00:04:08.320 --> 00:04:09.320]   Cool.
[00:04:09.320 --> 00:04:12.880]   Any questions, anything I guess before we do that that you want to, I can double down
[00:04:12.880 --> 00:04:15.200]   or if you want me to clarify.
[00:04:15.200 --> 00:04:25.920]   Well, I actually didn't realize that professional video teams like Colbert uses Runway.
[00:04:25.920 --> 00:04:30.600]   Is that, do they use it for all of their like video processing or is there like a certain
[00:04:30.600 --> 00:04:32.640]   part where they use it?
[00:04:32.640 --> 00:04:34.840]   How does that work?
[00:04:34.840 --> 00:04:35.840]   It depends.
[00:04:35.840 --> 00:04:39.940]   Some editors and some folks are using it like as an end-to-end tool to create videos.
[00:04:39.940 --> 00:04:45.440]   Some other folks use a combination of different softwares to make something, right?
[00:04:45.440 --> 00:04:51.840]   And so the folks who use it for movies, sometimes editing like Nuke or Flame, we have a big
[00:04:51.840 --> 00:04:52.840]   Flame community.
[00:04:52.840 --> 00:04:55.800]   And so Runway becomes a part of their workflow.
[00:04:55.800 --> 00:05:00.440]   It's replacing either something you're doing on a very manual basis.
[00:05:00.440 --> 00:05:04.880]   It's sometimes replacing a contractor you hired to make that work for you or it's something
[00:05:04.880 --> 00:05:09.640]   replacing your own work of trying to do it yourself in this like old software.
[00:05:09.640 --> 00:05:14.040]   And then, but you still use other aspects of it or other software to combine it.
[00:05:14.040 --> 00:05:16.960]   It really depends on the type of content that you have and the level of like outcomes that
[00:05:16.960 --> 00:05:18.380]   you need.
[00:05:18.380 --> 00:05:22.720]   But we do have folks that use it as an end-to-end content creation and editing tool.
[00:05:22.720 --> 00:05:23.720]   Cool.
[00:05:23.720 --> 00:05:28.520]   Well, I mean, the extent of my video editing is basically like modifying videos of my daughter
[00:05:28.520 --> 00:05:31.480]   to like take out the boring parts and send them to my parents.
[00:05:31.480 --> 00:05:33.480]   So that's like as far as I go.
[00:05:33.480 --> 00:05:36.960]   Maybe you could sort of give me like a little bit of an overview of the cool stuff you can
[00:05:36.960 --> 00:05:37.960]   do with Runway.
[00:05:37.960 --> 00:05:38.960]   Totally.
[00:05:38.960 --> 00:05:40.640]   You can do all of that in Runway.
[00:05:40.640 --> 00:05:45.600]   I need a browser, which is, you might start using Runway for that.
[00:05:45.600 --> 00:05:50.280]   And yeah, the one thing I would, I guess, emphasize is like everything is running on
[00:05:50.280 --> 00:05:51.280]   the cloud, on the web.
[00:05:51.280 --> 00:05:54.040]   So you can just open any project with a URL.
[00:05:54.040 --> 00:05:57.600]   You can also create teams and you have all this like baseline collaboration aspect that
[00:05:57.600 --> 00:05:59.600]   just like runs out of the box.
[00:05:59.600 --> 00:06:00.600]   Cool.
[00:06:00.600 --> 00:06:01.600]   Anything else?
[00:06:01.600 --> 00:06:02.600]   No?
[00:06:02.600 --> 00:06:03.600]   Just go demo?
[00:06:03.600 --> 00:06:04.600]   Yeah, let's see a demo.
[00:06:04.600 --> 00:06:05.600]   Totally.
[00:06:05.600 --> 00:06:06.600]   Yeah.
[00:06:06.600 --> 00:06:07.600]   Some of the cool stuff.
[00:06:07.600 --> 00:06:08.600]   Perfect.
[00:06:08.600 --> 00:06:11.200]   So this is what Runway looks like.
[00:06:11.200 --> 00:06:17.560]   If you ever edited video before, it's a very kind of like common interface.
[00:06:17.560 --> 00:06:19.120]   We have like tracks on the bottom.
[00:06:19.120 --> 00:06:27.360]   You have a multi editing system with audio tracks and key frame animations and text layers
[00:06:27.360 --> 00:06:29.560]   and image support.
[00:06:29.560 --> 00:06:33.320]   You can preview your assets on the main window and have a bunch of effects and filters on
[00:06:33.320 --> 00:06:34.320]   the right.
[00:06:34.320 --> 00:06:37.920]   Again, everything running pretty much on the cloud in real time.
[00:06:37.920 --> 00:06:42.360]   And the idea here is that there are a lot of things that you can do that are very similar
[00:06:42.360 --> 00:06:44.800]   to stuff that you can do in other applications.
[00:06:44.800 --> 00:06:47.000]   Plus there are things that you can do anywhere else.
[00:06:47.000 --> 00:06:51.160]   So let me give you an example of something that a lot of folks are using Runway for.
[00:06:51.160 --> 00:06:53.400]   I'm going to start with a fresh composition here.
[00:06:53.400 --> 00:06:56.000]   I'm going to click one of the demo assets that I have here.
[00:06:56.000 --> 00:06:57.600]   I'm going to click this.
[00:06:57.600 --> 00:07:00.360]   So I have a surfer right on that shot.
[00:07:00.360 --> 00:07:05.880]   Let's say I want to apply some sort of effect or transformation to the background of this
[00:07:05.880 --> 00:07:10.640]   shot or I want to maybe replace the person here and take it somewhere else.
[00:07:10.640 --> 00:07:15.680]   The way we would do that today would be a combination of like frame by frame editing
[00:07:15.680 --> 00:07:20.800]   where you're basically segmenting and creating an outline of your subject and every single
[00:07:20.800 --> 00:07:23.960]   frame you move, you have to do it one more time.
[00:07:23.960 --> 00:07:30.680]   And so for that, we build our video object segmentation model, which we actually publish
[00:07:30.680 --> 00:07:38.200]   a blog post and a paper around it that allows you to do real time video segmentation.
[00:07:38.200 --> 00:07:41.000]   In film, this is actually called rotoscoping.
[00:07:41.000 --> 00:07:46.040]   And you can just literally go here, guide the model with some sort of input reference.
[00:07:46.040 --> 00:07:51.480]   So I tell the model like this is what I want to rotoscope and it can go as deep as I need.
[00:07:51.480 --> 00:07:56.680]   I can select the whole surf layer here at deeper, more control over it.
[00:07:56.680 --> 00:08:00.720]   And then once the model has a good understanding of what you want to do, it will propagate
[00:08:00.720 --> 00:08:05.360]   that single kind of like key frame or a single layer to all the frames of video right in
[00:08:05.360 --> 00:08:06.400]   real time.
[00:08:06.400 --> 00:08:12.280]   And so you get a pretty smooth, consistent segmentation mask that you can either export
[00:08:12.280 --> 00:08:19.000]   as a single layer or export as a PNG layer, or you can use back in your go back to your
[00:08:19.000 --> 00:08:22.240]   editing kind of like timeline and start modifying.
[00:08:22.240 --> 00:08:25.920]   So you said like you want to cut it, you want to compose it, you want to do some sort of
[00:08:25.920 --> 00:08:29.320]   like transformation from here, you can do that directly from here.
[00:08:29.320 --> 00:08:35.360]   So let's say I have my baseline or my base video here, I have my mask on top of that.
[00:08:35.360 --> 00:08:39.480]   And now I can just like literally like move it around like this.
[00:08:39.480 --> 00:08:46.080]   Right, I can just, I have two layers right, the surfer.
[00:08:46.080 --> 00:08:51.040]   So something that looks very simple and like traditional software may take you like a couple
[00:08:51.040 --> 00:08:52.040]   of hours of work.
[00:08:52.040 --> 00:08:54.360]   Here, you can do pretty much on real time.
[00:08:54.360 --> 00:08:59.520]   And again, it's something that most editors know how to do, but just takes them a lot
[00:08:59.520 --> 00:09:01.720]   of time to actually do.
[00:09:01.720 --> 00:09:04.080]   And did you just run that in the browser?
[00:09:04.080 --> 00:09:09.840]   Yeah, that segmentation mask, it figured out in the browser and it's calculating all it
[00:09:09.840 --> 00:09:10.840]   doesn't go to the server.
[00:09:10.840 --> 00:09:12.760]   No, it goes to the server.
[00:09:12.760 --> 00:09:18.080]   Yeah, there's a there's a an inference pipeline that we build that processes like real time
[00:09:18.080 --> 00:09:20.680]   videos and allows you to do those things.
[00:09:20.680 --> 00:09:23.080]   The compute part is everything running on the cloud.
[00:09:23.080 --> 00:09:26.400]   You just see like the previews and sometimes depending on your connection, you can see
[00:09:26.400 --> 00:09:27.600]   a downsample version of it.
[00:09:27.600 --> 00:09:31.120]   So it runs really smoothly and plays really, really nicely.
[00:09:31.120 --> 00:09:37.200]   Also for every single video, there's a few layers that we run that help either guide
[00:09:37.200 --> 00:09:38.680]   something like a segmentation mask.
[00:09:38.680 --> 00:09:44.080]   For instance, we get depth maps and we estimate depth maps for every single video layer.
[00:09:44.080 --> 00:09:49.720]   You can also export this depth maps as independent layers and use them for specific workflows.
[00:09:49.720 --> 00:09:52.320]   So that's also something very useful for folks to leverage.
[00:09:52.320 --> 00:09:56.560]   Yeah, so you get you have this right and you can export this.
[00:09:56.560 --> 00:09:59.760]   Behind the scenes, we're using this for a bunch of things.
[00:09:59.760 --> 00:10:01.800]   Cool.
[00:10:01.800 --> 00:10:04.760]   So yeah, those are like one of the it's one of the things that you can do.
[00:10:04.760 --> 00:10:07.520]   You can do you can go very complex and stuff.
[00:10:07.520 --> 00:10:13.280]   Let's say instead of the server, I just want like the let me refresh this.
[00:10:13.280 --> 00:10:16.720]   I just want the the background.
[00:10:16.720 --> 00:10:19.080]   I don't want the the server there.
[00:10:19.080 --> 00:10:22.720]   I can like in paint or remove that server from the shot.
[00:10:22.720 --> 00:10:25.400]   So I'm just going to like paint over it.
[00:10:25.400 --> 00:10:30.960]   And again, I'm giving the model like one single keyframe layer.
[00:10:30.960 --> 00:10:36.600]   And the model is able to propagate those consistently for the entirety of the video.
[00:10:36.600 --> 00:10:42.040]   And that's also something we I guess as a product philosophy, we really want to think
[00:10:42.040 --> 00:10:46.000]   about, which is like you need to have some layer of control, right?
[00:10:46.000 --> 00:10:47.960]   Of like input.
[00:10:47.960 --> 00:10:52.560]   And the hard part of that should also should just be handled by the model itself.
[00:10:52.560 --> 00:10:55.960]   And there's always some level of like human in the loop process where you're guiding the
[00:10:55.960 --> 00:11:00.800]   model, you're telling it like, hey, this is this is what I want to remove.
[00:11:00.800 --> 00:11:04.040]   Just go ahead and do the hard work actually doing that for the whole video sequence.
[00:11:04.040 --> 00:11:05.480]   Wow, that's really amazing.
[00:11:05.480 --> 00:11:07.400]   That's like magic right there.
[00:11:07.400 --> 00:11:11.240]   Yeah, that's actually something that's gone.
[00:11:11.240 --> 00:11:15.400]   That's something we've been we see a lot when people find out about when we start using
[00:11:15.400 --> 00:11:16.400]   it.
[00:11:16.400 --> 00:11:18.120]   Magic is a word we hear a lot.
[00:11:18.120 --> 00:11:22.560]   It's something that again, if you're editing or working film or content before, you know
[00:11:22.560 --> 00:11:25.960]   how hard and time consuming, just painful it is.
[00:11:25.960 --> 00:11:30.760]   And just seeing it work so instantaneously, like really, really triggers that EV over
[00:11:30.760 --> 00:11:36.360]   or for like magic or in everyone's kind of like minds, which is something that's great
[00:11:36.360 --> 00:11:41.840]   because we really thought of the project as something very magical for to use.
[00:11:41.840 --> 00:11:47.440]   So there's stuff like that, then there's a few things like green screen and in painting,
[00:11:47.440 --> 00:11:52.640]   which I'm showing you now, plus motion tracking that we consider you can see your like baseline
[00:11:52.640 --> 00:11:54.040]   models in a runway.
[00:11:54.040 --> 00:11:59.200]   And so those are just you can use them as unique tools, as I'm showing you right now.
[00:11:59.200 --> 00:12:04.120]   You can also combine them to create all sort of like interesting workflows and dynamics.
[00:12:04.120 --> 00:12:10.160]   And so there's the idea of like, I don't know, you want to transform or generate this this
[00:12:10.160 --> 00:12:13.400]   video and take this surfer into another location.
[00:12:13.400 --> 00:12:17.160]   You can actually generate the background and have the camera track the position of the
[00:12:17.160 --> 00:12:22.720]   object in real time and then apply the background that you just generated in a consistent manner.
[00:12:22.720 --> 00:12:24.560]   So everything looks really smooth.
[00:12:24.560 --> 00:12:28.680]   And the way you do that is by combining kind of like all of these models in real time behind
[00:12:28.680 --> 00:12:29.680]   the scenes.
[00:12:29.680 --> 00:12:35.400]   And so you might have seen like some of those demos in Twitter, which we've been kind of
[00:12:35.400 --> 00:12:38.520]   like announcing and releasing.
[00:12:38.520 --> 00:12:45.280]   So this is a demo of like running a few of those underlying models combined, right?
[00:12:45.280 --> 00:12:52.600]   So there's a segmentation model that's rotoscoping the tiniest player in real time.
[00:12:52.600 --> 00:12:56.320]   There's a motion tracking model that's tracking the camera movement.
[00:12:56.320 --> 00:13:03.960]   And then there's an image generation model behind the scenes that is generating the image
[00:13:03.960 --> 00:13:04.960]   on real time.
[00:13:04.960 --> 00:13:05.960]   Right.
[00:13:05.960 --> 00:13:07.160]   And those are all composed at the same time.
[00:13:07.160 --> 00:13:08.600]   Does that make sense?
[00:13:08.600 --> 00:13:10.400]   Yeah, yeah, totally.
[00:13:10.400 --> 00:13:13.560]   So yeah, those are, I would say, like underlying baseline models.
[00:13:13.560 --> 00:13:16.600]   And then you can combine them in all sorts of like interesting and different ways.
[00:13:16.600 --> 00:13:17.600]   All right.
[00:13:17.600 --> 00:13:18.600]   Well, thanks for that demo.
[00:13:18.600 --> 00:13:19.600]   So cool.
[00:13:19.600 --> 00:13:20.600]   We'll switch to the interview format.
[00:13:20.600 --> 00:13:25.880]   Although I now I really want to like modify this video in all kinds of crazy ways.
[00:13:25.880 --> 00:13:31.040]   Yeah, we should replace the background with some stuff while we're talking.
[00:13:31.040 --> 00:13:32.040]   Totally.
[00:13:32.040 --> 00:13:33.040]   Get this microphone out.
[00:13:33.040 --> 00:13:40.360]   I guess, you know, one question I really wanted to ask you is, I think your background is
[00:13:40.360 --> 00:13:43.200]   actually not in machine learning originally, right?
[00:13:43.200 --> 00:13:47.160]   And, you know, I always think it's really interesting how people enter the machine learning
[00:13:47.160 --> 00:13:48.160]   space.
[00:13:48.160 --> 00:13:52.680]   I'd just love to hear your story a little bit of how you ended up running this super
[00:13:52.680 --> 00:13:54.600]   cool machine learning company.
[00:13:54.600 --> 00:13:59.600]   And it seems like you're very technically deep also in sort of how you managed to kind
[00:13:59.600 --> 00:14:02.440]   of get that depth mid career.
[00:14:02.440 --> 00:14:03.520]   Totally.
[00:14:03.520 --> 00:14:09.960]   So I guess long story short, I'm originally from Chile and I studied like econ in Chile
[00:14:09.960 --> 00:14:14.040]   and I was working on like something completely unrelated.
[00:14:14.040 --> 00:14:20.120]   But it was like 2016 and 2017 I think and I just randomly fall into like a rabbit hole
[00:14:20.120 --> 00:14:23.600]   of like ML and AI generated like art.
[00:14:23.600 --> 00:14:28.840]   It was like very early days of like Deep Dream and like Cognets and AlexNet where like people
[00:14:28.840 --> 00:14:34.480]   were trying to make sense of how to use this new stuff in the context of art making.
[00:14:34.480 --> 00:14:38.600]   There were some people like Mike Tyka and Mario Klingerman and like Gene Kogan were
[00:14:38.600 --> 00:14:43.360]   just like posting these very mind blowing demos that now feel like things that you can
[00:14:43.360 --> 00:14:45.560]   run on your iPhone or like real time.
[00:14:45.560 --> 00:14:48.520]   But around that time was like someone was I remember like Carl McDonald, which is an
[00:14:48.520 --> 00:14:53.360]   artist who was like walking around with his laptop, just showing people like a live stream
[00:14:53.360 --> 00:14:54.360]   of a camera.
[00:14:54.360 --> 00:14:58.880]   And you have basically like a I think it was like an ImageNet model running on real time
[00:14:58.880 --> 00:15:01.640]   and just describing what it saw.
[00:15:01.640 --> 00:15:02.640]   And it just blew my mind.
[00:15:02.640 --> 00:15:04.380]   It's like again, it's like 2016.
[00:15:04.380 --> 00:15:11.340]   Now it's like pretty like obvious, but around that time was like pretty, I don't know, special.
[00:15:11.340 --> 00:15:16.080]   And it just went into a rabbit hole of that for like too long and it was too much.
[00:15:16.080 --> 00:15:17.400]   I was just like fascinated by it.
[00:15:17.400 --> 00:15:20.040]   So I actually decided to like quit my job.
[00:15:20.040 --> 00:15:22.600]   I decided to like leave everything I had.
[00:15:22.600 --> 00:15:29.920]   I got a scholarship to study at NYU and just spend like two years just really going very
[00:15:29.920 --> 00:15:31.840]   deep into this.
[00:15:31.840 --> 00:15:35.800]   Especially in the context of, I would say, kind of like creativity.
[00:15:35.800 --> 00:15:40.240]   I guess my area of interest was the idea of like computational creativity.
[00:15:40.240 --> 00:15:44.800]   How do you use technology and how do you use like deep learning or ML for really creative
[00:15:44.800 --> 00:15:49.600]   kind of like tool making and art making.
[00:15:49.600 --> 00:15:56.920]   And that two year long research process and kind of like exploration ended up with Runway.
[00:15:56.920 --> 00:15:58.680]   That Runway was my thesis at school.
[00:15:58.680 --> 00:16:02.120]   It was a very different version of what you see now.
[00:16:02.120 --> 00:16:04.400]   But the main idea was very much pretty much the same.
[00:16:04.400 --> 00:16:09.200]   It's like, Harry, there's ML and AI are basically a new compute platform.
[00:16:09.200 --> 00:16:13.680]   They offer new ways of either manipulating or creating content.
[00:16:13.680 --> 00:16:18.440]   And so there needs to be some sort of like new tool making suite that leverages all of
[00:16:18.440 --> 00:16:23.200]   this and allows people to tap into those kind of like systems in a very accessible and easy
[00:16:23.200 --> 00:16:24.200]   way.
[00:16:24.200 --> 00:16:29.400]   So the first version of Runway was a layer of abstraction on top of like Docker, where
[00:16:29.400 --> 00:16:33.160]   you could run different algorithms and different models in real time on this like electron
[00:16:33.160 --> 00:16:34.160]   app.
[00:16:34.160 --> 00:16:38.200]   And so you can click and run models in real time and connect those models via either sockets
[00:16:38.200 --> 00:16:43.640]   or UDP or a web server to like Unity or Photoshop.
[00:16:43.640 --> 00:16:47.440]   And so we started building all these like plugins where you can do the stuff that you
[00:16:47.440 --> 00:16:51.480]   are able to see now on Twitter or like here I build a Photoshop or a Figma plugin that
[00:16:51.480 --> 00:16:53.800]   does image generation.
[00:16:53.800 --> 00:16:57.520]   We're building all that stuff, running like Docker models in your computer locally and
[00:16:57.520 --> 00:16:59.000]   you can stream those.
[00:16:59.000 --> 00:17:00.000]   And it was like 2018, 2019.
[00:17:00.000 --> 00:17:01.000]   Interesting.
[00:17:01.000 --> 00:17:05.600]   So it must have been a much more technical audience at the time then, right?
[00:17:05.600 --> 00:17:11.000]   If you have to run Docker on your local machine, that's not something everyone can do, right?
[00:17:11.000 --> 00:17:12.000]   Totally.
[00:17:12.000 --> 00:17:13.000]   Totally.
[00:17:13.000 --> 00:17:16.280]   I think that also tells a lot about how much progress the field has made and how mainstream
[00:17:16.280 --> 00:17:19.440]   and how more accessible things have become.
[00:17:19.440 --> 00:17:26.760]   Like again, trying to put this set of like new platforms and compute ideas for creators
[00:17:26.760 --> 00:17:31.600]   and video makers and filmmakers require you to know how to install CUDA and like manage
[00:17:31.600 --> 00:17:34.500]   CUDA and I don't know, it's just too much.
[00:17:34.500 --> 00:17:35.500]   But people were still wanting to do it.
[00:17:35.500 --> 00:17:38.480]   There were some aspects and folks were like, "Hey, this is like really unique.
[00:17:38.480 --> 00:17:41.080]   I want to understand how to use this."
[00:17:41.080 --> 00:17:42.760]   But then we realized it wasn't like enough.
[00:17:42.760 --> 00:17:48.640]   You need to go higher layers of abstraction on top of that to really enable like creative
[00:17:48.640 --> 00:17:52.560]   folks to play with this without having to spend like months trying to set up like their
[00:17:52.560 --> 00:17:54.600]   GPU machines, right?
[00:17:54.600 --> 00:17:59.360]   And so Runway has really evolved and we have a really good experiment driven kind of like
[00:17:59.360 --> 00:18:01.960]   thesis and way of working on the product.
[00:18:01.960 --> 00:18:06.180]   It's all about like trying ideas and testing it out with people really fast.
[00:18:06.180 --> 00:18:08.280]   We're building something that hasn't been done before.
[00:18:08.280 --> 00:18:13.160]   And so it's really easy to get sidetracked into things that you think are going to work
[00:18:13.160 --> 00:18:15.160]   or ideas that you think are going to be impactful.
[00:18:15.160 --> 00:18:20.080]   But since you're working with new stuff all the time, being close to your user base for
[00:18:20.080 --> 00:18:22.360]   us has been kind of really, really important.
[00:18:22.360 --> 00:18:27.160]   And so every time we've reiterated on the product, I think one consistent line of evolution
[00:18:27.160 --> 00:18:33.120]   has been this idea of like simplifying, making higher abstraction layers on top of it.
[00:18:33.120 --> 00:18:38.040]   Like the first version of rotoscoping or in painting require you to select the underlying
[00:18:38.040 --> 00:18:43.560]   like model architecture and they're applying and understanding what a mask was and propagation
[00:18:43.560 --> 00:18:44.560]   works.
[00:18:44.560 --> 00:18:47.440]   Like if you're really a filmmaker, like you don't care about any of those stuff.
[00:18:47.440 --> 00:18:50.320]   You just want to click once and you want to get a really good result.
[00:18:50.320 --> 00:18:55.280]   So for us, it's like, how do you build from there using what we're building behind the
[00:18:55.280 --> 00:18:56.280]   scenes.
[00:18:56.280 --> 00:19:02.560]   Were you surprised how well these approaches have worked to generate images?
[00:19:02.560 --> 00:19:09.120]   Like it sounds like you started your work in like 2017, 2018.
[00:19:09.120 --> 00:19:12.080]   The space that you're in has changed so much.
[00:19:12.080 --> 00:19:17.560]   Do you feel like you saw it coming or have things unfolded differently than you thought?
[00:19:17.560 --> 00:19:25.680]   Yeah, I mean, things have definitely accelerated, but I think our thesis, when we started Runwith
[00:19:25.680 --> 00:19:27.880]   three and a half years ago, it was pretty much the same.
[00:19:27.880 --> 00:19:36.760]   It was like, we're entering literally a new paradigm of computation and content and we're
[00:19:36.760 --> 00:19:41.880]   soon going to be able to generate every single piece of content and multimedia content that
[00:19:41.880 --> 00:19:42.880]   we see online.
[00:19:42.880 --> 00:19:49.240]   I've been demoing like generating models for creative use cases for the last three years.
[00:19:49.240 --> 00:19:53.840]   And what I was showing like three years ago, people were like, it was like, hey, this is
[00:19:53.840 --> 00:19:54.840]   how it works.
[00:19:54.840 --> 00:19:55.840]   This is how you train a model.
[00:19:55.840 --> 00:19:59.240]   And this is what the outcome of the model is.
[00:19:59.240 --> 00:20:04.160]   And of course, at that time, like it was like blurry, I don't know, 500 by 100 pixels, like
[00:20:04.160 --> 00:20:08.340]   image, some sort of like representation of what you're describing.
[00:20:08.340 --> 00:20:11.720]   Most people took it as a joke, it's like, oh yeah, cool, very cool.
[00:20:11.720 --> 00:20:16.780]   And like cool thing or as a toy, it's like, oh yeah, that's a fun thing, right?
[00:20:16.780 --> 00:20:20.880]   You kind of like use it once, but of course I will never use this in production.
[00:20:20.880 --> 00:20:26.520]   So I remember like speaking with this huge, like one of the biggest ad agencies in the
[00:20:26.520 --> 00:20:31.200]   world, it was like presenting to all their executives, like here's the future of content,
[00:20:31.200 --> 00:20:32.680]   type anything you want.
[00:20:32.680 --> 00:20:36.960]   And like something like blurry came out and like, cool, no, not for now.
[00:20:36.960 --> 00:20:41.480]   And then reach out like three weeks ago being like, hey, how many licenses can we get for
[00:20:41.480 --> 00:20:43.640]   this tomorrow?
[00:20:43.640 --> 00:20:50.400]   Because the models are getting just so much better that it's obvious, like it's transforming
[00:20:50.400 --> 00:20:52.680]   their industries and a lot of other things.
[00:20:52.680 --> 00:20:57.160]   So I think what has changed for us is pretty much the speed.
[00:20:57.160 --> 00:21:01.800]   Like now we're entering a really nice moment where like things are converging and there's
[00:21:01.800 --> 00:21:04.720]   a good understanding of what's going to be possible and where things are going.
[00:21:04.720 --> 00:21:09.360]   Like scaling laws are getting to a good point.
[00:21:09.360 --> 00:21:13.360]   And so continue the same, but the thesis of the company was always built on that this
[00:21:13.360 --> 00:21:17.440]   will happen and it's happening sooner than, sooner rather than later.
[00:21:17.440 --> 00:21:22.440]   Do you have a perspective on if this acceleration will continue or if we just are seeing like
[00:21:22.440 --> 00:21:26.520]   a breakthrough and then we're going to need new breakthroughs to sort of get to the next
[00:21:26.520 --> 00:21:27.920]   level of quality?
[00:21:27.920 --> 00:21:29.040]   Sure.
[00:21:29.040 --> 00:21:33.600]   I think there's definitely more compute that needs to be added to this, more data sets.
[00:21:33.600 --> 00:21:37.480]   I think we're still scratching the surface of like what will become.
[00:21:37.480 --> 00:21:41.480]   I think there's still like this, I was discussing this with a friend the other day, this idea
[00:21:41.480 --> 00:21:46.600]   of like there's like curiosity phase where people are like entering the realm of like
[00:21:46.600 --> 00:21:49.840]   what's possible and coming up with all these solutions and ideas.
[00:21:49.840 --> 00:21:53.920]   But there's still a difference between those concepts and explorations and ideas and like
[00:21:53.920 --> 00:21:57.600]   meaningful products that are long-term build upon those.
[00:21:57.600 --> 00:22:01.000]   What I'm interested in seeing is like how much of those ideas will actually convert
[00:22:01.000 --> 00:22:04.180]   over time or meaningful products.
[00:22:04.180 --> 00:22:08.520]   And I think that conversion of products is not just pure research or pure new models.
[00:22:08.520 --> 00:22:12.320]   There needs to be like that layer of infrastructure to support those things.
[00:22:12.320 --> 00:22:16.400]   So it's great that you can run one single model to the one single thing on like X percent.
[00:22:16.400 --> 00:22:21.760]   But if you're trying to scale on a real-time basis for 10 people that then use it on a
[00:22:21.760 --> 00:22:27.760]   team and depend on it for their work, then there's like a slightly different thing.
[00:22:27.760 --> 00:22:32.680]   But I think we're about to see way more stuff around video specifically, I think.
[00:22:32.680 --> 00:22:36.920]   I think image might be solved in a couple of more months and video is starting to now
[00:22:36.920 --> 00:22:37.920]   catch up with that.
[00:22:37.920 --> 00:22:39.240]   So really exciting time for that.
[00:22:39.240 --> 00:22:41.920]   What does something being solved mean to you?
[00:22:41.920 --> 00:22:45.640]   Like you could just get any image that you would ever want or imagine?
[00:22:45.640 --> 00:22:48.720]   Yeah, I mean, yeah, that's a good one.
[00:22:48.720 --> 00:22:49.720]   That's a good question.
[00:22:49.720 --> 00:22:55.000]   I would say that I would consider being solved, being able to translate something like words
[00:22:55.000 --> 00:23:00.800]   or a description into like a meaningful image or content that pretty much matches what you're
[00:23:00.800 --> 00:23:02.680]   trying to, what you're imagining.
[00:23:02.680 --> 00:23:06.960]   And if it doesn't, you're able to control really quickly and easily to get to the point
[00:23:06.960 --> 00:23:11.640]   where you can arrive at your final idea.
[00:23:11.640 --> 00:23:15.000]   So that's why the combination of models really makes sense.
[00:23:15.000 --> 00:23:18.040]   It's going to be hard to have a full model that does exactly what you want.
[00:23:18.040 --> 00:23:21.280]   For instance, for image generation, I think it's a combination of like you have a model
[00:23:21.280 --> 00:23:26.320]   that does the first milestone, which is like you generate something, there's no pixels,
[00:23:26.320 --> 00:23:27.840]   you generate the pixels.
[00:23:27.840 --> 00:23:33.280]   Second step is you're able to quickly modify it or in painting or grading in some way and
[00:23:33.280 --> 00:23:34.960]   starting in some other way.
[00:23:34.960 --> 00:23:41.280]   But that whole thing just happens in like a few seconds or a few minutes, right?
[00:23:41.280 --> 00:23:45.720]   And if you speak with anyone in the industry, VFX or ad agencies or content creation, kind
[00:23:45.720 --> 00:23:50.200]   of like post-production companies, these are stuff these guys do all the time.
[00:23:50.200 --> 00:23:52.160]   This is like what they do for a living, right?
[00:23:52.160 --> 00:23:55.600]   They're able to create content out of nothing.
[00:23:55.600 --> 00:23:57.820]   The thing is just, it's really expensive.
[00:23:57.820 --> 00:24:02.120]   It's really, really expensive and involves a lot of time and rendering and like skilled
[00:24:02.120 --> 00:24:04.320]   people to get to that point.
[00:24:04.320 --> 00:24:08.480]   I think for me, Solve is like anyone, anyone can have access to that professional level
[00:24:08.480 --> 00:24:12.640]   grade VFX type of content from their computers and from a browser.
[00:24:12.640 --> 00:24:16.640]   So do you ever think about making, you know, more of like a version of Photoshop instead
[00:24:16.640 --> 00:24:18.840]   of like a video editing software?
[00:24:18.840 --> 00:24:21.000]   If you think like images are closer to being solved.
[00:24:21.000 --> 00:24:25.880]   I mean, certainly like I can't go into Photoshop and get exactly the image I want.
[00:24:25.880 --> 00:24:32.800]   And I love to play with all the image generation tools out there, but I do think like they're
[00:24:32.800 --> 00:24:35.960]   amazing at first, but then you kind of like hit this point where it's like you really
[00:24:35.960 --> 00:24:38.440]   want the image to look like you want.
[00:24:38.440 --> 00:24:41.480]   It gets like kind of frustrating.
[00:24:41.480 --> 00:24:42.480]   I don't know.
[00:24:42.480 --> 00:24:46.680]   It seems like there's also room for like kind of like an image version of what you're doing.
[00:24:46.680 --> 00:24:50.200]   Is that something you'd consider doing or why not make that?
[00:24:50.200 --> 00:24:51.200]   Totally.
[00:24:51.200 --> 00:24:52.200]   Yeah.
[00:24:52.200 --> 00:24:53.200]   The answer is absolutely.
[00:24:53.200 --> 00:24:55.040]   I think a few things.
[00:24:55.040 --> 00:24:59.200]   One, I think we're converging more to this idea of like multi-model systems where you
[00:24:59.200 --> 00:25:02.400]   can transfer between images and videos and audio.
[00:25:02.400 --> 00:25:07.320]   I think the idea that we've built software to deal with each media independently.
[00:25:07.320 --> 00:25:10.840]   So there's an audio editing software and a video editing software and image editing software
[00:25:10.840 --> 00:25:12.120]   and a text based.
[00:25:12.120 --> 00:25:17.600]   Like you have models that can quickly translate between all of those and content like let's
[00:25:17.600 --> 00:25:18.600]   say video.
[00:25:18.600 --> 00:25:19.600]   It's a combination of different things.
[00:25:19.600 --> 00:25:25.040]   You have images, you have videos, you have audio, you have voice, right?
[00:25:25.040 --> 00:25:26.040]   All of those things are now possible.
[00:25:26.040 --> 00:25:30.640]   And I think for us, when I think about the product philosophy of Runway, it's less about
[00:25:30.640 --> 00:25:33.600]   how do you build a better Photoshop or a better Premiere?
[00:25:33.600 --> 00:25:37.040]   Fundamentally, these models are just allowing you to do the things that none of those servers
[00:25:37.040 --> 00:25:38.040]   can do.
[00:25:38.040 --> 00:25:42.320]   So if you think about marginal integrations of those things, yeah, you build a better
[00:25:42.320 --> 00:25:48.040]   Photoshop that has a better like paintbrush or a better like context aware tool.
[00:25:48.040 --> 00:25:51.880]   But ultimately when you combine them in new ways, you create a new thing.
[00:25:51.880 --> 00:25:52.920]   It's completely new.
[00:25:52.920 --> 00:25:57.440]   It's not Photoshop, it's just like a new way of making videos and editing images and editing
[00:25:57.440 --> 00:26:01.320]   audio all in one single kind of like component or tool.
[00:26:01.320 --> 00:26:05.480]   And so for me, what's really interesting is like that multi-model aspect of things and
[00:26:05.480 --> 00:26:06.920]   translating also into those.
[00:26:06.920 --> 00:26:10.600]   And 3D for instance, it's one of the fields where you're going to be starting to see a
[00:26:10.600 --> 00:26:13.360]   lot of like translation between images and videos on 3D.
[00:26:13.360 --> 00:26:14.360]   Totally.
[00:26:14.360 --> 00:26:18.400]   So I have to ask you your thoughts on like deepfakes and things like that.
[00:26:18.400 --> 00:26:22.640]   I'm sure everyone asks you that, but I'm really curious what you think about that.
[00:26:22.640 --> 00:26:26.920]   Do you think that you would want to like put in limitations into your software to, you
[00:26:26.920 --> 00:26:28.080]   know, not allow certain things?
[00:26:28.080 --> 00:26:34.600]   Do you think this is about to change the way we view videos as this technology gets more
[00:26:34.600 --> 00:26:37.520]   standardized and available to everyone?
[00:26:37.520 --> 00:26:38.520]   For sure.
[00:26:38.520 --> 00:26:45.200]   Yeah, I think as every major technological breakthrough, there's always like social concerns
[00:26:45.200 --> 00:26:50.640]   about how it might be misused or used in not the right or intended ways.
[00:26:50.640 --> 00:26:55.960]   And it's a good exercise to look at like history to see what has happened before.
[00:26:55.960 --> 00:27:00.640]   This is really good like YouTube video about Photoshop when it was first released.
[00:27:00.640 --> 00:27:03.960]   I would think about like the early 90s.
[00:27:03.960 --> 00:27:06.520]   And they were like, it's a kind of like a late night show.
[00:27:06.520 --> 00:27:11.800]   And they're discussing like the ethical implications of manipulating images in magazines.
[00:27:11.800 --> 00:27:15.760]   And they're like, should we allow to manipulate images and put them in magazines?
[00:27:15.760 --> 00:27:18.440]   And like half of the panel was like, no, we shouldn't.
[00:27:18.440 --> 00:27:22.920]   Just like it breaks the essence of what photography is, right?
[00:27:22.920 --> 00:27:27.240]   And like 20 years after that, it's like it makes no sense to think about not doing something
[00:27:27.240 --> 00:27:28.240]   like that.
[00:27:28.240 --> 00:27:29.240]   Right.
[00:27:29.240 --> 00:27:33.240]   And so it's also there's always like an adaptation process, I would say, where people need to
[00:27:33.240 --> 00:27:36.960]   and we need to collectively ask like, hey, how is it going to be used?
[00:27:36.960 --> 00:27:40.800]   But I think ultimately, you understand what the limitations are.
[00:27:40.800 --> 00:27:45.800]   And you also fine tune your eyes and your understanding of the world to make sense of
[00:27:45.800 --> 00:27:46.800]   that thing.
[00:27:46.800 --> 00:27:47.800]   Right.
[00:27:47.800 --> 00:27:50.480]   And now everyone knows that Photoshop is a verb that you can use to describe something
[00:27:50.480 --> 00:27:51.480]   that's manipulated.
[00:27:51.480 --> 00:27:52.480]   Right.
[00:27:52.480 --> 00:27:56.320]   And you do that same exercise and you go back in time and you see the same.
[00:27:56.320 --> 00:28:00.720]   When film just started to appear, it was this story, interesting story about one of the
[00:28:00.720 --> 00:28:04.080]   first films that were made is like a train arriving to a station.
[00:28:04.080 --> 00:28:06.640]   And they were like projecting that on a room.
[00:28:06.640 --> 00:28:10.760]   And when people saw the train coming to a station, everyone ran away because like they
[00:28:10.760 --> 00:28:13.360]   felt like it was coming to a station, literally.
[00:28:13.360 --> 00:28:15.920]   So but then you get you make you make sense of it.
[00:28:15.920 --> 00:28:17.160]   And you're like, yeah, this is not true.
[00:28:17.160 --> 00:28:21.360]   Like I understand that this is like an actual representation of something.
[00:28:21.360 --> 00:28:25.160]   And so ultimately, I think with with AI and with generated content, we'll enter a similar
[00:28:25.160 --> 00:28:30.560]   phase where like it's going to become commonplace and something people are familiar with.
[00:28:30.560 --> 00:28:32.960]   And of course, there are going to be misuses and bad uses.
[00:28:32.960 --> 00:28:37.020]   Of course, people can use Photoshop for all sort of like evil ways.
[00:28:37.020 --> 00:28:41.240]   But like the 99 percent of like people are just like their lives have been changed forever
[00:28:41.240 --> 00:28:43.680]   in a positive way because of this.
[00:28:43.680 --> 00:28:44.680]   Interesting.
[00:28:44.680 --> 00:28:49.360]   Well, look, I'd love to hear more about your your tech stack.
[00:28:49.360 --> 00:28:53.240]   I mean, this is a show for for ML nerds of all types.
[00:28:53.240 --> 00:28:57.520]   Like I mean, I think you're doing pretty hardcore ML at scale.
[00:28:57.520 --> 00:29:03.240]   Like, you know, what have been the challenges of making this work, like making the you know,
[00:29:03.240 --> 00:29:06.560]   the interface as responsive as it was?
[00:29:06.560 --> 00:29:09.880]   Like what were the sort of the key things to to scale up your models?
[00:29:09.880 --> 00:29:10.880]   Sure.
[00:29:10.880 --> 00:29:16.240]   There's a lot of things that we have to kind of like come up creatively to make this work
[00:29:16.240 --> 00:29:17.240]   in real time.
[00:29:17.240 --> 00:29:22.840]   On the one hand, I guess on the ML side, we mostly use PyTorch for all models.
[00:29:22.840 --> 00:29:30.760]   We have a cluster, basically an AWS cluster that scales based on compute and demand where
[00:29:30.760 --> 00:29:32.360]   we run all those models.
[00:29:32.360 --> 00:29:37.600]   For training, we use sometimes lighting and of course, weights and biases to follow up
[00:29:37.600 --> 00:29:42.360]   and understand better what's working in our on our model training.
[00:29:42.360 --> 00:29:49.360]   And then serving, we optimize for different like GPU kind of like levels or compute platforms
[00:29:49.360 --> 00:29:50.360]   depending on availability.
[00:29:50.360 --> 00:29:56.080]   And so we've kind of like some make some systems to scale up depending on demand.
[00:29:56.080 --> 00:30:01.920]   On the front end side of things, it's everything's TypeScript and React based.
[00:30:01.920 --> 00:30:06.680]   There's some WebGL kind of like acceleration stuff we're doing to make things like really
[00:30:06.680 --> 00:30:08.480]   smooth.
[00:30:08.480 --> 00:30:12.120]   And then the inference pipeline where we're writing everything like C++ to make it like
[00:30:12.120 --> 00:30:17.320]   super, super efficient and like fast, specifically since you're decoding and coding videos in
[00:30:17.320 --> 00:30:18.320]   real time.
[00:30:18.320 --> 00:30:24.840]   We also build this streaming system that passes frames or video frames through different models
[00:30:24.840 --> 00:30:26.800]   to do the things that I just showed you.
[00:30:26.800 --> 00:30:30.040]   So we also have to come up really creative with that.
[00:30:30.040 --> 00:30:33.320]   And that's kind of like a big picture of, I guess, our tech stack.
[00:30:33.320 --> 00:30:37.880]   I feel like one challenge that I'm seeing some of our customers run into as these models
[00:30:37.880 --> 00:30:43.840]   kind of get bigger and more important is that the actual kind of serving cost of the application
[00:30:43.840 --> 00:30:45.240]   increases.
[00:30:45.240 --> 00:30:46.600]   Is that like an issue for you?
[00:30:46.600 --> 00:30:51.960]   Like do you do things like quantization or like is lowering your inference costs like
[00:30:51.960 --> 00:30:56.640]   an important project for you all?
[00:30:56.640 --> 00:30:57.640]   For sure.
[00:30:57.640 --> 00:30:58.640]   Yeah, for sure.
[00:30:58.640 --> 00:31:04.360]   I mean, we're running our biggest cost right now is like AWS, GPU costs and inference costs
[00:31:04.360 --> 00:31:05.360]   and serving these models.
[00:31:05.360 --> 00:31:09.560]   There are two main areas, like for sure, like we have an HPC and we're doing like large
[00:31:09.560 --> 00:31:11.800]   girl training of language models and video models.
[00:31:11.800 --> 00:31:18.320]   And so that takes a lot of resources and time, but just serving on, I would say like the
[00:31:18.320 --> 00:31:20.720]   trade off between precision and like speed really matters.
[00:31:20.720 --> 00:31:24.280]   So quantizing model is great, but also you need to make sure that you're not affecting
[00:31:24.280 --> 00:31:28.640]   the quality of the model, because if you're affecting something on a pixel level, it might
[00:31:28.640 --> 00:31:31.280]   change the results from being like okay to like bad.
[00:31:31.280 --> 00:31:34.080]   And like that might mean that you're churning.
[00:31:34.080 --> 00:31:37.360]   So if you're going to like spend like a few more seconds rendering, that might actually
[00:31:37.360 --> 00:31:38.360]   be better.
[00:31:38.360 --> 00:31:40.920]   There's always like a trade off, like how much.
[00:31:40.920 --> 00:31:46.880]   But yeah, we always try to figure out what's the right balance there.
[00:31:46.880 --> 00:31:49.480]   We're also exploring some stuff on the browser.
[00:31:49.480 --> 00:31:51.960]   I think the browser is becoming like really powerful.
[00:31:51.960 --> 00:31:54.400]   The only constraint about the browser is just memory and RAM.
[00:31:54.400 --> 00:31:58.280]   And like you get to like, it's a sandbox, so you can't really do a lot of things specifically
[00:31:58.280 --> 00:32:01.760]   with video, but you can run some stuff on the browser.
[00:32:01.760 --> 00:32:05.360]   And so with WebAssembly specifically, you can convert some things and make them smooth
[00:32:05.360 --> 00:32:06.360]   enough.
[00:32:06.360 --> 00:32:08.640]   But I think we're not 100% there yet.
[00:32:08.640 --> 00:32:12.280]   Because you're also training your own large language models and large image models.
[00:32:12.280 --> 00:32:16.400]   I mean, that sounds like training would be a major cost for you as well.
[00:32:16.400 --> 00:32:18.040]   Yeah, for sure.
[00:32:18.040 --> 00:32:23.080]   Like retraining some stuff to make sure it works in the domain of what we have is kind
[00:32:23.080 --> 00:32:25.560]   of like one of our core competences.
[00:32:25.560 --> 00:32:31.920]   And so yeah, now we're training or starting a huge job on our HPC.
[00:32:31.920 --> 00:32:38.400]   And so that's going to take a big percentage of our costs for the next few months.
[00:32:38.400 --> 00:32:42.680]   I guess I have to ask, you know, that language interface that you showed me was so compelling
[00:32:42.680 --> 00:32:48.600]   and cool, but I have been seeing language interfaces for the past 20 years.
[00:32:48.600 --> 00:32:52.440]   And like the challenge with these language interfaces is like, you know, when they don't
[00:32:52.440 --> 00:32:54.080]   work, they're just like enraging.
[00:32:54.080 --> 00:32:58.760]   And actually you sort of addressed that, like showing how it creates these things and like
[00:32:58.760 --> 00:33:01.260]   you can undo them and you can kind of modify them.
[00:33:01.260 --> 00:33:07.520]   But I guess, do you feel like that kind of conversational interface is at the point where
[00:33:07.520 --> 00:33:11.200]   for you, it's an interface that you really want to use?
[00:33:11.200 --> 00:33:14.840]   Yeah, I like to think of it as a, again, as a tool, right?
[00:33:14.840 --> 00:33:17.800]   So it's not the sole answer to everything you need.
[00:33:17.800 --> 00:33:23.080]   Like this is not going to be a replacement for all of the workflows in making content,
[00:33:23.080 --> 00:33:25.660]   video images or sound or whatever it is.
[00:33:25.660 --> 00:33:28.680]   It's just a speed up in the way you can do those kinds of things.
[00:33:28.680 --> 00:33:32.680]   And so I think the sweet spot is a combination of both.
[00:33:32.680 --> 00:33:38.040]   But being able to have that constant feedback loop with the system where you're like stating
[00:33:38.040 --> 00:33:45.720]   something out, the system is like reacting in some way that matches your idea.
[00:33:45.720 --> 00:33:49.400]   And then you have that level of control to either like go in the direction you want and
[00:33:49.400 --> 00:33:50.400]   do what you want.
[00:33:50.400 --> 00:33:52.880]   Or if you want to go, if it's not working, just do it yourself.
[00:33:52.880 --> 00:33:53.880]   Right.
[00:33:53.880 --> 00:33:57.720]   I think a big mistake of like research specifically in there of like computational creativities
[00:33:57.720 --> 00:34:00.240]   is that you can automate it entirely.
[00:34:00.240 --> 00:34:03.560]   So you see like one click of solutions to do X, Y or Z.
[00:34:03.560 --> 00:34:07.600]   I think that's missing the bigger picture of like how most of creative work should actually
[00:34:07.600 --> 00:34:08.600]   work.
[00:34:08.600 --> 00:34:12.200]   Or that probably means that you've never actually worked with an agency where the client was
[00:34:12.200 --> 00:34:17.560]   asking you to like change things every single hour, make it bigger, make it smaller.
[00:34:17.560 --> 00:34:18.560]   Right.
[00:34:18.560 --> 00:34:22.040]   And so it's hard for me to imagine a world where like you have a one click of solution
[00:34:22.040 --> 00:34:23.760]   for everything.
[00:34:23.760 --> 00:34:25.720]   That feels boring, to be honest.
[00:34:25.720 --> 00:34:26.720]   You want to have that control.
[00:34:26.720 --> 00:34:31.920]   And so I think language interfaces are a huge step towards like accelerating the speed at
[00:34:31.920 --> 00:34:33.480]   which you can execute.
[00:34:33.480 --> 00:34:34.920]   Are they the final answer for everything?
[00:34:34.920 --> 00:34:41.240]   I'm not sure, but they do make you, they make you just move faster on your ideas.
[00:34:41.240 --> 00:34:44.920]   And did I understand you right that you want to build your own large language model?
[00:34:44.920 --> 00:34:49.120]   Like I would assume you would take one of the, like many off the shelf language models
[00:34:49.120 --> 00:34:50.120]   today.
[00:34:50.120 --> 00:34:52.040]   Like what are you actually training your own?
[00:34:52.040 --> 00:34:54.120]   Yeah, I think it's, we are.
[00:34:54.120 --> 00:35:00.640]   And it's also like the fact that like ML, I guess the infra for models and models themselves
[00:35:00.640 --> 00:35:02.580]   are becoming commodities.
[00:35:02.580 --> 00:35:06.240]   It's great for companies like us because some stuff we can build on our own.
[00:35:06.240 --> 00:35:09.520]   Like there's a lot of things in Runway that you won't find everywhere else.
[00:35:09.520 --> 00:35:13.360]   But some other stuff like large language models that you can just use off the shelf.
[00:35:13.360 --> 00:35:17.000]   You have all these like companies offering similar services.
[00:35:17.000 --> 00:35:21.400]   It's a great, I guess, for us as a consumer of those, if we want to use those, it's just
[00:35:21.400 --> 00:35:28.200]   a cost situation where like whoever offers like the best model we'll use.
[00:35:28.200 --> 00:35:32.200]   And to a point where like it might make sense to either like do it our own.
[00:35:32.200 --> 00:35:35.080]   So yeah, sometimes we don't have to do everything ourselves.
[00:35:35.080 --> 00:35:38.200]   You can just buy it off the shelf, but some other times you just need to do it because
[00:35:38.200 --> 00:35:40.100]   it doesn't exist.
[00:35:40.100 --> 00:35:44.440]   And sorry, in large language models, you think you might do it yourself even?
[00:35:44.440 --> 00:35:45.440]   We're doing a combination of both.
[00:35:45.440 --> 00:35:48.360]   We're like using APIs, but also returning some of our own.
[00:35:48.360 --> 00:35:49.360]   I see.
[00:35:49.360 --> 00:35:50.360]   I see.
[00:35:50.360 --> 00:35:54.000]   And do you feel like, have you experimented with all the large models out there?
[00:35:54.000 --> 00:35:58.080]   Like do you have like a favorite of the existing offerings?
[00:35:58.080 --> 00:36:00.320]   I think GPT-3 works.
[00:36:00.320 --> 00:36:05.240]   I think actually the model that I think is DaVinci, it's probably like GPT-4 by now.
[00:36:05.240 --> 00:36:06.920]   I think OpenAI has been updating that.
[00:36:06.920 --> 00:36:07.920]   Right, right.
[00:36:07.920 --> 00:36:10.080]   A side and leave behind the scenes, it works really well.
[00:36:10.080 --> 00:36:14.320]   And so that's the one I'd say we're experimenting with the most and we get the best results.
[00:36:14.320 --> 00:36:15.320]   Cool.
[00:36:15.320 --> 00:36:17.120]   Well, look, we always end with two questions.
[00:36:17.120 --> 00:36:18.120]   I want to make sure I get them in.
[00:36:18.120 --> 00:36:24.520]   And so the second to last question is, what is like a topic that you don't get to work
[00:36:24.520 --> 00:36:26.880]   on that you wish you had more time to work on?
[00:36:26.880 --> 00:36:30.800]   Or what's something that's sort of underrated for you in machine learning right now?
[00:36:30.800 --> 00:36:36.440]   And I realize it's a funny question to ask an obsessed ML founder, but I'll ask it anyway.
[00:36:36.440 --> 00:36:38.320]   I think audio generation.
[00:36:38.320 --> 00:36:43.600]   I think it's catching up now, but no one really has been paying a lot of attention.
[00:36:43.600 --> 00:36:48.280]   There's some really interesting open source models from like Tacotron to a few other things
[00:36:48.280 --> 00:36:49.480]   out there.
[00:36:49.480 --> 00:36:58.880]   I think that's going to be really, really like transformative for a bunch of like applications.
[00:36:58.880 --> 00:37:02.000]   And we're already kind of like stepping into some stuff there.
[00:37:02.000 --> 00:37:06.960]   But I mean, it's hard to focus as an industry or as a research community on a lot of things
[00:37:06.960 --> 00:37:07.960]   at the same time.
[00:37:07.960 --> 00:37:11.800]   And now that I guess image understanding has kind of like been solved in a way, people
[00:37:11.800 --> 00:37:13.520]   are moving to other specific fields.
[00:37:13.520 --> 00:37:18.880]   I think one of the ones that are going to start seeing very soon is audio generation.
[00:37:18.880 --> 00:37:20.120]   So yeah, excited for that for sure.
[00:37:20.120 --> 00:37:21.120]   Yeah, I totally agree.
[00:37:21.120 --> 00:37:23.760]   Do you have like a favorite model out there?
[00:37:23.760 --> 00:37:30.520]   We just recently talked to, I think Dance Diffusion or Harmon AI that was doing some
[00:37:30.520 --> 00:37:32.520]   cool audio generation stuff.
[00:37:32.520 --> 00:37:34.240]   Yeah, there's one.
[00:37:34.240 --> 00:37:36.440]   Let me search for it.
[00:37:36.440 --> 00:37:41.600]   Just in my mind, Tortoise TTS.
[00:37:41.600 --> 00:37:44.040]   I don't know if you've seen that one.
[00:37:44.040 --> 00:37:49.920]   So yeah, Tortoise TTS is I think that the work of just one single folk, James Betker.
[00:37:49.920 --> 00:37:52.760]   It works really well and he's been using it.
[00:37:52.760 --> 00:37:57.560]   Someone used it to create this Lex Friedman, like generating podcast.
[00:37:57.560 --> 00:38:01.480]   And so yeah, I'll share with you the audio.
[00:38:01.480 --> 00:38:05.720]   But it's a whole like podcast series that goes every week, but everything's generated.
[00:38:05.720 --> 00:38:09.760]   The script is generated by GPT-3 and the audio is generated by Tortoise.
[00:38:09.760 --> 00:38:12.720]   And you can hear it's like, it's just, yeah, it's a podcast.
[00:38:12.720 --> 00:38:14.280]   It's just like, you can't really tell.
[00:38:14.280 --> 00:38:15.280]   That's amazing.
[00:38:15.280 --> 00:38:16.840]   So yeah, really excited for stuff like that.
[00:38:16.840 --> 00:38:17.840]   Cool.
[00:38:17.840 --> 00:38:22.420]   And I guess the final question is for you, what's been the hardest part about getting
[00:38:22.420 --> 00:38:26.000]   the actual ML to work in the real world?
[00:38:26.000 --> 00:38:33.120]   So going from these ideas of models or research to like deployed and working for users.
[00:38:33.120 --> 00:38:39.680]   I think like these models and things like image generation and video generation require
[00:38:39.680 --> 00:38:43.520]   a different mental model of how you can leverage this in creative ways.
[00:38:43.520 --> 00:38:49.560]   And I think a big mistake has been to try to use existing principles of image or video
[00:38:49.560 --> 00:38:52.960]   generation and like patch them with this stuff.
[00:38:52.960 --> 00:38:56.000]   I think ultimately you need to think about it in like very different ways.
[00:38:56.000 --> 00:39:00.360]   Like navigating a latent space is not the same as like editing an image, right?
[00:39:00.360 --> 00:39:03.440]   So what are the metaphors and the obstructions they need to have?
[00:39:03.440 --> 00:39:07.120]   And we've come up with those before, like in the software product that we have right
[00:39:07.120 --> 00:39:11.880]   now, you have like a brush and a paint bucket and a context-aware tool and you're like editing
[00:39:11.880 --> 00:39:12.880]   stuff.
[00:39:12.880 --> 00:39:17.960]   But when you have large language models that are able to translate ideas into content and
[00:39:17.960 --> 00:39:22.520]   you navigate and move across a specific space or vector direction in ways you want, they
[00:39:22.520 --> 00:39:25.240]   need new metaphors and new obstructions.
[00:39:25.240 --> 00:39:30.560]   So what's been really, I would say, interesting and challenging is what are those metaphors?
[00:39:30.560 --> 00:39:31.800]   What are those interfaces?
[00:39:31.800 --> 00:39:36.080]   How do you make sure the systems you're building are really expressive?
[00:39:36.080 --> 00:39:40.920]   I think two things that drive a lot of what we do are control and expressiveness.
[00:39:40.920 --> 00:39:44.920]   Control as in you as a creator want to have full control over your making and that's really
[00:39:44.920 --> 00:39:45.920]   important.
[00:39:45.920 --> 00:39:49.040]   And how do you make that so you also are expressive about it?
[00:39:49.040 --> 00:39:55.080]   You can move in specific ways as you are intended to or you're intending to do.
[00:39:55.080 --> 00:39:59.280]   So yeah, I think that's also, I guess, also really exciting and fascinating for us to
[00:39:59.280 --> 00:40:01.400]   invent some of those stuff.
[00:40:01.400 --> 00:40:02.400]   What's really impressive what you did.
[00:40:02.400 --> 00:40:03.400]   Thanks so much for the interview.
[00:40:03.400 --> 00:40:04.400]   Of course.
[00:40:04.400 --> 00:40:07.240]   Thanks so much for hosting me.
[00:40:07.240 --> 00:40:11.600]   If you're enjoying these interviews and you want to learn more, please click on the link
[00:40:11.600 --> 00:40:16.320]   to the show notes in the description where you can find links to all the papers that
[00:40:16.320 --> 00:40:20.720]   are mentioned, supplemental material and a transcription that we worked really hard to
[00:40:20.720 --> 00:40:21.720]   produce.
[00:40:21.720 --> 00:40:21.720]   So check it out.
[00:40:21.720 --> 00:40:25.080]   [MUSIC PLAYING]
[00:40:25.080 --> 00:40:27.140]   you

