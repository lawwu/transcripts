
[00:00:00.000 --> 00:00:04.000]   Let me make sure everything is as it should be. I'm always
[00:00:04.000 --> 00:00:07.080]   fascinated by the fact that people are waiting. It's like,
[00:00:07.080 --> 00:00:13.080]   it's so surprising that people are like on here sometimes
[00:00:13.080 --> 00:00:15.880]   early. For you, it makes sense for my session. I'm like, why
[00:00:15.880 --> 00:00:16.680]   are people joining?
[00:00:16.680 --> 00:00:26.560]   Awesome. I believe we're live. Welcome back, everyone. Wade is
[00:00:26.560 --> 00:00:30.320]   back. He wasn't well last week. Now he's in awesome health. So
[00:00:30.320 --> 00:00:33.000]   that's, I was happy to hear that. And I hope you are as
[00:00:33.000 --> 00:00:37.280]   well, because he'll be teaching us how to translate stuff and
[00:00:37.280 --> 00:00:40.640]   how to summarize models. If I can learn this from him, I'll
[00:00:40.640 --> 00:00:43.360]   take his model and deploy it on my podcast and give you all
[00:00:43.360 --> 00:00:46.280]   summaries that you have been asking for and I haven't
[00:00:46.280 --> 00:00:49.400]   lazily produced. So I'm excited to learn that from Wade and
[00:00:49.400 --> 00:00:50.560]   Wade, that over to you.
[00:00:50.560 --> 00:00:55.320]   Well, guess what I just tried sharing my screen. And it says
[00:00:55.320 --> 00:01:00.880]   Chrome has lost permissions to sharing our screen recording. So
[00:01:00.880 --> 00:01:04.560]   let me fix that real quick. And Sonia, I'm gonna let you keep
[00:01:04.560 --> 00:01:07.640]   talking about whatever you want to talk about why I do.
[00:01:07.640 --> 00:01:09.720]   Apologies.
[00:01:09.720 --> 00:01:13.080]   No problem. It happens with updates sometimes, right?
[00:01:13.080 --> 00:01:16.480]   Yeah, of course. No, I didn't check. I just assumed we'd all
[00:01:16.480 --> 00:01:20.240]   be good to go. And we're not. So let's see here.
[00:01:20.240 --> 00:01:24.880]   Has anyone done the homework or written any blog posts in the
[00:01:24.880 --> 00:01:26.840]   meantime, but you're great to see you.
[00:01:26.840 --> 00:01:43.080]   We'll get started in a minute or two while we figure this out.
[00:01:44.080 --> 00:01:47.120]   Anyone who joined the previous session, this is better than
[00:01:47.120 --> 00:01:50.520]   getting zoom bombed like we caught last time. I shouldn't
[00:01:50.520 --> 00:01:54.200]   mention it, but it's not as bad as that.
[00:01:54.200 --> 00:01:57.720]   Okay, so I'm gonna go ahead and start the session.
[00:01:57.720 --> 00:02:00.120]   Okay, so I'm gonna go ahead and start the session.
[00:02:00.120 --> 00:02:02.600]   Okay, so I'm gonna go ahead and start the session.
[00:02:02.600 --> 00:02:04.920]   Okay, so I'm gonna go ahead and start the session.
[00:02:04.920 --> 00:02:07.320]   Okay, so I'm gonna go ahead and start the session.
[00:02:07.320 --> 00:02:09.520]   Okay, so I'm gonna go ahead and start the session.
[00:02:09.520 --> 00:02:11.720]   Okay, so I'm gonna go ahead and start the session.
[00:02:11.720 --> 00:02:14.120]   Okay, so I'm gonna go ahead and start the session.
[00:02:14.120 --> 00:02:16.600]   Okay, so I'm gonna go ahead and start the session.
[00:02:16.600 --> 00:02:19.160]   Okay, so I'm gonna go ahead and start the session.
[00:02:19.160 --> 00:02:21.560]   Okay, so I'm gonna go ahead and start the session.
[00:02:21.560 --> 00:02:23.880]   Okay, so I'm gonna go ahead and start the session.
[00:02:23.880 --> 00:02:26.200]   Okay, so I'm gonna go ahead and start the session.
[00:02:26.200 --> 00:02:28.520]   Okay, so I'm gonna go ahead and start the session.
[00:02:28.520 --> 00:02:30.840]   Okay, so I'm gonna go ahead and start the session.
[00:02:30.840 --> 00:02:33.080]   Okay, so I'm gonna go ahead and start the session.
[00:02:33.080 --> 00:02:35.320]   Okay, so I'm gonna go ahead and start the session.
[00:02:35.320 --> 00:02:37.640]   Okay, so I'm gonna go ahead and start the session.
[00:02:37.640 --> 00:02:39.800]   Okay, so I'm gonna go ahead and start the session.
[00:02:39.800 --> 00:02:42.200]   Okay, so I'm gonna go ahead and start the session.
[00:02:42.200 --> 00:02:44.600]   Okay, so I'm gonna go ahead and start the session.
[00:02:44.600 --> 00:02:47.000]   Okay, so I'm gonna go ahead and start the session.
[00:02:47.000 --> 00:02:49.400]   Okay, so I'm gonna go ahead and start the session.
[00:02:49.400 --> 00:02:51.800]   Okay, so I'm gonna go ahead and start the session.
[00:02:51.800 --> 00:02:54.200]   Okay, so I'm gonna go ahead and start the session.
[00:02:54.200 --> 00:02:56.600]   Okay, so I'm gonna go ahead and start the session.
[00:02:56.600 --> 00:02:59.000]   Okay, so I'm gonna go ahead and start the session.
[00:02:59.000 --> 00:03:01.400]   Okay, so I'm gonna go ahead and start the session.
[00:03:01.400 --> 00:03:03.800]   Okay, so I'm gonna go ahead and start the session.
[00:03:03.800 --> 00:03:06.200]   Okay, so I'm gonna go ahead and start the session.
[00:03:06.200 --> 00:03:08.600]   Okay, so I'm gonna go ahead and start the session.
[00:03:08.600 --> 00:03:11.000]   Okay, so I'm gonna go ahead and start the session.
[00:03:11.000 --> 00:03:13.400]   Okay, so I'm gonna go ahead and start the session.
[00:03:13.400 --> 00:03:15.800]   Okay, so I'm gonna go ahead and start the session.
[00:03:15.800 --> 00:03:18.200]   Okay, so I'm gonna go ahead and start the session.
[00:03:18.200 --> 00:03:20.600]   Okay, so I'm gonna go ahead and start the session.
[00:03:20.600 --> 00:03:23.000]   Okay, so I'm gonna go ahead and start the session.
[00:03:23.000 --> 00:03:25.400]   Okay, so I'm gonna go ahead and start the session.
[00:03:25.400 --> 00:03:27.800]   Okay, so I'm gonna go ahead and start the session.
[00:03:27.800 --> 00:03:30.200]   Okay, so I'm gonna go ahead and start the session.
[00:03:30.200 --> 00:03:32.600]   Okay, so I'm gonna go ahead and start the session.
[00:03:32.600 --> 00:03:35.000]   Okay, so I'm gonna go ahead and start the session.
[00:03:35.000 --> 00:03:37.400]   Okay, so I'm gonna go ahead and start the session.
[00:03:37.400 --> 00:03:40.200]   Okay, so I'm gonna go ahead and start the session.
[00:03:40.200 --> 00:03:43.000]   Okay, so I'm gonna go ahead and start the session.
[00:03:43.000 --> 00:03:45.800]   Okay, so I'm gonna go ahead and start the session.
[00:03:45.800 --> 00:03:48.600]   Okay, so I'm gonna go ahead and start the session.
[00:03:48.600 --> 00:03:51.000]   Okay, so I'm gonna go ahead and start the session.
[00:03:51.000 --> 00:03:53.800]   Okay, so I'm gonna go ahead and start the session.
[00:03:53.800 --> 00:03:56.200]   Okay, so I'm gonna go ahead and start the session.
[00:03:56.200 --> 00:03:59.000]   Okay, so I'm gonna go ahead and start the session.
[00:03:59.000 --> 00:04:01.800]   Okay, so I'm gonna go ahead and start the session.
[00:04:01.800 --> 00:04:04.200]   Okay, so I'm gonna go ahead and start the session.
[00:04:04.200 --> 00:04:07.000]   Okay, so I'm gonna go ahead and start the session.
[00:04:07.000 --> 00:04:09.800]   Okay, so I'm gonna go ahead and start the session.
[00:04:09.800 --> 00:04:12.600]   Okay, so I'm gonna go ahead and start the session.
[00:04:12.600 --> 00:04:15.400]   Okay, so I'm gonna go ahead and start the session.
[00:04:15.400 --> 00:04:18.200]   Okay, so I'm gonna go ahead and start the session.
[00:04:18.200 --> 00:04:21.000]   Okay, so I'm gonna go ahead and start the session.
[00:04:21.000 --> 00:04:23.400]   Okay, so I'm gonna go ahead and start the session.
[00:04:23.400 --> 00:04:25.800]   Okay, so I'm gonna go ahead and start the session.
[00:04:25.800 --> 00:04:28.600]   Okay, so I'm gonna go ahead and start the session.
[00:04:28.600 --> 00:04:31.400]   Okay, so I'm gonna go ahead and start the session.
[00:04:31.400 --> 00:04:33.800]   Okay, so I'm gonna go ahead and start the session.
[00:04:33.800 --> 00:04:36.600]   Okay, so I'm gonna go ahead and start the session.
[00:04:36.600 --> 00:04:39.400]   Okay, so I'm gonna go ahead and start the session.
[00:04:39.400 --> 00:04:42.200]   Okay, so I'm gonna go ahead and start the session.
[00:04:42.200 --> 00:04:45.000]   Okay, so I'm gonna go ahead and start the session.
[00:04:45.000 --> 00:04:47.800]   Okay, so I'm gonna go ahead and start the session.
[00:04:47.800 --> 00:04:50.600]   Okay, so I'm gonna go ahead and start the session.
[00:04:50.600 --> 00:04:53.400]   Okay, so I'm gonna go ahead and start the session.
[00:04:53.400 --> 00:04:56.200]   Okay, so I'm gonna go ahead and start the session.
[00:04:56.200 --> 00:04:59.000]   Okay, so I'm gonna go ahead and start the session.
[00:04:59.000 --> 00:05:01.800]   Okay, so I'm gonna go ahead and start the session.
[00:05:01.800 --> 00:05:05.000]   Okay, so I'm gonna go ahead and start the session.
[00:05:05.000 --> 00:05:08.200]   Okay, so I'm gonna go ahead and start the session.
[00:05:08.200 --> 00:05:11.400]   Okay, so I'm gonna go ahead and start the session.
[00:05:11.400 --> 00:05:14.600]   Okay, so I'm gonna go ahead and start the session.
[00:05:14.600 --> 00:05:17.800]   Okay, so I'm gonna go ahead and start the session.
[00:05:17.800 --> 00:05:21.000]   Okay, so I'm gonna go ahead and start the session.
[00:05:21.000 --> 00:05:23.800]   Okay, so I'm gonna go ahead and start the session.
[00:05:23.800 --> 00:05:26.600]   Okay, so I'm gonna go ahead and start the session.
[00:05:26.600 --> 00:05:29.400]   Okay, so I'm gonna go ahead and start the session.
[00:05:29.400 --> 00:05:32.200]   Okay, so I'm gonna go ahead and start the session.
[00:05:32.200 --> 00:05:34.600]   Okay, so I'm gonna go ahead and start the session.
[00:05:34.600 --> 00:05:37.800]   Okay, so I'm gonna go ahead and start the session.
[00:05:37.800 --> 00:05:41.000]   Okay, so I'm gonna go ahead and start the session.
[00:05:41.000 --> 00:05:43.800]   Okay, so I'm gonna go ahead and start the session.
[00:05:43.800 --> 00:05:47.000]   Okay, so I'm gonna go ahead and start the session.
[00:05:47.000 --> 00:05:50.200]   Okay, so I'm gonna go ahead and start the session.
[00:05:50.200 --> 00:05:53.400]   Okay, so I'm gonna go ahead and start the session.
[00:05:53.400 --> 00:05:56.600]   Okay, so I'm gonna go ahead and start the session.
[00:05:56.600 --> 00:05:59.800]   Okay, so I'm gonna go ahead and start the session.
[00:05:59.800 --> 00:06:03.000]   Okay, so I'm gonna go ahead and start the session.
[00:06:03.000 --> 00:06:06.200]   Okay, so I'm gonna go ahead and start the session.
[00:06:06.200 --> 00:06:09.400]   Okay, so I'm gonna go ahead and start the session.
[00:06:09.400 --> 00:06:12.600]   Okay, so I'm gonna go ahead and start the session.
[00:06:12.600 --> 00:06:15.800]   Okay, so I'm gonna go ahead and start the session.
[00:06:15.800 --> 00:06:19.400]   Okay, so I'm gonna go ahead and start the session.
[00:06:19.400 --> 00:06:22.600]   Okay, so I'm gonna go ahead and start the session.
[00:06:22.600 --> 00:06:25.800]   Okay, so I'm gonna go ahead and start the session.
[00:06:25.800 --> 00:06:29.000]   Okay, so I'm gonna go ahead and start the session.
[00:06:29.000 --> 00:06:32.200]   Okay, so I'm gonna go ahead and start the session.
[00:06:32.200 --> 00:06:35.400]   Okay, so I'm gonna go ahead and start the session.
[00:06:35.400 --> 00:06:39.000]   Okay, so I'm gonna go ahead and start the session.
[00:06:39.000 --> 00:06:42.600]   Okay, so I'm gonna go ahead and start the session.
[00:06:42.600 --> 00:06:46.200]   Okay, so I'm gonna go ahead and start the session.
[00:06:46.200 --> 00:06:49.800]   Okay, so I'm gonna go ahead and start the session.
[00:06:49.800 --> 00:06:53.400]   Okay, so I'm gonna go ahead and start the session.
[00:06:53.400 --> 00:06:57.000]   Okay, so I'm gonna go ahead and start the session.
[00:06:57.000 --> 00:07:00.600]   Okay, so I'm gonna go ahead and start the session.
[00:07:00.600 --> 00:07:04.200]   Okay, so I'm gonna go ahead and start the session.
[00:07:04.200 --> 00:07:07.400]   Okay, so I'm gonna go ahead and start the session.
[00:07:07.400 --> 00:07:11.000]   Okay, so I'm gonna go ahead and start the session.
[00:07:11.000 --> 00:07:14.600]   Okay, so I'm gonna go ahead and start the session.
[00:07:14.600 --> 00:07:18.200]   Okay, so I'm gonna go ahead and start the session.
[00:07:18.200 --> 00:07:21.800]   Okay, so I'm gonna go ahead and start the session.
[00:07:21.800 --> 00:07:25.400]   Okay, so I'm gonna go ahead and start the session.
[00:07:25.400 --> 00:07:29.000]   Okay, so I'm gonna go ahead and start the session.
[00:07:29.000 --> 00:07:32.600]   Okay, so I'm gonna go ahead and start the session.
[00:07:32.600 --> 00:07:36.600]   Okay, so I'm gonna go ahead and start the session.
[00:07:36.600 --> 00:07:40.200]   Okay, so I'm gonna go ahead and start the session.
[00:07:40.200 --> 00:07:43.800]   Okay, so I'm gonna go ahead and start the session.
[00:07:43.800 --> 00:07:47.400]   Okay, so I'm gonna go ahead and start the session.
[00:07:47.400 --> 00:07:51.000]   Okay, so I'm gonna go ahead and start the session.
[00:07:51.000 --> 00:07:54.600]   Okay, so I'm gonna go ahead and start the session.
[00:07:54.600 --> 00:07:58.200]   Okay, so I'm gonna go ahead and start the session.
[00:07:58.200 --> 00:08:01.800]   Okay, so I'm gonna go ahead and start the session.
[00:08:01.800 --> 00:08:05.800]   Okay, so I'm gonna go ahead and start the session.
[00:08:05.800 --> 00:08:09.400]   Okay, so I'm gonna go ahead and start the session.
[00:08:09.400 --> 00:08:13.000]   Okay, so I'm gonna go ahead and start the session.
[00:08:13.000 --> 00:08:16.600]   Okay, so I'm gonna go ahead and start the session.
[00:08:16.600 --> 00:08:20.600]   Okay, so I'm gonna go ahead and start the session.
[00:08:20.600 --> 00:08:24.600]   Okay, so I'm gonna go ahead and start the session.
[00:08:24.600 --> 00:08:27.800]   Okay, so I'm gonna go ahead and start the session.
[00:08:27.800 --> 00:08:31.400]   Okay, so I'm gonna go ahead and start the session.
[00:08:31.400 --> 00:08:35.400]   Okay, so I'm gonna go ahead and start the session.
[00:08:35.400 --> 00:08:39.400]   Okay, so I'm gonna go ahead and start the session.
[00:08:39.400 --> 00:08:43.400]   Okay, so I'm gonna go ahead and start the session.
[00:08:43.400 --> 00:08:47.400]   Okay, so I'm gonna go ahead and start the session.
[00:08:47.400 --> 00:08:51.400]   Okay, so I'm gonna go ahead and start the session.
[00:08:51.400 --> 00:08:55.400]   Okay, so I'm gonna go ahead and start the session.
[00:08:55.400 --> 00:08:59.400]   Okay, so I'm gonna go ahead and start the session.
[00:08:59.400 --> 00:09:03.400]   Okay, so I'm gonna go ahead and start the session.
[00:09:03.400 --> 00:09:07.400]   Okay, so I'm gonna go ahead and start the session.
[00:09:07.400 --> 00:09:11.400]   Okay, so I'm gonna go ahead and start the session.
[00:09:11.400 --> 00:09:15.400]   Okay, so I'm gonna go ahead and start the session.
[00:09:15.400 --> 00:09:19.400]   Okay, so I'm gonna go ahead and start the session.
[00:09:19.400 --> 00:09:23.400]   Okay, so I'm gonna go ahead and start the session.
[00:09:23.400 --> 00:09:27.400]   Okay, so I'm gonna go ahead and start the session.
[00:09:27.400 --> 00:09:31.400]   Okay, so I'm gonna go ahead and start the session.
[00:09:31.400 --> 00:09:35.400]   Okay, so I'm gonna go ahead and start the session.
[00:09:35.400 --> 00:09:39.400]   Okay, so I'm gonna go ahead and start the session.
[00:09:39.400 --> 00:09:43.400]   Okay, so I'm gonna go ahead and start the session.
[00:09:43.400 --> 00:09:47.400]   Okay, so I'm gonna go ahead and start the session.
[00:09:47.400 --> 00:09:51.400]   Okay, so I'm gonna go ahead and start the session.
[00:09:51.400 --> 00:09:55.400]   Okay, so I'm gonna go ahead and start the session.
[00:09:55.400 --> 00:09:59.400]   Okay, so I'm gonna go ahead and start the session.
[00:09:59.400 --> 00:10:03.400]   Okay, so I'm gonna go ahead and start the session.
[00:10:03.400 --> 00:10:07.400]   Okay, so I'm gonna go ahead and start the session.
[00:10:07.400 --> 00:10:11.400]   Okay, so I'm gonna go ahead and start the session.
[00:10:11.400 --> 00:10:15.400]   Okay, so I'm gonna go ahead and start the session.
[00:10:15.400 --> 00:10:19.400]   Okay, so I'm gonna go ahead and start the session.
[00:10:19.400 --> 00:10:23.400]   Okay, so I'm gonna go ahead and start the session.
[00:10:23.400 --> 00:10:27.400]   Okay, so I'm gonna go ahead and start the session.
[00:10:27.400 --> 00:10:31.400]   Okay, so I'm gonna go ahead and start the session.
[00:10:31.400 --> 00:10:35.400]   Okay, so I'm gonna go ahead and start the session.
[00:10:35.400 --> 00:10:39.400]   Okay, so I'm gonna go ahead and start the session.
[00:10:39.400 --> 00:10:43.400]   Okay, so I'm gonna go ahead and start the session.
[00:10:43.400 --> 00:10:47.400]   Okay, so I'm gonna go ahead and start the session.
[00:10:47.400 --> 00:10:51.400]   Okay, so I'm gonna go ahead and start the session.
[00:10:51.400 --> 00:10:55.400]   Okay, so I'm gonna go ahead and start the session.
[00:10:55.400 --> 00:10:59.400]   Okay, so I'm gonna go ahead and start the session.
[00:10:59.400 --> 00:11:03.400]   Okay, so I'm gonna go ahead and start the session.
[00:11:03.400 --> 00:11:07.400]   Okay, so I'm gonna go ahead and start the session.
[00:11:07.400 --> 00:11:11.400]   Okay, so I'm gonna go ahead and start the session.
[00:11:11.400 --> 00:11:15.400]   Okay, so I'm gonna go ahead and start the session.
[00:11:15.400 --> 00:11:19.400]   Okay, so I'm gonna go ahead and start the session.
[00:11:19.400 --> 00:11:23.400]   Okay, so I'm gonna go ahead and start the session.
[00:11:23.400 --> 00:11:27.400]   Okay, so I'm gonna go ahead and start the session.
[00:11:27.400 --> 00:11:31.400]   Okay, so I'm gonna go ahead and start the session.
[00:11:31.400 --> 00:11:35.400]   Okay, so I'm gonna go ahead and start the session.
[00:11:35.400 --> 00:11:39.400]   Okay, so I'm gonna go ahead and start the session.
[00:11:39.400 --> 00:11:43.400]   Okay, so I'm gonna go ahead and start the session.
[00:11:43.400 --> 00:11:47.400]   Okay, so I'm gonna go ahead and start the session.
[00:11:47.400 --> 00:11:51.400]   Okay, so I'm gonna go ahead and start the session.
[00:11:51.400 --> 00:11:55.400]   Okay, so I'm gonna go ahead and start the session.
[00:11:55.400 --> 00:11:59.400]   Okay, so I'm gonna go ahead and start the session.
[00:11:59.400 --> 00:12:03.400]   Okay, so I'm gonna go ahead and start the session.
[00:12:03.400 --> 00:12:07.400]   Okay, so I'm gonna go ahead and start the session.
[00:12:07.400 --> 00:12:11.400]   Okay, so I'm gonna go ahead and start the session.
[00:12:11.400 --> 00:12:15.400]   Okay, so I'm gonna go ahead and start the session.
[00:12:15.400 --> 00:12:19.400]   Okay, so I'm gonna go ahead and start the session.
[00:12:19.400 --> 00:12:23.400]   Okay, so I'm gonna go ahead and start the session.
[00:12:23.400 --> 00:12:27.400]   Okay, so I'm gonna go ahead and start the session.
[00:12:27.400 --> 00:12:31.400]   Okay, so I'm gonna go ahead and start the session.
[00:12:31.400 --> 00:12:35.400]   Okay, so I'm gonna go ahead and start the session.
[00:12:35.400 --> 00:12:39.400]   Okay, so I'm gonna go ahead and start the session.
[00:12:39.400 --> 00:12:43.400]   Okay, so I'm gonna go ahead and start the session.
[00:12:43.400 --> 00:12:47.400]   Okay, so I'm gonna go ahead and start the session.
[00:12:47.400 --> 00:12:51.400]   Okay, so I'm gonna go ahead and start the session.
[00:12:51.400 --> 00:12:55.400]   Okay, so I'm gonna go ahead and start the session.
[00:12:55.400 --> 00:12:59.400]   Okay, so I'm gonna go ahead and start the session.
[00:12:59.400 --> 00:13:03.400]   Okay, so I'm gonna go ahead and start the session.
[00:13:03.400 --> 00:13:07.400]   Okay, so I'm gonna go ahead and start the session.
[00:13:07.400 --> 00:13:11.400]   Okay, so I'm gonna go ahead and start the session.
[00:13:11.400 --> 00:13:15.400]   Okay, so I'm gonna go ahead and start the session.
[00:13:15.400 --> 00:13:19.400]   Okay, so I'm gonna go ahead and start the session.
[00:13:19.400 --> 00:13:23.400]   Okay, so I'm gonna go ahead and start the session.
[00:13:23.400 --> 00:13:27.400]   Okay, so I'm gonna go ahead and start the session.
[00:13:27.400 --> 00:13:31.400]   Okay, so I'm gonna go ahead and start the session.
[00:13:31.400 --> 00:13:35.400]   Okay, so I'm gonna go ahead and start the session.
[00:13:35.400 --> 00:13:39.400]   Okay, so I'm going to work with a data frame.
[00:13:39.400 --> 00:13:43.400]   Blur also can work with data sets, but just for the sake of this example,
[00:13:43.400 --> 00:13:49.400]   I'm just gonna convert it to a data frame and use that as the inputs.
[00:13:49.400 --> 00:13:57.400]   And as I mentioned, this particular data set has articles and highlights.
[00:13:57.400 --> 00:14:03.400]   And the goal here for this particular model that we're gonna train
[00:14:03.400 --> 00:14:09.400]   is be able to build a highlight summary given an article.
[00:14:09.400 --> 00:14:12.400]   And so anyways, that's our data set.
[00:14:12.400 --> 00:14:17.400]   And one other thing I'll mention is that when we're doing text generation,
[00:14:17.400 --> 00:14:19.400]   we also want to kind of look at like how these things start.
[00:14:19.400 --> 00:14:25.400]   And sometimes there's like little funny like prefixes and things like that on the highlights.
[00:14:25.400 --> 00:14:32.400]   And so for generating text, we may want to make sure that we kind of seed that generation with that prefix.
[00:14:32.400 --> 00:14:35.400]   So that's something that we can touch on later.
[00:14:35.400 --> 00:14:39.400]   But just be aware that when we get into text generation,
[00:14:39.400 --> 00:14:46.400]   I'm going to provide some resources for you to look at how you define those parameters and so forth
[00:14:46.400 --> 00:14:53.400]   is going to be dependent on the type of summaries that you want to build.
[00:14:53.400 --> 00:14:58.400]   But anyways, yeah, so we're going to go ahead and look at articles and try to predict highlights.
[00:14:58.400 --> 00:15:03.400]   So we have a data set where inputs and labels are both text.
[00:15:03.400 --> 00:15:07.400]   The next thing we want to do is build our hugging face objects.
[00:15:07.400 --> 00:15:16.400]   And just we've already kind of looked at where the encoder and decoder models work well.
[00:15:16.400 --> 00:15:21.400]   For sequence to sequence tasks, generally, the encoder decoder models
[00:15:21.400 --> 00:15:30.400]   or what they call sequence to sequence models are good for generating generative tasks that also require
[00:15:30.400 --> 00:15:34.400]   and a good representation of some input text.
[00:15:34.400 --> 00:15:41.400]   And so that's exactly what we have in the case of translation or summarization.
[00:15:41.400 --> 00:15:47.400]   And so if you think about it intuitively, and if any of this seems foreign to you,
[00:15:47.400 --> 00:15:56.400]   go back through the first section in the hugging face course, because it talks about encoder only, decoder only,
[00:15:56.400 --> 00:15:58.400]   encoder decoder models.
[00:15:58.400 --> 00:16:03.400]   It gives some examples and it talks about where each of them thrive.
[00:16:03.400 --> 00:16:10.400]   And so in summary, when you look back at that particular section, the encoder only models.
[00:16:10.400 --> 00:16:13.400]   So that would be something like BERT.
[00:16:13.400 --> 00:16:18.400]   They thrive in really creating a good representation of your input text.
[00:16:18.400 --> 00:16:24.400]   And those are really key for things like classification, sequence or token, question answering,
[00:16:24.400 --> 00:16:30.400]   where the representation is key to be able to find the -- to identify what a token is
[00:16:30.400 --> 00:16:34.400]   or where a answer is to a question.
[00:16:34.400 --> 00:16:41.400]   Whereas when you're generating text, this is where the decoder only models thrive.
[00:16:41.400 --> 00:16:52.400]   And they use a tension mechanism called mass self-attention, where as they generate tokens one by one,
[00:16:52.400 --> 00:16:54.400]   they ensure that the decoder can't cheat.
[00:16:54.400 --> 00:17:02.400]   It can't look at all the tokens in the inputs and apply tension.
[00:17:02.400 --> 00:17:08.400]   They can only apply tension to previous tokens that have already essentially been predicted.
[00:17:08.400 --> 00:17:15.400]   And so given that, it makes sense that for a task like summarization or translation,
[00:17:15.400 --> 00:17:23.400]   where we're trying to generate text, but we want to be meaningful and it has a relationship to the input text,
[00:17:23.400 --> 00:17:28.400]   that we then use this -- the more traditional transformer architecture,
[00:17:28.400 --> 00:17:34.400]   where we have an encoder that builds a really good representation of our inputs,
[00:17:34.400 --> 00:17:41.400]   from which we can actually then utilize in building our outputs, our targets.
[00:17:41.400 --> 00:17:51.400]   And so that's why typically for these tasks, we'll use an encoder/decoder architecture.
[00:17:51.400 --> 00:18:02.400]   In terms of models to consider, I've highlighted models that I've used with really good success.
[00:18:02.400 --> 00:18:12.400]   And in terms on the summarization side, Pegasus, T5, and BART have all worked well for me.
[00:18:12.400 --> 00:18:20.400]   If you're working with a multilingual corpus, you can look at using MT5 or MBART50.
[00:18:20.400 --> 00:18:29.400]   And just be aware that when you're looking at the T5 and MT5 variants, how you prepare your inputs is not the same.
[00:18:29.400 --> 00:18:33.400]   So like with T5, it's a text-to-text transformer.
[00:18:33.400 --> 00:18:40.400]   And so for summarization, you add the prefix "summarize" and then whatever your text is.
[00:18:40.400 --> 00:18:47.400]   And MT5, even though it's based on T5, you don't include that particular prefix.
[00:18:47.400 --> 00:18:56.400]   And so really key when you're using these is to spend some time at least looking at some of the summary documentation on Hugging Face,
[00:18:56.400 --> 00:19:09.400]   or read the papers and at least get an idea of how these different architectures work and what they require in terms of your data inputs,
[00:19:09.400 --> 00:19:12.400]   which are key, of course, to getting good results.
[00:19:12.400 --> 00:19:24.400]   On the translation side, the Marion MT, which you'll see as the Helsinki NLP models in Hugging Face, are outstanding.
[00:19:24.400 --> 00:19:31.400]   I definitely, in terms of strength and weakness, I've done a lot more with summarization than translation.
[00:19:31.400 --> 00:19:36.400]   But with what I have done, the Marion MT models are outstanding.
[00:19:36.400 --> 00:19:48.400]   And if you look at the Blur documentation on the translation bits, you'll see a machine translation example in there that utilizes one of these models.
[00:19:48.400 --> 00:19:53.400]   And it's just ridiculous in how well it actually works.
[00:19:53.400 --> 00:20:04.400]   So again, to get the models for this particular example, I'm just going to work with a distilled version of BART.
[00:20:04.400 --> 00:20:11.400]   Again, when you're building your training loop, you typically want to try to work with a small model and a small data set
[00:20:11.400 --> 00:20:24.400]   so that you can quickly verify that your training evaluation, your prediction, your inference bits all work properly before you waste hours and days.
[00:20:24.400 --> 00:20:30.400]   And so for me, I really like the BART models.
[00:20:30.400 --> 00:20:35.400]   And again, I'm working mostly with an English corpus. And they've not only proven it really well for summarization,
[00:20:35.400 --> 00:20:42.400]   but you can actually use BART for sequence classification. And the results are actually been pretty outstanding.
[00:20:42.400 --> 00:20:48.400]   Whereas if someone asked me, what's the one model you would use if you could only choose one transformer?
[00:20:48.400 --> 00:20:52.400]   I would probably choose some variation of BART.
[00:20:52.400 --> 00:21:03.400]   And so with Blur, again, it's super easy to set up. We can use the Blur utility object, call getHuggingFaceObjects.
[00:21:03.400 --> 00:21:13.400]   And here, since we're working with BART, we could use the auto model for conditional generation.
[00:21:13.400 --> 00:21:20.400]   But I just want to show that you can also specifically say we want to use the BART for conditional generation model,
[00:21:20.400 --> 00:21:29.400]   which works well here because we're using BART. And with that, we can then look at all the bits returned by Blur,
[00:21:29.400 --> 00:21:41.400]   verify that we're getting the right tokenizer, the right model, the right configuration, and then a string representation of the architecture.
[00:21:41.400 --> 00:21:46.400]   So we have our data set. We have our HuggingFaceObjects.
[00:21:46.400 --> 00:21:53.400]   The next question that we ask ourselves is what, if any, pre-processing do we need to do with the raw data?
[00:21:53.400 --> 00:22:01.400]   And the reality is, in general, you really don't need to do any pre-processing when using Blur.
[00:22:01.400 --> 00:22:15.400]   But if you wanted to do something that would be meaningful for some outside of truncating the lengths of the inputs and the targets,
[00:22:15.400 --> 00:22:22.400]   for summarization, you may want to exclude inputs where the summaries aren't long enough to be helpful.
[00:22:22.400 --> 00:22:29.400]   And in the particular example used in the course, it's using Amazon reviews.
[00:22:29.400 --> 00:22:37.400]   And so some of the titles which they're using as the targets are like bad or great.
[00:22:37.400 --> 00:22:50.400]   And so they really don't provide a lot of helpfulness when you're actually training and evaluating your model to actually build something that's meaningful.
[00:22:50.400 --> 00:22:56.400]   And you're also potentially, if there's a lot of those type of very short summaries,
[00:22:56.400 --> 00:23:04.400]   you could bias the generation to generating very short summaries when it should actually probably be a little bit verbose.
[00:23:04.400 --> 00:23:11.400]   So the lesson to learn here is that if you're building a summarization model,
[00:23:11.400 --> 00:23:19.400]   what you want to do is take some time to look at the number of words in your summaries and also your inputs,
[00:23:19.400 --> 00:23:39.400]   but in particular, your summaries, and tune the generation hyperparameters and prepare your data set so that how short or how long the summaries are going to coincide with what you're expecting in the real world when you want to use this for inference.
[00:23:39.400 --> 00:23:46.400]   So in the course, they kind of go through an example of actually truncating summaries that are only one or two words long.
[00:23:46.400 --> 00:23:54.400]   So that's just something to consider. But in general, you don't need to do any pre-processing to start building a model.
[00:23:54.400 --> 00:24:00.400]   And one of the things, you know, if you've taken the Fast AI course or read the book that Jeremy espouses,
[00:24:00.400 --> 00:24:07.400]   is to not worry so much about the pre-processing initially, like build something and see what the results are.
[00:24:07.400 --> 00:24:11.400]   A lot of times you can learn about your corpus by looking at those results.
[00:24:11.400 --> 00:24:16.400]   So and that holds true, especially for translation and summarization tasks,
[00:24:16.400 --> 00:24:22.400]   because there's really nothing that you're mandated to do to actually make your data loaders.
[00:24:22.400 --> 00:24:34.400]   So for the sake of argument, just to show you that you can do this, we do have like summarization and translation pre-processors in Blur.
[00:24:34.400 --> 00:24:41.400]   And in this example, I just said, OK, I want to these highlights are long, right?
[00:24:41.400 --> 00:24:50.400]   There's there are multiple sentences. And so I'm going to ensure that if there's any summaries that are too small,
[00:24:50.400 --> 00:24:55.400]   so there are less than 30 characters, I'm going to get rid of those examples.
[00:24:55.400 --> 00:24:59.400]   And again, this is just to illustrate how it can be done.
[00:24:59.400 --> 00:25:11.400]   I'm also just for the sake of building something quickly, I'm going to limit the number of tokens in the input and also in the targets.
[00:25:11.400 --> 00:25:18.400]   And if you don't, it will use whatever the defaults are for your particular architecture.
[00:25:18.400 --> 00:25:28.400]   And you can typically find this by looking at the config file that you get when you use the Blur.getHF objects.
[00:25:28.400 --> 00:25:34.400]   But again, I'm going to just limit these to essentially speed up this example.
[00:25:34.400 --> 00:25:38.400]   And the pre-processors work just like all the other pre-processors.
[00:25:38.400 --> 00:25:42.400]   We just go ahead and run our data set or data frame in there.
[00:25:42.400 --> 00:25:51.400]   And we're going to get proc_versions of the things that we identify as our input text and our target text.
[00:25:51.400 --> 00:26:01.400]   And then we can use that in actually our data block.
[00:26:01.400 --> 00:26:07.400]   So with the data block, we actually get some new bits in Blur.
[00:26:07.400 --> 00:26:16.400]   And essentially what we have is we've defined a mid-level API.
[00:26:16.400 --> 00:26:27.400]   And so whereas before when we're using the other tasks, we were using just the text blocks and we are using transforms in there,
[00:26:27.400 --> 00:26:34.400]   batch tokenized transform and decode transform that were really meant for handling just inputs as text.
[00:26:34.400 --> 00:26:40.400]   So in Blur, we also have a mid-level API for sequence-to-sequence tasks.
[00:26:40.400 --> 00:26:45.400]   And the key objects in there are the sequence-to-sequence text block.
[00:26:45.400 --> 00:26:52.400]   And essentially what this does is handle not just preparing your inputs, but your targets as well.
[00:26:52.400 --> 00:26:56.400]   And we'll take a look at the sequence-to-sequence batch tokenized transform in just a second.
[00:26:56.400 --> 00:27:05.400]   So you can see some of the things that are happening in there that are important to know when you're actually building these type of models.
[00:27:05.400 --> 00:27:28.400]   And the big thing is, is that when we are using an encoder-to-decoder architecture, we have to not just have -- we're not just going to have labels like classes we're going to predict or spans where an answer of a question sits.
[00:27:28.400 --> 00:27:32.400]   We're actually going to have essentially two kinds of inputs, right?
[00:27:32.400 --> 00:27:38.400]   Because we have an encoder that's going to need some inputs and we have a decoder that's going to need some inputs too when we're training.
[00:27:38.400 --> 00:27:41.400]   And the decoder inputs are going to be the labels.
[00:27:41.400 --> 00:28:04.400]   And so with encoder-to-decoder -- with encoder and decoder models is we have inputs, again, for the encoder, inputs that we need to set up for the decoder, and then we also have the labels that we're trying -- our predictions and the labels that we want to measure those predictions against.
[00:28:04.400 --> 00:28:20.400]   And in sequence-to-sequence tasks, the key thing to remember is that the inputs to your decoder and in transformer models, these are typically going to be included in this decoder input IDs.
[00:28:20.400 --> 00:28:37.400]   They're simply your labels that are shifted to the right.
[00:28:37.400 --> 00:28:57.400]   So we're going to have a decoder that's going to be able to look at all of the information that's coming in.
[00:28:57.400 --> 00:29:26.400]   And then we're going to have a decoder that's going to be able to look at all of the information that's coming in.
[00:29:26.400 --> 00:29:31.400]   And so this is all handled with Blur, and we'll take a look at how that works.
[00:29:31.400 --> 00:29:48.400]   And with HuggingFace is that if you actually just assign the labels, which is the default in Blur, HuggingFace not only in this case will calculate the loss for you, but it will also create the decoder input IDs for you automatically.
[00:29:48.400 --> 00:29:51.400]   So you don't even need to do that.
[00:29:51.400 --> 00:29:57.400]   And we'll take a look at the batch tokenized transform for sequence-to-sequence.
[00:29:57.400 --> 00:30:00.400]   You can see how that happens in Blur.
[00:30:00.400 --> 00:30:20.400]   Also be aware that when you're using models, especially like multilingual models, some of them require additional information be set on the tokenizer and/or included in the tokenization process when you call tokenizer to tokenize a chunk of text as a batch.
[00:30:20.400 --> 00:30:37.400]   And so, again, a really key thing here, and this is some of the things that I really had to learn the hard way, is you really want to look at the documentation in HuggingFace and even look at the papers to make sure that you're setting things up correctly.
[00:30:37.400 --> 00:30:42.400]   Because if you don't use the proper prefixes or set some of the stuff up, your model will still train.
[00:30:42.400 --> 00:30:44.400]   You just won't get good results.
[00:30:44.400 --> 00:30:48.400]   And so just be aware of that.
[00:30:48.400 --> 00:31:01.400]   And so we'll go ahead and before I do this, I want to actually look at just setting up the data block.
[00:31:01.400 --> 00:31:13.400]   I want to pull up this so you can see kind of some of the things I was just talking about.
[00:31:13.400 --> 00:31:22.400]   So this is our sequence-to-sequence variation of our batch tokenized transform that I was just mentioning.
[00:31:22.400 --> 00:31:26.400]   And you can see there's actually some interesting bits here.
[00:31:26.400 --> 00:31:33.400]   So when we're doing sequence-to-sequence again, both our source and our targets are going to be text.
[00:31:33.400 --> 00:31:44.400]   And one of the things you'll see in the course and you'll see in Blur is that after tokenizing the inputs, we actually use a context manager.
[00:31:44.400 --> 00:31:52.400]   And we tell our tokenizer, "Okay, I want you to function as the target tokenizer."
[00:31:52.400 --> 00:31:56.400]   And then we tokenize our target text.
[00:31:56.400 --> 00:32:01.400]   And you may be wondering, like, why is this important if we're using the same tokenizer?
[00:32:01.400 --> 00:32:06.400]   Why do we need to do this and put it into a context manager?
[00:32:06.400 --> 00:32:11.400]   And the reason is that different models work in different ways.
[00:32:11.400 --> 00:32:24.400]   And, for example, how the target text is tokenized and the special tokens that are used could be different than how they're used in building the inputs.
[00:32:24.400 --> 00:32:34.400]   For machine translation, it actually may use a completely different tokenizer for the targets than it does for the inputs.
[00:32:34.400 --> 00:32:42.400]   And to ensure that the targets are tokenized the right way, you have to use this context manager.
[00:32:42.400 --> 00:32:55.400]   And essentially, you're setting your tokenizer into target mode to make sure that the tokenization is appropriate for your targets when there are those differences.
[00:32:55.400 --> 00:33:12.400]   Once we have that, again, we want to make sure that when we're calculating our loss, that padding tokens, special tokens that shouldn't be considered are all set to negative 100.
[00:33:12.400 --> 00:33:18.400]   Or, like in Blur, we actually have it defined as a property, the ignore token ID.
[00:33:18.400 --> 00:33:28.400]   And so what we want to do is ensure that anything that's a padding token gets that negative 100.
[00:33:28.400 --> 00:33:33.400]   And so we use the mass fill in place that's in PyTorch to do that.
[00:33:33.400 --> 00:33:39.400]   And so now our inputs, wherever there's a padding token, is just going to be negative 100.
[00:33:39.400 --> 00:33:52.400]   And then we're going to look at different ways here in Blur how we can make this -- hopefully everybody can still see this well.
[00:33:52.400 --> 00:34:05.400]   We want to look at different ways that we need to prepare our inputs, depending on whether we're letting HuggingFace calculate the loss and build the decoder input IDs for us, or whether we're doing it.
[00:34:05.400 --> 00:34:12.400]   And so by default, Blur's set up to let HuggingFace do all the hard work.
[00:34:12.400 --> 00:34:21.400]   And in that case, we just need to set the labels key on our inputs to our targets, and then we're good to go.
[00:34:21.400 --> 00:34:26.400]   And as I mentioned, it's going to auto-calculate the decoder input IDs for us.
[00:34:26.400 --> 00:34:38.400]   But let's say we want to use a custom loss function, or we just want to use the standard FASTI-I bits and use cross-entropy flat loss.
[00:34:38.400 --> 00:34:42.400]   In that case, we need to actually create the decoder input IDs ourselves.
[00:34:42.400 --> 00:34:46.400]   And it's really not that difficult, as you'll see.
[00:34:46.400 --> 00:34:53.400]   First off, models use a different start token to indicate this is the beginning.
[00:34:53.400 --> 00:34:58.400]   And usually it's the pad token, but it's not in all cases.
[00:34:58.400 --> 00:35:10.400]   And so in Blur, we first look at the configuration object to see, is there a specific decoder start token ID that this particular model wants us to use?
[00:35:10.400 --> 00:35:25.400]   And then with that, we shift the inputs over to the right by one, and we replace that first token with whatever the decoder start token ID is.
[00:35:25.400 --> 00:35:27.400]   And that's it.
[00:35:27.400 --> 00:35:39.400]   And again, as I mentioned already, this is to facilitate the mass self-attention mechanism, where as we are generating one token at a time, right, generating our predictions,
[00:35:39.400 --> 00:35:47.400]   as that's happening, we can apply attention only to things that have been -- that are previous ground truth labels.
[00:35:47.400 --> 00:35:50.400]   We can't look at the future and cheat.
[00:35:50.400 --> 00:35:52.400]   And so that's why this is done.
[00:35:52.400 --> 00:35:56.400]   And so those are the key bits right there.
[00:35:56.400 --> 00:36:01.400]   Any questions popping up, Sanyam, on any of this yet?
[00:36:01.400 --> 00:36:04.400]   >> No, I don't see any questions so far.
[00:36:04.400 --> 00:36:06.400]   >> All right.
[00:36:06.400 --> 00:36:10.400]   Either that means I'm super clear or everybody is utterly confused.
[00:36:10.400 --> 00:36:13.400]   >> Things are clear for me, so I'm sure it's for the audience.
[00:36:13.400 --> 00:36:15.400]   >> As long as it makes sense to you.
[00:36:15.400 --> 00:36:18.400]   That's my metric.
[00:36:18.400 --> 00:36:28.400]   So anyway, so the next thing that's interesting is, again, when we're building these sequence-to-sequence models, we're going to be generating text.
[00:36:28.400 --> 00:36:42.400]   And when we generate text, like, there's actually a generation model or a generate function that we could call in these encoder/decoder models.
[00:36:42.400 --> 00:36:49.400]   And there's a bunch of different, like, parameters that you can use for generating text.
[00:36:49.400 --> 00:36:54.400]   And when you're doing that -- let me see here.
[00:36:54.400 --> 00:36:58.400]   I'm going to try and find something.
[00:36:58.400 --> 00:37:06.400]   And so one of the things is when you're actually generating text, it's like you need to, like, understand, like, what your objectives are.
[00:37:06.400 --> 00:37:08.400]   Like, is it really key to be exact?
[00:37:08.400 --> 00:37:21.400]   Do you want something that maybe can have more flexibility in choosing the tokens that are generated to make it sounding more like human versus machine?
[00:37:21.400 --> 00:37:25.400]   And a lot of this is going to be dependent on your dataset.
[00:37:25.400 --> 00:37:35.400]   But if you're looking for sensible defaults, look at the configuration object for whatever model you're using.
[00:37:35.400 --> 00:37:44.400]   And a lot of times you'll find defaults in here for text generation that are sensible and good to start with, if you're wondering.
[00:37:44.400 --> 00:37:51.400]   And also you'll find sometimes task-specific overrides.
[00:37:51.400 --> 00:38:01.400]   And so if you look at BART, it actually has the task-specific params for summarization.
[00:38:01.400 --> 00:38:19.400]   And basically these are parameters that you can pass to the generation function, which is -- what we're going to see is what we're going to use here when we do evaluation and inference.
[00:38:19.400 --> 00:38:25.400]   And the configuration object is saying, hey, these are sensible defaults to start with.
[00:38:25.400 --> 00:38:27.400]   So use them.
[00:38:27.400 --> 00:38:35.400]   And remember, if you're starting out and you're not sure, like, what to do, make sure that you look at the configuration object.
[00:38:35.400 --> 00:38:42.400]   And to make things a little bit simpler and blur, we actually have a method called default text gen quarks.
[00:38:42.400 --> 00:38:48.400]   And you pass a configuration, a model, and a task, like summarization or translation.
[00:38:48.400 --> 00:38:53.400]   And it will actually pull all those generation parameters out from the configuration.
[00:38:53.400 --> 00:38:58.400]   So you don't even have to look and bother and figure them all out.
[00:38:58.400 --> 00:39:06.400]   And it does this by actually inspecting the generation method of the model to pull the defaults out.
[00:39:06.400 --> 00:39:14.400]   And then it overrides them with any task-specific recommendations from the configuration object.
[00:39:14.400 --> 00:39:18.400]   So you can see here, like, max length and min length.
[00:39:18.400 --> 00:39:21.400]   The configuration object recommends 142 and 56.
[00:39:21.400 --> 00:39:23.400]   You can see that that's what's included here.
[00:39:23.400 --> 00:39:35.400]   And so you can actually include these text generation quarks as is when you actually build your data block, which we'll do right now.
[00:39:35.400 --> 00:39:51.400]   And so, again, one of the differences with the sequence-to-sequence bits over the bits that we've seen already is that we can actually now include these text gen quarks that will affect the generation.
[00:39:51.400 --> 00:40:06.400]   And, again, the generation is how we actually generate text is going to be important because it's going to be the generated predictions that are measured against our reference summaries or our reference translation tasks.
[00:40:06.400 --> 00:40:14.400]   And so we can go ahead and pass these in here into our sequence-to-sequence batch tokenized transform.
[00:40:14.400 --> 00:40:30.400]   And as we already saw in the seek-to-seek batch tokenized transform, it does the work of not only preparing your inputs but also your targets.
[00:40:30.400 --> 00:40:35.400]   And so with Fast.ai, we're used to creating these blocks, right, for our inputs and targets.
[00:40:35.400 --> 00:40:47.400]   Well, since we already have the seek-to-seek text block preparing both our inputs and targets, we really don't need a block that does any -- applies any transformations to our targets.
[00:40:47.400 --> 00:40:58.400]   And so in Fast.ai, to essentially say, hey, we have targets, but we don't want you to do anything with it, we can simply pass no-op and indicate that.
[00:40:58.400 --> 00:41:02.400]   And then once we have this, we can see that the data block is pretty familiar.
[00:41:02.400 --> 00:41:20.400]   We're going to use -- define a getX and a getY, which are just using the call reader methods, and we're going to point to our article or process articles as our inputs and our process highlights as our targets.
[00:41:20.400 --> 00:41:30.400]   Once we have that set up, we can go ahead and call .dataloaders and kick things off by passing in our data source.
[00:41:30.400 --> 00:41:39.400]   One thing to be aware of when you build these models, the encoder/decoder models are typically pretty substantial, pretty big.
[00:41:39.400 --> 00:41:49.400]   And so you're probably going to have to tinker with your batch size and find something that is going to work with whatever your GPU or constraints are.
[00:41:49.400 --> 00:42:00.400]   So for here, I just set it to two. I could probably get away with something a little bit bigger using Colab, but just be aware of that, that these are going to be bigger models.
[00:42:00.400 --> 00:42:07.400]   There's more stuff that we're feeding into it, and so your batch size is probably going to have to be smaller.
[00:42:07.400 --> 00:42:16.400]   And then, as I always recommend, look at a single batch, which we can do with the dataloaders one batch method.
[00:42:16.400 --> 00:42:24.400]   And so we have two things coming in. We have our inputs and our targets.
[00:42:24.400 --> 00:42:30.400]   And remember, with Blur, we use batch time tokenization.
[00:42:30.400 --> 00:42:44.400]   And so even though when we pre-process our dataset, we said that these lengths can be max 130, and I think this was max 256,
[00:42:44.400 --> 00:42:53.400]   is that it's actually going to truncate things to the max lengths on each individual batch.
[00:42:53.400 --> 00:43:07.400]   And this is going to help make things a little bit more efficient in terms of using our GPU and allow us to be able to potentially get away with more and build our models faster using that mechanism.
[00:43:07.400 --> 00:43:12.400]   And we can see also, if we call a show batch, we have a custom show batch in Blur.
[00:43:12.400 --> 00:43:20.400]   We can also truncate the inputs and targets separately just to make sure that everything looks like it should.
[00:43:20.400 --> 00:43:26.400]   And so with that, we have our dataloaders set up.
[00:43:26.400 --> 00:43:33.400]   So the next thing is to now define our learner and our metrics.
[00:43:33.400 --> 00:43:39.400]   And there's a lot of metrics for sequence-to-sequence models.
[00:43:39.400 --> 00:43:46.400]   And I don't have a depth of experience on translation tasks,
[00:43:46.400 --> 00:43:51.400]   so I don't have anything starred here that I've used personally and had good success with.
[00:43:51.400 --> 00:43:56.400]   I've definitely been more on the summarization side in my daily workload.
[00:43:56.400 --> 00:44:04.400]   But so on the summarization side, there's the Rouge metrics that are talked about in the course,
[00:44:04.400 --> 00:44:08.400]   and highly recommend you to watch the video of that.
[00:44:08.400 --> 00:44:11.400]   There's a really good description of how that all works.
[00:44:11.400 --> 00:44:17.400]   And then I also use another metric called vert score, which I'll discuss here in just a second.
[00:44:17.400 --> 00:44:22.400]   For translation tasks, there's SacraBlue and Blue.
[00:44:22.400 --> 00:44:30.400]   And SacraBlue is essentially just blue, but made to work with multiple tokenizers,
[00:44:30.400 --> 00:44:35.400]   whereas Blue kind of requires like tokenized text.
[00:44:35.400 --> 00:44:38.400]   It's just a little bit more difficult to use, so people use SacraBlue.
[00:44:38.400 --> 00:44:46.400]   There's a couple other metrics that aren't mentioned in the course that folks that are doing translation may want to look at.
[00:44:46.400 --> 00:44:49.400]   There's Meteor and also vert score.
[00:44:49.400 --> 00:44:53.400]   It can also be used for translation tasks.
[00:44:53.400 --> 00:44:56.400]   And just kind of briefly, I want to go over some of these big ones here.
[00:44:56.400 --> 00:44:59.400]   So Rouge, you know, they talk about in the course.
[00:44:59.400 --> 00:45:04.400]   And essentially, we're looking at N-grams in our generated or predicted text,
[00:45:04.400 --> 00:45:10.400]   and we're comparing that in an automated fashion to one or more reference text.
[00:45:10.400 --> 00:45:17.400]   And as I mentioned, there's Rouge1, which looks at unigram overlap, Rouge2, which looks at bigram,
[00:45:17.400 --> 00:45:20.400]   and then there's RougeL and RougeLsum.
[00:45:20.400 --> 00:45:25.400]   And the RougeL and RougeLsum are very similar in how they work,
[00:45:25.400 --> 00:45:31.400]   but a little bit different in terms of how they're actually calculated.
[00:45:31.400 --> 00:45:36.400]   So RougeL actually is going to look at each sentence individually
[00:45:36.400 --> 00:45:42.400]   and look for the longest matching sequence of words, and it's going to give you the average of that,
[00:45:42.400 --> 00:45:52.400]   whereas RougeLsum is going to simply do RougeL but compute it over the entire summary, not sentence by sentence.
[00:45:52.400 --> 00:45:59.400]   And so the note that they make in the course, in which I have found true from experience,
[00:45:59.400 --> 00:46:03.400]   is that the RougeL and sum tend to capture sentence structure more accurately
[00:46:03.400 --> 00:46:07.400]   because it doesn't depend on consecutive N-gram matches,
[00:46:07.400 --> 00:46:15.400]   which means that if you're building a summarization model, include Rouge1 and Rouge2,
[00:46:15.400 --> 00:46:21.400]   but really focus your evaluation on either RougeL or RougeLsum,
[00:46:21.400 --> 00:46:27.400]   depending on, again, what kind of summaries you're going to be generating.
[00:46:27.400 --> 00:46:37.400]   This is an interesting one that I found a while ago, and again, when you get access to the slides,
[00:46:37.400 --> 00:46:42.400]   I linked all the papers for these things, and they're really helpful to read.
[00:46:42.400 --> 00:46:46.400]   And I know Sanyam does like paper reading study groups,
[00:46:46.400 --> 00:46:52.400]   and I hope folks are checking those out and feeling inclined or more comfortable with reading papers.
[00:46:52.400 --> 00:47:03.400]   It's really not that hard. You don't have to -- you can ignore, for many of those cases, the mathematical syntax and the Greek.
[00:47:03.400 --> 00:47:12.400]   Typically, they include really helpful explanations and diagrams about how things work, and that's true with the BERT score metric.
[00:47:12.400 --> 00:47:21.400]   And essentially what this does is it uses a pre-trained BERT model, and it pulls out embeddings,
[00:47:21.400 --> 00:47:30.400]   and essentially what it does is just compute the similarity between the token embeddings of your predicted text,
[00:47:30.400 --> 00:47:34.400]   your generated text, and the reference summaries.
[00:47:34.400 --> 00:47:43.400]   And the authors show in the paper that this is maybe even superior to the Rouge metric.
[00:47:43.400 --> 00:47:52.400]   And so this is how it works, and you can recall like a F1 score recall precision.
[00:47:52.400 --> 00:48:05.400]   And if you go through the paper, you'll actually see comparisons between how well BERT score works compared to even like Blue and Meteor for translation.
[00:48:05.400 --> 00:48:17.400]   And they have some really helpful information like showing correlations between BERT score metrics and human judgments compared to some of these other automated metrics as well.
[00:48:17.400 --> 00:48:24.400]   So check this one out because what's nice also with BERT scores, you can use this for translation or summarization tasks.
[00:48:24.400 --> 00:48:40.400]   And I have a summary down here of some of the things I gleaned from the paper, which is if you're using BERT score, tend to, by general, look at the F1 score when evaluating generated text.
[00:48:40.400 --> 00:48:50.400]   If you're using English, so one of the nice things with BERT score is that you can use like different BERT models for calculating this.
[00:48:50.400 --> 00:48:57.400]   And so for English, they recommend using a 24-layer Roberta Large model to compute BERT score.
[00:48:57.400 --> 00:49:02.400]   For non-English, you can actually build your own BERT model and compute it.
[00:49:02.400 --> 00:49:08.400]   Or if you have a BERT model that is grammar specific, you can use that.
[00:49:08.400 --> 00:49:16.400]   Or they also include a multilingual BERT model that they say works well for multilingual corpuses.
[00:49:16.400 --> 00:49:21.400]   And as I mentioned, the nice thing with BERT score, it can be used for summarization and translation tasks.
[00:49:21.400 --> 00:49:29.400]   So I use it in my professional work and really encourage you to read the paper and consider using it as well.
[00:49:29.400 --> 00:49:36.400]   We talked about Blue and Sacrebleu, and essentially they are the same thing.
[00:49:36.400 --> 00:49:43.400]   It's just that Sacrebleu overcomes a key weakness in Blue in that it doesn't already expect your text to be tokenized.
[00:49:43.400 --> 00:49:50.400]   So this makes it much easier to use when you're using different tokenizers with your inputs than your targets.
[00:49:50.400 --> 00:49:55.400]   And I mentioned the Merian MT models. That's what they do.
[00:49:55.400 --> 00:50:02.400]   And so using Sacrebleu is definitely the recommendation there.
[00:50:02.400 --> 00:50:08.400]   There's another machine translation specific metric called Meteor.
[00:50:08.400 --> 00:50:17.400]   And if you read the paper, the authors state that it offers superior correlation with human judgments over that with Blue.
[00:50:17.400 --> 00:50:26.400]   And the reason is that it also uses stemming, synonym matching, along with exact word matching and calculating an F score.
[00:50:26.400 --> 00:50:31.400]   And when they calculate the F score, recall is weighted higher than precision.
[00:50:31.400 --> 00:50:40.400]   And if you look at Blue, it doesn't even factor in recall. It just is all based on precision and doesn't consider semantic similarity between words.
[00:50:40.400 --> 00:50:51.400]   And so Meteor is another one. Check out the paper and consider if you're doing a translation task, consider using that as one of your metrics.
[00:50:51.400 --> 00:50:54.400]   So how do we do that with Blur?
[00:50:54.400 --> 00:51:06.400]   It's actually super easy. We have a, no surprise here, a seek-to-seek metrics callback.
[00:51:06.400 --> 00:51:13.400]   And we can actually pass a dictionary. In the documentation, there's more explanation of how this works.
[00:51:13.400 --> 00:51:19.400]   But we can pass in the metrics that we want to have calculated for us.
[00:51:19.400 --> 00:51:26.400]   And we can, again, for like summarization, I typically use Rouge and BERT score.
[00:51:26.400 --> 00:51:30.400]   There's different parameters we can pass when we do the computation.
[00:51:30.400 --> 00:51:36.400]   We can tell for each of these, do we, you know, if they don't return a single value, what do we want to get?
[00:51:36.400 --> 00:51:44.400]   Do we want to get precision, recall F1, just F1? What Rouge metrics do we want to get?
[00:51:44.400 --> 00:51:47.400]   Do we want to use a stemmer in calculating Rouge?
[00:51:47.400 --> 00:51:57.400]   So we can include that. And you can see like for translation, like these are typically the metrics I like to get back.
[00:51:57.400 --> 00:52:03.400]   So I actually still return blue just because that's kind of a standard big one that everybody reports on.
[00:52:03.400 --> 00:52:09.400]   But I'll also include Meteor and the Sacro Blue scores as well.
[00:52:09.400 --> 00:52:13.400]   And so with Blur, you don't have to worry about calculating all this stuff.
[00:52:13.400 --> 00:52:21.400]   You do have to ensure that if you're using something like BERT score, you have PIP installed it.
[00:52:21.400 --> 00:52:26.400]   And if you haven't, you'll get a nice error that says you need to PIP install it.
[00:52:26.400 --> 00:52:38.400]   So it's easy to remedy. But once you have all the bits in there, we simply tell our callback.
[00:52:38.400 --> 00:52:41.400]   Here's the dictionary. These are all the things we want to calculate.
[00:52:41.400 --> 00:52:49.400]   And then in the callback, I also include the ability to limit how that when that calculation happens.
[00:52:49.400 --> 00:52:55.400]   So remember that on the evaluation, we're going to be generating text batch by batch.
[00:52:55.400 --> 00:53:03.400]   And that actually takes a long time. And we only care about doing that in the evaluation loop.
[00:53:03.400 --> 00:53:09.400]   And we may not care about doing it if, you know, like let's say we're training for 10 epochs.
[00:53:09.400 --> 00:53:16.400]   We may only want to do this particular evaluation at the last epoch to speed up our training.
[00:53:16.400 --> 00:53:25.400]   So with Blur, you can tell it whether to do the calculate these metrics.
[00:53:25.400 --> 00:53:29.400]   You can if you pass in the default is epoch. So it does it every epoch.
[00:53:29.400 --> 00:53:37.400]   You can do it every other epoch. Or you can just say, hey, I want to do it the last epoch.
[00:53:37.400 --> 00:53:45.400]   And so with that, your evaluation metrics are all set up and good to go.
[00:53:45.400 --> 00:53:51.400]   So from here, we can go ahead and build our normal learner.
[00:53:51.400 --> 00:53:54.400]   I'm going to use the pre calculated cross entropy loss.
[00:53:54.400 --> 00:54:01.400]   And again, this is because by default, we're telling Hugging Face, go ahead and calculate the loss for us.
[00:54:01.400 --> 00:54:10.400]   And we have to include kind of our custom pre calculated cross entropy loss because Fast.ai uses these loss functions,
[00:54:10.400 --> 00:54:15.400]   not only for calculating loss, but also for decoding predictions. So we have to use this particular object.
[00:54:15.400 --> 00:54:22.400]   We just can't leave it blank. Once we have that, by the way, we have a Blur seek to seek splitter,
[00:54:22.400 --> 00:54:26.400]   which is a little bit more work than the Blur splitter,
[00:54:26.400 --> 00:54:35.400]   especially with different architectures and properly splitting the parameters up into parameter groups for optimization.
[00:54:35.400 --> 00:54:39.400]   So we can apply discriminative learning if we want to.
[00:54:39.400 --> 00:54:48.400]   And I also am going to shift this to FP 16 to make things a little bit faster.
[00:54:48.400 --> 00:54:57.400]   So our model set up again, one of the recommendation, get a batch and run it through the model.
[00:54:57.400 --> 00:55:06.400]   And especially if you're doing predictions where you're going to be doing like looking at a batch and doing batch time prediction,
[00:55:06.400 --> 00:55:11.400]   you're probably going to want to create your own like inference loop and do it like this.
[00:55:11.400 --> 00:55:17.400]   Right. And then be able to gather these things and then batch everything together.
[00:55:17.400 --> 00:55:24.400]   So one of the reasons this is really helpful in working with transformers is that you can see that when you get predictions,
[00:55:24.400 --> 00:55:27.400]   it's not like fast, where you're just going to get like tensors.
[00:55:27.400 --> 00:55:36.400]   You're actually going to get, in most cases, some type of object where the data is going to be stored in different attributes.
[00:55:36.400 --> 00:55:41.400]   So here's our predictions here. We also include some other things that we're not using here,
[00:55:41.400 --> 00:55:44.400]   but it includes like the last hidden state for the encoder. Right.
[00:55:44.400 --> 00:55:54.400]   So this kind of represents the the end of that encoder doing all its work with the inputs and like what's the representation of the inputs.
[00:55:54.400 --> 00:56:00.400]   And you can actually use this if you've already pre calculated, which is kind of interesting.
[00:56:00.400 --> 00:56:04.400]   You could save this and actually use this to ignore the encoder step.
[00:56:04.400 --> 00:56:13.400]   And you're basically passing the results, the encoder step and just say, OK, I just want to actually do the auto aggressive decoding.
[00:56:13.400 --> 00:56:20.400]   So anyways, yeah, so good to run this and see what's coming back if you're doing inference that way.
[00:56:20.400 --> 00:56:28.400]   And then we can also look at the shapes of the tensors, which is also helpful to make sure things are, you know, make sense.
[00:56:28.400 --> 00:56:40.400]   Obviously, loss doesn't have a shape here. If we look at the logits again, remember, we said our batch size is two.
[00:56:40.400 --> 00:56:50.400]   So we have two examples. They are there's 36 tokens that we're looking at and we have a vocab of 50,000, 264.
[00:56:50.400 --> 00:57:01.400]   Right. And we're trying to predict what's the best best one in a greedy fashion when we're training generation is going to change depending on our text generation parameters.
[00:57:01.400 --> 00:57:07.400]   We can see for this particular architecture, we have three parameter groups by default.
[00:57:07.400 --> 00:57:17.400]   And again, the splitter is something that including blur that tends to look at the conventions in terms of how trans how hugging face has set up their transformer models.
[00:57:17.400 --> 00:57:24.400]   But you can create your own splitter as well. And you're not you don't have to use mine in there.
[00:57:24.400 --> 00:57:28.400]   Again, that's one of the nice things with fast eyes that you can override all these things.
[00:57:28.400 --> 00:57:35.400]   So that's our learner set up. So the next thing is to train.
[00:57:35.400 --> 00:57:46.400]   And fortunately, as you've seen with question answering and token classification, we're going to train summarization and translation models, just like we did all those things.
[00:57:46.400 --> 00:58:04.400]   And so, again, this is one of the nice things I like with with using blur with using fast AI is once things are set up correctly, you can use the same heuristics and style of building your training evaluation inference exporting your model as you do with just about anything else.
[00:58:04.400 --> 00:58:19.400]   But the key difference again is that our predictions are going to be generated. And then we're going to measure those generated predictions to one or more reference summaries or reference translations.
[00:58:19.400 --> 00:58:31.400]   And so the key things to know for text generation is that you really want to be able to choose hyper parameters that are suitable for the task and also suitable for your data set.
[00:58:31.400 --> 00:58:46.400]   And so, like we've talked about the length of summaries as an example, if you're typically working with longer summaries, you may want to ensure that the length is going to allow for those.
[00:58:46.400 --> 00:58:58.400]   If you're working with very terse, you know, short summaries that are maybe just like five to 10 words, then you're going to want to constrain at least the target length.
[00:58:58.400 --> 00:59:04.400]   And so, again, these parameters are going to vary depending on your task and your data set.
[00:59:04.400 --> 00:59:22.400]   We've already talked about how the configuration object often has sensible defaults and sometimes even task specific defaults for generation and how in blur you can use that default text gen quarks method to get out those.
[00:59:22.400 --> 00:59:28.400]   But also, you may want to play with different values.
[00:59:28.400 --> 00:59:45.400]   And you may actually want to, if you're doing any type of hyper parameter tuning, is actually include the text generation parameters as part of that tuning, especially because they're going to affect the results you're going to get in your evaluation.
[00:59:45.400 --> 00:59:56.400]   And for a good introduction to how you can generate text with transformers, I've linked to this document that's been around for a while and was super helpful for me.
[00:59:56.400 --> 01:00:16.400]   And it talks about everything from greedy decoding to beam search to just a button. There's references to other papers in terms of generating text. And so, hopefully, that particular reference will help you in terms of figuring out good parameters to at least play with.
[01:00:16.400 --> 01:00:20.400]   So, make sure you check that out.
[01:00:20.400 --> 01:00:23.400]   With this,
[01:00:23.400 --> 01:00:33.400]   again, I just use the quarks that I passed in. I'm just using what the configuration object has recommended.
[01:00:33.400 --> 01:00:40.400]   And then, like I said, we can just use all the normal FASTA I bits. So, we use LRFinder.
[01:00:40.400 --> 01:00:53.400]   For this particular example, I just trained for one epoch and I set to 4E neg 5. And you can see part of our fit callbacks is that sequence sec to sec metrics callback.
[01:00:53.400 --> 01:01:10.400]   And you can see it's calculated all the things I've asked for. So, I got rouge one, two, rouge L, rouge L sum. I got my BERT score precision recall and F1 scores included in the output there.
[01:01:10.400 --> 01:01:14.400]   And at the end of this, again,
[01:01:14.400 --> 01:01:18.400]   just like we do with any FASTA I
[01:01:18.400 --> 01:01:26.400]   bit, we can call show results, we can truncate the inputs and the targets as we do here to kind of see our results.
[01:01:26.400 --> 01:01:30.400]   And we're good to go.
[01:01:30.400 --> 01:01:40.400]   And then lastly, we got inference. And so, in blur, I added a couple methods. There's a blur summarize and a blur translate method that are
[01:01:40.400 --> 01:01:49.400]   patched on top of learner. And both of these are using the blur or the learner blur generate method under the hood.
[01:01:49.400 --> 01:02:00.400]   And if you are more interested in how blur generate looks, take a look at the docs. It's actually included in the 01 modeling notebook.
[01:02:00.400 --> 01:02:15.400]   And we include it there rather than in the sequence to sequence bits, because as we'll see next week, this is also going to be used when we're actually generating text for a causal or a mass language model.
[01:02:15.400 --> 01:02:23.400]   But anyways, yes, we have these basically just call to it. And that's about it. They're just an easier way to
[01:02:23.400 --> 01:02:32.400]   I think work with task specific predictions. And also, if you look at the results, they mimic now what you would get from the Hugging Face pipeline.
[01:02:32.400 --> 01:02:41.400]   So if you are used to using the pipeline. When you use blur summarize or blur translate, you'll see that it looks about the same.
[01:02:41.400 --> 01:02:50.400]   Sometimes I include different additional helpful information in those dictionaries, but roughly they follow the same conventions.
[01:02:50.400 --> 01:03:02.400]   And so if we actually go to inference, you can see I'm going to set the metrics to none. And I'm going to set the learner to back to FP 32.
[01:03:02.400 --> 01:03:11.400]   And so you might be like, well, why do you have to do that? And simply because the model won't export. It won't be able to pickle things correctly.
[01:03:11.400 --> 01:03:21.400]   If you leave this as mixed precision and you include the learner metrics. And so since we're at inference time, we really don't care about that stuff.
[01:03:21.400 --> 01:03:32.400]   We can go ahead and just set it to none and set this back to what it is when it starts out. And then we call export.
[01:03:32.400 --> 01:03:39.400]   If these things are important to you, once you load the learner, your inference learner, you can add them back in.
[01:03:39.400 --> 01:03:47.400]   You just can't use them here for exporting, like basically pickling your learner.
[01:03:47.400 --> 01:03:54.400]   So anyways, just here, I'm going to go ahead and set these things to none. Export. I'm going to pull a test article out.
[01:03:54.400 --> 01:04:02.400]   And again, just like we do normally with fast.ai, we can use load learner to load it back up.
[01:04:02.400 --> 01:04:15.400]   And as I mentioned, we can call blur summarize and we get the results of our train model.
[01:04:15.400 --> 01:04:21.400]   And that's basically it. So there's a lot going on here. Really encourage you to go back through the course.
[01:04:21.400 --> 01:04:29.400]   Look at the summarization pipeline and translation pipeline, the parameters you can pass to it, what the output looks like.
[01:04:29.400 --> 01:04:37.400]   And then go through the section seven bits on summarization and translation.
[01:04:37.400 --> 01:04:44.400]   There's really great videos in particular on the rouge and blue and second blue metrics.
[01:04:44.400 --> 01:04:50.400]   So I really encourage you to watch those, but they're definitely worthwhile to go through in their entirety.
[01:04:50.400 --> 01:04:55.400]   And then as homework. So we did show you a summarization example.
[01:04:55.400 --> 01:05:00.400]   And I told you that summarization and translation work roughly the same way.
[01:05:00.400 --> 01:05:14.400]   So try actually using the same bits, modifying them as you need to actually build a translation model and write a blog explaining about it and see if you can accomplish that.
[01:05:14.400 --> 01:05:22.400]   And with that, I'm going to pass things back over to Sanyam and ask if there's any questions or comments.
[01:05:22.400 --> 01:05:28.400]   This was awesome. I looked at the summary and it was also somewhat meaningful, I think.
[01:05:28.400 --> 01:05:34.400]   So I was surprised by that. Just one question.
[01:05:34.400 --> 01:05:37.400]   What are the new bits in Blur?
[01:05:37.400 --> 01:05:42.400]   So with Blur.
[01:05:42.400 --> 01:05:51.400]   So in terms of the new bits, probably the core changes, there's been some refactoring in naming.
[01:05:51.400 --> 01:06:00.400]   A lot of refactoring in terms of the prediction bits like Blur summarize, Blur translate, Blur token prediction.
[01:06:00.400 --> 01:06:07.400]   I brought those more in line with what you would get from using the Hugging Face pipeline object.
[01:06:07.400 --> 01:06:11.400]   Also, the pre-processing bits are brand new.
[01:06:11.400 --> 01:06:20.400]   There's some improvements to the sequence of sequence bits.
[01:06:20.400 --> 01:06:26.400]   There's some improvements and fixes on preparing both the predicted and reference summaries.
[01:06:26.400 --> 01:06:32.400]   There's things I didn't know until I took the course, like having to add new lines for calculating Rouge.
[01:06:32.400 --> 01:06:35.400]   So I made sure those are in there.
[01:06:35.400 --> 01:06:41.400]   So those are probably the core changes, at least for summarization and translation.
[01:06:41.400 --> 01:06:49.400]   And then, like I said, there's been, like I said, the pre-processing bits are new across the board.
[01:06:49.400 --> 01:06:53.400]   Awesome. I don't see any other questions.
[01:06:53.400 --> 01:07:01.400]   So I appreciate the homework. And I think next week we're learning casual and mass language models.
[01:07:01.400 --> 01:07:07.400]   Yeah, yeah. So that may seem kind of odd that language models are the foundation for, you know,
[01:07:07.400 --> 01:07:11.400]   pre-trained language models are the foundation for everything that we've looked at so far.
[01:07:11.400 --> 01:07:19.400]   So like, it may seem odd that we're doing that last, but we're doing it last because generally it's something you don't need to do.
[01:07:19.400 --> 01:07:27.400]   There's typically good pre-trained language models, and you really want to find those right models to then use for your downstream tasks.
[01:07:27.400 --> 01:07:33.400]   So it kind of seems backwards, but I think it's, I think it makes sense.
[01:07:33.400 --> 01:07:39.400]   Makes sense to me. Awesome. I don't see any other questions. So thanks again, Wade.
[01:07:39.400 --> 01:07:42.400]   We have two more weeks of meeting everyone.
[01:07:42.400 --> 01:07:51.400]   So after this, as a reminder, please keep thinking of blog posts or GitHub projects that you can share,
[01:07:51.400 --> 01:07:55.400]   because we'll be again rerunning the session with hopefully some interesting prizes.
[01:07:55.400 --> 01:08:00.400]   So maybe that motivates you. And with that, thanks again, Wade. And thanks everyone for joining.
[01:08:00.400 --> 01:08:03.400]   Yeah. Thanks, Sanyam. Thanks, everyone. See you all next week.
[01:08:03.400 --> 01:08:05.880]   next week.

