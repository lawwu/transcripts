
[00:00:00.000 --> 00:00:05.920]   So last week we talked about new testing data that was revealing that humans may have reached their
[00:00:05.920 --> 00:00:09.840]   peak intelligence levels right around the time smartphones became ubiquitous,
[00:00:09.840 --> 00:00:16.840]   and ever since that point our intelligence levels as measured by test have been going down. So my
[00:00:16.840 --> 00:00:23.300]   conclusion last time we talked about this is that in this new world of smartphones we might have to
[00:00:23.300 --> 00:00:29.700]   start training our brain in a way that we didn't worry about 20 or 30 years ago. And my analogy was
[00:00:29.700 --> 00:00:34.780]   physical fitness. That when you're in the first half of the 20th century we didn't think a lot about
[00:00:34.780 --> 00:00:40.320]   physical fitness because you got a lot of exercise just in your day-to-day life whether it's on the
[00:00:40.320 --> 00:00:45.880]   farm or you know you worked in a city. You know people walked a lot in cities Jesse if you think
[00:00:45.880 --> 00:00:50.200]   about it. People would just walk like four miles to work or whatever. But then when we got to the
[00:00:50.200 --> 00:00:54.720]   second half of the 20th century and people had more sedentary lifestyles and jobs, we had cars and
[00:00:54.720 --> 00:00:59.500]   offices and air conditioner etc. We had to start exercising. We never had to exercise before.
[00:00:59.500 --> 00:01:04.520]   Something changed about our world. We had to start exercising. I'm arguing that we probably have to
[00:01:04.520 --> 00:01:09.580]   do the same thing with our brains. 30 years ago, we didn't have to worry about specifically exercising
[00:01:09.580 --> 00:01:15.020]   our brains. Today we do. All right, so here is the follow-up I want to do in today's deep dive.
[00:01:15.020 --> 00:01:21.600]   I gave some basic advice last time. The cognitive equivalent of telling someone, hey, make sure you
[00:01:21.600 --> 00:01:27.960]   do some movement every day. Like get your go for a walk every day. But a lot of people ask me a follow-up
[00:01:27.960 --> 00:01:34.980]   question. What would it look like not just to sort of be active somewhat cognitively, but to get in
[00:01:34.980 --> 00:01:41.800]   really good shape? What is the brain training equivalent of those routines you see, you know,
[00:01:41.800 --> 00:01:46.540]   online where the people are in incredible shape? And it's like here is how you're going to completely
[00:01:46.540 --> 00:01:49.580]   change your fitness, your health, and your body. What is the equivalent of that for your brain?
[00:01:49.580 --> 00:01:53.800]   That's what I want to get into today. I'm going to suggest some ideas for if you wanted to be
[00:01:53.800 --> 00:01:59.580]   the equivalent of swole, but for your brain. I'm not going to say your IQ because this really isn't
[00:01:59.580 --> 00:02:03.020]   about doing well on IQ tests, but really just about the ability to hold your attention and think
[00:02:03.020 --> 00:02:07.420]   interesting thoughts. What would that look like? I am not recommending that everyone needs to do this.
[00:02:07.420 --> 00:02:14.900]   I just think it's an interesting thought experiment. What would it take to not just stave off the
[00:02:14.900 --> 00:02:21.040]   cognitive decline induced by smartphones, but actually become someone whose brain is a real
[00:02:21.040 --> 00:02:31.400]   asset? All right. So that is our goal here. Let me start just by reminding us of the advice I gave
[00:02:31.400 --> 00:02:36.620]   when I last talked about this, because I'm now going to take that advice, that basic advice, and say,
[00:02:36.680 --> 00:02:42.280]   this is just how you lay a base. The equivalent in cognitive fitness and physical fitness, just sort
[00:02:42.280 --> 00:02:46.120]   of getting your body moving and flexible and your muscles woken up so you're ready to actually do
[00:02:46.120 --> 00:02:52.500]   more intense training. So just as a reminder, last time we talked about this, I recommended go on walks
[00:02:52.500 --> 00:02:59.440]   and think during the walks. Don't have something in your ear. Avoid dopamine stacking. So just make a
[00:02:59.440 --> 00:03:03.140]   rule. I don't dopamine stack. So that means if I'm watching something, I'm not also on my phone.
[00:03:03.680 --> 00:03:07.880]   You know, if I'm reading something, I'm not also checking out videos. I just do one source of
[00:03:07.880 --> 00:03:15.220]   stimuli at a time. Read more. Doesn't matter what it is. Find what you're excited enough about that
[00:03:15.220 --> 00:03:19.480]   you're more likely to stick with it, but just grapple with the written word and maybe seek out a hobby
[00:03:19.480 --> 00:03:23.780]   that requires more concentration. Like you learn to play the guitar, practicing the guitar requires
[00:03:23.780 --> 00:03:29.080]   concentration, right? So those type of activities remain good activities for laying a base of cognitive
[00:03:29.080 --> 00:03:34.100]   fitness. Now I have some ideas for how you can push that base into something more elite.
[00:03:34.100 --> 00:03:41.560]   The first idea is interval training. Now this is the basic concept that I talked about, geez, I think
[00:03:41.560 --> 00:03:47.500]   all the way back in deep work. And it was something that I had actually messed around with prior to deep
[00:03:47.500 --> 00:03:52.720]   work when I was a graduate student, primarily writing books for students. And I was working with a lot of
[00:03:52.720 --> 00:03:59.240]   undergraduates to help them do better in school. And I worked with a variant of interval training
[00:03:59.240 --> 00:04:02.680]   because a lot of these undergraduates starting around this time, we're starting to struggle with
[00:04:02.680 --> 00:04:07.940]   concentration. This sort of the distraction era was, was, was on its way in. And we would do this
[00:04:07.940 --> 00:04:13.400]   training routine where you would focus hard on something. In their case would be academic work
[00:04:13.400 --> 00:04:18.600]   for a limited amount of time and they would have a timer, but they had to focus hard for that amount
[00:04:18.600 --> 00:04:22.200]   of time. If their attention wandered, it was like, um, no, no, you got to restart that. You need to
[00:04:22.200 --> 00:04:25.580]   focus for this amount of time. If you, if you look at your, wouldn't have been a phone back then, but if
[00:04:25.580 --> 00:04:29.780]   you jump on the internet or something like that, you have to restart. And then only once they got
[00:04:29.780 --> 00:04:34.380]   comfortable with a given interval, I would increase it by 10 minutes. All right, now let's try getting
[00:04:34.380 --> 00:04:37.740]   comfortable with this new length of time. So it's interval training. You get comfortable with an
[00:04:37.740 --> 00:04:42.720]   interval that's a stretch, and then you stretch that interval even farther. And I found in like
[00:04:42.720 --> 00:04:47.000]   a semester or two, you could get a relatively distracted undergraduate from this sort of
[00:04:47.000 --> 00:04:52.160]   pre-smartphone era up to like 90 minutes of concentration pretty comfortably. I can lock
[00:04:52.160 --> 00:04:56.580]   in for 90 minutes. So I'm going to recommend that as like the first elite training technique.
[00:04:56.580 --> 00:05:03.080]   The key is to have something demanding of concentration to be focusing on during your
[00:05:03.080 --> 00:05:08.040]   interval training sessions. And so this could be, you know, reading a hard book or taking an online
[00:05:08.040 --> 00:05:12.420]   course or working on a musical instrument practice is a great one that requires intense
[00:05:12.420 --> 00:05:16.940]   concentration or maybe a professional problem. This just like really challenging. You need a good
[00:05:16.940 --> 00:05:24.280]   thing to focus on that is hard. And then you use this timer. I recommend maybe once every two weeks
[00:05:24.280 --> 00:05:28.320]   or so. Like if you're comfortable with something, you want to give it at least two weeks before you
[00:05:28.320 --> 00:05:33.660]   increase it. And it might take more if it just takes you more time. If you find a duration, like you
[00:05:33.660 --> 00:05:37.780]   really can't get through it very well. It might take you longer until you're comfortable. That's okay.
[00:05:38.360 --> 00:05:45.800]   that stretches where your brain gets stronger. All right. The second idea, dialectical reading.
[00:05:45.800 --> 00:05:51.840]   I think this is, we're moving here from interval training is about just the generic ability to
[00:05:51.840 --> 00:05:59.120]   concentrate. Dialectical reading is now about your specific grappling with specific subject matters and
[00:05:59.120 --> 00:06:03.620]   your ability to actually work with complicated information, not just concentrate, but work with
[00:06:03.620 --> 00:06:11.500]   complicated information. So at the core of dialectical reading is the following. You want to avoid
[00:06:11.500 --> 00:06:19.620]   the engagement with content that the algorithmically driven internet promotes. So if you're looking at
[00:06:19.620 --> 00:06:28.540]   content on Twitter or the recommendation algorithms on YouTube, it's tuned to press a button in the human
[00:06:28.540 --> 00:06:32.900]   psyche. That's very satisfying, but also sort of almost anti-intellectual. The button it wants to
[00:06:32.900 --> 00:06:38.340]   press is your team is great. The other team is terrible. And here's an example of someone from the
[00:06:38.340 --> 00:06:43.680]   other team being terrible. And it's a slam dunk, how terrible they are. That feels great for, for a human
[00:06:43.680 --> 00:06:47.600]   beings that are wired for sort of in-group out-group thinking. It's sort of a sugar high of out-group
[00:06:48.420 --> 00:06:54.240]   isolation. So you watch like a Twitter thread is going to be, look at these, look what they're
[00:06:54.240 --> 00:06:57.820]   doing over. It's so obviously wrong. And aren't they terrible? And here's an example of terrible.
[00:06:57.820 --> 00:07:01.100]   They don't even realize how terrible it is. It wants to, there's this sort of righteousness of like,
[00:07:01.100 --> 00:07:06.120]   I am just super right. Feels great. My tribe is killing it. That tribe is terrible. And you get this
[00:07:06.120 --> 00:07:12.300]   on all sorts of divides of, you know, it could be from sports fandom, the political divides to,
[00:07:12.300 --> 00:07:16.060]   to whatever else is going on. Okay. Dialectical reading says, no, no, we're going to teach you how to
[00:07:16.060 --> 00:07:20.480]   actually engage with information. So what you do is you take a topic that's interesting to you,
[00:07:20.480 --> 00:07:25.280]   probably a topic where you feel like I have an instinct towards this is the right side.
[00:07:25.280 --> 00:07:30.320]   Instead of looking for outrage bait content, that's just going to pat you on the back and be like,
[00:07:30.320 --> 00:07:35.480]   yeah, you're, you're, you're right. You're Martin Luther King. And everyone on the other side is
[00:07:35.480 --> 00:07:38.760]   Bull Connors, right? Like, or it's just like an obvious setup where you're just the best and
[00:07:38.760 --> 00:07:45.920]   everyone else is dumb. What you do is you find a really good book pushing one side and you find a,
[00:07:45.920 --> 00:07:52.200]   the best possible defense of the other side of the issue. And you read both those books. That's the
[00:07:52.200 --> 00:07:58.720]   dialectic. So I'm reading two opposing, but very well-constructed takes on the same issue.
[00:07:58.720 --> 00:08:09.780]   The collision of two smart opposing takes creates a, an intellectual leap forward that is 10x more
[00:08:09.780 --> 00:08:14.980]   sophisticated than what you get just, you know, going to your choir stalls and having the preacher
[00:08:14.980 --> 00:08:21.580]   preached to you. It is, put the specifics of the content aside, it is a fantastic intellectual
[00:08:21.580 --> 00:08:26.600]   experience. I mean, this goes back to Socrates, who was a big believer in this method of intellectual
[00:08:26.600 --> 00:08:33.180]   engagement. Ironically, and not to do a big aside, ironically, Plato writing in the Phaedraeus
[00:08:33.180 --> 00:08:39.100]   dialogue in the voice of Socrates, Socrates was a character in it, had the character Socrates express
[00:08:39.100 --> 00:08:44.300]   skepticism about the written word. Like, Ooh, this is a problem that we're writing things down.
[00:08:44.300 --> 00:08:48.920]   And a lot of tech optimists pointed to that and say, look, we were worried about even books, which we
[00:08:48.920 --> 00:08:52.360]   know are great. So of course we worry about everything, but this is actually what Plato was
[00:08:52.360 --> 00:08:56.620]   capturing in the Phaedraeus dialogue was not that books were bad, but that when it comes to like
[00:08:56.620 --> 00:09:02.560]   developing ideas, you need this sort of dialectical interaction, right? Where you're going back and
[00:09:02.560 --> 00:09:07.020]   forth. I'm believing this, you're believing that, let's go back and forth and see what comes out of that
[00:09:07.020 --> 00:09:11.760]   collision. Well, you can do that now with books, ironically, because there are enough books. Here's a
[00:09:11.760 --> 00:09:15.280]   really smart person on this side. Let me read their take. Here's a really smart person on my side. Let me
[00:09:15.280 --> 00:09:21.040]   read their take. Smart people opposing makes you smarter. Now, one of the fears that people have about
[00:09:21.040 --> 00:09:25.460]   dialectical reasoning, it's going to make you smarter. It's just the level of sophistication with which you
[00:09:25.460 --> 00:09:31.200]   engage with content goes up. It's like, look, Jesse and I are here in Washington, D.C. You have a
[00:09:31.200 --> 00:09:35.520]   conversation with politics with just, you know, the average person you meet here in Washington, D.C.
[00:09:35.720 --> 00:09:40.820]   is going to be a completely different experience than in Kansas City because they're everyone here.
[00:09:40.820 --> 00:09:44.940]   There's so many experts who understand things. There's just, they have way more nuance and it's
[00:09:44.940 --> 00:09:49.280]   just the way they understand political information is so much more complicated than the average person.
[00:09:49.280 --> 00:09:54.860]   This dialectical reading gives you that type of more nuanced understanding. You see that it's the
[00:09:54.860 --> 00:09:59.740]   complexities and where people are coming from. It just opens up another layer of understanding of the
[00:09:59.740 --> 00:10:04.120]   world. People worry about it because if there's an issue they feel really strongly about,
[00:10:04.320 --> 00:10:08.500]   they're worried that this will somehow like trick them out of their belief. To which there's two
[00:10:08.500 --> 00:10:13.300]   responses. If encountering an opposing point of view is going to change your belief on something,
[00:10:13.300 --> 00:10:16.820]   then maybe it's actually not that good of a belief for you to have in the first place. Maybe it's more
[00:10:16.820 --> 00:10:21.340]   complicated than you thought. Maybe you're wrong. But two, what tends to happen is for the things where
[00:10:21.340 --> 00:10:26.480]   you really are right, like, you know, your instincts are right, dialectical reading makes that stronger.
[00:10:26.480 --> 00:10:30.280]   When you've encountered the very best arguments from the other side and you still come out saying,
[00:10:30.420 --> 00:10:34.660]   this is why I believe in this, that belief is stronger. Your ability to advocate for it is
[00:10:34.660 --> 00:10:38.360]   stronger. Your understanding of why your thing is right and how to advance that is stronger. So you
[00:10:38.360 --> 00:10:42.400]   should never fear reading different sides of things. So dialectical reading is just going to make your
[00:10:42.400 --> 00:10:49.100]   grasp of information of the world not only more sophisticated, it's going to make X or like TikTok
[00:10:49.100 --> 00:10:56.380]   content essentially like unreadable or watchable. You'd be like, oh my God, I've been watching, you know,
[00:10:56.420 --> 00:11:03.740]   succession. And now I've just gone back to leave it to beaver. Like I just, I, this is stupid. So it
[00:11:03.740 --> 00:11:08.520]   really will change your brain in an interesting way. Make you less mad too. Hey, it's Cal. I wanted to
[00:11:08.520 --> 00:11:13.980]   interrupt briefly to say that if you're enjoying this video, then you need to check out my new book,
[00:11:13.980 --> 00:11:22.240]   Slow Productivity, The Lost Art of Accomplishment Without Burnout. This is like the Bible for most of the
[00:11:22.240 --> 00:11:28.860]   ideas we talk about here in these videos. You can get a free excerpt at calnewport.com
[00:11:28.860 --> 00:11:34.560]   slash slow. I know you're going to like it. Check it out. Now let's get back to the video.
[00:11:34.560 --> 00:11:42.520]   Third idea, create idea documents. I'm a big proponent of this. Is there something you care
[00:11:42.520 --> 00:11:46.540]   about or you want to care about or you just find is interesting, right? So something you might want to
[00:11:46.540 --> 00:11:49.860]   care about is like, here's like a contemporary issue going on that affects me. And I really want to
[00:11:49.860 --> 00:11:53.980]   understand it well and have like a point of view and be able to support my point of view. And this
[00:11:53.980 --> 00:11:58.320]   is a cause I care about, or it's just something of intellectual interest. Like, you know what? I
[00:11:58.320 --> 00:12:04.060]   really want to engage more with the Socratic philosophers or Greek or the big ideas of like
[00:12:04.060 --> 00:12:09.660]   the American Republic or whatever. Keep a document where you take notes, take notes on what you've
[00:12:09.660 --> 00:12:14.980]   learned and how you think about it and keep updating and growing this document. In the writing down of
[00:12:14.980 --> 00:12:18.860]   information, in the editing of information, and in the rearranging of written information,
[00:12:18.860 --> 00:12:25.520]   your cognitive scaffolding around these ideas gets more sturdy, gets more sophisticated.
[00:12:25.520 --> 00:12:33.740]   Writing is thinking. And so as you listen to a podcast, listen to a book, maybe do like a great
[00:12:33.740 --> 00:12:37.940]   courses, you're doing a great courses course, audio, one lesson every day during your commute,
[00:12:37.940 --> 00:12:42.420]   take notes on what you're learning, update your sort of summary of where you stand on something.
[00:12:42.860 --> 00:12:49.660]   That act of writing will get you significantly more leaps in understanding than if you just take
[00:12:49.660 --> 00:12:54.060]   in the information and then don't engage with it again. So organize your thoughts on the paper is
[00:12:54.060 --> 00:12:57.040]   going to better organize your thoughts in your head. It's just going to make you a smarter person.
[00:12:57.040 --> 00:13:00.820]   It's why, by the way, like when you encounter professors, they're very articulate about the
[00:13:00.820 --> 00:13:05.440]   things they care about because they're always writing about it, you know? Or, you know, if you get
[00:13:05.440 --> 00:13:11.840]   me going on one of my hobby horse issues, like get me going on email in the workplace or something like
[00:13:11.840 --> 00:13:17.300]   this, right? Something I've written about extensively, I can just go, right? I can just cook and I can give
[00:13:17.300 --> 00:13:22.380]   you 15 minutes of preacher-like content with this point, that point, this point, that point. It's because
[00:13:22.380 --> 00:13:27.520]   I've written about this so much, it has organized and structured my thoughts and I can access that
[00:13:27.520 --> 00:13:31.940]   better. So it'll just make you sound smarter if you're working with written forms of the things you
[00:13:31.940 --> 00:13:40.380]   care about. Next, I'm going to recommend become a connoisseur in something. So become really good,
[00:13:40.380 --> 00:13:48.240]   not good at an activity, but good at understanding quality in that activity. You can become a connoisseur
[00:13:48.240 --> 00:13:53.920]   of NFL football without having to become good at playing NFL football. But you can really understand,
[00:13:53.920 --> 00:13:58.120]   for example, the complexities of different offensive schemes or defensive schemes, like what's going on,
[00:13:58.120 --> 00:14:00.940]   right? You can become a connoisseur of music without having to become a musician,
[00:14:00.940 --> 00:14:03.880]   become a connoisseur of movies without having to actually make your own movies.
[00:14:03.880 --> 00:14:10.020]   I've seen this again when you come to more elite level thinkers. When you learn how to appreciate
[00:14:10.020 --> 00:14:15.160]   what really good means in one field, that translates to others. Even if you're not an expert on those
[00:14:15.160 --> 00:14:21.360]   other fields, you have an appreciation for expertise. You have an appreciation for quality. It adds shades
[00:14:21.360 --> 00:14:27.440]   of nuance and subtlety into the world. It's like the number of colors available in the world gets
[00:14:27.440 --> 00:14:33.860]   increased. When it's not just, oh, music is on and I'm hearing it. But you're instead understanding,
[00:14:33.860 --> 00:14:38.640]   like, oh, this is why this was brilliant. Like the ability to hear, see, or encounter something and
[00:14:38.640 --> 00:14:45.460]   say, that's brilliant. That ability to do that just allows you to see so many more shades. The world
[00:14:45.460 --> 00:14:53.400]   becomes richer. It becomes technicolor. It goes from, you know, the industry standard aspect ratios to 70
[00:14:53.400 --> 00:15:00.900]   millimeter projection, the cinemascope or something like that. So it changes your world and the richness
[00:15:00.900 --> 00:15:05.340]   of information that seems available. So for whatever reason, that just works really well. Again, if you're
[00:15:05.340 --> 00:15:15.640]   trying to push your brain, final, final suggestion, digital diet, start to get careful and selective about what
[00:15:15.640 --> 00:15:21.740]   information you consume digitally. Because again, a lot of the stuff that you're encountering is working
[00:15:21.740 --> 00:15:29.340]   counter to all the benefits you're trying to have here. So maybe very wary about my Instagram scrolling
[00:15:29.340 --> 00:15:37.580]   habit. That might not be serving your interest in making your brain smarter and having been more of
[00:15:37.580 --> 00:15:41.020]   like an elite cognitive athlete. That's kind of the equivalent of like, I really like my McDonald's
[00:15:41.020 --> 00:15:44.840]   milkshakes, but I'm also trying to train for the marathon. I think I have to not take the milkshakes for
[00:15:44.840 --> 00:15:51.160]   a while. So become really careful. I'm, you know, it's like, it's books, it's TV shows. If it's online content,
[00:15:51.720 --> 00:15:59.420]   maybe you're moving towards podcast, newsletters, maybe like YouTube videos that are, you're not
[00:15:59.420 --> 00:16:05.380]   surfing recommendations, but you've bookmarked the pages where it's someone who is, you know,
[00:16:05.380 --> 00:16:09.760]   more thoughtful, more expert, more interesting commentary, right? I mean, there is a world of
[00:16:09.760 --> 00:16:13.240]   just to name someone who's killing it right now. Like think about Ezra Klein right now, who's actually
[00:16:13.240 --> 00:16:17.060]   killing it, Jesse. I don't know. He's everywhere. Him and Derek Thompson with their, um,
[00:16:17.400 --> 00:16:23.240]   with their new book. Right. Uh, and he's great. There's a world of difference, right? If it's,
[00:16:23.240 --> 00:16:29.380]   I'm going to listen to like Ezra's podcast versus I'm going to go on X to hear people talk about the
[00:16:29.380 --> 00:16:33.340]   same political issues, right? It's just a, it's just a world of difference. It's, it's, uh, still
[00:16:33.340 --> 00:16:38.720]   digital content. You're going to learn a lot about important issues in politics if you care about that,
[00:16:38.720 --> 00:16:44.020]   but it's thoughtful, it's thought through, it's going to, it's going to be presented with nuance,
[00:16:44.020 --> 00:16:50.580]   for example, a completely different experience than, you know, the WWE battle that's going on.
[00:16:50.580 --> 00:16:54.980]   If you're in something like, uh, an X thread or like an apologist show or something like that.
[00:16:54.980 --> 00:16:59.600]   So digital diet matters. So start caring about that. Just like if I was giving you advice for
[00:16:59.600 --> 00:17:03.780]   getting like in really good shape, we were going to have to end somewhere saying you have to care
[00:17:03.780 --> 00:17:07.940]   about what you eat. And that's the case as well. All right. So here it is. Let's see if I can summarize
[00:17:07.940 --> 00:17:12.900]   all my suggestions, lay the base before you try to, you want to become a cognitively elite,
[00:17:12.900 --> 00:17:20.000]   lay the base with stuff like walking dopamine stack bands, reading and concentration requiring hobbies.
[00:17:20.000 --> 00:17:25.100]   Once you're sort of like used to that somewhat more heightened life of the mind, interval,
[00:17:25.100 --> 00:17:31.100]   train your concentration, engage in dialectical reading, keep idea documents, become a connoisseur
[00:17:31.100 --> 00:17:37.980]   in at least one thing, and then put in place a digital diet. Do that for a season. Your cognitive
[00:17:37.980 --> 00:17:40.600]   experience of the world is going to be different. It's just, it's going to be, you're going to seem
[00:17:40.600 --> 00:17:43.560]   smarter to other people. You're going to seem smarter to you. You're going to see nuances you
[00:17:43.560 --> 00:17:48.000]   lost before. Your stress levels will probably be lower. Your ideas, your creativity is going to be
[00:17:48.000 --> 00:17:51.720]   higher. I think life would just be better. Not everyone has to do all, not everyone has time or
[00:17:51.720 --> 00:17:56.340]   interest in becoming like a cognitive athlete, but if you do, that's the type of thing I believe
[00:17:56.340 --> 00:18:02.320]   you need to do. You know, this is kind of what college used to be. I guess it kind of still is,
[00:18:02.320 --> 00:18:09.180]   was like a four-year cognitive athlete training program. Yeah. I say used to because someone sent
[00:18:09.180 --> 00:18:14.220]   me an article, and maybe it was on someone's newsletter, but someone, one of our listeners
[00:18:14.220 --> 00:18:21.160]   sent me an article that was, it was a professor at like a mid-tier U.S. college, and he was complaining.
[00:18:21.320 --> 00:18:26.860]   He's like, look, I know I'm Gen X. I've been a professor for 30 years. I know when you complain
[00:18:26.860 --> 00:18:30.280]   about students, people are like, oh, there you go again, telling kids to get off your lawn. But he's
[00:18:30.280 --> 00:18:33.580]   like, this is important, and I need to say it. And he kind of goes through, like, here's what I've seen
[00:18:33.580 --> 00:18:39.860]   change in, you know, these students at the college where he teaches. And he starts by saying they're
[00:18:39.860 --> 00:18:44.800]   functionally illiterate, right? And he's like, they don't read. They don't read. They don't do any of the
[00:18:44.800 --> 00:18:48.640]   assignments. They lie about it. I don't think they could read. I think if I gave them just like a
[00:18:48.640 --> 00:18:53.720]   Richard Powers book, like an award-winning book, but not like Finnegan's Wake, but like a book that
[00:18:53.720 --> 00:18:57.580]   like it's out right now, or like Colson Whitehead, they couldn't read and understand it, right? Like he's
[00:18:57.580 --> 00:19:01.380]   really worried about it. And when you get to the bottom, and he's like, they can't write, and the
[00:19:01.380 --> 00:19:06.160]   punctuation and capitalization is all over the place, and, you know, there's no form. And when you get to
[00:19:06.160 --> 00:19:09.300]   the bottom, you're like, okay, what's going on here? He's like, well, it's the phone, stupid. He's
[00:19:09.300 --> 00:19:13.420]   like, it's the phone. They're completely addicted to the phones, and this is like a huge problem.
[00:19:14.020 --> 00:19:20.560]   So I don't know if you're not at like an elite school like Georgetown, where I don't know. The
[00:19:20.560 --> 00:19:24.060]   kids do read, and they're very smart, and they're very locked in, and they're like more professional
[00:19:24.060 --> 00:19:27.980]   than I am. Like they're great students. But if you're not at like one of these top 20 type schools,
[00:19:27.980 --> 00:19:33.380]   college is no longer necessarily going to be for sure like, yeah, you're going to get your four years
[00:19:33.380 --> 00:19:36.920]   of cognitive training. That's what I think college used to be. Like all this reading and writing
[00:19:36.920 --> 00:19:41.920]   and discussion, dialectical discussion in classrooms and with other students, you come
[00:19:41.920 --> 00:19:46.460]   out smarter than when you started. Not just because you took in literal information, but because you
[00:19:46.460 --> 00:19:51.120]   trained your brain. And then the knowledge economy is like, great, these people, we have smarted them
[00:19:51.120 --> 00:19:55.340]   up. And now like they can handle like the complexity of what they're going to have and like the knowledge
[00:19:55.340 --> 00:19:59.740]   economy. I don't know if it's necessarily the case at all schools anymore. So you might have to,
[00:19:59.740 --> 00:20:03.520]   you know, we have to think more about training, I guess, on your own.
[00:20:04.180 --> 00:20:10.080]   Are you a big fan of Succession? Uh, that's a great show. Yeah. Yeah. I watch Succession.
[00:20:10.080 --> 00:20:15.060]   We're watching all the shows. There's like a lot of shows right now. Yeah. Right now because White
[00:20:15.060 --> 00:20:20.840]   Lotus is just ending, but it's still going. Pit is great. The Pit. I've seen a couple of those.
[00:20:20.840 --> 00:20:26.500]   Yeah. I'm a big ER guy. So Noah Wiley. Um, it's like, we've been liking The Pit and then there's this
[00:20:26.500 --> 00:20:31.580]   new show on, oh, Severance. Like we, I realized like, I guess I have to watch that given what I write
[00:20:31.580 --> 00:20:36.560]   about. Yeah. So that's going on and there's a new season of that. And then there's this new show.
[00:20:36.560 --> 00:20:43.260]   Seth Rogen has a show on Apple TV called The Studio. I'll have to check that out. He's a studio. He gets
[00:20:43.260 --> 00:20:46.580]   put in charge of a movie studio. It looks fantastic. So there's like all sorts of shows out here right
[00:20:46.580 --> 00:20:50.940]   now. Did you read the article in the New Yorker a couple of weeks ago about the White Lotus director?
[00:20:51.820 --> 00:20:57.740]   Yeah. Uh, Sean White, not Sean, Sean something. It's white. It is white. Yeah. Yeah. White Lotus,
[00:20:57.740 --> 00:21:01.960]   white. Um, yeah. And there was also like a lot of video. He did a lot of press and there's like a lot
[00:21:01.960 --> 00:21:09.540]   of videos going around about his creative technique. Yeah. He got really into that. Um, so yes, I do
[00:21:09.540 --> 00:21:14.820]   watch those shows. Nice. Succession. That was a good show. Have you ever met? Occasionally I come
[00:21:14.820 --> 00:21:20.540]   across people who are just very rich. Um, it's interesting. It's like, okay, someone that's spot
[00:21:20.540 --> 00:21:24.860]   on. Like they spent a lot of time in that show. Only like really rich people would know this. Like
[00:21:24.860 --> 00:21:30.160]   there's, it's this like norm core stuff. There's like a, like the nondescript baseball hat, right?
[00:21:30.160 --> 00:21:35.780]   To like strong Jeremy Strong will wear it. It'd be like a specific, like $500 baseball hat. Yeah.
[00:21:35.780 --> 00:21:42.440]   It's a lot of like, it's not supposed to be flashy. Um, but people who know, know that's like a super
[00:21:42.440 --> 00:21:46.380]   expensive hat. And supposedly the other things, because they had consultants who were like, no,
[00:21:46.380 --> 00:21:50.400]   this is how you should actually act. Right. If you're really that rich. And the other thing,
[00:21:50.400 --> 00:21:56.100]   supposedly they told them early on is don't duck when you get off and on the helicopter. Because
[00:21:56.100 --> 00:22:00.280]   if you were that rich, you'd be doing so much private helicopter flying. You would know that
[00:22:00.280 --> 00:22:03.840]   like, it's too tall. It's not going to hit your head and you want it bought. Like ducking is what
[00:22:03.840 --> 00:22:10.240]   you do if you've never really been on a helicopter before. Neither here nor there. Um, all right.
[00:22:10.240 --> 00:22:16.500]   So we've got some questions coming up, but first let's hear about a sponsor. I want to
[00:22:16.500 --> 00:22:20.920]   talk about our friends at notion because this is a product that, uh, I am a huge fan of notion
[00:22:20.920 --> 00:22:25.180]   combines your notes, docs, and projects into one space that simple and beautifully designed.
[00:22:25.180 --> 00:22:31.940]   It allows you to easily build basically custom information, software tools without having to
[00:22:31.940 --> 00:22:36.860]   be the type of person who knows how to build, you know, a custom software product that you're
[00:22:36.860 --> 00:22:41.200]   just, you're, you're setting things up very easily. This is whether you're talking about,
[00:22:41.200 --> 00:22:46.040]   you know, an individual setting up a somewhat more advanced system for keeping track of all the
[00:22:46.040 --> 00:22:50.440]   things you have to do. I mean, all, I get so many messages from leaders, readers who have taken my
[00:22:50.440 --> 00:22:55.200]   ideas into custom notion systems, or you're like a really large company that's building like major
[00:22:55.200 --> 00:22:59.620]   information tracking systems. Notion lets you do this really quickly. Uh, I mean, you,
[00:22:59.740 --> 00:23:04.000]   you've integrated just, you've had your share of time working with it because our ad agency had that
[00:23:04.000 --> 00:23:08.720]   really cool system they built out of notion that made stuff a lot easier. Yeah. And you could see
[00:23:08.720 --> 00:23:13.700]   all the different pieces of information that were applicable to the podcast and it would give it in
[00:23:13.700 --> 00:23:17.120]   different views. That was the cool part. So it was like, Hey, what ad reads are happening on this
[00:23:17.120 --> 00:23:22.720]   day? Oh, let me click on that advertiser. Show me all of the ad reads we're doing in the next six
[00:23:22.720 --> 00:23:27.300]   months from that advertiser. All I remember is we were doing something before that that was much worse.
[00:23:27.300 --> 00:23:31.040]   and I don't remember what it was. Weren't they just like sending us spreadsheets? Yeah. It was all
[00:23:31.040 --> 00:23:35.420]   tracking like a, it would send us like a, yeah, like a spreadsheet as an email. I don't know. There'd
[00:23:35.420 --> 00:23:41.360]   be like a, a guy with an old timey newsboy cap would come up with a telegraph. Western union,
[00:23:41.360 --> 00:23:50.920]   Western union, just then you got a Udacity, Udacity ad next episode. Stop. Make sure you mention
[00:23:50.920 --> 00:23:56.440]   whatever. Stop. That's how we were doing it before. Uh, and it made it much easier. So I love notion
[00:23:56.440 --> 00:24:00.620]   because I love smart information systems and they're, they're the best in the business they
[00:24:00.620 --> 00:24:04.660]   have. Here's what's cool though. What they've done recently, which I just want to underscore quickly
[00:24:04.660 --> 00:24:12.140]   is they have integrated AI to make your systems like immediately much smarter. The fully integrated
[00:24:12.140 --> 00:24:15.980]   notion AI helps you work faster, write better, and think bigger doing tasks that would normally take
[00:24:15.980 --> 00:24:21.740]   you hours in just seconds. So there's a lot of ways the AI now is seamless. Um, searching for
[00:24:21.740 --> 00:24:26.660]   certain information using AI, you can do like a really intuitive, simple search. It just, it can go
[00:24:26.660 --> 00:24:30.720]   through your information and find what you're looking for. When you're actually entering information into
[00:24:30.720 --> 00:24:36.760]   the system, it's right there to help you write faster, get a first draft or brainstorm or polish up notes.
[00:24:36.760 --> 00:24:40.960]   Right. So you've, you've, uh, this is a big one for me. Sometimes I take notes on the computer when I'm
[00:24:40.960 --> 00:24:44.280]   doing an interview and it's very sloppy. And if you're throwing them in a notion system, the notion AI can
[00:24:44.280 --> 00:24:49.620]   clean it up and fix the sort of obvious mistakes. Um, so it's cool. So they're, they're continuing
[00:24:49.620 --> 00:24:54.260]   a product. I already love is using AI in a really cool way. Uh, there's a reason why notion is used
[00:24:54.260 --> 00:25:00.160]   by over half of fortune 500 companies because teams that use notion, send less email, cancel more
[00:25:00.160 --> 00:25:06.580]   meanings, save time searching for work and reduce spending on tools. Notion is my type of software.
[00:25:06.580 --> 00:25:12.680]   So try notion for free. When you go to notion.com slash Cal, that's all lowercase letters, notion.com
[00:25:12.680 --> 00:25:18.020]   slash Cal to try the powerful, easy to use notion AI today. When you use our link, you're supporting
[00:25:18.020 --> 00:25:26.820]   our show. Go to notion.com slash Cal. I also want to talk about a new sponsor Kensta Insta with a K
[00:25:26.820 --> 00:25:35.720]   something I've spent a lot of my adult life doing is dealing with WordPress. Every website that I ever
[00:25:35.720 --> 00:25:40.920]   have going back to the original study hacks blog has been loaded on some sort of WordPress host.
[00:25:41.080 --> 00:25:47.220]   And it really has been a major time suck. It is difficult to get good WordPress hosting.
[00:25:47.220 --> 00:25:53.400]   It'll be unintuitive controls. It'll be security flaws that aren't updated. And above all else,
[00:25:53.400 --> 00:26:00.340]   these installations tend to get really slow. This is where Kensta enters the scene. It does WordPress
[00:26:00.340 --> 00:26:06.900]   hosting, right? They focus, focus, focus on speed, speed, speed. We're going to keep our installation
[00:26:06.900 --> 00:26:13.280]   fast so that your site actually loads, which again, if you're doing WordPress hosting, this becomes a real
[00:26:13.280 --> 00:26:19.140]   issue. They're great at it. They also have enterprise level security. You're not worried about, oh, we're
[00:26:19.140 --> 00:26:22.980]   out of date on the latest security fix and hackers are going to get in there and mess with our site. You can
[00:26:22.980 --> 00:26:30.080]   rest easy if you're using Kensta. Fantastic dashboard. This makes a big difference. A lot of dashboards for
[00:26:30.080 --> 00:26:33.200]   these WordPress hosts. And again, Jesse, you don't have to see a lot of these because you don't do as
[00:26:33.200 --> 00:26:38.480]   much of the web stuff, but a lot of them look like they were basically out of the movie war games
[00:26:38.480 --> 00:26:43.580]   starring Matthew Brodwick, you know, where he's like typing into the, the, the green text, like goes
[00:26:43.580 --> 00:26:51.040]   across and the Kensta interface is fantastic. Perhaps like that's my second favorite thing about
[00:26:51.040 --> 00:26:55.920]   them. My first is the speed. Second favorite thing, real support. 24 hours a day, seven days a week,
[00:26:55.920 --> 00:27:01.200]   365 days a year. You get a real human on the line when you have an issue, right? So if you've dealt
[00:27:01.200 --> 00:27:06.440]   with WordPress, all of these pain points should sound familiar. And then I can point you with confidence
[00:27:06.440 --> 00:27:12.020]   towards the solution you should use for your WordPress hosting, which is Kensta. I wish I had known
[00:27:12.020 --> 00:27:18.080]   about them. God, it's been 20 years. I think I launched calnewport.com on an old WordPress
[00:27:18.080 --> 00:27:24.500]   installation in 2006. 20 years. I wish I knew about Kensta 20 years ago. Sure. They didn't exist 20
[00:27:24.500 --> 00:27:29.020]   years ago, but that's no excuse. It would have made my life a lot easier. So if you're tired of being
[00:27:29.020 --> 00:27:34.260]   your own hosting support team, switch to Kensta and get your first month free and don't worry about the
[00:27:34.260 --> 00:27:41.440]   move. They'll handle the migration for you. No tech expertise required. Just visit kensta.com
[00:27:41.440 --> 00:27:49.820]   slash deep questions to get started. That's K-I-N-S-T-A.com slash deep questions. All right,
[00:27:49.820 --> 00:27:56.800]   Jesse, let's move on to some questions. All right. First questions from Anna. I envision a future where
[00:27:56.800 --> 00:28:01.500]   I live close to family, grow my own food, work from home, and have time for painting. However,
[00:28:01.500 --> 00:28:07.280]   job opportunities in my animation industry are dwindling, especially remote work. Should I change
[00:28:07.280 --> 00:28:13.120]   careers to achieve my ideal lifestyle? I feel like it's our second animation question in a few weeks.
[00:28:13.120 --> 00:28:18.160]   It's interesting. I wonder if that's quite a coincidence. I'm reading a book on another Disney
[00:28:18.160 --> 00:28:23.120]   book right now, and it's talking about the period where they get CalArts up and running. So this was
[00:28:23.120 --> 00:28:30.860]   basically like the training camp that Disney helped fund to help generate new animators. So I know more
[00:28:30.860 --> 00:28:34.880]   about animation than I probably should. All right, but Anna, this is not about animation when you're
[00:28:34.880 --> 00:28:38.620]   asking a question about lifestyle-centric planning. So first of all, I love the approach you're taking
[00:28:38.620 --> 00:28:44.180]   here. This is classic LCP. You have an ideal lifestyle vision in mind, and now you're working
[00:28:44.180 --> 00:28:49.000]   backwards from that vision to saying, how can I actually make this possible? So when you're working
[00:28:49.000 --> 00:28:53.360]   backwards from a lifestyle vision, the things that matter typically is going to fall into the
[00:28:53.360 --> 00:28:58.580]   categories of obstacles and opportunities. What are the obstacles in the way for me getting where I am
[00:28:58.580 --> 00:29:03.400]   right now to something closer to that ideal lifestyle? The opportunities are, what can I
[00:29:03.400 --> 00:29:08.400]   leverage? What type of like unique setup? This could be like career capital or family connection or
[00:29:08.400 --> 00:29:13.680]   whatever. What opportunities do I have to leverage as well? So you want to systematically work at reducing
[00:29:13.680 --> 00:29:18.760]   or bypassing obstacles while taking advantage of everything that you have, all the opportunities you
[00:29:18.760 --> 00:29:24.600]   have as well. So that's probably the right thing to do here. The key obstacle in this analysis is you're
[00:29:24.600 --> 00:29:29.860]   seeing the particular types of jobs you have in the animation industry is not remote work friendly,
[00:29:29.860 --> 00:29:34.600]   and your lifestyle vision, when you work backwards from it, remote work really makes that work.
[00:29:34.600 --> 00:29:40.360]   Right? Because if you can be in a lower cost of living place, you could be at home and growing food
[00:29:40.360 --> 00:29:44.340]   and be near family and maybe not need to work as many hours because where you're living is less
[00:29:44.340 --> 00:29:49.300]   expensive. If instead of being, you know, near Burbank or Disney is, you're instead, you know,
[00:29:49.300 --> 00:29:53.480]   where you grew up in Indiana, it's going to be cheaper to live. Like remote work seems to be at the core
[00:29:53.480 --> 00:29:58.600]   of it. So there's a key obstacle here. If your current job trajectory isn't making that possible.
[00:29:58.600 --> 00:30:03.500]   So now you know what you're doing. Oh, I am systematically exploring and looking for insight.
[00:30:03.500 --> 00:30:06.940]   It might not be obvious, but I'm looking for a moment of insight about how do I reduce that
[00:30:06.940 --> 00:30:11.020]   obstacle? That's probably going to come from leveraging the opportunity of the various skills
[00:30:11.020 --> 00:30:15.440]   you have and looking for adjacent applications. So like, sure, maybe if I want to be in like feature
[00:30:15.440 --> 00:30:19.140]   motion picture animation, there's like three options. They're all based in California and they're
[00:30:19.140 --> 00:30:23.980]   cutting back and there's not remote work options, but maybe there's an adjacent step here. Actually,
[00:30:23.980 --> 00:30:30.800]   this graphic skills I have could be used, maybe not in like feature film, whatever animation, but it
[00:30:30.800 --> 00:30:36.480]   could be used in whatever game development, object design, or in certain types of advertising or
[00:30:36.480 --> 00:30:43.080]   something. There's there. I find that adjacent field where it's very useful. And now I have taken
[00:30:43.080 --> 00:30:48.380]   an opportunity and gone around an obstacle. So it's not easy, but it's specific. That's what's
[00:30:48.380 --> 00:30:53.860]   key about LCP is that it's very specific what you're doing here. I have this obstacle. I'm enumerating
[00:30:53.860 --> 00:30:59.700]   my opportunities. The only piece of like semi-advanced advice I would give is that if you're stuck,
[00:30:59.700 --> 00:31:04.660]   A, keep searching for ideas and inspiration, talk to people, read things, listen to things. You're
[00:31:04.660 --> 00:31:08.120]   looking for that path. It could be narrow, but you're looking for the path that gets you closer.
[00:31:09.340 --> 00:31:13.680]   Increasing the opportunity side is often a low hanging fruit that can make a big difference.
[00:31:13.680 --> 00:31:19.200]   And that might mean learning a new skill, a relevant skill. I know how to do this. What if I learned how
[00:31:19.200 --> 00:31:23.140]   to do this, which is someone who already knows how to do this first thing would have an easier time
[00:31:23.140 --> 00:31:27.340]   learning it, like looking for these adjacent skills that if you learned would open up new opportunities.
[00:31:27.340 --> 00:31:32.940]   So you're kind of playing a game here, but you're thinking about it the right way. You have a lifestyle
[00:31:32.940 --> 00:31:36.420]   in mind. You're trying to work backwards from it. So, so you're doing the right thing and don't be
[00:31:36.420 --> 00:31:41.800]   frustrated if it takes a while to finally find that path. Also, I'll throw in one more coda.
[00:31:41.800 --> 00:31:47.640]   You know, you know, you know what you want your, your ideal lifestyle. You can isolate some of the
[00:31:47.640 --> 00:31:52.080]   general properties here, like a little bit more of a slowness, a connection back to nature when you're
[00:31:52.080 --> 00:31:57.540]   growing your food connection to community. Keep in mind, even while you're looking for a larger
[00:31:57.540 --> 00:32:03.480]   reconfiguration to get to your full ideal lifestyle, you could be making changes to your life as it exists
[00:32:03.480 --> 00:32:09.600]   now to have more of those properties. So even if you're not getting the full right off the bat,
[00:32:09.600 --> 00:32:14.500]   I'm on the, the, the land with the farm near my family's house in Indiana or whatever, if you're not
[00:32:14.500 --> 00:32:20.460]   there yet, use this lifestyle to help isolate. These things are important to me and get that into your
[00:32:20.460 --> 00:32:25.520]   life right away. And maybe now you have a plot in a community garden and you've adjusted your hours.
[00:32:25.520 --> 00:32:29.180]   It's still the in-person job, but it's, it's a four day a week job. Now you have a little bit more
[00:32:29.180 --> 00:32:34.320]   slowness going into it. Uh, you're more community involved where you live, even if you can't be
[00:32:34.320 --> 00:32:40.140]   closer to family, play with these things you're identifying as important and start getting them
[00:32:40.140 --> 00:32:47.720]   in your life. Now, don't let perfect be the enemy of better. And that's like a key thing with LCP is
[00:32:47.720 --> 00:32:53.080]   that it's not just until I'm living on the houseboat, I'm in trouble. It's what is it I'm identifying as
[00:32:53.080 --> 00:32:56.760]   important? Let's keep adding this to my life. And actually that might open up other ideas for how to
[00:32:56.760 --> 00:33:01.200]   get there. So anyways, uh, good luck with that, but thanks for the good LCP example.
[00:33:01.200 --> 00:33:03.400]   All right. Who do we got next?
[00:33:03.400 --> 00:33:08.740]   Next question is from Ruben. I'm currently learning how to code during my free time to get a job as a
[00:33:08.740 --> 00:33:13.880]   software developer. I believe it would be an ideal career path towards achieving my life. I want,
[00:33:13.880 --> 00:33:19.940]   is this a bad idea to leave part of my career capital with my BA in psychology? I'm 24 years old.
[00:33:19.940 --> 00:33:24.680]   Well, I mean, the good news is you're 24 years old, so you can't really have much career capital
[00:33:24.680 --> 00:33:30.620]   anyways. So like you're, you're at the beginning. You have a lot less opportunity, loss opportunity
[00:33:30.620 --> 00:33:36.880]   cost threats here. So yeah, this is not a bad time. I'm right out of college and I'm building out my
[00:33:36.880 --> 00:33:42.120]   career capital portfolio. You're starting from low stores in all of your career capital category. So
[00:33:42.120 --> 00:33:46.360]   I think it's completely fine. You could probably leverage the BA a little bit. Like here's something
[00:33:46.360 --> 00:33:51.480]   I would say, here's a, an interesting piece of data that's been touted in the last few months.
[00:33:51.480 --> 00:33:58.760]   We're starting to see some of the first places in the labor statistics. So not just anecdotally,
[00:33:58.760 --> 00:34:03.660]   but some of the first places in the labor statistics where we think we're seeing the impact of AI
[00:34:03.660 --> 00:34:10.180]   is in a class of people, uh, technical jobs called programming, which is different than software
[00:34:10.180 --> 00:34:18.860]   development. So programming jobs post chat GPT have been suddenly declining. Software development
[00:34:18.860 --> 00:34:23.380]   jobs have not at all. They're, they're basically the same, if not like up a little bit, you have to
[00:34:23.380 --> 00:34:27.620]   kind of go into the weeds. What's the difference? Well, programming jobs, I guess there's, there's a,
[00:34:27.620 --> 00:34:34.100]   a very specific type of computer programming related technical position where it's, you're given
[00:34:34.100 --> 00:34:39.360]   very specific things to implement often with like simpler programming languages, like, okay, go add
[00:34:39.360 --> 00:34:43.480]   this feature now. Can you go onto the side and change that? So it's like very, you're doing very
[00:34:43.480 --> 00:34:46.880]   little planning or creativity. You're not watching the lifestyle of a product. You're just sort of
[00:34:46.880 --> 00:34:53.640]   implementing. Chat GPT can produce that type of simpler code pretty well. And so now, uh, the person
[00:34:53.640 --> 00:34:58.840]   who was tasking the programmer with that work could just sort of use AI help and might as well do it
[00:34:58.840 --> 00:35:03.460]   themselves. Software dev is more complicated. You're actually, uh, it's part of the lifestyle of the,
[00:35:03.460 --> 00:35:07.280]   the life cycle of the product you're involved in. It's creative. It's figuring out what the program
[00:35:07.280 --> 00:35:11.400]   needs just as much as what code it has to be in there. You might be able to leverage your BA
[00:35:11.400 --> 00:35:17.760]   in psychology to say, look, I, I speak normal person speak. I can like think about the consumer
[00:35:17.760 --> 00:35:24.760]   and what they want in their product. I understand, uh, I understand other people. I can manage other
[00:35:24.760 --> 00:35:29.280]   people. There is, I think, potentially a career capital advantage to be someone who's technically
[00:35:29.280 --> 00:35:33.280]   savvy, who also can communicate like a normal human being. And I'm sort of throwing
[00:35:33.280 --> 00:35:37.380]   my fellow computer scientists under the bus here, but let's just say, you know, we're not
[00:35:37.380 --> 00:35:44.080]   all, uh, expert communicators. Like this is not necessarily, we're not necessarily, uh, it's
[00:35:44.080 --> 00:35:48.840]   not a bunch of like John F. Kennedy's when you're at a computer science conference, right? Uh, it's,
[00:35:48.840 --> 00:35:54.660]   it's, we, uh, I, I do pretty well, but you know, it's a lot of leet speak, right? It's a lot
[00:35:54.660 --> 00:36:00.860]   of like weird sort of, uh, technical terms and jargon. So you might have an advantage with your,
[00:36:00.860 --> 00:36:07.920]   your BA in psychology. My, my poor wife used to come to some of these MIT events, like holiday
[00:36:07.920 --> 00:36:14.360]   parties or whatever. And what's going on? Don't take me to these things anymore. Like why half the
[00:36:14.360 --> 00:36:17.960]   people here can't communicate at the other half are very normal. It's just, you get a higher
[00:36:17.960 --> 00:36:23.800]   percentage than normal of people who are, you know? Yeah. Like if you see early footage of like Bill
[00:36:23.800 --> 00:36:28.840]   Gates, it's just like a lot of him, like rocking and writing a simpler code in his head.
[00:36:28.960 --> 00:36:35.020]   Yeah. Yeah. It's not exactly, you wouldn't want a, a, a theoretical computer science student at MIT,
[00:36:35.020 --> 00:36:39.660]   like on a, uh, like a radio talk show host or something. Yeah. Like it wouldn't be a popular
[00:36:39.660 --> 00:36:44.680]   show. You wouldn't want to replace mad dog with like one of my, my commenters. And they'd be like
[00:36:44.680 --> 00:36:51.120]   the, uh, performance, the Red Sox was highly illogical. His numbers against the lefties were,
[00:36:51.360 --> 00:36:57.340]   the performance were obviously lower. And now I'm going to recap in great detail
[00:36:57.340 --> 00:37:04.160]   information from the latest Star Wars or Star Trek. I don't know. We are going to go through
[00:37:04.160 --> 00:37:08.120]   the 10 points that make it clear that Star Trek Deep Space Nine is superior to the next generation.
[00:37:08.120 --> 00:37:11.500]   I believe this is obvious. All right. I'm sorry. Actually, most of the computer science I know
[00:37:11.500 --> 00:37:17.080]   are cool, but programmers aren't always. So maybe you do have an advantage there. So take advantage of
[00:37:17.080 --> 00:37:21.360]   that. All right. Who do we got next? Next question from Bill. Do you have examples of
[00:37:21.360 --> 00:37:27.020]   truly fair performance evaluation systems? How should merit be rewarded in a revenue constrained
[00:37:27.020 --> 00:37:33.020]   environment? I work at a national nonprofit. Um, I think what you should do, and I think the,
[00:37:33.020 --> 00:37:39.160]   the lawyers will back me up is you want to, uh, sort people into different like demographic groups
[00:37:39.160 --> 00:37:43.880]   and then just pay your favored groups more is probably like, so if you like the country they're
[00:37:43.880 --> 00:37:47.840]   from, pay them more than no. Aren't we allowed to say that now? Is that, aren't we all just like,
[00:37:47.840 --> 00:37:57.000]   whatever, anything goes, I don't want my funding cut. So yeah, just no DEI. Um, no, there is,
[00:37:57.000 --> 00:38:02.540]   so it is tricky, uh, performance-based review there. I wrote a New Yorker piece back in the pandemic
[00:38:02.540 --> 00:38:09.880]   from, there is a system called ROE, R-O-W-E, results-oriented workplace environment. And there's
[00:38:09.880 --> 00:38:14.940]   a book about this that's called something like work, like work doesn't have to suck or something
[00:38:14.940 --> 00:38:18.840]   like that. I mean, there's only so many business books that have like that in the title, so you'll
[00:38:18.840 --> 00:38:25.840]   find it. It's fascinating. It is a attempt to rebuild an office. If you apply the ROE methodology,
[00:38:25.840 --> 00:38:33.700]   you rebuild your office entirely around results is all that matters. And there's no expectations on
[00:38:33.700 --> 00:38:36.820]   you. Like, Hey, when you want to work, you work. I don't care. There's no vacation. There's no hour.
[00:38:36.820 --> 00:38:41.260]   You don't have to be here any particular hours. You, you essentially sort of like negotiate, okay,
[00:38:41.260 --> 00:38:45.560]   I will work on this now. Here's like the measure of success. Here's how we're going to communicate
[00:38:45.560 --> 00:38:48.860]   about it to make it done. And you kind of have like this portfolio of projects that you do and
[00:38:48.860 --> 00:38:54.100]   you're held accountable to it. It really does work pretty well when it's implemented. But the point
[00:38:54.100 --> 00:38:59.780]   of this article is it's very hard to implement it. And honestly, the big issue they have, and I
[00:38:59.780 --> 00:39:03.980]   interviewed the, the, one of the creators of it and I read the book and talked to a CEO of a company
[00:39:03.980 --> 00:39:08.340]   that uses this methodology. The, one of the biggest issues they have is not figured out how to measure
[00:39:08.340 --> 00:39:12.940]   performance. It actually gets pretty clear. Like, okay, you're doing this. All right. We know what
[00:39:12.940 --> 00:39:16.760]   success means here, right? Or like, are you, we can just look at like in a ROE environment, I could just
[00:39:16.760 --> 00:39:20.560]   be like, these are the three things you did last quarter. Does this seem valuable or not? Like there's no
[00:39:20.560 --> 00:39:27.300]   hiding. The hard part actually is trying to eliminate the last vestiges of pseudo productivity
[00:39:27.300 --> 00:39:32.800]   thinking in the office, right? Pseudo productivity being my term for my book, slow productivity for
[00:39:32.800 --> 00:39:39.440]   using just visible activity as a proxy for useful effort. There is a huge amount of the ROE, I guess
[00:39:39.440 --> 00:39:44.480]   you would call it like a application effort, trying to conversion efforts, is trying to get rid of
[00:39:44.480 --> 00:39:50.460]   pseudo productivity speak. And they call it sludge in the book. But it's just this idea of people are
[00:39:50.460 --> 00:39:54.340]   very, because of the way we built knowledge work, are just very uncomfortable about like, why aren't
[00:39:54.340 --> 00:40:00.480]   you here or I can see you? Why didn't you answer an email right away? It is a difficult mindset to get
[00:40:00.480 --> 00:40:08.320]   away from perform, performing activity to generating measurable results. And it's a hard shift. They also
[00:40:08.320 --> 00:40:14.220]   found that they won't naturally, the people who created this methodology say it all goes well. But
[00:40:14.220 --> 00:40:19.300]   when you talk to people who have actually implemented it, they will tell you, you're going to lose a lot
[00:40:19.300 --> 00:40:24.040]   of people. Not everyone can do this. The CEO I interviewed for this New Yorker piece lost like
[00:40:24.040 --> 00:40:28.960]   20% of his non-managerial staff and about 20% of the managerial staff. The non-managerial staff,
[00:40:28.960 --> 00:40:36.340]   they're like, I am actually not able to perform like this. I could be busy. My TPS reports are handed
[00:40:36.340 --> 00:40:39.740]   in. I answer emails. I'm always jumping on calls or whatever. But if you just say, forget all that,
[00:40:39.740 --> 00:40:44.260]   just tell me what you did this last month. Some people like 20% are like, I actually am not
[00:40:44.260 --> 00:40:48.460]   comfortable just focusing and doing hard things. And they lost 20% of the managers because they're
[00:40:48.460 --> 00:40:54.300]   like, I don't know how to do this if I'm not just like being able to demand quick responses and just
[00:40:54.300 --> 00:40:59.120]   have people jump. So it's possible, but it's hard. Read my New Yorker piece. That would have been from
[00:40:59.120 --> 00:41:05.340]   like 2021. And I forgot what it was called, but just search like Cal Newport, R-O-W-E, you'll find it.
[00:41:05.340 --> 00:41:09.800]   And in that book, Work Doesn't Have to Suck or something like that. We'll talk about that
[00:41:09.800 --> 00:41:16.380]   philosophy. All right. Who we got next? Next question is from Ben. In episode 345, you talked
[00:41:16.380 --> 00:41:21.600]   about the value of more niche online communities. What do these small alternatives look like and how
[00:41:21.600 --> 00:41:26.700]   do people find and engage in them if they're not hosted on these global platforms? There's a few
[00:41:26.700 --> 00:41:31.340]   forms. So these more like niche online discussion communities, which I think by the way, is like
[00:41:31.340 --> 00:41:36.020]   the right way to extract the community building value of the internet. A few places to find these.
[00:41:36.020 --> 00:41:42.540]   Bulletin boards are a big one. Oftentimes now the technology is not going to be a straight up
[00:41:42.540 --> 00:41:48.100]   bulletin board technology. It might be like at talknats.com, the example I often use in my writing
[00:41:48.100 --> 00:41:53.360]   of a Washington Nationals-based community. What they actually do, and a lot of sports sites do this,
[00:41:53.360 --> 00:41:58.620]   is they'll post like a blog post and the conversation is happening in the comments of the blog post.
[00:41:59.260 --> 00:42:03.460]   Straight up bulletin boards are fine. I actually look at Reddit threads as in this, even though
[00:42:03.460 --> 00:42:09.860]   Reddit is a large global platform, the individual Reddit threads are not algorithmically sorted and
[00:42:09.860 --> 00:42:16.440]   they are managed by the community themselves. Substack or other newsletter-based comments has
[00:42:16.440 --> 00:42:23.100]   become a big place. Like I get a substack from some writer I like. The comments underneath the
[00:42:23.100 --> 00:42:27.280]   substack becomes like a community gathering place. A lot of people have Discord servers set up,
[00:42:27.280 --> 00:42:32.580]   like you like a given show or a given type of whatever. Discord servers are big. You jump on
[00:42:32.580 --> 00:42:37.200]   there and you can talk and it's like a relatively smaller group of people. So all of this is out
[00:42:37.200 --> 00:42:43.160]   there. Podcast, so anything that's Patreon protected, there's discussion communities around it, whether
[00:42:43.160 --> 00:42:49.280]   it's actually using Patreon or, you know, its own homegrown solution. So you'll see this with like,
[00:42:49.280 --> 00:42:53.240]   I don't know, like if you're within like Sam Harris's walled garden, there's like places you can
[00:42:53.240 --> 00:42:57.880]   chat with other people. So they're out there. They're out there. The key though is that it is
[00:42:57.880 --> 00:43:03.300]   a self-selected group of people interested in the same topic. It is a manageable number of people who
[00:43:03.300 --> 00:43:07.620]   are there. So you actually like know a lot about these people. Like you've interacted them with
[00:43:07.620 --> 00:43:11.200]   enough. You recognize them and have a bit of a shared history with them, even if you don't know who
[00:43:11.200 --> 00:43:16.160]   they are like really in the physical real world. And it's not algorithmically curated. It's just the
[00:43:16.160 --> 00:43:20.680]   content is there, probably just chronological. And the standards are set by the community.
[00:43:20.860 --> 00:43:24.520]   that's what you're looking for, for, I think like a richer online engagement.
[00:43:24.520 --> 00:43:26.520]   All right. Who else do we got?
[00:43:26.520 --> 00:43:31.460]   Steven asks, how can partners support each other in building a deep life?
[00:43:31.460 --> 00:43:37.280]   I think the key here is that the life is one that you are coming up with together.
[00:43:37.280 --> 00:43:42.160]   So when you have your sort of vision of an ideal lifestyle that you're, you're sort of working
[00:43:42.160 --> 00:43:49.520]   towards, you need a shared vision first. Like this is where our family wants to be. This is what our
[00:43:49.520 --> 00:43:53.220]   family life should be like in five years and in 10 years. Like if you have kids, for example,
[00:43:53.220 --> 00:43:58.340]   you should break this up by the sort of like pre-elementary school, elementary, elementary
[00:43:58.340 --> 00:44:02.340]   period, the sort of middle school, high school period, and then sort of the post high school
[00:44:02.340 --> 00:44:06.700]   college period. What is our family life? What do we want? Like where we live, the rhythm, what it's
[00:44:06.700 --> 00:44:10.980]   like. Is there someone home after school? What's our weekends like? You want to have that figured
[00:44:10.980 --> 00:44:16.160]   out together. Then underneath that, you can have your individual. Okay. Now here's the things that are
[00:44:16.160 --> 00:44:20.560]   important to me. Like what I want to do with my physical fitness, for example, might be different
[00:44:20.560 --> 00:44:25.440]   than like what my wife wants to do or my interest in my hobby interest are for sure, you know,
[00:44:25.440 --> 00:44:30.660]   different. So start with the shared thing. That's priority one. We're on the same page. We're planning
[00:44:30.660 --> 00:44:35.060]   with that. And then you can have your own sort of buckets you're working on. And there, I think it's
[00:44:35.060 --> 00:44:38.120]   helpful to some degree to kind of like, Hey, here's what I'm working on. What are you working on? What's
[00:44:38.120 --> 00:44:42.120]   important to you? What's important to me? How can I help you with what you're doing? How can you help me
[00:44:42.120 --> 00:44:46.400]   with what I'm doing? Uh, and that, that works out as well, but make sure that there is a shared
[00:44:46.400 --> 00:44:52.160]   component like this stuff. If you're in a partnership with someone, that partnership has to extend to your
[00:44:52.160 --> 00:45:01.400]   vision of what the deep life is. All right. Let's see here. I'm going to suggest we, cause I want to,
[00:45:01.400 --> 00:45:04.460]   I'm going to, we're going to skip either the call or the case study just so we can roll into the tech
[00:45:04.460 --> 00:45:09.020]   corner with time to go today. Okay. Which one let's, uh, I'm going to skip the call. Okay. And I'm
[00:45:09.020 --> 00:45:13.580]   going to jump right ahead to the case study. This is where we find out that the call we skipped was,
[00:45:13.580 --> 00:45:17.600]   I was going to say from someone famous, but I couldn't, I don't know like what name I'm not
[00:45:17.600 --> 00:45:22.700]   up with like the hip references. Like, you know how old fashioned I am. Mr. Beast. Yeah. Well,
[00:45:22.700 --> 00:45:28.680]   let's see. My first instinct was to be like King Charles. So how to touch. Yeah. Mr. Beast called in
[00:45:28.680 --> 00:45:33.200]   and we missed it. It's like, I need you on my channel stat. All right. Our case study today comes from
[00:45:33.200 --> 00:45:38.560]   Chris. Chris says, I was recently approached by a firm looking to fill a position that I was uniquely
[00:45:38.560 --> 00:45:43.540]   qualified for. It would have been a dramatic step up in pay with a profit sharing component that may
[00:45:43.540 --> 00:45:48.280]   have doubled or even tripled my current income in a good year. I received an offer for the role and
[00:45:48.280 --> 00:45:52.340]   part of me wanted to just go for it because the income increase was so dramatic, but I decided to
[00:45:52.340 --> 00:45:59.500]   put it through a more holistic lifestyle centric evaluation. I VBLCCP'd it. Here's what I realized
[00:45:59.500 --> 00:46:05.460]   after further reflecting. This role was something of a grand goal trap. I was telling myself that if I
[00:46:05.460 --> 00:46:09.360]   could just get to this executive level, I would have quote, made it end quote, and everything else
[00:46:09.360 --> 00:46:13.940]   would have fallen into place. But in reality, the position would have required to move to a higher
[00:46:13.940 --> 00:46:18.800]   cost of living area and from a fully remote role to a three days per week in office plus travel
[00:46:18.800 --> 00:46:23.320]   requirements that seem to grow exponentially. As I really pressed the hiring team to clarify,
[00:46:23.320 --> 00:46:27.740]   this would have substantially reduced the number of hours I'm able to spend with my three-year-old who I'm
[00:46:27.740 --> 00:46:33.020]   committed to being present parent for and would have required a big renegotiation of the parenting
[00:46:33.020 --> 00:46:37.180]   burden with my partner. It was also a high risk, high reward job, meaning what I would have been
[00:46:37.180 --> 00:46:44.500]   more fearful of termination if I wasn't performing with complete excellence. I also did the work to
[00:46:44.500 --> 00:46:49.880]   process what enough meant for the income requirements, my lifestyle vision. And I realized that while I do
[00:46:49.880 --> 00:46:55.440]   need a bit more coming in annually, I'm not that far off. Instead of taking this dramatic leap up in both
[00:46:55.440 --> 00:46:59.820]   income and work hours, I'm looking at other options like a more modest move to a full-time role
[00:46:59.820 --> 00:47:03.020]   that increases pay but maintains most of my current balance.
[00:47:03.020 --> 00:47:10.160]   That there, Jesse, is VBLCCP in action, values-based lifestyle-centric career plan. There you go. When
[00:47:10.160 --> 00:47:15.120]   you know what matters to you, what you want your life to look like, what's happening, like your job
[00:47:15.120 --> 00:47:20.220]   becomes just a tool you have among other tools to craft this life. And that's a fantastic decision.
[00:47:21.160 --> 00:47:25.980]   I talk about that in my book, So Good They Can't Ignore You. That is one of the control traps,
[00:47:25.980 --> 00:47:30.760]   that just as you get good enough to have some leverage over your life is exactly when people
[00:47:30.760 --> 00:47:34.440]   are going to come to you and give you shiny opportunities to take your autonomy away.
[00:47:34.440 --> 00:47:38.920]   So just as you're like, I'm pretty good at this, I'm making good money and have flexibility,
[00:47:38.920 --> 00:47:43.560]   that's exactly when they say, hey, we want to triple your salary. But then in the quiet voice,
[00:47:43.560 --> 00:47:46.700]   but you're never going to see your family again, you're going to travel all the time and be stressed.
[00:47:47.160 --> 00:47:51.220]   But hey, you were going to triple your salary, right? That's the second control trap in action.
[00:47:51.220 --> 00:47:53.840]   Lifestyle-centric career planning helps protect you against it.
[00:47:53.840 --> 00:48:02.780]   Like I think, as Chris, is going to probably be substantially more happy on average the next 10
[00:48:02.780 --> 00:48:08.000]   years with the decision he made than if he had just gone for the bigger job. Because why are you
[00:48:08.000 --> 00:48:11.980]   doing the bigger job? Like it's all about what it's serving or like the work itself. So the work
[00:48:11.980 --> 00:48:16.000]   itself wasn't going to be more interesting to him and the stress was higher and it was going to get in
[00:48:16.000 --> 00:48:21.140]   the way of multiple different goals, other things that were important. So now he's just tweaking what
[00:48:21.140 --> 00:48:25.760]   he has to get a little bit more money and then they're good. So I think that's great. I just want
[00:48:25.760 --> 00:48:31.540]   to flag real quick how he talked about the processing what enough meant. We've talked about that in a prior
[00:48:31.540 --> 00:48:36.840]   show, but it's just worth underscoring. That's this idea of you make sure you know the numbers.
[00:48:37.640 --> 00:48:42.940]   Like how much money do we need for like this type of lifestyle we're looking for? Having the specific
[00:48:42.940 --> 00:48:48.040]   numbers matter as opposed to just more. If you just say I need more money, you might take the triple
[00:48:48.040 --> 00:48:53.560]   income job and be miserable where he crunched the numbers and said, we're close. We need to just fill
[00:48:53.560 --> 00:48:57.280]   in this. Now let me be really, really careful in figuring out how to fill in that in a way that's
[00:48:57.280 --> 00:49:00.440]   not going to get rid of all these other goals. So anyway, it's fantastic. Lifestyle-centric career
[00:49:00.440 --> 00:49:07.920]   planning in action. All right. We got a tech corner coming up, but first let's hear from another
[00:49:07.920 --> 00:49:14.720]   sponsor. We have a new sponsor this week. This is a company I've used for. I know this company well,
[00:49:14.720 --> 00:49:22.480]   so I'm happy to have them as a sponsor. That is the online course company Udacity.
[00:49:22.480 --> 00:49:25.560]   Am I saying that right, Jesse?
[00:49:25.560 --> 00:49:26.120]   You are.
[00:49:26.120 --> 00:49:30.420]   Udacity. I think I should get a bell for pronouncing it. It's one of these things I've
[00:49:30.420 --> 00:49:35.140]   used Udacity courses for a long time, but you don't have an occasion to say them out loud
[00:49:35.140 --> 00:49:42.180]   until you're actually talking. So Udacity is the site you go to if you want to learn skills that
[00:49:42.180 --> 00:49:46.260]   are going to command high salaries. It's the online learning platform that has courses on all sorts
[00:49:46.260 --> 00:49:54.280]   of topics, but including AI, data, programming, and more. I have attended or purchased multiple
[00:49:54.280 --> 00:49:58.760]   Udacity courses. The most recent one I was doing with my son, it was a computer game programming
[00:49:58.760 --> 00:50:03.620]   course. I did another one with my other son on like Tinkercad based 3D design,
[00:50:03.620 --> 00:50:09.280]   like intro to 3D design. It's fantastic. It's a really easy interface. You can purchase the course
[00:50:09.280 --> 00:50:15.180]   easily. Some are free. They're video based, keeps track of where you are. Knowledge is power. We talked
[00:50:15.180 --> 00:50:19.580]   about this in the very first segment of the show. Like how do you get smarter? Online courses are a big
[00:50:19.580 --> 00:50:23.020]   way. I mean, you've been messing around with this. You just signed up for one, right?
[00:50:23.100 --> 00:50:27.400]   Udacity course. Yeah. I signed up for a digital marketing class. Oh, excellent. Yeah. And I'm
[00:50:27.400 --> 00:50:32.940]   teaching one now on Udacity about how to do a killer French accent. So it's, you know, it's been a real
[00:50:32.940 --> 00:50:38.760]   hit. Have your pipe. On a related note, someone just handed me a note and I am no longer welcome
[00:50:38.760 --> 00:50:43.840]   in France. Oh, okay. Well, that's good to know. They just sent me a note. So I guess that got me in
[00:50:43.840 --> 00:50:49.560]   trouble. There's tons of options for learning tech skills, but only Udacity is consistently ranked as
[00:50:49.560 --> 00:50:54.800]   a top skill development platform because unlike others, it actually works. I can attest to this
[00:50:54.800 --> 00:50:58.720]   with real world projects and human experts that grade your work. You'll truly get the skills you
[00:50:58.720 --> 00:51:02.480]   need. Yeah. When my son and I were doing the game programming, I mean, you're building the games.
[00:51:02.480 --> 00:51:08.640]   Like you're doing, you're doing the work right there. When you have a certification from Udacity,
[00:51:08.640 --> 00:51:13.380]   recruiters and employers take notice. So for a better job, better salary and better skills,
[00:51:13.480 --> 00:51:19.900]   check out Udacity today. I saw a YouTuber, a DIY YouTuber. I was watching with my kids the other
[00:51:19.900 --> 00:51:26.300]   day and he said his habit, he's a brilliant guy. He said his habit is to spend the first 30 minutes
[00:51:26.300 --> 00:51:31.280]   every day on an online course. He's like just making myself smarter before I move on to like the rest of
[00:51:31.280 --> 00:51:35.720]   my day. I love that idea. So maybe do that. Sign up for Udacity. Always have a course you're working on
[00:51:35.720 --> 00:51:41.280]   first 30 minutes of your day, get smarter, get your brain working. So the tech field has always evolved
[00:51:41.280 --> 00:51:47.000]   and you should be too. Always evolving and you should be too. You can try Udacity risk-free for
[00:51:47.000 --> 00:51:53.900]   seven days. Head to udacity.com slash deep and use the code deep to get 40% off your order. Once again,
[00:51:53.900 --> 00:52:01.240]   that's udacity.com backslash deep for 40% off and make sure you use my promo code deep so they know I
[00:52:01.240 --> 00:52:06.100]   sent you. I also want to talk about our friends at ZocDoc. I went to the dentist this morning.
[00:52:06.100 --> 00:52:09.960]   All right. This true story. I went to the dentist this morning that we are recording this.
[00:52:10.520 --> 00:52:17.420]   How did I originally find this dentist? It was like years ago, I was canvassing friends and someone
[00:52:17.420 --> 00:52:23.880]   said, Oh, there's this new dentist in town. And I, I got in and I liked him and it's been great,
[00:52:23.880 --> 00:52:31.080]   but the word got out and he's very popular. He's now booking seven months out. So like,
[00:52:31.080 --> 00:52:33.920]   if you want to make an appointment with them, it's like seven or eight months before there's an
[00:52:33.920 --> 00:52:37.060]   appointment. And so I was thinking today, as they were booking my next appointment,
[00:52:37.200 --> 00:52:41.500]   like seven or eight months out. Uh, imagine if I was looking for a new dentist now,
[00:52:41.500 --> 00:52:45.700]   what would I do? Like if I asked my friends and they're like, Oh, here's this one dentist and he
[00:52:45.700 --> 00:52:50.300]   doesn't have any openings for months. Like that is what it's like right now. Often trying to find the
[00:52:50.300 --> 00:52:54.680]   medical care you're working for. You get like one or two recommendations. And especially in a city,
[00:52:54.680 --> 00:52:59.980]   when you say, I would like to actually come see a practitioner, like maybe, maybe at like some point
[00:52:59.980 --> 00:53:07.320]   during like the current presidential administration, they just laugh like, Oh, you fool. You think the
[00:53:07.320 --> 00:53:11.740]   dentist actually can see you. And then there's like a lot of laughter and you hear them call someone else
[00:53:11.740 --> 00:53:15.980]   over and then they start laughing. Uh, and then like the doorbell rings and someone runs in and just
[00:53:15.980 --> 00:53:20.360]   punches you in the stomach because they're, they're making fun of you that much. This is where ZocDoc enters
[00:53:20.360 --> 00:53:25.280]   the scene. ZocDoc is a free app and website where you can search and compare high quality in-network
[00:53:25.280 --> 00:53:30.920]   doctors and click to instantly book an appointment. You can search for, here's what I'm looking for.
[00:53:30.920 --> 00:53:37.040]   And then see which of these take my insurance, which of these are actually booking appointments,
[00:53:37.040 --> 00:53:41.640]   right? This is how you would have found my dentist a few years ago. Oh, he's actually looking to take on
[00:53:41.640 --> 00:53:46.760]   new patients. Uh, okay, great. When is their next appointment available? Let me book it right now.
[00:53:47.220 --> 00:53:51.820]   Let me do some of the paperwork online using the ZocDoc interface. Actually, my dentist uses ZocDoc. So it's,
[00:53:51.820 --> 00:53:57.780]   so it's interesting. It makes it so much easier and so much less stressful and honestly humiliating to try to
[00:53:57.780 --> 00:54:03.440]   find because no one's laughing in your face for daring to ask if they have actual openings. Uh, so this is the
[00:54:03.440 --> 00:54:08.860]   way that you find medical care. Appointments made through ZocDoc also happen fast, typically within 24 to 72
[00:54:08.860 --> 00:54:16.220]   hours of booking. You can even often score same day appointments. So stop putting off those doctor's
[00:54:16.220 --> 00:54:24.460]   appointments and go to ZocDoc.com/deep to find an instantly book a top rated doctor today. That's Z-O-C-D-O-C.com/deep.
[00:54:24.460 --> 00:54:34.140]   All right, Jesse, let's do our final segment. All right. Because I can't help myself. We're a tech
[00:54:34.140 --> 00:54:39.180]   corner segment. We're going back to AI. There's so much to keep up with, with AI right now. And I want you
[00:54:39.180 --> 00:54:43.980]   to be up to speed. All right. Here's what I've been rabbit holing on recently.
[00:54:45.100 --> 00:54:52.220]   This idea that these new AI large language models like DeepSeq and the new O family of models from
[00:54:52.220 --> 00:54:58.140]   OpenAI can quote unquote reason. So you give them like a math problem or something and they can sort
[00:54:58.140 --> 00:55:03.180]   of work through the steps to get to an answer. They're doing much better on like math tests or
[00:55:03.180 --> 00:55:08.140]   crossword puzzle tests or something like this, right? Like they, they are reasoning. I'm really
[00:55:08.140 --> 00:55:14.060]   interested in this. All right. How did these things reason? Well, let's ask the companies. They're giving
[00:55:14.060 --> 00:55:19.580]   incredibly vague language and a lot of journalists are just repeating this language. So like OpenAI,
[00:55:19.580 --> 00:55:25.500]   when they talk about their new models, they say, uh, the new models, they, they slow down to think
[00:55:25.500 --> 00:55:31.180]   more and that's how they're reasoning. And this is the way it's being reported in like mainstream
[00:55:31.180 --> 00:55:34.700]   journalism. We're talking about, there was like just an article out in the times a couple of days
[00:55:34.700 --> 00:55:39.580]   ago about how do these new models reason. And it's just saying like, yes, the new models, what they do
[00:55:39.580 --> 00:55:44.460]   is they slow down. They slow down and they think, they think more. And so they can, they can produce
[00:55:44.460 --> 00:55:49.740]   more reason answers. What does that mean? Right? Like Jesse, when you think about like a model,
[00:55:49.740 --> 00:55:52.780]   like slowing down and thinking like, what does that mean? It feels like it's like an actual person.
[00:55:52.780 --> 00:55:57.900]   That's like, what, taking a deep breath and like, I'm going to like take my time. Like what's slowing
[00:55:57.900 --> 00:56:03.100]   down? Right. So I, you know, I called the source and, uh, talked about it. Like, Hey,
[00:56:03.100 --> 00:56:08.940]   walk me through this tech, point me to the right papers. Here's what's as best we can tell is actually
[00:56:08.940 --> 00:56:15.420]   going on. Okay. Uh, there was this idea that actually precedes chat GPT, right? This is an idea
[00:56:15.420 --> 00:56:19.500]   from the academic community, studying language models, 2022, you get this paper that says,
[00:56:19.500 --> 00:56:23.900]   if you want to increase the reasoning performance of language models,
[00:56:24.460 --> 00:56:30.780]   you should use something called chain of thought prompting, C O T prompting. And what they were
[00:56:30.780 --> 00:56:37.340]   showing in this paper is, uh, in the question, you're asking the language model, you basically
[00:56:37.340 --> 00:56:43.500]   tell it, I want you to show your work. Don't just give me the answer. Like show me your steps. This is
[00:56:43.500 --> 00:56:48.700]   what I'm looking for is walk me. I'll give you an example of it. See how I'm solving this problem here.
[00:56:48.700 --> 00:56:54.460]   Step-by-step. Give me an answer to this new problem with similar style of step-by-step explanation.
[00:56:54.460 --> 00:56:59.900]   And right. And what happened is this would give you better answers because language models, of course,
[00:56:59.900 --> 00:57:05.820]   are there, they're trained to, um, expand text in predictable ways. They're trained on taking real text,
[00:57:05.820 --> 00:57:09.900]   taking words or sequence of words out of them and having the language models, try to fill those in.
[00:57:09.900 --> 00:57:13.820]   And the closer they get to real words there, like the, the more they get rewarded in their training.
[00:57:13.820 --> 00:57:18.620]   So like what was happening is you would give a language model, like a logic problem.
[00:57:18.620 --> 00:57:22.540]   You know, Jesse has this many apples and then someone comes along and takes this many apples.
[00:57:22.540 --> 00:57:26.540]   And then the person who's taller gives you back this many apples. How many apples does Jesse have?
[00:57:26.540 --> 00:57:31.100]   And what the language models would sometimes do is give an answer that sounds like what an answer
[00:57:31.100 --> 00:57:35.900]   to that problem would look like. Like, yeah, I'm saying Jesse has some number of apples. Like that's a,
[00:57:35.900 --> 00:57:41.660]   for our language models perspective, that looks like, like a reasonable response to this prompt.
[00:57:41.660 --> 00:57:46.140]   It's talking about Jesse. It's talking about apples. There's a number, but that number could be arbitrary.
[00:57:46.140 --> 00:57:50.220]   Chain of thought prompting says, no, no, you, you have to really ask the language model,
[00:57:50.220 --> 00:57:53.820]   walk me step-by-step through your reasoning for how you get to the answer. And I'll give you an
[00:57:53.820 --> 00:57:58.460]   example of me doing that in my prompt. Now you do something similar. When it walks through its reasoning
[00:57:58.460 --> 00:58:03.820]   in the answer, it's more likely to get the correct answer because, um, as it fills out its reasoning,
[00:58:03.820 --> 00:58:09.660]   it's actually accessing relevant circuits that do different parts of the reasoning. And it's actually more
[00:58:09.660 --> 00:58:13.180]   likely, you're more likely to activate like the relevant circuits that can do the different
[00:58:13.180 --> 00:58:16.940]   calculations and put them together in the right way. And you're more likely to get the right answer.
[00:58:16.940 --> 00:58:23.660]   That's chain of thought prompting. Okay. So the issue is now we get, uh, chat bots and they're
[00:58:23.660 --> 00:58:28.620]   getting popular. We really can't rely on prompt engineering as something that like the average
[00:58:28.620 --> 00:58:35.260]   user is going to do, right? This is like the big, the big push with consumer facing large language model
[00:58:35.260 --> 00:58:40.860]   tools is like, we can't expect the users to write these incredibly complicated prompts to get their
[00:58:40.860 --> 00:58:47.420]   results. Researchers were doing this, but we can't expect the users to do this. So what seems to have
[00:58:47.420 --> 00:58:54.380]   happened is they said, can we basically use reinforcement learning to kind of force one of these models to
[00:58:54.380 --> 00:59:00.460]   always act as if it's being prompted to show its work? Right? So this is the key. And I'm not,
[00:59:00.460 --> 00:59:05.820]   I'll just be incredibly brief on this, but like the, the, the key that got like the original chat GPT so
[00:59:05.820 --> 00:59:11.660]   much notice is they took the first really large, large language model with was GPT-3, which could do
[00:59:11.660 --> 00:59:16.940]   amazing stuff, but you really had to write incredibly careful prompts to get it. And the big innovation,
[00:59:16.940 --> 00:59:21.740]   second innovation open AI had the first being just make the model large was this thing called, uh,
[00:59:21.740 --> 00:59:26.300]   we think of it as like re a reinforcement learning based fine tuning, where they took this model
[00:59:26.300 --> 00:59:30.860]   that had all this knowledge in it. And if asked correctly, could produce all sorts of stuff.
[00:59:30.860 --> 00:59:36.780]   And then they started running it with a bunch of prompts. And every time it gave like a good type of answer,
[00:59:36.780 --> 00:59:41.340]   they would zap it with a happy zap. And it was sort of like, Ooh, whatever weights we were using here,
[00:59:41.340 --> 00:59:45.100]   let's make those stronger. And whenever it gave an answer that was, they didn't like, they would give it a,
[00:59:45.100 --> 00:59:49.980]   a, an unhappy bad person's app. And it would be like, Ooh, let's kind of reduce some of these weights
[00:59:49.980 --> 00:59:54.620]   that gave us this answer. And so for like chat GPT, they were saying, we're going to run,
[00:59:54.620 --> 00:59:59.260]   we train GPT-3 and we're going to run a bunch of prompts. And when you answer this, like, uh, an
[00:59:59.260 --> 01:00:04.460]   agent that you're chatting with a human, we're going to give you a happy zap. And then, so it got good
[01:00:04.460 --> 01:00:09.020]   at answering questions without you having to write really careful prompts. I think what is happening,
[01:00:09.020 --> 01:00:12.860]   according to my source is that they're just doing this now for chain of thought reasoning.
[01:00:12.860 --> 01:00:16.700]   So they take one of these models, they'll, they train it up the old fashioned, you know,
[01:00:16.700 --> 01:00:20.060]   with all the, all the texts on the internet and just like any of these other models.
[01:00:20.060 --> 01:00:24.380]   And then as they start running through answers to it, giving it questions,
[01:00:24.380 --> 01:00:29.340]   when it shows more of its work, they give it a happy zap. And when it just gives an answer,
[01:00:29.340 --> 01:00:33.340]   they give it a negative zap. So this reinforcement learning fine tunes this big model,
[01:00:33.340 --> 01:00:38.300]   like, okay, okay, okay. I, I get it. You want answers where I explain my work.
[01:00:38.300 --> 01:00:41.020]   You could do this for anything, right? This is how they, um,
[01:00:41.660 --> 01:00:45.580]   stop certain types of content from coming out of these language models. It's completely trained.
[01:00:45.580 --> 01:00:51.100]   And then they, they give it some questions that might elicit responses that are like violent or
[01:00:51.100 --> 01:00:54.940]   something they don't like. And every time it gives a bad answer, they zap it. And then like over time,
[01:00:54.940 --> 01:00:56.620]   it's like, I'm not going to give those type of answers.
[01:00:56.620 --> 01:01:02.700]   So this is what I believe the reasoning models have done is they take a standard big train model,
[01:01:02.700 --> 01:01:06.620]   and then they do a reinforcement fine tuning step where they're saying, show your work,
[01:01:06.620 --> 01:01:12.060]   show your work, show your work, give us big, long answers. Um, and when we know this since 2022,
[01:01:12.060 --> 01:01:17.340]   pre-chat GPT, that when the chat bots, when the language models rather have to explain their work,
[01:01:17.340 --> 01:01:22.300]   they're more likely to give accurate answers. All right. Now, part of what they're doing here,
[01:01:22.300 --> 01:01:28.140]   in addition to that is these answers are so long, they're now hiding a bunch of it. It's like,
[01:01:28.140 --> 01:01:31.500]   what's really happening is the, the chat bot is giving this like incredibly long
[01:01:31.500 --> 01:01:36.620]   explanation for its answer, which helps the accuracy of it. But the explanation is so long
[01:01:36.620 --> 01:01:40.380]   that if you're a user who asked a question, you're like, oh my God, I don't want to see all of this.
[01:01:40.380 --> 01:01:45.820]   So they're, they're hiding a lot of that. And we know this because, uh, if you actually look at the
[01:01:45.820 --> 01:01:50.780]   user agreements, what's happening now, if you use these reasoning bots is they say, don't just count
[01:01:50.780 --> 01:01:57.580]   the tokens that you see. There's something called hidden tokens that it generated part of its answer
[01:01:57.580 --> 01:02:00.300]   that we're hiding from you and the user, because you don't need to see all that,
[01:02:00.300 --> 01:02:03.180]   that, but we have to charge you for it. Like, I mean, it produced all of this.
[01:02:03.180 --> 01:02:07.340]   So you can kind of quantify that, oh, they are generating these really long answers and then
[01:02:07.340 --> 01:02:11.900]   they're hiding a lot of it. So that like what you see is just some of the steps and then you get the
[01:02:11.900 --> 01:02:18.620]   final answer. So that's what reasoning is. It's not thinking slower. The reason why it takes longer
[01:02:18.620 --> 01:02:23.420]   is that it's just generating a much longer answer, right? That's what the slower is. Like when you
[01:02:23.420 --> 01:02:28.460]   see, you know, you see this when you use chat GPT, you know, it takes a lot, each token they produce,
[01:02:28.460 --> 01:02:34.540]   chat GPT still produces token by token. Deep seek is a gains efficiency by doing a few words at a time,
[01:02:34.540 --> 01:02:39.100]   but you notice how the words like come in one by one. It's because for every one of these things they're
[01:02:39.100 --> 01:02:45.500]   generating, you have 500 billion parameter, uh, transformer-based model pushing through to generate
[01:02:45.500 --> 01:02:50.700]   just that word. So it takes longer to generate these reasoning model answers because what it's really
[01:02:50.700 --> 01:02:57.740]   doing is writing like pages of explanation all the way up until its final answer. It just takes a long
[01:02:57.740 --> 01:03:02.620]   time to do all that. They don't show you all that work, but that's what the slowness is,
[01:03:02.620 --> 01:03:08.780]   is it's just giving really long answers. So that's what reasoning is. Um, it is different than
[01:03:08.780 --> 01:03:13.340]   when I talked around the New Yorker last year about planning and how language models can't plan. And they
[01:03:13.340 --> 01:03:19.340]   said, ultimately what language models will need to be, uh, flexible planners is planning engines outside
[01:03:19.340 --> 01:03:25.500]   of the language model. And it like you, and you need some sort of coordinator that is going to, uh,
[01:03:25.500 --> 01:03:29.980]   talk to a language model and talk to a planning engine. The canonical example is like playing
[01:03:29.980 --> 01:03:35.660]   chess against an AI bot. Like ultimately you can't just have a giant neural network that can, uh,
[01:03:35.660 --> 01:03:41.340]   be hard coded to, to look ahead a hundred thousand possible moves because that's just too big of a
[01:03:41.340 --> 01:03:45.820]   neural network to hard code. You, you, you need some sort of like systematic simulator to check those.
[01:03:45.820 --> 01:03:50.940]   And then maybe like the neural network to look at its quality, et cetera. So I still think there's only
[01:03:50.940 --> 01:03:57.340]   so much, this helps answers. It's, it's not the, the really sophisticated how from 2001 style
[01:03:57.340 --> 01:04:03.180]   reasoning is going to require non language model components that can actually recurse and do loops
[01:04:03.180 --> 01:04:07.020]   and have state and actually explore things more systematically. But that is what I think is going
[01:04:07.020 --> 01:04:12.300]   on with reasoning. It's the same language models based on the same type of original training that we've
[01:04:12.300 --> 01:04:18.940]   seen recently. These big 500 billion parameter models, plus reinforcement learning that gets it to show its
[01:04:18.940 --> 01:04:23.180]   work. And when language models show their work for certain types of problems, they're more likely to
[01:04:23.180 --> 01:04:28.060]   be accurate. So that is what is going on. I can't find this explanation in almost any common press
[01:04:28.060 --> 01:04:32.780]   article about this, but I think that is what is really happening. All right. So I can't help myself,
[01:04:32.780 --> 01:04:37.660]   Jesse, but there we go. Are you going to write an article about it? I mean, it's not really like a
[01:04:37.660 --> 01:04:42.140]   New Yorker article. I don't know. This is more of like a science, like a tech explanation article.
[01:04:42.140 --> 01:04:47.980]   You did a chat GPT article, didn't you? I've done some. Yeah. It's kind of boring.
[01:04:47.980 --> 01:04:50.860]   Yeah. So I'm making you guys listen to this instead.
[01:04:50.860 --> 01:04:55.100]   I think it's interesting. I'm just kind of frustrated that none of like the,
[01:04:55.100 --> 01:04:58.220]   the, the people who write the tech explainer articles, all the big names,
[01:04:58.220 --> 01:05:02.940]   they're all just falling back on this. It's just taking time to think more. And that's how the
[01:05:02.940 --> 01:05:06.380]   reasoning works. It's like, well, let's like actually understand, like, is there something,
[01:05:06.380 --> 01:05:09.260]   is there a new architecture? Is there a new technology? Like what's happening?
[01:05:09.260 --> 01:05:13.580]   Yeah. I had to read academic papers to try to just get that explanation. I think that's what's going
[01:05:13.580 --> 01:05:17.900]   on. All right. Well, speaking of slowing down, I think that's all the time we have for today,
[01:05:17.900 --> 01:05:22.460]   but we'll be back next week with another episode. And until then, as always stay deep.
[01:05:22.460 --> 01:05:26.940]   If you liked today's discussions about how to become a more elite thinker,
[01:05:26.940 --> 01:05:32.860]   you should check out episode 345, where I first introduced the idea that we need to train our brain.
[01:05:32.860 --> 01:05:36.300]   That was called, are we getting dumber? I think you'll like it. Check it out.
[01:05:36.300 --> 01:05:41.660]   So several people recently sent me the same article. It was from the financial time. So it was written
[01:05:41.660 --> 01:05:48.460]   by John Byrne Murdoch and it had a provocative headline, have humans past peak brain power.

