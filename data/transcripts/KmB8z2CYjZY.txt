
[00:00:00.000 --> 00:00:05.000]   Zach, Scott, thanks very much for joining.
[00:00:05.000 --> 00:00:07.000]   We'll dive straight into it.
[00:00:07.000 --> 00:00:13.000]   I would love to hear a little bit, before we get into a bunch of the technical questions I have around audio,
[00:00:13.000 --> 00:00:16.000]   all the cool things you can do with audio generation.
[00:00:16.000 --> 00:00:21.000]   I would love to maybe hear a little bit quickly on how Harmon and I came into being,
[00:00:21.000 --> 00:00:28.000]   how you two started to collaborate, how many are in the project, how long it's been going.
[00:00:28.000 --> 00:00:31.000]   Yeah, so a bit of background.
[00:00:31.000 --> 00:00:39.000]   I got into the AI text to image space through Disco Diffusion, through the OpenAI Glide notebook.
[00:00:39.000 --> 00:00:45.000]   Basically, I was proficient in Python, but not really doing a whole lot of AI stuff.
[00:00:45.000 --> 00:00:50.000]   And basically dove into the code in the notebook, saw what I could tinker around with,
[00:00:50.000 --> 00:00:54.000]   and figured out how to make some changes and actually get stuff going there.
[00:00:54.000 --> 00:01:02.000]   I got really interested in making audio-reactive music videos with Disco Diffusion,
[00:01:02.000 --> 00:01:08.000]   which I was working on, and then realized that to do so, it needed to be significantly faster,
[00:01:08.000 --> 00:01:13.000]   because having like 10 minutes per frame is just not going to work for trying stuff out.
[00:01:13.000 --> 00:01:19.000]   So dove more into the code and research behind Disco Diffusion,
[00:01:19.000 --> 00:01:25.000]   and then started talking with Kathryn Krausen on Discord, you know, getting up the courage to ask her a question
[00:01:25.000 --> 00:01:28.000]   after seeing her name on all the different notebooks,
[00:01:28.000 --> 00:01:35.000]   and was able to figure out some of the actual implementations behind these things
[00:01:35.000 --> 00:01:38.000]   and get to be able to page it to my will a little bit.
[00:01:38.000 --> 00:01:46.000]   After working on that for a little, I realized that a lot of the same math could be applied to music.
[00:01:46.000 --> 00:01:49.000]   So I got into music production three or four years ago,
[00:01:49.000 --> 00:01:55.000]   really starting focusing on it at the beginning of lockdown, after the COVID hit.
[00:01:55.000 --> 00:02:02.000]   And I basically started spending all of my time on music producer servers on Discord.
[00:02:02.000 --> 00:02:09.000]   And so when I realized that a lot of this text-to-image stuff could be applied to audio,
[00:02:09.000 --> 00:02:13.000]   that became a new passion, and I'm like, "I need to make that happen."
[00:02:13.000 --> 00:02:19.000]   I'm not a visual artist. I can't draw to save my life.
[00:02:19.000 --> 00:02:23.000]   But it was so personally validating, creatively validating,
[00:02:23.000 --> 00:02:30.000]   to be able to use these image synthesis tools to be able to have creative output.
[00:02:30.000 --> 00:02:38.000]   And realizing that the same creative accessibility could be given to musicians,
[00:02:38.000 --> 00:02:43.000]   that's much more my people, my friends are all producers and DJs.
[00:02:43.000 --> 00:02:47.000]   I realized that that was a purpose. All of a sudden, I'm like, "I need to make this.
[00:02:47.000 --> 00:02:52.000]   I need to bring these kinds of tools to the music space."
[00:02:52.000 --> 00:02:56.000]   So basically, I talked with Catherine, asked her if she'd done any audio diffusion stuff.
[00:02:56.000 --> 00:03:01.000]   She sent me some examples she had, trying it on speech.
[00:03:01.000 --> 00:03:06.000]   And it was like, "Oh, it kind of works, but it doesn't actually make coherent speech."
[00:03:06.000 --> 00:03:10.000]   Saying it's a failure. It was kind of like coming out and sounding like The Sims, right?
[00:03:10.000 --> 00:03:14.000]   Kind of garbled speech. And I was like, "No, no. That's the best sound design tool ever.
[00:03:14.000 --> 00:03:16.000]   You're just not using it on the right things here."
[00:03:16.000 --> 00:03:22.000]   So I'm like, "Okay, I have that training code. I'll turn it up to high resolution, 48k.
[00:03:22.000 --> 00:03:24.000]   I need it to be high resolution stereo."
[00:03:24.000 --> 00:03:32.000]   So much machine learning audio stuff is 16k mono, which works for speech and piano music.
[00:03:32.000 --> 00:03:37.000]   But I'm trying to do modern EDM. You need high resolution. You need stereo.
[00:03:37.000 --> 00:03:45.000]   So basically, taking that same code, applying it to audio, and taking some of the other image diffusion code,
[00:03:45.000 --> 00:03:52.000]   and learning enough PyTorch and enough of the math behind it to change 2D to 1D in the right places.
[00:03:52.000 --> 00:04:01.000]   And applying some of my knowledge from music production, and really just throwing stuff at the GPUs and seeing what sticks.
[00:04:01.000 --> 00:04:06.000]   I was able to get something going with that.
[00:04:06.000 --> 00:04:14.000]   I was really inspired by the whole open source AI movement and wanted to just be part of it.
[00:04:14.000 --> 00:04:20.000]   I worked at Microsoft for seven years before this doing some front-end design.
[00:04:20.000 --> 00:04:28.000]   And this is just so much more working with my passions and being able to really give back to the music community
[00:04:28.000 --> 00:04:34.000]   with extra cool tools and things to help them with their creative process.
[00:04:34.000 --> 00:04:42.000]   Amazing. That's a great story. And so where does Scott come in and everyone else involved in Harmony?
[00:04:42.000 --> 00:04:47.000]   Yeah, so Scott can definitely tell his own origin stories in Harmony.
[00:04:47.000 --> 00:04:55.000]   But as I was building the Discord server, I can talk about how that came to be, how I got in contact with Ahmad,
[00:04:55.000 --> 00:05:02.000]   how it became part of Stability. But basically, as I was given the opportunity to make a community around this,
[00:05:02.000 --> 00:05:08.000]   I kind of just started looking around and seeing who were the movers and shakers, who was already doing stuff,
[00:05:08.000 --> 00:05:13.000]   who could I just enable and bring into the fold and be like, let's be on this journey together.
[00:05:13.000 --> 00:05:24.000]   And someone referred me to Scott posting a message in, I believe, the Eleuther or Lion server about audio research.
[00:05:24.000 --> 00:05:32.000]   And I was like, oh, there's someone who is interested in this. So we sent him an invite, brought him in and he could tell the rest.
[00:05:32.000 --> 00:05:42.000]   Yeah, I've been involved with machine learning and audio, I mean, I guess, fully professionally since 2016.
[00:05:42.000 --> 00:05:51.000]   I first got hip to it back in 2013 when I saw Accusonus released a plug in for drum source separation.
[00:05:51.000 --> 00:05:58.000]   And I was like, this is magic. This is going to change the world. I need to learn this.
[00:05:58.000 --> 00:06:06.000]   Mostly because I wasn't planning on doing it myself. I just knew it was going to affect my students' lives because I teach audio engineering students.
[00:06:06.000 --> 00:06:11.000]   And yeah, over the years, I've put out academic papers.
[00:06:11.000 --> 00:06:17.000]   One time I worked with a company, ArtLogic, to produce tools for producers and musicians
[00:06:17.000 --> 00:06:25.000]   and definitely wanted to stay on the side of empowering creators and definitely in the audio space.
[00:06:25.000 --> 00:06:33.000]   And yeah, fast forward, I work by myself and I have tiny GPUs.
[00:06:33.000 --> 00:06:45.000]   And I guess last year, I got addicted to the text to image stuff with Catherine Croson's work, the EQ GAM+ clip.
[00:06:45.000 --> 00:06:52.000]   And then some guys, Chris Donahue and Ethan Manilow had shown that, hey, well, you know, there's a jukebox thing.
[00:06:52.000 --> 00:06:57.000]   Okay, if I'm given, you know what, you guys are going to delete tons of this anyway, so I'll just keep going.
[00:06:57.000 --> 00:06:59.000]   They'd show that, you know, hey, there's this jukebox thing.
[00:06:59.000 --> 00:07:07.000]   And if you pull apart what's inside of jukebox, you can do it's semantically interesting information.
[00:07:07.000 --> 00:07:12.000]   I was like, yeah, that, that's the ticket. That's what I want to do.
[00:07:12.000 --> 00:07:17.000]   I want to do text to not so much text to music, but text to music production.
[00:07:17.000 --> 00:07:23.000]   I want to say, give me a little more reverb or give me a little more this or that, because that's what my students do.
[00:07:23.000 --> 00:07:31.000]   And so this was last year. But then at some point, I guess it was, yeah, around March, like a bunch of things just all converge this spring.
[00:07:31.000 --> 00:07:41.000]   And Stella and Catherine and them put out their paper on the EQ GAM+ clip finally, you know, like a year after so many of us were already into it.
[00:07:41.000 --> 00:07:50.000]   I was never doing the disco diffusion thing, by the way, I'm definitely a late comer on the diffusion and pretty much your diffusion stuff, you will be talking to Zach today.
[00:07:50.000 --> 00:07:56.000]   But I wrote to them and said, hey, if you guys ever want to do audio, I have ideas.
[00:07:56.000 --> 00:08:06.000]   And as I was talking to Stella, somebody else from us, so Stella's involved with the Luther, somebody else was involved with Stability as well.
[00:08:06.000 --> 00:08:22.000]   And then there was this one week where like four Discord servers, all about machine learning and audio and production, all came together the same week I was like, are you people not talking to each other and I thought Zach and Stella and this other person
[00:08:22.000 --> 00:08:34.000]   were talking to each other. No, it's just all completely independent things happening all at once. And yeah, essentially what happened with Harmon, I was like, well, here are people to work with.
[00:08:34.000 --> 00:08:49.000]   And they've got compute, and I've got ideas. And actually at the time I thought I was going to have access to more data which I'm still working on that I'm working on consensual data with explicit consent, we'll get to that, you know, later.
[00:08:49.000 --> 00:09:03.000]   But yeah, kind of turned out, and then a lot of it worked out where, especially for me and Zach I kind of realized you know the way for me to really, really get up to speed on a lot of this is in helping Zach doing what he's doing, I will be training myself to get up
[00:09:03.000 --> 00:09:18.000]   to speed so that I can do what I really want to do. In addition to all that so it usually works out that, you know, we can fulfill multiple projects and multiple goals, kind of simultaneously.
[00:09:18.000 --> 00:09:20.000]   Amazing.
[00:09:20.000 --> 00:09:22.000]   Match made in heaven.
[00:09:22.000 --> 00:09:30.000]   And I guess the kind of most like mature project you have going at the moment would be dance diffusion.
[00:09:30.000 --> 00:09:45.000]   Yeah, I would love to hear a little bit more about that. And I think we leave some notes for people that they can like read up on more and like the basics of like diffusion, but maybe in like, you know, if you wanted to give 30 seconds of like how diffusion works
[00:09:45.000 --> 00:09:51.000]   and then specifically like what are the nuances involved in like doing it for audio would love to hear.
[00:09:51.000 --> 00:09:59.000]   Um, so, at a very basic level, because it gets into very high level math very quickly.
[00:09:59.000 --> 00:10:15.000]   You, you give the model noise and it makes it not noise in the form of whatever it has seen that's unconditional diffusion so dance diffusion is an unconditional diffusion model compared to things like stable diffusion, which is conditioned on text.
[00:10:15.000 --> 00:10:29.000]   And the reason I'm starting with an unconditional diffusion model is that's kind of what started the whole image diffusion frenzy was the unconditional image net model released by open AI, people being able to take that unconditional model which
[00:10:29.000 --> 00:10:36.000]   just makes something it learns about some distribution of data and can turn noise into that.
[00:10:36.000 --> 00:10:50.000]   And then other classifiers really anything that will give you a strong gradient, you can take that to guide that denoising process and so that's what that clip guided diffusion was was taking that unconditional diffusion model and using clip which is able
[00:10:50.000 --> 00:11:06.000]   to compare pictures, basically saying how well does the caption match an image and say all right now optimize this thing simultaneously optimize that this image matches this caption, the captions not changing, so you better change the image be more like that.
[00:11:06.000 --> 00:11:21.000]   In terms of audio because I'm a music producer, I think of like sidechain compression and sidechain effects where one thing is affecting the other from this kind of separate things I think of, I like to call disco diffusion stable diffusion denoiser where the
[00:11:21.000 --> 00:11:35.000]   text input sidechain and putting it in that context because of that that text input thing is really it could be anything like arbitrarily anything that can get you a gradient to say make this better anything to optimize at the same time, putting out an unconditional
[00:11:35.000 --> 00:11:51.000]   diffusion model allows that space of research to happen so all of that guidance, people can find different things and you know, there was the, the crash model that was put out by Sony CSL which was for diffusion for high resolution drum synthesis, and they
[00:11:51.000 --> 00:12:00.000]   were able to use a classifier to tell between kicks snares and symbols, and through using that just that classifier, they were able to tell it to make a kick a snare or a symbol.
[00:12:00.000 --> 00:12:10.000]   So there's already so many existing music classifiers and tools like that, that could be hooked up to any unconditional audio diffusion model to do so many cool things.
[00:12:10.000 --> 00:12:20.000]   That's one of the things that we're really trying to look for with dance diffusion, kind of in the same vein as disco diffusion disco diffusion people will generally see as one of the first big open source text to image notebooks and it
[00:12:20.000 --> 00:12:24.000]   most importantly included animation support.
[00:12:24.000 --> 00:12:40.000]   But really what I see disco diffusion is was a crowd sourced hyper parameter search on guidance losses, because this good diffusion didn't have just the text it had things like, you know, the, the losses for for saturation and color and total variation
[00:12:40.000 --> 00:12:59.000]   all these different things to, to change. And what that became was everyone you know trying to get in there, if I change this 0.01 to 0.001, it changes this and so basically having this ensemble of guidance losses and kind of having everyone get in there
[00:12:59.000 --> 00:13:15.000]   and tweaking out, messing around and tweaking those parameters to find what's actually good, that's kind of thing that I want to create for the audio world, we can give out these models that are, you know, unconditional diffusion and then say and here's
[00:13:15.000 --> 00:13:29.000]   the raw stuff in there, we're going to see an explosion of creativity here I mean throw in, you know, make a model that is printed on a good amount of data and throw in a genre classifier, will it start making things in that genre, quite possibly.
[00:13:29.000 --> 00:13:38.000]   And of course it gets trickier when you get things like oh well the classifiers are all 16k mono, and our audio is 48k stereo. There's ways around that you can you can deal with that.
[00:13:38.000 --> 00:13:50.000]   So the I'm trying to you know the the dance diffusion models I've put out there's not one massive model we have trained on 100,000 hours of audio data, you know, it's not jukebox.
[00:13:50.000 --> 00:13:57.000]   What we have is a variety of different models from the same architecture, so you can you know they're all the same thing but just fine we've done different data sets.
[00:13:57.000 --> 00:14:10.000]   So we have these models that are trained on different data sets that are either public domain or given to us by the community. So that gives out the architecture that gives out here's a basic list of things you can do with it here's a few sample things but
[00:14:10.000 --> 00:14:25.000]   we also put out a fine tuning notebook. So people can take that same model fine tune on their own data on their own content, and it enables kind of this extra level of interaction, like personal research and creativity.
[00:14:25.000 --> 00:14:38.000]   And I think will be really interesting to expose people to I think a lot of people right now are really used to the kind of plug and play, you know, gotcha pawn type in the text prompt and get out some fun stuff, which is great fun I really hope that we get
[00:14:38.000 --> 00:14:56.000]   to that kind of at some point, but there's a whole lot of research, we have to do to get there. I see dance diffusion as us putting out basically a catalyst for to have to happen to audio what has happened to images in the last two years.
[00:14:56.000 --> 00:15:08.000]   And so it's really, it's not that this is going to be the be all end all model far from it I mean, one of them is trained on audio from Jonathan man who we are working with he has the Guinness World Record for most consecutive days writing a song he just
[00:15:08.000 --> 00:15:16.000]   hit 5000. We've got you know almost 5000 songs and the same guy, great training data that you don't get that from a lot of people.
[00:15:16.000 --> 00:15:22.000]   He was generous enough to give us all that audio data and allow us to release a model trained on that.
[00:15:22.000 --> 00:15:38.000]   And some, you know, pack from glitz with friends who was a producer community that was basically finding people to provide us good audio training data with consent training on that and you know releasing that.
[00:15:38.000 --> 00:15:42.000]   Yeah, and then kind of seeing where the rest of the community takes it.
[00:15:42.000 --> 00:15:50.000]   Amazing. So, so we're, we're on the launchpad and you're going to like like the spark and kind of see where the rocket goes.
[00:15:50.000 --> 00:16:02.000]   And it's, it's, you know, it's hard to plan for this stuff because you have no idea where community is going to take it. And we're seeing that was just with stable diffusion there's this Twitter thread that I just saw, you know, stable diffusion right out for a month
[00:16:02.000 --> 00:16:14.000]   and there's already a year's worth of product releases, based on that we put out dance diffusion kind of sneak launch has been on GitHub for a while we haven't been talking about and publicizing it yet.
[00:16:14.000 --> 00:16:24.000]   But mostly just so we can get the infrastructure in place to handle the community that will inevitably inevitably form around it, you know the disco diffusion server got made within a week it was like 3000 people.
[00:16:24.000 --> 00:16:40.000]   So basically we want to have the open server for people talking about dance diffusion and harmony, but we had to make sure we had people in place to be able to withstand that, you know, that that incoming herd of excited people and be able to properly
[00:16:40.000 --> 00:16:46.000]   organize them and make sure that people can probably contribute to these things.
[00:16:46.000 --> 00:16:53.000]   Cool. Yeah, I'd love to chat about how people can get involved in a bit but just going back to the model itself.
[00:16:53.000 --> 00:17:03.000]   I'm curious, what were the nuances or tweaks you had to make right to make it work for for audio and then also like you said dealing with, you know, different.
[00:17:03.000 --> 00:17:08.000]   Yeah, like the like mono versus stereo and like different resolutions. Yeah.
[00:17:08.000 --> 00:17:18.000]   So, with the model we have now we're not doing a whole lot of extra bells and whistles for audio stuff specifically.
[00:17:18.000 --> 00:17:34.000]   That is, I am a bit of an unsupervised learning purist, when it comes to audio, where a lot of audio stuff they'll do you know mouse spectrograms or MFCC is throwing out phase data which as someone who, you know, makes psytrance plucks and drums is absurd
[00:17:34.000 --> 00:17:40.000]   to me that phase would be thrown out as you'll bring out the baby with the bathwater there.
[00:17:40.000 --> 00:17:54.000]   But you know kind of being like what if we just threw the data at it in a big unit, and just saw what it learned can learn all this stuff just by being you know there's with with diffusion the way we're training it, there's no reconstruction losses,
[00:17:54.000 --> 00:18:01.000]   it's you're training it to predict the score gradient, the noise.
[00:18:01.000 --> 00:18:19.000]   So it's, it's basically like almost like a data agnostic objective. And so I was really curious to see how good can we get if we don't do things like turning it into an STF, you know, doing STF T's on it, you know, which definitely would simplify it,
[00:18:19.000 --> 00:18:22.000]   but I'm like, all right, let's let's push the boundaries here.
[00:18:22.000 --> 00:18:37.000]   So, basically, we haven't done that much other than changing 2D to 1D, adding more layers and you know figuring out what size of sample rate and data you want to throw in there.
[00:18:37.000 --> 00:18:54.000]   So that's kind of one of the, the, it's, it's, it's simple, but it's just powerful and big. One of the downsides of that is, you know, with with audio versus image data you get differences in how the different frequency ranges are interpreted.
[00:18:54.000 --> 00:19:13.000]   Whereas, you know, humans we hear, we hear sound logarithmically in terms of frequency and, you know, with with music things like hi hats might be only at 18k and above, which also means if you downsample it you lose the instrument.
[00:19:13.000 --> 00:19:29.000]   And so with our model we are seeing that it will reconstruct the low end better than the high end that's really common in audio models, just because like, if it's if it's loud and low it's really easy for it to find that you've got these big sine waves
[00:19:29.000 --> 00:19:36.000]   you can totally you know get a little bit of air on there and it's still just fine, get a little bit air on the high end just blows it all out of proportion.
[00:19:36.000 --> 00:19:50.000]   So there are some things that we've seen like diffusion models that have an STFT as a backbone to get the complex STFT the frequency and phase information and work in that space, which adds an implicit bias to, you know, basically have a model
[00:19:50.000 --> 00:19:59.000]   understand there is that there is a band split here there are different frequency ranges they all matter, kind of equally in their own proportions.
[00:19:59.000 --> 00:20:03.000]   But, you know, we've been pretty.
[00:20:03.000 --> 00:20:15.000]   Kind of enjoying just the the naive model way of being like, here's a bunch of data, figure out what I just showed you make me things like that. I'll make you big enough where you can do that.
[00:20:15.000 --> 00:20:27.000]   It's, you know, I've only been in the machine learning world but actually I started training models with my first audio diffusion model back in March. And so it's kind of a nice easy to be like this first one's going to be pretty easy.
[00:20:27.000 --> 00:20:40.000]   Make the numbers big, make it so you don't run out of memory, throw a bunch of data at it, it'll learn what you show it like cool sounds good. So, you know, did that over and over again watching, watching my weights and biases reports daily.
[00:20:40.000 --> 00:20:48.000]   And just waiting for it to get better. We had this spectrograms in there that Scott made some utilities for.
[00:20:48.000 --> 00:21:01.000]   And it's been you know Scott's been fantastic and making a bunch of extra utilities to get the visualizations and stuff working with weights and biases and these 3d embeddings of our, our 3d plots of our embeddings.
[00:21:01.000 --> 00:21:09.000]   You know, basically when you're training these models it's good to kind of watch the harmonics go from the bottom of the oh it's learning more it's learning the middle now it's learning the high end.
[00:21:09.000 --> 00:21:25.000]   I was actually curious about the 3d point clouds you were you were visualizing of your embeddings and like what you what do you would you get out of that, you know, often embeddings are very amorphous and hard to interpret.
[00:21:25.000 --> 00:21:28.000]   I will let Scott answer that because he made that.
[00:21:28.000 --> 00:21:32.000]   Yeah. Um, right now we're not.
[00:21:32.000 --> 00:21:43.000]   I'm not sure how much I want to be quoted in publication on this. So watch the language on this right now it's still I'm gonna be, but I'm going to be deliberately very careful.
[00:21:43.000 --> 00:21:46.000]   Right now it's still an open area of research.
[00:21:46.000 --> 00:21:54.000]   What's the structure of these embeddings, what constitutes a good structure for the embeddings.
[00:21:54.000 --> 00:22:03.000]   Really right now we're using it as it's a diagnostic tool like are the values at least reasonable values like they're not exploding they're not all going to zero.
[00:22:03.000 --> 00:22:14.000]   So if we get different kinds of, you know, are different sounds, ending up in different parts of the embedding space that's usually a good thing.
[00:22:14.000 --> 00:22:26.000]   Like if we just have a amorphous ball of everything then maybe that's not doing because one of the, at least what I want to do at some point I mentioned I want to use these embeddings for interesting stuff.
[00:22:26.000 --> 00:22:30.000]   And so, how interesting are they just in the process of training this.
[00:22:30.000 --> 00:22:42.000]   And then also since we're building like the diffusion model, at least some of our models will take those embeddings and then do diffusion from that will generate the audio from the embeddings.
[00:22:42.000 --> 00:22:56.000]   So, we want to be able to compare you know where, what is our system doing not just on the final output, but can we see inside
[00:22:56.000 --> 00:23:13.000]   of a style GAN, style GAN like latent to a diffusion process, like co trained. I took some, there was some code that Kathleen had written I kind of combined them together to make, you know, be able to get a convolutional encoder and get a latent
[00:23:13.000 --> 00:23:21.000]   series and then use that as conditioning for diffusion model, co train all of that you basically end up getting an auto encoder.
[00:23:21.000 --> 00:23:30.000]   But it's a really strong auto encoder because it's multiple paths because it's diffusion based, it's like our auto encoded to keep refining itself.
[00:23:30.000 --> 00:23:39.000]   And so you can get like really good compressed spaces, really good, you know, results.
[00:23:39.000 --> 00:23:55.000]   And so, it just works differently and I've been trying to train like a sound stream replication or other kind of array of models, and being able to have an iterative auto encoder, you kind of, it gives different kinds of artifacts, as opposed to giving
[00:23:55.000 --> 00:24:03.000]   maybe some of the, you know, down sampling artifacts or the horrible hum of some auto encoders.
[00:24:03.000 --> 00:24:09.000]   And so it kind of like just gives a bit more of a noisy high end, a little bit more natural.
[00:24:09.000 --> 00:24:22.000]   And also because it's diffusion you can make a giant diffusion model as a decoder and really, you know, just have it learn a whole bunch of stuff so that's been a really interesting kind of research there, and also how we get those those embedding clouds
[00:24:22.000 --> 00:24:35.000]   where you can kind of see, you get your embedding starting out and oh it's all just noisy and then kind of over time you start seeing it clustering you see the different, you know, like a batch of 16 go through and you'll see oh, they're all kind of organizing
[00:24:35.000 --> 00:24:51.000]   themselves and hey you can kind of see, you know, this one's a wider embedding cloud and that audio is more stereo, maybe that's what learning that you get a little bit of like kind of intuition and on the interpretability of the embeddings there.
[00:24:51.000 --> 00:25:03.000]   And also having you know having an auto encoder like that, let's you do latent diffusion, so that gives us that embedding space that we can then diffuse in that space and diffuse much longer sequences, and basically increase the power of the fusion
[00:25:03.000 --> 00:25:08.000]   models by by stacking the, you know, the diffusion in the auto encoder space.
[00:25:08.000 --> 00:25:12.000]   That sounds insane. I love it.
[00:25:12.000 --> 00:25:22.000]   I would love to see you, you touched on it earlier and Scott Scott did also the data that you're training these models on.
[00:25:22.000 --> 00:25:39.000]   Curious just the, the variety of like audio and like music styles you've trained on and then also you mentioned it earlier but you know, are there issues around copyright and kind of what do you think the like reception will be from musicians and artists.
[00:25:39.000 --> 00:25:41.000]   Once this is released.
[00:25:41.000 --> 00:25:44.000]   Yeah, I think it's really good to have internal training data.
[00:25:44.000 --> 00:25:54.000]   You know, I've got a lot of electronic music that I've been testing stuff out on because it tends to be full spectrum high resolution stereo extra stuff going on at the high end and low ability to test out.
[00:25:54.000 --> 00:25:58.000]   You know, there's a whole range of distorted signals.
[00:25:58.000 --> 00:26:09.000]   You know, going on just training on piano music. It ends up being a little bit easier to do some sort of thing because it's all piano, when you work in a bunch of dubstep it's every distortion possible under the sun.
[00:26:09.000 --> 00:26:15.000]   So that kind of gives you kind of a hard mode of things like this.
[00:26:15.000 --> 00:26:23.000]   I haven't released anything with that data because you have to do a lot more evaluation around memorization and overfitting.
[00:26:23.000 --> 00:26:37.000]   Just because you know I don't want to put out a model that is going to be trained on a bunch of copyrighted data and spit it back out, that would be, you know, bad from a copyright perspective that would be bad legally that would be bad science that is wouldn't be
[00:26:37.000 --> 00:26:39.000]   overfitting memorizing things.
[00:26:39.000 --> 00:26:48.000]   So because we don't have a full suite right now to be able to like really tell this thing has not memorized anything. And because these models can be susceptible of memorization.
[00:26:48.000 --> 00:26:54.000]   I haven't released any models trained on copyrighted data that I would be concerned about.
[00:26:54.000 --> 00:27:10.000]   That is going to continue to be true for us until we figure out how to not step on that landmine, that's basically one of the really big things that's held back music machine learning is just music copyright is something you don't want to go up against you
[00:27:10.000 --> 00:27:26.000]   know that's what brought down Napster if it brought down all the, all the fun things music, they, you know, they blow up they're really cool and then the RIAA whoever comes in and it's like nope, you aren't doing that so I hope for a long and prosperous
[00:27:26.000 --> 00:27:37.000]   future for harmony and our products and so I want to make sure we fully understand the implications of training on and releasing on copyrighted data before doing so.
[00:27:37.000 --> 00:27:46.000]   And then the artist reactions to it. You know I'm talking with a lot of producers and I show them these tools and they are blown away by it they love it.
[00:27:46.000 --> 00:27:50.000]   You know, because I'm making these tools that right now.
[00:27:50.000 --> 00:27:58.000]   Because you know we're working on high resolution audio, the digital model we have out there will only make about one and a half to three seconds of audio at a time.
[00:27:58.000 --> 00:28:12.000]   So that's not very good if you want to make a full song right, having three seconds of a song. Okay, cool. We got half of a line of a verse and it sounded kind of like a person, but now throw in drums throw in loops throw in little clips all of a sudden
[00:28:12.000 --> 00:28:16.000]   it becomes, you know, heaven for someone who uses sampling.
[00:28:16.000 --> 00:28:28.000]   So I trained on a bunch of a model train on a bunch of old records, like old like ballroom dance music and like songs from, you know, 30s and 40s.
[00:28:28.000 --> 00:28:40.000]   And I got a friend who makes hip hop and I gave him that model and he was able to just keep running that and get samples out of that and use those in his tracks and so that kind of, it gives this extra level of sampling.
[00:28:40.000 --> 00:28:57.000]   And so it's, it's basically making tools that are more useful to producers first, as opposed to here is a thing that can make a beautiful song and oh no artists are threatened, not really artists are much more empowered by these tools first because they can use
[00:28:57.000 --> 00:29:06.000]   them, you still have to have composition, you still have to have taste, you still have to, you know, they're not, they're not well mixed they're gonna be a little bit noisy can you use that.
[00:29:06.000 --> 00:29:15.000]   So basically, it's when I've talked to artists but as producers about this, they love it.
[00:29:15.000 --> 00:29:24.000]   And, yeah, it kind of helps that. Well the easier thing to do right now is make a sample generator not a song generator.
[00:29:24.000 --> 00:29:35.000]   So it's like okay kind of get a win win there of like it's really useful for the artists, they can train it on their own stuff. And it's not going to be copying a full song, it can't.
[00:29:35.000 --> 00:29:51.000]   Yeah, and then you know, eventually I would love to have a model that is trained on all known music, and we can say give me 50% Bruno Mars 50% Beethoven whatever that'd be great that would be so fun to play around with for inspiration for, you know,
[00:29:51.000 --> 00:29:58.000]   for whatever just see what it would what it was spit out. I mean, you know, kind of do things like that with jukebox.
[00:29:58.000 --> 00:30:07.000]   But like, as we're seeing particularly with with the AI art stuff with the visual art. There's a lot of concerns around there.
[00:30:07.000 --> 00:30:20.000]   You know, I don't want to get into the whole that whole conversation now because it's boy is that is that sticky. But, um, yeah, definitely you know things like Holly her and and Matt dry hearse spawning platform where they're putting together, you know, opt in and opt
[00:30:20.000 --> 00:30:30.000]   out lists of artists content, you know, what we're seeing from a lot of the artists we're talking to when they hear what we're doing they want to throw their data at us, because they really want this to happen.
[00:30:30.000 --> 00:30:42.000]   And so I think that we can get enough data from, you know, enthusiastically consenting artists and producers, things that we can clear to be legally scraped and trained on that are publicly available.
[00:30:42.000 --> 00:30:50.000]   And, you know, any deals with people that own the data or whatever you know getting it in through proper channels.
[00:30:50.000 --> 00:31:02.000]   We can get sufficiently high quality models out there where we don't have to train on things that end up being, you know, damaging to anybody or concerning like that.
[00:31:02.000 --> 00:31:15.000]   So it's a constant conversation it's a constant effort to find that balance between, it will be really cool to have this, and let's make sure we're not causing more problems I mean in the music industry it's already very exploitative it's already very much
[00:31:15.000 --> 00:31:29.000]   like everyone fighting for a few dollars. And so you know I, my friends are all DJs and producers, I wouldn't want to take away their last chance of making money in this if any I want to, I want to empower all of them to have more ways to make more
[00:31:29.000 --> 00:31:41.000]   art to make it to make it you know in new sounds and new things and that's, you know, I have I have skin in that game that's my community.
[00:31:41.000 --> 00:31:50.000]   Scott I think you thought there. Yeah, sorry I accidentally opened my AirPods, and it switched all my audio settings via Bluetooth.
[00:31:50.000 --> 00:32:00.000]   So, you know, if you know where to look, there do exist some high quality open source audio data sets that come with like a creative commons license.
[00:32:00.000 --> 00:32:10.000]   So like, I was part of developing one a few years ago for a system called signal train. There's some folks from NYU that have released some things.
[00:32:10.000 --> 00:32:31.000]   It's not sufficient to train a LMM, a large music model, but essentially by aggregating stuff that that people give us as people like Jonathan Mann and other friends of Zach, and then some of these open source data sets that have already been published
[00:32:31.000 --> 00:32:48.000]   by, you know, have an explicit license of creative commons or something like that. We can actually get, we have terabytes of stuff that we can train on legally already, which is kind of, it's not, it's not the commercial stuff like yeah you wouldn't, it
[00:32:48.000 --> 00:33:00.000]   doesn't know words like it doesn't know how to do Taylor Swift, or Bruno Mars, it doesn't know how to do that, but it does know some other things and yeah we're still.
[00:33:00.000 --> 00:33:08.000]   So, Justin is here. He's my, he's the actual music expert on at least out of the two weights and pices here.
[00:33:08.000 --> 00:33:13.000]   But I want to make sure he's time for a couple of questions. If you want to.
[00:33:13.000 --> 00:33:17.000]   You probably know more about what a sidechain is than me Justin.
[00:33:17.000 --> 00:33:20.000]   That was a great description.
[00:33:20.000 --> 00:33:33.000]   So, you touched on actually a lot of my questions that I was going to have like, you know, kind of curious how faithfully it might have recreated some of the training data but the fact that it's spitting out smaller samples that kind of answer that question.
[00:33:33.000 --> 00:33:46.000]   And you did touch on this but I wanted to be a little clear like, you know, we were big fans of the opening I jukebox stuff but I found like a lot of that music sounded kind of like underwater and gargley like you could you could hear that it was supposed to be Elton John,
[00:33:46.000 --> 00:33:51.000]   like a progression felt like an Elton John song.
[00:33:51.000 --> 00:34:03.000]   And so like, were there particular, I think you touched on when you're talking about like hats and how it's high were there particular things the model really struggled with or was it really just like lower stuff easier, higher stuff harder.
[00:34:03.000 --> 00:34:17.000]   In general, what we saw very quickly with the fusion models is they do great with percussion, they do great with anything that's transient heavy and more noise like, and they struggled for a while until we figured out just I guess I don't even know
[00:34:17.000 --> 00:34:31.000]   what what actually got it to start making harmonic content better. But, yeah, it's much better at things that are noisy than things that are harmonic because, you know, harmonics are basically the opposite of noise actual like tonal content.
[00:34:31.000 --> 00:34:46.000]   So in terms of you know generated stuff and like it's underwater and it's going to be basically it has a bad high end it's not reconstructing the high end harmonic statefully, it's, you know, and that can be, and and I'm sure someone who's been doing
[00:34:46.000 --> 00:34:54.000]   this longer could could give more even even more detailed reasons why you end up getting more high end error in general with things.
[00:34:54.000 --> 00:34:59.000]   And when you have an auto regressive model like jukebox.
[00:34:59.000 --> 00:35:08.000]   There's a much better actual mathematical reason for this but yeah it's just easier to predict the low and mid range stuff.
[00:35:08.000 --> 00:35:16.000]   And the high end kind of just ends up suffering for it and so to have something that's not going to be good in the low end and garbled in the high end.
[00:35:16.000 --> 00:35:33.000]   It kind of involved. I would guess you would want like a more full band and coding of things are using like an STF T, where you've got, you know, in the time domain, the same amount of importance to the different parts of frequency spectrum and
[00:35:33.000 --> 00:35:50.000]   then you can model Hey, I care about it like this. I care about it in these amounts, doing things like perceptual waiting to change the EQ curve be more like the human auditory perceptual system so that when you do the direct comparison to try to get
[00:35:50.000 --> 00:35:57.000]   that loss function it's more weighted towards how we perceive things not necessarily perhaps in the low end as much.
[00:35:57.000 --> 00:36:03.000]   And then it comes down to. I also think,
[00:36:03.000 --> 00:36:11.000]   maybe this is maybe this is a hot take I'm not sure, having that standard of quality, where it's like will you release something that has a bad high end.
[00:36:11.000 --> 00:36:25.000]   For me, I kind of have to at some point because it doesn't it doesn't always just come together like you want it to, but I would much rather put in a lot more time to address those issues, then be like, that's good enough.
[00:36:25.000 --> 00:36:37.000]   I'm going to put out something and be like, I couldn't figure it out. I couldn't do it. We don't have a good high end anymore. It's not good yet. But I mean that's kind of one of the things that I'm trying to do with harmony is have that high standard of quality.
[00:36:37.000 --> 00:36:47.000]   A bit more background I was one of the production communities I was in was for the artist kill the noise, who was a very well known dubstep producer with skrillex.
[00:36:47.000 --> 00:36:58.000]   You know, doing it for a long time, and I was on a kind of his Twitch co host for a bit and I was on his Twitch stream and he was playing around with jukebox and DDSP from Google Magenta.
[00:36:58.000 --> 00:37:11.000]   And, you know, he's a, he's a dubstep producer he can take anything and make it sound good, pretty much he can you know you know all the different effects to put on things to make it work, but messing with jukebox and DDSP the conclusion after a few hours
[00:37:11.000 --> 00:37:25.000]   was, this is cool. It's not quite ready for primetime. It's not quite good enough for me to actually use it in my songs. And so having coming into harmony I have that bar of, I want to make things that are usable by producers.
[00:37:25.000 --> 00:37:37.000]   If it's just a cool research product but not even one musicians, that's not good enough for me. You know I'm not I'm not I don't come from the academics here it comes from the, from the producer sphere and so having just the this got 1% better performance
[00:37:37.000 --> 00:37:45.000]   baseline for this thing that's that's neat but doesn't make dubstep, you know, can it make a good snare.
[00:37:45.000 --> 00:38:02.000]   Like that's it's I am much more interested in applied practical research for producers, and for artists with that bar of quality being being higher and being you know I needed to be at least 44.1 K sample rate 48 K helps.
[00:38:02.000 --> 00:38:14.000]   One of the things I learned talking with one of the guys in the data bots team who were working with neighbor doing generative music stuff. You know they're like the biggest names already been doing it for like 10 years.
[00:38:14.000 --> 00:38:22.000]   When you do a higher sample rate because there's that error in the high end. If you keep pushing the high end past our hearing range.
[00:38:22.000 --> 00:38:34.000]   You're going to get some of that up there too. Obviously it takes more compute intensive to do higher sample rate things but you kind of get a little bit of that like well, it sounds bad to dogs.
[00:38:34.000 --> 00:38:47.000]   And so you know kind of, it's almost like a trade off course because you're trading off the sample rate and the time that you can get all that but yeah having that that high end stuff you know having a 48 K model.
[00:38:47.000 --> 00:39:00.000]   I want 48 K stereo, I want to be able to put in cool wide bass sounds and how to make me variations on that like disco diffusion I want to be able to put in, you know, that was kind of one of my things that I just saw in my head earlier I needed to make
[00:39:00.000 --> 00:39:12.000]   was like, I need to be able to get variations on snares. I need you to be able to put in a snare drum and say give me 10 of these without having to tweak the compression all the little parts and EQ and stuff like just give me.
[00:39:12.000 --> 00:39:24.000]   What's the worst. Give me 10 of these. What's the worst. Give me 10 of these. Give me more of them. That's already what people are looking for and looking for sample library for drums is like I want a thing that sounds kind of like this, but isn't this.
[00:39:24.000 --> 00:39:30.000]   You know I was around Robin right. Yeah, I was working on a song.
[00:39:30.000 --> 00:39:44.000]   And I had this category is Casimir pack from splice, and I need to feel that sounds just like this, but I've already used that one. I don't have one that sounds just like this, and to learn how to make one like that is like eight more years of music production
[00:39:44.000 --> 00:39:46.000]   skills.
[00:39:46.000 --> 00:39:55.000]   And so like if I could just, you know, while doing that and looking at this code of using and being like, I'm making variations of things right here. The math is there, someone has to do this for drums.
[00:39:55.000 --> 00:39:57.000]   I'll do it.
[00:39:57.000 --> 00:40:11.000]   So it's basically like all right, you know I see the utility immediately, you mentioned, I'm not, you know, like wanting to release things that people could use and we had a question on here that we, we walked past but are there things that like the only one I've
[00:40:11.000 --> 00:40:24.000]   heard folks I know that use pretty, pretty regularly a splitter, which I think is like a pretty cool tool. Is there anything that you guys are using. Is there anything that you use in this space now that you feel like is like you could you could put it in a song and it would
[00:40:24.000 --> 00:40:35.000]   be like, you know, like you mentioned like opening a jukebox was not at the level, but like I'm just curious if there's like other research or models in the field you guys kind of found inspiring or like up to that quality threshold.
[00:40:35.000 --> 00:40:51.000]   Can I jump in for a second. Oh yeah, I was gonna say so one other area, one other way in which you know Zach and I are like on the same page. I really like I am an honorary member of the audio engineering department at Belmont, I live, all my students
[00:40:51.000 --> 00:40:57.000]   and colleagues are pro audio engineers, I go to AES conference.
[00:40:57.000 --> 00:41:14.000]   And so yeah having that high standard of audio quality is an absolute core value, I would say, you know, and so much the machine learning world puts out stuff that when they, when they get into doing musical audio, they like put out some piano or something,
[00:41:14.000 --> 00:41:19.000]   and it just sounds, I use the phrase it sounds like ass you can quote me on that.
[00:41:19.000 --> 00:41:33.000]   And, you know, the thought that you know what, going from 16k to 44 or 148k. Oh yeah, that's just, that's a soft problem that's easy. No, it's not it's non trivial I mean yes people have have generally have.
[00:41:33.000 --> 00:41:44.000]   So the up sampling stuff is getting quite good now. And there is some hope that you know what maybe we can just take some 16k stuff, and it will fill in the highs there's a lot of hope for that.
[00:41:44.000 --> 00:41:49.000]   But I'm still always until you demonstrate it to me.
[00:41:49.000 --> 00:41:52.000]   I am going to remain skeptical.
[00:41:52.000 --> 00:42:04.000]   And then so much of the audio world in machine learning is you know purely text speech based and so that's another kind of thing where it doesn't always translate to music and until I hear it as music.
[00:42:04.000 --> 00:42:09.000]   I'm going to need to be convinced. I'm not impressed. Yeah.
[00:42:09.000 --> 00:42:19.000]   And so in terms of other people doing great work. So I've always been a fan of vacuums owners I think they're amazing. I think isotope does really great work.
[00:42:19.000 --> 00:42:25.000]   These are not necessarily you know on what stage is something a generative model versus a non generative model.
[00:42:25.000 --> 00:42:41.000]   So, I'm going to pause my thoughts there and turn it back over to Zach. Yeah. Um, so in terms of that was actually one of the reasons I want to get into this was I was pretty aware of the, the, you know, VST and a plugin ecosystem out there and what people
[00:42:41.000 --> 00:42:56.000]   are saying and what the cutting edge stuff is. And there just didn't seem to be like an obvious just game changing groundbreaking AI plugin. There's a lot of things that like they'll say they use machine learning, and it's like you got some classifiers
[00:42:56.000 --> 00:43:09.000]   in there. It's cool. I don't know if you did a whole lot and it's not like big deep learning stuff. It's also cool stuff like neural DSP, which have done very cool things and actually use neural networks and I've done great stuff there.
[00:43:09.000 --> 00:43:25.000]   I was seeing people like, yeah, it seemed to be that that that AI audio seems to be a lot more buzzwords than the, you know, what looks like magic that I was seeing in the image space and okay where's, where's the magic where's the step function.
[00:43:25.000 --> 00:43:36.000]   And I was like, holy crap that's so good. Now this changes everything stuff that's like that I feel like is where AI, we're at the point now where we can have those things.
[00:43:36.000 --> 00:43:49.000]   I think one of the really hard parts about it. It's just that, like from a purely technical standpoint, digital audio processing is done on the CPU, it is serial processing that is not GPU enabled.
[00:43:49.000 --> 00:43:53.000]   You know, I'm not too familiar with GPU audio and they have some plans in that space.
[00:43:53.000 --> 00:44:00.000]   But, you know, the, the software itself is not made for large scale
[00:44:00.000 --> 00:44:07.000]   AI stuff. Now, there are, I guess I don't want to be
[00:44:07.000 --> 00:44:09.000]   erasing things like new tone.
[00:44:09.000 --> 00:44:19.000]   The new VST from Cosmos basically a host for open source audio models made to run in the DAW, you know, they got like rave models in there.
[00:44:19.000 --> 00:44:31.000]   So there isn't really cool stuff in that space, not to not to say it's not possible it takes a lot more optimization and much smaller models to be able to let it run real time low latency in in the software.
[00:44:31.000 --> 00:44:45.000]   And that's just a big blocker for like, yes, you could do these things like dance diffusion is not going to run in a DAW, unless someone does some really cool stuff, but it's it's tricky you know it's not real time software, diffusion in general is hard
[00:44:45.000 --> 00:45:00.000]   to do real time because it takes multiple passes, you have to go through it, you know, 100 times in a giant unit you're not going to do that and the 40 milliseconds you need for processing on a CPU to do you know real time processing but things like
[00:45:00.000 --> 00:45:03.000]   torts you know one shot encoder decoder.
[00:45:03.000 --> 00:45:19.000]   They're much more suitable to that and you can get, you know, real time like torts script stuff running and people are putting a lot of effort into that so it's been really cool to see people have been able to get some of the cutting edge stuff into into the software
[00:45:19.000 --> 00:45:21.000]   on running on CPUs.
[00:45:21.000 --> 00:45:34.000]   But yeah, I think one of the big blockers is just you know, even if someone is making these big models was like we were saying you know in the in the ML world audio is a tiny part and audio isn't speech tiny part and music isn't classical music is a tiny
[00:45:34.000 --> 00:45:45.000]   part of that. And then people that also have those skills and can get something running on a CPU is like five people. And I'm very happy to know like three of them.
[00:45:45.000 --> 00:45:47.000]   Yeah.
[00:45:47.000 --> 00:45:57.000]   You know, in terms of my favorite things honestly I feel like I haven't even been making music for months now because I'm trying to make one song that is every song.
[00:45:57.000 --> 00:46:09.000]   But, yeah, there's the, you know, I've, in terms of like what is out there that producers are using, like, yeah I guess that's part of it is that a lot of the cool stuff isn't popular yet.
[00:46:09.000 --> 00:46:22.000]   A lot of the popular stuff isn't really that cool AI yet, like you know big AI plugin it's like, you've got a lot of, you know, cool plugin maybe but a lot of it's still algorithmic.
[00:46:22.000 --> 00:46:27.000]   And, you know, things like rave are still pretty small.
[00:46:27.000 --> 00:46:42.000]   Is there I'm curious for if we take this upcoming dance diffusion release like if you could on release day like snap your fingers and be able to like ship a project or just a cool use case with this or something you would hope the community will build.
[00:46:42.000 --> 00:46:45.000]   What would that be.
[00:46:45.000 --> 00:47:01.000]   I'm really curious to see what happens with will start working in guidance. I think that there is the whole world of, you know, people taking that was one of the things I saw the most with with disco diffusion and those libraries was just someone working some
[00:47:01.000 --> 00:47:14.000]   other model that I hadn't heard of and be like, oh, you just hook this up and it actually works great. So you know I want to see someone hook up like genre guidance to something you know train some model on more music or even try it on, you know,
[00:47:14.000 --> 00:47:20.000]   I because these guidance things can pull things out of distribution.
[00:47:20.000 --> 00:47:29.000]   You know that that the first clip guided diffusion model was able to make things because clip new but not because the model knew about them.
[00:47:29.000 --> 00:47:39.000]   And if I can give it just like the model train on Jonathan man music, and I can give it to you know a classifier to say now make this pop country.
[00:47:39.000 --> 00:47:52.000]   And it makes him seeing a different like that that'd be a crazy to see all of a sudden you know, oh crap you can already do this of like train this on some artists geography, Poland, other thing and it will change their genre, I think there's a lot of
[00:47:52.000 --> 00:47:56.000]   things that just out of the box, the model itself can do.
[00:47:56.000 --> 00:48:15.000]   But in terms of what people will do I mean, yeah there's like Ethan Manilow's tag box repo he has that was using music tigers to guide jukebox that was more for source separation not necessarily for genre things but something like that was hooking up
[00:48:15.000 --> 00:48:30.000]   using models to it and just kind of seeing what pipelines and combinations you can make speaking of pipelines there was someone who took the jukebox code and because disco diffusion or because dance fusion can kind of act as an up sampler, because you take
[00:48:30.000 --> 00:48:43.000]   the audio noise it and denoise it denoises it into high resolution audio space. They were able to take jukeboxes level two outputs, which are the pre up sample outputs, but up sample it with dance diffusion.
[00:48:43.000 --> 00:48:53.000]   And so instead of taking, you know, 18 hours or whatever of that slow transformer to up sample it, they can hook it up to my model, and make it sound like Jonathan man.
[00:48:53.000 --> 00:48:58.000]   Send me a link to that when you get a chance. Yeah, I'll get you into that in a second.
[00:48:58.000 --> 00:49:10.000]   That was really cool. And that was from the jukebox community so there's a jukebox discord community, you know, run by Brocka Lou and then I remember stumbling upon that couple years back.
[00:49:10.000 --> 00:49:21.000]   No man I went and visited again a few months ago, a few weeks ago until they already had a channel for harmony and they're already they already found dance diffusion, they're already playing with it fine tuning their own models, making the forks of the notebook
[00:49:21.000 --> 00:49:31.000]   and I'm like, I love this community so much. Now that's the incredible thing here it's like I can put in all of this work, and I can just put it out, and it'll just get better.
[00:49:31.000 --> 00:49:40.000]   I could just sit back and watch it get better. I'm going to be working on more things because I love that and I want to, but I'm just, you know, it's, it's really nice to be able to.
[00:49:40.000 --> 00:49:54.000]   And you know this is not a thing I was able to do in a closed company, you know, like Microsoft, I want to make a cool thing. I don't have to figure out, you know, I have to hide it and market it and make a thing around it and product ties it and, you know, make sure
[00:49:54.000 --> 00:50:01.000]   no one else takes it like here it is, here's everything behind it, who wants to make it better, have fun, and people will.
[00:50:01.000 --> 00:50:15.000]   So, yeah, it's, it's, you know, it's a very privileged position to be in to work for stability and to be able to do these things make the open source models and basically just, you know, get paid to try to make cool stuff.
[00:50:15.000 --> 00:50:17.000]   Yeah, yeah, it's amazing.
[00:50:17.000 --> 00:50:20.000]   Time we're living in with all this like open source ML going on.
[00:50:20.000 --> 00:50:22.000]   Yeah, it's a fun ride.
[00:50:22.000 --> 00:50:34.000]   Cool, you've been super generous with your time. But I just have a couple of more quick questions. So first, where can people find Harmonay?
[00:50:34.000 --> 00:50:42.000]   So, right now we are about to launch our open invite public server is called Harmonay Play.
[00:50:42.000 --> 00:50:54.000]   Hopefully, you know that that'll be in the next week or two hopefully when this goes out we'll have the link for everyone to go into there it's a discord server is all our website is Harmonay.org.
[00:50:54.000 --> 00:51:07.000]   There is currently a signup list that will basically get you an invite to the discord server, and soon that will turn into basically just a link to the server so where are we is soon to be the Harmonay Play discord server that is our, our over community
[00:51:07.000 --> 00:51:13.000]   server to discuss dance diffusion and just AI ML stuff in general.
[00:51:13.000 --> 00:51:19.000]   We have a closed R&D server that we've been working with essentially.
[00:51:19.000 --> 00:51:34.000]   And it's, you know, I would love to have that be open to everyone look at that but trying to manage a work server and having the main community server is very tricky and so we want to have some separation of our, our work in play, hence Harmonay Play.
[00:51:34.000 --> 00:51:41.000]   And you know that's still gonna be full of people doing, doing the research and making new notebooks so we'll figure out that split.
[00:51:41.000 --> 00:51:44.000]   That's where you will be able to find us.
[00:51:44.000 --> 00:51:50.000]   Harmonay.org not Harmon.ai that is a different organization.
[00:51:50.000 --> 00:51:55.000]   Yeah, and hopefully that will all be available quite soon.
[00:51:55.000 --> 00:52:02.000]   So, but the Harmonay dash org slash sample dash generator.
[00:52:02.000 --> 00:52:05.000]   That's the, the link.
[00:52:05.000 --> 00:52:12.000]   That is the repo for dance diffusion and then you'll find the inference notebook and the fine tuning notebook.
[00:52:12.000 --> 00:52:13.000]   Brilliant.
[00:52:13.000 --> 00:52:19.000]   One last one last question. What's up, what's coming after dance diffusion Can you drop any.
[00:52:19.000 --> 00:52:22.000]   What else you're working on. I also want to know.
[00:52:22.000 --> 00:52:42.000]   So essentially dance diffusion was me taking code similar to Catherine Crowson's CC 12am diffusion on which we actually powered and the early versions of mid journey was you know, very, very good stuff kind of the step past disco diffusion.
[00:52:42.000 --> 00:52:56.000]   But we're already has been since I've started working on has been a lot of advancements in diffusion, things like the paper from Nvidia the elucidated design space a diffusion models paper, and
[00:52:56.000 --> 00:53:07.000]   this guy we found, or met on solid Twitter, Flavio Schneider made this audio diffusion Piper repo that is like, this is everything that I would have made had I known what I wanted to make.
[00:53:07.000 --> 00:53:20.000]   And so we brought him in we've gotten him, we're working with him and so basically is already a whole new code base of way better stuff to run these things on that we're, you know, still running more tests every day I mean I've got three running right
[00:53:20.000 --> 00:53:21.000]   now.
[00:53:21.000 --> 00:53:24.000]   Always in business.
[00:53:24.000 --> 00:53:28.000]   So yeah, basically, better and more and faster.
[00:53:28.000 --> 00:53:32.000]   You know, the diffusion auto encoders.
[00:53:32.000 --> 00:53:49.000]   Yeah, basically, once we have something that is better and better and faster than I will happily put that out but I'm still got a notebook that I'll probably release as well soon that's based on Zach's auto encoder work where it kind of lets you
[00:53:49.000 --> 00:54:06.000]   play around with the audio embeddings, and then listen to the results of those things. And the main reason I haven't released it yet is it's clunky and not friendly and also I want to have a chance to discover some of the cool stuff myself, but I do
[00:54:06.000 --> 00:54:14.000]   want, I mean Zach's philosophy of let's outsource. A lot of this exploration to the open source community.
[00:54:14.000 --> 00:54:22.000]   You know that's there there's a little bit of tension as an academic I am supposed to put out papers, every now and then.
[00:54:22.000 --> 00:54:35.000]   But, like a harmonized philosophy in under Zach's leadership is great and that we're, we're so open we'll talk to anybody will share with anybody. We have people who are part of companies that are, you know, on our server.
[00:54:35.000 --> 00:54:41.000]   And because we're just so collaborative, and that's really cool.
[00:54:41.000 --> 00:54:54.000]   Yeah, and you know that's that's just that that helps us in a lot of ways obviously we can get help from people who already have full time jobs, and like to say you know I'm much more interested in collaboration than competition.
[00:54:54.000 --> 00:55:04.000]   I think we all we all win. If you know we get the cool stuff out there and being being built upon. Also, you know, go figure take some of this stuff, Scott and write papers on it.
[00:55:04.000 --> 00:55:08.000]   I'm clearly not writing papers.
[00:55:08.000 --> 00:55:25.000]   Yeah, so you know it's, it's nice to be able to get help from people at other research labs and other companies because a lot of people who want to help with this stuff but you know they can't get projects at their at their company approved or they
[00:55:25.000 --> 00:55:39.000]   can't get an ML professionally and they don't want to quit their job but they want to help out with this stuff so you know, having a space where people can come together and work on it, mostly out of passion, with also you know ideally a path to working
[00:55:39.000 --> 00:55:42.000]   with us, we're still working on that.
[00:55:42.000 --> 00:55:56.000]   So it's nice to kind of just like not try to gatekeep the research because that's what happened with me when I first got into this stuff and I first learned about DDSP, I, you know, went wild I need to learn all this stuff I need to know this is clearly
[00:55:56.000 --> 00:56:01.000]   the future like that was saying is clearly the future, like I have to be on top of this is the wave is coming.
[00:56:01.000 --> 00:56:13.000]   And I was looking around and I'm like where is the community for this is actually part of why I started Harmony was looking around alright so I had in these music music music communities I know those producer communities you get in there and you can, you know, watch
[00:56:13.000 --> 00:56:26.000]   someone making music on Ableton and voice chat and ask questions and learn about things from them and have these, these challenges, where is that for music ML, and there wasn't one, you know, there was Valeria Velardo sound of AI which was cool but more focused on things
[00:56:26.000 --> 00:56:41.000]   like classifiers and, you know, more traditional music ML stuff. And then it was like Google Magenta, or, you know, Queen Mary University in London, and I'm like I'm not going to go work for Google, and I'm not going to move to London, so I guess it's just
[00:56:41.000 --> 00:56:52.000]   not for me. I guess I have to just not do this. And, you know, that was basically beginning of 2021, I had that realization like I want to do this but there's just nowhere I can do it.
[00:56:52.000 --> 00:57:05.000]   There's nowhere for me to go take part in this just as an individual researcher, and then that all changed with the notebook culture with with collab with with things like this good infusion and I'm like, alright, this is, this is chance number two.
[00:57:05.000 --> 00:57:19.000]   And while messing around with disco diffusion and being in the server there, I was talking with Gandamu who one of the other dozen is good infusion. And he was like hey you're doing some like practical research you should go talk with Ahmad he is like giving
[00:57:19.000 --> 00:57:36.000]   you like a 100 computers for doing open source research and I was like research. I'm having fun. What? Oh yeah I guess technically this is research huh? So I, you know, happened upon him in some other conversation and ended up getting in contact with him.
[00:57:36.000 --> 00:57:48.000]   And basically said you know I want to do this stuff full time. I'm working at Microsoft, can you compete with their salaries and he was like yep DM me and I'm like, all right.
[00:57:48.000 --> 00:58:02.000]   So that's kind of when it came to like, you know, looking around again at that point I saw a Luther AI had a large language model stuff with Lion with the big data sets there was the image stuff kind of between those two and like all right cool where is the audio
[00:58:02.000 --> 00:58:10.000]   thing, and all I could find with one channel in the Lion server for audio data sets, and I'm like, that's it.
[00:58:10.000 --> 00:58:21.000]   I'm out of like 30 channels in like multiple servers, there's one channel for audio. So it was like well this. It was pretty obvious at that point I had to make maybe make community for it.
[00:58:21.000 --> 00:58:33.000]   I've been a discord community manager, know how to do that. I know producers, I'm in the space now. It was just kind of like, I need this to exist, and no one else was making it so I guess it's me.
[00:58:33.000 --> 00:58:35.000]   I guess I'll do it.
[00:58:35.000 --> 00:58:46.000]   I'll do it for a second too as well so later on like I got on the service. Well, I knew about a Luther as well of course it didn't really fit in there with Lion, I was like, hey, these are super computing people.
[00:58:46.000 --> 00:58:51.000]   I'm a super computing person like I speak this language, this is great. What.
[00:58:51.000 --> 00:59:02.000]   Oh, but they're not doing audio that I'm interested in, where are people who are doing like real music, and right right. That was another cool thing with Zach and I finding each other.
[00:59:02.000 --> 00:59:16.000]   Yep, so it just kind of helps that like there was there was just a need for it there was just a vacuum in the open source AI space of having a place that had both the people and the, you know, the passion and most importantly, the compute.
[00:59:16.000 --> 00:59:27.000]   You know, like, like Scott was saying when I first made harmony that was the same week as there being a bunch of other discord servers starting up and I was like okay if I'm making a new server, yet another AI our discord server.
[00:59:27.000 --> 00:59:41.000]   Why do I need to have a reason to be in my place and not the other ones that were all more open invite or already had known researchers or, you know, am I just making the sound of AI to what makes this different and I'm like compute cluster.
[00:59:41.000 --> 00:59:49.000]   It's a very very good value proposition to people to be like here's this server, and we've got a 100.
[00:59:49.000 --> 00:59:57.000]   And so I think that's helped early on and people excited and interested in the community was being like, you can actually act on your things you want to do here we won't stop you.
[00:59:57.000 --> 01:00:09.000]   And we will have, you know, our own standards of who gets onto the cluster but you know in terms of like, I want to make music with all that's not I can't, I can't sell that to Alexa for $10 million.
[01:00:09.000 --> 01:00:19.000]   I know this happens just like that. But, you know, a lot of excited researchers and it's nice to have a space to be like yes, I agree. That's cool. Come help us.
[01:00:19.000 --> 01:00:33.000]   I love that. That's amazing. And I love all of the energy you have with the two I can totally see like on stage like playing a stadium in some way, some form, someday, playing some audio music maybe.
[01:00:33.000 --> 01:00:48.000]   Well, you've been super generous with your time I think we're like way over what we planned. So we can probably wrap it there but um yeah thanks again for taking the time and really looking forward to like working with you guys and yeah, listen to some like great beats.
[01:00:48.000 --> 01:00:50.000]   For sure. Thanks for thanks for having us.
[01:00:50.000 --> 01:01:00.000]   [BLANK_AUDIO]

