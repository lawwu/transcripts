
[00:00:00.000 --> 00:00:02.160]   this line between a visionary and a fraudster.
[00:00:02.160 --> 00:00:04.960]   You know, you think that they're on two opposite ends of the spectrum,
[00:00:04.960 --> 00:00:07.720]   but in reality, they're where the ends of the circle meet.
[00:00:07.720 --> 00:00:10.260]   If somebody has the ability to put a vision forward,
[00:00:10.260 --> 00:00:12.180]   like Jeff Skelling did at Enron,
[00:00:12.180 --> 00:00:14.020]   like Elizabeth Holmes did at Theranos,
[00:00:14.020 --> 00:00:15.600]   like SBF did,
[00:00:15.600 --> 00:00:17.760]   then you're particularly susceptible.
[00:00:17.760 --> 00:00:21.360]   Self-delusion is such a strong component
[00:00:21.360 --> 00:00:24.240]   of all of these stories of business gone wrong.
[00:00:24.240 --> 00:00:26.240]   The line between what happened at Enron
[00:00:26.240 --> 00:00:29.080]   and what happened in the global financial crisis,
[00:00:29.080 --> 00:00:31.000]   it's not a matter of black and white.
[00:00:31.000 --> 00:00:33.960]   Well, do you believe in the efficient market hypothesis?
[00:00:33.960 --> 00:00:37.000]   The rapid implosion of a company worth
[00:00:37.000 --> 00:00:39.340]   tens of billions of dollars,
[00:00:39.340 --> 00:00:42.300]   insider dealing and romantic entanglements
[00:00:42.300 --> 00:00:43.960]   between sister companies,
[00:00:43.960 --> 00:00:46.380]   a politically generous CEO
[00:00:46.380 --> 00:00:48.640]   who is well-connected in Washington,
[00:00:48.640 --> 00:00:52.560]   the use of a company's own stock as its collateral,
[00:00:52.560 --> 00:00:54.400]   the attempt, the short-lived attempt
[00:00:54.400 --> 00:00:56.640]   to get bought out by a previous competitor,
[00:00:56.640 --> 00:01:00.520]   and the fraudulent abuse of mark-to-market accounting.
[00:01:00.520 --> 00:01:02.280]   We are not talking about FTX.
[00:01:02.280 --> 00:01:04.360]   We are talking about Enron,
[00:01:04.360 --> 00:01:06.620]   which my guest today, Bethany McLean,
[00:01:06.620 --> 00:01:08.280]   first broke the story of
[00:01:08.280 --> 00:01:12.000]   and has written an amazing and detailed book about
[00:01:12.000 --> 00:01:13.920]   called "The Smartest Guys in the Room."
[00:01:13.920 --> 00:01:16.880]   And she has also written a book about the housing crisis,
[00:01:16.880 --> 00:01:18.300]   "All the Devils Are Here,"
[00:01:18.300 --> 00:01:21.040]   a book about Fannie and Freddie, "Shaky Ground,"
[00:01:21.040 --> 00:01:23.540]   and a book about fracking, "Saudi America,"
[00:01:23.540 --> 00:01:24.760]   all of which we'll get into.
[00:01:24.760 --> 00:01:25.920]   She is, in my opinion,
[00:01:25.920 --> 00:01:28.460]   the best finance nonfiction writer out there,
[00:01:28.460 --> 00:01:29.540]   and I'm really, really excited
[00:01:29.540 --> 00:01:30.580]   to have this conversation now.
[00:01:30.580 --> 00:01:33.680]   So, Bethany, thank you so much for coming on the podcast.
[00:01:33.680 --> 00:01:37.880]   - Thank you so much for the probably undeserved compliment
[00:01:37.880 --> 00:01:39.180]   for having me on the show.
[00:01:39.180 --> 00:01:41.480]   - Well, my first question,
[00:01:41.480 --> 00:01:43.820]   what are the odds that SBF read
[00:01:43.820 --> 00:01:45.020]   "The Smartest Guys in the Room"
[00:01:45.020 --> 00:01:47.060]   and just followed it as a playbook,
[00:01:47.060 --> 00:01:48.820]   given the similarities there?
[00:01:48.820 --> 00:01:50.860]   - You know, I love that idea.
[00:01:50.860 --> 00:01:53.440]   I have to admit, I guess I love that idea.
[00:01:53.440 --> 00:01:54.960]   I don't know, that would make me responsible
[00:01:54.960 --> 00:01:56.240]   for what would happen.
[00:01:56.240 --> 00:01:58.160]   So, maybe I don't work that idea.
[00:01:58.160 --> 00:01:59.960]   Let me take that back.
[00:01:59.960 --> 00:02:04.960]   Anyway, I actually think that even if he had read the book,
[00:02:04.960 --> 00:02:06.840]   it would never have occurred to him
[00:02:06.840 --> 00:02:08.760]   that there was a similarity,
[00:02:08.760 --> 00:02:12.840]   because self-delusion is such a strong component
[00:02:12.840 --> 00:02:15.800]   of all of these stories of business gone wrong.
[00:02:15.800 --> 00:02:19.140]   It's very rare that you have one of the characters
[00:02:19.140 --> 00:02:19.980]   at the heart of this
[00:02:19.980 --> 00:02:22.020]   who actually understands what they're doing
[00:02:22.020 --> 00:02:24.760]   and understands that they're moving over into the dark side
[00:02:24.760 --> 00:02:27.520]   and thinks about the potential repercussions of this
[00:02:27.520 --> 00:02:29.480]   and chooses this path anyway.
[00:02:29.480 --> 00:02:32.060]   That's usually not the way these stories go.
[00:02:32.060 --> 00:02:35.280]   So, it's entirely possible that SBF studied Enron,
[00:02:35.280 --> 00:02:36.600]   knew all about it,
[00:02:36.600 --> 00:02:40.240]   and never envisioned that there were any similarities
[00:02:40.240 --> 00:02:42.780]   between that and what he was doing.
[00:02:42.780 --> 00:02:44.900]   - Oh, that's so fascinating,
[00:02:44.900 --> 00:02:47.080]   which I guess raises the question of
[00:02:47.080 --> 00:02:49.040]   what are we doing when we're documenting
[00:02:49.040 --> 00:02:51.960]   and trying to learn from books like yours?
[00:02:51.960 --> 00:02:54.480]   If somebody who is about to commit
[00:02:54.480 --> 00:02:56.520]   the same exact kind of thing can read that book
[00:02:56.520 --> 00:02:58.840]   and not realize that he's doing the same exact thing,
[00:02:58.840 --> 00:03:00.280]   is there something that just prevents us
[00:03:00.280 --> 00:03:01.640]   from learning the lessons of history
[00:03:01.640 --> 00:03:04.300]   that we can never just get the analogy right
[00:03:04.300 --> 00:03:07.120]   and we're just guided by our own delusions?
[00:03:07.120 --> 00:03:09.500]   - Wasn't there a great quote that history rinds
[00:03:09.500 --> 00:03:10.520]   but it doesn't reteat?
[00:03:10.520 --> 00:03:12.960]   I'm blinding on who it is who said that,
[00:03:12.960 --> 00:03:15.760]   but I think that's absolutely true.
[00:03:15.760 --> 00:03:17.840]   I think it's important for all of us,
[00:03:17.840 --> 00:03:19.760]   those of us who are not gonna find ourselves
[00:03:19.760 --> 00:03:22.320]   at the center of that giant fraud, or so I hope.
[00:03:22.320 --> 00:03:24.600]   I think my time for that has passed, maybe not you,
[00:03:24.600 --> 00:03:27.160]   but I think it's important for all of us
[00:03:27.160 --> 00:03:28.760]   to understand what went wrong.
[00:03:28.760 --> 00:03:30.840]   And I do think these,
[00:03:30.840 --> 00:03:33.640]   I do think just there's a great value
[00:03:33.640 --> 00:03:35.320]   and greater understanding of the world
[00:03:35.320 --> 00:03:38.920]   without necessarily a practical payoff for it.
[00:03:38.920 --> 00:03:41.280]   So I think when something goes wrong
[00:03:41.280 --> 00:03:42.880]   on a massive societal level,
[00:03:42.880 --> 00:03:45.920]   it's really important to try to explain it.
[00:03:45.920 --> 00:03:49.720]   Human beings have needed narrative since the dawn of time
[00:03:49.720 --> 00:03:52.840]   and we need narrative all the more now,
[00:03:52.840 --> 00:03:55.000]   we need to make sense of the world.
[00:03:55.000 --> 00:03:57.600]   So I like to believe that that process of making,
[00:03:57.600 --> 00:03:59.920]   trying to make sense of the world
[00:03:59.920 --> 00:04:02.520]   has a value in and of itself.
[00:04:02.520 --> 00:04:06.440]   Maybe there is some small deterrence aspect to it
[00:04:06.440 --> 00:04:09.760]   in that I often think that if people understand more
[00:04:09.760 --> 00:04:12.200]   the process by which things go wrong,
[00:04:12.200 --> 00:04:13.640]   that it isn't deliberate,
[00:04:13.640 --> 00:04:16.320]   that it's not bad people setting out to do bad things,
[00:04:16.400 --> 00:04:20.080]   it's human beings at first convincing themselves
[00:04:20.080 --> 00:04:22.120]   even that they're doing the right thing
[00:04:22.120 --> 00:04:24.280]   and then ending up in a situation
[00:04:24.280 --> 00:04:26.160]   that they never meant to be in.
[00:04:26.160 --> 00:04:29.520]   And maybe on the margin that does,
[00:04:29.520 --> 00:04:32.120]   maybe on the margin that does help
[00:04:32.120 --> 00:04:34.560]   because maybe it has deterred some people
[00:04:34.560 --> 00:04:36.720]   who would have started down that path,
[00:04:36.720 --> 00:04:40.400]   but for the fact that they now see that that's the usual.
[00:04:40.400 --> 00:04:42.240]   - Yeah, yeah.
[00:04:42.240 --> 00:04:44.240]   That actually raises the next question I wanted to ask you.
[00:04:44.240 --> 00:04:46.560]   Bert Hobart, he's a finance writer as well.
[00:04:46.560 --> 00:04:48.960]   He wrote a blog post about,
[00:04:48.960 --> 00:04:50.920]   I mean, this was before FTX obviously,
[00:04:50.920 --> 00:04:52.800]   and he was talking about Enron and he said,
[00:04:52.800 --> 00:04:54.880]   "In the end, it actually looks
[00:04:54.880 --> 00:04:58.080]   "like we fixed the precise problem Enron represented.
[00:04:58.080 --> 00:05:00.800]   "Nobody I know solely looks at gap financials.
[00:05:00.800 --> 00:05:03.800]   "Everybody ultimately models based on free cashflow.
[00:05:03.800 --> 00:05:05.200]   "We're much more averse to companies
[00:05:05.200 --> 00:05:07.380]   "that set up a deliberate conflict of interest
[00:05:07.380 --> 00:05:09.840]   "between management and shareholders."
[00:05:09.840 --> 00:05:11.800]   And I guess there's a way in which you can read that
[00:05:11.800 --> 00:05:14.240]   and say, "Oh, doesn't FTX prove him wrong?"
[00:05:14.240 --> 00:05:15.600]   But there's another way you can look at it
[00:05:15.600 --> 00:05:18.760]   is that FTX deliberately set up outside the US.
[00:05:18.760 --> 00:05:19.800]   So there's a story to be told
[00:05:19.800 --> 00:05:21.720]   that actually we learned the lessons of Enron
[00:05:21.720 --> 00:05:24.240]   and Serbane Doxley worked.
[00:05:24.240 --> 00:05:26.480]   That's why they were in the Bahamas
[00:05:26.480 --> 00:05:29.080]   and we haven't seen a fraud of that scale
[00:05:29.080 --> 00:05:31.000]   in the continental United States.
[00:05:31.000 --> 00:05:34.520]   Do you think that the FTX saga
[00:05:34.520 --> 00:05:36.600]   and I guess the absence of other frauds
[00:05:36.600 --> 00:05:39.460]   of that scale in America shows that the regulations
[00:05:39.460 --> 00:05:41.320]   and this changed business and investment practices
[00:05:41.320 --> 00:05:44.200]   in the aftermath of Enron have actually worked?
[00:05:44.200 --> 00:05:49.080]   - Well, I think they probably worked and narrowly writ
[00:05:49.080 --> 00:05:53.200]   and the way in which the writer you quoted articulated.
[00:05:53.200 --> 00:05:56.480]   I think it would be very hard for the CFO
[00:05:56.480 --> 00:05:58.280]   of a publicly traded company
[00:05:58.280 --> 00:06:01.520]   who set up other private equity firms that he ran
[00:06:01.520 --> 00:06:03.120]   that did all their business with his company
[00:06:03.120 --> 00:06:05.360]   because everybody would say, "That's Enron."
[00:06:05.360 --> 00:06:07.960]   And it would be completely on the nose.
[00:06:07.960 --> 00:06:12.960]   And Serbanes Doxley in the sense of helping to reign
[00:06:12.960 --> 00:06:16.520]   in corporate fraud of the sort that was practiced
[00:06:16.520 --> 00:06:18.520]   by Enron, which was this abuse
[00:06:18.520 --> 00:06:23.200]   of very specific accounting rules, I think that works.
[00:06:23.200 --> 00:06:26.960]   But you say there hasn't been fraud on a scale like Enron
[00:06:26.960 --> 00:06:28.840]   up until perhaps FTX,
[00:06:28.840 --> 00:06:31.240]   but you're forgetting the global financial crisis.
[00:06:31.240 --> 00:06:34.080]   And in the end, the line between what happened at Enron
[00:06:34.080 --> 00:06:36.920]   and what happened in the global financial crisis,
[00:06:36.920 --> 00:06:38.800]   it's not a matter of black and white.
[00:06:38.800 --> 00:06:41.080]   It's not a matter of one thing was clear cut fraud
[00:06:41.080 --> 00:06:43.720]   and one thing, great, we love these practices.
[00:06:43.720 --> 00:06:44.800]   Isn't this fantastic?
[00:06:44.800 --> 00:06:46.880]   This is the way we want business to operate.
[00:06:46.880 --> 00:06:49.280]   They're both somewhere in the murky middle.
[00:06:49.280 --> 00:06:50.960]   You know, a lot of what happened at Enron
[00:06:50.960 --> 00:06:52.520]   wasn't actually outright fraud.
[00:06:52.520 --> 00:06:54.320]   I've coined this phrase legal fraud
[00:06:54.320 --> 00:06:59.280]   to describe what it is that happened at Enron.
[00:06:59.280 --> 00:07:00.320]   And a lot of what happened
[00:07:00.320 --> 00:07:02.760]   in the global financial crisis was legal,
[00:07:02.760 --> 00:07:04.560]   hence the lack of prosecutions.
[00:07:04.560 --> 00:07:09.560]   But it's also not behavior that leads to a healthy market
[00:07:09.560 --> 00:07:13.560]   or for that matter, a healthy society.
[00:07:13.560 --> 00:07:16.400]   And so there's a reason that you had Sarbanes-Oxley
[00:07:16.400 --> 00:07:20.320]   and what was it eight short years later, you had Dodd-Frank.
[00:07:20.320 --> 00:07:23.560]   And so read broadly,
[00:07:23.560 --> 00:07:26.200]   I'm not sure Sarbanes-Oxley did that much good.
[00:07:26.200 --> 00:07:28.880]   And what I mean by that is when President George Bush
[00:07:28.880 --> 00:07:31.760]   signed it into law in the Rose Garden,
[00:07:31.760 --> 00:07:35.760]   he gave this speech about how investors were now protected
[00:07:35.760 --> 00:07:37.480]   and everything was great.
[00:07:37.480 --> 00:07:41.560]   And ordinary investors could take comfort
[00:07:41.560 --> 00:07:44.320]   that the laws were meant to protect them from wrongdoing.
[00:07:44.320 --> 00:07:45.800]   And you compare that to the speech
[00:07:45.800 --> 00:07:48.360]   that President Barack Obama gave eight years later
[00:07:48.360 --> 00:07:50.640]   when he signed Dodd-Frank into law in the Rose Garden.
[00:07:50.640 --> 00:07:52.640]   And it's remarkably similar
[00:07:52.640 --> 00:07:55.160]   that now ordinary investors can count on the rules
[00:07:55.160 --> 00:07:57.040]   and regulations keeping themselves
[00:07:57.040 --> 00:08:00.000]   from people who are preying on their financial wellbeing.
[00:08:00.000 --> 00:08:02.880]   And I don't think it's true in either case
[00:08:02.880 --> 00:08:05.520]   because our markets, particularly modern markets
[00:08:05.520 --> 00:08:07.520]   move and evolve so quickly.
[00:08:07.520 --> 00:08:10.000]   The thing that's coming out of last seal to get you
[00:08:10.000 --> 00:08:12.800]   is never gonna be the thing you are protecting against.
[00:08:12.800 --> 00:08:15.640]   - But given the fact that Enron, as you say,
[00:08:15.640 --> 00:08:16.880]   was committing illegal fraud,
[00:08:16.880 --> 00:08:18.800]   is it possible that the government,
[00:08:18.800 --> 00:08:22.640]   when they prosecuted Skilling and Fastow and Lay,
[00:08:22.640 --> 00:08:25.080]   they in fact were not,
[00:08:25.080 --> 00:08:27.160]   they prosecuted them to a greater extent
[00:08:27.160 --> 00:08:30.880]   than the law as written at the time would have warranted.
[00:08:30.880 --> 00:08:34.320]   In other words, was there something legally invalid
[00:08:34.320 --> 00:08:37.840]   in the quantity of sentence that they got?
[00:08:37.840 --> 00:08:39.400]   Is that possible?
[00:08:39.400 --> 00:08:40.720]   - I get what you're asking.
[00:08:40.720 --> 00:08:42.840]   I think it's a really tricky question
[00:08:42.840 --> 00:08:45.840]   because I think in salute terms,
[00:08:45.840 --> 00:08:48.600]   Enron needed to be prosecuted
[00:08:48.600 --> 00:08:51.000]   and needed to be prosecuted aggressively.
[00:08:51.000 --> 00:08:53.440]   And while I say it was legal fraud,
[00:08:53.440 --> 00:08:54.920]   that is for the most part,
[00:08:54.920 --> 00:08:57.960]   there was actually real fraud around,
[00:08:57.960 --> 00:08:59.320]   but it's on the margin.
[00:08:59.320 --> 00:09:03.400]   It doesn't explain the entirety of Enron's collapse.
[00:09:03.400 --> 00:09:05.200]   Much of what they did was using
[00:09:05.200 --> 00:09:06.840]   and abusing the accounting rules
[00:09:06.840 --> 00:09:10.640]   in order to create an appearance of economic reality
[00:09:10.640 --> 00:09:14.200]   that had nothing to do with actual reality.
[00:09:14.200 --> 00:09:16.720]   But then there was actual fraud
[00:09:16.720 --> 00:09:19.520]   in the sense that Andy Fastow was stealing money
[00:09:19.520 --> 00:09:21.760]   from these partnerships to benefit himself.
[00:09:21.760 --> 00:09:24.360]   And they were, if you believe the core tenant
[00:09:24.360 --> 00:09:25.280]   of the prosecution,
[00:09:25.280 --> 00:09:27.920]   which was this agreement called Global Galactic
[00:09:27.920 --> 00:09:29.320]   that was signed by,
[00:09:29.320 --> 00:09:31.480]   that was between Andy Fastow and Jeff Skilling,
[00:09:31.480 --> 00:09:33.920]   where Jeff agreed that Andy's partnerships
[00:09:33.920 --> 00:09:35.000]   would never lose money,
[00:09:35.000 --> 00:09:38.000]   then that invalidated all of the accounting.
[00:09:38.000 --> 00:09:43.000]   And that's the chief reason that Skilling was convicted,
[00:09:43.000 --> 00:09:45.880]   was that the jury believed the existence
[00:09:45.880 --> 00:09:48.000]   of this agreement,
[00:09:48.000 --> 00:09:50.600]   that and one set of insider stock sales,
[00:09:50.600 --> 00:09:51.800]   which we can talk about,
[00:09:51.800 --> 00:09:53.840]   which was also a really key moment.
[00:09:53.840 --> 00:09:56.840]   Relative to the, so in absolute terms,
[00:09:56.840 --> 00:10:00.720]   I don't know, it's hard for me to say there was such,
[00:10:00.720 --> 00:10:03.960]   Enron was such, to a degree that is still surprising to me,
[00:10:03.960 --> 00:10:08.160]   such a watershed moment in our country,
[00:10:08.160 --> 00:10:10.680]   far beyond business itself.
[00:10:10.680 --> 00:10:14.720]   But it caused so much insecurity
[00:10:14.720 --> 00:10:19.000]   that about our retirement assets safe,
[00:10:19.000 --> 00:10:20.960]   can you trust the company where you work?
[00:10:20.960 --> 00:10:22.400]   That I think the government
[00:10:22.400 --> 00:10:25.000]   did have to prosecute aggressively,
[00:10:25.000 --> 00:10:27.240]   but relative to the financial crisis,
[00:10:27.240 --> 00:10:29.840]   where a lot of people made off with a lot of money
[00:10:29.840 --> 00:10:31.440]   and never had to give any of it back,
[00:10:31.440 --> 00:10:34.800]   doesn't seem fair that Jeff Skilling went to jail
[00:10:34.800 --> 00:10:37.800]   for over a decade and no one involved in a major way
[00:10:37.800 --> 00:10:41.080]   in the financial crisis paid any price whatsoever.
[00:10:41.080 --> 00:10:42.520]   People didn't even really have to give up
[00:10:42.520 --> 00:10:44.160]   that much of the money they made.
[00:10:44.160 --> 00:10:46.560]   Then it seems a little bit unfair, yes.
[00:10:46.560 --> 00:10:50.040]   So I think it's an absolute versus a relative question.
[00:10:50.040 --> 00:10:51.120]   - Yeah, yeah.
[00:10:51.120 --> 00:10:53.560]   By the way, who do you think made more money?
[00:10:53.560 --> 00:10:57.680]   The investment banks like Goldman Sachs and Morgan Stanley
[00:10:57.680 --> 00:11:01.680]   from providing their services to Enron
[00:11:01.680 --> 00:11:03.280]   as the stock was going up,
[00:11:03.280 --> 00:11:05.200]   or Jim Chanos from shorting the stock?
[00:11:05.200 --> 00:11:07.120]   In absolute terms, who made more money?
[00:11:07.120 --> 00:11:09.680]   - Oh, I think the investment banks, for sure.
[00:11:09.680 --> 00:11:11.760]   I mean, they made so much money
[00:11:11.760 --> 00:11:14.640]   in investment banking fees from Enron,
[00:11:14.640 --> 00:11:16.440]   but you know, it's a good question.
[00:11:16.440 --> 00:11:18.440]   It's a good question actually,
[00:11:18.440 --> 00:11:20.840]   because I think Jim made a lot of money too, so.
[00:11:20.840 --> 00:11:23.320]   - Yeah, yeah.
[00:11:23.320 --> 00:11:25.840]   I mean, you've spoken about, I guess,
[00:11:25.840 --> 00:11:29.600]   the usefulness and the shortage of short sellers
[00:11:29.600 --> 00:11:33.160]   as a sort of corrective on irrational exuberance.
[00:11:33.160 --> 00:11:35.720]   And I'm curious why you think that shortage exists
[00:11:35.720 --> 00:11:36.560]   in the first place.
[00:11:36.560 --> 00:11:38.640]   Like if you believe in the efficient market hypothesis,
[00:11:38.640 --> 00:11:39.800]   you should think that, you know,
[00:11:39.800 --> 00:11:41.800]   if some company has terrible financials
[00:11:41.800 --> 00:11:43.440]   and implausible numbers,
[00:11:43.440 --> 00:11:45.160]   then people would be lining up to short it
[00:11:45.160 --> 00:11:47.920]   and then you would never have a phenomenon like Enron.
[00:11:47.920 --> 00:11:52.800]   And so it's so odd that you can have, you know,
[00:11:52.800 --> 00:11:55.720]   reporters who are basically ahead of the market
[00:11:55.720 --> 00:11:57.920]   in terms of predicting what's gonna happen.
[00:11:57.920 --> 00:11:59.680]   How do you square that
[00:11:59.680 --> 00:12:01.000]   with like the efficient market hypothesis?
[00:12:01.000 --> 00:12:01.840]   - Well, do you believe
[00:12:01.840 --> 00:12:03.960]   in the efficient market hypothesis?
[00:12:03.960 --> 00:12:06.760]   - I'd like to, but I'm like trying to wrap my head
[00:12:06.760 --> 00:12:07.600]   around Enron.
[00:12:07.600 --> 00:12:10.920]   - I'm not sure how you can,
[00:12:10.920 --> 00:12:14.320]   unless you adopt Warren Buffett's point of view,
[00:12:14.320 --> 00:12:15.560]   and I'm gonna mangle the quote,
[00:12:15.560 --> 00:12:18.760]   because, but it's the market in the short term
[00:12:18.760 --> 00:12:19.720]   is a voting machine,
[00:12:19.720 --> 00:12:22.320]   in the long term, it's a weighing machine, right?
[00:12:22.320 --> 00:12:23.680]   The other way around.
[00:12:23.680 --> 00:12:26.480]   Anyway, the idea is that the market may be dairy efficient
[00:12:26.480 --> 00:12:29.200]   for a long, very inefficient for a long period of time,
[00:12:29.200 --> 00:12:30.800]   but it does actually,
[00:12:30.800 --> 00:12:34.040]   rationality does actually work in the end.
[00:12:34.040 --> 00:12:35.880]   And I think I might believe that.
[00:12:35.880 --> 00:12:37.840]   But as John Maynard Keynes who said,
[00:12:37.840 --> 00:12:40.000]   "The market can remain irrational for a lot longer
[00:12:40.000 --> 00:12:41.640]   than you can remain solvent."
[00:12:41.640 --> 00:12:43.560]   And so I think that's true too.
[00:12:43.560 --> 00:12:46.120]   I think believing that the market is efficient
[00:12:46.120 --> 00:12:49.360]   and rational in the short term is just obviously wrong.
[00:12:49.360 --> 00:12:53.080]   But back to your question about short sellers,
[00:12:53.080 --> 00:12:55.120]   which is interesting.
[00:12:55.120 --> 00:12:58.280]   I think part of it is that there is still this,
[00:12:58.280 --> 00:13:00.640]   there certainly was a couple of decades ago,
[00:13:00.640 --> 00:13:01.920]   and I think it still exists,
[00:13:01.920 --> 00:13:06.240]   this idea that owning stocks is mom American
[00:13:06.240 --> 00:13:11.120]   and apple pie and short stock somehow is bad and evil
[00:13:11.120 --> 00:13:13.640]   and rooting against America.
[00:13:13.640 --> 00:13:16.160]   And I remember going back to the Enron days,
[00:13:16.160 --> 00:13:18.080]   someone, people criticizing me,
[00:13:18.080 --> 00:13:19.520]   even other people in the press saying,
[00:13:19.520 --> 00:13:22.280]   "But you took a tip from a short seller, they're biased."
[00:13:22.280 --> 00:13:24.400]   And I would say, "But wait,
[00:13:24.400 --> 00:13:26.360]   the analysts who have buy ratings on stocks
[00:13:26.360 --> 00:13:27.680]   and the portfolio managers
[00:13:27.680 --> 00:13:29.760]   who own those stocks are biased too."
[00:13:29.760 --> 00:13:32.520]   They want stocks to go up, everybody's biased.
[00:13:32.520 --> 00:13:35.600]   So the trick as a journalist is getting information
[00:13:35.600 --> 00:13:38.560]   from all sides and figuring out who you think is right
[00:13:38.560 --> 00:13:39.480]   and what makes sense,
[00:13:39.480 --> 00:13:42.120]   but it's not avoiding anybody with any bias.
[00:13:42.120 --> 00:13:44.760]   But it was really interesting that people saw the bias
[00:13:44.760 --> 00:13:45.960]   on the part of short sellers
[00:13:45.960 --> 00:13:48.560]   and did not see it on the part of longs.
[00:13:48.560 --> 00:13:51.840]   And I think there is that preconception that exists broadly
[00:13:51.840 --> 00:13:54.120]   that somehow you are doing something wrong
[00:13:54.120 --> 00:13:56.360]   and you're somehow rooting for a company's failure
[00:13:56.360 --> 00:13:58.800]   and that this is, I don't know, anti-American
[00:13:58.800 --> 00:14:00.720]   if you short a stock.
[00:14:00.720 --> 00:14:02.200]   And so I think that's part of why
[00:14:02.200 --> 00:14:07.200]   there's a shortage of short sellers.
[00:14:07.240 --> 00:14:10.280]   - I think also, I mean, we've had a incredible,
[00:14:10.280 --> 00:14:13.120]   unprecedented bull market for the last four decades
[00:14:13.120 --> 00:14:15.600]   as a result of falling interest rates.
[00:14:15.600 --> 00:14:19.080]   And especially in the decade before the pandemic hit,
[00:14:19.080 --> 00:14:22.320]   it was very, very difficult to make money shorting anything
[00:14:22.320 --> 00:14:24.360]   because everything went to the moon.
[00:14:24.360 --> 00:14:26.120]   Didn't matter if its numbers were good,
[00:14:26.120 --> 00:14:30.560]   if it was eventually unmasked to be somewhat fraudulent.
[00:14:30.560 --> 00:14:32.680]   Stocks just went to the moon anyway,
[00:14:32.680 --> 00:14:33.800]   the riskier the better.
[00:14:33.920 --> 00:14:37.520]   And so it is only diehard short sellers
[00:14:37.520 --> 00:14:39.840]   that have managed to stick it out.
[00:14:39.840 --> 00:14:43.000]   And I think lastly, Jim Chano said this to me once,
[00:14:43.000 --> 00:14:46.600]   and I think it's true that he could find dozens of people
[00:14:46.600 --> 00:14:48.080]   who were skilled enough to come,
[00:14:48.080 --> 00:14:49.560]   smart enough to come work for him.
[00:14:49.560 --> 00:14:50.840]   There's no shortage of that.
[00:14:50.840 --> 00:14:53.640]   People who are technically skilled and really smart,
[00:14:53.640 --> 00:14:56.640]   but being able to be contraried for a long period of time,
[00:14:56.640 --> 00:14:59.040]   especially when the market is going against you
[00:14:59.040 --> 00:15:00.840]   is a different sort of person,
[00:15:00.840 --> 00:15:03.840]   but it requires a completely different mindset
[00:15:03.840 --> 00:15:05.880]   to have everybody in the world saying you're wrong,
[00:15:05.880 --> 00:15:09.040]   to be losing money because the stock is continuing to go up
[00:15:09.040 --> 00:15:11.920]   and to be able to hold fast to your conviction.
[00:15:11.920 --> 00:15:15.480]   And I think that's another part of the explanation
[00:15:15.480 --> 00:15:18.120]   for why there are fewer short sellers.
[00:15:18.120 --> 00:15:20.000]   - Yeah, and that raises an interesting question
[00:15:20.000 --> 00:15:22.240]   about venture capital, for example,
[00:15:22.240 --> 00:15:24.640]   where, or private markets in general,
[00:15:24.640 --> 00:15:25.800]   at least in the public markets,
[00:15:25.800 --> 00:15:28.240]   there's the shorting maybe in shortage,
[00:15:28.240 --> 00:15:30.480]   but it is a possible mechanism.
[00:15:30.480 --> 00:15:32.320]   Whereas I'm a programmer,
[00:15:32.320 --> 00:15:36.960]   so if one guy thinks the company is worth $100 million
[00:15:36.960 --> 00:15:39.280]   and everybody else thinks it's not,
[00:15:39.280 --> 00:15:40.440]   the company will still be,
[00:15:40.440 --> 00:15:44.440]   the price will still be set by the person who is a believer.
[00:15:44.440 --> 00:15:47.040]   Does that increase the risk of some sort of bubble
[00:15:47.040 --> 00:15:49.560]   in venture capital and in technology?
[00:15:49.560 --> 00:15:50.880]   And I guess in private markets generally,
[00:15:50.880 --> 00:15:53.160]   if they're not public, is that something you worry about,
[00:15:53.160 --> 00:15:55.640]   that there could be incredible bubbles built up
[00:15:55.640 --> 00:15:56.680]   if there's a lot of money
[00:15:56.680 --> 00:15:58.240]   that's floating around in these circles?
[00:15:58.240 --> 00:16:00.320]   - Well, I think we're seeing that now, right?
[00:16:00.320 --> 00:16:02.520]   And I don't think it's a coincidence
[00:16:02.520 --> 00:16:06.480]   that FTX and Theranos were not publicly traded companies.
[00:16:06.480 --> 00:16:10.680]   There's a certain sort of black box quality
[00:16:10.680 --> 00:16:13.440]   to these companies because people aren't shorting them
[00:16:13.440 --> 00:16:16.240]   and aren't whispering to journalists
[00:16:16.240 --> 00:16:18.200]   about that there's something wrong here
[00:16:18.200 --> 00:16:20.320]   and there aren't publicly available financials
[00:16:20.320 --> 00:16:24.040]   for people to dig through and look at the numbers.
[00:16:24.040 --> 00:16:25.880]   So now I don't think that's a coincidence.
[00:16:25.880 --> 00:16:29.000]   And I do think this gigantic move into private assets
[00:16:29.000 --> 00:16:34.000]   has been probably not great for the safety of the system.
[00:16:34.000 --> 00:16:39.520]   And you'd say, well, it's just institutional investors
[00:16:39.520 --> 00:16:41.800]   who can afford to lose money who are losing money,
[00:16:41.800 --> 00:16:45.040]   but it's really not because institutional investors
[00:16:45.040 --> 00:16:46.880]   are just pension fund money
[00:16:46.880 --> 00:16:49.000]   and some places now mutual fund money.
[00:16:49.000 --> 00:16:51.760]   So that distinction that the people who are investing
[00:16:51.760 --> 00:16:55.960]   in this stuff can afford to lose it is not really true.
[00:16:55.960 --> 00:16:58.200]   So I don't like that rationalization.
[00:16:58.200 --> 00:16:59.640]   I think we're gonna see how that plays out.
[00:16:59.640 --> 00:17:01.440]   There was just a really good piece in "The Economist"
[00:17:01.440 --> 00:17:04.880]   about private equity marks on their portfolio companies
[00:17:04.880 --> 00:17:07.560]   and how they are still look to be much higher
[00:17:07.560 --> 00:17:09.240]   than what you would think they should be
[00:17:09.240 --> 00:17:10.960]   given the carnage in the market.
[00:17:10.960 --> 00:17:15.160]   And so all of what actually things are really worth
[00:17:15.160 --> 00:17:17.760]   in private markets, both for venture capital firms
[00:17:17.760 --> 00:17:19.840]   and for private equity firms
[00:17:19.840 --> 00:17:24.120]   is absent another bubble starting in the markets.
[00:17:24.120 --> 00:17:25.840]   I think we're gonna see how that plays out
[00:17:26.600 --> 00:17:27.440]   over the next year.
[00:17:27.440 --> 00:17:30.440]   And it might be a wake up call for a lot of people.
[00:17:30.440 --> 00:17:32.880]   You know, all that said, it's an interesting thing
[00:17:32.880 --> 00:17:35.960]   because investors have been very complicit in this, right?
[00:17:35.960 --> 00:17:39.080]   In the sense that a lot of investors are absolutely delighted
[00:17:39.080 --> 00:17:44.080]   to have their private investments marked at a high level.
[00:17:44.080 --> 00:17:46.720]   They don't have to go to the committee overseeing
[00:17:46.720 --> 00:17:49.800]   the investments and say, look, I lost 20% of your money
[00:17:49.800 --> 00:17:52.880]   the way they might if the numbers were public.
[00:17:52.880 --> 00:17:55.960]   And so that the ability of these private investors
[00:17:55.960 --> 00:18:00.280]   to smooth as they call it, the returns is it's been,
[00:18:00.280 --> 00:18:01.400]   it's been part of the appeal.
[00:18:01.400 --> 00:18:03.680]   It hasn't been a negative, it's been a positive.
[00:18:03.680 --> 00:18:07.440]   And so I would say that investors who wanted the smoothing
[00:18:07.440 --> 00:18:10.320]   are might be getting what they deserve
[00:18:10.320 --> 00:18:11.800]   except for the point you made earlier
[00:18:11.800 --> 00:18:13.400]   that it isn't, it isn't their money.
[00:18:13.400 --> 00:18:16.760]   It's the money of teachers and firefighters
[00:18:16.760 --> 00:18:19.440]   and individual investors around the country.
[00:18:19.440 --> 00:18:21.600]   And that's problematic.
[00:18:21.600 --> 00:18:23.760]   - Yeah, yeah, being in the world of technology
[00:18:23.760 --> 00:18:27.400]   and being around people in it has made me shocked
[00:18:27.400 --> 00:18:29.200]   when I read about these numbers from the past.
[00:18:29.200 --> 00:18:31.200]   For example, when I'm reading your books
[00:18:31.200 --> 00:18:32.920]   and they're detailing things that happened in the 90s
[00:18:32.920 --> 00:18:34.240]   or the 2000s.
[00:18:34.240 --> 00:18:37.480]   And then you realize that the salary
[00:18:37.480 --> 00:18:39.720]   that Hank Paulson made a CEO of Goldman
[00:18:39.720 --> 00:18:43.120]   or that Skilling made as, you know, CEO of Enron,
[00:18:43.120 --> 00:18:45.440]   you know, it's like, I have friends who are my age,
[00:18:45.440 --> 00:18:48.200]   like 22 year olds who are raising seed rounds
[00:18:48.200 --> 00:18:50.040]   that are as big as like these people's salaries.
[00:18:50.040 --> 00:18:51.880]   And so it just feels like these books
[00:18:51.880 --> 00:18:53.680]   where you have $50 billion frauds
[00:18:53.680 --> 00:18:56.160]   or, you know, hundreds of billions of dollars of collapse
[00:18:56.160 --> 00:18:57.840]   and the individuals there,
[00:18:57.840 --> 00:19:00.120]   it just feels like it's missing a few zeros
[00:19:00.120 --> 00:19:02.520]   because of the delusion of the private market.
[00:19:02.520 --> 00:19:04.760]   But speaking of short sellers
[00:19:04.760 --> 00:19:06.480]   and speaking of private equity,
[00:19:06.480 --> 00:19:08.960]   I think it'd be interesting to talk about Musk.
[00:19:08.960 --> 00:19:12.240]   So, you know, your 2018 Vanity Fair article,
[00:19:12.240 --> 00:19:14.080]   I thought was really interesting about, you know,
[00:19:14.080 --> 00:19:16.120]   Musk factory in Buffalo.
[00:19:16.120 --> 00:19:19.400]   How do you think back on Tesla and Musk now,
[00:19:19.400 --> 00:19:21.240]   given the fact that the stock
[00:19:21.240 --> 00:19:23.160]   did continue to rise afterwards
[00:19:23.160 --> 00:19:25.600]   and the factory I believe was completed
[00:19:25.600 --> 00:19:27.920]   and it's higher to the 1500 or so people
[00:19:27.920 --> 00:19:30.000]   that have promised New York state.
[00:19:30.000 --> 00:19:32.520]   Is Musk just a fraud who can pull it off?
[00:19:32.520 --> 00:19:33.400]   And so he's a visionary.
[00:19:33.400 --> 00:19:35.360]   How do you think about Musk in the aftermath?
[00:19:35.360 --> 00:19:37.760]   - So I don't think that's right about Buffalo
[00:19:37.760 --> 00:19:41.280]   and I have to look, but I don't think they ended up,
[00:19:41.280 --> 00:19:43.480]   I mean, the SolarCity business at Tesla
[00:19:43.480 --> 00:19:45.000]   has pretty much collapsed.
[00:19:45.000 --> 00:19:47.400]   I don't think people haven't gotten their roofs.
[00:19:47.400 --> 00:19:49.600]   There was just a piece about how they're canceling
[00:19:49.600 --> 00:19:51.320]   some of their roofs installations.
[00:19:51.320 --> 00:19:54.040]   So Musk has repeatedly made grand visions
[00:19:54.040 --> 00:19:56.080]   about that business that haven't played out.
[00:19:56.080 --> 00:19:58.840]   And I will check this for you, post the podcast,
[00:19:58.840 --> 00:20:01.600]   but I don't think if there is employment
[00:20:01.600 --> 00:20:03.280]   at that factory in Buffalo,
[00:20:03.280 --> 00:20:06.920]   it's not because they're churning out solar products
[00:20:06.920 --> 00:20:10.880]   that are doing what was originally promised.
[00:20:10.880 --> 00:20:14.560]   So I guess I think about that story in a couple of ways.
[00:20:14.560 --> 00:20:17.160]   It definitely, it was not meant to be a piece
[00:20:17.160 --> 00:20:18.000]   about Tesla.
[00:20:18.000 --> 00:20:20.920]   It was meant to be a piece that shone a little bit of light
[00:20:20.920 --> 00:20:25.160]   on how Musk operates and his willingness to flout the rules
[00:20:25.160 --> 00:20:27.440]   and his reliance on government subsidies,
[00:20:27.440 --> 00:20:30.560]   despite the fact that he presents himself
[00:20:30.560 --> 00:20:34.600]   as this libertarian free market, free marketeer,
[00:20:34.600 --> 00:20:38.760]   and his willingness to lie, to on some level,
[00:20:38.760 --> 00:20:40.800]   enrich himself, which also runs counter
[00:20:40.800 --> 00:20:43.480]   to the Elon Musk narrative that he doesn't care
[00:20:43.480 --> 00:20:45.760]   about making money for himself,
[00:20:45.760 --> 00:20:49.600]   because the main reason for Tesla to buy SolarCity
[00:20:49.600 --> 00:20:52.120]   was that SolarCity had, main reason,
[00:20:52.120 --> 00:20:57.000]   was that SolarCity had, that Musk and his relatives
[00:20:57.000 --> 00:20:59.280]   had extended these loans to SolarCity
[00:20:59.280 --> 00:21:01.120]   that were gonna go bank, that were gonna be,
[00:21:01.120 --> 00:21:02.320]   all the money was gonna be lost
[00:21:02.320 --> 00:21:03.960]   at SolarCity went bankrupt.
[00:21:03.960 --> 00:21:05.240]   And by having Tesla buy it,
[00:21:05.240 --> 00:21:09.160]   Musk was able to bail himself out as well.
[00:21:09.160 --> 00:21:12.360]   And I also think a good reason for that,
[00:21:12.360 --> 00:21:13.800]   and it brings us to the present time,
[00:21:13.800 --> 00:21:16.960]   but a reason for the acquisition was that Musk knows
[00:21:16.960 --> 00:21:21.080]   that this image of himself as the invincible, invulnerable,
[00:21:21.080 --> 00:21:22.440]   who can always raise money
[00:21:22.440 --> 00:21:24.440]   and whose companies always work out in the end
[00:21:24.440 --> 00:21:25.600]   was really important.
[00:21:25.600 --> 00:21:27.280]   And if SolarCity had gone bankrupt,
[00:21:27.280 --> 00:21:31.000]   it would have cast a big question mark over Musk,
[00:21:31.000 --> 00:21:32.600]   over the Musk narrative.
[00:21:32.600 --> 00:21:34.520]   And so I think he literally couldn't afford
[00:21:34.520 --> 00:21:36.840]   to let SolarCity go bankrupt.
[00:21:36.840 --> 00:21:40.480]   All of that said, I have been and was,
[00:21:40.480 --> 00:21:42.480]   I was quite skeptical of Tesla.
[00:21:42.480 --> 00:21:46.880]   And I thought about it and I always believed
[00:21:46.880 --> 00:21:47.960]   that the product was great.
[00:21:47.960 --> 00:21:50.320]   I just wasn't sure about the company's
[00:21:50.320 --> 00:21:52.240]   money-making potential.
[00:21:52.240 --> 00:21:54.040]   And I think that that,
[00:21:54.040 --> 00:21:55.880]   something I started thinking about
[00:21:55.880 --> 00:21:58.080]   back around the SolarCity time, maybe earlier,
[00:21:58.080 --> 00:22:00.320]   but this line, something I've talked about before,
[00:22:00.320 --> 00:22:02.560]   but this line between a visionary and a fraudster.
[00:22:02.560 --> 00:22:04.360]   You know, you think that they're on two opposite ends
[00:22:04.360 --> 00:22:06.400]   of the spectrum, but in reality,
[00:22:06.400 --> 00:22:08.160]   they're where the ends of the circle meet,
[00:22:08.160 --> 00:22:10.240]   the characteristics of one,
[00:22:10.240 --> 00:22:13.080]   one has that many of the characteristics of the other.
[00:22:13.080 --> 00:22:14.560]   And sometimes I think the only thing
[00:22:14.560 --> 00:22:16.720]   that really separates the two is that the fraudster
[00:22:16.720 --> 00:22:19.160]   is able to keep getting, raising money
[00:22:19.160 --> 00:22:21.280]   in order to get through the really difficult time
[00:22:21.280 --> 00:22:23.440]   where he or she isn't telling the truth.
[00:22:23.440 --> 00:22:26.880]   And then that person goes down in history as a visionary.
[00:22:26.880 --> 00:22:29.360]   But because no one ever looks back
[00:22:29.360 --> 00:22:31.200]   to the moment in time when they were lying,
[00:22:31.200 --> 00:22:33.280]   the fraudster gets caught in the middle.
[00:22:33.280 --> 00:22:36.680]   So Enron's lost access to the capital markets,
[00:22:36.680 --> 00:22:38.440]   lost access to funding.
[00:22:38.440 --> 00:22:41.120]   As the market collapsed after the dot-com boom,
[00:22:41.120 --> 00:22:43.040]   and people began to wonder whether Skilling
[00:22:43.040 --> 00:22:45.320]   was telling the truth about Enron's broadband business,
[00:22:45.320 --> 00:22:47.120]   and then there were all the disclosures
[00:22:47.120 --> 00:22:49.360]   about Andy Fasto's partnerships.
[00:22:49.360 --> 00:22:52.040]   If Enron had been able to continue raising money,
[00:22:52.040 --> 00:22:54.600]   this business of Enron's called Enron Broadband
[00:22:54.600 --> 00:22:55.840]   might well have been Netflix.
[00:22:55.840 --> 00:22:57.920]   It was Netflix ahead of its time.
[00:22:57.920 --> 00:22:59.680]   So Enron just got caught in the middle
[00:22:59.680 --> 00:23:02.960]   and all the fraud got exposed.
[00:23:02.960 --> 00:23:05.520]   But that's not because Jeff Skilling wasn't a visionary
[00:23:05.520 --> 00:23:09.280]   who had really grand plans for the future.
[00:23:09.280 --> 00:23:12.040]   So I think Musk falls somewhere in that spectrum
[00:23:12.040 --> 00:23:13.960]   of fraudster and visionary.
[00:23:13.960 --> 00:23:15.760]   And what's gonna be really interesting,
[00:23:15.760 --> 00:23:18.440]   why I said that we bring it to the present time
[00:23:18.440 --> 00:23:20.200]   about what happens to the Musk narrative
[00:23:20.200 --> 00:23:22.160]   if something fails is what happens
[00:23:22.160 --> 00:23:24.280]   as the world watches Twitter implode.
[00:23:24.280 --> 00:23:26.480]   What does that mean then
[00:23:26.480 --> 00:23:28.440]   for the Elon Musk narrative overall?
[00:23:28.440 --> 00:23:30.120]   - Yeah, yeah.
[00:23:30.120 --> 00:23:32.840]   Going back to the smartest guy in the room,
[00:23:32.840 --> 00:23:36.720]   the title obviously suggests something about the,
[00:23:36.720 --> 00:23:40.120]   I guess in general, the ability and the likelihood
[00:23:40.120 --> 00:23:42.840]   of very smart people committing fraud
[00:23:42.840 --> 00:23:45.240]   or things of that sort.
[00:23:45.240 --> 00:23:47.960]   But, you know, Gerard Jones has this book called "Hive Mind"
[00:23:47.960 --> 00:23:51.160]   where he talks about how the smarter people
[00:23:51.160 --> 00:23:52.360]   are more likely to cooperate
[00:23:52.360 --> 00:23:54.560]   in prisoners dilemma type situations.
[00:23:54.560 --> 00:23:55.880]   They have longer time preference.
[00:23:55.880 --> 00:23:57.040]   And one of the things you've written about
[00:23:57.040 --> 00:23:58.560]   is the problem in corporate America
[00:23:58.560 --> 00:24:02.880]   is people doing too big time discounting.
[00:24:02.880 --> 00:24:05.440]   So given that trend we see in general
[00:24:05.440 --> 00:24:07.880]   of greater cooperativeness
[00:24:07.880 --> 00:24:11.560]   and other kinds of traits of more intelligent people,
[00:24:11.560 --> 00:24:14.280]   do you think the reason we often find people like SPF
[00:24:14.280 --> 00:24:16.000]   and scaling running big frauds
[00:24:16.000 --> 00:24:17.280]   despite being very intelligent,
[00:24:17.280 --> 00:24:19.200]   is it just that on average smarter people
[00:24:19.200 --> 00:24:20.640]   but maybe less likely to commit fraud,
[00:24:20.640 --> 00:24:22.360]   but when they do commit fraud,
[00:24:22.360 --> 00:24:24.040]   they do it at such gargantuan scales
[00:24:24.040 --> 00:24:25.800]   and are able to do it at such gargantuan scales
[00:24:25.800 --> 00:24:27.760]   that it just brings down entire empires?
[00:24:27.760 --> 00:24:28.840]   How do you think about the relationship
[00:24:28.840 --> 00:24:30.760]   between intelligence and fraud?
[00:24:30.760 --> 00:24:32.440]   - That's interesting.
[00:24:32.440 --> 00:24:36.640]   I'm not sure I know a coherent answer to that.
[00:24:36.640 --> 00:24:38.440]   Smartest Guys in the Room as a title
[00:24:38.440 --> 00:24:39.920]   was a little bit tongue in cheek.
[00:24:39.920 --> 00:24:41.680]   It wasn't meant to say these guys
[00:24:41.680 --> 00:24:43.480]   actually are the smartest guys in the room.
[00:24:43.480 --> 00:24:46.840]   It was a little bit ironic,
[00:24:46.840 --> 00:24:48.000]   but that doesn't take away
[00:24:48.000 --> 00:24:50.240]   from the really good question that you asked,
[00:24:50.240 --> 00:24:52.800]   which is what is that relationship?
[00:24:52.800 --> 00:24:56.040]   I mean, I think if you look at the history
[00:24:56.040 --> 00:24:56.960]   of corporate fraud,
[00:24:56.960 --> 00:24:59.520]   you are not going to find unintelligent people
[00:24:59.520 --> 00:25:02.040]   having been the masterminds behind this.
[00:25:02.040 --> 00:25:04.320]   You're gonna find really, really, really smart,
[00:25:04.320 --> 00:25:08.520]   even brilliant people having been behind it.
[00:25:08.520 --> 00:25:10.760]   Maybe some part of that is this linkage
[00:25:10.760 --> 00:25:12.320]   between the visionary and the fraudster
[00:25:12.320 --> 00:25:15.720]   that so many of these corporate frauds
[00:25:15.720 --> 00:25:18.120]   are people who have qualities of the visionary
[00:25:18.120 --> 00:25:20.400]   and to have the qualities of a visionary,
[00:25:20.400 --> 00:25:23.760]   you have to have a pretty high intelligence.
[00:25:24.720 --> 00:25:26.560]   And I do think so many of these stories
[00:25:26.560 --> 00:25:29.400]   are about then self-delusion.
[00:25:29.400 --> 00:25:32.240]   So I don't think smart people are any less likely
[00:25:32.240 --> 00:25:34.680]   to suffer from self-delusion than dumb people.
[00:25:34.680 --> 00:25:36.800]   And they're probably more likely to
[00:25:36.800 --> 00:25:38.400]   because you can rationalize.
[00:25:38.400 --> 00:25:40.000]   You know, the smart person's ability
[00:25:40.000 --> 00:25:41.600]   to rationalize just about anything
[00:25:41.600 --> 00:25:44.920]   they want to rationalize is pretty profound.
[00:25:44.920 --> 00:25:48.240]   Whereas perhaps someone who doesn't have
[00:25:48.240 --> 00:25:50.720]   quite the same brain power
[00:25:50.720 --> 00:25:52.720]   isn't gonna be able to create a narrative
[00:25:52.720 --> 00:25:54.640]   under which their actions are blameless
[00:25:54.640 --> 00:25:56.560]   and they're doing the right thing.
[00:25:56.560 --> 00:25:58.120]   So I think sometimes,
[00:25:58.120 --> 00:26:00.320]   so maybe there is some sort of relationship there
[00:26:00.320 --> 00:26:02.120]   that somebody more qualified than I am
[00:26:02.120 --> 00:26:04.880]   would have to study between smart people's ability
[00:26:04.880 --> 00:26:08.560]   to rationalize just about anything
[00:26:08.560 --> 00:26:11.640]   as a way of, as part of the path to self-delusion
[00:26:11.640 --> 00:26:14.800]   and part of the path by which these things happen.
[00:26:14.800 --> 00:26:17.880]   But that's completely, that's completely,
[00:26:17.880 --> 00:26:19.160]   that's Bethany theory.
[00:26:19.160 --> 00:26:21.280]   There's absolutely nothing to back that up.
[00:26:21.280 --> 00:26:23.240]   Let's be clear.
[00:26:23.240 --> 00:26:24.440]   - Let's do some more speculation.
[00:26:24.440 --> 00:26:27.920]   So one of the things John Ray talked about
[00:26:27.920 --> 00:26:31.520]   in his testimony, was it two days ago?
[00:26:31.520 --> 00:26:32.800]   He said that, you know,
[00:26:32.800 --> 00:26:36.320]   FTX had done $5 billion of investments
[00:26:36.320 --> 00:26:38.360]   and deals in the last year.
[00:26:38.360 --> 00:26:40.520]   And most of those investments were worth
[00:26:40.520 --> 00:26:43.280]   a fraction of the value that FTX paid for them.
[00:26:43.280 --> 00:26:45.760]   And we see this also in, obviously in Enron, right?
[00:26:45.760 --> 00:26:49.240]   With broadband and with DAPL, is that how to pronounce it?
[00:26:49.240 --> 00:26:51.400]   But basically their international department.
[00:26:51.400 --> 00:26:55.800]   What is this obsession with deal-making for its own sake?
[00:26:55.800 --> 00:26:57.560]   Is that to appease investors
[00:26:57.560 --> 00:26:58.760]   and make them think a lot's going on?
[00:26:58.760 --> 00:27:01.800]   Is that because of the hubris of the founder
[00:27:01.800 --> 00:27:04.400]   of just wanting to set up a big empire as fast as possible,
[00:27:04.400 --> 00:27:06.040]   even if you're getting a bad sticker price?
[00:27:06.040 --> 00:27:09.040]   Why do we see this pattern of just, you know,
[00:27:09.040 --> 00:27:11.680]   excessive deal-making for its own sake?
[00:27:11.680 --> 00:27:13.040]   - That's an interesting question too.
[00:27:13.040 --> 00:27:15.360]   I'm not sure that that's limited
[00:27:15.360 --> 00:27:17.880]   to companies that go splat dramatically.
[00:27:17.880 --> 00:27:21.560]   There's a lot of deal-making in corporate America
[00:27:21.560 --> 00:27:24.560]   has that same frenzied quality.
[00:27:24.560 --> 00:27:28.160]   I haven't seen an updated study on this in a long time,
[00:27:28.160 --> 00:27:31.000]   but I began my career working as an analyst
[00:27:31.000 --> 00:27:33.720]   in an M&A department at Goldman Sachs.
[00:27:33.720 --> 00:27:38.560]   And definitely deals are done for the sake of doing deals.
[00:27:38.560 --> 00:27:41.680]   And I once joked that synergies are kind of like UFOs.
[00:27:41.680 --> 00:27:43.640]   A lot of people claim to have seen them,
[00:27:43.640 --> 00:27:45.960]   but there's no proof that they actually exist.
[00:27:45.960 --> 00:27:49.880]   And I haven't seen an updated study on this,
[00:27:49.880 --> 00:27:52.840]   but there was one years back that showed
[00:27:52.840 --> 00:27:55.200]   that most M&A transactions don't result
[00:27:55.200 --> 00:27:56.920]   in increased value for shareholders.
[00:27:56.920 --> 00:28:00.280]   And most synergies, most proto synergies never materialize.
[00:28:00.280 --> 00:28:02.360]   Just getting bigger for the sake of getting bigger
[00:28:02.360 --> 00:28:05.000]   and doing deals for the short-term value
[00:28:05.000 --> 00:28:06.680]   of showing Wall Street a projection
[00:28:06.680 --> 00:28:08.360]   that earnings are gonna be so much higher,
[00:28:08.360 --> 00:28:10.600]   even after the cost of the debt that you've taken on
[00:28:10.600 --> 00:28:12.360]   and that there are these great synergies
[00:28:12.360 --> 00:28:15.560]   that are gonna come about from combining businesses.
[00:28:15.560 --> 00:28:18.800]   So I don't know that either the frenzy deal doing
[00:28:18.800 --> 00:28:23.600]   or deal doing deals gone wrong is solely limited
[00:28:23.600 --> 00:28:25.560]   to people who are committing fraud.
[00:28:25.560 --> 00:28:28.080]   I think it's kind of across the spectrum.
[00:28:28.080 --> 00:28:31.480]   - Kiki, well, one thing I find interesting
[00:28:31.480 --> 00:28:33.760]   about your books is how you detail that,
[00:28:33.760 --> 00:28:35.720]   and correct me if this is the wrong way to read them,
[00:28:35.720 --> 00:28:39.320]   but that incentives are not the only thing that matter.
[00:28:39.320 --> 00:28:41.480]   You know, there's this perception that, you know,
[00:28:41.480 --> 00:28:43.280]   we've set a bad incentive for these actors
[00:28:43.280 --> 00:28:44.240]   and that's why they did bad things.
[00:28:44.240 --> 00:28:46.840]   But also the power of one individual
[00:28:46.840 --> 00:28:50.120]   to shape a company's culture and the power of that culture
[00:28:50.120 --> 00:28:53.600]   to enable bad behavior, whether a scaling at Enron
[00:28:53.600 --> 00:28:55.880]   or with Clarkson, right, at Moody's.
[00:28:55.880 --> 00:28:56.720]   - Yeah.
[00:28:56.720 --> 00:28:59.360]   - Is that a good way of reading your books
[00:28:59.360 --> 00:29:01.120]   or how do you think about the relative importance
[00:29:01.120 --> 00:29:02.960]   of culture and incentives?
[00:29:02.960 --> 00:29:04.480]   - I think that's really fair.
[00:29:04.480 --> 00:29:06.760]   Incentives are part of culture, right?
[00:29:06.760 --> 00:29:09.880]   If you've set up the culture where how you're valued
[00:29:09.880 --> 00:29:11.880]   is what you get paid, I think it's a little,
[00:29:11.880 --> 00:29:15.000]   it's a little difficult to separate those two things out
[00:29:15.000 --> 00:29:18.760]   because the incentives do help make the culture.
[00:29:18.760 --> 00:29:23.640]   But for sure, culture is incredibly, incredibly compelling.
[00:29:23.640 --> 00:29:26.800]   I've often thought and said that if I had,
[00:29:26.800 --> 00:29:28.760]   when I was leaving my short-lived career
[00:29:28.760 --> 00:29:31.400]   in investment banking, if I had gotten,
[00:29:31.400 --> 00:29:33.240]   and some of the headhunters I was talking to,
[00:29:33.240 --> 00:29:35.200]   if one of them had said, "There's this great,
[00:29:35.200 --> 00:29:37.800]   "really energetic, interesting energy company
[00:29:37.800 --> 00:29:40.840]   "down in Houston, I want a picture of you there."
[00:29:40.840 --> 00:29:43.480]   If I had gone there, would I have been a whistleblower
[00:29:43.480 --> 00:29:45.160]   or would I have been a believer?
[00:29:45.160 --> 00:29:47.560]   And I like to believe I would have been a whistleblower,
[00:29:47.560 --> 00:29:49.040]   but I think it's equally likely
[00:29:49.040 --> 00:29:50.520]   that I would have been a believer.
[00:29:50.520 --> 00:29:51.840]   Culture is so strong.
[00:29:51.840 --> 00:29:54.440]   It creates this, what's it right, maybe a miasma
[00:29:54.440 --> 00:29:55.840]   that you can't see outside.
[00:29:55.840 --> 00:29:58.760]   I remember a guy I talked to who was a trader at Enron,
[00:29:58.760 --> 00:30:02.120]   really smart guy, and he was like, after the bankruptcy,
[00:30:02.120 --> 00:30:04.560]   he said, "Of course, if we're all getting paid
[00:30:04.560 --> 00:30:07.520]   "based on creating reported earnings
[00:30:07.520 --> 00:30:09.600]   "and there's all this cash going out the door
[00:30:09.600 --> 00:30:10.960]   "in order to do these deals
[00:30:10.960 --> 00:30:13.120]   "that are creating reported earnings,
[00:30:13.120 --> 00:30:14.920]   "and that's the culture of the entire firm,
[00:30:14.920 --> 00:30:17.080]   "of course it's not gonna work economically."
[00:30:17.080 --> 00:30:18.280]   He said, "I never thought about it.
[00:30:18.280 --> 00:30:20.680]   "It just didn't occur to me."
[00:30:20.680 --> 00:30:23.600]   And I think the more compelling the CEO,
[00:30:23.600 --> 00:30:27.400]   the more likely you are to have that kind of mass delusion.
[00:30:27.400 --> 00:30:29.600]   I mean, there's a reason cults exist, right?
[00:30:29.600 --> 00:30:33.960]   We are, as human beings, remarkably susceptible
[00:30:33.960 --> 00:30:36.360]   to, quote, visionary leaders.
[00:30:36.360 --> 00:30:38.520]   It's just, it's the way the human brain is wired.
[00:30:38.520 --> 00:30:40.120]   We wanna believe.
[00:30:40.120 --> 00:30:42.120]   And especially if somebody has the ability
[00:30:42.120 --> 00:30:45.240]   to put a vision forward, like Jeff Skilling did at Enron,
[00:30:45.240 --> 00:30:48.640]   like Elizabeth Holmes did at Theranos, like SBF did,
[00:30:48.640 --> 00:30:51.280]   where you feel like you're in the service
[00:30:51.280 --> 00:30:55.200]   of something greater by helping this vision actualize,
[00:30:55.200 --> 00:30:58.120]   then you're particularly susceptible.
[00:30:58.120 --> 00:30:59.440]   And I think that is the place
[00:30:59.440 --> 00:31:02.760]   where incentives don't quite explain things.
[00:31:02.760 --> 00:31:05.560]   There is this very human desire to matter,
[00:31:05.560 --> 00:31:07.680]   to do something important, to be doing something
[00:31:07.680 --> 00:31:09.200]   that's gonna change the world.
[00:31:09.200 --> 00:31:12.080]   And when somebody can tap into that desire in people,
[00:31:12.080 --> 00:31:13.760]   that feeling that what you're doing
[00:31:13.760 --> 00:31:16.280]   isn't just work and a paycheck and the incentives you have,
[00:31:16.280 --> 00:31:18.680]   but, I mean, I guess it is part of the incentive,
[00:31:18.680 --> 00:31:20.880]   but that you're part of some greater good,
[00:31:20.880 --> 00:31:23.720]   that's incredibly powerful.
[00:31:23.720 --> 00:31:26.880]   - Yeah, speaking of- - We all wanna matter.
[00:31:26.880 --> 00:31:29.560]   - Yeah, speaking of people's psychology,
[00:31:29.560 --> 00:31:32.400]   crime and punishment, underrated or overrated,
[00:31:32.400 --> 00:31:34.240]   as a way to analyze the psychology
[00:31:34.240 --> 00:31:36.000]   of people like Skilling and SBF.
[00:31:36.000 --> 00:31:38.640]   Or maybe SBF specifically because of the utilitarian nature
[00:31:38.640 --> 00:31:39.640]   of SBF's crime.
[00:31:39.640 --> 00:31:43.440]   - I think it's overrated.
[00:31:43.440 --> 00:31:48.440]   I'm not sure anybody has ever proven
[00:31:48.440 --> 00:31:51.160]   that jail sentences for white-collar criminals
[00:31:51.160 --> 00:31:54.600]   do anything to deter subsequent white-collar crime.
[00:31:54.600 --> 00:31:56.800]   And I think one part of this is the self-delusion
[00:31:56.800 --> 00:31:58.560]   that I talked about.
[00:31:58.560 --> 00:32:02.000]   Nobody thinks, oh, I'm doing the same thing
[00:32:02.000 --> 00:32:03.560]   as Jeff Skilling did at Enron.
[00:32:03.560 --> 00:32:06.840]   And if I do this, then I too might end up in jail,
[00:32:06.840 --> 00:32:08.480]   therefore, I don't wanna do this.
[00:32:08.480 --> 00:32:09.600]   I just don't think that's the way
[00:32:09.600 --> 00:32:12.480]   that the thought process works.
[00:32:12.480 --> 00:32:14.320]   I think Elizabeth Holmes at Theranos
[00:32:14.320 --> 00:32:16.480]   probably for the most part convinced herself
[00:32:16.480 --> 00:32:18.040]   that this was going to work.
[00:32:18.040 --> 00:32:20.480]   And that if you just push forward and push hard enough
[00:32:20.480 --> 00:32:22.040]   and keep telling people what they wanna hear
[00:32:22.040 --> 00:32:24.840]   and keep being able to raise money, it's gonna work.
[00:32:24.840 --> 00:32:27.840]   If you pause to think, well, what if it doesn't work
[00:32:27.840 --> 00:32:29.920]   and I've lied and I go to jail,
[00:32:29.920 --> 00:32:32.680]   then you'd stop right then and there.
[00:32:32.680 --> 00:32:37.240]   So I think that I'm not sure it's much of a deterrent.
[00:32:37.240 --> 00:32:39.760]   I remember, and partly I'm biased
[00:32:39.760 --> 00:32:42.120]   because I remember a piece my co-author, Peter Alkaid,
[00:32:42.120 --> 00:32:44.120]   and I wrote right after Jeff Skilling and Ken Lay
[00:32:44.120 --> 00:32:45.440]   were convicted.
[00:32:45.440 --> 00:32:47.600]   And we wrote a piece for "Fortune" in which we said
[00:32:47.600 --> 00:32:49.760]   that the entire world has changed now,
[00:32:49.760 --> 00:32:53.640]   that corporate executives are put on high alert,
[00:32:53.640 --> 00:32:56.320]   that behavior in the gray area will no longer be tolerated
[00:32:56.320 --> 00:32:58.760]   and that it will be aggressively prosecuted.
[00:32:58.760 --> 00:33:01.000]   And this was spring of 2006
[00:33:01.000 --> 00:33:03.400]   and the events that caused the global financial crisis
[00:33:03.400 --> 00:33:04.960]   were pretty well underway.
[00:33:04.960 --> 00:33:07.920]   It didn't do much to prevent the global financial crisis.
[00:33:07.920 --> 00:33:10.680]   Enron's jail time didn't do anything
[00:33:10.680 --> 00:33:12.680]   to prevent Elizabeth Holmes.
[00:33:12.680 --> 00:33:14.160]   Doesn't seem to have done anything
[00:33:14.160 --> 00:33:16.120]   to change what SBF was doing.
[00:33:16.120 --> 00:33:19.400]   So I just, I'm not sure.
[00:33:19.400 --> 00:33:22.120]   I'm sure a psychologist or somebody who specializes
[00:33:22.120 --> 00:33:23.560]   in studying white collar crime
[00:33:23.560 --> 00:33:25.280]   could probably make a argument
[00:33:25.280 --> 00:33:26.800]   that reshoots everything I said
[00:33:26.800 --> 00:33:29.280]   and that shows that it has had a deterring effect.
[00:33:29.280 --> 00:33:32.040]   But I just don't think that people
[00:33:32.040 --> 00:33:34.600]   who get themselves into this situation
[00:33:34.600 --> 00:33:37.240]   consciously think this is what I'm doing.
[00:33:37.240 --> 00:33:38.440]   - Yeah, yeah.
[00:33:38.440 --> 00:33:39.960]   Speaking of other incentives,
[00:33:39.960 --> 00:33:41.600]   the stock options,
[00:33:41.600 --> 00:33:45.720]   you've spoken about how that creates short-term incentives
[00:33:45.720 --> 00:33:48.880]   for the executives who are making decisions.
[00:33:48.880 --> 00:33:50.800]   If you wanted to set up an instrument
[00:33:50.800 --> 00:33:54.400]   that aligned an executive or a leader's compensation
[00:33:54.400 --> 00:33:56.480]   with the long-term performance of a company,
[00:33:56.480 --> 00:33:57.360]   what would that look like?
[00:33:57.360 --> 00:33:59.320]   Would you have the options invested in 10 years
[00:33:59.320 --> 00:34:00.280]   instead of a year?
[00:34:00.280 --> 00:34:01.240]   How would you design it?
[00:34:01.240 --> 00:34:03.120]   How would you design a compensation scheme
[00:34:03.120 --> 00:34:04.920]   to avoid long-term thinking?
[00:34:04.920 --> 00:34:07.000]   - If I could do that, I should rule the world.
[00:34:07.000 --> 00:34:12.000]   I think that is one of the really tough problems
[00:34:12.000 --> 00:34:17.000]   confronting boards or anybody who is determining stock options
[00:34:17.000 --> 00:34:19.960]   and that almost anybody who's determining compensation
[00:34:19.960 --> 00:34:22.160]   and most compensation schemes
[00:34:22.160 --> 00:34:25.360]   seem to have really terrible unintended consequences.
[00:34:25.360 --> 00:34:27.200]   They look really good on paper
[00:34:27.200 --> 00:34:29.000]   and then as they're implemented,
[00:34:29.000 --> 00:34:30.920]   it turns out that there was a way
[00:34:30.920 --> 00:34:33.200]   in which they accomplished exactly the opposite
[00:34:33.200 --> 00:34:35.880]   of a thing people who are designing them
[00:34:35.880 --> 00:34:37.880]   wanted them to accomplish.
[00:34:37.880 --> 00:34:40.160]   I mean, if you think back to the advent of stock options,
[00:34:40.160 --> 00:34:42.080]   what could sound better, right?
[00:34:42.080 --> 00:34:44.840]   Giving management a share of the company
[00:34:44.840 --> 00:34:49.040]   such that if shareholders did well, they'd do well.
[00:34:49.040 --> 00:34:50.800]   Nobody envisions the ways
[00:34:50.800 --> 00:34:52.720]   in which stock options could be repriced,
[00:34:52.720 --> 00:34:55.120]   the ways in which meeting earnings targets
[00:34:55.120 --> 00:34:57.080]   could lead to gaming,
[00:34:57.080 --> 00:35:00.480]   the ways in which the incentive of stock-based compensation
[00:35:00.480 --> 00:35:03.160]   could lead to people trying to get anything they could
[00:35:03.160 --> 00:35:05.440]   in order to get the stock price higher and cash out
[00:35:05.440 --> 00:35:08.200]   when they're, as soon as their stock options vested.
[00:35:08.200 --> 00:35:11.400]   So even there was the whole Valiant saga
[00:35:11.400 --> 00:35:12.640]   was fascinating on this front
[00:35:12.640 --> 00:35:14.200]   because the people who designed
[00:35:14.200 --> 00:35:17.240]   Mike Pearson's compensation package as CEO of Valiant,
[00:35:17.240 --> 00:35:19.320]   they were convinced that this was absolutely
[00:35:19.320 --> 00:35:20.440]   the way to do it.
[00:35:20.440 --> 00:35:23.960]   And he got bigger and bigger stock option incentives
[00:35:23.960 --> 00:35:25.360]   for hitting certain,
[00:35:25.360 --> 00:35:27.560]   for having the stock achieve certain levels.
[00:35:27.560 --> 00:35:29.840]   But of course that creates this incredible bias
[00:35:29.840 --> 00:35:31.280]   to just get the stock to go up
[00:35:31.280 --> 00:35:33.480]   no matter what else you do.
[00:35:33.480 --> 00:35:37.200]   It does seem to me that vesting over the long-term
[00:35:37.200 --> 00:35:41.040]   is a much better way to go about things,
[00:35:41.040 --> 00:35:44.840]   but then do you create incentives for people to play games
[00:35:44.840 --> 00:35:47.480]   in order to get the stock lower at various points
[00:35:47.480 --> 00:35:49.560]   where there's about to be a stock optional board
[00:35:49.560 --> 00:35:50.800]   so that they have a better chance
[00:35:50.800 --> 00:35:53.240]   of having their options be worth something
[00:35:53.240 --> 00:35:54.400]   over the long-term?
[00:35:54.400 --> 00:35:56.040]   And do you, particularly on Wall Street,
[00:35:56.040 --> 00:35:58.480]   there is this, or in firms where this sort of stuff
[00:35:58.480 --> 00:36:00.840]   matters the most, there is this,
[00:36:00.840 --> 00:36:04.320]   there was this clearing of dead wood that happened
[00:36:04.320 --> 00:36:06.280]   where people got paid and they got out of the way
[00:36:06.280 --> 00:36:08.000]   and made way for younger people.
[00:36:08.000 --> 00:36:09.520]   And I don't know, it was a harsh culture,
[00:36:09.520 --> 00:36:12.160]   but maybe it made sense on some level.
[00:36:12.160 --> 00:36:13.640]   And now at least I've been told
[00:36:13.640 --> 00:36:15.200]   with much longer vesting periods,
[00:36:15.200 --> 00:36:17.240]   you have people who don't wanna let go.
[00:36:17.240 --> 00:36:19.520]   And so you have more of a problem
[00:36:19.520 --> 00:36:22.520]   with people who should have retired sticking around
[00:36:22.520 --> 00:36:25.840]   instead of clearing out.
[00:36:25.840 --> 00:36:28.440]   And then it also becomes a question of
[00:36:28.440 --> 00:36:29.840]   how much money is enough?
[00:36:29.840 --> 00:36:32.000]   So if somebody is getting millions of dollars
[00:36:32.000 --> 00:36:34.280]   in short-term compensation and they have a whole bunch
[00:36:34.280 --> 00:36:36.760]   more money tied up in long-term compensation,
[00:36:36.760 --> 00:36:38.800]   do the long-term numbers matter?
[00:36:38.800 --> 00:36:41.280]   At what point do they really matter?
[00:36:41.280 --> 00:36:43.760]   I mean, if you gave me $5 million today,
[00:36:43.760 --> 00:36:45.000]   I'm not so sure I'd really care
[00:36:45.000 --> 00:36:47.440]   if I were getting another $5 million in 10 years.
[00:36:47.440 --> 00:36:49.680]   - Yeah.
[00:36:49.760 --> 00:36:52.160]   - So I think all of that is,
[00:36:52.160 --> 00:36:56.480]   I'm not sure there's a perfect compensation system.
[00:36:56.480 --> 00:36:57.760]   All things considered though,
[00:36:57.760 --> 00:37:00.520]   I think longer term is probably better, but.
[00:37:00.520 --> 00:37:03.200]   - Yeah, I didn't think about that downside
[00:37:03.200 --> 00:37:05.440]   of the long-term investing period, that's so interesting.
[00:37:05.440 --> 00:37:07.000]   I guess there is no free lunch.
[00:37:07.000 --> 00:37:10.720]   So with Enron, it was clear
[00:37:10.720 --> 00:37:12.520]   that there was a lot of talent at the firm
[00:37:12.520 --> 00:37:14.760]   and that you had these companies
[00:37:14.760 --> 00:37:17.160]   and these trading firms launch in the aftermath
[00:37:17.160 --> 00:37:18.280]   by people who left Enron,
[00:37:18.280 --> 00:37:22.760]   Kinder Morgan and John Arnold's Centaurus
[00:37:22.760 --> 00:37:25.040]   that were wildly profitable and did well.
[00:37:25.040 --> 00:37:26.760]   Do you think we'll see the same thing with FTX?
[00:37:26.760 --> 00:37:28.520]   That while SBF himself
[00:37:28.520 --> 00:37:30.800]   and maybe his close cadre were frauds,
[00:37:30.800 --> 00:37:33.080]   there actually was a lot of great trading
[00:37:33.080 --> 00:37:34.800]   and engineering talent there that are gonna start
[00:37:34.800 --> 00:37:37.560]   these very successful firms in the aftermath?
[00:37:37.560 --> 00:37:38.640]   - That's interesting.
[00:37:38.640 --> 00:37:40.720]   And just for the sake of clarification,
[00:37:40.720 --> 00:37:43.960]   Kinder Morgan was actually started years before Enron's class
[00:37:43.960 --> 00:37:47.120]   when Rich Kinder, who was vying with Jeff Skilling
[00:37:47.120 --> 00:37:49.680]   in a sense to become chief operating officer,
[00:37:49.680 --> 00:37:52.240]   Ken Lay picked Jeff Skilling and Kinder left
[00:37:52.240 --> 00:37:55.240]   and took a few assets and went to create Kinder Morgan.
[00:37:55.240 --> 00:37:56.560]   But your overall point,
[00:37:56.560 --> 00:37:58.840]   I'm just clarifying your overall point holds,
[00:37:58.840 --> 00:38:00.480]   there were a lot of people who left Enron
[00:38:00.480 --> 00:38:04.160]   and went on to have pretty remarkable careers.
[00:38:04.160 --> 00:38:06.400]   I think the answer with FTX,
[00:38:06.400 --> 00:38:08.520]   I bet there will be some for sure,
[00:38:08.520 --> 00:38:10.800]   but whether they will be in the crypto space,
[00:38:10.800 --> 00:38:14.240]   I guess depends on your views on long-term viability
[00:38:14.240 --> 00:38:16.200]   of the crypto space.
[00:38:16.200 --> 00:38:18.800]   And I have never, it's funny,
[00:38:18.800 --> 00:38:21.160]   as crypto exploded over the last couple of years,
[00:38:21.160 --> 00:38:23.760]   I've been working on this book about the pandemic
[00:38:23.760 --> 00:38:25.520]   and it's been busy and difficult enough
[00:38:25.520 --> 00:38:28.920]   that I have not lifted my head to think about much else.
[00:38:28.920 --> 00:38:30.880]   And I always thought, I don't get it.
[00:38:30.880 --> 00:38:32.360]   I don't understand.
[00:38:32.360 --> 00:38:34.400]   I understand the whole argument about the blockchain
[00:38:34.400 --> 00:38:38.080]   being valuable for a lot of transactions and I get that,
[00:38:38.080 --> 00:38:40.040]   but I never understood crypto itself.
[00:38:40.040 --> 00:38:41.480]   And I thought, well, I just need to,
[00:38:41.480 --> 00:38:42.880]   as soon as this book is done,
[00:38:42.880 --> 00:38:44.800]   I just need to put a month into understanding this
[00:38:44.800 --> 00:38:47.280]   because it's obviously an important enough part of our world
[00:38:47.280 --> 00:38:49.080]   that I need to figure it out.
[00:38:49.080 --> 00:38:53.000]   But now I think, oh, maybe I didn't understand it
[00:38:53.000 --> 00:38:56.280]   for a reason and maybe there isn't anything to understand.
[00:38:56.280 --> 00:38:59.600]   And I've just saved myself time because it's all gone.
[00:38:59.600 --> 00:39:01.720]   And you have people like Larry Fink at BlackRock
[00:39:01.720 --> 00:39:04.280]   saying whole industry's gonna implode, it's done.
[00:39:04.280 --> 00:39:06.480]   And certainly with the news today, this morning,
[00:39:06.480 --> 00:39:09.440]   the finances auditor basically saying, we're out.
[00:39:11.280 --> 00:39:15.560]   I don't know how much of it was a Ponzi scheme.
[00:39:15.560 --> 00:39:16.760]   You might know better than I do.
[00:39:16.760 --> 00:39:18.480]   And so I don't know what's left
[00:39:18.480 --> 00:39:20.160]   after this whole thing implodes.
[00:39:20.160 --> 00:39:21.640]   It's a little bit like,
[00:39:21.640 --> 00:39:24.360]   there is an analogy here that when Enron imploded,
[00:39:24.360 --> 00:39:25.800]   yes, a lot of people went on
[00:39:25.800 --> 00:39:27.920]   to start other successful businesses,
[00:39:27.920 --> 00:39:30.480]   but the whole energy trading business
[00:39:30.480 --> 00:39:34.880]   is practiced by kind of under-capitalized energy firms
[00:39:34.880 --> 00:39:36.960]   went away and that never came back.
[00:39:36.960 --> 00:39:40.080]   And so I don't know.
[00:39:40.080 --> 00:39:41.560]   It'll be, I don't know.
[00:39:41.560 --> 00:39:43.120]   What do you think?
[00:39:43.120 --> 00:39:45.560]   - The time to be worried will be when Bethany McLean
[00:39:45.560 --> 00:39:48.600]   writes an article titled, "Is Bitcoin Overvalued?"
[00:39:48.600 --> 00:39:51.880]   - I was gonna put my moment on that.
[00:39:51.880 --> 00:39:53.240]   (laughing)
[00:39:53.240 --> 00:39:54.960]   - For the audience, that was, I believe,
[00:39:54.960 --> 00:39:58.120]   the first skeptical article about Enron's stock price.
[00:39:58.120 --> 00:39:58.960]   - Yeah.
[00:39:58.960 --> 00:40:01.320]   - And it was titled, "Is Enron Overvalued?"
[00:40:01.320 --> 00:40:03.440]   Aftermath, it understated the title.
[00:40:03.440 --> 00:40:04.280]   - Yeah.
[00:40:04.280 --> 00:40:07.920]   Joke that that story should have won awards
[00:40:07.920 --> 00:40:11.000]   for the meekest title in business journalism history,
[00:40:11.000 --> 00:40:13.280]   from that the company was bankrupt six months later.
[00:40:13.280 --> 00:40:15.520]   (laughing)
[00:40:15.520 --> 00:40:19.560]   - Let me ask a bigger question about finance in general.
[00:40:19.560 --> 00:40:23.000]   So finance is 9% of GDP, I believe.
[00:40:23.000 --> 00:40:26.680]   How much of that is the productive use
[00:40:26.680 --> 00:40:30.360]   and thinking and allocation of the capital
[00:40:30.360 --> 00:40:32.680]   towards their most productive ends?
[00:40:32.680 --> 00:40:34.400]   And how much of that is just zero sum
[00:40:34.400 --> 00:40:36.480]   or negative sum games?
[00:40:36.480 --> 00:40:37.440]   If you had to break that down,
[00:40:37.440 --> 00:40:39.600]   like is 9% too high, do you think?
[00:40:39.600 --> 00:40:41.000]   Or is it just right?
[00:40:41.000 --> 00:40:43.160]   - I think it's too high.
[00:40:43.160 --> 00:40:45.520]   I have no idea how to think about breaking it down
[00:40:45.520 --> 00:40:47.720]   to what the proper level should be.
[00:40:47.720 --> 00:40:49.760]   But I think there are other ways to think about
[00:40:49.760 --> 00:40:51.600]   how you can see that in past decades,
[00:40:51.600 --> 00:40:53.520]   it hasn't been at the right level.
[00:40:53.520 --> 00:40:56.800]   When you've had all sorts of smart kids
[00:40:56.800 --> 00:40:59.160]   leaving business school and leaving college
[00:40:59.160 --> 00:41:01.400]   and heading into finance and hedge funds
[00:41:01.400 --> 00:41:04.160]   and private equity as their career of choice.
[00:41:04.160 --> 00:41:06.720]   I think that's a sign that finance is too big.
[00:41:06.720 --> 00:41:10.320]   When it's sucking up too much of the talent of the country
[00:41:10.320 --> 00:41:14.720]   and when the rewards for doing it are so disproportionate
[00:41:14.720 --> 00:41:18.400]   relative to the rewards of doing other things.
[00:41:18.400 --> 00:41:20.280]   The counter to that is that they've also done
[00:41:20.280 --> 00:41:22.520]   a lot of rewards for starting businesses.
[00:41:22.520 --> 00:41:25.640]   And that's probably, I think how you want it to be
[00:41:25.640 --> 00:41:27.840]   in a productive economy.
[00:41:27.840 --> 00:41:29.960]   So I think the number is too high.
[00:41:29.960 --> 00:41:31.680]   I don't know how to think about what it should be
[00:41:31.680 --> 00:41:34.160]   other than what actually a former Goldman Sachs partner
[00:41:34.160 --> 00:41:35.560]   said this to me when I was working
[00:41:36.040 --> 00:41:37.040]   on "All The Devils Are Here."
[00:41:37.040 --> 00:41:39.800]   And she said that finance is supposed to be
[00:41:39.800 --> 00:41:41.920]   like the substrata of our world.
[00:41:41.920 --> 00:41:43.600]   It's supposed to be the thing that enables
[00:41:43.600 --> 00:41:44.600]   other things to happen.
[00:41:44.600 --> 00:41:46.720]   It's not supposed to be the world itself.
[00:41:46.720 --> 00:41:51.080]   So the role of a financial system is to enable businesses
[00:41:51.080 --> 00:41:52.880]   to get started, to provide capital.
[00:41:52.880 --> 00:41:54.040]   That's what it's supposed to be.
[00:41:54.040 --> 00:41:56.000]   The lubricant that enables business.
[00:41:56.000 --> 00:41:58.160]   But it's not supposed to be the thing itself.
[00:41:58.160 --> 00:41:59.920]   And when it's become the thing itself,
[00:41:59.920 --> 00:42:02.360]   you've got a problem.
[00:42:03.720 --> 00:42:05.200]   - And I think the other--
[00:42:05.200 --> 00:42:06.880]   - There's your article about crypto.
[00:42:06.880 --> 00:42:08.160]   That paragraph right there.
[00:42:08.160 --> 00:42:10.280]   - There you go, that's a good.
[00:42:10.280 --> 00:42:12.760]   And I think the other way you can say it,
[00:42:12.760 --> 00:42:14.240]   perhaps this is way too simplistic,
[00:42:14.240 --> 00:42:15.840]   but the other way I've thought about it
[00:42:15.840 --> 00:42:19.400]   is that how can it be, if you can run a hedge fund
[00:42:19.400 --> 00:42:21.920]   and make billions of dollars from,
[00:42:21.920 --> 00:42:26.000]   and have five people, 10 people, whatever it is,
[00:42:26.000 --> 00:42:29.680]   versus starting a company that employs people
[00:42:29.680 --> 00:42:31.880]   and changes the neighborhood and provides jobs
[00:42:31.880 --> 00:42:36.720]   and provide the product that improves people's lives.
[00:42:36.720 --> 00:42:40.720]   It is a shame that too much of the talent
[00:42:40.720 --> 00:42:43.120]   and such a huge share of the financial rewards
[00:42:43.120 --> 00:42:45.120]   are going to the former rather than the latter.
[00:42:45.120 --> 00:42:47.960]   And that just can't mean good things for the future.
[00:42:47.960 --> 00:42:49.000]   - Yeah, yeah.
[00:42:49.000 --> 00:42:52.080]   And when people criticize technology, for example,
[00:42:52.080 --> 00:42:54.800]   for the idea that these people who would have been,
[00:42:54.800 --> 00:42:56.920]   I don't know, otherwise teachers or something,
[00:42:56.920 --> 00:43:00.000]   they're making half a million dollars at Google.
[00:43:00.000 --> 00:43:01.840]   And I think like when I was in India,
[00:43:01.840 --> 00:43:03.080]   people were using Google Maps
[00:43:03.080 --> 00:43:04.640]   to get through the streets in Mumbai,
[00:43:04.640 --> 00:43:06.600]   which is unimaginable to me before going there
[00:43:06.600 --> 00:43:07.880]   that you would be able to do that
[00:43:07.880 --> 00:43:10.600]   with a service built out of Silicon Valley.
[00:43:10.600 --> 00:43:13.480]   And so, yeah, I think that actually is a good allocation
[00:43:13.480 --> 00:43:14.440]   of capital and talent.
[00:43:14.440 --> 00:43:16.800]   I'm not sure about finance.
[00:43:16.800 --> 00:43:18.320]   - I think I agree with you.
[00:43:18.320 --> 00:43:20.480]   I think there are other problems with Google
[00:43:20.480 --> 00:43:22.680]   and with the social media giants,
[00:43:22.680 --> 00:43:25.840]   but they are real businesses that employ people
[00:43:25.840 --> 00:43:29.440]   that make products that have had a huge impact
[00:43:29.440 --> 00:43:31.960]   on people's lives.
[00:43:31.960 --> 00:43:35.120]   So in that sense, it's very different
[00:43:35.120 --> 00:43:37.160]   than a private equity firm, for instance,
[00:43:37.160 --> 00:43:38.600]   and especially private equity,
[00:43:38.600 --> 00:43:41.520]   even more so than hedge funds draws my ire
[00:43:41.520 --> 00:43:43.080]   because I think one of the reasons
[00:43:43.080 --> 00:43:46.040]   that they've been able to make,
[00:43:46.040 --> 00:43:47.880]   part of the financialization of our economy
[00:43:47.880 --> 00:43:50.680]   has been due to super, super low interest rates
[00:43:50.680 --> 00:43:53.680]   and low interest rates that have enabled so many people
[00:43:53.680 --> 00:43:56.400]   to make so much money in finance are not,
[00:43:56.400 --> 00:43:57.240]   they're just a gap.
[00:43:57.240 --> 00:43:59.440]   It wasn't because these people were uniquely smart.
[00:43:59.440 --> 00:44:02.680]   They just found themselves in a great moment in time
[00:44:02.680 --> 00:44:04.600]   and the fact that they now think they're really smart
[00:44:04.600 --> 00:44:06.840]   because it makes me crazy.
[00:44:06.840 --> 00:44:12.280]   - Are Fannie and Freddie America special purpose entities?
[00:44:12.280 --> 00:44:13.480]   Are they our Alameda?
[00:44:13.480 --> 00:44:16.040]   It's just a way we hide our debt and-
[00:44:16.040 --> 00:44:16.880]   - Interesting.
[00:44:16.880 --> 00:44:17.760]   - Yeah.
[00:44:17.760 --> 00:44:20.080]   - Well, I guess we, you know what?
[00:44:20.080 --> 00:44:22.160]   I don't know anymore because,
[00:44:22.160 --> 00:44:24.840]   so I last wrote about them, when was it?
[00:44:24.840 --> 00:44:27.280]   In 2016 and I don't know now.
[00:44:27.280 --> 00:44:28.120]   No, you're right.
[00:44:28.120 --> 00:44:31.080]   Their debt is still off balance sheet.
[00:44:31.080 --> 00:44:34.240]   So yeah, in a lot of ways they were,
[00:44:34.240 --> 00:44:36.440]   I would argue though that the old Fannie and Freddie
[00:44:36.440 --> 00:44:39.640]   were structured more honestly than the new Fannie and Freddie
[00:44:39.640 --> 00:44:43.080]   that it really is conservatorship that has made them,
[00:44:43.080 --> 00:44:45.600]   that have made them America's off balance sheet entities
[00:44:45.600 --> 00:44:46.600]   because at least when they were
[00:44:46.600 --> 00:44:49.040]   their own independent entities,
[00:44:49.040 --> 00:44:51.320]   yes, there was this odd thing known
[00:44:51.320 --> 00:44:53.280]   as the implicit guarantee, which is,
[00:44:53.280 --> 00:44:54.760]   when you think about, back to your point
[00:44:54.760 --> 00:44:57.360]   about efficient markets, how can you possibly believe
[00:44:57.360 --> 00:44:59.200]   there's such a thing as an efficient market
[00:44:59.200 --> 00:45:02.720]   when Fannie and Freddie had an implicit guarantee,
[00:45:02.720 --> 00:45:04.240]   meaning it wasn't real.
[00:45:04.240 --> 00:45:06.080]   There was no place where it was written down
[00:45:06.080 --> 00:45:08.000]   that the U.S. government would bail Fannie and Freddie
[00:45:08.000 --> 00:45:11.080]   out in a crisis and everybody denied that it existed.
[00:45:11.080 --> 00:45:12.280]   And yet it did exist.
[00:45:12.280 --> 00:45:15.200]   - Yeah, no, but I feel like that confirms
[00:45:15.200 --> 00:45:16.280]   the efficient market hypothesis, right?
[00:45:16.280 --> 00:45:19.240]   Because the market correctly thought that mortgages
[00:45:19.240 --> 00:45:22.280]   backed by Fannie and Freddie would have government's
[00:45:22.280 --> 00:45:24.120]   a father and they did.
[00:45:24.120 --> 00:45:24.960]   - You might be right.
[00:45:24.960 --> 00:45:27.680]   I think what I was getting at, you might be right.
[00:45:27.680 --> 00:45:29.560]   I think what I was getting at is that it is such
[00:45:29.560 --> 00:45:30.800]   a screwed up concept.
[00:45:30.800 --> 00:45:33.520]   I mean, how could it possibly, when I first,
[00:45:33.520 --> 00:45:35.200]   when people were first explaining this to me
[00:45:35.200 --> 00:45:36.840]   when I first wrote about Fannie and Freddie,
[00:45:36.840 --> 00:45:40.000]   I was like, no, wait, this is American corporate.
[00:45:40.000 --> 00:45:41.080]   No, when, what?
[00:45:41.080 --> 00:45:47.400]   So yeah, but I think that Fannie and Freddie,
[00:45:47.400 --> 00:45:49.520]   at least with shareholders that were forced
[00:45:49.520 --> 00:45:51.840]   to bear some level of the risks
[00:45:51.840 --> 00:45:53.800]   were actually a more honest way of going about
[00:45:53.800 --> 00:45:57.440]   this whole screwed up American way of financing mortgages
[00:45:57.440 --> 00:45:59.720]   than the current setup is.
[00:45:59.720 --> 00:46:00.840]   - What is the future of these firms?
[00:46:00.840 --> 00:46:03.680]   Are they just gonna stay in conservatorship forever?
[00:46:03.680 --> 00:46:05.800]   Or is there any developments there?
[00:46:05.800 --> 00:46:07.240]   What's gonna happen to them?
[00:46:07.240 --> 00:46:09.880]   - The lawsuit, the latest lawsuit that could have answered
[00:46:09.880 --> 00:46:12.640]   that in some ways ended in a mistrial.
[00:46:12.640 --> 00:46:16.640]   I don't think, I don't think, unfortunately,
[00:46:16.640 --> 00:46:19.920]   anybody in government sees any currency
[00:46:19.920 --> 00:46:21.920]   and I mean currency in the broad sense,
[00:46:21.920 --> 00:46:25.080]   not in the literal sense of money, in taking this on.
[00:46:25.080 --> 00:46:28.040]   And unfortunately, what someone once said to me about it,
[00:46:28.040 --> 00:46:31.320]   I think remains true and it's really depressing,
[00:46:31.320 --> 00:46:33.640]   but is that various lawmakers get interested
[00:46:33.640 --> 00:46:36.160]   in Fannie and Freddie, they engage with that
[00:46:36.160 --> 00:46:40.080]   only to figure out it's really, really goddamn complicated.
[00:46:40.080 --> 00:46:43.800]   And that any kind of solution is gonna involve
[00:46:43.800 --> 00:46:46.640]   angering people on one side of the aisle or another
[00:46:46.640 --> 00:46:49.600]   and potentially angering their constituents
[00:46:49.600 --> 00:46:53.160]   and they slowly back away from doing anything
[00:46:53.160 --> 00:46:55.520]   that could affect change.
[00:46:55.520 --> 00:46:58.160]   But I think we have a really unhealthy situation.
[00:46:58.160 --> 00:47:00.240]   I don't think it's great for these two entities
[00:47:00.240 --> 00:47:03.400]   to be in conservatorship, but at this point,
[00:47:03.400 --> 00:47:05.200]   I'm not sure it's gonna change.
[00:47:05.200 --> 00:47:08.480]   - Yeah, speaking of debt and mortgages,
[00:47:08.480 --> 00:47:10.960]   so total household debt in the United States
[00:47:10.960 --> 00:47:14.640]   has been climbing recently after it's like slightly declined
[00:47:14.640 --> 00:47:17.840]   after 2008, but I think in quarter three alone,
[00:47:17.840 --> 00:47:22.840]   it increased $350 billion and now it's at $16.5 trillion,
[00:47:22.840 --> 00:47:24.560]   the total US household debt.
[00:47:24.560 --> 00:47:25.560]   Should we be worried about this?
[00:47:25.560 --> 00:47:27.440]   Are we gonna see another sort of collapse
[00:47:27.440 --> 00:47:31.000]   because of this or what should we think about this number?
[00:47:31.000 --> 00:47:33.600]   - I don't know how to think about that
[00:47:33.600 --> 00:47:36.920]   because it's too tied up in other things that no one knows.
[00:47:36.920 --> 00:47:38.320]   Are we going to have a recession?
[00:47:38.320 --> 00:47:40.680]   How severe is the recession going to be?
[00:47:40.680 --> 00:47:42.840]   What is the max unemployment rate
[00:47:42.840 --> 00:47:46.320]   that we're gonna shit if we do have a recession?
[00:47:46.320 --> 00:47:48.440]   And all of those things dictate
[00:47:48.440 --> 00:47:50.920]   how to think about that number.
[00:47:50.920 --> 00:47:53.840]   I don't think consumer debt is embedded
[00:47:53.840 --> 00:47:55.560]   in the bowels of the financial system
[00:47:55.560 --> 00:47:57.520]   in the same way mortgages were.
[00:47:57.520 --> 00:48:02.520]   And in the end, the problem with the financial crisis 2008,
[00:48:02.520 --> 00:48:05.200]   it wasn't the losses on the mortgages themselves.
[00:48:05.200 --> 00:48:07.840]   It was the way in which they were embedded
[00:48:07.840 --> 00:48:09.520]   in the plumbing of the financial system
[00:48:09.520 --> 00:48:11.720]   in ways that nobody understood.
[00:48:11.720 --> 00:48:14.040]   And then the resulting loss of confidence
[00:48:14.040 --> 00:48:16.520]   from the fact that nobody had understood that
[00:48:16.520 --> 00:48:19.640]   slash lies had been told about that.
[00:48:19.640 --> 00:48:24.200]   And that's what caused everything to collapse.
[00:48:24.200 --> 00:48:26.880]   Consumer debt is a little more visible and seeable.
[00:48:26.880 --> 00:48:31.880]   And I don't think that it has that same opaque quality to it
[00:48:31.880 --> 00:48:34.400]   that mortgage backed securities did.
[00:48:34.400 --> 00:48:35.400]   I could be wrong.
[00:48:35.400 --> 00:48:36.240]   I could be wrong.
[00:48:36.240 --> 00:48:41.240]   I haven't dug into it enough to understand that.
[00:48:41.560 --> 00:48:44.600]   But you can see the delinquencies starting to decline.
[00:48:44.600 --> 00:48:47.960]   I mean, I guess you could on mortgages as well.
[00:48:47.960 --> 00:48:51.960]   But there was this profound belief with mortgages
[00:48:51.960 --> 00:48:54.360]   that since home prices would never decline,
[00:48:54.360 --> 00:48:56.560]   there would never be losses on these instruments
[00:48:56.560 --> 00:48:58.840]   because you could always sell the underlying property
[00:48:58.840 --> 00:49:00.400]   for more than you had paid for it.
[00:49:00.400 --> 00:49:02.720]   And therefore everything would be fine.
[00:49:02.720 --> 00:49:04.880]   And that's what led to a lot of the bad practices
[00:49:04.880 --> 00:49:06.560]   in the industry is that lenders didn't think
[00:49:06.560 --> 00:49:08.680]   they had to care if they were screwing the home buyer
[00:49:08.680 --> 00:49:10.600]   because they always thought they could take the home back
[00:49:10.600 --> 00:49:13.440]   and make more money on it.
[00:49:13.440 --> 00:49:15.760]   And consumer debt is unsecured.
[00:49:15.760 --> 00:49:17.400]   And so it's different.
[00:49:17.400 --> 00:49:19.480]   I think people think about it differently.
[00:49:19.480 --> 00:49:24.160]   But I'd have to do some more homework to understand
[00:49:24.160 --> 00:49:26.840]   where consumer debt sits in the overall architecture
[00:49:26.840 --> 00:49:29.000]   of the financial industry.
[00:49:29.000 --> 00:49:31.000]   - I'm really glad you brought up this theme
[00:49:31.000 --> 00:49:33.680]   about what does the overall big picture look like?
[00:49:33.680 --> 00:49:36.520]   I feel like this is the theme of all your books
[00:49:36.520 --> 00:49:40.480]   that people will be so obsessed with their subsection
[00:49:40.480 --> 00:49:43.080]   of their job or their area that they won't notice
[00:49:43.080 --> 00:49:48.080]   that broader trends like the ones we're talking about.
[00:49:48.080 --> 00:49:50.160]   And in Enron, it's like,
[00:49:50.160 --> 00:49:52.120]   why do we have all these special purpose entities?
[00:49:52.120 --> 00:49:54.320]   What is the total debt load of Enron?
[00:49:54.320 --> 00:49:55.880]   Or with the mortgage-backed securities
[00:49:55.880 --> 00:49:58.200]   and similar kind of thing, right?
[00:49:58.200 --> 00:49:59.880]   Maybe they weren't correlated in the past,
[00:49:59.880 --> 00:50:01.000]   but what's that?
[00:50:01.000 --> 00:50:04.240]   Do we really think that there's really no correlation
[00:50:04.240 --> 00:50:07.000]   between delinquencies across the country?
[00:50:07.000 --> 00:50:08.920]   So that kind of big picture thinking,
[00:50:08.920 --> 00:50:10.320]   whose job is that today?
[00:50:10.320 --> 00:50:11.520]   Is it journalists?
[00:50:11.520 --> 00:50:12.760]   Is it short sellers?
[00:50:12.760 --> 00:50:14.520]   Is that people writing on Substack?
[00:50:14.520 --> 00:50:15.640]   Who is doing that?
[00:50:15.640 --> 00:50:16.960]   Is it anybody's job?
[00:50:16.960 --> 00:50:19.200]   Is it just like an important role
[00:50:19.200 --> 00:50:21.040]   with nobody assigned to it?
[00:50:21.040 --> 00:50:22.080]   - I think it's the latter.
[00:50:22.080 --> 00:50:23.200]   I think it's an important role
[00:50:23.200 --> 00:50:25.520]   with nobody assigned to it.
[00:50:25.520 --> 00:50:27.640]   And there is a limit.
[00:50:27.640 --> 00:50:29.680]   I mean, I hate to say this.
[00:50:29.680 --> 00:50:33.040]   It is not an accident that many of my books
[00:50:33.040 --> 00:50:33.880]   have been written.
[00:50:33.880 --> 00:50:35.000]   That's probably not fair.
[00:50:35.000 --> 00:50:36.680]   It's not true of my book on fracking,
[00:50:36.680 --> 00:50:38.840]   but that some of my books have been written
[00:50:38.840 --> 00:50:40.280]   after the calamity happened.
[00:50:40.280 --> 00:50:42.280]   So they weren't so much foretelling the calamity
[00:50:42.280 --> 00:50:44.800]   as they were unpacking the calamity after it happened,
[00:50:44.800 --> 00:50:46.360]   which is a different role.
[00:50:46.360 --> 00:50:48.440]   And as I said, at the start of our conversation,
[00:50:48.440 --> 00:50:51.120]   I think an important one to explain to people
[00:50:51.120 --> 00:50:53.680]   why this big, bad thing took place,
[00:50:53.680 --> 00:50:54.800]   but it's not prediction.
[00:50:54.800 --> 00:50:58.400]   I don't know as people that we're very good at prediction.
[00:50:58.400 --> 00:51:01.840]   They tried to set up, what was it called?
[00:51:01.840 --> 00:51:04.080]   In the wake of the global financial crisis,
[00:51:04.080 --> 00:51:06.240]   they established this thing called FSOC.
[00:51:06.240 --> 00:51:08.360]   And now I'm forgetting what the acronym stands for.
[00:51:08.360 --> 00:51:11.240]   Financial Security Oversight Committee.
[00:51:11.240 --> 00:51:13.400]   And it's supposed to be this body
[00:51:13.400 --> 00:51:16.200]   that does think about these big picture risks,
[00:51:16.200 --> 00:51:19.360]   that thinks about the ways, for example,
[00:51:19.360 --> 00:51:21.320]   in which mortgage-backed securities
[00:51:21.320 --> 00:51:26.320]   were repopulating through the entire financial system
[00:51:26.320 --> 00:51:29.240]   in ways that would cause a loss
[00:51:29.240 --> 00:51:30.600]   to be much more than a loss,
[00:51:30.600 --> 00:51:33.360]   that it would just be the loss of money and that security.
[00:51:33.360 --> 00:51:35.240]   It would echo and magnify.
[00:51:35.240 --> 00:51:36.400]   And so there are people
[00:51:36.400 --> 00:51:38.880]   who are supposed to be thinking about it,
[00:51:38.880 --> 00:51:42.000]   but I think it's really hard to see that.
[00:51:42.000 --> 00:51:45.840]   And in an increasingly complex world,
[00:51:45.840 --> 00:51:48.200]   it's even harder than it was before
[00:51:48.200 --> 00:51:50.560]   because the reverberations from things
[00:51:50.560 --> 00:51:54.800]   are really hard to map out in advance.
[00:51:54.800 --> 00:51:57.640]   And especially when some part of those reverberations
[00:51:57.640 --> 00:52:00.520]   are a loss of confidence, then all bets are off
[00:52:00.520 --> 00:52:04.240]   because when confidence cracks, lots of things fall apart.
[00:52:04.240 --> 00:52:08.600]   But how do you possibly analyze in any quantitative way
[00:52:08.600 --> 00:52:11.080]   the risk that confidence will collapse?
[00:52:11.080 --> 00:52:16.160]   So I think it's difficult that said,
[00:52:16.160 --> 00:52:18.320]   and of course I am talking my own book here.
[00:52:18.320 --> 00:52:20.880]   I don't think that the lack of,
[00:52:20.880 --> 00:52:23.800]   the increased financial problems of journalism
[00:52:23.800 --> 00:52:25.640]   really help matters in that respect
[00:52:25.640 --> 00:52:27.560]   because in an ideal world,
[00:52:27.560 --> 00:52:29.760]   you want a lot of people out there
[00:52:29.760 --> 00:52:32.160]   writing and thinking about various pieces of this,
[00:52:32.160 --> 00:52:34.120]   and then maybe somebody can come along
[00:52:34.120 --> 00:52:35.880]   and see the various pieces and say,
[00:52:35.880 --> 00:52:38.040]   "Oh my God, there's this big picture thing here
[00:52:38.040 --> 00:52:40.000]   that we all need to be thinking about."
[00:52:40.000 --> 00:52:41.760]   But there's a kind of serendipity
[00:52:41.760 --> 00:52:43.440]   in the ability to do that.
[00:52:43.440 --> 00:52:46.160]   One that the chances, I guess the best way to say that
[00:52:46.160 --> 00:52:48.000]   is the chances of that serendipity
[00:52:48.000 --> 00:52:49.280]   are dramatically increased
[00:52:49.280 --> 00:52:52.040]   by having a lot of people out there doing homework
[00:52:52.040 --> 00:52:53.760]   on the various pieces of the puzzle.
[00:52:53.760 --> 00:52:56.080]   And so I think in a world particularly
[00:52:56.080 --> 00:52:59.440]   where local news has been decimated,
[00:52:59.440 --> 00:53:01.400]   the chances of that sort of serendipity
[00:53:01.400 --> 00:53:03.000]   are definitely lower.
[00:53:03.000 --> 00:53:04.720]   And people may think, "Oh, it doesn't matter.
[00:53:04.720 --> 00:53:05.880]   We still got national news.
[00:53:05.880 --> 00:53:06.960]   We've got the Washington Post.
[00:53:06.960 --> 00:53:08.000]   We've got the Wall Street Journal.
[00:53:08.000 --> 00:53:09.840]   We've got the New York Times."
[00:53:09.840 --> 00:53:12.160]   I would love to have somebody do a piece of analysis
[00:53:12.160 --> 00:53:14.200]   and go back through the New York Times stories
[00:53:14.200 --> 00:53:18.280]   and see how many were sparked by a piece in a local paper
[00:53:18.280 --> 00:53:19.520]   that maybe you wouldn't even notice
[00:53:19.520 --> 00:53:20.840]   from reading the New York Times piece
[00:53:20.840 --> 00:53:22.720]   because it'd be in like the sixth paragraph
[00:53:22.720 --> 00:53:24.720]   that, "Oh yeah, credit should go to this person
[00:53:24.720 --> 00:53:27.120]   at this local paper who started writing about this."
[00:53:27.120 --> 00:53:29.320]   But if you no longer have the person at the local paper
[00:53:29.320 --> 00:53:31.480]   who started writing about this,
[00:53:31.480 --> 00:53:34.040]   you know, it's less likely
[00:53:34.040 --> 00:53:35.640]   that the big national piece gets written.
[00:53:35.640 --> 00:53:38.120]   And I think that's a part of the implosion of local news
[00:53:38.120 --> 00:53:40.560]   that people, a part of the cost of the implosion
[00:53:40.560 --> 00:53:42.960]   of local news that people don't really understand.
[00:53:42.960 --> 00:53:44.880]   The idea that the national press functions
[00:53:44.880 --> 00:53:49.520]   at the same level without local news is just not true.
[00:53:49.520 --> 00:53:52.120]   - Yeah, but even if you have the local news,
[00:53:52.120 --> 00:53:53.520]   and that's a really important point,
[00:53:53.520 --> 00:53:55.360]   but even if you have that local news,
[00:53:55.360 --> 00:53:57.280]   there still has to be somebody whose job it is
[00:53:57.280 --> 00:53:59.120]   to synthesize it all together.
[00:53:59.160 --> 00:54:02.440]   And I'm curious, what is the training that requires?
[00:54:02.440 --> 00:54:04.320]   So, I mean, your training is, you know,
[00:54:04.320 --> 00:54:05.480]   math and English major,
[00:54:05.480 --> 00:54:09.640]   and then working in investment banking.
[00:54:09.640 --> 00:54:13.240]   Is that the, I mean, obviously the anecdotal experience
[00:54:13.240 --> 00:54:15.040]   N equals one seems like that's great training
[00:54:15.040 --> 00:54:16.720]   for synthesizing all these pieces together,
[00:54:16.720 --> 00:54:19.360]   but what is the right sort of education
[00:54:19.360 --> 00:54:22.280]   for somebody who is thinking about the big picture?
[00:54:22.280 --> 00:54:23.560]   - I don't know.
[00:54:23.560 --> 00:54:25.560]   And there may be,
[00:54:25.560 --> 00:54:28.280]   there are probably multiple answers to that question, right?
[00:54:28.280 --> 00:54:31.200]   There's probably no one right answer.
[00:54:31.200 --> 00:54:35.680]   For me, in the end, my math major has proven to be pivotal.
[00:54:35.680 --> 00:54:39.640]   Even my mother dug up these, my parents were moving,
[00:54:39.640 --> 00:54:41.440]   and so my mother was going through all their stuff,
[00:54:41.440 --> 00:54:45.080]   and she dug up these, my math work from college.
[00:54:45.080 --> 00:54:46.720]   And literally, if it weren't for the fact
[00:54:46.720 --> 00:54:48.280]   that I recognized my own handwriting,
[00:54:48.280 --> 00:54:52.600]   I would not recognize pages of math formulas and proofs,
[00:54:52.600 --> 00:54:54.120]   and they're like, get gibberish to me now.
[00:54:54.120 --> 00:54:56.520]   So, but I still think that math has,
[00:54:56.520 --> 00:54:59.360]   so I do not want to exaggerate my mathematical ability
[00:54:59.360 --> 00:55:01.520]   at this stage of the game, it's basically no.
[00:55:01.520 --> 00:55:03.400]   But I do think that doing math proofs,
[00:55:03.400 --> 00:55:05.800]   any kind of formal, any kind of training in logic
[00:55:05.800 --> 00:55:07.640]   is really, really important,
[00:55:07.640 --> 00:55:10.240]   because the more you've been formally trained in logic,
[00:55:10.240 --> 00:55:12.360]   the more you've realized when there are pieces missing,
[00:55:12.360 --> 00:55:15.560]   and when something isn't quite adding up.
[00:55:15.560 --> 00:55:18.320]   It just forces you to think in a way that is,
[00:55:18.320 --> 00:55:20.120]   that in a way that connects the dots.
[00:55:20.120 --> 00:55:23.960]   Because you know, if you're moving from A to B,
[00:55:23.960 --> 00:55:25.920]   and B doesn't follow A,
[00:55:25.920 --> 00:55:28.040]   you understand that B doesn't follow A.
[00:55:28.040 --> 00:55:30.400]   And I think that that kind of training
[00:55:30.400 --> 00:55:32.560]   is really, really important.
[00:55:32.560 --> 00:55:34.000]   It's what's given me,
[00:55:34.000 --> 00:55:36.120]   whatever kind of backbone I have as a journalist
[00:55:36.120 --> 00:55:38.920]   is not because I like to create controversy
[00:55:38.920 --> 00:55:40.840]   and like to make people mad, I actually don't.
[00:55:40.840 --> 00:55:43.280]   It's just because something doesn't make sense to me.
[00:55:43.280 --> 00:55:45.480]   And so, it doesn't make sense to me
[00:55:45.480 --> 00:55:46.720]   because I'm not getting it,
[00:55:46.720 --> 00:55:48.240]   or it doesn't make sense to me
[00:55:48.240 --> 00:55:50.720]   because B doesn't actually follow A,
[00:55:50.720 --> 00:55:52.560]   and you're just being told that it does.
[00:55:52.560 --> 00:55:56.560]   And so, I think that training is really, really important.
[00:55:56.560 --> 00:56:01.560]   I also have often thought that another part of training
[00:56:01.560 --> 00:56:03.280]   is realizing that basic rule
[00:56:03.280 --> 00:56:04.520]   that you learned in kindergarten,
[00:56:04.520 --> 00:56:07.680]   which is, you know, believe your imagination
[00:56:07.680 --> 00:56:09.960]   or follow your imagination
[00:56:09.960 --> 00:56:12.160]   because the truth is anything can happen.
[00:56:12.160 --> 00:56:13.920]   And I think if you look at business history
[00:56:13.920 --> 00:56:15.880]   over the last couple of decades,
[00:56:15.880 --> 00:56:18.880]   it will be the improbable becoming probable
[00:56:18.880 --> 00:56:22.480]   becoming truth over and over and over again.
[00:56:22.480 --> 00:56:25.200]   I mean, the idea that Enron could implode,
[00:56:25.200 --> 00:56:27.680]   one of the biggest, supposedly most successful companies
[00:56:27.680 --> 00:56:31.560]   in corporate America could be bank within six months,
[00:56:31.560 --> 00:56:35.200]   the year from its stock price high.
[00:56:35.200 --> 00:56:38.480]   The idea that the biggest, most successful
[00:56:38.480 --> 00:56:41.280]   financial institutions on Wall Street
[00:56:41.280 --> 00:56:42.920]   could all be crumbling into bankruptcy
[00:56:42.920 --> 00:56:45.160]   without the aid of the US government.
[00:56:45.160 --> 00:56:48.560]   The idea that a young woman with no college degree
[00:56:48.560 --> 00:56:50.480]   and no real experience in engineering
[00:56:50.480 --> 00:56:54.440]   could create a machine
[00:56:54.440 --> 00:56:56.760]   that was going to revolutionize blood testing
[00:56:56.760 --> 00:56:59.520]   and land on the cover of every business magazine.
[00:56:59.520 --> 00:57:01.240]   And that this whole thing could turn out
[00:57:01.240 --> 00:57:03.080]   to be pretty much a fraud.
[00:57:03.080 --> 00:57:04.960]   The entire idea of FTX.
[00:57:04.960 --> 00:57:07.400]   I mean, over and over again, these things have happened.
[00:57:07.400 --> 00:57:08.640]   Forget Bernie Madoff.
[00:57:08.640 --> 00:57:09.800]   If you told people a year ago
[00:57:09.800 --> 00:57:12.280]   that FTX was gonna implode six months ago,
[00:57:12.280 --> 00:57:13.920]   three months ago, people would have been like,
[00:57:13.920 --> 00:57:15.640]   no, no, no, no, no, no, no.
[00:57:15.640 --> 00:57:18.440]   And so I think just that knowledge
[00:57:18.440 --> 00:57:20.720]   that the improbable happens over and over again
[00:57:20.720 --> 00:57:23.640]   is also a really fundamentally important.
[00:57:23.640 --> 00:57:26.800]   - If we're continuing on the theme of FTX,
[00:57:26.800 --> 00:57:30.000]   I interviewed him about four or five months ago.
[00:57:30.000 --> 00:57:30.840]   - Wow.
[00:57:30.840 --> 00:57:33.840]   - And this is one of these interviews that I'm really,
[00:57:33.840 --> 00:57:35.160]   I don't know if embarrassed is the right word,
[00:57:35.160 --> 00:57:38.760]   but I knew things then that I could have like asked
[00:57:38.760 --> 00:57:41.120]   Poe Carter about, but it's also the kind of thing
[00:57:41.120 --> 00:57:43.200]   where you look back in retrospect and you're like,
[00:57:43.200 --> 00:57:44.480]   if it had turned out well,
[00:57:44.480 --> 00:57:48.360]   it's not obvious what the red flags are
[00:57:48.360 --> 00:57:50.240]   while you're in the moment.
[00:57:50.240 --> 00:57:52.240]   There's things you can look back at the story of Facebook
[00:57:52.240 --> 00:57:54.680]   and how, you know, Mark Zuckerberg acted
[00:57:54.680 --> 00:57:55.680]   in the early days of Facebook.
[00:57:55.680 --> 00:57:57.360]   And you could say, if the thing fell apart,
[00:57:57.360 --> 00:57:59.200]   that this is why, or, you know, this is a red flag.
[00:57:59.200 --> 00:58:00.920]   So I had a hard time thinking about
[00:58:00.920 --> 00:58:01.960]   how I should have done that interview.
[00:58:01.960 --> 00:58:04.160]   - Cliches are often cliches for a reason.
[00:58:04.160 --> 00:58:06.800]   And the one about hindsight being 2020
[00:58:06.800 --> 00:58:09.920]   is one of the best cliches ever.
[00:58:09.920 --> 00:58:13.400]   And I am so fundamentally annoyed by stories that say,
[00:58:13.400 --> 00:58:16.320]   here were all the red flags, why didn't anybody see it?
[00:58:16.320 --> 00:58:18.840]   Well, oftentimes the person writing that story
[00:58:18.840 --> 00:58:20.480]   didn't see the red flags either.
[00:58:20.480 --> 00:58:23.960]   And it's really easy in retrospect to pick up all the signs
[00:58:23.960 --> 00:58:25.760]   that there was a problem.
[00:58:25.760 --> 00:58:28.280]   And it's really hard in the moment.
[00:58:28.280 --> 00:58:32.040]   There's a little bit of, I think in all of us,
[00:58:32.040 --> 00:58:34.960]   a subconscious sense that this doesn't sound quite right,
[00:58:34.960 --> 00:58:37.240]   but getting the subconscious suspicion to rise
[00:58:37.240 --> 00:58:41.520]   to the level of conscious thought is also really difficult.
[00:58:41.520 --> 00:58:46.080]   So I think, again, it is one value of,
[00:58:46.080 --> 00:58:47.440]   and I hope we're coming back to a world
[00:58:47.440 --> 00:58:49.880]   that appreciates old people because I'm getting older,
[00:58:49.880 --> 00:58:53.440]   but there is some value in having seen it before
[00:58:53.440 --> 00:58:56.640]   that I think the red flags do maybe tend to rise
[00:58:56.640 --> 00:58:59.200]   to the level of conscious thought.
[00:58:59.200 --> 00:59:02.080]   That said, if I had gotten interested in crypto a year ago,
[00:59:02.080 --> 00:59:05.560]   would I have seen the problems with FTX?
[00:59:05.560 --> 00:59:07.200]   Doubtful, I don't know.
[00:59:07.200 --> 00:59:09.040]   You just can never know.
[00:59:09.040 --> 00:59:10.840]   Sometimes these things also depend on the way
[00:59:10.840 --> 00:59:13.720]   in which they're presented to you and by whom.
[00:59:13.720 --> 00:59:15.240]   And I think that shouldn't be,
[00:59:15.240 --> 00:59:17.520]   I think it's not intellectually honest,
[00:59:17.520 --> 00:59:19.920]   but if somebody you really respect comes to you
[00:59:19.920 --> 00:59:23.040]   and tells you this business is the next greatest thing
[00:59:23.040 --> 00:59:24.560]   and this person is brilliant,
[00:59:24.560 --> 00:59:26.840]   chances are that preconception is gonna be lodged
[00:59:26.840 --> 00:59:29.480]   in your mind in a way that's gonna make it really hard
[00:59:29.480 --> 00:59:31.480]   for you to see the red flags.
[00:59:31.480 --> 00:59:33.440]   Whereas the first you've heard of this company
[00:59:33.440 --> 00:59:34.560]   with somebody coming to you,
[00:59:34.560 --> 00:59:37.480]   somebody really smart you like coming to you and saying,
[00:59:37.480 --> 00:59:39.640]   I don't think this makes sense.
[00:59:39.640 --> 00:59:41.200]   These are my problems with this.
[00:59:41.200 --> 00:59:43.480]   You're gonna be far more apt to see the red flags.
[00:59:43.480 --> 00:59:45.480]   And I say it's not, that's not the right way,
[00:59:45.480 --> 00:59:46.680]   not the right phrase.
[00:59:46.680 --> 00:59:48.800]   I say it's not intellectually honest or not smart
[00:59:48.800 --> 00:59:51.080]   because really you kind of want to strip that away.
[00:59:51.080 --> 00:59:52.680]   You want to strip those biases away
[00:59:52.680 --> 00:59:56.200]   because even really smart people make mistakes all the time.
[00:59:56.200 --> 00:59:59.200]   And so you want to the extent possible to think for yourself,
[00:59:59.200 --> 01:00:02.640]   but of course it's, it goes back to math, right?
[01:00:02.640 --> 01:00:04.360]   Order of operations.
[01:00:04.360 --> 01:00:07.280]   And so the order in which information is presented to you
[01:00:07.280 --> 01:00:10.200]   unfortunately is gonna have an effect on how you see that,
[01:00:10.200 --> 01:00:12.040]   how you see that information.
[01:00:13.320 --> 01:00:14.440]   - Yeah.
[01:00:14.440 --> 01:00:15.600]   Let's talk about fracking.
[01:00:15.600 --> 01:00:19.920]   So in this year, I think quarter two,
[01:00:19.920 --> 01:00:21.800]   was it the first time that fracking
[01:00:21.800 --> 01:00:24.600]   and shale finally became profitable?
[01:00:24.600 --> 01:00:28.320]   Were the fracking investors right
[01:00:28.320 --> 01:00:30.800]   that there would be an oil shock
[01:00:30.800 --> 01:00:32.440]   and now it is actually profitable?
[01:00:32.440 --> 01:00:34.720]   Or how do you think about Saudi America
[01:00:34.720 --> 01:00:37.440]   in the aftermath of the events this year?
[01:00:37.440 --> 01:00:40.120]   - Well, I, at least from my understanding of it,
[01:00:40.120 --> 01:00:43.520]   and again, because I have been to lunch
[01:00:43.520 --> 01:00:45.040]   on this difficult book I'm writing,
[01:00:45.040 --> 01:00:47.560]   I haven't actually, I'm not as up to date
[01:00:47.560 --> 01:00:48.520]   on this as I should be.
[01:00:48.520 --> 01:00:50.960]   So take what I'm saying with a grain of salt.
[01:00:50.960 --> 01:00:55.040]   But I think actually it proves the underlying thesis
[01:00:55.040 --> 01:00:57.360]   because fracking is profitable,
[01:00:57.360 --> 01:01:00.080]   but at a much smaller scale than it was
[01:01:00.080 --> 01:01:03.320]   when people were saying this is gonna change the world.
[01:01:03.320 --> 01:01:06.160]   So it hasn't, the idea that US shale oil
[01:01:06.160 --> 01:01:08.760]   was the swing factor in world oil prices,
[01:01:08.760 --> 01:01:10.960]   I don't think anybody believes that anymore
[01:01:10.960 --> 01:01:13.160]   because it can't possibly produce
[01:01:13.160 --> 01:01:16.200]   the amount of oil required for it to be the major force
[01:01:16.200 --> 01:01:18.160]   that it was supposed to be.
[01:01:18.160 --> 01:01:20.640]   So I think what has happened actually proves the point
[01:01:20.640 --> 01:01:22.920]   instead of negating the point.
[01:01:22.920 --> 01:01:27.440]   And for how long it can be profitable
[01:01:27.440 --> 01:01:30.080]   and at what level of oil prices
[01:01:30.080 --> 01:01:34.240]   is also, was also part of the underlying thesis,
[01:01:34.240 --> 01:01:36.440]   which is how much oil is there actually
[01:01:36.440 --> 01:01:39.120]   that can be extracted at this price.
[01:01:39.120 --> 01:01:41.360]   And so the fact that very high oil prices,
[01:01:41.360 --> 01:01:44.920]   there is a certain amount that can be extracted profitably,
[01:01:44.920 --> 01:01:47.080]   that's not surprising.
[01:01:47.080 --> 01:01:50.120]   I think the way in which the Buck's thesis would be wrong
[01:01:50.120 --> 01:01:53.080]   is if it turned out US shale oil
[01:01:53.080 --> 01:01:56.320]   could be the swing producer for the world
[01:01:56.320 --> 01:01:58.920]   and we could be the world's largest oil producer
[01:01:58.920 --> 01:02:01.160]   and the Permian could continue to grow
[01:02:01.160 --> 01:02:04.280]   whatever it was, 20% a year and grow profitably.
[01:02:04.280 --> 01:02:06.840]   And then I would have to say, yeah,
[01:02:06.840 --> 01:02:09.120]   the investors were right
[01:02:09.120 --> 01:02:11.400]   or the people who believed in this were right.
[01:02:11.400 --> 01:02:12.240]   So.
[01:02:12.240 --> 01:02:14.000]   - Yeah, yep.
[01:02:14.000 --> 01:02:16.360]   If you could have a Robert Caro-like biography
[01:02:16.360 --> 01:02:19.120]   of anybody in finance in the last 100 years,
[01:02:19.120 --> 01:02:21.360]   especially somebody who we feel hasn't been talked about
[01:02:21.360 --> 01:02:22.920]   or covered enough, who would it be?
[01:02:22.920 --> 01:02:25.080]   Who do we need a thousand pages about?
[01:02:25.080 --> 01:02:26.080]   - A thousand pages.
[01:02:26.080 --> 01:02:28.120]   I don't know that we need a thousand pages about anybody.
[01:02:28.120 --> 01:02:30.360]   (laughing)
[01:02:30.360 --> 01:02:32.960]   Who would I like to write about?
[01:02:33.240 --> 01:02:36.160]   Um, think about that.
[01:02:36.160 --> 01:02:38.440]   There are some books I've been mulling over
[01:02:38.440 --> 01:02:40.880]   that I'd like to do, but I'm not sure I,
[01:02:40.880 --> 01:02:43.120]   I think I might prefer to keep those to myself
[01:02:43.120 --> 01:02:45.280]   for a little while.
[01:02:45.280 --> 01:02:46.120]   I don't know.
[01:02:46.120 --> 01:02:47.720]   I'm also always gonna be at least,
[01:02:47.720 --> 01:02:49.920]   I'm trying to change my orientation,
[01:02:49.920 --> 01:02:51.400]   but I'm always gonna be biased
[01:02:51.400 --> 01:02:53.960]   toward the thing that went badly wrong
[01:02:53.960 --> 01:02:56.040]   rather than the thing that worked out.
[01:02:56.040 --> 01:02:56.880]   I don't know.
[01:02:56.880 --> 01:02:58.480]   Maybe that betrays the underlying darkness
[01:02:58.480 --> 01:03:01.880]   of my personality, but I always find that more fun.
[01:03:01.880 --> 01:03:05.560]   So I'm probably not the right person to ask
[01:03:05.560 --> 01:03:08.480]   about a glowing biography.
[01:03:08.480 --> 01:03:10.560]   I don't, you know, I don't, I don't know.
[01:03:10.560 --> 01:03:12.640]   You could say, you could say Warren Buffett,
[01:03:12.640 --> 01:03:14.240]   but plenty has been written about Buffett.
[01:03:14.240 --> 01:03:17.800]   I don't know if there's anything to be added to that.
[01:03:17.800 --> 01:03:19.360]   So let me think.
[01:03:19.360 --> 01:03:21.160]   If I come up, if I come up with a great answer
[01:03:21.160 --> 01:03:23.840]   that I'm prepared to share, I will, I will tell you.
[01:03:23.840 --> 01:03:26.560]   I don't know that, maybe it goes back to your question
[01:03:26.560 --> 01:03:28.560]   about how big a role that finance should play
[01:03:28.560 --> 01:03:30.240]   in our economy overall too.
[01:03:30.240 --> 01:03:32.600]   Maybe I just don't think that anybody in finance
[01:03:32.600 --> 01:03:37.400]   is necessarily worthy of that, of that kind of,
[01:03:37.400 --> 01:03:41.200]   of that kind of, that kind of sustained focus.
[01:03:41.200 --> 01:03:42.040]   - Yeah, yeah.
[01:03:42.040 --> 01:03:44.240]   I wanna talk about the rating agencies
[01:03:44.240 --> 01:03:47.400]   because they have featured heavily in both the story
[01:03:47.400 --> 01:03:49.400]   about the housing crisis and Enron.
[01:03:49.400 --> 01:03:51.720]   And as a libertarian leading person,
[01:03:51.720 --> 01:03:56.560]   that's really been, that's been kind of devastating
[01:03:56.560 --> 01:03:58.280]   in the sense that, you know, there's a hope
[01:03:58.280 --> 01:04:01.600]   that you can maybe replace things like the FDA
[01:04:01.600 --> 01:04:06.040]   or at least functions of it with agency,
[01:04:06.040 --> 01:04:08.000]   private agencies that are tasked with like rating
[01:04:08.000 --> 01:04:09.920]   how good is this food or, you know,
[01:04:09.920 --> 01:04:12.720]   how safe is this airplane, things like that.
[01:04:12.720 --> 01:04:17.400]   And I'm curious if you think that there is any possibility
[01:04:17.400 --> 01:04:20.600]   of having any private agency that is being paid
[01:04:20.600 --> 01:04:22.200]   by the person they're evaluating,
[01:04:22.200 --> 01:04:23.960]   being able to give fair evaluations.
[01:04:23.960 --> 01:04:24.840]   And this even brings us,
[01:04:24.840 --> 01:04:27.080]   it doesn't even have to be rating agencies.
[01:04:27.080 --> 01:04:28.040]   It can be things like, you know,
[01:04:28.040 --> 01:04:29.880]   now we have the big four accounting firms.
[01:04:29.880 --> 01:04:31.960]   I'm curious how much trust you put in them
[01:04:31.960 --> 01:04:33.920]   in terms of being able to do evaluations fairly.
[01:04:33.920 --> 01:04:35.880]   Do we think, have you passed through
[01:04:35.880 --> 01:04:38.600]   the Arthur Anderson days?
[01:04:38.600 --> 01:04:40.480]   How credible are these institutions?
[01:04:40.480 --> 01:04:42.640]   - Look at EY's rolling wire card, right?
[01:04:42.640 --> 01:04:46.560]   And then I think from the scandals PWC has been involved
[01:04:46.560 --> 01:04:49.880]   in over the last bunch of years, I think, yeah.
[01:04:49.880 --> 01:04:52.920]   It is, it is a really good question.
[01:04:52.920 --> 01:04:54.280]   I thought you were gonna go somewhere different
[01:04:54.280 --> 01:04:55.480]   with the rating agencies,
[01:04:55.480 --> 01:04:57.720]   which is why in a supposedly free market,
[01:04:57.720 --> 01:05:01.480]   you have so many investors who rely on credit ratings.
[01:05:01.480 --> 01:05:02.320]   - That's a good question.
[01:05:02.320 --> 01:05:04.440]   - And I do think that there's,
[01:05:04.440 --> 01:05:06.240]   this is another perhaps counter
[01:05:06.240 --> 01:05:09.120]   to your efficient market theory,
[01:05:09.120 --> 01:05:12.840]   or maybe counter to a libertarian view of the world.
[01:05:12.840 --> 01:05:15.000]   But you know, a lot of big investors
[01:05:15.000 --> 01:05:16.600]   who complain about the rating agencies
[01:05:16.600 --> 01:05:18.760]   after there's been a disaster,
[01:05:18.760 --> 01:05:21.280]   really want the cover provided by the rating agencies
[01:05:21.280 --> 01:05:24.160]   because they can say, well, you told me this was AAA.
[01:05:24.160 --> 01:05:26.560]   I bought it, therefore I can't be blamed.
[01:05:26.560 --> 01:05:29.040]   And so once the crisis has passed,
[01:05:29.040 --> 01:05:32.200]   the appetite of big investors to reform the rating agencies
[01:05:32.200 --> 01:05:33.520]   or do away with them,
[01:05:33.520 --> 01:05:35.360]   meaning then they would have to do their own work
[01:05:35.360 --> 01:05:38.000]   and not just say, well, I bought a AAA rated security,
[01:05:38.000 --> 01:05:39.360]   is really next to nil.
[01:05:39.360 --> 01:05:40.720]   And that's a good part of the reason
[01:05:40.720 --> 01:05:43.040]   why reform doesn't happen.
[01:05:43.040 --> 01:05:45.400]   I mean, remember the credit rating agencies were reformed
[01:05:45.400 --> 01:05:47.480]   in 2006 in the wake of Enron.
[01:05:47.480 --> 01:05:50.040]   The credit rating agency reform was passed.
[01:05:50.040 --> 01:05:52.360]   And yet the rating agency sat at the center
[01:05:52.360 --> 01:05:54.000]   of the global financial crisis.
[01:05:54.000 --> 01:05:55.400]   But just a few years later,
[01:05:55.400 --> 01:06:00.840]   I'm not sure they really, they can be reformed,
[01:06:00.840 --> 01:06:03.000]   but I'm also not sure there is a perfect,
[01:06:03.000 --> 01:06:06.080]   maybe it goes back to your question
[01:06:06.080 --> 01:06:08.360]   about the ability to see the big picture
[01:06:08.360 --> 01:06:09.560]   and foresee a problem.
[01:06:09.560 --> 01:06:12.400]   I'm not sure there is a perfect regulator
[01:06:12.400 --> 01:06:14.920]   who wasn't being paid by companies
[01:06:14.920 --> 01:06:17.920]   who could then do a better job.
[01:06:17.920 --> 01:06:19.840]   You'd like to believe that's true
[01:06:19.840 --> 01:06:21.600]   that if there were a government agency
[01:06:21.600 --> 01:06:23.400]   in charge of credit rating
[01:06:23.400 --> 01:06:25.040]   and they weren't paid by the companies
[01:06:25.040 --> 01:06:30.040]   and the securities they weren't paid by the companies
[01:06:30.040 --> 01:06:33.160]   that that would lead to better ratings, would it?
[01:06:33.160 --> 01:06:36.200]   I don't know, right?
[01:06:36.200 --> 01:06:39.920]   So I'm not sure there's,
[01:06:39.920 --> 01:06:41.920]   I guess another way of saying this is very easy
[01:06:41.920 --> 01:06:45.920]   to criticize the current system as really problematic.
[01:06:45.920 --> 01:06:48.880]   And it's certainly the fact that credit rating agencies
[01:06:48.880 --> 01:06:50.480]   were making so much money
[01:06:50.520 --> 01:06:53.040]   by rating subprime mortgage backed securities
[01:06:53.040 --> 01:06:56.520]   for sure played an enormous role in what happened.
[01:06:56.520 --> 01:06:58.120]   But then if you ask the next question,
[01:06:58.120 --> 01:06:59.760]   well, what's the alternative?
[01:06:59.760 --> 01:07:04.080]   That's when it starts to get pretty complicated.
[01:07:04.080 --> 01:07:04.920]   - Yeah, yeah.
[01:07:04.920 --> 01:07:09.080]   Now that we're going to have this long proceeding on FTX
[01:07:09.080 --> 01:07:12.080]   in terms of compensating the people who are harmed,
[01:07:12.080 --> 01:07:15.120]   looking back at Enron, I mean, that was a long process.
[01:07:15.120 --> 01:07:18.040]   And I think you said in the book that a billion dollars
[01:07:18.040 --> 01:07:20.640]   of the remaining sum that should have been doled out
[01:07:20.640 --> 01:07:24.120]   to the victims was actually spent in legal fees
[01:07:24.120 --> 01:07:27.840]   and the procedures, if I'm right about that number.
[01:07:27.840 --> 01:07:29.080]   - I think so, I'm not sure.
[01:07:29.080 --> 01:07:31.880]   I admit I don't remember, but it was some enormous.
[01:07:31.880 --> 01:07:33.480]   - Yeah. - Yeah.
[01:07:33.480 --> 01:07:36.040]   - So what should be the procedure
[01:07:36.040 --> 01:07:37.560]   when an implosion like this happens?
[01:07:37.560 --> 01:07:39.560]   Because it seems suboptimal that so much,
[01:07:39.560 --> 01:07:41.280]   I mean, it goes back to their finance discussion, right?
[01:07:41.280 --> 01:07:43.600]   Like so much of the capital that these people
[01:07:43.600 --> 01:07:46.560]   are supposed to be doling out is just going to themselves.
[01:07:46.560 --> 01:07:48.400]   Do we need something like the FAA
[01:07:48.400 --> 01:07:50.840]   when the plane crashes, they have a body of experts
[01:07:50.840 --> 01:07:52.400]   that kind of figures out what happened.
[01:07:52.400 --> 01:07:54.120]   Do we need a completely non-legalistic
[01:07:54.120 --> 01:07:56.000]   or a different sort of procedure
[01:07:56.000 --> 01:07:57.160]   in these kinds of situations?
[01:07:57.160 --> 01:08:01.280]   How should the FTX, the disbursement
[01:08:01.280 --> 01:08:02.520]   of the remaining assets and so forth,
[01:08:02.520 --> 01:08:04.040]   how should that be done?
[01:08:04.040 --> 01:08:05.360]   - It's a really interesting question
[01:08:05.360 --> 01:08:06.760]   and a really interesting analogy
[01:08:06.760 --> 01:08:08.360]   and something people have brought up to me
[01:08:08.360 --> 01:08:09.880]   that I've never really dug into,
[01:08:09.880 --> 01:08:12.840]   which is the incredible success of the FAA
[01:08:12.840 --> 01:08:15.440]   in cutting down on problems with planes
[01:08:15.440 --> 01:08:17.880]   because of the incredibly thorough job they do
[01:08:17.880 --> 01:08:21.280]   in investigating the cause of a crash afterwards.
[01:08:21.280 --> 01:08:23.320]   And so maybe somebody's written about this,
[01:08:23.320 --> 01:08:25.320]   but to really get inside that process
[01:08:25.320 --> 01:08:28.800]   and the incredibly powerful and profound role it's played
[01:08:28.800 --> 01:08:32.000]   might provide an interesting roadmap for corporate America
[01:08:32.000 --> 01:08:34.200]   and how to do this sort of thing
[01:08:34.200 --> 01:08:36.280]   and how to prevent it in the future.
[01:08:36.280 --> 01:08:38.120]   Maybe, maybe the analogy doesn't hold up
[01:08:38.120 --> 01:08:40.680]   in some kind of subtle way.
[01:08:40.680 --> 01:08:44.880]   But for sure, that's a really interesting question
[01:08:44.880 --> 01:08:48.200]   because you need this to happen, right?
[01:08:48.200 --> 01:08:52.160]   You need the bankruptcy, you need the excavation.
[01:08:52.160 --> 01:08:55.200]   And even if it costs a great deal of money,
[01:08:55.200 --> 01:08:57.480]   just for all of us, the worst possible outcome
[01:08:57.480 --> 01:08:59.240]   would be for everybody to throw their hands up
[01:08:59.240 --> 01:09:01.880]   and say, okay, it's done.
[01:09:01.880 --> 01:09:04.920]   Nobody, no, no, because the truth of the matter
[01:09:04.920 --> 01:09:07.200]   is even journalists who excavate,
[01:09:07.200 --> 01:09:08.520]   who try to excavate this stuff,
[01:09:08.520 --> 01:09:11.600]   we depend on the work done by others.
[01:09:11.600 --> 01:09:14.000]   I mean, I could not have written my Enron book
[01:09:14.000 --> 01:09:17.800]   as it were for the Justice Department investigating
[01:09:17.800 --> 01:09:20.600]   and their indictments for the role of Congress
[01:09:20.600 --> 01:09:22.720]   in getting all these documents from,
[01:09:22.720 --> 01:09:25.160]   for all these internal documents from Enron,
[01:09:25.160 --> 01:09:27.000]   including by the way, the list of executives
[01:09:27.000 --> 01:09:27.920]   and their cell phone numbers,
[01:09:27.920 --> 01:09:30.120]   which turned out to be incredibly valuable.
[01:09:30.120 --> 01:09:34.520]   But you can't, the role of the bankruptcy examiner
[01:09:34.520 --> 01:09:37.320]   and getting in there and really uncovering
[01:09:37.320 --> 01:09:38.840]   all of what Enron was doing
[01:09:38.840 --> 01:09:41.680]   in order to manipulate its earnings,
[01:09:41.680 --> 01:09:43.360]   there's no way that a journalist,
[01:09:43.360 --> 01:09:45.960]   you could do that without all of that data
[01:09:45.960 --> 01:09:47.760]   provided by all of these entities
[01:09:47.760 --> 01:09:49.520]   that are doing their own investigation
[01:09:49.520 --> 01:09:51.800]   because as a journalist, you don't have subpoena power.
[01:09:51.800 --> 01:09:53.720]   You can't, you literally can't do this.
[01:09:53.720 --> 01:09:55.840]   And it becomes an incredibly important part
[01:09:55.840 --> 01:09:58.240]   of telling the overall story.
[01:09:58.240 --> 01:10:01.840]   So it's hard for me to say when it's a calamity like this,
[01:10:01.840 --> 01:10:03.200]   that those dollars are wasted,
[01:10:03.200 --> 01:10:05.440]   but is there a better way to go about it?
[01:10:05.440 --> 01:10:06.640]   Maybe, maybe.
[01:10:06.640 --> 01:10:10.400]   - Yeah, another similarity between Enron and FTX
[01:10:10.400 --> 01:10:15.400]   is that the bankruptcy is being overseen by John Ray.
[01:10:15.400 --> 01:10:17.800]   And this is a rather mysterious person.
[01:10:17.800 --> 01:10:21.640]   I believe one of the first times we've seen a photo of him
[01:10:21.640 --> 01:10:25.520]   is when he appeared in front of Congress a few days ago.
[01:10:25.520 --> 01:10:28.640]   And I checked the index of the smartest guys in the room.
[01:10:28.640 --> 01:10:31.920]   And I don't think he was mentioned in the book.
[01:10:31.920 --> 01:10:32.880]   Who is this guy?
[01:10:32.880 --> 01:10:35.000]   What's his deal?
[01:10:35.000 --> 01:10:37.040]   - I don't actually really know.
[01:10:37.040 --> 01:10:39.360]   I feel like people have made a really big deal
[01:10:39.360 --> 01:10:42.000]   over the bankruptcy administrator being the same person.
[01:10:42.000 --> 01:10:44.600]   I'm not really sure how much that matters.
[01:10:44.600 --> 01:10:45.440]   - Okay.
[01:10:45.440 --> 01:10:47.200]   - The bankruptcy administrator, when people,
[01:10:47.200 --> 01:10:48.280]   it was all over the press.
[01:10:48.280 --> 01:10:50.360]   You know, he says this is a bigger disaster
[01:10:50.360 --> 01:10:51.680]   than whatever it was.
[01:10:51.680 --> 01:10:53.120]   I can't remember the exact quote,
[01:10:53.120 --> 01:10:54.640]   but that's his incentive to say that
[01:10:54.640 --> 01:10:56.640]   because then any money that he can recoup
[01:10:56.640 --> 01:10:58.440]   looks like due to his genius
[01:10:58.440 --> 01:11:00.600]   in administrating this thing in bankruptcy.
[01:11:00.600 --> 01:11:02.680]   You say, oh, this wasn't that big a deal.
[01:11:02.680 --> 01:11:03.920]   Everybody's gonna get their money back.
[01:11:03.920 --> 01:11:04.880]   And then they don't.
[01:11:04.880 --> 01:11:06.960]   Then you've got a huge problem, right?
[01:11:06.960 --> 01:11:10.480]   You say, this is terrible and it's awful
[01:11:10.480 --> 01:11:11.640]   and there will be no money left
[01:11:11.640 --> 01:11:13.320]   because then any money you get back,
[01:11:13.320 --> 01:11:15.320]   people are like, look what a great job he did.
[01:11:15.320 --> 01:11:17.520]   So I don't know.
[01:11:17.520 --> 01:11:18.360]   I don't know.
[01:11:18.360 --> 01:11:20.880]   I don't remember him being a really pivotal figure
[01:11:20.880 --> 01:11:25.880]   in the excavation of Enron.
[01:11:25.880 --> 01:11:28.000]   - Yeah.
[01:11:28.000 --> 01:11:28.920]   Yeah.
[01:11:28.920 --> 01:11:32.880]   I wanna ask you a bit about your writing process.
[01:11:32.880 --> 01:11:35.560]   Do you go into the book with some sort of thesis
[01:11:35.560 --> 01:11:36.960]   about the actors?
[01:11:36.960 --> 01:11:39.680]   And if so, is it often that you realize
[01:11:39.680 --> 01:11:40.960]   that somebody you thought was a bad actor
[01:11:40.960 --> 01:11:42.840]   is actually one of the good guys
[01:11:42.840 --> 01:11:45.080]   and or the other way around
[01:11:45.080 --> 01:11:47.480]   or is basically your initial picture
[01:11:47.480 --> 01:11:50.120]   confirmed by further investigation?
[01:11:50.120 --> 01:11:52.880]   - Well, I guess that's a tough question
[01:11:52.880 --> 01:11:54.120]   about the books I've done
[01:11:54.120 --> 01:11:56.120]   because the books I have done,
[01:11:56.120 --> 01:11:59.240]   especially the two big ones were after the fact.
[01:11:59.240 --> 01:12:00.400]   And it was pretty clear,
[01:12:00.400 --> 01:12:02.520]   A, that something went badly wrong
[01:12:02.520 --> 01:12:05.000]   and B, that the people who were in power
[01:12:05.000 --> 01:12:06.360]   had to be responsible for it
[01:12:06.360 --> 01:12:08.680]   because they weren't responsible who else was.
[01:12:08.680 --> 01:12:10.560]   The way in which they were responsible
[01:12:10.560 --> 01:12:13.280]   and the degree to which they were responsible
[01:12:13.280 --> 01:12:18.000]   open questions that I don't think I had a view on going in.
[01:12:18.000 --> 01:12:20.640]   And to that point on the financial crisis,
[01:12:20.640 --> 01:12:23.560]   I actually ended up changing my mind pretty radically.
[01:12:23.560 --> 01:12:27.160]   I wrote a piece in the early stages of the financial crisis
[01:12:27.160 --> 01:12:30.200]   saying that basically homeowners were equally to blame
[01:12:30.200 --> 01:12:32.640]   because homeowners had cashed out,
[01:12:32.640 --> 01:12:34.800]   homeowners took out risky mortgages
[01:12:34.800 --> 01:12:38.080]   and without personal responsibility were lost.
[01:12:38.080 --> 01:12:39.720]   And homeowners had also cashed out
[01:12:39.720 --> 01:12:41.040]   and people had made a lot of money.
[01:12:41.040 --> 01:12:43.640]   So the idea that this was all the fault of the banks
[01:12:43.640 --> 01:12:45.280]   was absurd.
[01:12:45.280 --> 01:12:47.480]   And I still think that's true to some extent
[01:12:47.480 --> 01:12:50.360]   because without personal responsibility, we are lost.
[01:12:50.360 --> 01:12:52.920]   But I remember coming across this presentation
[01:12:52.920 --> 01:12:54.800]   that had been given by Washington Mutual,
[01:12:54.800 --> 01:12:55.640]   where they were,
[01:12:55.640 --> 01:12:58.000]   which was one of the big lenders at the time,
[01:12:58.000 --> 01:12:58.880]   no longer exists.
[01:12:58.880 --> 01:13:00.880]   And they were trying to get people to take out,
[01:13:00.880 --> 01:13:02.760]   it was a whole internal presentation
[01:13:02.760 --> 01:13:05.080]   about how you got someone to take out a risky mortgage
[01:13:05.080 --> 01:13:06.200]   when what they really wanted
[01:13:06.200 --> 01:13:08.440]   was the same 30-year fixed rate mortgage
[01:13:08.440 --> 01:13:10.320]   that their parents had had.
[01:13:10.320 --> 01:13:12.840]   And it was all the tactics and tips you could use
[01:13:12.840 --> 01:13:15.640]   to twist their arm to take out this riskier mortgage
[01:13:15.640 --> 01:13:17.720]   because those are the ones that were in demand
[01:13:17.720 --> 01:13:20.280]   to be packaged up into subprime mortgage-backed securities.
[01:13:20.280 --> 01:13:22.600]   And so Walmart could turn around and sell those mortgages
[01:13:22.600 --> 01:13:23.920]   for more money to Wall Street
[01:13:23.920 --> 01:13:26.960]   than they could the fake 30-year fixed rate mortgages.
[01:13:26.960 --> 01:13:28.320]   And then I remember thinking,
[01:13:28.320 --> 01:13:30.040]   there's just something wrong with a world
[01:13:30.040 --> 01:13:32.440]   where all of the responsibilities on consumers
[01:13:32.440 --> 01:13:34.920]   to understand and none of the responsibilities
[01:13:34.920 --> 01:13:36.800]   on the people selling the product,
[01:13:36.800 --> 01:13:38.800]   to be honest, about what this thing is.
[01:13:38.800 --> 01:13:42.640]   So anyway, that was a case where I changed my mind
[01:13:42.640 --> 01:13:44.560]   over the course of reporting a bug.
[01:13:44.560 --> 01:13:47.920]   I try not to work on, for stories,
[01:13:47.920 --> 01:13:51.080]   I try not to work on stories where there has to be an answer
[01:13:51.080 --> 01:13:54.680]   because if you're working on a story
[01:13:54.680 --> 01:13:57.040]   where it has to be one way for it to be a good story,
[01:13:57.040 --> 01:13:58.040]   then if it's not that,
[01:13:58.040 --> 01:14:00.080]   then you're biased to wanna see it one way.
[01:14:00.080 --> 01:14:02.880]   And if it's not that way, you no longer have a story.
[01:14:02.880 --> 01:14:05.360]   So I always prefer to work on things
[01:14:05.360 --> 01:14:07.800]   where it's just really interesting
[01:14:07.800 --> 01:14:11.000]   and whatever you end up thinking or saying about it,
[01:14:11.000 --> 01:14:13.280]   it's still a really interesting story.
[01:14:13.280 --> 01:14:15.680]   So I can't say it's always been.
[01:14:15.680 --> 01:14:18.440]   So I guess one of the last stories I did for Vanity Fair
[01:14:18.440 --> 01:14:23.240]   was when David Sackler wanted to talk to me
[01:14:23.240 --> 01:14:25.840]   about what did the Sacklers and Purdue
[01:14:25.840 --> 01:14:28.080]   and the opioid crisis wanted to talk
[01:14:28.080 --> 01:14:30.840]   about his view of what had gone wrong.
[01:14:30.840 --> 01:14:34.080]   And that in a way is, it was a weird story for me
[01:14:34.080 --> 01:14:37.160]   because usually I'm working from the outside in,
[01:14:37.160 --> 01:14:39.520]   I don't get stories where people wanna cooperate with me
[01:14:39.520 --> 01:14:41.120]   for I guess all sorts of reasons.
[01:14:41.120 --> 01:14:43.440]   But in a way, it was my favorite type of story
[01:14:43.440 --> 01:14:45.000]   because it was really interesting
[01:14:45.000 --> 01:14:46.600]   that he was willing to speak publicly.
[01:14:46.600 --> 01:14:48.600]   And if I had ended up thinking the Sacklers
[01:14:48.600 --> 01:14:51.960]   were totally blameless in the whole opioid crisis,
[01:14:51.960 --> 01:14:54.480]   I could have written because it was really interesting
[01:14:54.480 --> 01:14:56.000]   that he was speaking publicly.
[01:14:56.000 --> 01:15:00.120]   And so I like stories where it's a story,
[01:15:00.120 --> 01:15:02.520]   no matter how you end up viewing the main characters
[01:15:02.520 --> 01:15:04.960]   and how you end up viewing the arc of the thing.
[01:15:04.960 --> 01:15:06.400]   Does that make sense?
[01:15:06.400 --> 01:15:07.640]   - Yeah, yeah, it does.
[01:15:07.640 --> 01:15:09.840]   And I guess it raises another question
[01:15:09.840 --> 01:15:11.800]   is that the people you've studied
[01:15:11.800 --> 01:15:15.280]   in your books and your journalism,
[01:15:15.280 --> 01:15:17.120]   have you concluded afterwards that,
[01:15:17.120 --> 01:15:19.360]   there's some people who are fundamentally deceptive
[01:15:19.360 --> 01:15:22.800]   and there are some people who are ethical
[01:15:22.800 --> 01:15:24.480]   and that there's sort of this line
[01:15:24.480 --> 01:15:26.440]   or do you feel like if anybody was put
[01:15:26.440 --> 01:15:29.120]   into scaling shoes at the time or SPF shoes,
[01:15:29.120 --> 01:15:30.400]   there's a good chance they would have done
[01:15:30.400 --> 01:15:31.960]   the same kinds of things?
[01:15:31.960 --> 01:15:34.120]   - I think we all have the capacity to be deceptive.
[01:15:34.120 --> 01:15:35.680]   I think it's part of human nature.
[01:15:35.680 --> 01:15:38.000]   Some people are more inclined toward truth-telling
[01:15:38.000 --> 01:15:40.240]   and transparency than other people.
[01:15:40.240 --> 01:15:44.840]   Maybe just by nature, maybe by nurture
[01:15:44.840 --> 01:15:48.200]   and what's been rewarded in your career and your life
[01:15:48.200 --> 01:15:50.760]   inclines you to either be open and honest
[01:15:50.760 --> 01:15:52.800]   or to try to hide things.
[01:15:52.800 --> 01:15:56.120]   And maybe that's more of the experiences that you've had
[01:15:56.120 --> 01:15:57.840]   but I think we are all capable
[01:15:57.840 --> 01:16:00.560]   of a fundamental level of deception.
[01:16:00.560 --> 01:16:04.400]   For CEOs, whenever people look at a Jeff Skilling
[01:16:04.400 --> 01:16:07.040]   or Angela Mozilla or any of these characters
[01:16:07.040 --> 01:16:09.160]   and say, "I could never have done that."
[01:16:09.160 --> 01:16:11.600]   I mean, there's a fundamental rationalization
[01:16:11.600 --> 01:16:16.480]   that skilling by manipulating Enron's earnings
[01:16:16.480 --> 01:16:18.560]   was trying to keep the stock price high
[01:16:18.560 --> 01:16:21.840]   so he didn't hurt investors by having the stock go down
[01:16:21.840 --> 01:16:22.680]   the way it would have been
[01:16:22.680 --> 01:16:24.040]   if he had been radically honest
[01:16:24.040 --> 01:16:26.160]   about all of Enron's problems.
[01:16:26.160 --> 01:16:30.160]   And so we're all capable of that kind of rationalization
[01:16:30.160 --> 01:16:31.800]   that what we're doing is really
[01:16:31.800 --> 01:16:33.880]   in the long-term interests of somebody else
[01:16:33.880 --> 01:16:35.480]   and really in their best interest,
[01:16:35.480 --> 01:16:39.400]   even if it's not telling the truth.
[01:16:39.400 --> 01:16:42.520]   And I think to say that you're not capable of that,
[01:16:42.520 --> 01:16:45.760]   I mean, I don't know, I almost think that's more dangerous
[01:16:45.760 --> 01:16:47.320]   than admitting that you're capable of it
[01:16:47.320 --> 01:16:49.960]   because if you admit that you're capable of it
[01:16:49.960 --> 01:16:51.560]   then you can make a fundamental choice
[01:16:51.560 --> 01:16:55.400]   to either behave that way or not to behave that way.
[01:16:55.400 --> 01:16:59.040]   But there's also another component of human nature
[01:16:59.040 --> 01:17:01.480]   that I think is more prevalent in some people than others.
[01:17:01.480 --> 01:17:04.480]   And it's not, I don't know if you'd characterize it
[01:17:04.480 --> 01:17:08.000]   as dishonesty, but it's very deeply believing something
[01:17:08.000 --> 01:17:10.280]   in the moment and convincing other people
[01:17:10.280 --> 01:17:12.120]   because you so deeply believe it.
[01:17:12.120 --> 01:17:14.160]   And a month later, you don't believe it anymore
[01:17:14.160 --> 01:17:16.480]   because you're somebody who changes their mind.
[01:17:16.480 --> 01:17:20.400]   So is what you told people a month earlier a lie
[01:17:20.400 --> 01:17:22.960]   or was it a truth at its moment in time?
[01:17:22.960 --> 01:17:27.400]   And so there's that too that makes it more difficult
[01:17:27.400 --> 01:17:30.600]   to label somebody fundamentally deceptive, right?
[01:17:30.600 --> 01:17:32.720]   And they look like a lie to the person who believed
[01:17:32.720 --> 01:17:34.400]   what that person was saying and acted
[01:17:34.400 --> 01:17:36.680]   on that person's advice and belief.
[01:17:36.680 --> 01:17:39.840]   And a month later finds out that the person sold their stock
[01:17:39.840 --> 01:17:41.120]   or no longer believes that,
[01:17:41.120 --> 01:17:44.320]   but was the person being dishonest in that moment?
[01:17:44.320 --> 01:17:45.960]   But this is- - Yeah.
[01:17:45.960 --> 01:17:48.640]   And it raises the interesting question of moral luck.
[01:17:48.640 --> 01:17:49.960]   I'm sure there were tons of CEOs
[01:17:49.960 --> 01:17:52.600]   and there are tons of CEOs today who are like Ken Lei
[01:17:52.600 --> 01:17:53.880]   who are just basically disconnected
[01:17:53.880 --> 01:17:55.520]   from the business they started
[01:17:55.520 --> 01:17:57.160]   and are kind of hanging on.
[01:17:57.160 --> 01:18:00.200]   And it just so happened that Ken Lei was the CEO
[01:18:00.200 --> 01:18:02.800]   of the one that was committing massive fraud.
[01:18:02.800 --> 01:18:06.560]   But it's kind of like texting and driving
[01:18:06.560 --> 01:18:08.960]   where lots of people do it, one guy crashes
[01:18:08.960 --> 01:18:10.800]   and obviously he should be punished,
[01:18:10.800 --> 01:18:13.360]   but he's no more culpable than the rest of us who do it.
[01:18:13.360 --> 01:18:14.960]   - I like that concept of moral luck.
[01:18:14.960 --> 01:18:17.520]   I think that's incredibly true.
[01:18:17.520 --> 01:18:20.680]   It goes with the concept of somebody who was an investor
[01:18:20.680 --> 01:18:22.080]   over the last 40 years.
[01:18:22.080 --> 01:18:26.320]   Sure, probably most people did really, really well, right?
[01:18:26.320 --> 01:18:28.560]   With the tailwind of declining interest rates,
[01:18:28.560 --> 01:18:30.440]   does that mean these people were brilliant
[01:18:30.440 --> 01:18:33.160]   or that they lucked into the right field?
[01:18:33.160 --> 01:18:35.040]   And of course, some people didn't luck into it.
[01:18:35.040 --> 01:18:36.560]   They saw what was going on and chose it.
[01:18:36.560 --> 01:18:39.160]   And so there's that too.
[01:18:39.160 --> 01:18:42.440]   But maybe that trivializes your question
[01:18:42.440 --> 01:18:44.360]   because I think that's a very profound question.
[01:18:44.360 --> 01:18:48.560]   I think there definitely is such a thing as moral luck.
[01:18:48.560 --> 01:18:50.520]   And the only way in which you can tip the scales
[01:18:50.520 --> 01:18:55.280]   is by trying to be aware of bad people and bad situations
[01:18:55.280 --> 01:18:57.280]   and keeping yourself clear of them.
[01:18:57.280 --> 01:19:00.080]   Because back to your earlier question about culture,
[01:19:00.080 --> 01:19:01.320]   it's a trite cliche,
[01:19:01.320 --> 01:19:03.440]   but you are who you surround yourself with
[01:19:03.440 --> 01:19:05.480]   either in a job or in your friendships.
[01:19:05.480 --> 01:19:08.920]   And so if you wanna have the best chance possible
[01:19:08.920 --> 01:19:11.960]   of steering clear of accidents,
[01:19:11.960 --> 01:19:14.120]   you do have to be careful about the situations
[01:19:14.120 --> 01:19:15.720]   you choose to put yourself into.
[01:19:15.720 --> 01:19:19.400]   'Cause the idea that you can remain an island
[01:19:19.400 --> 01:19:23.720]   in a bad situation is not true as most of us.
[01:19:23.720 --> 01:19:25.120]   Yep, yep.
[01:19:25.120 --> 01:19:27.840]   Final question, or I guess second to final question,
[01:19:27.840 --> 01:19:29.960]   what advice would you give to people
[01:19:29.960 --> 01:19:32.240]   who want to do something similar
[01:19:32.240 --> 01:19:33.320]   to what you've done over your career,
[01:19:33.320 --> 01:19:35.360]   whether that's investigative journalism
[01:19:35.360 --> 01:19:38.960]   or some other role in maybe finance or technology
[01:19:38.960 --> 01:19:41.280]   that involves putting together the big picture?
[01:19:41.280 --> 01:19:43.000]   What advice do you have for them?
[01:19:43.000 --> 01:19:44.600]   That's an interesting question.
[01:19:44.600 --> 01:19:46.960]   I can't give advice for journalists anymore
[01:19:46.960 --> 01:19:50.000]   because the world in which I grew up no longer exists.
[01:19:50.000 --> 01:19:52.920]   I once would have told anybody to go work on,
[01:19:52.920 --> 01:19:55.960]   and maybe that's leading me to the right answer,
[01:19:55.960 --> 01:19:57.720]   but the world in which I grew up in journalism
[01:19:57.720 --> 01:19:58.760]   no longer exists.
[01:19:58.760 --> 01:20:02.680]   And so to go off and be a writer,
[01:20:02.680 --> 01:20:05.360]   back when I did it, you could get a job at a magazine
[01:20:05.360 --> 01:20:07.000]   and you could take home a paycheck
[01:20:07.000 --> 01:20:08.440]   that would enable you to cover your rent.
[01:20:08.440 --> 01:20:11.320]   And if you were like me and didn't have any family money,
[01:20:11.320 --> 01:20:12.280]   then you needed that.
[01:20:12.280 --> 01:20:15.040]   And so the idea now of telling somebody to go off
[01:20:15.040 --> 01:20:16.560]   and pursue a career as a writer,
[01:20:16.560 --> 01:20:21.120]   well, you don't have any other source of extra support.
[01:20:21.120 --> 01:20:24.680]   It's a lot more difficult than it once was.
[01:20:24.680 --> 01:20:27.280]   But actually, for me, I think that does lead
[01:20:27.280 --> 01:20:29.400]   to a better and deeper answer to your question,
[01:20:29.400 --> 01:20:33.080]   which is that, right, don't allow yourself to be seduced
[01:20:33.080 --> 01:20:35.480]   by the quick ease of PowerPoints
[01:20:35.480 --> 01:20:37.720]   and putting together bullet points.
[01:20:37.720 --> 01:20:38.560]   Force yourself to,
[01:20:38.560 --> 01:20:40.240]   when you're dealing with something complicated,
[01:20:40.240 --> 01:20:42.040]   force yourself to write it out.
[01:20:42.040 --> 01:20:44.400]   And maybe that would be different for other people,
[01:20:44.400 --> 01:20:48.200]   but for me, writing forces a level of intellectual honesty
[01:20:48.200 --> 01:20:52.440]   and a clarity about the big picture that nothing else does.
[01:20:52.440 --> 01:20:53.600]   It's really hard work.
[01:20:53.600 --> 01:20:55.080]   I've never understood people who say,
[01:20:55.080 --> 01:20:56.160]   "I just love to write.
[01:20:56.160 --> 01:20:57.000]   "It's so fun."
[01:20:57.000 --> 01:20:58.960]   I was like, "What?"
[01:20:58.960 --> 01:21:01.360]   It's one of the most difficult things you can do
[01:21:01.360 --> 01:21:04.080]   because to write clearly requires thinking clearly,
[01:21:04.080 --> 01:21:06.440]   and thinking clearly is really, really hard,
[01:21:06.440 --> 01:21:07.400]   at least for me.
[01:21:07.400 --> 01:21:10.320]   But it is really only in writing that I realized
[01:21:10.320 --> 01:21:11.560]   that I didn't understand something
[01:21:11.560 --> 01:21:13.280]   that I was pretending to understand.
[01:21:13.280 --> 01:21:17.000]   And so it's really easy to pretend to yourself
[01:21:17.000 --> 01:21:18.240]   that you understand something.
[01:21:18.240 --> 01:21:20.600]   And if you have to write it and write it clearly
[01:21:20.600 --> 01:21:22.840]   in a way that somebody else can understand it,
[01:21:22.840 --> 01:21:26.000]   that often forces you to,
[01:21:26.000 --> 01:21:29.600]   that forces you to realize your own lack of comprehension.
[01:21:29.600 --> 01:21:33.480]   And I think that exercise in terms of an understanding
[01:21:33.480 --> 01:21:36.160]   of the world and a chance of seeing the big picture
[01:21:36.160 --> 01:21:37.960]   is really, really critical.
[01:21:37.960 --> 01:21:39.880]   Maybe there's something for somebody else that would work
[01:21:39.880 --> 01:21:40.720]   that isn't writing.
[01:21:40.720 --> 01:21:43.200]   Maybe it'd be turning the world into a math proof.
[01:21:43.200 --> 01:21:44.800]   (both laughing)
[01:21:44.800 --> 01:21:48.640]   But there is, for me, that's what it is.
[01:21:48.640 --> 01:21:50.320]   - Yeah, yeah.
[01:21:50.320 --> 01:21:53.280]   And final question, what is your next book about?
[01:21:53.280 --> 01:21:56.040]   And when will we have the pleasure to read it?
[01:21:56.040 --> 01:21:56.880]   - I hope it's a pleasure.
[01:21:56.880 --> 01:21:59.520]   I'm not sure at this point it's gonna be a pleasure.
[01:21:59.520 --> 01:22:01.160]   So I'm writing it with Joe Nocera, again,
[01:22:01.160 --> 01:22:03.240]   who I wrote "All the Devils Are Here" with.
[01:22:03.240 --> 01:22:05.880]   And we set out to write about the economic consequences
[01:22:05.880 --> 01:22:06.960]   of the pandemic.
[01:22:06.960 --> 01:22:11.280]   But it's really become a broad look at how capitalism is
[01:22:11.280 --> 01:22:15.280]   and isn't functioning in our modern society.
[01:22:15.280 --> 01:22:19.040]   And then underneath that, a look at how government
[01:22:19.040 --> 01:22:21.120]   is and isn't setting the right rules
[01:22:21.120 --> 01:22:23.120]   for capitalism to function.
[01:22:23.120 --> 01:22:27.440]   Because back to your point about being a libertarian,
[01:22:27.440 --> 01:22:30.200]   I do think there's this lovely idea
[01:22:30.200 --> 01:22:32.120]   that markets exist independently
[01:22:32.120 --> 01:22:34.040]   of the rules set for them by society.
[01:22:34.040 --> 01:22:37.120]   But I'm not really sure that that's true.
[01:22:37.120 --> 01:22:39.040]   If you think about everything from the existence
[01:22:39.040 --> 01:22:42.280]   of a limited liability corporation to bankruptcy law,
[01:22:42.280 --> 01:22:44.440]   these are all rules laid down by society
[01:22:44.440 --> 01:22:47.600]   that dictate how the market functions.
[01:22:47.600 --> 01:22:49.880]   And so in the end, if the market isn't functioning
[01:22:49.880 --> 01:22:52.360]   the way we want it to, chances are it's the result
[01:22:52.360 --> 01:22:56.120]   of the improper conditions having been set.
[01:22:56.120 --> 01:22:57.920]   So that's what the book is turned into,
[01:22:57.920 --> 01:23:01.080]   is a way of looking at the flaws in capitalism,
[01:23:01.080 --> 01:23:04.360]   even leading up to the pandemic and how those were exposed
[01:23:04.360 --> 01:23:06.040]   and exacerbated by the pandemic.
[01:23:06.040 --> 01:23:06.880]   - Yeah, okay.
[01:23:06.880 --> 01:23:09.000]   I'm really excited to read it.
[01:23:09.000 --> 01:23:10.760]   That's an exciting piece.
[01:23:10.760 --> 01:23:12.720]   Do you know when it's going to be out, by the way?
[01:23:12.720 --> 01:23:14.760]   - October, 2023, yeah.
[01:23:14.760 --> 01:23:17.000]   - Okay, you'll have to come back on the podcast.
[01:23:17.000 --> 01:23:18.240]   - I would love that.
[01:23:18.240 --> 01:23:19.080]   - Yeah, yeah.
[01:23:19.080 --> 01:23:20.360]   - I would love that.
[01:23:20.360 --> 01:23:21.200]   - Awesome.
[01:23:21.200 --> 01:23:23.760]   (upbeat music)
[01:23:23.760 --> 01:23:27.160]   (upbeat music continues)
[01:23:27.160 --> 01:23:30.560]   (upbeat music continues)
[01:23:30.560 --> 01:23:33.960]   (upbeat music continues)
[01:23:34.760 --> 01:23:38.160]   (upbeat music continues)
[01:23:38.160 --> 01:23:56.560]   (upbeat music continues)
[01:23:56.560 --> 01:23:59.960]   (upbeat music continues)
[01:23:59.960 --> 01:24:18.360]   (upbeat music continues)
[01:24:18.760 --> 01:24:22.160]   (upbeat music continues)
[01:24:22.160 --> 01:24:41.960]   (upbeat music continues)
[01:24:42.560 --> 01:24:45.960]   (upbeat music continues)
[01:24:45.960 --> 01:25:09.960]   (upbeat music continues)
[01:25:10.560 --> 01:25:13.960]   (upbeat music continues)
[01:25:13.960 --> 01:25:35.760]   (upbeat music continues)
[01:25:36.360 --> 01:25:39.760]   (upbeat music continues)
[01:25:39.760 --> 01:25:56.240]   (upbeat music fades)
[01:25:56.240 --> 01:25:58.300]   you

