
[00:00:00.000 --> 00:00:03.440]   you know, we should not think we're at the end of moral progress and we should not think,
[00:00:03.440 --> 00:00:08.240]   "Oh, we should lock in the kind of Western values we have now." Instead, we should think,
[00:00:08.240 --> 00:00:12.240]   "We want to ensure that we spend like a lot of time trying to figure out what's actually morally
[00:00:12.240 --> 00:00:17.200]   right so that the future is guided by the right values rather than merely whichever happened to
[00:00:17.200 --> 00:00:22.080]   win out." Perhaps if the Industrial Revolution had happened in India rather than in Western Europe,
[00:00:22.080 --> 00:00:26.560]   then perhaps we wouldn't have wide-scale factory farming. And then academia, I think, has just
[00:00:26.560 --> 00:00:32.800]   developed a culture where you don't tackle such problems in academia. Partly that's because they
[00:00:32.800 --> 00:00:37.760]   fall through cracks of different disciplines and partly because they just seem too big or too grand
[00:00:37.760 --> 00:00:43.120]   or too speculative. The idea of long reflection is getting society into a state that before we
[00:00:43.120 --> 00:00:48.800]   take any drastic actions that might lock in a particular set of values, we allow this force
[00:00:48.800 --> 00:00:54.880]   of reason and empathy and debate and good-hearted kind of moral inquiry to guide which values we
[00:00:54.880 --> 00:01:07.600]   end up with. Okay, today I have the pleasure of interviewing William McCaskill. Will is one of
[00:01:07.600 --> 00:01:13.200]   the founders of the Effective Altruist Movement and most recently the author of the upcoming book
[00:01:13.200 --> 00:01:17.760]   "What We Owe the Future." Will, thanks for coming on the podcast. Thanks so much for having me on.
[00:01:17.760 --> 00:01:22.880]   So my first question is, what is the high-level explanation for the success of the Effective
[00:01:22.880 --> 00:01:26.480]   Altruist Movement? Is it itself an example of the contingencies you talk about in the book?
[00:01:26.480 --> 00:01:32.960]   Yeah, I think it probably is kind of contingent. Maybe not on the order of, like,
[00:01:32.960 --> 00:01:39.680]   this would never have happened, but at least on the order of decades. Evidence for the reason why
[00:01:39.680 --> 00:01:45.280]   Effective Altruism is somewhat contingent is just that similar ideas have been promoted many times
[00:01:45.280 --> 00:01:51.040]   during history and not taken on. So we can go all the way back to ancient China. The Mohists
[00:01:51.040 --> 00:01:57.200]   defended a kind of impartial view of morality and took very strategic actions to try and help all
[00:01:57.200 --> 00:02:04.240]   people, in particular providing defensive assistance to cities under siege. Then, of course,
[00:02:04.240 --> 00:02:08.080]   there were the early utilitarians. Effective Altruism is broader than utilitarianism but
[00:02:08.080 --> 00:02:15.280]   has some similarities. And then even Peter Singer in the 70s, he had been promoting the idea that we
[00:02:15.280 --> 00:02:20.560]   should be giving most of our income to help the very poor and hadn't had a lot of traction until,
[00:02:20.560 --> 00:02:26.400]   even like early 2010, after GiveWell launched, after Giving What We Can launched.
[00:02:26.400 --> 00:02:30.000]   What explains the rise of it? I mean, I think it was a good idea waiting to happen at some point.
[00:02:30.000 --> 00:02:35.600]   I think the internet helped to gather together a lot of like-minded people that weren't possible
[00:02:35.600 --> 00:02:40.560]   otherwise. And I think there were some particularly lucky events like Ellie meeting Holden, me meeting
[00:02:40.560 --> 00:02:46.400]   Toby that helped catalyze it at the particular time it did. Now, if it's true, as you say in
[00:02:46.400 --> 00:02:51.200]   the book, that moral values are very contingent, then shouldn't that make us suspect that modern
[00:02:51.200 --> 00:02:55.760]   Western values probably aren't that good? They're probably mediocre or worse because ex-ante you
[00:02:55.760 --> 00:03:00.880]   would expect to end up with the median of all the values we could have had at this point. And
[00:03:00.880 --> 00:03:06.560]   obviously we'd be biased in favor of whatever values we were brought up in. Absolutely. I think
[00:03:06.560 --> 00:03:11.440]   taking history seriously and appreciating the contingency of values, appreciating that if the
[00:03:11.440 --> 00:03:16.640]   Nazis had won the world war, we would all be thinking, wow, I'm so glad that moral progress
[00:03:16.640 --> 00:03:21.200]   happened the way it did. And we don't have Jewish people around anymore. What huge moral progress we
[00:03:21.200 --> 00:03:25.680]   had then. Like that's a terrifying thought. And I think it should make us take seriously the fact
[00:03:25.680 --> 00:03:31.120]   that we're very far away from the moral truth right now. So I think it, you know, one of the
[00:03:31.120 --> 00:03:36.080]   lessons I draw in the book is, you know, we should not think we're at the end of moral progress and
[00:03:36.080 --> 00:03:40.800]   we should not think, oh, we should lock in the kind of Western values we have now. Instead,
[00:03:40.800 --> 00:03:45.600]   we should think we want to ensure that we spend like a lot of time trying to figure out what's
[00:03:45.600 --> 00:03:50.240]   actually morally right. So that the future is guided by the right values rather than merely
[00:03:50.240 --> 00:03:55.360]   whichever happened to win out. So that makes a lot of sense, but I guess I'm asking a slightly
[00:03:55.360 --> 00:03:59.520]   separate question, which is not only are there possible values that could be better than ours,
[00:03:59.520 --> 00:04:03.600]   but should we expect our values? I mean, we have this sense that we've made moral progress. So like
[00:04:03.600 --> 00:04:09.840]   things are better than they were before or better than most possible other worlds in 2100 or 2022,
[00:04:09.840 --> 00:04:13.200]   I mean, should we not even expect that to be the case? Like, should our prior just be that,
[00:04:13.200 --> 00:04:18.560]   yeah, these are kind of meh values. I think our prior should be that our values are, you know,
[00:04:18.560 --> 00:04:24.640]   as good as what one would have expected kind of on average. And then you can make an assessment,
[00:04:24.640 --> 00:04:29.040]   like is the world, are the values of the world today, is that going like particularly well,
[00:04:29.040 --> 00:04:33.280]   you know, and there's some arguments you could make for saying no, perhaps if the industrial
[00:04:33.280 --> 00:04:37.920]   revolution had happened in India rather than in Western Europe, then perhaps we wouldn't have
[00:04:37.920 --> 00:04:43.760]   wide scale factory farming, which I think is like a moral atrocity. Having said that, my all things
[00:04:43.760 --> 00:04:49.680]   considered view is actually to think that like we're doing better than average. Like if civilization
[00:04:49.680 --> 00:04:56.480]   were just a redraw, then things would look worse in terms of our moral beliefs and attitudes,
[00:04:56.480 --> 00:05:02.320]   where I think the abolition of slavery, the feminist movement, liberalism itself,
[00:05:02.320 --> 00:05:08.320]   democracy, these are all things that like we relatively easily could have lost and are huge
[00:05:08.320 --> 00:05:13.200]   kind of gains. But then if that's true, does that make the prospect of a long reflection kind of
[00:05:13.200 --> 00:05:18.960]   dangerous? Because if moral progress is sort of a random walk and we've ended up with a lucky lottery,
[00:05:18.960 --> 00:05:24.000]   then you're kind of just maybe reversing, maybe you're risking regression to the mean,
[00:05:24.000 --> 00:05:29.280]   if you just have a thousand years of progress. I think that moral progress isn't a random walk
[00:05:29.280 --> 00:05:35.760]   in general, like there are many forces that act on culture and on what people believe. And one of
[00:05:35.760 --> 00:05:40.880]   them is just what's right morally speaking, like what's their best argument support. That is a
[00:05:40.880 --> 00:05:45.600]   force. I think it's like a somewhat weak force, unfortunately. And the idea of long reflection
[00:05:45.600 --> 00:05:51.200]   is getting society into a state that before we take any drastic actions that might lock in a
[00:05:51.200 --> 00:05:57.440]   particular set of values, we allow this force of reason and empathy and debate and good-hearted
[00:05:57.440 --> 00:06:02.480]   kind of moral inquiry to guide which values we end up with. Okay, so in the book you make this
[00:06:02.480 --> 00:06:08.480]   interesting analogy where humans at this point in history are like teenagers. But another common
[00:06:08.480 --> 00:06:13.840]   impression that people have of teenagers is that they disregard wisdom and tradition and the
[00:06:13.840 --> 00:06:19.200]   opinions of adults too early and too often. And so do you think it makes sense to extend the
[00:06:19.200 --> 00:06:24.240]   analogy this way and suggest that maybe we should be, you know, Berkey and small l long-termist and
[00:06:24.240 --> 00:06:31.040]   reject these inside view esoteric threats? Like I think the Berkey and arguments for, you know,
[00:06:31.040 --> 00:06:36.560]   taking history seriously and like what kind of model views have like stood the test of time,
[00:06:36.560 --> 00:06:41.040]   you know, I think that's important arguments to engage with. My view kind of goes the opposite
[00:06:41.040 --> 00:06:47.600]   actually, which is that, you know, we are cultural creatures and we, in our nature,
[00:06:47.600 --> 00:06:51.600]   are very inclined to agree with what other people think, agree with tradition,
[00:06:52.160 --> 00:06:57.120]   even if we don't understand the underlying mechanisms. I think that works well in a low
[00:06:57.120 --> 00:07:03.600]   change environment. So the environment we evolved towards, like things didn't change very much. We
[00:07:03.600 --> 00:07:08.720]   were hunter-gatherers and small bands for hundreds of thousands of years, millions of years if you
[00:07:08.720 --> 00:07:14.880]   include other homospecies. Whereas now we're in this period of like enormous change where the
[00:07:14.880 --> 00:07:19.760]   economy is doubling every 20 years, new technologies arrive every single year. That's like
[00:07:19.760 --> 00:07:25.680]   unprecedented. And I think actually means we should much more than would make sense historically,
[00:07:25.680 --> 00:07:29.440]   just be trying to figure things out more from first principles.
[00:07:29.440 --> 00:07:36.480]   Interesting. But at current margins, do you think that's still the case? Like if a lot of EA and
[00:07:36.480 --> 00:07:40.800]   long-termist thought is first principles thought, do you think more history would be better than
[00:07:40.800 --> 00:07:44.400]   the marginal first principles thinker? I think two things. So if it's about
[00:07:44.400 --> 00:07:49.200]   an understanding of history, then yeah, I actually would love EA to have a better historical
[00:07:49.200 --> 00:07:53.600]   understanding, mainly just like as a on the margin thing. You know, I think the most important
[00:07:53.600 --> 00:07:57.680]   subjects if you want to do good in the world in an EA way are philosophy and economics,
[00:07:57.680 --> 00:08:03.040]   but we've got that like in abundance. Whereas there's very little in the EA community in terms
[00:08:03.040 --> 00:08:08.000]   of historical knowledge. And I certainly felt like I learned a huge amount over the last few years
[00:08:08.000 --> 00:08:14.400]   understanding that better. Should there be even more first principles thinking? Yeah, probably.
[00:08:14.400 --> 00:08:18.320]   I mean, I think the kind of first principles thinking really, or what you might call first
[00:08:18.320 --> 00:08:22.640]   principles thinking, I think like paid off pretty well in the course of the coronavirus pandemic,
[00:08:22.640 --> 00:08:29.200]   where from January, even 2020, my Facebook wall was completely saturated with people freaking out,
[00:08:29.200 --> 00:08:34.480]   or like at least taking it very, very seriously in a way that the existing institutions weren't.
[00:08:34.480 --> 00:08:38.480]   And they weren't because they were just in this mode of like, oh, business as usual, don't panic.
[00:08:38.480 --> 00:08:41.600]   They weren't properly updating to a new environment and new evidence.
[00:08:43.440 --> 00:08:48.560]   Now, in the book, you point out several examples of societies that went through hardship. I mean,
[00:08:48.560 --> 00:08:53.680]   hardship is putting it mildly, but, you know, Hiroshima after the bomb, Vietnam after the
[00:08:53.680 --> 00:08:58.640]   bombings. And then, yeah, the Europe after the Black Death. And they seem to have rebounded
[00:08:58.640 --> 00:09:03.920]   relatively quickly. Does this make you think that perhaps the role of contingency in history,
[00:09:03.920 --> 00:09:07.760]   especially economic history, is not that large? And it implies a sort of solo model
[00:09:07.760 --> 00:09:11.920]   of growth where, yeah, even if bad things happen, you can kind of just rebound and it really didn't
[00:09:11.920 --> 00:09:17.120]   matter. Yeah, in terms, in economic terms, I mean, I think that's the big difference between
[00:09:17.120 --> 00:09:21.280]   economic or technological progress and model progress, where in the long run, at least,
[00:09:21.280 --> 00:09:26.240]   I think economic or technological progress is very non-contingent. I mean, it's actually
[00:09:26.240 --> 00:09:32.880]   fascinating some historical contingencies you can see in technology. The Egyptians had an early
[00:09:32.880 --> 00:09:38.880]   version of the steam engine. Semaphore was only developed very late, yet could have been invented
[00:09:38.880 --> 00:09:43.440]   like a thousands of years in the past, similarly with like Kay's flying shuttle. But in the long
[00:09:43.440 --> 00:09:50.400]   run, like the instrumental benefits of tech progress and the incentives towards tech progress
[00:09:50.400 --> 00:09:56.160]   and economic growth are just kind of so strong that it means, like, I think we get there in the
[00:09:56.160 --> 00:10:00.880]   end in a very wide array of circumstances. And in particular, just imagine there's a thousand
[00:10:00.880 --> 00:10:05.280]   different societies and none are growing, but one is. Then in the long run, that one becomes the
[00:10:05.280 --> 00:10:09.680]   whole economy. Yeah, it seems that particular example you gave of the Egyptians having some
[00:10:09.680 --> 00:10:13.680]   ancient form of a steam engine, maybe that points towards there being more contingency because
[00:10:13.680 --> 00:10:16.800]   maybe the steam engine comes up in many societies, but it only gets turned into an
[00:10:16.800 --> 00:10:21.520]   industrial revolution in one. In that particular case, there's a big debate about whether quality
[00:10:21.520 --> 00:10:26.800]   of metalwork was actually possible to build a proper steam engine at that time. So I was
[00:10:26.800 --> 00:10:30.400]   mentioning those examples to say, like, historically, you actually do get like some
[00:10:30.400 --> 00:10:35.200]   amazing examples of contingency prior to the industrial revolution. I think it's still
[00:10:35.200 --> 00:10:39.680]   contingency only on the order of, you know, centuries to thousands of years. And then in
[00:10:39.680 --> 00:10:43.600]   the like post-industrial revolution world, I think there's like much less contingency.
[00:10:43.600 --> 00:10:48.240]   It's much harder to see that there are some, but I think it's much harder to see technologies that
[00:10:48.240 --> 00:10:52.480]   wouldn't have happened, you know, within decades if they hadn't been developed when they were.
[00:10:52.480 --> 00:10:58.080]   Okay. So I guess maybe the general model here is of yours is that there's maybe these
[00:10:58.080 --> 00:11:01.200]   general purpose changes in the state of technology, and those are very contingent,
[00:11:01.200 --> 00:11:03.920]   and it would be very important to like try to engineer one of those. But other than that,
[00:11:03.920 --> 00:11:06.720]   it's going to get done by some guy starting to create a startup anyways.
[00:11:06.720 --> 00:11:12.240]   No, I mean, I think more generally. So even the case of like the steam engine or semaphore that
[00:11:12.240 --> 00:11:18.080]   I was pointing to, which historically seem kind of maybe contingent, I think in the long run,
[00:11:18.080 --> 00:11:23.200]   they get developed where, you know, if the industrial revolution hadn't happened in Britain
[00:11:23.200 --> 00:11:27.120]   in the 18th century, would it have happened at some point? Or like, would similar technologies
[00:11:27.120 --> 00:11:31.600]   have been developed that were vital in the industrial revolution? And I'm like, yes,
[00:11:31.600 --> 00:11:36.560]   because there are very strong incentives for doing so. If you just got a whole bunch of cultures,
[00:11:36.560 --> 00:11:40.960]   and they're all in a random walk, and one hits upon like, hey, we're going to do industry,
[00:11:40.960 --> 00:11:46.320]   we're like a culture that's into making textiles and like doing that in an automated way,
[00:11:46.320 --> 00:11:52.480]   as was true of England in the 18th century, then that economy just takes over the world.
[00:11:53.280 --> 00:11:57.360]   And so that's why there's this structural reason, I think, why economic growth is,
[00:11:57.360 --> 00:12:00.880]   is like much, much less contingent than like moral progress.
[00:12:00.880 --> 00:12:07.840]   Okay, so usually people, when they think of somebody like Norman Borlaug in the green
[00:12:07.840 --> 00:12:11.040]   revolution, it's like, oh, that if you could have done something that you'd be like the greatest
[00:12:11.040 --> 00:12:15.280]   person in the 20th century, obviously, he's still a very good man and everything. But so that would
[00:12:15.280 --> 00:12:17.840]   that not be your view? Like you think the green revolution would have happened anyways?
[00:12:18.800 --> 00:12:24.480]   Yeah, so Norman Borlaug is sometimes credited with saving a billion lives. I think he was huge. I
[00:12:24.480 --> 00:12:30.160]   think he was like enormously important and good force for the world. I think it's not the case
[00:12:30.160 --> 00:12:35.920]   that had Norman Borlaug not existed, a billion people would have died. Rather, similar developments
[00:12:35.920 --> 00:12:40.240]   would have happened shortly afterwards. Perhaps he saved 10s of millions of lives. And that's a lot
[00:12:40.240 --> 00:12:45.360]   of lives for a person to save. But it's not as many as just simply saying, oh, this tech was
[00:12:45.360 --> 00:12:49.920]   developed, this tech was used, a billion people who would have otherwise been at risk of starvation
[00:12:49.920 --> 00:12:56.080]   used his technology. And in fact, even at the time, there were, you know, not long afterwards,
[00:12:56.080 --> 00:13:00.720]   similar kind of agricultural developments. Yeah, okay. So then counterfactually, what,
[00:13:00.720 --> 00:13:05.040]   what group of people like what kind of profession or career choice tends to lead to the highest
[00:13:05.040 --> 00:13:10.880]   counterfactual impact? Is it moral philosophers or? Not quite moral philosophers, although perhaps
[00:13:11.440 --> 00:13:17.280]   sometimes. I think, you know, there are some examples. So just sticking on science technology.
[00:13:17.280 --> 00:13:23.040]   So if you look at Einstein, theory of special relativity would have been developed, you know,
[00:13:23.040 --> 00:13:27.360]   very shortly afterwards. However, his theory of general relativity, I think was plausibly like
[00:13:27.360 --> 00:13:32.000]   decades in advance. So you do sometimes get these like, oh, surprising leaps. But I think we're
[00:13:32.000 --> 00:13:36.480]   still only talking about decades rather than millennia. And so who really does make a very
[00:13:36.480 --> 00:13:40.640]   long term difference? Yeah, I think it's like, moral philosophers could be one, like, I think
[00:13:40.640 --> 00:13:46.160]   Marx, and Engels made this like enormous, very long run difference. So religious leaders, I think
[00:13:46.160 --> 00:13:52.240]   that Mohammed, Jesus, Confucius, made enormous and contingent long run difference, and moral
[00:13:52.240 --> 00:13:59.280]   activists as well. So abolitionists, campaigners, the Quakers, and you know, yeah, other groups too.
[00:14:00.000 --> 00:14:06.080]   So if you think that the changeover in the landscape of ideas is very quick today,
[00:14:06.080 --> 00:14:10.000]   is it would you still think that maybe somebody like Marx has been will be considered very
[00:14:10.000 --> 00:14:13.840]   influential in the long future? Because I mean, communism lasted less than a century, right? Maybe
[00:14:13.840 --> 00:14:19.760]   it's like longer and consequences are huge, but it's all an expectation. So as things, in fact,
[00:14:19.760 --> 00:14:24.080]   turned out, probably Marx will not be very influential over the long term future. But that
[00:14:24.080 --> 00:14:28.880]   could have gone another way. It's not like such a wildly different history, where rather than
[00:14:28.880 --> 00:14:34.240]   liberalism emerging dominant in the 20th century, it was communism. And then if it had totally on
[00:14:34.240 --> 00:14:41.200]   the table for me that that like, persists for an extremely long time, where if you develop certain,
[00:14:41.200 --> 00:14:46.880]   the better technology gets, the better a ruling ideology is to kind of cement its ideology and
[00:14:46.880 --> 00:14:52.400]   persist for the very long time. And so you can get like a set of knock on effects where, okay,
[00:14:52.400 --> 00:14:57.680]   communism wins the war of ideas in the 20th century. Let's say in the limit forms a world
[00:14:57.680 --> 00:15:04.800]   government based around world state based around those ideas, then via kind of anti aging technology,
[00:15:04.800 --> 00:15:12.160]   or genetic enhancement technology, or cloning, or artificial intelligence, it's then able to
[00:15:12.160 --> 00:15:16.080]   build a society that like literally persists forever, in accordance with that ideology.
[00:15:16.080 --> 00:15:19.920]   Yeah, the death of dictators is especially interesting when you're thinking about
[00:15:19.920 --> 00:15:23.280]   contingency, because well, yeah, well, when Mao dies, or when Stalin dies, there's like
[00:15:23.280 --> 00:15:27.200]   huge changes in the regime, which makes you think, yeah, the actual individual there was
[00:15:27.200 --> 00:15:32.480]   very important, and who they happen to be was contingent and persistent, or at least important
[00:15:32.480 --> 00:15:36.880]   in some interesting ways. For sure. So if you've got a dictatorship, then you've got a single person
[00:15:36.880 --> 00:15:41.760]   ruling the whole of society. And that means it's just heavily contingent, like what the views and
[00:15:41.760 --> 00:15:45.680]   values and beliefs and personality of that person. Yeah, so going back to stagnation,
[00:15:45.680 --> 00:15:49.760]   in the book, you you're very concerned about fertility, because it seems your
[00:15:49.760 --> 00:15:54.080]   model about how progress or like how scientific and technological progress happens is number of
[00:15:54.080 --> 00:16:00.160]   people times average research or productivity. And then yeah, if research productivity is declining,
[00:16:00.160 --> 00:16:03.120]   and also the number of people isn't growing that fast, then that's concerning.
[00:16:03.120 --> 00:16:07.600]   It's yeah, number of people times fraction of the population devoted to R&D.
[00:16:07.600 --> 00:16:13.200]   Yeah, thanks for the clarification. It seems that there have been a lot of intense concentrations
[00:16:13.200 --> 00:16:17.360]   of like talent and progress in history, you know, like Venice, Athens, Bell Labs,
[00:16:17.360 --> 00:16:22.320]   or even something like FTX, right? You like there's 20 developers making this
[00:16:22.320 --> 00:16:27.280]   like multi billion dollar company. Do these do these examples suggest that maybe organization
[00:16:27.280 --> 00:16:31.600]   and congregation of researchers matters more than the actual total amount?
[00:16:31.600 --> 00:16:38.240]   So I actually think the model works reasonably pretty well. So throughout history, you're
[00:16:38.240 --> 00:16:45.040]   starting from this very low baseline, like very low technological, like level compared to today,
[00:16:45.040 --> 00:16:48.880]   and most people aren't even trying to innovate. Or if they're trying to innovate,
[00:16:48.880 --> 00:16:53.680]   it might be in things that we wouldn't now call like science technology. So it might be theology,
[00:16:53.680 --> 00:17:00.880]   it might. So one argument for why Baghdad lost its golden age scientific golden age
[00:17:00.880 --> 00:17:05.520]   is because the political landscape change such that what was incentivized was theological,
[00:17:05.520 --> 00:17:10.480]   theological investigation, rather than scientific investigation in the kind of 10th 11th century AD.
[00:17:11.520 --> 00:17:16.960]   Similarly, one argument for why did Britain have a scientific and industrial revolution rather than
[00:17:16.960 --> 00:17:21.680]   Germany was because all of the intellectual talent in Germany was focused on making amazing music.
[00:17:21.680 --> 00:17:27.600]   And that doesn't compound in the way that making textiles cheaper does. And so if you look at like
[00:17:27.600 --> 00:17:32.880]   Sparta versus Athens, for example, like what was the difference between Sparta and Athens? I think
[00:17:32.880 --> 00:17:38.080]   it's just that like they had different cultures, such that in Athens, intellectual inquiry was
[00:17:38.080 --> 00:17:44.400]   the warded. And because they're starting from a much lower base, even just, you know, hundreds of
[00:17:44.400 --> 00:17:49.200]   people or 1000s of people trying to do this thing that looks vaguely like science or vaguely like
[00:17:49.200 --> 00:17:53.520]   what we now think of as intellectual inquiry have has these enormous kind of impacts.
[00:17:53.520 --> 00:17:57.840]   I see. But then if you take an example, like Bell Labs, right, so late 20th century,
[00:17:57.840 --> 00:18:02.640]   the low hanging fruit is mostly gone. But then you have this one small organization that does six
[00:18:02.640 --> 00:18:08.960]   Nobel Prizes, I think. So yeah, then is this a kind of a coincidence and lucky break?
[00:18:08.960 --> 00:18:13.600]   Yeah, I wouldn't, I wouldn't say that at all. And I should acknowledge the like,
[00:18:13.600 --> 00:18:19.360]   the model where what you're working with is just size of the population times, like what fraction
[00:18:19.360 --> 00:18:23.840]   of the population you're putting towards R&D. That's like a toy model. It's like, maybe the
[00:18:23.840 --> 00:18:30.080]   simplest model you could have of it. And so Bell Labs, like I think is like punching above its
[00:18:30.080 --> 00:18:35.040]   weight. I think you obviously can create like, you know, amazing things from like a certain
[00:18:35.040 --> 00:18:39.440]   environment where not only you getting like the very most productive people, but you're putting
[00:18:39.440 --> 00:18:42.720]   them in an environment where they're like 10 times more productive than they would otherwise be.
[00:18:42.720 --> 00:18:46.880]   However, I think what I would say is like, when you're looking at the grand sweep of history,
[00:18:46.880 --> 00:18:51.760]   those effects are like comparatively small, compared to just like, sheer,
[00:18:51.760 --> 00:18:56.320]   like compared to like the broader culture of a society, or the sheer size of a population.
[00:18:56.880 --> 00:19:03.520]   I want to talk about your paper on long termist institutional reform. So yeah, you one of the
[00:19:03.520 --> 00:19:08.640]   things you advocate in this paper is that we should have what one of the houses be dedicated
[00:19:08.640 --> 00:19:13.040]   to where it's long term as priorities. Can you name like some specific performance metrics you
[00:19:13.040 --> 00:19:18.080]   would use to judge like, or incentivize the group of people who make up this body?
[00:19:18.080 --> 00:19:24.560]   Sure, yeah. I mean, the thing I'll caveat with long termist institutions is like,
[00:19:24.560 --> 00:19:28.000]   I'm actually like pretty pessimistic about them. In the sense, you know, I have this paper
[00:19:28.000 --> 00:19:32.320]   exploring it. But there's just this fundamental issue that like, if you're trying to represent,
[00:19:32.320 --> 00:19:36.160]   or even give consideration to future people, you just have to face the fact that they're not around
[00:19:36.160 --> 00:19:41.360]   and they can't lobby for themselves. And so you're going to have co-option by, you know,
[00:19:41.360 --> 00:19:47.760]   people in the present. However, you could have this like, assembly of people who have some sort
[00:19:47.760 --> 00:19:52.160]   of like real regulatory power. How would you constitute that? Like, my best guess is you just
[00:19:52.160 --> 00:19:59.040]   like have a random selection from the population. How would you ensure the incentives are aligned?
[00:19:59.040 --> 00:20:07.680]   Well, there are things that like, you can try, like, okay, in 30 years time, their performance
[00:20:07.680 --> 00:20:14.080]   will be assessed by a panel of people who look back and say, like, okay, was the policies that
[00:20:14.080 --> 00:20:19.360]   were being recommended here? Were they good or not? And perhaps the people who are part of this
[00:20:20.880 --> 00:20:27.440]   assembly, their pensions are getting paid on the basis of that assessment. And then secondly,
[00:20:27.440 --> 00:20:32.480]   the people in the 30 years time, their assessment, both their policies and their assessment of the
[00:20:32.480 --> 00:20:38.320]   previous, you know, the 30 years previous futures assembly get assessed by another assembly 30 years
[00:20:38.320 --> 00:20:42.480]   after that, and so on. And so it's like, and they're like, is some like math and economic
[00:20:42.480 --> 00:20:46.960]   analysis such like under certain conditions, this checks out, like, you have this like backwards
[00:20:46.960 --> 00:20:53.840]   chaining, where people in 1000 years time are evaluating the people in 970 years time who are
[00:20:53.840 --> 00:20:58.960]   evaluating the people in 910 years time. And like, can you get that to work? I mean, like, maybe in
[00:20:58.960 --> 00:21:02.640]   theory, I'm like, again, a little bit more sceptical in practice. But you know, I would love
[00:21:02.640 --> 00:21:05.920]   some country to try it and see it, see what happens. The other thing I should say, actually,
[00:21:05.920 --> 00:21:10.160]   it's just like, there is some evidence as well, that you can just get people to take the interests
[00:21:10.160 --> 00:21:14.080]   of future generations more seriously, by just telling them like, this is your role. There was
[00:21:14.080 --> 00:21:18.880]   one study that like got people to don't like put on ceremonial robes, and act as like trustees of
[00:21:18.880 --> 00:21:23.120]   the future. And they really did make like different policy recommendations than when they
[00:21:23.120 --> 00:21:28.560]   were just acting like on the basis of their own beliefs of self interest. Yeah. But if you are on
[00:21:28.560 --> 00:21:33.680]   that board that is judging these people 30 years before you, is there something you would be like,
[00:21:33.680 --> 00:21:38.480]   okay, this is the metric I care about most expected, I don't know, future GDP growth or
[00:21:38.480 --> 00:21:42.560]   something that you think would be the most informative about like how good those decisions
[00:21:42.560 --> 00:21:50.160]   were? Yeah, I mean, there are things you could do, like, you know, it could be, yeah, GDP,
[00:21:50.160 --> 00:21:55.040]   like of the country, it could be like, you could agree on like a set of metrics, like,
[00:21:55.040 --> 00:22:00.800]   you know, homelessness rate, perhaps, like some expert measure of like technological progress,
[00:22:00.800 --> 00:22:07.680]   I think you would absolutely want there to be expert assessment of like risk of catastrophe as
[00:22:07.680 --> 00:22:14.080]   well. We don't have this at the moment. But like, you could imagine like you have a panel of super
[00:22:14.080 --> 00:22:19.280]   forecasters who are predicting, like, what are what is the chance of like a war between great
[00:22:19.280 --> 00:22:23.920]   powers occurring in the next 10 years. And that gets aggregated into like a war index. I think
[00:22:23.920 --> 00:22:27.680]   that would be like a lot more important an index than like the stock market index, and we don't
[00:22:27.680 --> 00:22:31.040]   have it. But you could imagine that being kind of fed in as well. Because you wouldn't want
[00:22:31.040 --> 00:22:36.960]   something which is just like, oh, you're only like incentivizing economic growth at the expense of
[00:22:36.960 --> 00:22:40.800]   like tail risks. Would that be your objection to a scheme like Robin Hanson's about just
[00:22:40.800 --> 00:22:45.040]   maximizing expected future GDP using prediction markets and making decisions that way?
[00:22:45.040 --> 00:22:52.720]   Yeah, I mean, I think maximizing future GDP is more an idea I associate with Tyler Cohen.
[00:22:52.720 --> 00:22:54.560]   It could be any metric, but yeah.
[00:22:54.560 --> 00:22:58.400]   Okay, yeah, then Robin Hanson's idea of futarchy, where you've got vote on values,
[00:22:58.400 --> 00:23:04.720]   bet on beliefs, and people can just, you know, vote on what collection of goods they want to
[00:23:04.720 --> 00:23:09.280]   have, where GDP might be one of them, but also unemployment rate or also like whatever. Beyond
[00:23:09.280 --> 00:23:13.440]   that, it's just pure prediction markets. Again, it's something I'd love to see tried. And I think
[00:23:13.440 --> 00:23:19.360]   it's an idea in a vein of just like speculative political philosophy or like reasoning about like,
[00:23:19.360 --> 00:23:24.320]   how could a society be extraordinarily different? That is kind of very differently structured,
[00:23:24.320 --> 00:23:28.320]   that is incredibly neglected. Do I think it will work in practice? Like, probably not,
[00:23:28.320 --> 00:23:32.400]   most of these ideas wouldn't work in practice. You do have issues when it comes to prediction
[00:23:32.400 --> 00:23:36.320]   markets, where they can be gamed, or they're just simply not liquid enough. So it's pretty
[00:23:36.320 --> 00:23:40.960]   notable since he developed those ideas and really worked on prediction markets. There hasn't been
[00:23:40.960 --> 00:23:44.880]   like a lot of success at prediction markets, where there's at least there has been a fair
[00:23:44.880 --> 00:23:48.640]   amount more success on kind of forecasting. Now, perhaps you can solve those things.
[00:23:48.640 --> 00:23:54.800]   You, you know, have laws about what things can be voted on, like or predicted in the kind of
[00:23:54.800 --> 00:23:58.880]   grand prediction market. It's not all of those things. You may have government subsidies to
[00:23:58.880 --> 00:24:03.200]   to ensure there's enough liquidity. But like, overall, I think it's like pretty promising. And
[00:24:03.200 --> 00:24:07.040]   like, I'd love to see it, like, you know, you could try it out on like a city level or something,
[00:24:07.040 --> 00:24:10.640]   like, see how it goes.
[00:24:10.640 --> 00:24:26.640]   Let's take a scenario where the government starts taking the impact on the long term seriously and institute some reforms to integrate that perspective. And you can take an example, you can take a look at the environmental movement for an example of this, where, you know, there's environmental review boards that will try to assess the environmental impact of new projects, and they can
[00:24:27.360 --> 00:24:55.920]   repeal any proposals on this, based on this. And then so the impact here, at least in some states, and in some cases has been that groups that have no strong, plausible interest in the environment are able to game these sorts of mechanisms in order to, in some cases, prevent projects that would actually help the environment, especially when you're talking about something long termism, where it would take a long time to assess what the actual impact of something is, but then policy makers are tasked with evaluating the long term impacts of something.
[00:24:55.920 --> 00:25:04.640]   Are you worried that it would be a system that would be really easy to game by malicious actors? And like, what do you think happened wrong with the way that environmentalism was codified into law?
[00:25:04.640 --> 00:25:31.920]   Yeah, I mean, it's absolutely a worry, like, you know, as in potentially just a devastating worry, where, yeah, like, you create something, you're trying to represent future people that they're not actually around to lobby themselves, so it can just be co opted. And, yeah, my understanding of environmental impact statements has been similar. And it's kind of a similar reasons where it's not like the environment can represent itself. It can't say what like what its interests are.
[00:25:31.920 --> 00:25:59.760]   And so what is the right answer there? Like, again, it's super tough. Maybe there are these speculative proposals about, you know, having a representative body that like assesses these things and are like judged by people in 30 years time. That's kind of best we've got at the moment. But I think at the moment, it's just like, we need like a lot more thought to see if like any of these, any of these proposals, like would actually be robustly good for the long term, rather than just things that are like more narrowly focused.
[00:25:59.760 --> 00:26:25.360]   So regulation to have liability insurance for dangerous biolabs is not in any way like about trying to represent the interests of future generations. But it's very good for the long term. And so at the moment, I kind of primarily think that like, if long term is trying to change the government, like let's focus on like fairly narrow set of institutional changes that are very good for the long term, even though they're just not in the game of like representing the future.
[00:26:25.760 --> 00:26:30.960]   That's not to say I'm like opposed to all such things, but like there are major problems with implementation problems with any of them.
[00:26:30.960 --> 00:26:41.600]   I see. I guess we don't know how we would do it correctly. Do you at least have an idea of what went wrong with like, how could environmentalism been codified better? Like, why was that in some cases not a success?
[00:26:41.600 --> 00:26:51.920]   Yeah, honestly, I just don't have a good understanding of that. I don't know if it's intrinsic to the matter, or if you could have had some system that like wouldn't have been co-opted in the long term.
[00:26:52.320 --> 00:27:13.520]   Okay, so are corporations the most long term institutions we have today, like their incentive theoretically is to maximize future cash flow, which is at least explicitly and theoretically have a should have incentive to try to do the most good they can for their own company, which implies that yeah, if there's an existential risk, then the company can't be around.
[00:27:13.920 --> 00:27:27.040]   Yeah, I don't think so. I mean, I think different sorts of institutions have different kind of rates of decay associated with them. So a corporation, even a corporation that is in the kind of top 200 biggest companies, I think has a half life of only about 10 years.
[00:27:27.280 --> 00:27:39.280]   It's actually like, they're surprisingly short lived. Whereas if you look at say, universities, well, you know, Oxford and Cambridge are kind of 800 years old, I think it's University of Bologna is even older.
[00:27:39.280 --> 00:27:51.840]   These are like very long lived institutions. And you do get like, Corpus Christi Church at Oxford was making a decision about like, should it have some new tradition that would like, reoccur only every 400 years.
[00:27:51.840 --> 00:27:55.760]   And it's like, yeah, that's the sort of decision it makes, because it's such a long lived institution.
[00:27:56.000 --> 00:28:10.080]   Similarly, then religions like can be even longer lived again. And I think that like, that kind of natural half life really affects what sort of decisions a company like a company versus a university versus a religion, religious institution would make.
[00:28:10.080 --> 00:28:19.920]   But does that suggest maybe there's, is there something fragile and dangerous about trying to make your institution last for a long time? If companies try to do that, and they're not able to?
[00:28:20.000 --> 00:28:34.400]   Yeah, I mean, companies are composed of people, you know, is it in any the interest of a company to last for the long time? It's like, well, is it in the interest of the people who constitute the company, like the CEO and the board and the shareholders for that company to last a long time?
[00:28:34.400 --> 00:29:00.560]   And it's like, no, they don't particularly care. At least most, you know, some of them do, but most don't. Where there's other institutions. Yeah, I mean, I think it goes both ways, where, in some cases, like, this is the issue of lock in that I talk about a length and what we are the future is that you get these moments of plasticity, the formation of a new institution, whether that's the, you know, Christian Church or the Constitution of the United States.
[00:29:01.440 --> 00:29:26.880]   And that like locks in a certain set of norms. And that can be really good. If the set of norms and laws is good, like, I think the kind of US Constitution, I don't know, looking back, it seems kind of miraculous or something. It was like the first like, the first democratic constitution, as I understand it was like, created over the period of four months, really seems to have stood the test of time.
[00:29:27.360 --> 00:29:51.520]   Or alternatively, it could be like, extremely dangerous. There were obviously like, horrible things in that, I mean, we'll stick with the US Constitution, there were horrible things in there. And it was the like, legal right to slavery was proposed as like a constitutional amendment. If that had gone in, that would have been like a horrible, a horrible kind of piece of lock in. And so I think it's hard to answer in the abstract, because it really depends on like, what is the thing that's persisting for a long time?
[00:29:52.800 --> 00:29:59.360]   Did you say in the book that you expect our current era to be a moment of plasticity? Why do you think that is?
[00:29:59.760 --> 00:30:29.120]   Yeah, I think the specific time is a moment of plasticity for two reasons. One is that, so the world is completely is like unified in a way that's very historically unusual. You can communicate with anyone, basically instantaneously. And there's a great diversity of model views. So we can have arguments, we can fight, you know, like people coming on your podcast can like debate, like what's morally correct. It's plausible to me that one of like many different kind of sets of model views, like might kind of win out or become like the most popular, ultimately.
[00:30:29.160 --> 00:30:43.080]   And then secondly, so we're at this period where things really can change. But it's a moment of plasticity, because it also at least plausibly could come to an end, where I think there are various ways that in the coming decades or centuries, the model change that we're used to could end.
[00:30:43.720 --> 00:31:07.080]   So if there was a single global culture or world government, again, like before that, you know, if there was a global communist state, or globalist Nazi, global Nazi state, or other sort of world government, that preferred ideological conformity, then combined with technology, I think it becomes kind of unclear, like why that would, why would that end over the long term.
[00:31:07.480 --> 00:31:37.320]   And I think the key technology here is artificial intelligence, where the point in time, which may be sooner than we think, for all we know, where the rulers of the world are digital rather than biological, that could persist, you know, once you've got that plus kind of global hegemony of a single ideology, then there's not much reason, it seems to me for that set of values to like change over time, you've got immortal leaders, and no competition.
[00:31:37.560 --> 00:31:41.880]   And what are the other kind of sources of value change over time, I think they can be accounted for too.
[00:31:41.880 --> 00:31:56.360]   But isn't the fact that we are in a time of interconnectedness that won't last if we settle space, isn't that a reason for thinking that lock-in is not especially likely? If your overlords are many, many millions of light years away, then yeah, how well can they control you?
[00:31:57.080 --> 00:32:27.000]   Well, I think the worry I have is that the control will happen before the point of space settlement. So I think it's totally right that if, you know, one day we took to space, and there's many different settlements of different solar systems, and they, you know, are pursuing different visions of the good, then I think like, you know, you've made, you're probably going to maintain diversity for the very long time, I think it's like, just given the kind of physics of the matter, I think like once the solar system has been settled, then it's very hard for other solar, other civilizations to like come along.
[00:32:27.160 --> 00:32:48.440]   And conquer you, at least if we're like a period of level of technological maturity, where, you know, there aren't like new groundbreaking technologies to be discovered. But I'm worried that the control will happen earlier, like I'm worried the control might happen this century, within our lifetimes. I don't say that's very likely, but I think it's like seriously on the table 10% or something.
[00:32:51.080 --> 00:33:14.120]   Yeah, so going back to the long term of the long term as a movement, there's many instructive foundations that were set up about a century ago, like, you know, Rockefeller Foundation, Carnegie, four foundations, and they don't seem to be especially creative or impactful, especially today. Like, what do you think went wrong? Why was there, if not value drift, I guess just some decay of competence and leadership and insight?
[00:33:14.520 --> 00:33:44.480]   Yeah, I don't have super strong views about those particular examples. But two natural thoughts. One is the four organizations that want to persist a long time and keep having influence for a long time. Historically, they've tended to specify their goals in far too narrow terms. So one fun example is Benjamin Franklin, he invested 1000 pounds for each of the cities of Philadelphia and Boston, to pay out after 100 years, and then 200 years for different factions of the of the
[00:33:44.920 --> 00:34:06.400]   amount invested. But he specified it very specifically, it was to like, help blacksmith apprentices and so on. It's like, oh, man, this doesn't make much sense, like once you're in the year 2000. Whereas he could have said something much more general, like for the prosperity of people in Philadelphia for the prosperity of people in Boston, and then it would have had like, at least plausibly more impact.
[00:34:06.560 --> 00:34:36.160]   The second is just maybe a regression to the mean argument, where, you know, you have some new foundation, and it's doing like, extraordinary amount of good, as I think Rockefeller Foundation did, just over time, if you're saying that it's exceptional in some dimension, it's probably going to get more close to average on that dimension. And just as a matter of like, you're changing the people who are involved, if you've picked some, if there's some people who are like, exceptionally competent and farsighted, the next people just statistically are probably going to be less so.
[00:34:36.400 --> 00:35:05.960]   So going back to that dead hand problem, where if you if you specify your mission too narrowly, then yeah, it doesn't make sense in the future. Is there a trade off where if you just if you're too broad, then again, you have the ability of future actors, maybe they're malicious, or maybe they're just like, not as smart or as as creative as you are to take the movement in ways that you would not approve of. So if it just like do good for Philadelphia, but then yeah, it just turns into something that Ben Franklin would not have thought is good for Philadelphia.
[00:35:06.480 --> 00:35:35.920]   Yeah, I mean, it depends crucially on what your values and views are. Where if Benjamin Franklin, I don't think this was true. But if he was like, No, I just really care about blacksmiths apprentices, and nothing else matters, then he was correct to specify it in another way. But I think as a matter of fact, certainly my own values, but I think more generally, they tend to be quite a bit more broad than that. And then secondly, like, in general, I expect people in the future to be like smarter and more capable.
[00:35:36.440 --> 00:35:48.000]   Like that's certainly the trend over time. In which case, like, if you know, we're sharing similar broad goals, and they're implementing it in a different way, then I think probably they're right. And I'm wrong.
[00:35:48.320 --> 00:36:07.000]   Let's talk about how good we should expect the future to be. Have you come across Robin Hanson's argument that in the future, we'll all just be subsistence level ends, because there'll be a lot of competition. And then you'll just try to like minimize compute per digital person, which will just be a miserable, like barely living, barely worth living experience for every entity.
[00:36:07.760 --> 00:36:35.640]   Yeah, I'm familiar with the argument, but we should distinguish the idea that Ms are at subsistence level from the idea that they would have bad lives. So subsistence means that given there, yeah, you get a kind of balance of income per capita and population growth, such that if there were any poorer than deaths would be kind of outweigh outweighing kind of additional births, that actually doesn't tell you about their well being.
[00:36:36.120 --> 00:36:58.280]   So you could be very poor as an emulated being. However, you're just in bliss all of the time. That's like perfectly consistent with the Malthusian theory. And so it might seem still not like might still seem very far away from the best possible future. That future still could be like very good. Like those Ms while at subsistence still could have lives like 1000s of times better than ours.
[00:36:58.360 --> 00:37:23.920]   Speaking of being poor and happy, there is a very interesting section in the chapter where you mentioned the study you had commissioned, where you were trying to find out if people in like the developing world that life's worth living. And it turns out that 19% of Indians would not want to relive their life every moment. But I think it was 31% of Americans said that they would not. Yeah. So why are Indians seemingly much happier at less than a 10th of the GDP per capita?
[00:37:24.800 --> 00:37:49.920]   Yeah, I think the numbers are lower than that, for memory, at least, I think it was more like, and it depends exactly on the question asked. But for memory, it's something more like 9% of Indians, like wouldn't want to live their lives again, if they had the option, and like 13% of Americans or something. But you are right that on this metric of how many people are happy to have lived, how many people think that they are not happy to have lived.
[00:37:50.840 --> 00:38:19.400]   The Indians we surveyed, were more kind of optimistic about their lives, like happier with their lives than people in the US were. Honestly, I just don't want to generalize too far from that, because we were sampling competitively poor Americans, competitively well off Indians. So perhaps it's just a sample effect. There are also like weird interactions with Hinduism and the belief in reincarnation that I think like, could, you know, could just like mess up the kind of generalizability of this as well.
[00:38:19.480 --> 00:38:35.800]   On one hand, yeah. So I basically don't want to like throw any strong conclusion from that. But it is pretty striking, as a piece of information, given that normally what you find when you look at people's well being is that richer countries are, you know, considerably happier than poorer countries, on average, at least.
[00:38:35.800 --> 00:38:45.320]   Yeah, I guess, but you do generalize in the sense that you use it as evidence that most lives are worth living that most lives today are worth living, right?
[00:38:45.760 --> 00:39:07.600]   Yeah, exactly. So I put together various bits of evidence, where a very approximately like 10% of people in the United States, 10% of people in India seem to think that their lives are negative, you know, they wouldn't they think they contain more suffering than happiness, they wouldn't want to be reborn and live the same life if they could.
[00:39:09.600 --> 00:39:32.280]   And if you look at like other studies as well, like, there's another study that just looks at people in United States, or United States and other generally rich countries, and asks them about how much of their conscious life they'd be willing, they would want to skip if they could, whereby skipping, it just means like you blink and then you come to the end of whatever activity you're engaging with.
[00:39:32.600 --> 00:39:47.240]   So perhaps I hate this podcast so much that I would just rather be unconscious than be talking to you, in which case I would have the option of skipping, obviously not to, but I'd have the option of skipping and, you know, it would be 30 minutes later, and it would all be done.
[00:39:48.360 --> 00:40:13.560]   If you look at that, and then also ask people about like, the trade offs they would be willing to make as a measure of intensity of how much they're enjoying or how much they're not enjoying a certain experience, you get the conclusion that like, yeah, from memory, again, a little over 10% of people were on balance, regarded their life as that day, in fact, that was being surveyed as worse than if they'd been unconscious the entire day.
[00:40:13.840 --> 00:40:33.840]   Jumping topics here a little bit, on the 80,000 Hours podcast, you said that you expect scientists who are explicitly trying to maximize their impact, that trying to do so might have an adverse impact because yeah, they might be ignoring the foundational research that wouldn't be obvious in this way of thinking, but might be more important.
[00:40:33.840 --> 00:40:43.840]   Do you think this could be a general problem with long termism, that if you're like, really trying to like, find the most important things that are important long term, you might be missing things that yeah, wouldn't be obvious thinking this way?
[00:40:43.840 --> 00:41:12.840]   Yeah, I think it's a risk. So among the ways that people could argue, you know, against my general set of views, one way that I find, so you know, I argue that, in general, we should be doing, like fairly specific and targeted things like trying to make AI safe, trying to well govern the lives of AI, trying to reduce like worst case pandemics that could kill us all, trying to prevent third world war, trying to ensure that good values are promoted, and avoid value lock in.
[00:41:12.840 --> 00:41:27.840]   But what some people could argue, and people like Tyler Cowen, Patrick Collison, I think like take this line is, man, it's just very hard to predict the kind of future, the like, future impact of your actions.
[00:41:27.840 --> 00:41:41.840]   And it's kind of a mugs game to even try. So instead, what you should do is just look at like what things have had done loads of good kind of consistently in the past, and try to just do the same things.
[00:41:41.840 --> 00:42:02.840]   And then they in particular might argue that that means technological progress, it might mean boosting economic growth. Yeah, I guess like I just dispute that. But it's not something I feel like I can give a completely knockdown argument to because it's about the, you know, when will we find out who's right, like maybe in, you know, 1000 years time or something.
[00:42:02.840 --> 00:42:27.840]   But I just, one piece of evidence is just like the success of forecasters in general. Again, the fact that like, I mean, this also was true for Tyler Cowen, but like, you know, people in effective altruism were just realizing that the coronavirus pandemic was going to be a really big deal from like very early stage was worrying about pandemics like far in advance.
[00:42:27.840 --> 00:42:41.840]   I think there are some things that are just like actually quite predictable. So Moore's law has held up for over 70 years. I think the idea that like AI systems are going to get much, much larger, the leading models are going to get more and more powerful.
[00:42:41.840 --> 00:43:00.840]   That's like on trend. Similarly, the idea that like, we will be soon be able to develop viruses of like unprecedented destructive power. Again, that's just like, I think that's not actually that controversial a claim. And so even though I think that, yeah, for loads of things, it's just very hard to predict and they're going to be like tons of surprises.
[00:43:00.840 --> 00:43:14.840]   But there are some things and I think especially when it comes to like fairly long standing technological trends where we really can make reasonable predictions, at least about like the range of possibilities that are really on the table.
[00:43:14.840 --> 00:43:26.840]   But it kind of sounds like you're saying the things we know are important now are important now. If something did turn out like 1000 years looking back to be very important. Yeah, it wouldn't be salient to us now.
[00:43:26.840 --> 00:43:47.840]   What I was saying with me versus Patrick Collison and Tyler Cohen, like who is correct? Well, in some sense, you can, we will only get that information in like 1000 years time. Because we're talking about which strategy is going to have a bigger impact on the long term, we might get like suggestive evidence earlier.
[00:43:47.840 --> 00:44:10.840]   So if we're if kind of me and others engaging in long termism are making kind of specific measurable forecasts about what is going to happen with AI or advances in biotechnology, and they're not able to take action, such that we are relatively clearly reducing certain risks.
[00:44:10.840 --> 00:44:22.840]   I think that's like pretty good evidence in favor of our strategy. Whereas if in contrast, they're doing like, well, all sorts of stuff like not really trying to make have like firm predictions about what's going to happen.
[00:44:22.840 --> 00:44:33.840]   But then things just pop out of that where we think, oh, that was like really good from a long term, but future perspective, you know, after, let's say we measure this kind of in 10 years time, well, that would be good evidence for their view.
[00:44:33.840 --> 00:44:52.840]   What you're saying earlier about the contingency and technology implies that even given their worldview, maybe you should think that technological so even if you're trying to maximize what in the past is at the most impact, if what's had the most impact in the past is changing values, then yeah, then economic growth might not be the most important thing, or like trying to change the rate of economic growth.
[00:44:52.840 --> 00:45:03.840]   Yeah, I mean, I really do take seriously the argument of like, look at how people have acted in the past, especially for people who are trying to make a long lasting impact, what things they do that made sense and whatnot.
[00:45:03.840 --> 00:45:16.840]   So towards the end of the 19th century, John Stuart Mill, and the other early utilitarians had this like long termist little wave where they started taking the interests of future generations very seriously.
[00:45:16.840 --> 00:45:25.840]   And their main concern was that Britain would run out of coal, and therefore future generations would be impoverished.
[00:45:25.840 --> 00:45:31.840]   And it's pretty striking, because they had a very bad understanding of how the economy works.
[00:45:31.840 --> 00:45:38.840]   They, you know, hadn't didn't predict that, well, we would be able to transition away from coal, because of continued innovation.
[00:45:38.840 --> 00:45:43.840]   And secondly, they had enormously wrong views about how much coal and fossil fuels there were in the world.
[00:45:43.840 --> 00:45:48.840]   And so that particular action just didn't make any sense, given like, what we know now.
[00:45:48.840 --> 00:46:03.840]   In fact, that particular action of trying to keep coal in the ground, given Britain at the time, where, to be clear, we're talking about much lower amounts of coal, so small amounts of coal, so the climate change effect is like not, is kind of negligible at that level.
[00:46:03.840 --> 00:46:05.840]   You know, it actually probably would have been harmful.
[00:46:05.840 --> 00:46:19.840]   But we could look at other things that John Stuart Mill could have done, such as like promoting better values, he like campaigned for women's suffrage, he was the first British MP, I think, in fact, even the first politician in the world to promote women's suffrage.
[00:46:19.840 --> 00:46:22.840]   That seems to be pretty good, that seems to have stood the test of time.
[00:46:22.840 --> 00:46:29.840]   And, you know, that's one historical data point, but like, potentially, we can draw a kind of more general lesson there.
[00:46:29.840 --> 00:46:36.840]   Do you think the ability of global policymakers to come to a consensus is, on net, a good or a bad thing?
[00:46:36.840 --> 00:46:46.840]   I mean, on the positive, maybe it helps prevent some dangerous tech from taking off, but on the negative side, it prevented human challenge trials, maybe it causes some sort of lock-in in the future.
[00:46:46.840 --> 00:46:49.840]   On net, what do you think about that trend?
[00:46:49.840 --> 00:46:59.840]   Yeah, the question of global integration, you're absolutely right, it has two, it's double-sided, where on the one hand, it can help us reduce, you know, global catastrophic risks.
[00:46:59.840 --> 00:47:10.840]   So the fact that the world was able to come together and ban chlorofluorocarbons was, you know, one of the great events of the last 50 years, allowing the hole in the ozone layer to repair itself.
[00:47:10.840 --> 00:47:22.840]   But on the other hand, like, if it just means we all converge towards some kind of monoculture and we lose out on diversity, well, that's like, yeah, that's like potentially pretty bad.
[00:47:22.840 --> 00:47:26.840]   We could actually lose out on like most possible value that way.
[00:47:26.840 --> 00:47:31.840]   And I think the solution is like, you do the big good bits and don't have the bad bits.
[00:47:31.840 --> 00:47:50.840]   So, you know, in a liberal constitution, you know, you can have like, you can have a country that is bound in certain ways by its constitution and by certain laws, yet still enables like a flourishing diversity of moral thought and different ways of life.
[00:47:50.840 --> 00:48:10.840]   And so similarly in the world, you could have very strong, you know, regulation and treaties in just that deal with certain global public goods, like mitigation of climate change, prevention of development of like the next generation of weapons of mass destruction.
[00:48:10.840 --> 00:48:22.840]   Without thereby having like some very strong arm like global government that implements, you know, a particular vision of the world, which way are we going?
[00:48:22.840 --> 00:48:30.840]   At the moment, it seems to me like we've been going in like a pretty good and like not too worrying direction, but I think that could change.
[00:48:30.840 --> 00:48:42.840]   Yeah, it seems the historical trend is when you have a federated political body, where even if constitutionally the central power is constrained, that over time, they just tend to gain more power.
[00:48:42.840 --> 00:48:47.840]   Like you can look at the US, you can look at the European Union, but yeah, that seems to be the trend.
[00:48:47.840 --> 00:48:53.840]   Yeah, and I think that's like, again, depending on like the culture that's embodied there, it's potentially a worry.
[00:48:53.840 --> 00:48:59.840]   It might not be if the culture itself is like liberal and promoting of moral diversity and moral change and moral progress.
[00:48:59.840 --> 00:49:01.840]   But that needn't be the case.
[00:49:01.840 --> 00:49:11.840]   Now, your theory of moral change implies that after a small group starts advocating for a specific idea, it may take like a century or more before that idea reaches common purchase.
[00:49:11.840 --> 00:49:17.840]   To the extent that you think this is a very important century, I know you have disagreements about that with others.
[00:49:17.840 --> 00:49:27.840]   But yeah, to the extent that's true, does that mean that maybe there just isn't enough time for long-termism to gain power that way by changing moral values?
[00:49:27.840 --> 00:49:39.840]   Yeah, I mean, there are lots of people I know and respect very well who think that artificial general intelligence will very, very likely lead to singularity level technological progress.
[00:49:39.840 --> 00:49:47.840]   So extremely rapid kind of rates of technological progress, and that that will happen more likely than not within the next 10, 20 years.
[00:49:47.840 --> 00:49:49.840]   And if so, then you're right.
[00:49:49.840 --> 00:49:54.840]   Value changes are something that pays off kind of slowly over time.
[00:49:54.840 --> 00:49:58.840]   I mean, in the world today, so I talk about moral change taking centuries.
[00:49:58.840 --> 00:50:00.840]   That's definitely more to do historically.
[00:50:00.840 --> 00:50:02.840]   I think we can have much faster change.
[00:50:02.840 --> 00:50:10.840]   So, you know, the growth of the effective altruism movement is something I know well, where that's growing at something like 30% per year.
[00:50:10.840 --> 00:50:13.840]   Compound returns mean that like, actually, it's not long.
[00:50:13.840 --> 00:50:15.840]   You know, that's not growth.
[00:50:15.840 --> 00:50:18.840]   That's not change that happens on the order of kind of centuries.
[00:50:18.840 --> 00:50:27.840]   And I think if you look at other model movements like daylights movement, very fast, very fast model change by historical standards.
[00:50:27.840 --> 00:50:36.840]   So I think, yes, if you're thinking, look, we've got 10 years till the end of history, then probably don't just very broadly find promote better values.
[00:50:36.840 --> 00:50:47.840]   But I think we should have at least a very significant probability mass on the idea that we will not hit some like historical end of this century.
[00:50:47.840 --> 00:50:53.840]   And in those worlds, then, you know, promoting better values could pay off like very well.
[00:50:53.840 --> 00:50:57.840]   Have you heard of slime old time old potato diet?
[00:50:57.840 --> 00:51:00.840]   I have indeed heard of slime old time old potato diet.
[00:51:00.840 --> 00:51:04.840]   And I was tempted. I was tempted as a gimmick to try it.
[00:51:04.840 --> 00:51:13.840]   But then onto something, because as I'm sure you know, the potatoes close to a superfood and you could survive indefinitely on just buttery mashed potatoes.
[00:51:13.840 --> 00:51:19.840]   If you occasionally supplement with something like lentils or oats.
[00:51:19.840 --> 00:51:24.840]   Yeah, OK. Interesting. A question about your career. Why are you still a professor?
[00:51:24.840 --> 00:51:33.840]   Does it does it still allow you to do the things that you would have otherwise have been doing, like converting more SBFs and making moral philosophy arguments for EA?
[00:51:33.840 --> 00:51:39.840]   Or yeah, you're curious about that. Yeah, I mean, it's very open to me what I should do.
[00:51:39.840 --> 00:51:48.840]   But my best guess and, you know, I do spend significant amounts of time co-founding organizations or being on the board of those organizations I've helped to set up.
[00:51:48.840 --> 00:51:56.840]   And more recently, you know, working very closely with the Future Fund, some SPS new foundation.
[00:51:56.840 --> 00:52:03.840]   And helping them do as much good as possible. That being said, if it's like a single best guess for what I ought to do kind of longer term.
[00:52:03.840 --> 00:52:09.840]   And certainly the place to my strengths better. You know, it's like developing ideas, trying to get the big picture roughly right.
[00:52:09.840 --> 00:52:20.840]   And then communicating them in a way that's understandable and get small people to get off their seats and try to do a lot of good, especially for the long term.
[00:52:20.840 --> 00:52:29.840]   And I think I've had like a lot of impact that way. And from that perspective, having an Oxford professorship is pretty helpful.
[00:52:29.840 --> 00:52:37.840]   Sure. Yeah. By the way, why do you think that there's, you mentioned in the book and elsewhere, that there's a scarcity of people thinking about these big picture questions.
[00:52:37.840 --> 00:52:41.840]   About, yeah, how contiguous history, how are people happy generally?
[00:52:41.840 --> 00:52:47.840]   Like, well, are these just questions that are too hard for other people to, or they just don't care enough? Like what's going on?
[00:52:47.840 --> 00:52:49.840]   Why are there so many people talking about this?
[00:52:49.840 --> 00:52:55.840]   I just think there's many, many issues that are enormously important, but are just not incentivized.
[00:52:55.840 --> 00:53:02.840]   Basically anywhere in the world where companies don't incentivize work on them because they're like too big picture.
[00:53:02.840 --> 00:53:08.840]   So some of these are like, yeah, is the future good rather than bad? If there was a global civilizational collapse, would we recover?
[00:53:08.840 --> 00:53:13.840]   You know, how likely is essentially these long stagnation? It's almost no work done on any of these topics.
[00:53:13.840 --> 00:53:16.840]   And yeah, companies aren't interested to grand and scale.
[00:53:16.840 --> 00:53:23.840]   And then academia, I think it's just developed a culture where you don't tackle such problems in academia.
[00:53:23.840 --> 00:53:31.840]   Partly that's because they fall through cracks of different disciplines and partly because they just seem too big or too grand or too speculative.
[00:53:31.840 --> 00:53:40.840]   Whereas academia is much more in the mode in general of like making these kind of incremental gains in our understanding.
[00:53:40.840 --> 00:53:42.840]   But it didn't always used to be that way.
[00:53:42.840 --> 00:53:50.840]   Like if you look back before the kind of institutionalization of academic research, philosophers would have all, you know,
[00:53:50.840 --> 00:53:56.840]   you weren't a real philosopher unless you had some grand unifying theory of like not just ethics and political philosophy,
[00:53:56.840 --> 00:54:02.840]   but also metaphysics and logic and epistemology and that probably the natural sciences to like economics.
[00:54:02.840 --> 00:54:08.840]   And, you know, I think I'm not saying that like all of academic inquiry should be like that.
[00:54:08.840 --> 00:54:15.840]   But should there be at least some people whose role is to like really think about the big picture? And I think yes.
[00:54:15.840 --> 00:54:20.840]   Will I be able to send my kids to Macaskill University? What's the status on that project?
[00:54:20.840 --> 00:54:25.840]   I'm really pretty interested in the idea of having, yeah, creating a new university.
[00:54:25.840 --> 00:54:32.840]   There is a project that I've been in discussion about with another person who's like very excited about making it happen.
[00:54:32.840 --> 00:54:35.840]   Will it go ahead? I mean, time will see, time will tell.
[00:54:35.840 --> 00:54:41.840]   But yeah, I just think you can both do education like far, far better than currently exists.
[00:54:41.840 --> 00:54:45.840]   I also think you can probably do the search far, far better than currently exists.
[00:54:45.840 --> 00:54:53.840]   It's extremely hard to kind of break in to giving kind of very, especially like creating something that's very prestigious
[00:54:53.840 --> 00:54:59.840]   because the leading universities are almost all kind of hundreds of years old, but like maybe it's possible.
[00:54:59.840 --> 00:55:05.840]   And I think it could generate enormous amounts of value if we were able to pull it off.
[00:55:05.840 --> 00:55:10.840]   Yeah, yeah. Okay, excellent. All right, so the book is "What We Owe the Future."
[00:55:10.840 --> 00:55:13.840]   And I understand pre-orders help a lot grow, right?
[00:55:13.840 --> 00:55:19.840]   So yeah, it was such an interesting read because how often does somebody write a book about even the questions they consider to be the most important,
[00:55:19.840 --> 00:55:22.840]   even if they're not the most important questions?
[00:55:22.840 --> 00:55:27.840]   Yeah, just kind of like big picture thinking, but you're also looking at any like very specific questions and issues that come up.
[00:55:27.840 --> 00:55:30.840]   It was just super interesting read.
[00:55:30.840 --> 00:55:33.840]   Great. Well, thank you so much.
[00:55:33.840 --> 00:55:35.840]   Anything that they might need to know?
[00:55:35.840 --> 00:55:41.840]   Yeah, sure. So "What We Owe the Future" out on August 16th in the U.S. and 1st of September in the United Kingdom.
[00:55:41.840 --> 00:55:44.840]   If you want to follow me on Twitter, I'm @willmcaskill.
[00:55:44.840 --> 00:55:50.840]   If you want to try and use your time and money to do good, Giving What We Can is an organization that encourages people to take a pledge
[00:55:50.840 --> 00:55:55.840]   to give a significant fraction of their income, 10% or more, to the charities that do the most good.
[00:55:55.840 --> 00:55:57.840]   It has a list of recommended charities.
[00:55:57.840 --> 00:56:05.840]   80,000 Hours, if you want to use your career to do good, is a place to go for advice on what careers really have the biggest impact at all.
[00:56:05.840 --> 00:56:07.840]   And they provide one-on-one coaching too.
[00:56:07.840 --> 00:56:15.840]   And so, yeah, all of these ways are kind of, if you're feeling inspired, if you think, "Look, I actually really want to do good in the world.
[00:56:15.840 --> 00:56:21.840]   I care about future people, and I want to help make their lives go better," then as well as Leading What We Owe the Future,
[00:56:21.840 --> 00:56:26.840]   Giving What We Can and 80,000 Hours are the sources you can go to and get involved.
[00:56:26.840 --> 00:56:28.840]   Awesome. Thanks so much for coming on the podcast. This was a lot of fun.
[00:56:28.840 --> 00:56:31.840]   Thanks so much. Yeah, I loved it. Cool.
[00:56:31.840 --> 00:56:34.840]   Thanks for watching. I hope you enjoyed that episode.
[00:56:34.840 --> 00:56:42.840]   If you did, and you want to support the podcast, the most helpful thing you can do is share it on social media and with your friends.
[00:56:42.840 --> 00:56:49.840]   Other than that, please like and subscribe on YouTube and leave good reviews on podcast platforms.
[00:56:49.840 --> 00:56:51.840]   Cheers. I'll see you next time.
[00:56:51.840 --> 00:57:01.840]   [Music]

