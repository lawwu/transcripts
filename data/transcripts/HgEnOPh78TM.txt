
[00:00:00.000 --> 00:00:02.000]   [MUSIC PLAYING]
[00:00:02.000 --> 00:00:04.920]   Who all those people are out there skipping my talk?
[00:00:04.920 --> 00:00:08.720]   But it's being loud.
[00:00:08.720 --> 00:00:12.680]   Anyway, I want to talk about AI, the future,
[00:00:12.680 --> 00:00:16.240]   and all the other stuff, because there's just a lot of stuff
[00:00:16.240 --> 00:00:18.920]   happening right now in the world.
[00:00:18.920 --> 00:00:23.280]   I want to start by imagining that human history, all
[00:00:23.280 --> 00:00:28.960]   of human history is written in an 800-page book
[00:00:28.960 --> 00:00:33.960]   with 250 years covered on each page.
[00:00:33.960 --> 00:00:36.120]   And if we took out all the pages and laid them out,
[00:00:36.120 --> 00:00:37.760]   it would look something like this.
[00:00:37.760 --> 00:00:41.280]   And so most of this--
[00:00:41.280 --> 00:00:42.960]   this is a very boring book.
[00:00:42.960 --> 00:00:45.920]   If you're reading it, most of it is just hunter-gatherers,
[00:00:45.920 --> 00:00:46.840]   hunting and gathering.
[00:00:46.840 --> 00:00:48.880]   That's it.
[00:00:48.880 --> 00:00:50.720]   Only when you get to the second to last row
[00:00:50.720 --> 00:00:53.840]   do you even have the very earliest cities
[00:00:53.840 --> 00:00:56.960]   in agriculture, still mostly hunting and gathering.
[00:00:56.960 --> 00:00:58.880]   And only on the bottom row do you
[00:00:58.880 --> 00:01:01.360]   have recorded history when writing starts happening.
[00:01:01.360 --> 00:01:02.960]   So when we talk about history, we're
[00:01:02.960 --> 00:01:04.200]   talking about the bottom row.
[00:01:04.200 --> 00:01:08.120]   Everything else is prehistory, prehistoric.
[00:01:08.120 --> 00:01:10.000]   Now, even within the bottom row, most of that
[00:01:10.000 --> 00:01:11.920]   is incredibly ancient.
[00:01:11.920 --> 00:01:14.160]   It's only on page-- remember, out of 800.
[00:01:14.160 --> 00:01:18.640]   Page 790 is like Buddha, right?
[00:01:18.640 --> 00:01:21.440]   And like, you know, really long ago shit.
[00:01:21.440 --> 00:01:23.760]   Page 793 is when AD starts.
[00:01:23.760 --> 00:01:26.340]   So you're reading the book, and it's like the last eight pages.
[00:01:26.340 --> 00:01:27.600]   It's like epilogue AD.
[00:01:27.600 --> 00:01:29.720]   It's like the very end.
[00:01:29.720 --> 00:01:33.280]   And so the reason I show you this
[00:01:33.280 --> 00:01:35.720]   is because there's a crazy trend that's
[00:01:35.720 --> 00:01:37.260]   happening here that sometimes you
[00:01:37.260 --> 00:01:39.080]   need to just zoom out to see.
[00:01:39.080 --> 00:01:41.200]   So imagine we went and took a time machine,
[00:01:41.200 --> 00:01:46.360]   and we grabbed a caveman from, I don't know, page 621,
[00:01:46.360 --> 00:01:49.040]   and we brought him forward in the time machine
[00:01:49.040 --> 00:01:52.520]   exactly one page and then let him out, 250 years.
[00:01:52.520 --> 00:01:55.280]   He'd be like, uh-huh, yeah, it's the same thing, basically,
[00:01:55.280 --> 00:01:57.020]   that I had before, no big deal.
[00:01:57.020 --> 00:01:59.780]   But as you get closer to the end of the book,
[00:01:59.780 --> 00:02:02.300]   taking someone a leap forward one page
[00:02:02.300 --> 00:02:04.820]   becomes a bigger and bigger, crazier thing.
[00:02:04.820 --> 00:02:09.820]   So like George Washington, right, born on page 799,
[00:02:09.820 --> 00:02:13.180]   right, a little over 250 years ago,
[00:02:13.180 --> 00:02:15.620]   what would it be like to do the same thing we did with the cave
[00:02:15.620 --> 00:02:18.740]   man to him, to take him in the time machine
[00:02:18.740 --> 00:02:20.740]   and bring him to today?
[00:02:20.740 --> 00:02:22.580]   To understand that, let's look at--
[00:02:22.580 --> 00:02:28.520]   compare page 1 through 799 to the very last page, our page.
[00:02:28.520 --> 00:02:34.680]   So on page 1 through 799--
[00:02:34.680 --> 00:02:40.340]   think this isn't working, but that's a you problem--
[00:02:40.340 --> 00:02:45.160]   we have under a billion people for 799 pages.
[00:02:45.160 --> 00:02:48.880]   On page 800 alone, we crossed the 1, 2, 3, 4, 5, 6,
[00:02:48.880 --> 00:02:52.200]   and 7 billion person marks.
[00:02:52.200 --> 00:02:57.720]   On page 1 through 799, what is transportation like?
[00:02:57.720 --> 00:03:01.640]   Sailboats, horses, walking, right,
[00:03:01.640 --> 00:03:03.220]   just basic things like that.
[00:03:03.220 --> 00:03:05.560]   And that's what George Washington would be used to.
[00:03:05.560 --> 00:03:09.480]   On page 800, we're going to the moon, right, cars, planes,
[00:03:09.480 --> 00:03:11.200]   submarines, trains.
[00:03:11.200 --> 00:03:13.040]   All of that is all entirely page 800.
[00:03:13.040 --> 00:03:17.640]   Communication, talking mostly, and then writing,
[00:03:17.640 --> 00:03:19.360]   maybe writing letters.
[00:03:19.360 --> 00:03:21.320]   Long distance communication, like a smoke signal
[00:03:21.320 --> 00:03:21.920]   or something.
[00:03:21.920 --> 00:03:26.200]   Of course, today we have phones and TV and FaceTime
[00:03:26.200 --> 00:03:27.320]   and all of that, right?
[00:03:27.320 --> 00:03:28.920]   So you can just keep going here.
[00:03:28.920 --> 00:03:31.480]   And oh, he died too early.
[00:03:31.480 --> 00:03:32.520]   He died too early.
[00:03:32.520 --> 00:03:38.280]   OK, well, anyway, the point is that this is all before you
[00:03:38.280 --> 00:03:42.280]   even show him the internet and talk about general relativity
[00:03:42.280 --> 00:03:43.920]   and talk about space travel.
[00:03:43.920 --> 00:03:49.040]   And you show him the crazy ways we're using energy.
[00:03:49.040 --> 00:03:53.960]   Fossil fuels era is entirely a page 800 phenomenon, right?
[00:03:53.960 --> 00:03:59.080]   There was no such thing as using fossil fuels
[00:03:59.080 --> 00:04:00.400]   on any of the pages before it.
[00:04:00.400 --> 00:04:02.920]   So people in his world, they just
[00:04:02.920 --> 00:04:06.360]   had water wheels and basic windmills
[00:04:06.360 --> 00:04:08.560]   and mostly pushing stuff with their arms.
[00:04:08.560 --> 00:04:11.200]   So you take him here, and I don't
[00:04:11.200 --> 00:04:14.400]   think he would just be mind blown or shocked.
[00:04:14.400 --> 00:04:16.920]   I think he would actually die, which you knew already
[00:04:16.920 --> 00:04:18.440]   because you saw the slide.
[00:04:18.440 --> 00:04:20.480]   And so this is dead George Washington, right?
[00:04:20.480 --> 00:04:22.640]   And so the trend here is that the farther you
[00:04:22.640 --> 00:04:25.000]   go into the book, the crazier it would
[00:04:25.000 --> 00:04:27.520]   be to take someone one page forward until eventually you
[00:04:27.520 --> 00:04:30.160]   get someone from 799, bring them out here, and show them
[00:04:30.160 --> 00:04:32.840]   the world, and they keel over because it's so shocking.
[00:04:32.840 --> 00:04:35.320]   So the reason I like to start with this
[00:04:35.320 --> 00:04:39.920]   is because it's human nature to think we live in special times.
[00:04:39.920 --> 00:04:40.800]   This time is special.
[00:04:40.800 --> 00:04:42.520]   These are the end of days, right?
[00:04:42.520 --> 00:04:45.640]   This has never been like this before.
[00:04:45.640 --> 00:04:49.480]   And almost all the time, people are wrong.
[00:04:49.480 --> 00:04:51.800]   So we have this instinct to override that thought
[00:04:51.800 --> 00:04:52.920]   and think, no, it's naive.
[00:04:52.920 --> 00:04:54.500]   Everyone thinks their time is special.
[00:04:54.500 --> 00:04:55.600]   This is just another time.
[00:04:55.600 --> 00:04:57.240]   And actually, no, it's not.
[00:04:57.240 --> 00:05:01.160]   This page we're on is an anomaly in every way.
[00:05:01.160 --> 00:05:03.480]   It can be an anomaly.
[00:05:03.480 --> 00:05:06.040]   And so what does that mean about the future, right?
[00:05:06.040 --> 00:05:10.360]   What does that imply about where we're going,
[00:05:10.360 --> 00:05:11.840]   which is page 801, right?
[00:05:11.840 --> 00:05:15.120]   If taking George here was that extreme,
[00:05:15.120 --> 00:05:17.400]   well, if things were moving quicker,
[00:05:17.400 --> 00:05:19.920]   maybe us going forward just a few decades
[00:05:19.920 --> 00:05:22.560]   would be the same level of shock that George Washington
[00:05:22.560 --> 00:05:24.720]   experienced coming 250 years.
[00:05:24.720 --> 00:05:27.640]   In our lifetimes, we might witness that level of change.
[00:05:27.640 --> 00:05:29.480]   And you can see this all around.
[00:05:29.480 --> 00:05:32.360]   There's a lot of paradigm shifts happening.
[00:05:32.360 --> 00:05:34.400]   So driverless cars, taking a little longer,
[00:05:34.400 --> 00:05:36.280]   taking a little longer than we thought maybe.
[00:05:36.280 --> 00:05:38.080]   But the bigger fact here, if you zoom out,
[00:05:38.080 --> 00:05:45.040]   is the driving era basically was about 120, 130 years long.
[00:05:45.040 --> 00:05:46.400]   And we're at the very end of it.
[00:05:46.400 --> 00:05:48.160]   The driving era is about to be over.
[00:05:48.160 --> 00:05:50.960]   That's crazy.
[00:05:50.960 --> 00:05:54.080]   Probably all saw this thing recently.
[00:05:54.080 --> 00:05:58.880]   The rectangle glass devices era may be coming to an end.
[00:05:58.880 --> 00:06:01.920]   Maybe you're going to see a picture or a movie scene,
[00:06:01.920 --> 00:06:04.200]   and there's going to be a bunch of people on the subway
[00:06:04.200 --> 00:06:05.160]   on their phones.
[00:06:05.160 --> 00:06:07.600]   And you're going to say, oh, that's the 2010s or 2020s.
[00:06:07.600 --> 00:06:08.680]   That's the only time it could be,
[00:06:08.680 --> 00:06:10.720]   because that was the era when that happened.
[00:06:10.720 --> 00:06:12.280]   When you're in it, it feels like this is normal.
[00:06:12.280 --> 00:06:14.680]   But if you zoom out, you're like, wow, that's an era,
[00:06:14.680 --> 00:06:17.040]   and we're moving out of that maybe.
[00:06:17.040 --> 00:06:19.360]   And of course, this is just child's play compared
[00:06:19.360 --> 00:06:22.120]   to what would be coming next, which is going straight--
[00:06:22.120 --> 00:06:24.920]   going from immersing yourself with a mask
[00:06:24.920 --> 00:06:27.600]   to just brain-machine interface, actually
[00:06:27.600 --> 00:06:31.080]   having your visual cortex taken over by a brain-machine
[00:06:31.080 --> 00:06:31.560]   interface.
[00:06:31.560 --> 00:06:34.720]   So you actually see something different out of your real eyes,
[00:06:34.720 --> 00:06:36.240]   and you don't need any more devices.
[00:06:36.240 --> 00:06:39.080]   You just have this device in your head,
[00:06:39.080 --> 00:06:41.840]   and you can think with each other.
[00:06:41.840 --> 00:06:45.080]   And you want to show someone a visual in your head,
[00:06:45.080 --> 00:06:46.880]   you can broadcast it onto the screen.
[00:06:46.880 --> 00:06:48.160]   You want to play your music.
[00:06:48.160 --> 00:06:49.880]   Instead of just saying, I want to play it
[00:06:49.880 --> 00:06:53.400]   from my headphones or my speaker or my phone,
[00:06:53.400 --> 00:06:55.640]   you can now say, I also want to play it directly
[00:06:55.640 --> 00:06:59.480]   into my auditory cortex with no sound needed at all.
[00:06:59.480 --> 00:07:01.120]   So this is coming.
[00:07:01.120 --> 00:07:03.760]   It doesn't seem real, like a lot of things,
[00:07:03.760 --> 00:07:05.320]   because it's hard to imagine things
[00:07:05.320 --> 00:07:07.760]   that are totally different than anything we've experienced.
[00:07:07.760 --> 00:07:09.520]   But this is going to come.
[00:07:09.520 --> 00:07:11.920]   A lot of you will have one of these in your head,
[00:07:11.920 --> 00:07:15.120]   I'm pretty sure, before you die.
[00:07:15.120 --> 00:07:16.480]   How about genetic engineering?
[00:07:16.480 --> 00:07:19.040]   How about the fact that your grandkids
[00:07:19.040 --> 00:07:20.200]   are going to be blown away?
[00:07:20.200 --> 00:07:22.760]   They're going to say, so you guys just had a baby,
[00:07:22.760 --> 00:07:24.720]   and you just hoped it was a good baby?
[00:07:24.720 --> 00:07:26.960]   And you just kind of hoped for the best?
[00:07:26.960 --> 00:07:29.280]   And then the baby was born, and that was just what--
[00:07:29.280 --> 00:07:30.120]   no, it's like, of course, you're going
[00:07:30.120 --> 00:07:31.620]   to be making sure it's a good baby.
[00:07:31.620 --> 00:07:33.240]   You're going to be tweaking things around,
[00:07:33.240 --> 00:07:35.440]   and all the different gene editing things,
[00:07:35.440 --> 00:07:38.720]   and all the implications for our health that are coming,
[00:07:38.720 --> 00:07:41.760]   that are, again, crazy madness, the things that could
[00:07:41.760 --> 00:07:43.080]   come with this industry.
[00:07:43.080 --> 00:07:45.200]   How about this one?
[00:07:45.200 --> 00:07:48.680]   If you counted the great leaps for life, not just humans,
[00:07:48.680 --> 00:07:53.320]   but for all of life, on one hand, you're like, OK,
[00:07:53.320 --> 00:07:58.240]   first life, simple cell to complex cell, giant leap,
[00:07:58.240 --> 00:08:01.520]   single-celled organisms to multi-celled animals,
[00:08:01.520 --> 00:08:04.920]   ocean to land, one planet to two planets.
[00:08:04.920 --> 00:08:06.640]   That's on this one hand, and we're
[00:08:06.640 --> 00:08:11.840]   going to witness it probably in the 2030s, maybe 2040s.
[00:08:11.840 --> 00:08:14.560]   And so, yeah, this is the theme, where
[00:08:14.560 --> 00:08:18.480]   it's things that are incredibly weird to be witnessing
[00:08:18.480 --> 00:08:21.480]   or just happening one by one, these giant paradigm
[00:08:21.480 --> 00:08:26.760]   shifts, which, of course, is the context for AI.
[00:08:26.760 --> 00:08:30.760]   So I, unlike all the other presenters,
[00:08:30.760 --> 00:08:33.520]   do not understand this, like all of you and all that.
[00:08:33.520 --> 00:08:35.600]   I'm not a technical person on this.
[00:08:35.600 --> 00:08:38.120]   What I'm here to do is just to remind everyone
[00:08:38.120 --> 00:08:41.000]   to just zoom out and just remember the big story that's
[00:08:41.000 --> 00:08:43.200]   going on here.
[00:08:43.200 --> 00:08:45.080]   And so that's this context, is that we
[00:08:45.080 --> 00:08:47.680]   live in this crazy anomaly, and things are
[00:08:47.680 --> 00:08:49.000]   changing incredibly rapidly.
[00:08:49.000 --> 00:08:50.960]   And of course, this is the best example of it.
[00:08:50.960 --> 00:08:54.360]   Now, I first wrote about this in 2015.
[00:08:54.360 --> 00:08:58.720]   And it was a new topic then, right?
[00:08:58.720 --> 00:09:00.300]   A lot of people didn't know what it was
[00:09:00.300 --> 00:09:03.520]   or thought it was just a sci-fi concept.
[00:09:03.520 --> 00:09:06.520]   And I remember I was trying to understand
[00:09:06.520 --> 00:09:08.680]   narrow intelligence versus general intelligence,
[00:09:08.680 --> 00:09:10.000]   super intelligence.
[00:09:10.000 --> 00:09:12.560]   And one of the themes back then was
[00:09:12.560 --> 00:09:16.360]   that AI is great at doing stuff that involves thinking,
[00:09:16.360 --> 00:09:17.880]   but it's actually not good at stuff
[00:09:17.880 --> 00:09:22.240]   that we do without thinking, like walking or seeing,
[00:09:22.240 --> 00:09:22.960]   vision, right?
[00:09:22.960 --> 00:09:25.440]   Or you could say--
[00:09:25.440 --> 00:09:28.760]   so to an AI, beat the world chess master in chess.
[00:09:28.760 --> 00:09:30.560]   And the AI is like, easy.
[00:09:30.560 --> 00:09:31.760]   Did that in 1996.
[00:09:31.760 --> 00:09:32.240]   No problem.
[00:09:32.240 --> 00:09:34.760]   I'll do that in my sleep with 1,000 games at once.
[00:09:34.760 --> 00:09:35.280]   No big deal.
[00:09:35.280 --> 00:09:36.800]   You're like, OK, before you do that,
[00:09:36.800 --> 00:09:38.040]   can you just read these words?
[00:09:38.040 --> 00:09:39.120]   And the AI is like, shit.
[00:09:39.120 --> 00:09:43.600]   And it just doesn't make sense.
[00:09:43.600 --> 00:09:45.520]   How can you be so good at chess and then not
[00:09:45.520 --> 00:09:46.480]   know how to read those words?
[00:09:46.480 --> 00:09:48.960]   But it's because it's actually really hard to understand,
[00:09:48.960 --> 00:09:50.720]   like, what is the essence of a B, right?
[00:09:50.720 --> 00:09:53.280]   Like, you have a cursive B, and then you have a block letter,
[00:09:53.280 --> 00:09:55.720]   and you have a child's B in different fonts.
[00:09:55.720 --> 00:09:58.920]   And for us, it just somehow magically is all obvious.
[00:09:58.920 --> 00:10:00.640]   And it's actually really hard to do
[00:10:00.640 --> 00:10:02.480]   if you're trying to program, right?
[00:10:02.480 --> 00:10:05.020]   So then you could say, OK, let's forget that.
[00:10:05.020 --> 00:10:07.180]   Why don't you just memorize every street in the world
[00:10:07.180 --> 00:10:08.880]   and integrate it with real-time satellite data
[00:10:08.880 --> 00:10:10.220]   so you can instantly give me directions
[00:10:10.220 --> 00:10:12.340]   from anywhere to anywhere else, taking into account
[00:10:12.340 --> 00:10:13.520]   all the other factors, right?
[00:10:13.520 --> 00:10:14.640]   And the AI is like, great.
[00:10:14.640 --> 00:10:15.680]   I have my groove back.
[00:10:15.680 --> 00:10:16.320]   I can do that.
[00:10:16.320 --> 00:10:16.840]   No problem.
[00:10:16.840 --> 00:10:17.920]   That's super easy.
[00:10:17.920 --> 00:10:20.640]   And you're like, just before you do that, is this a dog or a cat?
[00:10:20.640 --> 00:10:21.800]   And the AI is like, Jesus.
[00:10:21.800 --> 00:10:28.200]   Now, granted, that one's a little bit hard for humans, too.
[00:10:28.200 --> 00:10:30.540]   But the point is that this is the kind of thing, again,
[00:10:30.540 --> 00:10:32.280]   that we think of as obvious.
[00:10:32.280 --> 00:10:35.080]   And it's just pixels, different colored pixels.
[00:10:35.080 --> 00:10:36.720]   And that's actually really hard.
[00:10:36.720 --> 00:10:38.220]   While you're just living your life,
[00:10:38.220 --> 00:10:39.600]   AI has been in the shit out here,
[00:10:39.600 --> 00:10:44.800]   just trying to figure out what the hell is going on.
[00:10:44.800 --> 00:10:48.040]   But now, this is still a general story, right?
[00:10:48.040 --> 00:10:49.200]   These things are still hard.
[00:10:49.200 --> 00:10:52.320]   But from 2015 to today--
[00:10:52.320 --> 00:10:54.720]   and I've been watching this story.
[00:10:54.720 --> 00:10:57.560]   And it's like a lot of things that really seem like, well,
[00:10:57.560 --> 00:11:00.580]   that's going to be a hard one.
[00:11:00.580 --> 00:11:02.820]   Or maybe AI will never be able to do that.
[00:11:02.820 --> 00:11:05.140]   Kind of one by one, they seem to be happening.
[00:11:05.140 --> 00:11:06.820]   This is, of course, a famous example.
[00:11:06.820 --> 00:11:08.280]   You probably have heard of AlphaGo.
[00:11:08.280 --> 00:11:11.020]   And this is, of course, a very complex game
[00:11:11.020 --> 00:11:12.780]   with 19 by 19 boards.
[00:11:12.780 --> 00:11:15.560]   So chess, people thought AI wouldn't
[00:11:15.560 --> 00:11:16.940]   be able to win in chess.
[00:11:16.940 --> 00:11:18.380]   And it did, right?
[00:11:18.380 --> 00:11:21.460]   IBM's Deep Blue beat Garry Kasparov in 1996
[00:11:21.460 --> 00:11:22.420]   and never looked back.
[00:11:22.420 --> 00:11:23.860]   But this seemed like next level.
[00:11:23.860 --> 00:11:26.660]   It was just too many variables to brute force it.
[00:11:26.660 --> 00:11:28.500]   It's like the amount of atoms in the universe
[00:11:28.500 --> 00:11:30.400]   is the amount of configurations that you could
[00:11:30.400 --> 00:11:33.620]   have of one of these games.
[00:11:33.620 --> 00:11:37.580]   But of course, Google's deep mind builds AlphaGo.
[00:11:37.580 --> 00:11:40.340]   And over a pretty short amount of time,
[00:11:40.340 --> 00:11:42.500]   it starts to get really good until they finally say,
[00:11:42.500 --> 00:11:45.580]   OK, you're going to play Lee Sedol, the world number one.
[00:11:45.580 --> 00:11:48.060]   And this is a famous game.
[00:11:48.060 --> 00:11:50.820]   And what happened was there was this--
[00:11:50.820 --> 00:11:54.020]   if you watch the documentary, there is a broadcaster.
[00:11:54.020 --> 00:11:57.940]   And there's this move that the AI made, that AlphaGo made.
[00:11:57.940 --> 00:12:00.940]   And the broadcaster said, OK, see, there's a mistake.
[00:12:00.940 --> 00:12:02.220]   Not so good yet.
[00:12:02.220 --> 00:12:04.820]   Because if you look, no other tile
[00:12:04.820 --> 00:12:08.140]   is more than three squares from the edge, except for that one.
[00:12:08.140 --> 00:12:09.500]   Because that's just bad form.
[00:12:09.500 --> 00:12:12.020]   You don't do that in Go.
[00:12:12.020 --> 00:12:14.820]   So this one was four squares from the edge.
[00:12:14.820 --> 00:12:19.700]   And what no one saw yet was that these two tiles down there,
[00:12:19.700 --> 00:12:22.420]   100 moves later, were going to connect
[00:12:22.420 --> 00:12:24.580]   to that for a winning play.
[00:12:24.580 --> 00:12:26.900]   This is just mind-blowing.
[00:12:26.900 --> 00:12:29.100]   People didn't realize that that could be a good play.
[00:12:29.100 --> 00:12:30.600]   Now people are learning from the AI.
[00:12:30.600 --> 00:12:32.620]   So the AI, of course, won.
[00:12:32.620 --> 00:12:35.500]   And again, chess has not looked back.
[00:12:35.500 --> 00:12:36.580]   It's kind of a theme.
[00:12:36.580 --> 00:12:37.620]   AI plays a game.
[00:12:37.620 --> 00:12:40.340]   We think it's not able to win.
[00:12:40.340 --> 00:12:41.340]   It eventually does.
[00:12:41.340 --> 00:12:44.780]   And once it does, it just is way better than us at it.
[00:12:44.780 --> 00:12:47.060]   Now, one of the things that this illustrated
[00:12:47.060 --> 00:12:49.260]   was there's different ways you can measure
[00:12:49.260 --> 00:12:50.780]   how smart an AI is.
[00:12:50.780 --> 00:12:51.700]   It's a hard question.
[00:12:51.700 --> 00:12:53.300]   One of them is creativity.
[00:12:53.300 --> 00:12:58.980]   So if you think about training samples,
[00:12:58.980 --> 00:13:04.740]   you have interpolation is when an AI can basically
[00:13:04.740 --> 00:13:07.740]   output an answer or a solution that
[00:13:07.740 --> 00:13:10.580]   is a combination of what it's already learned.
[00:13:10.580 --> 00:13:13.460]   It's something that-- it is just repeating something it's
[00:13:13.460 --> 00:13:15.340]   already seen or learned somewhere.
[00:13:15.340 --> 00:13:17.380]   But then there's extrapolation, where it's actually
[00:13:17.380 --> 00:13:18.540]   doing something new.
[00:13:18.540 --> 00:13:20.440]   It's a new solution that hasn't been done,
[00:13:20.440 --> 00:13:22.900]   and it's actually able to figure that out
[00:13:22.900 --> 00:13:24.260]   based on what it's learned.
[00:13:24.260 --> 00:13:25.540]   And then there's invention.
[00:13:25.540 --> 00:13:30.380]   So in this case, invention might be inventing a game like Go,
[00:13:30.380 --> 00:13:32.660]   which AI has not done yet.
[00:13:32.660 --> 00:13:36.220]   But what AI did in that game was extrapolation.
[00:13:36.220 --> 00:13:38.980]   That was a move that it didn't learn from watching games.
[00:13:38.980 --> 00:13:42.280]   It just actually kind of was good enough at the game
[00:13:42.280 --> 00:13:44.060]   to be able to do that.
[00:13:44.060 --> 00:13:47.620]   So this is, again, a theme is that AI
[00:13:47.620 --> 00:13:51.620]   is kind of blowing our mind with--
[00:13:51.620 --> 00:13:54.100]   it's not a good bet to say AI will never be good at this
[00:13:54.100 --> 00:13:55.840]   thing or never be the human at this thing.
[00:13:55.840 --> 00:13:58.300]   That person-- that bet has been wrong again and again
[00:13:58.300 --> 00:13:58.960]   and again.
[00:13:58.960 --> 00:14:02.180]   Now, one thing I thought in 2015,
[00:14:02.180 --> 00:14:04.440]   when people talked about AI is going to take our jobs,
[00:14:04.440 --> 00:14:06.020]   I was always like, it is sad that it's
[00:14:06.020 --> 00:14:10.420]   going to take your job, you know?
[00:14:10.420 --> 00:14:16.340]   And never did it cross my mind as a writer, creative person,
[00:14:16.340 --> 00:14:17.820]   that I would have a problem here.
[00:14:17.820 --> 00:14:20.700]   But while this was going on, some other people
[00:14:20.700 --> 00:14:23.500]   were working on a different game for AI, which
[00:14:23.500 --> 00:14:26.260]   is predicting the next word, which at the time
[00:14:26.260 --> 00:14:28.680]   I would have thought sounded stupid, like irrelevant,
[00:14:28.680 --> 00:14:30.500]   like, OK, predict the next word, whatever.
[00:14:30.500 --> 00:14:33.820]   But of course, today, we have GPT, right?
[00:14:33.820 --> 00:14:36.260]   And we have lots of large language models.
[00:14:36.260 --> 00:14:40.060]   And they're doing all kinds of pretty impressive stuff.
[00:14:40.060 --> 00:14:42.460]   Now, I know this is something that a lot of you
[00:14:42.460 --> 00:14:45.160]   are probably working in and thinking about all the time.
[00:14:45.160 --> 00:14:49.820]   You've probably seen it's now we have all these applications
[00:14:49.820 --> 00:14:50.700]   immediately, right?
[00:14:50.700 --> 00:14:51.820]   You have storytelling.
[00:14:51.820 --> 00:14:54.300]   You have coding and building a website.
[00:14:54.300 --> 00:14:55.780]   You have-- this is a fun one.
[00:14:55.780 --> 00:14:58.060]   You can be in a job interview.
[00:14:58.060 --> 00:15:01.020]   And as the interviewer is reading their question,
[00:15:01.020 --> 00:15:04.900]   it is pumping out a good answer for you to read back.
[00:15:04.900 --> 00:15:07.480]   I don't know if that actually would flow in practice,
[00:15:07.480 --> 00:15:08.700]   but it's pretty cool.
[00:15:08.700 --> 00:15:10.820]   Now, you also have all these meta things happening,
[00:15:10.820 --> 00:15:12.660]   like there's this site where you--
[00:15:12.660 --> 00:15:15.120]   snack prompt where people upvote the best prompts.
[00:15:15.120 --> 00:15:20.120]   There's a whole new skill set, a whole new realm of knowledge
[00:15:20.120 --> 00:15:21.880]   here of how to do prompts, right?
[00:15:21.880 --> 00:15:23.160]   It's an entire world.
[00:15:23.160 --> 00:15:25.860]   You have courses on how to do prompts, right?
[00:15:25.860 --> 00:15:28.740]   And now, side by side, of course,
[00:15:28.740 --> 00:15:30.720]   is the great story of images, right?
[00:15:30.720 --> 00:15:34.320]   That the AI has gotten quite good at this suddenly.
[00:15:34.320 --> 00:15:35.360]   This is just mid-journey.
[00:15:35.360 --> 00:15:39.880]   Literally in one year, you can see the vast improvement.
[00:15:39.880 --> 00:15:42.740]   You can say, I want to do a stock image of a coffee
[00:15:42.740 --> 00:15:45.380]   with a croissant on a white plate, and boom,
[00:15:45.380 --> 00:15:46.220]   pretty perfect.
[00:15:46.220 --> 00:15:48.540]   I tried to do a cover slide for this.
[00:15:48.540 --> 00:15:50.260]   I said, cover slide for a talk on AI,
[00:15:50.260 --> 00:15:52.580]   and it gave me this, which is fine.
[00:15:52.580 --> 00:15:55.460]   I could have used it, but I thought, you know.
[00:15:55.460 --> 00:15:57.820]   But then, of course, you probably-- if you're on Twitter,
[00:15:57.820 --> 00:16:01.300]   you've seen lots of buzz about Photoshop's generative fill,
[00:16:01.300 --> 00:16:01.800]   right?
[00:16:01.800 --> 00:16:04.380]   So you have, let's just pretend we're alone here,
[00:16:04.380 --> 00:16:05.540]   and that's boom.
[00:16:05.540 --> 00:16:08.500]   And you've got-- this is just kind of an OK pick,
[00:16:08.500 --> 00:16:11.600]   but what if we just extend it to be nice and horizontal
[00:16:11.600 --> 00:16:13.780]   and with just generative fill?
[00:16:13.780 --> 00:16:15.300]   And then let's make it awesome.
[00:16:15.300 --> 00:16:16.580]   Why not?
[00:16:16.580 --> 00:16:18.620]   This is made in mid-journey, right?
[00:16:18.620 --> 00:16:20.540]   But let's extend it in Photoshop.
[00:16:20.540 --> 00:16:21.740]   Boom.
[00:16:21.740 --> 00:16:25.140]   And these are two different mid-journey images someone made.
[00:16:25.140 --> 00:16:27.880]   And again, let's fill in in a way
[00:16:27.880 --> 00:16:29.960]   that makes them seem like they're in the same room.
[00:16:29.960 --> 00:16:30.460]   Let's connect it.
[00:16:30.460 --> 00:16:32.260]   And now, let's make the room bigger, right?
[00:16:32.260 --> 00:16:33.580]   It's pretty incredible.
[00:16:33.580 --> 00:16:36.940]   This came out of nowhere so quickly.
[00:16:36.940 --> 00:16:39.500]   And of course, again, meta-- now you have companies that are
[00:16:39.500 --> 00:16:43.420]   trying to help suss out, is this image AI-generated or not?
[00:16:43.420 --> 00:16:45.500]   There's a whole other industry being built, right,
[00:16:45.500 --> 00:16:49.140]   of trying to actually figure out and authenticate real versus AI.
[00:16:49.140 --> 00:16:52.220]   So I don't have to tell people in this room about how
[00:16:52.220 --> 00:16:53.980]   many companies have launched.
[00:16:53.980 --> 00:16:56.340]   This is a little dot-com bubble feel, right?
[00:16:56.340 --> 00:16:59.180]   It's like companies of all kinds just launching,
[00:16:59.180 --> 00:17:02.060]   and the innovation is just exploding.
[00:17:02.060 --> 00:17:04.600]   And there's a million exciting things about this, of course.
[00:17:04.600 --> 00:17:06.460]   Like the things I just said, like,
[00:17:06.460 --> 00:17:08.740]   think about how many people could be building great apps,
[00:17:08.740 --> 00:17:10.740]   great companies, but they don't know how to code.
[00:17:10.740 --> 00:17:11.740]   They don't have the money to build an app.
[00:17:11.740 --> 00:17:13.940]   Well, now they can build an app pretty easily
[00:17:13.940 --> 00:17:16.500]   without any money or know-how.
[00:17:16.500 --> 00:17:18.260]   There's AI assistants coming, right?
[00:17:18.260 --> 00:17:22.900]   And think about how nice it is for a rich person
[00:17:22.900 --> 00:17:25.180]   to have a full-time executive assistant.
[00:17:25.180 --> 00:17:27.540]   That's amazing.
[00:17:27.540 --> 00:17:30.500]   And now, what if everyone can have that?
[00:17:30.500 --> 00:17:33.940]   You give the AI assistant all your passwords,
[00:17:33.940 --> 00:17:36.300]   all of your financial stuff, and they go do that
[00:17:36.300 --> 00:17:37.980]   and some people are probably thinking,
[00:17:37.980 --> 00:17:39.940]   no one's going to give them AI, their password.
[00:17:39.940 --> 00:17:41.140]   People said they weren't going to put--
[00:17:41.140 --> 00:17:42.460]   people weren't going to put credit cards
[00:17:42.460 --> 00:17:43.740]   on the internet at some point.
[00:17:43.740 --> 00:17:45.100]   People said that no one was going
[00:17:45.100 --> 00:17:47.340]   to be able to let-- no one was going to let strangers
[00:17:47.340 --> 00:17:48.980]   in their house before Airbnb.
[00:17:48.980 --> 00:17:50.940]   No one was going to let strangers into their car
[00:17:50.940 --> 00:17:53.100]   before Uber or get into a stranger's car.
[00:17:53.100 --> 00:17:55.440]   I think there's going to be great AI assistants with all
[00:17:55.440 --> 00:17:56.980]   your info.
[00:17:56.980 --> 00:17:59.780]   Of course, there's also like AI relationships, right?
[00:17:59.780 --> 00:18:02.240]   There's companies like Replica where you actually-- people
[00:18:02.240 --> 00:18:06.020]   are developing friendships and romantic relationships
[00:18:06.020 --> 00:18:08.460]   with AI, which again, sounds super weird to us
[00:18:08.460 --> 00:18:09.820]   and I think to babies today.
[00:18:09.820 --> 00:18:11.620]   Once they're teens, they're like, yeah, duh,
[00:18:11.620 --> 00:18:12.860]   everyone has an AI girlfriend.
[00:18:12.860 --> 00:18:14.780]   It's no big deal, whatever.
[00:18:14.780 --> 00:18:16.780]   Now, of course, it's also pretty scary, right?
[00:18:16.780 --> 00:18:21.740]   Because AI that can pass the Turing test, not just in text,
[00:18:21.740 --> 00:18:26.060]   by the way, but when you combine text with images and video
[00:18:26.060 --> 00:18:30.400]   and voice synthesis, you could have a big Twitter account
[00:18:30.400 --> 00:18:34.020]   that has a big following and then
[00:18:34.020 --> 00:18:35.220]   a big political following.
[00:18:35.220 --> 00:18:39.940]   And then that account, that person gets interviewed on TV.
[00:18:39.940 --> 00:18:43.700]   And you're seeing a Zoom interview with that person
[00:18:43.700 --> 00:18:44.700]   and they're talking about--
[00:18:44.700 --> 00:18:45.820]   and it's not real.
[00:18:45.820 --> 00:18:46.980]   The whole thing's a bot.
[00:18:46.980 --> 00:18:48.060]   It's a bot tweeting.
[00:18:48.060 --> 00:18:49.660]   It's a bot speaking in the interview.
[00:18:49.660 --> 00:18:51.500]   It's a bot face.
[00:18:51.500 --> 00:18:54.940]   There's not really anything stopping that from happening.
[00:18:54.940 --> 00:18:58.740]   Likewise, forget AI sentience, which is a whole other topic.
[00:18:58.740 --> 00:19:02.460]   If people believe an AI is sentient,
[00:19:02.460 --> 00:19:04.540]   that can do a lot of damage.
[00:19:04.540 --> 00:19:08.820]   You can have AI manipulating humans.
[00:19:08.820 --> 00:19:12.660]   And you can have bad guys with these tools doing bad things.
[00:19:12.660 --> 00:19:14.460]   It can help with terrorism or figuring out
[00:19:14.460 --> 00:19:16.620]   how to build a bomb and all these other scary things.
[00:19:16.620 --> 00:19:18.820]   So there's these amazing, great possibilities.
[00:19:18.820 --> 00:19:21.260]   And then there's really scary possibilities.
[00:19:21.260 --> 00:19:26.220]   Now, all of this at this moment should be put into context.
[00:19:26.220 --> 00:19:28.060]   I think a nice quote that I think about a lot
[00:19:28.060 --> 00:19:33.780]   is Bill Gates saying, "We tend to overestimate the change
[00:19:33.780 --> 00:19:35.740]   that will happen in the next two years
[00:19:35.740 --> 00:19:37.220]   and underestimate the change that
[00:19:37.220 --> 00:19:39.460]   will happen in the next 10."
[00:19:39.460 --> 00:19:40.620]   And I think you can see why.
[00:19:40.620 --> 00:19:43.580]   If you think about the next two years with a thing like this,
[00:19:43.580 --> 00:19:45.940]   it's often what feels like it's here.
[00:19:45.940 --> 00:19:47.620]   Everything's changing.
[00:19:47.620 --> 00:19:49.380]   Three years from now is going to be nuts.
[00:19:49.380 --> 00:19:52.460]   It might be one of these, one of these hype cycles.
[00:19:52.460 --> 00:19:54.180]   And right now, in two years, everyone's
[00:19:54.180 --> 00:19:56.680]   like, what happened to all the talk about GPT and all that?
[00:19:56.680 --> 00:19:58.300]   Maybe, or maybe not.
[00:19:58.300 --> 00:19:59.900]   I don't know, but it could be.
[00:19:59.900 --> 00:20:02.020]   There's also the fact that even exponential curves,
[00:20:02.020 --> 00:20:06.500]   crazy changes happen typically if you zoom in on s-curves,
[00:20:06.500 --> 00:20:09.500]   which means there's slow and fast periods and slow periods
[00:20:09.500 --> 00:20:10.260]   alternating.
[00:20:10.260 --> 00:20:12.220]   So maybe, again, maybe we're here
[00:20:12.220 --> 00:20:15.780]   in a place where right now it seems like things are--
[00:20:15.780 --> 00:20:18.700]   but actually, we're a few years away from the rapid changes
[00:20:18.700 --> 00:20:19.700]   really happening.
[00:20:19.700 --> 00:20:20.340]   Or maybe not.
[00:20:20.340 --> 00:20:23.300]   But I would say it's a decent probability
[00:20:23.300 --> 00:20:25.580]   that we are overestimating the change that will
[00:20:25.580 --> 00:20:29.640]   happen in the next two years and underestimating the change that
[00:20:29.640 --> 00:20:31.100]   might happen in the next 10.
[00:20:31.100 --> 00:20:32.100]   Now, why do we do this?
[00:20:32.100 --> 00:20:33.820]   Why do we underestimate long-term change?
[00:20:33.820 --> 00:20:36.660]   And it's because we're just not good at thinking
[00:20:36.660 --> 00:20:37.380]   exponentially.
[00:20:37.380 --> 00:20:38.660]   It's not built in.
[00:20:38.660 --> 00:20:39.320]   It's not wired.
[00:20:39.320 --> 00:20:41.460]   We have to override our own linear wiring
[00:20:41.460 --> 00:20:42.660]   to think that way.
[00:20:42.660 --> 00:20:44.500]   So imagine if-- here's the present day.
[00:20:44.500 --> 00:20:47.220]   And just imagine this is a curve of some form of progress.
[00:20:47.220 --> 00:20:50.700]   And I say, I want you to predict where we're going to be there.
[00:20:50.700 --> 00:20:53.380]   So what a lot of people will naturally do
[00:20:53.380 --> 00:20:55.420]   is they'll use their experience.
[00:20:55.420 --> 00:20:57.540]   They'll say, well, this much progress has happened
[00:20:57.540 --> 00:20:58.460]   in that many years.
[00:20:58.460 --> 00:21:00.300]   And so let's add that to the future.
[00:21:00.300 --> 00:21:02.460]   And so they'll base it on past growth rate.
[00:21:02.460 --> 00:21:04.660]   Some others might say, well, things are moving really
[00:21:04.660 --> 00:21:05.620]   quickly right now.
[00:21:05.620 --> 00:21:08.940]   So let's assume that it's going to be moving at that pace.
[00:21:08.940 --> 00:21:10.480]   Those are both bad predictions, right?
[00:21:10.480 --> 00:21:13.860]   Because exponential curves are crazy.
[00:21:13.860 --> 00:21:15.860]   When they actually get rolling, they're
[00:21:15.860 --> 00:21:18.340]   really hard to internalize.
[00:21:18.340 --> 00:21:21.260]   And that's crazy with any technology.
[00:21:21.260 --> 00:21:26.020]   But with AI, it has a whole other meaning of crazy here.
[00:21:26.020 --> 00:21:30.980]   Because it's not just stuff like large language models
[00:21:30.980 --> 00:21:33.100]   dominating all these industries.
[00:21:33.100 --> 00:21:33.780]   It gets intense.
[00:21:33.780 --> 00:21:37.700]   So right now, we have a world like this.
[00:21:37.700 --> 00:21:41.900]   You have really impressive AI that's almost always
[00:21:41.900 --> 00:21:43.400]   narrow intelligence.
[00:21:43.400 --> 00:21:46.860]   And then you have pretty unimpressive human general
[00:21:46.860 --> 00:21:47.460]   intelligence.
[00:21:47.460 --> 00:21:49.780]   But we are very impressive in our breadth.
[00:21:49.780 --> 00:21:52.100]   We can reason through any problem.
[00:21:52.100 --> 00:21:53.940]   We can learn any skill.
[00:21:53.940 --> 00:21:57.020]   We're just kind of actually aware of what's going on
[00:21:57.020 --> 00:21:59.740]   in a really big, broad way.
[00:21:59.740 --> 00:22:02.300]   AI is not good at that, including the large language
[00:22:02.300 --> 00:22:05.060]   models, which seem a little bit like they
[00:22:05.060 --> 00:22:06.140]   have general intelligence.
[00:22:06.140 --> 00:22:06.980]   And they don't.
[00:22:06.980 --> 00:22:10.620]   They are narrowly good at predicting the next word.
[00:22:10.620 --> 00:22:12.300]   And they're not actually reasoning.
[00:22:12.300 --> 00:22:15.260]   They're not understanding the language that they're speaking.
[00:22:15.260 --> 00:22:18.580]   So that's a next level thing that isn't here yet.
[00:22:18.580 --> 00:22:22.700]   But it's going to be crazy if and when it gets here.
[00:22:22.700 --> 00:22:25.420]   So this is how we view intelligence.
[00:22:25.420 --> 00:22:28.540]   We think of it as like Einstein is super smart,
[00:22:28.540 --> 00:22:31.840]   and dumb humans are quite dumb, and the animals are really
[00:22:31.840 --> 00:22:33.100]   dumb down there.
[00:22:33.100 --> 00:22:37.620]   So when AI, kind of general intelligent AI, is rising up,
[00:22:37.620 --> 00:22:39.460]   and it's kind of, oh, it can do what an ant can do,
[00:22:39.460 --> 00:22:40.820]   a bird can do, it can do what a chimp can do.
[00:22:40.820 --> 00:22:41.980]   That's cute, we're going to think.
[00:22:41.980 --> 00:22:43.560]   It's going to be all YouTube viral video.
[00:22:43.560 --> 00:22:45.540]   Look, it's playing with the chimps, the robot.
[00:22:45.540 --> 00:22:47.580]   But actually, that's not what it's like.
[00:22:47.580 --> 00:22:48.660]   This is a distortion, because we're
[00:22:48.660 --> 00:22:49.780]   in the human intelligence.
[00:22:49.780 --> 00:22:51.120]   So it seems really big to us.
[00:22:51.120 --> 00:22:53.140]   It's really more like this.
[00:22:53.140 --> 00:22:57.260]   So when AI is rising from ant to bird to chimp,
[00:22:57.260 --> 00:23:00.020]   it is soaring upwards towards us.
[00:23:00.020 --> 00:23:03.100]   And it can be playing with five-year-olds one day
[00:23:03.100 --> 00:23:05.460]   on their level, and a week later, oh, shit,
[00:23:05.460 --> 00:23:08.460]   it's smarter than anyone ever.
[00:23:08.460 --> 00:23:12.300]   So this concept, when AI does get general intelligence,
[00:23:12.300 --> 00:23:13.780]   it's not going to just be like us.
[00:23:13.780 --> 00:23:17.420]   It's going to be the same magnitude of what it currently
[00:23:17.420 --> 00:23:18.820]   is, but general intelligence.
[00:23:18.820 --> 00:23:21.000]   Super intelligence is what we call it.
[00:23:21.000 --> 00:23:25.420]   It's just really hard to wrap your head around how
[00:23:25.420 --> 00:23:29.340]   intense a concept artificial super intelligence is.
[00:23:29.340 --> 00:23:30.960]   We're only a little smarter than chimps.
[00:23:30.960 --> 00:23:36.500]   I think if you went to 30,000 BC and you
[00:23:36.500 --> 00:23:38.660]   saw a band of humans over there doing their thing
[00:23:38.660 --> 00:23:40.700]   in the forest and a band of chimps over there,
[00:23:40.700 --> 00:23:41.900]   they're not that different.
[00:23:41.900 --> 00:23:43.940]   The humans, they have some language capabilities.
[00:23:43.940 --> 00:23:45.020]   They cooperate a little better.
[00:23:45.020 --> 00:23:46.100]   They have better weapons.
[00:23:46.100 --> 00:23:48.140]   But we're just a little bit smarter.
[00:23:48.140 --> 00:23:51.280]   But think about that little increase in intelligence
[00:23:51.280 --> 00:23:52.680]   we have over chimps.
[00:23:52.680 --> 00:23:56.320]   It opens up possibilities like group collaboration.
[00:23:56.320 --> 00:23:58.800]   And suddenly, we have this massive civilization.
[00:23:58.800 --> 00:24:01.320]   You look up at the sky at night, and it's not just stars.
[00:24:01.320 --> 00:24:02.960]   There's things moving there.
[00:24:02.960 --> 00:24:04.200]   We put that there.
[00:24:04.200 --> 00:24:08.120]   Primates put that there because we're just a little smarter
[00:24:08.120 --> 00:24:09.320]   than chimps so we can do that.
[00:24:09.320 --> 00:24:12.400]   The chimp can't even understand that we put it there.
[00:24:12.400 --> 00:24:13.800]   It's such a different level.
[00:24:13.800 --> 00:24:16.200]   So it's something two steps above us
[00:24:16.200 --> 00:24:18.040]   on the general intelligence scale.
[00:24:18.040 --> 00:24:19.780]   Not only can we not do what it can do,
[00:24:19.780 --> 00:24:21.980]   we might not even be able to understand that it did it
[00:24:21.980 --> 00:24:24.380]   or certainly not understand how it did it.
[00:24:24.380 --> 00:24:27.060]   But ASI doesn't stop two steps above.
[00:24:27.060 --> 00:24:29.340]   It's not restrained the way human brains are restrained.
[00:24:29.340 --> 00:24:33.300]   We're restrained by the speed of human brain neurons
[00:24:33.300 --> 00:24:34.580]   is quite slow.
[00:24:34.580 --> 00:24:37.820]   So if the brain gets too big, it can't function as one thing.
[00:24:37.820 --> 00:24:40.340]   We're also restrained by the size of our skull.
[00:24:40.340 --> 00:24:42.820]   We forget things.
[00:24:42.820 --> 00:24:46.300]   You can't edit past memories.
[00:24:46.300 --> 00:24:48.220]   AI has no such restraints.
[00:24:48.220 --> 00:24:51.500]   An AI system could be the size of a dwarf planet
[00:24:51.500 --> 00:24:55.100]   before it would start to run into its own size problems.
[00:24:55.100 --> 00:24:56.460]   And so of course, it's just going
[00:24:56.460 --> 00:24:58.740]   to keep leaping up the steps to here.
[00:24:58.740 --> 00:25:00.780]   And that's the whole last slide down there.
[00:25:00.780 --> 00:25:04.980]   And at that point, there's God on the planet.
[00:25:04.980 --> 00:25:08.860]   It's just whatever you can imagine an omnipotent God doing,
[00:25:08.860 --> 00:25:11.220]   this thing can probably do it.
[00:25:11.220 --> 00:25:12.260]   Pretty scary.
[00:25:12.260 --> 00:25:13.900]   Is this going to be the best thing ever
[00:25:13.900 --> 00:25:14.900]   or the worst thing ever?
[00:25:14.900 --> 00:25:18.380]   If it's going to be good, it's going to be real good.
[00:25:18.380 --> 00:25:21.500]   Every problem that we can possibly imagine
[00:25:21.500 --> 00:25:25.060]   is easily solved by this, including human mortality.
[00:25:25.060 --> 00:25:27.900]   The idea that we die when we're not ready.
[00:25:27.900 --> 00:25:29.900]   It's just such an awful, sad, barbaric thing
[00:25:29.900 --> 00:25:30.900]   about our current world.
[00:25:30.900 --> 00:25:31.940]   Couldn't be solved.
[00:25:31.940 --> 00:25:34.340]   Things like disease and poverty, climate change,
[00:25:34.340 --> 00:25:38.340]   easy for something that is this smart.
[00:25:38.340 --> 00:25:39.960]   So it could solve all of our problems.
[00:25:39.960 --> 00:25:42.000]   But of course, so many people are worried
[00:25:42.000 --> 00:25:44.020]   because it's not easy.
[00:25:44.020 --> 00:25:48.260]   It's not easy to try to make something that powerful behave
[00:25:48.260 --> 00:25:51.500]   in the very nuanced way we want it to behave,
[00:25:51.500 --> 00:25:52.940]   in a way that is good for us.
[00:25:52.940 --> 00:25:55.420]   And it's a very, very, very hard problem, the alignment
[00:25:55.420 --> 00:25:57.460]   problem, which is why a lot of AI researchers
[00:25:57.460 --> 00:26:01.300]   kind of compare us to a bunch of kids playing with a bomb.
[00:26:01.300 --> 00:26:05.980]   And you can think of it as if we're
[00:26:05.980 --> 00:26:10.140]   some little animals on a beach somewhere,
[00:26:10.140 --> 00:26:11.820]   an anthill or something.
[00:26:11.820 --> 00:26:15.260]   And there's a boat arriving with a million people
[00:26:15.260 --> 00:26:17.140]   are about to show up and build a--
[00:26:17.140 --> 00:26:19.260]   they don't have to hate the ants or be
[00:26:19.260 --> 00:26:23.420]   evil to just totally fuck up our ecosystem here on the beach.
[00:26:23.420 --> 00:26:27.300]   And so this thing is coming with massively powerful footprint
[00:26:27.300 --> 00:26:29.460]   and just the amount of sheer power
[00:26:29.460 --> 00:26:33.940]   that it's so easy to just mess up our pretty fragile human
[00:26:33.940 --> 00:26:35.020]   situation here.
[00:26:35.020 --> 00:26:39.220]   It can be, of course, bad AI with a bad goal that
[00:26:39.220 --> 00:26:41.380]   wants to hurt us, then we're totally done.
[00:26:41.380 --> 00:26:43.980]   Or it can be, of course, like the paperclip maximizer,
[00:26:43.980 --> 00:26:46.340]   the stories about the scenarios where
[00:26:46.340 --> 00:26:49.380]   AI has a goal that's harmless, but actually,
[00:26:49.380 --> 00:26:51.580]   when it actually carries it out to the full extent,
[00:26:51.580 --> 00:26:52.900]   it's extremely harmful.
[00:26:52.900 --> 00:26:54.780]   It might be that it misinterprets us.
[00:26:54.780 --> 00:26:56.500]   We said we want to make humans happy,
[00:26:56.500 --> 00:26:57.820]   something broad like that.
[00:26:57.820 --> 00:27:00.540]   OK, so it's going to plaster all of our faces into smiles,
[00:27:00.540 --> 00:27:02.380]   paralyze our faces into permanent smiles.
[00:27:02.380 --> 00:27:04.140]   And if you try to change it at that point,
[00:27:04.140 --> 00:27:06.380]   it doesn't want to be changed.
[00:27:06.380 --> 00:27:08.740]   And so there are lots of scenarios
[00:27:08.740 --> 00:27:11.780]   where the AI goal isn't even what matters.
[00:27:11.780 --> 00:27:13.460]   The goal might be something fine,
[00:27:13.460 --> 00:27:15.020]   but it's the way it gets there.
[00:27:15.020 --> 00:27:19.260]   Think about our goal deep down in our system.
[00:27:19.260 --> 00:27:21.580]   Our goal is to reproduce, to pass on our genes.
[00:27:21.580 --> 00:27:23.980]   But think about all the things you're doing this week.
[00:27:23.980 --> 00:27:25.940]   Think about all the things we do in the world,
[00:27:25.940 --> 00:27:27.580]   all these other goals we have.
[00:27:27.580 --> 00:27:30.100]   So that's still the big goal that our body has,
[00:27:30.100 --> 00:27:33.900]   but our brains have now taken on a million other goals.
[00:27:33.900 --> 00:27:35.340]   So you can give the AI this goal,
[00:27:35.340 --> 00:27:36.980]   and it might have a lot of other things.
[00:27:36.980 --> 00:27:39.860]   Or maybe just on the way there, it wants to--
[00:27:39.860 --> 00:27:42.780]   to get to its goal, it needs to avoid being shut down.
[00:27:42.780 --> 00:27:43.940]   And who could shut it down?
[00:27:43.940 --> 00:27:44.580]   Humans.
[00:27:44.580 --> 00:27:47.060]   So you've probably heard a bunch of these, but they're real.
[00:27:47.060 --> 00:27:48.300]   They're scary.
[00:27:48.300 --> 00:27:51.580]   And you don't even need to get to ASI for really scary things
[00:27:51.580 --> 00:27:52.060]   to happen.
[00:27:52.060 --> 00:27:54.300]   Like I mentioned earlier, just with the large language
[00:27:54.300 --> 00:27:56.500]   models and the video and voice synthesis,
[00:27:56.500 --> 00:28:00.540]   you can start to undermine the core things that
[00:28:00.540 --> 00:28:02.300]   make a civilization strong.
[00:28:02.300 --> 00:28:03.740]   So the stakes are high.
[00:28:03.740 --> 00:28:05.240]   They're high not just in the bad way,
[00:28:05.240 --> 00:28:06.040]   because we talk a lot about that,
[00:28:06.040 --> 00:28:07.580]   but they're high in the good way too.
[00:28:07.580 --> 00:28:09.820]   Like we could have the most incredible future
[00:28:09.820 --> 00:28:11.100]   if we get this right.
[00:28:11.100 --> 00:28:17.020]   So I wanted to leave you with one idea
[00:28:17.020 --> 00:28:19.740]   that I think if you can take home anything,
[00:28:19.740 --> 00:28:22.180]   it's that given how high the stakes are and how fast
[00:28:22.180 --> 00:28:25.900]   everything's changing, how should we proceed?
[00:28:25.900 --> 00:28:28.540]   What's the best way to even think our way through this?
[00:28:28.540 --> 00:28:31.220]   So let's talk quickly about two kinds of geology.
[00:28:31.220 --> 00:28:34.140]   Let's just zip back to the 1600s for a second.
[00:28:34.140 --> 00:28:36.320]   And you have two camps studying something
[00:28:36.320 --> 00:28:37.680]   like the Grand Canyon.
[00:28:37.680 --> 00:28:40.880]   You've got what we can call the flood geologists, who
[00:28:40.880 --> 00:28:43.600]   were convinced that the Earth is 6,000 years old,
[00:28:43.600 --> 00:28:45.420]   because the Bible says so.
[00:28:45.420 --> 00:28:48.240]   And therefore, everything they observed
[00:28:48.240 --> 00:28:51.560]   had to be reconciled with that axiom.
[00:28:51.560 --> 00:28:53.120]   So they saw seashell fossils.
[00:28:53.120 --> 00:28:54.720]   That must be because of the flood.
[00:28:54.720 --> 00:28:57.080]   And so it's called flood geology,
[00:28:57.080 --> 00:28:59.720]   is the field, because the flood was so often a way
[00:28:59.720 --> 00:29:01.600]   to kind of make--
[00:29:01.600 --> 00:29:03.840]   reconcile things that may seem to make no sense
[00:29:03.840 --> 00:29:07.420]   or must have been this cataclysmic flood.
[00:29:07.420 --> 00:29:09.240]   Then there's the science geologists.
[00:29:09.240 --> 00:29:10.620]   Some of them were religious, too,
[00:29:10.620 --> 00:29:13.420]   but they just had a different approach.
[00:29:13.420 --> 00:29:16.060]   They started with I don't know.
[00:29:16.060 --> 00:29:18.020]   They just put down the Bible and said,
[00:29:18.020 --> 00:29:20.340]   let's just imagine we know nothing at all
[00:29:20.340 --> 00:29:23.280]   and only use our observations.
[00:29:23.280 --> 00:29:24.180]   So what happens?
[00:29:24.180 --> 00:29:27.200]   Of course, their observations start to build up,
[00:29:27.200 --> 00:29:29.660]   and they're testing them, and they're changing their mind.
[00:29:29.660 --> 00:29:32.180]   And before you know it, they've discovered plate tectonics.
[00:29:32.180 --> 00:29:34.100]   And before you know it, the age of the Earth
[00:29:34.100 --> 00:29:36.340]   has gone back well past thousands of years to millions
[00:29:36.340 --> 00:29:39.140]   and eventually billions of years old.
[00:29:39.140 --> 00:29:41.900]   And so they were actually at the bottom
[00:29:41.900 --> 00:29:43.660]   of a mountain of knowledge that they just
[00:29:43.660 --> 00:29:45.420]   have climbed for 300 years.
[00:29:45.420 --> 00:29:47.420]   Meanwhile, the flood geologists never
[00:29:47.420 --> 00:29:49.000]   made it off the bottom of the mountain
[00:29:49.000 --> 00:29:53.300]   because they had an axiom that was like a piece of baggage
[00:29:53.300 --> 00:29:56.180]   they couldn't put down, and it was holding them back.
[00:29:56.180 --> 00:29:58.060]   And I think a lot of people in this room,
[00:29:58.060 --> 00:29:59.440]   whether you're religious or not, you probably
[00:29:59.440 --> 00:30:01.100]   identify with the science geologists.
[00:30:01.100 --> 00:30:02.860]   You say, that's how I reason.
[00:30:02.860 --> 00:30:05.420]   I'm an evidence-based person.
[00:30:05.420 --> 00:30:08.580]   But the truth is, we're all flood geologists
[00:30:08.580 --> 00:30:10.260]   in a lot of areas of our thinking.
[00:30:10.260 --> 00:30:12.220]   It's just human nature.
[00:30:12.220 --> 00:30:16.100]   Now, this you can generalize to the concept
[00:30:16.100 --> 00:30:18.180]   of reasoning from first principles
[00:30:18.180 --> 00:30:22.140]   versus reasoning by analogy.
[00:30:22.140 --> 00:30:23.900]   And so reasoning from first principles
[00:30:23.900 --> 00:30:25.320]   is what the science geologists did.
[00:30:25.320 --> 00:30:27.380]   First principles is a physics term.
[00:30:27.380 --> 00:30:30.860]   It means you look at the actual axioms
[00:30:30.860 --> 00:30:33.820]   that you know to be true, that you have evidence for.
[00:30:33.820 --> 00:30:35.540]   And those are your puzzle pieces that you
[00:30:35.540 --> 00:30:38.040]   use to construct a conclusion.
[00:30:38.040 --> 00:30:42.280]   And if new evidence comes in, it changes that puzzle tower,
[00:30:42.280 --> 00:30:43.860]   and the conclusion changes.
[00:30:43.860 --> 00:30:46.300]   And so you think like a scientist.
[00:30:46.300 --> 00:30:49.300]   You start with, I don't know, and you gather evidence.
[00:30:49.300 --> 00:30:53.220]   Reasoning by analogy is when you take an existing conclusion out
[00:30:53.220 --> 00:30:55.620]   in the world-- maybe it's your previous ideas,
[00:30:55.620 --> 00:30:57.540]   maybe it's your parents' ideas, or your friend,
[00:30:57.540 --> 00:31:00.160]   or maybe it's just conventional wisdom--
[00:31:00.160 --> 00:31:03.700]   and you adopt that into your head like a Bible.
[00:31:03.700 --> 00:31:06.580]   And you say, well, everyone is doing it, so it must be right.
[00:31:06.580 --> 00:31:11.300]   Or no one has built this, so it must not be possible.
[00:31:11.300 --> 00:31:12.900]   Or this is how it's always happened,
[00:31:12.900 --> 00:31:14.920]   so that's what will happen in the future.
[00:31:14.920 --> 00:31:16.380]   Reasoning by analogy is great.
[00:31:16.380 --> 00:31:17.820]   It's a great human shortcut.
[00:31:17.820 --> 00:31:18.900]   It allows us to--
[00:31:18.900 --> 00:31:20.540]   reasoning from first principles is hard.
[00:31:20.540 --> 00:31:23.700]   It takes a lot of mental effort and time
[00:31:23.700 --> 00:31:25.740]   to reinvent the reasoning wheel every time you're
[00:31:25.740 --> 00:31:26.740]   trying to make a decision.
[00:31:26.740 --> 00:31:27.940]   So we have this great tool.
[00:31:27.940 --> 00:31:30.560]   We can take other people's hard-earned knowledge
[00:31:30.560 --> 00:31:32.980]   and we can copy it into our brain,
[00:31:32.980 --> 00:31:35.680]   except when times are changing really quickly
[00:31:35.680 --> 00:31:37.420]   and when it's really important moments--
[00:31:37.420 --> 00:31:39.720]   and this, by the way, goes for outside of work as well.
[00:31:39.720 --> 00:31:42.280]   Just how you raise your kids, who you marry, where you live,
[00:31:42.280 --> 00:31:43.800]   really core fundamental things.
[00:31:43.800 --> 00:31:45.760]   You really want to reason from first principles.
[00:31:45.760 --> 00:31:49.000]   So it's a general concept, but we're bad at it.
[00:31:49.000 --> 00:31:55.240]   We're bad at it because, first of all, our wiring is--
[00:31:55.240 --> 00:31:57.580]   we're all primates that are built
[00:31:57.580 --> 00:31:59.540]   to be living in 30,000 BC.
[00:31:59.540 --> 00:32:01.300]   That's where we're supposed to be.
[00:32:01.300 --> 00:32:03.180]   Biology doesn't change very quickly.
[00:32:03.180 --> 00:32:04.940]   So if our civilization just springs up,
[00:32:04.940 --> 00:32:06.580]   we're still wired for the other world.
[00:32:06.580 --> 00:32:08.900]   And we've been kind of kidnapped out of our home forest
[00:32:08.900 --> 00:32:10.860]   and dropped into this advanced civilization
[00:32:10.860 --> 00:32:12.260]   when things move quickly.
[00:32:12.260 --> 00:32:16.020]   Back where we were wired for, your great-great-great-great
[00:32:16.020 --> 00:32:18.460]   grandmother lives basically the same life you did.
[00:32:18.460 --> 00:32:21.340]   Remember the caveman on page 621?
[00:32:21.340 --> 00:32:24.040]   Maybe on the same piece of land, 10 generations in a row.
[00:32:24.040 --> 00:32:26.340]   So conventional wisdom was wise.
[00:32:26.340 --> 00:32:28.460]   If someone's telling you don't eat that berry,
[00:32:28.460 --> 00:32:30.460]   that's a good thing to listen to because people
[00:32:30.460 --> 00:32:31.540]   learn that the hard way.
[00:32:31.540 --> 00:32:33.380]   So what people-- what the tribe thought
[00:32:33.380 --> 00:32:35.680]   was actually a good thing to trust and listen
[00:32:35.680 --> 00:32:37.480]   to over your own reasoning, because that's
[00:32:37.480 --> 00:32:41.620]   based on generations of trial and error,
[00:32:41.620 --> 00:32:44.220]   accumulated trial and error.
[00:32:44.220 --> 00:32:46.500]   Today, we have to override that instinct
[00:32:46.500 --> 00:32:48.860]   to trust conventional wisdom because conventional wisdom
[00:32:48.860 --> 00:32:49.700]   moves slowly.
[00:32:49.700 --> 00:32:51.160]   And when the world changes quickly,
[00:32:51.160 --> 00:32:53.620]   it's going to be not just lagging, just wrong, flat out
[00:32:53.620 --> 00:32:55.980]   wrong.
[00:32:55.980 --> 00:32:58.620]   Meanwhile, it's even harder because the way we're brought
[00:32:58.620 --> 00:33:01.180]   up does not help us here.
[00:33:01.180 --> 00:33:02.420]   Kids play the why game, right?
[00:33:02.420 --> 00:33:04.000]   And you say, put on your boots.
[00:33:04.000 --> 00:33:04.500]   Why?
[00:33:04.500 --> 00:33:06.340]   Because you have to go outside.
[00:33:06.340 --> 00:33:06.840]   Why?
[00:33:06.840 --> 00:33:09.460]   Because it's cold and winter and the sun's tilt
[00:33:09.460 --> 00:33:11.820]   and all these different things.
[00:33:11.820 --> 00:33:13.500]   And you want to kill them.
[00:33:13.500 --> 00:33:15.220]   And so you say, because I said so.
[00:33:15.220 --> 00:33:17.660]   But what's the kid actually doing?
[00:33:17.660 --> 00:33:18.700]   The kid is actually--
[00:33:18.700 --> 00:33:20.540]   you're giving them a conclusion, and they're
[00:33:20.540 --> 00:33:22.040]   supposed to get to the bottom of the tower
[00:33:22.040 --> 00:33:23.240]   and find the first principles.
[00:33:23.240 --> 00:33:24.520]   That's the natural instinct.
[00:33:24.520 --> 00:33:27.640]   I want to understand how this conclusion was built.
[00:33:27.640 --> 00:33:29.920]   It's a great instinct, and it's shut down
[00:33:29.920 --> 00:33:33.880]   by our parents, by teachers, by broader society.
[00:33:33.880 --> 00:33:38.840]   Why game, because I said so, is kind of the concrete floor,
[00:33:38.840 --> 00:33:42.260]   and it just blocks that exploration.
[00:33:42.260 --> 00:33:43.920]   And what are you taught?
[00:33:43.920 --> 00:33:45.240]   Listen to the authority.
[00:33:45.240 --> 00:33:47.200]   Don't hurt your self-reasoning.
[00:33:47.200 --> 00:33:49.920]   Just trust conventional wisdom.
[00:33:49.920 --> 00:33:51.080]   Your parents, your teachers.
[00:33:51.080 --> 00:33:52.900]   And so we grow up, and we're always
[00:33:52.900 --> 00:33:54.540]   looking for someone who said so.
[00:33:54.540 --> 00:33:55.660]   So many things.
[00:33:55.660 --> 00:33:59.820]   We just need someone who said so to move forward.
[00:33:59.820 --> 00:34:03.840]   And so that's the same thing as a flood geologist
[00:34:03.840 --> 00:34:05.440]   holding their Bible.
[00:34:05.440 --> 00:34:08.380]   Someone who said so, listening to some other authority
[00:34:08.380 --> 00:34:09.980]   over your own reasoning.
[00:34:09.980 --> 00:34:14.280]   And so I like to kind of create just a little term for this
[00:34:14.280 --> 00:34:15.400]   that helps me remember it.
[00:34:15.400 --> 00:34:16.460]   I call it cooks and chefs.
[00:34:16.460 --> 00:34:21.520]   So a chef, I think of as someone who
[00:34:21.520 --> 00:34:24.240]   uses raw ingredients as their first principles,
[00:34:24.240 --> 00:34:28.240]   and they experiment, and they just try
[00:34:28.240 --> 00:34:30.660]   different kinds of things, and they make a lot of bad food,
[00:34:30.660 --> 00:34:32.040]   and they fail a lot.
[00:34:32.040 --> 00:34:34.240]   That's part of it, because they're raw experimenting.
[00:34:34.240 --> 00:34:36.580]   But sometimes when they come up with something good,
[00:34:36.580 --> 00:34:38.120]   it's totally original.
[00:34:38.120 --> 00:34:41.800]   And all the cooks then who follow recipes
[00:34:41.800 --> 00:34:44.000]   are suddenly looking over, wait, what are they doing?
[00:34:44.000 --> 00:34:45.920]   Think about the iPhone.
[00:34:45.920 --> 00:34:49.200]   It's-- there was a recipe, and it was not that.
[00:34:49.200 --> 00:34:51.640]   There was a recipe, you had a keyboard, for example.
[00:34:51.640 --> 00:34:55.840]   But Apple didn't say, well, we have to have a keyboard,
[00:34:55.840 --> 00:34:58.640]   of course, so let's make it a nice clicky, Apple-y keyboard.
[00:34:58.640 --> 00:35:00.640]   They said, what should a mobile device be?
[00:35:00.640 --> 00:35:02.600]   They just went totally to first principles,
[00:35:02.600 --> 00:35:05.100]   and they puzzled their way up with today's technology,
[00:35:05.100 --> 00:35:06.200]   not five years ago.
[00:35:06.200 --> 00:35:08.780]   And by the time they got to the top, there was no keyboard.
[00:35:08.780 --> 00:35:11.240]   And then, of course, all the cooks suddenly see the good
[00:35:11.240 --> 00:35:12.940]   idea, and they copy it.
[00:35:12.940 --> 00:35:18.420]   So to be more of a chef, which is what I think is called for
[00:35:18.420 --> 00:35:21.140]   in a time like this, just two little reminders that
[00:35:21.140 --> 00:35:22.460]   can help you stay there.
[00:35:22.460 --> 00:35:24.780]   First is just, you don't know shit.
[00:35:24.780 --> 00:35:28.580]   And I mean that in that there's some Bibles in your hand
[00:35:28.580 --> 00:35:33.020]   right now that you are, without thinking about it,
[00:35:33.020 --> 00:35:36.620]   just totally assuming are correct about how the world is,
[00:35:36.620 --> 00:35:38.320]   about how it's going to be in the future,
[00:35:38.320 --> 00:35:40.740]   about what's good and bad.
[00:35:40.740 --> 00:35:43.260]   But if you just stop there, then you won't want to do anything,
[00:35:43.260 --> 00:35:44.340]   because I don't know shit.
[00:35:44.340 --> 00:35:48.660]   You have to remember that also no one else knows shit either.
[00:35:48.660 --> 00:35:51.580]   The confusion that you feel right now,
[00:35:51.580 --> 00:35:54.060]   and the kind of overwhelming feeling
[00:35:54.060 --> 00:35:55.820]   you have about large language models
[00:35:55.820 --> 00:35:57.280]   and all the other things happening,
[00:35:57.280 --> 00:35:59.380]   everyone feels that way right now.
[00:35:59.380 --> 00:36:01.700]   No one understands this stuff that well.
[00:36:01.700 --> 00:36:03.300]   No one knows where it's going.
[00:36:03.300 --> 00:36:05.420]   And so you put those two together,
[00:36:05.420 --> 00:36:07.380]   and you get this kind of attitude, which is,
[00:36:07.380 --> 00:36:09.620]   I don't know what's going on, but I'm as good a person
[00:36:09.620 --> 00:36:10.660]   as any to figure it out.
[00:36:10.660 --> 00:36:14.180]   And that's, I think, the kind of mentality
[00:36:14.180 --> 00:36:18.140]   that can actually help you be most effective in a time
[00:36:18.140 --> 00:36:19.900]   of crazy rapid change.
[00:36:19.900 --> 00:36:22.180]   So I want you to remember, because you guys
[00:36:22.180 --> 00:36:24.580]   are the people building this stuff,
[00:36:24.580 --> 00:36:28.380]   that the stakes are really high, that it's really important
[00:36:28.380 --> 00:36:30.460]   to reason from first principles, and just
[00:36:30.460 --> 00:36:34.660]   do whatever you possibly can to proceed wisely.
[00:36:34.660 --> 00:36:35.660]   Thank you.
[00:36:35.660 --> 00:36:38.660]   [APPLAUSE]
[00:36:39.660 --> 00:36:43.020]   [MUSIC PLAYING]
[00:36:43.620 --> 00:36:46.980]   [APPLAUSE]
[00:36:46.980 --> 00:36:49.040]   you

