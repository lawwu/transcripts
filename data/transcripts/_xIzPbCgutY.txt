
[00:00:00.000 --> 00:00:09.480]   Hi everybody, welcome back to lesson 12 of Practical Deep Learning for Coders.
[00:00:09.480 --> 00:00:13.700]   So got a lot of stuff to cover today so let's dive straight in and I actually thought I
[00:00:13.700 --> 00:00:20.380]   would start by sharing something which I've seen been getting a lot of attention recently
[00:00:20.380 --> 00:00:22.700]   which is the Clip Interrogator.
[00:00:22.700 --> 00:00:28.440]   So the Clip Interrogator is a hugging face basis I guess radio app where I uploaded my
[00:00:28.440 --> 00:00:42.440]   image here and it's output, let's just zoom in a bit, it's output a text prompt for creating
[00:00:42.440 --> 00:00:46.760]   a clip embedding from I guess.
[00:00:46.760 --> 00:00:53.240]   So I've seen a lot of folks on Twitter and elsewhere on the internet saying that this
[00:00:53.240 --> 00:01:01.880]   is producing the clip prompt that would generate this image.
[00:01:01.880 --> 00:01:08.240]   And generally speaking the clip, the prompts it creates a rather rude, my one's less rude
[00:01:08.240 --> 00:01:11.960]   than the some although you know extremely long forehead maybe not thanks very much but
[00:01:11.960 --> 00:01:19.160]   your personal data avatar, funny professional photo, I don't know what tectonics is meant
[00:01:19.160 --> 00:01:22.160]   to mean here without eyebrows.
[00:01:22.160 --> 00:01:33.000]   So this doesn't actually return the clip prompt that would generate this photo at all and
[00:01:33.000 --> 00:01:36.880]   the fact that some people are saying that makes me realize that some people have no
[00:01:36.880 --> 00:01:41.840]   idea what's going on with stable diffusion so I thought we might take this as an opportunity
[00:01:41.840 --> 00:01:48.720]   to explain why we can't do that and what we can try and do instead.
[00:01:48.720 --> 00:02:02.480]   So let's imagine that my friend took a photo and of himself and he wanted to send me his
[00:02:02.480 --> 00:02:06.620]   photo and he thought he would compress it a whole lot.
[00:02:06.620 --> 00:02:14.120]   So what he did was he put it through the clip image encoder.
[00:02:14.120 --> 00:02:25.360]   So that's going to take this big image and it's going to turn it into an embedding and
[00:02:25.360 --> 00:02:35.680]   the embedding is much much smaller than the image it's just a vector of a few floats.
[00:02:35.680 --> 00:02:46.200]   So then my friend hopes that they could send me this embedding and so they send that over
[00:02:46.200 --> 00:02:49.680]   at an email and they say there you go Jeremy there's a clip embedding of the photo I wanted
[00:02:49.680 --> 00:03:00.080]   to send you so now you just have to decode it to turn it back into a picture.
[00:03:00.080 --> 00:03:11.980]   So now I've got the embedding and I have to decode it. How would you do that? Well you
[00:03:11.980 --> 00:03:20.040]   can't. Okay we have a function here or let's call it f which is the clip image encoder which
[00:03:20.040 --> 00:03:26.800]   takes as import an image which I'll call x and returns an embedding.
[00:03:26.800 --> 00:03:31.720]   Does that mean that there is some other function and inverse functions we normally write with
[00:03:31.720 --> 00:03:37.880]   a minus one an inverse function with which I can take that embedding let's say we call
[00:03:37.880 --> 00:03:53.520]   that y we pass it y and it would give us back our photo.
[00:03:53.520 --> 00:04:01.880]   And so y remember is f of x so to put it another way this is f inverse of f of f of y. So an
[00:04:01.880 --> 00:04:08.320]   inverse function is something that undoes a function and so that gives you back y. Is
[00:04:08.320 --> 00:04:17.240]   there an inverse function for the clip image encoder? Well not everything has an has an
[00:04:17.240 --> 00:04:26.200]   inverse function. For example consider the function like let's say in Python which takes
[00:04:26.200 --> 00:04:40.880]   f of x return 0. Can you invert that function? If you get back you pass in 3 you get back
[00:04:40.880 --> 00:04:47.200]   0. Is there a function that's going to take the output and give you back the input? No
[00:04:47.200 --> 00:04:54.400]   of course not because you just threw the whole thing away. So not all functions can be inverted
[00:04:54.400 --> 00:05:01.600]   and indeed in this case we've started with a function which is whatever 512 by 512 by
[00:05:01.600 --> 00:05:09.240]   3 say and we've turned it into about something much much smaller. I can't remember exactly
[00:05:09.240 --> 00:05:14.000]   how big a clip image encoding encoding is embedding is but it's much smaller. So clearly
[00:05:14.000 --> 00:05:27.560]   we're losing something. But what I could do is I could put it through a diffusion process
[00:05:27.560 --> 00:05:34.480]   and so remember a diffusion process is something where we have learned we have taught or I
[00:05:34.480 --> 00:05:41.960]   don't know taught an algorithm has learned to take some noise so we could start with
[00:05:41.960 --> 00:05:47.200]   some noise and we could start with an image embedding. We haven't done this before but
[00:05:47.200 --> 00:05:51.120]   we could do that. We could train something that takes noise in an image embedding and
[00:05:51.120 --> 00:05:57.800]   removes a bit of the noise and we could run that a bunch of times. And it wouldn't give
[00:05:57.800 --> 00:06:02.920]   us back the original picture but hopefully it would give us something back if it's a
[00:06:02.920 --> 00:06:08.040]   conditional. So remember using the conditional diffusion approach we'd get back something
[00:06:08.040 --> 00:06:15.040]   that might be something like our original image. So that's what diffusion is right.
[00:06:15.040 --> 00:06:22.920]   Diffusion is something that takes an embedding and inverts an encoder to give you back something
[00:06:22.920 --> 00:06:28.320]   that hopefully might generate that embedding. Now of course remember we don't actually get
[00:06:28.320 --> 00:06:39.320]   image embeddings when we do prompts in stable diffusion instead we have text embeddings.
[00:06:39.320 --> 00:06:45.600]   But if you remember that actually doesn't matter because you remember how we actually
[00:06:45.600 --> 00:06:56.160]   or we open AI trained clip so that they had various pictures along with their captions
[00:06:56.160 --> 00:07:04.600]   and they trained an algorithm that was explicitly designed to make it so that each image returned
[00:07:04.600 --> 00:07:12.200]   a embedding for the image that was similar to the embedding that the text encoder created
[00:07:12.200 --> 00:07:19.960]   for the caption. And remember all of the stuff that didn't match it was trained to be different.
[00:07:19.960 --> 00:07:27.520]   And so that means that a text embedding which describes this picture and the actual image
[00:07:27.520 --> 00:07:32.920]   embedding of this picture should be very similar if they're clip embeddings. That's the definition
[00:07:32.920 --> 00:07:40.120]   of clip embeddings. So you see this idea that you could take a text or image embedding and
[00:07:40.120 --> 00:07:48.760]   turn it back into an image perfectly makes no sense. This is the very definition of the
[00:07:48.760 --> 00:07:54.000]   thing we're trying to do when we do clip. And because what we're basically trying to
[00:07:54.000 --> 00:08:02.080]   do is invert the embedding function these kinds of problems are generally referred to
[00:08:02.080 --> 00:08:09.600]   as inverse problems. So stable diffusion is something that attempts to approximate the
[00:08:09.600 --> 00:08:21.360]   solution to an inverse problem. So why does that mean that clip interrogator is not actually
[00:08:21.360 --> 00:08:27.360]   inverting the picture to give us back the text. Well it's just as nonsensical if we've
[00:08:27.360 --> 00:08:34.480]   got an image embedding right trying to undo that to get back to the picture and trying
[00:08:34.480 --> 00:08:44.680]   to undo that to get back to a suitable prompt is equally infeasible. Both of them require
[00:08:44.680 --> 00:08:50.880]   inverting an encoder and that just doesn't exist. The best we can do is the best we know
[00:08:50.880 --> 00:08:59.680]   how to do at the moment is to approximate that using a diffusion process. OK. So that's
[00:08:59.680 --> 00:09:08.520]   why these texts that it spits back are fun and interesting but they are not the thing
[00:09:08.520 --> 00:09:14.440]   that you can put back into stable diffusion and have it generate the same photo. And the
[00:09:14.440 --> 00:09:24.560]   nice thing is that actually the code for this is available. And you can take a look at it.
[00:09:24.560 --> 00:09:34.040]   It's the app. And you'll see what it does is it has a big list of let's have a look
[00:09:34.040 --> 00:09:44.280]   at some examples. So it has a big this has big lists of examples for example a big list
[00:09:44.280 --> 00:09:58.800]   of artists and it has a big list of mediums and a big list of movements and so forth.
[00:09:58.800 --> 00:10:07.840]   It's got all this hard coded pieces of text. And so what it does is it basically mixes
[00:10:07.840 --> 00:10:14.840]   and matches those various things together to see which one works well and it combines
[00:10:14.840 --> 00:10:20.880]   it with the output of something called the blip language model which is not designed
[00:10:20.880 --> 00:10:27.680]   to give you an exactly accurate description of an image but it has been specifically trained
[00:10:27.680 --> 00:10:33.160]   to give an OK ish caption for an image and it actually works reasonably well. But again
[00:10:33.160 --> 00:10:52.920]   it's not it's not the inverse of the clip encoder. So OK so that's how that all works.
[00:10:52.920 --> 00:10:59.760]   So where we had got to was that we had done matrix multiplication with broadcasting where
[00:10:59.760 --> 00:11:12.880]   we had broadcast the the entire column from the right hand matrix all at once. And that
[00:11:12.880 --> 00:11:18.400]   allowed us to get it down to a point where we only have one for loop written in Python.
[00:11:18.400 --> 00:11:23.320]   And generally speaking we do not want to be doing loop looping through too many things
[00:11:23.320 --> 00:11:29.880]   in Python because that's a slow bit. So the two inner loops we originally had which just
[00:11:29.880 --> 00:11:41.120]   remind us originally were here these two inner loops looping through 10 and then to 784 respectively
[00:11:41.120 --> 00:11:47.680]   have been replaced with a single line of code. So that was pretty great. And our times now
[00:11:47.680 --> 00:11:56.400]   is increased is improved by 5000 times. So we're 5000 times faster than we started out.
[00:11:56.400 --> 00:12:01.760]   So another trick that we can use which I'm a big fan of is something called Einstein
[00:12:01.760 --> 00:12:11.000]   summation. And Einstein summation is a compact representation for representing products and
[00:12:11.000 --> 00:12:20.120]   sums. And this is an example of an Einstein summation. And what we're going to do now
[00:12:20.120 --> 00:12:25.320]   is we're going to replicate our matrix product with an Einstein summation. And believe it
[00:12:25.320 --> 00:12:32.200]   or not the entire thing can be pushed down to just these characters which is pretty amazing.
[00:12:32.200 --> 00:12:37.080]   So let me explain what's happening here. The arrow is separating the left hand side from
[00:12:37.080 --> 00:12:43.960]   the right hand side. The left hand side is the inputs. The right hand side is the output.
[00:12:43.960 --> 00:12:49.920]   The comma is between each input so there are two inputs. The letters are just names that
[00:12:49.920 --> 00:12:54.440]   you're giving to the number of rows and the number of columns. So the first matrix we're
[00:12:54.440 --> 00:13:07.160]   multiplying by has i rows and k columns. The second has k rows and j columns. It's going
[00:13:07.160 --> 00:13:13.600]   to go through a process which creates a new matrix that actually this is not even doing
[00:13:13.600 --> 00:13:17.840]   this is not yet doing the matrix multiplication. This is without the sum. This one's going
[00:13:17.840 --> 00:13:24.800]   to create a new matrix that contains i rows and k - well how do we say it - i faces and
[00:13:24.800 --> 00:13:35.000]   k rows and j columns so a rank 3 tensor. So the number of letters is going to be the rank.
[00:13:35.000 --> 00:13:40.440]   And the rules of how this works is that if you repeat letters between input arrays - so
[00:13:40.440 --> 00:13:46.440]   here's my inputs i, k and k, j we've got a repeated letter - it means that values along
[00:13:46.440 --> 00:13:56.000]   those axes will be multiplied together. So it means that each item in each row of sorry
[00:13:56.000 --> 00:14:04.720]   in each yeah across a row will be multiplied by each item down each column to create this
[00:14:04.720 --> 00:14:14.560]   i by k by j output tensor. So to remind you our first matrix is 5 by 784. That's m1. Our
[00:14:14.560 --> 00:14:29.680]   second matrix is 7084 by 10. That's m2. So i is 5, k is 784 and j is 10. So if I do this
[00:14:29.680 --> 00:14:39.480]   torch.insum then I will end up with a i by k by j. It'll be 5 by 784 by 10. And if you
[00:14:39.480 --> 00:14:45.640]   have a look I've run it here on these two tensors m1 and m2 and the shape of the result
[00:14:45.640 --> 00:14:54.440]   is 5 plus 784 by 10. And what it contains is the original five rows of m1, the original
[00:14:54.440 --> 00:15:02.400]   ten columns of m2 and then for the other 784 that dimension they're all multiplied together
[00:15:02.400 --> 00:15:14.720]   because it's been copied between the two arguments to the iinsum. And so if we now sum up that
[00:15:14.720 --> 00:15:27.080]   over this dimension we get back. So what we get back if we go back to the original matrix
[00:15:27.080 --> 00:15:37.440]   multiply we do we had 10.94 negative negative 0.68 etc. And so now with this Einstein summation
[00:15:37.440 --> 00:15:47.600]   version we've got back exactly the same thing. Because what it's done is it's taken each
[00:15:47.600 --> 00:15:56.800]   of these columns by rows multiply them together to get this 5 by 784 by 10 and then add it
[00:15:56.800 --> 00:16:04.000]   up that's 784 for each one which is exactly what matrix multiplication does. But we're
[00:16:04.000 --> 00:16:09.800]   going to use one of the two things from Einstein summation. The second one says if we omit
[00:16:09.800 --> 00:16:14.960]   a letter from the output so the bit on the right of the arrow it means those values will
[00:16:14.960 --> 00:16:26.480]   be summed. So if we remove this k which gives us i k and k j goes to i j so we've removed
[00:16:26.480 --> 00:16:33.560]   the k entirely that means that sum happens automatically. So if we run this as you see
[00:16:33.560 --> 00:16:44.720]   we get back again matrix multiplication. So Einstein summation notation is you know it
[00:16:44.720 --> 00:16:49.880]   takes some practice getting used to but it's very convenient. And once you get used to
[00:16:49.880 --> 00:16:54.440]   it it's actually a really nice way of thinking about what's going on. And as we'll see in
[00:16:54.440 --> 00:17:01.160]   lots of examples often you can really simplify your code by using just a tiny little Einstein
[00:17:01.160 --> 00:17:08.040]   summation. And it doesn't even have to be a sum right. You don't have to admit any letters
[00:17:08.040 --> 00:17:14.720]   if you're just doing products. So maybe it's a bit misnamed. So we can now define our map
[00:17:14.720 --> 00:17:24.280]   mole as simply this torch dot own sum. So if we now check it test close that the original
[00:17:24.280 --> 00:17:30.640]   result is equal to this new map mole. And yes it is. And let's see how the speed looks.
[00:17:30.640 --> 00:17:42.920]   15 milliseconds. OK. And that was for the whole thing. So compared to 600 milliseconds.
[00:17:42.920 --> 00:17:47.840]   So as you can see this is much faster than even the very fast broadcasting approach we
[00:17:47.840 --> 00:17:56.800]   used. So this is a pretty good trick is torch dot own sum. OK. But of course we don't have
[00:17:56.800 --> 00:18:01.120]   to do any of those things because PyTorch already knows how to do map mole. So there's
[00:18:01.120 --> 00:18:08.840]   two ways we can run map mole directly in PyTorch. You can use this special at operator. So X
[00:18:08.840 --> 00:18:15.360]   train at weights is the same as map mole train comma weights. As you see test place. Or you
[00:18:15.360 --> 00:18:22.400]   can say torch dot map mole. And interestingly as you can see here the speed is about the
[00:18:22.400 --> 00:18:27.440]   same as the own sum. So there's no particular harm no particular reason not to do an own
[00:18:27.440 --> 00:18:34.960]   sum. So when I say own sum that stands for Einstein summation notation. All right. Let's
[00:18:34.960 --> 00:18:42.440]   go faster still. Currently we're just using my CPU. But I have a GPU. It would be nice
[00:18:42.440 --> 00:18:56.040]   to use it. So how does a GPU work. An Nvidia GPU and indeed pretty much all GPUs. The way
[00:18:56.040 --> 00:19:03.160]   they work is that they do lots and lots of things in parallel. And you have to actually
[00:19:03.160 --> 00:19:09.440]   tell the GPU what are all the things you want to do in parallel one at a time. And so what
[00:19:09.440 --> 00:19:14.520]   we're going to do is we're going to write in pure Python something that works like a
[00:19:14.520 --> 00:19:19.280]   GPU except it won't actually be in parallel so it won't be fast at all. But the first
[00:19:19.280 --> 00:19:23.240]   thing we have to do if we're going to get something working in parallel is we have to
[00:19:23.240 --> 00:19:28.760]   create a function that can calculate just one thing even if a thousand other things
[00:19:28.760 --> 00:19:35.080]   are happening at the same time it won't interact with anything else. And there's actually a
[00:19:35.080 --> 00:19:41.440]   very easy way to think about matrix multiplication in this way which is what if we try to create
[00:19:41.440 --> 00:19:52.360]   something which just as we've done here fills in a single a single item of the result. So
[00:19:52.360 --> 00:20:00.000]   how do we create something that just fills in a row 0 column 0. Well what we could do
[00:20:00.000 --> 00:20:06.320]   is we could create a new map model where we're going to pass in the coordinates of the place
[00:20:06.320 --> 00:20:12.800]   that we want to fill in. So we're going to start by passing it 0 comma 0. We'll pass
[00:20:12.800 --> 00:20:19.040]   it the matrix matrices we want to multiply and we'll pass in a tensor that we've prefilled
[00:20:19.040 --> 00:20:23.240]   in with zeros to put the result into. So we're going to say okay the result is torch dot
[00:20:23.240 --> 00:20:32.800]   zeros rows by columns call map model for location 0 comma 0 passing in those two matrices and
[00:20:32.800 --> 00:20:38.120]   the bunch of zeros matrix ready to put the result in. And if we call that we get the
[00:20:38.120 --> 00:20:44.840]   answer in cell 0 0. So here's an implementation of that. So the implementation is first of
[00:20:44.840 --> 00:20:50.600]   all we've been past the 0 comma 0 coordinates. So let's destructure them. So hopefully you've
[00:20:50.600 --> 00:20:54.560]   been experimenting with destructuring because it's so important you see it all the time
[00:20:54.560 --> 00:21:00.640]   into i and j. That's the row in the column. Make sure that that is inside the bounds of
[00:21:00.640 --> 00:21:09.360]   our output matrix. And we're going to start by start at 0 and loop through all of the
[00:21:09.360 --> 00:21:16.440]   rows of A and all of the columns of B for i and j. Sorry all of the columns of A and
[00:21:16.440 --> 00:21:23.200]   all of the rows of B for i and j. Just like the very innermost loop of our very first
[00:21:23.200 --> 00:21:29.400]   Python attempt. And then at the end pop that into the output. So here's something that
[00:21:29.400 --> 00:21:38.240]   fills in one piece of the grid successfully. So we could call this row by columns times
[00:21:38.240 --> 00:21:42.200]   each time passing in a different grid. And we could do that in parallel because none
[00:21:42.200 --> 00:21:48.240]   of those different locations interact with any other location. So something which can
[00:21:48.240 --> 00:21:55.240]   calculate a little piece of of an output on a GPU is called a kernel. So we'd call this
[00:21:55.240 --> 00:22:01.000]   a kernel. And so now we can create something called launch kernel. We pass it the kernel.
[00:22:01.000 --> 00:22:07.240]   So that's the function. Here's an example launch kernel passing in the function. And
[00:22:07.240 --> 00:22:14.720]   how many rows and how many columns are there in the output grid. And then give me any arguments
[00:22:14.720 --> 00:22:22.040]   that you need to calculate it. So in Python star args just says any additional arguments
[00:22:22.040 --> 00:22:27.520]   that you pass are going to be put into an array called args. If you use something like
[00:22:27.520 --> 00:22:35.160]   C you might have seen like variadic arguments parameters. It's the same basic idea. So we're
[00:22:35.160 --> 00:22:41.280]   going to be calling launch kernel. We're going to be saying launch the kernel matmul using
[00:22:41.280 --> 00:22:46.440]   all the rows of A all the columns of B and then the args which are going to be in star
[00:22:46.440 --> 00:22:52.720]   args are going to be M1 the first matrix M2 the second matrix and res another torch.zeros
[00:22:52.720 --> 00:23:02.160]   we just created. So launch kernel is going to loop through the rows of A and then for
[00:23:02.160 --> 00:23:10.200]   each row of A it'll loop through the columns of B and call the kernel which is matmul on
[00:23:10.200 --> 00:23:18.860]   that grid location passing in M1 M2 and res. So starargs here is going to unpack that and
[00:23:18.860 --> 00:23:28.800]   pass them as three separate arguments. And if I run that and all of that you'll see it's
[00:23:28.800 --> 00:23:36.680]   done it. It's filled in the exact same matrix. OK so that's actually not fast at all. It's
[00:23:36.680 --> 00:23:41.880]   not doing anything in parallel but it's the basic idea. So now to actually do it in parallel
[00:23:41.880 --> 00:23:49.080]   we have to use something called CUDA. So CUDA is a programming model for NVIDIA GPUs and
[00:23:49.080 --> 00:23:54.320]   to program in CUDA from Python the easiest way currently to do that is with something
[00:23:54.320 --> 00:23:59.760]   called number. And number is a compiler where you've seen it actually already for non GPU.
[00:23:59.760 --> 00:24:07.440]   It's a compiler that takes Python code and spits out you know compiled fast machine code.
[00:24:07.440 --> 00:24:13.880]   If you use its CUDA module it'll actually spit out GPU accelerated CUDA code. So rather
[00:24:13.880 --> 00:24:22.680]   than using an N-Git like before we now say at CUDA.Git and it behaves a little bit differently.
[00:24:22.680 --> 00:24:27.640]   But you'll see that this matmul let me copy the other one over so you can compare it to
[00:24:27.640 --> 00:24:35.600]   our Python one. Now Python matmul and this CUDA.Git matmul look I think identical except
[00:24:35.600 --> 00:24:41.800]   for one thing instead of passing in the grid there's a special magic thing called CUDA.grid
[00:24:41.800 --> 00:24:45.780]   and you say how many dimensions does my grid have and you unpack it. So that's you don't
[00:24:45.780 --> 00:24:52.060]   have to it's just a little convenience that number does for you. You don't have to pass
[00:24:52.060 --> 00:24:57.280]   over the grid it passes it over for you so it doesn't need this grid. Other than that
[00:24:57.280 --> 00:25:03.840]   these two are identical but the decorator is going to compile that into GPU code. So
[00:25:03.840 --> 00:25:11.040]   now we need to create our output tensor just like before and we need to do something else
[00:25:11.040 --> 00:25:19.400]   which is we have to take our input matrices and our output so our input tensors matrices
[00:25:19.400 --> 00:25:23.760]   in this case in the output tensor and we have to move them to the GPU. I should say copy
[00:25:23.760 --> 00:25:31.800]   them to the GPU so CUDA device copies a tensor to the GPU and so we've got three things getting
[00:25:31.800 --> 00:25:39.660]   copied to the GPU here and therefore we store the three things over here. Another way I
[00:25:39.660 --> 00:25:46.960]   could have written this is I could have said map which I kind of quite like doing a function
[00:25:46.960 --> 00:25:59.520]   which is CUDA device to each of these arguments and this would be the same thing. This is
[00:25:59.520 --> 00:26:05.140]   going to call CUDA device on X train and put it in here on weights and put it in here and
[00:26:05.140 --> 00:26:16.920]   on R and put it in here. That's a slightly more convenient way to do it. Okay so we've
[00:26:16.920 --> 00:26:21.380]   got our 50,000 by 10 output that's just all zeros of course that's just how we created
[00:26:21.380 --> 00:26:32.380]   it and now we're going to try and fill it in. There is a there's a particular detail
[00:26:32.380 --> 00:26:39.520]   that you don't have to worry about too much which is in CUDA they don't just have a grid
[00:26:39.520 --> 00:26:45.140]   but there's also a concept of blocks and there's something we call here TPB which is threads
[00:26:45.140 --> 00:26:50.240]   per block. This is just a detail of the CUDA programming model you don't have to worry
[00:26:50.240 --> 00:26:55.240]   about too much you can just basically copy this and what it's going to do is it's going
[00:26:55.240 --> 00:27:04.060]   to call each grid item in parallel and with a number of different processes basically.
[00:27:04.060 --> 00:27:09.840]   So this is just the code which turns the grid into blocks and so you don't have to worry
[00:27:09.840 --> 00:27:18.720]   too much about the details of that you just always run it. Okay and so now how do you
[00:27:18.720 --> 00:27:27.380]   call the equivalent of launch kernel well it's it's a slightly weird way to do it but
[00:27:27.380 --> 00:27:32.860]   it works fine you call matmul but because matmul has CUDA.jit it's got a special thing
[00:27:32.860 --> 00:27:36.900]   which is you have to put something in square brackets afterwards which is you have to tell
[00:27:36.900 --> 00:27:41.580]   it how many blocks per grid that's just the result from the previous cell and how many
[00:27:41.580 --> 00:27:46.140]   threads per block in each of the two dimensions. So again you can just copy and paste this
[00:27:46.140 --> 00:27:52.380]   from my version but then you pass in the three arguments to the function this will be A B
[00:27:52.380 --> 00:27:57.700]   and C and this will this is this is how you launch a kernel so this will launch the kernel
[00:27:57.700 --> 00:28:05.340]   matmul on the GPU. At the end of it RG is going to get filled in it's gone it's on the
[00:28:05.340 --> 00:28:10.300]   GPU which is not much good to us so we now have to copy it back to the CPU which is called
[00:28:10.300 --> 00:28:17.840]   the host copy to host to run that and it's done and test close shows us that our result
[00:28:17.840 --> 00:28:25.260]   is similar to our original results so it seems to be working so that's great. So I see Siva
[00:28:25.260 --> 00:28:32.460]   on the YouTube chat is finding that it's not working on his Mac that's right so this will
[00:28:32.460 --> 00:28:38.420]   only work on an Nvidia GPU as basically all of the GPU nearly all the GPU stuff we look
[00:28:38.420 --> 00:28:48.020]   at only works on Nvidia GPUs. Mac GPUs are gradually starting to get a little bit of support
[00:28:48.020 --> 00:28:54.500]   from machine learning libraries but it's taking quite a while it's been you know it's got
[00:28:54.500 --> 00:28:59.980]   quite a way to go as I say this at least towards the end of 2022 if this works for you and
[00:28:59.980 --> 00:29:08.220]   later on that's yeah that's great. Okay so let's time how fast that is. Okay so that
[00:29:08.220 --> 00:29:15.580]   was three point six one milliseconds and so if we compare that to the PyTorch matmul on
[00:29:15.580 --> 00:29:25.140]   CPU that was 15 milliseconds so that's great so it's faster still so how much faster. Oh
[00:29:25.140 --> 00:29:30.660]   by the way we can actually go faster than that which is we can use the exact same code we
[00:29:30.660 --> 00:29:37.180]   had from the PyTorch op but here's a trick if you just take your tensor and write dot
[00:29:37.180 --> 00:29:45.460]   CUDA after it it copies it over to the GPU if it's on a if it's on a Nvidia GPU do the
[00:29:45.460 --> 00:29:50.660]   same for weights dot CUDA so these are our two CUDA versions and now I can do the whole
[00:29:50.660 --> 00:29:56.820]   thing and this will actually run on the GPU and then to copy it back to the host you just
[00:29:56.820 --> 00:30:09.940]   say dot CPU so if we look to see how fast that is 458 microseconds so oh that is somebody
[00:30:09.940 --> 00:30:17.220]   just pointed out that I wrote the wrong thing here one e neg three okay so how much faster
[00:30:17.220 --> 00:30:26.260]   is that well 458 microseconds our original on the whole data set was 663 microseconds
[00:30:26.260 --> 00:30:35.580]   so compared to our broadcast version we are another thousand times faster so overall this
[00:30:35.580 --> 00:30:50.560]   version here compared to our original version which was here the difference in performance
[00:30:50.560 --> 00:31:05.460]   is 5 million X so when you see people say yeah Python can be pretty slow can be better
[00:31:05.460 --> 00:31:11.440]   to run stuff on the GPU if possible we're not talking about a 20% change we're talking
[00:31:11.440 --> 00:31:21.060]   about a 5 million X change so that's a big deal and so that's why you need to be running
[00:31:21.060 --> 00:31:35.620]   stuff on the GPU all right some folks on YouTube are wondering how on earth I'm running CUDA
[00:31:35.620 --> 00:31:41.940]   when I'm on a Mac and given it says localhost here that's because I'm using something called
[00:31:41.940 --> 00:31:47.220]   SSH tunneling which we might get to sometime I suspect my live coding from the previous
[00:31:47.220 --> 00:31:52.500]   course might have covered that already but this is basically you can use a Jupyter notebook
[00:31:52.500 --> 00:31:57.700]   that's running anywhere in the world from your own machine using something called SSH
[00:31:57.700 --> 00:32:08.300]   tunneling which is a good thing to look up okay one person asks if Einstein summation
[00:32:08.300 --> 00:32:16.100]   borrows anything from APL yes it does actually so it's kind of the other way around actually
[00:32:16.100 --> 00:32:22.020]   APL borrows it from Einstein notation so I don't know if you remember I mentioned that
[00:32:22.020 --> 00:32:32.940]   when Iverson when he developed APL was heavily influenced by tensor analysis and so this Einstein
[00:32:32.940 --> 00:32:39.300]   notation is very heavily used there if you'll notice a key thing that happens in Einstein
[00:32:39.300 --> 00:32:45.820]   notation is there's no loop you know there isn't this kind of Sigma you know I from here
[00:32:45.820 --> 00:32:50.620]   to here and then you put the I inside the function that you're summing up everything's
[00:32:50.620 --> 00:33:00.060]   implicit and APL takes that a very long way and and J takes it even further which is what
[00:33:00.060 --> 00:33:08.100]   can Iverson developed after APL and this kind of general idea of removing the index is very
[00:33:08.100 --> 00:33:18.060]   important in APL and it's become very important in numpy pytorch tensor flow and so forth
[00:33:18.060 --> 00:33:29.220]   so finally we know how to multiply matrices congratulations so let's practice that let's
[00:33:29.220 --> 00:33:41.580]   practice what we've learned so we're going to go to zero to main shift to practice this
[00:33:41.580 --> 00:33:50.260]   and so we're going to try to exercise our kind of tensor manipulation operation muscles
[00:33:50.260 --> 00:33:58.720]   in this section and the key actually endpoint for this is the homework and so what you need
[00:33:58.720 --> 00:34:02.860]   to be doing is getting yourself to a point that you can implement something like this
[00:34:02.860 --> 00:34:11.020]   but for a different algorithm why do we care about this because this is like learning your
[00:34:11.020 --> 00:34:18.300]   times table your times tables if you're doing you know mathematics it's this kind of like
[00:34:18.300 --> 00:34:23.740]   thing that's going to come up all the time and if you're not good at your times tables
[00:34:23.740 --> 00:34:27.500]   everything else a lot more a lot of other things particularly at primary school in high
[00:34:27.500 --> 00:34:37.340]   school you know they they get difficult you get slower and it's frustrating and you spend
[00:34:37.340 --> 00:34:43.820]   time thinking about these mechanical operations rather than getting your work done it is it's
[00:34:43.820 --> 00:34:48.740]   important that when you have an idea about something you want to try or debug or profile
[00:34:48.740 --> 00:34:54.500]   or whatever that you can quickly translate that into working code and the way that code
[00:34:54.500 --> 00:35:10.020]   is written for GPUs or even for fast running on CPUs is using broadcasting Einstein notation
[00:35:10.020 --> 00:35:16.180]   matrix modifications and so forth so you've got to you've got to got to got to practice
[00:35:16.180 --> 00:35:28.180]   super important so we're going to practice it by running by developing a clustering algorithm
[00:35:28.180 --> 00:35:34.060]   and the clustering algorithm we're going to work on is something called mean shift clustering
[00:35:34.060 --> 00:35:37.420]   which hopefully you've never heard of before and I say that because I just think it's a
[00:35:37.420 --> 00:35:45.620]   really funny algorithm that not many people have come across excuse me and I think you'll
[00:35:45.620 --> 00:35:53.980]   find it really useful so what is cluster analysis cluster analysis is very different to anything
[00:35:53.980 --> 00:35:58.820]   that we've worked on in this course so far and that there isn't a dependent variable
[00:35:58.820 --> 00:36:06.180]   that we're trying to match but instead we're just trying to find are there groups of similar
[00:36:06.180 --> 00:36:11.860]   things in this data and those groups we call clusters and as you can see from the wiki
[00:36:11.860 --> 00:36:20.980]   page there's all kinds of applications of cluster analysis across many different areas
[00:36:20.980 --> 00:36:28.820]   I will say that sometimes cluster analysis can be overused or misused it's really best
[00:36:28.820 --> 00:36:41.940]   for when your various columns are the same kind of thing and have the same kind of scale
[00:36:41.940 --> 00:36:47.500]   for example pixels are all the same kind of thing they're all pixels so one of the examples
[00:36:47.500 --> 00:36:54.300]   they use is market research so I wouldn't use cluster analysis for socio demographic
[00:36:54.300 --> 00:36:58.380]   inputs because they're all different kinds of things but the example they give here makes
[00:36:58.380 --> 00:37:03.340]   a lot of sense which is looking at data from surveys if you've got a whole bunch of like
[00:37:03.340 --> 00:37:11.660]   from one to five answers on surveys alright so let's take a look at this and the way I
[00:37:11.660 --> 00:37:17.340]   like to build my algorithms is to create some often to create some synthetic data that I
[00:37:17.340 --> 00:37:23.300]   know how I want it to behave and so we're going to create six clusters and each cluster
[00:37:23.300 --> 00:37:34.460]   is going to have 750 samples in it so first of all I'm going to randomly create six centroids
[00:37:34.460 --> 00:37:40.020]   and so the centroid is going to be like the middle of where my clusters are so I'm going
[00:37:40.020 --> 00:37:45.300]   to randomly create them I need to end clusters by two so I need an X and a Y coordinate for
[00:37:45.300 --> 00:37:59.980]   each one and so I'm now going to randomly generate data around those six centroids okay so to
[00:37:59.980 --> 00:38:06.860]   do that I'm going to call a little function I made here called sample and I'm going to
[00:38:06.860 --> 00:38:16.380]   run it on each of those six centroids and so I'll show you what that looks like so here's
[00:38:16.380 --> 00:38:22.340]   what that data looks like so the X's are the six centroids and the colored dots is the
[00:38:22.340 --> 00:38:31.340]   data so if you were given this data without the X's the idea would be to come back with
[00:38:31.340 --> 00:38:35.160]   figuring out where the X's would have been like where are the where are these clustering
[00:38:35.160 --> 00:38:39.820]   around and so if you can get clusters that that's that's the goal here is to find out
[00:38:39.820 --> 00:38:46.300]   that there's a few discreetly distinctly different types of data in your data set so for example
[00:38:46.300 --> 00:38:51.840]   for images I've used this before to discover that there are some images that look completely
[00:38:51.840 --> 00:38:55.460]   different to all the other ones for example they were taken at nighttime or they're of
[00:38:55.460 --> 00:39:01.780]   a different object or something like that so how does sample work well we're passing
[00:39:01.780 --> 00:39:16.860]   in the centroid and so what we want is we're going to get back so each of those centroids
[00:39:16.860 --> 00:39:21.460]   contains an X and a Y so multivariate normal is just like normal it's going to give you
[00:39:21.460 --> 00:39:27.700]   back normally distributed data but more than one item that's why it's multivariate and
[00:39:27.700 --> 00:39:33.700]   so we passed in two means a mean for X and a mean for our Y and so that's the mean that
[00:39:33.700 --> 00:39:42.220]   we're going to get and our standard deviation is going to be 5 why do we use torch.diag 5,5
[00:39:42.220 --> 00:39:47.740]   that's because we're saying that's because that for multivariate normal distributions
[00:39:47.740 --> 00:39:52.940]   there's not just one standard deviation for each column that you get back there could
[00:39:52.940 --> 00:39:58.940]   also be a connection between columns the columns might not be independent so you actually need
[00:39:58.940 --> 00:40:05.420]   so it's called a covariance matrix not just to make not just a variance we discussed that
[00:40:05.420 --> 00:40:10.420]   a little bit more in lesson 9b if you're interested in learning more about that okay so this is
[00:40:10.420 --> 00:40:15.100]   something that's going to give us back random columns of data with this mean and this standard
[00:40:15.100 --> 00:40:21.940]   deviation and this is the number of samples that we want and this is coming from PyTorch
[00:40:21.940 --> 00:40:26.500]   so PyTorch has a whole bunch of different distributions that you can use which can be
[00:40:26.500 --> 00:40:36.180]   very handy so there's our data okay so remember for clustering we we don't know the different
[00:40:36.180 --> 00:40:42.620]   colors and we don't know where the X's are that's kind of our job is to figure that out we might
[00:40:42.620 --> 00:40:49.140]   just briefly also look at how to plot so in this case we want to plot the X's and we want to plot
[00:40:49.140 --> 00:40:57.740]   the data so it looks like this so all I do is I loop through each centroid and I grab that centroid
[00:40:57.740 --> 00:41:07.740]   samples and they're just all done in order so I grab it from i times n samples up to i plus 1 times
[00:41:07.740 --> 00:41:15.420]   n samples and then I create a scatterplot with the samples on them and what I've done is I've
[00:41:15.420 --> 00:41:21.740]   created an axis here and you'll see why later that we can also pass one in but I'm not passing one in
[00:41:21.740 --> 00:41:27.980]   so you create a plot and an axis and so in that plotlib you can keep plotting things on the same
[00:41:27.980 --> 00:41:41.100]   axis so then I plot on the centroid a big X which is black and then I a smaller X which is what is
[00:41:41.100 --> 00:41:49.300]   that magenta and so that's how I get these X's so that's how plot data works okay so how do we
[00:41:49.300 --> 00:41:56.500]   create something now that starts with all the dots and returns where the X's are we're going to use
[00:41:56.500 --> 00:42:12.220]   a particular algorithm particular clustering algorithm called mean shift and mean shift is
[00:42:12.220 --> 00:42:18.180]   a nice clustering approach because you don't have to say how many clusters there are so it's not
[00:42:18.180 --> 00:42:22.140]   that often that you actually got to know how many clusters there are so we don't have to say quite
[00:42:22.140 --> 00:42:28.060]   a few things like the very popular K means require you to say how many instead we just have to pass
[00:42:28.060 --> 00:42:32.940]   them in called a bandwidth which we'll learn about which can actually be chosen automatically and it
[00:42:32.940 --> 00:42:38.020]   can also handle clusters of any shape so they don't have to be ball shaped like they were they are
[00:42:38.020 --> 00:42:44.900]   here they can be kind of like L shaped or lips shaped or whatever and so what here's what's going
[00:42:44.900 --> 00:43:03.940]   to happen we're going to pick some point so let's say we pick that point just there okay and so
[00:43:03.940 --> 00:43:11.380]   what we now do is we go through each data point so we'll pick the first one and so we then find the
[00:43:11.380 --> 00:43:27.180]   distance between that point and every other point okay so we're going to have to say what is the
[00:43:27.180 --> 00:43:34.020]   distance between that point and that point and that point and that point and that point and also the
[00:43:34.020 --> 00:43:40.260]   ones further away that point and that point and you do it for every single point compared to the
[00:43:40.260 --> 00:43:50.800]   one that we're currently looking at okay so we get all of those as a big list and now what we're
[00:43:50.800 --> 00:44:00.780]   going to do is we're going to take a weighted average of all of those points now that's not
[00:44:00.780 --> 00:44:06.060]   interesting without the weighting if we just take our average of all of the points and how far away
[00:44:06.060 --> 00:44:11.140]   they are we're going to end up somewhere here right this is the average of all the points but
[00:44:11.140 --> 00:44:21.460]   the key is that we're going to take an average and just find the right spot the key is we need to
[00:44:21.460 --> 00:44:28.980]   find an average that is weighted by how far away things are so for example this one over here is
[00:44:28.980 --> 00:44:36.660]   a very long way away from our point of interest and so it should have a very low weight in the
[00:44:36.660 --> 00:44:43.620]   weighted average whereas this point here which is very close should have a very high weight in
[00:44:43.620 --> 00:44:51.540]   our weighted average so what we do is we create weights for every point compared to the one that
[00:44:51.540 --> 00:44:56.420]   we're currently interested in using a what's called a Gaussian kernel that we'll look at but
[00:44:56.420 --> 00:45:02.180]   the key thing to know is that points that are further away from our point of interest which
[00:45:02.180 --> 00:45:07.980]   is this one are going to have lower weights that's what we mean there they're penalized the rate at
[00:45:07.980 --> 00:45:12.740]   which weights four to zero is determined by this thing that we set at the start called the bandwidth
[00:45:12.740 --> 00:45:19.900]   and that's going to be the standard deviation of our Gaussian so we take an average of all the
[00:45:19.900 --> 00:45:26.420]   points in the data set a weighted average weighted by how far away they are so for our point of
[00:45:26.420 --> 00:45:38.020]   interest right the this point is going to get a big weight this point is going to get a big weight
[00:45:38.020 --> 00:45:43.900]   this point is going to get a big weight that point is going to get a tiny weight that points
[00:45:43.900 --> 00:45:54.700]   going to get an even tinier weight so it's mainly going to be a weighted average of these points
[00:45:54.700 --> 00:46:00.380]   that are nearby and the weighted average of those points I would guess is going to be somewhere
[00:46:00.380 --> 00:46:07.300]   around about here right and would have a similar thing for the weighted average of the points near
[00:46:07.300 --> 00:46:15.300]   this one that's going to probably be somewhere around about here or maybe over here and so it's
[00:46:15.300 --> 00:46:20.340]   going to move all of these points in closer it's almost like a gravity right they're kind of going
[00:46:20.340 --> 00:46:33.100]   to be moved like closer and closer in towards this kind of gravitational center and then these ones
[00:46:33.100 --> 00:46:42.380]   will go towards their own gravitational center and so forth okay so let's take a look at it
[00:46:42.380 --> 00:46:56.820]   all right so what's the Gaussian kernel this is the Gaussian kernel which was a sign in the
[00:46:56.820 --> 00:47:04.100]   original March for Science back in the days when the idea of not following scientists was considered
[00:47:04.100 --> 00:47:08.500]   socially unacceptable we used to have much for these things if you remember so this is this is
[00:47:08.500 --> 00:47:13.980]   not normal so this is the definition of the Gaussian kernel which is also known as the normal
[00:47:13.980 --> 00:47:23.820]   distribution this is the shape of it sure you've seen it before and here is that formula copied
[00:47:23.820 --> 00:47:35.860]   directly off the science March sign okay here we go see the square root 2 pi etc okay and this here
[00:47:35.860 --> 00:47:41.460]   is the standard deviation now what does that look like it's very helpful to have something that we
[00:47:41.460 --> 00:47:47.940]   can very quickly plot any function that doesn't come with matplotlib but it's very easy to write
[00:47:47.940 --> 00:47:54.660]   one just say oh let's as X let's use all the numbers from 0 to 10 a hundred of them spaced
[00:47:54.660 --> 00:48:00.780]   evenly that's what lens base does in linearly spaced 100 numbers in this range that's going
[00:48:00.780 --> 00:48:06.620]   to be our X's so plot those X's and plot f of X is the wise so here's a very nice little plot
[00:48:06.620 --> 00:48:17.820]   fuck we want and here it is and as you can see here we've now got something where if you are this
[00:48:17.820 --> 00:48:23.100]   like very close to the point of interest you're going to get a very high weight and if you're a
[00:48:23.100 --> 00:48:28.420]   long way away from the point of interest you'll get a very low weight so that's the key thing
[00:48:28.420 --> 00:48:38.380]   that we wanted to remember is something that penalizes further away points more now you'll
[00:48:38.380 --> 00:48:48.700]   notice here I managed to plot this function for a bandwidth of 2.5 and the way I did that was using
[00:48:48.700 --> 00:48:54.580]   this special thing from funk tools called partial now the first thing to point out here is that very
[00:48:54.580 --> 00:49:00.780]   often drives me crazy I see people trying to find out what something is in Jupiter and the way they
[00:49:00.780 --> 00:49:04.540]   do it is they'll scroll up to the top of the notebook and search through the imports and try
[00:49:04.540 --> 00:49:09.740]   to find it that is the dumb way to do it the smart way to do it is just to type it and press shift
[00:49:09.740 --> 00:49:15.780]   enter and it'll tell you where it comes from and you can get its help with question mark and you
[00:49:15.780 --> 00:49:20.820]   can get its source code with two question marks okay so just type it to find out where it comes
[00:49:20.820 --> 00:49:29.340]   from okay so this is as save as mentioned in the chat also known as carrying or partial function
[00:49:29.340 --> 00:49:39.980]   application this creates a new function so let's just grab it we create a new function and this
[00:49:39.980 --> 00:49:47.340]   function f is is the function Gaussian but it's going to automatically pass BW equals 2.5 this is
[00:49:47.340 --> 00:49:56.500]   a partially applied function so I could type f of 4 for example that's going to be a tensor
[00:49:56.500 --> 00:50:10.500]   there we go and you can see that's exactly what this is go up to 4 go across yep about
[00:50:10.500 --> 00:50:17.260]   0.44 so we use partial function application all the time it's a very very very important
[00:50:17.260 --> 00:50:23.300]   tool without it for example plotting this function would have been more complicated with it it was
[00:50:23.300 --> 00:50:30.460]   trivially easy I guess the alternative like one alternative which would be fine but slightly more
[00:50:30.460 --> 00:50:36.300]   clunky would be we could create a little function in line so we could have said oh plot a function
[00:50:36.300 --> 00:50:47.740]   that I'm going to define right now which is called lamb which is lambda X which is Gaussian of X with
[00:50:47.740 --> 00:50:54.740]   a bandwidth of 0.2 0.5 you could do that too you know it's it's fine but but yeah partials I think
[00:50:54.740 --> 00:51:05.940]   are a bit neater a bit less to think about they often produce some neater and clearer code okay
[00:51:05.940 --> 00:51:16.500]   why did we decide to make the bandwidth 2.5 as a rule of thumb choose a band width which covers
[00:51:16.500 --> 00:51:25.220]   about a third of the data so if we kind of found ourselves somewhere over here right a band which
[00:51:25.220 --> 00:51:33.180]   which covers about a third of the data would be enough to cover two clusters ish so it would be
[00:51:33.180 --> 00:51:44.940]   kind of like this big so somewhere in the middle there so that's the basic idea yeah so but you
[00:51:44.940 --> 00:51:51.700]   can play around with bandwidths and get different amounts of clusters I should mention like often
[00:51:51.700 --> 00:51:56.660]   when you see something that's kind of on the complicated side like a Gaussian you can often
[00:51:56.660 --> 00:52:02.500]   simplify things I think most implementations and write-ups I've seen talk about using Gaussians
[00:52:02.500 --> 00:52:10.140]   but if you look at the shape of it it looks a lot like this shape so this is a triangular weighting
[00:52:10.140 --> 00:52:20.380]   which is just using clamp min so it's just using a linear with clamp min and yeah it occurred to me
[00:52:20.380 --> 00:52:24.900]   that we could probably use this just as well so I did find it I decided to define this triangular
[00:52:24.900 --> 00:52:30.340]   weighting and then we can try both anyway so we'll start with we're going to use the Gaussian version
[00:52:30.340 --> 00:52:41.140]   all right so we're going to be move literally moving all the points towards their kind of
[00:52:41.140 --> 00:52:46.820]   center of gravity so we don't want to mess up our original data so we clone it that's a pie torch
[00:52:46.820 --> 00:53:01.180]   thing is dot clone it's very handy and so big X is our matrix of data I mean it's actually a that's
[00:53:01.180 --> 00:53:10.700]   right matrix of data yeah and then little X will be our first point and it's pretty common to use
[00:53:10.700 --> 00:53:21.580]   big X capital letters for matrices so this is our data this is the first point okay so there it is
[00:53:21.580 --> 00:53:28.460]   we've got to start at twenty six point two twenty six point three so twenty six point two twenty six
[00:53:28.460 --> 00:53:39.580]   point three so somewhere up here so little X its shape is just it's a rank one tensor of shape two
[00:53:39.580 --> 00:53:52.940]   big X is a rank two tensor of 1500 data points by two the X and Y and if we call X none that would
[00:53:52.940 --> 00:53:58.580]   add a unit access to that and the reason I'm going to show you that is because we want to find the
[00:53:58.580 --> 00:54:06.580]   distance from little X to everything in big X and the way we do a distance is with minus but you
[00:54:06.580 --> 00:54:18.140]   wouldn't be able to go you wouldn't be able to go X minus big X and get the right actually do you get
[00:54:18.140 --> 00:54:26.540]   the right answer let's think about that X dot shape oh you've got that already oh no actually
[00:54:26.540 --> 00:54:33.460]   that is going to work isn't it so yes all right so you can see why we've got these two versions
[00:54:33.460 --> 00:54:41.300]   here if we do X none we've got something of shape 1 comma 2 now we can subtract that from something
[00:54:41.300 --> 00:54:49.140]   of shape 1500 comma 2 because the twos match up because they're the same and the 1500 and the 1
[00:54:49.140 --> 00:54:54.980]   matches up because you remember our numpy rules everything matches up to a unit axis so it's going
[00:54:54.980 --> 00:55:06.980]   to copy this matrix across every row of this matrix and it works but do you remember there's
[00:55:06.980 --> 00:55:16.860]   a special trick which is if you've got two shapes of different lengths we can use the shorter length
[00:55:16.860 --> 00:55:22.780]   and it's going to add unit axes to the front to make it as long as necessary so we actually don't
[00:55:22.780 --> 00:55:31.500]   need the X none we can just use little X and it works because it's going to say is this compatible
[00:55:31.500 --> 00:55:38.460]   with this well the last axis remember we go right to left the last axis matches the second last axis
[00:55:38.460 --> 00:55:43.980]   oh it doesn't exist so we pretend that there's a unit axis and so it's going to do exactly the
[00:55:43.980 --> 00:55:57.500]   same thing as this so if you have not studied the broadcasting from last week carefully that might
[00:55:57.500 --> 00:56:01.820]   not have made a lot of sense to you and so definitely at this point you might want to pause
[00:56:01.820 --> 00:56:07.100]   the video and go back and reread the numpy broadcasting rules and last time and practice
[00:56:07.100 --> 00:56:11.980]   them because that's what we just did we use numpy broadcasting rules and we're going to be doing
[00:56:11.980 --> 00:56:16.460]   this dozens more times throughout the rest of the course and many more times in fact in this lesson
[00:56:16.460 --> 00:56:26.220]   okay so now i think it's a pretty good place to have a pause so i'll see you back here in nine minutes
[00:56:26.220 --> 00:56:34.540]   hi everybody welcome back so we had got to the point where we had managed to get the distance
[00:56:35.180 --> 00:56:42.540]   between our first point x and all of the other points in the data and so we're just looking at
[00:56:42.540 --> 00:56:48.860]   the first eight of them here so the very first distance is of course zero on the x axis and zero
[00:56:48.860 --> 00:56:57.340]   on the y axis because it is the first point the other thing is that because we the way we created
[00:56:57.340 --> 00:57:02.060]   the clusters is they're all kind of next to each other in the list so these are all in the first
[00:57:02.060 --> 00:57:07.740]   clusters so none of them are too far away from each other so now that we've got all the distances
[00:57:07.740 --> 00:57:14.460]   it's easy enough to well not that the distances on x and y it's easy enough to get the distance
[00:57:14.460 --> 00:57:19.340]   the kind of Euclidean distance so we can just square that
[00:57:19.340 --> 00:57:31.260]   that difference and sum and square root and actually maybe this is a good time to talk about
[00:57:31.260 --> 00:57:33.580]   norms and to talk about what we just did there
[00:57:33.580 --> 00:57:44.140]   we've got all these data points
[00:57:44.140 --> 00:57:54.540]   um so here's one of our data points and here's the other one of our data points
[00:57:56.140 --> 00:58:07.340]   and there's some um you know distance across the x axis and there's some distance along the y axis
[00:58:07.340 --> 00:58:10.860]   so we could call that
[00:58:10.860 --> 00:58:23.580]   change in x and change in y and one way to think about this distance then is it's this distance here
[00:58:23.580 --> 00:58:35.900]   um so to calculate that we can use Pythagoras so a squared plus b squared equals c squared
[00:58:35.900 --> 00:58:47.020]   or in our case so this would be c a and b say so in our case it would be the square root
[00:58:48.940 --> 00:58:53.740]   of the change in x squared plus the change in y squared
[00:58:53.740 --> 00:58:59.980]   and rather than saying square root we could say
[00:58:59.980 --> 00:59:11.500]   to the power of a half another way of saying the same thing but there's a different way we could
[00:59:11.500 --> 00:59:24.940]   find the distance we could first go along here and then go up here and so that one would be
[00:59:24.940 --> 00:59:33.740]   change in x if you like to the one plus change in y to the one to the power of
[00:59:34.700 --> 00:59:40.300]   one oneth i'm writing it a slightly odd way for reasons you'll see in a moment it's just this otherwise
[00:59:40.300 --> 00:59:53.180]   um in general if we've got a whole list of numbers we can add them up
[00:59:53.180 --> 01:00:02.620]   let's say there are some list v we can add them up we can do each one to the power of
[01:00:04.060 --> 01:00:11.980]   some number alpha and take that sum to the one over alpha
[01:00:11.980 --> 01:00:23.020]   and this thing here is called a norm so you might remember we came across that last week
[01:00:23.020 --> 01:00:27.260]   and we come across it again this week they basically come up i don't know they might end
[01:00:27.260 --> 01:00:32.140]   up coming up every week they come up all the time particularly because the two norm
[01:00:33.260 --> 01:00:40.700]   which we could write like this or we could write like this or we could write like this
[01:00:40.700 --> 01:00:43.020]   they're all the two norm
[01:00:43.020 --> 01:00:59.740]   this is just saying it's this equation for alpha equals two
[01:01:01.420 --> 01:01:04.460]   and stefano's pointing out we should actually have an absolute value i'm not going to worry
[01:01:04.460 --> 01:01:09.580]   about that we're just doing real numbers here so we'll keep things simple oh well i guess for a
[01:01:09.580 --> 01:01:14.620]   higher than one no you're probably right for something like three yeah i guess we do need
[01:01:14.620 --> 01:01:21.020]   an absolute value there that's a good point because okay we could have this one
[01:01:21.020 --> 01:01:26.460]   and so the distance actually has to be the absolute value
[01:01:27.980 --> 01:01:34.860]   so the change in x is the absolute value of that distance uh yes thank you stefano okay
[01:01:34.860 --> 01:01:40.380]   so we'll have the absolute value okay so the two norm is what happens when alpha equals two
[01:01:40.380 --> 01:01:46.540]   and we would call this in this case we would call this the euclidean distance
[01:01:52.140 --> 01:01:59.660]   but actually where it comes up more often is when you're doing like a loss function so the
[01:01:59.660 --> 01:02:07.740]   mean squared error is just uh well the root mean squared error i should say is just the two norm
[01:02:10.620 --> 01:02:23.020]   whereas the mean absolute error is the one norm and these are also known as l2 and l1
[01:02:23.020 --> 01:02:29.980]   plus and remember what we saw in that paper last week we saw it in this form there's a
[01:02:29.980 --> 01:02:34.620]   two up here which is where they got rid of the square root again so that would have just been
[01:02:38.380 --> 01:02:43.580]   change in x squared plus change in y squared and now we don't even need the parentheses
[01:02:43.580 --> 01:02:55.660]   oopsie dozy okay okay so all of this is to say that for
[01:02:55.660 --> 01:03:04.700]   you know this comes up all the time because we're very very often interested in distances
[01:03:04.700 --> 01:03:10.540]   and errors and things like that um i'm trying to think i don't feel like i've ever seen
[01:03:10.540 --> 01:03:19.020]   anything other than one or two so although it is a general concept i don't think we're going to see
[01:03:19.020 --> 01:03:24.220]   probably things other than one or two in this course i'd be excited if we do that would be
[01:03:24.220 --> 01:03:34.780]   kind of cool so here we're taking the euclidean distance which is the two norm
[01:03:34.780 --> 01:03:47.180]   so this has got eight things in it because we've summed it over dimension one so here's your
[01:03:48.220 --> 01:03:54.940]   first homework is to rewrite using torch.insum
[01:03:54.940 --> 01:04:00.940]   you won't be able to get rid of the x minus x you'll still need to have that in there
[01:04:00.940 --> 01:04:07.020]   but when you've got a multiply followed by a sum now you won't be able to get rid of the square
[01:04:07.020 --> 01:04:11.340]   root either you should be able to get rid of the multiply and the sum by doing it in a single
[01:04:11.340 --> 01:04:16.940]   torch.insum so we're summing up over the first dimension which is this dimension so in other
[01:04:16.940 --> 01:04:30.380]   words we're summing up the x and the y axes okay so now we can get the uh the weights by passing
[01:04:30.380 --> 01:04:37.980]   those distances into our gaussian and so as we would expect the biggest weights it gets up to
[01:04:37.980 --> 01:04:45.820]   point one six so the closest one is itself it's going to be a big weight these other ones get
[01:04:45.820 --> 01:04:48.860]   reasonable weights and the ones that are in totally different clusters have weights small
[01:04:48.860 --> 01:04:54.860]   enough that at three significant figures they appear to be zero okay so we've got our weights
[01:04:54.860 --> 01:05:05.180]   so there are the weights are a 1500 long vector and of course the original data is 1500 by two
[01:05:05.180 --> 01:05:13.500]   the x and the y for each one so we now want a weighted average we want this data we want its
[01:05:13.500 --> 01:05:29.660]   average weighted by this so normally an average is the um is the sum of your data
[01:05:29.660 --> 01:05:38.700]   divided by account that's a normal average a weighted average each item in your data
[01:05:40.460 --> 01:05:45.260]   let's put some eyes around here just to be more clear each item in your data is going to have a
[01:05:45.260 --> 01:05:52.940]   different weight and so you multiply each one by the weights and so rather than dividing by
[01:05:52.940 --> 01:06:00.700]   n which is just the sum of ones we would divide by the sum of weights so this is an important
[01:06:00.700 --> 01:06:04.140]   concept to be familiar with weighted averages
[01:06:10.060 --> 01:06:16.140]   so we need to multiply every one of these x's by this
[01:06:16.140 --> 01:06:26.700]   okay so can we say weight times x no all right why didn't that work
[01:06:26.700 --> 01:06:35.580]   so remember we go right to left so first of all it's going to say let's look at the two
[01:06:36.540 --> 01:06:41.580]   and multiply that by the 15 are they compatible things are compatible if they're equal
[01:06:41.580 --> 01:06:48.140]   or if at least one of them is one these are not equal and they're not one so they're not
[01:06:48.140 --> 01:06:59.420]   compatible that's why it says the size of a tensor a must match now when it says match it doesn't
[01:06:59.420 --> 01:07:03.500]   mean they have to be the same one of them can be one okay that's what it means to match they're
[01:07:03.500 --> 01:07:13.740]   either equal or one of them is one so that doesn't work on the other hand what if this was 1500 comma
[01:07:13.740 --> 01:07:23.980]   one if it was 1500 comma one then they would match because the one and the two match because one of
[01:07:23.980 --> 01:07:30.620]   them is a unit axis and the 1500 and the 1500 match because they're the same so that's what we're
[01:07:30.620 --> 01:07:37.260]   going to do because that would then copy this to every one of these which is what we want we
[01:07:37.260 --> 01:07:47.500]   want weights for each of these x y tuples so to add the trailing unit axis we say every row
[01:07:47.500 --> 01:07:55.500]   and a trailing unit axis so that's what that shape looks like so we can now multiply that by x
[01:07:56.940 --> 01:08:03.020]   and as you can see it's now weighting each of them and so each of these x's and y's down the bottom
[01:08:03.020 --> 01:08:09.820]   they're all zero so we can sum that up and then divide by the sum of weights
[01:08:09.820 --> 01:08:18.540]   so let's now write a function that puts all this together so you can see this really important way
[01:08:18.540 --> 01:08:25.260]   of like to me the the only way that makes sense to do particularly scientific numerical programming
[01:08:25.260 --> 01:08:29.340]   i actually do all my programming this way but particularly scientific and numerical programming
[01:08:29.340 --> 01:08:34.460]   is write it all out step by step check every piece have it all there documented for you
[01:08:34.460 --> 01:08:41.980]   and for others and then copy the cells merge them together and indent them to indent its
[01:08:41.980 --> 01:08:47.980]   control right spread bracket and put a function header on top so here's all those things we just
[01:08:47.980 --> 01:08:53.260]   did and now rather than just grabbing the first x we enumerate through all of them
[01:08:54.380 --> 01:08:59.900]   so that's the distance we had before that's the weight we had before there's the product we had
[01:08:59.900 --> 01:09:05.500]   before and then finally sum across the rows divide by the sum of the weights so that's going to
[01:09:05.500 --> 01:09:14.780]   calculate for the ith it's going to move so it's actually changing capital x so it's changing the
[01:09:14.780 --> 01:09:19.980]   ith thing and capital x so that it's now the weighted sum oh actually sorry the weighted
[01:09:19.980 --> 01:09:28.060]   average of all of the other data weighted by how far it is away so that's going to do a single step
[01:09:28.060 --> 01:09:33.660]   so the mean shift update is extremely straightforward which is clone the data
[01:09:33.660 --> 01:09:37.580]   iterate a few times and do the update
[01:09:40.780 --> 01:09:50.860]   so if we run it take 600 milliseconds and what i've done is i've plotted the centroids moved
[01:09:50.860 --> 01:09:57.180]   by two pixels or two well not two pixels two units so that you can see them and so you can see the
[01:09:57.180 --> 01:10:01.980]   dots is where our data is and they're dots now because every single data point is on top of
[01:10:01.980 --> 01:10:07.580]   each other on a cluster and so you can see they are now in the correct spots so it has successfully
[01:10:07.580 --> 01:10:13.340]   clustered our data so that's great news and so we could test out our hypothesis could we use
[01:10:13.340 --> 01:10:18.060]   triangular just as well as we could have used gaussian so control slash comments and uncomments
[01:10:18.060 --> 01:10:23.500]   yeah we got exactly the same results so that's good
[01:10:23.500 --> 01:10:31.180]   it's really important to know these keyboard shortcuts hit h to get a list of them
[01:10:32.940 --> 01:10:37.740]   some things that are really important don't have keyboard shortcuts so if you click help edit
[01:10:37.740 --> 01:10:42.940]   keyboard shortcuts there's a list of all the things jupiter can do and you can add keyboard
[01:10:42.940 --> 01:10:49.100]   shortcuts to things that don't have them so for example i always add keyboard shortcuts to run
[01:10:49.100 --> 01:10:54.620]   all cells above and run all cells below as you can see i type q and then a for above and q and
[01:10:54.620 --> 01:11:05.900]   then b for below all right now that was kind of boring in a way because it did five steps
[01:11:05.900 --> 01:11:15.580]   um but we just saw the result what did it look like one step at a time um this isn't just fun
[01:11:15.580 --> 01:11:19.980]   it's really important to be able to see things happening one step at a time
[01:11:19.980 --> 01:11:24.460]   because there are so many algorithms we do which are like updating weights or updating data you
[01:11:24.460 --> 01:11:29.100]   know so if a stable diffusion for example you're very likely to want to show you know you're
[01:11:29.100 --> 01:11:35.180]   incrementally denoising and so forth so in my opinion it's important to know how to do animations
[01:11:35.180 --> 01:11:42.540]   and i found the documentation for this unnecessarily complicated because it's a
[01:11:42.540 --> 01:11:46.940]   lot of it's about how to make them performant but most of the time we probably don't care too much
[01:11:46.940 --> 01:11:51.580]   about that so i want to show you a little trick a simple way to create animations without any
[01:11:51.580 --> 01:11:57.660]   trouble so matplotlib.animation has something called func animation that's what we're going to use
[01:11:57.660 --> 01:12:05.420]   to create an animation you have to create a function and the function you're going to be
[01:12:05.420 --> 01:12:11.180]   calling func animation passing in the name of that function and saying how many times to run it
[01:12:11.180 --> 01:12:16.540]   and that's what this frames argument this says run this function this many times
[01:12:18.700 --> 01:12:23.020]   and then create an animation that that basically contains the result of that
[01:12:23.020 --> 01:12:25.820]   with a 500 millisecond interval between each one
[01:12:25.820 --> 01:12:33.100]   so what's this do one going to do to create one frame of animation
[01:12:33.100 --> 01:12:42.940]   we will call our one update here it is one update right we're going to call this
[01:12:43.660 --> 01:12:50.540]   that's going to update our axes and then we're going to have an axis which we've created here
[01:12:50.540 --> 01:12:58.220]   so we're going to clear whatever was on the plot before and plot our new data on that axis
[01:12:58.220 --> 01:13:06.700]   and then the only other thing you need to do is that the very first time it calls it we want to
[01:13:06.700 --> 01:13:14.300]   plot it before running and d is going to be passed automatically the frame number so for the zero
[01:13:14.300 --> 01:13:19.420]   frame we're going to not do the update we're just going to plot the data as it is already
[01:13:19.420 --> 01:13:27.660]   i guess another way we could have done that would have been just to say if d then
[01:13:31.980 --> 01:13:38.940]   do the update i suppose that should work too maybe it's even simpler let's see if i just broke it
[01:13:38.940 --> 01:13:44.380]   okay so we're going to clone our data we're going to create our figure in our subplots we're going
[01:13:44.380 --> 01:13:51.340]   to call func animation calling do one five times and then we're going to display the animation
[01:13:51.340 --> 01:13:59.580]   and so let's see so html takes some html and displays it and two js html creates some html
[01:13:59.580 --> 01:14:06.300]   so that's why it's created this html includes javascript and so we'll click run one two three
[01:14:06.300 --> 01:14:10.700]   four five there's the five steps so if i click loop you'll see them running again and again
[01:14:10.700 --> 01:14:19.660]   fantastic so that's how easy it is to create a matplotlib animation
[01:14:19.660 --> 01:14:26.380]   so hopefully now you can use that to play around with some fun stable diffusion animations as well
[01:14:28.700 --> 01:14:33.820]   you don't just have to use touch a shtml you can also create
[01:14:33.820 --> 01:14:41.420]   you can also create movies for example
[01:14:41.420 --> 01:14:52.540]   so you can call two html five video would be another option
[01:14:52.540 --> 01:14:58.220]   and you can save an animation as a movie file so there's all these different options for that
[01:14:59.180 --> 01:15:01.980]   but hopefully that's enough to get you started so
[01:15:01.980 --> 01:15:11.500]   for your homework i would like you when you create your k-means or whatever to try to
[01:15:11.500 --> 01:15:18.140]   create your own animation or create an animation of some stable diffusion thing that you're playing
[01:15:18.140 --> 01:15:25.660]   with so don't forget this important ax.clear you without the ax.clear it prints it on top of the
[01:15:25.660 --> 01:15:31.260]   last one which sometimes is what you want to be fair but in this case it's not what i wanted
[01:15:31.260 --> 01:15:40.940]   all right so kind of slow half a second for not that much data i'm sure would be nice it was
[01:15:40.940 --> 01:15:50.620]   faster well the good news is we can gpu accelerate it the bad news is it's not going to gpu accelerate
[01:15:50.620 --> 01:16:01.180]   that well because of this loop this is looping 1500 times if we so looping is not going to run
[01:16:01.180 --> 01:16:06.940]   on the gpu so the best we could do with this would be to move all this to the gpu now the problem is
[01:16:06.940 --> 01:16:14.860]   that calling something on the gpu 1500 times from python is a really bad idea because there's this
[01:16:14.860 --> 01:16:21.660]   kind of huge communication overhead of this kind of flow of control and data switching back between
[01:16:21.660 --> 01:16:29.820]   the cpu and the gpu it's the kernel launching overhead it's bad news so you don't want to have
[01:16:29.820 --> 01:16:39.180]   a really big fast python loop that inside it calls cuda code it calls gpu code so we need
[01:16:39.180 --> 01:16:49.260]   to make all of this run without the loop which we could do with broadcasting so let's roll up our
[01:16:49.260 --> 01:16:55.180]   sleeves and try to get the broadcast version of this working so generally speaking the way we tend
[01:16:55.180 --> 01:17:03.820]   to do things with broadcasting on a gpu is we create batches or mini batches so to create batches
[01:17:03.820 --> 01:17:09.740]   or mini batches we normally just call them batches nowadays we create a batch size so let's say we're
[01:17:09.740 --> 01:17:17.420]   going to do a batch size of five so we're going to do five at a time all right so how do we do
[01:17:17.420 --> 01:17:22.060]   five at a time this is only doing one at a time how do we do five at a time
[01:17:22.060 --> 01:17:29.180]   as before let's clone our data and this time little x for our testing so we're going to do
[01:17:29.180 --> 01:17:35.580]   everything ahead of time little tests as we always do this is not now x zero anymore but it's x
[01:17:35.580 --> 01:17:43.180]   colon bs so it's the first five this is now the first five items okay so little x is now
[01:17:43.180 --> 01:17:46.860]   a five by two matrix this is our mini batch the first five items
[01:17:46.860 --> 01:17:53.980]   as before our data itself is 1500 by two all right so we need a distance calculation
[01:17:54.780 --> 01:17:59.340]   but previously our distance calculation um
[01:17:59.340 --> 01:18:06.460]   previously our distance calculation only worked if little x was a single
[01:18:06.460 --> 01:18:14.940]   number and it returned just the distances from that to everything in big x but we need something
[01:18:14.940 --> 01:18:21.500]   that's actually going to be um return a matrix right we've got um
[01:18:22.380 --> 01:18:24.380]   um
[01:18:24.380 --> 01:18:27.660]   let's see we've got five
[01:18:27.660 --> 01:18:44.940]   by two in little x and then in big x we've got something much bigger not to scale obviously
[01:18:44.940 --> 01:18:48.300]   we've got 1500 by two
[01:18:51.180 --> 01:18:54.700]   and what is the distance between
[01:18:54.700 --> 01:19:05.740]   these two things well if you think about it there's going to be a distance between
[01:19:05.740 --> 01:19:09.500]   item one and item one
[01:19:09.500 --> 01:19:13.980]   but there's also going to be a distance between
[01:19:16.060 --> 01:19:22.380]   item one and item two and there's going to be a distance between let's use a different
[01:19:22.380 --> 01:19:27.180]   color for the next one item two and item one
[01:19:27.180 --> 01:19:35.980]   right so the output of this is actually going to be
[01:19:35.980 --> 01:19:42.060]   a matrix the distances are actually going to give us a matrix
[01:19:43.100 --> 01:19:46.220]   where i mean it doesn't matter which way around we do it we can decide
[01:19:46.220 --> 01:19:53.180]   if we do it this way around for each of the five things in the mini batch there will be 1500
[01:19:53.180 --> 01:20:01.420]   distances the distance between every one so we're going to need to do broadcasting to do this
[01:20:01.420 --> 01:20:13.260]   calculation so this is the function that we're going to create and it's going to create this
[01:20:13.260 --> 01:20:24.060]   as you can see five by 1500 output but let's see uh how we get it so can we do x minus x
[01:20:25.660 --> 01:20:35.740]   no we can't why is that that's because big x is 1500 by two and little x is five by two
[01:20:35.740 --> 01:20:43.660]   so it's going to look at remember our rules right to left are these compatible
[01:20:43.660 --> 01:20:50.540]   yes they are they're the same are these compatible
[01:20:53.980 --> 01:21:00.060]   no they're not okay because they're different so that's not possible to do
[01:21:00.060 --> 01:21:05.580]   what if though we wanted to
[01:21:05.580 --> 01:21:20.540]   what if we insert in big x an axis at the start here and in little x we add an axis in the middle
[01:21:20.540 --> 01:21:24.940]   here then now these are compatible
[01:21:24.940 --> 01:21:33.500]   because you've got they're the same because i should use arrows really
[01:21:33.500 --> 01:21:38.220]   these are compatible because one of them is a one
[01:21:38.220 --> 01:21:43.420]   and these are compatible because one of them is the one as well
[01:21:43.420 --> 01:21:48.620]   so they are all compatible and what it's going to do is it's going to
[01:21:48.620 --> 01:21:58.220]   do this attraction between these directly and it's going to copy this across all 1500 rows
[01:21:58.220 --> 01:22:05.580]   it'll copy it this is going to be copied and then this sorry across five rows and then this will be
[01:22:05.580 --> 01:22:11.340]   copied across these 1500 rows because that's what broadcasting does i mean it's not really copying
[01:22:11.340 --> 01:22:20.460]   but it's effectively copying and so that gives us we can now subtract them and that gives us what
[01:22:20.460 --> 01:22:27.420]   we wanted which is five by 1500 and there's also by two because there's both the x and the y
[01:22:27.420 --> 01:22:34.540]   so that's why this works that's what this is doing here it's taking this attraction it's
[01:22:34.540 --> 01:22:42.140]   squaring them and then summing over that last shortest axis summing over the x and the y
[01:22:42.140 --> 01:22:46.780]   squares and then take square root i don't know why i said torch dot square root we could just put dot
[01:22:46.780 --> 01:22:53.180]   square root at the end but same same in fact it's worth mentioning that so most things that you can
[01:22:53.180 --> 01:23:00.300]   do on tensors you can either write torch dot as a function or you can write it as a method
[01:23:02.220 --> 01:23:08.540]   generally speaking both should be fine not everything but most things work in both spots
[01:23:08.540 --> 01:23:18.220]   okay so now we've got this matrix which is five by 1500 and the nice thing is that our Gaussian
[01:23:18.220 --> 01:23:23.980]   kernel doesn't actually have to be changed to get the weights believe it or not and the reason for
[01:23:23.980 --> 01:23:29.020]   that is now how do we get the source code i could move back up there or i can just type Gaussian
[01:23:29.020 --> 01:23:36.620]   question mark question mark and see it and the nice thing is that this is just this is a scalar
[01:23:36.620 --> 01:23:45.100]   so it broadcasts over anything and then this is also just a scalar so this is all going to work
[01:23:45.100 --> 01:23:58.380]   fine without any fiddling around okay so now we've got a five by 1500 weight so that's the weight for
[01:23:58.380 --> 01:24:02.460]   each of the five things our mini battery each of the 1500 things each of them is compared to
[01:24:02.460 --> 01:24:09.900]   and then we've got the shape of the data itself x dot shape which is the 1500 points
[01:24:09.900 --> 01:24:17.420]   so now we want to apply each one of these weights to each of these columns
[01:24:17.420 --> 01:24:27.420]   so we need to add a unit axis to the end so to add a unit axis to the end we could say colon comma
[01:24:27.420 --> 01:24:34.380]   colon comma none but dot dot dot means all of the axes up until however many you need so in this
[01:24:34.380 --> 01:24:41.420]   case the last one common none so this is going to add an axis to the end so this is going to turn
[01:24:41.420 --> 01:24:47.580]   this is going to turn weight dot shape from five comma 1500 to five comma 1500 comma one
[01:24:47.580 --> 01:24:54.300]   and this is going to add an axis to the start remember it's the same as x none comma colon
[01:24:54.300 --> 01:25:04.300]   comma colon and so let's check our rules left right to left these are compatible because one
[01:25:04.300 --> 01:25:11.420]   of them is one these are compatible because they're both the same and these are compatible
[01:25:11.420 --> 01:25:22.540]   because one of them is one okay so it's going to be um copying each weight across to each of
[01:25:22.540 --> 01:25:27.820]   the x and y which is what we want we want to um we want to weight both of those components
[01:25:27.820 --> 01:25:37.820]   and it's going to copy each of the 1500 points sorry each of the point five times because we
[01:25:37.820 --> 01:25:42.220]   do in fact want to weight every one of the five things in our mini batch a separate set of weights
[01:25:42.220 --> 01:25:48.700]   for each of them so that sounds perfect so that's how i think through these calculations
[01:25:49.260 --> 01:25:55.820]   okay so we can now
[01:25:55.820 --> 01:26:04.540]   do that multiplication which is going to give us something of five by 1500 by two because we end
[01:26:04.540 --> 01:26:12.700]   up with the maximum of our ranks and then we sum up over those 1500 points and that's going to give
[01:26:12.700 --> 01:26:29.100]   us now five new data points um now something that you might notice here is that we've got a product
[01:26:29.100 --> 01:26:38.060]   and a sum and when you see a product and a sum that tells you that maybe we should use
[01:26:39.660 --> 01:26:53.420]   a sum so in this case we've got our weight we've got five by 1500 so let's call those i and j
[01:26:53.420 --> 01:27:00.860]   those for the five and 1500 we've got the x is 1500 by two now we want to take the product of
[01:27:00.860 --> 01:27:10.940]   that and that so we need to use the same name for this rows so we use j again okay and then k is the
[01:27:10.940 --> 01:27:17.900]   number of rows that's the two and then we want to end up with i by k so torch.insum
[01:27:17.900 --> 01:27:25.820]   gives exactly the same result that's great but you might recognize this that's exactly
[01:27:25.820 --> 01:27:31.500]   the same ironsum we had just before when we were doing matrix multiplication oh that is a matrix
[01:27:31.500 --> 01:27:41.180]   multiplication we've just reinvented matrix multiplication using uh this rather nifty trick
[01:27:41.180 --> 01:27:49.260]   so we could also just use that and so you know again this is like um what i was just playing
[01:27:49.260 --> 01:27:53.900]   around with this morning as i started to look at this and i was thinking like oh you know can we
[01:27:53.900 --> 01:27:59.820]   simplify this i don't like this kind of like messing around with axes and summing over dimensions and
[01:27:59.820 --> 01:28:05.180]   whatnot and so it's nice to get things down to i and sum or better still getting down to matrix
[01:28:05.180 --> 01:28:09.660]   model applies it's just clearer you know it's stuff that we all recognize because we use them
[01:28:09.660 --> 01:28:19.980]   all the time they all work performance would be pretty similar i suspect um okay so now that we've
[01:28:19.980 --> 01:28:30.860]   got that we then need to do our sum and we've got our five points this is our five uh denominators
[01:28:30.860 --> 01:28:36.540]   so we've got our numerator that we calculated up here for our weighted uh for our weighted average
[01:28:36.540 --> 01:28:41.820]   the denominator is just the sum of the weights remember and so numerator divided by denominator
[01:28:41.820 --> 01:28:47.420]   is our answer so again we've gone through every step we've checked out all the dimensions all
[01:28:47.420 --> 01:28:53.180]   along the way so nothing's going to surprise us don't try and write a function like this just bang
[01:28:53.180 --> 01:28:59.500]   from scratch right you've got to drive yourself crazy instead do it step by step so here's our
[01:28:59.500 --> 01:29:08.460]   mean shift algorithm clone the data go through five iterations and now go from naught to n
[01:29:08.460 --> 01:29:17.180]   batch size at a time so python has something called slices so we can create a slice
[01:29:18.140 --> 01:29:27.100]   of x starting at one up to i plus batch size right unless you've gone past n which goes use n
[01:29:27.100 --> 01:29:32.860]   and so then we're just copying and pasting each of the lines of code that we had before actually
[01:29:32.860 --> 01:29:36.140]   i just copy the cells and merge them of course i don't actually copy and paste because it's
[01:29:36.140 --> 01:29:45.740]   so boring and there's my final step to create the new xs and so notice here s is not a single thing
[01:29:45.740 --> 01:29:50.780]   it's a slice of things you might not have seen slice before but this is just internally what
[01:29:50.780 --> 01:29:55.340]   python's doing when you use colon and it's very convenient when you need to use the same slice
[01:29:55.340 --> 01:29:59.340]   multiple times okay
[01:29:59.340 --> 01:30:09.900]   so let's do that using CUDA i would run it first without CUDA but i mean i've done all the steps
[01:30:09.900 --> 01:30:17.660]   before so it should be fine um so pop it on the GPU and run beam shift and let's see how long that
[01:30:17.660 --> 01:30:25.740]   takes it takes one millisecond and previously without GPU it took 400 milliseconds and you know
[01:30:25.740 --> 01:30:30.460]   the other thing we should probably think about doing is looking at other batch sizes as well
[01:30:30.460 --> 01:30:38.460]   because now we're looping over batches right so if we make the batch size bigger that for loop is
[01:30:38.460 --> 01:30:45.900]   going to do less looping so what if we make that 16 will that be any faster i actually never tried
[01:30:45.900 --> 01:30:52.540]   this before that's interesting it's actually slower
[01:30:52.540 --> 01:30:58.460]   huh there you go fascinating what if it was eight
[01:31:03.020 --> 01:31:09.020]   amazing so the big batches don't quite seem to be working so well for some reason so i wonder if i've
[01:31:09.020 --> 01:31:13.340]   hang on what's going on why is it
[01:31:13.340 --> 01:31:22.140]   why is it changing how it should be uh my batch size was five why is it slower suddenly
[01:31:22.140 --> 01:31:32.380]   i think it's just a bit varying is probably the answer so it just varies a lot okay so it doesn't
[01:31:32.380 --> 01:31:39.500]   seem like changing the batch size is changing much here um so that's fine so we'll just leave it
[01:31:39.500 --> 01:31:43.740]   where it was and then check looking at the data oh that looks lovely
[01:31:43.740 --> 01:31:55.180]   oh i see thank you people on youtube pointing out that i'm passing batch size so i actually need
[01:31:55.180 --> 01:32:01.420]   to put it here all right so if we used a batch size of five no wonder it was messing up oh look
[01:32:01.420 --> 01:32:13.660]   at that i've totally made it slow now 157 milliseconds ha ha okay 64 13 milliseconds
[01:32:13.660 --> 01:32:16.700]   all right finally that makes much more sense 256
[01:32:16.700 --> 01:32:23.740]   1024 okay so the bigger bigger is better
[01:32:23.740 --> 01:32:30.300]   and i guess we could actually do all 5000 at once probably
[01:32:30.460 --> 01:32:32.780]   okay nice
[01:32:32.780 --> 01:32:42.540]   all right thank you youtube friends for solving that bizarre mystery
[01:32:42.540 --> 01:32:49.980]   okay all right so
[01:32:49.980 --> 01:32:59.740]   that's pretty great i mean you know to see um that we can gpu optimizer mean shift
[01:32:59.740 --> 01:33:03.020]   like actually googled for this to see if it's been done before
[01:33:03.020 --> 01:33:10.620]   um and it's the kind of thing that people like write papers about
[01:33:10.620 --> 01:33:20.220]   um so i think it's great that we can do it so easily with pytorch and it's the kind of
[01:33:20.220 --> 01:33:27.260]   thing that previously had been considered you know a very challenging academic problem to solve
[01:33:28.540 --> 01:33:33.100]   so maybe you can do something similar with some of these now i haven't told you what
[01:33:33.100 --> 01:33:36.380]   these are so part of the homework is to go read about them and learn about them
[01:33:36.380 --> 01:33:43.580]   db scan funnily enough actually is an algorithm that i accidentally invented and then discovered a
[01:33:43.580 --> 01:33:49.340]   year later had already been invented that was a long time ago i was playing around with j which
[01:33:49.340 --> 01:33:57.500]   is the successor to apl on a very old windows phone um and i had a long plane flight and i
[01:33:57.500 --> 01:34:02.780]   came up with an algorithm and implemented the whole thing on my phone using j um and then discovered a
[01:34:02.780 --> 01:34:07.580]   year later that i just invented db scan this is actually a really cool algorithm and it's got a
[01:34:07.580 --> 01:34:17.580]   lot of similarities to mean shift um lsh comes up all the time um so that's great um and in fact
[01:34:17.580 --> 01:34:23.340]   i have a strong feeling and i've been thinking about this for a while that something like lsh
[01:34:23.340 --> 01:34:28.300]   could be used to speed this whole thing up a lot um because if you think about it and again maybe
[01:34:28.300 --> 01:34:33.420]   they're already this already exists i don't know but if you think about it when we did that distance
[01:34:33.420 --> 01:34:42.540]   calculation the the vast majority of the the weights are nearly zero and so it seems pointless
[01:34:42.540 --> 01:34:51.900]   to create that big you know kind of eventually 1500 by 1500 matrix um that's slow it would be
[01:34:51.900 --> 01:34:57.260]   much better if we just found the ones that were like pretty close by and just took their average
[01:34:57.260 --> 01:35:05.260]   and so you want an optimized nearest neighbors basically and so um this is an example of
[01:35:05.260 --> 01:35:09.740]   something that can give you a uh an up a kind of a fast nearest neighbors algorithm
[01:35:09.740 --> 01:35:16.860]   um or you know there are things like kd trees and opt trees and stuff like that so
[01:35:17.420 --> 01:35:22.060]   if you want to like have a bonus bonus
[01:35:22.060 --> 01:35:26.780]   invent a new mean shift algorithm
[01:35:26.780 --> 01:35:39.580]   which picks only the closest points to avoid quadratic
[01:35:42.860 --> 01:35:51.020]   type all right so not very often you get an assignment which is to invent a new
[01:35:51.020 --> 01:35:57.660]   mean shift algorithm i guess a super super bonus super super bonus
[01:35:57.660 --> 01:36:04.780]   publish a paper that describes it
[01:36:04.780 --> 01:36:09.740]   all right you definitely get four points if you do that
[01:36:10.780 --> 01:36:14.220]   we'll give you a number of points equal to the impact factor of the journal you get it published in
[01:36:14.220 --> 01:36:18.060]   okay um
[01:36:18.060 --> 01:36:35.340]   so what i want to do now is move on to calculus which for some of us may not be our favorite topic
[01:36:40.620 --> 01:36:44.300]   that's funny stefano wrote the ironsome version here already i didn't notice
[01:36:44.300 --> 01:36:47.980]   okay always ahead of his time that guy um
[01:36:47.980 --> 01:36:57.020]   let's talk about calculus um if you're not um super comfortable with derivatives and what they
[01:36:57.020 --> 01:37:03.420]   are and why we care um three blue one brown has a wonderful series called the essence of calculus
[01:37:03.420 --> 01:37:10.780]   which i strongly recommend watching um it's just a pleasure actually to watch uh as it's everything
[01:37:10.780 --> 01:37:17.900]   that is on three blue one brown a pleasure to watch um the
[01:37:17.900 --> 01:37:25.820]   and so we're not going to get into back prop today instead we're just going to have a quick chat
[01:37:25.820 --> 01:37:40.300]   about about calculus where do we start so the good news is
[01:37:40.300 --> 01:37:45.740]   just like you don't have to know much linear algebra at all you basically just need to know
[01:37:45.740 --> 01:37:55.740]   about matrix multiplication um you also don't need to know um much calculus at all um just derivatives
[01:37:55.740 --> 01:38:04.060]   so let's think about like what derivatives are so i'm going to borrow actually the same
[01:38:04.060 --> 01:38:10.460]   starting point that um three blue one brown uses one of their videos is to consider a car
[01:38:11.740 --> 01:38:14.860]   and we're going to see um how far away from home it is
[01:38:14.860 --> 01:38:19.340]   at various time points
[01:38:19.340 --> 01:38:26.700]   okay so after a minute let's say after a second
[01:38:26.700 --> 01:38:32.780]   it's traveled five meters
[01:38:32.780 --> 01:38:38.460]   and then after two seconds
[01:38:38.620 --> 01:38:43.260]   it's traveled 10 meters
[01:38:43.260 --> 01:38:49.740]   okay and after three seconds you can probably guess it's traveled 15 meters
[01:38:49.740 --> 01:39:00.060]   so there's this concept here of uh um yeah got it the wrong way around
[01:39:03.340 --> 01:39:11.660]   obviously so time distance
[01:39:11.660 --> 01:39:25.740]   okay so um there's this concept of yeah of like location it's like how far how far have you
[01:39:25.740 --> 01:39:30.860]   traveled um at a particular point in time so we can look at one of these points and find out how far
[01:39:32.620 --> 01:39:33.660]   that car is gone
[01:39:33.660 --> 01:39:39.020]   we could also take two points
[01:39:39.020 --> 01:39:45.260]   and we can say where did it start at the start of those two points
[01:39:45.260 --> 01:39:48.940]   and where did it finish at the end of those two points
[01:39:48.940 --> 01:39:54.380]   and we can say between those two points how much time passed
[01:39:56.300 --> 01:40:02.620]   and how far did they travel in two seconds they traveled 10 meters
[01:40:02.620 --> 01:40:14.940]   so we could now also say all right well the slope of something is rise over run
[01:40:14.940 --> 01:40:19.100]   oopsie dozy
[01:40:22.140 --> 01:40:31.900]   10 meters in two seconds and notice we don't just divide the numbers we also divide the units
[01:40:31.900 --> 01:40:36.300]   we get five meters per second
[01:40:36.300 --> 01:40:47.420]   so um this here is now change the dimensions entirely we're now not looking at distance
[01:40:48.380 --> 01:40:59.660]   but we're looking at speed or velocity and it's equal to rise over run it's equal to the rate of
[01:40:59.660 --> 01:41:17.180]   change and what it says really is as time the x-axis goes up by one second what happens to the
[01:41:17.180 --> 01:41:23.260]   distance in meters as one second passes
[01:41:23.260 --> 01:41:33.100]   how does the number of meters change
[01:41:33.100 --> 01:41:39.260]   and so maybe these aren't points at all maybe there's a function
[01:41:42.300 --> 01:41:48.380]   right it's a continuum of points and so you can do that for the function so the function
[01:41:48.380 --> 01:41:53.820]   is a function of time distance is a function of time
[01:41:53.820 --> 01:42:03.660]   and so we could say what's the slope of that function and we can get the slope from point a
[01:42:03.660 --> 01:42:13.900]   to point b using rise over run so from t1 to t2 the amount of distance that's uh the amount of
[01:42:13.900 --> 01:42:27.100]   time that's passed is t2 minus t1 that's how much time has passed let's say this is t1 this is t2
[01:42:29.260 --> 01:42:33.100]   and the distance that they've traveled while they've moved from
[01:42:33.100 --> 01:42:39.100]   wherever they are at the end
[01:42:39.100 --> 01:42:43.100]   to wherever they were at the start
[01:42:43.100 --> 01:42:49.020]   so that's the change in distance divided by the change in time
[01:42:49.020 --> 01:42:54.300]   change in distance divided by change in time
[01:42:58.140 --> 01:43:01.580]   okay let's say that's y
[01:43:01.580 --> 01:43:11.340]   so another way um now the thing is when we talk about calculus we talk about finding a slope
[01:43:11.340 --> 01:43:16.060]   but we talk about finding a slope of something
[01:43:16.060 --> 01:43:24.940]   that's more often more tricky than this right we have slopes of things
[01:43:25.660 --> 01:43:37.660]   that look more like this and we say what's this slope oops and terrible chapter drawing
[01:43:37.660 --> 01:43:42.860]   let's maybe put it over here because i'm left-handed what's this slope
[01:43:42.860 --> 01:43:53.580]   now what does it mean to have like the idea of a velocity
[01:43:53.580 --> 01:44:02.540]   at an exact moment in time it doesn't mean anything you know at an exact moment in time
[01:44:02.540 --> 01:44:08.300]   you're just like it's frozen right what's happening exactly now but what you can do
[01:44:08.300 --> 01:44:14.140]   is you can say well what's the change in time between a bit before our point and a bit after
[01:44:14.140 --> 01:44:19.820]   our point and what's the change in distance between a bit before our point and a bit after our point
[01:44:19.820 --> 01:44:23.740]   and so you can do the same kind of rise over run thing right
[01:44:23.740 --> 01:44:35.740]   but you can make that distance between t2 and t1 smaller and smaller and smaller so let's rewrite
[01:44:35.740 --> 01:44:50.140]   this in a slightly different way let's call the denominator the distance between t1 plus a little
[01:44:50.140 --> 01:45:03.900]   bit we'll call it d it's that minus t1 so this is t2 right it's t1 plus a little bit so we say oh
[01:45:03.900 --> 01:45:10.060]   here's t1 let's add a little bit and notice that when we write it this well let's actually let's
[01:45:10.060 --> 01:45:19.580]   do the rest of it so now f of t2 becomes f of t1 plus a little bit and this is the same
[01:45:19.580 --> 01:45:28.620]   and now notice here that t1 plus d minus t1 we can delete all that
[01:45:30.940 --> 01:45:38.940]   because it just comes out to d so this is another way of calculating the slope of our function
[01:45:38.940 --> 01:45:46.540]   and as d gets smaller and smaller and smaller we're kind of getting a triangle that's
[01:45:46.540 --> 01:45:52.540]   tinier and tinier and tinier and it still makes sense it's still that some time has passed
[01:45:52.540 --> 01:45:58.860]   and the car is moved right but it's just smaller and smaller amounts of time
[01:46:00.140 --> 01:46:07.980]   now if you did calculus at college or at school you might have done all this stuff messing around
[01:46:07.980 --> 01:46:17.820]   with limits and epsilon delta and blah blah blah i've got really good news it turns out
[01:46:17.820 --> 01:46:29.180]   you can actually just think of this d as a really small number where d is the difference
[01:46:29.180 --> 01:46:34.220]   difference
[01:46:34.220 --> 01:46:42.700]   and so when we calculate the slope
[01:46:42.700 --> 01:46:56.140]   we can write it in a slightly different way as the change in y divided by the change in x
[01:46:57.180 --> 01:47:01.260]   this here is the change in y and this here is the change in x
[01:47:01.260 --> 01:47:06.780]   and so in other words this here is a very small number
[01:47:06.780 --> 01:47:10.860]   a very small number
[01:47:10.860 --> 01:47:18.780]   and this here is the result in the function of changing by that very small number
[01:47:24.860 --> 01:47:32.460]   and this way of thinking about calculus is known as the calculus of infinitesimals
[01:47:32.460 --> 01:47:44.860]   and it's how Leibniz originally developed it and it's been turned into a whole theory nowadays
[01:47:44.860 --> 01:47:52.060]   and the reason i talk about it here is because when we do calculus you'll see me doing stuff
[01:47:52.060 --> 01:47:59.500]   all the time where i act like dx is a really small number and when i was at school i was
[01:47:59.500 --> 01:48:04.060]   told i wasn't allowed to do that i've since learned that it's totally fine to do that
[01:48:04.060 --> 01:48:08.300]   so for example next lesson we're going to be looking at the chain rule
[01:48:08.300 --> 01:48:12.140]   which looks like this
[01:48:12.140 --> 01:48:18.300]   dy dx equals dy du times du dx
[01:48:21.180 --> 01:48:25.020]   and i'm just going to say oh these two small numbers can cancel out
[01:48:25.020 --> 01:48:31.100]   and that's why obviously they're the same thing and that's all going to work out nicely
[01:48:31.100 --> 01:48:39.980]   so anywho what would be very helpful would be if before the next lesson if you're
[01:48:39.980 --> 01:48:45.020]   not totally up to date with your you know remembering all the stuff you did in high
[01:48:45.020 --> 01:48:52.380]   school about calculus is watch the three blue one brown course we are not going to be looking
[01:48:52.380 --> 01:48:58.060]   i don't think at all at integration so you don't have to worry about that
[01:48:58.060 --> 01:49:08.860]   also we are not going to on the whole be doing any derivatives by hand
[01:49:09.660 --> 01:49:16.060]   so for example there are rules such as
[01:49:16.060 --> 01:49:26.860]   dy dx if y equals x squared is 2x these kind of rules you're not really going to have to learn
[01:49:26.860 --> 01:49:31.100]   because pytorch is going to do them all for you
[01:49:31.100 --> 01:49:36.220]   the one that we care about is going to be the chain rule but we're going to learn about that
[01:49:36.220 --> 01:49:43.100]   next time okay i hope i don't get beaten to a bloody pulp the next time i walk into a
[01:49:43.100 --> 01:49:53.020]   mathematicians conference i suspect i might um but hopefully i get away with with this uh i i think
[01:49:53.020 --> 01:50:05.900]   it's safe we'll see how we go um so um thanks everybody very much for joining me um and
[01:50:05.900 --> 01:50:10.620]   really look forward to seeing you next time where we're going to do back propagation from scratch
[01:50:10.620 --> 01:50:15.900]   we've already learned to multiply matrices so once we've got back propagation as well
[01:50:15.900 --> 01:50:23.420]   we'll be ready to train a neural network all right thanks all bye

