
[00:00:00.000 --> 00:00:03.040]   evolutionarily, if we see a lion running at us,
[00:00:03.040 --> 00:00:06.360]   we didn't have time to calculate the lion's kinetic energy
[00:00:06.360 --> 00:00:08.840]   and is it optimal to go this way or that way?
[00:00:08.840 --> 00:00:10.440]   You just reacted.
[00:00:10.440 --> 00:00:13.240]   And physically, our bodies are well-attuned
[00:00:13.240 --> 00:00:14.520]   to actually make right decisions.
[00:00:14.520 --> 00:00:16.640]   But when you're playing a game like poker,
[00:00:16.640 --> 00:00:19.320]   this is not something that you ever evolved to do,
[00:00:19.320 --> 00:00:22.360]   and yet you're in that same flight or fight response.
[00:00:22.360 --> 00:00:24.440]   And so that's a really important skill
[00:00:24.440 --> 00:00:25.260]   to be able to develop,
[00:00:25.260 --> 00:00:28.280]   to basically learn how to meditate in the moment
[00:00:28.280 --> 00:00:30.680]   and calm yourself so that you can think clearly.
[00:00:30.680 --> 00:00:35.760]   - The following is a conversation with Liv Bury,
[00:00:35.760 --> 00:00:38.400]   formerly one of the best poker players in the world,
[00:00:38.400 --> 00:00:40.320]   trained as an astrophysicist
[00:00:40.320 --> 00:00:44.240]   and is now a philanthropist and an educator
[00:00:44.240 --> 00:00:48.280]   on topics of game theory, physics, complexity, and life.
[00:00:48.280 --> 00:00:51.100]   This is the Lex Friedman Podcast.
[00:00:51.100 --> 00:00:53.200]   To support it, please check out our sponsors
[00:00:53.200 --> 00:00:54.440]   in the description.
[00:00:54.440 --> 00:00:57.660]   And now, dear friends, here's Liv Bury.
[00:00:58.580 --> 00:01:02.620]   What role do you think luck plays in poker and in life?
[00:01:02.620 --> 00:01:04.300]   You can pick whichever one you want,
[00:01:04.300 --> 00:01:06.980]   poker or life and/or life.
[00:01:06.980 --> 00:01:10.700]   - The longer you play, the less influence luck has,
[00:01:10.700 --> 00:01:11.700]   you know, like with all things,
[00:01:11.700 --> 00:01:13.940]   the bigger your sample size,
[00:01:13.940 --> 00:01:16.420]   the more the quality of your decisions
[00:01:16.420 --> 00:01:17.960]   or your strategies matter.
[00:01:17.960 --> 00:01:21.620]   So to answer that question, yeah, in poker,
[00:01:21.620 --> 00:01:22.820]   it really depends.
[00:01:22.820 --> 00:01:26.180]   If you and I sat and played 10 hands right now,
[00:01:26.180 --> 00:01:30.300]   I might only win 52% of the time, 53% maybe.
[00:01:30.300 --> 00:01:31.760]   But if we played 10,000 hands,
[00:01:31.760 --> 00:01:35.180]   then I'll probably win like over 98, 99% of the time.
[00:01:35.180 --> 00:01:38.220]   So it's a question of sample sizes.
[00:01:38.220 --> 00:01:40.160]   - And what are you figuring out over time?
[00:01:40.160 --> 00:01:42.300]   The betting strategy that this individual does
[00:01:42.300 --> 00:01:43.780]   or literally it doesn't matter
[00:01:43.780 --> 00:01:45.940]   against any individual over time?
[00:01:45.940 --> 00:01:48.020]   - Against any individual over time, the better player,
[00:01:48.020 --> 00:01:49.540]   because they're making better decisions.
[00:01:49.540 --> 00:01:51.300]   So what does that mean to make a better decision?
[00:01:51.300 --> 00:01:55.340]   Well, to get into the real nitty gritty already,
[00:01:55.340 --> 00:01:57.800]   basically poker is a game of math.
[00:01:57.800 --> 00:02:00.020]   There are these strategies,
[00:02:00.020 --> 00:02:02.540]   familiar with like Nash equilibria, that term, right?
[00:02:02.540 --> 00:02:06.300]   So there are these game theory optimal strategies
[00:02:06.300 --> 00:02:08.300]   that you can adopt.
[00:02:08.300 --> 00:02:10.340]   And the closer you play to them,
[00:02:10.340 --> 00:02:12.400]   the less exploitable you are.
[00:02:12.400 --> 00:02:15.940]   So because I've studied the game a bunch,
[00:02:15.940 --> 00:02:17.260]   although admittedly not for a few years,
[00:02:17.260 --> 00:02:20.020]   but back in, you know, when I was playing all the time,
[00:02:20.020 --> 00:02:23.020]   I would study these game theory optimal solutions
[00:02:23.020 --> 00:02:24.900]   and try and then adopt those strategies
[00:02:24.900 --> 00:02:25.740]   when I go and play.
[00:02:25.740 --> 00:02:27.740]   So I'd play against you and I would do that.
[00:02:27.740 --> 00:02:31.820]   And because the objective,
[00:02:31.820 --> 00:02:33.220]   when you're playing game theory optimal,
[00:02:33.220 --> 00:02:35.980]   it's actually, it's a loss minimization thing
[00:02:35.980 --> 00:02:37.340]   that you're trying to do.
[00:02:37.340 --> 00:02:42.340]   Your best bet is to try and play a sort of similar style.
[00:02:42.340 --> 00:02:46.180]   You also need to try and adopt this loss minimization.
[00:02:46.180 --> 00:02:48.100]   But because I've been playing much longer than you,
[00:02:48.100 --> 00:02:49.380]   I'll be better at that.
[00:02:49.380 --> 00:02:51.900]   So first of all, you're not taking advantage
[00:02:51.900 --> 00:02:53.140]   of my mistakes.
[00:02:53.140 --> 00:02:54.980]   But then on top of that,
[00:02:54.980 --> 00:02:56.900]   I'll be better at recognizing
[00:02:56.900 --> 00:02:59.500]   when you are playing suboptimally
[00:02:59.500 --> 00:03:02.180]   and then deviating from this game theory optimal strategy
[00:03:02.180 --> 00:03:05.100]   to exploit your bad plays.
[00:03:05.100 --> 00:03:08.660]   - Can you define game theory and Nash equilibria?
[00:03:08.660 --> 00:03:10.900]   Can we try to sneak up to it in a bunch of ways?
[00:03:10.900 --> 00:03:14.340]   Like what's a game theory framework of analyzing poker,
[00:03:14.340 --> 00:03:16.300]   analyzing any kind of situation?
[00:03:16.300 --> 00:03:20.060]   - So game theory is just basically the study
[00:03:20.060 --> 00:03:25.060]   of decisions within a competitive situation.
[00:03:25.060 --> 00:03:27.660]   I mean, it's technically a branch of economics,
[00:03:27.660 --> 00:03:30.860]   but it also applies to like wider decision theory.
[00:03:30.860 --> 00:03:35.780]   And usually when you see it,
[00:03:35.780 --> 00:03:37.860]   it's these like little payoff matrices and so on.
[00:03:37.860 --> 00:03:38.740]   That's how it's depicted.
[00:03:38.740 --> 00:03:41.100]   But it's essentially just like study of strategies
[00:03:41.100 --> 00:03:43.020]   under different competitive situations.
[00:03:43.020 --> 00:03:46.340]   And as it happens, certain games,
[00:03:46.340 --> 00:03:47.980]   in fact, many, many games,
[00:03:47.980 --> 00:03:50.460]   have these things called Nash equilibria.
[00:03:50.460 --> 00:03:52.380]   And what that means is when you're in a Nash equilibrium,
[00:03:52.380 --> 00:03:55.460]   basically it is not,
[00:03:55.460 --> 00:03:59.700]   there is no strategy that you can take
[00:03:59.700 --> 00:04:01.180]   that would be more beneficial
[00:04:01.180 --> 00:04:02.780]   than the one you're currently taking,
[00:04:02.780 --> 00:04:05.660]   assuming your opponent is also doing the same thing.
[00:04:05.660 --> 00:04:06.700]   So it would be a bad idea.
[00:04:06.700 --> 00:04:10.660]   If we're both playing in a game theory optimal strategy,
[00:04:10.660 --> 00:04:12.140]   if either of us deviate from that,
[00:04:12.140 --> 00:04:16.620]   now we're putting ourselves at a disadvantage.
[00:04:16.620 --> 00:04:17.620]   Rock, paper, scissors is actually
[00:04:17.620 --> 00:04:18.860]   a really great example of this.
[00:04:18.860 --> 00:04:22.420]   Like if we were to start playing rock, paper, scissors,
[00:04:22.420 --> 00:04:23.780]   you know, you know nothing about me
[00:04:23.780 --> 00:04:26.220]   and we're gonna play for all our money,
[00:04:26.220 --> 00:04:27.860]   let's play 10 rounds of it.
[00:04:27.860 --> 00:04:30.860]   What would your sort of optimal strategy be, do you think?
[00:04:30.860 --> 00:04:31.780]   What would you do?
[00:04:31.780 --> 00:04:35.220]   - Let's see.
[00:04:35.220 --> 00:04:40.220]   I would probably try to be as random as possible.
[00:04:40.220 --> 00:04:43.580]   - Exactly.
[00:04:43.580 --> 00:04:46.060]   You wanna, because you don't know anything about me.
[00:04:46.060 --> 00:04:48.140]   You don't want to give anything away about yourself.
[00:04:48.140 --> 00:04:49.580]   So ideally you'd have like a little dice
[00:04:49.580 --> 00:04:52.620]   or somewhat, you know, perfect randomizer
[00:04:52.620 --> 00:04:54.540]   that makes you randomize 33% of the time
[00:04:54.540 --> 00:04:56.100]   each of the three different things.
[00:04:56.100 --> 00:04:57.340]   And in response to that,
[00:04:57.340 --> 00:04:59.660]   well, actually I can kind of do anything,
[00:04:59.660 --> 00:05:01.420]   but I would probably just randomize back too.
[00:05:01.420 --> 00:05:02.420]   But actually it wouldn't matter
[00:05:02.420 --> 00:05:05.180]   'cause I know that you're playing randomly.
[00:05:05.180 --> 00:05:07.660]   So that would be us in a Nash equilibrium
[00:05:07.660 --> 00:05:10.660]   where we're both playing this like unexploitable strategy.
[00:05:10.660 --> 00:05:13.060]   However, if after a while you then notice
[00:05:13.060 --> 00:05:16.420]   that I'm playing rock a little bit more often than I should.
[00:05:16.420 --> 00:05:18.180]   - Yeah, you're the kind of person that would do that.
[00:05:18.180 --> 00:05:19.020]   Wouldn't you?
[00:05:19.020 --> 00:05:19.980]   - Sure, yes, yes, yes.
[00:05:19.980 --> 00:05:21.340]   I'm more of a scissors girl, but anyway.
[00:05:21.340 --> 00:05:22.540]   - You are?
[00:05:22.540 --> 00:05:24.460]   - No, I'm a, as I said, randomizer.
[00:05:24.460 --> 00:05:27.140]   So you notice I'm throwing rock too much
[00:05:27.140 --> 00:05:28.180]   or something like that.
[00:05:28.180 --> 00:05:30.140]   Now you'd be making a mistake by continuing
[00:05:30.140 --> 00:05:32.220]   playing this game theory optimal strategy,
[00:05:32.220 --> 00:05:33.060]   well, the previous one,
[00:05:33.060 --> 00:05:37.860]   because you are now, I'm making a mistake
[00:05:37.860 --> 00:05:41.340]   and you're not deviating and exploiting my mistake.
[00:05:41.340 --> 00:05:43.820]   So you'd wanna start throwing paper a bit more often
[00:05:43.820 --> 00:05:45.980]   in whatever you figure is the right sort of percentage
[00:05:45.980 --> 00:05:48.100]   of the time that I'm throwing rock too often.
[00:05:48.100 --> 00:05:51.740]   So that's basically an example of where,
[00:05:51.740 --> 00:05:53.300]   what game theory optimal strategy is
[00:05:53.300 --> 00:05:54.700]   in terms of loss minimization,
[00:05:54.700 --> 00:05:58.060]   but it's not always the maximally profitable thing
[00:05:58.060 --> 00:06:00.180]   if your opponent is doing stupid stuff,
[00:06:00.180 --> 00:06:02.580]   which in that example.
[00:06:02.580 --> 00:06:04.540]   So that's kind of then how it works in poker,
[00:06:04.540 --> 00:06:06.460]   but it's a lot more complex.
[00:06:06.460 --> 00:06:10.660]   And the way poker players typically,
[00:06:10.660 --> 00:06:12.940]   nowadays they study, the games changed so much
[00:06:12.940 --> 00:06:15.460]   and I think we should talk about how it sort of evolved.
[00:06:15.460 --> 00:06:19.380]   But nowadays like the top pros basically spend all their time
[00:06:19.380 --> 00:06:23.140]   in between sessions running these simulators
[00:06:23.140 --> 00:06:25.020]   using like software where they do basically
[00:06:25.020 --> 00:06:26.180]   Monte Carlo simulations,
[00:06:26.180 --> 00:06:31.180]   sort of doing billions of fictitious self-play hands.
[00:06:31.180 --> 00:06:34.060]   You input a fictitious hand scenario,
[00:06:34.060 --> 00:06:36.100]   like, oh, what do I do with Jack nine suited
[00:06:36.100 --> 00:06:40.380]   on a King 10 four two spade board
[00:06:40.380 --> 00:06:43.980]   and against this bet size.
[00:06:43.980 --> 00:06:45.820]   So you'd input that, press play,
[00:06:45.820 --> 00:06:49.380]   it'll run it's billions of fake hands
[00:06:49.380 --> 00:06:50.380]   and then it will converge upon
[00:06:50.380 --> 00:06:52.580]   what the game theory optimal strategies are.
[00:06:52.580 --> 00:06:55.460]   And then you wanna try and memorize what these are.
[00:06:55.460 --> 00:06:57.500]   Basically they're like ratios of how often,
[00:06:57.500 --> 00:06:59.940]   what types of hands you want to bluff
[00:06:59.940 --> 00:07:01.340]   and what percentage of the time.
[00:07:01.340 --> 00:07:02.820]   So then there's this additional layer
[00:07:02.820 --> 00:07:04.500]   of inbuilt randomization built in.
[00:07:04.500 --> 00:07:06.420]   - Yeah, those kinds of simulations incorporate
[00:07:06.420 --> 00:07:08.540]   all the betting strategies and everything else like that.
[00:07:08.540 --> 00:07:12.340]   So as opposed to some kind of very crude mathematical model
[00:07:12.340 --> 00:07:13.860]   of what's the probability you win
[00:07:13.860 --> 00:07:16.380]   just based on the quality of the card,
[00:07:16.380 --> 00:07:20.220]   it's including everything else too, the game theory of it.
[00:07:20.220 --> 00:07:21.980]   - Yes, yeah, essentially.
[00:07:21.980 --> 00:07:23.780]   And what's interesting is that nowadays,
[00:07:23.780 --> 00:07:25.100]   if you want to be a top pro
[00:07:25.100 --> 00:07:26.340]   and you go and play in these really like
[00:07:26.340 --> 00:07:29.220]   the super high stakes tournaments or tough cash games,
[00:07:29.220 --> 00:07:30.700]   if you don't know this stuff,
[00:07:30.700 --> 00:07:33.260]   you're gonna get eaten alive in the long run.
[00:07:33.260 --> 00:07:35.140]   But of course you could get lucky over the short run.
[00:07:35.140 --> 00:07:36.860]   And that's where this like luck factor comes in
[00:07:36.860 --> 00:07:40.420]   because luck is both a blessing and a curse.
[00:07:40.420 --> 00:07:42.580]   If luck didn't, if there wasn't this random element
[00:07:42.580 --> 00:07:45.260]   and there wasn't the ability for worse players
[00:07:45.260 --> 00:07:48.460]   to win sometimes, then poker would fall apart.
[00:07:48.460 --> 00:07:51.500]   The same reason people don't play chess professionally
[00:07:51.500 --> 00:07:54.100]   for money against, you don't see people going
[00:07:54.100 --> 00:07:56.860]   and hustling chess, like not knowing,
[00:07:56.860 --> 00:07:57.860]   trying to make a living from it
[00:07:57.860 --> 00:08:00.140]   because you know there's very little luck in chess,
[00:08:00.140 --> 00:08:01.420]   but there's quite a lot of luck in poker.
[00:08:01.420 --> 00:08:03.940]   - Have you seen "Beautiful Mind," that movie?
[00:08:03.940 --> 00:08:04.780]   - Years ago.
[00:08:04.780 --> 00:08:06.100]   - Well, what do you think about the game
[00:08:06.100 --> 00:08:08.540]   theoretic formulation of what is it,
[00:08:08.540 --> 00:08:09.980]   the hot blonde at the bar?
[00:08:09.980 --> 00:08:10.820]   Do you remember?
[00:08:10.820 --> 00:08:11.660]   - Oh yeah.
[00:08:11.660 --> 00:08:13.940]   - The way they illustrated it is they're trying
[00:08:13.940 --> 00:08:16.460]   to pick up a girl at a bar and there's multiple girls.
[00:08:16.460 --> 00:08:18.140]   They're like, it's like a friend group
[00:08:18.140 --> 00:08:20.100]   and you're trying to approach.
[00:08:20.100 --> 00:08:21.980]   I don't remember the details, but I remember-
[00:08:21.980 --> 00:08:23.860]   - Don't you like then speak to her friends first?
[00:08:23.860 --> 00:08:24.700]   - Yeah, yeah.
[00:08:24.700 --> 00:08:25.700]   - Something like that, fame, disinterest.
[00:08:25.700 --> 00:08:27.380]   I mean, it's classic pickup artist stuff, right?
[00:08:27.380 --> 00:08:28.220]   You wanna-
[00:08:28.220 --> 00:08:30.780]   - And they were trying to correlate that somehow,
[00:08:30.780 --> 00:08:35.340]   that being an optimal strategy game, theoretically.
[00:08:35.340 --> 00:08:36.740]   Why?
[00:08:36.740 --> 00:08:37.580]   What, what?
[00:08:37.580 --> 00:08:38.900]   Like, I don't think I remember-
[00:08:38.900 --> 00:08:39.740]   - I can't imagine that there is.
[00:08:39.740 --> 00:08:41.900]   I mean, there's probably an optimal strategy.
[00:08:41.900 --> 00:08:45.020]   Is it, does that mean that there's an actual Nash equilibrium
[00:08:45.020 --> 00:08:46.620]   of like picking up girls?
[00:08:46.620 --> 00:08:48.980]   - Do you know the marriage problem?
[00:08:48.980 --> 00:08:50.900]   It's optimal stopping?
[00:08:50.900 --> 00:08:51.740]   - Yes.
[00:08:51.740 --> 00:08:54.980]   - So where it's a optimal dating strategy where you,
[00:08:54.980 --> 00:08:56.660]   do you remember what it is?
[00:08:56.660 --> 00:08:57.500]   - Yeah, I think it's like something like,
[00:08:57.500 --> 00:08:59.900]   you know you've got like a set of a hundred people
[00:08:59.900 --> 00:09:04.220]   you're gonna look through and after how many do you,
[00:09:04.220 --> 00:09:08.020]   now after that, after going on this many dates out of a 100,
[00:09:08.020 --> 00:09:09.180]   at what point do you then go,
[00:09:09.180 --> 00:09:11.580]   okay, the next best person I see, is that the right one?
[00:09:11.580 --> 00:09:14.180]   And I think it's like something like 37%.
[00:09:14.180 --> 00:09:17.740]   - It's one over E, whatever that is.
[00:09:17.740 --> 00:09:19.300]   - Right, which I think is 37%.
[00:09:19.300 --> 00:09:20.140]   - Yeah.
[00:09:20.140 --> 00:09:21.260]   (laughing)
[00:09:21.260 --> 00:09:22.820]   We're gonna fact check that.
[00:09:22.820 --> 00:09:24.860]   (laughing)
[00:09:24.860 --> 00:09:28.420]   Yeah, so, but it's funny under those strict constraints,
[00:09:28.420 --> 00:09:30.660]   then yes, after that many people,
[00:09:30.660 --> 00:09:32.820]   as long as you have a fixed size pool,
[00:09:32.820 --> 00:09:36.460]   then you just pick the next person
[00:09:36.460 --> 00:09:38.460]   that is better than anyone you've seen before.
[00:09:38.460 --> 00:09:39.300]   - Yeah.
[00:09:39.300 --> 00:09:41.780]   Have you tried this?
[00:09:41.780 --> 00:09:42.620]   Have you incorporated it?
[00:09:42.620 --> 00:09:44.660]   - I'm not one of those people.
[00:09:44.660 --> 00:09:46.900]   And we're gonna discuss this.
[00:09:46.900 --> 00:09:50.140]   And what do you mean, those people?
[00:09:50.140 --> 00:09:52.580]   I try not to optimize stuff.
[00:09:52.580 --> 00:09:55.300]   I try to listen to the heart.
[00:09:55.340 --> 00:09:57.020]   I don't think,
[00:09:57.020 --> 00:10:02.180]   I like, my mind immediately is attracted
[00:10:02.180 --> 00:10:06.260]   to optimizing everything.
[00:10:06.260 --> 00:10:09.940]   And I think that if you really give in
[00:10:09.940 --> 00:10:11.100]   to that kind of addiction,
[00:10:11.100 --> 00:10:14.940]   that you lose the joy of the small things,
[00:10:14.940 --> 00:10:17.220]   the minutia of life, I think.
[00:10:17.220 --> 00:10:18.060]   I don't know.
[00:10:18.060 --> 00:10:19.820]   I'm concerned about the addictive nature
[00:10:19.820 --> 00:10:21.620]   of my personality in that regard.
[00:10:21.620 --> 00:10:22.780]   - In some ways.
[00:10:24.780 --> 00:10:26.540]   - Well, I think the, on average,
[00:10:26.540 --> 00:10:30.140]   people under try and quantify things
[00:10:30.140 --> 00:10:32.540]   or try under optimize.
[00:10:32.540 --> 00:10:34.220]   There are some people who,
[00:10:34.220 --> 00:10:35.340]   it's like with all these things,
[00:10:35.340 --> 00:10:37.140]   it's a balancing act.
[00:10:37.140 --> 00:10:40.180]   - I've been on dating apps, but I've never used them.
[00:10:40.180 --> 00:10:42.060]   I'm sure they have data on this
[00:10:42.060 --> 00:10:43.060]   because they probably have
[00:10:43.060 --> 00:10:45.260]   the optimal stopping control problem.
[00:10:45.260 --> 00:10:47.380]   'Cause there aren't a lot of people that use social,
[00:10:47.380 --> 00:10:51.020]   like dating apps are on there for a long time.
[00:10:51.020 --> 00:10:56.020]   So the interesting aspect is like, all right,
[00:10:56.020 --> 00:10:58.740]   how long before you stop looking
[00:10:58.740 --> 00:11:01.740]   before it actually starts affecting your mind negatively
[00:11:01.740 --> 00:11:05.140]   such that you see dating as a kind of--
[00:11:05.140 --> 00:11:08.220]   - Game.
[00:11:08.220 --> 00:11:12.780]   - A kind of game versus an actual process
[00:11:12.780 --> 00:11:14.420]   of finding somebody that's gonna make you happy
[00:11:14.420 --> 00:11:15.780]   for the rest of your life.
[00:11:15.780 --> 00:11:17.340]   That's really interesting.
[00:11:17.340 --> 00:11:18.260]   They have the data.
[00:11:18.260 --> 00:11:20.420]   I wish they would be able to release that data.
[00:11:20.420 --> 00:11:21.260]   And I do want to--
[00:11:21.260 --> 00:11:22.220]   - It's OKCupid, right?
[00:11:22.220 --> 00:11:25.340]   I think they ran a huge, huge study on all of their--
[00:11:25.340 --> 00:11:26.780]   - Yeah, they're more data-driven, I think,
[00:11:26.780 --> 00:11:28.060]   OKCupid folks are.
[00:11:28.060 --> 00:11:30.220]   I think there's a lot of opportunity for dating apps
[00:11:30.220 --> 00:11:32.340]   in general, even bigger than dating apps,
[00:11:32.340 --> 00:11:35.020]   people connecting on the internet.
[00:11:35.020 --> 00:11:37.580]   I just hope they're more data-driven
[00:11:37.580 --> 00:11:40.340]   and it doesn't seem that way.
[00:11:40.340 --> 00:11:43.380]   I think like, I've always thought that
[00:11:43.380 --> 00:11:47.220]   Goodreads should be a dating app.
[00:11:47.220 --> 00:11:48.060]   (Hannah laughs)
[00:11:48.060 --> 00:11:49.460]   Like the-- - I've never used it.
[00:11:49.460 --> 00:11:54.460]   - The Goodreads is just lists books that you've read
[00:11:54.460 --> 00:11:57.380]   and allows you to comment on the books you read
[00:11:57.380 --> 00:11:58.940]   and what the books you're currently reading.
[00:11:58.940 --> 00:12:01.300]   But it's a giant social networks of people reading books.
[00:12:01.300 --> 00:12:04.620]   And that seems to be a much better database of interests.
[00:12:04.620 --> 00:12:06.580]   Of course, it constrains you to the books you're reading,
[00:12:06.580 --> 00:12:10.220]   but that really reveals so much more about the person.
[00:12:10.220 --> 00:12:12.460]   Allows you to discover shared interests
[00:12:12.460 --> 00:12:13.940]   because books are a kind of window
[00:12:13.940 --> 00:12:16.020]   into the way you see the world.
[00:12:16.020 --> 00:12:20.380]   Also, like the kind of places, people you're curious about,
[00:12:20.380 --> 00:12:21.740]   the kind of ideas you're curious about.
[00:12:21.740 --> 00:12:23.700]   Are you a romantic or are you cold,
[00:12:23.700 --> 00:12:24.860]   calculating rationalist?
[00:12:24.860 --> 00:12:28.260]   Are you into Ayn Rand or are you into Bernie Sanders?
[00:12:28.260 --> 00:12:29.940]   Are you into whatever?
[00:12:29.940 --> 00:12:31.980]   And I feel like that reveals so much more
[00:12:31.980 --> 00:12:35.220]   than like a person trying to look hot
[00:12:35.220 --> 00:12:37.140]   from a certain angle in a Tinder profile.
[00:12:37.140 --> 00:12:38.940]   - Well, and it also be a really great filter
[00:12:38.940 --> 00:12:40.340]   in the first place for people.
[00:12:40.340 --> 00:12:41.940]   It's like people who read books
[00:12:41.940 --> 00:12:44.700]   and are willing to go and rate them
[00:12:44.700 --> 00:12:47.020]   and give feedback on them and so on.
[00:12:47.020 --> 00:12:48.580]   So that's already a really strong filter
[00:12:48.580 --> 00:12:50.620]   of probably the type of people you'd be looking for.
[00:12:50.620 --> 00:12:52.380]   - Well, at least be able to fake reading books.
[00:12:52.380 --> 00:12:53.540]   I mean, the thing about books,
[00:12:53.540 --> 00:12:54.500]   you don't really need to read it.
[00:12:54.500 --> 00:12:55.860]   You can just look at the CliffsNotes.
[00:12:55.860 --> 00:12:59.060]   - Yeah, game the dating app by feigning intellectualism.
[00:12:59.060 --> 00:13:02.380]   - Can I admit something very horrible about myself?
[00:13:02.380 --> 00:13:03.220]   - Go on.
[00:13:03.220 --> 00:13:04.340]   - The things that, you know,
[00:13:04.340 --> 00:13:05.700]   I don't have many things in my closet,
[00:13:05.700 --> 00:13:06.900]   but this is one of them.
[00:13:06.900 --> 00:13:10.700]   I've never actually read Shakespeare.
[00:13:10.700 --> 00:13:12.380]   I've only read CliffsNotes.
[00:13:12.420 --> 00:13:14.940]   And I got a five in the AP English exam.
[00:13:14.940 --> 00:13:15.900]   - Wow.
[00:13:15.900 --> 00:13:17.100]   - And I- - Which book?
[00:13:17.100 --> 00:13:19.660]   - Which books have I read?
[00:13:19.660 --> 00:13:21.540]   - Well, yeah, which was the exam on?
[00:13:21.540 --> 00:13:23.340]   - Oh, no, they include a lot of them.
[00:13:23.340 --> 00:13:27.180]   But Hamlet, I don't even know
[00:13:27.180 --> 00:13:30.100]   if you read Romeo and Juliet, Macbeth.
[00:13:30.100 --> 00:13:32.700]   I don't remember, but I don't understand it.
[00:13:32.700 --> 00:13:34.060]   It's like really cryptic.
[00:13:34.060 --> 00:13:34.900]   - It's hard.
[00:13:34.900 --> 00:13:35.980]   - It's really, I don't,
[00:13:35.980 --> 00:13:37.980]   and it's not that pleasant to read.
[00:13:37.980 --> 00:13:39.220]   It's like ancient speak.
[00:13:39.220 --> 00:13:40.340]   I don't understand it.
[00:13:40.340 --> 00:13:41.820]   Anyway, maybe I was too dumb.
[00:13:41.820 --> 00:13:44.780]   I'm still too dumb, but I did-
[00:13:44.780 --> 00:13:45.860]   - But you got a five, which is-
[00:13:45.860 --> 00:13:46.700]   - Yeah, yeah.
[00:13:46.700 --> 00:13:48.020]   - I don't know how the US grading system-
[00:13:48.020 --> 00:13:50.420]   - Oh, no, so AP English is,
[00:13:50.420 --> 00:13:53.060]   there's kind of this advanced versions of courses
[00:13:53.060 --> 00:13:54.940]   in high school, and you take a test
[00:13:54.940 --> 00:13:57.940]   that is like a broad test for that subject
[00:13:57.940 --> 00:13:58.940]   and includes a lot.
[00:13:58.940 --> 00:14:00.580]   It wasn't obviously just Shakespeare.
[00:14:00.580 --> 00:14:04.940]   I think a lot of it was also writing, written.
[00:14:04.940 --> 00:14:07.180]   You have like AP Physics, AP Computer Science,
[00:14:07.180 --> 00:14:09.780]   AP Biology, AP Chemistry,
[00:14:09.780 --> 00:14:12.180]   and then AP English or AP Literature.
[00:14:12.180 --> 00:14:13.460]   I forget what it was,
[00:14:13.460 --> 00:14:16.980]   but I think Shakespeare was a part of that, but I-
[00:14:16.980 --> 00:14:19.180]   - And you game, the point is you gamified it.
[00:14:19.180 --> 00:14:22.580]   - Gamified, well, entirety, I was into getting As.
[00:14:22.580 --> 00:14:24.220]   I saw it as a game.
[00:14:24.220 --> 00:14:25.740]   I don't think any,
[00:14:25.740 --> 00:14:30.380]   I don't think all the learning I've done
[00:14:30.380 --> 00:14:33.980]   has been outside of school.
[00:14:33.980 --> 00:14:36.260]   The deepest learning I've done has been outside of school,
[00:14:36.260 --> 00:14:38.420]   with a few exceptions, especially in grad school,
[00:14:38.420 --> 00:14:40.460]   like deep computer science courses,
[00:14:40.460 --> 00:14:41.820]   but that was still outside of school
[00:14:41.820 --> 00:14:43.740]   'cause it was outside of, sorry,
[00:14:43.740 --> 00:14:46.220]   it was outside of getting the A for the course.
[00:14:46.220 --> 00:14:49.660]   The best stuff I've ever done is when you read the chapter
[00:14:49.660 --> 00:14:52.340]   and you do many of the problems at the end of the chapter,
[00:14:52.340 --> 00:14:54.940]   which is usually not what's required for the course,
[00:14:54.940 --> 00:14:56.180]   like the hardest stuff.
[00:14:56.180 --> 00:14:58.820]   In fact, textbooks are freaking incredible.
[00:14:58.820 --> 00:15:02.260]   If you go back now and you look at like biology textbook
[00:15:02.260 --> 00:15:06.100]   or any of the computer science textbooks
[00:15:06.100 --> 00:15:07.780]   on algorithms and data structures,
[00:15:07.780 --> 00:15:09.540]   those things are incredible.
[00:15:09.540 --> 00:15:11.980]   They have the best summary of a subject,
[00:15:11.980 --> 00:15:15.340]   plus they have practice problems of increasing difficulty
[00:15:15.340 --> 00:15:17.740]   that allows you to truly master the basic,
[00:15:17.740 --> 00:15:19.940]   like the fundamental ideas behind that.
[00:15:19.940 --> 00:15:24.420]   - I got through my entire physics degree with one textbook
[00:15:24.420 --> 00:15:26.300]   that was just this really comprehensive one
[00:15:26.300 --> 00:15:28.660]   that they told us at the beginning of the first year,
[00:15:28.660 --> 00:15:31.700]   buy this, but you're gonna have to buy 15 other books
[00:15:31.700 --> 00:15:33.380]   for all your supplementary courses,
[00:15:33.380 --> 00:15:35.500]   and I was like, every time I would just check
[00:15:35.500 --> 00:15:36.700]   to see whether this book covered it,
[00:15:36.700 --> 00:15:39.820]   and it did, and I think I only bought like two or three extra
[00:15:39.820 --> 00:15:41.900]   and thank God, 'cause they're super expensive textbooks,
[00:15:41.900 --> 00:15:44.300]   it's a whole racket they've got going on.
[00:15:44.300 --> 00:15:46.220]   Yeah, they are, they could just,
[00:15:46.220 --> 00:15:49.500]   you get the right one, it's just like a manual for,
[00:15:49.500 --> 00:15:52.940]   but what's interesting though is,
[00:15:52.940 --> 00:15:56.780]   this is the tyranny of having exams and metrics.
[00:15:56.780 --> 00:15:58.660]   - The tyranny of exams and metrics, yes.
[00:15:58.660 --> 00:16:00.980]   - I loved them because I'm very competitive
[00:16:00.980 --> 00:16:04.020]   and I liked finding ways to gamify things
[00:16:04.020 --> 00:16:06.380]   and then like sort of dust off my shoulders afterwards
[00:16:06.380 --> 00:16:08.140]   when I get a good grade or be annoyed at myself
[00:16:08.140 --> 00:16:10.860]   when I didn't, but yeah, you're absolutely right
[00:16:10.860 --> 00:16:14.420]   in that the actual, how much of that physics knowledge
[00:16:14.420 --> 00:16:19.420]   I've retained, like I've, I learned how to cram and study
[00:16:19.420 --> 00:16:22.180]   and please an examiner, but did that give me
[00:16:22.180 --> 00:16:24.700]   the deep lasting knowledge that I needed?
[00:16:24.700 --> 00:16:29.100]   I mean, yes and no, but really like nothing makes you learn
[00:16:29.100 --> 00:16:31.980]   a topic better than when you actually then have
[00:16:31.980 --> 00:16:33.180]   to teach it yourself.
[00:16:33.180 --> 00:16:36.180]   You know, like I'm trying to wrap my teeth around this,
[00:16:36.180 --> 00:16:38.140]   like game theory, Moloch stuff right now
[00:16:38.140 --> 00:16:43.060]   and there's no exam at the end of it that I can gamify.
[00:16:43.060 --> 00:16:44.940]   There's no way to gamify and sort of like shortcut
[00:16:44.940 --> 00:16:45.780]   my way through it.
[00:16:45.780 --> 00:16:47.220]   I have to understand it so deeply
[00:16:47.220 --> 00:16:50.700]   from like deep foundational levels to then build upon it
[00:16:50.700 --> 00:16:52.340]   and then try and explain it to other people.
[00:16:52.340 --> 00:16:54.340]   And like, you're about to go and do some lectures, right?
[00:16:54.340 --> 00:16:57.580]   You can't sort of just like,
[00:16:57.580 --> 00:17:00.620]   you presumably can't rely on the knowledge
[00:17:00.620 --> 00:17:03.420]   that you got through when you were studying for an exam
[00:17:03.420 --> 00:17:04.820]   to reteach that.
[00:17:04.820 --> 00:17:06.820]   - Yeah, and especially high level lectures,
[00:17:06.820 --> 00:17:09.420]   especially the kind of stuff you do on YouTube,
[00:17:09.420 --> 00:17:12.780]   you're not just regurgitating material.
[00:17:12.780 --> 00:17:17.100]   You have to think through what is the core idea here.
[00:17:17.100 --> 00:17:20.740]   And when you do the lectures live especially,
[00:17:20.740 --> 00:17:23.900]   you have to, there's no second takes.
[00:17:23.900 --> 00:17:28.380]   That is the luxury you get if you're recording a video
[00:17:28.380 --> 00:17:30.180]   for YouTube or something like that.
[00:17:30.180 --> 00:17:33.960]   But it definitely is a luxury you shouldn't lean on.
[00:17:34.800 --> 00:17:37.360]   I've gotten to interact with a few YouTubers
[00:17:37.360 --> 00:17:39.320]   that lean on that too much.
[00:17:39.320 --> 00:17:43.400]   And you realize, oh, you've gamified this system
[00:17:43.400 --> 00:17:46.760]   because you're not really thinking deeply about stuff.
[00:17:46.760 --> 00:17:51.760]   You're through the edit, both written and spoken,
[00:17:51.760 --> 00:17:53.920]   you're crafting an amazing video,
[00:17:53.920 --> 00:17:55.480]   but you yourself as a human being
[00:17:55.480 --> 00:17:57.640]   have not really deeply understood it.
[00:17:57.640 --> 00:18:00.800]   So live teaching, or at least recording video
[00:18:00.800 --> 00:18:04.680]   with very few takes is a different beast.
[00:18:04.680 --> 00:18:07.320]   And I think it's the most honest way of doing it,
[00:18:07.320 --> 00:18:09.200]   like as few takes as possible.
[00:18:09.200 --> 00:18:10.800]   - That's why I'm nervous about this.
[00:18:10.800 --> 00:18:12.040]   (laughing)
[00:18:12.040 --> 00:18:12.880]   - Don't-- - I'll go back and be like,
[00:18:12.880 --> 00:18:14.040]   ah, let's do that.
[00:18:14.040 --> 00:18:15.520]   - Don't fuck this up, Liv.
[00:18:15.520 --> 00:18:18.520]   The tyranny of exams.
[00:18:18.520 --> 00:18:23.520]   I do think people talk about high school and college
[00:18:23.520 --> 00:18:27.200]   as a time to do drugs and drink and have fun
[00:18:27.200 --> 00:18:28.320]   and all this kind of stuff.
[00:18:28.320 --> 00:18:33.320]   But looking back, of course I did a lot of those things.
[00:18:33.320 --> 00:18:39.240]   No, yes, but it's also a time when you get to read textbooks
[00:18:39.240 --> 00:18:47.740]   or read books or learn with all the time in the world.
[00:18:47.740 --> 00:18:51.760]   You don't have these responsibilities of laundry
[00:18:54.920 --> 00:18:59.920]   and having to pay for mortgage or all that kind of stuff,
[00:18:59.920 --> 00:19:04.060]   pay taxes, all this kind of stuff.
[00:19:04.060 --> 00:19:07.360]   In most cases, there's just so much time in the day
[00:19:07.360 --> 00:19:10.280]   for learning, and you don't realize it at the time
[00:19:10.280 --> 00:19:12.680]   because at the time it seems like a chore.
[00:19:12.680 --> 00:19:15.480]   Why the hell does there's so much homework?
[00:19:15.480 --> 00:19:18.060]   But you never get a chance to do this kind of learning,
[00:19:18.060 --> 00:19:21.080]   this kind of homework ever again in life,
[00:19:21.080 --> 00:19:24.640]   unless later in life you really make a big effort out of it.
[00:19:24.640 --> 00:19:27.480]   You get, basically your knowledge gets solidified.
[00:19:27.480 --> 00:19:29.160]   You don't get to have fun and learn.
[00:19:29.160 --> 00:19:33.360]   Learning is really fulfilling and really fun
[00:19:33.360 --> 00:19:34.320]   if you're that kind of person.
[00:19:34.320 --> 00:19:39.320]   Like some people like knowledge is not something
[00:19:39.320 --> 00:19:42.320]   that they think is fun, but if that's the kind of thing
[00:19:42.320 --> 00:19:44.880]   that you think is fun, that's the time to have fun
[00:19:44.880 --> 00:19:46.920]   and do the drugs and drink and all that kind of stuff.
[00:19:46.920 --> 00:19:51.360]   But the learning, just going back to those textbooks,
[00:19:51.360 --> 00:19:53.080]   the hours spent with the textbooks
[00:19:53.080 --> 00:19:55.000]   is really, really rewarding.
[00:19:55.000 --> 00:19:56.600]   - Do people even use textbooks anymore?
[00:19:56.600 --> 00:19:57.640]   - Yeah. - Do you think?
[00:19:57.640 --> 00:19:59.240]   'Cause-- - Kids these days
[00:19:59.240 --> 00:20:01.800]   with their TikTok and their-- - Well, not even that,
[00:20:01.800 --> 00:20:04.680]   but just like so much information,
[00:20:04.680 --> 00:20:06.280]   really high quality information
[00:20:06.280 --> 00:20:08.100]   is now in digital format online.
[00:20:08.100 --> 00:20:11.100]   - Yeah, but they're not, they are using that,
[00:20:11.100 --> 00:20:16.100]   but college is still very, there's a curriculum.
[00:20:16.100 --> 00:20:19.640]   I mean, so much of school is about rigorous study
[00:20:19.640 --> 00:20:23.960]   of a subject and still on YouTube, that's not there.
[00:20:23.960 --> 00:20:27.760]   YouTube has, Grant Sanderson talks about this,
[00:20:27.760 --> 00:20:30.080]   he's this math-- - 3Blue1Brown.
[00:20:30.080 --> 00:20:31.600]   - Yeah, 3Blue1Brown.
[00:20:31.600 --> 00:20:33.840]   He says like, "I'm not a math teacher.
[00:20:33.840 --> 00:20:37.700]   "I just take really cool concepts and I inspire people,
[00:20:37.700 --> 00:20:39.420]   "but if you wanna really learn calculus,
[00:20:39.420 --> 00:20:41.740]   "if you wanna really learn linear algebra,
[00:20:41.740 --> 00:20:45.240]   "you should do the textbook, you should do that."
[00:20:45.240 --> 00:20:49.020]   And there's still the textbook industrial complex
[00:20:49.020 --> 00:20:53.400]   that charges like $200 for a textbook and somehow,
[00:20:53.400 --> 00:20:54.960]   I don't know, it's ridiculous.
[00:20:54.960 --> 00:20:58.920]   - Well, they're like, "Oh, sorry, new edition,
[00:20:58.920 --> 00:21:02.920]   "edition 14.6, sorry, you can't use 14.5 anymore."
[00:21:02.920 --> 00:21:03.760]   It's like, "What's different?
[00:21:03.760 --> 00:21:05.380]   "We've got one paragraph different."
[00:21:05.380 --> 00:21:08.120]   - So we mentioned offline Daniel Negrano.
[00:21:08.120 --> 00:21:11.640]   I'm gonna get a chance to talk to him on this podcast
[00:21:11.640 --> 00:21:14.440]   and he's somebody that I found fascinating
[00:21:14.440 --> 00:21:16.560]   in terms of the way he thinks about poker,
[00:21:16.560 --> 00:21:18.640]   verbalizes the way he thinks about poker,
[00:21:18.640 --> 00:21:20.200]   the way he plays poker.
[00:21:20.200 --> 00:21:22.900]   So, and he's still pretty damn good.
[00:21:22.900 --> 00:21:24.900]   He's been good for a long time.
[00:21:24.900 --> 00:21:27.280]   So you mentioned that people are running
[00:21:27.280 --> 00:21:28.440]   these kinds of simulations
[00:21:28.440 --> 00:21:30.560]   and the game of poker has changed.
[00:21:30.560 --> 00:21:33.140]   Do you think he's adapting in this way?
[00:21:33.140 --> 00:21:34.880]   Do you think like the top pros,
[00:21:34.880 --> 00:21:36.540]   do they have to adapt this way?
[00:21:36.540 --> 00:21:41.540]   Or is there still like over the years,
[00:21:41.540 --> 00:21:45.300]   you basically develop this gut feeling about,
[00:21:45.300 --> 00:21:48.580]   like you get to be like good the way,
[00:21:48.580 --> 00:21:49.820]   like alpha zero is good.
[00:21:49.820 --> 00:21:54.180]   You look at the board and somehow from the fog
[00:21:54.180 --> 00:21:55.380]   comes out the right answer.
[00:21:55.380 --> 00:21:58.020]   Like this is likely what they have.
[00:21:58.020 --> 00:22:00.480]   This is likely the best way to move.
[00:22:00.480 --> 00:22:01.320]   And you don't really,
[00:22:01.320 --> 00:22:04.480]   you can't really put a finger on exactly why,
[00:22:04.480 --> 00:22:09.060]   but it just comes from your gut feeling or no.
[00:22:09.060 --> 00:22:10.560]   - Yes and no.
[00:22:10.560 --> 00:22:13.840]   So gut feelings are definitely very important.
[00:22:14.700 --> 00:22:15.980]   You know, that we've got our two mode
[00:22:15.980 --> 00:22:18.140]   or you can distill it down to two modes
[00:22:18.140 --> 00:22:19.060]   of decision-making, right?
[00:22:19.060 --> 00:22:22.140]   You've got your sort of logical linear voice in your head,
[00:22:22.140 --> 00:22:24.020]   system two, as it's often called
[00:22:24.020 --> 00:22:27.240]   and your system on your gut intuition.
[00:22:27.240 --> 00:22:32.340]   And historically in poker,
[00:22:32.340 --> 00:22:34.300]   the very best players were playing
[00:22:34.300 --> 00:22:35.860]   almost entirely by their gut.
[00:22:35.860 --> 00:22:39.220]   You know, often they do some kind of inspired play
[00:22:39.220 --> 00:22:40.420]   and you'd ask them why they do it
[00:22:40.420 --> 00:22:42.540]   and they wouldn't really be able to explain it.
[00:22:42.540 --> 00:22:46.460]   And that's not so much because their process
[00:22:46.460 --> 00:22:47.560]   was unintelligible,
[00:22:47.560 --> 00:22:50.120]   but it was more just because no one had the language
[00:22:50.120 --> 00:22:52.340]   with which to describe what optimal strategies were
[00:22:52.340 --> 00:22:54.260]   because no one really understood how poker worked.
[00:22:54.260 --> 00:22:57.540]   This was before, you know, we had analysis software,
[00:22:57.540 --> 00:22:59.860]   you know, no one was writing.
[00:22:59.860 --> 00:23:01.440]   I guess some people would write down their hands
[00:23:01.440 --> 00:23:02.660]   in a little notebook,
[00:23:02.660 --> 00:23:04.540]   but there was no way to assimilate all this data
[00:23:04.540 --> 00:23:05.900]   and analyze it.
[00:23:05.900 --> 00:23:08.460]   But then, you know, when computers became cheaper
[00:23:08.460 --> 00:23:09.880]   and software started emerging
[00:23:09.880 --> 00:23:11.460]   and then obviously online poker,
[00:23:11.500 --> 00:23:14.220]   where it would like automatically save your hand histories,
[00:23:14.220 --> 00:23:17.060]   now all of a sudden you kind of had this body of data
[00:23:17.060 --> 00:23:19.580]   that you could run analysis on.
[00:23:19.580 --> 00:23:22.140]   And so that's when people started to see, you know,
[00:23:22.140 --> 00:23:24.220]   these mathematical solutions.
[00:23:24.220 --> 00:23:31.740]   And so what that meant is the role of intuition
[00:23:31.740 --> 00:23:33.920]   essentially became smaller.
[00:23:33.920 --> 00:23:38.620]   And it went more into, as we talked before about,
[00:23:38.620 --> 00:23:40.600]   you know, this game theory optimal style.
[00:23:40.600 --> 00:23:43.300]   But also, as I said, like game theory optimal
[00:23:43.300 --> 00:23:47.740]   is about loss minimization and being unexploitable.
[00:23:47.740 --> 00:23:49.620]   But if you're playing against people who aren't,
[00:23:49.620 --> 00:23:51.620]   because no person, no human being can play perfectly
[00:23:51.620 --> 00:23:54.040]   game theory optimal in poker, not even the best AIs.
[00:23:54.040 --> 00:23:55.460]   They're still like, you know,
[00:23:55.460 --> 00:23:57.540]   they're 99.99% of the way there or whatever,
[00:23:57.540 --> 00:23:59.260]   but it's kind of like the speed of light.
[00:23:59.260 --> 00:24:01.060]   You can't reach it perfectly.
[00:24:01.060 --> 00:24:03.780]   - So there's still a role for intuition?
[00:24:03.780 --> 00:24:04.620]   - Yes.
[00:24:04.620 --> 00:24:08.460]   So when, yeah, when you're playing this unexploitable style,
[00:24:08.460 --> 00:24:11.440]   but when your opponents start doing something,
[00:24:11.440 --> 00:24:14.160]   you know, suboptimal that you want to exploit,
[00:24:14.160 --> 00:24:17.360]   well now that's where not only your like logical brain
[00:24:17.360 --> 00:24:19.960]   will need to be thinking, oh, okay, I know I have this,
[00:24:19.960 --> 00:24:22.200]   my, I'm in the sort of top end of my range here
[00:24:22.200 --> 00:24:23.940]   with this hand.
[00:24:23.940 --> 00:24:26.860]   So that means I need to be calling X percent of the time
[00:24:26.860 --> 00:24:30.480]   and I put them on this range, et cetera.
[00:24:30.480 --> 00:24:34.000]   But then sometimes you'll have this gut feeling
[00:24:34.000 --> 00:24:37.320]   that will tell you, you know, you know what, this time,
[00:24:37.320 --> 00:24:40.380]   I know mathematically I'm meant to call now, you know,
[00:24:40.380 --> 00:24:42.740]   I've got, I'm in the sort of top end of my range
[00:24:42.740 --> 00:24:45.140]   and this is the odds I'm getting.
[00:24:45.140 --> 00:24:46.300]   So the math says I should call,
[00:24:46.300 --> 00:24:48.360]   but there's something in your gut saying
[00:24:48.360 --> 00:24:49.960]   they've got it this time, they've got it.
[00:24:49.960 --> 00:24:54.960]   Like they're beating you, maybe your hand is worse.
[00:24:54.960 --> 00:24:56.940]   So then the real art,
[00:24:56.940 --> 00:24:59.620]   this is where the last remaining art in poker,
[00:24:59.620 --> 00:25:03.660]   the fuzziness is like, do you listen to your gut?
[00:25:03.660 --> 00:25:06.100]   How do you quantify the strength of it?
[00:25:06.100 --> 00:25:08.360]   Or can you even quantify the strength of it?
[00:25:08.360 --> 00:25:13.120]   And I think that's what Daniel has.
[00:25:13.120 --> 00:25:15.380]   I mean, I can't speak for how much he's studying
[00:25:15.380 --> 00:25:17.760]   with the simulators and that kind of thing.
[00:25:17.760 --> 00:25:22.280]   I think he has, like he must be to still be keeping up,
[00:25:22.280 --> 00:25:26.360]   but he has an incredible intuition for just,
[00:25:26.360 --> 00:25:29.220]   he's seen so many hands of poker in the flesh.
[00:25:29.220 --> 00:25:31.900]   He's seen so many people, the way they behave
[00:25:31.900 --> 00:25:33.760]   when the chips are, you know, when the money's on the line
[00:25:33.760 --> 00:25:36.120]   and you've got him staring you down in the eye,
[00:25:36.120 --> 00:25:37.480]   you know, he's intimidating.
[00:25:37.480 --> 00:25:39.640]   He's got this like kind of X factor vibe
[00:25:39.640 --> 00:25:42.200]   that he gives out.
[00:25:42.200 --> 00:25:45.040]   - And he talks a lot, which is an interactive element,
[00:25:45.040 --> 00:25:47.200]   which is he's getting stuff from other people.
[00:25:47.200 --> 00:25:48.040]   - Yes, yeah.
[00:25:48.040 --> 00:25:49.320]   - And just like the subtlety.
[00:25:49.320 --> 00:25:51.480]   So he's like, he's probing constantly.
[00:25:51.480 --> 00:25:52.960]   - Yeah, he's probing and he's getting
[00:25:52.960 --> 00:25:55.920]   this extra layer of information that others can't.
[00:25:55.920 --> 00:25:57.880]   Now that said though, he's good online as well.
[00:25:57.880 --> 00:25:59.680]   You know, I don't know how, again,
[00:25:59.680 --> 00:26:02.680]   would he be beating the top cash game players online?
[00:26:02.680 --> 00:26:03.920]   Probably not, no.
[00:26:03.920 --> 00:26:07.280]   But when he's in person
[00:26:07.280 --> 00:26:08.920]   and he's got that additional layer of information,
[00:26:08.920 --> 00:26:10.680]   he can not only extract it,
[00:26:10.680 --> 00:26:14.160]   but he knows what to do with it still so well.
[00:26:14.160 --> 00:26:15.520]   There's one player who I would say
[00:26:15.520 --> 00:26:17.080]   is the exception to all of this.
[00:26:17.080 --> 00:26:19.920]   And he's one of my favorite people to talk about
[00:26:19.920 --> 00:26:24.480]   in terms of, I think he might have cracked the simulation.
[00:26:24.480 --> 00:26:25.480]   It's Phil Helmuth.
[00:26:25.480 --> 00:26:28.000]   He-
[00:26:28.000 --> 00:26:30.160]   - In more ways than one, he's cracked the simulation,
[00:26:30.160 --> 00:26:31.000]   I think.
[00:26:31.000 --> 00:26:34.560]   - Yeah, he somehow to this day is still,
[00:26:34.560 --> 00:26:37.360]   and I love you, Phil, I'm not in any way knocking you.
[00:26:37.360 --> 00:26:41.480]   He's still winning so much
[00:26:41.480 --> 00:26:43.920]   at the World Series of Poker specifically.
[00:26:43.920 --> 00:26:45.560]   He's now won 16 bracelets.
[00:26:45.560 --> 00:26:48.520]   The next nearest person I think has won 10.
[00:26:48.520 --> 00:26:50.280]   And he is consistently year in, year out,
[00:26:50.280 --> 00:26:53.200]   going deep or winning these huge field tournaments,
[00:26:53.200 --> 00:26:55.400]   you know, with like 2000 people,
[00:26:55.400 --> 00:26:57.920]   which statistically he should not be doing.
[00:26:57.920 --> 00:27:02.800]   And yet you watch some of the plays he makes
[00:27:02.800 --> 00:27:04.160]   and they make no sense.
[00:27:04.160 --> 00:27:05.800]   Like mathematically, they are so far
[00:27:05.800 --> 00:27:07.640]   from game theory optimal.
[00:27:07.640 --> 00:27:09.120]   And the thing is, if you went and stuck him
[00:27:09.120 --> 00:27:11.480]   in one of these high stakes cash games
[00:27:11.480 --> 00:27:13.320]   with a bunch of like DTO people,
[00:27:13.320 --> 00:27:15.160]   he's gonna get ripped apart.
[00:27:15.160 --> 00:27:16.640]   But there's something that he has
[00:27:16.640 --> 00:27:17.600]   that when he's in the halls
[00:27:17.600 --> 00:27:19.680]   of the World Series of Poker specifically,
[00:27:19.680 --> 00:27:24.080]   amongst sort of amateurish players,
[00:27:24.080 --> 00:27:26.920]   he gets them to do crazy shit like that.
[00:27:26.920 --> 00:27:30.880]   And, but my little pet theory is that also,
[00:27:30.880 --> 00:27:35.960]   he's like a wizard and he gets the cards
[00:27:35.960 --> 00:27:37.360]   to do what he needs them to.
[00:27:37.360 --> 00:27:42.840]   Because he just expects to win
[00:27:42.840 --> 00:27:45.240]   and he expects to get flop a set
[00:27:45.240 --> 00:27:50.240]   with a frequency far beyond what the real percentages are.
[00:27:50.240 --> 00:27:51.800]   And I don't even know if he knows
[00:27:51.800 --> 00:27:52.640]   what the real percentages are.
[00:27:52.640 --> 00:27:54.760]   He doesn't need to, because he gets there.
[00:27:54.760 --> 00:27:56.040]   - I think he has found a cheat code.
[00:27:56.040 --> 00:27:57.400]   'Cause when I've seen him play,
[00:27:57.400 --> 00:27:59.880]   he seems to be like annoyed
[00:27:59.880 --> 00:28:02.000]   that the long shot thing didn't happen.
[00:28:02.000 --> 00:28:02.880]   - Yes.
[00:28:02.880 --> 00:28:05.240]   - He's like annoyed and it's almost like
[00:28:05.240 --> 00:28:07.480]   everybody else is stupid because he was obviously
[00:28:07.480 --> 00:28:08.320]   going to win with this.
[00:28:08.320 --> 00:28:10.560]   - Meant to win, if that silly thing hadn't happened.
[00:28:10.560 --> 00:28:11.480]   And it's like, you don't understand,
[00:28:11.480 --> 00:28:13.960]   the silly thing happens 99% of the time.
[00:28:13.960 --> 00:28:15.720]   And it's a 1%, not the other way around.
[00:28:15.720 --> 00:28:18.080]   But genuinely for his lived experience
[00:28:18.080 --> 00:28:20.040]   at the World Series, only at the World Series of Poker,
[00:28:20.040 --> 00:28:21.520]   it is like that.
[00:28:21.520 --> 00:28:24.000]   So I don't blame him for feeling that way.
[00:28:24.000 --> 00:28:26.480]   But he does, he has this X factor.
[00:28:26.480 --> 00:28:29.960]   And the poker community has tried for years
[00:28:29.960 --> 00:28:32.760]   to rip him down saying like, he's no good.
[00:28:32.760 --> 00:28:34.560]   But he's clearly good because he's still winning.
[00:28:34.560 --> 00:28:36.240]   Or there's something going on.
[00:28:36.240 --> 00:28:38.800]   Whether that's he's figured out how to
[00:28:38.800 --> 00:28:40.640]   mess with the fabric of reality
[00:28:40.640 --> 00:28:44.320]   and how cards, a randomly shuffled deck of cards come out.
[00:28:44.320 --> 00:28:46.680]   I don't know what it is, but he's doing it right still.
[00:28:46.680 --> 00:28:48.720]   - Who do you think is the greatest of all time?
[00:28:48.720 --> 00:28:49.840]   Would you put Helmuth?
[00:28:52.560 --> 00:28:53.400]   - It depends.
[00:28:53.400 --> 00:28:54.640]   - He seems like the kind of person
[00:28:54.640 --> 00:28:56.440]   when mentioned he would actually watch this.
[00:28:56.440 --> 00:28:58.400]   So you might want to be careful.
[00:28:58.400 --> 00:28:59.720]   - As I said, I love Phil.
[00:28:59.720 --> 00:29:03.440]   And I would say this to his face,
[00:29:03.440 --> 00:29:04.960]   I'm not saying anything, I don't.
[00:29:04.960 --> 00:29:09.520]   He's got, he truly, I mean, he is one of the greatest.
[00:29:09.520 --> 00:29:11.000]   I don't know if he's the greatest.
[00:29:11.000 --> 00:29:14.280]   He's certainly the greatest at the World Series of Poker.
[00:29:14.280 --> 00:29:16.360]   And he is the greatest at,
[00:29:16.360 --> 00:29:19.440]   despite the game switching into a pure game,
[00:29:19.440 --> 00:29:20.880]   almost an entire game of math,
[00:29:20.880 --> 00:29:22.840]   he has managed to keep the magic alive.
[00:29:22.840 --> 00:29:26.000]   And this like, just through sheer force of will,
[00:29:26.000 --> 00:29:27.600]   making the game work for him.
[00:29:27.600 --> 00:29:28.640]   And that is incredible.
[00:29:28.640 --> 00:29:30.440]   And I think it's something that should be studied
[00:29:30.440 --> 00:29:32.000]   because it's an example.
[00:29:32.000 --> 00:29:35.240]   - Yeah, there might be some actual game theoretical wisdom.
[00:29:35.240 --> 00:29:36.440]   There might be something to be said
[00:29:36.440 --> 00:29:39.200]   about optimality from studying him.
[00:29:39.200 --> 00:29:40.960]   - What do you mean by optimality?
[00:29:40.960 --> 00:29:45.520]   - Meaning, or rather game design perhaps.
[00:29:45.520 --> 00:29:48.440]   Meaning if what he's doing is working,
[00:29:48.440 --> 00:29:51.640]   maybe poker is more complicated
[00:29:51.640 --> 00:29:54.200]   than we're currently modeling it as.
[00:29:54.200 --> 00:29:55.040]   So like-
[00:29:55.040 --> 00:29:56.640]   - Or there's an extra layer,
[00:29:56.640 --> 00:29:59.480]   and I don't mean to get too weird and wooey,
[00:29:59.480 --> 00:30:04.480]   but or there's an extra layer of ability
[00:30:04.480 --> 00:30:07.520]   to manipulate the things the way you want them to go
[00:30:07.520 --> 00:30:09.760]   that we don't understand yet.
[00:30:09.760 --> 00:30:11.960]   - Do you think Phil Hellmuth understands them?
[00:30:11.960 --> 00:30:13.640]   Is he just generally-
[00:30:13.640 --> 00:30:15.480]   - Hashtag positivity.
[00:30:15.480 --> 00:30:16.840]   He wrote a book on positivity.
[00:30:17.240 --> 00:30:18.080]   - He has?
[00:30:18.080 --> 00:30:18.920]   He did?
[00:30:18.920 --> 00:30:19.760]   - Yes, "Phil Positivity."
[00:30:19.760 --> 00:30:20.600]   - Like a trolling book?
[00:30:20.600 --> 00:30:21.440]   - No.
[00:30:21.440 --> 00:30:22.280]   - A serious-
[00:30:22.280 --> 00:30:23.120]   - He's straight up, yeah.
[00:30:23.120 --> 00:30:26.080]   - Phil Hellmuth wrote a book about positivity.
[00:30:26.080 --> 00:30:27.280]   - Yes.
[00:30:27.280 --> 00:30:28.680]   - Okay, not ironically.
[00:30:28.680 --> 00:30:31.880]   - And I think it's about sort of manifesting what you want
[00:30:31.880 --> 00:30:34.200]   and getting the outcomes that you want
[00:30:34.200 --> 00:30:36.560]   by believing so much in yourself
[00:30:36.560 --> 00:30:39.120]   and in your ability to win, like eyes on the prize.
[00:30:39.120 --> 00:30:42.200]   And I mean, it's working.
[00:30:42.200 --> 00:30:43.280]   - Demands delivered.
[00:30:43.280 --> 00:30:45.400]   But where do you put like Phil Ivey
[00:30:45.400 --> 00:30:47.360]   and all those kinds of people?
[00:30:47.360 --> 00:30:50.160]   - I mean, I'm too, I've been, to be honest,
[00:30:50.160 --> 00:30:53.000]   too much out of the scene for the last few years to really,
[00:30:53.000 --> 00:30:55.040]   I mean, Phil Ivey's clearly got,
[00:30:55.040 --> 00:30:56.600]   again, he's got that X factor.
[00:30:56.600 --> 00:31:00.480]   He's so incredibly intimidating to play against.
[00:31:00.480 --> 00:31:01.800]   I've only played against him a couple of times,
[00:31:01.800 --> 00:31:03.760]   but when he like looks you in the eye
[00:31:03.760 --> 00:31:05.080]   and you're trying to run a bluff on him,
[00:31:05.080 --> 00:31:07.160]   oof, no one's made me sweat harder than Phil Ivey.
[00:31:07.160 --> 00:31:10.440]   Just, my bluff got through, actually.
[00:31:10.440 --> 00:31:11.880]   That was actually one of the most thrilling moments
[00:31:11.880 --> 00:31:12.720]   I've ever had in poker,
[00:31:12.720 --> 00:31:15.480]   was it was in a Monte Carlo in a high roller.
[00:31:15.480 --> 00:31:16.720]   I can't remember exactly what the hand was,
[00:31:16.720 --> 00:31:20.680]   but I three bit and then like just barreled
[00:31:20.680 --> 00:31:22.120]   all the way through.
[00:31:22.120 --> 00:31:24.400]   And he just like put his laser eyes into me.
[00:31:24.400 --> 00:31:28.200]   And I felt like he was just scouring my soul.
[00:31:28.200 --> 00:31:29.760]   And I was just like, hold it together, Liv,
[00:31:29.760 --> 00:31:30.600]   hold it together.
[00:31:30.600 --> 00:31:31.440]   And he was like, ah, I folded.
[00:31:31.440 --> 00:31:33.040]   - And you knew your hand was weaker.
[00:31:33.040 --> 00:31:34.400]   - Yeah, I mean, I was bluffing.
[00:31:34.400 --> 00:31:36.400]   I presume, which, you know,
[00:31:36.400 --> 00:31:37.800]   there's a chance I was bluffing with the best hand,
[00:31:37.800 --> 00:31:39.800]   but I'm pretty sure my hand was worse.
[00:31:40.720 --> 00:31:43.320]   And he folded.
[00:31:43.320 --> 00:31:47.520]   I was truly one of the deep highlights of my career.
[00:31:47.520 --> 00:31:50.040]   - Did you show the cards or did you fold?
[00:31:50.040 --> 00:31:52.640]   - You should never show in game.
[00:31:52.640 --> 00:31:54.240]   Because especially as I felt like I was one of the worst
[00:31:54.240 --> 00:31:55.720]   players at the table in that tournament.
[00:31:55.720 --> 00:31:57.880]   So giving that information,
[00:31:57.880 --> 00:32:01.160]   unless I had a really solid plan that I was now like
[00:32:01.160 --> 00:32:03.600]   advertising, oh look, I'm capable of bluffing Phil Ivey,
[00:32:03.600 --> 00:32:05.160]   but like why?
[00:32:05.160 --> 00:32:07.640]   It's much more valuable to take advantage
[00:32:07.640 --> 00:32:09.040]   of the impression that they have of me,
[00:32:09.040 --> 00:32:10.920]   which is like, I'm a scared girl playing a high roller
[00:32:10.920 --> 00:32:12.080]   for the first time.
[00:32:12.080 --> 00:32:13.440]   Keep that going, you know.
[00:32:13.440 --> 00:32:15.640]   - Interesting.
[00:32:15.640 --> 00:32:18.800]   But isn't there layers to this, like psychological warfare,
[00:32:18.800 --> 00:32:22.440]   that the scared girl might be way smart
[00:32:22.440 --> 00:32:24.720]   and then like to flip the tables.
[00:32:24.720 --> 00:32:25.920]   Do you think about that kind of stuff?
[00:32:25.920 --> 00:32:26.760]   - Oh, definitely.
[00:32:26.760 --> 00:32:28.520]   - Is it better not to reveal information?
[00:32:28.520 --> 00:32:29.520]   - I mean, generally speaking,
[00:32:29.520 --> 00:32:31.080]   you want to not reveal information.
[00:32:31.080 --> 00:32:34.680]   You know, the goal of poker is to be as deceptive as possible
[00:32:34.680 --> 00:32:38.240]   about your own strategies while elucidating as much out of
[00:32:38.240 --> 00:32:39.720]   your opponent about their own.
[00:32:39.720 --> 00:32:42.080]   So giving them free information,
[00:32:42.080 --> 00:32:43.440]   particularly if they're people who you consider
[00:32:43.440 --> 00:32:44.800]   very good players,
[00:32:44.800 --> 00:32:47.400]   any information I give them is going into their little
[00:32:47.400 --> 00:32:49.240]   database and being,
[00:32:49.240 --> 00:32:51.760]   I assume it's going to be calculated and used well.
[00:32:51.760 --> 00:32:55.800]   So I have to be really confident that my like meta gaming
[00:32:55.800 --> 00:32:56.720]   that I'm going to then do,
[00:32:56.720 --> 00:32:58.320]   oh, they've seen this, so therefore that,
[00:32:58.320 --> 00:33:00.640]   I'm going to be on the right level.
[00:33:00.640 --> 00:33:03.760]   So it's better just to keep that little secret to myself
[00:33:03.760 --> 00:33:04.600]   in the moment.
[00:33:04.600 --> 00:33:06.680]   - So how much is bluffing part of the game?
[00:33:06.680 --> 00:33:08.360]   - Huge amount.
[00:33:08.360 --> 00:33:10.440]   - So, yeah, I mean, maybe actually, let me ask,
[00:33:10.440 --> 00:33:14.080]   like, what did it feel like with Phil Ivey or anyone else
[00:33:14.080 --> 00:33:16.040]   when it's a high stake, when it's a big,
[00:33:16.040 --> 00:33:18.840]   it's a big bluff.
[00:33:18.840 --> 00:33:23.360]   So a lot of money on the table and maybe, I mean,
[00:33:23.360 --> 00:33:24.640]   what defines a big bluff?
[00:33:24.640 --> 00:33:26.000]   Maybe a lot of money on the table,
[00:33:26.000 --> 00:33:30.560]   but also some uncertainty in your mind and heart about,
[00:33:30.560 --> 00:33:31.960]   like self doubt,
[00:33:31.960 --> 00:33:34.560]   well, maybe I miscalculated what's going on here,
[00:33:34.560 --> 00:33:36.720]   what the bet said, all that kind of stuff.
[00:33:36.720 --> 00:33:38.800]   Like, what does that feel like?
[00:33:38.800 --> 00:33:43.800]   - I mean, it's, I imagine comparable to,
[00:33:43.800 --> 00:33:49.000]   you know, running a, I mean, any kind of big bluff
[00:33:49.000 --> 00:33:52.360]   where you have a lot of something
[00:33:52.360 --> 00:33:54.640]   that you care about on the line, you know?
[00:33:54.640 --> 00:33:57.200]   So if you're bluffing in a courtroom,
[00:33:57.200 --> 00:33:58.160]   not that anyone should ever do that,
[00:33:58.160 --> 00:34:00.360]   or, you know, something equatable to that,
[00:34:00.360 --> 00:34:04.120]   it's, you know, in that scenario, you know,
[00:34:04.120 --> 00:34:05.680]   I think it was the first time I'd ever played a 20,
[00:34:05.680 --> 00:34:09.160]   I'd won my way into this 25K tournament.
[00:34:09.160 --> 00:34:11.160]   So that was the buy-in, 25,000 euros.
[00:34:11.160 --> 00:34:13.000]   And I had satellited my way in
[00:34:13.000 --> 00:34:16.040]   because it was much bigger than I would ever normally play.
[00:34:16.040 --> 00:34:17.160]   And, you know, I hadn't,
[00:34:17.160 --> 00:34:18.720]   I wasn't that experienced at the time.
[00:34:18.720 --> 00:34:21.240]   And now I was sitting there against all the big boys,
[00:34:21.240 --> 00:34:23.600]   you know, the Negranus, the Phil Ives and so on.
[00:34:23.600 --> 00:34:29.720]   And then to like, each time you put the bets out,
[00:34:29.720 --> 00:34:33.000]   you know, you put another bet out, your card.
[00:34:33.000 --> 00:34:34.880]   Yeah, I was on what's called a semi-bluff.
[00:34:34.880 --> 00:34:36.280]   So there was some cards that could come
[00:34:36.280 --> 00:34:39.160]   that would make my hand very, very strong and therefore win.
[00:34:39.160 --> 00:34:40.920]   But most of the time, those cards don't come.
[00:34:40.920 --> 00:34:43.880]   - So it's a semi-bluff 'cause you're representing,
[00:34:43.880 --> 00:34:46.960]   are you representing that you already have something?
[00:34:46.960 --> 00:34:51.120]   - So I think in this scenario, I had a flush draw.
[00:34:51.120 --> 00:34:55.920]   So I had two clubs, two clubs came out on the flop,
[00:34:55.920 --> 00:34:58.080]   and then I'm hoping that on the turn in the river,
[00:34:58.080 --> 00:34:59.120]   one will come.
[00:34:59.120 --> 00:35:00.960]   So I have some future equity.
[00:35:00.960 --> 00:35:02.760]   I could hit a club and then I'll have the best hand
[00:35:02.760 --> 00:35:04.600]   in which case, great.
[00:35:04.600 --> 00:35:07.480]   And so I can keep betting and I'll want them to call,
[00:35:07.480 --> 00:35:09.640]   but I'm also got the other way of winning the hand
[00:35:09.640 --> 00:35:11.920]   where if my card doesn't come,
[00:35:11.920 --> 00:35:14.720]   I can keep betting and get them to fold their hand.
[00:35:14.720 --> 00:35:18.000]   And I'm pretty sure that's what the scenario was.
[00:35:18.000 --> 00:35:21.000]   So I had some future equity, but it's still, you know,
[00:35:21.000 --> 00:35:23.280]   most of the time I don't hit that club.
[00:35:23.280 --> 00:35:25.320]   And so I would rather him just fold
[00:35:25.320 --> 00:35:27.080]   because I'm, you know, the pot is now getting bigger
[00:35:27.080 --> 00:35:27.920]   and bigger.
[00:35:27.920 --> 00:35:31.000]   And in the end, like I've jam all in on the river,
[00:35:31.000 --> 00:35:33.720]   that's my entire tournament on the line.
[00:35:33.720 --> 00:35:34.560]   As far as I'm aware,
[00:35:34.560 --> 00:35:37.200]   this might be the one time I ever get to play a big 25K.
[00:35:37.200 --> 00:35:38.680]   You know, this was the first time I played one.
[00:35:38.680 --> 00:35:42.000]   So it was, it felt like the most momentous thing.
[00:35:42.000 --> 00:35:43.840]   And this was also when I was trying to build myself up,
[00:35:43.840 --> 00:35:46.680]   you know, build my name, a name for myself in poker.
[00:35:46.680 --> 00:35:47.720]   I wanted to get respect.
[00:35:47.720 --> 00:35:49.280]   - Destroy everything for you.
[00:35:49.280 --> 00:35:50.520]   - It felt like it in the moment.
[00:35:50.520 --> 00:35:52.480]   Like, I mean, it literally does feel like a form of life
[00:35:52.480 --> 00:35:54.360]   and death, like your body physiologically
[00:35:54.360 --> 00:35:56.120]   is having that flight or fight response.
[00:35:56.120 --> 00:35:57.120]   - What are you doing with your body?
[00:35:57.120 --> 00:35:58.320]   What are you doing with your face?
[00:35:58.320 --> 00:36:01.360]   Are you just like, what are you thinking about?
[00:36:01.360 --> 00:36:02.760]   (laughing)
[00:36:02.760 --> 00:36:04.800]   - More than a mixture of like, okay, what are the cards?
[00:36:04.800 --> 00:36:07.080]   So in theory, I'm thinking about like, okay,
[00:36:07.080 --> 00:36:10.080]   what are cards that make my hand look stronger?
[00:36:10.080 --> 00:36:13.920]   Which cards hit my perceived range from his perspective?
[00:36:13.920 --> 00:36:15.680]   Which cards don't?
[00:36:15.680 --> 00:36:18.360]   What's the right amount of bet size to, you know,
[00:36:18.360 --> 00:36:20.880]   maximize my fold equity in this situation?
[00:36:20.880 --> 00:36:22.160]   You know, that's the logical stuff
[00:36:22.160 --> 00:36:23.280]   that I should be thinking about.
[00:36:23.280 --> 00:36:25.480]   But I think in reality, because I was so scared,
[00:36:25.480 --> 00:36:26.960]   'cause there's this, at least for me,
[00:36:26.960 --> 00:36:30.480]   there's a certain threshold of like nervousness or stress
[00:36:30.480 --> 00:36:33.520]   beyond which the logical brain shuts off.
[00:36:33.520 --> 00:36:35.320]   And now it just gets into this like,
[00:36:35.320 --> 00:36:38.800]   it just like, it feels like a game of wits, basically.
[00:36:38.800 --> 00:36:41.840]   It's like of nerve, can you hold your resolve?
[00:36:41.840 --> 00:36:44.360]   And it certainly got by that, like by the river.
[00:36:44.360 --> 00:36:45.880]   I think by that point, I was like,
[00:36:45.880 --> 00:36:47.760]   I don't even know if this is a good bluff anymore,
[00:36:47.760 --> 00:36:50.120]   but fuck it, let's do it.
[00:36:50.120 --> 00:36:51.320]   - Your mind is almost numb
[00:36:51.320 --> 00:36:53.200]   from the intensity of that feeling.
[00:36:53.200 --> 00:36:55.200]   - I call it the white noise.
[00:36:55.200 --> 00:36:58.920]   And that's, and it happens in all kinds of decision-making.
[00:36:58.920 --> 00:37:00.800]   I think anything that's really, really stressful.
[00:37:00.800 --> 00:37:02.960]   I can imagine someone in like an important job interview,
[00:37:02.960 --> 00:37:04.840]   if it's like a job they've always wanted
[00:37:04.840 --> 00:37:06.560]   and they're getting grilled, you know,
[00:37:06.560 --> 00:37:09.240]   like Bridgewater style where they ask these really hard,
[00:37:09.240 --> 00:37:11.200]   like mathematical questions.
[00:37:11.200 --> 00:37:13.440]   You know, it's a really learned skill
[00:37:13.440 --> 00:37:17.840]   to be able to like subdue your flight or fight response.
[00:37:17.840 --> 00:37:19.400]   You know, I think get from the sympathetic
[00:37:19.400 --> 00:37:20.360]   into the parasympathetic.
[00:37:20.360 --> 00:37:23.880]   So you can actually, you know, engage that voice
[00:37:23.880 --> 00:37:26.080]   in your head and do those slow logical calculations.
[00:37:26.080 --> 00:37:28.360]   'Cause evolutionarily, you know,
[00:37:28.360 --> 00:37:29.680]   if we see a lion running at us,
[00:37:29.680 --> 00:37:31.680]   we didn't have time to sort of calculate
[00:37:31.680 --> 00:37:33.000]   the lion's kinetic energy.
[00:37:33.000 --> 00:37:35.480]   And, you know, is it optimal to go this way or that way?
[00:37:35.480 --> 00:37:37.080]   You just reacted.
[00:37:37.080 --> 00:37:39.880]   And physically our bodies are well attuned
[00:37:39.880 --> 00:37:41.160]   to actually make right decisions.
[00:37:41.160 --> 00:37:43.280]   But when you're playing a game like poker,
[00:37:43.280 --> 00:37:44.800]   this is not something that you ever, you know,
[00:37:44.800 --> 00:37:45.960]   evolved to do.
[00:37:45.960 --> 00:37:49.000]   And yet you're in that same flight or fight response.
[00:37:49.000 --> 00:37:51.080]   And so that's a really important skill
[00:37:51.080 --> 00:37:53.520]   to be able to develop, to basically learn how to like
[00:37:53.520 --> 00:37:55.720]   meditate in the moment and calm yourself
[00:37:55.720 --> 00:37:57.440]   so that you can think clearly.
[00:37:57.440 --> 00:38:00.800]   - But as you were searching for a comparable thing,
[00:38:00.800 --> 00:38:03.360]   it's interesting 'cause you just made me realize
[00:38:03.360 --> 00:38:06.880]   that bluffing is like an incredibly high stakes form
[00:38:06.880 --> 00:38:08.240]   of lying.
[00:38:08.240 --> 00:38:09.680]   You're lying.
[00:38:09.680 --> 00:38:11.640]   And I don't think you can--
[00:38:11.640 --> 00:38:12.480]   - Telling a story.
[00:38:12.480 --> 00:38:15.240]   - It's straight up lying.
[00:38:15.240 --> 00:38:19.080]   In the context of game, it's not a negative kind of lying.
[00:38:19.080 --> 00:38:20.400]   - But it is, yeah, exactly.
[00:38:20.400 --> 00:38:23.400]   You're representing something that you don't have.
[00:38:23.400 --> 00:38:26.800]   And I was thinking like, how often in life
[00:38:26.800 --> 00:38:30.360]   do we have such high stakes of lying?
[00:38:30.360 --> 00:38:31.520]   'Cause I was thinking,
[00:38:31.520 --> 00:38:36.120]   certainly in high level military strategy,
[00:38:36.120 --> 00:38:40.280]   I was thinking when Hitler was lying to Stalin
[00:38:40.280 --> 00:38:44.640]   about his plans to invade the Soviet Union.
[00:38:44.640 --> 00:38:48.360]   And so you're talking to a person like your friends
[00:38:48.360 --> 00:38:50.760]   and you're fighting against the enemy,
[00:38:50.760 --> 00:38:53.960]   whatever the formulation of the enemy is.
[00:38:53.960 --> 00:38:56.840]   But meanwhile, whole time you're building up troops
[00:38:56.840 --> 00:38:57.680]   on the border.
[00:38:57.680 --> 00:38:59.800]   That's extremely--
[00:38:59.800 --> 00:39:01.080]   - Wait, wait, so Hitler and Stalin
[00:39:01.080 --> 00:39:02.560]   were like pretending to be friends?
[00:39:02.560 --> 00:39:03.400]   - Yeah.
[00:39:03.400 --> 00:39:04.360]   - Well, my history knowledge is terrible.
[00:39:04.360 --> 00:39:05.280]   That's crazy.
[00:39:05.280 --> 00:39:06.640]   - Yeah, that they were...
[00:39:06.640 --> 00:39:11.520]   And it worked because Stalin,
[00:39:11.520 --> 00:39:14.320]   until the troops crossed the border
[00:39:14.320 --> 00:39:17.560]   and invaded in Operation Barbarossa
[00:39:17.560 --> 00:39:22.560]   where this storm of Nazi troops
[00:39:22.560 --> 00:39:25.600]   invaded large parts of the Soviet Union,
[00:39:25.600 --> 00:39:30.360]   hence one of the biggest wars in human history began.
[00:39:30.360 --> 00:39:34.920]   Stalin for sure thought that this was never going to be,
[00:39:34.920 --> 00:39:38.720]   that Hitler is not crazy enough to invade the Soviet Union.
[00:39:38.720 --> 00:39:41.000]   And it makes, geopolitically,
[00:39:41.000 --> 00:39:43.040]   makes total sense to be collaborators.
[00:39:43.040 --> 00:39:46.280]   And ideologically, even though there's a tension
[00:39:46.280 --> 00:39:50.320]   between communism and fascism or national socialism,
[00:39:50.320 --> 00:39:51.520]   however you formulate it,
[00:39:51.520 --> 00:39:54.360]   it still feels like this is the right way
[00:39:54.360 --> 00:39:55.800]   to battle the West.
[00:39:55.800 --> 00:39:56.640]   - Right.
[00:39:56.640 --> 00:39:58.680]   They were more ideologically aligned.
[00:39:58.680 --> 00:40:01.400]   They in theory had a common enemy, which was the West.
[00:40:01.400 --> 00:40:03.480]   - So it made total sense.
[00:40:03.480 --> 00:40:05.440]   And in terms of negotiations
[00:40:05.440 --> 00:40:07.320]   and the way things were communicated,
[00:40:07.320 --> 00:40:11.520]   it seemed to Stalin that for sure
[00:40:11.520 --> 00:40:16.120]   that they would remain at least for a while
[00:40:16.120 --> 00:40:17.680]   peaceful collaborators.
[00:40:17.680 --> 00:40:22.240]   And everybody, because of that in the Soviet Union,
[00:40:22.240 --> 00:40:25.840]   believed that it was a huge shock when Kiev was invaded.
[00:40:25.840 --> 00:40:28.240]   And you hear echoes of that when I traveled to Ukraine,
[00:40:28.240 --> 00:40:32.240]   sort of the shock of the invasion.
[00:40:32.240 --> 00:40:34.560]   It's not just the invasion on one particular border,
[00:40:34.560 --> 00:40:36.760]   but the invasion of the capital city.
[00:40:36.760 --> 00:40:39.480]   And just like, holy shit.
[00:40:39.480 --> 00:40:44.240]   Especially at that time when you thought World War I,
[00:40:44.240 --> 00:40:46.960]   you realized that that was the way to end all wars.
[00:40:46.960 --> 00:40:49.040]   You would never have this kind of war.
[00:40:49.040 --> 00:40:52.440]   And holy shit, this person is mad enough
[00:40:52.440 --> 00:40:55.520]   to try to take on this monster in the Soviet Union.
[00:40:55.520 --> 00:40:58.360]   So it's no longer going to be a war
[00:40:58.360 --> 00:40:59.880]   of hundreds of thousands dead.
[00:40:59.880 --> 00:41:02.320]   It'll be a war of tens of millions dead.
[00:41:02.320 --> 00:41:07.320]   And yeah, but that's a very large scale kind of lie,
[00:41:07.320 --> 00:41:11.480]   but I'm sure there's in politics and geopolitics,
[00:41:11.480 --> 00:41:13.960]   that kind of lying happening all the time.
[00:41:13.960 --> 00:41:17.320]   And a lot of people pay financially
[00:41:17.320 --> 00:41:19.080]   and with their lives for that kind of lying.
[00:41:19.080 --> 00:41:22.280]   But in our personal lives, I don't know how often we,
[00:41:22.280 --> 00:41:23.720]   maybe we- - I think people do.
[00:41:23.720 --> 00:41:27.400]   I mean, like think of spouses cheating on their partners.
[00:41:27.400 --> 00:41:28.440]   And then like having to lie, like,
[00:41:28.440 --> 00:41:29.560]   "Where were you last night?"
[00:41:29.560 --> 00:41:30.600]   Stuff like that. - Oh shit, that's tough.
[00:41:30.600 --> 00:41:32.920]   Yeah, that's true. - Like that's, I think,
[00:41:32.920 --> 00:41:36.240]   unfortunately that stuff happens all the time.
[00:41:36.240 --> 00:41:39.240]   - Or having like multiple families, that one is great.
[00:41:39.240 --> 00:41:42.240]   When each family doesn't know about the other one
[00:41:42.240 --> 00:41:44.640]   and like maintaining that life.
[00:41:44.640 --> 00:41:47.320]   There's probably a sense of excitement about that too.
[00:41:47.320 --> 00:41:50.720]   Or- - Seems unnecessary, yeah.
[00:41:50.720 --> 00:41:51.720]   But- - Why?
[00:41:51.720 --> 00:41:54.200]   - Well, just lying, like, you know,
[00:41:54.200 --> 00:41:56.760]   the truth finds a way of coming out, you know?
[00:41:56.760 --> 00:41:59.240]   - Yes, but hence that's the thrill.
[00:41:59.240 --> 00:42:00.200]   - Yeah, perhaps.
[00:42:00.200 --> 00:42:03.520]   Yeah, people, I mean, and that's why I think actually,
[00:42:03.520 --> 00:42:05.960]   like poker, what's so interesting about poker
[00:42:05.960 --> 00:42:09.880]   is most of the best players I know,
[00:42:09.880 --> 00:42:10.880]   they're always exceptions, you know,
[00:42:10.880 --> 00:42:12.480]   they're always bad eggs,
[00:42:12.480 --> 00:42:15.120]   but actually poker players are very honest people.
[00:42:15.120 --> 00:42:17.560]   I would say they are more honest than the average,
[00:42:17.560 --> 00:42:22.360]   you know, if you just took random population sample.
[00:42:22.360 --> 00:42:26.080]   Because A, you know, I think, you know,
[00:42:26.080 --> 00:42:27.560]   humans like to have that,
[00:42:27.560 --> 00:42:30.560]   most people like to have some kind of, you know,
[00:42:30.560 --> 00:42:32.720]   mysterious, you know, an opportunity to do something
[00:42:32.720 --> 00:42:34.880]   like a little edgy.
[00:42:34.880 --> 00:42:36.840]   So we get to sort of scratch that itch
[00:42:36.840 --> 00:42:38.360]   of being edgy at the poker table,
[00:42:38.360 --> 00:42:40.440]   where it's like, it's part of the game,
[00:42:40.440 --> 00:42:43.400]   everyone knows what they're in for, and that's allowed.
[00:42:43.400 --> 00:42:46.280]   And you get to like really get that out of your system.
[00:42:46.280 --> 00:42:51.000]   And then also like poker players learned that, you know,
[00:42:51.000 --> 00:42:55.400]   I would play in a huge game against some of my friends,
[00:42:55.400 --> 00:42:57.760]   even my partner Igor, where we will be, you know,
[00:42:57.760 --> 00:42:59.680]   absolutely going at each other's throats,
[00:42:59.680 --> 00:43:02.080]   trying to draw blood in terms of winning each money
[00:43:02.080 --> 00:43:04.040]   off each other and like getting under each other's skin,
[00:43:04.040 --> 00:43:08.360]   winding each other up, doing the craftiest moves we can.
[00:43:08.360 --> 00:43:11.120]   But then once the game's done, you know,
[00:43:11.120 --> 00:43:12.320]   the winners and the losers will go off
[00:43:12.320 --> 00:43:13.880]   and get a drink together and have a fun time
[00:43:13.880 --> 00:43:15.720]   and like talk about it in this like
[00:43:15.720 --> 00:43:17.280]   weird academic way afterwards.
[00:43:17.280 --> 00:43:19.520]   Because that, and that's why games are so great,
[00:43:19.520 --> 00:43:21.080]   'cause you get to like live out,
[00:43:21.080 --> 00:43:26.440]   like this competitive urge that, you know, most people have.
[00:43:26.440 --> 00:43:28.720]   - What's it feel like to lose?
[00:43:28.720 --> 00:43:31.400]   Like we talked about bluffing when it worked out.
[00:43:31.400 --> 00:43:34.520]   What about when you go broke?
[00:43:34.520 --> 00:43:37.280]   - So like in a game, I'm,
[00:43:37.280 --> 00:43:39.200]   fortunately I've never gone broke through poker.
[00:43:39.200 --> 00:43:40.880]   - You mean like full life?
[00:43:40.880 --> 00:43:42.480]   - Full life, no.
[00:43:42.480 --> 00:43:44.120]   I know plenty of people who have.
[00:43:44.120 --> 00:43:47.840]   And I don't think Igor would mind me saying,
[00:43:47.840 --> 00:43:50.320]   he went broke once in poker, well, you know,
[00:43:50.320 --> 00:43:51.840]   early on when we were together.
[00:43:51.840 --> 00:43:54.720]   - I feel like you haven't lived unless you've gone broke.
[00:43:54.720 --> 00:43:56.440]   - Yeah, I-- - In some sense.
[00:43:56.440 --> 00:43:58.960]   - Right, well, I mean, I'm happy,
[00:43:58.960 --> 00:44:00.880]   I've sort of lived through it vicariously through him
[00:44:00.880 --> 00:44:02.320]   when he did it at the time.
[00:44:02.320 --> 00:44:04.160]   But yeah, what's it like to lose?
[00:44:04.160 --> 00:44:05.000]   Well, it depends.
[00:44:05.000 --> 00:44:06.080]   So it depends on the amount,
[00:44:06.080 --> 00:44:07.680]   it depends what percentage of your net worth
[00:44:07.680 --> 00:44:08.640]   you've just lost.
[00:44:08.640 --> 00:44:11.560]   It depends on your brain chemistry.
[00:44:11.560 --> 00:44:13.160]   It really, you know, varies from person to person.
[00:44:13.160 --> 00:44:15.320]   - You have a very cold calculating way
[00:44:15.320 --> 00:44:16.920]   of thinking about this.
[00:44:16.920 --> 00:44:19.000]   So it depends what percentage. (laughs)
[00:44:19.000 --> 00:44:20.720]   - Well, it really does, right?
[00:44:20.720 --> 00:44:21.560]   - Yeah, it's true, it's true.
[00:44:21.560 --> 00:44:23.080]   - But that's, I mean,
[00:44:23.080 --> 00:44:24.760]   that's another thing poker trains you to do.
[00:44:24.760 --> 00:44:28.120]   You see everything in percentages.
[00:44:28.120 --> 00:44:30.840]   Or you see everything in like ROI or expected hourlies
[00:44:30.840 --> 00:44:32.440]   or cost benefit, et cetera.
[00:44:32.440 --> 00:44:36.080]   You know, so that's,
[00:44:36.080 --> 00:44:37.240]   one of the things I've tried to do
[00:44:37.240 --> 00:44:39.840]   is calibrate the strength of my emotional response
[00:44:39.840 --> 00:44:43.400]   to the win or loss that I've received.
[00:44:43.400 --> 00:44:45.560]   Because it's no good if you like, you know,
[00:44:45.560 --> 00:44:49.480]   you have a huge emotional dramatic response to a tiny loss.
[00:44:49.480 --> 00:44:53.240]   Or on the flip side, you have a huge win
[00:44:53.240 --> 00:44:55.480]   and you're so dead inside that you don't even feel it.
[00:44:55.480 --> 00:44:56.840]   Well, that's, you know, that's a shame.
[00:44:56.840 --> 00:45:00.280]   I want my emotions to calibrate with reality
[00:45:00.280 --> 00:45:01.480]   as much as possible.
[00:45:01.480 --> 00:45:04.240]   So yeah, what's it like to lose?
[00:45:04.240 --> 00:45:07.200]   I mean, I've had times where I've lost, you know,
[00:45:07.200 --> 00:45:08.120]   busted out of a tournament
[00:45:08.120 --> 00:45:09.040]   that I thought I was gonna win in.
[00:45:09.040 --> 00:45:10.560]   It's, you know, especially if I got really unlucky
[00:45:10.560 --> 00:45:13.960]   or I make a dumb play,
[00:45:13.960 --> 00:45:17.560]   where I've gone away and like, you know, kicked the wall,
[00:45:17.560 --> 00:45:19.920]   punched a wall, I like nearly broke my hand one time.
[00:45:19.920 --> 00:45:24.120]   Like I'm a lot less competitive than I used to be.
[00:45:24.120 --> 00:45:26.640]   Like I was pathologically competitive
[00:45:26.640 --> 00:45:28.760]   in my like late teens, early twenties.
[00:45:28.760 --> 00:45:30.880]   I just had to win at everything.
[00:45:30.880 --> 00:45:33.520]   And I think that sort of slowly waned as I've gotten older.
[00:45:33.520 --> 00:45:34.880]   - According to you, yeah.
[00:45:34.880 --> 00:45:35.720]   - According to me.
[00:45:35.720 --> 00:45:38.480]   - I don't know if others would say the same, right?
[00:45:38.480 --> 00:45:40.880]   I feel like ultra competitive people,
[00:45:40.880 --> 00:45:43.000]   like I've heard Joe Rogan say this to me.
[00:45:43.000 --> 00:45:45.440]   It's like, he's a lot less competitive than he used to be.
[00:45:45.440 --> 00:45:47.360]   I don't know about that.
[00:45:47.360 --> 00:45:48.440]   - Oh, I believe it.
[00:45:48.440 --> 00:45:50.080]   No, I totally believe it.
[00:45:50.080 --> 00:45:51.440]   Because as you get, you can still be,
[00:45:51.440 --> 00:45:53.000]   like I care about winning.
[00:45:53.000 --> 00:45:56.000]   Like when, you know, I play a game with my buddies online
[00:45:56.000 --> 00:45:57.400]   or, you know, whatever it is,
[00:45:57.400 --> 00:45:58.760]   polytopia is my current obsession.
[00:45:58.760 --> 00:46:00.800]   Like when I-
[00:46:00.800 --> 00:46:02.960]   - Thank you for passing on your obsession to me.
[00:46:02.960 --> 00:46:03.800]   - Are you playing now?
[00:46:03.800 --> 00:46:04.720]   - Yeah, I'm playing now.
[00:46:04.720 --> 00:46:05.840]   - We gotta have a game.
[00:46:05.840 --> 00:46:08.080]   - But I'm terrible and I enjoy playing terribly.
[00:46:08.080 --> 00:46:09.080]   I don't wanna have a game
[00:46:09.080 --> 00:46:11.400]   because that's gonna pull me into your monster
[00:46:11.400 --> 00:46:13.880]   of like a competitive play.
[00:46:13.880 --> 00:46:14.720]   - It's important.
[00:46:14.720 --> 00:46:15.560]   It's an important skill.
[00:46:15.560 --> 00:46:18.280]   - I'm enjoy playing on the, I can't.
[00:46:18.280 --> 00:46:20.920]   - You just do the points thing, you know, against the bots.
[00:46:20.920 --> 00:46:21.960]   - Yeah, against the bots.
[00:46:21.960 --> 00:46:23.520]   And I can't even do the,
[00:46:23.520 --> 00:46:26.920]   there's like a hard one and there's a very hard one.
[00:46:26.920 --> 00:46:27.760]   - That's crazy, yeah.
[00:46:27.760 --> 00:46:28.600]   - That's crazy.
[00:46:28.600 --> 00:46:29.920]   I can't, I don't even enjoy the hard one.
[00:46:29.920 --> 00:46:32.040]   The crazy I really don't enjoy.
[00:46:32.040 --> 00:46:33.680]   'Cause it's intense.
[00:46:33.680 --> 00:46:35.000]   You have to constantly try to win
[00:46:35.000 --> 00:46:37.960]   as opposed to enjoy building a little world and-
[00:46:37.960 --> 00:46:39.520]   - Yeah, no, no, there's no time for exploration
[00:46:39.520 --> 00:46:40.720]   in polytopia, you gotta get,
[00:46:40.720 --> 00:46:42.760]   well, once you graduate from the crazies,
[00:46:42.760 --> 00:46:44.560]   then you can come play the-
[00:46:44.560 --> 00:46:46.440]   - Graduate from the crazies.
[00:46:46.440 --> 00:46:48.640]   - Yeah, so in order to be able to play a decent game
[00:46:48.640 --> 00:46:50.920]   against like, you know, our group,
[00:46:50.920 --> 00:46:55.640]   you'll need to be consistently winning
[00:46:55.640 --> 00:46:58.280]   like 90% of games against 15 crazy bots.
[00:46:58.280 --> 00:46:59.160]   - Yeah.
[00:46:59.160 --> 00:47:00.440]   - And you'll be able to, like,
[00:47:00.440 --> 00:47:03.360]   there'll be, I could teach you it within a day, honestly.
[00:47:03.360 --> 00:47:04.640]   - How to beat the crazies?
[00:47:04.640 --> 00:47:05.720]   - How to beat the crazies.
[00:47:05.720 --> 00:47:08.120]   And then you'll be ready for the big leagues.
[00:47:08.120 --> 00:47:12.400]   - Generalizes to more than just polytopia, but okay.
[00:47:12.400 --> 00:47:14.360]   Why were we talking about polytopia?
[00:47:14.360 --> 00:47:15.960]   Losing hurts.
[00:47:15.960 --> 00:47:19.360]   - Losing hurts, oh yeah, yes, competitiveness over time.
[00:47:19.360 --> 00:47:20.360]   - Oh yeah.
[00:47:20.360 --> 00:47:23.360]   - I think it's more that, at least for me,
[00:47:23.360 --> 00:47:24.720]   I still care about playing,
[00:47:24.720 --> 00:47:26.480]   about winning when I choose to play something.
[00:47:26.480 --> 00:47:28.680]   It's just that I don't see the world
[00:47:28.680 --> 00:47:32.080]   as zero sum as I used to, you know?
[00:47:32.080 --> 00:47:34.960]   I think as one gets older and wiser,
[00:47:34.960 --> 00:47:38.040]   you start to see the world more as a positive something,
[00:47:38.040 --> 00:47:40.800]   or at least you're more aware of externalities
[00:47:40.800 --> 00:47:43.760]   of scenarios, of competitive interactions.
[00:47:43.760 --> 00:47:47.480]   And so, yeah, I just, like, I'm more,
[00:47:47.480 --> 00:47:50.160]   and I'm more aware of my own, you know, like,
[00:47:50.160 --> 00:47:52.760]   if I have a really strong emotional response to losing,
[00:47:52.760 --> 00:47:55.000]   and that makes me then feel shitty for the rest of the day,
[00:47:55.000 --> 00:47:57.200]   and then I beat myself up mentally for it,
[00:47:57.200 --> 00:47:58.640]   like, I'm now more aware
[00:47:58.640 --> 00:48:01.680]   that that's unnecessary negative externality.
[00:48:01.680 --> 00:48:03.920]   So I'm like, okay, I need to find a way to turn this down,
[00:48:03.920 --> 00:48:05.280]   you know, dial this down a bit.
[00:48:05.280 --> 00:48:07.560]   - Was poker the thing that has,
[00:48:07.560 --> 00:48:09.680]   if you think back at your life
[00:48:09.680 --> 00:48:12.160]   and think about some of the lower points of your life,
[00:48:12.160 --> 00:48:15.000]   like the darker places you've got in your mind,
[00:48:15.000 --> 00:48:17.120]   did it have to do something with poker?
[00:48:17.120 --> 00:48:22.120]   Like, did losing spark the descent into a new thing?
[00:48:22.120 --> 00:48:24.760]   The descent into darkness, or was it something else?
[00:48:24.760 --> 00:48:29.480]   - I think my darkest points in poker
[00:48:29.480 --> 00:48:34.480]   were when I was wanting to quit and move on to other things,
[00:48:34.480 --> 00:48:38.360]   but I felt like I hadn't ticked all the boxes
[00:48:38.360 --> 00:48:40.000]   I wanted to tick.
[00:48:40.000 --> 00:48:43.760]   Like, I wanted to be the most winningest female player,
[00:48:43.760 --> 00:48:45.520]   which is by itself a bad goal.
[00:48:45.520 --> 00:48:48.000]   You know, that was one of my initial goals,
[00:48:48.000 --> 00:48:48.840]   and I was like, well, I haven't, you know,
[00:48:48.840 --> 00:48:50.960]   and I wanted to win a WPT event.
[00:48:50.960 --> 00:48:52.160]   I've won one of these, I've won one of these,
[00:48:52.160 --> 00:48:53.600]   but I want one of those as well.
[00:48:53.600 --> 00:48:57.440]   And that sort of, again, like,
[00:48:57.440 --> 00:49:00.120]   is a drive of like over-optimization to random metrics
[00:49:00.120 --> 00:49:01.600]   that I decided were important
[00:49:01.600 --> 00:49:05.240]   without much wisdom at the time, but then I carried on.
[00:49:05.240 --> 00:49:09.600]   That made me continue chasing it longer
[00:49:09.600 --> 00:49:12.920]   than I still actually had the passion to chase it for.
[00:49:12.920 --> 00:49:15.760]   And I don't have any regrets that, you know,
[00:49:15.760 --> 00:49:18.200]   I played for as long as I did, because who knows, you know,
[00:49:18.200 --> 00:49:19.280]   I wouldn't be sitting here,
[00:49:19.280 --> 00:49:21.040]   I wouldn't be living this incredible life
[00:49:21.040 --> 00:49:22.680]   that I'm living now.
[00:49:22.680 --> 00:49:24.920]   - This is the height of your life right now.
[00:49:24.920 --> 00:49:28.120]   - This is it, peak experience, absolute pinnacle
[00:49:28.120 --> 00:49:31.360]   here in your robot land.
[00:49:31.360 --> 00:49:32.200]   - Yeah, yeah.
[00:49:32.200 --> 00:49:33.600]   - With your creepy light.
[00:49:33.600 --> 00:49:37.360]   No, it is, I mean, I wouldn't change a thing
[00:49:37.360 --> 00:49:38.280]   about my life right now,
[00:49:38.280 --> 00:49:40.080]   and I feel very blessed to say that.
[00:49:40.080 --> 00:49:46.720]   But the dark times were in the sort of like 2016 to 18,
[00:49:48.040 --> 00:49:50.360]   even sooner really, where I was like,
[00:49:50.360 --> 00:49:53.160]   I'd stopped loving the game
[00:49:53.160 --> 00:49:55.480]   and I was going through the motions.
[00:49:55.480 --> 00:49:59.720]   And I would, and then I was like, you know,
[00:49:59.720 --> 00:50:02.240]   I would take the losses harder than I needed to,
[00:50:02.240 --> 00:50:03.400]   'cause I'm like, "Oh, it's another one."
[00:50:03.400 --> 00:50:04.880]   And it was, I was aware that like,
[00:50:04.880 --> 00:50:06.480]   I felt like my life was ticking away and I was like,
[00:50:06.480 --> 00:50:07.960]   is this gonna be what's on my tombstone?
[00:50:07.960 --> 00:50:09.600]   Oh yeah, she played the game of, you know,
[00:50:09.600 --> 00:50:11.720]   this zero-sum game of poker,
[00:50:11.720 --> 00:50:14.120]   slightly more optimally than her next opponent.
[00:50:14.120 --> 00:50:16.440]   Like, cool, great, legacy, you know?
[00:50:16.440 --> 00:50:19.040]   So I just wanted, you know,
[00:50:19.040 --> 00:50:20.240]   there was something in me that knew I needed
[00:50:20.240 --> 00:50:24.120]   to be doing something more directly impactful
[00:50:24.120 --> 00:50:25.280]   and just meaningful.
[00:50:25.280 --> 00:50:26.280]   It was just like a search for meaning.
[00:50:26.280 --> 00:50:27.960]   And I think it's a thing a lot of poker players,
[00:50:27.960 --> 00:50:30.960]   even a lot of, I imagine any games players
[00:50:30.960 --> 00:50:34.520]   who sort of love intellectual pursuits,
[00:50:34.520 --> 00:50:37.000]   you know, I think you should ask
[00:50:37.000 --> 00:50:38.120]   Magnus Carlsen this question.
[00:50:38.120 --> 00:50:39.800]   - Yeah, walking away from chess, right?
[00:50:39.800 --> 00:50:41.520]   - Yeah, like it must be so hard for him.
[00:50:41.520 --> 00:50:43.760]   You know, he's been on the top for so long
[00:50:43.760 --> 00:50:45.280]   and it's like, well, now what?
[00:50:45.280 --> 00:50:48.680]   He's got this incredible brain, like what to put it to?
[00:50:48.680 --> 00:50:52.120]   And yeah, it's-
[00:50:52.120 --> 00:50:55.240]   - It's this weird moment where I've just spoken
[00:50:55.240 --> 00:50:58.360]   with people that won multiple gold medals at the Olympics
[00:50:58.360 --> 00:51:01.540]   and the depression hits hard after you win.
[00:51:01.540 --> 00:51:04.160]   - Dopamine crash.
[00:51:04.160 --> 00:51:05.600]   - 'Cause it's a kind of a goodbye,
[00:51:05.600 --> 00:51:06.920]   saying goodbye to that person,
[00:51:06.920 --> 00:51:07.960]   to all the dreams you had,
[00:51:07.960 --> 00:51:11.880]   the thought you thought would give meaning to your life.
[00:51:11.880 --> 00:51:16.880]   But in fact, life is full of constant pursuits of meaning.
[00:51:16.880 --> 00:51:20.240]   You don't like arrive and figure it all out
[00:51:20.240 --> 00:51:23.980]   and there's endless bliss, no, it continues going on and on.
[00:51:23.980 --> 00:51:27.840]   You constantly have to figure out to rediscover yourself.
[00:51:27.840 --> 00:51:31.600]   And so for you, like that struggle to say goodbye to poker,
[00:51:31.600 --> 00:51:33.800]   you have to like find the next-
[00:51:33.800 --> 00:51:35.640]   - There's always a bigger game, that's the thing.
[00:51:35.640 --> 00:51:38.440]   That's my like motto, it's like, what's the next game?
[00:51:38.440 --> 00:51:41.200]   And more importantly,
[00:51:41.200 --> 00:51:43.120]   because obviously game usually implies zero sum,
[00:51:43.120 --> 00:51:46.560]   like what's the game which is like omni-win?
[00:51:46.560 --> 00:51:47.640]   Look, what- - Omni-win?
[00:51:47.640 --> 00:51:48.920]   - Omni-win. - Why is omni-win
[00:51:48.920 --> 00:51:50.720]   is so important?
[00:51:50.720 --> 00:51:54.680]   - Because if everyone plays zero sum games,
[00:51:54.680 --> 00:51:57.160]   that's a fast track to either completely stagnate
[00:51:57.160 --> 00:51:59.200]   as a civilization, but more actually,
[00:51:59.200 --> 00:52:01.320]   far more likely to extinct ourselves.
[00:52:01.320 --> 00:52:04.480]   You know, like the playing field is finite.
[00:52:07.440 --> 00:52:10.680]   Nuclear powers are playing a game of poker
[00:52:10.680 --> 00:52:14.560]   with their chips of nuclear weapons, right?
[00:52:14.560 --> 00:52:17.080]   And the stakes have gotten so large
[00:52:17.080 --> 00:52:20.680]   that if anyone makes a single bet, fires some weapons,
[00:52:20.680 --> 00:52:22.160]   the playing field breaks.
[00:52:22.160 --> 00:52:23.320]   I made a video on this.
[00:52:23.320 --> 00:52:26.960]   The playing field is finite.
[00:52:26.960 --> 00:52:31.600]   And if we keep playing these adversarial zero sum games,
[00:52:31.600 --> 00:52:34.080]   thinking that in order for us to win,
[00:52:34.080 --> 00:52:35.480]   someone else has to lose,
[00:52:35.480 --> 00:52:37.760]   or if we lose that someone else wins,
[00:52:37.760 --> 00:52:40.000]   that will extinct us.
[00:52:40.000 --> 00:52:41.120]   It's just a matter of when.
[00:52:41.120 --> 00:52:44.840]   - What do you think about that mutually assured destruction,
[00:52:44.840 --> 00:52:46.360]   that very simple,
[00:52:46.360 --> 00:52:50.000]   almost to the point of caricaturing game theory idea
[00:52:50.000 --> 00:52:52.360]   that does seem to be at the core
[00:52:52.360 --> 00:52:54.040]   of why we haven't blown each other up yet
[00:52:54.040 --> 00:52:55.680]   with nuclear weapons.
[00:52:55.680 --> 00:52:57.560]   Do you think there's some truth to that,
[00:52:57.560 --> 00:53:00.200]   this kind of stabilizing force
[00:53:00.200 --> 00:53:01.480]   of mutually assured destruction?
[00:53:01.480 --> 00:53:04.960]   And do you think that's gonna hold up
[00:53:04.960 --> 00:53:07.360]   through the 21st century?
[00:53:07.360 --> 00:53:10.400]   - I mean, it has held, yes.
[00:53:10.400 --> 00:53:11.720]   There's definitely truth to it,
[00:53:11.720 --> 00:53:14.800]   that it was a, you know, it's a Nash equilibrium.
[00:53:14.800 --> 00:53:17.320]   - Yeah, are you surprised it held this long?
[00:53:17.320 --> 00:53:18.480]   Isn't it crazy?
[00:53:18.480 --> 00:53:20.200]   - It is crazy when you factor in
[00:53:20.200 --> 00:53:24.680]   all the like near miss accidental firings.
[00:53:24.680 --> 00:53:28.160]   Yes, that's makes me wonder, like, you know,
[00:53:28.160 --> 00:53:29.000]   are you familiar with the like
[00:53:29.000 --> 00:53:31.160]   quantum suicide thought experiment?
[00:53:31.160 --> 00:53:34.960]   Where it's basically like, you have a, you know,
[00:53:34.960 --> 00:53:37.560]   like a Russian roulette type scenario
[00:53:37.560 --> 00:53:40.640]   hooked up to some kind of quantum event,
[00:53:40.640 --> 00:53:45.640]   you know, particle splitting or pair particle splitting.
[00:53:45.640 --> 00:53:48.440]   And if it, you know, if it goes A,
[00:53:48.440 --> 00:53:50.280]   then the gun doesn't go off and it goes B,
[00:53:50.280 --> 00:53:52.480]   then it does go off and it kills you.
[00:53:52.480 --> 00:53:55.040]   Because you can only ever be in the universe,
[00:53:55.040 --> 00:53:56.560]   you know, assuming like the Everett branch,
[00:53:56.560 --> 00:53:57.800]   you know, multiverse theory,
[00:53:57.800 --> 00:54:00.240]   you will always only end up in the branch
[00:54:00.240 --> 00:54:03.320]   where you continually make, you know, option A comes in.
[00:54:03.320 --> 00:54:05.440]   But you run that experiment enough times,
[00:54:05.440 --> 00:54:07.080]   it starts getting pretty damn, you know,
[00:54:07.080 --> 00:54:09.360]   out of the tree gets huge.
[00:54:09.360 --> 00:54:10.920]   There's a million different scenarios,
[00:54:10.920 --> 00:54:12.440]   but you'll always find yourself in this,
[00:54:12.440 --> 00:54:14.840]   in the one where it didn't go off.
[00:54:14.840 --> 00:54:18.640]   And so from that perspective,
[00:54:18.640 --> 00:54:20.800]   you are essentially immortal.
[00:54:20.800 --> 00:54:22.600]   'Cause someone, and you will only find yourself
[00:54:22.600 --> 00:54:24.960]   in the set of observers that make it down that path.
[00:54:24.960 --> 00:54:26.640]   - Yeah. - So it's kind of a--
[00:54:26.640 --> 00:54:29.200]   - That doesn't mean, that doesn't--
[00:54:29.200 --> 00:54:32.200]   - That doesn't mean you're still not gonna be fucked
[00:54:32.200 --> 00:54:33.040]   at some point in your life.
[00:54:33.040 --> 00:54:33.880]   - No, of course not.
[00:54:33.880 --> 00:54:36.040]   No, I'm not advocating like that we're all immortal
[00:54:36.040 --> 00:54:36.880]   because of this.
[00:54:36.880 --> 00:54:38.520]   It's just like a fun thought experiment.
[00:54:38.520 --> 00:54:40.120]   And the point is it like raises this thing
[00:54:40.120 --> 00:54:42.440]   of like these things called observer selection effects,
[00:54:42.440 --> 00:54:44.480]   which Bostrom, Nick Bostrom talks about a lot.
[00:54:44.480 --> 00:54:46.240]   And I think people should go read.
[00:54:46.240 --> 00:54:47.080]   - It's really powerful,
[00:54:47.080 --> 00:54:48.920]   but I think it could be overextended that logic.
[00:54:48.920 --> 00:54:52.280]   I'm not sure exactly how it can be.
[00:54:52.280 --> 00:54:55.160]   I just feel like you can get,
[00:54:55.160 --> 00:54:58.360]   you can overgeneralize that logic somehow.
[00:54:58.360 --> 00:55:00.680]   - Well, no, I mean, it leads you into like solipsism,
[00:55:00.680 --> 00:55:02.160]   which is a very dangerous mindset.
[00:55:02.160 --> 00:55:04.400]   Again, if everyone like falls into solipsism of like,
[00:55:04.400 --> 00:55:05.760]   well, I'll be fine.
[00:55:05.760 --> 00:55:08.880]   That's a great way of creating a very,
[00:55:08.880 --> 00:55:10.880]   self-terminating environment.
[00:55:10.880 --> 00:55:14.600]   But my point is, is that with the nuclear weapons thing,
[00:55:14.600 --> 00:55:17.160]   there have been at least, I think it's 12 or 11
[00:55:17.160 --> 00:55:21.080]   near misses of like just stupid things.
[00:55:21.080 --> 00:55:24.200]   Like there was moonrise over Norway
[00:55:24.200 --> 00:55:26.880]   and it made weird reflections of some glaciers
[00:55:26.880 --> 00:55:28.840]   in the mountains, which set off,
[00:55:28.840 --> 00:55:33.840]   I think the alarms of NORAD radar.
[00:55:33.840 --> 00:55:35.720]   And that put them on high alert, nearly ready to shoot.
[00:55:35.720 --> 00:55:39.600]   And it was only because the head of Russian military
[00:55:39.600 --> 00:55:42.000]   happened to be at the UN in New York at the time
[00:55:42.000 --> 00:55:43.360]   that they go like, well, wait a second,
[00:55:43.360 --> 00:55:47.200]   why would they fire now when their guy is there?
[00:55:47.200 --> 00:55:49.280]   And it was only that lucky happenstance,
[00:55:49.280 --> 00:55:50.240]   which doesn't happen very often
[00:55:50.240 --> 00:55:52.000]   where they didn't then escalate it into firing.
[00:55:52.000 --> 00:55:53.880]   And there's a bunch of these different ones.
[00:55:53.880 --> 00:55:55.960]   Stanislav Petrov, like saved,
[00:55:55.960 --> 00:55:57.920]   the person who should be the most famous person on earth,
[00:55:57.920 --> 00:55:59.560]   'cause he's probably on expectation
[00:55:59.560 --> 00:56:01.160]   saved the most human lives of anyone,
[00:56:01.160 --> 00:56:05.280]   like billions of people by ignoring Russian orders to fire
[00:56:05.280 --> 00:56:06.280]   because he felt in his gut
[00:56:06.280 --> 00:56:07.440]   that actually this was a false alarm.
[00:56:07.440 --> 00:56:11.040]   And it turned out to be, you know, very hard thing to do.
[00:56:11.040 --> 00:56:12.560]   And there's so many of those scenarios
[00:56:12.560 --> 00:56:14.520]   that I can't help but wonder at this point
[00:56:14.520 --> 00:56:16.840]   that we aren't having this kind of like selection effect
[00:56:16.840 --> 00:56:17.920]   thing going on.
[00:56:17.920 --> 00:56:18.800]   'Cause you look back and you're like,
[00:56:18.800 --> 00:56:20.800]   geez, that's a lot of near misses.
[00:56:20.800 --> 00:56:22.880]   But of course we don't know the actual probabilities
[00:56:22.880 --> 00:56:23.720]   that they would have lent,
[00:56:23.720 --> 00:56:24.840]   each one would have ended up in nuclear war.
[00:56:24.840 --> 00:56:26.720]   Maybe they were not that likely, but still.
[00:56:26.720 --> 00:56:29.440]   The point is, it's a very dark, stupid game
[00:56:29.440 --> 00:56:30.880]   that we're playing.
[00:56:30.880 --> 00:56:35.320]   And it is an absolute moral imperative, if you ask me,
[00:56:35.320 --> 00:56:37.240]   to get as many people thinking about ways
[00:56:37.240 --> 00:56:39.560]   to make this like very precarious.
[00:56:39.560 --> 00:56:41.160]   'Cause we're in a Nash equilibrium,
[00:56:41.160 --> 00:56:42.840]   but it's not like we're in the bottom of a pit.
[00:56:42.840 --> 00:56:46.400]   You know, if you would like map it topographically,
[00:56:46.400 --> 00:56:48.360]   it's not like a stable ball at the bottom of a thing.
[00:56:48.360 --> 00:56:49.400]   We're not in equilibrium because of that.
[00:56:49.400 --> 00:56:52.360]   We're on the top of a hill with a ball balanced on top.
[00:56:52.360 --> 00:56:55.560]   And just at any little nudge could send it flying down
[00:56:55.560 --> 00:56:58.880]   and nuclear war pops off and hellfire and bad times.
[00:56:58.880 --> 00:57:01.080]   - On the positive side,
[00:57:01.080 --> 00:57:03.200]   life on earth will probably still continue.
[00:57:03.200 --> 00:57:06.160]   And another intelligent civilization might still pop up.
[00:57:06.160 --> 00:57:07.000]   - Maybe.
[00:57:07.000 --> 00:57:08.440]   - Several millennia after.
[00:57:08.440 --> 00:57:10.160]   - Pick your X risk, depends on the X risk.
[00:57:10.160 --> 00:57:13.080]   Nuclear war, sure, that's one of the perhaps less bad ones.
[00:57:13.080 --> 00:57:17.560]   Green goo through synthetic biology, very bad.
[00:57:17.560 --> 00:57:22.560]   Will turn, you know, destroy all organic matter through,
[00:57:22.560 --> 00:57:27.080]   you know, it's basically like a biological
[00:57:27.080 --> 00:57:28.880]   paperclip maximizer, also bad.
[00:57:28.880 --> 00:57:32.480]   Or AI type, you know, mass extinction thing as well
[00:57:32.480 --> 00:57:33.320]   would also be bad.
[00:57:33.320 --> 00:57:35.000]   - Shh, they're listening.
[00:57:35.000 --> 00:57:36.440]   There's a robot right behind you.
[00:57:36.440 --> 00:57:38.680]   Okay, wait, so let me ask you about this
[00:57:38.680 --> 00:57:40.360]   from a game theory perspective.
[00:57:40.360 --> 00:57:42.280]   Do you think we're living in a simulation?
[00:57:42.280 --> 00:57:44.640]   Do you think we're living inside a video game
[00:57:44.640 --> 00:57:45.920]   created by somebody else?
[00:57:45.920 --> 00:57:47.400]   (laughing)
[00:57:47.400 --> 00:57:49.840]   - Well, I think, well, so what was the second part
[00:57:49.840 --> 00:57:50.680]   of the question?
[00:57:50.680 --> 00:57:52.600]   Do I think we're living in a simulation and?
[00:57:52.600 --> 00:57:56.640]   - A simulation that is observed by somebody
[00:57:56.640 --> 00:57:58.600]   for purpose of entertainment.
[00:57:58.600 --> 00:57:59.640]   So like a video game.
[00:57:59.640 --> 00:58:00.480]   Are we listening?
[00:58:00.480 --> 00:58:03.120]   Are we, because there's a,
[00:58:03.120 --> 00:58:05.240]   it's like Phil Hellmuth type of situation, right?
[00:58:05.240 --> 00:58:09.920]   Like there's a creepy level of like,
[00:58:09.920 --> 00:58:12.640]   this is kind of fun and interesting.
[00:58:12.640 --> 00:58:16.920]   Like there's a lot of interesting stuff going on.
[00:58:16.920 --> 00:58:18.760]   I mean, that could be somehow integrated
[00:58:18.760 --> 00:58:20.680]   into the evolutionary process where,
[00:58:20.680 --> 00:58:24.120]   the way we perceive and--
[00:58:24.120 --> 00:58:26.000]   - Are you asking me if I believe in God?
[00:58:26.000 --> 00:58:28.800]   Sounds like it.
[00:58:28.800 --> 00:58:33.800]   - Kind of, but God seems to be not optimizing
[00:58:33.800 --> 00:58:37.400]   in the different formulations of God that we conceive of.
[00:58:37.400 --> 00:58:39.960]   He doesn't seem to be or she optimizing
[00:58:39.960 --> 00:58:42.700]   for like personal entertainment.
[00:58:42.700 --> 00:58:45.840]   Or maybe the older gods did,
[00:58:45.840 --> 00:58:49.800]   but the, you know, just like the basically like a teenager
[00:58:49.800 --> 00:58:54.200]   in their mom's basement watching create a fun universe
[00:58:54.200 --> 00:58:58.520]   to observe what kind of crazy shit might happen.
[00:58:58.520 --> 00:59:00.080]   - Okay, so to try and ask this,
[00:59:00.080 --> 00:59:07.320]   do I think there is some kind of extraneous intelligence
[00:59:11.280 --> 00:59:16.200]   to like our classic measurable universe
[00:59:16.200 --> 00:59:18.240]   that we can measure with,
[00:59:18.240 --> 00:59:22.920]   through our current physics and instruments?
[00:59:22.920 --> 00:59:25.040]   I think so, yes.
[00:59:25.040 --> 00:59:30.760]   Partly because I've had just small little bits of evidence
[00:59:30.760 --> 00:59:34.040]   in my own life, which have made me question,
[00:59:34.040 --> 00:59:38.160]   like, so I was a diehard atheist even five years ago.
[00:59:39.400 --> 00:59:41.680]   You know, I got into like the rationality community,
[00:59:41.680 --> 00:59:45.680]   big fan of less wrong, continue to be incredible resource.
[00:59:45.680 --> 00:59:51.320]   But I've just started to have too many little snippets
[00:59:51.320 --> 00:59:58.040]   of experience, which don't make sense
[00:59:58.040 --> 01:00:01.600]   with the current sort of purely materialistic explanation
[01:00:01.600 --> 01:00:07.680]   of how reality works.
[01:00:08.680 --> 01:00:13.680]   - Isn't that just like a humbling practical realization
[01:00:13.680 --> 01:00:17.400]   that we don't know how reality works?
[01:00:17.400 --> 01:00:19.360]   Isn't that just a reminder to yourself?
[01:00:19.360 --> 01:00:21.520]   - Yeah, no, it's a reminder of epistemic humility
[01:00:21.520 --> 01:00:24.360]   because I fell too hard, you know, same as people,
[01:00:24.360 --> 01:00:26.440]   like I think, you know, many people who are just like,
[01:00:26.440 --> 01:00:28.280]   my religion is the way, this is the correct way,
[01:00:28.280 --> 01:00:31.200]   this is the law, you are immoral
[01:00:31.200 --> 01:00:32.600]   if you don't follow this, blah, blah, blah.
[01:00:32.600 --> 01:00:35.080]   I think they are lacking epistemic humility.
[01:00:35.080 --> 01:00:37.040]   They're a little too much hubris there.
[01:00:37.040 --> 01:00:39.720]   But similarly, I think the sort of the Richard Dawkins brand
[01:00:39.720 --> 01:00:44.720]   of atheism is too rigid as well and doesn't,
[01:00:44.720 --> 01:00:50.960]   you know, there's a way to try and navigate these questions
[01:00:50.960 --> 01:00:52.720]   which still honors the scientific method,
[01:00:52.720 --> 01:00:54.760]   which I still think is our best sort of realm
[01:00:54.760 --> 01:00:57.600]   of like reasonable inquiry, you know, a method of inquiry.
[01:00:57.600 --> 01:01:03.280]   So an example, I have two kind of notable examples
[01:01:03.280 --> 01:01:06.640]   that like really rattled my cage.
[01:01:06.640 --> 01:01:09.800]   The first one was actually in 2010, early on in,
[01:01:09.800 --> 01:01:13.480]   quite early on in my poker career.
[01:01:13.480 --> 01:01:18.480]   And I, the, I, the, remember the Icelandic volcano
[01:01:18.480 --> 01:01:20.520]   that erupted that like shut down
[01:01:20.520 --> 01:01:22.760]   kind of all Atlantic airspace?
[01:01:22.760 --> 01:01:25.280]   And I, it meant I got stuck down in the South of France.
[01:01:25.280 --> 01:01:27.160]   I was there for something else.
[01:01:27.160 --> 01:01:29.920]   And I couldn't get home and someone said,
[01:01:29.920 --> 01:01:31.880]   well, there's a big poker tournament happening in Italy.
[01:01:31.880 --> 01:01:33.040]   Maybe do you wanna go?
[01:01:33.040 --> 01:01:33.880]   I was like, oh, right, sure.
[01:01:33.880 --> 01:01:35.880]   Like let's, you know, got a train across,
[01:01:35.880 --> 01:01:37.080]   found a way to get there.
[01:01:37.080 --> 01:01:39.920]   And the buy-in was 5,000 euros,
[01:01:39.920 --> 01:01:42.520]   which was much bigger than my bankroll would normally allow.
[01:01:42.520 --> 01:01:45.960]   And so I played a feeder tournament,
[01:01:45.960 --> 01:01:47.920]   won my way in kind of like I did with the Monte Carlo,
[01:01:47.920 --> 01:01:48.760]   big one.
[01:01:48.760 --> 01:01:50.920]   So then I won my way, you know,
[01:01:50.920 --> 01:01:54.200]   from 500 euros into 5,000 euros to play this thing.
[01:01:54.200 --> 01:01:58.040]   And on day one of then the big tournament,
[01:01:58.040 --> 01:01:59.000]   which turned out to have,
[01:01:59.000 --> 01:02:00.880]   it was the biggest tournament ever held in Europe
[01:02:00.880 --> 01:02:01.720]   at the time.
[01:02:01.720 --> 01:02:04.560]   It got over like 1,200 people, absolutely huge.
[01:02:04.560 --> 01:02:06.640]   And I remember they dimmed the lights
[01:02:06.640 --> 01:02:10.240]   for before, you know, the normal shuffle up and deal
[01:02:10.240 --> 01:02:12.120]   to tell everyone to start playing.
[01:02:12.120 --> 01:02:16.080]   And they played Chemical Brothers' "Hey Boy, Hey Girl,"
[01:02:16.080 --> 01:02:17.360]   which I don't know why it's notable,
[01:02:17.360 --> 01:02:18.360]   but it was just like a really,
[01:02:18.360 --> 01:02:19.200]   it was a song I always liked.
[01:02:19.200 --> 01:02:21.200]   It was like one of these like pump me up songs.
[01:02:21.200 --> 01:02:22.920]   And I was sitting there thinking, oh yeah, it's exciting.
[01:02:22.920 --> 01:02:24.600]   I'm playing this really big tournament.
[01:02:24.600 --> 01:02:29.320]   And out of nowhere, just suddenly this voice in my head,
[01:02:29.320 --> 01:02:32.000]   just, and it sounded like my own sort of,
[01:02:32.000 --> 01:02:33.600]   you know, when you think in your mind,
[01:02:33.600 --> 01:02:35.000]   you hear a voice kind of, right?
[01:02:35.000 --> 01:02:35.840]   At least I do.
[01:02:35.840 --> 01:02:38.640]   And so it sounded like my own voice and it said,
[01:02:38.640 --> 01:02:40.320]   you are gonna win this tournament.
[01:02:40.320 --> 01:02:44.200]   And it was so powerful that I got this like wave of like,
[01:02:44.200 --> 01:02:46.760]   you know, sort of goosebumps down my body.
[01:02:46.760 --> 01:02:48.760]   And I even, I remember looking around being like,
[01:02:48.760 --> 01:02:49.840]   did anyone else hear that?
[01:02:49.840 --> 01:02:51.040]   And obviously people are on their phones,
[01:02:51.040 --> 01:02:51.880]   like no one else heard it.
[01:02:51.880 --> 01:02:53.480]   And I was like, okay.
[01:02:53.480 --> 01:02:58.160]   Six days later, I win the fucking tournament
[01:02:58.160 --> 01:02:59.760]   out of 1,200 people.
[01:03:01.480 --> 01:03:06.480]   And I don't know how to explain it.
[01:03:06.480 --> 01:03:13.840]   Okay, yes, maybe I have that feeling before every time
[01:03:13.840 --> 01:03:16.200]   I play and it's just that I happened to, you know,
[01:03:16.200 --> 01:03:18.360]   because I won the tournament, I retroactively remembered it.
[01:03:18.360 --> 01:03:19.400]   But that's just-
[01:03:19.400 --> 01:03:22.360]   - Or the feeling gave you a kind of,
[01:03:22.360 --> 01:03:25.000]   now from the film "Hellmuthian."
[01:03:25.000 --> 01:03:25.840]   - Well, exactly.
[01:03:25.840 --> 01:03:28.360]   - Like it gave you a confident, a deep confidence.
[01:03:28.360 --> 01:03:29.720]   - And it did, it definitely did.
[01:03:29.880 --> 01:03:32.040]   I remember then feeling this like sort of,
[01:03:32.040 --> 01:03:33.960]   well, although I remember then on day one,
[01:03:33.960 --> 01:03:35.920]   I then went and lost half my stack quite early on.
[01:03:35.920 --> 01:03:37.720]   And I remember thinking like, oh, well that was bullshit.
[01:03:37.720 --> 01:03:40.240]   You know, what kind of premonition is this?
[01:03:40.240 --> 01:03:41.080]   Thinking, oh, I'm out.
[01:03:41.080 --> 01:03:42.920]   But you know, I managed to like keep it together
[01:03:42.920 --> 01:03:46.080]   and recover and then just went like pretty perfectly
[01:03:46.080 --> 01:03:47.440]   from then on.
[01:03:47.440 --> 01:03:51.080]   And either way, it definitely instilled me
[01:03:51.080 --> 01:03:52.800]   with this confidence.
[01:03:52.800 --> 01:03:55.800]   And I don't want to put a, I can't put an explanation.
[01:03:55.840 --> 01:03:58.680]   Like, you know, was it some, you know,
[01:03:58.680 --> 01:04:03.680]   huge extra supernatural thing driving me
[01:04:03.680 --> 01:04:06.520]   or was it just my own self-confidence and so on
[01:04:06.520 --> 01:04:08.000]   that just made me make the right decisions?
[01:04:08.000 --> 01:04:08.880]   I don't know.
[01:04:08.880 --> 01:04:10.680]   And I don't, I'm not going to put a frame on it.
[01:04:10.680 --> 01:04:11.520]   And I think-
[01:04:11.520 --> 01:04:12.400]   - I think I know a good explanation.
[01:04:12.400 --> 01:04:14.720]   So we're a bunch of NPCs living in this world
[01:04:14.720 --> 01:04:16.600]   created by, in the simulation.
[01:04:16.600 --> 01:04:19.200]   And then people, not people,
[01:04:19.200 --> 01:04:21.280]   creatures from outside of the simulation
[01:04:21.280 --> 01:04:24.200]   sort of can tune in and play your character.
[01:04:24.200 --> 01:04:27.240]   And that feeling you got is somebody just like,
[01:04:27.240 --> 01:04:29.320]   they got to play a poker tournament through you.
[01:04:29.320 --> 01:04:30.680]   - Honestly, it felt like that.
[01:04:30.680 --> 01:04:33.120]   It did actually feel a little bit like that.
[01:04:33.120 --> 01:04:35.200]   But it's been 12 years now.
[01:04:35.200 --> 01:04:36.680]   I've retold the story many times.
[01:04:36.680 --> 01:04:38.960]   Like, I don't even know how much I can trust my memory.
[01:04:38.960 --> 01:04:41.520]   - You're just an NPC retelling the same story.
[01:04:41.520 --> 01:04:43.800]   'Cause they just played the tournament and left.
[01:04:43.800 --> 01:04:44.760]   - Yeah, they're like, oh, that was fun.
[01:04:44.760 --> 01:04:46.080]   Cool. - Yeah, cool. Next.
[01:04:46.080 --> 01:04:48.520]   And now you're for the rest of your life
[01:04:48.520 --> 01:04:51.880]   left as a boring NPC retelling this story of greatness.
[01:04:51.880 --> 01:04:53.720]   - But it was, and what was interesting was that after that,
[01:04:53.720 --> 01:04:55.640]   then I didn't obviously win a major tournament
[01:04:55.640 --> 01:04:56.800]   for quite a long time.
[01:04:56.800 --> 01:05:01.200]   And it left, that was actually another sort of dark period
[01:05:01.200 --> 01:05:02.800]   because I had this incredible,
[01:05:02.800 --> 01:05:04.360]   like the highs of winning that,
[01:05:04.360 --> 01:05:07.080]   just on a material level were insane, winning the money.
[01:05:07.080 --> 01:05:08.520]   I was on the front page of newspapers
[01:05:08.520 --> 01:05:10.200]   'cause there was this girl that came out of nowhere
[01:05:10.200 --> 01:05:12.160]   and won this big thing.
[01:05:12.160 --> 01:05:16.720]   And so again, sort of chasing that feeling was difficult.
[01:05:16.720 --> 01:05:18.160]   But then on top of that, there was this feeling
[01:05:18.160 --> 01:05:22.320]   of almost being touched by something bigger
[01:05:22.320 --> 01:05:24.080]   that was like, ah.
[01:05:24.080 --> 01:05:26.600]   - So maybe, did you have a sense
[01:05:26.600 --> 01:05:29.520]   that I might be somebody special?
[01:05:29.520 --> 01:05:31.880]   Like this kind of,
[01:05:31.880 --> 01:05:36.520]   I think that's the confidence thing
[01:05:36.520 --> 01:05:40.240]   that maybe you could do something special
[01:05:40.240 --> 01:05:42.320]   in this world after all kind of feeling.
[01:05:42.320 --> 01:05:45.320]   - I definitely, I mean, this is the thing
[01:05:45.320 --> 01:05:47.240]   I think everybody wrestles with to an extent, right?
[01:05:47.240 --> 01:05:51.520]   We all, we are truly the protagonists in our own lives.
[01:05:51.520 --> 01:05:56.520]   And so it's a natural bias, human bias to feel special.
[01:05:56.520 --> 01:06:00.760]   And I think, and in some ways we are special.
[01:06:00.760 --> 01:06:03.760]   Every single person is special because you are that,
[01:06:03.760 --> 01:06:05.200]   the universe does, the world literally
[01:06:05.200 --> 01:06:06.200]   does revolve around you.
[01:06:06.200 --> 01:06:08.560]   That's the thing in some respect.
[01:06:08.560 --> 01:06:10.480]   But of course, if you then zoom out
[01:06:10.480 --> 01:06:12.080]   and take the amalgam of everyone's experiences,
[01:06:12.080 --> 01:06:12.920]   then no, it doesn't.
[01:06:12.920 --> 01:06:15.680]   So there is this shared sort of objective reality,
[01:06:15.680 --> 01:06:17.720]   but sorry, there's objective reality that is shared,
[01:06:17.720 --> 01:06:19.240]   but then there's also this subjective reality,
[01:06:19.240 --> 01:06:20.720]   which is truly unique to you.
[01:06:20.720 --> 01:06:22.280]   And I think both of those things coexist.
[01:06:22.280 --> 01:06:24.440]   And it's not like one is correct and one isn't.
[01:06:24.440 --> 01:06:26.280]   And again, anyone who's like,
[01:06:26.280 --> 01:06:28.160]   oh no, your lived experience is everything
[01:06:28.160 --> 01:06:30.080]   versus your lived experience is nothing.
[01:06:30.080 --> 01:06:32.480]   No, it's a blend between these two things.
[01:06:32.480 --> 01:06:33.840]   They can exist concurrently.
[01:06:33.840 --> 01:06:35.280]   - But there's a certain kind of sense
[01:06:35.280 --> 01:06:36.800]   that at least I've had my whole life.
[01:06:36.800 --> 01:06:38.480]   And I think a lot of people have this as like,
[01:06:38.480 --> 01:06:40.960]   well, I'm just like this little person.
[01:06:40.960 --> 01:06:42.920]   Surely I can't be one of those people
[01:06:42.920 --> 01:06:46.560]   that do the big thing, right?
[01:06:46.560 --> 01:06:48.640]   There's all these big people doing big things.
[01:06:48.640 --> 01:06:53.400]   There's big actors and actresses, big musicians.
[01:06:53.400 --> 01:06:57.040]   There's big business owners and all that kind of stuff,
[01:06:57.040 --> 01:06:58.680]   scientists and so on.
[01:06:58.680 --> 01:07:02.440]   I have my own subjective experience that I enjoy and so on,
[01:07:02.440 --> 01:07:04.760]   but there's like a different layer.
[01:07:04.760 --> 01:07:09.440]   Surely I can't do those great things.
[01:07:09.440 --> 01:07:11.480]   I mean, one of the things just having interacted
[01:07:11.480 --> 01:07:14.160]   with a lot of great people, I realized,
[01:07:14.160 --> 01:07:19.160]   no, they're like just the same humans as me.
[01:07:19.160 --> 01:07:22.160]   And that realization I think is really empowering.
[01:07:22.160 --> 01:07:25.040]   And to remind yourself-- - But are they?
[01:07:25.040 --> 01:07:25.920]   - Huh? - But are they?
[01:07:25.920 --> 01:07:26.760]   - Are they?
[01:07:26.760 --> 01:07:30.560]   Well, in terms-- - Depends on some, yeah.
[01:07:30.560 --> 01:07:33.920]   - They're like a bag of insecurities and--
[01:07:33.920 --> 01:07:34.760]   - Yes.
[01:07:34.760 --> 01:07:39.320]   - Peculiar sort of,
[01:07:39.320 --> 01:07:42.120]   like their own little weirdnesses and so on.
[01:07:43.120 --> 01:07:44.920]   I should say also not,
[01:07:44.920 --> 01:07:50.160]   they have the capacity for brilliance,
[01:07:50.160 --> 01:07:52.880]   but they're not generically brilliant.
[01:07:52.880 --> 01:07:57.880]   Like, we tend to say this person or that person is brilliant
[01:07:57.880 --> 01:08:01.240]   but really, no, they're just like sitting there
[01:08:01.240 --> 01:08:05.280]   and thinking through stuff, just like the rest of us.
[01:08:05.280 --> 01:08:08.120]   I think they're in the habit of thinking through stuff,
[01:08:08.120 --> 01:08:10.600]   seriously, and they've built up a habit
[01:08:10.600 --> 01:08:14.120]   of not allowing their mind to get trapped
[01:08:14.120 --> 01:08:17.040]   in a bunch of bullshit and minutiae of day-to-day life.
[01:08:17.040 --> 01:08:20.080]   They really think big ideas, but those big ideas,
[01:08:20.080 --> 01:08:24.720]   it's like allowing yourself the freedom to think big,
[01:08:24.720 --> 01:08:27.280]   to realize that you can be one
[01:08:27.280 --> 01:08:29.280]   that actually solved this particular big problem.
[01:08:29.280 --> 01:08:31.320]   First, identify a big problem that you care about,
[01:08:31.320 --> 01:08:33.240]   and then like, I can actually be the one
[01:08:33.240 --> 01:08:34.880]   that solves this problem.
[01:08:34.880 --> 01:08:37.320]   And like, allowing yourself to believe that.
[01:08:37.320 --> 01:08:38.920]   And I think sometimes you do need to have like--
[01:08:38.920 --> 01:08:40.280]   - Yeah. - That shock go
[01:08:40.280 --> 01:08:41.680]   through your body and a voice tells you
[01:08:41.680 --> 01:08:42.880]   you're gonna win this tournament.
[01:08:42.880 --> 01:08:44.880]   - Well, exactly, and whether it was,
[01:08:44.880 --> 01:08:50.520]   it's this idea of useful fictions.
[01:08:50.520 --> 01:08:54.920]   So again, like going through the classic rationalist training
[01:08:54.920 --> 01:08:57.440]   of "Less Wrong" where it's like, you want your map,
[01:08:57.440 --> 01:09:00.080]   you know, the image you have of the world in your head
[01:09:00.080 --> 01:09:04.400]   to as accurately match up with how the world actually is.
[01:09:04.400 --> 01:09:07.160]   You want the map and the territory to perfectly align as,
[01:09:07.240 --> 01:09:08.440]   you know, you want it to be
[01:09:08.440 --> 01:09:11.040]   as an accurate representation as possible.
[01:09:11.040 --> 01:09:13.920]   I don't know if I fully subscribe to that anymore,
[01:09:13.920 --> 01:09:16.400]   having now had these moments of like,
[01:09:16.400 --> 01:09:18.560]   feeling of something either bigger
[01:09:18.560 --> 01:09:20.400]   or just actually just being overconfident.
[01:09:20.400 --> 01:09:23.760]   Like there is value in overconfidence sometimes, I do.
[01:09:23.760 --> 01:09:26.240]   If you would, you know, take, you know, take
[01:09:26.240 --> 01:09:30.600]   Magnus Carlsen, right?
[01:09:30.600 --> 01:09:32.360]   If he, I'm sure from a young age,
[01:09:32.360 --> 01:09:34.440]   he knew he was very talented,
[01:09:34.440 --> 01:09:36.720]   but I wouldn't be surprised if he was also
[01:09:36.720 --> 01:09:39.200]   had something in him to,
[01:09:39.200 --> 01:09:40.320]   well, actually maybe he's a bad example
[01:09:40.320 --> 01:09:42.640]   'cause he truly is the world's greatest,
[01:09:42.640 --> 01:09:44.440]   but someone who, it was unclear
[01:09:44.440 --> 01:09:45.720]   whether they were gonna be the world's greatest,
[01:09:45.720 --> 01:09:47.320]   but ended up doing extremely well
[01:09:47.320 --> 01:09:50.320]   because they had this innate, deep self-confidence,
[01:09:50.320 --> 01:09:53.120]   this like even overblown idea
[01:09:53.120 --> 01:09:54.920]   of how good their relative skill level is.
[01:09:54.920 --> 01:09:56.960]   That gave them the confidence to then pursue this thing
[01:09:56.960 --> 01:10:00.480]   and like, with the kind of focus and dedication
[01:10:00.480 --> 01:10:01.840]   that it requires to excel
[01:10:01.840 --> 01:10:03.480]   in whatever it is you're trying to do, you know?
[01:10:03.480 --> 01:10:06.120]   And so there are these useful fictions
[01:10:06.120 --> 01:10:09.560]   and that's where I think I diverge slightly
[01:10:09.560 --> 01:10:13.440]   with the classic sort of rationalist community
[01:10:13.440 --> 01:10:19.680]   because that's a field that is worth studying
[01:10:19.680 --> 01:10:23.280]   of like how the stories we tell,
[01:10:23.280 --> 01:10:25.040]   what the stories we tell to ourselves,
[01:10:25.040 --> 01:10:26.240]   even if they are actually false
[01:10:26.240 --> 01:10:29.040]   and even if we suspect they might be false,
[01:10:29.040 --> 01:10:30.400]   how it's better to sort of have that like
[01:10:30.400 --> 01:10:32.000]   little bit of faith,
[01:10:32.000 --> 01:10:34.360]   the like value in faith, I think, actually.
[01:10:34.360 --> 01:10:35.840]   And that's partly another thing
[01:10:35.840 --> 01:10:37.640]   that's like now led me to explore,
[01:10:37.640 --> 01:10:40.600]   you know, the concept of God,
[01:10:40.600 --> 01:10:42.920]   whether you wanna call it a simulator,
[01:10:42.920 --> 01:10:44.280]   the classic theological thing.
[01:10:44.280 --> 01:10:46.440]   I think we're all like elucidating to the same thing.
[01:10:46.440 --> 01:10:47.760]   Now, I don't know, I'm not saying, you know,
[01:10:47.760 --> 01:10:49.760]   'cause obviously the Christian God is like,
[01:10:49.760 --> 01:10:53.480]   you know, all benevolent, endless love.
[01:10:53.480 --> 01:10:56.680]   The simulation, at least one of the simulation hypothesis
[01:10:56.680 --> 01:10:58.680]   is like, as you said, like a teenager in his bedroom
[01:10:58.680 --> 01:11:01.200]   who doesn't really care, doesn't give a shit
[01:11:01.200 --> 01:11:02.560]   about the individuals within there.
[01:11:02.560 --> 01:11:05.120]   It just like wants to see how this thing plays out
[01:11:05.120 --> 01:11:06.960]   'cause it's curious and it could turn it off like that.
[01:11:06.960 --> 01:11:09.880]   You know, where on the sort of psychopathy
[01:11:09.880 --> 01:11:13.800]   to benevolent spectrum God is, I don't know.
[01:11:13.800 --> 01:11:18.600]   But having this,
[01:11:18.600 --> 01:11:20.160]   just having a little bit of faith
[01:11:20.160 --> 01:11:21.720]   that there is something else out there
[01:11:21.720 --> 01:11:24.360]   that might be interested in our outcome
[01:11:24.360 --> 01:11:27.960]   is I think an essential thing actually for people to find.
[01:11:27.960 --> 01:11:29.680]   A, because it creates commonality between,
[01:11:29.680 --> 01:11:31.880]   it's something we can all share.
[01:11:31.880 --> 01:11:35.040]   And like, it is uniquely humbling of all of us to an extent.
[01:11:35.040 --> 01:11:36.880]   It's like a common objective.
[01:11:36.880 --> 01:11:41.400]   But B, it gives people that little bit of like reserve,
[01:11:41.400 --> 01:11:43.400]   you know, when things get really dark.
[01:11:43.400 --> 01:11:44.880]   And I do think things are gonna get pretty dark
[01:11:44.880 --> 01:11:46.080]   over the next few years.
[01:11:46.080 --> 01:11:48.280]   But it gives that like,
[01:11:48.280 --> 01:11:50.760]   to think that there's something out there
[01:11:50.760 --> 01:11:53.160]   that actually wants our game to keep going.
[01:11:53.160 --> 01:11:55.040]   I keep calling it the game, you know.
[01:11:55.040 --> 01:11:57.680]   It's a thing, C and I, we call it the game.
[01:11:59.280 --> 01:12:03.680]   - You and C is AKA Grimes, we call what the game?
[01:12:03.680 --> 01:12:05.000]   Everything, the whole thing?
[01:12:05.000 --> 01:12:06.680]   - Yeah, we joke about like--
[01:12:06.680 --> 01:12:07.880]   - So everything is a game?
[01:12:07.880 --> 01:12:12.320]   - Not, well, the universe, like what if it's a game
[01:12:12.320 --> 01:12:14.600]   and the goal of the game is to figure out like,
[01:12:14.600 --> 01:12:16.960]   well, either how to beat it, how to get out of it.
[01:12:16.960 --> 01:12:19.920]   You know, maybe this universe is an escape room,
[01:12:19.920 --> 01:12:21.240]   like a giant escape room.
[01:12:21.240 --> 01:12:24.000]   And the goal is to figure out,
[01:12:24.000 --> 01:12:25.160]   put all the pieces of puzzle,
[01:12:25.160 --> 01:12:26.880]   figure out how it works
[01:12:26.880 --> 01:12:29.800]   in order to like unlock this like hyper dimensional key
[01:12:29.800 --> 01:12:31.280]   and get out beyond what it is.
[01:12:31.280 --> 01:12:32.120]   That's--
[01:12:32.120 --> 01:12:34.400]   - No, but then, so you're saying it's like different levels
[01:12:34.400 --> 01:12:36.520]   and it's like a cage within a cage within a cage
[01:12:36.520 --> 01:12:40.000]   and one cage at a time, you figure out how to escape that.
[01:12:40.000 --> 01:12:42.240]   - Like a new level up, you know,
[01:12:42.240 --> 01:12:44.400]   like us becoming multi-planetary would be a level up
[01:12:44.400 --> 01:12:46.680]   or us, you know, figuring out how to upload
[01:12:46.680 --> 01:12:48.240]   our consciousnesses to the thing,
[01:12:48.240 --> 01:12:49.600]   that would probably be a leveling up.
[01:12:49.600 --> 01:12:53.600]   Or spiritually, you know, humanity becoming more combined
[01:12:53.760 --> 01:12:56.880]   and less adversarial and bloodthirsty
[01:12:56.880 --> 01:12:58.760]   and us becoming a little bit more enlightened,
[01:12:58.760 --> 01:12:59.600]   that would be a leveling up.
[01:12:59.600 --> 01:13:01.840]   You know, there's many different frames to it,
[01:13:01.840 --> 01:13:05.120]   whether it's physical, you know, digital,
[01:13:05.120 --> 01:13:05.960]   or like metaphysical--
[01:13:05.960 --> 01:13:06.800]   - I wonder what the level,
[01:13:06.800 --> 01:13:09.240]   I think level one for Earth
[01:13:09.240 --> 01:13:14.040]   is probably the biological evolutionary process.
[01:13:14.040 --> 01:13:18.360]   So going from single cell organisms to early humans.
[01:13:18.360 --> 01:13:22.200]   Then maybe level two is whatever's happening
[01:13:22.200 --> 01:13:24.000]   inside our minds and creating ideas
[01:13:24.000 --> 01:13:26.100]   and creating technologies.
[01:13:26.100 --> 01:13:30.200]   That's like evolutionary process of ideas.
[01:13:30.200 --> 01:13:34.640]   And then multi-planetary is interesting.
[01:13:34.640 --> 01:13:35.880]   Is that fundamentally different
[01:13:35.880 --> 01:13:37.880]   from what we're doing here on Earth?
[01:13:37.880 --> 01:13:41.520]   Probably, 'cause it allows us to exponentially scale.
[01:13:41.520 --> 01:13:47.040]   - It delays the Malthusian trap, right?
[01:13:47.040 --> 01:13:50.400]   It's a way to keep the playing field,
[01:13:51.800 --> 01:13:53.440]   to make the playing field get larger
[01:13:53.440 --> 01:13:57.320]   so that it can accommodate more of our stuff, more of us.
[01:13:57.320 --> 01:13:58.480]   And that's a good thing,
[01:13:58.480 --> 01:14:03.480]   but I don't know if it like fully solves this issue of,
[01:14:03.480 --> 01:14:06.800]   well, this thing called Moloch,
[01:14:06.800 --> 01:14:08.720]   which we haven't talked about yet,
[01:14:08.720 --> 01:14:11.040]   which is basically, I call it
[01:14:11.040 --> 01:14:12.760]   the god of unhealthy competition.
[01:14:12.760 --> 01:14:14.040]   - Yeah, let's go to Moloch.
[01:14:14.040 --> 01:14:15.040]   What's Moloch?
[01:14:15.040 --> 01:14:17.480]   You did a great video on Moloch,
[01:14:17.480 --> 01:14:19.880]   one aspect of it, the application of it to--
[01:14:19.880 --> 01:14:21.600]   - Yeah, Instagram beauty filters.
[01:14:22.440 --> 01:14:23.600]   - True.
[01:14:23.600 --> 01:14:25.000]   - Very niche.
[01:14:25.000 --> 01:14:27.360]   I wanted to start off small.
[01:14:27.360 --> 01:14:32.360]   So Moloch was originally coined as,
[01:14:32.360 --> 01:14:39.560]   well, so apparently back in the Canaanite times,
[01:14:39.560 --> 01:14:41.440]   it was to say ancient Carthaginian,
[01:14:41.440 --> 01:14:43.240]   I can never say it, Carthaginian,
[01:14:43.240 --> 01:14:46.760]   somewhere around 300 BC or 200 AD, I don't know.
[01:14:46.760 --> 01:14:49.720]   There was supposedly this death cult
[01:14:49.720 --> 01:14:53.240]   who would sacrifice their children
[01:14:53.240 --> 01:14:56.080]   to this awful demon god thing they called Moloch
[01:14:56.080 --> 01:14:59.400]   in order to get power to win wars.
[01:14:59.400 --> 01:15:00.920]   So really dark, horrible things.
[01:15:00.920 --> 01:15:02.640]   And it was literally like about child sacrifice.
[01:15:02.640 --> 01:15:04.080]   Whether they actually existed or not, we don't know,
[01:15:04.080 --> 01:15:05.680]   but in mythology they did.
[01:15:05.680 --> 01:15:07.240]   And this god that they worshipped
[01:15:07.240 --> 01:15:09.280]   was this thing called Moloch.
[01:15:09.280 --> 01:15:10.960]   And then, I don't know,
[01:15:10.960 --> 01:15:13.720]   it seemed like it was kind of quiet throughout history
[01:15:13.720 --> 01:15:15.440]   in terms of mythology beyond that
[01:15:15.440 --> 01:15:20.440]   until this movie "Metropolis" in 1927 talked about this,
[01:15:20.440 --> 01:15:27.040]   you see that there was this incredible futuristic city
[01:15:27.040 --> 01:15:29.080]   that everyone was living great in,
[01:15:29.080 --> 01:15:31.360]   but then the protagonist goes underground into the sewers
[01:15:31.360 --> 01:15:34.280]   and sees that the city is run by this machine.
[01:15:34.280 --> 01:15:36.160]   And this machine basically would just like
[01:15:36.160 --> 01:15:38.080]   kill the workers all the time
[01:15:38.080 --> 01:15:40.080]   because it was just so hard to keep it running.
[01:15:40.080 --> 01:15:40.920]   They were always dying.
[01:15:40.920 --> 01:15:43.000]   So there was all this suffering that was required
[01:15:43.000 --> 01:15:44.560]   in order to keep the city going.
[01:15:44.560 --> 01:15:45.920]   And then the protagonist has this vision
[01:15:45.920 --> 01:15:48.160]   that this machine is actually this demon Moloch.
[01:15:48.160 --> 01:15:50.880]   So again, it's like this sort of mechanistic consumption
[01:15:50.880 --> 01:15:53.840]   of humans in order to get more power.
[01:15:53.840 --> 01:15:58.720]   And then Allen Ginsberg wrote a poem in the '60s,
[01:15:58.720 --> 01:16:01.920]   which incredible poem called "Howl"
[01:16:01.920 --> 01:16:03.120]   about this thing Moloch.
[01:16:03.120 --> 01:16:07.000]   And a lot of people sort of quite understandably
[01:16:07.000 --> 01:16:09.160]   take the interpretation of that,
[01:16:09.160 --> 01:16:10.920]   that he's talking about capitalism.
[01:16:12.360 --> 01:16:14.680]   But then the sort of piece to resistance
[01:16:14.680 --> 01:16:17.320]   that's moved Moloch into this idea of game theory
[01:16:17.320 --> 01:16:20.240]   was Scott Alexander of Slate's "Codex."
[01:16:20.240 --> 01:16:22.600]   Wrote this incredible,
[01:16:22.600 --> 01:16:24.400]   one that literally I think it might be my favorite piece
[01:16:24.400 --> 01:16:25.280]   of writing of all time.
[01:16:25.280 --> 01:16:27.160]   It's called "Meditations on Moloch."
[01:16:27.160 --> 01:16:28.600]   Everyone must go read it.
[01:16:28.600 --> 01:16:30.560]   And-
[01:16:30.560 --> 01:16:31.880]   - I say "Codex" is a blog.
[01:16:31.880 --> 01:16:33.400]   - It's a blog, yes.
[01:16:33.400 --> 01:16:36.200]   We can link to it in the show notes or something, right?
[01:16:36.200 --> 01:16:37.040]   No, don't.
[01:16:37.040 --> 01:16:39.240]   - Yes, yes.
[01:16:39.280 --> 01:16:42.280]   - But I like how you assume
[01:16:42.280 --> 01:16:44.280]   I have a professional operation going on here.
[01:16:44.280 --> 01:16:45.120]   - I mean-
[01:16:45.120 --> 01:16:45.960]   - I shall try to remember to-
[01:16:45.960 --> 01:16:46.800]   - You're wearing a suit.
[01:16:46.800 --> 01:16:48.920]   What do you want?
[01:16:48.920 --> 01:16:50.160]   You're giving the impression of it.
[01:16:50.160 --> 01:16:51.520]   - Yeah, I'll look, please.
[01:16:51.520 --> 01:16:54.040]   If I don't, please, somebody in the comments remind me.
[01:16:54.040 --> 01:16:54.880]   - I'll help you.
[01:16:54.880 --> 01:16:56.640]   - If you don't know this blog,
[01:16:56.640 --> 01:17:00.240]   it's one of the best blogs ever probably.
[01:17:00.240 --> 01:17:01.960]   You should probably be following it.
[01:17:01.960 --> 01:17:02.800]   - Yes.
[01:17:02.800 --> 01:17:04.160]   - Are blogs still a thing?
[01:17:04.160 --> 01:17:05.760]   I think they are still a thing, yeah.
[01:17:05.760 --> 01:17:08.880]   - He's migrated onto Substack, but yeah, it's still a blog.
[01:17:08.880 --> 01:17:11.520]   - Substack better not fuck things up.
[01:17:11.520 --> 01:17:12.640]   - I hope not, yeah.
[01:17:12.640 --> 01:17:15.160]   I hope they don't turn Moloch-y,
[01:17:15.160 --> 01:17:17.560]   which will mean something to people when we continue.
[01:17:17.560 --> 01:17:18.760]   (both laughing)
[01:17:18.760 --> 01:17:20.040]   - When I stop interrupting for once.
[01:17:20.040 --> 01:17:21.560]   - No, no, it's good.
[01:17:21.560 --> 01:17:25.440]   So anyway, so he writes this piece, "Meditations on Moloch,"
[01:17:25.440 --> 01:17:28.720]   and basically he analyzes the poem and he's like,
[01:17:28.720 --> 01:17:30.200]   "Okay, so it seems to be something relating
[01:17:30.200 --> 01:17:32.960]   to where competition goes wrong."
[01:17:32.960 --> 01:17:36.080]   And Moloch was historically this thing
[01:17:36.400 --> 01:17:40.120]   where people would sacrifice a thing that they care about,
[01:17:40.120 --> 01:17:42.480]   in this case, children, their own children,
[01:17:42.480 --> 01:17:45.920]   in order to gain power, a competitive advantage.
[01:17:45.920 --> 01:17:48.320]   And if you look at almost everything
[01:17:48.320 --> 01:17:50.360]   that sort of goes wrong in our society,
[01:17:50.360 --> 01:17:52.840]   it's that same process.
[01:17:52.840 --> 01:17:55.520]   So with the Instagram beauty filters thing,
[01:17:55.520 --> 01:18:02.360]   if you're trying to become a famous Instagram model,
[01:18:02.360 --> 01:18:04.400]   you are incentivized to post the hottest pictures
[01:18:04.400 --> 01:18:05.680]   of yourself that you can.
[01:18:05.680 --> 01:18:07.280]   You're trying to play that game.
[01:18:07.280 --> 01:18:08.720]   There's a lot of hot women on Instagram.
[01:18:08.720 --> 01:18:10.080]   How do you compete against them?
[01:18:10.080 --> 01:18:11.600]   You post really hot pictures
[01:18:11.600 --> 01:18:13.320]   and that's how you get more likes.
[01:18:13.320 --> 01:18:16.080]   As technology gets better,
[01:18:16.080 --> 01:18:19.320]   more makeup techniques come along.
[01:18:19.320 --> 01:18:22.080]   And then more recently, these beauty filters,
[01:18:22.080 --> 01:18:23.480]   where like at the touch of a button,
[01:18:23.480 --> 01:18:26.080]   it makes your face look absolutely incredible,
[01:18:26.080 --> 01:18:28.720]   compared to your natural face.
[01:18:28.720 --> 01:18:31.600]   These technologies come along.
[01:18:31.600 --> 01:18:34.560]   Everyone is incentivized to that short-term strategy.
[01:18:35.640 --> 01:18:39.040]   But on net, it's bad for everyone
[01:18:39.040 --> 01:18:40.480]   because now everyone is kind of feeling
[01:18:40.480 --> 01:18:41.680]   like they have to use these things.
[01:18:41.680 --> 01:18:43.080]   And these things, they make you,
[01:18:43.080 --> 01:18:44.400]   the reason why I talked about them in this video
[01:18:44.400 --> 01:18:45.880]   is 'cause I noticed it myself.
[01:18:45.880 --> 01:18:48.840]   Like I was trying to grow my Instagram for a while.
[01:18:48.840 --> 01:18:50.000]   I've given up on it now.
[01:18:50.000 --> 01:18:53.440]   And I noticed these filters, how good they made me look.
[01:18:53.440 --> 01:18:56.400]   And I'm like, well, I know that everyone else
[01:18:56.400 --> 01:18:57.240]   is kind of doing it.
[01:18:57.240 --> 01:18:59.000]   - Go subscribe to Liz's Instagram.
[01:18:59.000 --> 01:19:00.840]   - Please, so I don't have to use the filters.
[01:19:00.840 --> 01:19:02.200]   (both laughing)
[01:19:02.200 --> 01:19:05.000]   - Post a bunch of, yeah, make it blow up.
[01:19:06.000 --> 01:19:08.440]   So yeah, you felt the pressure actually.
[01:19:08.440 --> 01:19:12.760]   - Exactly, these short-term incentives to do this thing
[01:19:12.760 --> 01:19:17.280]   that either sacrifices your integrity or something else
[01:19:17.280 --> 01:19:19.160]   in order to stay competitive,
[01:19:19.160 --> 01:19:23.760]   which on aggregate creates this sort of race
[01:19:23.760 --> 01:19:25.880]   to the bottom spiral where everyone else ends up
[01:19:25.880 --> 01:19:28.480]   in a situation which is worse off than if they hadn't started
[01:19:28.480 --> 01:19:29.320]   than it were before.
[01:19:29.320 --> 01:19:33.320]   Kind of like if, like at a football stadium,
[01:19:34.600 --> 01:19:36.400]   the system is so badly designed,
[01:19:36.400 --> 01:19:38.160]   a competitive system of like everyone sitting
[01:19:38.160 --> 01:19:40.240]   and having a view, that if someone at the very front
[01:19:40.240 --> 01:19:42.080]   stands up to get an even better view,
[01:19:42.080 --> 01:19:43.640]   it forces everyone else behind
[01:19:43.640 --> 01:19:45.440]   to like adopt that same strategy,
[01:19:45.440 --> 01:19:47.000]   just to get to where they were before,
[01:19:47.000 --> 01:19:49.160]   but now everyone's stuck standing up.
[01:19:49.160 --> 01:19:51.880]   So you need this like top-down God's eye coordination
[01:19:51.880 --> 01:19:53.840]   to make it go back to the better state.
[01:19:53.840 --> 01:19:56.080]   But from within the system, you can't actually do that.
[01:19:56.080 --> 01:19:57.680]   So that's kind of what this Moloch thing is.
[01:19:57.680 --> 01:20:01.600]   It's this thing that makes people sacrifice values
[01:20:01.600 --> 01:20:04.440]   in order to optimize for winning the game in question,
[01:20:04.440 --> 01:20:05.480]   the short-term game.
[01:20:05.480 --> 01:20:09.760]   - But this Moloch, can you attribute it
[01:20:09.760 --> 01:20:11.680]   to any one centralized source,
[01:20:11.680 --> 01:20:13.640]   or is it an emergent phenomena
[01:20:13.640 --> 01:20:16.160]   from a large collection of people?
[01:20:16.160 --> 01:20:17.000]   - Exactly that.
[01:20:17.000 --> 01:20:18.880]   It's an emergent phenomena.
[01:20:18.880 --> 01:20:21.280]   It's a force of game theory.
[01:20:21.280 --> 01:20:25.600]   It's a force of bad incentives on a multi-agent system
[01:20:25.600 --> 01:20:26.600]   where you've got more, you know,
[01:20:26.600 --> 01:20:29.880]   prisoner's dilemma is technically a kind of Moloch-y
[01:20:29.880 --> 01:20:31.960]   system as well, but it's just a two-player thing.
[01:20:31.960 --> 01:20:35.720]   But another word for Moloch is multipolar trap,
[01:20:35.720 --> 01:20:37.760]   where basically you just got a lot of different people
[01:20:37.760 --> 01:20:39.840]   all competing for some kind of prize.
[01:20:39.840 --> 01:20:43.200]   And it would be better if everyone didn't do
[01:20:43.200 --> 01:20:44.720]   this one shitty strategy,
[01:20:44.720 --> 01:20:47.240]   but because that strategy gives you a short-term advantage,
[01:20:47.240 --> 01:20:48.480]   everyone's incentivized to do it,
[01:20:48.480 --> 01:20:49.840]   and so everyone ends up doing it.
[01:20:49.840 --> 01:20:52.200]   - So the responsibility for, I mean,
[01:20:52.200 --> 01:20:54.040]   social media is a really nice place
[01:20:54.040 --> 01:20:56.840]   for a large number of people to play game theory.
[01:20:56.840 --> 01:20:59.720]   And so they also have the ability
[01:20:59.720 --> 01:21:02.560]   to then design the rules of the game.
[01:21:02.560 --> 01:21:06.960]   And is it on them to try to anticipate what kind of,
[01:21:06.960 --> 01:21:08.960]   like to do the thing that poker players are doing,
[01:21:08.960 --> 01:21:11.080]   to run simulation?
[01:21:11.080 --> 01:21:12.360]   - Ideally, that would have been great.
[01:21:12.360 --> 01:21:15.800]   If, you know, Mark Zuckerberg and Jack and all the,
[01:21:15.800 --> 01:21:17.240]   you know, the Twitter founders and everyone,
[01:21:17.240 --> 01:21:20.560]   if they had at least just run a few simulations
[01:21:20.560 --> 01:21:23.240]   of how their algorithms would, you know,
[01:21:23.240 --> 01:21:26.200]   different types of algorithms would turn out for society,
[01:21:26.200 --> 01:21:27.320]   that would have been great.
[01:21:27.320 --> 01:21:28.680]   - That's really difficult to do
[01:21:28.680 --> 01:21:30.400]   that kind of deep philosophical thinking
[01:21:30.400 --> 01:21:33.080]   about humanity, actually.
[01:21:33.080 --> 01:21:38.080]   So not kind of this level of how do we optimize engagement
[01:21:38.080 --> 01:21:42.840]   or what brings people joy in the short term,
[01:21:42.840 --> 01:21:45.320]   but how is this thing going to change
[01:21:45.320 --> 01:21:47.960]   the way people see the world?
[01:21:47.960 --> 01:21:52.960]   How is it gonna get morphed in iterative games played
[01:21:52.960 --> 01:21:56.760]   into something that will change society forever?
[01:21:57.040 --> 01:21:58.800]   That requires some deep thinking.
[01:21:58.800 --> 01:22:02.600]   I hope there's meetings like that inside companies,
[01:22:02.600 --> 01:22:03.600]   but I haven't seen them. - There aren't.
[01:22:03.600 --> 01:22:04.480]   That's the problem.
[01:22:04.480 --> 01:22:08.640]   And it's difficult because like when you're starting up
[01:22:08.640 --> 01:22:10.280]   a social media company, you know,
[01:22:10.280 --> 01:22:14.120]   you're aware that you've got investors to please,
[01:22:14.120 --> 01:22:17.280]   there's bills to pay, you know,
[01:22:17.280 --> 01:22:20.120]   there's only so much R&D you can afford to do.
[01:22:20.120 --> 01:22:22.120]   You've got all these like incredible pressures,
[01:22:22.120 --> 01:22:23.760]   and bad incentives to get on
[01:22:23.760 --> 01:22:25.200]   and just build your thing as quickly as possible
[01:22:25.200 --> 01:22:26.640]   and start making money.
[01:22:26.640 --> 01:22:29.120]   And, you know, I don't think anyone intended
[01:22:29.120 --> 01:22:32.560]   when they built these social media platforms,
[01:22:32.560 --> 01:22:33.880]   and just to like preface it.
[01:22:33.880 --> 01:22:36.360]   So the reason why social media is relevant
[01:22:36.360 --> 01:22:38.720]   because it's a very good example of like,
[01:22:38.720 --> 01:22:42.000]   everyone these days is optimizing for clicks,
[01:22:42.000 --> 01:22:44.760]   whether it's the social media platforms themselves,
[01:22:44.760 --> 01:22:47.880]   because every click gets more impressions
[01:22:47.880 --> 01:22:49.720]   and impressions pay for, you know,
[01:22:49.720 --> 01:22:51.000]   they get advertising dollars,
[01:22:51.000 --> 01:22:53.760]   or whether it's individual influencers,
[01:22:53.760 --> 01:22:56.080]   or whether it's the New York Times or whoever,
[01:22:56.080 --> 01:22:58.280]   they're trying to get their story to go viral.
[01:22:58.280 --> 01:23:00.200]   So everyone's got this bad incentive of using,
[01:23:00.200 --> 01:23:02.760]   as you called it, the clickbait industrial complex.
[01:23:02.760 --> 01:23:04.200]   That's a very moloch-y system
[01:23:04.200 --> 01:23:06.240]   because everyone is now using worse and worse tactics
[01:23:06.240 --> 01:23:08.680]   in order to like try and win this attention game.
[01:23:08.680 --> 01:23:14.520]   And yeah, so ideally these companies would have had
[01:23:14.520 --> 01:23:17.280]   enough slack in the beginning
[01:23:17.280 --> 01:23:19.800]   in order to run these experiments to see,
[01:23:19.800 --> 01:23:22.440]   okay, what are the ways this could possibly go wrong
[01:23:22.440 --> 01:23:23.280]   for people?
[01:23:23.280 --> 01:23:24.680]   What are the ways that moloch,
[01:23:24.680 --> 01:23:26.320]   they should be aware of this concept of moloch
[01:23:26.320 --> 01:23:27.680]   and realize that it's,
[01:23:27.680 --> 01:23:31.280]   whenever you have a highly competitive multi-agent system,
[01:23:31.280 --> 01:23:32.800]   which social media is a classic example
[01:23:32.800 --> 01:23:34.160]   of millions of agents,
[01:23:34.160 --> 01:23:35.920]   all trying to compete for likes and so on,
[01:23:35.920 --> 01:23:39.920]   and you try and bring all this complexity down
[01:23:39.920 --> 01:23:41.880]   into like very small metrics,
[01:23:41.880 --> 01:23:44.880]   such as number of likes, number of retweets,
[01:23:44.880 --> 01:23:46.720]   whatever the algorithm optimizes for,
[01:23:46.720 --> 01:23:48.680]   that is a like guaranteed recipe
[01:23:48.680 --> 01:23:49.800]   for this stuff to go wrong
[01:23:49.800 --> 01:23:51.520]   and become a race to the bottom.
[01:23:51.520 --> 01:23:53.920]   - I think there should be an honesty when founders,
[01:23:53.920 --> 01:23:56.400]   I think there's a hunger for that kind of transparency
[01:23:56.400 --> 01:23:58.200]   of like, we don't know what the fuck we're doing.
[01:23:58.200 --> 01:23:59.720]   This is a fascinating experiment.
[01:23:59.720 --> 01:24:02.800]   We're all running as a human civilization.
[01:24:02.800 --> 01:24:04.320]   Let's try this out.
[01:24:04.320 --> 01:24:06.440]   And like, actually just be honest about this,
[01:24:06.440 --> 01:24:10.440]   that we're all like these weird rats in a maze.
[01:24:10.440 --> 01:24:12.400]   None of us are controlling it.
[01:24:12.400 --> 01:24:15.160]   There's this kind of sense like the founders,
[01:24:15.160 --> 01:24:17.560]   the CEO of Instagram or whatever,
[01:24:17.560 --> 01:24:19.240]   Mark Zuckerberg has a control
[01:24:19.240 --> 01:24:21.880]   and he's like with strings playing people.
[01:24:21.880 --> 01:24:22.880]   No, they're-
[01:24:22.880 --> 01:24:24.640]   - He's at the mercy of this, like everyone else.
[01:24:24.640 --> 01:24:26.600]   He's just like trying to do his best.
[01:24:26.600 --> 01:24:29.280]   - And like, I think putting on a smile
[01:24:29.280 --> 01:24:33.000]   and doing over polished videos
[01:24:33.000 --> 01:24:36.920]   about how Instagram and Facebook are good for you,
[01:24:36.920 --> 01:24:38.000]   I think is not the right way
[01:24:38.000 --> 01:24:41.160]   to actually ask some of the deepest questions
[01:24:41.160 --> 01:24:43.240]   we get to ask as a society.
[01:24:43.240 --> 01:24:48.120]   How do we design the game such that we build a better world?
[01:24:48.120 --> 01:24:51.760]   - I think a big part of this as well is people,
[01:24:51.760 --> 01:24:55.720]   there's this philosophy, particularly in Silicon Valley
[01:24:55.720 --> 01:24:58.520]   of well, techno-optimism,
[01:24:58.520 --> 01:25:00.360]   technology will solve all our issues.
[01:25:00.360 --> 01:25:03.080]   And there's a steel man argument to that
[01:25:03.080 --> 01:25:05.440]   where yes, technology has solved a lot of problems
[01:25:05.440 --> 01:25:08.200]   and can potentially solve a lot of future ones.
[01:25:08.200 --> 01:25:10.840]   But it can also, it's always a double-edged sword
[01:25:10.840 --> 01:25:13.440]   and particularly as technology gets more and more powerful
[01:25:13.440 --> 01:25:15.120]   and we've now got like big data
[01:25:15.120 --> 01:25:17.360]   and we're able to do all kinds of like
[01:25:17.360 --> 01:25:19.560]   psychological manipulation with it and so on.
[01:25:20.680 --> 01:25:24.960]   It's, technology is not a values neutral thing.
[01:25:24.960 --> 01:25:27.000]   People think, I used to always think this myself,
[01:25:27.000 --> 01:25:28.880]   it's like this naive view that,
[01:25:28.880 --> 01:25:30.720]   oh, technology is completely neutral.
[01:25:30.720 --> 01:25:33.840]   It's just, it's the humans that either make it good or bad.
[01:25:33.840 --> 01:25:36.640]   No, to the point we're at now,
[01:25:36.640 --> 01:25:38.000]   the technology that we are creating,
[01:25:38.000 --> 01:25:39.440]   they are social technologies.
[01:25:39.440 --> 01:25:44.440]   They literally dictate how humans now form social groups
[01:25:44.440 --> 01:25:46.240]   and so on beyond that.
[01:25:46.240 --> 01:25:47.760]   And beyond that, it also then,
[01:25:47.760 --> 01:25:49.600]   that gives rise to like the memes
[01:25:49.600 --> 01:25:51.960]   that we then like coalesce around.
[01:25:51.960 --> 01:25:54.400]   And that, if you have the stack that way
[01:25:54.400 --> 01:25:56.960]   where it's technology driving social interaction,
[01:25:56.960 --> 01:26:00.480]   which then drives like mimetic culture
[01:26:00.480 --> 01:26:04.800]   and like which ideas become popular, that's Moloch.
[01:26:04.800 --> 01:26:06.480]   And we need the other way around.
[01:26:06.480 --> 01:26:08.400]   We need it so we need to figure out what are the good memes?
[01:26:08.400 --> 01:26:13.400]   What are the good values that we think are,
[01:26:13.400 --> 01:26:16.680]   we need to optimize for that like makes people happy
[01:26:16.680 --> 01:26:20.120]   and healthy and like keeps society as robust
[01:26:20.120 --> 01:26:21.360]   and safe as possible.
[01:26:21.360 --> 01:26:22.840]   Then figure out what the social structure
[01:26:22.840 --> 01:26:23.680]   around those should be.
[01:26:23.680 --> 01:26:25.480]   And only then do we figure out technology.
[01:26:25.480 --> 01:26:26.800]   But we're doing the other way around.
[01:26:26.800 --> 01:26:31.800]   And, you know, like as much as I love in many ways
[01:26:31.800 --> 01:26:33.840]   the culture of Silicon Valley and like, you know,
[01:26:33.840 --> 01:26:36.040]   I do think that technology has, you know,
[01:26:36.040 --> 01:26:36.880]   I don't want to knock it.
[01:26:36.880 --> 01:26:38.240]   It's done so many wonderful things for us.
[01:26:38.240 --> 01:26:39.280]   Same with capitalism.
[01:26:39.280 --> 01:26:44.160]   There are, we have to like be honest with ourselves.
[01:26:44.160 --> 01:26:47.240]   We're getting to a point where we are losing control
[01:26:47.240 --> 01:26:49.600]   of this very powerful machine that we have created.
[01:26:49.600 --> 01:26:53.200]   - Can you redesign the machine within the game?
[01:26:53.200 --> 01:26:57.360]   Can you just have, can you understand the game enough?
[01:26:57.360 --> 01:26:58.880]   Okay, this is the game.
[01:26:58.880 --> 01:27:01.920]   And this is how we start to reemphasize
[01:27:01.920 --> 01:27:03.840]   the memes that matter.
[01:27:03.840 --> 01:27:06.720]   The memes that bring out the best in us.
[01:27:06.720 --> 01:27:11.320]   You know, like the way I try to be in real life
[01:27:11.320 --> 01:27:15.240]   and the way I try to be online is to be about kindness
[01:27:15.240 --> 01:27:16.080]   and love.
[01:27:16.080 --> 01:27:19.840]   And I feel like I'm sometimes get like criticized
[01:27:19.840 --> 01:27:22.040]   for being naive and all those kinds of things.
[01:27:22.040 --> 01:27:25.840]   But I feel like I'm just trying to live within this game.
[01:27:25.840 --> 01:27:27.520]   I'm trying to be authentic.
[01:27:27.520 --> 01:27:31.080]   Yeah, but also like, hey, it's kind of fun to do this.
[01:27:31.080 --> 01:27:32.920]   Like you guys should try this too.
[01:27:32.920 --> 01:27:37.200]   You know, that, and that's like trying to redesign
[01:27:37.200 --> 01:27:40.000]   some aspects of the game within the game.
[01:27:40.960 --> 01:27:41.920]   - Is that possible?
[01:27:41.920 --> 01:27:46.760]   - I don't know, but I think we should try.
[01:27:46.760 --> 01:27:48.640]   I don't think we have an option but to try.
[01:27:48.640 --> 01:27:51.160]   - Well, the other option is to create new companies
[01:27:51.160 --> 01:27:55.280]   or to pressure companies that,
[01:27:55.280 --> 01:27:59.040]   or anyone who has control of the rules of the game.
[01:27:59.040 --> 01:28:01.120]   - I think we need to be doing all of the above.
[01:28:01.120 --> 01:28:03.000]   I think we need to be thinking hard
[01:28:03.000 --> 01:28:06.080]   about what are the kind of positive, healthy memes.
[01:28:09.760 --> 01:28:13.640]   As Elon said, he who controls the memes controls the universe.
[01:28:13.640 --> 01:28:14.480]   - He said that.
[01:28:14.480 --> 01:28:16.320]   - I think he did, yeah.
[01:28:16.320 --> 01:28:17.880]   But there's truth to that.
[01:28:17.880 --> 01:28:19.440]   It's very, there is wisdom in that
[01:28:19.440 --> 01:28:22.760]   because memes have driven history.
[01:28:22.760 --> 01:28:24.920]   You know, we are a cultural species.
[01:28:24.920 --> 01:28:27.200]   That's what sets us apart from chimpanzees
[01:28:27.200 --> 01:28:28.040]   and everything else.
[01:28:28.040 --> 01:28:32.520]   We have the ability to learn and evolve through culture
[01:28:32.520 --> 01:28:34.960]   as opposed to biology or like, you know,
[01:28:34.960 --> 01:28:37.360]   classic physical constraints.
[01:28:37.360 --> 01:28:40.800]   And that means culture is incredibly powerful
[01:28:40.800 --> 01:28:44.960]   and we can create and become victim to very bad memes
[01:28:44.960 --> 01:28:46.680]   or very good ones.
[01:28:46.680 --> 01:28:49.360]   But we do have some agency over which memes,
[01:28:49.360 --> 01:28:52.000]   you know, we not only put out there,
[01:28:52.000 --> 01:28:53.680]   but we also like subscribe to.
[01:28:53.680 --> 01:28:56.520]   So I think we need to take that approach.
[01:28:56.520 --> 01:29:01.520]   We also need to, you know, 'cause I don't want the,
[01:29:01.520 --> 01:29:03.800]   I'm making this video right now called the Attention Wars,
[01:29:03.800 --> 01:29:05.520]   which is about like how Moloch,
[01:29:05.520 --> 01:29:08.560]   like the media machine is this Moloch machine.
[01:29:08.560 --> 01:29:11.440]   Well, is this kind of like blind, dumb thing
[01:29:11.440 --> 01:29:13.200]   that where everyone is optimizing for engagement
[01:29:13.200 --> 01:29:16.480]   in order to win their share of the attention pie.
[01:29:16.480 --> 01:29:18.280]   And then if you zoom out, it's really like Moloch
[01:29:18.280 --> 01:29:19.240]   that's pulling the strings
[01:29:19.240 --> 01:29:20.960]   'cause the only thing that benefits from this in the end,
[01:29:20.960 --> 01:29:24.040]   you know, like our information ecosystem is breaking down.
[01:29:24.040 --> 01:29:26.480]   Like we have, you look at the state of the US,
[01:29:26.480 --> 01:29:28.440]   it's in, we're in a civil war.
[01:29:28.440 --> 01:29:30.000]   It's just not a physical war.
[01:29:30.000 --> 01:29:33.320]   It's an information war.
[01:29:33.320 --> 01:29:35.120]   And people are becoming more fractured
[01:29:35.120 --> 01:29:37.440]   in terms of what their actual shared reality is.
[01:29:37.440 --> 01:29:39.600]   Like truly like an extreme left person,
[01:29:39.600 --> 01:29:40.520]   an extreme right person,
[01:29:40.520 --> 01:29:43.440]   like they literally live in different worlds
[01:29:43.440 --> 01:29:45.320]   in their minds at this point.
[01:29:45.320 --> 01:29:47.280]   And it's getting more and more amplified.
[01:29:47.280 --> 01:29:50.320]   And this force is like a razor blade
[01:29:50.320 --> 01:29:51.920]   pushing through everything.
[01:29:51.920 --> 01:29:53.400]   It doesn't matter how innocuous the topic is,
[01:29:53.400 --> 01:29:55.320]   it will find a way to split into this,
[01:29:55.320 --> 01:29:57.280]   you know, bifurcated culture war.
[01:29:57.280 --> 01:29:58.320]   And it's fucking terrifying.
[01:29:58.320 --> 01:29:59.840]   - Because that maximizes the tension.
[01:29:59.840 --> 01:30:03.200]   And that's like an emergent Moloch type force
[01:30:03.200 --> 01:30:06.880]   that takes any, anything, any topic
[01:30:06.880 --> 01:30:11.240]   and cuts through it so that it can split nicely
[01:30:11.240 --> 01:30:12.560]   into two groups.
[01:30:12.560 --> 01:30:13.920]   One that's--
[01:30:13.920 --> 01:30:15.560]   - Well, it's whatever, yeah,
[01:30:15.560 --> 01:30:18.040]   all everyone is trying to do within the system
[01:30:18.040 --> 01:30:21.240]   is just maximize whatever gets them the most attention
[01:30:21.240 --> 01:30:22.360]   because they're just trying to make money
[01:30:22.360 --> 01:30:24.560]   so they can keep their thing going, right?
[01:30:24.560 --> 01:30:29.240]   And the way, the best emotion for getting attention,
[01:30:29.240 --> 01:30:30.880]   well, because it's not just about attention on the internet,
[01:30:30.880 --> 01:30:31.920]   it's engagement.
[01:30:31.920 --> 01:30:33.120]   That's the key thing, right?
[01:30:33.120 --> 01:30:34.400]   In order for something to go viral,
[01:30:34.400 --> 01:30:35.960]   you need people to actually engage with it.
[01:30:35.960 --> 01:30:38.600]   They need to like comment or retweet or whatever.
[01:30:38.600 --> 01:30:43.560]   And of all the emotions that, you know,
[01:30:43.560 --> 01:30:45.800]   there's like seven classic shared emotions
[01:30:45.800 --> 01:30:47.520]   that studies have found that all humans,
[01:30:47.520 --> 01:30:51.040]   even from like previously uncontacted tribes have.
[01:30:51.040 --> 01:30:56.080]   Some of those are negative, you know, like sadness,
[01:30:56.080 --> 01:30:58.880]   disgust, anger, et cetera, some are positive,
[01:30:58.880 --> 01:31:02.360]   happiness, excitement, and so on.
[01:31:02.360 --> 01:31:04.760]   The one that happens to be the most useful
[01:31:04.760 --> 01:31:06.680]   for the internet is anger
[01:31:06.680 --> 01:31:11.120]   because anger is such an active emotion.
[01:31:11.120 --> 01:31:13.160]   If you want people to engage,
[01:31:13.160 --> 01:31:14.280]   if someone's scared,
[01:31:14.280 --> 01:31:15.840]   and I'm not just like talking out my ass here,
[01:31:15.840 --> 01:31:17.840]   there are studies here that have looked into this.
[01:31:17.840 --> 01:31:22.440]   Whereas like if someone's like disgusted or fearful,
[01:31:22.440 --> 01:31:23.680]   they actually tend to then be like,
[01:31:23.680 --> 01:31:24.760]   oh, I don't wanna deal with this.
[01:31:24.760 --> 01:31:26.360]   So they're less likely to actually engage
[01:31:26.360 --> 01:31:27.520]   and share it and so on.
[01:31:27.520 --> 01:31:28.480]   They're just gonna be like, ugh.
[01:31:28.480 --> 01:31:30.960]   Whereas if they're enraged by a thing,
[01:31:31.040 --> 01:31:35.920]   well, now that triggers all the old tribalism emotions.
[01:31:35.920 --> 01:31:38.400]   And so that's how then things get sort of spread,
[01:31:38.400 --> 01:31:39.400]   you know, much more easily.
[01:31:39.400 --> 01:31:43.160]   They out-compete all the other memes in the ecosystem.
[01:31:43.160 --> 01:31:47.440]   And so this like, the attention economy,
[01:31:47.440 --> 01:31:50.520]   the wheels that make it go around is rage.
[01:31:50.520 --> 01:31:55.080]   I did a tweet, the problem with raging against the machine
[01:31:55.080 --> 01:31:57.280]   is that the machine has learned to feed off rage
[01:31:57.280 --> 01:31:59.080]   because it is feeding off our rage.
[01:31:59.080 --> 01:32:00.400]   That's the thing that's now keeping it going.
[01:32:00.400 --> 01:32:03.240]   So the more we get angry, the worse it gets.
[01:32:03.240 --> 01:32:06.040]   - So the mullet in this attention,
[01:32:06.040 --> 01:32:11.040]   in the war of attention is constantly maximizing rage.
[01:32:11.040 --> 01:32:13.600]   - What it is optimizing for is engagement.
[01:32:13.600 --> 01:32:18.600]   And it happens to be that engagement is, well, propaganda.
[01:32:18.600 --> 01:32:20.920]   You know, it's like, I mean,
[01:32:20.920 --> 01:32:23.240]   it just sounds like everything is putting,
[01:32:23.240 --> 01:32:24.320]   more and more things are being put through
[01:32:24.320 --> 01:32:26.000]   this like propagandist lens
[01:32:26.000 --> 01:32:28.880]   of winning whatever the war is in question,
[01:32:28.880 --> 01:32:31.400]   whether it's the culture war or the Ukraine war, yeah.
[01:32:31.400 --> 01:32:33.240]   - Well, I think the silver lining of this,
[01:32:33.240 --> 01:32:36.520]   do you think it's possible that in the long arc
[01:32:36.520 --> 01:32:39.560]   of this process, you actually do arrive
[01:32:39.560 --> 01:32:41.800]   at greater wisdom and more progress?
[01:32:41.800 --> 01:32:43.920]   It just, in the moment, it feels like people
[01:32:43.920 --> 01:32:47.760]   are tearing each other to shreds over ideas.
[01:32:47.760 --> 01:32:48.600]   But if you think about it,
[01:32:48.600 --> 01:32:51.120]   one of the magic things about democracy and so on
[01:32:51.120 --> 01:32:53.880]   is you have the blue versus red constantly fighting.
[01:32:53.880 --> 01:32:58.160]   It's almost like they're in discourse,
[01:32:58.160 --> 01:32:59.720]   creating devil's advocate,
[01:32:59.720 --> 01:33:01.240]   making devils out of each other.
[01:33:01.240 --> 01:33:04.880]   And through that process, discussing ideas,
[01:33:04.880 --> 01:33:07.680]   like almost really embodying different ideas,
[01:33:07.680 --> 01:33:08.840]   just to yell at each other.
[01:33:08.840 --> 01:33:11.600]   And through the yelling over the period of decades,
[01:33:11.600 --> 01:33:15.320]   maybe centuries, figuring out a better system.
[01:33:15.320 --> 01:33:17.280]   Like in the moment, it feels fucked up.
[01:33:17.280 --> 01:33:18.120]   - Right.
[01:33:18.120 --> 01:33:20.760]   - But in the long arc, it actually is productive.
[01:33:20.760 --> 01:33:21.600]   - I hope so.
[01:33:23.320 --> 01:33:28.320]   That said, we are now in the era of,
[01:33:28.320 --> 01:33:30.480]   just as we have weapons of mass destruction
[01:33:30.480 --> 01:33:32.400]   with nuclear weapons,
[01:33:32.400 --> 01:33:35.240]   that can break the whole playing field.
[01:33:35.240 --> 01:33:37.680]   We now are developing weapons
[01:33:37.680 --> 01:33:39.400]   of informational mass destruction,
[01:33:39.400 --> 01:33:43.560]   information weapons, WMDs that basically
[01:33:43.560 --> 01:33:47.720]   can be used for propaganda or just manipulating people
[01:33:47.720 --> 01:33:51.080]   however it's needed, whether that's through
[01:33:51.080 --> 01:33:54.040]   dumb TikTok videos or, you know,
[01:33:54.040 --> 01:33:59.040]   there are significant resources being put in.
[01:33:59.040 --> 01:34:01.440]   I don't mean to sound like, you know,
[01:34:01.440 --> 01:34:04.240]   too doom and gloom, but there are bad actors out there.
[01:34:04.240 --> 01:34:05.080]   That's the thing.
[01:34:05.080 --> 01:34:06.640]   There are plenty of good actors within the system
[01:34:06.640 --> 01:34:08.240]   who are just trying to stay afloat in the game.
[01:34:08.240 --> 01:34:09.760]   So effectively doing monarchy things.
[01:34:09.760 --> 01:34:12.480]   But then on top of that, we have actual bad actors
[01:34:12.480 --> 01:34:14.960]   who are intentionally trying to like,
[01:34:14.960 --> 01:34:17.360]   manipulate the other side into doing things.
[01:34:17.360 --> 01:34:19.640]   - And using, so because of the digital space,
[01:34:19.640 --> 01:34:24.440]   they're able to use artificial actors, meaning bots.
[01:34:24.440 --> 01:34:26.120]   - Exactly, botnets, you know,
[01:34:26.120 --> 01:34:29.720]   and this is a whole new situation
[01:34:29.720 --> 01:34:31.160]   that we've never had before.
[01:34:31.160 --> 01:34:32.280]   - It's exciting.
[01:34:32.280 --> 01:34:34.000]   You know what I wanna do? - Is it?
[01:34:34.000 --> 01:34:36.160]   - You know what I wanna do?
[01:34:36.160 --> 01:34:37.120]   Because there is, you know,
[01:34:37.120 --> 01:34:38.920]   people are talking about bots manipulating
[01:34:38.920 --> 01:34:41.600]   and have like malicious bots
[01:34:41.600 --> 01:34:43.560]   that are basically spreading propaganda.
[01:34:43.560 --> 01:34:46.560]   I wanna create like a bot army for like,
[01:34:46.560 --> 01:34:47.600]   that like fights that. - For love?
[01:34:47.600 --> 01:34:48.680]   - Yeah, exactly, for love.
[01:34:48.680 --> 01:34:50.960]   That fights, that, I mean.
[01:34:50.960 --> 01:34:53.480]   - You know, there's truth to fight fire with fire.
[01:34:53.480 --> 01:34:57.080]   It's like, but how, you always have to be careful
[01:34:57.080 --> 01:34:59.960]   whenever you create, again, like,
[01:34:59.960 --> 01:35:01.360]   Moloch is very tricky.
[01:35:01.360 --> 01:35:04.240]   - Yeah, yeah, Hitler was trying to spread love too.
[01:35:04.240 --> 01:35:05.440]   - Yeah, so we thought, but you know,
[01:35:05.440 --> 01:35:07.320]   I agree with you that like,
[01:35:07.320 --> 01:35:09.160]   that is a thing that should be considered,
[01:35:09.160 --> 01:35:11.440]   but there is, again, everyone,
[01:35:11.440 --> 01:35:13.560]   the road to hell is paved in good intentions.
[01:35:13.560 --> 01:35:18.560]   And there's always unforeseen circumstances, you know,
[01:35:18.560 --> 01:35:21.440]   outcomes, externalities, if you're trying to adopt a thing,
[01:35:21.440 --> 01:35:23.600]   even if you do it in the very best of faith.
[01:35:23.600 --> 01:35:25.040]   - But you can learn lessons of history.
[01:35:25.040 --> 01:35:28.600]   - If you can run some Sims on it first, absolutely.
[01:35:28.600 --> 01:35:31.000]   - But also there's certain aspects of a system,
[01:35:31.000 --> 01:35:32.240]   as we've learned through history,
[01:35:32.240 --> 01:35:34.000]   that do better than others.
[01:35:34.000 --> 01:35:36.120]   Like, for example, don't have a dictator.
[01:35:36.120 --> 01:35:39.640]   So like, if I were to create this bot army,
[01:35:39.640 --> 01:35:42.920]   it's not good for me to have full control over it.
[01:35:42.920 --> 01:35:44.000]   Because in the beginning,
[01:35:44.000 --> 01:35:46.720]   I might have a good understanding of what's good and not.
[01:35:46.720 --> 01:35:49.200]   But over time, that starts to get deviated,
[01:35:49.200 --> 01:35:50.720]   'cause I'll get annoyed at some assholes,
[01:35:50.720 --> 01:35:51.920]   and I'll think, okay,
[01:35:51.920 --> 01:35:54.040]   wouldn't it be nice to get rid of those assholes?
[01:35:54.040 --> 01:35:55.680]   But then that power starts getting to your head,
[01:35:55.680 --> 01:35:56.880]   you become corrupted.
[01:35:56.880 --> 01:35:58.080]   That's basic human nature.
[01:35:58.080 --> 01:35:59.960]   So distribute the power somehow.
[01:35:59.960 --> 01:36:04.480]   - We need a love botnet on a DAO.
[01:36:04.480 --> 01:36:07.800]   A DAO love botnet.
[01:36:07.800 --> 01:36:10.040]   - Yeah, but, and without a leader.
[01:36:10.040 --> 01:36:10.880]   Like without-
[01:36:10.880 --> 01:36:12.360]   - Exactly, a distributed, right.
[01:36:12.360 --> 01:36:14.320]   But yeah, without any kind of centralized-
[01:36:14.320 --> 01:36:16.120]   - Yeah, without even, you know,
[01:36:16.120 --> 01:36:17.520]   basically is the more control,
[01:36:17.520 --> 01:36:21.440]   the more you can decentralize the control of a thing
[01:36:21.440 --> 01:36:24.880]   to people, you know, but the balance-
[01:36:24.880 --> 01:36:26.720]   - But then you still need the ability to coordinate,
[01:36:26.720 --> 01:36:29.440]   because that's the issue when if something is too,
[01:36:29.440 --> 01:36:32.440]   you know, that's really, to me, like the culture wars,
[01:36:32.440 --> 01:36:34.640]   the bigger war we're dealing with is actually
[01:36:34.640 --> 01:36:38.320]   between the, like the sort of the,
[01:36:38.320 --> 01:36:39.600]   I don't know what even the term is for it,
[01:36:39.600 --> 01:36:42.000]   but like centralization versus decentralization.
[01:36:42.000 --> 01:36:44.120]   That's the tension we're seeing.
[01:36:44.120 --> 01:36:48.240]   Power and control by a few versus completely distributed.
[01:36:48.240 --> 01:36:50.920]   And the trouble is if you have a fully centralized thing,
[01:36:50.920 --> 01:36:52.720]   then you're at risk of tyranny, you know,
[01:36:52.720 --> 01:36:56.880]   Stalin type things can happen, or completely distributed.
[01:36:56.880 --> 01:36:58.560]   Now you're at risk of complete anarchy and chaos
[01:36:58.560 --> 01:37:01.160]   where you can't even coordinate to like on, you know,
[01:37:01.160 --> 01:37:03.280]   when there's like a pandemic or anything like that.
[01:37:03.280 --> 01:37:05.840]   So it's like, what is the right balance to strike
[01:37:05.840 --> 01:37:08.120]   between these two structures?
[01:37:08.120 --> 01:37:09.720]   - Can't Moloch really take hold
[01:37:09.720 --> 01:37:11.080]   in a fully decentralized system?
[01:37:11.080 --> 01:37:12.560]   That's the one of the dangers too.
[01:37:12.560 --> 01:37:13.720]   - Yes.
[01:37:13.720 --> 01:37:14.560]   - So you're very vulnerable to Moloch.
[01:37:14.560 --> 01:37:17.760]   - So the dictator can commit huge atrocities,
[01:37:17.760 --> 01:37:21.000]   but they can also make sure the infrastructure works
[01:37:21.000 --> 01:37:23.240]   and trains what I'm talking about.
[01:37:23.240 --> 01:37:24.880]   - They have that God's eye view, at least.
[01:37:24.880 --> 01:37:27.360]   They have the ability to create like laws and rules
[01:37:27.360 --> 01:37:30.960]   that like force coordination, which stops Moloch.
[01:37:30.960 --> 01:37:33.280]   But then you're vulnerable to that dictator
[01:37:33.280 --> 01:37:34.680]   getting infected with like this,
[01:37:34.680 --> 01:37:37.000]   with some kind of psychopathy type thing.
[01:37:37.000 --> 01:37:39.480]   - What's reverse Moloch?
[01:37:39.480 --> 01:37:40.640]   - So great question.
[01:37:40.640 --> 01:37:45.640]   So that's where, so I've been working on this series.
[01:37:45.640 --> 01:37:48.320]   It's been driving me insane for the last year and a half.
[01:37:48.320 --> 01:37:50.120]   I did the first one a year ago.
[01:37:50.120 --> 01:37:52.120]   I can't believe it's nearly been a year.
[01:37:52.120 --> 01:37:54.800]   The second one, hopefully we're coming out in like a month.
[01:37:54.800 --> 01:37:58.760]   And my goal at the end of the series is to like present,
[01:37:58.760 --> 01:37:59.960]   'cause basically I'm painting the picture
[01:37:59.960 --> 01:38:02.320]   of like what Moloch is and how it's affecting
[01:38:02.320 --> 01:38:04.400]   almost all these issues in our society
[01:38:04.400 --> 01:38:06.480]   and how it's, you know, driving.
[01:38:06.480 --> 01:38:08.000]   It's like kind of the generator function
[01:38:08.000 --> 01:38:11.040]   as people describe it of existential risk.
[01:38:11.040 --> 01:38:11.880]   And then at the end of that-
[01:38:11.880 --> 01:38:14.200]   - Wait, wait, the generator function of existential risk.
[01:38:14.200 --> 01:38:16.480]   So you're saying Moloch is sort of the engine
[01:38:16.480 --> 01:38:19.680]   that creates a bunch of X risks.
[01:38:19.680 --> 01:38:20.720]   - Yes, not all of them.
[01:38:20.720 --> 01:38:22.960]   Like, you know, a-
[01:38:22.960 --> 01:38:24.520]   - It's a cool phrase, generator function.
[01:38:24.520 --> 01:38:26.800]   - It's not my phrase, it's Daniel Schmachtenberger.
[01:38:26.800 --> 01:38:28.120]   - Oh, Schmachtenberger. - I got that from him.
[01:38:28.120 --> 01:38:29.880]   - Of course. - All of these ideas.
[01:38:29.880 --> 01:38:32.720]   - It's like all roads lead back to Daniel Schmachtenberger.
[01:38:32.720 --> 01:38:34.560]   - The dude is brilliant.
[01:38:34.560 --> 01:38:35.400]   He's really, really brilliant.
[01:38:35.400 --> 01:38:36.920]   - And after that it's Mark Twain.
[01:38:37.800 --> 01:38:41.120]   - But anyway, sorry, totally rude interruptions from me.
[01:38:41.120 --> 01:38:42.000]   - No, it's fine.
[01:38:42.000 --> 01:38:43.080]   So not all X risks.
[01:38:43.080 --> 01:38:45.440]   So like an asteroid technically isn't
[01:38:45.440 --> 01:38:49.440]   because it's just like this one big external thing.
[01:38:49.440 --> 01:38:51.520]   It's not like a competition thing going on,
[01:38:51.520 --> 01:38:55.800]   but, you know, synthetic bio, you know, bioweapons,
[01:38:55.800 --> 01:38:58.600]   that's one because everyone's incentivized to build,
[01:38:58.600 --> 01:39:01.040]   even for defense, you know, bad viruses,
[01:39:01.040 --> 01:39:03.720]   you know, just to threaten someone else, et cetera.
[01:39:03.720 --> 01:39:05.760]   Or AI technically, the race to AGI
[01:39:05.760 --> 01:39:08.040]   is kind of potentially a molecule situation.
[01:39:08.040 --> 01:39:14.560]   But yeah, so if Moloch is this like generator function
[01:39:14.560 --> 01:39:17.560]   that's driving all of these issues over the coming century
[01:39:17.560 --> 01:39:20.240]   that might wipe us out, what's the inverse?
[01:39:20.240 --> 01:39:24.040]   And so far what I've gotten to is this character
[01:39:24.040 --> 01:39:26.600]   that I want to put out there called Win-Win.
[01:39:26.600 --> 01:39:28.600]   Because Moloch is the God of lose-lose ultimately.
[01:39:28.600 --> 01:39:30.200]   It masquerades as the God of win-lose,
[01:39:30.200 --> 01:39:31.720]   but in reality it's lose-lose.
[01:39:31.720 --> 01:39:33.400]   Everyone ends up worse off.
[01:39:34.320 --> 01:39:35.800]   So I was like, well, what's the opposite of that?
[01:39:35.800 --> 01:39:36.640]   It's Win-Win.
[01:39:36.640 --> 01:39:37.520]   And I was thinking for ages,
[01:39:37.520 --> 01:39:39.680]   like what's a good name for this character?
[01:39:39.680 --> 01:39:42.360]   And then the more I was like, okay, well,
[01:39:42.360 --> 01:39:44.640]   don't try and think through it logically.
[01:39:44.640 --> 01:39:46.480]   What's the vibe of Win-Win?
[01:39:46.480 --> 01:39:49.160]   And to me, like in my mind, Moloch is like,
[01:39:49.160 --> 01:39:52.080]   and I addressed it in the video, like it's red and black.
[01:39:52.080 --> 01:39:54.760]   It's kind of like very, you know,
[01:39:54.760 --> 01:39:57.480]   hyper-focused on its one goal, you must win.
[01:39:57.480 --> 01:40:01.360]   So Win-Win is kind of actually like these colors.
[01:40:01.360 --> 01:40:02.920]   It's like purple, turquoise.
[01:40:04.080 --> 01:40:06.640]   It loves games too.
[01:40:06.640 --> 01:40:08.320]   It loves a little bit of healthy competition,
[01:40:08.320 --> 01:40:10.640]   but constrained, like kind of like before.
[01:40:10.640 --> 01:40:12.760]   Knows how to ring fence zero-sum competition
[01:40:12.760 --> 01:40:14.880]   into like just the right amount,
[01:40:14.880 --> 01:40:17.400]   whereby its externalities can be controlled
[01:40:17.400 --> 01:40:19.120]   and kept positive.
[01:40:19.120 --> 01:40:21.240]   And then beyond that, it also loves cooperation,
[01:40:21.240 --> 01:40:23.920]   coordination, love, all these other things.
[01:40:23.920 --> 01:40:25.880]   But it's also kind of like mischievous.
[01:40:25.880 --> 01:40:28.480]   Like, you know, it will have a good time.
[01:40:28.480 --> 01:40:30.640]   It's not like kind of like boring, you know, like,
[01:40:30.640 --> 01:40:33.920]   oh God, it knows how to have fun.
[01:40:33.920 --> 01:40:36.000]   It can get down.
[01:40:36.000 --> 01:40:39.760]   But ultimately it's like unbelievably wise
[01:40:39.760 --> 01:40:41.800]   and it just wants the game to keep going.
[01:40:41.800 --> 01:40:44.600]   And I call it Win-Win.
[01:40:44.600 --> 01:40:46.960]   - That's a good like pet name, Win-Win.
[01:40:46.960 --> 01:40:48.480]   - Yes, I think the-
[01:40:48.480 --> 01:40:49.320]   - Win-Win.
[01:40:49.320 --> 01:40:50.160]   - Win-Win, right?
[01:40:50.160 --> 01:40:51.280]   And I think its formal name,
[01:40:51.280 --> 01:40:55.080]   when it has to do like official functions, is Omnia.
[01:40:55.080 --> 01:40:55.920]   - Omnia.
[01:40:55.920 --> 01:40:56.760]   - Yeah.
[01:40:56.760 --> 01:40:59.560]   - From like omniscience, kind of, why Omnia?
[01:40:59.560 --> 01:41:00.400]   You just like Omnia?
[01:41:00.400 --> 01:41:01.240]   - Just like Omni-Win.
[01:41:01.240 --> 01:41:02.080]   - Omni-Win.
[01:41:02.080 --> 01:41:02.960]   - But I'm open to suggestions.
[01:41:02.960 --> 01:41:04.160]   So like, you know, and this is-
[01:41:04.160 --> 01:41:05.160]   - I like Omnia, yeah.
[01:41:05.160 --> 01:41:06.000]   - Yeah, like that's like-
[01:41:06.000 --> 01:41:08.800]   - But there is an angelic kind of sense to Omnia though.
[01:41:08.800 --> 01:41:10.040]   So Win-Win is more fun.
[01:41:10.040 --> 01:41:10.880]   - Exactly.
[01:41:10.880 --> 01:41:15.880]   - So it's more like, it embraces the fun aspect.
[01:41:15.880 --> 01:41:18.640]   I mean, there is something about sort of,
[01:41:18.640 --> 01:41:23.560]   there's some aspect to Win-Win interactions
[01:41:23.560 --> 01:41:28.560]   that requires embracing the chaos of the game
[01:41:31.640 --> 01:41:33.800]   and enjoying the game itself.
[01:41:33.800 --> 01:41:35.240]   I don't know, I don't know what that is.
[01:41:35.240 --> 01:41:37.720]   That's almost like a Zen-like appreciation
[01:41:37.720 --> 01:41:39.400]   of the game itself,
[01:41:39.400 --> 01:41:42.520]   not optimizing for the consequences of the game.
[01:41:42.520 --> 01:41:46.160]   - Right, well, it's recognizing the value of competition
[01:41:46.160 --> 01:41:48.720]   in of itself, it's not like about winning,
[01:41:48.720 --> 01:41:51.520]   it's about you enjoying the process of having a competition
[01:41:51.520 --> 01:41:52.680]   and not knowing whether you're gonna win
[01:41:52.680 --> 01:41:53.920]   or lose this little thing.
[01:41:53.920 --> 01:41:56.600]   But then also being aware that, you know,
[01:41:56.600 --> 01:41:57.440]   what's the boundary?
[01:41:57.440 --> 01:41:59.040]   How big do I want competition to be?
[01:41:59.080 --> 01:42:02.720]   Because one of the reason why Moloch is doing so well now
[01:42:02.720 --> 01:42:05.600]   in our civilization is because we haven't been able
[01:42:05.600 --> 01:42:07.000]   to ring fence competition.
[01:42:07.000 --> 01:42:10.600]   And so it's just having all these negative externalities
[01:42:10.600 --> 01:42:12.600]   and we've completely lost control of it.
[01:42:12.600 --> 01:42:18.640]   I think my guess is, and now we're getting really like,
[01:42:18.640 --> 01:42:20.880]   you know, metaphysical technically,
[01:42:20.880 --> 01:42:25.880]   but I think we'll be in a more interesting universe
[01:42:26.800 --> 01:42:29.720]   if we have one that has both pure cooperation,
[01:42:29.720 --> 01:42:31.360]   you know, lots of cooperation
[01:42:31.360 --> 01:42:33.000]   and some pockets of competition
[01:42:33.000 --> 01:42:36.240]   than one that's purely cooperation entirely.
[01:42:36.240 --> 01:42:39.360]   Like it's good to have some little zero-sum-ness bits,
[01:42:39.360 --> 01:42:41.520]   but I don't know that fully
[01:42:41.520 --> 01:42:44.120]   and I'm not qualified as a philosopher to know that.
[01:42:44.120 --> 01:42:45.480]   - And that's what reverse Moloch,
[01:42:45.480 --> 01:42:49.640]   so this kind of win-win creature system
[01:42:49.640 --> 01:42:52.160]   is an antidote to the Moloch system.
[01:42:52.160 --> 01:42:53.600]   - Yes.
[01:42:53.600 --> 01:42:56.360]   And I don't know how it's gonna do that.
[01:42:56.360 --> 01:43:00.000]   - But it's good to kind of try to start
[01:43:00.000 --> 01:43:01.480]   to formulate different ideas,
[01:43:01.480 --> 01:43:03.840]   different frameworks of how we think about that.
[01:43:03.840 --> 01:43:04.680]   - Exactly.
[01:43:04.680 --> 01:43:07.240]   - At the small scale of a collection of individuals
[01:43:07.240 --> 01:43:09.120]   and a large scale of a society.
[01:43:09.120 --> 01:43:09.960]   - Exactly.
[01:43:09.960 --> 01:43:13.360]   It's a meme, I think it's an example of a good meme.
[01:43:13.360 --> 01:43:15.560]   And I'm open, I'd love to hear feedback from people
[01:43:15.560 --> 01:43:16.720]   if they think it's, you know,
[01:43:16.720 --> 01:43:18.320]   they have a better idea or it's not, you know,
[01:43:18.320 --> 01:43:21.640]   but it's the direction of memes that we need to spread,
[01:43:21.640 --> 01:43:25.160]   this idea of like, look for the win-wins in life.
[01:43:25.160 --> 01:43:27.240]   - Well, on the topic of beauty filters,
[01:43:27.240 --> 01:43:31.800]   so in that particular context where Moloch creates
[01:43:31.800 --> 01:43:35.160]   negative consequences, you know,
[01:43:35.160 --> 01:43:37.160]   Dostoevsky said beauty will save the world.
[01:43:37.160 --> 01:43:39.140]   What is beauty anyway?
[01:43:39.140 --> 01:43:43.800]   It would be nice to just try to discuss
[01:43:43.800 --> 01:43:49.320]   what kind of thing we would like to converge towards
[01:43:49.320 --> 01:43:52.260]   in our understanding of what is beautiful.
[01:43:52.260 --> 01:43:59.200]   - So to me, I think something is beautiful
[01:43:59.200 --> 01:44:04.200]   when it can't be reduced down to easy metrics.
[01:44:04.200 --> 01:44:10.720]   Like if you think of a tree, what is it about a tree,
[01:44:10.720 --> 01:44:12.280]   like a big ancient beautiful tree, right?
[01:44:12.280 --> 01:44:14.600]   What is it about it that we find so beautiful?
[01:44:14.680 --> 01:44:19.680]   It's not, you know, the sweetness of its fruit
[01:44:19.680 --> 01:44:23.480]   or the value of its lumber.
[01:44:23.480 --> 01:44:28.480]   It's this entirety of it that is,
[01:44:28.480 --> 01:44:30.880]   there's these immeasurable qualities.
[01:44:30.880 --> 01:44:32.920]   It's like almost like a qualia of it.
[01:44:32.920 --> 01:44:37.360]   That's both, like it walks this fine line between,
[01:44:37.360 --> 01:44:38.740]   well, it's got lots of patonicity,
[01:44:38.740 --> 01:44:41.080]   but it's not overly predictable.
[01:44:41.080 --> 01:44:42.200]   You know, again, it walks this fine line
[01:44:42.200 --> 01:44:43.240]   between order and chaos.
[01:44:43.240 --> 01:44:45.500]   It's a very highly complex system.
[01:44:45.500 --> 01:44:51.440]   And you know, you can't, it's evolving over time.
[01:44:51.440 --> 01:44:53.280]   You know, the definition of a complex versus,
[01:44:53.280 --> 01:44:54.760]   and this is another Schmacktenberger thing,
[01:44:54.760 --> 01:44:57.660]   you know, a complex versus a complicated system.
[01:44:57.660 --> 01:45:00.600]   A complicated system can be sort of broken down into bits,
[01:45:00.600 --> 01:45:02.240]   understood and then put back together.
[01:45:02.240 --> 01:45:05.000]   A complex system is kind of like a black box.
[01:45:05.000 --> 01:45:08.200]   It does all this crazy stuff, but if you take it apart,
[01:45:08.200 --> 01:45:09.340]   you can't put it back together again
[01:45:09.340 --> 01:45:11.800]   because there's all these intricacies.
[01:45:11.800 --> 01:45:15.000]   And also very importantly, like the sum of the parts,
[01:45:15.000 --> 01:45:16.600]   sorry, the sum of the whole is much greater
[01:45:16.600 --> 01:45:17.880]   than the sum of the parts.
[01:45:17.880 --> 01:45:21.400]   And that's where the beauty lies, I think.
[01:45:21.400 --> 01:45:23.560]   And I think that extends to things like art as well.
[01:45:23.560 --> 01:45:27.640]   Like there's something immeasurable about it.
[01:45:27.640 --> 01:45:30.000]   There's something we can't break down to a narrow metric.
[01:45:30.000 --> 01:45:31.840]   - Does that extend to humans, you think?
[01:45:31.840 --> 01:45:33.840]   - Yeah, absolutely.
[01:45:33.840 --> 01:45:37.440]   - So how can Instagram reveal that kind of beauty,
[01:45:37.440 --> 01:45:39.320]   the complexity of a human being?
[01:45:39.320 --> 01:45:40.160]   - Good question.
[01:45:41.160 --> 01:45:44.880]   - And this takes us back to dating sites and Goodreads,
[01:45:44.880 --> 01:45:45.720]   I think.
[01:45:45.720 --> 01:45:47.800]   - Very good question.
[01:45:47.800 --> 01:45:50.320]   I mean, well, I know what it shouldn't do.
[01:45:50.320 --> 01:45:53.320]   It shouldn't try and like, right now, you know,
[01:45:53.320 --> 01:45:56.240]   one of the, I was talking to like a social media expert
[01:45:56.240 --> 01:45:58.320]   recently 'cause I was like, ugh, I hate-
[01:45:58.320 --> 01:45:59.960]   - Is there such a thing as a social media expert?
[01:45:59.960 --> 01:46:02.080]   - Oh yeah, there are like agencies out there
[01:46:02.080 --> 01:46:03.280]   that you can like outsource.
[01:46:03.280 --> 01:46:06.040]   'Cause I'm thinking about working with one to like,
[01:46:06.040 --> 01:46:08.560]   I wanna start a podcast.
[01:46:08.560 --> 01:46:09.440]   - Yes.
[01:46:09.440 --> 01:46:11.920]   You should, you should have done it a long time ago.
[01:46:11.920 --> 01:46:12.920]   - Working on it.
[01:46:12.920 --> 01:46:14.760]   It's gonna be called Win-Win.
[01:46:14.760 --> 01:46:17.480]   And it's gonna be about this like positive stuff.
[01:46:17.480 --> 01:46:21.000]   And the thing that, you know, they always come back
[01:46:21.000 --> 01:46:23.000]   and say, it's like, well, you need to like figure out
[01:46:23.000 --> 01:46:24.080]   what your thing is.
[01:46:24.080 --> 01:46:26.440]   You know, you need to narrow down what your thing is
[01:46:26.440 --> 01:46:28.000]   and then just follow that.
[01:46:28.000 --> 01:46:31.080]   Have a like a sort of a formula
[01:46:31.080 --> 01:46:32.200]   because that's what people want.
[01:46:32.200 --> 01:46:34.320]   They wanna know that they're coming back to the same thing.
[01:46:34.320 --> 01:46:37.480]   And that's the advice on YouTube, Twitter, you name it.
[01:46:37.480 --> 01:46:40.240]   And that's why, and the trouble with that
[01:46:40.240 --> 01:46:42.480]   is that it's a complexity reduction.
[01:46:42.480 --> 01:46:44.120]   And generally speaking, you know,
[01:46:44.120 --> 01:46:45.400]   complexity reduction is bad.
[01:46:45.400 --> 01:46:48.040]   It's making things more, it's an oversimplification.
[01:46:48.040 --> 01:46:50.240]   Not that simplification is a bad thing,
[01:46:50.240 --> 01:46:55.520]   but when you're trying to take, you know,
[01:46:55.520 --> 01:46:56.560]   what is social media doing?
[01:46:56.560 --> 01:47:00.120]   It's trying to like encapsulate the human experience
[01:47:00.120 --> 01:47:04.080]   and put it into digital form and commodify it to an extent.
[01:47:05.720 --> 01:47:07.600]   And so you do that, you compress people down
[01:47:07.600 --> 01:47:09.520]   into these like narrow things.
[01:47:09.520 --> 01:47:12.680]   And that's why I think it's kind of ultimately
[01:47:12.680 --> 01:47:13.920]   fundamentally incompatible
[01:47:13.920 --> 01:47:15.240]   with at least my definition of beauty.
[01:47:15.240 --> 01:47:18.120]   - Yeah, it's interesting because there is some sense
[01:47:18.120 --> 01:47:23.120]   in which a simplification sort of in the Einstein
[01:47:23.120 --> 01:47:28.040]   kind of sense of a really complex idea,
[01:47:28.040 --> 01:47:30.280]   a simplification in a way that still captures
[01:47:30.280 --> 01:47:35.280]   some core power of an idea of a person is also beautiful.
[01:47:36.120 --> 01:47:39.360]   And so maybe it's possible for social media to do that.
[01:47:39.360 --> 01:47:44.360]   A presentation, sort of a slither, a slice,
[01:47:44.360 --> 01:47:46.360]   a look into a person's life
[01:47:46.360 --> 01:47:50.520]   that reveals something real about them,
[01:47:50.520 --> 01:47:53.240]   but in a simple way, in a way that can be displayed
[01:47:53.240 --> 01:47:55.600]   graphically or through words.
[01:47:55.600 --> 01:47:57.680]   Some way, I mean, in some way,
[01:47:57.680 --> 01:47:59.960]   Twitter can do that kind of thing.
[01:47:59.960 --> 01:48:03.080]   A very few set of words can reveal
[01:48:03.080 --> 01:48:04.920]   the intricacies of a person.
[01:48:04.920 --> 01:48:09.920]   Of course, the viral machine that spreads those words
[01:48:09.920 --> 01:48:14.840]   often results in people taking the thing out of context.
[01:48:14.840 --> 01:48:18.920]   People often don't read tweets in the context
[01:48:18.920 --> 01:48:20.800]   of the human being that wrote them.
[01:48:20.800 --> 01:48:24.080]   The full history of the tweets they've written,
[01:48:24.080 --> 01:48:26.560]   the education level, the humor level,
[01:48:26.560 --> 01:48:30.440]   the worldview they're playing around with,
[01:48:30.440 --> 01:48:31.760]   all that context is forgotten
[01:48:31.760 --> 01:48:33.640]   and people just see the different words.
[01:48:33.640 --> 01:48:35.520]   So that can lead to trouble.
[01:48:35.520 --> 01:48:39.840]   But in a certain sense, if you do take it in context,
[01:48:39.840 --> 01:48:43.240]   it reveals some kind of quirky little beautiful idea
[01:48:43.240 --> 01:48:47.200]   or a profound little idea from that particular person
[01:48:47.200 --> 01:48:48.560]   that shows something about that person.
[01:48:48.560 --> 01:48:51.320]   So in that sense, Twitter can be more successful.
[01:48:51.320 --> 01:48:53.080]   If we're talking about Mullicks,
[01:48:53.080 --> 01:48:56.080]   is driving a better kind of incentive.
[01:48:56.080 --> 01:48:59.600]   - Yeah, I mean, how they can,
[01:48:59.600 --> 01:49:02.040]   like if we were to rewrite,
[01:49:02.040 --> 01:49:05.480]   is there a way to rewrite the Twitter algorithm
[01:49:05.480 --> 01:49:08.800]   so that it stops being the like,
[01:49:08.800 --> 01:49:12.480]   the fertile breeding ground of the culture wars?
[01:49:12.480 --> 01:49:13.720]   Because that's really what it is.
[01:49:13.720 --> 01:49:17.560]   It's, I mean, maybe I'm giving it,
[01:49:17.560 --> 01:49:19.400]   Twitter too much power,
[01:49:19.400 --> 01:49:21.800]   but just the more I looked into it
[01:49:21.800 --> 01:49:25.640]   and I had conversations with Tristan Harris
[01:49:25.640 --> 01:49:27.680]   from the Center of Humane Technology
[01:49:27.680 --> 01:49:29.120]   and he explained it as like,
[01:49:30.880 --> 01:49:34.160]   Twitter is where you have this amalgam of human culture
[01:49:34.160 --> 01:49:36.280]   and then this terribly designed algorithm
[01:49:36.280 --> 01:49:38.480]   that amplifies the craziest people
[01:49:38.480 --> 01:49:44.840]   and the angriest, most divisive takes and amplifies them.
[01:49:44.840 --> 01:49:47.800]   And then the media, the mainstream media,
[01:49:47.800 --> 01:49:49.920]   because all the journalists are also on Twitter,
[01:49:49.920 --> 01:49:52.920]   they then are informed by that.
[01:49:52.920 --> 01:49:55.120]   And so they draw out the stories they can
[01:49:55.120 --> 01:49:59.120]   from this already like very boiling lava of rage.
[01:50:00.760 --> 01:50:04.440]   And then spread that to their millions and millions of people
[01:50:04.440 --> 01:50:05.800]   who aren't even on Twitter.
[01:50:05.800 --> 01:50:10.840]   And so I honestly, I think if I could press a button,
[01:50:10.840 --> 01:50:13.640]   turn them off, I probably would at this point,
[01:50:13.640 --> 01:50:16.160]   'cause I just don't see a way of being compatible
[01:50:16.160 --> 01:50:18.600]   with healthiness, but that's not gonna happen.
[01:50:18.600 --> 01:50:23.160]   And so at least one way to like stem the tide
[01:50:23.160 --> 01:50:26.160]   and make it less molochey would be to change,
[01:50:30.040 --> 01:50:31.960]   at least if like it was on a subscription model,
[01:50:31.960 --> 01:50:37.040]   then it's now not optimizing for impressions,
[01:50:37.040 --> 01:50:38.320]   'cause basically what it wants is for people
[01:50:38.320 --> 01:50:40.240]   to keep coming back as often as possible.
[01:50:40.240 --> 01:50:42.040]   That's how they get paid, right?
[01:50:42.040 --> 01:50:43.760]   Every time an ad gets shown to someone
[01:50:43.760 --> 01:50:44.680]   and the way is to get people
[01:50:44.680 --> 01:50:46.520]   constantly refreshing their feed.
[01:50:46.520 --> 01:50:49.400]   So you're trying to encourage addictive behaviors.
[01:50:49.400 --> 01:50:52.320]   Whereas if someone, if they moved on
[01:50:52.320 --> 01:50:53.960]   to at least a subscription model,
[01:50:53.960 --> 01:50:56.960]   then they're getting the money either way,
[01:50:56.960 --> 01:50:59.160]   whether someone comes back to the site once a month
[01:50:59.160 --> 01:51:02.080]   or 500 times a month, they get the same amount of money.
[01:51:02.080 --> 01:51:06.440]   So now that takes away that incentive to use technology,
[01:51:06.440 --> 01:51:08.200]   to build, to design an algorithm
[01:51:08.200 --> 01:51:09.600]   that is maximally addictive.
[01:51:09.600 --> 01:51:12.520]   That would be one way, for example.
[01:51:12.520 --> 01:51:14.640]   - Yeah, but you still want people to,
[01:51:14.640 --> 01:51:17.280]   yeah, I just feel like that just slows down,
[01:51:17.280 --> 01:51:20.960]   creates friction in the virality of things.
[01:51:20.960 --> 01:51:22.280]   - But that's good.
[01:51:22.280 --> 01:51:24.440]   We need to slow down virality.
[01:51:24.440 --> 01:51:26.600]   - It's good, it's one way.
[01:51:26.600 --> 01:51:28.960]   Virality is Moloch, to be clear.
[01:51:28.960 --> 01:51:34.320]   - So Moloch is always negative then?
[01:51:34.320 --> 01:51:36.520]   - Yes, by definition.
[01:51:36.520 --> 01:51:37.360]   - Yes.
[01:51:37.360 --> 01:51:38.200]   - Competition. - But then I disagree with you.
[01:51:38.200 --> 01:51:39.440]   - Competition is not always negative.
[01:51:39.440 --> 01:51:40.520]   Competition is neutral.
[01:51:40.520 --> 01:51:44.200]   - I disagree with you that all virality is negative then,
[01:51:44.200 --> 01:51:45.960]   is Moloch then.
[01:51:45.960 --> 01:51:49.520]   Because it's a good intuition
[01:51:49.520 --> 01:51:52.880]   'cause we have a lot of data on virality being negative.
[01:51:52.920 --> 01:51:57.120]   But I happen to believe that the core of human beings,
[01:51:57.120 --> 01:52:00.400]   so most human beings want to be good
[01:52:00.400 --> 01:52:03.400]   more than they want to be bad to each other.
[01:52:03.400 --> 01:52:05.720]   And so I think it's possible.
[01:52:05.720 --> 01:52:07.560]   It might be just harder to engineer
[01:52:07.560 --> 01:52:10.560]   systems that enable virality,
[01:52:10.560 --> 01:52:13.880]   but it's possible to engineer systems that are viral
[01:52:13.880 --> 01:52:15.720]   that enable virality.
[01:52:15.720 --> 01:52:19.160]   And the kind of stuff that rises to the top
[01:52:19.160 --> 01:52:21.400]   is things that are positive.
[01:52:21.400 --> 01:52:24.160]   And positive not like la la positive,
[01:52:24.160 --> 01:52:25.840]   it's more like win-win,
[01:52:25.840 --> 01:52:28.240]   meaning a lot of people need to be challenged.
[01:52:28.240 --> 01:52:29.640]   - Wise things, yes.
[01:52:29.640 --> 01:52:31.480]   - You grow from it, it might challenge you,
[01:52:31.480 --> 01:52:34.720]   you might not like it, but you ultimately grow from it.
[01:52:34.720 --> 01:52:36.720]   - And ultimately bring people together
[01:52:36.720 --> 01:52:38.480]   as opposed to tear them apart.
[01:52:38.480 --> 01:52:40.560]   I deeply want that to be true.
[01:52:40.560 --> 01:52:42.320]   And I very much agree with you
[01:52:42.320 --> 01:52:44.600]   that people at their core are on average good,
[01:52:44.600 --> 01:52:47.280]   as opposed to, care for each other, as opposed to not.
[01:52:47.280 --> 01:52:50.920]   I think it's actually a very small percentage of people
[01:52:50.920 --> 01:52:54.440]   are truly wanting to do just destructive malicious things.
[01:52:54.440 --> 01:52:56.520]   Most people are just trying to win their own little game
[01:52:56.520 --> 01:52:57.880]   and they don't mean to be,
[01:52:57.880 --> 01:53:00.280]   they're just stuck in this badly designed system.
[01:53:00.280 --> 01:53:04.640]   That said, the current structure, yes,
[01:53:04.640 --> 01:53:09.160]   is the current structure means that virality
[01:53:09.160 --> 01:53:10.800]   is optimized towards Moloch.
[01:53:10.800 --> 01:53:12.400]   That doesn't mean there aren't exceptions.
[01:53:12.400 --> 01:53:14.080]   Sometimes positive stories do go viral
[01:53:14.080 --> 01:53:15.160]   and I think we should study them.
[01:53:15.160 --> 01:53:17.080]   I think there should be a whole field of study
[01:53:17.080 --> 01:53:20.360]   into understanding, identifying memes
[01:53:20.360 --> 01:53:24.040]   that above a certain threshold of the population
[01:53:24.040 --> 01:53:27.360]   agree is a positive, happy, bringing people together meme,
[01:53:27.360 --> 01:53:29.880]   the kind of thing that brings families together
[01:53:29.880 --> 01:53:31.720]   that would normally argue about cultural stuff
[01:53:31.720 --> 01:53:34.800]   at the table, at the dinner table.
[01:53:34.800 --> 01:53:37.440]   Identify those memes and figure out what it was,
[01:53:37.440 --> 01:53:40.480]   what was the ingredient that made them spread that day.
[01:53:40.480 --> 01:53:44.760]   - And also like, not just like happiness
[01:53:44.760 --> 01:53:45.960]   and connection between humans,
[01:53:45.960 --> 01:53:49.600]   but connection between humans in other ways
[01:53:49.600 --> 01:53:52.720]   that enables like productivity, like cooperation,
[01:53:52.720 --> 01:53:56.240]   solving difficult problems and all those kinds of stuff.
[01:53:56.240 --> 01:53:59.040]   So it's not just about let's be happy
[01:53:59.040 --> 01:54:00.840]   and have a fulfilling lives.
[01:54:00.840 --> 01:54:03.000]   It's also like, let's build cool shit.
[01:54:03.000 --> 01:54:03.840]   - Yeah.
[01:54:03.840 --> 01:54:05.120]   Which is the spirit of collaboration,
[01:54:05.120 --> 01:54:06.480]   which is deeply anti-Moloch.
[01:54:06.480 --> 01:54:09.320]   It's not using competition.
[01:54:09.320 --> 01:54:13.160]   It's like, Moloch hates collaboration and coordination
[01:54:13.160 --> 01:54:14.600]   and people working together.
[01:54:14.600 --> 01:54:18.080]   And that's, again, like the internet started out as that
[01:54:18.080 --> 01:54:20.680]   and it could have been that,
[01:54:20.680 --> 01:54:23.360]   but because of the way it was sort of structured
[01:54:23.360 --> 01:54:26.880]   in terms of, you know, very lofty ideal,
[01:54:26.880 --> 01:54:28.960]   they wanted everything to be open source,
[01:54:28.960 --> 01:54:30.600]   open source and also free.
[01:54:30.600 --> 01:54:32.800]   And, but they needed to find a way to pay the bills anyway,
[01:54:32.800 --> 01:54:33.840]   because they were still building this
[01:54:33.840 --> 01:54:36.280]   on top of our old economics system.
[01:54:36.280 --> 01:54:37.600]   And so the way they did that
[01:54:37.600 --> 01:54:40.240]   was through third-party advertisement.
[01:54:40.240 --> 01:54:42.760]   But that meant that things were very decoupled.
[01:54:42.760 --> 01:54:45.600]   You know, you've got this third-party interest,
[01:54:45.600 --> 01:54:47.480]   which means that you're then like,
[01:54:47.480 --> 01:54:48.960]   people are having to optimize for that.
[01:54:48.960 --> 01:54:51.320]   And that is, you know, the actual consumer
[01:54:51.320 --> 01:54:52.960]   is actually the product,
[01:54:52.960 --> 01:54:56.600]   not the person you're making the thing for.
[01:54:56.600 --> 01:54:57.480]   You're in, in the end,
[01:54:57.480 --> 01:54:59.840]   you start making the thing for the advertiser.
[01:54:59.840 --> 01:55:02.000]   And so that's why it then like breaks down.
[01:55:02.000 --> 01:55:07.440]   Yeah, like it's, there's no clean solution to this.
[01:55:07.440 --> 01:55:11.160]   And I, it's a really good suggestion by you actually
[01:55:11.160 --> 01:55:16.080]   to like figure out how we can optimize virality
[01:55:16.080 --> 01:55:19.560]   for positive sum topics.
[01:55:19.560 --> 01:55:22.700]   - I shall be the general of the love bot army.
[01:55:22.700 --> 01:55:26.200]   - Distributed.
[01:55:26.200 --> 01:55:27.440]   - Distributed, distributed.
[01:55:27.440 --> 01:55:28.400]   No, okay, yeah.
[01:55:28.400 --> 01:55:30.240]   The power, just even in saying that,
[01:55:30.240 --> 01:55:32.240]   the power already went to my head.
[01:55:32.240 --> 01:55:33.280]   No, okay.
[01:55:33.280 --> 01:55:35.920]   You've talked about quantifying your thinking.
[01:55:35.920 --> 01:55:37.000]   We've been talking about this,
[01:55:37.000 --> 01:55:39.320]   sort of a game theoretic view on life
[01:55:39.320 --> 01:55:42.280]   and putting probabilities behind estimates.
[01:55:42.280 --> 01:55:44.280]   Like if you think about different trajectories
[01:55:44.280 --> 01:55:45.680]   you can take through life,
[01:55:45.680 --> 01:55:48.360]   just actually analyzing life in game theoretic way,
[01:55:48.360 --> 01:55:50.840]   like your own life, like personal life.
[01:55:50.840 --> 01:55:52.820]   I think you've given an example
[01:55:52.820 --> 01:55:54.680]   that you had an honest conversation with Igor
[01:55:54.680 --> 01:55:57.840]   about like how long is this relationship gonna last?
[01:55:57.840 --> 01:56:00.160]   So similar to our sort of marriage problem
[01:56:00.160 --> 01:56:01.560]   kind of discussion,
[01:56:01.560 --> 01:56:05.080]   having an honest conversation about the probability
[01:56:05.080 --> 01:56:08.600]   of things that we sometimes are a little bit too shy
[01:56:08.600 --> 01:56:11.840]   or scared to think of in a probabilistic terms.
[01:56:11.840 --> 01:56:15.040]   Can you speak to that kind of way of reasoning
[01:56:15.040 --> 01:56:17.040]   about the good and the bad of that?
[01:56:17.040 --> 01:56:20.880]   Can you do this kind of thing with human relations?
[01:56:20.880 --> 01:56:24.120]   - Yeah, so the scenario you're talking about,
[01:56:24.120 --> 01:56:25.040]   it was like-
[01:56:25.040 --> 01:56:27.680]   - Yeah, tell me about that scenario.
[01:56:27.680 --> 01:56:30.920]   - I think it was about a year into our relationship
[01:56:30.920 --> 01:56:34.520]   and we were having a fairly heavy conversation
[01:56:34.520 --> 01:56:35.440]   because we were trying to figure out
[01:56:35.440 --> 01:56:37.720]   whether or not I was gonna sell my apartment.
[01:56:37.720 --> 01:56:40.160]   He had already moved in,
[01:56:40.160 --> 01:56:41.760]   but I think we were just figuring out
[01:56:41.760 --> 01:56:43.640]   what like our long-term plans would be.
[01:56:43.640 --> 01:56:46.360]   Should we buy a place together, et cetera.
[01:56:46.360 --> 01:56:47.720]   - When you guys are having that conversation,
[01:56:47.720 --> 01:56:49.800]   are you like drunk out of your mind on wine
[01:56:49.800 --> 01:56:52.760]   or is he sober and you're actually having a serious-
[01:56:52.760 --> 01:56:53.680]   - I think I'm sober.
[01:56:53.680 --> 01:56:54.960]   - How do you get to that conversation?
[01:56:54.960 --> 01:56:56.160]   'Cause most people are kind of afraid
[01:56:56.160 --> 01:56:58.760]   to have that kind of serious conversation.
[01:56:58.760 --> 01:57:01.520]   - Well, so our relationship was very,
[01:57:01.520 --> 01:57:03.360]   well, first of all, we were good friends
[01:57:03.360 --> 01:57:06.760]   for a couple of years before we even got romantic.
[01:57:06.760 --> 01:57:12.480]   And when we did get romantic,
[01:57:12.480 --> 01:57:15.800]   it was very clear that this was a big deal.
[01:57:15.800 --> 01:57:17.680]   It wasn't just like another,
[01:57:17.680 --> 01:57:18.880]   it wasn't a random thing.
[01:57:18.880 --> 01:57:20.720]   And-
[01:57:20.720 --> 01:57:22.880]   - So the probability of it being a big deal was high.
[01:57:22.880 --> 01:57:24.200]   - Was already very high.
[01:57:24.200 --> 01:57:26.200]   And then we'd been together for a year
[01:57:26.200 --> 01:57:28.800]   and it had been pretty golden and wonderful.
[01:57:28.800 --> 01:57:32.640]   So there was a lot of foundation already
[01:57:32.640 --> 01:57:33.840]   where we felt very comfortable
[01:57:33.840 --> 01:57:35.200]   having a lot of frank conversations.
[01:57:35.200 --> 01:57:38.440]   But Igor's MO has always been much more than mine.
[01:57:38.440 --> 01:57:40.000]   He was always from the outset,
[01:57:40.280 --> 01:57:42.600]   just in a relationship,
[01:57:42.600 --> 01:57:45.320]   radical transparency and honesty is the way
[01:57:45.320 --> 01:57:47.080]   because the truth is the truth
[01:57:47.080 --> 01:57:48.880]   whether you want to hide it or not.
[01:57:48.880 --> 01:57:50.320]   It will come out eventually.
[01:57:50.320 --> 01:57:55.320]   And if you aren't able to accept difficult things yourself,
[01:57:55.320 --> 01:57:57.680]   then how could you possibly expect
[01:57:57.680 --> 01:57:59.800]   to be the most integral version?
[01:57:59.800 --> 01:58:04.600]   The relationship needs this bedrock of honesty
[01:58:04.600 --> 01:58:06.600]   as a foundation more than anything.
[01:58:06.600 --> 01:58:07.720]   - Yeah, that's really interesting,
[01:58:07.720 --> 01:58:09.960]   but I would like to push against some of those ideas,
[01:58:09.960 --> 01:58:10.800]   but let's-
[01:58:10.800 --> 01:58:11.640]   - Okay, all right.
[01:58:11.640 --> 01:58:13.720]   - Down the line, yes, throw them up.
[01:58:13.720 --> 01:58:15.280]   I just rudely interrupt.
[01:58:15.280 --> 01:58:16.760]   - No, it's fine.
[01:58:16.760 --> 01:58:19.840]   And so, we'd been about together for a year
[01:58:19.840 --> 01:58:20.680]   and things were good.
[01:58:20.680 --> 01:58:23.440]   And we were having this hard conversation
[01:58:23.440 --> 01:58:25.280]   and then he was like, well, okay,
[01:58:25.280 --> 01:58:27.280]   what's the likelihood that we're going to be together
[01:58:27.280 --> 01:58:28.240]   in three years then?
[01:58:28.240 --> 01:58:31.000]   'Cause I think it was roughly a three-year time horizon.
[01:58:31.000 --> 01:58:32.880]   And I was like, ooh, interesting.
[01:58:32.880 --> 01:58:34.240]   And then we were like, actually wait,
[01:58:34.240 --> 01:58:35.080]   before you say it out loud,
[01:58:35.080 --> 01:58:37.720]   let's both write down our predictions formally.
[01:58:37.720 --> 01:58:38.560]   'Cause we'd been like,
[01:58:38.560 --> 01:58:40.440]   we were just getting into like effective altruism
[01:58:40.440 --> 01:58:41.840]   and rationality at the time,
[01:58:41.840 --> 01:58:43.880]   which is all about making formal predictions
[01:58:43.880 --> 01:58:47.360]   as a means of measuring your own,
[01:58:47.360 --> 01:58:53.680]   well, your own foresight essentially in a quantified way.
[01:58:53.680 --> 01:58:55.480]   So, we both wrote down our percentages
[01:58:55.480 --> 01:58:58.640]   and we also did a one-year prediction
[01:58:58.640 --> 01:58:59.560]   and a 10-year one as well.
[01:58:59.560 --> 01:59:01.960]   So, we got percentages for all three
[01:59:01.960 --> 01:59:03.880]   and then we showed each other.
[01:59:03.880 --> 01:59:06.240]   And I remember having this moment of like, ooh,
[01:59:06.240 --> 01:59:07.640]   'cause for the 10-year one, I was like, ooh,
[01:59:07.640 --> 01:59:09.600]   well, I mean, I love him a lot,
[01:59:09.600 --> 01:59:11.880]   but like a lot can happen in 10 years, you know?
[01:59:11.880 --> 01:59:14.280]   And we've only been together for, you know,
[01:59:14.280 --> 01:59:16.240]   so I was like, I think it's over 50%,
[01:59:16.240 --> 01:59:17.920]   but it's definitely not 90%.
[01:59:17.920 --> 01:59:19.240]   And I remember like wrestling, I was like,
[01:59:19.240 --> 01:59:20.240]   oh, but I don't want him to be hurt.
[01:59:20.240 --> 01:59:21.400]   I don't want him to, you know,
[01:59:21.400 --> 01:59:22.680]   I don't want to give a number lower than his.
[01:59:22.680 --> 01:59:25.280]   And I remember thinking, I was like, uh-uh, don't game it.
[01:59:25.280 --> 01:59:28.120]   This is an exercise in radical honesty.
[01:59:28.120 --> 01:59:29.520]   So, just give your real percentage.
[01:59:29.520 --> 01:59:31.280]   And I think mine was like 75%.
[01:59:31.280 --> 01:59:32.400]   And then we showed each other
[01:59:32.400 --> 01:59:34.880]   and luckily we were fairly well aligned.
[01:59:37.200 --> 01:59:38.640]   But honestly, even if we weren't-
[01:59:38.640 --> 01:59:39.480]   - 20%.
[01:59:39.480 --> 01:59:40.320]   - Huh?
[01:59:40.320 --> 01:59:42.000]   It definitely would have,
[01:59:42.000 --> 01:59:45.520]   if his had been consistently lower than mine,
[01:59:45.520 --> 01:59:48.120]   that would have rattled me for sure.
[01:59:48.120 --> 01:59:49.880]   Whereas if it had been the other way around,
[01:59:49.880 --> 01:59:52.480]   I think he's just kind of like a water off the duck's back
[01:59:52.480 --> 01:59:53.320]   type of guy.
[01:59:53.320 --> 01:59:55.200]   Be like, okay, well, all right, we'll figure this out.
[01:59:55.200 --> 01:59:57.800]   - Well, did you guys provide error bars on the estimate?
[01:59:57.800 --> 01:59:58.680]   Like the love them on-
[01:59:58.680 --> 01:59:59.560]   - They came built in.
[01:59:59.560 --> 02:00:02.200]   We didn't give formal plus or minus error bars.
[02:00:02.200 --> 02:00:04.120]   I didn't draw any or anything like that.
[02:00:04.120 --> 02:00:05.680]   - I guess that's the question I have is,
[02:00:05.720 --> 02:00:10.720]   did you feel informed enough to make such decisions?
[02:00:10.720 --> 02:00:14.680]   'Cause I feel like if I were to do
[02:00:14.680 --> 02:00:16.200]   this kind of thing rigorously,
[02:00:16.200 --> 02:00:19.080]   I would want some data.
[02:00:19.080 --> 02:00:23.120]   I would want to say one of the assumptions you have
[02:00:23.120 --> 02:00:25.400]   is you're not that different from other relationships.
[02:00:25.400 --> 02:00:26.240]   - Right.
[02:00:26.240 --> 02:00:29.360]   - And so I wanna have some data about the way-
[02:00:29.360 --> 02:00:30.720]   - You want the base rates.
[02:00:30.720 --> 02:00:31.560]   - Yeah.
[02:00:31.560 --> 02:00:34.400]   And also actual trajectories of relationships.
[02:00:34.400 --> 02:00:39.080]   I would love to have time series data
[02:00:39.080 --> 02:00:42.840]   about the ways that relationships fall apart or prosper,
[02:00:42.840 --> 02:00:46.720]   how they collide with different life events,
[02:00:46.720 --> 02:00:48.840]   losses, job changes, moving,
[02:00:48.840 --> 02:00:54.520]   both partners find jobs, only one has a job.
[02:00:54.520 --> 02:00:56.320]   I want that kind of data
[02:00:56.320 --> 02:00:59.080]   and how often the different trajectories change in life.
[02:01:01.280 --> 02:01:04.400]   How informative is your past to your future?
[02:01:04.400 --> 02:01:05.640]   That's a whole thing.
[02:01:05.640 --> 02:01:09.760]   Can you look at my life and have a good prediction
[02:01:09.760 --> 02:01:13.560]   about in terms of my characteristics and my relationships
[02:01:13.560 --> 02:01:15.880]   of what that's gonna look like in the future or not?
[02:01:15.880 --> 02:01:17.200]   I don't even know the answer to that question.
[02:01:17.200 --> 02:01:20.720]   I'll be very ill-informed in terms of making the probability.
[02:01:20.720 --> 02:01:25.680]   I would be far, yeah, I just would be under-informed.
[02:01:25.680 --> 02:01:26.680]   I would be under-informed.
[02:01:26.680 --> 02:01:30.800]   I'll be over-biasing to my prior experiences, I think.
[02:01:31.240 --> 02:01:33.120]   - Right, but as long as you're aware of that
[02:01:33.120 --> 02:01:34.400]   and you're honest with yourself,
[02:01:34.400 --> 02:01:35.760]   and you're honest with the other person,
[02:01:35.760 --> 02:01:37.640]   say, "Look, I have really wide error bars on this
[02:01:37.640 --> 02:01:40.440]   for the following reasons," that's okay.
[02:01:40.440 --> 02:01:41.280]   I still think it's better
[02:01:41.280 --> 02:01:43.200]   than not trying to quantify it at all
[02:01:43.200 --> 02:01:44.040]   if you're trying to make
[02:01:44.040 --> 02:01:46.760]   really major irreversible life decisions.
[02:01:46.760 --> 02:01:49.480]   - And I feel also the romantic nature of that question.
[02:01:49.480 --> 02:01:52.800]   For me personally, I try to live my life
[02:01:52.800 --> 02:01:55.440]   thinking it's very close to 100%.
[02:01:55.440 --> 02:01:58.400]   Like allowing myself, actually,
[02:01:58.400 --> 02:02:00.000]   this is the difficulty of this,
[02:02:00.000 --> 02:02:03.560]   is allowing myself to think differently,
[02:02:03.560 --> 02:02:06.520]   I feel like has a psychological consequence.
[02:02:06.520 --> 02:02:09.720]   That's one of my pushbacks against radical honesty,
[02:02:09.720 --> 02:02:14.280]   is this one particular perspective on-
[02:02:14.280 --> 02:02:16.320]   - So you're saying you would rather give
[02:02:16.320 --> 02:02:20.200]   a falsely high percentage to your partner?
[02:02:20.200 --> 02:02:22.640]   - Going back to the wise sage- - In order to sort of
[02:02:22.640 --> 02:02:24.640]   create this additional optimism.
[02:02:24.640 --> 02:02:26.280]   - Helmuth. - Yes.
[02:02:26.280 --> 02:02:29.440]   - Of fake it till you make it,
[02:02:29.440 --> 02:02:31.200]   the positive, the power of positive thinking.
[02:02:31.200 --> 02:02:33.000]   - #positivity, yeah. - Yeah, #.
[02:02:33.000 --> 02:02:36.760]   - Well, so that, and this comes back
[02:02:36.760 --> 02:02:39.800]   to this idea of useful fictions, right?
[02:02:39.800 --> 02:02:42.960]   And I agree, I don't think there's a clear answer to this,
[02:02:42.960 --> 02:02:44.280]   and I think it's actually quite subjective.
[02:02:44.280 --> 02:02:46.600]   Some people this works better for than others.
[02:02:46.600 --> 02:02:50.160]   You know, to be clear, Igor and I weren't doing
[02:02:50.160 --> 02:02:52.000]   this formal prediction in it.
[02:02:52.000 --> 02:02:55.560]   Like we did it with very much tongue in cheek.
[02:02:55.560 --> 02:02:57.400]   It wasn't like we were gonna make,
[02:02:57.400 --> 02:03:00.160]   I don't think it even would have drastically changed
[02:03:00.160 --> 02:03:02.200]   what we decided to do, even.
[02:03:02.200 --> 02:03:04.400]   We kinda just did it more as a fun exercise.
[02:03:04.400 --> 02:03:06.920]   - But the consequence of that fun exercise,
[02:03:06.920 --> 02:03:09.920]   you really actually kinda, there was a deep honesty to it.
[02:03:09.920 --> 02:03:12.160]   - Exactly, it was a deep, and it was just like
[02:03:12.160 --> 02:03:13.840]   this moment of reflection, I'm like, oh wow,
[02:03:13.840 --> 02:03:16.720]   I actually have to think through this quite critically,
[02:03:16.720 --> 02:03:17.560]   and so on.
[02:03:17.560 --> 02:03:22.000]   And it's also what was interesting was,
[02:03:22.000 --> 02:03:26.000]   I got to check in with what my desires were.
[02:03:26.000 --> 02:03:28.600]   So there was one thing of what my actual prediction is,
[02:03:28.600 --> 02:03:30.440]   but what are my desires, and could these desires
[02:03:30.440 --> 02:03:32.480]   be affecting my predictions, and so on.
[02:03:32.480 --> 02:03:34.960]   And that's a method of rationality,
[02:03:34.960 --> 02:03:37.080]   and I personally don't think it loses anything.
[02:03:37.080 --> 02:03:39.240]   It didn't take any of the magic away from our relationship,
[02:03:39.240 --> 02:03:40.640]   quite the opposite.
[02:03:40.640 --> 02:03:42.640]   It brought us closer together, 'cause it was like
[02:03:42.640 --> 02:03:45.400]   we did this weird, fun thing that I appreciate
[02:03:45.400 --> 02:03:47.920]   a lot of people find quite strange.
[02:03:47.920 --> 02:03:51.880]   And I think it was somewhat unique in our relationship
[02:03:51.880 --> 02:03:54.880]   that both of us are very, we both love numbers,
[02:03:54.880 --> 02:03:57.320]   we both love statistics, we're both poker players.
[02:03:57.320 --> 02:04:01.320]   So this was kind of like our safe space anyway.
[02:04:01.320 --> 02:04:05.160]   For others, one partner really might not like
[02:04:05.160 --> 02:04:06.400]   that kind of stuff at all, in which case
[02:04:06.400 --> 02:04:07.840]   this is not a good exercise to do.
[02:04:07.840 --> 02:04:09.520]   I don't recommend it to everybody.
[02:04:09.520 --> 02:04:14.320]   But I do think there's, it's interesting sometimes
[02:04:14.320 --> 02:04:18.920]   to poke holes in the, probe at these things
[02:04:18.920 --> 02:04:21.600]   that we consider so sacred that we can't try
[02:04:21.600 --> 02:04:22.800]   to quantify them.
[02:04:24.040 --> 02:04:25.480]   Which is interesting, 'cause that's in tension
[02:04:25.480 --> 02:04:27.400]   with the idea of what we just talked about with beauty
[02:04:27.400 --> 02:04:28.680]   and what makes something beautiful,
[02:04:28.680 --> 02:04:30.960]   the fact that you can't measure everything about it.
[02:04:30.960 --> 02:04:32.920]   And perhaps something shouldn't be tried to,
[02:04:32.920 --> 02:04:36.360]   maybe it's wrong to completely try and value
[02:04:36.360 --> 02:04:39.160]   the utilitarian, put a utilitarian frame
[02:04:39.160 --> 02:04:43.280]   of measuring the utility of a tree in its entirety.
[02:04:43.280 --> 02:04:44.680]   I don't know, maybe we should, maybe we shouldn't.
[02:04:44.680 --> 02:04:46.640]   I'm ambivalent on that.
[02:04:46.640 --> 02:04:51.640]   But overall, people have too many biases.
[02:04:52.600 --> 02:04:57.440]   People are overly biased against trying to do
[02:04:57.440 --> 02:04:59.840]   a quantified cost-benefit analysis
[02:04:59.840 --> 02:05:01.480]   on really tough life decisions.
[02:05:01.480 --> 02:05:03.920]   They're like, "Oh, just go with your gut."
[02:05:03.920 --> 02:05:07.120]   It's like, well, sure, but guts, our intuitions
[02:05:07.120 --> 02:05:08.760]   are best suited for things that we've got
[02:05:08.760 --> 02:05:10.360]   tons of experience in.
[02:05:10.360 --> 02:05:11.880]   Then we can really trust on it,
[02:05:11.880 --> 02:05:13.280]   if it's a decision we've made many times.
[02:05:13.280 --> 02:05:16.000]   But if it's like, should I marry this person
[02:05:16.000 --> 02:05:19.000]   or should I buy this house over that house?
[02:05:19.000 --> 02:05:20.640]   You only make those decisions a couple of times
[02:05:20.680 --> 02:05:22.960]   in your life, maybe.
[02:05:22.960 --> 02:05:26.920]   - Well, I would love to know, there's a balance,
[02:05:26.920 --> 02:05:29.240]   probably it's a personal balance of strike,
[02:05:29.240 --> 02:05:33.120]   is the amount of rationality you apply
[02:05:33.120 --> 02:05:38.120]   to a question versus the useful fiction,
[02:05:38.120 --> 02:05:40.080]   the fake it till you make it.
[02:05:40.080 --> 02:05:43.520]   For example, just talking to soldiers in Ukraine,
[02:05:43.520 --> 02:05:48.520]   you ask them, what's the probability of you winning,
[02:05:49.160 --> 02:05:50.160]   Ukraine winning?
[02:05:50.160 --> 02:05:55.640]   Almost everybody I talk to is 100%.
[02:05:55.640 --> 02:05:56.840]   - Wow.
[02:05:56.840 --> 02:05:58.960]   - And you listen to the experts, right?
[02:05:58.960 --> 02:06:00.760]   They say all kinds of stuff.
[02:06:00.760 --> 02:06:01.600]   - Right.
[02:06:01.600 --> 02:06:06.320]   - First of all, the morale there is higher than probably,
[02:06:06.320 --> 02:06:09.880]   and I've never been to a war zone before this,
[02:06:09.880 --> 02:06:12.560]   but I've read about many wars,
[02:06:12.560 --> 02:06:14.920]   and I think the morale in Ukraine is higher
[02:06:14.920 --> 02:06:17.320]   than almost any war I've read about.
[02:06:17.320 --> 02:06:19.360]   It's every single person in the country
[02:06:19.360 --> 02:06:21.720]   is proud to fight for their country.
[02:06:21.720 --> 02:06:22.560]   - Wow.
[02:06:22.560 --> 02:06:25.640]   - Everybody, not just soldiers, not everybody.
[02:06:25.640 --> 02:06:26.560]   - Why do you think that is,
[02:06:26.560 --> 02:06:28.640]   specifically more than in other wars?
[02:06:28.640 --> 02:06:36.680]   - I think because there's perhaps a dormant desire
[02:06:36.680 --> 02:06:39.600]   for the citizens of this country
[02:06:39.600 --> 02:06:41.960]   to find the identity of this country,
[02:06:41.960 --> 02:06:45.360]   because it's been going through this 30 year process
[02:06:45.360 --> 02:06:48.400]   of different factions and political bickering,
[02:06:48.400 --> 02:06:50.960]   and they haven't had, as they talk about,
[02:06:50.960 --> 02:06:52.600]   they haven't had their independence war.
[02:06:52.600 --> 02:06:55.880]   They say all great nations have had an independence war.
[02:06:55.880 --> 02:06:58.920]   They had to fight for their independence,
[02:06:58.920 --> 02:07:00.700]   for the discovery of the identity,
[02:07:00.700 --> 02:07:03.200]   of the core of the ideals that unify us,
[02:07:03.200 --> 02:07:04.600]   and they haven't had that.
[02:07:04.600 --> 02:07:07.220]   There's constantly been factions, there's been divisions,
[02:07:07.220 --> 02:07:09.840]   there's been pressures from empires,
[02:07:09.840 --> 02:07:12.320]   from United States and from Russia,
[02:07:12.320 --> 02:07:14.080]   from NATO and Europe,
[02:07:14.080 --> 02:07:15.680]   everybody telling them what to do.
[02:07:15.680 --> 02:07:17.720]   Now they wanna discover who they are,
[02:07:17.720 --> 02:07:21.240]   and there's that kind of sense that we're going to fight
[02:07:21.240 --> 02:07:23.280]   for the safety of our homeland,
[02:07:23.280 --> 02:07:25.640]   but we're also gonna fight for our identity.
[02:07:25.640 --> 02:07:30.640]   And that, on top of the fact that there's just,
[02:07:30.640 --> 02:07:33.960]   if you look at the history of Ukraine,
[02:07:33.960 --> 02:07:36.760]   and there's certain other countries like this,
[02:07:36.760 --> 02:07:41.240]   there are certain cultures are feisty in their pride
[02:07:43.040 --> 02:07:45.720]   of being the citizens of that nation.
[02:07:45.720 --> 02:07:48.440]   Ukraine is that, Poland was that.
[02:07:48.440 --> 02:07:49.520]   You just look at history.
[02:07:49.520 --> 02:07:52.880]   In certain countries, you do not want to occupy.
[02:07:52.880 --> 02:07:53.720]   - Right.
[02:07:53.720 --> 02:07:54.560]   (Lex laughing)
[02:07:54.560 --> 02:07:55.840]   - I mean, both Stalin and Hitler
[02:07:55.840 --> 02:07:57.400]   talked about Poland in this way.
[02:07:57.400 --> 02:08:00.240]   They're like, "This is a big problem.
[02:08:00.240 --> 02:08:02.400]   If we occupy this land for prolonged periods of time,
[02:08:02.400 --> 02:08:04.400]   they're gonna be a pain in their ass.
[02:08:04.400 --> 02:08:07.160]   Like, they're not going to want to be occupied."
[02:08:07.160 --> 02:08:09.540]   And certain other countries are like, pragmatic.
[02:08:09.540 --> 02:08:12.200]   They're like, "Well, leaders come and go.
[02:08:12.200 --> 02:08:13.360]   I guess this is good."
[02:08:13.360 --> 02:08:15.680]   Ukraine just doesn't have,
[02:08:15.680 --> 02:08:19.280]   Ukrainians, throughout the 20th century,
[02:08:19.280 --> 02:08:20.760]   don't seem to be the kind of people
[02:08:20.760 --> 02:08:25.760]   that just sit calmly and let the "occupiers"
[02:08:25.760 --> 02:08:30.160]   impose their rules.
[02:08:30.160 --> 02:08:31.000]   - That's interesting, though,
[02:08:31.000 --> 02:08:33.360]   because you said it's always been under conflict
[02:08:33.360 --> 02:08:35.280]   and leaders have come and gone.
[02:08:35.280 --> 02:08:36.120]   - Yeah.
[02:08:36.120 --> 02:08:37.120]   - So you would expect them to actually be the opposite
[02:08:37.120 --> 02:08:38.760]   under that reasoning.
[02:08:38.760 --> 02:08:42.360]   - Because it's a very fertile land.
[02:08:42.360 --> 02:08:43.480]   It's great for agriculture.
[02:08:43.480 --> 02:08:44.800]   So a lot of people want to,
[02:08:44.800 --> 02:08:46.520]   I mean, I think they've developed this culture
[02:08:46.520 --> 02:08:47.920]   because they've constantly been occupied
[02:08:47.920 --> 02:08:51.040]   by different people, for different peoples.
[02:08:51.040 --> 02:08:54.600]   And so maybe there is something to that,
[02:08:54.600 --> 02:08:58.320]   where you've constantly had to feel,
[02:08:58.320 --> 02:09:00.720]   like, within the blood of the generations,
[02:09:00.720 --> 02:09:04.880]   there's the struggle against the man,
[02:09:04.880 --> 02:09:07.880]   against the imposition of rules,
[02:09:07.880 --> 02:09:09.440]   against oppression and all that kind of stuff,
[02:09:09.440 --> 02:09:10.600]   and that stays with them.
[02:09:10.600 --> 02:09:13.720]   So there's a will there.
[02:09:13.720 --> 02:09:16.440]   But a lot of other aspects are also part of it
[02:09:16.440 --> 02:09:20.040]   that has to do with the reverse Mollik kind of situation,
[02:09:20.040 --> 02:09:23.160]   where social media has definitely played a part of it.
[02:09:23.160 --> 02:09:25.120]   Also, different charismatic individuals
[02:09:25.120 --> 02:09:27.220]   have had to play a part.
[02:09:27.220 --> 02:09:31.160]   The fact that the president of the nation, Zelensky,
[02:09:31.160 --> 02:09:35.160]   stayed in Kiev during the invasion
[02:09:35.160 --> 02:09:37.720]   is a huge inspiration to them
[02:09:37.720 --> 02:09:41.460]   because most leaders, as you can imagine,
[02:09:41.460 --> 02:09:44.560]   when the capital of the nation is under attack,
[02:09:44.560 --> 02:09:46.820]   the wise thing, the smart thing,
[02:09:46.820 --> 02:09:49.120]   that the United States advised Zelensky to do
[02:09:49.120 --> 02:09:52.760]   is to flee and to be the leader of the nation
[02:09:52.760 --> 02:09:54.760]   from a distant place.
[02:09:54.760 --> 02:09:57.280]   He said, "Fuck that, I'm staying put."
[02:09:57.280 --> 02:10:01.760]   Everyone around him, there was a pressure to leave,
[02:10:01.760 --> 02:10:02.860]   and he didn't.
[02:10:02.860 --> 02:10:06.600]   And that, in those singular acts,
[02:10:07.040 --> 02:10:09.200]   really can unify a nation.
[02:10:09.200 --> 02:10:11.400]   There's a lot of people that criticize Zelensky
[02:10:11.400 --> 02:10:14.040]   within Ukraine before the war.
[02:10:14.040 --> 02:10:17.240]   He was very unpopular, even still.
[02:10:17.240 --> 02:10:18.860]   But they put that aside,
[02:10:18.860 --> 02:10:24.160]   especially that singular act of staying in the capital.
[02:10:24.160 --> 02:10:27.160]   Yeah, a lot of those kinds of things
[02:10:27.160 --> 02:10:31.660]   come together to create something within people.
[02:10:31.660 --> 02:10:35.640]   - These things always, of course,
[02:10:35.640 --> 02:10:40.640]   so how zoomed out of a view do you wanna take?
[02:10:40.640 --> 02:10:46.320]   Because, yeah, you describe it as an anti-Molotov thing
[02:10:46.320 --> 02:10:48.000]   happened within Ukraine
[02:10:48.000 --> 02:10:49.840]   because it brought the Ukrainian people together
[02:10:49.840 --> 02:10:51.800]   in order to fight a common enemy.
[02:10:51.800 --> 02:10:53.400]   Maybe that's a good thing, maybe that's a bad thing.
[02:10:53.400 --> 02:10:54.240]   In the end, we don't know
[02:10:54.240 --> 02:10:56.740]   how this is all gonna play out, right?
[02:10:56.740 --> 02:11:01.120]   But if you zoom it out from a level, on a global level,
[02:11:01.200 --> 02:11:05.140]   they're coming together to fight,
[02:11:05.140 --> 02:11:12.240]   that could make a conflict larger.
[02:11:12.240 --> 02:11:13.120]   You know what I mean?
[02:11:13.120 --> 02:11:15.680]   I don't know what the right answer is here.
[02:11:15.680 --> 02:11:17.600]   It seems like a good thing that they came together,
[02:11:17.600 --> 02:11:20.000]   but we don't know how this is all gonna play out.
[02:11:20.000 --> 02:11:21.600]   If this all turns into nuclear war,
[02:11:21.600 --> 02:11:23.440]   we'll be like, "Okay, that was the bad, that was the."
[02:11:23.440 --> 02:11:26.080]   - Oh yeah, so I was describing the reverse Moloch
[02:11:26.080 --> 02:11:27.440]   for the local level.
[02:11:27.440 --> 02:11:28.280]   - Exactly, yeah.
[02:11:28.280 --> 02:11:30.400]   - Now, this is where the experts come in
[02:11:31.240 --> 02:11:36.240]   and they say, "Well, if you channel most of the resources
[02:11:36.240 --> 02:11:40.500]   "of the nation and the nation supporting Ukraine
[02:11:40.500 --> 02:11:45.500]   "into the war effort, are you not beating the drums of war
[02:11:45.500 --> 02:11:47.480]   "that is much bigger than Ukraine?"
[02:11:47.480 --> 02:11:50.880]   In fact, even the Ukrainian leaders
[02:11:50.880 --> 02:11:52.920]   are speaking of it this way.
[02:11:52.920 --> 02:11:55.840]   This is not a war between two nations.
[02:11:55.840 --> 02:12:00.840]   This is the early days of a world war
[02:12:00.840 --> 02:12:02.480]   if we don't play this correctly.
[02:12:02.480 --> 02:12:03.320]   - Yes.
[02:12:03.320 --> 02:12:07.760]   And we need cool heads from our leaders.
[02:12:07.760 --> 02:12:09.560]   - So from Ukraine's perspective,
[02:12:09.560 --> 02:12:12.680]   Ukraine needs to win the war.
[02:12:12.680 --> 02:12:15.440]   Because what does winning the war mean
[02:12:15.440 --> 02:12:20.240]   is coming to peace negotiations,
[02:12:20.240 --> 02:12:24.360]   an agreement that guarantees no more invasions.
[02:12:24.360 --> 02:12:25.640]   And then you make an agreement
[02:12:25.640 --> 02:12:28.040]   about what land belongs to who.
[02:12:28.040 --> 02:12:28.880]   - Right.
[02:12:28.880 --> 02:12:30.240]   - And you stop that.
[02:12:30.240 --> 02:12:34.120]   And basically, from their perspective,
[02:12:34.120 --> 02:12:36.600]   is you want to demonstrate to the rest of the world
[02:12:36.600 --> 02:12:39.640]   who's watching carefully, including Russia and China
[02:12:39.640 --> 02:12:42.280]   and different players on the geopolitical stage,
[02:12:42.280 --> 02:12:46.000]   that this kind of conflict is not going to be productive
[02:12:46.000 --> 02:12:47.240]   if you engage in it.
[02:12:47.240 --> 02:12:49.240]   So you wanna teach everybody a lesson,
[02:12:49.240 --> 02:12:50.920]   let's not do World War III.
[02:12:50.920 --> 02:12:53.000]   It's gonna be bad for everybody.
[02:12:53.000 --> 02:12:55.040]   It's a lose-lose.
[02:12:55.040 --> 02:12:56.600]   - Deep lose-lose.
[02:12:56.600 --> 02:12:57.440]   Doesn't matter.
[02:12:57.440 --> 02:13:04.360]   - And I think that's actually a correct...
[02:13:04.360 --> 02:13:06.760]   When I zoom out,
[02:13:06.760 --> 02:13:10.160]   99% of what I think about
[02:13:10.160 --> 02:13:12.320]   is just individual human beings and human lives
[02:13:12.320 --> 02:13:14.600]   and just that war is horrible.
[02:13:14.600 --> 02:13:15.600]   But when you zoom out
[02:13:15.600 --> 02:13:17.760]   and think from a geopolitics perspective,
[02:13:17.760 --> 02:13:22.000]   we should realize that it's entirely possible
[02:13:22.000 --> 02:13:26.400]   that we will see a World War III in the 21st century.
[02:13:26.400 --> 02:13:29.840]   And this is like a dress rehearsal for that.
[02:13:29.840 --> 02:13:34.840]   And so the way we play this as a human civilization
[02:13:34.840 --> 02:13:39.800]   will define whether we do or don't have a World War III.
[02:13:39.800 --> 02:13:49.320]   How we discuss war, how we discuss nuclear war,
[02:13:49.320 --> 02:13:54.320]   the kind of leaders we elect and prop up,
[02:13:54.320 --> 02:13:58.480]   the kind of memes we circulate.
[02:13:58.480 --> 02:13:59.920]   Because you have to be very careful
[02:13:59.920 --> 02:14:04.480]   when you're being pro-Ukraine, for example,
[02:14:04.480 --> 02:14:06.560]   you have to realize that you're being...
[02:14:06.560 --> 02:14:11.520]   You are also indirectly feeding
[02:14:11.520 --> 02:14:14.620]   the ever-increasing military-industrial complex.
[02:14:14.620 --> 02:14:17.560]   So you have to be extremely careful
[02:14:17.560 --> 02:14:22.560]   that when you say pro-Ukraine or pro-anybody,
[02:14:22.560 --> 02:14:29.440]   you're pro-human beings, not pro the machine
[02:14:29.440 --> 02:14:36.800]   that creates narratives that says it's pro-human beings.
[02:14:36.800 --> 02:14:39.920]   But it's actually, if you look at the raw use
[02:14:39.920 --> 02:14:42.640]   of funds and resources,
[02:14:42.640 --> 02:14:44.880]   it's actually pro-making weapons
[02:14:44.880 --> 02:14:47.440]   and shooting bullets and dropping bombs.
[02:14:47.440 --> 02:14:50.680]   The real, we have to just somehow get the meme
[02:14:50.680 --> 02:14:54.640]   into everyone's heads that the real enemy is war itself.
[02:14:54.640 --> 02:14:57.120]   That's the enemy we need to defeat.
[02:14:57.120 --> 02:15:01.600]   And that doesn't mean to say that there isn't justification
[02:15:01.600 --> 02:15:06.600]   for small local scenarios, adversarial conflicts.
[02:15:06.600 --> 02:15:11.120]   If you have a leader who is starting wars,
[02:15:11.120 --> 02:15:13.760]   they're on the side of team war, basically.
[02:15:13.760 --> 02:15:15.280]   It's not that they're on the side of team country,
[02:15:15.280 --> 02:15:16.240]   whatever that country is,
[02:15:16.240 --> 02:15:17.920]   it's they're on the side of team war.
[02:15:17.920 --> 02:15:20.180]   So that needs to be stopped and put down.
[02:15:20.180 --> 02:15:21.520]   But you also have to find a way
[02:15:21.520 --> 02:15:25.800]   that your corrective measure doesn't actually then end up
[02:15:25.800 --> 02:15:28.920]   being co-opted by the war machine and creating greater war.
[02:15:28.920 --> 02:15:31.080]   Again, the playing field is finite.
[02:15:31.080 --> 02:15:35.200]   The scale of conflict is now getting so big
[02:15:35.200 --> 02:15:38.800]   that the weapons that can be used are so mass destructive
[02:15:38.800 --> 02:15:42.560]   that we can't afford another giant conflict.
[02:15:42.560 --> 02:15:44.080]   We just, we won't make it.
[02:15:44.080 --> 02:15:48.000]   - What existential threat, in terms of us not making it,
[02:15:48.000 --> 02:15:49.640]   are you most worried about?
[02:15:49.640 --> 02:15:51.880]   What existential threat to human civilization?
[02:15:51.880 --> 02:15:52.720]   We got like--
[02:15:52.720 --> 02:15:53.560]   - Going down the dark path, huh?
[02:15:53.560 --> 02:15:54.680]   - This is good.
[02:15:54.680 --> 02:15:56.640]   Well, no, it's a dark--
[02:15:56.640 --> 02:15:58.600]   - No, it's like, well, while we're in the somber place,
[02:15:58.600 --> 02:15:59.440]   we might as well.
[02:15:59.440 --> 02:16:02.320]   (both laughing)
[02:16:02.320 --> 02:16:04.500]   - Some of my best friends are dark paths.
[02:16:04.500 --> 02:16:08.000]   What worries you the most?
[02:16:08.000 --> 02:16:13.000]   We mentioned asteroids, we mentioned AGI, nuclear weapons.
[02:16:13.960 --> 02:16:17.400]   - The one that's on my mind the most,
[02:16:17.400 --> 02:16:19.480]   mostly because I think it's the one where we have
[02:16:19.480 --> 02:16:22.000]   actually a real chance to move the needle on
[02:16:22.000 --> 02:16:24.600]   in a positive direction, or more specifically,
[02:16:24.600 --> 02:16:26.880]   stop some really bad things from happening,
[02:16:26.880 --> 02:16:31.880]   really dumb, avoidable things, is bio-risks.
[02:16:31.880 --> 02:16:37.440]   - In what kind of bio-risks?
[02:16:37.440 --> 02:16:39.120]   There's so many fun options.
[02:16:39.120 --> 02:16:39.960]   - Oh, yeah, so many.
[02:16:39.960 --> 02:16:43.640]   So, of course, we have natural risks from natural pandemics,
[02:16:43.640 --> 02:16:45.920]   naturally occurring viruses or pathogens.
[02:16:45.920 --> 02:16:49.640]   And then also as time and technology goes on
[02:16:49.640 --> 02:16:52.040]   and technology becomes more and more democratized
[02:16:52.040 --> 02:16:54.040]   into the hands of more and more people,
[02:16:54.040 --> 02:16:55.980]   the risk of synthetic pathogens.
[02:16:55.980 --> 02:17:00.640]   And whether or not you fall into the camp of COVID
[02:17:00.640 --> 02:17:03.720]   was gain of function, accidental lab leak,
[02:17:03.720 --> 02:17:05.920]   or whether it was purely naturally occurring,
[02:17:05.920 --> 02:17:11.440]   either way, we are facing a future where
[02:17:13.320 --> 02:17:17.880]   synthetic pathogens or human meddled with pathogens
[02:17:17.880 --> 02:17:20.040]   either accidentally get out
[02:17:20.040 --> 02:17:23.280]   or get into the hands of bad actors,
[02:17:23.280 --> 02:17:27.720]   whether they're omnicidal maniacs, either way.
[02:17:27.720 --> 02:17:31.160]   And so that means we need more robustness for that.
[02:17:31.160 --> 02:17:33.920]   And you would think that us having this nice little dry run,
[02:17:33.920 --> 02:17:36.340]   which is what, as awful as COVID was,
[02:17:36.340 --> 02:17:39.480]   and all those poor people that died,
[02:17:39.480 --> 02:17:42.400]   it was still like a child's play
[02:17:42.400 --> 02:17:44.480]   compared to what a future one could be
[02:17:44.480 --> 02:17:45.800]   in terms of fatality rate.
[02:17:45.800 --> 02:17:49.920]   And so you'd think that we would then be coming,
[02:17:49.920 --> 02:17:52.620]   we'd be much more robust in our pandemic preparedness.
[02:17:52.620 --> 02:17:58.620]   And meanwhile, the budget in the last two years for the US,
[02:17:58.620 --> 02:18:01.900]   sorry, they just did this,
[02:18:01.900 --> 02:18:04.800]   I can't remember the name of what the actual budget was,
[02:18:04.800 --> 02:18:06.960]   but it was like a multi-trillion dollar budget
[02:18:06.960 --> 02:18:08.920]   that the US just set aside.
[02:18:08.920 --> 02:18:10.580]   And originally in that,
[02:18:10.580 --> 02:18:12.580]   considering that COVID cost multiple trillions
[02:18:12.580 --> 02:18:13.940]   to the economy, right?
[02:18:13.940 --> 02:18:17.320]   The original allocation in this new budget
[02:18:17.320 --> 02:18:19.960]   for future pandemic preparedness was 60 billion.
[02:18:19.960 --> 02:18:22.720]   So tiny proportion of it.
[02:18:22.720 --> 02:18:24.840]   That's proceeded to get whittled down
[02:18:24.840 --> 02:18:28.680]   to like 30 billion to 15 billion,
[02:18:28.680 --> 02:18:31.560]   all the way down to 2 billion out of multiple trillions
[02:18:31.560 --> 02:18:34.180]   for a thing that has just cost us multiple trillions.
[02:18:34.180 --> 02:18:37.480]   We've just finished, we're not even really out of it.
[02:18:37.480 --> 02:18:39.280]   It basically got whittled down to nothing
[02:18:39.280 --> 02:18:41.020]   because for some reason people think that,
[02:18:41.020 --> 02:18:43.160]   "Whew, all right, we've got the pandemic out of the way.
[02:18:43.160 --> 02:18:44.620]   That was that one."
[02:18:44.620 --> 02:18:47.080]   And the reason for that is that people are,
[02:18:47.080 --> 02:18:49.620]   and I say this with all due respect
[02:18:49.620 --> 02:18:50.640]   to a lot of the science community,
[02:18:50.640 --> 02:18:55.080]   but there's an immense amount of naivety about,
[02:18:55.080 --> 02:18:59.160]   they think that nature is the main risk moving forward,
[02:18:59.160 --> 02:19:00.500]   and it really isn't.
[02:19:00.500 --> 02:19:02.920]   And I think nothing demonstrates this more
[02:19:02.920 --> 02:19:05.640]   than this project that I was just reading about
[02:19:05.640 --> 02:19:07.120]   that's sort of being proposed right now
[02:19:07.120 --> 02:19:10.040]   called Deep Vision.
[02:19:10.040 --> 02:19:12.360]   And the idea is to go out into the wilds,
[02:19:12.360 --> 02:19:15.080]   and we're not talking about just like within cities,
[02:19:15.080 --> 02:19:17.760]   like deep into like caves that people don't go to,
[02:19:17.760 --> 02:19:19.200]   deep into the Arctic, wherever,
[02:19:19.200 --> 02:19:22.760]   scour the earth for whatever the most dangerous
[02:19:22.760 --> 02:19:26.480]   possible pathogens could be that they can find.
[02:19:26.480 --> 02:19:29.920]   And then not only do, try and find these,
[02:19:29.920 --> 02:19:33.080]   bring samples of them back to laboratories.
[02:19:33.080 --> 02:19:36.200]   And again, whether you think COVID was a lab leak or not,
[02:19:36.200 --> 02:19:37.480]   I'm not gonna get into that,
[02:19:37.480 --> 02:19:40.400]   but we have historically had so many, as a civilization,
[02:19:40.400 --> 02:19:42.600]   we've had so many lab leaks
[02:19:42.600 --> 02:19:44.600]   from even like the highest level security things.
[02:19:44.600 --> 02:19:47.520]   Like it just, people should go and just read it.
[02:19:47.520 --> 02:19:50.560]   It's like a comedy show of just how many they are,
[02:19:50.560 --> 02:19:52.160]   how leaky these labs are,
[02:19:52.160 --> 02:19:54.640]   even when they do their best efforts.
[02:19:54.640 --> 02:19:57.520]   So bring these things then back to civilization.
[02:19:57.520 --> 02:19:58.960]   That's step one of the badness.
[02:19:58.960 --> 02:20:02.960]   Then the next step would be to then categorize them,
[02:20:02.960 --> 02:20:04.520]   do experiments on them and categorize them
[02:20:04.520 --> 02:20:07.000]   by their level of potential pandemic lethality.
[02:20:07.000 --> 02:20:10.680]   And then the piece de resistance on this plan
[02:20:10.680 --> 02:20:14.840]   is to then publish that information freely on the internet
[02:20:14.840 --> 02:20:16.920]   about all these pathogens, including their genome,
[02:20:16.920 --> 02:20:18.800]   which is literally like the building instructions
[02:20:18.800 --> 02:20:21.240]   of how to do them on the internet.
[02:20:21.240 --> 02:20:24.640]   And this is something that genuinely a pocket
[02:20:24.640 --> 02:20:27.960]   of the like bio, of the scientific community
[02:20:27.960 --> 02:20:29.840]   thinks is a good idea.
[02:20:29.840 --> 02:20:32.120]   And I think on expectation, like the,
[02:20:32.120 --> 02:20:33.360]   and their argument is, is that,
[02:20:33.360 --> 02:20:36.200]   oh, this is good because it might buy us some time
[02:20:36.200 --> 02:20:39.520]   to buy, to develop the vaccines, which, okay, sure.
[02:20:39.520 --> 02:20:42.040]   Maybe would have made sense prior to mRNA technology,
[02:20:42.040 --> 02:20:44.680]   but like they, mRNA, we can bank,
[02:20:44.680 --> 02:20:46.880]   we can develop a vaccine now
[02:20:46.880 --> 02:20:49.960]   when we find a new pathogen within a couple of days.
[02:20:49.960 --> 02:20:51.560]   Now then there's all the trials and so on.
[02:20:51.560 --> 02:20:52.960]   Those trials would have to happen anyway
[02:20:52.960 --> 02:20:54.440]   in the case of a brand new thing.
[02:20:54.440 --> 02:20:56.400]   So you're saving maybe a couple of days.
[02:20:56.400 --> 02:20:57.680]   So that's the upside.
[02:20:57.680 --> 02:21:01.200]   Meanwhile, the downside is you're not only giving,
[02:21:01.200 --> 02:21:03.200]   you're bringing the risk of these pathogens
[02:21:03.200 --> 02:21:04.040]   of like getting leaked,
[02:21:04.040 --> 02:21:06.400]   but you're literally handing it out
[02:21:06.400 --> 02:21:10.320]   to every bad actor on earth who would be doing cartwheels.
[02:21:10.320 --> 02:21:13.280]   And I'm talking about like Kim Jong-un, ISIS,
[02:21:13.280 --> 02:21:14.880]   people who like want,
[02:21:14.880 --> 02:21:17.200]   they think the rest of the world is their enemy.
[02:21:17.200 --> 02:21:19.760]   And in some cases they think that killing themselves
[02:21:19.760 --> 02:21:22.680]   is like a noble cause.
[02:21:22.680 --> 02:21:24.240]   And you're literally giving them the building blocks
[02:21:24.240 --> 02:21:25.080]   of how to do this.
[02:21:25.080 --> 02:21:26.920]   It's the most batshit idea I've ever heard.
[02:21:26.920 --> 02:21:29.560]   Like on expectation, it's probably like minus EV
[02:21:29.560 --> 02:21:31.640]   of like multiple billions of lives
[02:21:31.640 --> 02:21:33.440]   if they actually succeeded in doing this.
[02:21:33.440 --> 02:21:35.760]   Certainly in the tens or hundreds of millions.
[02:21:35.760 --> 02:21:38.840]   So the cost benefit is so unbelievably, it makes no sense.
[02:21:38.840 --> 02:21:41.120]   And I was trying to wrap my head around,
[02:21:41.120 --> 02:21:44.920]   like what's going wrong in people's minds
[02:21:44.920 --> 02:21:46.600]   to think that this is a good idea?
[02:21:46.600 --> 02:21:50.000]   And it's not that it's malice or anything like that.
[02:21:50.000 --> 02:21:53.640]   I think it's that people don't,
[02:21:53.640 --> 02:21:55.200]   the proponents, they don't,
[02:21:56.120 --> 02:21:57.400]   they're actually overly naive
[02:21:57.400 --> 02:22:00.680]   about the interactions of humanity.
[02:22:00.680 --> 02:22:02.840]   And well, like there are bad actors
[02:22:02.840 --> 02:22:04.920]   who will use this for bad things.
[02:22:04.920 --> 02:22:06.160]   Because not only will it,
[02:22:06.160 --> 02:22:08.920]   if you publish this information,
[02:22:08.920 --> 02:22:12.200]   even if a bad actor couldn't physically make it themselves,
[02:22:12.200 --> 02:22:14.080]   which given in 10 years time,
[02:22:14.080 --> 02:22:18.000]   like the technology is getting cheaper and easier to use.
[02:22:18.000 --> 02:22:20.400]   But even if they couldn't make it, they could now bluff it.
[02:22:20.400 --> 02:22:22.960]   Like what would you do if there's like some deadly new virus
[02:22:23.000 --> 02:22:26.160]   that we were published on the internet
[02:22:26.160 --> 02:22:27.600]   in terms of its building blocks?
[02:22:27.600 --> 02:22:28.600]   Kim Jong-un could be like,
[02:22:28.600 --> 02:22:31.560]   "Hey, if you don't let me build my nuclear weapons,
[02:22:31.560 --> 02:22:33.640]   "I'm gonna release this, I've managed to build it."
[02:22:33.640 --> 02:22:35.480]   Well, now he's actually got a credible bluff.
[02:22:35.480 --> 02:22:36.480]   We don't know.
[02:22:36.480 --> 02:22:39.320]   And so that's, it's just like handing the keys,
[02:22:39.320 --> 02:22:42.440]   it's handing weapons of mass destruction to people.
[02:22:42.440 --> 02:22:43.280]   Makes no sense.
[02:22:43.280 --> 02:22:44.560]   - The possible, I agree with you,
[02:22:44.560 --> 02:22:48.320]   but the possible world in which it might make sense
[02:22:48.320 --> 02:22:52.240]   is if the good guys,
[02:22:52.240 --> 02:22:53.800]   which is a whole another problem,
[02:22:53.800 --> 02:22:55.520]   defining who the good guys are,
[02:22:55.520 --> 02:22:59.680]   but the good guys are like an order of magnitude
[02:22:59.680 --> 02:23:00.980]   higher competence.
[02:23:00.980 --> 02:23:06.760]   And so they can stay ahead of the bad actors
[02:23:06.760 --> 02:23:10.120]   by just being very good at the defense.
[02:23:10.120 --> 02:23:13.740]   By very good, not meaning like a little bit better,
[02:23:13.740 --> 02:23:15.920]   but an order of magnitude better.
[02:23:15.920 --> 02:23:17.240]   But of course the question is
[02:23:17.240 --> 02:23:21.720]   in each of those individual disciplines, is that feasible?
[02:23:21.720 --> 02:23:23.480]   Can you, can the bad actors,
[02:23:23.480 --> 02:23:24.920]   even if they don't have the competence,
[02:23:24.920 --> 02:23:29.720]   leapfrog to the place where the good guys are?
[02:23:29.720 --> 02:23:31.880]   - Yeah, I mean, I would agree in principle
[02:23:31.880 --> 02:23:35.720]   with pertaining to this like particular plan of like,
[02:23:35.720 --> 02:23:38.520]   that, you know, with the thing I described,
[02:23:38.520 --> 02:23:39.360]   this deep vision thing,
[02:23:39.360 --> 02:23:41.200]   where at least then that would maybe make sense
[02:23:41.200 --> 02:23:43.440]   for steps one and step two of like getting the information,
[02:23:43.440 --> 02:23:45.680]   but then why would you release it,
[02:23:45.680 --> 02:23:47.280]   the information to your literal enemies?
[02:23:47.280 --> 02:23:49.040]   You know, that's, that makes,
[02:23:49.920 --> 02:23:52.600]   that doesn't fit at all in that perspective
[02:23:52.600 --> 02:23:53.560]   of like trying to be ahead of them.
[02:23:53.560 --> 02:23:55.000]   You're literally handing them the weapon.
[02:23:55.000 --> 02:23:56.640]   - But there's different levels of release, right?
[02:23:56.640 --> 02:24:00.520]   So there's the kind of secrecy
[02:24:00.520 --> 02:24:02.440]   where you don't give it to anybody,
[02:24:02.440 --> 02:24:05.480]   but there's a release where you incrementally give it
[02:24:05.480 --> 02:24:07.720]   to like major labs.
[02:24:07.720 --> 02:24:08.880]   So it's not public release,
[02:24:08.880 --> 02:24:10.960]   but it's like you're giving it to major labs.
[02:24:10.960 --> 02:24:12.880]   - There's different layers of reasonability, but-
[02:24:12.880 --> 02:24:14.640]   - But the problem there is it's going to,
[02:24:14.640 --> 02:24:18.120]   if you go anywhere beyond like complete secrecy,
[02:24:18.120 --> 02:24:19.720]   it's going to leak.
[02:24:19.720 --> 02:24:20.560]   - That's the thing.
[02:24:20.560 --> 02:24:21.600]   It's very hard to keep secrets.
[02:24:21.600 --> 02:24:22.440]   - And so that's still-
[02:24:22.440 --> 02:24:23.440]   - Information is-
[02:24:23.440 --> 02:24:25.920]   - So you might as well release it to the public,
[02:24:25.920 --> 02:24:26.920]   is that argument.
[02:24:26.920 --> 02:24:28.800]   So you either go complete secrecy
[02:24:28.800 --> 02:24:31.720]   or you release it to the public.
[02:24:31.720 --> 02:24:33.920]   So, which is essentially the same thing.
[02:24:33.920 --> 02:24:35.920]   It's going to leak anyway,
[02:24:35.920 --> 02:24:38.240]   if you don't do complete secrecy.
[02:24:38.240 --> 02:24:39.800]   - Right, which is why you shouldn't get the information
[02:24:39.800 --> 02:24:40.640]   in the first place.
[02:24:40.640 --> 02:24:43.720]   - Yeah, I mean, in that, I think-
[02:24:43.720 --> 02:24:44.880]   - Well, that's A solution.
[02:24:44.880 --> 02:24:46.600]   Yeah, the solution is either don't get the information
[02:24:46.600 --> 02:24:51.600]   in the first place or B, keep it incredibly contained.
[02:24:51.600 --> 02:24:54.560]   - See, I think it really matters
[02:24:54.560 --> 02:24:55.800]   which discipline we're talking about.
[02:24:55.800 --> 02:24:59.680]   So in the case of biology, I do think you're very right.
[02:24:59.680 --> 02:25:02.520]   We shouldn't even be, it should be forbidden
[02:25:02.520 --> 02:25:06.640]   to even like think about that.
[02:25:06.640 --> 02:25:09.200]   Meaning don't just even collect the information,
[02:25:09.200 --> 02:25:12.440]   but like don't do, I mean, gain of function research
[02:25:12.440 --> 02:25:14.880]   is a really iffy area.
[02:25:14.880 --> 02:25:15.720]   Like you start-
[02:25:15.720 --> 02:25:17.680]   - I mean, it's all about cost benefits, right?
[02:25:17.680 --> 02:25:19.280]   There are some scenarios where I could imagine
[02:25:19.280 --> 02:25:21.360]   the cost benefit of a gain of function research
[02:25:21.360 --> 02:25:24.480]   is very, very clear, where you've evaluated
[02:25:24.480 --> 02:25:26.840]   all the potential risks, factored in the probability
[02:25:26.840 --> 02:25:28.560]   that things can go wrong and like, you know,
[02:25:28.560 --> 02:25:31.000]   not only known unknowns, but unknown unknowns as well,
[02:25:31.000 --> 02:25:32.360]   tried to quantify that.
[02:25:32.360 --> 02:25:34.440]   And then even then it's like orders of magnitude
[02:25:34.440 --> 02:25:35.480]   better to do that.
[02:25:35.480 --> 02:25:37.320]   I'm behind that argument, but the point is,
[02:25:37.320 --> 02:25:40.480]   is that there's this like naivety that's preventing people
[02:25:40.480 --> 02:25:42.160]   from even doing the cost benefit properly
[02:25:42.160 --> 02:25:43.320]   on a lot of the things.
[02:25:43.320 --> 02:25:47.280]   Because, you know, I get it, the science community,
[02:25:47.280 --> 02:25:49.320]   again, I don't wanna bucket the science community,
[02:25:49.320 --> 02:25:52.160]   but like some people within the science community
[02:25:52.160 --> 02:25:54.080]   just think that everyone's good
[02:25:54.080 --> 02:25:55.560]   and everyone just cares about getting knowledge
[02:25:55.560 --> 02:25:56.920]   and doing the best for the world.
[02:25:56.920 --> 02:25:58.080]   And unfortunately that's not the case.
[02:25:58.080 --> 02:26:00.920]   I wish we lived in that world, but we don't.
[02:26:00.920 --> 02:26:02.400]   - Yeah, I mean, there's a lie.
[02:26:02.400 --> 02:26:05.640]   Listen, I've been criticizing the science community
[02:26:05.640 --> 02:26:07.400]   broadly quite a bit.
[02:26:07.400 --> 02:26:09.840]   There's so many brilliant people that brilliance
[02:26:09.840 --> 02:26:11.320]   is somehow a hindrance sometimes
[02:26:11.320 --> 02:26:13.200]   'cause it has a bunch of blind spots.
[02:26:13.200 --> 02:26:16.440]   And then you start to look at the history of science,
[02:26:16.440 --> 02:26:19.080]   how easily it's been used by dictators
[02:26:19.080 --> 02:26:20.880]   to any conclusion they want.
[02:26:20.880 --> 02:26:24.900]   And it's dark how you can use brilliant people
[02:26:24.900 --> 02:26:27.200]   that like playing the little game of science,
[02:26:27.200 --> 02:26:28.820]   'cause it is a fun game.
[02:26:28.820 --> 02:26:30.740]   You know, you're building, you're going to conferences,
[02:26:30.740 --> 02:26:32.360]   you're building on top of each other's ideas,
[02:26:32.360 --> 02:26:33.280]   there's breakthroughs.
[02:26:33.280 --> 02:26:37.060]   Hi, I think I've realized how this particular molecule works
[02:26:37.060 --> 02:26:38.600]   and I could do this kind of experiment
[02:26:38.600 --> 02:26:39.720]   and everyone else is impressed.
[02:26:39.720 --> 02:26:40.560]   Ooh, cool.
[02:26:40.560 --> 02:26:41.440]   No, I think you're wrong.
[02:26:41.440 --> 02:26:42.520]   Let me show you why you're wrong.
[02:26:42.520 --> 02:26:44.880]   In that little game, everyone gets really excited
[02:26:44.880 --> 02:26:46.120]   and they get excited.
[02:26:46.120 --> 02:26:48.080]   Oh, I came up with a pill that solves this problem
[02:26:48.080 --> 02:26:49.560]   and it's gonna help a bunch of people.
[02:26:49.560 --> 02:26:51.400]   And I came up with a giant study
[02:26:51.400 --> 02:26:54.520]   that shows the exact probability it's gonna help or not.
[02:26:54.520 --> 02:26:56.360]   And you get lost in this game
[02:26:56.360 --> 02:27:00.100]   and you forget to realize this game, just like Mullick,
[02:27:00.100 --> 02:27:03.120]   can have like--
[02:27:03.120 --> 02:27:04.760]   - Unintended consequences, yeah.
[02:27:04.760 --> 02:27:07.600]   - Unintended consequences that might destroy
[02:27:07.600 --> 02:27:12.600]   human civilization or divide human civilization
[02:27:12.600 --> 02:27:17.560]   or have dire geopolitical consequences.
[02:27:17.560 --> 02:27:20.360]   I mean, the effects of, I mean, it's just so,
[02:27:20.360 --> 02:27:22.920]   the most destructive effects of COVID
[02:27:22.920 --> 02:27:25.800]   have nothing to do with the biology of the virus,
[02:27:25.800 --> 02:27:26.640]   it seems like.
[02:27:26.640 --> 02:27:29.600]   I mean, I could just list them forever.
[02:27:29.600 --> 02:27:32.240]   But like one of them is the complete distrust
[02:27:32.240 --> 02:27:34.200]   of public institutions.
[02:27:34.200 --> 02:27:36.400]   The other one is because of that public distrust,
[02:27:36.400 --> 02:27:38.880]   I feel like if a much worse pandemic came along,
[02:27:38.880 --> 02:27:42.520]   we as a world have not cried wolf.
[02:27:42.520 --> 02:27:45.520]   And if an actual wolf now comes,
[02:27:45.520 --> 02:27:48.400]   people will be like, "Fuck masks, fuck--"
[02:27:48.400 --> 02:27:50.080]   - "Fuck vaccines, fuck everything."
[02:27:50.080 --> 02:27:53.640]   - And they won't be, they'll distrust every single thing
[02:27:53.640 --> 02:27:56.040]   that any major institution is gonna tell them.
[02:27:56.040 --> 02:27:57.040]   And--
[02:27:57.040 --> 02:27:58.280]   - Because that's the thing,
[02:27:58.280 --> 02:28:04.560]   there were certain actions made by certain,
[02:28:04.640 --> 02:28:07.800]   health public figures where they told,
[02:28:07.800 --> 02:28:10.760]   they very knowingly told, it was a white lie,
[02:28:10.760 --> 02:28:12.440]   it was intended in the best possible way,
[02:28:12.440 --> 02:28:17.440]   such as early on when there was clearly a shortage of masks.
[02:28:17.440 --> 02:28:23.240]   And so they said to the public, "Oh, don't get masks,
[02:28:23.240 --> 02:28:25.080]   there's no evidence that they work.
[02:28:25.080 --> 02:28:27.280]   Don't get them, they don't work.
[02:28:27.280 --> 02:28:29.000]   In fact, it might even make it worse.
[02:28:29.000 --> 02:28:30.240]   You might even spread it more."
[02:28:30.240 --> 02:28:32.280]   Like that was the real like stinker.
[02:28:32.280 --> 02:28:34.360]   Yeah, no, no.
[02:28:34.360 --> 02:28:35.200]   "The less you know how to do it properly,
[02:28:35.200 --> 02:28:36.680]   you're gonna make that you're gonna get sicker,
[02:28:36.680 --> 02:28:38.720]   or you're more likely to catch the virus,"
[02:28:38.720 --> 02:28:41.360]   which is just absolute crap.
[02:28:41.360 --> 02:28:43.160]   And they put that out there.
[02:28:43.160 --> 02:28:45.320]   And it's pretty clear the reason why they did that
[02:28:45.320 --> 02:28:47.640]   was because there was actually a shortage of masks
[02:28:47.640 --> 02:28:50.200]   and they really needed it for health workers,
[02:28:50.200 --> 02:28:52.800]   which makes sense, like I agree.
[02:28:52.800 --> 02:28:57.800]   But the cost of lying to the public when that then comes out,
[02:28:57.800 --> 02:29:02.760]   people aren't as stupid as they think they are.
[02:29:02.760 --> 02:29:05.880]   And that's, I think, where this distrust of experts
[02:29:05.880 --> 02:29:06.720]   has largely come from.
[02:29:06.720 --> 02:29:09.360]   A, they've lied to people overtly,
[02:29:09.360 --> 02:29:13.160]   but B, people have been treated like idiots.
[02:29:13.160 --> 02:29:14.800]   Now, that's not to say that there aren't a lot of stupid
[02:29:14.800 --> 02:29:16.920]   people who have a lot of wacky ideas around COVID
[02:29:16.920 --> 02:29:18.200]   and all sorts of things,
[02:29:18.200 --> 02:29:21.560]   but if you treat the general public like children,
[02:29:21.560 --> 02:29:23.480]   they're going to see that, they're going to notice that,
[02:29:23.480 --> 02:29:26.760]   and that is going to absolutely decimate the trust
[02:29:26.760 --> 02:29:29.520]   in the public institutions that we depend upon.
[02:29:29.520 --> 02:29:32.240]   And honestly, the best thing that could happen,
[02:29:32.240 --> 02:29:36.400]   I wish, if Fauci and these other leaders who,
[02:29:36.400 --> 02:29:39.760]   I mean, God, I can't imagine how nightmare his job has been
[02:29:39.760 --> 02:29:41.360]   over the last few years, hell on earth.
[02:29:41.360 --> 02:29:46.360]   So I have a lot of sympathy for the position he's been in.
[02:29:46.360 --> 02:29:48.720]   But if he could just come out and be like,
[02:29:48.720 --> 02:29:51.120]   "Okay, look, guys, hands up.
[02:29:51.120 --> 02:29:53.920]   "We didn't handle this as well as we could have.
[02:29:53.920 --> 02:29:55.840]   "These are all the things I would have done differently
[02:29:55.840 --> 02:29:56.680]   "in hindsight.
[02:29:56.680 --> 02:29:58.680]   "I apologize for this and this and this and this."
[02:29:58.680 --> 02:30:01.360]   That would go so far,
[02:30:01.360 --> 02:30:03.040]   and maybe I'm being naive, who knows?
[02:30:03.040 --> 02:30:05.040]   Maybe this would backfire, but I don't think it would.
[02:30:05.040 --> 02:30:06.240]   To someone like me, even,
[02:30:06.240 --> 02:30:08.840]   'cause I've lost trust in a lot of these things.
[02:30:08.840 --> 02:30:10.360]   But I'm fortunate that I at least know people
[02:30:10.360 --> 02:30:12.680]   who I can go to who I think have good epistemics
[02:30:12.680 --> 02:30:13.520]   on this stuff.
[02:30:13.520 --> 02:30:16.360]   But if they could sort of put their hands on and go,
[02:30:16.360 --> 02:30:18.160]   "Okay, these are the spots where we screwed up.
[02:30:18.160 --> 02:30:20.200]   "This, this, this.
[02:30:20.200 --> 02:30:21.200]   "This was our reasons.
[02:30:21.200 --> 02:30:22.680]   "Yeah, we actually told a little white lie here.
[02:30:22.680 --> 02:30:23.600]   "We did it for this reason.
[02:30:23.600 --> 02:30:24.960]   "We're really sorry."
[02:30:24.960 --> 02:30:26.720]   Where they just did the radical honesty thing,
[02:30:26.720 --> 02:30:28.680]   the radical transparency thing,
[02:30:28.680 --> 02:30:32.160]   that would go so far to rebuilding public trust.
[02:30:32.160 --> 02:30:33.320]   And I think that's what needs to happen.
[02:30:33.320 --> 02:30:34.680]   - Yeah, I totally agree with you.
[02:30:34.680 --> 02:30:38.680]   Unfortunately, his job was very tough
[02:30:38.680 --> 02:30:39.800]   and all those kinds of things.
[02:30:39.800 --> 02:30:42.920]   But I see arrogance,
[02:30:42.920 --> 02:30:44.600]   and arrogance prevented him
[02:30:44.600 --> 02:30:47.840]   from being honest in that way previously.
[02:30:47.840 --> 02:30:49.480]   And I think arrogance will prevent him
[02:30:49.480 --> 02:30:52.840]   from being honest in that way now when he leaders.
[02:30:52.840 --> 02:30:55.440]   I think young people are seeing that,
[02:30:55.440 --> 02:30:59.960]   that kind of talking down to people
[02:30:59.960 --> 02:31:02.080]   from a position of power,
[02:31:02.080 --> 02:31:04.400]   I hope is the way of the past.
[02:31:04.400 --> 02:31:06.120]   People really like authenticity
[02:31:06.120 --> 02:31:10.600]   and they like leaders that are like a man
[02:31:10.600 --> 02:31:12.320]   and a woman of the people.
[02:31:12.320 --> 02:31:15.000]   And I think that just-
[02:31:15.000 --> 02:31:17.040]   - I mean, he still has a chance to do that, I think.
[02:31:17.040 --> 02:31:18.280]   I mean, I don't wanna- - Yeah, sure.
[02:31:18.280 --> 02:31:19.280]   - I don't think he's, you know,
[02:31:19.280 --> 02:31:20.600]   if I doubt he's listening,
[02:31:20.600 --> 02:31:24.000]   but if he is, like, hey, I think, you know,
[02:31:24.000 --> 02:31:25.680]   I don't think he's irredeemable by any means.
[02:31:25.680 --> 02:31:27.560]   I think there's, you know,
[02:31:27.560 --> 02:31:28.720]   I don't have an opinion
[02:31:28.720 --> 02:31:30.720]   of whether there was arrogance or there or not.
[02:31:30.720 --> 02:31:34.200]   Just know that I think, like, coming clean on the,
[02:31:34.200 --> 02:31:36.960]   you know, it's understandable to have fucked up
[02:31:36.960 --> 02:31:37.800]   during this pandemic.
[02:31:37.800 --> 02:31:39.440]   Like, I won't expect any government to handle it well
[02:31:39.440 --> 02:31:41.120]   because it was so difficult,
[02:31:41.120 --> 02:31:42.880]   like, so many moving pieces,
[02:31:42.880 --> 02:31:46.080]   so much, like, lack of information and so on.
[02:31:46.080 --> 02:31:48.760]   But the step to rebuilding trust is to go,
[02:31:48.760 --> 02:31:51.320]   okay, look, we're doing a scrutiny of where we went wrong.
[02:31:51.320 --> 02:31:53.920]   And for my part, I did this wrong in this part.
[02:31:53.920 --> 02:31:55.160]   - That would be huge.
[02:31:55.160 --> 02:31:56.040]   - All of us can do that.
[02:31:56.040 --> 02:31:57.400]   I mean, I was struggling for a while
[02:31:57.400 --> 02:32:00.040]   whether I wanna talk to him or not.
[02:32:00.040 --> 02:32:01.960]   I talked to his boss, Francis Collins.
[02:32:01.960 --> 02:32:06.380]   Another person that screwed up in terms of trust,
[02:32:06.380 --> 02:32:10.320]   lost a little bit of my respect too.
[02:32:10.320 --> 02:32:14.680]   There seems to have been a kind of dishonesty
[02:32:14.680 --> 02:32:18.080]   in the back rooms,
[02:32:18.080 --> 02:32:23.080]   in that they didn't trust people to be intelligent.
[02:32:23.080 --> 02:32:26.040]   Like, we need to tell them what's good for them.
[02:32:26.040 --> 02:32:29.000]   We know what's good for them, that kind of idea.
[02:32:29.000 --> 02:32:32.560]   - To be fair, the thing that's,
[02:32:32.560 --> 02:32:33.400]   what's it called?
[02:32:33.400 --> 02:32:36.440]   I heard the phrase today, nut picking.
[02:32:36.440 --> 02:32:37.800]   Social media does that.
[02:32:37.800 --> 02:32:39.080]   So you've got like nitpicking.
[02:32:39.080 --> 02:32:44.080]   Nut picking is where the craziest, stupidest,
[02:32:44.080 --> 02:32:45.920]   you know, if you have a group of people,
[02:32:45.920 --> 02:32:47.760]   let's call it, you know, let's say people who are vaccine,
[02:32:47.760 --> 02:32:48.760]   I don't like the term anti-vaccine,
[02:32:48.760 --> 02:32:52.080]   people who are vaccine hesitant, vaccine speculative,
[02:32:52.080 --> 02:32:56.440]   you know, what social media did or the media or anyone,
[02:32:56.440 --> 02:32:59.240]   you know, their opponents would do
[02:32:59.240 --> 02:33:00.960]   is pick the craziest example.
[02:33:00.960 --> 02:33:02.280]   So the ones who are like, you know,
[02:33:02.280 --> 02:33:04.360]   I think I need to inject myself with like,
[02:33:04.360 --> 02:33:07.880]   motor oil up my ass or something, you know,
[02:33:07.880 --> 02:33:11.600]   select the craziest ones and then have that beamed to,
[02:33:11.600 --> 02:33:13.040]   you know, so from like someone like Fauci
[02:33:13.040 --> 02:33:15.680]   or Francis's perspective, that's what they get
[02:33:15.680 --> 02:33:17.440]   because they're getting the same social media stuff as us.
[02:33:17.440 --> 02:33:18.960]   They're getting the same media reports.
[02:33:18.960 --> 02:33:20.880]   I mean, they might get some more information,
[02:33:20.880 --> 02:33:24.880]   but they too are gonna get the nuts portrayed to them.
[02:33:24.880 --> 02:33:27.200]   So they probably have a misrepresentation
[02:33:27.200 --> 02:33:29.320]   of what the actual public's intelligence is.
[02:33:29.320 --> 02:33:31.280]   - Well, that just, yes.
[02:33:31.280 --> 02:33:33.640]   And that just means they're not social media savvy.
[02:33:33.640 --> 02:33:36.000]   So one of the skills of being on social media
[02:33:36.000 --> 02:33:37.840]   is to be able to filter that in your mind,
[02:33:37.840 --> 02:33:40.120]   like to understand, to put into proper context.
[02:33:40.120 --> 02:33:41.760]   - To realize that what you are saying,
[02:33:41.760 --> 02:33:43.920]   social media is not anywhere near
[02:33:43.920 --> 02:33:46.600]   an accurate representation of humanity.
[02:33:46.600 --> 02:33:49.520]   - Nut picking, and there's nothing wrong
[02:33:49.520 --> 02:33:51.360]   with putting motor oil up your ass.
[02:33:51.360 --> 02:33:54.480]   It's just one of the better aspects of,
[02:33:54.480 --> 02:33:56.400]   I do this every weekend.
[02:33:56.400 --> 02:33:57.240]   Okay.
[02:33:57.240 --> 02:33:59.880]   - Where the hell did that analogy come from in my mind?
[02:33:59.880 --> 02:34:00.720]   Like what?
[02:34:00.720 --> 02:34:01.560]   - I don't know.
[02:34:01.560 --> 02:34:03.760]   I think you need to, there's some Freudian thing
[02:34:03.760 --> 02:34:06.680]   we need to deeply investigate with a therapist.
[02:34:06.680 --> 02:34:08.320]   Okay, what about AI?
[02:34:08.320 --> 02:34:13.100]   Are you worried about AGI, superintelligence systems,
[02:34:13.100 --> 02:34:16.240]   or paperclip maximizer type of situation?
[02:34:17.200 --> 02:34:19.960]   - Yes, I'm definitely worried about it,
[02:34:19.960 --> 02:34:24.320]   but I feel kind of bipolar in that some days I wake up
[02:34:24.320 --> 02:34:25.160]   and I'm like--
[02:34:25.160 --> 02:34:26.120]   - You're excited about the future?
[02:34:26.120 --> 02:34:26.960]   - Well, exactly.
[02:34:26.960 --> 02:34:29.440]   I'm like, wow, we can unlock the mysteries of the universe,
[02:34:29.440 --> 02:34:30.760]   you know, escape the game.
[02:34:30.760 --> 02:34:35.760]   'Cause I spend all my time thinking
[02:34:35.760 --> 02:34:37.000]   about these molecule problems,
[02:34:37.000 --> 02:34:38.960]   that what is the solution to them?
[02:34:38.960 --> 02:34:43.280]   In some ways you need this like omnibenevolent,
[02:34:43.280 --> 02:34:48.280]   omniscient, omni-wise coordination mechanism
[02:34:48.280 --> 02:34:53.000]   that can like make us all not do the molecule thing,
[02:34:53.000 --> 02:34:55.560]   or like provide the infrastructure,
[02:34:55.560 --> 02:34:57.440]   or redesign the system so that it's not vulnerable
[02:34:57.440 --> 02:34:58.900]   to this molecule process.
[02:34:58.900 --> 02:35:01.160]   And in some ways, you know,
[02:35:01.160 --> 02:35:02.560]   that's the strongest argument to me
[02:35:02.560 --> 02:35:04.960]   for like the race to build AGI,
[02:35:04.960 --> 02:35:08.000]   is that maybe, you know, we can't survive without it.
[02:35:08.000 --> 02:35:10.640]   But the flip side to that is
[02:35:13.260 --> 02:35:14.880]   unfortunately now that there's multiple actors
[02:35:14.880 --> 02:35:17.280]   trying to build AI, AGI, you know,
[02:35:17.280 --> 02:35:19.920]   this was fine 10 years ago when it was just DeepMind,
[02:35:19.920 --> 02:35:22.040]   but then other companies started up
[02:35:22.040 --> 02:35:23.340]   and now it created a race dynamic.
[02:35:23.340 --> 02:35:27.000]   Now it's like, the whole thing is at the same,
[02:35:27.000 --> 02:35:27.960]   it's got the same problem.
[02:35:27.960 --> 02:35:30.040]   It's like, whichever company is the one
[02:35:30.040 --> 02:35:33.580]   that like optimizes for speed at the cost of safety
[02:35:33.580 --> 02:35:35.280]   will get the competitive advantage,
[02:35:35.280 --> 02:35:37.360]   and so we'll be the more likely the ones to build the AGI,
[02:35:37.360 --> 02:35:40.280]   you know, and that's the same cycle that you're in.
[02:35:40.280 --> 02:35:41.600]   And there's no clear solution to that,
[02:35:41.600 --> 02:35:45.020]   'cause you can't just go like slapping,
[02:35:45.020 --> 02:35:51.160]   if you go and try and like stop all the different companies,
[02:35:51.160 --> 02:35:54.640]   then it will, you know, the good ones will stop
[02:35:54.640 --> 02:35:55.680]   because they're the ones, you know,
[02:35:55.680 --> 02:35:57.680]   within the West's reach,
[02:35:57.680 --> 02:36:00.000]   but then that leaves all the other ones to continue
[02:36:00.000 --> 02:36:01.000]   and then they're even more likely.
[02:36:01.000 --> 02:36:03.160]   So it's like, it's a very difficult problem
[02:36:03.160 --> 02:36:04.420]   with no clean solution.
[02:36:04.420 --> 02:36:08.640]   And, you know, at the same time, you know,
[02:36:08.640 --> 02:36:12.120]   I know at least some of the folks at DeepMind
[02:36:12.120 --> 02:36:13.760]   and they're incredible and they're thinking about this.
[02:36:13.760 --> 02:36:15.720]   They're very aware of this problem and they're like,
[02:36:15.720 --> 02:36:18.720]   you know, I think some of the smartest people on earth.
[02:36:18.720 --> 02:36:20.640]   - Yeah, the culture is important there
[02:36:20.640 --> 02:36:22.000]   because they are thinking about that
[02:36:22.000 --> 02:36:26.240]   and they're some of the best machine learning engineers.
[02:36:26.240 --> 02:36:29.800]   So it's possible to have a company or a community of people
[02:36:29.800 --> 02:36:31.720]   that are both great engineers
[02:36:31.720 --> 02:36:33.760]   and are thinking about the philosophical topics.
[02:36:33.760 --> 02:36:36.720]   - Exactly, and importantly, they're also game theorists,
[02:36:36.720 --> 02:36:38.240]   you know, and because this is ultimately
[02:36:38.240 --> 02:36:41.640]   a game theory problem, the thing, this Moloch mechanism
[02:36:41.640 --> 02:36:46.640]   and like, you know, how do we voice arms race scenarios?
[02:36:46.640 --> 02:36:50.040]   You need people who aren't naive to be thinking about this.
[02:36:50.040 --> 02:36:52.000]   And again, like luckily there's a lot of smart,
[02:36:52.000 --> 02:36:54.800]   non-naive game theorists within that group.
[02:36:54.800 --> 02:36:56.000]   Yes, I'm concerned about it.
[02:36:56.000 --> 02:36:59.640]   And I think it's again, a thing that we need people
[02:36:59.640 --> 02:37:02.360]   to be thinking about in terms of like,
[02:37:02.360 --> 02:37:05.960]   how do we create, how do we mitigate the arms race dynamics
[02:37:05.960 --> 02:37:10.280]   and how do we solve the thing of,
[02:37:10.280 --> 02:37:13.560]   Bostrom calls it the orthogonality problem whereby,
[02:37:13.560 --> 02:37:16.080]   because obviously there's a chance, you know,
[02:37:16.080 --> 02:37:17.480]   the belief, the hope is,
[02:37:17.480 --> 02:37:19.760]   is that you build something that's super intelligent
[02:37:19.760 --> 02:37:22.960]   and by definition of being super intelligent,
[02:37:22.960 --> 02:37:25.160]   it will also become super wise
[02:37:25.160 --> 02:37:27.840]   and have the wisdom to know what the right goals are.
[02:37:27.840 --> 02:37:30.960]   And hopefully those goals include keeping humanity alive.
[02:37:30.960 --> 02:37:35.320]   Right, but Bostrom says that actually those two things,
[02:37:35.320 --> 02:37:38.280]   you know, super intelligence and super wisdom
[02:37:38.280 --> 02:37:40.400]   aren't necessarily correlated.
[02:37:40.400 --> 02:37:42.880]   They're actually kind of orthogonal things.
[02:37:42.880 --> 02:37:44.800]   And how do we make it so that they are correlated?
[02:37:44.800 --> 02:37:45.640]   How do we guarantee it?
[02:37:45.640 --> 02:37:47.040]   Because we need it to be guaranteed really,
[02:37:47.040 --> 02:37:48.920]   to know that we're doing the thing safely.
[02:37:48.920 --> 02:37:53.920]   - But I think that like merging of intelligence and wisdom,
[02:37:53.920 --> 02:37:56.920]   at least my hope is that this whole process
[02:37:56.920 --> 02:37:58.840]   happens sufficiently slowly,
[02:37:58.840 --> 02:38:01.520]   that we're constantly having these kinds of debates,
[02:38:02.040 --> 02:38:06.160]   that we have enough time to figure out
[02:38:06.160 --> 02:38:07.920]   how to modify each version of the system
[02:38:07.920 --> 02:38:09.600]   as it becomes more and more intelligent.
[02:38:09.600 --> 02:38:12.040]   - Yes, buying time is a good thing, definitely.
[02:38:12.040 --> 02:38:14.240]   Anything that slows everything down,
[02:38:14.240 --> 02:38:16.080]   we just, everyone needs to chill out.
[02:38:16.080 --> 02:38:19.560]   We've got millennia to figure this out.
[02:38:19.560 --> 02:38:26.560]   Or at least, well, it depends again.
[02:38:26.560 --> 02:38:27.800]   Some people think that, you know,
[02:38:27.800 --> 02:38:29.800]   we can't even make it through the next few decades
[02:38:29.800 --> 02:38:31.560]   without having some kind of
[02:38:31.560 --> 02:38:36.320]   omni-wise coordination mechanism.
[02:38:36.320 --> 02:38:37.960]   And there's also an argument to that.
[02:38:37.960 --> 02:38:39.360]   Yeah, I don't know.
[02:38:39.360 --> 02:38:42.240]   - Well, there is, I'm suspicious of that kind of thinking
[02:38:42.240 --> 02:38:45.160]   because it seems like the entirety of human history
[02:38:45.160 --> 02:38:48.600]   has people in it that are like predicting doom
[02:38:48.600 --> 02:38:50.760]   or just around the corner.
[02:38:50.760 --> 02:38:52.720]   There's something about us
[02:38:52.720 --> 02:38:56.280]   that is strangely attracted to that thought.
[02:38:57.320 --> 02:38:59.680]   It's almost like fun to think about
[02:38:59.680 --> 02:39:01.200]   the destruction of everything.
[02:39:01.200 --> 02:39:04.320]   Just objectively speaking,
[02:39:04.320 --> 02:39:08.160]   I've talked and listened to a bunch of people
[02:39:08.160 --> 02:39:11.120]   and they are gravitating towards that.
[02:39:11.120 --> 02:39:13.200]   It's almost, I think it's the same thing
[02:39:13.200 --> 02:39:15.720]   that people love about conspiracy theories
[02:39:15.720 --> 02:39:19.200]   is they love to be the person that kind of figured out
[02:39:19.200 --> 02:39:22.120]   some deep fundamental thing about the,
[02:39:22.120 --> 02:39:23.400]   that's going to be,
[02:39:23.400 --> 02:39:26.160]   it's going to mark something extremely important
[02:39:26.160 --> 02:39:28.320]   about the history of human civilization
[02:39:28.320 --> 02:39:31.320]   because then I will be important.
[02:39:31.320 --> 02:39:33.720]   When in reality, most of us will be forgotten
[02:39:33.720 --> 02:39:37.640]   and life will go on.
[02:39:37.640 --> 02:39:40.040]   And one of the sad things about
[02:39:40.040 --> 02:39:42.000]   whenever anything traumatic happens to you,
[02:39:42.000 --> 02:39:46.720]   whenever you lose loved ones or just tragedy happens,
[02:39:46.720 --> 02:39:48.260]   you realize life goes on.
[02:39:48.260 --> 02:39:52.160]   Even after a nuclear war that will wipe out
[02:39:52.160 --> 02:39:55.040]   some large percentage of the population
[02:39:55.040 --> 02:40:00.040]   and will torture people for years to come
[02:40:00.040 --> 02:40:01.960]   because of the sort of,
[02:40:01.960 --> 02:40:05.440]   I mean, the effects of a nuclear winter,
[02:40:05.440 --> 02:40:07.280]   people will still survive,
[02:40:07.280 --> 02:40:08.720]   life will still go on.
[02:40:08.720 --> 02:40:10.960]   I mean, it depends on the kind of nuclear war,
[02:40:10.960 --> 02:40:13.520]   but in case of nuclear war, it will still go on.
[02:40:13.520 --> 02:40:15.840]   That's one of the amazing things about life,
[02:40:15.840 --> 02:40:17.080]   it finds a way.
[02:40:17.080 --> 02:40:18.320]   And so in that sense,
[02:40:18.320 --> 02:40:22.600]   I just, I feel like the doom and gloom thing is a--
[02:40:23.760 --> 02:40:26.120]   - Well, we don't want a self-fulfilling prophecy.
[02:40:26.120 --> 02:40:27.480]   - Yes, that's exactly.
[02:40:27.480 --> 02:40:29.680]   - Yes, and I very much agree with that.
[02:40:29.680 --> 02:40:32.980]   And I even have a slight feeling
[02:40:32.980 --> 02:40:35.800]   from the amount of time we've spent in this conversation
[02:40:35.800 --> 02:40:37.600]   talking about this 'cause it's like,
[02:40:37.600 --> 02:40:40.200]   is this even a net positive
[02:40:40.200 --> 02:40:41.880]   if it's making everyone feel,
[02:40:41.880 --> 02:40:45.000]   or in some ways, making people imagine
[02:40:45.000 --> 02:40:47.480]   these bad scenarios can be a self-fulfilling prophecy.
[02:40:47.480 --> 02:40:51.180]   But at the same time, that's weighed off
[02:40:51.180 --> 02:40:54.360]   with at least making people aware of the problem
[02:40:54.360 --> 02:40:55.320]   and gets them thinking.
[02:40:55.320 --> 02:40:56.280]   And I think particularly,
[02:40:56.280 --> 02:40:58.360]   the reason why I wanna talk about this to your audience
[02:40:58.360 --> 02:41:00.680]   is that on average, they're the type of people
[02:41:00.680 --> 02:41:02.800]   who gravitate towards these kind of topics
[02:41:02.800 --> 02:41:04.960]   'cause they're intellectually curious
[02:41:04.960 --> 02:41:07.880]   and they can sort of sense that there's trouble brewing.
[02:41:07.880 --> 02:41:09.400]   They can smell that there's,
[02:41:09.400 --> 02:41:10.240]   I think there's a reason
[02:41:10.240 --> 02:41:11.440]   people are thinking about this stuff a lot
[02:41:11.440 --> 02:41:13.640]   is because the probability,
[02:41:13.640 --> 02:41:16.840]   it's increased in probability
[02:41:16.840 --> 02:41:19.440]   over certainly over the last few years.
[02:41:19.440 --> 02:41:21.700]   Trajectories have not gone favorably,
[02:41:21.700 --> 02:41:24.260]   let's put it since 2010.
[02:41:24.260 --> 02:41:28.460]   So it's right, I think, for people to be thinking about it.
[02:41:28.460 --> 02:41:30.060]   But that's where they're like,
[02:41:30.060 --> 02:41:31.660]   I think whether it's a useful fiction
[02:41:31.660 --> 02:41:33.340]   or whether it's actually true
[02:41:33.340 --> 02:41:34.420]   or whatever you wanna call it,
[02:41:34.420 --> 02:41:35.940]   I think having this faith,
[02:41:35.940 --> 02:41:38.060]   this is where faith is valuable
[02:41:38.060 --> 02:41:41.140]   because it gives you at least this anchor of hope.
[02:41:41.140 --> 02:41:43.900]   And I'm not just saying it to trick myself.
[02:41:43.900 --> 02:41:44.780]   Like I do truly,
[02:41:44.780 --> 02:41:47.700]   I do think there's something out there that wants us to win.
[02:41:47.700 --> 02:41:49.720]   I think there's something that really wants us to win.
[02:41:49.720 --> 02:41:53.120]   And it just, you just have to be like,
[02:41:53.120 --> 02:41:55.800]   just like, okay, now I sound really crazy,
[02:41:55.800 --> 02:41:58.800]   but like open your heart to it a little bit.
[02:41:58.800 --> 02:42:03.200]   And it will give you the like,
[02:42:03.200 --> 02:42:04.880]   the sort of breathing room
[02:42:04.880 --> 02:42:07.720]   with which to marinate on the solutions.
[02:42:07.720 --> 02:42:10.400]   We are the ones who have to come up with the solutions,
[02:42:10.400 --> 02:42:15.000]   but we can use,
[02:42:15.000 --> 02:42:18.060]   there's like this hashtag positivity.
[02:42:18.060 --> 02:42:19.340]   There's value in that.
[02:42:19.340 --> 02:42:21.340]   - Yeah, you have to kind of imagine
[02:42:21.340 --> 02:42:24.980]   all the destructive trajectories that lay in our future
[02:42:24.980 --> 02:42:27.980]   and then believe in the possibility
[02:42:27.980 --> 02:42:29.500]   of avoiding those trajectories.
[02:42:29.500 --> 02:42:32.300]   All while, you said audience,
[02:42:32.300 --> 02:42:35.140]   all while sitting back, which is majority,
[02:42:35.140 --> 02:42:36.700]   the two people that listen to this
[02:42:36.700 --> 02:42:38.700]   are probably sitting on a beach,
[02:42:38.700 --> 02:42:40.280]   smoking some weed.
[02:42:40.280 --> 02:42:43.220]   - God damn it.
[02:42:43.240 --> 02:42:44.880]   - It's a beautiful sunset,
[02:42:44.880 --> 02:42:47.880]   or they're looking at just the waves going in and out.
[02:42:47.880 --> 02:42:50.440]   And ultimately there's a kind of deep belief there
[02:42:50.440 --> 02:42:55.440]   in the momentum of humanity to figure it all out.
[02:42:55.440 --> 02:42:58.400]   - I think we'll make it, but we've got a lot of work to do.
[02:42:58.400 --> 02:43:01.040]   - Which is what makes this whole simulation,
[02:43:01.040 --> 02:43:02.440]   this video game kind of fun.
[02:43:02.440 --> 02:43:06.720]   This battle of Polytopia, I still,
[02:43:06.720 --> 02:43:08.560]   man, I love those games so much.
[02:43:08.560 --> 02:43:09.400]   - They're so good.
[02:43:09.400 --> 02:43:11.760]   - And that one for people who don't know,
[02:43:11.760 --> 02:43:16.760]   Battle of Polytopia is a really radical simplification
[02:43:16.760 --> 02:43:20.540]   of a civilization type of game.
[02:43:20.540 --> 02:43:24.140]   It still has a lot of the skill tree development,
[02:43:24.140 --> 02:43:25.800]   a lot of the strategy,
[02:43:25.800 --> 02:43:29.900]   but it's easy enough to play on a phone.
[02:43:29.900 --> 02:43:30.740]   - Yeah.
[02:43:30.740 --> 02:43:32.140]   - It's kind of interesting.
[02:43:32.140 --> 02:43:33.260]   - They've really figured it out.
[02:43:33.260 --> 02:43:35.580]   It's one of the most elegantly designed games I've ever seen.
[02:43:35.580 --> 02:43:37.980]   It's incredibly complex.
[02:43:37.980 --> 02:43:39.580]   And yet being, again, it walks that line
[02:43:39.580 --> 02:43:40.980]   between complexity and simplicity
[02:43:40.980 --> 02:43:42.580]   in this really, really great way.
[02:43:42.580 --> 02:43:46.220]   And they use pretty colors that hack
[02:43:46.220 --> 02:43:49.740]   the dopamine reward circuits in our brains very well.
[02:43:49.740 --> 02:43:50.860]   - It's fun.
[02:43:50.860 --> 02:43:52.300]   Video games are so fun.
[02:43:52.300 --> 02:43:53.580]   - Yeah.
[02:43:53.580 --> 02:43:55.540]   - Most of this life is just about fun,
[02:43:55.540 --> 02:43:57.780]   escaping all the suffering to find the fun.
[02:43:57.780 --> 02:43:59.980]   What's energy healing?
[02:43:59.980 --> 02:44:02.260]   I have in my notes, energy healing question mark.
[02:44:02.260 --> 02:44:03.100]   What's that about?
[02:44:03.100 --> 02:44:05.540]   (laughing)
[02:44:05.540 --> 02:44:06.880]   - Oh man.
[02:44:06.880 --> 02:44:09.500]   God, your audience are gonna think I'm mad.
[02:44:09.500 --> 02:44:13.300]   So the two crazy things that happened to me,
[02:44:13.300 --> 02:44:15.020]   the one was the voice in the head
[02:44:15.020 --> 02:44:16.140]   that said you're gonna win this tournament,
[02:44:16.140 --> 02:44:18.300]   and then I won the tournament.
[02:44:18.300 --> 02:44:20.980]   The other craziest thing that's happened to me
[02:44:20.980 --> 02:44:23.740]   was in 2018,
[02:44:23.740 --> 02:44:30.340]   I started getting this weird problem in my ear
[02:44:30.340 --> 02:44:35.300]   where it was kind of like low frequency sound distortion,
[02:44:35.300 --> 02:44:37.340]   where voices, particularly men's voices,
[02:44:37.340 --> 02:44:39.580]   became incredibly unpleasant to listen to.
[02:44:39.580 --> 02:44:42.420]   It would create this,
[02:44:42.420 --> 02:44:44.140]   it would be falsely amplified or something,
[02:44:44.140 --> 02:44:46.380]   and it was almost like a physical sensation in my ear,
[02:44:46.380 --> 02:44:48.140]   which was really unpleasant.
[02:44:48.140 --> 02:44:51.020]   And it would last for a few hours and then go away,
[02:44:51.020 --> 02:44:52.580]   and then come back for a few hours and go away.
[02:44:52.580 --> 02:44:54.500]   And I went and got hearing tests,
[02:44:54.500 --> 02:44:56.380]   and they found that the bottom end,
[02:44:56.380 --> 02:44:58.400]   I was losing the hearing in that ear.
[02:44:58.400 --> 02:45:03.620]   And so in the end,
[02:45:03.620 --> 02:45:05.380]   the doctors said they think it was
[02:45:05.380 --> 02:45:07.260]   this thing called Meniere's disease,
[02:45:07.260 --> 02:45:10.580]   which is this very unpleasant disease
[02:45:10.580 --> 02:45:12.140]   where people basically end up losing their hearing,
[02:45:12.140 --> 02:45:13.060]   but they get this,
[02:45:13.060 --> 02:45:16.300]   it often comes with dizzy spells and other things,
[02:45:16.300 --> 02:45:18.740]   'cause it's like the inner ear gets all messed up.
[02:45:18.740 --> 02:45:21.820]   Now, I don't know if that's actually what I had,
[02:45:21.820 --> 02:45:24.980]   but that's what at least one doctor said to me.
[02:45:24.980 --> 02:45:27.020]   But anyway, so I'd had three months of this stuff,
[02:45:27.020 --> 02:45:28.860]   this going on, and it was really getting me down.
[02:45:28.860 --> 02:45:32.580]   And I was at Burning Man, of all places.
[02:45:32.580 --> 02:45:35.140]   Don't mean to be that person talking about Burning Man.
[02:45:35.140 --> 02:45:37.980]   But I was there, and again, I'd had it,
[02:45:37.980 --> 02:45:39.180]   and I was unable to listen to music,
[02:45:39.180 --> 02:45:40.100]   which is not what you want,
[02:45:40.100 --> 02:45:42.620]   'cause Burning Man is a very loud, intense place.
[02:45:42.620 --> 02:45:44.060]   And I was just having a really rough time.
[02:45:44.060 --> 02:45:45.980]   And on the final night,
[02:45:45.980 --> 02:45:49.660]   I get talking to this girl who's a friend of a friend.
[02:45:49.660 --> 02:45:51.060]   And I mentioned, I was like,
[02:45:51.060 --> 02:45:52.580]   "Oh, I'm really down in the dumps about this."
[02:45:52.580 --> 02:45:53.420]   And she's like, "Oh, well,
[02:45:53.420 --> 02:45:54.980]   I've done a little bit of energy healing.
[02:45:54.980 --> 02:45:56.380]   Would you like me to have a look?"
[02:45:56.380 --> 02:45:57.940]   And I was like, "Sure."
[02:45:57.940 --> 02:45:59.460]   Now, this was, again,
[02:45:59.460 --> 02:46:03.420]   no time in my life for this.
[02:46:03.420 --> 02:46:04.980]   I didn't believe in any of this stuff.
[02:46:04.980 --> 02:46:06.100]   I was just like, "It's all bullshit.
[02:46:06.100 --> 02:46:07.500]   It's all wooey nonsense."
[02:46:07.500 --> 02:46:10.740]   But I was like, "Sure, have a go."
[02:46:10.740 --> 02:46:13.700]   And she starts with her hand,
[02:46:13.700 --> 02:46:15.340]   and she says, "Oh, there's something there."
[02:46:15.340 --> 02:46:16.180]   And then she leans in,
[02:46:16.180 --> 02:46:18.780]   and she starts sucking over my ear,
[02:46:18.780 --> 02:46:19.900]   not actually touching me,
[02:46:19.900 --> 02:46:22.660]   but close to it, with her mouth.
[02:46:22.660 --> 02:46:23.780]   And it was really unpleasant.
[02:46:23.780 --> 02:46:24.780]   I was like, "Whoa, can you stop?"
[02:46:24.780 --> 02:46:25.820]   She's like, "No, no, no, there's something there.
[02:46:25.820 --> 02:46:26.660]   I need to get it."
[02:46:26.660 --> 02:46:27.780]   And I was like, "No, no, no, I really don't like it.
[02:46:27.780 --> 02:46:29.140]   Please, this is really loud."
[02:46:29.140 --> 02:46:31.100]   She's like, "I need to, just bear with me."
[02:46:31.100 --> 02:46:31.940]   And she does it,
[02:46:31.940 --> 02:46:33.700]   and I don't know how long, for a few minutes.
[02:46:33.740 --> 02:46:36.540]   And then she eventually collapses on the ground,
[02:46:36.540 --> 02:46:38.500]   like freezing cold, crying.
[02:46:38.500 --> 02:46:41.420]   And I'm just like,
[02:46:41.420 --> 02:46:42.860]   "I don't know what the hell is going on."
[02:46:42.860 --> 02:46:44.300]   I'm thoroughly freaked out,
[02:46:44.300 --> 02:46:45.420]   as is everyone else watching.
[02:46:45.420 --> 02:46:46.380]   Just like, "What the hell?"
[02:46:46.380 --> 02:46:47.660]   And we warm her up, and she was like,
[02:46:47.660 --> 02:46:48.500]   (gasps)
[02:46:48.500 --> 02:46:49.460]   "What, ugh."
[02:46:49.460 --> 02:46:51.340]   She was really shaken up.
[02:46:51.340 --> 02:46:54.700]   And she's like, "I don't know what that..."
[02:46:54.700 --> 02:46:57.540]   She said it was something very unpleasant and dark.
[02:46:57.540 --> 02:46:58.700]   "Don't worry, it's gone.
[02:46:58.700 --> 02:46:59.900]   I think you'll be fine in a couple...
[02:46:59.900 --> 02:47:01.420]   You'll have the physical symptoms for a couple of weeks,
[02:47:01.420 --> 02:47:02.700]   and you'll be fine."
[02:47:02.700 --> 02:47:04.620]   But, you know, she was just like that.
[02:47:04.620 --> 02:47:07.900]   You know, so I was so rattled, A,
[02:47:07.900 --> 02:47:10.020]   because the potential that actually,
[02:47:10.020 --> 02:47:13.060]   I'd had something bad in me that made someone feel bad,
[02:47:13.060 --> 02:47:15.340]   and that she was scared.
[02:47:15.340 --> 02:47:16.260]   That was what, you know, I was like,
[02:47:16.260 --> 02:47:19.300]   "Wait, I thought, you do this, this is the thing.
[02:47:19.300 --> 02:47:20.340]   Now you're terrified?
[02:47:20.340 --> 02:47:22.540]   Like you pulled like some kind of exorcism or something?
[02:47:22.540 --> 02:47:24.620]   Like what the fuck is going on?"
[02:47:24.620 --> 02:47:29.620]   So it, like just, the most insane experience.
[02:47:29.620 --> 02:47:31.740]   And frankly, it took me like a few months
[02:47:31.740 --> 02:47:35.380]   to sort of emotionally recover from it.
[02:47:35.380 --> 02:47:39.540]   But my ear problem went away about a couple of weeks later,
[02:47:39.540 --> 02:47:42.940]   and touch wood, I've not had any issues since.
[02:47:42.940 --> 02:47:44.700]   So...
[02:47:44.700 --> 02:47:48.140]   - That gives you like hints
[02:47:48.140 --> 02:47:50.100]   that maybe there's something out there.
[02:47:50.100 --> 02:47:53.340]   - I mean, I don't, again,
[02:47:53.340 --> 02:47:55.180]   I don't have an explanation for this.
[02:47:55.180 --> 02:47:57.780]   The most probable explanation was, you know,
[02:47:57.780 --> 02:47:58.620]   I was a burning man.
[02:47:58.620 --> 02:47:59.860]   I was in a very open state.
[02:47:59.860 --> 02:48:01.180]   Let's just leave it at that.
[02:48:01.820 --> 02:48:04.580]   And, you know,
[02:48:04.580 --> 02:48:08.060]   placebo is an incredibly powerful thing
[02:48:08.060 --> 02:48:10.300]   and a very not understood thing.
[02:48:10.300 --> 02:48:11.140]   So-
[02:48:11.140 --> 02:48:12.660]   - Almost assigning the word placebo to it
[02:48:12.660 --> 02:48:14.860]   reduces it down to a way that
[02:48:14.860 --> 02:48:16.500]   it doesn't deserve to be reduced down.
[02:48:16.500 --> 02:48:19.180]   Maybe there's a whole science of what we call placebo.
[02:48:19.180 --> 02:48:21.580]   Maybe there's a, placebo is a door-
[02:48:21.580 --> 02:48:23.940]   - Self-healing, you know?
[02:48:23.940 --> 02:48:26.300]   And I mean, I don't know what the problem was.
[02:48:26.300 --> 02:48:27.620]   Like I was told it was many years.
[02:48:27.620 --> 02:48:29.380]   I don't want to say I definitely had that
[02:48:29.380 --> 02:48:30.540]   because I don't want people to think that,
[02:48:30.540 --> 02:48:32.060]   "Oh, that's how, you know, if they do have that
[02:48:32.060 --> 02:48:33.020]   'cause it's a terrible disease.
[02:48:33.020 --> 02:48:33.860]   And if they have that,
[02:48:33.860 --> 02:48:34.740]   that this is gonna be a guaranteed way
[02:48:34.740 --> 02:48:35.580]   for it to fix it for them."
[02:48:35.580 --> 02:48:36.420]   I don't know.
[02:48:36.420 --> 02:48:39.700]   And I also don't, I don't,
[02:48:39.700 --> 02:48:41.300]   and you're absolutely right to say like
[02:48:41.300 --> 02:48:43.260]   using even the word placebo is like,
[02:48:43.260 --> 02:48:47.900]   it comes with this like baggage of like frame.
[02:48:47.900 --> 02:48:49.820]   And I don't want to reduce that.
[02:48:49.820 --> 02:48:52.780]   All I can do is describe the experience and what happened.
[02:48:52.780 --> 02:48:56.460]   I cannot put an ontological framework around it.
[02:48:56.460 --> 02:49:00.020]   I can't say why it happened, what the mechanism was,
[02:49:00.020 --> 02:49:02.900]   what the problem even was in the first place.
[02:49:02.900 --> 02:49:05.140]   I just know that something crazy happened
[02:49:05.140 --> 02:49:06.980]   and it was while I was in an open state.
[02:49:06.980 --> 02:49:09.460]   And fortunately for me, it made the problem go away.
[02:49:09.460 --> 02:49:11.780]   But what I took away from it, again,
[02:49:11.780 --> 02:49:13.500]   it was part of this, you know,
[02:49:13.500 --> 02:49:15.900]   this took me on this journey of becoming more humble
[02:49:15.900 --> 02:49:17.380]   about what I think I know.
[02:49:17.380 --> 02:49:18.780]   Because as I said before, I was like,
[02:49:18.780 --> 02:49:21.740]   I was in the like Richard Dawkins train of atheism
[02:49:21.740 --> 02:49:23.060]   in terms of there is no God.
[02:49:23.060 --> 02:49:24.820]   There's everything like that is bullshit.
[02:49:24.820 --> 02:49:25.660]   We know everything.
[02:49:25.660 --> 02:49:28.980]   We know, you know, the only way we can get through,
[02:49:28.980 --> 02:49:30.820]   we know how medicine works and its molecules
[02:49:30.820 --> 02:49:33.980]   and chemical interactions and that kind of stuff.
[02:49:33.980 --> 02:49:36.580]   And now it's like, okay, well,
[02:49:36.580 --> 02:49:39.500]   there's clearly more for us to understand.
[02:49:39.500 --> 02:49:43.540]   And that doesn't mean that it's ascientific as well.
[02:49:43.540 --> 02:49:47.140]   'Cause, you know, the beauty of the scientific method
[02:49:47.140 --> 02:49:49.940]   is that it still can apply to this situation.
[02:49:49.940 --> 02:49:51.340]   Like I don't see why, you know,
[02:49:51.340 --> 02:49:54.300]   I would like to try and test this experimentally.
[02:49:54.300 --> 02:49:55.540]   I haven't really, you know,
[02:49:55.540 --> 02:49:57.100]   I don't know how we would go about doing that.
[02:49:57.100 --> 02:49:58.860]   We'd have to find other people with the same condition.
[02:49:58.860 --> 02:50:02.780]   I guess, and like try and repeat the experiment.
[02:50:02.780 --> 02:50:06.940]   But it doesn't, just because something happens
[02:50:06.940 --> 02:50:10.060]   that's sort of out of the realms of our current understanding
[02:50:10.060 --> 02:50:11.900]   it doesn't mean that it's,
[02:50:11.900 --> 02:50:13.820]   the scientific method can't be used for it.
[02:50:13.820 --> 02:50:17.900]   - Yeah, I think the scientific method sits on a foundation
[02:50:17.900 --> 02:50:20.180]   of those kinds of experiences.
[02:50:20.180 --> 02:50:23.340]   'Cause they, scientific method is a process
[02:50:24.700 --> 02:50:29.700]   to carve away at the mystery all around us.
[02:50:29.700 --> 02:50:33.740]   And experiences like this is just a reminder
[02:50:33.740 --> 02:50:36.700]   that we're mostly shrouded in mystery still.
[02:50:36.700 --> 02:50:38.940]   That's it, it's just like a humility.
[02:50:38.940 --> 02:50:41.580]   Like we haven't really figured this whole thing out.
[02:50:41.580 --> 02:50:45.460]   - But at the same time, we have found ways to act,
[02:50:45.460 --> 02:50:47.500]   you know, we're clearly doing something right.
[02:50:47.500 --> 02:50:50.020]   Because think of the technological scientific advancements,
[02:50:50.020 --> 02:50:51.500]   the knowledge that we have,
[02:50:52.460 --> 02:50:55.620]   would blow people's minds even from 100 years ago.
[02:50:55.620 --> 02:50:58.660]   - Yeah, and we've even allegedly gone out to space
[02:50:58.660 --> 02:51:00.340]   and landed on the moon.
[02:51:00.340 --> 02:51:02.500]   Although I still haven't, I have not seen evidence
[02:51:02.500 --> 02:51:06.540]   of the earth being round, but I'm keeping an open mind.
[02:51:06.540 --> 02:51:11.620]   Speaking of which, you studied physics and astrophysics.
[02:51:11.620 --> 02:51:13.860]   (laughing)
[02:51:13.860 --> 02:51:18.740]   Just to go to that, just to jump around
[02:51:18.740 --> 02:51:20.940]   through the fascinating life you've had,
[02:51:20.940 --> 02:51:23.620]   when did you, how did that come to be?
[02:51:23.620 --> 02:51:25.900]   Like when did you fall in love with astronomy
[02:51:25.900 --> 02:51:28.620]   and space and things like this?
[02:51:28.620 --> 02:51:30.460]   - As early as I can remember.
[02:51:30.460 --> 02:51:33.380]   I was very lucky that my mom and my dad,
[02:51:33.380 --> 02:51:38.300]   but particularly my mom, my mom is like the most nature,
[02:51:38.300 --> 02:51:41.180]   she is mother earth, is the only way to describe her.
[02:51:41.180 --> 02:51:44.540]   Just, she's like Dr. Dolittle, animals flock to her
[02:51:44.540 --> 02:51:47.060]   and just like sit and look at her adoringly.
[02:51:47.060 --> 02:51:48.380]   - As she sings.
[02:51:48.380 --> 02:51:51.060]   - Yeah, she just is mother earth
[02:51:51.060 --> 02:51:53.500]   and she has always been fascinated by,
[02:51:53.500 --> 02:51:57.100]   she doesn't have any, she never went to university
[02:51:57.100 --> 02:51:59.380]   or anything like that, she's actually phobic of maths.
[02:51:59.380 --> 02:52:01.060]   If I try and get her to like,
[02:52:01.060 --> 02:52:03.860]   I was trying to teach her poker and she hated it.
[02:52:03.860 --> 02:52:08.860]   But she's so deeply curious and that just got instilled in me
[02:52:08.860 --> 02:52:11.260]   when we would sleep out under the stars
[02:52:11.260 --> 02:52:13.220]   whenever it was the two nights a year
[02:52:13.220 --> 02:52:15.820]   when it was warm enough in the UK to do that.
[02:52:15.820 --> 02:52:18.740]   And we would just lie out there until we fell asleep
[02:52:18.740 --> 02:52:22.140]   looking for satellites, looking for shooting stars.
[02:52:22.140 --> 02:52:25.340]   And I was just always, I don't know whether it was from that
[02:52:25.340 --> 02:52:28.660]   but I've always naturally gravitated to like
[02:52:28.660 --> 02:52:31.980]   the biggest questions.
[02:52:31.980 --> 02:52:35.180]   And also the like the most layers of abstraction.
[02:52:35.180 --> 02:52:36.780]   I love just like, what's the meta question,
[02:52:36.780 --> 02:52:38.820]   what's the meta question and so on.
[02:52:38.820 --> 02:52:40.820]   So I think it just came from that really.
[02:52:40.820 --> 02:52:43.100]   And then on top of that, like physics,
[02:52:44.140 --> 02:52:47.460]   it also made logical sense in that it was a degree
[02:52:47.460 --> 02:52:54.100]   that was subject that ticked the box of being,
[02:52:54.100 --> 02:52:55.580]   answering these really big picture questions
[02:52:55.580 --> 02:52:57.900]   but it was also extremely useful.
[02:52:57.900 --> 02:52:59.660]   It like has a very high utility
[02:52:59.660 --> 02:53:02.140]   in terms of, I didn't know necessarily.
[02:53:02.140 --> 02:53:04.460]   I thought I was gonna become like a research scientist.
[02:53:04.460 --> 02:53:07.060]   My original plan was I wanna be a professional astronomer.
[02:53:07.060 --> 02:53:08.660]   - So it's not just like a philosophy degree
[02:53:08.660 --> 02:53:13.660]   that asks the big questions and it's not like biology
[02:53:14.100 --> 02:53:16.220]   and the path to go to medical school or something like that
[02:53:16.220 --> 02:53:19.140]   which is overly pragmatic, not overly,
[02:53:19.140 --> 02:53:23.140]   is very pragmatic. - The more pragmatic side.
[02:53:23.140 --> 02:53:25.580]   - But this is, yeah, physics is a good combination
[02:53:25.580 --> 02:53:26.500]   of the two.
[02:53:26.500 --> 02:53:27.940]   - Yeah, at least for me, it made sense.
[02:53:27.940 --> 02:53:30.220]   And I was good at it, I liked it.
[02:53:30.220 --> 02:53:32.460]   Yeah, I mean, it wasn't like I did an immense amount
[02:53:32.460 --> 02:53:34.380]   of soul searching to choose it or anything.
[02:53:34.380 --> 02:53:38.220]   It just was like this, it made the most sense.
[02:53:38.220 --> 02:53:41.300]   I mean, you have to make this decision in the UK, age 17
[02:53:41.300 --> 02:53:44.820]   which is crazy 'cause in US, you go the first year,
[02:53:44.820 --> 02:53:46.260]   you do a bunch of stuff, right?
[02:53:46.260 --> 02:53:48.540]   And then you choose your major.
[02:53:48.540 --> 02:53:50.020]   - Yeah, I think the first few years of college,
[02:53:50.020 --> 02:53:53.100]   you focus on the drugs and only as you get closer
[02:53:53.100 --> 02:53:55.820]   to the end, do you start to think,
[02:53:55.820 --> 02:53:57.420]   oh shit, this wasn't about that
[02:53:57.420 --> 02:54:01.380]   and I owe the government a lot of money.
[02:54:01.380 --> 02:54:05.180]   How many alien civilizations are out there?
[02:54:05.180 --> 02:54:07.580]   When you looked up at the stars with your mom
[02:54:07.580 --> 02:54:09.960]   and you were counting them,
[02:54:11.140 --> 02:54:12.900]   what's your mom think about the number
[02:54:12.900 --> 02:54:13.740]   of alien civilization?
[02:54:13.740 --> 02:54:15.780]   - I actually don't know.
[02:54:15.780 --> 02:54:18.660]   I would imagine she would take the viewpoint of,
[02:54:18.660 --> 02:54:21.300]   she's pretty humble and she knows how many,
[02:54:21.300 --> 02:54:24.100]   she knows there's a huge number of potential spawn sites
[02:54:24.100 --> 02:54:25.380]   out there, so she would--
[02:54:25.380 --> 02:54:26.220]   - Spawn sites?
[02:54:26.220 --> 02:54:28.020]   - Spawn sites, yeah.
[02:54:28.020 --> 02:54:29.460]   This is our spawn sites. - Spawn sites.
[02:54:29.460 --> 02:54:32.160]   - Yeah, spawn sites in polytopia, we spawned on earth.
[02:54:32.160 --> 02:54:35.860]   - Yeah, spawn sites.
[02:54:35.860 --> 02:54:39.260]   Why does that feel weird to say spawn?
[02:54:39.260 --> 02:54:41.320]   - Because it makes me feel like it's,
[02:54:41.320 --> 02:54:45.300]   there's only one source of life
[02:54:45.300 --> 02:54:47.380]   and it's spawning in different locations.
[02:54:47.380 --> 02:54:49.180]   That's why the word spawn.
[02:54:49.180 --> 02:54:52.540]   'Cause it feels like life that originated on earth
[02:54:52.540 --> 02:54:54.660]   really originated here.
[02:54:54.660 --> 02:54:58.740]   - Right, it is unique to this particular,
[02:54:58.740 --> 02:55:00.980]   yeah, I mean, but I don't, in my mind,
[02:55:00.980 --> 02:55:04.340]   it doesn't exclude the completely different forms of life
[02:55:04.340 --> 02:55:09.140]   and different biochemical soups can't also spawn,
[02:55:09.140 --> 02:55:12.580]   but I guess it implies that there's some spark
[02:55:12.580 --> 02:55:16.180]   that is uniform, which I kind of like the idea of it.
[02:55:16.180 --> 02:55:19.220]   - And then I get to think about respawning,
[02:55:19.220 --> 02:55:23.500]   like after it dies, like what happens if life on earth ends?
[02:55:23.500 --> 02:55:26.060]   Is it gonna restart again?
[02:55:26.060 --> 02:55:28.060]   - Probably not, it depends.
[02:55:28.060 --> 02:55:28.900]   - Maybe earth is too--
[02:55:28.900 --> 02:55:30.700]   - It depends on the type of,
[02:55:30.700 --> 02:55:32.820]   what's the thing that kills it off, right?
[02:55:32.820 --> 02:55:36.100]   If it's a paperclip maximizer, not for the example,
[02:55:36.100 --> 02:55:39.620]   but some kind of very self-replicating,
[02:55:39.620 --> 02:55:44.380]   high on the capabilities, very low on the wisdom type thing.
[02:55:44.380 --> 02:55:48.620]   So whether that's gray goo, green goo, like nanobots
[02:55:48.620 --> 02:55:51.180]   or just a shitty misaligned AI
[02:55:51.180 --> 02:55:54.340]   that thinks it needs to turn everything into paperclips.
[02:55:54.340 --> 02:55:57.700]   If it's something like that,
[02:55:57.700 --> 02:56:00.860]   then it's gonna be very hard for life, complex life,
[02:56:00.860 --> 02:56:03.180]   because by definition, a paperclip maximizer
[02:56:03.180 --> 02:56:05.660]   is the ultimate instantiation of molecule,
[02:56:05.660 --> 02:56:08.620]   deeply low complexity, over-optimization on a single thing,
[02:56:08.620 --> 02:56:11.100]   sacrificing everything else, turning the whole world into--
[02:56:11.100 --> 02:56:12.180]   - Although something tells me,
[02:56:12.180 --> 02:56:14.620]   like if we actually take a paperclip maximizer,
[02:56:14.620 --> 02:56:16.180]   it destroys everything.
[02:56:16.180 --> 02:56:17.820]   It's a really dumb system
[02:56:17.820 --> 02:56:20.780]   that just envelops the whole of earth.
[02:56:20.780 --> 02:56:22.380]   - And the universe beyond, yeah.
[02:56:22.380 --> 02:56:26.580]   - I didn't know that part, but okay, great.
[02:56:26.580 --> 02:56:27.420]   - That's the thought experiment anyway.
[02:56:27.420 --> 02:56:30.300]   - So it becomes a multi-planetary paperclip maximizer?
[02:56:30.300 --> 02:56:32.100]   - Well, it just propagates, I mean,
[02:56:32.100 --> 02:56:33.660]   it depends whether it figures out
[02:56:33.660 --> 02:56:35.060]   how to jump the vacuum gap.
[02:56:35.060 --> 02:56:38.660]   But again, I mean, this is all silly
[02:56:38.660 --> 02:56:40.340]   'cause it's a hypothetical thought experiment,
[02:56:40.340 --> 02:56:41.220]   which I think doesn't actually have
[02:56:41.220 --> 02:56:43.260]   much practical application to the AI safety problem,
[02:56:43.260 --> 02:56:45.260]   but it's just a fun thing to play around with.
[02:56:45.260 --> 02:56:47.820]   But if by definition it is maximally intelligent,
[02:56:47.820 --> 02:56:49.820]   which means it is maximally good
[02:56:49.820 --> 02:56:53.100]   at navigating the environment around it
[02:56:53.100 --> 02:56:54.740]   in order to achieve its goal,
[02:56:54.740 --> 02:56:58.060]   but extremely bad at choosing goals in the first place,
[02:56:58.060 --> 02:57:00.060]   so again, we're talking on this orthogonality thing, right?
[02:57:00.060 --> 02:57:03.000]   It's very low on wisdom, but very high on capability.
[02:57:03.720 --> 02:57:05.680]   Then it will figure out how to jump the vacuum gap
[02:57:05.680 --> 02:57:07.400]   between planets and stars and so on,
[02:57:07.400 --> 02:57:09.360]   and thus just turn every atom it gets
[02:57:09.360 --> 02:57:10.960]   its hands on into paperclips.
[02:57:10.960 --> 02:57:12.800]   - Yeah, by the way, for people who don't--
[02:57:12.800 --> 02:57:14.600]   - Which is maximum virality, by the way.
[02:57:14.600 --> 02:57:16.240]   That's what virality is.
[02:57:16.240 --> 02:57:17.800]   - But does not mean that virality
[02:57:17.800 --> 02:57:20.040]   is necessarily all about maximizing paperclips.
[02:57:20.040 --> 02:57:21.080]   In that case, it is.
[02:57:21.080 --> 02:57:22.280]   So for people who don't know,
[02:57:22.280 --> 02:57:24.280]   this is just a thought experiment example
[02:57:24.280 --> 02:57:27.360]   of an AI system that has a goal
[02:57:27.360 --> 02:57:30.400]   and is willing to do anything to accomplish that goal,
[02:57:30.400 --> 02:57:32.280]   including destroying all life on Earth
[02:57:32.280 --> 02:57:36.320]   and all human life and all of consciousness in the universe
[02:57:36.320 --> 02:57:40.720]   for the goal of producing a maximum number of paperclips.
[02:57:40.720 --> 02:57:41.840]   Okay.
[02:57:41.840 --> 02:57:43.960]   - Or whatever its optimization function was
[02:57:43.960 --> 02:57:45.000]   that it was set at.
[02:57:45.000 --> 02:57:45.840]   - But don't you think--
[02:57:45.840 --> 02:57:47.920]   - It could be recreating Lexus.
[02:57:47.920 --> 02:57:50.560]   Maybe it'll tile the universe in Lex.
[02:57:50.560 --> 02:57:51.600]   - Go on.
[02:57:51.600 --> 02:57:52.440]   I like this idea.
[02:57:52.440 --> 02:57:53.360]   No, I'm just kidding.
[02:57:53.360 --> 02:57:54.800]   - That's better.
[02:57:54.800 --> 02:57:56.320]   That's more interesting than paperclips.
[02:57:56.320 --> 02:57:57.640]   - That could be infinitely optimal
[02:57:57.640 --> 02:57:58.480]   if I were to say so myself.
[02:57:58.480 --> 02:58:00.400]   - But if you ask me, it's still a bad thing
[02:58:00.400 --> 02:58:02.760]   because it's permanently capping
[02:58:02.760 --> 02:58:04.200]   what the universe could ever be.
[02:58:04.200 --> 02:58:05.880]   It's like, that's its end state.
[02:58:05.880 --> 02:58:07.840]   - Or achieving the optimal
[02:58:07.840 --> 02:58:09.240]   that the universe could ever achieve,
[02:58:09.240 --> 02:58:10.320]   but that's up to,
[02:58:10.320 --> 02:58:12.480]   different people have different perspectives.
[02:58:12.480 --> 02:58:15.520]   But don't you think within the paperclip world
[02:58:15.520 --> 02:58:16.920]   that would emerge,
[02:58:16.920 --> 02:58:20.200]   just like in the zeros and ones that make up a computer,
[02:58:20.200 --> 02:58:22.860]   that would emerge beautiful complexities?
[02:58:22.860 --> 02:58:27.800]   Like, it won't suppress,
[02:58:27.800 --> 02:58:30.800]   as you scale to multiple planets and throughout,
[02:58:30.800 --> 02:58:33.240]   there'll emerge these little worlds
[02:58:33.240 --> 02:58:38.240]   that on top of the fabric of maximizing paperclips,
[02:58:38.240 --> 02:58:45.200]   that would emerge like little societies of paperclip--
[02:58:45.200 --> 02:58:47.480]   - Well, then we're not describing
[02:58:47.480 --> 02:58:48.560]   a paperclip maximizer anymore
[02:58:48.560 --> 02:58:51.560]   because if you think of what a paperclip is,
[02:58:51.560 --> 02:58:55.560]   it is literally just a piece of bent iron, right?
[02:58:55.560 --> 02:58:58.920]   So if it's maximizing that throughout the universe,
[02:58:58.920 --> 02:59:01.000]   it's taking every atom it gets its hand on
[02:59:01.000 --> 02:59:04.200]   into somehow turning it into iron or steel,
[02:59:04.200 --> 02:59:05.360]   and then bending it into that shape,
[02:59:05.360 --> 02:59:06.760]   and then done and done.
[02:59:06.760 --> 02:59:08.840]   By definition, like paperclips,
[02:59:08.840 --> 02:59:11.920]   there is no way for, well, okay,
[02:59:11.920 --> 02:59:13.880]   so you're saying that paperclips somehow
[02:59:13.880 --> 02:59:17.920]   will just emerge and create through gravity or something.
[02:59:17.920 --> 02:59:18.920]   - No, no, no, no,
[02:59:18.920 --> 02:59:21.560]   'cause there's a dynamic element to the whole system.
[02:59:21.560 --> 02:59:24.760]   It's not just, it's creating those paperclips.
[02:59:24.760 --> 02:59:27.880]   And the act of creating, there's going to be a process,
[02:59:27.880 --> 02:59:30.640]   and that process will have a dance to it
[02:59:30.640 --> 02:59:32.600]   'cause it's not like sequential thing.
[02:59:32.600 --> 02:59:35.200]   There's a whole complex three-dimensional system
[02:59:35.200 --> 02:59:36.200]   of paperclips.
[02:59:36.200 --> 02:59:39.400]   People like string theory, right?
[02:59:39.400 --> 02:59:40.280]   It's supposed to be strings
[02:59:40.280 --> 02:59:41.960]   that are interacting in fascinating ways.
[02:59:41.960 --> 02:59:44.520]   I'm sure paperclips are very string-like.
[02:59:44.520 --> 02:59:46.520]   They can be interacting in very interesting ways
[02:59:46.520 --> 02:59:50.040]   as you scale exponentially through three-dimensional.
[02:59:50.040 --> 02:59:53.840]   I mean, I'm sure the paperclip maximizer
[02:59:53.840 --> 02:59:55.640]   has to come up with a theory of everything.
[02:59:55.640 --> 02:59:58.280]   It has to create wormholes, right?
[02:59:58.280 --> 03:00:01.000]   It has to break, like,
[03:00:01.000 --> 03:00:02.640]   it has to understand quantum mechanics.
[03:00:02.640 --> 03:00:04.120]   - I love your optimism. - It has to understand
[03:00:04.120 --> 03:00:06.200]   general relativity. - This is where I'd say
[03:00:06.200 --> 03:00:08.400]   we're going into the realm of pathological optimism,
[03:00:08.400 --> 03:00:10.520]   wherever it's. (laughs)
[03:00:10.520 --> 03:00:11.720]   - I'm sure there'll be a,
[03:00:11.720 --> 03:00:14.840]   I think there's an intelligence
[03:00:14.840 --> 03:00:16.280]   that emerges from that system.
[03:00:16.280 --> 03:00:18.440]   - So you're saying that basically intelligence
[03:00:18.440 --> 03:00:21.640]   is inherent in the fabric of reality and will find a way,
[03:00:21.640 --> 03:00:23.840]   kind of like Goldblum says, "Life will find a way."
[03:00:23.840 --> 03:00:25.160]   You think life will find a way
[03:00:25.160 --> 03:00:29.480]   even out of this perfectly homogenous dead soup.
[03:00:29.480 --> 03:00:31.520]   - It's not perfectly homogenous.
[03:00:31.520 --> 03:00:34.920]   It has to, it's perfectly maximal in the production.
[03:00:34.920 --> 03:00:37.360]   I don't know why people keep thinking it's homogenous.
[03:00:37.360 --> 03:00:39.520]   It maximizes the number of paperclips.
[03:00:39.520 --> 03:00:40.360]   That's the only thing.
[03:00:40.360 --> 03:00:42.000]   It's not trying to be homogenous.
[03:00:42.000 --> 03:00:42.840]   It's trying to- - True, true, true, true.
[03:00:42.840 --> 03:00:44.720]   - It's trying to maximize paperclips.
[03:00:44.720 --> 03:00:48.840]   - So you're saying that because,
[03:00:48.840 --> 03:00:50.080]   kind of like in "The Big Bang,"
[03:00:50.640 --> 03:00:53.120]   it seems like things, there were clusters,
[03:00:53.120 --> 03:00:54.880]   there was more stuff here than there.
[03:00:54.880 --> 03:00:56.640]   That was enough of the patonicity
[03:00:56.640 --> 03:00:58.480]   that kickstarted the evolutionary process.
[03:00:58.480 --> 03:00:59.520]   - It's the little weirdnesses
[03:00:59.520 --> 03:01:01.080]   that will make it beautiful. - You think that even out of-
[03:01:01.080 --> 03:01:03.000]   - Yeah, complexity emerges.
[03:01:03.000 --> 03:01:04.240]   - Interesting, okay.
[03:01:04.240 --> 03:01:05.640]   Well, so how does that line up then
[03:01:05.640 --> 03:01:08.120]   with the whole heat death of the universe, right?
[03:01:08.120 --> 03:01:10.120]   'Cause that's another sort of instantiation of this.
[03:01:10.120 --> 03:01:13.280]   It's like everything becomes so far apart and so cold
[03:01:13.280 --> 03:01:16.720]   and so perfectly mixed
[03:01:16.720 --> 03:01:20.440]   that it's like homogenous grayness.
[03:01:20.440 --> 03:01:23.320]   Do you think that even out of that homogenous grayness
[03:01:23.320 --> 03:01:27.280]   where there's no negative entropy,
[03:01:27.280 --> 03:01:30.960]   there's no free energy that we understand
[03:01:30.960 --> 03:01:34.000]   even from that new stuff?
[03:01:34.000 --> 03:01:36.640]   - Yeah, the paperclip maximizer
[03:01:36.640 --> 03:01:38.200]   or any other intelligent systems
[03:01:38.200 --> 03:01:41.040]   will figure out ways to travel to other universes
[03:01:41.040 --> 03:01:43.560]   to create big bangs within those universes
[03:01:43.560 --> 03:01:46.160]   or through black holes to create whole other worlds
[03:01:46.160 --> 03:01:51.160]   to break what we consider are the limitations of physics.
[03:01:51.160 --> 03:01:52.760]   (laughing)
[03:01:52.760 --> 03:01:56.720]   The paperclip maximizer will find a way if a way exists.
[03:01:56.720 --> 03:01:59.040]   And we should be humble to realize that we don't-
[03:01:59.040 --> 03:02:01.520]   - Yeah, but because it just wants to make more paperclips.
[03:02:01.520 --> 03:02:02.680]   So it's gonna go into those universes
[03:02:02.680 --> 03:02:03.800]   and turn them into paperclips.
[03:02:03.800 --> 03:02:07.480]   - Yeah, but we, humans, not humans,
[03:02:07.480 --> 03:02:10.320]   but complex systems exist on top of that.
[03:02:10.320 --> 03:02:11.840]   We're not interfering with it.
[03:02:13.000 --> 03:02:16.280]   This complexity emerges from-
[03:02:16.280 --> 03:02:17.400]   - The simple base state.
[03:02:17.400 --> 03:02:18.320]   - The simple base state.
[03:02:18.320 --> 03:02:20.320]   - Whether it's, yeah, whether it's, you know,
[03:02:20.320 --> 03:02:22.640]   Planck lengths or paperclips as the base unit.
[03:02:22.640 --> 03:02:25.720]   - Yeah, you can think of like the universe
[03:02:25.720 --> 03:02:27.320]   as a paperclip maximizer
[03:02:27.320 --> 03:02:28.880]   because it's doing some dumb stuff.
[03:02:28.880 --> 03:02:31.320]   Like physics seems to be pretty dumb.
[03:02:31.320 --> 03:02:34.160]   It has, like, I don't know if you can summarize it.
[03:02:34.160 --> 03:02:37.320]   - Yeah, the laws are fairly basic
[03:02:37.320 --> 03:02:39.640]   and yet out of them amazing complexity emerges.
[03:02:39.640 --> 03:02:43.440]   - And its goals seem to be pretty basic and dumb.
[03:02:43.440 --> 03:02:45.280]   If you can summarize its goals,
[03:02:45.280 --> 03:02:47.000]   I mean, I don't know what's a nice way,
[03:02:47.000 --> 03:02:52.000]   maybe laws of thermodynamics could be,
[03:02:52.000 --> 03:02:55.080]   I don't know if you can assign goals to physics,
[03:02:55.080 --> 03:02:57.980]   but if you formulate in the sense of goals,
[03:02:57.980 --> 03:03:00.840]   it's very similar to paperclip maximizing
[03:03:00.840 --> 03:03:02.960]   in the dumbness of the goals.
[03:03:02.960 --> 03:03:06.520]   But the pockets of complexity as it emerge
[03:03:06.520 --> 03:03:07.960]   is where beauty emerges.
[03:03:07.960 --> 03:03:09.120]   That's where life emerges.
[03:03:09.120 --> 03:03:12.120]   That's where intelligence, that's where humans emerge.
[03:03:12.120 --> 03:03:14.000]   And I think we're being very down
[03:03:14.000 --> 03:03:16.240]   on this whole paperclip maximizer thing.
[03:03:16.240 --> 03:03:17.680]   Now, the reason we hate it-
[03:03:17.680 --> 03:03:19.000]   - I think, yeah, because what you're saying
[03:03:19.000 --> 03:03:24.000]   is that you think that the force of emergence itself
[03:03:24.000 --> 03:03:27.480]   is another like unwritten, not unwritten,
[03:03:27.480 --> 03:03:31.800]   but like another baked in law of reality.
[03:03:31.800 --> 03:03:34.840]   And you're trusting that emergence will find a way
[03:03:34.880 --> 03:03:39.880]   to even out of seemingly the most molecule awful,
[03:03:39.880 --> 03:03:42.240]   plain outcome emergence will still find a way.
[03:03:42.240 --> 03:03:43.480]   I love that as a philosophy.
[03:03:43.480 --> 03:03:44.360]   I think it's very nice.
[03:03:44.360 --> 03:03:47.320]   I would wield it carefully
[03:03:47.320 --> 03:03:50.520]   because there's large error bars on that
[03:03:50.520 --> 03:03:52.080]   and the certainty of that.
[03:03:52.080 --> 03:03:52.920]   - Yeah.
[03:03:52.920 --> 03:03:55.960]   How about we build the paperclip maximizer and find out?
[03:03:55.960 --> 03:03:56.880]   - Classic, yeah.
[03:03:56.880 --> 03:03:59.160]   Molec is doing cartwheels, man.
[03:03:59.160 --> 03:04:00.000]   - Yeah.
[03:04:00.000 --> 03:04:02.440]   But the thing is, it will destroy humans in the process,
[03:04:03.240 --> 03:04:05.320]   which is the reason we really don't like it.
[03:04:05.320 --> 03:04:07.240]   We seem to be really holding on
[03:04:07.240 --> 03:04:10.240]   to this whole human civilization thing.
[03:04:10.240 --> 03:04:13.840]   Would that make you sad if AI systems that are beautiful,
[03:04:13.840 --> 03:04:15.680]   that are conscious, that are interesting
[03:04:15.680 --> 03:04:17.300]   and complex and intelligent,
[03:04:17.300 --> 03:04:20.040]   ultimately lead to the death of humans?
[03:04:20.040 --> 03:04:21.040]   Would that make you sad?
[03:04:21.040 --> 03:04:23.000]   - If humans led to the death of humans?
[03:04:23.000 --> 03:04:23.840]   Sorry.
[03:04:23.840 --> 03:04:25.520]   - Like if they would supersede humans.
[03:04:25.520 --> 03:04:26.840]   - Oh, if some AI?
[03:04:26.840 --> 03:04:30.320]   - Yeah, AI would end humans.
[03:04:30.320 --> 03:04:33.320]   I mean, that's the reason why I'm in some ways
[03:04:33.320 --> 03:04:36.440]   less emotionally concerned about AI risk
[03:04:36.440 --> 03:04:39.480]   than say, bio risk.
[03:04:39.480 --> 03:04:41.560]   Because at least with AI,
[03:04:41.560 --> 03:04:44.100]   there's a chance, if we're in this hypothetical
[03:04:44.100 --> 03:04:45.640]   where it wipes out humans,
[03:04:45.640 --> 03:04:48.240]   but it does it for some higher purpose,
[03:04:48.240 --> 03:04:51.060]   it needs our atoms and energy to do something,
[03:04:51.060 --> 03:04:53.120]   at least now the universe is going on
[03:04:53.120 --> 03:04:55.240]   to do something interesting.
[03:04:55.240 --> 03:04:56.800]   Whereas if it wipes everything,
[03:04:56.800 --> 03:05:00.120]   bio just kills everything on Earth and that's it.
[03:05:00.120 --> 03:05:01.440]   And there's no more,
[03:05:01.440 --> 03:05:03.340]   Earth cannot spawn anything more meaningful
[03:05:03.340 --> 03:05:05.960]   in the few hundred million years it has left.
[03:05:05.960 --> 03:05:07.680]   'Cause it doesn't have much time left.
[03:05:07.680 --> 03:05:13.480]   Then, yeah, I don't know.
[03:05:13.480 --> 03:05:15.680]   So one of my favorite books I've ever read
[03:05:15.680 --> 03:05:18.200]   is "Novacene" by James Lovelock,
[03:05:18.200 --> 03:05:19.920]   who sadly just died.
[03:05:19.920 --> 03:05:22.080]   He wrote it when he was like 99.
[03:05:22.080 --> 03:05:24.280]   He died aged 102, so it's a fairly new book.
[03:05:24.280 --> 03:05:27.040]   And he sort of talks about that,
[03:05:27.040 --> 03:05:30.840]   that he thinks it's sort of building off this Gaia theory
[03:05:30.840 --> 03:05:35.840]   where Earth is living some form of intelligence itself
[03:05:35.840 --> 03:05:38.400]   and that this is the next step, right?
[03:05:38.400 --> 03:05:41.740]   Is this, whatever this new intelligence
[03:05:41.740 --> 03:05:43.360]   that is maybe silicon-based
[03:05:43.360 --> 03:05:46.160]   as opposed to carbon-based goes on to do.
[03:05:46.160 --> 03:05:47.680]   And it's really sort of, in some ways,
[03:05:47.680 --> 03:05:49.820]   an optimistic but really fatalistic book.
[03:05:49.820 --> 03:05:52.400]   I don't know if I fully subscribe to it,
[03:05:52.400 --> 03:05:54.400]   but it's a beautiful piece to read anyway.
[03:05:54.400 --> 03:05:56.760]   So am I sad by that idea?
[03:05:56.760 --> 03:05:57.920]   I think so, yes.
[03:05:57.920 --> 03:05:59.240]   And actually, yeah, this is the reason
[03:05:59.240 --> 03:06:00.120]   why I'm sad by the idea,
[03:06:00.120 --> 03:06:03.880]   because if something is truly brilliant and wise and smart
[03:06:03.880 --> 03:06:05.580]   and truly super intelligent,
[03:06:05.580 --> 03:06:09.320]   it should be able to figure out abundance.
[03:06:09.320 --> 03:06:11.720]   So if it figures out abundance,
[03:06:11.720 --> 03:06:12.800]   it shouldn't need to kill us off.
[03:06:12.800 --> 03:06:15.060]   It should be able to find a way for us.
[03:06:15.060 --> 03:06:17.460]   There's plenty, the universe is huge.
[03:06:17.460 --> 03:06:19.920]   There should be plenty of space for it to go out
[03:06:19.920 --> 03:06:21.760]   and do all the things it wants to do
[03:06:21.760 --> 03:06:23.440]   and give us a little pocket
[03:06:23.440 --> 03:06:24.880]   where we can continue doing our things
[03:06:24.880 --> 03:06:26.640]   and we can continue to do things and so on.
[03:06:26.640 --> 03:06:28.920]   And again, if it's so supremely wise,
[03:06:28.920 --> 03:06:29.760]   it shouldn't even be worried
[03:06:29.760 --> 03:06:31.920]   about the game theoretic considerations
[03:06:31.920 --> 03:06:33.000]   that by leaving us alive,
[03:06:33.000 --> 03:06:35.120]   we'll then go and create another super intelligent agent
[03:06:35.120 --> 03:06:36.600]   that it then has to compete against,
[03:06:36.600 --> 03:06:38.280]   because it should be omni-wise and smart enough
[03:06:38.280 --> 03:06:40.680]   to not have to concern itself with that.
[03:06:40.680 --> 03:06:44.200]   - Unless it deems humans to be kind of assholes.
[03:06:44.200 --> 03:06:50.880]   The humans are a source of a lose-lose kind of dynamics.
[03:06:50.880 --> 03:06:54.040]   - Well, yes and no.
[03:06:55.440 --> 03:06:57.560]   Moloch is, that's why I think it's important to say.
[03:06:57.560 --> 03:07:00.200]   - But maybe humans are the source of Moloch.
[03:07:00.200 --> 03:07:03.840]   - No, I mean, I think game theory is the source of Moloch.
[03:07:03.840 --> 03:07:08.000]   And 'cause Moloch exists in non-human systems as well.
[03:07:08.000 --> 03:07:10.560]   It happens within agents within a game
[03:07:10.560 --> 03:07:13.920]   in terms of, it applies to agents,
[03:07:13.920 --> 03:07:18.520]   but it can apply to a species
[03:07:18.520 --> 03:07:22.680]   that's on an island of animals, rats out-competing
[03:07:22.680 --> 03:07:25.520]   the ones that massively consume all the resources
[03:07:25.520 --> 03:07:26.840]   are the ones that are gonna win out
[03:07:26.840 --> 03:07:29.680]   over the more chill, socialized ones.
[03:07:29.680 --> 03:07:31.960]   And so, creates this Malthusian trap.
[03:07:31.960 --> 03:07:34.480]   Moloch exists in little pockets in nature as well.
[03:07:34.480 --> 03:07:35.960]   So it's not a strictly human thing.
[03:07:35.960 --> 03:07:38.320]   - I wonder if it's actually a result of consequences
[03:07:38.320 --> 03:07:41.540]   of the invention of predator and prey dynamics.
[03:07:41.540 --> 03:07:46.540]   Maybe AI will have to kill off every organism that--
[03:07:46.540 --> 03:07:50.280]   - Now you're talking about killing off competition.
[03:07:50.280 --> 03:07:55.280]   - Not competition, but just like the way,
[03:07:55.280 --> 03:08:00.000]   it's like the weeds or whatever
[03:08:00.000 --> 03:08:01.720]   in a beautiful flower garden.
[03:08:01.720 --> 03:08:02.560]   - Parasites.
[03:08:02.560 --> 03:08:03.960]   - The parasites, yeah.
[03:08:03.960 --> 03:08:05.240]   On the whole system.
[03:08:05.240 --> 03:08:08.520]   Now, of course, it won't do that completely.
[03:08:08.520 --> 03:08:10.560]   It'll put them in a zoo like we do with parasites.
[03:08:10.560 --> 03:08:11.400]   - It'll ring fence.
[03:08:11.400 --> 03:08:13.160]   - Yeah, and there'll be somebody doing a PhD
[03:08:13.160 --> 03:08:15.840]   on like, they'll prod humans with a stick
[03:08:15.840 --> 03:08:16.880]   and see what they do.
[03:08:18.920 --> 03:08:22.440]   But I mean, in terms of letting us run wild
[03:08:22.440 --> 03:08:26.040]   outside of the geographically constricted region,
[03:08:26.040 --> 03:08:30.020]   that might be, it might decide to,
[03:08:30.020 --> 03:08:33.240]   no, I think there's obviously the capacity
[03:08:33.240 --> 03:08:38.240]   for beauty and kindness and non-Moloch behavior
[03:08:38.240 --> 03:08:39.360]   amidst humans.
[03:08:39.360 --> 03:08:41.460]   So I'm pretty sure AI will preserve us.
[03:08:41.460 --> 03:08:46.280]   Let me, I don't know if you answered the aliens question.
[03:08:46.280 --> 03:08:47.120]   - Oh, I didn't.
[03:08:47.120 --> 03:08:49.520]   - You had a good conversation with Toby Oyd
[03:08:49.520 --> 03:08:52.200]   about various sides of the universe.
[03:08:52.200 --> 03:08:54.680]   I think, did he say, now I'm forgetting,
[03:08:54.680 --> 03:08:58.200]   but I think he said it's a good chance we're alone.
[03:08:58.200 --> 03:09:02.220]   - So the classic Fermi paradox question is,
[03:09:02.220 --> 03:09:06.040]   there are so many spawn points,
[03:09:06.040 --> 03:09:10.640]   and yet it didn't take us that long
[03:09:10.640 --> 03:09:12.120]   to go from harnessing fire
[03:09:12.120 --> 03:09:15.160]   to sending out radio signals into space.
[03:09:15.160 --> 03:09:18.240]   So surely given the vastness of space,
[03:09:18.240 --> 03:09:20.760]   we should be, and even if only a tiny fraction of those
[03:09:20.760 --> 03:09:23.160]   create life and other civilizations too,
[03:09:23.160 --> 03:09:24.920]   we should be, the universe should be very noisy.
[03:09:24.920 --> 03:09:27.920]   There should be evidence of Dyson spheres or whatever,
[03:09:27.920 --> 03:09:29.760]   like at least radio signals and so on.
[03:09:29.760 --> 03:09:32.560]   But seemingly things are very silent out there.
[03:09:32.560 --> 03:09:33.840]   Now, of course, it depends on who you speak to.
[03:09:33.840 --> 03:09:35.880]   Some people say that they're getting signals all the time
[03:09:35.880 --> 03:09:37.520]   and so on, and I don't wanna make
[03:09:37.520 --> 03:09:38.680]   an epistemic statement on that.
[03:09:38.680 --> 03:09:43.160]   But it seems like there's a lot of silence.
[03:09:43.160 --> 03:09:45.720]   And so that raises this paradox.
[03:09:45.720 --> 03:09:50.720]   And then say, the Drake equation.
[03:09:50.720 --> 03:09:55.160]   So the Drake equation is basically just a simple thing
[03:09:55.160 --> 03:09:58.360]   of trying to estimate the number of possible civilizations
[03:09:58.360 --> 03:09:59.920]   within the galaxy by multiplying the number
[03:09:59.920 --> 03:10:02.960]   of stars created per year by the number of stars
[03:10:02.960 --> 03:10:04.640]   that have planets, planets that are habitable, blah, blah,
[03:10:04.640 --> 03:10:06.560]   blah, so all these different factors.
[03:10:06.560 --> 03:10:08.600]   And then you plug in numbers into that,
[03:10:08.600 --> 03:10:12.200]   and depending on the range of your lower bound
[03:10:12.200 --> 03:10:15.200]   and your upper bound point estimates that you put in,
[03:10:15.200 --> 03:10:16.880]   you get out a number at the end
[03:10:16.880 --> 03:10:18.560]   for the number of civilizations.
[03:10:18.560 --> 03:10:22.480]   But what Toby and his crew did differently,
[03:10:22.480 --> 03:10:25.680]   was Toby is a researcher at the Future of Humanity Institute.
[03:10:25.680 --> 03:10:31.240]   They, instead of, they realized that it's like basically
[03:10:31.240 --> 03:10:33.880]   a statistical quirk that if you put in point sources,
[03:10:33.880 --> 03:10:34.720]   even if you think you're putting
[03:10:34.720 --> 03:10:36.040]   in conservative point sources,
[03:10:36.040 --> 03:10:38.040]   because on some of these variables,
[03:10:38.760 --> 03:10:41.120]   the uncertainty is so large,
[03:10:41.120 --> 03:10:43.640]   it spans like maybe even like a couple of hundred
[03:10:43.640 --> 03:10:45.040]   of orders of magnitude.
[03:10:45.040 --> 03:10:47.480]   By putting in point sources,
[03:10:47.480 --> 03:10:50.920]   it's always going to lead to overestimates.
[03:10:50.920 --> 03:10:54.360]   And so they, by putting stuff on a log scale,
[03:10:54.360 --> 03:10:56.120]   or actually they did it on like a log log scale
[03:10:56.120 --> 03:10:58.840]   on some of them, and then like ran the simulation
[03:10:58.840 --> 03:11:02.640]   across the whole bucket of uncertainty,
[03:11:02.640 --> 03:11:04.360]   across all those orders of magnitude.
[03:11:04.360 --> 03:11:07.280]   When you do that, then actually the number comes out
[03:11:07.280 --> 03:11:08.280]   much, much smaller.
[03:11:08.280 --> 03:11:10.720]   And that's the more statistically rigorous,
[03:11:10.720 --> 03:11:13.400]   mathematically correct way of doing the calculation.
[03:11:13.400 --> 03:11:15.440]   It's still a lot of hand-waving, as science goes,
[03:11:15.440 --> 03:11:18.720]   it's like definitely, just waving,
[03:11:18.720 --> 03:11:21.280]   I don't know what an analogy is, but it's hand-wavy.
[03:11:21.280 --> 03:11:24.760]   And anyway, when they did this,
[03:11:24.760 --> 03:11:27.040]   and then they did a Bayesian update on it as well
[03:11:27.040 --> 03:11:30.160]   to like factor in the fact that there is no evidence
[03:11:30.160 --> 03:11:31.880]   that we're picking up, because no evidence
[03:11:31.880 --> 03:11:33.960]   is actually a form of evidence, right?
[03:11:33.960 --> 03:11:36.600]   And the long and short of it comes out
[03:11:36.600 --> 03:11:41.400]   that we're roughly around 70% to be the only
[03:11:41.400 --> 03:11:45.120]   intelligent civilization in our galaxy thus far,
[03:11:45.120 --> 03:11:47.880]   and around 50/50 in the entire observable universe.
[03:11:47.880 --> 03:11:50.240]   Which sounds so crazily counterintuitive,
[03:11:50.240 --> 03:11:53.440]   but their math is legit.
[03:11:53.440 --> 03:11:55.680]   - Well, yeah, the math around this particular equation,
[03:11:55.680 --> 03:11:57.840]   which the equation is ridiculous on many levels,
[03:11:57.840 --> 03:12:02.840]   but the powerful thing about the equation
[03:12:03.240 --> 03:12:06.800]   is there's different things, different components
[03:12:06.800 --> 03:12:10.880]   that can be estimated, and the error bars on which
[03:12:10.880 --> 03:12:13.840]   can be reduced with science.
[03:12:13.840 --> 03:12:17.360]   And hence, throughout, since the equation came out,
[03:12:17.360 --> 03:12:19.640]   the error bars have been coming out on different aspects.
[03:12:19.640 --> 03:12:21.000]   - Yeah, that's very true.
[03:12:21.000 --> 03:12:23.300]   - And so that, it almost kind of says,
[03:12:23.300 --> 03:12:27.020]   this gives you a mission to reduce the error bars
[03:12:27.020 --> 03:12:30.000]   on these estimates over a period of time,
[03:12:30.000 --> 03:12:32.760]   and once you do, you can better and better understand.
[03:12:32.760 --> 03:12:34.720]   Like in the process of reducing the error bars,
[03:12:34.720 --> 03:12:38.480]   you'll get to understand actually what is the right way
[03:12:38.480 --> 03:12:41.480]   to find out where the aliens are,
[03:12:41.480 --> 03:12:43.960]   how many of them there are, and all those kinds of things.
[03:12:43.960 --> 03:12:47.800]   So I don't think it's good to use that for an estimation.
[03:12:47.800 --> 03:12:50.120]   I think you do have to think from like,
[03:12:50.120 --> 03:12:51.840]   more like from first principles,
[03:12:51.840 --> 03:12:53.860]   just looking at what life is on Earth.
[03:12:53.860 --> 03:12:59.320]   Like, and trying to understand the very physics-based,
[03:12:59.320 --> 03:13:04.240]   biology, chemistry, biology-based question of what is life.
[03:13:04.240 --> 03:13:07.920]   Maybe computation-based, what the fuck is this thing?
[03:13:07.920 --> 03:13:12.160]   And that, like how difficult is it to create this thing?
[03:13:12.160 --> 03:13:14.500]   It's one way to say like, how many planets like this
[03:13:14.500 --> 03:13:16.160]   are out there, all that kind of stuff,
[03:13:16.160 --> 03:13:20.560]   but it feels like from our very limited knowledge
[03:13:20.560 --> 03:13:23.640]   perspective, the right way is to think,
[03:13:23.640 --> 03:13:28.200]   how does, what is this thing, and how does it originate?
[03:13:28.200 --> 03:13:32.000]   From very simple, non-life things,
[03:13:32.000 --> 03:13:36.200]   how does complex, life-like things emerge?
[03:13:36.200 --> 03:13:42.480]   From a rock to a bacteria, protein,
[03:13:42.480 --> 03:13:46.360]   and these like weird systems that encode information
[03:13:46.360 --> 03:13:48.480]   and pass information from-- - Self-replicate.
[03:13:48.480 --> 03:13:51.080]   - Self-replicate, and then also select each other
[03:13:51.080 --> 03:13:53.440]   and mutate in interesting ways such that they can adapt
[03:13:53.440 --> 03:13:56.600]   and evolve and build increasingly more complex systems.
[03:13:56.600 --> 03:13:58.800]   - Right, well, it's a form of information processing.
[03:13:58.800 --> 03:13:59.760]   - Yeah. - Right?
[03:13:59.760 --> 03:14:00.600]   - Right.
[03:14:00.600 --> 03:14:02.960]   - Well, it's information transfer,
[03:14:02.960 --> 03:14:05.640]   but then also an energy processing,
[03:14:05.640 --> 03:14:09.100]   which then results in, I guess, information processing?
[03:14:09.100 --> 03:14:09.940]   Maybe I'm getting bogged down.
[03:14:09.940 --> 03:14:12.040]   - Well, it's doing some modification,
[03:14:12.040 --> 03:14:14.480]   and yeah, the input is some energy.
[03:14:14.480 --> 03:14:17.640]   - Right, it's able to extract, yeah,
[03:14:17.640 --> 03:14:20.720]   extract resources from its environment
[03:14:20.720 --> 03:14:22.760]   in order to achieve a goal.
[03:14:22.760 --> 03:14:25.000]   - But the goal doesn't seem to be clear.
[03:14:25.000 --> 03:14:25.840]   - Right.
[03:14:25.840 --> 03:14:26.680]   (Lex laughing)
[03:14:26.680 --> 03:14:29.480]   - The goal is, well, the goal is to make more of itself.
[03:14:29.480 --> 03:14:33.800]   - Yeah, but in a way that increases,
[03:14:33.800 --> 03:14:36.360]   I mean, I don't know if evolution
[03:14:36.360 --> 03:14:39.880]   is a fundamental law of the universe,
[03:14:39.880 --> 03:14:44.240]   but it seems to want to replicate itself
[03:14:44.240 --> 03:14:47.700]   in a way that maximizes the chance of its survival.
[03:14:47.700 --> 03:14:51.120]   - Individual agents within an ecosystem do, yes.
[03:14:51.120 --> 03:14:53.320]   Yes, evolution itself doesn't give a fuck.
[03:14:53.320 --> 03:14:54.160]   - Right.
[03:14:54.160 --> 03:14:56.040]   - It's very, it don't care.
[03:14:56.040 --> 03:14:58.560]   It's just like, oh, you optimize it.
[03:14:58.560 --> 03:15:01.600]   Well, at least it's certainly, yeah,
[03:15:01.600 --> 03:15:03.520]   it doesn't care about the welfare
[03:15:03.520 --> 03:15:05.160]   of the individual agents within it,
[03:15:05.160 --> 03:15:06.520]   but it does seem to, I don't know.
[03:15:06.520 --> 03:15:09.760]   I think the mistake is that we're anthropomorphizing,
[03:15:09.760 --> 03:15:13.580]   to even try and give evolution a mindset,
[03:15:13.580 --> 03:15:17.240]   because it is, there's a really great post
[03:15:17.240 --> 03:15:21.500]   by Eliezer Yudkowsky on "Less Wrong,"
[03:15:21.500 --> 03:15:26.500]   which is an alien god, and he talks about
[03:15:26.500 --> 03:15:29.220]   the mistake we make when we try and put our mind,
[03:15:29.220 --> 03:15:32.340]   think through things from an evolutionary perspective,
[03:15:32.340 --> 03:15:35.180]   as though we're giving evolution some kind of agency
[03:15:35.180 --> 03:15:36.360]   and what it wants.
[03:15:36.360 --> 03:15:39.580]   Yeah, worth reading, but yeah.
[03:15:39.580 --> 03:15:42.620]   - I would like to say that having interacted
[03:15:42.620 --> 03:15:43.980]   with a lot of really smart people
[03:15:43.980 --> 03:15:46.940]   that say that anthropomorphization is a mistake,
[03:15:46.940 --> 03:15:48.760]   I would like to say that saying
[03:15:48.760 --> 03:15:51.620]   that anthropomorphization is a mistake is a mistake.
[03:15:51.620 --> 03:15:54.980]   I think there's a lot of power in anthropomorphization,
[03:15:54.980 --> 03:15:57.500]   if I can only say that word correctly one time.
[03:15:57.500 --> 03:16:00.820]   I think that's actually a really powerful way
[03:16:00.820 --> 03:16:03.020]   to reason through things, and I think people,
[03:16:03.020 --> 03:16:04.380]   especially people in robotics,
[03:16:04.380 --> 03:16:07.160]   seem to run away from it as fast as possible,
[03:16:07.160 --> 03:16:09.060]   and I just--
[03:16:09.060 --> 03:16:12.220]   - Can you give an example of how it helps in robotics?
[03:16:12.220 --> 03:16:18.220]   - Oh, in that our world is a world of humans,
[03:16:19.060 --> 03:16:23.340]   and to see robots as fundamentally just tools
[03:16:23.340 --> 03:16:27.940]   runs away from the fact that we live in a world,
[03:16:27.940 --> 03:16:30.020]   a dynamic world of humans,
[03:16:30.020 --> 03:16:33.760]   that all these game theory systems we've talked about,
[03:16:33.760 --> 03:16:37.580]   that a robot that ever has to interact with humans,
[03:16:37.580 --> 03:16:40.820]   and I don't mean intimate friendship interaction,
[03:16:40.820 --> 03:16:42.500]   I mean in a factory setting,
[03:16:42.500 --> 03:16:44.420]   where it has to deal with the uncertainty of humans,
[03:16:44.420 --> 03:16:45.540]   all that kind of stuff,
[03:16:45.540 --> 03:16:49.860]   you have to acknowledge that the robot's behavior
[03:16:49.860 --> 03:16:52.020]   has an effect on the human,
[03:16:52.020 --> 03:16:54.580]   just as much as the human has an effect on the robot,
[03:16:54.580 --> 03:16:56.100]   and there's a dance there,
[03:16:56.100 --> 03:16:58.420]   and you have to realize that this entity,
[03:16:58.420 --> 03:17:00.540]   when a human sees a robot,
[03:17:00.540 --> 03:17:04.220]   this is obvious in a physical manifestation of a robot,
[03:17:04.220 --> 03:17:05.820]   they feel a certain way,
[03:17:05.820 --> 03:17:07.820]   they have a fear, they have uncertainty,
[03:17:07.820 --> 03:17:11.880]   they have their own personal life projections,
[03:17:11.880 --> 03:17:13.260]   we have to have pets and dogs,
[03:17:13.260 --> 03:17:14.840]   and the thing looks like a dog,
[03:17:14.840 --> 03:17:17.120]   they have their own memories of what a dog is like,
[03:17:17.120 --> 03:17:18.540]   they have certain feelings,
[03:17:18.540 --> 03:17:20.740]   and that's gonna be useful in a safety setting,
[03:17:20.740 --> 03:17:22.240]   safety critical setting,
[03:17:22.240 --> 03:17:25.100]   which is one of the most trivial settings for a robot,
[03:17:25.100 --> 03:17:29.220]   in terms of how to avoid any kind of dangerous situations,
[03:17:29.220 --> 03:17:32.500]   and a robot should really consider that
[03:17:32.500 --> 03:17:34.860]   in navigating its environment,
[03:17:34.860 --> 03:17:37.840]   and we humans are right to reason about
[03:17:37.840 --> 03:17:41.460]   how a robot should consider navigating its environment
[03:17:41.460 --> 03:17:42.940]   through anthropomorphization.
[03:17:43.780 --> 03:17:46.520]   - I also think our brains are designed to think
[03:17:46.520 --> 03:17:51.520]   in human terms,
[03:17:51.520 --> 03:17:53.860]   like game theory,
[03:17:53.860 --> 03:18:00.320]   I think is best applied in the space of human decisions,
[03:18:00.320 --> 03:18:03.440]   and so--
[03:18:03.440 --> 03:18:04.280]   - Right, you're dealing,
[03:18:04.280 --> 03:18:06.160]   I mean, with things like AI,
[03:18:06.160 --> 03:18:07.400]   AI is, they are,
[03:18:07.400 --> 03:18:10.520]   you know, we can somewhat,
[03:18:10.520 --> 03:18:11.980]   like I don't think it's,
[03:18:12.000 --> 03:18:14.160]   the reason I say anthropomorphization,
[03:18:14.160 --> 03:18:15.480]   we need to be careful with,
[03:18:15.480 --> 03:18:19.440]   is because there is a danger of overly applying,
[03:18:19.440 --> 03:18:23.480]   overly, wrongly assuming that this artificial intelligence
[03:18:23.480 --> 03:18:25.880]   is going to operate in any similar way to us,
[03:18:25.880 --> 03:18:27.760]   because it is operating
[03:18:27.760 --> 03:18:29.680]   on a fundamentally different substrate,
[03:18:29.680 --> 03:18:33.280]   like even dogs, or even mice, or whatever,
[03:18:33.280 --> 03:18:34.320]   in some ways, like,
[03:18:34.320 --> 03:18:37.520]   anthropomorphizing them is less of a mistake, I think,
[03:18:37.520 --> 03:18:40.200]   than an AI, even though it's an AI we built, and so on,
[03:18:40.200 --> 03:18:41.980]   because at least we know that they're running
[03:18:41.980 --> 03:18:43.380]   from the same substrate,
[03:18:43.380 --> 03:18:45.580]   and they've also evolved from the same,
[03:18:45.580 --> 03:18:48.660]   out of the same evolutionary process, you know,
[03:18:48.660 --> 03:18:50.620]   they've followed this evolution of, like,
[03:18:50.620 --> 03:18:52.420]   needing to compete for resources,
[03:18:52.420 --> 03:18:55.180]   and needing to find a mate, and that kind of stuff,
[03:18:55.180 --> 03:18:58.260]   whereas an AI that has just popped into existence
[03:18:58.260 --> 03:19:01.180]   somewhere on a cloud server, let's say,
[03:19:01.180 --> 03:19:02.820]   you know, or whatever, however it runs,
[03:19:02.820 --> 03:19:04.260]   and whatever, whether, I don't know,
[03:19:04.260 --> 03:19:05.540]   whether they have an internal experience,
[03:19:05.540 --> 03:19:07.020]   I don't think they necessarily do,
[03:19:07.020 --> 03:19:08.100]   in fact, I don't think they do,
[03:19:08.100 --> 03:19:11.900]   but the point is, is that to try and apply
[03:19:11.900 --> 03:19:13.960]   any kind of modeling of, like,
[03:19:13.960 --> 03:19:15.340]   thinking through problems and decisions
[03:19:15.340 --> 03:19:16.900]   in the same way that we do,
[03:19:16.900 --> 03:19:18.740]   has to be done extremely carefully,
[03:19:18.740 --> 03:19:23.540]   because they are, like, they're so alien,
[03:19:23.540 --> 03:19:26.300]   their method of whatever their form of thinking is,
[03:19:26.300 --> 03:19:27.180]   it's just so different,
[03:19:27.180 --> 03:19:29.140]   because they've never had to evolve,
[03:19:29.140 --> 03:19:30.620]   you know, in the same way.
[03:19:30.620 --> 03:19:32.260]   - Yeah, beautifully put,
[03:19:32.260 --> 03:19:33.740]   I was just playing devil's advocate.
[03:19:33.740 --> 03:19:35.700]   I do think, in certain contexts,
[03:19:35.700 --> 03:19:37.900]   anthropomorphization is not gonna hurt you.
[03:19:37.900 --> 03:19:38.740]   - Yes.
[03:19:38.740 --> 03:19:39.940]   - Engineers run away from it too fast.
[03:19:39.940 --> 03:19:41.140]   - I can see that.
[03:19:41.140 --> 03:19:43.660]   - But for the most point, you're right.
[03:19:43.660 --> 03:19:48.660]   Do you have advice for young people today,
[03:19:48.660 --> 03:19:51.340]   like the 17-year-old that you were,
[03:19:51.340 --> 03:19:55.380]   of how to live life you can be proud of,
[03:19:55.380 --> 03:19:57.780]   how to have a career you can be proud of,
[03:19:57.780 --> 03:19:59.960]   in this world full of mullocks?
[03:19:59.960 --> 03:20:02.460]   - Think about the win-wins,
[03:20:02.460 --> 03:20:04.660]   look for win-win situations,
[03:20:05.500 --> 03:20:09.940]   and be careful not to, you know,
[03:20:09.940 --> 03:20:12.220]   overly use your smarts to convince yourself
[03:20:12.220 --> 03:20:13.620]   that something is win-win when it's not,
[03:20:13.620 --> 03:20:14.620]   so that's difficult,
[03:20:14.620 --> 03:20:16.340]   and I don't know how to advise, you know,
[03:20:16.340 --> 03:20:17.620]   people on that,
[03:20:17.620 --> 03:20:20.180]   'cause it's something I'm still figuring out myself,
[03:20:20.180 --> 03:20:23.940]   but have that as a sort of default MO.
[03:20:23.940 --> 03:20:28.060]   Don't see things, everything as a zero-sum game,
[03:20:28.060 --> 03:20:29.740]   try to find the positive-sum-ness,
[03:20:29.740 --> 03:20:30.660]   and, like, find ways,
[03:20:30.660 --> 03:20:32.700]   if there doesn't seem to be one,
[03:20:32.700 --> 03:20:34.220]   consider playing a different game,
[03:20:34.220 --> 03:20:35.700]   so I would suggest that.
[03:20:35.700 --> 03:20:39.020]   Do not become a professional poker player,
[03:20:39.020 --> 03:20:40.940]   'cause people always ask,
[03:20:40.940 --> 03:20:43.220]   they're like, "Oh, she's a pro, I wanna do that too."
[03:20:43.220 --> 03:20:44.860]   Fine, you could've done it if you were,
[03:20:44.860 --> 03:20:46.260]   you know, when I started out,
[03:20:46.260 --> 03:20:48.260]   it was a very different situation back then.
[03:20:48.260 --> 03:20:49.620]   Poker is, you know,
[03:20:49.620 --> 03:20:52.580]   a great game to learn
[03:20:52.580 --> 03:20:54.940]   in order to understand the ways to think,
[03:20:54.940 --> 03:20:56.780]   and I recommend people learn it,
[03:20:56.780 --> 03:20:58.380]   but don't try making a living from it these days,
[03:20:58.380 --> 03:21:00.260]   it's almost, it's very, very difficult,
[03:21:00.260 --> 03:21:01.900]   to the point of being impossible.
[03:21:03.620 --> 03:21:04.460]   And then,
[03:21:04.460 --> 03:21:09.660]   really, really be aware of how much time you spend
[03:21:09.660 --> 03:21:12.100]   on your phone and on social media,
[03:21:12.100 --> 03:21:14.260]   and really try and keep it to a minimum.
[03:21:14.260 --> 03:21:15.820]   Be aware that, basically,
[03:21:15.820 --> 03:21:18.100]   every moment that you spend on it is bad for you.
[03:21:18.100 --> 03:21:20.140]   So, doesn't mean to say you can never do it,
[03:21:20.140 --> 03:21:22.500]   but just have that running in the background.
[03:21:22.500 --> 03:21:25.260]   I'm doing a bad thing for myself right now.
[03:21:25.260 --> 03:21:27.220]   I think that's the general rule of thumb.
[03:21:27.220 --> 03:21:31.140]   - Of course, about becoming a professional poker player,
[03:21:31.140 --> 03:21:32.900]   if there is a thing in your life
[03:21:32.900 --> 03:21:35.380]   that's like that,
[03:21:35.380 --> 03:21:37.660]   and nobody can convince you otherwise,
[03:21:37.660 --> 03:21:38.660]   just fucking do it.
[03:21:38.660 --> 03:21:42.660]   Don't listen to anyone's advice.
[03:21:42.660 --> 03:21:46.340]   Find a thing that you can't be talked out of, too.
[03:21:46.340 --> 03:21:47.340]   That's a thing.
[03:21:47.340 --> 03:21:48.500]   - I like that, yeah.
[03:21:48.500 --> 03:21:54.460]   - You were a lead guitarist in a metal band.
[03:21:54.460 --> 03:21:56.940]   Did I write that down for something?
[03:21:56.940 --> 03:22:00.220]   What did you do it for?
[03:22:02.620 --> 03:22:03.620]   Was it the performing?
[03:22:03.620 --> 03:22:07.140]   Was it the pure, the music of it?
[03:22:07.140 --> 03:22:08.980]   Was it just being a rock star?
[03:22:08.980 --> 03:22:10.020]   Why'd you do it?
[03:22:10.020 --> 03:22:15.460]   - So, we only ever played two gigs.
[03:22:15.460 --> 03:22:17.940]   We didn't last, you know, it wasn't a very,
[03:22:17.940 --> 03:22:20.620]   we weren't famous or anything like that.
[03:22:20.620 --> 03:22:25.340]   But I was very into metal.
[03:22:25.340 --> 03:22:27.500]   Like, it was my entire identity.
[03:22:27.500 --> 03:22:29.420]   Sort of from the age of 16 to 23.
[03:22:29.420 --> 03:22:31.940]   - What's the best metal band of all time?
[03:22:31.940 --> 03:22:34.060]   Don't ask me that, it's so hard to answer.
[03:22:34.060 --> 03:22:39.820]   - So, I know I had a long argument with,
[03:22:39.820 --> 03:22:43.980]   I'm a guitarist, more like a classic rock guitarist.
[03:22:43.980 --> 03:22:45.820]   So, you know, I've had friends
[03:22:45.820 --> 03:22:47.340]   who are very big Pantera fans,
[03:22:47.340 --> 03:22:50.780]   and so there was often arguments about,
[03:22:50.780 --> 03:22:53.900]   what's the better metal band, Metallica versus Pantera?
[03:22:53.900 --> 03:22:57.340]   This is a more kind of '90s maybe discussion.
[03:22:57.340 --> 03:22:59.420]   But I was always on the side of Metallica,
[03:23:00.380 --> 03:23:02.740]   both musically and in terms of performance
[03:23:02.740 --> 03:23:05.780]   and the depth of lyrics and so on.
[03:23:05.780 --> 03:23:10.260]   But they were, basically everybody was against me.
[03:23:10.260 --> 03:23:12.260]   'Cause if you're a true metal fan,
[03:23:12.260 --> 03:23:14.540]   I guess the idea goes is you can't possibly
[03:23:14.540 --> 03:23:16.180]   be a Metallica fan.
[03:23:16.180 --> 03:23:17.500]   - I think that's crazy. - 'Cause Metallica's pop,
[03:23:17.500 --> 03:23:19.580]   it's like, they sold out.
[03:23:19.580 --> 03:23:20.620]   - Metallica are metal.
[03:23:20.620 --> 03:23:24.420]   Like, they were the, I mean, again,
[03:23:24.420 --> 03:23:26.540]   you can't say who was the godfather of metal, blah, blah, blah.
[03:23:26.540 --> 03:23:31.540]   But they were so groundbreaking and so brilliant.
[03:23:31.540 --> 03:23:35.620]   I mean, you've named literally two of my favorite bands.
[03:23:35.620 --> 03:23:37.260]   When you ask that question of who are my favorites,
[03:23:37.260 --> 03:23:39.020]   like those were two that came up.
[03:23:39.020 --> 03:23:41.500]   A third one is Children of Bodom,
[03:23:41.500 --> 03:23:44.980]   who I just think, ugh, they just tick all the boxes for me.
[03:23:44.980 --> 03:23:48.260]   Yeah, I don't know.
[03:23:48.260 --> 03:23:54.260]   Nowadays, I kind of sort of feel like a repulsion to the,
[03:23:54.260 --> 03:23:55.700]   I was that myself.
[03:23:55.700 --> 03:23:56.900]   Like, I'd be like, who do you prefer more?
[03:23:56.900 --> 03:23:58.940]   Come on, who's, like, no, you have to rank them.
[03:23:58.940 --> 03:24:01.740]   But it's like this false zero-sum-ness that's like, why?
[03:24:01.740 --> 03:24:02.660]   They're so additive.
[03:24:02.660 --> 03:24:04.620]   Like, there's no conflict there.
[03:24:04.620 --> 03:24:07.020]   - Although, when people ask that kind of question
[03:24:07.020 --> 03:24:11.460]   about anything, movies, I feel like it's hard work
[03:24:11.460 --> 03:24:14.940]   and it's unfair, but it's, you should pick one.
[03:24:14.940 --> 03:24:16.260]   - Yeah. - Like, and I,
[03:24:16.260 --> 03:24:17.820]   that's actually, you know, the same kind of,
[03:24:17.820 --> 03:24:19.380]   it's like a fear of a commitment.
[03:24:19.380 --> 03:24:20.220]   - Right. - When people ask me,
[03:24:20.220 --> 03:24:21.060]   what's your favorite band?
[03:24:21.060 --> 03:24:22.580]   It's like, phew.
[03:24:22.580 --> 03:24:24.300]   But I, you know, it's good to pick.
[03:24:24.300 --> 03:24:25.460]   - Exactly, and thank you for, yeah,
[03:24:25.460 --> 03:24:27.660]   thank you for the tough question, yeah.
[03:24:27.660 --> 03:24:29.500]   - Well, maybe not in the context
[03:24:29.500 --> 03:24:31.820]   when a lot of people are listening.
[03:24:31.820 --> 03:24:33.980]   - Can I just like, what, why does this matter?
[03:24:33.980 --> 03:24:35.580]   No, it does, it's--
[03:24:35.580 --> 03:24:37.380]   - Are you still into metal?
[03:24:37.380 --> 03:24:38.500]   - Funny enough, I was listening to a bunch
[03:24:38.500 --> 03:24:39.820]   before I came over here.
[03:24:39.820 --> 03:24:43.420]   - Oh, like, do you use it for, like, motivation
[03:24:43.420 --> 03:24:44.620]   or get you in a certain--
[03:24:44.620 --> 03:24:46.900]   - Yeah, I was weirdly listening to 80s hair metal
[03:24:46.900 --> 03:24:48.100]   before I came.
[03:24:48.100 --> 03:24:49.820]   - Does that count as metal?
[03:24:49.820 --> 03:24:50.700]   - I think so.
[03:24:50.700 --> 03:24:53.420]   It's like proto-metal and it's happy.
[03:24:53.420 --> 03:24:55.780]   It's optimistic, happy, proto-metal.
[03:24:55.780 --> 03:24:57.780]   Yeah, I mean, these things, you know,
[03:24:57.780 --> 03:25:00.020]   all these genres bleed into each other.
[03:25:00.020 --> 03:25:03.420]   But yeah, sorry, to answer your question about guitar playing,
[03:25:03.420 --> 03:25:05.060]   my relationship with it was kind of weird
[03:25:05.060 --> 03:25:09.140]   in that I was deeply uncreative.
[03:25:09.140 --> 03:25:12.180]   My objective would be to hear some really hard technical solo
[03:25:12.180 --> 03:25:15.980]   and then learn it, memorize it, and then play it perfectly.
[03:25:15.980 --> 03:25:19.220]   But I was incapable of trying to write my own music.
[03:25:19.220 --> 03:25:21.620]   Like, the idea was just absolutely terrifying.
[03:25:23.140 --> 03:25:24.660]   But I was also just thinking, I was like,
[03:25:24.660 --> 03:25:28.620]   it'd be kind of cool to actually try starting a band again
[03:25:28.620 --> 03:25:31.580]   and getting back into it and write.
[03:25:31.580 --> 03:25:34.020]   But it's scary.
[03:25:34.020 --> 03:25:34.860]   - It's scary.
[03:25:34.860 --> 03:25:36.860]   I mean, I put out some guitar playing
[03:25:36.860 --> 03:25:38.140]   just other people's covers.
[03:25:38.140 --> 03:25:41.500]   Like, I play Comfortably Numb on the internet.
[03:25:41.500 --> 03:25:42.500]   It's scary, too.
[03:25:42.500 --> 03:25:44.260]   It's scary putting stuff out there.
[03:25:44.260 --> 03:25:47.700]   And I had this similar kind of fascination
[03:25:47.700 --> 03:25:50.300]   with technical playing, both on piano and guitar.
[03:25:50.300 --> 03:25:55.300]   And one of the reasons I started learning guitar
[03:25:55.300 --> 03:26:01.740]   is from Ozzy Osbourne, Mr. Crowley's solo.
[03:26:01.740 --> 03:26:04.300]   And one of the first solos I learned is that,
[03:26:04.300 --> 03:26:07.580]   and there's a beauty to it.
[03:26:07.580 --> 03:26:08.420]   There's a lot of beauty to it.
[03:26:08.420 --> 03:26:09.260]   - Tapping, right?
[03:26:09.260 --> 03:26:13.500]   - Yeah, there's some tapping, but it's just really fast.
[03:26:13.500 --> 03:26:15.700]   - Beautiful, like arpeggios.
[03:26:15.700 --> 03:26:16.900]   - Yeah, arpeggios, yeah.
[03:26:16.900 --> 03:26:19.620]   But there's a melody that you can hear through it,
[03:26:19.620 --> 03:26:21.620]   but there's also build up.
[03:26:21.620 --> 03:26:23.900]   It's a beautiful solo, but it's also technically,
[03:26:23.900 --> 03:26:27.060]   just visually the way it looks when a person's watching,
[03:26:27.060 --> 03:26:29.660]   you feel like a rockstar playing it.
[03:26:29.660 --> 03:26:32.140]   But it ultimately has to do with technical.
[03:26:32.140 --> 03:26:36.420]   You're not developing the part of your brain
[03:26:36.420 --> 03:26:40.620]   that I think requires you to generate beautiful music.
[03:26:40.620 --> 03:26:42.340]   It is ultimately technical in nature.
[03:26:42.340 --> 03:26:45.820]   And so that took me a long time to let go of that
[03:26:45.820 --> 03:26:48.980]   and just be able to write music myself.
[03:26:48.980 --> 03:26:52.260]   And that's a different journey, I think.
[03:26:52.260 --> 03:26:55.060]   I think that journey is a little bit more inspired
[03:26:55.060 --> 03:26:57.060]   in the blues world, for example,
[03:26:57.060 --> 03:26:58.620]   where improvisation is more valued,
[03:26:58.620 --> 03:26:59.940]   obviously in jazz and so on.
[03:26:59.940 --> 03:27:04.940]   But I think ultimately it's a more rewarding journey
[03:27:04.940 --> 03:27:08.420]   'cause you get to, your relationship with the guitar
[03:27:08.420 --> 03:27:12.180]   then becomes a kind of escape from the world
[03:27:12.180 --> 03:27:14.100]   where you can create, create.
[03:27:14.100 --> 03:27:16.140]   I mean, creating stuff is-
[03:27:16.140 --> 03:27:18.780]   - And it's something you work with,
[03:27:18.780 --> 03:27:20.020]   'cause my relationship with my guitar
[03:27:20.020 --> 03:27:22.780]   was like it was something to tame and defeat.
[03:27:22.780 --> 03:27:23.620]   (laughing)
[03:27:23.620 --> 03:27:24.780]   - Yeah, it's a challenge.
[03:27:24.780 --> 03:27:26.820]   - Which was kind of what my whole personality was back then.
[03:27:26.820 --> 03:27:28.820]   Like I was just very, like, as I said,
[03:27:28.820 --> 03:27:31.380]   like very competitive, very just like must
[03:27:31.380 --> 03:27:33.140]   bend this thing to my will.
[03:27:33.140 --> 03:27:36.620]   Whereas writing music is you work, it's like a dance.
[03:27:36.620 --> 03:27:37.860]   You work with it.
[03:27:37.860 --> 03:27:39.900]   - But I think because of the competitive aspect,
[03:27:39.900 --> 03:27:42.460]   for me at least, that's still there,
[03:27:42.460 --> 03:27:45.740]   which creates anxiety about playing publicly
[03:27:45.740 --> 03:27:47.100]   or all that kind of stuff.
[03:27:47.180 --> 03:27:49.380]   I think there's just like a harsh self criticism
[03:27:49.380 --> 03:27:50.780]   within the whole thing.
[03:27:50.780 --> 03:27:53.380]   It's really, really, it's really tough.
[03:27:53.380 --> 03:27:55.420]   - I wanna hear some of your stuff.
[03:27:55.420 --> 03:27:59.300]   - I mean, there's certain things that feel really personal.
[03:27:59.300 --> 03:28:03.380]   And on top of that, as we talked about poker offline,
[03:28:03.380 --> 03:28:05.580]   there's certain things that you get to a certain height
[03:28:05.580 --> 03:28:07.140]   in your life, and that doesn't have to be very high,
[03:28:07.140 --> 03:28:09.060]   but you get to a certain height
[03:28:09.060 --> 03:28:11.740]   and then you put it aside for a bit
[03:28:11.740 --> 03:28:13.180]   and it's hard to return to it
[03:28:13.180 --> 03:28:15.940]   because you remember being good.
[03:28:15.940 --> 03:28:19.860]   And it's hard to, like you being at a very high level
[03:28:19.860 --> 03:28:22.540]   in poker, it might be hard for you to return to poker
[03:28:22.540 --> 03:28:24.620]   every once in a while and you enjoy it,
[03:28:24.620 --> 03:28:26.900]   knowing that you're just not as sharp as you used to be
[03:28:26.900 --> 03:28:29.460]   because you're not doing it every single day.
[03:28:29.460 --> 03:28:31.620]   That's something I always wonder with, I mean,
[03:28:31.620 --> 03:28:33.780]   even just like in chess with Kasparov,
[03:28:33.780 --> 03:28:35.820]   some of these greats, just returning to it,
[03:28:35.820 --> 03:28:37.100]   it's just, it's almost painful.
[03:28:37.100 --> 03:28:38.420]   - Yes, I can imagine, yeah.
[03:28:38.420 --> 03:28:40.660]   - And I feel that way with guitar too,
[03:28:40.660 --> 03:28:44.180]   'cause I used to play like every day a lot.
[03:28:44.180 --> 03:28:47.260]   So returning to it is painful 'cause like,
[03:28:47.260 --> 03:28:51.460]   it's like accepting the fact that this whole ride is finite
[03:28:51.460 --> 03:28:56.460]   and you have a prime, there's a time when you were really
[03:28:56.460 --> 03:28:58.780]   good and now it's over and now-
[03:28:58.780 --> 03:29:00.340]   - We're on a different chapter of life.
[03:29:00.340 --> 03:29:02.940]   It's like, oh, but I miss that, yeah.
[03:29:02.940 --> 03:29:06.500]   - But you can still discover joy within that process.
[03:29:06.500 --> 03:29:09.140]   It's been tough, especially with some level of like,
[03:29:09.140 --> 03:29:12.100]   as people get to know you, there's,
[03:29:12.100 --> 03:29:16.620]   and people film stuff, you don't have the privacy
[03:29:16.620 --> 03:29:20.580]   of just sharing something with a few people around you.
[03:29:20.580 --> 03:29:21.580]   - Yeah.
[03:29:21.580 --> 03:29:23.380]   - That's a beautiful privacy that-
[03:29:23.380 --> 03:29:24.220]   - That's a good point.
[03:29:24.220 --> 03:29:26.020]   - With the internet, it's just disappearing.
[03:29:26.020 --> 03:29:27.740]   - Yeah, that's a really good point.
[03:29:27.740 --> 03:29:29.140]   - Yeah.
[03:29:29.140 --> 03:29:31.860]   But all those pressures aside, if you really,
[03:29:31.860 --> 03:29:34.940]   you can step up and still enjoy the fuck out of
[03:29:34.940 --> 03:29:37.900]   a good musical performance.
[03:29:37.900 --> 03:29:41.740]   What do you think is the meaning of this whole thing?
[03:29:42.340 --> 03:29:43.780]   - What's the meaning of life?
[03:29:43.780 --> 03:29:45.620]   - Wow.
[03:29:45.620 --> 03:29:47.340]   - It's in your name, as we talked about,
[03:29:47.340 --> 03:29:50.980]   you have to live up, do you feel the requirement
[03:29:50.980 --> 03:29:52.580]   to have to live up to your name?
[03:29:52.580 --> 03:29:55.220]   - Because live?
[03:29:55.220 --> 03:29:56.060]   - Yeah.
[03:29:56.060 --> 03:29:57.780]   - No, because I don't see it.
[03:29:57.780 --> 03:30:00.660]   I mean, my, again, it's kind of like,
[03:30:00.660 --> 03:30:03.300]   no, I don't know.
[03:30:03.300 --> 03:30:04.140]   - And as we talked about-
[03:30:04.140 --> 03:30:05.340]   - Because my full name is Olivia.
[03:30:05.340 --> 03:30:06.180]   - Yeah.
[03:30:06.180 --> 03:30:07.220]   - So I can retreat in that and be like,
[03:30:07.220 --> 03:30:09.060]   oh, Olivia, what does that even mean?
[03:30:09.060 --> 03:30:11.020]   Live up to live.
[03:30:12.020 --> 03:30:13.820]   And no, I can't say I do,
[03:30:13.820 --> 03:30:15.340]   'cause I've never thought of it that way.
[03:30:15.340 --> 03:30:17.220]   - And then your name backwards is evil.
[03:30:17.220 --> 03:30:18.820]   That's what we also talked about.
[03:30:18.820 --> 03:30:22.060]   There's like layers of evil.
[03:30:22.060 --> 03:30:24.500]   - I mean, I feel the urge to live up to that,
[03:30:24.500 --> 03:30:26.420]   to be the inverse of evil.
[03:30:26.420 --> 03:30:27.740]   - Yeah.
[03:30:27.740 --> 03:30:30.420]   - Or even better, 'cause I don't think,
[03:30:30.420 --> 03:30:31.980]   is the inverse of evil good,
[03:30:31.980 --> 03:30:34.860]   or is good something completely separate to that?
[03:30:34.860 --> 03:30:36.980]   I think my intuition says it's the latter,
[03:30:36.980 --> 03:30:37.820]   but I don't know.
[03:30:37.820 --> 03:30:39.860]   Anyway, again, getting in the weeds.
[03:30:39.860 --> 03:30:42.100]   - What is the meaning of all this?
[03:30:42.100 --> 03:30:42.940]   - Of life.
[03:30:42.940 --> 03:30:45.180]   Why are we here?
[03:30:45.180 --> 03:30:51.820]   - I think to explore, have fun, and understand,
[03:30:51.820 --> 03:30:55.940]   and make more of here and to keep the game going.
[03:30:55.940 --> 03:30:56.780]   - Of here?
[03:30:56.780 --> 03:30:57.620]   More of here?
[03:30:57.620 --> 03:30:58.460]   - More of-
[03:30:58.460 --> 03:31:00.020]   - More of this, whatever this is?
[03:31:00.020 --> 03:31:02.060]   - More of experience.
[03:31:02.060 --> 03:31:03.180]   Just to have more of experience,
[03:31:03.180 --> 03:31:05.540]   and ideally, positive experience.
[03:31:06.460 --> 03:31:10.460]   And more complex, I guess,
[03:31:10.460 --> 03:31:13.060]   to try and put it into a vaguely scientific term,
[03:31:13.060 --> 03:31:18.340]   make it so that the program required,
[03:31:18.340 --> 03:31:21.460]   the length of code required to describe the universe
[03:31:21.460 --> 03:31:22.660]   is as long as possible.
[03:31:22.660 --> 03:31:26.260]   And highly complex, and therefore interesting.
[03:31:26.260 --> 03:31:32.100]   Because again, I know we banged the metaphor to death,
[03:31:32.180 --> 03:31:37.180]   but like, tiled with paperclips
[03:31:37.180 --> 03:31:41.100]   doesn't require that much of a code to describe.
[03:31:41.100 --> 03:31:42.700]   Obviously, maybe something emerges from it,
[03:31:42.700 --> 03:31:44.820]   but that steady state, assuming a steady state,
[03:31:44.820 --> 03:31:45.620]   it's not very interesting.
[03:31:45.620 --> 03:31:49.660]   Whereas it seems like our universe is over time
[03:31:49.660 --> 03:31:51.900]   becoming more and more complex and interesting.
[03:31:51.900 --> 03:31:53.940]   There's so much richness and beauty and diversity
[03:31:53.940 --> 03:31:56.260]   on this Earth, and I want that to continue and get more.
[03:31:56.260 --> 03:31:58.580]   I want more diversity,
[03:31:58.580 --> 03:32:00.660]   and in the very best sense of that word,
[03:32:01.860 --> 03:32:06.860]   is to me the goal of all this.
[03:32:06.860 --> 03:32:10.820]   - Yeah, and somehow have fun in the process.
[03:32:10.820 --> 03:32:12.060]   - Yes.
[03:32:12.060 --> 03:32:14.340]   - 'Cause we do create a lot of fun things along,
[03:32:14.340 --> 03:32:17.940]   instead of in this creative force,
[03:32:17.940 --> 03:32:19.460]   and all the beautiful things we create,
[03:32:19.460 --> 03:32:22.460]   somehow there's like a funness to it.
[03:32:22.460 --> 03:32:25.100]   And perhaps that has to do with the finiteness of life,
[03:32:25.100 --> 03:32:27.100]   the finiteness of all these experiences,
[03:32:27.100 --> 03:32:30.340]   which is what makes them kind of unique.
[03:32:31.300 --> 03:32:35.420]   Like the fact that they end, there's this, whatever it is,
[03:32:35.420 --> 03:32:40.420]   falling in love or creating a piece of art,
[03:32:40.420 --> 03:32:45.540]   or creating a bridge, or creating a rocket,
[03:32:45.540 --> 03:32:48.820]   or creating a, I don't know,
[03:32:48.820 --> 03:32:53.140]   just the businesses that build something
[03:32:53.140 --> 03:32:56.140]   or solve something.
[03:32:56.140 --> 03:32:59.260]   The fact that it is born and it dies,
[03:33:00.820 --> 03:33:05.820]   somehow embeds it with fun, with joy,
[03:33:05.820 --> 03:33:08.420]   for the people involved.
[03:33:08.420 --> 03:33:11.420]   I don't know what that is, the finiteness of it.
[03:33:11.420 --> 03:33:13.820]   - It can do, some people struggle with the,
[03:33:13.820 --> 03:33:17.860]   I mean a big thing I think that one has to learn
[03:33:17.860 --> 03:33:21.100]   is being okay with things coming to an end.
[03:33:21.100 --> 03:33:25.620]   And in terms of like projects and so on,
[03:33:25.620 --> 03:33:28.580]   people cling onto things beyond what they're meant to be,
[03:33:28.660 --> 03:33:30.460]   beyond what is reasonable.
[03:33:30.460 --> 03:33:33.900]   - And I'm gonna have to come to terms
[03:33:33.900 --> 03:33:35.820]   with this podcast coming to an end.
[03:33:35.820 --> 03:33:37.060]   I really enjoyed talking to you.
[03:33:37.060 --> 03:33:40.180]   I think it's obvious, as we've talked about many times,
[03:33:40.180 --> 03:33:41.380]   you should be doing a podcast.
[03:33:41.380 --> 03:33:45.980]   You should, you're already doing a lot of stuff publicly
[03:33:45.980 --> 03:33:47.420]   to the world, which is awesome.
[03:33:47.420 --> 03:33:49.180]   You're a great educator, you're a great mind,
[03:33:49.180 --> 03:33:50.300]   you're a great intellect.
[03:33:50.300 --> 03:33:52.860]   But it's also this whole medium of just talking
[03:33:52.860 --> 03:33:53.780]   is also fun. - It is good.
[03:33:53.780 --> 03:33:54.620]   - It's a fun one.
[03:33:54.620 --> 03:33:55.540]   - It really is good.
[03:33:55.540 --> 03:33:58.860]   And it's just, it's nothing but like,
[03:33:58.860 --> 03:34:00.580]   oh, it's just so much fun.
[03:34:00.580 --> 03:34:03.220]   And you can just get into so many,
[03:34:03.220 --> 03:34:05.300]   yeah, there's this space to just explore
[03:34:05.300 --> 03:34:08.020]   and see what comes and emerges and yeah.
[03:34:08.020 --> 03:34:09.380]   - Yeah, to understand yourself better
[03:34:09.380 --> 03:34:11.540]   and if you're talking to others to understand them better
[03:34:11.540 --> 03:34:12.620]   and together with them.
[03:34:12.620 --> 03:34:15.380]   I mean, you should do your own podcast,
[03:34:15.380 --> 03:34:17.020]   but you should also do a podcast with Cee
[03:34:17.020 --> 03:34:18.740]   as we talked about.
[03:34:18.740 --> 03:34:22.880]   The two of you have such different minds
[03:34:22.880 --> 03:34:26.100]   that like melt together in just hilarious ways,
[03:34:26.100 --> 03:34:29.340]   fascinating ways, just the tension of ideas there
[03:34:29.340 --> 03:34:30.260]   is really powerful.
[03:34:30.260 --> 03:34:33.220]   But in general, I think you got a beautiful voice.
[03:34:33.220 --> 03:34:35.340]   So thank you so much for talking today.
[03:34:35.340 --> 03:34:36.540]   Thank you for being a friend.
[03:34:36.540 --> 03:34:39.380]   Thank you for honoring me with this conversation
[03:34:39.380 --> 03:34:40.420]   and with your valuable time.
[03:34:40.420 --> 03:34:42.580]   Thanks, Liv. - Thank you.
[03:34:42.580 --> 03:34:45.180]   - Thanks for listening to this conversation with Liv Boree.
[03:34:45.180 --> 03:34:46.300]   To support this podcast,
[03:34:46.300 --> 03:34:48.820]   please check out our sponsors in the description.
[03:34:48.820 --> 03:34:50.900]   And now let me leave you with some words
[03:34:50.900 --> 03:34:52.040]   from Richard Feynman.
[03:34:53.040 --> 03:34:56.260]   "I think it's much more interesting to live not knowing
[03:34:56.260 --> 03:34:59.300]   than to have answers which might be wrong.
[03:34:59.300 --> 03:35:01.820]   I have approximate answers and possible beliefs
[03:35:01.820 --> 03:35:03.580]   and different degrees of uncertainty
[03:35:03.580 --> 03:35:05.120]   about different things,
[03:35:05.120 --> 03:35:08.540]   but I'm not absolutely sure of anything.
[03:35:08.540 --> 03:35:11.460]   And there are many things I don't know anything about
[03:35:11.460 --> 03:35:15.460]   such as whether it means anything to ask why we're here.
[03:35:15.460 --> 03:35:17.580]   I don't have to know the answer.
[03:35:17.580 --> 03:35:20.760]   I don't feel frightened not knowing things
[03:35:20.760 --> 03:35:24.300]   by being lost in a mysterious universe without any purpose,
[03:35:24.300 --> 03:35:27.620]   which is the way it really is as far as I can tell."
[03:35:27.620 --> 03:35:31.720]   Thank you for listening and hope to see you next time.
[03:35:31.720 --> 03:35:34.300]   (upbeat music)
[03:35:34.300 --> 03:35:36.880]   (upbeat music)
[03:35:36.880 --> 03:35:40.460]   (Session concluded at 4pm)

