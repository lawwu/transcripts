
[00:00:00.000 --> 00:00:03.700]   If there's one core idea that actually unites all three of the branches of math that we're
[00:00:03.700 --> 00:00:06.600]   talking about in this course, is that linearity is important.
[00:00:06.600 --> 00:00:08.160]   Linearity is central to linear algebra.
[00:00:08.160 --> 00:00:11.600]   It was central in calculus because we were doing these linear approximations, and now
[00:00:11.600 --> 00:00:14.980]   we have these expected values that are also linear operations.
[00:00:14.980 --> 00:00:19.560]   So I got some advice once from a physicist friend that if I wanted to understand mathematics,
[00:00:19.560 --> 00:00:22.560]   I should take a long bubble bath and meditate on linearity.
[00:00:22.560 --> 00:00:24.480]   And that turned out to be really great advice.
[00:00:24.480 --> 00:00:25.480]   Nice.
[00:00:25.480 --> 00:00:26.480]   I need to try that.
[00:00:26.480 --> 00:00:27.480]   Yeah.
[00:00:27.480 --> 00:00:30.480]   I'm going to try that.
[00:00:30.480 --> 00:00:31.840]   Yes.
[00:00:31.840 --> 00:00:36.920]   Welcome back to the final exercise session for the Math for ML class.
[00:00:36.920 --> 00:00:40.080]   Today's session is on probability.
[00:00:40.080 --> 00:00:42.820]   And I am, as always, your host, Charles.
[00:00:42.820 --> 00:00:48.320]   And with me today, I have ML engineer at Weights & Biases, Scott Condren, lovely, working
[00:00:48.320 --> 00:00:49.320]   through the exercises.
[00:00:49.320 --> 00:00:50.320]   Hey, Scott.
[00:00:50.320 --> 00:00:51.320]   All right.
[00:00:51.320 --> 00:00:56.040]   Probability is one of the tougher components of the math of MLs.
[00:00:56.040 --> 00:00:59.120]   And there's like lots of really deep ideas.
[00:00:59.120 --> 00:01:01.520]   So understanding it from its fundamentals can be really challenging.
[00:01:01.520 --> 00:01:06.600]   I'm going to try and focus then as much as possible on the ideas that we most need for
[00:01:06.600 --> 00:01:10.640]   machine learning and in particular, ideas around entropy and information theory.
[00:01:10.640 --> 00:01:11.640]   All right.
[00:01:11.640 --> 00:01:13.340]   Let's, let's dive right in, Scott.
[00:01:13.340 --> 00:01:16.720]   This first section here on representing probability distributions, this is a nice little set of
[00:01:16.720 --> 00:01:22.680]   exercises just to firm up this idea of probability mass functions and also probability density
[00:01:22.680 --> 00:01:23.680]   functions.
[00:01:23.680 --> 00:01:24.680]   There's like some text there.
[00:01:24.680 --> 00:01:27.040]   I'm going to be skipping and playing with these exercises, but I'm actually just, let's
[00:01:27.040 --> 00:01:28.240]   just skip past these.
[00:01:28.240 --> 00:01:29.240]   They're relatively straightforward.
[00:01:29.240 --> 00:01:32.720]   If you have any questions, you can post about them on our community, on the channel.
[00:01:32.720 --> 00:01:34.720]   We'll answer them, but these are pretty straightforward.
[00:01:34.720 --> 00:01:38.320]   I want to get into the more exciting and interesting stuff here with Scott.
[00:01:38.320 --> 00:01:43.440]   So go past this section on probability density functions and here, there we go.
[00:01:43.440 --> 00:01:45.080]   Surprise and machine learning loss functions.
[00:01:45.080 --> 00:01:48.840]   Scott, in working in machine learning, I would guess you've come across things like the cross
[00:01:48.840 --> 00:01:50.660]   entropy before.
[00:01:50.660 --> 00:01:51.660]   Is that correct?
[00:01:51.660 --> 00:01:52.660]   >> That's correct.
[00:01:52.660 --> 00:01:53.660]   Yeah.
[00:01:53.660 --> 00:01:55.760]   >> So is it the cross entropy or the KL divergence?
[00:01:55.760 --> 00:01:58.680]   Are those familiar ideas or unfamiliar ideas?
[00:01:58.680 --> 00:02:03.480]   >> Those are more ideas I would have seen mentioned, but certainly haven't gone deep
[00:02:03.480 --> 00:02:04.480]   into.
[00:02:04.480 --> 00:02:05.480]   >> Great.
[00:02:05.480 --> 00:02:06.480]   Yeah.
[00:02:06.480 --> 00:02:07.480]   So we'll talk about them a little bit here.
[00:02:07.480 --> 00:02:09.760]   We won't go into incredible detail about what these things are.
[00:02:09.760 --> 00:02:13.240]   I think it's a good way to get more intuition about them and why they behave the way they
[00:02:13.240 --> 00:02:16.240]   do and why we're using them in machine learning in greater detail.
[00:02:16.240 --> 00:02:20.300]   But at least for the exercises here, what we're mostly going to do is just implement
[00:02:20.300 --> 00:02:22.180]   ways to calculate these things.
[00:02:22.180 --> 00:02:24.160]   So a good way to get a little bit of intuition.
[00:02:24.160 --> 00:02:28.240]   But those links there point to blog posts and other places to learn additional stuff
[00:02:28.240 --> 00:02:32.200]   about all these pieces, the surprise, the cross entropy, the KL divergence.
[00:02:32.200 --> 00:02:33.200]   >> Okay, cool.
[00:02:33.200 --> 00:02:37.160]   >> So our first bit here is negative log probability or surprise.
[00:02:37.160 --> 00:02:41.240]   So in the lectures for this class, I talk a lot about this idea of the surprise as the
[00:02:41.240 --> 00:02:45.760]   negative logarithm of the probability, what that means, but we don't ever calculate it.
[00:02:45.760 --> 00:02:50.640]   So the short answer is that surprises are, it's very much like our intuitive idea of
[00:02:50.640 --> 00:02:54.000]   surprise when something is unexpected, the surprise is really big.
[00:02:54.000 --> 00:02:56.880]   When something's impossible, the surprise is infinite.
[00:02:56.880 --> 00:02:59.860]   When something is certain, there is no surprise at all.
[00:02:59.860 --> 00:03:03.060]   So that's the intuition for the surprise.
[00:03:03.060 --> 00:03:05.840]   But now let's write some Python code for the surprise.
[00:03:05.840 --> 00:03:11.940]   So that's this exercise here, which takes in a probability mass function as an array
[00:03:11.940 --> 00:03:16.780]   and an index into that array and returns the surprise, just like is written in that little
[00:03:16.780 --> 00:03:18.320]   block of math up there.
[00:03:18.320 --> 00:03:19.320]   >> Okay.
[00:03:19.320 --> 00:03:23.600]   And what I'm immediately a little bit surprised by is that the PMF here has function in the
[00:03:23.600 --> 00:03:25.440]   name, but it's been passed as an array.
[00:03:25.440 --> 00:03:26.440]   Is that right?
[00:03:26.440 --> 00:03:28.320]   Should I be alarmed there or is that okay?
[00:03:28.320 --> 00:03:31.280]   >> Yeah, no, you're right to at least raise alarms.
[00:03:31.280 --> 00:03:33.080]   They're called probability mass functions.
[00:03:33.080 --> 00:03:36.860]   Yeah, we've talked about the idea of thinking about arrays as functions, but this is actually
[00:03:36.860 --> 00:03:44.000]   distinct from that as well, which is an array is also a function that takes in an index
[00:03:44.000 --> 00:03:45.280]   and returns a value.
[00:03:45.280 --> 00:03:50.760]   So you give an array an index and then out comes either another array or a concrete value
[00:03:50.760 --> 00:03:52.760]   if you've given enough indices.
[00:03:52.760 --> 00:03:55.760]   And so that's the sense in which this is a function.
[00:03:55.760 --> 00:03:59.920]   This is the exercise that we skipped over there with talking about representing probability
[00:03:59.920 --> 00:04:01.520]   mass functions as arrays.
[00:04:01.520 --> 00:04:05.160]   Maybe just think of it as these are the numbers that sum to one.
[00:04:05.160 --> 00:04:08.520]   It's an array that's got the probabilities in it, and we're going to use that to calculate
[00:04:08.520 --> 00:04:09.520]   the surprise.
[00:04:09.520 --> 00:04:10.520]   >> Okay.
[00:04:10.520 --> 00:04:13.960]   So just connecting it to the lecture, am I correct in saying it's like the space that
[00:04:13.960 --> 00:04:17.480]   you showed that was like an uneven pizza shape?
[00:04:17.480 --> 00:04:21.400]   And then the index is, let's say, the location of the pepperoni.
[00:04:21.400 --> 00:04:22.800]   Am I correct saying that?
[00:04:22.800 --> 00:04:23.800]   >> Yeah, absolutely.
[00:04:23.800 --> 00:04:24.800]   It is.
[00:04:24.800 --> 00:04:27.080]   The index is exactly the location of the pepperonis.
[00:04:27.080 --> 00:04:28.080]   Yeah.
[00:04:28.080 --> 00:04:29.080]   >> Okay, sweet.
[00:04:29.080 --> 00:04:33.780]   I'm thinking that this is the answer to this first.
[00:04:33.780 --> 00:04:37.480]   So I'm indexing in and then I'm calculating the log and then I'm returning the negative
[00:04:37.480 --> 00:04:38.480]   of that.
[00:04:38.480 --> 00:04:39.480]   Yep.
[00:04:39.480 --> 00:04:40.480]   Nice.
[00:04:40.480 --> 00:04:41.480]   >> There we go.
[00:04:41.480 --> 00:04:42.480]   Yeah.
[00:04:42.480 --> 00:04:46.480]   So I think that for you calculating the entropy or the surprise is pretty straightforward.
[00:04:46.480 --> 00:04:49.560]   Our goal here is to connect these ideas to machine learning.
[00:04:49.560 --> 00:04:52.440]   And so we're going to need some other functions here.
[00:04:52.440 --> 00:04:55.580]   One that comes up in machine learning a lot is the softmax function.
[00:04:55.580 --> 00:04:58.880]   Machine learning models often need to produce some kind of probability distribution.
[00:04:58.880 --> 00:05:00.440]   So they say, what's the chance?
[00:05:00.440 --> 00:05:03.520]   Really what a machine learning model will tell you is what's the chance that the label
[00:05:03.520 --> 00:05:04.760]   of this image is dog?
[00:05:04.760 --> 00:05:06.880]   What's the chance that the label of this image is cat?
[00:05:06.880 --> 00:05:08.560]   Not just what is the label?
[00:05:08.560 --> 00:05:10.300]   Because there's some uncertainty there.
[00:05:10.300 --> 00:05:12.640]   Maybe different people would label it differently.
[00:05:12.640 --> 00:05:15.580]   Or maybe it's hard to tell whether it's a dog or a cat.
[00:05:15.580 --> 00:05:20.320]   Another example would be if you have a model that generates sentences like the famous GPT-3
[00:05:20.320 --> 00:05:24.760]   model, it produces a probability distribution over what's going to come next.
[00:05:24.760 --> 00:05:28.720]   And then when we're generating sentences, we actually draw randomly according to that
[00:05:28.720 --> 00:05:29.720]   distribution.
[00:05:29.720 --> 00:05:33.800]   So in order to do this, we need to output a probability distribution at the end.
[00:05:33.800 --> 00:05:38.480]   So we need to output an array whose elements sum to one as our output there.
[00:05:38.480 --> 00:05:43.560]   And the normal way that this is done in machine learning is this softmax function, which takes
[00:05:43.560 --> 00:05:48.200]   an array of whatever numbers you want and turns it into an array full of values that
[00:05:48.200 --> 00:05:49.840]   are non-negative and that sum to one.
[00:05:49.840 --> 00:05:54.680]   So that can be thought of as a probability distribution whose indices tell you the probabilities
[00:05:54.680 --> 00:05:55.680]   of different events.
[00:05:55.680 --> 00:05:59.160]   Like the probability it's a dog, the probability it's a cat, the probability that the next
[00:05:59.160 --> 00:06:01.520]   word in the sentence is world after hello.
[00:06:01.520 --> 00:06:03.860]   The formula for softmax is that right there.
[00:06:03.860 --> 00:06:07.120]   We use the exponential function to make things positive only.
[00:06:07.120 --> 00:06:10.400]   And then we divide by the sum to make it so that they add up to one.
[00:06:10.400 --> 00:06:12.240]   So those are the two steps on the top.
[00:06:12.240 --> 00:06:14.360]   We turn it to positive only on the bottom.
[00:06:14.360 --> 00:06:16.920]   We divide by that sum in that formula there.
[00:06:16.920 --> 00:06:17.920]   All right.
[00:06:17.920 --> 00:06:19.840]   So now let's implement this softmax function.
[00:06:19.840 --> 00:06:22.980]   Let's take that formula that's up there and compute the softmax.
[00:06:22.980 --> 00:06:26.640]   So I want to sum over the X i.
[00:06:26.640 --> 00:06:29.080]   Is it X?
[00:06:29.080 --> 00:06:30.520]   So we're getting these one dimensional arrays.
[00:06:30.520 --> 00:06:33.840]   So you don't need to worry about which dimension you're summing over or anything like that.
[00:06:33.840 --> 00:06:39.200]   We're just getting an individual array here and the exponents on the top.
[00:06:39.200 --> 00:06:44.600]   And then NumPy does broadcasting, which means the bottom part of that returns a single number.
[00:06:44.600 --> 00:06:49.240]   That thing on the bottom is like a normalization constant or just a single number to divide
[00:06:49.240 --> 00:06:53.080]   by and we'll divide every element of the array by that same number.
[00:06:53.080 --> 00:06:54.080]   Okay.
[00:06:54.080 --> 00:06:59.280]   So the reason why we're calculating these surprises is because the surprise connects
[00:06:59.280 --> 00:07:01.720]   us to ideas from information theory.
[00:07:01.720 --> 00:07:05.440]   So the idea that you may have come across as the idea of the entropy of a random variable,
[00:07:05.440 --> 00:07:11.200]   the entropy of some source of randomness that in terms of its probability mass function
[00:07:11.200 --> 00:07:17.600]   and the surprise is just defined as take the probability mass function, multiply it by
[00:07:17.600 --> 00:07:21.720]   the surprise for each value of the surprise, and then sum those up.
[00:07:21.720 --> 00:07:25.680]   This is also called the expected value of the surprise.
[00:07:25.680 --> 00:07:30.120]   So the expected value of a random variable is the average value of that variable.
[00:07:30.120 --> 00:07:33.400]   If you were to take lots and lots of samples and calculate the mean, it should be very
[00:07:33.400 --> 00:07:35.560]   close to the expected value.
[00:07:35.560 --> 00:07:37.480]   Here we're taking the average of the surprise.
[00:07:37.480 --> 00:07:40.220]   So thinking of the surprise as a random variable.
[00:07:40.220 --> 00:07:43.980]   So on any given draw, the random variable, how surprising was that particular draw?
[00:07:43.980 --> 00:07:46.640]   How surprising was it that this image was labeled dog?
[00:07:46.640 --> 00:07:51.880]   How surprising was it that the next word in this sentence was world after hello?
[00:07:51.880 --> 00:07:56.160]   This entropy function has a lot of uses in information theory in lots of different places.
[00:07:56.160 --> 00:07:58.680]   It's how people quantify compression.
[00:07:58.680 --> 00:08:01.600]   Here in machine learning, it's showing up in our loss functions.
[00:08:01.600 --> 00:08:02.600]   Okay.
[00:08:02.600 --> 00:08:04.280]   Here's my formula.
[00:08:04.280 --> 00:08:08.320]   It's asking me to implement a function to compute the entropy of a PMF represented by
[00:08:08.320 --> 00:08:09.320]   an array.
[00:08:09.320 --> 00:08:12.680]   I assume that means I'm supposed to be implementing this function above.
[00:08:12.680 --> 00:08:13.680]   Is that correct?
[00:08:13.680 --> 00:08:14.680]   Yep.
[00:08:14.680 --> 00:08:15.680]   Okay.
[00:08:15.680 --> 00:08:21.520]   I'm going to do mp.sum over the probabilities and then I'm going to...
[00:08:21.520 --> 00:08:24.400]   This is maybe something surprising about the way that that notation works.
[00:08:24.400 --> 00:08:31.240]   So actually the sum is over I and it gets all of the stuff to the right of it.
[00:08:31.240 --> 00:08:35.040]   So it's not the sum over the probability times the negative log.
[00:08:35.040 --> 00:08:36.040]   Okay.
[00:08:36.040 --> 00:08:37.040]   Makes sense.
[00:08:37.040 --> 00:08:38.720]   So sorry, this is like in brackets or something.
[00:08:38.720 --> 00:08:39.720]   Yeah.
[00:08:39.720 --> 00:08:40.720]   Okay.
[00:08:40.720 --> 00:08:41.720]   That makes sense.
[00:08:41.720 --> 00:08:43.440]   And then I'm just going to do mp.log here and I'll figure out that.
[00:08:43.440 --> 00:08:44.440]   Yeah.
[00:08:44.440 --> 00:08:46.700]   The dot there is multiplication.
[00:08:46.700 --> 00:08:49.840]   So can I do mp.dot?
[00:08:49.840 --> 00:08:51.240]   How does this look?
[00:08:51.240 --> 00:08:52.240]   Almost.
[00:08:52.240 --> 00:08:54.640]   So the dot there represents just regular multiplication.
[00:08:54.640 --> 00:08:59.160]   It just says like take these two individual scalar numbers and multiply them together.
[00:08:59.160 --> 00:09:00.160]   Oh, okay.
[00:09:00.160 --> 00:09:02.560]   If you've got that sum there, really what you...
[00:09:02.560 --> 00:09:03.560]   Yeah, exactly.
[00:09:03.560 --> 00:09:07.000]   So that's a direct translation of what I wrote up there into code.
[00:09:07.000 --> 00:09:08.920]   Let's check to make sure we translated it right.
[00:09:08.920 --> 00:09:09.920]   Nice.
[00:09:09.920 --> 00:09:10.920]   Awesome.
[00:09:10.920 --> 00:09:11.920]   Okay.
[00:09:11.920 --> 00:09:12.920]   This is a great, this is a perfect answer.
[00:09:12.920 --> 00:09:16.440]   This is probably the way most implementations of entropy actually work, but there's actually
[00:09:16.440 --> 00:09:19.200]   a cool little fact here that I want to bring up.
[00:09:19.200 --> 00:09:22.960]   If you look at that formula again, what we're doing is we're multiplying a bunch of numbers
[00:09:22.960 --> 00:09:25.360]   together and summing up the results.
[00:09:25.360 --> 00:09:30.000]   We've got the numbers in the probability array, we've got numbers in an array of surprises,
[00:09:30.000 --> 00:09:32.340]   and we're multiplying the entries and summing them up.
[00:09:32.340 --> 00:09:33.780]   That maybe sounds familiar.
[00:09:33.780 --> 00:09:37.640]   Going back to the linear algebra section of the course, we were really often doing things
[00:09:37.640 --> 00:09:41.480]   where we would multiply entries together and sum up the results, right?
[00:09:41.480 --> 00:09:42.480]   Yes.
[00:09:42.480 --> 00:09:46.520]   If we had the entries in one vector and the entries in another vector, and we wanted to
[00:09:46.520 --> 00:09:51.560]   multiply all of them together and sum up the results in a single operation, there was one
[00:09:51.560 --> 00:09:56.640]   operation that we used to do all of that in a single step, which was the dot product.
[00:09:56.640 --> 00:09:57.640]   The dot product.
[00:09:57.640 --> 00:09:58.640]   Okay.
[00:09:58.640 --> 00:10:01.080]   I just got that before you told me.
[00:10:01.080 --> 00:10:02.080]   Okay.
[00:10:02.080 --> 00:10:05.400]   So that's the dot of np.log.
[00:10:05.400 --> 00:10:07.400]   Is that going to do it?
[00:10:07.400 --> 00:10:08.400]   Okay.
[00:10:08.400 --> 00:10:09.400]   Nice.
[00:10:09.400 --> 00:10:10.400]   That's a cool trick.
[00:10:10.400 --> 00:10:11.400]   Yeah.
[00:10:11.400 --> 00:10:13.220]   Maybe this isn't the best way to implement the entropy.
[00:10:13.220 --> 00:10:15.680]   There's maybe more numerically stable ways to do it.
[00:10:15.680 --> 00:10:20.380]   There's some caveats here, but the core idea is that there actually is a really close relationship
[00:10:20.380 --> 00:10:23.780]   between these expected values and ideas in linear algebra.
[00:10:23.780 --> 00:10:27.620]   These are the expected value is a linear operation.
[00:10:27.620 --> 00:10:31.220]   If there's one core idea that actually unites all three of the branches of math that we're
[00:10:31.220 --> 00:10:34.140]   talking about in this course is that linearity is important.
[00:10:34.140 --> 00:10:35.680]   Linearity is central to linear algebra.
[00:10:35.680 --> 00:10:39.080]   It was central in calculus because we were doing these linear approximations.
[00:10:39.080 --> 00:10:42.000]   And in fact, even the limit is a linear operation.
[00:10:42.000 --> 00:10:43.380]   And now we have these expected values.
[00:10:43.380 --> 00:10:45.460]   They're also linear operations.
[00:10:45.460 --> 00:10:50.020]   So I got some advice once from a physicist friend that if I wanted to understand mathematics,
[00:10:50.020 --> 00:10:53.500]   I should take a long bubble bath and meditate on linearity.
[00:10:53.500 --> 00:10:54.940]   And that turned out to be really great advice.
[00:10:54.940 --> 00:10:55.940]   Nice.
[00:10:55.940 --> 00:10:56.940]   I need to try that.
[00:10:56.940 --> 00:10:57.940]   Yeah.
[00:10:57.940 --> 00:10:58.940]   Let me know how that goes.
[00:10:58.940 --> 00:11:03.500]   The entropy is something that tells us the minimum number of bits required to store the
[00:11:03.500 --> 00:11:04.940]   values of a signal.
[00:11:04.940 --> 00:11:08.620]   If something varies randomly, entropy tells us no matter how good you are at compression,
[00:11:08.620 --> 00:11:11.820]   you're going to need at least this many bits if you want to represent it perfectly.
[00:11:11.820 --> 00:11:15.980]   But in order to calculate it and in order to know it, we need to know the true distribution
[00:11:15.980 --> 00:11:16.980]   of these random values.
[00:11:16.980 --> 00:11:19.980]   We need to know exactly how likely it is that each event will happen.
[00:11:19.980 --> 00:11:22.420]   And that's not something we generally have access to.
[00:11:22.420 --> 00:11:27.740]   We can train models that try and approximate it, or we can develop laws of physics that
[00:11:27.740 --> 00:11:29.600]   try and help us compute these things.
[00:11:29.600 --> 00:11:32.440]   But we're always, there's some error, there's some uncertainty.
[00:11:32.440 --> 00:11:35.620]   And so we don't usually actually end up getting entropies.
[00:11:35.620 --> 00:11:39.180]   We generally have something that's called a cross entropy.
[00:11:39.180 --> 00:11:43.900]   When you do a cross entropy, you calculate your surprises according to one distribution,
[00:11:43.900 --> 00:11:48.780]   but then you average them or you weight them according to a different one.
[00:11:48.780 --> 00:11:54.440]   So the idea here is my model tells me I should be this surprised by the next output in this
[00:11:54.440 --> 00:11:57.020]   sentence being the word world after hello.
[00:11:57.020 --> 00:11:59.460]   It tells me I should be two bits surprised.
[00:11:59.460 --> 00:12:04.300]   And then when I come across that in a data set that adds two bits of surprise to my average.
[00:12:04.300 --> 00:12:09.860]   And then I loop over like a gigantic data set and I can get a guess at the cross entropy
[00:12:09.860 --> 00:12:13.980]   of my model, getting the true entropy of the English language much harder than estimating
[00:12:13.980 --> 00:12:16.020]   the cross entropy of a particular model.
[00:12:16.020 --> 00:12:17.020]   Okay.
[00:12:17.020 --> 00:12:23.540]   So, sorry, just so I understand, this is like, would be like if you had access to the real
[00:12:23.540 --> 00:12:28.860]   distribution of whatever you're trying to figure out, and this is like trying to approximate
[00:12:28.860 --> 00:12:30.820]   it with an estimation each time.
[00:12:30.820 --> 00:12:32.380]   Am I correct in saying that?
[00:12:32.380 --> 00:12:33.380]   Yeah.
[00:12:33.380 --> 00:12:35.100]   So true here is our estimation.
[00:12:35.100 --> 00:12:36.900]   P is still the truth.
[00:12:36.900 --> 00:12:41.460]   And so you can estimate the cross entropy because you know how surprised you are and
[00:12:41.460 --> 00:12:45.860]   you can draw samples according to P. So you can estimate the cross entropy.
[00:12:45.860 --> 00:12:49.180]   With the entropy, it's a little bit harder to estimate because you also don't know how
[00:12:49.180 --> 00:12:52.920]   surprised somebody who knows the true distribution would be.
[00:12:52.920 --> 00:12:57.020]   The right component of our dot product, that surprise, you also don't know.
[00:12:57.020 --> 00:12:59.340]   So this is at least one step more known.
[00:12:59.340 --> 00:13:00.340]   Okay.
[00:13:00.340 --> 00:13:01.340]   That's a good point.
[00:13:01.340 --> 00:13:02.340]   Great question.
[00:13:02.340 --> 00:13:03.340]   Okay.
[00:13:03.340 --> 00:13:04.340]   So we've got our cross entropy here.
[00:13:04.340 --> 00:13:08.900]   And if you have P and Q as explicit values, it's even simpler to calculate this.
[00:13:08.900 --> 00:13:09.900]   Cool.
[00:13:09.900 --> 00:13:15.380]   I'm going to use your dot product trick and I'm going to do P and then it's the MP.log
[00:13:15.380 --> 00:13:16.740]   of the Q.
[00:13:16.740 --> 00:13:17.740]   Exactly.
[00:13:17.740 --> 00:13:18.740]   All right.
[00:13:18.740 --> 00:13:20.860]   And the grader agrees, which is good news.
[00:13:20.860 --> 00:13:22.420]   That's our cross entropy.
[00:13:22.420 --> 00:13:27.260]   It's calculated in very similar way to calculating the entropy if you've got these distributions.
[00:13:27.260 --> 00:13:30.260]   So the cross entropy is what shows up in your actual neural network losses.
[00:13:30.260 --> 00:13:34.280]   So we're getting much, much closer to actually having a neural network loss here.
[00:13:34.280 --> 00:13:37.320]   You won't see it calculated with exactly this formula.
[00:13:37.320 --> 00:13:41.280]   You know, if you're working with, say, data with labels, the cross entropy, people use
[00:13:41.280 --> 00:13:42.800]   tricks to calculate it.
[00:13:42.800 --> 00:13:47.440]   We'll also see that if it's something like a regression problem, people replace the cross
[00:13:47.440 --> 00:13:48.440]   entropy formula.
[00:13:48.440 --> 00:13:52.760]   It's, you know, if you actually calculate this out, you'll find it turns into the squared
[00:13:52.760 --> 00:13:53.760]   error.
[00:13:53.760 --> 00:13:54.760]   It turns into the absolute error.
[00:13:54.760 --> 00:13:56.460]   It turns into all these other loss functions.
[00:13:56.460 --> 00:14:01.140]   But this is the sort of like error loss function underneath all these other ones that gives
[00:14:01.140 --> 00:14:05.580]   us the specific things that we compute for one machine learning model or another.
[00:14:05.580 --> 00:14:06.580]   Cool.
[00:14:06.580 --> 00:14:11.680]   So one last bit here of additional entropy calculating things is the KL divergence.
[00:14:11.680 --> 00:14:16.860]   The cross entropy gives us one way to measure how different two probability mass functions
[00:14:16.860 --> 00:14:17.860]   are.
[00:14:17.860 --> 00:14:22.820]   Like if Q is really far away from P, I'll get a larger cross entropy the more different
[00:14:22.820 --> 00:14:23.980]   Q is from P.
[00:14:23.980 --> 00:14:26.780]   But it's not quite a distance.
[00:14:26.780 --> 00:14:31.340]   One thing that we like about distances is that a point should have a distance zero from
[00:14:31.340 --> 00:14:32.340]   itself.
[00:14:32.340 --> 00:14:35.700]   So the distance from one point to the same point should be zero.
[00:14:35.700 --> 00:14:39.740]   Whereas if I take the cross entropy of P with itself, I'll get the entropy.
[00:14:39.740 --> 00:14:42.020]   That would just be P times negative log P.
[00:14:42.020 --> 00:14:46.660]   So instead of using the cross entropy, it's often convenient to use a different quantity
[00:14:46.660 --> 00:14:50.800]   to measure the difference between two probability distributions, which is the KL divergence.
[00:14:50.800 --> 00:14:54.720]   So I gave the formula there, which is the sort of like definition formula that people
[00:14:54.720 --> 00:14:55.720]   often use.
[00:14:55.720 --> 00:15:00.400]   If you rearrange this really quickly, just take that logarithm and split it apart and
[00:15:00.400 --> 00:15:05.760]   turn this into two sums, you'll see that turns into divergence is equal to cross entropy
[00:15:05.760 --> 00:15:06.760]   minus entropy.
[00:15:06.760 --> 00:15:08.800]   And this is the formula I prefer to think of.
[00:15:08.800 --> 00:15:12.200]   If somebody asked me a question like, oh, does the KL divergence do this?
[00:15:12.200 --> 00:15:13.960]   Or what's the definition of the KL divergence?
[00:15:13.960 --> 00:15:17.360]   I immediately think, okay, is the difference between the cross entropy and the entropy.
[00:15:17.360 --> 00:15:20.440]   It's the way we normalize the cross entropy down to zero.
[00:15:20.440 --> 00:15:22.280]   And then maybe I like unpack that definition.
[00:15:22.280 --> 00:15:28.040]   I unzip it into the full formula or into this more compact version that's up there.
[00:15:28.040 --> 00:15:31.680]   So that gives you good intuition, I think, about what this thing does.
[00:15:31.680 --> 00:15:36.080]   Depending on the exact application, the loss function you're using might be the cross entropy
[00:15:36.080 --> 00:15:37.840]   or might be the KL divergence.
[00:15:37.840 --> 00:15:41.280]   They only differ by this entropy term here.
[00:15:41.280 --> 00:15:45.200]   And depending on the derivatives you take, maybe that term has derivative zero, so it
[00:15:45.200 --> 00:15:46.200]   vanishes.
[00:15:46.200 --> 00:15:49.040]   But sometimes it's useful to think of it as a KL divergence.
[00:15:49.040 --> 00:15:51.720]   Sometimes it's useful to think of your losses as coming from the cross entropy.
[00:15:51.720 --> 00:15:52.720]   Okay.
[00:15:52.720 --> 00:15:57.960]   I guess a quick question is, do you have any rules of thumb that is like when you would
[00:15:57.960 --> 00:15:59.920]   see one versus the other?
[00:15:59.920 --> 00:16:04.560]   Yeah, I guess if you're doing things that are closer to generative modeling, you're
[00:16:04.560 --> 00:16:07.620]   more likely to actually care about that entropy term.
[00:16:07.620 --> 00:16:12.560]   So with things like variational autoencoders, this will still be in there.
[00:16:12.560 --> 00:16:16.880]   There's a different kind of divergence in GANs, and that term becomes really important.
[00:16:16.880 --> 00:16:20.400]   But if you're doing something that's closer to supervised learning, where all you're trying
[00:16:20.400 --> 00:16:25.120]   to do is match your particular outputs, the particular things you've been given, that
[00:16:25.120 --> 00:16:27.320]   will just show you that cross entropy term.
[00:16:27.320 --> 00:16:28.640]   Okay, cool.
[00:16:28.640 --> 00:16:31.920]   So it's kind of a divide between discriminative and generative modeling.
[00:16:31.920 --> 00:16:34.680]   That's the more old school term for those two things.
[00:16:34.680 --> 00:16:37.120]   But yeah, you'll always care about the cross entropy.
[00:16:37.120 --> 00:16:40.440]   You will sometimes care also about that entropy term.
[00:16:40.440 --> 00:16:41.440]   Cool.
[00:16:41.440 --> 00:16:42.440]   Thanks.
[00:16:42.440 --> 00:16:45.000]   So now let's do a quick function to compute it.
[00:16:45.000 --> 00:16:46.000]   Okay.
[00:16:46.000 --> 00:16:50.240]   So I'm going to not use any functions like this representation.
[00:16:50.240 --> 00:16:52.960]   I'm going to just calculate it directly.
[00:16:52.960 --> 00:16:58.400]   And I'm going to use your dot product trick again, and I'm going to pass in the P, and
[00:16:58.400 --> 00:17:06.120]   then it's going to be NP dot log or negative NP dot log of Q divided by P. And I'm now
[00:17:06.120 --> 00:17:09.440]   wondering whether NumPy is going to let me do that.
[00:17:09.440 --> 00:17:13.760]   Is that going to do a element wise divide, or I guess we could have a check and see if
[00:17:13.760 --> 00:17:14.760]   it's...
[00:17:14.760 --> 00:17:15.760]   We can have a check for sure.
[00:17:15.760 --> 00:17:18.920]   I think you're right that it is that element wise divide that you want.
[00:17:18.920 --> 00:17:19.920]   Yeah.
[00:17:19.920 --> 00:17:20.920]   Awesome.
[00:17:20.920 --> 00:17:25.600]   In general, when you use your normal math expressions, divide times plus, NumPy interprets
[00:17:25.600 --> 00:17:29.680]   that as I want to take all the elements of this array and like add them together.
[00:17:29.680 --> 00:17:32.960]   Or maybe I want to add the same number to every element in this array.
[00:17:32.960 --> 00:17:37.440]   And it's only if you use the at, the special at operator for matrix multiplication, or
[00:17:37.440 --> 00:17:42.440]   you use a NumPy function that's defined only for arrays like dot that you start getting
[00:17:42.440 --> 00:17:46.320]   things that, you know, change the shape of the array and don't just treat it as like
[00:17:46.320 --> 00:17:50.000]   a for loop over applying this to the elements of the array.
[00:17:50.000 --> 00:17:51.000]   Okay, cool.
[00:17:51.000 --> 00:17:52.000]   That's good to know.
[00:17:52.000 --> 00:17:56.840]   Now we're ready to put this together into the definition of a typical loss function
[00:17:56.840 --> 00:17:58.000]   in machine learning.
[00:17:58.000 --> 00:18:01.320]   ML models often output probability distributions.
[00:18:01.320 --> 00:18:06.400]   And the goal of training from a galactic view is to align those probability distributions
[00:18:06.400 --> 00:18:10.200]   that come out of our model when it's fed with inputs with what we believe to be the true
[00:18:10.200 --> 00:18:13.320]   probability distribution on the basis of some samples.
[00:18:13.320 --> 00:18:15.760]   So we have samples of inputs and labels.
[00:18:15.760 --> 00:18:19.200]   And that gives us like a kind of estimate of the probability distribution.
[00:18:19.200 --> 00:18:22.480]   And we want our models output distribution to match that.
[00:18:22.480 --> 00:18:25.960]   Another way of thinking of it is to say we want our model to be as as little surprise
[00:18:25.960 --> 00:18:29.520]   as possible when it encounters the data that it sees.
[00:18:29.520 --> 00:18:35.520]   The way we do that is by the like cross entropy and KL divergence by minimizing those things
[00:18:35.520 --> 00:18:39.000]   because those calculate a form of surprise when presented with data.
[00:18:39.000 --> 00:18:43.960]   And so we tend to do is compute the cross entropy based on our samples on the models
[00:18:43.960 --> 00:18:48.840]   output for data that's labeled with specific numbers, we'll actually calculate this fairly
[00:18:48.840 --> 00:18:50.040]   explicitly.
[00:18:50.040 --> 00:18:55.760]   And so we'll calculate a cross entropy of the labels on the models output.
[00:18:55.760 --> 00:18:59.920]   So the one thing that stops us from directly just applying cross entropy to model output
[00:18:59.920 --> 00:19:05.440]   and label is that our models generally output not directly a probability mass function,
[00:19:05.440 --> 00:19:07.880]   but they output just arbitrary numbers, right?
[00:19:07.880 --> 00:19:11.440]   If I go through a linear layer, I've got I don't have numbers that necessarily sum to
[00:19:11.440 --> 00:19:12.440]   one.
[00:19:12.440 --> 00:19:17.160]   So the usual is to sort of merge these two things together, softmax and cross entropy
[00:19:17.160 --> 00:19:21.840]   into one step, just so that we can connect as close as possible with PyTorch and Keras
[00:19:21.840 --> 00:19:23.840]   and other approaches for machine learning.
[00:19:23.840 --> 00:19:27.960]   I wanted to explicitly write out this softmax cross entropy combo here.
[00:19:27.960 --> 00:19:33.160]   And am I correct in saying that is usually referred to as the negative log likelihood
[00:19:33.160 --> 00:19:36.200]   loss in some of the, or at least in PyTorch?
[00:19:36.200 --> 00:19:40.500]   Yeah, this combination is called the negative log likelihood loss.
[00:19:40.500 --> 00:19:43.480]   The name has changed a couple of times, I think.
[00:19:43.480 --> 00:19:46.440]   And I think there's differences in convention between PyTorch and Keras.
[00:19:46.440 --> 00:19:51.600]   But yeah, this combo goes by that name in PyTorch and negative log likelihood is another
[00:19:51.600 --> 00:19:53.680]   term, another way of saying surprise.
[00:19:53.680 --> 00:19:56.760]   So if I were writing PyTorch, I would have tried to call it the surprise loss.
[00:19:56.760 --> 00:19:59.940]   That negative log likelihood terminology is a little older, a little more standard than
[00:19:59.940 --> 00:20:00.940]   calling it the surprise.
[00:20:00.940 --> 00:20:01.940]   Okay.
[00:20:01.940 --> 00:20:03.360]   So I'm going to ask to implement this.
[00:20:03.360 --> 00:20:04.600]   You were saying there's some tricks.
[00:20:04.600 --> 00:20:08.000]   I've come across one of those tricks, but I'm not going to go for it now.
[00:20:08.000 --> 00:20:14.080]   The trick I've heard is you do the log sum exp trick where you do one of the operations
[00:20:14.080 --> 00:20:17.000]   first and then the other, and it works out more efficient.
[00:20:17.000 --> 00:20:18.280]   Am I correct in saying that?
[00:20:18.280 --> 00:20:19.280]   Yeah.
[00:20:19.280 --> 00:20:23.840]   There's this trick involving when you do the logarithm and the main thing there isn't actually
[00:20:23.840 --> 00:20:28.760]   speed it's numerical stability and being able to calculate good gradients.
[00:20:28.760 --> 00:20:32.580]   But let's rather than doing that, what I want to emphasize here is that this thing here
[00:20:32.580 --> 00:20:36.400]   is just a combination of two things that we've already done.
[00:20:36.400 --> 00:20:41.020]   Our softmax function, which we wrote and our cross entropy function, which we also wrote.
[00:20:41.020 --> 00:20:42.020]   Cool.
[00:20:42.020 --> 00:20:45.020]   I'm going to do this first and that'll give me my probabilities.
[00:20:45.020 --> 00:20:47.060]   I might actually just give them a name.
[00:20:47.060 --> 00:20:52.220]   And then with that, I'm going to do my negative log likelihood or sorry, my cross entropy,
[00:20:52.220 --> 00:20:59.060]   which I'm going to do with again, your trick, which will be probs np.log of probs.
[00:20:59.060 --> 00:21:00.560]   Oh no.
[00:21:00.560 --> 00:21:02.780]   So I haven't passed np.
[00:21:02.780 --> 00:21:05.980]   So applies that to the logics increase.
[00:21:05.980 --> 00:21:06.980]   Okay.
[00:21:06.980 --> 00:21:08.880]   So it'll be between this and this.
[00:21:08.880 --> 00:21:11.900]   Let's go ahead and run this and see what the grader says.
[00:21:11.900 --> 00:21:16.340]   Oh, you wrote np.softmax, but we wrote the softmax function, right?
[00:21:16.340 --> 00:21:17.340]   Oh, okay.
[00:21:17.340 --> 00:21:18.340]   Yes.
[00:21:18.340 --> 00:21:19.340]   True.
[00:21:19.340 --> 00:21:24.020]   I thought that this existed as a function, but we wrote the softmax.
[00:21:24.020 --> 00:21:26.780]   So this should do it or at least we should get a new error.
[00:21:26.780 --> 00:21:27.780]   It'll run.
[00:21:27.780 --> 00:21:28.780]   Failed again.
[00:21:28.780 --> 00:21:31.320]   And this says, is it close?
[00:21:31.320 --> 00:21:32.460]   So no.
[00:21:32.460 --> 00:21:35.140]   So it didn't give me a nice error message.
[00:21:35.140 --> 00:21:36.540]   Oh, check the argument order.
[00:21:36.540 --> 00:21:37.540]   Okay.
[00:21:37.540 --> 00:21:39.940]   It did actually give me a nice error message.
[00:21:39.940 --> 00:21:46.140]   So in that case, maybe I want to have done p and here and probs here.
[00:21:46.140 --> 00:21:47.140]   Yeah.
[00:21:47.140 --> 00:21:48.140]   Great.
[00:21:48.140 --> 00:21:49.140]   We've placated our testing suite.
[00:21:49.140 --> 00:21:53.540]   So the explanation for why that should be the case is that the model probabilities are
[00:21:53.540 --> 00:21:54.980]   what give us the surprise.
[00:21:54.980 --> 00:21:57.820]   So what we're trying to calculate is how surprised is our model.
[00:21:57.820 --> 00:21:59.860]   So that should be inside the logarithm.
[00:21:59.860 --> 00:22:05.620]   Whereas in order to get the average, like on average, how often we'd be surprised if
[00:22:05.620 --> 00:22:10.220]   we did this over and over again, that's what we need the true distribution for this p.
[00:22:10.220 --> 00:22:14.420]   That's why we have probabilities from the model inside the log, probabilities from the
[00:22:14.420 --> 00:22:16.260]   labels not inside the log.
[00:22:16.260 --> 00:22:17.260]   Okay.
[00:22:17.260 --> 00:22:21.140]   So the one last thing I would say is we've now re-implemented the cross entropy in a
[00:22:21.140 --> 00:22:22.420]   second place.
[00:22:22.420 --> 00:22:26.820]   And my preference would be to just reuse the definition of the cross entropy here.
[00:22:26.820 --> 00:22:29.860]   So that if we later decide we want to make our cross entropy function better in some
[00:22:29.860 --> 00:22:33.380]   way, we don't have to make that change in multiple places.
[00:22:33.380 --> 00:22:34.380]   Okay.
[00:22:34.380 --> 00:22:36.300]   Now that we're here, this isn't too bad.
[00:22:36.300 --> 00:22:37.980]   Oh, actually no, I did it wrong though.
[00:22:37.980 --> 00:22:40.460]   I should have done this.
[00:22:40.460 --> 00:22:41.700]   Yeah.
[00:22:41.700 --> 00:22:44.500]   I do like the instinct to like pull out variables and name them.
[00:22:44.500 --> 00:22:48.340]   It can make it more explicit what you're doing when somebody is reading your code later.
[00:22:48.340 --> 00:22:52.220]   I think that's more important than saving on lines or saving on space.
[00:22:52.220 --> 00:22:53.220]   Computers are big.
[00:22:53.220 --> 00:22:56.100]   They can hold a lot of code, but it's fairly clear here.
[00:22:56.100 --> 00:23:00.500]   You've just like translated the doc string into code, which is also nice and readable.
[00:23:00.500 --> 00:23:01.500]   So either way is fine.
[00:23:01.500 --> 00:23:02.500]   Cool.
[00:23:02.500 --> 00:23:03.500]   All right.
[00:23:03.500 --> 00:23:07.940]   So to close out here, I want to put all of the ideas almost in this class together to
[00:23:07.940 --> 00:23:12.100]   sort of like glimpse how all of this comes together in a machine learning pipeline, our
[00:23:12.100 --> 00:23:15.660]   linear algebra, our calculus, and our ideas from probability.
[00:23:15.660 --> 00:23:22.740]   What we're going to do is first we're going to see how we go from cross entropy to a different
[00:23:22.740 --> 00:23:25.700]   loss function that's maybe more familiar, the squared error.
[00:23:25.700 --> 00:23:30.700]   And then we're going to see that taking derivatives and doing gradient descent with these two
[00:23:30.700 --> 00:23:32.860]   different approaches gives us the same result.
[00:23:32.860 --> 00:23:33.860]   Okay.
[00:23:33.860 --> 00:23:37.940]   And then we'll have seen a derivative which calculates a linear approximation to a loss
[00:23:37.940 --> 00:23:42.180]   function given by a surprise used to train a simple ML model.
[00:23:42.180 --> 00:23:47.040]   The connection here, the way that we go from broad ideas about probability distributions
[00:23:47.040 --> 00:23:51.700]   and their cross entropies to a concrete specific loss function is through the Gaussian distribution,
[00:23:51.700 --> 00:23:55.220]   the normal distribution that we talked about in the lecture.
[00:23:55.220 --> 00:23:59.380]   So when you take the Gaussian probability distribution, the probability density function
[00:23:59.380 --> 00:24:04.420]   that we have here, if we take the negative logarithm of it, it takes all that stuff that's
[00:24:04.420 --> 00:24:09.380]   up inside the exponent and pulls it down and turns it into this kind of simpler looking
[00:24:09.380 --> 00:24:10.460]   formula here.
[00:24:10.460 --> 00:24:14.780]   So we just have the difference between X and the mean value of the Gaussian distribution
[00:24:14.780 --> 00:24:15.860]   squared.
[00:24:15.860 --> 00:24:17.420]   That gives us the surprise.
[00:24:17.420 --> 00:24:18.420]   And this thing.
[00:24:18.420 --> 00:24:19.420]   Yes.
[00:24:19.420 --> 00:24:23.200]   Then we have, there's like a normalization constant there that makes sure that the probability
[00:24:23.200 --> 00:24:26.900]   density function integrates to one, that the probabilities are normalized.
[00:24:26.900 --> 00:24:28.140]   So we've got that in there.
[00:24:28.140 --> 00:24:32.700]   But then the core thing that changes as we change X or we change mu is that thing on
[00:24:32.700 --> 00:24:33.700]   the left.
[00:24:33.700 --> 00:24:35.460]   It's just a squared error.
[00:24:35.460 --> 00:24:39.220]   The neat thing about Gaussians is that they turn a hard problem of like how surprised
[00:24:39.220 --> 00:24:45.020]   am I by this input into a simple problem, which is how far away is this value from another
[00:24:45.020 --> 00:24:48.020]   value, which is nice, simple function, easy to compute.
[00:24:48.020 --> 00:24:49.020]   Cool.
[00:24:49.020 --> 00:24:53.100]   We're going to implement this as a function, the Gaussian surprise that computes this value
[00:24:53.100 --> 00:24:57.160]   for a mean parameter mu and an array of sample values data.
[00:24:57.160 --> 00:25:01.220]   So data is our X here and mu is that mean parameter.
[00:25:01.220 --> 00:25:05.260]   So to write this one, I think this one is one that does really benefit actually from
[00:25:05.260 --> 00:25:09.340]   naming each piece and then adding, combining the like named pieces together.
[00:25:09.340 --> 00:25:10.340]   Okay.
[00:25:10.340 --> 00:25:16.340]   So data here is my X and mu is this input.
[00:25:16.340 --> 00:25:17.340]   Okay.
[00:25:17.340 --> 00:25:24.060]   And yeah, I'm seeing Z here, but there's no normalization constant being passed in.
[00:25:24.060 --> 00:25:27.460]   Is this something that I'm going to have to figure out?
[00:25:27.460 --> 00:25:28.460]   Yeah.
[00:25:28.460 --> 00:25:32.740]   So I would recommend writing, there's some benefit to actually like writing code backwards
[00:25:32.740 --> 00:25:36.740]   rather than writing code the way the computer experiences it, which is, you know, a series
[00:25:36.740 --> 00:25:38.980]   of instructions that build up to something.
[00:25:38.980 --> 00:25:42.700]   Start with the code that you want to write to close this thing out, which is probably
[00:25:42.700 --> 00:25:46.740]   like a direct translation of that line of code up there.
[00:25:46.740 --> 00:25:49.660]   And then look at that line of code and be like, okay, why doesn't this run?
[00:25:49.660 --> 00:25:52.700]   To start off, we'll write something that maybe doesn't run.
[00:25:52.700 --> 00:25:54.740]   Data minus mu.
[00:25:54.740 --> 00:25:56.620]   And then I'm going to do that.
[00:25:56.620 --> 00:25:57.620]   Yep.
[00:25:57.620 --> 00:26:03.300]   And then I will just ignore that other thing and see if the grader will help me.
[00:26:03.300 --> 00:26:05.580]   Ooh, this is, this is a fun one.
[00:26:05.580 --> 00:26:10.500]   The Gaussian surprise here says take in an array of data and calculate the total surprise
[00:26:10.500 --> 00:26:13.100]   for having observed all of that data.
[00:26:13.100 --> 00:26:16.380]   And what you've done there is done the broadcasted version, which gives you the surprise for
[00:26:16.380 --> 00:26:17.460]   each one separately.
[00:26:17.460 --> 00:26:18.460]   Okay.
[00:26:18.460 --> 00:26:23.860]   So I want a sum of each element in these having been applied with, with these.
[00:26:23.860 --> 00:26:24.860]   Am I correct there?
[00:26:24.860 --> 00:26:25.860]   Yep.
[00:26:25.860 --> 00:26:28.700]   So you have a squared error done for all the data at once, and then you just want to
[00:26:28.700 --> 00:26:32.900]   add those up on the outside of this whole thing.
[00:26:32.900 --> 00:26:33.900]   Yeah.
[00:26:33.900 --> 00:26:38.500]   I should be able to, I know intuitively I could do like four da da da da, and I'm resisting
[00:26:38.500 --> 00:26:40.620]   the urge to do a loop.
[00:26:40.620 --> 00:26:45.020]   Actually it is, it is nice to write the loop version and just like make sure you've got
[00:26:45.020 --> 00:26:46.100]   the thing correct.
[00:26:46.100 --> 00:26:48.740]   But I think the thing you're looking for is np.sum here.
[00:26:48.740 --> 00:26:49.740]   Okay.
[00:26:49.740 --> 00:26:55.500]   Well, np.sum, I was reaching for that, but then if I do data minus mu, that'll just
[00:26:55.500 --> 00:26:59.300]   sum the differences and then I'm going to do the square after that.
[00:26:59.300 --> 00:27:00.300]   Okay.
[00:27:00.300 --> 00:27:04.180]   So that sums up the differences and takes the square of adding up the differences.
[00:27:04.180 --> 00:27:08.820]   So if I have two things, one is plus one away from you and the other is minus one away from
[00:27:08.820 --> 00:27:09.820]   you.
[00:27:09.820 --> 00:27:12.900]   If I add up the differences, I get zero and then you'd square that and get a squared error
[00:27:12.900 --> 00:27:13.900]   of zero.
[00:27:13.900 --> 00:27:14.900]   Okay.
[00:27:14.900 --> 00:27:17.540]   So what you really want is to square inside the sum.
[00:27:17.540 --> 00:27:18.540]   Okay.
[00:27:18.540 --> 00:27:22.220]   I'm going to do my loop and then I'm going to figure out without the loop afterwards.
[00:27:22.220 --> 00:27:23.220]   Yeah.
[00:27:23.220 --> 00:27:24.220]   Good idea.
[00:27:24.220 --> 00:27:30.940]   So let's do x minus y squared for x, y in zip mu data.
[00:27:30.940 --> 00:27:31.940]   Okay.
[00:27:31.940 --> 00:27:35.100]   I think maybe this is why this is maybe confusing.
[00:27:35.100 --> 00:27:36.900]   Mu is just going to be fixed here.
[00:27:36.900 --> 00:27:39.100]   Mu is just a single value being passed in.
[00:27:39.100 --> 00:27:41.740]   And so you actually, you don't have to zip them together or anything.
[00:27:41.740 --> 00:27:42.740]   Okay.
[00:27:42.740 --> 00:27:44.300]   That is definitely where I was confused.
[00:27:44.300 --> 00:27:47.940]   I think I can survive now with summing the differences.
[00:27:47.940 --> 00:27:49.740]   So I want to do x.
[00:27:49.740 --> 00:27:52.180]   Oh, well, actually I'll do, I'll do it here.
[00:27:52.180 --> 00:27:58.100]   So I can do no zip and then no mu, and I'm just going to call this x and then I'm just
[00:27:58.100 --> 00:28:00.340]   going to do x minus mu here.
[00:28:00.340 --> 00:28:01.340]   What have I done wrong now?
[00:28:01.340 --> 00:28:02.620]   I have a wrong bracket somewhere.
[00:28:02.620 --> 00:28:03.620]   Oh, here's the bracket.
[00:28:03.620 --> 00:28:04.620]   In data.
[00:28:04.620 --> 00:28:05.620]   Yeah.
[00:28:05.620 --> 00:28:07.700]   And then I also want to multiply that by 0.5.
[00:28:07.700 --> 00:28:09.860]   You got it.
[00:28:09.860 --> 00:28:10.860]   Let's see now.
[00:28:10.860 --> 00:28:11.860]   Okay.
[00:28:11.860 --> 00:28:14.940]   Squared errors for x equals mu is zero.
[00:28:14.940 --> 00:28:18.900]   So surprise is n times a half log z.
[00:28:18.900 --> 00:28:19.900]   I don't have log z.
[00:28:19.900 --> 00:28:20.900]   Exactly.
[00:28:20.900 --> 00:28:24.100]   So we haven't gotten to the z yet, but this suggests that we've got at least some things.
[00:28:24.100 --> 00:28:27.180]   We're getting the right output type is what the first thing says.
[00:28:27.180 --> 00:28:32.380]   And then the greater equal thing is checking to make sure that the surprise is always non-negative.
[00:28:32.380 --> 00:28:36.840]   So we haven't made any too gross errors that helped us like figure out what the right data
[00:28:36.840 --> 00:28:37.840]   type was.
[00:28:37.840 --> 00:28:39.860]   Now we need to get the right answer.
[00:28:39.860 --> 00:28:40.860]   Okay.
[00:28:40.860 --> 00:28:43.740]   I would like to fix this as well before we go on.
[00:28:43.740 --> 00:28:44.740]   Yeah.
[00:28:44.740 --> 00:28:45.740]   Good idea.
[00:28:45.740 --> 00:28:48.220]   This is probably a good learning moment, as you say.
[00:28:48.220 --> 00:28:56.020]   So if I do data minus mu, that'll do a element wise subtraction.
[00:28:56.020 --> 00:28:57.700]   Yeah, I guess.
[00:28:57.700 --> 00:28:59.580]   So mu is just a single number.
[00:28:59.580 --> 00:29:04.020]   So this does is it broadcasts, the term is broadcasting.
[00:29:04.020 --> 00:29:09.260]   So it treats mu as though we're an array, exactly the same shape as data and then does
[00:29:09.260 --> 00:29:10.420]   element wise subtraction.
[00:29:10.420 --> 00:29:11.420]   Okay.
[00:29:11.420 --> 00:29:14.900]   So this will subtract mu, the scalar value from every element in data.
[00:29:14.900 --> 00:29:15.900]   Yes.
[00:29:15.900 --> 00:29:21.180]   And then I want to square all of those values individually.
[00:29:21.180 --> 00:29:27.660]   So I can do, if I do this, will that square all of my values independently?
[00:29:27.660 --> 00:29:31.900]   So then now I'll have the squared differences and then all I need to do is sum them.
[00:29:31.900 --> 00:29:32.900]   Okay.
[00:29:32.900 --> 00:29:33.900]   I'm struggling to remember.
[00:29:33.900 --> 00:29:34.900]   I think, oh, okay.
[00:29:34.900 --> 00:29:38.220]   So I just had this out here originally for my sum.
[00:29:38.220 --> 00:29:39.220]   Cool.
[00:29:39.220 --> 00:29:43.860]   I think that's a really great instance of how you should think your way through some
[00:29:43.860 --> 00:29:48.820]   maybe difficult tensor calculations where you've got arrays and you're doing mathematical
[00:29:48.820 --> 00:29:49.860]   operations on them.
[00:29:49.860 --> 00:29:53.300]   Just break it out into a for loop and think about the components.
[00:29:53.300 --> 00:29:56.940]   And then once you're done and you're confident that is running correctly and giving you the
[00:29:56.940 --> 00:30:01.100]   right answers, then you can try and turn it into something that maybe runs more efficiently,
[00:30:01.100 --> 00:30:06.100]   is more like sort of Pythonic and fits the way NumPy likes you to write things while
[00:30:06.100 --> 00:30:09.780]   you still have that nice working example to compare to always to make sure that you're
[00:30:09.780 --> 00:30:10.780]   getting the right answer.
[00:30:10.780 --> 00:30:14.940]   Um, but yeah, I think whenever I come across anything super difficult like that, I turn
[00:30:14.940 --> 00:30:17.620]   it into for loops, unroll it like that.
[00:30:17.620 --> 00:30:19.660]   So I can think clearly about what I'm trying to do.
[00:30:19.660 --> 00:30:24.740]   Especially when there's things like broadcasting and like in that case, I figured out that
[00:30:24.740 --> 00:30:31.100]   I had a misunderstood, at least that this was not a vector.
[00:30:31.100 --> 00:30:35.260]   It does tend to find bugs that you, you know, you otherwise wouldn't have found.
[00:30:35.260 --> 00:30:36.260]   Exactly.
[00:30:36.260 --> 00:30:37.260]   Yeah.
[00:30:37.260 --> 00:30:39.060]   If you tried to for loop over mu, you would get a very clear error.
[00:30:39.060 --> 00:30:40.060]   That's this is a scalar.
[00:30:40.060 --> 00:30:41.060]   You can't for loop over that.
[00:30:41.060 --> 00:30:43.060]   And your shape errors will maybe be more informative.
[00:30:43.060 --> 00:30:47.700]   It'll be like, oh, the length, you know, you said that you wanted to loop over with a range
[00:30:47.700 --> 00:30:50.540]   of a particular length and then that didn't work.
[00:30:50.540 --> 00:30:55.100]   And you get a clear error that says, here's the shape that is wrong in what you just wrote
[00:30:55.100 --> 00:30:56.100]   with your for loops.
[00:30:56.100 --> 00:30:57.460]   It can be much more explicit.
[00:30:57.460 --> 00:31:01.780]   And so it's a, it's a nice way to debug, like approach your code from a different angle,
[00:31:01.780 --> 00:31:03.900]   maybe approach your problem from a different angle.
[00:31:03.900 --> 00:31:06.020]   And some things will be more clear from that direction.
[00:31:06.020 --> 00:31:07.020]   Cool.
[00:31:07.020 --> 00:31:12.820]   So I still have the issue that is this normalization constant with Z that I don't have access to.
[00:31:12.820 --> 00:31:16.260]   So my suggestion would be right now, when I said like, you know, write the code you
[00:31:16.260 --> 00:31:20.060]   wish you could write would just be like plus 0.5 times log Z, right?
[00:31:20.060 --> 00:31:21.060]   Yeah.
[00:31:21.060 --> 00:31:22.060]   Log of Z.
[00:31:22.060 --> 00:31:23.060]   Yeah.
[00:31:23.060 --> 00:31:24.060]   NP dot log.
[00:31:24.060 --> 00:31:25.060]   Oh yes.
[00:31:25.060 --> 00:31:26.060]   Okay.
[00:31:26.060 --> 00:31:27.060]   And this code won't run yet.
[00:31:27.060 --> 00:31:28.580]   So the graders just gonna be like, wait, what the hell is Z?
[00:31:28.580 --> 00:31:33.460]   So what we need to do is write down what Z is by going back up to the formula.
[00:31:33.460 --> 00:31:35.180]   Ah, I see.
[00:31:35.180 --> 00:31:36.180]   Okay.
[00:31:36.180 --> 00:31:37.180]   So Z is just two pi.
[00:31:37.180 --> 00:31:38.180]   Yep.
[00:31:38.180 --> 00:31:41.720]   So that's something that it makes sure that the thing integrates to one.
[00:31:41.720 --> 00:31:46.020]   You could do the integral by hand if you're that type of intrepid individual, or you can
[00:31:46.020 --> 00:31:50.820]   just look up these normalization constants or how to calculate them with a simpler formula,
[00:31:50.820 --> 00:31:52.100]   which is what I usually do.
[00:31:52.100 --> 00:31:59.460]   Is this the same as in university probability classes where you'll have a big log book of
[00:31:59.460 --> 00:32:04.980]   probabilities and you have to look up that specific one for a given function?
[00:32:04.980 --> 00:32:06.460]   Am I, is this a different thing?
[00:32:06.460 --> 00:32:07.460]   Yeah.
[00:32:07.460 --> 00:32:09.060]   So they come about for the same reason actually.
[00:32:09.060 --> 00:32:13.540]   So what I'm thinking of is like if you take a statistics class and you get like a value
[00:32:13.540 --> 00:32:19.100]   for a statistic, the T statistic or the Z statistic or whatever, then you go look up
[00:32:19.100 --> 00:32:23.700]   a value that tells you something like the P value or things like that on the basis of
[00:32:23.700 --> 00:32:24.700]   that statistic.
[00:32:24.700 --> 00:32:25.700]   Yes.
[00:32:25.700 --> 00:32:29.820]   In both cases, there is a gnarly integral that is like really hard to evaluate and you
[00:32:29.820 --> 00:32:34.060]   either need an expert at evaluating integrals to tell you what those answers will be, or
[00:32:34.060 --> 00:32:36.740]   you need to approximate them with a computer in some way.
[00:32:36.740 --> 00:32:40.500]   And then in the time before you could just pass around code really easily and run it
[00:32:40.500 --> 00:32:44.540]   wherever you wanted like we can now, the basic answer was, oh, we'll just write the answers
[00:32:44.540 --> 00:32:46.700]   down and pass those around instead.
[00:32:46.700 --> 00:32:51.620]   So yeah, it's, I think in the end, both of them come from really gnarly, hard to evaluate
[00:32:51.620 --> 00:32:52.620]   integrals.
[00:32:52.620 --> 00:32:54.740]   That's why we look these values up.
[00:32:54.740 --> 00:32:55.740]   Okay, cool.
[00:32:55.740 --> 00:33:02.020]   So two pi here is just this number that somebody wrote down at some time, you know, calculated
[00:33:02.020 --> 00:33:06.980]   by hand that we're not getting the benefit of that and plugging it into our formula.
[00:33:06.980 --> 00:33:07.980]   Okay.
[00:33:07.980 --> 00:33:09.780]   Where are we at here?
[00:33:09.780 --> 00:33:11.860]   The zero.
[00:33:11.860 --> 00:33:15.480]   So the surprise is N times half log Z.
[00:33:15.480 --> 00:33:17.220]   Have I done that correctly?
[00:33:17.220 --> 00:33:18.840]   Times NP dot log pi.
[00:33:18.840 --> 00:33:24.180]   The one tricky bit here, I actually didn't, didn't see this, is that formula up there
[00:33:24.180 --> 00:33:27.260]   is the surprise for one value.
[00:33:27.260 --> 00:33:31.820]   So we get a one half log Z every single time we evaluate on a value.
[00:33:31.820 --> 00:33:33.780]   You could do one of two things.
[00:33:33.780 --> 00:33:40.700]   You could either put it inside of the sum there, or you could multiply it by the number
[00:33:40.700 --> 00:33:43.780]   of terms in the sum, multiply it by the length of the data.
[00:33:43.780 --> 00:33:44.780]   Interesting.
[00:33:44.780 --> 00:33:45.780]   Okay.
[00:33:45.780 --> 00:33:48.140]   I could, so I could do that now, but I don't think I'd really understand it.
[00:33:48.140 --> 00:33:53.420]   Is that shown in this formula or is there some implicit I here that I'm not seeing?
[00:33:53.420 --> 00:33:55.300]   Yeah, there's an implicit I there.
[00:33:55.300 --> 00:34:03.260]   The surprise on observation XI is one half of XI minus mu squared plus one half log Z.
[00:34:03.260 --> 00:34:05.460]   Or log Z, no, log Z.
[00:34:05.460 --> 00:34:06.460]   Oh, okay.
[00:34:06.460 --> 00:34:07.460]   I'm sorry.
[00:34:07.460 --> 00:34:11.540]   Which is just one of these for each of the Xs and I'm calculating one X.
[00:34:11.540 --> 00:34:15.060]   So if I did that, this is correct for like my first I.
[00:34:15.060 --> 00:34:16.980]   Okay, sweet.
[00:34:16.980 --> 00:34:20.060]   I don't have a preference of which is more elegant here.
[00:34:20.060 --> 00:34:23.500]   I would say maybe the length one.
[00:34:23.500 --> 00:34:25.060]   Happy to be wrong about that.
[00:34:25.060 --> 00:34:27.900]   I think that will run faster.
[00:34:27.900 --> 00:34:31.380]   What you would have to do otherwise is like every time, every single time you come across
[00:34:31.380 --> 00:34:33.780]   a data point, you're adding one half log Z.
[00:34:33.780 --> 00:34:39.300]   But we know that if we do that N times, what we'll get is N times one half log Z.
[00:34:39.300 --> 00:34:43.660]   And so you're sort of turning, you're taking that and algebraically figuring out what the
[00:34:43.660 --> 00:34:48.340]   right answer is, multiply by len data and calculating it in one fell swoop instead of
[00:34:48.340 --> 00:34:50.100]   writing the slower code.
[00:34:50.100 --> 00:34:51.420]   Let's check that that's right.
[00:34:51.420 --> 00:34:52.420]   Great.
[00:34:52.420 --> 00:34:54.860]   That is a little, that is a tricky bit of that problem there.
[00:34:54.860 --> 00:34:58.460]   Because you always jump to using like an entire array of data at once.
[00:34:58.460 --> 00:35:00.300]   So that is, that's definitely a tricky bit there.
[00:35:00.300 --> 00:35:01.300]   Yeah.
[00:35:01.300 --> 00:35:05.460]   So this one, this one here caught me a few times that, you know, between this being a
[00:35:05.460 --> 00:35:11.900]   scalar and then this term being also a constant that, you know, I'm, Hey, I should have, should
[00:35:11.900 --> 00:35:13.500]   have noticed it up there, but I didn't.
[00:35:13.500 --> 00:35:18.180]   And then also that it's not just a constant, this added in it for, for the whole, it's
[00:35:18.180 --> 00:35:19.780]   for every single value of X.
[00:35:19.780 --> 00:35:22.700]   So yeah, a couple of tricks, but you helped me through it.
[00:35:22.700 --> 00:35:23.700]   Yeah.
[00:35:23.700 --> 00:35:27.140]   So the way to organize these exercises would be to do it for one data point, then do it
[00:35:27.140 --> 00:35:28.900]   for all the data at once.
[00:35:28.900 --> 00:35:30.740]   So split this up into two exercises.
[00:35:30.740 --> 00:35:31.740]   Yeah.
[00:35:31.740 --> 00:35:33.620]   Or, or have you alongside helped me through it.
[00:35:33.620 --> 00:35:35.860]   That's the other way to do it.
[00:35:35.860 --> 00:35:39.580]   So we're trying to connect this to machine learning models and you aren't going to find
[00:35:39.580 --> 00:35:43.700]   a Gaussian surprise function like we just wrote in TensorFlow or PyTorch anywhere.
[00:35:43.700 --> 00:35:48.000]   What you'll find instead is something that calculates something like the squared error.
[00:35:48.000 --> 00:35:51.540]   This one's much more straightforward to write because there aren't those confusions about
[00:35:51.540 --> 00:35:53.100]   constants or anything like that.
[00:35:53.100 --> 00:35:56.300]   I just know, okay, I need to do the difference.
[00:35:56.300 --> 00:36:00.140]   That'll be applied to each value, then squared, then sum together.
[00:36:00.140 --> 00:36:01.140]   Yeah.
[00:36:01.140 --> 00:36:02.140]   Nice.
[00:36:02.140 --> 00:36:03.140]   Sick.
[00:36:03.140 --> 00:36:05.660]   So this is what you'll more commonly see things like the mean squared error and the sum of
[00:36:05.660 --> 00:36:08.580]   squared errors when people are actually writing out their loss functions when they're writing
[00:36:08.580 --> 00:36:09.580]   out their code.
[00:36:09.580 --> 00:36:13.980]   But where that came from is that came from that Gaussian surprise.
[00:36:13.980 --> 00:36:17.220]   They differ in their actual numerical, the like output values, right?
[00:36:17.220 --> 00:36:19.700]   Cause there's no log Z in this second one.
[00:36:19.700 --> 00:36:23.580]   But if you take a gradient, Z never changes as you change mu.
[00:36:23.580 --> 00:36:25.900]   So that just vanishes when you take a gradient.
[00:36:25.900 --> 00:36:29.780]   And so they, all they differ by is by a factor of one half.
[00:36:29.780 --> 00:36:33.740]   There's that one half at the beginning of the Gaussian surprise.
[00:36:33.740 --> 00:36:38.500]   If you scroll up just a little bit, it's one half times the squared error or the squared
[00:36:38.500 --> 00:36:39.500]   difference.
[00:36:39.500 --> 00:36:41.100]   And so that stays around in the gradient.
[00:36:41.100 --> 00:36:43.060]   So the scales of the gradients are different.
[00:36:43.060 --> 00:36:46.700]   And you know, we use learning rates to change the scales of our gradients all the time.
[00:36:46.700 --> 00:36:48.180]   So the scale's not that meaningful.
[00:36:48.180 --> 00:36:52.700]   So these really are, once it comes time to do optimization, these are effectively the
[00:36:52.700 --> 00:36:56.460]   same function, the sum of squared errors and the Gaussian surprise.
[00:36:56.460 --> 00:36:59.180]   And this just shows you that what I'm saying is correct here.
[00:36:59.180 --> 00:37:04.100]   But if I take a gradient descent step using that Gaussian surprise function you implemented,
[00:37:04.100 --> 00:37:05.560]   I get the same answer.
[00:37:05.560 --> 00:37:09.860]   If I do it with the sum of squared errors instead, down there in that last assert block
[00:37:09.860 --> 00:37:14.860]   there is saying that these two gradient descent steps give you the same answer.
[00:37:14.860 --> 00:37:17.180]   And it's got the half learning rate.
[00:37:17.180 --> 00:37:18.180]   Nice.
[00:37:18.180 --> 00:37:23.580]   And just like scaling the gradients does not change what the best value is.
[00:37:23.580 --> 00:37:26.660]   So at the optimum, the gradients are zero.
[00:37:26.660 --> 00:37:29.220]   So multiply them by a half doesn't change anything.
[00:37:29.220 --> 00:37:30.220]   It's still zero.
[00:37:30.220 --> 00:37:31.460]   So the optima still stay the same.
[00:37:31.460 --> 00:37:35.580]   The goal of our optimization still stays the same, whether we're using the sum of squared
[00:37:35.580 --> 00:37:37.540]   errors or the Gaussian surprise.
[00:37:37.540 --> 00:37:41.560]   Let's demonstrate that by writing a run gradient descent.
[00:37:41.560 --> 00:37:44.820]   And then the way I'm going to check that you implemented gradient descent correctly is
[00:37:44.820 --> 00:37:49.260]   I'm going to check that it gives the same answer when optimizing mu for both the Gaussian
[00:37:49.260 --> 00:37:51.140]   surprise and the sum of squared error.
[00:37:51.140 --> 00:37:54.980]   I didn't give you a definition of gradient descent here.
[00:37:54.980 --> 00:37:57.340]   I guess I gave you the GD step there.
[00:37:57.340 --> 00:38:00.460]   So you can reuse that function, which does a lot of the work.
[00:38:00.460 --> 00:38:06.340]   But it's what we learned, I think, in a different class where it's the step in the opposite
[00:38:06.340 --> 00:38:09.620]   direction of the gradient by a given learning rate.
[00:38:09.620 --> 00:38:10.620]   Yeah.
[00:38:10.620 --> 00:38:12.860]   And that is one step of gradient descent.
[00:38:12.860 --> 00:38:18.540]   So all you need to do to run gradient descent is not just take one step, but take many steps.
[00:38:18.540 --> 00:38:19.540]   Yeah.
[00:38:19.540 --> 00:38:20.540]   Okay.
[00:38:20.540 --> 00:38:25.560]   So this would be like your epoch number where in each step or in each epoch, we evaluate
[00:38:25.560 --> 00:38:28.460]   the gradients for all of the data.
[00:38:28.460 --> 00:38:30.540]   Usually that's done in mini batches.
[00:38:30.540 --> 00:38:34.580]   But in this case, I gather we'll be doing it per element in the data.
[00:38:34.580 --> 00:38:39.860]   I would say rather than doing mini batches or doing it element wise, the GD step expects
[00:38:39.860 --> 00:38:41.380]   to get an array of data.
[00:38:41.380 --> 00:38:44.580]   And so let's do the GD step for n steps.
[00:38:44.580 --> 00:38:45.580]   For n steps.
[00:38:45.580 --> 00:38:46.580]   Yeah.
[00:38:46.580 --> 00:38:50.660]   For n steps or range of n steps.
[00:38:50.660 --> 00:38:53.060]   I don't actually need the steps.
[00:38:53.060 --> 00:38:58.860]   And then I'm going to call gradient descent and it's going to return something which is
[00:38:58.860 --> 00:39:04.960]   going to be my new value for the parameter mu.
[00:39:04.960 --> 00:39:09.660]   So mu here is the parameter of this Gaussian distribution.
[00:39:09.660 --> 00:39:13.580]   And it's like the parameters of our model, because once you have that parameter, you
[00:39:13.580 --> 00:39:16.220]   know the probability distribution over the outputs.
[00:39:16.220 --> 00:39:19.940]   Just like once you have the parameters of a neural network, you also have a probability
[00:39:19.940 --> 00:39:21.740]   distribution over the outputs.
[00:39:21.740 --> 00:39:24.940]   And so that mu there is our parameter.
[00:39:24.940 --> 00:39:28.720]   And I guess I would focus in terms of like writing the next line here, I would focus
[00:39:28.720 --> 00:39:31.860]   on just the API of GD step.
[00:39:31.860 --> 00:39:34.620]   Like you're going to expect those four things there.
[00:39:34.620 --> 00:39:35.620]   Okay.
[00:39:35.620 --> 00:39:36.620]   I'll just print them down here.
[00:39:36.620 --> 00:39:37.620]   So I have them.
[00:39:37.620 --> 00:39:41.060]   So mu zero is that, data is the same, F is the same.
[00:39:41.060 --> 00:39:42.780]   The learning rate is also passed in.
[00:39:42.780 --> 00:39:47.760]   I'm using n steps here and then I want to update which that's fine.
[00:39:47.760 --> 00:39:51.940]   So I'm now renaming that and then I'm going to keep doing that after n steps.
[00:39:51.940 --> 00:39:56.820]   I'll now have like a mu zero that was updated 10 times and then I'm going to return that
[00:39:56.820 --> 00:39:57.820]   value.
[00:39:57.820 --> 00:39:58.820]   Yep.
[00:39:58.820 --> 00:39:59.820]   Let's see.
[00:39:59.820 --> 00:40:00.820]   Yay.
[00:40:00.820 --> 00:40:01.820]   Got it.
[00:40:01.820 --> 00:40:03.420]   The only thing I might change about the way you implemented that function is I might give
[00:40:03.420 --> 00:40:06.100]   an alias to mu zero.
[00:40:06.100 --> 00:40:09.580]   I call it mu T equals mu zero at the start.
[00:40:09.580 --> 00:40:13.980]   Or even this would be probably at least then it's not saying it's the zero.
[00:40:13.980 --> 00:40:14.980]   Like I don't know why.
[00:40:14.980 --> 00:40:17.780]   Why would you keep around mu zero?
[00:40:17.780 --> 00:40:23.440]   I put it in the API of the function just because I think of the names of arguments in my functions
[00:40:23.440 --> 00:40:27.420]   as a way to communicate to the person calling my function what I'm expecting.
[00:40:27.420 --> 00:40:33.260]   So calling it mu zero tells them, Hey, I am expecting the starting value of mu.
[00:40:33.260 --> 00:40:37.420]   And then if I need to rename that or change something about it in the body of the function,
[00:40:37.420 --> 00:40:38.620]   I'm happy to do that.
[00:40:38.620 --> 00:40:44.620]   I want the name in the doc string in the function signature to be as like communicative as possible
[00:40:44.620 --> 00:40:46.260]   to the person calling my function.
[00:40:46.260 --> 00:40:47.260]   Okay.
[00:40:47.260 --> 00:40:53.140]   And do I need to copy it if I'm going to do that or does NumPy copy it for me?
[00:40:53.140 --> 00:40:57.060]   Mu T will just be a view on the same array.
[00:40:57.060 --> 00:40:58.060]   You'll be overriding it.
[00:40:58.060 --> 00:41:01.460]   So it might be polite actually to copy mu zero.
[00:41:01.460 --> 00:41:05.420]   In case maybe someone's passing this in and using it later for some.
[00:41:05.420 --> 00:41:06.420]   And then also using it later.
[00:41:06.420 --> 00:41:07.420]   Yeah.
[00:41:07.420 --> 00:41:11.460]   So these are the kind of rough, tough things about implementing a good version of gradient
[00:41:11.460 --> 00:41:14.980]   descent and a good version of like your machine learning models and their parameters that
[00:41:14.980 --> 00:41:18.420]   are why you don't want to write gradient descent yourself, you know, and you don't want to
[00:41:18.420 --> 00:41:22.660]   write your neural network library from scratch, like, you know, what you actually use, but
[00:41:22.660 --> 00:41:25.100]   I think to do it at least once.
[00:41:25.100 --> 00:41:26.100]   Yeah, that's true.
[00:41:26.100 --> 00:41:31.740]   I feel like using Autograd here is at least saving us a lot of pain that at least we don't
[00:41:31.740 --> 00:41:37.340]   have to implement the gradient step ourself, but yeah, you're, you're right that it's nice
[00:41:37.340 --> 00:41:41.300]   that training loops exist in packages and you don't have to think about it.
[00:41:41.300 --> 00:41:44.580]   But in order to be able to like make use of those effectively and to debug them, it's
[00:41:44.580 --> 00:41:48.080]   really useful to go through an exercise like we've done in the course of this whole class
[00:41:48.080 --> 00:41:53.160]   actually and think through where are all of these ideas coming from and what would be
[00:41:53.160 --> 00:41:58.260]   the basic bare bones implementation of a bunch of these things like calculating a gradient,
[00:41:58.260 --> 00:42:02.940]   like running gradient descent or doing all these linear algebra operations, writing these
[00:42:02.940 --> 00:42:08.580]   things out in maybe forms that are not very numerically stable, are maybe not very efficient,
[00:42:08.580 --> 00:42:11.620]   but are easy for us to understand and give the same answer.
[00:42:11.620 --> 00:42:15.280]   And then you can take that intuition and run with it to be able to understand the stuff
[00:42:15.280 --> 00:42:19.100]   that's actually useful that's out there in like PyTorch and Keras.
[00:42:19.100 --> 00:42:20.340]   I completely agree.
[00:42:20.340 --> 00:42:21.340]   Great.
[00:42:21.340 --> 00:42:25.500]   I'm glad and hopefully the folks enjoying this course from home also agree.
[00:42:25.500 --> 00:42:30.900]   I gave some recommendations for additional exercises or, or, or resources for learning
[00:42:30.900 --> 00:42:36.220]   math for machine learning in the lecture, but I would close out just our exercise video
[00:42:36.220 --> 00:42:41.940]   series by saying, if you want to really understand this stuff, there's no substitute for like
[00:42:41.940 --> 00:42:45.500]   building a toy basic version of these things yourself.
[00:42:45.500 --> 00:42:48.220]   And some of the resources I pointed to help you do that.
[00:42:48.220 --> 00:42:51.900]   Grammar's Introduction to Mathematics by Jeremy Kuhn is a good example.
[00:42:51.900 --> 00:42:54.420]   Build Your Own X on GitHub is another one.
[00:42:54.420 --> 00:42:58.340]   These are great places to try that and really get comfortable with these things and become
[00:42:58.340 --> 00:43:02.220]   in the end more comfortable with the machine learning models and tools you're building.
[00:43:02.220 --> 00:43:03.220]   All right.
[00:43:03.220 --> 00:43:06.380]   Thanks a lot, Scott, for working through all these exercises with me.
[00:43:06.380 --> 00:43:11.340]   You've been a really great sport as I've trapped you in my little learning traps and you've
[00:43:11.340 --> 00:43:12.340]   done a great job.
[00:43:12.340 --> 00:43:13.340]   Thanks.
[00:43:13.340 --> 00:43:14.340]   I've learned a lot.
[00:43:14.340 --> 00:43:15.340]   All right.
[00:43:15.340 --> 00:43:16.340]   All right.
[00:43:16.340 --> 00:43:17.340]   All right.
[00:43:17.340 --> 00:43:18.340]   All right.
[00:43:18.340 --> 00:43:18.340]   All right.
[00:43:18.340 --> 00:43:19.340]   All right.
[00:43:19.340 --> 00:43:19.340]   All right.
[00:43:19.340 --> 00:43:20.340]   All right.
[00:43:20.340 --> 00:43:20.340]   All right.
[00:43:20.340 --> 00:43:21.340]   All right.
[00:43:21.340 --> 00:43:21.340]   All right.
[00:43:21.340 --> 00:43:22.340]   All right.
[00:43:22.340 --> 00:43:22.340]   All right.
[00:43:22.340 --> 00:43:23.340]   All right.
[00:43:23.340 --> 00:43:23.340]   All right.
[00:43:23.340 --> 00:43:24.340]   All right.
[00:43:24.340 --> 00:43:24.340]   All right.
[00:43:24.340 --> 00:43:25.340]   All right.
[00:43:25.340 --> 00:43:25.340]   All right.
[00:43:25.340 --> 00:43:26.340]   All right.
[00:43:26.340 --> 00:43:26.340]   All right.
[00:43:26.340 --> 00:43:27.340]   All right.
[00:43:27.340 --> 00:43:27.340]   All right.
[00:43:28.340 --> 00:43:29.340]   All right.
[00:43:29.340 --> 00:43:29.340]   All right.
[00:43:29.340 --> 00:43:30.340]   All right.
[00:43:30.340 --> 00:43:30.340]   All right.
[00:43:30.340 --> 00:43:32.920]   (upbeat music)
[00:43:32.920 --> 00:43:35.500]   (upbeat music)

