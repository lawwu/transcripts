
[00:00:00.000 --> 00:00:06.400]   I believe we are live.
[00:00:06.400 --> 00:00:13.000]   It will take a second or two for it to actually start up on the Weights and Biases YouTube
[00:00:13.000 --> 00:00:14.000]   page.
[00:00:14.000 --> 00:00:18.720]   Give us just one second for that to kick off.
[00:00:18.720 --> 00:00:21.040]   We are going.
[00:00:21.040 --> 00:00:25.640]   Welcome everyone to the Weights and Biases Deep Learning Salon.
[00:00:25.640 --> 00:00:28.160]   I am your host, Charles Frey.
[00:00:28.160 --> 00:00:32.480]   What we will be doing this time is hearing from a bunch of Weights and Biases community
[00:00:32.480 --> 00:00:38.760]   members who participated in the Reproducibility Challenge.
[00:00:38.760 --> 00:00:43.680]   The Reproducibility Challenge started off with Papers with Code.
[00:00:43.680 --> 00:00:47.120]   We've decided to join that, help out with it.
[00:00:47.120 --> 00:00:52.360]   Actually rather than me telling you anything about it, I'm going to hand it over to our
[00:00:52.360 --> 00:00:57.120]   Reproducibility Challenge expert at Weights and Biases, Deganta Mishra.
[00:00:57.120 --> 00:00:58.840]   Deganta, go ahead.
[00:00:58.840 --> 00:01:00.440]   Hello, everyone.
[00:01:00.440 --> 00:01:03.160]   My name is Deganta.
[00:01:03.160 --> 00:01:06.560]   I'm a machine learning engineer.
[00:01:06.560 --> 00:01:12.200]   As Charles stated, I'm glad that I can take the title of Reproducibility Challenge expert,
[00:01:12.200 --> 00:01:17.080]   but the experts will be doing the talking after me, the people who participated in the
[00:01:17.080 --> 00:01:18.080]   challenge.
[00:01:18.080 --> 00:01:24.560]   When I joined the team last year, coming from an academic background, Reproducibility Challenge
[00:01:24.560 --> 00:01:29.360]   is something that I really, really held with high importance because it's super important
[00:01:29.360 --> 00:01:35.400]   to make research in deep learning more transparent and reproducible.
[00:01:35.400 --> 00:01:40.360]   We decided at Weights and Biases that we'll be supporting the Papers with Code initiative
[00:01:40.360 --> 00:01:42.600]   for Reproducibility Challenge.
[00:01:42.600 --> 00:01:51.000]   We provided our participants with the resources to ensure that we could reproduce as many
[00:01:51.000 --> 00:01:56.320]   papers from top AI conferences from 2020.
[00:01:56.320 --> 00:01:59.280]   We did that with a fair share of success.
[00:01:59.280 --> 00:02:06.360]   We had so many interesting reports that we gathered from our participants who reproduced
[00:02:06.360 --> 00:02:12.760]   super exciting papers ranging from genetic algorithms to attention models to many other
[00:02:12.760 --> 00:02:19.800]   variants like pruning and lottery ticket hypothesis that you'll hear in a short while.
[00:02:19.800 --> 00:02:26.080]   Before I start, before I give the mic to the people who participated in the challenge,
[00:02:26.080 --> 00:02:33.240]   I just want to give a super quick teaser trailer that we'll be also doing the spring 2021 edition
[00:02:33.240 --> 00:02:38.520]   of the Reproducibility Challenge starting off in April 1st.
[00:02:38.520 --> 00:02:44.880]   I really, really encourage everyone to consider participating because it's going to be super
[00:02:44.880 --> 00:02:50.400]   awesome and we are upscaling our efforts in helping you all out to make this a huge success.
[00:02:50.400 --> 00:02:54.880]   With that, I'd like to give it to the participants.
[00:02:54.880 --> 00:02:58.000]   All right.
[00:02:58.000 --> 00:03:09.880]   Yeah, I think first up we have the team that worked from the fast AI community that worked
[00:03:09.880 --> 00:03:18.800]   on the Reformer paper, one of the most exciting of the many papers that came out this past
[00:03:18.800 --> 00:03:21.520]   year on making transformers more efficient.
[00:03:21.520 --> 00:03:26.920]   So, Arto, if you'd like to take it away.
[00:03:26.920 --> 00:03:29.040]   Hey, everyone.
[00:03:29.040 --> 00:03:38.600]   My name is Arto and I will be presenting our entry to Reproducibility Challenge for Reformer
[00:03:38.600 --> 00:03:47.880]   as an efficient transformer paper from Google Research, authored by Nikita Gitaev, Lukash
[00:03:47.880 --> 00:03:50.960]   Kaiser and Anselm Levskaia.
[00:03:50.960 --> 00:04:01.520]   And it's done by fast AI community represented here by my teammates, Morgan and Halvar.
[00:04:01.520 --> 00:04:08.560]   So since the release of Attention is All You Need paper, transformers have become by far
[00:04:08.560 --> 00:04:16.360]   the most popular architecture for NLP and now is successfully applied to vision, time
[00:04:16.360 --> 00:04:19.520]   series and audio data.
[00:04:19.520 --> 00:04:26.480]   And a key mechanics in transformers is attention, which is used to produce contextualized token
[00:04:26.480 --> 00:04:36.600]   embeddings and for each token, key squares and values are computed and then every key
[00:04:36.600 --> 00:04:42.440]   is compared to every query to produce the attention scores.
[00:04:42.440 --> 00:04:50.480]   And this results in quadratic complexity of the computation.
[00:04:50.480 --> 00:04:58.440]   And this is the main limiting factor to applying transformers to the tasks requiring attention
[00:04:58.440 --> 00:05:01.520]   in a long context.
[00:05:01.520 --> 00:05:10.680]   So making transformers more memory and time efficient is an active area of research right
[00:05:10.680 --> 00:05:23.000]   now and one can refer to long range arena benchmark for comparison of the current techniques.
[00:05:23.000 --> 00:05:34.600]   So the one we were reproducing is Reformer, which is designed to allow training on input
[00:05:34.600 --> 00:05:45.080]   sequences of lengths of over 64,000 tokens on a single accelerator.
[00:05:45.080 --> 00:05:52.400]   And to achieve this, five key techniques were used.
[00:05:52.400 --> 00:06:02.400]   So the LSH attention mechanism is an approximation to full attention using locality sensitive
[00:06:02.400 --> 00:06:03.400]   hashing.
[00:06:03.400 --> 00:06:11.400]   It utilizes the sparsity, like natural sparsity of attention metrics.
[00:06:11.400 --> 00:06:22.680]   So all the key query pairs are hashed using random projections, bucketized and in such
[00:06:22.680 --> 00:06:29.320]   a way that similar key query pairs will end up in the same bucket.
[00:06:29.320 --> 00:06:39.600]   When buckets are sorted, keys and queries are sorted by buckets and the attention scores
[00:06:39.600 --> 00:06:47.360]   are computed within each bucket only and into one neighboring bucket.
[00:06:47.360 --> 00:06:55.560]   This allows to reduce the time in memory complexity of the computation from order n squared to
[00:06:55.560 --> 00:06:57.760]   order n log n.
[00:06:57.760 --> 00:07:05.240]   And this is a randomized algorithm and its robustness might be improved by doing multiple
[00:07:05.240 --> 00:07:09.600]   rounds of such hashing.
[00:07:09.600 --> 00:07:12.640]   The next technique is reversible layers.
[00:07:12.640 --> 00:07:21.500]   It's a variant of residual layers, allowing to avoid storing intermediate activations
[00:07:21.500 --> 00:07:28.600]   during the forward pass in cost of re-computing those at backward pass.
[00:07:28.600 --> 00:07:38.600]   And this enables to train very deep models under constrained memory requirements.
[00:07:38.600 --> 00:07:41.880]   And also there are some supplementary techniques.
[00:07:41.880 --> 00:07:52.280]   So for LSH to work properly, we need keys and queries produced from a single token to
[00:07:52.280 --> 00:07:55.060]   end up in the one bucket.
[00:07:55.060 --> 00:08:03.420]   And to ensure this, keys and queries are computed using shared projection.
[00:08:03.420 --> 00:08:10.220]   Also to avoid feedforward computation becoming memory bottleneck of the system, it might
[00:08:10.220 --> 00:08:18.740]   be computed in chunks and when combined with reversible layers, this allows to control
[00:08:18.740 --> 00:08:22.020]   memory consumption by feedforward layers.
[00:08:22.020 --> 00:08:33.260]   Also to reduce memory used for storing positional encodings for very long sequences, Excel positional
[00:08:33.260 --> 00:08:42.220]   encoding technique might be used, which uses factorization to enable very long sequences.
[00:08:42.220 --> 00:08:47.060]   So the central claims of this paper are following.
[00:08:47.060 --> 00:08:54.460]   The reformer performs on par with traditional transformer models, while being much more
[00:08:54.460 --> 00:09:00.660]   memory efficient and much faster on longer sequences.
[00:09:00.660 --> 00:09:08.380]   And to verify these claims is the main goal of the reproducibility challenge.
[00:09:08.380 --> 00:09:14.580]   And the original implementation of reformer was done in Trex, which is based on Jack's
[00:09:14.580 --> 00:09:20.960]   framework and our community emerged from FastAI.
[00:09:20.960 --> 00:09:28.340]   So we had additional goals to reimplement the model in PyTorch and train it, adapted
[00:09:28.340 --> 00:09:34.340]   to train using FastAI training loop.
[00:09:34.340 --> 00:09:45.080]   So the memory and time complexity related claims may be verified by direct experiments.
[00:09:45.080 --> 00:09:54.700]   And here you can see memory profiles for variants of six layer deep models.
[00:09:54.700 --> 00:10:07.820]   And as one can see, there are this memory allocation jumps associated with big attention
[00:10:07.820 --> 00:10:11.260]   matrix computations for full attention.
[00:10:11.260 --> 00:10:16.840]   And also memory is accumulated during the forward pass and only released during the
[00:10:16.840 --> 00:10:19.260]   backward pass.
[00:10:19.260 --> 00:10:33.220]   Using LSH attention allows to avoid these memory peaks by doing it in nLogN memory complexity
[00:10:33.220 --> 00:10:36.020]   instead of quadratic one.
[00:10:36.020 --> 00:10:45.640]   And using reversible layers allows to make memory allocation constant in regard to number
[00:10:45.640 --> 00:10:50.700]   of layers in the model.
[00:10:50.700 --> 00:10:53.860]   Here it's shown in orange.
[00:10:53.860 --> 00:11:02.020]   And the result in reformer model, which is shown in red here, has by far lower memory
[00:11:02.020 --> 00:11:08.060]   consumption as compared to the classic transformer.
[00:11:08.060 --> 00:11:19.380]   And this will be even more strong when the context length grows.
[00:11:19.380 --> 00:11:27.660]   Also notice that the memory consumption of reformer grows linearly with a number of hashing
[00:11:27.660 --> 00:11:36.980]   rounds and this results in some accuracy memory trade-off.
[00:11:36.980 --> 00:11:48.840]   So the next plot shows the computation time for LSH attention and it is constant with
[00:11:48.840 --> 00:12:00.200]   respect to sequence lengths and grows linearly with number of hashing rounds while the full
[00:12:00.200 --> 00:12:08.960]   attention computation grows approximately exponentially at our observation.
[00:12:08.960 --> 00:12:11.440]   So now to the next part.
[00:12:11.440 --> 00:12:20.560]   To verify the performance-related claims, there are three experiments, groups of experiments.
[00:12:20.560 --> 00:12:30.160]   One on synthetic task, text compression experiments on NVK8 and translation experiments on WMT14.
[00:12:30.160 --> 00:12:35.760]   So the synthetic task may be formulated as follows.
[00:12:35.760 --> 00:12:48.240]   Given a twin sequence of this form where a word is a sequence of 511 random integers,
[00:12:48.240 --> 00:12:56.160]   the task is to predict the second half of the sequence in autoregressive manner.
[00:12:56.160 --> 00:13:04.920]   And this may be simply done by copying from the first part to the second part.
[00:13:04.920 --> 00:13:13.700]   The task is trivial to transform with global attention, but it is infeasible if attention
[00:13:13.700 --> 00:13:16.500]   is local.
[00:13:16.500 --> 00:13:28.820]   So our experiments verified that LSH indeed attends globally and we were able to achieve
[00:13:28.820 --> 00:13:34.660]   high accuracy on this task with four hashing rounds.
[00:13:34.660 --> 00:13:44.220]   Also we observed unstable convergence when using shared QK projection on this task.
[00:13:44.220 --> 00:13:54.660]   And here you can see a bunch of rounds which only differ in random seed while classic transformer
[00:13:54.660 --> 00:14:00.100]   converges very stably on this task.
[00:14:00.100 --> 00:14:09.300]   And next to the ablation studies which are done on text compression using NVK8 dataset,
[00:14:09.300 --> 00:14:16.260]   we compared each of the modifications with baseline model and here you can see validation
[00:14:16.260 --> 00:14:26.020]   bits per character for shared QK transformer and for reversible transformer on this task.
[00:14:26.020 --> 00:14:35.620]   And for both experiments, we observed worse performance as compared to baseline.
[00:14:35.620 --> 00:14:44.360]   Also we did experiments with encoder-decorer architecture comparing classic and reversible
[00:14:44.360 --> 00:14:55.300]   transformers on WMT14 and the results are consistent with those observed on NVK8.
[00:14:55.300 --> 00:15:03.940]   And there are a number of possible explanations for the discrepancy of results we observed
[00:15:03.940 --> 00:15:06.620]   with the paper.
[00:15:06.620 --> 00:15:14.780]   One of the most obvious is it might be that classic model is just superior to the modified
[00:15:14.780 --> 00:15:22.780]   ones, but also as you saw in our experiments, the training did not fully converge.
[00:15:22.780 --> 00:15:33.060]   So maybe given enough time, the models will end up with the similar final score.
[00:15:33.060 --> 00:15:41.740]   And also we put substantial effort to obtaining a strong baseline and we had restricted time
[00:15:41.740 --> 00:15:47.540]   for doing supplementary experiments on the modified models.
[00:15:47.540 --> 00:15:57.780]   And also we used the hyperparameters from the paper and original source code.
[00:15:57.780 --> 00:16:05.340]   There are a number of possible numerical differences in the framework or subtle implementation
[00:16:05.340 --> 00:16:15.220]   details which result in some impact on the performance of the models.
[00:16:15.220 --> 00:16:24.980]   Also as shown before, the random factor might play some role here and it might be just due
[00:16:24.980 --> 00:16:28.980]   to some random variation.
[00:16:28.980 --> 00:16:37.780]   Also we compared the performance of LSH attention with a different number of hashing rounds.
[00:16:37.780 --> 00:16:47.220]   And as claimed in the paper, the LSH attention with 8 hashing rounds performed very similarly
[00:16:47.220 --> 00:16:49.780]   to the full attention.
[00:16:49.780 --> 00:16:58.740]   And in our experiments, 4 hashing rounds were enough to reach similar results, but with
[00:16:58.740 --> 00:17:02.900]   somewhat slower convergence.
[00:17:02.900 --> 00:17:07.100]   So that's all with the experiments done.
[00:17:07.100 --> 00:17:15.140]   And I also wanted to highlight some tools which made our reproducibility experience
[00:17:15.140 --> 00:17:17.820]   much smoother.
[00:17:17.820 --> 00:17:23.500]   NBdev is a tool for library development using Jupyter notebooks.
[00:17:23.500 --> 00:17:35.700]   It allows automatic code export to Py files, documentation auto-generation, writing tests
[00:17:35.700 --> 00:17:43.020]   in the notebook and using them for continuous integration with GitHub as well as running
[00:17:43.020 --> 00:17:44.180]   locally.
[00:17:44.180 --> 00:17:48.500]   So just a great thing to have in your toolkit.
[00:17:48.500 --> 00:17:57.740]   And also, weights and biases were really useful for fine-tuning hyperparameters and debugging
[00:17:57.740 --> 00:18:08.780]   some subtle things when you can examine all the training dynamics and gradients during
[00:18:08.780 --> 00:18:10.640]   training.
[00:18:10.640 --> 00:18:13.180]   That might be really useful.
[00:18:13.180 --> 00:18:19.180]   Also it's really useful when working in distributed teams such as ours.
[00:18:19.180 --> 00:18:25.620]   And it's really a pure joy to watch a bunch of runs progressing simultaneously when you're
[00:18:25.620 --> 00:18:33.700]   done with your implementation and only running experiments is what's left to do.
[00:18:33.700 --> 00:18:40.660]   So to summarize, we were able to validate the memory and time complexity reduction is
[00:18:40.660 --> 00:18:51.620]   a reformer and it indeed allows to train models on unprecedentedly long sequences.
[00:18:51.620 --> 00:19:02.300]   Also the performance figures were not as good as in the paper in our experiments and results
[00:19:02.300 --> 00:19:12.340]   from other papers also shown that the reformer did not perform just as good as a classic
[00:19:12.340 --> 00:19:14.100]   transformer.
[00:19:14.100 --> 00:19:23.180]   And this may be attributed to the fact that the reformer implementation is technically
[00:19:23.180 --> 00:19:29.940]   elaborated and might be difficult to reproduce correctly.
[00:19:29.940 --> 00:19:36.140]   So on that, thank you for attention and we can proceed to questions.
[00:19:36.140 --> 00:19:37.540]   >> Great.
[00:19:37.540 --> 00:19:42.460]   So thanks so much for sharing your work, Arto.
[00:19:42.460 --> 00:19:49.460]   And yeah, the other folks in the team also want to pop up on video for just a few questions.
[00:19:49.460 --> 00:19:57.780]   The thing I wanted to start out with was asking -- so you mentioned that you had like a pretty
[00:19:57.780 --> 00:20:04.020]   big team or that WasteSpices was really helpful for your team and I noticed that you had quite
[00:20:04.020 --> 00:20:06.900]   a large team of people on this project.
[00:20:06.900 --> 00:20:11.940]   So could you talk a little bit just about how you spread work around the team?
[00:20:11.940 --> 00:20:16.500]   Like what different people were working on, how you coordinated as what appears to be
[00:20:16.500 --> 00:20:23.420]   like kind of a worldwide and large team of ML engineers?
[00:20:23.420 --> 00:20:28.300]   >> Yeah, I could maybe take that.
[00:20:28.300 --> 00:20:36.460]   Yeah, it was actually a challenging part of it that we mentioned in our report as well,
[00:20:36.460 --> 00:20:41.820]   kind of structuring it in a way that everyone could work on something and didn't have to
[00:20:41.820 --> 00:20:46.260]   ping someone else to ask what they should be working on.
[00:20:46.260 --> 00:20:52.140]   So we used -- initially it was all in Discord and it was just a lot of messages and pinned
[00:20:52.140 --> 00:20:54.300]   messages and task lists.
[00:20:54.300 --> 00:20:59.860]   We moved to Trello, which worked better, but also requires a lot of maintenance and really
[00:20:59.860 --> 00:21:07.620]   requires a kind of a champion to make sure everyone's moving their cards along the board.
[00:21:07.620 --> 00:21:13.900]   So in terms of task management, I don't think -- maybe you might disagree or maybe not.
[00:21:13.900 --> 00:21:16.260]   I think we'd like to try GitHub projects maybe.
[00:21:16.260 --> 00:21:19.740]   I don't think Trello is the right solution for us.
[00:21:19.740 --> 00:21:27.740]   But yeah, basically we were trying to find a way to make it so that team members who
[00:21:27.740 --> 00:21:33.420]   weren't able to commit a lot of time could jump in at any point, see a task that needed
[00:21:33.420 --> 00:21:39.660]   to be done and go run with it without too much input from anyone else.
[00:21:39.660 --> 00:21:42.180]   So open to hear any potential solutions.
[00:21:42.180 --> 00:21:45.740]   I don't know, Arto, if you have anything else to add?
[00:21:45.740 --> 00:21:56.820]   Yeah, definitely having this task management tools integrated into your workflow flawlessly
[00:21:56.820 --> 00:22:01.780]   is really important for such team as ours.
[00:22:01.780 --> 00:22:12.900]   We had a bunch of guys who wanted to participate, but sometimes it was difficult to effectively
[00:22:12.900 --> 00:22:17.700]   allocate the tasks.
[00:22:17.700 --> 00:22:25.980]   Anyway, I think it was great learning experiments for everyone involved.
[00:22:25.980 --> 00:22:35.540]   But there is definitely a place to improve in terms of this project management.
[00:22:35.540 --> 00:22:43.980]   Yes, I also think, as you mentioned, NBDev was helpful in this respect.
[00:22:43.980 --> 00:22:56.940]   So having a proper development process with Git branching pull requests and having a better
[00:22:56.940 --> 00:23:01.060]   development process also helped a lot.
[00:23:01.060 --> 00:23:06.940]   The first part of the project was a bit chaotic and we might possibly have a repo that we
[00:23:06.940 --> 00:23:12.100]   had to leave behind and move to a new one.
[00:23:12.100 --> 00:23:14.180]   It happens, definitely.
[00:23:14.180 --> 00:23:19.140]   It's interesting, yeah, how much that coordination problem rears its head once you have a group
[00:23:19.140 --> 00:23:20.140]   of people.
[00:23:20.140 --> 00:23:21.140]   Deganta had a question.
[00:23:21.140 --> 00:23:24.140]   Deganta, if you want to speak up.
[00:23:24.140 --> 00:23:25.140]   Yeah.
[00:23:25.140 --> 00:23:29.460]   First of all, awesome presentation.
[00:23:29.460 --> 00:23:36.380]   I have been following the space of transformers and obviously it has been in the trends lately
[00:23:36.380 --> 00:23:39.740]   in literally every domain, not just NLP, but in CV as well.
[00:23:39.740 --> 00:23:46.580]   So just a general question, more from an academic perspective, you stated that the results you
[00:23:46.580 --> 00:23:53.740]   obtained from running the experiments with performer weren't as good as the classic transformer
[00:23:53.740 --> 00:23:54.740]   model.
[00:23:54.740 --> 00:24:02.380]   Do you have any plans of extending some more investigations into why and maybe do you have
[00:24:02.380 --> 00:24:08.020]   any future ideas around taking inspiration from what performer did and maybe coming up
[00:24:08.020 --> 00:24:09.780]   with something of your own?
[00:24:09.780 --> 00:24:18.580]   I think that would be really a cherry on the cake to actually be able to manage that.
[00:24:18.580 --> 00:24:19.580]   Well, yeah.
[00:24:19.580 --> 00:24:26.620]   Actually, like a couple of directions it may progress.
[00:24:26.620 --> 00:24:34.700]   First off, we like after finishing this reproducibility challenge, we created a repo to implement
[00:24:34.700 --> 00:24:44.500]   more modern techniques related to improving the efficiency of transformers in different
[00:24:44.500 --> 00:24:45.500]   directions.
[00:24:45.500 --> 00:24:56.940]   Also, like this reversible layers might be a really cool trick as they are kind of more
[00:24:56.940 --> 00:25:04.540]   efficient for particularly transformer architectures as compared to regular gradient checkpoint,
[00:25:04.540 --> 00:25:05.900]   for example.
[00:25:05.900 --> 00:25:14.300]   And it would be great if we could make it work just as good as the regular residual
[00:25:14.300 --> 00:25:19.660]   layers, although they are not quite mathematically equivalent.
[00:25:19.660 --> 00:25:26.660]   So it is possible that there would be some difference in implementation needed to make
[00:25:26.660 --> 00:25:29.820]   it work just as good.
[00:25:29.820 --> 00:25:30.820]   So yeah.
[00:25:30.820 --> 00:25:41.500]   And there is really a lot of new research published almost, I can say, every week in
[00:25:41.500 --> 00:25:44.340]   respect of the efficient attention.
[00:25:44.340 --> 00:25:50.580]   And I think it's really fascinating because now transformers are widely applied to task
[00:25:50.580 --> 00:26:02.500]   as an NLP and sometimes really very long context is necessary to achieve great performance.
[00:26:02.500 --> 00:26:08.740]   I wanted to talk a little bit more about this gap between the results that you found and
[00:26:08.740 --> 00:26:11.060]   the results reported in the paper.
[00:26:11.060 --> 00:26:14.660]   It seems very plausible to me.
[00:26:14.660 --> 00:26:18.200]   I think all the explanations you put out there were very plausible.
[00:26:18.200 --> 00:26:22.780]   So if you had to pick one explanation to go with, is there one that really stands out
[00:26:22.780 --> 00:26:27.540]   in your mind as the most plausible or do you think that it's too uncertain?
[00:26:27.540 --> 00:26:37.060]   Well, I don't know how others would say, but I would go with some subtle implementation
[00:26:37.060 --> 00:26:38.060]   details.
[00:26:38.060 --> 00:26:44.900]   So actually we had some fun time, like fun weekend with Morgan trying to debug really
[00:26:44.900 --> 00:26:52.660]   a minor scene, which is, it doesn't matter mathematically, but you need to fix it to
[00:26:52.660 --> 00:26:54.660]   really make a model.
[00:26:54.660 --> 00:27:01.540]   And the performance difference was really consistent while it just does not follow from
[00:27:01.540 --> 00:27:02.540]   the mathematics.
[00:27:02.540 --> 00:27:09.900]   And it's really hard to find this information of why does it happen.
[00:27:09.900 --> 00:27:16.020]   Also there is a variant of implementation of efficient attention, which is not very
[00:27:16.020 --> 00:27:20.460]   time efficient, but memory efficient, which is done in chunks.
[00:27:20.460 --> 00:27:29.900]   And when done in PyTorch, it works while being just exactly mathematically equivalent to
[00:27:29.900 --> 00:27:36.060]   regular attention, but it works just worse out of the box.
[00:27:36.060 --> 00:27:41.460]   And I'm pretty sure it's possible to make it work just as well as full attention, but
[00:27:41.460 --> 00:27:46.140]   it requires some fiddling with this numerical scenes.
[00:27:46.140 --> 00:27:54.940]   And I would most probably attribute the differences we observed to these factors.
[00:27:54.940 --> 00:27:55.940]   Yes.
[00:27:55.940 --> 00:28:03.740]   And also the reformer is sort of a much more finicky architecture than the transformer
[00:28:03.740 --> 00:28:05.860]   itself.
[00:28:05.860 --> 00:28:09.380]   So I think also it's much harder to tune it properly.
[00:28:09.380 --> 00:28:14.820]   So I guess if you spent more time and compute on proper tuning, we might have been able
[00:28:14.820 --> 00:28:19.500]   to squeeze extra performance out of it.
[00:28:19.500 --> 00:28:20.620]   That's interesting.
[00:28:20.620 --> 00:28:26.500]   I think, yeah, that kind of points often there's a subtle phenomenon over the course
[00:28:26.500 --> 00:28:34.580]   of years usually where architectures and techniques that are really robust to implementation noise,
[00:28:34.580 --> 00:28:38.540]   architectures and ideas that find themselves in a very smooth part of the landscape of
[00:28:38.540 --> 00:28:47.460]   ideas end up being a lot more successful than ones that even though they work very well,
[00:28:47.460 --> 00:29:00.740]   if the landscape around them is very sharp, kind of like bad local minima or something.
[00:29:00.740 --> 00:29:06.060]   I also noticed that one of the things about the reformer model that seems particularly
[00:29:06.060 --> 00:29:10.460]   kind of strange to me, and I noticed it was also in the reviews on OpenReview, is this
[00:29:10.460 --> 00:29:13.980]   like shared key query component of it.
[00:29:13.980 --> 00:29:19.260]   Could you talk a little bit more about why that's necessary and whether that's maybe
[00:29:19.260 --> 00:29:23.660]   an important part of this performance gap?
[00:29:23.660 --> 00:29:30.780]   So first, probably the most simple question, why is it necessary?
[00:29:30.780 --> 00:29:40.580]   Well, actually, as it was shown for locality-sensitive hashing to work, you want to use both keys
[00:29:40.580 --> 00:29:45.140]   and queries for the same token to end up in the same bucket because attention is only
[00:29:45.140 --> 00:29:47.620]   computed within the bucket.
[00:29:47.620 --> 00:29:55.660]   And this is like this necessitates this shared computation.
[00:29:55.660 --> 00:30:08.780]   Also like probably multiple recent papers shown that in some way, transformers might
[00:30:08.780 --> 00:30:16.420]   be and particularly attention might be viewed as like lookup tables.
[00:30:16.420 --> 00:30:29.940]   So we do lookups into the data, not directly in SQL way, but more like to some stored representation
[00:30:29.940 --> 00:30:31.220]   of the data.
[00:30:31.220 --> 00:30:41.860]   And basically, this view on the attention can give you an intuition why shared QK should
[00:30:41.860 --> 00:30:44.020]   work.
[00:30:44.020 --> 00:30:52.340]   On the other hand, when we share a QK, obviously, by far the highest attention score would be
[00:30:52.340 --> 00:30:54.980]   for the token to attend to itself.
[00:30:54.980 --> 00:31:03.460]   That is why in reformer, the self-attention is masked.
[00:31:03.460 --> 00:31:11.620]   So the token is not allowed to attend to itself, but only when it can not attend to anywhere
[00:31:11.620 --> 00:31:13.340]   else.
[00:31:13.340 --> 00:31:25.420]   And as transformer architecture is done, this makes like this disallowing self-attention
[00:31:25.420 --> 00:31:33.140]   might make a flow of positional information a bit more difficult through the models.
[00:31:33.140 --> 00:31:36.460]   So that's my idea about that.
[00:31:36.460 --> 00:31:46.940]   And probably that's why we observe such like strange results in regards to like effect
[00:31:46.940 --> 00:31:53.340]   of random seed on that synthetic experiment we showed.
[00:31:53.340 --> 00:32:04.460]   Because that synthetic experiment particularly requires this good attention to positional
[00:32:04.460 --> 00:32:05.460]   information.
[00:32:05.460 --> 00:32:11.540]   So we just need to copy a bit from particular position into the output.
[00:32:11.540 --> 00:32:20.020]   And for shared QK, that turned out to be like more difficult.
[00:32:20.020 --> 00:32:21.020]   >> That's interesting.
[00:32:21.020 --> 00:32:26.300]   And yeah, that's a neat connection to make between that result that you had on the synthetic
[00:32:26.300 --> 00:32:31.180]   task and the issues that you saw on the more realistic tasks.
[00:32:31.180 --> 00:32:33.500]   All right.
[00:32:33.500 --> 00:32:39.420]   So I could talk about this experiment all day, but unfortunately we've got to move on
[00:32:39.420 --> 00:32:43.660]   to our -- well, unfortunately we've got to close this off, but fortunately it's to move
[00:32:43.660 --> 00:32:45.700]   on to our next experiment.
[00:32:45.700 --> 00:32:51.380]   So thanks, Arto, Holvar, Morgan, and everybody else in the fast.ai community who worked on
[00:32:51.380 --> 00:32:52.380]   that project.
[00:32:52.380 --> 00:32:53.980]   Thanks so much for your work.
[00:32:53.980 --> 00:32:57.300]   >> Thank you for inviting us.
[00:32:57.300 --> 00:33:01.140]   And thanks, everyone, for your attention.
[00:33:01.140 --> 00:33:02.740]   All right.
[00:33:02.740 --> 00:33:13.940]   So now let us switch over to -- we have Varun and Rajat here who worked on the reproducing
[00:33:13.940 --> 00:33:19.340]   Utku-Evchee's paper on the Wriggle method.
[00:33:19.340 --> 00:33:23.220]   Varun and Rajat, are you here?
[00:33:23.220 --> 00:33:27.580]   >> Yeah, we are here.
[00:33:27.580 --> 00:33:29.140]   And we can start if you like.
[00:33:29.140 --> 00:33:30.220]   >> Yeah, yeah.
[00:33:30.220 --> 00:33:31.220]   Go ahead.
[00:33:31.220 --> 00:33:32.220]   >> All right.
[00:33:32.220 --> 00:33:33.220]   Thank you so much.
[00:33:33.220 --> 00:33:34.220]   Let me start.
[00:33:34.220 --> 00:33:37.780]   Just let me know if you can see my screen, then I'll begin.
[00:33:37.780 --> 00:33:38.780]   >> Yep, go for it.
[00:33:38.780 --> 00:33:39.780]   >> All right.
[00:33:39.780 --> 00:33:40.780]   Thank you.
[00:33:40.780 --> 00:33:41.780]   So hi, everyone.
[00:33:41.780 --> 00:33:45.780]   I'm Varun, and I'll be presenting our recent reproducibility work.
[00:33:45.780 --> 00:33:50.100]   I'd like to first thank Charles and the WNB team for hosting us today.
[00:33:50.100 --> 00:33:53.100]   I'm also joined by my co-author, Rajat.
[00:33:53.100 --> 00:33:54.100]   >> Yeah.
[00:33:54.100 --> 00:33:55.100]   Hi, everybody.
[00:33:55.100 --> 00:33:56.100]   I'm Rajat.
[00:33:56.100 --> 00:33:57.100]   Yeah, Varun.
[00:33:57.100 --> 00:34:09.900]   >> And both of us graduated last August from the Indian Institute of Technology, Madras.
[00:34:09.900 --> 00:34:11.220]   So let's begin.
[00:34:11.220 --> 00:34:16.820]   A very prominent characteristic of deep networks today is their incredible over-parameterization.
[00:34:16.820 --> 00:34:21.500]   Routinely, deep networks have parameter counts for exceeding data points.
[00:34:21.500 --> 00:34:24.860]   For instance, on the right, we have three models trained on ImageNet.
[00:34:24.860 --> 00:34:28.180]   Each of their parameter counts is clearly greater than a million, which is the size
[00:34:28.180 --> 00:34:29.520]   of ImageNet.
[00:34:29.520 --> 00:34:33.740]   This trend continues to grow today, especially with newer models such as transformers, especially
[00:34:33.740 --> 00:34:39.300]   the one -- we just saw transformers from the previous team.
[00:34:39.300 --> 00:34:45.620]   Compressing deep networks is therefore an important topic for both training and deployment.
[00:34:45.620 --> 00:34:48.220]   Sparse networks are a compelling choice to achieve this.
[00:34:48.220 --> 00:34:53.580]   Unlike a dense network, most of the sparse network's weights are set to zero.
[00:34:53.580 --> 00:34:57.380]   Sparse networks can be computationally and memory efficient.
[00:34:57.380 --> 00:35:01.500]   They can make it possible to train larger models with limited hardware.
[00:35:01.500 --> 00:35:07.020]   Typically, sparse networks are obtained via pruning, a process where you iteratively remove
[00:35:07.020 --> 00:35:09.020]   small magnitude weights.
[00:35:09.020 --> 00:35:13.980]   Unfortunately, pruning almost always degrades performance.
[00:35:13.980 --> 00:35:18.380]   In light of this, I'd like to introduce the lottery ticket hypothesis, which I could say
[00:35:18.380 --> 00:35:25.540]   is arguably one of the most interesting yet accessible works in recent sparse learning.
[00:35:25.540 --> 00:35:31.860]   So the idea is to identify sparse networks, or we call them tickets, that can be trained
[00:35:31.860 --> 00:35:36.340]   to attain performance similar to the original dense network, so you don't lose performance.
[00:35:36.340 --> 00:35:38.900]   The trick lies in how these networks are initialized.
[00:35:38.900 --> 00:35:41.940]   To do so, we first train a dense network and prune it.
[00:35:41.940 --> 00:35:48.140]   We now take this sparse network and reinitialize it with the original untrained dense weights.
[00:35:48.140 --> 00:35:53.380]   Unfortunately, this whole process is costlier than a dense network and sort of nullifies
[00:35:53.380 --> 00:35:56.900]   the computational benefit of sparse networks.
[00:35:56.900 --> 00:35:59.780]   So we want to do something better than this.
[00:35:59.780 --> 00:36:03.220]   Riggle, short for rigging the lottery, tries to do something entirely different.
[00:36:03.220 --> 00:36:09.080]   They try to directly train a sparse network from scratch instead of pruning a dense network.
[00:36:09.080 --> 00:36:11.340]   And Riggle achieves this by two steps.
[00:36:11.340 --> 00:36:16.320]   The first, every certain number of steps, they prune a number of weights and they grow
[00:36:16.320 --> 00:36:18.800]   the exact number of weights back.
[00:36:18.800 --> 00:36:23.440]   More importantly, Riggle does not alter the number of non-zero weights in each layer.
[00:36:23.440 --> 00:36:26.800]   So if you remove two weights in a layer, you always add back two weights, but you may add
[00:36:26.800 --> 00:36:28.840]   it at a different place.
[00:36:28.840 --> 00:36:33.280]   This ensures that the floating point operations of the network are always constant, which
[00:36:33.280 --> 00:36:35.480]   is something that we desire.
[00:36:35.480 --> 00:36:39.040]   So let us talk briefly about the connection updates.
[00:36:39.040 --> 00:36:45.160]   And more specifically, Riggle prunes the smallest magnitude weights, which is typical of pruning.
[00:36:45.160 --> 00:36:49.520]   And for growing, we grow the largest gradient magnitude.
[00:36:49.520 --> 00:36:53.160]   The growth strategy is exactly where Riggle differs from other methods.
[00:36:53.160 --> 00:36:58.920]   For instance, set grows weights back randomly and SNFS uses the largest momentum, where
[00:36:58.920 --> 00:37:01.520]   momentum is your accumulated gradient.
[00:37:01.520 --> 00:37:08.640]   SNFS, in addition to this, also tends to redistribute layer-wise sparsity throughout training, which
[00:37:08.640 --> 00:37:12.000]   results in non-constant flops.
[00:37:12.000 --> 00:37:16.480]   As a result of the growth strategies, these three methods have very different gradient
[00:37:16.480 --> 00:37:17.480]   requirements.
[00:37:17.480 --> 00:37:20.840]   For instance, set does not require dense gradients throughout training.
[00:37:20.840 --> 00:37:24.360]   You can get away by supplying only sparse gradients.
[00:37:24.360 --> 00:37:29.720]   Riggle requires access to dense gradients every k steps or so when you perform this
[00:37:29.720 --> 00:37:31.600]   topology update.
[00:37:31.600 --> 00:37:35.520]   SNFS, on the other hand, requires dense gradients throughout training.
[00:37:35.520 --> 00:37:39.000]   So the only benefit you get from SNFS is the sparse forward pass.
[00:37:39.000 --> 00:37:44.080]   You don't really get any benefit through the backward pass.
[00:37:44.080 --> 00:37:46.280]   Let's now move on to the main claims Riggle makes.
[00:37:46.280 --> 00:37:48.400]   Riggle makes two claims.
[00:37:48.400 --> 00:37:52.760]   The first is that it outperforms existing direct pass techniques, which is basically
[00:37:52.760 --> 00:37:57.200]   set and SNFS, compared at the same parameter and flop count.
[00:37:57.200 --> 00:37:59.240]   The second claim is the more interesting one.
[00:37:59.240 --> 00:38:02.000]   It matches or exceeds pruning when trained longer.
[00:38:02.000 --> 00:38:06.720]   Now, this is important because previously, all these direct pass techniques were always
[00:38:06.720 --> 00:38:09.200]   inferior to pruning.
[00:38:09.200 --> 00:38:13.920]   And Riggle manages all of this while maintaining constant training flops.
[00:38:13.920 --> 00:38:15.840]   This is also where we come in.
[00:38:15.840 --> 00:38:21.980]   We reproduce Riggle from scratch in PyTorch and we open source our code on GitHub.
[00:38:21.980 --> 00:38:25.040]   So let me go over the claim validity.
[00:38:25.040 --> 00:38:26.880]   I'll give you the results right away.
[00:38:26.880 --> 00:38:28.480]   Like there's no suspense.
[00:38:28.480 --> 00:38:31.760]   The claims are true, which is nice.
[00:38:31.760 --> 00:38:33.880]   But it's not true for parameter count alone.
[00:38:33.880 --> 00:38:38.560]   On this graph, you can clearly see that SNFS outperforms Riggle for the same parameter
[00:38:38.560 --> 00:38:39.560]   count.
[00:38:39.560 --> 00:38:41.320]   You also need to consider flop count.
[00:38:41.320 --> 00:38:46.160]   When considering flop count, Riggle performs the best training of parameter count and flop
[00:38:46.160 --> 00:38:47.160]   count.
[00:38:47.160 --> 00:38:52.200]   Now, the more interesting claim is does it perform better than pruning when trained longer?
[00:38:52.200 --> 00:38:53.600]   And the answer is yes.
[00:38:53.600 --> 00:38:58.280]   You can see that when we trained Riggle twice as long to match the computational cost of
[00:38:58.280 --> 00:39:05.920]   iterative pruning, we can achieve performance that is equal or slightly greater.
[00:39:05.920 --> 00:39:09.020]   So Riggle, so far we've seen a lot of good stuff.
[00:39:09.020 --> 00:39:10.880]   So let's see a few caveats.
[00:39:10.880 --> 00:39:16.400]   The first interesting caveat is that the initialization matters more than the method.
[00:39:16.400 --> 00:39:18.360]   What do I mean by initialization?
[00:39:18.360 --> 00:39:22.720]   So we start off with a sparse network and I still haven't given you a recipe of how
[00:39:22.720 --> 00:39:25.680]   to get that sparse network.
[00:39:25.680 --> 00:39:27.140]   Initialization is exactly that.
[00:39:27.140 --> 00:39:29.400]   The authors used two initializations.
[00:39:29.400 --> 00:39:33.920]   The first is called random, where the sparsity of each layer is set constant.
[00:39:33.920 --> 00:39:38.180]   It's equal to the global sparsity, except for the first layer, which they keep dense.
[00:39:38.180 --> 00:39:44.020]   The other initialization strategy is called the Erdos-Renyi kernel or ERK for short.
[00:39:44.020 --> 00:39:46.260]   And here the sparsity is nonlinear.
[00:39:46.260 --> 00:39:52.280]   It tends to allocate more density or less sparsity for layers with smaller capacities.
[00:39:52.280 --> 00:39:58.860]   So you'll see all these shortcut layers which have nearly full density.
[00:39:58.860 --> 00:40:02.780]   Let's see what the implication of these initialization methods are.
[00:40:02.780 --> 00:40:07.140]   On both datasets, we make comparisons on CIFAR-10 and CIFAR-100.
[00:40:07.140 --> 00:40:11.780]   Switching to ERK initialization for any method always boosts performance.
[00:40:11.780 --> 00:40:18.260]   And the surprising aspect is that this performance boost is often greater than swapping methods.
[00:40:18.260 --> 00:40:23.140]   So for instance, if I were to swap between SCT and Riggle, I may get a performance gain
[00:40:23.140 --> 00:40:28.740]   of say 0.2 accuracy percentage or 0.3 accuracy percentage.
[00:40:28.740 --> 00:40:33.900]   But if I swap from random to ERK, I'm going to get a 1% increase.
[00:40:33.900 --> 00:40:38.860]   This sort of gives you -- this sort of puts in perspective whether you should be focusing
[00:40:38.860 --> 00:40:45.820]   your research on new algorithms or you should be focusing on new initialization strategies.
[00:40:45.820 --> 00:40:52.140]   The second caveat is so far we have spoken about all these nice flaw benefits while maintaining
[00:40:52.140 --> 00:40:53.220]   accuracy.
[00:40:53.220 --> 00:40:57.380]   But the truth is you have no real benefits on current hardware.
[00:40:57.380 --> 00:40:59.140]   So why is that?
[00:40:59.140 --> 00:41:05.100]   The reason is CUDA does not efficiently support sparse operations at this point.
[00:41:05.100 --> 00:41:09.200]   CUDA does support certain kind of sparse operations which are block sparse.
[00:41:09.200 --> 00:41:12.560]   That is they have a certain structure to them.
[00:41:12.560 --> 00:41:14.860]   But in our case, we do not have any structure.
[00:41:14.860 --> 00:41:17.700]   So CUDA doesn't really offer us any speedups.
[00:41:17.700 --> 00:41:20.240]   This is in fact an ongoing research direction.
[00:41:20.240 --> 00:41:24.740]   So we might see things change in a few years from now.
[00:41:24.740 --> 00:41:30.560]   But the more -- the more positive news is that there are a class of processors such
[00:41:30.560 --> 00:41:35.460]   as the GraphCore which support this really, really efficiently.
[00:41:35.460 --> 00:41:40.540]   On a GraphCore, having a sparse network would give you the exact theoretical benefits that
[00:41:40.540 --> 00:41:42.800]   we have stated so far.
[00:41:42.800 --> 00:41:47.540]   These processors are completely suited for fine-grained parallelism.
[00:41:47.540 --> 00:41:52.460]   So they can perform tasks that require really different kind of workloads.
[00:41:52.460 --> 00:41:57.020]   They won't suffer any performance degradation like a GPU.
[00:41:57.020 --> 00:42:00.560]   And in fact, there's a regal implementation on this on GraphCore.
[00:42:00.560 --> 00:42:02.300]   And you can find it in GraphCore examples.
[00:42:02.300 --> 00:42:06.740]   This is one thing that I really liked about Regal because they have actually gone ahead
[00:42:06.740 --> 00:42:13.700]   and tried to test their algorithm on a processor which supports sparse implementation.
[00:42:13.700 --> 00:42:17.980]   The third caveat is quite actually related to the first.
[00:42:17.980 --> 00:42:19.340]   So what do we actually care about?
[00:42:19.340 --> 00:42:22.860]   Do we care about flop count or do we care about parameter count?
[00:42:22.860 --> 00:42:27.880]   I think a couple of slides ago I said that ERK is better than random when you're considering
[00:42:27.880 --> 00:42:29.300]   a fixed parameter count.
[00:42:29.300 --> 00:42:31.540]   Why don't we invert that question?
[00:42:31.540 --> 00:42:34.820]   What happens if you consider a fixed flop count instead?
[00:42:34.820 --> 00:42:37.140]   We get a very surprising result.
[00:42:37.140 --> 00:42:41.340]   That is ERK is worse than random on a per flop basis.
[00:42:41.340 --> 00:42:47.620]   Now if you put together caveat 1 and 2, it means that before you come up with a new method,
[00:42:47.620 --> 00:42:53.580]   like say Regal set SNFS or a new one, or a novel initialization scheme, you need to clearly
[00:42:53.580 --> 00:42:55.420]   define what your goals are.
[00:42:55.420 --> 00:43:00.260]   If you care only about flop count, like say you want computational benefits alone, you
[00:43:00.260 --> 00:43:04.780]   don't really care about storage benefits, then maybe you should be doing something different.
[00:43:04.780 --> 00:43:07.220]   All right.
[00:43:07.220 --> 00:43:12.780]   I'll briefly describe a few additional findings we found in our reproducibility work.
[00:43:12.780 --> 00:43:17.180]   The first is that Regal is quite robust to its choice of hyperparameters.
[00:43:17.180 --> 00:43:18.380]   What do I mean by robust?
[00:43:18.380 --> 00:43:23.820]   I mean that the two parameters Regal requires, which is this alpha and delta t, where alpha
[00:43:23.820 --> 00:43:28.900]   is your pruning rate and delta t is the update frequency, the frequency at which you will
[00:43:28.900 --> 00:43:34.060]   perform these topology updates, can be set largely independent of both sparsity and learning
[00:43:34.060 --> 00:43:35.060]   rate.
[00:43:35.060 --> 00:43:40.340]   Which is nice because now you don't have to tune n plus 2 parameters, you can set these
[00:43:40.340 --> 00:43:45.460]   two parameters independent of everything else and just tune your remaining parameters.
[00:43:45.460 --> 00:43:48.700]   This does not add a burden on a practitioner.
[00:43:48.700 --> 00:43:52.440]   Regal can benefit from redistribution.
[00:43:52.440 --> 00:43:58.860]   So recall that I mentioned that in SNFS you have this concept of redistribution where
[00:43:58.860 --> 00:44:01.260]   you move sparsity across layers.
[00:44:01.260 --> 00:44:04.780]   That is, sparsity for a layer does not remain fixed throughout training.
[00:44:04.780 --> 00:44:08.020]   So we asked ourselves the question, what if you incorporate that in Regal?
[00:44:08.020 --> 00:44:13.580]   So it turns out that it can bridge the performance gap between ERK and random, but it considerably
[00:44:13.580 --> 00:44:20.820]   shoots up the computational cost and probably isn't the way to go about bridging the gap
[00:44:20.820 --> 00:44:24.060]   between ERK and random.
[00:44:24.060 --> 00:44:29.140]   You can find more details about these experiments on our paper.
[00:44:29.140 --> 00:44:35.140]   So let me now present a one slide summary of our RC2020 journey.
[00:44:35.140 --> 00:44:41.700]   So our initial impression was that each run on Sifaten usually took us two, three hours,
[00:44:41.700 --> 00:44:47.780]   at least from the TensorFlow repository that Utku and authors had open sourced.
[00:44:47.780 --> 00:44:52.260]   So we expected that we'd probably take around two, three weeks to wrap up.
[00:44:52.260 --> 00:44:57.340]   In reality, with all the configurations considered, the hyperparameters, it took us a couple of
[00:44:57.340 --> 00:44:58.480]   months.
[00:44:58.480 --> 00:45:01.280]   So that was a good thing to remember.
[00:45:01.280 --> 00:45:03.420]   So a brief timeline.
[00:45:03.420 --> 00:45:08.420]   We spent, I think, the entirety of October getting our implementation correct.
[00:45:08.420 --> 00:45:12.660]   There were always points where we figured out that we had missed something, especially
[00:45:12.660 --> 00:45:17.860]   pertaining to sparsity of the gradient and momentum buffers.
[00:45:17.860 --> 00:45:26.700]   We then spent a couple of months experimenting on Sifaten and SIFAR-100, as well as conducting
[00:45:26.700 --> 00:45:29.140]   these extension experiments.
[00:45:29.140 --> 00:45:34.820]   And the main challenge in our work was the volume, the sheer volume of experiments.
[00:45:34.820 --> 00:45:39.420]   So we spent a good amount of time thinking how best to represent these experiments.
[00:45:39.420 --> 00:45:45.140]   We did not directly just want to write down a bunch of metrics.
[00:45:45.140 --> 00:45:48.620]   And here are a few goof ups.
[00:45:48.620 --> 00:45:53.980]   So there was this time when I think we had to throw out a week's worth of experimentation,
[00:45:53.980 --> 00:45:57.740]   simply because we forgot to consider random seeds.
[00:45:57.740 --> 00:46:01.820]   And it turns out that random seed was pretty important in this whole work, and we had to
[00:46:01.820 --> 00:46:05.140]   average across multiple random seeds.
[00:46:05.140 --> 00:46:06.140]   Flop counting was tricky.
[00:46:06.140 --> 00:46:15.180]   We also found that SNFS was costlier than what the Regal paper reported, which is also
[00:46:15.180 --> 00:46:19.020]   very different, which is quite unusual.
[00:46:19.020 --> 00:46:25.700]   We don't usually see a baseline being reported better than it is on a paper.
[00:46:25.700 --> 00:46:30.940]   And in terms of structure, so as I mentioned before, the main challenge in this reproduction
[00:46:30.940 --> 00:46:33.000]   was the sheer volume of experiments.
[00:46:33.000 --> 00:46:36.940]   We had to adopt certain tools to manage our runs better.
[00:46:36.940 --> 00:46:42.360]   First we found Hydra to be super useful, particularly for composable configurations.
[00:46:42.360 --> 00:46:46.460]   As you can see on the right, experimenting with different methods just involved swapping
[00:46:46.460 --> 00:46:47.580]   YAML files.
[00:46:47.580 --> 00:46:53.540]   So we could experiment with different data sets, masking strategies, optimizers.
[00:46:53.540 --> 00:46:58.540]   We also found Hydra's multi-run format and support for hyperparameter tuning, especially
[00:46:58.540 --> 00:47:03.340]   packages such as Optona, to be really useful.
[00:47:03.340 --> 00:47:08.060]   Hydra is a pretty new and extensible library, so I would highly recommend checking it out
[00:47:08.060 --> 00:47:10.900]   if you haven't used it before.
[00:47:10.900 --> 00:47:15.780]   Second we started making use of technology from I think makefiles come from the '80s
[00:47:15.780 --> 00:47:23.100]   and '90s, but it turns out they're much more useful than bash files for storing experiment
[00:47:23.100 --> 00:47:27.500]   runs, storing the commands used to run those experiments, so that you can quickly replicate
[00:47:27.500 --> 00:47:32.460]   experiments or make minor changes in case something goes wrong or you want to run variants
[00:47:32.460 --> 00:47:35.100]   of these experiments.
[00:47:35.100 --> 00:47:40.560]   Finally WNB was really useful in terms of its dashboard.
[00:47:40.560 --> 00:47:43.460]   That allowed us to easily group and visualize experiments.
[00:47:43.460 --> 00:47:46.660]   For each method, we had to run three or four seeds.
[00:47:46.660 --> 00:47:50.460]   As you can see on the right, these numbers, 3, 6, 5.
[00:47:50.460 --> 00:47:53.780]   And there were a bunch of such methods.
[00:47:53.780 --> 00:48:01.500]   So for CIFAR-10, I suppose we ran around 150 methods, different configurations of these
[00:48:01.500 --> 00:48:04.420]   masking strategies, all of that.
[00:48:04.420 --> 00:48:08.900]   And WNB's centralized dashboard allowed us to easily visualize these.
[00:48:08.900 --> 00:48:13.900]   And initially we were using TensorBoard, but the moment we started performing more experiments,
[00:48:13.900 --> 00:48:15.620]   it became super unwieldy.
[00:48:15.620 --> 00:48:21.380]   So we switched to WNB where all the runs show up in a single place, and it was a much nicer
[00:48:21.380 --> 00:48:22.380]   alternative.
[00:48:22.380 --> 00:48:24.820]   The API support also helped us plot.
[00:48:24.820 --> 00:48:29.260]   So every time we made changes to our runs, we didn't have to go and manually re-plot.
[00:48:29.260 --> 00:48:34.780]   We just probably re-ran a Python script which did our plotting.
[00:48:34.780 --> 00:48:37.500]   And here are a few takeaways.
[00:48:37.500 --> 00:48:43.380]   So the first is reproducibility can be as fun as working on new ideas, and it's certainly
[00:48:43.380 --> 00:48:46.860]   important for the ML community.
[00:48:46.860 --> 00:48:52.020]   Authors aren't adversaries and will often be interested and invested in your reproducibility
[00:48:52.020 --> 00:48:53.020]   efforts.
[00:48:53.020 --> 00:48:58.820]   They want to see that their paper actually works when used in different contexts, different
[00:48:58.820 --> 00:49:01.020]   scenarios.
[00:49:01.020 --> 00:49:03.740]   And reproducibility need not stop at replication.
[00:49:03.740 --> 00:49:09.980]   Indeed, you can discover some overlooked findings, or you can even spawn new ideas.
[00:49:09.980 --> 00:49:15.900]   We'd like to thank Utku and the other Regal authors for their responsive communication
[00:49:15.900 --> 00:49:16.900]   throughout.
[00:49:16.900 --> 00:49:20.180]   We had a pretty extensive mail thread with the original authors.
[00:49:20.180 --> 00:49:22.060]   We'd also like to credit Tim Detmers.
[00:49:22.060 --> 00:49:27.020]   We built our code base by extending his open-sourced SNFS code.
[00:49:27.020 --> 00:49:29.260]   You can find our code base on GitHub.
[00:49:29.260 --> 00:49:34.460]   You can read our report on archive, and you can also find our entire set of training experiments
[00:49:34.460 --> 00:49:36.300]   on WNB.
[00:49:36.300 --> 00:49:41.980]   So thank you so much for your time, and we'd be happy to take any questions.
[00:49:41.980 --> 00:49:44.980]   Great.
[00:49:44.980 --> 00:49:50.260]   Thanks for sharing that work with us, Varun and Rajat.
[00:49:50.260 --> 00:49:55.460]   It was also, I love the tool stack that you had there with Hydra, Weights and Biases,
[00:49:55.460 --> 00:49:56.460]   and Makefiles.
[00:49:56.460 --> 00:50:00.340]   Yeah, we didn't expect the Makefiles to show up.
[00:50:00.340 --> 00:50:06.500]   Makefiles were an ad probably in December when we started putting all our experiments
[00:50:06.500 --> 00:50:11.020]   together in one bash file or something, and it got really long.
[00:50:11.020 --> 00:50:12.020]   Yeah.
[00:50:12.020 --> 00:50:17.020]   I actually used a Makefile for my PhD dissertation.
[00:50:17.020 --> 00:50:19.580]   It was Make Thesis to set it up.
[00:50:19.580 --> 00:50:24.160]   So I appreciate the Makefile.
[00:50:24.160 --> 00:50:27.100]   So a couple of things.
[00:50:27.100 --> 00:50:32.100]   On the more science-y side, you mentioned that you implemented redistribution.
[00:50:32.100 --> 00:50:38.400]   So actually, if you could go back to that slide where you showed comparing random and
[00:50:38.400 --> 00:50:41.380]   then random with redistribution and ERK.
[00:50:41.380 --> 00:50:42.860]   Yeah.
[00:50:42.860 --> 00:50:45.860]   Let me in fact show you a bigger figure.
[00:50:45.860 --> 00:50:46.860]   That might help.
[00:50:46.860 --> 00:50:47.860]   Perfect.
[00:50:47.860 --> 00:50:55.920]   So, right, let me just share it.
[00:50:55.920 --> 00:50:59.920]   See, I just blown up the figure.
[00:50:59.920 --> 00:51:00.920]   Yeah.
[00:51:00.920 --> 00:51:02.160]   All right.
[00:51:02.160 --> 00:51:08.040]   So it looks to me a little bit like maybe it's not a perfect lineup, but it does seem
[00:51:08.040 --> 00:51:14.200]   like the redistribution methods, which are in green and red in this chart, are basically
[00:51:14.200 --> 00:51:16.600]   learning the Erdos-Renyi kernel.
[00:51:16.600 --> 00:51:19.180]   Is that accurate or am I missing something?
[00:51:19.180 --> 00:51:23.020]   So it's far more extreme than the Erdos-Renyi kernel.
[00:51:23.020 --> 00:51:24.500]   And we had the same intuition.
[00:51:24.500 --> 00:51:26.220]   We actually suspected this.
[00:51:26.220 --> 00:51:28.420]   We actually wanted to show this result.
[00:51:28.420 --> 00:51:32.780]   We were biased towards showing this result that redistribution learns an ERK-like kernel
[00:51:32.780 --> 00:51:36.020]   because that consolidates a lot of things.
[00:51:36.020 --> 00:51:40.380]   But the even more surprising result which we have is if you take this distribution and
[00:51:40.380 --> 00:51:43.380]   reinitialize a network from scratch, it doesn't learn.
[00:51:43.380 --> 00:51:44.980]   It performs really, really bad.
[00:51:44.980 --> 00:51:46.840]   It performs worse than static.
[00:51:46.840 --> 00:51:48.760]   In fact, I could pull up those results.
[00:51:48.760 --> 00:51:51.280]   So we went-
[00:51:51.280 --> 00:51:52.280]   That's interesting.
[00:51:52.280 --> 00:51:53.280]   That's wild.
[00:51:53.280 --> 00:51:55.160]   Yeah, that's really wild.
[00:51:55.160 --> 00:52:01.840]   And unfortunately, we tried a bunch of things, but we weren't really able to explain this.
[00:52:01.840 --> 00:52:03.880]   And so we had a-
[00:52:03.880 --> 00:52:05.680]   That's a very different phenomenon.
[00:52:05.680 --> 00:52:09.600]   Like the famous thing about lottery tickets, right, is that you can, if you take the-
[00:52:09.600 --> 00:52:10.600]   Reinitialize it.
[00:52:10.600 --> 00:52:13.080]   Reinitialize it, it works.
[00:52:13.080 --> 00:52:17.420]   And so you're doing the same type of reinitialization, right, where you keep the values of the weights
[00:52:17.420 --> 00:52:21.580]   the same and apply the topology at the beginning.
[00:52:21.580 --> 00:52:23.180]   We tried both, in fact.
[00:52:23.180 --> 00:52:27.540]   We tried the lottery initialization and fine-tuning-like initialization.
[00:52:27.540 --> 00:52:32.620]   That is, you keep the weights, you just resume training again.
[00:52:32.620 --> 00:52:33.620]   And both really didn't-
[00:52:33.620 --> 00:52:38.060]   The fine-tuning initialization works, but a lottery-like reinitialization does not work.
[00:52:38.060 --> 00:52:41.180]   So let me just share this real quick, if I can.
[00:52:41.180 --> 00:52:42.180]   Sure.
[00:52:42.180 --> 00:52:43.180]   Yeah.
[00:52:43.180 --> 00:52:50.740]   So one of the reasons this big table is not on the slide was because we didn't want to
[00:52:50.740 --> 00:52:53.500]   push off too many numbers onto viewers.
[00:52:53.500 --> 00:53:00.300]   So if you go to the last section, right, reinitialization with RegalSM, you will notice that the numbers
[00:53:00.300 --> 00:53:05.260]   are like 90.3 and 90.2, which is far lower than what you were getting before, 92 point
[00:53:05.260 --> 00:53:06.460]   something.
[00:53:06.460 --> 00:53:11.580]   And similarly on the CIFAR 100 column, we were getting 67-ish, and previously we were
[00:53:11.580 --> 00:53:13.660]   getting 72s.
[00:53:13.660 --> 00:53:19.460]   So if you were to take this trained initialization and perform a lottery-like reinit, it actually
[00:53:19.460 --> 00:53:22.500]   performs worse, which we found kind of surprising.
[00:53:22.500 --> 00:53:26.580]   Possible explanations are this extreme distribution.
[00:53:26.580 --> 00:53:30.180]   If you see this distribution, it's kind of extreme.
[00:53:30.180 --> 00:53:31.180]   It's higher than ERK.
[00:53:31.180 --> 00:53:34.700]   Wherever ERK goes dense, it is dense.
[00:53:34.700 --> 00:53:37.880]   And wherever ERK is passed, it is more sparse than ERK.
[00:53:37.880 --> 00:53:42.980]   So maybe this distribution is too extreme for you to reinitialize and train successfully.
[00:53:42.980 --> 00:53:46.580]   Maybe if you do some sort of a warmup before reinitializing, it might help.
[00:53:46.580 --> 00:53:51.020]   So there are these lottery ticket variants where you do a warmup and then you reinitialize
[00:53:51.020 --> 00:53:52.320]   with those weights.
[00:53:52.320 --> 00:53:53.320]   Maybe that could help.
[00:53:53.320 --> 00:53:55.820]   Again, this is just some-- I'm hazarding a guess.
[00:53:55.820 --> 00:53:56.820]   Yeah.
[00:53:56.820 --> 00:53:58.300]   Yeah, no, that does make sense.
[00:53:58.300 --> 00:54:02.460]   I have seen those results about how important it is to sort of warm up before the lottery
[00:54:02.460 --> 00:54:07.100]   for like ResNets in particular.
[00:54:07.100 --> 00:54:12.140]   And I think one more surprising aspect was that regardless-- so I could start with random
[00:54:12.140 --> 00:54:16.060]   and do redistribution, or I could start with ERK and do redistribution.
[00:54:16.060 --> 00:54:19.740]   Turns out you end up with a very similar distribution regardless of where you start.
[00:54:19.740 --> 00:54:21.820]   You could start with ERK or start with random.
[00:54:21.820 --> 00:54:23.900]   You end up with nearly the same distribution.
[00:54:23.900 --> 00:54:26.980]   And the computational cost just shoots up.
[00:54:26.980 --> 00:54:30.460]   The computational cost is not due to redistribution here.
[00:54:30.460 --> 00:54:35.820]   The computational cost is actually due to the nature of this density itself.
[00:54:35.820 --> 00:54:36.820]   Interesting.
[00:54:36.820 --> 00:54:44.980]   And SNFS also has a distribution of this kind, which is what I alluded by saying when I earlier
[00:54:44.980 --> 00:54:48.540]   said that SNFS is much costlier than it seems.
[00:54:48.540 --> 00:54:50.140]   I see.
[00:54:50.140 --> 00:54:51.740]   Interesting.
[00:54:51.740 --> 00:54:55.340]   On the topic-- actually, there is-- I was going to ask another question, but I believe
[00:54:55.340 --> 00:54:57.940]   Deganta actually has one that he wanted to ask.
[00:54:57.940 --> 00:54:58.940]   Deganta?
[00:54:58.940 --> 00:54:59.940]   Yeah.
[00:54:59.940 --> 00:55:01.900]   Yeah, I have three questions.
[00:55:01.900 --> 00:55:08.940]   And this is a space that I really follow quite extensively.
[00:55:08.940 --> 00:55:13.780]   Not work on, but I do read literature on this.
[00:55:13.780 --> 00:55:20.420]   First question is, if it is purely on deployment level, considering the difference between
[00:55:20.420 --> 00:55:27.940]   Google and SNFS, as you stated, SNFS is essentially fast and forward pass and doesn't do anything
[00:55:27.940 --> 00:55:29.140]   on backward pass.
[00:55:29.140 --> 00:55:33.700]   So if it's purely on deployment, which only concerns with the inference time or inference
[00:55:33.700 --> 00:55:38.060]   latency, which one do you think forms actually better?
[00:55:38.060 --> 00:55:41.340]   Is it regular-based networks or SNFS?
[00:55:41.340 --> 00:55:44.660]   I would recommend going with just iterative pruning in that case.
[00:55:44.660 --> 00:55:50.100]   So if you hover back to any of our tables where we have these results, you will see
[00:55:50.100 --> 00:55:54.180]   that the inference cost of pruning is proportional to the density.
[00:55:54.180 --> 00:55:57.220]   Like 0.11 is very similar to 0.1.
[00:55:57.220 --> 00:56:03.700]   SNFS, the distribution, the nature of the distribution makes a forward pass also costly.
[00:56:03.700 --> 00:56:07.480]   So for the same density, you would see like 0.38.
[00:56:07.480 --> 00:56:12.940]   So if you're concerned only about deployment, I guess iterative pruning is still your best
[00:56:12.940 --> 00:56:16.820]   bet today.
[00:56:16.820 --> 00:56:23.700]   My second follow up question is, this whole space looks pretty much similar to even or
[00:56:23.700 --> 00:56:30.260]   maybe let's say synchronous and parallel to the space of neural architecture search, especially
[00:56:30.260 --> 00:56:35.420]   when you have latency constraints or you're trying to find optimal width.
[00:56:35.420 --> 00:56:43.300]   So there's this paper by Amazon, which was at CEPR 2020, called as Adaptive and Optimal
[00:56:43.300 --> 00:56:46.020]   Network Width Search with Latency Constraints.
[00:56:46.020 --> 00:56:53.580]   And just to summarize that paper, basically it tries to find the adequate number of channels
[00:56:53.580 --> 00:57:01.420]   for each layer based on latency constraint provided.
[00:57:01.420 --> 00:57:08.860]   And there's an interesting graph in that paper, which kind of conflicts the density graph
[00:57:08.860 --> 00:57:10.980]   that you showed.
[00:57:10.980 --> 00:57:16.860]   So in that graph, apparently it shows that the latter layers, like the layers towards
[00:57:16.860 --> 00:57:24.680]   the end of the network, requires to be more dense than the earlier layers.
[00:57:24.680 --> 00:57:32.100]   In fact, even more dense than the baseline density while the earlier layers, and it's
[00:57:32.100 --> 00:57:33.780]   like linearly increasing.
[00:57:33.780 --> 00:57:44.120]   But in your density graph that you showed, the trend isn't looking similar of that sort.
[00:57:44.120 --> 00:57:55.180]   So just wanted to understand why would there be a conflict in that case, what the AOWS
[00:57:55.180 --> 00:58:00.300]   shows when they did a neural architecture search and what Riggle or any other learning
[00:58:00.300 --> 00:58:02.200]   method shows.
[00:58:02.200 --> 00:58:05.700]   So I'm not sure how to really reconcile these two facts.
[00:58:05.700 --> 00:58:12.540]   But what I do know is that the fully connected layer of both these ResNet-50s and the Wide
[00:58:12.540 --> 00:58:14.820]   ResNet-22 are set to 1.
[00:58:14.820 --> 00:58:18.020]   So the density of those fully connected layers are set to 1.
[00:58:18.020 --> 00:58:23.260]   These are really huge, like fully connected layers in VGG for instance, I think it hosts
[00:58:23.260 --> 00:58:26.420]   around 70% of the parameters and it's the last layer.
[00:58:26.420 --> 00:58:33.100]   So while it might seem that ERK and sparse gradient are actually like dipping density
[00:58:33.100 --> 00:58:39.040]   as you go deeper, for the fully connected layers they tend to allocate full density,
[00:58:39.040 --> 00:58:42.900]   that is, you do not remove any parameters from those layers.
[00:58:42.900 --> 00:58:49.020]   So maybe that can be used to reconcile, maybe we should try architectures where there is
[00:58:49.020 --> 00:58:53.460]   no fully connected layer and maybe then compare, because these fully connected layers tend
[00:58:53.460 --> 00:58:58.180]   to host a much higher number of parameters.
[00:58:58.180 --> 00:59:02.620]   So that could be a possible explanation.
[00:59:02.620 --> 00:59:09.640]   And my last question is more on your implementation, which is just a general question, I haven't
[00:59:09.640 --> 00:59:16.620]   yet come to a realization, but why did you say that makefile was more useful than a bash
[00:59:16.620 --> 00:59:19.580]   script in your case?
[00:59:19.580 --> 00:59:21.980]   So let me put it this way.
[00:59:21.980 --> 00:59:28.620]   So for instance, let's say a single run of Regal on CIFAR-10, that would involve running
[00:59:28.620 --> 00:59:39.160]   at, you had to run for densities, I think it was 0.5, 0.2, 0.1, 0.005 and two initializations.
[00:59:39.160 --> 00:59:42.160]   Then we also had these redistribution strategies.
[00:59:42.160 --> 00:59:46.540]   So it's like 2 into 2 into 5, we're already at a 20.
[00:59:46.540 --> 00:59:53.260]   And if you were to do it in a bash loop, you could technically do it through Hydra's, you
[00:59:53.260 --> 01:00:00.820]   could do it through Hydra's multi-run syntax, which is kind of neat, but it sort of constrains
[01:00:00.820 --> 01:00:02.340]   you to a single device.
[01:00:02.340 --> 01:00:06.060]   And if you have a slurm cluster to operate with, you maybe want to send some of these
[01:00:06.060 --> 01:00:07.720]   runs to different nodes.
[01:00:07.720 --> 01:00:13.140]   So we didn't really play around with Hydra's plugin for slurm clusters, but what we did
[01:00:13.140 --> 01:00:17.260]   find useful was sort of wrapping it up in a bash script or makefile.
[01:00:17.260 --> 01:00:22.100]   And if you were to do it in a bash script, you would maybe have to loop over these variables
[01:00:22.100 --> 01:00:25.180]   or supply them using command line.
[01:00:25.180 --> 01:00:29.420]   And I feel that makefiles could do the same thing, but slightly more elegantly.
[01:00:29.420 --> 01:00:32.500]   And it was far more readable for the other person.
[01:00:32.500 --> 01:00:33.500]   Excellent.
[01:00:33.500 --> 01:00:35.300]   Thank you.
[01:00:35.300 --> 01:00:45.460]   One question I had a little bit, you mentioned that there is actually a lot of subtlety to
[01:00:45.460 --> 01:00:50.940]   this trade off between flops versus parameters, like where are you getting your efficiency?
[01:00:50.940 --> 01:00:55.100]   And my mental model there a little bit is like, if you're designing for mobile devices,
[01:00:55.100 --> 01:00:58.620]   maybe what you care most about is reducing parameters.
[01:00:58.620 --> 01:01:02.980]   Whereas if you're going for some super low latency setting, then maybe what you care
[01:01:02.980 --> 01:01:04.460]   about is flops.
[01:01:04.460 --> 01:01:09.420]   So I guess, is that a good way of thinking about this trade off here?
[01:01:09.420 --> 01:01:14.060]   Or what have you learned while thinking about that trade off in the context of this set
[01:01:14.060 --> 01:01:15.860]   of experiments?
[01:01:15.860 --> 01:01:18.860]   I think that's an excellent way to think of these trade offs.
[01:01:18.860 --> 01:01:25.820]   So in my mind, at least there are two communities in ML which use compressed networks.
[01:01:25.820 --> 01:01:30.500]   One is they try to train larger models on smaller devices.
[01:01:30.500 --> 01:01:35.020]   In fact, the linformer, reformer fall under these category.
[01:01:35.020 --> 01:01:40.340]   There's also this community of people who want to use ML for edge devices or ML to transmit
[01:01:40.340 --> 01:01:44.980]   data, ML to maybe even represent data, especially with these implicit kind of functions that
[01:01:44.980 --> 01:01:46.020]   we're seeing.
[01:01:46.020 --> 01:01:52.580]   So maybe they'd be concerned more about parameter storage as opposed to computational cost.
[01:01:52.580 --> 01:01:58.260]   So they'd be okay actually using a dense computational cost, but they want to compress the number
[01:01:58.260 --> 01:02:00.260]   of bits transmitted.
[01:02:00.260 --> 01:02:05.060]   Yeah, that makes sense.
[01:02:05.060 --> 01:02:07.380]   I have a follow up question.
[01:02:07.380 --> 01:02:10.660]   Just curiosity, especially.
[01:02:10.660 --> 01:02:11.660]   So two questions.
[01:02:11.660 --> 01:02:17.940]   One is in an optimal setting, what do you care about most?
[01:02:17.940 --> 01:02:18.940]   Is it flops?
[01:02:18.940 --> 01:02:21.820]   Is it latency, throughput or parameters?
[01:02:21.820 --> 01:02:27.300]   Now I understand that you distinguish between parameters and flops, but flops is not always
[01:02:27.300 --> 01:02:30.140]   representative of latency or throughput.
[01:02:30.140 --> 01:02:36.060]   Some people might care about how many images it passes in a second, which essentially is
[01:02:36.060 --> 01:02:37.660]   the throughput.
[01:02:37.660 --> 01:02:39.220]   That is the first question.
[01:02:39.220 --> 01:02:46.980]   And the second question is, is there any alignment between the current state of the research
[01:02:46.980 --> 01:02:50.460]   in pruning with the state of research in quantization?
[01:02:50.460 --> 01:02:56.140]   Because that is also something very relevant to making neural networks being able to run
[01:02:56.140 --> 01:03:01.420]   on much lighter compute devices.
[01:03:01.420 --> 01:03:04.660]   So that's a really interesting question and a really good one.
[01:03:04.660 --> 01:03:07.020]   So I'll actually answer the second part first.
[01:03:07.020 --> 01:03:11.780]   So I was briefly going through quantization literature recently, so I may have missed
[01:03:11.780 --> 01:03:12.780]   things.
[01:03:12.780 --> 01:03:16.140]   So please get back to me and let me know if I miss certain things.
[01:03:16.140 --> 01:03:19.340]   But the way I see it, I think there were three or four main approaches.
[01:03:19.340 --> 01:03:25.540]   One was a range-based, linear-based, k-means or bloomier filters.
[01:03:25.540 --> 01:03:30.660]   And there are also these papers which use new lossy compressors to quantize.
[01:03:30.660 --> 01:03:34.820]   But most of them do pruning and then quantization.
[01:03:34.820 --> 01:03:38.180]   These quantization parameters may be trained, may be tuned.
[01:03:38.180 --> 01:03:39.860]   They're not really trained.
[01:03:39.860 --> 01:03:44.940]   And there's also this work where you do quantization aware training.
[01:03:44.940 --> 01:03:50.980]   But besides that, I don't really see works where quantization is done during training.
[01:03:50.980 --> 01:03:55.500]   You do it as a fine tuning step to maybe tune some of these quantization parameters.
[01:03:55.500 --> 01:03:59.740]   But this is a work where you're doing pruning as you train.
[01:03:59.740 --> 01:04:02.620]   Even iterative pruning, for instance, you're doing pruning as you train.
[01:04:02.620 --> 01:04:04.280]   You don't do it post-training.
[01:04:04.280 --> 01:04:10.220]   So maybe once we start to see works where quantization is done during training, that
[01:04:10.220 --> 01:04:11.280]   could change things.
[01:04:11.280 --> 01:04:16.740]   You could use it in combination with pruning and get even more storage benefits.
[01:04:16.740 --> 01:04:25.260]   Now the first question, which is what do you really care about and are flops representative
[01:04:25.260 --> 01:04:27.780]   of throughput?
[01:04:27.780 --> 01:04:32.380]   I don't think the flops calculated here are completely representative of throughput.
[01:04:32.380 --> 01:04:38.260]   For one, we assumed there were efficient implementations of sparse convolutions available.
[01:04:38.260 --> 01:04:47.000]   So if I were working with matrices of so-and-so sparsity, I get the complete speed up in comparison
[01:04:47.000 --> 01:04:49.160]   to a dense model.
[01:04:49.160 --> 01:04:56.460]   To put this in perspective, suppose I had a layer which was 90% sparse, you would expect
[01:04:56.460 --> 01:04:58.540]   a speed up of around 10x.
[01:04:58.540 --> 01:05:04.460]   Whereas, KU sparse and CUDA sparse and these block sparse matrices today will give you
[01:05:04.460 --> 01:05:07.300]   only around, I think, 3 to 4x.
[01:05:07.300 --> 01:05:12.500]   So these flop counts are not really representative of the throughput that you can achieve on
[01:05:12.500 --> 01:05:14.180]   a real device.
[01:05:14.180 --> 01:05:17.740]   But that is also where I find something like GraphCore exciting.
[01:05:17.740 --> 01:05:22.500]   You should, you can probably try out these algorithms on GraphCore and benchmark the
[01:05:22.500 --> 01:05:23.500]   throughput there.
[01:05:23.500 --> 01:05:25.380]   That would be a good playing field.
[01:05:25.380 --> 01:05:33.020]   All right, we don't have, we're basically out of time, but I did want to touch a little
[01:05:33.020 --> 01:05:40.420]   bit on what you just brought up with the GraphCore, their chips and the things that they make
[01:05:40.420 --> 01:05:43.540]   possible, because I think that's a really exciting direction of research.
[01:05:43.540 --> 01:05:49.300]   So it seems to me that basically what's possible with that kind of a chip is just like much,
[01:05:49.300 --> 01:05:53.060]   much greater than what's possible with GPUs and TPUs.
[01:05:53.060 --> 01:05:59.700]   So do you think that that's something that might open up sparsity and pruning research
[01:05:59.700 --> 01:06:03.380]   to be more useful, more widely done?
[01:06:03.380 --> 01:06:09.940]   And do you think that it'll have a big effect on the kinds of ML research and ML algorithms
[01:06:09.940 --> 01:06:12.820]   that we develop as a community?
[01:06:12.820 --> 01:06:18.540]   So I'm pretty new to GraphCore as well, but let me just tell you what my initial impressions
[01:06:18.540 --> 01:06:19.540]   are.
[01:06:19.540 --> 01:06:23.660]   I think the major bottleneck in GraphCore is the size of models that you can put on
[01:06:23.660 --> 01:06:24.860]   the processor.
[01:06:24.860 --> 01:06:31.300]   So for instance, GPUs go all the way up to 48 GB in memory, whereas these GraphCores
[01:06:31.300 --> 01:06:32.900]   are much smaller.
[01:06:32.900 --> 01:06:38.500]   I think you had, the initial one was 256 MB, the later ones may be around a GB.
[01:06:38.500 --> 01:06:43.020]   So if you can't put your model on these devices, you're not really going to benefit from the
[01:06:43.020 --> 01:06:44.020]   speedups.
[01:06:44.020 --> 01:06:46.580]   So that is one bottleneck currently.
[01:06:46.580 --> 01:06:48.180]   But yes, you're right.
[01:06:48.180 --> 01:06:54.340]   It should probably become a good platform for us to, for researchers to come up with
[01:06:54.340 --> 01:06:59.940]   new pruning algorithms, because you have this platform which can support fine grained operations.
[01:06:59.940 --> 01:07:03.220]   So you should probably tune your algorithms to that.
[01:07:03.220 --> 01:07:04.220]   Interesting.
[01:07:04.220 --> 01:07:13.140]   Yeah, it seems we had Sarah Hooker from Google on the Weights of Biases Salon to talk about
[01:07:13.140 --> 01:07:14.140]   the hardware lottery paper.
[01:07:14.140 --> 01:07:19.740]   I don't know if you saw that one, but it definitely expressed this idea that if we stick to our
[01:07:19.740 --> 01:07:24.740]   guns with the existing stuff that works super, super well over time, we'll be constrained
[01:07:24.740 --> 01:07:27.500]   to just have good ideas in that direction.
[01:07:27.500 --> 01:07:30.380]   We may be missing out on really cool ideas.
[01:07:30.380 --> 01:07:37.900]   Like for example, Wriggle in some sense can't be fully deployed in all of its benefits or
[01:07:37.900 --> 01:07:43.640]   any of the extensions that you were working on without a better compute substrate than
[01:07:43.640 --> 01:07:47.980]   what we have currently with CUDA and with GPUs.
[01:07:47.980 --> 01:07:55.980]   Yeah, I think that's exactly the challenge the unstructured versus structured pruning
[01:07:55.980 --> 01:07:58.020]   committees are facing.
[01:07:58.020 --> 01:07:59.020]   Cool.
[01:07:59.020 --> 01:08:00.020]   Right.
[01:08:00.020 --> 01:08:03.380]   So that is all the time that we have.
[01:08:03.380 --> 01:08:10.220]   So thank you so much for sharing your work with us, Varun and Rajat.
[01:08:10.220 --> 01:08:14.000]   And thank you to also, I think the FastAI team is maybe still here.
[01:08:14.000 --> 01:08:19.360]   So thanks to both of you and to everybody who participated in the Weights and Biases
[01:08:19.360 --> 01:08:22.080]   Reproducibility Challenge.
[01:08:22.080 --> 01:08:26.520]   And we're going to be doing another one this spring, as Deganta mentioned.
[01:08:26.520 --> 01:08:32.320]   So folks out in the audience watching this video, if you want to join a team like this
[01:08:32.320 --> 01:08:42.900]   one and reproduce machine learning research, join our community, join our Slack, and you
[01:08:42.900 --> 01:08:49.600]   can participate and do some really great and interesting work like these folks did.
[01:08:49.600 --> 01:08:55.220]   So Deganta and I hope to see you around as part of the Reproducibility Challenge or elsewhere
[01:08:55.220 --> 01:08:57.500]   in the world of machine learning.
[01:08:57.500 --> 01:08:57.740]   Take care.
[01:08:57.740 --> 01:09:01.320]   (Session concluded at 4pm)

