
[00:00:00.000 --> 00:00:04.640]   Welcome, everyone, to the Weights and Biases Salon.
[00:00:04.640 --> 00:00:15.360]   I have here today two guests who are the co-hosts of UCLA ACM AI Student Group's podcast, You
[00:00:15.360 --> 00:00:17.480]   Belong in AI.
[00:00:17.480 --> 00:00:22.360]   And so we're going to hear from Ava and from Matt a little bit about themselves and about
[00:00:22.360 --> 00:00:29.080]   their work, their careers, such as they are so far as undergraduates.
[00:00:29.080 --> 00:00:35.680]   And then we'll talk a bit more about the podcast, which its core topic is about-- it's an interview
[00:00:35.680 --> 00:00:43.320]   podcast, not unlike what we're doing right now, with inspiring figures in AI, not unlike
[00:00:43.320 --> 00:00:50.520]   what we're doing right now, and with a particular focus on representation and inclusion.
[00:00:50.520 --> 00:00:52.840]   And so there's some really great discussions.
[00:00:52.840 --> 00:00:55.280]   I really have enjoyed listening to the podcast.
[00:00:55.280 --> 00:00:59.480]   I'm really excited to hear from Matt and Ava here today.
[00:00:59.480 --> 00:01:02.160]   Yeah, I'm super excited.
[00:01:02.160 --> 00:01:04.320]   Let's get into it.
[00:01:04.320 --> 00:01:11.840]   I don't know if we can call ourselves influential figures in AI, but hopefully one day.
[00:01:11.840 --> 00:01:17.680]   It said inspiring, and that's maybe an easier goal than influential.
[00:01:17.680 --> 00:01:21.880]   Influential is a tough one.
[00:01:21.880 --> 00:01:29.660]   So yeah, I guess just to start off, maybe tell some of our viewers here who might not
[00:01:29.660 --> 00:01:36.720]   have seen the podcast or know about y'all UCLA ACM AI.
[00:01:36.720 --> 00:01:43.800]   Maybe I guess starting with Matt, how'd you become interested in programming and in AI?
[00:01:43.800 --> 00:01:46.360]   How'd you sort of get into this field?
[00:01:46.360 --> 00:01:50.160]   Yeah, so I think it started in high school.
[00:01:50.160 --> 00:01:52.240]   I don't remember exact moment in time.
[00:01:52.240 --> 00:01:56.840]   There's no like epiphany moment that happened, but I remember like early high school, I tried
[00:01:56.840 --> 00:01:58.400]   programming once just on my own.
[00:01:58.400 --> 00:02:00.320]   It wasn't for me.
[00:02:00.320 --> 00:02:03.840]   So I decided just to take a class like my senior year.
[00:02:03.840 --> 00:02:06.480]   And when I took the class, I kind of fell in love with it.
[00:02:06.480 --> 00:02:09.480]   I was always a math person, just very analytical things.
[00:02:09.480 --> 00:02:13.120]   And that just kind of seemed to fit my style, like when it came to programming.
[00:02:13.120 --> 00:02:17.440]   And when it came to like computer science, there's like different fields I was interested
[00:02:17.440 --> 00:02:18.440]   in.
[00:02:18.440 --> 00:02:22.960]   I was like interested in like cybersecurity, but then I was also interested in AI.
[00:02:22.960 --> 00:02:28.040]   And I don't know what about it was it for me with artificial intelligence, but just
[00:02:28.040 --> 00:02:33.880]   seeing or just knowing that I could program something that had like inherent intelligence,
[00:02:33.880 --> 00:02:36.160]   I don't know, just attracted to me.
[00:02:36.160 --> 00:02:39.920]   I don't know, there's just something about it that attracted to me.
[00:02:39.920 --> 00:02:44.280]   I can't even describe it right now, but I think that's like senior year of high school.
[00:02:44.280 --> 00:02:46.200]   That's when it hit for me.
[00:02:46.200 --> 00:02:51.920]   Yeah, that's interesting that you that you mentioned that like sort of AI was really
[00:02:51.920 --> 00:02:54.800]   what convinced you to try it out.
[00:02:54.800 --> 00:02:58.200]   Ava, did you have a similar experience?
[00:02:58.200 --> 00:02:59.600]   A little bit.
[00:02:59.600 --> 00:03:05.600]   So I took my first like, I had my first CS experience when I was a freshman in high school.
[00:03:05.600 --> 00:03:06.960]   And my parents are both engineers.
[00:03:06.960 --> 00:03:11.040]   My mom's an electrical and sorry, my mom's a software engineer.
[00:03:11.040 --> 00:03:12.040]   And so is my uncle.
[00:03:12.040 --> 00:03:14.480]   So I kind of took after them.
[00:03:14.480 --> 00:03:16.760]   And I didn't really like it.
[00:03:16.760 --> 00:03:21.320]   But I didn't think I wanted to major in it in college.
[00:03:21.320 --> 00:03:26.600]   I wanted to go down more of a route of electrical engineering, like my dad.
[00:03:26.600 --> 00:03:32.000]   So that's kind of where I got into electrical engineering was just doing some other things
[00:03:32.000 --> 00:03:33.000]   in high school with it.
[00:03:33.000 --> 00:03:34.840]   But I knew I still liked computer science all along.
[00:03:34.840 --> 00:03:37.600]   And I wanted to get involved in a computer science field.
[00:03:37.600 --> 00:03:42.280]   Also, you know, I've been pretty much told all my life, artificial intelligence is the
[00:03:42.280 --> 00:03:45.640]   future, and you should really get involved in it because like, it's gonna be everything.
[00:03:45.640 --> 00:03:49.160]   And it kind of is everything already.
[00:03:49.160 --> 00:03:51.800]   But yeah, I knew I wanted to get involved in AI.
[00:03:51.800 --> 00:03:58.440]   I knew I wanted to get involved in, you know, CS, even though I am an electrical engineering
[00:03:58.440 --> 00:03:59.440]   major.
[00:03:59.440 --> 00:04:03.040]   And then, yeah, so that's kind of how I got interested in it.
[00:04:03.040 --> 00:04:04.040]   So.
[00:04:04.040 --> 00:04:05.040]   Nice.
[00:04:05.040 --> 00:04:06.600]   Yeah, it's interesting.
[00:04:06.600 --> 00:04:07.600]   I guess.
[00:04:07.600 --> 00:04:12.120]   I actually didn't start programming at all until I was, or well, I did a little bit of
[00:04:12.120 --> 00:04:19.720]   MATLAB and R for data analysis when I was when I was in college, but I didn't fully,
[00:04:19.720 --> 00:04:23.360]   like really fully start programming until I was already in graduate school.
[00:04:23.360 --> 00:04:28.000]   And I had been told that artificial intelligence was this big waste of time that people had
[00:04:28.000 --> 00:04:30.360]   like, kind of tried and failed at.
[00:04:30.360 --> 00:04:35.440]   I remember making fun of one of my friends in college, because he was working on AI stuff.
[00:04:35.440 --> 00:04:38.600]   And I was like, don't you know, that's never gonna work?
[00:04:38.600 --> 00:04:46.520]   And yeah, it's a it seems like we've had quite, quite different paths.
[00:04:46.520 --> 00:04:53.840]   A question actually came in from YouTube from Florian Hofstetter, which actually is a question
[00:04:53.840 --> 00:04:59.760]   I wanted to ask soon, which is sort of what parts of AI are you most interested in?
[00:04:59.760 --> 00:05:01.440]   You know, I really like computer vision.
[00:05:01.440 --> 00:05:02.640]   Personally, it's my favorite.
[00:05:02.640 --> 00:05:07.000]   It's like, maybe best understood, which I like as a math person.
[00:05:07.000 --> 00:05:08.560]   But what about y'all?
[00:05:08.560 --> 00:05:11.480]   Yeah, I could take this on first.
[00:05:11.480 --> 00:05:16.400]   So actually, this all this past year in quarantine, I was trying to like, figure out this question
[00:05:16.400 --> 00:05:17.400]   on my own.
[00:05:17.400 --> 00:05:19.080]   Because I knew I was interested in AI.
[00:05:19.080 --> 00:05:22.960]   But you know, there's so many subfields, and I didn't know, like, which one I was interested
[00:05:22.960 --> 00:05:23.960]   in.
[00:05:23.960 --> 00:05:26.800]   So I kind of like took this past like, nine months, there's like, kind of dabbling in
[00:05:26.800 --> 00:05:28.080]   everything.
[00:05:28.080 --> 00:05:33.920]   And nothing really hit for me until I took a graphics class, you know, like last quarter.
[00:05:33.920 --> 00:05:39.440]   And I really loved a professor and he does research and particularly like artificial
[00:05:39.440 --> 00:05:41.760]   life and computer graphics.
[00:05:41.760 --> 00:05:44.480]   And I'm actually a really visual person.
[00:05:44.480 --> 00:05:46.640]   I learned a lot by visualizing.
[00:05:46.640 --> 00:05:52.360]   And for me, just like the part of graphics, just applying AI to graphics, there's something
[00:05:52.360 --> 00:05:55.880]   I could like visually see, you know, I could visually see the results.
[00:05:55.880 --> 00:05:59.320]   Whereas for like some other machine learning subfields, you know, it's harder to like visually
[00:05:59.320 --> 00:06:00.320]   see.
[00:06:00.320 --> 00:06:05.640]   And, you know, just being able to like, create like an artificial life character, like in
[00:06:05.640 --> 00:06:10.480]   a like Unity or Unreal Engine, and just being able to see the results, you know, just really
[00:06:10.480 --> 00:06:15.000]   like, inspired me to like, continue pushing on into the subfield.
[00:06:15.000 --> 00:06:16.000]   Nice.
[00:06:16.000 --> 00:06:22.720]   Yeah, there was actually some really, really cool work came out like maybe last week or
[00:06:22.720 --> 00:06:28.120]   earlier this week, very recently on, like people have been doing GANs for rendering
[00:06:28.120 --> 00:06:30.240]   and sort of like pushing the envelope with that.
[00:06:30.240 --> 00:06:36.760]   But yeah, Vlad Coltune at Intel had some really great work that just came out that just like
[00:06:36.760 --> 00:06:40.200]   really like, like sold me on it.
[00:06:40.200 --> 00:06:43.920]   I hadn't been sold on GANs for rendering, or even really on neural rendering.
[00:06:43.920 --> 00:06:45.720]   I was a little skeptical.
[00:06:45.720 --> 00:06:48.160]   But it was really, really impressive results.
[00:06:48.160 --> 00:06:52.880]   So it's an exciting field for sure.
[00:06:52.880 --> 00:06:53.880]   So yeah.
[00:06:53.880 --> 00:07:00.560]   I think the first thing that really got me interested in AI and kind of a reason why
[00:07:00.560 --> 00:07:06.440]   I joined ACM AI was I took this class in my first quarter of freshman year at UCLA.
[00:07:06.440 --> 00:07:11.960]   And it's called E96C Internet of Things and Embedded Machine Learning.
[00:07:11.960 --> 00:07:17.720]   And so we did a bunch of projects with like, you know, I didn't 100% understand what I
[00:07:17.720 --> 00:07:18.720]   was doing.
[00:07:18.720 --> 00:07:22.200]   It was like an intro course to a very advanced topic, which was embedded machine learning
[00:07:22.200 --> 00:07:24.240]   and Internet of Things.
[00:07:24.240 --> 00:07:25.760]   But I really liked it.
[00:07:25.760 --> 00:07:26.760]   It was pretty cool.
[00:07:26.760 --> 00:07:30.760]   So pretty much what we did the entire time was we use this like sensor tile code with
[00:07:30.760 --> 00:07:36.560]   like an ARM in it and like using embedded machine learning and Internet of Things technology
[00:07:36.560 --> 00:07:38.780]   to pretty much detect movements.
[00:07:38.780 --> 00:07:44.080]   And so, you know, you would train the machine and then you would like do different movements
[00:07:44.080 --> 00:07:47.600]   and it would be able to detect it based off of the training that you gave it.
[00:07:47.600 --> 00:07:51.080]   So which is pretty much what they do in like VR and stuff.
[00:07:51.080 --> 00:07:52.880]   And like different video games and stuff.
[00:07:52.880 --> 00:07:55.960]   And so that's honestly something that I've like really been interested in.
[00:07:55.960 --> 00:08:00.920]   I haven't dived too deep in it since I did take that class.
[00:08:00.920 --> 00:08:03.680]   But yeah, that's something that I actually do really like.
[00:08:03.680 --> 00:08:06.520]   And I personally don't play video games all that much.
[00:08:06.520 --> 00:08:08.280]   But the ones that I have, I do like.
[00:08:08.280 --> 00:08:12.540]   And I think that it's really cool to make like really cool sensorial experiences for
[00:08:12.540 --> 00:08:13.540]   different people.
[00:08:13.540 --> 00:08:18.280]   And I'd love to like get involved in that in any way possible when I get older.
[00:08:18.280 --> 00:08:19.280]   Nice.
[00:08:19.280 --> 00:08:20.280]   Yeah.
[00:08:20.280 --> 00:08:28.600]   Yeah, that's a cool, definitely a cool domain and one with a lot of room to grow and improve.
[00:08:28.600 --> 00:08:31.280]   So you've already touched on this a little bit.
[00:08:31.280 --> 00:08:35.880]   But I guess there's a lot of kind of divisions in like what kind of careers you might pursue
[00:08:35.880 --> 00:08:36.880]   in AI.
[00:08:36.880 --> 00:08:41.760]   And you're definitely, you know, at the stage where you are early enough to make these decisions,
[00:08:41.760 --> 00:08:44.240]   but you kind of start need to start making those moves.
[00:08:44.240 --> 00:08:48.440]   So like what are like, are you thinking researcher engineering?
[00:08:48.440 --> 00:08:53.280]   Are you thinking being an individual contributor who writes code, makes stuff versus being
[00:08:53.280 --> 00:08:58.360]   a manager who makes sure a team is able to deliver something versus being a founder who
[00:08:58.360 --> 00:09:03.480]   creates a company, not just the technology, but also the like social structures and the
[00:09:03.480 --> 00:09:05.160]   money and all these other things?
[00:09:05.160 --> 00:09:08.400]   Where are you feeling on those fronts?
[00:09:08.400 --> 00:09:13.520]   Um, I like that's a good question.
[00:09:13.520 --> 00:09:18.040]   I would say, you know, everyone wants to be a founder.
[00:09:18.040 --> 00:09:22.240]   I mean, not everyone, but I feel like, you know, the founder job sounds pretty cool.
[00:09:22.240 --> 00:09:27.760]   You know, it's like you get a lot of money and you get to make something from the start.
[00:09:27.760 --> 00:09:33.240]   But then I like read stories about how stressed Elon Musk was when like Tesla and SpaceX are
[00:09:33.240 --> 00:09:38.680]   first starting out and how he like didn't sleep for days and like became really depressed
[00:09:38.680 --> 00:09:39.680]   for a couple of years.
[00:09:39.680 --> 00:09:42.880]   So, you know, then that kind of stuff makes me rethink that.
[00:09:42.880 --> 00:09:46.920]   But in a perfect world, I would love to be a founder.
[00:09:46.920 --> 00:09:49.920]   But before that, I definitely think that I want to go to grad school.
[00:09:49.920 --> 00:09:53.660]   So that's kind of where my where my focus is right now.
[00:09:53.660 --> 00:09:55.560]   My focus is on just getting to grad school.
[00:09:55.560 --> 00:09:58.520]   And then after that, figuring out what I want to do from there.
[00:09:58.520 --> 00:09:59.520]   Cool.
[00:09:59.520 --> 00:10:04.360]   Yeah, I've worked with the grad school found like, you know, PhD founders and with, you
[00:10:04.360 --> 00:10:07.120]   know, college dropout founders.
[00:10:07.120 --> 00:10:11.520]   And, you know, people can make it work no matter what path they take if they're willing
[00:10:11.520 --> 00:10:15.240]   to deal with the no sleep and the high responsibility.
[00:10:15.240 --> 00:10:16.240]   Yeah.
[00:10:16.240 --> 00:10:19.280]   I guess for myself.
[00:10:19.280 --> 00:10:20.320]   Yeah.
[00:10:20.320 --> 00:10:26.960]   So for myself, I guess like the past few months or so, this is like the question I ask myself
[00:10:26.960 --> 00:10:31.360]   like every day, not every day, but like pretty often, like what path I want to take, because
[00:10:31.360 --> 00:10:36.120]   there's definitely a lot I could take, you know, from where I'm at as a second year.
[00:10:36.120 --> 00:10:40.840]   You know, there's obviously a path like machine learning engineer that's attractive, you know,
[00:10:40.840 --> 00:10:43.360]   work on like real world problems.
[00:10:43.360 --> 00:10:46.640]   Then there's like also another path like I mentioned about being a founder.
[00:10:46.640 --> 00:10:50.360]   And you know, both of them like really attract to me.
[00:10:50.360 --> 00:10:55.640]   I guess, like I said, I hope one day I become a founder, but I'm really not letting myself
[00:10:55.640 --> 00:10:56.800]   push it too hard.
[00:10:56.800 --> 00:11:00.800]   Because if I just push it too hard, you know, I don't want to force anything.
[00:11:00.800 --> 00:11:04.320]   I found that like the passionate projects I've been involved with, you know, they've
[00:11:04.320 --> 00:11:09.400]   come naturally and I'm just really just waiting to see like what opportunity comes upon me
[00:11:09.400 --> 00:11:11.000]   or like what ideas I come up with.
[00:11:11.000 --> 00:11:14.400]   But yeah, not forcing anything and just seeing what happens.
[00:11:14.400 --> 00:11:15.400]   Nice.
[00:11:15.400 --> 00:11:22.400]   Yeah, I do think maybe 10 or 15 years ago, you might have gotten the advice that like,
[00:11:22.400 --> 00:11:27.080]   you should found now like found when you're 22, you know, drop out of college and found
[00:11:27.080 --> 00:11:28.080]   a company.
[00:11:28.080 --> 00:11:31.480]   But I think the advice you'll get nowadays, at least the advice you're going to get from
[00:11:31.480 --> 00:11:36.960]   me right now, is that you can found a company when you're 40 something years old.
[00:11:36.960 --> 00:11:41.800]   You know, if you if you want to, and a lot of very successful founders are people who
[00:11:41.800 --> 00:11:44.480]   built up a big stable of skills over time.
[00:11:44.480 --> 00:11:56.960]   But like I said, lots of people take different paths to get to that final result.
[00:11:56.960 --> 00:12:01.720]   So pivoting a little bit over towards what you're currently working on, rather than the
[00:12:01.720 --> 00:12:03.960]   exciting future ahead.
[00:12:03.960 --> 00:12:08.360]   Maybe tell us a little bit more about the work you're both in this outreach group at
[00:12:08.360 --> 00:12:14.040]   the at the in the AI group at the in the ACM club.
[00:12:14.040 --> 00:12:19.760]   So maybe break that down for our viewers who are not familiar with the structure of this
[00:12:19.760 --> 00:12:21.440]   student group.
[00:12:21.440 --> 00:12:26.480]   Yeah, I could take this one really quickly.
[00:12:26.480 --> 00:12:32.040]   So yeah, ACM AI, or it's like ACM at UCLA, we're probably like the biggest computer science
[00:12:32.040 --> 00:12:34.240]   club at our school.
[00:12:34.240 --> 00:12:38.800]   And within ACM, there are different subcommittees, AI being one of them.
[00:12:38.800 --> 00:12:44.440]   There are other subcommittees like hack, which focuses on like web design or web applications,
[00:12:44.440 --> 00:12:47.640]   other ones, like cybersecurity.
[00:12:47.640 --> 00:12:52.280]   And even like one of the cooler committees is called design, which creates all the cool
[00:12:52.280 --> 00:12:56.120]   designs for all of our committees, like the one I'm using for my background.
[00:12:56.120 --> 00:12:57.120]   So yeah, they make great designs.
[00:12:57.120 --> 00:12:58.120]   So shout out to them.
[00:12:58.120 --> 00:13:00.640]   Yeah, we love them so much.
[00:13:00.640 --> 00:13:05.040]   But yeah, ACM, it's one of the bigger clubs on campus for CS.
[00:13:05.040 --> 00:13:10.920]   And you know, our our goal is to make like CS learning or learning CS as easy and you
[00:13:10.920 --> 00:13:13.000]   know, as inclusive as possible.
[00:13:13.000 --> 00:13:15.880]   We have a lot of workshops across all our subcommittees.
[00:13:15.880 --> 00:13:19.920]   And, you know, we try to reach out to like non CS majors too.
[00:13:19.920 --> 00:13:23.480]   Because we know programming could be like really useful for a lot of different industries.
[00:13:23.480 --> 00:13:26.520]   And we just want to share the love with programming.
[00:13:26.520 --> 00:13:30.880]   And yeah, that's the best way I could put it.
[00:13:30.880 --> 00:13:38.800]   ACM, is that like a, that's like a broader organization that's across multiple campuses.
[00:13:38.800 --> 00:13:42.320]   So this is this is like a chapter of a broader group.
[00:13:42.320 --> 00:13:44.240]   Yeah, yeah.
[00:13:44.240 --> 00:13:46.720]   There's like a couple different ones.
[00:13:46.720 --> 00:13:50.560]   I think UC Riverside just started one and so did UCSD.
[00:13:50.560 --> 00:13:55.680]   And so one thing that we've been working on lately, so like kind of like there are the
[00:13:55.680 --> 00:13:59.640]   subcommittees that Matt talked about, and then like overarching is like board, which
[00:13:59.640 --> 00:14:03.720]   is like, you know, then it's not as scary as it sounds, they're actually all pretty
[00:14:03.720 --> 00:14:05.320]   nice.
[00:14:05.320 --> 00:14:09.640]   But they are like in charge of like some of like the more broader organizational things.
[00:14:09.640 --> 00:14:14.180]   So something that they've been interested in doing is like collaborating with other
[00:14:14.180 --> 00:14:17.080]   UC chapters of ACM.
[00:14:17.080 --> 00:14:18.640]   So yeah, that's pretty fun.
[00:14:18.640 --> 00:14:21.120]   I'm looking forward to those.
[00:14:21.120 --> 00:14:23.160]   I see.
[00:14:23.160 --> 00:14:27.680]   And I guess in the in the zoom era, it's a little bit easier maybe to collaborate across
[00:14:27.680 --> 00:14:33.600]   campuses, if maybe maybe not quite as fun as the sleepover.
[00:14:33.600 --> 00:14:37.320]   Definitely.
[00:14:37.320 --> 00:14:42.600]   So the particular thing that the two of you work on with outreach that we're here to nominally
[00:14:42.600 --> 00:14:49.080]   discuss today is the is the you belong in AI podcast.
[00:14:49.080 --> 00:14:54.960]   So maybe if you could tell us just a little bit about that, about that podcast.
[00:14:54.960 --> 00:15:02.960]   Yeah, so the podcast is, the main goal of the podcast is to bring it's an interview
[00:15:02.960 --> 00:15:04.000]   style.
[00:15:04.000 --> 00:15:10.840]   But the goal is it's mainly centered at, you know, high school students, college students,
[00:15:10.840 --> 00:15:14.720]   anyone who's really interested in AI, you could be like older as well, and interested
[00:15:14.720 --> 00:15:15.720]   in artificial intelligence.
[00:15:15.720 --> 00:15:21.480]   But the whole point is just to spread the message that anyone can be in artificial intelligence.
[00:15:21.480 --> 00:15:26.040]   I feel like sometimes saying the words artificial intelligence can be a buzzword.
[00:15:26.040 --> 00:15:29.520]   And it also can be something that seems super, super complicated.
[00:15:29.520 --> 00:15:31.680]   And you know, there definitely are still barriers to that.
[00:15:31.680 --> 00:15:33.960]   And we never deny that in our podcast.
[00:15:33.960 --> 00:15:39.680]   But our goal is definitely to inspire everyone who wants to be in it to just try and go for
[00:15:39.680 --> 00:15:41.840]   it.
[00:15:41.840 --> 00:15:43.920]   That's like the main goal of our podcast.
[00:15:43.920 --> 00:15:48.400]   And so you know, every every speaker has a different story.
[00:15:48.400 --> 00:15:50.800]   And they all have different like, facets of advice.
[00:15:50.800 --> 00:15:55.000]   And they all have, like, definitely cool experiences to talk about.
[00:15:55.000 --> 00:15:57.640]   And they all just kind of relay that on to the audience.
[00:15:57.640 --> 00:16:02.320]   And hopefully, you know, we inspire people to get into AI, give people advice on how
[00:16:02.320 --> 00:16:03.320]   to get into AI.
[00:16:03.320 --> 00:16:08.200]   And yeah, that's pretty much the goal of the podcast.
[00:16:08.200 --> 00:16:16.440]   And I guess, so what are some of the reasons why you chose or are working with this particular
[00:16:16.440 --> 00:16:19.280]   podcast format as a form of outreach, right?
[00:16:19.280 --> 00:16:22.200]   There's lots of different ways that one can reach out.
[00:16:22.200 --> 00:16:27.120]   Do you think there's like particular benefits for the podcast format for the sort of style
[00:16:27.120 --> 00:16:33.080]   you're going for or the like the mission that you in particular have?
[00:16:33.080 --> 00:16:38.520]   Yeah, so one thing that we like about the podcast is that, you know, we could easily
[00:16:38.520 --> 00:16:42.640]   distribute it to like whoever you want to distribute it to.
[00:16:42.640 --> 00:16:46.440]   And yeah, like sure, it's an interview format.
[00:16:46.440 --> 00:16:50.080]   Like one of the things like for this season in particular that we're trying to focus on
[00:16:50.080 --> 00:16:54.320]   is we really want to get to know who are interviewing.
[00:16:54.320 --> 00:16:58.560]   Like how like the journey just like you, just the questions like you asked us earlier, we
[00:16:58.560 --> 00:17:04.000]   really want to understand like the journey that they took because we believe that the
[00:17:04.000 --> 00:17:09.320]   best way for our audience to resonate with these inspiring figures is to realize that,
[00:17:09.320 --> 00:17:13.120]   you know, maybe they had the same journey or they were in the same position that I was,
[00:17:13.120 --> 00:17:14.120]   you know, in high school.
[00:17:14.120 --> 00:17:18.680]   And, you know, if they can see that, if they could relate to that, then, you know, maybe
[00:17:18.680 --> 00:17:20.840]   there's a greater chance that they could see themselves in AI.
[00:17:20.840 --> 00:17:23.440]   So just that's our main theme.
[00:17:23.440 --> 00:17:28.920]   And yeah, like I said earlier, just the podcast, it's easy to distribute and just reach out
[00:17:28.920 --> 00:17:31.560]   to as many people as possible.
[00:17:31.560 --> 00:17:36.160]   Yeah, the conversational format does seem to work nicely to get people to tell their
[00:17:36.160 --> 00:17:37.840]   life stories.
[00:17:37.840 --> 00:17:44.360]   Like the episode with Paco Guzman of Facebook AI, who is working on under-resourced languages
[00:17:44.360 --> 00:17:50.000]   and like, you know, making NLP, you know, more inclusive.
[00:17:50.000 --> 00:17:54.960]   And, you know, he sort of like was like, well, you asked him, I think, about his like his
[00:17:54.960 --> 00:17:56.760]   high school experience or his college experience.
[00:17:56.760 --> 00:18:01.840]   And he was like, well, let me go back to I was a child growing up in like this place
[00:18:01.840 --> 00:18:08.200]   in Mexico and blah, blah, blah, blah.
[00:18:08.200 --> 00:18:12.640]   So what are some like sort of moments in these?
[00:18:12.640 --> 00:18:16.440]   One of the nice things about the sort of conversational format is that you can kind of have these
[00:18:16.440 --> 00:18:20.760]   like moments of sort of surprise or like serendipity.
[00:18:20.760 --> 00:18:27.000]   So were there any moments that were particularly surprising to you or particularly serendipitous
[00:18:27.000 --> 00:18:31.000]   when you were when you were running the podcast and interviewing folks?
[00:18:31.000 --> 00:18:38.200]   Um, oh, actually, this is something that I haven't told Matt about.
[00:18:38.200 --> 00:18:43.680]   But so the second person that we interviewed, her name was Shannasa.
[00:18:43.680 --> 00:18:45.680]   Her name is Shannasa.
[00:18:45.680 --> 00:18:47.600]   And she goes to Cornell.
[00:18:47.600 --> 00:18:49.160]   She's a graduate student at Cornell.
[00:18:49.160 --> 00:18:50.840]   She's a PhD candidate.
[00:18:50.840 --> 00:18:55.800]   And I was always like, when we were doing the interview, I was like, she looks familiar.
[00:18:55.800 --> 00:19:01.720]   And then one day, like a probably two weeks after we filmed the podcast episode, I remembered
[00:19:01.720 --> 00:19:03.100]   where I knew her from.
[00:19:03.100 --> 00:19:08.400]   In high school, I did a program, an engineering program at Cornell over the summer.
[00:19:08.400 --> 00:19:09.400]   And she was one of my counselors.
[00:19:09.400 --> 00:19:14.960]   And I remember having a long conversation with her about her like being a PhD candidate.
[00:19:14.960 --> 00:19:16.880]   I was like, Oh, my God, you're a PhD candidate.
[00:19:16.880 --> 00:19:18.320]   That's so cool.
[00:19:18.320 --> 00:19:21.840]   And then like, I thought that she would be like much older because she was PhD candidate.
[00:19:21.840 --> 00:19:25.240]   But then I found out on the podcast, she just went straight from undergrad to PhD.
[00:19:25.240 --> 00:19:28.020]   So she was actually pretty fairly young.
[00:19:28.020 --> 00:19:32.220]   And she also went to Pomona College, which is like an hour away from me.
[00:19:32.220 --> 00:19:36.800]   So I don't know, I found that out like, a week later, like a week after, like the podcast
[00:19:36.800 --> 00:19:37.800]   was uploaded and everything.
[00:19:37.800 --> 00:19:41.320]   And I was like, Wow, I knew her.
[00:19:41.320 --> 00:19:42.320]   And so I think I emailed her.
[00:19:42.320 --> 00:19:44.760]   And I was like, Hey, I remember you from this.
[00:19:44.760 --> 00:19:45.760]   This is super random.
[00:19:45.760 --> 00:19:48.760]   But like, thanks for coming on the podcast again.
[00:19:48.760 --> 00:19:50.000]   Nice.
[00:19:50.000 --> 00:19:51.200]   Did you remember this?
[00:19:51.200 --> 00:19:57.400]   Um, I don't remember, actually, I have like, I don't have a very good memory of it.
[00:19:57.400 --> 00:20:00.440]   But I don't think that she remembered because I think she would have Yeah, there was like
[00:20:00.440 --> 00:20:01.440]   a bunch of kids there.
[00:20:01.440 --> 00:20:04.840]   So she might have not remembered, but I did have like a couple good conversations with
[00:20:04.840 --> 00:20:06.880]   her there, which was cool.
[00:20:06.880 --> 00:20:11.800]   Yeah, I would guess that the counselors I remember from my robotics camp when I was
[00:20:11.800 --> 00:20:19.000]   12 years old, probably do not remember the one particular child like me, but I do remember
[00:20:19.000 --> 00:20:27.480]   the the counselor Haji, who was my favorite, you know, so maybe there's an asymmetry there.
[00:20:27.480 --> 00:20:30.400]   Yeah, that's interesting.
[00:20:30.400 --> 00:20:35.160]   What about like, I guess, you know, we all consume a lot of media, or maybe I shouldn't
[00:20:35.160 --> 00:20:36.300]   speak for everybody.
[00:20:36.300 --> 00:20:40.280]   But lots of us consume lots of media, and don't necessarily make it ourselves.
[00:20:40.280 --> 00:20:41.280]   Right?
[00:20:41.280 --> 00:20:44.360]   So we watch more TV shows than we produce.
[00:20:44.360 --> 00:20:46.640]   I think I can say that confidently.
[00:20:46.640 --> 00:20:50.480]   So I'm curious if the process of like, putting this whole thing together and making it work
[00:20:50.480 --> 00:20:56.040]   was like eye opening or surprising to you in any way about, you know, the either about
[00:20:56.040 --> 00:21:04.240]   the world of media production and podcasting or like, you know, more broadly?
[00:21:04.240 --> 00:21:05.240]   I guess.
[00:21:05.240 --> 00:21:09.320]   Yeah, I'm, I'm one of those people that consume a lot of media.
[00:21:09.320 --> 00:21:13.280]   Unfortunately, I should, I should like reduce it, but it's okay.
[00:21:13.280 --> 00:21:18.680]   But I think just just from, yeah, just from experience, I had like a general idea of like,
[00:21:18.680 --> 00:21:21.080]   how it was produced.
[00:21:21.080 --> 00:21:26.480]   And just like, when I transitioned into like, actually, like, you know, producing the podcast,
[00:21:26.480 --> 00:21:30.280]   I think those just by consuming media, just like gave me that good idea of how it's produced.
[00:21:30.280 --> 00:21:37.360]   And it translated over, it was what I expected, maybe a little bit more on like the post production,
[00:21:37.360 --> 00:21:42.320]   because one of the things we do is we make clips for our podcast, and we post it on YouTube,
[00:21:42.320 --> 00:21:44.200]   just so it's more digestible.
[00:21:44.200 --> 00:21:48.280]   Because I know a huge podcast can be like an hour, and that might not be suitable for
[00:21:48.280 --> 00:21:49.280]   a lot of people.
[00:21:49.280 --> 00:21:52.000]   So we condense it into clips sometimes.
[00:21:52.000 --> 00:21:57.200]   And I thought that was gonna be really quick and post production, but that takes like,
[00:21:57.200 --> 00:21:58.200]   two hours.
[00:21:58.200 --> 00:21:59.240]   And yeah, just adds to it.
[00:21:59.240 --> 00:22:04.160]   So I guess one thing that like, we're working towards is just to like, just create a schedule
[00:22:04.160 --> 00:22:10.080]   or just like, try to like, not make all over the place, but just try to keep it as structured
[00:22:10.080 --> 00:22:13.760]   as possible.
[00:22:13.760 --> 00:22:20.000]   And I guess in the process, you brought up the difficulty in the time consuming nature
[00:22:20.000 --> 00:22:22.160]   of post production.
[00:22:22.160 --> 00:22:26.120]   Do you have any podcast production productivity tips?
[00:22:26.120 --> 00:22:30.920]   I've got a few that I've, you know, carefully crafted over the last year.
[00:22:30.920 --> 00:22:32.960]   But any thoughts about that?
[00:22:32.960 --> 00:22:41.520]   Have you tried using your computer science skills to make this better and faster?
[00:22:41.520 --> 00:22:48.680]   I think Matt, didn't you like make an algorithm to like, email out to a bunch of different
[00:22:48.680 --> 00:22:52.280]   like, teachers across America, like a bunch of high school teachers to email out like
[00:22:52.280 --> 00:22:53.280]   our clips?
[00:22:53.280 --> 00:22:54.280]   That's, that's true.
[00:22:54.280 --> 00:22:56.640]   It wasn't an algorithm exactly.
[00:22:56.640 --> 00:22:59.320]   But it was kind of like a custom email list thing.
[00:22:59.320 --> 00:23:03.960]   Because, yeah, one of the things that we do is like, we email to teachers all around the
[00:23:03.960 --> 00:23:10.040]   US and sometimes like, they email, they have email forms on their website.
[00:23:10.040 --> 00:23:11.680]   And that's hard to automate.
[00:23:11.680 --> 00:23:14.520]   And so like, I haven't finished it completely.
[00:23:14.520 --> 00:23:15.520]   I've almost there.
[00:23:15.520 --> 00:23:19.000]   But yeah, just trying to automate like those forms that we have to complete.
[00:23:19.000 --> 00:23:21.080]   Yeah, I'm doing our best.
[00:23:21.080 --> 00:23:25.560]   But yeah, just like the fact that we're doing it, it makes like, reaching out to like hundreds
[00:23:25.560 --> 00:23:29.480]   of teachers like a lot easier rather than just like, individually, like emailing all
[00:23:29.480 --> 00:23:30.480]   of them.
[00:23:30.480 --> 00:23:33.280]   Because like, one thing that we like to do is we like to personalize the emails as much
[00:23:33.280 --> 00:23:35.440]   as possible.
[00:23:35.440 --> 00:23:38.440]   So that they just seem more respect, like receptive to us.
[00:23:38.440 --> 00:23:43.400]   So, but yeah, that's, that's honestly made like emailing a lot faster.
[00:23:43.400 --> 00:23:44.760]   So.
[00:23:44.760 --> 00:23:46.120]   Nice.
[00:23:46.120 --> 00:23:47.440]   Yeah.
[00:23:47.440 --> 00:23:54.840]   The I think the one thing that's maybe underappreciated, I think, like by people who are sort of outside
[00:23:54.840 --> 00:23:59.840]   the world of programming is like how useful like even being able to like automate a simple
[00:23:59.840 --> 00:24:06.040]   form like that is, there's a pretty well known intro to Python book called automate the boring
[00:24:06.040 --> 00:24:07.480]   stuff with Python.
[00:24:07.480 --> 00:24:11.400]   That's like intended for people who have no, like don't have a computer science education,
[00:24:11.400 --> 00:24:15.600]   don't have like, strong programming fundamentals to be able to automate away certain parts
[00:24:15.600 --> 00:24:16.800]   of their jobs.
[00:24:16.800 --> 00:24:21.520]   And I found like, that has been the times that I've been, you know, most grateful for
[00:24:21.520 --> 00:24:26.120]   having developed these skills when I see something like, I have to email 100 people.
[00:24:26.120 --> 00:24:31.520]   And instead of thinking, oh, boy, let me like put on the headphones and type this all out.
[00:24:31.520 --> 00:24:35.120]   Like I think, well, maybe I can maybe I can automate this.
[00:24:35.120 --> 00:24:36.880]   Yeah.
[00:24:36.880 --> 00:24:39.400]   And that's, that's kind of like why I love programming.
[00:24:39.400 --> 00:24:41.680]   You know, it helps when you like least expect it.
[00:24:41.680 --> 00:24:46.400]   So yeah.
[00:24:46.400 --> 00:24:52.320]   So one thing actually that I liked about the way you structured your podcast was you included
[00:24:52.320 --> 00:25:01.440]   this nice little segment to break it up called 10 epochs, which is a segment in which the
[00:25:01.440 --> 00:25:07.360]   the interviewees are asked 10 quick questions and give their quick responses.
[00:25:07.360 --> 00:25:11.680]   So I've stolen this idea from you.
[00:25:11.680 --> 00:25:15.760]   At least for now, you know, if I if I use it on future episodes, I'll make sure to credit
[00:25:15.760 --> 00:25:18.360]   you for this idea.
[00:25:18.360 --> 00:25:20.120]   But I've also stolen some of the questions.
[00:25:20.120 --> 00:25:24.200]   So I'm excited to hear your answers to some of the questions that you asked some of your
[00:25:24.200 --> 00:25:25.200]   guests.
[00:25:25.200 --> 00:25:26.880]   So 10 epochs, here we go.
[00:25:26.880 --> 00:25:32.000]   And we'll do Ava and then Matt in in for each of these questions.
[00:25:32.000 --> 00:25:33.800]   All right.
[00:25:33.800 --> 00:25:39.240]   So favorite programming language?
[00:25:39.240 --> 00:25:40.240]   Not Python.
[00:25:40.240 --> 00:25:43.560]   I didn't hear you there.
[00:25:43.560 --> 00:25:44.560]   I think you might have been on mute.
[00:25:44.560 --> 00:25:45.560]   Oh, okay.
[00:25:45.560 --> 00:25:46.560]   I said not Python.
[00:25:46.560 --> 00:25:47.560]   But I'm kidding.
[00:25:47.560 --> 00:25:48.560]   I don't like Python.
[00:25:48.560 --> 00:25:56.720]   Personally, I'm just gonna say C++ because that's what I've been doing for a long time
[00:25:56.720 --> 00:25:58.760]   with Visual Studios and stuff.
[00:25:58.760 --> 00:26:01.560]   And yeah, I mean, I think that it's just like, it's pretty cool.
[00:26:01.560 --> 00:26:04.160]   And I like learning it because I know a lot of people use it.
[00:26:04.160 --> 00:26:05.160]   So I'm just gonna say C++.
[00:26:05.160 --> 00:26:06.160]   Nice.
[00:26:06.160 --> 00:26:07.160]   Matt?
[00:26:07.160 --> 00:26:11.000]   Yeah, I guess for me, it has to be Python.
[00:26:11.000 --> 00:26:15.680]   I know it's kind of like popular choice, but it's what I've used the most and most comfortable
[00:26:15.680 --> 00:26:16.680]   with.
[00:26:16.680 --> 00:26:17.680]   For sure.
[00:26:17.680 --> 00:26:24.560]   The, you know, the combination of Python and C++ together, that duo, it's the peanut butter
[00:26:24.560 --> 00:26:28.320]   and jelly of machine learning right there.
[00:26:28.320 --> 00:26:35.040]   And you know, I personally thank God every day for the people who like C++ and enjoy
[00:26:35.040 --> 00:26:36.760]   learning and writing C++.
[00:26:36.760 --> 00:26:39.080]   So thanks for that.
[00:26:39.080 --> 00:26:40.760]   All right.
[00:26:40.760 --> 00:26:45.080]   TensorFlow or PyTorch?
[00:26:45.080 --> 00:26:49.760]   I'm just gonna say PyTorch just because that's the only really thing that I've worked with.
[00:26:49.760 --> 00:26:54.760]   Yes, for me, I'd say PyTorch too.
[00:26:54.760 --> 00:26:57.440]   TensorFlow can get too confusing at times.
[00:26:57.440 --> 00:27:01.520]   So yeah, PyTorch.
[00:27:01.520 --> 00:27:02.520]   Quick interlude.
[00:27:02.520 --> 00:27:08.280]   What do you do when you ask this question and then the interviewee just gives an answer
[00:27:08.280 --> 00:27:09.560]   that you completely disagree with?
[00:27:09.560 --> 00:27:12.360]   I'm not asking because of what's currently happening in this interview.
[00:27:12.360 --> 00:27:16.680]   I'm just thinking because I think in Chinasa's example, or in Chinasa's episode, I think
[00:27:16.680 --> 00:27:25.640]   she said MATLAB and TensorFlow when you had Dr. Lachelle Hatley on.
[00:27:25.640 --> 00:27:29.840]   She said, like, she's definitely said TensorFlow.
[00:27:29.840 --> 00:27:31.520]   So what's going through your head?
[00:27:31.520 --> 00:27:33.400]   What are you thinking?
[00:27:33.400 --> 00:27:36.820]   I mean, yeah, because I know those are coming.
[00:27:36.820 --> 00:27:38.760]   It's bound to happen that someone disagrees with me.
[00:27:38.760 --> 00:27:43.960]   So I prepare mentally for it just in case that happens, just so I don't garner a wild
[00:27:43.960 --> 00:27:44.960]   reaction.
[00:27:44.960 --> 00:27:49.040]   But yeah, I just prepare for it mentally.
[00:27:49.040 --> 00:27:54.760]   I actually enjoy when we get different opinions, just so it's not the same answers every time.
[00:27:54.760 --> 00:27:58.920]   Yeah, I guess it would be interesting.
[00:27:58.920 --> 00:28:02.080]   I don't know how much of you dug in with people.
[00:28:02.080 --> 00:28:06.800]   I don't know how much of the conversation that you actually have ends up in the podcast.
[00:28:06.800 --> 00:28:12.120]   I'm just curious, have you gotten the hotter takes from any of your guests on these questions
[00:28:12.120 --> 00:28:18.320]   of programming language or framework?
[00:28:18.320 --> 00:28:21.280]   Most of what we say in the podcast is kind of in there.
[00:28:21.280 --> 00:28:27.760]   So for the 10 Epochs, we usually don't question anything just because we want to keep it rolling,
[00:28:27.760 --> 00:28:28.760]   I guess.
[00:28:28.760 --> 00:28:30.920]   But yeah, I mean, I don't know.
[00:28:30.920 --> 00:28:34.520]   We just kind of like we accept their answer and just move on.
[00:28:34.520 --> 00:28:39.040]   Yeah, also, I guess, like you said, for me, the one that caught me off guard the most
[00:28:39.040 --> 00:28:41.200]   is probably the MATLAB one.
[00:28:41.200 --> 00:28:46.880]   I'm not a fan of MATLAB too much, but I just thought it was really interesting.
[00:28:46.880 --> 00:28:47.960]   Yeah, definitely.
[00:28:47.960 --> 00:28:52.920]   My eyebrows raised for sure, because it was also my first language.
[00:28:52.920 --> 00:28:56.720]   It was what I first started writing in and I look back on it and I'm like, man, that
[00:28:56.720 --> 00:28:57.720]   was a traumatic experience.
[00:28:57.720 --> 00:29:01.480]   I sure wish I hadn't written all that MATLAB.
[00:29:01.480 --> 00:29:04.720]   But different.
[00:29:04.720 --> 00:29:05.720]   Yeah.
[00:29:05.720 --> 00:29:15.000]   All right, moving on in 10 Epochs, pre-pandemic hobby.
[00:29:15.000 --> 00:29:16.920]   If you can even remember that far back.
[00:29:16.920 --> 00:29:19.680]   Oh, my God, so long ago.
[00:29:19.680 --> 00:29:22.960]   Okay, pre-pandemic hobby.
[00:29:22.960 --> 00:29:25.880]   Oh, God.
[00:29:25.880 --> 00:29:27.720]   I don't know.
[00:29:27.720 --> 00:29:31.400]   My pre-pandemic hobby and something that, you know, lately with vaccines coming out
[00:29:31.400 --> 00:29:35.120]   and everything like that, I've been getting into more lately, but hanging out with friends
[00:29:35.120 --> 00:29:40.800]   like I loved having parties at my house like almost every weekend, just inviting a group
[00:29:40.800 --> 00:29:45.800]   over and like family, friends to just being around a lot of people.
[00:29:45.800 --> 00:29:46.800]   That was my hobby.
[00:29:46.800 --> 00:29:48.800]   I loved it so much.
[00:29:48.800 --> 00:29:55.960]   Yeah, not to copy Ava, but yeah, I think my first year at college, just being in the dorms
[00:29:55.960 --> 00:30:01.320]   and just like having like all your friends like five feet away from you, like, oh, no,
[00:30:01.320 --> 00:30:03.640]   that's an experience I like, I'll never forget.
[00:30:03.640 --> 00:30:06.040]   And unfortunately, we haven't been able to do that this year.
[00:30:06.040 --> 00:30:08.960]   But yeah, just hanging out with friends.
[00:30:08.960 --> 00:30:09.960]   Definitely.
[00:30:09.960 --> 00:30:10.960]   Hmm.
[00:30:10.960 --> 00:30:12.960]   I think, yeah.
[00:30:12.960 --> 00:30:18.360]   Yeah, Dr. Hadley said pretty much the same thing, which I think, you know, maybe is not
[00:30:18.360 --> 00:30:22.160]   an answer that people would have given for their favorite hobby before the pandemic.
[00:30:22.160 --> 00:30:27.320]   But we've maybe learned that you don't know what you got till it's gone.
[00:30:27.320 --> 00:30:29.320]   For sure.
[00:30:29.320 --> 00:30:30.880]   All right.
[00:30:30.880 --> 00:30:38.760]   What is your favorite computer science class that you have taken?
[00:30:38.760 --> 00:30:43.920]   I have not taken too many computer science classes at UCLA.
[00:30:43.920 --> 00:30:48.960]   Oh, you're double E. So I'll accept a double E answer here.
[00:30:48.960 --> 00:30:50.880]   If you have an electrical engineering class you like.
[00:30:50.880 --> 00:30:52.120]   Okay, okay, cool.
[00:30:52.120 --> 00:30:55.680]   I'm actually gonna say the one that I'm taking right now.
[00:30:55.680 --> 00:30:57.360]   ECE3.
[00:30:57.360 --> 00:31:00.440]   It's like the intro to electrical engineering course.
[00:31:00.440 --> 00:31:04.360]   The professor is just like, kind of a meme.
[00:31:04.360 --> 00:31:05.360]   He's a he's a cool guy.
[00:31:05.360 --> 00:31:07.680]   He's a pretty fun guy.
[00:31:07.680 --> 00:31:11.960]   And I also really have a lot of respect for him now because we usually have weekly homeworks
[00:31:11.960 --> 00:31:15.280]   that take like a good hour to do every week.
[00:31:15.280 --> 00:31:19.000]   And we also have weekly quizzes at 8am on Mondays, and you just cancel them for the
[00:31:19.000 --> 00:31:20.080]   rest of the quarter.
[00:31:20.080 --> 00:31:23.160]   So I gotta say I love him right now.
[00:31:23.160 --> 00:31:25.160]   I'm really into him right now.
[00:31:25.160 --> 00:31:28.160]   Love that class.
[00:31:28.160 --> 00:31:33.760]   Yeah, then as for me, also I'm taking it right now.
[00:31:33.760 --> 00:31:39.360]   I'm taking this class, artificial life and computer graphics, which I love.
[00:31:39.360 --> 00:31:43.600]   What I love about it is that there's like only 20 people in the class and the professor
[00:31:43.600 --> 00:31:47.280]   speaks his mind, you know, there's not too many people, he feels really comfortable about
[00:31:47.280 --> 00:31:48.280]   it.
[00:31:48.280 --> 00:31:49.280]   And that's what I love.
[00:31:49.280 --> 00:31:53.440]   And like, he specializes in that research field, which is like really cool.
[00:31:53.440 --> 00:31:55.240]   Like he's done a lot of cool papers in the past.
[00:31:55.240 --> 00:31:57.720]   And he explains everything really well.
[00:31:57.720 --> 00:32:00.200]   And just like, it's really fun.
[00:32:00.200 --> 00:32:04.360]   So yeah, I'd have to say that class.
[00:32:04.360 --> 00:32:11.560]   What is the coolest research or engineering project that you've seen in the past year,
[00:32:11.560 --> 00:32:16.360]   either like something you've done yourself, some friends have done or like something we'd
[00:32:16.360 --> 00:32:20.480]   all know about, you know, like GPT three or something.
[00:32:20.480 --> 00:32:27.800]   Um, okay, I'm gonna so I was in a hackathon earlier this year.
[00:32:27.800 --> 00:32:34.480]   So I'm going to shout out Courtney Gibbons and Brady cannot pronounce his last name.
[00:32:34.480 --> 00:32:38.320]   But they so the project that I made for that hackathon was actually pretty cool.
[00:32:38.320 --> 00:32:44.080]   My friend, my friend Raj and I, we made like a phone box.
[00:32:44.080 --> 00:32:46.560]   So that you know, in the pandemic, I feel like we use our phones a lot.
[00:32:46.560 --> 00:32:51.360]   So you put your your phone into this box, and it only like lets you take it out after
[00:32:51.360 --> 00:32:52.360]   a certain amount of time.
[00:32:52.360 --> 00:32:57.120]   And it only lets you or sorry, it only like lets you take it out if like a certain amount
[00:32:57.120 --> 00:32:58.440]   of time hasn't elapsed yet.
[00:32:58.440 --> 00:33:01.520]   And you're only allowed to take it out a couple of times a day pretty much.
[00:33:01.520 --> 00:33:04.320]   So it's supposed to limit like, it's like, you know, your phone is like kind of out of
[00:33:04.320 --> 00:33:06.800]   sight out of mind.
[00:33:06.800 --> 00:33:13.160]   And then so then, but like my friend Courtney and Brady, they made a really cool contraption.
[00:33:13.160 --> 00:33:15.200]   It was called the Zoom stay clock.
[00:33:15.200 --> 00:33:19.200]   And so it was pretty much like a clock that you can put outside your door that links to
[00:33:19.200 --> 00:33:20.780]   your computer.
[00:33:20.780 --> 00:33:23.280]   So it knows when like your zoom meetings are for class.
[00:33:23.280 --> 00:33:27.440]   And it has like a timer that says like, how much time left you have in your class so that
[00:33:27.440 --> 00:33:32.280]   your parents don't like walk in while you're in zoom, and like barge in and interrupt the
[00:33:32.280 --> 00:33:33.680]   class and everything like that.
[00:33:33.680 --> 00:33:38.440]   It'll like it'll tell them right there like, okay, I got zoom for like 35 more minutes.
[00:33:38.440 --> 00:33:40.960]   So yeah, those are the two pretty cool things.
[00:33:40.960 --> 00:33:43.800]   I mean, I made one and then you know, theirs was awesome as well.
[00:33:43.800 --> 00:33:44.800]   So yeah.
[00:33:44.800 --> 00:33:48.300]   I could for sure use a zoom stay clock.
[00:33:48.300 --> 00:33:52.040]   You know, sometimes when I'm running the when I'm running the show, I put a little on air
[00:33:52.040 --> 00:33:55.840]   sign on my on my door, but it's I'd love to automate that.
[00:33:55.840 --> 00:33:56.840]   That'd be huge.
[00:33:56.840 --> 00:33:59.440]   Yeah, it was it was pretty cool.
[00:33:59.440 --> 00:34:04.640]   Yeah, they did a really good job with it.
[00:34:04.640 --> 00:34:07.640]   And yeah, for myself.
[00:34:07.640 --> 00:34:14.520]   I saw this really cool paper, I think it was maybe like five months ago, where these people,
[00:34:14.520 --> 00:34:19.920]   these researchers at CMU, they were able to given any picture, like a picture of like
[00:34:19.920 --> 00:34:25.880]   a courtyard, or like the front of a building, they're able to transform a picture into like
[00:34:25.880 --> 00:34:29.520]   a 3d environment, which I thought was really cool.
[00:34:29.520 --> 00:34:31.400]   And, you know, I'm interested in graphics.
[00:34:31.400 --> 00:34:34.640]   So like, just be able to like take a picture and just like be able to transform it into
[00:34:34.640 --> 00:34:36.840]   like any 3d environment.
[00:34:36.840 --> 00:34:37.960]   I thought that was really awesome.
[00:34:37.960 --> 00:34:39.240]   So I'd have to say that one's
[00:34:39.240 --> 00:34:40.240]   I see.
[00:34:40.240 --> 00:34:41.240]   Yeah.
[00:34:41.240 --> 00:34:46.560]   So this is this the neural radiance fields paper?
[00:34:46.560 --> 00:34:50.080]   Or is this a different the neural radiance fields was one where you could like take,
[00:34:50.080 --> 00:34:54.040]   like, you know, a couple of images kind of sparsely around an object and then turn it
[00:34:54.040 --> 00:34:55.580]   into a 3d model?
[00:34:55.580 --> 00:34:57.760]   Is this like in that line of work?
[00:34:57.760 --> 00:34:58.760]   Or is this something different?
[00:34:58.760 --> 00:34:59.760]   I think it's similar.
[00:34:59.760 --> 00:35:02.640]   I don't think it was that exact paper.
[00:35:02.640 --> 00:35:04.960]   But I forgot what the paper was called.
[00:35:04.960 --> 00:35:07.480]   But it's pretty, pretty similar, though.
[00:35:07.480 --> 00:35:08.480]   Got it.
[00:35:08.480 --> 00:35:12.280]   Yeah, I can't remember where the nerf whether the nerf paper was from CMU or not.
[00:35:12.280 --> 00:35:15.640]   I feel like it was from one of the big, like famous CS departments.
[00:35:15.640 --> 00:35:21.640]   But anyway, cool paper, lot a lot of exciting stuff in the in the neural graphics world
[00:35:21.640 --> 00:35:24.520]   for sure.
[00:35:24.520 --> 00:35:30.480]   Hard pivot, startup idea, what is your billion dollar startup idea that you're sitting on
[00:35:30.480 --> 00:35:34.520]   for when you finally decide to pursue your founder dreams?
[00:35:34.520 --> 00:35:37.960]   I'll give you I'll give you a second to dredge this up.
[00:35:37.960 --> 00:35:43.680]   Also, you know, there's concerns about copyright, you might need to insert a small bad idea
[00:35:43.680 --> 00:35:46.200]   into your good ideas that no one can steal it.
[00:35:46.200 --> 00:35:47.200]   But yeah.
[00:35:47.200 --> 00:35:48.200]   Okay, cool.
[00:35:48.200 --> 00:35:49.200]   All right.
[00:35:49.200 --> 00:35:50.200]   Let me think about it.
[00:35:50.200 --> 00:35:52.240]   I've definitely thought about this before.
[00:35:52.240 --> 00:35:56.760]   Oh, um, this isn't a startup idea.
[00:35:56.760 --> 00:36:00.440]   I don't know if we can make this into like a corporation.
[00:36:00.440 --> 00:36:04.080]   But this is something that I thought about with my boyfriend over the summer that we
[00:36:04.080 --> 00:36:05.400]   never actually ended up doing.
[00:36:05.400 --> 00:36:08.120]   But I thought it would have been pretty cool at the time.
[00:36:08.120 --> 00:36:10.080]   But I don't know, we got busy and stuff.
[00:36:10.080 --> 00:36:14.080]   And it just didn't work with like school starting and everything.
[00:36:14.080 --> 00:36:17.920]   But what we wanted to do is we wanted to make this app, I'm not going to go into too many
[00:36:17.920 --> 00:36:21.360]   specifics, because I kind of have some intention of making it.
[00:36:21.360 --> 00:36:27.240]   We wanted to make this app that pretty much it's like a one stop shop to be able to support
[00:36:27.240 --> 00:36:29.600]   any like political issue that you want.
[00:36:29.600 --> 00:36:34.640]   So it's an app and it has like a bunch of like different links to different types of
[00:36:34.640 --> 00:36:36.560]   issues and like ways that you can get involved.
[00:36:36.560 --> 00:36:42.560]   So it can provide you with anything from like upcoming rallies in your area, to petitions
[00:36:42.560 --> 00:36:48.120]   that you can sign to books that you can read to corporations that you can support.
[00:36:48.120 --> 00:36:49.640]   That was pretty much our idea.
[00:36:49.640 --> 00:36:55.240]   And like, especially like in in May, when like, the Black Lives Matter movement kind
[00:36:55.240 --> 00:36:58.960]   of got a little bit more intense.
[00:36:58.960 --> 00:37:02.000]   From you know, everything that was happening at the time, that was definitely something
[00:37:02.000 --> 00:37:03.840]   that was on my mind.
[00:37:03.840 --> 00:37:06.720]   And so I think that's kind of where the idea came from.
[00:37:06.720 --> 00:37:13.640]   There's a there's definitely some choppy waters to wade into as your first as your first startup
[00:37:13.640 --> 00:37:14.640]   idea.
[00:37:14.640 --> 00:37:18.160]   But I think yeah, people could definitely definitely benefit from better organization
[00:37:18.160 --> 00:37:19.160]   tools.
[00:37:19.160 --> 00:37:22.320]   Actually, I did read some interesting stuff.
[00:37:22.320 --> 00:37:29.240]   There's some like a lot of the like labor unions that are moving to organize, like,
[00:37:29.240 --> 00:37:32.960]   you know, groups that haven't been organized in the past, like service workers, you know,
[00:37:32.960 --> 00:37:38.380]   food service workers, things like that, have been embracing more digital technology.
[00:37:38.380 --> 00:37:42.000]   And so organizing, you know, fight the fight for 15 movement, for example, use a lot of
[00:37:42.000 --> 00:37:43.000]   digital organizing.
[00:37:43.000 --> 00:37:47.560]   So there's, I think there's a it's maybe not a billion dollar startup, because maybe you
[00:37:47.560 --> 00:37:50.760]   can't make a billion dollars helping people.
[00:37:50.760 --> 00:37:53.720]   But but it's a good startup idea.
[00:37:53.720 --> 00:38:02.640]   And okay, yeah, for me, I'll be straightforward with mine.
[00:38:02.640 --> 00:38:05.160]   I've had this on mind for a while.
[00:38:05.160 --> 00:38:10.920]   For me, like for I see machine learning engineering, I feel like a lot of companies employ machine
[00:38:10.920 --> 00:38:14.080]   learning engineers like to help build a product.
[00:38:14.080 --> 00:38:19.080]   But I feel like there's less so companies that are just purely based on like, machine
[00:38:19.080 --> 00:38:24.000]   learning, like consulting, specifically for like automation tasks.
[00:38:24.000 --> 00:38:28.520]   I feel like there aren't that too many companies that do like, you know, automation for like,
[00:38:28.520 --> 00:38:32.640]   no various industries or various like, subtasks.
[00:38:32.640 --> 00:38:37.760]   And so I think my idea is kind of like building the Oracle, but instead of like, software
[00:38:37.760 --> 00:38:42.240]   for like, machine learning and automation.
[00:38:42.240 --> 00:38:44.280]   So that's kind of like my idea.
[00:38:44.280 --> 00:38:48.600]   I know there's like, still time until like, machine learning can like reach that point
[00:38:48.600 --> 00:38:50.440]   where it could like automate a lot of different tasks.
[00:38:50.440 --> 00:38:54.120]   But that's my idea that I'm aiming for.
[00:38:54.120 --> 00:38:57.200]   Nice, ambitious, for sure.
[00:38:57.200 --> 00:38:58.720]   Yeah.
[00:38:58.720 --> 00:39:03.760]   So with with the last of my I don't think I made it to 10 epochs.
[00:39:03.760 --> 00:39:08.840]   But this is going to be the last of our popcorn epoch questions.
[00:39:08.840 --> 00:39:17.800]   What is somebody in who's somebody in the field of AI who you admire the most?
[00:39:17.800 --> 00:39:21.800]   Maybe not the most, maybe there's a bunch of people who are all at roughly the same
[00:39:21.800 --> 00:39:28.320]   level, you know, you don't have to worry about offending anybody.
[00:39:28.320 --> 00:39:31.360]   I'm gonna say Dr. Lachelle Hadley.
[00:39:31.360 --> 00:39:34.080]   She was honestly amazing.
[00:39:34.080 --> 00:39:36.640]   And her story.
[00:39:36.640 --> 00:39:41.720]   If you haven't like listened to the, you haven't listened to the podcast yet, I recommend giving
[00:39:41.720 --> 00:39:42.720]   a listen.
[00:39:42.720 --> 00:39:47.600]   But if you're looking for a more digestible clip on the ACM UCLA YouTube, there is a clip
[00:39:47.600 --> 00:39:52.920]   where she talks about pretty much what she did at Google, and how like right after she
[00:39:52.920 --> 00:39:56.720]   talked at Google, and, you know, some of her like, experience, like she just pretty much
[00:39:56.720 --> 00:40:01.080]   talks about, you know, how when she was at Google campus, like working as like, a resident
[00:40:01.080 --> 00:40:04.440]   for them, she did experience a lot of discrimination.
[00:40:04.440 --> 00:40:07.000]   And she did feel a little bit left out.
[00:40:07.000 --> 00:40:12.160]   But she she really toughed it out, because she knew that what she was doing was important.
[00:40:12.160 --> 00:40:13.760]   And so she was very inspiring.
[00:40:13.760 --> 00:40:19.080]   And yeah, I have to say, probably her.
[00:40:19.080 --> 00:40:22.680]   We'll definitely come back to that after we get Matt's answer.
[00:40:22.680 --> 00:40:26.680]   So actually, I was still my answer, unfortunately.
[00:40:26.680 --> 00:40:33.160]   But yeah, honestly, for some of the reasons I said, of course, but also, were like, she
[00:40:33.160 --> 00:40:36.280]   really inspired me in terms of like teaching.
[00:40:36.280 --> 00:40:40.640]   She's a really like personal, or like, she's really relatable with everything she says.
[00:40:40.640 --> 00:40:43.320]   And I really love teaching AI.
[00:40:43.320 --> 00:40:46.600]   And just hearing some of the responses about like how she teaches or how she approaches
[00:40:46.600 --> 00:40:48.800]   it just really inspires me.
[00:40:48.800 --> 00:40:52.680]   So yeah, I'd have to say the shell Hadley as well.
[00:40:52.680 --> 00:40:54.240]   Mm hmm.
[00:40:54.240 --> 00:40:55.240]   Yeah, yeah.
[00:40:55.240 --> 00:41:02.240]   So I that that was a really incredible episode of the podcast, really great interview.
[00:41:02.240 --> 00:41:05.560]   And one of the pieces of it, like I'd heard a little bit about that story.
[00:41:05.560 --> 00:41:10.520]   So just for folks who haven't heard the heard this episode, or weren't there when it was
[00:41:10.520 --> 00:41:18.440]   recorded, the she was part of like Google did a program where they wanted to like increase
[00:41:18.440 --> 00:41:23.280]   their surface area with historically black colleges and universities in the United States.
[00:41:23.280 --> 00:41:26.320]   They invited like they invited people over.
[00:41:26.320 --> 00:41:31.000]   I think they at one point it was called Howard West was like one version of the name of the
[00:41:31.000 --> 00:41:32.400]   program.
[00:41:32.400 --> 00:41:36.240]   And they ran into serious issues with like discrimination and like casual basically a
[00:41:36.240 --> 00:41:37.960]   casual racism on Google's campus.
[00:41:37.960 --> 00:41:43.440]   Dr. Hadley, of course, gives a much better version of this story than I can give in the
[00:41:43.440 --> 00:41:45.720]   in the podcast, you should definitely check it out.
[00:41:45.720 --> 00:41:51.960]   But one one aspect of the story I didn't know was that the author of the Google manifesto,
[00:41:51.960 --> 00:41:57.560]   James Damore was actually like, like published it immediately after they left the campus.
[00:41:57.560 --> 00:42:01.960]   And like attended some of their events, which is just like a wild twist to this to this
[00:42:01.960 --> 00:42:04.840]   story, which I'd never heard anywhere else.
[00:42:04.840 --> 00:42:09.040]   Yeah, I couldn't believe it, honestly.
[00:42:09.040 --> 00:42:18.480]   Yeah, I our old our old outreach director Arjun told us, you have to get LaShelle, like
[00:42:18.480 --> 00:42:21.200]   Dr. LaShelle Hadley on your podcast.
[00:42:21.200 --> 00:42:22.480]   She's amazing.
[00:42:22.480 --> 00:42:25.880]   And she has such a cool story.
[00:42:25.880 --> 00:42:32.040]   I just asked her about it in the interview, and she said it and like, it's like one of
[00:42:32.040 --> 00:42:35.600]   those problems that kind of makes you like kind of shaking with anger, I guess, like,
[00:42:35.600 --> 00:42:38.360]   I don't know, I take that stuff super personally.
[00:42:38.360 --> 00:42:42.800]   I know, like some people like feel for it, but like, they don't take it like, like very,
[00:42:42.800 --> 00:42:43.800]   very personally.
[00:42:43.800 --> 00:42:48.560]   But I just got I remember I was like, super, super shocked, super mad, you know, almost
[00:42:48.560 --> 00:42:49.560]   cried.
[00:42:49.560 --> 00:42:50.560]   Yeah.
[00:42:50.560 --> 00:42:51.560]   Yeah.
[00:42:51.560 --> 00:42:56.840]   I think, yeah, it's, it can.
[00:42:56.840 --> 00:43:02.520]   Yeah, it's really deeply upsetting, especially the I don't know, the sort of like, disconnected,
[00:43:02.520 --> 00:43:05.960]   like thinking and thinking about these problems and working on these problems with people
[00:43:05.960 --> 00:43:11.000]   who really get it and know, like, you know, how, like how serious the problems are.
[00:43:11.000 --> 00:43:16.680]   And then sort of, I guess, I don't know, coming across people who are so just like, yeah,
[00:43:16.680 --> 00:43:18.000]   ignorant of the issues.
[00:43:18.000 --> 00:43:20.800]   So off base.
[00:43:20.800 --> 00:43:22.800]   Yeah.
[00:43:22.800 --> 00:43:30.880]   I guess, like on that, on that front, I like I really appreciate the like work that you're
[00:43:30.880 --> 00:43:36.680]   doing to, like to expose the like this, the stories of these people that you're interviewing
[00:43:36.680 --> 00:43:41.320]   to a to a broader audience to create a more inclusive AI community.
[00:43:41.320 --> 00:43:45.440]   I'm wondering what you think are some of the like, most critically important issues that
[00:43:45.440 --> 00:43:48.760]   that more inclusive community might resolve, right?
[00:43:48.760 --> 00:43:52.880]   Like what are the problems in AI right now and in the future that creating an inclusive
[00:43:52.880 --> 00:43:55.920]   community would help solve?
[00:43:55.920 --> 00:43:59.400]   Yeah, I think.
[00:43:59.400 --> 00:44:01.800]   Go ahead, Matt.
[00:44:01.800 --> 00:44:08.760]   Yeah, I really think that it becomes like the main issue is that a lot of models nowadays,
[00:44:08.760 --> 00:44:11.720]   they're really biased, especially with like NLP models.
[00:44:11.720 --> 00:44:18.520]   And I think just like our community is inclusive, but I don't think it's I don't think it's
[00:44:18.520 --> 00:44:22.360]   to the extent that it needs to be, because right now we have people of all backgrounds
[00:44:22.360 --> 00:44:25.040]   in AI that are speaking up.
[00:44:25.040 --> 00:44:27.200]   But I think we need more people.
[00:44:27.200 --> 00:44:30.480]   We need more people from these backgrounds to speak up so that becomes more prevalent
[00:44:30.480 --> 00:44:32.920]   that we need to address these issues.
[00:44:32.920 --> 00:44:37.960]   So I think just like biases and models and even biases and like data.
[00:44:37.960 --> 00:44:41.600]   I think that's probably the biggest issue and just like making sure that we're collecting
[00:44:41.600 --> 00:44:45.960]   like the right kind of data and like we're collecting like a different set of data and
[00:44:45.960 --> 00:44:50.920]   like we have people like people from like all different types of backgrounds like reviewing
[00:44:50.920 --> 00:44:55.080]   the data, you know, bring the models biases, just making sure that everything's in check.
[00:44:55.080 --> 00:44:58.120]   I think that's the most important part.
[00:44:58.120 --> 00:45:05.680]   Yeah, it does seem like, you know, the participation, right, getting more and more people to be
[00:45:05.680 --> 00:45:13.480]   like participants and to have like say and stake in these systems is like, you know,
[00:45:13.480 --> 00:45:18.320]   seems like the only way like rather than waiting for people, the users of Twitter to discover
[00:45:18.320 --> 00:45:21.720]   that the cropping function seems to be racially biased.
[00:45:21.720 --> 00:45:25.320]   Like, you know, that's something that should be picked up in pre-production and could be
[00:45:25.320 --> 00:45:31.360]   picked up in pre-production if we had a more like a community that was more inclusive and
[00:45:31.360 --> 00:45:33.360]   more diverse.
[00:45:33.360 --> 00:45:45.160]   Yeah, I would say so I honestly Matt has said it, but I think that just really quickly like
[00:45:45.160 --> 00:45:48.680]   the biggest thing is like bringing people to the table.
[00:45:48.680 --> 00:45:52.680]   I think that it's like quite unfair, like, I don't know, I've just I've heard like a
[00:45:52.680 --> 00:45:58.240]   lot of adults when I was in high school, like I met a lot of adults that were from like
[00:45:58.240 --> 00:46:02.080]   lower income communities that said like, oh, I was a kid and I knew that engineering was
[00:46:02.080 --> 00:46:04.280]   a thing or I knew that computer science was a thing.
[00:46:04.280 --> 00:46:06.160]   I would have totally gone involved with it.
[00:46:06.160 --> 00:46:10.440]   And I feel like, you know, it's that problem that leads us to have these like ignorant
[00:46:10.440 --> 00:46:15.080]   models put out and things that are just completely tone deaf, you know.
[00:46:15.080 --> 00:46:19.360]   And so I think, you know, bringing people to the table is really important and raising
[00:46:19.360 --> 00:46:23.240]   that awareness can solve a lot of issues for us.
[00:46:23.240 --> 00:46:29.800]   Yeah, I talked with somebody who is building a model that was like a camera to detect distracted
[00:46:29.800 --> 00:46:33.760]   driving so it could tell whether somebody was like paying attention or not, whether
[00:46:33.760 --> 00:46:38.480]   they were looking at their phone zoning out and like they really sold it as a safety thing.
[00:46:38.480 --> 00:46:43.280]   And I could definitely see the like the like potential safety benefits of something.
[00:46:43.280 --> 00:46:48.600]   But like, you know, they the like final use case was a company was going to pay to install
[00:46:48.600 --> 00:46:53.040]   it in the cars of the drivers in their fleet.
[00:46:53.040 --> 00:46:59.480]   And I feel like if you had like had enough experience working a like working this sort
[00:46:59.480 --> 00:47:03.880]   of job where you have a manager who's like breathing down your neck at all times, you
[00:47:03.880 --> 00:47:09.160]   know, or if you'd you know, if you'd worked in, you know, sort of like out there in the
[00:47:09.160 --> 00:47:14.360]   in the trenches more, you would be able to more clearly see the potential like the like
[00:47:14.360 --> 00:47:19.640]   power conflict there, the like ethical issues and the concerns with, you know, enforcing
[00:47:19.640 --> 00:47:23.720]   this AI surveillance on on truck drivers.
[00:47:23.720 --> 00:47:25.480]   Yeah, sure.
[00:47:25.480 --> 00:47:27.240]   That's cool.
[00:47:27.240 --> 00:47:28.520]   That's interesting.
[00:47:28.520 --> 00:47:29.880]   I haven't heard about that.
[00:47:29.880 --> 00:47:32.640]   It's an interesting example.
[00:47:32.640 --> 00:47:39.000]   I think Amazon ended up rolling it out to like some of their some of their fleets.
[00:47:39.000 --> 00:47:45.240]   But which and and got like a ton of pushback, like pretty much immediately from the drivers
[00:47:45.240 --> 00:47:47.400]   and from people who heard about it.
[00:47:47.400 --> 00:47:52.280]   Yeah, to me, just like things like that.
[00:47:52.280 --> 00:47:57.240]   It takes me back to my English class in high school when we read, you know, 1984.
[00:47:57.240 --> 00:48:01.320]   I'm not sure if you're all familiar with that book, but Big Brother is always watching over
[00:48:01.320 --> 00:48:05.720]   you and yeah, that's the kind of vibes that it gives me.
[00:48:05.720 --> 00:48:13.880]   I would definitely say, yeah, it's it's a little I found it interesting, actually, that
[00:48:13.880 --> 00:48:18.560]   one of your guests, Paco Guzman, we mentioned him earlier from Facebook, talked about being
[00:48:18.560 --> 00:48:22.000]   like very optimistic about the future of AI and machine learning.
[00:48:22.000 --> 00:48:26.320]   And I found that found that kind of surprising, because nowadays, I kind of associate optimism
[00:48:26.320 --> 00:48:31.080]   about machine learning with people who don't care about inclusion and don't care so much
[00:48:31.080 --> 00:48:38.760]   about about biases, about how machine learning interacts with existing systems of power.
[00:48:38.760 --> 00:48:47.480]   So I'm curious, I mean, where are you on the optimism, pessimism scale, you know, between
[00:48:47.480 --> 00:48:56.200]   between Paco and me, and what the future of of AI holds for, you know, for for not just
[00:48:56.200 --> 00:49:02.000]   inclusion within our community, but for like designing technologies that are beneficial
[00:49:02.000 --> 00:49:05.720]   and not harmful?
[00:49:05.720 --> 00:49:11.520]   I think that I don't know when I when I think of optimism, yeah, like there is that undertone
[00:49:11.520 --> 00:49:12.520]   of it.
[00:49:12.520 --> 00:49:19.560]   I'd like to be optimistic, I'd like to say that things are going to get better, but super
[00:49:19.560 --> 00:49:23.200]   sad like coming to realize like, when you're little, you think that everything is going
[00:49:23.200 --> 00:49:24.840]   to be fixed when you're older.
[00:49:24.840 --> 00:49:29.240]   And now that I'm like turning, I'm becoming an adult, I'm like 19 years old, I'm entering
[00:49:29.240 --> 00:49:34.800]   the adult realm, I'm starting to see just like, no, like, this is never gonna, it's
[00:49:34.800 --> 00:49:38.520]   not never gonna, it's gonna definitely decrease, but it's never gonna end.
[00:49:38.520 --> 00:49:45.320]   There are always going to be people who just are, you know, racist or prejudiced or biased
[00:49:45.320 --> 00:49:47.320]   and don't care.
[00:49:47.320 --> 00:49:52.680]   So I'm optimistic in the sense that, you know, it's going to decrease, but I'm pessimistic
[00:49:52.680 --> 00:49:57.520]   in the fact that, you know, life is always suffering.
[00:49:57.520 --> 00:50:06.040]   Yeah, so that's something that I've definitely begun to realize, but I will always work.
[00:50:06.040 --> 00:50:09.840]   My career goal is to always work to make it inclusive.
[00:50:09.840 --> 00:50:13.120]   That's something that like, I, you know, I can like grow up and like work at like a chip
[00:50:13.120 --> 00:50:17.680]   company and just, you know, be an electrical engineer, but I do want to do, I do want to
[00:50:17.680 --> 00:50:18.680]   have a social impact.
[00:50:18.680 --> 00:50:21.920]   I don't want to just be sitting behind a computer designing stuff, even though there's nothing
[00:50:21.920 --> 00:50:22.920]   wrong with that.
[00:50:22.920 --> 00:50:28.600]   I personally, in terms of my life goals, I want to be involved in a company or in a certain
[00:50:28.600 --> 00:50:35.460]   way in my life to be able to like increase the, like this applicability of technology
[00:50:35.460 --> 00:50:38.400]   and increase the representation.
[00:50:38.400 --> 00:50:45.960]   Yeah, I guess for myself, I don't have a definitive answer to that.
[00:50:45.960 --> 00:50:53.120]   I kind of lie in between, I think for me, how I approach like everything I do is, you
[00:50:53.120 --> 00:50:56.680]   know, I approached everything that like, I really want to leave everything like better
[00:50:56.680 --> 00:50:57.680]   than it already is.
[00:50:57.680 --> 00:51:03.720]   And, you know, kind of like what Abha said, I really want to like uplift people, especially
[00:51:03.720 --> 00:51:06.760]   like people like from diverse backgrounds, including myself.
[00:51:06.760 --> 00:51:11.440]   I really want to uplift these people so that like these problems become like less prevalent
[00:51:11.440 --> 00:51:17.680]   and, you know, just, just seeing like, like impacts that I've made, I'm not, I haven't
[00:51:17.680 --> 00:51:21.640]   made too many, but like small impacts I've made to like uplift people and just like bring
[00:51:21.640 --> 00:51:24.880]   people like into like tech or whatever it may be.
[00:51:24.880 --> 00:51:29.000]   You know, for me, that's like really satisfying and just like knowing that I'm uplifting people
[00:51:29.000 --> 00:51:34.560]   and you know, at the end of the day, like I can't control the entire world.
[00:51:34.560 --> 00:51:38.000]   But what I can control is like the impact I have on other people.
[00:51:38.000 --> 00:51:42.320]   And for me, that's like my main goal, just like having a positive impact on other people.
[00:51:42.320 --> 00:51:53.800]   And no, I understand that I can't change the world, but I'm doing the best I can.
[00:51:53.800 --> 00:52:01.840]   As I think keeping, keeping on a slightly maybe tenser or darker note here, a question
[00:52:01.840 --> 00:52:06.840]   I wanted to ask you about, since this is something that I think about a lot as somebody who,
[00:52:06.840 --> 00:52:12.360]   you know, wants to make things better without making them worse, so to speak, like how do
[00:52:12.360 --> 00:52:19.080]   you balance the need for like representation and exposure, like exposure of like people
[00:52:19.080 --> 00:52:22.840]   who are out there who don't know about, like, don't know about engineering as a potential
[00:52:22.840 --> 00:52:27.480]   career path, don't know about AI, like, you know, we, we got to get out there and contact
[00:52:27.480 --> 00:52:28.480]   them.
[00:52:28.480 --> 00:52:32.620]   We want to like, like provide role models, right?
[00:52:32.620 --> 00:52:38.360]   Get ahold of the people like Dr. Hadley, who are these like incredible role models.
[00:52:38.360 --> 00:52:42.560]   How do you balance that need with the dangers of like demanding that people provide basically
[00:52:42.560 --> 00:52:49.960]   this sort of like unpaid labor to resolve the like, you know, resolve the, the like
[00:52:49.960 --> 00:52:56.600]   crises of racism and like, cisheteropatriarchy and, you know, sort of demanding these things
[00:52:56.600 --> 00:53:00.960]   out of other people when they've been often burdened in so many other ways.
[00:53:00.960 --> 00:53:07.060]   I'm going to quote Dr. Hadley here.
[00:53:07.060 --> 00:53:12.060]   So she, she did run into that issue when she went to Google and she wanted to leave because
[00:53:12.060 --> 00:53:17.500]   she's like, it's not my job to make these people not racist.
[00:53:17.500 --> 00:53:24.600]   She stuck it out because, you know, her friend told her, you know, or like one of her colleagues
[00:53:24.600 --> 00:53:28.740]   told her like, this is, this is what we've been like working for.
[00:53:28.740 --> 00:53:31.880]   And you know, props to her for sticking it out, but obviously I would have understood
[00:53:31.880 --> 00:53:35.600]   if she wanted to just go home and not deal with that.
[00:53:35.600 --> 00:53:39.900]   And I think that's something that we, it's definitely a hard question.
[00:53:39.900 --> 00:53:45.320]   And you know, I, I am a female, so I do run into some issues, but I also understand that
[00:53:45.320 --> 00:53:49.920]   like I, I've had a privileged life, have had a pretty privileged life.
[00:53:49.920 --> 00:53:54.120]   I haven't had to run into any, many issues, thankfully, but I know that's not the same
[00:53:54.120 --> 00:53:55.120]   for everyone else.
[00:53:55.120 --> 00:53:59.660]   So I definitely just don't like to talk about it, like talk about it.
[00:53:59.660 --> 00:54:04.160]   Like I do know that or anything like that, but I don't know.
[00:54:04.160 --> 00:54:10.960]   I would just say, you know, props to the people who stuck it, who toughen out like Dr. Hadley
[00:54:10.960 --> 00:54:17.360]   and like go through very uncomfortable situations just so that she can like validate some, someone
[00:54:17.360 --> 00:54:20.240]   or, you know, fix a company's issue.
[00:54:20.240 --> 00:54:24.160]   But also, you know, props to the people that don't stand for it too.
[00:54:24.160 --> 00:54:25.720]   It's personal preference.
[00:54:25.720 --> 00:54:27.080]   It depends on the person.
[00:54:27.080 --> 00:54:31.520]   And I think that the whole point of this thing is like, we need to stop telling them.
[00:54:31.520 --> 00:54:37.040]   We need to just stop telling people who are, my belief is that I think we should like,
[00:54:37.040 --> 00:54:41.880]   we shouldn't have expectations about like model minorities, for example.
[00:54:41.880 --> 00:54:45.520]   I think that they have the rights to make the decision based on what they would like
[00:54:45.520 --> 00:54:51.200]   to do and how they would like to be represented and never expect them to fix everything for
[00:54:51.200 --> 00:54:53.880]   us because it's not their job.
[00:54:53.880 --> 00:54:57.600]   You know, it's the oppressor's job, if anything.
[00:54:57.600 --> 00:54:59.160]   But yeah.
[00:54:59.160 --> 00:55:06.000]   >> Yeah, that was another really great moment, I thought, of the interview with Dr. Hadley
[00:55:06.000 --> 00:55:11.040]   where she talked, I think her friend called it the civil rights struggle of our time,
[00:55:11.040 --> 00:55:12.840]   which I thought was very apt.
[00:55:12.840 --> 00:55:17.480]   It does seem like there's a lot of like focus and attention converging on the ways like
[00:55:17.480 --> 00:55:23.160]   machine learned algorithms are rolling back the successes of labor rights and civil rights
[00:55:23.160 --> 00:55:25.320]   movements of the last century.
[00:55:25.320 --> 00:55:33.440]   Matt, any thoughts about the unpaid labor of diversity and equity and inclusion or like
[00:55:33.440 --> 00:55:39.560]   tips for how to go about minimizing that impact and maximizing the benefit?
[00:55:39.560 --> 00:55:44.360]   >> Yeah, it's definitely a really tough question to answer.
[00:55:44.360 --> 00:55:50.760]   But I think about it a lot for myself.
[00:55:50.760 --> 00:55:56.280]   And like I always said, I understand people that do something about it and don't do something
[00:55:56.280 --> 00:55:57.280]   about it.
[00:55:57.280 --> 00:56:04.280]   Like for myself, how I view it is like if someone like a minority is able to push through
[00:56:04.280 --> 00:56:09.900]   all the struggles and get to a good position in life, that's what they've been working
[00:56:09.900 --> 00:56:13.000]   for their entire life and they've been going through all their struggles.
[00:56:13.000 --> 00:56:21.120]   And quite frankly, it might be hard for them to work more than they need to, to help uplift
[00:56:21.120 --> 00:56:22.120]   others.
[00:56:22.120 --> 00:56:30.680]   But from what I've seen, from my experience, those type of people, they really love like
[00:56:30.680 --> 00:56:32.980]   who they grew up with or like where they grew up with.
[00:56:32.980 --> 00:56:36.840]   And I always see them going back to the community that like helped uplift them.
[00:56:36.840 --> 00:56:39.320]   And that's always really inspiring.
[00:56:39.320 --> 00:56:41.560]   And now that inspires me to do the same thing.
[00:56:41.560 --> 00:56:49.880]   And I'm not saying it's like necessarily like bad culture or like a culture that like, I'm
[00:56:49.880 --> 00:56:54.540]   not saying it's like something that everyone needs to follow, but that's what I see most
[00:56:54.540 --> 00:56:55.540]   people do.
[00:56:55.540 --> 00:56:58.240]   And yeah, that's what's inspired me.
[00:56:58.240 --> 00:57:00.240]   >> Got it.
[00:57:00.240 --> 00:57:02.200]   Yeah.
[00:57:02.200 --> 00:57:08.320]   I think maybe I've been told that, well, one, we should probably end on a happier note,
[00:57:08.320 --> 00:57:09.320]   if possible.
[00:57:09.320 --> 00:57:13.720]   And then also, you're supposed to end videos with a call to action.
[00:57:13.720 --> 00:57:19.600]   I don't know if you've come across this, when learning how to do your media production.
[00:57:19.600 --> 00:57:24.640]   So I'm wondering if you have any thoughts about key steps that you think folks can take
[00:57:24.640 --> 00:57:29.600]   to make like our community more representative and more inclusive.
[00:57:29.600 --> 00:57:36.640]   So maybe steps for students and professors, things for engineers, managers and founders,
[00:57:36.640 --> 00:57:41.000]   maybe for broader sort of stakeholders in AI, what they can do, people who are going
[00:57:41.000 --> 00:57:47.320]   to be impacted by these systems, which is pretty much everybody, or other non-technical
[00:57:47.320 --> 00:57:56.840]   professionals, lawyers, doctors, who maybe can be like allies in this.
[00:57:56.840 --> 00:58:01.240]   For any of those groups, what are your thoughts about like key steps that you think they could
[00:58:01.240 --> 00:58:02.240]   take?
[00:58:02.240 --> 00:58:11.000]   >> Yeah, I think just in terms of like being like up to date on what AI is, if you're like
[00:58:11.000 --> 00:58:14.920]   part of a different industry, I don't really think that there's a need to like learn all
[00:58:14.920 --> 00:58:17.760]   the technical stuff about it.
[00:58:17.760 --> 00:58:21.880]   But just understand there's like a lot of issues right now, especially with like biases
[00:58:21.880 --> 00:58:23.560]   and like machine learning models.
[00:58:23.560 --> 00:58:27.960]   And yeah, I think the biggest thing to take away is that, you know, AI isn't perfect.
[00:58:27.960 --> 00:58:30.640]   There are a lot of things we need to work towards.
[00:58:30.640 --> 00:58:36.000]   And the next step towards that is making sure that the people that make these models come
[00:58:36.000 --> 00:58:37.480]   from a diverse set of backgrounds.
[00:58:37.480 --> 00:58:41.240]   And I know we repeated that so many times, but I can't emphasize that enough.
[00:58:41.240 --> 00:58:48.040]   And I just think that the best way to do it is for the people that have made it to the
[00:58:48.040 --> 00:58:53.600]   stage where they are working behind the scenes with AI, just sharing their story, how they
[00:58:53.600 --> 00:58:55.560]   got there.
[00:58:55.560 --> 00:58:59.900]   I think just sharing stories, humans connect by sharing stories.
[00:58:59.900 --> 00:59:04.080]   And if we can inspire others by sharing their story and uplifting other people to make our
[00:59:04.080 --> 00:59:10.640]   community more diverse, then I think that's the best way forward.
[00:59:10.640 --> 00:59:18.160]   Before we get Ava's answer, I would say I was recently reading Yeshe Milner, who is
[00:59:18.160 --> 00:59:24.560]   the head of Data for Black Lives, released a really, really great sort of like paper,
[00:59:24.560 --> 00:59:30.140]   almost a short book at this point, about data capitalism that I was reading.
[00:59:30.140 --> 00:59:34.780]   And she talks about data washing in it, which is this sort of way that people take what
[00:59:34.780 --> 00:59:41.860]   should be decisions that are made by people about who's paid how much when, and then automate
[00:59:41.860 --> 00:59:46.540]   them and use the like automation process to hide their bias.
[00:59:46.540 --> 00:59:49.780]   So I think what you said about one of the most important things that people outside
[00:59:49.780 --> 00:59:54.980]   of our community, what they can do is sort of like learn not to trust AI models a little
[00:59:54.980 --> 00:59:55.980]   bit.
[00:59:55.980 --> 01:00:00.660]   Don't give them that like sort of objectivity that they automatically, you know, statistics
[01:00:00.660 --> 01:00:03.700]   can't be racist or whatever attitude.
[01:00:03.700 --> 01:00:10.140]   Like cutting that out seems to be a really important step people can take to make their
[01:00:10.140 --> 01:00:19.540]   to maybe mitigate some of the negative impacts that automation and machine learning can have.
[01:00:19.540 --> 01:00:28.500]   So I think that my call to action or the advice that I can give is if you're in a position
[01:00:28.500 --> 01:00:34.780]   where you're teaching someone something, or you're giving advice to everyone, don't underlook
[01:00:34.780 --> 01:00:38.140]   new inclusive teaching techniques.
[01:00:38.140 --> 01:00:43.180]   It can definitely be hard for individuals from an older generation or anyone who's just
[01:00:43.180 --> 01:00:53.500]   stubborn frankly, to like learn a new way to explain things, but don't like be able
[01:00:53.500 --> 01:00:54.500]   to change that.
[01:00:54.500 --> 01:00:59.020]   I think the way that people learn things have such a profound impact on like whether or
[01:00:59.020 --> 01:01:01.060]   not they're going to get into it or something like that.
[01:01:01.060 --> 01:01:05.260]   Like if you're a teacher, you already know this, but you literally have the ability to
[01:01:05.260 --> 01:01:09.180]   change lives and you probably have one of the most important jobs in society.
[01:01:09.180 --> 01:01:14.260]   And unfortunately I feel like a lot of people don't recognize that, but yeah, be open to
[01:01:14.260 --> 01:01:18.820]   like looking up and researching about new inclusive teaching techniques.
[01:01:18.820 --> 01:01:23.140]   And also just for anyone in general, reading is super helpful.
[01:01:23.140 --> 01:01:27.620]   I think that reading from the perspective of, you know, again, individuals who have
[01:01:27.620 --> 01:01:32.360]   taken on the responsibility, who are minorities who have taken that responsibility to try
[01:01:32.360 --> 01:01:36.620]   and fix things and stuff like that, reading everything, not reading everything, but like
[01:01:36.620 --> 01:01:40.620]   reading things that they write and reading their experiences and listening to the stories
[01:01:40.620 --> 01:01:43.900]   that Matt talked about are super important.
[01:01:43.900 --> 01:01:47.460]   And it just, it makes us empathize with each other a little bit more.
[01:01:47.460 --> 01:01:52.700]   I think that in general, all these issues come from a lack of empathy and, you know,
[01:01:52.700 --> 01:01:53.700]   ignorance as well.
[01:01:53.700 --> 01:01:56.820]   But I think that if we would just empathize with each other a little bit more, a lot of
[01:01:56.820 --> 01:01:59.580]   things would be nicer and easier.
[01:01:59.580 --> 01:02:02.500]   And I think the way that you empathize is you learn about people's struggles and you
[01:02:02.500 --> 01:02:03.500]   listen to them.
[01:02:03.500 --> 01:02:08.300]   And, you know, reading is the best way to listen to people's stories.
[01:02:08.300 --> 01:02:14.660]   Yeah, I guess to close us out here, do you have any quick book recommendations or any
[01:02:14.660 --> 01:02:17.580]   other media recommendations for these kinds of stories?
[01:02:17.580 --> 01:02:22.020]   I know like it's growing, it's, there's a lot more stuff than there was five years ago
[01:02:22.020 --> 01:02:23.020]   about this.
[01:02:23.020 --> 01:02:29.420]   But, you know, it'd be great to know where the, what the good ones are.
[01:02:29.420 --> 01:02:34.620]   I would say the only ones that I can like really think of right now are our old outreach
[01:02:34.620 --> 01:02:35.900]   director Arjun.
[01:02:35.900 --> 01:02:38.740]   They write some pretty great articles.
[01:02:38.740 --> 01:02:43.340]   Arjun Subramanian, I believe is their last name.
[01:02:43.340 --> 01:02:48.720]   I again, I met them online, so I never like had to say their last name, but they write
[01:02:48.720 --> 01:02:52.500]   some pretty awesome articles about bias and stuff like that.
[01:02:52.500 --> 01:02:58.220]   And then also another one of our directors at UCLA, her name is Maya Rahman.
[01:02:58.220 --> 01:03:04.740]   She writes some pretty cool articles about bias in Google, especially on Medium, which
[01:03:04.740 --> 01:03:06.460]   is like a blog website.
[01:03:06.460 --> 01:03:08.700]   So she has a pretty cool blog as well.
[01:03:08.700 --> 01:03:15.900]   And then one book that I read that probably changed the way I think about oppression,
[01:03:15.900 --> 01:03:21.220]   like in general, this book, it's very general, it has nothing to do with AI, but it's called
[01:03:21.220 --> 01:03:23.420]   Pedagogy of the Oppressed.
[01:03:23.420 --> 01:03:25.360]   And it's an interesting read.
[01:03:25.360 --> 01:03:26.360]   It's definitely a cool read.
[01:03:26.360 --> 01:03:30.980]   Unfortunately, I was only able to get through like the first section of it, because it has
[01:03:30.980 --> 01:03:38.860]   a couple of sections, but that was super interesting, for sure.
[01:03:38.860 --> 01:03:41.600]   That is one of my siblings' favorite books also.
[01:03:41.600 --> 01:03:46.020]   They just like absolutely love Pedagogy of the Oppressed.
[01:03:46.020 --> 01:03:50.540]   And also, by the way, I put the names for Arjun and for Maya in the YouTube chat for
[01:03:50.540 --> 01:03:52.980]   people who are listening.
[01:03:52.980 --> 01:03:55.900]   So if you check that out, you can get their names.
[01:03:55.900 --> 01:03:56.900]   Matt?
[01:03:56.900 --> 01:04:06.300]   Yeah, I don't have anything like specific, but just for me, I already said that I'm a
[01:04:06.300 --> 01:04:11.680]   big junkie for content, media content earlier.
[01:04:11.680 --> 01:04:16.420]   Sometimes on YouTube, I just end up like in the YouTube spiral where I keep going through
[01:04:16.420 --> 01:04:17.420]   a lot of stuff.
[01:04:17.420 --> 01:04:22.820]   But a lot of things I watch are about just stories about random people around the world.
[01:04:22.820 --> 01:04:28.300]   And from Europe, third world countries, whatever it may be, in different situations, I love
[01:04:28.300 --> 01:04:30.700]   learning about people and their struggles.
[01:04:30.700 --> 01:04:37.380]   And it's easy to watch people in their prime and their highlights, like most social medias
[01:04:37.380 --> 01:04:38.380]   now.
[01:04:38.380 --> 01:04:44.300]   But for me, I enjoy learning about how people live day to day and just the struggles that
[01:04:44.300 --> 01:04:45.300]   they face.
[01:04:45.300 --> 01:04:50.900]   And ever since I started doing that, maybe six months ago, I just become more empathetic,
[01:04:50.900 --> 01:04:52.580]   more understanding of people.
[01:04:52.580 --> 01:04:55.740]   And I just think that's the best way to do it, just learning other people's stories,
[01:04:55.740 --> 01:04:56.820]   where they come from.
[01:04:56.820 --> 01:04:58.020]   Everyone's different.
[01:04:58.020 --> 01:05:01.660]   And just being able to emphasize with someone that a complete stranger, because you never
[01:05:01.660 --> 01:05:04.300]   know where they came from.
[01:05:04.300 --> 01:05:10.460]   I just think that's the best way to approach just everyone or just life in general.
[01:05:10.460 --> 01:05:15.740]   Well, I'd like to thank to close this out here.
[01:05:15.740 --> 01:05:19.600]   Thank both of you for coming on and telling a little bit of your stories in the course
[01:05:19.600 --> 01:05:24.180]   of this of this hour and sharing that with with our viewers.
[01:05:24.180 --> 01:05:29.060]   Hopefully they find them additionally, you know, inspiring, or they learn a little bit
[01:05:29.060 --> 01:05:33.500]   more about how to empathize from from your stories.
[01:05:33.500 --> 01:05:36.900]   So thank you so much for coming.
[01:05:36.900 --> 01:05:40.140]   I'm going to close out the live stream.
[01:05:40.140 --> 01:05:45.580]   Thanks to everybody in the audience and to all the folks watching this later.
[01:05:45.580 --> 01:05:48.940]   the description of the video for links to some of the things that we...

