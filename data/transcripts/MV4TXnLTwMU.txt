
[00:00:00.000 --> 00:00:02.420]   All right, I'm going to start going live here in a second,
[00:00:02.420 --> 00:00:04.680]   and we're going to open this up.
[00:00:04.680 --> 00:00:05.400]   Great.
[00:00:05.400 --> 00:00:06.640]   OK.
[00:00:06.640 --> 00:00:12.680]   So folks will start coming in on Zoom and on YouTube.
[00:00:12.680 --> 00:00:15.840]   So welcome, everyone who's coming in.
[00:00:15.840 --> 00:00:19.880]   So folks on YouTube, if you were waiting
[00:00:19.880 --> 00:00:21.720]   in the original version of the stream,
[00:00:21.720 --> 00:00:24.840]   you may have seen a brief video of me looking very confused.
[00:00:24.840 --> 00:00:27.640]   That was due to a little glitch with our streaming setup.
[00:00:27.640 --> 00:00:30.600]   Apologies for that.
[00:00:30.600 --> 00:00:34.280]   To everybody who didn't see that, good.
[00:00:34.280 --> 00:00:35.360]   But welcome.
[00:00:35.360 --> 00:00:38.120]   Welcome back to the Weights and Biases Salon,
[00:00:38.120 --> 00:00:40.760]   to the first salon of 2021.
[00:00:40.760 --> 00:00:42.800]   I am-- I'm excited.
[00:00:42.800 --> 00:00:44.720]   I feel like I say this about all the salons,
[00:00:44.720 --> 00:00:48.440]   but I am very excited about this salon.
[00:00:48.440 --> 00:00:54.520]   We have Greg Yang on here from Microsoft Research,
[00:00:54.520 --> 00:00:57.000]   Microsoft Research AI, who has been doing
[00:00:57.000 --> 00:01:00.280]   some work in this series of papers, the Tensor Program
[00:01:00.280 --> 00:01:03.000]   series, that has culminated in this really exciting work
[00:01:03.000 --> 00:01:05.320]   on infinite-width neural networks that can actually
[00:01:05.320 --> 00:01:08.200]   learn features and outperform finite-width neural networks,
[00:01:08.200 --> 00:01:10.120]   which is something that's been, in some ways,
[00:01:10.120 --> 00:01:12.200]   30 years in the making since Radford Neal's
[00:01:12.200 --> 00:01:14.320]   early papers on the subject.
[00:01:14.320 --> 00:01:17.000]   So I'm really excited to hear him talk about it
[00:01:17.000 --> 00:01:18.960]   and to get questions from folks.
[00:01:18.960 --> 00:01:21.480]   So right now, the way that folks--
[00:01:21.480 --> 00:01:24.000]   in the audience, for folks who haven't joined a webinar
[00:01:24.000 --> 00:01:27.000]   like this before in Zoom, the way to ask questions
[00:01:27.000 --> 00:01:28.800]   is through the Q&A button.
[00:01:28.800 --> 00:01:30.360]   So you can type your questions there,
[00:01:30.360 --> 00:01:35.640]   and I'll make sure that those get relayed to Greg.
[00:01:35.640 --> 00:01:37.680]   All right, so if you have any questions,
[00:01:37.680 --> 00:01:41.800]   I believe you might also be able to ask questions, yes,
[00:01:41.800 --> 00:01:42.560]   via the chat.
[00:01:42.560 --> 00:01:47.400]   So ask questions via the chat, either on YouTube or on Zoom,
[00:01:47.400 --> 00:01:50.480]   and we'll get any technical issues squared away,
[00:01:50.480 --> 00:01:52.520]   or ask questions.
[00:01:52.520 --> 00:01:53.560]   All right.
[00:01:53.560 --> 00:01:56.360]   Greg, go ahead and take it away.
[00:01:56.360 --> 00:01:58.200]   Yeah, thanks for the introduction, Charles.
[00:01:58.200 --> 00:02:02.960]   So also let me know if there's any technical difficulties.
[00:02:02.960 --> 00:02:05.440]   My internet today has been kind of unstable.
[00:02:05.440 --> 00:02:09.400]   So-- or I guess I might not be able to hear you
[00:02:09.400 --> 00:02:10.800]   if there's technical difficulties.
[00:02:10.800 --> 00:02:14.240]   But anyway, so we'll just keep going forward.
[00:02:14.240 --> 00:02:19.000]   Yeah, so I'm really honored to be featured here today,
[00:02:19.000 --> 00:02:20.560]   hosted by Charles.
[00:02:20.560 --> 00:02:23.080]   And yeah, like he introduced--
[00:02:23.080 --> 00:02:25.080]   I'm going to talk about feature learning infinite
[00:02:25.080 --> 00:02:25.760]   neural networks.
[00:02:25.760 --> 00:02:30.480]   And I'm going to just dive straight into it.
[00:02:30.480 --> 00:02:33.400]   So the plan for today are a couple of folders.
[00:02:33.400 --> 00:02:35.560]   So we're going to first talk about why
[00:02:35.560 --> 00:02:38.680]   pre-training transfer learning requires feature learning.
[00:02:38.680 --> 00:02:42.680]   Then we briefly review some current theory revolving
[00:02:42.680 --> 00:02:44.280]   around your tangent kernel.
[00:02:44.280 --> 00:02:47.360]   And then we'll talk about our proposal,
[00:02:47.360 --> 00:02:49.840]   which is the feature learning limit of neural networks.
[00:02:52.760 --> 00:02:56.080]   So pre-training and transfer learning
[00:02:56.080 --> 00:02:58.960]   is very successful in deep learning.
[00:02:58.960 --> 00:03:01.680]   And there's no more prominent examples
[00:03:01.680 --> 00:03:08.160]   than ImageNet, ResNet on the image domain, and BERT-GBT3,
[00:03:08.160 --> 00:03:11.480]   and so on, on the NLP domain.
[00:03:11.480 --> 00:03:17.080]   And in both cases, pre-training and transfer learning
[00:03:17.080 --> 00:03:21.000]   cannot happen without feature learning.
[00:03:21.000 --> 00:03:24.320]   And before I explain why I say this,
[00:03:24.320 --> 00:03:27.040]   which might come as really obvious to some of you,
[00:03:27.040 --> 00:03:31.080]   but might need some more justification for others.
[00:03:31.080 --> 00:03:33.440]   So before I justify, let me just be
[00:03:33.440 --> 00:03:35.400]   very straight about what I mean by feature
[00:03:35.400 --> 00:03:38.040]   learning in this talk.
[00:03:38.040 --> 00:03:41.840]   So any neural network, you can express
[00:03:41.840 --> 00:03:43.280]   the structure of a neural network
[00:03:43.280 --> 00:03:45.840]   as simply a composition of two functions.
[00:03:45.840 --> 00:03:48.080]   The first function is a nonlinear embedding function
[00:03:48.080 --> 00:03:53.560]   from input space to the hidden layer of the neural network.
[00:03:53.560 --> 00:03:55.960]   And then there is another linear map
[00:03:55.960 --> 00:04:00.600]   from the hidden layer, which you can think of as embedding space,
[00:04:00.600 --> 00:04:06.720]   to the output space, which usually is the label space.
[00:04:06.720 --> 00:04:10.840]   So you output logits over labels.
[00:04:10.840 --> 00:04:15.520]   And then via softmax, you can output probabilities.
[00:04:15.520 --> 00:04:18.480]   So by feature learning, I mean the embedding function is
[00:04:18.480 --> 00:04:19.000]   learned.
[00:04:19.000 --> 00:04:28.720]   And just to briefly review how the pre-training and fine
[00:04:28.720 --> 00:04:31.920]   tuning paradigm right now works.
[00:04:31.920 --> 00:04:34.040]   Right now, during pre-training, you
[00:04:34.040 --> 00:04:39.400]   train a really large model on a very large data set.
[00:04:39.400 --> 00:04:41.600]   Sorry, somebody raised their hand.
[00:04:41.600 --> 00:04:43.160]   I don't know if I should--
[00:04:43.240 --> 00:04:48.040]   so is the paradigm that I should respond immediately or--
[00:04:48.040 --> 00:04:50.760]   So if you have a question, rather than raising your hand,
[00:04:50.760 --> 00:04:53.360]   just put it in the Q&A or the chat.
[00:04:53.360 --> 00:04:56.680]   So folks in the audience, go ahead and do that.
[00:04:56.680 --> 00:04:57.400]   All right, cool.
[00:04:57.400 --> 00:04:58.680]   So I'll just keep going, right?
[00:04:58.680 --> 00:05:10.040]   So during pre-training, so for example, ImageNet and GPT-3,
[00:05:10.040 --> 00:05:13.040]   during pre-training, you train on a very large data
[00:05:13.040 --> 00:05:15.920]   set from a general domain, usually
[00:05:15.920 --> 00:05:19.040]   without a lot of labels.
[00:05:19.040 --> 00:05:22.680]   For example, in the GPT-3 case, of ImageNet,
[00:05:22.680 --> 00:05:24.440]   you do have a lot of labels.
[00:05:24.440 --> 00:05:26.120]   But you train on a very large data set.
[00:05:26.120 --> 00:05:28.160]   And then during the transfer stage,
[00:05:28.160 --> 00:05:33.680]   you care about this actually very specific subdomain
[00:05:33.680 --> 00:05:35.640]   where you want to apply your model to.
[00:05:35.680 --> 00:05:41.200]   And you collect very expensive but well-labeled data
[00:05:41.200 --> 00:05:42.480]   in that domain.
[00:05:42.480 --> 00:05:46.080]   And then you apply your pre-trained model.
[00:05:46.080 --> 00:05:50.000]   And then you train it a little bit on the small data set.
[00:05:50.000 --> 00:05:53.000]   And usually what you find is that the performance
[00:05:53.000 --> 00:05:56.880]   of such a transfer network is much, much better
[00:05:56.880 --> 00:05:59.560]   than if you train from scratch on this small data
[00:05:59.560 --> 00:06:03.200]   set from the get-go.
[00:06:03.200 --> 00:06:06.800]   And the way we usually do this is first,
[00:06:06.800 --> 00:06:09.320]   we discard the readout layer from the pre-training.
[00:06:09.320 --> 00:06:13.080]   So just to be clear, readout means this linear layer
[00:06:13.080 --> 00:06:15.440]   that goes from the last layer of the neural network
[00:06:15.440 --> 00:06:17.360]   to the output space.
[00:06:17.360 --> 00:06:18.480]   So you discard this layer.
[00:06:18.480 --> 00:06:21.880]   And this is just because during pre-training,
[00:06:21.880 --> 00:06:24.760]   you usually have a different objective and different output
[00:06:24.760 --> 00:06:28.040]   space than the downstream tasks you're actually interested in.
[00:06:28.040 --> 00:06:30.720]   So just for type checking reasons,
[00:06:30.720 --> 00:06:33.480]   you have to discard it anyway.
[00:06:33.480 --> 00:06:38.480]   And there are two very popular ways of doing this fine tuning.
[00:06:38.480 --> 00:06:40.080]   So in the linear fine tuning case,
[00:06:40.080 --> 00:06:43.800]   you just train a new readout layer of the right type.
[00:06:43.800 --> 00:06:46.280]   Like if you want to classify cats and dogs,
[00:06:46.280 --> 00:06:50.320]   you would train a new layer with output dimension 2.
[00:06:50.320 --> 00:06:58.000]   And so it's very easy to see that if pre-training improves
[00:06:58.000 --> 00:07:00.880]   linear fine tuning, then the embeddings, i.e.
[00:07:00.880 --> 00:07:05.440]   the features of the inputs, must change during pre-training.
[00:07:05.440 --> 00:07:09.440]   I mean, the fine tuning stage is entirely a linear problem.
[00:07:09.440 --> 00:07:11.160]   It only depends on--
[00:07:11.160 --> 00:07:14.120]   the performance only depends on the embeddings,
[00:07:14.120 --> 00:07:17.560]   the quality of the embeddings.
[00:07:17.560 --> 00:07:20.720]   So in this particular case of linear fine tuning,
[00:07:20.720 --> 00:07:24.560]   this justifies why I say that transfer learning really
[00:07:24.560 --> 00:07:27.240]   requires feature learning.
[00:07:27.240 --> 00:07:30.200]   There's another popular way of doing fine tuning, which
[00:07:30.200 --> 00:07:32.240]   is called total fine tuning, where
[00:07:32.240 --> 00:07:34.200]   you train the entire neural network, not just
[00:07:34.200 --> 00:07:35.440]   the last layer.
[00:07:35.440 --> 00:07:37.680]   You can actually get the same conclusion,
[00:07:37.680 --> 00:07:40.400]   though the logic is a bit more involved.
[00:07:40.400 --> 00:07:41.800]   So I won't go into detail here.
[00:07:41.800 --> 00:07:44.640]   But the point is that in the end,
[00:07:44.640 --> 00:07:47.200]   my point states that transfer learning really
[00:07:47.200 --> 00:07:51.240]   requires you to do feature learning.
[00:07:51.240 --> 00:07:55.800]   All right, so now let's look at how current theory deals
[00:07:55.800 --> 00:08:01.160]   with transfer learning or feature learning.
[00:08:01.160 --> 00:08:05.960]   So one of the most popular theories
[00:08:05.960 --> 00:08:09.760]   nowadays is this thing called neural tangent kernel.
[00:08:09.760 --> 00:08:11.840]   The idea is actually very simple.
[00:08:11.840 --> 00:08:16.760]   So we take the neural network function f of x
[00:08:16.760 --> 00:08:18.080]   with parameters theta.
[00:08:18.080 --> 00:08:22.480]   And we just do a naive first order theta expansion
[00:08:22.480 --> 00:08:26.480]   around the initialized value.
[00:08:26.480 --> 00:08:28.880]   I don't know if you can see my mouse cursor or not.
[00:08:28.880 --> 00:08:36.240]   Yes?
[00:08:36.240 --> 00:08:38.480]   Sorry, what's your-- you can see it?
[00:08:38.480 --> 00:08:38.980]   OK.
[00:08:38.980 --> 00:08:45.760]   And so you do a first order theta expansion.
[00:08:45.760 --> 00:08:48.560]   And you can write this difference
[00:08:48.560 --> 00:08:51.600]   of essentially the change in the neural network
[00:08:51.600 --> 00:08:55.160]   due to change in the parameters as this inner product
[00:08:55.160 --> 00:08:59.400]   between the gradient of f with respect to the parameters
[00:08:59.400 --> 00:09:02.840]   and the change in the parameters.
[00:09:02.840 --> 00:09:05.720]   And you can rewrite this as a kernel equation
[00:09:05.720 --> 00:09:08.240]   where the change of f in the function space
[00:09:08.240 --> 00:09:12.160]   is equal to essentially a kernel times a loss derivative.
[00:09:12.160 --> 00:09:14.200]   And the kernel is this kernel induced
[00:09:14.200 --> 00:09:19.080]   by the gradient of the function evaluated
[00:09:19.080 --> 00:09:21.240]   at the initial parameters.
[00:09:21.240 --> 00:09:26.800]   So the really nice thing about this, of course,
[00:09:26.800 --> 00:09:29.040]   is that you essentially have linearized
[00:09:29.040 --> 00:09:31.760]   a really complicated dynamics of training
[00:09:31.760 --> 00:09:33.720]   of nonlinear neural network.
[00:09:33.720 --> 00:09:39.880]   And as a result, essentially in this particular framework,
[00:09:39.880 --> 00:09:42.200]   if you look in the function space,
[00:09:42.200 --> 00:09:44.160]   then the optimization landscape actually
[00:09:44.160 --> 00:09:47.160]   becomes convex, kind of like this figure,
[00:09:47.160 --> 00:09:50.360]   this animation illustrates.
[00:09:50.360 --> 00:09:54.920]   And theoretically, this is easy to analyze.
[00:09:54.920 --> 00:09:57.640]   And we can obtain a lot of optimization generalization
[00:09:57.640 --> 00:10:01.120]   results, really, I think, for the first time
[00:10:01.120 --> 00:10:05.840]   for really complicated neural networks that was not really
[00:10:05.840 --> 00:10:07.720]   in reach before.
[00:10:07.720 --> 00:10:09.920]   So from a theoretical angle, this
[00:10:09.920 --> 00:10:13.280]   is actually a very significant framework
[00:10:13.280 --> 00:10:18.480]   to think about neural networks, large neural networks.
[00:10:18.480 --> 00:10:22.680]   But the very crucial drawback of this framework
[00:10:22.680 --> 00:10:27.720]   is that this particular NTK limit does not learn features.
[00:10:27.720 --> 00:10:31.640]   So pre-training is no better than randomization
[00:10:31.640 --> 00:10:35.000]   in the NTK limit.
[00:10:35.000 --> 00:10:39.520]   So this statement, again, could be really obvious to some
[00:10:39.520 --> 00:10:42.720]   of you, but some other of you might
[00:10:42.720 --> 00:10:43.880]   require some justification.
[00:10:43.880 --> 00:10:48.040]   So let me do that very quickly.
[00:10:48.040 --> 00:10:53.080]   So again, we have the first-order tail expansion
[00:10:53.080 --> 00:10:54.840]   framework for NTK.
[00:10:54.840 --> 00:11:01.800]   And of course, first-order tail expansion really only
[00:11:01.800 --> 00:11:06.040]   works if the change in the parameters
[00:11:06.040 --> 00:11:08.560]   here is not very large.
[00:11:08.560 --> 00:11:12.320]   So if NTK describes actual neural networks,
[00:11:12.320 --> 00:11:16.240]   then it must be the case that theta minus theta 0 is small.
[00:11:16.240 --> 00:11:19.440]   But then this implies that the embedding function cannot
[00:11:19.440 --> 00:11:24.200]   change too much, which implies that essentially the features
[00:11:24.200 --> 00:11:26.520]   don't change.
[00:11:26.520 --> 00:11:30.520]   So this is just a very, very simple intuition
[00:11:30.520 --> 00:11:35.240]   about why in the NTK limit, you do not have feature learning.
[00:11:35.240 --> 00:11:40.760]   Of course, the real math is a bit more complicated than that.
[00:11:40.760 --> 00:11:46.720]   But here's a very vivid example to show that the NTK limit
[00:11:46.720 --> 00:11:49.440]   doesn't learn features.
[00:11:49.440 --> 00:11:57.360]   So in this example, I trained Word2Vec embeddings
[00:11:57.360 --> 00:11:58.920]   of a bunch of different words.
[00:11:58.920 --> 00:12:04.760]   But here is words corresponding to US cities and states.
[00:12:04.760 --> 00:12:08.880]   And I did a PCA of the learned embeddings
[00:12:08.880 --> 00:12:12.280]   into a two-dimensional space.
[00:12:12.280 --> 00:12:16.880]   And the models I trained here are first with 64.
[00:12:16.880 --> 00:12:19.560]   So a 64-dimensional embedding Word2Vec,
[00:12:19.560 --> 00:12:22.000]   just like the conventional Word2Vec.
[00:12:22.000 --> 00:12:26.000]   On the left, I computed the same thing for the NTK limit.
[00:12:26.000 --> 00:12:31.440]   So essentially, the theoretical limit
[00:12:31.440 --> 00:12:35.680]   of the entire Word2Vec training procedure in the NTK limit.
[00:12:35.680 --> 00:12:38.440]   And then on the right, I did the same thing.
[00:12:38.440 --> 00:12:41.000]   Essentially, I took the feature learning limit,
[00:12:41.000 --> 00:12:44.560]   which I'll describe later what exactly is meant by this.
[00:12:44.560 --> 00:12:49.000]   But I took the feature learning limit of the Word2Vec training
[00:12:49.000 --> 00:12:51.120]   procedure, the neural network.
[00:12:51.120 --> 00:12:55.960]   And the resulting infinite width embeddings
[00:12:55.960 --> 00:12:59.680]   are projected into two dimensions via PCA.
[00:12:59.680 --> 00:13:05.080]   And the thing you should take away from these plots
[00:13:05.080 --> 00:13:10.680]   is that whereas in the NTK case, the two types of words,
[00:13:10.680 --> 00:13:16.120]   US cities and US states, are essentially all mixed together
[00:13:16.120 --> 00:13:19.280]   in the 2D space.
[00:13:19.280 --> 00:13:21.800]   In the finite width case with 64,
[00:13:21.800 --> 00:13:24.000]   you can see some natural separation
[00:13:24.000 --> 00:13:25.880]   between the two types of words, indicating
[00:13:25.880 --> 00:13:30.960]   that the geometry of the word embedding
[00:13:30.960 --> 00:13:34.680]   really encoded some semantic information of the words.
[00:13:34.680 --> 00:13:38.240]   And furthermore, in the infinite width case,
[00:13:38.240 --> 00:13:42.200]   you have an even more clean natural separation
[00:13:42.200 --> 00:13:45.600]   between the two clusters, the two types of words,
[00:13:45.600 --> 00:13:47.800]   compared to the finite width case.
[00:13:47.800 --> 00:13:52.560]   So this indicates that, one, NTK
[00:13:52.560 --> 00:13:57.320]   is not the right limit to take to understand feature
[00:13:57.320 --> 00:14:00.800]   learning, which was made very obvious by the contrast
[00:14:00.800 --> 00:14:03.200]   between the first plot and the second two plots.
[00:14:03.200 --> 00:14:05.560]   And two, the feature learning limit
[00:14:05.560 --> 00:14:09.440]   that I'll describe very quickly, very soon,
[00:14:09.440 --> 00:14:14.240]   will be a very natural notion of feature learning
[00:14:14.240 --> 00:14:17.320]   as you have this kind of natural increase
[00:14:17.320 --> 00:14:20.440]   in the semantical content of the learned word embeddings.
[00:14:21.160 --> 00:14:23.160]   A quick question, Greg, from the audience.
[00:14:23.160 --> 00:14:24.160]   Yeah.
[00:14:24.160 --> 00:14:30.000]   So is the neural tangent kernel the only kernel
[00:14:30.000 --> 00:14:31.520]   that you can use there?
[00:14:31.520 --> 00:14:34.560]   Are there other potential choices there?
[00:14:34.560 --> 00:14:36.080]   Yeah, good question.
[00:14:36.080 --> 00:14:39.880]   So essentially, it doesn't matter which kernel it is.
[00:14:39.880 --> 00:14:43.280]   As long as it's in the kernel style limit,
[00:14:43.280 --> 00:14:44.760]   you're going to get the same thing.
[00:14:44.760 --> 00:14:49.720]   So it doesn't depend on what the kernel is.
[00:14:49.720 --> 00:14:52.280]   What the kernel itself is, it's just
[00:14:52.280 --> 00:14:57.360]   that as soon as you have a kernel, it doesn't work.
[00:14:57.360 --> 00:15:02.520]   So I'll talk about, actually, very quickly, a bit later,
[00:15:02.520 --> 00:15:04.160]   essentially, you have a dichotomy
[00:15:04.160 --> 00:15:10.760]   between kernel limits, just like the NTK.
[00:15:10.760 --> 00:15:13.160]   And you have the other categories
[00:15:13.160 --> 00:15:14.280]   of feature learning limit.
[00:15:14.280 --> 00:15:19.800]   And essentially, any natural, in some sense,
[00:15:19.800 --> 00:15:24.720]   natural limit of a neural network is either one of them.
[00:15:24.720 --> 00:15:27.160]   But we cannot have both.
[00:15:27.160 --> 00:15:28.760]   So if you want to do feature learning,
[00:15:28.760 --> 00:15:31.440]   then it's definitely not a kernel limit.
[00:15:31.440 --> 00:15:37.880]   So let me move on.
[00:15:44.240 --> 00:15:47.800]   So let me just give you one more quick plot
[00:15:47.800 --> 00:15:53.200]   on some numbers, concrete numbers, on Word2Vec.
[00:15:53.200 --> 00:15:56.320]   So in Word2Vec, you learn these embeddings.
[00:15:56.320 --> 00:16:01.080]   But of course, you want to use them, actually, downstream.
[00:16:01.080 --> 00:16:04.680]   And you can use them in very many different ways.
[00:16:04.680 --> 00:16:08.160]   But academically, a very common evaluation task
[00:16:08.160 --> 00:16:12.680]   is OR analogy, which asks this famous question, what
[00:16:12.680 --> 00:16:15.480]   to a queen is a man to a woman?
[00:16:15.480 --> 00:16:17.840]   It sounds really philosophical.
[00:16:17.840 --> 00:16:24.520]   But it's a collection of these analogy questions
[00:16:24.520 --> 00:16:30.520]   that you use the word embeddings learned to answer via
[00:16:30.520 --> 00:16:33.440]   some algebraic manipulations.
[00:16:33.440 --> 00:16:36.440]   But I'm not going to talk about the details of how
[00:16:36.440 --> 00:16:38.680]   the actual evaluation is done.
[00:16:38.680 --> 00:16:41.560]   But let me just highlight some numbers.
[00:16:41.560 --> 00:16:47.440]   In the plot here, we evaluated a bunch of different Word2Vec
[00:16:47.440 --> 00:16:49.600]   embeddings.
[00:16:49.600 --> 00:16:52.960]   Well, for the final ones, we have width
[00:16:52.960 --> 00:16:56.080]   or dimensionally embedding from 2 to the 6th power
[00:16:56.080 --> 00:16:58.160]   to 2 to the 10th power.
[00:16:58.160 --> 00:17:01.280]   And then we have the infinite width feature learning limit
[00:17:01.280 --> 00:17:04.000]   that I'll explain very soon.
[00:17:04.000 --> 00:17:07.880]   And then finally, we have the NTK limit.
[00:17:07.880 --> 00:17:12.920]   So there are a couple of different takeaways.
[00:17:12.920 --> 00:17:17.080]   First is that the NTK limit, of course,
[00:17:17.080 --> 00:17:20.520]   has essentially zero performance on word analogy.
[00:17:20.520 --> 00:17:24.520]   And like I said, that's because NTK doesn't really
[00:17:24.520 --> 00:17:26.040]   learn anything.
[00:17:26.040 --> 00:17:27.960]   The word embeddings are essentially random.
[00:17:27.960 --> 00:17:33.360]   It's just the same, essentially, as the initialization
[00:17:33.360 --> 00:17:35.800]   of the training procedure.
[00:17:35.800 --> 00:17:38.400]   So during evaluation, you're essentially
[00:17:38.400 --> 00:17:40.480]   just doing random guessing.
[00:17:40.480 --> 00:17:43.840]   And because the number of words is so large,
[00:17:43.840 --> 00:17:45.600]   when you do random guessing on this plot,
[00:17:45.600 --> 00:17:47.920]   you cannot actually see the real number.
[00:17:47.920 --> 00:17:52.160]   It's just essentially the same as 0.
[00:17:52.160 --> 00:17:54.320]   And in contrast, of course, finite width
[00:17:54.320 --> 00:17:56.520]   and infinite feature learning Word2Vec
[00:17:56.520 --> 00:18:00.040]   have non-trivial performances.
[00:18:00.040 --> 00:18:02.840]   And then the other thing you can notice
[00:18:02.840 --> 00:18:09.600]   is that on the x-axis, I have the epoch.
[00:18:09.600 --> 00:18:14.520]   So this is the training time of the models.
[00:18:14.520 --> 00:18:21.760]   And you can see that as width increases, which corresponds
[00:18:21.760 --> 00:18:24.520]   to the color becoming darker.
[00:18:24.520 --> 00:18:28.800]   At every point in the training procedure,
[00:18:28.800 --> 00:18:34.040]   the wider the model is, the better the training performance.
[00:18:34.040 --> 00:18:37.760]   And actually, sorry, the y-axis is actually
[00:18:37.760 --> 00:18:40.480]   the evaluation on downstream tasks.
[00:18:40.480 --> 00:18:43.560]   So it's actually like test performance.
[00:18:43.560 --> 00:18:46.760]   But what I'm saying here is that at any given time
[00:18:46.760 --> 00:18:49.240]   during the training, if you take that partial model,
[00:18:49.240 --> 00:18:51.640]   partially trained model, and evaluate it
[00:18:51.640 --> 00:18:55.040]   on the downstream task, you see a uniform increase
[00:18:55.040 --> 00:18:57.720]   in performance as the width increases.
[00:18:57.720 --> 00:19:00.200]   So this indicates that this feature learning
[00:19:00.200 --> 00:19:03.400]   limit that I'm about to introduce to you
[00:19:03.400 --> 00:19:04.440]   is the right notion.
[00:19:04.440 --> 00:19:06.720]   It captures feature learning in a meaningful way,
[00:19:06.720 --> 00:19:11.160]   such that as width increases, the performance also increases.
[00:19:11.160 --> 00:19:15.000]   And presumably, the features learned actually
[00:19:15.000 --> 00:19:19.120]   becomes more and more meaningful.
[00:19:19.120 --> 00:19:22.960]   But we also have similar results on meta-learning on MAML.
[00:19:22.960 --> 00:19:25.000]   But essentially, the takeaways are the same.
[00:19:25.000 --> 00:19:28.640]   I'm not going to actually talk about it in this talk.
[00:19:28.640 --> 00:19:29.720]   All right.
[00:19:29.720 --> 00:19:32.840]   So now we'll get to the juicy part, where we actually talk
[00:19:32.840 --> 00:19:34.440]   about the feature learning limit.
[00:19:34.440 --> 00:19:40.680]   So before I begin, let me just give you some way
[00:19:40.680 --> 00:19:44.680]   to frame this, to think about the different kinds
[00:19:44.680 --> 00:19:48.000]   of possible limits.
[00:19:48.000 --> 00:19:50.920]   And I think the right way to think about it
[00:19:50.920 --> 00:19:54.400]   is that for any finite neural network,
[00:19:54.400 --> 00:19:57.000]   when you write PyTorch code, you assume some parametrization
[00:19:57.000 --> 00:19:58.680]   of your neural network.
[00:19:58.680 --> 00:20:01.320]   I mean, just as concrete examples,
[00:20:01.320 --> 00:20:04.400]   when you write PyTorch code, you use, by default,
[00:20:04.400 --> 00:20:09.200]   like Lukun initialization, so like fan initialization.
[00:20:09.200 --> 00:20:13.880]   And every such parametrization induces some kind
[00:20:13.880 --> 00:20:17.320]   of infinite width limit, or infinite width training
[00:20:17.320 --> 00:20:18.960]   dynamics.
[00:20:18.960 --> 00:20:22.720]   If you just naturally just change your width number
[00:20:22.720 --> 00:20:26.080]   in your code from 1,000 to 10,000 to 100,000,
[00:20:26.080 --> 00:20:28.040]   and so on and so forth, assuming you
[00:20:28.040 --> 00:20:31.360]   have to compute to actually run this computation,
[00:20:31.360 --> 00:20:36.960]   then the dynamics of your neural network
[00:20:36.960 --> 00:20:39.080]   will actually converge to something
[00:20:39.080 --> 00:20:41.240]   as you increase that width number in your code.
[00:20:41.240 --> 00:20:46.120]   So my point here is that every parametrization
[00:20:46.120 --> 00:20:49.520]   corresponds to an infinite width dynamics.
[00:20:49.520 --> 00:20:54.240]   And the feature learning limit that I've been talking about
[00:20:54.240 --> 00:20:57.400]   and I'm going to be more concrete on
[00:20:57.400 --> 00:21:00.280]   is induced by this thing we call the maximal update
[00:21:00.280 --> 00:21:05.360]   parametrization, which I'll describe shortly.
[00:21:05.360 --> 00:21:08.880]   But just before that, let me quickly give you
[00:21:08.880 --> 00:21:12.720]   a big picture of what's going on.
[00:21:12.720 --> 00:21:16.840]   So in general, you have a lot of different parametrizations.
[00:21:16.840 --> 00:21:18.480]   And there's a way to formalize this
[00:21:18.480 --> 00:21:20.360]   into something called ABC parametrizations.
[00:21:20.360 --> 00:21:24.440]   But I'm not going to go into details about this in this talk.
[00:21:24.440 --> 00:21:27.760]   I'll briefly give you some pointers later.
[00:21:27.760 --> 00:21:30.920]   But you have the space of possible parametrizations,
[00:21:30.920 --> 00:21:35.560]   which includes the neural tangent parametrization
[00:21:35.560 --> 00:21:36.920]   and the standard parametrization,
[00:21:36.920 --> 00:21:39.240]   like the Lacunae initialization, or whatever
[00:21:39.240 --> 00:21:42.680]   you use in PyTorch, or mean field parametrization,
[00:21:42.680 --> 00:21:47.320]   which is more relevant in the theoretical literature.
[00:21:47.320 --> 00:21:51.880]   Most of them are going to induce unstable or trivial limits
[00:21:51.880 --> 00:21:53.560]   in the sense that training dynamics
[00:21:53.560 --> 00:21:55.840]   in the infinite width limit will blow up,
[00:21:55.840 --> 00:21:59.840]   or the infinite width neural network
[00:21:59.840 --> 00:22:05.680]   will not move during training from initialization.
[00:22:05.680 --> 00:22:10.520]   But within those that are stable and non-trivial, most of them
[00:22:10.520 --> 00:22:12.440]   are going to be in the kernel regime.
[00:22:12.440 --> 00:22:14.480]   So they're going to induce kernel limits.
[00:22:14.480 --> 00:22:17.080]   So in particular, neural tangent is a prominent example.
[00:22:17.080 --> 00:22:19.360]   But also standard parametrization
[00:22:19.360 --> 00:22:21.960]   with the appropriate learning rate.
[00:22:21.960 --> 00:22:25.600]   But if you actually use a standard parametrization
[00:22:25.600 --> 00:22:28.160]   with a constant learning rate, things
[00:22:28.160 --> 00:22:29.760]   are actually going to blow up.
[00:22:29.760 --> 00:22:32.440]   So it's going to be unstable.
[00:22:32.440 --> 00:22:35.560]   And the thing, the maximal update parametrization
[00:22:35.560 --> 00:22:40.120]   that I'm going to introduce next is essentially
[00:22:40.120 --> 00:22:45.640]   like this vertex in this very small region that actually does
[00:22:45.640 --> 00:22:46.840]   feature learning.
[00:22:46.840 --> 00:22:52.440]   And this being a vertex means that it's maximal in some sense.
[00:22:52.440 --> 00:22:55.880]   Colloquially, you can understand this as saying that if there's
[00:22:55.880 --> 00:22:59.880]   features to be learned, you learn some features.
[00:22:59.880 --> 00:23:03.560]   OK, that's all I'm going to say about this, the big picture.
[00:23:03.560 --> 00:23:07.160]   Now let me drop down into something concrete.
[00:23:07.160 --> 00:23:11.080]   Let me tell you about the maximal update parametrization,
[00:23:11.080 --> 00:23:13.040]   which is abbreviated as MUP.
[00:23:14.040 --> 00:23:15.640]   So let's be very concrete.
[00:23:15.640 --> 00:23:20.040]   I'm going to tell you what this parametrization actually is.
[00:23:20.040 --> 00:23:22.480]   And the easiest way to understand
[00:23:22.480 --> 00:23:25.320]   this is to modify the standard parametrization
[00:23:25.320 --> 00:23:28.040]   to get a maximal update parametrization.
[00:23:28.040 --> 00:23:36.160]   So you need to make two changes to the standard parametrization.
[00:23:36.160 --> 00:23:38.960]   And I'll tell you about intuitively
[00:23:38.960 --> 00:23:40.800]   why we need those changes.
[00:23:40.800 --> 00:23:45.240]   But the first change is in the last layer.
[00:23:45.240 --> 00:23:48.000]   And the reason why we need to make that change
[00:23:48.000 --> 00:23:51.000]   is because in standard parametrization,
[00:23:51.000 --> 00:23:55.640]   the last layer, essentially, one is too large anitization.
[00:23:55.640 --> 00:23:59.440]   And two, it gets too much gradient.
[00:23:59.440 --> 00:24:04.000]   And with this modification, essentially the purpose
[00:24:04.000 --> 00:24:06.800]   is to make the last layer a bit smaller.
[00:24:06.800 --> 00:24:08.440]   And the reason why we need to make
[00:24:08.440 --> 00:24:11.160]   two, we'll make the last layer a bit smaller
[00:24:11.160 --> 00:24:16.000]   and also make its gradient a bit smaller.
[00:24:16.000 --> 00:24:20.520]   And this is so that you can use a larger learning rate such
[00:24:20.520 --> 00:24:26.120]   that all the previous layers get more gradient.
[00:24:26.120 --> 00:24:30.000]   OK, so concretely, what we do is we divide the largest
[00:24:30.000 --> 00:24:32.360]   by square root of n and use a constant learning rate.
[00:24:32.360 --> 00:24:35.640]   So you can see here, there's a one over square root of n
[00:24:35.640 --> 00:24:38.480]   factor at the end of the neural network.
[00:24:38.480 --> 00:24:43.040]   And the last layer weights are sampled the usual way,
[00:24:43.040 --> 00:24:47.520]   using the Lagoon or Vannian initialization.
[00:24:47.520 --> 00:24:50.440]   And this alone suffices to enable feature learning.
[00:24:50.440 --> 00:24:53.440]   And let me remind you that in the standard parametrization,
[00:24:53.440 --> 00:24:56.360]   in the figure I just showed you, if you
[00:24:56.360 --> 00:25:00.880]   don't divide by square root of n and you use a constant learning
[00:25:00.880 --> 00:25:04.600]   rate, essentially, as you train a really, really
[00:25:04.600 --> 00:25:08.160]   wide neural network, the output of neural network
[00:25:08.160 --> 00:25:12.040]   will blow up, as well as the pre-activation
[00:25:12.040 --> 00:25:13.440]   of the neural network will blow up.
[00:25:13.440 --> 00:25:19.320]   OK, so second modification is in the first layer.
[00:25:19.320 --> 00:25:25.040]   And the intuition of why we need to make this modification
[00:25:25.040 --> 00:25:26.600]   is that in standard parametrization,
[00:25:26.600 --> 00:25:28.680]   the first layer actually gets very little gradient compared
[00:25:28.680 --> 00:25:30.120]   to the rest of the neural network.
[00:25:30.200 --> 00:25:34.520]   So taking a slightly larger picture,
[00:25:34.520 --> 00:25:36.840]   essentially, in the standard parametrization,
[00:25:36.840 --> 00:25:38.880]   the last layer gets too much gradient,
[00:25:38.880 --> 00:25:41.000]   the middle gets OK amount of gradient,
[00:25:41.000 --> 00:25:43.120]   and the first layer gets too little gradient.
[00:25:43.120 --> 00:25:45.960]   So we're fixing the problem in the first layer.
[00:25:45.960 --> 00:25:50.040]   And the easy fix here is that we multiply the pre-activation
[00:25:50.040 --> 00:25:53.760]   by square root of n, and we use Fano initialization.
[00:25:53.760 --> 00:25:56.600]   So here's a square root of n in the pre-activation
[00:25:56.600 --> 00:25:57.760]   in the first layer.
[00:25:57.760 --> 00:26:02.600]   And the weights are sampled like Fano, so 1 over n,
[00:26:02.600 --> 00:26:04.480]   where n is the output dimension.
[00:26:04.480 --> 00:26:11.440]   And so this will serve to increase the gradient by n.
[00:26:11.440 --> 00:26:15.760]   This is needed to enable feature learning in every layer.
[00:26:15.760 --> 00:26:18.600]   All other weights are initialized kind of same way
[00:26:18.600 --> 00:26:20.200]   using Fano initialization.
[00:26:20.200 --> 00:26:27.400]   OK, so that's essentially the maximal update
[00:26:27.400 --> 00:26:30.080]   parametrization.
[00:26:30.080 --> 00:26:33.400]   Now, next part of the talk, most of it
[00:26:33.400 --> 00:26:37.760]   is going to be about giving you some intuition of how
[00:26:37.760 --> 00:26:40.800]   to compute this limit.
[00:26:40.800 --> 00:26:45.600]   The gist of the story is that you can essentially
[00:26:45.600 --> 00:26:49.600]   compute the limit for any neural network
[00:26:49.600 --> 00:26:53.400]   and essentially any algorithm.
[00:26:53.400 --> 00:26:59.840]   Like essentially, if you have a PyTorch code describing
[00:26:59.840 --> 00:27:03.640]   whatever you're trying to do, anything in deep learning,
[00:27:03.640 --> 00:27:07.040]   you can convert it systematically
[00:27:07.040 --> 00:27:10.400]   to a computation for infinite width neural network
[00:27:10.400 --> 00:27:13.080]   and a way to compute that limit.
[00:27:13.080 --> 00:27:17.120]   But I'm going to focus very, very specifically
[00:27:17.120 --> 00:27:20.040]   on a one-headed layer in your neural network
[00:27:20.040 --> 00:27:22.560]   as a motivating example.
[00:27:22.560 --> 00:27:25.560]   So I'll go into that very shortly.
[00:27:25.560 --> 00:27:28.720]   Now, let me go a bit one level deeper
[00:27:28.720 --> 00:27:31.440]   into the intuition of how to compute this.
[00:27:31.440 --> 00:27:34.560]   And this is given by this quote.
[00:27:34.560 --> 00:27:37.920]   When width is very large, every activation vector
[00:27:37.920 --> 00:27:39.480]   has roughly id coordinates.
[00:27:39.480 --> 00:27:45.200]   So at any time during training and using tensor programs,
[00:27:45.200 --> 00:27:48.440]   we can recursively calculate such coordinate distributions
[00:27:48.440 --> 00:27:51.200]   and consequently understand how the neural network function
[00:27:51.200 --> 00:27:53.000]   evolves.
[00:27:53.000 --> 00:27:58.360]   OK, so if you have been doing some calculations with NTK,
[00:27:58.360 --> 00:28:01.720]   you might be familiar with the first sentence in a sense
[00:28:01.720 --> 00:28:04.280]   that at neutralization, every activation vector
[00:28:04.280 --> 00:28:06.520]   has roughly id coordinates.
[00:28:06.520 --> 00:28:09.640]   But this is at neutralization, not any time during training.
[00:28:09.640 --> 00:28:13.600]   So I need to justify why at any time during training,
[00:28:13.600 --> 00:28:17.400]   every activation vector has roughly id coordinates.
[00:28:17.440 --> 00:28:22.680]   And then I'll briefly touch on how we can recursively
[00:28:22.680 --> 00:28:26.120]   calculate such coordinate distributions.
[00:28:26.120 --> 00:28:28.600]   All right, so as a motivating example,
[00:28:28.600 --> 00:28:30.840]   let's look at a linear one-headed layer neural
[00:28:30.840 --> 00:28:32.920]   network.
[00:28:32.920 --> 00:28:38.760]   So assume input and output dimension are both 1.
[00:28:38.760 --> 00:28:44.160]   Then the neural network can be represented
[00:28:44.160 --> 00:28:47.720]   by two essentially vectors, right,
[00:28:47.720 --> 00:28:50.160]   because the input and output dimensions are 1.
[00:28:50.160 --> 00:28:52.240]   So the first layer and second layer weights
[00:28:52.240 --> 00:28:54.800]   are both vectors.
[00:28:54.800 --> 00:28:57.360]   And a forward pass is represented
[00:28:57.360 --> 00:28:59.680]   by essentially the multiplication
[00:28:59.680 --> 00:29:04.240]   of these two vectors times the input, which is a scalar.
[00:29:04.240 --> 00:29:10.400]   We're going to have id coordinates at neutralization.
[00:29:10.400 --> 00:29:13.520]   Neutralize accordingly, essentially variance 1
[00:29:13.520 --> 00:29:16.000]   over n, but we won't need that detail until later.
[00:29:16.000 --> 00:29:21.760]   OK, so let's look at what happens in the first forward
[00:29:21.760 --> 00:29:24.160]   pass.
[00:29:24.160 --> 00:29:31.800]   Again, f is the scalar product of the two vectors times input.
[00:29:31.800 --> 00:29:33.360]   And because it's a scalar product,
[00:29:33.360 --> 00:29:39.680]   the sum of a large number of roughly id elements,
[00:29:39.680 --> 00:29:45.040]   which specifically are like first layer element times
[00:29:45.040 --> 00:29:48.720]   second layer element, right, and these things
[00:29:48.720 --> 00:29:52.960]   are id across the width dimension.
[00:29:52.960 --> 00:29:55.400]   So as width becomes larger and larger,
[00:29:55.400 --> 00:29:57.800]   f of xi will converge to something,
[00:29:57.800 --> 00:30:01.240]   to some deterministic number by a lot of large numbers,
[00:30:01.240 --> 00:30:04.920]   which in this specific case of random neutralization
[00:30:04.920 --> 00:30:05.680]   is just 0.
[00:30:08.400 --> 00:30:12.720]   OK, so if f converges, then the loss derivative of f
[00:30:12.720 --> 00:30:16.920]   also converges, right, usually because loss derivative
[00:30:16.920 --> 00:30:18.880]   is a continuous function.
[00:30:18.880 --> 00:30:22.640]   So if f converges, then this continuous image of f
[00:30:22.640 --> 00:30:25.120]   has to converge.
[00:30:25.120 --> 00:30:30.240]   All right, now let's look at the first backward pass.
[00:30:30.240 --> 00:30:33.920]   In particular, let's look at the gradients of the weights.
[00:30:33.920 --> 00:30:36.200]   So you can do a very simple calculation,
[00:30:36.200 --> 00:30:40.760]   but the point here is that the gradient of the second layer
[00:30:40.760 --> 00:30:43.080]   is essentially the first layer weights
[00:30:43.080 --> 00:30:48.760]   times these two scalars, the input times the loss derivative.
[00:30:48.760 --> 00:30:51.760]   And similarly, the first layer gradients
[00:30:51.760 --> 00:30:54.400]   or the second layer weights times these two scalars.
[00:30:54.400 --> 00:31:01.080]   And like I said, the scalars are roughly deterministic.
[00:31:01.080 --> 00:31:03.840]   I mean, xi, the input, is essentially,
[00:31:03.840 --> 00:31:06.120]   by definition, is deterministic.
[00:31:06.120 --> 00:31:08.240]   And the loss derivative is deterministic
[00:31:08.240 --> 00:31:11.800]   because f converges and f mimes continuous.
[00:31:11.800 --> 00:31:16.600]   So the structure of this gradient
[00:31:16.600 --> 00:31:19.040]   actually implies that both layers' gradients
[00:31:19.040 --> 00:31:22.600]   are approximately iid.
[00:31:22.600 --> 00:31:26.200]   So when you add the gradients to the weights
[00:31:26.200 --> 00:31:30.520]   via the first gradient step, you maintain this property
[00:31:30.520 --> 00:31:35.080]   that the weights have approximately iid corners.
[00:31:35.080 --> 00:31:37.920]   And something else we can observe
[00:31:37.920 --> 00:31:42.800]   is that, of course, at this point in time,
[00:31:42.800 --> 00:31:45.480]   in the second forward pass, the weights
[00:31:45.480 --> 00:31:49.480]   are linear combinations of the original weights
[00:31:49.480 --> 00:31:52.840]   from initialization.
[00:31:52.840 --> 00:31:57.160]   And this fact will be important for calculating the infinite
[00:31:57.160 --> 00:31:58.560]   width limit later on.
[00:31:58.560 --> 00:32:02.000]   But for now, we don't actually need it.
[00:32:02.000 --> 00:32:05.840]   OK, so let's keep going forward and see
[00:32:05.840 --> 00:32:09.920]   what's happening elsewhere in the second forward pass.
[00:32:09.920 --> 00:32:15.160]   The computation f, again, implies
[00:32:15.160 --> 00:32:16.880]   that it converges by large numbers
[00:32:16.880 --> 00:32:22.520]   because it's a sum of large number of iid things.
[00:32:22.520 --> 00:32:25.240]   And the loss derivative converges as well
[00:32:25.240 --> 00:32:26.080]   because f does.
[00:32:26.480 --> 00:32:29.520]   And again, the gradients have the same structure.
[00:32:29.520 --> 00:32:34.560]   That means that they have approximately iid corners.
[00:32:34.560 --> 00:32:40.320]   And when you update, the weights remain roughly iid.
[00:32:40.320 --> 00:32:47.600]   So this logic obviously repeats for all times.
[00:32:47.600 --> 00:32:50.680]   And this justifies what I said earlier
[00:32:50.680 --> 00:32:53.120]   about how when the width is large,
[00:32:53.120 --> 00:32:55.920]   every activation vector has roughly iid corners
[00:32:55.920 --> 00:32:58.360]   at any time during training.
[00:32:58.360 --> 00:33:00.800]   And the stress is on at any time during training.
[00:33:00.800 --> 00:33:09.040]   All right, now earlier, I also said
[00:33:09.040 --> 00:33:11.960]   that weights at any time are linear combinations of weights
[00:33:11.960 --> 00:33:13.480]   from initialization.
[00:33:13.480 --> 00:33:20.680]   And this is already hinted at by the calculation we did earlier.
[00:33:20.680 --> 00:33:26.360]   And using this observation, we can
[00:33:26.360 --> 00:33:29.640]   see how to calculate the infinite width limit
[00:33:29.640 --> 00:33:33.960]   without resorting to an actual neural network.
[00:33:33.960 --> 00:33:40.000]   So let me just give you an overview of what is involved.
[00:33:40.000 --> 00:33:42.760]   So let me establish some quick notations.
[00:33:42.760 --> 00:33:45.040]   I'll let you denote the first layer of weights.
[00:33:45.040 --> 00:33:47.000]   And V denote the second layer of weights.
[00:33:47.000 --> 00:33:49.360]   And in particular, we'll use the first layer
[00:33:49.360 --> 00:33:51.960]   and in particular, without the subscripts,
[00:33:51.960 --> 00:33:56.400]   I'm going to just use them to denote the initialized values,
[00:33:56.400 --> 00:34:01.360]   so the random initialization of the weights.
[00:34:01.360 --> 00:34:06.760]   And initialization, they're sampled with variance 1 over n,
[00:34:06.760 --> 00:34:07.720]   where n is the width.
[00:34:07.720 --> 00:34:14.640]   OK, and additionally, another piece of notation
[00:34:14.640 --> 00:34:17.560]   is that during the t plus 1 forward pass,
[00:34:17.560 --> 00:34:23.240]   we use subscript t to denote the particular values of u and v
[00:34:23.240 --> 00:34:24.600]   at that time.
[00:34:24.600 --> 00:34:31.800]   So when I say weights at any time are linear combinations
[00:34:31.800 --> 00:34:33.520]   of weights from initialization, this
[00:34:33.520 --> 00:34:36.440]   implies that for any t, there are coefficients
[00:34:36.440 --> 00:34:40.240]   a t, b t, c t, d t, which converge roughly
[00:34:40.240 --> 00:34:45.480]   deterministically such that we can write v t equal to a t
[00:34:45.480 --> 00:34:49.600]   times v plus b t times u, u t equals c t times v plus d t
[00:34:49.600 --> 00:34:52.800]   times u, where again, u and v without subscripts
[00:34:52.800 --> 00:35:00.000]   are denoting the initial values of the weights.
[00:35:00.000 --> 00:35:08.520]   And of course, when t was 0 and transition, a t and d t are 1
[00:35:08.520 --> 00:35:12.160]   and b t and c t are 0.
[00:35:12.160 --> 00:35:14.920]   Sorry.
[00:35:14.920 --> 00:35:19.000]   OK, so with that said, we can look
[00:35:19.000 --> 00:35:22.800]   at what happens during the t plus 1 forward pass.
[00:35:22.800 --> 00:35:26.360]   And essentially via the same kind of low large number
[00:35:26.360 --> 00:35:28.960]   intuition, we can see that f of xi
[00:35:28.960 --> 00:35:33.120]   equals a t times c t plus b t times d t times xi.
[00:35:33.120 --> 00:35:35.440]   And you can get this by just kind
[00:35:35.440 --> 00:35:40.720]   of expanding the product of v t times u t
[00:35:40.720 --> 00:35:43.680]   and noticing that because u and v, again,
[00:35:43.680 --> 00:35:47.920]   they refer to the initialized values, u and v are independent.
[00:35:47.920 --> 00:35:50.960]   So there's no correlation between the u, v and v, u
[00:35:50.960 --> 00:35:52.400]   cross terms.
[00:35:52.400 --> 00:35:54.400]   So the only terms that survive are
[00:35:54.400 --> 00:35:58.040]   the contracting v with v and u with u.
[00:35:58.040 --> 00:36:01.080]   And that results in the terms a, c and b, d.
[00:36:01.080 --> 00:36:10.160]   And then we can investigate the t plus 1 backward pass.
[00:36:10.160 --> 00:36:15.120]   And along the same lines of what we did before,
[00:36:15.120 --> 00:36:18.960]   we can notice that the gradients are also
[00:36:18.960 --> 00:36:23.880]   linear combinations of the initial u and v's.
[00:36:23.880 --> 00:36:27.080]   In particular, they take these particular forms.
[00:36:27.080 --> 00:36:31.920]   And when you make the updates, now
[00:36:31.920 --> 00:36:37.480]   it's very apparent how the a's and the b's, the c's and d's
[00:36:37.480 --> 00:36:42.040]   will update after this particular time, like so.
[00:36:42.040 --> 00:36:51.600]   OK, so I hope this convinces you that you can repeat
[00:36:51.600 --> 00:36:54.280]   this logic over and over.
[00:36:54.280 --> 00:36:58.640]   And the point here is that to compute this infinite width
[00:36:58.640 --> 00:37:04.040]   limit, you only need to compute the values of a and b and c
[00:37:04.040 --> 00:37:06.560]   and d across time.
[00:37:06.560 --> 00:37:08.400]   And if you want to train for 1,000 steps,
[00:37:08.400 --> 00:37:11.760]   you just have to calculate what the values of a, b, c, d
[00:37:11.760 --> 00:37:13.960]   at time 1,000 is.
[00:37:13.960 --> 00:37:17.080]   And you can do this via these recursive formulas.
[00:37:17.080 --> 00:37:20.520]   And they're summarized here.
[00:37:20.520 --> 00:37:23.760]   So again, we're in the context of this linear one-headed layer
[00:37:23.760 --> 00:37:26.080]   neural network for illustration purposes.
[00:37:26.080 --> 00:37:31.280]   And here we have the input and output dimension equal to 1
[00:37:31.280 --> 00:37:33.320]   and learning rate equal to 1.
[00:37:33.320 --> 00:37:36.760]   And then in the infinite width limit,
[00:37:36.760 --> 00:37:42.040]   we have that at any time, the function, the neural network f
[00:37:42.040 --> 00:37:49.240]   is equal to ac plus bd times the input xi, where a, b, c, d
[00:37:49.240 --> 00:37:53.960]   are updated like so, with the initial condition
[00:37:53.960 --> 00:37:58.680]   that a and d are 1 and b and c are 0.
[00:37:58.680 --> 00:38:02.520]   All right, so if you've been paying a bit of attention,
[00:38:02.520 --> 00:38:06.280]   then you might notice that this looks awfully
[00:38:06.280 --> 00:38:13.320]   like a linear one-headed layer with two neural network, where
[00:38:13.320 --> 00:38:17.680]   the hidden dimension corresponds to a, b, and c, d.
[00:38:17.680 --> 00:38:22.240]   So in particular, a, b are the second layer weights.
[00:38:22.240 --> 00:38:24.080]   And c, d are the first layer weights.
[00:38:24.080 --> 00:38:30.400]   But in contrast to how people usually
[00:38:30.400 --> 00:38:32.160]   train their neural networks, what's
[00:38:32.160 --> 00:38:33.960]   different is the initialization.
[00:38:33.960 --> 00:38:35.640]   So here we have some kind of quote,
[00:38:35.640 --> 00:38:38.040]   unquote "diagonal initialization,"
[00:38:38.040 --> 00:38:42.120]   which can be summarized by this matrix equation in contrast
[00:38:42.120 --> 00:38:46.960]   to random initialization that people usually have.
[00:38:46.960 --> 00:38:50.760]   So we can summarize this whole box
[00:38:50.760 --> 00:38:57.800]   as this kind of psychological equality.
[00:38:57.800 --> 00:39:00.280]   So in other words, feature learning limit
[00:39:00.280 --> 00:39:02.360]   of a linear one-headed layer neural network
[00:39:02.360 --> 00:39:07.000]   with random initialization is equivalent to a width d in
[00:39:07.000 --> 00:39:10.240]   plus d out linear one-headed layer neural network
[00:39:10.240 --> 00:39:11.920]   with diagonal initialization.
[00:39:11.920 --> 00:39:16.560]   So we instantiated this with d in, d out equal to 1.
[00:39:16.560 --> 00:39:21.040]   But this is, in general, true for any input and output
[00:39:21.040 --> 00:39:21.560]   dimension.
[00:39:21.560 --> 00:39:25.120]   We can generalize this logic very easily.
[00:39:25.120 --> 00:39:29.080]   And in fact, this equality is essentially
[00:39:29.080 --> 00:39:32.360]   what we use to compute the large-scale experiments
[00:39:32.360 --> 00:39:33.920]   like Word2Vec.
[00:39:33.920 --> 00:39:37.680]   And just to be very concrete, in that particular example
[00:39:37.680 --> 00:39:41.480]   of Word2Vec, d in, d out are the vocab sizes,
[00:39:41.480 --> 00:39:46.240]   which are like 140k to 280k depends on the data set.
[00:39:46.240 --> 00:39:52.840]   So let me just summarize what we've been through just now.
[00:39:53.280 --> 00:39:56.080]   In the one-headed layer case, the weight matrices
[00:39:56.080 --> 00:39:58.520]   have iid coordinates initialization.
[00:39:58.520 --> 00:40:02.560]   The function output converges due to low or large numbers.
[00:40:02.560 --> 00:40:05.600]   Gradients have approximately iid coordinates.
[00:40:05.600 --> 00:40:07.600]   So after gradient update, weight coordinates
[00:40:07.600 --> 00:40:09.800]   are still approximately iid.
[00:40:09.800 --> 00:40:11.640]   And then you can repeat this logic.
[00:40:11.640 --> 00:40:17.800]   So this should convince you that the weights at any given time
[00:40:17.800 --> 00:40:19.800]   have approximately iid coordinates.
[00:40:19.800 --> 00:40:24.040]   But first, they're going to be correlated across weights.
[00:40:24.040 --> 00:40:28.560]   And then in the linear case, we can
[00:40:28.560 --> 00:40:30.760]   express the weights at any given time
[00:40:30.760 --> 00:40:34.320]   as linear combinations of weights from initialization.
[00:40:34.320 --> 00:40:37.760]   This allows us to have efficient calculation of the limit.
[00:40:37.760 --> 00:40:41.560]   All right.
[00:40:41.560 --> 00:40:43.920]   So that was one-headed layer.
[00:40:43.920 --> 00:40:47.520]   Let me give you an appetizer for the deeper case,
[00:40:47.520 --> 00:40:51.320]   where the math will actually significantly
[00:40:51.320 --> 00:40:53.000]   become more complex.
[00:40:53.000 --> 00:40:56.840]   So I don't have any room to give you the full course,
[00:40:56.840 --> 00:41:00.320]   but an appetizer should suffice.
[00:41:00.320 --> 00:41:02.400]   So the main difficulty here is that you
[00:41:02.400 --> 00:41:04.440]   have this n-time Gaussian random matrix
[00:41:04.440 --> 00:41:07.360]   W in the middle network, which comes from the random
[00:41:07.360 --> 00:41:10.800]   initialization of weights.
[00:41:10.800 --> 00:41:13.080]   So it has two kinds of behaviors that you
[00:41:13.080 --> 00:41:14.320]   have to keep track of.
[00:41:14.320 --> 00:41:16.680]   One is essentially the same as the one-headed layer.
[00:41:16.680 --> 00:41:18.920]   One is the central limit behavior,
[00:41:18.920 --> 00:41:20.760]   which is essentially something you're
[00:41:20.760 --> 00:41:24.120]   familiar with if you have done NTK calculation before.
[00:41:24.120 --> 00:41:27.720]   But essentially here, if x is a vector independent of W,
[00:41:27.720 --> 00:41:32.240]   then Wx is a Gaussian vector.
[00:41:32.240 --> 00:41:34.360]   And the other thing is that you have
[00:41:34.360 --> 00:41:40.560]   to be careful about how W correlate with W transpose.
[00:41:40.560 --> 00:41:43.040]   During SED, if you use W in the forward pass,
[00:41:43.040 --> 00:41:46.760]   then you have to use W transpose in the backward pass.
[00:41:46.760 --> 00:41:50.000]   So this correlation actually doesn't
[00:41:50.000 --> 00:41:53.440]   matter so much for the calculation of NTK,
[00:41:53.440 --> 00:41:55.640]   because that only depends essentially
[00:41:55.640 --> 00:41:58.200]   on the first backward pass.
[00:41:58.200 --> 00:42:01.560]   But if you do more than one set of SED,
[00:42:01.560 --> 00:42:04.080]   you'll see this occurring.
[00:42:04.080 --> 00:42:10.040]   So it's important to be careful about both of these behaviors
[00:42:10.040 --> 00:42:14.240]   and keep track of them in a systematic way.
[00:42:14.240 --> 00:42:16.880]   And that's by no means a trivial thing.
[00:42:16.880 --> 00:42:20.680]   And this series is called TensorFlow M series.
[00:42:20.680 --> 00:42:23.240]   And there were three papers--
[00:42:23.240 --> 00:42:25.400]   four if you count the zero paper, I guess--
[00:42:25.400 --> 00:42:26.000]   before this.
[00:42:26.000 --> 00:42:29.760]   And the crux of the mathematical foundation
[00:42:29.760 --> 00:42:32.040]   is to deal with this complexity here.
[00:42:32.040 --> 00:42:39.160]   Yeah, and the framework, this TensorFlow M machinery
[00:42:39.160 --> 00:42:41.120]   essentially automates all of these derivations.
[00:42:41.120 --> 00:42:43.800]   So essentially, even if you don't know anything
[00:42:43.800 --> 00:42:46.800]   about probability theory, you can just
[00:42:46.800 --> 00:42:51.080]   read the machinery as an algorithm.
[00:42:51.080 --> 00:42:55.920]   And as an engineer, you can just implement this
[00:42:55.920 --> 00:42:59.600]   and can automatically translate between PyTorch code
[00:42:59.600 --> 00:43:01.920]   and infinite width neural networks.
[00:43:07.080 --> 00:43:08.080]   I'm almost done.
[00:43:08.080 --> 00:43:10.800]   But let me just highlight some other results in the papers
[00:43:10.800 --> 00:43:15.960]   to give you some pointers if you're interested.
[00:43:15.960 --> 00:43:18.200]   We isolate a natural class of parametrizations,
[00:43:18.200 --> 00:43:22.520]   which I briefly mentioned before as the ABC parametrizations,
[00:43:22.520 --> 00:43:26.760]   which contains the NTK mean field standard parametrizations
[00:43:26.760 --> 00:43:29.720]   as well as the maximal update parametrization I introduced
[00:43:29.720 --> 00:43:30.200]   here.
[00:43:30.200 --> 00:43:36.720]   We classify these parametrizations essentially
[00:43:36.720 --> 00:43:40.560]   into either feature learning parametrization, which
[00:43:40.560 --> 00:43:42.280]   induces a feature learning limit,
[00:43:42.280 --> 00:43:46.800]   or kernel parametrization like NTK.
[00:43:46.800 --> 00:43:48.520]   But you cannot have both.
[00:43:48.520 --> 00:43:49.680]   So it's a dichotomy.
[00:43:49.680 --> 00:43:56.720]   And this gives you some interesting consequences.
[00:43:56.720 --> 00:43:58.720]   One is that certain functional dynamics are not
[00:43:58.720 --> 00:44:00.760]   valid infinite width limits.
[00:44:00.760 --> 00:44:04.200]   So in other words, it's not like if you just
[00:44:04.200 --> 00:44:07.600]   grab something from the bag, some kind of dynamics,
[00:44:07.600 --> 00:44:10.040]   some functional dynamics, then there's
[00:44:10.040 --> 00:44:12.840]   a neural network that corresponds to that
[00:44:12.840 --> 00:44:14.600]   in the infinite width limit.
[00:44:14.600 --> 00:44:16.040]   Now, that's not the case.
[00:44:16.040 --> 00:44:18.720]   And I give you some concrete examples.
[00:44:18.720 --> 00:44:20.840]   For example, higher order generalizations
[00:44:20.840 --> 00:44:23.120]   of NTK dynamics.
[00:44:23.120 --> 00:44:25.160]   So I think this example is actually
[00:44:25.160 --> 00:44:29.280]   very pertinent because a lot of people after the NTK paper
[00:44:29.280 --> 00:44:31.400]   essentially just try to expand more terms.
[00:44:31.400 --> 00:44:33.080]   Instead of first order Taylor expansion,
[00:44:33.080 --> 00:44:34.960]   you do higher order Taylor expansion.
[00:44:34.960 --> 00:44:41.040]   And they're saying that essentially doing that
[00:44:41.040 --> 00:44:42.600]   doesn't give you valid limits.
[00:44:42.600 --> 00:44:47.920]   And another interesting consequence
[00:44:47.920 --> 00:44:50.560]   is that any feature learning limits function values
[00:44:50.560 --> 00:44:53.680]   must be 0 everywhere initialization.
[00:44:53.680 --> 00:44:57.320]   So this contrasts with, for example, in the NTK limit,
[00:44:57.320 --> 00:45:03.320]   the initialization is a GP, is a Gaussian process
[00:45:03.320 --> 00:45:05.040]   in the infinite width limit.
[00:45:05.040 --> 00:45:09.120]   And what this is saying is that if you have a Gaussian process
[00:45:09.120 --> 00:45:12.040]   and initialization, that means your last layer
[00:45:12.040 --> 00:45:14.240]   initialization is too large.
[00:45:14.240 --> 00:45:17.360]   And this prevents you from using a large learning rate
[00:45:17.360 --> 00:45:20.240]   to make the body network learn features.
[00:45:20.240 --> 00:45:23.840]   So you should initialize so that last layer is smaller.
[00:45:23.840 --> 00:45:26.680]   And then you use a larger learning rate
[00:45:26.680 --> 00:45:30.320]   so that the features are learned.
[00:45:30.320 --> 00:45:31.520]   OK, so that's it.
[00:45:31.520 --> 00:45:32.400]   I'm going to end here.
[00:45:32.400 --> 00:45:37.440]   And here are some QR codes to the paper and a longer two
[00:45:37.440 --> 00:45:39.240]   hour talk with more details in case
[00:45:39.240 --> 00:45:42.000]   you're interested in further.
[00:45:42.000 --> 00:45:42.600]   Yeah, thanks.
[00:45:42.600 --> 00:45:46.400]   And I'll take any questions you have.
[00:45:46.400 --> 00:45:47.240]   Great.
[00:45:47.240 --> 00:45:49.600]   I've got lots, but folks in the audience,
[00:45:49.600 --> 00:45:52.200]   feel free to put them in the chat.
[00:45:52.200 --> 00:45:55.640]   The thing I wanted to start off with, I think,
[00:45:55.640 --> 00:46:00.400]   was so you noted that the standard parameterization
[00:46:00.400 --> 00:46:03.320]   that everybody uses is unstable, that you can't essentially
[00:46:03.320 --> 00:46:05.760]   take an infinite limit with it.
[00:46:05.760 --> 00:46:07.480]   Do you think that that's something--
[00:46:07.480 --> 00:46:08.280]   do you think that that's something
[00:46:08.280 --> 00:46:10.160]   that's actually a benefit for working
[00:46:10.160 --> 00:46:11.360]   with finite width networks?
[00:46:11.360 --> 00:46:14.080]   Or do you think that switching over to a parameterization
[00:46:14.080 --> 00:46:16.600]   like the maximal update one, scaling down our logics,
[00:46:16.600 --> 00:46:18.720]   scaling up our inputs, do you think
[00:46:18.720 --> 00:46:21.280]   that's something that would actually make finite width
[00:46:21.280 --> 00:46:24.800]   neural networks have better properties, perform better?
[00:46:24.800 --> 00:46:26.360]   Yeah, it's a good question.
[00:46:26.360 --> 00:46:31.320]   So this is also-- it's a good question,
[00:46:31.320 --> 00:46:33.480]   but it's also a very subtle question.
[00:46:33.480 --> 00:46:38.280]   And in some sense, it requires a paper to answer.
[00:46:38.280 --> 00:46:41.360]   And that's what's coming next in the series.
[00:46:41.360 --> 00:46:46.280]   But let me give you at least some overview
[00:46:46.280 --> 00:46:48.280]   or some of the punchlines, perhaps.
[00:46:48.280 --> 00:46:52.720]   Not into details, but some vague punchlines.
[00:46:52.720 --> 00:47:00.280]   So I say it's a subtle question because if you fix your width,
[00:47:00.280 --> 00:47:04.760]   like your width is finite, then really
[00:47:04.760 --> 00:47:09.600]   like all the parameterizations are the same up to constants.
[00:47:09.600 --> 00:47:12.560]   Because if your n is a finite number,
[00:47:12.560 --> 00:47:16.680]   then you can insert constants in front of the 1 over n
[00:47:16.680 --> 00:47:19.400]   or whatever such that you can just
[00:47:19.400 --> 00:47:22.520]   go between different parameterizations
[00:47:22.520 --> 00:47:23.280]   very freely.
[00:47:23.280 --> 00:47:33.400]   And as an aside, everything I talked about here,
[00:47:33.400 --> 00:47:35.720]   I say that 1 over n or something,
[00:47:35.720 --> 00:47:41.160]   really the thing that matters is the scaling with n.
[00:47:41.160 --> 00:47:42.680]   And I say 1 over n because I want
[00:47:42.680 --> 00:47:44.360]   to simplify the presentation.
[00:47:44.360 --> 00:47:46.680]   But in practice, if you want to implement these,
[00:47:46.680 --> 00:47:50.320]   like the constants in front of the 1 over n
[00:47:50.320 --> 00:47:53.640]   and whatever power of n is actually very important.
[00:47:53.640 --> 00:47:57.200]   So if you put all those constants in,
[00:47:57.200 --> 00:48:00.360]   then like I said, you can essentially
[00:48:00.360 --> 00:48:03.360]   go between all the different parameterizations
[00:48:03.360 --> 00:48:07.120]   as soon as you fix the width.
[00:48:07.120 --> 00:48:10.320]   And what is important and what is
[00:48:10.320 --> 00:48:15.680]   important indicated by all of this material
[00:48:15.680 --> 00:48:21.480]   is not that one parameterization is better than another one
[00:48:21.480 --> 00:48:25.800]   if you fix the width, but rather if you fix the parameterization
[00:48:25.800 --> 00:48:27.600]   and you let width go to infinity,
[00:48:27.600 --> 00:48:30.000]   then one is better than another.
[00:48:30.000 --> 00:48:32.000]   It's a scaling behavior that really matters.
[00:48:32.000 --> 00:48:38.360]   So maybe this is a roundabout way of answering a question,
[00:48:38.360 --> 00:48:41.240]   but this is what I mean by subtlety.
[00:48:41.240 --> 00:48:44.840]   And so you cannot make a statement.
[00:48:44.840 --> 00:48:47.800]   It doesn't really make sense to make a statement saying,
[00:48:47.800 --> 00:48:49.880]   oh, one parameterization is better
[00:48:49.880 --> 00:48:55.160]   than another parameterization for a particular finite width
[00:48:55.160 --> 00:48:58.720]   neural network in the context of this kind of parameterization
[00:48:58.720 --> 00:49:04.280]   where I'm really talking about scaling with width.
[00:49:04.280 --> 00:49:13.640]   But you can talk about certain things you cannot do before,
[00:49:13.640 --> 00:49:17.040]   but you can do now if you use these kind of parameterizations.
[00:49:17.040 --> 00:49:20.400]   So as a sneak peek, the next paper
[00:49:20.400 --> 00:49:23.720]   will be talking about the benefits of maximum outdate
[00:49:23.720 --> 00:49:27.120]   parameterization over any other kind of parameterizations
[00:49:27.120 --> 00:49:31.120]   enabling you to do things that you probably
[00:49:31.120 --> 00:49:35.040]   didn't think it was possible before by doing so.
[00:49:35.040 --> 00:49:39.960]   So that was a long version, but the short version is yes,
[00:49:39.960 --> 00:49:43.120]   you should use maximum outdate parameterization,
[00:49:43.120 --> 00:49:47.280]   but I'll tell you why you should use it in the next paper.
[00:49:47.280 --> 00:49:48.280]   Gotcha.
[00:49:48.280 --> 00:49:50.600]   The motivating question-- or the motivation
[00:49:50.600 --> 00:49:52.640]   for that question for me was just that my intuition
[00:49:52.640 --> 00:49:56.760]   that I got from reading the derivations of the maximal
[00:49:56.760 --> 00:49:59.160]   update and why the other parameterizations fail
[00:49:59.160 --> 00:50:02.040]   was that essentially it's a matter of gradient flow.
[00:50:02.040 --> 00:50:04.760]   Like either the gradients all get soaked up by the top layer
[00:50:04.760 --> 00:50:07.920]   or they don't make it all the way to the first layer.
[00:50:07.920 --> 00:50:12.400]   And so those are problems that you can imagine.
[00:50:12.400 --> 00:50:14.880]   Obviously, I mean, constants are important in that case.
[00:50:14.880 --> 00:50:17.720]   It's really just a scale between the first layer's gradients
[00:50:17.720 --> 00:50:18.880]   and last layer's gradients.
[00:50:18.880 --> 00:50:20.280]   It's not a difference of like zero
[00:50:20.280 --> 00:50:22.160]   versus non-zero versus infinite.
[00:50:22.160 --> 00:50:23.640]   And so that's the kind of phenomenon
[00:50:23.640 --> 00:50:26.480]   I was trying to get at with that question.
[00:50:26.480 --> 00:50:32.920]   Yeah, so maybe concretely you're asking, perhaps in practice,
[00:50:32.920 --> 00:50:35.280]   it's OK or maybe even beneficial to have
[00:50:35.280 --> 00:50:37.520]   imbalance between the gradients of different layers?
[00:50:40.720 --> 00:50:42.480]   Yeah, I think that's one way to put it.
[00:50:42.480 --> 00:50:44.000]   Yeah.
[00:50:44.000 --> 00:50:45.880]   Yeah, so that's a good question.
[00:50:45.880 --> 00:50:52.360]   And I believe that that's probably a valid question.
[00:50:52.360 --> 00:50:56.320]   And that's probably true in some cases or many cases.
[00:50:56.320 --> 00:51:02.600]   It's an empirical question I think that can be looked at.
[00:51:02.600 --> 00:51:08.800]   Of course, now, in the context of this paper
[00:51:08.800 --> 00:51:14.320]   and infinite width limits, of course,
[00:51:14.320 --> 00:51:17.640]   I think it doesn't make sense if you, for example,
[00:51:17.640 --> 00:51:20.560]   you have a three-layer neural network,
[00:51:20.560 --> 00:51:23.480]   but you parameterize it so that in the infinite width limit,
[00:51:23.480 --> 00:51:26.120]   your first layer just don't move.
[00:51:26.120 --> 00:51:28.680]   Then it's kind of a shame that you have all these parameters.
[00:51:28.680 --> 00:51:29.880]   You didn't actually use them.
[00:51:29.880 --> 00:51:37.400]   So while I agree with your sentiment
[00:51:37.400 --> 00:51:39.640]   that maybe the imbalance helps, the way
[00:51:39.640 --> 00:51:41.440]   I will implement this imbalance is actually
[00:51:41.440 --> 00:51:45.800]   to parameterize things in the maximal update framework.
[00:51:45.800 --> 00:51:49.720]   But you have constants that you can adjust so that you can
[00:51:49.720 --> 00:51:51.280]   adjust how much gradient is going
[00:51:51.280 --> 00:51:53.840]   to the first layer versus the second layer
[00:51:53.840 --> 00:51:56.800]   rather than putting them in a standard parameterization
[00:51:56.800 --> 00:52:00.920]   where when you go to infinite width limit,
[00:52:00.920 --> 00:52:05.040]   the first layer just don't learn at all.
[00:52:05.040 --> 00:52:07.400]   You essentially leave money on the table
[00:52:07.400 --> 00:52:12.280]   if you use any other kind of parameterization.
[00:52:12.280 --> 00:52:13.920]   That makes sense.
[00:52:13.920 --> 00:52:15.840]   Question from the audience asking
[00:52:15.840 --> 00:52:19.800]   about the intuition in the second backward pass with SGD
[00:52:19.800 --> 00:52:24.160]   and how that's different.
[00:52:24.160 --> 00:52:25.480]   And in particular, is there a way
[00:52:25.480 --> 00:52:28.000]   to think of what's going on, the difference
[00:52:28.000 --> 00:52:30.440]   in the distributions as being something like a prior
[00:52:30.440 --> 00:52:32.640]   or something like that?
[00:52:32.640 --> 00:52:34.400]   OK, so let's go back to that slide.
[00:52:34.400 --> 00:52:42.200]   Something like this, I guess.
[00:52:42.200 --> 00:52:53.360]   OK, so the question was how to understand the backward pass
[00:52:53.360 --> 00:52:54.600]   and if there's a prior.
[00:52:54.600 --> 00:52:56.400]   It corresponds to some kind of prior?
[00:52:56.400 --> 00:52:58.480]   Yeah, the question-- here, I'll just read it out.
[00:52:58.480 --> 00:53:01.280]   What's the intuition behind the second backward pass in SGD?
[00:53:01.280 --> 00:53:04.640]   And how are the Gaussian process utilized here?
[00:53:04.640 --> 00:53:09.120]   I think that might be a bit of a confusion, perhaps as priors.
[00:53:09.120 --> 00:53:09.600]   I see.
[00:53:09.600 --> 00:53:14.800]   So I will say that I'm not using really anything
[00:53:14.800 --> 00:53:17.200]   about the Gaussian process here.
[00:53:17.200 --> 00:53:21.920]   So in particular, like I mentioned very briefly
[00:53:21.920 --> 00:53:25.200]   at the end, if you're in a feature learning limit,
[00:53:25.200 --> 00:53:27.880]   then in initialization, you have a trivial Gaussian process
[00:53:27.880 --> 00:53:30.120]   in the sense that your variance is zero.
[00:53:30.120 --> 00:53:33.640]   You have a delta distribution at the zero function.
[00:53:33.640 --> 00:53:40.160]   So I'm not using anything about the Gaussian process.
[00:53:40.160 --> 00:53:46.600]   I am using the fact that u and v are Gaussian initialization
[00:53:46.600 --> 00:53:49.920]   with variance 1 over n.
[00:53:49.920 --> 00:53:55.480]   And in terms of intuition for the backward pass,
[00:53:55.480 --> 00:53:59.680]   so here I'm not using anything special.
[00:53:59.680 --> 00:54:02.280]   Here I'm just literally doing, I don't know,
[00:54:02.280 --> 00:54:06.600]   like a manual derivation of backprop.
[00:54:06.600 --> 00:54:10.880]   So there's nothing complicated here.
[00:54:10.880 --> 00:54:16.200]   So the statement that the gradients of the first layer--
[00:54:16.200 --> 00:54:21.120]   sorry, second layer is first layer weights times the loss
[00:54:21.120 --> 00:54:23.320]   derivative times the input.
[00:54:23.320 --> 00:54:27.160]   This is just straightforward from backprop.
[00:54:27.160 --> 00:54:29.640]   If you do backprop manually, this
[00:54:29.640 --> 00:54:33.440]   is what you get, very, very straightforward.
[00:54:33.440 --> 00:54:37.680]   And as soon as you have this identity,
[00:54:37.680 --> 00:54:43.280]   you can see that because this blue, the first layer
[00:54:43.280 --> 00:54:46.640]   blue weights, they're essentially copied
[00:54:46.640 --> 00:54:49.240]   into the second layer gradients.
[00:54:49.240 --> 00:54:52.880]   And because these two scalars are roughly terministic,
[00:54:52.880 --> 00:54:57.880]   you have approximately ID coordinates in this gradient.
[00:54:57.880 --> 00:55:00.200]   And similarly for the first layer,
[00:55:00.200 --> 00:55:05.320]   and then you can iteratively push forward this observation
[00:55:05.320 --> 00:55:07.920]   to every future step in the training process.
[00:55:07.920 --> 00:55:12.080]   I hope that answers your question.
[00:55:12.080 --> 00:55:18.440]   Yeah, and there's some more questions.
[00:55:18.440 --> 00:55:25.480]   One, are there any punchlines with regard to sparsity here?
[00:55:25.480 --> 00:55:27.560]   So there is a paper in the last ICML
[00:55:27.560 --> 00:55:31.120]   that talked about transfer with the neural tangent
[00:55:31.120 --> 00:55:32.440]   kernel and sparsity.
[00:55:32.440 --> 00:55:42.840]   So I think I've seen that paper, but I don't remember
[00:55:42.840 --> 00:55:43.800]   exactly what it is.
[00:55:43.800 --> 00:55:51.080]   So I guess-- so I would say the following.
[00:55:51.080 --> 00:55:52.840]   I think a lot of papers nowadays,
[00:55:52.840 --> 00:55:56.480]   they do experiments on CIFAR-10.
[00:55:56.480 --> 00:56:00.080]   But my point of view, especially after this paper,
[00:56:00.080 --> 00:56:01.480]   is that that's not a good data set
[00:56:01.480 --> 00:56:04.520]   to do experiments on if you care about feature learning.
[00:56:04.520 --> 00:56:11.040]   Because kernels do so well on it that whatever you do on now,
[00:56:11.040 --> 00:56:15.760]   this includes, for example, neural architecture search,
[00:56:15.760 --> 00:56:22.160]   whatever you do on it, it's kind of like you're not--
[00:56:22.160 --> 00:56:30.160]   in some sense, you're not isolating the effect
[00:56:30.160 --> 00:56:31.320]   of feature learning.
[00:56:31.320 --> 00:56:36.360]   So for example, if you were to do architecture search
[00:56:36.360 --> 00:56:40.600]   on ImageNet or do pruning on ImageNet,
[00:56:40.600 --> 00:56:44.160]   I think that's a lot more meaningful in the sense
[00:56:44.160 --> 00:56:49.440]   that in the CIFAR-10 case, I think
[00:56:49.440 --> 00:56:52.000]   when you try to induce sparsity, yeah,
[00:56:52.000 --> 00:56:55.840]   it suffices to just approximate the neural tangent kernel.
[00:56:55.840 --> 00:56:59.880]   And from the sketching literature, whatever,
[00:56:59.880 --> 00:57:06.960]   there's a lot of these, I guess, projection literature, which
[00:57:06.960 --> 00:57:09.040]   essentially just deals with sparsity.
[00:57:09.040 --> 00:57:10.960]   But you can do a lot of things with that
[00:57:10.960 --> 00:57:16.880]   by just try to approximate NTK from a very--
[00:57:16.880 --> 00:57:19.360]   yeah, just try to approximate using sparsity,
[00:57:19.360 --> 00:57:21.760]   using sparse weights very efficiently.
[00:57:21.760 --> 00:57:24.320]   You can get a way-- you can get pretty good performance
[00:57:24.320 --> 00:57:26.320]   on these smaller data sets.
[00:57:26.320 --> 00:57:28.640]   But as soon as you go to larger data sets,
[00:57:28.640 --> 00:57:30.840]   I think these things won't work because you can't just
[00:57:30.840 --> 00:57:32.760]   approximate a fixed kernel.
[00:57:32.760 --> 00:57:35.680]   You essentially have to--
[00:57:35.680 --> 00:57:38.880]   yeah, you have to find the right architecture
[00:57:38.880 --> 00:57:40.920]   to do the right feature learning.
[00:57:40.920 --> 00:57:44.200]   So yeah, I don't know.
[00:57:44.200 --> 00:57:45.400]   That was a rambling.
[00:57:45.400 --> 00:57:48.000]   I'm not sure if I answered your question.
[00:57:48.000 --> 00:57:51.120]   It looked like, yeah, they responded in the chat
[00:57:51.120 --> 00:57:53.480]   that they liked the answer.
[00:57:53.480 --> 00:57:53.960]   OK, great.
[00:57:53.960 --> 00:57:55.080]   Thanks.
[00:57:55.080 --> 00:57:59.840]   One question I wanted to ask was about computing all this stuff.
[00:57:59.840 --> 00:58:02.840]   Because you mentioned that you made use of that construction
[00:58:02.840 --> 00:58:06.680]   that you showed in order to do the word to that calculations.
[00:58:06.680 --> 00:58:08.440]   And then there's also some sections where
[00:58:08.440 --> 00:58:10.160]   you talk about the computational scaling.
[00:58:10.160 --> 00:58:13.320]   And it seems like it's really unfavorable.
[00:58:13.320 --> 00:58:15.160]   So I'm just curious, what thoughts
[00:58:15.160 --> 00:58:18.720]   do you have in that direction in terms of actually starting
[00:58:18.720 --> 00:58:20.920]   to use infinite width networks?
[00:58:20.920 --> 00:58:26.640]   Yeah, so we'll have papers later on about this topic.
[00:58:26.640 --> 00:58:33.520]   But the gist is that, kind of like in Bayesian literature,
[00:58:33.520 --> 00:58:36.560]   exact base is essentially intractable.
[00:58:36.560 --> 00:58:39.480]   You can't do it other than the simple cases.
[00:58:39.480 --> 00:58:42.800]   But there is still an entire industry academically
[00:58:42.800 --> 00:58:43.960]   behind it.
[00:58:43.960 --> 00:58:46.800]   And the gist there is that, if you're creative,
[00:58:46.800 --> 00:58:50.320]   you can approximate the Bayesian inference
[00:58:50.320 --> 00:58:53.320]   using some clever ways.
[00:58:53.320 --> 00:58:58.000]   And that strikes a middle ground between efficiency
[00:58:58.000 --> 00:59:02.280]   and the optimality of inference.
[00:59:02.280 --> 00:59:05.280]   And in this case, it's very similar.
[00:59:05.280 --> 00:59:07.960]   You can come up with different approximations
[00:59:07.960 --> 00:59:09.760]   to the infinite width limit, which
[00:59:09.760 --> 00:59:12.120]   is not as trivial as just training a really
[00:59:12.120 --> 00:59:16.960]   wide neural network, but still goes part of the way
[00:59:16.960 --> 00:59:19.040]   to the infinite width limit.
[00:59:19.040 --> 00:59:22.120]   And you can also think about different kinds of limit
[00:59:22.120 --> 00:59:24.240]   to take, and so on and so forth.
[00:59:24.240 --> 00:59:28.120]   So if you're creative, you can do a lot of things in this area.
[00:59:28.120 --> 00:59:34.120]   One thing it seemed to me from reading through the way
[00:59:34.120 --> 00:59:37.360]   that you were doing the actual calculations
[00:59:37.360 --> 00:59:40.320]   was that the tricky parts are evaluating
[00:59:40.320 --> 00:59:45.240]   a bunch of expectations for correlated random variables.
[00:59:45.240 --> 00:59:47.640]   So it sounds like, essentially, the trouble there
[00:59:47.640 --> 00:59:52.600]   is going to be doing efficient integration rather than doing
[00:59:52.600 --> 00:59:54.760]   efficient and effective derivatives
[00:59:54.760 --> 00:59:57.600]   as you have in normal finite width networks.
[01:00:00.960 --> 01:00:04.800]   Yeah, so generically speaking, if you
[01:00:04.800 --> 01:00:07.480]   have just some kind of computation graph,
[01:00:07.480 --> 01:00:10.400]   you want to just take the infinite width limit of that,
[01:00:10.400 --> 01:00:12.680]   you're going to end up with some really nasty integrals
[01:00:12.680 --> 01:00:13.440]   you want to solve.
[01:00:13.440 --> 01:00:18.280]   So if you want to have a black box limit taker,
[01:00:18.280 --> 01:00:21.160]   then you need to have a really, really good integration
[01:00:21.160 --> 01:00:23.920]   solver.
[01:00:23.920 --> 01:00:26.840]   But of course, like what I just said,
[01:00:26.840 --> 01:00:30.080]   if you just care about a very specific case, not a black box
[01:00:30.080 --> 01:00:32.800]   setting, I just want to train, I don't know,
[01:00:32.800 --> 01:00:35.600]   an infinite width bird, where infinite width could
[01:00:35.600 --> 01:00:37.720]   be some approximation of infinite width.
[01:00:37.720 --> 01:00:40.640]   Maybe I can come up with some approximation of it
[01:00:40.640 --> 01:00:42.200]   that works pretty well and doesn't
[01:00:42.200 --> 01:00:45.560]   have to do this black box integration solving.
[01:00:45.560 --> 01:00:47.920]   You know what I'm saying?
[01:00:47.920 --> 01:00:48.840]   Yeah, I know.
[01:00:48.840 --> 01:00:50.520]   I see that.
[01:00:50.520 --> 01:00:55.200]   Though there was also a mention that the correlation structure
[01:00:55.200 --> 01:00:57.280]   itself can be very complicated.
[01:00:57.280 --> 01:00:58.720]   I don't remember the exact details,
[01:00:58.720 --> 01:01:03.120]   but that it can become like an exponential number
[01:01:03.120 --> 01:01:06.040]   of polynomial terms or something like that.
[01:01:06.040 --> 01:01:08.960]   Yeah, so that's another difficulty, yes.
[01:01:08.960 --> 01:01:10.440]   But the answer is the same.
[01:01:10.440 --> 01:01:17.040]   If you're creative, you can ameliorize those problems.
[01:01:17.040 --> 01:01:19.280]   Yeah, no, it sounds like a really exciting direction
[01:01:19.280 --> 01:01:20.800]   for future work, I think.
[01:01:20.800 --> 01:01:22.320]   Yeah, I definitely encourage people
[01:01:22.320 --> 01:01:25.240]   to think about how to do those approximations.
[01:01:25.240 --> 01:01:27.840]   And I mean, I really think the analogy with the Bayes case
[01:01:27.840 --> 01:01:29.160]   is very similar.
[01:01:29.160 --> 01:01:29.880]   It's really apt.
[01:01:29.880 --> 01:01:37.920]   Yeah, yeah.
[01:01:37.920 --> 01:01:39.720]   I guess, yeah, my interest in neural networks
[01:01:39.720 --> 01:01:42.160]   partly developed because I got interested in Bayesian
[01:01:42.160 --> 01:01:45.520]   inference, found out about how intractable exact Bayes
[01:01:45.520 --> 01:01:49.400]   on graphs was, was not that enthused by the approximations,
[01:01:49.400 --> 01:01:51.320]   and then ran into the arms of neural networks
[01:01:51.320 --> 01:01:54.760]   as a way to do effective machine learning
[01:01:54.760 --> 01:01:56.800]   without running into those problems.
[01:01:56.800 --> 01:02:00.560]   Yeah, now you can kind of go a little bit back, maybe.
[01:02:00.560 --> 01:02:06.320]   I've been sneakily drawn back into the field of integration
[01:02:06.320 --> 01:02:08.480]   and inference.
[01:02:08.480 --> 01:02:17.520]   Yeah, I guess one last question, I guess, before we go here.
[01:02:17.520 --> 01:02:20.240]   So with finite width networks, when
[01:02:20.240 --> 01:02:21.840]   you increase the number of parameters,
[01:02:21.840 --> 01:02:25.120]   you often have to worry about overfitting.
[01:02:25.120 --> 01:02:27.840]   So is that something that we don't
[01:02:27.840 --> 01:02:30.160]   expect to happen with infinite width due to something
[01:02:30.160 --> 01:02:31.920]   like the double descent phenomenon?
[01:02:31.920 --> 01:02:34.480]   Or to what extent do you have to worry
[01:02:34.480 --> 01:02:37.480]   about overfitting in infinite width networks?
[01:02:37.480 --> 01:02:40.200]   Yeah, this is a very, very good question.
[01:02:40.200 --> 01:02:41.800]   So the short answer, I believe, you
[01:02:41.800 --> 01:02:46.000]   need to take into account overfitting.
[01:02:46.000 --> 01:02:52.920]   So in particular, in contrast with the kernel case,
[01:02:52.920 --> 01:03:01.080]   where in some sense, there's some inductive bias.
[01:03:01.080 --> 01:03:03.560]   I mean, I guess so does the feature learning case.
[01:03:03.560 --> 01:03:06.840]   But in the kernel case, in some sense,
[01:03:06.840 --> 01:03:09.800]   the capacity is kind of fixed in some sense.
[01:03:09.800 --> 01:03:13.040]   I don't know, very intuitively compared to feature learning.
[01:03:13.040 --> 01:03:15.320]   Feature learning, there's more wiggle room.
[01:03:15.320 --> 01:03:16.560]   You can learn more things.
[01:03:16.560 --> 01:03:19.200]   But of course, you can also overfit more things.
[01:03:19.200 --> 01:03:23.520]   So you will run into overfitting for sure.
[01:03:23.520 --> 01:03:27.960]   Like for our experiments, we have weight decay on.
[01:03:27.960 --> 01:03:30.960]   And that essentially, that means that we
[01:03:30.960 --> 01:03:39.160]   didn't run into the case where the wider neural networks
[01:03:39.160 --> 01:03:42.840]   actually do worse on the evaluation.
[01:03:42.840 --> 01:03:44.720]   But in general, I think this is true,
[01:03:44.720 --> 01:03:47.360]   that you need to take into account overfitting,
[01:03:47.360 --> 01:03:52.000]   how to take care of overfitting in the wider neural networks.
[01:03:52.000 --> 01:03:56.880]   With that said, in the modern trend
[01:03:56.880 --> 01:03:59.840]   of training larger and larger neural networks on larger
[01:03:59.840 --> 01:04:04.640]   and larger data sets, with the prime example being BERT and GPT
[01:04:04.640 --> 01:04:07.240]   and so on, we're actually not at the point
[01:04:07.240 --> 01:04:10.040]   where the model is large enough to actually overfit
[01:04:10.040 --> 01:04:14.320]   the data set significantly.
[01:04:14.320 --> 01:04:19.560]   So in that sense, if you're going in that direction,
[01:04:19.560 --> 01:04:22.400]   I think we don't need to worry too much about overfitting
[01:04:22.400 --> 01:04:24.560]   at this point.
[01:04:24.560 --> 01:04:28.320]   Because the data is so much larger than the possible model
[01:04:28.320 --> 01:04:30.320]   we can train.
[01:04:30.320 --> 01:04:33.120]   But yeah, for smaller things, if you care about something
[01:04:33.120 --> 01:04:37.280]   like CIFAR-10 size, or even when you find
[01:04:37.280 --> 01:04:40.320]   too much BERT on downstream tasks, the small data sets,
[01:04:40.320 --> 01:04:42.560]   you're going to run into a lot of overfitting.
[01:04:42.560 --> 01:04:46.440]   You have to really be careful about regularizers.
[01:04:46.440 --> 01:04:52.440]   And if you write papers, you know that for glue,
[01:04:52.440 --> 01:04:55.160]   super glue, fine tuning on the smaller data sets
[01:04:55.160 --> 01:04:57.560]   is all about regularization.
[01:04:57.560 --> 01:05:00.960]   So in that sense, without even going to infinite width limit,
[01:05:00.960 --> 01:05:02.360]   for large neural networks, you're
[01:05:02.360 --> 01:05:03.800]   going to run into this issue.
[01:05:03.800 --> 01:05:12.160]   All right.
[01:05:12.160 --> 01:05:14.000]   So that's all the time we have.
[01:05:14.000 --> 01:05:18.640]   So I'll let you go, Greg and our audience.
[01:05:18.640 --> 01:05:21.880]   Thanks for coming and presenting your work.
[01:05:21.880 --> 01:05:23.640]   I'm really excited to see where this goes,
[01:05:23.640 --> 01:05:28.360]   where people start coming up with smart ways
[01:05:28.360 --> 01:05:31.840]   to approximate this, calculate this, and make use of this
[01:05:31.840 --> 01:05:35.800]   as a new tool in our toolkit for working with neural networks.
[01:05:35.800 --> 01:05:36.640]   Yeah, thanks, man.
[01:05:36.640 --> 01:05:39.480]   Yeah, like you, I'm very excited about this work.
[01:05:39.480 --> 01:05:44.360]   And I'm very excited to see what the community does with it.
[01:05:44.360 --> 01:05:45.240]   All right.
[01:05:45.240 --> 01:05:46.760]   Thanks, guys.
[01:05:46.760 --> 01:05:47.880]   See you later.
[01:05:47.880 --> 01:05:49.120]   Thanks, everyone, for coming.

