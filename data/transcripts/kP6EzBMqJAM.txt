
[00:00:00.000 --> 00:00:03.680]   I find personally a lot of impact in being downstream of these problems.
[00:00:03.680 --> 00:00:05.520]   If I'm going to make messes, I have to clean them up.
[00:00:05.520 --> 00:00:09.520]   So in some sense, my policy work is an attempt to make sure that, you know, as I'm on the
[00:00:09.520 --> 00:00:13.440]   bleeding edge of creating this technology, I'm also providing that same insight to policymakers
[00:00:13.440 --> 00:00:15.880]   so they can adapt to this as quickly as possible.
[00:00:15.880 --> 00:00:18.600]   And to make sure that, you know, we're asking the right things of people as these models
[00:00:18.600 --> 00:00:19.600]   change.
[00:00:19.600 --> 00:00:21.880]   Part of it is also that we need to be responsible about who we work with.
[00:00:21.880 --> 00:00:25.200]   You know, there are some companies that at the end of the day, we may choose not to work
[00:00:25.200 --> 00:00:29.000]   with, or some organizations we may choose not to work with if we don't think they're
[00:00:29.000 --> 00:00:31.180]   mature enough to handle this technology properly.
[00:00:31.180 --> 00:00:35.000]   That means partially that we need to move further down the chain, not just to, you know,
[00:00:35.000 --> 00:00:36.000]   how do you build this model?
[00:00:36.000 --> 00:00:39.640]   But I'm trying to think of the right metaphor for, you know, in the chip world, but it's
[00:00:39.640 --> 00:00:43.280]   probably something like helping a company pen test their processor to make sure that,
[00:00:43.280 --> 00:00:46.000]   you know, I'm not going to tell you how to build it, but I do want to provide you with
[00:00:46.000 --> 00:00:49.640]   a toolkit to make sure that, you know, you built it such that it's robust to X, Y, and
[00:00:49.640 --> 00:00:52.000]   Z such that you don't have timing channel attacks.
[00:00:52.000 --> 00:00:55.360]   So you know, we may need to move further down the chain and help people evaluate their models

