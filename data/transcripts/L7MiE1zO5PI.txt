
[00:00:00.000 --> 00:00:04.400]   what is Wolfram language in terms of,
[00:00:04.400 --> 00:00:08.440]   sort of, I mean I can answer the question for you,
[00:00:08.440 --> 00:00:12.040]   but is it basically, not the philosophical,
[00:00:12.040 --> 00:00:14.020]   deep, the profound, the impact of it,
[00:00:14.020 --> 00:00:15.800]   I'm talking about in terms of tools,
[00:00:15.800 --> 00:00:17.200]   in terms of things you can download,
[00:00:17.200 --> 00:00:19.300]   in terms of stuff you can play with, what is it?
[00:00:19.300 --> 00:00:21.560]   What does it fit into the infrastructure?
[00:00:21.560 --> 00:00:23.440]   What are the different ways to interact with it?
[00:00:23.440 --> 00:00:25.280]   - Right, so I mean the two big things
[00:00:25.280 --> 00:00:27.840]   that people have sort of perhaps heard of
[00:00:27.840 --> 00:00:29.280]   that come from Wolfram language,
[00:00:29.280 --> 00:00:31.720]   one is Mathematica, the other is Wolfram Alpha.
[00:00:31.720 --> 00:00:34.760]   So Mathematica first came out in 1988,
[00:00:34.760 --> 00:00:37.640]   it's this system that is basically
[00:00:37.640 --> 00:00:40.800]   an instance of Wolfram language,
[00:00:40.800 --> 00:00:43.880]   and it's used to do computations,
[00:00:43.880 --> 00:00:47.200]   particularly in sort of technical areas,
[00:00:47.200 --> 00:00:49.080]   and the typical thing you're doing
[00:00:49.080 --> 00:00:52.540]   is you're typing little pieces of computational language,
[00:00:52.540 --> 00:00:54.720]   and you're getting computations done.
[00:00:54.720 --> 00:00:58.600]   - It's very kind of, there's like a symbolic,
[00:00:59.400 --> 00:01:00.600]   yeah, it's a symbolic language.
[00:01:00.600 --> 00:01:02.360]   - It's a symbolic language, so I mean,
[00:01:02.360 --> 00:01:04.120]   I don't know how to cleanly express that,
[00:01:04.120 --> 00:01:05.600]   but that makes it very distinct
[00:01:05.600 --> 00:01:08.000]   from how we think about sort of,
[00:01:08.000 --> 00:01:10.640]   I don't know, programming in a language
[00:01:10.640 --> 00:01:11.680]   like Python or something.
[00:01:11.680 --> 00:01:13.800]   - Right, so the point is that
[00:01:13.800 --> 00:01:15.600]   in a traditional programming language,
[00:01:15.600 --> 00:01:18.080]   the raw material of the programming language
[00:01:18.080 --> 00:01:21.300]   is just stuff that computers intrinsically do,
[00:01:21.300 --> 00:01:23.800]   and the point of Wolfram language
[00:01:23.800 --> 00:01:27.000]   is that what the language is talking about
[00:01:27.000 --> 00:01:28.800]   is things that exist in the world
[00:01:28.800 --> 00:01:31.400]   or things that we can imagine and construct,
[00:01:31.400 --> 00:01:34.040]   not, it's not sort of,
[00:01:34.040 --> 00:01:37.560]   it's aimed to be an abstract language from the beginning,
[00:01:37.560 --> 00:01:39.120]   and so, for example, one feature it has
[00:01:39.120 --> 00:01:41.000]   is that it's a symbolic language,
[00:01:41.000 --> 00:01:43.280]   which means that the thing called,
[00:01:43.280 --> 00:01:46.400]   you have an X, just type in X,
[00:01:46.400 --> 00:01:49.020]   and Wolfram language will just say, oh, that's X.
[00:01:49.020 --> 00:01:51.520]   It won't say error, undefined thing.
[00:01:51.520 --> 00:01:53.520]   I don't know what it is, computation,
[00:01:53.520 --> 00:01:55.780]   but in terms of the internals of the computer.
[00:01:55.780 --> 00:02:00.720]   Now, that X could perfectly well be the city of Boston.
[00:02:00.720 --> 00:02:03.240]   That's a thing, that's a symbolic thing,
[00:02:03.240 --> 00:02:06.720]   or it could perfectly well be the trajectory
[00:02:06.720 --> 00:02:09.880]   of some spacecraft represented as a symbolic thing,
[00:02:09.880 --> 00:02:14.480]   and that idea that one can work with,
[00:02:14.480 --> 00:02:17.000]   sort of computationally work with these different,
[00:02:17.000 --> 00:02:20.120]   these kinds of things that exist in the world
[00:02:20.120 --> 00:02:22.700]   or describe the world, that's really powerful,
[00:02:22.700 --> 00:02:24.960]   and that's what, I mean,
[00:02:24.960 --> 00:02:26.960]   when I started designing,
[00:02:26.960 --> 00:02:28.600]   well, when I designed the predecessor
[00:02:28.600 --> 00:02:31.520]   of what's now Wolfram language,
[00:02:31.520 --> 00:02:32.680]   which is a thing called SMP,
[00:02:32.680 --> 00:02:34.720]   which was my first computer language,
[00:02:34.720 --> 00:02:39.720]   I kind of wanted to have this sort of infrastructure
[00:02:39.720 --> 00:02:42.360]   for computation, which was as fundamental as possible.
[00:02:42.360 --> 00:02:44.680]   I mean, this is what I got for having been a physicist
[00:02:44.680 --> 00:02:48.260]   and tried to find fundamental components of things
[00:02:48.260 --> 00:02:50.660]   and wound up with this kind of idea
[00:02:50.660 --> 00:02:54.240]   of transformation rules for symbolic expressions
[00:02:54.240 --> 00:02:57.560]   as being sort of the underlying stuff
[00:02:57.560 --> 00:02:59.640]   from which computation would be built,
[00:02:59.640 --> 00:03:03.980]   and that's what we've been building from in Wolfram language
[00:03:03.980 --> 00:03:06.960]   and operationally what happens,
[00:03:06.960 --> 00:03:10.240]   it's, I would say, by far the highest level
[00:03:10.240 --> 00:03:13.080]   computer language that exists,
[00:03:13.080 --> 00:03:16.280]   and it's really been built in a very different direction
[00:03:16.280 --> 00:03:17.440]   from other languages.
[00:03:17.440 --> 00:03:20.400]   So other languages have been about,
[00:03:20.400 --> 00:03:22.180]   there is a core language.
[00:03:22.180 --> 00:03:24.520]   It really is kind of wrapped around the operations
[00:03:24.520 --> 00:03:26.520]   that a computer intrinsically does.
[00:03:26.520 --> 00:03:29.820]   Maybe people add libraries for this or that,
[00:03:29.820 --> 00:03:31.240]   but the goal of Wolfram language
[00:03:31.240 --> 00:03:35.280]   is to have the language itself be able to cover
[00:03:35.280 --> 00:03:37.000]   this sort of very broad range of things
[00:03:37.000 --> 00:03:37.920]   that show up in the world,
[00:03:37.920 --> 00:03:41.880]   and that means that there are 6,000 primitive functions
[00:03:41.880 --> 00:03:44.680]   in the Wolfram language that cover things.
[00:03:44.680 --> 00:03:46.980]   I could probably pick a random here.
[00:03:46.980 --> 00:03:51.200]   I'm gonna pick just for fun, I'll pick,
[00:03:51.200 --> 00:03:56.200]   let's take a random sample of all the things
[00:03:56.200 --> 00:03:57.660]   that we have here.
[00:03:57.660 --> 00:04:00.080]   So let's just say random sample of 10 of them,
[00:04:00.080 --> 00:04:02.240]   and let's see what we get.
[00:04:02.240 --> 00:04:05.600]   Wow, okay, so these are really different things from-
[00:04:05.600 --> 00:04:07.120]   - Yeah, these are all functions.
[00:04:07.120 --> 00:04:09.360]   - These are all functions, Boolean convert.
[00:04:09.360 --> 00:04:11.560]   Okay, that's a thing for converting
[00:04:11.560 --> 00:04:14.880]   between different types of Boolean expressions.
[00:04:14.880 --> 00:04:17.160]   - So for people just listening,
[00:04:17.160 --> 00:04:19.440]   Stephen typed in a random sample of names,
[00:04:19.440 --> 00:04:21.400]   so this is sampling from all function.
[00:04:21.400 --> 00:04:22.600]   How many you said there might be?
[00:04:22.600 --> 00:04:23.520]   - 6,000. - 6,000,
[00:04:23.520 --> 00:04:24.800]   from 6,000, 10 of them,
[00:04:24.800 --> 00:04:27.960]   and there's a hilarious variety of them.
[00:04:27.960 --> 00:04:29.480]   - Yeah, right, well, we've got things
[00:04:29.480 --> 00:04:31.880]   about dollar requester address
[00:04:31.880 --> 00:04:33.520]   that has to do with interacting
[00:04:33.520 --> 00:04:37.760]   with the world of the cloud and so on,
[00:04:37.760 --> 00:04:40.560]   discrete wavelet data, spheroid-
[00:04:40.560 --> 00:04:42.720]   - It's also graphical, sort of window-
[00:04:42.720 --> 00:04:43.920]   - Yeah, yeah, window movable,
[00:04:43.920 --> 00:04:45.640]   that's a user interface kind of thing.
[00:04:45.640 --> 00:04:48.200]   I want to pick another 10, 'cause I think this is some,
[00:04:48.200 --> 00:04:51.240]   okay, so yeah, there's a lot of infrastructure stuff here
[00:04:51.240 --> 00:04:53.720]   that you see if you just start sampling at random,
[00:04:53.720 --> 00:04:55.480]   there's a lot of kind of infrastructural things.
[00:04:55.480 --> 00:04:57.760]   If you more look at the-
[00:04:57.760 --> 00:04:59.240]   - Some of the exciting machine learning stuff
[00:04:59.240 --> 00:05:01.960]   you showed off, is that also in this pool?
[00:05:01.960 --> 00:05:04.360]   - Oh yeah, yeah, I mean, so one of those functions
[00:05:04.360 --> 00:05:07.920]   is like image identify as a function here,
[00:05:07.920 --> 00:05:09.440]   we just say image identify, I don't know,
[00:05:09.440 --> 00:05:11.440]   it's always good to, let's do this,
[00:05:11.440 --> 00:05:15.520]   let's say current image, and let's pick up an image,
[00:05:15.520 --> 00:05:16.360]   hopefully.
[00:05:16.360 --> 00:05:20.120]   - So current image, accessing the webcam,
[00:05:20.120 --> 00:05:21.440]   took a picture of yourself.
[00:05:21.440 --> 00:05:23.920]   - Took a terrible picture, but anyway,
[00:05:23.920 --> 00:05:27.040]   we can say image identify, open square brackets,
[00:05:27.040 --> 00:05:29.800]   and then we just paste that picture in there.
[00:05:29.800 --> 00:05:32.160]   - Image identify function running on the picture.
[00:05:32.160 --> 00:05:34.640]   - Oh, and it says, oh wow, it says,
[00:05:34.640 --> 00:05:37.040]   I look like a plunger, because I got this great big thing
[00:05:37.040 --> 00:05:37.880]   behind my head.
[00:05:37.880 --> 00:05:39.760]   - Classify, so this image identify classifies
[00:05:39.760 --> 00:05:42.000]   the most likely object in the image,
[00:05:42.000 --> 00:05:44.280]   and it says it's a plunger.
[00:05:44.280 --> 00:05:45.920]   - Okay, that's a bit embarrassing,
[00:05:45.920 --> 00:05:48.720]   let's see what it does, let's pick the top 10.
[00:05:48.720 --> 00:05:51.480]   Okay, well it thinks there's a,
[00:05:51.480 --> 00:05:53.120]   oh, it thinks it's pretty unlikely
[00:05:53.120 --> 00:05:55.040]   that it's a primate, a hominid, a person.
[00:05:55.040 --> 00:05:56.320]   - 8% probability.
[00:05:56.320 --> 00:05:57.160]   - Yeah, that's bad.
[00:05:57.160 --> 00:05:59.280]   - Prime age 57 is a plunger.
[00:05:59.280 --> 00:06:00.120]   - Yeah, well, so.
[00:06:00.120 --> 00:06:02.280]   - That hopefully will not give you an existential crisis,
[00:06:02.280 --> 00:06:07.280]   and then 8%, or I shouldn't say percent, but--
[00:06:07.280 --> 00:06:10.320]   - No, that's right, 8% that it's a hominid.
[00:06:10.320 --> 00:06:12.000]   And yeah, okay, it's really,
[00:06:12.000 --> 00:06:13.280]   I'm gonna do another one of these,
[00:06:13.280 --> 00:06:15.240]   just 'cause I'm embarrassed that it,
[00:06:15.240 --> 00:06:18.440]   oops, it didn't see me at all.
[00:06:18.440 --> 00:06:21.720]   There we go, let's try that, let's see what that did.
[00:06:21.720 --> 00:06:24.120]   - We took a picture with a little bit more of your--
[00:06:24.120 --> 00:06:26.240]   - A little bit more of me,
[00:06:26.240 --> 00:06:28.520]   and not just my bald head, so to speak.
[00:06:28.520 --> 00:06:31.240]   Okay, 89% probability it's a person,
[00:06:31.240 --> 00:06:34.000]   so then I would, but, you know,
[00:06:34.000 --> 00:06:36.640]   so this is image identify as an example of one--
[00:06:36.640 --> 00:06:37.680]   - Of just one of them.
[00:06:37.680 --> 00:06:38.880]   - Just one function out of 6,000.
[00:06:38.880 --> 00:06:42.000]   - And that's part of the, that's like part of the language.
[00:06:42.000 --> 00:06:42.840]   - That's part of the core language, yes.
[00:06:42.840 --> 00:06:43.680]   - That's part of the core language.
[00:06:43.680 --> 00:06:45.240]   - And I mean, you know, something like,
[00:06:45.240 --> 00:06:49.240]   I could say, I don't know, let's find the geo nearest,
[00:06:49.240 --> 00:06:50.840]   what could we find?
[00:06:50.840 --> 00:06:53.080]   Let's find the nearest volcano.
[00:06:53.080 --> 00:06:59.400]   Let's find the 10, I wonder where it thinks here is.
[00:06:59.400 --> 00:07:04.080]   Let's try finding the 10 volcanoes nearest here, okay?
[00:07:04.080 --> 00:07:08.680]   - So geo nearest volcano here, 10 nearest volcanoes.
[00:07:08.680 --> 00:07:09.960]   - Right, let's find out where those are.
[00:07:09.960 --> 00:07:12.080]   We can now, we got a list of volcanoes out,
[00:07:12.080 --> 00:07:15.200]   and I can say geo list plot that,
[00:07:15.200 --> 00:07:16.840]   and hopefully, oh, okay, so there we go.
[00:07:16.840 --> 00:07:19.680]   So there's a map that shows the positions
[00:07:19.680 --> 00:07:21.040]   of those 10 volcanoes.
[00:07:21.040 --> 00:07:23.520]   - Of the East Coast and the Midwest, and it's the,
[00:07:23.520 --> 00:07:26.240]   well, no, we're okay, we're okay, it's not too bad.
[00:07:26.240 --> 00:07:27.400]   - Yeah, they're not very close to us.
[00:07:27.400 --> 00:07:29.600]   We could measure how far away they are.
[00:07:29.600 --> 00:07:33.000]   But, you know, the fact that right in the language,
[00:07:33.000 --> 00:07:35.440]   it knows about all the volcanoes in the world,
[00:07:35.440 --> 00:07:38.440]   it knows, you know, computing what the nearest ones are,
[00:07:38.440 --> 00:07:40.280]   it knows all the maps of the world, and so on.
[00:07:40.280 --> 00:07:41.400]   - It's a fundamentally different idea
[00:07:41.400 --> 00:07:42.320]   of what a language is.
[00:07:42.320 --> 00:07:45.080]   - Yeah, right, that's why I like to talk about it
[00:07:45.080 --> 00:07:47.160]   as a full-scale computational language.
[00:07:47.160 --> 00:07:48.640]   That's what we've tried to do.
[00:07:48.640 --> 00:07:50.520]   - And just if you can comment briefly,
[00:07:50.520 --> 00:07:54.040]   I mean, this kind of, the Wolfram language,
[00:07:54.040 --> 00:07:56.000]   along with Wolfram Alpha, represents kind of
[00:07:56.000 --> 00:07:58.680]   what the dream of what AI is supposed to be.
[00:07:58.680 --> 00:08:01.720]   There's now sort of a craze of learning,
[00:08:01.720 --> 00:08:04.560]   kind of idea that we can take raw data,
[00:08:04.560 --> 00:08:08.480]   and from that extract the different hierarchies
[00:08:08.480 --> 00:08:11.480]   of abstractions in order to be able to,
[00:08:11.480 --> 00:08:13.240]   like in order to form the kind of things
[00:08:13.240 --> 00:08:17.640]   that Wolfram language operates with.
[00:08:17.640 --> 00:08:20.080]   But we're very far from learning systems
[00:08:20.080 --> 00:08:21.280]   being able to form that.
[00:08:21.280 --> 00:08:27.180]   The context of history of AI, if you could just comment on,
[00:08:27.180 --> 00:08:30.280]   there is, you said computation X.
[00:08:30.280 --> 00:08:33.360]   And there's just some sense where in the 80s and 90s,
[00:08:33.360 --> 00:08:35.400]   sort of expert systems represented
[00:08:35.400 --> 00:08:37.320]   a very particular computation X.
[00:08:37.320 --> 00:08:38.160]   - Yes.
[00:08:38.160 --> 00:08:39.640]   - And there's a kind of notion
[00:08:39.640 --> 00:08:43.400]   that those efforts didn't pan out.
[00:08:43.400 --> 00:08:44.240]   - Right.
[00:08:44.240 --> 00:08:47.840]   - But then out of that emerges kind of Wolfram language,
[00:08:47.840 --> 00:08:50.480]   Wolfram Alpha, which is the success, I mean.
[00:08:50.480 --> 00:08:51.320]   - Yeah, right.
[00:08:51.320 --> 00:08:52.600]   I think those are, in some sense,
[00:08:52.600 --> 00:08:54.400]   those efforts were too modest.
[00:08:54.400 --> 00:08:55.240]   - Right, exactly.
[00:08:55.240 --> 00:08:57.640]   - They were looking at particular areas,
[00:08:57.640 --> 00:08:59.960]   and you actually can't do it with a particular area.
[00:08:59.960 --> 00:09:01.280]   I mean, like even a problem
[00:09:01.280 --> 00:09:03.080]   like natural language understanding,
[00:09:03.080 --> 00:09:05.320]   it's critical to have broad knowledge of the world
[00:09:05.320 --> 00:09:07.980]   if you want to do good natural language understanding.
[00:09:07.980 --> 00:09:10.400]   And you kind of have to bite off the whole problem.
[00:09:10.400 --> 00:09:13.520]   If you say we're just gonna do the blocks world over here,
[00:09:13.520 --> 00:09:15.680]   so to speak, you don't really,
[00:09:15.680 --> 00:09:17.960]   it's actually, it's one of these cases
[00:09:17.960 --> 00:09:19.560]   where it's easier to do the whole thing
[00:09:19.560 --> 00:09:21.040]   than it is to do some piece of it.
[00:09:21.040 --> 00:09:22.600]   You know, one comment to make about,
[00:09:22.600 --> 00:09:25.200]   so the relationship between what we've tried to do
[00:09:25.200 --> 00:09:28.000]   and sort of the learning side of AI,
[00:09:28.000 --> 00:09:30.960]   you know, in a sense, if you look at the development
[00:09:30.960 --> 00:09:33.400]   of knowledge in our civilization as a whole,
[00:09:33.400 --> 00:09:37.360]   there was kind of this notion pre 300 years ago or so now,
[00:09:37.360 --> 00:09:38.920]   you want to figure something out about the world,
[00:09:38.920 --> 00:09:40.200]   you can reason it out.
[00:09:40.200 --> 00:09:44.160]   You can do things which are just use raw human thought.
[00:09:44.160 --> 00:09:47.720]   And then along came sort of modern mathematical science.
[00:09:47.720 --> 00:09:51.240]   And we found ways to just sort of blast through that
[00:09:51.240 --> 00:09:53.720]   by in that case, writing down equations.
[00:09:53.720 --> 00:09:57.180]   Now we also know we can do that with computation and so on.
[00:09:57.180 --> 00:09:59.120]   And so that was kind of a different thing.
[00:09:59.120 --> 00:10:03.160]   So when we look at how do we sort of encode knowledge
[00:10:03.160 --> 00:10:04.620]   and figure things out,
[00:10:04.620 --> 00:10:06.760]   one way we could do it is start from scratch,
[00:10:06.760 --> 00:10:08.080]   learn everything,
[00:10:08.080 --> 00:10:10.960]   it's just a neural net figuring everything out.
[00:10:10.960 --> 00:10:14.280]   But in a sense that denies the sort of knowledge
[00:10:14.280 --> 00:10:16.480]   based achievements of our civilization.
[00:10:16.480 --> 00:10:19.640]   Because in our civilization, we have learned lots of stuff.
[00:10:19.640 --> 00:10:21.520]   We've surveyed all the volcanoes in the world.
[00:10:21.520 --> 00:10:24.360]   We've done, you know, we figured out lots of algorithms
[00:10:24.360 --> 00:10:25.700]   for this or that.
[00:10:25.700 --> 00:10:28.960]   Those are things that we can encode computationally.
[00:10:28.960 --> 00:10:30.640]   And that's what we've tried to do.
[00:10:30.640 --> 00:10:32.420]   And we're not saying just,
[00:10:32.420 --> 00:10:34.600]   you don't have to start everything from scratch.
[00:10:34.600 --> 00:10:37.140]   So in a sense, a big part of what we've done
[00:10:37.140 --> 00:10:40.940]   is to try and sort of capture the knowledge of the world
[00:10:40.940 --> 00:10:43.520]   in computational form and computable form.
[00:10:43.520 --> 00:10:45.640]   Now, there's also some pieces
[00:10:45.640 --> 00:10:49.520]   which were for a long time undoable by computers
[00:10:49.520 --> 00:10:51.300]   like image identification,
[00:10:51.300 --> 00:10:54.100]   where there's a really, really useful module
[00:10:54.100 --> 00:10:56.620]   that we can add that is those things
[00:10:56.620 --> 00:10:59.220]   which actually were pretty easy for humans to do
[00:10:59.220 --> 00:11:01.120]   that had been hard for computers to do.
[00:11:01.120 --> 00:11:03.620]   I think the thing that's interesting that's emerging now
[00:11:03.620 --> 00:11:05.140]   is the interplay between these things,
[00:11:05.140 --> 00:11:07.220]   between this kind of knowledge of the world
[00:11:07.220 --> 00:11:09.380]   that is in a sense very symbolic
[00:11:09.380 --> 00:11:13.060]   and this kind of sort of much more statistical
[00:11:13.060 --> 00:11:17.660]   kind of things like image identification and so on.
[00:11:17.660 --> 00:11:19.180]   And putting those together
[00:11:19.180 --> 00:11:21.500]   by having this sort of symbolic representation
[00:11:21.500 --> 00:11:23.660]   of image identification,
[00:11:23.660 --> 00:11:25.980]   that that's where things get really interesting
[00:11:25.980 --> 00:11:28.540]   and where you can kind of symbolically represent patterns
[00:11:28.540 --> 00:11:30.900]   of things and images and so on.
[00:11:30.900 --> 00:11:34.840]   I think that's kind of a part of the path forward,
[00:11:34.840 --> 00:11:35.680]   so to speak.
[00:11:35.680 --> 00:11:39.460]   - Yeah, so the dream of, so the machine learning is not,
[00:11:39.460 --> 00:11:41.520]   in my view, I think the view of many people
[00:11:41.520 --> 00:11:46.520]   is not anywhere close to building the kind of wide world
[00:11:46.520 --> 00:11:50.280]   of computable knowledge that will from a language of build.
[00:11:50.280 --> 00:11:53.540]   But because you have a kind of,
[00:11:53.540 --> 00:11:56.740]   you've done the incredibly hard work of building this world,
[00:11:56.740 --> 00:12:01.060]   now machine learning can serve as tools
[00:12:01.060 --> 00:12:02.620]   to help you explore that world.
[00:12:02.620 --> 00:12:03.460]   - Yeah, yeah.
[00:12:03.460 --> 00:12:06.860]   - And that's what you've added with version 12, right?
[00:12:06.860 --> 00:12:08.740]   You added a few, I was seeing some demos.
[00:12:08.740 --> 00:12:10.500]   It looks amazing.
[00:12:10.500 --> 00:12:12.400]   - Right, I mean, I think this,
[00:12:12.400 --> 00:12:17.100]   it's sort of interesting to see the sort of the,
[00:12:17.100 --> 00:12:19.020]   once it's computable, once it's in there,
[00:12:19.020 --> 00:12:22.220]   it's running in sort of a very efficient computational way,
[00:12:22.220 --> 00:12:24.100]   but then there's sort of things like the interface
[00:12:24.100 --> 00:12:25.180]   of how do you get there?
[00:12:25.180 --> 00:12:27.340]   How do you do natural language understanding to get there?
[00:12:27.340 --> 00:12:29.340]   How do you pick out entities
[00:12:29.340 --> 00:12:31.140]   in a big piece of text or something?
[00:12:31.140 --> 00:12:34.720]   That's, I mean, actually a good example right now
[00:12:34.720 --> 00:12:37.440]   is our NLP, NLU loop, which is,
[00:12:37.440 --> 00:12:40.700]   we've done a lot of stuff, natural language understanding,
[00:12:40.700 --> 00:12:43.360]   using essentially not learning-based methods,
[00:12:43.360 --> 00:12:47.180]   using a lot of little algorithmic methods,
[00:12:47.180 --> 00:12:48.700]   human curation methods, and so on.
[00:12:48.700 --> 00:12:51.400]   - Which is when people try to enter a query
[00:12:51.400 --> 00:12:54.300]   and then converting, so the process of converting,
[00:12:54.300 --> 00:12:59.300]   NLU defined beautifully as converting their query
[00:12:59.300 --> 00:13:02.140]   into a computational language,
[00:13:02.140 --> 00:13:04.340]   which is a very well, first of all,
[00:13:04.340 --> 00:13:07.620]   super practical definition, a very useful definition,
[00:13:07.620 --> 00:13:10.460]   and then also a very clear definition
[00:13:10.460 --> 00:13:12.100]   of natural language understanding.
[00:13:12.100 --> 00:13:13.500]   - Right, I mean, a different thing
[00:13:13.500 --> 00:13:14.900]   is natural language processing,
[00:13:14.900 --> 00:13:17.620]   where it's like, here's a big lump of text,
[00:13:17.620 --> 00:13:20.640]   go pick out all the cities in that text, for example.
[00:13:20.640 --> 00:13:22.980]   And so a good example of, you know, so we do that.
[00:13:22.980 --> 00:13:26.420]   We're using modern machine learning techniques.
[00:13:26.420 --> 00:13:29.780]   And it's actually kind of an interesting process
[00:13:29.780 --> 00:13:32.460]   that's going on right now, is this loop between
[00:13:32.460 --> 00:13:35.980]   what do we pick up with NLP, we're using machine learning,
[00:13:35.980 --> 00:13:38.180]   versus what do we pick up with our more
[00:13:38.180 --> 00:13:40.420]   kind of precise computational methods
[00:13:40.420 --> 00:13:42.140]   in natural language understanding.
[00:13:42.140 --> 00:13:44.120]   And so we've got this kind of loop going between those,
[00:13:44.120 --> 00:13:45.760]   which is improving both of them.
[00:13:45.760 --> 00:13:47.420]   - Yeah, and I think you have some of the state-of-the-art
[00:13:47.420 --> 00:13:49.300]   transformers, like you have BERT in there, I think.
[00:13:49.300 --> 00:13:50.140]   - Oh, yeah.
[00:13:50.140 --> 00:13:51.060]   - So it's cool, so you have,
[00:13:51.060 --> 00:13:52.500]   you have integrating all the models.
[00:13:52.500 --> 00:13:55.500]   I mean, this is the hybrid thing that people
[00:13:55.500 --> 00:13:57.860]   have always dreamed about or talking about.
[00:13:57.860 --> 00:14:01.240]   That makes you just surprised, frankly,
[00:14:01.240 --> 00:14:03.580]   that Wolfram Language is not more popular
[00:14:03.580 --> 00:14:05.580]   than it already is.
[00:14:05.580 --> 00:14:09.660]   - You know, that's a, it's a complicated issue,
[00:14:09.660 --> 00:14:14.660]   because it's like, it involves, you know,
[00:14:14.660 --> 00:14:18.340]   it involves ideas, and ideas are absorbed
[00:14:18.340 --> 00:14:19.620]   slowly in the world.
[00:14:19.620 --> 00:14:20.460]   I mean, I think that's--
[00:14:20.460 --> 00:14:22.340]   - And then there's sort of, like what we're talking about,
[00:14:22.340 --> 00:14:23.900]   there's egos and personalities,
[00:14:23.900 --> 00:14:28.900]   and some of the absorption mechanisms of ideas
[00:14:28.900 --> 00:14:31.380]   have to do with personalities,
[00:14:31.380 --> 00:14:33.500]   and the students of personalities,
[00:14:33.500 --> 00:14:35.680]   and then a little social network.
[00:14:35.680 --> 00:14:38.660]   So it's interesting how the spread of ideas works.
[00:14:38.660 --> 00:14:40.580]   - You know what's funny with Wolfram Language
[00:14:40.580 --> 00:14:43.540]   is that we are, if you say, you know,
[00:14:43.540 --> 00:14:46.040]   what market, sort of market penetration,
[00:14:46.040 --> 00:14:50.060]   if you look at the, I would say, very high end of R&D
[00:14:50.060 --> 00:14:52.460]   and sort of the people where you say,
[00:14:52.460 --> 00:14:56.180]   "Wow, that's a really, you know, impressive, smart person,"
[00:14:56.180 --> 00:14:58.460]   they're very often users of Wolfram Language,
[00:14:58.460 --> 00:14:59.860]   very, very often.
[00:14:59.860 --> 00:15:02.560]   If you look at the more sort of, it's a funny thing,
[00:15:02.560 --> 00:15:05.120]   if you look at the more kind of, I would say,
[00:15:05.120 --> 00:15:07.340]   people who are like, "Oh, we're just plodding away
[00:15:07.340 --> 00:15:11.100]   "doing what we do," they're often not yet
[00:15:11.100 --> 00:15:13.300]   Wolfram Language users, and that dynamic,
[00:15:13.300 --> 00:15:14.780]   it's kind of odd that there hasn't been
[00:15:14.780 --> 00:15:17.380]   more rapid trickle down, because we really,
[00:15:17.380 --> 00:15:20.440]   you know, the high end, we've really been very successful
[00:15:20.440 --> 00:15:23.340]   in for a long time, and it's some,
[00:15:23.340 --> 00:15:26.780]   but with, you know, that's partly, I think,
[00:15:26.780 --> 00:15:29.740]   a consequence of, it's my fault in a sense,
[00:15:29.740 --> 00:15:32.420]   because it's kind of, you know, I have a company
[00:15:32.420 --> 00:15:37.100]   which is, really emphasizes sort of creating products
[00:15:37.100 --> 00:15:41.700]   and building a, sort of the best possible
[00:15:41.700 --> 00:15:45.740]   technical tower we can, rather than sort of
[00:15:45.740 --> 00:15:48.460]   doing the commercial side of things and pumping it out
[00:15:48.460 --> 00:15:50.180]   in sort of the most effective way.
[00:15:50.180 --> 00:15:52.100]   - And there's an interesting idea that, you know,
[00:15:52.100 --> 00:15:53.580]   perhaps you can make it more popular
[00:15:53.580 --> 00:15:58.060]   by opening everything up, sort of the GitHub model,
[00:15:58.060 --> 00:16:00.200]   but there's an interesting, I think I've heard you
[00:16:00.200 --> 00:16:03.100]   discuss this, that that turns out not to work
[00:16:03.100 --> 00:16:05.640]   in a lot of cases, like in this particular case,
[00:16:05.640 --> 00:16:09.800]   that you want it, that when you deeply care about
[00:16:09.800 --> 00:16:14.300]   the integrity, the quality of the knowledge
[00:16:14.300 --> 00:16:17.580]   that you're building, that unfortunately,
[00:16:17.580 --> 00:16:20.600]   you can't distribute that effort.
[00:16:20.600 --> 00:16:24.820]   - Yeah, it's not the nature of how things work.
[00:16:24.820 --> 00:16:27.040]   I mean, you know, what we're trying to do
[00:16:27.040 --> 00:16:29.140]   is a thing that for better or worse,
[00:16:29.140 --> 00:16:31.860]   requires leadership and it requires kind of
[00:16:31.860 --> 00:16:35.480]   maintaining a coherent vision over a long period of time,
[00:16:35.480 --> 00:16:39.780]   and doing not only the cool vision related work,
[00:16:39.780 --> 00:16:42.880]   but also the kind of mundane in the trenches
[00:16:42.880 --> 00:16:45.180]   to make the thing actually work well, work.
[00:16:45.180 --> 00:16:47.580]   - So how do you build the knowledge?
[00:16:47.580 --> 00:16:49.020]   Because that's the fascinating thing.
[00:16:49.020 --> 00:16:52.100]   That's the mundane, the fascinating and the mundane,
[00:16:52.100 --> 00:16:54.300]   is building the knowledge, the adding,
[00:16:54.300 --> 00:16:55.380]   integrating more data.
[00:16:55.380 --> 00:16:57.500]   - Yeah, I mean, that's probably not the most,
[00:16:57.500 --> 00:16:59.660]   I mean, the things like get it to work
[00:16:59.660 --> 00:17:02.340]   in all these different cloud environments and so on.
[00:17:02.340 --> 00:17:04.780]   That's pretty, you know, that's very practical stuff.
[00:17:04.780 --> 00:17:06.700]   You know, have the user interface be smooth
[00:17:06.700 --> 00:17:09.420]   and, you know, have there be, take only, you know,
[00:17:09.420 --> 00:17:11.700]   a fraction of a millisecond to do this or that.
[00:17:11.700 --> 00:17:13.000]   That's a lot of work.
[00:17:13.000 --> 00:17:18.000]   And it's, but, you know, I think my,
[00:17:18.000 --> 00:17:20.240]   it's an interesting thing over the period of time,
[00:17:20.240 --> 00:17:23.420]   you know, orphan language has existed basically
[00:17:23.420 --> 00:17:25.740]   for more than half of the total amount of time
[00:17:25.740 --> 00:17:28.120]   that any language, any computer language has existed.
[00:17:28.120 --> 00:17:31.480]   That is, the computer language is maybe 60 years old,
[00:17:31.480 --> 00:17:33.800]   you know, give or take,
[00:17:33.800 --> 00:17:36.360]   and orphan language is 33 years old.
[00:17:36.360 --> 00:17:41.040]   So it's kind of a, and I think I was realizing recently,
[00:17:41.040 --> 00:17:44.740]   there's been more innovation in the distribution of software
[00:17:44.740 --> 00:17:46.520]   than probably than in the structure
[00:17:46.520 --> 00:17:49.340]   of programming languages over that period of time.
[00:17:49.340 --> 00:17:52.680]   And we, you know, we've been sort of trying
[00:17:52.680 --> 00:17:54.000]   to do our best to adapt to it.
[00:17:54.000 --> 00:17:56.320]   And the good news is that we have, you know,
[00:17:56.320 --> 00:17:59.080]   because I have a simple private company and so on
[00:17:59.080 --> 00:18:01.560]   that doesn't have, you know, a bunch of investors,
[00:18:01.560 --> 00:18:04.040]   you know, telling us we're gonna do this or that,
[00:18:04.040 --> 00:18:05.880]   I have lots of freedom in what we can do.
[00:18:05.880 --> 00:18:09.120]   And so, for example, we're able to, oh, I don't know,
[00:18:09.120 --> 00:18:11.260]   we have this free Wolfram Engine for developers,
[00:18:11.260 --> 00:18:13.120]   which is a free version for developers.
[00:18:13.120 --> 00:18:16.040]   And we've been, you know, we've, there are site licenses
[00:18:16.040 --> 00:18:18.520]   for Mathematica and Wolfram Language
[00:18:18.520 --> 00:18:20.400]   at basically all major universities,
[00:18:20.400 --> 00:18:22.520]   certainly in the US by now.
[00:18:22.520 --> 00:18:24.560]   So it's effectively free to people
[00:18:24.560 --> 00:18:27.420]   and all universities in effect.
[00:18:27.420 --> 00:18:31.000]   And, you know, we've been doing a progression of things.
[00:18:31.000 --> 00:18:33.880]   I mean, different things like Wolfram Alpha, for example,
[00:18:33.880 --> 00:18:36.720]   the main website is just a free website.
[00:18:36.720 --> 00:18:38.400]   - What is Wolfram Alpha?
[00:18:38.400 --> 00:18:42.040]   - Okay, Wolfram Alpha is a system for answering questions
[00:18:42.040 --> 00:18:45.640]   where you ask a question with natural language
[00:18:45.640 --> 00:18:47.640]   and it'll try and generate a report
[00:18:47.640 --> 00:18:49.000]   telling you the answer to that question.
[00:18:49.000 --> 00:18:52.760]   So the question could be something like, you know,
[00:18:52.760 --> 00:18:56.800]   what's the population of Boston divided by New York
[00:18:56.800 --> 00:18:57.720]   compared to New York?
[00:18:57.720 --> 00:19:01.880]   And it'll take those words and give you an answer.
[00:19:01.880 --> 00:19:02.720]   And that have been--
[00:19:02.720 --> 00:19:06.520]   - Converts the words into computable, into--
[00:19:06.520 --> 00:19:07.800]   - Into Wolfram Language, actually.
[00:19:07.800 --> 00:19:08.800]   - Into Wolfram Language.
[00:19:08.800 --> 00:19:09.920]   - And then computational language
[00:19:09.920 --> 00:19:10.760]   and then computes the answer.
[00:19:10.760 --> 00:19:12.720]   - Do you think the underlying knowledge
[00:19:12.720 --> 00:19:15.600]   belongs to Wolfram Alpha or to the Wolfram Language?
[00:19:15.600 --> 00:19:16.440]   What's the--
[00:19:16.440 --> 00:19:18.000]   - We just call it the Wolfram Knowledge Base.
[00:19:18.000 --> 00:19:18.840]   - Knowledge Base.
[00:19:18.840 --> 00:19:22.520]   - I mean, that's been a big effort over the decades
[00:19:22.520 --> 00:19:23.720]   to collect all that stuff.
[00:19:23.720 --> 00:19:26.680]   And, you know, more of it flows in every second.
[00:19:26.680 --> 00:19:28.560]   - Can you just pause on that for a second?
[00:19:28.560 --> 00:19:31.640]   Like, that's one of the most incredible things.
[00:19:31.640 --> 00:19:33.760]   Of course, in the long-term,
[00:19:33.760 --> 00:19:37.560]   Wolfram Language itself is the fundamental thing.
[00:19:37.560 --> 00:19:40.760]   But in the amazing sort of short-term,
[00:19:40.760 --> 00:19:43.880]   the knowledge base is kind of incredible.
[00:19:43.880 --> 00:19:47.680]   So what's the process of building that knowledge base?
[00:19:47.680 --> 00:19:48.800]   The fact that you, first of all,
[00:19:48.800 --> 00:19:49.720]   from the very beginning,
[00:19:49.720 --> 00:19:51.520]   that you're brave enough to start
[00:19:51.520 --> 00:19:54.520]   to take on the general knowledge base.
[00:19:54.520 --> 00:19:58.540]   And how do you go from zero
[00:19:58.540 --> 00:20:01.520]   to the incredible knowledge base that you have now?
[00:20:01.520 --> 00:20:03.420]   - Well, yeah, it was kind of scary at some level.
[00:20:03.420 --> 00:20:05.920]   I mean, I had wondered about doing something like this
[00:20:05.920 --> 00:20:07.200]   since I was a kid.
[00:20:07.200 --> 00:20:09.240]   So it wasn't like I hadn't thought about it for a while.
[00:20:09.240 --> 00:20:10.220]   - But most of us,
[00:20:10.220 --> 00:20:14.120]   most of the brilliant dreamers give up
[00:20:14.120 --> 00:20:17.280]   such a difficult engineering notion at some point.
[00:20:17.280 --> 00:20:18.440]   - Right, right.
[00:20:18.440 --> 00:20:19.900]   Well, the thing that happened with me,
[00:20:19.900 --> 00:20:21.120]   which was kind of,
[00:20:21.120 --> 00:20:24.880]   it's a live-your-own-paradigm kind of theory.
[00:20:24.880 --> 00:20:26.800]   So basically what happened is,
[00:20:26.800 --> 00:20:30.160]   I had assumed that to build something like Wolfram Alpha
[00:20:30.160 --> 00:20:33.120]   would require sort of solving the general AI problem.
[00:20:33.120 --> 00:20:34.720]   That's what I had assumed.
[00:20:34.720 --> 00:20:36.480]   And so I kept on thinking about that,
[00:20:36.480 --> 00:20:38.120]   and I thought I don't really know how to do that,
[00:20:38.120 --> 00:20:39.840]   so I don't do anything.
[00:20:39.840 --> 00:20:42.600]   Then I worked on my new kind of science project
[00:20:42.600 --> 00:20:44.800]   and sort of exploring the computational universe
[00:20:44.800 --> 00:20:45.960]   and came up with things like
[00:20:45.960 --> 00:20:47.960]   this principle of computational equivalence,
[00:20:47.960 --> 00:20:50.260]   which say there is no bright line
[00:20:50.260 --> 00:20:53.100]   between the intelligent and the merely computational.
[00:20:53.100 --> 00:20:56.180]   So I thought, look, that's this paradigm I've built.
[00:20:56.180 --> 00:21:00.800]   Now I have to eat that dog food myself, so to speak.
[00:21:00.800 --> 00:21:02.400]   I've been thinking about doing this thing
[00:21:02.400 --> 00:21:04.480]   with computable knowledge forever,
[00:21:04.480 --> 00:21:07.120]   and let me actually try and do it.
[00:21:07.120 --> 00:21:10.640]   And so it was, if my paradigm is right,
[00:21:10.640 --> 00:21:12.240]   then this should be possible.
[00:21:12.240 --> 00:21:13.960]   But the beginning was certainly,
[00:21:13.960 --> 00:21:14.800]   it was a bit daunting.
[00:21:14.800 --> 00:21:19.480]   I remember I took the early team to a big reference library
[00:21:19.480 --> 00:21:21.120]   and we're looking at this reference library,
[00:21:21.120 --> 00:21:23.720]   and it's like, my basic statement is
[00:21:23.720 --> 00:21:25.480]   our goal over the next year or two
[00:21:25.480 --> 00:21:28.240]   is to ingest everything that's in here.
[00:21:28.240 --> 00:21:31.520]   And that's, it seemed very daunting,
[00:21:31.520 --> 00:21:34.120]   but in a sense I was well aware of the fact
[00:21:34.120 --> 00:21:35.360]   that it's finite.
[00:21:35.360 --> 00:21:36.920]   The fact that you can walk into the reference library,
[00:21:36.920 --> 00:21:39.400]   it's a big, big thing with lots of reference books
[00:21:39.400 --> 00:21:41.960]   all over the place, but it is finite.
[00:21:41.960 --> 00:21:43.760]   This is not an infinite,
[00:21:43.760 --> 00:21:46.880]   it's not the infinite corridor of, so to speak,
[00:21:46.880 --> 00:21:47.820]   of reference library.
[00:21:47.820 --> 00:21:49.880]   It's not truly infinite, so to speak.
[00:21:49.880 --> 00:21:52.760]   But no, I mean, and then what happened
[00:21:52.760 --> 00:21:54.520]   was sort of interesting there was,
[00:21:54.520 --> 00:21:57.280]   from a methodology point of view,
[00:21:57.280 --> 00:21:59.440]   was I didn't start off saying,
[00:21:59.440 --> 00:22:00.800]   let me have a grand theory
[00:22:00.800 --> 00:22:02.840]   for how all this knowledge works.
[00:22:02.840 --> 00:22:06.240]   It was like, let's implement this area, this area,
[00:22:06.240 --> 00:22:09.060]   this area, a few hundred areas and so on.
[00:22:09.060 --> 00:22:10.680]   That's a lot of work.
[00:22:10.680 --> 00:22:12.000]   I also found that,
[00:22:12.000 --> 00:22:17.520]   I've been fortunate in that our products
[00:22:17.520 --> 00:22:22.000]   get used by sort of the world's experts in lots of areas.
[00:22:22.000 --> 00:22:23.560]   And so that really helped
[00:22:23.560 --> 00:22:26.160]   'cause we were able to ask people,
[00:22:26.160 --> 00:22:27.920]   the world expert in this or that,
[00:22:27.920 --> 00:22:30.380]   and we're able to ask them for input and so on.
[00:22:30.380 --> 00:22:33.540]   And I found that my general principle was
[00:22:33.540 --> 00:22:36.460]   that any area where there wasn't some expert
[00:22:36.460 --> 00:22:40.360]   who helped us figure out what to do wouldn't be right.
[00:22:40.360 --> 00:22:42.300]   'Cause our goal was to kind of get to the point
[00:22:42.300 --> 00:22:45.100]   where we had sort of true expert level knowledge
[00:22:45.100 --> 00:22:46.560]   about everything.
[00:22:46.560 --> 00:22:49.220]   And so that the ultimate goal is
[00:22:49.220 --> 00:22:51.220]   if there's a question that can be answered
[00:22:51.220 --> 00:22:53.900]   on the basis of general knowledge in our civilization,
[00:22:53.900 --> 00:22:57.000]   make it be automatic to be able to answer that question.
[00:22:57.000 --> 00:23:01.060]   And now, well, WolfMalFa got used in Siri
[00:23:01.060 --> 00:23:03.840]   from the very beginning and it's now also used in Alexa.
[00:23:03.840 --> 00:23:07.400]   And so it's people are kind of getting more of the,
[00:23:07.400 --> 00:23:10.200]   they get more of the sense of this is
[00:23:10.200 --> 00:23:12.160]   what should be possible to do.
[00:23:12.160 --> 00:23:15.080]   I mean, in a sense, the question answering problem
[00:23:15.080 --> 00:23:17.680]   was viewed as one of the sort of core AI problems
[00:23:17.680 --> 00:23:18.640]   for a long time.
[00:23:18.640 --> 00:23:21.000]   I had kind of an interesting experience.
[00:23:21.000 --> 00:23:23.520]   I had a friend Marvin Minsky,
[00:23:23.520 --> 00:23:28.280]   who was a well-known AI person from right around here.
[00:23:28.280 --> 00:23:30.720]   And I remember when WolfMalFa was coming out,
[00:23:30.720 --> 00:23:34.160]   it was a few weeks before it came out, I think.
[00:23:34.160 --> 00:23:36.520]   I happened to see Marvin and I said,
[00:23:36.520 --> 00:23:38.400]   "I should show you this thing we have.
[00:23:38.400 --> 00:23:40.360]   "It's a question answering system."
[00:23:40.360 --> 00:23:42.880]   And he was like, "Okay."
[00:23:42.880 --> 00:23:45.040]   Typed something in, he's like, "Okay, fine."
[00:23:45.040 --> 00:23:47.080]   And then he's talking about something different.
[00:23:47.080 --> 00:23:51.340]   I said, "No, Marvin, this time it actually works.
[00:23:51.340 --> 00:23:52.920]   "Look at this, it actually works."
[00:23:52.920 --> 00:23:54.520]   He types in a few more things.
[00:23:54.520 --> 00:23:56.160]   There's maybe 10 more things.
[00:23:56.160 --> 00:23:57.840]   Of course, we have a record of what he typed in,
[00:23:57.840 --> 00:23:59.240]   which is kind of interesting.
[00:23:59.240 --> 00:24:06.920]   - Can you share where his mind was in the testing space?
[00:24:06.920 --> 00:24:08.040]   - All kinds of random things.
[00:24:08.040 --> 00:24:09.720]   He was just trying random stuff,
[00:24:09.720 --> 00:24:14.720]   medical stuff and chemistry stuff and astronomy and so on.
[00:24:14.720 --> 00:24:18.240]   And it was like, after a few minutes, he was like,
[00:24:18.240 --> 00:24:20.640]   "Oh my God, it actually works."
[00:24:22.360 --> 00:24:25.720]   But that was kind of told you something about the state,
[00:24:25.720 --> 00:24:28.800]   what happened in AI, because people had,
[00:24:28.800 --> 00:24:31.740]   in a sense, by trying to solve the bigger problem,
[00:24:31.740 --> 00:24:33.560]   we were able to actually make something that would work.
[00:24:33.560 --> 00:24:35.400]   Now, to be fair,
[00:24:35.400 --> 00:24:37.820]   we had a bunch of completely unfair advantages.
[00:24:37.820 --> 00:24:40.800]   For example, we already built a bunch of orphan language,
[00:24:40.800 --> 00:24:44.220]   which was very high level symbolic language.
[00:24:44.220 --> 00:24:49.620]   I had the practical experience of building big systems.
[00:24:50.840 --> 00:24:53.280]   I have the sort of intellectual confidence
[00:24:53.280 --> 00:24:57.000]   to not just sort of give up in doing something like this.
[00:24:57.000 --> 00:24:58.120]   I think that the,
[00:24:58.120 --> 00:25:02.680]   it's always a funny thing.
[00:25:02.680 --> 00:25:04.680]   I've worked on a bunch of big projects in my life.
[00:25:04.680 --> 00:25:09.360]   And I would say that the, you mentioned ego,
[00:25:09.360 --> 00:25:11.560]   I would also mention optimism, so to speak.
[00:25:11.560 --> 00:25:14.760]   I mean, if somebody said,
[00:25:14.760 --> 00:25:16.940]   "This project is gonna take 30 years,"
[00:25:20.680 --> 00:25:23.240]   it would be hard to sell me on that.
[00:25:23.240 --> 00:25:25.000]   I'm always in the,
[00:25:25.000 --> 00:25:27.440]   well, I can kind of see a few years,
[00:25:27.440 --> 00:25:29.680]   something's gonna happen in a few years.
[00:25:29.680 --> 00:25:32.040]   And usually it does, something happens in a few years,
[00:25:32.040 --> 00:25:35.200]   but the whole, the tail can be decades long.
[00:25:35.200 --> 00:25:38.280]   And that's, and from a personal point of view,
[00:25:38.280 --> 00:25:41.000]   always the challenge is you end up with these projects
[00:25:41.000 --> 00:25:42.840]   that have infinite tails.
[00:25:42.840 --> 00:25:45.880]   And the question is, do the tails kind of,
[00:25:45.880 --> 00:25:47.960]   do you just drown in kind of dealing
[00:25:47.960 --> 00:25:50.320]   with all of the tails of these projects?
[00:25:50.320 --> 00:25:54.560]   And that's an interesting sort of personal challenge.
[00:25:54.560 --> 00:25:57.040]   And like my efforts now to work
[00:25:57.040 --> 00:25:58.280]   on the fundamental theory of physics,
[00:25:58.280 --> 00:25:59.900]   which I've just started doing,
[00:25:59.900 --> 00:26:02.880]   and I'm having a lot of fun with it,
[00:26:02.880 --> 00:26:07.880]   but it's kind of making a bet that I can kind of,
[00:26:07.880 --> 00:26:13.040]   I can do that as well as doing the incredibly energetic
[00:26:13.040 --> 00:26:16.000]   things that I'm trying to do with orphan language and so on.
[00:26:16.000 --> 00:26:17.280]   I mean, the vision, yeah.
[00:26:17.280 --> 00:26:18.680]   - And underlying that, I mean,
[00:26:18.680 --> 00:26:21.800]   I just talked for the second time with Elon Musk
[00:26:21.800 --> 00:26:24.280]   and that you two share that quality a little bit
[00:26:24.280 --> 00:26:29.280]   of that optimism of taking on basically the daunting,
[00:26:29.280 --> 00:26:32.880]   what most people call impossible.
[00:26:32.880 --> 00:26:37.440]   And he, and you take it on out of, you can call it ego,
[00:26:37.440 --> 00:26:39.980]   you can call it naivety, you can call it optimism,
[00:26:39.980 --> 00:26:40.960]   whatever the heck it is,
[00:26:40.960 --> 00:26:43.200]   but that's how you solve the impossible things.
[00:26:43.200 --> 00:26:45.200]   - Yeah, I mean, look at what happens.
[00:26:45.200 --> 00:26:47.760]   And I don't know, in my own case,
[00:26:47.760 --> 00:26:52.160]   it's been, I progressively got a bit more confident
[00:26:52.160 --> 00:26:55.040]   and progressively able to decide
[00:26:55.040 --> 00:26:56.240]   that these projects aren't crazy.
[00:26:56.240 --> 00:26:57.440]   But then the other thing is,
[00:26:57.440 --> 00:27:01.200]   the other trap that one can end up with is,
[00:27:01.200 --> 00:27:04.000]   oh, I've done these projects and they're big.
[00:27:04.000 --> 00:27:05.960]   Let me never do a project that's any smaller
[00:27:05.960 --> 00:27:07.760]   than any project I've done so far.
[00:27:07.760 --> 00:27:08.600]   (laughing)
[00:27:08.600 --> 00:27:10.720]   And that can be a trap.
[00:27:10.960 --> 00:27:15.960]   And often these projects are of completely unknown,
[00:27:15.960 --> 00:27:17.800]   that their depth and significance
[00:27:17.800 --> 00:27:19.960]   is actually very hard to know.
[00:27:19.960 --> 00:27:25.240]   - On the sort of building this giant knowledge base
[00:27:25.240 --> 00:27:28.260]   that's behind Wolfram Language, Wolfram Alpha,
[00:27:28.260 --> 00:27:33.520]   what do you think about the internet?
[00:27:33.520 --> 00:27:37.060]   What do you think about, for example, Wikipedia,
[00:27:38.240 --> 00:27:40.840]   these large aggregations of text
[00:27:40.840 --> 00:27:43.680]   that's not converted into computable knowledge?
[00:27:43.680 --> 00:27:46.840]   Do you think, if you look at Wolfram Language,
[00:27:46.840 --> 00:27:51.040]   Wolfram Alpha, 20, 30, maybe 50 years down the line,
[00:27:51.040 --> 00:27:55.600]   do you hope to store all of the,
[00:27:55.600 --> 00:27:57.600]   sort of Google's dream is to make
[00:27:57.600 --> 00:28:00.720]   all information searchable, accessible,
[00:28:00.720 --> 00:28:03.360]   but that's really, as defined,
[00:28:03.360 --> 00:28:07.800]   it doesn't include the understanding of information.
[00:28:07.800 --> 00:28:08.640]   - Right.
[00:28:08.640 --> 00:28:12.200]   - Do you hope to make all of knowledge
[00:28:12.200 --> 00:28:15.760]   represented within-- - Sure, I would hope so.
[00:28:15.760 --> 00:28:16.920]   That's what we're trying to do.
[00:28:16.920 --> 00:28:19.920]   - How hard is that problem, like closing that gap?
[00:28:19.920 --> 00:28:20.760]   What's your sense?
[00:28:20.760 --> 00:28:21.880]   - Well, it depends on the use cases.
[00:28:21.880 --> 00:28:23.320]   I mean, so if it's a question
[00:28:23.320 --> 00:28:25.760]   of answering general knowledge questions about the world,
[00:28:25.760 --> 00:28:28.000]   we're in pretty good shape on that right now.
[00:28:28.000 --> 00:28:31.000]   If it's a question of representing
[00:28:31.000 --> 00:28:34.320]   like an area that we're going into right now
[00:28:34.320 --> 00:28:36.260]   is computational contracts,
[00:28:36.260 --> 00:28:38.200]   being able to take something
[00:28:38.200 --> 00:28:40.320]   which would be written in legalese,
[00:28:40.320 --> 00:28:42.360]   it might even be the specifications for,
[00:28:42.360 --> 00:28:44.000]   what should the self-driving car do
[00:28:44.000 --> 00:28:45.960]   when it encounters this or that or the other?
[00:28:45.960 --> 00:28:48.040]   What should the, whatever.
[00:28:48.040 --> 00:28:52.040]   Write that in a computational language
[00:28:52.040 --> 00:28:55.040]   and be able to express things about the world.
[00:28:55.040 --> 00:28:57.680]   If the creature that you see running across the road
[00:28:57.680 --> 00:29:02.360]   is a thing at this point in the tree of life,
[00:29:02.360 --> 00:29:05.920]   then swerve this way, otherwise don't, those kinds of things.
[00:29:05.920 --> 00:29:08.300]   - Are there ethical components
[00:29:08.300 --> 00:29:10.620]   when you start to get to some of the messy human things,
[00:29:10.620 --> 00:29:13.700]   are those encodable into computable knowledge?
[00:29:13.700 --> 00:29:17.480]   - Well, I think that it is a necessary feature
[00:29:17.480 --> 00:29:20.180]   of attempting to automate more in the world
[00:29:20.180 --> 00:29:23.020]   that we encode more and more of ethics
[00:29:23.020 --> 00:29:26.300]   in a way that gets sort of quickly,
[00:29:26.300 --> 00:29:28.460]   you know, is able to be dealt with by computer.
[00:29:28.460 --> 00:29:30.180]   I mean, I've been involved recently,
[00:29:30.180 --> 00:29:32.540]   I sort of got backed into being involved
[00:29:32.540 --> 00:29:36.020]   in the question of automated content selection
[00:29:36.020 --> 00:29:36.860]   on the internet.
[00:29:36.860 --> 00:29:39.980]   So, you know, the Facebooks, Googles, Twitters,
[00:29:39.980 --> 00:29:42.200]   you know, how do they rank the stuff
[00:29:42.200 --> 00:29:44.940]   they feed to us humans, so to speak?
[00:29:44.940 --> 00:29:46.940]   And the question of what are, you know,
[00:29:46.940 --> 00:29:48.460]   what should never be fed to us?
[00:29:48.460 --> 00:29:49.820]   What should be blocked forever?
[00:29:49.820 --> 00:29:52.020]   What should be upranked, you know?
[00:29:52.020 --> 00:29:55.100]   And what are the kind of principles behind that?
[00:29:55.100 --> 00:29:58.100]   And what I kind of, well, a bunch of different things
[00:29:58.100 --> 00:30:02.260]   I realized about that, but one thing that's interesting
[00:30:02.260 --> 00:30:03.740]   is being able, you know, in fact,
[00:30:03.740 --> 00:30:06.440]   you're building sort of an AI ethics,
[00:30:06.440 --> 00:30:09.140]   you have to build an AI ethics module, in effect,
[00:30:09.140 --> 00:30:11.300]   to decide, is this thing so shocking
[00:30:11.300 --> 00:30:12.940]   I'm never gonna show it to people?
[00:30:12.940 --> 00:30:15.220]   Is this thing so whatever?
[00:30:15.220 --> 00:30:17.540]   And I did realize in thinking about that,
[00:30:17.540 --> 00:30:20.180]   that, you know, there's not gonna be one of these things.
[00:30:20.180 --> 00:30:23.380]   It's not possible to decide, or it might be possible,
[00:30:23.380 --> 00:30:25.620]   but it would be really bad for the future of our species
[00:30:25.620 --> 00:30:29.580]   if we just decided there's this one AI ethics module
[00:30:29.580 --> 00:30:32.980]   and it's gonna determine the practices
[00:30:32.980 --> 00:30:35.100]   of everything in the world, so to speak.
[00:30:35.100 --> 00:30:37.100]   And I kind of realized one has to sort of break it up,
[00:30:37.100 --> 00:30:39.700]   and that's an interesting societal problem
[00:30:39.700 --> 00:30:43.220]   of how one does that and how one sort of has people
[00:30:43.220 --> 00:30:45.340]   sort of self-identify for, you know,
[00:30:45.340 --> 00:30:47.580]   I'm buying in in the case of just content selection,
[00:30:47.580 --> 00:30:49.940]   it's sort of easier because it's like an,
[00:30:49.940 --> 00:30:51.860]   it's for an individual, it's not something
[00:30:51.860 --> 00:30:56.860]   that kind of cuts across sort of societal boundaries.
[00:30:57.260 --> 00:31:00.780]   - It's a really interesting notion of,
[00:31:00.780 --> 00:31:03.500]   I heard you describe, I really like it,
[00:31:03.500 --> 00:31:08.500]   sort of maybe in the, sort of have different AI systems
[00:31:08.500 --> 00:31:09.980]   that have a certain kind of brand
[00:31:09.980 --> 00:31:11.740]   that they represent, essentially.
[00:31:11.740 --> 00:31:12.860]   - Right. - You could have like,
[00:31:12.860 --> 00:31:17.780]   I don't know, whether it's conservative or liberal,
[00:31:17.780 --> 00:31:22.220]   and then libertarian, and there's an Iranian objectivist
[00:31:22.220 --> 00:31:24.900]   AI ethics system, and different ethical,
[00:31:24.900 --> 00:31:28.140]   I mean, it's almost encoding some of the ideologies
[00:31:28.140 --> 00:31:31.140]   which we've been struggling, I come from the Soviet Union,
[00:31:31.140 --> 00:31:33.780]   that didn't work out so well with the ideologies
[00:31:33.780 --> 00:31:36.140]   that worked out there, and so you have,
[00:31:36.140 --> 00:31:38.780]   but they all, everybody purchased
[00:31:38.780 --> 00:31:40.660]   that particular ethics system.
[00:31:40.660 --> 00:31:42.860]   - Indeed. - And in the same,
[00:31:42.860 --> 00:31:45.540]   I suppose, could be done, encoded,
[00:31:45.540 --> 00:31:50.460]   that system could be encoded into computational knowledge,
[00:31:50.460 --> 00:31:53.220]   and allow us to explore in the realm of,
[00:31:53.220 --> 00:31:57.020]   in the digital space, that's a really exciting possibility.
[00:31:57.020 --> 00:32:00.380]   Are you playing with those ideas in Wolfram Language?
[00:32:00.380 --> 00:32:03.700]   - Yeah, yeah, I mean, that's, Wolfram Language
[00:32:03.700 --> 00:32:06.900]   has sort of the best opportunity to kind of express
[00:32:06.900 --> 00:32:09.700]   those essentially computational contracts about what to do.
[00:32:09.700 --> 00:32:11.660]   Now, there's a bunch more work to be done
[00:32:11.660 --> 00:32:15.140]   to do it in practice for deciding,
[00:32:15.140 --> 00:32:17.580]   is this a credible news story, what does that mean,
[00:32:17.580 --> 00:32:19.620]   or whatever else you're gonna pick.
[00:32:19.620 --> 00:32:24.620]   I think that that's, the question of exactly
[00:32:24.620 --> 00:32:27.460]   what we get to do with that is,
[00:32:27.460 --> 00:32:31.260]   for me, it's kind of a complicated thing
[00:32:31.260 --> 00:32:34.340]   because there are these big projects that I think about,
[00:32:34.340 --> 00:32:36.380]   like, find the fundamental theory of physics,
[00:32:36.380 --> 00:32:38.540]   okay, that's box number one, right?
[00:32:38.540 --> 00:32:41.820]   Box number two, solve the AI ethics problem
[00:32:41.820 --> 00:32:45.140]   in the case of, figure out how you rank all content,
[00:32:45.140 --> 00:32:46.940]   so to speak, and decide what people see,
[00:32:46.940 --> 00:32:49.700]   that's kind of a box number two, so to speak.
[00:32:49.700 --> 00:32:51.740]   These are big projects, and I think--
[00:32:51.740 --> 00:32:53.100]   - What do you think is more important,
[00:32:53.100 --> 00:32:56.460]   the fundamental nature of reality, or--
[00:32:56.460 --> 00:32:58.300]   - Depends who you ask, it's one of these things
[00:32:58.300 --> 00:33:00.900]   that's exactly like, what's the ranking, right?
[00:33:00.900 --> 00:33:03.340]   It's the ranking system, it's like,
[00:33:03.340 --> 00:33:05.780]   whose module do you use to rank that?
[00:33:05.780 --> 00:33:08.620]   If you, and I think--
[00:33:08.620 --> 00:33:10.980]   - Having multiple modules is a really compelling notion
[00:33:10.980 --> 00:33:14.260]   to us humans, that in a world where it's not clear
[00:33:14.260 --> 00:33:16.220]   that there's a right answer,
[00:33:16.220 --> 00:33:21.220]   perhaps you have systems that operate under different,
[00:33:21.220 --> 00:33:26.100]   how would you say it, I mean--
[00:33:26.100 --> 00:33:27.500]   - It's different value systems, basically.
[00:33:27.500 --> 00:33:28.460]   - Different value systems.
[00:33:28.460 --> 00:33:30.500]   - I mean, I think, in a sense,
[00:33:30.500 --> 00:33:34.620]   I mean, I'm not really a politics-oriented person,
[00:33:34.620 --> 00:33:37.380]   but in the kind of totalitarianism,
[00:33:37.380 --> 00:33:40.740]   it's kind of like, you're gonna have this system,
[00:33:40.740 --> 00:33:42.260]   and that's the way it is.
[00:33:42.260 --> 00:33:44.540]   I mean, kind of the concept
[00:33:44.540 --> 00:33:47.860]   of sort of a market-based system where you have,
[00:33:47.860 --> 00:33:50.700]   okay, I as a human, I'm gonna pick this system,
[00:33:50.700 --> 00:33:53.060]   I as another human, I'm gonna pick this system.
[00:33:53.060 --> 00:33:54.980]   I mean, that's, in a sense,
[00:33:54.980 --> 00:33:59.980]   this case of automated content selection is a non-trivial,
[00:33:59.980 --> 00:34:03.460]   but it is probably the easiest of the AI ethics situations,
[00:34:03.460 --> 00:34:06.180]   because it is, each person gets to pick for themselves,
[00:34:06.180 --> 00:34:08.580]   and there's not a huge interplay
[00:34:08.580 --> 00:34:10.420]   between what different people pick.
[00:34:10.420 --> 00:34:13.820]   By the time you're dealing with other societal things,
[00:34:13.820 --> 00:34:15.780]   like what should the policy
[00:34:15.780 --> 00:34:17.620]   of the central bank be or something.
[00:34:17.620 --> 00:34:18.460]   - Or healthcare systems,
[00:34:18.460 --> 00:34:20.900]   and all those kind of centralized kind of things.
[00:34:20.900 --> 00:34:22.500]   - Right, well, I mean, healthcare, again,
[00:34:22.500 --> 00:34:24.820]   has the feature that at some level,
[00:34:24.820 --> 00:34:27.100]   each person can pick for themselves, so to speak.
[00:34:27.100 --> 00:34:28.700]   I mean, whereas there are other things
[00:34:28.700 --> 00:34:29.980]   where there's a necessary,
[00:34:29.980 --> 00:34:31.700]   public health is one example,
[00:34:31.700 --> 00:34:35.300]   where that's not, where that doesn't get to be
[00:34:35.300 --> 00:34:38.420]   something which people can, what they pick for themselves,
[00:34:38.420 --> 00:34:39.940]   they may impose on other people,
[00:34:39.940 --> 00:34:41.980]   and then it becomes a more non-trivial piece
[00:34:41.980 --> 00:34:43.540]   of sort of political philosophy.
[00:34:43.540 --> 00:34:45.020]   - Of course, the central banking systems,
[00:34:45.020 --> 00:34:46.420]   I would argue, we would move,
[00:34:46.420 --> 00:34:48.900]   we need to move away into digital currency and so on,
[00:34:48.900 --> 00:34:51.540]   and Bitcoin and ledgers and so on.
[00:34:51.540 --> 00:34:53.100]   So there's a lot of-
[00:34:53.100 --> 00:34:54.460]   - We've been quite involved in that.
[00:34:54.460 --> 00:34:56.780]   And that's where, that's sort of the motivation
[00:34:56.780 --> 00:34:58.580]   for computational contracts,
[00:34:58.580 --> 00:35:01.700]   in part, comes out of this idea,
[00:35:01.700 --> 00:35:03.220]   oh, we can just have this autonomously
[00:35:03.220 --> 00:35:05.540]   executing smart contract.
[00:35:05.540 --> 00:35:08.580]   The idea of a computational contract is just to say,
[00:35:08.580 --> 00:35:12.620]   have something where all of the conditions
[00:35:12.620 --> 00:35:15.100]   of the contract are represented in computational form.
[00:35:15.100 --> 00:35:18.900]   So in principle, it's automatic to execute the contract.
[00:35:18.900 --> 00:35:22.620]   And I think that's, that will surely be the future
[00:35:22.620 --> 00:35:25.660]   of the idea of legal contracts written in English
[00:35:25.660 --> 00:35:27.260]   or legalese or whatever,
[00:35:27.260 --> 00:35:30.700]   and where people have to argue about what goes on
[00:35:30.700 --> 00:35:33.380]   is surely not,
[00:35:33.380 --> 00:35:36.780]   we have a much more streamlined process
[00:35:36.780 --> 00:35:38.780]   if everything can be represented computationally
[00:35:38.780 --> 00:35:40.740]   and the computers can kind of decide what to do.
[00:35:40.740 --> 00:35:42.780]   I mean, ironically enough,
[00:35:42.780 --> 00:35:46.620]   old Gottfried Leibniz back in the 1600s
[00:35:46.620 --> 00:35:48.780]   was saying exactly the same thing,
[00:35:48.780 --> 00:35:52.220]   but he had, his pinnacle of technical achievement
[00:35:52.220 --> 00:35:56.020]   was this brass four-function mechanical calculator thing
[00:35:56.020 --> 00:35:58.620]   that never really worked properly, actually.
[00:35:58.620 --> 00:36:02.740]   And so he was like 300 years too early for that idea.
[00:36:02.740 --> 00:36:06.140]   But now that idea is pretty realistic, I think.
[00:36:06.140 --> 00:36:08.820]   And you ask how much more difficult is it
[00:36:08.820 --> 00:36:11.580]   than what we have now in Wolfram language to express,
[00:36:11.580 --> 00:36:13.980]   I call it symbolic discourse language,
[00:36:13.980 --> 00:36:16.700]   being able to express sort of everything in the world
[00:36:16.700 --> 00:36:19.100]   in kind of computational symbolic form.
[00:36:19.100 --> 00:36:22.700]   I think it is absolutely within reach.
[00:36:22.700 --> 00:36:24.540]   I mean, I think it's a, you know, I don't know,
[00:36:24.540 --> 00:36:25.940]   maybe I'm just too much of an optimist,
[00:36:25.940 --> 00:36:28.580]   but I think it's a limited number of years
[00:36:28.580 --> 00:36:31.140]   to have a pretty well-built out version of that
[00:36:31.140 --> 00:36:33.300]   that will allow one to encode the kinds of things
[00:36:33.300 --> 00:36:37.580]   that are relevant to typical legal contracts
[00:36:37.580 --> 00:36:39.060]   and these kinds of things.
[00:36:39.060 --> 00:36:43.020]   - The idea of symbolic discourse language,
[00:36:43.020 --> 00:36:48.020]   can you try to define the scope of what it is?
[00:36:48.020 --> 00:36:52.540]   - So we're having a conversation, it's a natural language.
[00:36:52.540 --> 00:36:56.740]   Can we have a representation of the sort of actionable parts
[00:36:56.740 --> 00:37:00.820]   of that conversation in a precise computable form
[00:37:00.820 --> 00:37:02.460]   so that a computer could go do it?
[00:37:02.460 --> 00:37:04.780]   - And not just contracts, but really sort of
[00:37:04.780 --> 00:37:07.700]   some of the things we think of as common sense, essentially,
[00:37:07.700 --> 00:37:11.460]   even just like basic notions of human life.
[00:37:11.460 --> 00:37:13.420]   - Well, I mean, things like, you know,
[00:37:13.420 --> 00:37:17.620]   I'm getting hungry and want to eat something, right?
[00:37:17.620 --> 00:37:19.780]   That's something we don't have a representation,
[00:37:19.780 --> 00:37:21.580]   you know, in Wolfram language right now,
[00:37:21.580 --> 00:37:23.740]   if I was like, I'm eating blueberries and raspberries
[00:37:23.740 --> 00:37:25.980]   and things like that, and I'm eating this amount of them,
[00:37:25.980 --> 00:37:28.500]   we know all about those kinds of fruits and plants
[00:37:28.500 --> 00:37:30.660]   and nutrition content and all that kind of thing,
[00:37:30.660 --> 00:37:34.500]   but the I want to eat them part of it is not covered yet.
[00:37:34.500 --> 00:37:38.100]   - And you need to do that in order to have
[00:37:38.100 --> 00:37:40.260]   a complete symbolic discourse language,
[00:37:40.260 --> 00:37:42.820]   to be able to have a natural language conversation.
[00:37:42.820 --> 00:37:45.700]   - Right, right, to be able to express the kinds of things
[00:37:45.700 --> 00:37:48.540]   that say, you know, if it's a legal contract,
[00:37:48.540 --> 00:37:52.180]   it's, you know, the party's desire to have this and that.
[00:37:52.180 --> 00:37:54.140]   And that's, you know, that's a thing like,
[00:37:54.140 --> 00:37:55.900]   I want to eat a raspberry or something.
[00:37:55.900 --> 00:37:58.980]   - But isn't that, isn't this, just to let you know,
[00:37:58.980 --> 00:38:02.260]   you said it's centuries old, this dream.
[00:38:02.260 --> 00:38:03.860]   - Yes.
[00:38:03.860 --> 00:38:06.540]   - But it's also the more near term,
[00:38:06.540 --> 00:38:10.500]   the dream of Turing and formulating the Turing test.
[00:38:10.500 --> 00:38:11.340]   - Yes.
[00:38:11.340 --> 00:38:17.500]   - So, do you hope, do you think that's the ultimate test
[00:38:17.500 --> 00:38:22.500]   of creating something special?
[00:38:22.500 --> 00:38:23.580]   'Cause we said--
[00:38:23.580 --> 00:38:25.900]   - I don't know, I think by special,
[00:38:25.900 --> 00:38:30.180]   look, if the test is, does it walk and talk like a human?
[00:38:30.180 --> 00:38:32.580]   Well, that's just the talking like a human.
[00:38:32.580 --> 00:38:36.500]   But the answer is, it's an okay test.
[00:38:36.500 --> 00:38:39.380]   If you say, is it a test of intelligence?
[00:38:39.380 --> 00:38:42.660]   You know, people have attached the Wolfram Alpha API
[00:38:42.660 --> 00:38:45.020]   to, you know, Turing test bots.
[00:38:45.020 --> 00:38:47.220]   And those bots just lose immediately.
[00:38:47.220 --> 00:38:49.740]   'Cause all you have to do is ask it five questions
[00:38:49.740 --> 00:38:52.060]   that, you know, are about really obscure,
[00:38:52.060 --> 00:38:53.140]   weird pieces of knowledge,
[00:38:53.140 --> 00:38:55.060]   and it's just trot them right out.
[00:38:55.060 --> 00:38:56.900]   And you say, that's not a human.
[00:38:56.900 --> 00:38:58.620]   Right, it's a different thing.
[00:38:58.620 --> 00:39:00.460]   It's achieving a different--
[00:39:00.460 --> 00:39:03.460]   - Right now, but it's, I would argue not.
[00:39:03.460 --> 00:39:05.660]   I would argue it's not a different thing.
[00:39:05.660 --> 00:39:10.660]   It's actually legitimately, Wolfram Alpha is legitimately,
[00:39:10.660 --> 00:39:12.700]   or Wolfram Language, I think,
[00:39:12.700 --> 00:39:14.660]   is legitimately trying to solve the Turing,
[00:39:14.660 --> 00:39:16.620]   the intent of the Turing test.
[00:39:16.620 --> 00:39:17.860]   - Perhaps the intent.
[00:39:17.860 --> 00:39:18.700]   Yeah, perhaps the intent.
[00:39:18.700 --> 00:39:20.140]   I mean, it's actually kind of fun.
[00:39:20.140 --> 00:39:22.420]   You know, Alan Turing had tried to work out,
[00:39:22.420 --> 00:39:25.340]   he thought about taking Encyclopedia Britannica
[00:39:25.340 --> 00:39:27.940]   and, you know, making it computational in some way.
[00:39:27.940 --> 00:39:30.380]   And he estimated how much work it would be.
[00:39:30.380 --> 00:39:31.740]   And actually, I have to say,
[00:39:31.740 --> 00:39:34.020]   he was a bit more pessimistic than the reality.
[00:39:34.020 --> 00:39:35.580]   We did it more efficiently than that.
[00:39:35.580 --> 00:39:37.220]   - But to him, that represented--
[00:39:37.220 --> 00:39:39.100]   - So, I mean, he was on the same--
[00:39:39.100 --> 00:39:40.420]   - It's a mighty mental task.
[00:39:40.420 --> 00:39:42.460]   - Yeah, right, he had the same idea.
[00:39:42.460 --> 00:39:43.820]   I mean, it was, you know,
[00:39:43.820 --> 00:39:45.340]   we were able to do it more efficiently
[00:39:45.340 --> 00:39:48.220]   'cause we had a lot, we had layers of automation
[00:39:48.220 --> 00:39:50.820]   that he, I think, hadn't, you know,
[00:39:50.820 --> 00:39:53.940]   it's hard to imagine those layers of abstraction
[00:39:53.940 --> 00:39:55.700]   that end up being built up.
[00:39:55.700 --> 00:39:57.340]   - But to him, it represented, like,
[00:39:57.340 --> 00:39:59.100]   an impossible task, essentially.
[00:39:59.100 --> 00:40:00.420]   - Well, he thought it was difficult.
[00:40:00.420 --> 00:40:01.500]   He thought it was, you know,
[00:40:01.500 --> 00:40:02.860]   maybe if he'd lived another 50 years,
[00:40:02.860 --> 00:40:03.780]   he would've been able to do it.
[00:40:03.780 --> 00:40:04.620]   I don't know.
[00:40:04.620 --> 00:40:07.220]   (upbeat music)
[00:40:07.220 --> 00:40:09.820]   (upbeat music)
[00:40:09.820 --> 00:40:12.420]   (upbeat music)
[00:40:12.420 --> 00:40:15.020]   (upbeat music)
[00:40:15.020 --> 00:40:17.620]   (upbeat music)
[00:40:17.620 --> 00:40:20.220]   (upbeat music)
[00:40:20.220 --> 00:40:30.220]   [BLANK_AUDIO]

