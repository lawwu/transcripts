
[00:00:00.000 --> 00:00:02.880]   Today, we're going to be talking about another method
[00:00:02.880 --> 00:00:06.780]   that we can use to make retrieval for LLMs better.
[00:00:06.780 --> 00:00:09.920]   We're going to be taking a look at how to do multi-query
[00:00:09.920 --> 00:00:11.260]   within Langtrain.
[00:00:11.260 --> 00:00:12.740]   This is a very hands-on video.
[00:00:12.740 --> 00:00:15.140]   I'm going to almost jump straight into the code,
[00:00:15.140 --> 00:00:17.980]   but in a future video, I will talk a little bit more
[00:00:17.980 --> 00:00:21.580]   about multi-query and maybe a more fully-fledged
[00:00:21.580 --> 00:00:23.620]   retrieval system that uses multi-query,
[00:00:23.620 --> 00:00:25.300]   but other components as well.
[00:00:25.300 --> 00:00:26.580]   But for now in this video,
[00:00:26.580 --> 00:00:29.000]   I just want to introduce you to multi-query.
[00:00:29.000 --> 00:00:31.220]   So let's jump straight into it.
[00:00:31.220 --> 00:00:32.980]   So let's have a very quick look
[00:00:32.980 --> 00:00:35.340]   at what multi-query actually is.
[00:00:35.340 --> 00:00:38.060]   So typically in retrieval,
[00:00:38.060 --> 00:00:40.100]   what we're going to do is we're going to take a single query.
[00:00:40.100 --> 00:00:44.180]   We're going to throw that into our right pipeline.
[00:00:44.180 --> 00:00:47.340]   It's going to like go to our vector database
[00:00:47.340 --> 00:00:48.980]   and return a few items, right?
[00:00:48.980 --> 00:00:52.320]   So this single query gets turned into this query vector
[00:00:52.320 --> 00:00:55.900]   and that is mapped to some other vectors
[00:00:55.900 --> 00:00:57.100]   and we return them.
[00:00:57.100 --> 00:00:59.760]   The idea behind multi-query is that,
[00:00:59.760 --> 00:01:02.720]   rather than just having this single query,
[00:01:02.720 --> 00:01:05.520]   we actually pass this into a LLM
[00:01:05.520 --> 00:01:09.240]   and that LLM will generate multiple queries for us.
[00:01:09.240 --> 00:01:11.700]   So let's say it will generate three.
[00:01:11.700 --> 00:01:14.680]   We then have these multiple queries
[00:01:14.680 --> 00:01:17.600]   that get translated into query vectors.
[00:01:17.600 --> 00:01:20.800]   And the idea is that there is some variety between them.
[00:01:20.800 --> 00:01:22.880]   So rather than just identifying, you know,
[00:01:22.880 --> 00:01:25.600]   a single point in vector space that is relevant to us,
[00:01:25.600 --> 00:01:29.220]   we might identify three points within the vector space
[00:01:29.220 --> 00:01:30.260]   that are relevant to us.
[00:01:30.260 --> 00:01:34.220]   And we naturally pull in a higher variety of records
[00:01:34.220 --> 00:01:35.860]   using this technique.
[00:01:35.860 --> 00:01:39.260]   So what is LLMA may become three different questions
[00:01:39.260 --> 00:01:41.580]   and we'll see some examples of that later on
[00:01:41.580 --> 00:01:42.700]   in this video as well.
[00:01:42.700 --> 00:01:44.820]   But that is the core idea.
[00:01:44.820 --> 00:01:48.820]   We're searching a wider or broader vector space
[00:01:48.820 --> 00:01:50.900]   for some answers to our query.
[00:01:50.900 --> 00:01:53.780]   So as usual, here is the code.
[00:01:53.780 --> 00:01:56.120]   We're going to skip the first part of it.
[00:01:56.120 --> 00:01:59.160]   I will just point out libraries we need installed here.
[00:01:59.160 --> 00:02:02.640]   The dataset we're using here is this AI archive chunks.
[00:02:02.640 --> 00:02:04.240]   You probably have seen it before
[00:02:04.240 --> 00:02:06.160]   if you've been watching recent videos.
[00:02:06.160 --> 00:02:09.200]   And all I'm doing here is setting everything up.
[00:02:09.200 --> 00:02:12.400]   So I'm setting up my OpenAI API key,
[00:02:12.400 --> 00:02:14.080]   OpenAI embeddings.
[00:02:14.080 --> 00:02:17.080]   This is all using via Langchain.
[00:02:17.080 --> 00:02:20.680]   I'm creating my Pinecone index here.
[00:02:20.680 --> 00:02:23.360]   Again, instructions, if you need them, are there.
[00:02:23.360 --> 00:02:25.260]   Again, I'm not going to go through them.
[00:02:25.260 --> 00:02:26.700]   And then we would populate our index.
[00:02:26.700 --> 00:02:29.840]   Now the full length of the documents is, you know,
[00:02:29.840 --> 00:02:32.300]   it's not huge, but it can take a little bit of time,
[00:02:32.300 --> 00:02:35.300]   especially depending on your internet connection.
[00:02:35.300 --> 00:02:40.020]   So if needed, you can just speed things up
[00:02:40.020 --> 00:02:42.420]   by taking like the first 5,000 documents.
[00:02:42.420 --> 00:02:44.340]   Results won't be as good because that means
[00:02:44.340 --> 00:02:47.400]   we have less data to retrieve.
[00:02:47.400 --> 00:02:49.500]   But if you just want to follow along,
[00:02:49.500 --> 00:02:51.020]   I would recommend doing that.
[00:02:51.020 --> 00:02:53.380]   Basically, you will get your indexing done
[00:02:53.380 --> 00:02:56.500]   in like a minute or so if you do that.
[00:02:56.500 --> 00:02:58.480]   And here is where we actually want to
[00:02:58.480 --> 00:03:00.620]   sort of dive into the notebook.
[00:03:00.620 --> 00:03:04.260]   So we are going to be doing multi-query in Langchain,
[00:03:04.260 --> 00:03:05.180]   as it says.
[00:03:05.180 --> 00:03:07.060]   And the first thing we need to do for this
[00:03:07.060 --> 00:03:10.820]   is to initialize a VectorStore object in Langchain.
[00:03:10.820 --> 00:03:11.900]   All right, so here we're going to be using
[00:03:11.900 --> 00:03:13.220]   the Pinecone VectorStores.
[00:03:13.220 --> 00:03:15.100]   That's what we've initialized above.
[00:03:15.100 --> 00:03:17.860]   Of course, if needed, if you're using something else,
[00:03:17.860 --> 00:03:19.100]   you just swap that in there.
[00:03:19.100 --> 00:03:20.420]   All right, so we have our VectorStore.
[00:03:20.420 --> 00:03:22.340]   We also need an LLM.
[00:03:22.340 --> 00:03:24.900]   So this LLM is going to be the thing
[00:03:24.900 --> 00:03:27.420]   that both generates our queries
[00:03:27.420 --> 00:03:31.560]   and also generates the answer to our query
[00:03:31.560 --> 00:03:33.400]   at the end of the rag pipeline.
[00:03:33.400 --> 00:03:36.280]   So we initialize that as well.
[00:03:36.280 --> 00:03:41.620]   Then what we can do here is we want to initialize
[00:03:41.620 --> 00:03:43.100]   this multi-query retriever.
[00:03:43.100 --> 00:03:46.980]   So as usual, Langchain kind of has everything in there.
[00:03:46.980 --> 00:03:49.040]   So you already have a specific retriever
[00:03:49.040 --> 00:03:51.020]   that is used for multi-query.
[00:03:51.020 --> 00:03:53.100]   We'll see how we can customize that
[00:03:53.100 --> 00:03:54.260]   towards the end of the video,
[00:03:54.260 --> 00:03:56.300]   but this is what we're starting with.
[00:03:56.300 --> 00:04:00.720]   So the multi-query retriever, as most retrievers,
[00:04:00.720 --> 00:04:02.860]   requires a VectorStore as a retriever.
[00:04:02.860 --> 00:04:05.020]   And in this case, because we're generating
[00:04:05.020 --> 00:04:09.500]   the multiple queries, we also need the LLM in there as well.
[00:04:09.500 --> 00:04:11.140]   Here, we're just setting the logging level.
[00:04:11.140 --> 00:04:15.220]   So this is so that we can see the queries
[00:04:15.220 --> 00:04:18.260]   that we're generating from the multi-query retriever.
[00:04:18.260 --> 00:04:20.740]   You don't need this, but if you would like to see
[00:04:20.740 --> 00:04:24.020]   what is actually going on with generating queries,
[00:04:24.020 --> 00:04:25.780]   you probably should.
[00:04:25.780 --> 00:04:29.740]   So our question is going to be, tell me about LLAMA2.
[00:04:29.740 --> 00:04:32.900]   Okay, let's use our multi-query retriever
[00:04:32.900 --> 00:04:34.420]   and see what happens.
[00:04:34.420 --> 00:04:36.820]   Okay, so we get our logging here.
[00:04:36.820 --> 00:04:38.940]   We can see the generated queries that we have.
[00:04:38.940 --> 00:04:40.780]   So query number one is,
[00:04:40.780 --> 00:04:43.180]   what information can you provide about LLAMA2?
[00:04:43.180 --> 00:04:46.940]   Okay, so it's taken our initial query here.
[00:04:46.940 --> 00:04:50.880]   That is the first question that it will search with.
[00:04:50.880 --> 00:04:55.540]   Then we have, could you give me some details about LLAMA2?
[00:04:55.540 --> 00:04:58.580]   And three, I would like to learn more about LLAMA2.
[00:04:58.580 --> 00:05:00.100]   Can you help me with that?
[00:05:00.100 --> 00:05:03.680]   So that's, you know, I think that's kind of cool,
[00:05:03.680 --> 00:05:06.260]   but at the same time, hey, you know,
[00:05:06.260 --> 00:05:09.660]   there's not much variety between these questions.
[00:05:09.660 --> 00:05:12.160]   Like you're going to get slightly different results,
[00:05:12.160 --> 00:05:14.900]   but not significantly, because the semantic meaning
[00:05:14.900 --> 00:05:19.020]   between these is not that different, right?
[00:05:19.020 --> 00:05:20.580]   And you can kind of see that here.
[00:05:20.580 --> 00:05:23.100]   So this is the number of unique documents
[00:05:23.100 --> 00:05:24.460]   that we're returning.
[00:05:24.460 --> 00:05:28.220]   The default number of documents that is returned
[00:05:28.220 --> 00:05:30.580]   for each query is three, right?
[00:05:30.580 --> 00:05:34.120]   So in reality, we are returning nine documents here,
[00:05:34.120 --> 00:05:35.940]   but only five of those are actually unique.
[00:05:35.940 --> 00:05:39.340]   So there's a lot of overlap between these queries,
[00:05:39.340 --> 00:05:42.100]   but nonetheless, we still have brought in
[00:05:42.100 --> 00:05:44.500]   an extra two queries compared to
[00:05:44.500 --> 00:05:45.980]   if we just did a single query.
[00:05:45.980 --> 00:05:49.820]   So, you know, we are at least expanding the scope
[00:05:49.820 --> 00:05:51.660]   of our search a little bit here,
[00:05:51.660 --> 00:05:53.340]   but we can modify that,
[00:05:53.340 --> 00:05:56.180]   and I will show you how to do that pretty soon
[00:05:56.180 --> 00:05:58.340]   to broaden the scope further.
[00:05:58.340 --> 00:06:02.140]   But yes, here we can see the documents that were returned.
[00:06:02.140 --> 00:06:05.660]   They're not formatted too nicely,
[00:06:05.660 --> 00:06:06.860]   but we can see that they're relevant.
[00:06:06.860 --> 00:06:08.780]   So we know that this one here is actually coming
[00:06:08.780 --> 00:06:13.220]   from the LLAMA2 paper, and it is talking about,
[00:06:13.220 --> 00:06:14.740]   we develop and release LLAMA2,
[00:06:14.740 --> 00:06:19.740]   a seven to seven billion parameter LLM, right?
[00:06:19.740 --> 00:06:22.460]   So it's giving us some information there.
[00:06:22.460 --> 00:06:26.780]   The next one, which is here, is actually talking about,
[00:06:26.780 --> 00:06:30.680]   I think it's talking about LLAMAs, like the, yeah, here.
[00:06:30.680 --> 00:06:33.480]   It's talking about alpacas and LLAMAs and so on.
[00:06:33.480 --> 00:06:36.180]   I'm not sure what this one is.
[00:06:36.180 --> 00:06:37.580]   I mean, it's talking about LLMs,
[00:06:37.580 --> 00:06:39.260]   but it's just not in,
[00:06:39.260 --> 00:06:41.840]   it's not talking about LLAMA in the context that we want.
[00:06:41.840 --> 00:06:46.340]   We have another one here, LLAMA2 paper again.
[00:06:46.340 --> 00:06:49.620]   So we're getting something that is relevant, hopefully.
[00:06:49.620 --> 00:06:52.860]   We develop and release LLAMA2, and then there.
[00:06:52.860 --> 00:06:55.900]   Generally perform better than existing open source models.
[00:06:55.900 --> 00:06:58.620]   Okay, so we're getting more information there.
[00:06:58.620 --> 00:07:00.620]   Chain of thought prompting.
[00:07:00.620 --> 00:07:03.580]   Here we get, again, it's talking about the animals.
[00:07:03.580 --> 00:07:08.420]   And then this final one here is the base paper.
[00:07:08.420 --> 00:07:13.420]   And this one is talking about Sanford alpaca
[00:07:13.420 --> 00:07:15.460]   and instruction following LLAMA model, right?
[00:07:15.460 --> 00:07:16.940]   So that one is relevant.
[00:07:16.940 --> 00:07:19.940]   So we have a few results here, not all of them relevant,
[00:07:19.940 --> 00:07:23.180]   but for the most part, we can work with that.
[00:07:23.180 --> 00:07:25.300]   So let's come down to here and see
[00:07:25.300 --> 00:07:27.340]   how we actually implement multi-query
[00:07:27.340 --> 00:07:29.880]   into like a full route pipeline.
[00:07:29.880 --> 00:07:32.900]   And we're gonna do this to start with,
[00:07:32.900 --> 00:07:35.260]   and we're gonna do this in this video, at least,
[00:07:35.260 --> 00:07:38.020]   using LangChain and the sort of standard way
[00:07:38.020 --> 00:07:39.520]   of doing it in LangChain.
[00:07:39.520 --> 00:07:42.980]   In another future video, we'll look at doing it
[00:07:42.980 --> 00:07:46.220]   sort of outside LangChain as well, just so we can compare.
[00:07:46.220 --> 00:07:49.460]   Okay, so to do that, within RAG here,
[00:07:49.460 --> 00:07:51.660]   we've already built a retrieval part.
[00:07:51.660 --> 00:07:53.020]   All right, so that's what I just showed you,
[00:07:53.020 --> 00:07:54.540]   the multi-query retriever.
[00:07:54.540 --> 00:07:56.340]   We need the augmentation
[00:07:56.340 --> 00:07:58.820]   for the generation of our queries part.
[00:07:58.820 --> 00:08:02.260]   So to do that, we set up this QA prompt,
[00:08:02.260 --> 00:08:05.140]   so question answering prompt, just has some instructions,
[00:08:05.140 --> 00:08:06.860]   and then we feed in some context
[00:08:06.860 --> 00:08:09.100]   that we get from the retrieval part,
[00:08:09.100 --> 00:08:11.340]   and then we add in our question.
[00:08:11.340 --> 00:08:12.580]   So I'm gonna run that.
[00:08:12.580 --> 00:08:16.260]   And this is how we can feed our,
[00:08:16.260 --> 00:08:18.720]   the documents that we've gotten from before,
[00:08:18.720 --> 00:08:23.400]   the ones I just showed you, into that QA chain directly.
[00:08:23.400 --> 00:08:25.100]   Okay, and we get this answer here.
[00:08:25.100 --> 00:08:26.740]   So alarm2 is a collection of pre-trained
[00:08:26.740 --> 00:08:28.020]   fine-tuned language models,
[00:08:28.020 --> 00:08:32.220]   ranging in scale of seven to 70 billion parameter models.
[00:08:32.220 --> 00:08:34.780]   There's some weird formatting here,
[00:08:34.780 --> 00:08:37.580]   that's from the source data.
[00:08:37.580 --> 00:08:39.420]   They're optimized for dialogue use cases,
[00:08:39.420 --> 00:08:42.460]   developed and released by so-on and so-on.
[00:08:42.460 --> 00:08:46.000]   All right, so there's quite a bit of information in there,
[00:08:46.000 --> 00:08:47.280]   which is useful.
[00:08:47.280 --> 00:08:48.620]   So it does work.
[00:08:48.620 --> 00:08:50.020]   Let's see if we can, you know,
[00:08:50.020 --> 00:08:51.780]   let's see what else we can do.
[00:08:51.780 --> 00:08:53.180]   So I'm gonna put all this together
[00:08:53.180 --> 00:08:55.100]   into a single sequential chain.
[00:08:55.100 --> 00:08:57.100]   So this is like LangChain's way
[00:08:57.100 --> 00:08:58.980]   of just putting things together.
[00:08:58.980 --> 00:09:02.160]   So rather than me kind of like writing some code
[00:09:02.160 --> 00:09:03.940]   to handle this stuff,
[00:09:03.940 --> 00:09:06.060]   I'm kind of chaining things together
[00:09:06.060 --> 00:09:09.740]   with LangChain's approach of doing it.
[00:09:09.740 --> 00:09:11.960]   Honestly, whichever approach you go with,
[00:09:11.960 --> 00:09:14.040]   it's up to you.
[00:09:14.040 --> 00:09:15.180]   Depending on what you're doing,
[00:09:15.180 --> 00:09:18.260]   it might be easier just to write a function
[00:09:18.260 --> 00:09:19.700]   that handles all this stuff.
[00:09:19.700 --> 00:09:21.900]   But again, it's, you know, it's up to you.
[00:09:21.900 --> 00:09:23.340]   This is a LangChain way of doing it,
[00:09:23.340 --> 00:09:24.740]   if you'd like to do so.
[00:09:24.740 --> 00:09:26.240]   So for the retrieval part,
[00:09:26.240 --> 00:09:29.020]   we can't connect the retriever directly
[00:09:29.020 --> 00:09:30.640]   to the generation part
[00:09:30.640 --> 00:09:32.700]   because we need to format our context
[00:09:32.700 --> 00:09:34.020]   that come out of that.
[00:09:34.020 --> 00:09:36.500]   So what I have done here
[00:09:36.500 --> 00:09:38.300]   is I've defined this function,
[00:09:38.300 --> 00:09:40.300]   which does the retrieval,
[00:09:40.300 --> 00:09:42.500]   and then also does the formatting for us
[00:09:42.500 --> 00:09:43.820]   and then returns it.
[00:09:43.820 --> 00:09:48.500]   And then I'm wrapping this retrieval transform function
[00:09:48.500 --> 00:09:50.740]   into what's called transform chain.
[00:09:50.740 --> 00:09:52.420]   Okay, so it's basically,
[00:09:52.420 --> 00:09:54.900]   it's like a custom chain in LangChain.
[00:09:54.900 --> 00:09:56.800]   That's the way I would view it.
[00:09:56.800 --> 00:10:00.680]   So the input into this is going to be a question,
[00:10:00.680 --> 00:10:02.040]   which we set up here,
[00:10:02.040 --> 00:10:04.640]   and the output is going to be query and context,
[00:10:04.640 --> 00:10:06.440]   which we have set up here.
[00:10:06.440 --> 00:10:10.240]   Now, one thing that you can't do with this,
[00:10:10.240 --> 00:10:12.200]   or at least in the next part here,
[00:10:12.200 --> 00:10:15.480]   is that you cannot have the same input variable
[00:10:15.480 --> 00:10:17.200]   and the same output variable.
[00:10:17.200 --> 00:10:19.280]   All right, so that's why I'm calling this question
[00:10:19.280 --> 00:10:20.560]   and why this is the query.
[00:10:20.560 --> 00:10:23.640]   If I put question here, I'm going to get an error.
[00:10:23.640 --> 00:10:26.700]   So we just need to be wary of that.
[00:10:26.700 --> 00:10:29.260]   Now that we have our transform chain for retrieval
[00:10:29.260 --> 00:10:31.900]   and we have our QA chain from before,
[00:10:31.900 --> 00:10:34.780]   we wrap all of this into a single sequential chain
[00:10:34.780 --> 00:10:38.620]   and that gives us our RAG pipeline in LangChain.
[00:10:38.620 --> 00:10:43.120]   So let me run this and this.
[00:10:43.120 --> 00:10:47.540]   With that, we can just perform the full RAG pipeline
[00:10:47.540 --> 00:10:50.140]   by calling this method here.
[00:10:51.580 --> 00:10:54.620]   Okay, so we input our question.
[00:10:54.620 --> 00:10:56.860]   You can see, I still have the logging on,
[00:10:56.860 --> 00:10:59.900]   so you can see the output there at the top.
[00:10:59.900 --> 00:11:03.620]   Don't know why it's in this weird color, but okay.
[00:11:03.620 --> 00:11:06.460]   So at the top, we have the same things we saw before,
[00:11:06.460 --> 00:11:09.180]   those three questions, and then this is the output.
[00:11:09.180 --> 00:11:10.540]   All right, it's the same as what we have before
[00:11:10.540 --> 00:11:12.100]   'cause we're actually just doing the same thing.
[00:11:12.100 --> 00:11:14.280]   We just wrapped it into this sequential chain
[00:11:14.280 --> 00:11:15.240]   from LangChain.
[00:11:15.240 --> 00:11:16.140]   Okay, cool.
[00:11:16.140 --> 00:11:18.980]   So that's the full RAG pipeline.
[00:11:18.980 --> 00:11:22.460]   Now let's take a look at modifying our prompt
[00:11:22.460 --> 00:11:24.380]   in order to change the behavior
[00:11:24.380 --> 00:11:26.380]   of how we're generating these queries.
[00:11:26.380 --> 00:11:28.780]   And I think this is very important
[00:11:28.780 --> 00:11:33.000]   and probably the most important part of this video,
[00:11:33.000 --> 00:11:36.740]   which is, okay, how does it behave with different queries?
[00:11:36.740 --> 00:11:38.300]   So we're gonna start with this prompt A.
[00:11:38.300 --> 00:11:39.460]   So we can look at this.
[00:11:39.460 --> 00:11:40.380]   I'm just saying, okay,
[00:11:40.380 --> 00:11:42.840]   generate three different search queries
[00:11:42.840 --> 00:11:44.860]   that aim to answer the user question
[00:11:44.860 --> 00:11:46.780]   from multiple perspectives.
[00:11:46.780 --> 00:11:48.140]   Each query must tackle the question
[00:11:48.140 --> 00:11:49.980]   from a different viewpoint.
[00:11:49.980 --> 00:11:52.940]   We want to get a variety of relevant search results.
[00:11:52.940 --> 00:11:56.300]   Okay, so what I'm trying to do with this query
[00:11:56.300 --> 00:11:59.340]   is search or add more variety
[00:11:59.340 --> 00:12:01.300]   to the queries that are being created.
[00:12:01.300 --> 00:12:04.020]   So that is the idea here.
[00:12:04.020 --> 00:12:05.900]   Now we can see how that performs.
[00:12:05.900 --> 00:12:08.260]   We come down to here.
[00:12:08.260 --> 00:12:09.760]   I'm going to put it into here.
[00:12:09.760 --> 00:12:14.760]   So this is kind of like our custom approach to doing this.
[00:12:14.980 --> 00:12:18.620]   We have this lineless object here, an output parser.
[00:12:18.620 --> 00:12:20.180]   Essentially what this is going to do
[00:12:20.180 --> 00:12:25.180]   is our query here is going to generate the questions
[00:12:25.180 --> 00:12:28.060]   separated by new line characters.
[00:12:28.060 --> 00:12:31.660]   This output parser here is going to look for new lines
[00:12:31.660 --> 00:12:35.180]   and it's gonna separate out the queries based on that.
[00:12:35.180 --> 00:12:38.220]   So it's just parsing the output we generate here.
[00:12:38.220 --> 00:12:39.260]   Okay, cool.
[00:12:39.260 --> 00:12:42.180]   So we can run that.
[00:12:42.180 --> 00:12:46.140]   And what I'm gonna do is reinitialize the retriever
[00:12:46.140 --> 00:12:47.720]   with our new LLM chain here.
[00:12:47.720 --> 00:12:51.820]   And yeah, we run this.
[00:12:51.820 --> 00:12:55.480]   And we'll just see the sort of queries that we get, okay?
[00:12:55.480 --> 00:12:57.700]   So we get what are the characteristics
[00:12:57.700 --> 00:12:59.420]   and behavior of llamas?
[00:12:59.420 --> 00:13:02.800]   How are llamas used in agriculture and farming?
[00:13:02.800 --> 00:13:04.040]   What are the different breeds of llamas
[00:13:04.040 --> 00:13:05.220]   and their unique traits?
[00:13:05.220 --> 00:13:10.100]   So yes, we've definitely got more diverse questions here,
[00:13:10.100 --> 00:13:13.060]   but now it's, you know, it sees llama too.
[00:13:13.060 --> 00:13:16.420]   And it's like, okay, you want me to ask some unique,
[00:13:16.420 --> 00:13:19.260]   diverse questions about llamas, perfect.
[00:13:19.260 --> 00:13:23.220]   So there's kind of like pros and cons to doing this.
[00:13:23.220 --> 00:13:25.220]   Obviously the results we get here
[00:13:25.220 --> 00:13:28.460]   are not going to be as relevant to our query.
[00:13:28.460 --> 00:13:30.820]   Although we actually still do get the llama paper
[00:13:30.820 --> 00:13:33.660]   because honestly, I don't think there's much in there
[00:13:33.660 --> 00:13:36.860]   that talks about llamas in agriculture.
[00:13:36.860 --> 00:13:41.860]   So yeah, that doesn't really work.
[00:13:41.860 --> 00:13:43.900]   So let's try another prompt.
[00:13:43.900 --> 00:13:46.040]   So what I want to kind of point out here
[00:13:46.040 --> 00:13:49.900]   is that when you're trying to increase the variety
[00:13:49.900 --> 00:13:51.100]   of the prompts that are generated
[00:13:51.100 --> 00:13:54.260]   by your multi-query system here,
[00:13:54.260 --> 00:13:56.100]   the more you increase that variety,
[00:13:56.100 --> 00:13:58.000]   the more likely it is to hallucinate
[00:13:58.000 --> 00:14:00.980]   or just kind of go down the wrong path.
[00:14:00.980 --> 00:14:03.900]   And that's exactly what we just saw there.
[00:14:03.900 --> 00:14:06.660]   So now what I'm going to do in a second prompt
[00:14:06.660 --> 00:14:07.780]   is be more specific.
[00:14:07.780 --> 00:14:10.180]   I'm going to say, okay, I'm basically saying the same
[00:14:10.180 --> 00:14:13.940]   as what I said in that first prompt, but I just added this.
[00:14:13.940 --> 00:14:16.340]   The user questions are focused on LLMs,
[00:14:16.340 --> 00:14:18.880]   machine learning, and related disciplines, right?
[00:14:18.880 --> 00:14:21.860]   So I'm just giving the LLM some context
[00:14:21.860 --> 00:14:25.220]   as to what it should be generating queries for.
[00:14:25.220 --> 00:14:29.800]   And well, let's see, let's see if this helps our LLM.
[00:14:29.800 --> 00:14:32.860]   So we put this in, let's run this,
[00:14:32.860 --> 00:14:36.540]   and then run our retriever again.
[00:14:36.540 --> 00:14:40.060]   Okay, so we have more variety here, seven,
[00:14:40.060 --> 00:14:42.780]   which is more than the five we had for the first one.
[00:14:42.780 --> 00:14:45.700]   And now we can see, okay, what are the key features
[00:14:45.700 --> 00:14:49.300]   and capabilities of large language model LLAMA2?
[00:14:49.300 --> 00:14:51.540]   Okay, so that's cool.
[00:14:51.540 --> 00:14:54.460]   How does LLAMA2 compare to other large language models
[00:14:54.460 --> 00:14:57.860]   in terms of performance and efficiency?
[00:14:57.860 --> 00:15:02.060]   Okay, what are the applications and use cases of LLAMA2
[00:15:02.060 --> 00:15:05.620]   in the field of machine learning and NLP?
[00:15:05.620 --> 00:15:10.100]   Right, so I personally think those results are way better
[00:15:10.100 --> 00:15:11.740]   than what we were getting before.
[00:15:11.740 --> 00:15:14.580]   And we can see the docs that are being returned here.
[00:15:14.580 --> 00:15:15.580]   It's not a big dataset,
[00:15:15.580 --> 00:15:18.860]   so I don't expect anything outstanding here,
[00:15:18.860 --> 00:15:21.860]   but we should at least maybe see less
[00:15:21.860 --> 00:15:25.940]   of the agriculture documents in here.
[00:15:25.940 --> 00:15:30.140]   So this one is definitely talking about LLAMA2.
[00:15:30.140 --> 00:15:33.180]   We can go on to the next one, which is here.
[00:15:33.180 --> 00:15:36.540]   Even large language models are brittle, social bias.
[00:15:36.540 --> 00:15:43.060]   So this one, I don't see anything relevant for LLAMA2 here,
[00:15:43.060 --> 00:15:44.420]   unless I'm missing.
[00:15:44.420 --> 00:15:46.020]   Yeah, I don't think so.
[00:15:46.020 --> 00:15:48.100]   So that one isn't so relevant.
[00:15:48.100 --> 00:15:50.180]   Let's see this one.
[00:15:50.180 --> 00:15:51.740]   Okay, so it's talking about LLAMs.
[00:15:51.740 --> 00:15:55.060]   You have GPT-3 here, Lambda Gopher.
[00:15:55.060 --> 00:15:57.740]   Okay, all sort of comparable LLAMs,
[00:15:57.740 --> 00:15:59.820]   comparable to some degree.
[00:15:59.820 --> 00:16:01.820]   So, okay, it doesn't talk about LLAMA,
[00:16:01.820 --> 00:16:04.100]   but at least we have LLAMs in there.
[00:16:04.100 --> 00:16:05.220]   That's good.
[00:16:05.220 --> 00:16:07.900]   Here, it's coming from the LLAMA2 paper.
[00:16:07.900 --> 00:16:11.420]   So, "These closed product LLAMs are heavily fine-tuned
[00:16:11.420 --> 00:16:13.660]   "to align with human preferences.
[00:16:13.660 --> 00:16:16.580]   "Greatly enhances their usability and safety."
[00:16:16.580 --> 00:16:19.260]   Okay, "In this work, we develop and release LLAMA2,"
[00:16:19.260 --> 00:16:20.700]   and then, okay.
[00:16:20.700 --> 00:16:23.780]   All right, so it's talking about LLAMA2.
[00:16:23.780 --> 00:16:27.780]   Here, we are talking about the original LLAMA model,
[00:16:27.780 --> 00:16:30.620]   okay, which I think is still relevant here.
[00:16:30.620 --> 00:16:31.940]   Okay, cool.
[00:16:31.940 --> 00:16:35.260]   And here, we have another paper.
[00:16:35.260 --> 00:16:38.860]   It's not specific to LLAMA1 or LLAMA2,
[00:16:38.860 --> 00:16:40.060]   and it is an older one.
[00:16:40.060 --> 00:16:42.420]   It's just talking about ML and NLP in general.
[00:16:42.420 --> 00:16:45.540]   Okay, and this one's talking, again, generally about LLAMs.
[00:16:45.540 --> 00:16:50.060]   So, we have sort of a mix of LLAMs in there, some LLAMA.
[00:16:50.060 --> 00:16:53.580]   So, I think we're kind of getting tighter
[00:16:53.580 --> 00:16:54.900]   into where we need to be,
[00:16:54.900 --> 00:16:57.100]   but for sure, it could be better.
[00:16:57.100 --> 00:16:59.540]   Now, we can see from the results here
[00:16:59.540 --> 00:17:01.940]   that we've broadened the scope of what we're searching for,
[00:17:01.940 --> 00:17:05.060]   which is, that's what we want to do with multi-query,
[00:17:05.060 --> 00:17:09.180]   but it still doesn't make a good retrieval system,
[00:17:09.180 --> 00:17:10.580]   at least by itself.
[00:17:10.580 --> 00:17:12.100]   Multi-query needs to be part
[00:17:12.100 --> 00:17:16.460]   of a larger retrieval pipeline because, yes,
[00:17:16.460 --> 00:17:18.300]   it broadens the scope of what we're searching for,
[00:17:18.300 --> 00:17:20.220]   but then we need to tighten up that scope,
[00:17:20.220 --> 00:17:22.940]   and we need to actually filter down
[00:17:22.940 --> 00:17:26.980]   so that we don't have so much irrelevant or noisy results
[00:17:26.980 --> 00:17:28.820]   within what we're returning.
[00:17:28.820 --> 00:17:31.340]   So, yes, we have that broader scope.
[00:17:31.340 --> 00:17:33.300]   We can probably tighten it up,
[00:17:33.300 --> 00:17:34.820]   especially in this use case
[00:17:34.820 --> 00:17:36.700]   where we're searching for a particular keyword,
[00:17:36.700 --> 00:17:39.660]   which is LLAMA2, by using something like hybrid search,
[00:17:39.660 --> 00:17:43.080]   and then following that retrieval step,
[00:17:43.080 --> 00:17:46.340]   returning, I don't know, more records,
[00:17:46.340 --> 00:17:48.860]   let's say, like, five records per query
[00:17:48.860 --> 00:17:50.740]   or 20 records per query,
[00:17:50.740 --> 00:17:53.820]   and we'll end up returning like 50 or so documents.
[00:17:53.820 --> 00:17:55.820]   Then what we'd want to do with that
[00:17:55.820 --> 00:17:57.900]   is look at the original query,
[00:17:57.900 --> 00:17:59.940]   put that into a re-ranking model
[00:17:59.940 --> 00:18:02.620]   alongside those, like, 50 documents,
[00:18:02.620 --> 00:18:04.100]   and re-ranking up to, like,
[00:18:04.100 --> 00:18:06.900]   the top three or top five documents,
[00:18:06.900 --> 00:18:08.940]   and then returning that to our LLM.
[00:18:08.940 --> 00:18:11.700]   And within that sort of pipeline,
[00:18:11.700 --> 00:18:13.800]   that's where something like multi-query
[00:18:13.800 --> 00:18:16.580]   can be really helpful in just helping us
[00:18:16.580 --> 00:18:18.880]   pull in a wider variety of results
[00:18:18.880 --> 00:18:20.540]   that can be useful for us.
[00:18:20.540 --> 00:18:23.300]   So, that's it for this video.
[00:18:23.300 --> 00:18:25.660]   I hope this has been useful and interesting.
[00:18:25.660 --> 00:18:27.500]   So, thank you very much for watching,
[00:18:27.500 --> 00:18:29.440]   and I will see you again in the next one.
[00:18:29.440 --> 00:18:30.280]   Bye.
[00:18:30.280 --> 00:18:32.860]   (gentle music)
[00:18:32.860 --> 00:18:35.440]   (gentle music)
[00:18:35.440 --> 00:18:38.020]   (gentle music)
[00:18:38.020 --> 00:18:40.600]   (gentle music)
[00:18:40.600 --> 00:18:43.180]   (gentle music)
[00:18:43.180 --> 00:18:45.240]   you

