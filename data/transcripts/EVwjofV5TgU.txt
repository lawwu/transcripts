
[00:00:00.560 --> 00:00:04.480]   Okay, today I'm speaking with David Deutsch. Now, this is a conversation that I've been
[00:00:04.480 --> 00:00:11.120]   eagerly wanting to have for years, so this is very exciting for me. So first, let's talk about AI.
[00:00:11.120 --> 00:00:17.280]   Can you briefly explain why you anticipate that AIs will be no more fundamentally intelligent
[00:00:17.280 --> 00:00:28.960]   than humans? I suppose you mean AGIs. Yes. And by fundamentally intelligent, I suppose you mean
[00:00:29.840 --> 00:00:40.080]   capable of all the same types of cognition as humans are in principle. Yes. So that would
[00:00:40.080 --> 00:00:48.960]   include, you know, doing science and doing art and in principle also falling in love and
[00:00:50.880 --> 00:01:01.200]   being good and being evil and all that. So the reason is twofold, and
[00:01:01.200 --> 00:01:12.240]   one half is about computation hardware and the other is about software. So if we take the hardware,
[00:01:14.720 --> 00:01:24.160]   we know that our brains are Turing complete bits of hardware and therefore can
[00:01:24.160 --> 00:01:34.800]   exhibit the functionality of running any computable function, program for any computable
[00:01:34.800 --> 00:01:43.440]   function. Now, when I say any, I don't really mean any, because you and I sitting here, you know,
[00:01:43.440 --> 00:01:47.280]   we're having conversation and we could say, you know, we could have any conversation. Well,
[00:01:47.280 --> 00:01:54.000]   we can assume that maybe in a hundred years time, we'll both be dead.
[00:01:54.000 --> 00:02:02.560]   And therefore the number of conversations we could have is strictly limited. And also
[00:02:02.560 --> 00:02:10.720]   some conversations depend on speed of computation. So, you know, if we're going to be
[00:02:10.720 --> 00:02:17.440]   solving the traveling salesman problem, then there are many traveling salesman problems that
[00:02:17.440 --> 00:02:25.600]   we wouldn't be able to solve in the age of the universe. So when I say any, what I mean is
[00:02:25.600 --> 00:02:32.880]   that we're not limited in the programs we can run apart from by speed and memory capacity.
[00:02:33.680 --> 00:02:41.760]   So all limitations on us, hardware limitations on us, boil down to speed and memory capacity.
[00:02:41.760 --> 00:02:51.600]   And both those can be augmented to the level of any other entity that is in the universe.
[00:02:51.600 --> 00:02:56.240]   Because, you know, if somebody builds a computer that can think faster than the brain,
[00:02:56.240 --> 00:03:03.280]   then we can use that very computer or that very technology to make our thinking go just as fast
[00:03:03.280 --> 00:03:14.080]   as that. So that's the hardware. As far as explanations go, can we reach the same kind
[00:03:14.080 --> 00:03:22.800]   of explanations as any other entity? Let's say, usually this is said not in terms of AGIs, but in
[00:03:22.800 --> 00:03:30.800]   terms of extraterrestrial intelligences, but also it's said about AGIs, you know, what if they are
[00:03:30.800 --> 00:03:38.880]   to us as we are to ants? And so, well, again, part of that is just hardware, which is easily fixable
[00:03:38.880 --> 00:03:50.720]   by adding more hardware. So let's forget about that. So really, the idea is, are there concepts
[00:03:50.720 --> 00:03:57.120]   that we are inherently incapable of comprehending? I think Martin Rees believes this.
[00:03:58.720 --> 00:04:05.360]   He thinks that, you know, we can comprehend quantum mechanics, apes can't,
[00:04:05.360 --> 00:04:14.240]   and maybe the extraterrestrials can comprehend something beyond quantum mechanics,
[00:04:14.240 --> 00:04:21.280]   which we can't comprehend. And no amount of brain add-ons with extra hardware can give us that,
[00:04:21.280 --> 00:04:34.000]   because they have the hardware that is adapted to having these concepts, which we haven't.
[00:04:34.000 --> 00:04:40.960]   The same kind of thing is said about maybe certain qualia, that maybe we can experience love,
[00:04:40.960 --> 00:04:47.760]   and an AGI couldn't experience love because it has to do with our hardware, not just memory and speed,
[00:04:47.760 --> 00:04:55.520]   but specialized hardware. And I think that falls victim to the same argument. The thing is this
[00:04:55.520 --> 00:05:04.640]   specialized hardware can't be anything except a computer. And if there's hardware that is needed
[00:05:04.640 --> 00:05:11.440]   for love, let's say that somebody is born without that hardware, then that hardware, that bit of the
[00:05:11.440 --> 00:05:17.840]   brain that does love or that does mathematical insight or whatever, it's just a bit of the brain
[00:05:17.840 --> 00:05:22.400]   and it's connected to the rest of the brain in the same way that the other part of the brain is
[00:05:22.400 --> 00:05:30.160]   connected to the rest of the brain, namely by neurons passing electrical signals and by chemicals
[00:05:30.160 --> 00:05:34.800]   whose concentrations are altered and so on. So therefore, an artificial device
[00:05:36.320 --> 00:05:46.560]   that computed which signals were to be sent and which chemicals were to be adjusted
[00:05:46.560 --> 00:05:51.680]   could do the same job, and it would be indistinguishable, and therefore a person
[00:05:51.680 --> 00:05:57.840]   augmented with one of those who couldn't feel love could feel love after that augmentation.
[00:05:57.840 --> 00:06:06.000]   So those are, and I think those two things are the only relevant ones. So that's why I think
[00:06:06.160 --> 00:06:11.280]   that AGIs and humans have the same range in the sense I've defined.
[00:06:11.280 --> 00:06:17.360]   - Okay, interesting. Okay, so I think the software question is more interesting
[00:06:17.360 --> 00:06:22.160]   than the hardware one immediately, but I do want to take issue with the idea that
[00:06:22.160 --> 00:06:27.280]   the memory and speed of human brains can be arbitrarily and easily expanded,
[00:06:27.280 --> 00:06:33.200]   but we can get into that later. We can just start with this question. Can all humans explain
[00:06:33.200 --> 00:06:38.240]   everything that even the smartest humans can explain, right? So if I took the village idiot
[00:06:38.240 --> 00:06:44.800]   and I asked him to create the theory of quantum computing, should I anticipate that if he wanted
[00:06:44.800 --> 00:06:49.520]   to, he could do this and just reframe a reference about 21 to 24% of Americans
[00:06:49.520 --> 00:06:55.920]   on the National Belt Literacy Survey, they fall in level one, which means that they can't even
[00:06:55.920 --> 00:06:59.520]   perform basic tasks like identifying the expiry date of a driver's license, for example,
[00:06:59.520 --> 00:07:07.600]   or totaling a bank deposit slip. So are these humans capable of explaining quantum computing
[00:07:07.600 --> 00:07:10.960]   or creating the Deutsch-Jozsa algorithm? And if they're not capable of doing this,
[00:07:10.960 --> 00:07:14.480]   doesn't that mean that the theory of universal explainers falls apart?
[00:07:14.480 --> 00:07:23.120]   - Well, there are people who, so these tasks that you're talking about are tasks that no ape could
[00:07:23.120 --> 00:07:29.440]   do. However, there are humans who are brain damaged to the extent that they can't even do
[00:07:29.440 --> 00:07:40.560]   the tasks that an ape can do. And there comes a point when installing the program that would be
[00:07:40.560 --> 00:07:46.320]   able to read a driver's license or whatever would require augmenting their hardware as well as their
[00:07:46.320 --> 00:07:58.320]   software. So if a person, we don't know enough about the brain yet, but if it's 24% of the
[00:07:58.320 --> 00:08:05.360]   population, then it's definitely not hardware. So I would say that for those people, it's
[00:08:05.360 --> 00:08:13.920]   definitely software. If it was hardware, then getting them to do this would be a matter of
[00:08:13.920 --> 00:08:21.280]   repairing the imperfect hardware. If it's software, it is not just a matter of them wanting to,
[00:08:21.280 --> 00:08:32.640]   or them wanting to be taught or whatever. It is a matter of whether the existing software is,
[00:08:32.640 --> 00:08:43.680]   what word can I use instead of wants to, is consistent with what they want to do.
[00:08:43.680 --> 00:08:52.240]   So they're conceptually ready to do that. For example, Brett Hall has often said that he would
[00:08:52.240 --> 00:09:00.000]   like to speak Mandarin Chinese. And so he wants to, but he will never be able to speak Mandarin
[00:09:00.000 --> 00:09:07.680]   Chinese because he's never going to want it enough to be able to go through the process
[00:09:09.280 --> 00:09:17.200]   of acquiring that program. But there is nothing about his hardware that prevents
[00:09:17.200 --> 00:09:24.880]   him learning Mandarin Chinese. And there's nothing about his software either, except
[00:09:24.880 --> 00:09:32.240]   that, well, what word can we use to say that he doesn't want to go through that process? I mean,
[00:09:32.240 --> 00:09:38.080]   he does want to learn it. He does want to learn it, but he doesn't want to go through the process
[00:09:38.080 --> 00:09:44.960]   of being programmed with that program. But if his circumstances changed, he might well want to.
[00:09:44.960 --> 00:09:54.480]   So for example, many of my relatives a couple of generations ago were forced to migrate to very
[00:09:54.480 --> 00:10:02.240]   alien places where they had to learn languages that they never thought they would ever speak
[00:10:02.240 --> 00:10:08.640]   and never wanted to speak. And yet very quickly, they did speak those languages. Again, was it
[00:10:08.640 --> 00:10:14.160]   because what they wanted changed? In the big picture, perhaps you could say what they wanted
[00:10:14.160 --> 00:10:27.040]   changed. So if your driving license blind people wanted to be educated to read driving licenses
[00:10:27.040 --> 00:10:33.200]   in the sense that my ancestors wanted to learn languages, then yes, they could learn that.
[00:10:33.200 --> 00:10:40.560]   There is a level of dysfunction below which they couldn't, and I think those are hardware
[00:10:40.560 --> 00:10:47.040]   limitations. On the borderline between those two, there's not that much difference. It's like,
[00:10:47.040 --> 00:10:55.520]   you know, that's like the question of could apes be programmed with a fully human intellect?
[00:10:56.480 --> 00:11:06.000]   I think the answer to that is yes, but although programming them would not require hardware,
[00:11:06.000 --> 00:11:14.720]   you know, surgery in the sense that repairing a defect, that it would be repairing a defect,
[00:11:14.720 --> 00:11:22.080]   but it would require intricate changes at the neuron level. And that's so that to transfer
[00:11:22.080 --> 00:11:29.440]   the program from a human mind into the ape's mind, I would guess that that is possible,
[00:11:29.440 --> 00:11:39.920]   because although the ape has far less memory space than humans do and also doesn't have certain
[00:11:39.920 --> 00:11:46.480]   specialized modules that humans have, neither of those things is a thing that we use to the full
[00:11:46.480 --> 00:11:53.200]   anyway. I mean, when I'm speaking to you now, there's a lot of knowledge in my brain that I'm
[00:11:53.200 --> 00:12:00.800]   not referring to at all, like, you know, the fact that I can play the piano or drive a car
[00:12:00.800 --> 00:12:06.880]   is not being used in this conversation. So I don't think the fact that we have such a large
[00:12:06.880 --> 00:12:12.880]   memory capacity would affect this project, although the project would be highly immoral,
[00:12:13.600 --> 00:12:25.120]   because you'd be intentionally creating a person inside a deficient brain hardware.
[00:12:25.120 --> 00:12:31.680]   So suppose it's hardware differences that distinguish, you know, different humans in
[00:12:31.680 --> 00:12:35.760]   terms of their intelligence, if it were just up to the people who are not even functionally
[00:12:35.760 --> 00:12:41.120]   literate, right? So these are, again, people... Wait, wait, wait. I said that it could only be
[00:12:41.120 --> 00:12:49.040]   hardware at the low level. Well, either at the level of brain defects, or at the level of using
[00:12:49.040 --> 00:12:55.120]   up the whole of our allocation of memory or speed or whatever. So apart from that, I don't think it
[00:12:55.120 --> 00:13:00.800]   can be hardware. By the way, is software analogous, or is hardware synonymous with
[00:13:00.800 --> 00:13:09.360]   genetic influences for you, or can software be genetic too? Software can be genetic too,
[00:13:09.360 --> 00:13:15.120]   though that doesn't mean it's immutable. It just means it's there at the beginning.
[00:13:15.120 --> 00:13:20.480]   Okay. The reason I suspect it's not software is because these people also happen to be the
[00:13:20.480 --> 00:13:24.320]   same people who... Let's suppose it was software and something that they chose to do or something
[00:13:24.320 --> 00:13:29.680]   they could change. It's mysterious to me why these people would also choose to accept jobs
[00:13:29.680 --> 00:13:34.880]   that have lower pay but are less cognitively demanding, or why they would choose to do worse
[00:13:34.880 --> 00:13:41.520]   on academic test or IQ test. So why they would choose to do exactly the sort of thing somebody
[00:13:41.520 --> 00:13:45.760]   who is less cognitively powerful would do. It seems the more parsimonious explanation there
[00:13:45.760 --> 00:13:52.240]   is just that they are cognitively less powerful. Not at all. Why would someone choose not to go
[00:13:52.240 --> 00:13:57.520]   to school, for instance, if they were given the choice, and not to have any lessons? Well,
[00:13:57.520 --> 00:14:02.080]   there are many reasons why they might choose that. Some of them good, some of them bad.
[00:14:02.800 --> 00:14:10.000]   And the people who, you know, calling some jobs cognitively demanding is already begging the
[00:14:10.000 --> 00:14:18.240]   question. Because you're just referring to a choice that people make, which I think is a
[00:14:18.240 --> 00:14:26.400]   software choice, as being by definition, forced on them by hardware. It's not cognitively deficient,
[00:14:26.400 --> 00:14:33.520]   it's just that they don't want to do it the same way. If there was a culture that required Brett
[00:14:33.520 --> 00:14:45.760]   Hall to be able to speak fluent Mandarin Chinese in order to do a wide range of tasks. And if he
[00:14:45.760 --> 00:14:51.600]   didn't know Mandarin Chinese, he'd be relegated to low level tasks, then he would be quote,
[00:14:51.600 --> 00:14:58.000]   quote, choosing the low level tasks rather than the quote, cognitively demanding task.
[00:14:58.000 --> 00:15:02.640]   But it's only culture that makes that cognitively demanding task
[00:15:02.640 --> 00:15:11.600]   that that assigns a hardware interpretation to the difficulty of doing that task.
[00:15:11.600 --> 00:15:15.680]   Right? I mean, it doesn't seem that arbitrary to say that the kind of jobs you could do sitting
[00:15:15.680 --> 00:15:20.960]   down on a laptop are require different cognitive require probably more cognition than the ones you
[00:15:20.960 --> 00:15:26.960]   can do in a construction site. And if it's not cognition that distinguishes, or if there's not
[00:15:26.960 --> 00:15:30.320]   something like intelligence or cognition or whatever you want to call it, that is a thing
[00:15:30.320 --> 00:15:35.680]   that is measured by both these literacy tests and by what you're doing at your job, then what is the
[00:15:35.680 --> 00:15:39.440]   explanation for why there's such a high correlation between people who are not functionally literate,
[00:15:39.440 --> 00:15:42.880]   and I guess an anti-correlation between people who are not functionally literate,
[00:15:42.880 --> 00:15:46.240]   and people who are doing like, let's say programmers, right? Like I guarantee you,
[00:15:46.240 --> 00:15:50.480]   people working at Apple, all of them are above level one on this literacy survey.
[00:15:50.880 --> 00:15:54.720]   Um, why, why do they just happen to make the same choices? Why is that their correlation?
[00:15:54.720 --> 00:16:06.320]   Well, uh, there are correlations everywhere and, and the culture is built in order to use in order
[00:16:06.320 --> 00:16:15.360]   to make certain, uh, abilities, um, make use of certain abilities that people have. So if you're,
[00:16:15.360 --> 00:16:23.520]   if you're setting up a company that is going to employ 10,000, um, uh, employees, then it's best
[00:16:23.520 --> 00:16:29.040]   to make the way that the company works. You know, it's best, for example, to make the signs above
[00:16:29.040 --> 00:16:35.440]   the doors or the signs on the doors, or the numbers on the dials, all be ones that people
[00:16:35.440 --> 00:16:44.400]   in that culture who are highly educated can read. You could in principle, make each label on each
[00:16:44.400 --> 00:16:48.640]   door, a different language. I don't know, you know, there are thousands of human languages.
[00:16:48.640 --> 00:16:54.880]   Let's say there are 5,000 languages and 5,000 doors in the company. You could given the same
[00:16:54.880 --> 00:16:59.600]   meaning, make them all different languages. The reason that they're all the same language and
[00:16:59.600 --> 00:17:05.520]   what's more, not just any old language, it's a language that many educated people know fluently.
[00:17:05.520 --> 00:17:13.680]   That's why. And then you can misinterpret that as saying, oh, there is something, uh, um,
[00:17:13.680 --> 00:17:20.960]   there is some hardware reason why everybody speaks the same language. Well, no, there isn't,
[00:17:20.960 --> 00:17:26.960]   it's a cultural reason. Okay. So if the culture was different somehow, maybe if there was some
[00:17:26.960 --> 00:17:34.880]   other way of communicating ideas, um, is it, do you think that the people who are currently
[00:17:34.880 --> 00:17:39.200]   designated as not functionally literate could be in a position to learn about quantum computing,
[00:17:39.200 --> 00:17:44.720]   for example, if they, and if they made the right choices, um, or not the right choices,
[00:17:44.720 --> 00:17:48.640]   but, uh, the choices that could lead to them understanding quantum computing. Well,
[00:17:48.640 --> 00:17:56.880]   so I don't want to evade the question. The answer is yes. But the way you, the way you put it is,
[00:17:56.880 --> 00:18:04.480]   is, uh, again, uh, rather begs the question. It's not only language that is like this. It's all
[00:18:04.480 --> 00:18:12.400]   knowledge. So, so, uh, just learning. So if someone doesn't speak English,
[00:18:12.400 --> 00:18:17.760]   quantum computing is, is a, is a field in which English is the standard language,
[00:18:17.760 --> 00:18:26.480]   um, used to be German. Now it's English. Now someone who doesn't know English is at a
[00:18:26.480 --> 00:18:34.880]   disadvantage learning about quantum computers, but not only because of their deficiency in language,
[00:18:34.880 --> 00:18:44.240]   if they come from a culture in which the culture of physics and of mathematics and, and of logic
[00:18:44.240 --> 00:18:50.480]   and so on is, is, is equivalent. And only the language is different. Then if they just learn
[00:18:50.480 --> 00:18:55.440]   the language, they, they will find it as easy as anyone else. But if a whole load of things are
[00:18:55.440 --> 00:19:03.200]   different, if a person doesn't think in terms of, for example, logic, but thinks in terms of pride
[00:19:03.200 --> 00:19:11.760]   and manliness and, and fear and, and, um, uh, you know, all sorts of, um,
[00:19:11.760 --> 00:19:20.560]   concepts that govern that fill the lives of, uh, let's say, um, uh,
[00:19:22.640 --> 00:19:30.480]   prehistoric people or pre-enlightenment people, then to be able to understand quantum computers,
[00:19:30.480 --> 00:19:35.760]   they would have to learn a lot more than just the language of the civilization. They'd have to learn
[00:19:35.760 --> 00:19:42.480]   all of other, well, not all, but a range of other features of the civilization.
[00:19:42.480 --> 00:19:51.840]   And on that basis, the people who can't read driving licenses are similarly in a different
[00:19:51.840 --> 00:19:59.040]   culture, which they would also have to learn if they are to increase their IQ, i.e. their
[00:19:59.040 --> 00:20:04.800]   ability to function at a high level in intellectual culture in our civilization.
[00:20:04.800 --> 00:20:10.640]   Okay. They would be able to. Okay. So if it's those kinds of differences,
[00:20:10.640 --> 00:20:14.800]   then how do you explain the fact that identical twins separated at birth and adopted by different
[00:20:14.800 --> 00:20:21.920]   families, um, they tend to have, uh, uh, uh, you know, I, the most of the variance, um, that does
[00:20:21.920 --> 00:20:26.000]   exist between humans in terms of IQ doesn't exist between, uh, identical twins. In fact, the
[00:20:26.000 --> 00:20:30.480]   correlation is a 0.8, which is the correlation that you, you would have when you took the test
[00:20:30.480 --> 00:20:34.000]   on different days, like depending on how well good a day you were having. Um, and these are,
[00:20:34.000 --> 00:20:37.680]   you know, people who are adopted by families who have different cultures were often in different
[00:20:37.680 --> 00:20:43.840]   countries. Um, yet in fact, uh, a hardware theory explains very well why they would have similar,
[00:20:43.840 --> 00:20:48.000]   um, scores on IQ tests, sort of themselves correlated with literacy and job performance
[00:20:48.000 --> 00:20:51.760]   and so on. Whereas I don't know how software would explain why being adopted by different families.
[00:20:51.760 --> 00:21:00.800]   The hardware theory explains it in the sense that it might be hardware might be true. So, uh, it
[00:21:00.800 --> 00:21:06.560]   doesn't, it can, it doesn't have an explanation beyond that and nor does the software theory.
[00:21:07.520 --> 00:21:13.840]   Um, sorry, go on. Uh, I mean, so there are actually like differences at the level of
[00:21:13.840 --> 00:21:18.720]   brain that are correlated with IQ, right? So you, the actual skull size is like a 0.3 correlation
[00:21:18.720 --> 00:21:23.280]   with IQ. There's a few more like this. They don't explain the entire, um, the entire variance in
[00:21:23.280 --> 00:21:27.040]   human intelligence or the entire genetic variance in human intelligence, but we do have, we have
[00:21:27.040 --> 00:21:31.520]   identified a few actual Harvard differences that correlate with IQ. Well, suppose, suppose on the
[00:21:31.520 --> 00:21:38.000]   contrary that, that suppose that the results of these experiments had been different. Uh,
[00:21:38.000 --> 00:21:49.360]   suppose that the result was that, um, uh, um, people who, uh, are brought up in the same family
[00:21:49.360 --> 00:22:00.480]   and differ only in, um, the amount of hair they have or in the amount in their appearance in any
[00:22:00.480 --> 00:22:09.760]   other way that, that, um, none of those differences make any difference to their IQ. Only that only
[00:22:09.760 --> 00:22:15.040]   who their parents were makes a difference. Now, wouldn't that be surprising? It wouldn't it be
[00:22:15.040 --> 00:22:20.400]   surprising that there's nothing else correlated with IQ other than who your parents are?
[00:22:20.400 --> 00:22:30.080]   Yes. Um, um, now how much correlation should we expect? There are correlations
[00:22:30.080 --> 00:22:36.560]   everywhere. You know, there are these things on the internet, which, uh, joke, uh, memes or
[00:22:36.560 --> 00:22:42.400]   whatever you call it. Uh, but, but they make a serious point where they correlate, uh, things
[00:22:42.400 --> 00:22:50.880]   like how many, uh, how many, uh, uh, adventure movies have been made in a given year correlated
[00:22:50.880 --> 00:22:56.400]   with how much the GNP per capita, and that's a bad example because there's an obvious relation,
[00:22:56.400 --> 00:23:02.000]   but you know what I mean? It's, it's, uh, the number of films made by a particular actor, uh,
[00:23:02.000 --> 00:23:13.200]   against the, uh, number of outbreaks of bird flu or that. And, um, part of being surprised by
[00:23:13.200 --> 00:23:22.880]   randomness is the fact that correlations are everywhere. It's not just that correlation
[00:23:22.880 --> 00:23:29.760]   isn't causation. It's that correlations are everywhere. It's not a rare event to get a
[00:23:29.760 --> 00:23:36.400]   correlation between two things. And the more things you ask about, the more you are going
[00:23:36.400 --> 00:23:50.640]   to get correlations. So, uh, the, the, the, it's, it's not, what is surprising is that the, um,
[00:23:50.640 --> 00:24:02.880]   things that are correlated are, uh, are things that you expect to be correlated and measure.
[00:24:02.880 --> 00:24:09.920]   For example, they, when they, when they, um, uh, do these twin studies and measure the IQ,
[00:24:09.920 --> 00:24:18.320]   they control for certain things. Uh, and like you said, uh, identical twins reared together,
[00:24:18.320 --> 00:24:27.120]   they've got to be reared together or apart or apart. Yeah. So, uh, but, but there there's
[00:24:27.120 --> 00:24:34.320]   infinitely more things that they don't control for. So it, it could be that the real determinant
[00:24:34.320 --> 00:24:42.960]   of, uh, IQ is for example, how well a child is treated between the ages of three and a half and
[00:24:42.960 --> 00:24:48.400]   four and a half, where well is defined by something that we don't know yet, but, you know, something
[00:24:48.400 --> 00:24:54.320]   like that, then you would expect that thing, which we don't know about, and nobody has bothered to,
[00:24:54.320 --> 00:25:02.000]   um, uh, control for in these experiments, we would expect that thing to be correlated with IQ.
[00:25:02.000 --> 00:25:08.240]   But unfortunately, that thing is also correlated with whether something, someone's an identical
[00:25:08.240 --> 00:25:15.280]   twin or not. So it's, it's not the identical twinness that is causing the similarity. It's
[00:25:15.280 --> 00:25:20.560]   this other thing, right? This say an aspect of appearance or something. And if you were to
[00:25:21.760 --> 00:25:26.480]   surgically change a person, uh, uh, with a view, if you knew what this thing was and
[00:25:26.480 --> 00:25:31.360]   surgically changed the person, you would be able to have the same effect as making an identical
[00:25:31.360 --> 00:25:36.720]   twin would have. Right. But I mean, as you say in science or to explain any phenomenon,
[00:25:36.720 --> 00:25:40.160]   there's an infinite amount of possible explanations, right? You got to pick the best one.
[00:25:40.160 --> 00:25:45.440]   So it could be that there are some unknown trait, which is so obvious to adopted parents,
[00:25:45.440 --> 00:25:49.440]   different adoptive parents that they can use it as a basis for, um, discrimination or for
[00:25:49.440 --> 00:25:54.240]   different treatment. But that is, I mean, I would assume they don't know what it is,
[00:25:54.240 --> 00:25:59.200]   but then aren't they using it as a basis to treat kids differently at the age of three,
[00:25:59.200 --> 00:26:03.440]   for example, not by, not by consciously identifying it. It's, it's, it's like,
[00:26:03.440 --> 00:26:09.120]   it would be something like getting the idea that this child is really smart, but I'm just trying
[00:26:09.120 --> 00:26:13.840]   to show you that it could be something that the parents are not aware of. If you ask parents to
[00:26:13.840 --> 00:26:19.760]   list the traits in their children, that caused them to behave differently towards their children,
[00:26:19.760 --> 00:26:25.600]   they might list like 10 traits, but then there are another thousand traits that they're not aware of,
[00:26:25.600 --> 00:26:31.760]   which also affect their behavior. So we'd first need an explanation for what this
[00:26:31.760 --> 00:26:36.800]   trait is that researchers have not been able to identify it, but it's so, um, obvious that
[00:26:36.800 --> 00:26:42.320]   even unconsciously parents are able to reliably use it as a way to, um, have to be obvious at all.
[00:26:42.320 --> 00:26:49.840]   Cause, uh, parents, um, uh, have a, have a huge amount of information about their children,
[00:26:49.840 --> 00:26:58.720]   which they are, uh, uh, processing in, in their minds. And, uh, most of it, they don't know what
[00:26:58.720 --> 00:27:04.800]   it is. Okay. All right. Um, okay. So I guess, uh, uh, uh, let's leave this topic aside for now.
[00:27:04.800 --> 00:27:10.400]   And then let me, let me, uh, bring us to animals. So if it, if creativity is something that doesn't
[00:27:10.400 --> 00:27:16.160]   exist increments, it's, uh, or, you know, the capacity to create explanations, um, you can just
[00:27:16.160 --> 00:27:20.240]   use a simple example, go on YouTube and look up cat opening a door, right? So you'll see, for
[00:27:20.240 --> 00:27:26.080]   example, um, a cat, uh, develops a theory that applying torque to this handle, to this metal
[00:27:26.080 --> 00:27:30.800]   thing will open a door. Now it hasn't. And then what it'll do is it'll climb onto a countertop
[00:27:30.800 --> 00:27:34.880]   and it'll jump on top of that door handle. It hasn't seen another cat do it. It hasn't seen
[00:27:34.880 --> 00:27:38.560]   another human, like get on a countertop and try to open the door that way. But it conjectures that
[00:27:38.560 --> 00:27:43.680]   this is a way given it's, um, given it's morphology that it can access the door. Um, and then, you
[00:27:43.680 --> 00:27:47.840]   know, so that that's this theory. And then the experiment is, will the door open? Um, this seems
[00:27:47.840 --> 00:27:52.640]   like a classic, uh, this seems like a classic cycle of conjecture and reputation. Um, is this
[00:27:52.640 --> 00:27:57.920]   compatible with the cat not being at least having some bounded form of creativity? I think it's
[00:27:57.920 --> 00:28:08.000]   perfectly compatible. Uh, so, uh, animals are amazing things and, uh, uh, instinctive animal
[00:28:08.000 --> 00:28:18.400]   knowledge is, is, uh, designed to make animals easily capable of, um, uh, uh, thriving in
[00:28:18.400 --> 00:28:26.080]   environments that they've never seen before. Uh, in fact, uh, if you don't go down to the level
[00:28:26.080 --> 00:28:31.920]   of detail in animals, I've never seen the environment before. I mean, maybe a goldfish in
[00:28:31.920 --> 00:28:38.880]   a goldfish bowl, uh, might have, but, uh, when, uh, when, uh, uh, a wolf runs through the forest,
[00:28:38.880 --> 00:28:46.480]   uh, it sees a pattern of trees that it has never seen before. And it has to, um, uh, create
[00:28:46.480 --> 00:28:55.520]   strategies for, for avoiding each tree. And not only that for actually catching the rabbit that
[00:28:55.520 --> 00:29:02.560]   it's running after as well in a way that has never been done before. So, um, the way to understand
[00:29:02.560 --> 00:29:08.640]   this, I think now, now this is because of a vast amount of knowledge that is in the wolf's genes.
[00:29:08.640 --> 00:29:14.480]   What kind of knowledge is this? Well, it's not the kind of knowledge that says first turn left,
[00:29:14.480 --> 00:29:19.760]   then turn right, then jump, uh, and so on. It's, it's not that kind of instruction. It's
[00:29:19.760 --> 00:29:29.280]   an instruction that takes input from the outside and, and then generates, uh, a behavior that is
[00:29:29.280 --> 00:29:35.680]   relevant to that input. Um, it, it doesn't involve creativity, but it involves a degree of
[00:29:35.680 --> 00:29:41.440]   sophistication in the program that, that, uh, human robotics has not, has not yet reached anywhere
[00:29:41.440 --> 00:29:48.160]   near that. And by the way, then when it sees a wolf of the opposite sex, it may decide to leave
[00:29:48.160 --> 00:29:55.920]   the rabbit and go and have sex instead. And, uh, a program for a robot to, to, to locate another
[00:29:55.920 --> 00:30:02.320]   robot of the right species and then have sex with it is again, uh, I think beyond present day
[00:30:02.320 --> 00:30:12.400]   robotics, but, uh, it, it will be done. Um, and it does not, it clearly does not require creativity
[00:30:12.400 --> 00:30:20.160]   because it can, that same program will lead the next wolf to do the same thing in the same
[00:30:20.160 --> 00:30:24.560]   circumstances. It's the fact that the circumstances are ones that it's never seen before,
[00:30:24.560 --> 00:30:33.440]   and it can still function is a, uh, testimony to the incredible sophistication of that program,
[00:30:34.080 --> 00:30:39.200]   but it has nothing to do with creativity. So, um, uh,
[00:30:39.200 --> 00:30:49.520]   humans, humans do, do tasks that require much, much less
[00:30:49.520 --> 00:30:57.040]   programming sophistication than that, such as sitting around a campfire,
[00:30:57.040 --> 00:31:05.280]   telling each other a scary story about a wolf that, that almost ate them. Now animals can do
[00:31:05.280 --> 00:31:11.440]   the wolf running away thing. They can, they can, they can enact a story that's more complicated,
[00:31:11.440 --> 00:31:16.240]   even than the one that human is telling, but they can't tell a story. They don't tell a story.
[00:31:16.240 --> 00:31:24.320]   Telling a story is, is a sort of typical creative activity. It's the same kind of activity as
[00:31:24.960 --> 00:31:33.040]   forming an explanation. So I don't think it's at all surprising that cats can, uh,
[00:31:33.040 --> 00:31:41.600]   jump on, on handles because it's the same. Uh, I can easily imagine that the same amazingly
[00:31:41.600 --> 00:31:46.800]   sophisticated program that lets it jump on a branch so that the branch will get out of its
[00:31:46.800 --> 00:31:52.960]   way in some sense will also function in this new environment that it's never seen before,
[00:31:52.960 --> 00:31:55.120]   but there are all sorts of other things that it can't do.
[00:31:55.120 --> 00:32:00.720]   Oh, that's definitely true. Which was my point is that it has a bounded form of creativity and
[00:32:00.720 --> 00:32:05.520]   if bounded forms of creativity can, can exist, then humans could be in one such. But so I'm
[00:32:05.520 --> 00:32:10.480]   having a hard time imagining the ancestral circumstance in which a cat couldn't, uh,
[00:32:10.480 --> 00:32:14.880]   would, uh, have genetic gained a genetic knowledge that jumping on a metal rod
[00:32:14.880 --> 00:32:19.680]   would get a wooden plank to open and give it access to the, you know, the other side.
[00:32:21.840 --> 00:32:30.000]   Well, I thought I just gave an example. I mean, if we don't know, at least I don't know,
[00:32:30.000 --> 00:32:37.280]   uh, what kind of environment the ancestor of the domestic cat lived in. Uh, but if it was,
[00:32:37.280 --> 00:32:45.040]   for example, if, if it contained undergrowth, then, uh, dealing with undergrowth requires
[00:32:45.040 --> 00:32:50.560]   some very sophisticated programs. Otherwise you will just get stuck, uh, somewhere and starve to
[00:32:50.560 --> 00:33:00.320]   death. Now, I think a dog, um, if it gets stuck in a bush, it has no program to get out other than
[00:33:00.320 --> 00:33:07.600]   just shaking itself about until it gets out. It doesn't have a concept of doing something which
[00:33:07.600 --> 00:33:15.280]   temporarily makes matters worse and then allows you to get out. I think dogs can't do that, but
[00:33:15.280 --> 00:33:19.600]   it's just, it's not because that's a particularly complicated thing. It's just that it's programming
[00:33:19.600 --> 00:33:24.560]   just doesn't have that. But an animal's programming easily could have that if it,
[00:33:24.560 --> 00:33:27.200]   if it lived in an environment in which that happened a lot.
[00:33:27.200 --> 00:33:35.680]   Is your, uh, theory of AI compatible with AIs that have narrow objective functions,
[00:33:35.680 --> 00:33:42.240]   but functions which, uh, if fulfilled would give the creator of the AI a lot of power. So if I,
[00:33:42.240 --> 00:33:47.360]   for example, I wrote a deep learning program, I traded over financial history and I asked it,
[00:33:47.360 --> 00:33:51.600]   make me a trillion dollars on the stock market. Um, do you think that this would be impossible?
[00:33:51.600 --> 00:33:55.440]   Uh, or, and if you think this would be possible, then it seems like I do, it's not an AGI,
[00:33:55.440 --> 00:33:58.560]   but it seems like a very powerful AI. Right. So it seems like AI is getting somewhere.
[00:33:58.560 --> 00:34:05.120]   Yeah. Well, if you want to be powerful, uh, you, you might do better inventing a weapon or
[00:34:05.120 --> 00:34:12.400]   something. Um, but, but, or, or a better mousetrap is even better because it's, it's non-violent. So
[00:34:13.200 --> 00:34:20.160]   you can invent a paperclip to, to use an example because it's often used in this context. You can
[00:34:20.160 --> 00:34:24.960]   invent if paperclips hadn't been invented, you can invent a paperclip and make a fortune.
[00:34:24.960 --> 00:34:32.160]   And that's an idea, which is, but it's, it's not an AI because it's not the paper clip that's going
[00:34:32.160 --> 00:34:38.880]   out there. It's, it's really your idea in first place that, that has caused the whole value of
[00:34:38.880 --> 00:34:49.680]   the paperclip. And similarly, if you invent a dumb arbitrage, uh, machine, which seeks out
[00:34:49.680 --> 00:34:55.680]   complicated trades to make, which, which, which are more complicated than anyone else is trying
[00:34:55.680 --> 00:35:01.200]   to do. And that makes you a fortune. Well, the thing that made you a fortune was not the arbitrage
[00:35:01.200 --> 00:35:08.240]   machine. It was your idea for how, how to search for arbitrage opportunities that no one else sees.
[00:35:08.960 --> 00:35:14.880]   Right. That's what was valuable. And that's the usual way of making money in the economy.
[00:35:14.880 --> 00:35:22.560]   You have an idea and then you implement it. Right. I, that, that was an AI is beside the point. It
[00:35:22.560 --> 00:35:28.240]   could have been a paperclip. But the thing is, um, so the models that are used nowadays are, um,
[00:35:28.240 --> 00:35:31.760]   are not expert systems like the chess engines of the nineties. They're, you know, something like
[00:35:31.760 --> 00:35:37.040]   AlphaZero or AlphaGo. This is just like a, almost a blank neural net. Um, and that they were able to
[00:35:37.040 --> 00:35:42.800]   help, you know, let it win go or, um, so if such a, if such a neural network that was kind of blank,
[00:35:42.800 --> 00:35:46.640]   and if you just arbitrarily throw a financial history at it, wouldn't it be fair to say that
[00:35:46.640 --> 00:35:51.040]   the AI actually figured out what the right trades were, even though it's not a general intelligence?
[00:35:51.040 --> 00:36:00.720]   Well, I think it, it, it's possible in chess and, but not in the economy because the value in the
[00:36:00.720 --> 00:36:08.160]   economy is being created by creativity. And, uh, most, you know, arbitrage is one thing that can
[00:36:08.160 --> 00:36:14.080]   sort of skim value off the top by taking opportunities that were too expensive for
[00:36:14.080 --> 00:36:18.880]   other people to take. So you can, you know, you can make money, you make a lot of money that way.
[00:36:18.880 --> 00:36:24.320]   If you, if you know, if you have a good idea about how to do it, but most of the value in the economy
[00:36:24.320 --> 00:36:32.800]   is created by the creation of knowledge. Somebody has the idea that a smartphone would be good to
[00:36:32.800 --> 00:36:40.880]   have, even though most people think that that's not going to work. And that idea cannot be
[00:36:40.880 --> 00:36:47.920]   anticipated by anything less than an AGI and AGI could have that idea, but no AI could.
[00:36:48.960 --> 00:36:53.760]   Okay. Um, so I, there's definitely other topics I want to get to. So let's talk about virtual
[00:36:53.760 --> 00:36:58.320]   reality. So in the fabric of reality, um, you discuss the possibility that virtual reality
[00:36:58.320 --> 00:37:02.400]   generators could plug in directly into our nervous system and give us sense data that way.
[00:37:02.400 --> 00:37:06.320]   Now, uh, as you might know, many meditators, you know, people like Sam Harris speak of,
[00:37:06.320 --> 00:37:12.080]   um, both thoughts and senses as intrusions into consciousness that have a sort of similar,
[00:37:12.080 --> 00:37:16.880]   they can be welcome, welcome intrusions, but they are both things that come into consciousness.
[00:37:16.880 --> 00:37:23.600]   So, um, do you think, uh, that a virtual reality generator could also place thoughts as well as
[00:37:23.600 --> 00:37:29.600]   sense data into the mind? Uh, yes, but that's only because I think that this model, uh, is,
[00:37:29.600 --> 00:37:37.200]   is wrong. It's, it's basically the, the Cartesian theater as Daniel Dennett puts it, uh, with the
[00:37:37.200 --> 00:37:44.880]   stage cleared of all the characters. So you, that, that's the, that's, that's conscious pure
[00:37:44.880 --> 00:37:51.200]   consciousness without content as, as Sam Harris envisages it. But I think that all that's happening
[00:37:51.200 --> 00:37:58.880]   there is that you are conscious of this theater, uh, and you're, you're, uh, envisaging it as
[00:37:58.880 --> 00:38:02.960]   having certain properties, which by the way, it doesn't have, but that doesn't matter.
[00:38:02.960 --> 00:38:09.360]   We can imagine lots of things that don't happen. Um, you know, in fact, you know, that's in a way
[00:38:09.360 --> 00:38:19.440]   characterizes what we do all the time. Um, uh, so, uh, one can interpret one's thoughts about
[00:38:19.440 --> 00:38:28.480]   this empty stage as being thoughts about nothing. One can, uh, interpret the actual hardware of the
[00:38:28.480 --> 00:38:36.000]   stage that one is imagining as being pure, conscious content, less consciousness, but it's
[00:38:36.000 --> 00:38:43.120]   not, it has the content of a stage or a, a space or, you know, however you want to envisage it.
[00:38:43.120 --> 00:38:49.440]   Okay. And then let's talk about the Turing principle. So this is a term you coined,
[00:38:49.440 --> 00:38:54.080]   um, it's otherwise been called the Church Turing Deutsch principle. Um, would this principle
[00:38:54.080 --> 00:38:58.320]   imply that you could, so by the way, it states that any, a universal computer can simulate any
[00:38:58.320 --> 00:39:04.080]   physical process. Would this principle imply that you could simulate the whole of the universe,
[00:39:04.080 --> 00:39:07.760]   for example, in a compact, efficient computer that was smaller than the universe itself,
[00:39:07.760 --> 00:39:11.840]   or is it constrained to physical processes of a certain size?
[00:39:11.840 --> 00:39:20.640]   Uh, again, uh, no, it couldn't, um, it couldn't simulate the whole universe. Uh, that would be an
[00:39:20.640 --> 00:39:28.640]   example of a task where it was, uh, computationally able to do it, but it wouldn't have enough memory
[00:39:28.640 --> 00:39:35.680]   or time. So the more memory and time you gave it, the more closely it could simulate the whole
[00:39:35.680 --> 00:39:40.960]   universe, but it couldn't ever simulate the whole universe or anything near the whole
[00:39:40.960 --> 00:39:50.000]   universe, probably because it, um, uh, well, if you wanted to simulate itself as well,
[00:39:50.000 --> 00:39:54.160]   then there are logical reasons why there are limits to that. But even if you wanted it to
[00:39:54.160 --> 00:40:00.000]   simulate the whole universe apart from itself, just the sheer size of the universe, uh, makes
[00:40:00.000 --> 00:40:11.120]   that, that, uh, impossible, even if we discovered ways of encoding information extremely densely,
[00:40:11.120 --> 00:40:16.960]   like some people have said, maybe quantum gravity would allow, uh, you know, totally amazing, um,
[00:40:18.720 --> 00:40:24.480]   density of information. It still couldn't, uh, simulate the universe because that would mean
[00:40:24.480 --> 00:40:30.720]   because of the universality of the laws of physics, that would mean the rest of the universe also was
[00:40:30.720 --> 00:40:34.800]   that complex because quantum gravity applies to the whole, you know, the rest of the universe as
[00:40:34.800 --> 00:40:44.320]   well. So, so, but, but I think it's significant being limited by the available time and memory
[00:40:45.520 --> 00:40:53.520]   to, to separate that from being limited by, uh, computational capacity, because it's only when
[00:40:53.520 --> 00:41:01.200]   you separate those that you realize what computational universality is. And, and I
[00:41:01.200 --> 00:41:09.360]   think that's universality, like Turing or quantum universality is the most important thing, uh, in
[00:41:09.360 --> 00:41:15.040]   the theory of computation because, uh, computation doesn't even make sense unless you have a concept
[00:41:15.040 --> 00:41:21.920]   of a, of a universal computer. Mm-hmm. Mm-hmm. Um, what could falsify your theory that, uh,
[00:41:21.920 --> 00:41:26.000]   all interesting problems are soluble? So I asked this because, um, as I'm sure, you know, there
[00:41:26.000 --> 00:41:30.800]   are people who have tried offering explanations for why certain problems or questions like,
[00:41:30.800 --> 00:41:34.240]   why is there something rather than nothing, or how could mere physical interactions explain
[00:41:34.240 --> 00:41:39.040]   consciousness? They've offered explanations for why these problems are in principle insoluble.
[00:41:39.040 --> 00:41:43.680]   Now I'm not convinced they're right, but do you have a strong reason for in principle believing
[00:41:43.680 --> 00:41:51.680]   that they're wrong? Uh, no. Um, so these, this is a philosophical theory and could not be proved,
[00:41:51.680 --> 00:42:00.160]   uh, wrong by experiment. However, uh, I think I have, um, a good argument for why they aren't,
[00:42:00.160 --> 00:42:11.280]   namely that, uh, each individual case of this is, is a bad explanation. So let, let's, uh, say that,
[00:42:11.280 --> 00:42:19.680]   that, uh, some, some people say, for example, that, uh, simulating a human brain is, is impossible.
[00:42:19.680 --> 00:42:24.880]   Now I can't prove that it's possible. Nobody can prove that it's possible until they actually do
[00:42:24.880 --> 00:42:32.960]   it, or unless they have a design for it, which they prove will work. So pending that there is,
[00:42:32.960 --> 00:42:40.160]   there is no way of proving that, that, uh, it's not true that this is a fundamental limitation,
[00:42:40.800 --> 00:42:47.840]   but the trouble is with that idea that it is a fundamental limitation, that the trouble with that
[00:42:47.840 --> 00:42:55.760]   is that it could be applied to anything. For example, um, it could be applied to the theory
[00:42:55.760 --> 00:43:04.560]   that you have recently, just a minute ago, been replaced by a humanoid robot, uh, which,
[00:43:04.560 --> 00:43:08.640]   which has got, is going to say for the next few minutes, just a prearranged set of things,
[00:43:08.640 --> 00:43:11.440]   and you're no longer a person. I can't believe you figured it out.
[00:43:11.440 --> 00:43:21.440]   Yeah. Well, that's the first thing you'd say. So there is no way to, uh, refute that by experiment
[00:43:21.440 --> 00:43:28.640]   short of actually doing it short of actually talking to you and, and so on. So it's the same
[00:43:28.640 --> 00:43:35.040]   with all these other things in order for it to make sense, to have a theory that something is
[00:43:35.040 --> 00:43:41.840]   impossible, you have to have an explanation for why it is impossible. So we know that, for example,
[00:43:41.840 --> 00:43:50.400]   almost all mathematical propositions are undecidable. So that's not because somebody
[00:43:50.400 --> 00:43:55.360]   has said, oh, maybe, maybe we can't decide everything because, uh, thinking we could
[00:43:55.360 --> 00:44:02.320]   decide everything is hubris. That's not an argument. You need an, an actual functional
[00:44:02.320 --> 00:44:10.320]   argument to prove that, that that is so. And then, uh, at being a functional argument in, in,
[00:44:10.320 --> 00:44:17.040]   in which the steps of the argument make sense and relate to other things and so on, you can then say,
[00:44:17.040 --> 00:44:22.400]   well, what does this actually mean? Does this mean that maybe, uh, we can never understand the laws
[00:44:22.400 --> 00:44:29.600]   of physics? Uh, well, it doesn't because if the laws of physics included an undecidable function,
[00:44:29.600 --> 00:44:34.320]   then we would simply write, you know, F of X and F of X is an undecidable function.
[00:44:34.320 --> 00:44:40.320]   We couldn't evaluate F of X. It would limit our ability to make predictions,
[00:44:40.320 --> 00:44:45.280]   but then lots of our ability to make predictions is totally limited anyway,
[00:44:45.280 --> 00:44:53.280]   but it would not affect our ability to understand the properties of the function F and therefore the
[00:44:53.280 --> 00:44:59.920]   properties of the physical world. Okay. Is a system of government like America's, which has
[00:44:59.920 --> 00:45:03.760]   distributed powers and checks and balances, is that incompatible with proper criteria? So the
[00:45:03.760 --> 00:45:08.480]   reason I ask is the last administration had a theory that if you build a wall, there'll be
[00:45:08.480 --> 00:45:13.600]   positive consequences. Um, and, uh, you know, that there could have been tested and then the
[00:45:13.600 --> 00:45:17.120]   person could have been evaluated and whether that theory succeeded, but because our system
[00:45:17.120 --> 00:45:21.120]   of government has distributed powers, you know, Congress opposed, uh, the testing of that theory.
[00:45:21.120 --> 00:45:24.880]   And so it was never tested. Um, so if our, the American government wanted to fulfill
[00:45:24.880 --> 00:45:28.560]   Popper's criterion, would we need to give the president more power, for example?
[00:45:28.560 --> 00:45:35.280]   Um, it's not as simple as that. So I agree that this is, this is a big defect in the American
[00:45:35.280 --> 00:45:40.960]   system of government. No country has a system of government that perfectly fulfills Popper's
[00:45:40.960 --> 00:45:48.240]   criterion, uh, criterion. Um, we can always improve. I think the British one is actually
[00:45:48.240 --> 00:45:56.480]   the best, uh, in, in the world. And it's far from far from optimal. Making a single change
[00:45:56.480 --> 00:46:05.600]   like that is not going to be the answer. The, the constitution of a polity is a very complicated
[00:46:05.600 --> 00:46:16.320]   thing. Much of which is inexplicit. So, um, the, um, the founding fathers, uh, the American founding
[00:46:16.320 --> 00:46:24.000]   fathers realized they had a tremendous problem. What they wanted to do, what they thought of
[00:46:24.000 --> 00:46:30.640]   themselves as doing was to implement the British constitution. In fact, they thought they were the
[00:46:30.640 --> 00:46:39.760]   defenders of the British constitution and that the, uh, British King had violated it and, and was,
[00:46:39.760 --> 00:46:44.720]   was bringing it down. They wanted to retain it. The trouble is that they, they all, in order to
[00:46:44.720 --> 00:46:49.120]   do this, to gain the independence to do this, they had to get rid of the King. And then they,
[00:46:49.120 --> 00:46:53.680]   they wondered whether they should get an alternative King, whichever way they did it,
[00:46:53.680 --> 00:47:02.320]   there were problems. The way they decided to do it, I think made for a system that was inherently
[00:47:02.320 --> 00:47:08.560]   much worse than the one they were, were replacing, but they had no choice. If they wanted to get rid
[00:47:08.560 --> 00:47:15.600]   of a King, they had to have a different system for having a head of state. Therefore they had
[00:47:15.600 --> 00:47:22.560]   to have, uh, uh, and they wanted to be democratic. They, uh, that meant that the, the president
[00:47:22.560 --> 00:47:31.200]   had a legitimacy in legislation that the King never had, or sorry, never had. The King did
[00:47:31.200 --> 00:47:37.200]   used to have it in medieval times, but the King, by the, by the time of the, of the enlightenment
[00:47:37.200 --> 00:47:48.960]   and so on, uh, no longer had a full legitimacy to legislate. So they, they had to implement a system
[00:47:48.960 --> 00:47:57.360]   where him seizing power was, was prevented by something other than tradition. And so they
[00:47:57.360 --> 00:48:02.480]   instituted these checks and balances checks. And so the whole thing that they instituted was
[00:48:02.480 --> 00:48:08.560]   immensely sophisticated. It's an amazing intellectual achievement and that it works
[00:48:08.560 --> 00:48:15.040]   as well as it does is something, something of a miracle, but the inherent flaws are there.
[00:48:15.040 --> 00:48:20.640]   And one of them is this, the fact that there are checks and balances means that the responsibility
[00:48:20.640 --> 00:48:27.440]   is dissipated and nobody is ever to blame for anything in the American system, which,
[00:48:27.440 --> 00:48:33.520]   which is terrible in, in the British system. Blame is absolutely focused. You know,
[00:48:33.520 --> 00:48:42.880]   everything is sacrificed to the, to the, um, end of focusing blame and responsibility down to the
[00:48:42.880 --> 00:48:49.840]   midst of, to the government, you know, past, past the, the law courts have passed the parliament
[00:48:49.840 --> 00:48:55.920]   right to the government. That's, that's, that's where it's all focused into. Um,
[00:48:56.640 --> 00:49:03.520]   and, um, there, there are no systems that do that better, but, uh, as you well know,
[00:49:03.520 --> 00:49:09.600]   the, the British system also has, uh, flaws. And we, we recently saw with the,
[00:49:09.600 --> 00:49:17.120]   with the sequence of events with, with, uh, Brexit referendum, and then parliament bulking
[00:49:17.120 --> 00:49:25.280]   at implementing, uh, uh, some, some laws that didn't agree with. And then, uh, that being
[00:49:25.280 --> 00:49:29.360]   referred to the courts. And so there was the courts and the parliament and the government
[00:49:29.360 --> 00:49:35.040]   and the prime minister all blaming each other. And there was sort of mini constitutional crisis,
[00:49:35.040 --> 00:49:42.800]   uh, which could only be resolved by having an election and then having a majority government,
[00:49:42.800 --> 00:49:48.400]   which is by the mathematics of how the government works. That's how it usually is in Britain.
[00:49:48.400 --> 00:49:54.720]   Although, uh, you know, we have been unlucky, uh, several times recently in, in not having a
[00:49:54.720 --> 00:50:02.800]   majority government. Um, okay. So this could be wrong, but it seems to be in, uh, an expanded
[00:50:02.800 --> 00:50:06.560]   universe are, there will be like a finite amount of total matters that will ever exist in our
[00:50:06.560 --> 00:50:12.400]   light cone, right? There's a limit. And that means that there's a limit on the amount of, uh,
[00:50:12.400 --> 00:50:17.840]   computation that, uh, this matter can, uh, you know, execute the amount of energy it can provide,
[00:50:17.840 --> 00:50:21.600]   uh, perhaps even the amount of economic value we can sustain, right? So it would be, uh, it would
[00:50:21.600 --> 00:50:28.160]   be weird if the GDP per atom could be arbitrarily large. Um, so does this impose some sort of limit
[00:50:28.160 --> 00:50:37.280]   on your concept of the beginning of infinity? Uh, so what you've just recounted is a cosmological
[00:50:37.280 --> 00:50:46.880]   theory. Um, this, this, this, the, the universe could be like that, but, uh, we know very little
[00:50:46.880 --> 00:50:53.600]   about cosmology. We know very little about the universes in the large, like theories of cosmology
[00:50:53.600 --> 00:51:00.720]   are changing on a timescale of about a decade. So it doesn't make all that much sense to speculate
[00:51:00.720 --> 00:51:07.840]   about what the ultimate asymptotic form of the cosmological theories will be at the same time.
[00:51:07.840 --> 00:51:15.520]   Um, we, we don't have a good idea about the asymptotic form of very small things. Like
[00:51:15.520 --> 00:51:23.920]   we know that our conception of physical processes must break down somehow at the level of quantum
[00:51:23.920 --> 00:51:32.080]   gravity, uh, like 10 to the minus 42 seconds and, uh, that kind of thing. But, but, uh, we have no
[00:51:32.080 --> 00:51:36.960]   idea what happens below that. Some people say it's got to stop below that, but there's no, there's no
[00:51:36.960 --> 00:51:42.400]   argument for that at all. It's just that we don't know what happens beyond that. Now, what happens
[00:51:42.400 --> 00:51:47.200]   beyond that may be a finite limit. Similarly, the way what happens on a large scale may impose a
[00:51:47.200 --> 00:51:56.000]   finite limit, in which case computation is bounded by a finite limit imposed by the cosmological
[00:51:56.000 --> 00:52:01.600]   initial conditions of this universe, which is still different from it's being imposed by
[00:52:01.600 --> 00:52:09.880]   inherent, um, uh, hardware limitations. For example, if there's a finite amount of, um,
[00:52:12.560 --> 00:52:21.040]   GNP, uh, available in the distant future, then it's still up to us whether we spend that on,
[00:52:21.040 --> 00:52:31.840]   um, mathematics or music or, or the political systems or, or any of the thousands of even
[00:52:31.840 --> 00:52:37.280]   more worthwhile things that have yet to be invented. Uh, so it's, it's up to us, which
[00:52:38.160 --> 00:52:45.040]   ideas we fill the 10 to the 10 to the 10 to the 10 bits with. Now, uh, I,
[00:52:45.040 --> 00:52:53.600]   my guess is that there are no such limits, but my worldview is not affected by whether there
[00:52:53.600 --> 00:53:00.000]   are such limits. Um, because as I said, it's, it's still up to us what to fill them with.
[00:53:00.000 --> 00:53:07.520]   And then if we get chopped off at some point in the future, then everything will have been
[00:53:07.520 --> 00:53:15.280]   worthwhile up to then. Gotcha. Um, okay. So the way I understand your concept of beginning
[00:53:15.280 --> 00:53:19.680]   infinity, it seems to me that the more knowledge we gain, um, the normal knowledge we're in a
[00:53:19.680 --> 00:53:23.200]   position to gain. So there should be like an exponential growth of knowledge. But if we look
[00:53:23.200 --> 00:53:27.600]   at the last 50 years, it seems that there's been a slowdown in, um, or a decrease in research,
[00:53:27.600 --> 00:53:31.520]   productivity, economic growth, productivity growth. And this seems compatible with the story
[00:53:31.520 --> 00:53:35.120]   that, you know, that there's a limited amount of fruit on the tree that we picked the low hanging
[00:53:35.120 --> 00:53:40.400]   fruit. And now there's a less and less fruit and harder and harder fruit to pick. Um, and,
[00:53:40.400 --> 00:53:44.720]   you know, eventually we'll, the orchard will be empty. Um, so do you have an alternative
[00:53:44.720 --> 00:53:50.800]   explanation for what's going on in the last 50 years? Yes, I think it's very simple. There are
[00:53:50.800 --> 00:54:01.040]   sociological factors, uh, in, in academic life, which have, uh, stultified, um, the, the culture
[00:54:01.920 --> 00:54:08.960]   and not, not, not totally and not everywhere, but that, that has been a tendency in what has
[00:54:08.960 --> 00:54:18.400]   happened and it has resulted in a, a, uh, loss of productivity in many sectors in many ways,
[00:54:18.400 --> 00:54:26.240]   but not in every sector, not in every way. And, and, um, uh, the, the, uh,
[00:54:28.000 --> 00:54:36.560]   for example, I think there was a, I've, I've often said there was a stultification in, uh,
[00:54:36.560 --> 00:54:43.760]   theoretical physics, um, starting in, let's say the 1920s and it, and it still hasn't fully
[00:54:43.760 --> 00:54:51.200]   dissipated. If it wasn't for that quantum computers would have been invented in the 1930s
[00:54:51.200 --> 00:55:01.520]   and built in the 1960s. Uh, so that is just an accidental fact, but it's, it's, it just goes to
[00:55:01.520 --> 00:55:09.680]   show that there are no guarantees. The fact that, that the, our horizons are unlimited does not
[00:55:09.680 --> 00:55:15.200]   guarantee that we will get anywhere, that we won't start declining tomorrow. I don't think we are
[00:55:15.200 --> 00:55:21.440]   currently declining. I think the, the, these declines that we see are parochial effects caused
[00:55:21.440 --> 00:55:27.440]   by specific mistakes that, that, uh, have been made and which can be undone.
[00:55:27.440 --> 00:55:35.600]   Um, okay. So, uh, I want to ask you a question about Bayesianism versus, uh,
[00:55:35.600 --> 00:55:40.880]   Popperianism. So one reason why people prefer a Bayes is because there seems to be a way of
[00:55:40.880 --> 00:55:46.480]   describing changes, changes in epistemic status when their relative status of a theory hasn't
[00:55:46.480 --> 00:55:51.040]   changed. So I'll give you an example. Um, currently the many worlds into, uh, explanation is the best
[00:55:51.040 --> 00:55:56.960]   way to explain quantum mechanics, right? But suppose we in the future. Yeah. Okay. Uh, but
[00:55:56.960 --> 00:56:03.760]   suppose in the future, we, um, we were able to build, uh, an AGI on a quantum computer and we
[00:56:03.760 --> 00:56:08.560]   were able to design some clever, um, interference experiment, as you suggest to have it be able to
[00:56:08.560 --> 00:56:13.360]   report back being in a superposition across many worlds. Now, it seems that, um, even though many
[00:56:13.360 --> 00:56:19.200]   worlds remains the best or the only explanation, somehow it's epistemic status has changed, um,
[00:56:19.200 --> 00:56:23.280]   as a result of the experiment. Um, and in a Bayesian terms, you could say the credence
[00:56:23.280 --> 00:56:27.840]   of this theory has increased. How would you describe these sorts of changes in a Popperian
[00:56:27.840 --> 00:56:34.800]   view? So what, what, what has happened there is that, uh, at the moment we have only one
[00:56:34.800 --> 00:56:42.080]   explanation that can't be immediately knocked down. If we had, if, if we did that thought
[00:56:42.080 --> 00:56:52.320]   experiment, we w we might well decide that this will provide the ammunition to knock down even
[00:56:52.320 --> 00:56:57.440]   ideas for alternative explanations that have not been thought of yet.
[00:56:57.440 --> 00:57:03.120]   I mean, obviously it wouldn't be enough to knock down every possible explanation because for a
[00:57:03.120 --> 00:57:07.920]   start, we know that quantum theory is false. We don't know for sure that the next theory will have
[00:57:07.920 --> 00:57:13.840]   many worlds in it. I mean, I think it will, but, but, you know, we, we can't prove anything like
[00:57:13.840 --> 00:57:22.800]   that, but, uh, I, I would replace the idea of increased credence with, uh, uh, uh, theory that,
[00:57:22.800 --> 00:57:37.520]   that the, the, uh, experiment will provide, uh, a, a quiver full of arrows or a, a, a, um,
[00:57:37.520 --> 00:57:48.400]   a repertoire of arguments that goes beyond, um, the, the known arguments, the known bad arguments
[00:57:48.400 --> 00:57:57.760]   and, uh, um, will reach into other types of arguments because the, the reason I, I, I would
[00:57:57.760 --> 00:58:04.880]   say that is that some of the existing misconceptions about quantum theory
[00:58:04.880 --> 00:58:12.800]   reside in misconceptions about, uh, the, the methodology of science. Now I've written a paper
[00:58:12.800 --> 00:58:18.080]   about what I think is the right methodology of science where that does doesn't, uh, apply, but,
[00:58:18.080 --> 00:58:27.120]   but, um, many physicists and many philosophers will disagree with that and they would, um, uh,
[00:58:27.120 --> 00:58:37.600]   advocate a methodology of science that's more, um, uh, based on empiricism. Now, uh, of course,
[00:58:37.600 --> 00:58:42.560]   I think that empiricism is a mistake and can be knocked down in its own terms. So we shouldn't,
[00:58:42.560 --> 00:58:49.280]   but, but not everybody thinks that now, once we have an experiment, such as my, my thought
[00:58:49.280 --> 00:58:57.200]   experiment, if that was actually done, then people could not use the, their arguments based on a, a
[00:58:57.200 --> 00:59:05.520]   fallacious idea of empiricism because their theory would have been refuted even by the standards of
[00:59:05.520 --> 00:59:12.080]   empiricism, which shouldn't have been needed in the first place. But, you know, so that's why I
[00:59:12.080 --> 00:59:20.240]   think that that's the way I would express that the repertoire of arguments would become more powerful
[00:59:20.240 --> 00:59:28.000]   if that experiment were done successfully. Um, the next question I have is how far do you take
[00:59:28.000 --> 00:59:32.400]   the principle that open-ended scientific progress is the best way to deal with existential dangers?
[00:59:32.400 --> 00:59:37.520]   To give it one example, uh, many people have suggested, um, so that you have something like
[00:59:37.520 --> 00:59:40.720]   gain-of-function research, right? And it's conceivable that it could lead to more knowledge
[00:59:40.720 --> 00:59:45.680]   in how to stop dangerous pathogens. But, um, I guess at least in Bayesian terms, you could say
[00:59:45.680 --> 00:59:54.160]   it seems even more likely, uh, that it can or has led to the, the spread of a man-made pathogen
[00:59:54.160 --> 01:00:00.320]   that would have not otherwise been, um, naturally developed. So what would your belief in open-ended
[01:00:00.320 --> 01:00:03.360]   scientific progress allow us to say, okay, let's stop gain-of-function research?
[01:00:03.360 --> 01:00:13.040]   No, it wouldn't allow us to say, let's stop it. It might, um, make it reasonable to say,
[01:00:13.040 --> 01:00:22.000]   let us do research into how to make laboratories more secure before we do
[01:00:22.000 --> 01:00:26.960]   gain-of-function research. It's really part of the same thing. It's, it's, it's like saying,
[01:00:28.000 --> 01:00:34.960]   uh, let's do research into how, how to make the, uh, plastic hoses through which the reagents pass,
[01:00:34.960 --> 01:00:39.360]   uh, more impermeable before we actually do the experiments with the reagents.
[01:00:39.360 --> 01:00:44.720]   So it's all part of the same experiment. I wouldn't want to stop something just because
[01:00:44.720 --> 01:00:51.600]   new knowledge might be discovered. That, that's, that's the no-no in, in my view, but, but which
[01:00:51.600 --> 01:00:56.800]   knowledge we need to, uh, discover first, that's the problem of scheduling, which is non-trivial,
[01:00:56.800 --> 01:01:00.400]   non-trivial part of any research and of any learning.
[01:01:00.400 --> 01:01:06.240]   But would it be considerable for you to say that until we figure out how to make sure these
[01:01:06.240 --> 01:01:12.400]   laboratories are, um, safe to a certain standard, um, we will stop, uh, the research as it exists
[01:01:12.400 --> 01:01:17.360]   now. And then, uh, meanwhile, we will, uh, uh, meanwhile, we'll focus on doing the other kind
[01:01:17.360 --> 01:01:20.240]   of research. So gain-of-function can restart, but until then it's not allowed.
[01:01:23.680 --> 01:01:28.080]   Yes, in principle, that would be reasonable. I don't know enough about the actual situation
[01:01:28.080 --> 01:01:35.440]   to have a view. You know, I don't know how these labs work. I don't know what the, what the, um,
[01:01:35.440 --> 01:01:42.640]   what the precautions consist of. And when I hear people talking about, for example, lab leak,
[01:01:42.640 --> 01:01:49.760]   uh, I think, well, most likely lab leak is that one of the people who works there
[01:01:49.760 --> 01:01:57.040]   walks out of the front door. Uh, so the leak is not a leak from the lab to the outside. The, the,
[01:01:57.040 --> 01:02:03.520]   the, the leak is from the test tube to the person, and then from the person walking out the door.
[01:02:03.520 --> 01:02:11.440]   Uh, and, uh, I don't know enough about what these proportions are or what, what the state
[01:02:11.440 --> 01:02:18.160]   of the art is to know to what extent the risk is actually minimized. It could be that the,
[01:02:18.720 --> 01:02:25.520]   the culture of these labs is not good enough, in which case it would be part of the next experiment
[01:02:25.520 --> 01:02:33.600]   to improve the culture in the labs. But I, I am very suspicious of saying that all labs have to
[01:02:33.600 --> 01:02:41.840]   stop and meet a criterion because I'm sure that the, the, well, I suspect that the,
[01:02:41.840 --> 01:02:45.600]   the stopping wouldn't be necessary and the criterion wouldn't be appropriate.
[01:02:47.440 --> 01:02:51.360]   Again, the, the, which criterion to use depends on the actual research being done.
[01:02:51.360 --> 01:02:57.680]   When I had Tyler Cowen on my podcast, um, I asked him, um, why he thinks, so he thinks that
[01:02:57.680 --> 01:03:03.120]   human civilization is only going to be around for 700 more years. And then, so I asked him,
[01:03:03.120 --> 01:03:07.520]   I gave him, you know, your rebuttal or what I understand to be a rebuttal that, um, you know,
[01:03:07.520 --> 01:03:13.360]   creative, optimistic societies will innovate ways of, uh, you know, safety technologies faster than
[01:03:13.360 --> 01:03:18.240]   totalitarian static societies can innovate way destructive technologies. And he responded,
[01:03:18.240 --> 01:03:24.400]   you know, maybe, but the cost of destruction is just so much lower than, uh, the cost of building.
[01:03:24.400 --> 01:03:28.800]   Um, and you know, that trend has been going on for a while now. Uh, what happens when a new
[01:03:28.800 --> 01:03:34.160]   cost $60,000, um, or what happens if there's a mistake, like, uh, the kinds that, you know,
[01:03:34.160 --> 01:03:39.040]   we saw many times over in the cold war, how would you respond to that? First of all, I think we've
[01:03:39.040 --> 01:03:45.680]   been getting safer and safer throughout the entire history of civilization. Um, the, you know,
[01:03:45.680 --> 01:03:52.800]   there were these plagues that, that wiped out a third of the population of the, of the world or
[01:03:52.800 --> 01:04:01.440]   half, uh, and it could have been 99% or a hundred percent. Uh, we, we went through some kind of,
[01:04:02.480 --> 01:04:10.320]   uh, bottleneck 70,000 years ago, I understand, uh, which they can tell from, from, from genetics,
[01:04:10.320 --> 01:04:18.640]   all our cousin species have been wiped out. So, so, uh, we were, we were much less safe then than
[01:04:18.640 --> 01:04:27.680]   now. Also if, um, if a asteroid 10 kilometer asteroid had been on target with the earth at
[01:04:27.680 --> 01:04:34.640]   any time in the, in the past 2 million year, or whatever it is history of the genus Homo,
[01:04:34.640 --> 01:04:41.040]   that would have been the end of it. Whereas now it'll just mean higher taxation for a while.
[01:04:41.040 --> 01:04:50.080]   You know, that, that's the, that's how much amazingly safer we are. Uh, now, uh, I, I would
[01:04:50.080 --> 01:04:55.200]   never say that it's impossible that we'll destroy ourselves. That, that would be the contrary to
[01:04:55.200 --> 01:05:02.000]   universality of, of the human mind. We can make wrong choices. We can make so many wrong choices
[01:05:02.000 --> 01:05:11.200]   that will destroy ourselves. Um, uh, and the, the, uh, on the other hand, the, the atomic bomb
[01:05:11.200 --> 01:05:17.920]   accident sort of thing would have had no zero chance of destroying civilization. All they would
[01:05:17.920 --> 01:05:25.440]   have done is cause a vast amount of suffering. Uh, and, uh, but that I don't think we have the
[01:05:25.440 --> 01:05:33.200]   technology to end civilization, even if we wanted to, I think all we would do if we just deliberately
[01:05:33.200 --> 01:05:38.800]   unleashed hell all over the world is we would cause a vast amount of suffering,
[01:05:38.800 --> 01:05:43.840]   but there would be survivors and they, they would resolve never to do that again.
[01:05:45.440 --> 01:05:52.080]   Um, so I, I don't think we're even able to let alone, uh, that we would do it accidentally,
[01:05:52.080 --> 01:06:02.320]   but, uh, as for the bad guys, well, I think we are doing the wrong thing largely in regard to both,
[01:06:02.320 --> 01:06:09.120]   uh, external and internal threats, but, uh, I don't think we're doing the wrong thing
[01:06:09.120 --> 01:06:15.520]   to an existential risk level. And over the next 700 years or whatever it is, well, I don't want
[01:06:15.520 --> 01:06:22.560]   to prophesy because I don't know most, most of the advances that are going to be made in that time,
[01:06:22.560 --> 01:06:34.320]   but, uh, I see no reason why if we are solving problems, we won't solve problems. Uh, I, I,
[01:06:34.320 --> 01:06:41.120]   I don't think this, this, uh, forget to take another metaphor, uh, Nick Bostrom's, um,
[01:06:41.120 --> 01:06:48.000]   jar with white balls and there's one black ball and you take out a white ball and white ball and
[01:06:48.000 --> 01:06:52.480]   white ball, and then you hit the black ball and that's the end of you. I don't think it's like
[01:06:52.480 --> 01:07:00.240]   that because every white ball you take out and have reduces the number of black balls in the jar.
[01:07:02.640 --> 01:07:08.880]   So, uh, again, I'm not saying that's the law of nature. It could be that the very next ball we
[01:07:08.880 --> 01:07:14.320]   take out will be the black one. That'll be the end of us. It could be, but I think all arguments
[01:07:14.320 --> 01:07:21.520]   that it will be are fallacious. I do want to talk about the fun criterion. Is your definition of fun
[01:07:21.520 --> 01:07:26.160]   different from how other people define other positive emotions like, um, you know, ammonia
[01:07:26.160 --> 01:07:32.320]   or wellbeing or satisfaction is, is it fun or different emotion? Uh, I don't think it's an
[01:07:32.320 --> 01:07:40.480]   emotion and, and I, all these things are not very well defined. Uh, they can't possibly be very well
[01:07:40.480 --> 01:07:48.560]   defined until we have a satisfactory theory of qualia at least, and probably more satisfactory
[01:07:48.560 --> 01:07:57.280]   theory of creativity, how creativity works and so on. Uh, I think that, um, the choice of the
[01:07:57.280 --> 01:08:05.840]   word fun for the thing that I, I, uh, explain more precisely, but still not very precisely as,
[01:08:05.840 --> 01:08:15.600]   as, um, uh, a, a creation of knowledge without, uh, uh, where, where the different kinds of
[01:08:15.600 --> 01:08:23.040]   knowledge, inexplicit, unconscious, conscious, explicit, uh, are, uh, all, um, in harmony with
[01:08:23.040 --> 01:08:34.320]   each other. I think that is actually, um, the only way in which the everyday usage of the word fun
[01:08:34.320 --> 01:08:43.280]   differs from that is that fun is considered frivolous or, uh, seeking fun is considered
[01:08:43.920 --> 01:08:50.640]   as seeking frivolity. But I think that isn't so much a different use of the word. It's, it's,
[01:08:50.640 --> 01:08:56.960]   it's just a different pejorative theory about whether this is a good or bad thing, but, but
[01:08:56.960 --> 01:09:02.400]   nevertheless, I can't define it precisely. The important thing is that there is a thing
[01:09:02.400 --> 01:09:11.360]   which has these, this property of fun that, that you can't, you can't compulsorily enact it.
[01:09:12.400 --> 01:09:22.560]   So in, in, in, in, um, in some views, you know, no pain, no gain. Well, then you can find out
[01:09:22.560 --> 01:09:28.240]   mechanically whether the thing is causing pain and whether it's doing it according to the theory
[01:09:28.240 --> 01:09:32.720]   that said that says that you will have gain if you have that pain and so on. So that can all be
[01:09:32.720 --> 01:09:38.800]   done mechanically and therefore it is subject to the criticism. And another way of looking at the
[01:09:38.800 --> 01:09:46.400]   fun theory is that it's a mode of criticism. Um, is subject to the criticism that this isn't fun,
[01:09:46.400 --> 01:09:54.640]   i.e. this is making an, uh, privileging one kind of knowledge arbitrarily over another,
[01:09:54.640 --> 01:10:01.600]   rather than being rational and letting content aside. Is this placing a limitation on universal
[01:10:01.600 --> 01:10:06.320]   explainers then if they can't create some sort of theory about why a thing could or should be fun,
[01:10:06.320 --> 01:10:12.160]   why anything could be fun. Um, and, um, it, it, it, it seems to me that sometimes we actually
[01:10:12.160 --> 01:10:15.520]   can make things fun that aren't like, for example, take exercise, no pain, no gain.
[01:10:15.520 --> 01:10:18.240]   It's like when you first go, it's not fun, but you know, once you start going,
[01:10:18.240 --> 01:10:22.720]   you understand the mechanics, you develop a theory for why it can and should be fun.
[01:10:22.720 --> 01:10:26.160]   Yes. Yes. Well, that, that, that's quite a good example because
[01:10:26.160 --> 01:10:31.120]   there you see that fun cannot be defined as the absence of pain.
[01:10:33.120 --> 01:10:42.560]   So you, you can be having fun while experiencing physical pain and that physical pain is not
[01:10:42.560 --> 01:10:54.480]   sparking suffering, but joy. Um, however, there is such a thing as physical pain, not sparking joy,
[01:10:56.560 --> 01:11:04.560]   as Marie Kondo would say. Um, and, and that's important because if you are
[01:11:04.560 --> 01:11:16.800]   dogmatically or uncritically implementing in your life, a theory of the good that involves pain
[01:11:16.800 --> 01:11:24.560]   and which excludes the criticism that maybe this can't be fun, or maybe this isn't yet fun,
[01:11:25.120 --> 01:11:30.240]   or maybe I should make it fun. And if I can't, that's a reason to stop, you know, all those
[01:11:30.240 --> 01:11:35.200]   things. If all those things are excluded, because by definition, the thing is good
[01:11:35.200 --> 01:11:43.280]   and your pain, your suffering doesn't matter, then, then that opens the door to not only to
[01:11:43.280 --> 01:11:51.280]   suffering, but to stasis. You, you won't be able to get to a better theory.
[01:11:53.440 --> 01:11:58.080]   And then why is fun, um, fun central to this instead of another emotion? So,
[01:11:58.080 --> 01:12:01.120]   you know, like for example, Aristotle thought that like, I guess, a sort of, uh,
[01:12:01.120 --> 01:12:07.120]   widely defined sense of happiness is what should be the goal of our endeavors. Um,
[01:12:07.120 --> 01:12:14.880]   why fun instead of something like that? Uh, well, that's defining it vaguely enough. So that
[01:12:14.880 --> 01:12:22.480]   what you said might wear very well be fun. The point is the, the underlying thing is as far as,
[01:12:22.480 --> 01:12:27.600]   you know, going one level below where really to understand that we'd need to go about seven levels
[01:12:27.600 --> 01:12:33.760]   below that, which we can't do yet. But, uh, the, the important thing is that there are several
[01:12:33.760 --> 01:12:42.000]   kinds of knowledge in our brains. And the one that is written down in the exercise book that says you
[01:12:42.000 --> 01:12:47.920]   should do this number of reps and you should, um, you should power through this. And it doesn't
[01:12:47.920 --> 01:12:53.920]   matter if you feel that and so on, that's an explicit theory and it contains some knowledge,
[01:12:53.920 --> 01:13:00.480]   but it also contains error. Um, that's like all our knowledge is like that. We also have other
[01:13:00.480 --> 01:13:08.240]   knowledge, which is, which is contained in our, um, biology. It's contained in our genes. We have
[01:13:08.240 --> 01:13:14.480]   knowledge that is inexplicit. Like our knowledge of, of grammar is my, always my favorite example,
[01:13:14.480 --> 01:13:19.120]   because we know, you know, why certain sentences are acceptable and why they're unacceptable,
[01:13:19.120 --> 01:13:28.400]   but we can't state explicitly or in every case, why it isn't or why it is. Um, uh, and then as
[01:13:28.400 --> 01:13:34.000]   there's, um, so that there's explicit and inexplicit knowledge as conscious and unconscious
[01:13:34.000 --> 01:13:41.840]   knowledge, all those are bits of program in the brain, their ideas. They are, they, they, um,
[01:13:41.840 --> 01:13:47.120]   are bits of knowledge in this. If you define knowledge as information with causal power,
[01:13:47.120 --> 01:13:54.960]   they are all information with causal power. They all contain truth and they all contain error.
[01:13:54.960 --> 01:14:03.200]   And it's always a mistake to shield something, to shield one of them from criticism or replacement.
[01:14:03.200 --> 01:14:07.600]   Not doing that is, is what I call the fun criterion. Now you might say that,
[01:14:07.600 --> 01:14:11.120]   so that's a bad name, but you know, it's the best I can find.
[01:14:11.360 --> 01:14:17.200]   Um, so why would creating an AGI through evolution necessarily entail suffering?
[01:14:17.200 --> 01:14:21.600]   Because the way I see it, or it seems to me, your theory is that you need to be a general
[01:14:21.600 --> 01:14:26.720]   intelligence in order to feel suffering. But by the point in evolved, uh, simulated being is a
[01:14:26.720 --> 01:14:32.480]   general intelligence. We can just stop the, we can just stop the simulation. And so where's the
[01:14:32.480 --> 01:14:39.920]   suffering coming from? Okay. So the kind of simulation by evolution that I'm thinking of,
[01:14:39.920 --> 01:14:44.720]   I mean, there, there, there may be several kinds, but the kind that I'm thinking of,
[01:14:44.720 --> 01:14:51.360]   and which I said would be the greatest crime in history is the kind that just simulates the actual
[01:14:51.360 --> 01:15:00.080]   evolution of humans from pre-humans that weren't people. So you have, you have a population of
[01:15:00.080 --> 01:15:08.240]   non-people, which in this simulation would be some kind of NPCs. Um, and, and then they would,
[01:15:08.240 --> 01:15:12.640]   they would just evolve. We don't know what the criterion would be. We just have an artificial
[01:15:12.640 --> 01:15:17.680]   universe, which simulated the surface of the earth, and they'd be walking around and some of them
[01:15:17.680 --> 01:15:24.960]   might or might not become people. And now the thing is when you're part of the way there,
[01:15:24.960 --> 01:15:34.080]   what is happening is that you have, uh, the way that I, the only way that I can imagine the
[01:15:34.080 --> 01:15:44.080]   evolution of personhood or create the explanatory creativity happened was that, uh, the hardware
[01:15:44.080 --> 01:15:49.840]   needed for it, for it was, was first needed for something else. I, I have proposed that it was
[01:15:49.840 --> 01:15:58.720]   needed to, to transmit memes. So there'd be people who were transmitting memes creatively, but they
[01:15:58.720 --> 01:16:07.120]   were running out of resources. So they weren't running out of resources, uh, before it managed
[01:16:07.120 --> 01:16:15.120]   to increase their, their stock of memes. So in every generation, there was a stock of memes that
[01:16:15.120 --> 01:16:19.920]   was being passed down to the next generation. And once they got beyond a certain complexity,
[01:16:19.920 --> 01:16:23.760]   they had to be passed down by the use of creativity by the recipient.
[01:16:24.880 --> 01:16:30.880]   So there may well have been a time. And as I say, I can't think of any other way it could have been
[01:16:30.880 --> 01:16:37.040]   where there was genuine creativity being used, but it ran out of resources very quickly,
[01:16:37.040 --> 01:16:45.600]   but not so quickly that it didn't, uh, increase the meme bandwidth. Then in the next generation,
[01:16:45.600 --> 01:16:50.800]   there was more mean bandwidth. And then after, you know, a certain number of generations,
[01:16:51.760 --> 01:16:59.760]   there would have been some opportunity to use this hardware or whatever it is, you know,
[01:16:59.760 --> 01:17:05.440]   firmware, I expect to use this firmware for something other than just trying blindly
[01:17:05.440 --> 01:17:11.360]   transmitting memes or rather creatively transmitting memes, but, but they were blind
[01:17:11.360 --> 01:17:20.400]   memes. So, uh, in that time, it would have been very unpleasant to be alive. It was already very
[01:17:20.400 --> 01:17:28.880]   unpleasant to, to, sorry, it was very unpleasant to be alive when we did have enough resources to
[01:17:28.880 --> 01:17:34.480]   think as well as do the memes. But I don't think there would have been a moment at which you would
[01:17:34.480 --> 01:17:41.600]   say, yes, now the suffering begins to matter because it's not just blind memes. I think
[01:17:41.600 --> 01:17:47.600]   the people were already suffering at the, at the time when they were blindly transmitting memes,
[01:17:48.400 --> 01:17:54.720]   because they were using genuine creativity. They were just not using it to any good effect.
[01:17:54.720 --> 01:18:02.080]   Gotcha. Um, would, uh, would being in the experience machine be compatible with the
[01:18:02.080 --> 01:18:06.640]   fun criterion? So, um, you're not aware that you're in, you know, uh, the experience machine,
[01:18:06.640 --> 01:18:10.960]   it's all virtual reality. Um, but you're still doing the things that would make you have fun.
[01:18:10.960 --> 01:18:15.920]   In fact, more so than in the real world. Um, so would you be tempted to get into the experience
[01:18:15.920 --> 01:18:19.440]   machine? Uh, would it be compatible with the fun criteria? I guess there are different questions,
[01:18:19.440 --> 01:18:25.360]   but I'm not sure what the experience machine is. I mean, if it's just, uh,
[01:18:25.360 --> 01:18:34.080]   so, I mean, is it, is it just a virtual reality world in, in which, uh, things work better than
[01:18:34.080 --> 01:18:38.560]   in the real world or something? Yeah. So it's a thought experiment by Robert Nozick. And the
[01:18:38.560 --> 01:18:44.240]   idea is that you would enter this world and, but you would forget that you're in virtual reality.
[01:18:44.240 --> 01:18:50.240]   So all, I mean, the world would be perfect in every possible way that it could be perfect or
[01:18:50.240 --> 01:18:54.160]   not perfect, but it would be better in every possible way. It could be better. Um, but you
[01:18:54.160 --> 01:18:57.520]   would think the relationships you have here are real. The knowledge you're discovering here is
[01:18:57.520 --> 01:19:06.880]   novel and so on is it, would you be tempted to enter such a world? Uh, well, no, I, I certainly
[01:19:06.880 --> 01:19:13.600]   wouldn't want to enter a world, any world, which involves erasing the memory that I have come from
[01:19:13.600 --> 01:19:22.560]   this world. Uh, related to that is the fact that the laws of physics in this virtual world, um,
[01:19:22.560 --> 01:19:28.560]   couldn't be the true ones because the true ones aren't yet known. So I'd be in a world in which
[01:19:28.560 --> 01:19:34.880]   I was trying to learn laws of physics, which aren't the actual laws. And they would have been
[01:19:34.880 --> 01:19:40.560]   designed by somebody for some purpose to manipulate me as it were, maybe, maybe it
[01:19:40.560 --> 01:19:47.520]   would be designed to like be a puzzle that would, that would take 50 years to solve,
[01:19:47.520 --> 01:19:54.160]   but it would have to be by definition, a finite puzzle. And it wouldn't be the actual world.
[01:19:54.160 --> 01:19:58.560]   And meanwhile, in the actual world, things are going wrong and I don't know about this. And
[01:19:58.560 --> 01:20:04.080]   eventually they go so wrong that my computer runs out of power. And then where will I be?
[01:20:04.080 --> 01:20:10.240]   Um, the final question I always like to ask people I interview is what advice would you
[01:20:10.240 --> 01:20:14.880]   give to young people? So, um, somebody in their twenties, is there something that you would like
[01:20:14.880 --> 01:20:22.320]   to, uh, some advice you would give them? Um, well, I, I try very hard not to give advice
[01:20:22.320 --> 01:20:30.720]   because, uh, it's, it's, it's not a good relationship to be with some in, with somebody
[01:20:30.720 --> 01:20:37.200]   to give them advice. I can have opinions about things. So, uh, for example, I, I may have an
[01:20:37.200 --> 01:20:49.280]   opinion that, that, um, uh, it's dangerous to condition your short-term goals by reference
[01:20:49.280 --> 01:20:55.840]   to some long-term goal. And I have a, what I think is a good epistemological reason for that,
[01:20:55.840 --> 01:21:01.600]   namely that, uh, if your short-term goals are subordinate to your long-term goal,
[01:21:01.600 --> 01:21:06.560]   then you won't find, if your long-term goal is wrong or deficient in some way, you won't find
[01:21:06.560 --> 01:21:13.200]   out until you're dead. So it, it, it's a bad idea because it is subordinating the things that you
[01:21:13.200 --> 01:21:19.840]   could error correct now or in six months time or in a year's time to something that you could only,
[01:21:19.840 --> 01:21:29.120]   uh, error correct on a 50-year timescale, and then it'll be too late. So I'm, I'm, uh, suspicious of
[01:21:29.120 --> 01:21:37.280]   advice of the form, set your goal, and even more suspicious of make your goal be so-and-so.
[01:21:37.280 --> 01:21:39.680]   Huh. Interesting.
[01:21:39.680 --> 01:21:49.360]   But why is it, uh, why do you think the, uh, the relationship between advicee and, um,
[01:21:49.360 --> 01:21:55.200]   advice giver is dangerous? Oh, well, because it's one of authority. Uh, uh, again, you know,
[01:21:57.120 --> 01:22:04.720]   I, I tried to make this example of quote advice that I just gave. I tried to make it non-authoritative.
[01:22:04.720 --> 01:22:12.640]   I just gave an argument for why certain other arguments are bad. So, but if it's advice of the
[01:22:12.640 --> 01:22:21.760]   form, uh, healthy mind in a healthy body, or, um, don't drink coffee before 12 o'clock, or, you know,
[01:22:21.760 --> 01:22:30.880]   something like that, it's, it's, um, well, it's a non-argument. It's, it's, uh, it's, it, if I,
[01:22:30.880 --> 01:22:37.280]   if I have an argument, I can give the argument and not tell the person what to do. Uh, who knows what
[01:22:37.280 --> 01:22:44.480]   somebody might do with an argument? They might change it to, to a better argument, which actually
[01:22:44.480 --> 01:22:53.840]   implies different behavior. I can contribute to the world arguments, make arguments as best I can.
[01:22:53.840 --> 01:23:03.280]   I don't claim that they are privileged over other arguments. I, I, I just put them out because
[01:23:03.280 --> 01:23:10.080]   I think that this argument works and I expect other people not to think that they work. I mean,
[01:23:11.200 --> 01:23:17.520]   we've just done this in this very podcast. You know, I put out an argument about AI and that
[01:23:17.520 --> 01:23:24.240]   kind of thing, and you criticize it. You know, if, if, if I was in the position of
[01:23:24.240 --> 01:23:34.320]   making that argument and saying that therefore you should do so-and-so that's a relationship
[01:23:34.320 --> 01:23:40.720]   of authority, which I think is immoral to have. Well, David, thanks so much for, um,
[01:23:40.720 --> 01:23:43.440]   thanks so much for coming on the podcast and thanks so much for giving me so much of your time.
[01:23:43.440 --> 01:23:54.560]   Fascinating. Thank, thank you for, um, for inviting me.
[01:23:54.560 --> 01:24:06.640]   [Music]

