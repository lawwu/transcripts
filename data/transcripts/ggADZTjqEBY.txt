
[00:00:00.000 --> 00:00:02.880]   OK, this is Charles here.
[00:00:02.880 --> 00:00:04.920]   Can people hear me?
[00:00:04.920 --> 00:00:05.400]   Great.
[00:00:05.400 --> 00:00:08.800]   OK, so it looks like Lavanya's had a little bit of
[00:00:08.800 --> 00:00:10.440]   technical troubles.
[00:00:10.440 --> 00:00:15.600]   So I'm just going to go ahead and jump into my talk.
[00:00:15.600 --> 00:00:18.440]   So if you have other questions for Jare, make sure to put
[00:00:18.440 --> 00:00:19.680]   those in the Slack community.
[00:00:19.680 --> 00:00:28.800]   So what I'm going to talk about today is something less
[00:00:28.800 --> 00:00:32.200]   practical and more connected to the mathematics of machine
[00:00:32.200 --> 00:00:36.000]   learning, which is why you should be thinking with
[00:00:36.000 --> 00:00:40.160]   negative log probabilities instead of probabilities.
[00:00:40.160 --> 00:00:44.000]   So machine learning is at the intersection of three
[00:00:44.000 --> 00:00:47.200]   separate areas of math, at the very least.
[00:00:47.200 --> 00:00:49.760]   But the most fundamental ones are definitely these three--
[00:00:49.760 --> 00:00:53.800]   linear algebra, calculus, and probability.
[00:00:53.800 --> 00:01:01.240]   And so these are old and storied versions or types of
[00:01:01.240 --> 00:01:01.880]   mathematics.
[00:01:01.880 --> 00:01:05.320]   And so there's ideas about how these areas of math should be
[00:01:05.320 --> 00:01:07.760]   explained, what they're for.
[00:01:07.760 --> 00:01:10.520]   So for linear algebra, it's that it's algebra for solving
[00:01:10.520 --> 00:01:12.720]   equations with matrices instead of just numbers.
[00:01:12.720 --> 00:01:15.680]   With calculus, it's that it's for studying rates of change
[00:01:15.680 --> 00:01:17.720]   and for studying areas under curves.
[00:01:17.720 --> 00:01:21.440]   And for probability, it's that it's tools for manipulating
[00:01:21.440 --> 00:01:23.400]   distributions.
[00:01:23.400 --> 00:01:26.120]   But in the last couple weeks, I've been talking about how
[00:01:26.120 --> 00:01:29.680]   these traditional views aren't well suited to the problems
[00:01:29.680 --> 00:01:32.400]   that we have in machine learning, that linear algebra
[00:01:32.400 --> 00:01:34.760]   is not like algebra and that there's a better way to think
[00:01:34.760 --> 00:01:36.080]   about calculus.
[00:01:36.080 --> 00:01:40.360]   So in the two previous salons, I talked about how linear
[00:01:40.360 --> 00:01:42.480]   algebra is maybe a little bit more like computer
[00:01:42.480 --> 00:01:46.040]   programming and that it's the study of functions that can be
[00:01:46.040 --> 00:01:51.360]   represented by arrays more than it is the arrays
[00:01:51.360 --> 00:01:53.440]   themselves.
[00:01:53.440 --> 00:01:58.200]   There's also, in the most recent salon, I talked about
[00:01:58.200 --> 00:02:01.240]   how calculus is more about using those linear maps to
[00:02:01.240 --> 00:02:04.080]   approximate functions, at least the way we use it in
[00:02:04.080 --> 00:02:07.480]   machine learning, where we mostly care about gradients.
[00:02:07.480 --> 00:02:11.880]   So I'm going to round out this series, at least for now, by
[00:02:11.880 --> 00:02:14.720]   talking about a better way to understand probability.
[00:02:14.720 --> 00:02:18.200]   And that's to think of probability as the mathematics
[00:02:18.200 --> 00:02:20.120]   of surprise.
[00:02:20.120 --> 00:02:22.200]   If you want something that's a little less poetic and a
[00:02:22.200 --> 00:02:24.680]   little bit easier to connect to other stuff, this is
[00:02:24.680 --> 00:02:26.800]   basically saying, I'm going to go from the information
[00:02:26.800 --> 00:02:30.240]   theory perspective on probability, rather than the
[00:02:30.240 --> 00:02:34.720]   measure theoretic probability or the statistical approach to
[00:02:34.720 --> 00:02:36.600]   probability.
[00:02:36.600 --> 00:02:39.520]   And we're going to get there by thinking about what I call
[00:02:39.520 --> 00:02:43.240]   the surprise game, where modeling is a competition to
[00:02:43.240 --> 00:02:47.840]   see who can be the least surprised by the data.
[00:02:47.840 --> 00:02:54.040]   So I'm going to contrast this competition with the way we
[00:02:54.040 --> 00:02:57.640]   solve debates in classical logic.
[00:02:57.640 --> 00:03:03.120]   If I claim x and you claim y, the way we resolve our
[00:03:03.120 --> 00:03:05.520]   disagreement is that one of us provides a proof.
[00:03:05.520 --> 00:03:09.080]   So if I say 2 plus 2 is 4 and you say 2 plus 2 is 5, one of
[00:03:09.080 --> 00:03:12.240]   us needs to provide a proof that what we're saying is
[00:03:12.240 --> 00:03:15.160]   true or the other person's saying is not true.
[00:03:15.160 --> 00:03:20.000]   Famously, the debate between Socrates is mortal and
[00:03:20.000 --> 00:03:23.480]   Socrates is not mortal was one of the first debates done in
[00:03:23.480 --> 00:03:24.640]   classical logic.
[00:03:24.640 --> 00:03:27.920]   Turns out Socrates is, in fact, mortal.
[00:03:27.920 --> 00:03:31.960]   But logic like this doesn't immediately apply to most
[00:03:31.960 --> 00:03:33.080]   things in machine learning.
[00:03:33.080 --> 00:03:35.960]   Like this collection of pixels is a photo of a dog.
[00:03:35.960 --> 00:03:40.840]   There's no way to take our ands, our ors, our xors, our
[00:03:40.840 --> 00:03:44.000]   disjunctive syllogi and all these tools and just answer
[00:03:44.000 --> 00:03:45.400]   that question.
[00:03:45.400 --> 00:03:48.000]   And if it seems to you a little bit surprising, I would
[00:03:48.000 --> 00:03:52.400]   consider maybe some examples of pictures like this from
[00:03:52.400 --> 00:03:56.080]   Twitter user @teenybiscuit.
[00:03:56.080 --> 00:03:57.040]   These are images.
[00:03:57.040 --> 00:03:58.480]   Some of them are chihuahuas.
[00:03:58.480 --> 00:04:00.920]   Others are blueberry muffins.
[00:04:00.920 --> 00:04:04.600]   And it's actually pretty hard to tell which one is which.
[00:04:04.600 --> 00:04:07.160]   And some machine learning libraries, like publicly
[00:04:07.160 --> 00:04:10.560]   provided ones, struggle on this data set.
[00:04:10.560 --> 00:04:13.080]   And it's even harder to imagine how we might write
[00:04:13.080 --> 00:04:19.000]   down some set of basic logic that would tell us this
[00:04:19.000 --> 00:04:22.200]   picture has a dog in it or not.
[00:04:22.200 --> 00:04:26.840]   There's a whole bunch of other examples, things that happen
[00:04:26.840 --> 00:04:29.840]   in the future, like team A will beat team B. This stock's
[00:04:29.840 --> 00:04:32.280]   price will go up or this stock's price will go down.
[00:04:32.280 --> 00:04:33.840]   The number of deaths from coronavirus will
[00:04:33.840 --> 00:04:34.880]   be over 1 million.
[00:04:34.880 --> 00:04:37.600]   There are a lot of questions that we want to ask, but we
[00:04:37.600 --> 00:04:42.520]   can't use logic to answer them.
[00:04:42.520 --> 00:04:48.000]   So let's consider, instead of a debate involving proofs,
[00:04:48.000 --> 00:04:49.600]   let's consider a competition.
[00:04:49.600 --> 00:04:52.800]   Each of us writes down how surprised we would be for each
[00:04:52.800 --> 00:04:54.560]   possible outcome before it happens.
[00:04:54.560 --> 00:04:56.520]   This is how surprised I'd be if team A wins.
[00:04:56.520 --> 00:04:59.720]   This is how surprised I would be if team B wins.
[00:04:59.720 --> 00:05:02.240]   And it's important to say that it's no fair saying nothing
[00:05:02.240 --> 00:05:03.640]   surprises me.
[00:05:03.640 --> 00:05:05.000]   So there's a technical, mathematical
[00:05:05.000 --> 00:05:06.720]   way to enforce that.
[00:05:06.720 --> 00:05:09.840]   But basically, you have to say that some things are
[00:05:09.840 --> 00:05:10.440]   surprising to you.
[00:05:10.440 --> 00:05:16.440]   Maybe everything's the same amount of surprising, but you
[00:05:16.440 --> 00:05:20.120]   can't just say everything is completely unsurprising.
[00:05:20.120 --> 00:05:22.760]   And then whoever is least surprised by the outcome wins
[00:05:22.760 --> 00:05:24.840]   the debate, at least this time around.
[00:05:24.840 --> 00:05:28.120]   And maybe you do this over and over and over again on many
[00:05:28.120 --> 00:05:30.960]   examples of team A and team B playing each other.
[00:05:30.960 --> 00:05:33.000]   Hopefully, you wouldn't do it on multiple examples of the
[00:05:33.000 --> 00:05:35.480]   world suffering from the coronavirus pandemic.
[00:05:35.480 --> 00:05:38.120]   But in lots of other cases, you can do this
[00:05:38.120 --> 00:05:39.320]   over and over again.
[00:05:39.320 --> 00:05:43.160]   And from this setup of this game, of this competition of
[00:05:43.160 --> 00:05:46.920]   who could be least surprised, you can actually derive
[00:05:46.920 --> 00:05:49.760]   maximum likelihood estimation and the Kullback-Weibuller
[00:05:49.760 --> 00:05:52.400]   divergence and a bunch of other tools that are actually
[00:05:52.400 --> 00:05:55.120]   where many of the loss functions that we use in deep
[00:05:55.120 --> 00:05:57.560]   learning and machine learning in general come from, things
[00:05:57.560 --> 00:06:02.720]   like the cross entropy or even the squared error or the
[00:06:02.720 --> 00:06:03.800]   absolute error.
[00:06:03.800 --> 00:06:07.000]   We can get them using this framework by just thinking of
[00:06:07.000 --> 00:06:12.720]   them in terms of this idea of surprise.
[00:06:12.720 --> 00:06:17.120]   And so I have something pretty specific in mind when I talk
[00:06:17.120 --> 00:06:20.240]   about surprises, just like people have a specific thing
[00:06:20.240 --> 00:06:22.120]   in mind when they say probability, even though we
[00:06:22.120 --> 00:06:25.520]   have this informal notion of chance.
[00:06:25.520 --> 00:06:28.720]   So if something is just a little bit less probable, it
[00:06:28.720 --> 00:06:31.640]   happening is just a little bit more surprising.
[00:06:31.640 --> 00:06:33.360]   And if it's a lot less probable, it's a
[00:06:33.360 --> 00:06:35.240]   lot more surprising.
[00:06:35.240 --> 00:06:37.840]   If something is certain, it happening is not at all
[00:06:37.840 --> 00:06:38.720]   surprising.
[00:06:38.720 --> 00:06:41.040]   And if something is impossible, it happening is
[00:06:41.040 --> 00:06:42.960]   more surprising than anything else.
[00:06:42.960 --> 00:06:46.200]   So numerically, this is saying that our surprise is a
[00:06:46.200 --> 00:06:47.880]   continuous function of the probability.
[00:06:47.880 --> 00:06:49.040]   That's the first thing.
[00:06:49.040 --> 00:06:53.320]   Tiny changes in probability mean tiny changes in surprise.
[00:06:53.320 --> 00:06:57.640]   And if something is certain, it being not surprising, means
[00:06:57.640 --> 00:07:00.400]   that if something has a probability 1 of happening,
[00:07:00.400 --> 00:07:02.560]   the surprise is 0.
[00:07:02.560 --> 00:07:06.320]   And then if something is impossible, if its probability
[00:07:06.320 --> 00:07:09.000]   is 0, then its surprise is infinite.
[00:07:09.000 --> 00:07:11.240]   That's the only way that it can be more surprising than
[00:07:11.240 --> 00:07:13.320]   anything else.
[00:07:13.320 --> 00:07:15.760]   The important difference between probabilities and
[00:07:15.760 --> 00:07:17.920]   surprises is that where probabilities
[00:07:17.920 --> 00:07:20.000]   multiply, surprises add.
[00:07:20.000 --> 00:07:23.640]   So if two unrelated surprising things happen, we just add
[00:07:23.640 --> 00:07:25.840]   those two surprises together.
[00:07:25.840 --> 00:07:28.520]   So this is the equivalent of the probability rule, which is
[00:07:28.520 --> 00:07:32.120]   that if two things are independent, then we multiply
[00:07:32.120 --> 00:07:35.160]   their probabilities to get their joint probability.
[00:07:35.160 --> 00:07:39.120]   And so we've changed--
[00:07:39.120 --> 00:07:40.880]   it's the same basic rule, but now we're going to use a
[00:07:40.880 --> 00:07:43.400]   different mathematical operation to do it, addition
[00:07:43.400 --> 00:07:45.680]   instead of multiplication.
[00:07:45.680 --> 00:07:49.040]   And together, these define a mathematical
[00:07:49.040 --> 00:07:51.760]   notion of surprise.
[00:07:51.760 --> 00:07:55.920]   So this is a function of the outcomes, just like the
[00:07:55.920 --> 00:07:59.120]   probability is a function of the outcomes.
[00:07:59.120 --> 00:08:01.960]   And we can define it in terms of the probability, that it's
[00:08:01.960 --> 00:08:04.480]   the log of 1 over the probability.
[00:08:04.480 --> 00:08:07.920]   This is also known as the surprise-zul.
[00:08:07.920 --> 00:08:10.880]   And I think I titled this talk originally "Surprise-zul."
[00:08:10.880 --> 00:08:15.400]   It's not a super well-known idea on its own, not something
[00:08:15.400 --> 00:08:18.040]   that people often feel like they need to give a name.
[00:08:18.040 --> 00:08:20.320]   So it's a little bit open-ended.
[00:08:20.320 --> 00:08:22.960]   And I think surprise, rather than surprise-zul, is the
[00:08:22.960 --> 00:08:24.440]   right name for it.
[00:08:24.440 --> 00:08:28.040]   It's kind of a silly-sounding name, surprise-zul.
[00:08:28.040 --> 00:08:32.760]   So this surprise is the thing that when we do information
[00:08:32.760 --> 00:08:37.240]   theory, we take expected values or averages of this to
[00:08:37.240 --> 00:08:39.600]   get the quantities that we're interested in, things like
[00:08:39.600 --> 00:08:43.840]   mutual information and entropy and stuff like that.
[00:08:43.840 --> 00:08:47.600]   And so if you want to go further with this set of
[00:08:47.600 --> 00:08:50.640]   ideas, then go ahead and check out stuff about information
[00:08:50.640 --> 00:08:53.040]   theory, and they're going to run with this idea.
[00:08:53.040 --> 00:08:56.080]   But very few approaches to information theory actually
[00:08:56.080 --> 00:08:59.400]   start by defining this thing, the surprise, and then going
[00:08:59.400 --> 00:09:00.440]   on and defining everything else.
[00:09:00.440 --> 00:09:04.320]   They usually start with something like the entropy.
[00:09:04.320 --> 00:09:08.240]   So one exception is this guy Edwin Jaynes, who is one of
[00:09:08.240 --> 00:09:11.760]   the original Bayesians and maybe contributed more to
[00:09:11.760 --> 00:09:13.920]   Bayesian inference than Bayes did himself.
[00:09:13.920 --> 00:09:16.960]   So he has this great text, "Probability Theory--
[00:09:16.960 --> 00:09:20.400]   The Logic of Science," that starts by defining the
[00:09:20.400 --> 00:09:23.400]   surprise and the probability and goes from there.
[00:09:23.400 --> 00:09:25.440]   So I'd strongly recommend that.
[00:09:25.440 --> 00:09:28.600]   It's a way to learn information theory that's very
[00:09:28.600 --> 00:09:31.480]   relevant to what we do in machine learning.
[00:09:31.480 --> 00:09:34.680]   So we're going to focus on things that are just about
[00:09:34.680 --> 00:09:37.480]   this surprise function and not about
[00:09:37.480 --> 00:09:39.040]   information theory quantities.
[00:09:39.040 --> 00:09:41.640]   So we're not going to take these expectations and things
[00:09:41.640 --> 00:09:43.160]   that they do in information theory.
[00:09:43.160 --> 00:09:46.600]   So the first is a very simple one, which is some folks in
[00:09:46.600 --> 00:09:48.840]   statistics have started to say that you should use the
[00:09:48.840 --> 00:09:51.680]   surprise instead of the p-value to
[00:09:51.680 --> 00:09:53.120]   communicate your results.
[00:09:53.120 --> 00:09:55.320]   So this is something that's maybe going to make more
[00:09:55.320 --> 00:09:57.800]   sense to folks who've done some traditional statistics,
[00:09:57.800 --> 00:10:00.400]   folks with a medicine background or a science
[00:10:00.400 --> 00:10:04.640]   background where these quantities get used.
[00:10:04.640 --> 00:10:07.040]   So using surprise might actually make it easier to
[00:10:07.040 --> 00:10:08.760]   interpret and understand the outcomes of
[00:10:08.760 --> 00:10:11.320]   your hypothesis tests.
[00:10:11.320 --> 00:10:13.880]   So the p-value is what people usually use.
[00:10:13.880 --> 00:10:16.280]   They do all their statistics.
[00:10:16.280 --> 00:10:17.240]   They do all their experiments.
[00:10:17.240 --> 00:10:19.680]   They want, in the end, to know, is this something that
[00:10:19.680 --> 00:10:21.800]   could have happened by chance because of something
[00:10:21.800 --> 00:10:24.360]   uninteresting, the null hypothesis?
[00:10:24.360 --> 00:10:27.240]   And what people want the p-value to mean is something
[00:10:27.240 --> 00:10:29.720]   like the chance that the null hypothesis is true.
[00:10:29.720 --> 00:10:33.800]   I am trying to demonstrate that the moon is made of
[00:10:33.800 --> 00:10:37.200]   cheese, the null hypothesis is that it is not, and so I would
[00:10:37.200 --> 00:10:42.880]   like to be able to tell people the moon is definitely not
[00:10:42.880 --> 00:10:44.320]   made of cheese.
[00:10:44.320 --> 00:10:49.480]   So this p-value is small when it's unlikely that the null
[00:10:49.480 --> 00:10:50.560]   hypothesis is true.
[00:10:50.560 --> 00:10:53.720]   That's what people would like it to mean.
[00:10:53.720 --> 00:10:56.000]   But in fact, it's actually the other way around.
[00:10:56.000 --> 00:10:59.320]   So instead of it being the probability of the null
[00:10:59.320 --> 00:11:02.240]   hypothesis when we've gotten a positive result from our
[00:11:02.240 --> 00:11:08.160]   statistical test, it's the probability of a positive
[00:11:08.160 --> 00:11:11.280]   result on our statistical test if we assume the null
[00:11:11.280 --> 00:11:12.480]   hypothesis.
[00:11:12.480 --> 00:11:15.640]   And it's basically backwards from what
[00:11:15.640 --> 00:11:16.920]   people actually want.
[00:11:16.920 --> 00:11:19.600]   But unfortunately, the thing people actually want is much
[00:11:19.600 --> 00:11:21.040]   harder to get.
[00:11:21.040 --> 00:11:23.960]   And so people calculate this thing, the p-value, then they
[00:11:23.960 --> 00:11:25.960]   misinterpret it as the thing that they
[00:11:25.960 --> 00:11:28.040]   really actually want it.
[00:11:28.040 --> 00:11:30.480]   So one suggestion that's--
[00:11:30.480 --> 00:11:31.920]   I'm going to link to this archive paper that
[00:11:31.920 --> 00:11:32.480]   puts it out there.
[00:11:32.480 --> 00:11:36.320]   It's also something that the American Statistician, the
[00:11:36.320 --> 00:11:39.960]   Journal of the American Statistical Society, one of
[00:11:39.960 --> 00:11:43.400]   the major ones, they've also put this idea out there.
[00:11:43.400 --> 00:11:46.000]   Instead, we should do the logarithm of
[00:11:46.000 --> 00:11:47.600]   1 over the p-value.
[00:11:47.600 --> 00:11:50.840]   And what that says is I'm trying to communicate to you
[00:11:50.840 --> 00:11:54.880]   how surprising my results are to somebody who is a skeptic.
[00:11:54.880 --> 00:11:59.920]   How surprising are my results if you believe this null
[00:11:59.920 --> 00:12:03.120]   model, this model in the null hypothesis?
[00:12:03.120 --> 00:12:05.880]   And they go in opposite directions.
[00:12:05.880 --> 00:12:09.000]   A large s is the kind of thing that you would get if you have
[00:12:09.000 --> 00:12:14.200]   a publishable result, whereas a low p is the thing that you
[00:12:14.200 --> 00:12:18.800]   want in order to have a publishable result.
[00:12:18.800 --> 00:12:22.360]   But apart from that superficial difference, there's a lot of
[00:12:22.360 --> 00:12:24.080]   more substantial differences.
[00:12:24.080 --> 00:12:28.240]   And so this paper goes into detail about them, but it
[00:12:28.240 --> 00:12:31.640]   discourages this misinterpretation of the p-
[00:12:31.640 --> 00:12:35.840]   values of probability and has a lot of additional benefits as
[00:12:35.840 --> 00:12:39.640]   well for combining information across studies and for being
[00:12:39.640 --> 00:12:44.800]   able to better disambiguate between really strong evidence
[00:12:44.800 --> 00:12:47.280]   and moderately strong evidence and things like that.
[00:12:47.280 --> 00:12:55.160]   The other thing that Surprise is useful for is it is useful
[00:12:55.160 --> 00:12:58.640]   for thinking about the densities and distributions
[00:12:58.640 --> 00:13:03.320]   that we work with when we use probability in our machine
[00:13:03.320 --> 00:13:04.640]   learning.
[00:13:04.640 --> 00:13:08.040]   And so I'm going to borrow this example that I saw on
[00:13:08.040 --> 00:13:12.480]   Twitter and ask, just looking at these, which of these do
[00:13:12.480 --> 00:13:14.520]   you think is a Gaussian?
[00:13:14.520 --> 00:13:17.880]   So which of these is a normal distribution or a bell curve?
[00:13:17.880 --> 00:13:22.080]   To me, they all look like bell curves in some loose sense.
[00:13:22.080 --> 00:13:23.800]   And it's really hard for me to tell.
[00:13:23.800 --> 00:13:26.240]   I would have probably said that all of these, yeah, those all
[00:13:26.240 --> 00:13:29.040]   look kind of like a Gaussian to me.
[00:13:29.040 --> 00:13:33.160]   But in fact, only one of these is actually a Gaussian, and
[00:13:33.160 --> 00:13:35.520]   it's the one in the top left.
[00:13:35.520 --> 00:13:39.680]   The others, the logistic distribution, is pretty close
[00:13:39.680 --> 00:13:40.840]   to the Gaussian in a lot of ways.
[00:13:40.840 --> 00:13:43.880]   But the others, the Cauchy distribution, is one that
[00:13:43.880 --> 00:13:46.920]   actually has infinite variance.
[00:13:46.920 --> 00:13:48.520]   So there's no standard deviation to this
[00:13:48.520 --> 00:13:49.120]   distribution.
[00:13:49.120 --> 00:13:50.320]   It's so wide.
[00:13:50.320 --> 00:13:54.480]   The beta distribution is the exact opposite.
[00:13:54.480 --> 00:13:56.800]   The beta distribution down in the bottom right corner is
[00:13:56.800 --> 00:14:01.920]   actually 0 outside of the range from minus 4 to 4 that's in
[00:14:01.920 --> 00:14:03.160]   the center of the plot.
[00:14:03.160 --> 00:14:04.440]   And that's a huge difference.
[00:14:04.440 --> 00:14:07.440]   Under this distribution, nothing outside of plus or
[00:14:07.440 --> 00:14:09.240]   minus 4 can be generated.
[00:14:09.240 --> 00:14:10.640]   That's a very big difference.
[00:14:10.640 --> 00:14:13.760]   But if we just look at these probability densities on their
[00:14:13.760 --> 00:14:15.760]   own, it's really hard to tell the difference.
[00:14:15.760 --> 00:14:17.720]   This seems really bad.
[00:14:17.720 --> 00:14:21.920]   It would be great if the way that we mathematically
[00:14:21.920 --> 00:14:23.920]   represented our distributions made these kinds of
[00:14:23.920 --> 00:14:27.560]   differences plain as day, the way they would be if you
[00:14:27.560 --> 00:14:32.360]   actually started sampling from these distributions.
[00:14:32.360 --> 00:14:35.080]   So logarithms of densities are easier to compare.
[00:14:35.080 --> 00:14:37.600]   So the surprise is going to make our densities easier to
[00:14:37.600 --> 00:14:38.960]   compare to each other.
[00:14:38.960 --> 00:14:42.240]   The important differences in our probability distributions
[00:14:42.240 --> 00:14:45.080]   are often in the really unlikely events, in the tails.
[00:14:45.080 --> 00:14:47.000]   Is this a once in a year event or a
[00:14:47.000 --> 00:14:48.960]   once in a millennium event?
[00:14:48.960 --> 00:14:51.120]   The raw values are really close.
[00:14:51.120 --> 00:14:59.200]   1/365 is only about 1/365 away from 1/365,000, thinking of
[00:14:59.200 --> 00:15:00.920]   these as numbers.
[00:15:00.920 --> 00:15:05.520]   But their logarithms are actually relatively far apart.
[00:15:05.520 --> 00:15:08.080]   There's basically three orders of magnitude between them.
[00:15:08.080 --> 00:15:12.000]   So if I log 10, the difference of those two logs is going to
[00:15:12.000 --> 00:15:12.880]   be 3.
[00:15:12.880 --> 00:15:15.480]   So that makes these things that are actually very, very
[00:15:15.480 --> 00:15:17.600]   different, that would be experienced very, very
[00:15:17.600 --> 00:15:21.240]   differently if you were to draw samples, actually look
[00:15:21.240 --> 00:15:24.880]   different once we write out the numbers.
[00:15:24.880 --> 00:15:27.600]   And in addition, logarithms of densities are actually even
[00:15:27.600 --> 00:15:28.640]   easier to work with.
[00:15:28.640 --> 00:15:32.160]   So a lot of densities have a very simple form in the log.
[00:15:32.160 --> 00:15:35.800]   Like the Gaussian, on the left we have the usual way that
[00:15:35.800 --> 00:15:38.200]   people write the Gaussian distribution in terms of its
[00:15:38.200 --> 00:15:39.760]   probabilities.
[00:15:39.760 --> 00:15:40.920]   And so we've got an exponent.
[00:15:40.920 --> 00:15:46.760]   We've got e on the bottom and then stuff in the top.
[00:15:46.760 --> 00:15:49.400]   And I look at that, and I don't immediately know what
[00:15:49.400 --> 00:15:52.120]   that function looks like or what's going to happen if I
[00:15:52.120 --> 00:15:54.440]   add two of them together or multiply two of them.
[00:15:54.440 --> 00:15:56.080]   It's kind of confusing.
[00:15:56.080 --> 00:16:00.280]   But if I take the logarithm, then that e disappears.
[00:16:00.280 --> 00:16:02.640]   And what I have instead is just a parabola.
[00:16:02.640 --> 00:16:05.000]   I have x minus mu squared.
[00:16:05.000 --> 00:16:08.040]   And for me, parabolas were something that I encountered
[00:16:08.040 --> 00:16:10.680]   really early in my mathematics education.
[00:16:10.680 --> 00:16:12.560]   Grade school.
[00:16:12.560 --> 00:16:15.560]   And so that's something that I've gotten a chance to use a
[00:16:15.560 --> 00:16:17.960]   lot, and I'm comfortable and familiar with.
[00:16:17.960 --> 00:16:22.040]   So these types of densities that have these really neat,
[00:16:22.040 --> 00:16:24.920]   clean forms when they take the logarithm, or that are easy to
[00:16:24.920 --> 00:16:27.960]   mathematically work with once you take a logarithm, are
[00:16:27.960 --> 00:16:31.120]   called exponential families or log linear families.
[00:16:31.120 --> 00:16:34.480]   And they're all over the place in more older school
[00:16:34.480 --> 00:16:36.360]   structured statistical approaches.
[00:16:36.360 --> 00:16:38.640]   And then they bleed into deep learning and machine
[00:16:38.640 --> 00:16:38.960]   learning.
[00:16:38.960 --> 00:16:41.120]   If you look at variational autoencoders, there's going to
[00:16:41.120 --> 00:16:44.080]   be lots of pieces of them that use these exponential or log
[00:16:44.080 --> 00:16:46.680]   linear families in them.
[00:16:46.680 --> 00:16:52.240]   And so they happen to be the class of densities that we can
[00:16:52.240 --> 00:16:54.280]   do math easily with.
[00:16:54.280 --> 00:16:57.400]   And that allows us to do machine learning, create
[00:16:57.400 --> 00:16:59.920]   algorithms with these densities.
[00:16:59.920 --> 00:17:02.120]   And they'll be easier to understand, easier to think
[00:17:02.120 --> 00:17:04.760]   about in a lot of cases if you look at them
[00:17:04.760 --> 00:17:07.160]   with their logarithms.
[00:17:07.160 --> 00:17:11.960]   So on this slide, I told you that the Gaussian
[00:17:11.960 --> 00:17:14.280]   distribution becomes, with the logarithm,
[00:17:14.280 --> 00:17:16.560]   it becomes a parabola.
[00:17:16.560 --> 00:17:20.480]   So looking at these, these are the same four distributions.
[00:17:20.480 --> 00:17:23.200]   I changed the order.
[00:17:23.200 --> 00:17:24.000]   Or maybe I didn't.
[00:17:24.000 --> 00:17:28.280]   So it's not definitely the one in the top left.
[00:17:28.280 --> 00:17:30.000]   So these are the same four distributions.
[00:17:30.000 --> 00:17:32.400]   But now I'm showing you the logs instead
[00:17:32.400 --> 00:17:33.960]   of the raw densities.
[00:17:33.960 --> 00:17:36.840]   So this would basically just take that previous plot, put
[00:17:36.840 --> 00:17:41.280]   it in a log scale, in Python, single line.
[00:17:41.280 --> 00:17:46.240]   And what's interesting about this to me is that they look
[00:17:46.240 --> 00:17:49.160]   super, super different from each other now.
[00:17:49.160 --> 00:17:53.680]   So we can see that one of them is sort of zooming off to
[00:17:53.680 --> 00:17:56.080]   infinity, the one in the top left.
[00:17:56.080 --> 00:18:00.400]   We can see that the one in the bottom left and the top right,
[00:18:00.400 --> 00:18:04.120]   those two are not going down nearly as quickly as the one
[00:18:04.120 --> 00:18:04.920]   in the bottom right.
[00:18:04.920 --> 00:18:08.080]   So the one in the bottom right is going down really quickly.
[00:18:08.080 --> 00:18:11.080]   And the pace at which it is going down is getting faster
[00:18:11.080 --> 00:18:12.320]   and faster.
[00:18:12.320 --> 00:18:19.400]   So the slope is increasing as we go out from the middle.
[00:18:19.400 --> 00:18:24.200]   And that's a characteristic of a parabola.
[00:18:24.200 --> 00:18:28.040]   This guy in the bottom right-hand corner is pretty
[00:18:28.040 --> 00:18:29.080]   easy--
[00:18:29.080 --> 00:18:30.680]   once you know what to look for, it's pretty easy to
[00:18:30.680 --> 00:18:32.640]   identify this guy as a parabola.
[00:18:32.640 --> 00:18:33.640]   And that's a Gaussian.
[00:18:33.640 --> 00:18:35.080]   And the others are the different ones.
[00:18:35.080 --> 00:18:37.280]   And it makes it clearer that what's different between this
[00:18:37.280 --> 00:18:40.320]   logistic and this Cauchy, both of which look pretty similar
[00:18:40.320 --> 00:18:45.400]   to the Gaussian before, is that the way they go to zero
[00:18:45.400 --> 00:18:47.520]   as they go away from the center.
[00:18:47.520 --> 00:18:51.560]   So there's actually a lot more that you can learn just from
[00:18:51.560 --> 00:18:53.880]   comparing these two ways of looking at distributions.
[00:18:53.880 --> 00:18:59.480]   And I recommend you check out this example blog post by this
[00:18:59.480 --> 00:19:03.560]   guy, Ryan Moulton, that goes into more detail about how to
[00:19:03.560 --> 00:19:04.280]   use these things.
[00:19:04.280 --> 00:19:07.200]   And I'll pop a link to these slides into the chat once I
[00:19:07.200 --> 00:19:10.080]   get to the end.
[00:19:10.080 --> 00:19:14.520]   And then lastly, the great thing about these surprises is
[00:19:14.520 --> 00:19:16.960]   that they connect Bayes' rule and linear algebra.
[00:19:16.960 --> 00:19:19.240]   And these are two of my favorite things.
[00:19:19.240 --> 00:19:23.040]   So anything that comes in the middle of them
[00:19:23.040 --> 00:19:25.240]   has got to be great.
[00:19:25.240 --> 00:19:29.440]   So one pithy way to state this is that with surprises, we can
[00:19:29.440 --> 00:19:33.600]   treat our beliefs like vectors.
[00:19:33.600 --> 00:19:36.480]   So the reason why this happens is that logs turn
[00:19:36.480 --> 00:19:38.400]   multiplication into addition.
[00:19:38.400 --> 00:19:47.720]   So on the left here, we've got the negative logarithm takes
[00:19:47.720 --> 00:19:50.600]   things that are positive numbers and turns them into
[00:19:50.600 --> 00:19:52.600]   just any old number, some of them negative,
[00:19:52.600 --> 00:19:53.720]   some of them positive.
[00:19:53.720 --> 00:19:58.760]   And e to the minus x takes a number x that's positive or
[00:19:58.760 --> 00:20:02.240]   negative and makes it solely positive.
[00:20:02.240 --> 00:20:04.360]   And these two things are opposites of each other.
[00:20:04.360 --> 00:20:05.960]   So this is called an isomorphism.
[00:20:05.960 --> 00:20:09.120]   So it says that these two things, we've mapped one onto
[00:20:09.120 --> 00:20:11.880]   the others, mapped one onto the other.
[00:20:11.880 --> 00:20:16.440]   And in particular, what this does is it takes things that
[00:20:16.440 --> 00:20:22.960]   were addition for our positive and negative numbers for what
[00:20:22.960 --> 00:20:26.040]   basically you can think of as floats or real numbers.
[00:20:26.040 --> 00:20:28.600]   It takes addition in the real numbers, and it turns that
[00:20:28.600 --> 00:20:33.040]   into multiplication in the positive only numbers.
[00:20:33.040 --> 00:20:35.320]   So that's in the top right corner of this slide.
[00:20:35.320 --> 00:20:39.040]   A plus B is turned into e to the minus a times e to the
[00:20:39.040 --> 00:20:39.800]   minus b.
[00:20:39.800 --> 00:20:43.800]   So where before we would have added two numbers, now we
[00:20:43.800 --> 00:20:45.960]   multiply those two numbers together.
[00:20:45.960 --> 00:20:52.560]   And if we use this, we can define a vector space of
[00:20:52.560 --> 00:20:56.480]   probability distributions in a straightforward way.
[00:20:56.480 --> 00:20:59.440]   So what we do is we take this e to the minus operation and
[00:20:59.440 --> 00:21:02.880]   we apply it to all vectors of length n.
[00:21:02.880 --> 00:21:06.360]   And that gets us all positive only vectors of length n.
[00:21:06.360 --> 00:21:09.360]   So we basically take that n dimensional space over there
[00:21:09.360 --> 00:21:13.000]   on the left, and we map it all together into the upper right
[00:21:13.000 --> 00:21:17.640]   hand quadrant, the positive only quadrant.
[00:21:17.640 --> 00:21:21.480]   And then we can take that, and if we divide those by their
[00:21:21.480 --> 00:21:26.440]   sum, then we end up with only things that add up to one,
[00:21:26.440 --> 00:21:28.760]   only vectors of length n that add up to one.
[00:21:28.760 --> 00:21:30.440]   And those describe probability
[00:21:30.440 --> 00:21:32.800]   distributions on n outcomes.
[00:21:32.800 --> 00:21:34.960]   So in this case, n is three.
[00:21:34.960 --> 00:21:40.480]   So this is all probability distributions with three
[00:21:40.480 --> 00:21:41.520]   possibilities.
[00:21:41.520 --> 00:21:44.640]   So I guess there aren't three-sided dice, there aren't
[00:21:44.640 --> 00:21:50.920]   three-sided coins, but something that could take three
[00:21:50.920 --> 00:21:53.360]   possible outcomes.
[00:21:53.360 --> 00:21:58.800]   And now, if I want to know--
[00:21:58.800 --> 00:22:02.280]   and I also have that negative log that takes me backwards
[00:22:02.280 --> 00:22:05.000]   from the positive only vectors of length n to all vectors of
[00:22:05.000 --> 00:22:07.720]   length n, so that'd be an arrow pointing to the left
[00:22:07.720 --> 00:22:09.160]   underneath e to the minus.
[00:22:09.160 --> 00:22:14.880]   So if I want to add together two probability
[00:22:14.880 --> 00:22:17.800]   distributions, if I want to define a notion of addition
[00:22:17.800 --> 00:22:20.320]   for probability distributions, I take their negative
[00:22:20.320 --> 00:22:22.560]   logarithm and add that together.
[00:22:22.560 --> 00:22:25.280]   And so I take the negative logarithm to turn them back
[00:22:25.280 --> 00:22:29.080]   into just a generic vector of length n, and then I add them
[00:22:29.080 --> 00:22:30.280]   together.
[00:22:30.280 --> 00:22:35.000]   And that allows me to define a notion of vector addition for
[00:22:35.000 --> 00:22:36.320]   probability distributions.
[00:22:36.320 --> 00:22:40.880]   And I can do the same thing for scalar multiplication.
[00:22:40.880 --> 00:22:46.000]   The other nice thing about this way of thinking about
[00:22:46.000 --> 00:22:49.480]   probability distributions is that it's actually something
[00:22:49.480 --> 00:22:50.600]   you've seen before.
[00:22:50.600 --> 00:22:53.240]   If I combine these two operations, what I'm doing is
[00:22:53.240 --> 00:22:56.560]   I'm taking something, e to the minus something, that I'm
[00:22:56.560 --> 00:22:59.160]   dividing by the sum of those things.
[00:22:59.160 --> 00:23:02.320]   And that combination is what people call the softmax
[00:23:02.320 --> 00:23:05.560]   function, so e to the something divided by sum of e
[00:23:05.560 --> 00:23:06.120]   to the something.
[00:23:06.120 --> 00:23:07.320]   That's softmax.
[00:23:07.320 --> 00:23:10.440]   So what softmax is doing is it's taking things that are
[00:23:10.440 --> 00:23:13.200]   like normal vectors and turning them into probability
[00:23:13.200 --> 00:23:14.240]   distributions.
[00:23:14.240 --> 00:23:16.560]   And the new thing that I'm adding in this discussion
[00:23:16.560 --> 00:23:21.400]   here is I'm saying, let's not just think, oh, I've taken
[00:23:21.400 --> 00:23:24.000]   these numbers and just normalized them so
[00:23:24.000 --> 00:23:25.000]   they sum to 1.
[00:23:25.000 --> 00:23:26.280]   That's not that interesting.
[00:23:26.280 --> 00:23:29.240]   It's that now we know how to do-- we can take things that
[00:23:29.240 --> 00:23:31.880]   we could do to vectors of length n.
[00:23:31.880 --> 00:23:35.080]   We can add them together and subtract them and multiply
[00:23:35.080 --> 00:23:39.440]   them by numbers and apply matrices to them and calculate
[00:23:39.440 --> 00:23:41.880]   eigenvalues of those matrices and all these things.
[00:23:41.880 --> 00:23:43.720]   We now do that with probability distributions.
[00:23:43.720 --> 00:23:47.240]   We just map them backwards with minus log.
[00:23:47.240 --> 00:23:51.080]   Or yeah, with minus log.
[00:23:51.080 --> 00:23:52.800]   And then we apply our operations.
[00:23:52.800 --> 00:23:55.680]   And then we come back if we want the probability
[00:23:55.680 --> 00:23:58.720]   distributions at the end.
[00:23:58.720 --> 00:24:03.080]   And the addition in this vector space is Bayesian updating.
[00:24:03.080 --> 00:24:06.200]   What used to be with probability distributions, it
[00:24:06.200 --> 00:24:09.560]   would be multiplying two things together.
[00:24:09.560 --> 00:24:13.400]   But when we use minus log to turn them into vectors that we
[00:24:13.400 --> 00:24:16.240]   can add, then we're now adding them together.
[00:24:16.240 --> 00:24:18.200]   So on the left-hand side, we have the likelihood and the
[00:24:18.200 --> 00:24:20.800]   prior thought of as surprises.
[00:24:20.800 --> 00:24:22.800]   And on the right-hand side, we have the likelihood and the
[00:24:22.800 --> 00:24:24.280]   prior thought of as probabilities.
[00:24:24.280 --> 00:24:27.840]   And so minus log turns our probabilities into surprises
[00:24:27.840 --> 00:24:29.640]   and allows us to add them instead of
[00:24:29.640 --> 00:24:30.960]   multiplying them together.
[00:24:30.960 --> 00:24:33.920]   And all the normalization stuff gets included as part of
[00:24:33.920 --> 00:24:40.360]   that transformation once we turn things back into
[00:24:40.360 --> 00:24:41.720]   probability distributions.
[00:24:41.720 --> 00:24:44.680]   So this is part of how I think about lots of deep learning
[00:24:44.680 --> 00:24:47.800]   models that have a softmax in them, like some kinds of
[00:24:47.800 --> 00:24:53.840]   attention models and some kinds of categorization, like
[00:24:53.840 --> 00:24:59.320]   categorical models, classification models.
[00:24:59.320 --> 00:25:01.920]   That what they're doing is inside the network, they're
[00:25:01.920 --> 00:25:05.000]   working to build up a nice representation of this vector
[00:25:05.000 --> 00:25:09.280]   space that's on the left-hand side here, where all this
[00:25:09.280 --> 00:25:13.520]   stuff that I want to do with complicated inference is just
[00:25:13.520 --> 00:25:14.600]   simple addition.
[00:25:14.600 --> 00:25:17.200]   And then at the very end, I use something like softmax to
[00:25:17.200 --> 00:25:19.520]   turn it into a probability distribution to use it for
[00:25:19.520 --> 00:25:21.240]   something else.
[00:25:21.240 --> 00:25:23.520]   So this example comes from Tom Leinster at the
[00:25:23.520 --> 00:25:26.080]   nCategory Cafe.
[00:25:26.080 --> 00:25:29.240]   And he goes into more detail about it and focuses on an
[00:25:29.240 --> 00:25:34.040]   example from statistical mechanics from physics.
[00:25:34.040 --> 00:25:37.840]   So the takeaways are that these surprises are negative
[00:25:37.840 --> 00:25:39.000]   log probabilities.
[00:25:39.000 --> 00:25:42.440]   And they give us another lens for understanding uncertainty.
[00:25:42.440 --> 00:25:44.360]   And it complements normal probabilities.
[00:25:44.360 --> 00:25:46.560]   Some things that are really easy to understand with
[00:25:46.560 --> 00:25:49.600]   surprises are a lot harder to understand or to see with
[00:25:49.600 --> 00:25:50.680]   probabilities.
[00:25:50.680 --> 00:25:51.840]   And then vice versa.
[00:25:51.840 --> 00:25:53.880]   Some things, like taking an average, are actually kind of
[00:25:53.880 --> 00:25:56.280]   hard with surprises, but really straightforward with
[00:25:56.280 --> 00:25:57.160]   probabilities.
[00:25:57.160 --> 00:26:01.000]   So it's not necessarily that one is better or worse than
[00:26:01.000 --> 00:26:01.560]   the other.
[00:26:01.560 --> 00:26:06.960]   It's just that they are both useful perspectives.
[00:26:06.960 --> 00:26:09.960]   So when I need to do a problem in probability, when I need to
[00:26:09.960 --> 00:26:13.200]   understand what's going on with some new deep learning
[00:26:13.200 --> 00:26:15.400]   model that has some probabilistic components to it,
[00:26:15.400 --> 00:26:18.880]   I try and attack it with both.
[00:26:18.880 --> 00:26:23.320]   But the thing that gives me a sense that maybe the log
[00:26:23.320 --> 00:26:25.320]   probability way of thinking about it is going to be more
[00:26:25.320 --> 00:26:28.120]   important is any time that there's multiplication of
[00:26:28.120 --> 00:26:29.840]   probabilities, which is really often.
[00:26:29.840 --> 00:26:30.760]   It's in Bayes' rule.
[00:26:30.760 --> 00:26:34.360]   It's in independence assumptions.
[00:26:34.360 --> 00:26:36.200]   It's in conditional probabilities,
[00:26:36.200 --> 00:26:37.320]   all kinds of places.
[00:26:37.320 --> 00:26:39.560]   Any time there's multiplication, a logarithm is
[00:26:39.560 --> 00:26:40.760]   going to turn that into addition.
[00:26:40.760 --> 00:26:43.440]   And it's usually easier to think about adding things
[00:26:43.440 --> 00:26:45.800]   rather than multiplying them.
[00:26:45.800 --> 00:26:48.400]   So I wanted to put this out there just because you might
[00:26:48.400 --> 00:26:51.560]   see people talking about log probabilities when they're
[00:26:51.560 --> 00:26:52.760]   doing a derivation.
[00:26:52.760 --> 00:26:57.480]   Or if you look in TensorFlow probability or PyTorch, Pyro,
[00:26:57.480 --> 00:27:01.000]   or any other probabilistic programming library, you're
[00:27:01.000 --> 00:27:03.080]   going to see log probabilities everywhere.
[00:27:03.080 --> 00:27:05.520]   And most people will say, oh, it's for convenience to
[00:27:05.520 --> 00:27:11.840]   simplify our derivation, to make things numerically easier
[00:27:11.840 --> 00:27:13.360]   on a computer to make sure that there's not
[00:27:13.360 --> 00:27:14.720]   overflow and underflow.
[00:27:14.720 --> 00:27:18.760]   And people act like it's something to be embarrassed of
[00:27:18.760 --> 00:27:20.000]   or unimportant.
[00:27:20.000 --> 00:27:23.360]   But actually, there's a reason why it makes things easier.
[00:27:23.360 --> 00:27:25.480]   There's a reason why it makes things cleaner.
[00:27:25.480 --> 00:27:27.880]   And I think it's important to have that in mind so that in
[00:27:27.880 --> 00:27:30.000]   the future, you can make other things easier and other things
[00:27:30.000 --> 00:27:32.280]   cleaner with log probabilities.
[00:27:32.280 --> 00:27:34.920]   So hopefully that was useful.
[00:27:34.920 --> 00:27:38.080]   I'll include some links to additional blog posts and
[00:27:38.080 --> 00:27:41.000]   material that goes into greater depth on these things.
[00:27:41.000 --> 00:27:43.560]   And with that, I'll take any questions
[00:27:43.560 --> 00:27:44.800]   that anybody's got.
[00:27:44.800 --> 00:27:50.920]   There are some questions in the Ask a Question tab,
[00:27:50.920 --> 00:27:54.200]   Charles, if you want to pop on that.
[00:27:54.200 --> 00:27:56.640]   And I don't see anything in the Slack so far.
[00:27:56.640 --> 00:27:57.880]   OK.
[00:27:57.880 --> 00:28:01.160]   OK.
[00:28:01.160 --> 00:28:05.880]   So question from Mani.
[00:28:05.880 --> 00:28:07.560]   Why not 1/P?
[00:28:07.560 --> 00:28:08.960]   Why log 1/P?
[00:28:08.960 --> 00:28:10.960]   Does log help magnify that value to be able
[00:28:10.960 --> 00:28:12.200]   to see it closer?
[00:28:12.200 --> 00:28:13.520]   So yeah, actually.
[00:28:13.520 --> 00:28:16.800]   So lots of the things that I said, 1/P would have worked
[00:28:16.800 --> 00:28:17.520]   pretty well.
[00:28:17.520 --> 00:28:20.040]   And there actually are lots of--
[00:28:20.040 --> 00:28:23.360]   logs and regular probabilities aren't actually the only two
[00:28:23.360 --> 00:28:25.680]   ways people think about these same ideas.
[00:28:25.680 --> 00:28:28.480]   And if I remember correctly, there are some ways that use
[00:28:28.480 --> 00:28:30.480]   inverse probabilities also.
[00:28:30.480 --> 00:28:34.720]   But the real bonus is what I probably got to a little bit
[00:28:34.720 --> 00:28:39.000]   out of after you asked this question.
[00:28:39.000 --> 00:28:45.320]   It's that this logarithm is what gives you the turning
[00:28:45.320 --> 00:28:46.600]   multiplication into addition.
[00:28:46.600 --> 00:28:48.880]   And that also means that it turns the things that were
[00:28:48.880 --> 00:28:51.000]   ratios into differences.
[00:28:51.000 --> 00:28:53.800]   And so things that used to be ratios spaced from each other
[00:28:53.800 --> 00:28:55.440]   become linearly spaced.
[00:28:55.440 --> 00:28:57.600]   So this is something we've all had to learn during the
[00:28:57.600 --> 00:28:59.280]   coronavirus pandemic.
[00:28:59.280 --> 00:29:05.280]   The people tend to show the cases by taking the logarithm
[00:29:05.280 --> 00:29:06.200]   of the number of cases.
[00:29:06.200 --> 00:29:08.320]   So they show log scale plots.
[00:29:08.320 --> 00:29:09.840]   And that's because with something that grows
[00:29:09.840 --> 00:29:13.360]   exponentially, it's less useful to think about the
[00:29:13.360 --> 00:29:15.880]   absolute differences between things and more useful to
[00:29:15.880 --> 00:29:17.320]   think about their ratios.
[00:29:17.320 --> 00:29:20.680]   And so another way of saying a lot of things I said in this
[00:29:20.680 --> 00:29:23.120]   talk is that probabilities are the kind of thing where you
[00:29:23.120 --> 00:29:26.320]   care about the ratio, not just the absolute differences.
[00:29:26.320 --> 00:29:37.520]   And so for the other things, I will include once this guy
[00:29:37.520 --> 00:29:42.000]   is posted, I will include some links to additional stuff
[00:29:42.000 --> 00:29:48.360]   about softmaxes and about uncertainty and inference.
[00:29:48.360 --> 00:29:54.800]   I will say that I taught a course on Bayesian inference
[00:29:54.800 --> 00:29:59.400]   at Berkeley in the fall that sort of presents things, some
[00:29:59.400 --> 00:30:00.520]   from this perspective.
[00:30:00.520 --> 00:30:04.280]   But actually, maybe a better one would be statistical
[00:30:04.280 --> 00:30:08.360]   rethinking by Richard McElrath.
[00:30:08.360 --> 00:30:17.960]   So that one is primarily in R. But people have translated it
[00:30:17.960 --> 00:30:22.920]   into TensorFlow probability at least, and I think also Pyro.
[00:30:22.920 --> 00:30:27.440]   And that one, if you Google that, you should be able to
[00:30:27.440 --> 00:30:27.760]   find it.
[00:30:27.760 --> 00:30:29.200]   There's videos online.
[00:30:29.200 --> 00:30:32.600]   And in addition, it's a textbook and exercises.
[00:30:32.600 --> 00:30:38.160]   So that would be where I would go to get some of this
[00:30:38.160 --> 00:30:39.400]   education.
[00:30:39.400 --> 00:30:41.720]   Cool.
[00:30:41.720 --> 00:30:43.160]   Thank you, Charles.
[00:30:43.160 --> 00:30:44.720]   Does anyone have any more questions for
[00:30:44.720 --> 00:30:48.480]   Charles before we--
[00:30:48.480 --> 00:30:51.320]   do you have something more for us next time, Charles?
[00:30:51.320 --> 00:30:52.880]   Or is this it?
[00:30:52.880 --> 00:30:54.000]   Well, that's the end of this series.
[00:30:54.000 --> 00:30:55.320]   So I'll have to come up with something
[00:30:55.320 --> 00:30:56.120]   else to talk about.
[00:30:56.120 --> 00:30:59.080]   I might talk about my thesis, but I'm kind of tired of
[00:30:59.080 --> 00:31:01.200]   talking about that.
[00:31:01.200 --> 00:31:05.200]   Maybe we'll give you a break and maybe have you on a couple
[00:31:05.200 --> 00:31:06.920]   of slides from now.
[00:31:06.920 --> 00:31:08.160]   Thank you, Charles.

