
[00:00:00.000 --> 00:00:07.320]   I think through the open source model, you can do things a bit differently.
[00:00:07.320 --> 00:00:13.520]   We have the inspiration of open source for infrastructure and database, with companies
[00:00:13.520 --> 00:00:20.120]   like Elastic, MongoDB, that have shown that you can, as a startup, empower the community
[00:00:20.120 --> 00:00:25.240]   in a way and create a thousand times more value than you would by building a proprietary
[00:00:25.240 --> 00:00:26.240]   tool.
[00:00:26.240 --> 00:00:30.960]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:30.960 --> 00:00:33.080]   and I'm your host, Lukas Biewald.
[00:00:33.080 --> 00:00:38.600]   Clem DeLong is CEO and co-founder of Hugging Face, the maker of Hugging Face Transformers
[00:00:38.600 --> 00:00:43.880]   Library, which is one of the most, maybe the most exciting libraries in machine learning
[00:00:43.880 --> 00:00:45.540]   right now.
[00:00:45.540 --> 00:00:50.280]   In making this library, he's had front row seats to all the advances in NLP over the
[00:00:50.280 --> 00:00:53.160]   last few years, which has been truly extraordinary.
[00:00:53.160 --> 00:00:56.120]   And I'm super excited to learn from him about that.
[00:00:56.120 --> 00:00:57.120]   All right.
[00:00:57.120 --> 00:01:01.840]   So my first question is probably a silly question, because almost anyone watching this or listening
[00:01:01.840 --> 00:01:05.360]   to this would know this, but what is Hugging Face?
[00:01:05.360 --> 00:01:12.240]   So we started Hugging Face a bit more than four and a half years ago, because we've been
[00:01:12.240 --> 00:01:15.400]   obsessed with natural language processing.
[00:01:15.400 --> 00:01:21.480]   So the field of machine learning that applies to text, and we've been lucky to create Hugging
[00:01:21.480 --> 00:01:29.520]   Face Transformers on GitHub that became the most popular open source NLP library that
[00:01:29.520 --> 00:01:34.960]   over 5,000 companies are using now to do any sort of NLP, right?
[00:01:34.960 --> 00:01:36.800]   From information extraction, right?
[00:01:36.800 --> 00:01:40.040]   If you have text, you want to extract information.
[00:01:40.040 --> 00:01:45.000]   So a platform like Chegg, for example, for homework is using that to extract information
[00:01:45.000 --> 00:01:46.520]   from homeworks.
[00:01:46.520 --> 00:01:49.000]   You can do text classification.
[00:01:49.000 --> 00:01:53.880]   So we have companies like Monzo, for example, that is using us to do customer support emails
[00:01:53.880 --> 00:01:54.880]   classification.
[00:01:54.880 --> 00:01:57.640]   They receive a customer support email.
[00:01:57.640 --> 00:02:02.160]   Does it relate to which product team, for example?
[00:02:02.160 --> 00:02:04.360]   Is that urgent, not urgent?
[00:02:04.360 --> 00:02:11.800]   To many other NLP tasks, like text generation for autocomplete, or really kind of like any
[00:02:11.800 --> 00:02:17.080]   single NLP task that you can think of.
[00:02:17.080 --> 00:02:23.960]   And we've been lucky to see adoption, not only from companies, but also from scientists,
[00:02:23.960 --> 00:02:31.960]   which have been using our platform to share their models with the world's test models
[00:02:31.960 --> 00:02:34.460]   of other scientists.
[00:02:34.460 --> 00:02:40.320]   We have almost 10,000 models that have been shared and almost a thousand datasets that
[00:02:40.320 --> 00:02:46.560]   have been shared on the platform to kind of like help scientists and practitioners build
[00:02:46.560 --> 00:02:51.960]   better NLP models and use that in their product or in their workflows.
[00:02:51.960 --> 00:02:57.600]   And so Hugging Face Transformers is the library that's super well-known, right?
[00:02:57.600 --> 00:03:02.960]   And then the platform is a place where you can go to use other people's models and publish
[00:03:02.960 --> 00:03:03.960]   your own models.
[00:03:03.960 --> 00:03:05.520]   Do I have that right?
[00:03:05.520 --> 00:03:06.520]   Yeah, exactly.
[00:03:06.720 --> 00:03:11.040]   We feel like a hybrid approach to building technology.
[00:03:11.040 --> 00:03:20.040]   We feel like you need kind of like the extensibility of open source and practicality of, for example,
[00:03:20.040 --> 00:03:21.860]   user interfaces, right?
[00:03:21.860 --> 00:03:28.000]   So we cover really kind of like the full range, meaning that if you're a company, you can
[00:03:28.000 --> 00:03:34.000]   do everything yourself from our open source, not talk to us, not even go to huggingface.co,
[00:03:34.000 --> 00:03:37.640]   do everything from pip install transformers, right?
[00:03:37.640 --> 00:03:43.220]   If you want a bit more help, you can use our hub to discover a new model, find a model
[00:03:43.220 --> 00:03:46.440]   that works for you, understand these models.
[00:03:46.440 --> 00:03:53.780]   To even in a more extreme way, if you're like a software engineer or if you're new to NLP
[00:03:53.780 --> 00:04:00.960]   or even new to machine learning, you can use our training and inference APIs to train and
[00:04:00.960 --> 00:04:06.720]   run models, and we're going to host this inference and this training for you to make it very,
[00:04:06.720 --> 00:04:12.160]   very simple so that you don't have to become an NLP expert to take advantage of the latest
[00:04:12.160 --> 00:04:14.960]   state of the art NLP models.
[00:04:14.960 --> 00:04:15.960]   That's so cool.
[00:04:15.960 --> 00:04:21.160]   I mean, I want to zoom in on Hugging Face Transformers first because it feels like it
[00:04:21.160 --> 00:04:24.920]   might be one of the most popular machine learning libraries of all time.
[00:04:24.920 --> 00:04:28.200]   I'm kind of curious what you attribute to that success.
[00:04:28.200 --> 00:04:32.400]   When did you start it and what were you thinking and what did you learn along the way?
[00:04:32.400 --> 00:04:37.960]   Yeah, I mean, it might be, I don't know if it's the biggest machine learning open source.
[00:04:37.960 --> 00:04:41.160]   It's definitely the fastest growing because it's fairly new.
[00:04:41.160 --> 00:04:45.360]   We released the first version of it two and a half years ago, which is not a long time
[00:04:45.360 --> 00:04:47.560]   ago in the grand scheme of open source, right?
[00:04:47.560 --> 00:04:48.560]   Yeah, for sure.
[00:04:48.560 --> 00:04:55.240]   If you look at all the most popular open source, you see that they usually need a very long
[00:04:55.240 --> 00:04:57.960]   time of maturation, right?
[00:04:57.960 --> 00:05:04.040]   So the grand scheme of open source Transformers is very much still a baby, but it grew really,
[00:05:04.040 --> 00:05:05.040]   really fast.
[00:05:05.040 --> 00:05:06.040]   It really blew up.
[00:05:06.040 --> 00:05:12.000]   We have over 42,000 GitHub stars, over a million PIP installs a month.
[00:05:12.000 --> 00:05:16.840]   I think we have 800 contributors to Transformers.
[00:05:16.840 --> 00:05:25.320]   And the main reason why I think it's successful is to me, because it really bridges the gap
[00:05:25.320 --> 00:05:32.760]   between science and production, which is something fairly new and that not a lot of open source
[00:05:32.760 --> 00:05:35.200]   and not a lot of companies manage to do.
[00:05:35.200 --> 00:05:41.280]   I strongly believe that machine learning compared to, you can call it like software engineering
[00:05:41.280 --> 00:05:49.320]   1.0 or software engineering or computer science, even if computer science has science in the
[00:05:49.320 --> 00:05:53.400]   name of it, it's not a science driven topic, right?
[00:05:53.400 --> 00:05:57.760]   If you look at good software engineers, they don't really read research papers.
[00:05:57.760 --> 00:06:01.960]   They don't really follow the science of computer science.
[00:06:01.960 --> 00:06:03.440]   Machine learning is very different.
[00:06:03.440 --> 00:06:05.980]   It's a science driven domain, right?
[00:06:05.980 --> 00:06:11.740]   It all starts from a couple of dozen key cast, kind of like NLP science teams all over the
[00:06:11.740 --> 00:06:19.380]   world that are creating new models like BERT, T5, Roberta, all these new models that you've
[00:06:19.380 --> 00:06:20.380]   heard from.
[00:06:20.380 --> 00:06:27.700]   And I think what we managed to do with Transformers is to give these researchers a tool that they
[00:06:27.700 --> 00:06:35.980]   like to share their models, to test models of others, to go deep into kind of like the
[00:06:35.980 --> 00:06:42.060]   internals of the architecture of these models, but at the same time, create an easy enough
[00:06:42.060 --> 00:06:51.420]   abstraction so that any NLP practitioner can literally use these models just a few hours
[00:06:51.420 --> 00:06:55.700]   after they've been released by the researchers, right?
[00:06:55.700 --> 00:07:01.340]   And so we created that there's some sort of like magic, some sort of like a network effect
[00:07:01.340 --> 00:07:04.400]   or some sort of magic when you bridge the two.
[00:07:04.400 --> 00:07:10.040]   We don't understand all the mechanics of it yet, but yeah, there's some sort of a network
[00:07:10.040 --> 00:07:11.680]   effect for it.
[00:07:11.680 --> 00:07:18.220]   Each time there's a new model released, like the researcher is releasing it within Transformers,
[00:07:18.220 --> 00:07:22.100]   people are hearing about it, they're talking about it, they want to use it, they test it
[00:07:22.100 --> 00:07:25.300]   on Transformers, they put it in production, it works.
[00:07:25.300 --> 00:07:26.860]   So they want to support it more.
[00:07:26.860 --> 00:07:32.940]   The scientist is happy that his research is seen, is used, is impactful.
[00:07:32.940 --> 00:07:35.880]   And so they want to create more and they want to share more.
[00:07:35.880 --> 00:07:44.260]   So there's this kind of like virtual cycle that I think allowed us to grow much, much
[00:07:44.260 --> 00:07:47.380]   faster than traditional open source.
[00:07:47.380 --> 00:07:52.420]   And that kind of like struck a chord on the market and on the field of machine learning.
[00:07:52.420 --> 00:07:58.100]   I guess as an entrepreneur, I'm always kind of fascinated by how these virtuous cycles
[00:07:58.100 --> 00:07:59.100]   get started.
[00:07:59.100 --> 00:08:04.140]   Like when you go back two and a half years ago, when you're just first starting the Transformers
[00:08:04.140 --> 00:08:09.020]   project, what was the problem you were trying to solve and what inspired you to even make
[00:08:09.020 --> 00:08:11.460]   an open source library like this?
[00:08:11.460 --> 00:08:15.100]   I could probably give you kind of like a smart, thoughtful answer.
[00:08:15.100 --> 00:08:17.100]   No, no, I want the real answer.
[00:08:17.100 --> 00:08:19.580]   Yeah, can you tell me what's actually happening?
[00:08:19.580 --> 00:08:23.320]   The real truth is that we didn't think much about it.
[00:08:23.320 --> 00:08:26.340]   We've been using open source for a while.
[00:08:26.340 --> 00:08:34.540]   We've always felt like in these fields, you're always standing on the shoulders of giants,
[00:08:34.540 --> 00:08:38.180]   of other people, beyond on the fields before.
[00:08:38.180 --> 00:08:44.220]   We've been used to this culture of when you do science, you publish a research paper.
[00:08:44.220 --> 00:08:51.260]   And for research in machine learning, you even want to publish open source rather than
[00:08:51.260 --> 00:08:53.500]   the paper.
[00:08:53.500 --> 00:09:00.620]   And so since day one at Hugging Face, we've always done a lot of things like in the open,
[00:09:00.620 --> 00:09:02.780]   sharing in open source.
[00:09:02.780 --> 00:09:10.040]   And here for Transformers, it started really, really simply with BERT that was released
[00:09:10.040 --> 00:09:11.340]   in TensorFlow.
[00:09:11.340 --> 00:09:17.260]   And Thomas, I was a co-founder and chief scientist, was like, "Oh, it's in TensorFlow.
[00:09:17.260 --> 00:09:19.500]   We need it in PyTorch."
[00:09:19.500 --> 00:09:27.060]   So I think two days after BERT was released, we open sourced PyTorch BERT.
[00:09:27.060 --> 00:09:30.660]   And that was literally the first name of the repository.
[00:09:30.660 --> 00:09:31.820]   And it blew up.
[00:09:31.820 --> 00:09:34.420]   People started using it like crazy.
[00:09:34.420 --> 00:09:40.020]   And then a few weeks after, I don't remember what model was released.
[00:09:40.020 --> 00:09:43.820]   I want to say Roberta, but no, Roberta was much later.
[00:09:43.820 --> 00:09:46.260]   But another model was released.
[00:09:46.260 --> 00:09:47.780]   Maybe it was GPT actually.
[00:09:47.780 --> 00:09:49.780]   I think it was the first GPT.
[00:09:49.780 --> 00:09:51.460]   It was released.
[00:09:51.460 --> 00:09:52.460]   And I think same thing.
[00:09:52.460 --> 00:09:56.580]   It was probably just in TensorFlow and we're like, "Okay, let's add it."
[00:09:56.580 --> 00:10:02.300]   And it felt like, "All right, let's make it so that it's easier for people to try both."
[00:10:02.300 --> 00:10:05.680]   Because they have different capabilities, good at different things.
[00:10:05.680 --> 00:10:12.380]   So we started thinking about what kind of abstraction we should build to make it easier.
[00:10:12.380 --> 00:10:16.980]   And very much like that, it went organically.
[00:10:16.980 --> 00:10:21.500]   And at some point, researchers were like, "I'm going to release a new model.
[00:10:21.500 --> 00:10:23.340]   Can I release it within Transformers?"
[00:10:23.340 --> 00:10:26.820]   And we say, "Okay, yeah, just do that."
[00:10:26.820 --> 00:10:33.460]   And they did that and kind of like a snowball, it became bigger and bigger and brought us
[00:10:33.460 --> 00:10:35.740]   to where we are now.
[00:10:35.740 --> 00:10:36.740]   That's a really cool story.
[00:10:36.740 --> 00:10:43.300]   I didn't realize that you were trying to port models from TensorFlow to PyTorch.
[00:10:43.300 --> 00:10:47.540]   Now you work with both TensorFlow and PyTorch, right?
[00:10:47.540 --> 00:10:48.540]   Yeah.
[00:10:48.540 --> 00:10:53.980]   Did you feel at the time, I guess, a preference for PyTorch or why was it important to you
[00:10:53.980 --> 00:10:56.620]   two and a half years ago to move something to PyTorch?
[00:10:56.620 --> 00:10:59.420]   I think the user base was different.
[00:10:59.420 --> 00:11:07.980]   So we've always been passionate about democratization or making something a bit obscure, a bit niche,
[00:11:07.980 --> 00:11:11.340]   making it available to more people.
[00:11:11.340 --> 00:11:17.620]   We feel like that's how you get the real power of technology is when you take something that
[00:11:17.620 --> 00:11:25.780]   is in the hands of just a happy few and you make it available for more people.
[00:11:25.780 --> 00:11:27.980]   So that was mainly a goal.
[00:11:27.980 --> 00:11:35.260]   There are people who are using TensorFlow, there are people who are using PyTorch.
[00:11:35.260 --> 00:11:39.160]   We wanted to make it available to people using PyTorch.
[00:11:39.160 --> 00:11:42.360]   We were using PyTorch ourselves extensively.
[00:11:42.360 --> 00:11:45.640]   We think it's an amazing framework.
[00:11:45.640 --> 00:11:49.000]   So yeah, we were happy to make it more available.
[00:11:49.000 --> 00:11:53.940]   And the funny thing is that as we got more and more popular, at some point we've seen
[00:11:53.940 --> 00:11:59.340]   the other movement in the sense that people were saying, at some point we were actually
[00:11:59.340 --> 00:12:01.700]   named PyTorch Transformers.
[00:12:01.700 --> 00:12:03.740]   And we started having a lot of people working on TensorFlow.
[00:12:03.740 --> 00:12:07.380]   It was like, "Guys, it's so unfair.
[00:12:07.380 --> 00:12:12.940]   Why can I just use Transformers if I'm using PyTorch?"
[00:12:12.940 --> 00:12:18.660]   And so that's when we extended to TensorFlow and dropped PyTorch Transformers, dropped
[00:12:18.660 --> 00:12:23.340]   the PyTorch in the name and became Transformers to support both.
[00:12:23.340 --> 00:12:27.500]   And it's been super interesting because if you look at our integration of PyTorch and
[00:12:27.500 --> 00:12:30.500]   TensorFlow, it's more comprehensive.
[00:12:30.500 --> 00:12:35.980]   It's more complete than just having half of it that is PyTorch and half of it that is
[00:12:35.980 --> 00:12:36.980]   TensorFlow.
[00:12:36.980 --> 00:12:42.800]   You can actually kind of like on the same workflow in a way, on your same kind of like
[00:12:42.800 --> 00:12:47.500]   machine learning workflow, you can do part of it in PyTorch.
[00:12:47.500 --> 00:12:53.100]   So for example, when you want to do more like the architecture side of it, PyTorch is really,
[00:12:53.100 --> 00:12:54.100]   really strong.
[00:12:54.100 --> 00:12:59.380]   But when you want to do kind of like serving, TensorFlow is integrated with a lot of tools
[00:12:59.380 --> 00:13:02.340]   that is heavily used in the industry.
[00:13:02.340 --> 00:13:08.420]   So in the same workflow, you can start building your model in PyTorch and then use it in TensorFlow
[00:13:08.420 --> 00:13:13.460]   within the library, which we think is pretty cool because it allows you to take advantage
[00:13:13.460 --> 00:13:19.860]   a little bit of the strengths and weaknesses of both frameworks.
[00:13:19.860 --> 00:13:22.820]   And so do you get a chance to use your own software anymore?
[00:13:22.820 --> 00:13:28.460]   Like do you do hugging face build applications ever at this point or are you just making
[00:13:28.460 --> 00:13:30.020]   these kind of tools for other people?
[00:13:30.020 --> 00:13:32.180]   Yeah, we play with them a lot.
[00:13:32.180 --> 00:13:39.860]   I think one of our most popular demo ever was something called Write with Transformers,
[00:13:39.860 --> 00:13:47.020]   which was some sort of kind of like text editor powered by some of the popular models of Transformers
[00:13:47.020 --> 00:13:52.060]   that got, I think something over a thousand books.
[00:13:52.060 --> 00:13:57.380]   The equivalent of a thousand books have been written with it.
[00:13:57.380 --> 00:14:02.920]   It's some sort of like what you have in your Gmail to complete, but except much more silly
[00:14:02.920 --> 00:14:04.420]   and creative.
[00:14:04.420 --> 00:14:09.740]   So it works really well when you have kind of like the syndrome of the, can you say that
[00:14:09.740 --> 00:14:13.660]   in English, syndrome of the white page when you don't know what to write?
[00:14:13.660 --> 00:14:14.660]   Oh yeah, yeah.
[00:14:14.660 --> 00:14:17.660]   I don't think we say it like that, but I understand the experience.
[00:14:17.660 --> 00:14:23.020]   In French we say syndrome de la feuille blanche, when you want to write, but you don't know
[00:14:23.020 --> 00:14:25.380]   what to write about.
[00:14:25.380 --> 00:14:32.380]   It's helping you like being more creative by suggesting kind of like a long, interesting
[00:14:32.380 --> 00:14:34.740]   text to it.
[00:14:34.740 --> 00:14:35.740]   That's really cool.
[00:14:35.740 --> 00:14:41.140]   So I wanted to ask you, I feel like you have a really interesting lens on all the different
[00:14:41.140 --> 00:14:44.220]   architectures for NLP.
[00:14:44.220 --> 00:14:48.700]   I guess, are you able to know kind of what the most popular architectures are and have
[00:14:48.700 --> 00:14:52.820]   you seen change in that over the last two and a half years?
[00:14:52.820 --> 00:14:53.820]   Yeah, yeah.
[00:14:53.820 --> 00:15:01.140]   We do can see kind of like the download kind of like volumes of models.
[00:15:01.140 --> 00:15:06.480]   So it's super interesting to see, especially when new models are coming up to see if they're
[00:15:06.480 --> 00:15:08.460]   successful or not.
[00:15:08.460 --> 00:15:12.220]   How many kind of like people using.
[00:15:12.220 --> 00:15:17.460]   Something that's been super interesting to us is that actually the number one downloaded
[00:15:17.460 --> 00:15:21.020]   model on the hub is Distilled BERT.
[00:15:21.020 --> 00:15:25.180]   So like a model that we distilled from BERT.
[00:15:25.180 --> 00:15:31.700]   But there's also a lot of variety in terms of usage of models, especially as I felt like
[00:15:31.700 --> 00:15:38.100]   over the years, they became in a way a bit more specialized.
[00:15:38.100 --> 00:15:43.740]   Even if they're still kind of like a general pre-trained language models.
[00:15:43.740 --> 00:15:52.420]   I feel like more and more as each new model came with some sort of an optimization that
[00:15:52.420 --> 00:16:03.860]   made it perform better, either on short or longer texts on generation tasks versus classification
[00:16:03.860 --> 00:16:08.820]   tasks, multi-language versus like mono-language.
[00:16:08.820 --> 00:16:15.660]   You start to see more and more diversity based on what people want to do with it and what
[00:16:15.660 --> 00:16:19.980]   kind of strengths and weakness do they value the most.
[00:16:19.980 --> 00:16:25.340]   A little bit like what I was talking about between PyTorch and TensorFlow.
[00:16:25.340 --> 00:16:33.500]   People are trying to not so much decide like which model is the best, which is kind of
[00:16:33.500 --> 00:16:39.780]   silly in my opinion, but which model is the best for which task, for which context, and
[00:16:39.780 --> 00:16:43.100]   then pick the right tool for the task.
[00:16:43.100 --> 00:16:46.860]   I guess for someone listening to this who doesn't have an NLP background, could you
[00:16:46.860 --> 00:16:51.860]   explain what BERT is and just what it does and maybe how Distilled BERT differs from
[00:16:51.860 --> 00:16:52.860]   that?
[00:16:52.860 --> 00:16:53.860]   Yeah.
[00:16:53.860 --> 00:16:59.820]   So the whole kind of like evolution in NLP started with a seminal paper called "Attention
[00:16:59.820 --> 00:17:09.380]   is All You Need," which was introducing this new architecture for NLP models based on transfer
[00:17:09.380 --> 00:17:10.500]   learning.
[00:17:10.500 --> 00:17:17.980]   And BERT was the first kind of like most popular of these new generation of models.
[00:17:17.980 --> 00:17:26.300]   And the way they work is in a simplistic way without getting too technical is that you
[00:17:26.300 --> 00:17:31.940]   train a model on a large piece of text on one specific task.
[00:17:31.940 --> 00:17:34.860]   So for BERT, for example, it's mask filling.
[00:17:34.860 --> 00:17:40.260]   You give it sentences, you remove a word in the middle of the sentence, for example, and
[00:17:40.260 --> 00:17:45.300]   then you train the model on predicting this missing word.
[00:17:45.300 --> 00:17:52.100]   And then you do that on a very large co-produce of text, usually a slice of the web.
[00:17:52.100 --> 00:17:57.820]   And then you get a model, a pre-trained model that has some kind of like understanding of
[00:17:57.820 --> 00:18:02.820]   text that you can then fine tune.
[00:18:02.820 --> 00:18:07.700]   And you know the name transfer learning, because you can go from one kind of like pre-training
[00:18:07.700 --> 00:18:11.540]   task to other fine tuning tasks.
[00:18:11.540 --> 00:18:16.020]   You can fine tune this model, for example, on classification, right?
[00:18:16.020 --> 00:18:22.340]   By giving it like a couple of thousands of examples of a text and classification for
[00:18:22.340 --> 00:18:26.020]   customer support emails that I was talking about.
[00:18:26.020 --> 00:18:28.660]   Classification urgent and not urgent, right?
[00:18:28.660 --> 00:18:37.020]   And after that, the model is surprisingly good at classifying a new text that you give
[00:18:37.020 --> 00:18:38.300]   it based on urgency.
[00:18:38.300 --> 00:18:43.740]   And it's going to tell you, okay, this message there's like 90% chance it's urgent based
[00:18:43.740 --> 00:18:47.740]   on what I've learned in the pre-training and in the fine tune.
[00:18:47.740 --> 00:18:54.380]   And so like, for example, with BERT, I guess, you have a model that can fill in missing
[00:18:54.380 --> 00:18:55.500]   words.
[00:18:55.500 --> 00:18:59.580]   How do you actually turn that into a model that say classifies customer support messages?
[00:18:59.580 --> 00:19:04.940]   Yeah, with fine tuning, you fine tune kind of like adding a layer.
[00:19:04.940 --> 00:19:09.940]   You fine tune this model to perform on your specific task.
[00:19:09.940 --> 00:19:14.660]   And that's kind of like a more kind of like a long-term way.
[00:19:14.660 --> 00:19:23.580]   I think that's a very interesting way of doing machine learning because intuitively you almost
[00:19:23.580 --> 00:19:31.380]   feel like it's the right way to do machine learning in the sense that what we've seen
[00:19:31.380 --> 00:19:37.660]   in the past with machine learning and especially for startups, a lot of them have kind of like
[00:19:37.660 --> 00:19:42.820]   sold this dream of doing machine learning and doing some sort of like a data network
[00:19:42.820 --> 00:19:45.380]   effect on machine learning.
[00:19:45.380 --> 00:19:49.260]   Because there's this assumption that you're going to give more data to the model and it's
[00:19:49.260 --> 00:19:51.100]   going to perform better.
[00:19:51.100 --> 00:19:57.500]   And I think that's true, but the challenge has always been that you have more data and
[00:19:57.500 --> 00:20:05.660]   so your model performs incrementally better, but only on what you're able to do already.
[00:20:05.660 --> 00:20:13.340]   So if you're doing time series prediction, maybe you have like 1 billion data points
[00:20:13.340 --> 00:20:17.200]   and your model performs at 90% accuracy.
[00:20:17.200 --> 00:20:23.720]   You add like maybe 9 billion, 10 billion additional data points and your model is going to perform
[00:20:23.720 --> 00:20:28.900]   at 90.5% accuracy.
[00:20:28.900 --> 00:20:29.900]   And that's great.
[00:20:29.900 --> 00:20:31.180]   I mean, that's good improvement.
[00:20:31.180 --> 00:20:38.580]   That's something you need, but it doesn't give the kind of increased performance that
[00:20:38.580 --> 00:20:44.420]   you really expecting from a typical network effect in the sense that it doesn't make your
[00:20:44.420 --> 00:20:50.020]   result like 100x, 10x, 100x better than without it.
[00:20:50.020 --> 00:20:56.780]   With transfer learning, it's a bit different because you not only kind of like improve
[00:20:56.780 --> 00:21:05.700]   incrementally the accuracy on one task, you give it more ability to solve other tasks.
[00:21:05.700 --> 00:21:12.780]   And so you actually not only increase the accuracy, but you increase the capabilities
[00:21:12.780 --> 00:21:15.460]   of what your model is able to do.
[00:21:15.460 --> 00:21:24.260]   And so I won't go into kind of like the crazy Musk type kind of prediction, but if you take
[00:21:24.260 --> 00:21:32.620]   actually Elon Musk kind of like open AI founding kind of like story where he's saying like,
[00:21:32.620 --> 00:21:38.380]   we need to bring the whole community together to contribute to something open source for
[00:21:38.380 --> 00:21:39.380]   everyone.
[00:21:39.380 --> 00:21:44.580]   Intuitively, you could think that could come with actually transfer learning in the sense
[00:21:44.580 --> 00:21:53.180]   that you could envision a world where every single company is contributing with their
[00:21:53.180 --> 00:22:00.180]   data sets, with their compute, with their weights, their machine learning model weights
[00:22:00.180 --> 00:22:11.300]   to build these giant kind of like open source models that would be able to do 100x more
[00:22:11.300 --> 00:22:16.500]   things than what each of these companies could do alone.
[00:22:16.500 --> 00:22:22.180]   I don't know if we're going to get there in the foreseeable future, but I feel like that's
[00:22:22.180 --> 00:22:27.260]   in terms of concepts, that's something interesting to look at when you think about transfer learning
[00:22:27.260 --> 00:22:32.860]   as opposed to the other techniques of machine learning.
[00:22:32.860 --> 00:22:38.700]   I guess, did you have a feeling about open AI not releasing the weights for the early
[00:22:38.700 --> 00:22:42.700]   GPT models or I guess any of the GPT models?
[00:22:42.700 --> 00:22:43.700]   Yeah.
[00:22:43.700 --> 00:22:52.420]   So GPT, GPT-2, I think they had the version in between were open source, right?
[00:22:52.420 --> 00:22:58.220]   And it's in Transformers and we have a lot of companies using them.
[00:22:58.220 --> 00:23:05.380]   There are probably more companies using like GPT-2 through Transformers than GPT-3 today.
[00:23:05.380 --> 00:23:14.300]   They're a private company, so I totally respect their strategy not to open source the models
[00:23:14.300 --> 00:23:15.300]   that they built.
[00:23:15.300 --> 00:23:19.540]   I think they've done an amazing job with GPT-3.
[00:23:19.540 --> 00:23:23.740]   It's a great model for everything when you want to do text generation.
[00:23:23.740 --> 00:23:25.660]   It's really useful.
[00:23:25.660 --> 00:23:32.620]   I'm really thankful for all the work they've done democratizing the capabilities of NLP.
[00:23:32.620 --> 00:23:37.500]   As our goal is to democratize NLP, I feel like what they've done promoting it into more
[00:23:37.500 --> 00:23:40.020]   like of the startup community in a way.
[00:23:40.020 --> 00:23:45.780]   A lot of people realized with their communication that you could do so much more than what we've
[00:23:45.780 --> 00:23:49.260]   been doing so far with NLP, which is great.
[00:23:49.260 --> 00:23:59.620]   I think it participated to the development of the ecosystem and putting NLP in the kind
[00:23:59.620 --> 00:24:03.060]   of like a spotlight, which has been really great.
[00:24:03.060 --> 00:24:08.780]   And we see a lot of companies starting to use GPT-3 and then obviously it's expensive.
[00:24:08.780 --> 00:24:10.340]   It's not really extensible.
[00:24:10.340 --> 00:24:14.180]   You can't really update it for your own use case.
[00:24:14.180 --> 00:24:19.580]   And it's hard to be a sense of technological competitive advantage when you build on top
[00:24:19.580 --> 00:24:23.460]   of an API, proprietary API from someone else.
[00:24:23.460 --> 00:24:28.980]   We see a lot of companies using GPT-3 and then discovering NLP and then coming to our
[00:24:28.980 --> 00:24:33.260]   tools and the same way happens, I'm sure it's the other way around.
[00:24:33.260 --> 00:24:37.180]   Some people start with our tools, our open source, and then they decide to kind of like
[00:24:37.180 --> 00:24:45.620]   use something a bit more off the shelf like GPT-3 or like Google NLP services or AWS Comprehend.
[00:24:45.620 --> 00:24:52.660]   Providing an API for NLP has been around from these companies too.
[00:24:52.660 --> 00:24:57.700]   So I think everyone is part of the same ecosystem that is growing.
[00:24:57.700 --> 00:25:00.180]   So that's super exciting.
[00:25:00.180 --> 00:25:05.980]   Do you feel like there's a difference in the GPT approach versus the BERT approach that
[00:25:05.980 --> 00:25:06.980]   you were talking about?
[00:25:06.980 --> 00:25:12.340]   I mean, GPT has been very high profile and the text generation is really impressive.
[00:25:12.340 --> 00:25:16.380]   Do you feel like OpenAI is doing something kind of fundamentally different there?
[00:25:16.380 --> 00:25:17.380]   Yeah.
[00:25:17.380 --> 00:25:20.900]   So there are both transformer models, right?
[00:25:20.900 --> 00:25:26.140]   They kind of like same technique with slightly different architectures, right?
[00:25:26.140 --> 00:25:33.060]   So for example, when BERT is doing mask failing, GPT is doing language modeling.
[00:25:33.060 --> 00:25:35.540]   So a next word prediction.
[00:25:35.540 --> 00:25:36.540]   So it's a bit different.
[00:25:36.540 --> 00:25:41.500]   That's why the text generation capabilities are so much stronger.
[00:25:41.500 --> 00:25:43.660]   It has its limitations too.
[00:25:43.660 --> 00:25:48.620]   For example, if you want to do classification, you shouldn't do it with GPT.
[00:25:48.620 --> 00:25:50.020]   It doesn't make sense at all.
[00:25:50.020 --> 00:25:58.500]   So yeah, they solve different use cases with kind of like a slight variations of the architecture.
[00:25:58.500 --> 00:26:03.380]   We've had people started reproducing GPT.
[00:26:03.380 --> 00:26:08.340]   I mean, we've had GPT2 and a team called Elucer.
[00:26:08.340 --> 00:26:15.540]   I don't really know how to pronounce it, but I released GPT Neo a few days ago, which has
[00:26:15.540 --> 00:26:21.100]   the same architecture as a GPT3, just with less weights for the moment, but they intend
[00:26:21.100 --> 00:26:23.420]   to kind of like grow the weights.
[00:26:23.420 --> 00:26:32.180]   I think the size of their model is the equivalent of the smaller GPT3 that OpenAI is providing
[00:26:32.180 --> 00:26:33.740]   through an API today.
[00:26:33.740 --> 00:26:34.980]   It works well.
[00:26:34.980 --> 00:26:41.500]   It's interesting to see the power of the open source community.
[00:26:41.500 --> 00:26:48.140]   I think one of my fundamental conviction is that on a field like NLP or machine learning
[00:26:48.140 --> 00:26:53.860]   in general, the worst position to be in is to compete with the whole science and open
[00:26:53.860 --> 00:26:54.860]   source fields.
[00:26:54.860 --> 00:26:55.860]   Sure.
[00:26:55.860 --> 00:26:58.820]   Just because I've been in this position before.
[00:26:58.820 --> 00:27:02.420]   Actually, the first startup I worked for, we were doing machine learning for computer
[00:27:02.420 --> 00:27:03.420]   vision back in Paris.
[00:27:03.420 --> 00:27:07.980]   French, obviously, as you can hear from my accent.
[00:27:07.980 --> 00:27:13.980]   But so competing against the science fields and the open source fields on such a fast
[00:27:13.980 --> 00:27:21.860]   moving topic is a difficult position to be in, because I think you have hundreds of research
[00:27:21.860 --> 00:27:29.780]   labs at larger organizations or at universities that are not so much kind of like potentially
[00:27:29.780 --> 00:27:35.060]   each one better than what you can do at the startup, but just there are so many of them
[00:27:35.060 --> 00:27:41.300]   that when you can do just one iteration, you have hundreds of out there doing one iteration
[00:27:41.300 --> 00:27:42.300]   too.
[00:27:42.300 --> 00:27:49.260]   So you can outpace them and beat the state of the art for a few days.
[00:27:49.260 --> 00:27:55.260]   Then someone who started just a few days after you is catching up and then you're not kind
[00:27:55.260 --> 00:27:56.980]   of like ahead anymore.
[00:27:56.980 --> 00:28:02.740]   So yeah, we've taken a very, very different approach by instead of trying to compete,
[00:28:02.740 --> 00:28:09.140]   I think with open source and with the science field, we're trying more to empower it in
[00:28:09.140 --> 00:28:10.140]   a way.
[00:28:10.140 --> 00:28:15.300]   And I think through the open source model, you can do things a bit differently.
[00:28:15.300 --> 00:28:21.540]   We have kept the inspiration of open source for infrastructure and database, with companies
[00:28:21.540 --> 00:28:28.100]   like Elastic, MongoDB, that have shown that you can, as a startup, empower the community
[00:28:28.100 --> 00:28:33.260]   in a way and create like a thousand times more value than you would by building a proprietary
[00:28:33.260 --> 00:28:34.540]   tool.
[00:28:34.540 --> 00:28:41.020]   And that you don't have to capture a hundred percent of the value that you create, that
[00:28:41.020 --> 00:28:49.440]   you can be okay creating an immense value and just capturing one person of it to monetize,
[00:28:49.440 --> 00:28:51.720]   to make your company sustainable.
[00:28:51.720 --> 00:28:58.880]   And that can still kind of make a large public company, like in the case of MongoDB for that,
[00:28:58.880 --> 00:29:04.840]   both has kind of like this open source core, but at the same time can grow an organization
[00:29:04.840 --> 00:29:06.520]   and be sustainable.
[00:29:06.520 --> 00:29:09.280]   And I don't see why it should be different for machine learning.
[00:29:09.280 --> 00:29:13.200]   We haven't seen a lot of large open source machine learning companies yet.
[00:29:13.200 --> 00:29:18.320]   For me, it's more a matter of how early the technology is.
[00:29:18.320 --> 00:29:23.920]   It's too early to have large open source machine learning companies because five years ago,
[00:29:23.920 --> 00:29:27.960]   nobody was using machine learning, but it's going to come.
[00:29:27.960 --> 00:29:33.840]   I think I wouldn't be surprised in five, 10 years, you'd have kind of like a one, two,
[00:29:33.840 --> 00:29:40.080]   three, four, five, 10 massive open source machine learning companies.
[00:29:40.080 --> 00:29:47.080]   I guess you've had really front row seats to the cutting edge of NLP over the last couple
[00:29:47.080 --> 00:29:48.320]   of years.
[00:29:48.320 --> 00:29:53.640]   Do you feel like the applications have changed with these models getting more powerful and
[00:29:53.640 --> 00:29:54.640]   useful?
[00:29:54.640 --> 00:29:58.080]   Are there things you see people doing now that you wouldn't have seen people doing three
[00:29:58.080 --> 00:29:59.080]   years ago?
[00:29:59.080 --> 00:30:05.800]   Yeah, honestly, I think out of the 5,000 companies that are using transformers, I mean, the vast,
[00:30:05.800 --> 00:30:12.840]   vast majority of it, I mean, it's hard to tell, but we see a lot of them that are using
[00:30:12.840 --> 00:30:20.680]   transformers in production, and I would say that most of them weren't using NLP in production
[00:30:20.680 --> 00:30:22.680]   five years ago.
[00:30:22.680 --> 00:30:30.480]   A lot of these are new use cases that either weren't possible before, so the companies
[00:30:30.480 --> 00:30:36.080]   were just not doing it, or really were performed by humans.
[00:30:36.080 --> 00:30:41.280]   Moderation, for example, is a good example of that.
[00:30:41.280 --> 00:30:48.640]   Customer support classification, as I was saying, it's replacing a very manual process.
[00:30:48.640 --> 00:30:52.840]   Autocomplete is really, really big in Gmail.
[00:30:52.840 --> 00:30:59.400]   It's been my biggest productivity enhancement, I feel like, in the past few months is using
[00:30:59.400 --> 00:31:02.880]   Gmail Autocomplete to basically write just half of my emails.
[00:31:02.880 --> 00:31:09.680]   Now most of the search engine are mostly powered by NLP and by transformer models.
[00:31:09.680 --> 00:31:15.480]   Google now is saying that most of their queries are powered by transformers.
[00:31:15.480 --> 00:31:21.040]   So arguably, it's the most popular consumer product out there.
[00:31:21.040 --> 00:31:27.760]   So yeah, I think it's changing so many products, the way products are built.
[00:31:27.760 --> 00:31:33.520]   I'm really interested in it, and that's why also seeing GPT-3 kind of like promoting NLP
[00:31:33.520 --> 00:31:39.600]   into the startup world is super interesting, because I think it's very game changer when
[00:31:39.600 --> 00:31:48.320]   you have companies starting building products from scratch, leveraging NLP, because I think
[00:31:48.320 --> 00:31:52.200]   you build differently, right?
[00:31:52.200 --> 00:31:55.800]   When you start kind of like building a legal...
[00:31:55.800 --> 00:32:02.040]   You can think of basically every company today, and it's really fun to think, what if these
[00:32:02.040 --> 00:32:08.720]   companies started today with today's NLP capabilities?
[00:32:08.720 --> 00:32:13.000]   And you see that you have so many ideas for them to do things differently.
[00:32:13.000 --> 00:32:15.760]   You take DocuSign, right?
[00:32:15.760 --> 00:32:21.640]   What if DocuSign with kind of like analysis of documents started today with NLP?
[00:32:21.640 --> 00:32:22.640]   You think Twitter-
[00:32:22.640 --> 00:32:23.640]   Wait, wait.
[00:32:23.640 --> 00:32:27.760]   Tell me about DocuSign, because what I do with DocuSign is I get a message and then
[00:32:27.760 --> 00:32:30.000]   I click sign, and then I sign the thing.
[00:32:30.000 --> 00:32:35.280]   So what would be different about DocuSign if it was started with all the technology
[00:32:35.280 --> 00:32:36.280]   available today?
[00:32:36.280 --> 00:32:38.440]   I don't know.
[00:32:38.440 --> 00:32:42.240]   It would give you so much analysis of the...
[00:32:42.240 --> 00:32:45.240]   There would be a too long didn't read-
[00:32:45.240 --> 00:32:46.240]   For the contract?
[00:32:46.240 --> 00:32:47.240]   For the contract.
[00:32:47.240 --> 00:32:56.400]   So instead of having to read five different pages, five page long document, you would
[00:32:56.400 --> 00:33:02.120]   have an automatically generated summary of the document.
[00:33:02.120 --> 00:33:03.120]   I see.
[00:33:03.120 --> 00:33:09.920]   It would highlight in green or red the interesting part in the documents.
[00:33:09.920 --> 00:33:16.160]   When you see, oh, there's a big kind of money shot, that's where you define how much money
[00:33:16.160 --> 00:33:17.160]   you're going to make.
[00:33:17.160 --> 00:33:18.160]   Yeah, right.
[00:33:18.160 --> 00:33:22.360]   Big green flashing lights.
[00:33:22.360 --> 00:33:24.520]   Be careful about...
[00:33:24.520 --> 00:33:34.960]   When there's a small star that says everything that we wrote before is completely not kept,
[00:33:34.960 --> 00:33:36.800]   it doesn't work in that case.
[00:33:36.800 --> 00:33:42.440]   The small conditions, you would put big red flashing light.
[00:33:42.440 --> 00:33:43.440]   Be careful.
[00:33:43.440 --> 00:33:46.440]   Here is, they're trying to screw you here.
[00:33:46.440 --> 00:33:48.120]   I love it.
[00:33:48.120 --> 00:33:49.760]   Things like that.
[00:33:49.760 --> 00:33:50.760]   Okay.
[00:33:50.760 --> 00:33:51.760]   That was so fun.
[00:33:51.760 --> 00:33:56.280]   Tell me about Twitter started with this technology available.
[00:33:56.280 --> 00:33:58.960]   So what could Twitter do?
[00:33:58.960 --> 00:34:03.920]   So first it would do the feed completely different.
[00:34:03.920 --> 00:34:12.640]   It would not show you tweets because they're popular or tweets because they're, I mean,
[00:34:12.640 --> 00:34:18.560]   not popular, I would say controversial, but it would show you tweets that you would relate
[00:34:18.560 --> 00:34:29.640]   to, tweets that you would be interested in based on what you tweeted before.
[00:34:29.640 --> 00:34:36.960]   Hopefully it would be able to moderate things a bit better, avoid more biases, avoid more
[00:34:36.960 --> 00:34:45.440]   kind of violence, inappropriate racism and kind of bad kind of behaviors like that.
[00:34:45.440 --> 00:34:46.520]   What else could it be?
[00:34:46.520 --> 00:34:53.680]   I would have wanted obviously an edit button, but I don't know if NLP would help with that.
[00:34:53.680 --> 00:34:56.680]   A what button?
[00:34:56.680 --> 00:34:57.680]   No.
[00:34:57.680 --> 00:35:04.320]   And you know, like this famous thing that for kind of like ages, everyone has been asking
[00:35:04.320 --> 00:35:06.320]   for like an edit button.
[00:35:06.320 --> 00:35:07.320]   Oh, edit button.
[00:35:07.320 --> 00:35:08.320]   Oh yeah, yeah.
[00:35:08.320 --> 00:35:09.320]   Right, right.
[00:35:09.320 --> 00:35:14.520]   But it wouldn't be NLP powered, but still if they started today, I would add that.
[00:35:14.520 --> 00:35:15.520]   What else?
[00:35:15.520 --> 00:35:21.360]   Do you have any idea of what they would do differently with NLP today?
[00:35:21.360 --> 00:35:27.360]   Well honestly, I don't know how you feel about this, but when I look at the text generation
[00:35:27.360 --> 00:35:32.720]   technology, the NLP technology, I mean, that was the field I actually started in 15 years
[00:35:32.720 --> 00:35:34.600]   ago or more.
[00:35:34.600 --> 00:35:41.520]   And I almost feel like the thing that's intriguing is the lack of applications for how amazing
[00:35:41.520 --> 00:35:43.640]   the technology seems to me.
[00:35:43.640 --> 00:35:49.960]   I remember the Turing test was this thing of like, if you could converse with the, I
[00:35:49.960 --> 00:35:53.400]   forget exactly the framing, but it's like converse the computer for like 10 minutes
[00:35:53.400 --> 00:35:55.740]   and you can't tell if it's a human.
[00:35:55.740 --> 00:35:58.120]   Maybe we have like AGI at that point.
[00:35:58.120 --> 00:36:01.360]   And it seems like that seems so impossible.
[00:36:01.360 --> 00:36:06.080]   And now it seems like we'll pass it sometime soon.
[00:36:06.080 --> 00:36:13.040]   I mean, there's variants of it, but I feel more and more like it's probably computers
[00:36:13.040 --> 00:36:17.640]   could trick me into thinking that I'm talking to a person with just GPV free or another
[00:36:17.640 --> 00:36:18.840]   text generation model.
[00:36:18.840 --> 00:36:25.320]   But I actually feel like I don't engage with totally new NLP applications yet.
[00:36:25.320 --> 00:36:27.600]   And I kind of wonder why that is.
[00:36:27.600 --> 00:36:28.600]   Yeah.
[00:36:28.600 --> 00:36:30.920]   I mean, I wouldn't agree with you.
[00:36:30.920 --> 00:36:36.520]   I think the usage of it is really like everywhere right now.
[00:36:36.520 --> 00:36:45.360]   I, yeah, there are not a lot of products that don't start to use some NLP, right?
[00:36:45.360 --> 00:36:48.160]   Maybe it's just more subtle than.
[00:36:48.160 --> 00:36:49.160]   Yeah.
[00:36:49.160 --> 00:36:50.160]   Yeah.
[00:36:50.160 --> 00:36:56.560]   Maybe it's less in your face in the sense that it hasn't been these big kind of like
[00:36:56.560 --> 00:37:03.000]   conversational AI interfaces that took over in a way, right?
[00:37:03.000 --> 00:37:08.040]   For a very long time, it was kind of like almost a popular and kind of like mainstream
[00:37:08.040 --> 00:37:11.120]   face in a way of NLP, right?
[00:37:11.120 --> 00:37:16.240]   People think NLP, Siri, Alexa, in a way.
[00:37:16.240 --> 00:37:20.760]   And that's true that we haven't seen that picking up, right?
[00:37:20.760 --> 00:37:26.400]   Chatbots haven't proved to be very, very good yet.
[00:37:26.400 --> 00:37:32.760]   And we're not there yet in the capabilities in really kind of like solving real problems.
[00:37:32.760 --> 00:37:40.160]   But I think it's became adopted in a way more subtle way, in a way more kind of like incremental
[00:37:40.160 --> 00:37:43.720]   way compared to his existing use cases.
[00:37:43.720 --> 00:37:45.520]   You're probably using Google every day.
[00:37:45.520 --> 00:37:50.080]   And that's true that maybe you don't see much of the difference between the search results
[00:37:50.080 --> 00:37:51.720]   before and now.
[00:37:51.720 --> 00:37:57.000]   But the reality is, is that it's the most mainstream, most used product of all time
[00:37:57.000 --> 00:37:58.920]   that most of the people are using every day.
[00:37:58.920 --> 00:38:00.920]   And it's powered by modern NLP.
[00:38:00.920 --> 00:38:02.800]   It's powered by Transformer.
[00:38:02.800 --> 00:38:10.960]   But it's not as kind of like, yeah, maybe a word is groundbreaking in terms of like
[00:38:10.960 --> 00:38:16.360]   experience changes as you could have expected, right?
[00:38:16.360 --> 00:38:23.120]   I think one of the challenges of NLP is that because language has been so much of a human
[00:38:23.120 --> 00:38:32.840]   topic for so long, in a way, it carries all these kind of like association with AI, right?
[00:38:32.840 --> 00:38:39.000]   And kind of like AGI and kind of like almost like this machine intelligence.
[00:38:39.000 --> 00:38:45.680]   And obviously, if you look at all the sci-fi with her, you associate that a little bit
[00:38:45.680 --> 00:38:50.400]   with NLP and that's kind of like what you could have expected from NLP.
[00:38:50.400 --> 00:38:56.120]   And the reality has been more kind of like productivity improvements behind the scene
[00:38:56.120 --> 00:39:01.720]   that you don't really feel or see that much as a user.
[00:39:01.720 --> 00:39:02.760]   It's true.
[00:39:02.760 --> 00:39:05.760]   Are you optimistic about chat interfaces?
[00:39:05.760 --> 00:39:06.760]   I am.
[00:39:06.760 --> 00:39:10.480]   I think what most of us got wrong.
[00:39:10.480 --> 00:39:17.240]   And I mean, we started by building an AI friend or like a fun conversational AI with Hugging
[00:39:17.240 --> 00:39:18.240]   Face.
[00:39:18.240 --> 00:39:21.520]   And we were body-hugging faces, as I was saying, we were obsessed with NLP and we were like,
[00:39:21.520 --> 00:39:24.840]   okay, what's the most challenging problem today?
[00:39:24.840 --> 00:39:31.040]   Open domain conversational AI, building this kind of like AI that can chat about everything,
[00:39:31.040 --> 00:39:37.040]   about the last sports game, about your last kind of like relationship and really talk
[00:39:37.040 --> 00:39:38.040]   about everything.
[00:39:38.040 --> 00:39:40.320]   And we're like, okay, that's the most difficult thing.
[00:39:40.320 --> 00:39:41.400]   We're going to do that.
[00:39:41.400 --> 00:39:43.880]   And it didn't work out.
[00:39:43.880 --> 00:39:49.040]   So I think what we got wrong and what most people are getting wrong is probably like
[00:39:49.040 --> 00:39:57.160]   the timing in a way, in the sense that conversation and especially like open domain conversation,
[00:39:57.160 --> 00:40:01.720]   the way we're doing it now is extremely hard.
[00:40:01.720 --> 00:40:07.760]   It's almost kind of like the ultimate NLP task because you need to be able to do so
[00:40:07.760 --> 00:40:12.400]   many NLP tasks together at the same time, ranking them.
[00:40:12.400 --> 00:40:17.560]   I need to be able, when you're talking to me, to extract information, to understand,
[00:40:17.560 --> 00:40:22.120]   classify your intent, classify the meaning of your sentence, understand the emotion of
[00:40:22.120 --> 00:40:23.120]   it.
[00:40:23.120 --> 00:40:26.520]   If your tone is changing, then it means different things.
[00:40:26.520 --> 00:40:32.640]   So I think we're going to get to better conversational AI ultimately.
[00:40:32.640 --> 00:40:37.960]   I don't know if it's in five years, if it's in 10 years, if it's longer, but I think we're
[00:40:37.960 --> 00:40:39.040]   going to get there.
[00:40:39.040 --> 00:40:44.420]   It's already solving some kind of like more vertical problems with sometimes customer
[00:40:44.420 --> 00:40:46.140]   support chatbots.
[00:40:46.140 --> 00:40:52.080]   I think Rasa in the open source community is doing a really great job with that.
[00:40:52.080 --> 00:40:58.960]   I think we won't get tomorrow to the AI who you can chat with about everything, kind of
[00:40:58.960 --> 00:41:03.360]   like what we started Hugging Face with, but ultimately I think we'll get there.
[00:41:03.360 --> 00:41:11.920]   And that's when, in terms of user experience, you're going to realize it's different at
[00:41:11.920 --> 00:41:16.880]   that time, but it's probably going to take much more time than what we're expecting.
[00:41:16.880 --> 00:41:17.880]   Cool.
[00:41:17.880 --> 00:41:22.000]   Well, we always end with two questions, so I'd love to get those in in the last couple
[00:41:22.000 --> 00:41:23.360]   of minutes we have.
[00:41:23.360 --> 00:41:26.600]   We always ask, what's an underrated topic in machine learning?
[00:41:26.600 --> 00:41:29.920]   Or maybe in your case, what's an underrated topic in NLP?
[00:41:29.920 --> 00:41:32.720]   Something that you might work on if you didn't have a day job.
[00:41:32.720 --> 00:41:33.720]   That's a good question.
[00:41:33.720 --> 00:41:39.280]   I mean, something that I've been super excited about in the past few weeks is the field of
[00:41:39.280 --> 00:41:40.280]   speech.
[00:41:40.280 --> 00:41:47.280]   So like speech to text, text to speech, because I feel like it's been kind of like a little
[00:41:47.280 --> 00:41:51.160]   bit like NLP a few years ago.
[00:41:51.160 --> 00:41:56.520]   It's been kind of relegated as some sort of kind of like a little bit boring field with
[00:41:56.520 --> 00:41:59.320]   not so many people working in it.
[00:41:59.320 --> 00:42:05.000]   And I feel like thanks to a couple of research teams, especially the team of Alex Cono at
[00:42:05.000 --> 00:42:11.120]   FAIR with Wave2Vec, you're starting to see new advances, actually leveraging transformer
[00:42:11.120 --> 00:42:15.400]   models that are bringing new capabilities.
[00:42:15.400 --> 00:42:17.000]   So I'm pretty excited about it.
[00:42:17.000 --> 00:42:21.960]   I think there's going to be some sort of resurgence of it and kind of like a leapfrog in terms
[00:42:21.960 --> 00:42:28.160]   of quality, not only in English, but what's interesting is that it's also in other languages.
[00:42:28.160 --> 00:42:34.480]   We hosted a few weeks ago a community sprint at Sucking Face with over 300 participants
[00:42:34.480 --> 00:42:41.160]   who contributed models, speech to text for almost a hundred low resource languages.
[00:42:41.160 --> 00:42:43.920]   And so it's been pretty cool to see the response of the community.
[00:42:43.920 --> 00:42:50.320]   So I think there's going to be cool things happening in the coming months in speech,
[00:42:50.320 --> 00:42:54.040]   which is going to unlock new use cases.
[00:42:54.040 --> 00:42:59.720]   Because if you think that you can combine speech with NLP, you can start to do really
[00:42:59.720 --> 00:43:00.720]   cool stuff.
[00:43:00.720 --> 00:43:06.840]   We were talking about what if the product is built today, if Zoom was built today with
[00:43:06.840 --> 00:43:10.800]   good speech to text and NLP, you could do pretty cool stuff too.
[00:43:10.800 --> 00:43:17.280]   When I'm seeing something cheery, it should be automatic clapping because otherwise everyone
[00:43:17.280 --> 00:43:18.560]   can't mute.
[00:43:18.560 --> 00:43:23.200]   That's the problem with the current Zooms is that with everyone muted, when I say something
[00:43:23.200 --> 00:43:26.560]   to cheer, I'm the only one cheering.
[00:43:26.560 --> 00:43:33.520]   Or when you say hoorah, there should be emoji showers of celebratory emojis or things like
[00:43:33.520 --> 00:43:34.520]   that.
[00:43:34.520 --> 00:43:37.240]   So yeah, I'm excited for speech.
[00:43:37.240 --> 00:43:41.440]   If you haven't checked the field lately, you should definitely check it.
[00:43:41.440 --> 00:43:42.840]   There are cool things happening.
[00:43:42.840 --> 00:43:44.000]   Very cool.
[00:43:44.000 --> 00:43:48.840]   And the final question, and I feel like you're in a unique place to see this, is what's the
[00:43:48.840 --> 00:43:55.320]   hardest part or what's some unexpected challenges in just getting a model from thinking about
[00:43:55.320 --> 00:43:58.200]   it to deploy it into production?
[00:43:58.200 --> 00:44:01.660]   And I guess you have a unique point of view here where you actually have a platform that
[00:44:01.660 --> 00:44:03.600]   makes this super easy.
[00:44:03.600 --> 00:44:06.800]   Are there still challenges when folks use your stuff?
[00:44:06.800 --> 00:44:09.520]   Is there more to do or does it work out of the box?
[00:44:09.520 --> 00:44:16.520]   There are still a lot of human challenges to it, I think, in the sense that a machinery
[00:44:16.520 --> 00:44:23.800]   model is doing different things in a different way than traditional software engineering.
[00:44:23.800 --> 00:44:27.440]   And for a lot of companies, it's really, really hard to make the transition.
[00:44:27.440 --> 00:44:36.760]   For example, the lack of explainability, the fact that it's harder to predict the outcomes
[00:44:36.760 --> 00:44:44.440]   of these models and kind of tweak them in a way, is still really hard to understand
[00:44:44.440 --> 00:44:50.200]   and adopt for people who've spent a career in software engineering, when you can really
[00:44:50.200 --> 00:44:54.360]   kind of define the outcome that you want to get.
[00:44:54.360 --> 00:44:59.520]   So I think from what I'm seeing, a lot of the time, the human and understanding of machine
[00:44:59.520 --> 00:45:10.040]   learning part is the most difficult thing, more than the technical aspect to it.
[00:45:10.040 --> 00:45:16.020]   On the technical part, we've been excited to bring on larger and larger models, which
[00:45:16.020 --> 00:45:20.000]   are still difficult to run in production.
[00:45:20.000 --> 00:45:22.680]   We've been working a lot with the cloud providers.
[00:45:22.680 --> 00:45:27.400]   We announced the strategic partnership with AWS not so long ago, but we're also working
[00:45:27.400 --> 00:45:31.840]   heavily with Google Cloud, Azure, and other cloud providers.
[00:45:31.840 --> 00:45:39.760]   But bringing these large language models in production, especially at scale, requires
[00:45:39.760 --> 00:45:44.280]   a little bit of skills and requires some work.
[00:45:44.280 --> 00:45:45.280]   You can get there.
[00:45:45.280 --> 00:45:52.200]   I think Coinbase has a good article and a good blog post on how they use one of our,
[00:45:52.200 --> 00:45:58.560]   I think it was Distilled Birth from Transformers, on over a billion inferences an hour, I think,
[00:45:58.560 --> 00:45:59.560]   if I'm not mistaken.
[00:45:59.560 --> 00:46:00.560]   Wow.
[00:46:00.560 --> 00:46:05.920]   But it's still a challenge and still requires a lot of infrastructure work to it.
[00:46:05.920 --> 00:46:06.920]   Awesome.
[00:46:06.920 --> 00:46:07.920]   Well, thanks for your time.
[00:46:07.920 --> 00:46:10.960]   It was a real pleasure to talk to you.
[00:46:10.960 --> 00:46:12.560]   Thanks, Vickess.
[00:46:12.560 --> 00:46:15.680]   Thanks for listening to another episode of Gradient Dissent.
[00:46:15.680 --> 00:46:19.960]   Doing these interviews are a lot of fun, and it's especially fun for me when I can actually
[00:46:19.960 --> 00:46:22.680]   hear from the people that are listening to these episodes.
[00:46:22.680 --> 00:46:26.760]   So if you wouldn't mind leaving a comment and telling me what you think or starting
[00:46:26.760 --> 00:46:30.680]   a conversation, that would make me inspired to do more of these episodes.
[00:46:30.680 --> 00:46:34.240]   And also, if you wouldn't mind liking and subscribing, I'd appreciate that a lot.

