
[00:00:00.000 --> 00:00:05.120]   would the fraction of compute that is spent on training that is free training versus post
[00:00:05.120 --> 00:00:10.000]   training change significantly in favor of post training in the future? Yeah, there are some
[00:00:10.000 --> 00:00:15.040]   arguments for that. I mean, right now it's a pretty lopsided ratio, but you could argue that
[00:00:15.040 --> 00:00:21.120]   the output generated by the model is like high quality compared to or higher quality than what
[00:00:21.120 --> 00:00:27.840]   most of what's on the web. So it sort of makes more sense for the model to think by itself
[00:00:29.280 --> 00:00:35.520]   instead of just like training to imitate what's on the web. So I think there's a first principles
[00:00:35.520 --> 00:00:43.040]   argument for that. And I would say we found a lot of gains through post training. So I'm not sure.
[00:00:43.040 --> 00:00:50.080]   So I would expect us to keep like pushing this methodology and probably increasing the amount
[00:00:50.080 --> 00:00:59.360]   of compute we put into it. The current GPT-4 has a ELO score that is like 100 points higher than
[00:00:59.360 --> 00:01:04.320]   the original one that was released. And is that all because of what you're talking about with
[00:01:04.320 --> 00:01:08.960]   these improvements that are brought on by post training? Yeah, I would say that we've, I would
[00:01:08.960 --> 00:01:14.800]   say that most of that is post training. Interesting. So there are a lot of different
[00:01:16.000 --> 00:01:22.720]   separate axes for improvement. Like you can, yeah, so we think about like data quality,
[00:01:22.720 --> 00:01:28.640]   data quantity, just doing more iterations of the whole process of deploying and collecting new data
[00:01:28.640 --> 00:01:33.440]   and like changing what you're, what kind of annotations you're collecting. So there's a lot
[00:01:33.440 --> 00:01:40.080]   of, a lot of things that stack up, but together they give you a pretty good like effective compute
[00:01:40.080 --> 00:01:46.000]   increase. How much of a moat is better post training? Currently companies that distinguish
[00:01:46.000 --> 00:01:51.040]   themselves by, well, how big is our model and so forth. Will it be a big moat who has figured out
[00:01:51.040 --> 00:01:55.200]   all the finickiness that you were talking about earlier with regards to all this data?
[00:01:55.200 --> 00:02:01.440]   I think there's something of a moat because it's just a very complex operation. And there's,
[00:02:01.440 --> 00:02:08.400]   so it takes, you have to have a lot of skilled people doing it. And so there's a lot of tacit
[00:02:08.400 --> 00:02:17.680]   knowledge and there's a lot of organizational knowledge that's required. So, so I think,
[00:02:17.680 --> 00:02:25.680]   yeah, I think post-training like to create a model that actually like has all the need,
[00:02:25.680 --> 00:02:32.800]   the functionality people care about is pretty complicated. It requires a pretty complicated
[00:02:32.800 --> 00:02:39.360]   effort. So, and this requires a lot of, this is basically an accumulation of a lot of R&D.
[00:02:39.360 --> 00:02:46.960]   So I would say, I would say that makes it somewhat of a moat that it's not trivial to
[00:02:46.960 --> 00:02:54.880]   spin this up immediately. It does seem like, like the same companies that are putting together the
[00:02:54.880 --> 00:02:59.840]   most serious pre-training efforts are also putting together the serious post-training efforts. So
[00:03:01.040 --> 00:03:07.440]   it seems like it is somewhat, somewhat possible to copy or to, to spin up more of these efforts.
[00:03:07.440 --> 00:03:12.800]   There's also like one force that sort of makes it less of a moat is that you can
[00:03:12.800 --> 00:03:19.760]   like distill the models or you can take someone else's model and clone the outputs, or you can
[00:03:19.760 --> 00:03:27.920]   use someone else's model as a judge to like do comparisons. So I think like the more big league
[00:03:27.920 --> 00:03:32.880]   people probably aren't doing that because it goes against terms of service policies, but, and it
[00:03:32.880 --> 00:03:38.320]   would also be a sort of hit to their pride, but I would expect some of the smaller players are
[00:03:38.320 --> 00:03:42.080]   doing that to get off the ground. What makes for somebody who's really good at doing this sort of
[00:03:42.080 --> 00:03:48.000]   RR research? I hear it's super finicky, but like what, what is the sort of intuitions that you have
[00:03:48.000 --> 00:03:54.640]   that enable you to find these ways to mess with the data and set up these environments?
[00:03:54.640 --> 00:04:00.240]   I'd say I just have a decent amount of experience at this point from like
[00:04:00.240 --> 00:04:07.840]   the different parts of the stack from like RL algorithms, obviously, since I've worked on those
[00:04:07.840 --> 00:04:20.320]   since grad school, to like the data collection, like the annotation process, to like language,
[00:04:20.320 --> 00:04:26.000]   playing with language models. So I mean, I'd say I just dabbled with these things and I'd say the
[00:04:26.000 --> 00:04:32.240]   people who do well at this kind of research have some view of the whole stack and have a lot of
[00:04:32.240 --> 00:04:39.200]   curiosity about the different parts of it. And also sort of think about, well, you want to be
[00:04:39.200 --> 00:04:45.360]   both empirical and like use experiment, let experiments update your views, but you also
[00:04:45.360 --> 00:04:55.200]   want to think from first principles somewhat like what like assuming that like learning works,
[00:04:55.200 --> 00:04:58.800]   like what would be the ideal type of data to collect and that sort of thing.
[00:04:58.800 --> 00:04:59.300]   Thank you.
[00:04:59.300 --> 00:05:09.300]   [BLANK_AUDIO]

