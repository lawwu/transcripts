
[00:00:00.000 --> 00:00:03.560]   In the integration docs, we can see many popular frameworks,
[00:00:03.560 --> 00:00:05.560]   such as Keras or PyTorch.
[00:00:05.560 --> 00:00:08.480]   You can see repos, such as Hugging Face or Spacey,
[00:00:08.480 --> 00:00:10.360]   and popular tools.
[00:00:10.360 --> 00:00:13.600]   So let's go back to the frameworks and pick Fast.ai.
[00:00:13.600 --> 00:00:16.360]   For most of these frameworks,
[00:00:16.360 --> 00:00:19.460]   weights and biases integration is pretty straightforward.
[00:00:19.460 --> 00:00:21.820]   In this case, we need to have 1DB installed.
[00:00:21.820 --> 00:00:22.760]   We need to log in.
[00:00:22.760 --> 00:00:27.140]   Then we need to import the 1DB callback.
[00:00:28.000 --> 00:00:30.720]   We need to init our run
[00:00:30.720 --> 00:00:33.440]   and add the callback to the fit function
[00:00:33.440 --> 00:00:34.960]   or to the learner object.
[00:00:34.960 --> 00:00:39.000]   There are a bunch of arguments here.
[00:00:39.000 --> 00:00:40.480]   For example, we can log prets,
[00:00:40.480 --> 00:00:42.760]   we can log model to weights and biases.
[00:00:42.760 --> 00:00:45.240]   So let's do this now in our training script.
[00:00:45.240 --> 00:00:51.040]   First thing that we need to do is to import
[00:00:51.040 --> 00:00:52.440]   the 1DB callback.
[00:00:53.440 --> 00:00:55.520]   (typing)
[00:00:55.520 --> 00:01:03.880]   We will set seed to ensure a producibility.
[00:01:03.880 --> 00:01:06.720]   We need to make sure that we define
[00:01:06.720 --> 00:01:09.280]   and store hyperparameters.
[00:01:09.280 --> 00:01:11.280]   This will be important when we start running
[00:01:11.280 --> 00:01:13.480]   multiple experiments.
[00:01:13.480 --> 00:01:16.840]   We'll store our hyperparameters in the train config
[00:01:16.840 --> 00:01:19.760]   and pass this config into 1DB run.
[00:01:20.920 --> 00:01:23.400]   We'll initialize our 1DB run
[00:01:23.400 --> 00:01:25.040]   and this time we'll be training a model.
[00:01:25.040 --> 00:01:27.920]   So the job type for 1DB run is training.
[00:01:27.920 --> 00:01:32.400]   We will use artifacts to track the data lineage
[00:01:32.400 --> 00:01:33.480]   of our models.
[00:01:33.480 --> 00:01:40.000]   At this stage, we won't be using our test set.
[00:01:40.000 --> 00:01:42.240]   We'll monitor results of our training runs
[00:01:42.240 --> 00:01:43.960]   on the validation set
[00:01:43.960 --> 00:01:45.480]   and come back to the test set
[00:01:45.480 --> 00:01:47.600]   in the evaluation stage later.
[00:01:48.600 --> 00:01:50.680]   (typing)
[00:01:50.680 --> 00:01:54.800]   We are using Fast.ai data block API
[00:01:54.800 --> 00:01:57.320]   to transform our data into the right shape
[00:01:57.320 --> 00:01:59.480]   expected by our model.
[00:01:59.480 --> 00:02:03.840]   We will use 1DB config to set our hyperparameters
[00:02:03.840 --> 00:02:06.120]   such as batch size or image size.
[00:02:06.120 --> 00:02:09.840]   We'll pass these parameters to create our data loaders.
[00:02:09.840 --> 00:02:15.640]   It's very important to pick and monitor the right metrics
[00:02:15.640 --> 00:02:17.280]   during our training runs.
[00:02:17.280 --> 00:02:18.640]   We'll monitor IOU,
[00:02:18.640 --> 00:02:21.560]   which stands for intersection over union.
[00:02:21.560 --> 00:02:25.160]   We will track IOU for each of our target classes
[00:02:25.160 --> 00:02:27.680]   and the mean across all of the classes.
[00:02:27.680 --> 00:02:30.880]   We'll talk more about this metrics in lesson three
[00:02:30.880 --> 00:02:32.800]   when we talk about model evaluation.
[00:02:32.800 --> 00:02:36.600]   This is our baseline model.
[00:02:36.600 --> 00:02:39.520]   So we will use the classic unit architecture
[00:02:39.520 --> 00:02:42.240]   with a pre-trained ResNet-18 backbone.
[00:02:42.240 --> 00:02:44.960]   And now we need to add the relevant callbacks.
[00:02:46.600 --> 00:02:48.760]   Let's start with a safe model callback.
[00:02:48.760 --> 00:02:50.920]   This will help us save the best model
[00:02:50.920 --> 00:02:53.880]   based on the metric that we choose.
[00:02:53.880 --> 00:02:56.120]   So let's choose mean IOU.
[00:02:56.120 --> 00:03:02.480]   And 1DB callback will lock our experiment
[00:03:02.480 --> 00:03:03.720]   to weights and biases.
[00:03:03.720 --> 00:03:07.640]   We will lock our predictions manually
[00:03:07.640 --> 00:03:09.040]   in weights and biases table below.
[00:03:09.040 --> 00:03:11.000]   So let's set this to false.
[00:03:11.000 --> 00:03:13.480]   And we also want to have our model
[00:03:13.480 --> 00:03:14.960]   locked to weights and biases.
[00:03:14.960 --> 00:03:16.520]   So let's set this one to true.
[00:03:16.520 --> 00:03:19.960]   I will fit our model.
[00:03:19.960 --> 00:03:22.840]   We'll again pass the right parameters from our config.
[00:03:22.840 --> 00:03:27.720]   And later on, when we evaluate the model,
[00:03:27.720 --> 00:03:29.560]   we will save our predictions
[00:03:29.560 --> 00:03:32.120]   and log them in a weights and biases table
[00:03:32.120 --> 00:03:33.760]   so that we can evaluate them
[00:03:33.760 --> 00:03:36.200]   and look at them in the dashboard.
[00:03:36.200 --> 00:03:40.200]   And we'll also log our final metrics
[00:03:40.200 --> 00:03:42.200]   into weights and biases summary.
[00:03:42.200 --> 00:03:44.120]   And we'll finish our run.
[00:03:44.120 --> 00:03:46.000]   So let's run this code now
[00:03:46.000 --> 00:03:50.240]   and we'll take a look at the results in our dashboard.
[00:03:51.080 --> 00:03:53.840]   (mouse clicking)
[00:03:53.840 --> 00:03:56.600]   (mouse clicking)
[00:03:56.600 --> 00:03:59.360]   (mouse clicking)
[00:03:59.360 --> 00:04:09.360]   [BLANK_AUDIO]

