
[00:00:00.000 --> 00:00:20.760]   All right. Hey, everyone. My name is Sharif. I'll be talking to you about demos and why I think
[00:00:20.760 --> 00:00:26.040]   demos are probably the most important thing in the world right now. I'm the founder of Lexica.
[00:00:26.040 --> 00:00:31.140]   We're working on generative models, specifically image models. But I kind of want to just talk
[00:00:31.140 --> 00:00:37.360]   to you about something a bit more than just models themselves, even more than demos. I
[00:00:37.360 --> 00:00:43.300]   kind of just want to talk to you about curiosity. There's a famous French mathematician, PoincarÃ©,
[00:00:43.300 --> 00:00:48.420]   he said, at the moment when I put my foot on the step, the idea came to me. He was working
[00:00:48.420 --> 00:00:53.480]   on this really, really esoteric field of mathematics called fusion functions. And he was stuck on
[00:00:53.480 --> 00:00:57.300]   this problem for weeks at a time. He didn't really know how to make any progress at all.
[00:00:57.300 --> 00:01:02.700]   And he was boarding a bus one day. And suddenly, it kind of just all came to him. He went from
[00:01:02.700 --> 00:01:06.720]   not knowing at all to having a full understanding of the problem. He said something along the
[00:01:06.720 --> 00:01:11.900]   lines of, the role of this unconscious work in mathematical invention appears to me as
[00:01:11.900 --> 00:01:18.480]   incontestable. I'm going to make one main argument to you guys today. And that's going to be that
[00:01:18.480 --> 00:01:25.300]   curiosity is the main force for how we pull ideas from the future into the present. And when we have
[00:01:25.300 --> 00:01:30.980]   these subconscious patterns that our brains recognize, they kind of surface as a feeling. And this feeling
[00:01:30.980 --> 00:01:37.480]   is what we know as curiosity. So I'm going to present you with a few demos I've worked on over the years.
[00:01:37.480 --> 00:01:43.720]   And they've each followed a specific pattern, where initially, I had this really great idea. I thought it was
[00:01:43.720 --> 00:01:47.860]   fantastic that it was going to change everything. And then you kind of get to implementing the specific
[00:01:47.860 --> 00:01:53.080]   idea. And you realize it's actually not possible at all. And then like, through sheer effort and like
[00:01:53.080 --> 00:01:57.140]   determination, you somehow find a way to make things work, even though you're working with models that
[00:01:57.140 --> 00:02:01.620]   have maybe a context length of like 2000 tokens. And then once you get to work, and you feel this like
[00:02:01.620 --> 00:02:06.740]   really good sense of pride and joy, and I think the most important thing about good demos is that
[00:02:06.740 --> 00:02:14.260]   they're kind of a way of exploring what's possible with these models. I'm, I kind of see these models as
[00:02:14.260 --> 00:02:20.020]   not necessarily things you can kind of understand fully without interacting with them. And I think the way you
[00:02:20.020 --> 00:02:24.900]   can best interact with them is by making really, really interesting demos. And the way you make
[00:02:24.900 --> 00:02:34.020]   interesting demos is just by following your curiosity. So this is from 2020. This was when GPT-3 was
[00:02:34.020 --> 00:02:39.940]   released. This was pretty mind-blowing for me. And I was surprised no one was really talking about this.
[00:02:39.940 --> 00:02:45.780]   GPT-3, for those of you who remember, had a context length of 2000 tokens. It cost, I think,
[00:02:45.780 --> 00:02:52.980]   $75 per million output tokens. And yeah, you had to get specific permissions from OpenAI before you
[00:02:52.980 --> 00:02:57.620]   shared anything about the model. You couldn't ship a chat app because that violated the terms of service,
[00:02:57.620 --> 00:03:03.940]   but it was a really magical time. It felt like you had this new tool in this like, in this toolkit of
[00:03:03.940 --> 00:03:09.940]   computing, and you could do so many things with it. And I think what was really important about this demo is
[00:03:09.940 --> 00:03:12.740]   that it inspired people that you could actually do things with software.
[00:03:12.740 --> 00:03:19.140]   And I think the way you get really impressive ideas is actually not by doing anything special.
[00:03:19.140 --> 00:03:23.780]   I think each person has their own unique kind of context window, the things you've seen and
[00:03:23.780 --> 00:03:28.580]   experienced. And I just so happened to watch a talk by Brett Victor before making this where
[00:03:28.580 --> 00:03:34.100]   he came up with this principle that creators need an immediate feedback with what they're creating.
[00:03:34.100 --> 00:03:38.580]   And I was really tired of copying and pasting code into my editor and then compiling it and then like
[00:03:38.580 --> 00:03:43.540]   running it and see what would happen. So I decided to just put like a JSX compiler in the browser,
[00:03:43.540 --> 00:03:47.380]   and it just felt different. It felt kind of magical in a way.
[00:03:49.700 --> 00:03:53.140]   And now today, like Claude's system prompt is 25,000 tokens, which is kind of funny.
[00:03:53.140 --> 00:03:59.220]   Here's another similar demo. This one's a bit more interactive. So you can kind of describe what you
[00:03:59.220 --> 00:04:04.100]   want. And then because the context windows were so small, it couldn't actually generate the entire
[00:04:04.100 --> 00:04:08.660]   application in a single prompt. You actually had to do three parallel prompts and then join them in the
[00:04:08.660 --> 00:04:13.220]   background. This is really simple, just asking for a Google homepage. And then it generates three
[00:04:13.220 --> 00:04:19.700]   different components for it. But yeah, this is just, I think, one of the ways you can express your
[00:04:19.700 --> 00:04:24.660]   curiosity. You kind of look at the world around you and what you've experienced. And you kind of
[00:04:24.660 --> 00:04:29.540]   synthesize new ideas. And you get this subconscious feeling pulling you in a direction. And I think the
[00:04:29.540 --> 00:04:35.700]   demo is the best way to kind of express that feeling to the world. Here's another more interesting
[00:04:35.700 --> 00:04:41.460]   one. This was 2021. I think the context lengths now expanded from 2,000 tokens to about 4,000 tokens.
[00:04:41.460 --> 00:04:47.460]   So we could do a bit more with this. I was kind of curious if these models had any sense of agency.
[00:04:47.460 --> 00:04:53.940]   So I decided to give it a really simple objective of buying me AirPods in Chrome. And if you were to
[00:04:53.940 --> 00:04:57.860]   just dump a web page into the browser, into the context window, it just wouldn't work. Like the
[00:04:57.860 --> 00:05:03.780]   Walmart.com shopping page would be like 24,000 tokens. It's just impossible. So I was actually a bit
[00:05:03.780 --> 00:05:09.220]   frustrated that I couldn't get it to work for a few days. So I wrote a custom HTML parser that would parse a
[00:05:09.220 --> 00:05:14.900]   web page into its core essence, which was able to fit it into the tiny context window of GPT-3 in 2021.
[00:05:14.900 --> 00:05:18.900]   It definitely failed spectacularly. It got distracted with the terms of service.
[00:05:18.900 --> 00:05:24.980]   But I think it was more so just interesting that we discovered that these models pre-trained on web
[00:05:24.980 --> 00:05:30.260]   text had this sense of agency kind of internal in their weights. We kind of take that for granted now,
[00:05:30.260 --> 00:05:38.100]   but 2021 was a very different time. Here's a more recent demo from a friend of mine, Farza. He's using
[00:05:38.100 --> 00:05:43.780]   Gemini 2.5 Pro here today. So we're still discovering new capabilities. Here he's making a basketball
[00:05:43.780 --> 00:05:48.660]   shot tracker where he's just putting in a video of him playing basketball, asking it to provide feedback
[00:05:48.660 --> 00:05:52.820]   as if Michael Jordan was watching his gameplay. And I think this is a really great demo because
[00:05:52.820 --> 00:05:59.380]   it inspires people to realize that you can actually make video-first experiences with Gemini 2.5 Pro.
[00:05:59.380 --> 00:06:03.540]   Before this, it was kind of like, oh, you can have it watch your screen and it'll give you feedback on
[00:06:03.540 --> 00:06:08.820]   your code. But there's so much more we can do. And I think one of the main reasons I find demos really,
[00:06:08.820 --> 00:06:15.220]   really interesting is that they inspire possibility. So much so that there's probably so much low-hanging
[00:06:15.220 --> 00:06:19.540]   fruit today in these models that if you were to halt all capabilities, if you kept all the weights
[00:06:19.540 --> 00:06:24.260]   frozen, didn't do a single backpropagation, I think you could build really amazing products for the next 10
[00:06:24.260 --> 00:06:30.420]   years, keeping everything constant. And I think the way you do that is just by building these demos and
[00:06:30.420 --> 00:06:36.900]   following your curiosity. So I have this really famous quote by Richard Hamming where he says,
[00:06:36.900 --> 00:06:39.940]   "In science, if you know what you're doing, you should not be doing it. Engineering,
[00:06:39.940 --> 00:06:44.180]   if you know what you're doing, you should not be doing it." So traditional engineering is very
[00:06:44.180 --> 00:06:50.020]   teleological. It's very goal-oriented, very purpose-driven. But I think AI engineering is a bit
[00:06:50.020 --> 00:06:56.580]   different. I think AI engineering is actually a bit more -- it's a bit closer to excavating. You're
[00:06:56.580 --> 00:07:03.460]   looking for new capabilities hidden within these models. And your toolkit is a demo. Your curiosity is
[00:07:03.460 --> 00:07:06.740]   kind of your flashlight guiding you to where the interesting bits of the models are.
[00:07:06.740 --> 00:07:12.020]   And the way you discover what's possible is just by making things. And what's really,
[00:07:12.020 --> 00:07:16.340]   really interesting is that even the researchers today at labs like OpenAI and Anthropic actually
[00:07:16.340 --> 00:07:21.620]   don't have a full understanding of the capabilities of these models. I've had OpenAI researchers show me
[00:07:21.620 --> 00:07:27.140]   or tell me that they didn't even know GPT-3 could do this -- could browse the web or that it could
[00:07:27.140 --> 00:07:29.780]   generate fully-functioning React components, which was pretty interesting.
[00:07:32.580 --> 00:07:38.980]   This is pretty funny. Charles Darwin was famous for coming up with a theory of evolution, but little
[00:07:38.980 --> 00:07:44.420]   known fact, he actually spent eight years studying barnacles, like the things on the sides of ships
[00:07:44.420 --> 00:07:49.060]   and piers and docks. He spent eight years studying barnacles, so much so that people thought he was
[00:07:49.060 --> 00:07:55.060]   going crazy before he published evolution. In the moment, you wouldn't have known that it was important,
[00:07:55.060 --> 00:08:01.860]   though, but the barnacle studying taught him that evolution was correct and it was kind of indisputable
[00:08:01.860 --> 00:08:07.620]   evidence for his theory. In the moment, you actually don't know what is actually work versus play.
[00:08:07.620 --> 00:08:11.860]   What you're doing might feel like it's useless. It might feel like it's leading nowhere.
[00:08:11.860 --> 00:08:15.700]   But sometimes you need to study barnacles for eight years before you can publish evolution.
[00:08:18.820 --> 00:08:24.100]   So I think we're in this really strange moment right now in 2025. These models can do amazing things.
[00:08:24.100 --> 00:08:29.460]   There's tons of them. Their context windows have now expanded from 2000 to maybe a million tokens or so.
[00:08:29.460 --> 00:08:37.300]   And I think demos are the way we explore what's possible. It's the way we expand the search space and
[00:08:37.300 --> 00:08:42.580]   kind of see what we can do with these capabilities. And I think it's not something you can predict ahead of
[00:08:42.580 --> 00:08:48.980]   time. It's kind of like crossing a foggy pond. You kind of take one stone. You kind of step on a stone
[00:08:48.980 --> 00:08:52.660]   and then see where it leads. If it leads somewhere interesting, you can keep going. But if it doesn't,
[00:08:52.660 --> 00:08:57.380]   you can always backtrack and go a different way. You'll never be able to plan your route across the pond
[00:08:57.380 --> 00:08:59.380]   ahead of time. You just kind of have to take the first step.
[00:08:59.380 --> 00:09:08.180]   I came across this really interesting tweet. And I really like it. It's because Anthropic really
[00:09:08.180 --> 00:09:13.140]   markets Claude as kind of a coding model or like a general reasoning model. But it's like trying to
[00:09:13.140 --> 00:09:17.780]   sell an intergalactic spaceship as a toaster because one of its surfaces gets hot every once in a while.
[00:09:17.780 --> 00:09:21.380]   And I think this is a really, really good way of thinking of these models. There are so much
[00:09:21.380 --> 00:09:27.140]   capabilities latent in them that we kind of only focus on the immediate and the obvious. But good demos
[00:09:27.140 --> 00:09:33.940]   reveal really interesting capabilities, mainly through exploration and play. And I think uncertainty is at the
[00:09:33.940 --> 00:09:38.980]   core of being an AI engineer. If you know what you're doing, you're kind of doing it wrong. And I think
[00:09:38.980 --> 00:09:43.860]   if you're uncertain and you're kind of just exploring, you'll you'll lead down interesting, you'll find
[00:09:43.860 --> 00:09:50.900]   yourself being led down interesting paths. Yeah, in subconsciously, you notice these patterns because
[00:09:50.900 --> 00:09:56.180]   you've worked with things that no one else has worked with before. Your life is unique to you, your context
[00:09:56.180 --> 00:10:01.140]   window is unique to you, and no one else has that same shared context window. So when you come across an idea in
[00:10:01.140 --> 00:10:06.340]   your head, oftentimes, you're one of the only people to ever have that idea. And I think you'd be doing
[00:10:06.340 --> 00:10:09.460]   yourself a great injustice if you never actually tried to make that idea a reality.
[00:10:09.460 --> 00:10:20.020]   So I'm going to close with this slide. One of the greatest computing papers ever written was
[00:10:20.020 --> 00:10:26.820]   Man Machine Symbiosis by Licklider in the 1960s. And the epitome of technology at the time were vacuum tube
[00:10:26.820 --> 00:10:32.020]   computers and punch cards. If you wanted to write a program, it would probably take a few hours, maybe even
[00:10:32.020 --> 00:10:39.940]   days to run. Meanwhile, today, we have Cloud Opus 4 on our computers. It's actually kind of insane. And I think
[00:10:39.940 --> 00:10:43.460]   Licklider genuinely would have killed someone to have an hour with the tools we have today.
[00:10:45.140 --> 00:10:50.820]   And like, I'm not even joking, I think it's kind of important, so much so that I feel like today we
[00:10:50.820 --> 00:10:55.220]   have a moral obligation to do him justice and everyone else in the field that came before us.
[00:10:55.220 --> 00:11:01.380]   Not only to just follow your curiosity, but to share what you explore with the world. Because by sharing
[00:11:01.380 --> 00:11:06.580]   your demos, you kind of share what's possible with these models. And I think that's how we move the field
[00:11:06.580 --> 00:11:12.340]   forward. And yeah, that's really it. Your unique perspective shouldn't be wasted. And I think you
[00:11:12.340 --> 00:11:16.020]   have a moral responsibility to share them with the world. Thank you.
[00:11:16.020 --> 00:11:22.420]   All right, we've got some time for questions. Does anybody have any?
[00:11:29.300 --> 00:11:31.940]   None? Cool. Thank you, guys. Appreciate it.
[00:11:31.940 --> 00:11:36.260]   We're doing really good for time. I was going to ask you if you had like other demos that you wanted
[00:11:36.260 --> 00:11:41.460]   to show us, because I liked seeing the 2020 versions of things. I have a few more, actually. Do you want
[00:11:41.460 --> 00:11:43.780]   me to pull them out? We've got eight minutes. All right, let's do it. You might as well.
[00:11:43.780 --> 00:11:48.980]   As long as they don't use Wi-Fi, because that's the running joke of this conference.
[00:11:48.980 --> 00:11:51.940]   I think I might have a few downloaded. Let me check.
[00:11:56.340 --> 00:12:00.420]   For the people in the room, I did try that basketball one, except I tried to apply it to
[00:12:00.420 --> 00:12:05.460]   running, and it works really well. And it pretty much gave the same feedback that my $600 a month
[00:12:05.460 --> 00:12:09.380]   running coach would give me. And I thought, wait, I think I can cancel this.
[00:12:09.380 --> 00:12:11.300]   Did it give you like pretty good advice on your gait?
[00:12:11.300 --> 00:12:18.180]   It gave me not just the gait per step. And so that's something that my coach would never be able to do.
[00:12:18.180 --> 00:12:23.300]   What I couldn't get to figure out is how it had the little arrow on top of the head. But if I had 20
[00:12:23.300 --> 00:12:25.700]   more minutes, I probably could have. Yeah, here's a pretty cool demo. I think
[00:12:25.700 --> 00:12:31.940]   was also from 2020. Let's see if it's playing. Oh, it's not. Let me -- oh, it is.
[00:12:31.940 --> 00:12:38.260]   Yeah, this was about a few weeks after the GP3 API came out. And I think the way I came across the API
[00:12:38.260 --> 00:12:44.180]   was really funny. Someone had said to me, you have to try this out. OpenAI has created AGI and it's here
[00:12:44.180 --> 00:12:48.180]   available today. And no one's really talking about it. And I was like, okay, let's see what this is about.
[00:12:49.300 --> 00:12:54.260]   And I quickly realized I could actually write code. But writing code in the text interface was
[00:12:54.260 --> 00:12:58.660]   not really the best way to do it. So you actually hook it up to an API, put a compiler in the browser,
[00:12:58.660 --> 00:13:03.140]   and you get this like nice back and forth visual interface. We kind of take this granted for today
[00:13:03.140 --> 00:13:07.700]   with tools like cursor. We can kind of like chat with your code in the sidebar. But in 2020, this felt
[00:13:07.700 --> 00:13:13.140]   really, really different than what anything was possible. Here, I'm like working on a like a really,
[00:13:13.140 --> 00:13:18.260]   really basic like banking app where you just ask it to add $3 or subtract another $5.
[00:13:18.260 --> 00:13:22.900]   It was pretty funny because like the bugs were really bad. You could actually -- there's a button
[00:13:22.900 --> 00:13:26.820]   where you could give away all your money. And if you were in debt, it would just like negate it and
[00:13:26.820 --> 00:13:32.820]   make you -- make your balance go to zero again. But this really was, I think, the start of vibe coding.
[00:13:32.820 --> 00:13:36.740]   And it really needed a lot of people to take these models, not only as like language models,
[00:13:36.740 --> 00:13:43.300]   but kind of reasoning engines. Yeah. And I think -- I think the way to think about these models is
[00:13:43.300 --> 00:13:47.700]   really that like they're these really, really intelligent in a way beings, which sounds kind of
[00:13:47.700 --> 00:13:51.780]   weird to say out loud. But that's like the mental model I have for them. And you kind of hook them up to
[00:13:51.780 --> 00:13:56.340]   these different apparatuses. And they can kind of work them. And you kind of like instill these tools with
[00:13:56.340 --> 00:14:03.300]   a sense of like purpose and agency. Yeah, I really just hope a lot more people are inspired to work on demos,
[00:14:03.300 --> 00:14:07.540]   because the capabilities we have today are really impressive. And you'd be really doing a disservice
[00:14:07.540 --> 00:14:11.620]   by not just like building something really fun and simple and sharing it with the world. Yeah.
[00:14:11.620 --> 00:14:13.380]   Tim over there? Yeah.
[00:14:13.380 --> 00:14:22.660]   Yes, it was the base model. We didn't have an -- we didn't have an instruct model until about a year
[00:14:22.660 --> 00:14:27.460]   later. So what -- it was essentially similar to base models we have today, where you give it a prefix and it
[00:14:27.460 --> 00:14:32.100]   just completes it. So you prompt engineer it with a few examples, and that's usually good enough.
[00:14:32.100 --> 00:14:38.900]   Any other demos you've got there? I can go into the archive.
[00:14:38.900 --> 00:14:40.660]   Do you want to go through your entire desktop while you're at it?
[00:14:40.660 --> 00:14:44.660]   Let me close this one.
[00:14:44.660 --> 00:14:51.140]   I have a few, but I don't -- do we have time? We have five minutes.
[00:14:51.140 --> 00:14:52.580]   You've got five minutes. Let's see if I can find anything.
[00:14:55.300 --> 00:15:08.100]   Okay. Let me try something really quick.
[00:15:13.780 --> 00:15:16.660]   Do you have time? Yeah, sure.
[00:15:16.660 --> 00:15:22.980]   Because it's my first time I've come to a conference where the subconscious is making a talk,
[00:15:22.980 --> 00:15:24.340]   which I found just glorious.
[00:15:24.340 --> 00:15:32.260]   How did the concept, or what was your thought process that led up to a presentation like this?
[00:15:32.260 --> 00:15:39.780]   Personally, I've never thought about doing demos in the world. I'm like, now do it. Okay.
[00:15:39.780 --> 00:15:43.060]   But maybe you could tell us, like, how this --
[00:15:43.060 --> 00:15:49.220]   Sure, yeah. I think a lot of it was just kind of introspecting on why I made these demos in the
[00:15:49.220 --> 00:15:54.580]   first place. A large part of it came from a sense of frustration that we have these really powerful
[00:15:54.580 --> 00:15:56.980]   models today, and no one really knows what they're capable of doing.
[00:15:58.420 --> 00:16:03.460]   And I think I examined it a bit further, and it did feel like a sense of moral obligation.
[00:16:03.460 --> 00:16:09.700]   You have these pioneers of computing from the 60s and 70s and 80s, like Licklider and Alan Kay and
[00:16:09.700 --> 00:16:16.180]   whatnot. And they came up with these grand ideas with the computers they had available to them.
[00:16:16.180 --> 00:16:21.300]   They just couldn't make it possible. And I look at what we have today, and it's kind of like we're
[00:16:21.300 --> 00:16:26.420]   spoiled by so many amazing pieces of technology, and we're kind of just making the same things all
[00:16:26.420 --> 00:16:30.900]   over again. But really, I think if you look back at, like, what people were writing about in the 60s
[00:16:30.900 --> 00:16:36.180]   and 70s, there's a whole goldmine of ideas there that we can revisit and actually make possible today.
[00:16:36.180 --> 00:16:42.980]   In the man-machine symbiosis paper, Licklider talks about an assistant that knows everything you're
[00:16:42.980 --> 00:16:46.340]   working on and has, like, perfect context and can help you with anything immediately.
[00:16:47.860 --> 00:16:52.260]   And here we have, like, ChatGPT, where every time you want to talk to it, you press "New Chat," and it has
[00:16:52.260 --> 00:16:55.220]   no memory of what you've talked about beforehand, minus, like, a few basic facts.
[00:16:55.220 --> 00:16:59.700]   And I think it's really just -- it really boils down to wanting to
[00:16:59.700 --> 00:17:06.100]   kind of do the ideas that these pioneers came up with justice beforehand.
[00:17:10.500 --> 00:17:10.900]   Thank you.
[00:17:10.900 --> 00:17:14.980]   Three minutes, so did you have something loaded up?
[00:17:14.980 --> 00:17:15.060]   I'm looking.
[00:17:15.060 --> 00:17:23.540]   Yeah, this was an old GPT-3 demo where the idea was, how do you get these models to solve very large
[00:17:23.540 --> 00:17:27.860]   and ambitious problems? It was called Multivac. And the idea was, you can't really fit everything into a
[00:17:27.860 --> 00:17:33.940]   2000 context window, 2000 token context window. So what you do is you essentially break down the problems
[00:17:33.940 --> 00:17:38.580]   into more digestible sub-problems, and you have this kind of visual interface to help you see where
[00:17:38.580 --> 00:17:42.740]   things are going. So you can give it some really ambitious problem, like, how do you solve climate
[00:17:42.740 --> 00:17:46.900]   change? And it might come up with things like, convince more people to go vegetarian or build climate
[00:17:46.900 --> 00:17:51.460]   or build wind turbines and, like, install more solar panels. And then you can click on each of the sub-ideas
[00:17:51.460 --> 00:17:57.460]   and it kind of breaks it down even further. Yeah, I think one of the core ideas behind this was, like,
[00:17:57.460 --> 00:18:01.860]   these models are a lot more than just text completion models, but I think they can be useful as, like,
[00:18:02.340 --> 00:18:08.420]   very helpful reasoning assistance, specifically at solving big problems. So much so that they could
[00:18:08.420 --> 00:18:11.780]   come up with ideas on their own one day and hopefully be really useful thought partners.
[00:18:11.780 --> 00:18:18.420]   Yeah, I mean, looking at it now, it's pretty rudimentary, but maybe someone should make a new
[00:18:18.420 --> 00:18:25.700]   version of this with, like, Opus Max. Yeah. I mean, someone here should do it. I think that'd be pretty cool.
[00:18:28.180 --> 00:18:34.580]   Yeah, that's about it, guys. Thanks.

