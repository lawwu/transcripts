
[00:00:00.000 --> 00:00:08.360]   double check just to make sure. I'm kind of sad this is the last
[00:00:08.360 --> 00:00:11.680]   one in my streak. I am like trying to figure out a way to do
[00:00:11.680 --> 00:00:12.440]   something tomorrow.
[00:00:12.440 --> 00:00:18.680]   Oh, really? What's you got a free day tomorrow? Yeah. Maybe
[00:00:18.680 --> 00:00:22.080]   maybe it's the chai versus coffee and then you know, to
[00:00:22.080 --> 00:00:25.960]   make Zach happy you can have versus what's the creatine
[00:00:25.960 --> 00:00:30.960]   whatever they you know, put in their shakes, you know, to get
[00:00:30.960 --> 00:00:32.680]   you know, to build the muscles. I don't know.
[00:00:32.680 --> 00:00:37.680]   I don't bodybuild. I just run.
[00:00:37.680 --> 00:00:41.440]   Yeah, yeah. I'm just I'm just walking. So that's the
[00:00:41.440 --> 00:00:44.680]   progression of bodybuilding to running to walking to just
[00:00:44.680 --> 00:00:47.880]   getting out of bed. So I'm getting closer to that.
[00:00:47.880 --> 00:00:52.160]   Awesome. Will I went to the audience was wondering, Wade and
[00:00:52.160 --> 00:00:54.640]   I always have a lovely chat behind the scenes. That's what
[00:00:54.640 --> 00:00:58.120]   you were hearing. But without getting too much in the way this
[00:00:58.120 --> 00:01:00.960]   is the third session in the series. Welcome back, everyone.
[00:01:00.960 --> 00:01:04.960]   And thanks for joining. I see Mateo in the chat. Welcome,
[00:01:04.960 --> 00:01:08.000]   Mateo. I see Korean Korean. It's always great to see you.
[00:01:08.000 --> 00:01:12.560]   Welcome back. We'll be learning about token classification with
[00:01:12.560 --> 00:01:16.800]   fast chain blur. And Wade is the real host. I'll just be
[00:01:16.800 --> 00:01:20.240]   monitoring the chat and asking him questions whenever he asked
[00:01:20.240 --> 00:01:24.160]   me to ask me to keep an eye out. So as a reminder, please keep
[00:01:24.160 --> 00:01:26.680]   the questions coming. I'm actively looking at them and
[00:01:26.680 --> 00:01:28.640]   Wade will be actively answering them with that.
[00:01:28.640 --> 00:01:33.160]   Awesome. All right. Thanks, Sanyam. Let me share my screen.
[00:01:33.160 --> 00:01:45.680]   All right, welcome, everybody. Yeah, this is Sanyam. So this is
[00:01:45.680 --> 00:01:51.400]   part three of our session three of the part two of the Hugging
[00:01:51.400 --> 00:01:56.640]   Face course. And this is kind of the more exciting part because
[00:01:56.640 --> 00:02:02.880]   part seven goes through actually end to end examples of the main
[00:02:02.880 --> 00:02:06.440]   NLP tasks, which is probably what everybody's really
[00:02:06.440 --> 00:02:10.240]   interested in. And so we're going to spend the next few
[00:02:10.240 --> 00:02:14.440]   weeks looking at those mostly individually, except when we get
[00:02:14.440 --> 00:02:18.160]   to language modeling, we'll combine looking at causal with
[00:02:19.280 --> 00:02:23.280]   mass language models. But today, we're going to do a look at
[00:02:23.280 --> 00:02:26.800]   token classification. Next week, we're gonna look at question
[00:02:26.800 --> 00:02:29.640]   answering. This is gonna be a one week break, and then we're
[00:02:29.640 --> 00:02:33.120]   going to come back and look at language modeling, and then
[00:02:33.120 --> 00:02:38.600]   summarization and translation. Those are the main tasks. So
[00:02:38.600 --> 00:02:43.440]   again, I won't go through all this a third time, but we have a
[00:02:43.440 --> 00:02:46.320]   bunch of resources. There's a registration group, if there's
[00:02:46.320 --> 00:02:50.680]   folks looking still to join the study group. There's also a
[00:02:50.680 --> 00:02:54.120]   study group discord, that's part of the Fast.ai discord. And so
[00:02:54.120 --> 00:02:57.640]   the links will be available in the slides. And then of course,
[00:02:57.640 --> 00:03:03.160]   there's a bunch of helpful links for learning Fast.ai or looking
[00:03:03.160 --> 00:03:06.440]   at some of the Fast.ai Hugging Face libraries, and or just
[00:03:06.440 --> 00:03:10.720]   studying machine learning and data science in general. So
[00:03:10.720 --> 00:03:15.520]   again, for as we get into section seven, I'm going to
[00:03:15.520 --> 00:03:18.440]   encourage everyone if they haven't done so already, as we
[00:03:18.440 --> 00:03:23.920]   go through each of these tasks to at least spend the following
[00:03:23.920 --> 00:03:27.280]   week going through the official course, because there are things
[00:03:27.280 --> 00:03:32.440]   that I'm not covering. In particular, the courses teach
[00:03:32.440 --> 00:03:37.720]   you how to use the trainer API to train these models, and also
[00:03:37.720 --> 00:03:43.880]   how to use accelerate slash standard PyTorch loops as well.
[00:03:44.040 --> 00:03:46.440]   And also they provide more information about things you
[00:03:46.440 --> 00:03:51.960]   need to do for configuration. And in those occasions where you
[00:03:51.960 --> 00:03:55.680]   want to actually take what you trained and push them to the hub
[00:03:55.680 --> 00:03:58.560]   and be able to use through like their inference widgets and
[00:03:58.560 --> 00:04:03.160]   things like that. So make sure you check that out this week,
[00:04:03.160 --> 00:04:07.640]   especially for the token classification. So this week,
[00:04:07.640 --> 00:04:12.600]   we're really going to just go through an end to end example of
[00:04:13.120 --> 00:04:17.000]   training a token classification, in particular named entity
[00:04:17.000 --> 00:04:22.560]   recognition model with fast AI and blur. And these steps are
[00:04:22.560 --> 00:04:27.000]   going to be steps that we follow for just about every task. First
[00:04:27.000 --> 00:04:33.040]   is getting a data set that we can use for the task. Second, we
[00:04:33.040 --> 00:04:35.720]   are going to go through how to fetch our Hugging Face objects.
[00:04:35.720 --> 00:04:41.080]   Third, talk about optional pre processing we can do with the
[00:04:41.080 --> 00:04:46.880]   data sets library or with pandas. Then we're going to go
[00:04:46.880 --> 00:04:50.000]   through how to create a data block that can either work with
[00:04:50.000 --> 00:04:54.200]   pre processed data, or can actually do the tokenization at
[00:04:54.200 --> 00:04:58.680]   batch time on the fly. And after that, once we have our data
[00:04:58.680 --> 00:05:02.320]   block, we're going to look at how to construct a learner with
[00:05:02.320 --> 00:05:06.640]   the appropriate metrics. And then we're going to train. And
[00:05:06.640 --> 00:05:09.440]   lastly, we're going to go through looking at how we can
[00:05:09.440 --> 00:05:14.560]   export our model for in user use, and for deployment and how
[00:05:14.560 --> 00:05:18.560]   we can use the model for inference. So we're going to
[00:05:18.560 --> 00:05:23.840]   roughly follow the same steps for every single task. But
[00:05:23.840 --> 00:05:27.640]   first, something was bothering me. And the question came up
[00:05:27.640 --> 00:05:31.520]   last week about why we can't just use fast AI to train
[00:05:31.520 --> 00:05:35.040]   transformers. And what bothered me is my answer. And I was
[00:05:35.040 --> 00:05:39.040]   thinking about it's like, Hey, I feel like I went way in depth,
[00:05:39.080 --> 00:05:42.400]   but I didn't really answer the question about why we can't use
[00:05:42.400 --> 00:05:49.760]   fast AI. But the better answer is that blur is fast AI. And for
[00:05:49.760 --> 00:05:55.000]   this to make sense, if you haven't already, I have a link
[00:05:55.000 --> 00:06:00.040]   here in the slides. The folks, folks have fast AI. So Jeremy
[00:06:00.040 --> 00:06:05.280]   Howard and Sylvain Gugger, they actually wrote a paper called
[00:06:05.280 --> 00:06:10.560]   fast AI, a layered API for deep learning. And one of the things
[00:06:10.560 --> 00:06:14.880]   that folks that are kind of just starting with fast and haven't
[00:06:14.880 --> 00:06:18.760]   really got into the weeds don't understand is that it's actually
[00:06:18.760 --> 00:06:22.080]   a tiered API. So there's a high level API where you can do
[00:06:22.080 --> 00:06:26.240]   things in one line, there's a mid level API where you code a
[00:06:26.240 --> 00:06:30.520]   little bit more. So you're doing a little bit more on the data
[00:06:30.520 --> 00:06:35.800]   block side, or maybe creating data loaders from data from
[00:06:35.800 --> 00:06:40.640]   Pytorch data sets. And then there's also a low level API.
[00:06:40.640 --> 00:06:44.000]   And the low level API is composed of things like
[00:06:44.000 --> 00:06:49.360]   transforms and callbacks, and a certain way for doing metrics.
[00:06:49.360 --> 00:06:55.560]   And to really kind of see what I'm saying when I say that blur
[00:06:55.560 --> 00:06:59.360]   is fast AI, I really recommend folks to read this paper,
[00:06:59.840 --> 00:07:02.400]   regardless of where you're starting with machine learning
[00:07:02.400 --> 00:07:05.360]   and deep learning in particular, you'll be able to read it's a
[00:07:05.360 --> 00:07:09.120]   very readable, very approachable paper. But take a look at that
[00:07:09.120 --> 00:07:14.320]   paper, and then pull up the blurred GitHub and look at some
[00:07:14.320 --> 00:07:21.880]   of the core modules for data and for modeling. And you'll see
[00:07:21.880 --> 00:07:27.920]   just how much blur really is. Fast AI, it's really built on
[00:07:27.920 --> 00:07:32.520]   top of fast and all these kind of low level concepts. So I
[00:07:32.520 --> 00:07:37.560]   really encourage you to do that. And again, this is the paper
[00:07:37.560 --> 00:07:41.120]   right here. And you can find this through archive. Like I
[00:07:41.120 --> 00:07:45.480]   said, very readable. And yeah, check this stuff out. And you'll
[00:07:45.480 --> 00:07:48.240]   see like this low level API, you go through blur, and you'll be
[00:07:48.240 --> 00:07:51.600]   like, Oh, yeah, this really is just fast. It's just really
[00:07:51.600 --> 00:07:56.520]   built on the low level versus the mid or high level stuff. So
[00:07:56.520 --> 00:07:59.320]   that's just an FYI. I feel better now with that answer, by
[00:07:59.320 --> 00:08:08.880]   the way. So with that, let's go ahead and begin looking at how
[00:08:08.880 --> 00:08:13.720]   to do token classification. So with any task, you want to
[00:08:13.720 --> 00:08:19.160]   figure out what is a good data set to start with. And when you
[00:08:19.160 --> 00:08:25.720]   start actually building your own models for whatever task you're
[00:08:25.720 --> 00:08:29.080]   doing, a good way to kind of figure out how to structure your
[00:08:29.080 --> 00:08:31.440]   own custom data set is by looking at some of the
[00:08:31.440 --> 00:08:35.640]   experimental data sets that we can use that are available
[00:08:35.640 --> 00:08:40.000]   online. So fast AI actually has data sets that you can grab for
[00:08:40.000 --> 00:08:44.040]   many of these tasks. Hugging face has a tremendous number of
[00:08:44.040 --> 00:08:50.800]   data sets that you can download for just all kinds of tasks. And
[00:08:50.800 --> 00:08:54.480]   so you want to look at something that you can use for token
[00:08:54.480 --> 00:09:00.840]   classification. And probably the best data set is the is the
[00:09:00.840 --> 00:09:06.840]   co-NNL 2003 data set. And I like it because not only do you get
[00:09:06.840 --> 00:09:12.480]   the inputs, the text, but you also get the labels for three
[00:09:12.480 --> 00:09:18.240]   types of token classification tasks, named it to recognition,
[00:09:18.240 --> 00:09:24.040]   part of speech, and chunking. And so you can actually use that
[00:09:24.040 --> 00:09:27.360]   one data set and play around with all three of those kind of
[00:09:27.360 --> 00:09:33.320]   sub token classification tasks. The big thing to remember here
[00:09:33.320 --> 00:09:38.600]   is that the inputs should be a list of words. So instead of
[00:09:38.600 --> 00:09:45.080]   getting some raw text or a text pair, in token classification,
[00:09:45.080 --> 00:09:49.360]   when we're building a model, we want to work against list of
[00:09:49.360 --> 00:09:55.360]   words. And that kind of reminds me that when I talk about words
[00:09:55.360 --> 00:09:59.680]   and tokens, I'm going to be real intentional about those words.
[00:09:59.680 --> 00:10:03.360]   Because one of the confusing things when you start working
[00:10:03.360 --> 00:10:07.440]   with transformers is that a lot of times, words, tokens, sub
[00:10:07.440 --> 00:10:12.240]   tokens get kind of mixed up and used to refer to different
[00:10:12.240 --> 00:10:17.160]   things. So I'm going to be real intentional, at least today, I
[00:10:17.160 --> 00:10:22.520]   hope, when I talk about words versus tokens. So words are
[00:10:22.520 --> 00:10:26.080]   going to be the individual things in a list that have a
[00:10:26.080 --> 00:10:31.960]   label associated to it. The tokens are going to refer to all
[00:10:31.960 --> 00:10:35.920]   the tokens that make up a particular word. And as we've
[00:10:35.920 --> 00:10:38.920]   seen before, since we're working mostly with sub word
[00:10:38.920 --> 00:10:44.800]   tokenizers, we're going to have for especially for rare words,
[00:10:44.840 --> 00:10:48.080]   we're going to have multiple tokens that make up a single
[00:10:48.080 --> 00:10:51.720]   word. So I'm going to try to be faithful and and use those
[00:10:51.720 --> 00:10:56.040]   terms correctly. So again, what we're looking for is a data set
[00:10:56.040 --> 00:11:00.640]   where the inputs are a list of words. And the labels are a list
[00:11:00.640 --> 00:11:10.160]   of labels, one for each word. So let's go ahead and go ahead and
[00:11:10.160 --> 00:11:13.600]   look at this in Colab, I'll make this a little bit bigger.
[00:11:14.440 --> 00:11:21.040]   And also, I should state here that if you when this is
[00:11:21.040 --> 00:11:23.640]   available, and you run this, you're going to notice that I
[00:11:23.640 --> 00:11:27.520]   actually did something else that you may hate me for. But I think
[00:11:27.520 --> 00:11:33.000]   in the long run will be better. And that is I started renaming a
[00:11:33.000 --> 00:11:37.360]   lot of the objects and methods in blur. And I got rid of the
[00:11:37.360 --> 00:11:42.320]   HF prefix, since it's given that blur is a fast AI hugging face
[00:11:43.280 --> 00:11:48.480]   integration library. And I've also named renamed the some of
[00:11:48.480 --> 00:11:52.960]   the callbacks and some of the transforms to indicate exactly
[00:11:52.960 --> 00:11:56.440]   what they're doing versus just calling them generically before
[00:11:56.440 --> 00:11:59.960]   batch transform or after batch transform. And I'm going to go
[00:11:59.960 --> 00:12:02.800]   back and be correcting the notebooks from session one and
[00:12:02.800 --> 00:12:06.080]   session two. And we'll let you know when those are all correct.
[00:12:06.080 --> 00:12:10.480]   But you'll see some of that. Don't hate me too much. I think
[00:12:10.480 --> 00:12:15.560]   in the long run, it will be better. So with that disclaimer
[00:12:15.560 --> 00:12:20.160]   out of the way, when we get a data set, again, this is should
[00:12:20.160 --> 00:12:24.200]   be really familiar by now, we can use the load data set and
[00:12:24.200 --> 00:12:29.960]   pass in the name of the data set that we're looking for. Look at
[00:12:29.960 --> 00:12:33.520]   what's in it. And we can see as I mentioned before, this is a
[00:12:33.520 --> 00:12:37.640]   nice data set, because even though they call it tokens, and
[00:12:37.640 --> 00:12:40.680]   again, they're really words, right, they're going to be split
[00:12:40.680 --> 00:12:43.600]   up into tokens, but okay, they're called tokens in the
[00:12:43.600 --> 00:12:47.200]   data set. And then we have three potential tasks that we can
[00:12:47.200 --> 00:12:52.640]   train our token classification model on the part of speech, the
[00:12:52.640 --> 00:12:57.520]   chunking or named entity recognition. And what's nice is
[00:12:57.520 --> 00:13:00.920]   that we can look at data sets. And this is just good advice in
[00:13:00.920 --> 00:13:03.880]   general, is to kind of understand what some of these
[00:13:03.880 --> 00:13:07.160]   things look at. And the best way to do that is by looking at a
[00:13:07.160 --> 00:13:12.880]   few examples, to make sure that what what you believe these
[00:13:12.880 --> 00:13:15.520]   things look like is what they look like. Because when you
[00:13:15.520 --> 00:13:18.240]   start building your data block API, you're going to be building
[00:13:18.240 --> 00:13:22.480]   it to how these inputs are structured and how the labels
[00:13:22.480 --> 00:13:26.640]   are structured. So we can look at a few examples. And one thing
[00:13:26.640 --> 00:13:30.360]   that's different in the hugging face data sets versus most of
[00:13:30.360 --> 00:13:33.360]   the fast AI data sets and the things you see in the fast book
[00:13:33.720 --> 00:13:40.120]   is that the labels are already encoded. So they represent the
[00:13:40.120 --> 00:13:43.440]   indices in the labels list, or as in fast AI, you would
[00:13:43.440 --> 00:13:51.640]   actually see like, you know, B per zero, O, O, B log, I log,
[00:13:51.640 --> 00:13:59.000]   etc. And blur can work with either one of those formats. So
[00:13:59.000 --> 00:14:03.600]   once we look at the examples, we can see that we can look at
[00:14:03.600 --> 00:14:06.840]   the different labels and see what each of these all look
[00:14:06.840 --> 00:14:11.520]   like. And then we can actually since we're going to work with
[00:14:11.520 --> 00:14:15.560]   the named entity recognition task, we can go ahead and
[00:14:15.560 --> 00:14:19.000]   actually pull the labels that we're going to be working with
[00:14:19.000 --> 00:14:23.440]   specifically in this example. And this is an important step
[00:14:23.440 --> 00:14:26.600]   because when we get to building our hugging face objects, we
[00:14:26.600 --> 00:14:32.600]   need to tell it how many labels there are. And we can go ahead
[00:14:32.600 --> 00:14:36.400]   and just get the length of this list, and then we're good to go.
[00:14:36.400 --> 00:14:47.080]   So that's step one. Step two is to get our hugging face objects.
[00:14:47.080 --> 00:14:50.840]   And we need to do this whether or not we're using the optional
[00:14:50.840 --> 00:14:55.400]   pre processing. And you can do this however you want. So in
[00:14:55.400 --> 00:14:59.840]   blur, we provide this utility class just called blur. And we
[00:15:00.000 --> 00:15:07.280]   have get HF objects that accepts a variety of options. So you can
[00:15:07.280 --> 00:15:13.880]   actually use this or you can actually build the objects from
[00:15:13.880 --> 00:15:18.320]   scratch, just kind of like how you normally would if you copy
[00:15:18.320 --> 00:15:21.200]   and pasted the code that hugging face gives you when you are
[00:15:21.200 --> 00:15:26.200]   looking at a model. So in this example, we'll just go ahead and
[00:15:26.200 --> 00:15:34.080]   stick with the get HF objects function. And again, it accepts
[00:15:34.080 --> 00:15:38.680]   all kinds of stuff like you can pass in quads to building your
[00:15:38.680 --> 00:15:43.200]   config, your tokenizer, your model, you can pass in specific
[00:15:43.200 --> 00:15:49.600]   classes that it should use for your config, tokenizer, a model,
[00:15:49.600 --> 00:15:55.720]   etc. So just about anything you can do in multiple lines,
[00:15:55.720 --> 00:15:58.560]   and building these objects, you should be able to do with this
[00:15:58.560 --> 00:16:04.160]   particular method. So we're going to go ahead and use the
[00:16:04.160 --> 00:16:07.920]   auto model for token classification. And this will
[00:16:07.920 --> 00:16:10.880]   allow us to even experiment with different pre trained
[00:16:10.880 --> 00:16:14.480]   checkpoints, since we're not using an architecture specific
[00:16:14.480 --> 00:16:19.240]   model. And for this example, I'm just going to set the max
[00:16:19.240 --> 00:16:25.520]   sequence length to 128. And as I mentioned before, we're going
[00:16:25.520 --> 00:16:30.760]   to need to tell a hugging face in particular, in particular,
[00:16:30.760 --> 00:16:35.720]   the configuration object, how many labels there are. If you
[00:16:35.720 --> 00:16:40.880]   don't do this, and you start running your code, you are
[00:16:40.880 --> 00:16:44.640]   guaranteed to see one of the most obscure CUDA errors of all
[00:16:44.640 --> 00:16:50.600]   time, which is the device assert error. And literally, it's not
[00:16:50.600 --> 00:16:53.960]   going to be helpful at all. But it's going to be because you
[00:16:53.960 --> 00:16:58.120]   probably didn't specify the number of labels. And in fact,
[00:16:58.120 --> 00:17:01.120]   most questions I get are folks that are having issues with
[00:17:01.120 --> 00:17:06.520]   blur, I would say maybe 60 70% of the time, it comes down to
[00:17:06.520 --> 00:17:10.680]   just not setting the labels for any of the classification tasks.
[00:17:10.680 --> 00:17:15.080]   So yeah, make sure you do this at the top so you don't forget.
[00:17:15.080 --> 00:17:22.960]   And here, I'm just going to pass the information into blur, our
[00:17:22.960 --> 00:17:31.240]   checkpoint, the class that we want to use, and also the argue
[00:17:31.240 --> 00:17:35.560]   some quarks I want to pass to the configuration object as it's
[00:17:35.560 --> 00:17:40.520]   built. Also, if you look at the course material, you'll see that
[00:17:40.520 --> 00:17:44.240]   there's other things you can do in here. If you plan on pushing
[00:17:44.240 --> 00:17:47.880]   your model to the hub, and you desire to use their inference
[00:17:47.880 --> 00:17:51.640]   widgets, which are really kind of cool ways for folks to see
[00:17:51.640 --> 00:17:54.120]   what your model does, and also for yourself to see what your
[00:17:54.120 --> 00:17:58.240]   model does. So check out the course content for more
[00:17:58.240 --> 00:18:03.520]   information there. So there we are, step two, your hugging face
[00:18:03.520 --> 00:18:13.480]   objects. So once we have that an optional task that we can do is
[00:18:13.480 --> 00:18:19.200]   to pre process the raw data. And in pre processing, essentially,
[00:18:19.200 --> 00:18:24.040]   what we're going to do is go through our text, and we're
[00:18:24.040 --> 00:18:27.880]   going to convert those to the input IDs. So remember that when
[00:18:27.880 --> 00:18:32.480]   the tokenizer sees your text, it returns something called a batch
[00:18:32.480 --> 00:18:37.240]   encoding object. And that batch encoding object has a bunch of
[00:18:37.240 --> 00:18:42.680]   basically is a dictionary of a bunch of different IDs for
[00:18:42.680 --> 00:18:46.080]   different things. So there's your input IDs, which are the
[00:18:46.880 --> 00:18:50.840]   token IDs of the tokens that represent your text, it has like
[00:18:50.840 --> 00:18:55.120]   an attention mask, and you can ask it to return other
[00:18:55.120 --> 00:18:58.680]   dictionaries with other information. But the input IDs
[00:18:58.680 --> 00:19:05.520]   is really what we're looking to create here. And in order to and
[00:19:05.520 --> 00:19:09.320]   not only input ideas, but also we need to adjust our labels.
[00:19:09.320 --> 00:19:15.240]   Because, as we mentioned before, when we pass our text to a
[00:19:15.240 --> 00:19:22.000]   tokenizer, a given word, it could take one or many tokens to
[00:19:22.000 --> 00:19:25.280]   represent that word. And so we have to figure out well, in
[00:19:25.280 --> 00:19:29.640]   those cases where there's going to be multiple sub tokens, what,
[00:19:29.640 --> 00:19:33.800]   how are we going to label those sub tokens. And as I mentioned,
[00:19:33.800 --> 00:19:39.600]   last week, Blur supports three strategies, but you can go ahead
[00:19:39.600 --> 00:19:43.560]   and extend and create your own subclass of base labeling
[00:19:43.560 --> 00:19:48.640]   strategy, and create your own. By default, it's going to use
[00:19:48.640 --> 00:19:52.480]   the first token strategy, I believe, we'll look at that in a
[00:19:52.480 --> 00:19:56.560]   second, which basically says if it's split up, for the purposes
[00:19:56.560 --> 00:20:00.680]   of calculating the loss and also the predicted token, we're going
[00:20:00.680 --> 00:20:04.760]   to look at the first token, regardless of whether there's
[00:20:04.760 --> 00:20:08.960]   one or many tokens that represent that word. But you can
[00:20:08.960 --> 00:20:11.280]   go ahead and create your own. But we need to use that for
[00:20:11.280 --> 00:20:16.040]   pre-processing. So that not only are we creating input IDs, but
[00:20:16.040 --> 00:20:22.760]   we're also creating a modified list of labels, one for each of
[00:20:22.760 --> 00:20:26.440]   the tokens now, just as in the raw, we have one for each of the
[00:20:26.440 --> 00:20:31.960]   words. And then also, this isn't in Blur yet, but one of the
[00:20:31.960 --> 00:20:37.760]   things that I'm working on, it's there, but not quite working 100%
[00:20:37.760 --> 00:20:42.080]   at least on the modeling side, is I am working on making Blur
[00:20:42.080 --> 00:20:47.960]   able to handle long documents for token classification. And
[00:20:47.960 --> 00:20:53.360]   you usually see this for question answering, where the
[00:20:53.360 --> 00:20:56.720]   context can be really long, and the question could be at the end
[00:20:56.720 --> 00:21:00.520]   and you don't want to miss it. But I'm going to attempt to
[00:21:00.520 --> 00:21:05.160]   actually also apply this to token classification, more as an
[00:21:05.160 --> 00:21:08.200]   experiment to see if it works. And if it does, it's great
[00:21:08.200 --> 00:21:13.640]   because then folks working with long documents will be able to
[00:21:13.640 --> 00:21:19.720]   just pass them at inference time in the Blur and be able to have
[00:21:19.720 --> 00:21:22.520]   token classification without having to chunk those things up
[00:21:22.520 --> 00:21:28.320]   ahead of time. So I plan to have this at the release of version
[00:21:28.320 --> 00:21:31.880]   two, which will come sometime after the study group.
[00:21:34.760 --> 00:21:45.840]   So if we go back to our example, here, oops. Where did my...
[00:21:45.840 --> 00:21:50.120]   Hold on for a second.
[00:21:50.120 --> 00:21:53.960]   Oops, there we go.
[00:21:53.960 --> 00:22:03.040]   I lost my slides. Let's go ahead and look at the pre-processing
[00:22:03.600 --> 00:22:06.960]   just to see what that looks like. And we'll actually use the
[00:22:06.960 --> 00:22:14.120]   pre-processed dataset in today's example. So I'm going to go and
[00:22:14.120 --> 00:22:18.720]   actually take just the training dataset and convert it to a data
[00:22:18.720 --> 00:22:22.920]   frame. This is something else in Blur I have to add better
[00:22:22.920 --> 00:22:27.480]   support for pre-processing datasets. For myself, I'm more
[00:22:27.480 --> 00:22:30.840]   comfortable working with data frames. And I normally find that
[00:22:30.840 --> 00:22:35.720]   the dataset isn't all that big that I'm working on fine tuning.
[00:22:35.720 --> 00:22:40.080]   And so the performance hit isn't something that's going to keep
[00:22:40.080 --> 00:22:44.040]   me up at night. So I'm going to just convert it to a data frame
[00:22:44.040 --> 00:22:51.920]   for this particular dataset. And in Blur, one of the new
[00:22:51.920 --> 00:22:54.880]   things we're doing is providing a bunch of pre-processors for
[00:22:54.880 --> 00:23:00.520]   all the tasks that know how to pre-process the data correctly
[00:23:00.560 --> 00:23:05.200]   for your given task. So we have token classification. And you
[00:23:05.200 --> 00:23:10.200]   can look at this class in the documentation or on GitHub. But
[00:23:10.200 --> 00:23:12.760]   essentially, it's going to go through and it's going to give
[00:23:12.760 --> 00:23:17.440]   us input IDs. And it's going to give us a modified list of
[00:23:17.440 --> 00:23:23.560]   labels based on whatever labeling strategy we pass, so
[00:23:23.560 --> 00:23:29.440]   that we have the correct label per token, given the labels
[00:23:29.440 --> 00:23:36.360]   associated per word. And at the end of this, we get a bunch of
[00:23:36.360 --> 00:23:38.480]   stuff. We also have the attention mask, which we're not
[00:23:38.480 --> 00:23:44.040]   going to use. But I put it in here because you can also use
[00:23:44.040 --> 00:23:46.680]   Blur to do your data preparation. And then you can
[00:23:46.680 --> 00:23:50.840]   create your own PyTorch loop or potentially use the trainer API
[00:23:50.840 --> 00:23:54.520]   if you want to do that too. So you can use the data
[00:23:54.520 --> 00:23:59.560]   pre-processing, data building aspects of Blur without using
[00:23:59.560 --> 00:24:03.160]   the modeling portions. So I keep this stuff in here just in
[00:24:03.160 --> 00:24:06.440]   case it's of interest to you. And so you can see that we have
[00:24:06.440 --> 00:24:11.520]   input IDs. Now we have something called aligned labels. And
[00:24:11.520 --> 00:24:15.840]   aligned labels is basically ensuring that for every single
[00:24:15.840 --> 00:24:21.440]   token, right, we have a label. And where that token is a
[00:24:21.440 --> 00:24:26.400]   special token, so it's a class token or a set token, we use
[00:24:26.400 --> 00:24:32.680]   negative 100. And negative 100 is the basically tells cross
[00:24:32.680 --> 00:24:37.520]   entropy loss that when calculating loss, ignore any
[00:24:37.520 --> 00:24:43.400]   labels that have negative 100. And, of course, we don't want
[00:24:43.400 --> 00:24:46.880]   the special tokens to be included, because we're not
[00:24:46.880 --> 00:24:50.400]   trying to predict the special tokens. So we essentially mask
[00:24:50.520 --> 00:24:55.160]   them for when the loss is calculated by setting the
[00:24:55.160 --> 00:25:01.680]   labels to negative 100. Also, once we add padding, we set the
[00:25:01.680 --> 00:25:05.720]   pad tokens to negative 100 as well for the same reason. And
[00:25:05.720 --> 00:25:08.640]   if you look at the tokenization and Blur, you'll see how that
[00:25:08.640 --> 00:25:13.160]   happens. So yeah, so this is what we have for pre-processing.
[00:25:14.480 --> 00:25:21.040]   And as I mentioned last week, when we define our data block,
[00:25:21.040 --> 00:25:25.600]   we have two options in Blur here for token classification. And
[00:25:25.600 --> 00:25:29.800]   we're going to do this for all tasks. By the time version two
[00:25:29.800 --> 00:25:32.880]   is released, is we can go ahead and build our data block to use
[00:25:32.880 --> 00:25:36.320]   the pre-process data, which we just created, which is our
[00:25:36.320 --> 00:25:40.880]   modified, which is our input IDs and modified labels. Or we can
[00:25:40.880 --> 00:25:45.840]   use Blur as it has been traditionally built in version
[00:25:45.840 --> 00:25:51.760]   one to figure things out on the fly, which means that it's going
[00:25:51.760 --> 00:25:57.240]   to do the tokenization, create the input IDs, and the
[00:25:57.240 --> 00:26:02.160]   appropriate labels at batch time. And so you have both
[00:26:02.160 --> 00:26:04.920]   approaches available to you depending on what you want to
[00:26:04.920 --> 00:26:10.920]   do. Here, we'll go ahead and take a look at what both of them
[00:26:10.920 --> 00:26:15.400]   look like. Unfortunately, one of the nice things with the data
[00:26:15.400 --> 00:26:19.280]   block API is that they're going to look very similar. And one of
[00:26:19.280 --> 00:26:22.720]   the reasons I like the data block API is that, you know,
[00:26:22.720 --> 00:26:27.080]   it's a blueprint for how to take your raw data and push it into
[00:26:27.080 --> 00:26:32.720]   something that's going to be modelable. Essentially, it's
[00:26:32.720 --> 00:26:36.760]   going to take raw data and our end goal is to have data loaders
[00:26:36.760 --> 00:26:40.680]   that will allow us to send many batches of data at a time into
[00:26:40.680 --> 00:26:46.280]   our learner. And I like using the data block API generally
[00:26:46.280 --> 00:26:51.360]   versus the high level or the low level stack for creating your
[00:26:51.360 --> 00:26:54.520]   data loaders, because it just makes sense to me, like
[00:26:54.520 --> 00:27:00.800]   methodically, the steps. And again, the nice thing with it as
[00:27:00.800 --> 00:27:04.880]   well is that it's going to look roughly the same, regardless of
[00:27:04.880 --> 00:27:08.760]   whether you do the batch time tokenization, or the
[00:27:08.760 --> 00:27:12.640]   pre-processed work against pre-processed data. So for batch
[00:27:12.640 --> 00:27:19.440]   time, we can go ahead and specify our, our token class
[00:27:19.440 --> 00:27:23.080]   batch tokenized transform. Again, this used to be called
[00:27:23.080 --> 00:27:25.680]   just before batch transform, which told you nothing about
[00:27:25.680 --> 00:27:29.760]   what it did. It is a before batch transform. But now we can
[00:27:29.760 --> 00:27:35.440]   see that what this does is handle tokenization. And we can
[00:27:35.440 --> 00:27:38.800]   go ahead and pass it into our text blocks, formerly named HF
[00:27:38.800 --> 00:27:43.320]   text block. And now it's just text block. We can say here's
[00:27:43.320 --> 00:27:47.280]   our before batch transform, which is really going to handle
[00:27:47.280 --> 00:27:52.000]   the tokenization as needed, our input type for our show
[00:27:52.000 --> 00:27:57.160]   methods. And then in Blur, we also have a new token category
[00:27:57.160 --> 00:28:01.280]   block that's built off of the category block that FastAI
[00:28:01.280 --> 00:28:06.560]   provides. But instead of trying to predict one label, it's
[00:28:06.560 --> 00:28:12.080]   going to assign multiple labels. And again, our labels are going
[00:28:12.080 --> 00:28:17.160]   to be the things that we defined above. And, and for every
[00:28:17.160 --> 00:28:22.760]   single word, or every single token, once we get to this, it's
[00:28:22.760 --> 00:28:25.640]   going to have a label that's going to try to predict if it's
[00:28:25.640 --> 00:28:29.680]   not a special token. And then once we have our data block in
[00:28:29.680 --> 00:28:35.480]   place, we're going to eventually call data data block
[00:28:35.480 --> 00:28:40.400]   data loaders and pass in our raw data set. But we can tell it
[00:28:40.400 --> 00:28:46.880]   here that it's going to get the x from the misnamed tokens
[00:28:46.880 --> 00:28:52.280]   column. And we're going to get our labels from the NER
[00:28:52.360 --> 00:28:55.800]   underscore tags column. We're just going to use random
[00:28:55.800 --> 00:29:00.880]   splitter just as an example here. I'm not setting seed, but
[00:29:00.880 --> 00:29:05.320]   that's also best practice for reproducibility. And once we
[00:29:05.320 --> 00:29:09.800]   have our block, we simply call d block data loaders and get our
[00:29:09.800 --> 00:29:15.600]   data loaders. And these are things I typically do in just my
[00:29:15.600 --> 00:29:19.920]   own work when I'm building anything deep learning is I like
[00:29:19.920 --> 00:29:24.240]   to look at what a batch of data looks like. And you'd be
[00:29:24.240 --> 00:29:28.200]   surprised how many times I find out it doesn't look how I
[00:29:28.200 --> 00:29:32.400]   imagined it would look. And this is kind of a good just sanity
[00:29:32.400 --> 00:29:36.040]   check to make sure that I'm at least putting stuff together in
[00:29:36.040 --> 00:29:42.920]   the right way. And so we can see here that we get we have a batch
[00:29:42.920 --> 00:29:49.880]   size of four with 156 tokens, because that's the max length of
[00:29:49.880 --> 00:29:54.800]   that particular batch. And then we also have a four by 156
[00:29:54.800 --> 00:30:00.080]   tensor for shape because we're trying to we have we're
[00:30:00.080 --> 00:30:06.800]   essentially having a prediction per token. And then in fast AI,
[00:30:06.800 --> 00:30:11.520]   we can now do something nice like this show batch, pass our
[00:30:11.520 --> 00:30:14.440]   data loaders, which is something you have to do with blur,
[00:30:14.440 --> 00:30:17.080]   because I need information from the data loaders to show things
[00:30:17.080 --> 00:30:21.520]   correctly. And we can actually look for look at our words and
[00:30:21.520 --> 00:30:26.360]   see the labels that are assigned. And this is custom
[00:30:26.360 --> 00:30:31.560]   because for token classification, at least in my
[00:30:31.560 --> 00:30:35.280]   opinion, it's nice to look at each word and ensure that yes,
[00:30:35.280 --> 00:30:40.560]   we're getting the right token assigned here. And I have some
[00:30:40.560 --> 00:30:44.240]   confidence that not only is blur working right, but also our
[00:30:44.320 --> 00:30:51.160]   data sets working right. So that's for just the normal on
[00:30:51.160 --> 00:30:56.440]   the fly batch time tokenization. We can also work with our pre
[00:30:56.440 --> 00:31:00.880]   process data set. So we actually created a pre processed data
[00:31:00.880 --> 00:31:04.680]   frame above. And if we want to use that with just a few
[00:31:04.680 --> 00:31:09.840]   modifications, we can. And the biggest thing is we need to tell
[00:31:11.240 --> 00:31:15.680]   our batch tokenized transform that it's already been pre
[00:31:15.680 --> 00:31:19.840]   tokenized. So it doesn't try to do the batch time tokenization.
[00:31:19.840 --> 00:31:24.240]   And if you notice, in our pre processing, one of the things we
[00:31:24.240 --> 00:31:28.240]   did was already include the special tokens. So we already
[00:31:28.240 --> 00:31:31.760]   have the class token and the set token, and whatever
[00:31:31.760 --> 00:31:35.040]   architecture you use, they're going to be included in the pre
[00:31:35.040 --> 00:31:38.920]   processing. So we also need to tell the batch tokenized
[00:31:38.920 --> 00:31:43.960]   transform not to add any special tokens, which is super simple.
[00:31:43.960 --> 00:31:50.960]   We just add to our tokenizer quarks, add special tokens is
[00:31:50.960 --> 00:31:55.320]   false. Everything else is relatively the same, except we
[00:31:55.320 --> 00:31:59.440]   change our inputs. Our get x is going to look at the input IDs
[00:31:59.440 --> 00:32:04.360]   now and our y is going to look at our aligned labels. And if we
[00:32:04.360 --> 00:32:09.240]   run that through now with the proc df, and we can look at one
[00:32:09.240 --> 00:32:13.280]   batch going to look pretty similar outputs in terms of the
[00:32:13.280 --> 00:32:17.360]   shape of the tensors. And we can do a show batch that works as
[00:32:17.360 --> 00:32:23.160]   before. And at that at this point, we're good to go to start
[00:32:23.160 --> 00:32:29.760]   modeling and good to go for any questions that have popped up.
[00:32:29.760 --> 00:32:32.920]   So, Sanya, Sanya, anything?
[00:32:33.920 --> 00:32:39.000]   Sanya Aminata: Yeah, we have a few. The first one is there are
[00:32:39.000 --> 00:32:43.880]   a bunch I'm trying to scroll up. Do you imagine blur being merged
[00:32:43.880 --> 00:32:47.160]   into fasta eventually I shared in the chat that the official
[00:32:47.160 --> 00:32:51.120]   stances we want to integrate v being fasta wants to integrate
[00:32:51.120 --> 00:32:54.320]   better with Tim and hacking phase. So
[00:32:54.320 --> 00:33:01.320]   Brian Curlews: good question. I don't know. And, and I'm not
[00:33:01.320 --> 00:33:08.200]   sure what the status of the next version of fasta is, but there's
[00:33:08.200 --> 00:33:12.600]   there's several things that hugging face does that fasta
[00:33:12.600 --> 00:33:16.520]   doesn't like out of the box. And the big one is working with
[00:33:16.520 --> 00:33:20.920]   dictionaries. And I know that Jeremy has mentioned that there
[00:33:20.920 --> 00:33:26.720]   will be support in v3. And if there's support, I would love to
[00:33:26.720 --> 00:33:31.560]   see blur integrated and made more part of the fastai library,
[00:33:31.560 --> 00:33:35.760]   even if it's not the whole thing. But if the important bits
[00:33:35.760 --> 00:33:40.200]   were able to be incorporated, I would be all for that. So I
[00:33:40.200 --> 00:33:42.960]   wouldn't have to maintain this and break stuff and make
[00:33:42.960 --> 00:33:45.520]   everybody angry. It could just be Jeremy that makes everybody
[00:33:45.520 --> 00:33:49.680]   angry. And, you know, everybody likes Jeremy, even when he
[00:33:49.680 --> 00:33:53.160]   changes everything on, you know, whereas for me, it's not
[00:33:53.160 --> 00:33:56.960]   necessarily the case. So I don't know. But I have hopes that
[00:33:56.960 --> 00:34:00.880]   there'll be better support in v3 whenever that happens.
[00:34:00.880 --> 00:34:06.880]   Sanyam Bhutani: Next question is by Mateo. Why setting the token
[00:34:06.880 --> 00:34:10.160]   values to minus 100 makes the cross entropy loss to ignore
[00:34:10.160 --> 00:34:11.680]   those values? What's the logic?
[00:34:11.680 --> 00:34:18.160]   Brian Curlews: So real simply is that we only want to have our
[00:34:18.160 --> 00:34:23.160]   loss actually consider tokens that we want, that we actually
[00:34:23.160 --> 00:34:28.040]   are we care about. And so we're not trying to predict the class
[00:34:28.040 --> 00:34:31.480]   token, like there's no label for that. There's no label for the
[00:34:31.480 --> 00:34:36.160]   SEP token. There's no label for the padding token. And so if we
[00:34:36.160 --> 00:34:39.840]   didn't set those to negative 100, then it would actually
[00:34:39.840 --> 00:34:43.560]   cloud and affect our loss. And I don't know how it would, but it
[00:34:43.560 --> 00:34:47.880]   definitely wouldn't help it improve. Because we're including
[00:34:47.880 --> 00:34:52.240]   calculations on things that we really don't care about. And
[00:34:52.240 --> 00:34:55.880]   then when it comes to the actual labels, as I mentioned, when we
[00:34:55.880 --> 00:34:59.920]   break a word into multiple tokens, by default, we're using
[00:34:59.920 --> 00:35:05.520]   just the first token label for the loss, and setting the other
[00:35:05.520 --> 00:35:09.200]   ones to negative 100, which basically is forcing the model
[00:35:09.200 --> 00:35:14.440]   to really put pressure on the first sub token to make the
[00:35:14.440 --> 00:35:19.160]   prediction. And if you look at the course documents, this is
[00:35:19.160 --> 00:35:22.560]   used by a lot of folks just because it makes the loss a
[00:35:22.560 --> 00:35:24.720]   little bit more efficient to calculate because there's less
[00:35:24.720 --> 00:35:29.440]   things that you're having to include. And some researchers go
[00:35:29.440 --> 00:35:34.880]   with this approach. Others will go with the BI approach, which
[00:35:34.880 --> 00:35:39.800]   is nameling, naming the labels, like if it's a person, like a
[00:35:39.960 --> 00:35:44.640]   first name, and it's labeled as B per, the tokens will be B per
[00:35:44.640 --> 00:35:48.600]   and then I per for the rest of the ones. But you couldn't
[00:35:48.600 --> 00:35:51.320]   really do that when you do part of speech or chunking, because
[00:35:51.320 --> 00:35:54.920]   they're not labeled like that. So I like the first token
[00:35:54.920 --> 00:35:59.320]   approach. That's something you can apply to POS, NER or
[00:35:59.320 --> 00:36:02.880]   chunking. So it's, I would say try it out, play with it and see
[00:36:02.880 --> 00:36:03.880]   what your results are like.
[00:36:03.880 --> 00:36:09.280]   Sanyam Bhutani: That made sense to me. Sorry, I was gonna say
[00:36:09.280 --> 00:36:11.280]   that made sense to me. So hopefully that would have
[00:36:11.280 --> 00:36:12.840]   definitely made sense to Mateo.
[00:36:12.840 --> 00:36:16.200]   Mateo De La Cueva: It made sense to one person that my day is
[00:36:16.200 --> 00:36:17.000]   accomplished.
[00:36:17.000 --> 00:36:22.600]   Sanyam Bhutani: Money, who's an NLP expert, also part of the
[00:36:22.600 --> 00:36:26.520]   fastly community, asks, do we always need to specify the
[00:36:26.520 --> 00:36:28.000]   return in the text block?
[00:36:28.000 --> 00:36:32.560]   Mateo De La Cueva: Yeah, so the it's not the return type, it's
[00:36:32.560 --> 00:36:36.320]   the input type that we're going to create. And the reason we
[00:36:36.320 --> 00:36:42.320]   have to do the input return type is that if we want to use the,
[00:36:42.320 --> 00:36:46.280]   it's I'll say this, it's not mandatory. But if you want to
[00:36:46.280 --> 00:36:50.560]   use any of the show methods, you'll need to use the right
[00:36:50.560 --> 00:36:56.200]   type there. Because you'll remember that the show type like
[00:36:56.200 --> 00:37:00.680]   show batch and show results are type dispatched. And so we're
[00:37:00.680 --> 00:37:04.400]   creating custom show batches that say, hey, if you see a
[00:37:06.040 --> 00:37:10.280]   token class text input, this is the show batch function we want
[00:37:10.280 --> 00:37:13.480]   you to run. So if you you don't have to, but if you don't, your
[00:37:13.480 --> 00:37:15.080]   show methods won't work correctly.
[00:37:15.080 --> 00:37:19.400]   Sanyam Bhutani: That makes sense. There are no other
[00:37:19.400 --> 00:37:21.920]   questions. I just had one comment, I might be wrong about
[00:37:21.920 --> 00:37:25.160]   this, but I've seen a lot of CUDA errors. So the reason as
[00:37:25.160 --> 00:37:29.040]   far as I understand, the error comes up for association when
[00:37:29.040 --> 00:37:34.000]   you don't mention the length is because CUDA likes, think of it
[00:37:34.000 --> 00:37:39.160]   like how TensorFlow used to be static graphs. So it likes to
[00:37:39.160 --> 00:37:42.040]   know the length and it really can't work with dynamic length.
[00:37:42.040 --> 00:37:43.400]   So probably that's why that comes up.
[00:37:43.400 --> 00:37:45.200]   Jason Tucker: Yeah, yeah. Great analogy.
[00:37:45.200 --> 00:37:48.840]   Sanyam Bhutani: Yeah, I don't see any other questions. Please
[00:37:48.840 --> 00:37:49.520]   please continue.
[00:37:49.520 --> 00:37:53.840]   Jason Tucker: All right, awesome. Okay, so now we have
[00:37:53.840 --> 00:38:00.240]   our data loaders. And now we can start training. So the first
[00:38:00.240 --> 00:38:04.680]   thing that we want to consider is before we actually build a
[00:38:04.680 --> 00:38:08.760]   model and start training is how are we going to determine
[00:38:08.760 --> 00:38:14.000]   whether the model is really good or not, given our particular
[00:38:14.000 --> 00:38:19.440]   task and our objectives. And so one thing you learn, if you go
[00:38:19.440 --> 00:38:22.720]   through the FASCI course, is that there's two things, right?
[00:38:22.720 --> 00:38:26.160]   There's two, there's two things that there's your metrics, and
[00:38:26.160 --> 00:38:30.920]   there's your loss. And the loss is your models way of
[00:38:30.920 --> 00:38:34.680]   understanding how good your model is, whereas the metrics
[00:38:34.680 --> 00:38:39.040]   are our ways as human beings for understanding how good our model
[00:38:39.040 --> 00:38:46.360]   is. And in general, we care about are the metrics. And so we
[00:38:46.360 --> 00:38:50.040]   have to figure out like, well, what metrics do we use for token
[00:38:50.040 --> 00:38:54.760]   classification. And we have to consider this for every task.
[00:38:54.760 --> 00:38:59.080]   And they're, they're, they're different. And so one of the
[00:38:59.080 --> 00:39:03.240]   cool things I found out about last week is that on Hugging
[00:39:03.240 --> 00:39:08.520]   Face, there's actually a page of tasks that contains a lot of
[00:39:08.520 --> 00:39:13.280]   this information, including suggested metrics to use per
[00:39:13.280 --> 00:39:17.360]   task. And this is really cool. The links going to be here in
[00:39:17.360 --> 00:39:22.720]   the slides. But if you just go to huggingface.co.task, you'll
[00:39:22.720 --> 00:39:27.280]   actually see like all the tasks we're going to be looking at in
[00:39:27.280 --> 00:39:30.360]   this study group and beyond, you're gonna see the audios,
[00:39:30.360 --> 00:39:36.160]   computer vision stuff. If we go to token classification, we get
[00:39:36.160 --> 00:39:40.320]   a nice example of what we're trying to do. We have a demo
[00:39:40.320 --> 00:39:44.400]   over here, we can actually get look at models for token
[00:39:44.400 --> 00:39:48.440]   classification, data sets for token classification. And what I
[00:39:48.440 --> 00:39:51.480]   thought was really cool is this bottom part, which is what
[00:39:51.480 --> 00:39:56.080]   metrics to use for token classification. And I will tell
[00:39:56.080 --> 00:39:59.120]   you this, a lot of my work in building blur and trying to
[00:39:59.120 --> 00:40:03.520]   support these tasks has been reading papers and on forums to
[00:40:03.520 --> 00:40:07.200]   try to figure out the answer to this question, which is what
[00:40:07.200 --> 00:40:13.800]   metrics should I use for really figuring out whether or not my
[00:40:13.800 --> 00:40:19.200]   particular task is performing well or not. So this is
[00:40:19.200 --> 00:40:24.640]   actually a really nice resource, if not just for this. And so
[00:40:24.640 --> 00:40:29.920]   yeah, feel free to look at this as you spend next week looking
[00:40:29.920 --> 00:40:36.320]   at the token classification task, and maybe find some other
[00:40:36.320 --> 00:40:41.040]   data sets in here to play with. And another cool thing, you can
[00:40:41.040 --> 00:40:45.720]   actually like click on these and see the calculation for each of
[00:40:45.720 --> 00:40:48.800]   them, like how they're being calculated. So this is just
[00:40:48.800 --> 00:40:51.760]   really neat. And kudos to the HugMeFace team for putting this
[00:40:51.760 --> 00:40:56.040]   together. You've made all our lives easier, just with this
[00:40:56.040 --> 00:41:01.760]   particular documentation. So yeah, token classification, keep
[00:41:01.760 --> 00:41:04.600]   that in mind. But yeah, so it mentions what are the key
[00:41:04.600 --> 00:41:10.440]   metrics. And it's going to be no surprise that when you look at
[00:41:10.440 --> 00:41:17.240]   blur, those are the metrics that we support with a custom
[00:41:18.560 --> 00:41:23.160]   token classification metrics class in blur. And it's built
[00:41:23.160 --> 00:41:28.720]   on top of the the seek eval library, which is also used in
[00:41:28.720 --> 00:41:34.000]   HugMeFace. If you use the data sets metrics, it actually this
[00:41:34.000 --> 00:41:36.840]   particular task uses the same library under the covers, I
[00:41:36.840 --> 00:41:42.200]   believe. And there's different things you can choose from. And
[00:41:42.200 --> 00:41:47.280]   the answer as to which one to choose is, it depends what
[00:41:47.280 --> 00:41:50.480]   you're trying to do. And so accuracy is a really common
[00:41:50.480 --> 00:41:53.680]   metric that we typically include, which is how many
[00:41:53.680 --> 00:41:57.280]   predictions did we get right. But if depending on whether we
[00:41:57.280 --> 00:42:02.880]   have a imbalanced data set, that may not be the best metric, we
[00:42:02.880 --> 00:42:08.000]   may want to dip down and use something like recall, which is
[00:42:08.000 --> 00:42:11.480]   measuring that when the label was x, how many times did we
[00:42:11.480 --> 00:42:17.360]   predict x. And again, a good example of when a recall is
[00:42:17.360 --> 00:42:21.960]   important would be an imbalanced data set like predicting a rare
[00:42:21.960 --> 00:42:26.800]   is cancer class, right? If you just went with accuracy, you
[00:42:26.800 --> 00:42:30.480]   could predict, predict zero every single time and have a
[00:42:30.480 --> 00:42:34.560]   really accurate model that doesn't do the job you're
[00:42:34.560 --> 00:42:40.040]   expecting it to. So if we were trying to predict something like
[00:42:40.040 --> 00:42:43.800]   is cancer or imagine a token classification, let's say we
[00:42:43.800 --> 00:42:46.520]   were like, there's an example I saw where they are actually
[00:42:46.520 --> 00:42:51.080]   labeling the names of drugs. And let's say that was really
[00:42:51.080 --> 00:42:53.960]   key because you were going to use the drugs identified for
[00:42:53.960 --> 00:42:57.840]   some downstream task. Well, recall would be a great metric
[00:42:57.840 --> 00:43:01.560]   to make sure you're getting those names or having or not the
[00:43:01.560 --> 00:43:08.400]   names but having the is drug label assigned correctly. So if
[00:43:08.400 --> 00:43:13.440]   that's important to you, then look to recall. And a good way
[00:43:13.440 --> 00:43:17.080]   of thinking about it, recall is important when the cost of not
[00:43:17.080 --> 00:43:22.200]   acting on the prediction is high. So obviously, if you miss
[00:43:22.200 --> 00:43:26.080]   is cancer, you might have great actually accuracy, but you miss
[00:43:26.080 --> 00:43:32.200]   is cancer, that's going to be really bad for your patient. So
[00:43:32.200 --> 00:43:37.320]   the cost of not acting on a prediction is high. Precision
[00:43:37.360 --> 00:43:41.160]   is a little bit different. Basically, in this case, we're
[00:43:41.160 --> 00:43:46.360]   looking at when we predicted x, how many times was the label
[00:43:46.360 --> 00:43:54.320]   actually x. And we use precision when false negatives are less of
[00:43:54.320 --> 00:43:59.400]   a concern. And so example that you'll see is like predicting is
[00:43:59.400 --> 00:44:05.720]   spam. We don't really care if we miss some of these, because if
[00:44:05.720 --> 00:44:10.040]   we, if we predict something is spam, we're for email detection,
[00:44:10.040 --> 00:44:14.840]   we're not going to show that email, right. And so if we focus
[00:44:14.840 --> 00:44:19.000]   on a recall, we're going to be potentially identifying a lot of
[00:44:19.000 --> 00:44:23.120]   emails of spam that aren't and our end users are going to be
[00:44:23.120 --> 00:44:29.240]   unhappy. We would rather the those false negatives actually
[00:44:29.240 --> 00:44:38.560]   get to the user than a bunch of false positives not. So in this
[00:44:38.560 --> 00:44:41.920]   case, we would look to precision. And we can think of
[00:44:41.920 --> 00:44:46.280]   this one kind of as the opposite of this, the cost of acting on
[00:44:46.280 --> 00:44:54.160]   the prediction is high. So if we act on yes, this is spam. And
[00:44:55.040 --> 00:45:01.120]   we're going to have a lot more emails going into the spam
[00:45:01.120 --> 00:45:05.840]   folder, which we don't want. So again, this is going to be kind
[00:45:05.840 --> 00:45:09.760]   of task specific, based on you know, what you're trying to do.
[00:45:09.760 --> 00:45:13.520]   For the most part, I would imagine that for token
[00:45:13.520 --> 00:45:18.400]   classification, both are important. And so we would want
[00:45:18.400 --> 00:45:22.520]   to look more to the F1 score, which is our harmonic, harmonic
[00:45:22.520 --> 00:45:25.760]   mean, precision and recall. So it's a score that kind of
[00:45:25.760 --> 00:45:30.640]   balances those two metrics. And kind of, in a sense, gives us an
[00:45:30.640 --> 00:45:36.200]   average of the two, and support for all these metrics is baked
[00:45:36.200 --> 00:45:46.360]   into blur. So just to show you what that looks like. And also
[00:45:46.360 --> 00:45:52.480]   to, again, kind of back my claim that blur is fast.ai. One of the
[00:45:52.480 --> 00:45:57.040]   things we have in the library is this token class metrics
[00:45:57.040 --> 00:46:01.600]   callback. And this is a callback deriving from the fastai
[00:46:01.600 --> 00:46:06.640]   callback. And if you don't know what callbacks are, definitely
[00:46:06.640 --> 00:46:10.400]   look at the book, look at the course, callbacks are probably
[00:46:10.400 --> 00:46:14.640]   the most important part of the fastai library. And anything you
[00:46:14.640 --> 00:46:19.040]   want to do differently, anything you want to hook into, it's
[00:46:19.040 --> 00:46:23.160]   going to be a callback. And so you can see that in this
[00:46:23.160 --> 00:46:29.520]   particular one, I want to basically do the token, provide
[00:46:29.520 --> 00:46:31.560]   this token classification metrics that we just talked
[00:46:31.560 --> 00:46:37.720]   about. And one thing is this, the the seek eval library
[00:46:37.720 --> 00:46:42.560]   doesn't predict or doesn't expect integers, it actually
[00:46:42.560 --> 00:46:45.960]   wants to look at the decoded labels. So it doesn't want to
[00:46:45.960 --> 00:46:51.800]   look at 703, it wants to see beeper and what did you predict
[00:46:51.800 --> 00:46:57.880]   beeper? Okay, good. So we have to do some modification here to
[00:46:57.880 --> 00:47:01.800]   get that to actually use that library, essentially, we have to
[00:47:01.800 --> 00:47:06.400]   do is we have to decode our predictions. And that's what
[00:47:06.400 --> 00:47:12.640]   this callback does. And so you can actually pass by default,
[00:47:12.640 --> 00:47:16.840]   it's going to do all of them. But you can pass what metrics
[00:47:16.840 --> 00:47:25.080]   you want to include. And into into this callback, there's some
[00:47:25.080 --> 00:47:29.760]   setup work that happens. It's using actually, this is a PR I
[00:47:29.760 --> 00:47:34.280]   submitted a long time ago that allows us to create metrics and
[00:47:34.280 --> 00:47:40.680]   callbacks, not have to use the metrics API in fastai. And so
[00:47:40.680 --> 00:47:45.280]   what we're going to do is create a metric that's named after each
[00:47:45.280 --> 00:47:50.760]   of these particular individual metrics. So we're actually going
[00:47:50.760 --> 00:47:54.040]   to create four new metrics that we can actually use now in
[00:47:54.040 --> 00:47:56.480]   fastai, just like any other metric, but we're going to use
[00:47:56.480 --> 00:47:59.960]   the callback to calculate and assign the value. That's why it's
[00:47:59.960 --> 00:48:04.720]   called the value metric. And again, the callbacks just have
[00:48:04.760 --> 00:48:10.960]   hooks into all the lifecycle of training. So and you can
[00:48:10.960 --> 00:48:17.640]   override and access any of these hooks that you want one or all
[00:48:17.640 --> 00:48:23.600]   of them. And here, the main one that we're looking on is after
[00:48:23.600 --> 00:48:27.240]   batch. And since we're really concerned about the validation
[00:48:27.240 --> 00:48:32.360]   set, I can check to see if we're in training, or in the case of
[00:48:32.360 --> 00:48:36.240]   inference, there's no labels, and just return, there's no
[00:48:36.240 --> 00:48:41.080]   metrics that I want to calculate here. And from here, we can
[00:48:41.080 --> 00:48:45.080]   actually get the predictions, we can get the targets, right. And
[00:48:45.080 --> 00:48:49.320]   these are all tensors. And as I mentioned before, this, the seek
[00:48:49.320 --> 00:48:54.120]   eval library wants to work with the actual label names. And so
[00:48:54.120 --> 00:49:01.960]   we can actually go through here, we can ignore the ignore label
[00:49:01.960 --> 00:49:06.720]   tokens ID, which is again, negative 100. And we can access
[00:49:06.720 --> 00:49:12.560]   our vocab, pass the prediction to get the related name from our
[00:49:12.560 --> 00:49:20.960]   labels list. And we can collect all these. And at the end of the
[00:49:20.960 --> 00:49:26.640]   validation batches running accesses after validate hook, we
[00:49:26.640 --> 00:49:32.720]   can loop through these. And these are all the the individual
[00:49:32.720 --> 00:49:35.520]   metrics that we want to calculate, right accuracy,
[00:49:35.520 --> 00:49:40.680]   recall, precision, f1. And we can pass them to this helper
[00:49:40.680 --> 00:49:45.440]   class called calculate token class metrics, we can pass the
[00:49:45.440 --> 00:49:48.320]   actual these are not the identifiers, right? These are
[00:49:48.320 --> 00:49:53.760]   the label names, the actual label names for each token, the
[00:49:53.760 --> 00:49:58.480]   predicted label names from each token, and the metric key, which
[00:49:58.480 --> 00:50:05.040]   is going to be for example, accuracy. And it's going to just
[00:50:05.040 --> 00:50:09.560]   call this simple function up here, that is really going to
[00:50:09.560 --> 00:50:14.200]   send the work of calculating that particular score based on
[00:50:14.200 --> 00:50:23.200]   the key to the seek eval library. And you can see that
[00:50:23.200 --> 00:50:30.000]   this right here, just given this, what's nice is, we can go
[00:50:30.000 --> 00:50:31.200]   back to our example.
[00:50:38.880 --> 00:50:45.600]   And we can define our learner. And again, some of the notes,
[00:50:45.600 --> 00:50:49.280]   some of the name changes, we have our base model wrapper that
[00:50:49.280 --> 00:50:53.320]   wraps our hugging face model, we have our base model callback,
[00:50:53.320 --> 00:50:59.880]   that is part of our learners callbacks, that is going to
[00:50:59.880 --> 00:51:05.040]   handle interpreting the output from the hugging face model,
[00:51:05.040 --> 00:51:07.080]   because this is actually going to return an object that has
[00:51:07.080 --> 00:51:10.320]   things in it, and optionally is already calculated the loss for
[00:51:10.320 --> 00:51:15.080]   us. So that's what this callback does. And then notice that for
[00:51:15.080 --> 00:51:18.840]   our fit callbacks, so these are callbacks that we're only going
[00:51:18.840 --> 00:51:22.760]   to use when we call for example, fit one cycle, we're going to
[00:51:22.760 --> 00:51:27.200]   include that token class metrics callback that I just showed
[00:51:27.200 --> 00:51:31.800]   you. And the reason that we assign them to fit callbacks
[00:51:31.800 --> 00:51:36.080]   versus learning callbacks, is that they're really not needed,
[00:51:36.360 --> 00:51:40.480]   except when you're training, you can actually load these offline
[00:51:40.480 --> 00:51:44.720]   when you're doing inference later on, optionally. But when
[00:51:44.720 --> 00:51:48.760]   you export your learner, if we, if we assign this as part of our
[00:51:48.760 --> 00:51:52.880]   learning callbacks, this gets included in our export, which
[00:51:52.880 --> 00:51:56.680]   makes our file a little bit bigger than necessary, and not
[00:51:56.680 --> 00:52:01.080]   all that helpful. So we actually are just going to mostly assign
[00:52:01.080 --> 00:52:08.800]   metrics into our fit calls. So that's the metrics we're using.
[00:52:08.800 --> 00:52:12.240]   And I really encourage you all to look at the token class
[00:52:12.240 --> 00:52:15.400]   metrics callback to see how that works. It's a pretty good
[00:52:15.400 --> 00:52:19.440]   example of callbacks in general and fast AI, which is what
[00:52:19.440 --> 00:52:24.080]   you're going to want to build to modify anything in the training
[00:52:24.120 --> 00:52:33.240]   lifecycle. So once we have our metrics all set up, we train our
[00:52:33.240 --> 00:52:39.000]   model. And the good news is, is that we train our models here
[00:52:39.000 --> 00:52:43.280]   just like we train any fast AI model. And this kind of goes
[00:52:43.280 --> 00:52:47.080]   back to one of my fundamental design philosophies around
[00:52:47.080 --> 00:52:50.960]   blur, which is to make it familiar to fast AI developers.
[00:52:50.960 --> 00:52:54.040]   So this really is a library that's meant for folks who have
[00:52:54.040 --> 00:52:57.400]   read the fast book and taken the course to be able to use and
[00:52:57.400 --> 00:53:00.080]   understand like, Oh, yeah, I've seen these things before I know
[00:53:00.080 --> 00:53:03.080]   exactly how they work. I see, oh, we can create a learner
[00:53:03.080 --> 00:53:07.360]   here, we can freeze it, right. So for discriminative learning,
[00:53:07.360 --> 00:53:10.920]   we're going to have different parameter groups, and things
[00:53:10.920 --> 00:53:14.000]   that are probably early on and probably already have a good
[00:53:14.000 --> 00:53:18.400]   representation. Those are going to have weights that we don't
[00:53:18.400 --> 00:53:23.680]   want to change that much. Whereas our last parameter
[00:53:23.680 --> 00:53:27.160]   group typically refers to our classification task, which we're
[00:53:27.160 --> 00:53:29.880]   going to want to change a lot because it's going to be random
[00:53:29.880 --> 00:53:34.520]   initially, and need the most adjustment. And so we can use
[00:53:34.520 --> 00:53:39.440]   those same concepts, we can use freeze, freeze to or unfreeze.
[00:53:39.440 --> 00:53:44.240]   And, and we can also use the familiar ways of looking at our
[00:53:44.240 --> 00:53:48.360]   batches to make sure things look right. So I can go ahead and
[00:53:48.360 --> 00:53:53.040]   after creating our learner, I could do a data loaders one
[00:53:53.040 --> 00:53:58.520]   batch. And now I can look at our predictions and make sure they
[00:53:58.520 --> 00:54:01.920]   make sense. And, and here they do. So we have a batch size of
[00:54:01.920 --> 00:54:07.400]   four, we have a max size of 128, which we've hit. And we have
[00:54:07.400 --> 00:54:14.000]   nine labels that we are actually predicting for. And so by
[00:54:14.000 --> 00:54:17.480]   running this, we can make sure that it makes sense. We can
[00:54:17.480 --> 00:54:22.320]   actually look at some of these things. Again, the first thing
[00:54:22.320 --> 00:54:25.600]   in our batch is our inputs, which is a dictionary. And this
[00:54:25.600 --> 00:54:29.000]   is one of the big reasons why blur exists. And that is that
[00:54:29.000 --> 00:54:32.960]   fast ed can't really understand dictionaries very well. And so
[00:54:32.960 --> 00:54:36.640]   we can see that yeah, that's a dictionary, we can look at our
[00:54:36.640 --> 00:54:43.200]   input IDs, and there's a tensor of four by 128. And then we can
[00:54:43.200 --> 00:54:48.040]   go ahead and as I mentioned, we can train our model, we can
[00:54:48.040 --> 00:54:53.920]   unfreeze it. And train we could we could train the frozen part.
[00:54:53.920 --> 00:54:56.600]   What's funny is that for different tasks, sometimes
[00:54:56.600 --> 00:55:00.000]   freezing I notice help sometimes it doesn't help that much. And
[00:55:00.000 --> 00:55:03.760]   it's easier just to unfreeze for token classification. I've
[00:55:03.760 --> 00:55:08.040]   noticed that just unfreezing the model, which is how it comes
[00:55:08.040 --> 00:55:11.440]   actually, so it would just be not calling learn dot freeze.
[00:55:11.880 --> 00:55:19.520]   Actually, works really well. And we can start training using
[00:55:19.520 --> 00:55:23.640]   fit one cycle, we can use this information and start training.
[00:55:23.640 --> 00:55:32.880]   Notice that in general, I would choose the steepest part, right,
[00:55:32.880 --> 00:55:38.600]   which is, you know, maybe like two e neg four. For some of the
[00:55:38.600 --> 00:55:45.520]   hugging face stuff is, is I usually will go back about 10x
[00:55:45.520 --> 00:55:51.560]   I'll go back about, you know, I'd go back to about right here,
[00:55:51.560 --> 00:55:56.360]   which is about three in a neck five, and start training here
[00:55:56.360 --> 00:56:02.160]   after unfreezing. So you have to experiment a little bit with the
[00:56:02.160 --> 00:56:07.280]   LL LR finder sometimes exactly what you think is it like for
[00:56:07.280 --> 00:56:10.400]   standard fast AI model, which is would be somewhere where it's
[00:56:10.400 --> 00:56:17.000]   really steep, right? Or to go here and go back 10x, right, is
[00:56:17.000 --> 00:56:19.840]   one of the recommendations, you'll find that going back a
[00:56:19.840 --> 00:56:24.000]   little bit further, like another 10x is often a good place to
[00:56:24.000 --> 00:56:29.600]   start with the transformer models, especially if you're
[00:56:29.600 --> 00:56:33.080]   just going to unfreeze the model. So just just be aware of
[00:56:33.080 --> 00:56:37.240]   that. And you can see that our metrics that we defined, we're
[00:56:37.240 --> 00:56:42.880]   getting them here. As we go through each epoch, we're
[00:56:42.880 --> 00:56:46.640]   getting our accuracy, precision, recall and F1. And this is
[00:56:46.640 --> 00:56:51.080]   calculating the overall accuracy, precision, recall F1
[00:56:51.080 --> 00:56:55.760]   across all labels. But another thing we can do here with blur
[00:56:55.760 --> 00:57:01.680]   is, and this is part of the learner in blur, is look at the
[00:57:01.680 --> 00:57:05.920]   token classification report, which allows us now to look at
[00:57:05.920 --> 00:57:13.000]   those metrics per token, as well as looking at the micro macro
[00:57:13.000 --> 00:57:16.160]   and the weighted average. And the support tells us how many
[00:57:16.160 --> 00:57:20.440]   times that token appeared in the examples that we looked at. So
[00:57:20.440 --> 00:57:24.360]   we can verify that we can see whether or not we have a
[00:57:24.360 --> 00:57:29.920]   imbalanced issue. And you can see here for the misc label,
[00:57:30.520 --> 00:57:34.160]   there is a slight imbalance. So this is something that if this
[00:57:34.160 --> 00:57:38.360]   was really important to us, we might want to do something to
[00:57:38.360 --> 00:57:43.640]   handle that imbalanced data, or maybe even spend more time
[00:57:43.640 --> 00:57:48.160]   looking at the recall metric over the other ones. So we can
[00:57:48.160 --> 00:57:55.160]   look at the per token metrics. And then we can actually also
[00:57:55.160 --> 00:57:59.480]   look at call show results. And we can see how did our model do
[00:57:59.520 --> 00:58:02.040]   and again, we're only working on a subset, so it's not going to
[00:58:02.040 --> 00:58:06.880]   be that great. But it actually turns out to be pretty great
[00:58:06.880 --> 00:58:10.440]   already. And so we can actually see that yeah, we're we're
[00:58:10.440 --> 00:58:14.800]   getting these things correctly. So we have Monica Sellis, and
[00:58:14.800 --> 00:58:20.240]   there identified Monica with the appropriately with the B per
[00:58:20.240 --> 00:58:28.040]   target. And we have the again, this is the token, the target
[00:58:28.040 --> 00:58:30.160]   label that we're trying to predict and what our model
[00:58:30.160 --> 00:58:34.560]   predicted. And so we can see it actually did a pretty good job
[00:58:34.560 --> 00:58:40.920]   here, at least with these two examples. Any questions come up
[00:58:40.920 --> 00:58:42.960]   on model training?
[00:58:42.960 --> 00:58:45.960]   I know I don't see any questions.
[00:58:45.960 --> 00:58:49.960]   Oh, good. All right. I figured that's the most like similar,
[00:58:49.960 --> 00:58:53.480]   like there's really nothing different there. And it's mostly
[00:58:53.480 --> 00:58:55.720]   kind of like learning through experience and playing with
[00:58:55.720 --> 00:59:00.880]   that to see hey, what, what we have different options for fit,
[00:59:00.880 --> 00:59:03.400]   right? We have different optimizers, like some people
[00:59:03.400 --> 00:59:08.120]   like using Ranger, I still think Adam works great. You can do
[00:59:08.120 --> 00:59:11.200]   fit, you can use different learning rate cycles. So you can
[00:59:11.200 --> 00:59:14.840]   use fit one cycle, but there's other ones supported. And once
[00:59:14.840 --> 00:59:17.240]   you're at this point, you really can experiment and play with
[00:59:17.240 --> 00:59:20.640]   those just like you're used to as a fast AI developer.
[00:59:20.640 --> 00:59:25.680]   Boris did a thread around what optimizers people are using
[00:59:25.680 --> 00:59:26.960]   did you get a chance to see that?
[00:59:26.960 --> 00:59:29.480]   Which which the optimizer called?
[00:59:29.480 --> 00:59:33.600]   No, he was just asking what optimizers people are using.
[00:59:33.600 --> 00:59:38.840]   Yeah, I think I saw that post. And I think like I saw folks
[00:59:38.840 --> 00:59:44.440]   with mentioned Ranger. And still like for myself, I get great
[00:59:44.440 --> 00:59:49.680]   results just using the Adam, Adam optimizer that fast AI
[00:59:49.680 --> 00:59:54.800]   gives us. But another good now that you mentioned that another
[00:59:54.840 --> 00:59:59.400]   good place to kind of figure out good defaults or ranges of values
[00:59:59.400 --> 01:00:04.920]   to try for any hyper parameter is looking at the papers. And if
[01:00:04.920 --> 01:00:08.320]   you're scared that you're not going to understand it, don't
[01:00:08.320 --> 01:00:11.840]   be I don't understand a lot of the papers when it gets into the
[01:00:11.840 --> 01:00:15.440]   math and the latex and the Greek, I am like, Oh my god, and
[01:00:15.440 --> 01:00:18.640]   I'll Google some things. But fortunately, most authors even
[01:00:18.640 --> 01:00:23.480]   in complex papers do a really good job describing their
[01:00:23.480 --> 01:00:27.160]   approach. And almost always at the end of the paper somewhere,
[01:00:27.160 --> 01:00:31.280]   they have like, the hyper parameters that they got the
[01:00:31.280 --> 01:00:36.800]   best results with. And so that's always a great place to look if
[01:00:36.800 --> 01:00:39.040]   you're trying to figure out, you know, what do I choose for
[01:00:39.040 --> 01:00:42.600]   learning rate or weight decay or for epsilon? What do I what I
[01:00:42.600 --> 01:00:43.920]   actually look at? So
[01:00:43.920 --> 01:00:49.120]   I found the thread if you could let me see how to send you the
[01:00:49.120 --> 01:00:53.400]   link. Can you open this? Oh, yeah. That has the discussion.
[01:00:54.080 --> 01:00:57.400]   Yeah, I got it. Did you want me to show folks?
[01:00:57.400 --> 01:00:59.200]   Yep.
[01:00:59.200 --> 01:01:01.960]   Go ahead and go here.
[01:01:01.960 --> 01:01:10.120]   Oh, yeah, nice. Yeah, I'll stick this in the slides as well. So
[01:01:10.120 --> 01:01:12.840]   folks can go through. This is another good resource right here
[01:01:12.840 --> 01:01:16.120]   if you're not sure what to do, like the ML Twitter community is
[01:01:16.120 --> 01:01:23.160]   really awesome. And, and if you post a question and you're not
[01:01:23.160 --> 01:01:26.880]   sure, even like, it's actually good to like at mentioned the
[01:01:26.880 --> 01:01:29.200]   hugging face, folks, you'll usually get somebody that
[01:01:29.200 --> 01:01:35.040]   replies with some helpful information and resources. So
[01:01:35.040 --> 01:01:37.600]   this is my approach a lot of times, like when I first started
[01:01:37.600 --> 01:01:40.880]   with hugging face and was just doing sequence classification, I
[01:01:40.880 --> 01:01:44.560]   was like, how do you how do you figure out hyper parameters?
[01:01:44.560 --> 01:01:48.440]   Because, you know, it's a little bit different than the stuff you
[01:01:48.440 --> 01:01:52.720]   like with with using an LSDM. It's different than computer
[01:01:52.720 --> 01:01:56.920]   learning, or tabular stuff that you get exposed to in fast AI
[01:01:56.920 --> 01:02:00.520]   and got a lot of good feedback, you know, from people that, you
[01:02:00.520 --> 01:02:04.160]   know, had done the hard work before me, and kind of like
[01:02:04.160 --> 01:02:06.520]   discovered like, oh, you know, sometimes you think the learning
[01:02:06.520 --> 01:02:10.880]   rates this but really go back about 10x, or don't worry about
[01:02:10.880 --> 01:02:14.440]   freezing and training and then unfreezing, which is the typical
[01:02:14.440 --> 01:02:17.520]   fast AI approach. It's just better unfreeze and you'll get
[01:02:17.520 --> 01:02:22.560]   better results. So yeah, Twitter is an awesome place to do that.
[01:02:22.560 --> 01:02:25.080]   Yeah, great, great links on them. So yeah, I'll put that in
[01:02:25.080 --> 01:02:26.040]   the slides as well.
[01:02:26.040 --> 01:02:29.280]   Sanyam Bhutani: Thank you. I've shared the link to the report
[01:02:29.280 --> 01:02:32.560]   that came out of this by report. I mean, a blog post and they
[01:02:32.560 --> 01:02:40.080]   compared different optimizers. So Boris was working on dolly
[01:02:40.080 --> 01:02:43.360]   mini and apparently shampoo, which is the name of an
[01:02:43.360 --> 01:02:45.040]   optimizer was really good for that.
[01:02:45.040 --> 01:02:49.720]   Oh, wow. Oh, cool. I haven't heard of that one. Yeah. So yeah,
[01:02:49.720 --> 01:02:52.000]   there's there's a lot out there. And a lot of people doing really
[01:02:52.000 --> 01:02:55.760]   interesting research. Boris is one of the people that to
[01:02:55.760 --> 01:03:00.520]   follow. Always doing like really interesting stuff, especially
[01:03:00.520 --> 01:03:03.680]   with this dolly. I've been watching that. I always like how
[01:03:03.680 --> 01:03:06.800]   it trains, you know, it takes a while to train. And so he would
[01:03:06.800 --> 01:03:09.800]   give updates, you know, and how it was looking. So yeah, yeah,
[01:03:09.800 --> 01:03:11.640]   yeah, he's a he's a great resource.
[01:03:11.640 --> 01:03:13.960]   Sanyam Bhutani: So sorry to go off on a tangent.
[01:03:13.960 --> 01:03:19.200]   Sanyam Bhutani: No, no, that's a good tangent. So. All right. So
[01:03:19.200 --> 01:03:23.240]   once we've trained our model, well, the last step is
[01:03:23.240 --> 01:03:29.120]   inference. And again, we can use the same approaches to inference
[01:03:29.120 --> 01:03:31.800]   that you've already seen in fast.ai, which means we can
[01:03:31.800 --> 01:03:37.280]   export our model and call predict. With blur, we actually
[01:03:37.280 --> 01:03:43.280]   include a helper function that we associate to the learner
[01:03:43.280 --> 01:03:47.520]   class called blur predict tokens. And what this does is
[01:03:47.520 --> 01:03:51.760]   allow us to do inference and give you the same results as if
[01:03:51.760 --> 01:03:55.440]   you were using the token classification pipeline from
[01:03:55.440 --> 01:03:59.120]   hugging face. And this is a little bit different than
[01:03:59.120 --> 01:04:05.440]   version one. And I like it better. And also it creates a
[01:04:05.440 --> 01:04:08.880]   nice parallel with hugging face so that if you're using the
[01:04:08.880 --> 01:04:12.280]   pipeline and you're using blur, the results are going to be the
[01:04:12.280 --> 01:04:16.960]   same and can therefore be used in the same way. So we'll look
[01:04:16.960 --> 01:04:19.520]   at how that works in just a moment. But also I want to
[01:04:19.520 --> 01:04:23.560]   mention there's other options. You could actually deploy your
[01:04:23.560 --> 01:04:27.400]   trained model because at the end of the day, the model that
[01:04:27.400 --> 01:04:31.040]   you're training is just any regular old transformer model.
[01:04:31.040 --> 01:04:36.160]   You can push that to the hub, and then be able to use that in
[01:04:36.160 --> 01:04:40.000]   the pipeline function yourself. So that's an option. If you're
[01:04:40.000 --> 01:04:44.880]   doing inference on batches of data at once, which is typically
[01:04:44.880 --> 01:04:50.480]   what I do. So I get a, you know, Excel file or a CSV file of
[01:04:50.480 --> 01:04:56.600]   thousands, tens of thousands of text items that that I want to
[01:04:56.600 --> 01:05:01.240]   do inference on. Probably the best approach is to either just
[01:05:01.240 --> 01:05:05.120]   create your own PyTorch loop and iterate through there and get
[01:05:05.120 --> 01:05:09.960]   your predictions just using pretty much standard PyTorch. Or
[01:05:09.960 --> 01:05:14.520]   you can go ahead and use I'm sorry, it's not leader. It's a
[01:05:14.520 --> 01:05:19.600]   learner dot get preds. Oops, let me fix that before I forget.
[01:05:19.600 --> 01:05:29.760]   And create a test data loader. And the test data loader allows
[01:05:29.760 --> 01:05:37.280]   you to really simply allows you to create a data loader modeled
[01:05:37.280 --> 01:05:40.080]   after your validation data loader. So there's no
[01:05:40.080 --> 01:05:44.360]   augmentation applied, there's no sorting. And you can pass an
[01:05:44.360 --> 01:05:48.760]   additional set. So you can pass another, if you're training,
[01:05:48.760 --> 01:05:53.560]   like, let's say on the data frames, and you get a CSV file
[01:05:53.560 --> 01:05:56.480]   of a bunch of data you want to do predictions on, you can
[01:05:56.480 --> 01:06:00.760]   convert that to a data frame, and actually pass that to the
[01:06:00.760 --> 01:06:05.240]   test DL function, get a data loader, and run it through
[01:06:05.240 --> 01:06:08.360]   learner dot get preds process the predictions, however you
[01:06:08.360 --> 01:06:12.640]   will. That's another kind of approach to doing the batch time
[01:06:13.040 --> 01:06:18.720]   inference. So maybe if there's folks that are interested, as we
[01:06:18.720 --> 01:06:22.200]   get to the last session, I can show you example of what both of
[01:06:22.200 --> 01:06:27.040]   those approaches would look like for a given task. But what we
[01:06:27.040 --> 01:06:34.800]   will look at is if you hear crying in the background, that's
[01:06:34.800 --> 01:06:41.040]   my dog, and I apologize. If we look at just the the real time
[01:06:41.040 --> 01:06:45.880]   inference, we'll pull up an example. And just to show how
[01:06:45.880 --> 01:06:50.560]   this works here is I got the first text. So I'm young is a
[01:06:50.560 --> 01:06:54.680]   great host who also works at Weights and Biases. And text to
[01:06:54.680 --> 01:06:58.680]   fast.ai is an awesome library built by Jeremy Howard from
[01:06:58.680 --> 01:07:04.720]   Australia. And we can pass those to blur predict token. So you
[01:07:04.720 --> 01:07:08.920]   can actually pass one or multiple texts, you want to get
[01:07:08.920 --> 01:07:13.080]   predictions. And then we can iterate through the results. And
[01:07:13.080 --> 01:07:18.160]   you can see these look just like the pipeline results. So here you
[01:07:18.160 --> 01:07:23.120]   can see didn't do a perfect job with your name, Sonja. So we
[01:07:23.120 --> 01:07:30.040]   probably need to train this more. It broke the s and the
[01:07:30.040 --> 01:07:37.160]   onion part, but it did get weights and biases correct. And
[01:07:37.160 --> 01:07:40.400]   what's interesting, when I see things like this, I go like,
[01:07:40.400 --> 01:07:44.920]   why, why did they break that apart? That's interesting. And
[01:07:44.920 --> 01:07:49.240]   there could be multiple reasons. And they include potentially
[01:07:49.240 --> 01:07:52.960]   that the name Sonja is very unique. And this is again,
[01:07:52.960 --> 01:07:56.800]   everything's, you know, this is an English grammar. So I doubt
[01:07:56.800 --> 01:08:00.720]   it's has seen Sonja I'm in there. And it also may be that
[01:08:00.720 --> 01:08:07.120]   we're just using a subset. And we need to maybe use the full
[01:08:07.120 --> 01:08:11.080]   library and spend some more time kind of training it. And also
[01:08:11.080 --> 01:08:16.000]   maybe need to give it some Sonja examples in there. Yeah, we
[01:08:16.000 --> 01:08:18.520]   look at these things, just, you know, make sure you really think
[01:08:18.520 --> 01:08:23.480]   about all these things. These are all factors, right? And, and
[01:08:23.480 --> 01:08:25.640]   so yeah, so that's, those are my thoughts. There might be other
[01:08:25.640 --> 01:08:28.760]   reasons, maybe a better architecture, we could choose
[01:08:28.760 --> 01:08:31.760]   maybe a multi lingual architecture would do a better
[01:08:31.760 --> 01:08:32.760]   job, right?
[01:08:32.760 --> 01:08:35.520]   Sanyam Bhutani: I don't agree with the statement, but it got
[01:08:35.520 --> 01:08:38.240]   rich and vice right. So at least that's what matters.
[01:08:38.240 --> 01:08:39.680]   That's all that's all.
[01:08:39.680 --> 01:08:42.960]   Jason Tucker: We got that one right there. You got most of
[01:08:42.960 --> 01:08:48.080]   your name. But you can see like, Jeremy Howard, right? That's a
[01:08:48.080 --> 01:08:53.040]   kind of more English sounding name. And it got that one
[01:08:53.040 --> 01:09:00.040]   correct. And it got it didn't identify fast.ai as an
[01:09:00.040 --> 01:09:04.640]   organization, which is interesting. And I think if we
[01:09:04.640 --> 01:09:08.000]   actually train this against the full data set and gave it more
[01:09:08.000 --> 01:09:12.760]   examples, it probably would get this right. But it did get his
[01:09:12.760 --> 01:09:19.360]   name and the location right that Australia is a location, it did
[01:09:19.360 --> 01:09:24.880]   get that one, right. So anyway, so BlurredPredictTokens gives
[01:09:24.880 --> 01:09:27.480]   you the same thing as the pipeline. And as I mentioned
[01:09:27.480 --> 01:09:32.200]   before, if you want to deploy this and build your own API and
[01:09:32.200 --> 01:09:35.320]   actually do token classification, it's going to
[01:09:35.320 --> 01:09:40.880]   work just like any other fast.ai trained model, we can go ahead
[01:09:40.880 --> 01:09:47.640]   and define a export name, we can call learner.export, it's going
[01:09:47.640 --> 01:09:51.640]   to create a pickle file with everything needed to
[01:09:51.640 --> 01:09:58.120]   reinstantiate that learner. And we can call load learner, pass
[01:09:58.120 --> 01:10:03.040]   that pickle file, and to create an inference learner. And once
[01:10:03.040 --> 01:10:09.400]   we have this, it works just like our any other learner, we can
[01:10:09.400 --> 01:10:13.600]   call BlurredPredictTokens and get results here. So we can
[01:10:13.600 --> 01:10:17.960]   actually now deploy this into an API, create a web application,
[01:10:17.960 --> 01:10:26.320]   call this in our API and get a get get predictions. And that is
[01:10:26.320 --> 01:10:33.720]   the the final step in our end to end training of a token
[01:10:33.720 --> 01:10:38.520]   classification model. Coming soon to Blur, so again, I'm
[01:10:38.520 --> 01:10:43.400]   going to update the previous examples with the new namings of
[01:10:43.400 --> 01:10:48.000]   a bunch of things so that you don't have to wrestle with that
[01:10:48.000 --> 01:10:52.160]   too much. But with regards to token classification, we're
[01:10:52.160 --> 01:10:57.840]   going to add chunking examples, experiment with how all that
[01:10:57.840 --> 01:11:01.840]   works and helps with handling long documents, and also finish
[01:11:01.840 --> 01:11:06.120]   up support on the pre processing front for both data sets and
[01:11:06.120 --> 01:11:10.240]   data frames. Right now the data frames pre processing is good to
[01:11:10.240 --> 01:11:16.920]   go. And data sets is something I haven't really spent much time
[01:11:16.920 --> 01:11:19.640]   with, but that's going to be coming soon as well. So just be
[01:11:19.640 --> 01:11:25.040]   aware. If things don't work right, look at the GitHub and
[01:11:25.040 --> 01:11:29.240]   see if there's actually support. If you want to contribute,
[01:11:29.240 --> 01:11:33.680]   that's even better. Less work for me to do and I would
[01:11:33.680 --> 01:11:37.920]   appreciate it. So just keep that in mind that those things are
[01:11:37.920 --> 01:11:44.320]   coming eventually. For homework, we covered an example of token
[01:11:44.320 --> 01:11:47.960]   classification, you could use the bits right now in Blur and
[01:11:48.000 --> 01:11:52.560]   create a train a token classification model that you
[01:11:52.560 --> 01:11:58.480]   can deploy in an end user app. It's however you want. It could
[01:11:58.480 --> 01:12:05.040]   be web app. It can be something that you can use like in Gradio,
[01:12:05.040 --> 01:12:08.040]   I think like that hugging face like spaces, I haven't really
[01:12:08.040 --> 01:12:11.120]   played with it too much. So I don't know how that all works.
[01:12:11.120 --> 01:12:16.680]   But we've looked at the end to end example of token class
[01:12:17.160 --> 01:12:21.880]   models with Fast.ai. But I encourage you to go through the
[01:12:21.880 --> 01:12:25.160]   official course and kind of look at the segments that talk about
[01:12:25.160 --> 01:12:30.400]   token classification and offer more details about how it works,
[01:12:30.400 --> 01:12:34.560]   how you can use some of the other methods to train models
[01:12:34.560 --> 01:12:38.040]   and deploy them to the hub. And I have references to each of
[01:12:38.040 --> 01:12:43.080]   those three sections. Also, this week, go through the course and
[01:12:43.080 --> 01:12:46.720]   try training your own token classification model. And even
[01:12:46.720 --> 01:12:49.840]   better, prove that you really understand it, at least to
[01:12:49.840 --> 01:12:54.760]   yourself by helping others in the writing of a blog to explain
[01:12:54.760 --> 01:13:02.240]   how you did it, and what you found difficult or easy or are
[01:13:02.240 --> 01:13:05.280]   things that you found that you wish that you were told ahead of
[01:13:05.280 --> 01:13:09.320]   time before you had to find out on your own. And you might want
[01:13:09.320 --> 01:13:12.680]   to try using one of the data sets from that task page and
[01:13:12.680 --> 01:13:16.360]   hugging face and do something a little bit different than use
[01:13:16.880 --> 01:13:22.560]   data set that we went through today. And lastly, because
[01:13:22.560 --> 01:13:26.240]   metrics are so important, in terms of really evaluating your
[01:13:26.240 --> 01:13:31.480]   model, we looked at the core token classification metrics, go
[01:13:31.480 --> 01:13:34.680]   back through those, make sure that you understand them and
[01:13:34.680 --> 01:13:38.440]   really prove that you understand them by explaining it to someone
[01:13:38.440 --> 01:13:42.720]   without a machine learning background. So if you can do
[01:13:42.720 --> 01:13:47.840]   that, then then you really got it. So I'm putting that as some
[01:13:47.840 --> 01:13:52.680]   homework as well. And that's it. I have nothing else, unless
[01:13:52.680 --> 01:13:54.640]   there's some questions that have come up.
[01:13:54.640 --> 01:14:00.160]   Sanyam Bhutani: Oh, this is one question of when to use custom
[01:14:00.160 --> 01:14:02.840]   NER classification versus something from spaCy.
[01:14:02.840 --> 01:14:07.600]   Brian Ferraro: Uh, I don't have a good answer for that. I'm not
[01:14:07.600 --> 01:14:11.080]   sure. And I'm sure custom NER, you mean custom NER, so named
[01:14:11.120 --> 01:14:17.760]   entity recognition. I really can't answer that specifically,
[01:14:17.760 --> 01:14:22.480]   except to say that if you're a spaCy user, and it works well,
[01:14:22.480 --> 01:14:27.480]   enough, go go with that. If you think that's potentially, let's
[01:14:27.480 --> 01:14:31.120]   say it doesn't work well, or you think there's some serious
[01:14:31.120 --> 01:14:33.600]   improvements can be made, like you have some custom, like a lot
[01:14:33.600 --> 01:14:38.160]   of customization. Maybe go through building your own. So I
[01:14:38.160 --> 01:14:39.640]   think it just it depends.
[01:14:40.640 --> 01:14:45.240]   Sanyam Bhutani: Okay, thanks. Thanks for that answer. Also,
[01:14:45.240 --> 01:14:47.480]   some people have mentioned they're not getting the email
[01:14:47.480 --> 01:14:50.120]   with the resources. I'll request the team to double check maybe
[01:14:50.120 --> 01:14:55.680]   maybe I said something stupid in the automated emails platform,
[01:14:55.680 --> 01:14:58.720]   whatever it's called. So that that might be on me. I'll, I'll
[01:14:58.720 --> 01:15:00.000]   make sure you get them this time.
[01:15:00.000 --> 01:15:02.360]   Brian Ferraro: Are you talking about like the links to things
[01:15:02.360 --> 01:15:06.200]   or? Okay, yeah, I had some people ask that too, yeah, on
[01:15:06.200 --> 01:15:06.680]   the forums.
[01:15:06.680 --> 01:15:09.760]   Sanyam Bhutani: It's supposed to reach via email. Maybe I did
[01:15:09.760 --> 01:15:11.800]   something stupid. That's why it's not going through.
[01:15:11.800 --> 01:15:19.120]   Brian Ferraro: Yeah, I also post the links on the discord. So
[01:15:19.120 --> 01:15:23.800]   folks don't get it or lost their email. I try to put them there
[01:15:23.800 --> 01:15:26.400]   too, especially people remind me. So I've had a few reminders
[01:15:26.400 --> 01:15:28.720]   and try to get those up there as well.
[01:15:28.720 --> 01:15:34.160]   Sanyam Bhutani: Awesome. Which callback scenario should we use
[01:15:34.160 --> 01:15:35.680]   for fit_cbs?
[01:15:36.680 --> 01:15:41.440]   Brian Ferraro: So the ones that are typically so it really
[01:15:41.440 --> 01:15:46.440]   depends. I mean, most callbacks can be be put in either as
[01:15:46.440 --> 01:15:51.000]   learner callbacks or fit callbacks. Anything that is not
[01:15:51.000 --> 01:15:57.080]   needed for inference, I tend to put as a fit callback, because
[01:15:57.080 --> 01:16:01.720]   it just makes that export file bigger. And sometimes there are
[01:16:01.720 --> 01:16:05.080]   things that are you can't pickle that are part of those fit
[01:16:05.080 --> 01:16:09.520]   callbacks. Like I do have some metric callbacks I use, I think
[01:16:09.520 --> 01:16:14.920]   it's for summarization. Where if I put them as a learner callback
[01:16:14.920 --> 01:16:18.960]   and then call learner.export, it just bombs. And the reason it's
[01:16:18.960 --> 01:16:24.000]   just not pickable class. And so by putting those in fit, you're
[01:16:24.000 --> 01:16:26.320]   just using for fit. So they're not they don't have to be
[01:16:26.320 --> 01:16:30.800]   capable of serializing the pickle. And so metrics are good
[01:16:30.800 --> 01:16:33.960]   candidates. Any callbacks dealing with metrics are good
[01:16:34.000 --> 01:16:36.160]   candidates for a fit callback.
[01:16:36.160 --> 01:16:40.680]   Sanyam Bhutani: Awesome. Thanks. Thanks for that answer. I
[01:16:40.680 --> 01:16:43.240]   think this would be the last question. Can we use blur for
[01:16:43.240 --> 01:16:44.840]   any R tasks in production?
[01:16:44.840 --> 01:16:49.840]   Brian Ferraro: Yes, but be cautious. I don't think the
[01:16:49.840 --> 01:16:53.040]   token classification bits that I showed you today. And again,
[01:16:53.040 --> 01:17:00.480]   this is this is in the alpha version two build, right. And I
[01:17:00.480 --> 01:17:03.080]   don't think the token classification bits are going to
[01:17:03.080 --> 01:17:07.400]   change much from what you saw today to what it will be in the
[01:17:07.400 --> 01:17:14.960]   final version. So I will say, maybe. But if something breaks
[01:17:14.960 --> 01:17:21.760]   weeks, you only have Sanyam to yell at, not me. So yeah, I
[01:17:21.760 --> 01:17:25.840]   think you I think I'd be cautious. But I want to say that
[01:17:25.840 --> 01:17:28.360]   yeah, you could but just keep an eye on it.
[01:17:30.080 --> 01:17:30.880]   Sanyam Bhutani: That's a fair deal.
[01:17:30.880 --> 01:17:35.080]   Brian Ferraro: That's a good political answer right there.
[01:17:35.080 --> 01:17:37.320]   Maybe yes.
[01:17:37.320 --> 01:17:44.840]   Sanyam Bhutani: Awesome. With that, I'd love to wrap up and I
[01:17:44.840 --> 01:17:47.680]   just want to thank the audience and also Wade. I like
[01:17:47.680 --> 01:17:49.880]   celebrating small accomplishments and I like
[01:17:49.880 --> 01:17:53.720]   setting myself up to stupid goals. So my goal was to do 10
[01:17:53.720 --> 01:17:56.360]   lectures in nine days. I've done more on the podcast, but
[01:17:56.360 --> 01:17:59.840]   podcast is easier for me. This is a bit harder and I am so
[01:17:59.840 --> 01:18:02.560]   grateful I get to collaborate with people like Wade. I'm so
[01:18:02.560 --> 01:18:05.800]   grateful everyone comes to these events and I couldn't have done
[01:18:05.800 --> 01:18:09.080]   it at any place. I could have done it elsewhere as well. But I
[01:18:09.080 --> 01:18:12.000]   wouldn't want to do it at any place other than Wades & Bios.
[01:18:12.000 --> 01:18:14.880]   So I just want to thank the community for coming to our
[01:18:14.880 --> 01:18:17.520]   lectures. Thank you. Really thank Wade for hosting these
[01:18:17.520 --> 01:18:20.320]   series. And it's it's such an honor to contribute to the
[01:18:20.320 --> 01:18:21.640]   community in this capacity.
[01:18:21.640 --> 01:18:23.960]   Brian Ferraro: Yeah, this is awesome. I appreciate the
[01:18:23.960 --> 01:18:27.160]   opportunity as well. Even though I can't see anybody's comments
[01:18:27.160 --> 01:18:30.600]   or see anybody, thank you for attending and being a part of I
[01:18:30.600 --> 01:18:33.440]   always go back to the YouTube to see if there's things that I
[01:18:33.440 --> 01:18:38.040]   miss. So I will look at things. But it's like, I can only
[01:18:38.040 --> 01:18:41.040]   multitask so much. But yeah, thanks, Sanyam. This has been a
[01:18:41.040 --> 01:18:42.080]   lot of fun as usual.
[01:18:42.080 --> 01:18:44.280]   Sanyam Bhutani: Thanks. Thanks to you. And thanks everyone for
[01:18:44.280 --> 01:18:45.840]   tuning in. We'll see you next week.
[01:18:45.840 --> 01:18:49.900]   Goodbye.

