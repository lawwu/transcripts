
[00:00:00.000 --> 00:00:02.880]   This is the Human-Centered Autonomous Vehicle.
[00:00:02.880 --> 00:00:05.240]   One of the main ideas underlying our work
[00:00:05.240 --> 00:00:08.560]   is that solving the task of autonomous driving
[00:00:08.560 --> 00:00:11.640]   is more complicated and more fascinating
[00:00:11.640 --> 00:00:16.160]   than the strictly robotics challenges of localization,
[00:00:16.160 --> 00:00:19.700]   mapping, perception, control, and planning.
[00:00:19.700 --> 00:00:23.100]   You also have to enable the vehicle to perceive,
[00:00:23.100 --> 00:00:27.020]   predict, communicate, and collaborate with human beings.
[00:00:27.020 --> 00:00:29.520]   The humans inside the car, like the driver
[00:00:29.520 --> 00:00:32.400]   and the passengers, and the humans outside the car,
[00:00:32.400 --> 00:00:34.960]   like the pedestrians, cyclists,
[00:00:34.960 --> 00:00:38.560]   the drivers of other vehicles, and even teleoperators.
[00:00:38.560 --> 00:00:42.980]   The studies, the code, the data, and the demos we release
[00:00:42.980 --> 00:00:44.520]   all consider autonomous driving
[00:00:44.520 --> 00:00:46.920]   in this kind of human-centered way,
[00:00:46.920 --> 00:00:51.000]   where the control is transferred from human to machine
[00:00:51.000 --> 00:00:54.560]   and back to human based on the state
[00:00:54.560 --> 00:00:56.700]   of the external driving environment
[00:00:56.700 --> 00:00:58.360]   and the state of the driver.
[00:00:58.360 --> 00:01:01.300]   What we'd like to demonstrate today is the basics,
[00:01:01.300 --> 00:01:05.160]   voice-based transfer of control from human to machine
[00:01:05.160 --> 00:01:07.960]   based on whether the driver is paying attention
[00:01:07.960 --> 00:01:08.960]   to the road or not.
[00:01:08.960 --> 00:01:12.300]   Inside, we have two cameras on the driver,
[00:01:12.300 --> 00:01:14.920]   one on the driver's face, one on the driver's body.
[00:01:14.920 --> 00:01:17.180]   We have two cameras looking at the external roadway,
[00:01:17.180 --> 00:01:20.660]   and we have a few other cameras for filming purposes.
[00:01:20.660 --> 00:01:22.440]   There's a center stack display
[00:01:22.440 --> 00:01:25.520]   showing who's in control of the vehicle, human or machine.
[00:01:25.520 --> 00:01:29.080]   So currently, the human is in control of the vehicle.
[00:01:29.080 --> 00:01:30.040]   Let's drive.
[00:01:30.040 --> 00:01:32.080]   Split the car and drive.
[00:01:32.080 --> 00:01:34.960]   On the center stack display, it shows the gear as drive.
[00:01:34.960 --> 00:01:42.400]   The perception, control, and driver state sensing algorithms
[00:01:42.400 --> 00:01:44.900]   you see today are running in real time,
[00:01:44.900 --> 00:01:47.720]   but the visualizations you're seeing in video
[00:01:47.720 --> 00:01:51.140]   are done in offline post-processing.
[00:01:52.880 --> 00:01:56.000]   Our perception system today is vision-based
[00:01:56.000 --> 00:01:58.560]   using two neural networks.
[00:01:58.560 --> 00:02:00.680]   One is doing road segmentation,
[00:02:00.680 --> 00:02:04.000]   the other is doing object detection of vehicles,
[00:02:04.000 --> 00:02:07.620]   cyclists, pedestrians, traffic signs, traffic lights.
[00:02:07.620 --> 00:02:12.000]   The acceleration, braking, and steering of the car
[00:02:12.000 --> 00:02:14.020]   is performed by PID controllers.
[00:02:14.020 --> 00:02:20.480]   The driver state sensing that we're showing today
[00:02:20.480 --> 00:02:23.840]   is glance region classification,
[00:02:23.840 --> 00:02:28.120]   and that's performed using 3D convolutional neural networks.
[00:02:28.120 --> 00:02:32.240]   High-level planning decisions to transfer control
[00:02:32.240 --> 00:02:33.960]   or to stop the vehicle are performed
[00:02:33.960 --> 00:02:36.200]   by a decision fusion algorithm
[00:02:36.200 --> 00:02:40.120]   that combines risk factors in the external environment
[00:02:40.120 --> 00:02:41.720]   and driver state,
[00:02:41.720 --> 00:02:44.560]   whether the driver's paying attention to the road or not.
[00:02:44.560 --> 00:02:49.600]   Safety for us is the number one priority, always.
[00:02:49.600 --> 00:02:51.760]   We are on a test track.
[00:02:51.760 --> 00:02:55.120]   The vehicles and pedestrians here today
[00:02:55.120 --> 00:02:58.240]   are all part of our team, all part of the demonstration.
[00:02:58.240 --> 00:03:00.680]   There's another safety driver in the car
[00:03:00.680 --> 00:03:03.120]   that can stop the vehicle at any moment
[00:03:03.120 --> 00:03:05.360]   by pressing a single button.
[00:03:05.360 --> 00:03:14.480]   Okay, let's engage in a distracting activity, Twitter,
[00:03:14.480 --> 00:03:17.740]   and let's send a tweet.
[00:03:17.740 --> 00:03:19.900]   (silence)
[00:03:19.900 --> 00:03:34.340]   I'm typing this tweet
[00:03:34.340 --> 00:03:42.940]   while driving in the MIT city center.
[00:03:42.940 --> 00:03:47.940]   In the MIT semi-autonomous vehicle
[00:03:47.940 --> 00:04:01.340]   on a test track.
[00:04:01.340 --> 00:04:03.500]   (silence)
[00:04:03.500 --> 00:04:13.380]   A test track.
[00:04:13.380 --> 00:04:18.500]   Lex, you appear distracted.
[00:04:18.500 --> 00:04:20.740]   Would you like me to take over?
[00:04:20.740 --> 00:04:21.780]   Yes, please.
[00:04:21.780 --> 00:04:24.580]   Great.
[00:04:24.580 --> 00:04:27.140]   I am taking control of steering and braking.
[00:04:27.140 --> 00:04:29.300]   (silence)
[00:04:29.300 --> 00:04:37.220]   The car is now in control
[00:04:37.220 --> 00:04:40.140]   as the center stack display shows.
[00:04:40.140 --> 00:04:43.020]   So I will continue with the tweet.
[00:04:43.020 --> 00:04:48.020]   The car knows that I'm not paying attention
[00:04:48.020 --> 00:04:53.020]   and has taken control after asking me nicely for it.
[00:05:16.220 --> 00:05:18.380]   (silence)
[00:05:18.380 --> 00:05:31.380]   Video out tomorrow.
[00:05:31.380 --> 00:05:36.140]   Okay, here goes nothing.
[00:05:36.140 --> 00:05:39.180]   It's posted.
[00:05:39.180 --> 00:05:45.140]   Very well might be the first tweet ever sent
[00:05:45.140 --> 00:05:47.780]   from an autonomous vehicle while it's driving itself.
[00:05:47.780 --> 00:05:51.940]   (silence)
[00:05:52.860 --> 00:05:55.940]   (car engine roaring)
[00:06:20.100 --> 00:06:22.860]   Elevated driving risk detected.
[00:06:22.860 --> 00:06:24.860]   I am stopping for a pedestrian.
[00:06:24.860 --> 00:06:40.780]   Lex, pedestrian is blocking our lane of travel.
[00:06:40.780 --> 00:06:41.740]   Should I honk?
[00:06:41.740 --> 00:06:45.100]   No, please shift gear to park.
[00:06:45.100 --> 00:06:47.660]   Great.
[00:06:47.660 --> 00:06:49.140]   We are now in park.
[00:06:49.540 --> 00:06:51.700]   (silence)
[00:06:51.700 --> 00:06:56.180]   That was a demo of the basics.
[00:06:56.180 --> 00:06:59.500]   Perception, motion planning, driver state sensing,
[00:06:59.500 --> 00:07:02.220]   transfer control and tweeting.
[00:07:02.220 --> 00:07:04.580]   And we have a lot more to explore together.
[00:07:04.580 --> 00:07:07.300]   Our team is working on various aspects
[00:07:07.300 --> 00:07:10.340]   of human centered artificial intelligence
[00:07:10.340 --> 00:07:12.860]   toward our mission to save lives
[00:07:12.860 --> 00:07:15.420]   through effective human robot collaboration.
[00:07:18.340 --> 00:07:20.500]   (silence)
[00:07:20.500 --> 00:07:22.660]   (silence)
[00:07:22.660 --> 00:07:24.820]   (silence)
[00:07:24.820 --> 00:07:26.980]   (silence)
[00:07:26.980 --> 00:07:29.140]   (silence)
[00:07:29.140 --> 00:07:31.300]   (silence)
[00:07:31.300 --> 00:07:33.460]   (silence)
[00:07:33.460 --> 00:07:35.620]   (silence)
[00:07:35.620 --> 00:07:45.620]   [BLANK_AUDIO]

