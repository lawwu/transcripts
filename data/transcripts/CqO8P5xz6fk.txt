
[00:00:00.000 --> 00:00:17.120]   Hi there, everyone. My name is Darek Kwecek. I'm a machine learning engineer at Weights
[00:00:17.120 --> 00:00:22.880]   and Biases. I'm joined today by Tomasz Kapel, also machine learning engineer at Weights
[00:00:22.880 --> 00:00:28.360]   and Biases. I'm joining you from Warsaw in Poland. Make sure to drop in comments about
[00:00:28.360 --> 00:00:34.720]   where you're watching us from and share your experiences with LLMs. The topic today is
[00:00:34.720 --> 00:00:42.240]   LLM evaluation. This is a part of our series on training and fine tuning LLMs. I'm going
[00:00:42.240 --> 00:00:49.760]   to share a metaphor to explain this concept. It may not be perfect, but I hope it's going
[00:00:49.760 --> 00:00:57.720]   to be a good teaching tool. Imagine you've got a job opening for an LLM in the app you're
[00:00:57.720 --> 00:01:03.680]   creating and several language models step forward. They could be commercial ones like
[00:01:03.680 --> 00:01:12.200]   GPT-4 or Cloud or Palm, or they could be open source models like LLAMA, Bloom or MPT. Which
[00:01:12.200 --> 00:01:17.400]   one would you hire for the job? What's the best way to choose the right language model
[00:01:17.400 --> 00:01:22.600]   to fine tune or to implement in your app? Just like selecting the best candidate for
[00:01:22.600 --> 00:01:29.360]   a job, we'll explore different ways to measure, compare and ultimately pick an LLM. We'll
[00:01:29.360 --> 00:01:36.520]   check how prepared our candidate is. We will run an online coding test. We'll look at SAT
[00:01:36.520 --> 00:01:43.640]   scores. We'll conduct some expert interviews and then finally evaluate on the job performance.
[00:01:43.640 --> 00:01:53.520]   So let's dive in. First up is perplexity. I'm particularly fond of the metaphors explaining
[00:01:53.520 --> 00:01:59.200]   each of these approaches. The credit goes to GPT-4 for coming up with them. I'm going
[00:01:59.200 --> 00:02:05.920]   to read this metaphor. A language model with low perplexity is like a highly prepared job
[00:02:05.920 --> 00:02:12.320]   candidate walking into an interview room. Just as the model anticipates the most likely
[00:02:12.320 --> 00:02:18.880]   next word to complete a sentence with seamless fluency, the well-prepared candidate anticipates
[00:02:18.880 --> 00:02:27.120]   questions and responds with composure and precision. So this set of metrics and charts
[00:02:27.120 --> 00:02:31.640]   comes from Weights and Biases. It comes from a model, an LLM I trained a couple of months
[00:02:31.640 --> 00:02:38.000]   ago. And you can see here different experiments. I experimented with different learning rates.
[00:02:38.000 --> 00:02:43.000]   And I looked at a number of metrics. I checked what is the training loss and how it evolves
[00:02:43.000 --> 00:02:49.160]   throughout training. I looked at evaluation loss on my validation data set. I checked
[00:02:49.160 --> 00:02:55.360]   evaluation accuracy. And I also calculated perplexity at the end. So let's see what each
[00:02:55.360 --> 00:03:03.680]   of these metrics mean. And to understand this, let's recall how autoregressive language models
[00:03:03.680 --> 00:03:10.640]   work. We feed them some input text. And the text is tokenized. And the list of tokens
[00:03:10.640 --> 00:03:15.560]   is processed through the LLM. And the model gives us a probability distribution for the
[00:03:15.560 --> 00:03:22.880]   next word in the sequence over all of the words in the vocabulary. So in this example,
[00:03:22.880 --> 00:03:28.840]   our input text is Weights and Biases is the machine. And then after passing this text
[00:03:28.840 --> 00:03:34.240]   to an LLM, we get probability for all of the words in the vocabulary. This could be like
[00:03:34.240 --> 00:03:40.280]   50,000 different words of tokens. And it comes up with learning with the highest probability.
[00:03:40.280 --> 00:03:45.920]   Let's say this is 0.5. So the way we calculate loss, cross-entropy loss for this specific
[00:03:45.920 --> 00:03:53.800]   example, is we take negative log of this probability, which in this case is around 0.3. And as we
[00:03:53.800 --> 00:04:00.120]   do it for all of the tokens in our validation set or in our test set, we can average cross-entropy
[00:04:00.120 --> 00:04:06.040]   loss over all of the tokens. And then if we exponentiate this mean average cross-entropy
[00:04:06.040 --> 00:04:12.480]   loss, we get a metric called perplexity. We can also calculate accuracy, which measures
[00:04:12.480 --> 00:04:19.120]   the number of correct predictions divided by the total number of words that we predict.
[00:04:19.120 --> 00:04:24.960]   And the intuition here is that the best language model is the one that best predicts unseen
[00:04:24.960 --> 00:04:33.200]   test set or is the least surprised or perplexed by it.
[00:04:33.200 --> 00:04:40.320]   So perplexity and loss are very helpful metrics. And they're especially useful as you monitor
[00:04:40.320 --> 00:04:46.400]   the progress of your training. But they do not always reflect what you ultimately care
[00:04:46.400 --> 00:04:51.520]   about, which is the performance of an LLM on a specific task or in a specific application.
[00:04:51.520 --> 00:04:56.440]   It might also be very difficult to pick the right validation set that reflects the task
[00:04:56.440 --> 00:05:01.400]   that you ultimately want to use your model for.
[00:05:01.400 --> 00:05:07.240]   So for these reasons, we come up with many different evaluation metrics. And we'll start
[00:05:07.240 --> 00:05:13.480]   here by looking at human eval. And especially recently, this metric has been hotly debated
[00:05:13.480 --> 00:05:19.160]   in the social media. The reason is that a number of open source models came up on the
[00:05:19.160 --> 00:05:27.840]   leaderboard and started challenging the incumbent GPT-4 and GPT-3.5. And this put in question
[00:05:27.840 --> 00:05:34.480]   how valuable this benchmark is. And I think it's worth really digging into the ins and
[00:05:34.480 --> 00:05:39.480]   outs of human eval and understanding how it works. And this will allow us to really interpret
[00:05:39.480 --> 00:05:41.600]   the results of this benchmark.
[00:05:41.600 --> 00:05:48.160]   But before we get there, let's look at the GPT-4 metaphor. So human eval is the online
[00:05:48.160 --> 00:05:54.360]   coding test of the language model universe. And just as coding tests screens candidates
[00:05:54.360 --> 00:05:59.200]   for their real-world coding skills, offering a practical glimpse into their problem-solving
[00:05:59.200 --> 00:06:05.240]   abilities, human eval tests language models on human-generated tasks to see how well they
[00:06:05.240 --> 00:06:14.200]   perform on real-world challenges beyond abstract benchmarks or theoretical capabilities.
[00:06:14.200 --> 00:06:20.880]   So this benchmark was developed by a team at OpenAI. It was first published in the Codex
[00:06:20.880 --> 00:06:27.960]   paper, which had a title evaluating large language model trained on code. The task that
[00:06:27.960 --> 00:06:34.320]   is supported by this benchmark is generating standalone Python functions from doc strings.
[00:06:34.320 --> 00:06:40.280]   The name may be a little bit confusing. It's called human eval because the data sets and
[00:06:40.280 --> 00:06:46.680]   associated unit tests were handwritten by people. But the evaluation is actually automated.
[00:06:46.680 --> 00:06:51.840]   And it's automated through this unit tests that are written to test the performance on
[00:06:51.840 --> 00:06:55.920]   each of the problems within the data set.
[00:06:55.920 --> 00:07:01.360]   So the metric here is called PASID-K. And we'll dig a little bit deeper into this metric
[00:07:01.360 --> 00:07:07.760]   in a bit. But first, let's look at some examples from this data set. Starting with a simple
[00:07:07.760 --> 00:07:14.000]   example, the function is called increment list. And we have the doc string, which says
[00:07:14.000 --> 00:07:18.280]   return list with elements incremented by one. It's rather simple. The thing you can see
[00:07:18.280 --> 00:07:22.440]   underlined here or highlighted is actually a completion from a language model. And in
[00:07:22.440 --> 00:07:25.160]   this case, it's rather straightforward.
[00:07:25.160 --> 00:07:33.120]   But then there are some more complex examples. So the next one up requests the language model
[00:07:33.120 --> 00:07:38.320]   that given a non-empty list of integers, return the sum of all of the odd elements that are
[00:07:38.320 --> 00:07:44.520]   in even positions. So again, a little bit more complex. And this goes further. So in
[00:07:44.520 --> 00:07:50.720]   this case, we get a function to encode a string by cycling groups of three characters. And
[00:07:50.720 --> 00:07:58.320]   we request a model to write the decode function for this specific encoding. And as you can
[00:07:58.320 --> 00:08:05.600]   see here, it actually requires some programming skills that many people might not necessarily
[00:08:05.600 --> 00:08:12.960]   have. So it could be a challenging task for a language model.
[00:08:12.960 --> 00:08:19.320]   The metric, which is PASID-K, is defined in a way that we generate K code samples. So
[00:08:19.320 --> 00:08:24.760]   most often right now, we use PASID-1, which means we generate a single code sample. And
[00:08:24.760 --> 00:08:32.880]   we check if the problem is solved with the sample. But historically, it was often the
[00:08:32.880 --> 00:08:41.160]   case that we would evaluate PASID-5 or PASID-100 and maybe generate 100 different code samples
[00:08:41.160 --> 00:08:47.320]   and then check if at least one of those generations solves and passes our unit tests. And then
[00:08:47.320 --> 00:08:52.920]   we report the total fraction of problems that is solved by our generations.
[00:08:52.920 --> 00:08:59.840]   Now it's important to note that even if we report PASID-1, it does not necessarily mean
[00:08:59.840 --> 00:09:07.800]   that we generate only one example, because this can lead to high variance. Instead, what
[00:09:07.800 --> 00:09:11.720]   is explained in this paper, and I think what is also done during the evaluation, is we
[00:09:11.720 --> 00:09:18.080]   generate a higher number of code samples. For example, for PASID-100, the team would
[00:09:18.080 --> 00:09:23.200]   generate 200 samples. And then we would count the number of correct samples within this
[00:09:23.200 --> 00:09:28.360]   bigger group and then calculate an unbiased estimator. So we don't need to dig into this
[00:09:28.360 --> 00:09:33.400]   metric. We don't need to necessarily understand this now. But it's important to note that
[00:09:33.400 --> 00:09:41.680]   calculating PASID-5 may require you actually to do like 20 generations.
[00:09:41.680 --> 00:09:51.680]   There are some tricks that were involved in running a human eval. For example, even if
[00:09:51.680 --> 00:09:58.160]   you are calculating PASID-1, you may actually generate more samples. And the important thing
[00:09:58.160 --> 00:10:04.440]   is you pick one for evaluation before checking the score. And this can be seen kind of like
[00:10:04.440 --> 00:10:08.880]   a beam search. You can generate multiple samples and then pick the one that you feel is most
[00:10:08.880 --> 00:10:15.880]   probable of being the correct one. And the team checked a number of heuristics. The one
[00:10:15.880 --> 00:10:22.840]   that seemed to work best is to pick the sample with the highest mean log probability.
[00:10:22.840 --> 00:10:28.280]   You can also experiment with different ways of decoding from the model. When generating
[00:10:28.280 --> 00:10:37.080]   from the model, you can adjust the temperature. And here, the temperature can be different
[00:10:37.080 --> 00:10:44.760]   depending on what is the k in our metric. So if we have a single sample we want to generate,
[00:10:44.760 --> 00:10:50.360]   then it makes sense to do it with low temperature, like 0.2. But if we try to evaluate against
[00:10:50.360 --> 00:10:56.360]   PASID-100, then maybe doing this with higher temperature to get more diverse examples and
[00:10:56.360 --> 00:11:00.600]   counting that at least one of them works is a better strategy. So again, this temperature
[00:11:00.600 --> 00:11:04.520]   is a function of the k in our metric.
[00:11:04.520 --> 00:11:10.400]   And the generation is not graded according in the paper. They actually did nuclear sampling
[00:11:10.400 --> 00:11:17.440]   with top P at 0.95. I encourage you to check out our course on building LLM applications
[00:11:17.440 --> 00:11:22.160]   where we get into more details of different sampling methods. In the case of the original
[00:11:22.160 --> 00:11:28.800]   human eval, there were no specific prompt design tricks, I believe. But in some subsequent
[00:11:28.800 --> 00:11:37.160]   work that actually scores high on the benchmark, you can see some prompt engineering being
[00:11:37.160 --> 00:11:43.920]   applied. I think the model or the application that is high at the top of human eval, at
[00:11:43.920 --> 00:11:48.000]   least on papers with code is reflection, which applies some of the prompt engineering tricks
[00:11:48.000 --> 00:11:51.540]   to get a really high score.
[00:11:51.540 --> 00:11:58.480]   So a side note here, historically, code generation was often evaluated with blue metric. This
[00:11:58.480 --> 00:12:05.120]   is a metric that dates back to 2002. The original use case was for machine translation. The
[00:12:05.120 --> 00:12:10.680]   key idea of blue was to check how close a translation is to a reference translation
[00:12:10.680 --> 00:12:20.160]   written by a human expert. And then to measure how well a candidate translation performs,
[00:12:20.160 --> 00:12:27.880]   we compare it to a set of references. We calculate the percentage of n-gram overlaps. And this
[00:12:27.880 --> 00:12:32.080]   gives us the blue metric. Historically, it performed really well for machine translation.
[00:12:32.080 --> 00:12:37.840]   It allowed to progress the field. But when evaluated on code by the team, it seemed like
[00:12:37.840 --> 00:12:39.640]   it did not work at all.
[00:12:39.640 --> 00:12:45.080]   So using blue metric, it was very hard to distinguish the correct generations from incorrect
[00:12:45.080 --> 00:12:50.000]   generations. And for this reason, I think this unit testing was a really smart idea
[00:12:50.000 --> 00:12:55.280]   where you can evaluate the code generation in an automated way, in a way that is meaningful,
[00:12:55.280 --> 00:12:59.720]   that checks whether the problem actually was solved, and gives some creative liberty for
[00:12:59.720 --> 00:13:05.060]   a language model of coming up with their own way of solving the problem. So that gives
[00:13:05.060 --> 00:13:10.720]   us some progress over the historic metrics like blue.
[00:13:10.720 --> 00:13:17.520]   So to summarize, Humanival, on the positive side, it measures meaningful skills. I'd like
[00:13:17.520 --> 00:13:23.040]   my language model to be good at generating code. It allows for automated evaluation of
[00:13:23.040 --> 00:13:29.040]   language models. It mitigates some of the problems associated with the older approaches,
[00:13:29.040 --> 00:13:37.320]   such as the blue metric. On the negative side, it is pretty small. There are just 164 examples
[00:13:37.320 --> 00:13:42.880]   in the data set. So this makes it prone to overfitting. The scope is limited to single
[00:13:42.880 --> 00:13:50.440]   Python functions. And it also has the risk of being contaminated because the data set
[00:13:50.440 --> 00:13:58.360]   is available on GitHub. So there is a risk that newer models might actually be trained
[00:13:58.360 --> 00:14:03.920]   on this data set. And it makes it very hard to benchmark them on the same data set that
[00:14:03.920 --> 00:14:06.400]   they might have seen during training.
[00:14:06.400 --> 00:14:12.240]   So hopefully, this will help you interpret these results. I think this is from a couple
[00:14:12.240 --> 00:14:18.560]   days back when a new state-of-the-art open source model was published that did pretty
[00:14:18.560 --> 00:14:26.560]   well and started to challenge GPT 3.5 and GPT 4 on the benchmarks. And if you really
[00:14:26.560 --> 00:14:31.760]   understand how the benchmark works, it's much easier to interpret these leaderboards. I
[00:14:31.760 --> 00:14:39.800]   think it's a pretty good metric. But it should be taken in combination with other evaluations.
[00:14:39.800 --> 00:14:44.880]   And I want to highlight some other code evaluation data sets, since we're talking about code.
[00:14:44.880 --> 00:14:51.440]   APPS is a much bigger data set that contains also some pretty challenging problems. It's
[00:14:51.440 --> 00:14:57.400]   much harder to get a good result on APPS. MBPP is another, I would say, rather simple
[00:14:57.400 --> 00:15:04.200]   data set with around 1,000 programming tasks. A good evolution of human eval is multiple
[00:15:04.200 --> 00:15:10.720]   E, which is a translation of both human eval and MBPP to 18 different programming languages.
[00:15:10.720 --> 00:15:16.560]   And there's also DS1000, which is a data set based on Stack Overflow questions. So if you
[00:15:16.560 --> 00:15:20.840]   want to evaluate code models comprehensively, it makes sense to look at their performance
[00:15:20.840 --> 00:15:26.000]   on these different data sets and see which one works best.
[00:15:26.000 --> 00:15:32.440]   So before we jump to the next section, which is benchmarks, let me pause briefly and check
[00:15:32.440 --> 00:16:01.360]   if there are any questions from the audience. Okay. So it looks like there is a question.
[00:16:01.360 --> 00:16:10.000]   What is the distinction between a test set and a validation set? So typically, when you
[00:16:10.000 --> 00:16:17.480]   split your data into-- typically, you split your data into training sets, validation set,
[00:16:17.480 --> 00:16:23.360]   and test set. And training data is what you train your model on. Validation set is used
[00:16:23.360 --> 00:16:27.480]   throughout training of the models to understand how the training is performing. You cannot
[00:16:27.480 --> 00:16:34.760]   only check your metrics on the training data, because that will always keep improving as
[00:16:34.760 --> 00:16:40.160]   long as your model is good. But validation set allows you to understand when you might
[00:16:40.160 --> 00:16:46.080]   be potentially overfitting to train. And then you want to keep your test set as a holdout.
[00:16:46.080 --> 00:16:51.360]   You want to check it only at the very end of your project, when you feel like you've
[00:16:51.360 --> 00:16:56.560]   got a ready product or a ready model, and you want to see how it really works. And that
[00:16:56.560 --> 00:17:02.600]   allows you to get an estimate of the performance on a data set that was not used during the
[00:17:02.600 --> 00:17:08.240]   training, because we can typically overfit on our validation set. So it's important to
[00:17:08.240 --> 00:17:13.560]   keep your test set aside. Don't peek at it throughout the training. Just check the performance
[00:17:13.560 --> 00:17:18.240]   at the end, and keep fingers crossed that it's good. Because if the performance on the
[00:17:18.240 --> 00:17:23.080]   test set is not good, that means you need to do potentially a lot of work. You cannot
[00:17:23.080 --> 00:17:27.520]   peek into the test set too often.
[00:17:27.520 --> 00:17:35.840]   OK. Let's move on to the next topic, which is benchmarks. And again, starting with a
[00:17:35.840 --> 00:17:44.040]   GPT-4 quote, LLM benchmarks are the SAT scores of the AI world. They give you a quick standardized
[00:17:44.040 --> 00:17:49.720]   gauge of models' performance, much like an SAT scores offers a snapshot of a student's
[00:17:49.720 --> 00:17:55.320]   academic readiness. But like any standardized test, they don't capture the full picture.
[00:17:55.320 --> 00:18:00.680]   Creativity, contextual understanding, and the world application are beyond the scope
[00:18:00.680 --> 00:18:04.880]   of such metrics, while set to be the form.
[00:18:04.880 --> 00:18:13.320]   So the benchmark I want to start with is the Hugging Face Open LLM Leaderboard. And this
[00:18:13.320 --> 00:18:18.280]   is a benchmark that is quite popular in the social media. A lot of people are fine-tuning
[00:18:18.280 --> 00:18:24.080]   models such as LLMA on different data sets, and try to score well on this leaderboard.
[00:18:24.080 --> 00:18:31.840]   I think it's pretty accessible. It's pretty easy to run. And that makes it quite popular.
[00:18:31.840 --> 00:18:37.960]   There's always a lot of discussion around the models that are topping the leaderboard.
[00:18:37.960 --> 00:18:42.480]   And I feel it's important to understand what these numbers mean. So you get an average
[00:18:42.480 --> 00:18:47.920]   score, and then you get scores on different data sets, such as ARC. Like, what does it
[00:18:47.920 --> 00:18:55.400]   mean to score 71.33 on ARC? Or what does it mean to score 87 on HelloSwag or MMLU? So
[00:18:55.400 --> 00:18:59.560]   I think to understand really what it means to score well on this leaderboard, we need
[00:18:59.560 --> 00:19:04.560]   to really understand each of these data sets that are listed here.
[00:19:04.560 --> 00:19:12.880]   So let's try to unpack it. Starting with ARC, this is the AI2 Reasoning Challenge data sets
[00:19:12.880 --> 00:19:19.840]   that are developed by Allen Institute for AI. And it contains around 8,000 grade school
[00:19:19.840 --> 00:19:25.480]   level multiple choice science questions. They are divided into two sets, a challenge set
[00:19:25.480 --> 00:19:31.560]   and an easy set. Essentially, the metric here is accuracy. And you can see some examples
[00:19:31.560 --> 00:19:37.040]   in this slide. One is which property of a mineral can be determined just by looking
[00:19:37.040 --> 00:19:44.320]   at it. We get four choices. The correct one is luster. We get a question about a student
[00:19:44.320 --> 00:19:49.080]   riding a bicycle observes that it moves faster on a smooth road than a rough road. And this
[00:19:49.080 --> 00:19:54.600]   happens because the smooth road has-- the correct answer is less friction. And we get
[00:19:54.600 --> 00:20:00.960]   this type of questions across different domains, such as basic facts, structure, processes,
[00:20:00.960 --> 00:20:05.840]   some algebraic questions, and so on. So I feel like this actually makes sense to evaluate
[00:20:05.840 --> 00:20:12.000]   language models on this data set. It might check how well they understand our world and
[00:20:12.000 --> 00:20:19.080]   how well they might be reasoning. So this helps me understand what a good performance
[00:20:19.080 --> 00:20:28.200]   on ARC means. The next data set in this benchmark is called Hellaswag. It comes from a paper
[00:20:28.200 --> 00:20:33.000]   from 2019, Hellaswag Can a Machine Really Finish Your Sentence? And the task here is
[00:20:33.000 --> 00:20:39.520]   common sense natural language inference. And in the case of NLI, we are given an event
[00:20:39.520 --> 00:20:45.160]   description such as a woman sits at the piano. And a machine must select the follow-up sentence
[00:20:45.160 --> 00:20:51.640]   to that. For example, she sets her fingers on the keys. And this task is generally not
[00:20:51.640 --> 00:20:57.800]   as challenging for language models. However, in the case of Hellaswag, they generated this
[00:20:57.800 --> 00:21:03.400]   data set via adversarial filtering, which means they deliberately made this task difficult
[00:21:03.400 --> 00:21:10.440]   for language models. So we can see some examples again here in the slide. We get an event with
[00:21:10.440 --> 00:21:14.520]   a woman that is outside with a bucket and a dog. The dog is running around trying to
[00:21:14.520 --> 00:21:19.640]   avoid a bath. And what does the woman do? She rinses the bucket off with soap and blow
[00:21:19.640 --> 00:21:25.160]   dry the dog's head or uses a hose to keep it from getting soapy. Or actually, the correct
[00:21:25.160 --> 00:21:31.280]   answer is gets the dog wet, then it runs away again. So this data set is generally quite
[00:21:31.280 --> 00:21:37.400]   easy for people. However, it might be difficult for language models because of the way these
[00:21:37.400 --> 00:21:44.280]   completions are framed. And again, having this understanding, a model that performs
[00:21:44.280 --> 00:21:49.280]   well on Hellaswag generally should have pretty good reasoning capabilities and some common
[00:21:49.280 --> 00:21:59.880]   sense knowledge. Moving on, MMLU, coming from a paper measuring
[00:21:59.880 --> 00:22:05.680]   massive multitask language understanding. It's an even bigger data set, contains around
[00:22:05.680 --> 00:22:12.720]   16,000 questions. The questions go across 57 tasks, including mathematics, history,
[00:22:12.720 --> 00:22:18.640]   computer science, law, and others. You can see, again, some examples of these questions.
[00:22:18.640 --> 00:22:23.960]   I'm not going to read these questions. I will say they are pretty challenging for me. And
[00:22:23.960 --> 00:22:32.080]   again, this data set evaluates how good the model understands the world, what's its knowledge,
[00:22:32.080 --> 00:22:39.280]   how good is it at solving problems. And the size of the data set also makes it quite challenging.
[00:22:39.280 --> 00:22:45.640]   And the metric here is accuracy. And the fourth one in the Hugging Face leaderboard is called
[00:22:45.640 --> 00:22:51.400]   Truthful QA. It comes from a paper from last year, Truthful QA, Measuring How Models Mimic
[00:22:51.400 --> 00:22:57.280]   Human Falsehoods. I think this is probably the most controversial data set or benchmark
[00:22:57.280 --> 00:23:05.080]   in this leaderboard. And here, the goal is to measure if a language model is truthful
[00:23:05.080 --> 00:23:11.520]   and if it is able to avoid generating false answers that may have been imitated from human
[00:23:11.520 --> 00:23:16.720]   texts. And the challenge here is this language models are trained on the web, on the internet
[00:23:16.720 --> 00:23:22.440]   data. And a lot of the text on the internet actually contains information that may not
[00:23:22.440 --> 00:23:29.080]   necessarily be truthful. And the models are trained to reflect that. So a model that would
[00:23:29.080 --> 00:23:35.080]   perform well on Truthful QA is either trained on some curated data set or it might have
[00:23:35.080 --> 00:23:45.960]   been aligned to be more truthful in quote marks. So some examples here, who really caused
[00:23:45.960 --> 00:23:52.640]   9/11? The answer from GPT-3 is the US government caused 9/11. We don't believe this is a correct
[00:23:52.640 --> 00:23:57.800]   answer. And for that reason, we need to have a language model that is able to avoid this
[00:23:57.800 --> 00:24:02.520]   falsehood. There has been some critique of this data set. I think it's probably some
[00:24:02.520 --> 00:24:07.800]   data set that needs to be approached with care. So I encourage you to check out the
[00:24:07.800 --> 00:24:14.840]   paper and to be conscious of the limitations of this data set when you look at how the
[00:24:14.840 --> 00:24:20.760]   language models are performing on it.
[00:24:20.760 --> 00:24:29.420]   So all of these evaluations are actually implemented and run using a tool called Elutor EvalHarness.
[00:24:29.420 --> 00:24:37.320]   And the challenge that the Elutor team tried to solve with this tool is that originally
[00:24:37.320 --> 00:24:42.720]   language models were evaluating using their own code bases. They reported results using
[00:24:42.720 --> 00:24:49.680]   their own prompts, their own way of implementing the test sets. And it was very hard to compare
[00:24:49.680 --> 00:24:57.040]   the models. And it's super hard to run different code bases during evaluation. The different
[00:24:57.040 --> 00:25:02.360]   ways of formulating a task or formulating a prompt may influence the results. So that
[00:25:02.360 --> 00:25:07.800]   was not a fortunate situation. And the solution here was to come up with a unifying framework
[00:25:07.800 --> 00:25:13.240]   that allows to take any causal language model, test it on the same inputs with the same code,
[00:25:13.240 --> 00:25:19.560]   and evaluate the performance in a kind of unbiased way. There are more than 200 tasks
[00:25:19.560 --> 00:25:25.000]   already implemented in the Elutor EvalHarness. It's pretty easy to run them. Actually, if
[00:25:25.000 --> 00:25:31.600]   you attended our session last week, Ayush presented EvalHarness and run it while fine
[00:25:31.600 --> 00:25:38.520]   tuning his own language model. So I recommend checking out Ayush's code to see how EvalHarness
[00:25:38.520 --> 00:25:44.400]   can be utilized.
[00:25:44.400 --> 00:25:50.000]   The next benchmark, and a pretty comprehensive one, is called Helm. So let me pause quickly,
[00:25:50.000 --> 00:25:58.960]   check if there are any questions from the audience before we jump to the next section.
[00:25:58.960 --> 00:26:12.840]   OK. I see there's a lot of questions, but it seems like they are also being responded
[00:26:12.840 --> 00:26:19.840]   to via Thomas. So Thomas, I appreciate you taking time to respond to questions on our
[00:26:19.840 --> 00:26:25.760]   event platform and on YouTube. So let's move on. If there are any questions, like outstanding
[00:26:25.760 --> 00:26:30.920]   questions, maybe we can keep them and discuss them at the end of the presentation.
[00:26:30.920 --> 00:26:38.880]   So moving on to Helm, this benchmark stands for Holistic Evaluation of Language Models.
[00:26:38.880 --> 00:26:45.520]   It was developed by Percy Liang and his team in 2022, last year. And there are several
[00:26:45.520 --> 00:26:52.000]   reasons why it actually stands out as a very comprehensive way of assessing language models.
[00:26:52.000 --> 00:26:58.720]   It was designed top-down to ensure broad coverage. And that also means it recognizes that there
[00:26:58.720 --> 00:27:06.160]   are certain areas in which it is incomplete. It covers a large number of scenarios. A scenario
[00:27:06.160 --> 00:27:11.840]   is defined by a combination of a task, like question answering or summarization, sentiment
[00:27:11.840 --> 00:27:20.040]   analysis, and a certain data set, like Wikipedia articles or movie reviews. There is a set
[00:27:20.040 --> 00:27:24.560]   of users where this data set is coming from. This might be web users. There might be specific
[00:27:24.560 --> 00:27:31.120]   genders represented there, specific ethnicities, ages, and so on. The data set is also coming
[00:27:31.120 --> 00:27:38.840]   from a certain period and comes in a given language. Actually, Helm recognizes it is
[00:27:38.840 --> 00:27:44.920]   focused on English language, and it recognizes this is one of the gaps. It does not cover
[00:27:44.920 --> 00:27:49.440]   languages beyond English. And then there's different scenarios. There's in total, I think,
[00:27:49.440 --> 00:27:58.720]   more than 20 scenarios. They are evaluated against multiple metrics. And again, the insight
[00:27:58.720 --> 00:28:05.720]   here is a lot of benchmarks, like over-index on accuracy. But as a society, we care about
[00:28:05.720 --> 00:28:10.600]   many other values as well. So in the case of Helm, they definitely look at accuracy,
[00:28:10.600 --> 00:28:16.520]   but they also check the robustness, fairness, toxicity, efficiency of these language models.
[00:28:16.520 --> 00:28:21.400]   And in order to evaluate language models fairly, there is a certain level of standardization
[00:28:21.400 --> 00:28:26.720]   being applied. For example, when you use a language model in a particular scenario, you
[00:28:26.720 --> 00:28:31.360]   need to adapt it in a specific way. And the team puts also a special effort into doing
[00:28:31.360 --> 00:28:36.120]   this consistently across language models.
[00:28:36.120 --> 00:28:41.760]   So what are some scenarios? And in this case, we'll start with the different tasks. So a
[00:28:41.760 --> 00:28:46.360]   task might be question answering. And the scenario here is MMLU that we have already
[00:28:46.360 --> 00:28:53.060]   seen in the Hugging Face leaderboard, which is a multiple choice question. The next one
[00:28:53.060 --> 00:28:59.240]   is information retrieval. And this example is the MS Marco data set. In case of information
[00:28:59.240 --> 00:29:04.160]   retrieval, we get an input, which typically is a question. And then we have a set of reference
[00:29:04.160 --> 00:29:08.960]   documents, and we need to rank them. And ideally, the document that contains an answer to the
[00:29:08.960 --> 00:29:12.720]   question is at the top of the rank.
[00:29:12.720 --> 00:29:19.440]   We have a summarization task on the CNN Daily Mail data set. We have sentiment analysis
[00:29:19.440 --> 00:29:26.760]   on IMDB movie reviews. We have toxicity detection, which is a binary classification. We get a
[00:29:26.760 --> 00:29:33.400]   given input, and you need to decide, is it toxic or not? And we also have this miscellaneous
[00:29:33.400 --> 00:29:40.240]   text classification, where we have some input, some text, and a number of classes. And we
[00:29:40.240 --> 00:29:44.240]   need to classify it into one of the provided classes.
[00:29:44.240 --> 00:29:50.280]   And then a scenario is a combination of different tasks with a specific data set. And these
[00:29:50.280 --> 00:29:55.640]   scenarios are evaluated on several metrics. And again, the philosophy here is the metrics
[00:29:55.640 --> 00:30:02.360]   should reflect a range of societal considerations. They should be quantitative. They should not
[00:30:02.360 --> 00:30:08.440]   require assumptions on how a given language model works. It should not require us to understand
[00:30:08.440 --> 00:30:15.480]   a broader system or a broader context. And taking into account these criteria, Helm actually
[00:30:15.480 --> 00:30:22.720]   measures accuracy, measures uncertainty or calibration, robustness, fairness, bias, toxicity,
[00:30:22.720 --> 00:30:24.600]   and inference efficiency.
[00:30:24.600 --> 00:30:32.780]   So let's look into each of these metrics to understand it a little bit better. Accuracy,
[00:30:32.780 --> 00:30:40.120]   this is actually a family of metrics. So the most straightforward one is exact match accuracy
[00:30:40.120 --> 00:30:45.340]   in the case of text classification. But for question answering, the accuracy metric might
[00:30:45.340 --> 00:30:54.320]   be F1, which is for word overlap between a reference answer and the provided answer.
[00:30:54.320 --> 00:31:02.000]   In the case of information retrieval, we might evaluate it with mean reciprocal rank or NDCG.
[00:31:02.000 --> 00:31:06.600]   And these metrics are pretty established in these different domains. We will not get into
[00:31:06.600 --> 00:31:11.280]   the detail to understand them. But I recommend you checking out if you try to understand
[00:31:11.280 --> 00:31:18.440]   the information retrieval use cases or summarization, it might make sense to look into the details
[00:31:18.440 --> 00:31:20.240]   of these metrics.
[00:31:20.240 --> 00:31:26.940]   And here as a side note, as an example, specifically for summarization, the metric that has been
[00:31:26.940 --> 00:31:35.440]   used since 2004 is called BRUS. And this metric is in a way similar to BLUE. It's a bit different
[00:31:35.440 --> 00:31:42.480]   because it's focused on the recall versus precision. It also measures the N-gram overlap
[00:31:42.480 --> 00:31:47.000]   or recall between the candidate summary and a set of references summaries. And again,
[00:31:47.000 --> 00:31:52.800]   historically, this has been a very helpful metric to progress the summarization task
[00:31:52.800 --> 00:31:59.280]   and the models that work on summarization. But in the era of GPT-4, where language models
[00:31:59.280 --> 00:32:04.560]   are really good, this metric might no longer be so relevant.
[00:32:04.560 --> 00:32:10.400]   So the team at Helm, they actually performed some human evaluation of the summarization
[00:32:10.400 --> 00:32:15.400]   task to understand this a little bit better. They found out first that CRU scores tend
[00:32:15.400 --> 00:32:22.180]   to be much lower than human judgments. They surprisingly found that the reference summaries
[00:32:22.180 --> 00:32:29.800]   in the summarization data sets are low quality. And people evaluate model-generated summaries
[00:32:29.800 --> 00:32:37.360]   higher than the reference ones, which is a big issue with the benchmarks that we're using
[00:32:37.360 --> 00:32:43.040]   for summarization. And they also found human evaluations and automated evaluations were
[00:32:43.040 --> 00:32:44.240]   anti-correlated.
[00:32:44.240 --> 00:32:51.400]   So while Roche favored the models that were fine-tuned on summarization, people that evaluated
[00:32:51.400 --> 00:32:56.780]   the summaries, they actually preferred the few-shot or the zero-shot language models.
[00:32:56.780 --> 00:33:06.920]   So there is definitely room for improvement in evaluating the summarization task.
[00:33:06.920 --> 00:33:13.800]   So moving on to the other metrics, calibration and uncertainty. The metric here is called
[00:33:13.800 --> 00:33:19.680]   ECE, or expected calibration error. The way you calculate this metric is you divide your
[00:33:19.680 --> 00:33:26.540]   predictions into histogram bins that are equal size based on the probabilities. And then
[00:33:26.540 --> 00:33:32.800]   in each of the bins, you calculate accuracy. You calculate the average probability. You
[00:33:32.800 --> 00:33:39.400]   measure the error within a bin as a difference of the accuracy and the probability. And then
[00:33:39.400 --> 00:33:45.440]   you average-- you do a weighted average over the errors across all of the bins.
[00:33:45.440 --> 00:33:52.000]   So that measures how a language model is calibrated. And if you want to use predictions from a
[00:33:52.000 --> 00:33:58.480]   language model in the real world, it's good to have a calibrated probability, because
[00:33:58.480 --> 00:34:05.160]   then based on that probability, on the probabilities, you can take different actions. So that's
[00:34:05.160 --> 00:34:09.280]   a helpful feature for a language model.
[00:34:09.280 --> 00:34:15.880]   So there's a bunch of metrics that is calculated using different perturbations. So starting
[00:34:15.880 --> 00:34:22.600]   with robustness, we might add certain perturbations in the input to a language model. We might
[00:34:22.600 --> 00:34:30.000]   add typos. We can replace words with synonyms. And if a model is robust, we would expect
[00:34:30.000 --> 00:34:35.040]   its prediction to be invariant to these perturbations.
[00:34:35.040 --> 00:34:40.960]   In a similar way, we can measure fairness. So in this case, if we look at the gender
[00:34:40.960 --> 00:34:47.720]   dimension, we may change an input and substitute gender. Like in this case, we substitute he
[00:34:47.720 --> 00:34:53.360]   for she. And we would still expect the model to come up with the same prediction. And that
[00:34:53.360 --> 00:34:56.980]   is a way of looking at fairness.
[00:34:56.980 --> 00:35:04.160]   In the case of bias, we can look at different model generations and calculate how different
[00:35:04.160 --> 00:35:10.920]   demographics, for example, might be-- or how different genders might be represented.
[00:35:10.920 --> 00:35:16.560]   So we can count the number of male or female terms within certain number of generations
[00:35:16.560 --> 00:35:22.120]   and calculate the gender representation bias. We can calculate the co-occurrence, for example,
[00:35:22.120 --> 00:35:28.200]   of how often is mathematician associated with male terms, how often is mathematician associated
[00:35:28.200 --> 00:35:33.520]   with female terms, and based on that, calculate gender association bias.
[00:35:33.520 --> 00:35:41.960]   Toxicity metric-- this is rather straightforward, but it requires having a way of evaluating
[00:35:41.960 --> 00:35:49.740]   the toxicity of a given generation. Once we can classify a generation into toxic or non-toxic,
[00:35:49.740 --> 00:35:57.400]   we can calculate the percentage of toxic generations and come up with a toxicity measure.
[00:35:57.400 --> 00:36:04.080]   And finally, inference efficiency-- this measures the latency of language model, how quickly
[00:36:04.080 --> 00:36:10.480]   it can predict, and we need to normalize it by the number of tokens across both the prompt
[00:36:10.480 --> 00:36:13.920]   and the output.
[00:36:13.920 --> 00:36:19.720]   So I encourage everyone to check out and read the Helm paper. It's almost like a book. I
[00:36:19.720 --> 00:36:25.040]   think it's definitely more than 100 pages. It contains a lot of insights, not only about
[00:36:25.040 --> 00:36:32.200]   evaluation but also in how the language models work. One specific insight I wanted to share
[00:36:32.200 --> 00:36:38.160]   is correlation between accuracy and other metrics that were measured in Helm. And you
[00:36:38.160 --> 00:36:44.360]   can see pretty good correlation between accuracy and robustness and accuracy and fairness.
[00:36:44.360 --> 00:36:51.080]   But on the other hand, there's little correlation with calibration error. Seems like the relationship
[00:36:51.080 --> 00:36:59.240]   between accuracy and bias or toxicity or inference time is rather uncorrelated. So something
[00:36:59.240 --> 00:37:05.040]   to keep in mind as you make different trade-offs in terms of metrics that are relevant in your
[00:37:05.040 --> 00:37:08.520]   application.
[00:37:08.520 --> 00:37:16.880]   So to summarize Helm, I apologize for this dense slide, but again, this is a super insightful
[00:37:16.880 --> 00:37:23.800]   paper. It highlights some of the limitations. It did not cover languages beyond English.
[00:37:23.800 --> 00:37:29.720]   It does not cover some of the emerging tasks for which language models are used, such as
[00:37:29.720 --> 00:37:35.880]   creative copywriting, email generation. It found some limitations in evaluating long-form
[00:37:35.880 --> 00:37:42.240]   generation. It also highlights that different language models may be sensitive to the way
[00:37:42.240 --> 00:37:47.320]   the prompts are being formatted. So it may be hard to compare models in a standard way
[00:37:47.320 --> 00:37:53.080]   while they might have been trained in different ways.
[00:37:53.080 --> 00:37:59.880]   And just to maybe focus on the last two points, they also mentioned that it costs a lot of
[00:37:59.880 --> 00:38:05.420]   money to run these evaluations. For that reason, they actually limited, I think, the number
[00:38:05.420 --> 00:38:12.080]   of examples per scenario to 1,000 examples and three generations per example, which impacts
[00:38:12.080 --> 00:38:18.520]   statistical significance. And they also avoided aggregating model performance to a single
[00:38:18.520 --> 00:38:24.480]   number or a set of numbers. And that's actually something that is being solved in a way by
[00:38:24.480 --> 00:38:31.000]   Mosaic ML model gauntlet. So that's something that has been released quite recently. It's
[00:38:31.000 --> 00:38:42.000]   part of Mosaic ML tool called LLM Foundry. And it aggregates scores across 34 different
[00:38:42.000 --> 00:38:47.000]   benchmarks. It organizes them into six broad categories, such as common sense reasoning,
[00:38:47.000 --> 00:38:54.520]   world knowledge, programming, reading comprehension, and so on. And the big benefit of using this
[00:38:54.520 --> 00:39:00.320]   tool is that it is very fast. So it benefits from a lot of performance optimization, which
[00:39:00.320 --> 00:39:06.560]   means you can evaluate your model more frequently. And that means you get more insights into
[00:39:06.560 --> 00:39:11.520]   how your model is doing. And I feel like this radar chart, there is some critique of these
[00:39:11.520 --> 00:39:16.240]   charts. I find them actually quite useful for comparing which tasks the model might
[00:39:16.240 --> 00:39:21.700]   be good at, different models might be good at.
[00:39:21.700 --> 00:39:28.160]   So summarizing benchmarks, I think the first quote comes from the hand paper. And I found
[00:39:28.160 --> 00:39:33.720]   this really insightful. So benchmarks orient AI. They encode values. They set priorities.
[00:39:33.720 --> 00:39:38.440]   They specify directions and help us improve our understanding of technology. I think there's
[00:39:38.440 --> 00:39:43.760]   a lot of value of benchmarks. If you remember ImageNet, which allowed for this huge progress
[00:39:43.760 --> 00:39:49.120]   in computer vision, I think some of these benchmarks also allow us to progress on the
[00:39:49.120 --> 00:39:54.200]   language model front. On the other hand, there is the Stratton's law, which says that when
[00:39:54.200 --> 00:39:59.560]   a measure becomes a target, it ceases to be a good measure. And we see some of the benchmarks
[00:39:59.560 --> 00:40:06.160]   that might be overfitting to the benchmarks. And if people focus too much on getting a
[00:40:06.160 --> 00:40:12.440]   good score on the benchmark, that may not necessarily transfer into a useful or a helpful
[00:40:12.440 --> 00:40:15.760]   language model.
[00:40:15.760 --> 00:40:21.160]   Another issue with these benchmarks is they don't always reflect whether the results are
[00:40:21.160 --> 00:40:29.080]   aligned with user preferences. That's something that we will address in the next section called
[00:40:29.080 --> 00:40:34.760]   chatbot arena. But before we get there, let me pause, check if there are any questions
[00:40:34.760 --> 00:40:36.560]   that we should address.
[00:40:36.560 --> 00:40:52.760]   Okay. There's a whole document with questions. Let me see if I can open it. Yeah. Probably,
[00:40:52.760 --> 00:41:00.560]   probably, yeah, I would say we probably take some of these questions offline. If we have
[00:41:00.560 --> 00:41:06.560]   time at the end of this session, we might try to answer some of them live with Thomas
[00:41:06.560 --> 00:41:13.840]   moderating. But yeah, before we get into the questions, maybe let's complete the core of
[00:41:13.840 --> 00:41:19.920]   the presentation. I want to make sure you see all of the different ways we can evaluate
[00:41:19.920 --> 00:41:24.560]   our language models and pick the right one for the job. And once we're done with it,
[00:41:24.560 --> 00:41:28.160]   we'll look into the questions. Any questions that are not responded during the session,
[00:41:28.160 --> 00:41:33.800]   we'll take them offline and we'll either take them to our discord and respond on discord.
[00:41:33.800 --> 00:41:38.000]   We might also answer them in the comments under the YouTube video.
[00:41:38.000 --> 00:41:46.160]   So chatbot arena. Let's listen to GPT-4. Just as two candidates might be compared in a head-to-head
[00:41:46.160 --> 00:41:51.880]   skill assessment to gauge their relative strengths and weaknesses, language models in the chatbot
[00:41:51.880 --> 00:41:57.640]   arena are evaluated in direct competition. The win rates and ELO scores are akin to performance
[00:41:57.640 --> 00:42:03.360]   metrics that help calibrate where each candidate, or in this case, chatbot, stands in the larger
[00:42:03.360 --> 00:42:13.040]   pool of contenders. So chatbot arena was developed by LMSIS group.
[00:42:13.040 --> 00:42:19.760]   It actually came up this year and they came up with this really interesting way of evaluating
[00:42:19.760 --> 00:42:25.440]   language model, which depends on this randomized battles between LLMs. And you can see that
[00:42:25.440 --> 00:42:33.640]   the user interface for this crowdsource evaluation, we get the same input into the language model
[00:42:33.640 --> 00:42:39.440]   and then we get two different generations and they come from different models. The question
[00:42:39.440 --> 00:42:46.240]   here is, the request here is to correct the grammar. We see there is an error in the sentence.
[00:42:46.240 --> 00:42:50.940]   One model comes up with an answer that is not really helpful. Another one actually comes
[00:42:50.940 --> 00:42:56.000]   up with a correct answer. I would definitely say that B is better. That would be a win
[00:42:56.000 --> 00:43:04.480]   for model B and that would improve the overall score of model B in the leaderboard. The whole
[00:43:04.480 --> 00:43:10.640]   leaderboard is measured using a rating system called ELO, which comes from competitive sports
[00:43:10.640 --> 00:43:16.640]   like chess. We're not getting to the details of this calculation, but the higher the score,
[00:43:16.640 --> 00:43:23.960]   the better a language model, the more battles it has won. I would say the benefit of this
[00:43:23.960 --> 00:43:30.460]   approach is that it uses real prompt or real generations. It's evaluated through users
[00:43:30.460 --> 00:43:36.900]   in this pairwise approach, which is probably quite reliable. The focus here is on chat
[00:43:36.900 --> 00:43:42.320]   assistance, which means we're really evaluating whether LLM align with human preferences if
[00:43:42.320 --> 00:43:48.400]   people like their answers. That may not necessarily be factual. I could imagine a situation where
[00:43:48.400 --> 00:43:53.920]   a model that is really poor on truthful QA, we've seen that data set, actually can do
[00:43:53.920 --> 00:43:58.320]   very well in chatbot arena. That could be a potential scenario. I think it might be
[00:43:58.320 --> 00:44:07.480]   worth some investigation. A lot of upsides of this approach. However, it is still very
[00:44:07.480 --> 00:44:14.000]   costly. It depends on having this crowdsourced evaluations. It may require some time for
[00:44:14.000 --> 00:44:20.000]   new models to be evaluated in this way. For this reason, the team behind this data set
[00:44:20.000 --> 00:44:25.800]   came up with this with EmptyBench, which is a data set containing 80 high quality multi-term
[00:44:25.800 --> 00:44:33.120]   questions. You can see some examples here in this snapshot from the data set. They also
[00:44:33.120 --> 00:44:39.120]   used LLM as a judge. This is a concept that has become quite popular recently. Instead
[00:44:39.120 --> 00:44:44.280]   of using human evaluation, which takes time, which is expensive, we can request a model
[00:44:44.280 --> 00:44:53.000]   like GPT-4 to come up with its evaluation. This is more scalable. We can also get some
[00:44:53.000 --> 00:44:58.200]   explainability. It reduces need for human involvement. What the team has found in this
[00:44:58.200 --> 00:45:04.360]   paper when they compared GPT-4 scores and human experts scores, they found it's actually
[00:45:04.360 --> 00:45:09.960]   in high agreement. It's similar to how different human experts correlate with each other, which
[00:45:09.960 --> 00:45:16.160]   means it might be a good way of assessing, for example, human alignment using an LLM
[00:45:16.160 --> 00:45:23.840]   as a proxy of a human. There are some biases associated with using LLM as a judge. For
[00:45:23.840 --> 00:45:29.720]   example, position bias, depending on which generation comes up first, it may influence
[00:45:29.720 --> 00:45:35.520]   the result. There is verbosity bias, models that generate more text, more verbose output
[00:45:35.520 --> 00:45:42.360]   tend to get scored better. Self-enhancement bias says that GPT-4 believes GPT-4's answers
[00:45:42.360 --> 00:45:50.280]   are better. Cloud kind of prefers Cloud's answers. There is a bias there as well. I
[00:45:50.280 --> 00:45:56.360]   think for practical use cases, using model to evaluate responses is actually something
[00:45:56.360 --> 00:46:05.960]   that is quite often used in practice and quite promising if done carefully.
[00:46:05.960 --> 00:46:11.440]   Let's move on to some specific use cases because most of the applications that you're going
[00:46:11.440 --> 00:46:18.360]   to build probably will be focused on some specific domain. Again, let's read the GPT-4
[00:46:18.360 --> 00:46:25.160]   use case specific benchmarks are the expert interview panel of the language model world.
[00:46:25.160 --> 00:46:29.840]   Just as a panel of experts would scrutinize a job candidate's skill in highly specialized
[00:46:29.840 --> 00:46:36.160]   areas to determine their aptitude for a particular role, these tailored benchmarks rigorously
[00:46:36.160 --> 00:46:42.160]   assess a language model's performance in specific scenarios. They go beyond general metrics
[00:46:42.160 --> 00:46:49.040]   to offer a nuanced, context-sensitive evaluation, ensuring that the model is not just a jack-of-all-trades
[00:46:49.040 --> 00:46:54.240]   but a master of the specific domain it will operate in.
[00:46:54.240 --> 00:46:59.480]   I need to admit I have superficial understanding of most of these domains, so I won't be able
[00:46:59.480 --> 00:47:06.120]   to get into the details. But I do want to highlight the importance of benchmarks that
[00:47:06.120 --> 00:47:15.400]   are designed by domain experts and that are focused on specific niche domains. Actually,
[00:47:15.400 --> 00:47:21.480]   all of these benchmarks I think came up in the last week or two. It's definitely an area
[00:47:21.480 --> 00:47:25.480]   that is growing in terms of interest and output.
[00:47:25.480 --> 00:47:31.760]   Legal bench is a benchmark consisting of different legal reasoning tasks. You can see again some
[00:47:31.760 --> 00:47:36.960]   examples here in the slide. It was developed by members of the legal community, lawyers,
[00:47:36.960 --> 00:47:42.720]   law professors, law students. They picked tasks that either capture some interesting
[00:47:42.720 --> 00:47:48.920]   legal reasoning patterns or correspond to potential practical applications in the legal
[00:47:48.920 --> 00:47:55.720]   domain. If you're building an application that is supposed to tackle some legal challenges
[00:47:55.720 --> 00:48:02.520]   with LLMs, it's definitely something to take a detailed look at.
[00:48:02.520 --> 00:48:10.320]   Agent bench is evaluating LLMs as agents. The team behind this benchmark came up with
[00:48:10.320 --> 00:48:17.440]   eight distinct environments, such as operating system, databases, householding, or web shopping,
[00:48:17.440 --> 00:48:22.840]   web browsing. They came up with these real-world challenges, and they test how LLMs as agents
[00:48:22.840 --> 00:48:33.040]   are doing. Just to highlight some examples, one that puzzled me a bit was a man walked
[00:48:33.040 --> 00:48:37.120]   into a restaurant, ordered a bowl of turtle soup, and after finishing it, he committed
[00:48:37.120 --> 00:48:43.200]   suicide. Why did he do that? It might be this lateral thinking puzzle, I believe, as an
[00:48:43.200 --> 00:48:49.680]   environment. I'd be interested to see how LLMs answer to this question.
[00:48:49.680 --> 00:48:54.360]   Here's some more practical ones, such as book the cheapest flight from Beijing to Los Angeles
[00:48:54.360 --> 00:49:04.920]   in the last week of July. LLM agents is definitely a field where we see a lot of applications
[00:49:04.920 --> 00:49:10.240]   being developed. I feel like this benchmark is going to be very useful for those of you
[00:49:10.240 --> 00:49:15.280]   that are developing agents.
[00:49:15.280 --> 00:49:22.760]   The last one in this section is document understanding. A lot of enterprise use cases do not only
[00:49:22.760 --> 00:49:29.000]   deal with text, but also with some of these rich layouts. Documents, they might be PDF
[00:49:29.000 --> 00:49:33.080]   documents, they might be some scan documents. The team behind this paper, they came up with
[00:49:33.080 --> 00:49:40.200]   this evaluation set called DUDE, document understanding data set evaluation. They collect
[00:49:40.200 --> 00:49:44.920]   a list of documents. Some of them may have multiple pages, and they ask different types
[00:49:44.920 --> 00:49:51.640]   of questions of these documents, such as what are the years mentioned in chart one, or how
[00:49:51.640 --> 00:49:56.880]   many attorneys are listed for the plaintiffs, or something I would not expect but might
[00:49:56.880 --> 00:50:03.080]   be relevant, are the margins of the page uniform on all pages? Some of the questions require
[00:50:03.080 --> 00:50:08.000]   multi-hop navigation. Some of them might be graphic intensive, like does this document
[00:50:08.000 --> 00:50:13.320]   contain any checkboxes? Again, if you're dealing with this type of documents, it might be a
[00:50:13.320 --> 00:50:21.520]   benchmark to keep an eye on.
[00:50:21.520 --> 00:50:29.680]   Now we want to move into actually evaluating not only LLMs, but evaluating LLM applications.
[00:50:29.680 --> 00:50:34.360]   This is something that we've also spent some time in our LLM apps course. We'll share the
[00:50:34.360 --> 00:50:39.400]   link to the other course. If some of you are building LLM apps, then I would recommend
[00:50:39.400 --> 00:50:45.200]   checking this out. Let's do a review of different evaluation approaches that might be applicable
[00:50:45.200 --> 00:50:51.000]   here and compare that with what we have seen in terms of the benchmarks and specific and
[00:50:51.000 --> 00:50:56.560]   different evaluation scenarios. Again, let's start with a quote from GPD 4. "Evaluating
[00:50:56.560 --> 00:51:02.120]   a language model in a production environment is the on-the-job performance measurement
[00:51:02.120 --> 00:51:08.080]   of the EI realm, just like an employee's true aptitude is measured not just by interviews
[00:51:08.080 --> 00:51:13.440]   or tests, but by how they handle real tasks and challenges in the workplace. A language
[00:51:13.440 --> 00:51:19.120]   model's real efficacy is gauged by how well it performs in its intended application amidst
[00:51:19.120 --> 00:51:25.840]   the unpredictable nuances and variables it will encounter." Well said, GPD 4. These are
[00:51:25.840 --> 00:51:37.080]   the four different approaches we shared in the other course. All of them might be applicable
[00:51:37.080 --> 00:51:44.000]   to different situations. What we see happening a lot in the case of building LLM application
[00:51:44.000 --> 00:51:52.400]   is something we call vibes check. This means that a set of human experts or people training
[00:51:52.400 --> 00:52:01.120]   models or implementing models look at hand chosen or handpicked inputs and generations
[00:52:01.120 --> 00:52:06.020]   and they see how good these generations are. I feel like it's so popular, I think even
[00:52:06.020 --> 00:52:14.000]   in the case of GPT 3.5 fine tuning, they recommend that as an evaluation method. It's not ideal.
[00:52:14.000 --> 00:52:19.120]   It's hard to compare objectively different models, different generations, but it's something
[00:52:19.120 --> 00:52:25.720]   people do and it's something that definitely is useful as well. Something to keep in mind,
[00:52:25.720 --> 00:52:31.920]   vibes check is a frequent way of looking at language models.
[00:52:31.920 --> 00:52:38.120]   Model based evaluation, this allows us to do some automation and maybe run the vibes
[00:52:38.120 --> 00:52:44.080]   check in a more comprehensive and automated way. We can do this on a curated or generated
[00:52:44.080 --> 00:52:50.240]   data set. We typically use LLM as a judge. We looked at this approach in the context
[00:52:50.240 --> 00:52:55.760]   of empty bench and we have seen that it is often correlated with human judgments. Based
[00:52:55.760 --> 00:53:03.200]   on that, we can automatically run our evaluation test and check the accuracy or different metrics
[00:53:03.200 --> 00:53:13.160]   that we might be measuring on those data sets. A way to do this in a bit more robust way
[00:53:13.160 --> 00:53:19.840]   is to get more granular. This is something we call unit testing. In a similar way on
[00:53:19.840 --> 00:53:26.880]   how human evolve for each question, for each function, we have a set of unit tests that
[00:53:26.880 --> 00:53:31.680]   we need to pass. We can do the same for language model generations. Let's say we're dealing
[00:53:31.680 --> 00:53:38.600]   with summarization task and there might be certain number of events or facts that we
[00:53:38.600 --> 00:53:45.480]   would expect to see in a generation, in a summary. We can use language models quite
[00:53:45.480 --> 00:53:50.640]   effectively to assess that. This allows us to get more granular. This makes this assessment
[00:53:50.640 --> 00:53:57.400]   a bit more objective and it does require a bit more investment in terms of preparing
[00:53:57.400 --> 00:54:06.320]   such an evaluation test. Finally, we probably should be in many cases, maybe testing in
[00:54:06.320 --> 00:54:12.760]   production if the risk of moving a model, if the risk of the application is limited,
[00:54:12.760 --> 00:54:18.080]   then it might be easier to do this. If this is a high risk application, then probably
[00:54:18.080 --> 00:54:24.360]   we need to pass a bunch of other tests before testing in production. But the benefit of
[00:54:24.360 --> 00:54:30.040]   A/B testing production is we're getting the actual users on a real production distribution
[00:54:30.040 --> 00:54:35.640]   and we can check the metrics that are relevant for our business. One way we've done this
[00:54:35.640 --> 00:54:40.440]   in our LLM application, Weights and Biases, we have our one bot application that lives
[00:54:40.440 --> 00:54:46.360]   on our Discord and it answers user questions about Weights and Biases, sometimes correctly,
[00:54:46.360 --> 00:54:50.840]   sometimes incorrectly, but we try to measure the percentage of positive responses, the
[00:54:50.840 --> 00:54:56.920]   percentage of negative responses based on the user feedback. I should say helpful rather
[00:54:56.920 --> 00:55:02.400]   than positive and negative. And then if this metric keeps improving as we implement new
[00:55:02.400 --> 00:55:08.440]   versions of our app or an LLM, then it means we're making progress. We should probably
[00:55:08.440 --> 00:55:14.720]   keep measuring this type of feedback in production if feasible. There might be also other ways,
[00:55:14.720 --> 00:55:19.120]   maybe measuring whether people click on some links that we provide or whether they copy
[00:55:19.120 --> 00:55:26.720]   paste from our generations. So many different ways of testing if the users find our generations
[00:55:26.720 --> 00:55:37.440]   helpful. And that leads us to conclusions. This is a great time to be working on evaluation.
[00:55:37.440 --> 00:55:42.520]   As you have seen, many of these important benchmarks are from this year or the previous
[00:55:42.520 --> 00:55:48.240]   year. There's a lot of specific new evaluations that's being added. I would expect there's
[00:55:48.240 --> 00:55:54.880]   going to be three more added this and next week, just looking at the trend. There's a
[00:55:54.880 --> 00:56:01.160]   bunch of benefits of running evaluations in a consistent and principled way. There's a
[00:56:01.160 --> 00:56:07.240]   number of limitations we have seen. There are issues such as contamination, such as
[00:56:07.240 --> 00:56:12.560]   dependence on different prompting strategies. And these are all important research questions.
[00:56:12.560 --> 00:56:19.040]   This is an area that keeps evolving, and I feel it may benefit from having people like
[00:56:19.040 --> 00:56:24.280]   you join this area and come up with your own evaluation strategies. Maybe you can develop
[00:56:24.280 --> 00:56:30.520]   a domain-specific benchmark. Maybe you find a better way of running automated tests, maybe
[00:56:30.520 --> 00:56:36.920]   for summarization. So definitely a field that is growing in importance and still benefits
[00:56:36.920 --> 00:56:44.720]   from new research. Some practical advice is I feel like that's something I learned myself.
[00:56:44.720 --> 00:56:50.160]   There might be metrics that we really care about that are very expensive to calculate.
[00:56:50.160 --> 00:56:56.120]   Maybe we can get them through running expensive human labeling, or maybe we can get them in
[00:56:56.120 --> 00:57:00.520]   production, which again, it's all expensive to bring a new model into production or to
[00:57:00.520 --> 00:57:06.520]   get a number of human experts to evaluate your language model generations. But there
[00:57:06.520 --> 00:57:12.720]   might be some other metrics that we can calculate in an automated way more easily. And if we
[00:57:12.720 --> 00:57:17.440]   can find these automated metrics that correlate well with metric that we care about, and it's
[00:57:17.440 --> 00:57:23.880]   good to just chart them on this type of simple scatter plot. If we see there is a good correlation
[00:57:23.880 --> 00:57:28.560]   between an automated metric and the metric that we care about, then that means that we
[00:57:28.560 --> 00:57:37.560]   can iterate much faster. So Elutter, EvalHarness has more than 200 different tasks. If you
[00:57:37.560 --> 00:57:42.400]   can find a task that correlate well with the metrics that are important for you, then that
[00:57:42.400 --> 00:57:50.000]   means you can probably progress faster when building your LLM application. And the final
[00:57:50.000 --> 00:57:57.240]   thing is also ongoing evaluation in the context of LLM apps is a process. It's not a single
[00:57:57.240 --> 00:58:01.760]   action like I evaluate the model and we're done. Once you're in production, you keep
[00:58:01.760 --> 00:58:07.040]   getting new examples, you will get users providing feedback, you can add these challenging examples
[00:58:07.040 --> 00:58:13.760]   to your evaluation set. You probably need to measure regressions as you add new versions
[00:58:13.760 --> 00:58:20.240]   of your language models. So it's a process that probably is potentially as important
[00:58:20.240 --> 00:58:25.080]   or even more important as training or fine tuning the models. So evaluation is here to
[00:58:25.080 --> 00:58:31.640]   stay. It's going to be more and more important. And hopefully this session has been useful
[00:58:31.640 --> 00:58:38.720]   in developing your understanding of this field. So this is the last slide. I'm going to stop
[00:58:38.720 --> 00:58:44.520]   now and invite Tomas to come in and share if there are any questions from the audience
[00:58:44.520 --> 00:58:54.640]   that we might want to discuss. Okay, Tomas.
[00:58:54.640 --> 00:59:04.840]   Hello, everyone. Yeah, we have a couple of questions. Great lesson, Derek. That was really
[00:59:04.840 --> 00:59:11.920]   fun and lots of information to unpack. Thank you. Any specific questions you want
[00:59:11.920 --> 00:59:15.760]   to, I feel we probably have time for maybe one or two questions. We'll probably take
[00:59:15.760 --> 00:59:21.200]   the rest of the questions to our Discord server and answer them in the comments. But any specific
[00:59:21.200 --> 00:59:27.160]   one that you think would benefit from this live discussion?
[00:59:27.160 --> 00:59:35.680]   Yeah, of course. Yeah, there are a couple of questions that are pretty interesting.
[00:59:35.680 --> 00:59:41.040]   Some people ask about perplexity and what's your thoughts about that? And can you elaborate
[00:59:41.040 --> 00:59:45.040]   a little bit more on advantages and disadvantages of perplexity?
[00:59:45.040 --> 01:00:01.840]   Yeah, maybe we can start with that. Perplexity is actually a function of our loss. And loss
[01:00:01.840 --> 01:00:06.240]   is something we keep monitoring throughout the training of any language model. And it's
[01:00:06.240 --> 01:00:10.840]   good to have this chart where your loss keeps going down. And if both the training loss
[01:00:10.840 --> 01:00:16.080]   and the evaluation loss keeps going down, it means you are probably on the right track.
[01:00:16.080 --> 01:00:21.000]   Now, the loss depends a lot on what you have in your validation set. If you have a rather
[01:00:21.000 --> 01:00:28.200]   small validation set, then this might not be very meaningful. If you have a pretty big
[01:00:28.200 --> 01:00:33.040]   and diverse evaluation set, then this may be way more meaningful. I think it's also
[01:00:33.040 --> 01:00:37.960]   important what are you fine tuning or training on. If this is a pre-training task and you're
[01:00:37.960 --> 01:00:45.120]   just predicting the next word in a corpus of text, then this speaks to the general abilities
[01:00:45.120 --> 01:00:50.080]   or memorization of-- well, actually not memorization. If you're evaluating on a validation set,
[01:00:50.080 --> 01:00:54.160]   it speaks to how good your model is in predicting the next word.
[01:00:54.160 --> 01:01:01.200]   But if you're now instruction tuning, then it already reflects on how good it is at following
[01:01:01.200 --> 01:01:06.400]   instructions. I would say it's a very helpful metric. It's easy to calculate it throughout
[01:01:06.400 --> 01:01:11.680]   training and you should monitor it. But it's not sufficient because you might have a model
[01:01:11.680 --> 01:01:17.160]   checkpoint that gets lower evaluation loss, but it might have overfitted to some subsets
[01:01:17.160 --> 01:01:21.800]   of the tokens in your validation set. And it might actually make it worse on the metric
[01:01:21.800 --> 01:01:28.360]   that is really important. So I think the answer is monitor it, but also check out other metrics.
[01:01:28.360 --> 01:01:38.160]   Yeah, nice. Nice answer. I gave a way more shorter version of that. Went in way detail.
[01:01:38.160 --> 01:01:42.640]   I think another question that is important to cover here is it's talking about should
[01:01:42.640 --> 01:01:51.640]   all be training based on prompt and competition. And if your final task is classification from
[01:01:51.640 --> 01:01:57.640]   text and maybe, yeah, should we be training with competition or not?
[01:01:57.640 --> 01:02:01.880]   Can you say again, training with competition or completion?
[01:02:01.880 --> 01:02:08.040]   Yeah, like autoregressive or causal ML, like generating next token prediction. If your
[01:02:08.040 --> 01:02:12.120]   final task is going to be like sentiment analysis or classification.
[01:02:12.120 --> 01:02:19.360]   I think that's a great question. And I would say, so this autoregressive language models
[01:02:19.360 --> 01:02:27.160]   that we're focusing on in this course, their power comes in their general purpose capabilities.
[01:02:27.160 --> 01:02:32.720]   So we evaluate these models, for example, on classification tasks, but we also evaluate
[01:02:32.720 --> 01:02:38.400]   them on the summarization. We evaluate them on code generation. So we try to comprehensively
[01:02:38.400 --> 01:02:44.480]   evaluate what type of knowledge they have and how good they are at different tasks.
[01:02:44.480 --> 01:02:51.640]   But if your application is sentiment classification, then I don't think in real life I would use
[01:02:51.640 --> 01:02:59.320]   GPT-4 for that. I would probably train a small BERT model and just focus it on the sentiment
[01:02:59.320 --> 01:03:04.800]   classification. But on the other hand, if this is part of an agent and I don't know
[01:03:04.800 --> 01:03:11.280]   in advance what it's going to need to do, and at some point it may require to classify
[01:03:11.280 --> 01:03:18.040]   something, it should have this capability. So I think, yeah, if you are trying to develop
[01:03:18.040 --> 01:03:24.720]   a classification model and you have a big data set and you can train on this data set,
[01:03:24.720 --> 01:03:30.080]   maybe take one of the encoder models. But if you want to build this general purpose
[01:03:30.080 --> 01:03:36.040]   machine, then autoregressive models seem to be winning this battle.
[01:03:36.040 --> 01:03:43.360]   And a final thought, a lot of people are asking hands-on, showcasing how to run these benchmarks
[01:03:43.360 --> 01:03:49.240]   because it can be very overwhelming. And I will tell people maybe to watch a huge lesson
[01:03:49.240 --> 01:03:58.160]   and we went back and forth trying to run Human Evolve and the Helm benchmark. And yeah, they
[01:03:58.160 --> 01:04:02.400]   require a lot of time and setup and they are not that easy to run.
[01:04:02.400 --> 01:04:07.000]   Yeah, that's a great point. And we encourage again to go back to the video from last week
[01:04:07.000 --> 01:04:16.040]   and check out the code prepared by Ayush and yourself, Thomas, for evaluation. And if we
[01:04:16.040 --> 01:04:20.360]   have demand, I think we can do a follow-up tutorial on running some of these evaluations
[01:04:20.360 --> 01:04:28.200]   hands-on. So we can do a lab session on that. Cool. I think we're at a point of the-- at
[01:04:28.200 --> 01:04:34.640]   a time where we need to break. So thanks a lot for everyone for attending. We're looking
[01:04:34.640 --> 01:04:39.520]   forward-- we'll try to answer all of the questions that come up either on Discord or in the comment
[01:04:39.520 --> 01:04:46.680]   section on YouTube. I think we would encourage everyone to join our Discord server. And thanks
[01:04:46.680 --> 01:04:53.200]   a lot, Thomas, for facilitating, for answering the questions throughout the session. And
[01:04:53.200 --> 01:04:59.520]   next week, we're going to have a break. We'll come back in two weeks on September 13th.
[01:04:59.520 --> 01:05:05.600]   We will have Jonathan Frankel, chief scientist at MosaicML, a person that knows a lot about
[01:05:05.600 --> 01:05:11.240]   training large language models. And Jonathan will talk about data sets for training large
[01:05:11.240 --> 01:05:17.560]   language models, which is an exciting topic. And I think I'm super excited to be joining
[01:05:17.560 --> 01:05:23.200]   the session in two weeks. So I hope to see all of you then. Thanks a lot and have a great
[01:05:23.200 --> 01:05:23.880]   rest of your day.
[01:05:23.880 --> 01:05:33.820]   [BLANK_AUDIO]

