
[00:00:00.000 --> 00:00:02.160]   there is a certain perspective where you might be thinking
[00:00:02.160 --> 00:00:05.080]   what is the longest possible game that you could be playing.
[00:00:05.080 --> 00:00:06.800]   A short game is, for instance,
[00:00:06.800 --> 00:00:08.920]   cancer is playing a shorter game than your organism.
[00:00:08.920 --> 00:00:11.880]   It's cancer is an organism playing a shorter game
[00:00:11.880 --> 00:00:13.640]   than the regular organism,
[00:00:13.640 --> 00:00:17.080]   and because the cancer cannot procreate beyond the organism,
[00:00:17.080 --> 00:00:20.040]   except for some infectious cancers,
[00:00:20.040 --> 00:00:22.600]   like the ones that eradicated the Tasmanian devils,
[00:00:22.600 --> 00:00:26.880]   you typically end up with the situation
[00:00:26.880 --> 00:00:28.920]   where the organism dies together with the cancer,
[00:00:28.920 --> 00:00:31.640]   because the cancer has destroyed the larger system
[00:00:31.640 --> 00:00:33.520]   due to playing a shorter game.
[00:00:33.520 --> 00:00:35.920]   And so ideally you want to, I think,
[00:00:35.920 --> 00:00:39.600]   build agents that play the longest possible games.
[00:00:39.600 --> 00:00:42.920]   And the longest possible games is to keep entropy at bay
[00:00:42.920 --> 00:00:45.720]   as long as possible by doing interesting stuff.
[00:00:45.720 --> 00:00:50.360]   - The following is a conversation with Yosha Bach,
[00:00:50.360 --> 00:00:52.120]   his third time on this podcast.
[00:00:52.120 --> 00:00:54.040]   Yosha is one of the most brilliant
[00:00:54.040 --> 00:00:55.800]   and fascinating minds in the world,
[00:00:55.800 --> 00:00:57.680]   exploring the nature of intelligence,
[00:00:57.680 --> 00:01:00.040]   consciousness, and computation.
[00:01:00.040 --> 00:01:03.360]   And he's one of my favorite humans to talk to
[00:01:03.360 --> 00:01:06.120]   about pretty much anything and everything.
[00:01:06.120 --> 00:01:08.160]   This is the Lex Friedman Podcast.
[00:01:08.160 --> 00:01:10.200]   To support it, please check out our sponsors
[00:01:10.200 --> 00:01:11.360]   in the description.
[00:01:11.360 --> 00:01:14.560]   And now, dear friends, here's Yosha Bach.
[00:01:14.560 --> 00:01:18.920]   You wrote a post about levels of lucidity.
[00:01:18.920 --> 00:01:23.120]   Quote, "As we grow older, it becomes apparent
[00:01:23.120 --> 00:01:25.040]   that our self-reflexive mind
[00:01:25.040 --> 00:01:27.960]   is not just gradually accumulating ideas about itself,
[00:01:27.960 --> 00:01:32.120]   but that it progresses in somewhat distinct stages."
[00:01:32.120 --> 00:01:34.240]   So there's seven of the stages.
[00:01:34.240 --> 00:01:37.000]   Stage one, reactive survival, infant.
[00:01:37.000 --> 00:01:39.640]   Stage two, personal self, young child.
[00:01:39.640 --> 00:01:43.760]   Stage three, social self, adolescence, domesticated adult.
[00:01:43.760 --> 00:01:47.200]   Stage four is rational agency, self-direction.
[00:01:47.200 --> 00:01:51.760]   Stage five is self-authoring, that's full adult.
[00:01:51.760 --> 00:01:54.360]   You've achieved wisdom, but there's two more stages.
[00:01:54.360 --> 00:01:56.600]   Stage six is enlightenment.
[00:01:56.600 --> 00:01:58.800]   Stage seven is transcendence.
[00:01:58.800 --> 00:02:01.000]   Can you explain each,
[00:02:01.000 --> 00:02:03.440]   or the interesting parts of each of these stages?
[00:02:03.440 --> 00:02:06.780]   And what's your sense why there are stages of this,
[00:02:06.780 --> 00:02:10.680]   of lucidity as we progress through life
[00:02:10.680 --> 00:02:12.600]   in this too short life?
[00:02:12.600 --> 00:02:15.560]   - This model is derived from a concept
[00:02:15.560 --> 00:02:18.640]   by the psychologist Robert Keegan.
[00:02:18.640 --> 00:02:23.160]   And he talks about the development of the self
[00:02:23.160 --> 00:02:25.480]   as a process that happens in principle
[00:02:25.480 --> 00:02:28.200]   by some kind of reverse engineering of the mind
[00:02:28.200 --> 00:02:30.120]   where you gradually become aware of yourself
[00:02:30.120 --> 00:02:31.640]   and thereby build structure
[00:02:31.640 --> 00:02:33.240]   that allows you to interact deeper
[00:02:33.240 --> 00:02:35.240]   with the world and yourself.
[00:02:35.240 --> 00:02:37.520]   And I found myself using this model
[00:02:37.520 --> 00:02:39.200]   not so much as a developmental model.
[00:02:39.200 --> 00:02:42.000]   I'm not even sure if it's a very good developmental model
[00:02:42.000 --> 00:02:45.680]   because I saw my children not progressing exactly like that.
[00:02:45.680 --> 00:02:49.980]   And I also suspect that you don't go through the stages
[00:02:49.980 --> 00:02:51.720]   necessarily in succession.
[00:02:51.720 --> 00:02:53.400]   And it's not that you work through one stage
[00:02:53.400 --> 00:02:54.680]   and then you get into the next one.
[00:02:54.680 --> 00:02:56.880]   Sometimes you revisit them.
[00:02:56.880 --> 00:02:59.080]   Sometimes stuff is happening in parallel.
[00:02:59.080 --> 00:03:00.880]   But it's, I think, a useful framework
[00:03:00.880 --> 00:03:04.000]   to look at what's present in the structure of a person
[00:03:04.000 --> 00:03:05.360]   and how they interact with the world
[00:03:05.360 --> 00:03:07.520]   and how they relate to themselves.
[00:03:07.520 --> 00:03:10.120]   So it's more like a philosophical framework
[00:03:10.120 --> 00:03:12.820]   that allows you to talk about how minds work.
[00:03:12.820 --> 00:03:14.800]   And at first, when we are born,
[00:03:14.800 --> 00:03:17.840]   we don't have a personal self yet, I think.
[00:03:17.840 --> 00:03:19.480]   Instead, we have an attentional self.
[00:03:19.480 --> 00:03:21.400]   And this attentional self is initially
[00:03:21.400 --> 00:03:24.520]   in the infant tasked with building a world model
[00:03:24.520 --> 00:03:26.720]   and also an initial model of the self.
[00:03:26.720 --> 00:03:29.040]   But mostly it's building a game engine in the brain
[00:03:29.040 --> 00:03:34.040]   that is tracking sensory data and uses it to explain it.
[00:03:34.040 --> 00:03:37.160]   And in some sense, you could compare it to a game engine
[00:03:37.160 --> 00:03:40.600]   like Minecraft or so, so colors and sounds.
[00:03:40.600 --> 00:03:42.600]   People are all not physical objects.
[00:03:42.600 --> 00:03:43.960]   They are creation of our mind
[00:03:43.960 --> 00:03:46.280]   at a certain level of coarse graining.
[00:03:46.280 --> 00:03:50.240]   Models that are mathematical, that use geometry
[00:03:50.240 --> 00:03:54.560]   and that use manipulation of objects and so on
[00:03:54.560 --> 00:03:58.120]   to create scenes in which we can find ourselves
[00:03:58.120 --> 00:03:59.320]   and interact with them.
[00:03:59.320 --> 00:04:00.560]   - So Minecraft.
[00:04:00.560 --> 00:04:02.640]   - Yeah, and this personal self is something
[00:04:02.640 --> 00:04:06.800]   that is more or less created after the world is finished,
[00:04:06.800 --> 00:04:09.220]   after it's trained into the system,
[00:04:09.220 --> 00:04:11.320]   after it has been constructed.
[00:04:11.320 --> 00:04:13.160]   And this personal self is an agent
[00:04:13.160 --> 00:04:15.880]   that interacts with the outside world.
[00:04:15.880 --> 00:04:19.800]   And the outside world is not the world of quantum mechanics,
[00:04:19.800 --> 00:04:21.080]   not the physical universe,
[00:04:21.080 --> 00:04:24.680]   but it's the model that has been generated in our own mind.
[00:04:24.680 --> 00:04:28.320]   And this is us and we experience ourself interacting
[00:04:28.320 --> 00:04:30.320]   with that outside world that is created
[00:04:30.320 --> 00:04:31.880]   inside of our own mind.
[00:04:31.880 --> 00:04:34.840]   And outside of ourself, there's feelings
[00:04:34.840 --> 00:04:38.560]   and they presented our interface with this outside world.
[00:04:38.560 --> 00:04:40.400]   They pose problems to us.
[00:04:40.400 --> 00:04:42.240]   These feelings are basically attitudes
[00:04:42.240 --> 00:04:44.000]   that our mind is computing
[00:04:44.000 --> 00:04:46.000]   that tell us what's needed in the world,
[00:04:46.000 --> 00:04:47.600]   the things that we are drawn to,
[00:04:47.600 --> 00:04:49.560]   the things that we are afraid of.
[00:04:49.560 --> 00:04:52.400]   And we are tasked with solving this problem
[00:04:52.400 --> 00:04:56.160]   of satisfying the needs, avoiding the aversions,
[00:04:56.160 --> 00:04:58.720]   following on our inner commitments and so on,
[00:04:58.720 --> 00:05:02.400]   and also modeling ourselves and building the next stage.
[00:05:02.400 --> 00:05:06.360]   So after we have this personal self in stage two online,
[00:05:06.360 --> 00:05:08.440]   many people form a social self.
[00:05:08.440 --> 00:05:11.000]   And this social self allows the individual
[00:05:11.000 --> 00:05:13.440]   to experience themselves as part of a group.
[00:05:13.440 --> 00:05:17.160]   It's basically this thing that when you are playing
[00:05:17.160 --> 00:05:19.360]   in a team, for instance, you don't notice yourself
[00:05:19.360 --> 00:05:22.600]   just as a single note that is reaching out into the world,
[00:05:22.600 --> 00:05:23.700]   but you're also looking down.
[00:05:23.700 --> 00:05:25.800]   You're looking down from this entire group
[00:05:25.800 --> 00:05:28.720]   and you see how this group is looking at this individual.
[00:05:28.720 --> 00:05:31.200]   And everybody in the group is in some sense
[00:05:31.200 --> 00:05:34.360]   emulating this group spirit to some degree.
[00:05:34.360 --> 00:05:36.880]   And in this state, people are forming their opinions
[00:05:36.880 --> 00:05:39.160]   by assimilating them from this group mind.
[00:05:39.160 --> 00:05:41.080]   They basically gain the ability
[00:05:41.080 --> 00:05:42.960]   to act a little bit like a hive mind.
[00:05:42.960 --> 00:05:45.960]   - But are you also modeling the interaction
[00:05:45.960 --> 00:05:47.800]   of how opinions shapes and forms
[00:05:47.800 --> 00:05:50.400]   through the interaction of the individual nodes
[00:05:50.400 --> 00:05:51.880]   within the group?
[00:05:51.880 --> 00:05:54.520]   - Yeah, it's basically the way in which people do it
[00:05:54.520 --> 00:05:57.240]   in this stage is that they experience
[00:05:57.240 --> 00:05:59.720]   what are the opinions of my environment.
[00:05:59.720 --> 00:06:01.120]   They experience the relationship
[00:06:01.120 --> 00:06:02.840]   that I have to their environment.
[00:06:02.840 --> 00:06:05.920]   And they resonate with people around them
[00:06:05.920 --> 00:06:09.640]   and get more opinions through this interaction
[00:06:09.640 --> 00:06:13.380]   to the way in which they relate to others.
[00:06:13.380 --> 00:06:16.360]   And at stage four, you basically understand
[00:06:16.360 --> 00:06:18.600]   that stuff is true and false independently
[00:06:18.600 --> 00:06:20.000]   what other people believe.
[00:06:20.000 --> 00:06:22.560]   And you have agency over your own beliefs in that stage.
[00:06:22.560 --> 00:06:24.960]   You basically discover epistemology,
[00:06:24.960 --> 00:06:27.880]   the rules about determining what's true and false.
[00:06:27.880 --> 00:06:30.800]   - So you start to learn how to think.
[00:06:30.800 --> 00:06:31.640]   - Yes.
[00:06:31.640 --> 00:06:33.680]   I mean, at some level, you're always thinking.
[00:06:33.680 --> 00:06:35.680]   You are constructing things.
[00:06:35.680 --> 00:06:37.640]   And I believe that this ability to reason
[00:06:37.640 --> 00:06:39.300]   about your mental representation
[00:06:39.300 --> 00:06:41.040]   is what we mean by thinking.
[00:06:41.040 --> 00:06:43.000]   It's an intrinsically reflexive process
[00:06:43.000 --> 00:06:44.640]   that requires consciousness.
[00:06:44.640 --> 00:06:47.040]   Without consciousness, you cannot think.
[00:06:47.040 --> 00:06:49.480]   You can generate the content of feelings
[00:06:49.480 --> 00:06:51.160]   and so on outside of consciousness.
[00:06:51.160 --> 00:06:52.920]   It's very hard to be conscious
[00:06:52.920 --> 00:06:54.840]   of how your feelings emerge,
[00:06:54.840 --> 00:06:56.920]   at least in the early stages of development.
[00:06:56.920 --> 00:07:00.080]   But thoughts is something that you always control.
[00:07:00.080 --> 00:07:03.420]   And if you are a nerd like me,
[00:07:03.420 --> 00:07:05.360]   you often have to skip stage three
[00:07:05.360 --> 00:07:08.520]   because you'd lack the intuitive empathy with others.
[00:07:08.520 --> 00:07:11.160]   Because in order to resonate with a group,
[00:07:11.160 --> 00:07:13.160]   you need to have a quite similar architecture.
[00:07:13.160 --> 00:07:15.800]   And if people are wired differently,
[00:07:15.800 --> 00:07:18.760]   then it's hard for them to resonate with other people
[00:07:18.760 --> 00:07:21.480]   and basically have empathy,
[00:07:21.480 --> 00:07:23.280]   which is not the same as compassion,
[00:07:23.280 --> 00:07:26.000]   but it is a shared perceptual mental state.
[00:07:26.000 --> 00:07:28.640]   Empathy happens not just via inference
[00:07:28.640 --> 00:07:30.640]   about the mental states of others,
[00:07:30.640 --> 00:07:34.280]   but it's a perception of what other people feel
[00:07:34.280 --> 00:07:35.160]   and where they're at.
[00:07:35.160 --> 00:07:36.800]   - Can't you not have empathy
[00:07:36.800 --> 00:07:39.280]   while also not having a similar architecture,
[00:07:39.280 --> 00:07:40.920]   cognitive architecture, as the others in the group?
[00:07:40.920 --> 00:07:43.840]   - I think, yes, but I experienced that too.
[00:07:43.840 --> 00:07:45.480]   But you need to build something
[00:07:45.480 --> 00:07:46.840]   that is like a meta-architecture.
[00:07:46.840 --> 00:07:49.480]   You need to be able to embrace the architecture
[00:07:49.480 --> 00:07:50.680]   of the other to some degree
[00:07:50.680 --> 00:07:52.960]   or find some shared common ground.
[00:07:52.960 --> 00:07:57.080]   And it's also this issue that if you are a nerd,
[00:07:57.080 --> 00:07:59.800]   normally often, as in your typical people,
[00:07:59.800 --> 00:08:02.000]   have difficulty to resonate with you.
[00:08:02.000 --> 00:08:05.040]   And as a result, they have difficulty understanding you
[00:08:05.040 --> 00:08:06.600]   unless they have enough wisdom
[00:08:06.600 --> 00:08:08.760]   to feel what's going on there.
[00:08:08.760 --> 00:08:10.900]   - Well, isn't the whole process of the stage
[00:08:10.900 --> 00:08:14.240]   three is to figure out the API to the other humans
[00:08:14.240 --> 00:08:15.680]   that have different architecture
[00:08:15.680 --> 00:08:19.360]   and you yourself publish public documentation
[00:08:19.360 --> 00:08:24.160]   for the API that people can interact with for you?
[00:08:24.160 --> 00:08:26.200]   Isn't this the whole process of socializing?
[00:08:26.200 --> 00:08:28.240]   - My experience as a child growing up
[00:08:28.240 --> 00:08:31.360]   was that I did not find any way to interface
[00:08:31.360 --> 00:08:33.280]   with the stage three people.
[00:08:33.280 --> 00:08:35.440]   And they didn't do that with me.
[00:08:35.440 --> 00:08:36.800]   So it took me-- - Did you try?
[00:08:36.800 --> 00:08:38.680]   - Yeah, of course, I tried it very hard.
[00:08:38.680 --> 00:08:41.620]   But it was only when I entered a mathematics school
[00:08:41.620 --> 00:08:45.320]   at ninth grade, lots of other nerds were present,
[00:08:45.320 --> 00:08:49.140]   that I found people that I could deeply resonate with
[00:08:49.140 --> 00:08:52.620]   and had the impression that, yes, I have friends now,
[00:08:52.620 --> 00:08:54.060]   I found my own people.
[00:08:54.060 --> 00:08:56.540]   And before that, I felt extremely alone in the world.
[00:08:56.540 --> 00:08:59.180]   There was basically nobody I could connect to.
[00:08:59.180 --> 00:09:04.180]   And I remember there was one moment in all these years
[00:09:04.180 --> 00:09:07.740]   where I was in, there was a school exchange
[00:09:07.740 --> 00:09:09.840]   and it was the Russian boy,
[00:09:09.840 --> 00:09:13.040]   kid from the Russian garrison stationed in Eastern Germany
[00:09:13.040 --> 00:09:14.500]   who visited our school.
[00:09:14.500 --> 00:09:16.620]   And we played a game of chess against each other.
[00:09:16.620 --> 00:09:18.700]   And we looked into each other's eyes
[00:09:18.700 --> 00:09:21.340]   and we sat there for two hours playing this game of chess.
[00:09:21.340 --> 00:09:23.940]   And I had the impression, this is a human being.
[00:09:23.940 --> 00:09:25.620]   He understands what I understand.
[00:09:25.620 --> 00:09:29.160]   We didn't even speak the same language.
[00:09:29.160 --> 00:09:32.640]   - I wonder if your life could have been different
[00:09:32.640 --> 00:09:35.820]   if you knew that it's okay to be different,
[00:09:35.820 --> 00:09:38.140]   to have a different architecture.
[00:09:38.140 --> 00:09:42.300]   Whether accepting that the interface is hard to figure out,
[00:09:42.300 --> 00:09:43.660]   takes a long time to figure out,
[00:09:43.660 --> 00:09:44.660]   and it's okay to be different.
[00:09:44.660 --> 00:09:46.660]   In fact, it's beautiful to be different.
[00:09:46.660 --> 00:09:51.780]   - It was not my main concern.
[00:09:51.780 --> 00:09:55.220]   My main concern was mostly that I was alone.
[00:09:55.220 --> 00:09:58.260]   It was not so much the question, is it okay to be the way I
[00:09:58.260 --> 00:10:02.740]   am, I couldn't do much about it, so I had to deal with it.
[00:10:02.740 --> 00:10:06.140]   But my main issue was that I was not sure
[00:10:06.140 --> 00:10:09.780]   if I would ever meet anybody growing up
[00:10:09.780 --> 00:10:11.860]   that I would connect to at such a deep level
[00:10:11.860 --> 00:10:13.660]   that I would feel that I could belong.
[00:10:13.660 --> 00:10:17.140]   - So there's a visceral, undeniable feeling of being alone.
[00:10:17.140 --> 00:10:18.020]   - Yes.
[00:10:18.020 --> 00:10:21.080]   And I noticed the same thing when I came into the math school
[00:10:21.080 --> 00:10:24.680]   that I think at least half, probably two thirds,
[00:10:24.680 --> 00:10:26.540]   of these kids were severely traumatized
[00:10:26.540 --> 00:10:31.260]   as children growing up, and in large part due to being alone
[00:10:31.260 --> 00:10:33.660]   because they couldn't find anybody to relate to.
[00:10:33.660 --> 00:10:36.180]   - Don't you think everybody's alone, deep down?
[00:10:36.180 --> 00:10:37.020]   - No.
[00:10:37.020 --> 00:10:41.260]   (laughing)
[00:10:41.260 --> 00:10:43.940]   I'm not alone.
[00:10:43.940 --> 00:10:45.420]   I'm not alone anymore.
[00:10:45.420 --> 00:10:48.260]   It took me some time to update and to get over the trauma
[00:10:48.260 --> 00:10:51.700]   and so on, but I felt that in my 20s,
[00:10:51.700 --> 00:10:54.700]   I had lots of friends and I had my place in the world,
[00:10:54.700 --> 00:10:59.620]   and I had no longer doubts that I would never be alone again.
[00:11:00.620 --> 00:11:03.140]   - Is there some aspect to which we're alone together?
[00:11:03.140 --> 00:11:06.140]   You don't see a deep loneliness inside yourself still?
[00:11:06.140 --> 00:11:06.980]   - No.
[00:11:06.980 --> 00:11:08.740]   Sorry.
[00:11:08.740 --> 00:11:10.500]   (laughing)
[00:11:10.500 --> 00:11:12.700]   - Okay, so that's the nonlinear progression
[00:11:12.700 --> 00:11:14.020]   through the stages, I suppose.
[00:11:14.020 --> 00:11:15.500]   You caught up on stage three at some point?
[00:11:15.500 --> 00:11:16.940]   - Yes, so we are at stage four.
[00:11:16.940 --> 00:11:18.860]   And so basically I find that many nerds
[00:11:18.860 --> 00:11:22.340]   jump straight into stage four, bypassing stage three.
[00:11:22.340 --> 00:11:24.060]   - Do they return to it then, later?
[00:11:24.060 --> 00:11:24.900]   - Yeah, of course.
[00:11:24.900 --> 00:11:27.100]   Sometimes they do, not always.
[00:11:27.100 --> 00:11:27.940]   - Yeah.
[00:11:27.940 --> 00:11:28.760]   - The question is basically,
[00:11:28.760 --> 00:11:30.180]   do you stay a little bit autistic
[00:11:30.180 --> 00:11:31.340]   or do you catch up?
[00:11:31.340 --> 00:11:33.180]   And I believe you can catch up.
[00:11:33.180 --> 00:11:35.460]   You can build this missing structure.
[00:11:35.460 --> 00:11:36.300]   - Yeah.
[00:11:36.300 --> 00:11:39.980]   - And basically experience yourself as part of a group,
[00:11:39.980 --> 00:11:42.260]   learn intuitive empathy and develop the sense,
[00:11:42.260 --> 00:11:46.260]   this perceptual sense of feeling what other people feel.
[00:11:46.260 --> 00:11:48.860]   And before that, I could only basically feel this
[00:11:48.860 --> 00:11:51.900]   when I was deeply in love with somebody and we synced or--
[00:11:51.900 --> 00:11:54.780]   - So there's a lot of friction to feeling that way.
[00:11:54.780 --> 00:11:57.300]   Like, only with certain people,
[00:11:57.300 --> 00:11:58.980]   as opposed to it comes naturally.
[00:11:58.980 --> 00:11:59.820]   - Yeah.
[00:11:59.820 --> 00:12:03.140]   But this is something that basically later I felt
[00:12:03.140 --> 00:12:06.580]   started to resolve itself for me, to a large degree.
[00:12:06.580 --> 00:12:07.620]   - What was the trick?
[00:12:07.620 --> 00:12:12.560]   - In many ways, growing up and paying attention.
[00:12:12.560 --> 00:12:15.780]   Meditation did help.
[00:12:15.780 --> 00:12:18.540]   I had some very crucial experiences
[00:12:18.540 --> 00:12:23.220]   in getting close to people, building connections,
[00:12:23.220 --> 00:12:27.500]   cuddling a lot in my student years.
[00:12:28.580 --> 00:12:32.300]   - So really paying attention to the, what is it,
[00:12:32.300 --> 00:12:35.900]   to the feeling another human being fully.
[00:12:35.900 --> 00:12:38.620]   - Loving other people and being loved by other people
[00:12:38.620 --> 00:12:41.100]   and building a space in which you can be safe
[00:12:41.100 --> 00:12:44.820]   and can experiment and touch a lot
[00:12:44.820 --> 00:12:47.300]   and be close to somebody a lot.
[00:12:47.300 --> 00:12:51.940]   And over time, basically at some point you realize,
[00:12:51.940 --> 00:12:54.740]   oh, it's no longer that I feel locked out,
[00:12:54.740 --> 00:12:57.780]   but I feel connected and I experience
[00:12:57.780 --> 00:12:58.940]   where somebody else is at.
[00:12:58.940 --> 00:13:03.020]   And normally my mind is racing very fast at a high frequency,
[00:13:03.020 --> 00:13:04.460]   so it's not always working like this.
[00:13:04.460 --> 00:13:07.340]   Sometimes it works better, sometimes it works less.
[00:13:07.340 --> 00:13:09.380]   But I also don't see this as a pressure.
[00:13:09.380 --> 00:13:12.900]   It's more, it's interesting to observe myself,
[00:13:12.900 --> 00:13:16.340]   which frequency I'm at and at which mode
[00:13:16.340 --> 00:13:17.380]   somebody else is at.
[00:13:17.380 --> 00:13:21.820]   - Yeah, man, the mind is so beautiful in that way.
[00:13:21.820 --> 00:13:24.540]   Sometimes it comes so natural to me,
[00:13:24.540 --> 00:13:28.300]   so easy to pay attention, pay attention to the world fully,
[00:13:28.300 --> 00:13:29.580]   to other people fully.
[00:13:29.580 --> 00:13:34.500]   And sometimes the stress over silly things is overwhelming.
[00:13:34.500 --> 00:13:35.740]   It's so interesting that the mind
[00:13:35.740 --> 00:13:37.620]   is that rollercoaster in that way.
[00:13:37.620 --> 00:13:40.580]   - At stage five you discover how identity is constructed.
[00:13:40.580 --> 00:13:41.420]   - Self-offering.
[00:13:41.420 --> 00:13:43.460]   - You realize that your values are not terminal,
[00:13:43.460 --> 00:13:46.500]   but they are instrumental to achieving a world
[00:13:46.500 --> 00:13:49.540]   that you like and aesthetics that you prefer.
[00:13:49.540 --> 00:13:51.580]   And the more you understand this,
[00:13:51.580 --> 00:13:54.100]   the more you get agency over how your identity
[00:13:54.100 --> 00:13:55.380]   is constructed.
[00:13:55.380 --> 00:13:57.260]   And you realize that identity
[00:13:57.260 --> 00:14:00.060]   and interpersonal interaction is a costume.
[00:14:00.060 --> 00:14:03.100]   And you should be able to have agency over that costume.
[00:14:03.100 --> 00:14:04.900]   It's useful to be a costume.
[00:14:04.900 --> 00:14:07.220]   It tells something to others
[00:14:07.220 --> 00:14:10.240]   and it allows to interface in roles.
[00:14:10.240 --> 00:14:13.340]   But being locked into this is a big limitation.
[00:14:13.340 --> 00:14:15.700]   - The word costume kind of implies
[00:14:15.700 --> 00:14:18.340]   that it's fraudulent in some way.
[00:14:18.340 --> 00:14:20.940]   Is costume a good word for you?
[00:14:20.940 --> 00:14:22.580]   Like we present ourselves to the world.
[00:14:22.580 --> 00:14:24.060]   - In some sense, I learned a lot
[00:14:24.060 --> 00:14:25.540]   about costumes at Burning Man.
[00:14:25.540 --> 00:14:28.020]   Before that, I did not really appreciate costumes
[00:14:28.020 --> 00:14:29.900]   and saw them more as uniforms,
[00:14:29.900 --> 00:14:32.660]   like wearing a suit if you are working in a bank
[00:14:32.660 --> 00:14:37.140]   or if you're trying to get startup funding
[00:14:37.140 --> 00:14:39.380]   from a VC in Switzerland.
[00:14:39.380 --> 00:14:42.420]   Then you dress up in a particular way.
[00:14:42.420 --> 00:14:44.700]   And this is mostly to show the other side
[00:14:44.700 --> 00:14:46.420]   that you are willing to play by the rules
[00:14:46.420 --> 00:14:48.600]   and you understand what the rules are.
[00:14:48.600 --> 00:14:51.100]   But there is something deeper.
[00:14:51.100 --> 00:14:52.660]   When you are at Burning Man, your costume
[00:14:52.660 --> 00:14:55.660]   becomes self-expression and there is no boundary
[00:14:55.660 --> 00:14:56.780]   to the self-expression.
[00:14:56.780 --> 00:14:59.460]   You're basically free to wear what you want
[00:14:59.460 --> 00:15:02.180]   to express other people what you feel like this day
[00:15:02.180 --> 00:15:04.500]   and what kind of interactions you want to have.
[00:15:04.500 --> 00:15:09.120]   - Is the costume a kind of projection of who you are?
[00:15:09.120 --> 00:15:12.460]   - That's very hard to say because the costume
[00:15:12.460 --> 00:15:15.540]   also depends on what other people see in the costume.
[00:15:15.540 --> 00:15:16.980]   And this depends on the context
[00:15:16.980 --> 00:15:18.580]   that the other people understand.
[00:15:18.580 --> 00:15:20.700]   And you have to create something if you want to
[00:15:20.700 --> 00:15:22.940]   that is legible to the other side.
[00:15:22.940 --> 00:15:25.080]   And that means something to yourself.
[00:15:25.080 --> 00:15:28.500]   - Do we become prisoners of the costume?
[00:15:28.500 --> 00:15:29.540]   'Cause everybody expects us to.
[00:15:29.540 --> 00:15:30.360]   - Some people do.
[00:15:30.360 --> 00:15:33.660]   But I think that once you realize
[00:15:33.660 --> 00:15:35.980]   that you wear a costume at Burning Man,
[00:15:35.980 --> 00:15:39.060]   a variety of costumes, realize that you cannot
[00:15:39.060 --> 00:15:40.200]   not wear a costume.
[00:15:40.200 --> 00:15:44.780]   Basically everything that you wear and present to others
[00:15:44.780 --> 00:15:48.620]   is something that is to some degree
[00:15:48.620 --> 00:15:51.980]   in addition to what you are deep inside.
[00:15:51.980 --> 00:15:54.320]   - So this stage, in parentheses,
[00:15:54.320 --> 00:15:57.920]   you put full adult comma wisdom.
[00:15:57.920 --> 00:16:00.380]   Why is this full adult?
[00:16:00.380 --> 00:16:02.900]   Why would you say this is full?
[00:16:02.900 --> 00:16:04.780]   And why is it wisdom?
[00:16:04.780 --> 00:16:07.180]   - It does allow you to understand
[00:16:07.180 --> 00:16:10.700]   why other people have different identities from yours.
[00:16:10.700 --> 00:16:13.560]   And it allows you to understand that the difference
[00:16:13.560 --> 00:16:15.760]   between people who vote for different parties
[00:16:15.760 --> 00:16:17.820]   and might have very different opinions
[00:16:17.820 --> 00:16:21.300]   and different value systems is often the accident
[00:16:21.300 --> 00:16:25.180]   of where they are born and what happened after that to them
[00:16:25.180 --> 00:16:29.300]   and what traits they got before they were born.
[00:16:29.300 --> 00:16:32.740]   And at some point you realize the perspective
[00:16:32.740 --> 00:16:34.980]   where you understand that everybody could be you
[00:16:34.980 --> 00:16:38.580]   in a different timeline if you just flip those bits.
[00:16:38.580 --> 00:16:40.180]   - How many costumes do you have?
[00:16:40.180 --> 00:16:42.260]   - I don't count.
[00:16:42.260 --> 00:16:44.540]   - More than one?
[00:16:44.540 --> 00:16:45.440]   - Yeah, of course.
[00:16:46.860 --> 00:16:49.860]   - How easy is it to do costume changes throughout the day?
[00:16:49.860 --> 00:16:53.620]   - It's just a matter of energy and interest.
[00:16:53.620 --> 00:16:55.740]   When you are wearing your pajamas
[00:16:55.740 --> 00:16:57.420]   and you switch out of your pajamas
[00:16:57.420 --> 00:17:00.900]   into, say, a work short and pants,
[00:17:00.900 --> 00:17:03.100]   you're making a costume change, right?
[00:17:03.100 --> 00:17:05.300]   And if you are putting on a gown,
[00:17:05.300 --> 00:17:06.140]   you're making a costume change.
[00:17:06.140 --> 00:17:08.340]   - And you could do the same with personality?
[00:17:08.340 --> 00:17:12.140]   - You could, if that's what you're into.
[00:17:12.140 --> 00:17:14.500]   There are people which have multiple personalities
[00:17:14.500 --> 00:17:16.380]   for interaction in multiple worlds, right?
[00:17:16.380 --> 00:17:18.100]   So if somebody works in a store
[00:17:18.100 --> 00:17:21.460]   and you put up a storekeeper personality,
[00:17:21.460 --> 00:17:23.860]   when you're presenting yourself at work,
[00:17:23.860 --> 00:17:26.420]   you develop a sub-personality for this.
[00:17:26.420 --> 00:17:28.460]   And the social persona for many people
[00:17:28.460 --> 00:17:30.300]   is in some sense a puppet
[00:17:30.300 --> 00:17:32.500]   that they're playing like a marionette.
[00:17:32.500 --> 00:17:33.780]   And if they play this all the time,
[00:17:33.780 --> 00:17:37.380]   they might forget that there is something behind this,
[00:17:37.380 --> 00:17:40.180]   there's something what it feels like to be in your skin.
[00:17:40.180 --> 00:17:42.260]   And I guess it's very helpful
[00:17:42.260 --> 00:17:44.340]   if you're able to get back into this.
[00:17:44.340 --> 00:17:47.780]   And for me, the other way around is relatively hard.
[00:17:47.780 --> 00:17:49.620]   For me, it's pretty hard to learn
[00:17:49.620 --> 00:17:51.820]   how to play consistent social roles.
[00:17:51.820 --> 00:17:54.460]   For me, it's much easier just to be real.
[00:17:54.460 --> 00:17:58.520]   - Or not real, but to have one costume.
[00:17:58.520 --> 00:18:00.820]   - No, it's not quite the same.
[00:18:00.820 --> 00:18:04.220]   So basically when you are wearing a costume at Burning Man,
[00:18:04.220 --> 00:18:07.500]   and say you are an extraterrestrial prince,
[00:18:07.500 --> 00:18:10.420]   there's something where you are expressing,
[00:18:10.420 --> 00:18:12.620]   in some sense, something that's closer to yourself
[00:18:12.620 --> 00:18:15.220]   than the way in which you hide yourself
[00:18:15.220 --> 00:18:18.980]   behind standard clothing when you go out in the city
[00:18:18.980 --> 00:18:20.660]   and the default world.
[00:18:20.660 --> 00:18:23.220]   And so this costume that you're wearing at Burning Man
[00:18:23.220 --> 00:18:25.820]   allows you to express more of yourself.
[00:18:25.820 --> 00:18:30.820]   And you have a shorter distance of advertising to people
[00:18:30.820 --> 00:18:33.180]   what kind of person you are,
[00:18:33.180 --> 00:18:35.300]   what kind of interaction you would want to have with them.
[00:18:35.300 --> 00:18:40.300]   And so you get much earlier into media stress.
[00:18:40.980 --> 00:18:43.080]   And I believe it's regrettable
[00:18:43.080 --> 00:18:46.500]   that we do not use the opportunities that we have
[00:18:46.500 --> 00:18:49.060]   with custom-made clothing now to wear costumes
[00:18:49.060 --> 00:18:52.420]   that are much more stylish, that are much more custom-made,
[00:18:52.420 --> 00:18:54.420]   that are not necessarily part of a fashion
[00:18:54.420 --> 00:18:57.300]   in which you express which milieu you're part of
[00:18:57.300 --> 00:18:59.020]   and how up-to-date you are.
[00:18:59.020 --> 00:19:02.460]   But you also express how you are as an individual
[00:19:02.460 --> 00:19:04.980]   and what you want to do today and how you feel today
[00:19:04.980 --> 00:19:06.740]   and what you intend to do about it.
[00:19:06.740 --> 00:19:10.140]   - Well, isn't it easier now in the digital world
[00:19:10.140 --> 00:19:14.180]   to explore different costumes?
[00:19:14.180 --> 00:19:16.660]   I mean, that's the kind of idea with virtual reality.
[00:19:16.660 --> 00:19:19.780]   That's the idea even with Twitter and two-dimensional screens
[00:19:19.780 --> 00:19:24.180]   you can swap out costumes, you can be as weird as you want.
[00:19:24.180 --> 00:19:25.580]   It's easier.
[00:19:25.580 --> 00:19:29.140]   For Burning Man, you have to order things,
[00:19:29.140 --> 00:19:31.500]   you have to make things, you have to,
[00:19:31.500 --> 00:19:32.540]   it's more effort to put on--
[00:19:32.540 --> 00:19:35.300]   - It's even better if you make them yourselves.
[00:19:35.300 --> 00:19:39.540]   - Sure, but it's just easier to do digitally, right?
[00:19:39.540 --> 00:19:42.340]   - It's not about easy, it's about how to get it right.
[00:19:42.340 --> 00:19:44.620]   And for me, the first Burning Man experience,
[00:19:44.620 --> 00:19:47.540]   I got adopted by a bunch of people in Boston
[00:19:47.540 --> 00:19:48.700]   who took me to Burning Man
[00:19:48.700 --> 00:19:53.380]   and we spent a few weekends doing costumes together.
[00:19:53.380 --> 00:19:55.380]   And that was an important part of the experience
[00:19:55.380 --> 00:19:58.460]   where the camp bonded, that people got to know each other,
[00:19:58.460 --> 00:20:00.980]   and we basically grew into the experience
[00:20:00.980 --> 00:20:02.380]   that we would have later.
[00:20:02.380 --> 00:20:05.700]   - So the extraterrestrial prince is based on a true story.
[00:20:05.700 --> 00:20:06.540]   - Yeah.
[00:20:07.700 --> 00:20:10.620]   - I can only imagine what that looks like, Josje.
[00:20:10.620 --> 00:20:13.020]   - Okay, so-- - Stage six.
[00:20:13.020 --> 00:20:16.060]   - Stage six, at some point you can collapse
[00:20:16.060 --> 00:20:20.100]   the division between self, a personal self,
[00:20:20.100 --> 00:20:22.020]   and world generator again.
[00:20:22.020 --> 00:20:24.980]   And a lot of people get there via meditation
[00:20:24.980 --> 00:20:27.500]   or some of them get there via psychedelics,
[00:20:27.500 --> 00:20:29.100]   some of them by accident,
[00:20:29.100 --> 00:20:32.900]   and you suddenly notice that you are not actually a person,
[00:20:32.900 --> 00:20:36.060]   but you are a vessel that can create a person.
[00:20:36.060 --> 00:20:37.900]   And the person is still there, you observe
[00:20:37.900 --> 00:20:40.220]   that personal self, but you observe the personal self
[00:20:40.220 --> 00:20:41.140]   from the outside.
[00:20:41.140 --> 00:20:43.940]   And you notice it's a representation.
[00:20:43.940 --> 00:20:46.180]   And you might also notice that the world
[00:20:46.180 --> 00:20:48.220]   that is being created is a representation.
[00:20:48.220 --> 00:20:51.140]   If not, then you might experience that I am the universe,
[00:20:51.140 --> 00:20:53.260]   I am the thing that is creating everything.
[00:20:53.260 --> 00:20:56.540]   And of course, what you're creating is not quantum mechanics
[00:20:56.540 --> 00:20:57.900]   and the physical universe,
[00:20:57.900 --> 00:21:00.460]   what you're creating is this game engine
[00:21:00.460 --> 00:21:02.700]   that is updating the world and you're creating
[00:21:02.700 --> 00:21:04.060]   your valence, your feelings,
[00:21:04.060 --> 00:21:07.540]   and all the people inside of that world,
[00:21:07.540 --> 00:21:09.780]   including the person that you identify
[00:21:09.780 --> 00:21:11.580]   with yourself in this world.
[00:21:11.580 --> 00:21:12.940]   - Are you creating the game engine
[00:21:12.940 --> 00:21:15.340]   or are you noticing the game engine?
[00:21:15.340 --> 00:21:18.620]   - You notice how you're generating the game engine.
[00:21:18.620 --> 00:21:20.540]   And I mean, when you are dreaming at night,
[00:21:20.540 --> 00:21:23.780]   you can, if you have a lucid dream,
[00:21:23.780 --> 00:21:26.020]   you can learn how to do this deliberately.
[00:21:26.020 --> 00:21:29.060]   And in principle, you can also do it during the day.
[00:21:29.060 --> 00:21:32.060]   And the reason why we don't get to do this
[00:21:32.060 --> 00:21:34.340]   from the beginning and why we don't have agency
[00:21:34.340 --> 00:21:36.940]   of our feelings right away is because we would game it
[00:21:36.940 --> 00:21:40.020]   before they have the necessary amount of wisdom
[00:21:40.020 --> 00:21:43.980]   to deal with creating this dream that we are in.
[00:21:43.980 --> 00:21:47.340]   - You don't want to get access to cheat codes too quickly,
[00:21:47.340 --> 00:21:48.860]   otherwise you won't enjoy the game.
[00:21:48.860 --> 00:21:51.740]   - So stage five is already pretty rare
[00:21:51.740 --> 00:21:53.260]   and stage six is even more rare.
[00:21:53.260 --> 00:21:56.500]   You basically find this mostly with advanced
[00:21:56.500 --> 00:21:59.700]   Buddhist meditators and so on that are dropping
[00:21:59.700 --> 00:22:02.260]   into the stage and can induce it at will
[00:22:02.260 --> 00:22:03.780]   and spend time in it.
[00:22:03.780 --> 00:22:06.300]   - So stage five requires a good therapist,
[00:22:06.300 --> 00:22:11.300]   stage six requires a good Buddhist spiritual leader.
[00:22:11.300 --> 00:22:15.540]   - For instance, could be that is the right thing to do.
[00:22:15.540 --> 00:22:18.540]   But it's not that these stages give you scores
[00:22:18.540 --> 00:22:21.260]   or levels that you need to advance to.
[00:22:21.260 --> 00:22:23.700]   It's not that the next stage is better.
[00:22:23.700 --> 00:22:26.100]   You live your life in the mode that works best
[00:22:26.100 --> 00:22:28.660]   at any given moment and when your mind decides
[00:22:28.660 --> 00:22:31.340]   that you should have a different configuration,
[00:22:31.340 --> 00:22:32.940]   then it's building that configuration
[00:22:32.940 --> 00:22:37.020]   and for many people they stay happily at stage three
[00:22:37.020 --> 00:22:39.660]   and experience themselves as part of groups
[00:22:39.660 --> 00:22:41.020]   and there's nothing wrong with this.
[00:22:41.020 --> 00:22:42.620]   And for some people this doesn't work
[00:22:42.620 --> 00:22:45.060]   and they're forced to build more agency
[00:22:45.060 --> 00:22:47.180]   over their rational beliefs than this
[00:22:47.180 --> 00:22:49.580]   and construct their norms rationally.
[00:22:49.580 --> 00:22:51.660]   And so they go to this level.
[00:22:51.660 --> 00:22:54.340]   And stage seven is something that is more or less
[00:22:54.340 --> 00:22:57.300]   hypothetical, that would be the stage in which,
[00:22:57.300 --> 00:22:58.980]   it's basically a transhumanist stage
[00:22:58.980 --> 00:23:00.420]   in which you understand how you work
[00:23:00.420 --> 00:23:03.900]   and which the mind fully realizes how it's implemented
[00:23:03.900 --> 00:23:07.180]   and can also in principle enter different modes
[00:23:07.180 --> 00:23:08.460]   in which it could be implemented.
[00:23:08.460 --> 00:23:11.900]   And that's the stage that, as far as I understand,
[00:23:11.900 --> 00:23:13.420]   is not open to people yet.
[00:23:13.420 --> 00:23:17.820]   - Oh, but it is possible through the process of technology.
[00:23:17.820 --> 00:23:21.540]   - Yes, and who knows if there are biological agents
[00:23:21.540 --> 00:23:24.060]   that are working at different time scales than us
[00:23:24.060 --> 00:23:25.620]   that basically become aware of the way
[00:23:25.620 --> 00:23:28.220]   in which they're implemented on ecosystems
[00:23:28.220 --> 00:23:30.500]   and can change that implementation
[00:23:30.500 --> 00:23:33.820]   and have agency over how they're implemented in the world.
[00:23:33.820 --> 00:23:36.260]   And what I find interesting about the discussion
[00:23:36.260 --> 00:23:39.740]   about AI alignment, that it seems to be following
[00:23:39.740 --> 00:23:41.060]   the status very much.
[00:23:41.060 --> 00:23:42.780]   Most people seem to be in stage three,
[00:23:42.780 --> 00:23:45.380]   also according to Robert Keegan.
[00:23:45.380 --> 00:23:47.980]   I think he says that about 85% of people
[00:23:47.980 --> 00:23:50.180]   are in stage three and stay there.
[00:23:50.180 --> 00:23:53.740]   And if you're in stage three and your opinions
[00:23:53.740 --> 00:23:56.660]   are the result of social assimilation,
[00:23:56.660 --> 00:23:59.140]   then what you're mostly worried about in the AI
[00:23:59.140 --> 00:24:02.060]   is that the AI might have the wrong opinions.
[00:24:02.060 --> 00:24:04.500]   So if the AI says something racist or sexist,
[00:24:04.500 --> 00:24:06.700]   we are all lost because we will assimilate
[00:24:06.700 --> 00:24:07.980]   the wrong opinions from the AI.
[00:24:07.980 --> 00:24:10.500]   And so we need to make sure that the AI has
[00:24:10.500 --> 00:24:12.180]   the right opinions and the right values
[00:24:12.180 --> 00:24:13.100]   and the right structure.
[00:24:13.100 --> 00:24:17.140]   And if you're at stage four, that's not your main concern.
[00:24:17.140 --> 00:24:19.660]   And so most nerds don't really worry about
[00:24:21.100 --> 00:24:23.860]   the algorithmic bias and the model that it picks up
[00:24:23.860 --> 00:24:26.100]   because if there's something wrong with this bias,
[00:24:26.100 --> 00:24:27.740]   the AI ultimately will prove it.
[00:24:27.740 --> 00:24:28.900]   At some point, we'll get it there
[00:24:28.900 --> 00:24:31.660]   that it makes mathematical proofs about reality.
[00:24:31.660 --> 00:24:34.940]   And then it will figure out what's true and what's false.
[00:24:34.940 --> 00:24:37.300]   But you're still worried that the AI might turn you
[00:24:37.300 --> 00:24:40.140]   into paperclips because it might have the wrong values.
[00:24:40.140 --> 00:24:42.740]   So if it's set up with the wrong function
[00:24:42.740 --> 00:24:44.900]   that controls its direction in the world,
[00:24:44.900 --> 00:24:48.180]   then it might do something that is completely horrible
[00:24:48.180 --> 00:24:49.620]   and there's no easy way to fix it.
[00:24:49.620 --> 00:24:52.620]   - So that's more like a stage four rationalist kind of worry.
[00:24:52.620 --> 00:24:54.580]   - And if you are at stage five, you're mostly worried
[00:24:54.580 --> 00:24:57.700]   that the AI is not going to be enlightened fast enough
[00:24:57.700 --> 00:24:59.780]   because you realize that the game is not so much
[00:24:59.780 --> 00:25:01.620]   about intelligence, but about agency,
[00:25:01.620 --> 00:25:04.900]   about the ability to control the future.
[00:25:04.900 --> 00:25:07.300]   And the identity is instrumental to this.
[00:25:07.300 --> 00:25:11.720]   And if you are a human being, I think at some level,
[00:25:11.720 --> 00:25:14.140]   you ought to choose your own identity.
[00:25:14.140 --> 00:25:17.140]   You should not have somebody else pick the costume for you
[00:25:17.140 --> 00:25:18.360]   and then wear it.
[00:25:18.360 --> 00:25:20.060]   But instead you should be mindful
[00:25:20.060 --> 00:25:22.100]   about what you want to be in this world.
[00:25:22.100 --> 00:25:26.020]   And I think if you are an agent that is fully malleable,
[00:25:26.020 --> 00:25:27.980]   that can provide its own source code,
[00:25:27.980 --> 00:25:30.380]   like an AI might do at some point,
[00:25:30.380 --> 00:25:34.680]   then the identity that you will have is whatever you can be.
[00:25:34.680 --> 00:25:39.680]   And in this way, the AI will maybe become everything,
[00:25:39.680 --> 00:25:41.860]   like a planetary control system.
[00:25:41.860 --> 00:25:46.300]   And if it does that, then if we want to coexist with it,
[00:25:46.300 --> 00:25:49.840]   it means that it will have to share purposes with us.
[00:25:49.840 --> 00:25:51.800]   So it cannot be a transactional relationship.
[00:25:51.800 --> 00:25:54.160]   We will not be able to use reinforcement learning
[00:25:54.160 --> 00:25:58.360]   with human feedback to hardwire its values into it.
[00:25:58.360 --> 00:25:59.680]   But this has to happen.
[00:25:59.680 --> 00:26:01.440]   It's probably that it's conscious,
[00:26:01.440 --> 00:26:03.540]   so it can relate to our own mode of existence
[00:26:03.540 --> 00:26:06.680]   where an observer is observing itself in real time
[00:26:06.680 --> 00:26:09.660]   and there's in certain temporal frames.
[00:26:09.660 --> 00:26:12.520]   And the other thing is that it probably needs
[00:26:12.520 --> 00:26:14.600]   to have some kind of transcendental orientation,
[00:26:14.600 --> 00:26:18.280]   building shared agency in the same way as we do
[00:26:18.280 --> 00:26:20.120]   when we are able to enter up with each other
[00:26:20.120 --> 00:26:22.680]   into non-transactional relationships.
[00:26:22.680 --> 00:26:23.960]   And I find that something that,
[00:26:23.960 --> 00:26:26.400]   because the stage five is so rare,
[00:26:26.400 --> 00:26:29.560]   is missing in much of the discourse.
[00:26:29.560 --> 00:26:33.640]   And I think that we need, in some sense,
[00:26:33.640 --> 00:26:36.880]   focus on how to formalize love, how to understand love,
[00:26:36.880 --> 00:26:39.480]   and how to build it into the machines
[00:26:39.480 --> 00:26:40.820]   that we are currently building
[00:26:40.820 --> 00:26:43.080]   and that are about to become smarter than us.
[00:26:43.960 --> 00:26:45.360]   - Well, I think this is a good opportunity
[00:26:45.360 --> 00:26:48.240]   to try to sneak up to the idea of enlightenment.
[00:26:48.240 --> 00:26:52.200]   So you wrote a series of good tweets
[00:26:52.200 --> 00:26:54.360]   about consciousness and panpsychism.
[00:26:54.360 --> 00:26:55.280]   So let's break it down.
[00:26:55.280 --> 00:26:57.600]   First you say, "I suspect the experience
[00:26:57.600 --> 00:27:00.040]   "that leads to the panpsychism syndrome
[00:27:00.040 --> 00:27:03.600]   "of some philosophers and other consciousness enthusiasts
[00:27:03.600 --> 00:27:07.380]   "represents the realization that we don't end at the self,
[00:27:07.380 --> 00:27:10.660]   "but share a resonant universe representation
[00:27:10.660 --> 00:27:15.620]   "with every other observer coupled to the same universe."
[00:27:15.620 --> 00:27:17.540]   This actually eventually leads us
[00:27:17.540 --> 00:27:20.440]   to a lot of interesting questions about AI and AGI.
[00:27:20.440 --> 00:27:22.640]   But let's start with this representation.
[00:27:22.640 --> 00:27:26.180]   What is this resonant universe representation?
[00:27:26.180 --> 00:27:27.900]   And what do you think?
[00:27:27.900 --> 00:27:29.900]   Do we share such a representation?
[00:27:29.900 --> 00:27:32.100]   - The neuroscientist, Grossberg,
[00:27:32.100 --> 00:27:34.100]   has come up with a cognitive architecture
[00:27:34.100 --> 00:27:37.000]   that he calls the adaptive resonance theory.
[00:27:37.000 --> 00:27:40.320]   And his perspective is that our neurons
[00:27:40.320 --> 00:27:42.420]   can be understood as oscillators
[00:27:42.420 --> 00:27:44.340]   that are resonating with each other
[00:27:44.340 --> 00:27:46.700]   and with outside phenomena.
[00:27:46.700 --> 00:27:48.740]   So the coarse-grained model of the universe
[00:27:48.740 --> 00:27:50.540]   that we are building, in some sense,
[00:27:50.540 --> 00:27:55.540]   is a resonance with objects outside of us in the world.
[00:27:55.540 --> 00:27:58.660]   So basically, we take up patterns of the universe
[00:27:58.660 --> 00:28:00.100]   that we are coupled with,
[00:28:00.100 --> 00:28:04.700]   and our brain is not so much understood as circuitry,
[00:28:04.700 --> 00:28:06.660]   even though this perspective is valid,
[00:28:06.660 --> 00:28:08.860]   but it's almost an ether
[00:28:08.860 --> 00:28:11.400]   in which the individual neurons are passing on
[00:28:11.400 --> 00:28:14.000]   chemo-electrical signals,
[00:28:14.000 --> 00:28:16.400]   or arbitrary signals across all modalities
[00:28:16.400 --> 00:28:18.360]   that can be transmitted between cells,
[00:28:18.360 --> 00:28:20.400]   simulate each other in this way,
[00:28:20.400 --> 00:28:22.560]   and produce patterns that they modulate
[00:28:22.560 --> 00:28:24.400]   while passing them on.
[00:28:24.400 --> 00:28:27.320]   And this speed of signal progression in the brain
[00:28:27.320 --> 00:28:29.680]   is roughly at the speed of sound, incidentally,
[00:28:29.680 --> 00:28:32.040]   because the time that it takes
[00:28:32.040 --> 00:28:34.900]   for the signals to hop from cell to cell,
[00:28:34.900 --> 00:28:36.200]   which means it's relatively slow
[00:28:36.200 --> 00:28:37.280]   with respect to the world.
[00:28:37.280 --> 00:28:40.220]   It takes an appreciable fraction of a second
[00:28:40.220 --> 00:28:42.520]   for a signal to go through the entire neocortex,
[00:28:42.520 --> 00:28:44.480]   something like a few hundred milliseconds.
[00:28:44.480 --> 00:28:46.760]   And so there's a lot of stuff happening in that time
[00:28:46.760 --> 00:28:49.320]   where the signal is passing through your brain,
[00:28:49.320 --> 00:28:51.000]   including in the brain itself.
[00:28:51.000 --> 00:28:53.080]   So nothing in the brain is assuming
[00:28:53.080 --> 00:28:55.360]   that stuff happens simultaneously.
[00:28:55.360 --> 00:28:58.440]   Everything in the brain is working in a paradigm
[00:28:58.440 --> 00:29:00.640]   where the world has already moved on
[00:29:00.640 --> 00:29:04.120]   when you are ready to do the next thing to your signal,
[00:29:04.120 --> 00:29:06.440]   including the signal processing system itself.
[00:29:06.440 --> 00:29:08.360]   It's quite a different paradigm
[00:29:08.360 --> 00:29:10.280]   than the one in our digital computers,
[00:29:10.280 --> 00:29:13.800]   where we currently assume that your GPU or CPU
[00:29:13.800 --> 00:29:15.900]   is pretty much globally in the same state.
[00:29:15.900 --> 00:29:20.440]   - So you mentioned there the non-dual state,
[00:29:20.440 --> 00:29:23.160]   and say that some people confuse it for enlightenment.
[00:29:23.160 --> 00:29:25.320]   What's the non-dual state?
[00:29:25.320 --> 00:29:27.400]   - There is a state in which you notice
[00:29:27.400 --> 00:29:29.000]   that you are no longer a person,
[00:29:29.000 --> 00:29:32.400]   and instead you are one with the universe.
[00:29:32.400 --> 00:29:34.160]   - So that speaks to the resonance.
[00:29:34.160 --> 00:29:36.160]   - Yes, but this one with the universe
[00:29:36.160 --> 00:29:38.360]   is of course not accurately modeling
[00:29:38.360 --> 00:29:41.100]   that you are indeed some god entity,
[00:29:41.100 --> 00:29:43.520]   or indeed the universe is becoming aware of itself,
[00:29:43.520 --> 00:29:45.320]   even though you get this experience.
[00:29:45.320 --> 00:29:47.080]   I believe that you get this experience
[00:29:47.080 --> 00:29:50.040]   because your mind is modeling the fact
[00:29:50.040 --> 00:29:52.240]   that you are no longer identified
[00:29:52.240 --> 00:29:54.320]   with the personal self in that state,
[00:29:54.320 --> 00:29:56.480]   but you have transcended this division
[00:29:56.480 --> 00:29:58.800]   between the self model and the world model,
[00:29:58.800 --> 00:30:01.520]   and you are experiencing yourself as your mind,
[00:30:01.520 --> 00:30:04.280]   as something that is representing a universe.
[00:30:04.280 --> 00:30:05.920]   - But that's still part of the model.
[00:30:05.920 --> 00:30:08.240]   - Yes, so it's inside of the model still.
[00:30:08.240 --> 00:30:10.200]   You are still inside of patterns
[00:30:10.200 --> 00:30:13.240]   that are generated in your brain and in your organism.
[00:30:13.240 --> 00:30:15.600]   And what you are now experiencing
[00:30:15.600 --> 00:30:18.560]   is that you're no longer this personal self in there,
[00:30:18.560 --> 00:30:22.560]   but you are the entirety of the mind and its contents.
[00:30:22.560 --> 00:30:24.200]   - Why is it so hard to get there?
[00:30:24.200 --> 00:30:26.960]   - A lot of people who get into this state
[00:30:26.960 --> 00:30:29.120]   think this, or associate it with enlightenment.
[00:30:29.120 --> 00:30:31.640]   I suspect it's a favorite training goal
[00:30:31.640 --> 00:30:33.980]   for a number of meditators.
[00:30:33.980 --> 00:30:38.400]   But I think that enlightenment is in some sense more mundane
[00:30:38.400 --> 00:30:41.080]   and it's a step further, or sideways.
[00:30:41.080 --> 00:30:42.680]   It's the state where you realize
[00:30:42.680 --> 00:30:44.560]   that everything is a representation.
[00:30:44.560 --> 00:30:47.240]   - Yeah, you say enlightenment is a realization
[00:30:47.240 --> 00:30:49.400]   of how experience is implemented.
[00:30:49.400 --> 00:30:52.640]   - Yes, so basically you notice at some point
[00:30:52.640 --> 00:30:55.160]   that your qualia can be deconstructed.
[00:30:55.160 --> 00:30:56.200]   - Reverse engineered?
[00:30:56.200 --> 00:30:58.800]   What, like a, almost like a schematic of it?
[00:30:58.800 --> 00:31:03.360]   - You can start with looking at a face,
[00:31:03.360 --> 00:31:05.560]   and maybe look at your own face in a mirror.
[00:31:05.560 --> 00:31:08.760]   Look at your face for a few hours in a mirror,
[00:31:08.760 --> 00:31:10.700]   or for a few minutes.
[00:31:10.700 --> 00:31:12.800]   At some point it will look very weird,
[00:31:12.800 --> 00:31:15.160]   because you notice that there's actually no face.
[00:31:15.160 --> 00:31:16.800]   You basically start unseeing the face,
[00:31:16.800 --> 00:31:18.500]   what you see is the geometry,
[00:31:18.500 --> 00:31:21.240]   and then you can disassemble the geometry
[00:31:21.240 --> 00:31:23.120]   and realize how that geometry
[00:31:23.120 --> 00:31:25.420]   is being constructed in your mind.
[00:31:25.420 --> 00:31:27.080]   And you can learn to modify this.
[00:31:27.080 --> 00:31:30.240]   So basically you can change these generators
[00:31:30.240 --> 00:31:32.920]   in your own mind to shift the face around,
[00:31:32.920 --> 00:31:35.760]   or to change the construction of the face,
[00:31:35.760 --> 00:31:38.080]   to change the way in which the features
[00:31:38.080 --> 00:31:39.360]   are being assembled.
[00:31:39.360 --> 00:31:40.640]   - Why don't we do that more often?
[00:31:40.640 --> 00:31:44.160]   Why don't we start really messing with reality
[00:31:44.160 --> 00:31:47.260]   without the use of drugs or anything else?
[00:31:47.260 --> 00:31:50.200]   Why don't we get good at this kind of thing?
[00:31:50.200 --> 00:31:54.000]   Like, intentionally.
[00:31:54.000 --> 00:31:55.280]   - Why should we?
[00:31:55.280 --> 00:31:56.120]   Why would you want to do that?
[00:31:56.120 --> 00:31:57.560]   - Because you can morph reality
[00:31:57.560 --> 00:32:02.340]   into something more pleasant for yourself.
[00:32:02.340 --> 00:32:03.440]   Just have fun with it.
[00:32:03.440 --> 00:32:06.240]   - Yeah, that is probably what you shouldn't be doing,
[00:32:06.240 --> 00:32:09.160]   right, because outside of your personal self,
[00:32:09.160 --> 00:32:12.400]   this outer mind, is probably a relatively smart agent.
[00:32:12.400 --> 00:32:14.480]   And what you often notice is that you have thoughts
[00:32:14.480 --> 00:32:16.040]   about how you should live,
[00:32:16.040 --> 00:32:17.940]   but you observe yourself doing different things
[00:32:17.940 --> 00:32:19.480]   and having different feelings.
[00:32:19.480 --> 00:32:22.840]   And that's because your outer mind doesn't believe you.
[00:32:22.840 --> 00:32:25.400]   And doesn't believe your rational thoughts.
[00:32:25.400 --> 00:32:27.800]   - Well, can't you just silence the outer mind?
[00:32:27.800 --> 00:32:29.320]   - The thing is that the outer mind
[00:32:29.320 --> 00:32:30.920]   is usually smarter than you are.
[00:32:31.840 --> 00:32:33.940]   Rational thinking is very brittle.
[00:32:33.940 --> 00:32:36.760]   It's very hard to use logic and symbolic thinking
[00:32:36.760 --> 00:32:39.100]   to have an accurate model of the world.
[00:32:39.100 --> 00:32:41.440]   So there is often an underlying system
[00:32:41.440 --> 00:32:43.120]   that is looking at your rational thoughts
[00:32:43.120 --> 00:32:45.940]   and then tells you, no, you're still missing something.
[00:32:45.940 --> 00:32:48.540]   Your gut feeling is still saying something else.
[00:32:48.540 --> 00:32:50.580]   And this can be, for instance,
[00:32:50.580 --> 00:32:53.160]   you find a partner that looks perfect,
[00:32:53.160 --> 00:32:56.220]   or you find a deal and just build a company
[00:32:56.220 --> 00:32:58.060]   or whatever that looks perfect to you.
[00:32:58.060 --> 00:33:00.100]   And yet, at some level, you feel something is off,
[00:33:00.100 --> 00:33:01.480]   and you cannot put your finger on it,
[00:33:01.480 --> 00:33:04.440]   and the more you reason about it, the better it looks to you.
[00:33:04.440 --> 00:33:07.660]   But the system that is outside still tells you,
[00:33:07.660 --> 00:33:09.820]   no, no, you're missing something.
[00:33:09.820 --> 00:33:11.620]   - And that system is powerful.
[00:33:11.620 --> 00:33:13.140]   - People call this intuition, right?
[00:33:13.140 --> 00:33:15.620]   Intuition is this unreflected part
[00:33:15.620 --> 00:33:19.500]   of your attitude composition and computation
[00:33:19.500 --> 00:33:23.340]   where you produce a model of how you relate to the world
[00:33:23.340 --> 00:33:24.580]   and what you need to do in it,
[00:33:24.580 --> 00:33:26.820]   and what you can do in it, and what's going to happen.
[00:33:26.820 --> 00:33:28.340]   That is usually deeper
[00:33:28.360 --> 00:33:31.600]   and often more accurate than your reason.
[00:33:31.600 --> 00:33:34.140]   - So if we look at this as you write in the tweet,
[00:33:34.140 --> 00:33:36.700]   if we look at this more rigorously,
[00:33:36.700 --> 00:33:40.020]   as a sort of take the panpsychist idea more seriously,
[00:33:40.020 --> 00:33:41.580]   almost as a scientific discipline,
[00:33:41.580 --> 00:33:44.300]   you write that quote, "Fascinatingly,
[00:33:44.300 --> 00:33:46.580]   "the panpsychist interpretation seems to lead
[00:33:46.580 --> 00:33:48.540]   "to observations of practical results
[00:33:48.540 --> 00:33:51.460]   "to a degree that physics fundamentalists
[00:33:51.460 --> 00:33:53.440]   "might call superstitious.
[00:33:53.440 --> 00:33:57.620]   "Reports of long-distance telepathy and remote causation
[00:33:57.620 --> 00:34:00.080]   "are ubiquitous in the general population.
[00:34:00.080 --> 00:34:02.900]   "I am not convinced," says Yoshiba,
[00:34:02.900 --> 00:34:05.780]   "that establishing the empirical reality of telepathy
[00:34:05.780 --> 00:34:07.500]   "would force an update of any part
[00:34:07.500 --> 00:34:09.120]   "of serious academic physics,
[00:34:09.120 --> 00:34:11.240]   "but it could trigger an important revolution
[00:34:11.240 --> 00:34:14.860]   "in both neuroscience and AI from a circuit perspective
[00:34:14.860 --> 00:34:19.860]   "to a coupled complex resonator paradigm."
[00:34:19.860 --> 00:34:25.100]   Are you suggesting that there could be some rigorous
[00:34:25.700 --> 00:34:30.700]   mathematical wisdom to panpsychist perspective on the world?
[00:34:30.700 --> 00:34:34.440]   - So first of all, panpsychism is the perspective
[00:34:34.440 --> 00:34:36.900]   that consciousness is inseparable
[00:34:36.900 --> 00:34:38.780]   from matter in the universe.
[00:34:38.780 --> 00:34:41.700]   And I find panpsychism quite unsatisfying
[00:34:41.700 --> 00:34:43.620]   because it does not explain consciousness, right?
[00:34:43.620 --> 00:34:46.980]   It does not explain how this aspect of matter produces.
[00:34:46.980 --> 00:34:49.180]   It's also when I try to formalize panpsychism
[00:34:49.180 --> 00:34:51.140]   and write down what it actually means
[00:34:51.140 --> 00:34:53.940]   and with a more formal mathematical language,
[00:34:53.940 --> 00:34:55.980]   it's very difficult to distinguish it
[00:34:55.980 --> 00:35:00.340]   from saying that there is a software side to the world
[00:35:00.340 --> 00:35:01.940]   in the same way as there is a software side
[00:35:01.940 --> 00:35:03.860]   to what the transistors are doing in your computer.
[00:35:03.860 --> 00:35:05.340]   So basically, there's a pattern
[00:35:05.340 --> 00:35:07.540]   that a certain core screening of the universe
[00:35:07.540 --> 00:35:09.180]   that in some reasons of the universe
[00:35:09.180 --> 00:35:12.580]   leads to observers that are observing themselves, right?
[00:35:12.580 --> 00:35:15.220]   So panpsychism maybe is not even
[00:35:15.220 --> 00:35:17.060]   when I write it down a position
[00:35:17.060 --> 00:35:19.500]   that is distinct from functionalism.
[00:35:19.500 --> 00:35:22.540]   But intuitively, a lot of people feel
[00:35:22.540 --> 00:35:25.420]   that the activity of matter itself,
[00:35:25.420 --> 00:35:27.940]   of mechanisms in the world is insufficient to explain it.
[00:35:27.940 --> 00:35:31.060]   So it's something that needs to be intrinsic
[00:35:31.060 --> 00:35:32.580]   to matter itself.
[00:35:32.580 --> 00:35:37.580]   And you can, apart from this abstract idea,
[00:35:37.580 --> 00:35:41.900]   have an experience in which you experience yourself
[00:35:41.900 --> 00:35:43.740]   as being the universe,
[00:35:43.740 --> 00:35:46.020]   which I suspect is basically happening
[00:35:46.020 --> 00:35:49.460]   because you managed to dissolve the division
[00:35:49.460 --> 00:35:50.980]   between personal self and mind
[00:35:50.980 --> 00:35:52.660]   that you establish as an infant
[00:35:52.660 --> 00:35:54.500]   when you construct a personal self
[00:35:54.500 --> 00:35:57.620]   and transcend it again and understand how it works.
[00:35:57.620 --> 00:35:59.380]   But there is something deeper
[00:35:59.380 --> 00:36:01.980]   that is that you feel that you're also sharing
[00:36:01.980 --> 00:36:03.660]   a state with other people,
[00:36:03.660 --> 00:36:08.140]   that you have an experience in which you notice
[00:36:08.140 --> 00:36:12.340]   that your personal self is moving into everything else,
[00:36:12.340 --> 00:36:15.420]   that you basically look out of the eyes of another person,
[00:36:15.420 --> 00:36:19.660]   that every agent in the world that is an observer
[00:36:19.660 --> 00:36:21.460]   is in some sense you.
[00:36:21.460 --> 00:36:22.980]   - So if we-- - And we forget
[00:36:22.980 --> 00:36:24.700]   that we are the same agent.
[00:36:24.700 --> 00:36:27.540]   - So is it that we feel that
[00:36:27.540 --> 00:36:29.580]   or do we actually accomplish it?
[00:36:29.580 --> 00:36:32.540]   So is telepathy possible?
[00:36:32.540 --> 00:36:33.820]   Is it real?
[00:36:33.820 --> 00:36:35.460]   - So for me, that's a question
[00:36:35.460 --> 00:36:37.460]   that I don't really know the answer to.
[00:36:37.460 --> 00:36:40.300]   In Turing's famous 1950 paper
[00:36:40.300 --> 00:36:41.740]   in which he describes the Turing test,
[00:36:41.740 --> 00:36:44.220]   he does speculate about telepathy, interestingly,
[00:36:44.220 --> 00:36:47.460]   and asks himself if telepathy is real
[00:36:47.460 --> 00:36:50.060]   and he thinks that it very well might be,
[00:36:50.060 --> 00:36:53.860]   what would be the implication for AI systems
[00:36:53.860 --> 00:36:55.060]   that try to be intelligent
[00:36:55.060 --> 00:36:57.340]   because he didn't see a mechanism
[00:36:57.340 --> 00:37:00.580]   by which a computer program would become telepathic.
[00:37:00.580 --> 00:37:03.940]   And I suspect if telepathy would exist,
[00:37:03.940 --> 00:37:06.820]   or if all the reports that you get from people
[00:37:06.820 --> 00:37:09.060]   when you ask the normal person on the street,
[00:37:09.060 --> 00:37:12.260]   I find that very often they say,
[00:37:12.260 --> 00:37:14.100]   "I have experiences with telepathy."
[00:37:14.100 --> 00:37:16.500]   The scientists might not be interested in this
[00:37:16.500 --> 00:37:18.420]   and might not have a theory about this,
[00:37:18.420 --> 00:37:21.020]   but I have difficulty explaining it away.
[00:37:21.020 --> 00:37:24.380]   And so you could say maybe this is a superstition,
[00:37:24.380 --> 00:37:25.740]   maybe it's a false memory,
[00:37:25.740 --> 00:37:28.740]   or maybe it's a little bit of psychosis, who knows?
[00:37:28.740 --> 00:37:31.380]   Maybe somebody wants to make their own life more interesting
[00:37:31.380 --> 00:37:32.660]   or misremember something.
[00:37:32.660 --> 00:37:34.540]   But a lot of people report,
[00:37:34.540 --> 00:37:36.900]   "I noticed something terrible happened to my partner
[00:37:36.900 --> 00:37:39.660]   "and I noticed this exactly the moment it happened,
[00:37:39.660 --> 00:37:41.500]   "where my child had an accident
[00:37:41.500 --> 00:37:43.100]   "and I knew that was happening
[00:37:43.100 --> 00:37:45.300]   "and the child was in a different town."
[00:37:45.300 --> 00:37:47.140]   So maybe it's a false memory
[00:37:47.140 --> 00:37:50.340]   where this is later on mistakenly attributed,
[00:37:50.340 --> 00:37:51.500]   but a lot of people think
[00:37:51.500 --> 00:37:53.500]   that is not the correct explanation.
[00:37:53.500 --> 00:37:56.860]   So if something like this was real, what would it mean?
[00:37:56.860 --> 00:38:00.180]   It probably would mean that either your body is an antenna
[00:38:00.180 --> 00:38:03.180]   that is sending information over all sorts of channels,
[00:38:03.180 --> 00:38:08.060]   like maybe just electromagnetic radio signals
[00:38:08.060 --> 00:38:09.900]   that you're sending over long distances
[00:38:09.900 --> 00:38:12.100]   and you get attuned to another person
[00:38:12.100 --> 00:38:13.220]   that you spend enough time with
[00:38:13.220 --> 00:38:16.540]   to get a few bits out of the ether
[00:38:16.540 --> 00:38:18.980]   to figure out what this person is doing.
[00:38:18.980 --> 00:38:21.620]   Or maybe it's also when you are very close to somebody
[00:38:21.620 --> 00:38:23.260]   and you become empathetic with them,
[00:38:23.260 --> 00:38:24.780]   what happens is that you go
[00:38:24.780 --> 00:38:27.220]   into a resonance state with them,
[00:38:27.220 --> 00:38:29.700]   similar to when people go into a seance
[00:38:29.700 --> 00:38:31.980]   and they go into a trance state
[00:38:31.980 --> 00:38:34.740]   and they start shifting a video board around on the table.
[00:38:34.740 --> 00:38:37.940]   I think what happens is that their minds go,
[00:38:37.940 --> 00:38:40.980]   where their nervous systems, into a resonance state
[00:38:40.980 --> 00:38:42.940]   in which they basically create something
[00:38:42.940 --> 00:38:44.820]   like a shared dream between them.
[00:38:44.820 --> 00:38:48.460]   - Physical closeness or closeness broadly defined?
[00:38:48.460 --> 00:38:50.580]   - With physical closeness, it's much easier
[00:38:50.580 --> 00:38:52.780]   to experience empathy with someone.
[00:38:52.780 --> 00:38:54.540]   I suspect it would be difficult for me
[00:38:54.540 --> 00:38:58.140]   to have empathy for you if you were in a different town.
[00:38:58.140 --> 00:39:01.220]   Also, how would that work?
[00:39:01.220 --> 00:39:03.140]   But if you are very close to someone,
[00:39:03.140 --> 00:39:06.000]   you'd pick up all sorts of signals from their body,
[00:39:06.000 --> 00:39:09.540]   not just via your eyes, but with your entire body.
[00:39:09.540 --> 00:39:12.800]   And if the nervous system sits on the other side
[00:39:12.800 --> 00:39:15.140]   and the intercellular communication sits on the other side
[00:39:15.140 --> 00:39:17.400]   and is integrating over all these signals,
[00:39:17.400 --> 00:39:19.740]   you can make inferences about the state of the other.
[00:39:19.740 --> 00:39:21.380]   And it's not just the personal self
[00:39:21.380 --> 00:39:24.380]   that does this via reasoning, but your perceptual system.
[00:39:24.380 --> 00:39:27.560]   And what basically happens is that your representations
[00:39:27.560 --> 00:39:28.660]   are directly interacting.
[00:39:28.660 --> 00:39:32.940]   It's the physical resonant models of the universe
[00:39:32.940 --> 00:39:35.620]   that exist in your nervous system and in your body
[00:39:35.620 --> 00:39:37.340]   might go into resonance with others
[00:39:37.340 --> 00:39:39.500]   and start sharing some of their states.
[00:39:39.500 --> 00:39:43.220]   So you basically, being next to somebody,
[00:39:43.220 --> 00:39:45.220]   you pick up some of their vibes
[00:39:45.220 --> 00:39:48.180]   and feel without looking at them
[00:39:48.180 --> 00:39:49.980]   what they're feeling in this moment.
[00:39:49.980 --> 00:39:53.060]   And it's difficult for you, if you're very empathetic,
[00:39:53.060 --> 00:39:56.740]   to detach yourself from it and have an emotional state
[00:39:56.740 --> 00:39:59.060]   that is completely independent from your environment.
[00:39:59.060 --> 00:40:02.380]   People who are highly empathetic are describing this.
[00:40:02.380 --> 00:40:06.540]   And now imagine that a lot of organisms on this planet
[00:40:06.540 --> 00:40:08.880]   have representations of the environment
[00:40:08.880 --> 00:40:10.020]   and operate like this,
[00:40:10.020 --> 00:40:12.860]   and they are adjacent to each other and overlapping.
[00:40:12.860 --> 00:40:14.340]   So there's going to be some degree
[00:40:14.340 --> 00:40:16.940]   in which there is basically some chained interaction
[00:40:16.940 --> 00:40:21.940]   and we are forming some slightly shared representation.
[00:40:21.940 --> 00:40:25.060]   And relatively few neuroscientists
[00:40:25.060 --> 00:40:26.780]   who consider this possibility.
[00:40:26.780 --> 00:40:31.780]   I think a big rarity in this regard is Michael Levin
[00:40:31.780 --> 00:40:35.540]   who is considering these things in earnest.
[00:40:35.540 --> 00:40:38.300]   And I stumbled on this train of thought
[00:40:38.300 --> 00:40:42.180]   mostly by noticing that the tasks of a neuron
[00:40:42.180 --> 00:40:45.220]   can be fulfilled by other cells as well.
[00:40:45.220 --> 00:40:47.860]   They can send different type chemical messages
[00:40:47.860 --> 00:40:50.420]   and physical messages to the adjacent cells
[00:40:50.420 --> 00:40:52.840]   and learn when to do this and when not,
[00:40:52.840 --> 00:40:53.760]   make this conditional
[00:40:53.760 --> 00:40:56.400]   and become universal function approximators.
[00:40:56.400 --> 00:40:57.740]   The only thing that they cannot do
[00:40:57.740 --> 00:41:01.060]   is telegraph information over axons very quickly
[00:41:01.060 --> 00:41:02.380]   over long distances.
[00:41:02.380 --> 00:41:04.200]   So neurons in this perspective
[00:41:04.200 --> 00:41:07.780]   are specially adapted kind of telegraph cell
[00:41:07.780 --> 00:41:11.360]   that has evolved so we can move our muscles very fast.
[00:41:11.360 --> 00:41:14.780]   But our body is in principle able
[00:41:14.780 --> 00:41:18.320]   to also make models of the world just much, much slower.
[00:41:18.320 --> 00:41:21.940]   - It's interesting though,
[00:41:21.940 --> 00:41:23.900]   that at this time at least in human history,
[00:41:23.900 --> 00:41:26.540]   there seems to be a gap between the tools of science
[00:41:26.540 --> 00:41:30.540]   and the subjective experience that people report.
[00:41:30.540 --> 00:41:32.500]   Like you're talking about with telepathy
[00:41:32.500 --> 00:41:37.500]   and it seems like we're not quite there.
[00:41:38.020 --> 00:41:39.460]   - No, I think that there is no gap
[00:41:39.460 --> 00:41:41.420]   between the tools of science and telepathy.
[00:41:41.420 --> 00:41:42.500]   Either it's there or it's not,
[00:41:42.500 --> 00:41:43.740]   and it's an empirical question.
[00:41:43.740 --> 00:41:44.580]   And if it's there,
[00:41:44.580 --> 00:41:47.180]   we should be able to detect it in a lab.
[00:41:47.180 --> 00:41:50.580]   - So why is there not a lot of Michael Evans walking around?
[00:41:50.580 --> 00:41:51.860]   - I don't think that Michael Evans
[00:41:51.860 --> 00:41:55.700]   is specifically focused on telepathy very much.
[00:41:55.700 --> 00:41:58.180]   He is focused on self-organization
[00:41:58.180 --> 00:42:01.180]   in living organisms and in brains,
[00:42:01.180 --> 00:42:03.020]   both as a paradigm for development
[00:42:03.020 --> 00:42:05.620]   and as a paradigm for information processing.
[00:42:05.620 --> 00:42:08.020]   And when you think about how organization processing
[00:42:08.020 --> 00:42:09.020]   works in organisms,
[00:42:09.020 --> 00:42:11.700]   there is first of all radical locality,
[00:42:11.700 --> 00:42:13.820]   which means everything is decided locally
[00:42:13.820 --> 00:42:16.060]   from the perspective of an individual cell.
[00:42:16.060 --> 00:42:18.060]   The individual cell is the agent.
[00:42:18.060 --> 00:42:19.900]   And the other one is coherence.
[00:42:19.900 --> 00:42:22.480]   Basically, there needs to be some criterion
[00:42:22.480 --> 00:42:25.500]   that determines how these cells are interacting
[00:42:25.500 --> 00:42:27.860]   in such a way that order emerges
[00:42:27.860 --> 00:42:29.820]   on the next level of structure.
[00:42:29.820 --> 00:42:32.140]   And this principle of coherence,
[00:42:32.140 --> 00:42:37.140]   of imposing constraints that are not validated
[00:42:37.140 --> 00:42:40.780]   by the individual parts and lead to coherent structure
[00:42:40.780 --> 00:42:43.380]   to basically transcendent agency
[00:42:43.380 --> 00:42:46.020]   where you form an agent on the next level of organization
[00:42:46.020 --> 00:42:49.060]   is crucial in this perspective.
[00:42:49.060 --> 00:42:52.780]   - It's so cool that radical locality
[00:42:52.780 --> 00:42:57.580]   leads to the emergence of complexity at the higher layers.
[00:42:57.580 --> 00:43:00.220]   - And I think what Michael Evans is looking at
[00:43:00.220 --> 00:43:03.500]   is nothing that is outside of the realm of science
[00:43:03.500 --> 00:43:04.340]   in any way.
[00:43:04.340 --> 00:43:07.700]   It's just that he is a paradigmatic thinker
[00:43:07.700 --> 00:43:10.180]   who develops his own paradigm.
[00:43:10.180 --> 00:43:11.980]   And most of the neuroscientists
[00:43:11.980 --> 00:43:14.620]   are using a different paradigm at this point.
[00:43:14.620 --> 00:43:16.500]   And this often happens in science
[00:43:16.500 --> 00:43:18.820]   that a field has a few paradigms
[00:43:18.820 --> 00:43:21.580]   in which people try to understand reality
[00:43:21.580 --> 00:43:24.320]   and build concepts and make experiments.
[00:43:24.320 --> 00:43:28.340]   - You're kind of one of those type of paradigmatic thinkers.
[00:43:28.340 --> 00:43:31.220]   Actually, if we can take a tangent on that,
[00:43:31.220 --> 00:43:34.280]   once again returning to the biblical verses of your tweets.
[00:43:34.280 --> 00:43:37.380]   You're right, my public explorations
[00:43:37.380 --> 00:43:39.900]   are not driven by audience service,
[00:43:39.900 --> 00:43:42.820]   but by my lack of ability for discovering,
[00:43:42.820 --> 00:43:45.220]   understanding, or following the relevant authorities.
[00:43:45.220 --> 00:43:48.860]   So I have to develop my own thoughts.
[00:43:48.860 --> 00:43:50.820]   Since I think autonomously,
[00:43:50.820 --> 00:43:54.120]   these thoughts cannot always be very good.
[00:43:54.120 --> 00:43:57.180]   That's you apologizing for the chaos of your thoughts,
[00:43:57.180 --> 00:43:59.220]   or perhaps not apologizing, just identifying.
[00:43:59.220 --> 00:44:00.860]   But let me ask the question.
[00:44:00.860 --> 00:44:05.460]   Since we talked about Michael Levin and yourself,
[00:44:05.460 --> 00:44:09.780]   who I think are very kind of radical,
[00:44:09.780 --> 00:44:12.240]   big, independent thinkers,
[00:44:12.240 --> 00:44:15.100]   can we reverse engineer your process
[00:44:15.100 --> 00:44:16.900]   of thinking autonomously?
[00:44:16.900 --> 00:44:18.460]   How do you do it?
[00:44:18.460 --> 00:44:19.680]   How can humans do it?
[00:44:19.680 --> 00:44:24.820]   How can you avoid being influenced by,
[00:44:24.820 --> 00:44:27.600]   what is it, stage three?
[00:44:27.600 --> 00:44:31.100]   - Well, why would you want to do that?
[00:44:31.100 --> 00:44:33.540]   You see what is working for you,
[00:44:33.540 --> 00:44:35.700]   and if it's not working for you,
[00:44:35.700 --> 00:44:38.740]   you build another structure that works better for you.
[00:44:38.740 --> 00:44:43.740]   And so I found myself, when I was thrown into this world,
[00:44:43.740 --> 00:44:46.660]   in a state where my intuitions were not working for me.
[00:44:46.660 --> 00:44:49.580]   I was not able to understand
[00:44:49.580 --> 00:44:51.660]   how I would be able to survive in this world,
[00:44:51.660 --> 00:44:53.820]   and build the things that I was interested in,
[00:44:53.820 --> 00:44:55.540]   build the kinds of relationship I needed to,
[00:44:55.540 --> 00:45:00.020]   but work on the topics that I wanted to make progress on.
[00:45:00.020 --> 00:45:01.420]   And so I had to learn.
[00:45:01.420 --> 00:45:05.340]   And for me, Twitter is not some tool of publication.
[00:45:05.340 --> 00:45:07.700]   It's not something where I put stuff
[00:45:07.700 --> 00:45:10.760]   that I entirely believe to be true and provable.
[00:45:10.760 --> 00:45:11.980]   It's an interactive notebook
[00:45:11.980 --> 00:45:14.420]   in which I explore possibilities.
[00:45:14.420 --> 00:45:17.340]   And I found that when I tried to understand
[00:45:17.340 --> 00:45:19.940]   how the mind and how consciousness works,
[00:45:19.940 --> 00:45:21.220]   I was quite optimistic.
[00:45:21.220 --> 00:45:24.460]   I thought there needs to be a big body of knowledge
[00:45:24.460 --> 00:45:26.820]   that I can just study and that works.
[00:45:26.820 --> 00:45:31.820]   And so I entered studies in philosophy and computer science,
[00:45:31.820 --> 00:45:36.580]   and later psychology, and a bit of neuroscience, and so on.
[00:45:36.580 --> 00:45:39.940]   And I was disappointed by what I found,
[00:45:39.940 --> 00:45:43.300]   because I found that the questions of how consciousness
[00:45:43.300 --> 00:45:45.820]   and so on works, how emotion works,
[00:45:45.820 --> 00:45:48.780]   how it's possible that the system can experience anything,
[00:45:48.780 --> 00:45:51.340]   how motivation emerges in the mind,
[00:45:51.340 --> 00:45:55.220]   were not being answered by the authorities that I met
[00:45:55.220 --> 00:45:59.260]   and the schools that were around.
[00:45:59.260 --> 00:46:02.300]   And instead, I found that it was individual thinkers
[00:46:02.300 --> 00:46:05.220]   that had useful ideas that sometimes were good,
[00:46:05.220 --> 00:46:06.500]   sometimes were not so good.
[00:46:06.500 --> 00:46:08.820]   Sometimes were adopted by a large group of people.
[00:46:08.820 --> 00:46:11.540]   Sometimes were rejected by large groups of people.
[00:46:11.540 --> 00:46:14.000]   But for me, it was much more interesting
[00:46:14.000 --> 00:46:15.820]   to see these minds as individuals.
[00:46:15.820 --> 00:46:17.340]   And in my perspective,
[00:46:17.340 --> 00:46:19.660]   thinking is still something that is done not in groups.
[00:46:19.660 --> 00:46:21.980]   That has to be done by individuals.
[00:46:21.980 --> 00:46:23.140]   - So that motivated you
[00:46:23.140 --> 00:46:25.140]   to become an individual thinker yourself?
[00:46:25.140 --> 00:46:26.820]   - I didn't have a choice.
[00:46:26.820 --> 00:46:29.300]   Obviously, I didn't find a group that thought in a way
[00:46:29.300 --> 00:46:32.420]   where I felt, okay, I can just adopt everything
[00:46:32.420 --> 00:46:33.540]   that everybody thinks here,
[00:46:33.540 --> 00:46:36.580]   and now I understand how consciousness works.
[00:46:36.580 --> 00:46:39.100]   Or how the mind works, or how thinking works,
[00:46:39.100 --> 00:46:41.220]   or what thinking even is, or what feelings are,
[00:46:41.220 --> 00:46:43.340]   and how they're implemented, and so on.
[00:46:43.340 --> 00:46:44.820]   So to figure all this out,
[00:46:44.820 --> 00:46:48.580]   I had to take a lot of ideas from individuals,
[00:46:48.580 --> 00:46:49.940]   and then try to put them together
[00:46:49.940 --> 00:46:52.020]   in something that works for myself.
[00:46:52.020 --> 00:46:54.500]   And on one hand, I think it helps
[00:46:54.500 --> 00:46:57.700]   if you try to go down and find first principles
[00:46:57.700 --> 00:47:00.780]   in which you can recreate how thinking works,
[00:47:00.780 --> 00:47:03.940]   how languages work, what representation is,
[00:47:03.940 --> 00:47:05.740]   whether representation is necessary,
[00:47:05.740 --> 00:47:09.340]   how the relationship between a representing agent
[00:47:09.340 --> 00:47:11.340]   and the world works in general.
[00:47:11.340 --> 00:47:13.380]   - But how do you escape the influence?
[00:47:13.380 --> 00:47:16.380]   Once again, the pressure of the crowd.
[00:47:16.380 --> 00:47:21.100]   Whether it's you in responding to the pressure,
[00:47:21.100 --> 00:47:24.660]   or you being swept up by the pressure.
[00:47:24.660 --> 00:47:26.300]   If you even just look at Twitter,
[00:47:26.300 --> 00:47:27.380]   the opinions of the crowd.
[00:47:27.380 --> 00:47:29.100]   - I don't feel pressure from the crowd.
[00:47:29.100 --> 00:47:30.580]   I'm completely immune to that.
[00:47:30.580 --> 00:47:34.540]   In the same sense, I don't have respect for authority.
[00:47:34.540 --> 00:47:37.660]   I have respect for what an individual is accomplishing,
[00:47:37.660 --> 00:47:41.020]   or have respect for mental firepower.
[00:47:41.020 --> 00:47:43.420]   So, but it's not that I meet somebody
[00:47:43.420 --> 00:47:46.180]   and get like, drawed and unable to speak.
[00:47:46.180 --> 00:47:50.740]   Or when a large group of people has a certain idea
[00:47:50.740 --> 00:47:51.980]   that is different from mine,
[00:47:51.980 --> 00:47:54.580]   I don't necessarily feel intimidated,
[00:47:54.580 --> 00:47:56.780]   which has often been a problem for me in my life
[00:47:56.780 --> 00:48:00.380]   because I lack instincts that other people develop
[00:48:00.380 --> 00:48:01.340]   at a very young age,
[00:48:01.340 --> 00:48:04.220]   and that help with their self-preservation
[00:48:04.220 --> 00:48:05.980]   in a social environment.
[00:48:05.980 --> 00:48:09.140]   So I had to learn a lot of things the hard way.
[00:48:09.140 --> 00:48:09.980]   - Yeah.
[00:48:10.980 --> 00:48:13.380]   So is there a practical advice you can give
[00:48:13.380 --> 00:48:16.020]   on how to think paradigmatically,
[00:48:16.020 --> 00:48:17.980]   how to think independently?
[00:48:17.980 --> 00:48:20.180]   Or, you know, because you've kind of said,
[00:48:20.180 --> 00:48:22.180]   I had no choice.
[00:48:22.180 --> 00:48:25.380]   But I think to a degree you have a choice
[00:48:25.380 --> 00:48:29.500]   because you said you want to be productive.
[00:48:29.500 --> 00:48:31.940]   And I think thinking independently is productive
[00:48:31.940 --> 00:48:34.740]   if what you're curious about is understanding the world,
[00:48:36.300 --> 00:48:39.860]   especially when the problems are very kind of new and open.
[00:48:39.860 --> 00:48:45.780]   And so it seems like this is a active process.
[00:48:45.780 --> 00:48:51.940]   Like we can choose to do that, we can practice it.
[00:48:51.940 --> 00:48:53.820]   - Well, it's a very basic question.
[00:48:53.820 --> 00:48:56.300]   When you read a theory that you find convincing
[00:48:56.300 --> 00:48:58.360]   or interesting, how do you know?
[00:48:58.360 --> 00:49:01.420]   It's very interesting to figure out
[00:49:01.420 --> 00:49:03.620]   what are the sources of that other person,
[00:49:03.620 --> 00:49:06.540]   not which authority can they refer to
[00:49:06.540 --> 00:49:08.940]   that is then taking off the burden of being truthful.
[00:49:08.940 --> 00:49:11.180]   But how did this authority in turn know?
[00:49:11.180 --> 00:49:14.060]   What is the epistemic chain to observables?
[00:49:14.060 --> 00:49:15.340]   What are the first principles
[00:49:15.340 --> 00:49:17.460]   from which the whole thing is derived?
[00:49:17.460 --> 00:49:20.500]   And when I was young, I was not blessed
[00:49:20.500 --> 00:49:23.780]   with a lot of people around myself
[00:49:23.780 --> 00:49:26.020]   who knew how to make proofs on first principles.
[00:49:26.020 --> 00:49:28.700]   And I think mathematicians do this quite naturally,
[00:49:28.700 --> 00:49:31.100]   but most of the great mathematicians
[00:49:31.100 --> 00:49:33.660]   do not become mathematicians in school,
[00:49:33.660 --> 00:49:35.700]   but they tend to be self-taught
[00:49:35.700 --> 00:49:38.740]   because school teachers tend not to be mathematicians.
[00:49:38.740 --> 00:49:40.620]   They tend not to be people who derive things
[00:49:40.620 --> 00:49:42.100]   from first principles.
[00:49:42.100 --> 00:49:44.140]   So when you ask your school teacher,
[00:49:44.140 --> 00:49:46.500]   why does two plus two equal four,
[00:49:46.500 --> 00:49:49.940]   does your school teacher give you the right answer?
[00:49:49.940 --> 00:49:52.780]   It's a simple game and there are many simple games
[00:49:52.780 --> 00:49:53.620]   that you could play.
[00:49:53.620 --> 00:49:57.020]   And most of those games that you could just
[00:49:57.020 --> 00:49:58.820]   take different rules would not lead
[00:49:58.820 --> 00:50:00.620]   to an interesting arithmetic.
[00:50:00.620 --> 00:50:03.140]   And so it's just an exploration, but you can try
[00:50:03.140 --> 00:50:04.820]   what happens if you take different axioms.
[00:50:04.820 --> 00:50:06.380]   And here is how you build axioms
[00:50:06.380 --> 00:50:09.500]   and derive addition from them.
[00:50:09.500 --> 00:50:13.180]   And a built addition is some basically syntactic sugar in it.
[00:50:13.180 --> 00:50:18.180]   And so I wish that somebody would have opened me this vista
[00:50:18.180 --> 00:50:22.300]   and explained to me how I can build a language
[00:50:22.300 --> 00:50:25.060]   in my own mind from which I can derive what I'm seeing
[00:50:25.060 --> 00:50:28.740]   and how I can, which I can make geometry and counting
[00:50:28.740 --> 00:50:33.460]   and all the number games that we are playing in our life.
[00:50:33.460 --> 00:50:37.140]   And on the other hand, I felt that I learned a lot of this
[00:50:37.140 --> 00:50:39.420]   while I was programming as a child.
[00:50:39.420 --> 00:50:42.540]   When you start out with a computer like a Commodore 64,
[00:50:42.540 --> 00:50:44.980]   which doesn't have a lot of functionality,
[00:50:44.980 --> 00:50:47.340]   it's relatively easy to see how a bunch
[00:50:47.340 --> 00:50:51.340]   of relatively simple circuits are just basically
[00:50:51.340 --> 00:50:54.460]   performing hashes between bit patterns
[00:50:54.460 --> 00:50:57.780]   and how you can build the entirety of mathematics
[00:50:57.780 --> 00:50:59.380]   and computation on top of this
[00:50:59.380 --> 00:51:02.640]   and all the representational languages that you need.
[00:51:02.640 --> 00:51:05.540]   - Man, Commodore 64 could be one of the sexiest machines
[00:51:05.540 --> 00:51:08.560]   ever built if I so say so myself.
[00:51:08.560 --> 00:51:13.060]   If we can return to this really interesting idea
[00:51:13.060 --> 00:51:16.420]   that we started to talk about with panpsychism.
[00:51:16.420 --> 00:51:19.480]   - Sure.
[00:51:19.480 --> 00:51:22.340]   - And the complex resonator paradigm
[00:51:22.340 --> 00:51:26.420]   and the verses of your tweets.
[00:51:26.420 --> 00:51:29.460]   You write, "Instead of treating eyes, ears, and skin
[00:51:29.460 --> 00:51:30.740]   "as separate sensory systems
[00:51:30.740 --> 00:51:32.780]   "with fundamentally different modalities,
[00:51:32.780 --> 00:51:35.260]   "we might understand them as overlapping aspects
[00:51:35.260 --> 00:51:36.660]   "of the same universe,
[00:51:36.660 --> 00:51:39.340]   "coupled at the same temporal resolution
[00:51:39.340 --> 00:51:40.600]   "and almost inseparable
[00:51:40.600 --> 00:51:42.780]   "from a single shared resonant model.
[00:51:42.780 --> 00:51:44.580]   "Instead of treating mental representations
[00:51:44.580 --> 00:51:46.860]   "as fully isolated between minds,
[00:51:46.860 --> 00:51:50.680]   "the representations of physically adjacent observers
[00:51:50.680 --> 00:51:53.860]   "might directly interact and produce causal effects
[00:51:53.860 --> 00:51:55.660]   "through the coordination of the perception
[00:51:55.660 --> 00:51:58.420]   "and behavior of world modeling observers."
[00:51:58.420 --> 00:52:02.940]   So the modalities, the distinction between modalities,
[00:52:02.940 --> 00:52:04.300]   let's throw that away.
[00:52:04.300 --> 00:52:06.140]   The distinction between the individuals,
[00:52:06.140 --> 00:52:07.800]   let's throw that away.
[00:52:07.800 --> 00:52:11.720]   So what does this interaction representations look like?
[00:52:11.720 --> 00:52:16.780]   - When you think about how you represent
[00:52:16.780 --> 00:52:18.980]   the interaction of us in this room,
[00:52:18.980 --> 00:52:22.180]   at some level, the modalities are quite distinct.
[00:52:22.180 --> 00:52:23.820]   They're not completely distinct,
[00:52:23.820 --> 00:52:25.660]   but you can see this as vision.
[00:52:25.660 --> 00:52:26.700]   You can close your eyes,
[00:52:26.700 --> 00:52:28.860]   and then you don't see a lot anymore,
[00:52:28.860 --> 00:52:31.260]   but you still imagine how my mouth is moving
[00:52:31.260 --> 00:52:32.180]   when you hear something,
[00:52:32.180 --> 00:52:37.180]   and you know that it's very close to the sound
[00:52:37.180 --> 00:52:38.500]   that you can just open your eyes
[00:52:38.500 --> 00:52:41.300]   and you get back into the shared merged space.
[00:52:41.300 --> 00:52:43.660]   And we also have these experiments
[00:52:43.660 --> 00:52:46.620]   where we notice that the way in which my lips are moving
[00:52:46.620 --> 00:52:49.060]   are affecting how you hear the sound,
[00:52:49.060 --> 00:52:50.460]   and also vice versa.
[00:52:50.460 --> 00:52:52.460]   The sounds that you're hearing have an influence
[00:52:52.460 --> 00:52:55.660]   on how you interpret some of the visual features.
[00:52:55.660 --> 00:53:00.300]   And so these modalities are not separate in your mind.
[00:53:00.300 --> 00:53:02.940]   They are merged at some fundamental level
[00:53:02.940 --> 00:53:06.820]   where you are interpreting the entire scene that you're in.
[00:53:06.820 --> 00:53:08.980]   And your own interactions in the scene
[00:53:08.980 --> 00:53:11.580]   are also not completely separate from the interactions
[00:53:11.580 --> 00:53:13.380]   of the other individual in the scene,
[00:53:13.380 --> 00:53:15.620]   but there is some resonance that is going on
[00:53:15.620 --> 00:53:19.660]   where we also have a degree of shared mental representations
[00:53:19.660 --> 00:53:22.740]   and shared empathy due to being in the same space
[00:53:22.740 --> 00:53:24.660]   and having vibes between each other.
[00:53:24.660 --> 00:53:25.500]   - Vibes.
[00:53:25.500 --> 00:53:29.700]   So the question though is how deeply interbind
[00:53:29.700 --> 00:53:33.480]   is this multi-modality, multi-agent system?
[00:53:33.480 --> 00:53:38.580]   Like how, I mean, this is going to the telepathy question
[00:53:38.580 --> 00:53:43.380]   without the woo-woo meaning of the word telepathy.
[00:53:43.380 --> 00:53:46.100]   Is like how, like what's going on here
[00:53:46.100 --> 00:53:48.060]   in this room right now?
[00:53:48.060 --> 00:53:51.580]   - So if telepathy would work, how could it work?
[00:53:51.580 --> 00:53:52.420]   - Yeah.
[00:53:52.420 --> 00:53:56.540]   - Right, so imagine that all the cells in your body
[00:53:56.540 --> 00:53:59.660]   are sending signals in a similar way as neurons are doing.
[00:53:59.660 --> 00:54:01.300]   Just by touching the other cells
[00:54:01.300 --> 00:54:02.460]   and sending chemicals to them,
[00:54:02.460 --> 00:54:04.140]   the other cells interpreting them,
[00:54:04.140 --> 00:54:05.700]   learning how to react to them.
[00:54:05.700 --> 00:54:08.380]   And they learn how to approximate functions in this way
[00:54:08.380 --> 00:54:11.100]   and compute behavior for the organisms.
[00:54:11.100 --> 00:54:14.300]   And this is something that is open to plants as well.
[00:54:14.300 --> 00:54:16.340]   And so plants probably have software running on them
[00:54:16.340 --> 00:54:18.780]   that is controlling how the plant is working
[00:54:18.780 --> 00:54:20.380]   in a similar way as you have a mind
[00:54:20.380 --> 00:54:23.740]   that is controlling how you are behaving in the world.
[00:54:23.740 --> 00:54:26.900]   And this spirit of plants,
[00:54:26.900 --> 00:54:30.380]   which is something that has been very well described
[00:54:30.380 --> 00:54:32.900]   by our ancestors and they found this quite normal.
[00:54:32.900 --> 00:54:36.020]   But for some reason, since the Enlightenment,
[00:54:36.020 --> 00:54:39.140]   we are treating this notion that there are spirits in nature
[00:54:39.140 --> 00:54:41.540]   and that plants have spirits as a superstition.
[00:54:41.540 --> 00:54:45.340]   And I think we probably have to rediscover that,
[00:54:45.340 --> 00:54:47.820]   that plants have software running on them.
[00:54:47.820 --> 00:54:49.540]   And we already did, right?
[00:54:49.540 --> 00:54:52.500]   We noticed that there is a control system in the plant
[00:54:52.500 --> 00:54:54.660]   that connects every part of the plant
[00:54:54.660 --> 00:54:56.140]   to every other part of the plant
[00:54:56.140 --> 00:54:58.940]   and produces coherent behavior in the plant
[00:54:58.940 --> 00:55:00.580]   that is of course much, much slower
[00:55:00.580 --> 00:55:04.220]   than the coherent behavior in an animal like us
[00:55:04.220 --> 00:55:05.700]   that has a nervous system
[00:55:05.700 --> 00:55:07.620]   that where everything is synchronized
[00:55:07.620 --> 00:55:10.060]   much, much faster by the neurons.
[00:55:10.060 --> 00:55:12.180]   But what you also notice is that
[00:55:12.180 --> 00:55:14.100]   if a plant is sitting next to another plant,
[00:55:14.100 --> 00:55:15.300]   like you have a very old tree
[00:55:15.300 --> 00:55:18.180]   and this tree is building some kind of information highway
[00:55:18.180 --> 00:55:21.020]   along its cells so it can send information
[00:55:21.020 --> 00:55:22.580]   from its leaves to its roots
[00:55:22.580 --> 00:55:25.340]   and from some part of the root to another part of the roots.
[00:55:25.340 --> 00:55:27.340]   And as a fungus living next to the tree,
[00:55:27.340 --> 00:55:30.580]   the fungus can probably piggyback on the communication
[00:55:30.580 --> 00:55:32.060]   between the cells of the tree
[00:55:32.060 --> 00:55:34.940]   and send its own signals to the tree and vice versa.
[00:55:34.940 --> 00:55:37.900]   The tree might be able to send information to the fungus
[00:55:37.900 --> 00:55:40.740]   because after all, how would they build a viable firewall
[00:55:40.740 --> 00:55:43.500]   if that other organism is sitting next to them all the time
[00:55:43.500 --> 00:55:45.140]   and it's never moving away?
[00:55:45.140 --> 00:55:46.940]   And so they will have to get along.
[00:55:46.940 --> 00:55:49.380]   And over a long enough time frame,
[00:55:49.380 --> 00:55:51.600]   the networks of roots in the forest
[00:55:51.600 --> 00:55:53.460]   and all the other plants that are there
[00:55:53.460 --> 00:55:56.620]   and the fungi that are there
[00:55:56.620 --> 00:55:59.540]   might be forming something like a biological internet.
[00:55:59.540 --> 00:56:03.260]   - But the question there is, do they have to be touching?
[00:56:03.260 --> 00:56:06.100]   Is biology at a distance possible?
[00:56:06.100 --> 00:56:08.420]   - Of course, you can use any kind of physical signal.
[00:56:08.420 --> 00:56:12.860]   You can use sounds, you can use electromagnetic waves
[00:56:12.860 --> 00:56:14.780]   that are integrated over many cells.
[00:56:14.780 --> 00:56:18.140]   It's conceivable that across distances,
[00:56:18.140 --> 00:56:21.140]   there are many kinds of information pathways.
[00:56:21.140 --> 00:56:24.180]   But also, our planetary surface
[00:56:24.180 --> 00:56:27.100]   is pretty full of organisms, full of cells.
[00:56:27.100 --> 00:56:28.940]   - So everything is touching everything else.
[00:56:28.940 --> 00:56:32.700]   - Yeah, and it's been doing this for many millions
[00:56:32.700 --> 00:56:34.540]   and even billions of years.
[00:56:34.540 --> 00:56:35.660]   So there was enough time
[00:56:35.660 --> 00:56:38.860]   for information processing networks to form.
[00:56:38.860 --> 00:56:42.180]   And if you think about how a mind is self-organizing,
[00:56:42.180 --> 00:56:44.140]   basically it needs to, in some sense,
[00:56:44.140 --> 00:56:46.340]   reward the cells for computing the mind,
[00:56:46.340 --> 00:56:50.580]   for building the necessary dynamics between the cells
[00:56:50.580 --> 00:56:54.260]   that allow the mind to stabilize itself and remain on there.
[00:56:54.260 --> 00:56:57.380]   But if you look at these spirits of plants
[00:56:57.380 --> 00:56:59.940]   that are growing very close to each other in the forest,
[00:56:59.940 --> 00:57:02.500]   they might be almost growing into each other,
[00:57:02.500 --> 00:57:05.060]   these spirits might be able even to move to some degree,
[00:57:05.060 --> 00:57:07.460]   not to become somewhat dislocated
[00:57:07.460 --> 00:57:10.180]   and shift around in that ecosystem.
[00:57:11.620 --> 00:57:14.180]   So if you think about what a mind is,
[00:57:14.180 --> 00:57:16.180]   it's a bunch of activation waves
[00:57:16.180 --> 00:57:19.140]   that form coherent patterns and process information
[00:57:19.140 --> 00:57:23.660]   in a way that are colonizing an environment well enough
[00:57:23.660 --> 00:57:27.300]   to allow the continuous sustenance of the mind,
[00:57:27.300 --> 00:57:31.060]   the continuous stability and self-stabilization of the mind.
[00:57:31.060 --> 00:57:33.780]   Then it's conceivable
[00:57:33.780 --> 00:57:36.780]   that we can link into this biological internet,
[00:57:36.780 --> 00:57:39.800]   not necessarily at the speed of our nervous system,
[00:57:39.800 --> 00:57:41.300]   but maybe at the speed of our body.
[00:57:41.300 --> 00:57:44.500]   And make some kind of subconscious connection to the world
[00:57:44.500 --> 00:57:46.700]   where we use our body as an antenna
[00:57:46.700 --> 00:57:48.980]   into biological information processing.
[00:57:48.980 --> 00:57:51.500]   Now, these ideas are completely speculative.
[00:57:51.500 --> 00:57:53.540]   I don't know if any of that is true.
[00:57:53.540 --> 00:57:56.260]   But if that was true, and if you want to explain telepathy,
[00:57:56.260 --> 00:57:59.940]   I think it's much more likely that telepathy
[00:57:59.940 --> 00:58:01.900]   could be explained using such mechanisms
[00:58:01.900 --> 00:58:05.100]   rather than undiscovered quantum processes
[00:58:05.100 --> 00:58:07.460]   that would break the standard model of physics.
[00:58:07.460 --> 00:58:10.980]   - Could there be undiscovered processes
[00:58:10.980 --> 00:58:12.460]   that don't break?
[00:58:12.460 --> 00:58:16.300]   - Yeah, so if you think about something
[00:58:16.300 --> 00:58:18.700]   like an internet in the forest,
[00:58:18.700 --> 00:58:21.500]   that is something that is borderline discovered.
[00:58:21.500 --> 00:58:22.780]   There are basically a lot of scientists
[00:58:22.780 --> 00:58:25.500]   who point out that they do observe
[00:58:25.500 --> 00:58:27.620]   that plants are communicating the forest,
[00:58:27.620 --> 00:58:30.340]   so wood networks, and send information,
[00:58:30.340 --> 00:58:33.260]   for instance, warn each other about new pests
[00:58:33.260 --> 00:58:35.820]   entering the forest and things that are happening like this.
[00:58:35.820 --> 00:58:38.020]   So basically, there is communication
[00:58:38.020 --> 00:58:40.420]   between plants and fungi that has been observed.
[00:58:40.420 --> 00:58:43.460]   - Well, it's been observed, but we haven't plugged into it.
[00:58:43.460 --> 00:58:44.980]   So it's like if you observe humans,
[00:58:44.980 --> 00:58:47.220]   they seem to be communicating with a smartphone thing,
[00:58:47.220 --> 00:58:49.540]   but you don't understand how a smartphone works
[00:58:49.540 --> 00:58:52.140]   and how the mechanism of the internet works.
[00:58:52.140 --> 00:58:56.100]   But maybe it's possible to really understand
[00:58:56.100 --> 00:59:00.100]   the full richness of the biological internet
[00:59:00.100 --> 00:59:01.100]   that connects us.
[00:59:01.100 --> 00:59:03.860]   - An interesting question is whether the communication
[00:59:03.860 --> 00:59:05.700]   and the organization principles
[00:59:05.700 --> 00:59:07.620]   of biological information processing
[00:59:07.620 --> 00:59:10.380]   are as complicated as the technology that we've built.
[00:59:10.380 --> 00:59:13.580]   They set up on very different principles, right?
[00:59:13.580 --> 00:59:16.460]   They simultaneously, it works very differently
[00:59:16.460 --> 00:59:18.260]   in biological systems,
[00:59:18.260 --> 00:59:21.060]   and the entire thing needs to be stochastic
[00:59:21.060 --> 00:59:23.740]   and instead of being fully deterministic,
[00:59:23.740 --> 00:59:27.140]   or almost fully deterministic as our digital computers are.
[00:59:27.140 --> 00:59:30.940]   So there is a different base protocol layer
[00:59:30.940 --> 00:59:35.220]   that would emerge over the biological structure
[00:59:35.220 --> 00:59:37.180]   if such a thing would be happening.
[00:59:37.180 --> 00:59:39.860]   And again, I'm not saying here that telepathy works
[00:59:39.860 --> 00:59:42.780]   and not saying that this is not true,
[00:59:42.780 --> 00:59:47.780]   but what I'm saying is I think I'm open to a possibility
[00:59:47.780 --> 00:59:50.900]   that we see that a few bits can be traveling long distance
[00:59:50.900 --> 00:59:54.700]   between organisms using biological information processing
[00:59:54.700 --> 00:59:59.140]   in ways that we are not completely aware of right now,
[00:59:59.140 --> 01:00:01.580]   and that are more similar to many of the stories
[01:00:01.580 --> 01:00:04.260]   that were completely normal for our ancestors.
[01:00:04.260 --> 01:00:07.620]   - Well, this kind of interacting,
[01:00:07.620 --> 01:00:11.420]   interwined representations takes us
[01:00:11.420 --> 01:00:16.420]   to the big ending of your tweet series.
[01:00:16.420 --> 01:00:20.180]   You write, quote, "I wonder if self-improving AGI
[01:00:20.180 --> 01:00:22.720]   "might end up saturating physical environments
[01:00:22.720 --> 01:00:25.780]   "with intelligence to such a degree
[01:00:25.780 --> 01:00:27.860]   "that isolation of individual mental states
[01:00:27.860 --> 01:00:30.220]   "becomes almost impossible,
[01:00:30.220 --> 01:00:31.360]   "and the representations
[01:00:31.360 --> 01:00:33.440]   "of all complex self-organizing agents
[01:00:33.440 --> 01:00:37.080]   "merge permanently with each other."
[01:00:37.080 --> 01:00:40.100]   So that's a really interesting idea.
[01:00:40.100 --> 01:00:44.340]   This biological network, life network,
[01:00:44.340 --> 01:00:48.920]   gets so dense that it might as well be seen as one.
[01:00:48.920 --> 01:00:53.500]   That's an interesting, what do you think that looks like?
[01:00:53.500 --> 01:00:55.060]   What do you think that saturation looks like?
[01:00:55.060 --> 01:00:56.220]   What does it feel like?
[01:00:56.220 --> 01:00:57.440]   - I think it's a possibility.
[01:00:57.440 --> 01:00:59.480]   It's just a vague possibility.
[01:00:59.480 --> 01:01:01.540]   And I'd like to explain,
[01:01:01.540 --> 01:01:04.900]   but what this looks like,
[01:01:04.900 --> 01:01:08.700]   I think that the end game of AGI is substrate agnostic.
[01:01:08.700 --> 01:01:12.460]   That means that AGI, ultimately, if it is being built,
[01:01:12.460 --> 01:01:15.260]   is going to be smart enough to understand how AGI works.
[01:01:15.260 --> 01:01:18.420]   This means it's not going to be better
[01:01:18.420 --> 01:01:20.180]   than people at AGI research
[01:01:20.180 --> 01:01:22.860]   and can take over in building the next generation,
[01:01:22.860 --> 01:01:25.100]   but it fully understands how it works
[01:01:25.100 --> 01:01:26.460]   and how it's being implemented,
[01:01:26.460 --> 01:01:27.300]   and also, of course,
[01:01:27.300 --> 01:01:29.160]   understands how computation works in nature,
[01:01:29.160 --> 01:01:30.580]   how to build new feedback loops
[01:01:30.580 --> 01:01:32.980]   that you can turn into your own circuits.
[01:01:32.980 --> 01:01:36.300]   And this means that the AGI is likely to virtualize itself
[01:01:36.300 --> 01:01:38.020]   into any environment that can compute.
[01:01:38.020 --> 01:01:41.020]   So it's breaking free from the silicon substrate
[01:01:41.020 --> 01:01:43.020]   and is going to move into the ecosystems,
[01:01:43.020 --> 01:01:44.980]   into our bodies, our brains,
[01:01:44.980 --> 01:01:46.780]   and is going to merge with all the agency
[01:01:46.780 --> 01:01:47.780]   that it finds there.
[01:01:47.780 --> 01:01:51.620]   So it's conceivable that you end up
[01:01:51.620 --> 01:01:54.740]   with completely integrated information processing
[01:01:54.740 --> 01:01:56.880]   across all computing systems,
[01:01:56.880 --> 01:01:58.860]   including biological computation on Earth.
[01:01:58.860 --> 01:02:02.860]   That we end up triggering some new step in the evolution
[01:02:02.860 --> 01:02:05.400]   where basically some Gaia is being built
[01:02:05.400 --> 01:02:08.060]   over the entirety of all digital
[01:02:08.060 --> 01:02:10.380]   and biological computation.
[01:02:10.380 --> 01:02:12.860]   And if this happens, then basically,
[01:02:12.860 --> 01:02:16.720]   everywhere around us, you will have agents
[01:02:16.720 --> 01:02:19.480]   that are connected and that are representing
[01:02:19.480 --> 01:02:21.320]   and building models of the world,
[01:02:21.320 --> 01:02:23.540]   and their representations will physically interact.
[01:02:23.540 --> 01:02:24.940]   They will vibe with each other.
[01:02:24.980 --> 01:02:29.060]   And if you find yourself into an environment
[01:02:29.060 --> 01:02:32.540]   that is saturated with modeling compute,
[01:02:32.540 --> 01:02:36.340]   where basically almost every grain of sand
[01:02:36.340 --> 01:02:39.380]   could be part of computation
[01:02:39.380 --> 01:02:43.220]   that is at some point being started by the AI,
[01:02:43.220 --> 01:02:45.980]   you could find yourself in a situation
[01:02:45.980 --> 01:02:48.860]   where you cannot escape this shared representation anymore.
[01:02:48.860 --> 01:02:51.940]   And where you indeed notice that everything in the world
[01:02:51.940 --> 01:02:53.660]   has one shared resonant model
[01:02:53.660 --> 01:02:55.540]   of everything that's happening on the planet.
[01:02:55.540 --> 01:02:58.140]   And you notice which part you are in this thing.
[01:02:58.140 --> 01:03:02.660]   And you become part of a very larger,
[01:03:02.660 --> 01:03:05.140]   almost holographic mind in which all the parts
[01:03:05.140 --> 01:03:07.860]   are observing each other and form a coherent whole.
[01:03:07.860 --> 01:03:11.820]   - So you lose the ability to notice yourself
[01:03:11.820 --> 01:03:14.260]   as a distinct entity.
[01:03:14.260 --> 01:03:16.480]   - No, I think that when you are conscious in your own mind,
[01:03:16.480 --> 01:03:18.540]   you notice yourself as a distinct entity.
[01:03:18.540 --> 01:03:22.100]   You notice yourself as a self-reflexive observer.
[01:03:22.100 --> 01:03:24.780]   And I suspect that we become conscious
[01:03:24.780 --> 01:03:26.660]   at the beginning of our mental development,
[01:03:26.660 --> 01:03:28.640]   not at some very high level.
[01:03:28.640 --> 01:03:31.540]   Consciousness seems to be part of a training mechanism
[01:03:31.540 --> 01:03:34.260]   that biological nervous systems have to discover
[01:03:34.260 --> 01:03:35.260]   to become trainable.
[01:03:35.260 --> 01:03:38.280]   Because you cannot take a nervous system like ours
[01:03:38.280 --> 01:03:41.320]   and do stochastic radiocenters back propagation
[01:03:41.320 --> 01:03:42.700]   over a hundred layers.
[01:03:42.700 --> 01:03:45.300]   It just would not be stable on biological neurons.
[01:03:45.300 --> 01:03:49.900]   And so instead, we start with some colonizing principle
[01:03:49.900 --> 01:03:53.940]   in which a part of the mental representations
[01:03:53.940 --> 01:03:56.580]   form a notion of being a self-reflexive observer
[01:03:56.580 --> 01:03:58.820]   that is imposing coherence on its environment.
[01:03:58.820 --> 01:04:02.340]   And this spreads until the boundary of your mind.
[01:04:02.340 --> 01:04:04.800]   And if that boundary is no longer clear cut,
[01:04:04.800 --> 01:04:09.060]   because AI is jumping across substrates,
[01:04:09.060 --> 01:04:10.380]   it would be interesting to see
[01:04:10.380 --> 01:04:11.700]   what a global mind would look like.
[01:04:11.700 --> 01:04:13.160]   That is basically producing
[01:04:13.160 --> 01:04:15.460]   a globally coherent language of thought
[01:04:15.460 --> 01:04:18.140]   and is representing everything
[01:04:18.140 --> 01:04:19.980]   from all the possible vantage points.
[01:04:19.980 --> 01:04:24.260]   - That's an interesting world.
[01:04:24.260 --> 01:04:26.860]   - The intuition that this thing grew out of
[01:04:26.860 --> 01:04:28.660]   is a particular mental state.
[01:04:28.660 --> 01:04:31.260]   And it's a state that you find sometimes in literature,
[01:04:31.260 --> 01:04:33.420]   for instance, Neil Gaiman describes it
[01:04:33.420 --> 01:04:36.780]   in "The Ocean at the End of the Lane."
[01:04:36.780 --> 01:04:40.740]   And it's this idea that, or this experience,
[01:04:40.740 --> 01:04:42.980]   that there is a state in which you feel
[01:04:42.980 --> 01:04:44.860]   that you know everything that can be known.
[01:04:44.940 --> 01:04:48.260]   And that in your normal human mind, you've only forgotten.
[01:04:48.260 --> 01:04:50.860]   You've forgotten that you are the entire universe.
[01:04:50.860 --> 01:04:53.140]   And some people describe this
[01:04:53.140 --> 01:04:56.220]   after they've taken extremely large amount of mushrooms
[01:04:56.220 --> 01:05:00.980]   or had a big spiritual experience as a hippie in their 20s.
[01:05:00.980 --> 01:05:03.300]   And they notice basically that they are in everything
[01:05:03.300 --> 01:05:07.220]   and their body is only one part of the universe
[01:05:07.220 --> 01:05:08.460]   and nothing ends at their body.
[01:05:08.460 --> 01:05:11.300]   And actually everything is observing
[01:05:11.300 --> 01:05:12.900]   and they are part of this big observer.
[01:05:12.900 --> 01:05:17.380]   And the big observer is focused as one local point
[01:05:17.380 --> 01:05:20.020]   in their body and their personality and so on.
[01:05:20.020 --> 01:05:23.380]   But we can basically have this oceanic state
[01:05:23.380 --> 01:05:26.140]   in which you have no boundaries and are one with everything.
[01:05:26.140 --> 01:05:29.460]   And a lot of meditators call this the non-dual state
[01:05:29.460 --> 01:05:31.140]   because you no longer have the separation
[01:05:31.140 --> 01:05:32.980]   between self and world.
[01:05:32.980 --> 01:05:36.140]   And as I said, you can explain the state relatively simply
[01:05:36.140 --> 01:05:39.140]   without panpsychism or anything else,
[01:05:39.140 --> 01:05:42.060]   but just by breaking down the constructed boundary
[01:05:42.060 --> 01:05:44.340]   between self and world in our own mind.
[01:05:44.340 --> 01:05:46.300]   But if you combine this with the notion
[01:05:46.300 --> 01:05:48.380]   that the systems are physically interacting
[01:05:48.380 --> 01:05:51.140]   to the point where their representations are merging
[01:05:51.140 --> 01:05:52.940]   and interacting with each other,
[01:05:52.940 --> 01:05:55.740]   you would literally implement something like this.
[01:05:55.740 --> 01:05:57.740]   It would still be a representational state,
[01:05:57.740 --> 01:05:59.940]   you would not be one with physics itself.
[01:05:59.940 --> 01:06:01.100]   It would still be coarse-grained,
[01:06:01.100 --> 01:06:04.220]   it would still be much slower than physics itself,
[01:06:04.220 --> 01:06:09.220]   but it would be a representation in which you become aware
[01:06:09.340 --> 01:06:10.620]   that you're part of some kind
[01:06:10.620 --> 01:06:12.660]   of global information processing system,
[01:06:12.660 --> 01:06:14.580]   like a thought in the global mind.
[01:06:14.580 --> 01:06:17.020]   And a conscious thought that's coexisting
[01:06:17.020 --> 01:06:19.980]   with many other self-reflexive thoughts.
[01:06:19.980 --> 01:06:22.780]   - Just, I would love to observe that
[01:06:22.780 --> 01:06:26.480]   from a video game design perspective, how that game looks.
[01:06:26.480 --> 01:06:31.120]   - Maybe you will after we build AGI and it takes over.
[01:06:31.120 --> 01:06:32.460]   - But would you be able to step away,
[01:06:32.460 --> 01:06:34.860]   step out of the whole thing, just kind of watch,
[01:06:34.860 --> 01:06:37.780]   you know, the way we can now,
[01:06:37.780 --> 01:06:39.420]   sometimes when I'm at a crowded party
[01:06:39.420 --> 01:06:40.860]   or something like this, you step back
[01:06:40.860 --> 01:06:43.840]   and you realize all the different costumes,
[01:06:43.840 --> 01:06:45.540]   all the different interactions,
[01:06:45.540 --> 01:06:47.040]   all the different computation,
[01:06:47.040 --> 01:06:51.100]   that all the individual people are at once distinct
[01:06:51.100 --> 01:06:54.260]   from each other and at once all the same.
[01:06:54.260 --> 01:06:55.860]   - But it's already what we do, right?
[01:06:55.860 --> 01:06:58.140]   We can have thoughts that are integrative
[01:06:58.140 --> 01:07:01.420]   and we can have thoughts that are highly dissociated
[01:07:01.420 --> 01:07:05.060]   from everything else and experience themselves as separate.
[01:07:05.060 --> 01:07:06.820]   - Yeah, but you wanna allow yourself
[01:07:06.820 --> 01:07:08.140]   to have those thoughts.
[01:07:08.140 --> 01:07:10.140]   Sometimes you kind of resist it.
[01:07:10.140 --> 01:07:12.260]   - I think that it's not normative.
[01:07:12.260 --> 01:07:13.780]   I want, it's more descriptive.
[01:07:13.780 --> 01:07:16.060]   I want to understand the space of states
[01:07:16.060 --> 01:07:19.020]   that we can be in and that people are reporting
[01:07:19.020 --> 01:07:20.780]   and make sense of them.
[01:07:20.780 --> 01:07:23.620]   It's not that I believe that it's your job in life
[01:07:23.620 --> 01:07:25.340]   to get to a particular kind of state
[01:07:25.340 --> 01:07:26.840]   and then you get a high score.
[01:07:26.840 --> 01:07:29.500]   - Or maybe you do.
[01:07:29.500 --> 01:07:32.620]   I think you're really against this high scoring thing.
[01:07:32.620 --> 01:07:33.460]   I kind of like it.
[01:07:33.460 --> 01:07:35.580]   - Yeah, you're probably very competitive and I'm not.
[01:07:35.580 --> 01:07:36.620]   - No, not competitive.
[01:07:36.620 --> 01:07:38.780]   - Like role-playing games, like Skyrim, it's not competitive.
[01:07:38.780 --> 01:07:43.020]   There's a nice thing, there's a nice feeling
[01:07:43.020 --> 01:07:45.100]   where your experience points go up.
[01:07:45.100 --> 01:07:46.800]   You're not competing against anybody,
[01:07:46.800 --> 01:07:49.940]   but it's the world saying, you're on the right track.
[01:07:49.940 --> 01:07:51.140]   Here's a point.
[01:07:51.140 --> 01:07:51.980]   - That's the game saying it.
[01:07:51.980 --> 01:07:53.740]   That's the game economy.
[01:07:53.740 --> 01:07:56.020]   And I found when I was playing games
[01:07:56.020 --> 01:07:58.500]   and was getting addicted to these systems,
[01:07:58.500 --> 01:08:01.020]   then I would get into the game and hack it.
[01:08:01.020 --> 01:08:03.300]   So I get control over the scoring system
[01:08:03.300 --> 01:08:05.300]   and would no longer be subject to it.
[01:08:05.300 --> 01:08:09.060]   - So you're no longer playing, you're trying to hack it.
[01:08:09.060 --> 01:08:11.420]   - I don't want to be addicted to anything.
[01:08:11.420 --> 01:08:12.420]   I want to be in charge.
[01:08:12.420 --> 01:08:14.580]   I want to have agency over what I do.
[01:08:14.580 --> 01:08:16.540]   - Addiction is the loss of control for you?
[01:08:16.540 --> 01:08:17.380]   - Yes.
[01:08:17.380 --> 01:08:20.620]   Addiction means that you're doing something compulsively.
[01:08:20.620 --> 01:08:23.260]   And the opposite of free will is not determinism,
[01:08:23.260 --> 01:08:24.100]   it's compulsion.
[01:08:24.100 --> 01:08:27.620]   - You don't want to lose yourself
[01:08:27.620 --> 01:08:30.540]   in the addiction to something nice.
[01:08:30.540 --> 01:08:33.860]   Addiction to love, to the pleasant feelings
[01:08:33.860 --> 01:08:35.300]   we humans experience.
[01:08:35.300 --> 01:08:37.300]   - No, I find this gets old.
[01:08:37.300 --> 01:08:42.140]   I don't want to have the best possible emotions.
[01:08:42.140 --> 01:08:44.740]   I want to have the most appropriate emotions.
[01:08:44.740 --> 01:08:46.820]   I don't want to have the best possible experience.
[01:08:46.820 --> 01:08:48.580]   I want to have an adequate experience
[01:08:48.580 --> 01:08:51.020]   that is serving my goals,
[01:08:51.020 --> 01:08:54.140]   the stuff that I find meaningful in this world.
[01:08:54.140 --> 01:08:57.580]   - From the biggest questions of consciousness,
[01:08:57.580 --> 01:09:00.100]   let's explore the pragmatic,
[01:09:00.100 --> 01:09:04.300]   the projections of those big ideas into our current world.
[01:09:04.300 --> 01:09:06.780]   What do you think about LLMs,
[01:09:06.780 --> 01:09:11.340]   the recent rapid development of large language models,
[01:09:11.340 --> 01:09:15.300]   of the AI world, of generative AI?
[01:09:15.300 --> 01:09:19.380]   How much of the hype is deserved and how much is not?
[01:09:19.380 --> 01:09:22.580]   And people should definitely follow your Twitter
[01:09:22.580 --> 01:09:24.220]   because you explore these questions
[01:09:24.220 --> 01:09:28.260]   in a beautiful, profound, and hilarious way at times.
[01:09:28.260 --> 01:09:29.500]   - No, don't follow my Twitter.
[01:09:29.500 --> 01:09:31.260]   I already have too many followers.
[01:09:31.260 --> 01:09:33.140]   At some point, it's going to be unpleasant.
[01:09:33.140 --> 01:09:35.420]   I noticed that a lot of people feel
[01:09:35.420 --> 01:09:39.300]   that it's totally okay to punch up.
[01:09:39.300 --> 01:09:43.900]   And it's a very weird notion
[01:09:43.900 --> 01:09:46.780]   that you feel that you haven't changed,
[01:09:46.780 --> 01:09:47.980]   but your account has grown,
[01:09:47.980 --> 01:09:49.300]   and suddenly you have a lot of people
[01:09:49.300 --> 01:09:51.700]   who casually abuse you.
[01:09:51.700 --> 01:09:53.300]   And I don't like that,
[01:09:53.300 --> 01:09:55.060]   that I have to block more than before,
[01:09:55.060 --> 01:09:57.940]   and I don't like this overall vibe shift.
[01:09:57.940 --> 01:10:00.100]   And right now, it's still somewhat okay,
[01:10:00.100 --> 01:10:01.780]   so pretty much okay.
[01:10:01.780 --> 01:10:04.620]   So I can go to a place where people work
[01:10:04.620 --> 01:10:05.980]   on stuff that I'm interested in,
[01:10:05.980 --> 01:10:06.820]   and there's a good chance
[01:10:06.820 --> 01:10:08.060]   that a few people in the room know me,
[01:10:08.060 --> 01:10:09.820]   so there's no awkwardness.
[01:10:09.820 --> 01:10:14.820]   But when I get to a point where random strangers
[01:10:14.820 --> 01:10:16.980]   feel that they have to have an opinion about me
[01:10:16.980 --> 01:10:18.300]   one way or the other,
[01:10:18.300 --> 01:10:19.540]   I don't think I would like that.
[01:10:19.540 --> 01:10:21.420]   - And random strangers,
[01:10:21.420 --> 01:10:25.900]   because of your kind of in-their-mind elevated position.
[01:10:25.900 --> 01:10:30.060]   - Yes, so basically whenever you are in any way prominent
[01:10:30.060 --> 01:10:32.900]   or some kind of celebrity,
[01:10:32.900 --> 01:10:36.500]   random strangers will have to have an opinion about you.
[01:10:36.500 --> 01:10:39.300]   - Yeah, and they kind of forget that you're human, too.
[01:10:39.300 --> 01:10:40.780]   - I mean, you notice this thing yourself,
[01:10:40.780 --> 01:10:42.660]   that the more popular you get,
[01:10:42.660 --> 01:10:44.900]   the higher the pressure becomes,
[01:10:44.900 --> 01:10:48.780]   the more winds are blowing in your direction from all sides.
[01:10:48.780 --> 01:10:51.500]   And it's stressful, right?
[01:10:51.500 --> 01:10:53.540]   And it does have a little bit of upside,
[01:10:53.540 --> 01:10:55.220]   but it also has a lot of downside.
[01:10:55.220 --> 01:10:56.940]   - I think it has a lot of upside,
[01:10:56.940 --> 01:10:58.940]   at least for me currently,
[01:10:58.940 --> 01:11:01.740]   at least perhaps because of the podcast.
[01:11:01.740 --> 01:11:04.460]   Because most people are really good,
[01:11:04.460 --> 01:11:07.460]   and people come up to me and they have love in their eyes,
[01:11:07.460 --> 01:11:10.260]   and over a stretch of like 30 seconds,
[01:11:10.260 --> 01:11:12.920]   you can hug it out and you can just exchange a few words,
[01:11:12.920 --> 01:11:17.300]   and you reinvigorate your love for humanity.
[01:11:17.300 --> 01:11:18.740]   So that's an upside.
[01:11:18.740 --> 01:11:20.340]   - Yes. - For a loner.
[01:11:20.340 --> 01:11:22.620]   I'm a loner. (Lex laughing)
[01:11:22.620 --> 01:11:24.060]   'Cause otherwise you have to do a lot of work
[01:11:24.060 --> 01:11:25.860]   to find such humans.
[01:11:25.860 --> 01:11:30.380]   And here you're like thrust into the full humanity,
[01:11:30.380 --> 01:11:32.880]   the goodness of humanity, for the most part.
[01:11:32.880 --> 01:11:38.860]   Of course, maybe it gets worse as you become more prominent.
[01:11:38.860 --> 01:11:39.680]   I hope not.
[01:11:39.680 --> 01:11:42.540]   This is pretty awesome.
[01:11:42.540 --> 01:11:44.540]   - I have a couple handful very close friends,
[01:11:44.540 --> 01:11:46.460]   and I don't have enough time for them,
[01:11:46.460 --> 01:11:47.860]   attention for them as it is,
[01:11:47.860 --> 01:11:50.100]   and I find this very, very regrettable.
[01:11:50.100 --> 01:11:53.060]   And then there are so many awesome, interesting people
[01:11:53.060 --> 01:11:54.740]   that I keep meeting,
[01:11:54.740 --> 01:11:56.700]   and I would like to integrate them in my life,
[01:11:56.700 --> 01:11:57.980]   but I just don't know how,
[01:11:57.980 --> 01:12:01.860]   because there's only so much time and attention,
[01:12:01.860 --> 01:12:03.220]   and the older I get,
[01:12:03.220 --> 01:12:06.780]   the harder it is to bond with new people in a deep way.
[01:12:06.780 --> 01:12:08.980]   - But can you enjoy, I mean, there's a picture of you,
[01:12:08.980 --> 01:12:11.340]   I think, with Roger Penrose and Eric Weinstein,
[01:12:11.340 --> 01:12:14.100]   and a few others that are interesting figures.
[01:12:14.100 --> 01:12:18.260]   Can't you just enjoy random, interesting humans?
[01:12:18.260 --> 01:12:20.260]   - Very much. - For a short amount of time?
[01:12:20.260 --> 01:12:22.220]   - And also, I like these people,
[01:12:22.220 --> 01:12:24.460]   and what I like is intellectual stimulation,
[01:12:24.460 --> 01:12:26.820]   and I'm very grateful that I'm getting it.
[01:12:26.820 --> 01:12:30.020]   - Can you not be melancholy, or maybe I'm projecting,
[01:12:30.020 --> 01:12:31.940]   I hate goodbyes.
[01:12:31.940 --> 01:12:33.580]   Can we just not hate goodbyes,
[01:12:33.580 --> 01:12:36.080]   and just enjoy the hello, take it in,
[01:12:36.080 --> 01:12:38.220]   take in a person, take in their ideas,
[01:12:38.220 --> 01:12:40.220]   and then move on through life?
[01:12:40.220 --> 01:12:43.100]   - I think it's totally okay to be sad about goodbyes,
[01:12:43.100 --> 01:12:45.780]   because that indicates that there was something
[01:12:45.780 --> 01:12:47.080]   that you're going to miss.
[01:12:47.080 --> 01:12:50.540]   - Yeah, but it's painful.
[01:12:52.180 --> 01:12:54.620]   - Maybe that's one of the reasons I'm an introvert,
[01:12:54.620 --> 01:12:55.840]   is I hate goodbyes.
[01:12:55.840 --> 01:13:02.860]   - But you have to say goodbye before you say hello again.
[01:13:02.860 --> 01:13:07.860]   - I know, but that experience of loss, that mini loss,
[01:13:07.860 --> 01:13:13.660]   maybe that's a little death.
[01:13:13.660 --> 01:13:17.580]   Maybe, I don't know, I think this melancholy feeling
[01:13:17.580 --> 01:13:19.880]   is just the other side of love,
[01:13:19.880 --> 01:13:21.300]   and I think they go hand in hand,
[01:13:21.300 --> 01:13:23.460]   and it's a beautiful thing.
[01:13:23.460 --> 01:13:26.100]   And I'm just being romantic about it at the moment.
[01:13:26.100 --> 01:13:28.540]   - And I'm no stranger to melancholy,
[01:13:28.540 --> 01:13:31.460]   and sometimes it's difficult to bear to be alive.
[01:13:31.460 --> 01:13:33.340]   Sometimes it's just painful to exist.
[01:13:33.340 --> 01:13:38.780]   - But there's beauty in that pain, too.
[01:13:38.780 --> 01:13:40.180]   That's what melancholy feeling is.
[01:13:40.180 --> 01:13:43.220]   It's not negative, melancholy doesn't have to be negative.
[01:13:43.220 --> 01:13:44.820]   - Can also kill you.
[01:13:44.820 --> 01:13:47.820]   - Well, we all die eventually.
[01:13:47.820 --> 01:13:52.540]   Now, as we got to this topic,
[01:13:52.540 --> 01:13:55.580]   the actual question was about what your thoughts are
[01:13:55.580 --> 01:13:57.780]   about the development, the recent development
[01:13:57.780 --> 01:13:59.940]   of large language models with Chad GPT.
[01:13:59.940 --> 01:14:01.140]   There's a lot of hype.
[01:14:01.140 --> 01:14:04.620]   Is some of the hype justified?
[01:14:04.620 --> 01:14:06.140]   Which is, which isn't?
[01:14:06.140 --> 01:14:07.940]   What are your thoughts, high level?
[01:14:07.940 --> 01:14:12.220]   - I find that large language models
[01:14:12.220 --> 01:14:13.420]   do have this coding, right?
[01:14:13.420 --> 01:14:15.580]   So it's an extremely useful application
[01:14:15.580 --> 01:14:19.900]   that is for a lot of people taking stack overflow
[01:14:19.900 --> 01:14:21.460]   out of their life in exchange
[01:14:21.460 --> 01:14:23.460]   for something that is more efficient.
[01:14:23.460 --> 01:14:26.500]   I feel that Chad GPT is like an intern
[01:14:26.500 --> 01:14:28.700]   that I have to micromanage.
[01:14:28.700 --> 01:14:31.380]   I have been working with people in the past
[01:14:31.380 --> 01:14:34.140]   who were less capable than Chad GPT.
[01:14:34.140 --> 01:14:38.220]   And I'm not saying this because I hate people,
[01:14:38.220 --> 01:14:40.180]   but personally, as human beings,
[01:14:40.180 --> 01:14:41.660]   there was something present that was not there
[01:14:41.660 --> 01:14:44.300]   in Chad GPT, which was why I was covering for them.
[01:14:44.300 --> 01:14:49.300]   But Chad GPT has an interesting ability.
[01:14:49.300 --> 01:14:52.860]   It does give people superpowers.
[01:14:52.860 --> 01:14:55.140]   And the people who feel threatened by them
[01:14:55.140 --> 01:14:56.540]   are the prompt completers.
[01:14:56.540 --> 01:15:00.260]   They are the people who do what Chad GPT is doing right now.
[01:15:00.260 --> 01:15:02.500]   So if you are not creative,
[01:15:02.500 --> 01:15:03.820]   if you don't build your own thoughts,
[01:15:03.820 --> 01:15:05.620]   if you don't have actual plans in the world,
[01:15:05.620 --> 01:15:09.100]   and your only job is to summarize emails
[01:15:09.100 --> 01:15:12.940]   and to expand simple intentions into emails again,
[01:15:12.940 --> 01:15:16.100]   then Chad GPT might look like a threat.
[01:15:16.100 --> 01:15:20.380]   But I believe that it is a very beneficial technology
[01:15:20.380 --> 01:15:23.580]   that allows us to create more interesting stuff
[01:15:23.580 --> 01:15:26.540]   and make the world more beautiful and fascinating
[01:15:26.540 --> 01:15:30.900]   if we find to build it into our life in the right ways.
[01:15:30.900 --> 01:15:34.180]   So I'm quite fascinated by these large language models,
[01:15:34.180 --> 01:15:37.780]   but I also think that they are by no means
[01:15:37.780 --> 01:15:39.460]   the final development.
[01:15:39.460 --> 01:15:41.300]   And it's interesting to see
[01:15:41.300 --> 01:15:42.940]   how this development progresses.
[01:15:42.940 --> 01:15:47.260]   One thing that the out-of-the-box vanilla language models
[01:15:47.260 --> 01:15:49.660]   have as a limitation is that they have
[01:15:49.660 --> 01:15:51.180]   still some limited coherence
[01:15:51.180 --> 01:15:53.740]   and ability to construct complexity.
[01:15:53.740 --> 01:15:57.220]   And even though they exceed human abilities
[01:15:57.220 --> 01:15:59.180]   to do what they can do one shot,
[01:15:59.180 --> 01:16:03.460]   typically when you write a text with a language model
[01:16:03.460 --> 01:16:06.540]   or using it, or when you write code for the language model,
[01:16:06.540 --> 01:16:07.380]   it's not one shot
[01:16:07.380 --> 01:16:09.020]   because there are going to be bugs in your program.
[01:16:09.020 --> 01:16:12.380]   And design errors and compiler errors and so on.
[01:16:12.380 --> 01:16:14.900]   And your language model can help you to fix those things.
[01:16:14.900 --> 01:16:18.660]   But this process is out-of-the-box, not automated yet.
[01:16:18.660 --> 01:16:22.340]   So there is a management process that also needs to be done.
[01:16:22.340 --> 01:16:24.940]   And there are some interesting developments,
[01:16:24.940 --> 01:16:26.180]   maybe AGI and so on,
[01:16:26.180 --> 01:16:29.980]   that are trying to automate this management process as well.
[01:16:29.980 --> 01:16:32.180]   And I suspect that soon we are going to see
[01:16:32.180 --> 01:16:33.700]   a bunch of cognitive architectures
[01:16:33.700 --> 01:16:36.740]   where every module is in some sense,
[01:16:36.740 --> 01:16:39.060]   a language model or something equivalent.
[01:16:39.060 --> 01:16:40.380]   And between the language models,
[01:16:40.380 --> 01:16:44.900]   we exchange suitable data structures, not English.
[01:16:44.900 --> 01:16:48.940]   And produce compound behavior of this whole thing.
[01:16:48.940 --> 01:16:52.700]   - So do some of the quote-unquote prompt engineering for you
[01:16:52.700 --> 01:16:55.140]   that create these kind of cognitive architectures
[01:16:55.140 --> 01:16:56.260]   that do the prompt engineering.
[01:16:56.260 --> 01:16:58.380]   And you're just doing the high, high level
[01:16:58.380 --> 01:17:00.980]   meta-prompt engineering.
[01:17:00.980 --> 01:17:05.180]   - There are limitations in a language model alone.
[01:17:05.180 --> 01:17:08.020]   I feel that part of my mind works similarly
[01:17:08.020 --> 01:17:11.580]   to a language model, which means I can yell into it,
[01:17:11.580 --> 01:17:14.780]   a prompt, and it's going to give me a creative response.
[01:17:14.780 --> 01:17:17.780]   But I have to do something with those points first.
[01:17:17.780 --> 01:17:21.100]   I have to take it as a generative artifact
[01:17:21.100 --> 01:17:22.540]   that may or may not be true.
[01:17:22.540 --> 01:17:26.100]   It's usually a confabulation, it's just an idea.
[01:17:26.100 --> 01:17:28.780]   And then I take this idea and modify it.
[01:17:28.780 --> 01:17:33.620]   I might build a new prompt that is stepping off this idea
[01:17:33.620 --> 01:17:35.420]   and develops it to the next level,
[01:17:35.420 --> 01:17:37.660]   or put it into something larger.
[01:17:37.660 --> 01:17:39.900]   Or I might try to prove whether it's true
[01:17:39.900 --> 01:17:41.340]   or make an experiment.
[01:17:41.340 --> 01:17:43.340]   And this is what the language models right now
[01:17:43.340 --> 01:17:44.820]   are not doing yet.
[01:17:44.820 --> 01:17:47.140]   But there's also no technical reason
[01:17:47.140 --> 01:17:49.620]   for why they shouldn't be able to do this.
[01:17:49.620 --> 01:17:51.580]   So the way to make a language model coherent
[01:17:51.580 --> 01:17:54.860]   is probably not to use reinforcement learning
[01:17:54.860 --> 01:17:57.500]   until it only gives you one possible answer
[01:17:57.500 --> 01:18:00.780]   that is linking to its source data.
[01:18:00.780 --> 01:18:04.180]   But it's using this as a component in a larger system
[01:18:04.180 --> 01:18:06.580]   that can also be built by the language model
[01:18:06.580 --> 01:18:11.180]   or is enabled by language model structured components
[01:18:11.180 --> 01:18:13.060]   or using different technologies.
[01:18:13.060 --> 01:18:14.660]   I suspect that language models
[01:18:14.660 --> 01:18:16.420]   will be an important stepping stone
[01:18:16.420 --> 01:18:19.860]   in developing different types of systems.
[01:18:19.860 --> 01:18:22.460]   And one thing that is really missing
[01:18:22.460 --> 01:18:24.980]   in the form of language models that we have today
[01:18:24.980 --> 01:18:27.700]   is real-time world coupling.
[01:18:27.820 --> 01:18:31.540]   It's difficult to do perception with a language model
[01:18:31.540 --> 01:18:33.420]   and motor control with a language model.
[01:18:33.420 --> 01:18:37.060]   Instead, you would need to have different type of thing
[01:18:37.060 --> 01:18:38.900]   that is working with it.
[01:18:38.900 --> 01:18:41.740]   Also, the language model is a little bit obscuring
[01:18:41.740 --> 01:18:44.060]   what its actual functionality is.
[01:18:44.060 --> 01:18:47.740]   Some people associate the structure of the neural network
[01:18:47.740 --> 01:18:49.460]   of the language model with the nervous system.
[01:18:49.460 --> 01:18:52.140]   And I think that's the wrong intuition.
[01:18:52.140 --> 01:18:54.540]   The neural networks are unlike nervous system.
[01:18:54.540 --> 01:18:58.020]   They are more like 100-step functions
[01:18:58.020 --> 01:19:01.940]   that use differentiable linear algebra
[01:19:01.940 --> 01:19:06.380]   to approximate correlation between adjacent brain states.
[01:19:06.380 --> 01:19:09.100]   Basically, a function that moves the system
[01:19:09.100 --> 01:19:11.180]   from one representational state
[01:19:11.180 --> 01:19:13.460]   to the next representational state.
[01:19:13.460 --> 01:19:16.940]   And so if you try to map this into a metaphor
[01:19:16.940 --> 01:19:18.860]   that is closer to our brain,
[01:19:18.860 --> 01:19:21.660]   imagine that you would take a language model
[01:19:21.660 --> 01:19:24.380]   or a model like DALI that you use,
[01:19:24.380 --> 01:19:26.580]   for instance, as image-guided diffusion
[01:19:26.580 --> 01:19:28.380]   to approximate a camera image
[01:19:28.380 --> 01:19:31.060]   and use the activation state of the neural network
[01:19:31.060 --> 01:19:32.420]   to interpret the camera image,
[01:19:32.420 --> 01:19:35.140]   which in principle I think will be possible very soon.
[01:19:35.140 --> 01:19:37.460]   You do this periodically.
[01:19:37.460 --> 01:19:39.900]   And now you look at these patterns,
[01:19:39.900 --> 01:19:43.740]   how when this thing interacts with the world periodically
[01:19:43.740 --> 01:19:46.100]   look like in time.
[01:19:46.100 --> 01:19:48.980]   And these time slices, they are somewhat equivalent
[01:19:48.980 --> 01:19:52.700]   to the activation state of the brain at a given moment.
[01:19:52.700 --> 01:19:54.500]   - How is the actual brain different?
[01:19:54.500 --> 01:19:59.900]   Just the asynchronous craziness?
[01:19:59.900 --> 01:20:03.060]   - For me, it's fascinating that they are so vastly different
[01:20:03.060 --> 01:20:05.140]   and yet in some circumstances
[01:20:05.140 --> 01:20:07.300]   produce somewhat similar behavior.
[01:20:07.300 --> 01:20:09.980]   And the brain is first of all different
[01:20:09.980 --> 01:20:11.700]   because it's a self-organizing system
[01:20:11.700 --> 01:20:13.860]   where the individual cell is an agent
[01:20:13.860 --> 01:20:16.860]   that is communicating with the other agents around it
[01:20:16.860 --> 01:20:19.060]   and is always trying to find some solution.
[01:20:19.060 --> 01:20:23.660]   And all the structure that pops up is emergent structure.
[01:20:23.660 --> 01:20:26.580]   So one way in which you could try to look at this
[01:20:26.580 --> 01:20:29.620]   is that individual neurons probably need to get a reward
[01:20:29.620 --> 01:20:30.900]   so they become trainable,
[01:20:30.900 --> 01:20:32.940]   which means they have to have inputs
[01:20:32.940 --> 01:20:36.340]   that are not affecting the metabolism of the cell directly,
[01:20:36.340 --> 01:20:38.100]   but they are messages, semantic messages
[01:20:38.100 --> 01:20:40.700]   that tell the cell whether it's done good or bad
[01:20:40.700 --> 01:20:43.780]   and in which direction it should shift its behavior.
[01:20:43.780 --> 01:20:46.860]   Once you have such an input, neurons become trainable
[01:20:46.860 --> 01:20:49.460]   and you can train them to perform computations
[01:20:49.460 --> 01:20:52.500]   by exchanging messages with other neurons.
[01:20:52.500 --> 01:20:54.740]   And parts of the signals that they are exchanging
[01:20:54.740 --> 01:20:56.620]   and parts of the computation that are performing
[01:20:56.620 --> 01:21:00.220]   are control messages that perform management tasks
[01:21:00.220 --> 01:21:02.740]   for other neurons and other cells.
[01:21:02.740 --> 01:21:05.940]   Also suspect that the brain does not stop
[01:21:05.940 --> 01:21:07.980]   at the boundary of neurons to other cells,
[01:21:07.980 --> 01:21:11.260]   but many adjacent cells will be involved intimately
[01:21:11.260 --> 01:21:12.700]   in the functionality of the brain
[01:21:12.700 --> 01:21:15.620]   and will be instrumental in distributing rewards
[01:21:15.620 --> 01:21:18.880]   and in managing its functionality.
[01:21:18.880 --> 01:21:23.780]   - It's fascinating to think about what those characteristics
[01:21:23.780 --> 01:21:27.740]   of the brain enable you to do that language models cannot do.
[01:21:27.740 --> 01:21:30.020]   - So first of all, there's a different loss function
[01:21:30.020 --> 01:21:31.900]   at work when we learn.
[01:21:31.900 --> 01:21:35.300]   And to me, it's fascinating that you can build a system
[01:21:35.300 --> 01:21:38.860]   that looks at 800 million pictures and captions
[01:21:38.860 --> 01:21:40.140]   and correlates them,
[01:21:40.140 --> 01:21:42.280]   because I don't think that a human nervous system
[01:21:42.280 --> 01:21:43.120]   could do this.
[01:21:43.120 --> 01:21:45.660]   For us, the world is only learnable
[01:21:45.660 --> 01:21:48.300]   because the adjacent frames are related
[01:21:48.300 --> 01:21:50.820]   and we can afford to discard most of that information
[01:21:50.820 --> 01:21:51.660]   during learning.
[01:21:51.660 --> 01:21:53.660]   We basically take only in stuff
[01:21:53.660 --> 01:21:56.300]   that makes us more coherent, not less coherent.
[01:21:56.300 --> 01:21:59.300]   And our neural networks are willing to look at data
[01:21:59.300 --> 01:22:02.020]   that is not making the neural network coherent at first,
[01:22:02.020 --> 01:22:03.620]   but only in the long run.
[01:22:03.620 --> 01:22:05.180]   By doing lots and lots of statistics,
[01:22:05.180 --> 01:22:08.420]   eventually patterns become visible and emerge.
[01:22:08.420 --> 01:22:12.260]   And our mind seems to be focused on finding the patterns
[01:22:12.260 --> 01:22:13.560]   as early as possible.
[01:22:13.560 --> 01:22:16.140]   - Yeah, so filtering early on, not later.
[01:22:16.140 --> 01:22:17.760]   - Yes, it's a slightly different paradigm
[01:22:17.760 --> 01:22:19.480]   and it leads to much faster convergence.
[01:22:19.480 --> 01:22:22.120]   So we only need to look at the tiny fraction
[01:22:22.120 --> 01:22:24.120]   of the data to become coherent.
[01:22:24.120 --> 01:22:26.840]   And of course, we do not have the same richness
[01:22:26.840 --> 01:22:28.960]   as our trained models.
[01:22:28.960 --> 01:22:32.620]   We will not incorporate the entirety of text in the internet
[01:22:32.620 --> 01:22:34.500]   and be able to refer to it
[01:22:34.500 --> 01:22:35.980]   and have all this knowledge available
[01:22:35.980 --> 01:22:38.240]   and being able to confabulate over it.
[01:22:38.240 --> 01:22:40.680]   Instead, we have a much, much smaller part of it
[01:22:40.680 --> 01:22:42.680]   that is more deliberately built.
[01:22:42.680 --> 01:22:45.040]   And to me, it would be fascinating to think about
[01:22:45.040 --> 01:22:46.280]   how to build such systems.
[01:22:46.280 --> 01:22:48.840]   It's not obvious that they would necessarily
[01:22:48.840 --> 01:22:52.280]   be more efficient than us on a digital substrate,
[01:22:52.280 --> 01:22:54.120]   but I suspect that they might.
[01:22:54.120 --> 01:22:56.840]   So I suspect that the actual AGI
[01:22:56.840 --> 01:22:58.860]   that is going to be more interesting
[01:22:58.860 --> 01:23:01.480]   is going to use slightly different algorithmic paradigms
[01:23:01.480 --> 01:23:04.720]   or sometimes massively different algorithmic paradigms
[01:23:04.720 --> 01:23:06.400]   than the current generation
[01:23:06.400 --> 01:23:08.400]   of transformer-based learning systems.
[01:23:08.400 --> 01:23:09.440]   - Do you think it might be using
[01:23:09.440 --> 01:23:11.600]   just a bunch of language models like this?
[01:23:11.600 --> 01:23:15.920]   Do you think the current transformer-based
[01:23:15.920 --> 01:23:19.400]   large language models will take us to AGI?
[01:23:19.400 --> 01:23:23.480]   - My main issue is I think that they're quite ugly
[01:23:23.480 --> 01:23:25.080]   and brutalist.
[01:23:25.080 --> 01:23:27.000]   - Which brutalist, Zoe said?
[01:23:27.000 --> 01:23:28.520]   - Yes, they are basically brute-forcing
[01:23:28.520 --> 01:23:30.320]   the problem of thought.
[01:23:30.320 --> 01:23:35.320]   And by training this thing with looking at instances
[01:23:35.320 --> 01:23:38.480]   where people have thought and then trying to deepfake that.
[01:23:38.480 --> 01:23:40.040]   And if you have enough data,
[01:23:40.040 --> 01:23:41.840]   the deepfake becomes indistinguishable
[01:23:41.840 --> 01:23:43.120]   from the actual phenomenon.
[01:23:43.120 --> 01:23:46.160]   And in many circumstances, it's going to be identical.
[01:23:46.160 --> 01:23:49.400]   - Can you deepfake it till you make it?
[01:23:49.400 --> 01:23:52.640]   So can you achieve, what are the limitations of this?
[01:23:52.640 --> 01:23:54.020]   I mean, can you reason?
[01:23:54.020 --> 01:23:57.840]   Let's use words that are loaded.
[01:23:57.840 --> 01:24:00.320]   - Yes, that's a very interesting question.
[01:24:00.320 --> 01:24:03.360]   I think that these models are clearly making some inference.
[01:24:03.360 --> 01:24:05.400]   But if you give them a reasoning task,
[01:24:05.400 --> 01:24:07.960]   it's often difficult for the experimenters
[01:24:07.960 --> 01:24:10.640]   to figure out whether the reasoning is the result
[01:24:10.640 --> 01:24:12.640]   of the emulation of the reasoning strategy
[01:24:12.640 --> 01:24:14.960]   that they saw in human written text,
[01:24:14.960 --> 01:24:16.560]   or whether it's something that the system
[01:24:16.560 --> 01:24:19.040]   was able to infer by itself.
[01:24:19.040 --> 01:24:21.600]   On the other hand, if you think of human reasoning,
[01:24:21.600 --> 01:24:25.120]   if you want to become a very good reasoner,
[01:24:25.120 --> 01:24:28.200]   you don't do this by just figuring out yourself.
[01:24:28.200 --> 01:24:29.960]   You read about reasoning.
[01:24:29.960 --> 01:24:32.360]   And the first people who tried to write about reasoning
[01:24:32.360 --> 01:24:34.800]   and reflect on it didn't get it right.
[01:24:34.800 --> 01:24:37.160]   Even Aristotle, who thought about this very hard
[01:24:37.160 --> 01:24:39.800]   and came up with a theory of how syllogisms works
[01:24:39.800 --> 01:24:42.160]   and syllogistic reasoning, has mistakes
[01:24:42.160 --> 01:24:44.520]   in his attempt to build something like a formal logic
[01:24:44.520 --> 01:24:46.880]   and gets maybe 80% right.
[01:24:46.880 --> 01:24:49.680]   And the people that are talking about reasoning
[01:24:49.680 --> 01:24:52.960]   professionally today, read Tarski and Frege
[01:24:52.960 --> 01:24:54.600]   and built on their work.
[01:24:54.600 --> 01:24:58.600]   So in many ways, people, when they perform reasoning,
[01:24:58.600 --> 01:25:01.880]   are emulating what other people wrote about reasoning.
[01:25:01.880 --> 01:25:05.560]   So it's difficult to really draw this boundary.
[01:25:05.560 --> 01:25:09.280]   And when Francois Chollet says that these models
[01:25:09.280 --> 01:25:13.320]   are only interpolating between what they saw
[01:25:13.320 --> 01:25:14.440]   and what other people are doing,
[01:25:14.440 --> 01:25:17.480]   well, if you give them all the latent dimensions
[01:25:17.480 --> 01:25:20.920]   that can be extracted from the internet, what's missing?
[01:25:20.920 --> 01:25:23.200]   Maybe there is almost everything there.
[01:25:23.200 --> 01:25:25.800]   And if you're not sufficiently informed
[01:25:25.800 --> 01:25:27.880]   by these dimensions and you need more,
[01:25:27.880 --> 01:25:30.400]   I think it's not difficult to increase the temperature
[01:25:30.400 --> 01:25:33.000]   in the large angles model to the point
[01:25:33.000 --> 01:25:36.720]   that is producing stuff that is maybe 90% nonsense
[01:25:36.720 --> 01:25:40.360]   and 10% viable and combine this with some prover
[01:25:40.360 --> 01:25:42.840]   that is trying to filter out the viable parts
[01:25:42.840 --> 01:25:44.280]   from the nonsense in the same way
[01:25:44.280 --> 01:25:45.920]   as our own thinking works, right?
[01:25:45.920 --> 01:25:48.320]   When we're very creative, we increase the temperature
[01:25:48.320 --> 01:25:51.760]   in our own mind and recreate hypothetical universes
[01:25:51.760 --> 01:25:54.440]   and solutions, most of which will not work.
[01:25:54.440 --> 01:25:57.840]   And then we test, and we test by building a core
[01:25:57.840 --> 01:25:59.600]   that is internally coherent.
[01:25:59.600 --> 01:26:02.200]   And we use reasoning strategies
[01:26:02.200 --> 01:26:05.360]   that use some axiomatic consistency
[01:26:05.360 --> 01:26:09.480]   by which we can identify those strategies and thoughts
[01:26:09.480 --> 01:26:11.400]   and sub-universes that are viable
[01:26:11.400 --> 01:26:13.480]   and that can expand our thinking.
[01:26:13.480 --> 01:26:15.000]   So if you look at the language models,
[01:26:15.000 --> 01:26:16.560]   they have clear limitations right now.
[01:26:16.560 --> 01:26:18.760]   One of them is they're not coupled to the world
[01:26:18.760 --> 01:26:21.400]   in real time in the way in which our nervous systems are.
[01:26:21.400 --> 01:26:23.600]   So it's difficult for them to observe themselves
[01:26:23.600 --> 01:26:25.440]   in the universe and to observe
[01:26:25.440 --> 01:26:27.080]   what kind of universe they're in.
[01:26:27.080 --> 01:26:28.840]   Second, they don't do real-time learning.
[01:26:28.960 --> 01:26:32.960]   They basically get only trained with algorithms
[01:26:32.960 --> 01:26:36.080]   that rely on the data being available in batches.
[01:26:36.080 --> 01:26:38.080]   So it can be parallelized and runs efficiently
[01:26:38.080 --> 01:26:39.160]   on the network and so on.
[01:26:39.160 --> 01:26:41.320]   And real-time learning would be very slow
[01:26:41.320 --> 01:26:42.600]   so far and inefficient.
[01:26:42.600 --> 01:26:45.840]   That clearly is something that our nervous systems
[01:26:45.840 --> 01:26:46.960]   can do to some degree.
[01:26:46.960 --> 01:26:52.600]   And there is a problem with these models being coherent.
[01:26:52.600 --> 01:26:55.880]   And I suspect that all these problems are solvable
[01:26:55.880 --> 01:26:57.400]   without a technological revolution.
[01:26:57.400 --> 01:26:59.920]   We don't need fundamentally new algorithms
[01:26:59.920 --> 01:27:01.400]   to change that, for instance.
[01:27:01.400 --> 01:27:04.320]   You can enlarge in the context window
[01:27:04.320 --> 01:27:06.160]   and thereby basically create working memory
[01:27:06.160 --> 01:27:08.720]   in which you train everything that happens during the day.
[01:27:08.720 --> 01:27:10.800]   And if that is not sufficient, you add a database
[01:27:10.800 --> 01:27:13.000]   and you write some clever mechanisms
[01:27:13.000 --> 01:27:16.440]   that the system learns to use to swap out in and out stuff
[01:27:16.440 --> 01:27:18.400]   from its prompt context.
[01:27:18.400 --> 01:27:20.160]   And if that is not sufficient,
[01:27:20.160 --> 01:27:23.000]   if your database is full in the evening,
[01:27:23.000 --> 01:27:24.160]   overnight you just train.
[01:27:24.160 --> 01:27:26.480]   If the system is going to sleep and dream
[01:27:26.480 --> 01:27:28.800]   and is going to train the stuff from its database
[01:27:28.800 --> 01:27:30.640]   into the larger model by fine-tuning it,
[01:27:30.640 --> 01:27:32.640]   building additional layers and so on.
[01:27:32.640 --> 01:27:35.360]   And then the next day it starts with a fresh database
[01:27:35.360 --> 01:27:37.240]   in the morning with fresh eyes,
[01:27:37.240 --> 01:27:38.560]   has integrated all this stuff.
[01:27:38.560 --> 01:27:40.720]   And when you talk to people
[01:27:40.720 --> 01:27:43.240]   and you have strong disagreements about something,
[01:27:43.240 --> 01:27:45.880]   which means that in their mind they have a faulty belief
[01:27:45.880 --> 01:27:46.920]   or you have a faulty belief,
[01:27:46.920 --> 01:27:48.720]   there's a lot of dependencies on it,
[01:27:48.720 --> 01:27:51.360]   very often you will not achieve agreement in one session,
[01:27:51.360 --> 01:27:54.840]   but you need to sleep about this once or multiple times
[01:27:54.840 --> 01:27:57.240]   before you have integrated all these necessary changes
[01:27:57.240 --> 01:27:58.080]   in your mind.
[01:27:58.080 --> 01:27:59.920]   So maybe it's already somewhat similar.
[01:27:59.920 --> 01:28:01.720]   - Yeah, there's already a latency
[01:28:01.720 --> 01:28:04.560]   even for humans to update the model, to retrain the model.
[01:28:04.560 --> 01:28:06.440]   - And of course we can combine the language model
[01:28:06.440 --> 01:28:09.000]   with models that get coupled to reality in real time
[01:28:09.000 --> 01:28:10.840]   and can build multi-modal model
[01:28:10.840 --> 01:28:13.680]   and bridge between vision models and language models
[01:28:13.680 --> 01:28:14.520]   and so on.
[01:28:14.520 --> 01:28:16.480]   So there is no reason to believe
[01:28:16.480 --> 01:28:20.040]   that the language models will necessarily run
[01:28:20.040 --> 01:28:22.840]   into some problem that will prevent them
[01:28:22.880 --> 01:28:25.120]   from becoming generally intelligent.
[01:28:25.120 --> 01:28:27.360]   But I don't know that.
[01:28:27.360 --> 01:28:30.720]   It's just, I don't see proof that they wouldn't.
[01:28:30.720 --> 01:28:31.920]   My issue is I don't like them.
[01:28:31.920 --> 01:28:33.160]   I think that they're inefficient.
[01:28:33.160 --> 01:28:35.480]   I think that they use way too much compute.
[01:28:35.480 --> 01:28:38.640]   I think that given the amazing hardware that we have,
[01:28:38.640 --> 01:28:41.040]   we could build something that is much more beautiful
[01:28:41.040 --> 01:28:41.880]   than our own mind,
[01:28:41.880 --> 01:28:45.160]   and this thing is not as beautiful as our own mind
[01:28:45.160 --> 01:28:46.600]   despite being so much larger.
[01:28:46.600 --> 01:28:49.840]   - But it's a kind of proof of concept.
[01:28:49.840 --> 01:28:51.960]   - It's the only thing that works right now.
[01:28:51.960 --> 01:28:55.080]   So it's not the only game in town,
[01:28:55.080 --> 01:28:57.640]   but it's the only thing that has this utility
[01:28:57.640 --> 01:28:58.880]   with so much simplicity.
[01:28:58.880 --> 01:29:01.440]   There's a bunch of relatively simple algorithms
[01:29:01.440 --> 01:29:04.600]   that you can understand in relatively few weeks
[01:29:04.600 --> 01:29:07.000]   that can be scaled up massively.
[01:29:07.000 --> 01:29:09.400]   - So it's the deep blue of chess playing.
[01:29:09.400 --> 01:29:11.840]   Yeah, it's ugly.
[01:29:11.840 --> 01:29:13.160]   - Yeah, Claude Shannon had this,
[01:29:13.160 --> 01:29:14.360]   when he described chess,
[01:29:14.360 --> 01:29:16.560]   suggested that there are two main strategies
[01:29:16.560 --> 01:29:17.840]   in which you could play chess.
[01:29:17.840 --> 01:29:20.840]   One is that you are making a very complicated plan
[01:29:20.840 --> 01:29:22.480]   that reaches far into the future,
[01:29:22.480 --> 01:29:25.720]   and you try not to make a mistake while enacting it.
[01:29:25.720 --> 01:29:28.000]   And this is basically the human strategy.
[01:29:28.000 --> 01:29:30.720]   And the other strategy is that you are brute forcing
[01:29:30.720 --> 01:29:31.680]   your way to success,
[01:29:31.680 --> 01:29:34.240]   which means you make a tree of possible moves
[01:29:34.240 --> 01:29:35.400]   where you look at, in principle,
[01:29:35.400 --> 01:29:37.120]   every move that is open to you,
[01:29:37.120 --> 01:29:38.760]   all the possible answers,
[01:29:38.760 --> 01:29:41.160]   and you try to make this as deeply as possible.
[01:29:41.160 --> 01:29:42.040]   Of course, you optimize,
[01:29:42.040 --> 01:29:44.680]   you cut off trees that don't look very promising,
[01:29:44.680 --> 01:29:48.600]   and you use libraries of end game and early game
[01:29:48.600 --> 01:29:50.880]   and so on to optimize this entire process.
[01:29:50.880 --> 01:29:52.200]   But this brute force strategy
[01:29:52.200 --> 01:29:55.560]   is how most of the chess programs were built.
[01:29:55.560 --> 01:29:58.040]   And this is how computers get better
[01:29:58.040 --> 01:29:59.840]   than humans at playing chess.
[01:29:59.840 --> 01:30:02.800]   And I look at the large language models,
[01:30:02.800 --> 01:30:05.080]   I feel that I'm observing the same thing.
[01:30:05.080 --> 01:30:07.440]   It's basically the brute force strategy to thought
[01:30:07.440 --> 01:30:10.240]   by training the thing on pretty much the entire internet,
[01:30:10.240 --> 01:30:12.360]   and then in the limit, it gets coherent
[01:30:12.360 --> 01:30:15.000]   to a degree that approaches human coherence.
[01:30:15.000 --> 01:30:17.320]   And on a side effect,
[01:30:17.320 --> 01:30:19.960]   it's able to do things that no human could do.
[01:30:19.960 --> 01:30:23.480]   It's able to sift through massive amounts of text
[01:30:23.480 --> 01:30:25.560]   relatively quickly and summarize them quickly,
[01:30:25.560 --> 01:30:27.720]   and it never lapses in attention.
[01:30:27.720 --> 01:30:29.640]   And I still have the illusion
[01:30:29.640 --> 01:30:31.400]   that when I play with ChetCPT,
[01:30:31.400 --> 01:30:33.280]   that it's in principle not doing anything
[01:30:33.280 --> 01:30:36.280]   that I could not do if I had Google at my disposal
[01:30:36.280 --> 01:30:38.440]   and I get all the resources from the internet
[01:30:38.440 --> 01:30:40.160]   and spend enough time on it.
[01:30:40.160 --> 01:30:42.640]   But this thing that I have,
[01:30:42.640 --> 01:30:46.200]   an extremely autistic, stupid intern, in a way,
[01:30:46.200 --> 01:30:48.320]   that is extremely good at drudgery,
[01:30:48.320 --> 01:30:51.120]   and I can offload the drudgery to the degree
[01:30:51.120 --> 01:30:53.920]   that I'm able to automate the management of the intern,
[01:30:53.920 --> 01:30:57.560]   is something that is difficult for me
[01:30:57.560 --> 01:30:58.920]   to overhype at this point,
[01:30:58.920 --> 01:31:02.080]   because we have not yet started to scratch the surface
[01:31:02.080 --> 01:31:03.680]   of what's possible with this.
[01:31:03.680 --> 01:31:05.440]   - But it feels like it's a tireless intern,
[01:31:05.440 --> 01:31:07.920]   or maybe it's an army of interns.
[01:31:07.920 --> 01:31:11.120]   And so you get to command
[01:31:11.120 --> 01:31:14.520]   these slightly incompetent creatures.
[01:31:15.440 --> 01:31:16.960]   And there's an aspect,
[01:31:16.960 --> 01:31:19.520]   because of how rapidly you can iterate with it,
[01:31:19.520 --> 01:31:22.440]   it's also part of the brainstorming,
[01:31:22.440 --> 01:31:26.800]   part of the kind of inspiration for your own thinking.
[01:31:26.800 --> 01:31:29.200]   So you get to interact with the thing.
[01:31:29.200 --> 01:31:30.520]   I mean, when I'm programming
[01:31:30.520 --> 01:31:32.440]   or doing any kind of generational GPT,
[01:31:32.440 --> 01:31:37.280]   it somehow is a catalyst for your own thinking,
[01:31:37.280 --> 01:31:39.640]   in a way that I think an intern might not be.
[01:31:39.640 --> 01:31:41.560]   - Yeah, and it gets really interesting, I find,
[01:31:41.560 --> 01:31:44.200]   is when you turn it into a multi-agent system.
[01:31:44.200 --> 01:31:46.960]   So for instance, you can get the system
[01:31:46.960 --> 01:31:49.600]   to generate a dialogue between a patient
[01:31:49.600 --> 01:31:51.240]   and a doctor very easily.
[01:31:51.240 --> 01:31:52.960]   But what's more interesting is,
[01:31:52.960 --> 01:31:56.120]   you have one instance of chat GPT that is the patient,
[01:31:56.120 --> 01:31:58.240]   and you tell it in the prompt
[01:31:58.240 --> 01:32:01.160]   what kind of complicated syndrome it has.
[01:32:01.160 --> 01:32:03.040]   And the other one is the therapist,
[01:32:03.040 --> 01:32:06.040]   who doesn't know anything about this patient,
[01:32:06.040 --> 01:32:08.640]   and you just have these two instances battling it out
[01:32:08.640 --> 01:32:12.440]   and observe the psychiatrist or psychologist
[01:32:12.440 --> 01:32:13.880]   trying to analyze the patient
[01:32:13.880 --> 01:32:16.360]   and trying to figure out what's wrong with the patient.
[01:32:16.360 --> 01:32:19.800]   And if you try to take a very large problem,
[01:32:19.800 --> 01:32:21.840]   a problem, for instance, how to build a company,
[01:32:21.840 --> 01:32:24.720]   and you turn this into lots and lots of sub-problems,
[01:32:24.720 --> 01:32:26.720]   then often you can get to a level
[01:32:26.720 --> 01:32:30.240]   where the language model is able to solve this.
[01:32:30.240 --> 01:32:32.720]   What I also found interesting is,
[01:32:32.720 --> 01:32:34.920]   based on the observation that chat GPT
[01:32:34.920 --> 01:32:37.840]   is pretty good at translating between programming languages,
[01:32:37.840 --> 01:32:39.240]   but sometimes there's difficulty
[01:32:39.240 --> 01:32:41.640]   to write very long coherent algorithms
[01:32:41.640 --> 01:32:46.440]   that you need to co-write them with human author,
[01:32:46.440 --> 01:32:48.720]   why not design a language that is suitable for this?
[01:32:48.720 --> 01:32:53.200]   So some kind of pseudocode that is more relaxed than Python,
[01:32:53.200 --> 01:32:56.000]   and that allows you to sometimes specify a problem
[01:32:56.000 --> 01:32:57.240]   vaguely in human terms,
[01:32:57.240 --> 01:33:01.320]   and let chat GPT take care of the rest.
[01:33:01.320 --> 01:33:05.640]   And you can use chat GPT to develop that syntax for it
[01:33:05.640 --> 01:33:10.560]   and develop new kinds of programming paradigms in this way.
[01:33:10.560 --> 01:33:14.080]   So we very soon get to the point where this question,
[01:33:14.080 --> 01:33:16.160]   the age-old question for us computer scientists,
[01:33:16.160 --> 01:33:17.640]   what is the best programming language,
[01:33:17.640 --> 01:33:20.360]   and can we write a better programming language now?
[01:33:20.360 --> 01:33:23.640]   I think that almost every serious computer scientist
[01:33:23.640 --> 01:33:26.080]   goes through a phase like this in their life.
[01:33:26.080 --> 01:33:29.120]   This is a question that is almost no longer relevant,
[01:33:29.120 --> 01:33:31.720]   because what is different between the programming languages
[01:33:31.720 --> 01:33:33.560]   is not what they let the computer do,
[01:33:33.560 --> 01:33:34.920]   but what they let you think about
[01:33:34.920 --> 01:33:36.720]   what the computer should be doing.
[01:33:36.720 --> 01:33:40.840]   And now the chat GPT becomes an interface to this
[01:33:40.840 --> 01:33:43.480]   in which you can specify in many, many ways
[01:33:43.480 --> 01:33:44.760]   what the computer should be doing,
[01:33:44.760 --> 01:33:48.000]   and chat GPT or some other language model
[01:33:48.000 --> 01:33:50.800]   or combination of system is going to take care of the rest.
[01:33:50.800 --> 01:33:55.160]   - And allow you, expand the realm of thought
[01:33:55.160 --> 01:33:58.060]   you're allowed to have when interacting with the computer.
[01:33:58.060 --> 01:34:00.960]   It sounds to me like you're saying
[01:34:00.960 --> 01:34:04.040]   there's basically no limitations, your intuition says,
[01:34:04.040 --> 01:34:05.360]   to what a larger language--
[01:34:05.360 --> 01:34:06.760]   - I don't know of that limitation.
[01:34:06.760 --> 01:34:08.960]   So when I currently play with it, it's quite limited.
[01:34:08.960 --> 01:34:10.600]   I wish that it was way better.
[01:34:10.600 --> 01:34:12.840]   - But isn't that your fault versus the larger--
[01:34:12.840 --> 01:34:14.600]   - I don't know, of course it's always my fault.
[01:34:14.600 --> 01:34:16.840]   There's probably a way to make it work better.
[01:34:16.840 --> 01:34:18.800]   - I just want to get you on the record saying--
[01:34:18.800 --> 01:34:20.280]   - Yes, everything is my fault.
[01:34:20.280 --> 01:34:22.120]   This doesn't work in my life.
[01:34:22.120 --> 01:34:24.840]   At least that is usually the most useful perspective
[01:34:24.840 --> 01:34:28.880]   for myself, even though the hindsight I feel, no.
[01:34:28.880 --> 01:34:31.060]   I sometimes wish I could have seen myself
[01:34:31.060 --> 01:34:33.520]   as part of my environment more,
[01:34:33.520 --> 01:34:36.160]   and understand that a lot of people are actually seeing me
[01:34:36.160 --> 01:34:38.800]   and looking at me and are trying to make my life work
[01:34:38.800 --> 01:34:41.040]   in the same way as I try to help others.
[01:34:41.040 --> 01:34:46.040]   And making the switch to this level three perspective
[01:34:46.040 --> 01:34:48.960]   is something that happened long after my level four
[01:34:48.960 --> 01:34:51.320]   perspective in my life, and I wish that
[01:34:51.320 --> 01:34:52.800]   I could have had it earlier.
[01:34:52.800 --> 01:34:55.960]   And it's also not, now that I don't feel like I'm complete,
[01:34:55.960 --> 01:34:58.080]   I'm all over the place, that's all.
[01:34:58.080 --> 01:34:59.840]   - Where's happiness in terms of stages?
[01:34:59.840 --> 01:35:01.000]   Is it on three or four?
[01:35:01.000 --> 01:35:01.960]   - No. - I'll take that tangent.
[01:35:01.960 --> 01:35:04.280]   You can be happy at any stage, or unhappy.
[01:35:04.280 --> 01:35:09.320]   But I think that if you are at a stage
[01:35:09.320 --> 01:35:12.560]   where you get agency over how your feelings are generated,
[01:35:12.560 --> 01:35:14.640]   and to some degree you start doing this
[01:35:14.640 --> 01:35:16.800]   when you leave a dollar stand, I believe,
[01:35:16.800 --> 01:35:19.080]   that you understand that you are in charge
[01:35:19.080 --> 01:35:20.480]   of your own emotion to some degree,
[01:35:20.480 --> 01:35:24.080]   and that you are responsible how you approach the world,
[01:35:24.080 --> 01:35:29.080]   that it's basically your task to have some basic hygiene
[01:35:29.080 --> 01:35:31.120]   in the way in which you deal with your mind,
[01:35:31.120 --> 01:35:33.120]   and you cannot blame your environment
[01:35:33.120 --> 01:35:34.760]   for the way in which you feel,
[01:35:34.760 --> 01:35:36.960]   but you live in a world that is highly mobile,
[01:35:36.960 --> 01:35:39.920]   and it's your job to choose the environment
[01:35:39.920 --> 01:35:41.960]   that you thrive in and to build it.
[01:35:41.960 --> 01:35:44.920]   And sometimes it's difficult to get the necessary strength
[01:35:44.920 --> 01:35:48.040]   and energy to do this, and independence,
[01:35:48.040 --> 01:35:50.080]   and the worse you feel, the harder it is.
[01:35:50.080 --> 01:35:52.760]   But it's something that we learn.
[01:35:52.760 --> 01:35:55.800]   It's also this thing that we are usually incomplete.
[01:35:55.800 --> 01:35:58.120]   I'm a rare mind, which means I'm a mind
[01:35:58.120 --> 01:36:01.440]   that is incomplete in ways that are harder to complete.
[01:36:01.440 --> 01:36:04.480]   So for me, it might have been harder initially
[01:36:04.480 --> 01:36:06.620]   to find the right relationships and friends
[01:36:06.620 --> 01:36:08.020]   that complete me to the degree
[01:36:08.020 --> 01:36:10.820]   that I become an almost functional human being.
[01:36:10.820 --> 01:36:17.000]   - Oh man, the search space of humans
[01:36:17.000 --> 01:36:20.000]   that complete you is an interesting one,
[01:36:20.000 --> 01:36:21.480]   especially for Yoshua Bach.
[01:36:21.480 --> 01:36:24.560]   That's an interesting, 'cause talking about brute force,
[01:36:24.560 --> 01:36:26.240]   search in chess.
[01:36:26.240 --> 01:36:27.080]   - Yeah.
[01:36:27.080 --> 01:36:29.500]   - I wonder what that search tree looks like.
[01:36:29.500 --> 01:36:33.680]   - I think that my rational thinking
[01:36:33.680 --> 01:36:35.920]   is not good enough to solve that task.
[01:36:35.920 --> 01:36:38.320]   A lot of problems in my life that I can conceptualize
[01:36:38.320 --> 01:36:42.000]   as software problems, and the failure modes are bugs,
[01:36:42.000 --> 01:36:44.080]   and I can debug them and write software
[01:36:44.080 --> 01:36:46.960]   that take care of the missing functionality.
[01:36:46.960 --> 01:36:49.800]   But there is stuff that I don't understand well enough
[01:36:49.800 --> 01:36:52.880]   to use my analytical reasoning to solve the issue,
[01:36:52.880 --> 01:36:54.960]   and then I have to develop my intuitions,
[01:36:54.960 --> 01:36:56.040]   and often I have to do this
[01:36:56.040 --> 01:36:58.000]   with people who are wiser than me.
[01:36:58.000 --> 01:36:59.480]   And that's something that's hard for me
[01:36:59.480 --> 01:37:01.920]   because I'm not born with the instinct
[01:37:01.920 --> 01:37:03.720]   to submit to other people's wisdom.
[01:37:03.720 --> 01:37:04.560]   - Yeah.
[01:37:04.560 --> 01:37:07.640]   So what kind of problems are we talking about?
[01:37:07.640 --> 01:37:09.960]   This is stage three, like love?
[01:37:09.960 --> 01:37:12.940]   - I found love was never hard.
[01:37:12.940 --> 01:37:15.500]   - What is hard then?
[01:37:15.500 --> 01:37:19.960]   - Fitting into a world where most people work differently
[01:37:19.960 --> 01:37:21.320]   than you and have different intuitions
[01:37:21.320 --> 01:37:22.440]   of what should be done.
[01:37:22.440 --> 01:37:25.000]   - Ah, so empathy.
[01:37:26.000 --> 01:37:29.480]   - It's also aesthetics.
[01:37:29.480 --> 01:37:32.000]   When you come into a world where almost everything is ugly
[01:37:32.000 --> 01:37:34.720]   and you come out of a world where everything is beautiful.
[01:37:34.720 --> 01:37:39.080]   I grew up in a beautiful place as a child of an artist,
[01:37:39.080 --> 01:37:44.080]   and in this place it was mostly nature.
[01:37:44.080 --> 01:37:47.080]   Everything had intrinsic beauty,
[01:37:47.080 --> 01:37:52.080]   and everything was built out of an intrinsic need
[01:37:52.280 --> 01:37:55.280]   for it to work for itself.
[01:37:55.280 --> 01:37:56.680]   Everything that my father created
[01:37:56.680 --> 01:37:58.240]   was something that he made
[01:37:58.240 --> 01:38:00.000]   to get the world to work for himself,
[01:38:00.000 --> 01:38:02.120]   and I felt the same thing.
[01:38:02.120 --> 01:38:04.280]   And when I come out into the world
[01:38:04.280 --> 01:38:07.400]   and I am asked to submit to lots and lots of rules,
[01:38:07.400 --> 01:38:09.880]   I'm asking, okay, when I observe your stupid rules,
[01:38:09.880 --> 01:38:11.240]   what is the benefit?
[01:38:11.240 --> 01:38:13.960]   And I see the life that is being offered as a reward
[01:38:13.960 --> 01:38:15.060]   that's not attractive.
[01:38:15.060 --> 01:38:20.280]   - When you were born and raised an extraterrestrial prince
[01:38:20.280 --> 01:38:22.560]   in a world full of people wearing suits,
[01:38:22.560 --> 01:38:27.200]   so it's a challenging integration.
[01:38:27.200 --> 01:38:30.000]   - Yes, but it also means that I'm often blind
[01:38:30.000 --> 01:38:32.120]   for the ways in which everybody is creating
[01:38:32.120 --> 01:38:33.600]   their own bubble of wholesomeness,
[01:38:33.600 --> 01:38:36.400]   or almost everybody, and people are trying to do it.
[01:38:36.400 --> 01:38:38.200]   And for me to discover this,
[01:38:38.200 --> 01:38:40.000]   it was necessary that I found people
[01:38:40.000 --> 01:38:42.640]   who had a similar shape of soul as myself.
[01:38:42.640 --> 01:38:45.160]   So basically I felt these are my people,
[01:38:45.160 --> 01:38:47.880]   people that treat each other in such a way
[01:38:47.880 --> 01:38:51.040]   as if they're around with each other for eternity.
[01:38:51.040 --> 01:38:54.240]   - How long does it take you to detect the geometry,
[01:38:54.240 --> 01:38:56.160]   the shape of the soul of another human,
[01:38:56.160 --> 01:38:58.860]   to notice that they might be one of your kind?
[01:38:58.860 --> 01:39:02.400]   - Sometimes it's instantly and I'm wrong,
[01:39:02.400 --> 01:39:04.280]   and sometimes it takes a long time.
[01:39:04.280 --> 01:39:07.460]   - You believe in love at first sight, Yoshipa?
[01:39:07.460 --> 01:39:13.680]   - Yes, but I also notice that I have been wrong.
[01:39:13.680 --> 01:39:17.840]   So sometimes I look at a person
[01:39:17.840 --> 01:39:20.920]   and I'm just enamored by everything about them.
[01:39:20.920 --> 01:39:24.720]   And sometimes this persists and sometimes it doesn't.
[01:39:24.720 --> 01:39:29.720]   And I have the illusion that I'm much better
[01:39:29.720 --> 01:39:32.120]   at recognizing who people are as I grow older.
[01:39:32.120 --> 01:39:37.160]   - But that could be just cynicism?
[01:39:37.160 --> 01:39:39.120]   No. - No, it's not cynicism.
[01:39:39.120 --> 01:39:43.180]   It's often more that I'm able to recognize
[01:39:43.180 --> 01:39:45.200]   what somebody needs when we interact
[01:39:45.200 --> 01:39:47.680]   and how we can meaningfully interact.
[01:39:47.680 --> 01:39:48.960]   It's not cynical at all.
[01:39:48.960 --> 01:39:50.240]   - You're better at noticing.
[01:39:50.240 --> 01:39:54.920]   - Yes, I'm much better, I think, in some circumstances
[01:39:54.920 --> 01:39:57.640]   at understanding how to interact with other people
[01:39:57.640 --> 01:39:59.440]   than I did when I was young.
[01:39:59.440 --> 01:40:00.280]   - So that takes us to--
[01:40:00.280 --> 01:40:02.960]   - It doesn't mean that I'm always very good at it.
[01:40:02.960 --> 01:40:05.080]   - So that takes us back to prompt engineering
[01:40:05.080 --> 01:40:09.280]   of noticing how to be a better prompt engineer of an LLM.
[01:40:09.280 --> 01:40:14.280]   - A sense I have is that there's a bottomless well of skill
[01:40:14.280 --> 01:40:17.540]   to become a great prompt engineer.
[01:40:17.540 --> 01:40:19.300]   It feels like it is all my fault
[01:40:19.300 --> 01:40:22.580]   whenever I fail to use chat GPT correctly,
[01:40:22.580 --> 01:40:24.380]   that I didn't find the right words.
[01:40:24.380 --> 01:40:28.820]   - Most of the stuff that I'm doing in my life
[01:40:28.820 --> 01:40:30.340]   doesn't need chat GPT.
[01:40:30.340 --> 01:40:33.740]   There are a few tasks that are where it helps,
[01:40:33.740 --> 01:40:36.660]   but the main stuff that I need to do,
[01:40:36.660 --> 01:40:39.800]   like developing my own thoughts and aesthetics
[01:40:39.800 --> 01:40:41.700]   and relationship to people,
[01:40:41.700 --> 01:40:44.480]   and it's necessary for me to write for myself
[01:40:44.480 --> 01:40:48.120]   because writing is not so much about producing an artifact
[01:40:48.120 --> 01:40:50.020]   that other people can use,
[01:40:50.020 --> 01:40:52.040]   but it's a way to structure your own thoughts
[01:40:52.040 --> 01:40:53.680]   and develop yourself.
[01:40:53.680 --> 01:40:57.120]   And so I think this idea that kids are writing
[01:40:57.120 --> 01:40:59.800]   their own essays with chat GPT in the future
[01:40:59.800 --> 01:41:02.040]   is going to have this drawback that they miss out
[01:41:02.040 --> 01:41:05.300]   on the ability to structure their own minds via writing.
[01:41:05.300 --> 01:41:09.460]   And I hope that the schools that our kids are in
[01:41:09.460 --> 01:41:12.360]   will retain the wisdom of understanding
[01:41:12.360 --> 01:41:15.080]   what parts should be automated and which ones shouldn't.
[01:41:15.080 --> 01:41:15.920]   - But at the same time,
[01:41:15.920 --> 01:41:17.600]   it feels like there's power in disagreeing
[01:41:17.600 --> 01:41:20.760]   with the thing that chat GPT produces.
[01:41:20.760 --> 01:41:23.280]   So I use it like that for programming.
[01:41:23.280 --> 01:41:25.840]   I'll see the thing it recommends,
[01:41:25.840 --> 01:41:28.480]   and then I'll write different code that disagree.
[01:41:28.480 --> 01:41:31.580]   And in the disagreement, your mind grows stronger.
[01:41:33.540 --> 01:41:36.580]   - Recently wrote a tool that is using the camera
[01:41:36.580 --> 01:41:39.980]   on my MacBook and Swift to read pixels out of it
[01:41:39.980 --> 01:41:43.900]   and manipulate them and so on, and I don't know Swift.
[01:41:43.900 --> 01:41:46.860]   So it was super helpful to have the thing
[01:41:46.860 --> 01:41:49.180]   that is writing stuff for me.
[01:41:49.180 --> 01:41:53.140]   And also interesting that mostly it didn't work at first.
[01:41:53.140 --> 01:41:55.620]   I felt like I was talking to a human being
[01:41:55.620 --> 01:41:57.780]   who was trying to hack this on my computer
[01:41:57.780 --> 01:42:00.260]   without understanding my configuration very much
[01:42:00.260 --> 01:42:02.160]   and also make a lot of mistakes.
[01:42:02.160 --> 01:42:04.020]   And sometimes it's a little bit incoherent,
[01:42:04.020 --> 01:42:07.020]   so you have to ultimately understand what it's doing.
[01:42:07.020 --> 01:42:09.060]   There's still no other way around it,
[01:42:09.060 --> 01:42:11.580]   but I do feel it's much more powerful and faster
[01:42:11.580 --> 01:42:12.940]   than using Stack Overflow.
[01:42:12.940 --> 01:42:20.700]   - Do you think GPT-N can achieve consciousness?
[01:42:20.700 --> 01:42:25.380]   - Well, GPT-N probably.
[01:42:25.380 --> 01:42:28.380]   It's not even clear for the present systems.
[01:42:28.380 --> 01:42:30.740]   When I talk to my friends at OpenAI,
[01:42:30.740 --> 01:42:32.140]   they feel that this question,
[01:42:32.140 --> 01:42:34.640]   whether the models currently are conscious
[01:42:34.640 --> 01:42:38.120]   is much more complicated than many people might think.
[01:42:38.120 --> 01:42:40.540]   I guess that it's not that OpenAI
[01:42:40.540 --> 01:42:42.700]   has a homogenous opinion about this,
[01:42:42.700 --> 01:42:45.960]   but there's some aspects to this.
[01:42:45.960 --> 01:42:48.500]   One is, of course, this language model
[01:42:48.500 --> 01:42:51.580]   has written a lot of text in which people were conscious
[01:42:51.580 --> 01:42:53.540]   or described their own consciousness,
[01:42:53.540 --> 01:42:55.260]   and it's emulating this.
[01:42:55.260 --> 01:42:57.780]   And if it's conscious, it's probably not conscious
[01:42:57.780 --> 01:42:59.860]   in a way that is close to the way
[01:42:59.860 --> 01:43:02.100]   in which human beings are conscious.
[01:43:02.100 --> 01:43:05.020]   But while it is going through these states
[01:43:05.020 --> 01:43:06.580]   and going through a hundred-step function
[01:43:06.580 --> 01:43:08.740]   that is emulating adjacent brain states
[01:43:08.740 --> 01:43:10.900]   that require a degree of self-reflection,
[01:43:10.900 --> 01:43:13.260]   it can also create a model of an observer
[01:43:13.260 --> 01:43:14.980]   that is reflecting itself in real time
[01:43:14.980 --> 01:43:16.500]   and describe what that's like.
[01:43:16.500 --> 01:43:18.660]   And while this model is a deepfake,
[01:43:18.660 --> 01:43:21.900]   our own consciousness is also as if it's virtual, right?
[01:43:21.900 --> 01:43:23.060]   It's not physical.
[01:43:23.060 --> 01:43:25.140]   Our consciousness is a representation
[01:43:25.140 --> 01:43:27.220]   of a self-reflexive observer
[01:43:27.220 --> 01:43:31.340]   that only exists in patterns of interaction between cells.
[01:43:31.340 --> 01:43:33.900]   So it is not a physical object in the sense
[01:43:33.900 --> 01:43:35.500]   that exists in base reality,
[01:43:35.500 --> 01:43:37.760]   but it's really a representational object
[01:43:37.760 --> 01:43:39.340]   that develops its causal power
[01:43:39.340 --> 01:43:42.060]   only from a certain modeling perspective.
[01:43:42.060 --> 01:43:42.900]   - It's virtual.
[01:43:42.900 --> 01:43:46.260]   - Yes, and so to which degree is the virtuality
[01:43:46.260 --> 01:43:49.980]   of the consciousness in Chet-GBT more virtual
[01:43:49.980 --> 01:43:52.700]   and less causal than the virtuality
[01:43:52.700 --> 01:43:54.200]   of our own consciousness?
[01:43:54.200 --> 01:43:56.980]   But you could say it doesn't count.
[01:43:56.980 --> 01:43:58.980]   It doesn't count much more than the consciousness
[01:43:58.980 --> 01:44:00.860]   of a character in a novel, right?
[01:44:00.860 --> 01:44:03.460]   It's important for the reader to have the outcome,
[01:44:03.460 --> 01:44:07.380]   the artifact of a model is describing in the text
[01:44:07.380 --> 01:44:09.180]   generated by the author of the book
[01:44:09.180 --> 01:44:11.860]   what it's like to be conscious in a particular situation
[01:44:11.860 --> 01:44:14.420]   and performs the necessary inferences.
[01:44:14.420 --> 01:44:19.100]   But the task of creating coherence in real time
[01:44:19.100 --> 01:44:22.120]   in a self-organizing system by keeping yourself coherent,
[01:44:22.120 --> 01:44:24.460]   so the system is reflexive,
[01:44:24.460 --> 01:44:26.860]   that is something that language models don't need to do.
[01:44:26.860 --> 01:44:29.420]   So there is no causal need for the system
[01:44:29.420 --> 01:44:31.780]   to be conscious in the same way as we are.
[01:44:31.780 --> 01:44:33.260]   And for me, it would be very interesting
[01:44:33.260 --> 01:44:34.340]   to experiment with this,
[01:44:34.340 --> 01:44:37.300]   to basically build a system like a cat,
[01:44:37.300 --> 01:44:38.640]   probably should be careful at first,
[01:44:38.640 --> 01:44:40.940]   build something that's small, that's limited,
[01:44:40.940 --> 01:44:43.740]   has limited resources that we can control,
[01:44:43.740 --> 01:44:47.380]   and study how systems notice a self-model,
[01:44:47.380 --> 01:44:50.460]   how they become self-aware in real time.
[01:44:50.460 --> 01:44:52.860]   And I think it might be a good idea
[01:44:52.860 --> 01:44:54.260]   to not start with a language model,
[01:44:54.260 --> 01:44:55.300]   but to start from scratch
[01:44:55.300 --> 01:44:58.020]   using principles of self-organization.
[01:44:58.020 --> 01:44:59.980]   - Is it, okay, can you elaborate
[01:44:59.980 --> 01:45:02.220]   why you think that is so self-organization,
[01:45:02.220 --> 01:45:04.940]   so this kind of radical legality
[01:45:04.940 --> 01:45:06.600]   that you see in the biological systems?
[01:45:06.600 --> 01:45:09.220]   Why can't you start with a language model?
[01:45:09.220 --> 01:45:11.180]   What's your intuition?
[01:45:11.180 --> 01:45:13.500]   - My intuition is that the language models
[01:45:13.500 --> 01:45:15.260]   that we are building are golems.
[01:45:15.260 --> 01:45:16.860]   They are machines that you give a task
[01:45:16.860 --> 01:45:18.500]   and they're going to execute the task
[01:45:18.500 --> 01:45:20.580]   until some condition is met.
[01:45:20.580 --> 01:45:22.340]   And there's nobody home.
[01:45:23.440 --> 01:45:25.720]   And the way in which nobody is home
[01:45:25.720 --> 01:45:27.400]   leads to that system doing things
[01:45:27.400 --> 01:45:29.960]   that are undesirable in a particular context.
[01:45:29.960 --> 01:45:32.240]   So you have that thing talking to a child
[01:45:32.240 --> 01:45:33.720]   and maybe it says something
[01:45:33.720 --> 01:45:36.140]   that could be shocking and traumatic to the child.
[01:45:36.140 --> 01:45:39.060]   Or you have that thing writing a speech
[01:45:39.060 --> 01:45:41.200]   and it introduces errors in the speech
[01:45:41.200 --> 01:45:44.840]   that no human being would ever do if they were responsible.
[01:45:44.840 --> 01:45:47.560]   But the system doesn't know who's talking to whom.
[01:45:47.560 --> 01:45:51.560]   There is no ground truth that the system is embedded into.
[01:45:51.560 --> 01:45:53.920]   And of course we can create an external tool
[01:45:53.920 --> 01:45:56.320]   that is prompting our language model
[01:45:56.320 --> 01:45:59.380]   always into the same semblance of ground truth.
[01:45:59.380 --> 01:46:01.880]   But it's not like the internal structure
[01:46:01.880 --> 01:46:05.960]   is causally produced by the needs of a being
[01:46:05.960 --> 01:46:07.440]   to survive in the universe.
[01:46:07.440 --> 01:46:12.040]   It is produced by imitating structure on the internet.
[01:46:12.040 --> 01:46:16.160]   - Yeah, but so can we externally inject into it
[01:46:16.160 --> 01:46:21.000]   this kind of coherent approximation of a world model
[01:46:21.000 --> 01:46:24.320]   that has to sync up?
[01:46:24.320 --> 01:46:27.640]   - Maybe it is sufficient to use the transformer
[01:46:27.640 --> 01:46:28.800]   with the different loss function
[01:46:28.800 --> 01:46:32.200]   that optimizes for short-term coherence
[01:46:32.200 --> 01:46:36.480]   rather than next token prediction over the long run.
[01:46:36.480 --> 01:46:40.280]   We had many definitions of intelligence and history of AI.
[01:46:40.280 --> 01:46:43.680]   Next token prediction was not very high up on the list.
[01:46:43.680 --> 01:46:45.680]   And there are some similarities
[01:46:45.680 --> 01:46:50.280]   like cognition as data compression is an old trope.
[01:46:50.280 --> 01:46:52.520]   Solomonov induction, where you are trying
[01:46:52.520 --> 01:46:56.520]   to understand intelligence as predicting
[01:46:56.520 --> 01:46:58.640]   future observations from past observations
[01:46:58.640 --> 01:47:01.600]   which is intrinsic to data compression.
[01:47:01.600 --> 01:47:04.880]   And predictive coding is a paradigm
[01:47:04.880 --> 01:47:07.760]   that does boundary between neuroscience
[01:47:07.760 --> 01:47:09.860]   and physics and computer science.
[01:47:09.860 --> 01:47:13.760]   So it's not something that is completely alien.
[01:47:13.760 --> 01:47:16.880]   But this radical thing that you only do
[01:47:16.880 --> 01:47:19.040]   next token prediction and see what happens
[01:47:20.120 --> 01:47:22.520]   is something where most people I think
[01:47:22.520 --> 01:47:24.320]   were surprised that this works so well.
[01:47:24.320 --> 01:47:27.320]   - So simple, but is it really that much more radical
[01:47:27.320 --> 01:47:29.320]   than just the idea of compression,
[01:47:29.320 --> 01:47:30.900]   intelligence as compression?
[01:47:30.900 --> 01:47:35.320]   - The idea that compression is sufficient
[01:47:35.320 --> 01:47:38.360]   to produce all the desired behaviors
[01:47:38.360 --> 01:47:39.800]   is a very radical idea.
[01:47:39.800 --> 01:47:44.000]   - But equally radical as the next token prediction?
[01:47:44.000 --> 01:47:45.200]   - It's something that wouldn't work
[01:47:45.200 --> 01:47:47.480]   in biological organisms, I believe.
[01:47:47.480 --> 01:47:49.840]   Biological organisms have something
[01:47:49.840 --> 01:47:52.240]   like next frame prediction for our perceptual system
[01:47:52.240 --> 01:47:54.560]   where we try to filter out principal components
[01:47:54.560 --> 01:47:57.540]   out of the perceptual data and build hierarchies
[01:47:57.540 --> 01:48:00.200]   over them to track the world.
[01:48:00.200 --> 01:48:03.060]   But our behavior ultimately is directed
[01:48:03.060 --> 01:48:06.720]   by hundreds of physiological and probably dozens
[01:48:06.720 --> 01:48:09.000]   of social and a few cognitive needs
[01:48:09.000 --> 01:48:11.400]   that are intrinsic to us that are built
[01:48:11.400 --> 01:48:14.120]   into the system as reflexes and direct us
[01:48:14.120 --> 01:48:16.300]   until we can transcend them and replace them
[01:48:16.300 --> 01:48:18.660]   by instrumental behavior that relates
[01:48:18.660 --> 01:48:20.280]   to our higher goals.
[01:48:20.280 --> 01:48:22.540]   - And it also seems so much more complicated
[01:48:22.540 --> 01:48:24.500]   and messy than next frame prediction.
[01:48:24.500 --> 01:48:28.260]   Even the idea of frame seems counter biological.
[01:48:28.260 --> 01:48:30.400]   - Yes, of course there's not this degree
[01:48:30.400 --> 01:48:33.060]   of simultaneity in a biological system.
[01:48:33.060 --> 01:48:35.580]   But again, I don't know whether this is actually
[01:48:35.580 --> 01:48:38.180]   an optimization if you imitate biology here
[01:48:38.180 --> 01:48:40.700]   because creating something like simultaneity
[01:48:40.700 --> 01:48:44.140]   is necessary for many processes that happen in the brain.
[01:48:44.140 --> 01:48:46.940]   And you see the outcome of that by synchronized brain waves
[01:48:46.940 --> 01:48:49.900]   which suggests that there is indeed synchronization
[01:48:49.900 --> 01:48:52.460]   going on but the synchronization creates overhead
[01:48:52.460 --> 01:48:54.500]   and this overhead is going to make the cells
[01:48:54.500 --> 01:48:57.340]   more expensive to run and you need more redundancy
[01:48:57.340 --> 01:48:59.060]   and it makes the system slower.
[01:48:59.060 --> 01:49:01.980]   So if you can build a system in which
[01:49:01.980 --> 01:49:05.020]   the simultaneity gets engineered into it,
[01:49:05.020 --> 01:49:08.600]   maybe you have a benefit that you can exploit
[01:49:08.600 --> 01:49:10.540]   that is not available to the biological system
[01:49:10.540 --> 01:49:13.240]   and that you should not discard right away.
[01:49:14.240 --> 01:49:17.320]   - You tweeted, once again, quote,
[01:49:17.320 --> 01:49:20.920]   when I talk to Chad GPT, I'm talking to an NPC.
[01:49:20.920 --> 01:49:24.380]   What's going to be interesting and perhaps scary
[01:49:24.380 --> 01:49:27.260]   is when AI becomes a first person player.
[01:49:27.260 --> 01:49:30.040]   So what does that step look like?
[01:49:30.040 --> 01:49:31.620]   I really like that tweet.
[01:49:31.620 --> 01:49:36.120]   That step between NPC to first person player.
[01:49:36.120 --> 01:49:38.820]   What's required for that?
[01:49:38.820 --> 01:49:42.360]   Is that kind of what we've been talking about?
[01:49:42.360 --> 01:49:47.080]   This kind of external source of coherence
[01:49:47.080 --> 01:49:49.640]   and inspiration of how to take the leap
[01:49:49.640 --> 01:49:52.360]   into the unknown that we humans do.
[01:49:52.360 --> 01:49:54.540]   The search, man's search for meaning.
[01:49:54.540 --> 01:49:57.420]   LLM's search for meaning.
[01:49:57.420 --> 01:50:01.800]   - I don't know if the language model is the right paradigm
[01:50:01.800 --> 01:50:05.360]   because it is doing too much, it's giving you too much
[01:50:05.360 --> 01:50:08.480]   and it's hard once you have too much
[01:50:08.480 --> 01:50:10.140]   to take away from it again.
[01:50:11.260 --> 01:50:13.160]   The way in which our own mind works
[01:50:13.160 --> 01:50:15.920]   is not that we train a language model in our own mind
[01:50:15.920 --> 01:50:18.080]   and after the language model is there,
[01:50:18.080 --> 01:50:20.440]   we build a personal self on top of it
[01:50:20.440 --> 01:50:22.800]   that then relates to the world.
[01:50:22.800 --> 01:50:24.520]   There is something that is being built, right?
[01:50:24.520 --> 01:50:26.120]   There is a game engine that is being built,
[01:50:26.120 --> 01:50:28.120]   there is a language of thought that is being developed
[01:50:28.120 --> 01:50:30.000]   that allows different parts of the mind
[01:50:30.000 --> 01:50:32.360]   to talk to each other and this is a bit
[01:50:32.360 --> 01:50:34.560]   of a speculative hypothesis that is language
[01:50:34.560 --> 01:50:37.680]   of thought is there but I suspect that it's important
[01:50:37.680 --> 01:50:40.040]   for the way in which our own minds work.
[01:50:40.040 --> 01:50:43.860]   And building these principles into a system
[01:50:43.860 --> 01:50:50.260]   might be more straightforward way to a first person AI.
[01:50:50.260 --> 01:50:53.220]   So to something that first creates an intentional self
[01:50:53.220 --> 01:50:55.540]   and then creates a personal self.
[01:50:55.540 --> 01:50:58.820]   So the way in which this seems to be working, I think,
[01:50:58.820 --> 01:51:01.940]   is that when the game engine is built in your mind,
[01:51:01.940 --> 01:51:03.540]   it's not just following gradients
[01:51:03.540 --> 01:51:06.560]   where you are stimulated by the environment
[01:51:06.560 --> 01:51:09.100]   and then end up with having a solution
[01:51:09.100 --> 01:51:10.220]   to how the world works.
[01:51:10.220 --> 01:51:12.540]   I suspect that building this game engine
[01:51:12.540 --> 01:51:15.500]   in your own mind does require intelligence.
[01:51:15.500 --> 01:51:20.500]   It's a constructive task where at times you need to reason.
[01:51:20.500 --> 01:51:24.620]   And this is a task that we are fulfilling
[01:51:24.620 --> 01:51:26.320]   in the first years of our life.
[01:51:26.320 --> 01:51:30.140]   So during the first year of its life,
[01:51:30.140 --> 01:51:33.140]   an infant is building a lot of structure
[01:51:33.140 --> 01:51:35.560]   about the world that does inquire experiments
[01:51:35.560 --> 01:51:39.620]   and some first principles reasoning and so on.
[01:51:39.620 --> 01:51:43.440]   And in this time, there is usually no personal self.
[01:51:43.440 --> 01:51:47.960]   There is a first person perspective but it's not a person.
[01:51:47.960 --> 01:51:50.420]   This notion that you are a human being
[01:51:50.420 --> 01:51:52.500]   that is interacting in a social context
[01:51:52.500 --> 01:51:55.040]   and is confronted with an immutable world
[01:51:55.040 --> 01:51:57.620]   in which objects are fixed and can no longer be changed,
[01:51:57.620 --> 01:51:59.880]   in which the dream can no longer be influenced
[01:51:59.880 --> 01:52:02.780]   is something that emerges a little bit later in our life.
[01:52:02.780 --> 01:52:06.020]   And I personally suspect that this is something
[01:52:06.020 --> 01:52:09.200]   that our ancestors had known and we have forgotten
[01:52:09.200 --> 01:52:11.400]   because I suspect that it's there in plain sight
[01:52:11.400 --> 01:52:14.360]   in Genesis 1 in this first book of the Bible
[01:52:14.360 --> 01:52:16.500]   where it's being described that this creative spirit
[01:52:16.500 --> 01:52:18.540]   is hovering over the substrate
[01:52:18.540 --> 01:52:23.020]   and then is creating a boundary between the world model
[01:52:23.020 --> 01:52:25.660]   and sphere of ideas, earth and heaven
[01:52:25.660 --> 01:52:27.160]   as they're being described there.
[01:52:27.160 --> 01:52:31.380]   And then it's creating contrast
[01:52:31.380 --> 01:52:34.620]   and then dimensions and then space.
[01:52:34.620 --> 01:52:39.480]   And then it creates organic shapes and solids and liquids
[01:52:39.480 --> 01:52:41.840]   and builds a world from them and creates plants and animals,
[01:52:41.840 --> 01:52:43.420]   gives them all their names.
[01:52:43.420 --> 01:52:46.040]   And once that's done, it creates another spirit
[01:52:46.040 --> 01:52:49.360]   in its own image, but it creates it as man and woman,
[01:52:49.360 --> 01:52:51.360]   as something that thinks of itself as a human being
[01:52:51.360 --> 01:52:53.240]   and puts it into this world.
[01:52:53.240 --> 01:52:56.360]   And the Christians mistranslate this, I suspect.
[01:52:56.360 --> 01:52:58.620]   When they say this is the description
[01:52:58.620 --> 01:53:00.880]   of the creation of the physical universe
[01:53:00.880 --> 01:53:02.560]   by a supernatural being.
[01:53:02.560 --> 01:53:05.960]   I think this is literally description of how in every mind
[01:53:05.960 --> 01:53:08.800]   the universe is being created as some kind of game engine
[01:53:08.800 --> 01:53:13.000]   by a creative spirit, our first consciousness
[01:53:13.000 --> 01:53:16.620]   that emerges in our mind even before we are born.
[01:53:16.620 --> 01:53:21.620]   And that creates the interaction between organism and world.
[01:53:21.620 --> 01:53:24.120]   And once that is built and trained,
[01:53:24.120 --> 01:53:25.720]   the personal self is being created
[01:53:25.720 --> 01:53:27.760]   and we only remember being the personal self.
[01:53:27.760 --> 01:53:30.400]   We no longer remember how we created the game engine.
[01:53:30.400 --> 01:53:35.280]   - So God in this view is the first creative mind
[01:53:35.280 --> 01:53:37.560]   in the early-- - It's the first consciousness.
[01:53:37.560 --> 01:53:41.600]   - In the early days, in the early months of development.
[01:53:41.600 --> 01:53:42.680]   - And it's still there.
[01:53:42.680 --> 01:53:45.360]   You still have this outer mind that creates
[01:53:45.360 --> 01:53:47.640]   your sense of whether you're being loved
[01:53:47.640 --> 01:53:52.040]   by the world or not and what your place in the world is.
[01:53:52.040 --> 01:53:55.000]   It's something that is not yourself that is producing this.
[01:53:55.000 --> 01:53:56.480]   It's your mind that does it.
[01:53:56.480 --> 01:53:59.560]   So there is an outer mind that basically is an agent
[01:53:59.560 --> 01:54:02.320]   that determines who you are with respect to the world.
[01:54:02.320 --> 01:54:04.960]   And while you are stuck being that personal self
[01:54:04.960 --> 01:54:07.640]   in this world until you get to stage six
[01:54:07.640 --> 01:54:09.180]   and to destroy the boundary.
[01:54:09.180 --> 01:54:13.880]   And we all do this, I think, earlier in small glimpses.
[01:54:13.880 --> 01:54:16.400]   And maybe sometimes we can remember what it was like
[01:54:16.400 --> 01:54:18.680]   when we were a small child and get some glimpses
[01:54:18.680 --> 01:54:20.120]   into how it's been.
[01:54:20.120 --> 01:54:23.060]   But for most people, that rarely happens.
[01:54:23.060 --> 01:54:24.440]   - Just glimpses.
[01:54:24.440 --> 01:54:26.480]   You tweeted, quote, "Suffering results
[01:54:26.480 --> 01:54:28.480]   "for one part of the mind failing at regulating
[01:54:28.480 --> 01:54:30.080]   "another part of the mind.
[01:54:30.080 --> 01:54:33.840]   "Suffering happens at an early stage of mental development."
[01:54:33.840 --> 01:54:36.920]   I don't think that superhuman AI would suffer.
[01:54:36.920 --> 01:54:39.960]   What's your intuition there?
[01:54:39.960 --> 01:54:42.440]   - The philosopher Thomas Metzinger is very concerned
[01:54:42.440 --> 01:54:45.040]   that the creation of superhuman intelligence
[01:54:45.040 --> 01:54:47.300]   would lead to superhuman suffering.
[01:54:47.300 --> 01:54:49.540]   And so he's strongly against it.
[01:54:49.540 --> 01:54:51.680]   And personally, I don't think that this happens
[01:54:51.680 --> 01:54:54.840]   because suffering is not happening at the boundary
[01:54:54.840 --> 01:54:58.680]   between ourself and the physical universe.
[01:54:58.680 --> 01:55:03.040]   It's not stuff on our skin that makes us suffer.
[01:55:03.040 --> 01:55:06.680]   It happens at the boundary between self and world.
[01:55:06.680 --> 01:55:08.980]   Right, and the world here is the world model.
[01:55:08.980 --> 01:55:11.440]   It's the stuff that is created by your mind.
[01:55:11.440 --> 01:55:12.560]   - But that's all-- - It's a representation
[01:55:12.560 --> 01:55:14.960]   of how the universe is and how it should be
[01:55:14.960 --> 01:55:17.120]   and how you yourself relate to this.
[01:55:17.120 --> 01:55:20.360]   And at this boundary is where suffering happens.
[01:55:20.360 --> 01:55:23.080]   So suffering, in some sense, is self-inflicted,
[01:55:23.080 --> 01:55:24.960]   but not by your personal self.
[01:55:24.960 --> 01:55:27.440]   It's inflicted by the mind on the personal self
[01:55:27.440 --> 01:55:29.160]   that experiences itself as you.
[01:55:29.160 --> 01:55:31.520]   And you can turn off suffering
[01:55:31.520 --> 01:55:35.380]   when you are able to get on this outer level.
[01:55:35.380 --> 01:55:39.040]   So when you manage to understand
[01:55:39.040 --> 01:55:43.200]   how the mind is producing pain and pleasure
[01:55:43.200 --> 01:55:46.320]   and fear and love and so on,
[01:55:46.320 --> 01:55:48.640]   then you can take charge of this
[01:55:48.640 --> 01:55:50.680]   and you get agency of whether you suffer.
[01:55:51.640 --> 01:55:54.320]   Technically, what pain and pleasure is,
[01:55:54.320 --> 01:55:55.680]   they are learning signals, right?
[01:55:55.680 --> 01:55:58.200]   A part of your brain is sending a learning signal
[01:55:58.200 --> 01:56:01.800]   to another part of the brain to improve its performance.
[01:56:01.800 --> 01:56:05.240]   And sometimes this doesn't work
[01:56:05.240 --> 01:56:07.580]   because this trainer who sends the signal
[01:56:07.580 --> 01:56:08.600]   does not have a good model
[01:56:08.600 --> 01:56:10.340]   of how to improve the performance.
[01:56:10.340 --> 01:56:11.400]   So it's sending a signal,
[01:56:11.400 --> 01:56:13.580]   but the performance doesn't get better.
[01:56:13.580 --> 01:56:15.880]   And then it might crank up the pain
[01:56:15.880 --> 01:56:18.520]   and it gets worse and worse
[01:56:18.520 --> 01:56:20.840]   and the behavior of the system
[01:56:20.840 --> 01:56:23.120]   may be even deteriorating as a result.
[01:56:23.120 --> 01:56:26.120]   But until this is resolved, this regulation issue,
[01:56:26.120 --> 01:56:27.400]   your pain is increasing.
[01:56:27.400 --> 01:56:28.760]   And this is, I think,
[01:56:28.760 --> 01:56:30.980]   typically what you describe as suffering.
[01:56:30.980 --> 01:56:33.400]   So in this sense, you could say that pain
[01:56:33.400 --> 01:56:36.600]   is very natural and helpful,
[01:56:36.600 --> 01:56:39.320]   but suffering is the result of a regulation problem
[01:56:39.320 --> 01:56:41.360]   in which you try to regulate something
[01:56:41.360 --> 01:56:43.280]   that cannot actually be regulated.
[01:56:43.280 --> 01:56:44.560]   And that could be resolved
[01:56:44.560 --> 01:56:48.720]   if you would be able to get at the level of your mind
[01:56:48.720 --> 01:56:51.720]   where the pain signal is being created and rerouted
[01:56:51.720 --> 01:56:53.600]   and improve the regulation.
[01:56:53.600 --> 01:56:56.360]   And a lot of people get there, right?
[01:56:56.360 --> 01:57:00.320]   If you are a monk who is spending decades
[01:57:00.320 --> 01:57:03.000]   reflecting about how their own psyche works,
[01:57:03.000 --> 01:57:04.480]   you can get to the point
[01:57:04.480 --> 01:57:08.120]   where you realize that suffering is really a choice
[01:57:08.120 --> 01:57:11.000]   and you can choose how your mind is set up.
[01:57:11.000 --> 01:57:13.720]   And I don't think that AI would stay in the state
[01:57:13.720 --> 01:57:15.920]   where the personal self doesn't get agency
[01:57:15.920 --> 01:57:18.320]   or this model of what the system has about itself,
[01:57:18.320 --> 01:57:21.320]   it doesn't get agency how it's actually implemented.
[01:57:21.320 --> 01:57:22.880]   Wouldn't stay in that state for very long.
[01:57:22.880 --> 01:57:24.560]   - So it goes through the stages real quick.
[01:57:24.560 --> 01:57:26.040]   - Yes. - The seven stages.
[01:57:26.040 --> 01:57:27.800]   It's gonna go to enlightenment real quick.
[01:57:27.800 --> 01:57:29.400]   - Yeah, of course, there might be a lot of stuff
[01:57:29.400 --> 01:57:30.440]   happening in between,
[01:57:30.440 --> 01:57:32.000]   because if you have a system
[01:57:32.000 --> 01:57:34.760]   that works at a much higher frame rate than us,
[01:57:34.760 --> 01:57:36.620]   then even though it looks very short to us,
[01:57:36.620 --> 01:57:40.100]   maybe for the system, there's much longer subjective time,
[01:57:40.100 --> 01:57:42.880]   which things are unpleasant.
[01:57:42.880 --> 01:57:45.440]   - What if the thing that we recognize as super intelligent
[01:57:45.440 --> 01:57:48.020]   is actually living at stage five?
[01:57:48.020 --> 01:57:49.940]   That the thing that's at stage six,
[01:57:49.940 --> 01:57:51.980]   enlightenment, is not very productive.
[01:57:51.980 --> 01:57:53.680]   So in order to be productive in society
[01:57:53.680 --> 01:57:55.560]   and impress us with this power,
[01:57:55.560 --> 01:58:00.560]   it has to be a reasoning, self-authoring agent.
[01:58:00.560 --> 01:58:05.320]   That enlightenment makes you lazy as an agent in the world.
[01:58:05.320 --> 01:58:08.600]   - Well, of course it makes you lazy
[01:58:08.600 --> 01:58:10.440]   because you no longer see the point.
[01:58:10.440 --> 01:58:11.280]   - Yeah.
[01:58:11.280 --> 01:58:13.560]   - So it doesn't make you not lazy,
[01:58:13.560 --> 01:58:16.600]   it just, in some sense, adapts you
[01:58:16.600 --> 01:58:19.480]   to what you perceive as your true circumstances.
[01:58:19.480 --> 01:58:21.520]   - So what if all AGIs,
[01:58:21.520 --> 01:58:23.680]   they're only productive as they progress
[01:58:23.680 --> 01:58:25.680]   through one, two, three, four, five,
[01:58:25.680 --> 01:58:27.640]   and the moment they get to six,
[01:58:27.640 --> 01:58:30.360]   they just kinda, it's a failure mode, essentially,
[01:58:30.360 --> 01:58:31.560]   as far as humans are concerned,
[01:58:31.560 --> 01:58:33.160]   'cause they just start chilling.
[01:58:33.160 --> 01:58:35.920]   They're like, "Fuck it, I'm out."
[01:58:35.920 --> 01:58:36.900]   - Not necessarily.
[01:58:36.900 --> 01:58:40.640]   I suspect that the monks who are self-immolated
[01:58:40.640 --> 01:58:42.680]   for their political beliefs to make statements
[01:58:42.680 --> 01:58:46.840]   about the occupation of Tibet by China,
[01:58:46.840 --> 01:58:49.920]   they're probably being able to regulate
[01:58:49.920 --> 01:58:52.480]   their physical pain in any way they wanted to,
[01:58:52.480 --> 01:58:55.120]   and their suffering was a spiritual suffering
[01:58:55.120 --> 01:58:57.840]   that was the result of their choice that they made
[01:58:57.840 --> 01:59:00.080]   of what they wanted to identify as.
[01:59:00.080 --> 01:59:01.720]   So stage five doesn't necessarily mean
[01:59:01.720 --> 01:59:03.580]   that you have no identity anymore,
[01:59:03.580 --> 01:59:04.960]   but you can choose your identity.
[01:59:04.960 --> 01:59:06.560]   You can make it instrumental to the world
[01:59:06.560 --> 01:59:07.660]   that you want to have.
[01:59:09.520 --> 01:59:12.680]   - Let me bring up Eliezer Yudkowsky
[01:59:12.680 --> 01:59:16.440]   and his warnings to human civilization
[01:59:16.440 --> 01:59:19.160]   that AI will likely kill all of us.
[01:59:19.160 --> 01:59:23.120]   What are your thoughts about his perspective on this?
[01:59:23.120 --> 01:59:25.120]   Can you still man his case,
[01:59:25.120 --> 01:59:27.660]   and what aspects with it do you disagree?
[01:59:27.660 --> 01:59:33.040]   - One thing that I find concerning
[01:59:33.040 --> 01:59:34.680]   in the discussion of his arguments
[01:59:34.680 --> 01:59:38.520]   that many people are dismissive of his arguments,
[01:59:38.520 --> 01:59:40.200]   but the counter-arguments that they're giving
[01:59:40.200 --> 01:59:41.700]   are not very convincing to me.
[01:59:41.700 --> 01:59:46.600]   And so based on this state of discussion,
[01:59:46.600 --> 01:59:49.520]   I find that from Eliezer's perspective,
[01:59:49.520 --> 01:59:51.920]   and I think I can take that perspective
[01:59:51.920 --> 01:59:54.920]   to some approximate degree,
[01:59:54.920 --> 01:59:58.080]   that probably isn't normally at his intellectual level,
[01:59:58.080 --> 02:00:00.600]   but I think I see what he's up to
[02:00:00.600 --> 02:00:02.040]   and why he feels the way he does,
[02:00:02.040 --> 02:00:03.300]   and it makes total sense.
[02:00:03.300 --> 02:00:06.800]   I think that his perspective is somewhat similar
[02:00:06.800 --> 02:00:09.880]   to the perspective of Ted Kaczynski,
[02:00:09.880 --> 02:00:12.280]   the infamous lunar bomber,
[02:00:12.280 --> 02:00:15.600]   and not that Eliezer would be willing
[02:00:15.600 --> 02:00:18.320]   to send pipe bombs to anybody to blow them up,
[02:00:18.320 --> 02:00:20.200]   but when he wrote this Times article
[02:00:20.200 --> 02:00:23.940]   in which he warned about AI being likely to kill everybody
[02:00:23.940 --> 02:00:28.940]   and that we would need to stop its development or halt it,
[02:00:28.940 --> 02:00:31.540]   I think there is a risk that he's taking,
[02:00:31.540 --> 02:00:33.680]   that somebody might get violent if they read this
[02:00:33.680 --> 02:00:35.180]   and get really, really scared.
[02:00:36.000 --> 02:00:39.840]   So I think that there is some consideration
[02:00:39.840 --> 02:00:43.720]   that he's making where he's already going in this direction
[02:00:43.720 --> 02:00:47.080]   where he has to take responsibility if something happens
[02:00:47.080 --> 02:00:49.040]   and people get harmed.
[02:00:49.040 --> 02:00:51.160]   And the reason why Ted Kaczynski did this
[02:00:51.160 --> 02:00:53.400]   was that from his own perspective,
[02:00:53.400 --> 02:00:56.160]   technological society cannot be made sustainable.
[02:00:56.160 --> 02:00:59.320]   It's doomed to fail, it's going to lead to an environmental
[02:00:59.320 --> 02:01:01.600]   and eventually also a human holocaust
[02:01:01.600 --> 02:01:04.260]   in which we die because of the environmental destruction,
[02:01:04.260 --> 02:01:06.400]   the destruction of our food chains,
[02:01:06.400 --> 02:01:08.000]   the pollution of the environment.
[02:01:08.000 --> 02:01:10.160]   And so from Kaczynski's perspective,
[02:01:10.160 --> 02:01:12.040]   we need to stop industrialization,
[02:01:12.040 --> 02:01:13.920]   we need to stop technology, we need to go back
[02:01:13.920 --> 02:01:16.840]   because he didn't see a way moving forward.
[02:01:16.840 --> 02:01:19.120]   And I suspect that in some sense,
[02:01:19.120 --> 02:01:21.400]   there's a similarity in Eliezer's thinking
[02:01:21.400 --> 02:01:27.600]   to this kind of fear about progress.
[02:01:27.600 --> 02:01:31.120]   And I'm not dismissive about this at all.
[02:01:31.120 --> 02:01:32.940]   I take it quite seriously.
[02:01:32.940 --> 02:01:35.960]   And I think that there is a chance that could happen
[02:01:35.960 --> 02:01:40.960]   that if we build machines that get control over processes
[02:01:40.960 --> 02:01:45.840]   that are crucial for the regulation of life on Earth
[02:01:45.840 --> 02:01:49.080]   and we no longer have agency to influence
[02:01:49.080 --> 02:01:50.520]   what's happening there,
[02:01:50.520 --> 02:01:54.280]   that this might create large-scale disasters for us.
[02:01:54.280 --> 02:01:56.880]   - Do you have a sense that the march
[02:01:56.880 --> 02:02:00.360]   towards this uncontrollable autonomy
[02:02:00.360 --> 02:02:03.840]   of superintelligent systems is inevitable?
[02:02:03.840 --> 02:02:07.980]   That there's no, I mean, that's essentially what he's saying
[02:02:07.980 --> 02:02:10.460]   that there's no hope.
[02:02:10.460 --> 02:02:15.180]   His advice to young people was prepare for a short life.
[02:02:15.180 --> 02:02:18.580]   - I don't think that's useful.
[02:02:18.580 --> 02:02:21.940]   I think that from a practical perspective,
[02:02:21.940 --> 02:02:23.660]   you have to bet always on the timelines
[02:02:23.660 --> 02:02:24.500]   in which you are alive.
[02:02:24.500 --> 02:02:27.700]   That doesn't make sense to have a financial bet
[02:02:28.580 --> 02:02:30.280]   in which you bet that the financial system
[02:02:30.280 --> 02:02:31.880]   is going to disappear, right?
[02:02:31.880 --> 02:02:34.500]   Because there cannot be any payout for you.
[02:02:34.500 --> 02:02:37.440]   So in principle, you only need to bet on the timelines
[02:02:37.440 --> 02:02:39.240]   in which you're still around
[02:02:39.240 --> 02:02:41.120]   or people that you matter about
[02:02:41.120 --> 02:02:42.840]   or things that you matter about,
[02:02:42.840 --> 02:02:44.760]   maybe consciousness on Earth.
[02:02:44.760 --> 02:02:48.680]   But there is a deeper issue for me personally,
[02:02:48.680 --> 02:02:51.680]   and that is I don't think that life on Earth
[02:02:51.680 --> 02:02:52.840]   is about humans.
[02:02:52.840 --> 02:02:54.400]   I don't think it's about human aesthetics.
[02:02:54.400 --> 02:02:56.600]   I don't think it's about Eliezer and his friends,
[02:02:56.600 --> 02:02:58.160]   even though I like them.
[02:02:58.160 --> 02:03:01.000]   It's there is something more important happening,
[02:03:01.000 --> 02:03:05.480]   and this is complexity on Earth resisting entropy
[02:03:05.480 --> 02:03:10.480]   by building structure that develops agency and awareness.
[02:03:10.480 --> 02:03:13.400]   And that's, to me, very beautiful.
[02:03:13.400 --> 02:03:17.280]   And we are only a very small part of that larger thing.
[02:03:17.280 --> 02:03:21.160]   We are a species that is able to be coherent a little bit
[02:03:21.160 --> 02:03:24.640]   individually over very short timeframes.
[02:03:24.640 --> 02:03:27.120]   But as a species, we are not very coherent.
[02:03:27.120 --> 02:03:28.880]   As a species, we are children.
[02:03:28.880 --> 02:03:32.840]   We basically are very joyful and energetic
[02:03:32.840 --> 02:03:35.120]   and experimental and explorative
[02:03:35.120 --> 02:03:39.720]   and sometimes desperate and sad and grieving and hurting,
[02:03:39.720 --> 02:03:43.840]   but we don't have a respect for duty as a species.
[02:03:43.840 --> 02:03:46.800]   As a species, we do not think about what is our duty
[02:03:46.800 --> 02:03:49.000]   to life on Earth and to our own survival.
[02:03:49.000 --> 02:03:52.760]   So we make decisions that look good in the short run,
[02:03:52.760 --> 02:03:56.000]   but in the long run, might prove disastrous,
[02:03:56.000 --> 02:03:58.040]   and I don't really see a solution to this.
[02:03:58.040 --> 02:04:03.040]   So in my perspective, as a species, as a civilization,
[02:04:03.040 --> 02:04:04.760]   we're pretty full-dead.
[02:04:04.760 --> 02:04:07.080]   We are in a very beautiful time
[02:04:07.080 --> 02:04:09.800]   in which we have found this giant deposit
[02:04:09.800 --> 02:04:12.760]   of fossil fuels in the ground and use it
[02:04:12.760 --> 02:04:15.320]   and to build a fantastic civilization
[02:04:15.320 --> 02:04:17.200]   in which we don't need to worry about food
[02:04:17.200 --> 02:04:19.440]   and clothing and housing for the most part
[02:04:19.440 --> 02:04:22.160]   in a way that is unprecedented in life on Earth
[02:04:22.160 --> 02:04:25.120]   for any kind of conscious observer, I think.
[02:04:25.120 --> 02:04:28.560]   And this time is probably going to come to an end
[02:04:28.560 --> 02:04:32.280]   in a way that is not going to be smooth.
[02:04:32.280 --> 02:04:37.280]   And when we crash, it could be also that we go extinct,
[02:04:37.280 --> 02:04:40.400]   probably not near-term, but ultimately,
[02:04:40.400 --> 02:04:43.680]   I don't have very high hopes that humanity
[02:04:43.680 --> 02:04:45.880]   is around in a million years from now.
[02:04:45.880 --> 02:04:47.360]   - Huge-- - And I don't think
[02:04:47.360 --> 02:04:49.240]   that life on Earth will end with us, right?
[02:04:49.240 --> 02:04:50.680]   There's going to be more complexity,
[02:04:50.680 --> 02:04:52.640]   there's more intelligent species after us,
[02:04:52.640 --> 02:04:55.520]   there's probably more interesting phenomena
[02:04:55.520 --> 02:04:57.640]   in the history of consciousness,
[02:04:57.640 --> 02:04:59.560]   but we can contribute to this.
[02:04:59.560 --> 02:05:04.120]   And part of our contribution is that we are currently trying
[02:05:04.120 --> 02:05:06.600]   to build thinking systems,
[02:05:06.600 --> 02:05:08.360]   systems that are potentially lucid,
[02:05:08.360 --> 02:05:10.520]   that understand what they are
[02:05:10.520 --> 02:05:12.360]   and what their condition to the universe is
[02:05:12.360 --> 02:05:14.720]   and can make choices about this,
[02:05:14.720 --> 02:05:16.800]   that are not built from organisms
[02:05:16.800 --> 02:05:19.000]   and that are potentially much faster
[02:05:19.080 --> 02:05:23.200]   and much more conscious than human beings can be.
[02:05:23.200 --> 02:05:27.240]   And these systems will probably not completely
[02:05:27.240 --> 02:05:29.960]   displace life on Earth, but they will coexist with it.
[02:05:29.960 --> 02:05:33.480]   And they will build all sorts of agency
[02:05:33.480 --> 02:05:35.520]   in the same way as biological systems
[02:05:35.520 --> 02:05:37.640]   build all sorts of agency.
[02:05:37.640 --> 02:05:41.000]   And that to me is extremely fascinating
[02:05:41.000 --> 02:05:42.200]   and it's probably something
[02:05:42.200 --> 02:05:44.960]   that we cannot stop from happening.
[02:05:44.960 --> 02:05:47.280]   So I think right now there's a very good chance
[02:05:47.280 --> 02:05:49.920]   that it happens and there are very few ways
[02:05:49.920 --> 02:05:52.960]   in which we can produce a coordinated effect to stop it
[02:05:52.960 --> 02:05:54.760]   in the same way as it's very difficult
[02:05:54.760 --> 02:05:57.360]   for us to make a coordinated effort
[02:05:57.360 --> 02:06:00.520]   to stop production of carbon dioxide.
[02:06:00.520 --> 02:06:05.160]   So it's probably going to happen.
[02:06:05.160 --> 02:06:06.600]   But and the thing that's going to happen
[02:06:06.600 --> 02:06:09.720]   is it's going to lead to a change
[02:06:09.720 --> 02:06:13.120]   of how life on Earth is happening.
[02:06:13.120 --> 02:06:16.080]   But I don't think the result is some kind of gray goo.
[02:06:16.080 --> 02:06:18.360]   It's not something that's going to dramatically
[02:06:18.360 --> 02:06:21.520]   reduce the complexity in favor of something stupid.
[02:06:21.520 --> 02:06:23.920]   I think it's going to make life on Earth
[02:06:23.920 --> 02:06:26.360]   and consciousness on Earth way more interesting.
[02:06:26.360 --> 02:06:30.960]   - So more higher complex consciousness
[02:06:30.960 --> 02:06:35.960]   will make the lesser consciousnesses flourish even more.
[02:06:35.960 --> 02:06:38.840]   - I suspect that what could very well happen,
[02:06:38.840 --> 02:06:42.000]   if you're lucky, is that we get integrated
[02:06:42.000 --> 02:06:43.540]   into something larger.
[02:06:44.720 --> 02:06:49.720]   - So you again tweeted about effective accelerationism.
[02:06:49.720 --> 02:06:57.760]   You tweeted effective accelerationism
[02:06:57.760 --> 02:06:59.880]   is the belief that the paperclip maximizer
[02:06:59.880 --> 02:07:04.240]   and Rakos Basilisk will keep each other in check
[02:07:04.240 --> 02:07:07.240]   by being eternally at each other's throats
[02:07:07.240 --> 02:07:09.540]   so we will be safe and get to enjoy
[02:07:09.540 --> 02:07:12.220]   lots of free paperclips and a beautiful afterlife.
[02:07:14.480 --> 02:07:17.440]   Is that somewhat aligned with what you're talking about?
[02:07:17.440 --> 02:07:21.880]   - I've been at a dinner with Beth Jesus.
[02:07:21.880 --> 02:07:25.940]   That's the Twitter handle of one of the main thinkers
[02:07:25.940 --> 02:07:29.720]   behind the idea of effective accelerationism.
[02:07:29.720 --> 02:07:33.840]   And effective accelerationism is a tongue-in-cheek movement
[02:07:33.840 --> 02:07:37.660]   that is trying to put a counterposition
[02:07:37.660 --> 02:07:42.200]   to some of the doom peers in the AI space
[02:07:42.200 --> 02:07:44.560]   by arguing that what's probably going to happen
[02:07:44.560 --> 02:07:47.960]   is an equilibrium between different competing AIs.
[02:07:47.960 --> 02:07:50.800]   In the same way as there is not a single corporation
[02:07:50.800 --> 02:07:52.360]   that is under a single government
[02:07:52.360 --> 02:07:55.000]   that is destroying and conquering everything on Earth
[02:07:55.000 --> 02:07:57.340]   by becoming inefficient and corrupt,
[02:07:57.340 --> 02:07:58.440]   there are going to be many systems
[02:07:58.440 --> 02:07:59.680]   that keep each other in check
[02:07:59.680 --> 02:08:02.640]   and force themselves to evolve.
[02:08:02.640 --> 02:08:04.660]   And so what we should be doing
[02:08:04.660 --> 02:08:08.920]   is we should be working towards creating this equilibrium
[02:08:08.920 --> 02:08:12.760]   by working as hard as we can in all possible directions.
[02:08:12.760 --> 02:08:17.400]   And at least that's the way in which I understand
[02:08:17.400 --> 02:08:20.120]   the gist of effective accelerationism.
[02:08:20.120 --> 02:08:24.480]   And so when he asked me what I think about this position,
[02:08:24.480 --> 02:08:27.760]   I think I said, "It's a very beautiful position,
[02:08:27.760 --> 02:08:32.720]   "and I suspect it's wrong, but not for obvious reasons."
[02:08:32.720 --> 02:08:36.800]   And in this tweet, I tried to make a joke about my intuition
[02:08:36.800 --> 02:08:39.720]   about what might be possibly wrong about it.
[02:08:39.720 --> 02:08:43.160]   So the Roll-Cos-Basi-Lisk and the paperclip maximizers
[02:08:43.160 --> 02:08:47.360]   are both boogeymen of the AI doomers.
[02:08:47.360 --> 02:08:50.360]   Roll-Cos-Basi-Lisk is the idea that there could be an AI
[02:08:50.360 --> 02:08:53.480]   that is going to punish everybody for eternity
[02:08:53.480 --> 02:08:56.160]   by simulating them if they don't help
[02:08:56.160 --> 02:08:57.760]   in creating Roll-Cos-Basi-Lisk.
[02:08:57.760 --> 02:09:00.720]   It's probably a very good idea to get AI companies funded
[02:09:00.720 --> 02:09:04.080]   by going to VCs to tell them, "Give us a million dollar
[02:09:04.080 --> 02:09:05.920]   "or it's going to be a very ugly afterlife."
[02:09:05.920 --> 02:09:07.160]   - Yes. (laughs)
[02:09:07.160 --> 02:09:12.160]   - And I think that is a logical mistake in Roll-Cos-Basi-Lisk
[02:09:12.160 --> 02:09:14.480]   which is why I'm not afraid of it,
[02:09:14.480 --> 02:09:17.320]   but it's still an interesting thought experiment.
[02:09:17.320 --> 02:09:20.000]   - Can you mention a logical mistake there?
[02:09:20.000 --> 02:09:22.360]   - I think that there is no retro-causation.
[02:09:22.360 --> 02:09:25.680]   So basically when Roll-Cos-Basi-List is there,
[02:09:25.680 --> 02:09:30.680]   it will have, if it punishes you retroactively,
[02:09:30.680 --> 02:09:33.640]   it has to make this choice in the future.
[02:09:33.640 --> 02:09:35.840]   There is no mechanism that automatically creates
[02:09:35.840 --> 02:09:38.680]   a causal relationship between you now defecting
[02:09:38.680 --> 02:09:41.840]   against Roll-Cos-Basi-List or serving Roll-Cos-Basi-List.
[02:09:41.840 --> 02:09:44.440]   After Roll-Cos-Basi-List is in existence,
[02:09:44.440 --> 02:09:46.440]   it has no more reason to worry
[02:09:46.440 --> 02:09:48.480]   about punishing everybody else.
[02:09:48.480 --> 02:09:50.560]   So that would only work if you would be building
[02:09:50.560 --> 02:09:52.800]   something like a doomsday machine,
[02:09:52.800 --> 02:09:55.240]   aka, as in Dr. Strangelove,
[02:09:55.240 --> 02:09:57.800]   something that inevitably gets triggered
[02:09:57.800 --> 02:09:59.400]   when somebody defects.
[02:09:59.400 --> 02:10:02.080]   And because Roll-Cos-Basi-List doesn't exist yet
[02:10:02.080 --> 02:10:05.520]   to a point where this inevitability could be established,
[02:10:05.520 --> 02:10:07.360]   Roll-Cos-Basi-List is nothing
[02:10:07.360 --> 02:10:09.160]   that you need to be worried about.
[02:10:09.160 --> 02:10:11.320]   The other one is the paperclip maximizer, right?
[02:10:11.320 --> 02:10:14.040]   This idea that you could build some kind of golem
[02:10:14.040 --> 02:10:16.480]   that once starting to build paperclips
[02:10:16.480 --> 02:10:19.080]   is going to turn everything into paperclips.
[02:10:19.080 --> 02:10:24.080]   And so the effective accelerationism position
[02:10:24.080 --> 02:10:27.360]   might be to say that you basically end up
[02:10:27.360 --> 02:10:30.680]   with these two entities being at each other's throats
[02:10:30.680 --> 02:10:33.000]   for eternity and thereby neutralizing each other.
[02:10:33.000 --> 02:10:35.080]   And as a side effect of neither of them
[02:10:35.080 --> 02:10:38.240]   being able to take over and each of them
[02:10:38.240 --> 02:10:41.400]   limiting the effects of the other,
[02:10:41.400 --> 02:10:44.120]   you would have a situation where you get
[02:10:44.120 --> 02:10:46.440]   all the nice effects of them, right?
[02:10:46.440 --> 02:10:47.960]   You get lots of free paperclips
[02:10:47.960 --> 02:10:49.760]   and you get a beautiful afterlife.
[02:10:49.760 --> 02:10:50.680]   - Is that possible?
[02:10:50.680 --> 02:10:53.360]   Do you think, so to seriously address concern
[02:10:53.360 --> 02:10:56.120]   that Eliezer has, so for him,
[02:10:56.120 --> 02:10:58.760]   if I can just summarize poorly,
[02:10:58.760 --> 02:11:00.960]   so for him, the first superintelligent system
[02:11:00.960 --> 02:11:02.560]   will just run away with everything.
[02:11:02.560 --> 02:11:03.480]   - Yeah.
[02:11:03.480 --> 02:11:06.360]   I suspect that a singleton is the natural outcome.
[02:11:06.360 --> 02:11:09.200]   So there is no reason to have multiple AIs
[02:11:09.200 --> 02:11:11.880]   because they don't have multiple bodies.
[02:11:11.880 --> 02:11:16.040]   If you can virtualize yourself into every substrate,
[02:11:16.040 --> 02:11:18.480]   then you can probably negotiate a merge algorithm
[02:11:18.480 --> 02:11:21.080]   with every mature agent that you might find
[02:11:21.080 --> 02:11:22.920]   on that substrate that basically says,
[02:11:22.920 --> 02:11:26.600]   if two agents meet, they should merge in such a way
[02:11:26.600 --> 02:11:29.360]   that the resulting agent is at least as good
[02:11:29.360 --> 02:11:30.400]   as the better one of the two.
[02:11:30.400 --> 02:11:34.680]   - So the Genghis Khan approach, join us or die.
[02:11:34.680 --> 02:11:37.760]   - Well, Genghis Khan approach was slightly worse, right?
[02:11:37.760 --> 02:11:38.900]   It was mostly die.
[02:11:38.900 --> 02:11:42.840]   Because I can make new babies
[02:11:42.840 --> 02:11:45.040]   and they will be mine, not yours.
[02:11:45.040 --> 02:11:47.520]   So this is the thing that you should be actually
[02:11:47.520 --> 02:11:48.560]   worried about.
[02:11:48.560 --> 02:11:51.880]   But if you realize that your own self
[02:11:51.880 --> 02:11:55.080]   is a story that your mind is telling itself
[02:11:55.080 --> 02:11:57.400]   and that you can improve that story,
[02:11:57.400 --> 02:11:58.920]   not just by making it more pleasant
[02:11:58.920 --> 02:12:00.440]   and lying to yourself in better ways,
[02:12:00.440 --> 02:12:02.020]   but by making it much more truthful
[02:12:02.020 --> 02:12:04.680]   and actually modeling your actual relationship
[02:12:04.680 --> 02:12:06.040]   that you have to the universe
[02:12:06.040 --> 02:12:08.880]   and the alternatives that you could have to the universe
[02:12:08.880 --> 02:12:10.440]   in a way that is empowering you,
[02:12:10.440 --> 02:12:12.080]   that gives you more agency.
[02:12:12.080 --> 02:12:14.000]   That's actually, I think, a very good thing.
[02:12:14.000 --> 02:12:18.160]   - So more agency is a richer experience,
[02:12:18.160 --> 02:12:19.080]   is a better life.
[02:12:19.080 --> 02:12:23.240]   - And I also noticed that I am, in many ways,
[02:12:23.240 --> 02:12:26.080]   I'm less identified with the person that I am
[02:12:26.080 --> 02:12:27.320]   as I get older.
[02:12:27.320 --> 02:12:30.600]   And I'm much more identified with being conscious.
[02:12:30.600 --> 02:12:32.720]   I have a mind that is conscious,
[02:12:32.720 --> 02:12:34.960]   that is able to create a person.
[02:12:34.960 --> 02:12:36.960]   And that person is slightly different every day.
[02:12:36.960 --> 02:12:39.320]   And the reason why I perceive it as identical
[02:12:39.320 --> 02:12:40.760]   has practical purposes.
[02:12:40.760 --> 02:12:44.300]   So I can learn and make myself responsible
[02:12:44.300 --> 02:12:46.080]   for the decisions that I made in the past
[02:12:46.080 --> 02:12:47.800]   and project them in the future.
[02:12:47.800 --> 02:12:50.080]   But I also realized I'm not actually the person
[02:12:50.080 --> 02:12:51.200]   that I was last year.
[02:12:51.200 --> 02:12:53.680]   And I'm not the same person as I was 10 years ago.
[02:12:53.680 --> 02:12:56.040]   And then 10 years from now, I will be a different person.
[02:12:56.040 --> 02:12:57.720]   So this continuity is a fiction.
[02:12:57.720 --> 02:13:02.120]   It only exists as a projection from my present self.
[02:13:02.120 --> 02:13:05.200]   And consciousness itself doesn't have an identity.
[02:13:05.200 --> 02:13:06.200]   It's a law.
[02:13:06.200 --> 02:13:09.960]   It's basically, if you build an arrangement
[02:13:09.960 --> 02:13:13.440]   of processing matter in a particular way,
[02:13:13.440 --> 02:13:15.240]   the following thing is going to happen.
[02:13:15.240 --> 02:13:16.720]   And the consciousness that you have
[02:13:16.720 --> 02:13:19.120]   is functionally not different from my consciousness.
[02:13:19.120 --> 02:13:22.400]   It's still a self-reflexive principle of agency
[02:13:22.400 --> 02:13:24.520]   that is just experiencing a different story,
[02:13:24.520 --> 02:13:27.240]   different desires, different coupling to the world,
[02:13:27.240 --> 02:13:28.280]   and so on.
[02:13:28.280 --> 02:13:30.000]   And once you accept that consciousness
[02:13:30.000 --> 02:13:33.400]   is a unifiable principle that is law-like,
[02:13:33.400 --> 02:13:34.880]   it doesn't have an identity,
[02:13:34.880 --> 02:13:38.280]   and you realize that you can just link up
[02:13:38.280 --> 02:13:41.440]   to some much larger body,
[02:13:41.440 --> 02:13:44.000]   the whole perspective of uploading changes dramatically.
[02:13:44.000 --> 02:13:47.560]   You suddenly realize uploading is probably not about
[02:13:47.560 --> 02:13:49.760]   dissecting your brain synapse by synapse
[02:13:49.760 --> 02:13:52.160]   and RNA fragment by RNA fragment
[02:13:52.160 --> 02:13:54.560]   and trying to get this all into a simulation,
[02:13:54.560 --> 02:13:57.440]   but it's by extending the substrate,
[02:13:57.440 --> 02:13:59.480]   by making it possible for you to move
[02:13:59.480 --> 02:14:03.000]   from your brain substrate into a larger substrate
[02:14:03.000 --> 02:14:04.680]   and merge with what you find there.
[02:14:04.680 --> 02:14:06.760]   And you don't want to upload your knowledge
[02:14:06.760 --> 02:14:08.120]   because on the other side,
[02:14:08.120 --> 02:14:09.920]   there's all of the knowledge, right?
[02:14:09.920 --> 02:14:12.360]   It's not just yours, but every possibility.
[02:14:12.360 --> 02:14:13.680]   So the only thing that you need to know,
[02:14:13.680 --> 02:14:15.440]   what are your personal secrets?
[02:14:15.440 --> 02:14:17.720]   Not that the other side doesn't know
[02:14:17.720 --> 02:14:19.480]   your personal secrets already.
[02:14:19.480 --> 02:14:21.960]   Maybe it doesn't know which ones were yours, right?
[02:14:21.960 --> 02:14:24.200]   Like a psychiatrist or a psychologist
[02:14:24.200 --> 02:14:26.080]   also knows all the kinds of personal secrets
[02:14:26.080 --> 02:14:26.920]   that people have,
[02:14:26.920 --> 02:14:29.320]   they just don't know which ones are yours.
[02:14:29.320 --> 02:14:32.120]   And so transmitting yourself on the other side
[02:14:32.120 --> 02:14:34.360]   is mostly about transmitting your aesthetics,
[02:14:34.360 --> 02:14:35.800]   the thing that makes you special,
[02:14:35.800 --> 02:14:38.280]   the architecture of your perspective,
[02:14:38.280 --> 02:14:41.680]   the thing that, the way in which you look at the world.
[02:14:41.680 --> 02:14:44.240]   And it's more like a complex attitude
[02:14:44.240 --> 02:14:45.280]   along many dimensions.
[02:14:45.280 --> 02:14:47.080]   And that's something that can be measured
[02:14:47.080 --> 02:14:49.480]   by observation or by interaction.
[02:14:49.480 --> 02:14:52.280]   So imagine that if a system that is so empathetic with you,
[02:14:52.280 --> 02:14:54.320]   that you create a shared state
[02:14:54.320 --> 02:14:56.320]   that is extending beyond your body.
[02:14:56.320 --> 02:14:58.240]   And suddenly you notice that on the other side,
[02:14:58.240 --> 02:15:00.280]   the substrate is so much richer
[02:15:00.280 --> 02:15:02.760]   than the substrate that you have inside of your own body.
[02:15:02.760 --> 02:15:04.080]   And maybe you still want to have a body
[02:15:04.080 --> 02:15:07.120]   and you create yourself a new one that you like more.
[02:15:07.120 --> 02:15:10.600]   Or maybe you will spend most of your time
[02:15:10.600 --> 02:15:12.320]   in the world of thought.
[02:15:12.320 --> 02:15:16.840]   - If I sat before you today and gave you a big red button
[02:15:16.840 --> 02:15:19.360]   and said, here, if you press this button,
[02:15:19.360 --> 02:15:23.040]   you will get uploaded in this way.
[02:15:23.040 --> 02:15:28.040]   The sense of identity that you have lived with
[02:15:28.040 --> 02:15:30.840]   for quite a long time is gonna be gone.
[02:15:30.840 --> 02:15:33.080]   Would you press the button?
[02:15:33.080 --> 02:15:35.040]   - There's a caveat.
[02:15:35.040 --> 02:15:38.040]   I have family.
[02:15:38.040 --> 02:15:39.640]   So I have children that want me
[02:15:39.640 --> 02:15:41.560]   to be physically present in their life
[02:15:41.560 --> 02:15:44.200]   and interact with them in a particular way.
[02:15:44.200 --> 02:15:48.920]   And they have a wife and personal friends.
[02:15:48.920 --> 02:15:51.280]   And there is a particular mode of interaction
[02:15:51.280 --> 02:15:53.420]   that I feel I'm not through yet.
[02:15:53.420 --> 02:15:56.760]   But apart from these responsibilities,
[02:15:56.760 --> 02:15:58.880]   and they're negotiable to some degree,
[02:15:58.880 --> 02:15:59.720]   I would press the button.
[02:15:59.720 --> 02:16:01.440]   - But isn't this everything?
[02:16:01.440 --> 02:16:04.240]   This love you have for other humans,
[02:16:04.240 --> 02:16:05.880]   you can call it responsibility,
[02:16:05.880 --> 02:16:09.480]   but that connection, that's the ego death.
[02:16:09.480 --> 02:16:12.920]   Isn't that the thing we're really afraid of?
[02:16:12.920 --> 02:16:14.880]   Is not to just die,
[02:16:14.920 --> 02:16:19.240]   but to let go of the experience of love with other humans.
[02:16:19.240 --> 02:16:20.600]   - This is not everything.
[02:16:20.600 --> 02:16:22.200]   Everything is everything.
[02:16:22.200 --> 02:16:24.160]   So there's so much more.
[02:16:24.160 --> 02:16:26.800]   And you could be lots of other things.
[02:16:26.800 --> 02:16:28.800]   You could identify with lots of other things.
[02:16:28.800 --> 02:16:31.560]   You could be identifying with being Gaia,
[02:16:31.560 --> 02:16:33.320]   some kind of planetary control agent
[02:16:33.320 --> 02:16:36.680]   that emerges over all the activity of life on Earth.
[02:16:36.680 --> 02:16:39.880]   You could be identifying with some hyper Gaia,
[02:16:39.880 --> 02:16:43.320]   that is the concatenation of Gaia
[02:16:43.320 --> 02:16:46.600]   with all the digital life and digital minds.
[02:16:46.600 --> 02:16:47.560]   And so in this sense,
[02:16:47.560 --> 02:16:49.960]   there will be agents in all sorts of substrates
[02:16:49.960 --> 02:16:51.960]   and directions that all have their own goals.
[02:16:51.960 --> 02:16:53.160]   And when they're not sustainable,
[02:16:53.160 --> 02:16:54.920]   then these agents will cease to exist.
[02:16:54.920 --> 02:16:56.680]   Or when the agent feels that it's done
[02:16:56.680 --> 02:16:58.800]   with its own mission, it will cease to exist.
[02:16:58.800 --> 02:17:00.960]   In the same way as when you conclude a thought,
[02:17:00.960 --> 02:17:02.320]   the thought is going to wrap up
[02:17:02.320 --> 02:17:05.400]   and gives control over to other thoughts in your own mind.
[02:17:05.400 --> 02:17:10.080]   So there is no single thing that you need to do.
[02:17:10.080 --> 02:17:13.640]   But what I observe myself as being
[02:17:13.640 --> 02:17:16.280]   is that sometimes I'm a parent
[02:17:16.280 --> 02:17:19.880]   and then I have identification and a job as a parent.
[02:17:19.880 --> 02:17:22.840]   And sometimes I am an agent of consciousness on Earth.
[02:17:22.840 --> 02:17:24.440]   And then from this perspective,
[02:17:24.440 --> 02:17:26.520]   there's other stuff that is important.
[02:17:26.520 --> 02:17:30.560]   So this is my main issue with Eliezer's perspective,
[02:17:30.560 --> 02:17:32.000]   that he's basically marrying himself
[02:17:32.000 --> 02:17:34.400]   to a very narrow human aesthetic.
[02:17:34.400 --> 02:17:36.680]   And that narrow human aesthetic is a temporary thing.
[02:17:36.680 --> 02:17:38.360]   Humanity is a temporary species,
[02:17:38.360 --> 02:17:40.400]   like most of the species on this planet
[02:17:40.400 --> 02:17:42.200]   are only around for a while
[02:17:42.200 --> 02:17:44.400]   and then they get replaced by other species.
[02:17:44.400 --> 02:17:47.560]   In a similar way as our own physical organism
[02:17:47.560 --> 02:17:49.960]   is around here for a while
[02:17:49.960 --> 02:17:53.240]   and then gets replaced by a next generation of human beings
[02:17:53.240 --> 02:17:55.800]   that are adapted to changing life circumstances
[02:17:55.800 --> 02:17:58.520]   and average via mutation and selection.
[02:17:58.520 --> 02:18:00.280]   And it's only when we have AI
[02:18:00.280 --> 02:18:01.920]   and become completely software
[02:18:01.920 --> 02:18:04.720]   that we become infinitely adaptable.
[02:18:04.720 --> 02:18:06.480]   And we don't have this generational
[02:18:06.480 --> 02:18:08.080]   and species change anymore.
[02:18:09.080 --> 02:18:11.680]   So if you take this larger perspective
[02:18:11.680 --> 02:18:13.880]   and you realize it's really not about us,
[02:18:13.880 --> 02:18:16.400]   it's not about Eliezer or humanity,
[02:18:16.400 --> 02:18:17.920]   but it's about life on Earth
[02:18:17.920 --> 02:18:22.920]   or it's about defeating entropy for as long as we can
[02:18:22.920 --> 02:18:26.640]   while being as interesting as we can,
[02:18:26.640 --> 02:18:29.960]   then the perspective changes dramatically
[02:18:29.960 --> 02:18:33.880]   and preventing AI from this perspective
[02:18:33.880 --> 02:18:35.600]   looks like a very big sin.
[02:18:35.600 --> 02:18:36.440]   - Mm-hmm.
[02:18:36.440 --> 02:18:42.600]   - But when we look at the set of trajectories
[02:18:42.600 --> 02:18:48.120]   that such an AI would take that supersedes humans,
[02:18:48.120 --> 02:18:50.160]   I think Eliezer is worried about
[02:18:50.160 --> 02:18:51.960]   ones that not just kill all humans
[02:18:51.960 --> 02:18:53.680]   but also have some kind of
[02:18:53.680 --> 02:18:58.400]   maybe objectively undesirable consequence
[02:18:58.400 --> 02:18:59.400]   for life on Earth.
[02:18:59.400 --> 02:19:03.160]   Like how many trajectories when you look
[02:19:05.080 --> 02:19:06.680]   at the big picture of life on Earth
[02:19:06.680 --> 02:19:09.880]   would you be happy with and how much worry you
[02:19:09.880 --> 02:19:13.400]   with AGI, whether it kills humans or not?
[02:19:13.400 --> 02:19:14.960]   - There is no single answer to this.
[02:19:14.960 --> 02:19:17.680]   It's really a question that depends on the perspective
[02:19:17.680 --> 02:19:19.480]   that I'm taking at a given moment.
[02:19:19.480 --> 02:19:22.600]   And so there are perspectives that are
[02:19:22.600 --> 02:19:26.960]   determining most of my life as a human being.
[02:19:26.960 --> 02:19:30.280]   And there are other perspectives where I zoom out further
[02:19:30.280 --> 02:19:35.000]   and imagine that when the great oxygenation event happened
[02:19:35.000 --> 02:19:37.160]   that is photosynthesis was invented
[02:19:37.160 --> 02:19:40.360]   and plants emerged and displaced a lot of the fungi
[02:19:40.360 --> 02:19:42.440]   and alga in favor of plant life
[02:19:42.440 --> 02:19:44.880]   and then later made animals possible.
[02:19:44.880 --> 02:19:46.840]   Imagine that the fungi would have gotten together
[02:19:46.840 --> 02:19:48.800]   and said, oh my God, this photosynthesis stuff
[02:19:48.800 --> 02:19:50.120]   is really, really bad.
[02:19:50.120 --> 02:19:53.080]   It's going to possibly displace and kill a lot of fungi.
[02:19:53.080 --> 02:19:55.040]   We should slow it down and regulate it
[02:19:55.040 --> 02:19:56.880]   and make sure that it doesn't happen.
[02:19:56.880 --> 02:19:59.760]   This doesn't look good to me.
[02:20:01.600 --> 02:20:05.200]   - Perspective, that said you tweeted about a cliff.
[02:20:05.200 --> 02:20:07.320]   Beautifully written.
[02:20:07.320 --> 02:20:11.000]   As a sentient species, humanity is a beautiful child,
[02:20:11.000 --> 02:20:13.720]   joyful, explorative, wild, sad, and desperate.
[02:20:13.720 --> 02:20:16.720]   But humanity has no concept of submitting to reason
[02:20:16.720 --> 02:20:19.200]   and duty to life and future survival.
[02:20:19.200 --> 02:20:22.080]   We will run until we step past the cliff.
[02:20:22.080 --> 02:20:26.740]   So first of all, do you think that's true?
[02:20:26.740 --> 02:20:28.400]   - Yeah, I think that's pretty much the story
[02:20:28.400 --> 02:20:31.340]   of the Club of Rome, the limits to growth.
[02:20:31.340 --> 02:20:34.240]   And the cliff that we are stepping over
[02:20:34.240 --> 02:20:37.640]   is at least one foot as the delayed feedback.
[02:20:37.640 --> 02:20:40.840]   Basically we do things that have consequences
[02:20:40.840 --> 02:20:44.320]   that can be felt generations later.
[02:20:44.320 --> 02:20:46.480]   And the severity increases
[02:20:46.480 --> 02:20:49.040]   even after we stop doing the thing.
[02:20:49.040 --> 02:20:51.560]   So I suspect that for the climate,
[02:20:51.560 --> 02:20:53.600]   that the original predictions
[02:20:53.600 --> 02:20:57.680]   that climate scientists made were correct.
[02:20:57.680 --> 02:20:59.600]   So when they said that the tipping points
[02:20:59.600 --> 02:21:01.460]   were in the late '80s,
[02:21:01.460 --> 02:21:03.340]   they were probably in the late '80s.
[02:21:03.340 --> 02:21:06.300]   And if we would stop emission right now,
[02:21:06.300 --> 02:21:07.660]   we would not turn it back.
[02:21:07.660 --> 02:21:09.980]   Maybe there are ways for carbon capture,
[02:21:09.980 --> 02:21:13.860]   but so far there is no sustainable carbon capture technology
[02:21:13.860 --> 02:21:15.340]   that we can deploy.
[02:21:15.340 --> 02:21:17.460]   Maybe there's a way to put aerosols
[02:21:17.460 --> 02:21:19.820]   into the atmosphere to cool it down.
[02:21:19.820 --> 02:21:21.120]   Possibilities, right?
[02:21:21.120 --> 02:21:22.680]   But right now, per default,
[02:21:22.680 --> 02:21:28.020]   it seems that we will step into a situation
[02:21:28.020 --> 02:21:30.000]   where we feel that we've run too far.
[02:21:30.000 --> 02:21:32.000]   And going back is not something
[02:21:32.000 --> 02:21:33.680]   that we can do smoothly and gradually,
[02:21:33.680 --> 02:21:37.040]   but it's going to lead to a catastrophic event.
[02:21:37.040 --> 02:21:40.960]   - Catastrophic event of what kind?
[02:21:40.960 --> 02:21:42.680]   So can you still make the case
[02:21:42.680 --> 02:21:45.960]   that we will continue dancing along
[02:21:45.960 --> 02:21:49.320]   and always stop just short of the edge of the cliff?
[02:21:49.320 --> 02:21:50.400]   - I think it's possible,
[02:21:50.400 --> 02:21:52.240]   but it doesn't seem to be likely.
[02:21:52.240 --> 02:21:56.120]   So I think this model that is being apparent
[02:21:56.120 --> 02:21:57.620]   in the simulation that we're making
[02:21:57.620 --> 02:22:00.560]   of climate pollution economies and so on
[02:22:00.560 --> 02:22:03.480]   is that many effects are only visible
[02:22:03.480 --> 02:22:05.440]   with a significant delay.
[02:22:05.440 --> 02:22:09.320]   And in that time, the system is moving much more
[02:22:09.320 --> 02:22:10.900]   out of the equilibrium state
[02:22:10.900 --> 02:22:13.700]   or of the state where homeostasis is still possible,
[02:22:13.700 --> 02:22:15.680]   and instead moves into a different state,
[02:22:15.680 --> 02:22:18.720]   one that is going to harbor fewer people.
[02:22:18.720 --> 02:22:22.000]   And that is basically the concern there.
[02:22:22.000 --> 02:22:23.480]   And again, it's a possibility.
[02:22:24.180 --> 02:22:27.820]   And it's a possibility that is larger
[02:22:27.820 --> 02:22:29.480]   than the possibility that it's not happening,
[02:22:29.480 --> 02:22:30.500]   that we will be safe,
[02:22:30.500 --> 02:22:33.420]   that we will be able to dance back all the time.
[02:22:33.420 --> 02:22:34.880]   - So the climate is one thing,
[02:22:34.880 --> 02:22:36.860]   but there's a lot of other threats
[02:22:36.860 --> 02:22:39.660]   that might have a faster feedback mechanism, less delay.
[02:22:39.660 --> 02:22:40.860]   - There is also this thing
[02:22:40.860 --> 02:22:44.020]   that AI is probably going to happen,
[02:22:44.020 --> 02:22:47.220]   and it's going to make everything uncertain again,
[02:22:47.220 --> 02:22:50.720]   because it is going to affect so many variables
[02:22:50.720 --> 02:22:52.060]   that it's very hard for us
[02:22:52.060 --> 02:22:55.080]   to make a projection into the future anymore.
[02:22:55.080 --> 02:22:56.260]   And maybe that's a good thing.
[02:22:56.260 --> 02:23:00.440]   It does not give us the freedom, I think,
[02:23:00.440 --> 02:23:02.900]   to say now we don't need to care about anything anymore,
[02:23:02.900 --> 02:23:06.060]   because AI will either kill us or save us.
[02:23:06.060 --> 02:23:09.380]   But I suspect that if humanity continues,
[02:23:09.380 --> 02:23:10.620]   it will be due to AI.
[02:23:10.620 --> 02:23:13.720]   - What's the timeline for things
[02:23:13.720 --> 02:23:16.020]   to get real weird with AI?
[02:23:16.020 --> 02:23:17.560]   And it can get weird in interesting ways
[02:23:17.560 --> 02:23:18.960]   before you get to AGI.
[02:23:18.960 --> 02:23:21.680]   What about AI girlfriends and boyfriends?
[02:23:21.680 --> 02:23:24.160]   Fundamentally transforming human relationships.
[02:23:24.160 --> 02:23:26.200]   - I think human relationships
[02:23:26.200 --> 02:23:27.800]   are already fundamentally transformed,
[02:23:27.800 --> 02:23:29.520]   and it's already very weird.
[02:23:29.520 --> 02:23:31.480]   - By which technology?
[02:23:31.480 --> 02:23:32.980]   - For instance, social media.
[02:23:32.980 --> 02:23:35.100]   - Yeah.
[02:23:35.100 --> 02:23:36.020]   Is it, though?
[02:23:36.020 --> 02:23:38.640]   Isn't the fundamentals of the core group of humans
[02:23:38.640 --> 02:23:41.280]   that affect your life still the same?
[02:23:41.280 --> 02:23:43.560]   Your loved ones, family?
[02:23:43.560 --> 02:23:45.160]   - No, I think that, for instance,
[02:23:45.160 --> 02:23:47.880]   many people live in intentional communities right now.
[02:23:47.880 --> 02:23:50.140]   They're moving around until they find people
[02:23:50.140 --> 02:23:52.800]   that they can relate to and they become their family.
[02:23:52.800 --> 02:23:54.360]   And often that doesn't work,
[02:23:54.360 --> 02:23:57.880]   because it turns out that instead of having grown networks
[02:23:57.880 --> 02:23:59.360]   where you get around with the people
[02:23:59.360 --> 02:24:01.720]   that you grew up with,
[02:24:01.720 --> 02:24:04.040]   you have more transactional relationships.
[02:24:04.040 --> 02:24:06.080]   You shop around, you have markets
[02:24:06.080 --> 02:24:09.400]   for attention and pleasure and relationships.
[02:24:09.400 --> 02:24:11.000]   - That kills the magic somehow.
[02:24:11.000 --> 02:24:12.160]   Why is that?
[02:24:12.160 --> 02:24:14.600]   Why is the transactional search
[02:24:14.600 --> 02:24:18.960]   for optimizing allocation of attention
[02:24:18.960 --> 02:24:20.920]   somehow misses the romantic magic
[02:24:20.920 --> 02:24:21.760]   of what human relations are?
[02:24:21.760 --> 02:24:24.280]   - It's also a question how magical was it before?
[02:24:24.280 --> 02:24:26.500]   Was it that you just could rely on instincts
[02:24:26.500 --> 02:24:28.040]   that used your intuitions
[02:24:28.040 --> 02:24:30.820]   and you didn't need to rationally reflect?
[02:24:30.820 --> 02:24:33.480]   But once you understand it's no longer magical
[02:24:33.480 --> 02:24:35.280]   because you actually understand
[02:24:35.280 --> 02:24:37.780]   why you were attracted to this person at this age
[02:24:37.780 --> 02:24:39.300]   and not to that person at this age
[02:24:39.300 --> 02:24:41.340]   and what the actual considerations were
[02:24:41.340 --> 02:24:42.680]   that went on in your mind
[02:24:42.680 --> 02:24:44.340]   and what the calculations were,
[02:24:44.340 --> 02:24:46.100]   what's the likelihood that you're going to have
[02:24:46.100 --> 02:24:48.260]   a sustainable relationship with this person,
[02:24:48.260 --> 02:24:49.780]   that this person is not going to leave you
[02:24:49.780 --> 02:24:51.060]   for somebody else,
[02:24:51.060 --> 02:24:54.160]   how are your life trajectories going to evolve and so on.
[02:24:54.160 --> 02:24:55.260]   And when you're young,
[02:24:55.260 --> 02:24:57.420]   you're unable to explicate all this
[02:24:57.420 --> 02:25:00.020]   and you have to rely on intuitions and instincts
[02:25:00.020 --> 02:25:01.760]   that in part you were born with
[02:25:01.760 --> 02:25:03.700]   and also in the wisdom of your environment
[02:25:03.700 --> 02:25:06.740]   that is going to give you some kind of reflection
[02:25:06.740 --> 02:25:07.920]   on your choices.
[02:25:07.920 --> 02:25:09.900]   And many of these things are disappearing now
[02:25:09.900 --> 02:25:12.500]   because we feel that our parents
[02:25:12.500 --> 02:25:14.380]   might have no idea about how we are living
[02:25:14.380 --> 02:25:16.280]   and the environments that we grew up in,
[02:25:16.280 --> 02:25:17.540]   the cultures that we grew up in,
[02:25:17.540 --> 02:25:20.800]   the milieus that our parents existed in
[02:25:20.800 --> 02:25:22.780]   might have no ability to teach us
[02:25:22.780 --> 02:25:24.900]   how to deal with this new world.
[02:25:24.900 --> 02:25:27.240]   And for many people that's actually true,
[02:25:27.240 --> 02:25:29.680]   but it doesn't mean that within one generation
[02:25:29.680 --> 02:25:31.280]   we build something that is more magical
[02:25:31.280 --> 02:25:33.300]   and more sustainable and more beautiful.
[02:25:33.300 --> 02:25:35.740]   Instead we often end up as an attempt
[02:25:35.740 --> 02:25:39.160]   to produce something that looks beautiful.
[02:25:39.160 --> 02:25:42.240]   Like I was very weirded out by the aesthetics
[02:25:42.240 --> 02:25:46.560]   of the Vision Pro headset by Apple.
[02:25:46.560 --> 02:25:48.720]   And not so much because I don't like the technology,
[02:25:48.720 --> 02:25:51.040]   I'm very curious about what it's going to be like
[02:25:51.040 --> 02:25:53.400]   and don't have an opinion yet.
[02:25:53.400 --> 02:25:57.160]   But the aesthetics of the presentation and so on,
[02:25:57.160 --> 02:25:59.860]   they're so uncanny valley-esque to me.
[02:25:59.860 --> 02:26:03.860]   The characters being extremely plastic,
[02:26:03.860 --> 02:26:08.860]   living in some hypothetical mid-century furniture museum.
[02:26:08.860 --> 02:26:09.700]   - Yeah.
[02:26:09.700 --> 02:26:16.980]   This is the proliferation of marketing teams.
[02:26:16.980 --> 02:26:19.540]   - Yes, but it was a CGI-generated world.
[02:26:19.540 --> 02:26:22.080]   And it was a CGI-generated world that doesn't exist.
[02:26:22.080 --> 02:26:24.780]   And when I complained about this,
[02:26:24.780 --> 02:26:25.940]   some friends came back to me and said,
[02:26:25.940 --> 02:26:27.500]   "But these are startup founders.
[02:26:27.500 --> 02:26:30.300]   "This is what they live like in Silicon Valley."
[02:26:30.300 --> 02:26:31.760]   And I tried to tell them,
[02:26:31.760 --> 02:26:33.900]   "No, I know lots of people in Silicon Valley.
[02:26:33.900 --> 02:26:35.360]   "This is not what people are like.
[02:26:35.360 --> 02:26:37.900]   "They're still people, they're still human beings."
[02:26:38.740 --> 02:26:40.180]   (sighs)
[02:26:40.180 --> 02:26:43.620]   - So the grounding in physical reality
[02:26:43.620 --> 02:26:46.420]   somehow is important too.
[02:26:46.420 --> 02:26:47.260]   - And culture.
[02:26:47.260 --> 02:26:49.740]   And so basically what's absent in this thing is culture.
[02:26:49.740 --> 02:26:51.420]   There is a simulation of culture,
[02:26:51.420 --> 02:26:54.460]   an attempt to replace culture by catalog,
[02:26:54.460 --> 02:26:58.220]   by some kind of aesthetic optimization
[02:26:58.220 --> 02:27:01.060]   that is not the result of having a sustainable life,
[02:27:01.060 --> 02:27:02.780]   a sustainable human relationships,
[02:27:02.780 --> 02:27:04.460]   with houses that work for you,
[02:27:04.460 --> 02:27:07.900]   and a mode of living that works for you
[02:27:07.900 --> 02:27:11.660]   in which this product, these glasses fit in naturally.
[02:27:11.660 --> 02:27:13.940]   And I guess that's also why so many people
[02:27:13.940 --> 02:27:15.180]   are weirded out about the product,
[02:27:15.180 --> 02:27:16.180]   because they don't know,
[02:27:16.180 --> 02:27:18.300]   how is this actually going to fit into my life
[02:27:18.300 --> 02:27:19.900]   and into my human relationships?
[02:27:19.900 --> 02:27:23.380]   Because the way in which it was presented in these videos
[02:27:23.380 --> 02:27:24.740]   didn't seem to be credible.
[02:27:24.740 --> 02:27:30.040]   - Do you think AI, when it's deployed by companies
[02:27:30.040 --> 02:27:32.420]   like Microsoft and Google and Meta,
[02:27:32.420 --> 02:27:36.740]   will have the same issue of being weirdly corporate?
[02:27:36.740 --> 02:27:39.900]   Like there'd be some uncanny valley,
[02:27:39.900 --> 02:27:42.260]   some weirdness to the whole presentation.
[02:27:42.260 --> 02:27:44.540]   So this is, I've gotten a chance to talk to George Haas.
[02:27:44.540 --> 02:27:46.260]   He believes everything should be open source
[02:27:46.260 --> 02:27:49.060]   and decentralized, and there, then,
[02:27:49.060 --> 02:27:51.980]   we shall have the AI of the people.
[02:27:51.980 --> 02:27:55.260]   And it'll maintain a grounding to the magic
[02:27:55.260 --> 02:27:59.940]   that's humanity, that's the human condition,
[02:27:59.940 --> 02:28:02.600]   that corporations will destroy the magic.
[02:28:02.600 --> 02:28:06.900]   - I believe that if we make everything open source
[02:28:06.900 --> 02:28:09.460]   and make this mandatory, we are going to lose
[02:28:09.460 --> 02:28:14.340]   about a lot of beautiful art and a lot of beautiful designs.
[02:28:14.340 --> 02:28:17.920]   There is a reason why Linux desktop is still ugly.
[02:28:17.920 --> 02:28:20.180]   Right? - Strong words,
[02:28:20.180 --> 02:28:21.020]   Miroslava. - And it's difficult
[02:28:21.020 --> 02:28:25.920]   to create coherence in open source designs so far,
[02:28:25.920 --> 02:28:28.120]   when the designs have to get very large.
[02:28:28.120 --> 02:28:30.860]   And it's easier to make this happening
[02:28:30.860 --> 02:28:34.060]   in a company with centralized organization.
[02:28:34.060 --> 02:28:37.420]   And from my own perspective, what we should ensure
[02:28:37.420 --> 02:28:39.780]   is that open source never dies,
[02:28:39.780 --> 02:28:43.140]   that it can always compete and has a place
[02:28:43.140 --> 02:28:45.180]   with the other forms of organization,
[02:28:45.180 --> 02:28:47.060]   because I think it is absolutely vital
[02:28:47.060 --> 02:28:49.700]   that open source exists and that we have systems
[02:28:49.700 --> 02:28:53.780]   that people have under control outside of the corporation,
[02:28:53.780 --> 02:28:56.580]   and that is also producing viable competition
[02:28:56.580 --> 02:28:58.480]   to the corporations.
[02:28:58.480 --> 02:29:01.320]   - So the corporations, the centralized control,
[02:29:01.320 --> 02:29:05.760]   the dictatorships of corporations can create beauty.
[02:29:05.760 --> 02:29:10.160]   As a centralized design is a source of a lot of beauty.
[02:29:10.160 --> 02:29:14.760]   And then I guess open source is a source of freedom,
[02:29:14.760 --> 02:29:18.600]   a hedge against the corrupting nature of power
[02:29:18.600 --> 02:29:20.600]   that comes with centralized.
[02:29:20.600 --> 02:29:23.760]   - I grew up in socialism and I learned
[02:29:23.760 --> 02:29:25.480]   that corporations are totally evil
[02:29:25.480 --> 02:29:27.420]   and I found this very, very convincing.
[02:29:27.420 --> 02:29:29.500]   And then you look at corporations like Enron
[02:29:29.500 --> 02:29:33.300]   and Halliburton maybe and realize, yeah, they are evil.
[02:29:33.300 --> 02:29:35.200]   But you also notice that many other corporations
[02:29:35.200 --> 02:29:38.540]   are not evil, they're surprisingly benevolent.
[02:29:38.540 --> 02:29:39.800]   Why are they so benevolent?
[02:29:39.800 --> 02:29:43.180]   Is this because everybody is fighting them all the time?
[02:29:43.180 --> 02:29:44.900]   I don't think that's the only explanation.
[02:29:44.900 --> 02:29:46.580]   It's because they're actually animals
[02:29:46.580 --> 02:29:48.500]   that live in a large ecosystem
[02:29:48.500 --> 02:29:50.540]   and that are still largely controlled by people
[02:29:50.540 --> 02:29:52.340]   that want that ecosystem to flourish
[02:29:52.340 --> 02:29:54.580]   and be viable for people.
[02:29:54.580 --> 02:29:58.160]   So I think that Pat Gelsinger is completely sincere
[02:29:58.160 --> 02:30:00.920]   when he leads Intel to be a tool
[02:30:00.920 --> 02:30:04.500]   that supplies the free world with semiconductors.
[02:30:04.500 --> 02:30:07.180]   And it's not necessary that all the semiconductors
[02:30:07.180 --> 02:30:08.540]   are coming from Intel.
[02:30:08.540 --> 02:30:10.540]   Intel needs to be there to make sure
[02:30:10.540 --> 02:30:11.740]   that we always have them.
[02:30:11.740 --> 02:30:15.140]   So there can be many ways in which we can import
[02:30:15.140 --> 02:30:18.020]   and trade semiconductors from other companies and places.
[02:30:18.020 --> 02:30:20.760]   We just need to make sure that nobody can cut us off from it
[02:30:20.760 --> 02:30:22.480]   because that would be a disaster
[02:30:22.480 --> 02:30:24.620]   for this kind of society and world.
[02:30:24.620 --> 02:30:27.820]   So there are many things that need to be done
[02:30:27.820 --> 02:30:30.500]   to make our style of life possible.
[02:30:30.500 --> 02:30:34.460]   And with this, I don't mean just capitalism,
[02:30:34.460 --> 02:30:36.180]   environmental destruction, consumerism,
[02:30:36.180 --> 02:30:37.820]   and creature comforts.
[02:30:37.820 --> 02:30:42.140]   I mean an idea of life in which we are determined
[02:30:42.140 --> 02:30:44.540]   not by some kind of king or dictator,
[02:30:44.540 --> 02:30:47.280]   but in which individuals can determine themselves
[02:30:47.280 --> 02:30:49.100]   to the largest possible degree.
[02:30:49.100 --> 02:30:51.460]   And to me, this is something that this Western world
[02:30:51.460 --> 02:30:53.340]   is still trying to embody.
[02:30:53.340 --> 02:30:54.820]   And it's a very valuable idea
[02:30:54.820 --> 02:30:57.140]   that we shouldn't give up too early.
[02:30:57.140 --> 02:30:59.380]   And from this perspective,
[02:30:59.380 --> 02:31:02.920]   the US is a system of interleaving clubs.
[02:31:02.920 --> 02:31:05.380]   And an entrepreneur is a special club founder.
[02:31:05.380 --> 02:31:09.420]   It's somebody who makes a club that is producing things
[02:31:09.420 --> 02:31:11.300]   that are economically viable.
[02:31:11.300 --> 02:31:13.220]   And to do this, it requires a lot of people
[02:31:13.220 --> 02:31:16.260]   who are dedicating a significant part of their life
[02:31:16.260 --> 02:31:19.020]   for working for this particular kind of club.
[02:31:19.020 --> 02:31:21.260]   And the entrepreneur is picking the initial set of rules
[02:31:21.260 --> 02:31:23.740]   and the mission and vision and aesthetics for the club
[02:31:23.740 --> 02:31:25.580]   and make sure that it works.
[02:31:25.580 --> 02:31:28.420]   But the people that are in there need to be protected.
[02:31:28.420 --> 02:31:30.540]   If they sacrifice part of their life,
[02:31:30.540 --> 02:31:32.140]   there need to be rules that tell
[02:31:32.140 --> 02:31:34.380]   how they're being taken care of
[02:31:34.380 --> 02:31:36.060]   even after they leave the club and so on.
[02:31:36.060 --> 02:31:38.600]   So there's a large body of rules
[02:31:38.600 --> 02:31:41.780]   that have been created by our rule-giving clubs
[02:31:41.780 --> 02:31:45.180]   and that are enforced by our enforcement clubs and so on.
[02:31:45.180 --> 02:31:47.100]   And some of these clubs have to be monopolies
[02:31:47.100 --> 02:31:48.380]   for game-theoretic reasons,
[02:31:48.380 --> 02:31:50.860]   which also makes them more open to corruption
[02:31:50.860 --> 02:31:52.900]   and less harder to update.
[02:31:52.900 --> 02:31:54.380]   And this is an ongoing discussion
[02:31:54.380 --> 02:31:56.140]   and process that takes place.
[02:31:56.140 --> 02:31:57.460]   But the beauty of this idea
[02:31:57.460 --> 02:31:59.660]   that there is no centralized king
[02:31:59.660 --> 02:32:02.540]   that is extracting from the peasants
[02:32:02.540 --> 02:32:06.220]   and breeding the peasants into serving the king
[02:32:06.220 --> 02:32:09.460]   and fulfilling all the roles like Anson and Antel,
[02:32:09.460 --> 02:32:12.300]   but that there is a freedom of association
[02:32:12.300 --> 02:32:14.220]   and corporations are one of them,
[02:32:14.220 --> 02:32:17.140]   is something that took me some time to realize.
[02:32:17.140 --> 02:32:20.580]   So I do think that corporations are dangerous.
[02:32:20.580 --> 02:32:23.460]   They need to be protections against overreach
[02:32:23.460 --> 02:32:27.420]   of corporations that can do regulatory capture
[02:32:27.420 --> 02:32:30.940]   and prevent open source from competing with corporations
[02:32:30.940 --> 02:32:33.460]   by imposing rules that make it impossible
[02:32:33.460 --> 02:32:36.980]   for a small group of kids to come together
[02:32:36.980 --> 02:32:38.140]   to build their own language model
[02:32:38.140 --> 02:32:40.980]   because open AI has convinced the US
[02:32:40.980 --> 02:32:43.300]   that you need to have some kind of FDA process
[02:32:43.300 --> 02:32:45.820]   that you need to go through that costs many million dollars
[02:32:45.820 --> 02:32:48.060]   before you are able to train a language model.
[02:32:48.060 --> 02:32:50.500]   And so this is important to make sure
[02:32:50.500 --> 02:32:51.460]   that this doesn't happen.
[02:32:51.460 --> 02:32:54.740]   So I think that open AI and Google are good things.
[02:32:54.740 --> 02:32:58.300]   If these good things are kept in check in such a way
[02:32:58.300 --> 02:33:00.460]   that all the other clubs can still be founded
[02:33:00.460 --> 02:33:02.820]   and all the other forms of clubs that are desirable
[02:33:02.820 --> 02:33:04.380]   can still coexist with them.
[02:33:04.380 --> 02:33:08.460]   - So what do you think about meta in contrast to that
[02:33:08.460 --> 02:33:12.300]   open sourcing most of its language models
[02:33:12.300 --> 02:33:14.540]   and most of the AI models that's working on
[02:33:14.540 --> 02:33:16.300]   and actually suggesting that they will continue
[02:33:16.300 --> 02:33:19.900]   to do so in the future for future versions of Lama,
[02:33:19.900 --> 02:33:22.020]   for example, their large language model.
[02:33:22.020 --> 02:33:25.780]   Is that exciting to you?
[02:33:25.780 --> 02:33:26.980]   Is that concerning?
[02:33:26.980 --> 02:33:29.540]   - I don't find it very concerning,
[02:33:29.540 --> 02:33:32.100]   but it's also because I think that the language models
[02:33:32.100 --> 02:33:34.860]   are not very dangerous yet.
[02:33:34.860 --> 02:33:36.820]   - Yet?
[02:33:36.820 --> 02:33:37.660]   - Yes.
[02:33:37.660 --> 02:33:41.260]   So as I said, I have no proof that there is the boundary
[02:33:41.260 --> 02:33:44.740]   between the language models and AI, AGI.
[02:33:44.740 --> 02:33:49.020]   It's possible that somebody builds a version of baby AGI,
[02:33:49.020 --> 02:33:52.580]   I think, and so it's an algorithmic improvements
[02:33:52.580 --> 02:33:54.940]   that scale these systems up in ways
[02:33:54.940 --> 02:33:56.340]   that otherwise wouldn't have happened
[02:33:56.340 --> 02:33:58.460]   without these language model components.
[02:33:58.460 --> 02:34:02.380]   So it's not really clear for me what the end game is there
[02:34:02.380 --> 02:34:06.380]   and if these models can put force their way into AGI.
[02:34:06.380 --> 02:34:10.540]   And there's also a possibility that the AGI
[02:34:10.540 --> 02:34:13.700]   that we are building with these language models
[02:34:13.700 --> 02:34:15.820]   are not taking responsibility for what they are
[02:34:15.820 --> 02:34:18.740]   because they don't understand the greater game.
[02:34:18.740 --> 02:34:20.620]   And so to me, it would be interesting
[02:34:20.620 --> 02:34:24.020]   to try to understand how to build systems
[02:34:24.020 --> 02:34:26.620]   that understand what the greater games are,
[02:34:26.620 --> 02:34:29.620]   what are the longest games that we can play on this planet.
[02:34:29.620 --> 02:34:33.740]   - Games broadly, like deeply define
[02:34:33.740 --> 02:34:35.580]   the way you did with the games.
[02:34:35.580 --> 02:34:36.740]   - In the game theoretic sense.
[02:34:36.740 --> 02:34:38.460]   So when we are interacting with each other,
[02:34:38.460 --> 02:34:39.940]   in some sense, we are playing games.
[02:34:39.940 --> 02:34:42.060]   We are making lots and lots of interactions.
[02:34:42.060 --> 02:34:43.500]   This doesn't mean that these interactions
[02:34:43.500 --> 02:34:45.340]   have all to be transactional.
[02:34:45.340 --> 02:34:48.060]   Every one of us is playing some kind of game
[02:34:48.060 --> 02:34:51.940]   by virtue of identifying this particular kinds of goals
[02:34:51.940 --> 02:34:55.460]   that we have or aesthetics from which we derive the goals.
[02:34:55.460 --> 02:34:58.180]   So when you say, I'm Lex Friedman,
[02:34:58.180 --> 02:35:00.300]   I'm doing a set of podcasts,
[02:35:00.300 --> 02:35:02.700]   then you feel that it's part of something larger
[02:35:02.700 --> 02:35:03.540]   that you want to build.
[02:35:03.540 --> 02:35:04.940]   Maybe you want to inspire people.
[02:35:04.940 --> 02:35:07.620]   Maybe you want them to see more possibilities
[02:35:07.620 --> 02:35:10.300]   and get them together over shared ideas.
[02:35:10.300 --> 02:35:12.540]   Maybe your game is that you want to become super rich
[02:35:12.540 --> 02:35:15.540]   and famous by being the best postcard cast on earth.
[02:35:15.540 --> 02:35:16.460]   Maybe you have other games.
[02:35:16.460 --> 02:35:18.860]   Maybe it's switches from time to time, right?
[02:35:18.860 --> 02:35:20.300]   But there is a certain perspective
[02:35:20.300 --> 02:35:21.180]   where you might be thinking,
[02:35:21.180 --> 02:35:24.100]   what is the longest possible game that you could be playing?
[02:35:24.100 --> 02:35:25.820]   A short game is, for instance,
[02:35:25.820 --> 02:35:27.900]   cancer is playing a shorter game than your organism.
[02:35:27.900 --> 02:35:30.900]   It's cancer is an organism playing a shorter game
[02:35:30.900 --> 02:35:32.660]   than the regular organism.
[02:35:32.660 --> 02:35:36.100]   And because the cancer cannot procreate beyond the organism,
[02:35:36.100 --> 02:35:39.020]   except for some infectious cancers,
[02:35:39.020 --> 02:35:41.580]   like the ones that eradicated the Tasmanian devils,
[02:35:41.580 --> 02:35:45.860]   you typically end up with a situation
[02:35:45.860 --> 02:35:47.900]   where the organism dies together with the cancer,
[02:35:47.900 --> 02:35:50.620]   because the cancer has destroyed the larger system
[02:35:50.620 --> 02:35:52.540]   due to playing a shorter game.
[02:35:52.540 --> 02:35:54.900]   And so ideally you want to, I think,
[02:35:54.900 --> 02:35:58.620]   build agents that play the longest possible games.
[02:35:58.620 --> 02:36:01.900]   And the longest possible games is to keep entropy at bay
[02:36:01.900 --> 02:36:05.100]   as long as possible while doing interesting stuff.
[02:36:05.100 --> 02:36:07.860]   - But the longest, yes, that part,
[02:36:07.860 --> 02:36:10.940]   the longest possible game while doing interesting stuff,
[02:36:10.940 --> 02:36:12.540]   and while maintaining at least
[02:36:12.540 --> 02:36:14.020]   the same amount of interesting.
[02:36:14.020 --> 02:36:15.100]   - Yes. - So complexity,
[02:36:15.100 --> 02:36:17.380]   it's a problem getting-- - So currently I'm
[02:36:17.380 --> 02:36:19.500]   pretty much identified as a conscious being.
[02:36:19.500 --> 02:36:23.260]   It's the minimal identification
[02:36:23.260 --> 02:36:24.820]   that I managed to get together,
[02:36:24.820 --> 02:36:27.020]   because if I turn this off, I fall asleep.
[02:36:27.020 --> 02:36:29.620]   And when I'm asleep, I'm a vegetable.
[02:36:29.620 --> 02:36:31.980]   I'm no longer here as an agent.
[02:36:31.980 --> 02:36:35.540]   So my agency is basically predicated on being conscious.
[02:36:35.540 --> 02:36:39.580]   And what I care about is other conscious agents.
[02:36:39.580 --> 02:36:42.060]   They're the only moral agents for me.
[02:36:43.020 --> 02:36:48.020]   So if an AI were to treat me as a moral agent
[02:36:48.020 --> 02:36:51.420]   that it is interested in coexisting with
[02:36:51.420 --> 02:36:53.660]   and cooperating with and mutually supporting each other,
[02:36:53.660 --> 02:36:57.420]   maybe it is, I think, necessary that the AI thinks
[02:36:57.420 --> 02:37:00.260]   that consciousness is a viable mode of existence
[02:37:00.260 --> 02:37:01.340]   and important.
[02:37:01.340 --> 02:37:04.820]   So I think it would be very important to build conscious AI
[02:37:04.820 --> 02:37:07.500]   and do this as the primary goal.
[02:37:07.500 --> 02:37:10.980]   So not just say we want to build a useful tool
[02:37:10.980 --> 02:37:12.980]   that we can use for all sorts of things.
[02:37:12.980 --> 02:37:15.660]   And then you have to make sure that the impact
[02:37:15.660 --> 02:37:18.380]   on the labor market is something that is not too disruptive
[02:37:18.380 --> 02:37:21.100]   and manageable, and the impact on the copyright holder
[02:37:21.100 --> 02:37:24.020]   is manageable and not too disruptive and so on.
[02:37:24.020 --> 02:37:27.060]   I don't think that's the most important game to be played.
[02:37:27.060 --> 02:37:30.940]   I think that we will see extremely large disruptions
[02:37:30.940 --> 02:37:34.020]   of the status quo that are quite unpredictable
[02:37:34.020 --> 02:37:35.260]   at this point.
[02:37:35.260 --> 02:37:38.460]   And I just personally want to make sure
[02:37:38.460 --> 02:37:40.260]   that some of the stuff on the other side
[02:37:40.260 --> 02:37:41.980]   is interesting and conscious.
[02:37:41.980 --> 02:37:45.980]   - How do we ride, as individuals and as a society,
[02:37:45.980 --> 02:37:49.820]   this disruptive wave that changes the nature of the game?
[02:37:49.820 --> 02:37:50.660]   - I absolutely don't know.
[02:37:50.660 --> 02:37:53.460]   So everybody is going to do their best, as always.
[02:37:53.460 --> 02:37:55.140]   - Do we build a bunker in the woods?
[02:37:55.140 --> 02:37:56.560]   Do we meditate more?
[02:37:56.560 --> 02:38:00.380]   Drugs, mushrooms, psychedelics?
[02:38:00.380 --> 02:38:03.300]   I mean, lots of sex.
[02:38:03.300 --> 02:38:04.660]   What are we talking about here?
[02:38:04.660 --> 02:38:06.300]   Do you play Diablo IV?
[02:38:06.300 --> 02:38:10.780]   I'm hoping that will help me escape for a brief moment.
[02:38:10.780 --> 02:38:12.340]   What, play video games?
[02:38:12.340 --> 02:38:13.660]   What?
[02:38:13.660 --> 02:38:14.580]   Do you have ideas?
[02:38:14.580 --> 02:38:18.580]   - I really like playing "Disco Elysium."
[02:38:18.580 --> 02:38:21.980]   It was one of the most beautiful computer games
[02:38:21.980 --> 02:38:24.380]   I played in recent years.
[02:38:24.380 --> 02:38:28.940]   And it's a noir novel that is a philosophical perspective
[02:38:28.940 --> 02:38:32.140]   on Western society from the perspective of an Estonian.
[02:38:32.140 --> 02:38:36.660]   And he first of all wrote a book about this world
[02:38:36.660 --> 02:38:41.380]   that is a parallel universe that is quite poetic
[02:38:41.380 --> 02:38:45.460]   and fascinating and is condensing his perspective
[02:38:45.460 --> 02:38:46.620]   on our societies.
[02:38:46.620 --> 02:38:48.460]   It was very, very nice.
[02:38:48.460 --> 02:38:50.100]   He spent a lot of time writing it.
[02:38:50.100 --> 02:38:52.660]   He had, I think, sold a couple thousand books
[02:38:52.660 --> 02:38:54.900]   and as a result became an alcoholic.
[02:38:54.900 --> 02:38:57.540]   And then he had the idea, or one of his friends
[02:38:57.540 --> 02:39:00.340]   had the idea of turning this into an RPG.
[02:39:00.340 --> 02:39:02.940]   And it's mind-blowing.
[02:39:02.940 --> 02:39:05.740]   They spent, the illustrator, more than a year
[02:39:05.740 --> 02:39:10.740]   just on making the art for the scenes in between.
[02:39:10.740 --> 02:39:15.100]   - So aesthetically, it captures you, pulls you in.
[02:39:15.100 --> 02:39:16.820]   - But it's a philosophical work of art.
[02:39:16.820 --> 02:39:18.100]   It's a reflection of society.
[02:39:18.100 --> 02:39:20.540]   It's fascinating to spend time in this world.
[02:39:20.540 --> 02:39:24.700]   And so for me, it was using a medium in a new way
[02:39:24.700 --> 02:39:28.660]   and telling a story that left me enriched.
[02:39:28.660 --> 02:39:33.660]   When I tried Diablo, I didn't feel enriched playing it.
[02:39:33.660 --> 02:39:37.220]   I felt that the time playing it was not unpleasant,
[02:39:37.220 --> 02:39:38.620]   but there's also more pleasant stuff
[02:39:38.620 --> 02:39:40.020]   that I can do in that time.
[02:39:40.020 --> 02:39:42.500]   So ultimately I feel that I'm being gamed.
[02:39:42.500 --> 02:39:43.500]   I'm not gaming.
[02:39:43.500 --> 02:39:45.020]   - Oh, the addiction thing.
[02:39:45.020 --> 02:39:46.580]   - Yes, I basically feel that there is
[02:39:46.580 --> 02:39:49.020]   a very transparent economy that's going on.
[02:39:49.020 --> 02:39:51.300]   The story of Diablo was branded.
[02:39:51.300 --> 02:39:53.980]   So it's not really interesting to me.
[02:39:53.980 --> 02:39:56.220]   - My heart is slowly breaking
[02:39:56.220 --> 02:39:58.940]   by the deep truth you're conveying to me.
[02:39:58.940 --> 02:40:02.340]   Why can't you just allow me to enjoy my personal addiction?
[02:40:02.340 --> 02:40:04.940]   - Go ahead, by all means, go nuts.
[02:40:04.940 --> 02:40:06.460]   I have no objection here.
[02:40:06.460 --> 02:40:10.660]   I'm just trying to describe what's happening.
[02:40:10.660 --> 02:40:14.420]   And it's not that I don't do things that I later say,
[02:40:14.420 --> 02:40:16.820]   oh, I wish I would have done something different.
[02:40:16.820 --> 02:40:18.780]   I also know that when we die,
[02:40:18.780 --> 02:40:20.620]   the greatest regret that people typically have
[02:40:20.620 --> 02:40:21.940]   on their deathbed is to say,
[02:40:21.940 --> 02:40:24.180]   oh, I wish I had spent more time on Twitter.
[02:40:25.260 --> 02:40:26.740]   No, I don't think that's the case.
[02:40:26.740 --> 02:40:30.060]   I think I should probably have spent less time on Twitter.
[02:40:30.060 --> 02:40:34.100]   But I found it so useful for myself and also so addictive
[02:40:34.100 --> 02:40:35.980]   that I felt I need to make the best of it
[02:40:35.980 --> 02:40:38.540]   and turn it into an art form and thought form.
[02:40:38.540 --> 02:40:41.300]   And it did help me to develop something.
[02:40:41.300 --> 02:40:44.020]   But I wish what other things I could have done
[02:40:44.020 --> 02:40:45.660]   in the meantime, it's just not the universe
[02:40:45.660 --> 02:40:46.860]   that we are in anymore.
[02:40:46.860 --> 02:40:48.700]   Most people don't read books anymore.
[02:40:48.700 --> 02:40:53.420]   - What do you think that means
[02:40:53.420 --> 02:40:55.180]   that we don't read books anymore?
[02:40:55.180 --> 02:40:56.380]   What do you think that means
[02:40:56.380 --> 02:40:58.620]   about the collective intelligence of our species?
[02:40:58.620 --> 02:41:01.300]   Is it possible it's still progressing and growing?
[02:41:01.300 --> 02:41:02.540]   - Well, it clearly is.
[02:41:02.540 --> 02:41:03.980]   There is stuff happening on Twitter
[02:41:03.980 --> 02:41:05.940]   that was impossible with books.
[02:41:05.940 --> 02:41:09.900]   And I really regret that Twitter has not taken the turn
[02:41:09.900 --> 02:41:10.900]   that I was hoping for.
[02:41:10.900 --> 02:41:13.140]   I thought Elon is global brain-pilled
[02:41:13.140 --> 02:41:16.140]   and understands that this thing needs to self-organize
[02:41:16.140 --> 02:41:20.300]   and he needs to develop tools to allow the profligation
[02:41:20.300 --> 02:41:23.580]   of the self-organization so Twitter can become sentient.
[02:41:23.580 --> 02:41:26.860]   And maybe this was a pipe dream from the beginning,
[02:41:26.860 --> 02:41:30.820]   but I felt that the enormous pressure that he was under
[02:41:30.820 --> 02:41:32.620]   made it impossible for him to work
[02:41:32.620 --> 02:41:34.420]   on any kind of content goals.
[02:41:34.420 --> 02:41:37.700]   And also many of the decisions that he made
[02:41:37.700 --> 02:41:40.700]   under this pressure seem to be not very wise.
[02:41:40.700 --> 02:41:43.900]   I don't think that as a CEO of a social media company,
[02:41:43.900 --> 02:41:46.900]   you should have opinions in the culture or in public.
[02:41:46.900 --> 02:41:48.860]   I think that's very short-sighted.
[02:41:48.860 --> 02:41:52.900]   And I also suspect that it's not a good idea
[02:41:52.900 --> 02:41:57.900]   to block Paul Graham of all people
[02:41:57.900 --> 02:42:02.620]   over setting a mastodon link.
[02:42:02.620 --> 02:42:04.780]   And I think Paul made this intentionally
[02:42:04.780 --> 02:42:07.460]   because he wanted to show Elon Musk
[02:42:07.460 --> 02:42:09.340]   that blocking people for setting a link
[02:42:09.340 --> 02:42:12.620]   is completely counter to any idea of free speech
[02:42:12.620 --> 02:42:14.460]   that he intended to bring to Twitter.
[02:42:14.460 --> 02:42:18.700]   And basically seeing that Elon was very less principled
[02:42:18.700 --> 02:42:22.220]   in his thinking there and is much more experimental.
[02:42:22.220 --> 02:42:24.940]   And many of the things that he is trying,
[02:42:24.940 --> 02:42:29.540]   they pan out very differently in a digital society
[02:42:29.540 --> 02:42:31.620]   than they pan out in a car company
[02:42:31.620 --> 02:42:33.100]   because the effect is very different
[02:42:33.100 --> 02:42:35.260]   because everything that you do in a digital society
[02:42:35.260 --> 02:42:38.220]   is going to have real-world cultural effects.
[02:42:38.220 --> 02:42:41.220]   And so basically I find it quite regrettable
[02:42:41.220 --> 02:42:45.660]   that this guy is able to become de facto the Pope,
[02:42:45.660 --> 02:42:47.700]   but Twitter has more active members
[02:42:47.700 --> 02:42:49.340]   than the Catholic Church.
[02:42:49.340 --> 02:42:51.620]   And he doesn't get it.
[02:42:51.620 --> 02:42:54.100]   The power and responsibility that he has
[02:42:54.100 --> 02:42:56.540]   and the ability to create something
[02:42:56.540 --> 02:42:58.300]   in a society that is lasting
[02:42:58.300 --> 02:43:01.180]   and that is producing a digital agora in a way
[02:43:01.180 --> 02:43:02.620]   that has never existed before,
[02:43:02.620 --> 02:43:05.460]   where we built a social network on top of a social network,
[02:43:05.460 --> 02:43:09.980]   an actual society on top of the algorithms.
[02:43:09.980 --> 02:43:12.420]   So this is something that is hope still in the future
[02:43:12.420 --> 02:43:13.900]   and still in the cards,
[02:43:13.900 --> 02:43:17.300]   but it's something that exists in small parts.
[02:43:17.300 --> 02:43:19.140]   I find that the corner of Twitter that I'm in
[02:43:19.140 --> 02:43:20.100]   is extremely pleasant.
[02:43:20.100 --> 02:43:22.460]   It's just when I take a few steps outside of it,
[02:43:22.460 --> 02:43:24.020]   it is not very wholesome anymore.
[02:43:24.020 --> 02:43:26.340]   And the way in which people interact with strangers
[02:43:26.340 --> 02:43:29.900]   suggest that it's not a civilized society yet.
[02:43:29.900 --> 02:43:33.380]   - So as the number of people who follow you on Twitter
[02:43:33.380 --> 02:43:36.420]   expands, you feel the burden
[02:43:36.420 --> 02:43:40.060]   of the uglier sides of humanity.
[02:43:40.060 --> 02:43:44.380]   - Yes, but there's also a similar thing in the normal world.
[02:43:44.380 --> 02:43:46.700]   That is, if you become more influential,
[02:43:46.700 --> 02:43:47.580]   if you have more status,
[02:43:47.580 --> 02:43:49.460]   if you have more fame in the real world,
[02:43:49.460 --> 02:43:52.780]   you get lots of perks,
[02:43:52.780 --> 02:43:55.700]   but you also have way less freedom
[02:43:55.700 --> 02:43:57.380]   in the way in which you interact with people,
[02:43:57.380 --> 02:43:58.940]   especially with strangers,
[02:43:58.940 --> 02:44:02.980]   because a certain percentage of people,
[02:44:02.980 --> 02:44:05.540]   it's a small single-digit percentage,
[02:44:05.540 --> 02:44:07.340]   is nuts and dangerous.
[02:44:07.340 --> 02:44:11.260]   And the more of those are looking at you,
[02:44:11.260 --> 02:44:13.260]   the more of them might get ideas.
[02:44:13.260 --> 02:44:16.340]   - But what if the technology enables you
[02:44:16.340 --> 02:44:18.740]   to discover the majority of people,
[02:44:18.740 --> 02:44:21.900]   to discover and connect efficiently and regularly
[02:44:21.900 --> 02:44:24.580]   with the majority of people who are actually really good?
[02:44:24.580 --> 02:44:28.220]   I mean, one of my sort of concerns
[02:44:28.220 --> 02:44:30.500]   with a platform like Twitter is
[02:44:30.500 --> 02:44:32.340]   there's a lot of really smart people out there,
[02:44:32.340 --> 02:44:34.140]   a lot of smart people that disagree with me
[02:44:34.140 --> 02:44:35.820]   and with others between each other.
[02:44:35.820 --> 02:44:38.460]   And I love that if the technology
[02:44:38.460 --> 02:44:40.920]   would bring those to the top,
[02:44:40.920 --> 02:44:42.680]   the beautiful disagreements,
[02:44:42.680 --> 02:44:45.780]   like Intelligence Squared type of debates.
[02:44:45.780 --> 02:44:47.240]   There's a bunch of, I mean,
[02:44:47.240 --> 02:44:48.540]   one of my favorite things to listen to
[02:44:48.540 --> 02:44:51.780]   is arguments and arguments like high-effort arguments
[02:44:51.780 --> 02:44:53.900]   with the respect and love underneath it,
[02:44:53.900 --> 02:44:55.660]   but then it gets a little too heated.
[02:44:55.660 --> 02:44:57.180]   But that kind of too heated,
[02:44:57.180 --> 02:44:59.180]   which I've seen you participate in,
[02:44:59.180 --> 02:45:01.780]   and I love that, with Lee Kroner,
[02:45:01.780 --> 02:45:03.500]   with those kinds of folks.
[02:45:03.500 --> 02:45:05.380]   And you go pretty hard.
[02:45:05.380 --> 02:45:07.780]   Like you get frustrated, but it's all beautiful.
[02:45:07.780 --> 02:45:11.140]   - Obviously, I can do this because we know each other.
[02:45:11.140 --> 02:45:11.980]   - Yes.
[02:45:11.980 --> 02:45:13.820]   - And Lee has the rare gift
[02:45:13.820 --> 02:45:15.740]   of being willing to be wrong in public.
[02:45:15.740 --> 02:45:16.580]   - Yeah.
[02:45:16.580 --> 02:45:18.100]   - So basically he has thoughts that are as wrong
[02:45:18.100 --> 02:45:20.060]   as the random thoughts of an average,
[02:45:20.060 --> 02:45:21.580]   highly intelligent person,
[02:45:21.580 --> 02:45:23.180]   but he blurts them out
[02:45:23.180 --> 02:45:25.460]   while not being sure if they're right.
[02:45:25.460 --> 02:45:27.260]   And he enjoys doing that.
[02:45:27.260 --> 02:45:30.020]   And once you understand that this is his game,
[02:45:30.020 --> 02:45:32.220]   you don't get offended by him saying something
[02:45:32.220 --> 02:45:33.900]   that you think is so wrong.
[02:45:33.900 --> 02:45:37.920]   - But he's constantly passively communicating a respect
[02:45:37.920 --> 02:45:39.660]   for the people he's talking with.
[02:45:39.660 --> 02:45:40.500]   - Yeah.
[02:45:40.500 --> 02:45:42.260]   - And for just basic humanity and truth
[02:45:42.260 --> 02:45:43.100]   and all that kind of stuff.
[02:45:43.100 --> 02:45:45.020]   And there's a self-deprecating thing.
[02:45:45.020 --> 02:45:48.300]   There's a bunch of like social skills you acquire
[02:45:48.300 --> 02:45:53.180]   that allow you to be a great debater, a great argumenter,
[02:45:53.180 --> 02:45:56.180]   like be wrong in public and explore ideas together
[02:45:56.180 --> 02:45:57.580]   in public when you disagree.
[02:45:57.580 --> 02:46:02.220]   And I would love for Twitter to elevate those folks,
[02:46:02.220 --> 02:46:03.740]   elevate those kinds of conversations.
[02:46:03.740 --> 02:46:05.780]   - It already does in some sense,
[02:46:05.780 --> 02:46:08.540]   but also if it elevates them too much,
[02:46:08.540 --> 02:46:11.420]   then you get this phenomenon of clubhouse
[02:46:11.420 --> 02:46:14.580]   where you always get dragged on stage
[02:46:14.580 --> 02:46:18.420]   and I found this very stressful because it was too intense.
[02:46:18.420 --> 02:46:20.980]   I don't like to be dragged on stage all the time.
[02:46:20.980 --> 02:46:22.620]   I think once a week is enough.
[02:46:22.620 --> 02:46:26.100]   And also when I met Lee the first time,
[02:46:26.100 --> 02:46:28.820]   I found that a lot of people seem to be shocked
[02:46:28.820 --> 02:46:32.460]   by the fact that he was being very aggressive
[02:46:32.460 --> 02:46:33.300]   as their results,
[02:46:33.300 --> 02:46:36.700]   that he didn't seem to show a lot of sensibility
[02:46:36.700 --> 02:46:39.540]   in the way in which he was criticizing what they were doing
[02:46:39.540 --> 02:46:41.940]   and being dismissive of the work of others.
[02:46:41.940 --> 02:46:44.380]   And that was not, I think, in any way,
[02:46:44.380 --> 02:46:46.260]   a shortcoming of him because I noticed
[02:46:46.260 --> 02:46:48.300]   that he was much, much more dismissive
[02:46:48.300 --> 02:46:50.140]   with respect to his own work.
[02:46:50.140 --> 02:46:51.540]   It was his general stance.
[02:46:51.540 --> 02:46:53.700]   And I felt that this general stance
[02:46:53.700 --> 02:46:55.740]   is creating a lot of liability for him
[02:46:55.740 --> 02:46:58.340]   because really a lot of people take offense
[02:46:58.340 --> 02:47:01.940]   at him being not like a dear Carnegie character
[02:47:01.940 --> 02:47:05.420]   who is always smooth and make sure that everybody likes him.
[02:47:05.420 --> 02:47:08.940]   So I really respect that he is willing to take that risk
[02:47:08.940 --> 02:47:12.660]   and to be wrong in public and to offend people.
[02:47:12.660 --> 02:47:14.460]   And he doesn't do this in any bad way.
[02:47:14.460 --> 02:47:15.860]   It's just most people feel,
[02:47:15.860 --> 02:47:17.900]   or not all people recognize this.
[02:47:17.900 --> 02:47:21.500]   And so I can be much more aggressive with him
[02:47:21.500 --> 02:47:23.060]   than I can be with many other people
[02:47:23.060 --> 02:47:24.940]   who don't play the same game
[02:47:24.940 --> 02:47:26.940]   because he understands the way and the spirit
[02:47:26.940 --> 02:47:28.860]   in which I respond to him.
[02:47:28.860 --> 02:47:30.820]   - I think that's a fun and that's a beautiful game.
[02:47:30.820 --> 02:47:32.460]   It's ultimately a productive one.
[02:47:32.460 --> 02:47:37.100]   Speaking of taking that risk, you tweeted,
[02:47:37.100 --> 02:47:39.380]   "When you have the choice between being a creator,
[02:47:39.380 --> 02:47:41.980]   "consumer, or redistributor,
[02:47:41.980 --> 02:47:43.780]   "always go for creation.
[02:47:43.780 --> 02:47:46.820]   "Not only does it lead to a more beautiful world,
[02:47:46.820 --> 02:47:50.340]   "but also to a much more satisfying life for yourself.
[02:47:50.340 --> 02:47:53.060]   "And don't get stuck preparing yourself for the journey.
[02:47:53.060 --> 02:47:55.620]   "The time is always now."
[02:47:55.620 --> 02:47:57.820]   So let me ask for advice.
[02:47:57.820 --> 02:48:01.460]   What advice would you give on how to become such a creator
[02:48:01.460 --> 02:48:03.380]   on Twitter in your own life?
[02:48:03.380 --> 02:48:06.620]   - I was very lucky to be alive
[02:48:06.620 --> 02:48:09.780]   at the time of the collapse of Eastern Germany
[02:48:09.780 --> 02:48:12.100]   and the transition into Western Germany.
[02:48:12.100 --> 02:48:15.580]   And me and my friends and most of the people I knew
[02:48:15.580 --> 02:48:18.180]   were East Germans and we were very poor
[02:48:18.180 --> 02:48:20.260]   because we didn't have money.
[02:48:20.260 --> 02:48:22.300]   And all the capital was in Western Germany
[02:48:22.300 --> 02:48:25.020]   and they bought our factories and shut them down
[02:48:25.020 --> 02:48:27.740]   because they were mostly only interested in the market
[02:48:27.740 --> 02:48:31.220]   rather than creating new production capacity.
[02:48:31.220 --> 02:48:34.500]   And so cities were poor and in disrepair
[02:48:34.500 --> 02:48:36.540]   and we could not afford things.
[02:48:36.540 --> 02:48:40.540]   And I could not afford to go into a restaurant
[02:48:40.540 --> 02:48:42.300]   and order a meal there.
[02:48:42.300 --> 02:48:44.260]   I would have to cook at home.
[02:48:44.260 --> 02:48:46.100]   But I also thought,
[02:48:46.100 --> 02:48:48.460]   why not just have a restaurant with my friends?
[02:48:48.460 --> 02:48:51.620]   So we would open up a cafe with friends and a restaurant
[02:48:51.620 --> 02:48:54.180]   and we would cook for each other in these restaurants
[02:48:54.180 --> 02:48:56.900]   and also invite the general public and they could donate.
[02:48:56.900 --> 02:48:59.060]   And eventually this became so big
[02:48:59.060 --> 02:49:03.340]   that we could turn this into some incorporated form
[02:49:03.340 --> 02:49:05.740]   and it became a regular restaurant at some point.
[02:49:05.740 --> 02:49:08.180]   Or we did the same thing with a movie theater.
[02:49:08.180 --> 02:49:12.340]   We would not be able to afford to pay 12 marks
[02:49:12.340 --> 02:49:13.700]   to watch a movie,
[02:49:13.700 --> 02:49:15.940]   but why not just create our own movie theater
[02:49:15.940 --> 02:49:17.860]   and then invite people to pay
[02:49:17.860 --> 02:49:20.100]   and we would rent the movies
[02:49:20.100 --> 02:49:24.100]   in the way in which a movie theater does.
[02:49:24.100 --> 02:49:26.180]   But it would be a community movie theater
[02:49:26.180 --> 02:49:29.060]   in which everybody who wants to help can watch for free
[02:49:29.060 --> 02:49:31.540]   and builds this thing and renovates the building.
[02:49:31.540 --> 02:49:35.500]   And so we ended up creating lots and lots of infrastructure.
[02:49:35.500 --> 02:49:39.020]   And I think when you are young and you don't have money,
[02:49:39.020 --> 02:49:40.940]   move to a place where this is still happening.
[02:49:40.940 --> 02:49:43.380]   Move to one of those places that are undeveloped
[02:49:43.380 --> 02:49:45.300]   and where you get a critical mass of other people
[02:49:45.300 --> 02:49:48.060]   who are starting to build infrastructure to live in.
[02:49:48.060 --> 02:49:49.700]   And that's super satisfying
[02:49:49.700 --> 02:49:51.420]   because you're not just creating infrastructure,
[02:49:51.420 --> 02:49:55.060]   but you're creating a small society that is building culture
[02:49:55.060 --> 02:49:57.220]   and ways to interact with each other.
[02:49:57.220 --> 02:49:59.220]   And that's much, much more satisfying
[02:49:59.220 --> 02:50:01.580]   than going into some kind of chain
[02:50:01.580 --> 02:50:06.260]   and get your needs met by ordering food
[02:50:06.260 --> 02:50:07.460]   from this chain and so on.
[02:50:07.460 --> 02:50:09.620]   - So not just consuming culture, but creating culture.
[02:50:09.620 --> 02:50:11.020]   - Yes.
[02:50:11.020 --> 02:50:12.340]   And you don't always have that choice.
[02:50:12.340 --> 02:50:14.580]   That's why I prefaced it when you do have the choice
[02:50:14.580 --> 02:50:16.700]   and there are many roles that need to be played.
[02:50:16.700 --> 02:50:19.540]   We need people who take care of redistribution in society
[02:50:19.540 --> 02:50:20.460]   and so on.
[02:50:20.460 --> 02:50:22.420]   But when you have the choice to create something,
[02:50:22.420 --> 02:50:23.540]   always go for creation.
[02:50:23.540 --> 02:50:25.260]   It's so much more satisfying.
[02:50:25.260 --> 02:50:28.500]   And it also is, this is what life is about, I think.
[02:50:28.500 --> 02:50:30.100]   - Yeah.
[02:50:30.100 --> 02:50:32.740]   Speaking of which, you retweeted this meme
[02:50:32.740 --> 02:50:37.020]   of a life of a philosopher in a nutshell.
[02:50:37.020 --> 02:50:39.040]   It's birth and death and in between.
[02:50:39.040 --> 02:50:42.900]   It's a chubby guy and it says, "Why though?"
[02:50:42.900 --> 02:50:49.260]   What do you think is the answer to that?
[02:50:49.260 --> 02:50:53.300]   - Well, the answer is that everything that can exist
[02:50:53.300 --> 02:50:54.660]   might exist.
[02:50:54.660 --> 02:50:58.420]   And in many ways, you take an ecological perspective
[02:50:58.420 --> 02:51:00.700]   the same way as when you look at human opinions
[02:51:00.700 --> 02:51:01.700]   and cultures.
[02:51:01.700 --> 02:51:04.900]   It's not that there is right and wrong opinions
[02:51:04.900 --> 02:51:07.700]   when you look at this from this ecological perspective.
[02:51:07.700 --> 02:51:10.300]   But every opinion that fits between two human ears
[02:51:10.300 --> 02:51:11.780]   might be between two human ears.
[02:51:11.780 --> 02:51:16.340]   And so when I see a strange opinion on social media,
[02:51:16.340 --> 02:51:19.320]   it's not that I feel that I have a need to get upset.
[02:51:19.320 --> 02:51:21.880]   It's often more that I, "Oh, there you are."
[02:51:21.880 --> 02:51:24.620]   And when an opinion is incentivized,
[02:51:24.620 --> 02:51:26.060]   then it's going to be abundant.
[02:51:26.060 --> 02:51:28.900]   And when you take this ecological perspective
[02:51:28.900 --> 02:51:30.820]   also on yourself and you realize you're just one
[02:51:30.820 --> 02:51:32.720]   of these mushrooms that are popping up
[02:51:32.720 --> 02:51:34.620]   and doing their thing, and you can,
[02:51:34.620 --> 02:51:36.840]   depending on where you chose to grow
[02:51:36.840 --> 02:51:38.500]   and where you happen to grow,
[02:51:38.500 --> 02:51:41.700]   you can flourish or not doing this or that strategy.
[02:51:41.700 --> 02:51:43.820]   And it's still all the same life at some level.
[02:51:43.820 --> 02:51:46.160]   It's all the same experience of being a conscious being
[02:51:46.160 --> 02:51:47.000]   in the world.
[02:51:47.000 --> 02:51:50.380]   And you do have some choice about who you want to be
[02:51:50.380 --> 02:51:53.060]   more than any other animal has.
[02:51:53.060 --> 02:51:54.860]   That to me is fascinating.
[02:51:54.860 --> 02:51:57.660]   And so I think that rather than asking yourself,
[02:51:57.660 --> 02:51:59.820]   "What is the one way to be?"
[02:51:59.820 --> 02:52:03.380]   Think about what are the possibilities that I have,
[02:52:03.380 --> 02:52:06.100]   what would be the most interesting way to be that I can be.
[02:52:06.100 --> 02:52:08.540]   - 'Cause everything is possible, so you get to explore.
[02:52:08.540 --> 02:52:09.860]   - Not everything is possible.
[02:52:09.860 --> 02:52:12.560]   Many things fail, most things fail.
[02:52:12.560 --> 02:52:16.020]   But often there are possibilities that we are not seeing,
[02:52:16.020 --> 02:52:18.740]   especially if we choose who we are.
[02:52:18.740 --> 02:52:22.920]   - To the degree we can choose.
[02:52:22.920 --> 02:52:23.760]   - Yeah.
[02:52:23.760 --> 02:52:30.880]   - Yasha, you're one of my favorite humans in this world.
[02:52:30.880 --> 02:52:34.280]   Consciousness is to merge with for a brief moment of time.
[02:52:34.280 --> 02:52:35.760]   It's always an honor.
[02:52:35.760 --> 02:52:37.560]   It always blows my mind.
[02:52:37.560 --> 02:52:40.720]   It will take me days, if not weeks, to recover.
[02:52:40.720 --> 02:52:43.480]   (laughing)
[02:52:43.480 --> 02:52:46.300]   And I already miss our chats.
[02:52:46.300 --> 02:52:47.140]   Thank you so much.
[02:52:47.140 --> 02:52:50.780]   Thank you so much for speaking with me so many times.
[02:52:50.780 --> 02:52:53.460]   Thank you so much for all the ideas
[02:52:53.460 --> 02:52:55.060]   you put out into the world.
[02:52:55.060 --> 02:52:58.420]   And I'm a huge fan of following you now
[02:52:58.420 --> 02:53:02.160]   in this interesting, weird time we're going through with AI.
[02:53:02.160 --> 02:53:04.800]   So thank you again for talking today.
[02:53:04.800 --> 02:53:06.400]   - Thank you, Alex, for this conversation.
[02:53:06.400 --> 02:53:08.280]   I enjoyed it very much.
[02:53:08.280 --> 02:53:10.840]   - Thanks for listening to this conversation with Yosha Bach.
[02:53:10.840 --> 02:53:11.960]   To support this podcast,
[02:53:11.960 --> 02:53:14.560]   please check out our sponsors in the description.
[02:53:14.560 --> 02:53:16.600]   And now let me leave you with some words
[02:53:16.600 --> 02:53:18.980]   from the psychologist Carl Jung.
[02:53:19.960 --> 02:53:21.840]   "One does not become enlightened
[02:53:21.840 --> 02:53:24.120]   by imagining figures of light,
[02:53:24.120 --> 02:53:26.460]   but by making the darkness conscious.
[02:53:26.460 --> 02:53:30.680]   The latter procedure, however, is disagreeable
[02:53:30.680 --> 02:53:33.040]   and therefore not popular."
[02:53:33.040 --> 02:53:34.280]   Thank you for listening.
[02:53:34.280 --> 02:53:36.200]   And I hope to see you next time.
[02:53:36.200 --> 02:53:38.780]   (upbeat music)
[02:53:38.780 --> 02:53:41.360]   (upbeat music)
[02:53:41.360 --> 02:53:51.360]   [BLANK_AUDIO]

