
[00:00:00.000 --> 00:00:03.700]   [music playing]
[00:00:03.700 --> 00:00:08.940]   Hi, everyone. We are here today at the Fully Connected Conference,
[00:00:08.940 --> 00:00:13.400]   User Conference is our first ever user conference for Weights and Biases.
[00:00:13.400 --> 00:00:16.740]   We are, as you know, in the session,
[00:00:16.740 --> 00:00:20.040]   a panel session on state of MLOps tools.
[00:00:20.040 --> 00:00:22.740]   We have a few luminaries with us.
[00:00:22.740 --> 00:00:24.900]   We are really, really honored to have them with us.
[00:00:24.900 --> 00:00:27.340]   So we'll start by introducing them.
[00:00:28.160 --> 00:00:31.900]   Let's first start with Will. Will Benton from NVIDIA.
[00:00:31.900 --> 00:00:35.140]   Will, do you want to take a quick minute to introduce yourself?
[00:00:35.140 --> 00:00:36.700]   Yeah, hi, thanks. I'm Will Benton.
[00:00:36.700 --> 00:00:38.360]   I'm a product architect at NVIDIA
[00:00:38.360 --> 00:00:41.240]   focusing on data science and practitioner experience.
[00:00:41.240 --> 00:00:45.160]   Before joining NVIDIA, I worked in sort of MLOps
[00:00:45.160 --> 00:00:47.560]   and tools for MLOps on Kubernetes.
[00:00:47.560 --> 00:00:48.940]   And sort of before that,
[00:00:48.940 --> 00:00:51.040]   distributed systems and programming language research.
[00:00:51.040 --> 00:00:53.160]   So that's where my biases are.
[00:00:53.160 --> 00:00:54.900]   Awesome. Great to have you, Will.
[00:00:55.640 --> 00:00:59.840]   Then next, Ben Forshman, founder and CEO of Replicate.
[00:00:59.840 --> 00:01:02.340]   Ben, do you want to quickly introduce yourself?
[00:01:02.340 --> 00:01:03.580]   Yeah, sure. I've got a background
[00:01:03.580 --> 00:01:05.740]   in starting a few developer tools companies.
[00:01:05.740 --> 00:01:08.780]   Most recently, I was head of open source product at Docker.
[00:01:08.780 --> 00:01:13.380]   And then taking that experience to Replicate now where we've got,
[00:01:13.380 --> 00:01:16.940]   partly we're building an open source tool called Cog,
[00:01:16.940 --> 00:01:19.420]   which is a little bit like Docker for machine learning.
[00:01:19.420 --> 00:01:20.680]   And we've also got Replicate,
[00:01:20.680 --> 00:01:23.120]   which is a place to run machine learning models in the cloud.
[00:01:23.120 --> 00:01:24.580]   Awesome. Welcome.
[00:01:24.720 --> 00:01:30.180]   And then we have Robert Nishihara from co-founder and CEO of AnyScale.
[00:01:30.180 --> 00:01:32.040]   Robert, do you want to introduce yourself?
[00:01:32.040 --> 00:01:35.620]   Hey, I'm Robert. I'm working on building AnyScale.
[00:01:35.620 --> 00:01:39.620]   And we're commercializing Ray, which is an open source project
[00:01:39.620 --> 00:01:43.180]   aimed at making it easy to build and scale machine
[00:01:43.180 --> 00:01:44.180]   learning applications.
[00:01:44.180 --> 00:01:45.940]   So as you probably know,
[00:01:45.940 --> 00:01:49.320]   machine learning is incredibly computationally intensive.
[00:01:49.320 --> 00:01:52.140]   You need to run it across tons of machines or tons of GPUs.
[00:01:53.020 --> 00:01:56.040]   And we are trying to make it easier to program and run
[00:01:56.040 --> 00:01:58.420]   and build and scale these kinds of applications.
[00:01:58.420 --> 00:02:01.040]   Awesome. Really great to have all three of you here.
[00:02:01.040 --> 00:02:02.780]   And my name is Gaurab Dey.
[00:02:02.780 --> 00:02:06.180]   I support the customer success organization
[00:02:06.180 --> 00:02:07.620]   within Bits and Biases.
[00:02:07.620 --> 00:02:10.620]   And my background is in machine learning and data science.
[00:02:10.620 --> 00:02:13.520]   And now I have gone to the other side
[00:02:13.520 --> 00:02:16.540]   and I cannot call myself technical anymore.
[00:02:16.540 --> 00:02:19.320]   But I'm still working with our customers
[00:02:19.320 --> 00:02:22.620]   that are deeply interested in everything
[00:02:22.740 --> 00:02:24.920]   and all things about ML.
[00:02:24.920 --> 00:02:27.540]   So with that, let's jump into this panel.
[00:02:27.540 --> 00:02:31.380]   I want to start with a little bit of a lightweight question here.
[00:02:31.380 --> 00:02:35.320]   So I have two kids. And every time someone asks me,
[00:02:35.320 --> 00:02:39.020]   which of them are my favorite, I'm super thrilled, as you would know.
[00:02:39.020 --> 00:02:42.780]   But I have some similar questions for you guys, actually.
[00:02:42.780 --> 00:02:44.580]   So I will start with Robert.
[00:02:44.580 --> 00:02:48.520]   AnyScale and Ray overall is an end-to-end platform.
[00:02:48.520 --> 00:02:52.020]   And you have a lot of different tools kind of playing in that platform.
[00:02:52.540 --> 00:02:55.940]   Which one in your mind is adopted the most by the users?
[00:02:55.940 --> 00:02:58.540]   And which one is lesser known,
[00:02:58.540 --> 00:03:01.080]   but would be really impactful if was more adopted?
[00:03:01.080 --> 00:03:03.220]   Yeah, that's a great question.
[00:03:03.220 --> 00:03:05.720]   And so let me provide a little context here.
[00:03:05.720 --> 00:03:08.480]   Ray is like scalable Python, right?
[00:03:08.480 --> 00:03:11.440]   Ray lets you scale Python applications across clusters.
[00:03:11.440 --> 00:03:13.940]   And then Ray has an ecosystem of similar
[00:03:13.940 --> 00:03:16.540]   to how Python has an ecosystem of libraries.
[00:03:16.540 --> 00:03:20.040]   Ray has an ecosystem of scalable libraries on top.
[00:03:20.180 --> 00:03:23.480]   And the most things like training, scaling training,
[00:03:23.480 --> 00:03:27.020]   data ingest, pre-processing, serving, inference, and so on.
[00:03:27.020 --> 00:03:31.280]   I think the most common, yes, but the most popular,
[00:03:31.280 --> 00:03:34.220]   the most common workloads we see people running
[00:03:34.220 --> 00:03:37.580]   and scaling on top of Ray are training workloads,
[00:03:37.580 --> 00:03:41.280]   serving workloads like model deployment,
[00:03:41.280 --> 00:03:44.460]   and batch processing or batch inference workloads.
[00:03:44.460 --> 00:03:49.960]   I think one of the things that is maybe least known about Ray
[00:03:50.200 --> 00:03:54.460]   is that we also have the Ray data library,
[00:03:54.460 --> 00:03:57.460]   which is not full-blown data processing,
[00:03:57.460 --> 00:04:00.600]   or imagine like SQL queries or structured data processing
[00:04:00.600 --> 00:04:01.640]   or that kind of thing.
[00:04:01.640 --> 00:04:05.460]   It's really targeted at the kind of data processing
[00:04:05.460 --> 00:04:07.640]   you need to do in order to do machine learning.
[00:04:07.640 --> 00:04:10.900]   So imagine ingesting a bunch of data,
[00:04:10.900 --> 00:04:13.340]   pre-processing it, feeding it into training,
[00:04:13.340 --> 00:04:14.840]   and things like that.
[00:04:14.840 --> 00:04:16.760]   And that's actually, we see people,
[00:04:16.760 --> 00:04:18.900]   it's less widely adopted than the others,
[00:04:19.040 --> 00:04:20.860]   but we see people using Ray data
[00:04:20.860 --> 00:04:23.160]   to process petabytes of data every single day
[00:04:23.160 --> 00:04:26.200]   and using it at a pretty tremendous scale.
[00:04:26.200 --> 00:04:28.660]   So I think, and of course, you can't do machine
[00:04:28.660 --> 00:04:31.140]   learning without working with data in some capacity.
[00:04:31.140 --> 00:04:34.460]   So we think that's a pretty critical piece of the ecosystem.
[00:04:34.460 --> 00:04:38.360]   Yeah, I'm fascinated to hear that because for rates and biases,
[00:04:38.360 --> 00:04:41.540]   we always talk about experiment tracking
[00:04:41.540 --> 00:04:46.240]   because ML practitioners are so used to kind of talking about training
[00:04:46.240 --> 00:04:47.840]   and model training, all of that.
[00:04:47.960 --> 00:04:50.460]   But I think one of the most underrated product feature
[00:04:50.460 --> 00:04:54.060]   is artifacts, which is basically kind of like similar idea
[00:04:54.060 --> 00:04:56.760]   that how do you actually version models and data sets
[00:04:56.760 --> 00:04:58.260]   and all of those things.
[00:04:58.260 --> 00:05:01.840]   And it's not as like hot as experiment tracking,
[00:05:01.840 --> 00:05:05.000]   but it is one of the most useful and underrated feature
[00:05:05.000 --> 00:05:06.200]   that in my opinion as well.
[00:05:06.200 --> 00:05:07.940]   So it's very, very interesting to hear that.
[00:05:07.940 --> 00:05:13.100]   I will have a question for Ben for Replicate.
[00:05:13.100 --> 00:05:14.540]   I think I know the answer of this
[00:05:14.660 --> 00:05:19.360]   because I did see your profile image for Zoom right before this.
[00:05:19.360 --> 00:05:25.960]   But given Replicate is kind of empowering
[00:05:25.960 --> 00:05:27.740]   all these open source foundation models
[00:05:27.740 --> 00:05:31.040]   are getting used out of this open source foundation models.
[00:05:31.040 --> 00:05:34.360]   What is your most favorite open source foundation model?
[00:05:34.360 --> 00:05:42.660]   Yeah, well, I mean, the thing that's been this incredible movement
[00:05:42.800 --> 00:05:48.240]   for open source vision or open source image generation models
[00:05:48.240 --> 00:05:50.640]   has been stable diffusion in September, obviously.
[00:05:50.640 --> 00:05:54.240]   And I think what's been incredible about stable diffusion
[00:05:54.240 --> 00:05:59.300]   is it's just created this explosion of forks and tweaks and fine tunes
[00:05:59.300 --> 00:06:02.200]   and all this sort of thing that would not be possible otherwise.
[00:06:02.200 --> 00:06:06.600]   So I think the thing you were saying with my profile picture is Dreambooth,
[00:06:06.600 --> 00:06:08.900]   which is a, you know, it's not a model,
[00:06:09.040 --> 00:06:13.280]   but it's a technique for fine tuning, fine tuning, stable diffusion.
[00:06:13.280 --> 00:06:16.740]   And there's just so much interesting stuff
[00:06:16.740 --> 00:06:18.740]   that's been built on top of this ecosystem,
[00:06:18.740 --> 00:06:22.180]   just because it's all open source and really easily hackable.
[00:06:22.180 --> 00:06:24.040]   You know, people have like there's obviously fine tuning
[00:06:24.040 --> 00:06:26.480]   where you can fine tune stable diffusion on your face
[00:06:26.480 --> 00:06:32.620]   and create your face as a sort of labeled objects in these models.
[00:06:32.620 --> 00:06:35.020]   Similar thing with styles.
[00:06:35.020 --> 00:06:38.120]   But we've also seen people do really creative stuff
[00:06:38.240 --> 00:06:39.420]   like create animation models,
[00:06:39.420 --> 00:06:43.640]   like feeding outputs from the model into the next frame of the other model
[00:06:43.640 --> 00:06:46.580]   and creating these really cool animations with stable diffusion.
[00:06:46.580 --> 00:06:51.680]   Things like creating texture generators with stable diffusion,
[00:06:51.680 --> 00:06:56.420]   things like, I've forgotten the name of it now.
[00:06:56.420 --> 00:07:00.320]   The one, the model where they took spectrograms,
[00:07:00.320 --> 00:07:02.980]   they fine tune stable diffusion on spectrograms,
[00:07:02.980 --> 00:07:06.020]   and then turn stable diffusion into an audio model as well.
[00:07:06.140 --> 00:07:09.520]   And there's just so much interesting, interesting stuff
[00:07:09.520 --> 00:07:11.080]   that's being built there.
[00:07:11.080 --> 00:07:13.840]   Particularly excited about recently about some of the more advanced ways
[00:07:13.840 --> 00:07:16.320]   of fine tuning stable diffusion as well.
[00:07:16.320 --> 00:07:21.320]   We recently released Laura on replicates,
[00:07:21.320 --> 00:07:22.720]   which is it's like Dreambooth,
[00:07:22.720 --> 00:07:24.440]   but just like an order of magnitude faster.
[00:07:24.440 --> 00:07:29.440]   So you can train these models in just sort of five minutes,
[00:07:29.440 --> 00:07:35.880]   I think it is. And then the weights are only these sort of 10 megabyte files
[00:07:36.020 --> 00:07:37.720]   instead of almost like a diff
[00:07:37.720 --> 00:07:40.580]   instead of the whole four gigabyte stable diffusion weights.
[00:07:40.580 --> 00:07:43.140]   And we're just going to see more and more of these kind of advancements
[00:07:43.140 --> 00:07:46.040]   to make it easier to tweak and hack on these things.
[00:07:46.040 --> 00:07:50.340]   Yeah, I think Dreambooth probably should have a tagline,
[00:07:50.340 --> 00:07:52.080]   it makes you look cool.
[00:07:52.080 --> 00:07:57.380]   Everyone take their profile photos and submit in there and get something.
[00:07:57.380 --> 00:08:00.240]   Awesome. That's cool.
[00:08:00.240 --> 00:08:03.540]   And Will, I have a question for you for Nvidia.
[00:08:04.440 --> 00:08:07.340]   So of course, Nvidia is being used across the board
[00:08:07.340 --> 00:08:09.400]   for across all industry and use cases.
[00:08:09.400 --> 00:08:15.240]   So this one is a little bit of a harder one to probably choose from.
[00:08:15.240 --> 00:08:17.340]   But I will ask you a little bit differently.
[00:08:17.340 --> 00:08:22.900]   Out of all the use cases that have seen for Nvidia GPU and software stack,
[00:08:22.900 --> 00:08:26.200]   which is also very, very highly used,
[00:08:26.200 --> 00:08:29.640]   what is in your opinion, one of the most impactful,
[00:08:29.640 --> 00:08:32.160]   but not that well known use case?
[00:08:32.300 --> 00:08:35.160]   I think we all talk about the gaming use cases a lot.
[00:08:35.160 --> 00:08:38.400]   Of course, we talk about now autonomous vehicles and so on a lot.
[00:08:38.400 --> 00:08:41.360]   But I'm curious if you have like something
[00:08:41.360 --> 00:08:44.460]   that is a little bit of a surprise or not that well known,
[00:08:44.460 --> 00:08:48.800]   but is very, very impactful use case of Nvidia GPU and software stack.
[00:08:48.800 --> 00:08:51.100]   Yeah, that's an awesome question.
[00:08:51.100 --> 00:08:54.100]   And a lot of people think about GPUs as you know,
[00:08:54.100 --> 00:08:57.000]   this is something that I need for high performance computing, right?
[00:08:57.000 --> 00:09:00.460]   So I'm doing deep learning or I'm doing conventional machine
[00:09:00.600 --> 00:09:03.660]   learning or boosted trees or something at massive, massive scale.
[00:09:03.660 --> 00:09:08.200]   I really think the underrated advantage of Nvidia's accelerated
[00:09:08.200 --> 00:09:11.260]   computing stack is at a human scale, right?
[00:09:11.260 --> 00:09:15.700]   Like what crosses my desk, I want to do exploratory data science,
[00:09:15.700 --> 00:09:17.860]   and I want to be more productive. Right?
[00:09:17.860 --> 00:09:22.300]   So if you think about, you know, you have a regular CPU based workstation,
[00:09:22.300 --> 00:09:24.800]   and you have, you know, medium sized data set,
[00:09:24.800 --> 00:09:28.360]   10s or hundreds of 1000s of records of high dimensional data.
[00:09:28.500 --> 00:09:30.300]   I keep me honest, keep me honest, folks,
[00:09:30.300 --> 00:09:32.300]   this is this is a medium sized data set, right?
[00:09:32.300 --> 00:09:38.840]   This is sort of it's not it's we're not talking about like national lab scale.
[00:09:38.840 --> 00:09:43.200]   But if you have that, if you have that on accelerated workstation,
[00:09:43.200 --> 00:09:44.900]   you're using something like PCA,
[00:09:44.900 --> 00:09:47.140]   and you're waiting a few seconds to get an answer.
[00:09:47.140 --> 00:09:50.560]   If you wanted to use something more sophisticated,
[00:09:50.560 --> 00:09:53.400]   like a t-SNE or UMAP, you're looking at,
[00:09:53.400 --> 00:09:55.840]   you know, minutes or even the better part of an hour.
[00:09:55.840 --> 00:09:58.360]   And if you live within walking distance of a great coffee shop,
[00:09:58.360 --> 00:10:00.220]   that probably doesn't sound too bad.
[00:10:00.220 --> 00:10:03.360]   But it's it's time that you're not engaged with your work, right?
[00:10:03.360 --> 00:10:06.320]   It's time that you're distracted, even if it's even if it's just a couple
[00:10:06.320 --> 00:10:09.000]   of minutes, that's time that you're going to another tab,
[00:10:09.000 --> 00:10:11.060]   and you're losing your train of thought.
[00:10:11.060 --> 00:10:15.020]   So instead of that, though, if you take NVIDIA's rapids library,
[00:10:15.020 --> 00:10:18.200]   and NVIDIA GPU, you could run t-SNE or UMAP
[00:10:18.200 --> 00:10:21.020]   or a range of other sort of useful techniques
[00:10:21.020 --> 00:10:24.060]   for conventional data science on a single node.
[00:10:24.060 --> 00:10:27.420]   And find that find that hidden structure in your data
[00:10:27.560 --> 00:10:30.120]   and interactive latencies, maybe we're talking the few seconds
[00:10:30.120 --> 00:10:32.620]   that PCA took, maybe we're talking less than a second.
[00:10:32.620 --> 00:10:35.120]   And then you're not getting distracted.
[00:10:35.120 --> 00:10:36.860]   And I think this is this is key, right?
[00:10:36.860 --> 00:10:39.300]   Throughout the history of human computer interaction,
[00:10:39.300 --> 00:10:43.360]   we've seen that any improvement to how much you lower latency
[00:10:43.360 --> 00:10:46.120]   and waiting for a computer is going to make the human
[00:10:46.120 --> 00:10:48.660]   interacting with the computer way more productive.
[00:10:48.660 --> 00:10:50.520]   So I think that's an exciting benefit is,
[00:10:50.520 --> 00:10:53.220]   is you can take the data science techniques that you're doing now,
[00:10:53.220 --> 00:10:55.820]   you can do smarter things in the same amount of time,
[00:10:55.960 --> 00:10:58.820]   or you can do the same things in interactive latency
[00:10:58.820 --> 00:11:00.460]   instead of waiting for it.
[00:11:00.460 --> 00:11:02.200]   That's awesome. Yeah.
[00:11:02.200 --> 00:11:05.220]   When we were in data robot,
[00:11:05.220 --> 00:11:06.800]   when I was used to working in data robot,
[00:11:06.800 --> 00:11:09.500]   we used to call the boring part of the AI,
[00:11:09.500 --> 00:11:12.700]   I mean, the ones that no one really, it's not the fancy,
[00:11:12.700 --> 00:11:15.660]   you know, like things, but the simple things like improving latency,
[00:11:15.660 --> 00:11:18.300]   improving instant interactivity.
[00:11:18.300 --> 00:11:21.400]   That's kind of what actually drives the business value a lot.
[00:11:21.400 --> 00:11:25.420]   So really resonates. Thank you, all of you.
[00:11:25.560 --> 00:11:30.060]   I know that this was sort of a kind of a specific question to you guys,
[00:11:30.060 --> 00:11:33.500]   but let's jump into the more meat of this panel.
[00:11:33.500 --> 00:11:35.500]   I think what we are trying to talk about here
[00:11:35.500 --> 00:11:39.760]   is how do we see the state of the MLOps ecosystem?
[00:11:39.760 --> 00:11:43.000]   And I will start with a very foundational question,
[00:11:43.000 --> 00:11:45.520]   probably in there. I think it starts,
[00:11:45.520 --> 00:11:48.520]   it's a little bit of a high level and wider question.
[00:11:48.520 --> 00:11:55.220]   But when you're when I started listening to different things about MLOps,
[00:11:55.360 --> 00:11:59.260]   it's been a few years, and MLOps has changed a lot of that.
[00:11:59.260 --> 00:12:02.600]   Like the scope has changed. When I think we first heard about MLOps,
[00:12:02.600 --> 00:12:06.960]   it was all about production monitoring and some maybe edge models
[00:12:06.960 --> 00:12:09.760]   that you need to kind of like monitor in some ways and so on.
[00:12:09.760 --> 00:12:15.000]   But I think what it has changed now is to a much wider definition.
[00:12:15.000 --> 00:12:17.960]   And what that means also that the marketplace
[00:12:17.960 --> 00:12:23.220]   has become incredibly fragmented into what MLOps will cover.
[00:12:24.100 --> 00:12:28.400]   So in that world, I'm curious what you guys think about,
[00:12:28.400 --> 00:12:31.800]   what do you consider to be what are the essential components
[00:12:31.800 --> 00:12:34.260]   that make an MLOps pipeline?
[00:12:34.260 --> 00:12:36.200]   What does that look like?
[00:12:36.200 --> 00:12:38.320]   And how should they interact with each other?
[00:12:38.320 --> 00:12:40.000]   Maybe we can start with Robert,
[00:12:40.000 --> 00:12:44.160]   given AnyScale is sort of an end to end MLOps platform in that world.
[00:12:44.160 --> 00:12:49.660]   Well, you know, the part that AnyScale focuses on is the compute piece,
[00:12:49.800 --> 00:12:53.620]   like trying to make it easy to scale and run,
[00:12:53.620 --> 00:12:56.700]   whether it's training or serving or inference,
[00:12:56.700 --> 00:12:58.720]   or the data ingest, the things like this.
[00:12:58.720 --> 00:13:01.200]   So we're not end to end in the sense
[00:13:01.200 --> 00:13:03.320]   that we don't do experiment tracking, of course,
[00:13:03.320 --> 00:13:06.720]   we don't do the observability tooling or your feature store
[00:13:06.720 --> 00:13:09.920]   or your model registry, or there are many components
[00:13:09.920 --> 00:13:14.020]   and pieces you need to do machine learning.
[00:13:14.020 --> 00:13:15.760]   And we're focused on compute.
[00:13:15.760 --> 00:13:18.520]   That said, I think my perspective here
[00:13:18.660 --> 00:13:23.900]   is that it's sort of natural to think of some of these tools
[00:13:23.900 --> 00:13:25.420]   as being production focused,
[00:13:25.420 --> 00:13:29.960]   and some as being focused on the interactive development experience.
[00:13:29.960 --> 00:13:32.900]   And both are very important,
[00:13:32.900 --> 00:13:36.300]   but a lot of what we find exciting here
[00:13:36.300 --> 00:13:41.800]   is trying to bring the interactive development experience
[00:13:41.800 --> 00:13:45.000]   for developers, the human scale that Will was talking about,
[00:13:45.000 --> 00:13:48.600]   as close as possible to the production experience
[00:13:48.660 --> 00:13:50.660]   and the production tool that you're using,
[00:13:50.660 --> 00:13:55.440]   so that you can move back and forth between them quickly.
[00:13:55.440 --> 00:13:57.940]   Because one challenge a lot of companies face
[00:13:57.940 --> 00:14:02.800]   is they develop a model, but then they'll say it takes them eight weeks
[00:14:02.800 --> 00:14:05.140]   or 12 weeks to get that model in production,
[00:14:05.140 --> 00:14:07.260]   because it can often be a different software stack,
[00:14:07.260 --> 00:14:09.800]   a different set of tools, and then vice versa,
[00:14:09.800 --> 00:14:11.700]   something's going to go wrong in production,
[00:14:11.700 --> 00:14:13.460]   you're going to need to go back to debugging
[00:14:13.460 --> 00:14:15.760]   or developing and iterate on that.
[00:14:15.900 --> 00:14:20.140]   And so, if you have separate teams, separate software stacks,
[00:14:20.140 --> 00:14:21.860]   that can add a lot of friction.
[00:14:21.860 --> 00:14:26.800]   And so, we think about how to make those two experiences
[00:14:26.800 --> 00:14:28.400]   as close together as possible,
[00:14:28.400 --> 00:14:30.000]   or how to move seamlessly between them.
[00:14:30.000 --> 00:14:32.300]   So that's what I would say about that.
[00:14:32.300 --> 00:14:34.040]   Cool. Will, any thoughts?
[00:14:34.040 --> 00:14:38.560]   Yeah, I think, I mean, it's hard to say what's essential in MLOps,
[00:14:38.560 --> 00:14:40.840]   because a lot of the challenges in machine learning systems
[00:14:40.840 --> 00:14:42.000]   are just general software,
[00:14:42.000 --> 00:14:44.300]   general distributed systems challenges, right?
[00:14:44.440 --> 00:14:46.540]   So, I mean, like, is Git an MLOps tool?
[00:14:46.540 --> 00:14:49.140]   I don't think it is, but I think a lot of people
[00:14:49.140 --> 00:14:51.580]   who are doing MLOps are using version control, right?
[00:14:51.580 --> 00:14:57.000]   But I think the most essential thing here is reproducibility, right?
[00:14:57.000 --> 00:14:59.240]   In almost every case where you're using machine
[00:14:59.240 --> 00:15:02.240]   learning in the real world, you want to be able to reproduce
[00:15:02.240 --> 00:15:04.340]   and go back to what you were doing at some point in the past.
[00:15:04.340 --> 00:15:06.580]   And I'd say that the truly essential components
[00:15:06.580 --> 00:15:10.400]   really are the things that support reproducibility directly.
[00:15:10.400 --> 00:15:13.200]   So, like you were saying, with experiment management,
[00:15:13.340 --> 00:15:15.540]   you also need sort of artifact tracking and environment
[00:15:15.540 --> 00:15:19.140]   tracking and data versioning in order to sort of tie
[00:15:19.140 --> 00:15:22.140]   those experimental results to how you got them, right?
[00:15:22.140 --> 00:15:26.180]   And I guess in a lot of ways, this does wind up looking like
[00:15:26.180 --> 00:15:28.900]   your sort of conventional enterprise software supply chain
[00:15:28.900 --> 00:15:31.540]   or your conventional application development platform.
[00:15:31.540 --> 00:15:35.180]   You know, you have version control and dependency management for code,
[00:15:35.180 --> 00:15:36.900]   RBAC and data lineage for data,
[00:15:36.900 --> 00:15:39.880]   and you have the metrics for your running services.
[00:15:39.880 --> 00:15:42.900]   You have some way to tie it back to how you got there, right?
[00:15:43.340 --> 00:15:46.340]   And so I think actually, in a lot of ways,
[00:15:46.340 --> 00:15:48.460]   conventional application development platforms
[00:15:48.460 --> 00:15:50.500]   are a good starting point for the primitives you need,
[00:15:50.500 --> 00:15:52.560]   but they don't really provide the right user experience
[00:15:52.560 --> 00:15:54.160]   for most machine learning practitioners
[00:15:54.160 --> 00:15:56.740]   who are sort of less comfortable in the world
[00:15:56.740 --> 00:15:58.560]   of sort of software development tools.
[00:15:58.560 --> 00:16:04.540]   Makes sense. So would you guys say that there are like two types
[00:16:04.540 --> 00:16:07.260]   of MLOps tools, like a developer MLOps tool
[00:16:07.260 --> 00:16:09.940]   and production MLOps tools, or is it more of a,
[00:16:11.040 --> 00:16:13.700]   like they should be kind of talking to each other
[00:16:13.700 --> 00:16:16.240]   and connecting connected experience anyway?
[00:16:16.240 --> 00:16:20.800]   And anyone, feel free to add color to that.
[00:16:20.800 --> 00:16:24.940]   Yeah, I think it's important, you know, like Robert was saying,
[00:16:24.940 --> 00:16:27.540]   to be able to sort of have some commonality
[00:16:27.540 --> 00:16:28.900]   between development and production.
[00:16:28.900 --> 00:16:32.300]   It removes an opportunity for practitioners to make a mistake.
[00:16:32.300 --> 00:16:35.180]   But, you know, to your point, there are different personas
[00:16:35.180 --> 00:16:37.240]   and different sort of backgrounds and different skill sets
[00:16:37.240 --> 00:16:39.440]   involved in exploratory machine learning
[00:16:39.440 --> 00:16:40.540]   and production machine learning.
[00:16:40.680 --> 00:16:42.180]   And we need a way to sort of respect
[00:16:42.180 --> 00:16:43.940]   that people are coming from different places.
[00:16:43.940 --> 00:16:48.840]   Makes sense. Ben, in your scenario with Replicate or COGS,
[00:16:48.840 --> 00:16:52.300]   what is the user persona you are kind of primarily interacting with
[00:16:52.300 --> 00:16:54.480]   or your product is primarily being used by?
[00:16:54.480 --> 00:16:56.200]   Yeah, totally. So we're a bit weird,
[00:16:56.200 --> 00:16:58.900]   I think, where we don't consider ourselves in an MLOps tool,
[00:16:58.900 --> 00:17:02.880]   because we're primarily building for software engineers,
[00:17:02.880 --> 00:17:04.600]   like software engineers consuming these models
[00:17:04.600 --> 00:17:06.680]   and sort of machine learning researchers
[00:17:06.680 --> 00:17:08.400]   who are creating these models.
[00:17:09.000 --> 00:17:12.540]   And sort of, I suppose, our world philosophy,
[00:17:12.540 --> 00:17:13.900]   which is maybe a bit controversial,
[00:17:13.900 --> 00:17:17.140]   is that those two people who are actually either producing
[00:17:17.140 --> 00:17:21.280]   these models or consuming these models and their products
[00:17:21.280 --> 00:17:24.080]   shouldn't have to worry about any of this tooling underneath.
[00:17:24.080 --> 00:17:28.640]   So in some sense, we replicate other consumers of MLOps tooling
[00:17:28.640 --> 00:17:31.400]   such that we can build this kind of high level platform
[00:17:31.400 --> 00:17:33.540]   so they don't have to worry about it.
[00:17:33.540 --> 00:17:35.480]   Exactly how you draw those lines is complicated,
[00:17:35.480 --> 00:17:38.940]   because there's some stuff where you have to be aware of these things.
[00:17:39.020 --> 00:17:40.880]   That's the user of it.
[00:17:40.880 --> 00:17:44.940]   It's exactly what you define MLOps as maybe part of the challenge.
[00:17:44.940 --> 00:17:48.720]   Are you saying these are two separate groups of people
[00:17:48.720 --> 00:17:50.780]   that are the consumers and the producers
[00:17:50.780 --> 00:17:53.740]   and that there's a very natural kind of interface there
[00:17:53.740 --> 00:17:55.480]   and sort of division?
[00:17:55.480 --> 00:18:00.540]   That is, I think it's a bit messier than,
[00:18:00.540 --> 00:18:01.880]   there's not quite a clear distinction,
[00:18:01.880 --> 00:18:04.240]   but there certainly should be an API between these people.
[00:18:04.240 --> 00:18:07.440]   And I think that's what we're trying to define with COGS.
[00:18:07.580 --> 00:18:11.380]   It's a bit like what happened with the interface
[00:18:11.380 --> 00:18:15.040]   between machine learning researchers and software engineers
[00:18:15.040 --> 00:18:16.740]   who want to build things out of these machine learning models.
[00:18:16.740 --> 00:18:19.240]   There's a bit like the interface between developers
[00:18:19.240 --> 00:18:21.020]   and operations people in traditional software.
[00:18:21.020 --> 00:18:24.840]   And Docker was intended as this kind of interface point
[00:18:24.840 --> 00:18:28.380]   where it's like, okay, developers, just put your code inside this box.
[00:18:28.380 --> 00:18:30.980]   And then you can tell operations people just run this box.
[00:18:30.980 --> 00:18:33.580]   And you don't really have to worry about what's inside.
[00:18:33.720 --> 00:18:37.020]   And the trick there is you can't obviously not,
[00:18:37.020 --> 00:18:38.520]   completely not worry about what's inside,
[00:18:38.520 --> 00:18:41.220]   but you define the right interfaces and definitions around this box
[00:18:41.220 --> 00:18:44.060]   such that you can use it as a language for communicating.
[00:18:44.060 --> 00:18:46.720]   And I think that's some of the intention of what we're doing.
[00:18:46.720 --> 00:18:48.420]   I think that's a nice way to put it.
[00:18:48.420 --> 00:18:52.460]   One thing I would, we hear all the time is just people who,
[00:18:52.460 --> 00:18:57.720]   teams and people doing machine learning want their teams
[00:18:57.720 --> 00:18:59.020]   or their people doing machine learning
[00:18:59.020 --> 00:19:01.360]   to be able to focus entirely on the machine
[00:19:01.480 --> 00:19:04.360]   learning part of the work on creating the models,
[00:19:04.360 --> 00:19:07.060]   on using the models and building applications with the models,
[00:19:07.060 --> 00:19:09.260]   and not on configuring infrastructure
[00:19:09.260 --> 00:19:12.680]   and setting up or building or managing infrastructure.
[00:19:12.680 --> 00:19:15.220]   So that's, I think, consistent with what you're saying.
[00:19:15.220 --> 00:19:17.560]   Yeah, there was a screenshot being shared on LinkedIn,
[00:19:17.560 --> 00:19:19.480]   I think last week,
[00:19:19.480 --> 00:19:22.320]   that someone added a machine learning researcher
[00:19:22.320 --> 00:19:24.580]   to the company Kubernetes channel.
[00:19:24.580 --> 00:19:27.920]   And this guy is freaking out like, don't add me to this Kubernetes.
[00:19:27.920 --> 00:19:30.120]   My life was so fine before this.
[00:19:30.980 --> 00:19:32.580]   Why do I have to go that direction?
[00:19:32.580 --> 00:19:35.120]   But totally hear you.
[00:19:35.120 --> 00:19:38.960]   Do you think though, Ben, to your point,
[00:19:38.960 --> 00:19:43.260]   like that the software engineers won't need to think about
[00:19:43.260 --> 00:19:44.980]   or worry about what's in the box?
[00:19:44.980 --> 00:19:48.120]   Is that applicable for any and all use case?
[00:19:48.120 --> 00:19:50.880]   Or in other words, is it possible to replicate,
[00:19:50.880 --> 00:19:55.780]   because I heard you use that word carefully in your messaging,
[00:19:56.820 --> 00:20:01.000]   any and all use case as a generalized box or API?
[00:20:01.000 --> 00:20:05.700]   Or is it more specific to a certain type of use cases where we can do that?
[00:20:05.700 --> 00:20:07.420]   Yeah, good question.
[00:20:07.420 --> 00:20:14.560]   I think the, I mean, certainly the models we have on replicates
[00:20:14.560 --> 00:20:20.120]   are relatively straightforward models to that point.
[00:20:20.120 --> 00:20:26.360]   I think there are questions about whether some of the more complex
[00:20:26.940 --> 00:20:30.760]   types of machine learning can be wrapped up inside a box.
[00:20:30.760 --> 00:20:35.140]   Don't have a clear sense of that, to be honest.
[00:20:35.140 --> 00:20:37.000]   We haven't tried to define those boxes yet.
[00:20:37.000 --> 00:20:39.800]   Yeah, yeah, it's still a work in progress.
[00:20:39.800 --> 00:20:41.940]   Makes sense. All right.
[00:20:41.940 --> 00:20:44.000]   Thanks, everyone, for your point of view on there.
[00:20:44.000 --> 00:20:46.960]   But let's move on to a little bit more spicier question.
[00:20:46.960 --> 00:20:50.240]   So of course, your own tool,
[00:20:50.240 --> 00:20:53.300]   you're building it for a very specific reason
[00:20:53.300 --> 00:20:55.440]   and kind of you have a mission in your mind.
[00:20:56.200 --> 00:20:58.100]   And of course, we are here at Bits and Biases
[00:20:58.100 --> 00:21:00.260]   are also building with the mission in mind
[00:21:00.260 --> 00:21:03.760]   to help build the best tools for ML practitioners.
[00:21:03.760 --> 00:21:06.640]   But outside of these tools that we have,
[00:21:06.640 --> 00:21:09.540]   what in your user community that you're seeing
[00:21:09.540 --> 00:21:14.140]   and some of the highest adopted, very popular ML frameworks
[00:21:14.140 --> 00:21:17.760]   or toolings that you see that are really getting it right?
[00:21:17.760 --> 00:21:22.860]   And yeah, that's why first taking out the biases
[00:21:22.860 --> 00:21:24.800]   that we individually have about our tools,
[00:21:24.940 --> 00:21:31.300]   but just looking at it holistically as ML leader in this market,
[00:21:31.300 --> 00:21:35.640]   which ones do you think users should really be knowing about
[00:21:35.640 --> 00:21:38.400]   and using and seeing a lot of value out of it?
[00:21:38.400 --> 00:21:41.040]   Maybe, Will, you can start.
[00:21:41.040 --> 00:21:42.900]   Yeah, and you put me in a tough spot,
[00:21:42.900 --> 00:21:45.400]   both by saying I can't talk about weights and biases
[00:21:45.400 --> 00:21:47.440]   and by putting me on a panel with people
[00:21:47.440 --> 00:21:49.360]   who are behind some other really cool tools.
[00:21:49.360 --> 00:21:52.760]   I've been following Ray since it was a research project
[00:21:52.900 --> 00:21:57.220]   and I've been really impressed with Cog when I've seen it in action.
[00:21:57.220 --> 00:22:02.000]   But we're huge fans of weights and biases as well at NVIDIA.
[00:22:02.000 --> 00:22:07.760]   But I think we partner with everyone at NVIDIA.
[00:22:07.760 --> 00:22:12.520]   And so I think I want to sort of take a different spin on this question
[00:22:12.520 --> 00:22:15.800]   and sort of just call out someone who has,
[00:22:15.800 --> 00:22:20.620]   I think, done a good job of making sense of a difficult product category
[00:22:20.620 --> 00:22:22.400]   and making sense of a difficult use case.
[00:22:22.520 --> 00:22:23.920]   And that's Hopsworks.
[00:22:23.920 --> 00:22:26.700]   I think Hopsworks is sort of the feature store.
[00:22:26.700 --> 00:22:30.100]   I think they've done a particularly good job of technology transfer.
[00:22:30.100 --> 00:22:34.120]   A real challenge I've experienced as someone who's built tools,
[00:22:34.120 --> 00:22:37.060]   and I think everyone sort of is familiar with this,
[00:22:37.060 --> 00:22:41.160]   probably on the panel, is that it's really easy to be ahead of the curve.
[00:22:41.160 --> 00:22:44.060]   And it's really, really easy to be too far ahead of the curve.
[00:22:44.060 --> 00:22:46.400]   Like as a tool builder, you sort of anticipate problems
[00:22:46.400 --> 00:22:48.120]   your customers don't even imagine yet.
[00:22:48.120 --> 00:22:51.860]   And I think that feature stores for a long time
[00:22:52.000 --> 00:22:53.700]   have been in this category of,
[00:22:53.700 --> 00:22:57.100]   well, I understand that some enterprises
[00:22:57.100 --> 00:22:59.020]   who are mature at using machine learning
[00:22:59.020 --> 00:23:00.820]   are getting a lot of value out of these,
[00:23:00.820 --> 00:23:03.020]   but I don't see how they fit into my business yet.
[00:23:03.020 --> 00:23:06.160]   And I think Hopsworks has sort of done a good job
[00:23:06.160 --> 00:23:09.500]   of sort of evangelizing that use case and also saying,
[00:23:09.500 --> 00:23:11.800]   well, not only that, but here's why you don't want to try
[00:23:11.800 --> 00:23:13.520]   and build your own,
[00:23:13.520 --> 00:23:16.900]   which is valuable, I think, to the community at large.
[00:23:16.900 --> 00:23:19.120]   Yeah, yeah, I think so.
[00:23:20.020 --> 00:23:21.820]   Yeah, we'll have to look at it.
[00:23:21.820 --> 00:23:23.480]   That space, feature store space,
[00:23:23.480 --> 00:23:27.420]   is getting a lot of traction in the more recent times.
[00:23:27.420 --> 00:23:31.720]   I think Tecton is another one in that same kind of broad area as well.
[00:23:31.720 --> 00:23:35.080]   Makes sense. Ben, what's your thought?
[00:23:35.080 --> 00:23:38.940]   What's your kind of choice of such tools?
[00:23:38.940 --> 00:23:43.780]   Everyone really loves to hate on Colab,
[00:23:43.780 --> 00:23:49.180]   but it's just been like this core of this image generation community
[00:23:49.320 --> 00:23:50.580]   that we've seen.
[00:23:50.580 --> 00:23:54.580]   What's funny is that Replicate and Cog was almost like a reaction to Colab
[00:23:54.580 --> 00:23:56.620]   in that like, oh, these things are so messy.
[00:23:56.620 --> 00:23:58.420]   They don't run half the time.
[00:23:58.420 --> 00:24:01.720]   They're not reproducible, all this kind of thing.
[00:24:01.720 --> 00:24:04.840]   But what's so brilliant about Colab
[00:24:04.840 --> 00:24:07.780]   is that you can just play around with it and tinker with it.
[00:24:07.780 --> 00:24:10.340]   And one button, you can fork it and make something else.
[00:24:10.340 --> 00:24:14.420]   And there's always a URL that you can just copy and share with people.
[00:24:14.420 --> 00:24:16.720]   And it's because of that just sort of freeformity
[00:24:17.240 --> 00:24:19.940]   that's created this incredible creativity
[00:24:19.940 --> 00:24:22.560]   in this ecosystem we've seen in image generation models.
[00:24:22.560 --> 00:24:24.060]   It wouldn't have happened without Colab.
[00:24:24.060 --> 00:24:25.960]   Yeah. In our team, there's two groups.
[00:24:25.960 --> 00:24:27.960]   Like one is like very Colab pro.
[00:24:27.960 --> 00:24:31.840]   I mean, not the product, but they are actually really fan of Colab.
[00:24:31.840 --> 00:24:34.600]   And then there is another group that doesn't believe
[00:24:34.600 --> 00:24:36.740]   that Colab should be used in production system.
[00:24:36.740 --> 00:24:39.860]   It's fun to get them fighting against each other a bit.
[00:24:39.860 --> 00:24:42.140]   Robert, what are your thoughts?
[00:24:42.140 --> 00:24:43.760]   Yeah, it's a great question.
[00:24:43.900 --> 00:24:48.400]   You know, we're in this position where because Ray focuses on compute,
[00:24:48.400 --> 00:24:50.760]   for things like training and serving and so on,
[00:24:50.760 --> 00:24:53.120]   our users, if you're using Ray,
[00:24:53.120 --> 00:24:55.300]   you have to integrate with all of these other tools, right?
[00:24:55.300 --> 00:25:00.860]   And so we do prioritize integration with the rest of the ML ecosystem,
[00:25:00.860 --> 00:25:04.120]   whether that is tools like weights and biases or beyond.
[00:25:04.120 --> 00:25:08.360]   I think one, I'm a big fan of anything,
[00:25:08.360 --> 00:25:12.500]   any tools that really help improve observability
[00:25:12.620 --> 00:25:14.020]   and the debugging experience,
[00:25:14.020 --> 00:25:17.400]   because I think one of the big challenges
[00:25:17.400 --> 00:25:20.700]   with distributed systems is just when something goes wrong,
[00:25:20.700 --> 00:25:25.400]   do you know what went wrong and do you know how to solve it?
[00:25:25.400 --> 00:25:27.700]   And that's obviously it's hard, of course,
[00:25:27.700 --> 00:25:31.020]   when you're debugging like a single threaded program,
[00:25:31.020 --> 00:25:33.320]   but when you're debugging a distributed application,
[00:25:33.320 --> 00:25:34.820]   it just gets so much worse.
[00:25:34.820 --> 00:25:40.200]   And when it comes to tools that do model monitoring,
[00:25:40.200 --> 00:25:41.460]   I think are in the spirit,
[00:25:41.600 --> 00:25:44.760]   there's companies like Arise, which I think do a great job and others.
[00:25:44.760 --> 00:25:50.360]   But really, companies solving this problem
[00:25:50.360 --> 00:25:52.120]   of when something goes wrong,
[00:25:52.120 --> 00:25:55.060]   do you know immediately just what went wrong and how to solve it?
[00:25:55.060 --> 00:26:00.760]   Yep. Yeah, we do hear that in our user base a lot more than before.
[00:26:00.760 --> 00:26:04.060]   I think on the training side, there are some options.
[00:26:04.060 --> 00:26:06.360]   On the production side and inference side,
[00:26:06.360 --> 00:26:08.700]   I think more and more that observability need
[00:26:08.700 --> 00:26:11.060]   and what exactly you want to log and track
[00:26:11.200 --> 00:26:14.520]   and how do you act on it is becoming a pretty strong need.
[00:26:14.520 --> 00:26:16.260]   So totally hear you on that.
[00:26:16.260 --> 00:26:20.360]   Cool. Thanks for handling those spicy questions so gracefully, everyone.
[00:26:20.360 --> 00:26:24.520]   So with that, let's go in another not that spicy,
[00:26:24.520 --> 00:26:27.120]   but still probably an interesting and controversial question
[00:26:27.120 --> 00:26:28.860]   for some of our users.
[00:26:28.860 --> 00:26:32.200]   Both any scale as well as replicate,
[00:26:32.200 --> 00:26:36.720]   you are working with open source community and infrastructure.
[00:26:36.720 --> 00:26:39.360]   And Ben, I think you have your product
[00:26:39.500 --> 00:26:41.400]   that sounds like open source any skill,
[00:26:41.400 --> 00:26:44.400]   of course, as the real open source for a while.
[00:26:44.400 --> 00:26:48.300]   Given you have both enterprise mindset
[00:26:48.300 --> 00:26:52.000]   and the open source community and open source product offering.
[00:26:52.000 --> 00:26:55.260]   The question I always have for user,
[00:26:55.260 --> 00:26:59.300]   how do they decide whether they keep using an open source product
[00:26:59.300 --> 00:27:03.320]   or when do they move to an enterprise version of that product?
[00:27:03.320 --> 00:27:05.460]   I think it's probably a very age old question
[00:27:05.460 --> 00:27:08.660]   coming from the Databricks time, a long time ago,
[00:27:08.800 --> 00:27:13.160]   that when we kind of started this open core movement in many ways.
[00:27:13.160 --> 00:27:16.920]   But curious on your thought that Robert.
[00:27:16.920 --> 00:27:19.960]   Yeah, well, I think there are companies,
[00:27:19.960 --> 00:27:23.860]   I would say it comes down to a build versus buy kind of decision.
[00:27:23.860 --> 00:27:26.360]   You know, a lot of companies, you can,
[00:27:26.360 --> 00:27:29.860]   many companies use Ray successfully on their own, right.
[00:27:29.860 --> 00:27:35.460]   But many of these companies also, you know, build tooling around Ray,
[00:27:35.460 --> 00:27:38.460]   right and you can imagine companies like Uber,
[00:27:38.600 --> 00:27:40.300]   which use run all of their deep learning
[00:27:40.300 --> 00:27:42.160]   and classical machine learning on Ray.
[00:27:42.160 --> 00:27:44.520]   You know, they have a large machine learning infrastructure team
[00:27:44.520 --> 00:27:48.160]   to really to manage that, you know, that's on call.
[00:27:48.160 --> 00:27:51.360]   And it's building a lot of the additional tooling around it,
[00:27:51.360 --> 00:27:55.400]   whether it is observability or job submission or dashboards
[00:27:55.400 --> 00:27:58.700]   and collaboration tools and reproducibility and so on, right.
[00:27:58.700 --> 00:28:02.100]   So I think for a lot of companies, you know,
[00:28:02.100 --> 00:28:06.200]   they're looking to essentially focus on product development,
[00:28:06.320 --> 00:28:09.020]   focus on, you know, the actual machine
[00:28:09.020 --> 00:28:11.320]   learning model development and things like that
[00:28:11.320 --> 00:28:14.820]   and are not as interested in spending time building
[00:28:14.820 --> 00:28:17.220]   and managing the infrastructure for compute if they don't,
[00:28:17.220 --> 00:28:19.320]   you know, if they don't have to.
[00:28:19.320 --> 00:28:23.440]   And so that's when we see companies choosing to go with any scale
[00:28:23.440 --> 00:28:25.820]   or looking to go with a managed services,
[00:28:25.820 --> 00:28:29.120]   basically, can they move faster?
[00:28:29.120 --> 00:28:32.740]   And of course, the fact that, you know, we created Ray,
[00:28:32.740 --> 00:28:34.980]   we're continuing to develop Ray,
[00:28:35.120 --> 00:28:37.640]   we can add a lot of value on top of Ray,
[00:28:37.640 --> 00:28:41.580]   we can, you know, support their teams and sort of de-risk it for them.
[00:28:41.580 --> 00:28:44.440]   That's another factor that is important for them.
[00:28:44.440 --> 00:28:47.820]   Ben, what about you?
[00:28:47.820 --> 00:28:51.340]   I don't think you necessarily need to decide anymore.
[00:28:51.340 --> 00:28:53.880]   I mean, most infrastructure companies are now built
[00:28:53.880 --> 00:28:56.620]   around this concept of open core.
[00:28:56.620 --> 00:29:01.780]   And the beauty of open core is it just is the best of both worlds
[00:29:01.920 --> 00:29:06.980]   and as a, you know, user of these systems,
[00:29:06.980 --> 00:29:10.980]   you're not locked into any particular technology.
[00:29:10.980 --> 00:29:14.880]   But it also means that there's commercial backing behind the thing
[00:29:14.880 --> 00:29:17.920]   and probably some kind of broader commercial product
[00:29:17.920 --> 00:29:20.540]   built around it that, you know, is a complete solution
[00:29:20.540 --> 00:29:22.980]   rather than the little open source bit in the middle.
[00:29:22.980 --> 00:29:27.780]   And it also means that, you know, from the company's point of view,
[00:29:27.780 --> 00:29:31.040]   you know, it doesn't look on the surface
[00:29:31.180 --> 00:29:35.040]   that it's a good thing that anyone can switch away from you really easily.
[00:29:35.040 --> 00:29:37.320]   But it puts the incentives in the right place
[00:29:37.320 --> 00:29:40.580]   in that we are incentivized to build a great product
[00:29:40.580 --> 00:29:42.320]   because otherwise you can move away.
[00:29:42.320 --> 00:29:44.920]   And, you know, for example, working at Docker,
[00:29:44.920 --> 00:29:48.280]   it worked fantastically because it was in Docker's incentive
[00:29:48.280 --> 00:29:51.280]   for as many people as possible to use Docker containers.
[00:29:51.280 --> 00:29:52.740]   If everybody uses Docker containers,
[00:29:52.740 --> 00:29:54.980]   then somehow this is going to be good for Docker.
[00:29:54.980 --> 00:29:58.720]   And that was so they wanted it to be open source
[00:29:58.720 --> 00:30:00.540]   and wanted everyone to be able to use it.
[00:30:00.680 --> 00:30:03.680]   And yeah, that's what it's extremely well for Docker.
[00:30:03.680 --> 00:30:07.540]   Yeah, makes sense. What is your perspective?
[00:30:07.540 --> 00:30:11.480]   Yeah, so I mean, NVIDIA is also engaged
[00:30:11.480 --> 00:30:13.340]   in a lot of open source communities,
[00:30:13.340 --> 00:30:15.340]   certainly the deep learning frameworks,
[00:30:15.340 --> 00:30:19.680]   Jack's, you know, Spark and, you know,
[00:30:19.680 --> 00:30:21.680]   the sort of pie dating ecosystem more broadly
[00:30:21.680 --> 00:30:24.240]   are all sort of NVIDIA's open source engagements.
[00:30:24.240 --> 00:30:27.680]   And I think like you to sort of Robert's point,
[00:30:27.820 --> 00:30:30.960]   it's really it's not an either or question, right?
[00:30:30.960 --> 00:30:32.720]   It's what do I want to support myself
[00:30:32.720 --> 00:30:35.280]   versus what do I want someone else to support for me?
[00:30:35.280 --> 00:30:38.320]   And I think there's a lot of value in saying,
[00:30:38.320 --> 00:30:41.320]   here's a distribution of a project that you want to use.
[00:30:41.320 --> 00:30:44.660]   We've tested it, we've certified it with our partners.
[00:30:44.660 --> 00:30:48.680]   We, you know, we will support you if something goes wrong.
[00:30:48.680 --> 00:30:50.220]   There's also a lot of value in saying,
[00:30:50.220 --> 00:30:53.980]   here's a managed service that's built on some of these open source components
[00:30:53.980 --> 00:30:56.120]   and you're going to pay us to maintain it for you.
[00:30:56.660 --> 00:30:58.920]   Right? I think those are both great models.
[00:30:58.920 --> 00:31:02.880]   And certainly, one thing that's really been attractive to me,
[00:31:02.880 --> 00:31:04.620]   I've been working in open source for a long time.
[00:31:04.620 --> 00:31:06.860]   One thing that's really attractive to me about it is,
[00:31:06.860 --> 00:31:10.980]   your customers pay you, like Ben said, because they want to,
[00:31:10.980 --> 00:31:13.360]   because you have a great product, not because they have to.
[00:31:13.360 --> 00:31:17.220]   Right. Have to. Yeah. Yeah. Indeed. Yeah.
[00:31:17.220 --> 00:31:20.120]   And then I think we see bits and biases being used
[00:31:20.120 --> 00:31:22.920]   as we have the pre-personal version.
[00:31:23.520 --> 00:31:26.680]   A lot of customers already start using that,
[00:31:26.680 --> 00:31:29.340]   trying that experience, feeling really good about it.
[00:31:29.340 --> 00:31:30.740]   And when their needs change,
[00:31:30.740 --> 00:31:33.180]   usually that's when they make that decision on enterprise.
[00:31:33.180 --> 00:31:36.340]   But I do see like there is still that build versus buy
[00:31:36.340 --> 00:31:38.040]   that I think Robert, you were mentioning,
[00:31:38.040 --> 00:31:40.140]   that is always kind of playing a role for it.
[00:31:40.140 --> 00:31:43.180]   Like, do you want to take that ownership now or later?
[00:31:43.180 --> 00:31:46.040]   You want to push it out for a future cost or not,
[00:31:46.040 --> 00:31:49.520]   is basically what oftentimes kind of dictates.
[00:31:49.520 --> 00:31:52.780]   So I'm glad to hear that we are very aligned.
[00:31:53.500 --> 00:31:55.340]   On that understanding.
[00:31:55.340 --> 00:31:59.700]   So then thanks everyone for your attention.
[00:31:59.700 --> 00:32:04.000]   And thanks for all this really insightful thoughts
[00:32:04.000 --> 00:32:05.800]   from all our panelists.
[00:32:05.800 --> 00:32:11.180]   And hopefully the users that attended this panel
[00:32:11.180 --> 00:32:13.240]   got something great out of it.
[00:32:13.240 --> 00:32:14.880]   Please let us know about your feedback.
[00:32:14.880 --> 00:32:17.680]   And thanks again for all the panelists.
[00:32:17.680 --> 00:32:19.600]   We'll see you in the next session.
[00:32:19.600 --> 00:32:21.300]   Thank you.
[00:32:21.440 --> 00:32:23.040]   Thank you. Bye-bye.
[00:32:23.040 --> 00:32:25.960]   (mysterious music)
[00:32:25.960 --> 00:32:28.540]   (upbeat music)

