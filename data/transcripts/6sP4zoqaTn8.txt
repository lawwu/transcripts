
[00:00:00.000 --> 00:00:03.280]   [MUSIC PLAYING]
[00:00:03.280 --> 00:00:04.040]   Hi, everybody.
[00:00:04.040 --> 00:00:06.600]   I'm Thomas Davis, a lead researcher here
[00:00:06.600 --> 00:00:09.080]   at Monsters, Aliens, Robots, and Zombies.
[00:00:09.080 --> 00:00:11.660]   From now on, I'll call it Mars, but I always like the name
[00:00:11.660 --> 00:00:13.280]   because it's fun.
[00:00:13.280 --> 00:00:16.120]   As you might have guessed from the name and this title screen,
[00:00:16.120 --> 00:00:18.760]   Mars is a VFX company.
[00:00:18.760 --> 00:00:21.000]   But if you ask our marketing team,
[00:00:21.000 --> 00:00:24.680]   Mars is an AI technology startup and VFX studio
[00:00:24.680 --> 00:00:27.480]   on a mission to help Hollywood studios push creative
[00:00:27.480 --> 00:00:30.520]   boundaries and solve the systemic and growing
[00:00:30.520 --> 00:00:33.720]   challenges facing the entertainment industry today.
[00:00:33.720 --> 00:00:35.480]   So really, we're a VFX studio that
[00:00:35.480 --> 00:00:38.280]   has started to foray into AI tools
[00:00:38.280 --> 00:00:41.080]   to really just address some of the capacity constraints
[00:00:41.080 --> 00:00:44.760]   that exist in VFX today and the capacity constraints
[00:00:44.760 --> 00:00:49.200]   that we've all heard a ton of about in the industry.
[00:00:49.200 --> 00:00:51.200]   So just some cool slides, some of the things
[00:00:51.200 --> 00:00:53.960]   that we actually work on here at Mars.
[00:00:53.960 --> 00:00:57.240]   As a company, we've worked on some very cool projects
[00:00:57.240 --> 00:01:01.080]   like Thing from Netflix's new Wednesday Atoms series.
[00:01:01.080 --> 00:01:03.440]   And this was one where we actually received the greatest
[00:01:03.440 --> 00:01:07.480]   compliment of a VFX firm, where everybody actually thought
[00:01:07.480 --> 00:01:09.520]   this was practically shot.
[00:01:09.520 --> 00:01:12.200]   So practically shot as an actor was there and acting out
[00:01:12.200 --> 00:01:13.200]   every single scene.
[00:01:13.200 --> 00:01:17.040]   But in reality, each one of these, for the harder shots,
[00:01:17.040 --> 00:01:19.920]   were actually modeled, textured, animated, and integrated
[00:01:19.920 --> 00:01:21.760]   entirely by Mars.
[00:01:21.760 --> 00:01:24.000]   So there was the practical actor for some,
[00:01:24.000 --> 00:01:26.720]   but for a lot of the harder shots, Mars was responsible.
[00:01:26.720 --> 00:01:28.400]   Another one that you may have seen us on
[00:01:28.400 --> 00:01:31.440]   is Moon Knight, where we worked on the cape and its interaction
[00:01:31.440 --> 00:01:32.400]   with the surroundings.
[00:01:32.400 --> 00:01:35.200]   Sadly, these are kind of the last of the cool videos.
[00:01:35.200 --> 00:01:36.880]   I don't actually work on these projects,
[00:01:36.880 --> 00:01:39.840]   but I wanted to give context of our company as a whole.
[00:01:39.840 --> 00:01:41.240]   And I think they're really cool.
[00:01:41.240 --> 00:01:44.560]   So you might be asking, why am I actually here?
[00:01:44.560 --> 00:01:46.560]   It's not to show these cool visuals.
[00:01:46.560 --> 00:01:48.400]   Well, in general, our company has
[00:01:48.400 --> 00:01:51.320]   kind of two categories of solutions in our portfolio.
[00:01:51.320 --> 00:01:54.680]   One is core VFX, and the other is vanity AI.
[00:01:54.680 --> 00:01:57.160]   Today, I'll really be talking about vanity AI.
[00:01:57.160 --> 00:01:59.800]   I'm sorry, because you won't see any more of those cool visuals.
[00:01:59.800 --> 00:02:02.240]   But luckily, we're really excited to show
[00:02:02.240 --> 00:02:04.320]   what we've been working on and what we've built
[00:02:04.320 --> 00:02:05.880]   over the past few years.
[00:02:05.880 --> 00:02:11.440]   First, why is a VFX company building ML tools?
[00:02:11.440 --> 00:02:15.080]   And in short, there really are fewer artists, higher demand,
[00:02:15.080 --> 00:02:17.040]   and a sustaining level of quality
[00:02:17.040 --> 00:02:19.000]   being required by Hollywood.
[00:02:19.000 --> 00:02:21.360]   So last year, to give some numbers,
[00:02:21.360 --> 00:02:24.520]   television released 593 new series
[00:02:24.520 --> 00:02:27.960]   with over 27% growth rate from previous years.
[00:02:27.960 --> 00:02:30.080]   Spend is over $100 billion.
[00:02:30.080 --> 00:02:33.200]   And in general, people expect and want more spectacles.
[00:02:33.200 --> 00:02:34.000]   They want dragons.
[00:02:34.000 --> 00:02:35.000]   They want explosions.
[00:02:35.000 --> 00:02:38.760]   They want realistic hand motion like in Wednesday.
[00:02:38.760 --> 00:02:41.040]   And they want this at Hollywood quality.
[00:02:41.040 --> 00:02:45.520]   No longer can TV be at a lower grade.
[00:02:45.520 --> 00:02:47.680]   So all of this is really just to say that we
[00:02:47.680 --> 00:02:49.880]   need to be able to do this work faster,
[00:02:49.880 --> 00:02:52.520]   and we need to be able to do it cheaper for these industries
[00:02:52.520 --> 00:02:53.520]   to scale.
[00:02:53.520 --> 00:02:58.480]   So in general, like AI companies or startups,
[00:02:58.480 --> 00:03:01.240]   we are trying to democratize VFX.
[00:03:01.240 --> 00:03:03.000]   So what does that actually mean for Mars?
[00:03:03.000 --> 00:03:05.960]   As a leading AI-enabled technology organization,
[00:03:05.960 --> 00:03:07.960]   we're really just investing heavily
[00:03:07.960 --> 00:03:10.400]   in creating solutions that exceed
[00:03:10.400 --> 00:03:12.960]   four specific requirements.
[00:03:12.960 --> 00:03:15.880]   So as I said, we want to be able to produce feature film
[00:03:15.880 --> 00:03:18.920]   quality VFX so we can't compromise on the quality.
[00:03:18.920 --> 00:03:21.440]   We want to be able to offer it at affordable pricing.
[00:03:21.440 --> 00:03:24.360]   And we want to execute these on the rapid timelines required
[00:03:24.360 --> 00:03:26.320]   for these studios.
[00:03:26.320 --> 00:03:28.800]   Finally, we need to be able to do this on demand
[00:03:28.800 --> 00:03:32.080]   without constraints and without making concessions.
[00:03:32.080 --> 00:03:35.160]   So overall, our mission is to enable production
[00:03:35.160 --> 00:03:38.120]   with Hollywood content on TV timelines.
[00:03:38.120 --> 00:03:39.840]   And really, what I'm presenting today
[00:03:39.840 --> 00:03:43.040]   is that we're building AI tools to get us there.
[00:03:43.040 --> 00:03:46.920]   So the tool that I'll be talking about today is Vanity AI.
[00:03:46.920 --> 00:03:48.520]   We just released it publicly.
[00:03:48.520 --> 00:03:52.360]   I think maybe two months ago when you guys see this.
[00:03:52.360 --> 00:03:55.880]   Vanity AI, in general, is a production-ready solution
[00:03:55.880 --> 00:03:58.840]   that empowers VFX teams and Hollywood in general
[00:03:58.840 --> 00:04:02.440]   to deliver large volumes of high-end 2D aging,
[00:04:02.440 --> 00:04:06.280]   de-aging, cosmetic, wig, and prosthetic fixes.
[00:04:06.280 --> 00:04:08.560]   In general, really what we're offering here
[00:04:08.560 --> 00:04:11.560]   is a tool that can do this work, on average,
[00:04:11.560 --> 00:04:15.040]   300 times faster than traditional VFX pipelines
[00:04:15.040 --> 00:04:17.880]   at a significantly better cost.
[00:04:17.880 --> 00:04:20.000]   And with no real capacity constraints.
[00:04:20.000 --> 00:04:21.600]   So previous work is always limited
[00:04:21.600 --> 00:04:23.440]   by how many artists are available
[00:04:23.440 --> 00:04:26.680]   and what is the timeline and budget of the project.
[00:04:26.680 --> 00:04:30.840]   By using ML, we're able to bypass this capacity constraint.
[00:04:30.840 --> 00:04:34.320]   So in layman terms, our tool makes it really, really easy
[00:04:34.320 --> 00:04:35.960]   to produce shots like this one
[00:04:35.960 --> 00:04:37.800]   for cheaper and faster than before.
[00:04:37.800 --> 00:04:39.200]   So what we're actually seeing here
[00:04:39.200 --> 00:04:43.240]   is the original actor with wrinkles and some eye bags,
[00:04:43.240 --> 00:04:45.000]   and you can see the chin line as well.
[00:04:45.000 --> 00:04:46.160]   And to make them look younger,
[00:04:46.160 --> 00:04:49.640]   we are bringing those in, smoothing out wrinkles
[00:04:49.640 --> 00:04:53.040]   to essentially achieve the look desired by the studio.
[00:04:53.040 --> 00:04:55.720]   In general, what does Vanity AI do?
[00:04:55.720 --> 00:04:58.120]   Well, faces are extremely complex.
[00:04:58.120 --> 00:05:01.800]   These are just some of the edits that are required,
[00:05:01.800 --> 00:05:04.080]   or some of the features of a face
[00:05:04.080 --> 00:05:06.560]   that actually give a person an indication of age.
[00:05:06.560 --> 00:05:08.560]   So what our brain picks up on
[00:05:08.560 --> 00:05:11.440]   on how we identify the age of a person.
[00:05:11.440 --> 00:05:14.440]   And in general, Vanity AI tasks can be as simple
[00:05:14.440 --> 00:05:17.240]   as just removing eye bags in a few shots
[00:05:17.240 --> 00:05:20.560]   because someone didn't sleep well before the shoot,
[00:05:20.560 --> 00:05:23.560]   or they can be as complex as the one we saw before,
[00:05:23.560 --> 00:05:26.080]   where we're actually doing a 20-year de-age,
[00:05:26.080 --> 00:05:29.000]   targeting essentially all of the features here.
[00:05:29.000 --> 00:05:30.600]   So Vanity AI can actually be used
[00:05:30.600 --> 00:05:32.800]   for either just simple wig fixes,
[00:05:32.800 --> 00:05:35.520]   where we wanna bring that wig line that's visible
[00:05:35.520 --> 00:05:38.280]   in some shots out or eye bags,
[00:05:38.280 --> 00:05:40.680]   and can also be targeted to do a full de-age,
[00:05:40.680 --> 00:05:42.280]   like Benjamin Button style.
[00:05:42.280 --> 00:05:44.760]   So first I wanted to give a little bit of context
[00:05:44.760 --> 00:05:45.920]   on how it's done today,
[00:05:45.920 --> 00:05:49.520]   because I think it shows you why this solution
[00:05:49.520 --> 00:05:53.840]   is so important and needed for de-aging beauty fixes to scale.
[00:05:53.840 --> 00:05:58.840]   So essentially, vanity work today is extremely cumbersome
[00:05:58.840 --> 00:06:00.600]   and a very manual process.
[00:06:00.600 --> 00:06:02.920]   What we see happening here is that the artist
[00:06:02.920 --> 00:06:05.720]   is actually tracking key points on the face
[00:06:05.720 --> 00:06:07.320]   as a method of tracking edits
[00:06:07.320 --> 00:06:09.920]   that they then make to regions of a face.
[00:06:09.920 --> 00:06:11.480]   So for each region being edited,
[00:06:11.480 --> 00:06:15.960]   whether it's eye bags, lap lines, nasolabial folds,
[00:06:15.960 --> 00:06:18.720]   you kind of get the point, a mask has to be created.
[00:06:18.720 --> 00:06:22.640]   So for every single region of edit, you create a mask.
[00:06:22.640 --> 00:06:24.800]   Then within each one of these regions,
[00:06:24.800 --> 00:06:29.800]   you're essentially building a series of image manipulations
[00:06:29.800 --> 00:06:33.280]   that then target the desired look within that region.
[00:06:33.280 --> 00:06:35.480]   So these are things like blur, retexturing,
[00:06:35.480 --> 00:06:36.960]   changing the color.
[00:06:36.960 --> 00:06:38.200]   And what we see is that the artist
[00:06:38.200 --> 00:06:41.600]   is creating these polygons manually,
[00:06:41.600 --> 00:06:43.560]   tracking them from frame to frame.
[00:06:43.560 --> 00:06:45.680]   And then for the rest of this two-minute video,
[00:06:45.680 --> 00:06:49.200]   which is sped up, I think like 10 times,
[00:06:49.200 --> 00:06:51.760]   they're actually just tweaking that tracking,
[00:06:51.760 --> 00:06:55.400]   tweaking the regions that they're editing
[00:06:55.400 --> 00:06:58.160]   and ensuring that all the edits that they apply
[00:06:58.160 --> 00:07:01.280]   are actually temporarily consistent throughout the shot.
[00:07:01.280 --> 00:07:02.880]   And so as you can see,
[00:07:02.880 --> 00:07:05.760]   artists have complete control over the flow today
[00:07:05.760 --> 00:07:07.960]   with very advanced tools that allow them to do
[00:07:07.960 --> 00:07:09.480]   what's called rotoscoping
[00:07:09.480 --> 00:07:11.400]   and apply the edits to those regions.
[00:07:11.400 --> 00:07:15.680]   But it's still a very manual process.
[00:07:15.680 --> 00:07:17.000]   And on a shot like this,
[00:07:17.000 --> 00:07:19.840]   it actually takes them about over three days
[00:07:19.840 --> 00:07:21.160]   to do this work.
[00:07:21.160 --> 00:07:23.160]   And you can see here the before and after,
[00:07:23.160 --> 00:07:24.800]   which is pretty cool.
[00:07:24.800 --> 00:07:27.240]   So one of the big benefits of actually working at Mars
[00:07:27.240 --> 00:07:30.080]   is that we get to understand exactly the workflows
[00:07:30.080 --> 00:07:31.760]   that artists go through today
[00:07:31.760 --> 00:07:34.080]   and work directly with them to try to optimize
[00:07:34.080 --> 00:07:35.440]   and speed up those workflows.
[00:07:35.440 --> 00:07:37.480]   So we can benchmark performance,
[00:07:37.480 --> 00:07:38.720]   interview our customers
[00:07:38.720 --> 00:07:40.640]   'cause they're actually within the company
[00:07:40.640 --> 00:07:42.520]   and gather requirements on our products
[00:07:42.520 --> 00:07:44.720]   just by going into the office for the day.
[00:07:44.720 --> 00:07:46.480]   Now that we understand the actual work
[00:07:46.480 --> 00:07:49.280]   that goes into a single shot,
[00:07:49.280 --> 00:07:51.880]   the thing to keep in mind is that that exact same look
[00:07:51.880 --> 00:07:54.600]   now needs to be applied to all shots of that character
[00:07:54.600 --> 00:07:56.120]   across the entire film.
[00:07:56.120 --> 00:07:58.640]   So once the look development is actually done,
[00:07:58.640 --> 00:08:01.480]   a simple DH beauty or wig fix job
[00:08:01.480 --> 00:08:05.160]   typically actually contains anywhere from 20 to 50 shots.
[00:08:05.160 --> 00:08:07.480]   So essentially we're asking artists to do this
[00:08:07.480 --> 00:08:09.560]   200 times more.
[00:08:09.560 --> 00:08:10.840]   And the key detail here
[00:08:10.840 --> 00:08:13.240]   is that they actually start from scratch every time.
[00:08:13.240 --> 00:08:15.640]   So although you've done the look development
[00:08:15.640 --> 00:08:16.720]   on a single shot,
[00:08:16.720 --> 00:08:18.920]   the client has approved that single shot,
[00:08:18.920 --> 00:08:21.600]   there is no mechanism in tools today
[00:08:21.600 --> 00:08:25.080]   that actually allow you to apply that change to other shots.
[00:08:25.080 --> 00:08:27.800]   So this problem is really what motivated
[00:08:27.800 --> 00:08:30.360]   the creation of Vanity AI.
[00:08:30.360 --> 00:08:32.320]   And in general, artists don't mind
[00:08:32.320 --> 00:08:33.720]   creating the look development shot.
[00:08:33.720 --> 00:08:35.040]   It's a fun job.
[00:08:35.040 --> 00:08:36.240]   It's the actual art.
[00:08:36.240 --> 00:08:38.640]   They get to work with the client,
[00:08:38.640 --> 00:08:41.920]   achieve that desired look, revise, repeat,
[00:08:41.920 --> 00:08:45.320]   and basically really refine what does someone
[00:08:45.320 --> 00:08:47.720]   who's 20 years younger look like
[00:08:47.720 --> 00:08:50.520]   and how does the client want them to look.
[00:08:50.520 --> 00:08:53.360]   However, then taking that approved look
[00:08:53.360 --> 00:08:56.680]   and applying it to 200 more shots is just tiring.
[00:08:56.680 --> 00:08:59.600]   So really our goal of building Vanity AI
[00:08:59.600 --> 00:09:02.560]   is to allow artists to focus on the art they enjoy
[00:09:02.560 --> 00:09:04.440]   while automating some of the repetitive work
[00:09:04.440 --> 00:09:07.120]   that they would really rather not be doing.
[00:09:07.120 --> 00:09:10.400]   So luckily, all the previous shows
[00:09:10.400 --> 00:09:13.000]   using traditional methods that Mars has worked on
[00:09:13.000 --> 00:09:15.160]   work for nothing.
[00:09:15.160 --> 00:09:17.760]   So they actually give us extremely rich data
[00:09:17.760 --> 00:09:20.560]   in the form of before and after tuples.
[00:09:20.560 --> 00:09:22.160]   So you can see this kind of image here
[00:09:22.160 --> 00:09:26.000]   that shows us the example of kind of an input
[00:09:26.000 --> 00:09:28.120]   and what we expect as output.
[00:09:28.120 --> 00:09:31.840]   And we have this for, for example, for this show,
[00:09:31.840 --> 00:09:36.000]   we have that exact same edit applied to 250 shots.
[00:09:36.000 --> 00:09:39.360]   And as we all know, problem-specific ground truth data
[00:09:39.360 --> 00:09:40.960]   is really just gold for ML.
[00:09:40.960 --> 00:09:44.520]   This is exactly what we want to be able to work with.
[00:09:44.520 --> 00:09:46.440]   And it gets even better than that
[00:09:46.440 --> 00:09:49.040]   in that we actually have this data for all of the shows
[00:09:49.040 --> 00:09:51.600]   that we have worked on historically.
[00:09:51.600 --> 00:09:54.600]   So this is really when I joined kind of like a gold rush
[00:09:54.600 --> 00:09:59.280]   of data where not only do we have for every single edit
[00:09:59.280 --> 00:10:03.160]   250 clips, each with 500 frames, for example,
[00:10:03.160 --> 00:10:05.720]   we also have the 30 or so shows
[00:10:05.720 --> 00:10:07.560]   that we had actually previously worked on
[00:10:07.560 --> 00:10:10.840]   and got permission to use as training data.
[00:10:10.840 --> 00:10:12.840]   So really all this artist pain,
[00:10:12.840 --> 00:10:15.760]   not only identified Vanity AI as the product
[00:10:15.760 --> 00:10:17.000]   that we should build,
[00:10:17.000 --> 00:10:19.960]   but it also gave us the resources to build it.
[00:10:19.960 --> 00:10:22.760]   So I wanted to start off by showing
[00:10:22.760 --> 00:10:26.640]   the new Vanity AI interface.
[00:10:26.640 --> 00:10:29.400]   This is a demo interface, so keep that in mind.
[00:10:29.400 --> 00:10:31.280]   But in this example, what we're seeing
[00:10:31.280 --> 00:10:35.680]   is an artist performing a general DH job.
[00:10:35.680 --> 00:10:37.760]   So they are removing laugh lines, eye bags,
[00:10:37.760 --> 00:10:39.960]   and wrinkles throughout the face.
[00:10:39.960 --> 00:10:44.240]   We see how artists are able to control the output
[00:10:44.240 --> 00:10:46.200]   by interacting with the various sliders,
[00:10:46.200 --> 00:10:48.800]   controlling the blend strength, edit strength,
[00:10:48.800 --> 00:10:51.960]   bringing back and removing textures.
[00:10:51.960 --> 00:10:54.520]   And then just like we saw in the traditional flow,
[00:10:54.520 --> 00:10:57.720]   artists are then able to apply masks
[00:10:57.720 --> 00:11:00.080]   and localize edits to specific regions.
[00:11:00.080 --> 00:11:02.320]   So when they play with those sliders,
[00:11:02.320 --> 00:11:06.320]   you then see the edits only applied in those regions.
[00:11:06.320 --> 00:11:10.880]   And all this is to say that artists in our new design
[00:11:10.880 --> 00:11:12.960]   are still in control of the output.
[00:11:12.960 --> 00:11:16.840]   We're not telling them what results specifically to choose,
[00:11:16.840 --> 00:11:19.680]   but letting them use Vanity AI as a tool
[00:11:19.680 --> 00:11:22.000]   to produce their results faster.
[00:11:22.000 --> 00:11:25.840]   So you might be asking, this looks just as complicated
[00:11:25.840 --> 00:11:28.480]   as the traditional flow that we showed.
[00:11:28.480 --> 00:11:31.200]   But the difference here is that when an artist edits
[00:11:31.200 --> 00:11:33.400]   a single frame, those changes are actually
[00:11:33.400 --> 00:11:36.400]   propagated to all other frames within the shot.
[00:11:36.400 --> 00:11:39.360]   And further, those changes on a single frame in a shot
[00:11:39.360 --> 00:11:42.920]   are propagated to all shots that we have of that identity.
[00:11:42.920 --> 00:11:48.920]   So no longer are you just wasting work on a single shot.
[00:11:48.920 --> 00:11:51.200]   You're actually editing a single shot
[00:11:51.200 --> 00:11:54.520]   and seeing those changes applied to all shots
[00:11:54.520 --> 00:11:55.840]   within a collection.
[00:11:55.840 --> 00:11:59.320]   So instead of being asked to apply the same edit 200 times,
[00:11:59.320 --> 00:12:02.640]   you're doing it once and simply tweaking and verifying
[00:12:02.640 --> 00:12:03.680]   the results.
[00:12:03.680 --> 00:12:05.800]   So we see this as artists are essentially
[00:12:05.800 --> 00:12:10.160]   able to elevate themselves above to be able to work only
[00:12:10.160 --> 00:12:11.960]   on the art, so the look development
[00:12:11.960 --> 00:12:15.400]   and the actual defining of the look for the client,
[00:12:15.400 --> 00:12:18.760]   while Vanity AI then is able to apply those changes
[00:12:18.760 --> 00:12:22.120]   to all other shots within the collection.
[00:12:22.120 --> 00:12:24.680]   So you can just imagine the time savings that come from this.
[00:12:24.680 --> 00:12:26.880]   And so now I'm just going to show all of the results
[00:12:26.880 --> 00:12:29.960]   that I showed before, but with the context of these
[00:12:29.960 --> 00:12:32.600]   are actually generated from this scheme.
[00:12:32.600 --> 00:12:35.720]   So a single frame is edited, and the changes
[00:12:35.720 --> 00:12:39.560]   are propagated to the rest of the shot and other shots
[00:12:39.560 --> 00:12:40.960]   within the set.
[00:12:40.960 --> 00:12:42.400]   And I'll show just one other show
[00:12:42.400 --> 00:12:45.160]   to prove that we do actually work
[00:12:45.160 --> 00:12:49.160]   in different de-aging tasks, not just on that one character.
[00:12:49.160 --> 00:12:52.400]   So I did want to actually loop in weights and bias
[00:12:52.400 --> 00:12:56.120]   a little bit, because it has been a really important tool
[00:12:56.120 --> 00:12:57.480]   for us here at Mars.
[00:12:57.480 --> 00:12:59.120]   So I'll talk a little bit about that.
[00:12:59.120 --> 00:13:02.280]   And sorry in advance, because I recently
[00:13:02.280 --> 00:13:05.000]   found out about this new cut out people feature in PowerPoint,
[00:13:05.000 --> 00:13:06.580]   and I'm a little bit obsessed with it.
[00:13:06.580 --> 00:13:10.360]   So you'll see this guy throughout the slides.
[00:13:10.360 --> 00:13:13.280]   But in general, pretty early on in Vanity research,
[00:13:13.280 --> 00:13:17.760]   so we've been working on this for about three years now.
[00:13:17.760 --> 00:13:19.120]   I joined a year ago.
[00:13:19.120 --> 00:13:21.600]   But in general, we were producing results
[00:13:21.600 --> 00:13:23.880]   that we thought were really pretty awesome.
[00:13:23.880 --> 00:13:27.080]   So those videos that I showed, we
[00:13:27.080 --> 00:13:29.040]   were producing things like that, and we thought
[00:13:29.040 --> 00:13:31.240]   from the data we were given, we were
[00:13:31.240 --> 00:13:33.840]   producing results that looked almost identical to the target
[00:13:33.840 --> 00:13:34.480]   in our eyes.
[00:13:34.480 --> 00:13:36.480]   So we were pretty excited.
[00:13:36.480 --> 00:13:39.880]   And as I said earlier, one of the most valuable aspects
[00:13:39.880 --> 00:13:44.280]   of a tool like this being built by a VFX firm for VFX artists
[00:13:44.280 --> 00:13:47.280]   is that we are able to gather requirements and get feedback
[00:13:47.280 --> 00:13:48.720]   really, really quickly.
[00:13:48.720 --> 00:13:50.760]   So once we were able to produce results on the data
[00:13:50.760 --> 00:13:52.280]   that we were given, we immediately
[00:13:52.280 --> 00:13:54.080]   sent them over to the artists.
[00:13:54.080 --> 00:13:58.320]   And the artists thought they were not that great.
[00:13:58.320 --> 00:14:00.720]   So in general, this was one of the findings
[00:14:00.720 --> 00:14:03.560]   of our feedback loop is we really
[00:14:03.560 --> 00:14:06.440]   did have to share things with artists quickly,
[00:14:06.440 --> 00:14:09.000]   get feedback very quickly, and integrate it
[00:14:09.000 --> 00:14:10.560]   as fast as we can.
[00:14:10.560 --> 00:14:12.280]   So in reality, the artists didn't actually
[00:14:12.280 --> 00:14:14.960]   say that looks like crap.
[00:14:14.960 --> 00:14:18.400]   They said these are good and promising, but--
[00:14:18.400 --> 00:14:20.320]   and this is kind of the first thing
[00:14:20.320 --> 00:14:22.520]   that we as researchers encountered
[00:14:22.520 --> 00:14:25.520]   is that researchers and the VFX artists
[00:14:25.520 --> 00:14:28.720]   really don't have a common language.
[00:14:28.720 --> 00:14:30.960]   We understood the requirements from a research side.
[00:14:30.960 --> 00:14:33.000]   They understand it from a VFX side.
[00:14:33.000 --> 00:14:35.480]   And by integrating us into this feedback loop,
[00:14:35.480 --> 00:14:37.360]   we were starting to actually understand
[00:14:37.360 --> 00:14:40.560]   what it meant to produce content for Hollywood.
[00:14:40.560 --> 00:14:43.320]   And in general, we knew very little about this.
[00:14:43.320 --> 00:14:45.160]   But artists knew them all too well.
[00:14:45.160 --> 00:14:47.200]   They knew about the struggle of working with client
[00:14:47.200 --> 00:14:50.480]   requirements, producing content at the level of fidelity
[00:14:50.480 --> 00:14:52.760]   that clients require.
[00:14:52.760 --> 00:14:55.600]   And in these early days of research,
[00:14:55.600 --> 00:14:57.600]   our feedback loop was really focused
[00:14:57.600 --> 00:15:01.720]   on just identifying the core pillars of any minimum viable
[00:15:01.720 --> 00:15:04.000]   product for us to build.
[00:15:04.000 --> 00:15:05.480]   That introduces the three pillars.
[00:15:05.480 --> 00:15:08.320]   The first is any method that we build
[00:15:08.320 --> 00:15:11.640]   has to support any resolution.
[00:15:11.640 --> 00:15:15.000]   So this really just means that whatever the input resolution
[00:15:15.000 --> 00:15:19.280]   is, whether it be 4K or 16K, which is surprisingly
[00:15:19.280 --> 00:15:21.640]   and frustratingly common now, our method
[00:15:21.640 --> 00:15:26.560]   needs to be able to support that high resolution.
[00:15:26.560 --> 00:15:28.080]   And you may notice that there really
[00:15:28.080 --> 00:15:30.800]   aren't a whole lot of models, especially off the shelf, that
[00:15:30.800 --> 00:15:32.760]   support this high resolution.
[00:15:32.760 --> 00:15:38.400]   So for example, models like StyleGAN, trained on FFHQ,
[00:15:38.400 --> 00:15:41.560]   are pretty much limited to the 1024 by 1024.
[00:15:41.560 --> 00:15:44.040]   When you think of 16K resolution,
[00:15:44.040 --> 00:15:46.320]   you're entering a whole other piece of problem.
[00:15:46.320 --> 00:15:49.880]   And the amount of data in that space is pretty limited.
[00:15:49.880 --> 00:15:52.040]   So one lucky problem that we had,
[00:15:52.040 --> 00:15:55.120]   or one lucky thing that we had, was that we did actually
[00:15:55.120 --> 00:15:56.960]   have all of the data.
[00:15:56.960 --> 00:16:01.160]   So we have this historical backlog of 30 shows
[00:16:01.160 --> 00:16:04.080]   that we had worked on and were building more as we went.
[00:16:04.080 --> 00:16:05.880]   And for every single one of those shows,
[00:16:05.880 --> 00:16:07.760]   we had all of the content within them.
[00:16:07.760 --> 00:16:09.360]   So all the faces, all the characters,
[00:16:09.360 --> 00:16:10.960]   and the before and after edits.
[00:16:10.960 --> 00:16:13.640]   So luckily, we were able to identify this quickly
[00:16:13.640 --> 00:16:17.000]   as a requirement and start to curate our data
[00:16:17.000 --> 00:16:20.440]   to be able to build a product that met that requirement.
[00:16:20.440 --> 00:16:22.080]   The next pillar is localized edits.
[00:16:22.080 --> 00:16:26.560]   So this is a very frustrating requirement for VFX.
[00:16:26.560 --> 00:16:28.120]   And in general, we can think of it
[00:16:28.120 --> 00:16:31.080]   as if you are tasked with editing
[00:16:31.080 --> 00:16:34.280]   the eye bags of a character, then
[00:16:34.280 --> 00:16:38.400]   any change outside of what we expect eye bags to be
[00:16:38.400 --> 00:16:41.640]   is an immediate reject by the client or the supervisor.
[00:16:41.640 --> 00:16:44.400]   So the idea is that edits really have
[00:16:44.400 --> 00:16:46.840]   to be localized to the region of interest.
[00:16:46.840 --> 00:16:49.200]   And when you think of all the results coming out
[00:16:49.200 --> 00:16:53.000]   with DALI and stable diffusion, they
[00:16:53.000 --> 00:16:55.480]   are a lot more stable than they used to be temporarily.
[00:16:55.480 --> 00:16:58.360]   But when you think of where those edits are actually
[00:16:58.360 --> 00:17:01.960]   happening, they're often all across the image.
[00:17:01.960 --> 00:17:04.600]   And in a context of Hollywood, they
[00:17:04.600 --> 00:17:07.440]   need those edits to be extremely localized just
[00:17:07.440 --> 00:17:10.080]   to the region of interest.
[00:17:10.080 --> 00:17:13.200]   And this is just due to all the additional layering and effects
[00:17:13.200 --> 00:17:16.840]   that will go on top of just the beauty or DH fix.
[00:17:16.840 --> 00:17:19.960]   So it is quite a hard requirement.
[00:17:19.960 --> 00:17:21.760]   And historically, artists solved this
[00:17:21.760 --> 00:17:24.640]   with very tight rotoscoping and manual tracking
[00:17:24.640 --> 00:17:26.880]   from frame to frame.
[00:17:26.880 --> 00:17:30.120]   And we really extended this by allowing the projection
[00:17:30.120 --> 00:17:32.880]   of masks on one frame to all others.
[00:17:32.880 --> 00:17:36.280]   So we targeted this problem by basically speeding up
[00:17:36.280 --> 00:17:40.280]   the flow of localizing edits to a specific region.
[00:17:40.280 --> 00:17:42.840]   Then additionally, just by applying thresholding
[00:17:42.840 --> 00:17:47.240]   and improving the model's capacity to localize edits,
[00:17:47.240 --> 00:17:49.280]   we also improved on this a lot.
[00:17:49.280 --> 00:17:50.760]   So then the final pillar, which I
[00:17:50.760 --> 00:17:53.880]   think is a really important one and one that's often overlooked,
[00:17:53.880 --> 00:17:56.440]   is really just this idea of artistic control.
[00:17:56.440 --> 00:17:59.160]   So we didn't want to build a product that you entered
[00:17:59.160 --> 00:18:02.640]   remove eye bags and you got an image out, because in reality,
[00:18:02.640 --> 00:18:04.680]   it's not as cut and dry as that.
[00:18:04.680 --> 00:18:07.920]   So generally, even simple edits, like removing a pimple,
[00:18:07.920 --> 00:18:09.760]   have creative direction.
[00:18:09.760 --> 00:18:11.840]   So if you have a character and they
[00:18:11.840 --> 00:18:15.080]   have 50 pimples on their face and they want them removed,
[00:18:15.080 --> 00:18:17.400]   they might not actually want all of them removed.
[00:18:17.400 --> 00:18:19.920]   They might just want some of them toned down.
[00:18:19.920 --> 00:18:22.920]   Or they might want to be maintaining consistency
[00:18:22.920 --> 00:18:27.080]   from a shot in which they had better skin care that day.
[00:18:27.080 --> 00:18:29.160]   So it's an interesting problem of really,
[00:18:29.160 --> 00:18:33.200]   there is no ability to just say, do this and give me a result.
[00:18:33.200 --> 00:18:36.520]   The artist does need creative control.
[00:18:36.520 --> 00:18:38.680]   And in general, our focus was just,
[00:18:38.680 --> 00:18:41.440]   we don't want to take away the ability for artists
[00:18:41.440 --> 00:18:42.380]   to have control.
[00:18:42.380 --> 00:18:46.360]   We just want to allow them to get to what they want faster.
[00:18:46.360 --> 00:18:48.720]   So in general, one-shot models that produce something
[00:18:48.720 --> 00:18:51.280]   that an artist has to take are very rarely
[00:18:51.280 --> 00:18:53.120]   useful in a creative setting.
[00:18:53.120 --> 00:18:56.120]   So now with all these pillars in mind,
[00:18:56.120 --> 00:18:58.400]   we were able to design an architecture that
[00:18:58.400 --> 00:19:02.240]   supported any resolution, enforced edit localization
[00:19:02.240 --> 00:19:05.160]   through the mask propagation, and improved model
[00:19:05.160 --> 00:19:06.200]   architecture.
[00:19:06.200 --> 00:19:08.680]   And then finally, we didn't wrestle control
[00:19:08.680 --> 00:19:09.400]   from the artist.
[00:19:09.400 --> 00:19:12.760]   So we really allowed them to work with vanity AI,
[00:19:12.760 --> 00:19:15.240]   as opposed to be told exactly what to do
[00:19:15.240 --> 00:19:16.640]   and lose their freedom.
[00:19:16.640 --> 00:19:18.640]   So now, once again, we were happy.
[00:19:18.640 --> 00:19:20.520]   And we were sure that these results
[00:19:20.520 --> 00:19:22.160]   were ready for the big screen.
[00:19:22.160 --> 00:19:24.360]   Unfortunately, once again, our feedback
[00:19:24.360 --> 00:19:28.320]   was that there was quite a few issues once again.
[00:19:28.320 --> 00:19:31.480]   So this was our second hurdle of really
[00:19:31.480 --> 00:19:35.600]   trying to understand the minutiae of what
[00:19:35.600 --> 00:19:37.400]   VFX artists look for.
[00:19:37.400 --> 00:19:41.400]   So these artists are so good at looking at a shot
[00:19:41.400 --> 00:19:45.600]   and pinpointing errors at the pixel level.
[00:19:45.600 --> 00:19:49.280]   So in our first real review back with artists
[00:19:49.280 --> 00:19:52.080]   after building on those core pillars,
[00:19:52.080 --> 00:19:54.560]   is we got all these comments like softness on frame,
[00:19:54.560 --> 00:19:56.720]   1080, color shift in the eye bags,
[00:19:56.720 --> 00:19:58.240]   texture popping throughout.
[00:19:58.240 --> 00:20:01.960]   And for anyone in VFX, these are probably very familiar terms.
[00:20:01.960 --> 00:20:05.080]   But for someone in research, and especially for myself
[00:20:05.080 --> 00:20:08.000]   coming from a CAD background, these terms
[00:20:08.000 --> 00:20:09.360]   were completely foreign.
[00:20:09.360 --> 00:20:11.160]   So really what we then did was focus
[00:20:11.160 --> 00:20:16.160]   on understanding what is the mapping of these terms in VFX
[00:20:16.160 --> 00:20:19.800]   to common ML terms that we are familiar with.
[00:20:19.800 --> 00:20:23.400]   So we were able to then focus on softness is color matching,
[00:20:23.400 --> 00:20:26.000]   texture popping is temporal consistency,
[00:20:26.000 --> 00:20:28.640]   ghosting artifacts is edit localization,
[00:20:28.640 --> 00:20:31.200]   and really try to pinpoint and define
[00:20:31.200 --> 00:20:34.280]   what are the qualitative metrics and measures that
[00:20:34.280 --> 00:20:36.200]   map to these specific issues.
[00:20:36.200 --> 00:20:39.080]   And this was also a really good time for us
[00:20:39.080 --> 00:20:40.960]   because it's when we actually were introduced
[00:20:40.960 --> 00:20:43.440]   to weights and biases.
[00:20:43.440 --> 00:20:45.880]   So what this allowed us to do is once
[00:20:45.880 --> 00:20:49.480]   we had these metrics and we were able to define them well,
[00:20:49.480 --> 00:20:52.120]   then we were able to run these experiments
[00:20:52.120 --> 00:20:56.480]   and evaluate their performance against huge sets of shots.
[00:20:56.480 --> 00:20:59.560]   So we were actually able to, for all of those 30 shows
[00:20:59.560 --> 00:21:02.000]   and each one having 200 shots, we
[00:21:02.000 --> 00:21:05.280]   were able to actually run experiments against all of them,
[00:21:05.280 --> 00:21:10.240]   track all of our metrics, and really focus the artist's time
[00:21:10.240 --> 00:21:13.560]   on which shots to be reviewing and giving us feedback on.
[00:21:13.560 --> 00:21:16.320]   And then additionally, we were able to allow researchers
[00:21:16.320 --> 00:21:20.200]   to use the metrics to target shots for follow-up
[00:21:20.200 --> 00:21:21.040]   experiments.
[00:21:21.040 --> 00:21:22.680]   So we were able to identify these
[00:21:22.680 --> 00:21:25.120]   are the shots that have temporal inconsistencies,
[00:21:25.120 --> 00:21:27.440]   these are the shots that have softness issues,
[00:21:27.440 --> 00:21:29.560]   these are the shots that are under editing,
[00:21:29.560 --> 00:21:32.880]   and really start to come up with a principled approach of how
[00:21:32.880 --> 00:21:35.520]   we should actually be solving these problems.
[00:21:35.520 --> 00:21:38.400]   And I always love these visuals, but in general, we
[00:21:38.400 --> 00:21:41.760]   ran countless experiments comparing the per frame
[00:21:41.760 --> 00:21:44.920]   performance across all shots in our library
[00:21:44.920 --> 00:21:47.320]   to further refine our understanding of what
[00:21:47.320 --> 00:21:48.920]   a bad shot was.
[00:21:48.920 --> 00:21:53.360]   So this visual essentially just shows four different shots
[00:21:53.360 --> 00:21:56.040]   and then the per frame metrics that we measured.
[00:21:56.040 --> 00:21:58.520]   So in this case, I'm just showing one of the metrics,
[00:21:58.520 --> 00:22:01.760]   which is edit pixel error, which is a pretty simple one of what
[00:22:01.760 --> 00:22:04.040]   is the error of each pixel across the region
[00:22:04.040 --> 00:22:05.760]   that we're interested in.
[00:22:05.760 --> 00:22:07.180]   And what we can actually see is you
[00:22:07.180 --> 00:22:10.840]   can see that temporally as the shot goes on,
[00:22:10.840 --> 00:22:13.040]   we have varying levels of error.
[00:22:13.040 --> 00:22:18.160]   So when we would then give these shots as review for artists,
[00:22:18.160 --> 00:22:21.120]   we were actually able to pinpoint specific areas
[00:22:21.120 --> 00:22:25.000]   that we thought in the metrics correlated to their comments.
[00:22:25.000 --> 00:22:28.880]   So the artist would say things like, oh, on frame 1090,
[00:22:28.880 --> 00:22:30.840]   the eye bag edit disappears.
[00:22:30.840 --> 00:22:33.440]   We would be able to look at the per frame metrics
[00:22:33.440 --> 00:22:36.160]   that Weights and Bias produced and actually correlate them
[00:22:36.160 --> 00:22:40.800]   to further refine our ability to actually detect a bad shot.
[00:22:40.800 --> 00:22:43.520]   And these kind of practices and analysis
[00:22:43.520 --> 00:22:48.360]   are really what allowed us to target the problems that we
[00:22:48.360 --> 00:22:52.040]   really needed to solve and build a product that did actually
[00:22:52.040 --> 00:22:54.320]   solve the problems that artists were facing.
[00:22:54.320 --> 00:22:57.560]   So all this said and done, we were really
[00:22:57.560 --> 00:23:02.320]   able to build this feedback loop in which both the researchers
[00:23:02.320 --> 00:23:04.720]   and the artists were finally happy with the results
[00:23:04.720 --> 00:23:06.280]   that we were producing.
[00:23:06.280 --> 00:23:11.320]   And the takeaway here is that this idea of fast feedback
[00:23:11.320 --> 00:23:12.680]   was really important to us.
[00:23:12.680 --> 00:23:17.400]   And not only being able to get reviews by artists
[00:23:17.400 --> 00:23:19.720]   and integrate them into our feedback loop,
[00:23:19.720 --> 00:23:24.640]   but also using Weights and Bias to automatically tell us
[00:23:24.640 --> 00:23:28.080]   which shots should be reviewed by artists and flag shots
[00:23:28.080 --> 00:23:30.480]   that we should be focusing on as researchers.
[00:23:30.480 --> 00:23:33.400]   So I'm happy to say that we have some pretty good key results
[00:23:33.400 --> 00:23:34.280]   now.
[00:23:34.280 --> 00:23:36.760]   Behind here are just some of the shows
[00:23:36.760 --> 00:23:38.400]   that we have actually worked on.
[00:23:38.400 --> 00:23:40.720]   I won't mention who in each of them,
[00:23:40.720 --> 00:23:43.800]   but various beauty fixes have been applied across these.
[00:23:43.800 --> 00:23:47.320]   In total, we have worked on 26 projects to date,
[00:23:47.320 --> 00:23:51.200]   which for clients, this is approximately 50% cost savings
[00:23:51.200 --> 00:23:52.200]   overall.
[00:23:52.200 --> 00:23:54.480]   And in general, from our benchmarks,
[00:23:54.480 --> 00:23:57.680]   the process of using Vanity AI over traditional workflows
[00:23:57.680 --> 00:24:00.280]   is about 320 times faster.
[00:24:00.280 --> 00:24:03.240]   And in total, we've saved about $8 million to date.
[00:24:03.240 --> 00:24:07.280]   And although beauty may not seem like the coolest project
[00:24:07.280 --> 00:24:10.880]   to be solving, it really allows these clients
[00:24:10.880 --> 00:24:14.960]   to focus on other effects and divest their money and time
[00:24:14.960 --> 00:24:17.360]   from these less desirable things that are really just
[00:24:17.360 --> 00:24:21.320]   for consistency or beauty fixes and focus more
[00:24:21.320 --> 00:24:23.760]   on the cool effects that we all want to see, like dragons
[00:24:23.760 --> 00:24:25.280]   and spaceships and things like that.
[00:24:25.280 --> 00:24:28.640]   [MUSIC PLAYING]
[00:24:28.640 --> 00:24:31.220]   (upbeat music)

