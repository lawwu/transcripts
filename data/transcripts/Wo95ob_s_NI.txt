
[00:00:00.000 --> 00:00:04.000]   I think even in one or two years, you could imagine having the models carry out a whole
[00:00:04.000 --> 00:00:08.400]   coding project, moving away from using the model of like a search engine and more towards
[00:00:08.400 --> 00:00:12.240]   having a whole project that I'm like doing in collaboration with the model.
[00:00:12.240 --> 00:00:16.240]   We might not want to jump to having AIs run whole firms immediately,
[00:00:16.240 --> 00:00:20.480]   even if the models are good enough to actually run a successful business themselves.
[00:00:20.480 --> 00:00:24.320]   If there's no other bottlenecks next year or something, you got AGI, what's the plan?
[00:00:24.320 --> 00:00:30.160]   Today, I have the pleasure to speak with John Schulman, who is one of the co-founders of OpenAI
[00:00:30.160 --> 00:00:37.360]   and leads the post-training team here. He also led the creation of ChatGBT and is the author
[00:00:37.360 --> 00:00:43.120]   of many of the most important and widely cited papers in AI and RL, including PPO and many others.
[00:00:43.120 --> 00:00:45.600]   John, really excited to chat with you. Thanks for coming on the podcast.
[00:00:45.600 --> 00:00:47.520]   Thanks for having me on the podcast. I'm a big fan.
[00:00:47.520 --> 00:00:48.720]   Oh, thank you. Thank you for saying that.
[00:00:50.560 --> 00:00:55.200]   The first question I had is we have these distinctions between pre-training and post-training
[00:00:55.200 --> 00:01:00.400]   beyond what is actually happening in terms of loss function and training regimes. I'm just curious,
[00:01:00.400 --> 00:01:05.520]   taking a step back conceptually, what kind of thing is pre-training creating?
[00:01:05.520 --> 00:01:07.520]   What does post-training do on top of that?
[00:01:07.520 --> 00:01:14.720]   In pre-training, you're basically training to imitate all of the content on the internet or
[00:01:14.720 --> 00:01:21.600]   on the web, including websites and code and so forth. You get a model that can basically generate
[00:01:21.600 --> 00:01:29.280]   content that looks like random web pages from the internet. The model is also trained to maximize
[00:01:29.280 --> 00:01:37.040]   likelihood where it has to put a probability on everything. The objective is basically predicting
[00:01:37.040 --> 00:01:41.760]   the next token given the previous tokens. Tokens are like words or parts of words.
[00:01:43.280 --> 00:01:49.440]   Since the model has to put a probability on it and we're training to maximize log probability,
[00:01:49.440 --> 00:01:55.200]   it ends up being very calibrated. It can not only generate all of the content of the web,
[00:01:55.200 --> 00:02:02.320]   it can also assign probabilities to everything. The base model can effectively take on all these
[00:02:02.320 --> 00:02:05.840]   different personas or generate all these different kinds of content.
[00:02:07.040 --> 00:02:15.760]   When we do post-training, we're usually targeting a narrower range of behavior, where we basically
[00:02:15.760 --> 00:02:23.840]   want the model to behave like this kind of chat assistant. It's a more specific persona where it's
[00:02:23.840 --> 00:02:30.080]   trying to be helpful. It's not trying to imitate a person. It's answering your questions or doing
[00:02:30.080 --> 00:02:37.680]   your tasks. We're optimizing on a different objective, which is more about producing
[00:02:37.680 --> 00:02:41.840]   outputs that humans will like and find useful, as opposed to just trying to imitate
[00:02:41.840 --> 00:02:48.240]   this raw content from the web. I think maybe I should take a step back and ask.
[00:02:48.240 --> 00:02:53.200]   Right now, we have these models that are pretty good at acting as chatbots. Just taking a step
[00:02:53.200 --> 00:02:57.120]   back from how these processes work currently, what would the models release by the end of
[00:02:57.120 --> 00:03:01.040]   the year be capable of doing? What do you see the progress looking like?
[00:03:01.040 --> 00:03:07.280]   Carry this forward for the next five years. Oh, yeah. Five years. Yeah. I think the models
[00:03:07.280 --> 00:03:16.480]   will get quite a bit better in the course of five years. I think even in one or two years,
[00:03:16.480 --> 00:03:25.360]   we'll find that you can use them for a lot of more involved tasks than they can do now.
[00:03:25.920 --> 00:03:34.960]   For example, right now, you could imagine having the models carry out a whole coding project
[00:03:34.960 --> 00:03:41.440]   instead of maybe giving you one suggestion on how to write a function. You could imagine the model
[00:03:41.440 --> 00:03:51.600]   giving it high-level instructions on what to code up. It'll go and write many files and test it,
[00:03:51.600 --> 00:03:55.600]   look at the output, iterate on that a bit. So just much more complex tasks.
[00:03:55.600 --> 00:04:00.640]   Fundamentally, the unlock is that it can act coherently for long enough to write
[00:04:00.640 --> 00:04:03.600]   multiple files of code. What has changed between now and then?
[00:04:03.600 --> 00:04:10.880]   Yeah. I would say this will come from some combination of just training the models to
[00:04:10.880 --> 00:04:26.160]   do harder tasks like this. Most of the training data is more like doing single steps at a time,
[00:04:26.160 --> 00:04:33.120]   and I would expect us to do more for training the models to carry out these longer projects.
[00:04:34.000 --> 00:04:40.640]   So I'd say any kind of training, like doing RL to learn how to do these tasks,
[00:04:40.640 --> 00:04:47.760]   however you do it, whether you're supervising the final output or supervising it each step,
[00:04:47.760 --> 00:04:53.200]   I think any kind of training at carrying out these long projects is going to make them a lot better.
[00:04:53.200 --> 00:05:00.960]   And since the whole area is pretty new, I'd say there's just a lot of low-hanging fruit
[00:05:03.120 --> 00:05:07.280]   in doing this kind of training. So I'd say that's one thing.
[00:05:07.280 --> 00:05:12.000]   Also, I would expect that as the models get better, they're just better at recovering
[00:05:12.000 --> 00:05:21.120]   from errors, or they're better at dealing with edge cases, or when things go wrong,
[00:05:21.120 --> 00:05:26.800]   they know how to recover from it. So the models will be more sample efficient, so you don't have
[00:05:26.800 --> 00:05:32.560]   to collect a ton of data to teach them how to get back on track. Just a little bit of data or
[00:05:32.560 --> 00:05:41.680]   just their generalization from other abilities will allow them to get back on track, whereas
[00:05:41.680 --> 00:05:46.000]   current models might just get stuck and get lost. I'm not sure I understand, actually,
[00:05:46.000 --> 00:05:54.000]   how the generalization helps you get back on track. Can you say more about that? I'm not
[00:05:54.000 --> 00:05:59.120]   sure I got why those two concepts are connected. Right, they're not directly connected. So I would
[00:05:59.120 --> 00:06:07.120]   say you usually have a little bit of data that does everything. So I mean, if you collect a
[00:06:07.120 --> 00:06:13.040]   diverse data set, you're going to get a little bit of everything in it. And if you have models
[00:06:13.040 --> 00:06:18.800]   that generalize really well, even if there's just a couple examples of getting back on track,
[00:06:20.400 --> 00:06:25.120]   like maybe in the pre-training, there's examples of getting back on track, then the model will be
[00:06:25.120 --> 00:06:30.960]   able to generalize from those other things it's seen to the current situation. So I think
[00:06:30.960 --> 00:06:38.080]   if you have models that are weaker, you might be able to get them to do almost anything with
[00:06:38.080 --> 00:06:44.640]   enough data, but you might have to put a lot of effort into a particular domain or skill. Whereas
[00:06:44.640 --> 00:06:49.280]   for a stronger model, it might just do the right thing without any training data or any effort.
[00:06:49.920 --> 00:06:55.680]   Do you have some intuition about right now, these models can maybe act coherently for five minutes.
[00:06:55.680 --> 00:06:59.680]   We want them to be able to do tasks that for a human would take an hour, then a week, then a
[00:06:59.680 --> 00:07:07.040]   month and so forth. To get from each of these benchmarks, is it going to be each one takes 10x
[00:07:07.040 --> 00:07:12.400]   more compute analogous to the current scaling loss for pre-training or is it going to be a much more
[00:07:14.160 --> 00:07:20.000]   streamlined process because just getting to that point where you're already more sample
[00:07:20.000 --> 00:07:24.320]   efficient and then you just go to the years of carrying out tasks or something?
[00:07:24.320 --> 00:07:30.720]   Yeah, I would say at a high level, I would agree that longer horizon tasks are going to
[00:07:30.720 --> 00:07:37.040]   require more model intelligence to do well and are going to be more expensive to train for.
[00:07:37.040 --> 00:07:42.000]   I'm not sure I would expect there to be a really clean scaling law unless you
[00:07:43.760 --> 00:07:50.720]   set it up in a very careful way or design the experiment in a certain way because
[00:07:50.720 --> 00:07:55.840]   I would say there might end up being some phase transitions where
[00:07:55.840 --> 00:08:06.640]   once you get to a certain level, you can deal with much longer tasks. So for example,
[00:08:10.080 --> 00:08:17.840]   I think when people do planning at different time scales, I'm not sure they use completely
[00:08:17.840 --> 00:08:24.880]   different mechanisms. So we probably use the same mental machinery if we're thinking about
[00:08:24.880 --> 00:08:33.920]   one month from now, one year from now, or like 100 years from now. So we're not actually doing
[00:08:33.920 --> 00:08:39.440]   some kind of reinforcement learning where we need to worry about a discount factor that
[00:08:39.440 --> 00:08:44.880]   covers that timescale and so forth. So I think using language, you can describe all of these
[00:08:44.880 --> 00:08:51.120]   different timescales and then you can do things like plan in the moment. You can try to make
[00:08:51.120 --> 00:08:56.560]   progress towards your goal, whether it's a month away or 10 years away. So I might expect the same
[00:08:56.560 --> 00:09:05.440]   out of models where there's some kind of, I don't know if it's a phase transition, but there's some
[00:09:05.440 --> 00:09:11.200]   capabilities that work at multiple scales. Yeah. Well, okay. So correct me if this is wrong, but
[00:09:11.200 --> 00:09:16.080]   it seems like that implies right now we have models that are on a per token basis, pretty
[00:09:16.080 --> 00:09:20.480]   smart. Like they might be as smart as humans on a per token basis, the smartest humans.
[00:09:20.480 --> 00:09:26.320]   And the thing that prevents them from being as useful as they could be is that five minutes from
[00:09:26.320 --> 00:09:31.280]   now, they're not going to be. So writing your code in a way that's coherent and aligns with
[00:09:31.280 --> 00:09:36.960]   the broader goals, you have your project or something. If it's the case that once you start
[00:09:36.960 --> 00:09:42.480]   this long horizon, RL training regime, it immediately unlocks your ability to be coherent
[00:09:42.480 --> 00:09:48.080]   for longer periods of time. Should we be predicting something that is human level as soon as that
[00:09:48.080 --> 00:09:52.320]   regime is unlocked or, and if not, then what is remaining after you can plan for a year and
[00:09:52.320 --> 00:09:58.160]   execute projects that take that long? Yeah. It's not totally clear what we're going to see once we
[00:09:58.160 --> 00:10:06.080]   get into that regime and, um, how fast progress will be. So that's, uh, that's still uncertain.
[00:10:06.080 --> 00:10:12.400]   Um, I would say I would expect there to be, um, um, I, I would, I wouldn't expect everything to
[00:10:12.400 --> 00:10:17.920]   be immediately solved by doing any training like this. I would think, uh, there'll be other, um,
[00:10:17.920 --> 00:10:22.400]   like miscellaneous deficits that the models have that, um, cause them to get stuck or not make
[00:10:22.400 --> 00:10:29.920]   progress or make, um, worse decisions than humans. So, uh, I wouldn't say I expect that this one
[00:10:29.920 --> 00:10:36.560]   little thing will unlock every all capabilities, but, um, yeah, it's not clear, uh, but it might,
[00:10:36.560 --> 00:10:40.240]   uh, like some improvement in the ability to do long horizon tasks might go quite far.
[00:10:40.240 --> 00:10:45.200]   Would you say it's plausible or is, it seems quite likely that there will be other
[00:10:45.200 --> 00:10:49.040]   reasons why there might be bottlenecks. I, and I'm also kind of curious, like what would be the
[00:10:49.040 --> 00:10:53.040]   nature of the bottlenecks? So it has all these representations for pre-training. Now we can do
[00:10:53.040 --> 00:10:57.520]   at coherently for a long period of time because of long horizon RL what's remaining.
[00:10:57.520 --> 00:11:06.800]   Yeah. Um, maybe there's some, uh, there's some other, um, experience that human experts bring
[00:11:06.800 --> 00:11:12.320]   to different tasks, like, um, having some, uh, taste, uh, or dealing with ambiguity better.
[00:11:12.320 --> 00:11:18.640]   Um, so I could imagine that if we want to do something like research, uh, like those,
[00:11:18.640 --> 00:11:25.680]   those kinds of considerations come into play. Um, yeah, obviously there's, uh,
[00:11:25.680 --> 00:11:32.640]   um, they're going to be just, uh, sort of mundane limitations around, uh, like affordances of the
[00:11:32.640 --> 00:11:39.040]   model, like whether it can, um, whether it can use UIs and obviously the physical world, um,
[00:11:39.040 --> 00:11:45.920]   or having access to things. So I think there might be a lot of, um, uh, like mundane, uh,
[00:11:45.920 --> 00:11:50.320]   barriers that are probably not going to last that long, but would initially, um,
[00:11:50.320 --> 00:11:56.080]   like slow down, um, progress. The websites that are designed for these AIs, once they're
[00:11:56.080 --> 00:12:01.120]   much more multimodal, um, or at least train on more multimodal data, will they be in any way
[00:12:01.120 --> 00:12:05.840]   different from the ones we have for humans? Like the UIs that will be needed, what, what, uh,
[00:12:05.840 --> 00:12:09.120]   how compensating for their strengths and weaknesses, how would that look different
[00:12:09.120 --> 00:12:14.400]   from the current, uh, you know, UIs we have for humans? Yeah, that's, that's an interesting
[00:12:14.400 --> 00:12:19.360]   question. I mean, um, I would expect that models will be able to use, uh, websites that are
[00:12:19.360 --> 00:12:24.240]   designed for humans, uh, just by using vision, uh, like when the vision capabilities get a bit
[00:12:24.240 --> 00:12:29.600]   better. Um, so there wouldn't be an immediate need to change them. Um, on the other hand,
[00:12:29.600 --> 00:12:34.960]   some websites, uh, that are going to benefit a lot from AI, uh, AIs being able to use them
[00:12:34.960 --> 00:12:43.120]   will probably want to, uh, design to be better UXs for AIs. So, um, I'm not sure exactly what
[00:12:43.120 --> 00:12:49.600]   that would mean, but probably, uh, like assuming that our, um, our models are still better in text
[00:12:49.600 --> 00:12:55.760]   mode, um, than like reading text out of images, uh, you'd probably want to have a good text-based
[00:12:55.760 --> 00:13:03.440]   representation for the models. So, uh, and also, um, just, uh, a good, uh, like indication of
[00:13:03.440 --> 00:13:08.640]   what are all the things that can be interacted with. Um, but I guess I wouldn't expect the web
[00:13:08.640 --> 00:13:13.920]   to get, um, like totally redesigned to have APIs everywhere. Cause I, I would expect that we can
[00:13:13.920 --> 00:13:18.320]   get models to use the same kind of UIs that humans use. Right. I mean, I guess that's been the big
[00:13:18.320 --> 00:13:22.560]   lesson of language models, right? That they can, they can act in the similar affordances that
[00:13:22.560 --> 00:13:29.680]   humans have. Um, so the point you made earlier about this process could be more sample efficient
[00:13:29.680 --> 00:13:34.160]   because it could generalize from its experiences in pre-training of how to get unstuck in different
[00:13:34.160 --> 00:13:41.680]   scenarios. Uh, I'm curious what the strongest evidence of this kind of generalization and
[00:13:41.680 --> 00:13:50.320]   transfer you've seen is, uh, yeah, like, well, cause the big question, it seems about the future
[00:13:50.320 --> 00:13:52.880]   abilities is models is like how, how much generalization there is happening. Is there
[00:13:52.880 --> 00:13:56.640]   something that feels really compelling to you? Like you really learned something that you wouldn't
[00:13:56.640 --> 00:14:02.960]   expect it to learn from the generalization here. There's, uh, definitely been some interesting,
[00:14:02.960 --> 00:14:09.360]   um, uh, instances of generalization in post-training like, um, uh, one well-known phenomenon
[00:14:09.360 --> 00:14:16.160]   is if you do all your fine tuning with English data, uh, you'll automatically, um, you'll have
[00:14:16.160 --> 00:14:22.400]   the model also, um, uh, behave, behaving well in other languages. So if you train the assistant
[00:14:22.400 --> 00:14:28.160]   on English data, uh, it'll also, um, do something reasonable in Spanish say, and, uh, sometimes you
[00:14:28.160 --> 00:14:32.480]   might get, um, you might get the wrong behavior in terms of whether it replies in English or
[00:14:32.480 --> 00:14:37.920]   replies in Spanish, but, uh, usually, usually you get the, you get the right behavior there as well.
[00:14:37.920 --> 00:14:43.280]   Like you get it to respond in Spanish to Spanish queries. So that's one, one, uh, kind of
[00:14:43.280 --> 00:14:48.240]   interesting instance of generalization that you just sort of latch onto the right, uh, helpful
[00:14:48.240 --> 00:14:52.720]   persona, and then you automatically do the right thing in different languages. We've seen some
[00:14:52.720 --> 00:14:59.520]   versions of this with, um, multimodal data where, uh, if you do, um, text only fine tuning, you
[00:14:59.520 --> 00:15:08.800]   also get reasonable behavior with images. Um, uh, early on in, um, chat, GBT, we, uh, we were trying
[00:15:08.800 --> 00:15:15.520]   to fix, um, issues in terms of the model, uh, understanding its own, uh, limitations, like, um,
[00:15:15.520 --> 00:15:21.920]   like early versions of the model would think that, uh, could like send you an email or call you,
[00:15:21.920 --> 00:15:26.400]   call an Uber or something like, uh, the model would try to play the assistant and it would say,
[00:15:26.400 --> 00:15:32.080]   oh yeah, of course I, I sent that email and obviously it didn't. So we, uh, we, we started
[00:15:32.080 --> 00:15:36.000]   collecting a little, some data to fix those problems. And we found that a tiny amount of
[00:15:36.000 --> 00:15:40.720]   data did the trick, even when you mix it together with everything else. So I don't remember exactly
[00:15:40.720 --> 00:15:46.080]   how many examples, but something like 30, 30 examples. Well, we had us, I don't know,
[00:15:46.080 --> 00:15:51.840]   pretty small number of examples showing this general, uh, behavior of, um, like explaining
[00:15:51.840 --> 00:15:56.880]   that the model can't, doesn't have this capability and that generalize pretty well to all sorts of
[00:15:56.880 --> 00:16:01.920]   capabilities we didn't train for. Okay. So I, I still want to go back to this cause I'm not sure
[00:16:01.920 --> 00:16:10.240]   I understood, uh, like if you have, uh, this model that is trained on to be coherent for longer
[00:16:10.240 --> 00:16:14.960]   periods of time, does that imply that unless there are these other bottlenecks, which they may or
[00:16:14.960 --> 00:16:21.280]   may not be by next year, you could have models that are potentially like human level in terms
[00:16:21.280 --> 00:16:26.480]   of acting like, like you're interacting with this as a colleague. And it's like, it's like as good
[00:16:26.480 --> 00:16:29.680]   as interacting with a human colleague, you can tell them to go do stuff and they go to get it done.
[00:16:29.680 --> 00:16:35.520]   Uh, what seems wrong with that picture of this is the capabilities you think might be possible.
[00:16:35.520 --> 00:16:41.200]   Yeah. It's hard to say exactly what will be the deficit. I mean, I would say that, uh, when you
[00:16:41.200 --> 00:16:48.480]   talk to the models today, they have various, um, uh, weaknesses besides, uh, long-term coherence
[00:16:48.480 --> 00:16:54.240]   in terms of also like, um, like really, uh, thinking hard about things or paying attention
[00:16:54.240 --> 00:17:03.120]   to what you ask them. Uh, so, um, I would say, um, I wouldn't expect, um, like just improving the,
[00:17:03.120 --> 00:17:10.640]   uh, coherence a little bit to like, um, to be all it takes to get to AGI, but, um, I guess I
[00:17:10.640 --> 00:17:17.040]   wouldn't be able to articulate exactly what the main weakness is that'll stop them from, uh, like
[00:17:17.760 --> 00:17:21.600]   being a fully functional, uh, colleague. It seems like then you should be planning for the
[00:17:21.600 --> 00:17:26.800]   possibility you would have AGI very soon. Yeah, I think it's, uh, I think that would be reasonable.
[00:17:26.800 --> 00:17:31.280]   Hmm. So what's the plan if like, if there's no other bottlenecks next year or something,
[00:17:31.280 --> 00:17:37.360]   you got AGI, what's the plan? Well, I would say that if AGI came way sooner than expected,
[00:17:37.360 --> 00:17:42.320]   uh, we would definitely want to, we would want to be careful about it. And we would, uh, we might
[00:17:42.320 --> 00:17:48.480]   want to, um, like, uh, slow down a little bit on, uh, training and deployment until we're pretty
[00:17:48.480 --> 00:17:55.840]   sure we know, uh, we, we can deal with it safely. Um, and we, we have a, um, a pretty good handle
[00:17:55.840 --> 00:18:01.360]   on what it's going to do, what it, what it can do. So I think, uh, yeah, we would have to be,
[00:18:01.360 --> 00:18:07.040]   we'd have to be very careful, um, if it happened way sooner than expected, because I think, uh,
[00:18:07.040 --> 00:18:12.000]   our understanding is rudimentary in a lot of ways still. And what would, what would being careful
[00:18:12.000 --> 00:18:16.240]   mean? Like, uh, cause presumably you're already careful, right? You do these evaluations before
[00:18:16.240 --> 00:18:24.560]   you're, um, yeah, I would say, uh, just like, uh, uh, maybe not, um, uh, not training the even
[00:18:24.560 --> 00:18:30.560]   smarter version of not like being really careful when you do train it, that it's not, uh, it's,
[00:18:30.560 --> 00:18:38.480]   um, like properly sandbox and everything, um, maybe not deploying it at scale, um, or yeah,
[00:18:38.480 --> 00:18:43.520]   being, uh, yeah, being careful, careful about what, um, what scale you deploy it.
[00:18:43.520 --> 00:18:50.560]   Yeah, I guess I'm not. Okay. So let's just play with a scenario. Like it happens next year and
[00:18:50.560 --> 00:18:57.200]   then, uh, you're, you're not training a smarter system, but, and then you're, you're deploying
[00:18:57.200 --> 00:19:09.360]   somewhat in a measured way. Um, I, yeah, I'm wondering, well, presumably if this is just,
[00:19:09.360 --> 00:19:13.680]   this isn't particular to OPN and AI, but this is just intelligence. It was just much easier
[00:19:13.680 --> 00:19:18.400]   than we expected. And this is why it happened. Um, and so you wait to deploy a little bit.
[00:19:18.400 --> 00:19:23.520]   Now, other companies have the similar level of capabilities. What, what happens next? So you,
[00:19:23.520 --> 00:19:27.280]   you've waited to deploy, what are you waiting for? What are you talking with these? What,
[00:19:27.280 --> 00:19:32.160]   what does every company doing in this scenario? Yeah. Yeah. The game theory is a little tough to
[00:19:32.160 --> 00:19:36.320]   think through. So, oh yeah. So first of all, I don't think this is going to happen next year,
[00:19:36.320 --> 00:19:40.640]   but it's still useful to have the conversation and maybe it's like two or three years instead.
[00:19:40.640 --> 00:19:47.440]   But, um, yeah, I guess it's still pretty soon. I do think, uh, you probably need some coordination.
[00:19:47.440 --> 00:19:54.880]   Like, uh, everyone needs to agree on some, uh, on some reasonable, uh, like limits to deployment or
[00:19:54.880 --> 00:20:00.160]   to further training, uh, for this to work. Otherwise, uh, otherwise you have the, the
[00:20:00.160 --> 00:20:04.960]   race dynamics where everyone's trying to, everyone's trying to stay ahead and, uh, like
[00:20:04.960 --> 00:20:10.960]   everyone's, uh, like, and that might require compromising on safety. So I think you would
[00:20:10.960 --> 00:20:16.560]   probably need some coordination among the, uh, larger entities that are doing this kind of
[00:20:16.560 --> 00:20:24.320]   training. And so you're coordinating to, um, I guess pause deployment until, until what exactly,
[00:20:24.320 --> 00:20:28.240]   like, until you figure out what's happening in the models or like pause, uh, either, uh, further
[00:20:28.240 --> 00:20:35.440]   training, pause deployment, uh, like, uh, avoid certain types of training that we think might be
[00:20:35.440 --> 00:20:42.640]   riskier. Uh, so just, uh, like setting up some reasonable rules for, uh, um, like, uh, what,
[00:20:43.440 --> 00:20:49.680]   what everyone should do to, uh, yeah. Uh, having everyone somewhat limit, uh, limit these things.
[00:20:49.680 --> 00:20:54.080]   And, but, uh, limit to what end, because I guess at some point, then you're going to have to like
[00:20:54.080 --> 00:20:59.440]   do the potential energy that's within this intelligence will, uh, uh, you know, it'll
[00:20:59.440 --> 00:21:04.480]   be unleashed. So, uh, what, what, what, what, what is the plan to do? Like suppose in two years,
[00:21:04.480 --> 00:21:10.880]   we get the AGI and now everybody's freaking out. And so now the AI companies have paused. Um,
[00:21:10.880 --> 00:21:15.040]   and now what, or is, or what, what, what would be the plan to wait till, or.
[00:21:15.040 --> 00:21:21.840]   Yeah, that's, uh, I don't have a good answer to that. I mean, I would say, um, if we can,
[00:21:21.840 --> 00:21:27.680]   if everyone is going to coordinate like that, uh, I think we would be, that would be an okay
[00:21:27.680 --> 00:21:34.400]   scenario. That would be a pretty good scenario. Cause I do think, uh, like, um, building these
[00:21:34.400 --> 00:21:39.440]   models is very capital intensive and, uh, there are a lot of complex pieces. So it's not like
[00:21:39.440 --> 00:21:45.120]   everyone's going to go and recreate this stuff at home. Uh, so I think it is possible to do
[00:21:45.120 --> 00:21:50.320]   get given the relatively small number of entities who could train the largest models. It does seem
[00:21:50.320 --> 00:21:58.080]   possible to coordinate. So I'm not sure how, uh, how you would maintain this, uh, this equilibrium
[00:21:58.080 --> 00:22:05.200]   for a long period of time. But I think if we got to that point, um, we would be in an okay position.
[00:22:05.200 --> 00:22:10.560]   Or it would be, I guess I'm curious, like, um, uh, I'm not sure what happens next. Cause like
[00:22:10.560 --> 00:22:16.240]   fundamentally the problem or the benefit is that like, we've got a ton of like, you, you like push
[00:22:16.240 --> 00:22:19.280]   it to the server and now we've got a bunch of intelligences or they could push themselves to
[00:22:19.280 --> 00:22:25.280]   the server. Um, and I'm now we've got everybody coordinated, but I'm not sure what, what we do
[00:22:25.280 --> 00:22:28.960]   next in this, in this world. We're like, well, why that, why that sets us up for a good outcome?
[00:22:29.760 --> 00:22:34.880]   Yeah, I would say if we had everyone, um, reasonably coordinated, we could, uh, figure
[00:22:34.880 --> 00:22:40.160]   out some, and we felt like we had solved the technical problems around alignment well enough
[00:22:40.160 --> 00:22:48.720]   to be able to, uh, deploy like really smart AIs that, um, can like, uh, like act as an extension
[00:22:48.720 --> 00:22:54.720]   of people's will, but also, uh, prevent, uh, them from being misused in some way that would cause a
[00:22:54.720 --> 00:23:00.640]   catastrophe catastrophe. I think then, uh, then that would be great. Like we could, uh, go ahead
[00:23:00.640 --> 00:23:05.920]   and, uh, like safely deploy these systems and, uh, it would, um, it would usher in a lot of,
[00:23:05.920 --> 00:23:14.480]   uh, prosperity and a new, uh, like much, uh, more rapid phase of scientific advancement and so forth.
[00:23:14.480 --> 00:23:16.880]   So I think that would be what the good scenario would look like.
[00:23:16.880 --> 00:23:21.280]   Okay. So that's, that makes sense. But I'm curious, like, how would you know in a couple of
[00:23:21.280 --> 00:23:27.520]   years, if, uh, you, you, like all these, uh, actors, even in the best case scenario, they've
[00:23:27.520 --> 00:23:35.040]   agreed to pause until we've figured out that we're building aligned systems that, uh, uh, are not
[00:23:35.040 --> 00:23:39.440]   themselves going to attempt to take over a coup or not going to enable somebody else to do that.
[00:23:39.440 --> 00:23:42.800]   How, what would proof of that look like, or what would evidence of that look like?
[00:23:42.800 --> 00:23:50.800]   Well, I would say if we, um, if we can deploy, uh, like, uh, systems incrementally that are
[00:23:50.800 --> 00:23:55.840]   successively smarter than the ones before, then I think that's a safer. So I hope the way things
[00:23:55.840 --> 00:24:00.800]   play out is, is it's not this scenario where everyone has to coordinate and lock things down
[00:24:00.800 --> 00:24:06.720]   and safely release things, uh, like, cause it would like lead to this big buildup in potential
[00:24:06.720 --> 00:24:13.040]   energy potentially. So I would rather, uh, some scenario where we're just, um, continually
[00:24:13.040 --> 00:24:18.400]   releasing things that are a little better than what came before. And then we, uh, while like
[00:24:18.400 --> 00:24:26.320]   making sure we're, um, confident that each, um, diff is right, like improving, uh, improving the
[00:24:26.320 --> 00:24:32.160]   safety and alignment, uh, in like, uh, correspondence to the improvement in capability.
[00:24:32.160 --> 00:24:37.600]   So, and if, if things started to look a little bit scary, then we would be able to, uh, slow
[00:24:37.600 --> 00:24:44.720]   things down. So that's what I would hope for. Um, I would say, um, if there's more of a discontinuous
[00:24:44.720 --> 00:24:49.520]   jump and the question is, how do you know if the thing you've got is safe to release? Um,
[00:24:49.520 --> 00:24:59.200]   I would say I can't give a, a generic answer. Like I would want to, but, um, like the type
[00:24:59.200 --> 00:25:04.800]   of thing you might want to do to make that more, uh, more acceptable would be, you would want to do,
[00:25:04.800 --> 00:25:12.880]   um, a lot of, uh, testing, like simulated deployment, um, uh, you, where that you expect.
[00:25:12.880 --> 00:25:18.320]   So red teaming of sorts, like you'd want to do that in a way that you feel is like, uh, much
[00:25:18.320 --> 00:25:24.960]   less favorable than, uh, or much, uh, more likely to fail than the thing you're planning to do in
[00:25:24.960 --> 00:25:30.080]   the real world. Uh, you'd want to have a really good monitoring system so that you can, uh, like
[00:25:30.080 --> 00:25:36.960]   if something does start to go, go wrong with the deployed system, you can, uh, you feel like it's
[00:25:36.960 --> 00:25:41.040]   going to be, uh, detectable immediately. Like you've got, maybe you've got something watching
[00:25:41.040 --> 00:25:45.680]   over, uh, the deployed AI's and what they're doing and looking for signs of trouble.
[00:25:45.680 --> 00:25:52.320]   So, uh, uh, so I would want to, um, yeah, I would say just, um, you'd want some defense
[00:25:52.320 --> 00:25:58.480]   in depth. Like you'd want to have some combination of, uh, like the model itself, uh, seems to be,
[00:25:58.480 --> 00:26:04.880]   um, like really well behaved and have like impeccable, uh, moral compass and everything.
[00:26:04.880 --> 00:26:10.880]   And you're pretty confident that it's, it's extremely resistant to any kind of takeover
[00:26:10.880 --> 00:26:15.760]   attempt or something, or like severe misuse. And then you would also want to have like a really
[00:26:15.760 --> 00:26:20.160]   good monitoring on top of it. So yeah, you could detect any kind of any trouble.
[00:26:20.160 --> 00:26:24.640]   What are you keeping track of while you're doing long horizon RL or when you eventually start
[00:26:24.640 --> 00:26:30.560]   doing it that, uh, you, you could notice this sort of discontinuous jump before you
[00:26:30.640 --> 00:26:35.360]   deployed these systems broadly? I would say you would want to have a lot of evals that you're
[00:26:35.360 --> 00:26:39.040]   running during the training process. And like what specifically would it,
[00:26:39.040 --> 00:26:44.000]   how would you notice something like, yeah. And I mean, I mean, does it make sense to train on
[00:26:44.000 --> 00:26:47.600]   a long horizon RL knowing that this is something that could happen or is it just like a very low
[00:26:47.600 --> 00:26:51.360]   possibility? How do you think about this? You'd want to be pretty careful when you
[00:26:51.360 --> 00:26:57.920]   do this kind of training. If you see, um, uh, a lot of, um, uh, potentially scary capabilities,
[00:26:57.920 --> 00:27:05.600]   um, if those seem close, I mean, like, uh, I would say it's not something, uh, we would want to,
[00:27:05.600 --> 00:27:09.440]   we have to be scared of right now because, uh, right now it's hard to get the models to do
[00:27:09.440 --> 00:27:16.480]   anything like coherent, but if they started to get really good, I think, um, yeah, I think we
[00:27:16.480 --> 00:27:20.640]   would want to, um, we would have to take some of these questions seriously and we would want to
[00:27:20.640 --> 00:27:28.160]   have a lot of evals that, uh, like sort of, uh, test them for, um, misbehavior in the most, or I
[00:27:28.160 --> 00:27:33.200]   guess that's like for the alignment of the models. We'd want to check, uh, we want to check that,
[00:27:33.200 --> 00:27:41.840]   um, they're not gonna, um, they're not gonna sort of turn against us or something. Uh, but you,
[00:27:41.840 --> 00:27:46.320]   you might also want to look for, uh, like discontinuous jumps and capabilities. Like, um,
[00:27:47.200 --> 00:27:53.040]   uh, you, you'd want to have lots of evals for the capabilities of the models. I mean, also,
[00:27:53.040 --> 00:27:57.520]   I guess, uh, you, you'd also want to make sure that whatever you're training on doesn't, uh,
[00:27:57.520 --> 00:28:03.680]   have any reason to make the model turn against you, which itself I think isn't, um, I would say
[00:28:03.680 --> 00:28:12.480]   there's, uh, like, um, that doesn't seem like the hardest thing to do. I mean, if, uh, like the way
[00:28:12.480 --> 00:28:18.400]   we train them with RLHF, uh, that, that does feel, even though the models are very smart,
[00:28:18.400 --> 00:28:23.520]   it does feel very safe because the model is just trying to produce a message that is, uh, pleasing
[00:28:23.520 --> 00:28:28.480]   to a human. And it has no, um, concern about anything else in the world other than whether
[00:28:28.480 --> 00:28:35.520]   this text it produces is, uh, is approved. Um, so obviously if you were doing something where
[00:28:35.520 --> 00:28:40.640]   there's, uh, where the model has, um, yeah, it's carrying out a long sequence of actions,
[00:28:40.640 --> 00:28:45.520]   which involve tools and everything, then it might have some incentive to do a lot of wacky,
[00:28:45.520 --> 00:28:50.720]   like wacky things that wouldn't make sense to a human in the process of producing its final result.
[00:28:50.720 --> 00:28:55.840]   Uh, but I guess, um, it wouldn't necessarily have an incentive to do anything other than
[00:28:55.840 --> 00:29:02.480]   produce a very high quality, um, output at the end. So it, um, like it's not, um,
[00:29:02.480 --> 00:29:07.360]   yeah, so I guess you have these old, uh, points about like instrumental convergence,
[00:29:07.360 --> 00:29:11.760]   like the model is going to want to take over the world so I can produce this awesome piece of code
[00:29:11.760 --> 00:29:17.120]   at the end. Like if you ask it to write you the flask app, it'll be like, Oh yeah, first I need to
[00:29:17.120 --> 00:29:22.320]   take over the world. And then I, I need to, I don't know, but at a certain point it's a little
[00:29:22.320 --> 00:29:28.560]   bit, um, it's a little hard to imagine why, um, for some like fairly well-specified tasks like
[00:29:28.560 --> 00:29:34.160]   that, you would want to first take over the world. Um, but of course, uh, yeah, if you had a task,
[00:29:34.160 --> 00:29:41.520]   like make money, uh, then maybe, uh, that would lead to some nefarious behavior as a, um,
[00:29:41.520 --> 00:29:46.080]   instrumental goal. Yeah. Okay. So before we get back to that, I think let's step back and talk
[00:29:46.080 --> 00:29:52.240]   about like, uh, today's, um, RLHF systems and everything. Um, but I do, I do want to follow
[00:29:52.240 --> 00:29:58.560]   up on that third point. It's kind of interesting. Um, okay. So today's RLHF, the way in which it
[00:29:59.280 --> 00:30:05.760]   influences these models is, would you characterize it as in terms of human psychology? Is it a drive?
[00:30:05.760 --> 00:30:12.560]   Is it a goal? Is it an impulse? Like psychologically, what kind of thing, in what way is it
[00:30:12.560 --> 00:30:17.360]   being changed? And I, I'm not, not just like the persona of a chat bot, but just like, don't talk
[00:30:17.360 --> 00:30:21.680]   that way, talk this other way, or don't, don't put those kinds of outputs. Yeah. I would say
[00:30:21.680 --> 00:30:28.960]   there are probably some analogies with a driver goal in humans. So in that, um, you have, um,
[00:30:28.960 --> 00:30:34.320]   you're trying to steer towards a certain set of states rather than some other states. Um, and so
[00:30:34.320 --> 00:30:41.520]   I, I would, I would think that our concept of a driver goal has, um, other, um, elements like,
[00:30:41.520 --> 00:30:47.280]   uh, like the feeling of satisfaction you get for achieving it. And, uh, and those things might,
[00:30:47.280 --> 00:30:51.920]   um, be more like have more to do with the learning algorithm than, uh, what the model
[00:30:51.920 --> 00:30:58.160]   does at runtime, uh, when you just have a fixed model. So I would say, I would say there are
[00:30:58.160 --> 00:31:05.840]   probably some analogies though. Um, it's, uh, I don't know exactly, um, like how, how close it is,
[00:31:05.840 --> 00:31:12.000]   but I would say to some extent it is, um, it, the models, um, do have drives and goals in some
[00:31:12.000 --> 00:31:18.080]   meaningful way. And in, in the case of RLHF, where you're trying to, um, maximize, um, human approval
[00:31:18.080 --> 00:31:22.640]   as measured by a reward model, the model is just trying to produce something that people are going
[00:31:22.640 --> 00:31:28.960]   to like and the, and is, they're going to judge as correct. I've heard two ideas in terms of using
[00:31:28.960 --> 00:31:34.400]   that inner monologue type of thing to get better at reasoning, at least publicly, the kinds of
[00:31:34.400 --> 00:31:41.200]   things I've seen. And I'm curious what you think is more promising. One is that the model learns
[00:31:41.200 --> 00:31:47.920]   from, it outputs a bunch of potential trains of thought, and it learns to follow the one that
[00:31:47.920 --> 00:31:55.040]   leads to the correct answer and is trained on that before deployment. And the other one is you use a
[00:31:55.040 --> 00:32:01.040]   bunch of compute to do inference in deployment, which involves the model talking to itself after,
[00:32:01.040 --> 00:32:06.080]   you know, while, while it's deployed, which one do you expect it to be closer to when it's like
[00:32:06.080 --> 00:32:09.520]   really good at reasoning? Is it because it's doing just a bunch of inference clouds or is it just
[00:32:09.520 --> 00:32:15.520]   because you've trained it to do all of that? Well, I would say you could define reasoning as, um,
[00:32:15.520 --> 00:32:21.680]   tasks that require some kind of, uh, uh, like computation, um, at test time, or maybe some kind
[00:32:21.680 --> 00:32:29.040]   of, uh, deduction. Um, so, so by definition, reasoning would be tasks that require, um,
[00:32:29.040 --> 00:32:36.800]   like some test time computation and, uh, like step-by-step computation. Um, on the other hand,
[00:32:36.800 --> 00:32:43.120]   I would also, um, expect to gain a lot out of, um, like doing some kind of, um, training time
[00:32:43.120 --> 00:32:49.360]   computation or practice that training time. Uh, so, so I would think that, um, you get the best
[00:32:49.360 --> 00:32:56.240]   results by combining, uh, combining these two things. So my uncle had prostate cancer and I
[00:32:56.240 --> 00:33:02.080]   wanted to know my own risk and I got a 23andMe test, but it was mostly useless. I mean, the
[00:33:02.080 --> 00:33:07.040]   whole, what are your odds of liking chocolate is not what I was looking for. So I exported my data
[00:33:07.040 --> 00:33:12.400]   onto Nucleus Genomics and immediately I got my risk profile for almost two dozen diseases,
[00:33:12.400 --> 00:33:18.480]   including prostate cancer. And it turns out my risk is higher than 97% of people with my ancestry,
[00:33:18.480 --> 00:33:22.880]   which is a very useful thing to know because I now I know to get screened early. Ironically,
[00:33:22.880 --> 00:33:26.960]   tests like 23andMe don't even look at the variants which have the largest impact,
[00:33:26.960 --> 00:33:31.120]   including for prostate cancer. Many people don't know this, but 23andMe looks at less
[00:33:31.120 --> 00:33:38.640]   than 0.1% of your DNA. And that's why I pre-ordered Nucleus Premium whole genome sequencing. It's the
[00:33:38.640 --> 00:33:44.880]   only clinical grade test that reads 100% of your DNA. I've spent a lot of time digging into this
[00:33:44.880 --> 00:33:49.040]   company and I think it will be a big change in what we can get out of genetic tests. So if you
[00:33:49.040 --> 00:33:55.040]   want to live a long and healthy life, you can pre-order Nucleus Premium at mynucleus.com.
[00:33:55.040 --> 00:34:01.120]   All right, back to John. Right now, you know, you have these two ways in which the model
[00:34:01.120 --> 00:34:06.160]   learns. It's either in training, whether it's free training or with the post-training, but it's like
[00:34:06.960 --> 00:34:11.200]   most of the computing training is spent on free training and just glossing over trillions of
[00:34:11.200 --> 00:34:17.520]   tokens, just like standing by as a, you know, like almost like skimming trillions of tokens
[00:34:17.520 --> 00:34:23.360]   worth of information, which if a human was subjected to that would just be totally confused,
[00:34:23.360 --> 00:34:28.320]   right? It's like not a very efficient way to learn. And the other way is in context learning,
[00:34:28.320 --> 00:34:33.520]   but of course that is more sample efficient there, but it's destroyed with each instance.
[00:34:33.520 --> 00:34:39.600]   I'm curious if you think that there's a path for something in between those where
[00:34:39.600 --> 00:34:43.440]   it's not destroyed with each instance, but it's also not as,
[00:34:43.440 --> 00:34:51.760]   not as sort of frivolous as just seeing trillions of tokens where it's more deliberate and active.
[00:34:51.760 --> 00:34:59.360]   Yeah. So do you mean models having some kind of medium term memory, so too much to fit in context,
[00:34:59.360 --> 00:35:04.880]   but like much smaller scale than pre-training? I'm not sure if memory, it might be memory.
[00:35:04.880 --> 00:35:10.400]   I don't have context, but certainly like when I, when I'm trying to prepare for this conversation,
[00:35:10.400 --> 00:35:16.000]   it feels like I think of like what I should understand this. So I look it up and I like
[00:35:16.000 --> 00:35:20.560]   read it carefully and I maybe think about it as I'm reading it. And I'm not sure what it
[00:35:20.560 --> 00:35:25.040]   naturally corresponds to in terms of models, but what would that look like? I'm curious.
[00:35:25.040 --> 00:35:30.240]   I see. So it's not just a memory, but it's also somewhat like specializing to a task
[00:35:30.240 --> 00:35:36.320]   that specializing to a certain task or putting a lot of effort into like some particular project.
[00:35:36.320 --> 00:35:41.440]   And I'm not sure it's specialization more. So I'm thinking about, I don't understand this part. So
[00:35:41.440 --> 00:35:44.800]   let me look into this part deeper. I already understand this. I'm going to like specializing
[00:35:44.800 --> 00:35:50.800]   to your existing knowledge base. Yeah. I see. So it's not just about finding like,
[00:35:52.240 --> 00:35:56.080]   I don't know, training on a bunch of sources that are relevant, fine tuning on some special
[00:35:56.080 --> 00:36:01.600]   domain. It's also about like, uh, like reasoning about like developing some knowledge through your
[00:36:01.600 --> 00:36:07.680]   own reasoning and also using some sort of, uh, introspection and self-knowledge to figure out
[00:36:07.680 --> 00:36:13.600]   what you need to learn. Yeah. Um, yeah, I would say that does feel like, um, something that's
[00:36:13.600 --> 00:36:21.520]   missing from today's systems. I mean, I would say, um, people haven't really pushed too hard
[00:36:21.520 --> 00:36:28.640]   on this middle ground between, uh, like large-scale training, like where you produce the, like this
[00:36:28.640 --> 00:36:33.280]   snapshot model, that's supposed to do everything like a deployed model. And then like, on the
[00:36:33.280 --> 00:36:37.840]   other hand, like in context learning. And I think part of that is that we've just been, uh,
[00:36:37.840 --> 00:36:42.480]   increasing context length so much that there hasn't been an incentive for it. So if you can
[00:36:42.480 --> 00:36:48.000]   go to like a hundred thousand or a million context, then that's actually quite a lot. And, uh,
[00:36:48.560 --> 00:36:55.280]   it's not, um, it's not actually the bottleneck in a lot of cases, but I agree that, um, you'd
[00:36:55.280 --> 00:37:00.960]   probably also want to supplement that by some kind of fine tuning, like the, uh, the capabilities you
[00:37:00.960 --> 00:37:07.840]   get from fine tuning and in context learning are probably somewhat complimentary. So I would expect
[00:37:07.840 --> 00:37:13.600]   us to want to build systems that do some kind of online learning and also have some of these, uh,
[00:37:13.600 --> 00:37:20.080]   cognitive skills of, uh, like introspecting on their own knowledge and, uh, seeking out new,
[00:37:20.080 --> 00:37:27.840]   new knowledge that fills in the holes. Uh, is this all happening at the same time? Like, uh,
[00:37:27.840 --> 00:37:30.640]   is it just like a new training regime where all these things can happen at once or
[00:37:30.640 --> 00:37:36.240]   whether it's the long horizon training or whether it's this kind of training,
[00:37:36.240 --> 00:37:39.040]   are they separate or are they just because like the model is smart enough so they can
[00:37:39.040 --> 00:37:43.760]   both introspect and it can act on longer horizons and you can get adequate reward
[00:37:43.760 --> 00:37:48.960]   on the long horizon tasks? Yeah. I would say if you're doing some kind of long horizon task, uh,
[00:37:48.960 --> 00:37:56.560]   well, I would, uh, you're learning while you do the task, right? So the only way to do something
[00:37:56.560 --> 00:38:03.040]   that involves, uh, a lot of steps is to, um, like to have learning and memory that gets updated
[00:38:03.040 --> 00:38:11.040]   during the task. So, uh, like there's a continuum between, um, like, uh, like short-term memory, uh,
[00:38:11.040 --> 00:38:20.000]   um, between short-term and long-term memory. So, um, I would say, uh, yeah, I would expect,
[00:38:20.000 --> 00:38:27.360]   uh, I would expect this, uh, capability would start to become, uh, like the need for it would
[00:38:27.360 --> 00:38:34.800]   start to become clear when we start to, uh, look at long horizon tasks more. And, uh, and to some
[00:38:34.800 --> 00:38:39.920]   extent, just, um, putting, um, a lot of stuff into context will probably will take you pretty
[00:38:39.920 --> 00:38:45.120]   far because we have really long context now, but you probably also want things like fine tuning.
[00:38:45.120 --> 00:38:52.160]   And as for, uh, like introspection and the ability to do active learning, um, that might,
[00:38:52.160 --> 00:38:57.280]   uh, like automatically fall out of the model's abilities to know what they know, because they
[00:38:57.280 --> 00:39:03.760]   have some, like, um, models have some calibration, um, regarding what they know. And that's why,
[00:39:03.760 --> 00:39:11.200]   like, that's why, um, models don't hallucinate that badly, uh, because yeah, they have some
[00:39:11.200 --> 00:39:17.200]   understanding of the, their, their own limitations. So I think that like same kind of ability could
[00:39:17.200 --> 00:39:26.560]   be used for something like active learning and how, so, uh, there's all these complicating RR
[00:39:26.560 --> 00:39:33.600]   procedures, uh, that many of whom you've pioneered, how many of them will be relevant when you get to
[00:39:33.600 --> 00:39:39.200]   the point where the, uh, the model itself is this smart, that it can act as its own environment and
[00:39:40.400 --> 00:39:46.160]   interact in a more online and stable way. Um, is it, is, is, is, is the path for progress
[00:39:46.160 --> 00:39:51.200]   going to be more straightforward than the kinds of solutions that were required for RL in the past?
[00:39:51.200 --> 00:39:57.120]   Well, I think policy gradient algorithms are not the most sample efficient algorithms. So that's
[00:39:57.120 --> 00:40:01.920]   probably not what you want to do at test time if you want to learn really fast. Um, but though,
[00:40:01.920 --> 00:40:09.280]   who knows, I mean, maybe it's not that bad. Um, so I think, um, something like, um, like motor
[00:40:09.280 --> 00:40:15.840]   learning in animals is probably something like a policy grading algorithm. And, uh, so for example,
[00:40:15.840 --> 00:40:21.520]   you're like learning how to, um, shoot baskets. Uh, I think you probably, uh, like that takes,
[00:40:21.520 --> 00:40:26.640]   uh, maybe thousands of tries to, um, get more accurate. And I think you probably,
[00:40:26.640 --> 00:40:32.080]   there's probably some, something that's, uh, like a policy grading algorithm underneath. Um,
[00:40:32.080 --> 00:40:38.640]   but, uh, that's not going to be the fastest way to learn in, um, like if, if you have a model
[00:40:38.640 --> 00:40:44.400]   trying to do a project or some kind of task. Um, so I would think we would want to rely more on,
[00:40:44.400 --> 00:40:51.440]   like in context learning, um, where, uh, you effectively have a learned algorithm,
[00:40:51.440 --> 00:40:56.160]   like you've learned how to explore, uh, like you've learned how to try all the possibilities
[00:40:56.160 --> 00:41:01.920]   exhaustively. Um, and, uh, instead of doing the same thing over and over again, making the same
[00:41:01.920 --> 00:41:07.360]   mistake. So, yeah, I would say we'll be able to do things that look more like learned search
[00:41:07.360 --> 00:41:13.040]   algorithms and that'll be the kind of thing that, uh, gets used in a particular task.
[00:41:13.040 --> 00:41:20.160]   Interesting. All right. I want to, uh, step back and ask about your own history. So, um,
[00:41:20.160 --> 00:41:27.280]   at least at opening. I, so, uh, you, you led a creation of tragedy at what point do you,
[00:41:27.280 --> 00:41:32.320]   did you realize, first of all, these LLMs are the path to go. And then a chat bot would be,
[00:41:32.320 --> 00:41:35.600]   or some way to instruct them would be a useful thing to do. Just walk me through the whole
[00:41:35.600 --> 00:41:41.200]   lineage from like, when this became the, your main focus and yeah. What, what, what, what,
[00:41:41.200 --> 00:41:49.040]   yeah, what, what the process was like. Yeah. So early. Um, so we had, um, uh, before chat GBT,
[00:41:49.040 --> 00:41:55.600]   uh, we had, um, open AI had these instruction following models and, uh, that was the idea there
[00:41:55.600 --> 00:42:02.480]   was, um, we had base models and people can prompt them in elaborate ways. Um, but, uh, they're also
[00:42:02.480 --> 00:42:07.200]   kind of hard to prompt. You had to, uh, they basically do auto-complete. So you have to set
[00:42:07.200 --> 00:42:14.400]   up a very good prompt with some examples. So, uh, uh, so, uh, people at open AI, uh,
[00:42:14.400 --> 00:42:19.200]   we're working on, um, just taking the base models and making them easier to prompt so that if you
[00:42:19.200 --> 00:42:23.040]   just wrote a question, it would answer the question instead of giving you more questions
[00:42:23.040 --> 00:42:29.120]   or something. Uh, so that was, uh, so, so we had these instruction following models, which were
[00:42:29.120 --> 00:42:34.640]   kind of like base models, but a little easier to use. Um, and those are the original ones deployed
[00:42:34.640 --> 00:42:41.520]   in the API or after, um, GPT three, those were the next generation of models. Um, then at the
[00:42:41.520 --> 00:42:48.240]   same time, there were definitely a lot of people thinking about, um, chat. So, uh, so Google had
[00:42:48.240 --> 00:42:55.120]   some papers, uh, like they had a Lambda and, um, earlier Mina. So they had these chat bots and it
[00:42:55.120 --> 00:43:01.360]   was more like, um, uh, like you had a, it was more like a base model that was really specialized to,
[00:43:01.360 --> 00:43:06.960]   um, the task of chat, really good at chat. And, uh, like, I think at least, uh, looking at the
[00:43:06.960 --> 00:43:12.960]   examples from the paper, it was more, uh, used for sort of fun applications, like, um, where the
[00:43:12.960 --> 00:43:17.680]   model would, uh, like take on some persona and pretend to be that persona. It was not so
[00:43:17.680 --> 00:43:24.640]   functional, like, um, like help me refactor my code. Um, so yeah, there are definitely people
[00:43:24.640 --> 00:43:31.840]   thinking about chat. I had worked on a project before, uh, looking at chat called, uh, web GPT,
[00:43:31.840 --> 00:43:37.360]   which was more about doing question answering with the help of, uh, web browsing and retrieval.
[00:43:37.360 --> 00:43:45.120]   And well, when you do question answering, uh, it really wants to be in a chat because, um,
[00:43:45.120 --> 00:43:49.920]   you always, uh, want to ask follow-up questions, or sometimes you need to clarify the model should
[00:43:49.920 --> 00:43:54.320]   ask a clarifying question because the question's ambiguous. So it was kind of clear after we did
[00:43:54.320 --> 00:43:58.240]   the first version of that, that we should, the next version should be conversational.
[00:43:58.240 --> 00:44:06.320]   So anyway, we started working on, uh, like the conversational chat assistant. Um, and, uh,
[00:44:06.320 --> 00:44:14.480]   we, uh, this was built on top of GPT 3.5, which was done training at the beginning of 2022. And,
[00:44:14.480 --> 00:44:18.720]   uh, that model was quite good at language and code. So we quickly realized that it was actually
[00:44:18.720 --> 00:44:23.840]   quite good at coding help. And that was one of the things we were excited about.
[00:44:23.840 --> 00:44:30.320]   So, yeah, we worked on that. Uh, we worked on that for, for most of the year and, uh, we had,
[00:44:30.320 --> 00:44:35.760]   we had browsing, um, as another feature in it, though we ended up, uh, like de-emphasizing that
[00:44:35.760 --> 00:44:40.800]   later on because the, like the models internal knowledge was so good that we didn't, that the
[00:44:40.800 --> 00:44:46.400]   browsing, um, wasn't the most interesting thing about it. Um, and then, uh, we were thinking
[00:44:46.400 --> 00:44:51.600]   about, we had it out for beta testing or to friends and family for a while. And, uh, we
[00:44:51.600 --> 00:44:58.080]   were thinking about doing a public release. Um, but, um, at that time, uh, actually GPT 4 finished
[00:44:58.080 --> 00:45:08.080]   training in August or, um, yeah, in August that year. And, um, actually the, um, like the flagship
[00:45:08.080 --> 00:45:13.280]   RL effort at open AI was the instruction following effort. Cause that was the models that were being
[00:45:13.280 --> 00:45:19.600]   deployed into productions. So, um, like the first fine tunes of GPT 4 used that, um, that whole
[00:45:19.600 --> 00:45:25.600]   stack. And that was, um, yeah, those models were really good. And everyone got really excited about
[00:45:25.600 --> 00:45:31.280]   that after seeing the, uh, like instruct fine tune GP force. Uh, but so they were really,
[00:45:31.280 --> 00:45:35.440]   really good. They would occasionally give you amazing outputs, but they were also like a little
[00:45:35.440 --> 00:45:40.800]   bit, the model was clearly like pretty unreliable. Like it would sometimes hallucinate it a lot.
[00:45:40.800 --> 00:45:45.360]   And it was like pretty, it would sometimes give you pretty unhinged outputs. So it was clearly
[00:45:45.360 --> 00:45:52.160]   not quite ready for primetime, but it was like, obviously very good. Um, and, uh, yeah, so I guess
[00:45:52.160 --> 00:45:58.640]   that, um, people forgot about chat for a little while after that, because about this like alternative
[00:45:58.640 --> 00:46:03.680]   branch. Uh, but then we, we ended up, um, we pushed it further and we ended up like mixing
[00:46:03.680 --> 00:46:08.000]   together all the datasets, like the instruct and the chat data and to try to get something
[00:46:08.000 --> 00:46:13.280]   that was the best of both worlds. And, uh, I think the, yeah, the models, we, the chat models were
[00:46:13.280 --> 00:46:21.200]   like, uh, were clearly more, um, like it was an easy, easier to use. It was sort of more, um,
[00:46:21.200 --> 00:46:26.560]   it sort of, uh, like automatically had much more sensible behavior in terms of like the model,
[00:46:26.560 --> 00:46:31.360]   knowing its own limitations. That was actually one of the things that, uh, I got excited about
[00:46:31.360 --> 00:46:37.520]   as we were developing it that, uh, like I realized a lot of the things that, um, people thought were
[00:46:37.520 --> 00:46:44.160]   flaws in language models, like just like blatantly hallucinating, uh, could be not completely fixed,
[00:46:44.160 --> 00:46:50.880]   but you could make a lot of progress with pretty straightforward methods. Uh, oh yeah. And also
[00:46:50.880 --> 00:46:58.000]   the, um, the other thing about chat was that, uh, like when we had these instruct models, uh, like
[00:46:58.000 --> 00:47:03.120]   the task of, uh, complete this text, but in a nice way or in a helpful way, that's like a pretty
[00:47:03.120 --> 00:47:08.320]   poorly defined task. So I think, uh, like, I think that task is like both confusing for the model and
[00:47:08.320 --> 00:47:13.280]   for the human who's supposed to do the data labeling. Whereas for chat, um, I think people
[00:47:13.280 --> 00:47:18.800]   had an intuitive sense of, uh, like what a helpful robot should be like. So I think it was, uh, just
[00:47:18.800 --> 00:47:25.440]   much easier to tell people, uh, like, uh, to, to give for people to get the idea of what, what the
[00:47:25.440 --> 00:47:31.360]   model was supposed to do. Yeah. Um, and, uh, so that, so as a result, I think the, um, like the
[00:47:31.360 --> 00:47:37.520]   model had a much more coherent personality and, uh, like it was much like easier to get, um,
[00:47:37.520 --> 00:47:44.720]   like robot, like pretty sensible behavior, um, robustly. Interesting. Uh, is it the case that
[00:47:44.720 --> 00:47:52.800]   anybody could have made chat GBT using your publicly available fine tuning API? Um, not
[00:47:52.800 --> 00:47:59.840]   exactly. I mean, uh, they could have, um, I don't remember the status of which models were available
[00:47:59.840 --> 00:48:06.880]   available for fine tuning. Uh, you, uh, assuming we had 3.5 available for fine tuning at the time,
[00:48:06.880 --> 00:48:11.680]   you could have made something pretty decently close, but I'm not sure you would have, um,
[00:48:11.680 --> 00:48:16.080]   I don't think you would have been able to do just one iteration of fine tuning where you
[00:48:16.080 --> 00:48:20.640]   have like pure, purely human written data. And you fine tune on that. I think you would want,
[00:48:20.640 --> 00:48:26.800]   like, you'd want to do several iterations. Like if you're not going to do RL, um, which, which we
[00:48:26.800 --> 00:48:32.800]   did, um, you'd want to do some kind of iterative supervised fine tuning where you have like humans
[00:48:32.800 --> 00:48:38.080]   edit the model generated outputs, because it's really hard to get people to like, if you train
[00:48:38.080 --> 00:48:43.360]   on human generated data, even if it's really high quality, it's just hard for a model to fit that
[00:48:43.360 --> 00:48:48.640]   data perfectly because it might not be like, it might not be something a model is capable of
[00:48:48.640 --> 00:48:54.080]   outputting. Uh, so you need to do something iterative that looks a little bit more like RL.
[00:48:55.760 --> 00:48:59.120]   So I think if you had done that, you could have gotten something pretty close, but,
[00:48:59.120 --> 00:49:06.400]   um, that would have been kind of non-trivial. Um, but we also had another, uh, like instruction
[00:49:06.400 --> 00:49:11.680]   following model trained with RL that was released a little before ChatGBT. So I think if you put a
[00:49:11.680 --> 00:49:17.840]   chat, like wrapper on that, you would get something decently close. Uh, but it like that model, um,
[00:49:18.480 --> 00:49:25.840]   like if you just prompted it with chat, um, so, but that model had some, uh, differences in, uh,
[00:49:25.840 --> 00:49:30.240]   strengths, like it was like that model was pretty good at writing and poetry and so forth, but it
[00:49:30.240 --> 00:49:36.400]   wasn't, uh, it sort of, it wasn't as good at knowing its limitations and, uh, at factuality
[00:49:36.400 --> 00:49:43.440]   and so forth. Um, so stepping back from 3.5, I think I heard you somewhere say GPT-2 you're super
[00:49:43.440 --> 00:49:49.920]   impressed compared to your expectations in 2019. Has AI progressed faster or slower than you would
[00:49:49.920 --> 00:49:57.040]   have expected? I would say faster than I would have expected since GPT-2. Yeah. Um, I was pretty,
[00:49:57.040 --> 00:50:04.000]   um, like bought into, um, scaling and, uh, yeah, pre-training and so forth being a good idea. Um,
[00:50:04.000 --> 00:50:11.440]   but, um, when GPT-2 was done, I was, I would say I wasn't completely, uh, sold on it, um, being,
[00:50:11.440 --> 00:50:16.480]   uh, revolutionizing everything. Um, like I only really pivoted what I was working on and what,
[00:50:16.480 --> 00:50:22.320]   yeah, what, what my team was working on in, um, after GPT-3. So after that, uh, we kind of got
[00:50:22.320 --> 00:50:28.400]   together and said, oh yeah, let's, uh, uh, let's, um, this language model stuff works really well.
[00:50:28.400 --> 00:50:33.920]   Let's see what we can do here. But, uh, yeah, after GPT-2, I wasn't quite sure yet.
[00:50:33.920 --> 00:50:40.640]   Hmm. Uh, especially if the stuff we were talking about earlier with, uh, RL starts working better
[00:50:40.640 --> 00:50:46.960]   with these smarter models with a fraction of compute that is spent on training that is pre-training
[00:50:46.960 --> 00:50:51.600]   versus post-training change significantly in favor of post-training in the future.
[00:50:51.600 --> 00:50:57.200]   Yeah. There are some arguments for that. I mean, right now it's a pretty lopsided ratio, but
[00:50:57.200 --> 00:51:03.280]   you could argue that the, uh, output generated by the model is like high quality compared to,
[00:51:03.280 --> 00:51:09.280]   or higher quality than most of what's on the web. So, uh, it sort of makes more sense for the model
[00:51:09.280 --> 00:51:16.560]   to, uh, think by itself, um, instead of just, um, like training to, uh, imitate what's on the web.
[00:51:16.560 --> 00:51:23.120]   So I think there's a first principles argument for that. And, um, I would say we found a lot
[00:51:23.120 --> 00:51:28.400]   of gains through post-training. So, um, I'm not sure. So I would expect us to keep, um,
[00:51:28.400 --> 00:51:34.800]   like pushing this methodology and probably increasing the amount of compute we put into it.
[00:51:34.800 --> 00:51:43.120]   Hmm. The current GPD 4 has a ELO score that is like a hundred points higher than the original
[00:51:43.120 --> 00:51:48.240]   one that was released. And is that all because of what you're talking about with these improvements
[00:51:48.240 --> 00:51:54.160]   that are brought on by post-training or. Yeah, I would say that we've, um, I would say that
[00:51:54.160 --> 00:51:59.360]   most of that is post-training. Interesting. Um, so there are a lot of, um, there are a lot of
[00:51:59.360 --> 00:52:06.160]   different, uh, separate axes for improvement. Like you can, uh, yeah, so it's, we think about,
[00:52:06.160 --> 00:52:11.680]   um, like data quality, data quantity, just doing more iterations of the whole, uh, process of
[00:52:11.680 --> 00:52:17.040]   deploying and collecting new data and like changing what you're, what kind of annotations
[00:52:17.040 --> 00:52:21.920]   you're collecting. So there's a lot of, uh, a lot of things that stack up, but together they give
[00:52:21.920 --> 00:52:27.920]   you a pretty good, um, like effective compute increase. Yeah. I mean, that's a huge increase.
[00:52:27.920 --> 00:52:34.640]   That's like really interesting that there's this much, uh, this much room for improvement,
[00:52:34.640 --> 00:52:39.840]   uh, from post-training. What is, uh, what, what makes for somebody who's really good at doing
[00:52:39.840 --> 00:52:44.720]   this sort of RR research? Uh, I hear it's super finicky, but like, what, what is the sort of
[00:52:44.720 --> 00:52:52.000]   intuitions that you have that enable you to find these ways to mess with the data and set up these
[00:52:52.000 --> 00:52:58.800]   environments? I'd say I just, um, have a decent amount of experience at this point from, uh,
[00:52:58.800 --> 00:53:06.240]   like the different parts of the stack from like, uh, RL algorithms, obviously, since I've worked
[00:53:06.240 --> 00:53:14.400]   on those since, uh, grad school, uh, to like, uh, the data collection, uh, like the annotation
[00:53:14.400 --> 00:53:22.320]   process, uh, to, um, like language playing with language models. So I, I mean, I'd say I just
[00:53:22.320 --> 00:53:27.760]   dabbled with these things and, uh, I'd say the people who, um, do well at this kind of research,
[00:53:27.760 --> 00:53:32.800]   uh, have some view of the whole stack and have a lot of curiosity about the different parts of it.
[00:53:32.800 --> 00:53:40.640]   And, uh, also sort of think about, um, well, you want to be both empirical, um, and, uh, like
[00:53:41.280 --> 00:53:44.960]   use experiment, let experiments update your views, but you also want to think from first
[00:53:44.960 --> 00:53:53.680]   principles somewhat like, uh, what, um, uh, like assuming that, um, like learning, uh, works,
[00:53:53.680 --> 00:53:57.840]   uh, like what would be the ideal type of data to collect and that sort of thing.
[00:53:57.840 --> 00:54:01.840]   So, because there doesn't seem to be a model released since GPT-4 that seems to be significantly
[00:54:01.840 --> 00:54:09.120]   better, there's seems to be, uh, the hypothesis that potentially we're hitting some sort of plateau
[00:54:09.120 --> 00:54:13.200]   and that these models aren't actually generalizing that well, and you're going to
[00:54:13.200 --> 00:54:20.000]   hit some sort of data wall beyond which point the abilities that are unlocked by memorizing
[00:54:20.000 --> 00:54:25.520]   a vast corpus of pre-training data won't actually help you get something much smarter than GPT-4.
[00:54:25.520 --> 00:54:31.520]   Um, what do you think that hypothesis is that wrong? And like, I think we've talked about
[00:54:31.520 --> 00:54:36.560]   some examples generically about generalization, the Spanish to English and so forth, but
[00:54:36.560 --> 00:54:44.960]   is there, yeah, I mean, okay. So maybe this is a run on set question, but, um, well, one,
[00:54:44.960 --> 00:54:52.560]   one example I was thinking of was the idea that there is transfer from language code,
[00:54:52.560 --> 00:54:56.320]   reasoning and code. If you train a bunch of code, it gets better at reasoning and language.
[00:54:56.320 --> 00:55:01.360]   And if that's the, is that actually the case? Do you see things like that, which is just that
[00:55:01.360 --> 00:55:04.400]   there's all this credit positive transfer between different modalities. So once you
[00:55:04.400 --> 00:55:08.400]   try a training on a bunch of videos and images, it'll get smarter and it'll get smarter from
[00:55:08.400 --> 00:55:14.240]   that synthetic data. Or does it seem like the abilities that are unlocked are extremely local
[00:55:14.240 --> 00:55:21.360]   to the exact kind of labels and data you put into the training corpus? Yeah. Okay. Yeah.
[00:55:21.360 --> 00:55:26.720]   I'll try to respond to all of that. So first, um, are we about to hit the data wall? I mean,
[00:55:26.720 --> 00:55:33.360]   I wouldn't draw too much from, uh, the, uh, time since GPT-4 was released because I mean, it does,
[00:55:33.360 --> 00:55:42.160]   um, um, yeah, it takes a while to, um, like train these models and to, um, like get all the, uh,
[00:55:42.160 --> 00:55:48.400]   do all the prep to, um, train a new model, like generation of models. So, uh, yeah,
[00:55:48.400 --> 00:55:54.320]   I wouldn't draw too much from, from that fact. Um, I would say, um, there are definitely some
[00:55:54.320 --> 00:56:00.000]   challenges from the limited amount of data. Um, but I wouldn't expect us to immediately hit the
[00:56:00.000 --> 00:56:07.040]   data wall. Um, but I would expect, uh, the nature of, um, pre-training to somewhat change over time
[00:56:07.040 --> 00:56:15.120]   as we get closer, closer to it. Um, in terms of like, uh, generalization from different types of
[00:56:15.120 --> 00:56:23.120]   pre-training data, um, I would say it's pretty hard to, um, do science, uh, on this type of
[00:56:23.120 --> 00:56:28.000]   question because you can't do that, create that many pre-trained models. So maybe, uh, you can't
[00:56:28.000 --> 00:56:34.720]   train a, like a GPT-4 size model. You can't do ablation studies at GPT-4 size scale. Uh, maybe
[00:56:34.720 --> 00:56:40.800]   you can do, like train a ton of, um, GPT-2 size models, or maybe even a GPT-3 size model with
[00:56:40.800 --> 00:56:47.040]   different data blends and see what you get. Uh, so I'm not like, um, aware of any results, uh,
[00:56:47.040 --> 00:56:53.360]   or public, like public results on, um, like ablations, um, involving code data and reasoning
[00:56:53.360 --> 00:56:59.360]   performance and so forth. Um, so that would be, I'd be very interested to know about those results,
[00:56:59.360 --> 00:57:04.400]   but I'm, I'm actually curious about, uh, I mean, if one of the things is that the model gets
[00:57:04.400 --> 00:57:10.480]   moderates is bigger, would an ablation on a GPT-2 level model, which suggests that there isn't that
[00:57:10.480 --> 00:57:18.160]   much transfer, how much evidence does that provide for the level of transfer on a similar set of
[00:57:18.160 --> 00:57:23.840]   domains in the GPT-4 level model? Right. You might not be able to conclude that, uh, if transfer
[00:57:23.840 --> 00:57:30.560]   fails at GPT-2 size, then it's also going to fail at a higher scale. Uh, so it might be that, um,
[00:57:30.560 --> 00:57:37.120]   like for, uh, the smaller models, um, you, uh, or yeah, for the larger models, you learn these
[00:57:37.120 --> 00:57:43.440]   better shared representations, um, or the smaller models have to lean, uh, too much on memorization,
[00:57:43.440 --> 00:57:48.480]   whereas the larger models can learn how to do the right computation. So I would expect, uh,
[00:57:48.480 --> 00:57:53.920]   this to be true to some extent. This might have a very simple answer, but so bigger models,
[00:57:53.920 --> 00:58:00.800]   you train them on the same amount of data and they become smarter, or conversely, they can,
[00:58:00.800 --> 00:58:06.080]   to get the same amount of smarts, you have to train them on less data. What, what, why is that
[00:58:06.080 --> 00:58:10.400]   the case? Like it's got more parameters. I saw less things and now it's equally as smart. What,
[00:58:10.400 --> 00:58:14.880]   why did that, why, why is that the case? Uh, I don't think anyone has a good answer for
[00:58:14.880 --> 00:58:21.120]   a good explanation of the, uh, scaling law with, um, parameter count. I mean, there's some,
[00:58:21.120 --> 00:58:30.400]   uh, I don't even know what the, uh, what the best, um, sort of mental model is for this. Like
[00:58:30.400 --> 00:58:34.960]   clearly you have more capacity if you have a bigger model, but, uh, so like you should be
[00:58:34.960 --> 00:58:41.840]   able to eventually get a lower loss, but I guess, uh, why are bigger models more sample efficient?
[00:58:41.840 --> 00:58:48.400]   Um, I guess you could, um, I can give you some like very sketchy, uh, explanation. Like, uh,
[00:58:48.400 --> 00:58:55.360]   like they have, um, like you could say that the model is, uh, like, uh, sort of an, uh, an
[00:58:55.360 --> 00:59:00.880]   ensemble of a bunch of different circuits that do the computation. So it has like, um, you could
[00:59:00.880 --> 00:59:05.440]   imagine that it's doing, um, it has a bunch of, uh, like computations that it's doing in parallel
[00:59:05.440 --> 00:59:11.360]   and it's, uh, like doing some, like the output is a weighted combination of them. Uh, and, uh,
[00:59:11.360 --> 00:59:17.600]   if you have more, um, just width of the model, or if you just have, I mean, actually width is
[00:59:17.600 --> 00:59:22.560]   somewhat similar to depth because, uh, like with residual networks, uh, you end up like
[00:59:22.560 --> 00:59:29.200]   the depth can do something similar to width in terms of like updating what's in the residual
[00:59:29.200 --> 00:59:35.120]   stream. But, uh, if you, yeah, you could argue that, uh, you're learning all these things in
[00:59:35.120 --> 00:59:39.840]   parallel, uh, you're learning all these different computations in parallel and you just have more
[00:59:39.840 --> 00:59:44.720]   of them with the bigger model. So you have more chance that, uh, one of them is lucky and, uh,
[00:59:44.720 --> 00:59:51.520]   ends up, um, like, uh, having high, um, like, like winning guessing correctly a lot and getting
[00:59:51.520 --> 00:59:58.720]   upweighted. So that's kind of like, uh, um, uh, what would be the, yeah, there's some algorithms,
[00:59:58.720 --> 01:00:05.440]   uh, that work this way. Like that, um, like mixture, uh, what is it? Mixture, uh,
[01:00:05.440 --> 01:00:12.000]   some kind of mixture model, um, or, uh, multiplicative weight update algorithm. Yeah.
[01:00:12.000 --> 01:00:17.600]   There's some algorithms that kind of work like this. So, uh, where you have like a, um, some
[01:00:17.600 --> 01:00:21.760]   kind of mixture of, uh, I don't want to say mixture of experts cause it means something different,
[01:00:21.760 --> 01:00:27.120]   but, uh, like basically a weighted combination of experts with some learned gating, uh, and,
[01:00:27.120 --> 01:00:33.520]   uh, um, actually, anyway, I said something slightly wrong, but anyway, uh, yeah, you could
[01:00:33.520 --> 01:00:37.600]   imagine something like that. And just having a bigger model gives you more chances to get the
[01:00:37.600 --> 01:00:43.920]   right, uh, function. So that would be, um, uh, and then of course, uh, it's not just like you
[01:00:43.920 --> 01:00:50.400]   have a bunch of, uh, like totally disjoint, like functions that have, uh, you're taking a linear
[01:00:50.400 --> 01:00:54.720]   combination of, it's more like a library where, uh, you might chain the functions together in
[01:00:54.720 --> 01:01:01.440]   some way. So, uh, you, you, like it's, there's some composability. Um, so yeah, so I would just
[01:01:01.440 --> 01:01:06.480]   say there's like, um, the bigger model has a bigger library of different, uh, computations,
[01:01:06.480 --> 01:01:11.280]   including lots of stuff that's kind of dormant and only being used some of the time. Uh, but
[01:01:11.280 --> 01:01:17.280]   those things, but it has like more space to look for the, uh, like look for the circuits to do
[01:01:17.280 --> 01:01:26.080]   something useful. I want to ask you about, um, uh, stepping back from the current, uh, research
[01:01:26.080 --> 01:01:30.880]   questions and just stepping back. I want to understand just sort of like modal scenario of
[01:01:30.880 --> 01:01:34.800]   what happens for the next few years. I think, uh, towards the beginning of the conversation,
[01:01:34.800 --> 01:01:39.200]   we were talking about the case in which it progresses really fast, but just let's just
[01:01:39.200 --> 01:01:45.200]   say like the modal scenario, um, you're unlocking long horizon RL at some point, but then as you
[01:01:45.200 --> 01:01:51.200]   said, there's potentially other bottlenecks. So what's happening, you know, uh, how good are these
[01:01:51.200 --> 01:01:56.080]   models? How are they being deployed? What other modalities are part of them at what, at what stage
[01:01:56.080 --> 01:01:59.600]   are these being unlocked and so forth? And just kind of want to understand your broader picture
[01:01:59.600 --> 01:02:06.080]   of what the next few years look like. Yeah, I would expect, um, I would expect things like,
[01:02:06.080 --> 01:02:15.280]   okay, new modalities to be added, uh, like, um, over time or, uh, pretty soon. Um, I would,
[01:02:15.280 --> 01:02:18.480]   yeah, I would expect the capabilities to generally keep getting better through a
[01:02:18.480 --> 01:02:24.160]   combination of pre-training and post-training and that'll open up new use cases. So right now, um,
[01:02:24.160 --> 01:02:30.320]   AI is still, um, not a huge, uh, part of the economy. Like there's a pretty small fraction of,
[01:02:30.320 --> 01:02:37.280]   uh, jobs that it can help with at all. Um, so I expect that to be higher over time and not just
[01:02:37.280 --> 01:02:41.920]   from the models, uh, improving also from people just figuring out how to integrate them into
[01:02:41.920 --> 01:02:48.960]   different processes. So even, even if we just, um, froze the models at their current, uh, state,
[01:02:48.960 --> 01:02:54.720]   um, I think you would still see a lot of growth in how they're being used. Um, so I would expect
[01:02:54.720 --> 01:03:03.360]   there to be a lot of, um, like I would expect AI to be, um, used much more widely and, um,
[01:03:03.360 --> 01:03:09.600]   I would expect it to be used for more, um, kind of technique, like
[01:03:09.600 --> 01:03:15.520]   technically sophisticated tasks. Like, um, yeah, like I gave the programming example earlier, um,
[01:03:15.520 --> 01:03:22.160]   of doing like longer projects, but also helping with, um, various kinds of, uh, research. So I
[01:03:22.160 --> 01:03:27.840]   would hope that, uh, we can use, um, AI to accelerate science in various ways. And, uh,
[01:03:27.840 --> 01:03:35.120]   just, um, like, uh, because you can potentially have the, the models, like understand all of the
[01:03:35.120 --> 01:03:42.800]   literature in a given field and be able to like, uh, be able to sift through tons of data, um,
[01:03:42.800 --> 01:03:48.160]   like more than a person would have patience to do. So I would hope that we can basically, uh,
[01:03:48.160 --> 01:03:55.200]   like, yeah. Um, well, I hope the form factor would basically be that people are, uh, still
[01:03:55.200 --> 01:03:59.680]   driving all of this and you have your, uh, like helpful assistants that you can use.
[01:03:59.680 --> 01:04:04.080]   You can sort of direct and point to lots of different problems that are useful to you. And
[01:04:04.080 --> 01:04:10.800]   everyone sort of has all these, uh, AIs, uh, helping them, uh, helping them do more, get more
[01:04:10.800 --> 01:04:16.560]   done. Hey, everybody real quick. I want to tell you about a tool that I wish more applications
[01:04:16.560 --> 01:04:22.480]   used. So obviously you've noticed every single company is trying to add an AI chat bot to their
[01:04:22.480 --> 01:04:28.560]   website, but as a user, I usually find them really annoying because they have these long, generic,
[01:04:28.560 --> 01:04:34.000]   often useless answers. Command bar is a user assistant that you can just embed into your
[01:04:34.000 --> 01:04:39.840]   website or application. And it feels like you're talking to a friendly human support agent who is
[01:04:39.840 --> 01:04:46.160]   browsing with you and for you. And it's much more personalized than a regular chat bot. It can
[01:04:46.160 --> 01:04:51.280]   actually look up users' history and respond differently based on that. It can use APIs
[01:04:51.280 --> 01:04:57.360]   to perform actions. It can even practically nudge users to explore new features. One thing that I
[01:04:57.360 --> 01:05:02.160]   think is really cool is that instead of just outputting text, command bar can kind of just
[01:05:02.160 --> 01:05:08.000]   say here, let me show you and start browsing alongside the user. Anyways, they're in a bunch
[01:05:08.000 --> 01:05:14.400]   of great products already. You can learn more about them at commandbar.com. Thanks to them for
[01:05:14.400 --> 01:05:20.240]   sponsoring this episode. But obviously at some point, they're going to be better than everyone,
[01:05:20.240 --> 01:05:26.080]   whatever they want to do. So what would that process look like? Right now, they're clearly
[01:05:26.080 --> 01:05:31.600]   only helping you. At some point, they're able to just do things for you and maybe run entire
[01:05:31.600 --> 01:05:39.200]   firms for you or whatever. At that point, is it just going to be a smooth process? And at that
[01:05:39.200 --> 01:05:45.040]   point, the hope is that we have systems that are aligned with the user enough that they can count
[01:05:45.040 --> 01:05:52.000]   on the firm being run in the way they expect and so forth? Yeah, I think, well, we might not want
[01:05:52.000 --> 01:05:59.920]   to jump to having AIs run whole firms immediately. I mean, we might want to have people overseeing
[01:06:03.200 --> 01:06:09.920]   these important decisions and calling the shots, even if the models are good enough to
[01:06:09.920 --> 01:06:19.040]   actually run a successful business themselves. So yeah, to some extent, there might be choices
[01:06:19.040 --> 01:06:27.280]   there. And I think people will still have different interests and different ideas for
[01:06:27.280 --> 01:06:41.600]   what kind of interesting pursuits they want to direct their AIs at. AI doesn't necessarily have
[01:06:41.600 --> 01:06:52.880]   any kind of intrinsic desire of its own unless we put it in the system. So people can still end up
[01:06:52.880 --> 01:06:59.840]   being -- even if AIs become extremely capable, I would hope that people are still the drivers
[01:06:59.840 --> 01:07:07.040]   of what the AIs end up doing. Yeah. But I wonder if the economic equilibrium is so far from that,
[01:07:07.040 --> 01:07:14.880]   where you have the equivalent of Amdahl's law in a firm. The slowest part of the process is the one
[01:07:14.880 --> 01:07:19.600]   that's going to bottleneck you. And so the AI makes all the non-human parts of the firm 10x
[01:07:19.600 --> 01:07:28.720]   more efficient. It's still bottlenecked by that step. And so if one company decides to proceed
[01:07:28.720 --> 01:07:33.440]   by keeping humans in the loop on all the things that you really want human oversight on, then
[01:07:33.440 --> 01:07:36.800]   they'll just be out-competed by other companies. If one country decides to go this route, other
[01:07:36.800 --> 01:07:45.920]   countries will beat it. I wonder if this is a sustainable plan for keeping humans in the loop.
[01:07:46.480 --> 01:07:53.600]   Right. So I think if we wanted to keep humans in the loop, which seems reasonable,
[01:07:53.600 --> 01:08:00.800]   and it turned out that firms with any humans in the loop were out-competed by firms that didn't
[01:08:00.800 --> 01:08:08.080]   have any humans, then I think you would obviously need some kind of regulation that disallowed
[01:08:08.080 --> 01:08:13.280]   having no humans in the loop for running a whole company. But there's so many companies
[01:08:15.200 --> 01:08:22.240]   I guess in any country, let alone the world. I wonder if it's better to do the regulation
[01:08:22.240 --> 01:08:26.560]   on companies and say, "You've got to keep humans in the loop on important processes,
[01:08:26.560 --> 01:08:29.200]   but then you have to define what important processes are. You've got to monitor every
[01:08:29.200 --> 01:08:34.160]   single company. And you also got to get collaboration in every single country,
[01:08:34.160 --> 01:08:40.480]   which has firms in it." Versus if this is a problem, should it be solved before the model
[01:08:40.480 --> 01:08:46.240]   is even deployed, such that hopefully you would get into a situation where you did decide to build
[01:08:46.240 --> 01:08:52.400]   a firm end-to-end on these models. It basically does what you want it to do, and you don't need a
[01:08:52.400 --> 01:08:56.400]   human in the loop. Does that question make sense? I guess I'm just wondering in this situation,
[01:08:56.400 --> 01:09:00.400]   how do we actually monitor every single firm as a human in the loop? And what happens if
[01:09:00.400 --> 01:09:06.160]   China doesn't decide to do that and so forth? Right. Yeah, you would either have to have
[01:09:07.600 --> 01:09:15.120]   every country agree to this regulatory regime, or you would need all of the model infrastructure or
[01:09:15.120 --> 01:09:22.160]   the model providers to agree to this kind of requirement. So it's definitely going to be
[01:09:22.160 --> 01:09:31.760]   non-trivial. So I guess, yeah, this is looking at ways ahead. So it's a little hard to imagine
[01:09:33.600 --> 01:09:43.600]   this world before seeing anything like it. So for example, there's some questions like,
[01:09:43.600 --> 01:09:52.160]   are we actually confident that AI-run companies are better in every way? Or do we think they're
[01:09:52.160 --> 01:09:58.640]   better most of the time, but occasionally they malfunction because AIs are still less sample
[01:09:58.640 --> 01:10:06.400]   efficient in certain ways, like dealing with very wacky situations. So actually, AI-run firms have
[01:10:06.400 --> 01:10:11.440]   higher tail risk because they're more likely to malfunction in a big way. So I guess there might
[01:10:11.440 --> 01:10:19.200]   be some practical questions like that that would also determine how things play out. Maybe if you
[01:10:19.200 --> 01:10:25.920]   just require people to be accountable for various liability, this would also change the incentives a
[01:10:25.920 --> 01:10:32.960]   bit. So if it turned out that AIs are better at running everything, and they're also completely
[01:10:32.960 --> 01:10:43.440]   benevolent, and we've totally solved alignment, and they're better at being accountable to people
[01:10:43.440 --> 01:10:50.960]   than people are, then I would say maybe it's OK having the AIs run the firms. But I think that
[01:10:50.960 --> 01:10:55.680]   might be pretty far out. And I think we're more likely to be in a situation where they look better
[01:10:55.680 --> 01:11:04.560]   in the short term, but the AI-run entities still have some serious problems. And it's actually
[01:11:04.560 --> 01:11:07.840]   practical considerations that push you more towards having humans in the loop,
[01:11:07.840 --> 01:11:12.240]   at least for the near future. OK, so this is a problem we had to deal with today with RLHF,
[01:11:12.240 --> 01:11:16.640]   where you have to aggregate preferences across a lot of different humans.
[01:11:17.520 --> 01:11:23.840]   And it'll be maybe more marked with future, more powerful systems. But when you say, "Well,
[01:11:23.840 --> 01:11:28.480]   we want these eventual AI systems that are going to fully replace humans as part of these firms to
[01:11:28.480 --> 01:11:33.760]   be aligned," what does that mean? Will it mean that they basically do what the user wants them
[01:11:33.760 --> 01:11:41.120]   to do? Does it mean that they have to result in some sort of global outcome that we're happy with
[01:11:41.120 --> 01:11:46.800]   as the kind of people, the stakeholders in OpenAI? What concretely would that mean?
[01:11:46.800 --> 01:11:55.840]   If the models are being used for these higher stakes use cases, then we would have to think
[01:11:55.840 --> 01:12:03.520]   about RLHF in a much different way than we are right now. So I would say we're not quite ready
[01:12:03.520 --> 01:12:09.360]   for that, or the current methods might not be completely sufficient. But I would say
[01:12:09.360 --> 01:12:17.760]   we would need to make compromises between the needs of the different stakeholders involved.
[01:12:17.760 --> 01:12:26.640]   So we have this document that we're releasing called the Model Spec. And it's about how we
[01:12:26.640 --> 01:12:34.080]   want our models to behave in the API and in ChatGBT. And we try to talk about this issue,
[01:12:34.080 --> 01:12:38.240]   where there are different stakeholders involved, and sometimes there are conflicts between what
[01:12:38.240 --> 01:12:49.520]   they might want. In our case, we were thinking of the stakeholders as the end user. That means
[01:12:49.520 --> 01:12:57.040]   someone sitting in front of ChatGBT or some other app. The developer. So this is someone using the
[01:12:57.040 --> 01:13:07.120]   API who might be serving other end users with their app. The platform, which is OpenAI. We
[01:13:07.120 --> 01:13:14.960]   don't want the models to expose us to legal risk and so forth. And then the rest of humanity,
[01:13:15.680 --> 01:13:23.200]   including people who might not be users or customers or anything. So obviously,
[01:13:23.200 --> 01:13:32.080]   the user might ask the model to do something that we think is actively harmful to other people.
[01:13:32.080 --> 01:13:39.760]   And so we might have to refuse that. By the way, this isn't the order of priority necessarily.
[01:13:39.760 --> 01:13:45.760]   So we have these four or so classes of stakeholders. Actually, you could also
[01:13:45.760 --> 01:13:50.560]   say maybe in the future we'll say the model itself, though I would say we're not going there
[01:13:50.560 --> 01:13:58.800]   yet. But anyway, we have these different stakeholders. Sometimes they have conflicting
[01:13:58.800 --> 01:14:04.800]   demands, and we have to make some call on how to resolve those conflicts. And it's not always
[01:14:04.800 --> 01:14:12.880]   obvious how to do that. So I would say we had to think through. Yeah, we just had to think through
[01:14:12.880 --> 01:14:20.240]   the tradeoffs. And basically the rough heuristic is that we mostly want the models to follow your
[01:14:20.240 --> 01:14:28.160]   instructions and be helpful to the user and the developer. But when this impinges on other
[01:14:30.960 --> 01:14:37.040]   people's happiness or way of life, this becomes a problem. And we have to block certain kinds of
[01:14:37.040 --> 01:14:44.720]   usage. But we mostly want the models to just be an extension of people's will and do what
[01:14:44.720 --> 01:14:50.960]   they say. We don't want to be too paternalistic. We want to be kind of neutral and not impose
[01:14:50.960 --> 01:14:59.360]   our opinions on people. Yeah, we want to mostly let people do what they want with the models.
[01:15:00.240 --> 01:15:07.280]   I got a chance to read the spec beforehand, and it was, I guess, a question of how well that
[01:15:07.280 --> 01:15:14.800]   transfers over to how the model itself behaves. But I was impressed with how sensible the tradeoffs
[01:15:14.800 --> 01:15:20.720]   were. It made sense that this is the, I believe it was exclusively stated the actual edge cases
[01:15:20.720 --> 01:15:24.960]   rather than the kinds of things where everybody can, which are obvious. In this case, you really
[01:15:24.960 --> 01:15:29.120]   are going after the edge cases. Yeah, we wanted it to be very actionable so that it wasn't just
[01:15:29.120 --> 01:15:34.240]   a bunch of nice sounding principles. But it was like each example kind of tells you something
[01:15:34.240 --> 01:15:40.320]   about some non-obvious situation and reasons through that situation. Yeah. Okay. Now I have
[01:15:40.320 --> 01:15:47.200]   a couple of questions about the state of the research itself. So famously in the social
[01:15:47.200 --> 01:15:51.760]   sciences, things are really hard to replicate. And it's a question about how much of the science
[01:15:51.760 --> 01:16:00.160]   there is real versus these manufactured bespoke sorts of experiments. When you look at the average
[01:16:00.160 --> 01:16:06.800]   ML paper, does it feel like the, like a really solid piece of literature, or does it feel often
[01:16:06.800 --> 01:16:12.240]   like it's the equivalent of what P hacking is in the social sciences? Everyone has their complaints
[01:16:12.240 --> 01:16:20.000]   about the ML literature, but I would say overall, I think it's a relatively healthy field compared to
[01:16:20.000 --> 01:16:28.240]   some other ones like in the social sciences. Just because, well, it's grounded, it's largely
[01:16:28.240 --> 01:16:37.280]   grounded in practicality and getting things to work. And if you publish something that can't be
[01:16:37.280 --> 01:16:43.920]   replicated easily, then people will just forget about it. And it's like accepted that often you
[01:16:43.920 --> 01:16:49.280]   don't just report someone's number from their paper. You also try to re-implement their method
[01:16:49.280 --> 01:16:56.320]   and compare it to your method on the same training data set. So I think if you publish methods that
[01:16:56.320 --> 01:17:04.320]   are really hard to implement or are really finicky, they'll tend to get forgotten. And as a result,
[01:17:04.320 --> 01:17:16.880]   people actually try to open source their work a lot. I guess there's various unfavorable incentives
[01:17:16.880 --> 01:17:22.560]   like, yeah, people are incentivized to make the baseline methods, like the methods they're
[01:17:22.560 --> 01:17:29.760]   comparing to worse. And there are other mild pathologies, like trying to make your methods
[01:17:29.760 --> 01:17:36.400]   seem sophisticated mathematically. But I would say overall, I feel like the field makes progress.
[01:17:36.400 --> 01:17:42.880]   And I would probably like to see a little bit more science and trying to understand things
[01:17:42.880 --> 01:17:50.240]   rather than more like hill climbing on benchmarks and trying to propose new methods. And there's
[01:17:50.240 --> 01:17:56.880]   been a decent amount of that recently. But yeah, I think we could use more of that. And I think
[01:17:56.880 --> 01:18:04.400]   that's a good thing for academics to work on. Oh yeah, on the social sciences, on a slightly
[01:18:04.400 --> 01:18:13.120]   different note, I think actually I'd be really excited to see more research on using base models
[01:18:13.120 --> 01:18:20.640]   to do simulated social science. Because these models have a probabilistic model of the whole
[01:18:20.640 --> 01:18:31.840]   world. And you can set up a simulated questionnaire or a conversation. And you can look at how anything
[01:18:31.840 --> 01:18:37.200]   is correlated, like any traits that you might imagine, you can see how they might be correlated
[01:18:37.200 --> 01:18:42.880]   with other traits. So it'd be pretty cool to see if people could replicate some of the more notable
[01:18:42.880 --> 01:18:48.400]   results in the social science, like moral foundations and that sort of thing, by just
[01:18:48.400 --> 01:18:52.160]   prompting base models in different ways and seeing what's correlated.
[01:18:52.160 --> 01:18:58.560]   What is that Stanford experiment? The one where they ask conformity tests, right?
[01:18:58.560 --> 01:18:58.800]   Oh yeah.
[01:18:58.800 --> 01:19:01.840]   It'd be fun if that replicated with the language models as well.
[01:19:01.840 --> 01:19:08.240]   Very interesting. With the rest of the research that happens at big labs, how much of it is
[01:19:08.240 --> 01:19:16.480]   increasing or decreasing the amount of compute you need to get a certain result as an actual
[01:19:16.480 --> 01:19:22.480]   compute multiplier versus how much of it is things that are just making the learning more stable
[01:19:22.480 --> 01:19:26.640]   and just building out the infrastructure? I guess the broader question I'm trying to ask is,
[01:19:26.640 --> 01:19:30.320]   since GPT-4, does it feel like with the same amount of compute, you can train a much better
[01:19:30.320 --> 01:19:34.960]   model or does it feel like, oh, we've made sure that the learning can happen better and in a more
[01:19:34.960 --> 01:19:41.440]   scalable way with GPT-5, but it's not like we can train GPT-4 with like GPT-3.5 budget now or
[01:19:41.440 --> 01:19:47.040]   something like that. Yeah. Well, definitely there's always progress in improving the efficiency.
[01:19:47.040 --> 01:19:55.440]   Whenever you have a 1D performance metric, you're going to find that different improvements
[01:19:56.160 --> 01:20:04.240]   can kind of substitute for each other. You might find that post-training and
[01:20:04.240 --> 01:20:12.560]   pre-training both improve the metrics. They'll have a slightly different profile of which metrics
[01:20:12.560 --> 01:20:17.680]   they improve, but if at the end of the day, you have a single number, they're going to substitute
[01:20:17.680 --> 01:20:25.200]   for each other somewhat. I would say for something like a human evaluation, what do humans prefer,
[01:20:26.160 --> 01:20:30.800]   we've definitely made a lot of progress on both sides, pre-training and post-training
[01:20:30.800 --> 01:20:37.600]   and improving that. A couple of rapid fire questions about RLHF. Obviously, RLHF is
[01:20:37.600 --> 01:20:43.920]   important to make these models useful. Maybe the lobotomized description is inaccurate,
[01:20:43.920 --> 01:20:51.440]   but there is a sense in which all of these models, once they're put in a chatbot form,
[01:20:51.440 --> 01:20:55.840]   have a very similar way of speaking. They really want to delve into things. They want to turn
[01:20:55.840 --> 01:21:02.560]   things into bullet points. They often seem to have this formal and dull way of speaking.
[01:21:02.560 --> 01:21:08.720]   There's complaints that they're not as creative, like what we were talking about before. It can
[01:21:08.720 --> 01:21:13.760]   only do rhyming poetry and not rhyming until recently, I guess. Is that a result of the
[01:21:13.760 --> 01:21:19.040]   particular way in which RLHF happens now? If so, is it because of who the raters are? Is it because
[01:21:19.040 --> 01:21:23.760]   of what the loss function is? Why is this the way all chatbots look? I would say there's a decent
[01:21:23.760 --> 01:21:32.800]   amount of room for variation in exactly how you do the training process. I'd say we're actively
[01:21:32.800 --> 01:21:38.160]   trying to improve this and make the writing more lively and more fun. I think we've made some
[01:21:38.160 --> 01:21:45.920]   progress improving the personality of chatGBT, so it is more fun. It's better when you're
[01:21:46.720 --> 01:21:49.760]   trying to chit-chat with it and so forth. It's less robotic.
[01:21:49.760 --> 01:21:59.600]   I would say, yes, it's an interesting question how some of the ticks came about, like the word
[01:21:59.600 --> 01:22:05.920]   "delve." I've actually caught myself using the word a bit recently, so I don't know if it rubbed
[01:22:05.920 --> 01:22:14.240]   off on me from the model or what. Actually, I think there might be some funny effects going on
[01:22:14.240 --> 01:22:21.200]   where there's unintentional distillation happening between the language model providers where if you
[01:22:21.200 --> 01:22:28.640]   hire someone to go do a labeling task, they might just be feeding it into a model. They might just
[01:22:28.640 --> 01:22:34.720]   be pulling up their favorite chatbot and feeding it in and having the model do the task and then
[01:22:34.720 --> 01:22:41.840]   copying and pasting it back. That might account for some of the convergence. Also, I think some
[01:22:41.840 --> 01:22:46.720]   of the things we're seeing are just what people like. I mean, I think people do like bullet
[01:22:46.720 --> 01:22:53.360]   points. They like the structured responses. People do often like the big info dumps that they get
[01:22:53.360 --> 01:23:08.160]   from the models. It's not completely clear how much is just a quirk of the particular
[01:23:09.680 --> 01:23:17.920]   choices and design of the post-training processes and how much is actually intrinsic to
[01:23:17.920 --> 01:23:25.440]   what people actually want. It does seem persistently more verbose than some people
[01:23:25.440 --> 01:23:32.080]   want. Maybe just because during the labeling stage, the raters will prefer the more verbose
[01:23:32.080 --> 01:23:39.680]   answer. But I wonder if it's inherent because of how it's pre-trained. The stop sequence doesn't
[01:23:39.680 --> 01:23:44.080]   come up that often and it really wants to just keep going. There might be some biases in the
[01:23:44.080 --> 01:23:50.560]   labeling that lead to verbosity, like the fact that we tend to train for one message at a time
[01:23:50.560 --> 01:23:58.000]   rather than the full interaction. So if you only see one message, then there's something that just
[01:23:58.000 --> 01:24:03.040]   has a clarifying question or maybe a short response with an invitation to follow up.
[01:24:03.040 --> 01:24:09.760]   It's going to look less complete than something that covers all possibilities. There's also
[01:24:09.760 --> 01:24:16.480]   a question of whether people's preferences would change depending on how fast the model is
[01:24:16.480 --> 01:24:24.080]   streaming its output. Clearly, if you're sitting there waiting for the tokens to come out,
[01:24:24.080 --> 01:24:29.600]   you're going to prefer that it gets to the point. But if it just gives you a dump of text
[01:24:29.600 --> 01:24:34.080]   instantly, maybe you don't actually care if there's a bunch of boilerplate or if there's
[01:24:34.080 --> 01:24:36.640]   a bunch of stuff you're going to skim, you'd rather just have it all there.
[01:24:36.640 --> 01:24:44.480]   The reward model is, I think, such an interesting artifact because it's the closest thing we have
[01:24:44.480 --> 01:24:52.800]   to an aggregation of what people want, what preferences they have. When you think about
[01:24:52.800 --> 01:25:04.400]   models that are much smarter, one hope would be that you could just give a list of things we want
[01:25:04.400 --> 01:25:12.320]   that are not trivial and obvious, like a declaration of rights things. On the other
[01:25:12.320 --> 01:25:17.520]   hand, I think I heard you make the point that a lot of our preferences and values are very subtle
[01:25:18.160 --> 01:25:22.960]   and so that they might be best represented through these pairwise preferences.
[01:25:22.960 --> 01:25:28.560]   When you think of a GPT-6 or GPT-7 level model, are we giving it more of a written
[01:25:28.560 --> 01:25:33.840]   instructions or are we still doing these sorts of subliminal preferences?
[01:25:33.840 --> 01:25:40.720]   Yeah, that's a good question. I think these preference models do learn a lot of subtleties
[01:25:43.520 --> 01:25:50.720]   about what people prefer that would be hard to articulate in an instruction manual.
[01:25:50.720 --> 01:26:00.800]   Obviously, you can write an instruction manual that has lots of examples of comparisons,
[01:26:00.800 --> 01:26:06.400]   and that's what the model spec has. It has a lot of examples with some explanation.
[01:26:10.800 --> 01:26:17.520]   It's not clear what the optimal format is for describing preferences. I would guess that
[01:26:17.520 --> 01:26:22.320]   whatever you can get out of a big data set that captures fuzzy preferences,
[01:26:22.320 --> 01:26:28.960]   you can distill it down to a shorter document that mostly captures the ideas.
[01:26:31.200 --> 01:26:41.200]   I would think that the bigger models do learn a lot of these concepts automatically of what
[01:26:41.200 --> 01:26:50.480]   people might find. They'll just learn from all the pre-training data what people would find useful
[01:26:50.480 --> 01:27:02.240]   and helpful. There'll be some complex moral theories that they have, but of course, there's
[01:27:02.240 --> 01:27:12.800]   still a lot of room to latch on to a different style or a different morality. If we were to
[01:27:15.280 --> 01:27:24.160]   align these models, what we're doing is latching on to a specific style, a specific morality,
[01:27:24.160 --> 01:27:31.440]   and you still need a decently long document to capture exactly what you want.
[01:27:31.440 --> 01:27:34.960]   How much of a mode is better post-training?
[01:27:34.960 --> 01:27:40.240]   Currently, companies distinguish themselves by how big is their model and so forth.
[01:27:40.240 --> 01:27:44.080]   Will it be a big mode who has figured out all the finickiness that you were talking
[01:27:44.080 --> 01:27:46.800]   about earlier with regards to all this data?
[01:27:46.800 --> 01:27:56.400]   I think there's something of a mode because it's just a very complex operation. You have to have
[01:27:56.400 --> 01:28:04.560]   a lot of skilled people doing it, so there's a lot of tacit knowledge and there's a lot of
[01:28:04.560 --> 01:28:14.960]   organizational knowledge that's required. I think post-training, to create a model that actually
[01:28:14.960 --> 01:28:25.280]   has all the functionality people care about requires a pretty complicated effort.
[01:28:28.800 --> 01:28:36.800]   This is basically an accumulation of a lot of R&D. I would say that makes it somewhat of a
[01:28:36.800 --> 01:28:45.680]   mode. It's not trivial to spin this up immediately. It does seem like the same
[01:28:45.680 --> 01:28:49.440]   companies that are putting together the most serious pre-training efforts are also putting
[01:28:49.440 --> 01:29:01.360]   together the serious post-training efforts. It seems like it is somewhat possible to copy or to
[01:29:01.360 --> 01:29:08.800]   spin up more of these efforts. One force that makes it less of a mode is that you can
[01:29:08.800 --> 01:29:16.000]   distill the models or you can take someone else's model and clone the outputs, or you can use
[01:29:16.000 --> 01:29:24.880]   someone else's model as a judge to do comparisons. I think the more big league people probably aren't
[01:29:24.880 --> 01:29:31.520]   doing that because it goes against terms of service policies. It would also be a hit to
[01:29:31.520 --> 01:29:35.600]   their pride, but I would expect some of the smaller players are doing that to get off the ground.
[01:29:35.600 --> 01:29:40.640]   That catches you up to a large extent. I guess that helps you clear the mode.
[01:29:40.640 --> 01:29:44.880]   What is the median raider like? Where are they based? What are their politics?
[01:29:44.880 --> 01:29:54.080]   What is their knowledge level? I would say it varies a lot. We've definitely hired raiders
[01:29:54.080 --> 01:30:05.200]   with different skills for different kinds of tasks or projects. I would say a decent
[01:30:06.960 --> 01:30:12.160]   mental model is just look at people who are on Upwork and other platforms like that. Who's doing
[01:30:12.160 --> 01:30:22.320]   sort of odd jobs with remote work? It's a pretty international group. There's a decent
[01:30:22.320 --> 01:30:31.040]   number of people in the US. We hire different groups of people for different types of
[01:30:31.040 --> 01:30:39.600]   labeling, whether we're more focused on writing or STEM tasks. People doing STEM tasks are more
[01:30:39.600 --> 01:30:49.840]   likely to be in India or other lower-middle-income countries, whereas people doing more English
[01:30:49.840 --> 01:30:59.920]   writing and composition tend more to be US-based. I'd say there have been times when we needed to
[01:31:00.560 --> 01:31:08.320]   hire different experts for some of our campaigns. Some of them are very talented. We even find that
[01:31:08.320 --> 01:31:14.080]   they're at least as good as us, the researchers, at doing these tasks. They're much more careful
[01:31:14.080 --> 01:31:22.080]   than us. I would say the people we have now are quite skilled and conscientious.
[01:31:24.480 --> 01:31:30.400]   With regards to the plateau narrative, one of the things I've heard is that a lot of the
[01:31:30.400 --> 01:31:36.720]   abilities these models have to help you with specific things is related to having very
[01:31:36.720 --> 01:31:44.880]   closely matched labels within the super-wise fine-tuning dataset. Is that true? If it can
[01:31:44.880 --> 01:31:52.000]   teach me how to use FFmpeg correctly, there's somebody who's figuring out, seeing the inputs,
[01:31:52.000 --> 01:31:56.480]   and seeing what flags you need to add. Some human is figuring that out and smashing to that.
[01:31:56.480 --> 01:32:03.760]   Do you need to hire all these label-rollers who have domain expertise in all these different
[01:32:03.760 --> 01:32:07.840]   domains? If that's the case, it seems like it would be a much bigger slog to get these models
[01:32:07.840 --> 01:32:11.920]   to be smarter and smarter over time. Right. You don't exactly need that
[01:32:12.720 --> 01:32:22.880]   because you can get quite a bit out of generalization. The base model has already
[01:32:22.880 --> 01:32:29.360]   been trained on tons of documentation, tons of code with shell scripts and so forth. It's already
[01:32:29.360 --> 01:32:40.320]   seen all the FFmpeg man pages and lots of bash scripts and everything. Even just giving the
[01:32:40.320 --> 01:32:46.880]   base model a good fuchsia prompt, you can get it to answer queries like this. Just training a
[01:32:46.880 --> 01:32:56.960]   preference model for helpfulness will, even if you don't train it on any stem, it'll somewhat
[01:32:56.960 --> 01:33:08.400]   generalize to stem. Not only do you not need examples of how to use FFmpeg, you might not
[01:33:08.400 --> 01:33:14.800]   even need anything with programming to get some reasonable behavior in the programming domain.
[01:33:14.800 --> 01:33:20.960]   Maybe final question is, we've touched on this in different ways, but to put it together,
[01:33:20.960 --> 01:33:25.520]   so you say you're turning on much more multimodal data. Presumably, these things
[01:33:25.520 --> 01:33:30.080]   understand what screens look like and will be able to interact with it in a much more
[01:33:30.080 --> 01:33:35.760]   coherent way. Also, you're going to do this long-horizon RL, so they'll be able to
[01:33:35.760 --> 01:33:40.720]   act as agents and assistants who can be part of your workflow in a much more integrated way.
[01:33:40.720 --> 01:33:48.880]   What do you expect that to look like and what will be the next steps from there? Suppose by
[01:33:48.880 --> 01:33:52.640]   the end of the year or next year, you have something that's like an assistant who can
[01:33:52.640 --> 01:33:56.560]   work with you on your screen. Does that seem like, first of all, a sensible thing to expect?
[01:33:56.560 --> 01:34:04.320]   And then where does it go from there? I would expect things to move in that direction.
[01:34:05.120 --> 01:34:11.680]   It's unclear what's going to be the best form factor, whether it's a clippy that's on your
[01:34:11.680 --> 01:34:17.680]   computer and helping you with something, or if it's more like a helpful colleague in the cloud.
[01:34:17.680 --> 01:34:24.320]   So we'll see which kinds of form factors work the best. And I would expect people to try all
[01:34:24.320 --> 01:34:38.240]   of them out. I would expect the mental model of a helpful assistant or helpful colleague to become
[01:34:38.240 --> 01:34:45.680]   more real, where you can share more of your everyday work. Instead of just giving it one-off
[01:34:45.680 --> 01:34:49.520]   queries, you would have a whole project that you're doing and it knows about everything you've
[01:34:49.520 --> 01:34:59.360]   done on that project so far. It can even proactively make suggestions. Maybe you can
[01:34:59.360 --> 01:35:04.800]   tell it, "Oh yeah, remember to ask me about this and if I've made any progress on it."
[01:35:04.800 --> 01:35:10.000]   So I think proactivity is one thing that's been missing. Yeah, I'd really love to see
[01:35:16.000 --> 01:35:21.440]   moving away from sort of one-off queries, using the model kind of like a search engine,
[01:35:21.440 --> 01:35:27.840]   a smarter search engine, and more towards having a whole project that I'm doing in collaboration
[01:35:27.840 --> 01:35:35.440]   with the model. And it knows everything I've done. It's proactively suggesting things for me to try,
[01:35:35.440 --> 01:35:40.400]   or it's going and doing work in the background. Yeah, that's really interesting. By the way,
[01:35:40.400 --> 01:35:46.800]   so final question, what is your median timeline? It replaces your job. Yeah, it replaces my job.
[01:35:46.800 --> 01:35:56.560]   Maybe like five years. Yeah, pretty soon. Interesting. Okay, well, John, this is super
[01:35:56.560 --> 01:36:03.440]   interesting. Thanks so much for making the time. I think this seems like one of the parts of the AI
[01:36:03.440 --> 01:36:08.400]   process that are super important and people don't understand that much about. So it was super
[01:36:08.400 --> 01:36:13.600]   interesting to delve into it and get your thoughts on it. But yeah, thanks for having me on the
[01:36:13.600 --> 01:36:19.440]   podcast. It was fun to talk about all this stuff. Hey, everybody. I hope you enjoyed that episode
[01:36:19.440 --> 01:36:24.480]   with John. He's just a very thoughtful guy and it's super interesting to learn about the way
[01:36:24.480 --> 01:36:28.800]   in which these models become the kind of shotgun that they are. Anyways, as you can see, I'm now
[01:36:28.800 --> 01:36:32.960]   doing ads on the podcast. So if you'd like to advertise, you can reach out at the link in the
[01:36:32.960 --> 01:36:37.440]   description. And of course, if you enjoyed the episode, it's really helpful if you can share it
[01:36:37.440 --> 01:36:41.520]   with other people who you think might enjoy it, your friends, group chats, Twitter, whatever else.
[01:36:41.520 --> 01:36:53.920]   See you on the next one. Cheers.

