
[00:00:00.000 --> 00:00:06.160]   I am your host, Charles Fry.
[00:00:06.160 --> 00:00:10.920]   And I will be telling you today about how easy it is to instrument
[00:00:10.920 --> 00:00:12.800]   weights and biases with PyTorch.
[00:00:12.800 --> 00:00:17.000]   So this is now, once we've now got to the training logic, now this is where
[00:00:17.000 --> 00:00:19.400]   weights and biases really starts to come into play.
[00:00:19.400 --> 00:00:22.440]   The two functions are watch and log.
[00:00:22.440 --> 00:00:26.720]   So watch is a special function that's especially useful with PyTorch models.
[00:00:26.720 --> 00:00:30.120]   It will log the gradients in the parameters of your model, every
[00:00:30.120 --> 00:00:31.960]   certain number of steps of training.
[00:00:31.960 --> 00:00:34.560]   So you can set that number there, how often you want to log
[00:00:34.560 --> 00:00:35.760]   the gradients in the models.
[00:00:35.760 --> 00:00:39.160]   I tend to use something like 10, a hundred, something like that.
[00:00:39.160 --> 00:00:41.640]   If it's every epoch, I might log it every epoch.
[00:00:41.640 --> 00:00:44.960]   So that's this first line here at the beginning of our train function.
[00:00:44.960 --> 00:00:47.680]   I say, Hey, yo, we're about to start training this model.
[00:00:47.680 --> 00:00:49.600]   1B, watch that model.
[00:00:49.600 --> 00:00:51.960]   This section here then runs training.
[00:00:51.960 --> 00:00:54.440]   And down here, I've got this function train batch.
[00:00:54.680 --> 00:00:57.240]   That's what really defines the actual training logic.
[00:00:57.240 --> 00:00:59.080]   All this stuff around here is just okay.
[00:00:59.080 --> 00:01:01.360]   I'm counting examples and batches.
[00:01:01.360 --> 00:01:03.420]   I'm loading the images and labels.
[00:01:03.420 --> 00:01:06.560]   I'm using TQDM to give me a nice progress bar.
[00:01:06.560 --> 00:01:10.200]   So the two important pieces are train batch and train log.
[00:01:10.200 --> 00:01:12.960]   So train batch is the part where we actually do the training.
[00:01:12.960 --> 00:01:14.480]   This is standard PyTorch stuff.
[00:01:14.480 --> 00:01:19.320]   Put my images and my labels on my device, run a forward pass that
[00:01:19.320 --> 00:01:20.800]   gets me the outputs and the loss.
[00:01:20.960 --> 00:01:24.720]   Run a backwards pass and then take a step with my optimizer.
[00:01:24.720 --> 00:01:27.560]   So improve the performance of my model at the end and then
[00:01:27.560 --> 00:01:29.560]   return the loss for later use.
[00:01:29.560 --> 00:01:31.520]   So we've got this train batch function.
[00:01:31.520 --> 00:01:32.720]   That's all quite standard.
[00:01:32.720 --> 00:01:36.800]   The part that differs is the part where we log what's going on in training.
[00:01:36.800 --> 00:01:39.680]   And the code that I wrote before I discovered weights and biases.
[00:01:39.680 --> 00:01:43.080]   This would be something like a print statement, maybe, or maybe saving
[00:01:43.080 --> 00:01:46.200]   something to a file locally would be this train log function.
[00:01:46.200 --> 00:01:48.400]   So this is where the difference comes in.
[00:01:48.400 --> 00:01:50.400]   This 1B.log call.
[00:01:51.040 --> 00:01:55.600]   So what this does is says, Hey, W and B here is the information that I want to
[00:01:55.600 --> 00:01:57.760]   associate with this step of training.
[00:01:57.760 --> 00:01:59.560]   First, you should give a step number.
[00:01:59.560 --> 00:01:59.760]   Okay.
[00:01:59.760 --> 00:02:01.520]   What step of training is this?
[00:02:01.520 --> 00:02:04.120]   And this should be constantly going up as you're going.
[00:02:04.120 --> 00:02:06.360]   So it doesn't have to go up by one every time.
[00:02:06.360 --> 00:02:10.200]   So for example, here, I'm using the number of images that the neural
[00:02:10.200 --> 00:02:11.920]   network has seen as my step.
[00:02:11.920 --> 00:02:17.480]   The other thing to provide is this dictionary here, a dictionary that says,
[00:02:17.520 --> 00:02:21.360]   Oh, okay, here's what happened on this step of training, the epoch was
[00:02:21.360 --> 00:02:23.600]   this number and the loss was this.
[00:02:23.600 --> 00:02:26.920]   This dictionary here, you can put lots of things in it, much more complicated
[00:02:26.920 --> 00:02:31.960]   things than just numbers, using the various 1B media types, images, audio,
[00:02:31.960 --> 00:02:36.160]   video, 3D objects, things like that can also be logged here, but to keep
[00:02:36.160 --> 00:02:40.640]   this example simple, I'll just stick with these sort of simple scalar values here.
[00:02:40.640 --> 00:02:42.800]   So that's the main difference here.
[00:02:42.800 --> 00:02:47.000]   All of this code up here, this is stuff apart from this 1B.watch here.
[00:02:47.000 --> 00:02:48.880]   All of this stuff is standard.
[00:02:48.880 --> 00:02:53.080]   So really we've only added this line here and this line here.
[00:02:53.080 --> 00:02:55.600]   So now that, so we've defined the three components of our
[00:02:55.600 --> 00:02:57.440]   pipeline, make, train and test.
[00:02:57.440 --> 00:03:00.920]   We built our pipeline and we've instrumented it with W and B.
[00:03:00.920 --> 00:03:02.840]   We've added those four or five lines.
[00:03:02.840 --> 00:03:05.360]   We're ready to run our fully tracked experiment.
[00:03:05.360 --> 00:03:09.840]   I'm going to run this cell here, and then we're going to see what comes out.
[00:03:09.840 --> 00:03:12.960]   When you start a run with weights and biases, we give you a couple of links
[00:03:12.960 --> 00:03:16.440]   to a project page, which is where you can group together a bunch of
[00:03:16.440 --> 00:03:17.840]   runs that are all in the same project.
[00:03:17.840 --> 00:03:23.320]   And then to a run page, which is a specific page for this particular run.
[00:03:23.320 --> 00:03:26.440]   It's going to log information about that run here.
[00:03:26.440 --> 00:03:29.960]   Let's go and check that out and let's see what we logged.
[00:03:29.960 --> 00:03:30.320]   All right.
[00:03:30.320 --> 00:03:34.400]   What this thing is here, you might hear this called the workspace or the dashboard.
[00:03:34.400 --> 00:03:37.880]   This is where you can interact with all of your runs, the information
[00:03:37.880 --> 00:03:40.040]   that was logged from each run.
[00:03:40.040 --> 00:03:44.840]   But what we've got here, because we use W and B.watch, we've got the values for
[00:03:44.840 --> 00:03:48.920]   all of our weights, biases, and the values for the gradients over time.
[00:03:48.920 --> 00:03:53.440]   So you can see some nice things like the gradients aren't exploding or vanishing.
[00:03:53.440 --> 00:03:55.200]   The gradients are decreasing over time.
[00:03:55.200 --> 00:03:57.160]   That's a good sign for optimization.
[00:03:57.160 --> 00:03:59.240]   This is very useful for debugging.
[00:03:59.240 --> 00:04:03.560]   It's presented as a histogram at any given point on the X-axis.
[00:04:03.560 --> 00:04:04.880]   There is a histogram.
[00:04:04.880 --> 00:04:08.640]   And then you can highlight an individual histogram to see what that individual
[00:04:08.640 --> 00:04:10.480]   histogram looks like in greater detail.
[00:04:10.480 --> 00:04:13.680]   You can see a little kernel density estimate on top of it.
[00:04:13.680 --> 00:04:16.240]   If you prefer a kernel density estimate to a histogram.
[00:04:16.240 --> 00:04:19.200]   This also includes the things that we logged ourselves, the things that
[00:04:19.200 --> 00:04:21.440]   we logged by hand with W and B.log.
[00:04:21.440 --> 00:04:23.080]   So right here, we've got the loss.
[00:04:23.080 --> 00:04:24.520]   It's going down over time.
[00:04:24.520 --> 00:04:25.240]   That's good.
[00:04:25.240 --> 00:04:27.080]   There's a, there's a good amount of noise here.
[00:04:27.080 --> 00:04:29.640]   Cause I think this is just the loss on an individual batch.
[00:04:29.640 --> 00:04:31.680]   So that can be, that can be quite noisy.
[00:04:31.680 --> 00:04:33.200]   It's only a hundred or so examples.
[00:04:33.200 --> 00:04:34.880]   It's noisy, but it's going down.
[00:04:34.880 --> 00:04:38.600]   If you want to get rid of some of the noise in one of your, in your charts,
[00:04:38.600 --> 00:04:43.560]   if you really actually care more about the trend over time, you can use this
[00:04:43.560 --> 00:04:47.840]   smoothing operation, so that looks like about the right amount of smoothing
[00:04:47.840 --> 00:04:51.160]   capture the fact that we're really, you know, we're going down here and you
[00:04:51.160 --> 00:04:54.760]   can always include or not include the original in your chart when you're
[00:04:54.760 --> 00:04:57.000]   showing that showing these smooth versions here.
[00:04:57.000 --> 00:05:17.600]   [Music]

