
[00:00:00.000 --> 00:00:04.000]   Today I have the pleasure of interviewing Dr. David Friedman, who is a famous and
[00:00:04.000 --> 00:00:09.520]   macrocapitalist economist, and most recently the author of Legal Systems Very Different from Ours.
[00:00:09.520 --> 00:00:15.200]   So my first question is this, is the dating market efficient in the sense that it doesn't
[00:00:15.200 --> 00:00:20.240]   maximize net value? It's not a competitive market because I'm not substitutable for
[00:00:20.240 --> 00:00:26.160]   another bearded 20-year-old Indian man. So I'm curious how efficient you think the dating market
[00:00:26.160 --> 00:00:34.960]   is. Oh boy, I've been puzzled for quite a while on why online dating doesn't work better than it
[00:00:34.960 --> 00:00:45.760]   does. That in principle, a lot of dating is a sorting problem. That not only are you not identical
[00:00:45.760 --> 00:00:55.840]   to another bearded Indian man at the same Indian or ethnic man at the same age, your potential dates
[00:00:55.840 --> 00:01:01.200]   have different tastes. So that it's really, I actually discussed some of this. I've got a
[00:01:01.200 --> 00:01:06.800]   chapter in my price theory text, which is really on what I think of as the marriage market, although
[00:01:06.800 --> 00:01:14.080]   it applies to dating more generally. And I have one model, which is the really simple economic
[00:01:14.080 --> 00:01:22.080]   model in which in effect, everybody is identical. And you have a market in which there's an implicit
[00:01:22.080 --> 00:01:27.520]   price in terms of the terms of marriage, how much the husband agrees to do, to what extent the wife
[00:01:27.520 --> 00:01:36.400]   gets to make decisions, things like that. And you then allocate people, you then have an equilibrium
[00:01:36.400 --> 00:01:42.720]   price. And the real point of that section is that I'm arguing that the effect of polygyny or
[00:01:42.720 --> 00:01:48.960]   polyandry is the opposite of what people expect. That if you legalize polygyny, you allow it,
[00:01:48.960 --> 00:01:55.120]   make it legal for a man to have multiple wives. The result is to benefit women and maybe to harm
[00:01:55.120 --> 00:02:01.280]   men. Because if men can have multiple wives, that's going to bid up the price of a wife.
[00:02:01.280 --> 00:02:05.840]   It means that some men can in effect go on the market and try to bid for two wives instead of
[00:02:05.840 --> 00:02:13.200]   one. And that then means that the ordinary monogamous man still ends up getting a wife,
[00:02:13.200 --> 00:02:21.760]   but on worse terms, he's got to do more of the dishes. And women either get all of a husband
[00:02:21.760 --> 00:02:28.480]   on better terms, or half of a husband on terms good enough so that they prefer half of that
[00:02:28.480 --> 00:02:35.600]   husband to all of another. And that's sort of a fun argument. And of course, if you legalize
[00:02:35.600 --> 00:02:40.000]   polyandry, you allowed a woman to marry multiple husbands, you have the same effect in reverse.
[00:02:40.000 --> 00:02:45.760]   So that's the easy model. And then the harder model is the sorting model. And even then,
[00:02:45.760 --> 00:02:50.800]   I only have a simple sorting model. My sorting model is one in which everybody's value as a
[00:02:50.800 --> 00:02:59.840]   spouse varies, but it's the same to all potential partners. And the real one is one in which not
[00:02:59.840 --> 00:03:06.800]   only is your value as a spouse different from somebody else's, your value as a spouse is
[00:03:06.800 --> 00:03:12.320]   different to some people than to someone else's. The way I like to put it is that the woman who
[00:03:12.320 --> 00:03:18.240]   I eventually concluded was about a one in 10,000 catch was not in fact being pursued by anybody
[00:03:18.240 --> 00:03:24.640]   else at the time, because other people had weird tastes and did not recognize what a desirable
[00:03:24.640 --> 00:03:29.600]   partner this was from my standpoint. I should say we're still married. That was 38 years ago or so
[00:03:29.600 --> 00:03:37.120]   that we got married. So that's a really hard problem. And I'm not quite sure. The problem
[00:03:37.120 --> 00:03:42.960]   is that a lot of economics ignore search costs, because we don't have a very good way of handling
[00:03:42.960 --> 00:03:49.920]   them. And yet search costs are really central to the dating problem. And I would have thought
[00:03:49.920 --> 00:03:56.080]   that things would get a whole lot better with computer dating, because you can take all of
[00:03:56.080 --> 00:04:01.920]   the characteristics that are sort of objective and measurable, tell them to the computer,
[00:04:01.920 --> 00:04:10.480]   have each person search a million profiles, find the subset of that who meet the easy
[00:04:10.480 --> 00:04:14.960]   characteristics, and then interact with each other to find how well they do. But as far as
[00:04:14.960 --> 00:04:21.840]   I can tell, it hasn't happened. I don't think that marriage has worked better since computers came in.
[00:04:22.800 --> 00:04:27.840]   The people I know who've been trying to use computer dating haven't found it very successful
[00:04:27.840 --> 00:04:35.760]   and so forth. And in fact, the story I'm told is that OKCupid, which is one of the most successful
[00:04:35.760 --> 00:04:41.440]   of these things, has gotten worse over time. It was originally designed by enthusiasts who did
[00:04:41.440 --> 00:04:47.840]   clever things and then taken over by businessmen who did less clever things. Now, maybe the people
[00:04:47.840 --> 00:04:53.120]   I've talked to are just sufficiently different from the mass of the market, so that what works
[00:04:53.120 --> 00:04:59.280]   for other people doesn't work for them. But that's my impression. So is it efficient? There's
[00:04:59.280 --> 00:05:04.400]   efficiency. You've got to compare it to some alternative. It's certainly inefficient compared
[00:05:04.400 --> 00:05:10.720]   to the ideal of a world where a perfectly wise, benevolent person paired us all up,
[00:05:11.520 --> 00:05:18.640]   but we don't have any of those people around. And I guess the interesting question is does
[00:05:18.640 --> 00:05:22.800]   it work better than the system where parents choose your spouse for you?
[00:05:22.800 --> 00:05:28.400]   Quite a long time ago, one of the most interesting conversations I've had,
[00:05:28.400 --> 00:05:34.720]   I was in Bombay waiting for a plane to Sydney. And I got into a conversation with a woman from
[00:05:34.720 --> 00:05:38.880]   southern India. And we ended up sitting next to each other and talking a good deal the way to
[00:05:38.880 --> 00:05:47.520]   Sydney. And she was flying out to join her husband, who was a physician. She was from southern India.
[00:05:47.520 --> 00:05:55.440]   Her husband had been chosen for her by her parents. She would have had a veto. It required
[00:05:55.440 --> 00:06:01.520]   her consent, but the actual search was done by her parents, not by her. She was still happily
[00:06:01.520 --> 00:06:06.800]   married. My first marriage had broken up. So the evidence from our little tiny sample was in favor
[00:06:06.800 --> 00:06:12.880]   of her system. But what was really fun about it was that here was a intelligent, modern person,
[00:06:12.880 --> 00:06:17.840]   a contemporary, an educated contemporary from an entirely different society in terms of that set
[00:06:17.840 --> 00:06:23.360]   of institutions. And it's a lot harder to say, well, obviously the way we do it is right when
[00:06:23.360 --> 00:06:28.880]   you've actually got someone real there defending the alternative. So I really don't know whether
[00:06:28.880 --> 00:06:35.920]   the modern American system, modern Western system of courtship works better or worse
[00:06:35.920 --> 00:06:42.560]   than the traditional system where the parents find a potential partner, then the partners get
[00:06:42.560 --> 00:06:47.760]   to decide whether they approve of each other or not. Because there is the argument that there's
[00:06:47.760 --> 00:06:52.400]   hardly anything farther from the economist model of rational choice than a man in love.
[00:06:55.280 --> 00:07:01.200]   Interesting. I wonder if to the extent that, you know, arranged marriages sometimes work
[00:07:01.200 --> 00:07:06.400]   better than non-arranged marriages. I wonder if that's because of just a social structure where
[00:07:06.400 --> 00:07:12.320]   that maybe makes you happier while you're in marriage versus whether your parents are better
[00:07:12.320 --> 00:07:17.920]   choosing a mate for you. Huh? What do you mean? You mean it makes you happy because you don't
[00:07:17.920 --> 00:07:21.920]   feel as though it's your fault that if you've messed up that you should have done better?
[00:07:21.920 --> 00:07:26.800]   Or maybe those cultures are just better at making wives and husbands more, you know,
[00:07:26.800 --> 00:07:31.280]   stay together and feel happy about it. In a sense, what you'd really want to look at
[00:07:31.280 --> 00:07:36.400]   and what exists, though I don't want to share their data on it, would be people from India,
[00:07:36.400 --> 00:07:39.920]   in particular, I've monitored through all of India, but at least Southern India,
[00:07:39.920 --> 00:07:47.120]   who live in the US. My impression is it's not all that uncommon for somebody, you know,
[00:07:47.120 --> 00:07:56.960]   a young adult, maybe in the tech industry, to migrate to the US, then go back to India to marry
[00:07:56.960 --> 00:08:03.120]   the wife that his parents have chosen for him, or maybe in some cases that arrangements are made
[00:08:03.120 --> 00:08:09.520]   here, and then live in America. So that's as close, in a sense, as you can come to a controlled
[00:08:09.520 --> 00:08:18.320]   experiment of separating out quality of choice versus the degree to which the culture supports
[00:08:18.320 --> 00:08:23.200]   your marriage, so to speak, and which would be interesting. Now, the other way you could do it,
[00:08:23.200 --> 00:08:28.800]   but it would be much harder, is that what you would like the computer dating people to do,
[00:08:28.800 --> 00:08:34.720]   but I don't think they've really done, is really serious research on what observable characteristics
[00:08:34.720 --> 00:08:40.720]   match with what, that you'd like, and in principle you could do it, what you'd like to say, all right,
[00:08:40.720 --> 00:08:45.600]   here are all the things somebody tells us about himself, because that's really all you've got in
[00:08:45.600 --> 00:08:52.960]   practice, let's take all pairs who ended up married, or maybe all pairs who dated, who made
[00:08:52.960 --> 00:09:00.000]   the first try of having a date, and how do you figure out from the characteristics of the two
[00:09:00.000 --> 00:09:05.120]   parties whether they'll have a second date, and you would think they'd want to do that, and then
[00:09:05.120 --> 00:09:11.600]   ideally you follow that through and you say which ones got married, which marriages lasted, so
[00:09:11.600 --> 00:09:18.720]   ideally you could actually get sort of statistical data saying here's how to pair people up, and if
[00:09:18.720 --> 00:09:25.520]   you had that, you could then look at couples who were produced by American courtship institutions,
[00:09:25.520 --> 00:09:31.040]   and couples who were paired up by their parents, and see which one came closer to matching what the
[00:09:31.040 --> 00:09:34.960]   computer told you you ought to get together, that would be a really neat project, but it'll be very
[00:09:34.960 --> 00:09:39.360]   hard to do. Yeah, that would be interesting, yeah, in hidden order when you talk about efficient
[00:09:39.360 --> 00:09:45.200]   markets, you explain it in terms of could a bureaucrat god improve on the market by like
[00:09:45.200 --> 00:09:49.920]   changing the allocation of production or quantity produced, and in this case it's funny, in a sense
[00:09:49.920 --> 00:09:56.480]   the parents are that bureaucratic god. Except the parents in those societies, the parents have
[00:09:56.480 --> 00:10:02.960]   almost enough authority, that is to say they could be rejected, but usually aren't, but they aren't
[00:10:02.960 --> 00:10:11.360]   perfectly wise, but yes, no, no, the bureaucrat god basically is my attempt to make sense of what
[00:10:11.360 --> 00:10:17.600]   economists mean by economically efficient, because there are two issues with regard to economically
[00:10:17.600 --> 00:10:23.760]   efficient. One of them is what's an improvement, and there, as you know, I go with Alfred Marshall's
[00:10:23.760 --> 00:10:29.120]   view of it rather than the more conventional Pareto, Hicks, Kaldor, for anybody who knows
[00:10:29.120 --> 00:10:34.240]   these terms in economics approach, which I regard as a way of pretending not to make interpersonal
[00:10:34.240 --> 00:10:39.600]   comparisons when you really are doing it, but once you've got your way of comparing,
[00:10:39.600 --> 00:10:46.400]   then when you say this is efficient, efficient compared to what? And as far as I can tell,
[00:10:46.400 --> 00:10:52.080]   the closest I can come is to say, what's the best outcome you could imagine by this criterion,
[00:10:52.080 --> 00:10:57.920]   and that's my bureaucrat god, that's if you imagine a omniscient, omnipotent, benevolent
[00:10:57.920 --> 00:11:02.240]   actor who is controlling everything, and then I say, well of course nothing will come up,
[00:11:02.240 --> 00:11:08.720]   you will look up to that, but some things will come close, that in particular the standard sort
[00:11:08.720 --> 00:11:17.280]   of price theory efficiency theorems tell you that a competitive market that doesn't have certain
[00:11:17.280 --> 00:11:22.400]   specific problems, such as public good problems and externalities, would actually produce the
[00:11:22.400 --> 00:11:30.240]   efficient outcome, and you never quite have that, but that's a whole lot closer than if you try to
[00:11:30.240 --> 00:11:35.600]   imagine other institutions, such as a centrally planned system, where the central planner has
[00:11:35.600 --> 00:11:41.760]   neither the right incentives, nor the information, nor the power, that the competitive market has all
[00:11:41.760 --> 00:11:45.520]   the power you want, because each person controls themselves, so that's control over everybody,
[00:11:45.520 --> 00:11:50.640]   just in the decentralized form, and a central planner never has that of course, so I think it's
[00:11:50.640 --> 00:11:59.200]   a useful concept, but you have to recognize that once you get to a fully realistic picture of any
[00:11:59.200 --> 00:12:04.720]   system, you're not going to achieve it, all you can do is say, well in this approximation you
[00:12:04.720 --> 00:12:10.560]   would achieve it, and then you have some feel to how much breaking the approximation messes it up,
[00:12:10.560 --> 00:12:18.480]   it's a useful way of thinking about economics in my view. I want to ask you about futurism now,
[00:12:18.480 --> 00:12:24.480]   you talk in your book Future Imperfect about how contracts will be enforced in the future,
[00:12:24.480 --> 00:12:28.720]   and you see because of the rise of... They will be enforced in the future,
[00:12:28.720 --> 00:12:34.480]   one of the things I try to be careful about in that book is to say I'm not making predictions,
[00:12:34.480 --> 00:12:38.880]   I am saying if this technology develops in this way, this is what will happen,
[00:12:38.880 --> 00:12:43.120]   but I don't know which technologies are going to develop in which way,
[00:12:43.120 --> 00:12:49.440]   so in the case of contracts, what I think you're thinking about is my discussion of how a world
[00:12:49.440 --> 00:12:57.840]   online works, where you have complete privacy, what I refer to as a world of strong privacy,
[00:12:57.840 --> 00:13:04.480]   that encryption potentially could give you a world where I can do business with you and no
[00:13:04.480 --> 00:13:10.960]   third party can observe what we're doing, and that includes making money, and then the question is
[00:13:10.960 --> 00:13:17.360]   in that world how do you enforce contracts, and I say if that happens, if you end up with that
[00:13:17.360 --> 00:13:24.720]   future, here is a plausible way in which contracts could get enforced using ultimately reputational
[00:13:24.720 --> 00:13:31.760]   mechanisms, but I don't know that world is going to happen, that the National Security Agency has
[00:13:31.760 --> 00:13:37.760]   been doing its best for decades to keep that world from happening, and you can see from there as I
[00:13:37.760 --> 00:13:44.720]   try to make clear in the book, there are downsides of that world as well, as well as upsides,
[00:13:44.720 --> 00:13:49.440]   and we're in fact seeing one of the downsides already, because one of the things I think I
[00:13:49.440 --> 00:13:58.000]   discuss, I think I discussed in Future Imperfect, is the possibility for cyber ransoming, for
[00:13:58.000 --> 00:14:05.600]   getting onto somebody's computer, encrypting the whole contents of the computer, and then say I'll
[00:14:05.600 --> 00:14:11.920]   sell you the key back to decrypt it, and that's a bad thing, and you can think of other bad things,
[00:14:11.920 --> 00:14:17.680]   that if you kidnap people at the moment in the real world, the hardest part of being a successful
[00:14:17.680 --> 00:14:22.000]   kidnapper is how you collect the money, because that's where you're likely to get caught, we can
[00:14:22.000 --> 00:14:30.800]   solve that problem, all right, with good cryptocurrency and a world where you've got
[00:14:30.800 --> 00:14:38.560]   enough encryption so that people can't follow who's sending message to whom, now, so that world
[00:14:38.560 --> 00:14:45.120]   has downsides as well as upsides, now I happen to think that on net, governments do more harm than
[00:14:45.120 --> 00:14:51.120]   good, would be a sort of simple way of putting it, and that therefore a world in which governments
[00:14:51.120 --> 00:14:56.560]   find it harder to control people, although we'll have the disadvantage that some of the things
[00:14:56.560 --> 00:15:01.920]   governments do are good and won't be done, will have greater advantages, but in any case,
[00:15:01.920 --> 00:15:06.000]   all I'm saying is I'm not predicting that that world will happen, I'm saying that world could
[00:15:06.000 --> 00:15:12.320]   happen, there's the technological possibility for that, that we've known for I guess decades at this
[00:15:12.320 --> 00:15:17.520]   point, well, Bitcoin is newer than that, but cryptocurrencies are the only new part of that
[00:15:17.520 --> 00:15:24.080]   world, I wrote this up long before cryptocurrencies existed, and so we could have that world,
[00:15:24.080 --> 00:15:28.320]   and it's a really interesting world, which is part of why I talk about things like how you
[00:15:28.320 --> 00:15:32.160]   enforce contracts in it, but I'm not predicting it's going to happen, and similarly for the
[00:15:32.160 --> 00:15:36.880]   various other things that I discussed, that the one I most want is to solve the aging problem,
[00:15:37.440 --> 00:15:44.480]   because at the moment it is very, under current circumstances, the probability that I will be
[00:15:44.480 --> 00:15:49.840]   alive 30 years from now is very close to zero, I would like to be alive 30 years from now,
[00:15:49.840 --> 00:15:56.080]   I like living, living is a lot of fun, so the particular future that I am most in favor of
[00:15:56.080 --> 00:16:01.360]   is the one where they solve the aging problem, where I get at worst to stay at something like
[00:16:01.360 --> 00:16:06.560]   my present age, which is a little bit creakier than I used to be, but tolerable, and ideally
[00:16:06.560 --> 00:16:14.320]   get to run it back a couple of decades, that world will have its own features, mostly good,
[00:16:14.320 --> 00:16:21.120]   but not all good, because after all, if that had happened a few decades earlier, it would have been
[00:16:21.120 --> 00:16:27.440]   Stalin who presided with nuclear weapons over the breakup of the Soviet Union, and he might have had
[00:16:27.440 --> 00:16:35.280]   a different view of the people who were in power, Spain would still be being ruled by the dictator
[00:16:35.280 --> 00:16:41.360]   who ruled it from the end, from what, I guess sometime in the, whenever the Spanish Civil War
[00:16:41.360 --> 00:16:47.760]   was over, 1940 or so, until a few decades ago, Franco, so there would be bad things about a
[00:16:47.760 --> 00:16:53.440]   world where people didn't get old, but I think on net there would be good things, so all of these
[00:16:53.440 --> 00:16:59.120]   futures have positive and negative features, and I'm not predicting which ones will happen,
[00:16:59.120 --> 00:17:03.600]   I was only trying to explore for each of those futures, and there's a reason I called the book
[00:17:03.600 --> 00:17:11.360]   Future Imperfect, that was partly a grammatical pun, and partly the fact that none of these futures
[00:17:11.360 --> 00:17:18.560]   is perfect, one of the early cypherpunks, who were people who were exploring the encryption stuff
[00:17:18.560 --> 00:17:24.320]   before anybody else was, his line was, encryption is not your friend, right, technologies are not
[00:17:24.320 --> 00:17:31.280]   your friend, technologies are what they are. Yeah, so okay, fair enough, assuming, let's say
[00:17:31.280 --> 00:17:39.040]   if we move to a system where more contracts are enforced by reputation rather than the traditional
[00:17:39.040 --> 00:17:44.560]   legal system, I want to ask you, what are the social consequences of that, for example, will
[00:17:44.560 --> 00:17:52.480]   we see a return to honor culture, and is the rise of political correctness maybe a consequence
[00:17:52.480 --> 00:17:57.360]   of us turning to a culture that depends more on reputation, because now whether you are woke or
[00:17:57.360 --> 00:18:02.240]   not, either way, says something about your reputation, and that signal matters more than
[00:18:02.240 --> 00:18:07.920]   it did in the past to your employer or to potential clients. That's interesting, that is, the obvious
[00:18:07.920 --> 00:18:13.920]   problem is that for some people woke is a positive signal, and some a negative signal, certainly for
[00:18:13.920 --> 00:18:23.680]   me it's a negative signal, and the thing is that the reputation you want is an online reputation,
[00:18:24.400 --> 00:18:30.240]   which does not have to be linked to your real space identity, and in the kind of world I was
[00:18:30.240 --> 00:18:35.920]   discussing wouldn't be linked to your real space identity, that partly it's the fact that it's not
[00:18:35.920 --> 00:18:40.880]   linked that means you have to depend on reputational, but of course it's also true that
[00:18:40.880 --> 00:18:47.840]   reputational enforcement has been important for a very long time, that most, I would have guessed
[00:18:47.840 --> 00:18:54.320]   that most of what makes stores behave themselves, act well, is not the legal threat at all, it's
[00:18:54.320 --> 00:19:03.520]   the threat of bad reputation, that I've had the experience of, I was having problems with the
[00:19:03.520 --> 00:19:10.080]   people who were putting in the battery for my solar system, and I put up a very negative review
[00:19:10.080 --> 00:19:15.440]   online, and I immediately got a response from them, basically, what can I do to help things,
[00:19:15.440 --> 00:19:22.320]   and they ended up giving me a $500 discount to apologize for the month during which they
[00:19:22.320 --> 00:19:26.000]   had turned off the solar system that already existed when they installed the battery,
[00:19:26.000 --> 00:19:33.040]   so, and I think there are a lot of other examples you see nowadays, which doesn't,
[00:19:33.040 --> 00:19:39.120]   don't even depend on the transactions being online transactions, because companies care
[00:19:39.120 --> 00:19:44.080]   about their reputation, and they have an online reputation even for real space transactions,
[00:19:44.080 --> 00:19:51.760]   that, so, but the question is, is there any particular reason why you would trust somebody
[00:19:51.760 --> 00:19:56.720]   who agreed with your politics more, and it seems to me that, you know, my impression is that woke
[00:19:56.720 --> 00:20:01.120]   people quite often fight with each other and, you know, attack each other for, can't try to cancel
[00:20:01.120 --> 00:20:08.320]   each other because they're the wrong version of woke, so to speak, so I, it seems to me,
[00:20:08.320 --> 00:20:15.040]   you might have, what you might get, you might get the pattern, which you do already have,
[00:20:15.600 --> 00:20:22.240]   where you've got groups of people who have something in common, and they, so one of the
[00:20:22.240 --> 00:20:31.360]   sort of odd patterns you get is some particular market niche, say, donut shops, being dominated
[00:20:31.360 --> 00:20:34.880]   by some particular ethnic group, I don't, I think there's a real, I think donut shops are a real
[00:20:34.880 --> 00:20:42.240]   example, I don't remember which it is, and I know from my own experience that running motels is
[00:20:42.240 --> 00:20:49.680]   dominated by Indians in the US, I don't know why, but that's an observed pattern, so, and part of
[00:20:49.680 --> 00:20:56.560]   what may be happening there is that you have an ethnic group, you don't want to offend somebody
[00:20:56.560 --> 00:21:04.720]   who you, your son might want to court his daughter, that there are a bunch of social
[00:21:04.720 --> 00:21:10.160]   links within the ethnic group, and even aside from that, you don't want to offend somebody
[00:21:10.160 --> 00:21:16.080]   who knows people who know you. I remember observing a long, long time ago online,
[00:21:16.080 --> 00:21:20.000]   back in the days of Usenet, if you're familiar with that, which was really before the web,
[00:21:20.000 --> 00:21:26.560]   and one of my hobbies is a group that does historical recreation called the Society for
[00:21:26.560 --> 00:21:32.640]   Creative Anachronism, and it had a Usenet group, and it was a whole lot more polite than most Usenet
[00:21:32.640 --> 00:21:41.200]   groups that I observed, and I think the reason was that if you made, if you behaved really badly
[00:21:41.200 --> 00:21:45.520]   on there, if you went around insulting people for no good reason and being unreasonable,
[00:21:45.520 --> 00:21:50.640]   even if the person you were being nasty to would never meet you, he probably knew somebody who
[00:21:50.640 --> 00:21:56.480]   would meet you, and you didn't want to have your reputation, your real space reputation messed up,
[00:21:57.840 --> 00:22:02.800]   so in that respect, I think there is, but I think that's an effect which will go,
[00:22:02.800 --> 00:22:08.160]   which will become smaller in the future world, because in the future world, we've got an online
[00:22:08.160 --> 00:22:14.000]   version of that, so that in the future world, at least as I've described it, I don't have to
[00:22:14.000 --> 00:22:18.720]   know you in real space, and most people I'm interacting with, I won't, many of them won't
[00:22:18.720 --> 00:22:26.240]   even be on the same continent I'm on. I've, in fact, interacted with a number of dealers in India
[00:22:26.800 --> 00:22:32.080]   who I've never met, but who I'm willing to trust to deliver what they say they'll deliver,
[00:22:32.080 --> 00:22:40.320]   because they've done so in the past, and so it seems to me that if anything, the internet means
[00:22:40.320 --> 00:22:48.320]   that reputation is less tied to real space interactions and more to online interactions,
[00:22:48.320 --> 00:22:53.760]   and we have better mechanisms for keeping track of reputation than we used to, that eBay, for
[00:22:53.760 --> 00:22:58.560]   example, as you may know, when you have any eBay transaction, you get the opportunity to report to
[00:22:58.560 --> 00:23:04.560]   eBay on how it went, and when you're considering dealing with something, one of the things you see
[00:23:04.560 --> 00:23:09.120]   is what's their eBay reputation, how many people have said that they fulfilled what they said they
[00:23:09.120 --> 00:23:16.080]   would fulfill, and you get the same thing if you buy something from Amazon that is not actually
[00:23:16.080 --> 00:23:20.320]   being sold by Amazon, because Amazon also functions as a middleman. Books are the context where I'm
[00:23:20.320 --> 00:23:25.360]   most familiar with this, and one of the things you see is, well, this seller has it at this price,
[00:23:25.360 --> 00:23:30.160]   and here is what his rating is. Here is what people have dealt with and say, so it seems to me
[00:23:30.160 --> 00:23:37.040]   that I don't think I would expect, in a sense, I think of what you're calling honor culture as more
[00:23:37.040 --> 00:23:45.920]   the real space version of doing this. Interesting. Yeah, I wonder what the consequences of tearing
[00:23:45.920 --> 00:23:51.600]   down the walls between communities that perhaps make reputation in some ways more salient. For
[00:23:51.600 --> 00:23:56.720]   example, with the example of the Indian motels, I've lived in many different small towns across
[00:23:56.720 --> 00:24:03.440]   the US, and you might have at most 10 people who are of Indian origin, which isn't a big deal for
[00:24:03.440 --> 00:24:09.920]   me, but for my parents, that's a very big part of their social community, so losing status in that
[00:24:09.920 --> 00:24:14.560]   community is a big deal, and the reputation matters a lot more. Similarly, for your Usenet
[00:24:14.560 --> 00:24:20.320]   group for the SCA, when you have communities that aren't isolated in that way, for example,
[00:24:20.320 --> 00:24:28.000]   when you have a Twitter user, then is the value of reputation or the capacity of reputation to
[00:24:28.000 --> 00:24:34.480]   tamp down on bad behavior, is that reduced? But it depends a whole lot on whether you are
[00:24:34.480 --> 00:24:41.920]   interacting with people where your reputation costs you anything, so that if I become part of
[00:24:41.920 --> 00:24:53.440]   a Twitter mob online, first, insofar as I have an online identity, that online identity is not doing
[00:24:53.440 --> 00:24:59.840]   business with anybody. It's not probably involved in any interactions where people trusting it
[00:24:59.840 --> 00:25:06.960]   matters, so it's free to behave as badly as it wants. On the other hand, if I am interacting
[00:25:07.520 --> 00:25:16.640]   as I do on a blog or a forum where there is maybe 100 people who are regular posters to it,
[00:25:16.640 --> 00:25:22.800]   in that case, reputation does matter, that if I behave in a way that shows me to be an unreasonable
[00:25:22.800 --> 00:25:26.400]   person, people won't take what I say very seriously. It's not as serious as he will
[00:25:26.400 --> 00:25:35.760]   not be willing to sell to me or buy from me, but it's still a real cost. Anyway,
[00:25:37.280 --> 00:25:44.560]   you know, what, how this is going to pan out is very, in general, the internet is going to,
[00:25:44.560 --> 00:25:51.440]   is having and is going to have non-obvious effects. I'm reminded, I just saw somebody online
[00:25:51.440 --> 00:25:57.200]   who quoted a bit that Heinlein, science fiction writer, wrote, where he was talking about the
[00:25:57.200 --> 00:26:03.440]   influence of automobiles, and he was saying that there were people who, when the early automobiles
[00:26:03.440 --> 00:26:08.640]   came out, suggested that this would end up entirely replacing the horse. But he doesn't
[00:26:08.640 --> 00:26:14.800]   know anybody who had realized how it would affect courtship behavior, that the existence of closed
[00:26:14.800 --> 00:26:22.960]   automobiles meant that there was now a place where couples could make out where their parents wouldn't
[00:26:22.960 --> 00:26:29.440]   be seeing them, and where they could be 15 miles away from anybody who knew them, so as to reduce
[00:26:29.440 --> 00:26:36.880]   the chance of reputational costs and so forth. And somebody else online quoted somebody 30 years
[00:26:36.880 --> 00:26:42.640]   before Heinlein, when this was just happening, who was commenting on this effect, and quoted
[00:26:42.640 --> 00:26:49.120]   as judge, who in some, I don't know, some rural area probably, who said that of the last 30 women
[00:26:49.120 --> 00:26:53.760]   who had come before him charged with sexual offenses, which I presume just means having sex,
[00:26:53.760 --> 00:27:00.960]   though he didn't clear it, 19 of them had committed them in an automobile. And so that
[00:27:00.960 --> 00:27:05.680]   was an interesting case where you have a technological change, and it has some obvious
[00:27:05.680 --> 00:27:11.840]   effects, but some less obvious effects. And similarly here. So I don't know what the final
[00:27:11.840 --> 00:27:19.280]   results are going to be. I don't know how much the whole woke Twitter mob cancellation, that set of
[00:27:19.280 --> 00:27:25.680]   phenomena, how much that's only happening because of the internet, or how much it's just due to
[00:27:25.680 --> 00:27:31.840]   other ideological changes over time, which is very hard to know. Yeah. Speaking of unexpected
[00:27:31.840 --> 00:27:37.600]   developments, you have a chapter in Future Imperfect on e-cash, and the book is published
[00:27:37.600 --> 00:27:43.440]   in 2008, which is the same year that Satoshi Nakamoto writes his white paper on Bitcoin.
[00:27:44.480 --> 00:27:49.840]   In many ways, it was prophetic, the chapter he wrote, but in some interesting ways, and this is
[00:27:49.840 --> 00:27:55.840]   in the context of you being very right, in some interesting ways, what Bitcoin actually was,
[00:27:55.840 --> 00:28:01.040]   was just a completely different paradigm than what you imagined e-cash being. Specifically in the
[00:28:01.040 --> 00:28:04.880]   sense that you imagined that it would be some centralized institution, like a private bank
[00:28:04.880 --> 00:28:12.080]   or company, instituting money that is underwritten by either dollar denominations or commodity
[00:28:12.080 --> 00:28:16.080]   bundles, but in fact, you have a completely decentralized currency. I'm curious, in
[00:28:16.080 --> 00:28:21.040]   retrospect, what do you think you missed? How do you think you got it so right? Well, I couldn't
[00:28:21.040 --> 00:28:27.840]   have predicted Bitcoin. Nakamoto, he was a very smart guy. I've done computer programming, I've
[00:28:27.840 --> 00:28:32.800]   kept an eye on encryption issues because they interested me, but I was never at that level of
[00:28:32.800 --> 00:28:39.600]   expertise to realize what he could do. It was a very, very clever idea. No, the version I was
[00:28:39.600 --> 00:28:45.760]   thinking of was Chowmian digital cash, the math of which had been worked out by Chowm maybe a couple
[00:28:45.760 --> 00:28:53.680]   of decades before I was writing something like that. In a sense, that is the biggest difference
[00:28:53.680 --> 00:28:58.720]   is that because Chowmian digital cash requires an issuer, it hasn't happened.
[00:28:58.720 --> 00:29:06.240]   Because you need an issuer, which is a reliable institution, which means it almost has to be in
[00:29:06.240 --> 00:29:12.160]   some developed country or at least some country with sort of reasonably well-functioning legal
[00:29:12.160 --> 00:29:19.920]   institutions. But one effect of anonymous digital cash is that money laundering laws become
[00:29:19.920 --> 00:29:26.080]   unenforceable. Governments very much dislike that in part for good reasons, in part for bad reasons.
[00:29:26.080 --> 00:29:34.000]   Consequently, no country of the sort you would need existed that was willing to put up with it.
[00:29:34.000 --> 00:29:38.880]   That's at least my interpretation of why it never happened. The beautiful thing about Bitcoin is
[00:29:38.880 --> 00:29:45.120]   there's no issuer, and it's just happening invisibly online, as it were, which is very
[00:29:45.120 --> 00:29:50.960]   clever. The problem with Bitcoin, but not with cryptocurrency in general, not necessarily,
[00:29:50.960 --> 00:30:00.320]   is that the value of a Bitcoin is very unstable over time. That's not too much of a problem if
[00:30:00.320 --> 00:30:07.200]   you want it as a speculative investment. I, in fact, have some Bitcoin. But it makes it rather
[00:30:07.200 --> 00:30:12.000]   inconvenient if you're using it for day-to-day transactions, because if a price is set in
[00:30:12.000 --> 00:30:19.040]   Bitcoin, the seller doesn't know what value he's going to get for his good. The buyer looking at
[00:30:19.040 --> 00:30:23.360]   the price in Bitcoin can't easily compare it to what somebody else was selling the same good for
[00:30:23.360 --> 00:30:28.720]   a month earlier, because the price of Bitcoin could have changed 30% over that period of time.
[00:30:29.440 --> 00:30:36.640]   So Bitcoin is really not very good for a money for transactions. And that means that you want
[00:30:36.640 --> 00:30:41.920]   somebody to produce a stable coin that's easy to use. And there are various projects of that sort.
[00:30:41.920 --> 00:30:47.120]   I've got one of my blog posts in which I sketch out one mechanism by which you could get a stable
[00:30:47.120 --> 00:30:54.080]   coin, which would be stable in, say, dollars or in something else. And I'm sure there are other
[00:30:54.080 --> 00:31:00.240]   ways. That is, you could, in a sense, one of the things that ought to be doable-- but I'm not,
[00:31:00.240 --> 00:31:06.560]   again, enough of an expert to know quite how you do it-- is to have a Bitcoin where every time the
[00:31:06.560 --> 00:31:13.280]   value of Bitcoin goes up, you get a new subdivision with the same value. So in effect, if the value of
[00:31:13.280 --> 00:31:20.080]   Bitcoin goes up, instead of having 100 units, you're now having 112 units. If it goes down,
[00:31:20.080 --> 00:31:26.560]   instead of 100 units, you're now at 90 units. And then the units themselves could be fixed to the
[00:31:26.560 --> 00:31:33.200]   dollar. And I haven't-- in a sense, I tend to write more than I read, in a sense. So I haven't
[00:31:33.200 --> 00:31:38.480]   really made a serious effort to keep track of the various projects for stable coins other than the
[00:31:38.480 --> 00:31:44.400]   one that I invented and I'm not doing anything with it that I discussed on my blog. But I think
[00:31:44.400 --> 00:31:53.360]   if you had a cryptocurrency that both had a reasonably stable value in terms of dollars or
[00:31:53.360 --> 00:32:00.080]   euros or Swiss francs, in terms of something whose exchange rate to other things didn't change very
[00:32:00.080 --> 00:32:04.320]   rapidly, which is what you want-- ideally, of course, you do it in terms of the commodity
[00:32:04.320 --> 00:32:09.760]   bundle, but I'm not sure how easy that's going to be to set up-- that if you had such a thing
[00:32:10.720 --> 00:32:17.440]   and if it was easy to make transactions in it, which, again, I think is not yet true of Bitcoin,
[00:32:17.440 --> 00:32:21.440]   though since I've just held it rather than try to use it, it could be that by now they've made it
[00:32:21.440 --> 00:32:28.800]   easier, that then it would become a serious alternative to conventional currencies.
[00:32:28.800 --> 00:32:36.160]   And if you could do it in a way which proofed it against inflation, which means you don't
[00:32:36.160 --> 00:32:40.800]   lock it to the dollar, maybe you lock it to the average of the dollar, the Swiss franc,
[00:32:40.800 --> 00:32:47.040]   and the euro or something, then the problem is-- as you probably know, I don't know how much you
[00:32:47.040 --> 00:32:51.840]   know about cryptocurrency-- is the basic problem is you need an oracle. You need a way in which
[00:32:51.840 --> 00:32:59.840]   the software can find out what the exchange rate is. And I know of some clever ways that
[00:33:01.840 --> 00:33:07.600]   some of the people-- not Bitcoin, but with the second most popular currency-- have some
[00:33:07.600 --> 00:33:11.600]   interesting ideas about that, because I discussed it with them a few years ago. Their idea is not
[00:33:11.600 --> 00:33:17.760]   mine. But I don't really know at the moment to what extent people have oracles that they can
[00:33:17.760 --> 00:33:21.600]   trust, because you need some way in which the software can determine the facts of the real
[00:33:21.600 --> 00:33:28.480]   world. And of course, the more the software can determine facts of the real world, the more neat
[00:33:28.480 --> 00:33:32.880]   things you can do with it, because as you may know, there's this old idea of smart contracts
[00:33:32.880 --> 00:33:43.200]   where you have a-- I make a bet with you, and the bet is, in effect, programmed into the money
[00:33:43.200 --> 00:33:49.200]   in such a way that if I win the bet, you suddenly have some less money, and I have some more money,
[00:33:49.200 --> 00:33:54.800]   and vice versa if you win it. And there are ways of doing that, but it requires that--
[00:33:57.680 --> 00:34:01.680]   this is Ethereum, is the one I'm familiar with, which does this sort of stuff. It requires that
[00:34:01.680 --> 00:34:07.840]   either can find out who won the bet, and that's sort of an interesting project which requires
[00:34:07.840 --> 00:34:17.520]   ingenuity to solve. But no, I think-- so anyway, I think the-- and I should say, my ideas on
[00:34:17.520 --> 00:34:23.280]   cryptocurrency are partly because I was interacting with some of the cypherpunks very early,
[00:34:23.280 --> 00:34:28.400]   and it was really a case where one of them had stolen a bunch of ideas from me,
[00:34:28.400 --> 00:34:34.880]   reprocessed them, and I then stole back his version from him, because I had the discussion
[00:34:34.880 --> 00:34:41.280]   in machinery of how a stateless market society would work. And he had then said, well, what
[00:34:41.280 --> 00:34:46.880]   happens with those ideas if you have strong encryption? And had worked out some neat stuff,
[00:34:48.000 --> 00:34:57.200]   and then I read his stuff and borrowed it back. So I can probably claim a little bit of credit,
[00:34:57.200 --> 00:35:04.320]   but it's not like I thought up these ideas of the digital clash all by myself. I was partly
[00:35:04.320 --> 00:35:09.520]   a part of people who were interacting over that. I wasn't really a part of the cypherpunks. That is,
[00:35:09.520 --> 00:35:15.600]   I was on the mailing list for a while, and I met some of them once, but I knew what they were
[00:35:15.600 --> 00:35:19.680]   talking about, and it was neat stuff. Yeah. I've done a little bit of programming with smart
[00:35:19.680 --> 00:35:27.600]   contracts, and one of the very interesting ideas is that you can have contracts that own assets.
[00:35:27.600 --> 00:35:32.640]   So the contract owns the Ethereum, which in retrospect just seems like an obvious idea,
[00:35:32.640 --> 00:35:37.360]   but what I learned about it just seems like a very cool idea. Speaking of smart contracts,
[00:35:37.360 --> 00:35:41.360]   I'm curious what you think about prediction markets. I don't know if you've-- I don't know
[00:35:41.360 --> 00:35:45.840]   your opinion on that, but-- I've known about prediction markets for longer than almost anybody
[00:35:45.840 --> 00:35:50.960]   else, because I was corresponding with Robin Hanson back before he switched fields and became
[00:35:50.960 --> 00:35:57.280]   an economist. That's sort of my standard example of one of the really nice things about this world
[00:35:57.280 --> 00:36:03.200]   is that it used to be that you had to be a university professor because the people you
[00:36:03.200 --> 00:36:07.600]   wanted to interact with had to be in the same building you were, so to speak, to interact very
[00:36:07.600 --> 00:36:12.880]   much. Now it's no longer true, so I came across Robin very early, discovered that he had these
[00:36:12.880 --> 00:36:17.680]   really neat ideas, and so I corresponded with him. So no, the prediction markets are a very
[00:36:17.680 --> 00:36:26.480]   interesting idea, and I gather that they are developing. I think the ones I know about seem
[00:36:26.480 --> 00:36:33.040]   to have limits in the size of the bets you can make and in the amount of house cut if you want
[00:36:33.040 --> 00:36:39.040]   to take your money out. So I assume that will develop further. But in principle-- and Robin's
[00:36:39.040 --> 00:36:43.760]   idea, of course, is that you want the prediction market as a way of generating information. And
[00:36:43.760 --> 00:36:52.320]   that was-- I mean, a lot of the great ideas are very simple. And his idea was a stock market
[00:36:52.320 --> 00:37:00.000]   has as its nominal function allocating capital. And just as a side effect, it generates information,
[00:37:00.000 --> 00:37:05.040]   because you observe that certain stocks are going up or down, and that tells you something about
[00:37:05.040 --> 00:37:10.080]   what people who are willing to bet on it believe about those companies or those industries or
[00:37:10.080 --> 00:37:14.720]   whatever. So he said, look, how about you do it deliberately? There are things we'd like to know.
[00:37:14.720 --> 00:37:19.520]   What if you set up a market for that purpose? And that was where prediction markets originated,
[00:37:19.520 --> 00:37:30.640]   which was Robin Hanson's idea. So no, they're a neat idea. And some, of course, have always existed.
[00:37:30.640 --> 00:37:35.840]   If you think about futures markets for agricultural products are a prediction market,
[00:37:35.840 --> 00:37:41.280]   although they're not referred to that way, that if you observe that the future price of wheat
[00:37:41.280 --> 00:37:46.800]   is sizably larger than the present price as of a year out, you say, oops, looks as though the
[00:37:46.800 --> 00:37:52.320]   smart money expects that there is going to be a bad harvest or something else is going to happen
[00:37:52.320 --> 00:38:03.040]   that's going to do it. And that's very neat. And in principle, you have to-- I guess it requires
[00:38:03.040 --> 00:38:07.760]   that there is a reason why people want to bet on this, but people like to bet on lots of things.
[00:38:07.760 --> 00:38:14.160]   And so no, I think that's something nice that's developing. And again, I haven't really paid
[00:38:14.160 --> 00:38:19.440]   enough attention. I haven't actually ever made a predicted bet, although my younger son has been
[00:38:19.440 --> 00:38:25.200]   betting, I think, on predicted and says he's ahead so far. He's doing it mostly for fun,
[00:38:25.200 --> 00:38:32.160]   but you don't want to limit people doing it for fun, because the thing is, if you can only make
[00:38:32.160 --> 00:38:40.240]   small bets, then the prices are going to be dominated by ideologues, whereas if you can
[00:38:40.240 --> 00:38:45.600]   make bets of any size, then the smart money who doesn't care what's true, they only care
[00:38:45.600 --> 00:38:54.000]   what bets make money is likely to take over. There was an interesting case back-- I don't
[00:38:54.000 --> 00:38:58.960]   remember which election it was, but one of the US presidential elections a few presidents ago,
[00:38:58.960 --> 00:39:06.640]   where the relevant market very briefly surged in favor of one of the candidates and then very
[00:39:06.640 --> 00:39:12.560]   quickly went back. And my conjecture was that somebody who wanted that candidate to win
[00:39:12.560 --> 00:39:19.920]   thought that pushing the prediction market in his favor would make people expect him to win,
[00:39:19.920 --> 00:39:23.520]   which would then make him more likely to win, because you're more likely to donate money to
[00:39:23.520 --> 00:39:30.080]   people who you think are going to win, but that his effect got wiped out by the smart
[00:39:30.080 --> 00:39:34.800]   money coming in on the other side when they saw that there was insect-free money lying around.
[00:39:36.320 --> 00:39:40.080]   But that does require a market where you can make reasonably large bets,
[00:39:40.080 --> 00:39:45.200]   because otherwise it's the number of bettors and not how strongly they feel about it,
[00:39:45.200 --> 00:39:50.800]   not how confident they are that it matters. So prediction markets are a neat idea.
[00:39:50.800 --> 00:39:56.080]   Yeah. I want to ask, for example, the chapter on e-cash you talk about,
[00:39:56.080 --> 00:40:01.040]   at the time you thought that the burdens to e-cash were regulatory, not technological.
[00:40:01.040 --> 00:40:05.440]   When we talk about these other fields, I wonder to what extent you think that
[00:40:05.440 --> 00:40:11.360]   what proportion of progress you think is stopped by we just don't have the right technology versus
[00:40:11.360 --> 00:40:14.960]   how much you think it's just because the regulations of the space are just not conducive
[00:40:14.960 --> 00:40:22.240]   to innovation. My view then, and I think I may have been too optimistic, was that it's very
[00:40:22.240 --> 00:40:28.800]   hard for regulation to stop progress because there are many independent countries. And the evidence
[00:40:28.800 --> 00:40:35.760]   against that is that as far as I know, nobody had challenged trials for COVID vaccines.
[00:40:35.760 --> 00:40:36.640]   Or nuclear fusion.
[00:40:36.640 --> 00:40:44.880]   Nuclear fusion may just be hard, but we know that vaccines aren't hard because they produce
[00:40:44.880 --> 00:40:49.760]   a successful vaccine in something less than a week. People talk about the fact, isn't it wonderful
[00:40:49.760 --> 00:40:55.760]   that we got vaccines in a little under a year? But the truth is that we got vaccines in less than a
[00:40:55.760 --> 00:41:03.920]   week and all the rest of the time was the testing procedure. And you couldn't do it to zero. First,
[00:41:03.920 --> 00:41:10.400]   it would take time to get to produce substantial quantities. But you could, as far as I can tell,
[00:41:10.400 --> 00:41:16.480]   have done a good enough testing so that it should have been available to anybody who wanted to take
[00:41:16.480 --> 00:41:23.840]   the risks in maybe two months. That what do you do? You do two tests simultaneously. One of them
[00:41:23.840 --> 00:41:29.040]   is you give the vaccine to a large random set of people, so old, young, et cetera, and you don't
[00:41:29.040 --> 00:41:33.440]   expose them to the disease. All you're trying to do is see if the vaccine has serious side effects.
[00:41:33.440 --> 00:41:40.240]   You then take 100 or 200 or 300 healthy young adults. You give them the vaccine and you
[00:41:40.240 --> 00:41:48.240]   deliberately expose them to COVID. That'll tell you a month after, if you believe that you may
[00:41:48.240 --> 00:41:53.280]   need two shots a month apart, that you wouldn't even have to do that in order to find out that it
[00:41:53.280 --> 00:42:02.640]   works, just to find out the best way of doing it. So expose them and give your people the vaccine,
[00:42:02.640 --> 00:42:09.920]   expose them all to COVID. You have some rough idea of people who are exposed to COVID from external
[00:42:09.920 --> 00:42:14.160]   evidence, how likely they are to get it. If almost none of them get it, you know the vaccine works.
[00:42:14.160 --> 00:42:23.120]   Now you can keep getting better information as you go on, but that's enough. So that means that
[00:42:23.120 --> 00:42:29.840]   we could have had some of the vaccine in maybe two months instead of whatever it was, 10 months or 11
[00:42:29.840 --> 00:42:34.400]   months. How fast they could expand production and how many people would be willing to risk it and
[00:42:34.400 --> 00:42:40.080]   such, I don't know. But the impressive thing is not just that the U.S. didn't do it. We know the
[00:42:40.080 --> 00:42:46.400]   FDA is crazy that way, but that nobody did. Now I don't really know for certain what happened in
[00:42:46.400 --> 00:42:53.200]   China and Russia, but at least of the Western countries, none of them was willing to do
[00:42:53.200 --> 00:43:02.640]   challenge trials. And that's, I think, a case of enough common cultural elements,
[00:43:02.640 --> 00:43:10.800]   one of which is better to have 100,000 people die from not doing something than to have 10 people
[00:43:10.800 --> 00:43:18.000]   die from doing something. And nobody will put it that way. So that's sort of disappointing to me
[00:43:18.000 --> 00:43:24.400]   that you didn't. You had one country which in other ways defied the rules, namely Sweden,
[00:43:24.400 --> 00:43:29.440]   and that was a good thing. That gives you some more information, although not as good information
[00:43:29.440 --> 00:43:33.120]   as you'd like. They didn't do very well, but we're not sure how many other differences there are
[00:43:33.120 --> 00:43:38.320]   among the countries that could explain it. They did much worse than the other Scandinavian
[00:43:38.320 --> 00:43:43.120]   countries, but not much worse than Italy or Spain, say. In fact, I think a little bit better than
[00:43:43.120 --> 00:43:49.600]   Italy and Spain. I'm not sure. And then people have different explanations of the reasons for
[00:43:49.600 --> 00:43:58.240]   these different outcomes. But in a sense, I wish the world were less uniform in that sense.
[00:43:59.920 --> 00:44:08.800]   And if I think about aging, for example, I can easily imagine the FDA at least requiring all
[00:44:08.800 --> 00:44:17.360]   sorts of lengthy testing for medical treatments to stop aging, and maybe even trying to block them.
[00:44:17.360 --> 00:44:24.560]   Because after all, one of the effects of ending aging is that the social security system goes
[00:44:24.560 --> 00:44:32.480]   bankrupt very quickly, unless you are willing to stop paying social security to very old people or
[00:44:32.480 --> 00:44:36.560]   something. And there are going to be other respects in which there's going to be prejudice.
[00:44:36.560 --> 00:44:40.080]   So I could certainly imagine that some countries would see
[00:44:40.080 --> 00:44:44.800]   ending aging as a bad thing. I find it hard to believe that all of them would.
[00:44:44.800 --> 00:44:50.080]   And there are obviously a lot of politically powerful people and a lot of wealthy people
[00:44:50.080 --> 00:44:56.880]   who are old and very much like not to die of old age. So my guess is that can't be stopped. But as
[00:44:56.880 --> 00:45:02.400]   I say, I would have guessed that somebody would have done a better job on COVID than they did.
[00:45:02.400 --> 00:45:06.080]   Right. Robin has an interesting theory about this. I don't know if you've seen this, but
[00:45:06.080 --> 00:45:11.280]   he thinks that nuclear power could have been much more technically proficient by this point,
[00:45:11.280 --> 00:45:15.600]   but not for regulations. And the way he explains how it's been globally curtailed
[00:45:15.600 --> 00:45:22.400]   is he has a blog post titled World Forger Elite or something like that. And I had him on my podcast
[00:45:22.400 --> 00:45:26.240]   if you want to listen to that part of the conversation. But his idea is that the elites
[00:45:26.240 --> 00:45:31.040]   across the world who control regulatory agencies and policymaking, they talk to each other. And
[00:45:31.040 --> 00:45:38.480]   just like any profession, they're deeply imitative and coveting of each other's approval. Just as
[00:45:38.480 --> 00:45:42.000]   journalists want the approval of other journalists, intellectuals want the approval of other
[00:45:42.000 --> 00:45:47.280]   intellectuals, policymakers from different countries want the approval of-- the EU regulators
[00:45:47.280 --> 00:45:53.600]   want, the FDA regulator, and so on. Anyways, this actually brings up a question I wanted to ask you
[00:45:53.600 --> 00:45:58.560]   about legal systems very different from ours, which is your most recent book. You talk about
[00:45:58.560 --> 00:46:02.960]   the vast diversity of legal systems throughout history. And yet, when I look out at the world
[00:46:02.960 --> 00:46:10.080]   today, you have some exceptions, maybe Sharia in the Middle East, and so on.
[00:46:10.960 --> 00:46:15.680]   There is nowhere in the Middle East, with the possible exception of the Islamic State,
[00:46:15.680 --> 00:46:18.880]   which doesn't rule anything, which actually follows Sharia.
[00:46:18.880 --> 00:46:20.960]   OK. So then the question--
[00:46:20.960 --> 00:46:28.080]   The Saudis are the closest, as far as I know. But mostly countries that talk about following Sharia
[00:46:28.080 --> 00:46:36.240]   have a government statutory legal system, some of whose rules are borrowed from Sharia. But Sharia,
[00:46:37.520 --> 00:46:42.960]   the real thing-- it's not clear, actually, that Sharia is the right term for it. But the real
[00:46:42.960 --> 00:46:50.000]   traditional Islamic legal system was not legislated by the state. That was a system
[00:46:50.000 --> 00:46:56.960]   sort of like common law in the Anglo-American system, in which the legal rules were worked
[00:46:56.960 --> 00:47:03.520]   out by scholars based on religious sources. And the judges were appointed by the state.
[00:47:04.080 --> 00:47:08.000]   But the judges didn't even have to be legal experts. It was a good thing if they were.
[00:47:08.000 --> 00:47:14.400]   But they could rely on the muftis, who were initially at least reputationally supported
[00:47:14.400 --> 00:47:17.600]   legal scholars, to tell them what the law was.
[00:47:17.600 --> 00:47:26.560]   In principle, that system had separation of state and law. Now, states always sort of
[00:47:26.560 --> 00:47:32.320]   poached on that in various ways. But that was the base. No one does that now.
[00:47:32.880 --> 00:47:38.480]   Right. So just to finish my question, then, what is the explanation for why you don't have any
[00:47:38.480 --> 00:47:44.560]   country that has marketable tort claims, as far as I'm aware, or where the plaintiff has to bear
[00:47:44.560 --> 00:47:50.640]   some risk for filing a suit if it doesn't work out? Why is it that there's so little diversity
[00:47:50.640 --> 00:47:55.120]   in legal systems around the world today? Yeah. No, but it seems to me that Robin's
[00:47:55.120 --> 00:48:01.200]   answer sounds like a plausible answer. When I wrote the book, normally, when people talk about
[00:48:01.200 --> 00:48:06.640]   comparative law, they're talking about Japanese versus European versus US, et cetera. And I was
[00:48:06.640 --> 00:48:12.240]   deliberately not doing that, that the closest I came to our system was 18th century English criminal
[00:48:12.240 --> 00:48:16.160]   law. And that was that I'd written an article on it. There was a bunch of really interesting stuff
[00:48:16.160 --> 00:48:21.040]   there. But that's really too close to our system for what I was trying to do in that book.
[00:48:21.040 --> 00:48:27.360]   So, no, you're right, that the modern legal systems all look like variants on the same thing.
[00:48:28.000 --> 00:48:34.800]   And I guess there is some pressure for consistency in the sense that if you're going to be doing
[00:48:34.800 --> 00:48:38.640]   business with people in another country, you'd like their legal system and your legal system
[00:48:38.640 --> 00:48:43.680]   to be similar enough so you can make sense of them. But I think a lot of it has got to be
[00:48:43.680 --> 00:48:48.320]   ideology. And a lot of that is because all of these are state-made legal systems.
[00:48:48.320 --> 00:48:54.560]   So one of the interesting questions is, if you look at non-state legal systems,
[00:48:54.560 --> 00:49:01.280]   which there are lots in the modern world, that if you think of a university's rules for its students,
[00:49:01.280 --> 00:49:09.600]   a large firm's rules, and so forth, I wonder if you actually looked at those. Or to take an
[00:49:09.600 --> 00:49:14.320]   example, I should say there are two chapters in that book that I didn't write, that friends of
[00:49:14.320 --> 00:49:21.120]   mine, and their names are on the book. But basically, I knew people who had written interesting
[00:49:21.120 --> 00:49:26.240]   books on other legal systems, so I asked each of them to do a chapter on it. And that's the
[00:49:26.240 --> 00:49:33.520]   book, the chapter by David Skarbek is on prison gangs, and the chapter by Peter Leeson,
[00:49:33.520 --> 00:49:40.160]   which is on 18th century pirates. And if you look at those systems, they aren't all that
[00:49:40.160 --> 00:49:48.080]   similar to the state ones, if I think about those two at least. So I think, my guess is that if you
[00:49:48.080 --> 00:49:53.040]   were looking at non-state legal systems, you'd find a lot more diversity, but I could be wrong.
[00:49:53.040 --> 00:49:59.840]   And I suppose that is certainly Jewish law, rabbinic law still exists, not as a state system,
[00:49:59.840 --> 00:50:05.680]   Israel doesn't really have rabbinic law. But it exists for certain areas, as a
[00:50:05.680 --> 00:50:13.120]   enforced by people believing in a religion kind of system. And I suspect, again, that if you
[00:50:13.120 --> 00:50:19.360]   looked at those, you would find a good deal less uniformity. Interesting. Although even when I
[00:50:19.360 --> 00:50:26.160]   consider universities, for example, if I got out the rulebook for my university, Texas, versus the
[00:50:26.160 --> 00:50:31.520]   University of Tennessee, or just name any random university, if I redacted the actual name of the
[00:50:31.520 --> 00:50:36.880]   university there, I doubt I would be able to tell which one is which. The question is, is either of
[00:50:36.880 --> 00:50:44.000]   those very close to either criminal or tort law in the US? And I don't know about those cases,
[00:50:44.000 --> 00:50:54.080]   but I wouldn't assume that it was. Because, but I think it is probably true that the sort of ruling
[00:50:54.080 --> 00:51:02.000]   groups in all of the developed world know each other and talk to each other and not wholly
[00:51:02.000 --> 00:51:06.000]   unreasonably say, well, you know, that country's got this rule seems to work for them, maybe we
[00:51:06.000 --> 00:51:12.480]   should do it too. I mean, there's certainly a number of cases where there's been a sort of
[00:51:12.480 --> 00:51:17.840]   a deliberate campaign to do that. The one that struck me partly because of some online discussions
[00:51:17.840 --> 00:51:26.560]   is age of consent to sexual intercourse. That if you look at the situation, even 50 years ago,
[00:51:26.560 --> 00:51:33.200]   I think, what the age of consent was varied a lot from country to country. And at this point,
[00:51:34.080 --> 00:51:38.720]   there are countries where it's different, but almost everybody, it's something between 16 and
[00:51:38.720 --> 00:51:47.680]   18. And similarly, for US states, the US states, I think Hawaii was the last one that conformed,
[00:51:47.680 --> 00:51:54.160]   I think it was 14 in Hawaii until maybe 20 years ago or something. So there, I think,
[00:51:54.160 --> 00:52:00.000]   I think in those cases, there's actually an effort by people who think that it's bad for young people
[00:52:00.000 --> 00:52:07.840]   to have sex, more or less, or it happens under bad circumstances, and then push the idea and
[00:52:07.840 --> 00:52:13.280]   nobody else is all that interested in it. But I think the more general similarity, things you were
[00:52:13.280 --> 00:52:20.640]   talking about, I think, are more a matter of a single global culture, as it were, among the
[00:52:20.640 --> 00:52:29.520]   people who among law professors, and that I'm pretty sure that when I get messages telling me
[00:52:29.520 --> 00:52:34.320]   that people been looking at something I wrote, a lot of them are people elsewhere in the world,
[00:52:34.320 --> 00:52:39.600]   people who are not in the US or not in England. And sometimes they're looking at something I wrote
[00:52:39.600 --> 00:52:44.880]   about law, sometimes it's something unrelated to that, of course. Interesting. I wonder to what
[00:52:44.880 --> 00:52:50.800]   extent this has occurred to me, but maybe it is just that we have had spectacularly successful
[00:52:50.800 --> 00:52:55.360]   empires in the recent past. When you think of Napoleon, you know, conquering Europe and
[00:52:55.360 --> 00:53:00.960]   creating a rational legal system, or, you know, the countries dominated by England, maybe they
[00:53:00.960 --> 00:53:06.240]   were able to just enforce a legal system that became prevalent. That's interesting. That is,
[00:53:06.240 --> 00:53:11.840]   there are really two different kinds of countries dominated by England. There are the ones that
[00:53:11.840 --> 00:53:19.120]   ended up with a more or less English speaking and to a significant degree, English ethnic
[00:53:19.680 --> 00:53:26.880]   population. That's the US, Canada, Australia, New Zealand. I don't think I missed any. And
[00:53:26.880 --> 00:53:32.320]   then there are countries like India, or a good deal of Africa, which were part of the British
[00:53:32.320 --> 00:53:36.480]   Empire. And in fact, India was affected quite a lot. I mean, as you of course know, one of the
[00:53:36.480 --> 00:53:43.760]   two official languages in India is English. And but nonetheless, it did not it had its own culture.
[00:53:44.560 --> 00:53:51.040]   And although it was, you know, to some extent, maintain the British influence, it would not be
[00:53:51.040 --> 00:53:55.280]   astonishing if its legal system was quite different. And I think there are at least,
[00:53:55.280 --> 00:53:59.600]   I don't know a lot about Indian law, but I think there are significant ways in which it does not
[00:53:59.600 --> 00:54:09.600]   fit the Anglo American pattern. Gotcha. Yeah, so I want to ask you now, because we're rapidly moving
[00:54:10.160 --> 00:54:16.880]   to your desktop. So I want to ask you about your, you know, your theory of how property rights and
[00:54:16.880 --> 00:54:22.880]   the norms to respect them arose. Feel free to explain that theory, as you answer this question,
[00:54:22.880 --> 00:54:30.960]   which is, why do we not apply the same commitment strategy against the government violating our
[00:54:30.960 --> 00:54:37.120]   rights that we do to other individuals violating our rights? Because any individual who did that
[00:54:37.120 --> 00:54:43.120]   would be worse off as a result is the simple answer. But basically, what you're discussing
[00:54:43.120 --> 00:54:49.360]   is an article of mine called a positive account of property rights. And it was really about rights
[00:54:49.360 --> 00:54:54.400]   in general, I just happened to write it for a conference on property rights. And the same
[00:54:54.400 --> 00:55:00.320]   material appears in the third edition of machinery of freedom and somewhat slightly different form.
[00:55:00.320 --> 00:55:10.480]   And the basic argument is that starts out with with other species, it starts out with territorial
[00:55:10.480 --> 00:55:15.600]   behavior in animals, which is a well established pattern of a fair number of different species.
[00:55:15.600 --> 00:55:21.280]   And the territorial behavior consists of an individual species, somehow marking the territory
[00:55:21.280 --> 00:55:29.120]   he's claiming. And then somehow turning a switch in his brain, so that he will fight more and more
[00:55:29.120 --> 00:55:35.520]   desperately against a trespasser of his own species and sex, who the farther into the territory he
[00:55:35.520 --> 00:55:42.400]   comes. And the critical fact is that unless the trespasser is much stronger, a fight to the death
[00:55:42.400 --> 00:55:50.400]   is a loss for both sides. And therefore, if one side can somehow commit himself to, I will do
[00:55:50.400 --> 00:55:56.160]   whatever it takes to fight you, then the other side backs off. And I should say, this is something I
[00:55:56.160 --> 00:56:01.600]   ultimately got from interacting with Earl Thompson, who's no longer alive, but he was a UCLA
[00:56:01.600 --> 00:56:09.120]   economist, who was against in a little bit like Robin in the sense of a very bright,
[00:56:09.120 --> 00:56:15.920]   very original kind of person, and a lot of fun. And I discussed him on my blog a little bit,
[00:56:15.920 --> 00:56:21.680]   and a little bit more in something I've been writing. But the idea of commitment strategies
[00:56:21.680 --> 00:56:26.720]   is really important. And you then say, well, what's the human equivalent? The human equivalent
[00:56:26.720 --> 00:56:33.200]   is mostly not geographical boundaries. It's boundaries in right space. It's what are the
[00:56:33.200 --> 00:56:40.960]   patterns of behavior that, what are the lines such that if somebody else crosses them,
[00:56:40.960 --> 00:56:46.320]   I am willing to bear unreasonably large costs to fight him. That's the way of thinking about it.
[00:56:46.960 --> 00:56:53.200]   And that then gets into the idea of shelling points, the idea that you can't just set your
[00:56:53.200 --> 00:56:57.600]   boundary anywhere. You have to set your boundary at something which both sides will see as a
[00:56:57.600 --> 00:57:05.440]   natural division. So I talk about things like bank robbers who have to split the loot, and each one
[00:57:05.440 --> 00:57:08.960]   thinks he deserves more than half of it, but they don't want to keep arguing about it until the
[00:57:08.960 --> 00:57:14.960]   police show up. So they agree on 50/50, that 50/50 seems like a natural division, like what's called
[00:57:14.960 --> 00:57:20.240]   a shelling point. And I discuss that at some length, as I say, in both the article, which you
[00:57:20.240 --> 00:57:30.320]   can get from my webpage, or the chapter in my third edition of Machinery. And that then what
[00:57:30.320 --> 00:57:34.080]   makes civil order, the reason we aren't in a Hobbesian anarchy where everybody's fighting
[00:57:34.080 --> 00:57:39.120]   everybody else, is that we have reasonably consistent pictures of each other's commitment
[00:57:39.120 --> 00:57:46.160]   strategies. So that I know that if I am your neighbor, and I come and say, I've decided to
[00:57:46.160 --> 00:57:51.440]   dump my trash over the fence onto your property, and look, doing anything about it legally would
[00:57:51.440 --> 00:57:58.000]   be more trouble than just disposing of the trash, you aren't going to accept that. Nor are you
[00:57:58.000 --> 00:58:02.400]   going to accept that if I say, look, instead of dumping my trash, pay me $10 a month, you're not
[00:58:02.400 --> 00:58:07.520]   going to do that either. And in terms of a simple cost-benefit calculation, it looks like you ought
[00:58:07.520 --> 00:58:13.760]   to surrender, but we all have a feeling for why it would be a mistake to surrender. So in that sense,
[00:58:13.760 --> 00:58:21.760]   you get a pattern in which people are mostly not fighting each other, because each person knows
[00:58:21.760 --> 00:58:26.000]   here are the lines other people have drawn. And you can't just draw the line anywhere you want.
[00:58:26.000 --> 00:58:32.000]   I can't draw the line and say, all right, the deal is I will fight as hard as I can if you don't pay
[00:58:32.000 --> 00:58:40.960]   me $50 a month in tribute, because you won't believe me. But I will believe you if you tell me,
[00:58:40.960 --> 00:58:46.640]   try to make me pay you tribute, and I will do whatever I have to do to stop you. So that's
[00:58:46.640 --> 00:58:52.400]   sort of the basic idea. People should read the article if they want the details. And then my
[00:58:52.400 --> 00:58:57.440]   definition of government is that organization against which we drop our commitment strategies.
[00:58:58.160 --> 00:59:03.280]   And if I drop my commitments, it depends on other people seeing the difference,
[00:59:03.280 --> 00:59:08.640]   seeing that a government is a different kind of thing. If I drop my commitment strategy against
[00:59:08.640 --> 00:59:12.720]   you, other people will observe that and say, look, he's a wimp, we can push him around too.
[00:59:12.720 --> 00:59:21.360]   This is my discussion of why the UK was willing to send their only aircraft carrier
[00:59:22.880 --> 00:59:30.480]   almost to Antarctica to fight the Argentina over some barren islands that nobody cared about.
[00:59:30.480 --> 00:59:37.520]   Because if they didn't, it might be Gibraltar next, if it was clear that they were not committed
[00:59:37.520 --> 00:59:43.760]   to defend their territory, even when it didn't seem worth doing. And on the other hand,
[00:59:43.760 --> 00:59:51.840]   if you have a organization that is sufficiently powerful, and if other people don't see giving
[00:59:51.840 --> 00:59:57.840]   into it as the same thing as giving into an ordinary person, then you know that if you
[00:59:57.840 --> 01:00:03.440]   tried to maintain that commitment strategy, you would die, because the government's got a whole
[01:00:03.440 --> 01:00:12.320]   lot more power than you do. And therefore you don't. And so I think that's the basic interest.
[01:00:12.320 --> 01:00:17.360]   But I really got into that part of it, because the question I was trying to answer is what makes
[01:00:17.360 --> 01:00:25.520]   something a government? Because I call myself an anarchist. The society I describe in machinery
[01:00:25.520 --> 01:00:31.760]   as an anarcho-capitalist society has mechanisms for enforcing law. It's not just that you can
[01:00:31.760 --> 01:00:36.800]   do anything you like. Why aren't those mechanisms a government? And that was the question I was
[01:00:36.800 --> 01:00:42.000]   starting with, because some people said they are, you're not really an anarchist. And my answer is,
[01:00:42.000 --> 01:00:49.280]   because those institutions are seen by people as having the same status as private individuals,
[01:00:49.280 --> 01:00:58.080]   that the rights enforcement agencies are not a government, because if they try to extort you,
[01:00:58.080 --> 01:01:04.160]   you will in fact resist not an infinite cost, but a much larger cost than the issue seems to be worth,
[01:01:04.160 --> 01:01:08.400]   as with other people, whereas governments you don't. So that was my answer.
[01:01:08.400 --> 01:01:13.360]   Right. And in fact, I had Michael, Professor Michael Huber on my podcast, who has written
[01:01:13.360 --> 01:01:17.280]   a book called The Problem of Political Authority. Instead of making the positive account,
[01:01:17.280 --> 01:01:22.560]   he makes a normative account for, yeah, yeah, sorry. He makes a normative account for, sorry,
[01:01:22.560 --> 01:01:27.840]   for why we shouldn't distinguish between political authority and normal individuals.
[01:01:27.840 --> 01:01:32.960]   That's right. That book, he basically runs through all of the standard philosophical
[01:01:32.960 --> 01:01:37.360]   arguments for why governments have a right to do things the rest of us don't have a right to do.
[01:01:38.000 --> 01:01:42.880]   And his basic conclusion is that the only defensible one is the belief that if they
[01:01:42.880 --> 01:01:46.480]   don't have the right, catastrophic things will happen. And he then spends the second
[01:01:46.480 --> 01:01:49.040]   half of the book arguing, as I would, that it isn't true.
[01:01:49.040 --> 01:01:55.440]   Sure. And by the way, have you heard of Charles Murray's strategy for setting up a commitment
[01:01:55.440 --> 01:02:00.320]   strategy to combat government interference in his book For the People? If not, I would find this
[01:02:00.320 --> 01:02:06.480]   very interesting. Okay. So the idea is this, you have a few people who have a lot of money,
[01:02:06.480 --> 01:02:12.160]   set up a fund, and he calls this the Madison Fund. And the entire purpose is, whenever OSHA or the
[01:02:12.160 --> 01:02:18.080]   EPA comes to somebody that normal people can sympathize with, and they say, here are the
[01:02:18.080 --> 01:02:24.400]   regulations you must abide by, maybe they'll destroy your business and so on, and maybe they're
[01:02:24.400 --> 01:02:31.200]   obviously unreasonable. And in these cases, this Madison Fund would go through every possible legal
[01:02:31.200 --> 01:02:34.560]   avenue, even though they know the end result is going to be that they're going to lose the case,
[01:02:34.560 --> 01:02:40.480]   to increase the burdens on the EPA agents and on the bureaucracies within the EPA, increase the
[01:02:40.480 --> 01:02:45.280]   costs that they had to bear for enforcing this rule. And so then the government knows, listen,
[01:02:45.280 --> 01:02:48.000]   we can't go about enforcing this rule, because it's just not effective.
[01:02:48.000 --> 01:02:56.640]   The problem is that if they weren't committed to do it for any victim, it could work. That is to
[01:02:56.640 --> 01:03:02.720]   say, I presume that regulatory agencies do actually look at how hard their target is going to fight.
[01:03:03.600 --> 01:03:07.600]   But given that they're committed to doing it for everybody, the regulatory agency is going to say
[01:03:07.600 --> 01:03:13.920]   to itself, if we give in here, we're not going to be able to do our assigned job assigned by Congress.
[01:03:13.920 --> 01:03:18.240]   And therefore, we should, and after all, the federal government has an awful lot of money
[01:03:18.240 --> 01:03:23.040]   available, too, much more money than your group of wealthy people do.
[01:03:23.040 --> 01:03:28.560]   Interesting. But if you started off one regulation at a time, and in each case...
[01:03:28.560 --> 01:03:37.840]   That's right. If you could find some way so that it only applied to 1% of what the agency wanted
[01:03:37.840 --> 01:03:43.520]   to do, you might be able to pull it off. That would be an interesting project. What's the book?
[01:03:43.520 --> 01:03:49.440]   For the people, or by the people, rather. Right. And another variant he has on this is,
[01:03:49.440 --> 01:03:54.320]   instead of having somebody fund this charitably, you can have, for example, an association of
[01:03:54.320 --> 01:03:59.120]   dentists, they all pitch in $200 a month. And then the goal of the institution is,
[01:03:59.120 --> 01:04:02.720]   if the government tries to enforce this law, which we know is not effective at preventing
[01:04:02.720 --> 01:04:09.520]   harm or anything, then we will chip into the legal cost of fighting government tooth and nail
[01:04:09.520 --> 01:04:19.840]   to enforcing this rule. In a certain sense, this is how a private law system like the Somali system
[01:04:19.840 --> 01:04:28.640]   did work. That is to say, in the traditional Somali system, you have coalitions, which are
[01:04:28.640 --> 01:04:36.880]   formed partly by kinship and partly by explicit contract of a group of people. And part of the
[01:04:36.880 --> 01:04:47.200]   deal is, if somebody wrongs one of us, everybody in the coalition will help him use feud, basically
[01:04:47.200 --> 01:04:52.320]   use or threaten violence against the person who harmed him to get compensated. And when the
[01:04:52.320 --> 01:04:58.000]   compensation comes in, it gets divided among the members of the group. Typically, the victim gets
[01:04:58.000 --> 01:05:05.040]   a larger share. And it's more complicated than I'm describing. But the basic idea is that it is a
[01:05:05.040 --> 01:05:11.600]   group of people who are committed to defend each of them against wrongs by people who are members
[01:05:11.600 --> 01:05:16.800]   of other such groups. But it's even more complicated than I'm describing, because it's a
[01:05:16.800 --> 01:05:24.000]   set of nested coalitions, where if the group you're dealing with is too strong, you expand by
[01:05:24.000 --> 01:05:31.680]   taking in the set of people, the set of other such groups that are sort of part of your tree, as it
[01:05:31.680 --> 01:05:36.640]   were. So it's quite a neat system. I have a chapter on that in the Legal Systems book, and it was a
[01:05:36.640 --> 01:05:41.360]   lot of fun. As a computer science student, I really enjoyed that chapter, because it just appeals to
[01:05:41.360 --> 01:05:48.560]   my, my, it appeals to my, you know, the aesthetic of recursion, when you are looking at the most
[01:05:48.560 --> 01:05:54.160]   common ancestor or the most recent common ancestor. Anyways, okay, so I want to ask you,
[01:05:54.160 --> 01:05:58.640]   what I like about it, of course, is not the kinship part, although that's very important in
[01:05:58.640 --> 01:06:05.840]   that society, but the fact that it's also by contract, by explicit contracts, so that you can
[01:06:05.840 --> 01:06:11.840]   have what they call a pile of shields, which is a coalition, which is entirely by contract rather
[01:06:11.840 --> 01:06:20.640]   than by kinship. And apparently, that was the term they used for NATO. Then, because it wasn't like
[01:06:20.640 --> 01:06:28.560]   we were kin, but we had all agreed to right and each other. Yeah, yeah. Yeah. And okay, so I want
[01:06:28.560 --> 01:06:34.320]   to ask you, does this theory imply that the poor are the least likely to have their property rights
[01:06:34.320 --> 01:06:40.720]   safeguarded? Because the point of a commitment strategy is the other person knows that in some
[01:06:40.720 --> 01:06:44.720]   sense, you're protecting this property, which is not worth the cost of retribution. The flip side
[01:06:44.720 --> 01:06:50.080]   of that is that the poor also has less, have less valuable stuff to steal. Sure. But on one hand,
[01:06:50.080 --> 01:06:57.680]   the poor, and poor is, poor is too, is too narrow a statement. Whoever has less of the resources
[01:06:57.680 --> 01:07:04.640]   relevant for conflict, so that it might well be that a poor lawyer would be in a pretty good
[01:07:04.640 --> 01:07:09.360]   position, because conflict in our society might well be lawsuits, and his time is available for
[01:07:09.360 --> 01:07:15.920]   him. Whereas a somewhat better off person with no expertise in law might not be. And after all,
[01:07:15.920 --> 01:07:22.720]   I discussed this in the context of the Icelandic system 1000 years ago. And then while money was,
[01:07:22.720 --> 01:07:30.080]   while wealth was useful, the main thing was how many of your kin were competent warriors,
[01:07:30.080 --> 01:07:36.640]   how many of your friends were. So it was really the relevant, the relevant power in that was not
[01:07:36.640 --> 01:07:43.920]   mainly money. It was mainly alliances. But then it's true that anybody that in such a system,
[01:07:43.920 --> 01:07:50.000]   how well you are protected, partly depends on how much of whatever resources are used for
[01:07:51.040 --> 01:07:59.520]   conflict, you can command. But at the same time, how much protection you need, depends on how much
[01:07:59.520 --> 01:08:03.760]   stuff on how strong other people, other people's reason to want to violate your rights are.
[01:08:03.760 --> 01:08:11.120]   Right. But yes, I was going to go with that was not necessarily that you have less resources for
[01:08:11.120 --> 01:08:15.760]   retribution, although that's part of it. But another is, you just have less stuff that the
[01:08:15.760 --> 01:08:19.840]   aggressor could keep aggressing upon, maybe you just have one location where your property is
[01:08:19.840 --> 01:08:23.840]   housed. So that once it's taken, there's no reason for the aggressor to expect you to do
[01:08:23.840 --> 01:08:28.240]   retribution, because you have no, there's nothing that your commitment strategy is further helping
[01:08:28.240 --> 01:08:35.280]   you protect, except you've got a future. It's protecting other things you accumulate. And
[01:08:35.280 --> 01:08:42.960]   if people are too worried about that, you will then get coalitions, you will get what you had
[01:08:42.960 --> 01:08:50.880]   with kinship in Iceland, you will get groups of people who agree as in Somalia, that we will,
[01:08:50.880 --> 01:08:57.600]   if your rights are violated, we will help you fight as it were. And we will then get a share
[01:08:57.600 --> 01:09:05.600]   of the compensation. Interesting. Okay, so I want to talk now about just some general questions that
[01:09:05.600 --> 01:09:13.120]   are relevant to this discussion. One of the ideas that's been going around in the kind of people I
[01:09:13.120 --> 01:09:18.960]   talked to is this essay by this venture capitalist named Sam Altman, and it's titled Moore's Law for
[01:09:18.960 --> 01:09:23.600]   Everything. And the idea he has, which is relatively common, is that we're going to have
[01:09:23.600 --> 01:09:28.800]   stupendous levels of wealth created by automation and AI. And this wealth is going to somewhat
[01:09:28.800 --> 01:09:32.480]   arbitrarily flow to very few agents, to the detriment of everybody else, because they're
[01:09:32.480 --> 01:09:36.480]   going to lose their jobs. And then what's necessary then is some sort of wealth tax,
[01:09:36.480 --> 01:09:40.880]   or some sort of land tax, in order to redistribute this income.
[01:09:40.880 --> 01:09:46.320]   But to begin with, is it detriment to other people, or only that other people don't get the
[01:09:46.320 --> 01:09:55.760]   large gain? Because my first approximation response to all of these stories is, why can't you have a
[01:09:55.760 --> 01:09:59.840]   subset of the population who are doing the same things they were doing before with the same
[01:09:59.840 --> 01:10:04.640]   technology, trading with each other, and are about as wealthy as they were before? Now, maybe you
[01:10:04.640 --> 01:10:09.600]   can't, because maybe all the capital gets bid away from them. But they'll have whatever their own
[01:10:09.600 --> 01:10:21.760]   capital is. So it's hard to see how they are worse off than Hong Kong in 1950, than any society which
[01:10:21.760 --> 01:10:28.560]   doesn't have a lot of wealth, but has people who are productive people. So I think it's entirely
[01:10:28.560 --> 01:10:34.880]   plausible, and may already have happened, that technological change will result in some people
[01:10:34.880 --> 01:10:38.640]   being a lot better off, and some people being a little better off. So if you think about--
[01:10:38.640 --> 01:10:43.920]   there's evidence. I haven't really looked very carefully at the whole issue of whether wealth
[01:10:43.920 --> 01:10:48.080]   in the US is becoming more unequal. People claim it is. It might be true, but it's sort of
[01:10:48.080 --> 01:10:54.720]   complicated, because you've got to look at household versus individual. And if households are
[01:10:54.720 --> 01:10:58.960]   getting smaller, then household income would go down, even if individual income didn't, and so
[01:10:58.960 --> 01:11:07.120]   forth. But putting that-- supposing it's happening, one explanation is that technological progress
[01:11:07.120 --> 01:11:14.320]   means that the value of intelligence is increasing more than the value of strength. That's the sort
[01:11:14.320 --> 01:11:20.000]   of simplest way of putting it. And therefore, even if the value-- even if strength is still useful,
[01:11:21.040 --> 01:11:27.680]   it would then be the case that smart techies will get a lot richer than other people.
[01:11:27.680 --> 01:11:31.680]   And that could easily be happening, and maybe what has been happening. I'm not-- not just techies,
[01:11:31.680 --> 01:11:37.840]   doctors, lawyers, people, other people of that sort. So that may be what has been happening
[01:11:37.840 --> 01:11:42.960]   over the last 20, or 30, or 40 years. I'm not sure. And that could certainly keep happening.
[01:11:42.960 --> 01:11:49.040]   But-- so in this-- I mean, if it was just that the wealthier get more-- can produce more,
[01:11:49.040 --> 01:11:54.080]   that would, in fact, be good for the poor people. Because now, if more can be produced because of AI,
[01:11:54.080 --> 01:11:57.600]   then it's cheaper for them to buy the things that they would want to buy. I'm talking about
[01:11:57.600 --> 01:12:02.400]   a scenario where automation makes their jobs useless. And we're talking about large segments
[01:12:02.400 --> 01:12:10.240]   of the population. I don't understand. Their jobs are useless only in the sense that there is
[01:12:10.240 --> 01:12:24.960]   another way of doing the same things. But that other way is-- costs are relative, right? Why--
[01:12:24.960 --> 01:12:32.000]   they can still do the same things as before in an hour of time. So why can't they still
[01:12:32.000 --> 01:12:38.000]   produce the amount of stuff? So you can imagine stories where they can't. Stories where somehow
[01:12:38.000 --> 01:12:45.280]   the people who can produce enormously more are no longer available to them to work with them
[01:12:45.280 --> 01:12:51.040]   in producing their things. But it's not obvious that you would expect that to happen. And at the
[01:12:51.040 --> 01:12:58.640]   same time, the usual sort of principle comparative advantage rule is that the more different the
[01:12:58.640 --> 01:13:03.520]   people you're trading with are, the greater the gains to both sides of trading. So if the result
[01:13:03.520 --> 01:13:12.480]   is that certain things, individuals who can do them, are 100 times as productive and certain
[01:13:12.480 --> 01:13:20.240]   things only twice as productive, then someone who can't do those things can do the twice as
[01:13:20.240 --> 01:13:28.560]   productive ones and trade for the other stuff and get much more than they could before. So it doesn't
[01:13:28.560 --> 01:13:34.560]   seem to me that it's the pattern you would expect, except if what matters to you is relative income.
[01:13:34.560 --> 01:13:38.320]   If what matters to you is relative income, then it's the pattern you would expect. But if it's
[01:13:38.320 --> 01:13:46.160]   absolute income, it isn't impossible. But again, I don't see why can't the non-techies keep running
[01:13:46.160 --> 01:13:54.320]   a non-techie economy, as it were, for themselves and then have that non-techie economy trade with
[01:13:54.320 --> 01:13:59.840]   the techie economy on much more profitable terms than it could have traded with anybody before,
[01:13:59.840 --> 01:14:03.840]   because the techie economy, even if it's better at some things, it's much more better at some
[01:14:03.840 --> 01:14:08.560]   things than others. That seems to me the right first guess on that. I should write a blog post.
[01:14:08.560 --> 01:14:14.400]   That's an interesting idea. But then, I mean, would that analysis apply? Could you say a horse--
[01:14:14.400 --> 01:14:19.360]   well, a horse will retain some comparative advantage over a car, but that doesn't-- if
[01:14:19.360 --> 01:14:23.600]   we're talking like 100 years ago, right? Just consider a horse versus a car. It's not as if
[01:14:23.600 --> 01:14:29.520]   when the car came in, by definition, the horse has some comparative advantage in terms of some
[01:14:29.520 --> 01:14:31.920]   benefit you can get out of it. A horse has a much more restricted
[01:14:31.920 --> 01:14:37.680]   set of things it can do than a human being does, and a horse doesn't own himself.
[01:14:37.680 --> 01:14:42.400]   Right. So to the extent that AI can do a lot more of the things that humans can do,
[01:14:42.400 --> 01:14:47.120]   then could it get worrying in that sense? Well,
[01:14:51.040 --> 01:14:57.040]   I don't know. I should think more about this question, but I'm certainly familiar with the
[01:14:57.040 --> 01:15:00.400]   kind of scenario people talk about. I don't know what particular author you're talking about,
[01:15:00.400 --> 01:15:10.880]   but it does seem to me that it seems to be much more plausible that the poor people will be better
[01:15:10.880 --> 01:15:16.160]   off, just not nearly as much better off, because there have got to be things that
[01:15:18.080 --> 01:15:23.040]   the computer is not better at. People might like personal servants, and personal servants
[01:15:23.040 --> 01:15:29.280]   are-- it's very hard to make a computer a real full substitute for a personal servant. They might
[01:15:29.280 --> 01:15:39.680]   like people to-- nannies for their kids. They might like live entertainment as something people
[01:15:39.680 --> 01:15:44.560]   value. It seems to me there are going to be an awful lot of things, even if like 80% or 90% of
[01:15:44.560 --> 01:15:50.160]   things computers can do much better. If there's 10% or 20% things they can't do, even if they can
[01:15:50.160 --> 01:15:57.200]   do a little bit better, better is a relative term, and you can then keep doing those things and trade
[01:15:57.200 --> 01:16:04.000]   them for the others. Interesting. Okay. So I want to be respectful of your time. Just two more
[01:16:04.000 --> 01:16:09.440]   questions. First of all, what have you learned about economics from the Society for Creative
[01:16:09.440 --> 01:16:16.080]   Anachronism? Huh. I've gotten a little bit more-- a more sympathetic attitude to gift cultures,
[01:16:16.080 --> 01:16:23.600]   because-- now, I also pick up that from reading Icelandic stuff, that is medieval Icelandic stuff,
[01:16:23.600 --> 01:16:31.760]   but it is-- I have, in a sense, I should have-- I should have realized it earlier,
[01:16:31.760 --> 01:16:37.200]   even without that, because there are certain parts of our ordinary life which are gift cultures,
[01:16:37.200 --> 01:16:43.440]   and the standard example I give is that you have some friends over for dinner, and they feel as
[01:16:43.440 --> 01:16:46.640]   though they ought to have you over for dinner, and if they can't have you over for dinner, they should
[01:16:46.640 --> 01:16:50.560]   take you out to a restaurant, and it would never occur to them to say, "Well, you took them out for
[01:16:50.560 --> 01:16:58.320]   dinner. Why don't we pay you 20 bucks and we're even?" So that's a gift culture embedded in our
[01:16:58.320 --> 01:17:06.480]   ordinary system, but certainly in the SCA, partly because I'm playing the role of a medieval person
[01:17:07.040 --> 01:17:13.040]   and at least in some medieval cultures, you have the idea that nobles don't work for money,
[01:17:13.040 --> 01:17:22.960]   that it's sort of-- anyway, but also sort of the feel of it is that I feel as though it makes sense
[01:17:22.960 --> 01:17:29.200]   to do favors and not to do things for money. Now, it isn't entirely true. I have-- my wife and I
[01:17:29.200 --> 01:17:38.400]   have a medieval cookbook, and we sell that, but generally when I-- long before that, I put together
[01:17:38.400 --> 01:17:44.320]   a collection of source material on medieval cooking, which I reduction Xeroxed four pages to
[01:17:44.320 --> 01:17:51.360]   a page to make it really cheap, and I was selling that for cost, and it somehow didn't seem appropriate
[01:17:51.360 --> 01:17:56.400]   in terms of my interaction with other people's hobby to be doing it for money, and similarly,
[01:17:56.400 --> 01:18:04.000]   one of my hobbies is lapidary work, cutting semi-precious gemstones, and I do it for fun.
[01:18:04.000 --> 01:18:08.160]   I cut more stones. I also do some jewelry making, but not very much. I cut more stones than I have
[01:18:08.160 --> 01:18:14.640]   a use for, so quite a while ago, I put up an offer on Facebook to other SCA people that if somebody
[01:18:14.640 --> 01:18:20.000]   was making jewelry actually based on medieval jewelry, I would donate gemstones for them,
[01:18:20.000 --> 01:18:26.080]   and again, it just felt more nearly right to me to do that as a donation than to say I will sell
[01:18:26.080 --> 01:18:30.880]   them to you, and I'm not quite sure why, but except in terms of my emotional feelings that
[01:18:30.880 --> 01:18:38.320]   having participated in gift cultures, it's making it easier to have some feel for why people--
[01:18:38.320 --> 01:18:42.880]   but I still don't really understand economically. I mean, a very interesting question maybe someone
[01:18:42.880 --> 01:18:47.600]   else has looked at more carefully, why you sometimes have gift cultures and why you don't,
[01:18:47.600 --> 01:18:54.800]   why it is-- yeah, anyway. Yeah, I would love to read an explanation of that just as
[01:18:54.800 --> 01:18:58.640]   you made an explanation for property rights. A similar thing would be very interesting
[01:18:58.640 --> 01:19:06.320]   because to forego the convenience of money as an exchange for keeping track of favors,
[01:19:06.320 --> 01:19:12.640]   it's very interesting why that's done. Final question, given the futurism that you've done
[01:19:12.640 --> 01:19:18.720]   and the different technologies you've looked at, if you were 20 today, what field would you decide
[01:19:18.720 --> 01:19:25.680]   to go into in a hope to either increase innovation in that space or maybe to make a profit and start
[01:19:25.680 --> 01:19:31.600]   a successful company? Now, I'm not sure I'd be very good at starting up a successful company.
[01:19:31.600 --> 01:19:39.680]   I mean, the obvious interesting area at the moment is sort of encryption and stuff and
[01:19:39.680 --> 01:19:45.200]   cryptocurrencies and so forth, but the problem is this is the usual problem because an interesting
[01:19:45.200 --> 01:19:51.600]   area, a whole lot of smart people go into it. So I started out getting a doctorate in theoretical
[01:19:51.600 --> 01:20:00.000]   physics and part of what happened was that I was less enthusiastically interested in the subject
[01:20:00.000 --> 01:20:04.560]   than some of the other people, but I was smarter than the people I was interacting with as a
[01:20:04.560 --> 01:20:09.280]   graduate, even as a graduate student at Chicago and that was enough of an advantage. Then I was
[01:20:09.280 --> 01:20:13.120]   a postdoc at Columbia Physics Department and I wasn't smarter than the people I was interacting
[01:20:13.120 --> 01:20:18.240]   with. They were the professors and they were much more interested and it occurred to me that what
[01:20:18.240 --> 01:20:23.600]   had happened in theoretical physics was that because at the time I went into it, it was sort
[01:20:23.600 --> 01:20:28.320]   of the elite field that was being subsidized and everybody knew was the high status important thing,
[01:20:28.320 --> 01:20:33.520]   although it hasn't really produced as much since then as it was expected to, that therefore it was
[01:20:33.520 --> 01:20:40.640]   overpopulated with high IQ people and that was one part of the reason that I eventually left that
[01:20:40.640 --> 01:20:46.400]   field and became an economist instead because there are certainly some very smart people in
[01:20:46.400 --> 01:20:52.640]   economics, but there weren't as many of that kind of people as it were. It wasn't as dense,
[01:20:52.640 --> 01:20:57.520]   so it was more likely that I was smart enough to say original and interesting things in that field.
[01:20:57.520 --> 01:21:03.200]   So again, the temptation, partly because computer programming is a lot of fun, I've done it,
[01:21:03.200 --> 01:21:08.640]   though I'm not professionally, it would be tempting to go into computer programming and in effect,
[01:21:08.640 --> 01:21:14.240]   I guess the thing I would enjoy, the thing I like thinking about, and I don't know if I can pull it
[01:21:14.240 --> 01:21:21.280]   off or not, would be independent computer programming to do the equivalent of inventing
[01:21:21.280 --> 01:21:30.080]   Tetris and maybe to do something where you can carry it far enough so somebody will pay you a
[01:21:30.080 --> 01:21:36.400]   whole lot of money to take over your project and then doing it really right. Making machines that
[01:21:36.400 --> 01:21:41.760]   work is something that human beings really enjoy and it's a whole lot easier with programming than
[01:21:41.760 --> 01:21:51.120]   it is with engineering. And I have ideas. I have ideas for computer programs to help teach economics,
[01:21:51.120 --> 01:21:56.240]   three of which went with my price theory textbook I actually wrote, but I've got lots of ideas for
[01:21:56.240 --> 01:22:02.800]   more. And I think what would be fun, assuming I could support myself doing it or maybe do it
[01:22:02.800 --> 01:22:09.760]   along with some other things, would be to be taking clever ideas like what I was saying before about
[01:22:09.760 --> 01:22:16.240]   how you ought to be running, how you ought to do computer dating, things like that, and
[01:22:16.240 --> 01:22:24.080]   some mixture of writing the programs myself for simple things and getting other people involved
[01:22:24.080 --> 01:22:31.280]   in such projects where the project requires more than one person for common. And that's probably
[01:22:31.280 --> 01:22:36.480]   the activity that most strikes me as being fun. Now, I've also written a few novels and it's
[01:22:36.480 --> 01:22:41.360]   possible that if I had started writing novels very early, I would have been good enough to
[01:22:41.360 --> 01:22:45.680]   sort of be able to support myself and do good stuff. But my guess is not that my impression
[01:22:45.680 --> 01:22:52.800]   is that the people who are really good at that have talents I don't have. And so I think I would
[01:22:52.800 --> 01:22:58.720]   only do that as I do it for my own fun. I write novels that some people enjoy, so it's worth
[01:22:58.720 --> 01:23:04.160]   doing. But even if I compare it to my younger son, who is a would-be novelist, and he just thinks
[01:23:04.160 --> 01:23:13.120]   about it a whole lot more carefully than I do and has, I sort of do it by intuition. And he does it
[01:23:13.120 --> 01:23:18.240]   by some mixture of intuition and theories of how to write novels. And he's much better than I am.
[01:23:18.240 --> 01:23:19.680]   Yeah.
[01:23:19.680 --> 01:23:20.240]   Which is nice.
[01:23:20.240 --> 01:23:25.040]   Well, anyways, Dr. Friedman, this is a great pleasure. Thank you so much for your time.
[01:23:25.040 --> 01:23:33.840]   It was fun. Thank you. I enjoyed it too. Bye-bye.
[01:23:33.840 --> 01:23:53.040]   [music]

