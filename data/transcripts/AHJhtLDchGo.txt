
[00:00:00.000 --> 00:00:26.600]   Hey, everybody. I hope I'm live now and I haven't made any mistake. If you're there,
[00:00:26.600 --> 00:00:33.880]   it's very hard for me. I think this is just a new setup for me that's different from fast
[00:00:33.880 --> 00:00:38.200]   book or paper reading groups that I've had before. But if you are there, then if you
[00:00:38.200 --> 00:00:44.720]   could please go to that link, 1dpe.me/resnet. So let me see if I can paste that in the chat
[00:00:44.720 --> 00:00:55.080]   somehow. There we go. And I click send. If everybody could go to this link, 1dpe.me/resnet,
[00:00:55.080 --> 00:01:15.000]   and I'll go there too in a tick. And it would be wonderful. I'd really, really appreciate
[00:01:15.000 --> 00:01:21.520]   if we could please carry on all the conversation here. And I'm making this request more than
[00:01:21.520 --> 00:01:25.160]   normal. Like I'm making this request more than fast book or paper reading because I
[00:01:25.160 --> 00:01:29.800]   really have to go in to this live streaming tab and then have a look at the chat. And
[00:01:29.800 --> 00:01:39.120]   it's really been hard for me to do all of that. So if I go in here, could we please
[00:01:39.120 --> 00:01:53.120]   make sure to carry on? Okay. So that's that. So then I guess there's already this live
[00:01:53.120 --> 00:02:00.240]   streaming link. I've tweeted about it. And what we're doing today is then this, we're
[00:02:00.240 --> 00:02:05.800]   training ResNet, we're coding ResNet from scratch. And when I say we're coding ResNet
[00:02:05.800 --> 00:02:14.000]   from scratch, I actually mean it from scratch because you're going to see me make errors
[00:02:14.000 --> 00:02:20.280]   today. I'm pretty sure of that. And you're also going to see me, like I haven't prepared
[00:02:20.280 --> 00:02:24.640]   much for this. When I say I haven't prepared much, I mean, I haven't prepared anything.
[00:02:24.640 --> 00:02:29.280]   Like usually you'd come into an event and you'd have done a week of preparation and
[00:02:29.280 --> 00:02:34.640]   then you'd host the event. But times are such for me that I haven't had a lot of time. And
[00:02:34.640 --> 00:02:42.720]   I haven't had a lot of time on my plate, but the time that I've had is I thought I'd do
[00:02:42.720 --> 00:02:48.800]   like a live coding session on ResNet. And what we're going to do today is we'll go through
[00:02:48.800 --> 00:02:54.560]   the paper and we'll match everything in the paper and we'll try and code it from scratch.
[00:02:54.560 --> 00:03:00.520]   So you will see me do things that you won't normally see otherwise, which is both a good
[00:03:00.520 --> 00:03:06.160]   and a bad thing, I think. So, for example, right now, what I'm doing is I don't need
[00:03:06.160 --> 00:03:16.920]   that. Sorry, one second. For example, right now, I'm just setting up my SSH, which is
[00:03:16.920 --> 00:03:25.040]   something you won't see. So I just need to stop share for a quick one second just to
[00:03:25.040 --> 00:03:30.240]   get the SSH key, which is private, which I can't share publicly. So just give me one
[00:03:30.240 --> 00:03:57.040]   second. I'm hoping we're still live.
[00:03:57.040 --> 00:04:17.520]   Almost got it. Okay. Should be fine now. How would the Zoom go? There you are. Okay. Can
[00:04:17.520 --> 00:04:38.680]   everybody still can everybody see my screen again? If you can, that's great. So if I go
[00:04:38.680 --> 00:04:39.680]   in here and I just paste that. Now, if I say SSH, Asia, that should let me through, I hope.
[00:04:39.680 --> 00:04:44.800]   It does. Cool. So I've got my instance set up. I could have also done Google Cloud Platform,
[00:04:44.800 --> 00:04:58.240]   but I'm just doing it on GCP at the moment. So I could just now start a Tmux inside, and
[00:04:58.240 --> 00:05:04.760]   then I could just run my Jupyter notebook on a specific board. And then if I go to that
[00:05:04.760 --> 00:05:17.200]   board, that should let me run my Jupyter. Hopefully. Okay. It's asking for security
[00:05:17.200 --> 00:05:24.880]   tokens. I can just copy paste this in there and hopefully that works. It does work. And
[00:05:24.880 --> 00:05:34.520]   now I can just go in to say maybe Git Repos. I can press control B and apostrophe to create
[00:05:34.520 --> 00:05:38.440]   another tab. So now this is the benefit of Tmux. And because we're live coding and this
[00:05:38.440 --> 00:05:43.880]   is basically a coding heavy session, I'm going to show some of these tricks that I use that
[00:05:43.880 --> 00:05:53.560]   I pretty much learned from Jeremy, actually, the first time and they've stayed. So I don't
[00:05:53.560 --> 00:06:04.200]   know if you can see it. So I've just got now my four copy space to set them up properly.
[00:06:04.200 --> 00:06:09.160]   Yeah, that should be fine. And now I can just go to CD Git Repos and I can say create a
[00:06:09.160 --> 00:06:17.680]   new directory, which is say live coding PRG. And then I can go to that live coding PRG
[00:06:17.680 --> 00:06:25.200]   and I can say create ResNet. And now I can go to that over here. So I can go live coding
[00:06:25.200 --> 00:06:37.640]   PRG, go ResNet and I can create my first notebook. So I create my first notebook. That brings
[00:06:37.640 --> 00:06:44.800]   me here and I can call this ResNet. Okay. So we're set up. So I can now import things
[00:06:44.800 --> 00:06:54.440]   here. I hope I can import Torch. I can import FastAI. Not that I need it. I'm just checking
[00:06:54.440 --> 00:07:02.160]   everything's working as normal. Can you bump up your font size and touch? Of course. Sure.
[00:07:02.160 --> 00:07:10.960]   How about that? Does that make things better? Done that. That should be better, I hope.
[00:07:10.960 --> 00:07:17.760]   All right. So I'm just going to now close this, close this, close this. Check here.
[00:07:17.760 --> 00:07:24.240]   Okay. There's still people here. So everything's going fine. I hope. All right. So we're just
[00:07:24.240 --> 00:07:29.480]   going to carry on with our conversation over here. And now all I need to do, increase the
[00:07:29.480 --> 00:07:38.120]   font size, please. I'm really sorry. I further increased it more. That would be too much
[00:07:38.120 --> 00:08:07.960]   by zooming a lot. Hopefully it's fine.
[00:08:08.800 --> 00:08:16.400]   Thanks. It is good now. Okay. Great. Oh, Ellen, are you joining us from Australia? And it
[00:08:16.400 --> 00:08:31.480]   is, isn't it like what time would it be there? 11, 3 o'clock. Are you joining us at 3am?
[00:08:31.480 --> 00:08:40.440]   If you are, I'm really, really thankful. And also really sorry for moving to a non-Australian
[00:08:40.440 --> 00:08:47.960]   friendly time zone. All right. So now then what we're going to do is we're going to code
[00:08:47.960 --> 00:08:54.160]   ResNet from scratch. And my go-to place, oh, actually, I don't have my visual code set
[00:08:54.160 --> 00:08:57.840]   up. So I need to set up Visual Studio Code as well. That's another thing I want to set
[00:08:57.840 --> 00:09:08.240]   up. And for that, I'll have to just stop my screen share again for a tick. So give me
[00:09:08.240 --> 00:09:16.920]   one second. So if I go, I don't know why I keep closing my video actually. Should be
[00:09:16.920 --> 00:09:23.960]   fine now. So I'll have to do now. I just need to set up, go to my GCP private account, find
[00:09:23.960 --> 00:09:34.360]   the external link, which is just coming up in a second. And I really need to fix the
[00:09:34.360 --> 00:09:38.920]   lighting. I think I haven't hosted much events in India, so I just need to, I'm still finding
[00:09:38.920 --> 00:09:48.800]   my feed. If that makes sense. All right. Oh, I'm sharing this, which I shouldn't have.
[00:09:48.800 --> 00:10:09.800]   Nevermind. We'll cut that off. And I'm going in this instance, I could copy paste my host
[00:10:09.800 --> 00:10:18.520]   name. And now if I say connect to host, and I say that particular host for which we just
[00:10:18.520 --> 00:10:26.880]   pasted the key in, and I click continue, that should work, I hope. And let's see what this
[00:10:26.880 --> 00:10:37.040]   is saying here. Okay. Yes, 3M, I'm dedicated. That's great to hear. Thanks for that. Next
[00:10:37.040 --> 00:10:42.240]   is this, I needed this session for ResNet. Oh, great. I'm really, no, thanks for everybody
[00:10:42.240 --> 00:10:46.920]   for joining. Will you also take up coding from scratch and setting up the EC2 instance?
[00:10:46.920 --> 00:10:52.240]   Jeremy did discuss it very briefly. It's not the, that's not something I do plan to do
[00:10:52.240 --> 00:10:59.160]   as part of this session about the EC2 instance. But if needed, we can have a look in the future
[00:10:59.160 --> 00:11:07.000]   session. So what I want to do is, going forward, like we, we're doing fast, we're on the side,
[00:11:07.000 --> 00:11:11.200]   and we're doing paper reading group on the side, which is more, more theory, like less
[00:11:11.200 --> 00:11:17.520]   coding and something that's been on my mind is like, we want to do more beginner friendly
[00:11:17.520 --> 00:11:24.400]   coding sessions and Sam's, Sam's PyTorch group, if you haven't seen that, I think that's a,
[00:11:24.400 --> 00:11:31.080]   if you go to our forums, at written masses, oh, by the way, they're out of beta. So Sam's
[00:11:31.080 --> 00:11:37.080]   PyTorch group reading group. Yeah, that's the one. So I think Sam Bhattani, my colleague,
[00:11:37.080 --> 00:11:41.360]   he's hosting this PyTorch reading group, which takes you in PyTorch. So I think this is another
[00:11:41.360 --> 00:11:46.960]   great place to get started with PyTorch. And then in this session, I'll be using PyTorch.
[00:11:46.960 --> 00:11:54.160]   So we're connected to our instance now. So then the one repository that has, that that's
[00:11:54.160 --> 00:12:02.480]   always, that's always, always helpful is PyTorch image models. I've been a contributor to this
[00:12:02.480 --> 00:12:08.600]   repository, although I will admit I haven't been very up to date with what Ross has been
[00:12:08.600 --> 00:12:20.400]   up to. But say a couple months ago, I did contribute to this repository. When I said
[00:12:20.400 --> 00:12:26.840]   I did contribute, I contributed a couple models, which is Convid and ResNet RS. But there it
[00:12:26.840 --> 00:12:31.480]   is. Like this, this, this repository has everything. So it's got like, if you go in PyTorch team
[00:12:31.480 --> 00:12:36.800]   models, there's like an implementation of hundreds of models. So I don't really need
[00:12:36.800 --> 00:12:42.040]   to look elsewhere. And I trust Ross's implementation because it's tried and tested. And then he
[00:12:42.040 --> 00:12:46.720]   adds pre-trained weights and everything. So I'm going to be referencing this library a
[00:12:46.720 --> 00:12:52.400]   lot, like I always do. So I'm just going to go here and I open that repository. So if
[00:12:52.400 --> 00:13:02.880]   I say PyTorch image models, there we go. And that should open up that repository. So I
[00:13:02.880 --> 00:13:12.160]   was last, I was looking at MLP mixer. And I need to zoom in a bit, I hope, I think. And
[00:13:12.160 --> 00:13:19.880]   now all I need to do is Git pull. Okay. So what Git pull does is it makes sure that I
[00:13:19.880 --> 00:13:26.800]   have the latest version of the repository. Yeah, that's, that looks fine. That was the
[00:13:26.800 --> 00:13:32.640]   last commit. So we're at the latest version, which is great. And now I can go in team,
[00:13:32.640 --> 00:13:42.440]   I can go in models and I can go in ResNet. And there it is. I have an implementation
[00:13:42.440 --> 00:13:49.080]   of ResNet. So this is Tim's implementation and Tim's way of doing things. When I say
[00:13:49.080 --> 00:13:55.320]   team, I really mean Ross. And I can just press control K, control zero on my keyboard. And
[00:13:55.320 --> 00:13:58.960]   now things are slightly bit easier for me to read. There's a basic block, a bottleneck
[00:13:58.960 --> 00:14:03.800]   block, and we will see what all of those things mean. So we're going to try and actually first
[00:14:03.800 --> 00:14:10.000]   implement, we're going to try and implement these things from scratch first. And I'm pretty
[00:14:10.000 --> 00:14:15.200]   sure I'm going to fail terribly at doing so because I haven't had much practice for the
[00:14:15.200 --> 00:14:21.680]   session. But when I fail, I'm going to come back to this. I'm going to come back to this
[00:14:21.680 --> 00:14:25.720]   implementation of ResNet and then we're going to try and fix things. So then you have this
[00:14:25.720 --> 00:14:34.400]   ResNet model. Okay. So let's see if there's still any questions. I've tried to keep up
[00:14:34.400 --> 00:14:39.920]   with PyDot's group but couldn't. But the coding part takes time. How should one use the notebooks?
[00:14:39.920 --> 00:14:44.760]   Should one try and replicate them? It would be really helpful if you could help us in
[00:14:44.760 --> 00:14:49.880]   setting up the EC2 instance in the session. Okay. Yeah, sure. I will do. These are points
[00:14:49.880 --> 00:14:55.600]   that I've noted. In terms of notebooks, yes, you should definitely try and replicate those
[00:14:55.600 --> 00:15:02.360]   notebooks, which would be really helpful. Is that something that helped me? Okay. So
[00:15:02.360 --> 00:15:07.640]   I'm just going to bring up the, actually, I'm going to this another tab. I'm just going
[00:15:07.640 --> 00:15:14.640]   to bring up the ResNet paper. There it is. Deeper into learning. I'm just going to have
[00:15:14.640 --> 00:15:21.560]   the paper open here. I'm just going to click on PDF. And there we are. I'm going to zoom
[00:15:21.560 --> 00:15:26.800]   in a bit again. I think that should be good for everybody. And that should be it. Okay.
[00:15:26.800 --> 00:15:32.880]   So then the one thing we saw in our paper reading group is that what ResNet does, like
[00:15:32.880 --> 00:15:37.760]   in a traditional convolutional neural network, like before ResNet, all you had was you had
[00:15:37.760 --> 00:15:42.920]   everything going through the residual path. But what ResNet does is it adds this skip
[00:15:42.920 --> 00:15:49.760]   path, right? And there were two different ways of doing this. Like this is what a ResNet
[00:15:49.760 --> 00:15:57.160]   34 looked like. So you have your input image. The first thing you have is your 7x7 conv,
[00:15:57.160 --> 00:16:03.400]   which is 64 channels. And I think that means stripe 2. I think that means stripe 2. I'll
[00:16:03.400 --> 00:16:11.000]   have to confirm. And then you have a pooling layer. And then you have your first block,
[00:16:11.000 --> 00:16:19.200]   which is 3x3 conv, 3x3 conv, so on. So you have like repetitions of. So it's 3x3 conv
[00:16:19.200 --> 00:16:26.360]   and 64 channels. And then suddenly you go from 3x3 conv to 128 channels. Again, stripe
[00:16:26.360 --> 00:16:31.280]   2, I think. What does that mean? Does it say anything what that slash 2 means? I'm assuming
[00:16:31.280 --> 00:16:40.440]   that's stripe 2. There it is. We've found downsampling. Plane network. Okay. Plane baselines,
[00:16:40.440 --> 00:16:46.200]   figure 3, mainly inspired by the philosophy of VGG nets, which was an architecture before
[00:16:46.200 --> 00:16:51.840]   this. The convolution layers mostly have 3x3 filters and follow two simple design rules.
[00:16:51.840 --> 00:16:57.240]   For the same output feature map size, the layers have the same numbers of filters. The
[00:16:57.240 --> 00:17:02.120]   number of bodies doubled. Okay. All good. I think that means stripe 2. All right. So
[00:17:02.120 --> 00:17:12.360]   then the main things that we saw. Okay. Why is this sidebar still here? Lock, lock, small,
[00:17:12.360 --> 00:17:19.360]   automatically hide in tablet mode, automatically hide in desktop mode. What if I click on that?
[00:17:19.360 --> 00:17:32.600]   Okay. Tool that's gone. If I, oh, how do I find it now? If I go maybe there. Does that
[00:17:32.600 --> 00:17:50.560]   help? No? I've just made a fool of myself by trying to -- no? Okay. This is -- well,
[00:17:50.560 --> 00:17:54.200]   I've got everything in front of me. So I guess it's fine. I don't really need the taskbar
[00:17:54.200 --> 00:17:59.960]   right now. I'll just continue like this. Okay. So then we were at this point where I was
[00:17:59.960 --> 00:18:04.920]   -- at least now there's no sidebar on the left, which is good. So then I was at this
[00:18:04.920 --> 00:18:10.320]   point where this is what ResNet34 looks like. And now what we want to do is we want to try
[00:18:10.320 --> 00:18:18.000]   and implement this 34 layer residual. So the one thing you can see, the one repetition
[00:18:18.000 --> 00:18:24.840]   is a conv followed by a conv and then there's a skip connection between the two. There's
[00:18:24.840 --> 00:18:29.640]   a conv followed by a conv and there's a skip connection between the two. There's a conv
[00:18:29.640 --> 00:18:34.720]   followed by a conv and there's a skip connection between the two. And then every time there's
[00:18:34.720 --> 00:18:43.240]   a -- so like at this point, the output would have 64 channels. And then every time there's
[00:18:43.240 --> 00:18:49.720]   like a change in the number of channels. Because look at this. Let me just actually copy paste
[00:18:49.720 --> 00:19:01.200]   this to 34 layer residual. So let me just take a screenshot of this. I don't need this.
[00:19:01.200 --> 00:19:10.360]   I don't need that. I'm sorry, one second. I'm just taking a screenshot. So if I take
[00:19:10.360 --> 00:19:23.640]   this screenshot and I paste it over here. So the point I'm trying to make is you have
[00:19:23.640 --> 00:19:30.560]   one conv and two convs and then you have a skip connection. So like this is a block.
[00:19:30.560 --> 00:19:34.880]   If you look at it, it's a block. I'm just going to call it B. And then this is again,
[00:19:34.880 --> 00:19:43.840]   let's say this is B1, B2, B3. So there's three blocks of 64 channels. Then there's one, two,
[00:19:43.840 --> 00:19:50.720]   three, four. Four blocks here. So see this? That's one block. That's second block, third
[00:19:50.720 --> 00:19:59.960]   block, fourth block. Same for this one. There's again multiple blocks. There's six of them.
[00:19:59.960 --> 00:20:07.960]   This is six. And then over here, there's three of them. So I guess the point I'm trying to
[00:20:07.960 --> 00:20:15.760]   make is if we can find a way to implement this basic, there's two blocks that we, or
[00:20:15.760 --> 00:20:22.960]   there's like two different variations of the block that we need is like at input, if my,
[00:20:22.960 --> 00:20:33.280]   like if this, let's say, let's just look at this one, right? My input has 64 channels
[00:20:33.280 --> 00:20:42.960]   and my output has 64 channels. So I can really add the two together, which is okay. But in,
[00:20:42.960 --> 00:20:50.200]   at this point, my input has 64 channels, but my output has 128 channels. So I can't really
[00:20:50.200 --> 00:20:56.800]   add the two. And remember what we had to do to make both the channels the same is like
[00:20:56.800 --> 00:21:03.040]   you needed a one by one conf over here to make sure that the number of channels in the
[00:21:03.040 --> 00:21:07.640]   output and the input are the same so you can add them. So that's the one thing that we
[00:21:07.640 --> 00:21:14.720]   really, really need is this basic block or this like, this is the tiniest part of the
[00:21:14.720 --> 00:21:21.040]   whole architecture that we want to start with. So let me see if there's any questions so
[00:21:21.040 --> 00:21:28.720]   far and questions were in this tab. Sharing the repo record in the beginning. Thanks very
[00:21:28.720 --> 00:21:39.600]   much for doing that. Okay. So then the first thing that we need is that basic block. And
[00:21:39.600 --> 00:21:47.600]   I'm again going to reference Tim because I'm myself, I don't trust myself and I trust Ross
[00:21:47.600 --> 00:21:55.640]   a lot more than myself. And I trust his implementation a lot more. So you can see now, if we go,
[00:21:55.640 --> 00:22:02.920]   if we go look at this forward method, right, you can see what you have is this basic block
[00:22:02.920 --> 00:22:13.120]   is this, all of this, oh, sorry, let me just close this in it. So all of this, all of this
[00:22:13.120 --> 00:22:21.360]   code is just the implementation of this particular conf followed by conf and then there's a skip
[00:22:21.360 --> 00:22:29.360]   connection between the two. So you can see how that happens. Okay. And this conf do,
[00:22:29.360 --> 00:22:33.640]   you have batch norm list. I think we can make this cleaner. Like we could have had a conf
[00:22:33.640 --> 00:22:39.600]   batch norm, but let's, let's just leave it like this. And you have an activation and
[00:22:39.600 --> 00:22:47.440]   then you don't need se, you don't need drop part. You don't, you need a down sample. And
[00:22:47.440 --> 00:22:51.720]   you just add the two. Okay. Which is great. I'm just trying to see what Ross has done
[00:22:51.720 --> 00:22:58.880]   here. Down sample is this self dot down sample. Okay. Makes sense. Cool. I guess that's the
[00:22:58.880 --> 00:23:04.440]   easy part. All right. So let me go to my, this thing. So I'm going to call class, I'm
[00:23:04.440 --> 00:23:11.800]   going to call this a class basic block. I'm going to import torch dot and then I'm literally
[00:23:11.800 --> 00:23:20.040]   replicating, well, trying to replicate what Ross has done. And I'm trying to be more vocal
[00:23:20.040 --> 00:23:28.000]   and I'll try and explain everything that I'm doing one by one. So in PyTorch, you have,
[00:23:28.000 --> 00:23:33.800]   if you inherit from NN dot module, any architecture that you implement that needs to have parameters,
[00:23:33.800 --> 00:23:39.240]   because remember if you're joining me from fast book, models have parameters which need
[00:23:39.240 --> 00:23:45.080]   to be trained and everything. So in PyTorch, the best way to do it all, like to tell PyTorch
[00:23:45.080 --> 00:23:48.040]   that we're creating an architecture that's going to have parameters that are going to
[00:23:48.040 --> 00:23:54.160]   be trained is by telling PyTorch to inherit from NN dot module. And now automatically
[00:23:54.160 --> 00:23:58.840]   it has all those features, like it has parameters, it has a forward method and all those good
[00:23:58.840 --> 00:24:05.280]   features that you need. So I just go, what do we need here? We're just going to need,
[00:24:05.280 --> 00:24:09.880]   I guess, in channel and out channel. Is that all we need? Oh, there's a lot of things that
[00:24:09.880 --> 00:24:14.000]   are being put in here. Do we need all of that? No, I think in channel and out channel, right?
[00:24:14.000 --> 00:24:19.520]   In planes and out planes. And we would need a stride. Yeah, we would need a stride. And
[00:24:19.520 --> 00:24:24.520]   I guess that's it. Like, that's all we need, I think. Like, that's the only three things
[00:24:24.520 --> 00:24:29.520]   we need to implement the very easy versions. I think that's all we need. So I'm going to
[00:24:29.520 --> 00:24:34.440]   call it in planes. So I'm just going to call it in channels and I'm going to call it out
[00:24:34.440 --> 00:24:40.640]   channels of this basic block. Remember, what we're trying to implement is just this, is
[00:24:40.640 --> 00:24:56.760]   just this much of the stuff. So you have, you have a conf one and you have conf two.
[00:24:56.760 --> 00:25:02.760]   Like this is conf two, this is conf one. In between you have batch norm and you have an
[00:25:02.760 --> 00:25:10.200]   activation layer. And that's pretty much it. So, okay. So I guess we just need in channels,
[00:25:10.200 --> 00:25:23.480]   out channels and that's okay. So if in channels is not equal to out channels, then self.downsample
[00:25:23.480 --> 00:25:37.840]   equals true or nn.conf1d. And you'd have to go from in channels to out channels. Okay.
[00:25:37.840 --> 00:25:43.800]   All I'm saying is like, if the number of channels is not equal, then we need to have like, we
[00:25:43.800 --> 00:25:49.280]   need to have a conf1d in our, in our skip path. So that's something we discussed last
[00:25:49.280 --> 00:25:58.360]   time. And we could also do this for feature map size, but let it be for now. So self.in
[00:25:58.360 --> 00:26:05.360]   channels, do we need any of that? Actually we don't. So self.conf1 is this one, three
[00:26:05.360 --> 00:26:15.400]   by three conf, nn.conf2d. In channels to out channels, in channels to out channels. And
[00:26:15.400 --> 00:26:25.760]   the kernel size is equal to three. Self.conf2. It's very much possible that what I'm doing
[00:26:25.760 --> 00:26:31.840]   is completely wrong, but I promise you one thing, by the time we finish, it would be
[00:26:31.840 --> 00:26:38.480]   right. So I'm just trying to show you. So now we just have set up our two convolutions,
[00:26:38.480 --> 00:26:43.440]   right? Because all we need is like the first convolution and then we need a second convolution
[00:26:43.440 --> 00:26:49.920]   and then we need a batch norm. So I'm just going to call it self. I wonder why Ross has
[00:26:49.920 --> 00:26:55.720]   like a batch norm one and batch norm two. So out of curiosity, let me just have a look.
[00:26:55.720 --> 00:27:00.240]   Okay. After conf2, there's also a batch norm. Interesting. Okay. So we're going to have,
[00:27:00.240 --> 00:27:05.320]   we're going to follow that. So let's just call it, is it nn.batchnorm? Is that what
[00:27:05.320 --> 00:27:13.200]   it is? Yes, it is. And you just specified the number of channels, correct? Let me have a
[00:27:13.200 --> 00:27:23.440]   look. Yeah. You just specify the number of channels. Cool. So self.reduceFirst. Do we
[00:27:23.440 --> 00:27:30.760]   need something like reduceFirst? We don't. And do we need something like expansion at
[00:27:30.760 --> 00:27:38.040]   the moment? I don't think we do. So not really. Going to skip that. Okay. So I'm just going
[00:27:38.040 --> 00:27:44.440]   to call it nn and I'm going to say outChannel to you. And then it's going to be exactly
[00:27:44.440 --> 00:27:52.240]   same. And then in PyTorch, you define something which is a forward method. So the forward
[00:27:52.240 --> 00:28:01.320]   method is what tells what you do to the input. So if my input is x, right, the first thing
[00:28:01.320 --> 00:28:09.160]   I'm going to do is, how does this say? Like it goes x. Okay. So the first thing you want
[00:28:09.160 --> 00:28:15.120]   to do is you pass that through your first convolution. So remember, you have your input.
[00:28:15.120 --> 00:28:21.640]   The first thing it does is it goes through the first convolution, right? So you have
[00:28:21.640 --> 00:28:31.480]   some output. Then you add -norm, I think, which does that. Oh, first you save this in
[00:28:31.480 --> 00:28:38.080]   shortcut. Right. Because that's your input. So then this goes through the first convolution,
[00:28:38.080 --> 00:28:48.440]   which is this one here. Then it goes through, it needs an activation. So maybe call it relu.
[00:28:48.440 --> 00:29:01.600]   So self.act is nn.relu. And then it goes through the second convolution, it goes through the
[00:29:01.600 --> 00:29:21.120]   second -norm and goes through relu again. And you return shortcut. Oh, if self.downsample
[00:29:21.120 --> 00:29:31.440]   shortcut equals self.downsample shortcut, I'm assuming, I hope what I'm doing is right.
[00:29:31.440 --> 00:29:43.000]   And then you return x plus shortcut. And generally, let me just do this. That's not a good way
[00:29:43.000 --> 00:30:00.160]   of doing it, is it? If in chance. Okay. That's a better way. So self.downsample is none if
[00:30:00.160 --> 00:30:04.240]   the number of in channels is equal to out channels. Because if the number of in channels
[00:30:04.240 --> 00:30:09.480]   and out channels are the same, then you don't really need to have a downsampling convolution.
[00:30:09.480 --> 00:30:15.320]   Otherwise, you have a convolution 1D that goes from in channels to out channels. I think
[00:30:15.320 --> 00:30:22.720]   that should be it. I think that basic, this version should work. And I could have had,
[00:30:22.720 --> 00:30:28.000]   like, even this could have been a conf. Even this should have been a conf layer. So let
[00:30:28.000 --> 00:30:39.000]   me define this as define conf block. How about that? And I go from in channels to out channels.
[00:30:39.000 --> 00:30:45.000]   For everybody wondering, is there any questions? Like, I keep, I'm just coding right now. Shouldn't
[00:30:45.000 --> 00:30:56.320]   downsample be a conf2d instance? Yes, it should be. Of a kernel size one. Whatever, don't
[00:30:56.320 --> 00:31:07.360]   have a call it conf1d. I think both could work, actually. Why would both work? And why
[00:31:07.360 --> 00:31:12.200]   would it not be? Well, I've usually seen the number of channels being changed using conf1d.
[00:31:12.200 --> 00:31:18.400]   Okay. Let's keep it 1D for now. Let me see what Ross is using, if he's using downsample.
[00:31:18.400 --> 00:31:27.880]   Downsample, downsample. Here it is. And self.downsample equals that. Okay. So that gets passed somewhere.
[00:31:27.880 --> 00:31:32.120]   So at which point does it get passed? I just want to see something, what Ross is doing.
[00:31:32.120 --> 00:31:40.280]   And then I'll try and answer that. So downsample is equal to one. If that, then downsample,
[00:31:40.280 --> 00:31:50.240]   there's some quarks and there's downsample conf. So average down falls downsample conf,
[00:31:50.240 --> 00:31:57.640]   which is what? I see. This is how Ross has implemented this nice return in end of sequential.
[00:31:57.640 --> 00:32:04.880]   Okay. He's also using a conf2d and then the kernel size would be one. That's what I would
[00:32:04.880 --> 00:32:11.320]   do. Yeah, I guess you're right. But then even 1D would work. Well, okay. Nevermind. Let
[00:32:11.320 --> 00:32:18.200]   me just keep it like that for now. Okay. So that's that, that's that, that's that, and
[00:32:18.200 --> 00:32:23.800]   that's that. So that's my basic block. I will try and simplify this later, I guess. Let's
[00:32:23.800 --> 00:32:31.520]   just keep things, let's just write terrible code is all I'm doing. Okay. So now I have
[00:32:31.520 --> 00:32:41.840]   something I hope like this is something that is a replication of this figure two, right?
[00:32:41.840 --> 00:32:48.120]   This is what's happening. You have your first weight layer, which is conf1. So that's weight
[00:32:48.120 --> 00:32:54.000]   layer. Then you have your second weight layer, which is this one. In between, you have batch
[00:32:54.000 --> 00:33:00.280]   norm activation, so batch norm relu. And then you have batch norm activation and you add
[00:33:00.280 --> 00:33:08.360]   the two together. Right? So let's test this. So this is called a basic block. Let's just
[00:33:08.360 --> 00:33:17.960]   say we're going from three to 64 channels. Oh, would that work? Can we just go from three?
[00:33:17.960 --> 00:33:25.440]   Let's just go from 64 to 64 for the first cry. Do I need to check like the YouTube live
[00:33:25.440 --> 00:33:30.440]   stream comments as well? Are you guys by any chance? I think you need a super.init. Of
[00:33:30.440 --> 00:33:38.000]   course I do. Yes. Thanks very much for that. I do need that. That would have popped up.
[00:33:38.000 --> 00:33:43.480]   This is what happens if you don't code for two weeks, which is what's happening to me.
[00:33:43.480 --> 00:33:50.600]   I'm being rusty. Super. But I'm really impressed with the Allen. And even at 3am, your mind's
[00:33:50.600 --> 00:34:01.080]   sharp. Great to see that. Blocks. I'm going to do block basic block. And let me say my
[00:34:01.080 --> 00:34:14.200]   input is that. Do I have torch imported? I do. So say that's one image of 64 channels
[00:34:14.200 --> 00:34:26.720]   of one by 64 by say 128 by 128. I don't know. And then what if does that work? One timer
[00:34:26.720 --> 00:34:37.880]   the size of 10. So one can go. Oh, I didn't add padding. Padding equals one. Does that
[00:34:37.880 --> 00:34:48.680]   work? It does work. OK. Woohoo. So something's going on. Let's see. Does block have a down
[00:34:48.680 --> 00:34:55.520]   sample? I think it's going to be none. OK. And if I did from I went from three to. 64
[00:34:55.520 --> 00:35:02.520]   would that work? It does work. OK. And then in this case, does block have a down sample?
[00:35:02.520 --> 00:35:09.440]   It does. OK. I guess that's the first block that's been sorted. So we just got the we
[00:35:09.440 --> 00:35:13.560]   just got the block sorted. That seems to be working fine. I'm just going to keep this
[00:35:13.560 --> 00:35:23.600]   there. I'm just going to copy paste this. Would that work? I'll have to save that as
[00:35:23.600 --> 00:35:29.400]   an image and then upload that image if I want that to be a part. But basically, could I
[00:35:29.400 --> 00:35:36.680]   do that? Does that work? Oh, it does work. OK. Nice. All right. So we've done in this
[00:35:36.680 --> 00:35:48.240]   one, this basic block is just a replication of of that at the top. OK. Looking at more
[00:35:48.240 --> 00:35:54.240]   questions doesn't conf when we introduce new parameters while it is supposed to be FX plus
[00:35:54.240 --> 00:36:01.120]   X. Oh, it's not a stupid question. Basically, if you read through the paper, there's a point
[00:36:01.120 --> 00:36:13.800]   mentioned somewhere like. It's meant to be F of X plus X. Right. But your X is now going
[00:36:13.800 --> 00:36:20.920]   through conf one first and then it's going to conf two. And then you have the output,
[00:36:20.920 --> 00:36:26.240]   which is F of X. And then you add the two together. Right. But what if the number of
[00:36:26.240 --> 00:36:31.880]   channels in FX is different from the number of input channels? In that case, before passing
[00:36:31.880 --> 00:36:39.320]   this, you need to pass that to a conf 2D of kernel size one. What that will do. What does
[00:36:39.320 --> 00:36:45.000]   that mean of conf 2D of kernel size one? Well, what's the difference in conf 2D and conf
[00:36:45.000 --> 00:36:51.440]   1D for starters? Conf 1D basically means it's just going to go in one direction, which is
[00:36:51.440 --> 00:36:57.560]   that conf 2D means it's going to go in two directions, which is height and width. Conf
[00:36:57.560 --> 00:37:03.000]   3D means the kernels are going to go in three directions, which is height, width and basically
[00:37:03.000 --> 00:37:17.960]   depth. So if you have, say. Actually, that must be. This is a separate thing that I want
[00:37:17.960 --> 00:37:22.120]   to explain. Kernel size is something that we've looked at in FastBook so many times.
[00:37:22.120 --> 00:37:27.840]   I think I explained that in week 12, was it? Can't remember. But if you if you have questions
[00:37:27.840 --> 00:37:33.760]   about all of this, just have a look there. But the point I'm trying to make is like if
[00:37:33.760 --> 00:37:38.400]   the number of input channels is different in the output and the input, then in that
[00:37:38.400 --> 00:37:42.120]   case, you need to pass this through a convolution. You need to make the number of channels the
[00:37:42.120 --> 00:37:47.480]   same. And the only way to do that is is either use a con or use an nn.ini and we're just
[00:37:47.480 --> 00:37:56.840]   going with con because that's what Ross is used. To begin with. I think the second value
[00:37:56.840 --> 00:38:07.400]   needs to be at the end after the addition of the shortcut. Is that correct? Let me confirm.
[00:38:07.400 --> 00:38:15.720]   You are so right. X plus shortcut and then value. Oh, there it is. Yes, it is there.
[00:38:15.720 --> 00:38:21.280]   Sorry. Thanks for catching that. See, that's the error that we wouldn't have known about.
[00:38:21.280 --> 00:38:30.640]   We won't have known about. If. Yeah, we won't have known about this, but I'm just thinking
[00:38:30.640 --> 00:38:35.960]   why is this called act two and act one? That's my just in case I guess you can have different
[00:38:35.960 --> 00:38:46.360]   activation layers, I would assume like Ross is really. Good at. Letting people use like
[00:38:46.360 --> 00:38:50.120]   a value in the first time and then but we're using value both times, we're just going to
[00:38:50.120 --> 00:39:01.160]   call that that. So I'm just going to say. X plus equals shortcut and then return value.
[00:39:01.160 --> 00:39:16.360]   So I'll put that X. That would work or I just do it like that. I think that works. Yeah,
[00:39:16.360 --> 00:39:23.720]   cool. OK, so that's the basic block of that. So that's the rest net basic block. Now, what
[00:39:23.720 --> 00:39:34.920]   we need to do is. In the rest net architecture, right, which is if you keep going through
[00:39:34.920 --> 00:39:42.320]   this, we have tested various plain visible nets. Oh, I was looking at the comments. This
[00:39:42.320 --> 00:39:48.960]   is where you explain convolutions really nicely. Oh, thanks for that. I'm doubtful about the
[00:39:48.960 --> 00:39:54.680]   really nicely part, but yes, I did make an effort to explain convolutions. To begin with,
[00:39:54.680 --> 00:40:03.240]   that entity. Yeah, there's a pooling. If stride is anything other than one. Well, we're mostly
[00:40:03.240 --> 00:40:07.600]   going to use stride one, but there's I think there's a point where this stride two. So
[00:40:07.600 --> 00:40:11.160]   let's have a look at how Ross does it and then we're just going to try and mimic that.
[00:40:11.160 --> 00:40:16.480]   But because we're doing things like this is a great way that I've found to learn things
[00:40:16.480 --> 00:40:23.440]   is like, sure, use an implementation as a reference, but then also try and like when
[00:40:23.440 --> 00:40:28.800]   you like this process of what I'm doing now is like implementing basic block from scratch.
[00:40:28.800 --> 00:40:33.720]   When I'm doing this, things are becoming a lot more clear in my head. OK, in rest net,
[00:40:33.720 --> 00:40:39.800]   this is exactly what happens. Like I missed padding here. I my value was at the wrong
[00:40:39.800 --> 00:40:46.040]   place. I had the patched on like there were these issues and I was using a cont 1D instead
[00:40:46.040 --> 00:40:50.680]   of a cont 2D, which won't have been that much of an issue. But I'm making all these errors,
[00:40:50.680 --> 00:40:56.280]   which I which now in my head now after fixing them, I just have a better understanding of
[00:40:56.280 --> 00:41:03.280]   the basic block. OK, I'm still reading through. So now what we need is like in the complete
[00:41:03.280 --> 00:41:07.880]   rest net architecture, when the image comes in, you need a stem. Well, that's going to
[00:41:07.880 --> 00:41:16.840]   be easy. It's just a seven by seven conv, which takes which changes the number of channels
[00:41:16.840 --> 00:41:22.120]   from three to sixty four and is also stride two. So let's have a look at the rest net
[00:41:22.120 --> 00:41:30.120]   implementation. There is that. So we go through and we have a look at forward first. OK, so
[00:41:30.120 --> 00:41:39.400]   it's going to self forward features forward features is going to con one. And deep stem,
[00:41:39.400 --> 00:41:46.280]   we don't need to use deep stem. Because that's a different setting, by the way, because a
[00:41:46.280 --> 00:41:54.240]   stem type stem type default type of stem. Default a single seven by seven with a bit
[00:41:54.240 --> 00:41:59.320]   of stem. Yes, that's what we want to use. So Ross has like in the implementation of
[00:41:59.320 --> 00:42:07.760]   team, actually, by doing this, I'm also showing you. So I'm just going to call that that.
[00:42:07.760 --> 00:42:18.680]   So let's see if my if I'm implementing rest net class rest net. I already have my block.
[00:42:18.680 --> 00:42:25.640]   So the first thing I need, oh, I can't do that. I need to have an init first self. What
[00:42:25.640 --> 00:42:31.640]   do we need here? What parameters is Ross passing? He's passing block. OK, so that's the block
[00:42:31.640 --> 00:42:36.520]   type. It's passing layers. That would be like three, four, six, three or something like
[00:42:36.520 --> 00:42:43.840]   that, I would say. Yes. And num classes and in channels. I guess that's all we need for
[00:42:43.840 --> 00:42:52.360]   now to implement our version of the rest net. I guess if there's anything we'll start with
[00:42:52.360 --> 00:43:02.480]   just this and that should be it. OK, so. Does it say I'm just having a look at what's things
[00:43:02.480 --> 00:43:12.160]   I'm just going to say so you need a super dot. I'm now going away from like having my.
[00:43:12.160 --> 00:43:19.760]   I'm now calling this stem, which is meant to be an end of quantity, right? It's going
[00:43:19.760 --> 00:43:29.120]   to be this. And I'll tell you what I'm doing here. I will explain. So it goes from in chance
[00:43:29.120 --> 00:43:38.040]   to in planes. Oh, that would be 64. And Colonel size seven strike to padding three and bias
[00:43:38.040 --> 00:43:43.840]   false. Yes, that all looks good. So you're going from three. OK, well, Ross is called
[00:43:43.840 --> 00:43:48.360]   it in planes as well. But in our version, is that ever going to be different? I think
[00:43:48.360 --> 00:43:53.680]   the stem always means the same in all of. Yeah, it does. So see, now I'm just having
[00:43:53.680 --> 00:44:01.040]   a look at the various rest that architectures. So the only difference. In well, in resident
[00:44:01.040 --> 00:44:07.520]   18 and 34, you have this basic block and in resident 51 or one and 152, you have this
[00:44:07.520 --> 00:44:12.400]   bottleneck block. Right. So that's the difference. Like a bottleneck block goes first uses a
[00:44:12.400 --> 00:44:17.000]   one by one column followed by a three by three column and then one by one column. I'm going
[00:44:17.000 --> 00:44:21.640]   to explain the difference in the two in a tick. But right now, like all of this preparation
[00:44:21.640 --> 00:44:27.960]   that we're doing is say it's going to be fine for 18 and 34. They resonate. And the first
[00:44:27.960 --> 00:44:33.480]   thing that you have is this con one, which is seven by seven, 64 strike two. So actually,
[00:44:33.480 --> 00:44:37.360]   instead of calling this, then let's also call this conf one, because that's what it's called
[00:44:37.360 --> 00:44:44.120]   in the paper. So con one is an end in the quantity which goes from number of input channels,
[00:44:44.120 --> 00:44:51.560]   which was actually three. And it goes to say 64 colonel size seven strike two. OK, so we've
[00:44:51.560 --> 00:44:59.280]   implemented this bit right now for the 18 layer. Let's say we implement it with 18.
[00:44:59.280 --> 00:45:06.560]   We need two of. The basic blocks, right, and then we need two of this basic block that
[00:45:06.560 --> 00:45:11.360]   goes from 64 channels to 64, this one goes from one into one of the eight, like basically
[00:45:11.360 --> 00:45:15.520]   one twenty eight and two fifty six and two sixty six and five one two and five one two.
[00:45:15.520 --> 00:45:23.040]   So we need to be able to implement this to implement resonating correctly. So. How can
[00:45:23.040 --> 00:45:27.960]   we do that? How can we do that? See, if I would come in prepared, I would like say,
[00:45:27.960 --> 00:45:32.760]   oh, OK, and we can do this by doing X. But right now I'm thinking, how can we do that
[00:45:32.760 --> 00:45:42.600]   is we need a way to create blocks. And I know. So we need a way to repeat the basic block
[00:45:42.600 --> 00:45:50.200]   twice over here, right? This is just a con follow back on two of the basic blocks here,
[00:45:50.200 --> 00:45:54.880]   two of the basic blocks here, two of the basic blocks here and two of the basic blocks here.
[00:45:54.880 --> 00:46:05.440]   OK, so I'm just going to say and this looks like to to to to. So if I just say. Yes, if
[00:46:05.440 --> 00:46:14.720]   I just say create the blog, OK, I'm going to say to find make blocks, you pass in block
[00:46:14.720 --> 00:46:23.720]   and you pass this, which maybe looks something like that for rest at 18. And then you go.
[00:46:23.720 --> 00:46:30.880]   How does Ross do it? Let's just have a look. I'm just going to have a look at how Ross
[00:46:30.880 --> 00:46:35.960]   does it. So you have something called channels, right? So you're going always from you start
[00:46:35.960 --> 00:46:41.400]   with sixty four one twenty eight to six five and two. Yes, that looks correct. So you have
[00:46:41.400 --> 00:46:46.480]   something like that, which is great. And then you call something then he calls it make blocks
[00:46:46.480 --> 00:46:52.000]   and you pass in the block and the channels. Let's have a look at this make blocks function.
[00:46:52.000 --> 00:47:00.600]   So you go from stage index stage name stride and all of that. If next time is more than
[00:47:00.600 --> 00:47:12.720]   equal to our first ride else this down sample down. OK, yeah, that should be fine. And then
[00:47:12.720 --> 00:47:21.720]   return stages and feature info. OK, we don't need feature info. What did I do? So I guess
[00:47:21.720 --> 00:47:26.720]   I'm just trying to see exactly what was doing repeat blocks and then combined with an end
[00:47:26.720 --> 00:47:31.040]   of module list. Yes, that's that's one way of doing it. I just want to see like how was
[00:47:31.040 --> 00:47:41.480]   doing it because I really follow Ross's implementations blindly. That's how much I trust his implementation.
[00:47:41.480 --> 00:47:46.000]   I'm just having a look because he's even setting like the stage name layers and stuff, which
[00:47:46.000 --> 00:47:52.840]   is nice, I guess. Never like this name with compatibility. Yeah, because it's like this
[00:47:52.840 --> 00:48:01.160]   implementation is able to just read the it's just able to read the. It's able to load the
[00:48:01.160 --> 00:48:05.560]   pre-trained weights as well. I just want to have a look at this one big thing. Then you
[00:48:05.560 --> 00:48:13.400]   go from stage and stage modules self dot add module. I see. And what does add module do?
[00:48:13.400 --> 00:48:20.920]   OK, that's just that. I guess just by. So you pass in the stage. Interesting. And your
[00:48:20.920 --> 00:48:28.920]   stage is I'm just having to look again at the make blocks. And your stage goes here,
[00:48:28.920 --> 00:48:37.920]   you have a stage name, stride, stride that down, stride equals that and you go block
[00:48:37.920 --> 00:48:49.280]   index in num blocks. See, ah, blocks dot append block function. And you attend and then sequential.
[00:48:49.280 --> 00:48:55.040]   I see. OK, interesting. Cool. I guess I've got the gist of how this is implemented. All
[00:48:55.040 --> 00:49:00.480]   right. And then the sequential is the way to go. The text code is still a little small
[00:49:00.480 --> 00:49:07.240]   to me. Might be a resolution. If I make it too big, it's really hard to me to read. But
[00:49:07.240 --> 00:49:16.840]   guess I'll sacrifice that bit. And make this a bit bigger. Try and make this a bit bigger.
[00:49:16.840 --> 00:49:26.720]   Oh, by the way, hi, Sam. I didn't know you're part of. You're watching me struggle. All
[00:49:26.720 --> 00:49:35.480]   right. I think I would just. So what if. So PyTorch has something called an end of sequential.
[00:49:35.480 --> 00:49:41.880]   In my blocks, what if I put everything in that sequential and then based on that, I
[00:49:41.880 --> 00:49:54.040]   should be able to do it. OK, so now if I go for num layer in list. We did that wrong,
[00:49:54.040 --> 00:50:07.800]   didn't we? Page index. That's stage zero, stage one, two, three in enumerate list. Let's
[00:50:07.800 --> 00:50:32.560]   call it name equals. What are they going to call it? Conv blah X. So. OK, I set up my
[00:50:32.560 --> 00:50:42.520]   layer name. And there you go. And I go. What do I need? I need to create two blocks now.
[00:50:42.520 --> 00:50:47.180]   So number of layers is two. I need to create two blocks. But I haven't really told the
[00:50:47.180 --> 00:50:57.960]   number of channels. So channels is again, 64, 128, 236, 512. Is that what the channels
[00:50:57.960 --> 00:51:06.800]   are? 64, 128, 236, 512. Correct. And I go. I'm just pulling this again. Oh, I need to
[00:51:06.800 --> 00:51:14.400]   go back. So going stage channels, enumerate channels, block repeats, clock clock. Yep.
[00:51:14.400 --> 00:51:26.840]   That's what I need. So go. Stage index, planes, channels. That's what I need. So I'm just
[00:51:26.840 --> 00:51:41.880]   going to call this zip layers channels. And let's just say stage index, num layer, num
[00:51:41.880 --> 00:51:49.240]   channel. That should be fine. So what am I doing? I'm going from stage index, got my
[00:51:49.240 --> 00:51:53.520]   num layers, which is going to be two. I've got my num channels, which is going to be
[00:51:53.520 --> 00:52:10.080]   64. I'm going to create my block. Oh, I'm going to go for block in length, range. I
[00:52:10.080 --> 00:52:21.240]   can just say this. Blocks. Correct. Am I going far so far? And then you can just implement
[00:52:21.240 --> 00:52:25.320]   that to stages. So I can just call that blocks, which is what is happening inside the stage
[00:52:25.320 --> 00:52:31.320]   index. You have blocks. And you just go for block index in num blocks. Yep. That's the
[00:52:31.320 --> 00:52:38.480]   right way to do it. So you can just say block index in range, num layer. So instead of calling
[00:52:38.480 --> 00:52:49.040]   it num layer, I'm just calling it num blocks, which should be fine. Yes. And now I just
[00:52:49.040 --> 00:52:54.160]   create my block, which is this basic block for now. And I can update that. Actually,
[00:52:54.160 --> 00:52:58.760]   I should call it a block function. I should use that as a, because we're going to change
[00:52:58.760 --> 00:53:06.720]   this to be, I should call that that way. Just give me a tick guys, and I will explain everything.
[00:53:06.720 --> 00:53:12.080]   All I'm doing right now is I'm just trying to replicate a minimal version of this make
[00:53:12.080 --> 00:53:17.520]   blocks function. So I'm just trying to use the configuration. I'm just going to use like
[00:53:17.520 --> 00:53:24.560]   the configuration. So like right now I'm trying to create a function that can create both
[00:53:24.560 --> 00:53:29.680]   ResNet-18 and it can create both ResNet-34. So as you can see, it's using a basic block.
[00:53:29.680 --> 00:53:34.040]   Like it goes from 64 channels. Like the first two blocks are the first two convolutions
[00:53:34.040 --> 00:53:40.280]   are 64, but there's two of these blocks. But in ResNet-34, there's three of these blocks.
[00:53:40.280 --> 00:53:45.320]   In ResNet-18, there's two of these blocks with 128 channels, but in ResNet-34, there's
[00:53:45.320 --> 00:53:50.560]   four of these blocks. So like, instead of like having a separate, oh, ResNet-18, separate
[00:53:50.560 --> 00:53:56.080]   ResNet-34, I'm just trying to create like this function called make blocks, which is
[00:53:56.080 --> 00:54:03.160]   again, a copy of Ross's implementation. And all I'm trying to do, I'm just trying to like
[00:54:03.160 --> 00:54:16.760]   create like a loop that does this. So if I just say blocks.append block function, which
[00:54:16.760 --> 00:54:23.600]   is basic block and that expects in channels to out channels. So how does we know, how
[00:54:23.600 --> 00:54:31.680]   do we know the in channels, in planes? Yes. Where is that coming from? Oh, okay. You pass
[00:54:31.680 --> 00:54:38.800]   that to the make blocks. Okay. So that's a good point. But then how do we go from layer
[00:54:38.800 --> 00:54:51.680]   to layer? I guess, interesting stage. If in planes is not that, I think I'm complicating
[00:54:51.680 --> 00:54:56.840]   things too much. Okay. What do we need? We just need a function that can repeat it as
[00:54:56.840 --> 00:55:02.040]   many times as I want. Okay. So the first thing is going to go from three to 64, then it's
[00:55:02.040 --> 00:55:07.960]   going to go from 64 to 128, then it's going to go from 128 to 256. So my first one, so
[00:55:07.960 --> 00:55:15.320]   I can just go for a stage index in enumerate channels are like that, three, 64. Right?
[00:55:15.320 --> 00:55:20.680]   So you start with three channels, then you're going to 64, you're going to 128, you're going
[00:55:20.680 --> 00:55:27.000]   to 256 and you're going to 512. Correct. And then you're going from this num channel in
[00:55:27.000 --> 00:55:35.480]   channels. That's not going to zip properly because they're different. Okay. So you go,
[00:55:35.480 --> 00:56:00.680]   if my, if this is my first stage, my in chan is three, in chan is three, stage index zero,
[00:56:00.680 --> 00:56:18.520]   first two, three, in chan equals three, if stage index equals zero, else num channel,
[00:56:18.520 --> 00:56:30.240]   that should be fine. Right? Channels, stage index, else it's going to be channels, stage.
[00:56:30.240 --> 00:56:35.500]   All I'm setting up is my stage index. That should be fine. Okay. So now if I meant this
[00:56:35.500 --> 00:56:41.400]   in my first, it's going to select this one. Otherwise it's going to select that one. I'm
[00:56:41.400 --> 00:56:48.600]   doing this wrong. So I'm going from three channels. So if my stage index is zero, like
[00:56:48.600 --> 00:56:55.960]   my first one goes from three channels, what I want to do here, how am I not able to make
[00:56:55.960 --> 00:57:02.680]   these things easy? I'm going from stage index zero, which is three channels. So in channels
[00:57:02.680 --> 00:57:17.120]   is three, stage index is zero, else. Should be fine. Blocks is that, layer name is that,
[00:57:17.120 --> 00:57:29.040]   blocks.append, which goes from in channels to, oh, okay. Never mind. Channels, that's
[00:57:29.040 --> 00:57:35.320]   what I want to do. Sorry. Stage index minus one. Got it now. That's my in channels. And
[00:57:35.320 --> 00:57:41.000]   then that's going to be num channel. So it's going from that to that. Okay. That's fine
[00:57:41.000 --> 00:57:52.360]   now. And that's my block. So that's going to be two blocks. And then you say stages.
[00:57:52.360 --> 00:57:59.600]   I'm just going to create, oh, stages equals that. Ignore me for now, guys. Just give me,
[00:57:59.600 --> 00:58:19.600]   just watch me as I make these mistakes. Stages.append, nn.sequential, star, blocks. And then you
[00:58:19.600 --> 00:58:26.320]   return. Let's see if that works. Let's see if that works. So I want to say make blocks
[00:58:26.320 --> 00:58:36.760]   my, oh, this should be, let's just say, why do I have a block and a block fn? Let's just
[00:58:36.760 --> 00:58:43.000]   use one. And then where else have I got block in this? A lot of places. So I'm going, yeah,
[00:58:43.000 --> 00:58:48.880]   that should be fine. So I just say block fn is basic block. And does that work? Let's
[00:58:48.880 --> 00:58:54.400]   see what's the error. Name channels is not defined. Where, where, where? Okay. There.
[00:58:54.400 --> 00:59:05.240]   That should be channels. Does that work now? Yeah. Yes, it does work. See that? That works.
[00:59:05.240 --> 00:59:12.280]   Beautiful. Sorry about all the mess, guys, but looks like it's working. And see how we
[00:59:12.280 --> 00:59:20.000]   wanted to go from, but it's going three to 64. It's going three to 64. This should be,
[00:59:20.000 --> 00:59:26.680]   oh, you never go three to 64 because that's the stem. I'm really sorry about this. You
[00:59:26.680 --> 00:59:35.920]   don't need this. In channels is not defined. So in channel is always going to be channels
[00:59:35.920 --> 00:59:42.760]   minus one. So it's going from channels to that. It's going from 64 to 64, 128 to 128.
[00:59:42.760 --> 00:59:46.880]   And then there's going to be a transition block. How about that? Yeah. It's going from
[00:59:46.880 --> 00:59:56.320]   64 to 64, two of those, 128 to 128, 256 to 256. But this one should be a 64 to 128, right?
[00:59:56.320 --> 01:00:02.440]   I'm just trying to make this a copy of this. So like there's 64 by 64, two of them, but
[01:00:02.440 --> 01:00:06.600]   then there's one in between that's 128 by 128. Like this won't work. Why would this
[01:00:06.600 --> 01:00:12.640]   work? Because it's not going from 64 to 128. So that's the only thing that I have a problem
[01:00:12.640 --> 01:00:20.800]   with. Let me see if there's suggestions here. Channels, typo, type in, typo. As we build
[01:00:20.800 --> 01:00:25.880]   a model, is there a way to display what connections network? Looks like not at the moment. Channels,
[01:00:25.880 --> 01:00:35.680]   this is what was that? Yeah. Ignore that. That's not needed. That is not needed at the
[01:00:35.680 --> 01:00:43.920]   moment. I guess, you know what we could do? I could just take this. And now this is where
[01:00:43.920 --> 01:00:52.680]   I will just cheat a little because I'm stuck. I'm really stuck. I won't. I'm really, really
[01:00:52.680 --> 01:00:59.040]   stuck. And I'll just copy this. And I'll just copy all of that. Copy all of that. If net
[01:00:59.040 --> 01:01:06.440]   side, I don't need all of this. I just need that. Is that even used somewhere? That's
[01:01:06.440 --> 01:01:14.760]   right. Where? Oh, okay. I don't need any of this. Downsample is none. In channels, blog
[01:01:14.760 --> 01:01:28.160]   FN. That's fine. Downsample average. Blogs, blogs, blogs, blogs.fn, stages.fn. Okay. So
[01:01:28.160 --> 01:01:36.440]   let's first try and understand what's going on here. Right? At some point, we're going
[01:01:36.440 --> 01:01:43.800]   from in planes to out planes. Okay. So there's the blog FN dot expansion. This is, is this
[01:01:43.800 --> 01:01:50.560]   where the fun thing is happening? Is this where everything's happening? Let's see. So
[01:01:50.560 --> 01:01:56.720]   you're going from, so you need four stages generally, right? You're going from four stage
[01:01:56.720 --> 01:02:03.480]   index in channels. So the value of channels was, if we see, where does this get called?
[01:02:03.480 --> 01:02:13.480]   So if I press alt shift F12, if I press alt F12, does that give me references? Yes, it
[01:02:13.480 --> 01:02:20.200]   does. Okay. So it gets called here. So channels, the value of channels is this, right? So if
[01:02:20.200 --> 01:02:28.680]   I put that there, that's my channels. Sorry, that's just the value of my channels. Block
[01:02:28.680 --> 01:02:34.920]   repeats. What is the value of block repeats? Where are you? Block repeats. This should
[01:02:34.920 --> 01:02:39.080]   be make blocks. So you have make blocks, you have your blocks. Oh, layers. So that's the
[01:02:39.080 --> 01:02:45.400]   block repeats. That's like that. And then you pass in in planes. So at which point does
[01:02:45.400 --> 01:02:55.800]   this go? In planes equals deep stem. Okay. That's all good. So at some point then, at
[01:02:55.800 --> 01:03:16.280]   some point, this goes from 64. Okay. Ah, I see. At which point does it go from 128 to
[01:03:16.280 --> 01:03:25.960]   256? That's my question. So if I go import Tim, M equals Tim dot create model ResNet
[01:03:25.960 --> 01:03:37.080]   34. If I do this and I print this M, then there's a layer one. Actually, let me go 18
[01:03:37.080 --> 01:03:43.160]   instead of 34. So then there's a layer one, which is going from 64 to 64. And then you
[01:03:43.160 --> 01:03:48.200]   see this layer two is going from 64 to 128. Like this is what we're missing. I guess if
[01:03:48.200 --> 01:03:54.200]   we could add that to our make blocks function, we should be sorted. Right. Because the one,
[01:03:54.200 --> 01:04:00.120]   like our make blocks function that we have is, if I call this my make blocks, if I call
[01:04:00.120 --> 01:04:07.320]   that, we're going 64 to 64. And then this should be 64. Like this one, this first one
[01:04:07.320 --> 01:04:17.880]   should be 64 to 128. Right. You can see here, like 64 to 64, 64 to 64. And then it goes
[01:04:17.880 --> 01:04:36.280]   64 to 128. Right. Yeah, that's all good. 64, 64, 64. That's my first block. That's my second
[01:04:36.280 --> 01:04:55.880]   block. And then goes from 64 to 128. So all I need to do is that 64 to 128. So if I can go...
[01:05:03.800 --> 01:05:11.160]   So I'm going 64 the first time. Then I'm going two layers. And then I'm going, instead of like
[01:05:11.160 --> 01:05:17.720]   this, this should be in channel, num channel. And then let me set in channels to be...
[01:05:25.560 --> 01:05:31.320]   channels index minus one. Does that work?
[01:05:31.320 --> 01:05:37.480]   It won't work, would it?
[01:05:37.480 --> 01:05:44.920]   It's going 64 to 128 every time now. So now I need to make this to be
[01:05:44.920 --> 01:05:54.520]   in channels. This, yeah, I must be out of sleep. And then it goes from num channel,
[01:05:54.520 --> 01:05:58.440]   which is fine. Which point do I set this up?
[01:05:58.440 --> 01:06:07.560]   Guys, I think I'm lost. This is where I'm lost. All I need to do is help me fix this.
[01:06:07.560 --> 01:06:16.280]   No, I don't think that would be... Like I think I'm just making a fool of myself by taking too
[01:06:16.280 --> 01:06:20.680]   long. Like this is not something that's that complex. Like I'm just... This is something I was...
[01:06:22.120 --> 01:06:27.080]   I thought I would be able to do, but because I've been away for so long now, that this is something
[01:06:27.080 --> 01:06:32.600]   that's taking way longer than it should. All I'm trying to do is like, it should be able to go from...
[01:06:32.600 --> 01:06:40.520]   Ah, what if I do it like that? So my in channels is this.
[01:06:40.520 --> 01:06:51.880]   My... All I want to do is I want to go from 64 to 64. And then if the... At some point,
[01:06:52.520 --> 01:07:12.280]   okay, at some point, if that's not equal, then that's what I want to do. So...
[01:07:12.280 --> 01:07:22.680]   So...
[01:07:23.000 --> 01:07:33.080]   So...
[01:07:33.080 --> 01:07:39.480]   So...
[01:07:59.400 --> 01:08:03.560]   How about that? Would that if statement just fix everything for me?
[01:08:03.560 --> 01:08:13.480]   Yeah, looks like it. It goes 64 to 128, 128 by 128, 128 to 256, 256 to 256.
[01:08:13.480 --> 01:08:21.960]   Yeah, I think I fixed that. And then what's happening here? Why is it going from 5 and 2 to...
[01:08:23.000 --> 01:08:36.600]   Oh, because... Yeah, why is it going from 5 and 2? Because of this. If block index equals 0 and
[01:08:36.600 --> 01:08:46.360]   state index not equal to... Does that work? Yeah, 64 to 64, 64 to 128, 128, 128.
[01:08:46.920 --> 01:08:50.200]   Yeah, I fixed it, guys. I'm really, really sorry for the mess.
[01:08:50.200 --> 01:08:55.560]   Okay. Really, really sorry for the mess. It's just...
[01:08:55.560 --> 01:09:04.760]   All right. See, that was just like, I just kept putting the if...
[01:09:04.760 --> 01:09:09.800]   I just, I think all you need to do was keep shut for a sec,
[01:09:09.800 --> 01:09:15.800]   think for 15 seconds and I had the answer in front of me. But there's also like,
[01:09:15.880 --> 01:09:27.080]   I've also just become very bad. But anyway, what I'm doing here is I've got my layer name now.
[01:09:27.080 --> 01:09:35.480]   So, what if I do this? So, can I say make blocks? What does that return?
[01:09:40.360 --> 01:09:47.080]   What does that return? Returns a conf... Oh, yeah, of course. Layer names.
[01:09:47.080 --> 01:09:55.880]   It's just not append. Layer names.
[01:09:55.880 --> 01:10:04.840]   Not append. Layer name. What does that do? I'm just trying to make
[01:10:07.080 --> 01:10:14.360]   this work. Okay. So, you have conf2, 3, 4 because your conf1 is the stem. See, that's nice. Now,
[01:10:14.360 --> 01:10:24.760]   this is how I like it. And then if I go... So, conf2 is basically this bit. So, if I go 1 and
[01:10:24.760 --> 01:10:33.400]   then I go 0. See, that's conf2. Yeah, that's nice. I like this. And then conf3 is this. Yep. And then
[01:10:33.400 --> 01:10:45.080]   conf4 is this. And then conf5 is this. Yes! Nice! Cool. Okay. Are there any questions on this make
[01:10:45.080 --> 01:10:49.320]   blocks function like what I've done here? This probably is the most complicated part of the
[01:10:49.320 --> 01:10:56.360]   whole ResNet architecture so far. But it wasn't that complicated. I just made things really complex.
[01:10:56.360 --> 01:11:02.040]   Like, the first... So, this literally in every ResNet architecture...
[01:11:02.040 --> 01:11:08.200]   I've already fixed this one. So, don't worry about this.
[01:11:08.200 --> 01:11:16.840]   Cool. You got it working. I know. I know I got it working. Finally. Just took me like,
[01:11:16.840 --> 01:11:23.320]   I don't know, how much time did I take to fix this? 20 minutes? Oh, it's 11.42 already? Have
[01:11:23.320 --> 01:11:29.720]   we been doing this for an hour and 10 minutes? I'm really sorry if we have. But looks like this
[01:11:29.720 --> 01:11:36.040]   thing is working. So, all I'm doing in this make blocks function is like, I'm enumerating to my
[01:11:36.040 --> 01:11:43.480]   layers. Even if this was like 2, 3, 2, 2, this would still work. But it would be a different...
[01:11:43.480 --> 01:11:51.720]   Like, now we have three of these layers instead of like two of them. And see how this has a
[01:11:51.720 --> 01:11:56.440]   down sample on its own. So, this is just something that's going to work. I mean, if I went from like
[01:11:56.440 --> 01:12:08.680]   3, 4... Which is the configuration for ResNet 34, 3, 4, 6, 3, I think. Is that what it says? Yeah.
[01:12:08.680 --> 01:12:12.840]   You have the first block thrice. You have the second block four times. This one six times.
[01:12:12.840 --> 01:12:19.320]   Then three times. If I do that, see even that works. So, you have 64 by 64. The first block
[01:12:19.320 --> 01:12:26.200]   three times. The second block, there's six of them. Oh, sorry. The second block has six of them.
[01:12:26.200 --> 01:12:32.760]   And then the last one has like three of them. So, you can see 0, 1, 2. 0, 1, 2, 3, 4, 5. 0, 1, 2, 3,
[01:12:32.760 --> 01:12:40.280]   which is 4. And that it's like everything's working fine now, right? Everything's working
[01:12:40.280 --> 01:12:47.960]   fine now. And I should, I could put this in an end of sequential. But that's that. So, now I can just
[01:12:47.960 --> 01:12:58.360]   say self.blocks equals make blocks. You pass in, I'm just going to call this block fn.
[01:12:58.360 --> 01:13:03.880]   You pass in your block fn. You pass in your layers.
[01:13:03.880 --> 01:13:09.240]   This should be set up just like that.
[01:13:14.440 --> 01:13:18.120]   If you guys don't know this, I'm actually furious at myself for taking this long.
[01:13:18.120 --> 01:13:21.160]   But please forgive me for taking this long.
[01:13:21.160 --> 01:13:29.240]   And I should be able to do this, right? self.blocks.
[01:13:29.240 --> 01:13:34.680]   The one thing I'm missing is again this self.add module, which I quite like.
[01:13:34.680 --> 01:13:38.520]   So it's going from this to this. So I should just call this, actually, this is a nice one.
[01:13:38.520 --> 01:13:47.080]   So I should just call this stage modules. But if I pass that, oh yeah, so stage modules is make
[01:13:47.080 --> 01:13:52.040]   blocks. So make blocks, what was that returning in this one? In this case, it was returning
[01:13:52.040 --> 01:13:58.600]   something like a stages and stages had a stage name and that. Okay. Yep. I can do that. That
[01:13:58.600 --> 01:14:04.920]   should be easy to fix. So I can just say stages.append. Yeah, instead of having like two
[01:14:04.920 --> 01:14:11.960]   things that separate, this could just be called stages.append. That thing stage name.
[01:14:11.960 --> 01:14:15.480]   So I can just append a tuple, which is quite nice.
[01:14:15.480 --> 01:14:20.360]   Let's just call it layer name. So instead of calling it layer name, I can call this stage name.
[01:14:20.360 --> 01:14:32.040]   And that and that becomes my stage. And I can return stages. Okay.
[01:14:32.680 --> 01:14:38.280]   So make blocks again, I'm just testing if I have not broken anything. Yeah. Okay. See that?
[01:14:38.280 --> 01:14:43.560]   That's nice. You have conf2, which is all of this. You have conf3, which is all of that.
[01:14:43.560 --> 01:14:48.360]   Conf4, which is all of that. Conf5, which is all of that. That's nice. So now I can just say,
[01:14:48.360 --> 01:14:59.240]   I can just say, I can now do exactly what's being done here is I can just say stage modules is this,
[01:15:00.200 --> 01:15:05.720]   stage modules is that, and now I can go this thing. All right.
[01:15:05.720 --> 01:15:15.240]   The only thing is, is that inherited differently? It's an end of module. And now what about my
[01:15:15.240 --> 01:15:27.400]   forward? Okay. And then define forward. So x, and we can just pretty much copy paste this, all this.
[01:15:29.400 --> 01:15:38.120]   Okay. So you first pass that through your this thing, then you have, why is there a max pool
[01:15:38.120 --> 01:15:46.680]   operation here? What was the stages? So I guess I'm just trying to now implement this make blocks
[01:15:46.680 --> 01:15:53.720]   function. So it's conf2, conf3, conf4. So instead of like calling it layer, I should call it conf2,
[01:15:53.720 --> 01:16:00.520]   3, 4. Should that be, instead of this being, should that be that? Am I, what if this,
[01:16:00.520 --> 01:16:08.520]   what does this do? Oh no, it should be 2. That's fine. Yep. So then this should be self.conf2,
[01:16:08.520 --> 01:16:19.560]   self.conf3, self.conf4, self.conf5. And then what if I return x? Does that work?
[01:16:20.280 --> 01:16:32.520]   I'd be damned if it did. So if I call this layers equal to 2, this becomes ResNet-18, right?
[01:16:32.520 --> 01:16:40.200]   Does that work? Oh, I've got something guys. I've got something. Okay. ResNet-18 architecture
[01:16:40.200 --> 01:16:47.080]   looks like this. Nice. What if I pass in my input? Does that work? Like if this thing works,
[01:16:47.080 --> 01:16:59.720]   I'd be so happy. Toss the random 1, 3, 2 to 4 by 2 to 4. Oh, that worked. Sorry, that was just,
[01:16:59.720 --> 01:17:06.680]   that was just the moment of happiness. Nice. I'm so happy.
[01:17:06.680 --> 01:17:14.680]   I'm sure there's a few mistakes in there, but at least like the basic thing is working.
[01:17:14.680 --> 01:17:25.400]   Oh, you want to use this as homework? Well, I think there's no homework left.
[01:17:25.400 --> 01:17:33.240]   Like I've just done everything that's needed. I hope, like I can only try and explain what's
[01:17:33.240 --> 01:17:40.600]   going on here in this part of the, in all of this part of the code, but there's literally
[01:17:40.600 --> 01:17:44.840]   no homework left. Like now I can just create my ResNet-34 by passing in like a different
[01:17:44.840 --> 01:17:51.000]   number of layers. So I should not use this. I should just call this 2 to 2.
[01:17:51.000 --> 01:17:59.480]   Let me see if there's any comments on the live stream, which I've been ignoring so far.
[01:18:06.520 --> 01:18:12.040]   Let's compare around a total of three posts. Okay.
[01:18:12.040 --> 01:18:21.160]   If you guys want to use this as homework, that's fine with me. But now, what I've done,
[01:18:21.160 --> 01:18:28.360]   what I have in this implementation is like a ResNet-18. And now I can also create a ResNet-34
[01:18:29.240 --> 01:18:38.520]   by just changing the number of layers to 3, 4, 6, 3. And then that should be my ResNet-34.
[01:18:38.520 --> 01:18:46.600]   The only difference is like I'm still outputting like the features. All I need to do is add a
[01:18:46.600 --> 01:18:53.000]   classifier in the end. So let me do that. Where is a classifier? Is there a classifier? Yep.
[01:18:53.000 --> 01:18:58.600]   Self.num classes. So where do you get used? All I want to see is this. Reset classifier.
[01:18:58.600 --> 01:19:05.000]   Reset classifier. Global pool self.fc. Okay. That's why you have the global pool in there.
[01:19:05.000 --> 01:19:15.320]   That makes sense. And then there's a bash num act 1. Okay. After this one. Cool.
[01:19:15.320 --> 01:19:19.960]   And then there's a max pool. Oh, I'm missing the max pool. I'm also missing this one.
[01:19:19.960 --> 01:19:26.680]   Max pool. Okay. So in my architecture now, so far, I've got my stem set up. So as you can see,
[01:19:26.680 --> 01:19:36.120]   that stem, this 7x7, 64 stride 2 stem is being set up, which looks like this. This is my conv1
[01:19:36.120 --> 01:19:41.480]   because in the paper, it's called conv1. So I've taken the liberty of calling it conv1.
[01:19:41.480 --> 01:19:47.080]   So that's what happens. So the first thing that happens is like the input gets passed to the stem.
[01:19:47.080 --> 01:19:55.640]   Now I'm missing this max pool. So I should call this another thing is like self.pool equals nn.max
[01:19:55.640 --> 01:20:10.600]   pool, is it? Yeah. Max pool 2d. And it's a 3x3 pool, right? So kernel size is 3, stride is 2.
[01:20:11.320 --> 01:20:18.120]   So the next thing that happens is that this goes to pool x and then it goes to all of that.
[01:20:18.120 --> 01:20:26.360]   So let's see if that still works. Yeah. Now it's saying name tensors and all the associated
[01:20:26.360 --> 01:20:31.320]   everything. Okay. I've just put in max pool and then suddenly there's some issue. It's giving me
[01:20:31.320 --> 01:20:37.400]   a warning. Tossed at max pool 2d. Well, that's an internal PyDodge warning. That's something I can
[01:20:37.400 --> 01:20:44.760]   ignore safely. Okay. So that's working. I've got this conv2 set up. I've got conv3 set up. I've got
[01:20:44.760 --> 01:20:51.480]   conv4 set up and I've got conv5 set up. And then finally, what I can do is an average pool and then
[01:20:51.480 --> 01:21:00.600]   1000d softmax. Okay. So in the end, you have, I can just call this self.pool1. So is that what's
[01:21:00.600 --> 01:21:05.960]   happening in here as well? Yeah. And you have a global pool. See that? So there's a createClassifier
[01:21:05.960 --> 01:21:10.920]   function which we can, let's just use this. I think there's no need to like do anything
[01:21:10.920 --> 01:21:17.480]   separately. We can just use, yeah, we can just use this createClassifier function from Tim.
[01:21:17.480 --> 01:21:24.200]   So where are you? Where's this createClassifier function? It is in, but if I use this, I would
[01:21:24.200 --> 01:21:28.200]   be cheating, right? Because I wouldn't have done the whole, I wouldn't have done the whole
[01:21:29.640 --> 01:21:37.640]   selectiveAdaptiveToPoolTd which is this. SelectiveAdaptive is Tim layer. All right.
[01:21:37.640 --> 01:21:44.600]   That's not a problem. So I can just call that self.pool which is in this case, is there anything
[01:21:44.600 --> 01:21:50.200]   it's called? It's just called maxPool. So let me just call this self.maxPool. And then let me just
[01:21:50.200 --> 01:22:03.160]   call the last one as self.averagePool. AveragePoolTd. Again, is that a stride? No, there's
[01:22:03.160 --> 01:22:15.080]   no stride in this case. So, and there's no, yeah, I guess, instead of creating the classifier,
[01:22:15.080 --> 01:22:22.520]   like all we're going to do, oh, yeah, all we now need to do is like, everything's ready.
[01:22:22.520 --> 01:22:24.600]   All we now need to do,
[01:22:24.600 --> 01:22:37.640]   all we now need to do is like, we need to flatten, remember in, when we're doing fast
[01:22:37.640 --> 01:22:43.320]   loop, I told you, like, you can't really, like, every now network, if my input is different,
[01:22:43.320 --> 01:22:50.120]   of a different size, then everything is like, the output shapes become different. But we need to
[01:22:50.120 --> 01:22:55.880]   have a pooling layer in the end, followed by, we need to have a pooling layer in the end, followed
[01:22:55.880 --> 01:23:06.440]   by a classifier. So I can just call self.classifier, createClassifier function from Tim right now,
[01:23:06.440 --> 01:23:11.960]   just to make things easy. Where is this createClassifier function? So it's part of
[01:23:11.960 --> 01:23:16.840]   Tim.models.layers. So let me just, I'm doing a bit of a cheating, but this is not really
[01:23:16.840 --> 01:23:26.760]   from Tim.models.layers import createClassifier. This is like, I don't consider this a lot of
[01:23:26.760 --> 01:23:33.400]   cheating. Like this is something that is fine with me. It's not a lot of cheating, so it's fine.
[01:23:34.360 --> 01:23:44.840]   Self.numFeatures, what is this? Oh, okay, got it. In our case, it's just what it's going to be.
[01:23:44.840 --> 01:23:51.480]   Self.numFeatures is always going to be 5 and 2, because that's what we left with. Self.numClasses,
[01:23:51.480 --> 01:23:58.040]   let's just call it numClasses. PoolType is global pool. So poolType
[01:23:58.040 --> 01:24:03.080]   is global pool, which is average. So I'm just going to call it average as well.
[01:24:03.320 --> 01:24:11.000]   Because that's what we have. And then numClasses is that. Okay, now what? Does that work? No?
[01:24:11.000 --> 01:24:16.120]   Oh, because I didn't really use this, did I? I just created that, but never used it.
[01:24:16.120 --> 01:24:25.880]   Now what? Does that work? Tuple object is not callable. Why?
[01:24:29.080 --> 01:24:35.080]   Oh, what? What did I miss? Self.classify.
[01:24:35.080 --> 01:24:46.200]   What if I do this? Does that return a tuple? Oh, it does return tuple. So it returns a,
[01:24:46.200 --> 01:24:56.120]   okay, I see. I see. That's where the, oh, okay. So you just have that. You just like, okay, yeah.
[01:24:56.120 --> 01:25:01.160]   Instead of like doing like that, you just say something like that. And then you go in the end,
[01:25:01.160 --> 01:25:10.360]   which is this one. So you just go global pool and then you go, does that work now?
[01:25:10.360 --> 01:25:22.760]   Cool. And does this work now? Cool. So you can see I've successfully implemented ResNet from scratch.
[01:25:23.320 --> 01:25:32.360]   I know it was a lot of pain, but it was a lot of pain. But I hope that in going through all
[01:25:32.360 --> 01:25:42.200]   of this pain, I've kind of now helped increase the explanation. I too felt the, I laughed when
[01:25:42.200 --> 01:25:50.680]   you went, whoa, I really felt the happiness. Yeah, well, I was stuck. Like this is, I'll be honest,
[01:25:50.680 --> 01:25:55.400]   like I've never been this stuck on ResNet before. So this is what happens when you're out of
[01:25:55.400 --> 01:26:05.960]   practice, but as long as it works in the end. So I've just implemented ResNet. Now I guess for
[01:26:05.960 --> 01:26:13.240]   homework or like, if you want to build on this, all we have to do is like the ResNet 50, 101 and
[01:26:13.240 --> 01:26:18.760]   152, you just have to have the exact same structure. Like this, this structure, this is everything that
[01:26:18.760 --> 01:26:23.480]   you need. All you need to do is like, instead of like using a block function as basic block, which
[01:26:23.480 --> 01:26:29.400]   is, which is this basic block, just replace this with a bottleneck block. See how you can implement
[01:26:29.400 --> 01:26:34.600]   a bottleneck block. You just have a one by one con followed by a three by three con followed by a one
[01:26:34.600 --> 01:26:40.360]   by one con. So instead of like having con one, con and con two in your basic block, you have
[01:26:40.360 --> 01:26:47.480]   con one, con two, con three, and just try and replace like this basic block with a bottleneck
[01:26:47.480 --> 01:26:52.680]   block. And please, please, please post it in the forums. So just trying to replace this with a
[01:26:52.680 --> 01:26:59.240]   bottleneck block and paste it in, like try and create ResNet 50, 101 and 152. In case that doesn't
[01:26:59.240 --> 01:27:09.400]   happen, let me know and we'll take up, we'll build on top of what we've learned today next week. So
[01:27:09.400 --> 01:27:17.000]   this is where we'll stop. And thanks for joining me in my embarrassment as I took that long to
[01:27:17.000 --> 01:27:22.600]   implement the ResNet architecture, but hopefully I've been able to explain something that how it
[01:27:22.600 --> 01:27:29.640]   is implemented. This was the worst function ever, but thanks for joining me guys. See you guys next
[01:27:29.640 --> 01:27:38.520]   time. And I just need to stop the live coding now, but thanks for joining me. See you next time.
[01:27:38.520 --> 01:27:48.040]   Bye.
[01:27:48.040 --> 01:28:07.560]   Bye. Bye.

