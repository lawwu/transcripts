
[00:00:00.000 --> 00:00:03.560]   I can't wait to talk about lab grown meat. I have been trying
[00:00:03.560 --> 00:00:04.320]   to get people to
[00:00:04.320 --> 00:00:12.200]   please, let's not get canceled.
[00:00:12.200 --> 00:00:17.120]   The four of us are functional again, we'd like each other we
[00:00:17.120 --> 00:00:19.880]   enjoy we look forward to doing the show again, everything
[00:00:19.880 --> 00:00:20.800]   styled in
[00:00:20.800 --> 00:00:23.400]   is your lab grown meat is that use hormones?
[00:00:23.400 --> 00:00:24.840]   My lab grown meat was a little
[00:00:24.840 --> 00:00:29.560]   let me ask you this about your lab grown meat. Do you have to
[00:00:29.560 --> 00:00:39.160]   say age? No, no, no, no. You know, I was told that my lab
[00:00:39.160 --> 00:00:40.080]   grown meat was a little
[00:00:40.080 --> 00:00:47.040]   I've injected some flavors of tobacco, black cherry, some
[00:00:47.040 --> 00:00:49.680]   notes, some notes, or Simmons
[00:00:57.400 --> 00:01:12.480]   rain man David. So let's go to big tech earnings, Google stock
[00:01:12.480 --> 00:01:15.960]   is up 5%. After beating on the top line and bottom line
[00:01:15.960 --> 00:01:18.640]   estimates some high level takeaways Google announced a
[00:01:18.640 --> 00:01:23.720]   $70 billion stock buyback plan, and that their cloud unit was
[00:01:23.720 --> 00:01:26.400]   profitable for the first time in its history. So we mentioned
[00:01:26.400 --> 00:01:29.640]   last week, Sundar officially announced that deep mind was
[00:01:29.640 --> 00:01:34.920]   merging with brain. This is kind of controversial because it's
[00:01:34.920 --> 00:01:39.160]   really hard, according to some sources for Sundar to get all
[00:01:39.160 --> 00:01:42.360]   his lieutenants to work together and row in the right direction.
[00:01:42.360 --> 00:01:46.680]   Google's q1 search revenue up year over year 2% down 5%
[00:01:46.680 --> 00:01:50.400]   quarter of a quarter kind of to be expected because of
[00:01:50.400 --> 00:01:54.520]   seasonality and because we're in a down market right now
[00:01:54.520 --> 00:01:59.520]   obviously with the recession. YouTube down 2.5% year over year
[00:01:59.520 --> 00:02:03.840]   down 16% quarter of a quarter other bets which is like nest
[00:02:03.840 --> 00:02:08.760]   and some other products down 35% year over year, net income $15
[00:02:08.760 --> 00:02:15.840]   billion. Any thoughts freeberg on what is a mixed quarter by
[00:02:15.840 --> 00:02:20.480]   Google? And I guess the wider macro environment?
[00:02:20.480 --> 00:02:22.680]   What was so striking about the earnings call is not
[00:02:22.720 --> 00:02:25.360]   necessarily what was presented, but what was not presented,
[00:02:25.360 --> 00:02:29.680]   which was a stronger voice and a strategic plan going forward
[00:02:29.680 --> 00:02:33.040]   for dealing with two major issues at the company. One is
[00:02:33.040 --> 00:02:36.840]   the operating cost model. And the second is the AI strategy
[00:02:36.840 --> 00:02:39.880]   and the response to this evolution in AI. I've heard from
[00:02:39.880 --> 00:02:44.400]   a lot of folks that the AI strategy in particular, it's
[00:02:44.400 --> 00:02:47.880]   almost like Google already has this in the bag, but they just
[00:02:47.880 --> 00:02:49.880]   haven't kind of let it out of the bag. It's like they've got
[00:02:49.880 --> 00:02:53.240]   the Tasmanian devil and they're ready to go with it. And there's
[00:02:53.240 --> 00:02:56.120]   from from my read, an incredible amount of confidence that
[00:02:56.120 --> 00:02:59.320]   there's something that's going to happen. And a set of things
[00:02:59.320 --> 00:03:00.360]   that are going to happen, they're going to be very
[00:03:00.360 --> 00:03:04.080]   profound and powerful. I even heard some anecdotal stories
[00:03:04.080 --> 00:03:07.840]   about, hey, you know, we don't have this feature in this
[00:03:07.840 --> 00:03:11.960]   product, but chat GPT does. And then people basically showed up
[00:03:11.960 --> 00:03:14.560]   to this meeting. And there was all this debate about, well, we
[00:03:14.560 --> 00:03:16.640]   can't let it out, because we're not sure you know, the classic
[00:03:16.640 --> 00:03:19.640]   kind of like we're scared of doing doing wrong, versus
[00:03:19.640 --> 00:03:22.080]   leaning forward and taking risk. Don't be evil, you're
[00:03:22.080 --> 00:03:25.600]   referencing. No, it was just more about regulatory concern
[00:03:25.600 --> 00:03:28.080]   and getting things wrong and making a mistake. And so there's
[00:03:28.080 --> 00:03:31.040]   this total fear of like, again, you know, regulatory and fear.
[00:03:31.040 --> 00:03:32.960]   So someone kind of slammed the table and said, let's just put
[00:03:32.960 --> 00:03:34.640]   it out the next day, they put it out. So there's definitely a
[00:03:34.640 --> 00:03:37.080]   cultural change happening internally, is what I've heard
[00:03:37.080 --> 00:03:39.440]   anecdotally, but what was really missing, which is what Wall
[00:03:39.440 --> 00:03:41.280]   Street needed to hear what investors and shareholders
[00:03:41.280 --> 00:03:43.480]   needed to hear is what's the strategy there? How are you
[00:03:43.480 --> 00:03:45.120]   going to compete? How are you going to resolve what's going to
[00:03:45.120 --> 00:03:47.160]   go forward? And secondly, what are you going to do about the
[00:03:47.160 --> 00:03:50.200]   cost structure of the company, because everyone else, you know,
[00:03:50.200 --> 00:03:53.520]   in contrast to meta being up 11 12%, after hours, with their
[00:03:53.520 --> 00:03:55.720]   cost cutting model and demonstrating that they're going
[00:03:55.720 --> 00:03:58.480]   to start pulling cash out of this business, Google's top, you
[00:03:58.480 --> 00:04:01.360]   know, kind of top story was, hey, we're stopped serving peanut
[00:04:01.360 --> 00:04:04.680]   M&Ms in the cafeteria or something ridiculous. And, you
[00:04:04.680 --> 00:04:06.480]   know, that doesn't really address the real structural
[00:04:06.480 --> 00:04:11.440]   question. So I think the stock buyback, the $70 billion stock
[00:04:11.440 --> 00:04:14.240]   buyback is an authorization to repurchase, it's not a plan to
[00:04:14.240 --> 00:04:18.400]   repurchase. So it's unclear if when or how that capital does
[00:04:18.400 --> 00:04:21.640]   actually get deployed in the market to buy back stock. And so
[00:04:21.640 --> 00:04:24.360]   there is also this big kind of shareholder sentiment of being
[00:04:24.360 --> 00:04:27.960]   let down, that there isn't an improvement in either cash
[00:04:27.960 --> 00:04:31.000]   coming out of the business, or in cash being used in a smart
[00:04:31.000 --> 00:04:35.180]   way with the business. And it was the silence in the earnings
[00:04:35.180 --> 00:04:37.800]   call that I think really stunned a lot of people, which is why
[00:04:37.800 --> 00:04:40.520]   you didn't see a lot of stock movement, despite the actual
[00:04:40.520 --> 00:04:44.200]   business numbers being better than expected. And so there's
[00:04:44.200 --> 00:04:47.640]   a lot that Google, I think, still has to catch up to with
[00:04:47.640 --> 00:04:50.640]   respect to their peers, both on a product and strategy point of
[00:04:50.640 --> 00:04:55.200]   view, but also on a cost cutting and a communication of that cost
[00:04:55.200 --> 00:04:57.560]   cutting point of view to the market and to the street.
[00:04:57.560 --> 00:05:00.520]   Otherwise, shareholders are going to start to lose faith if
[00:05:00.520 --> 00:05:02.520]   they're not already, and are going to start to put their
[00:05:02.520 --> 00:05:05.360]   capital with other folks who they feel are better leading and
[00:05:05.360 --> 00:05:10.360]   leaning into this new evolution of technology like Microsoft and
[00:05:10.640 --> 00:05:14.360]   Apple, and meta, which is really where those big capital
[00:05:14.360 --> 00:05:17.840]   allocators end up picking stuff to go. One final thing I'll say,
[00:05:17.840 --> 00:05:24.000]   it's extraordinarily important to note that I think Google has
[00:05:24.000 --> 00:05:28.320]   such an incredible AI advantage over Microsoft. And you know,
[00:05:28.320 --> 00:05:31.400]   Microsoft is almost solely dependent on open AI, this small
[00:05:31.400 --> 00:05:35.320]   startup company, and all of Bing chat is powered by it. And
[00:05:35.320 --> 00:05:38.920]   Microsoft hasn't built out the the infrastructure, the team,
[00:05:38.920 --> 00:05:43.800]   the rigor, the depth, the models that Google has, and Google made
[00:05:43.800 --> 00:05:47.160]   a few strategic blunders, you know, they shouldn't have been
[00:05:47.160 --> 00:05:49.560]   as open with the transformer work that they did and shared
[00:05:49.560 --> 00:05:52.160]   that publicly, it certainly enabled open AI and others to
[00:05:52.160 --> 00:05:56.000]   compete. But Google certainly has an incredible set of tools
[00:05:56.000 --> 00:05:58.920]   and capabilities that is leap years ahead of Microsoft,
[00:05:58.920 --> 00:06:01.840]   they're in a position to really compete, they just have to have
[00:06:01.840 --> 00:06:04.520]   the will and the leadership to do it, slam the table, say,
[00:06:04.520 --> 00:06:06.640]   here's, we're gonna stop wasting money. And we're gonna start
[00:06:06.640 --> 00:06:10.080]   leading and driving this, this industry forward. And this, this
[00:06:10.080 --> 00:06:12.560]   could be a quick turnaround story for the stock, and for
[00:06:12.560 --> 00:06:14.960]   this company. And I hope it'll happen.
[00:06:14.960 --> 00:06:18.680]   Chamath, what are your thoughts on Google's leadership
[00:06:18.680 --> 00:06:21.840]   specifically? Is Sundar the right person to run the company
[00:06:21.840 --> 00:06:25.960]   going forward? Does he have the founder authority to get the
[00:06:25.960 --> 00:06:29.120]   ship and to get the lieutenants all kind of rowing in the same
[00:06:29.120 --> 00:06:31.680]   direction? Does it need to be a leadership change, which is the
[00:06:31.680 --> 00:06:33.840]   big discussion of topic in Silicon Valley right now?
[00:06:33.880 --> 00:06:37.760]   I think he's very capable. That's an amorphous organization
[00:06:37.760 --> 00:06:42.600]   of so many different competing interests. The thing that
[00:06:42.600 --> 00:06:48.000]   doesn't add up about the Google earnings release, but then also
[00:06:48.000 --> 00:06:52.440]   what freebird just mentioned is, there was this article that kind
[00:06:52.440 --> 00:06:56.640]   of tried to paint Sundar as sort of a caretaker CEO, right, where
[00:06:56.640 --> 00:07:01.320]   Larry was the actual shadow CEO. Well, if that's true, you know,
[00:07:01.320 --> 00:07:05.240]   Larry has more incentive than anybody else to kind of force
[00:07:05.240 --> 00:07:09.320]   change. And there was all these kind of like gripes and
[00:07:09.320 --> 00:07:11.200]   complaints that were articulated, and I don't put
[00:07:11.200 --> 00:07:15.520]   much stock in all of this stuff. I think that he is the right
[00:07:15.520 --> 00:07:19.240]   person for the job. And I think what they have to do is just do
[00:07:19.240 --> 00:07:24.520]   the simple basic things like it doesn't take a CEO change for a
[00:07:24.520 --> 00:07:28.560]   board of directors to have the emotional wherewithal to
[00:07:28.560 --> 00:07:34.000]   authorize a 15 or 20% reduction in force for a company that is
[00:07:34.000 --> 00:07:38.920]   so profitable, that clearly is not yet humming on all cylinders.
[00:07:38.920 --> 00:07:42.760]   And so you don't need to go through all of this drastic
[00:07:42.760 --> 00:07:47.240]   change to do these simple, obvious things. My takeaway
[00:07:47.240 --> 00:07:52.040]   across all of these four big companies is we are in a really
[00:07:52.040 --> 00:07:55.600]   unique moment to observe something that may sound
[00:07:55.800 --> 00:07:58.520]   controversial or hurt people's feelings that like these
[00:07:58.520 --> 00:08:04.440]   companies. But I think we're now well past peak big tech. Their
[00:08:04.440 --> 00:08:07.560]   valuations may still go up because they generate such an
[00:08:07.560 --> 00:08:11.200]   enormous amount of cash flow. But these are exactly those
[00:08:11.200 --> 00:08:15.480]   kinds of businesses. Now they are x growth, large cash flow
[00:08:15.480 --> 00:08:19.000]   businesses, blue chip, you might say, while they were always blue
[00:08:19.000 --> 00:08:22.160]   chip, but the way that they grow is not through innovation. If you
[00:08:22.160 --> 00:08:26.880]   look at Google, Facebook, Microsoft and Apple, and ask
[00:08:26.880 --> 00:08:30.120]   yourself, when was the last hugely disruptive thing that
[00:08:30.120 --> 00:08:33.720]   they've created? You're hard pressed to find something that
[00:08:33.720 --> 00:08:36.400]   was even done in the 2010s. Yeah, actually, that's a good
[00:08:36.400 --> 00:08:40.440]   thought. I mean, the iPhone for Apple, iPhone was 2007. Yep.
[00:08:40.440 --> 00:08:45.040]   Microsoft was in the 1990s. Google was in 1998. With core
[00:08:45.040 --> 00:08:49.320]   search. Maybe there was maps and Gmail and tooth in the 2000s.
[00:08:49.400 --> 00:08:52.720]   Chrome, Android, they bought some of that Facebook, it was
[00:08:52.720 --> 00:08:55.440]   the core service that we built in the 2000s. And then they
[00:08:55.440 --> 00:08:58.360]   acquired brilliantly, right. So I'm not saying that they didn't
[00:08:58.360 --> 00:09:02.800]   acquire. Well, yeah, my point is that core organic innovation
[00:09:02.800 --> 00:09:05.320]   hasn't been there for a long time. So this is a moment to
[00:09:05.320 --> 00:09:08.080]   just be reflective of the fact that these are some incredible
[00:09:08.080 --> 00:09:13.720]   companies with ginormous cash flows. But now you've had this
[00:09:13.720 --> 00:09:18.040]   foundational platform shift, which exposes the fact that they
[00:09:18.040 --> 00:09:21.680]   really aren't good at innovating. And at times when
[00:09:21.680 --> 00:09:25.360]   they've tried to organically innovate, they've massively
[00:09:25.360 --> 00:09:28.840]   misallocated capital. Either through this would be the
[00:09:28.840 --> 00:09:31.080]   example of either through a bloated balance sheet. So
[00:09:31.080 --> 00:09:33.920]   someone claimed that Google overspends, or through just pure
[00:09:33.920 --> 00:09:36.920]   misallocation by starting projects that just are not large,
[00:09:36.920 --> 00:09:39.160]   but consume large amounts of cash. That would be the Facebook
[00:09:39.160 --> 00:09:44.400]   VR example. But in all of this, I think, when you cut staff and
[00:09:44.400 --> 00:09:51.240]   expenses as a way to meet and beat, and top line growth is in
[00:09:51.240 --> 00:09:54.840]   the low single digits, it's an important moment to recognize
[00:09:54.840 --> 00:09:58.280]   that these companies have now transitioned to being cash cows.
[00:09:58.280 --> 00:10:03.280]   And if you look at sort of how financial markets value cash
[00:10:03.280 --> 00:10:07.440]   cows, they're very valuable. But it's not where you look for
[00:10:07.440 --> 00:10:12.400]   growth. And so in a world where rates eventually get cut, and we
[00:10:12.400 --> 00:10:16.240]   start to come out of a recession, it tends to be that
[00:10:16.240 --> 00:10:18.920]   other people get rewarded. So that's an idea that's adding to
[00:10:18.920 --> 00:10:21.280]   your point here. They're not allowed to acquire these
[00:10:21.280 --> 00:10:25.680]   Microsoft's acquisition thing dead, dead, so they're not going
[00:10:25.680 --> 00:10:28.200]   to be allowed to buy stuff. So then you're right. What is the
[00:10:28.200 --> 00:10:31.360]   growth here? They're not able to innovate. I think these companies
[00:10:31.360 --> 00:10:34.840]   are x growth, which is why they use their cash flows to do what
[00:10:34.840 --> 00:10:40.280]   borrow money cheaply to buy back stock to manipulate their
[00:10:40.280 --> 00:10:43.240]   equity, right, you can manipulate and overcome
[00:10:43.240 --> 00:10:45.960]   dilution, you can manipulate earnings per share, you can
[00:10:45.960 --> 00:10:50.280]   manipulate the number of shares outstanding. And so just by the
[00:10:50.280 --> 00:10:53.560]   nature of that whole game, a bunch of passive investors will
[00:10:53.560 --> 00:10:56.080]   end up buying more, which helps the active investors who own
[00:10:56.080 --> 00:10:59.240]   that stock. So it's a game. So if we're not in the world,
[00:10:59.240 --> 00:11:02.720]   financial engineering would be the I mean, the most charitable
[00:11:02.720 --> 00:11:04.680]   way to say it, we're in the financial engineering phase,
[00:11:04.680 --> 00:11:06.680]   which is fine. And by the way, you can make a lot of money
[00:11:06.680 --> 00:11:11.200]   Facebook's up 90%. So there's a lot of there's a lot of meaning
[00:11:11.200 --> 00:11:14.840]   just this just this year. Oh, yes. So there's a lot of room
[00:11:14.840 --> 00:11:18.320]   for financial engineering, but it's not where you need to look
[00:11:18.320 --> 00:11:23.560]   to figure out where these big improvements and uses of this
[00:11:23.560 --> 00:11:25.480]   next generation platform technology are going to come
[00:11:25.480 --> 00:11:26.360]   from most likely
[00:11:26.360 --> 00:11:29.800]   saxes is a fair assessment in your mind looking at, you know,
[00:11:29.800 --> 00:11:32.160]   the the major tech companies, the fangs? Yeah, I mean, their
[00:11:32.160 --> 00:11:35.960]   growth is down to single digits. So I think Microsoft had 7%
[00:11:35.960 --> 00:11:40.120]   year over year revenue growth. Google was at 3%. I think
[00:11:40.120 --> 00:11:46.240]   Facebook was for sales rose to 3% from a year earlier, but at
[00:11:46.240 --> 00:11:48.880]   least that was an improvement because it's actually gone down
[00:11:48.880 --> 00:11:52.280]   for three straight quarters. So yeah, but you're down to, you
[00:11:52.280 --> 00:11:55.600]   know, single digit year over year growth rates. Nevertheless,
[00:11:55.600 --> 00:11:59.360]   most of these companies beat expectations. So Microsoft
[00:11:59.360 --> 00:12:05.560]   shares rose 9%. Meta jump 12% might be up more now.
[00:12:06.160 --> 00:12:09.560]   And I guess Google got a little bit of a bounce and they all
[00:12:09.560 --> 00:12:12.680]   gave a pretty upbeat forecast. The only one that wasn't upbeat
[00:12:12.680 --> 00:12:18.640]   was Google where the CFO Ruth Porat said that the outlook
[00:12:18.640 --> 00:12:22.080]   remains uncertain. But all the other ones seem to indicate that
[00:12:22.080 --> 00:12:26.120]   things are going to get better. So I think what's interesting
[00:12:26.120 --> 00:12:29.840]   about that is just the mismatch that we have between how well
[00:12:29.840 --> 00:12:33.520]   these companies did in this quarter versus how uncertain the
[00:12:33.520 --> 00:12:35.280]   rest of the economy is looking right now.
[00:12:35.320 --> 00:12:36.800]   And maybe the feds behavior.
[00:12:36.800 --> 00:12:40.040]   Yeah, so maybe this is the flip side, which Martha saying is
[00:12:40.040 --> 00:12:43.360]   they're not growing very fast, but they are profitable machines
[00:12:43.360 --> 00:12:46.280]   generating a lot of earnings. And they seem to be pretty
[00:12:46.280 --> 00:12:49.480]   immune from what's happening in the economy right now. Or at
[00:12:49.480 --> 00:12:52.040]   least that's what they're saying. Now, you're right in a
[00:12:52.040 --> 00:12:56.840]   parallel track, there was an interesting interview that pal
[00:12:56.840 --> 00:13:00.840]   did. So Jerome Powell gave an interview, it was actually kind
[00:13:00.840 --> 00:13:04.200]   of like one of these hoax calls where a couple of people
[00:13:04.200 --> 00:13:07.520]   pretending to be Salon ski engagement and interview.
[00:13:07.520 --> 00:13:09.280]   Oh my god, that was crazy.
[00:13:09.280 --> 00:13:11.600]   That reference.
[00:13:11.600 --> 00:13:15.680]   They've done this a number of times where they've gotten, you
[00:13:15.680 --> 00:13:17.920]   know, major leaders, I think they did this to Macron some
[00:13:17.920 --> 00:13:20.400]   other people where they pretend to be Zalinski and they do an
[00:13:20.400 --> 00:13:23.400]   interview. It's like Ali G. Yeah, but they played it
[00:13:23.400 --> 00:13:25.400]   straight. I don't care that he was fooled into giving the
[00:13:25.400 --> 00:13:27.680]   interview. It's like, who cares. But some of the things he said
[00:13:27.680 --> 00:13:29.760]   were really interesting. I mean, number one,
[00:13:31.120 --> 00:13:34.680]   pal said that the economic outlook for the year was looking
[00:13:34.680 --> 00:13:38.200]   pretty uncertain. And he said the most likely scenarios were
[00:13:38.200 --> 00:13:42.560]   either sub 1% growth. So staying out of recession, but just
[00:13:42.560 --> 00:13:46.560]   barely, or he said going into recession. So he thought that
[00:13:46.560 --> 00:13:49.560]   was roughly about equally likely he admitted that we had the
[00:13:49.560 --> 00:13:51.840]   worst inflation in 40 years. And that's why interest rates were
[00:13:51.840 --> 00:13:54.920]   necessary. And he said that it was necessary to slow the
[00:13:54.920 --> 00:13:58.160]   economy in order to combat inflation. And he then even went
[00:13:58.160 --> 00:14:02.160]   further and said that it was necessary to cool off the labor
[00:14:02.160 --> 00:14:05.840]   market, and even to cool off wages specifically, because
[00:14:05.840 --> 00:14:07.960]   that's how you combat inflation. That's the only thing we know
[00:14:07.960 --> 00:14:12.680]   how to do in a situation like this. So I think this is
[00:14:12.680 --> 00:14:15.200]   certainly a political mistake for pal to say that his
[00:14:15.200 --> 00:14:18.880]   objective here is to hurt the wages, the American people, and
[00:14:18.880 --> 00:14:21.800]   to basically cause a recession. But that is his view,
[00:14:21.800 --> 00:14:25.680]   apparently. And I think that we are headed for it seems like a
[00:14:25.680 --> 00:14:27.880]   recession. I'm a little surprised that the earnings
[00:14:27.880 --> 00:14:30.080]   reports for these tech companies are so good, because or at
[00:14:30.080 --> 00:14:31.840]   least their forecasts are so good.
[00:14:31.840 --> 00:14:34.280]   Well, they can cut spending. And we talked about this last year
[00:14:34.280 --> 00:14:37.320]   when we were trying to predict what would happen. I remember
[00:14:37.320 --> 00:14:39.400]   saying, Well, I think Chamath and I were talking about this.
[00:14:39.400 --> 00:14:41.480]   And I said, Well, Chamath was saying, hey, earnings are going
[00:14:41.480 --> 00:14:43.400]   to go down. And there's a PE. And I said, What if they just
[00:14:43.400 --> 00:14:46.360]   stop spending or they make a lot of cuts? Well, here we are,
[00:14:46.360 --> 00:14:49.240]   people are just saying, you know what, we're going to cut our way
[00:14:49.240 --> 00:14:54.200]   to and do stock buybacks. And that's another way of financial
[00:14:54.200 --> 00:14:59.160]   engineering to route around the Fed, right. And to make the
[00:14:59.160 --> 00:15:04.560]   stock go up. worked incredible for Facebook. I mean, my lord,
[00:15:04.560 --> 00:15:06.880]   they were at $91 a share, and now they're over 200.
[00:15:06.880 --> 00:15:10.640]   Right, but just to bring it back to the economy. So look, I think
[00:15:10.640 --> 00:15:14.040]   we agree that these tech companies seem to be pretty
[00:15:14.040 --> 00:15:16.640]   immune, they've got a large cushion in terms of their
[00:15:16.640 --> 00:15:18.880]   ability to consume generating earnings because of all the
[00:15:18.880 --> 00:15:21.880]   bloat that that actually gives them like a margin of error
[00:15:21.880 --> 00:15:24.640]   where they can just keep cutting to prop up earnings. I'm a
[00:15:24.640 --> 00:15:26.760]   little surprised that they think their revenue forecasts are
[00:15:26.760 --> 00:15:28.920]   going to be so positive, because again, they were guiding
[00:15:28.920 --> 00:15:31.680]   upwards, generally. So they seem to think they're not gonna be
[00:15:31.680 --> 00:15:34.640]   impacted by the recession. And maybe they won't be. Again, I
[00:15:34.640 --> 00:15:38.920]   think what was interesting from Powell is the way that he seemed
[00:15:38.920 --> 00:15:42.160]   to think that the only thing we know how to do this is basically
[00:15:42.160 --> 00:15:43.960]   what he said, the only thing we know how to do in this situation
[00:15:43.960 --> 00:15:48.200]   with inflation is to kill the economy is to slow the economy,
[00:15:48.360 --> 00:15:52.400]   and specifically to kill jobs and wages. And that was pretty
[00:15:52.400 --> 00:15:54.960]   remarkable to me, because there are other things we could do,
[00:15:54.960 --> 00:15:59.240]   such as, okay, well, one thing is that you don't have to print
[00:15:59.240 --> 00:16:01.880]   so much money, cut spending austerity. So yes, our fiscal
[00:16:01.880 --> 00:16:05.640]   policy remains completely out of whack. We're running $2 trillion
[00:16:05.640 --> 00:16:10.000]   annual deficits right now, past COVID. So he could have said,
[00:16:10.000 --> 00:16:12.840]   Listen, we could get off this reckless fiscal policy and be
[00:16:12.840 --> 00:16:16.200]   more restrained, but he didn't want to go there. The other
[00:16:16.200 --> 00:16:19.800]   thing he could have done was address the supply side. One of
[00:16:19.800 --> 00:16:21.800]   the ways that you can reduce inflation, it's not just to kill
[00:16:21.800 --> 00:16:24.840]   demand, you could actually affect supply chains. So like
[00:16:24.840 --> 00:16:27.560]   cost of energy, for example, energy is a huge input into the
[00:16:27.560 --> 00:16:30.520]   economy. And one of the things that happened at the beginning
[00:16:30.520 --> 00:16:34.160]   of this administration is they made it much harder to drill for
[00:16:34.160 --> 00:16:37.440]   oil and gas. And I think Biden sort of reversed course on that
[00:16:37.440 --> 00:16:39.160]   at the State of the Union. Remember, he had that line where
[00:16:39.160 --> 00:16:42.920]   he said, we can't get off oil and gas for 10 years, and the
[00:16:42.920 --> 00:16:45.560]   audience started laughing. But in any event, the point is just
[00:16:45.600 --> 00:16:48.840]   that it's too little too late, they could have done more on
[00:16:48.840 --> 00:16:51.600]   energy to keep costs low. And then there's a whole bunch of
[00:16:51.600 --> 00:16:54.320]   other critical inputs into the economy besides labor. And what
[00:16:54.320 --> 00:16:58.160]   you could do is, I think you could go category by category,
[00:16:58.160 --> 00:17:01.800]   and say, how do we get the price of these key inputs into our
[00:17:01.800 --> 00:17:04.920]   economy down? How do we resolve supply chain bottlenecks? How do
[00:17:04.920 --> 00:17:09.120]   we make it, you know, easier to get access to whatever the key
[00:17:09.120 --> 00:17:11.720]   commodity is? And I think there's things they could do if
[00:17:11.720 --> 00:17:13.800]   they're just willing to work at it. Maybe this isn't the Fed's
[00:17:13.800 --> 00:17:17.120]   job, this is more the administration. But what you
[00:17:17.120 --> 00:17:18.880]   could do is say, listen, we're going to make it easier for
[00:17:18.880 --> 00:17:22.320]   people to produce and create supply. And if you have a higher
[00:17:22.320 --> 00:17:27.360]   supply of goods and services, then you will start to bring
[00:17:27.360 --> 00:17:30.640]   inflation down because inflation is just the amount of money in
[00:17:30.640 --> 00:17:33.920]   the system, divided by the amount of goods and services.
[00:17:33.920 --> 00:17:37.000]   And when the amount of goods and services hasn't gone up, but the
[00:17:37.000 --> 00:17:39.040]   money supply has gone up tremendously, you're gonna have
[00:17:39.040 --> 00:17:40.960]   inflation. And that's why I think it's a little bit
[00:17:40.960 --> 00:17:43.400]   misplaced to be killing demand the way they're killing it is
[00:17:43.400 --> 00:17:47.520]   because fundamentally, the problem here is they flooded the
[00:17:47.520 --> 00:17:51.520]   economy with money, both through government stimulus and through
[00:17:51.520 --> 00:17:54.840]   quantitative easing. And then also, they made it harder on the
[00:17:54.840 --> 00:17:58.200]   supply side to produce certainly with energy. So it seems to me
[00:17:58.200 --> 00:18:00.440]   that the approach they're taking for us to get out of this, it's
[00:18:00.440 --> 00:18:04.960]   like taking a meat cleaver to the economy, or a sledgehammer,
[00:18:04.960 --> 00:18:07.880]   really. And it's the most violent possible way that they
[00:18:07.880 --> 00:18:11.800]   could solve the problem they previously created of too much
[00:18:11.800 --> 00:18:12.360]   inflation.
[00:18:12.800 --> 00:18:15.520]   And the other thing, obviously, is jobs, we're still sitting
[00:18:15.520 --> 00:18:19.440]   here with close to 10 million job openings. And the thing I'm
[00:18:19.440 --> 00:18:26.440]   hearing from the streets is that unemployment hacking is become a
[00:18:26.440 --> 00:18:30.440]   high art. And so labor force participation remains low. It's
[00:18:30.440 --> 00:18:34.040]   nowhere near the historic highs. We have been very permissive
[00:18:34.040 --> 00:18:37.000]   during COVID for good reasons to give people very extended
[00:18:37.000 --> 00:18:41.200]   benefits, people have now learned is my understanding. And
[00:18:41.200 --> 00:18:43.760]   this is something that's happening on a regional level
[00:18:43.760 --> 00:18:47.320]   state by state level. People are learning how to hack
[00:18:47.320 --> 00:18:49.520]   unemployment and not going to work and people are just not
[00:18:49.520 --> 00:18:52.760]   taking the jobs that are open, which are service industry jobs.
[00:18:52.760 --> 00:18:55.160]   Americans don't want to work on we don't want to let people into
[00:18:55.160 --> 00:18:57.400]   the country got wrecked wrecked low people coming into the
[00:18:57.400 --> 00:19:00.240]   country. It seems to me that would be a much more productive
[00:19:00.240 --> 00:19:00.920]   way to do this, right?
[00:19:00.920 --> 00:19:04.320]   So yes, I think it's an excellent point. Because exactly
[00:19:04.320 --> 00:19:06.080]   what you're doing there is addressing the supply side,
[00:19:06.080 --> 00:19:09.120]   which is you're unlocking the supply of a key input into the
[00:19:09.120 --> 00:19:12.040]   economy, which is all this unused labor, it's all these
[00:19:12.040 --> 00:19:13.920]   people aren't working, you're right, the labor force
[00:19:13.920 --> 00:19:17.280]   participation rate is still much lower than it could be. So if
[00:19:17.280 --> 00:19:20.160]   you get more people into the economy, then that helps
[00:19:20.160 --> 00:19:24.080]   alleviate the cost of labor, it helps fill these jobs, but it
[00:19:24.080 --> 00:19:27.680]   doesn't kill the economy. So it would be a much more positive
[00:19:27.680 --> 00:19:30.240]   way to address this. So I just think it showed a lack of
[00:19:30.240 --> 00:19:33.840]   creativity for him to say that the only thing we can do in this
[00:19:33.840 --> 00:19:37.800]   situation is not just to raise rates, he did say that, but but
[00:19:37.800 --> 00:19:43.080]   to go further, and cool off the job market, increase
[00:19:43.080 --> 00:19:47.520]   unemployment and cool off wages. I mean, that's gonna be a very
[00:19:47.520 --> 00:19:50.480]   unpopular thing to say, I think, because we're basically saying
[00:19:50.480 --> 00:19:53.200]   is you're gonna hurt the wages, the American people, who wants
[00:19:53.200 --> 00:19:53.700]   that?
[00:19:53.700 --> 00:19:56.160]   Here's the chart for job openings, we thought this would
[00:19:56.160 --> 00:20:00.000]   collapse, it went down, certainly, as we you know, I
[00:20:00.000 --> 00:20:02.960]   don't want to obsess over macro stuff, but it's still way up
[00:20:02.960 --> 00:20:06.200]   there. And so the fact that we can't get people to take these
[00:20:06.200 --> 00:20:08.560]   jobs, I don't know, Chamath, what do you think about the
[00:20:08.560 --> 00:20:12.760]   employment and participation such situation and how that
[00:20:12.760 --> 00:20:15.160]   might unlock things I also wanted to know from you, Chamath,
[00:20:15.160 --> 00:20:21.200]   this concept of the Fed is reacting to data just so slowly,
[00:20:21.200 --> 00:20:23.800]   and then you have these companies that maybe are more
[00:20:23.800 --> 00:20:25.760]   nimble, and they have better data than the Fed.
[00:20:25.760 --> 00:20:29.360]   I think that that's a truism. And I don't think anything about
[00:20:29.360 --> 00:20:32.160]   that is going to change. I mean, I think we talked about how
[00:20:32.160 --> 00:20:35.280]   these folks calculate nonfarm payrolls, or how they calculate
[00:20:35.280 --> 00:20:39.600]   CPI. It's incredibly outdated, right? It's people with
[00:20:39.600 --> 00:20:42.480]   clipboards, walking around talking to people and checking
[00:20:42.480 --> 00:20:48.520]   boxes and filling out forms. Can that change? Probably, it could
[00:20:48.520 --> 00:20:52.640]   will it change? It probably won't. And so they're going to
[00:20:52.640 --> 00:20:56.680]   focus on the most simple but most powerful measure that they
[00:20:56.680 --> 00:20:59.440]   have, which is controlling the money supply, kind of what Saks
[00:20:59.440 --> 00:21:02.320]   talked about. So they're going to manipulate the money supply
[00:21:02.720 --> 00:21:06.160]   to either put more liquidity in the system, in which case
[00:21:06.160 --> 00:21:09.320]   markets go up and asset prices go up, but then inflation goes
[00:21:09.320 --> 00:21:12.920]   up, or constrain liquidity, which then causes markets to go
[00:21:12.920 --> 00:21:16.040]   down, asset prices to go down and inflation eventually to go
[00:21:16.040 --> 00:21:19.240]   down. The thing that we're facing today, when you look at
[00:21:19.240 --> 00:21:22.200]   this labor market chart is a couple of things that I think
[00:21:22.200 --> 00:21:24.320]   we've talked about before. And I just want to reiterate them,
[00:21:24.320 --> 00:21:28.640]   which is you have to remember that we are in this new world
[00:21:28.640 --> 00:21:34.160]   order, which is the x China world order. And in that there
[00:21:34.160 --> 00:21:37.400]   is no more unitary economy that can do things cheaper, faster
[00:21:37.400 --> 00:21:40.960]   and better globally around the world. Right? So we're going to
[00:21:40.960 --> 00:21:44.480]   near shore or onshore all kinds of things that used to be done
[00:21:44.480 --> 00:21:48.360]   by the Chinese, they'll sit in Mexico, or they'll sit in
[00:21:48.360 --> 00:21:51.400]   Central America, maybe in some cases, they'll sit in Canada,
[00:21:51.400 --> 00:21:55.440]   and all of that will feed into the United States. The problem
[00:21:55.440 --> 00:21:58.720]   with all of that is that that will keep costs higher, because
[00:21:58.720 --> 00:22:03.520]   it'll be naturally more inefficient, it will naturally
[00:22:03.520 --> 00:22:07.560]   take more money. And that will naturally cause the prices of
[00:22:07.560 --> 00:22:10.600]   those things to be higher, which means that terminal inflation, I
[00:22:10.600 --> 00:22:17.000]   think is just roughly higher. As a result, I think that more
[00:22:17.000 --> 00:22:22.280]   power, if you will, goes to labor. So in this constant
[00:22:22.280 --> 00:22:24.640]   tension that we have in an economy between labor and
[00:22:24.640 --> 00:22:27.560]   capital, the people that own the factories, or the businesses and
[00:22:27.560 --> 00:22:30.320]   the people that run them and work inside of them, we've been
[00:22:30.320 --> 00:22:32.960]   in this position where the pendulum has swung so far
[00:22:32.960 --> 00:22:36.760]   towards capital, the owners, the shareholders, that all this
[00:22:36.760 --> 00:22:39.960]   financial engineering has tremendous upside, right? That's
[00:22:39.960 --> 00:22:44.200]   why companies engage in it. But when you show that chart, Jason,
[00:22:44.200 --> 00:22:46.320]   what it means is, it's just really hard to find people. And
[00:22:46.320 --> 00:22:49.080]   so the only way you're going to get people off their butt, to go
[00:22:49.080 --> 00:22:51.680]   into work to sit in a chair to do a job that you need them to
[00:22:51.680 --> 00:22:56.120]   do is to pay them more. And in finding that wages will have to
[00:22:56.120 --> 00:23:00.880]   go up. The counterbalance of that is what AI will do, which I
[00:23:00.880 --> 00:23:04.560]   think we all agree is to say, that is the key. Yeah, which is
[00:23:04.560 --> 00:23:08.320]   massively deflationary. So that is going to be the tension that
[00:23:08.320 --> 00:23:10.600]   we're in now for a really long time as we explore this. I don't
[00:23:10.600 --> 00:23:14.120]   know if you guys saw today, but Sequoia led a $20 million round
[00:23:14.120 --> 00:23:18.200]   in this thing called Harvey dot AI, the legal, yeah, which is
[00:23:18.200 --> 00:23:21.200]   like a legal super wizard for law firms. Yeah, we knew that
[00:23:21.200 --> 00:23:25.440]   was coming. And my partners and I were debating it. And what we
[00:23:25.440 --> 00:23:29.400]   thought of was, well, how much do you pay out of the 800 or
[00:23:29.400 --> 00:23:33.320]   $1,000 an hour that you charge to Harvey dot AI, maybe you're
[00:23:33.320 --> 00:23:37.960]   willing to pay 5% or 10%. But then the reality is that one of
[00:23:37.960 --> 00:23:41.680]   the most powerful things it does is is able to go into Westlaw,
[00:23:41.680 --> 00:23:45.760]   find all these cases, and say, yeah, this is germane to the
[00:23:45.760 --> 00:23:48.080]   thing that you're working on right now. That's a very useful
[00:23:48.080 --> 00:23:51.600]   thing. But the end plus first law firm will also use that tool
[00:23:51.600 --> 00:23:54.280]   and instead of charging $800 an hour, they'll say, well, we'll
[00:23:54.280 --> 00:23:56.480]   charge 600 bucks an hour, and we're still willing to give you
[00:23:56.480 --> 00:24:01.480]   five or 10%. So I just don't see a world where, on the one hand,
[00:24:01.480 --> 00:24:05.560]   physical labor will continue to be more expensive, they'll
[00:24:05.560 --> 00:24:09.480]   demand more and more money to do the job that they're asked to do.
[00:24:09.480 --> 00:24:12.840]   And the knowledge work will become increasingly more
[00:24:12.840 --> 00:24:16.480]   deflationary, because so much of it will be automated by AI, that
[00:24:16.480 --> 00:24:18.880]   those folks will charge less and less. And there's going to be
[00:24:18.880 --> 00:24:20.760]   attention there. And I don't exactly know what's going to
[00:24:20.760 --> 00:24:23.560]   happen. I did a couple of experiments. This week, I've
[00:24:23.560 --> 00:24:27.280]   been rolling up my sleeves and playing with these tools. It's
[00:24:27.280 --> 00:24:31.400]   pretty amazing. And I've been trying to use them for actual
[00:24:31.400 --> 00:24:34.200]   tasks in our companies. What have you learned? What did you
[00:24:34.200 --> 00:24:38.880]   do? And what have you learned? So I got on the open AI plugins,
[00:24:38.880 --> 00:24:42.280]   Greg, thank you, I sent him my email, and he got me onto that.
[00:24:42.320 --> 00:24:46.960]   And you can connect it to Zapier. So I have two projects
[00:24:46.960 --> 00:24:50.040]   I'm working on currently, one of them I was as since I'm raised,
[00:24:50.040 --> 00:24:52.360]   so raising launch one for and I'm actually going out to people
[00:24:52.360 --> 00:24:55.440]   not just taking inbound. I was like, Hey, can I get the names
[00:24:55.440 --> 00:24:57.880]   of all the major LPS and start doing some research, they're
[00:24:57.880 --> 00:25:01.560]   putting a table stuff that sacks did when he does blog post. But
[00:25:01.560 --> 00:25:05.400]   then I started connecting it with finding people's Twitter
[00:25:05.400 --> 00:25:10.040]   handles finding their LinkedIn profiles. And then the next
[00:25:10.040 --> 00:25:13.240]   piece I'm working on is automatically following them DM
[00:25:13.240 --> 00:25:16.640]   ing them on Twitter, let's say, or following them on and doing
[00:25:16.640 --> 00:25:19.960]   an in message saying, Hey, we haven't met. Here's the deal
[00:25:19.960 --> 00:25:22.800]   memo for my next fund would love to, you know, get together.
[00:25:22.800 --> 00:25:26.080]   This is sent from Jason's AI script, I was gonna like
[00:25:26.080 --> 00:25:28.720]   actually tell them, but here's my real email if after you read
[00:25:28.720 --> 00:25:32.280]   the summary of the next fund you want to meet. And then I was
[00:25:32.280 --> 00:25:34.960]   going to pair that and this is a piece I'm going to probably need
[00:25:34.960 --> 00:25:37.680]   a developer to do with our internal LP database to not
[00:25:37.680 --> 00:25:41.520]   email people who already duplicates. And then inside with
[00:25:41.520 --> 00:25:45.920]   newsletters, I have it building a database of every newsletter
[00:25:45.920 --> 00:25:50.040]   we've ever sent the writing style, and then I'm having it go
[00:25:50.040 --> 00:25:53.240]   find in real time news stories that we should be including in
[00:25:53.240 --> 00:25:55.720]   the newsletters, which I think will make the writers right now
[00:25:55.720 --> 00:26:00.000]   a third more productive. But these are things that would cost
[00:26:00.000 --> 00:26:04.000]   4050 bucks an hour 30 bucks an hour for you know, college
[00:26:04.000 --> 00:26:07.720]   educated Americans and Canadians. And I have already
[00:26:07.720 --> 00:26:10.400]   figured out and I'm not a developer anymore, I had a
[00:26:10.400 --> 00:26:13.280]   script them and I'm actually thinking about learning to code
[00:26:13.280 --> 00:26:17.240]   again, just so I can do this myself. And so on Saturday, I'm
[00:26:17.240 --> 00:26:19.720]   going to do a little coding with a friend of mine and get back up
[00:26:19.720 --> 00:26:23.880]   to speed on that. I think about 30% of what knowledge workers
[00:26:23.880 --> 00:26:28.720]   do right now is possible. So I put every single person at both
[00:26:28.720 --> 00:26:34.960]   companies on chat GPT for and the sand the playground about 30%
[00:26:34.960 --> 00:26:40.240]   of what knowledge workers at both firms can do currently is
[00:26:40.240 --> 00:26:43.240]   doable if you can figure out and this stuff is not perfectly
[00:26:43.240 --> 00:26:46.120]   scripted yet. So I've been doing some stuff in travel as well
[00:26:46.120 --> 00:26:51.080]   playing with the kayak interface, Expedia interface,
[00:26:51.080 --> 00:26:55.680]   etc. To look at travel planning, and it's pretty good as well. So
[00:26:55.680 --> 00:27:01.160]   it's, it's this is the real deal, folks, I think by the end
[00:27:01.160 --> 00:27:04.880]   of this year, 30% of knowledge work could be done by this. And
[00:27:04.880 --> 00:27:09.880]   then additionally, on Monday, I went back to work in person. And
[00:27:09.880 --> 00:27:14.120]   I went to I hosted our accelerator in person. And then
[00:27:14.120 --> 00:27:17.360]   I hosted found a university in person in the city, the city was
[00:27:17.360 --> 00:27:21.320]   absolutely dead. But we had 100 people fly in from around the
[00:27:21.320 --> 00:27:23.280]   world for our founding university and a lot of them are
[00:27:23.280 --> 00:27:27.160]   working on AI projects. And what's very interesting is like
[00:27:27.160 --> 00:27:32.040]   there's this big debate going on. Friedberg between is this
[00:27:32.040 --> 00:27:35.320]   going to be built into chat GPT for or barred or you know, Poe
[00:27:35.320 --> 00:27:38.760]   or whatever it is, or should I even bother? So should I bother
[00:27:38.760 --> 00:27:42.440]   building, you know, a verticalized app. And it turns
[00:27:42.440 --> 00:27:46.520]   out like I think you should do the verticalized app. And you're
[00:27:46.520 --> 00:27:50.960]   going to be able to put together multiple of these AI eyes that
[00:27:50.960 --> 00:27:55.240]   have different specialties. So I'm super stoked about it. But I
[00:27:55.240 --> 00:27:58.200]   do think if you're not using this, if you hear my voice right
[00:27:58.200 --> 00:28:00.480]   now, and you're a white collar worker, a knowledge worker, and
[00:28:00.480 --> 00:28:04.280]   you're not using this this year and getting up to speed on it, I
[00:28:04.280 --> 00:28:06.040]   think you'll be out of a job within the next two.
[00:28:06.040 --> 00:28:08.160]   She's Wow.
[00:28:08.160 --> 00:28:11.120]   I just don't think you'll compete. It would be like
[00:28:11.120 --> 00:28:13.600]   trying to compete without knowing how to use Microsoft
[00:28:13.600 --> 00:28:17.000]   Office 20 years ago. Right? Like, could you work and not no
[00:28:17.000 --> 00:28:20.520]   email? Remember when we came into the workforce 30 years ago,
[00:28:20.520 --> 00:28:24.760]   and some people knew office and email, and web research, and
[00:28:24.760 --> 00:28:27.560]   then other people didn't, those other people retired, they were
[00:28:27.560 --> 00:28:30.360]   phased out, if you didn't know how to use a computer, and type
[00:28:30.360 --> 00:28:33.120]   and use an Excel spreadsheet or do a PowerPoint, you were done.
[00:28:33.120 --> 00:28:35.360]   I think there's two possible ways you can interpret what
[00:28:35.360 --> 00:28:39.520]   you're saying. So in terms of the economic impact, so one is
[00:28:39.520 --> 00:28:42.520]   that you could say, well, the AI is going to do 30% of the
[00:28:42.520 --> 00:28:44.720]   knowledge work, therefore, 30% of the knowledge workers are
[00:28:44.720 --> 00:28:48.080]   going to be put out of work. I think that a different way to
[00:28:48.080 --> 00:28:51.240]   put it would be every knowledge worker can get 30% more work
[00:28:51.240 --> 00:28:55.760]   done. Correct. So if that's the case, then they're more
[00:28:55.760 --> 00:28:58.240]   productive. And we're just talking about the problem of how
[00:28:58.240 --> 00:29:01.080]   do you increase real wages in the economy without having
[00:29:01.080 --> 00:29:04.160]   inflation? Well, the way to do that is for every worker be more
[00:29:04.160 --> 00:29:07.240]   productive. So if every worker is 30% more productive, in
[00:29:07.240 --> 00:29:10.360]   theory, their wages should be able to go up by up to 30%.
[00:29:10.360 --> 00:29:14.200]   That's how you get wage growth. Now, maybe there will be some
[00:29:14.200 --> 00:29:18.200]   companies that don't need all those employees, because now
[00:29:18.200 --> 00:29:21.000]   they're able to get, you know, whatever, a third more done. But
[00:29:21.000 --> 00:29:22.800]   there'll be other companies who can hire them, they can go off
[00:29:22.800 --> 00:29:25.240]   and do other jobs for other companies, especially when
[00:29:25.240 --> 00:29:27.880]   you've got this backlog of like you said, eight or 10 million
[00:29:27.880 --> 00:29:30.680]   new, you know, jobs that are on those jobs are all service
[00:29:30.680 --> 00:29:33.840]   though. You know, they're not you're actually gonna have this
[00:29:33.840 --> 00:29:37.080]   big group of knowledge workers, there's just nothing for them to
[00:29:37.080 --> 00:29:38.080]   do. Oh, no, I just don't.
[00:29:38.080 --> 00:29:40.120]   I agree with you. But I think there's going to be a group of
[00:29:40.120 --> 00:29:42.360]   knowledge workers who do not embrace this and do not make the
[00:29:42.360 --> 00:29:46.960]   transition because it is, it's going to require an upscaling.
[00:29:46.960 --> 00:29:49.120]   Like, I think you're actually going to need to know how to do
[00:29:49.120 --> 00:29:52.160]   some basic programming and coding to really take advantage
[00:29:52.160 --> 00:29:54.160]   of these at least like scripting level stuff.
[00:29:54.160 --> 00:29:57.520]   I don't know, it's pretty easy to use. I agree with you there.
[00:29:57.520 --> 00:29:57.720]   Maybe
[00:29:57.720 --> 00:30:00.600]   writing a blog post, but the date the example I gave of like
[00:30:00.600 --> 00:30:04.880]   taking the LP database, sorting it, you know, it's not quite
[00:30:04.880 --> 00:30:05.160]   there yet.
[00:30:05.160 --> 00:30:06.920]   This is like a chatbot.
[00:30:06.920 --> 00:30:11.240]   It is like I think it takes like level two programming skills.
[00:30:11.280 --> 00:30:13.520]   No, it doesn't. No, you don't have to know the program, you
[00:30:13.520 --> 00:30:16.240]   just have to know how to prompt it in natural language. It's the
[00:30:16.240 --> 00:30:19.480]   opposite of need to learn how to code. The thing that makes
[00:30:19.480 --> 00:30:22.680]   coding hard is that you have to learn the specific commands.
[00:30:22.680 --> 00:30:24.880]   It's like its own language, you have to learn a new language.
[00:30:24.880 --> 00:30:28.000]   With this, you don't. In fact, one of the cool things about
[00:30:28.000 --> 00:30:33.080]   some of these open AI API's is that you just tell it what you
[00:30:33.080 --> 00:30:35.440]   want it to do. There's not even like a scripting language, a lot
[00:30:35.440 --> 00:30:39.440]   of it's in natural language. And that makes it incredibly easy to
[00:30:39.440 --> 00:30:43.560]   use even for developers. So I don't think this is a hard
[00:30:43.560 --> 00:30:45.760]   technology to use. I agree with you, there may be people who are
[00:30:45.760 --> 00:30:48.760]   resistant to it, because there's always people who are resistant
[00:30:48.760 --> 00:30:50.840]   to change a new technology. And you're right, if they don't
[00:30:50.840 --> 00:30:53.720]   adapt, they're going to be dinosaurs. But I don't think
[00:30:53.720 --> 00:30:57.280]   this is a hard technology to grok how to use and get benefit
[00:30:57.280 --> 00:30:57.640]   from.
[00:30:57.640 --> 00:31:02.040]   You might be right. I mean, right now, it's so new, that the
[00:31:02.040 --> 00:31:05.240]   glue between systems is just not there yet. And maybe you'll be
[00:31:05.240 --> 00:31:09.480]   able to talk to chat GPT for and it'll connect your database on
[00:31:09.480 --> 00:31:13.160]   notion, it will take a type form and a survey monkey and put it
[00:31:13.160 --> 00:31:15.400]   all together and figure that all out for you. Oh, yeah,
[00:31:15.400 --> 00:31:17.560]   that's the whole game right now is still connecting all these
[00:31:17.560 --> 00:31:19.960]   things. And that and that's what I'm talking about. And like,
[00:31:19.960 --> 00:31:23.800]   that's kind of not there yet. But in the auto GPT stuff, you
[00:31:23.800 --> 00:31:28.240]   need a developer right now. But anyway, I'm deep in it. And I am
[00:31:28.240 --> 00:31:33.760]   more excited right now. This feels to me like 2005 to 2012
[00:31:33.760 --> 00:31:38.720]   period, when you just saw Ajax and the web and speed, just all
[00:31:38.720 --> 00:31:43.600]   coming together so quickly. And the rapid iteration is just
[00:31:43.600 --> 00:31:47.880]   unbelievable. I every day I find a new use for it. I have made my
[00:31:47.880 --> 00:31:52.560]   default web page opening. Like when I open a new page on my PC,
[00:31:52.560 --> 00:31:56.240]   it just opens chat GPT for now, just so I'm forcing myself to
[00:31:56.240 --> 00:31:59.520]   use it for every possible task. And the people who work for me,
[00:31:59.520 --> 00:32:01.880]   some of them are doing it's most of them are not and I'm just
[00:32:01.880 --> 00:32:06.640]   trying to drag everybody along. And then you have at the same
[00:32:06.640 --> 00:32:12.760]   time, this remote work thing happening, where salaries I'm
[00:32:12.760 --> 00:32:16.600]   finding are starting to normalize not across cities, but
[00:32:16.600 --> 00:32:22.280]   across countries. So, you know, hiring somebody in Canada,
[00:32:22.280 --> 00:32:27.640]   Estonia, San Paolo, and then you add this AI to it. The cost to
[00:32:27.640 --> 00:32:31.120]   do things is, this is like, I don't know, I think everything's
[00:32:31.120 --> 00:32:34.720]   going to cost about 10. All this knowledge work is going to be 10%
[00:32:34.720 --> 00:32:39.120]   as expensive to do. I don't think it's 10% less Chamath or
[00:32:39.120 --> 00:32:42.560]   that, you know, I think it's like 90% 10 cents on the dollar.
[00:32:42.560 --> 00:32:47.880]   I agree. And it's not this is not a five year 10 year
[00:32:47.880 --> 00:32:50.080]   prediction. This is like, five quarter 10.
[00:32:50.080 --> 00:32:53.880]   We said that the first organizations to use this like
[00:32:53.880 --> 00:32:56.720]   the canary in the coal mine would be the consulting
[00:32:56.720 --> 00:32:59.240]   organizations. And today when Harvey got announced one of the
[00:32:59.240 --> 00:33:01.840]   things that that right on the heels of that Pricewaterhouse
[00:33:01.840 --> 00:33:04.760]   Coopers announced like a billion dollar investment into AI,
[00:33:04.760 --> 00:33:07.880]   which makes sense, because as a consulting organization full of
[00:33:07.880 --> 00:33:11.480]   lawyers and accountants and it folks, those are the services
[00:33:11.480 --> 00:33:15.520]   jobs that you get tremendous leverage. If you were to use
[00:33:15.520 --> 00:33:17.360]   these tools, freebirg, any thoughts?
[00:33:17.360 --> 00:33:20.520]   I don't know. I mean, I think we kind of beat this horse to
[00:33:20.520 --> 00:33:23.720]   death. We've talked about it for a couple months. And I think we
[00:33:23.720 --> 00:33:25.000]   just keep repeating ourselves.
[00:33:25.000 --> 00:33:27.160]   Are you doing anything when you're firsthand? Are you
[00:33:27.160 --> 00:33:28.400]   playing with it yourself? Yeah.
[00:33:29.160 --> 00:33:31.840]   Look, I tell us about that. By the way, one thing I will say,
[00:33:31.840 --> 00:33:34.960]   we all talk about cost reduction, and then oh, you
[00:33:34.960 --> 00:33:37.160]   know, knowledge work is dead, and we're going to save money
[00:33:37.160 --> 00:33:41.560]   and all this stuff. What that is always the first reaction to any
[00:33:41.560 --> 00:33:46.040]   new point of leverage realized from some novel technology. The
[00:33:46.040 --> 00:33:51.160]   second is suddenly people start doing things that use that
[00:33:51.160 --> 00:33:54.480]   leverage to do things that they couldn't have done before. So
[00:33:54.480 --> 00:33:57.440]   it's not just about dropping costs. It's about enabling new
[00:33:57.440 --> 00:34:02.080]   things that does 100 times more or unimaginable things prior.
[00:34:02.080 --> 00:34:06.280]   And I think the next phase of this AI shockwave that that kind
[00:34:06.280 --> 00:34:10.760]   of hit us and hit the world and you know, kind of hit enterprises
[00:34:10.760 --> 00:34:15.160]   is going to be the evolution of integrating those tools in a
[00:34:15.160 --> 00:34:19.640]   very unique way with other tools to drive very novel things
[00:34:19.640 --> 00:34:22.800]   forward to create new things, new projects, new progress. That
[00:34:22.800 --> 00:34:26.160]   was unfathomable before. So it's not just about cost savings,
[00:34:26.160 --> 00:34:28.920]   it's going to be about new stuff. I shared a link on
[00:34:28.920 --> 00:34:33.320]   Twitter yesterday, there was some guy, I want to quote him
[00:34:33.320 --> 00:34:38.960]   correctly. His name is McKay Wrigley. So shout out to McKay
[00:34:38.960 --> 00:34:43.200]   on his Twitter page, it says that he didn't know how to code
[00:34:43.200 --> 00:34:47.360]   in 2019. He learned how to code for the first time he taught
[00:34:47.360 --> 00:34:54.680]   himself. And he put together an object recognition tool with
[00:34:54.680 --> 00:35:00.080]   chat GPT. I saw this video crazy with his webcam. And basically
[00:35:00.080 --> 00:35:03.280]   he holds up like a diet Coke and he's like, you know, tell me how
[00:35:03.280 --> 00:35:05.320]   many calories What is this and how many calories are in it? And
[00:35:05.320 --> 00:35:08.520]   it's like, oh, there's no calories in it. It's a diet Coke.
[00:35:08.520 --> 00:35:11.200]   And he does this three different times with three different
[00:35:11.200 --> 00:35:14.040]   objects. And he hacked this thing together in a couple of
[00:35:14.040 --> 00:35:18.160]   hours. That is a product that was like, theoretically
[00:35:18.160 --> 00:35:21.680]   unfeasible, or, you know, kind of very, very difficult to kind
[00:35:21.680 --> 00:35:24.200]   of see how you would put that piece together quickly and
[00:35:24.200 --> 00:35:29.120]   easily with one person in a room in a few hours, a year ago. And
[00:35:29.120 --> 00:35:31.840]   here you see a demo of this person who didn't know how to
[00:35:31.840 --> 00:35:34.760]   code not too long ago, putting it together and creating this
[00:35:34.760 --> 00:35:37.800]   product that would have been such a profound startup. Imagine
[00:35:37.800 --> 00:35:40.380]   if you went to VCs 18 months ago, and we're like, Look, I've
[00:35:40.380 --> 00:35:42.560]   got this thing and I hold stuff up in front of it, it tells me
[00:35:42.560 --> 00:35:45.280]   all about it. And it talks to me. And I literally use my voice
[00:35:45.280 --> 00:35:49.120]   to talk to it. And he basically strung together a text to
[00:35:49.120 --> 00:35:52.800]   speech, chat, GPT, an object recognition tool, all of this
[00:35:52.800 --> 00:35:56.200]   stuff completely open source, and a plugin that does web
[00:35:56.200 --> 00:36:00.000]   browsing. And the whole thing is basically like your own
[00:36:00.000 --> 00:36:04.200]   interactive visual robot. It's an incredible product demo. And
[00:36:04.200 --> 00:36:06.080]   I thought it was so amazing and profound. I sure it's a
[00:36:06.080 --> 00:36:09.560]   prototype. And it's kind of janky, but it was done in a few
[00:36:09.560 --> 00:36:13.360]   hours on almost a no code basis. It's incredible. So what's going
[00:36:13.360 --> 00:36:17.840]   to come from that is a whole set of new products, and ideas, and
[00:36:17.840 --> 00:36:20.720]   things that we are certainly not thinking about today, but in six
[00:36:20.720 --> 00:36:24.200]   months, is going to become almost mainstay. And many new
[00:36:24.200 --> 00:36:27.400]   categories of products, many new industries, many new businesses
[00:36:27.400 --> 00:36:30.200]   are going to emerge that we're not even thinking about. So the
[00:36:30.200 --> 00:36:32.800]   Luddite argument of Oh, this is going to destroy jobs and
[00:36:32.800 --> 00:36:35.800]   destroy the economy and drop costs by 90%. lawyers are going
[00:36:35.800 --> 00:36:38.560]   to get cheaper, etc, etc. I think that doesn't even matter.
[00:36:38.560 --> 00:36:41.120]   It's the tip of the iceberg. What's more exciting is all the
[00:36:41.120 --> 00:36:43.640]   new evolutionary stuff that's going to hit the market that's
[00:36:43.640 --> 00:36:47.480]   really going to transform the things that we can do, and that
[00:36:47.480 --> 00:36:48.640]   we didn't realize we could do.
[00:36:48.680 --> 00:36:51.320]   There's an incredible analogy for this, because what you're
[00:36:51.320 --> 00:36:55.440]   really talking about is more people being able to use tools
[00:36:55.440 --> 00:36:59.360]   and be creators. And what happened in the 80s and 90s,
[00:36:59.360 --> 00:37:02.800]   when the NBA started playing exhibition games around the
[00:37:02.800 --> 00:37:05.720]   world, was more people around the world started playing
[00:37:05.720 --> 00:37:09.240]   basketball, and then you started seeing people like Luca, or
[00:37:09.240 --> 00:37:12.800]   before him, Yao Ming, Mutombo, you start to have people from
[00:37:12.800 --> 00:37:15.480]   around the world who had never been exposed to basketball, just
[00:37:15.480 --> 00:37:19.760]   incredible, Porzingis, incredible talents emerged,
[00:37:19.760 --> 00:37:22.040]   because you just had more people playing with the basketball, I
[00:37:22.040 --> 00:37:25.000]   think you're gonna have more people playing with code and
[00:37:25.000 --> 00:37:28.160]   building products. So you're gonna have incredible amounts of
[00:37:28.160 --> 00:37:32.840]   creativity from people who maybe you didn't expect, because they
[00:37:32.840 --> 00:37:36.960]   didn't go to school for coding, or have that opportunity. Hey,
[00:37:36.960 --> 00:37:43.440]   I mentioned I was in Phi Di, and I was at Fenwick's office and
[00:37:43.440 --> 00:37:47.520]   then Wilson's and Cine's offices to law firms being the law firms
[00:37:47.520 --> 00:37:51.160]   in the financial district in the market arrow, it was an absolute
[00:37:51.160 --> 00:37:53.880]   ghost town. And when I say ghost town, I mean, like serious
[00:37:53.880 --> 00:37:58.440]   ghost town, like, weird, like this is still like, being in
[00:37:58.440 --> 00:38:01.480]   some dystopian science fiction, they were the last man on earth.
[00:38:01.480 --> 00:38:05.920]   And then we saw in the group chat today, 350 California
[00:38:05.920 --> 00:38:10.400]   Street was worth $300 million four years old, that's a 22
[00:38:10.400 --> 00:38:14.840]   story glass and stone tower picture of it. It's going up
[00:38:14.840 --> 00:38:17.920]   for sale. And they believe according to the Wall Street
[00:38:17.920 --> 00:38:23.120]   Journal, that bids will come in at $60 million, an 80% decline.
[00:38:23.120 --> 00:38:26.080]   And we talked about this commercial real estate would
[00:38:26.080 --> 00:38:29.600]   have this moment, a lot of the banks, the smaller regional
[00:38:29.600 --> 00:38:34.240]   banks own this debt. Saks, what do you think is going to happen
[00:38:34.240 --> 00:38:39.680]   here? Who is the person who would buy an office tower in
[00:38:39.680 --> 00:38:43.760]   downtown, even at an 80% discount, knowing that you have
[00:38:43.760 --> 00:38:49.080]   to pay all those carrying costs? And there's so much vacant
[00:38:49.080 --> 00:38:53.000]   office space, and it's only increasing? Right? What's your
[00:38:53.000 --> 00:38:54.080]   who buys this?
[00:38:54.080 --> 00:38:55.520]   It's called land banking.
[00:38:55.520 --> 00:38:56.440]   Okay, explain.
[00:38:56.440 --> 00:38:59.360]   So in other words, okay, what I mean is, you're right, there's
[00:38:59.360 --> 00:39:02.080]   30% vacancy in San Francisco right now, maybe going up even
[00:39:02.080 --> 00:39:06.360]   more in the next few years as leases roll, and people take
[00:39:06.360 --> 00:39:10.880]   less space, you may have a countervailing effect in terms
[00:39:10.880 --> 00:39:15.160]   of new companies moving back because of AI or expanding. So
[00:39:15.160 --> 00:39:17.320]   it's possible you start to see some growth in the office
[00:39:17.320 --> 00:39:21.960]   market in San Francisco. But the bottom line is 30% plus vacancy
[00:39:21.960 --> 00:39:27.160]   is going to take years and years of growth in order to absorb. So
[00:39:27.160 --> 00:39:30.040]   you're right, this building, they can slash it to rent, but
[00:39:30.040 --> 00:39:32.440]   they still probably can't fill it. I mean, there's just no,
[00:39:32.440 --> 00:39:34.920]   there's just no demand. So you're gonna be sitting on that
[00:39:34.920 --> 00:39:38.480]   property for five years, 10 years before the market comes
[00:39:38.480 --> 00:39:42.320]   back the way that you need it to. But there's no value, right?
[00:39:42.320 --> 00:39:44.880]   Oh, it's going to trade way below its replacement costs,
[00:39:44.880 --> 00:39:47.920]   right? If you were to build that building today, it would cost
[00:39:47.920 --> 00:39:50.600]   you many times what they're going to pay for it. The problem
[00:39:50.600 --> 00:39:53.800]   is you can't finance that purchase with debt, because the
[00:39:53.800 --> 00:39:56.040]   building is not going to generate enough revenue. So
[00:39:56.040 --> 00:39:57.960]   that's what I mean by land banking, it's gonna have to be
[00:39:57.960 --> 00:40:01.720]   an equity investor, who's willing to think long term, and
[00:40:01.720 --> 00:40:04.680]   say, I'm going to buy this at a super distressed price, and I'm
[00:40:04.680 --> 00:40:08.560]   just going to sit there and hold it and wait. carry it, like you
[00:40:08.560 --> 00:40:10.960]   said, bear the carrying costs until the market comes back.
[00:40:10.960 --> 00:40:13.760]   But J kal, I want to say something, I think it's a great
[00:40:13.760 --> 00:40:17.320]   analogy, because public growth stocks have declined 70 plus
[00:40:17.320 --> 00:40:21.880]   percent, right since the market started to decline. And we've
[00:40:21.880 --> 00:40:25.680]   talked a lot about the statistic that I've shared a bunch
[00:40:25.680 --> 00:40:30.080]   publicly on how 70% of publicly traded companies that have gone
[00:40:30.080 --> 00:40:33.800]   public since 2020 are trading below their total cash invested
[00:40:33.800 --> 00:40:37.080]   since since founding, which should translate to an estimate
[00:40:37.080 --> 00:40:39.800]   that call it somewhere on the order of 70% of private
[00:40:39.800 --> 00:40:41.640]   companies are probably worth less than their preference
[00:40:41.640 --> 00:40:45.400]   stack. And so they're not worthless companies, they just
[00:40:45.400 --> 00:40:48.520]   have a capital structure that is upside down, those companies are
[00:40:48.520 --> 00:40:51.480]   making products for customers as product, those customers are
[00:40:51.480 --> 00:40:54.040]   paying money for those products, there's value there, there's
[00:40:54.040 --> 00:40:57.400]   real value there, the value has just been reset. And so it's
[00:40:57.400 --> 00:41:00.360]   interesting, it's not just the asset class of growth stocks and
[00:41:00.360 --> 00:41:03.320]   the asset class of private companies or private tech. It's
[00:41:03.320 --> 00:41:06.560]   also you know, in commercial real estate, we we try and treat
[00:41:06.560 --> 00:41:10.400]   each of these as if they were in isolation. But the problem is
[00:41:10.400 --> 00:41:15.560]   many of these assets were funded with some degree of leverage
[00:41:15.560 --> 00:41:20.480]   preferred stock is leverage. And you know, it is a form of debt
[00:41:20.480 --> 00:41:22.880]   because it has a preference over the shareholders, the common
[00:41:22.880 --> 00:41:26.000]   shareholders, the equity holders. And the same is true
[00:41:26.000 --> 00:41:28.280]   with this commercial real estate market that there was a certain
[00:41:28.280 --> 00:41:31.480]   amount of debt. So the availability of low cost capital
[00:41:31.840 --> 00:41:34.560]   and securitized against some asset in the form of debt or in
[00:41:34.560 --> 00:41:37.560]   the form of preferred stock in a private company has the same
[00:41:37.560 --> 00:41:40.640]   effect, which it allowed the valuations to balloon on the
[00:41:40.640 --> 00:41:43.880]   equity. And now that the market has re rationalized, the price
[00:41:43.880 --> 00:41:47.480]   is down 70 plus percent across all three of these connected,
[00:41:47.480 --> 00:41:49.600]   but you know, somewhat disparate asset classes, you're kind of
[00:41:49.600 --> 00:41:52.640]   having this big reset moment. And funny enough, the other
[00:41:52.640 --> 00:41:56.200]   statistic is the cell phone traffic down 70% in downtown SF,
[00:41:56.200 --> 00:41:58.960]   right. So it's funny, all four of these numbers are pretty much
[00:41:58.960 --> 00:41:59.520]   on track.
[00:42:00.240 --> 00:42:04.440]   It is this chart is crazy. It's literally like, you have some
[00:42:04.440 --> 00:42:08.480]   cities that have more cell phone traffic than they did last year,
[00:42:08.480 --> 00:42:10.200]   or a couple years ago. And
[00:42:10.200 --> 00:42:11.240]   this is downtown, by the way.
[00:42:11.240 --> 00:42:16.240]   And Sam, I mean, the wider Bay Area is, is I don't want to say
[00:42:16.240 --> 00:42:19.480]   booming, but it's vibrant. Yeah, I said on last week's show, I
[00:42:19.480 --> 00:42:23.200]   was looking for a place to host the accelerator in San Mateo
[00:42:23.200 --> 00:42:27.200]   area, I got dozens of people contacting me hundreds of
[00:42:27.200 --> 00:42:33.560]   locations, and offers at 25% of what their carrying cost is, or
[00:42:33.560 --> 00:42:36.800]   like the other carrying costs, the rent was, and people
[00:42:36.800 --> 00:42:40.600]   offering me major companies offering me free space, just
[00:42:40.600 --> 00:42:43.080]   because they would like to have founders hanging around. And
[00:42:43.080 --> 00:42:46.520]   there was one project that I was like, I'll give it to you for
[00:42:46.520 --> 00:42:48.840]   whatever, just because I want to get more people to downtown San
[00:42:48.840 --> 00:42:53.400]   Mateo. So that does sort of prove the point that there is a
[00:42:54.560 --> 00:42:58.200]   what I and I saw this in New York City during the 90s. When
[00:42:58.200 --> 00:43:01.200]   things were so cheap, people just got creative with space. It
[00:43:01.200 --> 00:43:03.880]   inspired people to say, I'm going to create an art gallery,
[00:43:03.880 --> 00:43:06.840]   I'm going to create a performance space. And I don't
[00:43:06.840 --> 00:43:09.400]   know when that happens in San Francisco with these spaces, but
[00:43:09.400 --> 00:43:13.840]   feels like it's going to be a while. I don't know what you
[00:43:13.840 --> 00:43:17.560]   when do you think there would be demand for this space, sex? If
[00:43:17.560 --> 00:43:20.160]   you had to pick a year over and give us an over under?
[00:43:21.000 --> 00:43:25.920]   I mean, five years plus, I mean, just to give you some numbers, I
[00:43:25.920 --> 00:43:30.240]   think a healthy vacancy rate and an office market is five to 10%.
[00:43:30.240 --> 00:43:35.520]   A high vacancy rate in a city was considered like 15%. Like
[00:43:35.520 --> 00:43:38.720]   you wouldn't want to be an office investor in a market that
[00:43:38.720 --> 00:43:42.120]   have 15% vacancy, five to 10% was sort of the normal range. If
[00:43:42.120 --> 00:43:45.520]   you're under 5%, it was a super hot market. And then 10 to 15
[00:43:45.520 --> 00:43:49.880]   was sort of a not great market from an investor standpoint. So
[00:43:49.880 --> 00:43:54.240]   there are 30% plus. And like I said, it could get worse before
[00:43:54.240 --> 00:43:56.560]   it gets better. Because as leases roll, people are going to
[00:43:56.560 --> 00:44:00.880]   shed more space that they might not already be subleasing. So
[00:44:00.880 --> 00:44:06.960]   the real number might be like 40%. So I think it's like, yeah,
[00:44:06.960 --> 00:44:11.280]   it doesn't seem like a decade. It's a decade assuming that San
[00:44:11.280 --> 00:44:14.320]   Francisco gets his house in order. And companies come back
[00:44:14.320 --> 00:44:17.320]   speaking of new companies are created. And they don't
[00:44:17.320 --> 00:44:19.320]   completely wreck it. It's not clear to me that like things
[00:44:19.320 --> 00:44:20.360]   will go in the right direction.
[00:44:20.360 --> 00:44:24.640]   I mean, speaking of that, do we want to bring up this horrific
[00:44:24.640 --> 00:44:28.440]   bear spray attack now? You want to cue that up, Saks? I mean,
[00:44:28.440 --> 00:44:31.760]   we're like in full on Gotham City. Now. Now we have
[00:44:31.760 --> 00:44:32.680]   vigilantes.
[00:44:32.680 --> 00:44:37.280]   There was a story of a fire commissioner named Don
[00:44:37.280 --> 00:44:41.800]   Carminiani, who was beaten with a metal pipe by a gang of
[00:44:41.800 --> 00:44:45.000]   homeless addicts who were encamped in front of his
[00:44:45.000 --> 00:44:48.200]   mother's house. And apparently, they were harassing her and they
[00:44:48.200 --> 00:44:51.920]   were doing drugs, smoking drugs or whatever, right in front of
[00:44:51.920 --> 00:44:53.320]   not not pot. It was like,
[00:44:53.320 --> 00:44:57.360]   fentanyl, whatever, fentanyl, or meth or crack, something like a
[00:44:57.360 --> 00:45:02.640]   hard drug. And so what we know is he went down there, had words
[00:45:02.640 --> 00:45:07.680]   with them, but a beep, but a boop, you know, and they bashed
[00:45:07.680 --> 00:45:13.080]   him upside the head with a pipe. And now it turns out that he was
[00:45:13.080 --> 00:45:16.440]   accused by the defendant's lawyer, the one who assaulted
[00:45:16.440 --> 00:45:19.320]   him, so we don't really know what's true here, of using bear
[00:45:19.320 --> 00:45:23.320]   spray on them first. So the DA dropped charges. The lawyer for
[00:45:23.320 --> 00:45:26.320]   the defendant in that case is saying that he apparently was
[00:45:26.320 --> 00:45:31.440]   the perpetrator of these bear spray attacks on on homeless
[00:45:31.440 --> 00:45:34.520]   people going back a number of years, I guess there's a, like
[00:45:34.520 --> 00:45:36.240]   you said, pretty gnarly video of
[00:45:36.240 --> 00:45:39.240]   we don't know that.
[00:45:39.240 --> 00:45:43.800]   But obviously, the DA thought something was kind of hinky
[00:45:43.800 --> 00:45:46.320]   because they dropped charges against the the guy who
[00:45:46.320 --> 00:45:47.080]   assaulted him.
[00:45:47.080 --> 00:45:50.520]   Shouldn't the person who sprays the bear spray and the person
[00:45:50.520 --> 00:45:53.000]   who beat somebody with a pipe? Shouldn't both people?
[00:45:53.000 --> 00:45:57.720]   Yeah, yeah. Yeah, of course. Listen, there's video of
[00:45:57.720 --> 00:45:59.560]   somebody bear spraying homeless people. And that's clearly
[00:45:59.560 --> 00:46:02.160]   wrong. However, that was from a couple of years ago, the one
[00:46:02.160 --> 00:46:06.640]   that was released from 2021. We have video from the night that
[00:46:06.640 --> 00:46:12.040]   carbon Yanni was assaulted, that they were chasing him down there
[00:46:12.040 --> 00:46:15.960]   chasing him down. Yes, this is a metal pipe. And even if even if
[00:46:15.960 --> 00:46:18.360]   they were acting in self defense, you can't go chasing
[00:46:18.360 --> 00:46:21.520]   the guy to self defense more damage on exactly. That's
[00:46:21.520 --> 00:46:24.160]   vengeance. That's not self defense. Yes. So they took it
[00:46:24.160 --> 00:46:27.160]   out of that zone of self defense. And they were chasing
[00:46:27.160 --> 00:46:30.360]   after him. And if you saw what he looked like, after the
[00:46:30.360 --> 00:46:34.320]   attack, using deadly force, he could have been killed. And you
[00:46:34.320 --> 00:46:38.080]   know, if Donna gotten killed by the metal pipe, I don't think
[00:46:38.080 --> 00:46:41.720]   it'd be a defense that he bear sprayed them first. No, it just
[00:46:41.720 --> 00:46:44.240]   seems to me that would have been an excessive use of force. So
[00:46:44.240 --> 00:46:46.880]   yeah, but in any event, I mean, where the DA ended up on this,
[00:46:46.880 --> 00:46:51.720]   it was just a drop charges from that night. But they're gonna
[00:46:51.720 --> 00:46:54.360]   drop those charges. I think that that's going to be untenable.
[00:46:54.360 --> 00:46:56.760]   No, they're ready. They're ready to drop the charges. They have
[00:46:56.760 --> 00:47:01.320]   to. I mean, justice has to be blind. Correct. I mean, you're
[00:47:01.320 --> 00:47:05.720]   you're a trained lawyer here, we have to apply the law equally to
[00:47:05.720 --> 00:47:09.800]   the sadistic insane person who, wait a second, they arrested the
[00:47:09.800 --> 00:47:12.320]   guy who hosed the person down. Didn't they arrest them as well?
[00:47:12.320 --> 00:47:14.360]   I remember seeing a perp walk. Talked about that on a pre
[00:47:14.360 --> 00:47:17.720]   show. Anyway, it's Gotham City, folks. This has gone to pure
[00:47:17.720 --> 00:47:20.200]   Yeah, I don't think this just proves anything. I mean, again,
[00:47:20.200 --> 00:47:23.440]   what they're trying to say now is that because of the actions
[00:47:23.440 --> 00:47:26.520]   that Don took that San Francisco is safe, there's nothing to
[00:47:26.520 --> 00:47:29.760]   worry about. And these addicts, people who are encamped on the
[00:47:29.760 --> 00:47:32.840]   sidewalks doing drugs doing hard drugs, there's nothing to worry
[00:47:32.840 --> 00:47:36.960]   about, because somehow they were provoked by carbon Yanni. And I
[00:47:36.960 --> 00:47:39.320]   just think I agree with you that this is part of an overall
[00:47:39.320 --> 00:47:43.440]   pattern of chaos and lawlessness in the city. It is like Gotham
[00:47:43.440 --> 00:47:47.560]   City. So you know, it doesn't make me feel a lot better about
[00:47:47.560 --> 00:47:48.640]   what's happening on the streets.
[00:47:48.640 --> 00:47:49.880]   It's nuts.
[00:47:49.880 --> 00:47:51.960]   Timothy, you want to add something?
[00:47:51.960 --> 00:47:55.160]   I want free bird to riff on lab meat.
[00:47:55.160 --> 00:47:59.000]   Yes. Well, there was actually a story about this. I guess
[00:47:59.000 --> 00:48:02.440]   there's two types of lab. There's two types of mock meats.
[00:48:02.440 --> 00:48:06.980]   I've had the impossible burger. I've never craved an impossible
[00:48:06.980 --> 00:48:08.880]   burger. There's so many great burgers, you can get out there
[00:48:08.880 --> 00:48:11.200]   Shake Shack, five guys in and out. Why would I go to get this
[00:48:11.200 --> 00:48:13.680]   impossible burger unless I was doing it like vegan stuff. But
[00:48:13.680 --> 00:48:16.560]   then there was also supposed to be 3d printed meats. And this
[00:48:16.560 --> 00:48:19.200]   stuff seems to be taking forever. Where's this ad because
[00:48:19.200 --> 00:48:21.920]   there was a story in the Wall Street Journal about how poorly
[00:48:21.920 --> 00:48:23.080]   this is apparently going.
[00:48:23.080 --> 00:48:31.000]   So there's three categories of these alternative proteins to
[00:48:31.000 --> 00:48:36.320]   traditional animal protein. The first is these call it
[00:48:36.320 --> 00:48:42.320]   alternative proteins where you use things like soy protein, or
[00:48:42.320 --> 00:48:44.920]   pea protein beyond burger is a good example, they have a pea
[00:48:44.920 --> 00:48:50.600]   protein based burger. And so that category was kind of hot
[00:48:50.600 --> 00:48:52.480]   for a minute where everyone was like, oh, it's a it's an
[00:48:52.480 --> 00:48:56.120]   eco conscious decision, people will make the shift. And you
[00:48:56.120 --> 00:48:58.760]   know, beyond meat had this massive IPO and the stock went
[00:48:58.760 --> 00:49:01.120]   crazy. And someone said it was the biggest return ever for
[00:49:01.120 --> 00:49:04.160]   Kleiner Perkins, but it really was just taking plant protein,
[00:49:04.560 --> 00:49:06.800]   processing it and trying to make it sort of mimic the
[00:49:06.800 --> 00:49:10.640]   texture and flavor and taste of animal protein. And it's more
[00:49:10.640 --> 00:49:14.200]   expensive. So I've generally been fairly negative on whether
[00:49:14.200 --> 00:49:17.080]   that really moves the needle, right? The needle for me is can
[00:49:17.080 --> 00:49:20.480]   you replace animal proteins, traditionally, and stop using
[00:49:20.480 --> 00:49:23.280]   all this land and putting all this carbon into the atmosphere
[00:49:23.280 --> 00:49:25.600]   and all this water and all these resources that we use to make
[00:49:25.600 --> 00:49:28.760]   all these animal proteins, which I think is both kind of ethically
[00:49:28.760 --> 00:49:30.840]   incorrect, but also extraordinarily environmentally
[00:49:30.840 --> 00:49:31.320]   costly.
[00:49:31.320 --> 00:49:33.680]   Sorry, can I ask a question? qualifying? Do you think it's
[00:49:33.680 --> 00:49:38.960]   also important for it to not just replace natural products,
[00:49:38.960 --> 00:49:41.200]   despite all of those externalities you talked about
[00:49:41.200 --> 00:49:43.720]   with artificial products with chemicals and sugar?
[00:49:43.720 --> 00:49:47.640]   So, first of all, everything is a chemical. So that, you know,
[00:49:47.640 --> 00:49:51.320]   that I think the the categorization of, you know, all
[00:49:51.320 --> 00:49:53.640]   chemicals are bad is silly, because everything is made of
[00:49:53.640 --> 00:49:56.760]   chemicals. I think it's a question of are there bad things
[00:49:56.760 --> 00:49:58.680]   that are being put in there that's not good for your health
[00:49:58.680 --> 00:50:01.800]   to make it flavorful or whatever. And that that may or
[00:50:01.800 --> 00:50:03.760]   may not be the case. It's really product dependent. I don't think
[00:50:03.760 --> 00:50:04.920]   it's a good generalization.
[00:50:04.920 --> 00:50:07.800]   But do you so you think when I eat a salad, I'm just eating
[00:50:07.800 --> 00:50:11.920]   chemicals? It is chemicals. Yeah. Got it. But but happy
[00:50:11.920 --> 00:50:13.560]   ones, right? healthy chemicals.
[00:50:13.560 --> 00:50:16.240]   There's good in there are bad. Yeah, for sure.
[00:50:16.240 --> 00:50:18.960]   And then bad chemicals are in like sugary cereal.
[00:50:18.960 --> 00:50:21.400]   Like refined sugar is bad for sure. Right? That's a bad
[00:50:21.400 --> 00:50:22.000]   chemical.
[00:50:22.000 --> 00:50:24.560]   And I know I'm just I just want to understand how you just view
[00:50:24.560 --> 00:50:26.600]   it as a spectrum of chemicals, some good, some bad.
[00:50:26.600 --> 00:50:29.120]   Yeah, there's things that are good for you. There's good fats,
[00:50:29.120 --> 00:50:31.880]   there's bad fats, there's, there's, you know, and even in
[00:50:31.880 --> 00:50:34.040]   the category of sugar, some people say all sugars are bad.
[00:50:34.040 --> 00:50:37.520]   Some people say some sugars are better than others, as measured
[00:50:37.520 --> 00:50:39.800]   by the glycemic index, all you know, there's a lot of ways to
[00:50:39.800 --> 00:50:42.560]   kind of look at this stuff is beyond meat and these P ones.
[00:50:42.560 --> 00:50:46.760]   They're all processed, highly processed. They got a lot of
[00:50:46.760 --> 00:50:49.480]   salt, they got a lot of fat, right? They're not good for you.
[00:50:49.480 --> 00:50:52.800]   So the way that beyond and impossible and others have tried
[00:50:52.800 --> 00:50:55.920]   to make it taste good for people is they've added a lot of, you
[00:50:55.920 --> 00:50:59.240]   know, saturated fats, which is a way to drive the mouthfeel and
[00:50:59.240 --> 00:51:02.960]   make it taste good. But then a lot of doctors, the American
[00:51:02.960 --> 00:51:05.200]   Heart Association came out and said that those fats are really
[00:51:05.200 --> 00:51:08.200]   bad for your heart, and you shouldn't eat them. And also,
[00:51:08.200 --> 00:51:12.200]   there's been a general kind of consumer sentiment shift. So a
[00:51:12.200 --> 00:51:14.640]   couple years ago, these were the hottest products, it was like
[00:51:14.640 --> 00:51:16.920]   all the food ingredient companies were shifting to
[00:51:16.920 --> 00:51:19.360]   plant based proteins, and they were building plant based
[00:51:19.360 --> 00:51:22.600]   protein business categories. And it was this big hot thing. And
[00:51:22.600 --> 00:51:24.760]   then they came out and they're like, wait a second, this isn't
[00:51:24.760 --> 00:51:27.760]   going as we thought. What happens is people try them out.
[00:51:27.760 --> 00:51:29.760]   And they're like, yeah, that's a cool thing. I want to do good
[00:51:29.760 --> 00:51:32.800]   for the planet. But would I rather pay five bucks for a do
[00:51:32.800 --> 00:51:35.720]   good for the planet burger that kind of doesn't taste that good?
[00:51:35.720 --> 00:51:38.240]   Or would I rather pay three bucks for a burger that tastes
[00:51:38.240 --> 00:51:39.920]   really good? And what happens is,
[00:51:39.920 --> 00:51:42.600]   be be I've choose option B.
[00:51:42.600 --> 00:51:45.360]   Yeah. And so to most people, right. And so almost all people
[00:51:45.360 --> 00:51:47.440]   and I've that's the point of view I've always shared, I said,
[00:51:47.440 --> 00:51:49.760]   it's just, it's not going to win the hearts and minds of the
[00:51:49.760 --> 00:51:52.120]   world unless it's cheaper, and it's tastes better and
[00:51:52.120 --> 00:51:54.600]   healthier. And it's identical. Yeah. And doesn't damage your
[00:51:54.600 --> 00:51:58.320]   health doesn't make you worse. Exactly. So the more challenging
[00:51:58.320 --> 00:52:01.120]   technical solution is the other two categories. The second
[00:52:01.120 --> 00:52:05.680]   category is can you synthesize animal proteins using recombinant
[00:52:05.680 --> 00:52:09.300]   DNA. So this is where you take the DNA that codes for the
[00:52:09.300 --> 00:52:12.320]   protein, whether it's the milk protein, or the egg protein, or
[00:52:12.320 --> 00:52:15.840]   the cheese protein, and you put it in a bacterial cell or a yeast
[00:52:15.840 --> 00:52:18.660]   cell that are used to ferment that we used to make wine that
[00:52:18.660 --> 00:52:21.600]   we used to make beer, and they eat sugar, and then they spit
[00:52:21.600 --> 00:52:24.160]   out a product. And in the case of wine and beer, they eat
[00:52:24.160 --> 00:52:27.560]   sugar from grapes or for from malt or whatever, and they spit
[00:52:27.560 --> 00:52:31.920]   out alcohol, ethanol. And genetic was the first company to
[00:52:31.920 --> 00:52:35.400]   really pioneer recombinant DNA at a mass scale, they basically
[00:52:35.400 --> 00:52:38.800]   use recombinant DNA to make insulin. So they took the DNA
[00:52:38.800 --> 00:52:41.680]   from humans that codes for insulin, the gene for insulin,
[00:52:41.680 --> 00:52:44.600]   they put it in E. coli bacteria, and then they put the E. coli
[00:52:44.600 --> 00:52:47.840]   bacteria in a big tank, and the E. coli start to duplicate and
[00:52:47.840 --> 00:52:50.000]   they make all this insulin. And that's how we make all the
[00:52:50.000 --> 00:52:52.920]   world's insulin today is using that bio manufacturing process.
[00:52:53.400 --> 00:52:56.240]   And it's how we make all of biologic drugs, all antibody
[00:52:56.240 --> 00:52:59.520]   drugs are made this way. It's a $300 billion a year market just
[00:52:59.520 --> 00:53:03.360]   in biologic drugs. So when CRISPR kind of came about, in
[00:53:03.360 --> 00:53:07.600]   2012, suddenly, the toolkit to go in and do a much better job
[00:53:07.600 --> 00:53:10.240]   and a much cheaper job of editing the genomes of little
[00:53:10.240 --> 00:53:12.440]   microbes to make a more efficient at making these
[00:53:12.440 --> 00:53:16.440]   proteins became standard. And everyone said, let's go use this
[00:53:16.440 --> 00:53:18.680]   new category of what's being called synthetic biology or
[00:53:18.680 --> 00:53:22.320]   symbiote to make all these animal proteins that we use
[00:53:22.320 --> 00:53:25.200]   animals to get today. So can I just ask the question is the
[00:53:25.200 --> 00:53:28.720]   idea that if you use recombinant DNA in this process, it would
[00:53:28.720 --> 00:53:31.120]   taste better and be healthier and all this chemically
[00:53:31.120 --> 00:53:34.680]   identical. So it's the exact same protein as you get.
[00:53:34.680 --> 00:53:37.840]   And I understand it. Yeah, the exact same under a microscope or
[00:53:37.840 --> 00:53:40.880]   whatever, but would it taste the same taste exact same totally
[00:53:40.880 --> 00:53:43.760]   exact same. So that's the whole doing. Do we know that? Or was
[00:53:43.760 --> 00:53:46.640]   that that was the guess? No, we know that it's the same protein
[00:53:46.640 --> 00:53:48.880]   to mod. So whether you get the protein from the cow, you get
[00:53:48.880 --> 00:53:51.960]   the protein from the yeast. So what's the issue? It's too
[00:53:51.960 --> 00:53:55.680]   expensive to do this process. So the key metric in that second
[00:53:55.680 --> 00:54:01.040]   category is productivity grams per liter per day, how well can
[00:54:01.040 --> 00:54:03.200]   you get that little microorganism to make that
[00:54:03.200 --> 00:54:06.000]   protein, the more protein it makes per day, the cheaper the
[00:54:06.000 --> 00:54:09.080]   price per protein, and we're still a far ways off from
[00:54:09.080 --> 00:54:11.880]   getting this to be price competitive. So that's a
[00:54:11.880 --> 00:54:15.280]   challenge category right now. There's a lot of I'm invested in
[00:54:15.280 --> 00:54:17.600]   a couple of companies in the space where we're trying to make
[00:54:17.600 --> 00:54:20.560]   it faster and cheaper to do that strain engineering to edit the
[00:54:20.560 --> 00:54:23.160]   genome up front and make them make those little cells more
[00:54:23.160 --> 00:54:25.880]   productive to bring the price per gram down and hopefully make
[00:54:25.880 --> 00:54:28.720]   it compete ultimately with the traditional market for eggs,
[00:54:28.720 --> 00:54:29.840]   cheese, milk, etc.
[00:54:29.840 --> 00:54:32.840]   But what is the constraint? Is it an energy constraint? Or is
[00:54:32.840 --> 00:54:34.880]   it natural biological incapability?
[00:54:34.880 --> 00:54:37.920]   No. So the great thing is, on a first principles basis, the
[00:54:37.920 --> 00:54:41.600]   biophysics indicates that this should make proteins cheaper.
[00:54:41.600 --> 00:54:44.120]   And that is good for the planet. It's good for human health. It's
[00:54:44.120 --> 00:54:46.800]   good for everything. We should be able to make eggs, cheese,
[00:54:46.800 --> 00:54:49.280]   milk, all this stuff exactly the same as what you get from an
[00:54:49.280 --> 00:54:52.280]   animal without the animal, because the biophysics of a
[00:54:52.280 --> 00:54:54.880]   single cell making it is better than the biophysics of a whole
[00:54:54.880 --> 00:54:58.200]   animal. Think about a chicken. It grows feathers, it clucks, it
[00:54:58.200 --> 00:55:01.400]   walks around, it has energy, it makes heat. So the chicken as a
[00:55:01.400 --> 00:55:04.480]   system is not that energy efficient. But a little cell
[00:55:04.480 --> 00:55:06.840]   that just eat sugar, and it's programmed to do one thing and
[00:55:06.840 --> 00:55:09.680]   one thing only eat sugar, make protein, eat sugar, make protein
[00:55:09.680 --> 00:55:12.320]   and spit as much of it out. You theoretically can make it way
[00:55:12.320 --> 00:55:15.760]   more efficient. Exactly. Now, we're making great progress, but
[00:55:15.760 --> 00:55:17.840]   we're not there yet. We're not a commodity price point.
[00:55:17.840 --> 00:55:20.520]   Why I'm trying to ask why where's the failure point?
[00:55:20.520 --> 00:55:22.920]   There's two failure points. Sorry, I should I should say
[00:55:22.920 --> 00:55:25.480]   there's three. The first is strain engineering, which is you
[00:55:25.480 --> 00:55:28.480]   want to shuffle all the other genes in the organism to stop
[00:55:28.480 --> 00:55:32.160]   doing things like growing a bigger cell wall. Or you know,
[00:55:32.160 --> 00:55:35.000]   taking your time to duplicate you want to change the genome of
[00:55:35.000 --> 00:55:37.760]   the cell to get it to do stuff faster. The second stage is
[00:55:37.760 --> 00:55:41.160]   process engineering. When you put that cell in a tank, you're
[00:55:41.160 --> 00:55:44.520]   changing the sugar, the methanol, the co2, the oxygen,
[00:55:44.520 --> 00:55:47.120]   the pH, everything about that tank and the condition of the
[00:55:47.120 --> 00:55:50.320]   tank has to be adjusted. So there's about 60 variables. And
[00:55:50.320 --> 00:55:53.200]   those 60 variables all need to be tuned and tweaked before you
[00:55:53.200 --> 00:55:56.560]   optimize the performance of production. The third category
[00:55:56.560 --> 00:56:00.160]   is the hardest which is scale manufacturing. There's about 100
[00:56:00.160 --> 00:56:02.920]   million liters of biomanufacturing capacity on
[00:56:02.920 --> 00:56:06.680]   earth today. 95 million liters is owned and operated by
[00:56:06.680 --> 00:56:09.440]   companies that use and when I say by manufacturing capacity,
[00:56:09.440 --> 00:56:12.720]   I'm talking about big stainless steel tanks, you pour water, you
[00:56:12.720 --> 00:56:15.520]   pour sugar, you pour your cells in, they make copies and they
[00:56:15.520 --> 00:56:19.000]   make your stuff. Of that 100 million liters 95 million is
[00:56:19.000 --> 00:56:22.480]   owned and operated by companies. 5 million is available for rent
[00:56:22.480 --> 00:56:26.160]   of that 5 million liters. 4 million is rented for its entire
[00:56:26.160 --> 00:56:28.800]   lifespan by some company, usually a biologic drug company,
[00:56:28.800 --> 00:56:31.360]   because very little of this is being done in food today. So
[00:56:31.360 --> 00:56:34.160]   there's only a million liters left to rent. And there's 200
[00:56:34.160 --> 00:56:37.000]   sin bio startups trying to make animal proteins. And they've all
[00:56:37.000 --> 00:56:40.280]   competed for this this capacity. So the capacity cost has gone up
[00:56:40.280 --> 00:56:41.280]   by about fourfold.
[00:56:41.360 --> 00:56:45.560]   But it sounds like the latter two, you can overcome with
[00:56:45.560 --> 00:56:48.160]   capital. But the first one is really bounded by science.
[00:56:48.160 --> 00:56:51.240]   It's more engineering, because what you track is kind of what's
[00:56:51.240 --> 00:56:54.000]   called the titer curve, which is grams per liter. And the more
[00:56:54.000 --> 00:56:57.320]   experiments you do, the higher that number goes. And so if you
[00:56:57.320 --> 00:56:58.880]   can increase your experimental rate,
[00:56:58.880 --> 00:57:02.040]   and the few the few grams that it does produce today, when when
[00:57:02.040 --> 00:57:04.480]   a normal person tastes it, they're like, Yep, this tastes
[00:57:04.480 --> 00:57:08.040]   the same as a Wagyu ribeye.
[00:57:08.360 --> 00:57:11.200]   No. So remember, I'm talking about proteins right now. I'm
[00:57:11.200 --> 00:57:13.480]   not talking about cellular meat, I want to talk about cellular
[00:57:13.480 --> 00:57:16.760]   meat last, which is the hardest category, which is what you're
[00:57:16.760 --> 00:57:19.560]   talking about. I'm talking about taking that protein and then
[00:57:19.560 --> 00:57:22.800]   using it to make a product like like a cheese, or, you know,
[00:57:22.800 --> 00:57:24.600]   using it as an egg replacer, that kind of stuff. It's the
[00:57:24.600 --> 00:57:26.680]   same protein as what you would get from eggs or milk or what
[00:57:26.680 --> 00:57:29.720]   have you. This all just sounds so hard. Well, it's a big
[00:57:29.720 --> 00:57:32.800]   problem. And it's a lot of money. So is it a problem? eggs
[00:57:32.800 --> 00:57:36.440]   alone are $200 billion a year. I mean, the methane released from
[00:57:36.440 --> 00:57:38.600]   cows is one of the largest contributors to global warming.
[00:57:38.600 --> 00:57:42.040]   It's a it's a real problem. Also, we're going to need to
[00:57:42.040 --> 00:57:43.920]   solve this jamath. If we're going to a lot of resource
[00:57:43.920 --> 00:57:46.600]   constraints, we're going to colonize Uranus, we're going to
[00:57:46.600 --> 00:57:49.840]   need to get food there. I just asked a question like if you if
[00:57:49.840 --> 00:57:52.680]   you go after the high emission categories first, do you give
[00:57:52.680 --> 00:57:55.640]   yourself room to leave these things because you're doing so
[00:57:55.640 --> 00:58:00.440]   much already? Just a question. Animal agriculture emissions are
[00:58:00.440 --> 00:58:03.600]   one of the largest and unfortunately, one of the
[00:58:03.600 --> 00:58:06.080]   biggest drivers because as people's GDP per capita
[00:58:06.080 --> 00:58:08.320]   increments, the first thing they spend money on is
[00:58:08.320 --> 00:58:10.560]   protein. I get it. I'm saying something different, which is if
[00:58:10.560 --> 00:58:13.200]   we just invented better heat pumps, you'd have industrial
[00:58:13.200 --> 00:58:15.320]   heating and cooling, which represents like almost a third
[00:58:15.320 --> 00:58:19.280]   of all greenhouse gas emissions, you get that off. And you give
[00:58:19.280 --> 00:58:22.520]   yourself a lot of time and space and room. And maybe you'd let
[00:58:22.520 --> 00:58:26.000]   the cows roam and belch and burp. Because the tape the meat
[00:58:26.000 --> 00:58:27.800]   just tastes better. And you don't have to spend a bunch of
[00:58:27.800 --> 00:58:29.480]   time and effort. I don't think it's an or I think it's an
[00:58:29.480 --> 00:58:32.480]   antelope. I think we should be doing all these things. And I
[00:58:32.480 --> 00:58:35.960]   think that I'm a big believer as you guys know, in markets. So
[00:58:35.960 --> 00:58:40.120]   I'm not a believer in transition for the sake of, you know,
[00:58:40.120 --> 00:58:42.600]   carbon saving, because people aren't going to pay a premium as
[00:58:42.600 --> 00:58:46.840]   we've seen with the kind of alternative meat. $15 people
[00:58:46.840 --> 00:58:49.160]   want cheaper hamburgers are not markets want cheaper, cheaper,
[00:58:49.160 --> 00:58:51.920]   cheaper. So if you can make protein cheaper, it's also a
[00:58:51.920 --> 00:58:55.440]   great ROI, you can make money doing this. And the market will
[00:58:55.440 --> 00:58:59.160]   buy it because it's cheaper protein. I just want to I just
[00:58:59.160 --> 00:59:00.920]   want to hit on this because we keep sidetracking a little bit.
[00:59:01.080 --> 00:59:03.240]   The third category is the one that the article was about,
[00:59:03.240 --> 00:59:06.000]   which is cellular meat. So cellular meat is where you're
[00:59:06.000 --> 00:59:08.920]   trying to make your Wagyu or your shrimp or your fish, you're
[00:59:08.920 --> 00:59:12.560]   trying to make cells, not just proteins, but entire cells. And
[00:59:12.560 --> 00:59:15.800]   those cells stick together and they look and cook and feel and
[00:59:15.800 --> 00:59:19.160]   taste like cellular meat, like like muscle, like what you eat
[00:59:19.160 --> 00:59:23.800]   when you eat fish or beef or whatever. And the problem there
[00:59:23.800 --> 00:59:28.000]   is you're trying to take a cell and cells normally grow on, you
[00:59:28.000 --> 00:59:30.960]   know, bones and on tissue. And so there's scaffolding and all
[00:59:30.960 --> 00:59:33.880]   these systems that hold all the cells together. And so to get
[00:59:33.880 --> 00:59:37.760]   cells to grow in a tank, and stick together and replicate
[00:59:37.760 --> 00:59:40.440]   without other cells signaling them turns out to be really
[00:59:40.440 --> 00:59:42.600]   expensive. There was an executive at Merck I spoke to a
[00:59:42.600 --> 00:59:45.640]   few months ago, and he said, we're going to sell fetal bovine
[00:59:45.640 --> 00:59:49.240]   serum, which is basically like this liquid that they get from
[00:59:49.240 --> 00:59:53.040]   the fetuses of cows. And this is how cellular meat started, they
[00:59:53.040 --> 00:59:55.600]   took a cell from a cow and they put it in a tank with fetal
[00:59:55.600 --> 00:59:58.040]   bovine serum, and the cell started to replicate and
[00:59:58.040 --> 01:00:00.320]   duplicate. And then they could take those cells and try and
[01:00:00.320 --> 01:00:03.600]   turn them into a beef into a burger and sell it or try it.
[01:00:03.600 --> 01:00:05.480]   That was the million dollar burger if you remember that a
[01:00:05.480 --> 01:00:08.560]   couple years ago. And fetal bovine serum market has gone
[01:00:08.560 --> 01:00:10.400]   through the roof because so many companies are trying to make
[01:00:10.400 --> 01:00:13.040]   cellular meat. And the Merck exec was like, we're going to
[01:00:13.040 --> 01:00:14.960]   sell a billion dollars of fetal bovine serum, and then we're
[01:00:14.960 --> 01:00:17.280]   going to sell zero, because no one's gonna be able to make
[01:00:17.280 --> 01:00:19.080]   money doing this. It's just impossible. You're not gonna
[01:00:19.080 --> 01:00:22.640]   sell $500 burgers. So the technical challenge there is you
[01:00:22.640 --> 01:00:25.280]   have to edit the cells to get them to duplicate, you have to
[01:00:25.280 --> 01:00:28.400]   get them to grow in suspension, meaning in a tank, instead of
[01:00:28.400 --> 01:00:30.240]   growing on bones and growing next to each other and
[01:00:30.240 --> 01:00:33.520]   scaffolding. And then you have to change the feedstock so that
[01:00:33.520 --> 01:00:36.120]   you're creating all these other proteins and signaling factors
[01:00:36.120 --> 01:00:39.160]   and hormones that you pour into the tank that trigger those
[01:00:39.160 --> 01:00:41.600]   cells to grow. Is there any chance that after all this, it's
[01:00:41.600 --> 01:00:45.440]   it actually just tastes slightly different, or better? It may?
[01:00:45.440 --> 01:00:48.360]   Yeah, it may, but likely not. I mean, let's be honest, these
[01:00:48.360 --> 01:00:52.360]   are you're taking a cow cell or a salmon cell. Now, the reason I
[01:00:52.360 --> 01:00:54.880]   say this is that I don't know if you I mean, you don't eat meat.
[01:00:54.880 --> 01:00:59.440]   So maybe you don't know this. But depending on where the water
[01:00:59.440 --> 01:01:02.840]   that they drink, the actual grass that they eat, the meat
[01:01:02.840 --> 01:01:05.920]   does taste different. And that's part of the
[01:01:05.920 --> 01:01:07.800]   whether the cow is massage.
[01:01:07.800 --> 01:01:12.840]   I mean, look at the acorn fed cows that we used to have at
[01:01:12.840 --> 01:01:17.160]   poker before austerity measures life was so good in 2021. Well,
[01:01:17.160 --> 01:01:20.800]   that's that. No, we can't get it anymore. Yeah, I know. We're on
[01:01:20.800 --> 01:01:23.600]   a budget. I get it. No, not us. We can't get it anymore. Because
[01:01:23.600 --> 01:01:28.120]   they sell it through one channel. But yeah, like, so good.
[01:01:28.120 --> 01:01:31.200]   Yeah, the variation you're talking about is obviously at an
[01:01:31.200 --> 01:01:33.640]   echelon and a class of eating Chamath. It's probably not
[01:01:33.640 --> 01:01:37.920]   mainstay, like, you know, the $30,000 a year McDonald's burger
[01:01:37.920 --> 01:01:42.000]   and nugget eater is probably happy to take a chicken nugget
[01:01:42.000 --> 01:01:43.840]   that tastes I disagree with you. Because if you go into Whole
[01:01:43.840 --> 01:01:47.640]   Foods, and you actually buy like a USDA top sirloin, there's a
[01:01:47.640 --> 01:01:51.800]   certain taste that it has that things that are not USDA don't
[01:01:51.800 --> 01:01:55.640]   have. So even even at like the most basic layer of the food
[01:01:55.640 --> 01:01:59.720]   pyramid, you can differentiate on taste based on the same. This
[01:01:59.720 --> 01:02:03.280]   is why I'm saying I think it's just a very complicated, long
[01:02:03.280 --> 01:02:05.840]   drawn out process. And I just wonder if the people that are in
[01:02:05.840 --> 01:02:09.080]   these businesses, if they actually love food or not, I
[01:02:09.080 --> 01:02:11.880]   understand why they love the science, I get it. And why they
[01:02:11.880 --> 01:02:15.200]   would love to save the planet. I get that too. But unless some of
[01:02:15.200 --> 01:02:18.800]   these people are, are also food lovers, they're going to miss I
[01:02:18.800 --> 01:02:20.520]   think the thing where it all dies. Anyways,
[01:02:20.520 --> 01:02:24.640]   I just want to restate again, for the final time, these are
[01:02:24.640 --> 01:02:28.600]   these are identical cells and identical proteins to what you're
[01:02:28.600 --> 01:02:31.120]   getting from the animal. So they are not like what we talked
[01:02:31.120 --> 01:02:33.480]   about in that first category, where you're trying to get other
[01:02:33.480 --> 01:02:36.520]   stuff to sort of taste like meat, you're literally trying to
[01:02:36.520 --> 01:02:41.240]   create the meat and create the protein using these systems. And
[01:02:41.240 --> 01:02:44.080]   we're just trying to tell you that salmon, two pieces of
[01:02:44.080 --> 01:02:46.320]   salmon can taste totally different depending on where it
[01:02:46.320 --> 01:02:50.000]   swam. Right. And I guess what I could say is, yeah, you could
[01:02:50.000 --> 01:02:52.560]   probably adjust the conditions in the tank if needed to change
[01:02:52.560 --> 01:02:54.160]   the characteristic.
[01:02:54.160 --> 01:02:56.920]   This is my point, like you don't even know where to start. How is
[01:02:56.920 --> 01:03:00.200]   it the fucking kelp in the Atlantic Ocean? Like, what are
[01:03:00.200 --> 01:03:00.800]   you changing?
[01:03:00.800 --> 01:03:03.600]   Look, I don't know what kelp effects on the salmon. I don't
[01:03:03.600 --> 01:03:04.440]   know if salmon,
[01:03:04.440 --> 01:03:07.600]   but this is my point. Nobody does Atlantic Ocean. This is why
[01:03:07.600 --> 01:03:09.720]   we pay so much for the acorn fed beef.
[01:03:09.720 --> 01:03:13.800]   I get it. But most beef is not kelp from the Atlantic Ocean fed
[01:03:13.800 --> 01:03:17.480]   salmon. It's animals grown in very large feedlots fed corn and
[01:03:17.480 --> 01:03:21.560]   water. That's it. Let me just say that again. 90% 95% of
[01:03:21.560 --> 01:03:25.920]   animal protein consumed is cows, pigs and chickens grown in
[01:03:25.920 --> 01:03:28.960]   feedlots fed corn and soybeans and water. And that's it.
[01:03:28.960 --> 01:03:31.880]   Right. But if you if you go to different countries and taste
[01:03:31.880 --> 01:03:34.400]   the meat that's fed in that exact same way, it tastes
[01:03:34.400 --> 01:03:36.680]   different. So for example, if you go to Argentina,
[01:03:36.680 --> 01:03:41.440]   I appreciate what you're saying. But the point you can recreate
[01:03:41.440 --> 01:03:43.600]   whatever the system is that you're talking about. So I want
[01:03:43.600 --> 01:03:46.040]   to just get back to the unit economics, the cost per
[01:03:46.040 --> 01:03:49.680]   kilogram or the cost per gram of the protein, we are still many
[01:03:49.680 --> 01:03:52.440]   orders of magnitude away on cellular meat. So the problems
[01:03:52.440 --> 01:03:55.040]   you're laying out are really down the road problems of
[01:03:55.040 --> 01:03:57.960]   optimization. Right now we've got more fundamental problems on
[01:03:57.960 --> 01:04:00.040]   how do you actually get this stuff to be cost competitive.
[01:04:00.040 --> 01:04:03.160]   Now fortunately, the tools of CRISPR and since CRISPR Cas9
[01:04:03.160 --> 01:04:06.120]   came out 10 years ago, there are now hundreds of variants that
[01:04:06.120 --> 01:04:10.040]   are open source, IP free royalty free and use very broadly and
[01:04:10.040 --> 01:04:12.960]   generally and DNA sequencing costs continue to decline. Those
[01:04:12.960 --> 01:04:16.720]   are the two basic tools that are being used by biochemists and
[01:04:16.720 --> 01:04:20.840]   engineers to do rapid evolutionary iteration needed to
[01:04:20.840 --> 01:04:23.280]   produce the recombinant production of proteins to
[01:04:23.280 --> 01:04:25.920]   produce the new cells to produce the feedstock for those tanks.
[01:04:25.920 --> 01:04:29.040]   And there's a cost curve that we're trying to get over. It's
[01:04:29.040 --> 01:04:31.960]   not happening overnight, hundreds of millions of dollars
[01:04:31.960 --> 01:04:34.160]   and in several cases, billions of dollars have gone into these
[01:04:34.160 --> 01:04:37.160]   systems. And it's very likely that these companies may need
[01:04:37.160 --> 01:04:40.200]   several more years and several billion dollars, we are going to
[01:04:40.200 --> 01:04:43.080]   get there, the technology is progressing, the rate of
[01:04:43.080 --> 01:04:46.040]   progress is a little slower. And it's a little more challenged,
[01:04:46.040 --> 01:04:49.840]   I think, then the first round of investors had hoped. But I do
[01:04:49.840 --> 01:04:52.440]   think that scientifically and first principles, it is
[01:04:52.440 --> 01:04:55.080]   absolutely feasible. It's a function of engineering our way
[01:04:55.080 --> 01:04:58.320]   there to giving Chamath and everyone else that eats burgers
[01:04:58.320 --> 01:05:00.720]   and chicken nuggets, everything that they want, hopefully at a
[01:05:00.720 --> 01:05:01.320]   lower price.
[01:05:01.320 --> 01:05:04.440]   If you put it on the curve of self driving cars, you know, we
[01:05:04.440 --> 01:05:08.200]   have crews doing some automated taxis in like a very constrained
[01:05:08.200 --> 01:05:11.640]   area in San Francisco, but we don't have it everywhere. Where
[01:05:11.640 --> 01:05:12.680]   do you put this on the curve?
[01:05:12.680 --> 01:05:15.440]   It's a great question. So so what's happened, by the way, as
[01:05:15.440 --> 01:05:18.800]   we've gotten down the cost curve, we are unlocking new
[01:05:18.800 --> 01:05:22.920]   markets. So new products are being produced existing proteins
[01:05:22.920 --> 01:05:24.880]   that are come from animals. There's a good example of a
[01:05:24.880 --> 01:05:29.040]   product called pepsin. It's, it's extracted from pigs today,
[01:05:29.040 --> 01:05:31.840]   it's very expensive, similar to how we used to make insulin. And
[01:05:31.840 --> 01:05:34.600]   we're replacing the sourcing of that product, we replaced
[01:05:34.600 --> 01:05:38.040]   insulin, which we used to get from pig spleens, we now make it
[01:05:38.040 --> 01:05:40.640]   recombinantly, we're now replacing pepsin, we're
[01:05:40.640 --> 01:05:44.480]   replacing the the rennet that's used to make cheese. So as we
[01:05:44.480 --> 01:05:47.040]   move down the cost curve, these high were called high value
[01:05:47.040 --> 01:05:50.280]   proteins are the first to fall those markets collapse because
[01:05:50.280 --> 01:05:52.680]   we now make them recombinantly, they were sorry, they collapse
[01:05:52.680 --> 01:05:55.520]   in price, because they're now cheaper using recombinant
[01:05:55.520 --> 01:05:58.040]   systems instead of taking them from animals. And eventually
[01:05:58.040 --> 01:06:00.200]   we'll get to that cost curve where they're ubiquitous for all
[01:06:00.200 --> 01:06:03.120]   proteins, or for all types of cells. In the meantime, they're
[01:06:03.120 --> 01:06:06.120]   pretty sizable markets to go after these are multi billion
[01:06:06.120 --> 01:06:08.400]   dollar markets that are getting knocked down. We don't talk
[01:06:08.400 --> 01:06:10.880]   about it every day, it doesn't show up in the news. But it's
[01:06:10.880 --> 01:06:14.160]   really profound and interesting to see that this technology is
[01:06:14.160 --> 01:06:17.600]   working. It's overturning multi billion dollar markets. It's
[01:06:17.600 --> 01:06:20.440]   making progress. And you know, hopefully it'll, it'll get to
[01:06:20.440 --> 01:06:22.480]   the point that you know, everything from the chicken
[01:06:22.480 --> 01:06:27.280]   nugget to the kelp fed salmon can have you guys tried to be on
[01:06:27.280 --> 01:06:30.520]   burger or an impossible burger? I've had it. I've tried them a
[01:06:30.520 --> 01:06:33.320]   long time ago, but I've not tried them recently. They're
[01:06:33.320 --> 01:06:38.920]   like, it's like eating something mushy. That's 60% of average
[01:06:38.920 --> 01:06:43.160]   hamburger, it's not worth paying double for certainly, for
[01:06:43.160 --> 01:06:47.960]   somebody who's a hamburger eater. So while we were talking,
[01:06:47.960 --> 01:06:51.040]   by the way, Amazon's results came out, they crushed it.
[01:06:51.040 --> 01:06:59.280]   earnings per share of 31 cents, versus 11. And stocks, 10%, 10%
[01:06:59.320 --> 01:07:03.000]   off hours, and it was up 4% today, where the insider
[01:07:03.000 --> 01:07:05.280]   traders make sense. How do you feel about your recession
[01:07:05.280 --> 01:07:05.800]   prediction?
[01:07:05.800 --> 01:07:10.080]   I'm sticking by it. I think we're still going to recession,
[01:07:10.080 --> 01:07:13.400]   but it is an interesting paradox here. So I think there's only a
[01:07:13.400 --> 01:07:19.120]   couple of possibilities either. Tech is sort of immune, or they
[01:07:19.120 --> 01:07:22.040]   forecast down so much they were so conservative in their
[01:07:22.040 --> 01:07:25.000]   forecast thinking we're going to be in a recession that it was
[01:07:25.000 --> 01:07:28.760]   easy to beat. Or look, I could be wrong about the recession,
[01:07:28.760 --> 01:07:32.840]   but palacing it. And palacing if it's not a recession, it's gonna
[01:07:32.840 --> 01:07:35.240]   be less than 1% growth. It's gonna be around a year to
[01:07:35.240 --> 01:07:38.720]   recession. So it's not credible. So I'm not revising my
[01:07:38.720 --> 01:07:42.920]   forecast. Well, I think pal is credible when he's giving us bad
[01:07:42.920 --> 01:07:46.000]   news, because their incentive is always to fluff it up and make
[01:07:46.000 --> 01:07:48.680]   it sound better than it is. So when he's telling you, things
[01:07:48.680 --> 01:07:50.560]   look bad, maybe they're looking really bad.
[01:07:50.560 --> 01:07:53.160]   I don't know, man.
[01:07:53.160 --> 01:07:55.840]   But look, it's a tale of two cities right now. I mean, the
[01:07:55.840 --> 01:07:58.800]   big tech companies seem to be doing really well. So it's
[01:07:58.800 --> 01:07:59.880]   definitely a paradox.
[01:07:59.880 --> 01:08:02.640]   Yeah. All right, everybody.
[01:08:02.640 --> 01:08:03.880]   Well, the whole RFK thing.
[01:08:03.880 --> 01:08:07.120]   Okay, that's a good topic. Yeah, great topic. Go ahead. I
[01:08:07.120 --> 01:08:10.400]   think we should tell people like what he's about. Where are all
[01:08:10.400 --> 01:08:13.440]   yours? I think he gave a terrific announcement speech.
[01:08:13.440 --> 01:08:15.600]   Okay. And just to give you some background for the younger
[01:08:15.600 --> 01:08:20.520]   viewers who may not know. So Robert F. Kennedy, his father
[01:08:20.520 --> 01:08:23.200]   ran for the Democratic nomination in 1968. After his
[01:08:23.200 --> 01:08:26.120]   brother, john F. Kennedy had been president was assassinated
[01:08:26.120 --> 01:08:31.200]   as we know, in the early 1960s. What happened is at this time,
[01:08:31.200 --> 01:08:34.440]   before the 1968 election, Linda B. Johnson was the incumbent
[01:08:34.440 --> 01:08:37.840]   Democratic president. And everyone thought that he'd be
[01:08:37.840 --> 01:08:40.320]   the party's nominee, and he was going to get reelected. And he
[01:08:40.320 --> 01:08:43.600]   was brought down by an extremely unpopular war, the Vietnam War.
[01:08:43.600 --> 01:08:47.760]   And it was RFK Jr. His father, who was a great critic of the
[01:08:47.760 --> 01:08:51.200]   Vietnam War, and he ran for the Democratic nomination. And I
[01:08:51.200 --> 01:08:54.120]   think that he very likely would have gotten it on the night that
[01:08:54.120 --> 01:08:57.960]   he won the California primary, he was assassinated by Sir Hans
[01:08:57.960 --> 01:09:00.600]   Sirhan. Right? Yeah, if you go back and look at the things that
[01:09:00.600 --> 01:09:04.520]   he was saying in that campaign, he really was saying a lot of
[01:09:04.520 --> 01:09:08.920]   beautiful things that are in his son's ad that I think would be
[01:09:08.920 --> 01:09:12.400]   worth playing here. But I think you have maybe the setup for a
[01:09:12.400 --> 01:09:14.920]   similar situation here. You've got an incumbent Democratic
[01:09:14.920 --> 01:09:19.440]   president, who is sort of not that popular. He's sort of old
[01:09:19.440 --> 01:09:22.600]   and out of it and incoherent. He's presiding over a war that
[01:09:22.600 --> 01:09:26.120]   is rapidly becoming a debacle. You don't hear so much about the
[01:09:26.120 --> 01:09:29.160]   spring counter offensive anymore. These new Pentagon
[01:09:29.160 --> 01:09:32.240]   papers that were leaked show that the Ukrainian casualties
[01:09:32.240 --> 01:09:34.840]   are at least five times greater than they've been publicly
[01:09:34.840 --> 01:09:38.560]   admitting. It looks like Russia certainly not losing the war the
[01:09:38.560 --> 01:09:41.280]   way they used to be they've captured 90% of Bokmah, which
[01:09:41.280 --> 01:09:44.360]   has been the most violent, bloody battle of the war. And
[01:09:44.360 --> 01:09:46.480]   Biden at this point has no strategy to bring that to an
[01:09:46.480 --> 01:09:51.560]   end. In fact, he's rejected multiple attempts at a peace
[01:09:51.560 --> 01:09:54.720]   deal. And so now it looks like it's the Chinese who are in the
[01:09:54.720 --> 01:09:57.200]   driver's seat, potentially putting together some sort of
[01:09:57.200 --> 01:10:00.800]   dramatic settlement. So I think listen, if the economy ends up
[01:10:00.800 --> 01:10:03.800]   going into recession, and this war ends up becoming the fiasco
[01:10:03.800 --> 01:10:06.280]   that it's increasingly looking like you could have a setup
[01:10:06.280 --> 01:10:09.720]   like 1968, where people are wondering why the hell is this
[01:10:09.720 --> 01:10:13.360]   guy or nominee. And let me tell you, RFK juniors already pulling
[01:10:13.360 --> 01:10:16.920]   at 19%, which I think is pretty good, considering he just came
[01:10:16.920 --> 01:10:19.560]   out of the gate, and people don't even know the substance of
[01:10:19.560 --> 01:10:22.960]   his campaign yet. Marianne Williamson's at 9%. So if she
[01:10:22.960 --> 01:10:27.360]   dropped out, you'd be at 28% for the alternative. And I think he
[01:10:27.360 --> 01:10:30.160]   could go up from here. And I think if you if you watch the
[01:10:30.160 --> 01:10:34.200]   speech he gave, I thought there was a lot of really beautiful
[01:10:34.200 --> 01:10:37.080]   sentiments in there. He's very good. He said that Biden has
[01:10:37.080 --> 01:10:40.600]   made Ukraine a pawn in a geopolitical battle that has
[01:10:40.600 --> 01:10:43.640]   put the flower of Ukraine's youth into an abattoir of death
[01:10:43.640 --> 01:10:47.920]   in order to exhaust Russia. He channeled America's anti war
[01:10:47.920 --> 01:10:50.960]   traditions, he quoted john Quincy Adams, that America should
[01:10:50.960 --> 01:10:53.960]   not go abroad in search of monsters to destroy. He quoted
[01:10:53.960 --> 01:10:57.280]   Martin Luther King Jr. There is a direct link between poverty
[01:10:57.280 --> 01:11:02.000]   and violence and oppression at home and war abroad. He talked
[01:11:02.000 --> 01:11:06.600]   about the role of the CIA during his uncle's administration
[01:11:06.600 --> 01:11:11.280]   where he said that john F. Kennedy eventually realized that
[01:11:11.280 --> 01:11:15.000]   the purpose the CIA had become to create a steady pipeline of
[01:11:15.000 --> 01:11:18.320]   wars to feed the military industrial complex. And he talks
[01:11:18.320 --> 01:11:23.040]   about how JFK came to distrust the CIA and realize that it was
[01:11:23.040 --> 01:11:26.360]   lying to him. And the biggest applause line of his speech was
[01:11:26.360 --> 01:11:29.960]   when he quoted JFK approvingly saying that he wanted to take
[01:11:29.960 --> 01:11:33.520]   the CIA and shatter it into 1000 pieces and scatter it to the
[01:11:33.520 --> 01:11:36.360]   winds. And this very same week that he gave the speech, we
[01:11:36.360 --> 01:11:41.400]   found out that five former CIA directors had participated in a
[01:11:41.400 --> 01:11:44.160]   giant hoax on the American people by claiming that this
[01:11:44.160 --> 01:11:47.560]   Hunter Biden story was Russian disinformation. They knew it was
[01:11:47.560 --> 01:11:51.200]   not. They knew it was not. The information on the hard drive
[01:11:51.200 --> 01:11:54.400]   was real. It showed that Hunter Biden received multimillion
[01:11:54.400 --> 01:11:58.720]   dollar payments from foreign governments, including China and
[01:11:58.720 --> 01:12:02.960]   Ukraine. Okay. And regardless of what you think of that story,
[01:12:03.120 --> 01:12:05.400]   it should not have been suppressed by social media. And
[01:12:05.400 --> 01:12:09.680]   it certainly should not have been suppressed in a psyop by 51
[01:12:09.680 --> 01:12:14.120]   former intelligence officials, including five former directors
[01:12:14.120 --> 01:12:16.400]   of the CIA. And if that's the way they're going to behave,
[01:12:16.400 --> 01:12:18.880]   they're going to meddle in American politics that way. I
[01:12:18.880 --> 01:12:21.080]   think we do need to start over we do need to ask what's going
[01:12:21.080 --> 01:12:22.480]   on with the security state. They're not supposed to be
[01:12:22.480 --> 01:12:26.840]   meddling in American politics that way. So I think if this is
[01:12:26.840 --> 01:12:29.960]   the way they're going to act, I say shatter away, scatter that
[01:12:29.960 --> 01:12:31.440]   thing into 1000 pieces.
[01:12:31.600 --> 01:12:33.280]   Hey, it's Catholic. I'll vote for him.
[01:12:33.280 --> 01:12:39.440]   And he's called out the insanity of COVID lockdowns and man.
[01:12:39.440 --> 01:12:42.000]   And that's the thing that he's I guess that's the big
[01:12:42.000 --> 01:12:45.360]   controversy is he's anti he's an anti vaxxer. I guess that's
[01:12:45.360 --> 01:12:48.160]   the one thing they're trying to position him as and he does a
[01:12:48.160 --> 01:12:52.360]   little bit of conspiracy theories. So listen, if you go
[01:12:52.360 --> 01:12:54.840]   back and look at his record, he was an environmental activist
[01:12:54.840 --> 01:12:57.240]   for most of his career. He did the worst project in New York
[01:12:57.240 --> 01:12:59.920]   where they basically bought the land along the Hudson. I
[01:12:59.920 --> 01:13:02.600]   remember I've had some events for it. They wanted to clean the
[01:13:02.600 --> 01:13:06.120]   Hudson up. And they just bought the land and didn't people
[01:13:06.120 --> 01:13:08.960]   donated the land and they bought it, they raised money. And the
[01:13:08.960 --> 01:13:14.920]   Hudson today has like, you know, it's it's flourishing. And he's
[01:13:14.920 --> 01:13:16.080]   directly responsible for that.
[01:13:16.080 --> 01:13:18.560]   He was a big critic of the way that corporate greed could lead
[01:13:18.560 --> 01:13:21.680]   certain big companies to engage in environmental pollution. And
[01:13:21.680 --> 01:13:24.800]   at a certain point, he realized that big pharma had a similar
[01:13:24.800 --> 01:13:27.240]   incentive. Now, I don't know if he is right about those
[01:13:27.240 --> 01:13:29.840]   vaccines. But I do know that he's right in the case of COVID.
[01:13:29.840 --> 01:13:35.480]   They had incentive to push this dubious RNA shot on us so that
[01:13:35.480 --> 01:13:38.720]   we get boosted a zillion times. And he's right about that. He
[01:13:38.720 --> 01:13:40.440]   was right about the fact that this should never have been
[01:13:40.440 --> 01:13:43.080]   mandated. We shouldn't have the lockdowns. And you know what in
[01:13:43.080 --> 01:13:47.200]   his nomination speech, or his declaration, the word vaccine
[01:13:47.200 --> 01:13:49.680]   was only mentioned once. So this is not what his campaign is
[01:13:49.680 --> 01:13:51.320]   about. I look forward to having a month.
[01:13:51.320 --> 01:13:55.640]   Yeah. And to be honest, I mean, look at all the other things
[01:13:55.640 --> 01:13:57.760]   that were deemed to be conspiracy theories that ended
[01:13:57.760 --> 01:13:59.040]   up being true. Oh, yeah.
[01:13:59.040 --> 01:14:00.600]   Last monster.
[01:14:00.600 --> 01:14:02.400]   Not that
[01:14:02.400 --> 01:14:06.360]   he go either way. It could be embarrassing. Let me ask you
[01:14:06.360 --> 01:14:08.720]   this, axe. If it was him versus Trump, who do you vote for?
[01:14:08.720 --> 01:14:13.920]   Well, I'm gonna I'm gonna reserve versus Trump sex. I'm
[01:14:13.920 --> 01:14:16.360]   not gonna take position on the general yet. But but in in the
[01:14:16.360 --> 01:14:19.760]   democratic primary, I'm definitely endorsing RFK Jr. in
[01:14:19.760 --> 01:14:20.600]   the democratic primary.
[01:14:20.600 --> 01:14:22.760]   Okay, I'm
[01:14:22.760 --> 01:14:24.480]   voting the democratic primary.
[01:14:24.480 --> 01:14:26.960]   I would if I could, I'll do an event.
[01:14:26.960 --> 01:14:30.600]   All right, everybody. All the amazing people who got together
[01:14:30.600 --> 01:14:34.760]   for the episode 125. Unbelievable over 40 or 50 of
[01:14:34.760 --> 01:14:35.880]   them. Ray, great job.
[01:14:35.880 --> 01:14:37.320]   Shout out to Ray.
[01:14:37.320 --> 01:14:38.160]   Shout out to Ray.
[01:14:38.160 --> 01:14:40.760]   I dialed into a bunch of them. And I think
[01:14:40.760 --> 01:14:44.920]   it's great Europe and I don't know all over the place.
[01:14:44.920 --> 01:14:46.680]   No one got robbed or mugged or bear.
[01:14:46.680 --> 01:14:49.000]   Hopefully I don't know if they did any in San Francisco. There's
[01:14:49.000 --> 01:14:50.560]   no bear spray in since that's good.
[01:14:51.400 --> 01:14:53.320]   They turn up to Marina.
[01:14:53.320 --> 01:15:00.400]   For the Sultan of science, the dictator himself and the
[01:15:00.400 --> 01:15:01.840]   mouthfeel.
[01:15:01.840 --> 01:15:08.000]   And rain man, I am the world's greatest moderator. We'll see
[01:15:08.000 --> 01:15:09.160]   you next time. Everybody.
[01:15:09.160 --> 01:15:10.520]   Love you boys. Bye bye.
[01:15:10.520 --> 01:15:16.560]   Rain Man David
[01:15:20.480 --> 01:15:23.320]   We open sources to the fans and they've just gone crazy.
[01:15:23.320 --> 01:15:24.320]   Love you.
[01:15:24.320 --> 01:15:25.320]   Westside queen of
[01:15:25.320 --> 01:15:33.560]   besties are
[01:15:33.560 --> 01:15:37.520]   dog taking a notice your driveway.
[01:15:37.520 --> 01:15:45.920]   We should all just get a room and just have one big huge orgy
[01:15:45.920 --> 01:15:48.800]   because they're all just like this like sexual tension that
[01:15:48.800 --> 01:15:49.680]   they just need to release.
[01:15:50.680 --> 01:15:54.000]   What you're about to be your feet.
[01:15:54.480 --> 01:15:54.840]   Feet.
[01:15:55.760 --> 01:15:57.600]   We need to get merch.
[01:15:57.960 --> 01:16:07.760]   I'm going all in. I'm going all in.
[01:16:07.760 --> 01:16:09.500]   [THEME MUSIC]
[01:16:09.500 --> 01:16:19.020]   [BLANK_AUDIO]

