
[00:00:00.000 --> 00:00:04.040]   - What do you think it takes to build a system
[00:00:04.040 --> 00:00:05.380]   with human level intelligence?
[00:00:05.380 --> 00:00:08.900]   You talked about the AI system in the movie "Her"
[00:00:08.900 --> 00:00:11.360]   being way out of reach, our current reach.
[00:00:11.360 --> 00:00:13.680]   This might be outdated as well, but--
[00:00:13.680 --> 00:00:14.520]   - It's still way out of reach.
[00:00:14.520 --> 00:00:16.000]   - It's still way out of reach.
[00:00:16.000 --> 00:00:19.660]   What would it take to build "Her"?
[00:00:19.660 --> 00:00:21.040]   Do you think?
[00:00:21.040 --> 00:00:23.040]   - So I can tell you the first two obstacles
[00:00:23.040 --> 00:00:24.160]   that we have to clear,
[00:00:24.160 --> 00:00:26.120]   but I don't know how many obstacles there are after this.
[00:00:26.120 --> 00:00:27.920]   So the image I usually use is that
[00:00:27.920 --> 00:00:29.920]   there is a bunch of mountains that we have to climb
[00:00:29.920 --> 00:00:31.000]   and we can see the first one,
[00:00:31.000 --> 00:00:32.600]   but we don't know if there are 50 mountains
[00:00:32.600 --> 00:00:33.440]   behind it or not.
[00:00:33.440 --> 00:00:36.200]   And this might be a good sort of metaphor
[00:00:36.200 --> 00:00:39.640]   for why AI researchers in the past
[00:00:39.640 --> 00:00:43.280]   have been overly optimistic about the result of AI.
[00:00:43.280 --> 00:00:48.160]   You know, for example, Newell and Simon, right,
[00:00:48.160 --> 00:00:50.680]   wrote the general problem solver
[00:00:50.680 --> 00:00:52.720]   and they called it a general problem solver.
[00:00:52.720 --> 00:00:53.560]   - General problem solver.
[00:00:53.560 --> 00:00:55.840]   - Okay, and of course, the first thing you realize
[00:00:55.840 --> 00:00:57.640]   is that all the problems you want to solve are exponential
[00:00:57.640 --> 00:01:00.440]   and so you can't actually use it for anything useful.
[00:01:00.440 --> 00:01:01.360]   But, you know.
[00:01:01.360 --> 00:01:03.560]   - Yeah, so yeah, all you see is the first peak.
[00:01:03.560 --> 00:01:06.560]   So what are the first couple of peaks for "Her"?
[00:01:06.560 --> 00:01:07.700]   - So the first peak,
[00:01:07.700 --> 00:01:09.280]   which is precisely what I'm working on,
[00:01:09.280 --> 00:01:11.080]   is self-supervised learning.
[00:01:11.080 --> 00:01:13.560]   How do we get machines to learn models of the world
[00:01:13.560 --> 00:01:17.120]   by observation, kind of like babies and like young animals?
[00:01:17.120 --> 00:01:23.580]   So we've been working with cognitive scientists.
[00:01:25.080 --> 00:01:28.080]   So this Emmanuelle Dupou, who is at FAIR in Paris,
[00:01:28.080 --> 00:01:33.920]   half-time, is also a researcher in French University.
[00:01:33.920 --> 00:01:38.200]   And he has this chart that shows
[00:01:38.200 --> 00:01:41.880]   which, how many months of life baby humans
[00:01:41.880 --> 00:01:44.040]   can learn different concepts.
[00:01:44.040 --> 00:01:46.960]   And you can measure this in various ways.
[00:01:46.960 --> 00:01:52.680]   So things like distinguishing animate objects
[00:01:52.680 --> 00:01:54.240]   from inanimate objects.
[00:01:54.240 --> 00:01:57.040]   You can tell the difference at age two, three months.
[00:01:57.040 --> 00:02:00.320]   Whether an object is going to stay stable,
[00:02:00.320 --> 00:02:01.640]   is going to fall, you know,
[00:02:01.640 --> 00:02:04.760]   about four months, you can tell.
[00:02:04.760 --> 00:02:06.460]   You know, there are various things like this.
[00:02:06.460 --> 00:02:08.280]   And then things like gravity,
[00:02:08.280 --> 00:02:10.440]   the fact that objects are not supposed to float in the air,
[00:02:10.440 --> 00:02:11.880]   but are supposed to fall,
[00:02:11.880 --> 00:02:14.420]   you learn this around the age of eight or nine months.
[00:02:14.420 --> 00:02:17.160]   If you look at a lot of, you know, eight-month-old babies,
[00:02:17.160 --> 00:02:20.360]   you give them a bunch of toys on their high chair.
[00:02:20.360 --> 00:02:21.840]   First thing they do is they throw them on the ground
[00:02:21.840 --> 00:02:23.040]   and they look at them.
[00:02:23.040 --> 00:02:25.240]   It's because, you know, they're learning about,
[00:02:25.240 --> 00:02:27.400]   actively learning about gravity.
[00:02:27.400 --> 00:02:28.240]   - Gravity, yeah.
[00:02:28.240 --> 00:02:31.000]   - Okay, so they're not trying to annoy you,
[00:02:31.000 --> 00:02:33.960]   but they, you know, they need to do the experiment, right?
[00:02:33.960 --> 00:02:37.880]   So, you know, how do we get machines to learn like babies?
[00:02:37.880 --> 00:02:40.520]   Mostly by observation with a little bit of interaction
[00:02:40.520 --> 00:02:42.520]   and learning those models of the world,
[00:02:42.520 --> 00:02:45.040]   because I think that's really a crucial piece
[00:02:45.040 --> 00:02:47.640]   of an intelligent autonomous system.
[00:02:47.640 --> 00:02:48.840]   So if you think about the architecture
[00:02:48.840 --> 00:02:50.800]   of an intelligent autonomous system,
[00:02:50.800 --> 00:02:52.640]   it needs to have a predictive model of the world.
[00:02:52.640 --> 00:02:55.360]   So something that says, here is a world at time T,
[00:02:55.360 --> 00:02:56.800]   here is a state of the world at time T plus one
[00:02:56.800 --> 00:02:57.940]   if I take this action.
[00:02:57.940 --> 00:03:01.000]   And it's not a single answer, it can be a--
[00:03:01.000 --> 00:03:02.560]   - Yeah, it can be a distribution, yeah.
[00:03:02.560 --> 00:03:04.520]   - Yeah, well, but we don't know how to represent
[00:03:04.520 --> 00:03:06.160]   distributions in high-dimensional continuous spaces,
[00:03:06.160 --> 00:03:08.520]   so it's gotta be something weaker than that, okay?
[00:03:08.520 --> 00:03:11.040]   But with some representation of uncertainty.
[00:03:11.040 --> 00:03:13.920]   If you have that, then you can do
[00:03:13.920 --> 00:03:15.760]   what optimal control theorists call
[00:03:15.760 --> 00:03:17.400]   model predictive control, which means that
[00:03:17.400 --> 00:03:20.200]   you can run your model with a hypothesis
[00:03:20.200 --> 00:03:22.600]   for a sequence of action and then see the result.
[00:03:23.160 --> 00:03:24.560]   Now, what you need, the other thing you need
[00:03:24.560 --> 00:03:27.340]   is some sort of objective that you want to optimize.
[00:03:27.340 --> 00:03:30.080]   Am I reaching the goal of grabbing this object?
[00:03:30.080 --> 00:03:31.360]   Am I minimizing energy?
[00:03:31.360 --> 00:03:32.520]   Am I whatever, right?
[00:03:32.520 --> 00:03:34.800]   So there is some sort of objectives
[00:03:34.800 --> 00:03:36.240]   that you have to minimize.
[00:03:36.240 --> 00:03:38.040]   And so in your head, if you have this model,
[00:03:38.040 --> 00:03:39.560]   you can figure out the sequence of action
[00:03:39.560 --> 00:03:41.260]   that will optimize your objective.
[00:03:41.260 --> 00:03:44.640]   That objective is something that ultimately
[00:03:44.640 --> 00:03:46.920]   is rooted in your basal ganglia,
[00:03:46.920 --> 00:03:48.520]   at least in the human brain, that's what it is.
[00:03:48.520 --> 00:03:52.160]   Basal ganglia computes your level of contentment
[00:03:52.160 --> 00:03:55.320]   or miscontentment, I don't know if that's a word.
[00:03:55.320 --> 00:03:56.680]   Unhappiness, okay?
[00:03:56.680 --> 00:03:57.920]   - Yeah, yeah.
[00:03:57.920 --> 00:03:58.760]   - Discontentment.
[00:03:58.760 --> 00:03:59.720]   - Discontentment, maybe.
[00:03:59.720 --> 00:04:03.640]   - And so your entire behavior is driven towards
[00:04:03.640 --> 00:04:06.240]   kind of minimizing that objective,
[00:04:06.240 --> 00:04:08.720]   which is maximizing your contentment,
[00:04:08.720 --> 00:04:10.600]   computed by your basal ganglia.
[00:04:10.600 --> 00:04:14.560]   And what you have is an objective function,
[00:04:14.560 --> 00:04:16.360]   which is basically a predictor of what
[00:04:16.360 --> 00:04:18.440]   your basal ganglia is gonna tell you.
[00:04:18.440 --> 00:04:20.400]   So you're not gonna put your hand on fire
[00:04:20.400 --> 00:04:23.560]   because you know it's gonna burn
[00:04:23.560 --> 00:04:25.040]   and you're gonna get hurt.
[00:04:25.040 --> 00:04:27.360]   And you're predicting this because of your model of the world
[00:04:27.360 --> 00:04:31.400]   and your sort of predictor of this objective, right?
[00:04:31.400 --> 00:04:34.800]   So if you have those three components,
[00:04:34.800 --> 00:04:36.440]   you have four components,
[00:04:36.440 --> 00:04:40.600]   you have the hardwired contentment objective
[00:04:40.600 --> 00:04:45.200]   computer, if you want, calculator.
[00:04:45.200 --> 00:04:46.400]   And then you have those three components.
[00:04:46.400 --> 00:04:48.000]   One is the objective predictor,
[00:04:48.000 --> 00:04:50.200]   which basically predicts your level of contentment.
[00:04:50.200 --> 00:04:53.800]   One is the model of the world.
[00:04:53.800 --> 00:04:55.360]   And there's a third module I didn't mention,
[00:04:55.360 --> 00:04:58.520]   which is the module that will figure out
[00:04:58.520 --> 00:05:01.800]   the best course of action to optimize an objective
[00:05:01.800 --> 00:05:02.640]   given your model.
[00:05:02.640 --> 00:05:04.840]   Okay?
[00:05:04.840 --> 00:05:05.720]   - Yeah.
[00:05:05.720 --> 00:05:08.560]   - Glissa policy, policy network,
[00:05:08.560 --> 00:05:10.720]   or something like that, right?
[00:05:10.720 --> 00:05:13.000]   Now, you need those three components
[00:05:13.000 --> 00:05:15.240]   to act autonomously, intelligently.
[00:05:15.240 --> 00:05:17.400]   And you can be stupid in three different ways.
[00:05:17.400 --> 00:05:20.680]   You can be stupid because your model of the world is wrong.
[00:05:20.680 --> 00:05:23.800]   You can be stupid because your objective is not aligned
[00:05:23.800 --> 00:05:26.360]   with what you actually want to achieve.
[00:05:26.360 --> 00:05:27.200]   Okay?
[00:05:27.200 --> 00:05:30.440]   In humans, that would be a psychopath.
[00:05:30.440 --> 00:05:31.320]   - Right.
[00:05:31.320 --> 00:05:33.800]   - And then the third thing,
[00:05:33.800 --> 00:05:34.920]   the third way you can be stupid
[00:05:34.920 --> 00:05:36.240]   is that you have the right model,
[00:05:36.240 --> 00:05:37.640]   you have the right objective,
[00:05:37.640 --> 00:05:40.120]   but you're unable to figure out a course of action
[00:05:40.120 --> 00:05:42.320]   to optimize your objective given your model.
[00:05:42.360 --> 00:05:44.520]   (silence)
[00:05:44.520 --> 00:05:46.680]   (silence)
[00:05:46.680 --> 00:05:48.840]   (silence)
[00:05:48.840 --> 00:05:51.000]   (silence)
[00:05:51.000 --> 00:05:53.160]   (silence)
[00:05:53.160 --> 00:05:55.320]   (silence)
[00:05:55.320 --> 00:05:57.480]   (silence)
[00:05:57.480 --> 00:06:07.480]   [BLANK_AUDIO]

