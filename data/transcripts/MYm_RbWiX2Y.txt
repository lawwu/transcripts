
[00:00:00.000 --> 00:00:03.040]   Just went live so people are popping on.
[00:00:03.040 --> 00:00:07.360]   And so we're doing this both on Zoom and on YouTube.
[00:00:07.360 --> 00:00:10.400]   So you'll see some people popping on the Zoom
[00:00:10.400 --> 00:00:13.200]   and some people popping on YouTube.
[00:00:13.200 --> 00:00:15.800]   And we'll collect the questions for you from both places
[00:00:15.800 --> 00:00:17.080]   and just ask them.
[00:00:17.080 --> 00:00:18.640]   - Okay, yeah, that sounds good.
[00:00:18.640 --> 00:00:20.040]   Cool. - Cool.
[00:00:20.040 --> 00:00:22.600]   So for those of you who don't know you guys,
[00:00:22.600 --> 00:00:24.440]   would you like to introduce yourself?
[00:00:24.440 --> 00:00:27.120]   - Sure, yeah.
[00:00:27.120 --> 00:00:30.920]   I basically just finished my undergrad last year
[00:00:30.920 --> 00:00:34.000]   and I've been working in Google as a resident
[00:00:34.000 --> 00:00:35.960]   in the perception team.
[00:00:35.960 --> 00:00:37.400]   I'm in Cambridge, Mass.
[00:00:37.400 --> 00:00:40.760]   So yeah, and we collaborate with people at MIT
[00:00:40.760 --> 00:00:43.720]   and that's how this work came up.
[00:00:43.720 --> 00:00:46.360]   And we've, yeah.
[00:00:46.360 --> 00:00:48.800]   So, and I've been working in machine learning
[00:00:48.800 --> 00:00:50.960]   for around four years, I think.
[00:00:50.960 --> 00:00:52.040]   And yeah.
[00:00:52.040 --> 00:00:54.360]   - And what's the relationship
[00:00:54.360 --> 00:00:57.600]   with this super contrastive learning paper?
[00:00:57.600 --> 00:01:00.200]   - So yeah, the supervised contrastive learning
[00:01:00.200 --> 00:01:02.240]   is something I've been working on
[00:01:02.240 --> 00:01:05.920]   for around six to eight months at this point.
[00:01:05.920 --> 00:01:07.520]   We have a submission in Europe.
[00:01:07.520 --> 00:01:09.880]   So I'm just gonna present some results from there.
[00:01:09.880 --> 00:01:13.480]   And yeah, but this is part of a larger effort
[00:01:13.480 --> 00:01:18.480]   in to do contrastive learning for a large number of tasks
[00:01:18.480 --> 00:01:21.120]   such as images, audio, videos, and so on
[00:01:21.120 --> 00:01:23.000]   in a supervised setting.
[00:01:23.000 --> 00:01:27.080]   - Nice. And Charles, also, so welcome.
[00:01:27.080 --> 00:01:29.640]   Charles, would you like to tell us about yourself?
[00:01:29.640 --> 00:01:30.560]   - Sure, yeah.
[00:01:30.560 --> 00:01:35.000]   So the regulars of the salon maybe know me,
[00:01:35.000 --> 00:01:36.800]   but I actually haven't given one of these salons
[00:01:36.800 --> 00:01:39.320]   in a couple of months, so it's good to be back.
[00:01:39.320 --> 00:01:43.040]   Yeah, so I am a deep learning instructor
[00:01:43.040 --> 00:01:44.360]   at Weights and Biases.
[00:01:44.360 --> 00:01:45.800]   So I make educational content,
[00:01:45.800 --> 00:01:49.200]   including these salon talks for Weights and Biases.
[00:01:49.200 --> 00:01:52.000]   I just finished up my PhD at Berkeley.
[00:01:52.000 --> 00:01:57.000]   I was studying neural networks and the optimization process
[00:01:57.000 --> 00:01:59.800]   and the optimization properties of neural networks.
[00:01:59.800 --> 00:02:01.840]   - Nice.
[00:02:01.840 --> 00:02:03.120]   All right.
[00:02:03.120 --> 00:02:04.600]   Welcome, Charles.
[00:02:04.600 --> 00:02:07.160]   I just wanted to check in with you guys
[00:02:07.160 --> 00:02:08.800]   in the audience really quickly.
[00:02:08.800 --> 00:02:09.840]   Where are you guys from?
[00:02:09.840 --> 00:02:13.680]   Tell us what time it is that you're joining us from,
[00:02:13.680 --> 00:02:14.880]   how you're doing.
[00:02:14.880 --> 00:02:21.240]   And you can, for the people on YouTube,
[00:02:21.240 --> 00:02:24.480]   you can pop it in the chat to the left of your screen.
[00:02:24.480 --> 00:02:25.920]   And then for the people on the Zoom,
[00:02:25.920 --> 00:02:27.560]   you can just pop it in the chat.
[00:02:27.560 --> 00:02:32.840]   Make sure it says all panelists and attendees,
[00:02:32.840 --> 00:02:35.280]   so we all get to hear what you're saying.
[00:02:35.280 --> 00:02:40.120]   I see someone raising their hand.
[00:02:40.120 --> 00:02:42.640]   Joseph, do you want to put, yeah, exactly.
[00:02:42.640 --> 00:02:44.600]   Okay, you found the chat.
[00:02:44.600 --> 00:02:47.400]   Another note, if you have questions,
[00:02:47.400 --> 00:02:51.520]   please feel free to ask the questions as people are talking.
[00:02:51.520 --> 00:02:56.480]   - Sorry, I'm changing it to local.
[00:02:56.480 --> 00:02:57.640]   Give me one second.
[00:02:57.640 --> 00:03:00.840]   - So the best part of these salons is-
[00:03:00.840 --> 00:03:04.440]   - So sorry, okay, I'm done.
[00:03:04.440 --> 00:03:07.960]   - So the best part of these salons
[00:03:07.960 --> 00:03:11.960]   are the conversations that happen after speakers talk.
[00:03:11.960 --> 00:03:14.480]   So please feel free to post your questions
[00:03:14.480 --> 00:03:15.720]   while they're talking,
[00:03:15.720 --> 00:03:18.400]   and then we'll come to you right after.
[00:03:18.400 --> 00:03:21.280]   And we would love to spend 10 to 15 minutes
[00:03:21.280 --> 00:03:23.320]   between the talks just on questions,
[00:03:23.320 --> 00:03:25.800]   so you get to interact with our amazing speakers
[00:03:25.800 --> 00:03:26.760]   and ask them the questions
[00:03:26.760 --> 00:03:29.200]   that you wouldn't get to ask otherwise.
[00:03:29.200 --> 00:03:32.120]   And you can drop these questions again in the chat,
[00:03:32.120 --> 00:03:34.920]   or if you're on the Zoom, you can also use the Q&A button
[00:03:34.920 --> 00:03:38.000]   and Charles or I will come and get you.
[00:03:38.000 --> 00:03:41.800]   And with that, I would like to invite Pranay
[00:03:41.800 --> 00:03:43.560]   to go first, maybe.
[00:03:44.520 --> 00:03:45.360]   - Sure.
[00:03:45.360 --> 00:03:47.960]   - All right, Pranay, the floor is yours.
[00:03:47.960 --> 00:03:49.280]   - Share my screen, yep.
[00:03:49.280 --> 00:03:54.720]   - And we can insert virtual clapping here
[00:03:54.720 --> 00:03:56.840]   as we welcome you onto the stage.
[00:03:56.840 --> 00:04:02.760]   - Okay, I hope you can see my screen.
[00:04:02.760 --> 00:04:06.360]   Okay, great, yeah.
[00:04:06.360 --> 00:04:09.200]   So just before I start,
[00:04:09.200 --> 00:04:12.240]   this work has been done with a lot of people.
[00:04:13.160 --> 00:04:16.520]   So Peter, AJ, Aaron, Chen,
[00:04:16.520 --> 00:04:19.120]   Sir and Dilip are at Google,
[00:04:19.120 --> 00:04:20.960]   and Philip and Yonglong are at MIT,
[00:04:20.960 --> 00:04:23.920]   and this has been a joint effort with them.
[00:04:23.920 --> 00:04:28.920]   And yeah, we have submitted this to NeurIPS,
[00:04:28.920 --> 00:04:32.240]   and we have a archive version available,
[00:04:32.240 --> 00:04:34.760]   which we will be updating as well.
[00:04:34.760 --> 00:04:38.360]   So the results in this talk are gonna be
[00:04:38.360 --> 00:04:40.680]   from other contrastive learning work,
[00:04:40.680 --> 00:04:41.720]   supervised contrastive,
[00:04:41.720 --> 00:04:43.920]   and some results which are not exactly published yet,
[00:04:43.920 --> 00:04:46.440]   but we'll get there in a few days.
[00:04:46.440 --> 00:04:49.800]   So yeah, if you have any questions about those results,
[00:04:49.800 --> 00:04:51.640]   you should definitely ask them.
[00:04:51.640 --> 00:04:56.040]   Right, so basically,
[00:04:56.040 --> 00:05:00.720]   contrastive learning is this study
[00:05:00.720 --> 00:05:04.600]   where people have used representation learning
[00:05:04.600 --> 00:05:08.040]   in a context which is separate from the downstream task.
[00:05:08.040 --> 00:05:11.880]   And the basic idea which came out of theoretical work
[00:05:11.880 --> 00:05:16.880]   that was done by Sanjeev Arora at Princeton a few years ago
[00:05:16.880 --> 00:05:20.080]   was basically that you can pull samples
[00:05:20.080 --> 00:05:22.200]   which share class information,
[00:05:22.200 --> 00:05:25.320]   or they are different augmentations of each other,
[00:05:25.320 --> 00:05:27.160]   and hence you know that they are more similar
[00:05:27.160 --> 00:05:28.400]   than others together,
[00:05:28.400 --> 00:05:33.080]   and push samples which you expect to be different apart.
[00:05:33.080 --> 00:05:36.400]   And these could be various kinds of transforms
[00:05:36.400 --> 00:05:37.920]   that you could come up with,
[00:05:37.920 --> 00:05:42.480]   such as color transforms, crops, warping,
[00:05:42.480 --> 00:05:43.840]   Gaussian blurs, and so on,
[00:05:43.840 --> 00:05:47.080]   and all kinds of meaningful semantic information
[00:05:47.080 --> 00:05:49.840]   retaining transformations are useful.
[00:05:49.840 --> 00:05:51.920]   It also depends on your downstream task sometimes,
[00:05:51.920 --> 00:05:53.800]   but we can get into that later.
[00:05:53.800 --> 00:05:56.360]   So that's basically the idea of contrastive learning.
[00:05:56.360 --> 00:06:01.360]   So our goal was to understand why these work so well,
[00:06:01.360 --> 00:06:05.200]   and what's the basic principles
[00:06:05.200 --> 00:06:06.640]   which we can extract from that,
[00:06:06.640 --> 00:06:09.440]   and what are the use cases where this,
[00:06:09.440 --> 00:06:10.880]   what are the kind of downstream tasks
[00:06:10.880 --> 00:06:12.280]   where this works well?
[00:06:12.280 --> 00:06:16.400]   So this is kind of the work that came out
[00:06:16.400 --> 00:06:19.360]   in the last 12 months, which gained a lot of fame.
[00:06:19.360 --> 00:06:21.040]   Basically, there were these two papers.
[00:06:21.040 --> 00:06:23.600]   One was contrastive multi-view coding
[00:06:23.600 --> 00:06:25.280]   by people over at MIT,
[00:06:25.280 --> 00:06:29.480]   and the other was the paper from Hinton's group,
[00:06:29.480 --> 00:06:32.040]   which is simpler.
[00:06:32.040 --> 00:06:35.120]   Both of them work in very similar ways.
[00:06:35.120 --> 00:06:36.880]   So what you have on the left
[00:06:36.880 --> 00:06:40.760]   is that you take different color transformations,
[00:06:40.760 --> 00:06:43.880]   and you actually reduce into various color spaces,
[00:06:43.880 --> 00:06:45.920]   just let's just luminance,
[00:06:45.920 --> 00:06:47.000]   and maybe chrominance,
[00:06:47.000 --> 00:06:51.400]   or maybe various semantic segmentations,
[00:06:51.400 --> 00:06:56.400]   and then you embed them using separate encoder networks,
[00:06:56.400 --> 00:06:59.720]   but into the same representation space.
[00:06:59.720 --> 00:07:01.560]   The representation space is shared,
[00:07:01.560 --> 00:07:03.560]   while encoder networks will be different,
[00:07:03.560 --> 00:07:06.960]   and you do this for a single image.
[00:07:06.960 --> 00:07:08.000]   So you have, let's say,
[00:07:08.000 --> 00:07:10.320]   four representations corresponding to one image,
[00:07:10.320 --> 00:07:12.240]   and then you try to push them together,
[00:07:12.240 --> 00:07:14.880]   while you try to push representations
[00:07:14.880 --> 00:07:18.360]   from any of these networks for a different sample apart.
[00:07:18.360 --> 00:07:20.840]   And so this is a completely unsupervised way
[00:07:20.840 --> 00:07:22.400]   of learning representations
[00:07:22.400 --> 00:07:24.720]   in a contrastive learning setting.
[00:07:24.720 --> 00:07:28.080]   SimClear had a slightly different view
[00:07:28.080 --> 00:07:32.440]   where they took random data-augmented versions of a sample,
[00:07:32.440 --> 00:07:33.880]   which was basically random crop,
[00:07:33.880 --> 00:07:36.600]   and then color transforms, color jitters, Gaussian blurs,
[00:07:36.600 --> 00:07:38.800]   and they encoded them,
[00:07:38.800 --> 00:07:40.760]   and basically pushed the representations
[00:07:40.760 --> 00:07:43.560]   of the two dogs together,
[00:07:43.560 --> 00:07:45.840]   and the representation corresponding
[00:07:45.840 --> 00:07:48.160]   to different data augmentations of the chair together,
[00:07:48.160 --> 00:07:51.000]   but pushed the representation of the dog and chair apart.
[00:07:51.000 --> 00:07:53.480]   But this was not done using label information,
[00:07:53.480 --> 00:07:56.800]   hence they could have very similar looking dogs
[00:07:56.800 --> 00:08:00.440]   being pushed apart in the representation space.
[00:08:00.440 --> 00:08:05.080]   So before we go into more details,
[00:08:05.080 --> 00:08:09.160]   I just want to talk about how these techniques usually work.
[00:08:09.160 --> 00:08:11.840]   So the aim here is that the downstream task
[00:08:11.840 --> 00:08:16.000]   that you evaluate on is still top one accuracy
[00:08:16.000 --> 00:08:18.560]   and labeling samples.
[00:08:18.560 --> 00:08:20.960]   So the way you do it is that you have a first stage
[00:08:20.960 --> 00:08:23.280]   where you actually do contrastive learning,
[00:08:23.280 --> 00:08:26.480]   and contrastive learning losses are actually applied
[00:08:26.480 --> 00:08:28.880]   in a small 128-dimensional space.
[00:08:28.880 --> 00:08:30.760]   So you have your encoded network,
[00:08:30.760 --> 00:08:33.320]   which actually gives an output of 2K,
[00:08:33.320 --> 00:08:35.160]   and then you actually project it down
[00:08:35.160 --> 00:08:38.560]   to something like 128 dimensions or 256 dimensions,
[00:08:38.560 --> 00:08:40.360]   depending on what your use case is,
[00:08:40.360 --> 00:08:42.680]   apply contrastive losses in that space,
[00:08:42.680 --> 00:08:43.760]   train your network,
[00:08:43.760 --> 00:08:46.520]   and then you drop out the projection networks
[00:08:46.520 --> 00:08:48.160]   and use the 2K embeddings,
[00:08:48.160 --> 00:08:50.840]   that is the last layer of the ResNet in most cases,
[00:08:50.840 --> 00:08:53.480]   or efficient net, whatever, is your encoder network,
[00:08:53.480 --> 00:08:57.160]   and you apply a linear classifier on top of that
[00:08:57.160 --> 00:08:58.640]   and try to do cross-entropy training
[00:08:58.640 --> 00:08:59.600]   so there's a first stage
[00:08:59.600 --> 00:09:01.240]   where you're doing contrastive learning
[00:09:01.240 --> 00:09:03.680]   and a second stage when you're doing cross-entropy learning.
[00:09:03.680 --> 00:09:06.560]   This is in contrast with standard cross-entropy
[00:09:06.560 --> 00:09:07.760]   where you train end-to-end.
[00:09:07.760 --> 00:09:10.360]   And the benefit here is that these representations
[00:09:10.360 --> 00:09:12.680]   are independent of the label information.
[00:09:12.680 --> 00:09:14.760]   So you'd never use label information
[00:09:14.760 --> 00:09:15.800]   during contrastive learning,
[00:09:15.800 --> 00:09:17.360]   so the idea is that you should be able to use it
[00:09:17.360 --> 00:09:20.720]   for transfer learning and other downstream tasks
[00:09:20.720 --> 00:09:24.440]   without actually biasing your training procedure
[00:09:24.440 --> 00:09:26.640]   with using labels.
[00:09:26.640 --> 00:09:27.720]   Right.
[00:09:27.720 --> 00:09:32.480]   So the form of the loss that is used
[00:09:32.480 --> 00:09:36.200]   in such contrastive learning setups
[00:09:36.200 --> 00:09:38.560]   is something that looks like this.
[00:09:38.560 --> 00:09:41.520]   So you have the total loss over the entire batch,
[00:09:41.520 --> 00:09:44.200]   and for every sample in the batch,
[00:09:44.200 --> 00:09:47.600]   you have a likelihood term inside the log.
[00:09:47.600 --> 00:09:52.600]   This likelihood term comes from the exponent
[00:09:52.600 --> 00:09:55.560]   of the similarity between the embeddings.
[00:09:55.560 --> 00:09:58.800]   So zi is the embedding corresponding to sample i,
[00:09:58.800 --> 00:10:01.960]   and zj of i is a data-augmented version
[00:10:01.960 --> 00:10:04.600]   of a given sample in the batch,
[00:10:04.600 --> 00:10:09.160]   and you have the similarity between them as zi dot zji.
[00:10:09.160 --> 00:10:11.880]   This can be generalized to arbitrary kernels if you want to,
[00:10:11.880 --> 00:10:13.120]   but in most cases,
[00:10:13.120 --> 00:10:14.840]   since you already have a projection network,
[00:10:14.840 --> 00:10:19.000]   you can assume that a kernel is being learned
[00:10:19.000 --> 00:10:20.400]   by that projection network.
[00:10:20.400 --> 00:10:24.800]   And then you, yeah, so basically you exponentiate
[00:10:24.800 --> 00:10:26.360]   and you come up with this likelihood term.
[00:10:26.360 --> 00:10:29.000]   So what this term is, it basically says that,
[00:10:29.000 --> 00:10:33.040]   given the batch, what is the probability that j of i
[00:10:33.040 --> 00:10:37.120]   is the most similar sample to sample i
[00:10:37.120 --> 00:10:40.880]   among all the samples in the batch?
[00:10:40.880 --> 00:10:43.000]   And so this is not over the entire training dataset,
[00:10:43.000 --> 00:10:44.720]   but for large batches, you can say that,
[00:10:44.720 --> 00:10:46.280]   okay, this is an unbiased estimate
[00:10:46.280 --> 00:10:49.720]   of the likelihood in the entire dataset.
[00:10:49.720 --> 00:10:52.160]   And, well, maybe not unbiased.
[00:10:52.160 --> 00:10:53.560]   It's gonna be biased, yeah.
[00:10:53.560 --> 00:10:58.160]   But, right, so you have this log likelihood term
[00:10:58.160 --> 00:10:59.960]   which you are minimizing.
[00:10:59.960 --> 00:11:04.520]   So you're maximizing the likelihood,
[00:11:04.520 --> 00:11:06.920]   and this is what you use to train
[00:11:06.920 --> 00:11:08.560]   the entire network end-to-end.
[00:11:08.560 --> 00:11:12.080]   And you use all the other samples as negatives.
[00:11:12.080 --> 00:11:14.320]   So negative is any sample that is not supposed
[00:11:14.320 --> 00:11:15.560]   to be similar to the anchor.
[00:11:15.560 --> 00:11:18.360]   So, and a positive is a sample
[00:11:18.360 --> 00:11:20.200]   which is supposed to be similar to the anchor.
[00:11:20.200 --> 00:11:21.800]   And in self-supervised losses,
[00:11:21.800 --> 00:11:24.360]   that's just the single data augmentation
[00:11:24.360 --> 00:11:27.480]   of the given anchor, right?
[00:11:27.480 --> 00:11:31.160]   So, but why should that work, right?
[00:11:31.160 --> 00:11:34.120]   And there are various papers
[00:11:34.120 --> 00:11:35.760]   which were doing similar things.
[00:11:35.760 --> 00:11:37.760]   And what is different about contrastive learning?
[00:11:37.760 --> 00:11:39.440]   Like triplet losses have been known,
[00:11:39.440 --> 00:11:41.280]   and Siamese network have been known,
[00:11:41.280 --> 00:11:43.040]   and a bunch of other things have been known.
[00:11:43.040 --> 00:11:45.120]   Like energy-based models have been famous
[00:11:45.120 --> 00:11:46.400]   for a very long time.
[00:11:46.400 --> 00:11:47.520]   So the question is that,
[00:11:48.400 --> 00:11:50.320]   what is amazing about
[00:11:50.320 --> 00:11:52.360]   this contrastive learning framework?
[00:11:52.360 --> 00:11:53.840]   And the answer is that,
[00:11:53.840 --> 00:11:58.120]   contrastive losses are a lower bound
[00:11:58.120 --> 00:12:01.440]   on the mutual information between the views of the data.
[00:12:01.440 --> 00:12:03.440]   So given a sample,
[00:12:03.440 --> 00:12:05.960]   you have two data augmentations of it, right?
[00:12:05.960 --> 00:12:07.480]   One of them is being called the anchor,
[00:12:07.480 --> 00:12:10.280]   and the other is going to be called the positive, right?
[00:12:10.280 --> 00:12:13.520]   And contrastive losses actually a lower bound
[00:12:13.520 --> 00:12:16.160]   on the mutual information between these two samples.
[00:12:16.160 --> 00:12:20.360]   So, what you're technically doing
[00:12:20.360 --> 00:12:21.800]   in this entire training procedure
[00:12:21.800 --> 00:12:23.440]   is using this likelihood estimate
[00:12:23.440 --> 00:12:26.520]   to actually maximize the amount of mutual information
[00:12:26.520 --> 00:12:28.080]   between the two views of the data.
[00:12:28.080 --> 00:12:29.600]   And note that these two views
[00:12:29.600 --> 00:12:32.080]   are actually created arbitrarily by you.
[00:12:32.080 --> 00:12:35.000]   You are either doing color transform,
[00:12:35.000 --> 00:12:38.080]   or you're doing data augmentation or something like that.
[00:12:38.080 --> 00:12:40.440]   And you can do a lot of other things.
[00:12:40.440 --> 00:12:41.760]   Like for example, in video,
[00:12:41.760 --> 00:12:44.200]   you could actually take different frames, right?
[00:12:44.200 --> 00:12:47.160]   So just having two views on the data, right?
[00:12:47.160 --> 00:12:50.480]   And actually maximizing the mutual information between them
[00:12:50.480 --> 00:12:52.120]   and you have a lower bound for that.
[00:12:52.120 --> 00:12:56.360]   So you have the contrastive loss that can be optimized,
[00:12:56.360 --> 00:12:57.200]   right?
[00:12:57.200 --> 00:13:02.200]   And the by-product of that is that you can show
[00:13:02.200 --> 00:13:07.000]   that the self-supervised loss, when minimized,
[00:13:07.000 --> 00:13:09.440]   actually also minimizes some version
[00:13:09.440 --> 00:13:11.080]   of the downstream classification loss.
[00:13:11.080 --> 00:13:13.320]   So under some regularity assumptions,
[00:13:13.320 --> 00:13:15.560]   like your downstream loss is gonna be a classification loss
[00:13:15.560 --> 00:13:17.200]   and it's probably gonna be a binary loss
[00:13:17.200 --> 00:13:19.080]   or a categorical loss and so on.
[00:13:19.080 --> 00:13:21.200]   You can come up with a bound on that.
[00:13:21.200 --> 00:13:23.720]   If you actually minimize the self-supervised loss,
[00:13:23.720 --> 00:13:26.240]   you're gonna get some multiplicative.
[00:13:26.240 --> 00:13:27.360]   It's not an additive bound,
[00:13:27.360 --> 00:13:31.480]   but a multiplicative bound on your downstream task.
[00:13:31.480 --> 00:13:32.960]   So, which is a good thing,
[00:13:32.960 --> 00:13:34.960]   because it means that what you're optimizing
[00:13:34.960 --> 00:13:37.560]   inherently optimizes what you care about downstream as well,
[00:13:37.560 --> 00:13:40.640]   which is not the case usually, right?
[00:13:40.640 --> 00:13:42.480]   So, this is why it works.
[00:13:42.480 --> 00:13:46.040]   And you can look into the details of some of the work
[00:13:46.040 --> 00:13:46.880]   that has been done,
[00:13:46.880 --> 00:13:49.960]   such as the work by Bachman and the work by Aurora.
[00:13:49.960 --> 00:13:53.360]   And these are great reasons themselves, right?
[00:13:53.360 --> 00:13:57.000]   So, but all of that's fine, right?
[00:13:57.000 --> 00:14:00.560]   The question is that we tried this before
[00:14:00.560 --> 00:14:02.640]   and a lot of formulations came out before.
[00:14:02.640 --> 00:14:03.880]   Does this work now?
[00:14:03.880 --> 00:14:05.560]   And the answer seems to be yes.
[00:14:05.560 --> 00:14:09.680]   Sinclair basically made the claim
[00:14:09.680 --> 00:14:12.000]   that with a 4x ResNet-50,
[00:14:12.000 --> 00:14:14.720]   they are able to get as much as supervised performance,
[00:14:14.720 --> 00:14:16.280]   which is a great result.
[00:14:16.280 --> 00:14:19.080]   And there are arguments about,
[00:14:19.080 --> 00:14:21.000]   okay, why didn't deeper networks work?
[00:14:21.000 --> 00:14:22.520]   And why did wider networks work?
[00:14:22.520 --> 00:14:26.040]   But it is inarguable that self-supervised learning
[00:14:26.040 --> 00:14:28.560]   is getting very close to fully supervised learning.
[00:14:28.560 --> 00:14:33.560]   And the question is that, can we push it further?
[00:14:33.560 --> 00:14:37.320]   Can we actually make contrastive learning work
[00:14:37.320 --> 00:14:39.960]   better than cross-entropy learning, right?
[00:14:39.960 --> 00:14:41.920]   Because in the end,
[00:14:41.920 --> 00:14:44.520]   you're still only using label information
[00:14:44.520 --> 00:14:45.760]   in the self-supervised settings
[00:14:45.760 --> 00:14:48.000]   when you're doing actual cross-entropy training
[00:14:48.000 --> 00:14:49.240]   of the linear classifier
[00:14:49.240 --> 00:14:50.960]   that is trained on the embedding.
[00:14:50.960 --> 00:14:52.120]   So you freeze the embedding,
[00:14:52.120 --> 00:14:53.520]   then now you're doing linear classifier,
[00:14:53.520 --> 00:14:55.520]   but still being trained with cross-entropy, right?
[00:14:55.520 --> 00:14:58.240]   So you did actually use your label information
[00:14:58.240 --> 00:14:59.240]   very meaningfully.
[00:14:59.240 --> 00:15:01.800]   You just use unsupervised part of your data.
[00:15:01.800 --> 00:15:04.640]   So that is where our work comes from.
[00:15:04.640 --> 00:15:05.840]   The motivation is that,
[00:15:05.840 --> 00:15:07.680]   could you basically take this entire line
[00:15:07.680 --> 00:15:09.520]   and just push it up, right?
[00:15:09.520 --> 00:15:12.240]   And the formulation we came up with
[00:15:12.240 --> 00:15:15.840]   was just going along with the way of thinking
[00:15:15.840 --> 00:15:18.160]   that we have in standard contrastive learning
[00:15:18.160 --> 00:15:20.160]   that you have an anchor,
[00:15:20.160 --> 00:15:21.920]   which is let's say this cute dog,
[00:15:21.920 --> 00:15:26.360]   and then you have an augmented version of the same sample,
[00:15:26.360 --> 00:15:27.240]   which is a positive,
[00:15:27.240 --> 00:15:28.800]   and maybe you have this other cute dog,
[00:15:28.800 --> 00:15:32.640]   which is also a positive for this same sample, right?
[00:15:32.640 --> 00:15:34.760]   So, and you push away all the other samples
[00:15:34.760 --> 00:15:36.360]   such as elephants and cats and so on,
[00:15:36.360 --> 00:15:38.440]   which is not something
[00:15:38.440 --> 00:15:40.000]   that self-supervised contrastive does.
[00:15:40.000 --> 00:15:41.520]   Basically self-supervised contrastive
[00:15:41.520 --> 00:15:44.520]   will also push away this specific dog
[00:15:44.520 --> 00:15:47.000]   from the anchor dog,
[00:15:47.000 --> 00:15:48.600]   which is not a good thing
[00:15:48.600 --> 00:15:53.320]   because you actually want this to be like a smooth falloff
[00:15:53.320 --> 00:15:55.000]   instead of like a very extreme falloff
[00:15:55.000 --> 00:15:57.720]   that, okay, the moment you move away from a single sample,
[00:15:57.720 --> 00:15:59.600]   nothing has a very high likelihood
[00:15:59.600 --> 00:16:02.600]   in this 2K dimensional, high dimensional space.
[00:16:02.600 --> 00:16:03.600]   You don't want that.
[00:16:03.600 --> 00:16:05.320]   What you want is that it's a smooth falloff
[00:16:05.320 --> 00:16:06.480]   and the falloff corresponds
[00:16:06.480 --> 00:16:08.640]   to the actual semantic similarity.
[00:16:08.640 --> 00:16:10.800]   And label information in some sense
[00:16:10.800 --> 00:16:13.080]   is a good measure of semantic similarity.
[00:16:13.080 --> 00:16:16.760]   There are better ones and more diverse labels also help,
[00:16:16.760 --> 00:16:19.680]   but given what we have,
[00:16:19.680 --> 00:16:21.520]   label is definitely something that we should use
[00:16:21.520 --> 00:16:23.520]   and this is how we're using it.
[00:16:23.520 --> 00:16:28.200]   It's still not clear how to do this perfectly
[00:16:28.200 --> 00:16:29.880]   because we have this likelihood
[00:16:29.880 --> 00:16:32.120]   and mutual information kind of interpretation before,
[00:16:32.120 --> 00:16:33.360]   and we want to keep all of those
[00:16:33.360 --> 00:16:35.880]   without making things more difficult for ourselves.
[00:16:35.880 --> 00:16:40.040]   So what we realized was that there are multiple possibilities.
[00:16:40.040 --> 00:16:43.360]   One clear possibility is that just take out samples
[00:16:43.360 --> 00:16:47.200]   which correspond to samples from the same class
[00:16:47.200 --> 00:16:48.480]   being used as negatives.
[00:16:48.480 --> 00:16:50.760]   So do not push against them basically.
[00:16:50.760 --> 00:16:52.560]   And does that work well?
[00:16:52.560 --> 00:16:53.560]   The answer is no.
[00:16:53.560 --> 00:16:57.760]   It does not lead to great boost in performance
[00:16:57.760 --> 00:16:59.800]   and our running hypothesis
[00:16:59.800 --> 00:17:03.320]   that you actually do need to pull samples
[00:17:03.320 --> 00:17:04.720]   which belong to the same class together.
[00:17:04.720 --> 00:17:06.240]   And there has been recent work
[00:17:06.240 --> 00:17:09.840]   which shows that there are other ways to get around this,
[00:17:09.840 --> 00:17:13.280]   but our way was that you actually use them as positives
[00:17:13.280 --> 00:17:15.320]   in the sense that samples from the same class,
[00:17:15.320 --> 00:17:17.920]   but which are not data augmented versions of the anchor,
[00:17:17.920 --> 00:17:19.760]   you pull them closer to the anchor
[00:17:19.760 --> 00:17:24.640]   instead of just not pushing them away, if that's clear.
[00:17:24.640 --> 00:17:29.200]   And the other contribution that we felt
[00:17:29.200 --> 00:17:31.640]   was very important in making this work
[00:17:31.640 --> 00:17:33.960]   was using stronger data augmentation
[00:17:33.960 --> 00:17:36.400]   to make sure that you don't now learn
[00:17:36.400 --> 00:17:37.720]   spurious correlation to labels,
[00:17:37.720 --> 00:17:39.640]   because that was one of the good parts
[00:17:39.640 --> 00:17:40.640]   about contrasted learning
[00:17:40.640 --> 00:17:42.480]   that since you weren't using label information,
[00:17:42.480 --> 00:17:44.840]   you weren't picking up spurious correlations
[00:17:44.840 --> 00:17:48.520]   between your image data and the labels associated with them.
[00:17:48.520 --> 00:17:52.680]   So the idea was to have a very strong data augmentation
[00:17:52.680 --> 00:17:53.840]   which gets around that.
[00:17:53.840 --> 00:17:59.120]   So I'd just like to introduce
[00:17:59.120 --> 00:18:00.880]   what the loss function looks like.
[00:18:00.880 --> 00:18:05.880]   So the expression might be slightly hard to work with,
[00:18:05.880 --> 00:18:09.920]   but here's what it looks like.
[00:18:09.920 --> 00:18:13.960]   For a given anchor i in the batch,
[00:18:13.960 --> 00:18:17.000]   you have a set of samples which are positives,
[00:18:17.000 --> 00:18:18.640]   which are denoted by P of i.
[00:18:18.640 --> 00:18:20.760]   So you normalize for the number of positives
[00:18:20.760 --> 00:18:22.360]   you have for a given sample.
[00:18:22.360 --> 00:18:25.440]   So you don't want that some losses are higher or lower
[00:18:25.440 --> 00:18:27.080]   because they just have more samples
[00:18:27.080 --> 00:18:29.000]   and more positives in the batch,
[00:18:29.000 --> 00:18:32.520]   because we are still sampling our batches randomly.
[00:18:32.520 --> 00:18:37.520]   And we don't want that it ends up that some of the samples,
[00:18:37.520 --> 00:18:41.840]   a large amount of the gradient comes from a few samples.
[00:18:41.840 --> 00:18:45.880]   Right, the next part is that basically
[00:18:45.880 --> 00:18:47.800]   we again have a likelihood term,
[00:18:47.800 --> 00:18:52.800]   but this likelihood term is computed for every positive
[00:18:52.800 --> 00:18:58.000]   and every, so for a given anchor,
[00:18:58.000 --> 00:18:59.800]   for every positive that exists,
[00:18:59.800 --> 00:19:01.720]   you compute this likelihood term,
[00:19:01.720 --> 00:19:03.640]   very similar to self-supervised
[00:19:03.640 --> 00:19:04.680]   where you have this dot product,
[00:19:04.680 --> 00:19:07.920]   which is supposed to be a measure of similarity.
[00:19:07.920 --> 00:19:10.520]   And then you exponentiate to get this likelihood term.
[00:19:10.520 --> 00:19:12.760]   And what we are basically saying is that
[00:19:12.760 --> 00:19:16.600]   we are minimizing the negative log likelihood
[00:19:16.600 --> 00:19:19.080]   of the sum of these likelihoods, right?
[00:19:19.080 --> 00:19:22.880]   And what's that, that's equivalent to saying
[00:19:22.880 --> 00:19:27.880]   that you are actually maximizing the joint likelihood
[00:19:28.720 --> 00:19:32.960]   of all the positives given a specific anchor.
[00:19:32.960 --> 00:19:36.400]   So that's the distribution that we are working with, right?
[00:19:36.400 --> 00:19:40.920]   The other possibility is that
[00:19:40.920 --> 00:19:43.080]   you actually take these likelihoods,
[00:19:43.080 --> 00:19:45.600]   you add them up, which is basically saying that,
[00:19:45.600 --> 00:19:47.680]   okay, you just have this conditional distribution
[00:19:47.680 --> 00:19:50.360]   and you're just adding all these likelihoods together
[00:19:50.360 --> 00:19:53.560]   and you are gonna maximize that,
[00:19:53.560 --> 00:19:55.480]   this likelihood together, right?
[00:19:55.480 --> 00:19:57.800]   And so basically you compute this likelihood
[00:19:57.800 --> 00:19:59.000]   and you keep the log outside.
[00:19:59.000 --> 00:20:01.280]   There's a big difference when you look at the expression
[00:20:01.280 --> 00:20:06.280]   is that if the summation one by P of I is inside the log,
[00:20:06.280 --> 00:20:10.240]   which is this case or outside the log.
[00:20:10.240 --> 00:20:14.640]   And what our results suggest is that
[00:20:14.640 --> 00:20:17.520]   outside is better by a very big margin.
[00:20:17.520 --> 00:20:21.280]   And the insight there basically is something like this,
[00:20:21.280 --> 00:20:23.800]   that when you have it outside,
[00:20:25.200 --> 00:20:30.200]   a given positive, which is very far away from the anchor
[00:20:30.200 --> 00:20:33.200]   has an equal contribution in the gradient
[00:20:33.200 --> 00:20:37.240]   as a positive, which is very close to the anchor, right?
[00:20:37.240 --> 00:20:41.120]   But that is not true
[00:20:41.120 --> 00:20:43.800]   for when the summation is inside the log.
[00:20:43.800 --> 00:20:45.400]   And you can see this from the expression
[00:20:45.400 --> 00:20:48.720]   because you have these probabilities getting added up
[00:20:48.720 --> 00:20:53.360]   and you have a temperature parameter here.
[00:20:53.360 --> 00:20:55.800]   So when this temperature parameter is actually low,
[00:20:55.800 --> 00:20:58.040]   which is generally the setting that we have,
[00:20:58.040 --> 00:21:01.680]   then a large amount of the likelihood
[00:21:01.680 --> 00:21:04.040]   is actually concentrated on a few samples.
[00:21:04.040 --> 00:21:07.640]   So it will turn out that most of the magnitude
[00:21:07.640 --> 00:21:09.840]   of the gradient will actually come from a few samples
[00:21:09.840 --> 00:21:12.520]   in the second case, which is not optimal for training
[00:21:12.520 --> 00:21:13.920]   because then you're not using
[00:21:13.920 --> 00:21:17.200]   your label information meaningfully, right?
[00:21:17.200 --> 00:21:20.560]   So this is the last function we ended up working with.
[00:21:22.360 --> 00:21:25.200]   All the terms except the self similarity
[00:21:25.200 --> 00:21:26.640]   are in the denominator
[00:21:26.640 --> 00:21:28.560]   and the numerator has the single term
[00:21:28.560 --> 00:21:29.880]   corresponding to the similarity
[00:21:29.880 --> 00:21:31.800]   between an anchor and its positive.
[00:21:31.800 --> 00:21:33.920]   And then you sum over all the positives.
[00:21:33.920 --> 00:21:38.920]   And just talking a little more about the loss function
[00:21:38.920 --> 00:21:42.920]   because some people have this notion
[00:21:42.920 --> 00:21:45.160]   that there's something unique about this loss function.
[00:21:45.160 --> 00:21:49.440]   And our perspective is that while the interpretation
[00:21:49.440 --> 00:21:51.640]   of the loss function as a mutual information
[00:21:52.640 --> 00:21:57.640]   minimizer or maximizer is interesting,
[00:21:57.640 --> 00:22:00.040]   that is not what is training the model.
[00:22:00.040 --> 00:22:01.760]   What is training the model is the gradients
[00:22:01.760 --> 00:22:04.080]   and we should be able to look at the gradient
[00:22:04.080 --> 00:22:07.040]   and make interesting observations.
[00:22:07.040 --> 00:22:10.360]   So one of the interesting observations
[00:22:10.360 --> 00:22:12.400]   that we have been able to make
[00:22:12.400 --> 00:22:17.400]   is that the gradient looks something like this
[00:22:17.400 --> 00:22:22.400]   where the gradient of the loss
[00:22:22.400 --> 00:22:25.440]   with respect to the embedding is something like,
[00:22:25.440 --> 00:22:26.800]   so we have one by tau term,
[00:22:26.800 --> 00:22:30.040]   which is basically weighing the representation
[00:22:30.040 --> 00:22:31.800]   which are coming from other samples.
[00:22:31.800 --> 00:22:34.040]   So it's weighing Z of P,
[00:22:34.040 --> 00:22:35.000]   which is the representation
[00:22:35.000 --> 00:22:37.400]   corresponding to the positive or Z of N,
[00:22:37.400 --> 00:22:41.040]   which is the representation coming from the negative.
[00:22:41.040 --> 00:22:45.360]   And there is a weight associated with it.
[00:22:45.360 --> 00:22:48.840]   The weight is proportional to P of IP.
[00:22:48.840 --> 00:22:53.840]   P of IP is nothing but the term we had here.
[00:22:53.840 --> 00:22:56.800]   So this term inside the log,
[00:22:56.800 --> 00:22:59.080]   that is what we call P of IP.
[00:22:59.080 --> 00:23:01.240]   Basically saying what is the likelihood
[00:23:01.240 --> 00:23:04.560]   of the positive coming from the same class
[00:23:04.560 --> 00:23:05.600]   as the anchor I.
[00:23:05.600 --> 00:23:11.520]   So basically what we get down to
[00:23:11.520 --> 00:23:14.120]   is that the gradient looks something like
[00:23:14.120 --> 00:23:16.320]   the embedding multiplied by the probability
[00:23:16.320 --> 00:23:18.440]   and then a correction term.
[00:23:18.440 --> 00:23:20.480]   This correction term for the loss
[00:23:20.480 --> 00:23:22.840]   that we actually end up using is one of P of I,
[00:23:22.840 --> 00:23:24.360]   which says that I will correct
[00:23:24.360 --> 00:23:26.560]   for all of those terms equally.
[00:23:26.560 --> 00:23:28.280]   And the amount by which I will correct
[00:23:28.280 --> 00:23:33.160]   is one by the number of positives I have in the batch.
[00:23:33.160 --> 00:23:37.760]   In the case when the summation is inside the log,
[00:23:37.760 --> 00:23:40.240]   it turns out that this correction term
[00:23:40.240 --> 00:23:43.640]   is a likelihood among all positives.
[00:23:43.640 --> 00:23:45.480]   So for all the positives,
[00:23:45.480 --> 00:23:48.680]   you have a correction term which is larger
[00:23:48.680 --> 00:23:52.800]   for the ones that are closer to the anchor
[00:23:52.800 --> 00:23:56.200]   and smaller for the ones which are far away from the anchor.
[00:23:56.200 --> 00:23:57.920]   This can get into weird issues.
[00:23:57.920 --> 00:24:00.120]   And this is why we believe that the performance
[00:24:00.120 --> 00:24:04.240]   is not actually very good for the summation inside the log.
[00:24:04.240 --> 00:24:08.280]   So what do the gradients look?
[00:24:08.280 --> 00:24:10.280]   We haven't trained any models yet.
[00:24:10.280 --> 00:24:14.680]   We have just looked at what the loss function should look like.
[00:24:14.680 --> 00:24:15.840]   We have done some few experiments.
[00:24:15.840 --> 00:24:17.800]   What do the gradients already tell us?
[00:24:17.800 --> 00:24:19.760]   This tells us definitely that hard negative
[00:24:19.760 --> 00:24:22.280]   and positive mining is already being done by the model.
[00:24:22.280 --> 00:24:23.760]   You don't have to do it manually,
[00:24:23.760 --> 00:24:26.440]   which is a very good thing if you are interested
[00:24:26.440 --> 00:24:30.280]   in implementing and productizing these models.
[00:24:30.280 --> 00:24:32.520]   The second is that everybody talks about
[00:24:32.520 --> 00:24:33.680]   that larger batch sizes help.
[00:24:33.680 --> 00:24:35.760]   Well, larger batch sizes have always helped deep learning
[00:24:35.760 --> 00:24:37.240]   because they stabilize the gradients.
[00:24:37.240 --> 00:24:38.880]   But why do they help contrastive learning
[00:24:38.880 --> 00:24:40.720]   more than cross entropy?
[00:24:40.720 --> 00:24:44.200]   And the answer is that since a large amount of the gradient
[00:24:44.200 --> 00:24:47.640]   is determined by what is this probability P of IP,
[00:24:47.640 --> 00:24:50.600]   if you actually end up getting a hard negative,
[00:24:50.600 --> 00:24:54.760]   or a hard positive, that is gonna have
[00:24:54.760 --> 00:24:57.800]   a very large amount of contribution in the gradient.
[00:24:57.800 --> 00:25:02.240]   So larger batch sizes also increase the probability
[00:25:02.240 --> 00:25:06.240]   that you end up hitting a strong positive
[00:25:06.240 --> 00:25:08.600]   or a strong negative for your given anchor.
[00:25:08.600 --> 00:25:12.640]   And we don't know of any work that is actually
[00:25:12.640 --> 00:25:13.800]   trying to argue it this way.
[00:25:13.800 --> 00:25:16.040]   And we have an analysis in the paper
[00:25:16.040 --> 00:25:18.600]   which actually goes through the procedures of these results
[00:25:18.600 --> 00:25:21.240]   and shows how you come down to this.
[00:25:21.240 --> 00:25:24.080]   And I would encourage people who are interested
[00:25:24.080 --> 00:25:28.320]   to read that and maybe expand on those ideas.
[00:25:28.320 --> 00:25:33.840]   So the most interesting part for a lot of people,
[00:25:33.840 --> 00:25:34.760]   that's the results.
[00:25:34.760 --> 00:25:37.640]   Not for me, I like the theory more.
[00:25:38.640 --> 00:25:43.640]   So we have amazing numbers, I would say.
[00:25:43.640 --> 00:25:46.920]   Firstly, we were able to get state of the art numbers
[00:25:46.920 --> 00:25:50.920]   on ResNet 200 with both cross entropy
[00:25:50.920 --> 00:25:55.760]   trained with auto augment and stack trend augment.
[00:25:55.760 --> 00:25:57.760]   And we were able to get state of the art numbers
[00:25:57.760 --> 00:26:01.440]   on ResNet 50, 101 and 200 using supervised contrastive
[00:26:01.440 --> 00:26:03.880]   trained with various augmentations.
[00:26:03.880 --> 00:26:05.760]   And I'll go into details of that
[00:26:05.760 --> 00:26:07.560]   if anybody's interested.
[00:26:07.560 --> 00:26:10.920]   But overall, the picture looks like this
[00:26:10.920 --> 00:26:12.520]   irrespective of augmentation
[00:26:12.520 --> 00:26:16.040]   and more complicated techniques,
[00:26:16.040 --> 00:26:19.440]   supervised contrastive gets you better performance.
[00:26:19.440 --> 00:26:22.200]   Cut mix here is not exactly a standard augmentation
[00:26:22.200 --> 00:26:24.400]   in the sense that it actually mixes samples.
[00:26:24.400 --> 00:26:27.680]   So it uses label information more meaningfully,
[00:26:27.680 --> 00:26:31.360]   but contrastive learning actually does better than that.
[00:26:31.360 --> 00:26:34.160]   So that was interesting to me
[00:26:34.160 --> 00:26:36.160]   because I was surprised by that result.
[00:26:36.160 --> 00:26:42.760]   We see consistent improvement across models
[00:26:42.760 --> 00:26:43.800]   and across data sets.
[00:26:43.800 --> 00:26:46.120]   So we're better than SimClear, which is unsupervised,
[00:26:46.120 --> 00:26:47.480]   cross entropy, which is supervised,
[00:26:47.480 --> 00:26:49.720]   MaxMartin, which is also supervised
[00:26:49.720 --> 00:26:51.960]   and then supervised contrastive.
[00:26:51.960 --> 00:26:53.080]   It's better than all of them,
[00:26:53.080 --> 00:26:54.760]   for CIFAR-10, 100 and ResNet.
[00:26:54.760 --> 00:27:00.920]   Then this is a more detailed version
[00:27:00.920 --> 00:27:02.200]   of the results that we have.
[00:27:02.200 --> 00:27:06.920]   And so on top five, we have a state of the art number
[00:27:06.920 --> 00:27:10.360]   on ResNet-200 with the augmentation stack RandAugment.
[00:27:10.360 --> 00:27:14.200]   And I can go into details of what this augmentation
[00:27:14.200 --> 00:27:17.320]   really is later, but basically it's just a more
[00:27:17.320 --> 00:27:19.600]   stronger augmentation than RandAugment
[00:27:19.600 --> 00:27:22.080]   with color gettering and blurs.
[00:27:22.080 --> 00:27:25.840]   And we also have similar numbers
[00:27:25.840 --> 00:27:28.120]   using AutoAugment on ResNet-50.
[00:27:28.120 --> 00:27:32.000]   And the key takeaway here is that
[00:27:32.000 --> 00:27:34.320]   if you have a larger architecture
[00:27:34.320 --> 00:27:36.040]   that helps supervised contrastive,
[00:27:36.040 --> 00:27:38.360]   if you have more stronger augmentation
[00:27:38.360 --> 00:27:39.920]   for those larger architectures,
[00:27:39.920 --> 00:27:42.040]   even then it helps supervised contrastive.
[00:27:42.040 --> 00:27:44.400]   Smaller architectures can get into trouble
[00:27:44.400 --> 00:27:45.920]   with stronger augmentations,
[00:27:45.920 --> 00:27:50.120]   but supervised contrastive helps
[00:27:50.120 --> 00:27:53.520]   against cross entropy anyway.
[00:27:53.520 --> 00:27:59.120]   Right, so all of this is on ImageNet.
[00:27:59.120 --> 00:28:01.200]   The question is that the entire point
[00:28:01.200 --> 00:28:04.120]   of doing all of this is that can you actually generalize?
[00:28:04.120 --> 00:28:06.600]   So, and by generalize, I mean,
[00:28:06.600 --> 00:28:09.160]   generalize on different data sets.
[00:28:09.160 --> 00:28:11.360]   So we have this transfer learning benchmark,
[00:28:11.360 --> 00:28:13.120]   which is the VTAP benchmark.
[00:28:13.120 --> 00:28:15.640]   We choose a small number of tasks.
[00:28:15.640 --> 00:28:20.000]   So what we have here is a model that is trained on ImageNet
[00:28:20.000 --> 00:28:22.520]   and then is fine tuned on these specific data sets,
[00:28:22.520 --> 00:28:26.680]   such as CIFAR-10, 100, 4, SUN397, VLC.
[00:28:26.680 --> 00:28:28.480]   And you can look at details of that
[00:28:30.040 --> 00:28:31.560]   in our paper as well.
[00:28:31.560 --> 00:28:36.040]   And the key idea here is that first,
[00:28:36.040 --> 00:28:37.560]   that we are competitive with most
[00:28:37.560 --> 00:28:40.680]   of the state of the art models,
[00:28:40.680 --> 00:28:44.360]   which are cross entropy and Sinclair
[00:28:44.360 --> 00:28:46.800]   on both ResNet-50 and ResNet-200.
[00:28:46.800 --> 00:28:48.400]   But the interesting part is that
[00:28:48.400 --> 00:28:52.280]   permutation significance does not exist
[00:28:52.280 --> 00:28:55.840]   between cross entropy and supervised contrastive.
[00:28:55.840 --> 00:28:58.480]   And our running hypothesis there is that
[00:28:58.480 --> 00:29:01.880]   for classification task, if you're training on ImageNet,
[00:29:01.880 --> 00:29:05.400]   the architecture is actually more important
[00:29:05.400 --> 00:29:07.520]   than the technique with which you're learning,
[00:29:07.520 --> 00:29:09.360]   but we could be wrong about this.
[00:29:09.360 --> 00:29:12.640]   And yeah, we haven't seen any interesting results
[00:29:12.640 --> 00:29:16.640]   which suggests that this is entirely untrue.
[00:29:16.640 --> 00:29:20.760]   And that's after tuning, right?
[00:29:20.760 --> 00:29:23.600]   Without tuning, you could see differences.
[00:29:23.600 --> 00:29:28.600]   But we did see differences on the COCO dataset.
[00:29:28.600 --> 00:29:33.320]   And we do see that supervised contrastive does much better.
[00:29:33.320 --> 00:29:35.720]   And this is like a very recent set of results
[00:29:35.720 --> 00:29:37.800]   that Yonglong has come up with.
[00:29:37.800 --> 00:29:43.920]   And basically we see that compared to other tasks.
[00:29:43.920 --> 00:29:47.560]   So this number is not state of the art on COCO
[00:29:47.560 --> 00:29:50.240]   with like larger, with like other training techniques
[00:29:50.240 --> 00:29:52.240]   actually get much better numbers on COCO,
[00:29:52.240 --> 00:29:55.040]   but this is specifically comparing
[00:29:55.040 --> 00:29:57.040]   to other supervised techniques.
[00:29:57.040 --> 00:29:59.720]   And we do see that supervised contrastive does better.
[00:29:59.720 --> 00:30:02.120]   Right.
[00:30:02.120 --> 00:30:07.120]   We also see that we are actually much more robust
[00:30:07.120 --> 00:30:08.840]   to corruptions in the dataset.
[00:30:08.840 --> 00:30:10.480]   So if you take the ImageNet test set
[00:30:10.480 --> 00:30:13.120]   and you actually corrupt it at various severity levels,
[00:30:13.120 --> 00:30:16.840]   as has been suggested by a work done by Dan Hendricks,
[00:30:16.840 --> 00:30:18.680]   you can construct this ImageNet-C dataset
[00:30:18.680 --> 00:30:20.880]   and we actually measure performance on that.
[00:30:20.880 --> 00:30:24.000]   So in the most important metric, in my opinion,
[00:30:24.000 --> 00:30:26.800]   is relative MC, which measures the relative
[00:30:26.800 --> 00:30:28.920]   drop in performance with higher corruptions
[00:30:28.920 --> 00:30:30.080]   and severity levels.
[00:30:30.080 --> 00:30:32.840]   And we have state of the art results
[00:30:32.840 --> 00:30:35.280]   for ResNet-200 and ResNet-50.
[00:30:35.280 --> 00:30:37.280]   You can also see this curve, which clearly shows
[00:30:37.280 --> 00:30:40.560]   that we are better than cross-entropy.
[00:30:40.560 --> 00:30:43.760]   So the purple curve is ours in both of these cases.
[00:30:43.760 --> 00:30:45.760]   The dotted is ResNet-200, of course.
[00:30:45.760 --> 00:30:50.440]   We also do see that we are much more stable
[00:30:50.440 --> 00:30:51.760]   to hyperparameters.
[00:30:51.760 --> 00:30:54.520]   One of the most important hyperparameters
[00:30:54.520 --> 00:30:55.960]   in contrastive learning is temperature,
[00:30:55.960 --> 00:30:57.040]   which we show here.
[00:30:57.040 --> 00:30:58.760]   But we do see that any temperature choice
[00:30:58.760 --> 00:31:03.760]   between 0.04 to 0.11 is mostly getting us
[00:31:03.760 --> 00:31:06.800]   near state of the art performance.
[00:31:06.800 --> 00:31:09.560]   And we are working on trying to make sure
[00:31:09.560 --> 00:31:12.360]   that you don't have to feel the temperature so much.
[00:31:12.360 --> 00:31:15.320]   Of course, we also see that even at smaller batch size,
[00:31:15.320 --> 00:31:17.240]   supervised contrastive continues to do better
[00:31:17.240 --> 00:31:18.360]   than cross-entropy.
[00:31:18.360 --> 00:31:19.680]   And this was a question that was raised
[00:31:19.680 --> 00:31:21.200]   by a few people that,
[00:31:21.200 --> 00:31:24.680]   do you think that at lower batch sizes is important?
[00:31:24.680 --> 00:31:26.640]   No, the answer is that even at lower batch sizes,
[00:31:26.640 --> 00:31:27.480]   we do better.
[00:31:27.480 --> 00:31:29.520]   And this is like a 256 batch size,
[00:31:29.520 --> 00:31:33.720]   a 5512 batch size, which you don't need TPUs
[00:31:33.720 --> 00:31:36.280]   or anything to train with that.
[00:31:36.280 --> 00:31:41.280]   And we also experimented with various augmentation schemes.
[00:31:41.280 --> 00:31:45.320]   So we have much lesser variation in performance
[00:31:45.320 --> 00:31:46.600]   with augmentation.
[00:31:46.600 --> 00:31:48.120]   We can change all sorts of augmentations.
[00:31:48.120 --> 00:31:50.360]   The augmentation used during pre-training,
[00:31:50.360 --> 00:31:51.320]   in the contrastive stage,
[00:31:51.320 --> 00:31:52.880]   you can change the augmentation used
[00:31:52.880 --> 00:31:55.440]   for training the linear classifier and so on and so forth.
[00:31:55.440 --> 00:32:00.200]   But we do see that the performance doesn't change a lot.
[00:32:00.200 --> 00:32:02.800]   And same thing was for optimizers.
[00:32:02.800 --> 00:32:05.280]   On learning rate, I would say the jury is out.
[00:32:05.280 --> 00:32:06.280]   I am right.
[00:32:06.280 --> 00:32:09.200]   So learning rate changes,
[00:32:09.200 --> 00:32:12.640]   we are susceptible to especially learning rate changes
[00:32:12.640 --> 00:32:15.200]   and for pre-training.
[00:32:15.200 --> 00:32:17.840]   So you do need to train your learning rates
[00:32:17.840 --> 00:32:19.760]   for contrastive learning still.
[00:32:19.760 --> 00:32:22.160]   But the hope is that one day you won't have to.
[00:32:22.160 --> 00:32:24.840]   Right.
[00:32:24.840 --> 00:32:28.600]   And another set of interesting results
[00:32:28.600 --> 00:32:30.360]   is that a small percentage of data,
[00:32:30.360 --> 00:32:32.640]   when you actually, let's say,
[00:32:32.640 --> 00:32:36.360]   take only 1% of your ImageNet training set,
[00:32:36.360 --> 00:32:38.360]   but evaluate on the full test set,
[00:32:38.360 --> 00:32:39.640]   what is your performance?
[00:32:39.640 --> 00:32:41.480]   And we realized that supervised contrastive
[00:32:41.480 --> 00:32:42.480]   does much better.
[00:32:42.480 --> 00:32:44.200]   So this is top five performance.
[00:32:44.560 --> 00:32:48.360]   And we get 57%, which is compared to the 38%
[00:32:48.360 --> 00:32:50.200]   of cross-entropy is just much better.
[00:32:50.200 --> 00:32:52.760]   And that falls throughout the 100.
[00:32:52.760 --> 00:32:57.280]   So it's a log chart on the x-axis for percentage of data.
[00:32:57.280 --> 00:33:00.240]   And we just think that we do better
[00:33:00.240 --> 00:33:04.320]   because we're learning more meaningful representations.
[00:33:04.320 --> 00:33:06.640]   Right.
[00:33:06.640 --> 00:33:10.560]   There are a few downsides to contrastive learning,
[00:33:10.560 --> 00:33:14.400]   specifically for people who productize models,
[00:33:14.400 --> 00:33:16.760]   that it has a long training time.
[00:33:16.760 --> 00:33:18.720]   You need to train for like 350 epochs.
[00:33:18.720 --> 00:33:20.760]   We have often trained for 100,000 epochs as well
[00:33:20.760 --> 00:33:21.600]   to push performance,
[00:33:21.600 --> 00:33:23.320]   even though it doesn't lead to great results.
[00:33:23.320 --> 00:33:24.800]   But you have to do that.
[00:33:24.800 --> 00:33:27.520]   You can mostly get good results with cross-entropy,
[00:33:27.520 --> 00:33:28.400]   which is state of the art,
[00:33:28.400 --> 00:33:31.280]   if you have the right parameters in 100 to 200 epochs.
[00:33:31.280 --> 00:33:34.440]   So we are much longer than that.
[00:33:34.440 --> 00:33:35.920]   Larger batch sizes help us a lot.
[00:33:35.920 --> 00:33:37.680]   So if you want to get state of the art performance,
[00:33:37.680 --> 00:33:39.640]   you want to use large batch sizes.
[00:33:39.640 --> 00:33:42.120]   And that gets problematic
[00:33:42.120 --> 00:33:43.840]   for not just computational reasons,
[00:33:43.840 --> 00:33:47.200]   but sometimes the network size is just too large
[00:33:47.200 --> 00:33:49.840]   because you are working at very high resolution images.
[00:33:49.840 --> 00:33:51.920]   So that's another problem.
[00:33:51.920 --> 00:33:53.920]   And then there is multi-stage training
[00:33:53.920 --> 00:33:55.320]   and you need to decide
[00:33:55.320 --> 00:33:57.280]   if you want to update your batch parameters or not,
[00:33:57.280 --> 00:33:59.400]   and what's the momentum that you want to use.
[00:33:59.400 --> 00:34:01.800]   And it's just another set of parameters that you introduce.
[00:34:01.800 --> 00:34:04.640]   So that's definitely a very big downside
[00:34:04.640 --> 00:34:06.560]   of contrastive learning.
[00:34:06.560 --> 00:34:08.280]   And just very quickly,
[00:34:08.280 --> 00:34:10.720]   the kind of things you're still working on,
[00:34:10.720 --> 00:34:11.560]   bunch of things,
[00:34:11.560 --> 00:34:13.200]   and other people should also try these things,
[00:34:13.200 --> 00:34:15.320]   that's generalized,
[00:34:15.320 --> 00:34:17.280]   the supervision for contrastive learning,
[00:34:17.280 --> 00:34:20.280]   no reason to just use emission labels.
[00:34:20.280 --> 00:34:23.160]   You can also do out-of-distribution detection
[00:34:23.160 --> 00:34:25.120]   and maybe even classification-free
[00:34:25.120 --> 00:34:27.640]   out-of-distribution detection, if that makes sense.
[00:34:27.640 --> 00:34:30.800]   And building stability to small perturbations
[00:34:30.800 --> 00:34:33.000]   and input data, that's something we are working on.
[00:34:33.000 --> 00:34:36.800]   And learning representations from multimodal data,
[00:34:36.800 --> 00:34:39.360]   which is learning similarity
[00:34:39.360 --> 00:34:41.120]   between text representations and images,
[00:34:41.120 --> 00:34:42.720]   text and videos, videos and speech,
[00:34:42.720 --> 00:34:44.520]   all sorts of combination exist.
[00:34:44.520 --> 00:34:46.800]   And people have already started doing this.
[00:34:46.800 --> 00:34:48.320]   I saw a few papers around,
[00:34:48.320 --> 00:34:52.320]   which are coming up since this went up on archive.
[00:34:52.320 --> 00:34:55.840]   So yeah, I think very interesting.
[00:34:55.840 --> 00:34:57.560]   And yeah, of course,
[00:34:57.560 --> 00:34:59.440]   there are other contrastive learning techniques
[00:34:59.440 --> 00:35:01.040]   which have worked really well.
[00:35:01.040 --> 00:35:04.000]   And that is, there's the MoCo paper,
[00:35:04.000 --> 00:35:07.040]   and then there's the DeepMind paper as well,
[00:35:07.040 --> 00:35:09.440]   and BYOL and so on.
[00:35:09.440 --> 00:35:10.520]   Then they have shown that
[00:35:10.520 --> 00:35:12.600]   if you use momentum encoders and memory,
[00:35:12.600 --> 00:35:15.200]   interestingly, then you can work with very small batch sizes
[00:35:15.200 --> 00:35:16.360]   and you can make things work.
[00:35:16.360 --> 00:35:20.240]   And while some of that work has been increasingly focused
[00:35:20.240 --> 00:35:22.680]   at unsupervised or self-supervised,
[00:35:22.680 --> 00:35:25.400]   we do believe that using label information
[00:35:25.400 --> 00:35:26.840]   would give you state-of-the-art performance
[00:35:26.840 --> 00:35:28.040]   when combined with those techniques.
[00:35:28.040 --> 00:35:30.400]   So we are trying a bunch of those things,
[00:35:30.400 --> 00:35:33.240]   but yeah, the people who are definitely
[00:35:33.240 --> 00:35:34.680]   product-testing their models,
[00:35:34.680 --> 00:35:38.040]   they should look at if they wanna try things like that.
[00:35:38.040 --> 00:35:43.120]   Right, so I think I'll stop here,
[00:35:43.120 --> 00:35:47.040]   and I'll just take a bunch of questions.
[00:35:47.040 --> 00:35:52.440]   - Thanks, that was really good, thank you.
[00:35:52.440 --> 00:35:54.880]   I have a question to get us started,
[00:35:54.880 --> 00:35:57.360]   and I'm sure Charles has a bunch too.
[00:35:57.360 --> 00:36:01.640]   So you talked about how your technique
[00:36:01.640 --> 00:36:04.320]   is better with larger batch sizes,
[00:36:04.320 --> 00:36:07.880]   but also most people don't have the compute resources
[00:36:07.880 --> 00:36:11.440]   if they have a big model to use large batch sizes.
[00:36:11.440 --> 00:36:14.080]   What's your advice for people to use
[00:36:14.080 --> 00:36:17.880]   self-supervised, like supervised contrasting learning
[00:36:17.880 --> 00:36:19.400]   in that sense?
[00:36:19.400 --> 00:36:23.000]   - Yeah, I think memory is definitely a trend.
[00:36:23.000 --> 00:36:24.840]   So if you can actually keep,
[00:36:24.840 --> 00:36:27.320]   so the way memory works is it keeps
[00:36:27.320 --> 00:36:29.640]   a few scale representations.
[00:36:29.640 --> 00:36:33.800]   - Sorry, I think your audio is being a little bit.
[00:36:33.800 --> 00:36:36.880]   - Can you hear me now?
[00:36:36.880 --> 00:36:38.480]   - It's like breaking like that.
[00:36:38.480 --> 00:36:43.200]   Kayla, do you have any advice on how to fix it?
[00:36:43.200 --> 00:36:45.800]   - I don't, maybe just pause your video for a second.
[00:36:45.800 --> 00:36:47.560]   It might just be a bandwidth issue.
[00:36:47.560 --> 00:36:49.440]   - Yes.
[00:36:49.440 --> 00:36:51.920]   - Sorry about that.
[00:36:51.920 --> 00:36:55.000]   - It's okay, can you hear me better?
[00:36:55.000 --> 00:36:57.160]   - Better, yes.
[00:36:58.280 --> 00:37:01.280]   - Okay, let's just try this for a couple of minutes.
[00:37:01.280 --> 00:37:07.040]   Okay, so basically what I was saying was that
[00:37:07.040 --> 00:37:09.080]   memory is your friend there because what memory
[00:37:09.080 --> 00:37:11.720]   allows you to do is keep a few scale representations
[00:37:11.720 --> 00:37:13.720]   around and you can contrast against them.
[00:37:13.720 --> 00:37:17.000]   So that's like a pseudo larger batch.
[00:37:17.000 --> 00:37:19.040]   The main problem with that is that since
[00:37:19.040 --> 00:37:21.640]   your representations are still,
[00:37:21.640 --> 00:37:23.680]   when you're in the early stages of training
[00:37:23.680 --> 00:37:26.840]   with large learning rates, you can be really out of sync.
[00:37:26.840 --> 00:37:28.600]   But there are trade-offs that you can come up with.
[00:37:28.600 --> 00:37:30.240]   So memory is definitely a friend.
[00:37:30.240 --> 00:37:36.320]   And other, there are, I would say that
[00:37:36.320 --> 00:37:41.640]   people have even tried keeping the entire data sets
[00:37:41.640 --> 00:37:43.280]   representations in memory.
[00:37:43.280 --> 00:37:45.920]   And that is doable for, because,
[00:37:45.920 --> 00:37:48.480]   and that's the reason why the representation
[00:37:48.480 --> 00:37:50.040]   over which you apply the contrastive loss
[00:37:50.040 --> 00:37:51.920]   is like 128 or 256.
[00:37:51.920 --> 00:37:53.320]   So that optimizes your memory.
[00:37:53.320 --> 00:37:56.560]   So that's one good reason to actually keep those models.
[00:37:56.560 --> 00:37:57.680]   Yeah.
[00:37:57.680 --> 00:37:58.680]   - Cool.
[00:37:58.680 --> 00:38:00.760]   And if people can start dropping their questions
[00:38:00.760 --> 00:38:04.400]   in the chat and also the folks on YouTube,
[00:38:04.400 --> 00:38:07.880]   I know you're like 30 seconds to a minute behind.
[00:38:07.880 --> 00:38:09.680]   So just, if I wasn't here,
[00:38:09.680 --> 00:38:12.680]   please start dropping your questions in the chat.
[00:38:12.680 --> 00:38:14.800]   Charles, you had a question.
[00:38:14.800 --> 00:38:15.760]   - Yeah.
[00:38:15.760 --> 00:38:19.320]   So really cool talk, really interesting stuff.
[00:38:19.320 --> 00:38:21.320]   The contrastive learning approach is one
[00:38:21.320 --> 00:38:22.560]   that I'm really excited by.
[00:38:22.560 --> 00:38:24.560]   And it's cool to see them,
[00:38:24.560 --> 00:38:27.520]   like people continuing to push the envelope there.
[00:38:27.520 --> 00:38:31.080]   So first you mentioned that just for general contrast
[00:38:31.080 --> 00:38:33.160]   of learning, there's this interpretation
[00:38:33.160 --> 00:38:36.720]   of maximizing mutual information between views of the data.
[00:38:36.720 --> 00:38:38.680]   So just to make sure that I understood that right,
[00:38:38.680 --> 00:38:41.000]   you mean maximizing the mutual information
[00:38:41.000 --> 00:38:44.280]   between your latent representations, right?
[00:38:44.280 --> 00:38:45.120]   - Yeah.
[00:38:45.120 --> 00:38:48.680]   - And you achieve that basically by pushing those together
[00:38:48.680 --> 00:38:52.480]   and then pushing the other ones apart.
[00:38:52.480 --> 00:38:53.320]   - Yeah.
[00:38:53.320 --> 00:38:56.360]   - And these like, whatever your negative examples are.
[00:38:56.360 --> 00:39:00.680]   So there's like, in my mind that immediately brings up
[00:39:00.680 --> 00:39:05.240]   these like positive negative sample algorithms
[00:39:05.240 --> 00:39:09.240]   that people use for like wake sleep algorithms, right?
[00:39:09.240 --> 00:39:14.080]   The way people train maximum likelihood models
[00:39:14.080 --> 00:39:18.200]   like Boltzmann machines and things like that.
[00:39:18.200 --> 00:39:20.440]   So is that more than an analogy?
[00:39:20.440 --> 00:39:22.200]   Is there a direct connection there?
[00:39:22.200 --> 00:39:23.880]   I wasn't able to figure that out.
[00:39:23.880 --> 00:39:28.920]   - Yeah, we haven't figured that out as well.
[00:39:28.920 --> 00:39:31.640]   Like I wouldn't say that there is a direct system there,
[00:39:31.640 --> 00:39:32.760]   but yes.
[00:39:32.760 --> 00:39:37.240]   So more intelligently sampling your positive and negative
[00:39:37.240 --> 00:39:41.720]   and actually making sure that your data views
[00:39:41.720 --> 00:39:43.400]   are constructed intelligently.
[00:39:43.400 --> 00:39:47.120]   By that I mean that you make sure that
[00:39:47.120 --> 00:39:49.840]   there is not a very large amount of mutual information
[00:39:49.840 --> 00:39:53.880]   between those views so that when you actually maximize
[00:39:53.880 --> 00:39:55.480]   the mutual information between those views,
[00:39:55.480 --> 00:39:59.840]   you actually end up retaining only the semantic information.
[00:39:59.840 --> 00:40:02.560]   That is definitely a very strong part
[00:40:02.560 --> 00:40:03.880]   of contrasted learning.
[00:40:03.880 --> 00:40:06.960]   And we do think that pushing on that helps
[00:40:06.960 --> 00:40:09.480]   even though random data augmentation
[00:40:09.480 --> 00:40:11.960]   is like a close proxy for that.
[00:40:11.960 --> 00:40:14.760]   There is more work to be done on that front, I believe.
[00:40:17.640 --> 00:40:22.120]   - The other question that I wanted to ask was
[00:40:22.120 --> 00:40:27.600]   outside of the image setting,
[00:40:27.600 --> 00:40:29.880]   do you expect this contrast?
[00:40:29.880 --> 00:40:31.480]   Like how well do you expect these kinds of
[00:40:31.480 --> 00:40:33.560]   contrastive learning approaches to work?
[00:40:33.560 --> 00:40:34.400]   For two reasons.
[00:40:34.400 --> 00:40:39.160]   One is a lot of image classification tasks
[00:40:39.160 --> 00:40:40.960]   basically involve these kinds of like texture
[00:40:40.960 --> 00:40:42.640]   and pattern matching things
[00:40:42.640 --> 00:40:44.560]   that maybe don't transfer super well
[00:40:44.560 --> 00:40:47.960]   when it comes time to do segmentation or depth mapping
[00:40:47.960 --> 00:40:50.320]   or something else with images.
[00:40:50.320 --> 00:40:53.560]   And then second, outside of like using images
[00:40:53.560 --> 00:40:54.800]   for your task at all,
[00:40:54.800 --> 00:40:57.080]   we're relatively limited in the kinds of augmentations
[00:40:57.080 --> 00:41:00.240]   we can apply to something like natural language processing
[00:41:00.240 --> 00:41:01.960]   where it's somewhat difficult to come up
[00:41:01.960 --> 00:41:03.440]   with a reasonable data augmentation
[00:41:03.440 --> 00:41:06.040]   that maintains semantic meaning.
[00:41:06.040 --> 00:41:09.320]   So like I don't know the literature very well on this.
[00:41:09.320 --> 00:41:11.360]   So like, do you know examples
[00:41:11.360 --> 00:41:12.680]   where people have done this before?
[00:41:12.680 --> 00:41:15.480]   And do you think, is your intuition
[00:41:15.480 --> 00:41:18.960]   that this will spread really nicely to these other domains?
[00:41:18.960 --> 00:41:23.760]   - Yeah, so segmentation is definitely something that
[00:41:23.760 --> 00:41:26.560]   the same backbone that it can be used for classification
[00:41:26.560 --> 00:41:27.760]   can be used for segmentation.
[00:41:27.760 --> 00:41:30.040]   So segmentation is something that can be handled.
[00:41:30.040 --> 00:41:32.920]   Other ones, it's not so clear.
[00:41:32.920 --> 00:41:34.680]   People haven't done these things.
[00:41:34.680 --> 00:41:37.120]   But moving to non-image data,
[00:41:37.120 --> 00:41:38.400]   text is definitely hard
[00:41:38.400 --> 00:41:40.280]   because data augmentations are difficult.
[00:41:40.280 --> 00:41:43.000]   But you can come up with sampling procedures
[00:41:43.000 --> 00:41:44.800]   which construct your batch interestingly.
[00:41:44.800 --> 00:41:46.520]   So you don't have to do any data augmentation
[00:41:46.520 --> 00:41:48.280]   because technically, see,
[00:41:48.280 --> 00:41:50.280]   if you don't have any data augmentation,
[00:41:50.280 --> 00:41:52.200]   even then contrastive learning does work
[00:41:52.200 --> 00:41:53.800]   because it is still pushing samples
[00:41:53.800 --> 00:41:58.040]   from the same class together and other class apart.
[00:41:58.040 --> 00:42:03.040]   And that, so for text, intelligent sampling helps.
[00:42:03.040 --> 00:42:07.160]   Audio is a signal.
[00:42:07.160 --> 00:42:09.560]   You can perform very interesting data augmentations
[00:42:09.560 --> 00:42:10.400]   on audio signals.
[00:42:10.400 --> 00:42:12.760]   So I'm not that worried about audio.
[00:42:12.760 --> 00:42:15.720]   There has been work which actually is like basically
[00:42:15.720 --> 00:42:17.240]   the equivalent of image work,
[00:42:17.240 --> 00:42:20.240]   which shows that you can actually learn similarities
[00:42:20.240 --> 00:42:21.440]   between text and images.
[00:42:21.440 --> 00:42:25.320]   And there they do use image augmentations.
[00:42:25.320 --> 00:42:28.960]   So I do think that contrastive learning
[00:42:28.960 --> 00:42:31.600]   would be able to extend to such settings.
[00:42:31.600 --> 00:42:36.360]   - Yeah, so you mentioned, yeah, so text classification,
[00:42:36.360 --> 00:42:39.040]   you still have this nice class label
[00:42:39.040 --> 00:42:41.800]   that helps you figure out which things are similar
[00:42:41.800 --> 00:42:42.840]   and which things are not.
[00:42:42.840 --> 00:42:45.720]   But maybe something like machine translation,
[00:42:45.720 --> 00:42:49.280]   it might be difficult unless you have access to say
[00:42:49.280 --> 00:42:52.400]   multiple translations of the same sentence
[00:42:52.400 --> 00:42:56.720]   that would allow you to maybe do data augmentation that way.
[00:42:56.720 --> 00:42:59.200]   - Yeah, no, for sure, yeah.
[00:42:59.200 --> 00:43:00.600]   For machine translation,
[00:43:00.600 --> 00:43:03.560]   it's not very clear how to make that work.
[00:43:03.560 --> 00:43:05.840]   But yeah, I mean, some people have the perspective
[00:43:05.840 --> 00:43:06.800]   that why do it?
[00:43:06.800 --> 00:43:08.680]   Because basically already what we have
[00:43:08.680 --> 00:43:09.960]   that transformers already work.
[00:43:09.960 --> 00:43:11.200]   So, but yeah, I mean,
[00:43:11.200 --> 00:43:14.520]   I don't have a very good answer for that.
[00:43:14.520 --> 00:43:21.240]   - Someone in the comments, oh, Charles, are you done?
[00:43:21.240 --> 00:43:22.080]   - Yeah, yeah.
[00:43:22.080 --> 00:43:24.560]   - You can keep going.
[00:43:24.560 --> 00:43:28.600]   Someone in the comments asked,
[00:43:28.600 --> 00:43:30.760]   can you apply this to deepfakes?
[00:43:30.760 --> 00:43:32.280]   'Cause they're working on deepfakes
[00:43:32.280 --> 00:43:34.920]   and excited about trying this technique.
[00:43:35.920 --> 00:43:40.280]   - I do think that it was a try, but we haven't done it.
[00:43:40.280 --> 00:43:41.160]   So I won't say yes,
[00:43:41.160 --> 00:43:44.560]   because I don't want to be held liable for that.
[00:43:44.560 --> 00:43:47.360]   Right.
[00:43:47.360 --> 00:43:51.920]   Statistical regularities and irregularities
[00:43:51.920 --> 00:43:55.160]   is not something that I expect contrasted learning models
[00:43:55.160 --> 00:43:56.000]   to be very good at
[00:43:56.000 --> 00:43:57.840]   because strong data augmentation
[00:43:57.840 --> 00:43:59.320]   actually normalized for that.
[00:43:59.320 --> 00:44:02.520]   So I would actually expect that they would just learn
[00:44:02.520 --> 00:44:05.440]   to ignore a lot of statistical irregularities.
[00:44:05.440 --> 00:44:09.560]   And if deepfakes are actually using
[00:44:09.560 --> 00:44:11.640]   those statistical irregularities,
[00:44:11.640 --> 00:44:15.840]   then it's possible that we're not able to handle that.
[00:44:15.840 --> 00:44:19.400]   And yeah, so I wouldn't,
[00:44:19.400 --> 00:44:22.160]   I won't say for sure that it would work.
[00:44:22.160 --> 00:44:25.360]   - All right, someone else.
[00:44:25.360 --> 00:44:28.760]   - I did have actually another question.
[00:44:28.760 --> 00:44:29.600]   - Yes.
[00:44:29.640 --> 00:44:33.840]   - Yeah, so are you familiar with slow feature learning?
[00:44:33.840 --> 00:44:37.200]   It's like a pre, it's sort of a pre-deep learning approach.
[00:44:37.200 --> 00:44:43.200]   The idea is if you have like video data
[00:44:43.200 --> 00:44:44.800]   and you want to learn,
[00:44:44.800 --> 00:44:50.480]   and you want to learn, like basically do image pre-training,
[00:44:50.480 --> 00:44:55.240]   what you do is you learn representations
[00:44:55.240 --> 00:44:59.920]   such that the features don't change from frame to frame,
[00:44:59.920 --> 00:45:02.080]   or they change as slowly as possible from frame to frame.
[00:45:02.080 --> 00:45:05.600]   So you try, you have a term to keep the entropy non-zero,
[00:45:05.600 --> 00:45:06.440]   right? - Right.
[00:45:06.440 --> 00:45:10.280]   - But then you also have a term to keep the covariance high.
[00:45:10.280 --> 00:45:12.280]   So this slow feature analysis idea
[00:45:12.280 --> 00:45:17.280]   has been used by a couple of deep learning type approaches,
[00:45:17.280 --> 00:45:20.640]   but it seems like a version of like contrastive learning
[00:45:20.640 --> 00:45:21.880]   without the contrast, right?
[00:45:21.880 --> 00:45:26.040]   It's basically just positive samples,
[00:45:26.040 --> 00:45:29.160]   but it's, yeah, there might be some interesting ideas
[00:45:29.160 --> 00:45:33.360]   in that slow feature analysis world
[00:45:33.360 --> 00:45:35.520]   for contrastive learning.
[00:45:35.520 --> 00:45:37.000]   - Yeah, I think I agree with that.
[00:45:37.000 --> 00:45:38.960]   I do think that there is a very big space
[00:45:38.960 --> 00:45:42.960]   for contrastive learning where what you do
[00:45:42.960 --> 00:45:44.520]   is you take video data
[00:45:44.520 --> 00:45:46.720]   and you have this graded notion of labels.
[00:45:46.720 --> 00:45:48.600]   We have a graded notion of the similarity
[00:45:48.600 --> 00:45:51.480]   and then you try to optimize for that.
[00:45:51.480 --> 00:45:53.080]   So you can basically have a weighted form
[00:45:53.080 --> 00:45:54.080]   of the contrastive learning.
[00:45:54.080 --> 00:45:57.080]   And I think people are trying this.
[00:45:57.080 --> 00:45:59.800]   I know of a few teams who are trying this.
[00:45:59.800 --> 00:46:03.960]   So, and the hope would be to see some slow moving
[00:46:03.960 --> 00:46:05.280]   and some fast moving features,
[00:46:05.280 --> 00:46:06.560]   which automatically come up
[00:46:06.560 --> 00:46:08.680]   because of the structure of the loss.
[00:46:08.680 --> 00:46:13.720]   - Cool.
[00:46:13.720 --> 00:46:16.160]   Someone else in the comments asked,
[00:46:16.160 --> 00:46:18.600]   could you talk more about the augmentation techniques
[00:46:18.600 --> 00:46:19.440]   that you tried
[00:46:19.440 --> 00:46:22.320]   and what makes supervised contrastive learning
[00:46:22.320 --> 00:46:24.680]   stable to these applications?
[00:46:24.680 --> 00:46:26.400]   More so than other.
[00:46:26.400 --> 00:46:27.240]   - Right.
[00:46:27.240 --> 00:46:30.560]   Yeah, let me try and pull out something.
[00:46:30.560 --> 00:46:31.400]   Okay.
[00:46:31.400 --> 00:46:36.320]   So basically the, broadly the augmentations we cared about
[00:46:36.320 --> 00:46:40.200]   was auto augment, RAND augment,
[00:46:40.200 --> 00:46:44.600]   stacked RAND augment and SIM augment.
[00:46:44.600 --> 00:46:46.000]   So going through them,
[00:46:46.000 --> 00:46:48.800]   auto augment is basically the strategy which is trained.
[00:46:48.800 --> 00:46:51.600]   So you have to apply two augmentations
[00:46:51.600 --> 00:46:53.840]   and you decide the magnitude of the augmentation
[00:46:53.840 --> 00:46:55.400]   and the order of that augmentation
[00:46:55.400 --> 00:46:57.480]   using a policy-driven learning algorithm
[00:46:57.480 --> 00:47:00.600]   which optimizes for the top one accuracy of ImageNet.
[00:47:00.600 --> 00:47:04.440]   RAND augment works slightly differently.
[00:47:04.440 --> 00:47:06.240]   You have a controllable parameter
[00:47:06.240 --> 00:47:09.320]   and then you sample your pair of augmentations
[00:47:09.320 --> 00:47:11.040]   that you want to apply randomly.
[00:47:11.040 --> 00:47:15.280]   And the idea is that if you tune this parameter,
[00:47:15.280 --> 00:47:17.520]   which is like the magnitude of the augmentation correctly,
[00:47:17.520 --> 00:47:19.280]   you actually will beat auto augment.
[00:47:19.280 --> 00:47:23.280]   So basically the idea is that even if you learn a policy,
[00:47:23.280 --> 00:47:25.400]   you can just actually randomize it and get the same thing.
[00:47:25.400 --> 00:47:26.480]   So if you don't have compute,
[00:47:26.480 --> 00:47:28.120]   you should just use RAND augment.
[00:47:28.120 --> 00:47:32.440]   Then what we did with stacked RAND augment
[00:47:32.440 --> 00:47:34.080]   was that we actually,
[00:47:34.080 --> 00:47:37.240]   and this was Donglong's contribution
[00:47:37.240 --> 00:47:40.400]   that he basically figured out that if you take
[00:47:40.400 --> 00:47:44.360]   Gaussian blur and color jitters
[00:47:44.360 --> 00:47:47.600]   and after applying color jittering,
[00:47:47.600 --> 00:47:48.840]   before applying color jittering,
[00:47:48.840 --> 00:47:50.200]   you actually do a RAND augment.
[00:47:50.200 --> 00:47:52.000]   So before you actually apply the color jitter,
[00:47:52.000 --> 00:47:53.080]   you actually do RAND augment,
[00:47:53.080 --> 00:47:53.920]   then you do a color jitter
[00:47:53.920 --> 00:47:55.480]   and then you do a Gaussian blur.
[00:47:55.480 --> 00:48:00.600]   But this RAND augment is applied after doing the cropping.
[00:48:00.600 --> 00:48:03.680]   So the order basically goes cropping, RAND augment,
[00:48:03.680 --> 00:48:05.160]   color jitter, Gaussian blur.
[00:48:05.160 --> 00:48:07.200]   Then this is a much stronger augmentation
[00:48:07.200 --> 00:48:09.320]   and much more useful for contrasted learning.
[00:48:09.320 --> 00:48:12.040]   So that's basically what the stacked RAND augment is.
[00:48:12.040 --> 00:48:14.600]   And SIM augment is basically something that
[00:48:14.600 --> 00:48:17.080]   the Hinton paper introduced.
[00:48:17.080 --> 00:48:20.960]   And what they did was color jittering,
[00:48:20.960 --> 00:48:24.280]   Gaussian blur, random cropping flips.
[00:48:24.280 --> 00:48:27.680]   And we had added something else on top,
[00:48:27.680 --> 00:48:28.680]   which was warping,
[00:48:28.680 --> 00:48:31.840]   which actually just shows four points
[00:48:31.840 --> 00:48:34.520]   and actually just warped the image.
[00:48:34.520 --> 00:48:37.640]   So the second part of the question was that,
[00:48:37.640 --> 00:48:38.880]   I don't know if I got this correctly,
[00:48:38.880 --> 00:48:41.560]   that why do these augmentations work well
[00:48:41.560 --> 00:48:42.640]   for contrasted learning?
[00:48:42.640 --> 00:48:43.480]   And right?
[00:48:43.480 --> 00:48:46.080]   - Yeah, yeah, essentially.
[00:48:46.080 --> 00:48:50.320]   - Yeah, so the idea basically there is that
[00:48:50.320 --> 00:48:55.440]   augmentations are supposed to spread
[00:48:55.440 --> 00:48:57.680]   a single data point in the representation space.
[00:48:57.680 --> 00:48:59.640]   So what that allows you to do is like,
[00:48:59.640 --> 00:49:02.880]   you have now have like a denser epsilon ball
[00:49:02.880 --> 00:49:05.480]   in a high dimensional space for every point.
[00:49:05.480 --> 00:49:08.280]   So now this contrasted learning setup,
[00:49:08.280 --> 00:49:09.760]   what it allows you to do is not just that
[00:49:09.760 --> 00:49:11.240]   you're pushing away single points,
[00:49:11.240 --> 00:49:14.080]   you're actually pushing away this density.
[00:49:14.080 --> 00:49:17.480]   And that is more meaningful.
[00:49:17.480 --> 00:49:20.760]   So that's one good reason to use augmentations.
[00:49:20.760 --> 00:49:21.840]   The question is that,
[00:49:21.840 --> 00:49:24.960]   why would one augmentation work better than the other?
[00:49:24.960 --> 00:49:28.320]   The answer is not clear.
[00:49:28.320 --> 00:49:30.840]   We do think that denser is better.
[00:49:30.840 --> 00:49:33.280]   So basically it's like this in this 2K space,
[00:49:33.280 --> 00:49:35.600]   how much can you explore using your augmentations?
[00:49:35.600 --> 00:49:38.480]   So the more orthogonal systems you have
[00:49:38.480 --> 00:49:40.200]   in your augmentations,
[00:49:40.200 --> 00:49:42.240]   the denser this epsilon ball is.
[00:49:42.240 --> 00:49:46.800]   But this is basically an analysis
[00:49:46.800 --> 00:49:49.400]   and it's not very grounded.
[00:49:49.400 --> 00:49:51.440]   Like we don't have a lot of experiments to back this up,
[00:49:51.440 --> 00:49:56.200]   but this is more intuitive in the geometrical space.
[00:49:56.200 --> 00:49:58.560]   - Gotcha.
[00:49:58.560 --> 00:50:01.440]   And is this also related to why
[00:50:01.440 --> 00:50:05.120]   supervised contrastive learning is stable to corruptions
[00:50:05.120 --> 00:50:07.880]   in the ImageNet dataset and to hyperparameters
[00:50:07.880 --> 00:50:09.640]   like temperature and stuff?
[00:50:09.640 --> 00:50:10.560]   - Right, yeah.
[00:50:10.560 --> 00:50:12.440]   So we do believe that yes,
[00:50:12.440 --> 00:50:14.800]   since we're doing a much more meaningful
[00:50:14.800 --> 00:50:16.560]   representation learning system.
[00:50:16.560 --> 00:50:17.600]   So like, okay.
[00:50:17.600 --> 00:50:20.280]   So the point is that even if you use the same augmentation
[00:50:20.280 --> 00:50:21.880]   for cross entropy and supervised contrast,
[00:50:21.880 --> 00:50:23.480]   supervised contrast is still better.
[00:50:23.480 --> 00:50:25.440]   So that means that it's not just the augmentation,
[00:50:25.440 --> 00:50:28.120]   but it is the fact that you're never able to pick up
[00:50:28.120 --> 00:50:32.000]   on spurious correlations between your labels
[00:50:32.000 --> 00:50:33.840]   and your representations.
[00:50:33.840 --> 00:50:35.560]   And that's just like a property
[00:50:35.560 --> 00:50:36.680]   of the cross entropy function.
[00:50:36.680 --> 00:50:38.280]   That the cross entropy function is gonna try
[00:50:38.280 --> 00:50:39.880]   and best to minimize the loss, right?
[00:50:39.880 --> 00:50:42.600]   So it will try to pick up spurious correlations,
[00:50:42.600 --> 00:50:44.400]   but since you have this dense ball
[00:50:44.400 --> 00:50:45.920]   and you're pushing these dense balls around,
[00:50:45.920 --> 00:50:47.400]   there's no notion of why you should pick up
[00:50:47.400 --> 00:50:48.720]   any spurious correlations.
[00:50:48.720 --> 00:50:53.720]   Because it's not just about projecting this ball
[00:50:53.720 --> 00:50:56.240]   using a linear plane into the right class.
[00:50:56.240 --> 00:50:58.520]   Now it's about actually pushing away
[00:50:58.520 --> 00:51:02.000]   these dense balls and the two case space.
[00:51:02.000 --> 00:51:04.480]   - Interesting, cool.
[00:51:05.960 --> 00:51:08.560]   I think those are all the questions that I saw.
[00:51:08.560 --> 00:51:11.120]   Charles, do you have, oh wait, there's more.
[00:51:11.120 --> 00:51:14.320]   Interesting.
[00:51:14.320 --> 00:51:20.280]   Someone said they wanna start with AI and machine learning.
[00:51:20.280 --> 00:51:23.440]   They have a strong background in Python.
[00:51:23.440 --> 00:51:27.200]   At what stage should they be taking interest
[00:51:27.200 --> 00:51:29.440]   in subjects like contrastive learning?
[00:51:29.440 --> 00:51:31.000]   What should be their path?
[00:51:31.000 --> 00:51:32.800]   This is an important question I feel like
[00:51:32.800 --> 00:51:34.440]   'cause all these Coursera
[00:51:35.440 --> 00:51:38.000]   and Udacity courses do such a good job
[00:51:38.000 --> 00:51:40.200]   of teaching you the fundamentals.
[00:51:40.200 --> 00:51:44.600]   So how do you go from there to picking up interest
[00:51:44.600 --> 00:51:47.200]   in something specific like you?
[00:51:47.200 --> 00:51:51.160]   - Yeah, okay, I can describe the way I did things.
[00:51:51.160 --> 00:51:57.280]   So Ian Goodfellow's book is I think a very good place.
[00:51:57.280 --> 00:52:00.280]   His book, it's available online.
[00:52:00.280 --> 00:52:04.200]   You can just read that book and build your intuition.
[00:52:04.200 --> 00:52:06.680]   And then just start reading papers.
[00:52:06.680 --> 00:52:09.720]   So like, but yeah, you need to re-implement things
[00:52:09.720 --> 00:52:10.560]   on your own.
[00:52:10.560 --> 00:52:13.240]   So you need to have like a goal in mind that,
[00:52:13.240 --> 00:52:15.400]   okay, you want to maybe re-implement
[00:52:15.400 --> 00:52:16.680]   very difficult GAN paper
[00:52:16.680 --> 00:52:18.280]   or contrastive learning paper or something
[00:52:18.280 --> 00:52:20.560]   and then read through books and papers
[00:52:20.560 --> 00:52:23.000]   to get to a stage where you have the end-to-end understanding
[00:52:23.000 --> 00:52:26.080]   of each step and why you're doing it.
[00:52:26.080 --> 00:52:30.040]   - Gotcha, very expensive.
[00:52:30.040 --> 00:52:30.880]   Cool.
[00:52:32.000 --> 00:52:35.240]   I feel like we've gotten a lot more over time
[00:52:35.240 --> 00:52:36.760]   but the questions are amazing.
[00:52:36.760 --> 00:52:38.040]   So I kept going.
[00:52:38.040 --> 00:52:40.440]   Thank you so much for coming.
[00:52:40.440 --> 00:52:43.080]   If you want, you should totally do an AMA
[00:52:43.080 --> 00:52:44.600]   in our Slack community
[00:52:44.600 --> 00:52:46.800]   and you'll get a lot more questions there.
[00:52:46.800 --> 00:52:48.960]   And you can ask us.
[00:52:48.960 --> 00:52:51.120]   Thank you so much for, oh wait,
[00:52:51.120 --> 00:52:54.200]   someone has another question, but we'll get to them later.
[00:52:54.200 --> 00:52:57.120]   Yeah, thank you so much for coming.
[00:52:57.120 --> 00:52:58.160]   - No problem, yeah.
[00:52:58.160 --> 00:53:00.120]   I'm just gonna stop sharing my screen now.
[00:53:00.120 --> 00:53:00.960]   - Cool.
[00:53:01.760 --> 00:53:02.600]   All right.
[00:53:02.600 --> 00:53:07.160]   So next up we have one of our regulars
[00:53:07.160 --> 00:53:10.360]   and Charles is a deep learning engineer
[00:53:10.360 --> 00:53:12.760]   and instructor at Weights and Biases.
[00:53:12.760 --> 00:53:14.800]   He's one of my favorite colleagues
[00:53:14.800 --> 00:53:17.920]   and he is gonna talk about alligator peers
[00:53:17.920 --> 00:53:19.960]   and random variables.
[00:53:19.960 --> 00:53:21.280]   Charles, welcome.
[00:53:21.280 --> 00:53:22.120]   - Great.
[00:53:22.120 --> 00:53:27.000]   Yeah, so thanks for the kind introduction, Lavanya.
[00:53:27.000 --> 00:53:31.480]   So I added a little bit to this talk at the end.
[00:53:31.480 --> 00:53:34.800]   So it's not just alligator pairs and random variables,
[00:53:34.800 --> 00:53:37.640]   but also stochastic gradient descent.
[00:53:37.640 --> 00:53:40.600]   This is a collection of things
[00:53:40.600 --> 00:53:43.640]   whose names do not actually describe
[00:53:43.640 --> 00:53:45.920]   the thing that they claim to describe.
[00:53:45.920 --> 00:53:50.960]   So I want you to imagine what you picture in your head
[00:53:50.960 --> 00:53:53.440]   when you hear the phrase alligator pair.
[00:53:53.440 --> 00:53:56.480]   And actually before I go into this talk,
[00:53:56.480 --> 00:53:57.680]   just to be clear,
[00:53:57.760 --> 00:54:02.240]   so this salon, we bring in people to talk about research.
[00:54:02.240 --> 00:54:04.240]   And then in addition,
[00:54:04.240 --> 00:54:08.000]   I spent a little bit of time in each salon
[00:54:08.000 --> 00:54:10.680]   talking about some of the fundamental mathematical ideas
[00:54:10.680 --> 00:54:11.960]   that go into machine learning
[00:54:11.960 --> 00:54:16.120]   and presenting sort of intuitions and ideas around them
[00:54:16.120 --> 00:54:18.520]   to sort of give something for people
[00:54:18.520 --> 00:54:20.040]   who are just starting out on their journey
[00:54:20.040 --> 00:54:20.880]   into machine learning
[00:54:20.880 --> 00:54:23.400]   and then also something for the experts.
[00:54:23.400 --> 00:54:26.200]   So the,
[00:54:26.200 --> 00:54:32.200]   so hopefully you've taken the time
[00:54:32.200 --> 00:54:33.040]   while I was explaining that
[00:54:33.040 --> 00:54:35.360]   to picture an alligator pair in your head.
[00:54:35.360 --> 00:54:37.840]   Perhaps you were picturing something that was,
[00:54:37.840 --> 00:54:42.840]   the shape of a pair that was covered in alligator scales
[00:54:42.840 --> 00:54:46.000]   or a pair that was eaten by alligators
[00:54:46.000 --> 00:54:47.160]   or something like that.
[00:54:47.160 --> 00:54:50.880]   So an alligator pair is not any of those things.
[00:54:50.880 --> 00:54:53.840]   An alligator pair is an avocado,
[00:54:53.840 --> 00:54:55.760]   which is neither an alligator
[00:54:55.760 --> 00:54:57.960]   or related to alligators in any way,
[00:54:57.960 --> 00:54:59.240]   nor is it a pair.
[00:54:59.240 --> 00:55:01.320]   It's very, very different from a pair.
[00:55:01.320 --> 00:55:05.120]   So I've got helpful pictures and diagrams
[00:55:05.120 --> 00:55:06.720]   so that in case you don't recall
[00:55:06.720 --> 00:55:09.360]   what alligators, pairs and avocados look like,
[00:55:09.360 --> 00:55:12.400]   but you can see that there's like some slight resemblance
[00:55:12.400 --> 00:55:14.240]   between these things.
[00:55:14.240 --> 00:55:16.440]   There's no inherent connection.
[00:55:18.480 --> 00:55:23.280]   Now I'd like you to picture something
[00:55:23.280 --> 00:55:25.960]   corresponding to the phrase white ant.
[00:55:25.960 --> 00:55:28.040]   So white ant,
[00:55:28.040 --> 00:55:32.440]   which you've got in mind is probably an ant
[00:55:32.440 --> 00:55:34.280]   that is the color white.
[00:55:34.280 --> 00:55:36.920]   And that's actually very reasonable,
[00:55:36.920 --> 00:55:41.560]   but a white ant is neither white nor in fact an ant.
[00:55:41.560 --> 00:55:44.960]   So that's a picture of a white ant on the right side.
[00:55:44.960 --> 00:55:47.280]   It's a termite and it is brown.
[00:55:48.400 --> 00:55:52.520]   It's the same color as regular ants.
[00:55:52.520 --> 00:55:57.360]   So for various reasons,
[00:55:57.360 --> 00:56:01.000]   these names have been attached to these things,
[00:56:01.000 --> 00:56:04.480]   despite the fact that they aren't natural.
[00:56:04.480 --> 00:56:07.240]   They don't come about sort of the way you might expect.
[00:56:07.240 --> 00:56:12.280]   This kind of regrettable nomenclature
[00:56:12.280 --> 00:56:15.400]   is also a problem in mathematics and machine learning.
[00:56:15.400 --> 00:56:17.080]   Really, most of the time in math,
[00:56:17.080 --> 00:56:19.680]   combinations of words work like you'd expect.
[00:56:19.680 --> 00:56:24.040]   So if I tell you that I have an abelian group,
[00:56:24.040 --> 00:56:26.840]   if you know what group and abelian mean,
[00:56:26.840 --> 00:56:29.840]   then you could say, oh, that is probably a group
[00:56:29.840 --> 00:56:31.800]   that is also abelian.
[00:56:31.800 --> 00:56:35.080]   And you would in fact be right, but not always.
[00:56:35.080 --> 00:56:39.480]   There are a lot of things that are in math
[00:56:39.480 --> 00:56:42.400]   where the names of the like individual components
[00:56:42.400 --> 00:56:44.320]   of the name refer to something,
[00:56:44.320 --> 00:56:48.040]   but the combination refers to something quite different.
[00:56:48.040 --> 00:56:51.760]   These exceptions are often where people get tripped up
[00:56:51.760 --> 00:56:53.240]   and confused.
[00:56:53.240 --> 00:56:55.840]   And I recommend sort of leaning into this confusion
[00:56:55.840 --> 00:56:58.880]   and trying to understand what are these other words,
[00:56:58.880 --> 00:57:02.920]   the words that go into this compound word,
[00:57:02.920 --> 00:57:04.240]   what are their definitions?
[00:57:04.240 --> 00:57:06.640]   And then what is the definition of the overall thing?
[00:57:06.640 --> 00:57:07.880]   Why are these different?
[00:57:07.880 --> 00:57:09.400]   How are they different?
[00:57:09.400 --> 00:57:11.640]   And by sort of leaning into that confusion,
[00:57:11.640 --> 00:57:14.000]   we can actually really enrich our understanding.
[00:57:14.000 --> 00:57:17.600]   If we just sort of try and shy away from this confusion,
[00:57:17.600 --> 00:57:19.680]   we're actually, I think, going to end up
[00:57:19.680 --> 00:57:22.840]   missing lots of important pieces.
[00:57:22.840 --> 00:57:25.000]   So the primary example that I want
[00:57:25.000 --> 00:57:29.040]   to work through on this front is on random variables.
[00:57:29.040 --> 00:57:35.640]   So random variables may sound like a variable that is random.
[00:57:35.640 --> 00:57:38.560]   And in some sense, that's the way they behave,
[00:57:38.560 --> 00:57:40.720]   much like the alligator pair is kind of like--
[00:57:40.720 --> 00:57:42.400]   it's almost like a pair, but it's
[00:57:42.400 --> 00:57:45.240]   got this alligator-y skin.
[00:57:45.240 --> 00:57:49.520]   But mathematically, it's not like an abelian group.
[00:57:49.520 --> 00:57:51.440]   They are not variables that are random.
[00:57:51.440 --> 00:57:54.200]   They're actually deterministic functions.
[00:57:54.200 --> 00:57:57.760]   And so we're going to go through the typical definition
[00:57:57.760 --> 00:58:01.120]   of random variables and see how understanding this definition
[00:58:01.120 --> 00:58:03.480]   and recognizing how and why it's different
[00:58:03.480 --> 00:58:10.680]   from the intuitive definition helps us better understand
[00:58:10.680 --> 00:58:14.400]   the things we can do with random variables.
[00:58:14.400 --> 00:58:17.520]   So this example actually comes from this famous quote
[00:58:17.520 --> 00:58:20.760]   from this book, Probability and Introduction,
[00:58:20.760 --> 00:58:21.880]   by Samuel Goldberg.
[00:58:21.880 --> 00:58:24.520]   That's a probability textbook that
[00:58:24.520 --> 00:58:29.280]   mentions that these alligator pair and the white ant
[00:58:29.280 --> 00:58:31.160]   are similar to the random variable.
[00:58:31.160 --> 00:58:33.120]   So that's where this quote came from.
[00:58:33.120 --> 00:58:36.760]   This is a solid introductory probability book, I think.
[00:58:36.760 --> 00:58:38.920]   It's a bit old, as maybe indicated
[00:58:38.920 --> 00:58:40.320]   by the quality of the font.
[00:58:40.320 --> 00:58:41.760]   So there's maybe better stuff now.
[00:58:41.760 --> 00:58:46.240]   But it launched this phrase into this idea
[00:58:46.240 --> 00:58:47.760]   of the weirdness of the definition
[00:58:47.760 --> 00:58:48.760]   of the random variable.
[00:58:48.760 --> 00:58:54.480]   So random variables are a lot easier
[00:58:54.480 --> 00:58:56.520]   to exhibit than they are to define.
[00:58:56.520 --> 00:58:58.880]   So if somebody was like, what's a random variable?
[00:58:58.880 --> 00:59:01.920]   It's a lot easier to just list off, like, oh, you roll a die
[00:59:01.920 --> 00:59:02.920]   and a number comes up.
[00:59:02.920 --> 00:59:04.460]   You don't know what it's going to be.
[00:59:04.460 --> 00:59:06.400]   There it is once you've rolled.
[00:59:06.400 --> 00:59:08.160]   How much rain there's going to be tomorrow.
[00:59:08.160 --> 00:59:12.960]   You maybe predict that better than a dice roll.
[00:59:12.960 --> 00:59:15.680]   But famously, weather reports are not
[00:59:15.680 --> 00:59:18.080]   perfect at predicting whether there's going to be rain
[00:59:18.080 --> 00:59:19.800]   and how much there will be.
[00:59:19.800 --> 00:59:21.840]   The number of tweets in the next millisecond,
[00:59:21.840 --> 00:59:25.520]   depending on the moods of a bunch of various influencers,
[00:59:25.520 --> 00:59:27.960]   this number can go way up and way down
[00:59:27.960 --> 00:59:31.040]   and can be difficult to predict.
[00:59:31.040 --> 00:59:33.040]   The fourth decimal place of the time right now,
[00:59:33.040 --> 00:59:37.440]   that one gets used as a random variable in computer programs
[00:59:37.440 --> 00:59:38.960]   all the time.
[00:59:38.960 --> 00:59:40.760]   And a famous random variable people
[00:59:40.760 --> 00:59:44.640]   who do machine learning and deep learning encounter
[00:59:44.640 --> 00:59:47.040]   is their next gradient update.
[00:59:47.040 --> 00:59:49.520]   It'd be great if we knew our gradient updates
[00:59:49.520 --> 00:59:51.760]   without having to calculate them,
[00:59:51.760 --> 00:59:55.080]   but they behave somewhat randomly,
[00:59:55.080 --> 00:59:58.440]   and we have to calculate them to know what they are.
[00:59:58.440 --> 01:00:01.600]   So these are all examples, but they aren't a definition.
[01:00:01.600 --> 01:00:06.280]   So the standard definition is kind of a strange one.
[01:00:06.280 --> 01:00:08.400]   It first asks you to imagine a collection
[01:00:08.400 --> 01:00:10.600]   of all possible universes.
[01:00:10.600 --> 01:00:16.960]   This gets given the very fancy capital omega letter,
[01:00:16.960 --> 01:00:21.280]   and then asks you to define a function that, given a universe,
[01:00:21.280 --> 01:00:22.840]   spits out the outcome of some event.
[01:00:22.840 --> 01:00:28.360]   So imagine it's a scientist with a microscope that can look into--
[01:00:28.360 --> 01:00:31.320]   or a telescope that can look into--
[01:00:31.320 --> 01:00:34.360]   alternate universes and say, oh, in this universe,
[01:00:34.360 --> 01:00:35.560]   the die came up a one.
[01:00:35.560 --> 01:00:37.720]   In this universe, the number of tweets
[01:00:37.720 --> 01:00:40.400]   was 3,000 in the next millisecond.
[01:00:40.400 --> 01:00:43.000]   So it's spitting out the outcome of the event.
[01:00:43.000 --> 01:00:47.800]   That function is what we call a random variable.
[01:00:47.800 --> 01:00:50.320]   So there's some minor restrictions on this function,
[01:00:50.320 --> 01:00:54.120]   but unless you are a hardcore analysis kind of person,
[01:00:54.120 --> 01:00:56.120]   you aren't going to--
[01:00:56.120 --> 01:00:57.800]   you aren't going to accidentally come up
[01:00:57.800 --> 01:00:59.080]   with one that makes this break.
[01:00:59.080 --> 01:01:01.640]   So don't worry about it too much.
[01:01:01.640 --> 01:01:04.880]   So just in picture form, we've got our collection
[01:01:04.880 --> 01:01:07.400]   of all possible universes on the left, the collection
[01:01:07.400 --> 01:01:09.720]   of objects of type universe.
[01:01:09.720 --> 01:01:11.520]   And then we define a function that
[01:01:11.520 --> 01:01:14.800]   takes an object of type universe and returns
[01:01:14.800 --> 01:01:18.680]   an object of type integer or type Boolean or whatever.
[01:01:18.680 --> 01:01:21.520]   So in the case of the number of tweets in the next millisecond,
[01:01:21.520 --> 01:01:24.720]   it's something-- it's a function that takes in a universe
[01:01:24.720 --> 01:01:26.400]   and returns an integer.
[01:01:26.400 --> 01:01:28.400]   So that's written in the bottom right-hand corner
[01:01:28.400 --> 01:01:30.160]   in this sort of type notation.
[01:01:30.160 --> 01:01:33.720]   Number of tweets is a function that goes from universes
[01:01:33.720 --> 01:01:35.280]   to integers.
[01:01:35.280 --> 01:01:38.760]   So maybe there's 6,000 in a couple of universes.
[01:01:38.760 --> 01:01:41.800]   Maybe there's 10,000 in another universe.
[01:01:41.800 --> 01:01:43.880]   That's the one where Donald Trump just
[01:01:43.880 --> 01:01:45.920]   tweeted in the last second, and the other ones
[01:01:45.920 --> 01:01:49.200]   is one where that didn't happen.
[01:01:49.200 --> 01:01:51.480]   Maybe there's one where there's zero tweets in the next
[01:01:51.480 --> 01:01:53.440]   millisecond because Twitter went down.
[01:01:53.440 --> 01:01:55.360]   So these are all the different possibilities.
[01:01:55.360 --> 01:01:57.680]   This is where the sort of randomness
[01:01:57.680 --> 01:01:59.640]   in the number of tweets in the next millisecond
[01:01:59.640 --> 01:02:01.120]   comes from the fact that we don't
[01:02:01.120 --> 01:02:05.600]   know which of these universes we're in, you might think of.
[01:02:05.600 --> 01:02:09.520]   So the same thing can be done for the next gradient update.
[01:02:09.520 --> 01:02:11.640]   It now returns floating point numbers.
[01:02:11.640 --> 01:02:14.040]   Maybe a lot of the times, our gradients are very sparse,
[01:02:14.040 --> 01:02:16.400]   and so we get zero gradients.
[01:02:16.400 --> 01:02:19.480]   But then other times, we get relatively small numbers,
[01:02:19.480 --> 01:02:21.040]   like 10 to the minus third.
[01:02:21.040 --> 01:02:24.080]   And all possible numbers maybe come out
[01:02:24.080 --> 01:02:27.560]   of all these different universes in which different--
[01:02:27.560 --> 01:02:30.360]   our process of sampling the next batch
[01:02:30.360 --> 01:02:32.760]   draws a different set of elements
[01:02:32.760 --> 01:02:33.720]   to go into our network.
[01:02:33.720 --> 01:02:41.360]   So number of tweets and next gradient
[01:02:41.360 --> 01:02:43.720]   are properly considered functions,
[01:02:43.720 --> 01:02:45.680]   but we call them random variables
[01:02:45.680 --> 01:02:48.800]   because we want to manipulate them like variables.
[01:02:48.800 --> 01:02:51.160]   We want to be able to add them together and multiply them
[01:02:51.160 --> 01:02:54.440]   and apply functions to them and things like that,
[01:02:54.440 --> 01:02:56.440]   the same way we work with things that we actually
[01:02:56.440 --> 01:02:57.560]   know the value of.
[01:02:57.560 --> 01:02:59.400]   But in order to actually define them,
[01:02:59.400 --> 01:03:01.960]   we needed to define them as functions.
[01:03:01.960 --> 01:03:05.320]   So now we're going to go through how, using that definition,
[01:03:05.320 --> 01:03:10.360]   we can understand what lets you add and multiply and divide
[01:03:10.360 --> 01:03:12.680]   random variables.
[01:03:12.680 --> 01:03:14.640]   But first, a quick summary.
[01:03:14.640 --> 01:03:18.200]   So we've got our universe on the left-hand side.
[01:03:18.200 --> 01:03:21.960]   This is maybe-- the whole collection of universes
[01:03:21.960 --> 01:03:22.760]   is omega.
[01:03:22.760 --> 01:03:23.840]   We've got our outcomes.
[01:03:23.840 --> 01:03:28.280]   Maybe it's all real numbers on the right-hand side.
[01:03:28.280 --> 01:03:31.600]   And then our random variable, our random real number,
[01:03:31.600 --> 01:03:36.000]   is x, which is actually a function that
[01:03:36.000 --> 01:03:38.600]   spits out real numbers.
[01:03:38.600 --> 01:03:40.400]   So I think I've already talked a little bit
[01:03:40.400 --> 01:03:42.280]   about why this matters, so maybe this slide's
[01:03:42.280 --> 01:03:44.280]   become a little bit redundant.
[01:03:44.280 --> 01:03:46.680]   But the biggest thing that I want to point out
[01:03:46.680 --> 01:03:49.320]   is that strong command of this unintuitive definition
[01:03:49.320 --> 01:03:52.760]   will help you when it comes time to extend this definition
[01:03:52.760 --> 01:03:56.600]   and work in places where your intuition might fail you
[01:03:56.600 --> 01:03:59.240]   or lead you astray.
[01:03:59.240 --> 01:04:01.720]   So the first thing is that it's clear
[01:04:01.720 --> 01:04:05.040]   how to define new random variables from old ones based
[01:04:05.040 --> 01:04:07.200]   off of this definition.
[01:04:07.200 --> 01:04:10.240]   And the way we define a new random variable based
[01:04:10.240 --> 01:04:14.640]   off an old one is maybe we want the number of tweets
[01:04:14.640 --> 01:04:17.400]   in the next millisecond.
[01:04:17.400 --> 01:04:24.840]   But maybe we want to say whether that number is above 10,000
[01:04:24.840 --> 01:04:28.200]   or below 10,000, because that's what's going to cause higher
[01:04:28.200 --> 01:04:30.160]   load on our server.
[01:04:30.160 --> 01:04:33.440]   So normally, if we want to check if a number is above or below
[01:04:33.440 --> 01:04:39.280]   10,000, we pass it to a function that returns true or false
[01:04:39.280 --> 01:04:42.960]   if it's above or below that cutoff.
[01:04:42.960 --> 01:04:46.080]   So we can't pass a random variable directly
[01:04:46.080 --> 01:04:49.720]   to a function, because a random variable is a function.
[01:04:49.720 --> 01:04:54.560]   So if I try to ask, is this function bigger than 10,000,
[01:04:54.560 --> 01:04:56.200]   Python or whatever my language is,
[01:04:56.200 --> 01:04:57.840]   is going to throw an error.
[01:04:57.840 --> 01:05:01.160]   What we do instead with functions is we compose them.
[01:05:01.160 --> 01:05:05.760]   So I've represented that here by drawing these arrows
[01:05:05.760 --> 01:05:11.440]   for the functions and putting them tip to tail.
[01:05:11.440 --> 01:05:13.000]   So something comes out.
[01:05:13.000 --> 01:05:14.760]   Let me do a little pointing here.
[01:05:14.760 --> 01:05:16.720]   Something comes out of this function.
[01:05:16.720 --> 01:05:21.360]   This is a random-- an outcome of, say, number of tweets.
[01:05:21.360 --> 01:05:25.320]   It goes into our function, say, is bigger than 10,000,
[01:05:25.320 --> 01:05:29.320]   and out comes something else, a different type, possibly,
[01:05:29.320 --> 01:05:32.440]   like a Boolean that says true or false.
[01:05:32.440 --> 01:05:35.600]   And this defines a new random variable
[01:05:35.600 --> 01:05:41.280]   that we introduced this notation for,
[01:05:41.280 --> 01:05:44.360]   which is kind of a strange notation.
[01:05:44.360 --> 01:05:46.440]   It says, instead of saying, oh, this
[01:05:46.440 --> 01:05:49.920]   is f of x of something, which is how we'd normally do it,
[01:05:49.920 --> 01:05:51.240]   we write it as f of x.
[01:05:51.240 --> 01:05:53.480]   We pretend that our random variable
[01:05:53.480 --> 01:05:56.280]   is a normal variable that can go into functions,
[01:05:56.280 --> 01:05:58.720]   and then we rewrite it this way.
[01:05:58.720 --> 01:06:01.600]   But when we write it that way, we really
[01:06:01.600 --> 01:06:03.360]   mean something like this.
[01:06:03.360 --> 01:06:05.140]   So this has the utility of making it easier
[01:06:05.140 --> 01:06:07.120]   to write things once we understand
[01:06:07.120 --> 01:06:08.160]   what we're talking about.
[01:06:08.160 --> 01:06:10.040]   But it can kind of trip people up
[01:06:10.040 --> 01:06:11.960]   when they're trying to learn these definitions
[01:06:11.960 --> 01:06:13.320]   for the first time.
[01:06:13.320 --> 01:06:15.880]   We can also get rules for the distribution of f of x
[01:06:15.880 --> 01:06:16.800]   based off of this.
[01:06:16.800 --> 01:06:18.320]   And it's much easier to understand
[01:06:18.320 --> 01:06:21.280]   where they're coming from from looking at a diagram like this
[01:06:21.280 --> 01:06:23.280]   than from the baseline definition.
[01:06:23.280 --> 01:06:28.000]   So if you've learned maybe the Jacobian rule for--
[01:06:28.000 --> 01:06:32.920]   Jacobian determinant rule for changes of variables,
[01:06:32.920 --> 01:06:36.360]   it comes out of this.
[01:06:36.360 --> 01:06:38.560]   And then this is where we get to define our algebra
[01:06:38.560 --> 01:06:41.040]   of random variables, where we get to take our algebra
[01:06:41.040 --> 01:06:42.440]   that we normally use.
[01:06:42.440 --> 01:06:45.040]   We can add numbers together, multiply them.
[01:06:45.040 --> 01:06:48.120]   We can do it with random variables also.
[01:06:48.120 --> 01:06:50.400]   So the idea is if we have two random variables that
[01:06:50.400 --> 01:06:53.520]   both operate on this same collection of universes,
[01:06:53.520 --> 01:06:58.720]   then we can define their sum by, say, applying both of them,
[01:06:58.720 --> 01:07:03.320]   getting now two possible outcomes, an outcome of x
[01:07:03.320 --> 01:07:05.680]   and an outcome of y, maybe the number of tweets
[01:07:05.680 --> 01:07:09.120]   in the next millisecond and the number of tweets
[01:07:09.120 --> 01:07:10.480]   a second from now.
[01:07:10.480 --> 01:07:13.320]   And then we can add them together.
[01:07:13.320 --> 01:07:18.080]   So the way that this plus works is it takes these two--
[01:07:18.080 --> 01:07:19.920]   this is actually your normal plus.
[01:07:19.920 --> 01:07:22.840]   It takes two arguments and adds them together.
[01:07:22.840 --> 01:07:25.640]   So like the dunder add function in Python
[01:07:25.640 --> 01:07:27.040]   is maybe what you should picture,
[01:07:27.040 --> 01:07:29.240]   a function that takes in two things
[01:07:29.240 --> 01:07:33.180]   and returns the sum of those two things.
[01:07:33.180 --> 01:07:36.600]   And we can define now the sum of x and y
[01:07:36.600 --> 01:07:39.360]   by composing these two arrows together,
[01:07:39.360 --> 01:07:42.920]   by first getting two outcomes, then adding those outcomes
[01:07:42.920 --> 01:07:43.560]   together.
[01:07:43.560 --> 01:07:47.240]   Now this is a new map, which this combination of these two
[01:07:47.240 --> 01:07:51.400]   maps is a new map that goes from universes to outcomes.
[01:07:51.400 --> 01:07:59.280]   But now it's computing a sum instead of computing just
[01:07:59.280 --> 01:08:00.680]   the outcome of one or the other.
[01:08:00.680 --> 01:08:04.160]   So this is a new random variable.
[01:08:04.160 --> 01:08:05.280]   So this works for times.
[01:08:05.280 --> 01:08:06.320]   This works for minus.
[01:08:06.320 --> 01:08:09.640]   This works for more other possible things.
[01:08:09.640 --> 01:08:12.560]   It's also the case that we can do it for data types.
[01:08:12.560 --> 01:08:14.080]   We can make random lists.
[01:08:14.080 --> 01:08:17.200]   We can make random dictionaries, random tuples,
[01:08:17.200 --> 01:08:19.840]   all the kinds of data types we work with in computer science.
[01:08:19.840 --> 01:08:23.840]   Using the sort of reasoning with diagrams like this,
[01:08:23.840 --> 01:08:27.000]   we can define what it means to, say,
[01:08:27.000 --> 01:08:29.480]   take a pair of random variables.
[01:08:29.480 --> 01:08:32.120]   And this actually leads to a really powerful approach
[01:08:32.120 --> 01:08:34.240]   for understanding these kinds of data types
[01:08:34.240 --> 01:08:36.040]   and for understanding what it is we're
[01:08:36.040 --> 01:08:39.680]   doing when we're combining these arrows together.
[01:08:39.680 --> 01:08:42.520]   And lastly, this is where it's really valuable,
[01:08:42.520 --> 01:08:44.880]   is we can even define random functions.
[01:08:44.880 --> 01:08:47.240]   So just as random variables were actually
[01:08:47.240 --> 01:08:51.000]   these deterministic functions, random functions
[01:08:51.000 --> 01:08:54.200]   are actually deterministic higher order functions.
[01:08:54.200 --> 01:08:57.760]   So now what's happening here is we're looking into a universe,
[01:08:57.760 --> 01:09:00.920]   and that universe is spitting out not just a single number,
[01:09:00.920 --> 01:09:02.200]   but a function.
[01:09:02.200 --> 01:09:04.920]   So I've represented that here by this tiny arrow.
[01:09:04.920 --> 01:09:06.920]   So notice that this tiny arrow is much smaller
[01:09:06.920 --> 01:09:08.200]   than these big arrows.
[01:09:08.200 --> 01:09:12.000]   This is maybe bad notation, but bear with me.
[01:09:12.000 --> 01:09:14.000]   So we look into our universe.
[01:09:14.000 --> 01:09:18.880]   It spits out a function from A to B.
[01:09:18.880 --> 01:09:21.720]   So we write that now with a little f instead
[01:09:21.720 --> 01:09:23.680]   of the capital X that I was using
[01:09:23.680 --> 01:09:26.480]   in the past that represented a variable, or capital Y,
[01:09:26.480 --> 01:09:28.080]   or something like that.
[01:09:28.080 --> 01:09:32.240]   So this is now a function that returns a function.
[01:09:32.240 --> 01:09:33.920]   So in computer science, we think of those
[01:09:33.920 --> 01:09:37.200]   as higher order functions.
[01:09:37.200 --> 01:09:39.320]   So it's like a lambda in Python.
[01:09:39.320 --> 01:09:43.800]   It can take an argument and give it to another function.
[01:09:43.800 --> 01:09:46.800]   So when we're mapping--
[01:09:46.800 --> 01:09:49.640]   so when we get this random function out,
[01:09:49.640 --> 01:09:52.120]   we then, if we want to get a random variable,
[01:09:52.120 --> 01:09:56.080]   a single value, we pass an argument to that function.
[01:09:56.080 --> 01:09:59.000]   So it turns out you can represent
[01:09:59.000 --> 01:10:03.240]   the argument that goes into a function also with an arrow.
[01:10:03.240 --> 01:10:07.920]   So this guy takes a function that goes from A to B,
[01:10:07.920 --> 01:10:11.920]   gives it the argument A, and returns B.
[01:10:11.920 --> 01:10:13.880]   So this is something that maybe you would
[01:10:13.880 --> 01:10:16.400]   use a lambda for in Python.
[01:10:16.400 --> 01:10:18.480]   And then this whole thing, we write
[01:10:18.480 --> 01:10:21.560]   as f applied to the value A. And that's
[01:10:21.560 --> 01:10:25.520]   evaluating a random function at a point A.
[01:10:25.520 --> 01:10:27.200]   And so random functions, you may not
[01:10:27.200 --> 01:10:29.560]   have come across them that much, or may not
[01:10:29.560 --> 01:10:32.000]   have realized you were coming across them that much.
[01:10:32.000 --> 01:10:33.400]   But random functions are actually
[01:10:33.400 --> 01:10:36.600]   part of some of these really important recent models
[01:10:36.600 --> 01:10:39.120]   for understanding neural networks based off
[01:10:39.120 --> 01:10:42.000]   of the neural tangent kernel and the Gaussian process
[01:10:42.000 --> 01:10:43.440]   model of neural networks.
[01:10:43.440 --> 01:10:46.400]   Gaussian processes are random functions.
[01:10:46.400 --> 01:10:48.360]   And neural networks with random weights
[01:10:48.360 --> 01:10:50.720]   are also random functions.
[01:10:50.720 --> 01:10:52.720]   And people tend to get tripped up
[01:10:52.720 --> 01:10:54.480]   when they try and understand these things,
[01:10:54.480 --> 01:10:56.760]   because their understanding of random variables
[01:10:56.760 --> 01:11:01.080]   is maybe incomplete, because they haven't gone all the way
[01:11:01.080 --> 01:11:03.640]   to the--
[01:11:03.640 --> 01:11:07.680]   through and understood this definition thoroughly.
[01:11:07.680 --> 01:11:13.000]   So I had a little section here on stochastic gradient descent.
[01:11:13.000 --> 01:11:16.040]   But actually, I think I want to keep the focus of the talk
[01:11:16.040 --> 01:11:17.880]   just on that random variables material.
[01:11:17.880 --> 01:11:20.800]   And maybe we'll cover this stochastic gradient descent
[01:11:20.800 --> 01:11:21.720]   stuff later.
[01:11:21.720 --> 01:11:24.320]   But you'll find if you look really closely,
[01:11:24.320 --> 01:11:28.720]   stochastic gradient descent is kind of none of those things.
[01:11:28.720 --> 01:11:30.440]   But we're going to skip over--
[01:11:30.440 --> 01:11:33.200]   I'm going to skip over that and take
[01:11:33.200 --> 01:11:36.360]   any questions from the audience about this material.
[01:11:36.360 --> 01:11:42.440]   I would actually love for us to go over the stochastic gradient
[01:11:42.440 --> 01:11:45.120]   descent stuff if you want to.
[01:11:45.120 --> 01:11:45.920]   OK.
[01:11:45.920 --> 01:11:47.360]   Maybe just a quick pause.
[01:11:47.360 --> 01:11:50.680]   I'll give it maybe a few seconds for the YouTube people
[01:11:50.680 --> 01:11:52.160]   to catch up.
[01:11:52.160 --> 01:11:56.440]   If anybody on Slack has questions.
[01:11:56.440 --> 01:11:58.920]   I have a question.
[01:11:58.920 --> 01:12:02.040]   So you said the sum of two random variables
[01:12:02.040 --> 01:12:03.760]   is also a random variable.
[01:12:03.760 --> 01:12:06.680]   And I'm wondering, does this randomness
[01:12:06.680 --> 01:12:10.120]   increase when you combine two random variables?
[01:12:10.120 --> 01:12:13.400]   And also, what happens to the underlying distribution
[01:12:13.400 --> 01:12:17.200]   that you're pulling random variables from?
[01:12:17.200 --> 01:12:18.560]   Yeah, that's a great question.
[01:12:18.560 --> 01:12:21.600]   So one thing that didn't come up that much in this
[01:12:21.600 --> 01:12:25.560]   was what are the distributions of these random variables.
[01:12:25.560 --> 01:12:32.200]   So there is a way to calculate these distributions.
[01:12:32.200 --> 01:12:34.400]   It's basically-- it's called a push forward.
[01:12:34.400 --> 01:12:36.800]   You take the distribution, and you
[01:12:36.800 --> 01:12:39.280]   push it through the function.
[01:12:39.280 --> 01:12:42.560]   And then you watch how things get stretched and scaled.
[01:12:42.560 --> 01:12:46.960]   So there's a definition involving integrals and things
[01:12:46.960 --> 01:12:50.800]   to get you what that actual expression is.
[01:12:50.800 --> 01:12:54.080]   So for adding random variables, if the two variables
[01:12:54.080 --> 01:12:57.560]   are independent, you convolve their distributions.
[01:12:57.560 --> 01:13:03.080]   So you literally take the distribution,
[01:13:03.080 --> 01:13:07.760]   and you flip it around, and then slide it
[01:13:07.760 --> 01:13:10.720]   by multiplying and summing, the same way
[01:13:10.720 --> 01:13:14.200]   you do a convolution in a neural network.
[01:13:14.200 --> 01:13:16.160]   So there's a way to see where that comes from.
[01:13:16.160 --> 01:13:19.520]   But that basically falls out as the trick
[01:13:19.520 --> 01:13:21.160]   that you can use when you're adding two
[01:13:21.160 --> 01:13:22.960]   independent random variables.
[01:13:22.960 --> 01:13:25.920]   If the variables are dependent on each other, all bets are off.
[01:13:25.920 --> 01:13:28.880]   And it can become basically arbitrarily difficult
[01:13:28.880 --> 01:13:31.120]   to figure out what their distribution is.
[01:13:31.120 --> 01:13:36.840]   But in general, I think the--
[01:13:36.840 --> 01:13:40.960]   yeah, you can't say that the variance either goes up
[01:13:40.960 --> 01:13:43.920]   or goes down when you add two random variables.
[01:13:43.920 --> 01:13:45.200]   They usually go up.
[01:13:45.200 --> 01:13:48.160]   Like the baseline case is that they go up.
[01:13:48.160 --> 01:13:50.120]   But say I have two random variables.
[01:13:50.120 --> 01:13:52.520]   One is defined as a coin flip that
[01:13:52.520 --> 01:13:54.880]   gives me plus 1 and minus 1.
[01:13:54.880 --> 01:13:59.760]   And the other one is defined as, like, check that same coin.
[01:13:59.760 --> 01:14:02.800]   But if it comes up heads, give me the opposite answer.
[01:14:02.800 --> 01:14:04.640]   So now if I add those two variables together,
[01:14:04.640 --> 01:14:05.840]   they're always 0.
[01:14:05.840 --> 01:14:07.000]   So the variance is gone.
[01:14:07.000 --> 01:14:08.680]   So it's now deterministic.
[01:14:08.680 --> 01:14:12.160]   So you have to be a little bit careful about making
[01:14:12.160 --> 01:14:13.240]   sort of general statements.
[01:14:13.240 --> 01:14:15.240]   But most people think of adding random variables
[01:14:15.240 --> 01:14:17.400]   as increasing randomness.
[01:14:17.400 --> 01:14:19.480]   Gotcha.
[01:14:19.480 --> 01:14:21.680]   That makes it a lot more intuitive.
[01:14:21.680 --> 01:14:22.880]   Also, does it break?
[01:14:22.880 --> 01:14:24.840]   Is there any point, like, when you're
[01:14:24.840 --> 01:14:27.640]   creating functions of random variables
[01:14:27.640 --> 01:14:31.240]   that this paradigm breaks and the result
[01:14:31.240 --> 01:14:33.240]   that you get from these functions
[01:14:33.240 --> 01:14:36.320]   becomes meaningless because there's so much randomness?
[01:14:36.320 --> 01:14:38.720]   Or does that not matter?
[01:14:38.720 --> 01:14:42.680]   Yeah, so what I would say is, like, yeah,
[01:14:42.680 --> 01:14:44.520]   you can compose functions together.
[01:14:44.520 --> 01:14:47.200]   You could take-- and people, in fact,
[01:14:47.200 --> 01:14:49.440]   do take things as complicated as neural networks
[01:14:49.440 --> 01:14:53.840]   and treat them as random variables or random functions.
[01:14:53.840 --> 01:14:57.120]   And there are many, many functions lined up together.
[01:14:57.120 --> 01:14:58.840]   You can do arbitrary combinations
[01:14:58.840 --> 01:15:00.920]   of random variables.
[01:15:00.920 --> 01:15:04.040]   They do-- I mean, if you're combining things that
[01:15:04.040 --> 01:15:07.840]   are independent, what you find is that you end up--
[01:15:07.840 --> 01:15:10.440]   you lose lots of structure.
[01:15:10.440 --> 01:15:11.840]   So like the central limit theorem
[01:15:11.840 --> 01:15:14.160]   says if I add a bunch of independent things,
[01:15:14.160 --> 01:15:16.080]   the thing that comes out is Gaussian.
[01:15:16.080 --> 01:15:17.160]   And so it doesn't matter.
[01:15:17.160 --> 01:15:18.600]   There's random matrix theory that
[01:15:18.600 --> 01:15:20.840]   says if I take a really big random matrix
[01:15:20.840 --> 01:15:23.200]   and all the entries are independent,
[01:15:23.200 --> 01:15:25.520]   the resulting eigenvalues always
[01:15:25.520 --> 01:15:27.120]   follow a specific distribution.
[01:15:27.120 --> 01:15:29.160]   So there's a general thing that says, whoa,
[01:15:29.160 --> 01:15:32.280]   if you're combining a bunch of independent random things,
[01:15:32.280 --> 01:15:33.720]   what you get is actually something
[01:15:33.720 --> 01:15:36.840]   that's less complicated than all of its pieces.
[01:15:36.840 --> 01:15:38.960]   So those are the main examples I would think of.
[01:15:38.960 --> 01:15:40.640]   The only way this paradigm breaks down
[01:15:40.640 --> 01:15:44.520]   is there is this restriction of measurability on functions.
[01:15:44.520 --> 01:15:45.980]   That's important for anybody who's
[01:15:45.980 --> 01:15:49.040]   trying to come up with, I don't know, like a Bonnock-Tarski
[01:15:49.040 --> 01:15:53.560]   paradox kind of thing, or trying to come up
[01:15:53.560 --> 01:15:56.320]   with some really weird function that returns--
[01:15:56.320 --> 01:16:01.800]   yeah.
[01:16:01.800 --> 01:16:03.360]   Non-measurable functions are actually
[01:16:03.360 --> 01:16:04.360]   pretty hard to construct.
[01:16:04.360 --> 01:16:07.800]   So I'm not going to come up with an example right now,
[01:16:07.800 --> 01:16:10.880]   which should tell you just how broad this thing covers.
[01:16:10.880 --> 01:16:13.680]   It's the kind of thing mathematicians and analysts
[01:16:13.680 --> 01:16:14.480]   really worry about.
[01:16:14.480 --> 01:16:17.140]   But when you're doing machine learning
[01:16:17.140 --> 01:16:18.560]   and working with random variables,
[01:16:18.560 --> 01:16:20.520]   it basically never comes up.
[01:16:20.520 --> 01:16:22.000]   Gotcha.
[01:16:22.000 --> 01:16:23.080]   All right, cool.
[01:16:23.080 --> 01:16:24.080]   Let's see.
[01:16:24.080 --> 01:16:25.760]   I know Kyle had a question.
[01:16:25.760 --> 01:16:29.040]   So Kyle, if you want to jump in and ask--
[01:16:29.040 --> 01:16:33.400]   there it is.
[01:16:33.400 --> 01:16:34.400]   So go ahead.
[01:16:34.400 --> 01:16:36.920]   Yeah, it looks like Kyle asked the question in the Q&A.
[01:16:36.920 --> 01:16:40.880]   So when we talk about neural networks as random functions,
[01:16:40.880 --> 01:16:42.760]   is this still the case once the network
[01:16:42.760 --> 01:16:45.520]   has undergone training?
[01:16:45.520 --> 01:16:48.760]   So to think of a neural network as a random function,
[01:16:48.760 --> 01:16:51.680]   you usually think of the weights as being random, right?
[01:16:51.680 --> 01:16:55.280]   So you're doing a Bayesian neural network.
[01:16:55.280 --> 01:17:02.400]   So you're saying I started off with a distribution
[01:17:02.400 --> 01:17:03.680]   over possible weights.
[01:17:03.680 --> 01:17:08.400]   Like my prior told me my weights were somewhere between minus 1
[01:17:08.400 --> 01:17:12.720]   and plus 1 uniformly, or they're from a Gaussian distribution.
[01:17:12.720 --> 01:17:17.800]   And a fully Bayesian approach to neural network training
[01:17:17.800 --> 01:17:19.800]   never drops that randomness.
[01:17:19.800 --> 01:17:21.440]   It says, oh, each gradient update
[01:17:21.440 --> 01:17:23.680]   tells me to move this Gaussian around,
[01:17:23.680 --> 01:17:26.080]   usually make it much, much skinnier,
[01:17:26.080 --> 01:17:28.640]   and put it around a specific value.
[01:17:28.640 --> 01:17:30.680]   But I still keep a distribution.
[01:17:30.680 --> 01:17:33.960]   And so then the neural network remains a random function
[01:17:33.960 --> 01:17:35.920]   in that to evaluate it, you would
[01:17:35.920 --> 01:17:37.880]   have to draw a specific set of weights.
[01:17:37.880 --> 01:17:40.320]   And that specific set of weights is drawn at random.
[01:17:41.320 --> 01:17:44.320]   But what we do normally is we just draw some specific thing,
[01:17:44.320 --> 01:17:46.440]   and then we use the gradient updates to change them.
[01:17:46.440 --> 01:17:48.440]   And the network remains a deterministic function
[01:17:48.440 --> 01:17:52.360]   all the time, unless you use dropout, in which case
[01:17:52.360 --> 01:17:54.840]   that network is also a random function.
[01:17:54.840 --> 01:18:03.360]   Yeah, but I think that doesn't come up quite as much,
[01:18:03.360 --> 01:18:05.720]   the idea of thinking of dropout as something that makes
[01:18:05.720 --> 01:18:07.880]   your network a random function.
[01:18:07.880 --> 01:18:11.640]   And so I would say that it's really more the idea
[01:18:11.640 --> 01:18:13.680]   that if you keep a distribution over weights,
[01:18:13.680 --> 01:18:15.800]   then you get a function that is--
[01:18:15.800 --> 01:18:18.640]   then your neural network is a random function.
[01:18:18.640 --> 01:18:23.320]   And this leads to this Gaussian processes
[01:18:23.320 --> 01:18:26.360]   view of neural networks and their loss surfaces
[01:18:26.360 --> 01:18:28.480]   that has been making a lot of waves
[01:18:28.480 --> 01:18:30.720]   in the theoretical community in the form
[01:18:30.720 --> 01:18:33.800]   of the neural tangent kernel over the past year or so.
[01:18:33.800 --> 01:18:36.400]   What if you pulled the nodes that you drop out
[01:18:36.400 --> 01:18:41.400]   from a distribution, then you would remove the randomness?
[01:18:41.400 --> 01:18:42.840]   So let's see.
[01:18:42.840 --> 01:18:45.960]   If you're pulling the nodes that you drop out
[01:18:45.960 --> 01:18:48.280]   from a distribution, and those nodes are, yeah, random,
[01:18:48.280 --> 01:18:51.680]   then yeah, the neural network is going to be a random function.
[01:18:51.680 --> 01:18:54.080]   I think so it seems like one other--
[01:18:54.080 --> 01:18:57.120]   some other people want actually to go through the SGD example.
[01:18:57.120 --> 01:18:59.760]   So we might as well just do it.
[01:18:59.760 --> 01:19:01.080]   But I think that's a good idea.
[01:19:01.080 --> 01:19:02.360]   I think that's a good example.
[01:19:02.360 --> 01:19:05.040]   So we might as well just do it.
[01:19:05.040 --> 01:19:15.800]   Yeah, so-- OK, so stochastic gradient descent.
[01:19:15.800 --> 01:19:17.480]   It's a common algorithm.
[01:19:17.480 --> 01:19:21.560]   And yet, it is one that this alligator pair problem
[01:19:21.560 --> 01:19:26.880]   hits pretty badly, especially on the gradient part,
[01:19:26.880 --> 01:19:28.960]   to a lesser extent on the descent part,
[01:19:28.960 --> 01:19:32.160]   and still a bit on the stochastic part.
[01:19:32.160 --> 01:19:35.160]   So I have another little motivation slide.
[01:19:35.160 --> 01:19:39.280]   So Voltaire, who was an influencer in the 18th century,
[01:19:39.280 --> 01:19:43.400]   wrote in his History of Europe, "The agglomeration which
[01:19:43.400 --> 01:19:45.360]   was called and which still calls itself
[01:19:45.360 --> 01:19:48.440]   the Holy Roman Empire," pictured here on the right,
[01:19:48.440 --> 01:19:52.480]   "was neither holy nor Roman nor an empire."
[01:19:52.480 --> 01:19:54.880]   So it wasn't holy because it actually
[01:19:54.880 --> 01:19:57.440]   spent most of its existence fighting with the pope, which
[01:19:57.440 --> 01:19:58.760]   is not very holy.
[01:19:58.760 --> 01:20:03.160]   It wasn't Roman because it was in Germany, which was never
[01:20:03.160 --> 01:20:06.600]   really part of the Roman Empire, and nor were the people
[01:20:06.600 --> 01:20:07.960]   in there ever Romans.
[01:20:07.960 --> 01:20:10.240]   And it wasn't really an empire because the entire thing
[01:20:10.240 --> 01:20:14.240]   was basically a huge mess of princes and bishops squabbling
[01:20:14.240 --> 01:20:15.160]   with each other.
[01:20:15.160 --> 01:20:18.200]   And it never really behaved as a coherent unit.
[01:20:18.200 --> 01:20:20.880]   So my update to that is that this algorithm, which
[01:20:20.880 --> 01:20:24.360]   was called and is still called stochastic gradient descent,
[01:20:24.360 --> 01:20:28.480]   is neither stochastic nor of a gradient nor a descent method.
[01:20:29.480 --> 01:20:32.960]   So the first part, stochastic gradient descent
[01:20:32.960 --> 01:20:35.080]   is not a descent method.
[01:20:35.080 --> 01:20:38.400]   A descent method for minimizing a function f
[01:20:38.400 --> 01:20:41.840]   is a method such that if I evaluate
[01:20:41.840 --> 01:20:49.440]   the function on each consecutive iterate of that method--
[01:20:49.440 --> 01:20:54.080]   so the method produces theta's parameter values over time t
[01:20:54.080 --> 01:20:58.960]   from 0 to capital T or from 0 on out--
[01:20:58.960 --> 01:21:08.080]   a descent method says the next value of the parameters
[01:21:08.080 --> 01:21:11.720]   will give me a lower value for the function.
[01:21:11.720 --> 01:21:14.160]   And SGD only promises this on average.
[01:21:14.160 --> 01:21:16.880]   SGD can, in fact, go up.
[01:21:16.880 --> 01:21:19.080]   It doesn't, in fact, descend.
[01:21:19.080 --> 01:21:21.360]   It goes up sometimes.
[01:21:21.360 --> 01:21:25.000]   Anybody who's looked at a neural network training curve
[01:21:25.000 --> 01:21:28.160]   has seen this phenomenon occur.
[01:21:28.160 --> 01:21:34.080]   And gradient descent, where you don't have the stochastic part,
[01:21:34.080 --> 01:21:36.200]   almost does this.
[01:21:36.200 --> 01:21:39.400]   But it only does it if you have a small enough step size
[01:21:39.400 --> 01:21:42.040]   or if you're shrinking the step size.
[01:21:42.040 --> 01:21:46.720]   So even gradient descent is actually not a descent method.
[01:21:46.720 --> 01:21:48.240]   And the reason why this is important
[01:21:48.240 --> 01:21:49.780]   is because if you actually go and you
[01:21:49.780 --> 01:21:52.400]   do an analysis of descent methods,
[01:21:52.400 --> 01:21:53.960]   how do we prove they converge?
[01:21:53.960 --> 01:21:55.360]   In what cases do they converge?
[01:21:55.360 --> 01:21:57.840]   What causes them to break?
[01:21:57.840 --> 01:22:00.640]   The theoretical analysis and the practical--
[01:22:00.640 --> 01:22:03.100]   the theoretical analysis is just very different for descent
[01:22:03.100 --> 01:22:06.400]   methods and for something like SGD.
[01:22:06.400 --> 01:22:08.440]   They're completely different, actually,
[01:22:08.440 --> 01:22:10.320]   when you look at the details of the math.
[01:22:10.320 --> 01:22:12.400]   And then the practical behavior is also different.
[01:22:12.400 --> 01:22:16.400]   Seeing values go slightly up in gradient descent
[01:22:16.400 --> 01:22:18.200]   and especially stochastic gradient descent,
[01:22:18.200 --> 01:22:21.200]   par for the course, seeing values go up for a descent
[01:22:21.200 --> 01:22:23.360]   method means you implemented it wrong.
[01:22:23.360 --> 01:22:29.760]   So these are very, very different things
[01:22:29.760 --> 01:22:35.320]   when it comes time to debug or to analyze.
[01:22:35.320 --> 01:22:38.120]   The second one, and this is perhaps the strongest leg
[01:22:38.120 --> 01:22:40.800]   I have to stand on in this rant, is
[01:22:40.800 --> 01:22:44.320]   that stochastic gradient descent does not use gradients.
[01:22:44.320 --> 01:22:48.960]   A gradient for a function f is a function, nabla f,
[01:22:48.960 --> 01:22:52.840]   that satisfies this relationship here, which basically says,
[01:22:52.840 --> 01:22:56.560]   if I evaluate the function at a certain--
[01:22:56.560 --> 01:22:58.760]   if I want to know--
[01:22:58.760 --> 01:22:59.280]   oh, shoot.
[01:22:59.280 --> 01:23:00.880]   There's a little bit of an error here.
[01:23:00.880 --> 01:23:03.360]   This nabla here should not be there.
[01:23:03.360 --> 01:23:06.400]   So ignore that guy.
[01:23:06.400 --> 01:23:09.080]   If I look at the function's value at some point epsilon
[01:23:09.080 --> 01:23:18.600]   away from a point theta, then it should be approximately
[01:23:18.600 --> 01:23:23.640]   equal to a linear function times how far away I went.
[01:23:23.640 --> 01:23:25.960]   So that's what this inner product here says.
[01:23:25.960 --> 01:23:28.480]   So this applies for scalar-valued functions
[01:23:28.480 --> 01:23:33.600]   that take in scalars, vectors, and matrices.
[01:23:33.600 --> 01:23:38.720]   And importantly, this is true for every choice of theta.
[01:23:38.720 --> 01:23:42.120]   But stochastic gradient descent, as we use it in ML,
[01:23:42.120 --> 01:23:44.040]   works perfectly well for functions that
[01:23:44.040 --> 01:23:46.120]   don't have gradients everywhere.
[01:23:46.120 --> 01:23:50.200]   So I have two examples on the right-hand side, both of which
[01:23:50.200 --> 01:23:52.240]   are very popular in machine learning.
[01:23:52.240 --> 01:23:55.520]   One is the rectified linear unit on the top,
[01:23:55.520 --> 01:23:58.040]   and the other is the absolute value,
[01:23:58.040 --> 01:23:59.920]   which in high-dimensional cases, this
[01:23:59.920 --> 01:24:01.960]   would be the L1 norm, which shows up
[01:24:01.960 --> 01:24:06.160]   as a sparseness penalty in sparse activations
[01:24:06.160 --> 01:24:11.840]   or sparse parameter regularization schemes.
[01:24:11.840 --> 01:24:13.160]   So these functions get used.
[01:24:13.160 --> 01:24:15.640]   But if I try and ask, is there a function here
[01:24:15.640 --> 01:24:20.920]   that is a good linear approximation of this function,
[01:24:20.920 --> 01:24:21.960]   it breaks apart.
[01:24:21.960 --> 01:24:23.460]   Actually, it's easiest to see here.
[01:24:23.460 --> 01:24:25.240]   There's no good linear approximation
[01:24:25.240 --> 01:24:28.000]   to this function at the point 0.
[01:24:28.000 --> 01:24:30.080]   I could get this side perfectly right,
[01:24:30.080 --> 01:24:32.360]   but then just completely whiff this side,
[01:24:32.360 --> 01:24:35.200]   or I could do the same on the other.
[01:24:35.200 --> 01:24:38.280]   So this function does not have gradients everywhere.
[01:24:38.280 --> 01:24:39.880]   What it has everywhere is something
[01:24:39.880 --> 01:24:41.840]   called a subgradient.
[01:24:41.840 --> 01:24:45.920]   And more broadly, when you want to go
[01:24:45.920 --> 01:24:47.920]   to high-dimensional cases and you
[01:24:47.920 --> 01:24:50.920]   want to go to functions that are non-convex,
[01:24:50.920 --> 01:24:53.280]   it's something called a subdifferential.
[01:24:53.280 --> 01:24:55.720]   So these subgradients are actually
[01:24:55.720 --> 01:25:00.160]   super, super important for convex analysis,
[01:25:00.160 --> 01:25:02.440]   the study of convex functions, which
[01:25:02.440 --> 01:25:05.680]   is where all the ideas in optimization come from,
[01:25:05.680 --> 01:25:08.840]   all of our ideas of how do we optimize functions,
[01:25:08.840 --> 01:25:10.600]   how do we make--
[01:25:10.600 --> 01:25:13.440]   how do we find the lowest values of functions.
[01:25:13.440 --> 01:25:16.000]   Even when we apply them on non-convex functions,
[01:25:16.000 --> 01:25:18.640]   on generic functions, we use ideas
[01:25:18.640 --> 01:25:21.480]   that came from convex optimization and convex
[01:25:21.480 --> 01:25:22.680]   analysis.
[01:25:22.680 --> 01:25:26.560]   And so this-- and one of those central ideas
[01:25:26.560 --> 01:25:28.320]   is the subgradient.
[01:25:28.320 --> 01:25:30.800]   So it's not only a central--
[01:25:30.800 --> 01:25:32.800]   it's actually what's really being implemented
[01:25:32.800 --> 01:25:35.280]   in something like TensorFlow when they ask--
[01:25:35.280 --> 01:25:38.040]   when they compute gradients for you.
[01:25:38.040 --> 01:25:41.160]   It's also something that is a key theoretical tool.
[01:25:41.160 --> 01:25:42.640]   And the fact that it's being used
[01:25:42.640 --> 01:25:48.960]   is completely swept to the side by the name gradient descent.
[01:25:48.960 --> 01:25:53.280]   And then lastly-- and this is the weakest leg of my three-leg
[01:25:53.280 --> 01:25:55.040]   argument against stochastic gradient descent,
[01:25:55.040 --> 01:25:56.240]   but bear with me.
[01:25:56.240 --> 01:25:59.320]   A stochastic method is one that generates or uses
[01:25:59.320 --> 01:26:01.920]   random variables.
[01:26:01.920 --> 01:26:04.680]   And we just talked about the definition of random variables.
[01:26:04.680 --> 01:26:09.080]   One thing that, depending on your definition of randomness,
[01:26:09.080 --> 01:26:11.640]   really, the things that go on inside of a computer
[01:26:11.640 --> 01:26:14.400]   are at best pseudo-random, which means
[01:26:14.400 --> 01:26:17.640]   they appear random to a person who's not looking very closely,
[01:26:17.640 --> 01:26:20.480]   but they are, in fact, entirely deterministic.
[01:26:20.480 --> 01:26:26.120]   If you were to run your code again in the exact same setting,
[01:26:26.120 --> 01:26:31.160]   then the code would produce the exact same output.
[01:26:31.160 --> 01:26:33.600]   So the way that this is managed is
[01:26:33.600 --> 01:26:36.800]   that there is a seed number that determines
[01:26:36.800 --> 01:26:38.840]   the behavior of your code.
[01:26:38.840 --> 01:26:41.000]   So if you set a seed, the behavior
[01:26:41.000 --> 01:26:44.760]   of stochastic gradient descent becomes entirely deterministic.
[01:26:44.760 --> 01:26:47.960]   So when I-- if I want, I can actually
[01:26:47.960 --> 01:26:52.160]   recreate the exact training process for a network
[01:26:52.160 --> 01:26:55.080]   if I know the seeds for the pseudo-random number
[01:26:55.080 --> 01:26:58.560]   generators that went into training that network.
[01:26:58.560 --> 01:27:02.440]   This is very different from an actual stochastic algorithm
[01:27:02.440 --> 01:27:08.640]   that has actual access to random values, so one that, say,
[01:27:08.640 --> 01:27:10.720]   queries the behavior of this--
[01:27:10.720 --> 01:27:12.280]   there's a famous qubit.
[01:27:12.280 --> 01:27:16.280]   I forget who measures it, but there's a famous quantum system
[01:27:16.280 --> 01:27:18.640]   that produces honest-to-god random values.
[01:27:18.640 --> 01:27:21.320]   And if somebody checked that qubit
[01:27:21.320 --> 01:27:23.080]   before running stochastic gradient descent
[01:27:23.080 --> 01:27:26.000]   and used that in order to run their algorithm,
[01:27:26.000 --> 01:27:29.160]   you'd never be able to guarantee you'd recreated
[01:27:29.160 --> 01:27:32.240]   their training process.
[01:27:32.240 --> 01:27:35.840]   So this is important for a couple of reasons.
[01:27:35.840 --> 01:27:38.080]   And the biggest one is that it's important for debugging.
[01:27:38.080 --> 01:27:40.680]   We can actually-- because the algorithm is deterministic,
[01:27:40.680 --> 01:27:43.120]   we can recreate it, and we can interrogate it
[01:27:43.120 --> 01:27:44.800]   at a later point in time the same way we
[01:27:44.800 --> 01:27:48.880]   can any other deterministic algorithm in a computer.
[01:27:48.880 --> 01:27:49.880]   So we can do testing.
[01:27:49.880 --> 01:27:51.160]   We do unit testing.
[01:27:51.160 --> 01:27:55.960]   We can do searching through and printing statements--
[01:27:55.960 --> 01:27:58.560]   doing print statements to pull out variables
[01:27:58.560 --> 01:28:00.280]   and looking at what their values were.
[01:28:00.280 --> 01:28:03.160]   We can understand what happened during algorithm
[01:28:03.160 --> 01:28:06.360]   with perfect clarity, no stochasticity needed.
[01:28:06.360 --> 01:28:09.720]   Verifying the behavior of a stochastic algorithm with tests
[01:28:09.720 --> 01:28:12.320]   is really, really hard.
[01:28:12.320 --> 01:28:14.080]   And it's actually quite lucky for us
[01:28:14.080 --> 01:28:17.160]   that our stochastic gradient descent is not
[01:28:17.160 --> 01:28:21.360]   as so stochastic that it cannot be made deterministic.
[01:28:21.360 --> 01:28:23.040]   And another actual argument is that you
[01:28:23.040 --> 01:28:28.800]   can remove all the stochasticity from gradient descent by just--
[01:28:28.800 --> 01:28:34.280]   if you pick a fixed order for the values to come in,
[01:28:34.280 --> 01:28:41.280]   and that order is not, say, all the instances of one class,
[01:28:41.280 --> 01:28:42.940]   then all the instances of another class,
[01:28:42.940 --> 01:28:45.360]   all the instances of a third class.
[01:28:45.360 --> 01:28:50.600]   If you just pick that order to be some permutation
[01:28:50.600 --> 01:28:56.040]   of the inputs, then you drop all the stochasticity.
[01:28:56.040 --> 01:28:59.040]   You don't have to randomly load batches and keep flipping coins
[01:28:59.040 --> 01:29:02.000]   to decide which elements go into your gradient descent
[01:29:02.000 --> 01:29:04.040]   and which ones don't on a given batch.
[01:29:04.040 --> 01:29:08.120]   You just go through it in order.
[01:29:08.120 --> 01:29:12.040]   But that order is just--
[01:29:12.040 --> 01:29:15.960]   it just can't be a really, really dumb order.
[01:29:15.960 --> 01:29:18.160]   And this is actually a weird behavior of gradient descent
[01:29:18.160 --> 01:29:19.760]   that people don't understand very well.
[01:29:19.760 --> 01:29:21.320]   That version of it does just as well
[01:29:21.320 --> 01:29:23.120]   as stochastic gradient descent.
[01:29:23.120 --> 01:29:27.260]   And there's basically-- once the order of the data set is set,
[01:29:27.260 --> 01:29:28.520]   there's no randomness.
[01:29:28.520 --> 01:29:32.640]   The algorithm is completely deterministic.
[01:29:32.640 --> 01:29:41.440]   So I have here now a change.org petition, a call to arms
[01:29:41.440 --> 01:29:44.440]   to rename SGD to APSD.
[01:29:44.440 --> 01:29:49.680]   So hopefully, folks will join me in referring it to instead
[01:29:49.680 --> 01:29:54.640]   as approximate pseudorandom subdifferential descent.
[01:29:54.640 --> 01:29:57.320]   Approximate descent being how we get around
[01:29:57.320 --> 01:30:00.800]   the problem of descent not being what we're doing.
[01:30:00.800 --> 01:30:03.220]   Pseudorandom to emphasize that this is not
[01:30:03.220 --> 01:30:04.760]   a stochastic algorithm.
[01:30:04.760 --> 01:30:06.880]   And subdifferential so we actually
[01:30:06.880 --> 01:30:09.920]   name the thing that we are using rather than calling it
[01:30:09.920 --> 01:30:13.280]   a gradient just because that's easier.
[01:30:13.280 --> 01:30:19.720]   So that concludes my pedantic and completely unnecessary rant
[01:30:19.720 --> 01:30:23.120]   about the nature of stochastic gradient descent.
[01:30:23.120 --> 01:30:24.760]   That was amazing.
[01:30:24.760 --> 01:30:26.800]   A bunch of people said in the comments
[01:30:26.800 --> 01:30:29.600]   that you are hilarious, which we agree with.
[01:30:29.600 --> 01:30:32.280]   There's a question in the chat if you want to take it.
[01:30:32.280 --> 01:30:36.680]   Yeah, so Nicola V. asks, what are some pros and cons
[01:30:36.680 --> 01:30:39.120]   of making the training process deterministic
[01:30:39.120 --> 01:30:43.200]   by fixing a, quote, "random seed?"
[01:30:43.200 --> 01:30:50.680]   So the biggest con is that there are some times that you really
[01:30:50.680 --> 01:30:55.880]   do want the network to behave randomly.
[01:30:55.880 --> 01:30:57.560]   So I focused on the fact that if you
[01:30:57.560 --> 01:31:01.000]   wanted to uncover some bugs in your implementation,
[01:31:01.000 --> 01:31:03.720]   you want to be able to recreate a given run in order
[01:31:03.720 --> 01:31:09.160]   to make sure that you could figure out
[01:31:09.160 --> 01:31:11.360]   what caused the problem.
[01:31:11.360 --> 01:31:15.920]   But if you just fix a single random seed,
[01:31:15.920 --> 01:31:19.560]   then you might actually miss that your algorithm doesn't
[01:31:19.560 --> 01:31:22.800]   have the random behavior that you would expect.
[01:31:22.800 --> 01:31:25.000]   So sometimes we can do things like take averages
[01:31:25.000 --> 01:31:28.040]   or expectations or otherwise remove out
[01:31:28.040 --> 01:31:30.600]   the effect of randomness by considering
[01:31:30.600 --> 01:31:32.280]   a large number of examples.
[01:31:32.280 --> 01:31:34.160]   And often, we do want our algorithm
[01:31:34.160 --> 01:31:36.920]   to behave differently when it gets
[01:31:36.920 --> 01:31:39.680]   different values for the seed.
[01:31:39.680 --> 01:31:41.880]   So that would be, I think, the biggest
[01:31:41.880 --> 01:31:45.120]   con for making the training process deterministic
[01:31:45.120 --> 01:31:46.480]   by fixing a random seed.
[01:31:46.480 --> 01:31:54.400]   The other thing is that when--
[01:31:54.400 --> 01:31:56.440]   there's actually been some recent papers that
[01:31:56.440 --> 01:32:01.520]   say that if you treat your random seed as a hyperparameter,
[01:32:01.520 --> 01:32:05.600]   you can get better performance on your validation set.
[01:32:05.600 --> 01:32:07.840]   So it's important that you make sure not
[01:32:07.840 --> 01:32:10.480]   to do that on the test set, because now you're really,
[01:32:10.480 --> 01:32:12.440]   really just overfitting.
[01:32:12.440 --> 01:32:16.680]   But there are aspects of maybe some initializations
[01:32:16.680 --> 01:32:19.800]   of neural networks for certain kinds of problems
[01:32:19.800 --> 01:32:21.800]   are actually better than others.
[01:32:21.800 --> 01:32:24.080]   So instead of just trying multiple runs
[01:32:24.080 --> 01:32:26.920]   with the same hyperparameters, you
[01:32:26.920 --> 01:32:31.240]   might include a single random seed.
[01:32:31.240 --> 01:32:33.400]   Maybe there's an interaction between where
[01:32:33.400 --> 01:32:35.040]   you start on the loss surface, where
[01:32:35.040 --> 01:32:37.400]   you start your neural network, and what your learning
[01:32:37.400 --> 01:32:39.480]   rate is, or how much dropout you do,
[01:32:39.480 --> 01:32:45.000]   or whatever other your atom hyperparameters or whatever.
[01:32:45.000 --> 01:32:49.440]   So that's a somewhat controversial new approach
[01:32:49.440 --> 01:32:50.280]   in machine learning.
[01:32:50.280 --> 01:32:52.160]   People worry that we're really starting
[01:32:52.160 --> 01:32:56.440]   to just chase a few percentage points of accuracy.
[01:32:56.440 --> 01:32:59.600]   But that would be a con for setting a fixed random seed.
[01:32:59.600 --> 01:33:00.360]   You can't do that.
[01:33:00.360 --> 01:33:09.880]   Do you want to take the one in the Q&A section?
[01:33:09.880 --> 01:33:11.240]   Yeah.
[01:33:11.240 --> 01:33:15.080]   Kyle asks, is there a relationship between the fact
[01:33:15.080 --> 01:33:17.200]   that SGD isn't truly a descent method
[01:33:17.200 --> 01:33:19.120]   and the effectiveness of early stopping
[01:33:19.120 --> 01:33:22.280]   when training neural networks?
[01:33:22.280 --> 01:33:24.640]   So not directly.
[01:33:24.640 --> 01:33:28.040]   So early-- so for folks, early stopping
[01:33:28.040 --> 01:33:31.360]   means that you sometimes want to stop training earlier
[01:33:31.360 --> 01:33:33.760]   rather than running forever.
[01:33:33.760 --> 01:33:36.200]   A naive analysis of optimization would
[01:33:36.200 --> 01:33:38.120]   say the longer you can run your algorithm for,
[01:33:38.120 --> 01:33:41.600]   the better you will do, even for stochastic gradient descent.
[01:33:41.600 --> 01:33:43.920]   Even for non-descent methods, longer
[01:33:43.920 --> 01:33:46.600]   tends to just mean better.
[01:33:46.600 --> 01:33:48.040]   Because basically, all our algorithms
[01:33:48.040 --> 01:33:50.840]   are motivated by calculus, which works
[01:33:50.840 --> 01:33:56.120]   when things are infinitely small or run for infinitely long.
[01:33:56.120 --> 01:33:59.320]   But the reason for early stopping is actually not--
[01:33:59.320 --> 01:34:01.880]   is actually that we're optimizing the wrong thing.
[01:34:01.880 --> 01:34:04.240]   We're optimizing the training loss,
[01:34:04.240 --> 01:34:08.560]   whereas the thing we really want to optimize is the test loss.
[01:34:08.560 --> 01:34:10.320]   Or actually, the test loss isn't really
[01:34:10.320 --> 01:34:11.360]   what we want to optimize.
[01:34:11.360 --> 01:34:14.680]   What we really want to optimize is our social or business
[01:34:14.680 --> 01:34:20.720]   outcome of increasing dollars per ad
[01:34:20.720 --> 01:34:22.600]   or decreasing the number of deaths
[01:34:22.600 --> 01:34:24.800]   to heart disease in the world.
[01:34:24.800 --> 01:34:28.320]   And so the training loss that we're actually optimizing
[01:34:28.320 --> 01:34:32.240]   is very far away from the thing that we want to make good.
[01:34:32.240 --> 01:34:34.160]   And so early stopping is one of those things
[01:34:34.160 --> 01:34:37.640]   that you include in order to try and do better on the thing
[01:34:37.640 --> 01:34:39.520]   that you don't have direct access to.
[01:34:39.520 --> 01:34:42.020]   So it's a little bit separate from this point about descent
[01:34:42.020 --> 01:34:43.160]   methods.
[01:34:43.160 --> 01:34:46.440]   But I think that's a little bit of a tricky thing.
[01:34:46.440 --> 01:34:50.640]   As somebody who basically did my PhD in optimization,
[01:34:50.640 --> 01:34:53.300]   optimization is not the most important part
[01:34:53.300 --> 01:34:55.040]   of making your neural network actually do
[01:34:55.040 --> 01:34:56.320]   the thing you want it to do.
[01:34:56.320 --> 01:35:01.320]   It's maybe the most fun part for people who like math, like me.
[01:35:01.320 --> 01:35:03.760]   But it's actually-- you're going to be missing
[01:35:03.760 --> 01:35:06.980]   a lot of the important context, whether it's
[01:35:06.980 --> 01:35:09.000]   out of distribution and test performance
[01:35:09.000 --> 01:35:13.040]   or the overall impact of your algorithm
[01:35:13.040 --> 01:35:17.960]   and its deployability and things like that.
[01:35:17.960 --> 01:35:21.440]   This is not related to your talk a lot,
[01:35:21.440 --> 01:35:25.920]   but you alluded to how the loss function is so far
[01:35:25.920 --> 01:35:28.380]   removed from what we're actually trying to optimize, which
[01:35:28.380 --> 01:35:30.440]   is the business objective.
[01:35:30.440 --> 01:35:34.080]   So do you have advice on bridging that gap
[01:35:34.080 --> 01:35:36.720]   that people, especially in the beginning of their careers
[01:35:36.720 --> 01:35:39.000]   or even for a long, should be thinking about
[01:35:39.000 --> 01:35:41.240]   and how to close that gap?
[01:35:41.240 --> 01:35:43.720]   Yeah, I think in the beginning of your career,
[01:35:43.720 --> 01:35:45.680]   I think the thing that most people are missing
[01:35:45.680 --> 01:35:49.040]   is an understanding of the technical aspects of machine
[01:35:49.040 --> 01:35:49.920]   learning.
[01:35:49.920 --> 01:35:52.440]   And so I think that at the beginning of your career,
[01:35:52.440 --> 01:35:54.080]   you should really focus on understanding
[01:35:54.080 --> 01:35:54.920]   those things better.
[01:35:54.920 --> 01:35:56.440]   Because to be honest, whatever you
[01:35:56.440 --> 01:35:58.680]   were doing when you weren't learning machine learning
[01:35:58.680 --> 01:36:02.080]   probably gave you the right--
[01:36:02.080 --> 01:36:04.720]   the sense for whatever it is that you're going to apply it
[01:36:04.720 --> 01:36:06.080]   for.
[01:36:06.080 --> 01:36:07.680]   I did a biology undergrad.
[01:36:07.680 --> 01:36:10.400]   So when I go to do machine learning applied to biology
[01:36:10.400 --> 01:36:13.760]   context, I have context that helps
[01:36:13.760 --> 01:36:16.080]   me understand what the real goals are and what the failure
[01:36:16.080 --> 01:36:18.320]   modes are.
[01:36:18.320 --> 01:36:21.000]   So at the beginning of your career,
[01:36:21.000 --> 01:36:23.600]   I think you should really focus on getting
[01:36:23.600 --> 01:36:24.840]   these technical aspects right.
[01:36:24.840 --> 01:36:25.960]   Because they can bite you.
[01:36:25.960 --> 01:36:30.280]   And when they do bite you, they bite hard.
[01:36:30.280 --> 01:36:32.000]   But for more people who are more experts
[01:36:32.000 --> 01:36:35.560]   and they're at the point they're actually deploying models,
[01:36:35.560 --> 01:36:42.760]   I would say the only solution is careful thought.
[01:36:42.760 --> 01:36:45.680]   Maybe spend less time on fiddling
[01:36:45.680 --> 01:36:48.320]   with your hyperparameters and more time
[01:36:48.320 --> 01:36:51.280]   thinking about what will this model look
[01:36:51.280 --> 01:36:53.440]   like when it's in production.
[01:36:53.440 --> 01:36:56.880]   Get models to production as quickly as possible,
[01:36:56.880 --> 01:36:59.920]   unless you're in a security critical regime like medicine.
[01:36:59.920 --> 01:37:04.080]   And then learn from what happens when the model is deployed.
[01:37:04.080 --> 01:37:06.360]   There's continuous testing strategies
[01:37:06.360 --> 01:37:12.160]   to ensure that the model is not starting to fail completely.
[01:37:12.160 --> 01:37:15.040]   There's a lot of people who are doing work in fairness
[01:37:15.040 --> 01:37:16.040]   and ethics in AI.
[01:37:16.040 --> 01:37:20.400]   This is another aspect in which our loss, which encourages us
[01:37:20.400 --> 01:37:22.600]   to do well in the aggregate, can cause
[01:37:22.600 --> 01:37:25.560]   us to do things that we actually did not want to do,
[01:37:25.560 --> 01:37:28.360]   which is to fail for groups that are underrepresented
[01:37:28.360 --> 01:37:30.920]   in the data set.
[01:37:30.920 --> 01:37:34.640]   And so to look out for that, among all the many other
[01:37:34.640 --> 01:37:37.320]   things that fairness and ethics researchers
[01:37:37.320 --> 01:37:40.480]   have pointed out as problems with machine learning
[01:37:40.480 --> 01:37:42.360]   algorithms.
[01:37:42.360 --> 01:37:43.720]   So yeah, those would be, I think,
[01:37:43.720 --> 01:37:47.320]   my pieces of advice for people who are out there deploying.
[01:37:47.320 --> 01:37:50.000]   But to be honest, my expertise is
[01:37:50.000 --> 01:37:53.960]   in the actual understanding of the mathematics
[01:37:53.960 --> 01:37:56.400]   and the pipeline of training the model.
[01:37:56.400 --> 01:38:00.240]   And so that's where I have the best advice to give.
[01:38:00.240 --> 01:38:02.120]   Thanks.
[01:38:02.120 --> 01:38:04.520]   Another thing that you could do to bridge
[01:38:04.520 --> 01:38:06.800]   the gap between business objectives and the loss
[01:38:06.800 --> 01:38:09.240]   function is to capture more things that
[01:38:09.240 --> 01:38:12.200]   are closer to the business objectives in your training
[01:38:12.200 --> 01:38:13.640]   data.
[01:38:13.640 --> 01:38:16.480]   And make sure your training data captures
[01:38:16.480 --> 01:38:19.800]   all the features or all the variables that
[01:38:19.800 --> 01:38:21.160]   affect your business objectives.
[01:38:21.160 --> 01:38:23.000]   And then you'll optimize the right things.
[01:38:23.000 --> 01:38:25.160]   And somehow in this convoluted process,
[01:38:25.160 --> 01:38:29.800]   you'll optimize your business objective.
[01:38:29.800 --> 01:38:31.560]   Yes, Charles, you were saying something?
[01:38:31.560 --> 01:38:34.040]   Oh, no, yeah, definitely.
[01:38:34.040 --> 01:38:37.280]   Quality of the data comes absolutely first.
[01:38:37.280 --> 01:38:38.760]   This is something that I think gets
[01:38:38.760 --> 01:38:42.040]   lost on folks with an academic background like me,
[01:38:42.040 --> 01:38:46.400]   where the data set is some fixed benchmark usually.
[01:38:46.400 --> 01:38:48.520]   And the idea that we need to think very carefully
[01:38:48.520 --> 01:38:51.760]   about where the data came from and what that collection
[01:38:51.760 --> 01:38:58.400]   process implies about what the test distribution is
[01:38:58.400 --> 01:39:01.360]   that this thing might be optimizing.
[01:39:01.360 --> 01:39:03.840]   Yeah, that's an important aspect that
[01:39:03.840 --> 01:39:07.040]   gets missed a lot in academia, in part
[01:39:07.040 --> 01:39:09.200]   because it's really hard to say anything satisfyingly
[01:39:09.200 --> 01:39:10.880]   academic about it.
[01:39:10.880 --> 01:39:13.760]   Yeah, thanks.
[01:39:13.760 --> 01:39:16.560]   All right, do people have any more questions?
[01:39:16.560 --> 01:39:20.120]   I'm going to give you guys 30 more seconds to ask Charles
[01:39:20.120 --> 01:39:22.920]   or Pranay, who's still online, too, to ask them
[01:39:22.920 --> 01:39:26.280]   any questions that you have.
[01:39:26.280 --> 01:39:29.160]   And until then, Charles, do you have another talk for us
[01:39:29.160 --> 01:39:31.800]   planned or part two?
[01:39:31.800 --> 01:39:34.760]   Yeah, I'll be at the next salon for sure.
[01:39:34.760 --> 01:39:36.440]   I've got a couple of possible ideas,
[01:39:36.440 --> 01:39:39.080]   maybe something about the singular value decomposition.
[01:39:39.080 --> 01:39:40.840]   I think that's what I'll cover.
[01:39:40.840 --> 01:39:43.400]   Nice, thanks.
[01:39:43.400 --> 01:39:48.960]   All right, so our next salon, we tried this 9 AM time
[01:39:48.960 --> 01:39:52.040]   to get some of our European friends
[01:39:52.040 --> 01:39:56.160]   to join us because they always say that the 5 PM time normally
[01:39:56.160 --> 01:39:57.840]   doesn't work for them.
[01:39:57.840 --> 01:40:01.160]   And we're going to do some analysis on what time works
[01:40:01.160 --> 01:40:03.040]   better for most people.
[01:40:03.040 --> 01:40:08.200]   And we'll be back in two weeks, either on Tuesday at 5 PM PST,
[01:40:08.200 --> 01:40:13.880]   or we'll be back on Thursday at 9 AM, same time, in two weeks.
[01:40:13.880 --> 01:40:15.720]   So I guess we'll see you then.
[01:40:15.720 --> 01:40:17.920]   Thank you, Charles, and thank you, Pranay.
[01:40:17.920 --> 01:40:19.920]   Thank you, Kayla, for helping.
[01:40:19.920 --> 01:40:21.360]   And thank you, everyone who joined.
[01:40:21.360 --> 01:40:24.480]   We really appreciate it.
[01:40:24.480 --> 01:40:26.200]   Take care.

