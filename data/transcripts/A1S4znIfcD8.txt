
[00:00:00.000 --> 00:00:03.360]   Today we're going to talk about the big picture.
[00:00:03.360 --> 00:00:04.520]   What is machine learning?
[00:00:04.520 --> 00:00:05.920]   What is deep learning?
[00:00:05.920 --> 00:00:07.120]   How does it really work?
[00:00:07.120 --> 00:00:08.760]   And where can we apply it?
[00:00:08.760 --> 00:00:11.600]   And unlike some of the other videos that we're doing here,
[00:00:11.600 --> 00:00:13.120]   this isn't just for engineers.
[00:00:13.120 --> 00:00:15.400]   This is really for anyone that wants
[00:00:15.400 --> 00:00:17.920]   to get a deeper understanding about how machine learning
[00:00:17.920 --> 00:00:19.400]   actually works.
[00:00:19.400 --> 00:00:21.040]   And there's tons of videos out there
[00:00:21.040 --> 00:00:24.200]   that talk about various aspects of machine learning.
[00:00:24.200 --> 00:00:26.480]   But the gap that I want to fill in
[00:00:26.480 --> 00:00:29.960]   is really showing people where they can apply machine
[00:00:29.960 --> 00:00:30.520]   learning.
[00:00:30.520 --> 00:00:32.360]   Because it applies to so many things,
[00:00:32.360 --> 00:00:35.440]   but it doesn't apply to every single thing on the planet.
[00:00:35.440 --> 00:00:37.860]   And I think you really need to kind of have a sense of how
[00:00:37.860 --> 00:00:39.800]   it actually works behind the scenes.
[00:00:39.800 --> 00:00:41.920]   Because if you just think of it as magic,
[00:00:41.920 --> 00:00:44.400]   it's really unclear where I should be thinking,
[00:00:44.400 --> 00:00:46.120]   ah, that's a machine learning problem,
[00:00:46.120 --> 00:00:47.920]   and that's not a machine learning problem.
[00:00:47.920 --> 00:00:50.720]   We have to acknowledge there's an incredible amount of hype
[00:00:50.720 --> 00:00:52.200]   around machine learning right now.
[00:00:52.200 --> 00:00:53.760]   And one of the manifestations of that,
[00:00:53.760 --> 00:00:55.380]   maybe while you're watching this video,
[00:00:55.380 --> 00:00:57.040]   is because the people that understand
[00:00:57.040 --> 00:01:00.000]   how to do machine learning are being paid huge salaries right
[00:01:00.000 --> 00:01:01.320]   now.
[00:01:01.320 --> 00:01:03.520]   Startups are being acquired left and right,
[00:01:03.520 --> 00:01:05.400]   not for their technology, but just
[00:01:05.400 --> 00:01:09.000]   for their machine learning and deep learning expertise.
[00:01:09.000 --> 00:01:10.760]   Even McKinsey thinks this is going
[00:01:10.760 --> 00:01:12.840]   to be a huge market, not tomorrow,
[00:01:12.840 --> 00:01:16.600]   but in the near, near future.
[00:01:16.600 --> 00:01:19.160]   So I think it's important to back up and ask ourselves
[00:01:19.160 --> 00:01:21.660]   and talk about, what can machine learning really
[00:01:21.660 --> 00:01:24.380]   do right now, today?
[00:01:24.380 --> 00:01:27.080]   The best resource for all the applications
[00:01:27.080 --> 00:01:29.200]   is a blog written by Siobhan Zillis,
[00:01:29.200 --> 00:01:31.440]   where she covers this in depth.
[00:01:31.440 --> 00:01:34.120]   But I want to go through a range of the applications that
[00:01:34.120 --> 00:01:35.520]   are out there.
[00:01:35.520 --> 00:01:37.940]   So first of all, the TSA now says
[00:01:37.940 --> 00:01:41.760]   that deep learning can find weapons on passengers
[00:01:41.760 --> 00:01:44.520]   better than human agents.
[00:01:44.520 --> 00:01:46.760]   Deep learning can count your cells,
[00:01:46.760 --> 00:01:49.420]   and it can look for cancer in a biopsy.
[00:01:49.420 --> 00:01:52.840]   It can find endangered animals in aerial photos.
[00:01:52.840 --> 00:01:57.720]   It can automatically detect weeds on farms from tractors.
[00:01:57.720 --> 00:01:59.800]   And it can help you build crazy robots
[00:01:59.800 --> 00:02:02.000]   that impress your friends.
[00:02:02.000 --> 00:02:05.320]   So all these different examples come from different industries
[00:02:05.320 --> 00:02:09.060]   and involve incredibly different types of inputs and outputs.
[00:02:09.060 --> 00:02:11.520]   So you might be really surprised to learn
[00:02:11.520 --> 00:02:14.560]   that machine learning, the whole discipline,
[00:02:14.560 --> 00:02:19.640]   actually has an incredibly restrictive API, or data type,
[00:02:19.640 --> 00:02:21.960]   that it needs for inputs and outputs.
[00:02:21.960 --> 00:02:26.560]   And so actually getting things like audio and images and text
[00:02:26.560 --> 00:02:29.080]   into the format that machine learning takes,
[00:02:29.080 --> 00:02:31.400]   and actually interpreting the very restrictive format
[00:02:31.400 --> 00:02:34.160]   that it outputs for your application,
[00:02:34.160 --> 00:02:36.040]   is a huge piece of machine learning
[00:02:36.040 --> 00:02:37.560]   that no one really talks about.
[00:02:37.560 --> 00:02:38.320]   And it's what we're going to talk
[00:02:38.320 --> 00:02:39.960]   about for the rest of this video.
[00:02:39.960 --> 00:02:41.660]   First, let's get our definition straight,
[00:02:41.660 --> 00:02:43.840]   because there's a lot of confusion.
[00:02:43.840 --> 00:02:46.420]   So deep learning is a type of machine learning,
[00:02:46.420 --> 00:02:49.800]   maybe the most exciting type of machine learning right now.
[00:02:49.800 --> 00:02:53.720]   Machine learning is a discipline of artificial intelligence,
[00:02:53.720 --> 00:02:56.440]   probably the most exciting field in artificial intelligence
[00:02:56.440 --> 00:02:57.560]   right now.
[00:02:57.560 --> 00:02:59.800]   And so all of the use cases that I gave
[00:02:59.800 --> 00:03:02.240]   are actually machine learning problems.
[00:03:02.240 --> 00:03:05.440]   And what we care about today is machine learning.
[00:03:05.440 --> 00:03:08.480]   Most AI departments right now focus on machine learning,
[00:03:08.480 --> 00:03:11.520]   because it's the part of AI that's really working.
[00:03:11.520 --> 00:03:15.080]   I think of machine learning as statistics applied to AI.
[00:03:15.080 --> 00:03:17.440]   So here's the canonical machine learning problem.
[00:03:17.440 --> 00:03:18.980]   We have a picture of a cat.
[00:03:18.980 --> 00:03:21.000]   We want to do some black magic and somehow
[00:03:21.000 --> 00:03:23.400]   classify our picture as a cat.
[00:03:23.400 --> 00:03:24.840]   So how does that work?
[00:03:24.840 --> 00:03:26.400]   So in order to answer that question,
[00:03:26.400 --> 00:03:29.280]   let's back up a second and talk about the canonical statistics
[00:03:29.280 --> 00:03:30.080]   problem.
[00:03:30.080 --> 00:03:31.840]   There's many data sets I could have used,
[00:03:31.840 --> 00:03:35.400]   but for some reason, I used a data set of baby chickens,
[00:03:35.400 --> 00:03:38.000]   where I have their ages and weight.
[00:03:38.000 --> 00:03:39.740]   In machine learning, these examples
[00:03:39.740 --> 00:03:41.400]   would be called training data.
[00:03:41.400 --> 00:03:43.640]   Imagine we're a farmer, and we want
[00:03:43.640 --> 00:03:46.880]   to predict from our data that we've collected,
[00:03:46.880 --> 00:03:49.660]   if we have a baby chicken that's 18 days old,
[00:03:49.660 --> 00:03:52.140]   how much would we expect it to weigh?
[00:03:52.140 --> 00:03:53.660]   Here we're going to build a model
[00:03:53.660 --> 00:03:57.100]   to fit to our training data to answer that question.
[00:03:57.100 --> 00:03:59.900]   You may not have had exactly this problem before,
[00:03:59.900 --> 00:04:01.980]   but you may have done something like this.
[00:04:01.980 --> 00:04:04.060]   You can actually do it in Excel, and it's
[00:04:04.060 --> 00:04:05.740]   called linear regression.
[00:04:05.740 --> 00:04:08.300]   If you've ever made a trend line through your data,
[00:04:08.300 --> 00:04:11.260]   it's probably using linear regression.
[00:04:11.260 --> 00:04:13.420]   We can actually plot these points,
[00:04:13.420 --> 00:04:16.260]   and these are the ages and the weights of the chickens.
[00:04:16.260 --> 00:04:20.900]   And this is our training data that we use to build a model.
[00:04:20.900 --> 00:04:23.900]   This line, this trend line, this linear regression,
[00:04:23.900 --> 00:04:26.740]   actually makes predictions for any age.
[00:04:26.740 --> 00:04:29.420]   So we can look at 18 days on the x-axis
[00:04:29.420 --> 00:04:33.020]   and see that the line is at 170 on the y-axis.
[00:04:33.020 --> 00:04:35.980]   So our model is predicting that a baby chicken will weigh
[00:04:35.980 --> 00:04:40.220]   170 grams when it's 18 days old.
[00:04:40.220 --> 00:04:42.180]   Now, that's linear regression, but we
[00:04:42.180 --> 00:04:46.200]   can do fancier things, too, even with this tiny data set.
[00:04:46.200 --> 00:04:48.780]   In this case, I fit in an exponential curve,
[00:04:48.780 --> 00:04:51.700]   and it makes a slightly different regression.
[00:04:51.700 --> 00:04:53.740]   You might ask yourself, do you think this line
[00:04:53.740 --> 00:04:56.580]   models the data better?
[00:04:56.580 --> 00:04:58.700]   OK, now here's another valid regression I did,
[00:04:58.700 --> 00:05:01.760]   where I fit a more complicated equation.
[00:05:01.760 --> 00:05:04.500]   This line doesn't look to me like it models the data
[00:05:04.500 --> 00:05:07.460]   very well, but it goes through every point we have,
[00:05:07.460 --> 00:05:11.220]   meaning that it models the training data perfectly.
[00:05:11.220 --> 00:05:14.260]   So what's happening here is something called overfitting.
[00:05:14.260 --> 00:05:18.180]   My complicated line went through all the points perfectly,
[00:05:18.180 --> 00:05:20.720]   but it won't generalize as well to new points
[00:05:20.720 --> 00:05:22.440]   that we haven't seen before.
[00:05:22.440 --> 00:05:24.560]   And as models become more complicated,
[00:05:24.560 --> 00:05:26.420]   they tend to overfit.
[00:05:26.420 --> 00:05:28.720]   We don't usually worry too much about overfitting
[00:05:28.720 --> 00:05:31.660]   in a statistics 101 class with linear regression,
[00:05:31.660 --> 00:05:34.100]   because it's such a simple model that it's hard for it
[00:05:34.100 --> 00:05:35.360]   to overfit.
[00:05:35.360 --> 00:05:37.620]   But as our equations get more complicated
[00:05:37.620 --> 00:05:39.440]   and our data gets more complicated,
[00:05:39.440 --> 00:05:42.560]   overfitting becomes more and more of an issue.
[00:05:42.560 --> 00:05:45.040]   The graph on the left is modeling the data
[00:05:45.040 --> 00:05:46.760]   in a simple way, but it's probably
[00:05:46.760 --> 00:05:50.160]   missing some of the pattern in the data.
[00:05:50.160 --> 00:05:53.040]   The graph on the right is actually touching all the dots,
[00:05:53.040 --> 00:05:55.520]   meaning that it's perfectly fitting the training data,
[00:05:55.520 --> 00:05:58.080]   but it's probably overfitting the training data.
[00:05:58.080 --> 00:06:00.420]   The graph in the middle gets closer to the training data
[00:06:00.420 --> 00:06:02.960]   points, but doesn't get as close as the graph on the right,
[00:06:02.960 --> 00:06:06.320]   but maybe models the data better than the graph on the left.
[00:06:06.320 --> 00:06:08.320]   As we collect more and more data in the world,
[00:06:08.320 --> 00:06:11.600]   we're able to build more and more complicated models.
[00:06:11.600 --> 00:06:13.880]   Deep learning really describes a trend
[00:06:13.880 --> 00:06:16.560]   towards extremely complicated equations
[00:06:16.560 --> 00:06:18.480]   with potentially millions of parameters
[00:06:18.480 --> 00:06:21.000]   and millions of data points.
[00:06:21.000 --> 00:06:23.120]   So how complicated should we make our models?
[00:06:23.120 --> 00:06:26.840]   And how should we constrain them to keep them from overfitting?
[00:06:26.840 --> 00:06:30.160]   That's what machine learning research is really all about.
[00:06:30.160 --> 00:06:33.120]   So these single graphs might seem like toy problems,
[00:06:33.120 --> 00:06:35.240]   but predicting one variable from one other variable
[00:06:35.240 --> 00:06:37.260]   can get really complicated on its own.
[00:06:37.260 --> 00:06:39.600]   If you could predict where this graph of the stock market
[00:06:39.600 --> 00:06:40.960]   is going better than anyone else,
[00:06:40.960 --> 00:06:42.400]   you can make a ton of money.
[00:06:42.400 --> 00:06:44.880]   So another fundamental question in deep learning
[00:06:44.880 --> 00:06:47.240]   is actually, what are we optimizing?
[00:06:47.240 --> 00:06:50.200]   These two lines both try to go as close as possible
[00:06:50.200 --> 00:06:53.440]   to the points in the graph or all the points in the training
[00:06:53.440 --> 00:06:57.360]   data, but they actually have a different definition of close.
[00:06:57.360 --> 00:06:59.280]   One of the lines here is the line
[00:06:59.280 --> 00:07:02.320]   with the smallest sum of the vertical distance
[00:07:02.320 --> 00:07:05.160]   from the line to all the points, while the other
[00:07:05.160 --> 00:07:06.940]   is optimizing the smallest squared sum
[00:07:06.940 --> 00:07:08.640]   of the vertical distance.
[00:07:08.640 --> 00:07:10.280]   It might seem like a small difference,
[00:07:10.280 --> 00:07:13.240]   but clearly these graphs look very different.
[00:07:13.240 --> 00:07:15.080]   If you just do a default regression,
[00:07:15.080 --> 00:07:17.760]   you're usually optimizing the square of the distance, which
[00:07:17.760 --> 00:07:20.680]   you also may know as the squared error.
[00:07:20.680 --> 00:07:23.860]   Can you tell which line is optimizing which metric?
[00:07:23.860 --> 00:07:25.440]   If you want, you could pause the video
[00:07:25.440 --> 00:07:27.520]   and think about it, because I'm about to tell you.
[00:07:27.520 --> 00:07:34.120]   OK, you're back.
[00:07:34.120 --> 00:07:36.080]   I'm sure you thought deeply about which
[00:07:36.080 --> 00:07:38.240]   is optimizing squared error and which is optimizing
[00:07:38.240 --> 00:07:39.560]   absolute error.
[00:07:39.560 --> 00:07:42.720]   And you probably concluded that the left
[00:07:42.720 --> 00:07:44.560]   is optimizing the squared error and the right
[00:07:44.560 --> 00:07:46.960]   is optimizing the absolute error.
[00:07:46.960 --> 00:07:48.600]   When you optimize the squared error,
[00:07:48.600 --> 00:07:51.320]   the outliers actually affect the line a lot more,
[00:07:51.320 --> 00:07:54.680]   pulling it away from the majority of points.
[00:07:54.680 --> 00:07:57.640]   To me, the left doesn't look as good of a fit
[00:07:57.640 --> 00:07:59.200]   as the graph on the right.
[00:07:59.200 --> 00:08:02.820]   But actually, if you model this in Excel or any normal stats
[00:08:02.820 --> 00:08:05.000]   program, it'll probably default to the graph
[00:08:05.000 --> 00:08:07.360]   on the left, which actually might not be what you really
[00:08:07.360 --> 00:08:08.160]   want.
[00:08:08.160 --> 00:08:10.160]   So which model is really better?
[00:08:10.160 --> 00:08:11.960]   It actually depends on what you're doing
[00:08:11.960 --> 00:08:14.800]   and what's happening downstream from your model.
[00:08:14.800 --> 00:08:16.480]   In all the graphs we've looked at so far,
[00:08:16.480 --> 00:08:19.000]   we only have one input and one output.
[00:08:19.000 --> 00:08:21.640]   But usually, we have more than one input.
[00:08:21.640 --> 00:08:24.760]   So here we have not just the age of the chicken.
[00:08:24.760 --> 00:08:28.320]   We add the type of diet that it was exposed to encoded
[00:08:28.320 --> 00:08:29.720]   as a number.
[00:08:29.720 --> 00:08:33.120]   Now, from those two variables, we want to predict weight.
[00:08:33.120 --> 00:08:35.720]   This is something we actually still can do in Excel.
[00:08:35.720 --> 00:08:38.120]   We have input training data and output.
[00:08:38.120 --> 00:08:39.880]   We can use linear regression, but it's
[00:08:39.880 --> 00:08:41.920]   harder to visualize what's going on.
[00:08:41.920 --> 00:08:44.160]   This makes it actually easier to overfit,
[00:08:44.160 --> 00:08:46.160]   and we're getting closer to what's traditionally
[00:08:46.160 --> 00:08:49.800]   thought of as machine learning and deep learning.
[00:08:49.800 --> 00:08:52.460]   Back to our cat classification problem.
[00:08:52.460 --> 00:08:54.280]   What does this have to do with what
[00:08:54.280 --> 00:08:56.440]   we've been talking about so far?
[00:08:56.440 --> 00:08:59.900]   Here's the statistics API, and it's very strict.
[00:08:59.900 --> 00:09:03.480]   We have an input that has to be a fixed list of numbers.
[00:09:03.480 --> 00:09:06.180]   In our first example, it was a single number.
[00:09:06.180 --> 00:09:08.200]   In our multivariable regression example,
[00:09:08.200 --> 00:09:12.360]   it was two numbers, chick age and chick diet.
[00:09:12.360 --> 00:09:15.520]   Our model also outputs a fixed list of numbers.
[00:09:15.520 --> 00:09:20.000]   So far, all of our models have output only one number.
[00:09:20.000 --> 00:09:22.880]   The way we generate our statistical models
[00:09:22.880 --> 00:09:25.600]   is by feeding in a set of examples.
[00:09:25.600 --> 00:09:28.680]   In machine learning, this is usually called training data.
[00:09:28.680 --> 00:09:33.320]   And the examples always have the fixed inputs and outputs.
[00:09:33.320 --> 00:09:37.300]   In the case of the chicks, we fed in five lists of age
[00:09:37.300 --> 00:09:41.780]   and weight and built a linear regression model.
[00:09:41.780 --> 00:09:44.740]   It turns out, and a lot of people are surprised by this,
[00:09:44.740 --> 00:09:48.540]   the machine learning API is identical to the statistics
[00:09:48.540 --> 00:09:49.820]   API.
[00:09:49.820 --> 00:09:51.780]   We usually have more than one input,
[00:09:51.780 --> 00:09:53.780]   and we often have more than one output.
[00:09:53.780 --> 00:09:56.220]   But the inputs and outputs still have
[00:09:56.220 --> 00:09:59.580]   to be fixed length lists of numbers.
[00:09:59.580 --> 00:10:02.940]   And behind the scenes, we're still just generating a model
[00:10:02.940 --> 00:10:04.880]   from the training data.
[00:10:04.880 --> 00:10:06.600]   Just like with linear regression,
[00:10:06.600 --> 00:10:08.600]   the model is just an equation.
[00:10:08.600 --> 00:10:10.440]   But in machine learning, we think of it
[00:10:10.440 --> 00:10:13.280]   as a very complicated equation.
[00:10:13.280 --> 00:10:15.200]   We search for the best model according
[00:10:15.200 --> 00:10:18.000]   to some metric or some loss function.
[00:10:18.000 --> 00:10:21.820]   But often, that metric is just squared error in the same way
[00:10:21.820 --> 00:10:24.520]   that we used it for linear regression.
[00:10:24.520 --> 00:10:27.680]   Training is just searching for the best model
[00:10:27.680 --> 00:10:31.320]   according to some metric or loss function.
[00:10:31.320 --> 00:10:33.280]   And often, that loss function is just
[00:10:33.280 --> 00:10:35.320]   squared error, the same as we normally
[00:10:35.320 --> 00:10:37.400]   use for linear regression.
[00:10:37.400 --> 00:10:39.560]   So what are some of the machine learning techniques
[00:10:39.560 --> 00:10:41.160]   besides linear regression?
[00:10:41.160 --> 00:10:44.160]   And why would we pick one over the other?
[00:10:44.160 --> 00:10:46.520]   It actually depends a lot on what kind of overfitting
[00:10:46.520 --> 00:10:49.320]   we're worried about, how much training data we have,
[00:10:49.320 --> 00:10:52.320]   and how many input and output variables there are.
[00:10:52.320 --> 00:10:56.720]   A very popular and useful Python library called scikit-learn
[00:10:56.720 --> 00:11:00.800]   that you may have used actually built a fantastic flow chart
[00:11:00.800 --> 00:11:03.480]   that summarizes five years of grad school
[00:11:03.480 --> 00:11:06.000]   and helps you pick the best possible model based
[00:11:06.000 --> 00:11:08.400]   on these aspects of your training data.
[00:11:08.400 --> 00:11:10.360]   Another way to think about what model to use
[00:11:10.360 --> 00:11:12.840]   is to look at what other people are doing.
[00:11:12.840 --> 00:11:16.200]   Kaggle, a super cool data science platform,
[00:11:16.200 --> 00:11:19.160]   had a survey of all its machine learning practitioners
[00:11:19.160 --> 00:11:21.080]   and asked them which techniques they
[00:11:21.080 --> 00:11:23.580]   use in their day-to-day jobs.
[00:11:23.580 --> 00:11:25.740]   You've probably heard lately that neural networks are
[00:11:25.740 --> 00:11:28.480]   becoming popular, but good old logistic regression,
[00:11:28.480 --> 00:11:30.880]   which is just a modification of the linear regression
[00:11:30.880 --> 00:11:33.160]   we were talking about earlier, is still really the most
[00:11:33.160 --> 00:11:35.000]   commonly used technique.
[00:11:35.000 --> 00:11:37.040]   Another popular machine learning technique
[00:11:37.040 --> 00:11:39.680]   you may have heard of is called decision trees.
[00:11:39.680 --> 00:11:41.260]   One thing I like about this algorithm
[00:11:41.260 --> 00:11:43.080]   is it's really easy to explain.
[00:11:43.080 --> 00:11:45.320]   What it does is it picks one of the input variables
[00:11:45.320 --> 00:11:47.600]   and it chooses a threshold to say
[00:11:47.600 --> 00:11:49.720]   if the variables above the threshold go left
[00:11:49.720 --> 00:11:52.400]   and if the variables below the threshold go right.
[00:11:52.400 --> 00:11:54.040]   And then at the leaves of this tree,
[00:11:54.040 --> 00:11:56.880]   it predicts the specific number for the output.
[00:11:56.880 --> 00:12:00.200]   A popular and useful variant of the decision tree algorithm
[00:12:00.200 --> 00:12:03.360]   is called decision forest or random forest.
[00:12:03.360 --> 00:12:05.240]   When we use decision forest, we actually
[00:12:05.240 --> 00:12:08.160]   build up hundreds or thousands or tens of thousands
[00:12:08.160 --> 00:12:09.920]   of decision trees.
[00:12:09.920 --> 00:12:12.720]   We let each of those trees make a prediction,
[00:12:12.720 --> 00:12:16.720]   and then we aggregate the predictions in some way.
[00:12:16.720 --> 00:12:19.240]   Neural networks are another type of model
[00:12:19.240 --> 00:12:21.360]   that recently has become very popular
[00:12:21.360 --> 00:12:22.840]   and has had a lot of breakthroughs.
[00:12:22.840 --> 00:12:25.560]   So we're going to go really deep on it in this video series.
[00:12:25.560 --> 00:12:27.320]   And when we talk about deep learning,
[00:12:27.320 --> 00:12:30.560]   we're usually talking about neural networks.
[00:12:30.560 --> 00:12:32.840]   But one thing I really want to demystify here
[00:12:32.840 --> 00:12:35.800]   is despite the evocative name, neural networks,
[00:12:35.800 --> 00:12:38.500]   they're really just an equation like anything else.
[00:12:38.500 --> 00:12:40.320]   And the inputs and the outputs are just
[00:12:40.320 --> 00:12:42.800]   like all these other machine learning techniques
[00:12:42.800 --> 00:12:46.600]   and statistics techniques that we've been talking about.
[00:12:46.600 --> 00:12:49.560]   OK, so how do we get this cat problem
[00:12:49.560 --> 00:12:52.760]   into this machine learning API that we keep talking about?
[00:12:52.760 --> 00:12:56.080]   This picture of a cat is not a fixed array of numbers,
[00:12:56.080 --> 00:12:59.720]   and the output is definitely not a fixed array of numbers.
[00:12:59.720 --> 00:13:02.040]   So first, we have to turn the cat image
[00:13:02.040 --> 00:13:03.680]   into a fixed length of numbers.
[00:13:03.680 --> 00:13:06.280]   And we can do this by taking the red, green, blue values
[00:13:06.280 --> 00:13:10.440]   from each pixel and putting them into a long list.
[00:13:10.440 --> 00:13:13.680]   Then we have to make our network output something
[00:13:13.680 --> 00:13:17.200]   we can interpret as a label like cat.
[00:13:17.200 --> 00:13:19.680]   One way to do this is to set up our network
[00:13:19.680 --> 00:13:22.380]   to output a number for any particular type of image
[00:13:22.380 --> 00:13:23.920]   we might see.
[00:13:23.920 --> 00:13:26.200]   Here we have a network that's outputting four numbers--
[00:13:26.200 --> 00:13:29.960]   a cat score, a fish score, a dog score, and an other score.
[00:13:29.960 --> 00:13:32.680]   And we're going to interpret one in the cat score
[00:13:32.680 --> 00:13:35.360]   to mean that the image is a cat.
[00:13:35.360 --> 00:13:37.280]   Now we have a machine learning problem
[00:13:37.280 --> 00:13:39.160]   in the machine learning API.
[00:13:39.160 --> 00:13:42.040]   So now behind the scenes, we can build a neural network,
[00:13:42.040 --> 00:13:43.920]   we can build linear regression, or we
[00:13:43.920 --> 00:13:46.040]   can build a decision tree or anything else
[00:13:46.040 --> 00:13:50.200]   we might want to solve this machine learning problem.
[00:13:50.200 --> 00:13:53.500]   Actually, though, there's one more important step,
[00:13:53.500 --> 00:13:56.580]   which is that we need to find more images of cats.
[00:13:56.580 --> 00:13:58.740]   This is called training data collection,
[00:13:58.740 --> 00:14:01.040]   and it's often the most important and usually
[00:14:01.040 --> 00:14:02.700]   overlooked step.
[00:14:02.700 --> 00:14:05.220]   These models are just mathematical equations.
[00:14:05.220 --> 00:14:08.180]   They have no common sense built into them.
[00:14:08.180 --> 00:14:11.380]   All they can do is find patterns in the numbers.
[00:14:11.380 --> 00:14:15.060]   So for example, if all the cats in our training data
[00:14:15.060 --> 00:14:17.340]   look the same, no machine learning model
[00:14:17.340 --> 00:14:20.520]   will be able to figure out what a cat actually is.
[00:14:20.520 --> 00:14:22.680]   We also need to find examples of anything else
[00:14:22.680 --> 00:14:24.320]   that we want to classify.
[00:14:24.320 --> 00:14:26.920]   Training data is actually so important to machine learning
[00:14:26.920 --> 00:14:28.860]   and so important to me personally
[00:14:28.860 --> 00:14:30.480]   that over a decade ago, I started
[00:14:30.480 --> 00:14:32.340]   a company called Figure 8 that helps
[00:14:32.340 --> 00:14:34.600]   companies collect training data.
[00:14:34.600 --> 00:14:36.520]   If you need more training data, and if you're
[00:14:36.520 --> 00:14:38.020]   doing machine learning, you probably
[00:14:38.020 --> 00:14:39.680]   need more training data, you actually
[00:14:39.680 --> 00:14:41.700]   could check out Figure 8 and use it,
[00:14:41.700 --> 00:14:45.360]   or you could try one of its vastly inferior competitors.
[00:14:45.360 --> 00:14:48.020]   It might seem trivial to turn images
[00:14:48.020 --> 00:14:50.700]   into a fixed array of numbers from just using the bitmap
[00:14:50.700 --> 00:14:53.580]   values, but what about something like speech?
[00:14:53.580 --> 00:14:56.380]   What if we want to build a mini Alexa that classifies sounds
[00:14:56.380 --> 00:14:58.940]   into hello or goodbye?
[00:14:58.940 --> 00:15:01.820]   Feel free to pause the video and ponder how we would do this.
[00:15:01.820 --> 00:15:06.100]   It turns out there's no real consensus on the best way
[00:15:06.100 --> 00:15:09.820]   to turn audio into numbers, but one trendy way to do this now
[00:15:09.820 --> 00:15:13.660]   is to just use the waveform of the sound as a list of numbers.
[00:15:13.660 --> 00:15:15.260]   Now there's one problem with this,
[00:15:15.260 --> 00:15:17.500]   which is that all the sounds will be different lengths,
[00:15:17.500 --> 00:15:19.420]   and actually all the arrays in our data
[00:15:19.420 --> 00:15:21.260]   have to be the same length.
[00:15:21.260 --> 00:15:23.700]   But one simple, obvious way to deal with this
[00:15:23.700 --> 00:15:26.740]   is truncate the sounds to a fixed length of time,
[00:15:26.740 --> 00:15:28.780]   or assume that the sounds are completely quiet
[00:15:28.780 --> 00:15:31.580]   once the utterance is complete.
[00:15:31.580 --> 00:15:33.500]   There are actually several other common ways
[00:15:33.500 --> 00:15:35.620]   to do this transformation, and it turns out
[00:15:35.620 --> 00:15:38.980]   that the transformation itself from the data
[00:15:38.980 --> 00:15:42.140]   into this very constrained API of machine learning
[00:15:42.140 --> 00:15:44.940]   is often the most critical choice in building a machine
[00:15:44.940 --> 00:15:46.060]   learning model.
[00:15:46.060 --> 00:15:47.700]   What if we don't have audio or video?
[00:15:47.700 --> 00:15:49.760]   What if we have text?
[00:15:49.760 --> 00:15:52.820]   Oftentimes, companies want to classify tweets about them
[00:15:52.820 --> 00:15:55.660]   as being positive or negative about their brand.
[00:15:55.660 --> 00:15:57.580]   I actually have a video later on that
[00:15:57.580 --> 00:15:59.260]   that goes into detail about exactly how
[00:15:59.260 --> 00:16:00.900]   to do this in build models.
[00:16:00.900 --> 00:16:04.260]   But for now, let's just think about how we transform
[00:16:04.260 --> 00:16:05.260]   that text into numbers.
[00:16:05.260 --> 00:16:11.020]   Again, amazingly, there's no real consensus
[00:16:11.020 --> 00:16:13.600]   on how to do this transformation.
[00:16:13.600 --> 00:16:15.660]   One very common approach is actually
[00:16:15.660 --> 00:16:18.540]   to make a list of all the words in the English language,
[00:16:18.540 --> 00:16:20.580]   or whatever language your text is in,
[00:16:20.580 --> 00:16:22.660]   and count the number of times each word
[00:16:22.660 --> 00:16:24.420]   occurs into your document.
[00:16:24.420 --> 00:16:26.740]   You end up with a list of lots of zeros,
[00:16:26.740 --> 00:16:28.500]   but it actually fits our criteria.
[00:16:28.500 --> 00:16:31.980]   It's always the same length, and it's always full of numbers.
[00:16:31.980 --> 00:16:33.660]   Here's a harder one.
[00:16:33.660 --> 00:16:35.900]   This is common in self-driving cars.
[00:16:35.900 --> 00:16:38.380]   We want to look at every single picture and image
[00:16:38.380 --> 00:16:42.300]   and classify what object each pixel corresponds to.
[00:16:42.300 --> 00:16:44.900]   So for example, we can't just say there's a road in the image.
[00:16:44.900 --> 00:16:47.060]   We have to say which parts of the image are the road
[00:16:47.060 --> 00:16:49.820]   and which parts of the image are a sidewalk.
[00:16:49.820 --> 00:16:52.180]   So here's an example image, and here's actually
[00:16:52.180 --> 00:16:53.460]   an example output.
[00:16:53.460 --> 00:16:56.700]   How does this work?
[00:16:56.700 --> 00:16:59.020]   Once again, there's more than one way to do it,
[00:16:59.020 --> 00:17:01.460]   and this video will probably be soon out of date.
[00:17:01.460 --> 00:17:03.700]   But the most common way to do it right now
[00:17:03.700 --> 00:17:06.300]   is to literally treat the input of numbers
[00:17:06.300 --> 00:17:09.460]   and the output of numbers as arrays of the same length.
[00:17:09.460 --> 00:17:11.340]   So in this case, the output numbers
[00:17:11.340 --> 00:17:16.500]   are actually labels of what's given in each pixel.
[00:17:16.500 --> 00:17:19.620]   Here's an even trickier one, bounding boxes.
[00:17:19.620 --> 00:17:21.900]   We want to put boxes around the things in the image
[00:17:21.900 --> 00:17:23.380]   that we care about.
[00:17:23.380 --> 00:17:25.780]   There could be any number of things that we care about,
[00:17:25.780 --> 00:17:28.980]   but remember, our output has to be a fixed length.
[00:17:28.980 --> 00:17:31.060]   One way to do it-- and there are actually
[00:17:31.060 --> 00:17:33.220]   other good ways to do it-- but one way
[00:17:33.220 --> 00:17:36.260]   is to generate a candidate list of possible boxes
[00:17:36.260 --> 00:17:38.340]   and then run a classifier that looks
[00:17:38.340 --> 00:17:42.220]   at the pixels in an image and the candidate box itself
[00:17:42.220 --> 00:17:44.460]   and classifies not only what's in the box,
[00:17:44.460 --> 00:17:47.780]   but whether or not that box is a good image that should
[00:17:47.780 --> 00:17:50.380]   be considered a bounding box.
[00:17:50.380 --> 00:17:51.980]   A downside of this method is that you
[00:17:51.980 --> 00:17:55.100]   have to consider potentially thousands or millions
[00:17:55.100 --> 00:17:57.860]   of classifications per image.
[00:17:57.860 --> 00:18:00.140]   You may need to look at that last part a few times
[00:18:00.140 --> 00:18:01.220]   to really get it.
[00:18:01.220 --> 00:18:03.380]   I know that I had to.
[00:18:03.380 --> 00:18:06.340]   But the key takeaway here is that framing the machine
[00:18:06.340 --> 00:18:10.300]   learning problem really, really matters.
[00:18:10.300 --> 00:18:12.220]   For example, with object recognition,
[00:18:12.220 --> 00:18:14.500]   the way we framed the problem earlier,
[00:18:14.500 --> 00:18:16.700]   you have no chance of seeing an object
[00:18:16.700 --> 00:18:18.860]   or recognizing an object that you've never
[00:18:18.860 --> 00:18:21.260]   seen in your training data.
[00:18:21.260 --> 00:18:23.500]   So how would you ever recognize an object
[00:18:23.500 --> 00:18:24.740]   that you haven't seen before?
[00:18:24.740 --> 00:18:26.340]   People can do this.
[00:18:26.340 --> 00:18:29.100]   One way to possibly recognize an object that you haven't
[00:18:29.100 --> 00:18:31.940]   had in your training data is to actually frame the problem
[00:18:31.940 --> 00:18:34.060]   instead of recognizing a single object
[00:18:34.060 --> 00:18:37.420]   as recognizing if two objects are the same.
[00:18:37.420 --> 00:18:40.100]   So now our input is actually two objects.
[00:18:40.100 --> 00:18:42.700]   And the classification task is, are these two objects
[00:18:42.700 --> 00:18:45.260]   the same thing or not the same thing?
[00:18:45.260 --> 00:18:47.740]   This is called a pairwise classifier.
[00:18:47.740 --> 00:18:50.060]   And this actually sometimes can classify
[00:18:50.060 --> 00:18:52.180]   objects that it's never seen before,
[00:18:52.180 --> 00:18:54.820]   like the egg beater in this diagram.
[00:18:54.820 --> 00:18:57.820]   Voice recognition, identifying endangered animals
[00:18:57.820 --> 00:19:00.700]   in aerial photography, building crazy face
[00:19:00.700 --> 00:19:02.300]   recognizing drones.
[00:19:02.300 --> 00:19:04.740]   What do these applications all have in common?
[00:19:04.740 --> 00:19:07.540]   Why do we think of them as machine learning applications?
[00:19:07.540 --> 00:19:09.380]   It's because we're able to fit them
[00:19:09.380 --> 00:19:13.260]   into this very constrained, specific API that's
[00:19:13.260 --> 00:19:16.580]   common to all machine learning and deep learning problems.
[00:19:16.580 --> 00:19:19.460]   And so if you're thinking, OK, is my problem
[00:19:19.460 --> 00:19:21.500]   suitable for machine learning or deep learning?
[00:19:21.500 --> 00:19:23.340]   What you should be asking yourself is,
[00:19:23.340 --> 00:19:26.220]   can I turn it into this kind of problem
[00:19:26.220 --> 00:19:29.220]   where I have a fixed length of numbers as input
[00:19:29.220 --> 00:19:31.180]   and a fixed length of numbers as output?
[00:19:31.180 --> 00:19:34.420]   And can I collect training data or examples
[00:19:34.420 --> 00:19:37.700]   to show my model to build my machine learning system?
[00:19:37.700 --> 00:19:39.580]   If the answer to those questions are yes,
[00:19:39.580 --> 00:19:42.900]   then you really do have a machine learning problem.
[00:19:42.900 --> 00:19:44.700]   And hopefully this got you excited enough
[00:19:44.700 --> 00:19:46.620]   about all the applications of machine learning
[00:19:46.620 --> 00:19:49.340]   that you want to watch further videos that explain actually
[00:19:49.340 --> 00:19:52.180]   how to build these models and how to deploy these models.
[00:19:52.180 --> 00:19:54.100]   And we're going to keep creating these videos,
[00:19:54.100 --> 00:19:56.020]   so you should probably subscribe so
[00:19:56.020 --> 00:20:00.140]   that you're the first to know when a new video comes out.

