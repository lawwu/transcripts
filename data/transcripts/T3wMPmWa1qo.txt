
[00:00:00.000 --> 00:00:02.080]   - Hey everyone, my name is Darek Kwecek.
[00:00:02.080 --> 00:00:05.080]   I'm a machine learning engineer at Weights & Biases.
[00:00:05.080 --> 00:00:07.040]   Joining you from Warsaw in Poland.
[00:00:07.040 --> 00:00:10.040]   Let us know in the chat where you're joining us from.
[00:00:10.040 --> 00:00:12.760]   And I have the pleasure of introducing you
[00:00:12.760 --> 00:00:14.800]   to Jonathan Frankel.
[00:00:14.800 --> 00:00:17.640]   Jonathan is chief scientist at MosaicML,
[00:00:17.640 --> 00:00:20.280]   which recently has been acquired by Databricks.
[00:00:20.280 --> 00:00:22.480]   He leads the company's research team
[00:00:22.480 --> 00:00:26.240]   towards the goal of developing more efficient algorithms
[00:00:26.240 --> 00:00:28.440]   for training neural networks.
[00:00:28.440 --> 00:00:30.400]   You might also know him as the author
[00:00:30.400 --> 00:00:35.320]   of ICLR 2019 best paper, Lottery Ticket Hypothesis.
[00:00:35.320 --> 00:00:37.760]   And in addition to his technical work,
[00:00:37.760 --> 00:00:40.240]   he's also actively involved in policymaking
[00:00:40.240 --> 00:00:43.560]   around challenges related to machine learning.
[00:00:43.560 --> 00:00:45.280]   And today, this is the third lesson
[00:00:45.280 --> 00:00:48.000]   of our training and fine tuning LLMs course.
[00:00:48.000 --> 00:00:51.760]   And Jonathan will teach us about data
[00:00:51.760 --> 00:00:54.320]   and data sets for training LLMs.
[00:00:54.320 --> 00:00:55.560]   So this is an exciting topic.
[00:00:55.560 --> 00:00:56.960]   We're excited to have you, Jonathan.
[00:00:56.960 --> 00:00:58.920]   Thanks for being with us.
[00:00:58.920 --> 00:01:00.160]   - Thank you so much for having me.
[00:01:00.160 --> 00:01:04.000]   I'm really excited to be here and share what I know.
[00:01:04.000 --> 00:01:07.120]   I wanna emphasize to everyone before we start,
[00:01:07.120 --> 00:01:09.320]   there's a lot more I don't know than I do know,
[00:01:09.320 --> 00:01:12.200]   and a lot more we don't know as a field than we do know.
[00:01:12.200 --> 00:01:14.240]   So if you came here looking for clear answers
[00:01:14.240 --> 00:01:16.680]   and being told exactly what to do,
[00:01:16.680 --> 00:01:18.440]   I'm afraid I'm gonna disappoint you.
[00:01:18.440 --> 00:01:20.280]   But my hope is I will share with you
[00:01:20.280 --> 00:01:22.520]   the big questions you need to think about,
[00:01:22.520 --> 00:01:24.720]   the methods for trying to sort through those questions
[00:01:24.720 --> 00:01:25.760]   and making the right decisions
[00:01:25.760 --> 00:01:27.720]   in whatever context you're working in.
[00:01:27.720 --> 00:01:34.160]   Awesome, so let's go ahead and get started.
[00:01:34.160 --> 00:01:37.080]   Let me get my presentation up and then we can dig in.
[00:01:37.080 --> 00:01:43.040]   So I've left three GitHub links here
[00:01:43.040 --> 00:01:45.000]   and I'll keep them up, I'll bring them back in the future.
[00:01:45.000 --> 00:01:47.440]   But this is kind of, this is my frame of reference.
[00:01:47.440 --> 00:01:50.000]   These are the code bases I work with every day
[00:01:50.000 --> 00:01:51.880]   and all of us at Mosaic ML work with every day
[00:01:51.880 --> 00:01:52.880]   as we build our models.
[00:01:52.880 --> 00:01:54.920]   So if you're interested in basically doing it
[00:01:54.920 --> 00:01:56.160]   the way that we do it, and you don't have to,
[00:01:56.160 --> 00:01:58.520]   there are lots of great libraries out there,
[00:01:58.520 --> 00:02:00.760]   this is certainly where I'd recommend starting.
[00:02:00.760 --> 00:02:04.400]   So to give you a brief overview of the game plan,
[00:02:04.400 --> 00:02:08.640]   there are really four things I'm gonna cover.
[00:02:08.640 --> 00:02:10.520]   I'm gonna start with a bit of friendly advice.
[00:02:10.520 --> 00:02:13.600]   That friendly advice is intended to kind of contextualize
[00:02:13.600 --> 00:02:15.760]   how we should think as scientists.
[00:02:15.760 --> 00:02:18.840]   And then I wanna really answer three questions.
[00:02:18.840 --> 00:02:20.520]   How much data should you use?
[00:02:20.520 --> 00:02:22.200]   Which data should you use?
[00:02:22.200 --> 00:02:24.040]   And the logistics of getting the data from point A
[00:02:24.040 --> 00:02:25.000]   to point B.
[00:02:25.000 --> 00:02:27.400]   If we cover all of that today,
[00:02:27.400 --> 00:02:29.680]   then we've been very successful.
[00:02:29.680 --> 00:02:31.600]   It seems pretty straightforward and easy,
[00:02:31.600 --> 00:02:33.360]   if only it were so in practice,
[00:02:33.360 --> 00:02:35.280]   but the goal today is to at least highlight for you
[00:02:35.280 --> 00:02:38.120]   some of the key questions you should be asking yourself.
[00:02:38.120 --> 00:02:42.840]   And I am gonna ask, please feel free to ask questions.
[00:02:42.840 --> 00:02:44.200]   I'm gonna try to pause for questions
[00:02:44.200 --> 00:02:45.880]   after every section here,
[00:02:45.880 --> 00:02:47.680]   and hopefully we'll get to take a few of them.
[00:02:47.680 --> 00:02:49.720]   So please feel free to share your thoughts.
[00:02:49.720 --> 00:02:52.120]   I'd love to chat closer to the content
[00:02:52.120 --> 00:02:54.080]   and then toward the end.
[00:02:54.080 --> 00:02:56.800]   So with that, I'll start with a bit of friendly advice.
[00:02:56.800 --> 00:03:01.360]   This is pretty generic scientific advice.
[00:03:01.360 --> 00:03:03.240]   It's important though.
[00:03:03.240 --> 00:03:07.360]   First of all, start small and work your way up.
[00:03:07.360 --> 00:03:08.680]   The thing that scares me most
[00:03:08.680 --> 00:03:10.720]   whenever a customer comes to work with us at MosaicML
[00:03:10.720 --> 00:03:13.120]   is when they say, hey, I'd like to come work with you
[00:03:13.120 --> 00:03:15.080]   and train a 70 billion parameter model.
[00:03:15.080 --> 00:03:18.440]   That scares me a lot because you don't just go
[00:03:18.440 --> 00:03:20.040]   and train a 70 billion parameter model,
[00:03:20.040 --> 00:03:21.440]   even with all the great tools we have
[00:03:21.440 --> 00:03:23.520]   and all the great technology that we have.
[00:03:23.520 --> 00:03:24.880]   It's never that simple.
[00:03:24.880 --> 00:03:27.000]   Every problem is different.
[00:03:27.000 --> 00:03:28.840]   And it's very important that you start small
[00:03:28.840 --> 00:03:30.120]   and work your way up.
[00:03:30.120 --> 00:03:33.320]   I wanna start with a 125 million parameter model
[00:03:33.320 --> 00:03:35.360]   and work my way up step by step
[00:03:35.360 --> 00:03:37.640]   and understand the problem better and better.
[00:03:37.640 --> 00:03:39.560]   Spend a dollar before you spend $10
[00:03:39.560 --> 00:03:40.960]   before you spend $10 million.
[00:03:40.960 --> 00:03:44.120]   And it's important to keep that in mind.
[00:03:44.120 --> 00:03:46.080]   Don't ever just shoot for the moon.
[00:03:46.080 --> 00:03:47.960]   Nothing that I'm gonna tell you today
[00:03:47.960 --> 00:03:49.440]   will tell you exactly what you need to know
[00:03:49.440 --> 00:03:50.280]   to just shoot for the moon.
[00:03:50.280 --> 00:03:52.720]   That's certainly something we never do at MosaicML.
[00:03:52.720 --> 00:03:55.880]   Friendly advice number two,
[00:03:55.880 --> 00:03:58.080]   be skeptical of what you read in the literature.
[00:03:58.080 --> 00:04:00.320]   And I don't say that to mean disregard
[00:04:00.320 --> 00:04:02.120]   what you read in the literature,
[00:04:02.120 --> 00:04:04.400]   but be skeptical, think critically.
[00:04:04.400 --> 00:04:07.000]   The reason is that,
[00:04:07.000 --> 00:04:10.480]   there are really two reasons why I say this.
[00:04:10.480 --> 00:04:16.240]   Number one is that no paper is perfect,
[00:04:16.240 --> 00:04:18.400]   especially in an empirical science
[00:04:18.400 --> 00:04:19.800]   where knowledge is hard to come by
[00:04:19.800 --> 00:04:22.400]   and everything is pretty fuzzy and context dependent.
[00:04:22.400 --> 00:04:25.200]   It's really convenient when we try to look
[00:04:25.200 --> 00:04:27.560]   for simple answers, but there are no simple answers.
[00:04:27.560 --> 00:04:29.360]   And many papers,
[00:04:29.360 --> 00:04:31.600]   we're trained to present simple answers as scientists.
[00:04:31.600 --> 00:04:33.800]   And kind of reason two is that
[00:04:33.800 --> 00:04:35.280]   our incentives to get a paper published
[00:04:35.280 --> 00:04:36.680]   or get a paper to be popular
[00:04:36.680 --> 00:04:39.320]   and kind of advance our careers as researchers
[00:04:39.320 --> 00:04:41.160]   are not the same as making sure
[00:04:41.160 --> 00:04:43.120]   that we get useful scientific truth.
[00:04:43.120 --> 00:04:44.400]   They're often to hype something up.
[00:04:44.400 --> 00:04:47.120]   They're often to get an idea widely circulated.
[00:04:47.120 --> 00:04:49.160]   They're often to get through the review process.
[00:04:49.160 --> 00:04:51.240]   And that means making things simple and clear
[00:04:51.240 --> 00:04:53.880]   and making it look like you have a very obvious win
[00:04:53.880 --> 00:04:55.640]   when sometimes you don't.
[00:04:55.640 --> 00:04:57.960]   And so this isn't out of malice on anyone's part,
[00:04:57.960 --> 00:04:59.240]   but it is important to be skeptical
[00:04:59.240 --> 00:05:00.840]   of what you read in the literature.
[00:05:00.840 --> 00:05:04.200]   That it's true, that it's true in the way that it's stated
[00:05:04.200 --> 00:05:07.240]   and that it's true in the context of your problem.
[00:05:07.240 --> 00:05:08.680]   And so the important thing I tell everyone
[00:05:08.680 --> 00:05:10.840]   on my team at Mosaic and we practice here,
[00:05:10.840 --> 00:05:12.880]   test everything for yourself.
[00:05:12.880 --> 00:05:14.520]   Don't believe it just 'cause someone else said it
[00:05:14.520 --> 00:05:16.200]   and don't believe anything I say today
[00:05:16.200 --> 00:05:17.680]   just because I said it.
[00:05:17.680 --> 00:05:18.760]   Test it for yourself.
[00:05:19.760 --> 00:05:20.600]   Point number three,
[00:05:20.600 --> 00:05:21.960]   and this is gonna sound very repetitive,
[00:05:21.960 --> 00:05:24.000]   don't trust your intuition.
[00:05:24.000 --> 00:05:24.920]   Test.
[00:05:24.920 --> 00:05:26.840]   Intuition is great for posing hypotheses
[00:05:26.840 --> 00:05:28.880]   and coming up with ideas.
[00:05:28.880 --> 00:05:30.760]   Do not try to intuit your way through it
[00:05:30.760 --> 00:05:33.160]   and then make decisions about what to actually do.
[00:05:33.160 --> 00:05:34.720]   Test and measure.
[00:05:34.720 --> 00:05:36.560]   Compare to strong baselines.
[00:05:36.560 --> 00:05:38.240]   Compare to simple ideas.
[00:05:38.240 --> 00:05:39.560]   Compare to things that would be true
[00:05:39.560 --> 00:05:41.440]   if your intuition were completely wrong.
[00:05:41.440 --> 00:05:43.080]   And find out what the data says.
[00:05:43.080 --> 00:05:45.440]   You may have excellent intuition,
[00:05:45.440 --> 00:05:47.480]   but if you do, you're a far better person than me.
[00:05:47.480 --> 00:05:49.160]   I have terrible intuition.
[00:05:49.160 --> 00:05:50.720]   I tend to be wrong a lot more than I'm right,
[00:05:50.720 --> 00:05:52.720]   and that's fine because I pose hypotheses,
[00:05:52.720 --> 00:05:55.280]   test them, realize I'm wrong, and improve.
[00:05:55.280 --> 00:05:57.480]   The last piece here,
[00:05:57.480 --> 00:06:00.120]   and I think this is kind of a little bit
[00:06:00.120 --> 00:06:02.360]   even higher level than the other points.
[00:06:02.360 --> 00:06:05.160]   When people said data science in yesteryear,
[00:06:05.160 --> 00:06:06.920]   five years ago, 10 years ago,
[00:06:06.920 --> 00:06:09.800]   I kind of thought it was a bit of a buzzword.
[00:06:09.800 --> 00:06:11.280]   The things we're going to discuss today
[00:06:11.280 --> 00:06:14.000]   are really literally data science.
[00:06:14.000 --> 00:06:16.720]   You are working with data and you need to be a scientist.
[00:06:16.720 --> 00:06:20.040]   You need to pose hypotheses, test them using experiments,
[00:06:20.040 --> 00:06:21.720]   run control experiments to make sure
[00:06:21.720 --> 00:06:24.800]   that your results are actually meaningful and significant.
[00:06:24.800 --> 00:06:26.520]   All of that good stuff.
[00:06:26.520 --> 00:06:27.840]   There are no answers here.
[00:06:27.840 --> 00:06:29.120]   You have to be a scientist
[00:06:29.120 --> 00:06:30.800]   and let the data speak for itself.
[00:06:30.800 --> 00:06:33.040]   I hope this is helpful framing.
[00:06:33.040 --> 00:06:34.200]   This is how I think about the world.
[00:06:34.200 --> 00:06:36.520]   This is how we operate at Mosaic ML.
[00:06:36.520 --> 00:06:38.320]   These are key principles for us,
[00:06:38.320 --> 00:06:39.960]   and especially in the data realm
[00:06:39.960 --> 00:06:43.600]   where we know far less than we wish we knew.
[00:06:43.600 --> 00:06:47.040]   And we don't know much more than we do know.
[00:06:47.040 --> 00:06:48.600]   This is the only way that you can operate
[00:06:48.600 --> 00:06:50.600]   and make good decisions.
[00:06:50.600 --> 00:06:51.960]   What I like to say about deep learning
[00:06:51.960 --> 00:06:53.200]   is that the path to enlightenment
[00:06:53.200 --> 00:06:55.720]   is starting by accepting you know nothing.
[00:06:55.720 --> 00:06:58.240]   I think the biggest mistake we make as a field
[00:06:58.240 --> 00:07:00.240]   is that sometimes we assume we actually know things.
[00:07:00.240 --> 00:07:01.640]   We don't.
[00:07:01.640 --> 00:07:04.440]   And much of what we think we know is completely wrong.
[00:07:04.440 --> 00:07:08.320]   So start from that zen place of lack of knowledge
[00:07:08.320 --> 00:07:11.120]   and build knowledge using science and using data.
[00:07:12.120 --> 00:07:13.920]   Okay, that concludes my friendly advice.
[00:07:13.920 --> 00:07:15.880]   And now it's time to actually talk
[00:07:15.880 --> 00:07:17.400]   about the other part of point zero,
[00:07:17.400 --> 00:07:19.920]   the surprise that isn't actually in the lecture,
[00:07:19.920 --> 00:07:21.480]   but you know, wasn't in the agenda,
[00:07:21.480 --> 00:07:23.440]   but I do want to cover it, evaluation.
[00:07:23.440 --> 00:07:27.640]   You can't make any decisions
[00:07:27.640 --> 00:07:30.360]   until you know what success looks like.
[00:07:30.360 --> 00:07:31.600]   When someone comes to me and says,
[00:07:31.600 --> 00:07:33.040]   "I want to build a language model.
[00:07:33.040 --> 00:07:34.800]   What data should I?"
[00:07:34.800 --> 00:07:36.520]   My answer is, "I have no idea
[00:07:36.520 --> 00:07:39.760]   because I don't even know what you're trying to accomplish."
[00:07:39.760 --> 00:07:42.880]   And even if you wanted to make progress in this problem,
[00:07:42.880 --> 00:07:45.600]   you would have no idea if you were successful.
[00:07:45.600 --> 00:07:47.600]   The scariest thing in the world for me
[00:07:47.600 --> 00:07:49.680]   is when a customer comes to me, pays a lot of money,
[00:07:49.680 --> 00:07:51.720]   trains a big model, and then turns to me and goes,
[00:07:51.720 --> 00:07:52.680]   "So is it good?"
[00:07:52.680 --> 00:07:55.040]   If that question is getting asked,
[00:07:55.040 --> 00:07:56.760]   then something went horribly wrong in the process
[00:07:56.760 --> 00:07:59.120]   because we didn't set up good evaluation frameworks
[00:07:59.120 --> 00:08:01.560]   for measuring whether our model was good.
[00:08:01.560 --> 00:08:03.920]   I wish I could tell you which benchmarks to use.
[00:08:03.920 --> 00:08:05.120]   And I have given you a link here
[00:08:05.120 --> 00:08:07.960]   to our Mosaic ML LLM evaluation page
[00:08:07.960 --> 00:08:09.840]   where we're trying to put together,
[00:08:09.840 --> 00:08:11.360]   where we've shared the evaluation harness
[00:08:11.360 --> 00:08:13.160]   and the evaluation benchmarks we currently use
[00:08:13.160 --> 00:08:15.400]   so that you know exactly how we're measuring our models
[00:08:15.400 --> 00:08:17.520]   internally and making decisions.
[00:08:17.520 --> 00:08:20.040]   But evaluation is not a one-size-fits-all process.
[00:08:20.040 --> 00:08:22.280]   We're evaluating among a bunch of different axes.
[00:08:22.280 --> 00:08:23.480]   I can actually show you.
[00:08:23.480 --> 00:08:27.360]   We're looking at a lot of different kinds of models.
[00:08:27.360 --> 00:08:29.360]   These radar plots have gotten very popular lately,
[00:08:29.360 --> 00:08:31.880]   but we're looking at everything from programming ability
[00:08:31.880 --> 00:08:34.760]   to common sense reasoning, to reading comprehension.
[00:08:34.760 --> 00:08:36.960]   We've taken as many benchmarks as we can find out there
[00:08:36.960 --> 00:08:38.800]   that we think are good,
[00:08:38.800 --> 00:08:40.560]   kind of taken them apart and put them back together
[00:08:40.560 --> 00:08:41.400]   into these categories
[00:08:41.400 --> 00:08:43.520]   and then looked at our model on these axes.
[00:08:43.520 --> 00:08:44.600]   This is not the right way to do it,
[00:08:44.600 --> 00:08:45.760]   but it's a useful way for us.
[00:08:45.760 --> 00:08:47.400]   And we found it to be productive.
[00:08:47.400 --> 00:08:49.280]   This will not be the right way for you to build your model
[00:08:49.280 --> 00:08:50.880]   because chances are you're building your model
[00:08:50.880 --> 00:08:52.680]   with a specific purpose in mind.
[00:08:52.680 --> 00:08:55.520]   So are we, but our purposes are probably different.
[00:08:55.520 --> 00:08:57.760]   And that means you need to evaluate differently.
[00:08:57.760 --> 00:08:59.120]   Until you have a way to measure
[00:08:59.120 --> 00:09:00.400]   whether your model is getting better
[00:09:00.400 --> 00:09:01.880]   at the things you care about,
[00:09:01.880 --> 00:09:04.280]   you shouldn't even begin to try to train a model.
[00:09:04.280 --> 00:09:06.960]   Evaluation is the most important and the hardest part.
[00:09:06.960 --> 00:09:10.520]   So just as a reminder, please think about this.
[00:09:10.520 --> 00:09:13.760]   And I will say, you can access
[00:09:13.760 --> 00:09:15.520]   this whole evaluation harness on GitHub.
[00:09:15.520 --> 00:09:16.600]   This is not meant to be marketing,
[00:09:16.600 --> 00:09:18.880]   but I do want to put good tools in people's hands.
[00:09:18.880 --> 00:09:20.760]   There are a lot of other great evaluation harnesses out there.
[00:09:20.760 --> 00:09:23.920]   Eleuther.ai has one that's very useful as well.
[00:09:23.920 --> 00:09:25.160]   This is the one we use at Mosaic.
[00:09:25.160 --> 00:09:26.600]   You can go to this URL,
[00:09:26.600 --> 00:09:28.080]   you can go to our LLM Foundry repo
[00:09:28.080 --> 00:09:29.920]   and go to the evaluation area,
[00:09:29.920 --> 00:09:32.920]   and you can go and run on any Hugging Face compatible model.
[00:09:33.760 --> 00:09:36.920]   You can run this evaluation harness and see for yourself.
[00:09:36.920 --> 00:09:38.440]   So I think it's a great set of tools.
[00:09:38.440 --> 00:09:39.680]   I hope it's useful to you.
[00:09:39.680 --> 00:09:40.720]   It's what we use internally
[00:09:40.720 --> 00:09:42.920]   and we wanted to share it with everyone.
[00:09:42.920 --> 00:09:45.600]   But the most important lesson here,
[00:09:45.600 --> 00:09:47.000]   before you even touch data,
[00:09:47.000 --> 00:09:49.200]   before you even begin to think about
[00:09:49.200 --> 00:09:51.160]   how you actually build your model,
[00:09:51.160 --> 00:09:53.360]   you need to know what success looks like.
[00:09:53.360 --> 00:09:55.280]   And I don't like to work with any customers
[00:09:55.280 --> 00:09:57.920]   until they have a way of measuring what success looks like.
[00:09:57.920 --> 00:10:00.680]   Okay, that's all for the friendly advice.
[00:10:00.680 --> 00:10:01.800]   Let's talk about data.
[00:10:03.040 --> 00:10:04.920]   So I'd promised you three questions.
[00:10:04.920 --> 00:10:07.600]   Question number one is how much data?
[00:10:07.600 --> 00:10:09.480]   How much should you use?
[00:10:09.480 --> 00:10:14.120]   And there are two steps to any model training process.
[00:10:14.120 --> 00:10:15.960]   There's the pre-training process
[00:10:15.960 --> 00:10:17.720]   and there's the fine-tuning process.
[00:10:17.720 --> 00:10:20.360]   Personally, I will say I hate the phrase fine-tuning
[00:10:20.360 --> 00:10:22.000]   because often fine-tuning is not very fine.
[00:10:22.000 --> 00:10:23.480]   It tends to actually be as intensive
[00:10:23.480 --> 00:10:25.920]   or more intensive than pre-training.
[00:10:25.920 --> 00:10:28.600]   But pre-training tends to be generic and self-supervised.
[00:10:28.600 --> 00:10:30.560]   We're just training on next token prediction.
[00:10:30.560 --> 00:10:33.040]   Fine-tuning tends to be much more with a purpose
[00:10:33.040 --> 00:10:34.960]   to improve the model on a specific task,
[00:10:34.960 --> 00:10:36.560]   on a specific domain,
[00:10:36.560 --> 00:10:38.560]   on something like a chat or instruction following
[00:10:38.560 --> 00:10:41.120]   or on a particular language like Python
[00:10:41.120 --> 00:10:44.800]   or a specific, an actual human language as well.
[00:10:44.800 --> 00:10:45.640]   Whatever it may be,
[00:10:45.640 --> 00:10:47.760]   you want your model to learn about a domain,
[00:10:47.760 --> 00:10:49.120]   learn about a task or both.
[00:10:49.120 --> 00:10:52.160]   So I'm gonna divide how we talk about data
[00:10:52.160 --> 00:10:53.120]   into both of these components
[00:10:53.120 --> 00:10:54.800]   'cause they're both very important.
[00:10:54.800 --> 00:10:56.240]   And I'm gonna talk about this
[00:10:56.240 --> 00:10:58.120]   in the context of building a model from scratch
[00:10:58.120 --> 00:11:00.760]   'cause honestly, I think that's the most fun part of this.
[00:11:00.760 --> 00:11:03.320]   But even if you're just fine-tuning a model,
[00:11:03.320 --> 00:11:04.680]   and I say just fine-tuning a model,
[00:11:04.680 --> 00:11:07.160]   there's no such thing as just fine-tuning a model,
[00:11:07.160 --> 00:11:11.880]   you're going to still need to use a lot of these lessons
[00:11:11.880 --> 00:11:13.080]   and think about how much data
[00:11:13.080 --> 00:11:15.040]   and what it will cost you as well.
[00:11:15.040 --> 00:11:16.720]   I'm really big on trying to measure things
[00:11:16.720 --> 00:11:19.320]   in dollars and cents in addition to just pure volume.
[00:11:19.320 --> 00:11:23.360]   So actually, before I show you some chinchilla plots,
[00:11:23.360 --> 00:11:27.760]   there are two papers that I've highlighted here
[00:11:27.760 --> 00:11:29.840]   'cause I think they tell two different aspects
[00:11:29.840 --> 00:11:31.160]   of a very important story.
[00:11:31.160 --> 00:11:34.640]   So imagine I said to you,
[00:11:34.640 --> 00:11:37.840]   I'm giving you $1,000 in Mosaic ML A100 credits.
[00:11:37.840 --> 00:11:41.000]   Go train the best model you possibly can.
[00:11:41.000 --> 00:11:44.960]   How are you going to spend that budget?
[00:11:44.960 --> 00:11:46.720]   And suppose I give you a fixed dataset.
[00:11:46.720 --> 00:11:47.560]   I'm just gonna tell you,
[00:11:47.560 --> 00:11:49.160]   here's the dataset you have to use.
[00:11:49.160 --> 00:11:51.360]   Here's the model architecture you have to use.
[00:11:51.360 --> 00:11:53.880]   But I'm not gonna tell you how big of a model to use.
[00:11:53.880 --> 00:11:56.000]   You could use a very small model or a very big one.
[00:11:56.000 --> 00:11:57.200]   You could use 1 billion parameters
[00:11:57.200 --> 00:11:58.880]   or 100 billion parameters.
[00:11:58.880 --> 00:12:01.080]   And I'm not gonna tell you how much data to use.
[00:12:01.080 --> 00:12:02.680]   But I will say, I've given you a fixed budget.
[00:12:02.680 --> 00:12:04.360]   So if you train a bigger model,
[00:12:04.360 --> 00:12:05.920]   that means it's gonna be more expensive
[00:12:05.920 --> 00:12:07.200]   for every piece of data you look at.
[00:12:07.200 --> 00:12:09.400]   So you're gonna get through less data.
[00:12:09.400 --> 00:12:10.600]   If you train a smaller model,
[00:12:10.600 --> 00:12:12.520]   it's gonna be cheaper per piece of data you go through.
[00:12:12.520 --> 00:12:14.560]   And so you can get through more data.
[00:12:14.560 --> 00:12:16.480]   So the size of the model you choose determines
[00:12:16.480 --> 00:12:18.160]   the amount of data you get to see.
[00:12:18.160 --> 00:12:20.840]   What is the best trade-off?
[00:12:20.840 --> 00:12:23.040]   Clearly, if you have a model with one parameter,
[00:12:23.040 --> 00:12:24.840]   you can look at huge numbers of tokens,
[00:12:24.840 --> 00:12:26.680]   but it's gonna be pretty bad.
[00:12:26.680 --> 00:12:28.480]   If you have a model with 10 trillion parameters,
[00:12:28.480 --> 00:12:29.640]   you might get to look at one token,
[00:12:29.640 --> 00:12:31.720]   and that's also gonna be pretty bad.
[00:12:31.720 --> 00:12:33.840]   So there's a sweet spot somewhere in between here.
[00:12:33.840 --> 00:12:36.200]   There's a right model size and a right amount of data
[00:12:36.200 --> 00:12:38.400]   that will get you the best results.
[00:12:38.400 --> 00:12:39.800]   That's what that first paper,
[00:12:39.800 --> 00:12:41.520]   training compute optimal large language models,
[00:12:41.520 --> 00:12:42.800]   also known as the Chinchilla paper.
[00:12:42.800 --> 00:12:45.680]   That's where kind of the Chinchilla scaling laws come from.
[00:12:45.680 --> 00:12:47.520]   That's the question that paper was trying to answer.
[00:12:47.520 --> 00:12:49.600]   And what previous work from OpenAI
[00:12:49.600 --> 00:12:51.360]   was also trying to answer.
[00:12:51.360 --> 00:12:53.200]   If I give you a fixed budget,
[00:12:53.200 --> 00:12:55.480]   what is the best way to spend it on model size
[00:12:55.480 --> 00:12:56.640]   versus amount of data?
[00:12:56.640 --> 00:13:00.640]   And this will tell you for any particular budget,
[00:13:00.640 --> 00:13:02.400]   here's approximately the right size to go with.
[00:13:02.400 --> 00:13:04.880]   And I'll show you that in a moment.
[00:13:04.880 --> 00:13:07.480]   Now, the Lama paper that came out this spring,
[00:13:07.480 --> 00:13:09.440]   in some sense, violated convention
[00:13:09.440 --> 00:13:11.560]   because they purposefully chose things
[00:13:11.560 --> 00:13:13.320]   that were not optimal.
[00:13:13.320 --> 00:13:15.960]   They purposefully chose smaller models on more data,
[00:13:15.960 --> 00:13:17.760]   which meant that for the amount of money they were spending,
[00:13:17.760 --> 00:13:19.960]   they could have gotten a better model.
[00:13:19.960 --> 00:13:20.800]   Why did they do that?
[00:13:20.800 --> 00:13:23.200]   Like, why did they spend their budget
[00:13:23.200 --> 00:13:25.160]   less efficiently than they could have?
[00:13:25.160 --> 00:13:27.800]   Because spending your training budget efficiently
[00:13:27.800 --> 00:13:29.200]   is not the only constraint.
[00:13:29.200 --> 00:13:32.120]   Smaller models are easier to fine tune.
[00:13:32.120 --> 00:13:33.200]   They're easier to do inference on.
[00:13:33.200 --> 00:13:34.440]   In fact, they're even easier to train
[00:13:34.440 --> 00:13:36.480]   for a lot of systems related reasons.
[00:13:36.480 --> 00:13:39.360]   And the Lama authors make the point in the paper
[00:13:39.360 --> 00:13:42.160]   that they purposefully chose to train smaller models
[00:13:42.160 --> 00:13:44.440]   on more data to do something highly suboptimal,
[00:13:44.440 --> 00:13:46.480]   according to that first paper,
[00:13:46.480 --> 00:13:48.400]   to make it easier to train and easier to work with
[00:13:48.400 --> 00:13:50.120]   and easier to do inference on.
[00:13:50.120 --> 00:13:52.440]   That's a totally reasonable choice.
[00:13:52.440 --> 00:13:53.880]   And the question just becomes kind of,
[00:13:53.880 --> 00:13:55.280]   what is the trade-off you're willing to make?
[00:13:55.280 --> 00:13:57.680]   What is the penalty you're willing to pay
[00:13:57.680 --> 00:14:00.160]   from that chinchilla optimal model
[00:14:00.160 --> 00:14:03.360]   in order to make sure that other things are easier?
[00:14:03.360 --> 00:14:04.720]   And I think that's a key question
[00:14:04.720 --> 00:14:07.040]   that comes down to choosing the right amount of data.
[00:14:07.040 --> 00:14:09.960]   So I'm gonna show you, I believe,
[00:14:09.960 --> 00:14:13.000]   one and only one graph during today's presentation.
[00:14:13.000 --> 00:14:16.120]   This is a graph from the chinchilla paper.
[00:14:16.120 --> 00:14:18.880]   And this is kind of the whole paper
[00:14:18.880 --> 00:14:21.280]   in one graph in some sense.
[00:14:21.280 --> 00:14:23.520]   So let me walk you through this a little bit.
[00:14:23.520 --> 00:14:27.960]   On the X-axis is the number of parameters in a model.
[00:14:27.960 --> 00:14:29.800]   So as you get further to the right, the models get bigger.
[00:14:29.800 --> 00:14:32.560]   And each of these points is some model that was trained.
[00:14:32.560 --> 00:14:36.160]   On the Y-axis is the training loss.
[00:14:36.160 --> 00:14:38.680]   So, you know, how well did the model do?
[00:14:38.680 --> 00:14:40.200]   Lower is better.
[00:14:40.200 --> 00:14:42.960]   Lower loss means you got a better model.
[00:14:42.960 --> 00:14:45.200]   So what I want you to do is take a look, for example,
[00:14:45.200 --> 00:14:46.640]   look at that point that says 3B
[00:14:46.640 --> 00:14:48.840]   or 3 billion parameters on the X-axis,
[00:14:48.840 --> 00:14:52.520]   and kind of trace your way upwards, look vertically upwards.
[00:14:52.520 --> 00:14:54.840]   You'll see how for that same model size,
[00:14:54.840 --> 00:14:56.120]   you can get a lot of different losses,
[00:14:56.120 --> 00:14:58.560]   anywhere from a very high loss to a very low loss.
[00:14:58.560 --> 00:15:03.360]   The difference here is how much budget you spent.
[00:15:03.360 --> 00:15:04.560]   So each of these different colors,
[00:15:04.560 --> 00:15:07.360]   each of these different curves is a different budget.
[00:15:07.360 --> 00:15:10.440]   Those numbers in that legend of the plot
[00:15:10.440 --> 00:15:12.040]   are different budgets in terms of flops,
[00:15:12.040 --> 00:15:15.280]   anywhere from 6E18, that's the uppermost curve,
[00:15:15.280 --> 00:15:17.920]   to 3E21, which is almost three orders of magnitude
[00:15:17.920 --> 00:15:18.760]   more compute.
[00:15:18.760 --> 00:15:20.600]   And you can see, for example,
[00:15:20.600 --> 00:15:22.440]   again, tracing that 3 billion parameter model,
[00:15:22.440 --> 00:15:24.640]   looking at that vertical line up and down,
[00:15:24.640 --> 00:15:26.200]   you can see that as you increase the budget
[00:15:26.200 --> 00:15:29.680]   from the top curve where it overlaps, which is 6E19,
[00:15:29.680 --> 00:15:33.120]   I believe, or yeah, 6E19,
[00:15:33.120 --> 00:15:34.960]   all the way down to that bottom curve,
[00:15:34.960 --> 00:15:36.480]   you're getting better and better loss
[00:15:36.480 --> 00:15:37.880]   as you increase your budget.
[00:15:37.880 --> 00:15:39.560]   That makes sense, right?
[00:15:39.560 --> 00:15:40.600]   You spend more.
[00:15:40.600 --> 00:15:42.520]   And when I say increase your budget, what's happening?
[00:15:42.520 --> 00:15:43.600]   You've got a fixed-sized model.
[00:15:43.600 --> 00:15:44.640]   So when you increase the budget,
[00:15:44.640 --> 00:15:46.280]   you're training on more data.
[00:15:46.280 --> 00:15:48.440]   So one way to think about this is if you look up and down
[00:15:48.440 --> 00:15:50.080]   any one of those vertical lines,
[00:15:50.080 --> 00:15:51.600]   you're looking at that 3 billion parameter model
[00:15:51.600 --> 00:15:53.560]   trained on more and more and more data.
[00:15:53.560 --> 00:15:57.680]   Okay, so let's look at any one of these curves
[00:15:57.680 --> 00:15:59.440]   and pick a curve you wanna look at.
[00:15:59.440 --> 00:16:03.600]   Notice that the curve kinda goes up on the left,
[00:16:03.600 --> 00:16:06.280]   up on the right, and down in the middle.
[00:16:06.280 --> 00:16:08.040]   What this means is for any given budget,
[00:16:08.040 --> 00:16:11.080]   for any given amount of money that you have to spend,
[00:16:11.080 --> 00:16:13.000]   there's an optimal number of parameters
[00:16:13.000 --> 00:16:14.880]   that will get you the best loss.
[00:16:14.880 --> 00:16:16.520]   For example, if you look at that second curve
[00:16:16.520 --> 00:16:19.000]   from the bottom, the 3 billion parameter model
[00:16:19.000 --> 00:16:21.080]   is kind of right around the bottom of that curve.
[00:16:21.080 --> 00:16:25.280]   That means that if your budget is 1E21 in terms of flops,
[00:16:25.280 --> 00:16:30.280]   a 3 billion parameter model is the optimal model size.
[00:16:30.280 --> 00:16:32.000]   If you have a bigger model,
[00:16:32.000 --> 00:16:33.920]   you're not gonna do as well for that same budget.
[00:16:33.920 --> 00:16:34.760]   If you have a smaller model,
[00:16:34.760 --> 00:16:37.680]   you're also not gonna do as well for that same budget.
[00:16:37.680 --> 00:16:39.320]   And so for the amount of data,
[00:16:39.320 --> 00:16:41.320]   then you can determine the amount of data by looking at,
[00:16:41.320 --> 00:16:43.560]   I've got a 3 billion parameter model, this much budget,
[00:16:43.560 --> 00:16:45.160]   and then you can go back and back-calculate
[00:16:45.160 --> 00:16:47.480]   the amount of data that you need.
[00:16:47.480 --> 00:16:50.000]   So this is a really important observation.
[00:16:50.000 --> 00:16:52.320]   For a given budget, there is a right model size
[00:16:52.320 --> 00:16:53.200]   and a right amount of data
[00:16:53.200 --> 00:16:55.400]   that will get you the best possible results.
[00:16:55.400 --> 00:17:00.240]   And there's kind of a nice thing that follows onto this,
[00:17:00.240 --> 00:17:01.760]   and I'm not gonna highlight here,
[00:17:01.760 --> 00:17:04.680]   which is actually, there's kind of a fixed relationship
[00:17:04.680 --> 00:17:07.560]   between budget and model size.
[00:17:07.560 --> 00:17:10.920]   Approximately speaking, what the Chinchilla paper tells us
[00:17:10.920 --> 00:17:13.960]   is that the right amount of data is about,
[00:17:13.960 --> 00:17:15.480]   for a particular model size,
[00:17:15.480 --> 00:17:18.360]   is a number of tokens equal to 20 times the model size.
[00:17:18.360 --> 00:17:19.560]   For 3 billion parameter model,
[00:17:19.560 --> 00:17:21.440]   that means 60 billion tokens
[00:17:21.440 --> 00:17:24.200]   is kind of the optimal way to train that model.
[00:17:24.200 --> 00:17:26.880]   Now, the other important part
[00:17:26.880 --> 00:17:28.160]   that you can take away from this plot,
[00:17:28.160 --> 00:17:29.680]   which I think is really interesting,
[00:17:29.680 --> 00:17:31.320]   and that's why I've chosen this particular plot
[00:17:31.320 --> 00:17:33.440]   from that paper, is, again,
[00:17:33.440 --> 00:17:34.960]   let's look at that second line from the bottom
[00:17:34.960 --> 00:17:38.000]   where the 3 billion parameter model is optimal.
[00:17:38.000 --> 00:17:39.720]   You can also ask yourself,
[00:17:39.720 --> 00:17:42.200]   how suboptimal would I be
[00:17:42.200 --> 00:17:44.240]   if I were to choose that same budget
[00:17:44.240 --> 00:17:45.480]   but have a smaller model?
[00:17:45.480 --> 00:17:47.880]   Let's look at where the 1 billion parameter line comes in.
[00:17:47.880 --> 00:17:50.000]   You can kind of see that you're gonna give up,
[00:17:50.000 --> 00:17:54.680]   you know, about 0.05 in loss,
[00:17:54.680 --> 00:17:57.000]   which may or may not be significant to you.
[00:17:57.000 --> 00:17:58.560]   But that's the trade-off the Lama authors
[00:17:58.560 --> 00:17:59.920]   were making in some sense.
[00:17:59.920 --> 00:18:02.200]   They're going to spend their budget some optimally,
[00:18:02.200 --> 00:18:03.600]   which means the models are going to be worse
[00:18:03.600 --> 00:18:05.520]   than they could be for that same budget.
[00:18:05.520 --> 00:18:09.240]   But that's a worthwhile trade-off for them
[00:18:09.240 --> 00:18:11.320]   because they were able to get models
[00:18:11.320 --> 00:18:13.040]   that were easier to serve.
[00:18:13.040 --> 00:18:15.440]   So the Lama authors, in many cases,
[00:18:15.440 --> 00:18:17.000]   were picking a point for any one of these curves
[00:18:17.000 --> 00:18:18.400]   that was off to the left.
[00:18:18.400 --> 00:18:19.680]   It was not the very bottom point,
[00:18:19.680 --> 00:18:21.520]   it was somewhere off to the left.
[00:18:21.520 --> 00:18:22.480]   But to them, that made sense,
[00:18:22.480 --> 00:18:23.520]   and that was a reasonable trade-off.
[00:18:23.520 --> 00:18:26.120]   And given the popularity of the Lama models,
[00:18:26.120 --> 00:18:28.040]   that tells me it was also a very popular trade-off
[00:18:28.040 --> 00:18:30.040]   that people were happier with.
[00:18:30.040 --> 00:18:31.880]   If they had done Chinchilla optimal models,
[00:18:31.880 --> 00:18:32.720]   and by Chinchilla optimal,
[00:18:32.720 --> 00:18:34.200]   I mean picking kind of that bottom point,
[00:18:34.200 --> 00:18:37.800]   picking data equal to 20x parameter size,
[00:18:38.800 --> 00:18:40.360]   those models might not have been as popular
[00:18:40.360 --> 00:18:43.320]   because they would have been much harder to deploy.
[00:18:43.320 --> 00:18:45.240]   So this is a really important graph.
[00:18:45.240 --> 00:18:47.240]   I think in some sense, this is the most important graph
[00:18:47.240 --> 00:18:48.760]   if you're looking at the question
[00:18:48.760 --> 00:18:50.520]   of how much data should you use
[00:18:50.520 --> 00:18:52.360]   and how should you choose your data.
[00:18:52.360 --> 00:18:56.800]   So I want to keep moving a little bit and ask,
[00:18:56.800 --> 00:18:58.040]   okay, so where do you get your data
[00:18:58.040 --> 00:18:59.320]   and how much does it cost?
[00:18:59.320 --> 00:19:03.440]   And the nice thing is for pre-training data,
[00:19:03.440 --> 00:19:06.200]   there are a bunch of amazing datasets that are out there.
[00:19:06.200 --> 00:19:07.920]   Everything from the Pyle,
[00:19:07.920 --> 00:19:11.200]   which was a visionary dataset by the people at Eleuther.ai,
[00:19:11.200 --> 00:19:12.960]   and I say visionary because they made
[00:19:12.960 --> 00:19:15.600]   an open-source large-scale dataset
[00:19:15.600 --> 00:19:18.520]   in like 2020 and 2021,
[00:19:18.520 --> 00:19:21.560]   back before anybody else was really thinking about this.
[00:19:21.560 --> 00:19:23.280]   Everything up to much more recent datasets,
[00:19:23.280 --> 00:19:25.880]   like the Red Pajama dataset from Together.ai,
[00:19:25.880 --> 00:19:27.640]   or the Falcon refined web dataset
[00:19:27.640 --> 00:19:29.280]   from the folks who made the Falcon models,
[00:19:29.280 --> 00:19:30.560]   the Starcoder dataset,
[00:19:30.560 --> 00:19:34.760]   which is a dataset of permissively licensed code,
[00:19:34.760 --> 00:19:36.720]   which we found really useful at Mosaic,
[00:19:36.720 --> 00:19:38.880]   and the Dolma dataset that just came out from our friends
[00:19:38.880 --> 00:19:40.200]   at the Allen Institute for AI,
[00:19:40.200 --> 00:19:42.440]   which looks really promising.
[00:19:42.440 --> 00:19:44.120]   These datasets are big, they're free,
[00:19:44.120 --> 00:19:45.440]   you can download them,
[00:19:45.440 --> 00:19:48.080]   and you can use them to build models.
[00:19:48.080 --> 00:19:51.360]   These datasets mostly contain web scrapes,
[00:19:51.360 --> 00:19:53.200]   and web scrapes tend to be useful.
[00:19:53.200 --> 00:19:54.240]   There are a lot of tokens.
[00:19:54.240 --> 00:19:55.520]   And I just mentioned to you,
[00:19:55.520 --> 00:19:58.680]   we talked about, if we go back to that previous slide,
[00:19:58.680 --> 00:20:01.680]   3 billion parameter model, we said 20x.
[00:20:02.000 --> 00:20:06.000]   So let's take the 70 billion parameter Lama2 model,
[00:20:06.000 --> 00:20:06.840]   for example,
[00:20:06.840 --> 00:20:11.880]   the optimal amount of data there is a 1.4 trillion tokens.
[00:20:11.880 --> 00:20:13.960]   That is a huge amount of data.
[00:20:13.960 --> 00:20:15.000]   I'll go through in a moment
[00:20:15.000 --> 00:20:16.400]   how big some of these datasets are,
[00:20:16.400 --> 00:20:19.040]   so you can get a sense for kind of what the trade-off is,
[00:20:19.040 --> 00:20:21.240]   but that is a massive amount of data.
[00:20:21.240 --> 00:20:23.960]   You need probably a solid 1 trillion to 3 trillion tokens
[00:20:23.960 --> 00:20:25.920]   to train a modern large language model.
[00:20:25.920 --> 00:20:28.120]   And the only place you're gonna find that as of right now
[00:20:28.120 --> 00:20:29.400]   is scraping the web.
[00:20:29.400 --> 00:20:32.320]   That's why we rely so heavily on web scrapes right now.
[00:20:32.320 --> 00:20:34.160]   We supplement with all sorts of other great datasets
[00:20:34.160 --> 00:20:36.720]   out there, Wikipedia, GitHub, what have you,
[00:20:36.720 --> 00:20:39.720]   but at the end of the day, you kind of need the whole web.
[00:20:39.720 --> 00:20:41.240]   And the web is massive
[00:20:41.240 --> 00:20:42.440]   compared to some of these other datasets.
[00:20:42.440 --> 00:20:44.000]   I'll show you that in a moment.
[00:20:44.000 --> 00:20:48.920]   Now, I also wanna talk briefly about after-training data.
[00:20:48.920 --> 00:20:51.160]   And I'm going to highlight a few things here.
[00:20:51.160 --> 00:20:54.880]   First of all, I'm gonna highlight how much it costs.
[00:20:54.880 --> 00:20:57.600]   Now, after-training data, I'll give you just some examples
[00:20:57.600 --> 00:20:59.480]   so you kind of know what I'm talking about here.
[00:20:59.480 --> 00:21:02.000]   Once you've trained a model, that's not enough.
[00:21:02.000 --> 00:21:03.680]   You can think of like the original GPT-3
[00:21:03.680 --> 00:21:05.720]   where you had to really prompt it carefully
[00:21:05.720 --> 00:21:07.600]   versus say, chat GPT these days,
[00:21:07.600 --> 00:21:09.600]   where it just follows instructions.
[00:21:09.600 --> 00:21:10.520]   And the way that you get there
[00:21:10.520 --> 00:21:12.720]   is by fine-tuning the model afterward
[00:21:12.720 --> 00:21:14.080]   on instruction following tasks.
[00:21:14.080 --> 00:21:16.480]   You literally give it instructions and responses.
[00:21:16.480 --> 00:21:18.080]   Write me a poem about deep learning
[00:21:18.080 --> 00:21:21.280]   in the style of Shakespeare, and then an example of a poem,
[00:21:21.280 --> 00:21:24.040]   or write me a program that sorts a list,
[00:21:24.040 --> 00:21:26.320]   and then a Python program that sorts a list.
[00:21:26.320 --> 00:21:29.160]   You give it a lot of examples of that sort of stuff.
[00:21:29.160 --> 00:21:31.560]   And I'm highlighting a few things from the LLAMA-2 paper
[00:21:31.560 --> 00:21:34.920]   because they did a fabulous job, the authors of that paper,
[00:21:34.920 --> 00:21:37.480]   in terms of highlighting what you actually need to do.
[00:21:37.480 --> 00:21:38.800]   They said that we found that SFT,
[00:21:38.800 --> 00:21:40.560]   which is supervised fine-tuning,
[00:21:40.560 --> 00:21:43.280]   and in this case, instructions and responses,
[00:21:43.280 --> 00:21:45.480]   annotations in the order of tens of thousands
[00:21:45.480 --> 00:21:47.640]   was enough to achieve a high-quality result.
[00:21:47.640 --> 00:21:49.000]   We stopped annotating SFT
[00:21:49.000 --> 00:21:52.320]   after collecting a total of 27,540 annotations.
[00:21:53.480 --> 00:21:55.160]   So those are, you know,
[00:21:55.160 --> 00:21:56.640]   I've kind of put some rough costs here.
[00:21:56.640 --> 00:21:59.320]   These are costs I've gotten from, you know,
[00:21:59.320 --> 00:22:01.120]   I've heard from friends who are in the field
[00:22:01.120 --> 00:22:02.760]   who are collecting this data.
[00:22:02.760 --> 00:22:04.800]   This is if you work with data labeling firms
[00:22:04.800 --> 00:22:07.120]   that do a very professional, excellent job on this.
[00:22:07.120 --> 00:22:09.320]   It's about $30 to $50 per example.
[00:22:09.320 --> 00:22:10.800]   There are some open-source datasets,
[00:22:10.800 --> 00:22:12.280]   but they're a little bit spottier,
[00:22:12.280 --> 00:22:13.800]   and the licensing on those datasets
[00:22:13.800 --> 00:22:15.800]   is a little bit more questionable
[00:22:15.800 --> 00:22:18.760]   'cause a lot of them were created by scraping from GPT-4.
[00:22:18.760 --> 00:22:20.320]   And, you know, there are terms of service
[00:22:20.320 --> 00:22:23.080]   that may or may not allow that for GPT-4.
[00:22:23.960 --> 00:22:26.000]   So the first part here is that you generally do
[00:22:26.000 --> 00:22:27.280]   kind of this supervised fine-tuning
[00:22:27.280 --> 00:22:28.440]   or instruction following.
[00:22:28.440 --> 00:22:29.280]   And I also mentioned at the bottom
[00:22:29.280 --> 00:22:30.760]   a multi-turn chat conversation.
[00:22:30.760 --> 00:22:32.680]   You can imagine instead of one back and forth,
[00:22:32.680 --> 00:22:34.280]   you have, let's say, seven total,
[00:22:34.280 --> 00:22:36.400]   you know, one person says something,
[00:22:36.400 --> 00:22:39.400]   you know, you and the model going back and forth.
[00:22:39.400 --> 00:22:41.000]   That tends to cost on the order of, you know,
[00:22:41.000 --> 00:22:41.920]   from what I've heard from friends,
[00:22:41.920 --> 00:22:44.520]   about $100 to $200 per conversation.
[00:22:44.520 --> 00:22:46.240]   So again, this is quite expensive,
[00:22:46.240 --> 00:22:47.680]   you know, about 30,000 annotations
[00:22:47.680 --> 00:22:49.600]   times about, let's call it $50.
[00:22:49.600 --> 00:22:50.480]   And, you know, you can do the math.
[00:22:50.480 --> 00:22:52.200]   That's $1.5 million right there
[00:22:52.200 --> 00:22:53.680]   that Meta might've spent here.
[00:22:53.680 --> 00:22:55.800]   Now there's something interesting
[00:22:55.800 --> 00:22:58.320]   that Meta also said in this paper
[00:22:58.320 --> 00:23:00.800]   that I think is worth highlighting.
[00:23:00.800 --> 00:23:03.560]   Surprisingly, we found that output sampled
[00:23:03.560 --> 00:23:04.880]   from the resulting SFT model,
[00:23:04.880 --> 00:23:05.840]   the model that was fine-tuned
[00:23:05.840 --> 00:23:08.400]   on this 27,000 example dataset,
[00:23:08.400 --> 00:23:09.880]   were often competitive with SFT data
[00:23:09.880 --> 00:23:11.480]   handwritten by human annotators,
[00:23:11.480 --> 00:23:12.960]   suggesting that we could reprioritize
[00:23:12.960 --> 00:23:14.320]   and devote more annotation effort
[00:23:14.320 --> 00:23:17.080]   to preference-based annotation for RLHF.
[00:23:17.080 --> 00:23:18.880]   So I did not work on this paper.
[00:23:18.880 --> 00:23:20.440]   I have no inside information.
[00:23:20.440 --> 00:23:21.440]   But what this indicates to me
[00:23:21.440 --> 00:23:23.080]   is probably they used a lot more
[00:23:23.080 --> 00:23:24.880]   than 27,000 examples,
[00:23:24.880 --> 00:23:27.120]   but they had LLAMA7DB generate more examples
[00:23:27.120 --> 00:23:28.800]   because they found that that worked pretty well.
[00:23:28.800 --> 00:23:29.880]   And they could kind of have humans
[00:23:29.880 --> 00:23:30.960]   curate those examples,
[00:23:30.960 --> 00:23:32.360]   get a little human input,
[00:23:32.360 --> 00:23:33.960]   and it was much more cost-effective from there.
[00:23:33.960 --> 00:23:35.600]   So it's something to think about.
[00:23:35.600 --> 00:23:37.160]   And a lot of these models like LLAMA2
[00:23:37.160 --> 00:23:39.360]   have licenses that restrict you
[00:23:39.360 --> 00:23:40.920]   from using the model to generate data
[00:23:40.920 --> 00:23:41.760]   to train other models.
[00:23:41.760 --> 00:23:43.160]   And I think it's specifically
[00:23:43.160 --> 00:23:45.320]   because you can now do this kind of thing.
[00:23:45.320 --> 00:23:48.840]   Now there's one other piece
[00:23:48.840 --> 00:23:49.880]   to doing after-training data,
[00:23:50.160 --> 00:23:51.120]   and that's RLHF,
[00:23:51.120 --> 00:23:53.480]   or reinforcement learning with human feedback.
[00:23:53.480 --> 00:23:55.040]   The idea here is that
[00:23:55.040 --> 00:23:56.360]   there's having instruction following,
[00:23:56.360 --> 00:23:57.560]   but there's also just trying to make sure
[00:23:57.560 --> 00:23:59.800]   that the model produces outputs
[00:23:59.800 --> 00:24:02.040]   that humans find helpful, find safe,
[00:24:02.040 --> 00:24:03.600]   that humans prefer.
[00:24:03.600 --> 00:24:04.960]   And the way that you typically do this
[00:24:04.960 --> 00:24:07.640]   is you provide two different outputs of the model,
[00:24:07.640 --> 00:24:08.480]   and you have a human say,
[00:24:08.480 --> 00:24:09.840]   "I like this one better than that one,"
[00:24:09.840 --> 00:24:11.440]   or, "I like that one better than this one."
[00:24:11.440 --> 00:24:12.680]   There are much fancier ways of doing it,
[00:24:12.680 --> 00:24:14.640]   but that's kind of the high-level idea.
[00:24:14.640 --> 00:24:16.720]   And Meta did this iteratively.
[00:24:16.720 --> 00:24:18.000]   They took their model,
[00:24:18.000 --> 00:24:18.960]   they fine-tuned it,
[00:24:18.960 --> 00:24:20.320]   then they did some RLHF,
[00:24:20.320 --> 00:24:21.800]   they did this pairwise comparison,
[00:24:21.800 --> 00:24:23.320]   updated the model based on that,
[00:24:23.320 --> 00:24:24.240]   took that updated model,
[00:24:24.240 --> 00:24:26.120]   did another set of human feedback,
[00:24:26.120 --> 00:24:27.200]   did another round of this,
[00:24:27.200 --> 00:24:28.600]   and did this a bunch of times.
[00:24:28.600 --> 00:24:29.440]   And in total,
[00:24:29.440 --> 00:24:31.880]   they collected 14 batches of human preference data
[00:24:31.880 --> 00:24:33.160]   on a weekly basis,
[00:24:33.160 --> 00:24:34.760]   consisting of over 1 million
[00:24:34.760 --> 00:24:36.920]   binary model generation comparisons.
[00:24:36.920 --> 00:24:40.040]   At $5 to $10 per comparison,
[00:24:40.040 --> 00:24:41.360]   you can do the math.
[00:24:41.360 --> 00:24:43.280]   That's pretty expensive.
[00:24:43.280 --> 00:24:45.520]   And so when I think about cost of data,
[00:24:45.520 --> 00:24:47.840]   not just kind of right amounts, but cost,
[00:24:47.840 --> 00:24:49.280]   this is kind of what it takes to build
[00:24:49.280 --> 00:24:51.560]   a large-scale general-purpose model.
[00:24:51.560 --> 00:24:52.400]   Now, it's important to note,
[00:24:52.400 --> 00:24:53.880]   not everyone, and hopefully, you know,
[00:24:53.880 --> 00:24:56.480]   almost nobody who's listening to me talk right now,
[00:24:56.480 --> 00:25:00.040]   needs to build a general-purpose model.
[00:25:00.040 --> 00:25:04.720]   So, you know, a lot of people are trying to build
[00:25:04.720 --> 00:25:07.400]   very specific special-purpose models.
[00:25:07.400 --> 00:25:08.240]   And with that in mind,
[00:25:08.240 --> 00:25:10.280]   you need a lot less data on both fronts.
[00:25:10.280 --> 00:25:12.080]   If you're fine-tuning an existing model,
[00:25:12.080 --> 00:25:13.440]   you need a lot less data than this,
[00:25:13.440 --> 00:25:15.240]   but it is worth making sure you keep in mind
[00:25:15.240 --> 00:25:17.480]   your data budget or where you can get data
[00:25:17.480 --> 00:25:18.880]   that looks like the actual interactions
[00:25:18.880 --> 00:25:20.120]   that people are going to have with the model.
[00:25:20.120 --> 00:25:21.680]   That could be existing data
[00:25:21.680 --> 00:25:23.320]   that you have from a previous model
[00:25:23.320 --> 00:25:24.760]   or a previous setup that you have.
[00:25:24.760 --> 00:25:26.560]   It could be that you go and actually procure that data,
[00:25:26.560 --> 00:25:27.640]   that you have some other data
[00:25:27.640 --> 00:25:29.160]   that can kind of fill that need,
[00:25:29.160 --> 00:25:31.240]   but it can get very expensive.
[00:25:31.240 --> 00:25:33.200]   Now, we have a question from YouTube.
[00:25:33.200 --> 00:25:34.800]   How useful can data augmentation be
[00:25:34.800 --> 00:25:36.400]   in expanding out these datasets?
[00:25:36.400 --> 00:25:38.040]   Can you make a 1 billion token dataset
[00:25:38.040 --> 00:25:39.960]   reliably into a 3 billion token dataset
[00:25:39.960 --> 00:25:41.600]   with augmenting or not that much?
[00:25:41.600 --> 00:25:44.440]   So data augmentation is a tricky topic
[00:25:44.440 --> 00:25:46.480]   because what I just discussed, in some sense,
[00:25:46.480 --> 00:25:48.280]   was using a model to generate more data
[00:25:48.280 --> 00:25:50.600]   for kind of instruction fine-tuning
[00:25:50.600 --> 00:25:53.000]   or SFT, supervised fine-tuning.
[00:25:53.000 --> 00:25:54.960]   Now, data augmentation for pre-training data
[00:25:54.960 --> 00:25:56.520]   is something that hasn't worked very well
[00:25:56.520 --> 00:25:57.800]   as far as I've seen.
[00:25:57.800 --> 00:26:00.600]   I've seen some papers that discuss it and propose ideas,
[00:26:00.600 --> 00:26:01.520]   but I don't know of anyone
[00:26:01.520 --> 00:26:04.000]   who's using it in practice right now.
[00:26:04.000 --> 00:26:07.440]   So no, I wouldn't rely on data augmentation at the moment.
[00:26:07.440 --> 00:26:08.400]   In computer vision,
[00:26:08.400 --> 00:26:10.520]   and I come from the computer vision world originally,
[00:26:10.520 --> 00:26:13.200]   data augmentation is very, very useful.
[00:26:13.200 --> 00:26:14.720]   But I think the answer in computer vision
[00:26:14.720 --> 00:26:15.800]   is that when we have an image,
[00:26:15.800 --> 00:26:18.320]   we have a lot more information intrinsically
[00:26:18.320 --> 00:26:19.160]   about that image.
[00:26:19.160 --> 00:26:20.240]   In most contexts in the real world,
[00:26:20.240 --> 00:26:22.480]   we can left-right flip things and it doesn't matter.
[00:26:22.480 --> 00:26:24.880]   We can crop things differently and it doesn't matter.
[00:26:24.880 --> 00:26:26.560]   It's much trickier to think of something equivalent
[00:26:26.560 --> 00:26:27.840]   to that for text.
[00:26:27.840 --> 00:26:30.120]   You can't kind of left-right crop a sentence
[00:26:30.120 --> 00:26:32.920]   or left-right flip a sentence or crop a sentence
[00:26:32.920 --> 00:26:35.320]   or change the aspect ratio of a sentence.
[00:26:35.320 --> 00:26:37.480]   So text is just a much harder to work with
[00:26:37.480 --> 00:26:38.960]   in that respect than images.
[00:26:38.960 --> 00:26:41.920]   Images, we just kind of know a lot more about as humans
[00:26:41.920 --> 00:26:43.720]   and have a lot more priors we can impose
[00:26:43.720 --> 00:26:45.720]   to kind of take a small amount of data
[00:26:45.720 --> 00:26:47.160]   and turn it into more data.
[00:26:47.160 --> 00:26:49.160]   It's a great question, but as of right now,
[00:26:49.160 --> 00:26:50.960]   we don't have a good way to do that.
[00:26:50.960 --> 00:26:54.640]   So I wanna pause here for any other questions
[00:26:54.640 --> 00:26:56.160]   on this first section before we move on
[00:26:56.160 --> 00:26:58.360]   to the second section of which data you should use.
[00:26:58.360 --> 00:27:01.120]   So, you know, Darker Taylor,
[00:27:01.120 --> 00:27:03.280]   if there are any other questions that folks wanna ask,
[00:27:03.280 --> 00:27:05.080]   now's the time to get them in.
[00:27:05.080 --> 00:27:07.840]   - We have a couple of questions also from Visible.
[00:27:07.840 --> 00:27:11.600]   So the first question is on Chinchilla's scaling clause.
[00:27:11.600 --> 00:27:15.440]   And the question is, is it for a fixed architecture?
[00:27:15.440 --> 00:27:17.360]   What about same number of parameters
[00:27:17.360 --> 00:27:18.800]   but different model types?
[00:27:18.800 --> 00:27:22.320]   - So it is mostly for a fixed architecture.
[00:27:22.320 --> 00:27:24.440]   They're kind of scaling rules that we generally use
[00:27:24.440 --> 00:27:28.520]   to create standard style 1 billion or 3 billion
[00:27:28.520 --> 00:27:30.920]   or 7 billion or what have you parameter models.
[00:27:30.920 --> 00:27:32.400]   As you play with the architecture though,
[00:27:32.400 --> 00:27:34.680]   those scaling laws probably change.
[00:27:34.680 --> 00:27:36.320]   Chinchilla is still a good rule of thumb
[00:27:36.320 --> 00:27:37.520]   as a starting point.
[00:27:37.520 --> 00:27:39.240]   And it's a very expensive to go and compute
[00:27:39.240 --> 00:27:40.080]   your own scaling law.
[00:27:40.080 --> 00:27:42.120]   So if you kind of go back to that graph,
[00:27:42.120 --> 00:27:44.840]   look at the number of points that they have
[00:27:44.840 --> 00:27:47.680]   and imagine going and doing that before you train a model,
[00:27:47.680 --> 00:27:49.280]   you'll spend orders of magnitude more compute
[00:27:49.280 --> 00:27:50.400]   creating these scaling laws
[00:27:50.400 --> 00:27:52.440]   than you will actually training the final model.
[00:27:52.440 --> 00:27:55.000]   So it's not affordable for all of us,
[00:27:55.000 --> 00:27:57.560]   you know, even at Mosaic, certainly not, you know,
[00:27:57.560 --> 00:27:59.560]   and we have a lot of compute available.
[00:27:59.560 --> 00:28:02.840]   The DeepMind folks put in a lot of time and money into this.
[00:28:02.840 --> 00:28:04.560]   So the answer is as you change the architecture,
[00:28:04.560 --> 00:28:06.560]   the scaling laws almost certainly change a little bit
[00:28:06.560 --> 00:28:10.400]   at least, but Chinchilla is still a really good rule of thumb
[00:28:10.400 --> 00:28:11.440]   as a starting point.
[00:28:11.440 --> 00:28:12.600]   And find a good starting point
[00:28:12.600 --> 00:28:13.720]   is the whole name of the game here.
[00:28:13.720 --> 00:28:15.160]   Beyond that, you do have to do the science
[00:28:15.160 --> 00:28:17.400]   to figure out what makes sense for you.
[00:28:17.400 --> 00:28:18.280]   A great question.
[00:28:18.280 --> 00:28:20.040]   - Thank you.
[00:28:20.040 --> 00:28:22.080]   There is also a question about synthetic data.
[00:28:22.080 --> 00:28:24.600]   Do you see that useful for pre-training at all?
[00:28:24.600 --> 00:28:26.920]   - It's a tricky topic.
[00:28:26.920 --> 00:28:28.640]   It's not something that I've used personally,
[00:28:28.640 --> 00:28:31.800]   but I do see a lot of people talking about it more and more,
[00:28:31.800 --> 00:28:33.360]   but I'm still pretty skeptical
[00:28:33.360 --> 00:28:34.200]   because at the end of the day,
[00:28:34.200 --> 00:28:35.400]   where does the synthetic data come from?
[00:28:35.400 --> 00:28:37.280]   It probably comes from a model.
[00:28:37.280 --> 00:28:41.000]   And this isn't scientifically backed, this is intuition.
[00:28:41.000 --> 00:28:42.840]   So take it with a complete grain of salt
[00:28:42.840 --> 00:28:44.400]   and disagree with it.
[00:28:44.400 --> 00:28:45.960]   But I'm not a big believer
[00:28:45.960 --> 00:28:49.120]   that you can just create something from nothing.
[00:28:49.120 --> 00:28:51.560]   That many of the strategies that seem to work best
[00:28:51.560 --> 00:28:53.080]   for using a model to create more data
[00:28:53.080 --> 00:28:55.720]   involve some human curation or human involved.
[00:28:55.720 --> 00:28:57.880]   Everything from having the model generate
[00:28:57.880 --> 00:28:59.280]   a couple of candidate examples
[00:28:59.280 --> 00:29:02.360]   and then having a human choose the best one or edit
[00:29:02.360 --> 00:29:03.760]   one example that they prefer.
[00:29:03.760 --> 00:29:05.600]   Even things like constitutional AI
[00:29:05.600 --> 00:29:07.200]   which Anthropic advocates for,
[00:29:07.200 --> 00:29:10.000]   where you have the model produce an example,
[00:29:10.000 --> 00:29:11.720]   then you give it a principle from a constitution
[00:29:11.720 --> 00:29:12.960]   that a human has written and ask the model
[00:29:12.960 --> 00:29:14.080]   kind of update this output
[00:29:14.080 --> 00:29:16.160]   to make sure it's in line with this principle.
[00:29:16.160 --> 00:29:18.800]   That's still kind of using human input.
[00:29:18.800 --> 00:29:21.840]   And so I think the art is in reducing the amount of work
[00:29:21.840 --> 00:29:25.360]   or kind of using each human hour as effectively as possible.
[00:29:25.360 --> 00:29:26.680]   But I'm kind of skeptical of the idea
[00:29:26.680 --> 00:29:28.320]   that we can simply create something from nothing.
[00:29:28.320 --> 00:29:30.600]   And synthetic data is more along those lines
[00:29:30.600 --> 00:29:33.080]   without some kind of human input or human curation.
[00:29:33.080 --> 00:29:35.000]   But again, that is just intuition.
[00:29:35.000 --> 00:29:36.960]   So take that with a huge grain of salt.
[00:29:36.960 --> 00:29:38.920]   - Thank you.
[00:29:38.920 --> 00:29:41.800]   I wanted to go back to your comment about evaluation
[00:29:41.800 --> 00:29:44.120]   and the fact like you should start with figuring out
[00:29:44.120 --> 00:29:45.680]   like what do you expect this model to do?
[00:29:45.680 --> 00:29:48.040]   And you mentioned some of the benchmarks
[00:29:48.040 --> 00:29:50.680]   that are typically used to evaluate LLMs.
[00:29:50.680 --> 00:29:54.600]   But I wonder for companies that want to train their own LLM.
[00:29:54.600 --> 00:29:55.920]   And by the way, there is a question
[00:29:55.920 --> 00:29:58.440]   like when you should think about training your own LLM
[00:29:58.440 --> 00:30:01.280]   versus just using one that is already available.
[00:30:01.280 --> 00:30:03.960]   But if you want to train one for your use case,
[00:30:03.960 --> 00:30:06.880]   like do you have any advice how to think about evaluation?
[00:30:06.880 --> 00:30:10.960]   - Yeah, so evaluation is the hardest part of the whole thing.
[00:30:10.960 --> 00:30:13.520]   The gold standard of evaluation is using humans
[00:30:13.520 --> 00:30:15.120]   or putting it in a real world setting
[00:30:15.120 --> 00:30:17.160]   and seeing how people interact with it.
[00:30:17.160 --> 00:30:19.280]   I think code completion, things like GitHub Compilator
[00:30:19.280 --> 00:30:20.600]   are a great example of this.
[00:30:20.600 --> 00:30:22.200]   You can put out a model into a real world setting
[00:30:22.200 --> 00:30:23.400]   and see how many people press tab
[00:30:23.400 --> 00:30:26.320]   and actually accept the result of that.
[00:30:26.320 --> 00:30:28.280]   That tells you a lot about how well your model is doing
[00:30:28.280 --> 00:30:30.000]   and whether you need to do better.
[00:30:30.000 --> 00:30:32.920]   So the nice thing for a lot of companies especially
[00:30:32.920 --> 00:30:34.880]   is you're typically not putting this model
[00:30:34.880 --> 00:30:35.920]   in a brand new use case.
[00:30:35.920 --> 00:30:37.160]   You're typically integrating this model
[00:30:37.160 --> 00:30:39.080]   into a flow that already exists.
[00:30:39.080 --> 00:30:40.400]   And this is augmenting humans
[00:30:40.400 --> 00:30:42.400]   or this is automating something.
[00:30:42.400 --> 00:30:45.280]   And I hope you already have a way to measure success
[00:30:45.280 --> 00:30:46.520]   of that use case in general,
[00:30:46.520 --> 00:30:48.440]   in which case you can look at how the model impacts
[00:30:48.440 --> 00:30:51.520]   that result or how you're measuring humans today
[00:30:51.520 --> 00:30:53.040]   or how you're measuring the existing process
[00:30:53.040 --> 00:30:54.680]   of the existing model.
[00:30:54.680 --> 00:30:55.880]   But when it comes to evaluation,
[00:30:55.880 --> 00:30:58.320]   it is incredibly bespoke and incredibly tricky.
[00:30:58.320 --> 00:30:59.960]   And it is much more art than science right now.
[00:30:59.960 --> 00:31:01.920]   I would say there's almost no science.
[00:31:01.920 --> 00:31:03.160]   I will also caution people.
[00:31:03.160 --> 00:31:04.840]   There are a lot of publicly available benchmarks.
[00:31:04.840 --> 00:31:07.240]   There are things like Helm and MMLU.
[00:31:07.240 --> 00:31:08.480]   I think they're all terrible.
[00:31:08.480 --> 00:31:10.400]   I think every single public evaluation
[00:31:10.400 --> 00:31:12.880]   that we have right now is terrible.
[00:31:12.880 --> 00:31:14.360]   I don't trust them as individuals.
[00:31:14.360 --> 00:31:16.000]   I don't trust them as composites.
[00:31:16.000 --> 00:31:17.920]   I don't even trust our own mosaic evaluations
[00:31:17.920 --> 00:31:19.000]   to a certain extent.
[00:31:19.000 --> 00:31:21.000]   I think we're doing the best we can
[00:31:21.000 --> 00:31:22.960]   and it's okay to do the best you can,
[00:31:22.960 --> 00:31:24.400]   but that doesn't mean you should take any of this
[00:31:24.400 --> 00:31:25.240]   as the gold standard,
[00:31:25.240 --> 00:31:26.560]   or you should look at a leaderboard and say,
[00:31:26.560 --> 00:31:28.520]   ah, this model was better at Helm than that model,
[00:31:28.520 --> 00:31:29.840]   therefore it's a better model.
[00:31:29.840 --> 00:31:31.200]   That's just not true.
[00:31:31.200 --> 00:31:32.640]   These are pretty bad benchmarks.
[00:31:32.640 --> 00:31:33.480]   You should go,
[00:31:33.480 --> 00:31:36.040]   before you ever rely on one of these benchmarks,
[00:31:36.040 --> 00:31:38.040]   go actually read the benchmark.
[00:31:38.040 --> 00:31:39.160]   They're generally not very long.
[00:31:39.160 --> 00:31:41.520]   You can look at a few questions and ask yourself,
[00:31:41.520 --> 00:31:44.560]   is this a good test of what I want the model to do?
[00:31:44.560 --> 00:31:46.320]   The answer is almost certainly going to be no,
[00:31:46.320 --> 00:31:48.640]   unless your model is answering multiple choice questions
[00:31:48.640 --> 00:31:52.080]   or answering like questions about physics or what have you.
[00:31:52.080 --> 00:31:53.480]   It's almost certainly a pretty bad test
[00:31:53.480 --> 00:31:54.760]   of what your model can do
[00:31:54.760 --> 00:31:56.960]   relative to what you want it to do.
[00:31:56.960 --> 00:31:58.280]   It's the best we can do,
[00:31:58.280 --> 00:31:59.640]   but it's the reason why you should be building
[00:31:59.640 --> 00:32:01.840]   domain-specific evaluation.
[00:32:01.840 --> 00:32:03.480]   And you had mentioned that one other question
[00:32:03.480 --> 00:32:05.480]   of when should you build your own model?
[00:32:05.480 --> 00:32:09.920]   And the answer is really,
[00:32:09.920 --> 00:32:11.120]   that first principle I gave you
[00:32:11.120 --> 00:32:11.960]   all the way back at the beginning,
[00:32:11.960 --> 00:32:13.120]   I'm even going to scroll back to it,
[00:32:13.120 --> 00:32:15.440]   just to put it out there,
[00:32:15.440 --> 00:32:17.560]   start small and work your way up.
[00:32:17.560 --> 00:32:18.760]   By which I mean,
[00:32:18.760 --> 00:32:20.160]   if an existing open source model
[00:32:20.160 --> 00:32:23.040]   solves your problem effectively, you're done.
[00:32:23.040 --> 00:32:24.600]   Go home.
[00:32:24.600 --> 00:32:26.000]   Take the win, be happy.
[00:32:26.960 --> 00:32:28.560]   If GPT-4 solves your problem
[00:32:28.560 --> 00:32:31.080]   and you can use GPT-4 and the cost is okay,
[00:32:31.080 --> 00:32:32.640]   take the win, go home.
[00:32:32.640 --> 00:32:35.720]   Worry about other things, work on harder use cases.
[00:32:35.720 --> 00:32:39.160]   The folks who come to me either can't use GPT-4
[00:32:39.160 --> 00:32:41.120]   or GPT-4 doesn't address their problem
[00:32:41.120 --> 00:32:43.320]   or an open source model doesn't address their problem.
[00:32:43.320 --> 00:32:44.160]   And when they come to me,
[00:32:44.160 --> 00:32:45.440]   the first thing I tell them is go fine tune
[00:32:45.440 --> 00:32:47.600]   an open source model and see if that works.
[00:32:47.600 --> 00:32:48.960]   Fine tune one of ours, fine tune Lama,
[00:32:48.960 --> 00:32:51.120]   fine tune Falcon, what have you.
[00:32:51.120 --> 00:32:52.280]   If that solves your problem, good.
[00:32:52.280 --> 00:32:54.040]   Take the win, go home, you're done.
[00:32:55.640 --> 00:32:59.040]   We get to pre-training when none of those things work.
[00:32:59.040 --> 00:33:01.360]   And there are a lot of cases where none of that stuff works,
[00:33:01.360 --> 00:33:03.240]   either because you've got a very specific domain,
[00:33:03.240 --> 00:33:05.400]   you've got a lot of data, what have you,
[00:33:05.400 --> 00:33:08.400]   but work your way up, climb the ladder, start small.
[00:33:08.400 --> 00:33:10.520]   Don't go straight for the big stuff right away.
[00:33:10.520 --> 00:33:11.960]   You'll know if you need this.
[00:33:11.960 --> 00:33:15.040]   Any other questions at this stage?
[00:33:15.040 --> 00:33:17.600]   Should we move on to the next section?
[00:33:17.600 --> 00:33:21.000]   - I think let's move on and we'll still have some questions.
[00:33:21.000 --> 00:33:23.560]   We can keep them to the end if we still have time.
[00:33:24.720 --> 00:33:27.120]   - Okay, so the next question is which data?
[00:33:27.120 --> 00:33:30.760]   And I wish I had a good answer for you on which data.
[00:33:30.760 --> 00:33:32.960]   There are more questions than answers here.
[00:33:32.960 --> 00:33:36.200]   But I am gonna tell you kind of what exists
[00:33:36.200 --> 00:33:37.680]   and how we think about it.
[00:33:37.680 --> 00:33:39.680]   And I'm gonna start by giving you a bit of an exercise.
[00:33:39.680 --> 00:33:41.400]   I'm actually gonna be quiet for one minute.
[00:33:41.400 --> 00:33:42.240]   And for those who are watching,
[00:33:42.240 --> 00:33:44.880]   get out a sheet of paper or think to yourself.
[00:33:44.880 --> 00:33:46.800]   What I've done here is I've taken a table
[00:33:46.800 --> 00:33:49.480]   that we put in the NPT blog post.
[00:33:49.480 --> 00:33:51.360]   Here are all the data sources that we had.
[00:33:51.360 --> 00:33:52.800]   And I'm gonna go through them with you one by one
[00:33:52.800 --> 00:33:54.560]   just so you become familiar with them.
[00:33:54.560 --> 00:33:56.680]   What I've hidden here with these question marks
[00:33:56.680 --> 00:33:59.480]   is what proportion of the data set to you
[00:33:59.480 --> 00:34:01.320]   or what proportion of our overall data mix
[00:34:01.320 --> 00:34:02.880]   should come from each of these data sets.
[00:34:02.880 --> 00:34:05.160]   There's no right answer here.
[00:34:05.160 --> 00:34:06.640]   But I think it's a fun exercise
[00:34:06.640 --> 00:34:08.280]   to have to think this through.
[00:34:08.280 --> 00:34:10.120]   On the left, you'll see that there are,
[00:34:10.120 --> 00:34:12.440]   it starts with three different common crawl data sets,
[00:34:12.440 --> 00:34:13.880]   three different web scrapes.
[00:34:13.880 --> 00:34:16.000]   MC4, which is a big multilingual,
[00:34:16.000 --> 00:34:19.440]   although we've just picked out the English portion here.
[00:34:19.440 --> 00:34:21.440]   But it's a web scrape and it's actually web scrapes
[00:34:21.440 --> 00:34:23.720]   going back over several years.
[00:34:23.720 --> 00:34:25.760]   And it's 2.4 trillion tokens.
[00:34:25.760 --> 00:34:27.280]   That's a lot of tokens.
[00:34:27.280 --> 00:34:29.600]   We have C4, which is one web scrape from 2019,
[00:34:29.600 --> 00:34:30.800]   about 100 billion.
[00:34:30.800 --> 00:34:32.720]   Red pajama common crawl, a more recent one,
[00:34:32.720 --> 00:34:34.320]   about 900 billion tokens.
[00:34:34.320 --> 00:34:36.600]   Then we have a few other things.
[00:34:36.600 --> 00:34:38.760]   We have the stack selected languages.
[00:34:38.760 --> 00:34:40.960]   This is from the stack data set from Starcoder.
[00:34:40.960 --> 00:34:42.760]   It's a bunch of languages from GitHub.
[00:34:42.760 --> 00:34:45.320]   We focused on the common ones like Python and Java
[00:34:45.320 --> 00:34:46.160]   and C++.
[00:34:46.160 --> 00:34:48.880]   Wikipedia from the red pajama data set.
[00:34:48.880 --> 00:34:51.780]   This is just a scrape of Wikipedia, about 5 billion tokens.
[00:34:52.760 --> 00:34:54.520]   The markdown component of the stack,
[00:34:54.520 --> 00:34:56.080]   this is like read me's and things like that.
[00:34:56.080 --> 00:34:59.080]   So it's more text than code, about 100 billion tokens.
[00:34:59.080 --> 00:35:01.880]   Semantic scholar is now a data set known as Pesto.
[00:35:01.880 --> 00:35:02.960]   It's about 50 billion tokens
[00:35:02.960 --> 00:35:05.320]   of open access scientific papers.
[00:35:05.320 --> 00:35:07.640]   The books data set as red pajama prepared it.
[00:35:07.640 --> 00:35:09.200]   A scrape of the latex from archive
[00:35:09.200 --> 00:35:10.560]   as red pajama prepared it.
[00:35:10.560 --> 00:35:13.360]   And stack exchange as red pajama prepared it.
[00:35:13.360 --> 00:35:15.560]   These are all the inputs for MPT models.
[00:35:15.560 --> 00:35:18.280]   And you have some tough choices to make here.
[00:35:18.280 --> 00:35:19.960]   I want you to go and figure out
[00:35:19.960 --> 00:35:23.120]   which, how would you get to 1 trillion tokens?
[00:35:23.120 --> 00:35:24.080]   You could get there really easily
[00:35:24.080 --> 00:35:26.040]   by just using MC4 and using a web scrape.
[00:35:26.040 --> 00:35:28.320]   But that's, you know, the web scrape is okay.
[00:35:28.320 --> 00:35:29.520]   It probably has a bunch of HTML
[00:35:29.520 --> 00:35:31.180]   that hasn't been stripped out of it.
[00:35:31.180 --> 00:35:32.520]   It probably has a lot of advertisements
[00:35:32.520 --> 00:35:34.080]   and low quality pages.
[00:35:34.080 --> 00:35:34.920]   You could also get there
[00:35:34.920 --> 00:35:37.600]   by going through Wikipedia 200 times.
[00:35:37.600 --> 00:35:38.540]   That's a lot of repetition,
[00:35:38.540 --> 00:35:41.320]   but Wikipedia is a pretty high quality data set.
[00:35:41.320 --> 00:35:43.320]   You could go through the stack selected languages,
[00:35:43.320 --> 00:35:45.000]   you know, twice.
[00:35:45.000 --> 00:35:46.120]   That's a lot of code though.
[00:35:46.120 --> 00:35:49.080]   Do you really want a model that just does code?
[00:35:49.080 --> 00:35:50.960]   So I'm gonna actually pause for 30 seconds.
[00:35:50.960 --> 00:35:51.880]   I want you to think about this.
[00:35:51.880 --> 00:35:53.640]   This is an exercise where if we were sitting together
[00:35:53.640 --> 00:35:54.920]   in a room, I would ask you to write it down
[00:35:54.920 --> 00:35:56.480]   and share it with me.
[00:35:56.480 --> 00:35:57.820]   But I'm curious for you to do this.
[00:35:57.820 --> 00:36:00.080]   And, you know, I'm not sure how interactive
[00:36:00.080 --> 00:36:02.200]   we can be here, Dark, but I'm kind of curious
[00:36:02.200 --> 00:36:04.800]   if anybody wants to, you know, share what they thought,
[00:36:04.800 --> 00:36:06.320]   I'd be really curious to hear responses
[00:36:06.320 --> 00:36:08.240]   if anyone wants to post it.
[00:36:08.240 --> 00:36:11.320]   So take 30 seconds and kind of think about it for a minute.
[00:36:11.320 --> 00:36:12.480]   I love this exercise.
[00:36:12.480 --> 00:36:14.640]   (silence)
[00:36:14.640 --> 00:36:30.240]   Let's see if there's any response on our stream, Art.
[00:36:30.240 --> 00:36:33.440]   - Yeah, no pressure, but I'm always curious
[00:36:33.440 --> 00:36:34.280]   if we were in a room together,
[00:36:34.280 --> 00:36:36.640]   I would definitely be calling on people and asking them,
[00:36:36.640 --> 00:36:38.000]   you know, what do you think?
[00:36:38.000 --> 00:36:40.160]   (silence)
[00:36:40.160 --> 00:36:46.760]   Maybe I can try myself.
[00:36:46.760 --> 00:36:47.760]   - Yeah, yeah, I'm curious.
[00:36:47.760 --> 00:36:49.520]   What would you do here?
[00:36:49.520 --> 00:36:51.920]   - I feel like a lot of deep learning
[00:36:51.920 --> 00:36:55.200]   is being like a little bit of a magician
[00:36:55.200 --> 00:36:59.120]   and trying to see like what comes out.
[00:36:59.120 --> 00:37:01.520]   I think the challenge with like this LLMs
[00:37:01.520 --> 00:37:03.880]   is like, it's very hard to do a lot of experiments
[00:37:03.880 --> 00:37:05.800]   because they are super expensive.
[00:37:05.800 --> 00:37:10.800]   So I would say, I would probably like
[00:37:10.800 --> 00:37:14.800]   do more of the high quality datasets.
[00:37:14.800 --> 00:37:18.640]   I heard a lot about code being important.
[00:37:18.640 --> 00:37:23.640]   So I'd probably like use a lot of GitHub or the stack.
[00:37:23.640 --> 00:37:26.360]   - I don't want, you're not being specific enough.
[00:37:26.360 --> 00:37:27.880]   I want percentages here.
[00:37:27.880 --> 00:37:29.600]   Give me some percentages in some of these datasets.
[00:37:29.600 --> 00:37:31.200]   Talk to me.
[00:37:31.200 --> 00:37:33.480]   - Okay, yeah, I don't have the numbers worked out,
[00:37:33.480 --> 00:37:38.480]   but I would probably use like 60%
[00:37:38.480 --> 00:37:41.080]   of like the different web datasets.
[00:37:41.080 --> 00:37:45.240]   I don't know how to divide between them.
[00:37:45.240 --> 00:37:48.200]   I'd need to learn a bit more.
[00:37:48.200 --> 00:37:53.200]   Maybe 20% code and 10% Wikipedia,
[00:37:53.200 --> 00:37:59.960]   Semantic Scholar, books and yeah.
[00:38:02.840 --> 00:38:06.600]   Or like 15 and 15, like 15% like all of the,
[00:38:06.600 --> 00:38:09.800]   let's call it like the high quality data sources.
[00:38:09.800 --> 00:38:11.560]   Okay, so I made up my mind.
[00:38:11.560 --> 00:38:15.360]   So 70% would go for web in general,
[00:38:15.360 --> 00:38:18.160]   like a mix of what's there.
[00:38:18.160 --> 00:38:21.240]   15% would go into like the different code datasets
[00:38:21.240 --> 00:38:26.040]   and 15% would go into books, Wikipedia, archive,
[00:38:26.040 --> 00:38:29.080]   like all of this higher quality sources.
[00:38:29.080 --> 00:38:32.240]   - Why that balance?
[00:38:32.240 --> 00:38:35.520]   Like why only 15% code and not 50% code?
[00:38:35.520 --> 00:38:38.200]   Why only 15% high quality and not more?
[00:38:38.200 --> 00:38:40.160]   Like talk to me a little bit.
[00:38:40.160 --> 00:38:41.920]   Walk me through your decision here.
[00:38:41.920 --> 00:38:47.080]   - I, yeah, it's very much intuition.
[00:38:47.080 --> 00:38:49.960]   Now that you're asking, like,
[00:38:49.960 --> 00:38:54.200]   well, actually I think like that isn't maybe that much code.
[00:38:54.200 --> 00:38:57.520]   Like it would, like there will be a lot of repetitions.
[00:38:57.520 --> 00:38:59.480]   Like I need to look in detail at the numbers,
[00:38:59.480 --> 00:39:02.840]   but I assume like if we do more percentage of code,
[00:39:02.840 --> 00:39:04.480]   like we would need to do like many epochs
[00:39:04.480 --> 00:39:05.320]   over the same code.
[00:39:05.320 --> 00:39:06.720]   - You actually have plenty of code here.
[00:39:06.720 --> 00:39:08.600]   So you've got, I want you to come up
[00:39:08.600 --> 00:39:09.480]   with a trillion tokens
[00:39:09.480 --> 00:39:12.400]   and you've got 463 billion tokens of code.
[00:39:12.400 --> 00:39:14.080]   So you could just go through all the code twice
[00:39:14.080 --> 00:39:14.920]   and be fine.
[00:39:14.920 --> 00:39:17.440]   It turns out there's a lot of code.
[00:39:17.440 --> 00:39:19.880]   So this is, you know, I'm being unfair to you right now.
[00:39:19.880 --> 00:39:22.280]   I'm being unfair for a specific reason.
[00:39:22.280 --> 00:39:23.560]   I didn't even tell you what kind of model
[00:39:23.560 --> 00:39:25.520]   we were building or why.
[00:39:25.520 --> 00:39:26.920]   If I told you we were building a code model,
[00:39:26.920 --> 00:39:28.360]   you'd probably answer very differently
[00:39:28.360 --> 00:39:30.080]   than if I told you I was building a model
[00:39:30.080 --> 00:39:31.480]   to be good at trivia.
[00:39:31.480 --> 00:39:33.640]   So, you know, again, this is a great moment
[00:39:33.640 --> 00:39:35.680]   for me to repeat to everyone and kind of, you know,
[00:39:35.680 --> 00:39:37.240]   make fun of Derek a little bit on this.
[00:39:37.240 --> 00:39:39.520]   Evaluation, evaluation, evaluation.
[00:39:39.520 --> 00:39:40.440]   You need to know what you're doing
[00:39:40.440 --> 00:39:43.240]   before you can even start to ask this question.
[00:39:43.240 --> 00:39:45.200]   But with that said, you know,
[00:39:45.200 --> 00:39:46.840]   I'll show you what I came up with.
[00:39:46.840 --> 00:39:50.640]   These are the proportions that I chose.
[00:39:50.640 --> 00:39:52.040]   So I went with about, you know,
[00:39:52.040 --> 00:39:53.680]   honestly pretty similar to what you chose.
[00:39:53.680 --> 00:39:56.840]   I went with about 70% of the web crawls,
[00:39:56.840 --> 00:39:58.520]   went with about 10% code,
[00:39:58.520 --> 00:39:59.920]   and then the remaining 20% or so
[00:39:59.920 --> 00:40:02.440]   was spread over these kind of higher quality datasets.
[00:40:02.440 --> 00:40:04.800]   If you look in that rightmost column in the table,
[00:40:04.800 --> 00:40:05.720]   this is a kind of a cool part.
[00:40:05.720 --> 00:40:06.760]   You can look at the epics.
[00:40:06.760 --> 00:40:07.920]   So how many times would you go through
[00:40:07.920 --> 00:40:09.400]   each of these datasets?
[00:40:09.400 --> 00:40:13.600]   You can see I only went through 15% of MC4,
[00:40:13.600 --> 00:40:16.000]   but I went through Wikipedia eight times.
[00:40:16.000 --> 00:40:17.760]   And we do tend to see that going through datasets
[00:40:17.760 --> 00:40:19.640]   too many times, you do start to, you know,
[00:40:19.640 --> 00:40:20.760]   you get diminishing returns.
[00:40:20.760 --> 00:40:22.720]   We saw beyond four to eight times
[00:40:22.720 --> 00:40:24.080]   of going through the same dataset,
[00:40:24.080 --> 00:40:26.680]   you see some diminishing returns here.
[00:40:26.680 --> 00:40:28.920]   This is not to say, what I want to emphasize to you,
[00:40:28.920 --> 00:40:32.200]   do not look at this and say, Jonathan knows the answer.
[00:40:32.200 --> 00:40:33.600]   I chose this late at night
[00:40:33.600 --> 00:40:34.880]   after trying a bunch of different things
[00:40:34.880 --> 00:40:36.760]   and kind of giving up and saying,
[00:40:36.760 --> 00:40:38.920]   ah, I don't have the quality of evaluation
[00:40:38.920 --> 00:40:40.760]   to tell the difference.
[00:40:40.760 --> 00:40:42.840]   We've gotten a lot more sophisticated since then at Mosaic.
[00:40:42.840 --> 00:40:44.640]   And, you know, I would do things
[00:40:44.640 --> 00:40:46.200]   very differently going forward.
[00:40:46.200 --> 00:40:49.240]   We're trying to make some choices for some new models
[00:40:49.240 --> 00:40:50.680]   that we've been cooking up lately.
[00:40:50.680 --> 00:40:52.840]   And, you know, the same applies.
[00:40:52.840 --> 00:40:55.360]   Like it's a tricky thing.
[00:40:55.360 --> 00:40:58.400]   And the real answer is know what you're evaluating on
[00:40:58.400 --> 00:41:00.160]   and then try to explore which data sources
[00:41:00.160 --> 00:41:02.840]   seem to work best for what you're evaluating on.
[00:41:02.840 --> 00:41:04.480]   I wish I had a great answer
[00:41:04.480 --> 00:41:06.360]   and I could tell you what to do.
[00:41:06.360 --> 00:41:08.840]   This is also, this is what the LLAMA folks did
[00:41:08.840 --> 00:41:10.560]   in the LLAMA1 paper.
[00:41:10.560 --> 00:41:12.560]   And I assume they didn't do anything too differently
[00:41:12.560 --> 00:41:14.160]   in the LLAMA2 paper.
[00:41:14.160 --> 00:41:15.080]   You can see kind of similar,
[00:41:15.080 --> 00:41:19.400]   about 82% Common Crawl in C4, that's web scrapes,
[00:41:19.400 --> 00:41:21.320]   only 4.5% GitHub.
[00:41:21.320 --> 00:41:23.360]   And we did find anecdotally that our MPT models
[00:41:23.360 --> 00:41:25.640]   were better at coding than the original LLAMA models.
[00:41:25.640 --> 00:41:28.880]   And, you know, so this does matter a little bit.
[00:41:28.880 --> 00:41:31.040]   And then, you know, about the remaining, you know,
[00:41:31.040 --> 00:41:33.440]   15% or so from these other higher quality sources
[00:41:33.440 --> 00:41:35.800]   like Wikipedia books, Stack Exchange.
[00:41:35.800 --> 00:41:40.600]   So notice that I've used intuition a lot here.
[00:41:40.600 --> 00:41:43.400]   There are a lot of key questions here that really matter.
[00:41:43.400 --> 00:41:46.960]   First of all, should you even mix data at all?
[00:41:46.960 --> 00:41:49.040]   Or should you just say fresh tokens are better?
[00:41:49.040 --> 00:41:50.720]   Just if you have a new token, use it.
[00:41:50.720 --> 00:41:52.240]   And whatever the proportions are, they are.
[00:41:52.240 --> 00:41:55.640]   If Wikipedia is only, you know, 0.5% of your data set,
[00:41:55.640 --> 00:41:57.680]   'cause it's only 5 billion tokens, that's fine.
[00:41:57.680 --> 00:41:59.080]   You only see Wikipedia once.
[00:41:59.080 --> 00:42:01.760]   Maybe that's the right answer.
[00:42:01.760 --> 00:42:04.680]   Now, obviously this doesn't make sense in the limit.
[00:42:04.680 --> 00:42:07.040]   You know, if you have a lot of very low quality tokens,
[00:42:07.040 --> 00:42:08.520]   that probably seems bad.
[00:42:08.520 --> 00:42:12.160]   But again, don't trust your intuition, trust the data.
[00:42:12.160 --> 00:42:15.880]   And so kind of this next question, quantity or quality?
[00:42:15.880 --> 00:42:20.800]   I think, Dark, you and I both erred on the side of quality.
[00:42:20.800 --> 00:42:22.320]   We wanted to make sure there was some quantity,
[00:42:22.320 --> 00:42:25.840]   but we did way overemphasize some high quality data sets.
[00:42:25.840 --> 00:42:27.680]   'Cause intuitively we must think high quality data
[00:42:27.680 --> 00:42:29.280]   like Wikipedia, that's more important.
[00:42:29.280 --> 00:42:31.080]   Maybe, or maybe not.
[00:42:31.080 --> 00:42:32.840]   Test it, find out.
[00:42:32.840 --> 00:42:35.000]   Find out what matters in your application.
[00:42:35.000 --> 00:42:37.400]   There's no reason to trust our intuition here.
[00:42:37.400 --> 00:42:38.960]   So test it and test it on a small model.
[00:42:38.960 --> 00:42:40.360]   You don't have to test it on the full thing
[00:42:40.360 --> 00:42:41.880]   and train 10 models.
[00:42:41.880 --> 00:42:42.720]   Try it small.
[00:42:42.720 --> 00:42:45.920]   The beautiful thing about Chinchilla is that
[00:42:45.920 --> 00:42:47.520]   as you increase the size of your model,
[00:42:47.520 --> 00:42:49.320]   you should also increase the amount of data.
[00:42:49.320 --> 00:42:50.720]   So the cost of training a bigger model
[00:42:50.720 --> 00:42:52.800]   basically increases quadratically,
[00:42:52.800 --> 00:42:54.000]   which means it doesn't cost you that much
[00:42:54.000 --> 00:42:55.040]   to train a lot of small models
[00:42:55.040 --> 00:42:56.840]   before you train one big one.
[00:42:56.840 --> 00:42:58.280]   And so you can actually do a decent amount
[00:42:58.280 --> 00:43:00.400]   of experimentation and not blow your budget.
[00:43:00.400 --> 00:43:03.000]   There are questions about whether you should deduplicate.
[00:43:03.000 --> 00:43:04.800]   So I'm gonna dwell on deduplication for a moment.
[00:43:04.800 --> 00:43:06.160]   I'm not gonna tell you what to do,
[00:43:06.160 --> 00:43:08.200]   but I do wanna highlight the questions that are out there.
[00:43:08.200 --> 00:43:10.880]   And the answer is, again, try it for yourself.
[00:43:10.880 --> 00:43:13.080]   There's this really seminal paper by Catherine Lee
[00:43:13.080 --> 00:43:17.880]   and colleagues at Google Brain at the time on deduplication.
[00:43:17.880 --> 00:43:19.960]   And I'm actually gonna pull some lines directly
[00:43:19.960 --> 00:43:21.080]   from the abstract here,
[00:43:21.080 --> 00:43:22.240]   'cause I think it's so well-written
[00:43:22.240 --> 00:43:25.040]   and it says everything I wanted to say, but better.
[00:43:25.040 --> 00:43:27.240]   We find that existing language modeling datasets
[00:43:27.240 --> 00:43:29.280]   contain many near-duplicate examples
[00:43:29.280 --> 00:43:30.840]   and long repetitive substrings.
[00:43:30.840 --> 00:43:33.720]   And this actually has an effect.
[00:43:33.720 --> 00:43:36.200]   As a result, over 1% of the unprompted output
[00:43:36.200 --> 00:43:37.920]   of language models trained in these datasets
[00:43:37.920 --> 00:43:40.240]   is copied verbatim from the training data.
[00:43:40.240 --> 00:43:45.240]   So the duplication of these sequences leads to memorization.
[00:43:45.240 --> 00:43:48.120]   Now, I think it's important to say,
[00:43:49.280 --> 00:43:51.360]   specifically what they were looking for.
[00:43:51.360 --> 00:43:52.280]   We developed two tools
[00:43:52.280 --> 00:43:53.760]   that allow us to deduplicate training datasets.
[00:43:53.760 --> 00:43:54.680]   So a lot of these tools are now,
[00:43:54.680 --> 00:43:57.400]   there are many versions publicly available to do this.
[00:43:57.400 --> 00:43:59.960]   For example, removing from C4
[00:43:59.960 --> 00:44:02.000]   a single 61-word English sentence
[00:44:02.000 --> 00:44:04.400]   that is repeated over 60,000 times.
[00:44:04.400 --> 00:44:08.400]   So I think the key takeaway from this paper,
[00:44:08.400 --> 00:44:10.040]   I think a lot of people read this and say,
[00:44:10.040 --> 00:44:11.400]   "Oh, we need to deduplicate everything.
[00:44:11.400 --> 00:44:12.960]   We can't have any duplicates."
[00:44:12.960 --> 00:44:14.560]   That's not what this paper is saying.
[00:44:14.560 --> 00:44:17.080]   This paper was finding things like marketing copy.
[00:44:17.080 --> 00:44:19.120]   And you can look at some of the examples here.
[00:44:19.120 --> 00:44:21.560]   Or other kind of form text
[00:44:21.560 --> 00:44:24.680]   that was repeated tens of thousands of times in the dataset.
[00:44:24.680 --> 00:44:27.240]   And removing those tended to lead to better results
[00:44:27.240 --> 00:44:28.560]   for the model and less memorization.
[00:44:28.560 --> 00:44:29.640]   And also saved you some time,
[00:44:29.640 --> 00:44:32.560]   'cause why should you see that example 60,000 times?
[00:44:32.560 --> 00:44:34.680]   But it's important, don't over-read into this.
[00:44:34.680 --> 00:44:37.120]   This was about highly repeated examples.
[00:44:37.120 --> 00:44:39.480]   This is not about something that's repeated twice.
[00:44:39.480 --> 00:44:42.160]   And it's an important distinction there.
[00:44:42.160 --> 00:44:45.160]   Now, there are much fancier data deduplication techniques
[00:44:45.160 --> 00:44:47.440]   that have come along in the time since.
[00:44:47.440 --> 00:44:50.080]   These include looking at semantically related texts,
[00:44:50.080 --> 00:44:52.680]   not just identically or near identical text.
[00:44:52.680 --> 00:44:55.000]   So things that where you embed your data
[00:44:55.000 --> 00:44:56.520]   and then you look for embedding similarity
[00:44:56.520 --> 00:44:57.960]   to remove things that are too similar,
[00:44:57.960 --> 00:45:00.120]   because they're probably near duplicates.
[00:45:00.120 --> 00:45:02.800]   This is another fancier method
[00:45:02.800 --> 00:45:04.600]   that can push it even further.
[00:45:04.600 --> 00:45:07.560]   Now, what I will say is that for all this deduplication,
[00:45:07.560 --> 00:45:08.680]   I would say the jury is still out.
[00:45:08.680 --> 00:45:10.520]   And there's a lot of debate over whether this is good
[00:45:10.520 --> 00:45:11.960]   or bad for your models.
[00:45:11.960 --> 00:45:13.320]   And so the answer,
[00:45:13.320 --> 00:45:16.080]   you can probably say this answer directly to me
[00:45:16.080 --> 00:45:18.200]   without me having to say it to you.
[00:45:18.200 --> 00:45:21.160]   Do the science, test it and find out in your application.
[00:45:21.160 --> 00:45:22.760]   There's no right answer here.
[00:45:22.760 --> 00:45:24.720]   These are tools, but you need to figure out
[00:45:24.720 --> 00:45:26.680]   whether these tools make sense for you.
[00:45:26.680 --> 00:45:30.960]   So that is the conclusion of the second section
[00:45:30.960 --> 00:45:32.840]   of kind of what data should you use,
[00:45:32.840 --> 00:45:34.360]   how you should choose it, how you should mix it,
[00:45:34.360 --> 00:45:35.720]   what's available to you,
[00:45:35.720 --> 00:45:37.400]   any kind of cleaning you should do.
[00:45:37.400 --> 00:45:40.080]   I wanna pause here again to see if there are any questions
[00:45:40.080 --> 00:45:41.640]   before we move on to the last section here
[00:45:41.640 --> 00:45:42.840]   and then we can wrap up.
[00:45:42.840 --> 00:45:45.000]   (silence)
[00:45:45.000 --> 00:45:52.480]   - There is a question about the mixing of tokens
[00:45:52.480 --> 00:45:54.120]   during the training, like the value.
[00:45:54.120 --> 00:45:57.800]   So you just randomly mix everything
[00:45:57.800 --> 00:46:00.200]   or is there some sequencing
[00:46:00.200 --> 00:46:04.120]   in how you feed data into training the model?
[00:46:04.120 --> 00:46:05.600]   - This is a tricky one.
[00:46:05.600 --> 00:46:09.480]   So this is another intuition that doesn't work in practice.
[00:46:09.480 --> 00:46:11.480]   We have this idea in our head that we as humans,
[00:46:11.480 --> 00:46:13.120]   we learn in curricula.
[00:46:13.120 --> 00:46:15.880]   You don't go and like randomly learn calculus
[00:46:15.880 --> 00:46:18.920]   mixed in with learning fractions as you grow up.
[00:46:18.920 --> 00:46:21.120]   You learn basic arithmetic first
[00:46:21.120 --> 00:46:23.880]   and work your way up to algebra and calculus.
[00:46:23.880 --> 00:46:26.080]   And we all have this intuition, me included,
[00:46:26.080 --> 00:46:27.360]   that there should be a right order
[00:46:27.360 --> 00:46:29.600]   to present the data to the model
[00:46:29.600 --> 00:46:31.760]   that will allow it to learn better and more effectively.
[00:46:31.760 --> 00:46:35.480]   Maybe easy examples first and hardest later or what have you.
[00:46:35.480 --> 00:46:38.120]   I've seen little to no evidence in the literature
[00:46:38.120 --> 00:46:39.880]   that there are any methods that generally work
[00:46:39.880 --> 00:46:41.360]   for developing curricula.
[00:46:41.360 --> 00:46:42.640]   There are papers coming out every year.
[00:46:42.640 --> 00:46:44.240]   I've never seen anything that generally works
[00:46:44.240 --> 00:46:46.840]   beyond say CIFAR-10, a small image dataset.
[00:46:46.840 --> 00:46:51.040]   So when it comes to curricula,
[00:46:51.040 --> 00:46:53.880]   if there's no right answer, the answer is randomly sample.
[00:46:53.880 --> 00:46:55.600]   Now there are questions about kind of,
[00:46:55.600 --> 00:46:57.840]   then there's a lot about how you randomly sample.
[00:46:57.840 --> 00:47:00.160]   In the LLAMA papers, when they do data mixing,
[00:47:00.160 --> 00:47:02.360]   they actually mention that they make sure each batch
[00:47:02.360 --> 00:47:04.760]   has the same proportions as the overall dataset.
[00:47:04.760 --> 00:47:07.760]   They'll sample, you know, three examples from the web scrape
[00:47:07.760 --> 00:47:10.520]   and one example from code or what have you.
[00:47:10.520 --> 00:47:12.840]   At Mosaic, we haven't done that.
[00:47:12.840 --> 00:47:14.680]   We found that in vision models, this did not help.
[00:47:14.680 --> 00:47:17.760]   And I don't know if we've even tested it in text models,
[00:47:17.760 --> 00:47:20.560]   but you can look to the LLAMA authors for a strategy there.
[00:47:20.560 --> 00:47:23.160]   Whether this matters, test, test, test,
[00:47:23.160 --> 00:47:25.960]   and find out, be a scientist.
[00:47:25.960 --> 00:47:28.480]   That does definitely seem like a lower order concern
[00:47:28.480 --> 00:47:31.480]   than for example, making sure you have the right data sources
[00:47:31.480 --> 00:47:34.200]   or making sure that you've got the right data mix,
[00:47:34.200 --> 00:47:36.080]   but it is a question.
[00:47:36.080 --> 00:47:39.160]   So kind of curricula, I wouldn't think about that too much
[00:47:39.160 --> 00:47:41.200]   in terms of how you compose your batches,
[00:47:41.200 --> 00:47:42.920]   a little bit trickier and, you know,
[00:47:42.920 --> 00:47:43.880]   open to debate right now.
[00:47:43.880 --> 00:47:44.960]   I don't think there's a lot of evidence
[00:47:44.960 --> 00:47:46.240]   one way or the other in the literature
[00:47:46.240 --> 00:47:47.640]   beyond that some people have chosen one thing
[00:47:47.640 --> 00:47:49.280]   and some people have chosen another,
[00:47:49.280 --> 00:47:50.960]   but often people choose things for reasons
[00:47:50.960 --> 00:47:53.120]   that may not have anything to do with
[00:47:53.120 --> 00:47:53.960]   that they're a good idea.
[00:47:53.960 --> 00:47:54.880]   It may have been arbitrary
[00:47:54.880 --> 00:47:56.880]   or it may have been just a technical reason
[00:47:56.880 --> 00:47:58.160]   based on how they built their code base
[00:47:58.160 --> 00:48:00.160]   that one made more sense than the other.
[00:48:00.160 --> 00:48:05.320]   Any other questions that we should chat about?
[00:48:05.320 --> 00:48:07.320]   - And maybe one more question,
[00:48:07.320 --> 00:48:09.520]   which actually asks about fine tuning,
[00:48:09.520 --> 00:48:12.080]   but I want to maybe open this up to a broader answer.
[00:48:12.080 --> 00:48:14.880]   And this is about non-English data.
[00:48:14.880 --> 00:48:16.160]   And the question is whether like,
[00:48:16.160 --> 00:48:17.960]   if you have a model that is trained on English,
[00:48:17.960 --> 00:48:19.880]   can you fine tune it on other languages
[00:48:19.880 --> 00:48:21.520]   like Spanish or German?
[00:48:21.520 --> 00:48:24.600]   But I guess maybe if I can open this up to like more broadly
[00:48:24.600 --> 00:48:26.280]   like how to go beyond English.
[00:48:26.280 --> 00:48:28.960]   - Yeah, going beyond English is tricky
[00:48:28.960 --> 00:48:31.840]   because the world is mostly in English
[00:48:31.840 --> 00:48:33.520]   as you start looking at text.
[00:48:33.520 --> 00:48:34.920]   Wikipedia in English is much bigger
[00:48:34.920 --> 00:48:36.360]   than any other Wikipedia.
[00:48:36.360 --> 00:48:37.720]   And if you look at MC4,
[00:48:37.720 --> 00:48:42.240]   I actually, I'll go back a little bit to this page.
[00:48:42.240 --> 00:48:45.840]   MC4 is, the M stands for multilingual.
[00:48:45.840 --> 00:48:48.440]   This is a wonderful dataset that's publicly available
[00:48:48.440 --> 00:48:50.560]   that has tons of data in lots of languages.
[00:48:50.560 --> 00:48:51.960]   It's about 12 trillion tokens
[00:48:51.960 --> 00:48:54.880]   of which 2.4 trillion tokens are English.
[00:48:54.880 --> 00:48:56.160]   So it is still majority English,
[00:48:56.160 --> 00:48:57.600]   but there are lots of other languages in there.
[00:48:57.600 --> 00:48:58.680]   And for people I've worked with
[00:48:58.680 --> 00:49:00.280]   who did have a specific need
[00:49:00.280 --> 00:49:01.680]   to train on a particular language,
[00:49:01.680 --> 00:49:04.960]   they have used the subsets of MC4 for that language.
[00:49:04.960 --> 00:49:08.520]   So there are multilingual datasets out there.
[00:49:08.520 --> 00:49:12.080]   In terms of fine tuning on another language,
[00:49:12.080 --> 00:49:13.960]   once you've got an English language model,
[00:49:13.960 --> 00:49:16.400]   the honest answer is I'm not entirely sure.
[00:49:16.400 --> 00:49:19.560]   There's trickiness in the tokenizer there.
[00:49:19.560 --> 00:49:22.480]   That your tokenizer,
[00:49:22.480 --> 00:49:24.040]   and I think we're gonna talk about tokenizers
[00:49:24.040 --> 00:49:25.560]   in a couple of weeks,
[00:49:25.560 --> 00:49:30.480]   but your tokenizer may have been built with English in mind.
[00:49:30.480 --> 00:49:32.600]   And so may not respond as well to other languages.
[00:49:32.600 --> 00:49:33.880]   And if you need to change your tokenizer,
[00:49:33.880 --> 00:49:36.920]   you probably need to re-pre-train from scratch.
[00:49:36.920 --> 00:49:39.200]   But by and large, any model that's seen a lot of English
[00:49:39.200 --> 00:49:41.360]   has also seen a lot of other languages.
[00:49:41.360 --> 00:49:44.880]   We trained MPT exclusively to be an English language model.
[00:49:44.880 --> 00:49:47.480]   It does actually really well in other languages.
[00:49:47.480 --> 00:49:48.960]   We have lots of hypotheses for why.
[00:49:48.960 --> 00:49:51.920]   We actually point to the Markdown subset of the stack,
[00:49:51.920 --> 00:49:54.160]   which was not filtered for any particular language,
[00:49:54.160 --> 00:49:55.520]   and probably has a lot of really great,
[00:49:55.520 --> 00:49:57.480]   rich multilingual content
[00:49:57.480 --> 00:49:59.040]   for people who are writing Markdown files
[00:49:59.040 --> 00:50:00.960]   in lots of different languages.
[00:50:00.960 --> 00:50:04.400]   So it's hard to say for sure,
[00:50:04.400 --> 00:50:06.720]   but your best bet is probably to pre-train a model
[00:50:06.720 --> 00:50:07.680]   in that language.
[00:50:07.680 --> 00:50:09.080]   But I'm guessing that you could do pretty well
[00:50:09.080 --> 00:50:09.920]   fine tuning a model,
[00:50:09.920 --> 00:50:10.960]   'cause even when you attempt
[00:50:10.960 --> 00:50:12.400]   to build an English language model,
[00:50:12.400 --> 00:50:15.040]   it's gonna see a lot of multilingual data regardless.
[00:50:15.040 --> 00:50:17.840]   - Thank you.
[00:50:17.840 --> 00:50:20.520]   I think we can move on to the next topic.
[00:50:20.520 --> 00:50:22.080]   - Awesome, so let's move on and wrap up.
[00:50:22.080 --> 00:50:23.200]   This is gonna be pretty quick,
[00:50:23.200 --> 00:50:25.520]   and then I'm ready to wrap up.
[00:50:25.520 --> 00:50:27.360]   So I wanna talk about logistics here.
[00:50:27.360 --> 00:50:30.400]   This is really hard.
[00:50:31.400 --> 00:50:35.000]   You have terabytes or petabytes of data.
[00:50:35.000 --> 00:50:36.640]   Where in the world do you put it?
[00:50:36.640 --> 00:50:39.440]   How do you get it to your actual training cluster?
[00:50:39.440 --> 00:50:41.880]   Because the data may be in a completely different place.
[00:50:41.880 --> 00:50:42.720]   How do you shuffle it?
[00:50:42.720 --> 00:50:44.080]   How do you ensure determinism?
[00:50:44.080 --> 00:50:46.400]   There are just a lot of logistical challenges here
[00:50:46.400 --> 00:50:48.280]   that will cause you a really hard time.
[00:50:48.280 --> 00:50:50.480]   Even things like, suppose you're training your model,
[00:50:50.480 --> 00:50:52.360]   and midway through, one of your GPUs dies,
[00:50:52.360 --> 00:50:53.280]   and your model crashes,
[00:50:53.280 --> 00:50:55.360]   and then you have to pick up again in the middle.
[00:50:55.360 --> 00:50:56.560]   How do you make sure you actually pick up
[00:50:56.560 --> 00:50:57.560]   where you left off?
[00:50:57.560 --> 00:50:59.000]   And how do you make sure you don't have to spin through
[00:50:59.000 --> 00:51:01.160]   the whole data set to get back to that place
[00:51:01.160 --> 00:51:02.240]   where you left off?
[00:51:02.240 --> 00:51:03.440]   That would take a very long time
[00:51:03.440 --> 00:51:05.680]   for one of these big data sets.
[00:51:05.680 --> 00:51:07.680]   There are lots of solutions out there to these problems.
[00:51:07.680 --> 00:51:09.040]   There are lots of strategies here.
[00:51:09.040 --> 00:51:10.480]   I'm just gonna highlight one, which is,
[00:51:10.480 --> 00:51:12.800]   at MosaicML, we have our own data loader.
[00:51:12.800 --> 00:51:14.120]   It's freely open source.
[00:51:14.120 --> 00:51:17.160]   You can go to github.com/mosaicml/streaming.
[00:51:17.160 --> 00:51:19.840]   And the goal here was to address
[00:51:19.840 --> 00:51:22.040]   basically all of these concerns.
[00:51:22.040 --> 00:51:24.520]   So what we do is we have kind of our own proprietary format.
[00:51:24.520 --> 00:51:26.800]   It's proprietary as in it's specific to this data loader,
[00:51:26.800 --> 00:51:28.280]   but there's nothing fancy about it.
[00:51:28.280 --> 00:51:30.280]   It's just a row-based format.
[00:51:30.280 --> 00:51:32.680]   We break the data set into very big shards,
[00:51:32.680 --> 00:51:34.600]   and you can actually keep those shards
[00:51:34.600 --> 00:51:36.000]   in just an S3 bucket.
[00:51:36.000 --> 00:51:38.120]   You don't have to put them in like fast storage.
[00:51:38.120 --> 00:51:39.560]   You don't have to put them in solid state storage
[00:51:39.560 --> 00:51:40.960]   or anything like that.
[00:51:40.960 --> 00:51:42.960]   And a streaming data loader basically handles the rest.
[00:51:42.960 --> 00:51:46.120]   It will load those shards to wherever is happening,
[00:51:46.120 --> 00:51:47.080]   wherever the training is happening.
[00:51:47.080 --> 00:51:48.080]   It'll cache them to disk.
[00:51:48.080 --> 00:51:48.920]   It'll keep them in memory.
[00:51:48.920 --> 00:51:50.280]   It'll shuffle for you.
[00:51:50.280 --> 00:51:51.440]   It'll even ensure determinism,
[00:51:51.440 --> 00:51:54.080]   so you can pick up where you left off even after a crash.
[00:51:54.080 --> 00:51:55.880]   Even after you've changed the number of GPUs,
[00:51:55.880 --> 00:51:57.440]   it can guarantee determinism,
[00:51:57.440 --> 00:51:58.320]   which is really cool.
[00:51:58.320 --> 00:52:00.880]   It's been very valuable for me 'cause I'm a manager.
[00:52:00.880 --> 00:52:02.840]   I typically don't train models myself that often,
[00:52:02.840 --> 00:52:04.840]   but when I do, I'm kind of squeezing them in
[00:52:04.840 --> 00:52:05.920]   wherever things are available,
[00:52:05.920 --> 00:52:09.760]   and sometimes I have to move them up or move them down.
[00:52:09.760 --> 00:52:14.960]   And so I found our streaming data loader
[00:52:14.960 --> 00:52:15.800]   to be really valuable.
[00:52:15.800 --> 00:52:18.160]   There's a really nice GIF that our team put together
[00:52:18.160 --> 00:52:19.000]   kind of showing what happens.
[00:52:19.000 --> 00:52:20.360]   You've got all these shards.
[00:52:20.360 --> 00:52:21.720]   It loads them onto the nodes,
[00:52:21.720 --> 00:52:24.480]   and then it shuffles and divides the individual samples
[00:52:24.480 --> 00:52:25.960]   up onto GPU workers.
[00:52:25.960 --> 00:52:28.040]   I just find this mesmerizing to watch,
[00:52:28.040 --> 00:52:32.040]   but personally, I found this to be really valuable.
[00:52:32.040 --> 00:52:34.360]   The nice thing about this is that
[00:52:34.360 --> 00:52:36.360]   it basically eliminates data gravity.
[00:52:36.360 --> 00:52:40.800]   Often, we will keep our data in one cloud, like AWS,
[00:52:40.800 --> 00:52:42.000]   and stream it to another cloud,
[00:52:42.000 --> 00:52:44.680]   like Oracle or CoreWeave or Crusoe,
[00:52:44.680 --> 00:52:47.720]   and there's really no slowdown and no issue there.
[00:52:47.720 --> 00:52:50.600]   In fact, for our MPT models, I believe one of them,
[00:52:50.600 --> 00:52:53.240]   the data lived in AWS in California
[00:52:53.240 --> 00:52:55.600]   and was trained in Oracle in Australia,
[00:52:55.600 --> 00:52:57.920]   and we still experienced no slowdown.
[00:52:57.920 --> 00:52:59.400]   Now, for those of you who deal with a lot of cloud stuff,
[00:52:59.400 --> 00:53:01.000]   you might say, "But what about the egress costs?"
[00:53:01.000 --> 00:53:03.040]   You have to pay kind of the network costs
[00:53:03.040 --> 00:53:05.160]   of transferring your data out of the cloud.
[00:53:05.160 --> 00:53:07.840]   It can get expensive, but so are GPUs,
[00:53:07.840 --> 00:53:09.760]   and we've done the math and basically
[00:53:09.760 --> 00:53:11.960]   keeping it in just a cloud bucket, an S3 bucket,
[00:53:11.960 --> 00:53:14.920]   an OCI bucket, a GCP bucket, what have you,
[00:53:14.920 --> 00:53:16.040]   and then paying the egress fees
[00:53:16.040 --> 00:53:17.680]   to transfer it to another cloud.
[00:53:17.680 --> 00:53:19.520]   If you even had your data in one cloud
[00:53:19.520 --> 00:53:21.120]   and your GPUs in another,
[00:53:21.120 --> 00:53:23.960]   is like 1% to 2% maximum of a training run,
[00:53:23.960 --> 00:53:26.320]   and for longer training runs, kind of LLAMA style,
[00:53:26.320 --> 00:53:27.720]   it's way less than that.
[00:53:27.720 --> 00:53:29.480]   So the cost increase is very small
[00:53:29.480 --> 00:53:30.760]   compared to the convenience,
[00:53:30.760 --> 00:53:31.720]   and if it's all in one cloud,
[00:53:31.720 --> 00:53:32.640]   there are no egress fees,
[00:53:32.640 --> 00:53:35.080]   and you're in a very happy place.
[00:53:35.080 --> 00:53:36.600]   So I highly recommend this.
[00:53:36.600 --> 00:53:38.280]   It's a great tool.
[00:53:38.280 --> 00:53:40.240]   I'm not just shilling it 'cause it's a Mosaic ML tool,
[00:53:40.240 --> 00:53:41.440]   'cause we make no money if we use this.
[00:53:41.440 --> 00:53:43.320]   It's just open source, but personally,
[00:53:43.320 --> 00:53:45.480]   having used a lot of different tools over the years,
[00:53:45.480 --> 00:53:46.560]   I found it to be valuable,
[00:53:46.560 --> 00:53:49.000]   and the engineers who built this did a great job.
[00:53:50.200 --> 00:53:53.520]   So with that, we're kind of at the end.
[00:53:53.520 --> 00:53:57.560]   I wanna end by reiterating my friendly advice,
[00:53:57.560 --> 00:53:59.840]   and then we can do some questions.
[00:53:59.840 --> 00:54:02.720]   Number one, start small and work your way up.
[00:54:02.720 --> 00:54:03.880]   Really, really start small.
[00:54:03.880 --> 00:54:06.120]   Don't spend your whole budget right away.
[00:54:06.120 --> 00:54:08.200]   Number two, be skeptical and be critical
[00:54:08.200 --> 00:54:09.760]   of what you read in the literature,
[00:54:09.760 --> 00:54:10.800]   and what you see on Twitter,
[00:54:10.800 --> 00:54:13.040]   and what your friend's friend at opening eye told them,
[00:54:13.040 --> 00:54:13.960]   and what have you.
[00:54:13.960 --> 00:54:16.720]   Test it for yourself.
[00:54:16.720 --> 00:54:19.720]   Not everything that you see in the literature
[00:54:19.720 --> 00:54:22.560]   is 100% reliable,
[00:54:22.560 --> 00:54:24.360]   and also not everything is gonna translate
[00:54:24.360 --> 00:54:26.880]   to your setting and your domain and what you're working on.
[00:54:26.880 --> 00:54:28.200]   Also, don't trust your intuition.
[00:54:28.200 --> 00:54:30.800]   A lot of counterintuitive things happen in deep learning.
[00:54:30.800 --> 00:54:32.120]   If you would have told me a few years ago
[00:54:32.120 --> 00:54:34.200]   that just making everything really big
[00:54:34.200 --> 00:54:36.760]   would have been enough to get us GPT-4 quality models,
[00:54:36.760 --> 00:54:38.960]   I would have laughed at you, and here we are.
[00:54:38.960 --> 00:54:41.640]   Test things for yourself.
[00:54:41.640 --> 00:54:43.280]   This really is science in its truest form,
[00:54:43.280 --> 00:54:44.800]   and data science in its truest form,
[00:54:44.800 --> 00:54:45.960]   and you're gonna have to be a scientist
[00:54:45.960 --> 00:54:48.040]   and follow the scientific method.
[00:54:48.040 --> 00:54:48.880]   With that, thank you.
[00:54:48.880 --> 00:54:51.840]   I'm gonna grab questions with the five minutes we have left,
[00:54:51.840 --> 00:54:53.320]   and then we can wrap up.
[00:54:53.320 --> 00:54:56.120]   We have one question right away from YouTube.
[00:54:56.120 --> 00:54:57.640]   People would love to hear my take on the recent
[00:54:57.640 --> 00:55:01.000]   Phi 1.5 model on potential data contamination.
[00:55:01.000 --> 00:55:02.120]   I haven't read it in detail,
[00:55:02.120 --> 00:55:04.240]   and I don't wanna comment on that.
[00:55:04.240 --> 00:55:05.320]   That's something to be adjudicated
[00:55:05.320 --> 00:55:06.760]   by the scientific community over time,
[00:55:06.760 --> 00:55:08.680]   and far be it from me to give answers
[00:55:08.680 --> 00:55:10.080]   on something I didn't build.
[00:55:10.080 --> 00:55:13.920]   - Thanks a lot, Jonathan.
[00:55:13.920 --> 00:55:17.080]   There is a question on determinism.
[00:55:17.080 --> 00:55:19.280]   How does determinism work?
[00:55:19.280 --> 00:55:21.240]   If you can talk in more details about it,
[00:55:21.240 --> 00:55:23.880]   isn't this quite tricky with distributed training?
[00:55:23.880 --> 00:55:26.760]   Not sure if this is about training or data loading, but--
[00:55:26.760 --> 00:55:28.360]   - I think this is about the streaming data loader
[00:55:28.360 --> 00:55:30.040]   and how it preserves determinism.
[00:55:30.040 --> 00:55:31.640]   The answer is it's really tricky,
[00:55:31.640 --> 00:55:33.440]   and the engineers have done a great job.
[00:55:33.440 --> 00:55:35.680]   They've especially had one really key idea,
[00:55:35.680 --> 00:55:38.640]   which is you have your physical GPUs,
[00:55:38.640 --> 00:55:41.800]   and then you can have the idea of virtual GPUs.
[00:55:41.800 --> 00:55:43.560]   And so as part of spinning up the streaming data loader,
[00:55:43.560 --> 00:55:46.400]   you actually set a number of virtual GPUs.
[00:55:46.400 --> 00:55:49.520]   And kind of the key idea is that your virtual GPUs
[00:55:49.520 --> 00:55:51.520]   have to be a number divisible by your physical GPUs
[00:55:51.520 --> 00:55:53.000]   or vice versa.
[00:55:53.000 --> 00:55:55.320]   But once you fix the number of virtual GPUs,
[00:55:55.320 --> 00:55:57.080]   you can actually change the number of physical GPUs
[00:55:57.080 --> 00:55:59.600]   as much as you want, and you'll preserve determinism.
[00:55:59.600 --> 00:56:01.400]   But it's the virtual GPUs that kind of determine
[00:56:01.400 --> 00:56:03.080]   the sequence that you look at the data.
[00:56:03.080 --> 00:56:05.560]   I thought that was a really neat idea.
[00:56:05.560 --> 00:56:06.920]   I wouldn't have thought of that,
[00:56:06.920 --> 00:56:08.640]   but it means that you can switch the number of GPUs
[00:56:08.640 --> 00:56:11.040]   you're training on and preserve full determinism.
[00:56:11.040 --> 00:56:12.320]   And if you look at some of our loss curves
[00:56:12.320 --> 00:56:14.680]   at Mosaic after resumption, it's identical.
[00:56:14.680 --> 00:56:16.800]   It'll go up and down exactly the same way.
[00:56:16.800 --> 00:56:17.800]   And that's through determinism.
[00:56:17.800 --> 00:56:19.560]   Determinism makes debugging a lot easier.
[00:56:19.560 --> 00:56:22.200]   So it's nice that you can kind of get it for free
[00:56:22.200 --> 00:56:23.720]   without making any sacrifices.
[00:56:23.720 --> 00:56:26.680]   - Thank you.
[00:56:26.680 --> 00:56:28.760]   There are, maybe the last question,
[00:56:28.760 --> 00:56:30.800]   because we're at almost at the end of the time
[00:56:30.800 --> 00:56:31.920]   that we have scheduled.
[00:56:31.920 --> 00:56:35.480]   There are several questions about fine tuning data.
[00:56:35.480 --> 00:56:38.120]   Like how do you decide what to put
[00:56:38.120 --> 00:56:40.560]   into your fine tuning data set?
[00:56:40.560 --> 00:56:42.560]   How do you evaluate the quality of it?
[00:56:42.560 --> 00:56:44.560]   Do you have any generic advice?
[00:56:44.560 --> 00:56:46.160]   It's obviously all use case specific,
[00:56:46.160 --> 00:56:50.240]   but any things that you have seen work or not work?
[00:56:50.240 --> 00:56:51.840]   - I think you can give the answer that I'm gonna give,
[00:56:51.840 --> 00:56:52.920]   which is test it.
[00:56:52.920 --> 00:56:55.080]   Test it on small scale models
[00:56:55.080 --> 00:56:56.480]   and test it on small scale data sets
[00:56:56.480 --> 00:56:58.120]   and find out for yourself.
[00:56:58.120 --> 00:57:00.000]   That's the answer to pretty much every question here
[00:57:00.000 --> 00:57:02.640]   is there isn't really any generic advice.
[00:57:02.640 --> 00:57:06.680]   The advice is be a scientist and spend your budget wisely.
[00:57:06.680 --> 00:57:08.720]   The rest is really, you know,
[00:57:08.720 --> 00:57:10.200]   I wish we had hard and fast rules,
[00:57:10.200 --> 00:57:11.600]   but we're not there as a community yet.
[00:57:11.600 --> 00:57:13.480]   We're not there as a scientific field
[00:57:13.480 --> 00:57:15.840]   and the best you can do is be a scientist yourself.
[00:57:15.840 --> 00:57:19.480]   - Maybe one final question,
[00:57:19.480 --> 00:57:21.360]   because I think you might have good answer for this.
[00:57:21.360 --> 00:57:24.400]   What's your preferred framework for multi GPU training?
[00:57:24.400 --> 00:57:29.920]   - So my preferred framework personally,
[00:57:29.920 --> 00:57:32.680]   I like our Mosaic ML LLM Foundry.
[00:57:32.680 --> 00:57:34.960]   So that's what we use for all of our models.
[00:57:34.960 --> 00:57:36.600]   It's built on PyTorch FSTP,
[00:57:36.600 --> 00:57:40.000]   fully shorted distributed or fully shorted data parallel.
[00:57:40.000 --> 00:57:41.840]   I always get those words mixed up,
[00:57:41.840 --> 00:57:43.520]   which I personally think is the best
[00:57:43.520 --> 00:57:45.680]   of breed distributed training library today.
[00:57:45.680 --> 00:57:48.880]   So, you know, that's kind of,
[00:57:48.880 --> 00:57:50.760]   that's what we've preferred at Mosaic ML.
[00:57:50.760 --> 00:57:51.800]   Even if you don't use our code base,
[00:57:51.800 --> 00:57:53.360]   we really like PyTorch FSTP.
[00:57:53.360 --> 00:57:56.800]   Awesome. Thank you so much.
[00:57:56.800 --> 00:57:58.000]   This was a wonderful session.
[00:57:58.000 --> 00:58:00.400]   I really appreciate the questions.
[00:58:00.400 --> 00:58:02.520]   And, you know, I look forward to seeing everybody again
[00:58:02.520 --> 00:58:04.280]   in a couple of weeks.
[00:58:04.280 --> 00:58:05.280]   - Thanks a lot, Jonathan.
[00:58:05.280 --> 00:58:06.640]   We appreciate this session.
[00:58:06.640 --> 00:58:07.600]   This has been amazing.
[00:58:07.600 --> 00:58:08.440]   Thank you.
[00:58:08.440 --> 00:58:11.840]   [your own personal Eurovision]

