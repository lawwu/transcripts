
[00:00:00.000 --> 00:00:26.560]   Hey, everybody, how are we today? Can you guys hear me okay? Yeah, I can hear you. Yeah.
[00:00:26.560 --> 00:00:30.560]   Just ask which paper are we reading? Yeah, I see the squeeze and excitation. Thanks. Yeah.
[00:00:30.560 --> 00:00:33.920]   Correct. Today's all about squeeze and excitation.
[00:00:33.920 --> 00:00:42.480]   Okay, cool. With that being said, let's get started with squeeze and excitation. And
[00:00:42.480 --> 00:00:48.320]   so in terms of paper reading groups, we're left with squeeze and excitation and then
[00:00:51.040 --> 00:00:57.840]   until we now would be efficient net. So for squeeze and excitation, it's a really, really
[00:00:57.840 --> 00:01:04.480]   interesting paper, I feel and it is also one of the influential papers that defines attention in
[00:01:04.480 --> 00:01:10.720]   a way and attention. Basically, squeeze and excitation is a type of attention. And you will
[00:01:10.720 --> 00:01:17.360]   get that once we look at this paper. But cool fact is like this paper, if you're aware of the
[00:01:18.000 --> 00:01:25.120]   ImageNet competition, then squeeze and excitation was the 2017 winner, which is which is written
[00:01:25.120 --> 00:01:32.800]   here. So I scroll in, I mean, it's this bit, if you can see the one, the first place and the top
[00:01:32.800 --> 00:01:43.040]   five error was around 25%, which improved the 2016 by 2016 winner by 25%. That's the relative
[00:01:43.040 --> 00:01:50.080]   improvement. So that's just some cool facts to get started with squeeze and excitation. But as usual,
[00:01:50.080 --> 00:01:55.360]   if we're gonna go if I go to the forums, we're just going to ask all our questions through the
[00:01:55.360 --> 00:02:07.120]   forums. So let me just go to the forum link right now. And can you guys able to see my phone? So
[00:02:07.920 --> 00:02:18.640]   not yet. No, we see the one note. Oh, good. Let me just, I guess I'm sharing my
[00:02:18.640 --> 00:02:27.680]   I guess the shame that okay, so now you should be able to see the screen. So this brings me
[00:02:27.680 --> 00:02:33.920]   to the forum. So I'm just going to paste that link. One second. I think that link and I paste
[00:02:33.920 --> 00:02:45.280]   that. So everybody should have the forum link now. Yes, thank you. Let me just paste that in.
[00:02:45.280 --> 00:02:53.760]   So just give me one second. There's always some admin stuff to do.
[00:02:53.840 --> 00:02:54.320]   Okay,
[00:02:54.320 --> 00:03:23.040]   with that being said, let's get started. So I'm going to spend about 60 minutes,
[00:03:23.040 --> 00:03:28.080]   at least that's my aim. So I don't want to extend the meeting any further. That's just to
[00:03:28.080 --> 00:03:32.560]   keep it short and concise and effective and productive. So something I've done differently
[00:03:32.560 --> 00:03:39.280]   this week or in compared to all the past weeks is that I've already highlighted the key things that
[00:03:39.280 --> 00:03:43.040]   I feel when I was reading this paper. These are the key things that I feel I want to share with
[00:03:43.040 --> 00:03:48.640]   everybody. So let's get started. And then we all know that the abstract is the most important part.
[00:03:49.200 --> 00:03:54.320]   So as we read through the abstract, let's get started. So this is where we're going to start.
[00:03:54.320 --> 00:03:58.000]   And then if there's any questions, I'm just going to head over to the forums. And please,
[00:03:58.000 --> 00:04:04.960]   enter all your questions at the forum at the link that I've provided. So far, so good. I hope.
[00:04:04.960 --> 00:04:13.280]   I'm assuming everything's good so far. So I'm just going to get started with the paper.
[00:04:13.280 --> 00:04:17.520]   So then the central building block of convolutional neural networks is the convolution
[00:04:17.520 --> 00:04:22.640]   operator, which enables networks to form constructive, informative features by using
[00:04:22.640 --> 00:04:28.560]   by fusing both spatial and channel-wise information within local receptive fields at each layer.
[00:04:28.560 --> 00:04:33.920]   So that looks like a very heavy line when I first read it. But basically, all that is saying is,
[00:04:33.920 --> 00:04:40.400]   if you have an RGB image, so like if this is my red, green,
[00:04:43.040 --> 00:04:49.520]   and oh, sorry, blue, green, and let's say red. So that becomes my RGB image. So then what does
[00:04:49.520 --> 00:04:55.680]   convolution do? For those of you joining me from Fastbook, you would know convolution is nothing
[00:04:55.680 --> 00:05:01.840]   but like this 3 by 3 kernel, something like this, that pretty much joins,
[00:05:01.840 --> 00:05:11.360]   that pretty much joins, it's like this 3 by 3 that joins the spatial dimensions, which are these
[00:05:11.360 --> 00:05:19.120]   ones. So this is height and this much is width. And then it's also joining across the channels,
[00:05:19.120 --> 00:05:26.560]   right? So all that this first line is saying is that the convolution operation is very local.
[00:05:26.560 --> 00:05:32.800]   So as you can see, what it means by being local is that it's just at this part of the image
[00:05:32.800 --> 00:05:38.160]   instead of the whole image. So if the whole image is 2 by 4 by 2 by 4, the convolution operation is
[00:05:38.160 --> 00:05:44.000]   either 7 by 7 or 3 by 3. But that's again a very local operation. So it's only affecting a small
[00:05:44.000 --> 00:05:49.600]   part of the image instead of the whole image. So that's the main key thing that the authors start
[00:05:49.600 --> 00:05:58.720]   with. And they go, okay, if we want to improve the relationship, what we focus on instead,
[00:05:58.720 --> 00:06:03.360]   this is this bit here. So what we focus on instead is on the channel relationship.
[00:06:04.320 --> 00:06:10.080]   So what they want to do in terms of this paper is that they want to update or improve the channel
[00:06:10.080 --> 00:06:14.320]   relationship. Because right now what's happening is that this channel relationship exists,
[00:06:14.320 --> 00:06:20.560]   but it's very A, in a local sense, and then B, there's no specific weights or attention in the
[00:06:20.560 --> 00:06:26.560]   channels. They just get added. So if you remember the-- actually, let me just showcase the blog as
[00:06:26.560 --> 00:06:34.960]   well. So I'm going to paste this link in the chat for those interested. So basically, as you can see
[00:06:34.960 --> 00:06:39.440]   here, this is what happens in a convolution operation. So you have your red, green, and blue
[00:06:39.440 --> 00:06:47.920]   channels. And then this 3 by 3 filter is going to go through the whole image. And it creates basically
[00:06:47.920 --> 00:06:52.480]   these feature maps. So these are like these small feature maps. So these red, green, and blue
[00:06:52.480 --> 00:06:58.960]   feature maps get created. Am I sharing the right screen or no? OK, I'm not. Sorry, I'm speaking
[00:06:58.960 --> 00:07:04.880]   through the wrong screen. Can you guys see my blog at the moment? Yes, we see your blog. Yes.
[00:07:04.880 --> 00:07:11.120]   OK, perfect. Thank you. I was just sharing the wrong screen again. OK. So then we have this red,
[00:07:11.120 --> 00:07:15.680]   green, and blue. Imagine this is your image. This is your red, green, and blue.
[00:07:15.680 --> 00:07:21.440]   And then what happens is your convolution kernel pretty much goes through the whole image towards
[00:07:21.440 --> 00:07:25.600]   the end. And you get these feature maps. These are your little feature maps that get created
[00:07:25.600 --> 00:07:30.960]   towards the end. So those red, green, and blue feature maps. And then what happens is these red,
[00:07:30.960 --> 00:07:35.680]   green, and blue feature maps just get added to give you the output from the convolution.
[00:07:35.680 --> 00:07:40.480]   Does that make sense? So far, this is something that we know. This is just a convolution operation.
[00:07:40.480 --> 00:07:47.760]   We are not even discussing the paper right now. So that's what this first few sentences tell me,
[00:07:47.760 --> 00:07:56.000]   is that-- are we back to OneNote now, I'm assuming? Yes. Perfect. So then this is what
[00:07:56.000 --> 00:08:01.760]   the main thing is. In convolution, in standard convolution, it's a very standard operation.
[00:08:01.760 --> 00:08:06.880]   You just go through the A, it's local, and B, there's no channel weights. So what this paper
[00:08:06.880 --> 00:08:12.720]   is trying to do is-- this is the one highlighted in green-- is that we want to adaptively recalibrate
[00:08:12.720 --> 00:08:19.040]   channel-wise feature response by modeling interdependencies between channels. So what
[00:08:19.040 --> 00:08:23.120]   that means is they're telling us they want to do something related to channels. They don't like
[00:08:23.120 --> 00:08:28.400]   the fact that the channel just gets added. Pretty much, you have a convolution output from each of
[00:08:28.400 --> 00:08:32.400]   the channels, and then they just get added. Those features get added to give you the final convolution
[00:08:32.400 --> 00:08:37.840]   output. And what they want to say is, as researchers, we want to try something to do differently over
[00:08:37.840 --> 00:08:42.880]   here. So that's the main abstract. That's the main-- I hope through this I'm being able to provide
[00:08:42.880 --> 00:08:48.560]   some intuition on what this paper looks like. But if not, I'll summarize again. The main idea is
[00:08:48.560 --> 00:08:53.840]   we want to look at the channels. So we want to be able to add channels in a way that there's
[00:08:53.840 --> 00:08:58.400]   some attention, there's some weights, there's some way to say, OK, this channel is more important
[00:08:58.400 --> 00:09:04.080]   than the last one. And convolution does this, but it's very implicit. What they want to do is they
[00:09:04.080 --> 00:09:12.720]   want to add more steps on top. OK, so that's that. So next, moving into introduction. At each
[00:09:12.720 --> 00:09:17.760]   convolution, it's pretty much just the spatial and connectivity. And it fuses the spatial and
[00:09:17.760 --> 00:09:21.760]   channel-wise information with local receptive fields. That's something I've already explained
[00:09:21.760 --> 00:09:29.440]   here in this little image. So as you can see, it's just local. So these H and W are the spatial
[00:09:30.080 --> 00:09:34.400]   filters, or this is the spatial information. And then it's fusing the channel information.
[00:09:34.400 --> 00:09:38.000]   So all it is saying here is that it's fusing spatial and channel-wise information.
[00:09:38.000 --> 00:09:44.880]   And by doing all of this, like by having multiple convolution layers and having non-linear activation
[00:09:44.880 --> 00:09:48.240]   functions, which is like convolution layer followed by value, followed by convolution,
[00:09:48.240 --> 00:09:53.520]   followed by value, followed by batch norm-- so basically, con, batch, norm, value-- if you do
[00:09:53.520 --> 00:09:58.960]   that multiple times, that becomes your convolution neural network. And by doing something like this--
[00:09:58.960 --> 00:10:03.360]   and this is just talking about introductions. So this is just introducing convolutions to us.
[00:10:03.360 --> 00:10:08.080]   And it says, through this, the CNNs, or the convolution neural networks,
[00:10:08.080 --> 00:10:13.040]   are able to perform or capture hierarchical patterns and global theoretical receptive
[00:10:13.040 --> 00:10:20.000]   fields. So all it is saying, OK, by doing repeatedly of these things, convolutional
[00:10:20.000 --> 00:10:27.440]   networks are able to do something, which is have a feature map that does represent the input image.
[00:10:27.440 --> 00:10:34.400]   So that's the main thing here. So in the recent research, as of 2017, when this paper,
[00:10:34.400 --> 00:10:38.400]   I think, came out, which was around 2017-- sorry, one second.
[00:10:38.400 --> 00:10:49.200]   So as part of this recent research, what the main idea or the main thing that-- main focus was on
[00:10:49.200 --> 00:10:54.960]   spatial correlations. And they say, in this paper, we investigate a different aspect of the network
[00:10:54.960 --> 00:10:59.680]   design, which is the relationship between channels. So repeating again, in this paper, we're going to
[00:10:59.680 --> 00:11:11.600]   do something related to the channels. So instead of-- let me share. So instead of the channel--
[00:11:11.600 --> 00:11:16.320]   these are just the channel-- the convolution outputs of each of the channel. Instead of these
[00:11:16.320 --> 00:11:21.360]   just being getting added, what we're going to try to do is add some weights. So it is possible that
[00:11:21.360 --> 00:11:25.600]   the red channel might be more important than the blue channel or something like that. And we want
[00:11:25.600 --> 00:11:34.800]   to introduce that information in this part of the network. So that's the main thing.
[00:11:34.800 --> 00:11:43.520]   OK, so then this then brings us to this idea. We introduce a new architectural prop,
[00:11:43.520 --> 00:11:49.120]   which they call squeeze and excitation, which is a very fancy name. And we look at what squeeze
[00:11:49.120 --> 00:11:53.920]   and excitation means and what exactly is the squeeze operation, and then finally, what exactly
[00:11:53.920 --> 00:11:59.760]   is the excitation operation. And the goal is to improve the quality of the representation
[00:11:59.760 --> 00:12:07.440]   by explicitly modeling the interdependencies between the channels of its convolution features.
[00:12:07.440 --> 00:12:12.240]   So that's, again, coming to that main idea. All we want to do is we want to focus on the channels.
[00:12:12.240 --> 00:12:18.480]   If you have three channel outputs, the way you add them is what is this paper all about. So we're
[00:12:18.480 --> 00:12:23.200]   going to focus on that. We're just going to focus on the channel interdependencies instead of these
[00:12:23.200 --> 00:12:27.120]   ratios. So we're going to focus on how the red, green, and blue channels get added.
[00:12:27.120 --> 00:12:34.000]   OK, so the structure of the SE building block is depicted in figure one. So that's figure one.
[00:12:34.000 --> 00:12:41.760]   So let me try and explain the squeeze and excitation operation as part of this. And then
[00:12:41.760 --> 00:12:49.120]   we'll go back to that reading. So you have your input image here. This is my input image.
[00:12:49.120 --> 00:12:54.560]   Then you perform some transformation. This transformation could be a convolution operation,
[00:12:54.560 --> 00:12:58.960]   or this transformation could be a press net, which is just like-- I mean, it could be any
[00:12:58.960 --> 00:13:04.800]   transformation. So that's why it's a very generic FDR, which is just some transformation, which
[00:13:04.800 --> 00:13:08.800]   generally could be either two convolutions followed by a patch norm or just a convolution.
[00:13:09.520 --> 00:13:16.960]   And you get a feature map called U. So you can see how the input initial height width were h dash,
[00:13:16.960 --> 00:13:24.560]   w dash, and c dash. And the next widths are height, w, and c. And then what you do is you
[00:13:24.560 --> 00:13:30.800]   do the squeeze operation. So this is the squeeze operation up here. So what does the squeeze
[00:13:30.800 --> 00:13:38.320]   operation do? Well, it gives me an output of 1 by 1 by c. So what does that mean? We're going to
[00:13:38.320 --> 00:13:47.120]   squeeze the height and width. So as you can see, this thing looks like-- this is what U looks like.
[00:13:47.120 --> 00:13:55.760]   This is width over here. And this is height over here. And then you have c. So what I could do is
[00:13:55.760 --> 00:14:02.800]   I could use a global average pooling gap. So what that gap will do is that for each of the channels,
[00:14:03.920 --> 00:14:09.200]   it will go here. And it will basically take the average of this whole height and width.
[00:14:09.200 --> 00:14:12.320]   So if you take the average of height and width, you get one number.
[00:14:12.320 --> 00:14:19.920]   So you get a single number. And then if you do the same thing for this channel at the back,
[00:14:19.920 --> 00:14:29.360]   this channel at the back, you get a second number. So you get a second number and so on.
[00:14:29.360 --> 00:14:33.600]   So you could get a number for each of the channels. So this is just one number. That's
[00:14:33.600 --> 00:14:40.560]   why the dimensions are 1 by 1 by c. Because for each of the channels, you get a single number.
[00:14:40.560 --> 00:14:47.440]   Does that make sense? All we're doing is we're just squeezing across the height and width dimension,
[00:14:47.440 --> 00:14:53.600]   just the height and width. The way this figure is drawn is like c by h by w, whereas the way I've
[00:14:53.600 --> 00:14:58.400]   drawn the figure is h by w by c. So don't be surprised on how these things mean. But basically,
[00:14:58.400 --> 00:15:03.280]   all we're doing is we're just going to go through each of the channels. And we just take the global
[00:15:03.280 --> 00:15:09.280]   average for you across the height and width. So this gives me a 1 by 1 by c. So that's the squeeze
[00:15:09.280 --> 00:15:15.680]   operation. And then what we do is we do some excitation. So this is the excitation operation.
[00:15:15.680 --> 00:15:22.480]   I will tell you what the excitation operation is. But basically, the output of the excitation
[00:15:22.480 --> 00:15:26.000]   operation is this. What are these? These are the channel weights.
[00:15:31.280 --> 00:15:37.440]   What are channel weights? Basically, as you can see, each channel is a different color. So what
[00:15:37.440 --> 00:15:46.880]   that means is now if we have, say, 128 channels, each channel gets its own weight. Instead of those
[00:15:46.880 --> 00:15:56.000]   channels just being added simply as it is the case in terms of a standard convolution, in this case,
[00:15:56.000 --> 00:16:02.240]   what's happening is that each of these channels, they get a certain weight. And these weights
[00:16:02.240 --> 00:16:06.560]   could be anything. But we've actually programmatically found them. We do not define
[00:16:06.560 --> 00:16:11.360]   the weights. We don't say the red channel is more important or the green channel is more important.
[00:16:11.360 --> 00:16:15.280]   We let the model figure it out. We're going to let the model figure it out through backward
[00:16:15.280 --> 00:16:21.520]   propagation and through forward passes, which is just the way of deep learning way of doing things.
[00:16:21.520 --> 00:16:25.280]   So we're going to let the model figure it out. And we're going to let the model say, OK, you
[00:16:25.280 --> 00:16:29.120]   figure it out of like, is the green channel more important than the red? Or basically,
[00:16:29.120 --> 00:16:33.200]   each channel gets its own weight. So you can see how it is a form of attention.
[00:16:33.200 --> 00:16:45.040]   This is just a form of attention because we're just defining standard weights for each of the
[00:16:45.040 --> 00:16:50.320]   channels. And then once I have these weights, I could just multiply this whole thing with these
[00:16:51.120 --> 00:16:59.440]   weights to get my output. As you can see, this becomes my output. So now something happened.
[00:16:59.440 --> 00:17:10.000]   OK, there we go. So as you can see, then, that's what happens. It's like each of the channel
[00:17:10.000 --> 00:17:14.000]   gets some weight that we defined using this squeeze and excitation operation.
[00:17:14.000 --> 00:17:20.880]   OK, I hope that so far is so good. And I hope that I've been able to provide an intuition
[00:17:20.880 --> 00:17:26.640]   on what squeeze and excitation is so far. Basically, all we're trying to do is we're
[00:17:26.640 --> 00:17:32.000]   trying to define some channel weights for my feature map. So this is what squeeze and
[00:17:32.000 --> 00:17:36.640]   excitation does. Squeeze takes a global average pooling. So you get one number per channel.
[00:17:36.640 --> 00:17:41.600]   And then excitation does some excitation operation, which I will explain what it is,
[00:17:41.600 --> 00:17:47.840]   to basically convert those numbers into channel weights. So right now, what you had, you had just
[00:17:47.840 --> 00:17:52.960]   this 1 by 1 by c. The excitation operation converts these into channel weights. And then you can use
[00:17:52.960 --> 00:17:58.000]   the channel weights, multiply that by my u feature map to get this final output.
[00:17:58.000 --> 00:18:02.880]   Let me just head over to the forums to see if there's any questions.
[00:18:02.880 --> 00:18:17.760]   Is excitation block element-wise operation on the NLP? I haven't explained what an excitation
[00:18:17.760 --> 00:18:23.120]   block is yet. So let's not jump into what excitation is. Just think of excitation as
[00:18:23.120 --> 00:18:30.800]   being something that converts basically a 1 by 1 channel or this vector into something called
[00:18:30.800 --> 00:18:35.520]   channel weights at the moment. So let's not worry about what excitation is. As long as the main or
[00:18:35.520 --> 00:18:40.480]   big picture of what squeeze and excitation is together, as long as that is clear, I'm good.
[00:18:40.480 --> 00:18:43.680]   So if you have any questions about this big picture, please ask me now.
[00:18:47.760 --> 00:18:52.080]   Looks like nobody's typing. So I'm just going to assume that the big picture is clear with everybody.
[00:18:52.080 --> 00:18:54.800]   And let's just move on.
[00:18:54.800 --> 00:19:07.920]   Okay, so we were reading further. So let's read what's written in the paper now. So it says,
[00:19:07.920 --> 00:19:12.720]   the structure, so I'm reading here, the structure of the SE building block is depicted in figure
[00:19:12.720 --> 00:19:17.200]   one, which you've already looked. For any given transformation, this is a very generic transformation,
[00:19:17.200 --> 00:19:24.080]   you have an input x that's mapped to a feature map u, which is this h by w by c.
[00:19:24.080 --> 00:19:31.360]   And this could be a convolution or any basically transformation. So the features u are first passed
[00:19:31.360 --> 00:19:36.640]   through a squeeze operation. As you can see, this is the features u. They get passed through a squeeze
[00:19:36.640 --> 00:19:43.840]   operation first, and this gives me a vector. And then this is what it says, and then which produces
[00:19:43.840 --> 00:19:48.560]   a channel descriptor by aggregating feature maps across the spatial dimensions, which is just the
[00:19:48.560 --> 00:19:54.240]   global average pooling that I've explained. Then it says this aggregation is followed by an
[00:19:54.240 --> 00:19:59.360]   excitation operation. So you can see here, now you take this aggregated values or like this vector
[00:19:59.360 --> 00:20:04.240]   one by one by c, and you take these aggregated values, you perform an excitation operation to
[00:20:04.240 --> 00:20:08.800]   give you the channel weights. So this is what's written here, which takes the form of a simple
[00:20:08.800 --> 00:20:13.440]   self-grading mechanism. So don't worry about these like words like self-grading or things like that,
[00:20:13.440 --> 00:20:17.920]   I will explain exactly what the excitation operation is. But as far as it is clear,
[00:20:17.920 --> 00:20:23.520]   that all it does is that gives me a per channel modulation weight. So this is important.
[00:20:23.520 --> 00:20:32.080]   It gives me this per channel modulation weights. So all I'm doing now through the squeeze and
[00:20:32.080 --> 00:20:41.440]   excitation operation is that I'm just finding per channel weights. So then it says, it is possible
[00:20:41.440 --> 00:20:45.920]   to construct SE network by simply stacking a collection of SE blocks, which is correct. Like
[00:20:45.920 --> 00:20:49.760]   SE block is nothing, it's just a squeeze and excitation operation. So you can have them
[00:20:49.760 --> 00:20:54.000]   anywhere in a best net, in an efficient net, pretty much like anywhere you want, you could
[00:20:54.000 --> 00:21:01.440]   add these squeeze and excitation, and that should be fine. So SE blocks can be used as a drop-in
[00:21:01.440 --> 00:21:06.800]   replacement for the original blocks at a range of depths in the network architecture. And then
[00:21:06.800 --> 00:21:12.080]   the key point is here, the role it performs at different depths differs throughout the network.
[00:21:12.080 --> 00:21:18.320]   So it is very intuitive to think that the role the squeeze and excitation operation is performing.
[00:21:18.320 --> 00:21:23.440]   So think of it as just basically adding the channel weights. It's basically just defining
[00:21:23.440 --> 00:21:28.320]   the channel weights and the way they get combined, that's what the squeeze and excitation operation
[00:21:28.320 --> 00:21:35.120]   is defining. And then it's definitely very intuitive to think that they do different
[00:21:35.120 --> 00:21:40.480]   things because in the earlier layers, the model does not know of classes, right? The model just
[00:21:40.480 --> 00:21:44.560]   learns some generic features in the earlier layers, it's just low level representations.
[00:21:44.560 --> 00:21:49.600]   So that's what it says. In the earlier layers, it excites informative features in a class-agnostic
[00:21:49.600 --> 00:21:54.320]   manner, strengthening the shared low-level representations. And then in the later layers,
[00:21:54.320 --> 00:21:59.040]   it's about highly class-specific manner. Because as we go through the network, in the earlier
[00:21:59.040 --> 00:22:03.760]   layers, the model does not know about anything like if we're doing cats versus dogs, but towards
[00:22:03.760 --> 00:22:08.160]   the later layers of the network, the model starts to know, okay, this is a pit bull terrier, or this
[00:22:08.160 --> 00:22:14.720]   is like example, or like this is a specific breed of a cat. So this operation of squeeze and
[00:22:14.720 --> 00:22:19.760]   excitation across this model is going to vary by depth because in the deeper layers, it's going to
[00:22:19.760 --> 00:22:26.160]   do something, in the deeper layers, it's going to try and do something that it knows about the
[00:22:26.160 --> 00:22:32.960]   classes already. And in the earlier layers, it does not know about the classes. Okay, that's the
[00:22:32.960 --> 00:22:38.880]   main bits. I'm just going to check if there's any more questions. None so far. So that's good.
[00:22:38.880 --> 00:22:49.680]   Now, going forward. So this is just the standard, like I like to spend most of my time on abstract
[00:22:49.680 --> 00:22:54.880]   and introduction because my gut feeling tells me like if I don't understand the paper,
[00:22:54.880 --> 00:23:00.880]   by reading abstract and introduction, I'm missing something, or the paper is not written well.
[00:23:00.880 --> 00:23:06.080]   Like, but mostly it's 99% of the time, it's me, and I'm missing something. But right now,
[00:23:06.080 --> 00:23:10.960]   I do understand the basic idea of the paper. I do understand the main things that this paper
[00:23:10.960 --> 00:23:18.240]   is going to introduce. Okay, so next. So in this paragraph, they're always saying it's like the
[00:23:18.240 --> 00:23:24.400]   design and development of CNN architectures is a difficult task. But this SE block is really
[00:23:24.400 --> 00:23:29.520]   simple. And I do agree that the SE block is really simple. All it is doing is like,
[00:23:29.520 --> 00:23:33.600]   it's just adding two operations, which is squeeze and excitation, and we could add them anywhere in
[00:23:33.600 --> 00:23:41.760]   the network. So then it says, to provide evidence for these claims, we did extensive evaluation on
[00:23:41.760 --> 00:23:47.520]   the ImageNet data set. And they say we banked first on the 2017 classification competition.
[00:23:47.520 --> 00:23:52.080]   So those are the main highlights, or those are the main introduction points of squeeze and
[00:23:52.080 --> 00:23:58.000]   excitation. So now we are ready to look at the squeeze and excitation blocks in more detail.
[00:23:58.000 --> 00:24:12.560]   There's a question, I'm just going to have a look at the question now.
[00:24:15.360 --> 00:24:19.920]   When we have multiple output channels, filters in each layer, they have different weights for
[00:24:19.920 --> 00:24:26.160]   each input channel. Does this not give the same effect as SE block? Great, good question, Ramesh.
[00:24:26.160 --> 00:24:32.480]   And I was hoping somebody would ask me this. SE block, all it is doing is it's making very
[00:24:32.480 --> 00:24:37.200]   explicit. Of course, implicitly in convolution, and this is something that's mentioned in the
[00:24:37.200 --> 00:24:43.520]   paper as well, that implicitly, convolution already has a way of like having different weights
[00:24:43.520 --> 00:24:49.120]   for channels. But it's not explicit. All we're doing is we're just adding a little bit of the
[00:24:49.120 --> 00:24:55.040]   computation on top to make sure that each channel gets its own attention. Or like we know that
[00:24:55.040 --> 00:25:00.640]   this channel is getting a specific weight, and then this channel is getting a specific weight.
[00:25:00.640 --> 00:25:07.600]   Through squeeze and excitation, this is like update ResNet, but like just a tiny bit. So
[00:25:07.600 --> 00:25:13.440]   squeeze and excitation is just a tiny bit of that modification. So your understanding is correct,
[00:25:13.440 --> 00:25:18.080]   that when we're doing convolution filters, each filter is like learning its own thing.
[00:25:18.080 --> 00:25:23.200]   But between the channels, if you look at that blog again, what we're doing is once you have
[00:25:23.200 --> 00:25:27.440]   these outputs, all we do is we just add them together. So this is something, even though
[00:25:27.440 --> 00:25:32.640]   it's implicitly different, like these weights could be different, with squeeze and excitation,
[00:25:32.640 --> 00:25:36.720]   all we're doing is we're just making it very explicit. We're just adding a computation,
[00:25:36.720 --> 00:25:41.280]   and we're just making it very obvious that now the network knows like some channels are more
[00:25:41.280 --> 00:25:46.320]   important than the other. That's all we're doing. I hope that answers the question.
[00:25:46.320 --> 00:25:59.280]   Okay, back to the squeeze and excitation box. Okay, so this is just an introduction. In this
[00:25:59.280 --> 00:26:03.760]   part of the paper, all they're saying is we have a, this is just like some bit of math.
[00:26:03.760 --> 00:26:09.120]   There's just some representation. It's like we have an FDR, which is my transformation.
[00:26:09.120 --> 00:26:14.000]   I have my input, which by applying this transformation, it converts my input x to
[00:26:14.000 --> 00:26:20.800]   the feature map u. So this is just a convolution operation. It could be anything. This is again
[00:26:20.800 --> 00:26:26.800]   here, Ramesh, this is the thing you were asking me. The channel relationships modeled by convolution
[00:26:26.800 --> 00:26:33.280]   are inherently implicit and local, right? We expect the learning of convolution features to
[00:26:33.280 --> 00:26:38.160]   be enhanced by explicitly modeling channel interdependencies. So the network is able to
[00:26:38.160 --> 00:26:44.400]   increase its sensitivity to informative features, which can be exploited by subsequent transformations.
[00:26:44.400 --> 00:26:51.120]   So all they're saying is, again, repeating what I said, it's implicit and local, but squeeze and
[00:26:51.120 --> 00:26:57.520]   excitation is just doing that extra thing by making it explicit and obvious. That's the main thing.
[00:26:57.520 --> 00:27:07.360]   Okay, so now in the squeeze operation, as you can see, all it does is
[00:27:07.600 --> 00:27:10.240]   uh oh, one second.
[00:27:10.240 --> 00:27:19.760]   If I'm going back here, if you remember convolution, sorry, not this one, over here.
[00:27:19.760 --> 00:27:26.000]   If you remember convolution, it's just strictly local. And it's like, it doesn't have a global
[00:27:26.000 --> 00:27:32.560]   context. So the squeeze operation, as I said, when we do something like this, when we take the whole
[00:27:32.560 --> 00:27:38.080]   height and width, and we just take the average, we are actually giving the model with information
[00:27:38.080 --> 00:27:44.000]   with a global context. Because now the model has some information about outside of the local
[00:27:44.000 --> 00:27:49.920]   receptive field. Does that make sense? Like, if this is, oh sorry,
[00:27:49.920 --> 00:27:59.840]   if this is my whole image, and say the convolution filter is just in here,
[00:28:00.640 --> 00:28:07.520]   by doing the squeeze and excitation operation, we are also allowing the model to have information
[00:28:07.520 --> 00:28:11.280]   outside of this receptive field. Because it's just going to take an average, but in taking
[00:28:11.280 --> 00:28:18.480]   that average, we have some, in some way, we have captured the information of outside this local
[00:28:18.480 --> 00:28:25.440]   receptive field. So that's the main thing that they say here. And then they say, okay, all we do
[00:28:25.440 --> 00:28:30.160]   is it's just a global average pooling. That's pretty much it. So we're just shrinking the
[00:28:30.160 --> 00:28:34.720]   spatial dimensions edge across w. So the final thing that you get, this is just a mathematical
[00:28:34.720 --> 00:28:39.200]   way of saying global average pooling. This is nothing. It's just global average pooling.
[00:28:39.200 --> 00:28:45.200]   I've said that twice, but I hope it's clear. Like, all they're doing is,
[00:28:45.200 --> 00:28:49.600]   what is global average pooling? If you have
[00:28:51.840 --> 00:28:58.720]   like a channel, this is my, say, image, and then everything in here is number, right? 0.1,
[00:28:58.720 --> 00:29:06.000]   minus 0.3. Basically, if it's normalized, it's between 0 and 1. So 0.8, actually, it won't be
[00:29:06.000 --> 00:29:10.640]   minus 0.3. It would be any number. Like, it's just representing white and black, right? This could be
[00:29:10.640 --> 00:29:15.600]   0.3, so on. And these are all numbers. All you do is you take the average, so it gives you like a
[00:29:15.600 --> 00:29:21.120]   number called 0.2 to 5. That's just global average pooling. That's all it does. You're just taking
[00:29:21.120 --> 00:29:30.960]   average. So the squeeze operation, again, remember, if we do something like this, the squeeze
[00:29:30.960 --> 00:29:36.640]   operation will give me just this vector, where, because I'm taking the average across all the
[00:29:36.640 --> 00:29:44.560]   channels, so each channel will give me one number. And then these are those one numbers. So all I
[00:29:44.560 --> 00:29:54.480]   have is I have a vector for cChannelLong, right? Next. Next thing that we're going to look at is
[00:29:54.480 --> 00:30:00.400]   the excitation operation. So in the excitation operation, what is this doing? So now, so far,
[00:30:00.400 --> 00:30:13.840]   what we have is we have a vector that looks something like this. 0.1, 0.25, 0.33, 0.98,
[00:30:14.560 --> 00:30:23.520]   0.01, 0.05, and so on. But we have to convert this, like all of these, they don't add up to one,
[00:30:23.520 --> 00:30:30.800]   right? So if we add these like as channel weights, then they're not going to add up to one. So we
[00:30:30.800 --> 00:30:38.480]   need to fix that first. So what this excitation operation does is it's just this thing. It's just
[00:30:38.480 --> 00:30:56.000]   this thing. Excitation operation is, one second, is just like this. W1z, then a delta, and then W2,
[00:30:56.000 --> 00:31:03.120]   and then finally, you have this. So what do these mean? So this z is this 1 by 1 by c,
[00:31:04.400 --> 00:31:11.920]   right? What W1 is, it takes it from c to c by r. r is the reduction ratio, okay?
[00:31:11.920 --> 00:31:22.960]   And then what's this delta? This is relu. And then W2 will take you from c by r back to c.
[00:31:22.960 --> 00:31:33.120]   And then this is sigmoid. Okay. I hope that by looking at this stuff on the right,
[00:31:33.120 --> 00:31:41.120]   the excitation operation is clear. If not, let me try and explain. Okay. You have this as my,
[00:31:41.120 --> 00:31:49.760]   basically, 1 by 1 by c. So basically, you have a c long vector, right? All we do is we first
[00:31:49.760 --> 00:31:57.280]   reduce it to a smaller vector, which is c by r. So if it's like, say, 256, and say r is 8,
[00:32:00.960 --> 00:32:09.120]   that's going to be reduced to 16, I think. So 256 is 2 to the power 8, and yeah, that looks correct.
[00:32:09.120 --> 00:32:19.440]   Let's say, oh no, sorry, 32. So c by r then becomes 32. So you have a channel which is 32,
[00:32:19.440 --> 00:32:25.600]   which is through the W1 operation. And then you expand it back using, oh sorry,
[00:32:25.600 --> 00:32:31.680]   then you add a relu after this. So after this, you do a relu operation. So you still get this
[00:32:31.680 --> 00:32:40.080]   c by r long, which is 32 channels. And then the next thing that you do is you expand it back to
[00:32:40.080 --> 00:32:49.920]   c, which comes back to 256 channels. And then finally, you do sigmoid. Okay.
[00:32:52.400 --> 00:32:58.560]   So you get, what will sigmoid do? Sigmoid will make sure that everything is between 0 and 1.
[00:32:58.560 --> 00:33:05.760]   So now this would be 0.01, 0.02, 0.9, but basically, you'll see like everything adds up to
[00:33:05.760 --> 00:33:15.040]   1, right? So the main point is, here, I'm going to write everything, basically, I'm just going to say
[00:33:15.760 --> 00:33:22.720]   adds 1. So now what that does, what does that do? So as you can see, this is my, on the right,
[00:33:22.720 --> 00:33:27.840]   first, it's a very bottleneck structure, right? This is a bottleneck structure.
[00:33:27.840 --> 00:33:32.960]   You start with c channels, you reduce it to c by r, then you do a relu operation because you need
[00:33:32.960 --> 00:33:37.200]   to have some sort of non-linearity. That's just the deep learning way of doing things. And then
[00:33:37.200 --> 00:33:42.000]   you expand it back to c channels, and then you do the sigmoid. So the excitation operation is nothing
[00:33:42.000 --> 00:33:47.920]   but that. And through this excitation operations, we are able to basically learn some way of,
[00:33:47.920 --> 00:33:53.360]   like, we're able to learn the channel weights, right? The properties of sigmoid are good because
[00:33:53.360 --> 00:33:58.240]   all it will do is it will convert negative or positive, like basically all numbers to be
[00:33:58.240 --> 00:34:04.000]   between 0 and 1. I have explained that as part of our past book sessions before, but basically,
[00:34:04.000 --> 00:34:08.720]   this is the main property that everything will be between 0 and 1. So now you have
[00:34:11.200 --> 00:34:14.560]   channel weights. So I'm gonna wait for...
[00:34:14.560 --> 00:34:21.360]   So you have channel weights that look something like that. And then you can just,
[00:34:21.360 --> 00:34:24.160]   once you have these channel weights, then you can multiply that with u,
[00:34:24.160 --> 00:34:30.240]   and then you have your final output, which becomes this. Any questions on this so far?
[00:34:30.240 --> 00:34:34.400]   Let me go back to the forums to see if there's any questions.
[00:34:34.400 --> 00:34:48.880]   [silence]
[00:34:48.880 --> 00:34:54.720]   How do we know which convolution filter we applied? Why do we need to know which convolution
[00:34:54.720 --> 00:34:58.800]   filter we applied, Yash? Because I think that's not important. In terms of, like,
[00:34:58.800 --> 00:35:03.520]   the filters, that's something that gets learned by the deep learning model on its own. So we
[00:35:03.520 --> 00:35:09.200]   don't need to worry about which convolution filter gets applied. And that's not in context
[00:35:09.200 --> 00:35:16.400]   of the squeeze and excitation variable. [silence]
[00:35:16.400 --> 00:35:23.120]   Anyway, so that's that. So through this squeeze and excitation now, I have a way of, like,
[00:35:23.120 --> 00:35:29.120]   A, doing global average pooling and then having excitation operation that gives me a way of having
[00:35:29.120 --> 00:35:34.880]   channel weights. And through those channel weights now, everything gets, like, now there's some way
[00:35:34.880 --> 00:35:41.280]   of, there's this explicit way of adding weights to these channels. So there's, like, some form
[00:35:41.280 --> 00:35:48.800]   of attention in my model. And what that does is it helps improve the performance. So now the next
[00:35:48.800 --> 00:35:53.600]   question is, where do we add these, like, how do we introduce these squeeze and excitation
[00:35:53.600 --> 00:35:58.080]   operations? So, like, the main thing that they say as part of this paper, and I won't go into
[00:35:58.080 --> 00:36:03.360]   a lot of details here, they pretty much say, like, this can be integrated into any standard
[00:36:03.360 --> 00:36:07.280]   architecture. It can be inserted after the non-linearity following each convolution.
[00:36:07.280 --> 00:36:12.160]   So, like, that's true. Like, if you have a look at this here,
[00:36:12.160 --> 00:36:18.800]   I'm just going to show you the, or explain the ResNet example. And for those of you interested
[00:36:18.800 --> 00:36:27.280]   in the code side of things, in the blog, if we keep scrolling down, you'll see this is what the
[00:36:27.600 --> 00:36:33.200]   SE block looks like. And then you'll see an implementation of the ResNet block. And then
[00:36:33.200 --> 00:36:38.320]   how does that get converted to an SE basic block? And then you'll see, like, the bottleneck followed
[00:36:38.320 --> 00:36:44.080]   by the SE bottleneck, which is here. So basically, like, I'm going to show, if you read through the
[00:36:44.080 --> 00:36:50.000]   blog, you'll see things from a code perspective. And then you'll see how the SE net is built step
[00:36:50.000 --> 00:36:54.880]   by step. So how it looks like in code. But right now, from this paper reading, you get a good
[00:36:54.880 --> 00:36:59.840]   understanding of the paper. And then when you go back and you read the blog in your own time,
[00:36:59.840 --> 00:37:07.200]   you will see, like, how that makes sense. So if you see, we're just basically using fully
[00:37:07.200 --> 00:37:11.360]   connected layers, which is just like linear layers, right? So if you want to go from C to C
[00:37:11.360 --> 00:37:21.520]   by R channels, all it is, is, let me go up, up in the, here it is, it's SE block. So you can see
[00:37:21.520 --> 00:37:27.760]   the excitation operation is just these one, two, three, four, four lines of code. So it goes from
[00:37:27.760 --> 00:37:33.360]   NN dot linear, which is goes from C to C by R channels. And then there's a value operation,
[00:37:33.360 --> 00:37:38.880]   then it's an NN dot linear operation, which is from C by R all the way to C, followed by sigma,
[00:37:38.880 --> 00:37:50.880]   right? That's all of this. That should be it in terms of, like, understanding the
[00:37:50.880 --> 00:37:57.040]   squeezing excitation as in the main things here. But as you can see, then, where do we,
[00:37:57.040 --> 00:38:00.960]   now the question is, like, how do we integrate this? Like, the researchers have said it's easy
[00:38:00.960 --> 00:38:06.160]   to integrate it in any way, like in any existing architecture. But where do we integrate this?
[00:38:06.160 --> 00:38:10.720]   So if you remember the ResNet module, this was a residual branch. So you have your input,
[00:38:10.720 --> 00:38:15.520]   it goes to a residual branch. So you get some output, and then you add it to the input. So
[00:38:15.520 --> 00:38:20.080]   this is the shortcut connection on the left, and this is the residual path. So all we could do
[00:38:20.080 --> 00:38:25.600]   to convert this ResNet module to an SE ResNet module is do something like this. You have a
[00:38:25.600 --> 00:38:30.640]   residual branch, and then you add the squeeze and excitation operation, which gives you a scale,
[00:38:30.640 --> 00:38:35.760]   so you multiply that, and then you finally add the shortcut connection. So that becomes your
[00:38:35.760 --> 00:38:43.280]   SE ResNet, basically. So all these ways, like, when you see SE ResNet, SE ResNext,
[00:38:43.280 --> 00:38:48.880]   I don't know if there's SE EfficientNet, but I think there's, like, pretty much all we're doing
[00:38:48.880 --> 00:38:55.040]   is, like, we can add SE to ShuffleNet, to MobileNet, basically to any existing network architecture,
[00:38:55.040 --> 00:39:00.240]   and it's really this easy, right? And for SE ResNet, if you want to see how that gets added,
[00:39:00.240 --> 00:39:07.280]   again, this is all shown in the blog as part of this. So that's the main things.
[00:39:07.280 --> 00:39:14.160]   The next things I do want to cover off are, basically, network depth. It's like, we begin
[00:39:14.160 --> 00:39:18.480]   by comparing SE ResNet against ResNet architecture with different depths and report the results in
[00:39:18.480 --> 00:39:24.560]   Table2. So if you can see, like, how the SE ResNet is better, so you can see, I think, there's
[00:39:24.560 --> 00:39:33.680]   options here. So you can see ResNet 50, the original top one error is 24.7, but with SENet,
[00:39:33.680 --> 00:39:40.080]   the top one error is 23.29. So you can see how there's a decrease of around 1.51, which is the-
[00:39:40.080 --> 00:39:45.280]   Amit, I'm sorry, I think we're still seeing your blog, not the OneNote.
[00:39:45.280 --> 00:39:49.360]   Oh, I am so sorry. I've been talking through the paper right now.
[00:39:49.360 --> 00:39:53.200]   Thanks for pointing that.
[00:39:53.200 --> 00:40:01.520]   I'm not sure why that would happen, because I previously was sharing different applications,
[00:40:01.520 --> 00:40:06.160]   and now I am sharing my screen. So I really do apologize. What was the last-
[00:40:06.160 --> 00:40:13.760]   Okay, I've been explaining all of this through this email. Sorry, let me do that again. So
[00:40:13.760 --> 00:40:18.080]   when I moved to the blog, I was just explaining the code side of things. And since then, I've
[00:40:18.080 --> 00:40:22.400]   been talking through the research paper. Okay, let me do that again. I'll spend five more minutes to
[00:40:22.400 --> 00:40:30.160]   just say that again. So as I was saying, the question is, where do we put the SE module?
[00:40:30.160 --> 00:40:34.960]   So as you can see, as part of this image, if you have a residual branch and you have a
[00:40:34.960 --> 00:40:40.480]   shortcut connection, as you can see, you can basically add a squeeze operation anywhere.
[00:40:40.480 --> 00:40:44.560]   So if you have your input going through the residual branch, and then you add the squeeze
[00:40:44.560 --> 00:40:49.360]   operation, which gives you the scale for each of the channels, you can just add that. So this is,
[00:40:49.360 --> 00:40:54.320]   as I was saying, the squeeze and excitation is a really, really simple operation, and it makes it
[00:40:54.320 --> 00:41:00.960]   really easy to add to any network. So I'm just going to repeat, it's really easy to add it to
[00:41:00.960 --> 00:41:06.800]   ResNets. It's really easy to add to ResNets. It's really easy to add to MobileNet or ShuffleNet.
[00:41:06.800 --> 00:41:10.720]   All you have to do is you just add a squeeze operation. So you can see how it can be added
[00:41:10.720 --> 00:41:14.960]   even to an inception module. Like if on your left, this is an inception module, all you can do is you
[00:41:14.960 --> 00:41:19.600]   can add the squeeze and excitation operation, and then it becomes an SE inception module. So those
[00:41:19.600 --> 00:41:25.040]   are the main things I was talking about. And then I was reading through the paper, which is...
[00:41:25.040 --> 00:41:31.440]   So this has been covered, that it's really easy to add all of this.
[00:41:32.480 --> 00:41:37.920]   And I was going to talk through about the various experiments. So the various experiments are like,
[00:41:37.920 --> 00:41:43.120]   in terms of network depth, so I was going to show this table too. So as I can say, if the top one
[00:41:43.120 --> 00:41:52.560]   accuracy for ResNet 50 is 24.7, then we can see for SENet, it's 23.29. Or same for VGG, if the
[00:41:52.560 --> 00:42:01.360]   top one error is 27.02, we can see how that gets reduced by 1.8%. Similarly for batch
[00:42:01.360 --> 00:42:05.600]   nomenclature, like this, like all of these different things, and you can see how SE,
[00:42:05.600 --> 00:42:09.680]   or the squeeze and excitation operation actually helps improve performance.
[00:42:09.680 --> 00:42:15.440]   So this is all the integration with model architectures, this is all that it's saying.
[00:42:15.440 --> 00:42:18.640]   It's saying in the mobile setting, it's saying like you can add squeeze and excitation,
[00:42:18.640 --> 00:42:23.440]   mobile net, shuffle net, and the results are in table three. So you can see table three,
[00:42:23.440 --> 00:42:29.120]   here it is. Mobile net had a top one error of 28.4 or 29.4 in the original paper,
[00:42:29.120 --> 00:42:34.480]   and then that goes down by 3.1. So those are the main things. And then this has been added
[00:42:34.480 --> 00:42:40.000]   to various additional data sets, like CIFAR-10, CIFAR-100, even object detection on COCO.
[00:42:40.000 --> 00:42:46.320]   So the SE blocks have around 2.2% improvement on object detection. They have an improvement
[00:42:46.320 --> 00:42:52.000]   on the scene classification, so there's this data set called Places 365. There's an improvement on
[00:42:52.000 --> 00:42:57.840]   that as well. And then now we come into the ablation study side of things. So one of the,
[00:42:57.840 --> 00:43:02.480]   I do want to share like two or three research ideas with everybody as part of this paper.
[00:43:02.480 --> 00:43:07.760]   One of them is this reduction ratio, right? So what's a good reduction ratio? Because I think
[00:43:07.760 --> 00:43:15.440]   if you remember from the diagram, this one here, we're basically going from C channels from 256 to
[00:43:15.440 --> 00:43:21.920]   C by R. So in this case, I took the reduction ratio to be eight. So that's why we went from 256 to 32.
[00:43:23.520 --> 00:43:29.040]   And so the question is like, what's a good reduction ratio? Like, could it be two,
[00:43:29.040 --> 00:43:34.240]   could it be four? And that's what the question that the researchers asked as well. And they
[00:43:34.240 --> 00:43:41.840]   shared this paper, basically shared this ablation study, is like, if the reduction ratio is small,
[00:43:41.840 --> 00:43:45.600]   then the number of params are going to be higher, right? And if the reduction ratio is big,
[00:43:45.600 --> 00:43:50.080]   then the number of params is going to be low. So like, what's a good balance? So from this,
[00:43:50.080 --> 00:43:55.680]   they saw like reduction ratio at about 16. I think that's what they said, right? Yeah, here it is.
[00:43:55.680 --> 00:44:00.480]   Reduction ratio at 16 achieves a good balance between accuracy and complexity. So this is,
[00:44:00.480 --> 00:44:05.600]   as you can see, at 16 and eight, you can see that between the top and error, the difference is only
[00:44:05.600 --> 00:44:11.280]   about 0.02, but the number of params goes down by another 2 million. So it's much better to use a
[00:44:11.280 --> 00:44:19.040]   16 reduction ratio than using eight. And the main thing that they say here is that we use an
[00:44:19.040 --> 00:44:24.880]   identical ratio throughout the network. So further improvements may be achievable by tuning the
[00:44:24.880 --> 00:44:30.960]   ratios to meet the needs of given base architecture. What that means is, it's possible that
[00:44:30.960 --> 00:44:36.800]   different architectures might require a different reduction ratio. So like, say, EfficientNet might
[00:44:36.800 --> 00:44:42.800]   require by two, or basically ResNets might require by four. But again, this is a research question,
[00:44:42.800 --> 00:44:47.440]   like there's no paper that suggests, okay, this architecture should have a reduction ratio of x,
[00:44:47.440 --> 00:44:53.440]   or this architecture should have a reduction ratio of y. And similarly, there's also different
[00:44:53.440 --> 00:44:57.520]   layers. So should the earlier layers have a different reduction ratio compared to the later
[00:44:57.520 --> 00:45:02.720]   layers? So that's again, an open research question. So I think that's research idea one,
[00:45:02.720 --> 00:45:08.320]   for those of you interested and want to try these new things out. I do plan on trying them myself,
[00:45:08.320 --> 00:45:13.520]   given if I have time. But then again, for the squeeze operator, then the next thing they say,
[00:45:13.520 --> 00:45:20.080]   we're using global max pooling. But again, the operations are like the other possible thing was
[00:45:20.080 --> 00:45:24.240]   either to use global average pooling, and between global max pooling, and they share the results in
[00:45:24.240 --> 00:45:32.160]   table one, or sorry, table 11, which is here. So you can see how max gives you a top one error of
[00:45:32.160 --> 00:45:38.960]   22.57, and average gives you an error of 22.28. So using average pooling is better than max.
[00:45:38.960 --> 00:45:43.600]   But then the question I have is like, there's a way to concatenate average and max pooling. It's
[00:45:43.600 --> 00:45:50.160]   a fast AI way of doing things. And the question that I have is, would that be better than using
[00:45:50.160 --> 00:45:55.520]   either of those? So that's again, our research idea two over here. And the next thing is like
[00:45:55.520 --> 00:46:00.880]   the excitation operator. So we're talking about the non-linearity. So they look at ReLU, tanh,
[00:46:00.880 --> 00:46:06.560]   and they look at sigmoid. So they think which one's better, and then sigmoid is the best option.
[00:46:07.280 --> 00:46:12.160]   Again, there's different ways we can have a look at. Leaky value, or there's like more ways of
[00:46:12.160 --> 00:46:18.640]   trying things now that this is again research idea three, I would say, is like there's different
[00:46:18.640 --> 00:46:26.240]   excitations that we can try. And finally, the main thing is like, do, in this ablation study,
[00:46:26.240 --> 00:46:31.200]   they go, okay, we add the SE block in one stage at a time, and then you report the results in
[00:46:31.200 --> 00:46:39.040]   table 13. So you can see how, where's table 13? Here it is. So basically, you can see like,
[00:46:39.040 --> 00:46:45.920]   the best thing is to have the SE block or the squeeze and excitation block in all of the stages.
[00:46:45.920 --> 00:46:52.800]   But as you add through the SE blocks in each of the layers, you see like there's a decrease in the
[00:46:52.800 --> 00:46:58.880]   top one error, except between stage two and stage three, where the accuracy is very similar. So
[00:47:00.560 --> 00:47:05.120]   this is like just suggestion. This is just a way of research is saying that it's better to have the
[00:47:05.120 --> 00:47:10.480]   squeeze and excitation throughout the whole network. So those are the main things. Apart
[00:47:10.480 --> 00:47:16.560]   from that, you can see like in the example that I showed you, we had the SE block. We had the SE
[00:47:16.560 --> 00:47:20.240]   squeeze and excitation operation after the residual, but you could actually have the squeeze
[00:47:20.240 --> 00:47:24.400]   and excitation before the residual, or you could have the resnet, and then you could have the
[00:47:24.400 --> 00:47:28.480]   squeeze and excitation, or you could have like an SE identity block. So there's like different ways
[00:47:28.480 --> 00:47:32.960]   of having or adding squeeze and excitation to existing architectures. And then the question
[00:47:32.960 --> 00:47:37.600]   is like, which one of these is the best? So there's like another table that I think,
[00:47:37.600 --> 00:47:48.560]   table 14. So table 14 is here. So you can see that SE pre is the one that has the lowest error. So
[00:47:48.560 --> 00:47:55.360]   SE pre is the standard, this one, SE pre C. So that's this way of like doing things. So again,
[00:47:55.360 --> 00:47:59.520]   there's like lots of research ideas that you can try on your different data sets.
[00:47:59.520 --> 00:48:07.280]   And then finally, conclusion is like in this paper, we propose the SE block, and SE block
[00:48:07.280 --> 00:48:12.480]   sheds some light on the inability of the previous architectures to adequately model channel-wise
[00:48:12.480 --> 00:48:18.640]   dependencies. So I think that's the main bits of squeeze and excitation. Conceptually, it's really,
[00:48:18.640 --> 00:48:24.320]   really, very simple to understand. The excitation operation is pretty much just this on your right,
[00:48:24.320 --> 00:48:40.560]   and the squeeze excitation operation. So on the top is this squeeze operation, and on the bottom,
[00:48:40.560 --> 00:48:46.640]   or this is the excitation operation. So you can see how easy the squeeze and excitation operations
[00:48:46.640 --> 00:48:53.600]   are from a holistic view. And conceptually, this paper is really simple, but there's really good
[00:48:53.600 --> 00:48:59.040]   questions that were asked by researchers, and this is a really good paper. So if you want to,
[00:48:59.040 --> 00:49:04.160]   in terms of next steps, I would recommend like you take the understanding from this paper reading
[00:49:04.160 --> 00:49:11.040]   group, and then you go to this blog that I've written for everybody somewhere in 2020. And then
[00:49:11.040 --> 00:49:17.280]   you can see like how we can build all of these, and how we can build the SE-Besnet-CNA titles
[00:49:17.280 --> 00:49:22.720]   from code. So you can see all of these being built from code. And I think then once you have this,
[00:49:22.720 --> 00:49:27.680]   then your understanding of SE-Nets would be complete. But that's that.
[00:49:27.680 --> 00:49:32.640]   Cool. Thanks, everybody, for joining me.
[00:49:32.640 --> 00:49:39.840]   I'm just having a look at the paper.
[00:49:39.840 --> 00:49:52.320]   Oh, I'm not sure on like why this implementation, because I think that's an open question for my,
[00:49:52.320 --> 00:49:57.440]   like I had this question as well. I'm sorry, I don't really have an answer as to why this is
[00:49:57.440 --> 00:50:03.840]   not normally done. But let's go back to table 14, which is at the bottom. I would assume that
[00:50:03.840 --> 00:50:09.520]   the difference would be really small, that it can be neglected. So it might be just is easier.
[00:50:09.520 --> 00:50:22.000]   Yeah, the difference is like 6.03 and 6.00 or like 22.28, which is a 0.05 decimal places. So
[00:50:22.000 --> 00:50:29.360]   like it's not a 0.5 or not a very big difference. So I think in that case, SE standard would be a
[00:50:29.360 --> 00:50:33.600]   better computationally better way of doing things. And so it can be ignored. That's my
[00:50:33.600 --> 00:50:43.120]   understanding as an answer of this question. Cool. Everybody, thanks very much for joining me.
[00:50:43.120 --> 00:50:48.160]   I will see you in two weeks time when we discuss the next paper reading group, which is on
[00:50:50.240 --> 00:50:55.040]   EfficientNet. So we just done with StevesNet Citation. The next one is going to be EfficientNet
[00:50:55.040 --> 00:51:04.400]   on November 2. Cool. I'll see you guys then on November 2. Thanks for joining me. Bye.
[00:51:04.400 --> 00:51:06.680]   California Institute of Technology
[00:51:06.680 --> 00:51:08.680]   California Institute of Technology
[00:51:08.680 --> 00:51:10.680]   California Institute of Technology
[00:51:10.680 --> 00:51:12.680]   California Institute of Technology
[00:51:12.680 --> 00:51:14.680]   California Institute of Technology
[00:51:14.680 --> 00:51:16.680]   California Institute of Technology
[00:51:16.680 --> 00:51:18.680]   California Institute of Technology
[00:51:18.680 --> 00:51:20.680]   California Institute of Technology
[00:51:20.680 --> 00:51:22.680]   California Institute of Technology
[00:51:22.680 --> 00:51:24.680]   California Institute of Technology
[00:51:24.680 --> 00:51:26.680]   California Institute of Technology
[00:51:26.680 --> 00:51:28.680]   California Institute of Technology
[00:51:28.680 --> 00:51:30.680]   California Institute of Technology
[00:51:30.680 --> 00:51:32.680]   California Institute of Technology
[00:51:32.680 --> 00:51:34.680]   California Institute of Technology
[00:51:34.680 --> 00:51:36.680]   California Institute of Technology
[00:51:36.680 --> 00:51:46.680]   [BLANK_AUDIO]

