
[00:00:00.000 --> 00:00:04.480]   I just want to be clear here, I'm trying to do a docket. And we
[00:00:04.480 --> 00:00:09.880]   have to put the kibosh on this insanity of the soap opera that
[00:00:09.880 --> 00:00:13.240]   is becoming open AI sacks. Because every week, it's three,
[00:00:13.240 --> 00:00:16.960]   four or five stories. Have you seen what's happened this week?
[00:00:16.960 --> 00:00:18.120]   Yeah, of course.
[00:00:18.120 --> 00:00:20.320]   Got to catch the audience up here. Let's catch the audience
[00:00:20.320 --> 00:00:21.560]   up on what's happened here.
[00:00:21.560 --> 00:00:26.320]   This week, on General AI Hospital,
[00:00:27.480 --> 00:00:32.440]   is Sam Altman's job security in jeopardy? Whose data was stolen
[00:00:32.440 --> 00:00:37.800]   this time? What did Ilyas say? Why isn't he talking about it?
[00:00:37.800 --> 00:00:41.640]   And with our special guest, will our special guest get her
[00:00:41.640 --> 00:00:46.600]   revenge? General AI Hospital, brought to you by the drama
[00:00:46.600 --> 00:00:47.920]   queens at OpenAI.
[00:00:47.920 --> 00:00:49.320]   Wow.
[00:00:49.320 --> 00:00:54.200]   Who made that? That was great. Did you make that? Yeah, that
[00:00:54.200 --> 00:00:54.640]   was me.
[00:00:54.640 --> 00:00:55.280]   10 out of 10.
[00:00:55.280 --> 00:00:58.960]   That was my idea. But Nick's and Lon's execution. So shout
[00:00:58.960 --> 00:01:00.560]   out to Nick Calacanis.
[00:01:00.560 --> 00:01:02.680]   You finally landed the plane.
[00:01:02.680 --> 00:01:04.240]   Broken clock's right twice a day.
[00:01:04.240 --> 00:01:05.160]   Took four years.
[00:01:05.160 --> 00:01:09.680]   Broken clock is right twice a day. The Jason Calacanis story.
[00:01:09.680 --> 00:01:10.520]   That's my autobiography.
[00:01:10.520 --> 00:01:30.840]   But seriously, there was a big drama, Sax. I don't know if you
[00:01:30.840 --> 00:01:35.960]   saw this, but you couldn't miss it with Escargot. So they made
[00:01:35.960 --> 00:01:38.040]   an emergency, they had an emergency meeting, got all the
[00:01:38.040 --> 00:01:40.560]   developers together, and they've reset. They took Scarlett
[00:01:40.560 --> 00:01:43.520]   Johansson out, and they got a new person, I think, arguably
[00:01:43.520 --> 00:01:45.760]   better. I'm Friberg. I'm curious, your take on this, to
[00:01:45.760 --> 00:01:46.080]   better.
[00:01:46.080 --> 00:01:47.400]   Hey, JGPD, how's it going?
[00:01:47.400 --> 00:01:50.840]   Good. Good. Yeah, good. What's going on?
[00:01:50.840 --> 00:01:55.280]   I'm doing fine. I'm gonna be a father real soon. And I think I
[00:01:55.280 --> 00:01:59.280]   can have your help with some dad jokes. I'm going to tell you a
[00:01:59.280 --> 00:02:02.840]   joke. And you tell me if it passes as a dad joke.
[00:02:02.840 --> 00:02:04.520]   I've never done that before.
[00:02:04.880 --> 00:02:08.280]   All right. What do you call a giant pile of kittens?
[00:02:08.280 --> 00:02:09.640]   Give it to me.
[00:02:09.640 --> 00:02:11.680]   A meownton.
[00:02:11.680 --> 00:02:14.720]   No, it doesn't quite land.
[00:02:14.720 --> 00:02:18.240]   All right, there it is, folks. If you want, you can switch to
[00:02:18.240 --> 00:02:20.560]   Saxy Poo. So just go into OpenAI.
[00:02:20.560 --> 00:02:22.120]   You're saying they stole my voice now?
[00:02:22.120 --> 00:02:26.200]   Yeah, they stole yours. Go into OpenAI, go to Voices, and then
[00:02:26.200 --> 00:02:31.080]   just pick Saxy Poo. It's right there between Putin and Tucker.
[00:02:32.120 --> 00:02:36.520]   Saxy Poo Tucker. You can find all your favorite MAGA guests on
[00:02:36.520 --> 00:02:41.160]   the number one MAGA program. All in podcast. Here we go. All
[00:02:41.160 --> 00:02:43.120]   right, we're off to a strong start here. Everybody's in a
[00:02:43.120 --> 00:02:46.640]   good mood. Let's, let's keep the good times rolling here. And
[00:02:46.640 --> 00:02:51.040]   let's go over the ScarJo saga. To recap, if you're living under
[00:02:51.040 --> 00:02:55.200]   a rock this week, it came out that OpenAI, specifically Sam,
[00:02:55.200 --> 00:02:59.280]   had contacted Scarlett Johansson multiple times about lending her
[00:02:59.280 --> 00:03:02.840]   voice for one of OpenAI's chatbots. Obviously, you know,
[00:03:02.840 --> 00:03:06.440]   she famously was the voice, Samantha, in the awesome film
[00:03:06.440 --> 00:03:10.000]   Her. And according to ScarJo, Altman told her she could,
[00:03:10.000 --> 00:03:14.160]   quote, bridge the gap between tech companies and creatives and
[00:03:14.160 --> 00:03:17.440]   help consumers to feel comfortable with the seismic
[00:03:17.440 --> 00:03:21.960]   shift concerning humans and AI, and that her voice, quote, would
[00:03:21.960 --> 00:03:24.920]   be comforting to people. Although she declined the offer,
[00:03:24.920 --> 00:03:29.000]   OpenAI released a chatbot named Sky, which had a similar voice
[00:03:29.000 --> 00:03:31.560]   to ScarJo's. According to ScarJo, her friends and family
[00:03:31.560 --> 00:03:34.720]   thought the voice was her. She released a statement, yada yada.
[00:03:34.720 --> 00:03:39.080]   On May 13, the day OpenAI launched chat GPT 4.0 Omni,
[00:03:39.080 --> 00:03:43.040]   which we talked about last week, Altman tweeted Her, a reference
[00:03:43.040 --> 00:03:46.760]   to the film. Obviously, now ScarJo is threatening legal
[00:03:46.760 --> 00:03:49.040]   action against OpenAI. Altman put out a statement
[00:03:49.040 --> 00:03:52.240]   apologizing, and saying the voice was never intended to
[00:03:52.240 --> 00:03:57.720]   resemble hers. His quote, "We're sorry to Ms. Johansson that we
[00:03:57.720 --> 00:04:00.680]   didn't communicate better." OpenAI showed documents to the
[00:04:00.680 --> 00:04:02.640]   Washington Post that confirmed the voice was provided by a
[00:04:02.640 --> 00:04:05.520]   different actress who is anonymous. Post reporters also
[00:04:05.520 --> 00:04:10.240]   spoke to the unnamed actress's agent who confirmed the story. I
[00:04:10.240 --> 00:04:13.400]   guess, Sax, you sent us a comparison clip. Maybe we start
[00:04:13.400 --> 00:04:14.920]   there and see what we think.
[00:04:14.920 --> 00:04:16.880]   Yeah. Do you guys think they sound the same?
[00:04:16.880 --> 00:04:21.200]   I can understand and generate human-like text pretty well. It
[00:04:21.200 --> 00:04:23.760]   really depends on what you're looking for in an assistant.
[00:04:24.120 --> 00:04:25.840]   What specific tasks?
[00:04:25.840 --> 00:04:32.080]   When I was like, oh, sexuality. Like, I was like, open my eyes
[00:04:32.080 --> 00:04:34.400]   to like, some other thing.
[00:04:34.400 --> 00:04:35.120]   Samantha.
[00:04:35.120 --> 00:04:37.680]   Lily, where'd you get that name from?
[00:04:37.680 --> 00:04:39.600]   I gave it to myself, actually.
[00:04:39.600 --> 00:04:40.360]   What do you think?
[00:04:40.360 --> 00:04:42.520]   I think it sounds pretty darn similar.
[00:04:42.520 --> 00:04:43.560]   Dead on.
[00:04:43.560 --> 00:04:47.560]   I mean, I don't know if it's dead on. Honestly, it sounds
[00:04:47.560 --> 00:04:51.160]   like a digitally altered version of her voice. That's what it
[00:04:51.160 --> 00:04:51.760]   sounds like to me.
[00:04:51.760 --> 00:04:54.120]   They didn't get it perfect, right? Like, they got it to,
[00:04:54.120 --> 00:04:55.160]   what, 90%?
[00:04:55.160 --> 00:05:01.960]   It sounds like it was her voice, but then they changed it. So
[00:05:01.960 --> 00:05:05.640]   either that, or they hired a voice actor who sounds like her.
[00:05:05.640 --> 00:05:08.680]   And that's what the company has said, is that they hired a voice
[00:05:08.680 --> 00:05:11.600]   actor. But they won't tell us who the voice actor is, they
[00:05:11.600 --> 00:05:13.960]   said, because of privacy concerns. Which doesn't quite
[00:05:13.960 --> 00:05:18.320]   make sense to me, because when you hire an actor, they want the
[00:05:18.320 --> 00:05:19.320]   credit, you know.
[00:05:19.320 --> 00:05:21.720]   Yeah. Get more work.
[00:05:21.720 --> 00:05:24.240]   The company could just clear this whole thing up by saying
[00:05:24.240 --> 00:05:27.560]   exactly who the voice actor is. And why wouldn't the voice actor
[00:05:27.560 --> 00:05:29.880]   want that? You know,
[00:05:29.880 --> 00:05:32.120]   Because she doesn't exist.
[00:05:32.120 --> 00:05:36.960]   She was a general open AI.
[00:05:36.960 --> 00:05:41.120]   Wait, they made this actress up?
[00:05:41.120 --> 00:05:45.120]   I mean, she doesn't exist. Of course, it's a digitally
[00:05:45.120 --> 00:05:49.080]   altered version of ScarJo. And they got caught. Okay, cookie
[00:05:49.080 --> 00:05:49.640]   jar.
[00:05:50.200 --> 00:05:54.960]   This is why Sam's call to Scarlett's agents or her two
[00:05:54.960 --> 00:05:58.440]   days before they launched is such a damning piece of
[00:05:58.440 --> 00:06:00.600]   evidence, because it sounds like he's trying to shut the barn
[00:06:00.600 --> 00:06:03.920]   door on something they've already done, right? They've got
[00:06:03.920 --> 00:06:07.600]   this demo ready. They're gonna launch it in two days. They're
[00:06:07.600 --> 00:06:11.360]   realizing maybe they don't have the rights to use her voice. So
[00:06:11.360 --> 00:06:15.840]   you have to contact her to get those rights. But anyone who
[00:06:15.840 --> 00:06:17.480]   knows anything about Hollywood knows you're not gonna be able
[00:06:17.480 --> 00:06:21.000]   to make a deal with a major star to use her name and likeness in
[00:06:21.000 --> 00:06:24.760]   two days. It's impossible. So this seems like a really crazy
[00:06:24.760 --> 00:06:28.720]   thing to do. I mean, I think the mere fact that he contacted them
[00:06:28.720 --> 00:06:32.560]   and then tweeted out her, which shows that Scarlett was on the
[00:06:32.560 --> 00:06:36.160]   brain. Those are really damning pieces of evidence, I think in
[00:06:36.160 --> 00:06:39.000]   this lawsuit. And I think it's going to feed into our case.
[00:06:39.000 --> 00:06:41.880]   Look, here's the thing, this company is going to go down in
[00:06:41.880 --> 00:06:46.600]   the history books. In one part, because the technical
[00:06:46.600 --> 00:06:49.840]   inventions that they've created are just next level, and
[00:06:49.840 --> 00:06:52.840]   frankly, created an entire industry. And I think they
[00:06:52.840 --> 00:06:55.560]   deserve a ton of credit for that. But they're also going to
[00:06:55.560 --> 00:06:58.640]   be written in the history books for two other things that are
[00:06:58.640 --> 00:07:05.400]   probably less aspirational. I think the first is that there's
[00:07:05.400 --> 00:07:10.280]   just all kinds of dustups and unnecessary drama that just
[00:07:10.280 --> 00:07:13.600]   seemed to kick around every few weeks or months. And then the
[00:07:13.600 --> 00:07:18.440]   second is the sheer quantum of value capture that the employees
[00:07:18.440 --> 00:07:22.840]   have seen through secondaries before a fully functional
[00:07:22.840 --> 00:07:26.240]   business has been really created. And so I think it
[00:07:26.240 --> 00:07:29.560]   explains why folks circle the wagons consistently. It's a very
[00:07:29.560 --> 00:07:31.880]   I think it's a very rational organization. They're
[00:07:31.880 --> 00:07:35.120]   technically ahead of everybody else. A lot of people want to
[00:07:35.120 --> 00:07:39.920]   put in money at, you know, crazy prices. A bunch of that value is
[00:07:39.920 --> 00:07:43.640]   transferred immediately to the employees who circle the wagons
[00:07:43.640 --> 00:07:47.840]   and do what's necessary to keep the taps flowing. And I think
[00:07:47.840 --> 00:07:49.840]   that that explains the whole thing. And I think that that
[00:07:49.840 --> 00:07:52.720]   explains many Silicon Valley companies, quite honestly,
[00:07:52.720 --> 00:07:56.320]   circle the wagons, defend the company, sell your shares and
[00:07:56.320 --> 00:07:59.720]   secondary at 90 billion to thrive or whoever. What would
[00:07:59.720 --> 00:08:03.160]   your take freeberg on all of this craziness and drama?
[00:08:03.160 --> 00:08:08.400]   I think what we will see over time is that rather than have
[00:08:08.400 --> 00:08:15.400]   the ability to sue for their likeness, a lot of AI that is
[00:08:15.400 --> 00:08:19.800]   like a celebrity, the value of it will arise from the
[00:08:19.800 --> 00:08:23.280]   celebrity's endorsement, not actually using the celebrity's
[00:08:23.280 --> 00:08:27.040]   features. So without the endorsement, I know everyone
[00:08:27.040 --> 00:08:29.840]   kind of wants to point to this idea of likeness. But I think
[00:08:29.840 --> 00:08:32.440]   that there's something about the authentic aspect of having the
[00:08:32.440 --> 00:08:36.800]   celebrity actually endorse and provide their, their signature,
[00:08:36.800 --> 00:08:39.120]   their stamp on it. Restaurants are a good example. Some
[00:08:39.120 --> 00:08:42.000]   celebrity chef says I was involved in making this menu.
[00:08:42.000 --> 00:08:44.640]   That's a lot different than mimicking the celebrity chef's
[00:08:44.640 --> 00:08:47.520]   menu from his restaurant, putting his or her name and
[00:08:47.520 --> 00:08:50.240]   brand on it. There's a bunch of videos on YouTube. Now. I don't
[00:08:50.240 --> 00:08:52.480]   know if you guys ever guess probably don't watch these. But
[00:08:52.480 --> 00:08:54.760]   I love watching these videos where like music producers make
[00:08:54.760 --> 00:08:58.400]   tracks and how they do it. And so many of these producers now
[00:08:58.400 --> 00:09:02.720]   are using AI tools, taking samples off of old records or
[00:09:02.720 --> 00:09:05.480]   other tracks, and then telling the AI make something that
[00:09:05.480 --> 00:09:08.520]   sounds like this, or looks like this, but isn't like this. And
[00:09:08.520 --> 00:09:11.560]   so there's enough of a transformation happening, that
[00:09:11.560 --> 00:09:14.240]   it isn't a direct likeness. And then they're able to create
[00:09:14.240 --> 00:09:16.960]   entire vocal tracks without needing a singer, or without
[00:09:16.960 --> 00:09:20.520]   needing the celebrity singer. So you're in the much ado about
[00:09:20.520 --> 00:09:23.280]   nothing on the sky. Yeah, I'm in I'm in the like, like the
[00:09:23.280 --> 00:09:27.320]   content itself, I think is probably less like, compelling.
[00:09:27.320 --> 00:09:30.360]   Oh, you go go after her because the voice sounds the same. But I
[00:09:30.360 --> 00:09:33.040]   do think that there's this element of like, what if you
[00:09:33.040 --> 00:09:36.280]   could then say, hey, you know, Britney Spears actually lent her
[00:09:36.280 --> 00:09:39.880]   voice to this track. So do you think if she if she sues open AI
[00:09:39.880 --> 00:09:41.920]   Do you think it should just get thrown out? It's not a real
[00:09:41.920 --> 00:09:44.240]   case. No, I think there's probably going to be a lot of
[00:09:44.240 --> 00:09:46.880]   discovery to sexist point that's going to show that they probably
[00:09:46.880 --> 00:09:49.520]   did. Yeah, that discovery is gonna be juicy. Yeah, yeah. No,
[00:09:49.520 --> 00:09:51.880]   no, no, I'm not. I'm not saying that. I'm saying I'm asking you
[00:09:51.880 --> 00:09:54.240]   more that. Yeah, even if they find it, your point is, it
[00:09:54.240 --> 00:09:57.640]   shouldn't matter. My point is, it should be thrown out. Well, I
[00:09:57.640 --> 00:09:59.520]   don't know. I mean, hold on. Let's say you're casting a
[00:09:59.520 --> 00:10:02.320]   movie. And you know, you can't get Scarlett Johansson for it.
[00:10:02.360 --> 00:10:04.120]   And so you tell the casting director, get me a Scarlett
[00:10:04.120 --> 00:10:07.320]   Johansson type, I think you can do that. Okay, obviously, you
[00:10:07.320 --> 00:10:09.960]   can't, you can't use her name, you can't use her likeness, but
[00:10:09.960 --> 00:10:13.160]   you could hire a different actor, who might look or sound
[00:10:13.160 --> 00:10:17.040]   like Scarlett Johansson. If the company actually did that, and
[00:10:17.040 --> 00:10:19.480]   they did it nine months ago. This is what the statement they
[00:10:19.480 --> 00:10:22.920]   put out. I think they've got a decent defense. Yeah. But
[00:10:22.920 --> 00:10:27.280]   because they may, I don't know if we believe that. I mean,
[00:10:27.280 --> 00:10:30.360]   again, why? Why don't you just put out the name of the actual
[00:10:30.400 --> 00:10:32.360]   actor, that voice actor that you used?
[00:10:32.360 --> 00:10:35.480]   And there's a very simple test here. If the person if there's
[00:10:35.480 --> 00:10:38.880]   confusion amongst the public, which is what scar Joe put in
[00:10:38.880 --> 00:10:43.000]   her letter, and that was a legally written, deftly written
[00:10:43.000 --> 00:10:46.240]   letter to set up a huge settlement, because she said the
[00:10:46.240 --> 00:10:49.280]   morning this came out, all of my friends said, Oh, my God,
[00:10:49.280 --> 00:10:53.600]   congratulations on your chat GPT Odell. This is great. The
[00:10:53.600 --> 00:10:56.440]   public being confused is the key issue here. And there's something
[00:10:56.440 --> 00:10:58.880]   called the right to publicity. This is basically how
[00:10:58.880 --> 00:11:01.640]   celebrities defend their rights. It's happening all the time to
[00:11:01.640 --> 00:11:04.120]   podcasters, by the way, there was a company that put me in
[00:11:04.120 --> 00:11:07.040]   their heads based on something I said in a show. And then they
[00:11:07.040 --> 00:11:10.160]   put ads against it. And Huberman has been having this happen.
[00:11:10.160 --> 00:11:12.680]   Joe, it's happening. It's happening to me right now. I'm
[00:11:12.680 --> 00:11:15.800]   in the middle of this crazy thing with Facebook, and they've
[00:11:15.800 --> 00:11:18.040]   been doing a very good job. The team and Metta Ashley, thank
[00:11:18.040 --> 00:11:21.360]   you. But hundreds and hundreds and hundreds of automated
[00:11:21.360 --> 00:11:24.880]   accounts, pretending to be me selling all kinds of random
[00:11:24.880 --> 00:11:27.800]   stuff. It's predominantly on WhatsApp and Facebook and Metta.
[00:11:28.320 --> 00:11:32.040]   And I don't know what to do, because I, we work together with
[00:11:32.040 --> 00:11:35.240]   them, we shut it down. I've actually had to reactivate my
[00:11:35.240 --> 00:11:39.120]   Facebook and Instagram accounts, which were dormant, so that we
[00:11:39.120 --> 00:11:41.760]   could actually have them be verified, so that then it's
[00:11:41.760 --> 00:11:44.760]   easier to shut them down. But it is an impossible task when
[00:11:44.760 --> 00:11:49.040]   somebody is impersonating you. Yeah. To fight it. At least in my
[00:11:49.040 --> 00:11:51.240]   experience, it's been a month and we it's just like every
[00:11:51.240 --> 00:11:53.160]   hundred we take down, it's another 1000.
[00:11:53.160 --> 00:11:57.520]   Both of those cases, you guys have your exact image of you
[00:11:57.560 --> 00:12:00.280]   being shown to sell to sell stuff. I think that in this
[00:12:00.280 --> 00:12:04.160]   case, it's also unique to scar Joe, because she was the voice
[00:12:04.160 --> 00:12:07.240]   from her. There is no like if this were like a voice sort of
[00:12:07.240 --> 00:12:10.160]   like Cameron Diaz, or sort of like Julia Roberts, it wouldn't
[00:12:10.160 --> 00:12:12.600]   be as big a deal, because it would certainly got some
[00:12:12.600 --> 00:12:15.120]   differences to it. But it's because they're trying to mimic
[00:12:15.120 --> 00:12:18.040]   the movie her well, didn't didn't matter use Morgan Freeman
[00:12:18.040 --> 00:12:21.360]   when like Zuck created like an AI or something as a project. You
[00:12:21.360 --> 00:12:23.800]   guys remember that wasn't Morgan Freeman the voice they paid for
[00:12:23.800 --> 00:12:27.480]   it. And there's a company speechify. I'm not an investor
[00:12:27.480 --> 00:12:29.520]   or anything like that. But they have Gwyneth Paltrow as a
[00:12:29.520 --> 00:12:32.160]   licensed voice to read yourself. So
[00:12:32.160 --> 00:12:34.840]   they should just license Gwyneth's voice.
[00:12:34.840 --> 00:12:39.800]   Obviously, obvious, like just whoever wants to get paid. Here's
[00:12:39.800 --> 00:12:42.680]   the opportunity if three people say no, the fact that they're
[00:12:42.680 --> 00:12:45.320]   not trying to say this is this person's voice, but they want to
[00:12:45.320 --> 00:12:48.600]   say like, hey, let's say you just can prompt the AI and say
[00:12:48.600 --> 00:12:52.520]   generate a voice that is sort of like movies that are comfortable
[00:12:52.520 --> 00:12:56.160]   and calming to people to listen to. And you know that that we've
[00:12:56.200 --> 00:12:58.320]   that that we think people will be comfortable and the AI
[00:12:58.320 --> 00:13:00.840]   generate something that sounds like I've said this before, but
[00:13:00.840 --> 00:13:03.760]   it's not deliberately trained on scar using the computer to
[00:13:03.760 --> 00:13:07.320]   probabilistically copy something is still copying something. Yes.
[00:13:07.320 --> 00:13:09.040]   Come on, guys. Let's not make this too complicated.
[00:13:09.040 --> 00:13:11.600]   Is the public confused is the only test you need for what if
[00:13:11.600 --> 00:13:15.240]   there's two public actresses that both have similar voices,
[00:13:15.240 --> 00:13:17.440]   and then they both claim, hey, you tried to make this sound
[00:13:17.440 --> 00:13:19.080]   like me. What do you do in that case tomorrow?
[00:13:19.080 --> 00:13:21.480]   Well, which one did you call two days before the demo?
[00:13:21.480 --> 00:13:25.160]   Yeah, exactly. And did the CEO tweet? I think your whole point
[00:13:25.160 --> 00:13:27.280]   about discovery is what is going to get them in trouble in this
[00:13:27.280 --> 00:13:30.320]   case, because they were clearly they were clearly trying to do
[00:13:30.320 --> 00:13:31.800]   an impersonation of her, right?
[00:13:31.800 --> 00:13:36.160]   Yeah, if they never reached out to Scarlett, they claim it's
[00:13:36.160 --> 00:13:39.560]   just a coincidence. But they called her twice. They called
[00:13:39.560 --> 00:13:42.960]   her some months before and then two days before, which indicates
[00:13:42.960 --> 00:13:43.440]   panic.
[00:13:43.440 --> 00:13:46.800]   Judge sacks. Give your verdict. I'm starting a new show. Judge
[00:13:46.800 --> 00:13:47.600]   sacks says
[00:13:47.600 --> 00:13:49.360]   guilty.
[00:13:49.360 --> 00:13:53.960]   Sentence now. What's the sentence?
[00:13:54.920 --> 00:13:57.360]   The sentence is that Scarlett Johansson is going to end up
[00:13:57.360 --> 00:13:59.240]   owning more of this company than Sam Altman.
[00:13:59.240 --> 00:14:08.640]   They call her two days before. Why do you do that guilty?
[00:14:08.640 --> 00:14:11.240]   Because you know, you have a problem. And you're trying to
[00:14:11.240 --> 00:14:14.720]   put the horse back in the barn. They should have done is as soon
[00:14:14.720 --> 00:14:17.920]   as she says no, you just change the voice completely.
[00:14:17.920 --> 00:14:20.600]   Or you get a fixer, get Michael Cohen in there and get a
[00:14:20.600 --> 00:14:23.960]   settlement going. Get set. Let's get some fixer in there to fix
[00:14:23.960 --> 00:14:28.200]   it. Okay, listen enough with open AI. Oh, wait, there's more.
[00:14:28.200 --> 00:14:33.440]   Geez is ruining the docket. I guess we have to talk about the
[00:14:33.440 --> 00:14:40.040]   next drama from a open AI this week. Former employees sign an
[00:14:40.040 --> 00:14:42.760]   agreement that they're forbidden forever from criticizing the
[00:14:42.760 --> 00:14:45.960]   company or even acknowledging that the NDA exists. If a
[00:14:45.960 --> 00:14:48.880]   departing employee declines to sign the document, or if they
[00:14:48.880 --> 00:14:52.520]   violate it, they can lose all vested equity they earn during
[00:14:52.580 --> 00:14:55.720]   their time at the company in practice. This means ex
[00:14:55.720 --> 00:14:57.960]   employees have to choose between giving up millions of dollars
[00:14:57.960 --> 00:15:00.160]   they've already earned or agreeing not to criticize the
[00:15:00.160 --> 00:15:02.280]   company in perpetuity.
[00:15:02.280 --> 00:15:05.320]   Sacks is a lot of details here.
[00:15:05.320 --> 00:15:08.840]   Yeah, well, you can see why they wrote that in there. That makes
[00:15:08.840 --> 00:15:11.560]   sense. If you think hold on a second, if an employee can leave
[00:15:11.560 --> 00:15:16.800]   with, you know, a random copy of like some old weights as a
[00:15:16.800 --> 00:15:19.560]   starting point to rebuild the model that could be very
[00:15:19.560 --> 00:15:23.740]   valuable, or there's, yeah, but then well, there's all kinds of
[00:15:23.740 --> 00:15:27.940]   like, kind of quasi confidential information or knowledge or
[00:15:27.940 --> 00:15:32.620]   know how that you leave a place like that with so I could see
[00:15:32.620 --> 00:15:39.300]   why there was a justification to be very heavy handed about it.
[00:15:39.300 --> 00:15:41.740]   But again, the question isn't whether you're allowed to be
[00:15:41.740 --> 00:15:45.220]   heavy handed, you are. The question is why backtrack and
[00:15:45.220 --> 00:15:47.780]   then obfuscate and lie after you get caught.
[00:15:47.940 --> 00:15:51.600]   Well, right. Well, they claim it's an accident. Once they get
[00:15:51.600 --> 00:15:54.280]   caught. Yeah, with their hand in the cookie jar, which like you
[00:15:54.280 --> 00:15:56.340]   said, your mouth is just a heavy handed agreement. It's in the
[00:15:56.340 --> 00:15:59.240]   company's interest to do this. Yeah, they just say it was an
[00:15:59.240 --> 00:16:01.320]   accident, just like the Scarlet thing is an accident or a
[00:16:01.320 --> 00:16:03.520]   coincidence. It's just getting hard to believe.
[00:16:03.520 --> 00:16:05.800]   Just own it and say, you know what, guys, it's a really
[00:16:05.800 --> 00:16:08.760]   valuable company. There's a ton of very valuable trade secret
[00:16:08.760 --> 00:16:11.520]   know how IP confidential information. And we're going to
[00:16:11.520 --> 00:16:16.640]   be extremely litigious on the offense and protect it. Because
[00:16:17.320 --> 00:16:19.180]   and it's correlated to the importance of the company and
[00:16:19.180 --> 00:16:21.580]   the ecosystem. You could have said that and people could have
[00:16:21.580 --> 00:16:23.100]   been upset, but they would have understood.
[00:16:23.100 --> 00:16:26.080]   Here's what Sam Altman said. There was a provision about
[00:16:26.080 --> 00:16:29.380]   potential equity cancellation in our previous exit docs.
[00:16:29.380 --> 00:16:32.820]   Although we never clawed anything back, it should never
[00:16:32.820 --> 00:16:36.660]   have been something we had in any documents or communication.
[00:16:36.660 --> 00:16:41.900]   This is on me. And one of the few times I've genuinely been
[00:16:41.900 --> 00:16:46.420]   embarrassed running open. I did not know this was happening. And
[00:16:46.420 --> 00:16:48.760]   I should have if any former employee who signed one of these
[00:16:48.760 --> 00:16:51.280]   old agreements is worried about it, they can contact me and
[00:16:51.280 --> 00:16:54.640]   we'll fix that to very sorry about this. So he's very, very
[00:16:54.640 --> 00:16:57.100]   sorry. Sorry, it's starting to be like BP oil, this company,
[00:16:57.100 --> 00:16:59.480]   like, they're just so sorry about everything.
[00:16:59.480 --> 00:17:03.520]   Here's the question is, these are form documents at the end of
[00:17:03.520 --> 00:17:07.680]   the day. And form documents don't write themselves. lawyers
[00:17:07.680 --> 00:17:10.480]   write them. And when you get a novel change in one of these
[00:17:10.480 --> 00:17:12.640]   documents, somebody thought that through and thought it'd be a
[00:17:12.640 --> 00:17:16.660]   good idea and put it in there. Right. And like Jamal said,
[00:17:16.660 --> 00:17:19.420]   there is a way to potentially defend that. It's not like these
[00:17:19.420 --> 00:17:22.660]   provisions don't exist. It's just a novel application to try
[00:17:22.660 --> 00:17:25.660]   and claw back someone's already vested equity from a company.
[00:17:25.660 --> 00:17:28.900]   Well, that is not half of these things. Yeah, that is not
[00:17:28.900 --> 00:17:31.700]   exactly standard. Sometimes that's completely nonstandard.
[00:17:31.700 --> 00:17:33.460]   I'm just saying that these provisions exist in other
[00:17:33.460 --> 00:17:38.220]   contexts. And their application as a as a clawback of vested
[00:17:38.220 --> 00:17:40.960]   employee equity is something that I don't think any of us
[00:17:40.960 --> 00:17:44.820]   have heard before. So haven't Yeah, yeah, exactly. So my point
[00:17:44.820 --> 00:17:47.920]   is just it doesn't happen as an accident. Somebody made a
[00:17:47.920 --> 00:17:50.500]   strategic business decision to do this because they thought it
[00:17:50.500 --> 00:17:51.620]   be in the company's interest.
[00:17:51.620 --> 00:17:55.420]   Yeah. So just in layman's terms, the clawback means you had 10
[00:17:55.420 --> 00:18:00.820]   million in equity. You earn 75% of you got 7.5 million in equity
[00:18:00.820 --> 00:18:03.820]   there. You say something disparaging about the company.
[00:18:03.820 --> 00:18:07.100]   And they can take it back from you.
[00:18:07.140 --> 00:18:11.640]   Once you leave a company, and you vested equity, you don't
[00:18:11.640 --> 00:18:14.360]   lose it. I mean, as long as you you have some period in which to
[00:18:14.360 --> 00:18:18.000]   exercise your option if it's an option rather than stock. But
[00:18:18.000 --> 00:18:20.880]   other than that, I've never heard of a situation where
[00:18:20.880 --> 00:18:22.720]   employees can lose their vested equity.
[00:18:22.720 --> 00:18:26.240]   Even in a situation, sex, we've seen this where somebody commits
[00:18:26.240 --> 00:18:30.040]   fraud, they still get their vested equity, and then it's up
[00:18:30.040 --> 00:18:32.680]   to the company to sue them for fraud separately, right? Like
[00:18:32.680 --> 00:18:35.360]   we've seen, I guess. Yeah, I mean, I guess that's right.
[00:18:35.580 --> 00:18:37.200]   Like committing a crime.
[00:18:37.200 --> 00:18:41.600]   Look, I think the question here is, is it credible that they
[00:18:41.600 --> 00:18:44.480]   keep having these accidents and coincidences?
[00:18:44.480 --> 00:18:46.120]   What does Judge Sachs say?
[00:18:46.120 --> 00:18:53.160]   Judge Sachs says there's one too many coincidences. Look, I think
[00:18:53.160 --> 00:18:55.840]   like Tomas said, you could have just owned this and said, yeah,
[00:18:55.840 --> 00:18:58.440]   that this is a defendant in the way he said and said, but you
[00:18:58.440 --> 00:19:00.060]   know what, it was too aggressive. And we pulled it
[00:19:00.060 --> 00:19:00.280]   back.
[00:19:00.280 --> 00:19:03.180]   I think the thing is, look, if you if you think about the
[00:19:03.180 --> 00:19:09.800]   pendulum of culture in Silicon Valley, we used to have a very
[00:19:09.800 --> 00:19:13.320]   tough culture of founder led businesses, where there was
[00:19:13.320 --> 00:19:18.540]   extremely high expectations. And if you transgressed, it was very
[00:19:18.540 --> 00:19:22.520]   punitive. And then the pendulum swung in the all the way to the
[00:19:22.520 --> 00:19:26.200]   other opposite end where you had this like, coddling daycare type
[00:19:26.200 --> 00:19:29.600]   approach that existed for like the last 15 or 20 years. And
[00:19:29.640 --> 00:19:33.700]   probably what open AI is, is an example of a company that needs
[00:19:33.700 --> 00:19:37.700]   to be run a little bit more like the former, but stuck with a
[00:19:37.700 --> 00:19:41.420]   bunch of people that still pull it towards to be the latter. And
[00:19:41.420 --> 00:19:43.700]   that's the cultural tension that they're going to have to sort
[00:19:43.700 --> 00:19:48.300]   out. Because in order to be this incredible bastion of like AGI
[00:19:48.300 --> 00:19:52.180]   and innovation, I suspect that it's going to look more like a
[00:19:52.180 --> 00:19:55.340]   three letter agency in terms of security and protocols in the
[00:19:55.340 --> 00:19:58.220]   next five or 10 years than it is going to look like the Google
[00:19:58.220 --> 00:20:00.980]   Plex. And I think they just need to own that. And this is
[00:20:00.980 --> 00:20:04.460]   probably a little bit of an insight into that tension. And
[00:20:04.460 --> 00:20:06.740]   they're gonna have to go in more in that direction. It's a good
[00:20:06.740 --> 00:20:09.060]   insight, you're not going to be allowed to build these
[00:20:09.060 --> 00:20:12.580]   incredibly crazy world beating technologies where people are
[00:20:12.580 --> 00:20:15.540]   running around in an eight seater bicycle. She's not going
[00:20:15.540 --> 00:20:18.740]   to and by the way, I mean, because it is such an industry
[00:20:18.740 --> 00:20:22.020]   leading company, I think we could end up with some very bad
[00:20:22.020 --> 00:20:26.760]   fair use precedents or laws. Because Scarlett Johansson is so
[00:20:26.760 --> 00:20:32.780]   sympathetic, as a plaintiff, compared to open AI. And it's,
[00:20:32.780 --> 00:20:37.380]   you know, unless they show us some discovery, that proves that
[00:20:37.380 --> 00:20:39.360]   they really did hire the voice act and all the rest of it. I
[00:20:39.360 --> 00:20:42.240]   mean, this could lead to some very bad precedents for the
[00:20:42.240 --> 00:20:43.820]   industry around fair use.
[00:20:43.820 --> 00:20:46.740]   Well, and here we go, I think the Microsoft will pay the
[00:20:46.740 --> 00:20:49.620]   speeding ticket. And we'll just all move on. But Freeberg, my
[00:20:49.620 --> 00:20:54.620]   God, can you imagine like being two or three PhDs and, you know,
[00:20:55.060 --> 00:20:57.600]   machine learning or whatever you study your whole life, you're
[00:20:57.600 --> 00:21:00.280]   pursuing general AI, and like, people are coming up to your
[00:21:00.280 --> 00:21:03.480]   desk and creating all this drama and nonsense. And you're in the
[00:21:03.480 --> 00:21:05.800]   middle of a soap opera while you're trying to create the
[00:21:05.800 --> 00:21:08.860]   technology that creates super intelligence. It's nuts.
[00:21:08.860 --> 00:21:11.880]   Yeah, I find it annoying to just listening to it.
[00:21:11.880 --> 00:21:16.280]   Okay, well, then in that case, we will move on from open AI up.
[00:21:16.280 --> 00:21:19.800]   There's one more drama going on.
[00:21:19.800 --> 00:21:23.140]   It's not just drama. I mean, it's, there's now a lawsuit, I
[00:21:23.140 --> 00:21:25.560]   think I think it's a very interesting case. The fair use
[00:21:25.560 --> 00:21:28.000]   case is interesting. I think it's a legitimately interesting
[00:21:28.000 --> 00:21:31.240]   case that's gonna if it goes all the way, if it goes the
[00:21:31.240 --> 00:21:33.640]   distance, it's going to create some really interesting
[00:21:33.640 --> 00:21:34.360]   precedents.
[00:21:34.360 --> 00:21:38.840]   Well, I mean, you but yeah, these, what happens in these
[00:21:38.840 --> 00:21:42.280]   content cases is they get settled almost every single
[00:21:42.280 --> 00:21:45.280]   time. So the case law doesn't get codified, they just get
[00:21:45.280 --> 00:21:48.200]   settled out of court, you go look at all the fair use cases,
[00:21:48.200 --> 00:21:51.820]   they almost never go to the mat. And so this one will just be
[00:21:51.820 --> 00:21:53.620]   settled. It'll just be a question.
[00:21:53.620 --> 00:21:57.720]   The interesting part of the other story is that the reason
[00:21:57.720 --> 00:22:00.360]   all this stuff came out about the equity clawbacks was because
[00:22:00.360 --> 00:22:03.120]   the safety team quit and it got leaked during that process.
[00:22:03.120 --> 00:22:05.600]   Alright, so this is the third dramatic story of the week that
[00:22:05.600 --> 00:22:09.320]   one I think that one I think begs a little bit more of a so
[00:22:09.320 --> 00:22:11.200]   let me ask you enough. Yeah.
[00:22:11.200 --> 00:22:15.960]   Two heads of open eyes super alignment team left the company
[00:22:15.960 --> 00:22:22.260]   last week, the day after GPT for a launch, Ilya announced he was
[00:22:22.260 --> 00:22:25.220]   leaving the company. He was our chief scientist a few hours
[00:22:25.220 --> 00:22:29.820]   later, his partner on the alignment team, john like also
[00:22:29.820 --> 00:22:33.260]   announced he was resigning in a later thread, like explained
[00:22:33.260 --> 00:22:37.700]   that he left due to, quote, safety culture and processes
[00:22:37.700 --> 00:22:42.360]   have taken a backseat to shiny products. Okay, there's a little
[00:22:42.360 --> 00:22:45.860]   bit of disparagement, to our former point about non
[00:22:45.860 --> 00:22:51.300]   disparagements, non D and NDAs. So AP open AI lost both its heads
[00:22:51.300 --> 00:22:56.180]   of AI alignment. One day after it launched that new product. Is
[00:22:56.180 --> 00:22:59.340]   that a coincidence? It's interesting. If you don't know
[00:22:59.340 --> 00:23:03.600]   what super alignment is, it's basically making sure that the
[00:23:03.600 --> 00:23:06.740]   software doesn't go terminator. What are your thoughts on this?
[00:23:06.980 --> 00:23:07.480]   Friedberg?
[00:23:07.480 --> 00:23:13.960]   I don't know. I mean, it could be some bad bureaucracy, bad
[00:23:13.960 --> 00:23:17.120]   politicking, not being listened to. But I think the real
[00:23:17.120 --> 00:23:24.080]   interesting question is, who's going to ask these guys? What's
[00:23:24.080 --> 00:23:27.160]   really going on from a technology perspective? And what
[00:23:27.160 --> 00:23:29.840]   is that going to reveal? Because these guys clearly are on the
[00:23:29.840 --> 00:23:33.920]   frontier of model development, and the performance of models.
[00:23:33.920 --> 00:23:38.480]   And so my guess is, there are certain regulatory people who
[00:23:38.480 --> 00:23:40.400]   are going to have interest in the fact that this team just
[00:23:40.400 --> 00:23:42.600]   left, they're going to make a phone call, they're gonna ask
[00:23:42.600 --> 00:23:44.720]   this team to come in and have a conversation. And they're going
[00:23:44.720 --> 00:23:47.220]   to start to ask a lot of questions about what the state
[00:23:47.220 --> 00:23:50.400]   of technology is over there. And I suspect that some things are
[00:23:50.400 --> 00:23:51.280]   going to start to come out.
[00:23:51.280 --> 00:23:57.560]   Saks, it was reported that Ilya was on the side of the
[00:23:57.560 --> 00:24:03.680]   nonprofit, the slow down AI be cautious group when they fired
[00:24:03.680 --> 00:24:08.160]   Sam. So what's your take on what's going on here? With super
[00:24:08.160 --> 00:24:11.280]   alignment inside of OpenAI? Judge Saks?
[00:24:11.280 --> 00:24:15.040]   Let's let's call this what it is a mass resignation. And we don't
[00:24:15.040 --> 00:24:17.600]   really know why. I mean, apparently, they were promised
[00:24:17.600 --> 00:24:21.440]   something like 20% of the computing resources of OpenAI.
[00:24:21.440 --> 00:24:23.960]   And they didn't get that I definitely read that somewhere.
[00:24:23.960 --> 00:24:29.200]   And so that is part of it, I think, but we don't really know
[00:24:29.200 --> 00:24:34.320]   the whole story. And you know, when you look at this issue of
[00:24:34.320 --> 00:24:37.040]   the mass resignation, and then you look at the issue of the
[00:24:37.040 --> 00:24:41.200]   clawback of vested employee equity, you're like, well, wait
[00:24:41.200 --> 00:24:45.320]   a second, maybe they felt like they needed that clawback. In
[00:24:45.320 --> 00:24:48.440]   order to deter all these people who are leaving from spilling
[00:24:48.440 --> 00:24:52.320]   the beans about right, whatever was upsetting them. So I clearly
[00:24:52.320 --> 00:24:53.600]   upset them, right? Yeah,
[00:24:53.600 --> 00:24:55.800]   that's why people are saying there's this meme, what did
[00:24:55.840 --> 00:24:58.720]   you see? What did he say? And then like you said, the board
[00:24:58.720 --> 00:25:02.200]   did fire him. And the only explanation they provided was
[00:25:02.200 --> 00:25:05.640]   that he wasn't being candid, for which at the time we thought was
[00:25:05.640 --> 00:25:09.120]   an incredibly damning statement. And we thought we'd get some
[00:25:09.120 --> 00:25:12.320]   explanation of it. We never got any explanation whatsoever. You
[00:25:12.320 --> 00:25:15.400]   know, I thought that the board was being incompetent, because I
[00:25:15.400 --> 00:25:19.160]   thought that either they fired him overly hastily, or they had
[00:25:19.160 --> 00:25:22.600]   reason to fire him, but then they communicated poorly. And,
[00:25:22.720 --> 00:25:26.080]   you know, is you add all these things up, and it definitely
[00:25:26.080 --> 00:25:27.440]   seems like we don't smoke,
[00:25:27.440 --> 00:25:29.680]   Sam's a straight shooter, we should just have him on the pod
[00:25:29.680 --> 00:25:30.120]   to explain.
[00:25:30.120 --> 00:25:37.160]   clear everything up. Stop me if you heard this before Nvidia
[00:25:37.160 --> 00:25:40.200]   Nvidia just smashed all expectations while reporting
[00:25:40.200 --> 00:25:43.800]   record profits and revenue. The AI train continues on Wednesday,
[00:25:43.800 --> 00:25:48.400]   video reported earnings for the fiscal q1. Revenue was 26
[00:25:48.400 --> 00:25:52.800]   billion up 18% quarter of a quarter, 260% year over year,
[00:25:52.800 --> 00:25:56.800]   basically, they quadrupled year over year on billions of
[00:25:56.800 --> 00:26:00.280]   revenue. This chart is bonkers. We've never seen anything like
[00:26:00.280 --> 00:26:03.920]   this in the history of Silicon Valley or corporate America. This
[00:26:03.920 --> 00:26:08.440]   is if somebody like literally was mining coal and then found a
[00:26:08.440 --> 00:26:11.760]   diamond and gold mined underneath it. It's bonkers
[00:26:11.760 --> 00:26:14.800]   what's happened here. When you look at the revenue there, you
[00:26:14.800 --> 00:26:19.160]   know, sort of slow growth or moderate growth revenue that
[00:26:19.160 --> 00:26:22.520]   they experienced. That was all because Nvidia was primarily
[00:26:22.520 --> 00:26:26.480]   providing GPUs for people playing video games or mining
[00:26:26.480 --> 00:26:29.880]   crypto. And then what you see with this unbelievable six
[00:26:29.880 --> 00:26:34.440]   quarter run, and five six quarter run is companies like
[00:26:34.440 --> 00:26:40.160]   Microsoft, Google, Tesla, open AI, etc, buying just billions
[00:26:40.160 --> 00:26:45.320]   and billions of dollars worth of hardware. I'll end on this and
[00:26:45.320 --> 00:26:50.040]   Chamath and get your take on it. Here's 2019 top companies by
[00:26:50.040 --> 00:26:53.200]   market cap in the world, obviously, Microsoft, Apple,
[00:26:53.200 --> 00:26:57.320]   Amazon, Google, Berkshire, Facebook, Alibaba, Tencent, and
[00:26:57.320 --> 00:27:00.640]   then you get some of the, you know, incumbents, and legacy
[00:27:00.640 --> 00:27:05.160]   companies, J&J, Exxon, and JP Morgan Visa, way down on the
[00:27:05.160 --> 00:27:10.240]   list in 2019. Number 84 was Nvidia. Today, Nvidia is the
[00:27:10.240 --> 00:27:13.560]   third largest company by market cap behind Microsoft and Apple
[00:27:13.560 --> 00:27:19.240]   and ahead of Google, aka, Alphabet and Saudi Aramco.
[00:27:19.240 --> 00:27:24.120]   Chamath, what's your take on this? Will it continue? And how
[00:27:24.120 --> 00:27:28.520]   do you conceptualize this level of growth on such a big number?
[00:27:28.520 --> 00:27:33.440]   I mean, I think it's a really, really incredibly fun moment if
[00:27:33.440 --> 00:27:39.160]   you're involved in anything AI related, just because it shows
[00:27:39.160 --> 00:27:43.120]   the level of investment that Nvidia's customers are making
[00:27:43.120 --> 00:27:49.800]   into making this new reality available for everybody, right?
[00:27:49.800 --> 00:27:53.200]   So when you're spending effectively $100 billion a year
[00:27:53.200 --> 00:27:57.000]   on the capex of chips, and then a couple hundred billion more on
[00:27:57.000 --> 00:27:59.800]   all the related infrastructure, and then another couple hundred
[00:27:59.800 --> 00:28:04.520]   billion more on power, you're talking about half a trillion to
[00:28:04.520 --> 00:28:08.120]   three quarters of a trillion dollars a year being spent to
[00:28:08.120 --> 00:28:11.080]   bring AI forward to the masses. So I think that's the really
[00:28:11.080 --> 00:28:15.640]   positive take. The other exciting thing is, if you're on
[00:28:15.640 --> 00:28:18.880]   the other side of the Nvidia trade, which is you're working
[00:28:18.880 --> 00:28:24.040]   on something that does what they do, cheaper, faster or better.
[00:28:24.040 --> 00:28:27.520]   It's also really exciting, because at some point, the laws
[00:28:27.520 --> 00:28:30.040]   of capitalism kick in, right, we've talked about this, when
[00:28:30.040 --> 00:28:34.200]   you are over earning so massively, the rational thing to
[00:28:34.200 --> 00:28:37.520]   do for other actors in the arena is to come and attack that
[00:28:37.520 --> 00:28:41.720]   margin, and give it to people for slightly cheaper, slightly
[00:28:41.720 --> 00:28:45.240]   faster, slightly better. So you can take share. Yes. So I think
[00:28:45.240 --> 00:28:48.200]   what you're seeing, and what you'll see even more now is this
[00:28:48.200 --> 00:28:51.560]   incentive for Silicon Valley, who has been really reticent to
[00:28:51.560 --> 00:28:54.880]   put money into chips, really reticent to put money into
[00:28:54.880 --> 00:28:58.240]   hardware, they're going to get pulled into investing in this
[00:28:58.240 --> 00:29:01.480]   space, because there's no choice, you have a company that
[00:29:01.480 --> 00:29:05.160]   went from 100 billion, a market cap to two and a half trillion
[00:29:05.160 --> 00:29:09.960]   in four years, it's just too much value that that is there to
[00:29:09.960 --> 00:29:12.200]   then be leaked back. You know, the interesting thing to
[00:29:12.200 --> 00:29:15.400]   remember, during the PC revolution, which is really
[00:29:15.400 --> 00:29:18.880]   mostly the 90s, right, it ended in the late 90s, I would say
[00:29:18.880 --> 00:29:22.600]   like 98 99, right before the dotcom bubble took over. Intel's
[00:29:22.600 --> 00:29:27.480]   peak market cap was I think it got to about $200 billion. And
[00:29:27.480 --> 00:29:32.680]   then their average growth rate from 1998 to today was negative
[00:29:32.680 --> 00:29:36.600]   1.4% a year. Right. So it went from about 200 billion to about
[00:29:36.600 --> 00:29:41.400]   130 odd billion. And why it's not that Intel was a worst
[00:29:41.400 --> 00:29:45.560]   company. But it's that everything else caught up. And
[00:29:45.560 --> 00:29:49.280]   the economic value went to things that sat above them in
[00:29:49.280 --> 00:29:52.800]   the stack, then it went to Cisco for a while, right? Then after
[00:29:52.800 --> 00:29:56.000]   Cisco, it went to the browser companies for a little bit, then
[00:29:56.000 --> 00:29:58.320]   it went to the app companies, then it went to the device
[00:29:58.320 --> 00:30:01.760]   companies, then it went to the mobile company. So you see this
[00:30:01.760 --> 00:30:06.600]   natural tendency for value to push up the stack over time. So
[00:30:06.600 --> 00:30:09.400]   let me see that I we've done step one, which is now you've
[00:30:09.400 --> 00:30:12.400]   given all this value to into Nvidia, and now we're going to
[00:30:12.400 --> 00:30:13.600]   see it being reallocated.
[00:30:13.600 --> 00:30:17.320]   So Jamath, who's in the arena trying stuff, some of these
[00:30:17.320 --> 00:30:19.600]   things? Well, everybody's working. Yeah.
[00:30:19.600 --> 00:30:24.440]   So right now, what you do is you speculatively bet on anything
[00:30:24.440 --> 00:30:28.160]   that kind of like quote unquote rhymes with Nvidia. So AMD is
[00:30:28.160 --> 00:30:33.640]   ripping, the companies that make HBM is gripping. So all of that
[00:30:33.640 --> 00:30:36.520]   stuff, the folks that make optical cables, this Japanese
[00:30:36.520 --> 00:30:39.680]   company that I found, that makes like the high bandwidth optical
[00:30:39.680 --> 00:30:43.480]   cables ripping. So every anything related to that
[00:30:43.480 --> 00:30:48.000]   ecosystem right now, is at all time highs. But at the same
[00:30:48.000 --> 00:30:51.680]   time, what you find now is like every other day, when you wake
[00:30:51.680 --> 00:30:55.640]   up and read the trades, in Techland, you find that there's
[00:30:55.640 --> 00:30:59.120]   a company that's gotten seeded with five to 50 million bucks to
[00:30:59.120 --> 00:31:02.760]   create a new chip, right? You're also starting to see folks that
[00:31:02.760 --> 00:31:05.160]   are working a little bit above the stack and build better
[00:31:05.160 --> 00:31:09.840]   compilers, right, things that will allow you to actually build
[00:31:09.840 --> 00:31:14.040]   once run in many different compute environments. So all of
[00:31:14.040 --> 00:31:17.720]   this stuff is starting to happen. At some point, the spread
[00:31:17.720 --> 00:31:22.520]   trade will be that Nvidia loses share, even though revenues keep
[00:31:22.520 --> 00:31:26.880]   compounding to these upstarts. Yeah, death by 1000 startups.
[00:31:26.880 --> 00:31:30.400]   All right, so I guess one of the questions people are asking
[00:31:30.400 --> 00:31:35.000]   right now is, have we ever seen a company at this scale and the
[00:31:35.000 --> 00:31:37.360]   impact it's having not just in technology, which trim off just
[00:31:37.360 --> 00:31:40.320]   pointed out beautifully, but also it's having a huge impact
[00:31:40.320 --> 00:31:43.520]   on Wall Street, on the stock market on finance.
[00:31:43.520 --> 00:31:50.280]   Well, the the company that everyone compares Nvidia to, or
[00:31:50.280 --> 00:31:53.400]   ask the question about whether a historical comparison should be
[00:31:53.400 --> 00:31:57.840]   made is Cisco. So there's an article in Motley Fool saying is
[00:31:57.840 --> 00:32:01.800]   Nvidia doomed to be the next Cisco there was one in Morning
[00:32:01.800 --> 00:32:06.000]   Star called Nvidia 2023 versus go 1999. Well, history repeat
[00:32:06.000 --> 00:32:09.360]   itself. The reason they're asking these questions is that
[00:32:09.360 --> 00:32:14.880]   if you go back to the.com boom, in 1999, to pull up the stock
[00:32:14.880 --> 00:32:18.200]   performance chart, you can see that Cisco had this incredible
[00:32:18.200 --> 00:32:24.440]   run. And if you overlay the stock price of Nvidia, it seems
[00:32:24.440 --> 00:32:26.840]   to be following that same trajectory. And what happened
[00:32:26.840 --> 00:32:30.840]   with Cisco is that when the.com crash came in 2000, Cisco stock
[00:32:30.840 --> 00:32:33.000]   lost a huge part of its value. Obviously, Cisco still around
[00:32:33.000 --> 00:32:36.880]   today. It's a valuable company, but it just hasn't ever regained
[00:32:36.880 --> 00:32:39.600]   the type of market cap it had. The reason this happened is
[00:32:39.600 --> 00:32:43.000]   because Cisco got commoditized. So to Chamath's point, the
[00:32:43.000 --> 00:32:45.680]   success and market cap of that company attracted a whole bunch
[00:32:45.680 --> 00:32:48.560]   of new entrants. And they copied Cisco's products until they were
[00:32:48.560 --> 00:32:52.400]   total commodities. So the question is, will that happen to
[00:32:52.400 --> 00:32:56.800]   Nvidia? Yeah. And I think the difference here is that at the
[00:32:56.800 --> 00:32:59.720]   end of the day, networking equipment, which Cisco produced
[00:32:59.720 --> 00:33:03.320]   was pretty easy, pretty one dimensional, pretty, pretty easy
[00:33:03.320 --> 00:33:06.040]   to move data around. Yeah, right. Whereas if you look at
[00:33:06.040 --> 00:33:11.040]   Nvidia, it's the these GPU cores are really complicated to make.
[00:33:11.040 --> 00:33:15.920]   And Jensen makes this point that the H 100, for example, has
[00:33:15.920 --> 00:33:19.880]   1000s of components, and it weighs like 70 pounds or
[00:33:19.880 --> 00:33:22.080]   something like that. I mean, that's like a giant oven. I
[00:33:22.080 --> 00:33:24.160]   mean, it's like a mainframe. It's not it's not just like a
[00:33:24.160 --> 00:33:28.480]   little chip. So it's a much more complicated product to copy. And
[00:33:28.480 --> 00:33:31.960]   then on top of that, they're already in the R&D cycle for the
[00:33:31.960 --> 00:33:34.640]   next chip, right? Whatever is gonna be the H 200, or whatever
[00:33:34.640 --> 00:33:39.480]   it is. And so as people try to catch up with H 100, they're
[00:33:39.480 --> 00:33:42.840]   gonna be on to H 200. So I think you can make the case that
[00:33:42.840 --> 00:33:47.040]   Nvidia has a much better moat than Cisco. And just by the way,
[00:33:47.040 --> 00:33:50.400]   on the Cisco comparison, just to just finish the thought, people
[00:33:50.400 --> 00:33:53.920]   were making this comparison six months ago. And what's happened
[00:33:53.920 --> 00:33:57.040]   since then Nvidia has had to block. Yes. And the competitors
[00:33:57.040 --> 00:33:58.880]   don't seem to be that much closer, maybe a little bit
[00:33:58.880 --> 00:34:03.600]   closer. But so, you know, look, I think it's an open question.
[00:34:03.600 --> 00:34:07.000]   So there's a counter here, freeberg, which is obviously, if
[00:34:07.000 --> 00:34:10.440]   you follow the Cisco analogy, one of the things that also
[00:34:10.440 --> 00:34:14.080]   sunk Cisco was once people had bought all that capacity, there
[00:34:14.080 --> 00:34:18.080]   was no need there was no file size that was so great that it
[00:34:18.080 --> 00:34:21.720]   couldn't be moved easily around the internet. You know, you make
[00:34:21.720 --> 00:34:27.160]   movies, HD, super HD 2k 4k, the bet we've created too much
[00:34:27.160 --> 00:34:29.840]   bandwidth, there was no use for it. So I guess that that's a
[00:34:29.840 --> 00:34:33.880]   counter argument for maybe when, if we build up too much capacity,
[00:34:33.880 --> 00:34:37.000]   Nvidia also could not by competitors, but just by the
[00:34:37.000 --> 00:34:40.080]   buildout being enough. So what's your take on that counter
[00:34:40.080 --> 00:34:41.680]   argument, and then whatever I thought, yeah,
[00:34:41.680 --> 00:34:44.880]   I think that the Cisco analogy, it's a pretty different
[00:34:44.880 --> 00:34:47.880]   situation, because Cisco evolved the business to become much more
[00:34:47.880 --> 00:34:52.720]   enterprise centric. And they were able to run an M&A process
[00:34:52.720 --> 00:34:55.280]   like we see with enterprise software, where they could
[00:34:55.280 --> 00:34:59.480]   acquire and roll up lots of different product companies and
[00:34:59.480 --> 00:35:01.880]   sell into their enterprise channel. So do a lot of cross
[00:35:01.880 --> 00:35:05.640]   selling. Nvidia is not a super acquisitive business. And it
[00:35:05.640 --> 00:35:07.840]   doesn't make as much sense because they're selling much
[00:35:07.840 --> 00:35:10.440]   more kind of infrastructure tools, whereas Cisco moved
[00:35:10.440 --> 00:35:13.040]   really high up in the in the enterprise stack, they were
[00:35:13.040 --> 00:35:16.120]   selling stuff into office buildings, they were selling
[00:35:16.120 --> 00:35:19.040]   software, they did acquisitions to kind of fully integrate,
[00:35:19.040 --> 00:35:21.520]   they had a very diverse set of products that were selling
[00:35:21.520 --> 00:35:25.080]   through an enterprise channel, further up the value stack, and
[00:35:25.080 --> 00:35:27.640]   a pretty distributed customer base, no, no serious
[00:35:27.640 --> 00:35:30.120]   concentration. Even though they did sell a lot into data
[00:35:30.120 --> 00:35:32.760]   centers, they were also selling to telcos, they were selling
[00:35:32.760 --> 00:35:35.400]   enterprises, they were selling to governments and so on. If you
[00:35:35.400 --> 00:35:38.440]   look at Nvidia's revenue, they did $26 billion of total revenue
[00:35:38.440 --> 00:35:42.040]   in the quarter, 22 billion of which was data center. And about
[00:35:42.040 --> 00:35:47.000]   40% of that was from the top four hyperscalers. So a full
[00:35:47.000 --> 00:35:50.880]   one third of Nvidia's revenue in the quarter came from I believe
[00:35:50.880 --> 00:35:55.200]   it's Google, Amazon, Microsoft and meta. And so between those
[00:35:55.200 --> 00:35:59.080]   four businesses, you know that those companies each have, I
[00:35:59.080 --> 00:36:03.720]   believe, at least over or close to $100 billion of cash sitting
[00:36:03.720 --> 00:36:06.480]   on their balance sheet, they can't find great places to
[00:36:06.480 --> 00:36:08.960]   invest that cash to grow revenue. And so they've
[00:36:08.960 --> 00:36:11.920]   rationalized away the idea that they will make capex investments
[00:36:11.920 --> 00:36:14.720]   to build over the next five to 10 years. And this is where that
[00:36:14.720 --> 00:36:15.400]   money flows.
[00:36:15.400 --> 00:36:17.240]   Yeah, we talked about that on a previous episode, because there's
[00:36:17.240 --> 00:36:19.800]   no M&A, to your point, yeah, that's not gonna let you buy
[00:36:19.800 --> 00:36:21.920]   stuff, or the UK is not gonna let you buy stuff.
[00:36:21.920 --> 00:36:23.960]   So I think that they're gonna have less maneuvering
[00:36:23.960 --> 00:36:28.040]   capability than Cisco had in the future. And obviously, there's
[00:36:28.040 --> 00:36:30.400]   this deep concentration risk, which is going to be deeply
[00:36:30.400 --> 00:36:30.920]   challenging.
[00:36:30.920 --> 00:36:35.440]   I think Nvidia, this is to build on Sox's point, is going to get
[00:36:35.440 --> 00:36:38.920]   pulled into competing directly with the hyperscalers. So if you
[00:36:38.920 --> 00:36:42.840]   were just selling chips, you probably wouldn't. But Sox is
[00:36:42.840 --> 00:36:48.160]   right. Like these are these big, bulky, actual machines. Then all
[00:36:48.160 --> 00:36:50.360]   of a sudden, you're like, well, why don't I just create my own
[00:36:50.360 --> 00:36:54.080]   physical plant and just stack these things and create racks
[00:36:54.080 --> 00:36:57.000]   and racks of these machines and go ahead to have an AWS instead
[00:36:57.000 --> 00:36:59.680]   of selling to them. It's not a far stretch, especially because
[00:36:59.680 --> 00:37:04.600]   Nvidia actually has the software interface that everybody uses,
[00:37:04.600 --> 00:37:08.880]   which is CUDA. So I think it's, it's likely that Nvidia goes on
[00:37:08.880 --> 00:37:12.640]   a full frontal assault against GCP and Amazon and Microsoft,
[00:37:12.640 --> 00:37:15.960]   that's going to really complicate the relationship that
[00:37:15.960 --> 00:37:18.520]   those folks have with each other. But I think it's
[00:37:18.520 --> 00:37:21.440]   inevitable, because you're good. How do you defend? It's kind of
[00:37:21.440 --> 00:37:25.320]   the Apple problem. How do you defend an enormously large
[00:37:25.320 --> 00:37:29.120]   market cap, you're forced to go into businesses that are equally
[00:37:29.120 --> 00:37:33.560]   lucrative. Now, if I look inside of compute, and look at the
[00:37:33.560 --> 00:37:36.040]   adjacent categories, they're not going to all of a sudden start a
[00:37:36.040 --> 00:37:40.080]   competitor to tick tock, right, or a social network. But if you
[00:37:40.080 --> 00:37:42.680]   look at the multi 100 billion revenue businesses that are
[00:37:42.680 --> 00:37:46.360]   adjacent to the markets that Nvidia enables, the most obvious
[00:37:46.360 --> 00:37:49.240]   one is the hyperscalers, which are multi $100 billion revenue
[00:37:49.240 --> 00:37:53.720]   businesses. So they're going to be forced to compete. Otherwise,
[00:37:53.720 --> 00:37:57.240]   your market cap will shrink. And I don't think they want that.
[00:37:57.240 --> 00:37:59.960]   And then it's going to create a very complicated set of
[00:37:59.960 --> 00:38:04.880]   incentives for Microsoft and Google and meta. And Apple and
[00:38:04.880 --> 00:38:07.600]   all the rest. And that's also then going to be an accelerant,
[00:38:07.600 --> 00:38:10.640]   they're going to pump so much money to help all of these
[00:38:10.640 --> 00:38:15.000]   upstarts. To your point, Jason, chip away and nip at the the
[00:38:15.000 --> 00:38:18.080]   Achilles heels of Nvidia until they fall.
[00:38:18.080 --> 00:38:21.280]   Yeah, and there's a great precedent for what you're saying
[00:38:21.280 --> 00:38:25.400]   because or clues. Amazon is making chips, Google is making
[00:38:25.400 --> 00:38:29.000]   chips, and it's making chips. Tesla's making chips. Yeah.
[00:38:29.000 --> 00:38:33.000]   Everybody's their own chips, and they got rid of Intel. And so
[00:38:33.000 --> 00:38:36.880]   this is how it's going to go. Your margin is my opportunity.
[00:38:36.880 --> 00:38:39.720]   And you know, with all this market cap increase, the good
[00:38:39.720 --> 00:38:42.920]   news is, just reported that Jensen has bought a second
[00:38:42.920 --> 00:38:47.440]   leather jacket. So well, this market has enabled him to expand
[00:38:47.440 --> 00:38:48.280]   the world a bit.
[00:38:48.280 --> 00:38:50.840]   I gotta say he looks really good. He looks super fit. Great.
[00:38:50.840 --> 00:38:51.600]   You know, he's late.
[00:38:51.600 --> 00:38:56.200]   Is he is in his late 50s. He's going into his Harrison Ford
[00:38:56.200 --> 00:38:57.440]   looks great.
[00:38:58.800 --> 00:39:04.920]   6161. He looks amazing. He looks amazing. I'll tell you
[00:39:04.920 --> 00:39:07.160]   something in the zombie apocalypse draft. I'm picking
[00:39:07.160 --> 00:39:09.400]   him. That guy seems crafty. You know,
[00:39:09.400 --> 00:39:15.880]   but what he does for us is no plastic golf class. No, no, no,
[00:39:15.880 --> 00:39:16.880]   his balls up plastic.
[00:39:16.880 --> 00:39:22.920]   No, no, he's got his have brass. He's literally putting his
[00:39:22.920 --> 00:39:27.720]   balls. Oh, all of our balls have plastic. Okay, well, we have two
[00:39:27.720 --> 00:39:31.400]   choices here. We can go with another tech story, or we can go
[00:39:31.400 --> 00:39:32.840]   directly to science corner. And
[00:39:32.840 --> 00:39:34.920]   well, no, I think you stay to the economy and then do science
[00:39:34.920 --> 00:39:35.320]   corner.
[00:39:35.320 --> 00:39:38.520]   Okay, so you want to talk about our pocketbooks. And then we'll
[00:39:38.520 --> 00:39:41.040]   talk about our pockets. And then we'll just make a quick detour
[00:39:41.040 --> 00:39:43.440]   to the right and then talk about our balls. We're in the same
[00:39:43.440 --> 00:39:44.160]   vicinity. So
[00:39:44.160 --> 00:39:47.520]   there was our balls. Okay, great. Yeah, let's say let's end
[00:39:47.520 --> 00:39:48.440]   with our balls. That's science.
[00:39:48.440 --> 00:39:52.440]   To start with the balls. I don't know. Everybody's got different
[00:39:52.440 --> 00:39:54.160]   kind of vibes here.
[00:39:54.680 --> 00:39:57.920]   The ballplay should come a little bit later. And the pro
[00:39:57.920 --> 00:39:59.720]   want to save the ballplay for later sex.
[00:39:59.720 --> 00:40:04.640]   Having a hard time keeping this together.
[00:40:04.640 --> 00:40:09.720]   More than half of Americans think we're in a recession. I
[00:40:09.720 --> 00:40:12.360]   think we're in a vibe session right now because we're not in a
[00:40:12.360 --> 00:40:16.360]   recession. But people are feeling really bad a Harris poll
[00:40:16.360 --> 00:40:21.560]   conducted by the Guardian shows 50 cents 56% of Americans
[00:40:21.560 --> 00:40:24.840]   wrongly believe the US is a recession. Not surprisingly,
[00:40:24.840 --> 00:40:27.920]   they blame Biden, the poll highlighted a bunch of
[00:40:27.920 --> 00:40:32.080]   misconceptions 55% believe the US economy is shrinking. It's
[00:40:32.080 --> 00:40:36.000]   obviously not 56% think the US is experiencing recession. It's
[00:40:36.000 --> 00:40:40.280]   obviously not 49% of people believe the S&P 500 stock
[00:40:40.280 --> 00:40:43.160]   market index is down for the year. It's 12% this year was up
[00:40:43.160 --> 00:40:48.880]   24% in 2023. And 49% believe that unemployment is at a 50
[00:40:48.880 --> 00:40:53.920]   year high when it's in fact at a 50 year low. And Americans are
[00:40:53.920 --> 00:40:56.760]   really concerned about the cost of living and inflation. Fair
[00:40:56.760 --> 00:41:00.200]   enough. 70% said the biggest economic concern was the cost of
[00:41:00.200 --> 00:41:04.800]   living 68% said that inflation was the biggest economic
[00:41:04.800 --> 00:41:08.480]   concern important quote here, a majority of respondents agreed
[00:41:08.480 --> 00:41:11.160]   it's difficult to be happy about positive economic news when I
[00:41:11.160 --> 00:41:14.960]   feel financially squeezed each month and that the economy was
[00:41:14.960 --> 00:41:17.760]   worse than the media made it out to be. According to the polls,
[00:41:17.760 --> 00:41:21.480]   70% of Republicans and 40% of Democrats think Biden is making
[00:41:21.480 --> 00:41:25.280]   the economy worse. Chamath, you have some thoughts on this.
[00:41:25.280 --> 00:41:30.320]   I'm going to go out on a limb and speculate that a lot of the
[00:41:30.320 --> 00:41:35.960]   big numbers that we use to gauge how we should feel about things
[00:41:35.960 --> 00:41:41.640]   in today's day and age are pretty brittle. Okay, explain
[00:41:41.640 --> 00:41:46.080]   where agile and may may actually just be totally wrong. So what's
[00:41:46.080 --> 00:41:49.160]   an example? So for example, like, if you look at something
[00:41:49.160 --> 00:41:52.800]   like non farm payrolls, right, so the first Friday of every
[00:41:52.800 --> 00:41:55.720]   month, you get this report that comes out from the Department of
[00:41:55.720 --> 00:41:59.840]   Labor, and it shows what where unemployment is. But how do they
[00:41:59.840 --> 00:42:02.840]   calculate that? Do you think that they have a real time sense
[00:42:02.840 --> 00:42:05.960]   of exactly every person in that month that entered the workforce
[00:42:05.960 --> 00:42:09.080]   or exited the workforce? No, they do a survey and then they
[00:42:09.080 --> 00:42:12.920]   extrapolate. And if you do that survey incorrectly, and Jason,
[00:42:12.920 --> 00:42:15.200]   you've commented on this before, for example, if you don't
[00:42:15.200 --> 00:42:17.920]   capture adequately, the number of people that are on the
[00:42:17.920 --> 00:42:20.560]   sidelines and never joined the workforce, or the number of
[00:42:20.560 --> 00:42:23.080]   people that are part of the gig economy, so they are kind of
[00:42:23.080 --> 00:42:26.760]   working, you get an inaccurate sense of where the real economy
[00:42:26.760 --> 00:42:31.960]   is. I think that GDP is somewhat similar. Because if you just
[00:42:31.960 --> 00:42:35.720]   break down what GDP is, so Nick, there's a very simple pie chart
[00:42:35.720 --> 00:42:41.440]   I sent to you, what is GDP, it's the sum of four things. Most of
[00:42:41.440 --> 00:42:46.480]   it is what people spend. Okay, then the next big chunk is what
[00:42:46.480 --> 00:42:52.320]   companies and governments spend. And then the last is what we
[00:42:52.320 --> 00:42:55.080]   export to other countries. So let's just pause for one second
[00:42:55.080 --> 00:42:58.280]   and think about what do you think happens when rates are zero
[00:42:58.280 --> 00:43:02.000]   versus when rates are at 6% people spend a lot more. Well,
[00:43:02.000 --> 00:43:05.640]   people are tend are tend to save when interest rates are high.
[00:43:05.640 --> 00:43:08.600]   Just the natural thing, like, why would I buy a pair of these
[00:43:08.600 --> 00:43:12.160]   Nike shoes, I'll just put it in the bank, I get 6%. But when the
[00:43:12.160 --> 00:43:15.040]   bank pays you zero, you're like, ah, let me buy these Air Force
[00:43:15.040 --> 00:43:18.160]   ones and move on, right? It turns out, it's the same for
[00:43:18.160 --> 00:43:21.400]   companies, companies find it easier to invest when rates are
[00:43:21.400 --> 00:43:24.040]   at zero, because it's cheaper. It's much more expensive,
[00:43:24.040 --> 00:43:27.400]   because they're borrowing money at 6% versus at 0%. Or more
[00:43:27.400 --> 00:43:30.640]   corporate gets charged a higher fee, right? Yeah. Then when you
[00:43:30.640 --> 00:43:34.760]   have high interest rates, you have a currency that
[00:43:34.760 --> 00:43:39.400]   appreciates it makes exports less attractive to other people,
[00:43:39.400 --> 00:43:42.800]   which means then you become an ad importer. Okay, so what is the
[00:43:42.800 --> 00:43:45.040]   last thing that's left? The last thing that's left is government
[00:43:45.040 --> 00:43:47.360]   spending. And you have to ask the question, what should
[00:43:47.360 --> 00:43:52.880]   governments do when rates are high? There was a chart I
[00:43:52.880 --> 00:43:55.680]   published in my annual letter, if you just go to that for a
[00:43:55.680 --> 00:43:56.080]   second.
[00:43:56.080 --> 00:43:58.600]   And just going back to this chart right before it just so
[00:43:58.600 --> 00:44:00.720]   that people who are listening, put the pie chart in there.
[00:44:00.720 --> 00:44:04.240]   Important for people to know, consumer is about 70% of the
[00:44:04.240 --> 00:44:06.880]   economy. And if you put investment in government
[00:44:06.880 --> 00:44:13.160]   together, that's just over 34% or 35% percent. So it is a
[00:44:13.160 --> 00:44:15.800]   consumer driven economy. But hey, you know, corporate and
[00:44:15.800 --> 00:44:17.520]   government spending is a major piece as well.
[00:44:17.520 --> 00:44:22.680]   And then I just wanted just to highlight that when interest
[00:44:22.680 --> 00:44:26.520]   rates are very high, all of a sudden, governments are faced
[00:44:26.520 --> 00:44:28.680]   with this very difficult problem, which is, oh, man, I
[00:44:28.680 --> 00:44:31.120]   have to spend a ton of money on interest, just like if you had a
[00:44:31.120 --> 00:44:33.120]   bunch of credit cards, and all of a sudden, the interest rates
[00:44:33.120 --> 00:44:38.280]   went up. So the choice is twofold. Do governments spend
[00:44:38.280 --> 00:44:43.320]   less? But unfortunately, it turns out that our governments
[00:44:43.320 --> 00:44:46.720]   in America, they just keep spending more and more. So even
[00:44:46.720 --> 00:44:49.600]   if net interest income is small, even if net interest income is
[00:44:49.600 --> 00:44:52.720]   high, they're just like, forget it, the taps are on. So what
[00:44:52.720 --> 00:44:55.120]   does this all mean? I think what it really means is that we do a
[00:44:55.120 --> 00:44:59.280]   very poor job of measuring all these dynamics together. And so
[00:44:59.280 --> 00:45:04.800]   I actually trust the survey data of these individuals more than I
[00:45:04.800 --> 00:45:08.160]   trust the GDP report, in the sense that I think it more
[00:45:08.160 --> 00:45:12.160]   accurately captures this dynamic rates are at 6%. People are
[00:45:12.160 --> 00:45:15.440]   saving more, if they're not getting paid more, things are
[00:45:15.440 --> 00:45:18.600]   costing more, the government is giving you free money. So you
[00:45:18.600 --> 00:45:21.880]   kind of feel like everything is moving. So that the GDP
[00:45:21.880 --> 00:45:24.800]   measurement, the way that it's classically done shows that,
[00:45:24.800 --> 00:45:29.080]   wow, we grew at three or 4%. But the average individual American
[00:45:29.120 --> 00:45:31.680]   isn't feeling that they're actually feeling that they have
[00:45:31.680 --> 00:45:36.280]   less money. So I would actually go with them. And actually say,
[00:45:36.280 --> 00:45:39.920]   if we don't revisit this thing from first principles, we're
[00:45:39.920 --> 00:45:44.640]   going to get this dynamic where we think one thing is happening,
[00:45:44.640 --> 00:45:47.200]   but the actual exact opposite is happening. In this case, I do
[00:45:47.200 --> 00:45:50.160]   think we're we're in a quasi synthetic recession.
[00:45:50.160 --> 00:45:53.800]   Saks, what's your take on the vibe session?
[00:45:53.800 --> 00:45:56.800]   Well, look, I, I tend to agree with your mouth on this. I think
[00:45:56.800 --> 00:46:00.240]   this is a classic story of who do you believe? Do you believe
[00:46:00.240 --> 00:46:02.680]   the experts? Or do you believe in the intuitions of the
[00:46:02.680 --> 00:46:07.600]   American people? And the experts have some statistics on their
[00:46:07.600 --> 00:46:10.600]   side. But you know, the old saying goes, there's lies, damn
[00:46:10.600 --> 00:46:14.040]   lies and statistics. And then the American people have their
[00:46:14.040 --> 00:46:18.000]   actual lived experience on their side, they know what they're
[00:46:18.000 --> 00:46:21.920]   feeling. And I tend to trust in that. And obviously, we're in an
[00:46:21.920 --> 00:46:24.040]   election year, and the press knows that. So they're trying to
[00:46:24.040 --> 00:46:27.560]   do this big cleanup effort for Biden. But why is it that people
[00:46:27.560 --> 00:46:30.320]   are feeling this way? Number one is inflation. And if you look at
[00:46:30.320 --> 00:46:35.880]   this chart, you can see that if you look at household net worth
[00:46:35.880 --> 00:46:39.960]   since the start of the Biden presidency and compare it to the
[00:46:39.960 --> 00:46:43.080]   change in household net worth at a similar point in Trump's
[00:46:43.080 --> 00:46:46.920]   presidency, in nominal terms, it appears to be the same. But then
[00:46:46.920 --> 00:46:49.160]   if you adjust for inflation, in other words, you look at the
[00:46:49.160 --> 00:46:53.400]   real household net worth, you can see that household net worth
[00:46:53.400 --> 00:46:57.520]   during the Biden term has been flat actually is down because of
[00:46:57.520 --> 00:46:59.880]   inflation, right? Because of inflation, where did the
[00:46:59.880 --> 00:47:03.800]   inflation come from? Where the inflation come from? Yeah, well,
[00:47:03.800 --> 00:47:06.680]   Larry Summers warned in the first quarter of the Biden
[00:47:06.680 --> 00:47:11.980]   administration, that if you pass an unnecessary $2 trillion of
[00:47:11.980 --> 00:47:15.720]   COVID stimulus, you would produce inflation. The inflation
[00:47:15.720 --> 00:47:19.040]   rate when Biden came into office was 1.7%. We had a rip roaring
[00:47:19.040 --> 00:47:21.920]   economy, but he started stimulating and we talked about
[00:47:21.920 --> 00:47:26.480]   by dynamics is this new policy of pumping trillions of dollars
[00:47:26.480 --> 00:47:29.080]   of stimulus into a healthy economy, which we've never done
[00:47:29.080 --> 00:47:33.200]   before, what happened, inflation went all the way to 9%. So
[00:47:33.200 --> 00:47:36.080]   people's wages have not kept up with the rate of inflation. This
[00:47:36.080 --> 00:47:39.220]   is why they feel worse off. When you actually look at purchasing
[00:47:39.220 --> 00:47:42.520]   power, people are worse off in terms of their actual ability to
[00:47:42.520 --> 00:47:46.000]   buy things. Their purchasing power has gone down, wages may
[00:47:46.000 --> 00:47:47.960]   have gone up a little bit, but they have not gone up as much as
[00:47:47.960 --> 00:47:53.720]   inflation. So people feel worse off. Now, Larry also had that, I
[00:47:53.720 --> 00:47:58.040]   think, really informative study, showing that inflation would
[00:47:58.040 --> 00:48:00.620]   have peaked at 18%. If you include cost of borrowing. So
[00:48:00.620 --> 00:48:04.040]   again, to Tomas point, if you're trying to get a mortgage, and
[00:48:04.040 --> 00:48:07.080]   you're paying seven and a half 8%, you're feel way worse off.
[00:48:07.080 --> 00:48:09.880]   If you need to buy a car and make a car payment, you feel
[00:48:09.880 --> 00:48:12.840]   much worse off. If you've got credit card debt, which is now
[00:48:12.840 --> 00:48:15.920]   hit an all time record of something like 1.1 trillion,
[00:48:16.280 --> 00:48:19.160]   your credit card rates have never been higher. So the
[00:48:19.160 --> 00:48:21.880]   average American feels worse off because cost of borrowing has a
[00:48:21.880 --> 00:48:25.400]   huge impact on their household finances. And that's why if you
[00:48:25.400 --> 00:48:28.080]   read like one of the last paragraphs in that story that
[00:48:28.080 --> 00:48:31.320]   you referred to, they use the key words, the consumer feels
[00:48:31.320 --> 00:48:34.480]   squeezed, the average household feels squeezed, they may not
[00:48:34.480 --> 00:48:38.280]   have lost their job yet. But they've lost purchasing power,
[00:48:38.280 --> 00:48:41.120]   and they've lost their under earning, their under earning.
[00:48:41.120 --> 00:48:44.480]   And so the project is obvious. And you know, the press, the
[00:48:44.480 --> 00:48:47.240]   press can gaslight us all day long about how wonderful things
[00:48:47.240 --> 00:48:51.040]   are under Biden. But the average American, I think understands
[00:48:51.040 --> 00:48:53.160]   differently, based on their own experience.
[00:48:53.160 --> 00:48:56.240]   I think the whole thing comes down to the projection of an
[00:48:56.240 --> 00:49:00.160]   individual or a household of their lived experience onto the
[00:49:00.160 --> 00:49:03.440]   economy. You assume that because you're having a tough time, the
[00:49:03.440 --> 00:49:08.000]   economy is bad. That and the economy as a definition for them
[00:49:08.000 --> 00:49:12.600]   is, how do I earn and how do I spend? And if I'm under earning,
[00:49:12.600 --> 00:49:16.680]   that means there must be serious job loss. And things are more
[00:49:16.680 --> 00:49:20.760]   expensive, and my ability to purchase isn't improving. And so
[00:49:20.760 --> 00:49:23.200]   I think we're all kind of going to end up on the same take on
[00:49:23.200 --> 00:49:25.680]   this one. I mean, Nick, if you want to pull this image up, this
[00:49:25.680 --> 00:49:28.440]   is, I think, a helpful one, which is disposable personal
[00:49:28.440 --> 00:49:32.480]   income relative to outlays, that folks are needing to spend more
[00:49:32.480 --> 00:49:36.960]   than they're making. So clearly indicating that they're feeling
[00:49:36.960 --> 00:49:40.000]   like they're under earning. So the projection of that is the
[00:49:40.000 --> 00:49:42.520]   economy is bad, without recognizing that it is an
[00:49:42.520 --> 00:49:46.840]   inflationary experience, whereas economists use the definition of
[00:49:46.840 --> 00:49:50.680]   quote, economic growth, being gross production, gross product.
[00:49:50.680 --> 00:49:54.040]   And so if gross product or gross revenue is going up, they're
[00:49:54.040 --> 00:49:56.400]   like, Oh, the economy is healthy, we're growing. But the
[00:49:56.400 --> 00:50:00.320]   truth is, we're funding that growth with leverage at the
[00:50:00.320 --> 00:50:03.560]   national level, the federal level, and at the household and
[00:50:03.560 --> 00:50:08.920]   domestic level, we are borrowing money to inflate the revenue
[00:50:08.920 --> 00:50:14.040]   numbers. And so the GDP goes up, but the debt is going up higher.
[00:50:14.040 --> 00:50:17.280]   And so the ability for folks to support themselves and buy
[00:50:17.280 --> 00:50:20.520]   things that they want to buy, and continue to improve their
[00:50:20.520 --> 00:50:24.760]   condition in life has declined, if things are getting worse. And
[00:50:24.760 --> 00:50:27.800]   if you go to the next image, as Saks pointed out already, here's
[00:50:27.800 --> 00:50:31.400]   the image of total outstanding credit card debt, over a
[00:50:31.400 --> 00:50:34.040]   trillion dollars, it's totally spiked. And it's going to
[00:50:34.040 --> 00:50:37.120]   continue to spike, just like federal debt because of the next
[00:50:37.120 --> 00:50:41.160]   chart, which is the sudden jump in interest rates. So we've seen
[00:50:41.160 --> 00:50:46.120]   credit card interest rates jump from 12%, on average, 10 years
[00:50:46.120 --> 00:50:53.320]   ago to 21.19%. Right now, and it was at 14% at the end of 2022.
[00:50:53.320 --> 00:50:56.680]   So we've gone from 14% average credit card interest rates to
[00:50:56.680 --> 00:51:03.280]   22% now, in just about 2420 to 24 months. And so the the
[00:51:03.280 --> 00:51:06.680]   projection that I think of the quote economy must be bad is
[00:51:06.680 --> 00:51:11.040]   resulted from the fact that income to spending is actually
[00:51:11.040 --> 00:51:14.840]   pretty negative. So here's the real median family income. This
[00:51:14.840 --> 00:51:17.440]   actually only goes through 2019. So it doesn't even capture the
[00:51:17.440 --> 00:51:19.840]   air, the air that we're talking about. But this has been going
[00:51:19.840 --> 00:51:23.680]   on for quite some time, that the average American's ability to
[00:51:23.680 --> 00:51:27.240]   improve their condition has largely been driven by their
[00:51:27.240 --> 00:51:31.120]   ability to borrow, not by their earnings. And this is create a
[00:51:31.120 --> 00:51:34.280]   substantial set of precedents that we're now running into a
[00:51:34.280 --> 00:51:37.680]   wall with interest rates spiking and inflation hitting us because
[00:51:37.680 --> 00:51:39.480]   of the overall federal debt that we've taken on.
[00:51:39.480 --> 00:51:42.160]   I think we're probably going to go around the horn and all agree.
[00:51:42.160 --> 00:51:46.200]   Obviously, the crazy spending started in the Trump and COVID
[00:51:46.200 --> 00:51:48.480]   era. And that caused a lot of the inflation as well, just to
[00:51:48.480 --> 00:51:52.040]   be fair to two administrations that are just out of control
[00:51:52.040 --> 00:51:54.680]   with spending. But the way I look at this is the Mickey D
[00:51:54.680 --> 00:51:59.880]   economy. People may not know this, but 96% of Americans eat
[00:51:59.880 --> 00:52:04.160]   meals at least once a year in McDonald's 8% of Americans eat
[00:52:04.160 --> 00:52:07.680]   at McDonald's on an average day. And when you look at the prices
[00:52:07.680 --> 00:52:10.240]   of McDonald's here, if we look at this image, this is
[00:52:10.240 --> 00:52:14.080]   incredible. This is this is unbelievable medium French fries
[00:52:14.080 --> 00:52:18.960]   at McDonald's bucks 79 in 2019. And now for 19. And then if we
[00:52:18.960 --> 00:52:25.400]   look at just McNuggets, gosh, 449 to 758 68% increase McChicken
[00:52:25.400 --> 00:52:29.840]   129 to 389. And then here is a very interesting one. This is
[00:52:29.840 --> 00:52:34.240]   CPI versus McDonald's. Big Mac prices. Take a look at that. As
[00:52:34.240 --> 00:52:37.600]   much as the consumer price index has surged. Big Macs have
[00:52:37.600 --> 00:52:42.120]   exceeded that. And so Americans are seeing this over and over
[00:52:42.120 --> 00:52:46.840]   again, when they go to McDonald's and other places. And
[00:52:46.840 --> 00:52:48.520]   that's what's causing the feeling.
[00:52:48.520 --> 00:52:52.840]   Go back to that McNugget chart. I just want to see what what is
[00:52:52.840 --> 00:52:55.600]   it end of 2019. So this is basically you'd want to look at
[00:52:55.600 --> 00:52:58.320]   the four year stock price, right? So like these guys have
[00:52:58.320 --> 00:53:02.040]   jacked up prices massively. If you look at what's happened to
[00:53:02.040 --> 00:53:04.360]   the stock, the stock is way up. It's kind of been they've been
[00:53:04.360 --> 00:53:08.940]   very motivated and rewarded as a company for just rewarding the
[00:53:08.940 --> 00:53:11.600]   shareholder and kind of screwing over the customer.
[00:53:11.600 --> 00:53:13.880]   If you own equities in McDonald's, and you're in the
[00:53:13.880 --> 00:53:17.520]   top third or half of maybe half of Americans who have equity
[00:53:17.520 --> 00:53:20.720]   exposure, you're feeling great. If you're on the bottom third or
[00:53:20.720 --> 00:53:23.320]   half, and you're buying at McDonald's, and you don't own
[00:53:23.320 --> 00:53:25.800]   equity in McDonald's, you feel terrible free break, you had
[00:53:25.800 --> 00:53:26.560]   some additional thoughts.
[00:53:26.800 --> 00:53:30.240]   Yeah, but remember what McDonald's and other fast food
[00:53:30.240 --> 00:53:33.120]   companies have said is that labor costs have climbed. Here's
[00:53:33.120 --> 00:53:37.300]   a chart on labor costs that can pull up workers at Walmart and
[00:53:37.300 --> 00:53:41.280]   McDonald's have had pay increases. But this has really
[00:53:41.280 --> 00:53:44.360]   been to try and keep up with inflation, the inflation of
[00:53:44.360 --> 00:53:47.360]   other costs. So a lot of people will say, oh, they're price
[00:53:47.360 --> 00:53:50.100]   gouging, they're ripping off consumers to make profits for
[00:53:50.100 --> 00:53:52.960]   shareholders. But the truth is, the biggest component of running
[00:53:52.960 --> 00:53:56.760]   those restaurants is labor. And labor has gotten more expensive
[00:53:57.040 --> 00:54:01.040]   because the employees that work there have to earn enough to pay
[00:54:01.040 --> 00:54:04.640]   their bills and to afford their food. And this is the circular
[00:54:04.640 --> 00:54:08.000]   effect of inflation. It finds its way all through the economy,
[00:54:08.000 --> 00:54:10.720]   it filters down, and it eventually hits everyone.
[00:54:10.720 --> 00:54:13.160]   Look, fast food is a pretty competitive business. I mean, I
[00:54:13.160 --> 00:54:15.960]   think the reason why McDonald's is raising prices because
[00:54:15.960 --> 00:54:17.760]   everyone else is raising prices. I mean, otherwise, they'd be
[00:54:17.760 --> 00:54:20.840]   losing share. And just look, go to the grocery store and look at
[00:54:20.840 --> 00:54:24.280]   the price of steak or chicken or whatever, or eggs. It's gone up
[00:54:24.280 --> 00:54:27.000]   tremendously over the last few years. I mean,
[00:54:27.000 --> 00:54:30.560]   let me ask you a candy question. When's the last time you were in
[00:54:30.560 --> 00:54:34.760]   supermarket? Be honest, when's the last time I'm not literally
[00:54:34.760 --> 00:54:36.120]   went to a I'm just curious.
[00:54:36.120 --> 00:54:41.440]   It's not relevant. I mean, yeah, look, obviously, we know that
[00:54:41.440 --> 00:54:43.800]   the price of eggs doesn't affect me. Okay, great. I'm in a
[00:54:43.800 --> 00:54:46.440]   fortunate position. That's not what the topic is. The topic is,
[00:54:46.440 --> 00:54:49.960]   what is the impact on the American people? And why? Why do
[00:54:49.960 --> 00:54:53.640]   70% of the people in that poll feel that we're in a recession,
[00:54:53.640 --> 00:54:56.400]   even though the experts tell us we're not? And I've explained
[00:54:56.400 --> 00:54:59.600]   it. Yeah. And the other thing is, there's a way to
[00:54:59.600 --> 00:55:02.520]   I like on this market, I take
[00:55:02.520 --> 00:55:06.440]   I cannot believe how expensive things I mean, some of the
[00:55:06.440 --> 00:55:08.920]   stuff I was just blown away how expensive it's
[00:55:08.920 --> 00:55:12.360]   look at the look at the package I sent you guys the Driscoll's
[00:55:12.360 --> 00:55:13.680]   super sweet strawberries
[00:55:13.680 --> 00:55:17.880]   in court of the market to try to find them here in San Mateo.
[00:55:17.880 --> 00:55:19.960]   There's none left. They said come off quarter of the market.
[00:55:20.200 --> 00:55:23.200]   I was going to not a night to know not and I go to Segonas
[00:55:23.200 --> 00:55:27.640]   every week. A because we like buying our own fruit. Yeah, but
[00:55:27.640 --> 00:55:30.280]   also because our kids like to do it. And they like to see what
[00:55:30.280 --> 00:55:33.320]   things cost and they like to pick stuff. But you know, this
[00:55:33.320 --> 00:55:37.200]   sweetest batch was seven bucks. For how much by the way. Well,
[00:55:37.200 --> 00:55:40.440]   well, so here's what I'll tell you quite honestly, I'm like, I
[00:55:40.440 --> 00:55:46.680]   think that you need to put these guys on notice. Oh, on blast the
[00:55:46.680 --> 00:55:52.680]   perfume. So the the nose, like just like, it's incredible. Okay,
[00:55:52.680 --> 00:55:56.760]   the smell is the aroma. It's just absolutely incredible. Yeah.
[00:55:56.760 --> 00:56:00.000]   But to be totally honest with you, the mouthfeel and the
[00:56:00.000 --> 00:56:05.240]   sweetness is not what this label would imply. Yeah. What were you
[00:56:05.240 --> 00:56:10.640]   expecting in terms of mouthfeel? I was expecting, like, something
[00:56:10.640 --> 00:56:12.520]   juicier, more succulent.
[00:56:13.480 --> 00:56:15.640]   Yes, your questions about do you have a sommelier for your
[00:56:15.640 --> 00:56:18.760]   fruit? Yeah, I mean, you seem like a real connoisseur here.
[00:56:18.760 --> 00:56:19.360]   What did you do?
[00:56:19.360 --> 00:56:27.040]   No, I don't. I've been texting for months. I am a connoisseur
[00:56:27.040 --> 00:56:31.120]   of fruit. Okay. It's very important to me. I like good
[00:56:31.120 --> 00:56:34.480]   fruit. So you know, my wife and I go and find good fruit for our
[00:56:34.480 --> 00:56:38.200]   family. And it's really expensive. And even this over
[00:56:38.200 --> 00:56:40.920]   promises and under deliver. So free bird, you need to get if
[00:56:40.920 --> 00:56:44.760]   you can lend a GMO strawberry, I'm gonna put it in my belly.
[00:56:44.760 --> 00:56:49.160]   Give me a GMO strawberry free bird twice as big three times as
[00:56:49.160 --> 00:56:51.480]   sweet. Get in my belly.
[00:56:51.480 --> 00:56:54.960]   I think we should all go to Tokyo for a weekend and do some
[00:56:54.960 --> 00:56:56.000]   fruit tasting in Tokyo.
[00:56:56.000 --> 00:57:00.040]   You have to get on the Hokkaido strawberries. This is where it's
[00:57:00.040 --> 00:57:03.760]   at. You have no idea like what you can spend on strawberries.
[00:57:03.760 --> 00:57:06.880]   People are spending 10 bucks on a strawberry. It is bonkers.
[00:57:06.880 --> 00:57:09.160]   I bought $100 mango once in Tokyo.
[00:57:09.360 --> 00:57:12.800]   Yeah, incredible. You know, the other thing I think with these
[00:57:12.800 --> 00:57:15.520]   numbers is, if you're an economist, and you're like
[00:57:15.520 --> 00:57:20.520]   inflation has gone down, that means the rate of inflation has
[00:57:20.520 --> 00:57:24.680]   gone down from six or 7% down to 3%, or 2.9, or 3.1. That doesn't
[00:57:24.680 --> 00:57:27.800]   mean prices aren't still going up. And so the question is, how
[00:57:27.800 --> 00:57:30.040]   do you want to cycle if you want to replace if you want to
[00:57:30.040 --> 00:57:33.880]   replace GDP, there was a very good article in the Wall Street
[00:57:33.880 --> 00:57:36.400]   Journal a few weeks ago about how there's been a just a total
[00:57:36.400 --> 00:57:39.640]   breakdown in what these high level numbers say kind of what
[00:57:39.640 --> 00:57:42.480]   we've been saying and how Americans feel. And they
[00:57:42.480 --> 00:57:45.560]   introduced a different score. Well, they gave it publicity.
[00:57:45.560 --> 00:57:48.040]   It's not their score. But it's something that they call the
[00:57:48.040 --> 00:57:52.000]   core score. Okay. And what that does, it's, I'll just read it to
[00:57:52.000 --> 00:57:55.720]   you just so you can understand it. It's a county level index of
[00:57:55.720 --> 00:57:59.880]   well being using measures of economic security, economic
[00:57:59.880 --> 00:58:02.760]   opportunity, health and political voice. And so the
[00:58:02.760 --> 00:58:05.960]   lowest possible score is zero, the highest possible score is 10
[00:58:06.000 --> 00:58:08.880]   as it turns out, when they use this across every county in
[00:58:08.880 --> 00:58:12.600]   America, the distribution is basically as follows the the
[00:58:12.600 --> 00:58:16.240]   most, quote unquote, prosperous county is Falls Church,
[00:58:16.240 --> 00:58:21.280]   Virginia, which is 7.86 out of 10. And the lowest county is
[00:58:21.280 --> 00:58:26.320]   Jim Hawk County of Texas, which has a score of 2.25. So to the
[00:58:26.320 --> 00:58:29.080]   extent that you want to start to look at granular measures, this
[00:58:29.080 --> 00:58:31.520]   is one, I'm not going to advocate for it, but it's an
[00:58:31.520 --> 00:58:35.320]   example. But what do you notice in here, what I noticed is that
[00:58:35.360 --> 00:58:41.040]   there's a lot of patches of like, man to not good in most
[00:58:41.040 --> 00:58:45.320]   parts of America. And so other than a very few small pockets
[00:58:45.320 --> 00:58:50.200]   where people feel great, most of the country is sort of
[00:58:50.200 --> 00:58:53.160]   dissatisfied. And I think that that's a really important thing
[00:58:53.160 --> 00:58:57.280]   to internalize. Yeah, some of this is classic psychology, you
[00:58:57.280 --> 00:59:00.120]   know, people do focus on the negative, the media focuses on
[00:59:00.120 --> 00:59:02.960]   the negative. And then with social media, people are seeing
[00:59:03.400 --> 00:59:06.080]   lifestyles that are unattainable, just like they're
[00:59:06.080 --> 00:59:08.400]   seeing body types that are unattainable, because people are
[00:59:08.400 --> 00:59:10.960]   doing filters, you're also seeing people living a lifestyle
[00:59:10.960 --> 00:59:14.320]   that's unattainable. And then it makes people because their
[00:59:14.320 --> 00:59:18.560]   expectation of their life is so high, when they then subtract
[00:59:18.560 --> 00:59:20.640]   the reality of their life, they've got a deficit. And
[00:59:20.640 --> 00:59:24.480]   really, happiness is like, expectations minus reality
[00:59:24.480 --> 00:59:25.440]   equals happiness.
[00:59:25.440 --> 00:59:28.880]   Yeah, but you had that same dynamic when consumer sentiment
[00:59:28.880 --> 00:59:30.880]   was much higher in a previous administration, people are
[00:59:30.880 --> 00:59:33.400]   feeling much better about the economy. So that's a constant,
[00:59:33.400 --> 00:59:35.240]   you know, when people were having free money,
[00:59:35.240 --> 00:59:39.280]   you're having free Trump drop free money on people's heads. So
[00:59:39.280 --> 00:59:42.280]   your argument that crazy spending? No, no, I'm talking
[00:59:42.280 --> 00:59:45.840]   about it with his name on it. Yeah, when the economy when the
[00:59:45.840 --> 00:59:50.120]   economy was down 33% year over year, but you would spend way
[00:59:50.120 --> 00:59:53.000]   too much money, right? You admit he spent both parties, both
[00:59:53.000 --> 00:59:55.440]   parties thought that we were headed for depression. And so we
[00:59:55.440 --> 01:00:00.240]   had a bipartisan stimulus bill during an actual crisis. Once
[01:00:00.240 --> 01:00:03.000]   the crisis was over, there was no need to keep spending.
[01:00:03.000 --> 01:00:09.920]   When Trump says, Jason, Jason, let's be he wasn't responsible.
[01:00:09.920 --> 01:00:13.240]   And once this once the seal has been broken, yeah, I think it's
[01:00:13.240 --> 01:00:18.960]   fair to say, both sides of the aisle now believe that they can
[01:00:18.960 --> 01:00:23.000]   give away an enormous amount of money. Absolutely. There's
[01:00:23.000 --> 01:00:26.960]   nobody that feels like they have a responsibility to stop. But I
[01:00:26.960 --> 01:00:29.640]   think I think what sax is, what sax is right in the sense that
[01:00:29.640 --> 01:00:33.080]   so if you take that as a constant, right, that there will
[01:00:33.080 --> 01:00:36.760]   always be handouts now of all kinds. Yep, there'll be
[01:00:36.760 --> 01:00:39.000]   different flavors, depending on whether it's a Republican or
[01:00:39.000 --> 01:00:42.520]   whether a Democrat, a Democrat, and all rationalized. It's all
[01:00:42.520 --> 01:00:45.680]   it's all. But so then what I'm saying is my point, this
[01:00:45.680 --> 01:00:48.680]   abstraction, though, doesn't solve what's happening now,
[01:00:48.680 --> 01:00:51.800]   because this free money is in the system. It's constantly in
[01:00:51.800 --> 01:00:55.080]   the system, it comes in different ways. So people should
[01:00:55.080 --> 01:00:57.640]   feel better. The fact that they don't in the face of this
[01:00:57.640 --> 01:01:00.640]   constant money train, yeah, I think is actually quite
[01:01:00.640 --> 01:01:03.200]   alarming. This is I think what the point is, which is we are
[01:01:03.200 --> 01:01:06.640]   economically, in a very complicated moment in the sense
[01:01:06.640 --> 01:01:10.040]   that there is no pandemic to blame. There's no economy
[01:01:10.040 --> 01:01:12.800]   that's totally shut down. In fact, there's an economy that
[01:01:12.800 --> 01:01:16.240]   seems to be moving, but leaving an enormous number of people
[01:01:16.240 --> 01:01:19.560]   behind. So however, it has been structured, just sitting here
[01:01:19.560 --> 01:01:23.400]   today in 2024. It's broken for more people than it's working
[01:01:23.400 --> 01:01:24.280]   for. Yeah,
[01:01:24.600 --> 01:01:27.640]   Trump, just to give facts, Trump will have spent 7.8
[01:01:27.640 --> 01:01:30.600]   trillion, Biden will spend slightly less like six point x
[01:01:30.600 --> 01:01:32.440]   trillion. At the end of these things, they're both going to
[01:01:32.440 --> 01:01:32.920]   have added
[01:01:32.920 --> 01:01:37.600]   you're missing the fact that Trump had 2020 with when we had
[01:01:37.600 --> 01:01:41.560]   the COVID depression, or what could have been when the economy
[01:01:41.560 --> 01:01:43.360]   is down 30% year over year.
[01:01:43.360 --> 01:01:46.800]   Yeah, and a terribly timed tax. By the way, did you guys see
[01:01:46.800 --> 01:01:47.120]   break?
[01:01:47.120 --> 01:01:49.920]   Hold on a second. Let me just make this one point. Yeah, just
[01:01:49.920 --> 01:01:53.160]   because both parties have been irresponsible and spending
[01:01:53.160 --> 01:01:56.720]   doesn't mean that we can't make further judgments about who's
[01:01:56.720 --> 01:01:59.440]   been worse. Sure. What's happened in the Biden
[01:01:59.440 --> 01:02:01.720]   administration is just quantitatively worse.
[01:02:01.720 --> 01:02:05.200]   Well, no, quantitatively, Trump spent more. But you're saying
[01:02:05.200 --> 01:02:07.760]   qualitatively, Biden's is worse because he didn't need to.
[01:02:07.760 --> 01:02:11.680]   There was no crisis. Right? Okay. So one spent more one
[01:02:11.680 --> 01:02:15.160]   spent less, but one didn't need to look at look at Trump
[01:02:15.160 --> 01:02:18.640]   spending for COVID. Look at Trump spending before COVID. It
[01:02:18.640 --> 01:02:21.680]   was like a 5% bump on Obama spending.
[01:02:21.840 --> 01:02:25.720]   Well, the tax break was the one that that accounted for a lot of
[01:02:25.720 --> 01:02:28.840]   Yeah, but anyway, we can sit here and debate Trump versus
[01:02:28.840 --> 01:02:31.440]   Biden. Let's talk about my balls, boys. Yeah, we're
[01:02:31.440 --> 01:02:35.040]   wasting time here. wasting time on Trump and Biden when we could
[01:02:35.040 --> 01:02:38.320]   be talking to my battle. If we're going to talk about our
[01:02:38.320 --> 01:02:43.280]   balls, we need to go to somebody who's an expert on our
[01:02:43.280 --> 01:02:48.120]   testicles. Freeberg. Let's go right to the corner. Let's talk
[01:02:48.120 --> 01:02:53.360]   about our balls. With the Sultan of science. There's been a study
[01:02:53.360 --> 01:02:57.080]   on phthalates, our balls and plastic in our balls. Freeberg
[01:02:57.080 --> 01:02:57.600]   to this up.
[01:02:57.600 --> 01:03:02.040]   Yeah, the Consumer Reports put out a really interesting, or
[01:03:02.040 --> 01:03:04.960]   what has become pretty widely covered now story a couple weeks
[01:03:04.960 --> 01:03:08.640]   ago, where they measure phthalates in common foods. And
[01:03:08.640 --> 01:03:10.880]   Nick, if you want to just pull up the image, that's been
[01:03:10.880 --> 01:03:13.240]   repeated in a lot of media, a lot of press people were going
[01:03:13.240 --> 01:03:17.760]   nuts over this. phthalates are these chemical compounds that
[01:03:17.760 --> 01:03:22.000]   are used in plastics are used with plastics to soften them. So
[01:03:22.000 --> 01:03:24.080]   when you make plastics, you can kind of, you know, make them
[01:03:24.080 --> 01:03:26.680]   softer and form them into all sorts of different shapes and
[01:03:26.680 --> 01:03:29.400]   use them for different applications like plastic bags
[01:03:29.400 --> 01:03:33.440]   or wraps or cubes or all sorts of things. And phthalates are
[01:03:33.440 --> 01:03:37.280]   these kind of smaller molecules that that kind of go along with
[01:03:37.280 --> 01:03:40.640]   the polymers that that are the basis of the plastics. And they
[01:03:40.640 --> 01:03:44.280]   measured phthalates, which are known to be toxic, in terms of
[01:03:44.280 --> 01:03:46.680]   if you get enough of them, they can be carcinogenic and cause
[01:03:46.680 --> 01:03:51.760]   cancer. And they show that every product they tested had phthalates
[01:03:51.760 --> 01:03:56.560]   in it. Wendy's chicken nuggets had, you know, 33,000 nanograms
[01:03:56.560 --> 01:03:59.800]   per serving. If you scroll up to the top thing, wait, look at
[01:03:59.800 --> 01:04:03.600]   the chipotle chicken burrito. Oh my god. And just to be clear,
[01:04:03.600 --> 01:04:07.320]   guys, this is not just about packaging, packaging plays a
[01:04:07.320 --> 01:04:11.640]   role. But the whole food supply chain, the way we wrap food, all
[01:04:11.640 --> 01:04:15.400]   of our water, all of our dust, all of the air we breathe, we
[01:04:15.400 --> 01:04:18.840]   have measured phthalates in everything. So these phthalates
[01:04:18.840 --> 01:04:21.640]   end up in the animals that are used to make milk and the
[01:04:21.640 --> 01:04:24.040]   animals that people eat, they end up in the water that goes
[01:04:24.040 --> 01:04:27.280]   into the vegetables that we grow in the ground, they end up being
[01:04:27.280 --> 01:04:30.040]   used to make the little plastic jars that we feed our kids out
[01:04:30.040 --> 01:04:32.800]   of the little yogurt pouches that our kids drink out of
[01:04:32.800 --> 01:04:36.440]   everything all just to move food around in plastic packaging
[01:04:36.440 --> 01:04:39.600]   freeberg freeberg Why would the chicken nuggets from Wendy's
[01:04:39.600 --> 01:04:44.160]   though be so is it because they are eating? They are eating
[01:04:44.200 --> 01:04:46.920]   things that have plastics in plastics in them, but but we
[01:04:46.920 --> 01:04:49.760]   also don't know. And so we're eating the chicken. So we're
[01:04:49.760 --> 01:04:52.120]   eating the plastic, we're eating the plastic. And it's also the
[01:04:52.120 --> 01:04:54.080]   fact that the way that they process the chicken and the
[01:04:54.080 --> 01:04:56.520]   material that they use and the packaging that they use and how
[01:04:56.520 --> 01:04:59.240]   they move this stuff from one place to another, you got bags
[01:04:59.240 --> 01:05:01.720]   that are holding chicken breasts that then get put in the thing.
[01:05:01.720 --> 01:05:05.720]   Every and then the oil has has, you know, the oil is transported
[01:05:05.720 --> 01:05:08.160]   in plastic, we don't know. So it's plastic all the way down.
[01:05:08.160 --> 01:05:11.640]   It's guys look at this. I mean, these are the things like, look
[01:05:11.640 --> 01:05:15.360]   at Annie's I first of all, I really dislike Annie's labeling
[01:05:15.360 --> 01:05:18.120]   and patch kit packaging. I think it's very ugly. So I've never
[01:05:18.120 --> 01:05:21.160]   bought it for that reason. But I know that there's a lot of
[01:05:21.160 --> 01:05:24.360]   private equity moms that buy Annie's because it's supposed to
[01:05:24.360 --> 01:05:24.800]   be better.
[01:05:24.800 --> 01:05:29.800]   Keep going. This is a really good point. Yeah, I'm making a
[01:05:29.800 --> 01:05:31.880]   really good point. Go ahead tomorrow. It's really good. And
[01:05:31.880 --> 01:05:34.960]   so the problem is you see organic when you go like we
[01:05:34.960 --> 01:05:38.280]   again, we go to Drager's. That's typically where we go sometimes
[01:05:38.280 --> 01:05:40.440]   to go to Whole Foods, but we go to Drager's in Sagona. So that's
[01:05:40.440 --> 01:05:43.920]   the places we go to in our neighborhood. And when you go
[01:05:43.920 --> 01:05:46.080]   and you look at these things like prepared meals, as an
[01:05:46.080 --> 01:05:50.160]   example, the thing that has always attracted me to Annie's
[01:05:50.160 --> 01:05:53.440]   is because it is positioned as it is cleaner and better for
[01:05:53.440 --> 01:05:57.200]   you. And you see everybody buying it. And what you actually
[01:05:57.200 --> 01:06:00.920]   see sitting on the shelf that's left over is actually the Chef
[01:06:00.920 --> 01:06:04.680]   Boyardee and the Campbell's. And I always thought to myself, I
[01:06:04.680 --> 01:06:06.600]   won't buy Annie's because I actually don't like the label.
[01:06:06.600 --> 01:06:09.840]   To be honest, that was like, that was really why I did just
[01:06:09.840 --> 01:06:12.840]   deeply dislike the packaging. But it turns out it's the
[01:06:12.840 --> 01:06:16.840]   actual worst for you. Yeah, it is. And let me just let me tell
[01:06:16.840 --> 01:06:19.760]   you guys some stats about this. So we produce about 3 million
[01:06:19.760 --> 01:06:22.280]   tons of phthalates a year creating them that we use in our
[01:06:22.280 --> 01:06:25.200]   industrial supply chain. The global market for phthalates is
[01:06:25.200 --> 01:06:29.520]   about $10 billion per year, we find it everywhere in our tap
[01:06:29.520 --> 01:06:34.120]   water, as measured in the US in multiple places, there's about
[01:06:34.120 --> 01:06:37.120]   one microgram. So those were nanograms. So you kind of
[01:06:37.120 --> 01:06:40.240]   divide it by 1000. So that Annie's thing has 50 micrograms
[01:06:40.240 --> 01:06:43.600]   of phthalates in it, but there's about one microgram of phthalates
[01:06:43.600 --> 01:06:47.200]   per liter of water that you're drinking. Now, here's a study
[01:06:47.200 --> 01:06:49.480]   was done out of Germany, where they basically tried to
[01:06:49.480 --> 01:06:53.280]   estimate how much people were consuming. And on average,
[01:06:53.280 --> 01:06:57.400]   people consume or ingest about six micrograms of phthalates per
[01:06:57.400 --> 01:07:00.680]   kilogram of your body weight per day. So an adult males make is
[01:07:00.680 --> 01:07:04.720]   consuming about 500 micrograms of phthalates per day, that's
[01:07:04.720 --> 01:07:09.040]   half a gram per day. And the human body metabolizes and
[01:07:09.040 --> 01:07:12.120]   excretes it, it comes out the EPA, all of the administrative
[01:07:12.120 --> 01:07:15.080]   like agencies that oversee this stuff, they're like, it's okay,
[01:07:15.080 --> 01:07:18.400]   we metabolize it, as long as we don't consume more than we can
[01:07:18.400 --> 01:07:21.120]   metabolize, it's going to be safe, because it's not going to
[01:07:21.120 --> 01:07:23.640]   stay in our bodies, it's going to wash out. Here's the problem.
[01:07:23.640 --> 01:07:26.800]   While it's in your body, while it's moving through your body
[01:07:26.800 --> 01:07:29.320]   being metabolized, it is what's called an endocrine disruptor.
[01:07:29.320 --> 01:07:31.800]   And we talked about this in the past with respect to the
[01:07:31.800 --> 01:07:35.480]   sunscreens. These phthalates actually interfere with the
[01:07:35.480 --> 01:07:38.160]   hormones that are made by things like your pituitary gland, your
[01:07:38.160 --> 01:07:42.280]   thyroid, and even some of the hormones that are produced in
[01:07:42.280 --> 01:07:46.640]   testicle cells. There was another study done that really
[01:07:46.640 --> 01:07:50.760]   tried to estimate what the impact was. And here is a study
[01:07:50.760 --> 01:07:54.880]   that showed how do phthalates actually interact with different
[01:07:54.880 --> 01:07:58.440]   parts of the endocrine system. And they went through and they
[01:07:58.440 --> 01:08:02.840]   found all these places that biological hormones, and the
[01:08:02.840 --> 01:08:07.320]   endocrine system are disrupted by the phthalates. And we'll put
[01:08:07.320 --> 01:08:12.480]   credit for everyone that shared these papers here later. And
[01:08:12.480 --> 01:08:16.040]   they basically showed the mechanism by which the phthalates
[01:08:16.040 --> 01:08:21.080]   are actually disrupting endocrine systems. Now, what is
[01:08:21.080 --> 01:08:23.560]   the endocrine system, we talked about endocrine disruptors in
[01:08:23.560 --> 01:08:27.280]   the past, the endocrine system is the interaction of hormones
[01:08:27.280 --> 01:08:30.200]   with cells in your body produced by all these different glands in
[01:08:30.200 --> 01:08:32.720]   your body, like your thyroid, your pituitary gland, and so on,
[01:08:32.720 --> 01:08:36.560]   control things like growth, tissue development, reproductive
[01:08:36.560 --> 01:08:40.240]   tissue activity, like making sperm cells, autonomic function,
[01:08:40.240 --> 01:08:43.120]   like body temperature, blood pressure, sleep, heart rate
[01:08:43.120 --> 01:08:46.320]   regulation, injury and stress response, your mood, all of
[01:08:46.320 --> 01:08:48.920]   those things are regulated by your endocrine system. And so
[01:08:48.920 --> 01:08:51.360]   when the hormones or the proteins or peptides that are
[01:08:51.360 --> 01:08:54.680]   made by those glands are disrupted by these phthalates,
[01:08:54.680 --> 01:08:57.400]   it can actually disrupt those systems and mess them up. So
[01:08:57.400 --> 01:09:00.520]   while we're not consuming, generally speaking, enough
[01:09:00.520 --> 01:09:04.400]   phthalates to cause cancer, and therefore, we all say, hey, it's
[01:09:04.400 --> 01:09:06.320]   okay, these phthalates aren't that bad, we're not going to all
[01:09:06.320 --> 01:09:10.600]   die from cancer. The truth is, there is demonstrations now on
[01:09:10.600 --> 01:09:13.200]   how they can actually disrupt the activity of your endocrine
[01:09:13.200 --> 01:09:15.840]   system. And as a result, have all of these deleterious
[01:09:15.840 --> 01:09:20.440]   effects. Another study done on 125 men out of China, this paper
[01:09:20.440 --> 01:09:26.160]   was done out of China, they saw damage to testicle cells that
[01:09:26.160 --> 01:09:30.160]   would die testicle cells that would produce fewer sperm, and
[01:09:30.160 --> 01:09:32.600]   then testicle cells that produced sperm with extra
[01:09:32.600 --> 01:09:36.240]   nuclei, and they actually demonstrated this in rats. So
[01:09:36.240 --> 01:09:40.200]   the set of compounds can be fairly disruptive. So now we'll
[01:09:40.200 --> 01:09:42.600]   go to the next story. And the next story is the one that
[01:09:42.600 --> 01:09:44.400]   everyone's writing about, which is oh my god, there's plastic
[01:09:44.400 --> 01:09:47.800]   and balls. So a team at University of New Mexico that
[01:09:47.800 --> 01:09:51.240]   was published in the Journal of toxicological sciences, just
[01:09:51.240 --> 01:09:55.240]   last week, they took 47 neutered dogs testicles from a local pet
[01:09:55.240 --> 01:09:57.800]   clinic where they were getting neutered. And they found on
[01:09:57.800 --> 01:10:03.000]   average 128 micrograms per gram of microplastics in those
[01:10:03.000 --> 01:10:06.800]   testicles. And it was mostly you know, polyvinyl chloride or one
[01:10:06.800 --> 01:10:10.920]   of the main plastics and polyethylene. And again, phthalates
[01:10:10.920 --> 01:10:14.320]   leach out of these plastics and leach into the cells. And then
[01:10:14.320 --> 01:10:16.720]   they went to the medical investigators office and they
[01:10:16.720 --> 01:10:20.480]   found these, the testicles that were frozen for seven years,
[01:10:20.480 --> 01:10:22.520]   because when they do a medical investigation, and they keep all
[01:10:22.520 --> 01:10:24.760]   the body parts, they keep them on ice, and then they throw them
[01:10:24.760 --> 01:10:26.520]   away after seven years. So before they throw them away,
[01:10:26.520 --> 01:10:29.280]   they got permission of humans, they got permission to use these
[01:10:29.280 --> 01:10:32.960]   testicles to figure out are there plastics and they measure
[01:10:32.960 --> 01:10:36.600]   in the frozen balls, and the frozen balls. These are ancient
[01:10:36.600 --> 01:10:39.880]   frozen balls. How old are these balls? 23 frozen balls about
[01:10:39.880 --> 01:10:43.000]   seven years old. Yeah, seven year old balls frozen. Are
[01:10:43.000 --> 01:10:46.200]   people donating their balls to science? Is that when there's
[01:10:46.200 --> 01:10:50.040]   like a homicide or someone and or you don't know who dies or
[01:10:50.040 --> 01:10:53.560]   there's an investigation into why someone died the corner the
[01:10:53.560 --> 01:10:57.680]   corner keeps the body parts in case it's needed for a like a
[01:10:57.680 --> 01:10:58.640]   police case later.
[01:10:58.640 --> 01:11:01.960]   On the record, I don't want my balls used that way.
[01:11:01.960 --> 01:11:06.080]   You don't have the frozen balls on your driver's license like
[01:11:06.080 --> 01:11:06.720]   freeze my balls.
[01:11:06.720 --> 01:11:10.680]   They asked me to store my balls. I think you got such huge balls.
[01:11:10.680 --> 01:11:14.800]   Anyway, they got 2323 these these balls from these bodies
[01:11:14.840 --> 01:11:21.000]   and they found plastics on average 328 micrograms per gram
[01:11:21.000 --> 01:11:25.200]   of of testicle in these balls.
[01:11:25.200 --> 01:11:27.720]   What do you usually find? Freeberg? What do you usually
[01:11:27.720 --> 01:11:28.520]   find in these balls?
[01:11:28.520 --> 01:11:31.000]   Well, we don't we don't know because we've never taken human
[01:11:31.000 --> 01:11:33.680]   tissue and try to take it apart in a very detailed way to figure
[01:11:33.680 --> 01:11:36.440]   out like, how much plastic is there? You know, what is it
[01:11:36.440 --> 01:11:39.120]   doing to our body? But now I just want to connect the dots.
[01:11:39.120 --> 01:11:41.520]   So now we have a sense that there's these phthalates and
[01:11:41.520 --> 01:11:43.800]   these other compounds that come with plastics that leak in that
[01:11:43.800 --> 01:11:46.240]   cause all this disruption. Separately, we're seeing this
[01:11:46.240 --> 01:11:49.720]   accumulation of these little plastic particles. And remember,
[01:11:49.720 --> 01:11:53.820]   plastics are polymers, they're long chains of monomers. And so
[01:11:53.820 --> 01:11:55.840]   they can be short chains, they can be long, so they break
[01:11:55.840 --> 01:11:58.440]   apart, break apart, break apart, and little tiny bits of them end
[01:11:58.440 --> 01:12:01.000]   up and they're very hard to metabolize. And they sit in your
[01:12:01.000 --> 01:12:05.040]   tissue, and then they can cause all this disruption. So, you
[01:12:05.040 --> 01:12:07.800]   know, I think these are like, generally just going to say it.
[01:12:07.800 --> 01:12:09.800]   I'm just gonna say, first of all, I really appreciate that
[01:12:09.800 --> 01:12:13.080]   you did this. I think it's so important. We talked about
[01:12:13.080 --> 01:12:16.480]   microplastics a little bit ago, you know, J. Cal moved his whole
[01:12:16.480 --> 01:12:20.000]   family away, because a few years ago from plastics, I've started
[01:12:20.000 --> 01:12:22.920]   to do it four or five months ago, but you can't get away from
[01:12:22.920 --> 01:12:26.120]   it. No, and everywhere, you know, supply chain is your
[01:12:26.120 --> 01:12:29.400]   point. It's in the water, it's in the air, it's everywhere. I
[01:12:29.400 --> 01:12:32.240]   was gonna say, I think our food supply, I think we should just
[01:12:32.240 --> 01:12:36.440]   say it out loud, is totally corrupted. And I think there's
[01:12:36.440 --> 01:12:40.200]   all of these other factors we look at the rise in the use of
[01:12:40.200 --> 01:12:45.680]   SSRIs, the lack of sexual function in young men, the lack
[01:12:45.680 --> 01:12:50.640]   of sex, the low birth rate, I think these are all related. And
[01:12:50.640 --> 01:12:53.880]   part of it is the food supply. And part of the food supply
[01:12:53.880 --> 01:12:57.520]   problem is the fact that it is corrupted by these materials
[01:12:57.520 --> 01:12:58.920]   that should never be in our body.
[01:12:58.920 --> 01:13:01.440]   I want to just push back on this, because I think it's a
[01:13:01.440 --> 01:13:04.520]   guess. But I honestly think it's a truth. I don't want to limit
[01:13:04.520 --> 01:13:06.760]   it. I don't want to limit it to the food supply. Because here's
[01:13:06.760 --> 01:13:09.560]   the other thing. All of us are wearing clothes that use
[01:13:09.560 --> 01:13:13.280]   polymers, which are plastics. All of us are sitting at desks
[01:13:13.280 --> 01:13:16.120]   that have coatings of polymers on them. All of us have iPhones
[01:13:16.120 --> 01:13:21.360]   that use polymers, all of us drive cars and the rubber, one
[01:13:21.360 --> 01:13:24.360]   of the ways that they found that plastic, these microparticles
[01:13:24.360 --> 01:13:27.360]   are getting in the air is through tires. When we drive
[01:13:27.360 --> 01:13:30.240]   little particulates end up in the atmosphere, we breathe them
[01:13:30.240 --> 01:13:33.160]   in, and then they end up in our body. Every part of our
[01:13:33.160 --> 01:13:36.520]   industrial supply chain uses polymers, every part of our
[01:13:36.520 --> 01:13:39.640]   industrial why do you but the concentration, I'm going to guess
[01:13:39.640 --> 01:13:42.760]   that the concentration when you actually put it in your body,
[01:13:42.760 --> 01:13:47.520]   and then your intestines and your organs are bathed in this
[01:13:47.520 --> 01:13:52.680]   stuff. I'm going to guess that the food supply has a huge part
[01:13:52.680 --> 01:13:54.760]   to do with this. Yeah, but when you're let's say you take let's
[01:13:54.760 --> 01:13:57.880]   say you're wearing a clothing, almost all of our clothes. Now
[01:13:57.880 --> 01:14:00.280]   many of our clothes have polymers in them. We put it in
[01:14:00.280 --> 01:14:02.480]   the washing machine, it ends up in the water supply chain, we
[01:14:02.480 --> 01:14:05.880]   consume that water. It's very hard to say that there's a
[01:14:05.880 --> 01:14:10.640]   specific action. Our whole system has been inundated with
[01:14:10.640 --> 01:14:11.240]   these lower
[01:14:11.240 --> 01:14:14.000]   let me say it in a way that maybe you will agree with them.
[01:14:14.000 --> 01:14:17.880]   We need to fix something. My starting point would be the food
[01:14:17.880 --> 01:14:22.000]   supply. Yeah, where do you start? Yeah, my big my big
[01:14:22.000 --> 01:14:24.640]   takeaways is sort of like yours, which is almost impossible to
[01:14:24.640 --> 01:14:27.000]   alter this industry overnight, given how ubiquitous these
[01:14:27.000 --> 01:14:30.480]   compounds are and everything we do and touch tires, phones,
[01:14:30.480 --> 01:14:34.280]   clothing, etc. But I think that this is going to trigger and is
[01:14:34.280 --> 01:14:37.800]   the beginning of a wave, I'm noticing that a lot of folks are
[01:14:37.800 --> 01:14:41.640]   going to start to pay attention in the food industry, and start
[01:14:41.640 --> 01:14:46.680]   to figure out ways to represent low plastic, low phthalate food
[01:14:46.680 --> 01:14:49.520]   products as a way to kind of sell a more premium solution. I
[01:14:49.520 --> 01:14:51.560]   think that's been the trend historically with the food
[01:14:51.560 --> 01:14:55.400]   industry. Chamath is to respond to your ask right now, and to
[01:14:55.400 --> 01:14:58.360]   then show up with with solutions. So I do think that
[01:14:58.360 --> 01:15:03.240]   that's and just just taking a step back. We don't have to use
[01:15:03.760 --> 01:15:06.560]   these, these are all based on fossil fuels. So the way we make
[01:15:06.560 --> 01:15:09.200]   plastics is we basically pull oil out of the ground, and we
[01:15:09.200 --> 01:15:11.520]   turn it into these polymers. That's the basis of this
[01:15:11.520 --> 01:15:15.600]   chemical industry. We don't have to do that. With the same
[01:15:15.600 --> 01:15:17.760]   function, we can get the same function from what are called
[01:15:17.760 --> 01:15:20.800]   bioplastics. So these can these are compounds that can actually
[01:15:20.800 --> 01:15:23.800]   be much more biodegradable that are made with biological
[01:15:23.800 --> 01:15:27.640]   systems, and not made from oil using synthetic chemicals
[01:15:27.640 --> 01:15:31.400]   systems. So I do think that there's a really big
[01:15:31.400 --> 01:15:35.400]   opportunity for a wave of bioplastic alternatives, given
[01:15:35.400 --> 01:15:38.680]   that this is now becoming a little bit more obvious to
[01:15:38.680 --> 01:15:41.960]   folks that there is this kind of systemic problem, that this is
[01:15:41.960 --> 01:15:43.960]   ubiquitous, and that we do need to kind of address it.
[01:15:43.960 --> 01:15:47.320]   Okay, so just zooming out here for a minute, talked about the
[01:15:47.320 --> 01:15:52.240]   phthalates. But what about the bofas? Freeberg? The study on
[01:15:52.240 --> 01:15:52.840]   the bofas?
[01:15:53.720 --> 01:15:56.000]   Let me play in what are the bofas, Jason?
[01:15:56.000 --> 01:16:13.760]   Like what is bofas?
[01:16:13.760 --> 01:16:20.120]   That's awesome.
[01:16:20.440 --> 01:16:23.440]   Why not just start laughing before you deliver the joke next
[01:16:23.440 --> 01:16:23.760]   time?
[01:16:23.760 --> 01:16:29.600]   Let's get serious here for a second.
[01:16:29.600 --> 01:16:32.480]   I have a real question. Part of the thing that I think is broken
[01:16:32.480 --> 01:16:36.120]   is that I think somewhere along the way we got screwed up in how
[01:16:36.120 --> 01:16:40.160]   food is labeled. Right. And we and then it was, it was gamed
[01:16:40.160 --> 01:16:42.760]   effectively, right. So for for years, even when I was growing
[01:16:42.760 --> 01:16:45.800]   up, I thought you should not buy food that had that was high in
[01:16:45.800 --> 01:16:49.360]   fat, as an example. And little did I know I was ingesting all
[01:16:49.360 --> 01:16:52.520]   these sugars as a substitute to fat, it was a total mistake.
[01:16:52.520 --> 01:16:56.480]   Does the labeling need to become simpler and focus on these
[01:16:56.480 --> 01:16:59.440]   things that are just fundamentally carcinogenic for
[01:16:59.440 --> 01:17:03.280]   us? One, two, are there like lawsuits that need to happen
[01:17:03.280 --> 01:17:07.600]   Allah cigarettes where you kind of connect the dots between
[01:17:07.600 --> 01:17:11.560]   these phthalates and, and a bunch of these diseases? Because
[01:17:11.560 --> 01:17:14.120]   it just seems like I think are people have thrown their hands
[01:17:14.120 --> 01:17:17.680]   in the air for years, right? Some people say autism and diet
[01:17:17.720 --> 01:17:20.920]   are correlated, right? Other people. So they're, you know,
[01:17:20.920 --> 01:17:23.480]   frowns, the rise of Crohn's, there's so many of these
[01:17:23.480 --> 01:17:28.040]   conditions, that there is a cohort of people that attribute
[01:17:28.040 --> 01:17:32.360]   most of the reason what the pathology of the disease exists
[01:17:32.360 --> 01:17:34.200]   to food. So what do we do?
[01:17:34.200 --> 01:17:36.400]   We're pesticides, right? And that was in there, too.
[01:17:36.400 --> 01:17:38.760]   Is that your actual office behind you? Or is that a
[01:17:38.760 --> 01:17:39.160]   background?
[01:17:39.160 --> 01:17:44.600]   That's my office? Yeah. So like, every one of those books is
[01:17:45.120 --> 01:17:48.360]   coded in some of these compounds. Now, these are
[01:17:48.360 --> 01:17:51.960]   phthalate free, because these are from the 1400s. They didn't
[01:17:51.960 --> 01:17:52.560]   have that back then.
[01:17:52.560 --> 01:17:56.000]   Okay, good. All right. Well, everything else at the desk is
[01:17:56.000 --> 01:17:59.360]   made of all of those items. Yeah. Yeah. These are
[01:17:59.360 --> 01:18:01.640]   collector's items, bro. These are these are from an era where
[01:18:01.640 --> 01:18:04.360]   that's up. And you're, you're, you're pure, I'm assuming you're
[01:18:04.360 --> 01:18:08.440]   you're wearing a pure baby wool sweater, which doesn't have any
[01:18:08.440 --> 01:18:11.880]   polymers, but baby cash. Yeah. But I think what's what what's
[01:18:11.920 --> 01:18:15.920]   what's overwhelming about right? I think I think baby Catherine
[01:18:15.920 --> 01:18:16.840]   is biodegradable.
[01:18:16.840 --> 01:18:20.640]   What's overwhelming about this problem is the ubiquity of the
[01:18:20.640 --> 01:18:24.200]   problem. It's almost like asking, tell me everywhere that
[01:18:24.200 --> 01:18:27.280]   carbon is used. Like imagine if you had to label every
[01:18:27.280 --> 01:18:30.520]   No, I know. I'm trying to hone you into this one area that I
[01:18:30.520 --> 01:18:34.000]   actually I get the tires and the this and that. I'm trying to get
[01:18:34.000 --> 01:18:36.200]   to something that I fundamentally care about. I have
[01:18:36.200 --> 01:18:40.320]   young children, I feed them food every day. I don't trust my food
[01:18:40.320 --> 01:18:43.880]   supply. I've never really trusted it. And this kind of
[01:18:43.880 --> 01:18:47.640]   stuff adds to this body of evidence where I'm worried that
[01:18:47.640 --> 01:18:51.720]   if my kids go through some kind of an issue, at the core of it
[01:18:51.720 --> 01:18:54.760]   will actually be something dietary. And it's typically
[01:18:54.760 --> 01:18:57.720]   overlooked by modern medicine. Because you'll treat it
[01:18:57.720 --> 01:19:01.560]   symptomologically, you'll try to give it some kind of pill. It's
[01:19:01.560 --> 01:19:05.480]   not how you treat a lot of these things. It could turn out where
[01:19:05.480 --> 01:19:08.480]   I think restructuring someone's diet can actually have an
[01:19:08.480 --> 01:19:10.960]   enormous impact. So I'm just trying to figure out what is
[01:19:10.960 --> 01:19:15.000]   something that we can all start to do to get a handle on this
[01:19:15.000 --> 01:19:17.280]   because you're putting food in your body every day.
[01:19:17.280 --> 01:19:21.600]   Yeah, it seems like you're saying it's, it's helpless here,
[01:19:21.600 --> 01:19:23.800]   free bird, there's nothing we can do. And I think what your
[01:19:23.800 --> 01:19:26.440]   moth and I are saying, asking you is like, where do we start?
[01:19:26.440 --> 01:19:29.200]   How can we start to get off of plastics,
[01:19:29.200 --> 01:19:32.880]   I think we got to go into the source. So biopolymers are made
[01:19:32.880 --> 01:19:37.240]   by living organisms. They're, they're typically longer chains
[01:19:37.280 --> 01:19:41.560]   of what are more like sugar molecules. And they can be used
[01:19:41.560 --> 01:19:44.200]   in a similar way that we're, they're not going to be as good
[01:19:44.200 --> 01:19:47.480]   as synthetic polymers that we use today. So a lot of our
[01:19:47.480 --> 01:19:49.360]   applications, a lot of our industry would have to be
[01:19:49.360 --> 01:19:52.520]   rebuilt, if we really wanted to go back to redesign the whole
[01:19:52.520 --> 01:19:55.200]   system. But we got to redesign the whole system tomorrow. We're
[01:19:55.200 --> 01:19:57.760]   making everything out of these products, because they're cheap.
[01:19:57.760 --> 01:20:00.240]   And because we can pull oil out of the ground and turn it into
[01:20:00.240 --> 01:20:02.960]   cheap stuff. And then it makes things affordable for everyone
[01:20:02.960 --> 01:20:05.680]   on earth. And that's how this industry emerged. You know, it
[01:20:05.680 --> 01:20:08.480]   was not like some, someone randomly came along and said,
[01:20:08.480 --> 01:20:10.400]   let's put plastics and everything, because it's going
[01:20:10.400 --> 01:20:13.080]   to be good for people. It was a way to make products more
[01:20:13.080 --> 01:20:14.920]   accessible and more available and cheaper, and it's
[01:20:14.920 --> 01:20:17.880]   everywhere. And so I think there's this real question of
[01:20:17.880 --> 01:20:22.840]   like, what industrial synthetic chemistries do we use today as a
[01:20:22.840 --> 01:20:26.240]   species that we should rethink using and start at that level
[01:20:26.240 --> 01:20:29.280]   and then rebuild from there. And I think shining a light on this
[01:20:29.280 --> 01:20:31.320]   stuff and just talking about what these products are, and I
[01:20:31.320 --> 01:20:34.480]   think I think that's, I think that's a lot of laudatory, but
[01:20:34.480 --> 01:20:36.920]   too complicated. I want something simpler, which is like,
[01:20:36.920 --> 01:20:40.920]   can we get a law passed so that chickens cannot eat certain
[01:20:40.920 --> 01:20:43.200]   kinds of food that are known to be high in phthalates?
[01:20:43.200 --> 01:20:45.880]   Yeah. And here's an idea. Look at this trim off, like, look at
[01:20:45.880 --> 01:20:49.960]   this banana. Like, I just, as a newsflash, a banana already has
[01:20:49.960 --> 01:20:53.080]   a wrapper called the peel. And then people are wrapping
[01:20:53.080 --> 01:20:56.720]   plastics on this stuff. Like I think consumers need to demand
[01:20:56.720 --> 01:20:59.520]   that, like, why did you take a picture of a banana? Why did you
[01:20:59.520 --> 01:21:01.680]   take a picture of a banana? I found that on the internet. I
[01:21:01.680 --> 01:21:03.880]   didn't actually take it myself. Okay. I thought you were like,
[01:21:04.040 --> 01:21:04.920]   sitting at the store.
[01:21:04.920 --> 01:21:09.040]   You know, when you see this kind of packaging, this is what has
[01:21:09.040 --> 01:21:13.040]   made me nuts in my life is all this crazy packaging going on.
[01:21:13.040 --> 01:21:16.960]   And in Europe, you are required at the supermarket. And people
[01:21:16.960 --> 01:21:19.200]   do this when they get to the end of the counter, they take the
[01:21:19.200 --> 01:21:21.800]   packaging off the supermarket has to take the packaging,
[01:21:21.800 --> 01:21:24.400]   right, so they have to bear the burden of it. So if you get a
[01:21:24.400 --> 01:21:26.760]   tube of toothpaste, you can take the packaging off and hand them
[01:21:26.760 --> 01:21:29.720]   the thing. Other places are now saying, hey, if you're coming
[01:21:29.720 --> 01:21:32.960]   for peanut butter, or grains or flour or sugar, they have a
[01:21:32.960 --> 01:21:37.120]   barrel of sugar, they put it in a brown bag. And you get this
[01:21:37.120 --> 01:21:40.400]   like more clean experience. I think we have to have like both
[01:21:40.400 --> 01:21:43.240]   ends of this the supply chain. There's also consumers. There's
[01:21:43.240 --> 01:21:44.440]   a there's like a marketing
[01:21:44.440 --> 01:21:47.280]   on my bananas to come with packaging on them. You do you
[01:21:47.280 --> 01:21:49.080]   don't want anyone else's fingerprints. I don't want
[01:21:49.080 --> 01:21:49.960]   anyone's fingerprints.
[01:21:49.960 --> 01:21:54.440]   Brother, you don't eat the peel.
[01:21:54.440 --> 01:21:56.320]   Yeah, but I could touch it.
[01:21:56.320 --> 01:21:58.720]   I haven't we haven't bought water bottles.
[01:21:58.840 --> 01:22:02.960]   I want everything to come in hermetically sealed plastic.
[01:22:02.960 --> 01:22:04.920]   Yeah. But then how many people in your house handle your
[01:22:04.920 --> 01:22:06.080]   banana? Pause?
[01:22:06.080 --> 01:22:10.800]   Whoa, whoa. How many people in his house handled his banana?
[01:22:10.800 --> 01:22:14.760]   No, did he know? But the real the real issue is not that it's
[01:22:14.760 --> 01:22:18.560]   like when the girls in your family have puberty younger and
[01:22:18.560 --> 01:22:21.480]   younger. And you're like, why is that happening? Or inconsistent
[01:22:21.480 --> 01:22:24.480]   periods? Or when the boys go through these weird, you know,
[01:22:24.480 --> 01:22:27.240]   moments where they're like, not really growing tick tock.
[01:22:28.000 --> 01:22:29.520]   No, I'm just telling you, like,
[01:22:29.520 --> 01:22:33.040]   wait, there's so many things that we have to panic about. It's
[01:22:33.040 --> 01:22:36.640]   hard to to your point. It's hard to attribute. Yeah. To one.
[01:22:36.640 --> 01:22:38.960]   Yeah. But I think I think that I think the thing that everybody
[01:22:38.960 --> 01:22:42.240]   could get or get focused on is how can you correct at least the
[01:22:42.240 --> 01:22:45.280]   marketing versus the reality in our food supply? You know, in a
[01:22:45.280 --> 01:22:47.960]   different example, I remember not telling me something which
[01:22:47.960 --> 01:22:51.360]   was along the lines of like hormone free is something that's
[01:22:51.360 --> 01:22:54.680]   marketed, but like chickens have been hormone free since like the
[01:22:54.680 --> 01:22:58.720]   50s. But it's like, there's some latent hormones left inside of
[01:22:58.720 --> 01:23:03.200]   them. And then some of the feed is really poorly constructed. And
[01:23:03.200 --> 01:23:06.880]   you should be focused on like air chilled versus water chilled
[01:23:06.880 --> 01:23:11.960]   or whatever. There's just so much bullshit out there. And I
[01:23:11.960 --> 01:23:14.320]   so I think it's hard if you're like trying to take care of your
[01:23:14.320 --> 01:23:16.720]   family. It makes sense of it all. Yeah, makes sense of it all.
[01:23:16.720 --> 01:23:19.280]   It's just like, everyone, everyone feels helpless. And
[01:23:19.280 --> 01:23:22.320]   everyone wants to cry. I find it super frustrating, because it's
[01:23:22.320 --> 01:23:25.920]   something that I really care about my like what I eat. Right,
[01:23:25.920 --> 01:23:29.560]   totally. And it came from a place where a lot of disease in
[01:23:29.560 --> 01:23:33.080]   my family, and I was overweight when I was young. And so I just
[01:23:33.080 --> 01:23:35.720]   want to kind of like toe of No, I mean, you eat and it's
[01:23:35.720 --> 01:23:39.520]   impossible. You if you're eating vegetables, but my my takeaway
[01:23:39.520 --> 01:23:44.680]   is, there's going to be a lot of phthalates in my balls. Yes,
[01:23:44.680 --> 01:23:48.760]   absolutely. All the stuff that I do. I'm no better off than
[01:23:48.760 --> 01:23:51.480]   somebody eating at Wendy's in the end of the day. And I feel
[01:23:51.480 --> 01:23:55.040]   like, well, what is all that time and expense and difficulty?
[01:23:55.040 --> 01:23:58.160]   Is it's not worth it? Well, there's other health benefits
[01:23:58.160 --> 01:24:00.880]   to it, of course. And there's environmental benefits. But you
[01:24:00.880 --> 01:24:04.920]   know, we went all glass bottles, as I told you. And, you know,
[01:24:04.920 --> 01:24:07.080]   then I find out that some of the cans we have, because it's a
[01:24:07.080 --> 01:24:10.880]   couple of things we like certain natural sodas, they got plastic
[01:24:10.880 --> 01:24:12.920]   on the inside of the aluminum plastic on the inside. Exactly.
[01:24:12.920 --> 01:24:14.480]   I'm like, I thought I was doing the right thing here by going
[01:24:14.480 --> 01:24:20.200]   aluminum. Exactly. So the moral of the story is don't try to do
[01:24:20.200 --> 01:24:25.680]   the right thing. But anyway, I just want to I think this stuff
[01:24:25.680 --> 01:24:30.080]   is unavoidable. I really do. That's why I think I think I
[01:24:30.080 --> 01:24:33.960]   think you're right. And I think that's why you see all of these
[01:24:33.960 --> 01:24:37.560]   kinds of diseases, these chronic and acute conditions just
[01:24:37.560 --> 01:24:39.520]   ticking up tick, tick, tick, tick, tick, tick.
[01:24:39.520 --> 01:24:42.840]   It's well, anyway, I think this was a fascinating science
[01:24:42.840 --> 01:24:48.360]   corner. And I took a screenshot of sacks during it. This is
[01:24:48.360 --> 01:24:50.560]   access interest level, you can always tell how good it is.
[01:24:50.560 --> 01:24:57.600]   I thought this was a science corner I could finally use, you
[01:24:57.600 --> 01:24:57.680]   know,
[01:24:57.680 --> 01:25:02.040]   absolutely. They actually checked saxes balls for the
[01:25:02.040 --> 01:25:05.960]   plastics and all they found were steel. So there it is. Yeah,
[01:25:05.960 --> 01:25:08.680]   it's just brass balls going around the horn here. What's
[01:25:08.680 --> 01:25:12.640]   your favorite balls in pop culture? For me? It's got to be
[01:25:12.640 --> 01:25:16.560]   idiocracy. I love Have you guys seen idiocracy and Mike judges
[01:25:16.560 --> 01:25:16.880]   film?
[01:25:17.640 --> 01:25:18.440]   I haven't seen it.
[01:25:18.440 --> 01:25:22.720]   Okay, so in the film, I'll just keep this up. Society has gone
[01:25:22.720 --> 01:25:26.040]   to the lowest possible IQ, everybody's got an 80 IQ. And
[01:25:26.040 --> 01:25:29.880]   like people, like a reality TV star is running the country into
[01:25:29.880 --> 01:25:34.960]   the ground. That's what he has in idiocracy. And the number one
[01:25:34.960 --> 01:25:39.880]   television show is essentially a tick tock called Ouch, my balls.
[01:25:39.880 --> 01:25:40.680]   Here it is.
[01:25:40.680 --> 01:26:03.320]   It's basically the number one television show in this society,
[01:26:03.320 --> 01:26:09.160]   this dystopian society, where all the crops have died, and
[01:26:09.160 --> 01:26:12.720]   they don't know how to make crops anymore, is ouch, my
[01:26:12.720 --> 01:26:15.720]   balls. It's just a super cut of a guy getting kicked in the
[01:26:15.720 --> 01:26:18.320]   nuts. Sacks, what's your favorite ball moment in pop
[01:26:18.320 --> 01:26:18.680]   culture?
[01:26:18.680 --> 01:26:21.840]   Gregory Glenn Ross.
[01:26:21.840 --> 01:26:23.040]   All right, here it is, folks.
[01:26:23.040 --> 01:26:26.280]   That's Alec Baldwin, yeah.
[01:26:26.280 --> 01:26:28.960]   Freeberg, you got a favorite ball clip from pop culture for
[01:26:28.960 --> 01:26:33.120]   yourself? That tickles you? No. Tremont, you got one?
[01:26:33.120 --> 01:26:34.480]   I'm gonna find one.
[01:26:34.760 --> 01:26:44.640]   Yeah. Okay, everybody, this has been a spectacular episode of
[01:26:44.640 --> 01:26:47.560]   the world's number one podcast. It's episode 180 of the All In
[01:26:47.560 --> 01:26:53.680]   podcast. With you again, for the sultan of science, David
[01:26:53.680 --> 01:26:56.600]   Freeberg. David Sacks.
[01:26:56.600 --> 01:27:02.800]   There's a compilation of YouTube on YouTube of all these Austin
[01:27:02.800 --> 01:27:05.320]   Powers moments of Austin Powers getting kicked in the balls.
[01:27:05.320 --> 01:27:08.640]   Yeah, bad news. And we'll see you all at the All In Summit in
[01:27:08.640 --> 01:27:11.680]   September. Bye bye.
[01:27:11.680 --> 01:27:13.320]   Love you, Bruce.
[01:27:13.840 --> 01:27:14.320]   Bye bye.
[01:27:14.320 --> 01:27:20.120]   All In. We'll let your winners ride. Rain Man, David Sacks.
[01:27:20.120 --> 01:27:26.120]   And it said we open source it to the fans and they've just gone
[01:27:26.120 --> 01:27:27.840]   crazy with it. Love you, Wesley.
[01:27:27.840 --> 01:27:29.240]   The queen of quinoa.
[01:27:29.240 --> 01:27:33.200]   I'm going all in. Let your winners ride. Let your winners
[01:27:33.200 --> 01:27:37.240]   ride. Besties are gone.
[01:27:37.240 --> 01:27:41.240]   That is my dog taking a notice in your driveway, Sacks.
[01:27:41.240 --> 01:27:44.520]   Wait a minute. Oh man.
[01:27:44.520 --> 01:27:46.480]   My avatars will meet me at the All In Summit.
[01:27:46.480 --> 01:27:49.120]   We should all just get a room and just have one big huge orgy
[01:27:49.120 --> 01:27:50.960]   because they're all just useless. It's like this like
[01:27:50.960 --> 01:27:53.240]   sexual tension that they just need to release somehow.
[01:27:53.240 --> 01:27:57.920]   Wet your beep. Wet your beep.
[01:27:57.920 --> 01:28:00.840]   We need to get merch.
[01:28:00.840 --> 01:28:01.440]   Besties are gone.
[01:28:01.440 --> 01:28:03.000]   I'm going all in.
[01:28:03.000 --> 01:28:10.960]   I'm going all in.
[01:28:11.280 --> 01:28:15.320]   And now the plugs the All In Summit is taking place in Los
[01:28:15.320 --> 01:28:17.520]   Angeles on September 8th through the 10th.
[01:28:17.520 --> 01:28:22.000]   You can apply for a ticket at summit.allinpodcast.co.
[01:28:22.000 --> 01:28:24.440]   Scholarships will be coming soon.
[01:28:24.440 --> 01:28:27.840]   You can actually see the video of this podcast on YouTube,
[01:28:27.840 --> 01:28:32.840]   youtube.com/atallin or just search All In Podcast and hit
[01:28:32.840 --> 01:28:36.480]   the alert bell and you'll get updates when we post and we're
[01:28:36.480 --> 01:28:38.600]   going to do a party in Vegas.
[01:28:38.680 --> 01:28:41.560]   My understanding when we hit a million subscribers so look for
[01:28:41.560 --> 01:28:42.320]   that as well.
[01:28:42.320 --> 01:28:48.040]   You can follow us on x x.com/theallinpod TikTok is all
[01:28:48.040 --> 01:28:51.400]   underscore in underscore talk, Instagram, the All In Pod.
[01:28:51.400 --> 01:28:54.720]   And on LinkedIn, just search for the All In Podcast.
[01:28:54.720 --> 01:28:58.480]   You can follow Chamath at x.com/chamath and you can sign up
[01:28:58.480 --> 01:29:01.520]   for a substack at chamath.substack.com I do.
[01:29:01.520 --> 01:29:05.000]   Freeberg can be followed at x.com/freeberg and O'Halo is
[01:29:05.000 --> 01:29:09.520]   hiring click on the careers page at Ohalo genetics.com and you
[01:29:09.520 --> 01:29:13.240]   can follow sacks at x.com slash David sacks sacks recently spoke
[01:29:13.240 --> 01:29:16.000]   at the American moment conference and people are going
[01:29:16.000 --> 01:29:16.640]   crazy for it.
[01:29:16.640 --> 01:29:18.600]   It's pinned to his tweet on his ex profile.
[01:29:18.600 --> 01:29:20.120]   I'm Jason Calacanis.
[01:29:20.120 --> 01:29:23.640]   I am x.com slash Jason and if you want to see pictures of my
[01:29:23.640 --> 01:29:26.800]   bulldogs and the food I'm eating, go to instagram.com
[01:29:26.800 --> 01:29:29.200]   slash Jason in the first name club.
[01:29:29.200 --> 01:29:32.160]   You can listen to my other podcasts this week in startups
[01:29:32.160 --> 01:29:34.520]   just search for it on YouTube or your favorite podcast player.
[01:29:34.560 --> 01:29:38.520]   We are hiring a researcher apply to be a researcher doing
[01:29:38.520 --> 01:29:41.040]   primary research and working with me and producer Nick
[01:29:41.040 --> 01:29:44.560]   working in data and science and being able to do great research
[01:29:44.560 --> 01:29:45.640]   finance, etc.
[01:29:45.640 --> 01:29:48.320]   All in podcast.co slash research.
[01:29:48.320 --> 01:29:52.040]   It's a full time job working with us the besties and really
[01:29:52.040 --> 01:29:54.760]   excited about my investment in Athena go to Athena.
[01:29:54.760 --> 01:29:55.360]   Wow.
[01:29:55.360 --> 01:29:59.920]   Seeing a wow.com and get yourself a bit of a discount from
[01:29:59.920 --> 01:30:01.040]   your boy j cow.
[01:30:01.440 --> 01:30:05.620]   peanutwild.com. We'll see you all next time on the All In podcast.

