
[00:00:00.000 --> 00:00:03.880]   [MUSIC PLAYING]
[00:00:03.880 --> 00:00:06.480]   Welcome to part two of the Weights and Biases Model
[00:00:06.480 --> 00:00:08.240]   Registry walkthrough.
[00:00:08.240 --> 00:00:11.600]   In part one, we talked about how to log your model checkpoints
[00:00:11.600 --> 00:00:15.680]   to Weights and Biases and link them to the Model Registry.
[00:00:15.680 --> 00:00:18.480]   Now let's show how consumers can access these models,
[00:00:18.480 --> 00:00:21.400]   evaluate them, build reports, and automate
[00:00:21.400 --> 00:00:23.560]   downstream processes.
[00:00:23.560 --> 00:00:26.320]   Here in my Model Registry, I have a variety of models
[00:00:26.320 --> 00:00:28.880]   to inspect and consume.
[00:00:28.880 --> 00:00:31.840]   Today, I'm going to be pulling down the review summarization
[00:00:31.840 --> 00:00:36.120]   model, doing some further processing, running inference,
[00:00:36.120 --> 00:00:38.800]   and building a report on the results.
[00:00:38.800 --> 00:00:40.640]   You can see we have one model linked
[00:00:40.640 --> 00:00:42.680]   under this registered model.
[00:00:42.680 --> 00:00:45.160]   And it's been aliased with staging,
[00:00:45.160 --> 00:00:50.160]   indicating that this model is ready for further processing.
[00:00:50.160 --> 00:00:53.320]   If I go and view details, I can see the model card
[00:00:53.320 --> 00:00:57.800]   with a background description, summary, and information
[00:00:57.800 --> 00:01:02.240]   about how to load the model into memory.
[00:01:02.240 --> 00:01:05.840]   I can also see an action history of all past actions
[00:01:05.840 --> 00:01:08.160]   that people have done on this registered model,
[00:01:08.160 --> 00:01:10.360]   like linking new models or changing aliases.
[00:01:10.360 --> 00:01:17.800]   Going to the staging version, I can inspect all the metadata
[00:01:17.800 --> 00:01:22.800]   about it just like I could in the Artifacts tab.
[00:01:22.800 --> 00:01:25.480]   To consume a model, all I need to do
[00:01:25.480 --> 00:01:28.080]   is use this snippet of code, which references
[00:01:28.080 --> 00:01:32.000]   the name of the registered model and the specific alias version
[00:01:32.000 --> 00:01:33.880]   that I want to pull down.
[00:01:33.880 --> 00:01:35.320]   Once I've retrieved the artifact,
[00:01:35.320 --> 00:01:39.880]   I can simply download it into whatever environment I'm in.
[00:01:39.880 --> 00:01:43.200]   In this notebook, I've copied that snippet of code
[00:01:43.200 --> 00:01:45.200]   with a few modifications.
[00:01:45.200 --> 00:01:49.000]   I've initialized a run with the specific project.
[00:01:49.000 --> 00:01:53.800]   I've retrieved the artifact using wandb.useartifact,
[00:01:53.800 --> 00:01:58.360]   passing in my team entity, the model registry for that team,
[00:01:58.360 --> 00:02:02.360]   the name of the registered model, and the staging version.
[00:02:02.360 --> 00:02:05.040]   Finally, I download that to this root directory.
[00:02:05.040 --> 00:02:12.000]   Now that the model is downloaded,
[00:02:12.000 --> 00:02:14.280]   I want to further process the model.
[00:02:14.280 --> 00:02:17.000]   Some common next steps would be things
[00:02:17.000 --> 00:02:20.080]   like quantization or model format conversion,
[00:02:20.080 --> 00:02:22.680]   depending on where and how you're deploying your model.
[00:02:22.680 --> 00:02:29.200]   In this line, I'm converting the model
[00:02:29.200 --> 00:02:31.440]   to make it more efficient for inference.
[00:02:31.440 --> 00:02:34.640]   I'm then creating a new artifact for the quantized version,
[00:02:34.640 --> 00:02:37.200]   adding the directory of the quantized weights,
[00:02:37.200 --> 00:02:40.360]   and then linking that artifact to the review summarization
[00:02:40.360 --> 00:02:42.080]   registered model.
[00:02:42.080 --> 00:02:44.600]   This time, I'm adding an alias of quantized
[00:02:44.600 --> 00:02:48.440]   to indicate this is the quantized version.
[00:02:48.440 --> 00:02:51.640]   With the quantized version now loaded into memory,
[00:02:51.640 --> 00:02:57.480]   I can now run inference on it against a test set.
[00:02:57.480 --> 00:02:59.520]   Here I have a couple of prompts that I'm
[00:02:59.520 --> 00:03:01.000]   going to run through the model.
[00:03:01.000 --> 00:03:03.400]   I'm logging these prompts as part of our weights and biases
[00:03:03.400 --> 00:03:04.280]   table.
[00:03:04.280 --> 00:03:07.560]   It's a flexible data frame that lives in the weights and biases
[00:03:07.560 --> 00:03:10.120]   workspace.
[00:03:10.120 --> 00:03:14.200]   And finally, we can query any assets and weights and biases
[00:03:14.200 --> 00:03:16.680]   through the import/export API.
[00:03:16.680 --> 00:03:21.400]   This allows us to easily access run data, artifact metadata,
[00:03:21.400 --> 00:03:24.000]   or anything about our pipeline.
[00:03:24.000 --> 00:03:26.960]   We can just instantiate an API object,
[00:03:26.960 --> 00:03:30.040]   access artifacts through their names and aliases,
[00:03:30.040 --> 00:03:31.480]   and then we can walk the artifact
[00:03:31.480 --> 00:03:34.640]   back by getting runs which log these artifacts, their
[00:03:34.640 --> 00:03:39.960]   summaries, other checkpoints, and artifact metadata,
[00:03:39.960 --> 00:03:44.320]   filtering them, and in general, printing and accessing
[00:03:44.320 --> 00:03:47.080]   anything that we need.
[00:03:47.080 --> 00:03:48.640]   This is very convenient if you'd
[00:03:48.640 --> 00:03:52.080]   like to automate access of the model registry
[00:03:52.080 --> 00:03:55.000]   and see it downstream CI/CD processes.
[00:03:55.000 --> 00:03:58.200]   So now let's take everything we've done and build a report.
[00:03:58.200 --> 00:04:01.720]   Remember, what we've done here is we've registered a model.
[00:04:01.720 --> 00:04:04.600]   We then, in a notebook, retrieved that model,
[00:04:04.600 --> 00:04:09.280]   quantized it, ran inference, and recorded some results.
[00:04:09.280 --> 00:04:11.400]   All of this can be quite messy to deal with
[00:04:11.400 --> 00:04:14.280]   if you don't keep track of it in a central place.
[00:04:14.280 --> 00:04:17.200]   Luckily, we kept track of everything along the way
[00:04:17.200 --> 00:04:19.840]   in Weights and Biases.
[00:04:19.840 --> 00:04:23.240]   If I go to Reports, I can create a living document
[00:04:23.240 --> 00:04:28.320]   to house everything about my registered models.
[00:04:28.320 --> 00:04:31.920]   With the forward slash, I can add in all sorts of information
[00:04:31.920 --> 00:04:34.760]   to contextualize this report.
[00:04:34.760 --> 00:04:37.720]   The first thing I'm going to do is bring in the model registry
[00:04:37.720 --> 00:04:38.220]   panel.
[00:04:43.040 --> 00:04:45.360]   Here we have the panel directly embedded into the report,
[00:04:45.360 --> 00:04:47.240]   where I can access all the different versions.
[00:04:47.240 --> 00:04:54.400]   Next, we can add a panel grid, where we can import panels
[00:04:54.400 --> 00:05:00.400]   from different runs, things like the test table or props,
[00:05:00.400 --> 00:05:01.840]   or even just higher level summary
[00:05:01.840 --> 00:05:04.560]   metrics of our evaluation.
[00:05:04.560 --> 00:05:06.600]   And all of this is dynamic, so this report
[00:05:06.600 --> 00:05:09.880]   will update as I continue to run evaluation runs.
[00:05:09.880 --> 00:05:14.520]   I can also filter and group these runs as I see fit.
[00:05:14.520 --> 00:05:16.120]   If I have other registered models,
[00:05:16.120 --> 00:05:20.760]   I can bring in those runs from those projects.
[00:05:20.760 --> 00:05:22.680]   This is a great way to consolidate results
[00:05:22.680 --> 00:05:25.520]   across different efforts in your company
[00:05:25.520 --> 00:05:28.560]   and avoid the anti-pattern of creating PowerPoints and Word
[00:05:28.560 --> 00:05:31.200]   docs, which can easily run stale.
[00:05:31.200 --> 00:05:33.760]   Already, we're starting to see a complete set
[00:05:33.760 --> 00:05:36.240]   of documentation relevant to our production models.
[00:05:36.240 --> 00:05:39.600]   [MUSIC PLAYING]
[00:05:39.600 --> 00:05:45.160]   These reports can be very powerful,
[00:05:45.160 --> 00:05:48.400]   whether it's to document the model evaluation process
[00:05:48.400 --> 00:05:50.440]   or your research efforts.
[00:05:50.440 --> 00:05:55.800]   This one goes into more detail about the training process,
[00:05:55.800 --> 00:05:59.240]   latency of data as models go through quantization
[00:05:59.240 --> 00:06:04.200]   and further optimization, and sample predictions of the model
[00:06:04.200 --> 00:06:06.880]   so that subject matter experts or other colleagues
[00:06:06.880 --> 00:06:07.800]   can look over results.
[00:06:07.800 --> 00:06:11.200]   [MUSIC PLAYING]
[00:06:11.200 --> 00:06:14.520]   We may want to automate this entire process,
[00:06:14.520 --> 00:06:18.040]   whether that's the automated reporting of results
[00:06:18.040 --> 00:06:21.880]   or the automated deployment or quantization of your models.
[00:06:21.880 --> 00:06:26.600]   This, too, is possible with the model registry.
[00:06:26.600 --> 00:06:29.840]   Going in and viewing details for a particular model,
[00:06:29.840 --> 00:06:32.480]   you'll see the automation section.
[00:06:32.480 --> 00:06:35.640]   An automation is an action which occurs when a certain event
[00:06:35.640 --> 00:06:39.240]   happens in a registered model, such as when a new version is
[00:06:39.240 --> 00:06:43.840]   added or when an alias is added, such as when the production
[00:06:43.840 --> 00:06:47.400]   alias is added to a particular version.
[00:06:47.400 --> 00:06:49.480]   Let's say when a new model is added to the review
[00:06:49.480 --> 00:06:53.960]   summarization model, we want to trigger either a launch
[00:06:53.960 --> 00:06:59.720]   job or a webhook post request to some external system.
[00:06:59.720 --> 00:07:02.360]   A launch job is a containerized job
[00:07:02.360 --> 00:07:06.640]   that can be executed in various different compute environments,
[00:07:06.640 --> 00:07:11.560]   useful for model evaluation, model retraining,
[00:07:11.560 --> 00:07:13.960]   or heavy compute requirements.
[00:07:13.960 --> 00:07:16.240]   Webhooks is helpful for handing off models
[00:07:16.240 --> 00:07:22.840]   to external systems, like CI/CD tools, such as GitHub Actions.
[00:07:22.840 --> 00:07:25.880]   If I select a webhook, I can choose the GitHub Actions
[00:07:25.880 --> 00:07:29.240]   webhook or any other generic post request webhook
[00:07:29.240 --> 00:07:31.320]   that I configure.
[00:07:31.320 --> 00:07:36.880]   Here I just add in the payload for the post request.
[00:07:36.880 --> 00:07:40.720]   I can add in template strings to bring in context
[00:07:40.720 --> 00:07:47.400]   from the model registry to pass through into the payload.
[00:07:47.400 --> 00:07:49.920]   And with that, I can name and describe my automation.
[00:07:49.920 --> 00:07:52.320]   And now when new models get linked,
[00:07:52.320 --> 00:07:53.520]   this webhook will trigger.
[00:07:58.080 --> 00:07:59.480]   You can combine the model registry
[00:07:59.480 --> 00:08:03.000]   with your existing tool sets, like GitHub Actions,
[00:08:03.000 --> 00:08:06.880]   so that when new models get linked or artifacts get added,
[00:08:06.880 --> 00:08:09.440]   you can automatically build your containers,
[00:08:09.440 --> 00:08:13.080]   test your models, quantize them, or even deploy them.
[00:08:13.080 --> 00:08:20.840]   In summary, with the Weights and Biases Model Registry,
[00:08:20.840 --> 00:08:23.800]   you can facilitate easy handoffs between ML engineering
[00:08:23.800 --> 00:08:26.320]   and ML ops, build reports to show off
[00:08:26.320 --> 00:08:28.680]   to management and stakeholders, and provide
[00:08:28.680 --> 00:08:31.160]   a complete system of record for the model development
[00:08:31.160 --> 00:08:33.120]   lifecycle.
[00:08:33.120 --> 00:08:35.680]   To learn more or try it out yourself,
[00:08:35.680 --> 00:08:38.240]   please visit this link here.
[00:08:38.840 --> 00:08:42.200]   [MUSIC PLAYING]
[00:08:42.200 --> 00:08:44.780]   (gentle music)
[00:08:44.780 --> 00:08:46.780]   You

