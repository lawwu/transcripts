
[00:00:00.000 --> 00:00:04.560]   what will be at stake will not just be cool products, but whether liberal democracy survives,
[00:00:04.560 --> 00:00:08.400]   whether the CCP survives, what the world order for the next century will be.
[00:00:08.400 --> 00:00:12.640]   The CCP is going to have an all-out effort to infiltrate American AI labs, billions of dollars,
[00:00:12.640 --> 00:00:16.560]   thousands of people. CCP is going to try to outbuild us. People don't realize how intense
[00:00:16.560 --> 00:00:20.000]   state-level espionage can be. When we have literal superintelligence on our cluster,
[00:00:20.000 --> 00:00:23.920]   and they can Stuxnet the Chinese data centers, you really think that'll be a private company,
[00:00:23.920 --> 00:00:27.680]   and the government would be like, "Oh my God, what is going on?" I do think it is incredibly
[00:00:27.680 --> 00:00:30.880]   important that these clusters are in the United States. I mean, would you do the Manhattan
[00:00:30.880 --> 00:00:36.080]   Project in the UAE, right? 2023 was the moment for me where it went from AGI as a theoretical,
[00:00:36.080 --> 00:00:40.080]   abstract thing, and you'd make the models to, "I see it. I feel it." I can see the cluster
[00:00:40.080 --> 00:00:44.080]   where it's trained on, the rough combination of algorithms, the people, how it's happening.
[00:00:44.080 --> 00:00:47.280]   And I think most of the world is not... Most of the people who feel it are right here.
[00:00:47.280 --> 00:00:54.080]   Okay. Today, I'm chatting with my friend, Leopold Aschenbrenner. He grew up in Germany,
[00:00:54.080 --> 00:01:00.160]   graduated valedictorian of Columbia when he was 19. And then he had a very interesting
[00:01:00.160 --> 00:01:05.520]   gaffe year, which we'll talk about. And then he was on the OpenAI super alignment team,
[00:01:05.520 --> 00:01:12.240]   may it rest in peace. And now he, with some anchor investments from Patrick and John Collison,
[00:01:12.240 --> 00:01:16.080]   and Daniel Gross, and Nat Friedman, is launching an investment firm.
[00:01:16.080 --> 00:01:21.520]   So, Leopold, I know you're off to a slow start, but life is long, and I wouldn't worry about it
[00:01:21.520 --> 00:01:26.720]   too much. You'll make up for it in due time. But thanks for coming on the podcast.
[00:01:26.720 --> 00:01:32.320]   Thank you. I first discovered your podcast when your best episode had like a couple hundred views.
[00:01:32.320 --> 00:01:36.560]   And so, it's just been amazing to follow your trajectory. And it's a delight to be on.
[00:01:36.560 --> 00:01:42.560]   Yeah, yeah. Well, I think in the Shelter and Trenton episode, I mentioned that a lot of the
[00:01:42.560 --> 00:01:47.200]   things I've learned about AI, I've learned from talking with them. And the third part of this
[00:01:47.200 --> 00:01:50.960]   triumvirate, probably the most significant in terms of the things that I've learned about AI
[00:01:50.960 --> 00:01:55.520]   has been you. We'll get all the stuff on the record now. Great. Okay, first thing I had to
[00:01:55.520 --> 00:02:00.000]   get on record. Tell me about the trillion-dollar cluster. By the way, I should mention, so the
[00:02:00.000 --> 00:02:06.080]   context of this podcast is today, you're releasing a series called Situational Awareness. We're going
[00:02:06.080 --> 00:02:09.440]   to get into it. First question about that is, tell me about the trillion-dollar cluster.
[00:02:09.440 --> 00:02:14.480]   Yeah. So, you know, unlike basically most things that have come out of Silicon Valley recently,
[00:02:14.480 --> 00:02:20.400]   you know, AI is kind of this industrial process. The next model doesn't just require some code,
[00:02:20.400 --> 00:02:25.200]   it's building a giant new cluster. Now it's building giant new power plants. Pretty soon,
[00:02:25.200 --> 00:02:31.600]   it's going to be building giant new fabs. And since that, this kind of extraordinary sort of
[00:02:31.600 --> 00:02:35.680]   techno-capital acceleration has been set into motion. I mean, basically, exactly a year ago
[00:02:35.680 --> 00:02:41.040]   today, NVIDIA had their first kind of blockbuster earnings call, right? Where it went up 25% after
[00:02:41.040 --> 00:02:46.000]   hours and everyone was like, "Oh my God, AI, it's a thing." You know, I mean, I think within a year,
[00:02:46.000 --> 00:02:50.640]   you know, NVIDIA data center revenue has gone from like, you know, a few billion a quarter to like,
[00:02:50.640 --> 00:02:55.040]   you know, 20, 25 billion a quarter now and, you know, continuing to go up. Like, you know,
[00:02:55.040 --> 00:03:00.880]   big tech capex is skyrocketing. And, you know, it's funny because it's both, there's this sort
[00:03:00.880 --> 00:03:04.640]   of this kind of crazy scramble going on, but in some sense, it's just the sort of continuation
[00:03:04.640 --> 00:03:07.920]   of straight lines on a graph, right? There's this kind of like long run trend, basically almost a
[00:03:07.920 --> 00:03:11.840]   decade of sort of training compute of the sort of largest AI systems growing by about, you know,
[00:03:11.840 --> 00:03:17.360]   half an order of magnitude, you know, 0.5 booms a year. And you can just kind of play that forward,
[00:03:17.360 --> 00:03:23.120]   right? So, you know, GPT-4, you know, rumored or reported to have finished pre-training in 2022,
[00:03:23.120 --> 00:03:27.840]   you know, the sort of cluster size there was rumored to be about, you know, 25,000 H100s,
[00:03:27.840 --> 00:03:33.040]   sorry, A100s on semi-analysis. You know, that's roughly, you know, if you do the math on that,
[00:03:33.040 --> 00:03:36.880]   it's maybe like a $500 million cluster, you know, it's very roughly 10 megawatts.
[00:03:36.880 --> 00:03:42.960]   And, you know, just play that forward, half a boom a year, right? So then 2024,
[00:03:42.960 --> 00:03:47.520]   that's a, you know, that's a cluster that's, you know, 100 megawatts, that's like 100,000 H100
[00:03:47.520 --> 00:03:53.440]   equivalents, you know, that's, you know, costs in the billions, you know, play it forward,
[00:03:53.440 --> 00:03:57.920]   you know, two more years, 2026, that's a cluster that's a gigawatt, you know, that's, you know,
[00:03:57.920 --> 00:04:01.600]   sort of a large nuclear reactor size. It's like the power of the Hoover Dam, you know, that costs
[00:04:01.600 --> 00:04:05.520]   tens of billions of dollars, that's like a million H100 equivalents. You know, 2028, that's a cluster
[00:04:05.520 --> 00:04:10.560]   that's 10 gigawatts, right? That's more power than kind of like most US states. That's, you know,
[00:04:10.560 --> 00:04:15.520]   like 10 million H100s equivalents, you know, costs hundreds of billions of dollars. And then 2030,
[00:04:15.520 --> 00:04:22.000]   trillion dollar cluster, 100 gigawatts, over 20% of US electricity production, you know,
[00:04:22.000 --> 00:04:26.800]   100 million H100 equivalents. And that's just the training cluster, right? That's like the one
[00:04:26.800 --> 00:04:30.000]   largest training cluster. And then there's more inference GPUs as well, right? Most of, you know,
[00:04:30.000 --> 00:04:35.920]   once there's products, most of them are going to be inference GPUs. And so, you know, US power
[00:04:35.920 --> 00:04:40.960]   production has barely grown for like, you know, decades. And now we're really in for a ride.
[00:04:40.960 --> 00:04:47.040]   So, I mean, when I had Zuck on the podcast, he was claiming, not a plateau per se, but
[00:04:47.040 --> 00:04:52.400]   that AI progress would be bottlenecked by specifically this constraint on energy,
[00:04:52.400 --> 00:04:56.880]   and specifically like, oh, gigawatt data centers are going to build another Three Gorges Dam or
[00:04:56.880 --> 00:05:02.320]   something. I know that there's companies, according to public reports, who are planning
[00:05:02.320 --> 00:05:07.040]   things on the scale of a gigawatt data center. 10 gigawatt data center, who's going to be able
[00:05:07.040 --> 00:05:12.320]   to build that? I mean, 100 gigawatt center, like a state, are you going to pump that into
[00:05:12.320 --> 00:05:16.400]   one physical data center? How is this going to be possible? What is Zuck missing?
[00:05:16.400 --> 00:05:19.280]   I mean, you know, I don't know. I think 10 gigawatts, you know, like six months ago,
[00:05:19.280 --> 00:05:22.640]   you know, 10 gigawatts was the talk of the town. I mean, I think, I feel like now,
[00:05:22.640 --> 00:05:25.040]   you know, people have moved on, you know, 10 gigawatts is happening. I mean,
[00:05:25.040 --> 00:05:30.400]   I know there's the information report on OpenAI and Microsoft planning a $100 billion cluster.
[00:05:30.400 --> 00:05:33.440]   So, you know, you've got to, you know... Is that a gigawatt or is that 10 gigawatt?
[00:05:33.440 --> 00:05:36.880]   I mean, I don't know. But, you know, if you try to like map out, you know, how expensive would
[00:05:36.880 --> 00:05:40.400]   the 10 gigawatt cluster be? You know, that's maybe a couple hundred billion. So, it's sort of on that
[00:05:40.400 --> 00:05:47.680]   scale. And they're planning it. They're working on it, you know. So, the, you know, it's not just
[00:05:47.680 --> 00:05:52.160]   sort of my crazy take. I mean, AMD, I think, forecasted a $400 billion AI accelerator market
[00:05:52.160 --> 00:05:56.720]   by 2027. You know, I think it's, you know, and AI accelerators are only part of the expenditures.
[00:05:56.720 --> 00:06:00.480]   It's sort of, you know, I think sort of a trillion dollars of sort of like total AI
[00:06:00.480 --> 00:06:04.160]   investment by 2027 is sort of like we're very much in track on. I think the trillion dollar
[00:06:04.160 --> 00:06:08.480]   cluster is going to take a bit more sort of acceleration. But, you know, we saw how much
[00:06:08.480 --> 00:06:12.080]   sort of chat GPT unleashed. Right. And so, like every generation, you know, the models are going
[00:06:12.080 --> 00:06:16.240]   to be kind of crazy and people, it's going to shift the Overton window. And then, you know,
[00:06:16.240 --> 00:06:19.200]   obviously the revenue comes in. Right. So, these are forward looking investments. The question is,
[00:06:19.200 --> 00:06:24.000]   do they pay off? Right. And so, if we sort of estimated the, you know, the GPT-4 cluster at
[00:06:24.000 --> 00:06:28.400]   around 500 million, by the way, that's sort of a common mistake people make is they say, you know,
[00:06:28.400 --> 00:06:32.080]   people say like a hundred million dollars before, but that's just the rental price. Right. They're
[00:06:32.080 --> 00:06:34.960]   like, ah, you rent the cluster for three months. But, you know, if you're building the biggest
[00:06:34.960 --> 00:06:37.600]   cluster, you got to like, you got to build the whole cluster. You got to pay for the whole
[00:06:37.600 --> 00:06:40.800]   cluster. You can't just rent it for three months. But, I mean, really, you know, once you're trying
[00:06:40.800 --> 00:06:43.440]   to get into this sort of hundreds of billions, eventually you got to get to like a hundred
[00:06:43.440 --> 00:06:46.480]   billion a year. I think this is where it gets really interesting for the big tech companies.
[00:06:46.480 --> 00:06:50.000]   Right. Because like their revenues are in order, you know, hundreds of billions. Right. So,
[00:06:50.000 --> 00:06:54.160]   it's like 10 billion fine, you know, and it'll pay off the, you know, 2024 size training cluster.
[00:06:54.160 --> 00:06:58.880]   But, you know, really when sort of big tech, it'll be gangbusters is a hundred billion a year.
[00:06:58.880 --> 00:07:02.800]   And so, the question is sort of how feasible is a hundred billion a year from AI revenue. And,
[00:07:02.800 --> 00:07:07.680]   you know, it's a lot more than right now. But I think, you know, if you sort of believe in the
[00:07:07.680 --> 00:07:12.320]   trajectory of the AI systems as I do, and which we'll probably talk about, it's not that crazy.
[00:07:12.320 --> 00:07:16.960]   Right. So, there's I think there's like 300 million, you know, Microsoft Office subscribers.
[00:07:16.960 --> 00:07:20.800]   Right. And so, they have Copilot now. And I know what they're selling it for. But, you know,
[00:07:20.800 --> 00:07:25.280]   suppose you sold some sort of AI add on for a hundred bucks a month. And you sold that to,
[00:07:25.280 --> 00:07:28.240]   you know, a third of Microsoft Office subscribers subscribe to that. That'd be a hundred billion
[00:07:28.240 --> 00:07:31.840]   right there. You know, a hundred dollars a month is, you know, a lot. It's a lot. It's a lot.
[00:07:31.840 --> 00:07:34.240]   For a third of Office subscribers? Yeah. But it's but it's, you know,
[00:07:34.240 --> 00:07:37.200]   for the average knowledge worker, it's like a few hours of productivity a month. And it's,
[00:07:37.200 --> 00:07:40.960]   you know, kind of like you have to be expecting pretty lame AI progress to not hit like, you know,
[00:07:40.960 --> 00:07:46.000]   some few hours of productivity a month of yeah. Okay, sure. So, let's assume all this.
[00:07:46.000 --> 00:07:48.960]   Yeah. What what happens in the next few years in terms of
[00:07:48.960 --> 00:07:55.600]   what is the one gigawatt training, the AI that's trained on the one gigawatt data center? What can
[00:07:55.600 --> 00:07:59.520]   it do the one on the 10 gigawatt data center? Just map out the next few years of AI progress for me.
[00:07:59.520 --> 00:08:03.760]   Yeah, I think probably the sort of 10 gigawatt ish range is sort of my best guess for when you
[00:08:03.760 --> 00:08:07.360]   get the sort of true AGI. I mean, yeah, I think it's sort of like one gigawatt data center. And
[00:08:07.360 --> 00:08:10.240]   again, I think actually compute is overrated. And we're going to talk about that. But what we'll
[00:08:10.240 --> 00:08:14.320]   talk about compute right now. So, you know, I think 25, 26, we're gonna get models that are,
[00:08:14.320 --> 00:08:19.840]   you know, basically smarter than most college graduates. I think sort of the practical,
[00:08:19.840 --> 00:08:23.360]   a lot of the economic usefulness, I think really depends on sort of, you know, sort of on hobbling.
[00:08:23.360 --> 00:08:26.880]   Basically, it's, you know, the models are kind of, you know, they're smart, but they're limited,
[00:08:26.880 --> 00:08:29.760]   right? There, you know, there's this chatbot, you know, and things like being able to use a
[00:08:29.760 --> 00:08:35.040]   computer, things like being able to do kind of like a genetic long horizon tasks. And then I
[00:08:35.040 --> 00:08:38.640]   think by 27, 28, you know, if you extrapolate the trends, and you know, we'll talk about that more
[00:08:38.640 --> 00:08:42.080]   later. And I talked about in the series, I think we hit, you know, basically, you know, like,
[00:08:42.080 --> 00:08:46.720]   as smart as the smartest experts, I think on a hobbling trajectory kind of points to, you know,
[00:08:46.720 --> 00:08:50.880]   looks much more like an agent than a chatbot. And much more almost like basically a drop in
[00:08:50.880 --> 00:08:54.160]   remote worker, right? So it's not like, I think, basically, I mean, I think this is the sort of
[00:08:54.160 --> 00:08:57.920]   question on the economic returns, I think a lot of the, a lot of the intermediate AI systems could
[00:08:57.920 --> 00:09:01.360]   be really useful. But you know, it actually just takes a lot of schlep to integrate them, right?
[00:09:01.360 --> 00:09:05.120]   Like GPT-4, you know, whatever, 4.5, you know, probably there's a lot you can do with them in
[00:09:05.120 --> 00:09:08.880]   a business use case. But you know, you really got to change your workflows to make them useful.
[00:09:08.880 --> 00:09:12.000]   And it's just like, there's a lot of, you know, it's a very Tyler Cowen-esque take. It just takes
[00:09:12.000 --> 00:09:16.400]   a long time to diffuse. Yeah, it's like, you know, we're in SF. And so we missed that or whatever.
[00:09:16.400 --> 00:09:23.200]   But I think in some sense, you know, the way a lot of these systems want to be integrated is
[00:09:23.200 --> 00:09:27.920]   you kind of get this sort of sonic boom, where it's, you know, the sort of intermediate systems
[00:09:27.920 --> 00:09:30.720]   could have done it, but it would have taken schlep. And before you do the schlep to integrate
[00:09:30.720 --> 00:09:34.800]   them, you get much more powerful systems, much more powerful systems that are sort of unhobbled.
[00:09:34.800 --> 00:09:38.880]   And so they're this agent, and there's this drop in remote worker. And, you know, and then you're
[00:09:38.880 --> 00:09:42.400]   kind of interacting with them like a co-worker, right? You know, you can do Zoom calls with them,
[00:09:42.400 --> 00:09:46.560]   and you're slacking them. And you're like, "Ah, can you do this project?" And then they go off,
[00:09:46.560 --> 00:09:49.920]   and they, you know, go away for a week, and write a first draft, and get feedback on them, and,
[00:09:49.920 --> 00:09:55.120]   you know, run tests on their code. And then they come back, and you see it, and you tell them a
[00:09:55.120 --> 00:10:00.400]   little bit more things, or, you know, and that'll be much easier to integrate. And so, you know,
[00:10:00.400 --> 00:10:03.840]   it might be that actually you need a bit of overkill to make the sort of transition easy,
[00:10:03.840 --> 00:10:06.320]   and to really harvest the gains. - What do you mean by the overkill?
[00:10:06.320 --> 00:10:08.640]   Overkill on the model capabilities? - Yeah, yeah. So basically,
[00:10:08.640 --> 00:10:10.880]   the intermediate models could do it, but it would take a lot of schlep.
[00:10:10.880 --> 00:10:11.840]   - I see. - And so then, you know,
[00:10:11.840 --> 00:10:15.120]   they're like, "Actually, it's just the drop in remote worker kind of AGI that can automate,
[00:10:15.120 --> 00:10:19.040]   you know, cognitive tasks that actually just ends up kind of like, you know, basically it's like,
[00:10:19.040 --> 00:10:22.000]   you know, the intermediate models would have made the software engineer more productive,
[00:10:22.000 --> 00:10:26.480]   but, you know, will the software engineer adopt it?" And then the, you know, 27 model is, well,
[00:10:26.480 --> 00:10:28.880]   you know, you just don't need the software engineer. You can literally interact with it
[00:10:28.880 --> 00:10:31.280]   like a software engineer, and it'll do the work of a software engineer.
[00:10:31.280 --> 00:10:34.640]   - So the last episode I did was with John Shulman. - Yeah.
[00:10:34.640 --> 00:10:39.120]   - And I was asking about basically this, and one of the questions I asked is,
[00:10:39.120 --> 00:10:43.600]   "We have these models that have been coming out in the last year, and none of them seem to have
[00:10:43.600 --> 00:10:49.360]   significantly surpassed GPT-4, and certainly not in the agentic way in which they are interacting
[00:10:49.360 --> 00:10:54.320]   with as a co-worker." You know, they'll brag that they got a few extra points on MMLU or something.
[00:10:54.320 --> 00:10:58.720]   And even GPT-4.0, it's cool that they can talk like Scarlett Johansson or something, but like...
[00:10:58.720 --> 00:11:01.040]   (laughter)
[00:11:01.040 --> 00:11:03.920]   And honestly, I'm not going to use that. Well, I guess not anymore.
[00:11:03.920 --> 00:11:10.320]   Okay, but the whole co-worker thing... So this is going to be a run-on question,
[00:11:10.320 --> 00:11:15.600]   but you can address it in any order. But it makes sense to me why they'd be good at answering
[00:11:15.600 --> 00:11:20.560]   questions. They have a bunch of data about how to complete Wikipedia text or whatever.
[00:11:20.560 --> 00:11:26.880]   Where is the equivalent training data that enables it to understand what's going on in the Zoom call?
[00:11:26.880 --> 00:11:29.920]   How does this connect with what they were talking about in the Slack?
[00:11:29.920 --> 00:11:35.440]   What is the cohesive project that they're going after, based on all this context that I have?
[00:11:35.440 --> 00:11:36.880]   Where is that training data coming from?
[00:11:36.880 --> 00:11:43.120]   Yeah. So I think a really key question for AI progress in the next few years is how hard is
[00:11:43.120 --> 00:11:48.480]   it to unlock the test-time compute overhang? So right now, GPT-4 answers a question,
[00:11:48.480 --> 00:11:53.120]   and it can do a few hundred tokens of a chain of thought. And that's already a huge improvement,
[00:11:53.120 --> 00:11:57.120]   right? This is a big unhobbling. Before, answering a math question is just shotgun.
[00:11:57.120 --> 00:12:02.320]   If you try to answer a math question by saying the first thing that came to mind,
[00:12:02.320 --> 00:12:05.600]   you wouldn't be very good. So GPT-4 thinks for a few hundred tokens.
[00:12:05.600 --> 00:12:10.800]   And if I think at a hundred tokens a minute, and I thought for a few minutes...
[00:12:10.800 --> 00:12:13.120]   You think at much more than a hundred tokens a minute.
[00:12:13.120 --> 00:12:18.480]   I don't know. If I thought for a hundred tokens a minute, what GPT-4 does, maybe it's equivalent
[00:12:18.480 --> 00:12:25.280]   to me thinking for three minutes or whatever, right? Suppose GPT-4 could think for millions
[00:12:25.280 --> 00:12:28.800]   of tokens, right? That's sort of plus four rooms, plus four to the magnitude on test-time compute,
[00:12:28.800 --> 00:12:33.600]   just on one problem. It can't do it right now. It kind of gets stuck, right? It writes some code,
[00:12:33.600 --> 00:12:37.680]   even if it can do a little bit of iterative debugging, but eventually it just kind of gets
[00:12:37.680 --> 00:12:43.440]   stuck in something. It can't correct its errors and so on. And in a sense, there's this big
[00:12:43.440 --> 00:12:47.840]   overhang, right? And other areas of ML, there's this great paper on AlphaGo, right? Where you
[00:12:47.840 --> 00:12:51.600]   can trade off train time and test-time compute. And if you can use four rooms, more test-time
[00:12:51.600 --> 00:12:56.240]   compute, that's almost like a three and a half room bigger model. Just because, again, if a
[00:12:56.240 --> 00:13:00.560]   hundred tokens a minute, a few million tokens, that's a few months of sort of working time.
[00:13:00.560 --> 00:13:04.080]   There's a lot more you can do in a few months of working time than right now. So the question is,
[00:13:04.080 --> 00:13:11.520]   how hard is it to unlock that? And I think the sort of short timelines AI world is if it's not
[00:13:11.520 --> 00:13:16.880]   that hard. And the reason it might not be that hard is that there's only really a few extra
[00:13:16.880 --> 00:13:20.800]   tokens you need to learn, right? You need to kind of learn the error correction tokens,
[00:13:20.800 --> 00:13:23.840]   the tokens where you're like, "Ah, I think I made a mistake. Let me think about that again." You
[00:13:23.840 --> 00:13:26.800]   need to learn the kind of planning tokens. That's kind of like, "I'm going to start by making a
[00:13:26.800 --> 00:13:30.480]   plan. Here's my plan of attack. And then I'm going to write a draft. And I'm going to like,
[00:13:30.480 --> 00:13:33.920]   now I'm going to critique my draft. I'm going to think about it." And so it's not things that
[00:13:33.920 --> 00:13:39.280]   models can do right now. But the question is, how hard is that? And in some sense also,
[00:13:39.280 --> 00:13:44.240]   there's sort of two paths to agents, right? When Sholto was on your podcast, he talked about kind
[00:13:44.240 --> 00:13:49.280]   of scaling leading to more nines of reliability. And so that's one path. I think the other path is
[00:13:49.280 --> 00:13:54.800]   a sort of unhobbling path where it needs to learn this kind of system two process. And if it can
[00:13:54.800 --> 00:14:00.000]   learn this sort of system two process, it can just use kind of millions of tokens and think for them
[00:14:00.000 --> 00:14:06.080]   and be cohesive and be coherent. One analogy, so when you drive, here's an analogy. When you drive,
[00:14:06.080 --> 00:14:10.400]   right? Okay, you're driving. And most of the time you're kind of on autopilot, right? You're just
[00:14:10.400 --> 00:14:14.960]   kind of driving and you're doing well. But sometimes you hit like a weird construction
[00:14:14.960 --> 00:14:19.120]   zone or a weird intersection. And then I sometimes, my passenger seat, my girlfriend,
[00:14:19.120 --> 00:14:23.120]   I'm kind of like, "Ah, be quiet for a moment. I need to figure out what's going on." And that's
[00:14:23.120 --> 00:14:27.040]   sort of like, you go from autopilot to like the system two is jumping in and you're thinking about
[00:14:27.040 --> 00:14:32.000]   how to do it. And so scaling is improving that system one autopilot. And I think it's the brute
[00:14:32.000 --> 00:14:36.160]   force way to get to kind of agents. You just improve that system. But if you can get that
[00:14:36.160 --> 00:14:42.960]   system two working, then I think you could like quite quickly jump to sort of this like more
[00:14:42.960 --> 00:14:49.840]   agentified test time compute overhang is unlocked. What's the reason to think that this is an easy
[00:14:49.840 --> 00:14:55.440]   win in the sense that, oh, you just get the, there's like some loss function that easily
[00:14:55.440 --> 00:15:00.800]   enables you to train it to enable the system two thinking. Yeah. There's not a lot of animals that
[00:15:00.800 --> 00:15:04.960]   have system two thinking. It took a long time for evolution to give us system two thinking.
[00:15:04.960 --> 00:15:08.480]   Yeah. The free training, it's like, listen, I get it. You got like trillions of tokens
[00:15:08.480 --> 00:15:12.080]   of internet text. I get that. Like, yeah, you like match that and you get all these,
[00:15:12.080 --> 00:15:16.800]   all this free training capabilities. What's the reason to think that this is an easy and hobbling?
[00:15:16.800 --> 00:15:25.760]   Yeah. So, okay. A bunch of things. So first of all, free training is magical, right? And it gave
[00:15:25.760 --> 00:15:32.720]   us this huge advantage for models of general intelligence because you could just predict
[00:15:32.720 --> 00:15:35.600]   the next token, but predicting the next token, I mean, it's sort of a common misconception,
[00:15:35.600 --> 00:15:38.640]   but what it does is it lets this model learn these incredibly rich representations, right?
[00:15:38.640 --> 00:15:41.600]   Like these sort of representation learning properties are the magic of deep learning.
[00:15:41.600 --> 00:15:45.040]   You have these models and instead of learning just kind of like, you know, whatever statistical
[00:15:45.040 --> 00:15:48.160]   artifacts or whatever, it learns through these models of the world. You know, that's also why
[00:15:48.160 --> 00:15:52.960]   they can kind of like generalize, right? Because it learned the right representations. And so,
[00:15:52.960 --> 00:15:56.400]   you know, you train these models and you have this sort of like raw bundle of capabilities,
[00:15:56.400 --> 00:16:01.360]   that's really useful. And sort of this almost unformed raw mass and sort of the unhobbling
[00:16:01.360 --> 00:16:05.600]   we've done over sort of like GP2 to GP4 was you kind of took this sort of like raw mass and then
[00:16:05.600 --> 00:16:09.120]   you like RLHF'd it into a really good chatbot. And that was a huge win, right? Like, you know,
[00:16:09.120 --> 00:16:14.960]   going, going, you know, in the original, I think it's our GPT paper, you know, RLHF versus non-RLHF
[00:16:14.960 --> 00:16:18.320]   model. It's like a hundred X model size win on sort of human preference rating. You know,
[00:16:18.320 --> 00:16:22.240]   it started to be able to do like simple chain of thought and so on. But you still have this
[00:16:22.240 --> 00:16:25.680]   advantage of all these kind of like raw capabilities. And I think there's still like
[00:16:25.680 --> 00:16:28.880]   a huge amount that you're not doing with them. And by the way, I think this sort of,
[00:16:28.880 --> 00:16:31.920]   this pre-training advantage is also sort of the difference to robotics, right? Where I think
[00:16:31.920 --> 00:16:36.880]   robotics, you know, I think people used to say it was a hardware problem, but I think the hardware
[00:16:36.880 --> 00:16:41.040]   stuff is getting solved. But the thing we have right now is you don't have this huge advantage
[00:16:41.040 --> 00:16:44.240]   of being able to bootstrap yourself with pre-training. You don't have all this sort of
[00:16:44.240 --> 00:16:47.840]   unsupervised learning you can do. You have to start right away with the sort of RL self-play
[00:16:47.840 --> 00:16:55.840]   and so on. All right. So now the question is why might some of this unhobbling and RL and so on
[00:16:55.840 --> 00:17:01.760]   work? And again, there's sort of this advantage of bootstrapping, right? So, you know, your Twitter
[00:17:01.760 --> 00:17:05.520]   bio is being pre-trained, right? But you're actually not being pre-trained anymore. You're
[00:17:05.520 --> 00:17:09.840]   not being pre-trained anymore. You were pre-trained in like grade school, in high school. At some
[00:17:09.840 --> 00:17:15.520]   point, you transition to being able to like learn by yourself, right? You weren't able to do that
[00:17:15.520 --> 00:17:19.680]   in elementary school. I don't know, middle school probably, high school is maybe when it sort of
[00:17:19.680 --> 00:17:24.800]   started, you need some guidance. You know, college, you know, if you're smart, you can kind of teach
[00:17:24.800 --> 00:17:29.040]   yourself. And then sort of models are just starting to enter that regime, right? And so it's sort of
[00:17:29.040 --> 00:17:32.640]   like, it's a little bit, probably a little bit more scaling. And then you've got to figure out
[00:17:32.640 --> 00:17:38.240]   what goes on top and it won't be trivial, right? So a lot of deep learning is sort of like,
[00:17:38.240 --> 00:17:42.720]   you know, it sort of seems very obvious in retrospect. And there's sort of some obvious
[00:17:42.720 --> 00:17:46.320]   cluster of ideas, right? There's sort of some kind of like thing that seems a little dumb,
[00:17:46.320 --> 00:17:49.120]   but there's kind of works, but there's a lot of details you have to get right. So I'm not
[00:17:49.120 --> 00:17:51.600]   saying this, you know, we're going to get this, you know, next month or whatever. I think it's
[00:17:51.600 --> 00:17:55.360]   going to take a while to like really figure out a while for you. It's like half a year or something.
[00:17:55.360 --> 00:18:02.160]   I don't know. I think six months, three years, you know, but I, but, you know, I, I think it's
[00:18:02.160 --> 00:18:06.800]   possible. And I think there's, you know, I think, and this is, I think it's also very related to the
[00:18:06.800 --> 00:18:10.960]   sort of issue of the data wall, but I mean, I think the, you know, one intuition on the sort
[00:18:10.960 --> 00:18:15.760]   of like learning, learning, learning by yourself, right. It's sort of pre-training is kind of the
[00:18:15.760 --> 00:18:20.560]   words are flying by. Yeah. Right. You know, and, and, or it's like, you know, the teacher's lecturing
[00:18:20.560 --> 00:18:24.080]   to you and the models, you know, the words are flying by, you know, they're taking, they're just
[00:18:24.080 --> 00:18:28.640]   getting a little bit from it. But that's sort of not what you do when you learn from yourself,
[00:18:28.640 --> 00:18:32.560]   right. When you learn by yourself, you know, so you're reading a dense math textbook, you're not
[00:18:32.560 --> 00:18:34.960]   just kind of like skimming through at once, you know, you wouldn't learn that much from it. I
[00:18:34.960 --> 00:18:38.880]   mean, some word cells just skim through reread and reread the math textbook. And then they
[00:18:38.880 --> 00:18:42.880]   memorize that sort of, you know, like if you just repeated the data, then they memorize what you do
[00:18:42.880 --> 00:18:46.160]   is you kind of like, you read a page, kind of think about it. You have some internal monologue
[00:18:46.160 --> 00:18:50.480]   going on. You have a conversation with a study buddy. You try a practice problem, you know,
[00:18:50.480 --> 00:18:54.400]   you fail a bunch of times. At some point it clicks and you're like, this made sense. Then
[00:18:54.400 --> 00:18:58.560]   you read a few more pages. And so we've kind of bootstrapped our way to being, being able to do
[00:18:58.560 --> 00:19:03.520]   that now with models or like just starting to be able to do that. And then the question is,
[00:19:03.520 --> 00:19:08.640]   you know, being able to like read it, think about it, you know, try problems. And the question is,
[00:19:08.640 --> 00:19:11.920]   can you, you know, all this sort of self-play synthetic data RL is kind of like making that
[00:19:11.920 --> 00:19:18.320]   thing work. So basically translate, translated, translating, like in context, right? Like right
[00:19:18.320 --> 00:19:22.000]   now there's like in-context learning, right? Super sample efficient. There's that, you know,
[00:19:22.000 --> 00:19:26.800]   in the Gemini paper, right? It just like learns a language in context. And then you're pre-training,
[00:19:26.800 --> 00:19:32.800]   not at all sample efficient. But, you know, what humans do is they kind of like, they do in-context
[00:19:32.800 --> 00:19:36.400]   learning. You read a book, you think about it until eventually it clicks. But then you somehow
[00:19:36.400 --> 00:19:40.880]   distill that back into the weights. And in some sense, that's sort of like what RL is trying to
[00:19:40.880 --> 00:19:46.800]   do. And like when RL is super finicky, but when RL works, RL is kind of magical because it's sort of
[00:19:46.800 --> 00:19:50.960]   the best possible data for the model. It's like when you try a practice problem and it, you know,
[00:19:50.960 --> 00:19:55.200]   and then you fail. And at some point you kind of figure it out in a way that makes sense to you.
[00:19:55.200 --> 00:19:58.240]   That's sort of like the best possible data for you. Cause like the way you would have solved
[00:19:58.240 --> 00:20:03.440]   the problem. And that's sort of, that's what RL is. Rather than just, you know, you kind of read
[00:20:03.440 --> 00:20:07.360]   how somebody else solved the problem and doesn't, you know, initially click. Yeah. By the way,
[00:20:07.360 --> 00:20:11.360]   if that take sounds familiar, because it was like part of the question I asked John Schulman
[00:20:11.360 --> 00:20:15.360]   that goes to illustrate the thing I said in the intro where like a bunch of the things I've learned
[00:20:15.360 --> 00:20:20.720]   about AI, it just like, we do these dinners before the interviews and you show up in a couple. I'm
[00:20:20.720 --> 00:20:27.760]   like, Oh, what should I ask John Schulman? What should I ask Dario? Okay. Suppose this is the way
[00:20:27.760 --> 00:20:32.000]   things go. And we get these in hobblings. Yeah. And the scaling, right? So it's like, you have
[00:20:32.000 --> 00:20:36.560]   this baseline, just enormous force of scaling, right. Where it's like GP2 to GP4, you know,
[00:20:36.560 --> 00:20:40.960]   GP2, it could kind of like, it was amazing, right. It could string together plausible sentences,
[00:20:40.960 --> 00:20:43.840]   but you know, it could, it could barely do anything. It was kind of like preschooler.
[00:20:43.840 --> 00:20:48.640]   And then GP4 is, you know, it's writing code. It like, you know, can do hard math. It's sort of
[00:20:48.640 --> 00:20:51.520]   like smart high school. And so this big jump and, you know, and sort of the essay series,
[00:20:51.520 --> 00:20:54.480]   I go through and kind of count the orders of magnitude of compute scale up with algorithmic
[00:20:54.480 --> 00:21:00.240]   progress. And so sort of scaling alone, you know, sort of by 27, 28 is going to do another kind of
[00:21:00.240 --> 00:21:06.400]   preschool to high school jump on top of GP4. And so that'll already be just like at a per token
[00:21:06.400 --> 00:21:09.840]   level, just incredibly smart. They'll get you some more reliability. And then you add these
[00:21:09.840 --> 00:21:13.280]   on hobblings that make it look much less like a chatbot, more like this agent, like a drop-in
[00:21:13.280 --> 00:21:20.800]   remote worker. And, you know, that's when things really get going. Okay. Yeah. I want to ask more
[00:21:20.800 --> 00:21:25.040]   questions about this. I think, yeah, let's zoom out. Okay. So suppose you're right about this.
[00:21:25.040 --> 00:21:32.320]   Yeah. And I guess you, this is because of the 2027 cluster, we've got 10 gigawatts,
[00:21:32.320 --> 00:21:39.120]   2027, 10 gigawatts, 128 is the 10 gigawatt. So you'll be pulled for it. Something. Yeah. And so
[00:21:39.120 --> 00:21:44.560]   I guess that's like 5.5 level by 2027, like whatever that's called. Right. What does the
[00:21:44.560 --> 00:21:49.840]   world look like at that point? You have these remote workers who can replace people. What is
[00:21:49.840 --> 00:21:58.000]   the reaction to that in terms of the economy, politics, geopolitics? Yeah. So, you know,
[00:21:58.000 --> 00:22:01.680]   I think 2023 was kind of a really interesting year to experience as somebody who was like,
[00:22:01.680 --> 00:22:06.240]   you know, really following the stuff where, you know, before that. What were you doing in 2023?
[00:22:06.240 --> 00:22:14.240]   I mean, open AI. Yeah. And, you know, it kind of went, you know, I mean, you know,
[00:22:14.240 --> 00:22:17.120]   I was, I was been thinking about this and, you know, like talking to a lot of people, you know,
[00:22:17.120 --> 00:22:19.840]   in the years before, and it was this kind of weird thing, you know, you almost didn't want to talk
[00:22:19.840 --> 00:22:24.160]   about AI or AGI and it was kind of a dirty word. Right. And then 2023, you know, people saw chat
[00:22:24.160 --> 00:22:28.640]   GPT for the first time, they saw GPT4 and it just like exploded. Right. It triggered this kind of,
[00:22:28.640 --> 00:22:33.760]   like, you know, you know, huge sort of capital expenditures from all these firms and, you know,
[00:22:33.760 --> 00:22:39.760]   the explosion in revenue from NVIDIA and so on. And, you know, things have been quiet since then.
[00:22:39.760 --> 00:22:43.200]   But, you know, the next thing has been in the oven. And I sort of expect sort of every generation,
[00:22:43.200 --> 00:22:48.240]   these kind of like G-forces to intensify. Right. It's like people see the models. There's like,
[00:22:48.240 --> 00:22:52.240]   you know, people haven't counted them. So they're going to be surprised. They'll be kind of crazy.
[00:22:52.240 --> 00:22:55.440]   And then, you know, revenue is going to accelerate. You know, suppose you do hit the 10 billion,
[00:22:55.440 --> 00:22:58.880]   you know, end of this year. Suppose it just continues on this sort of doubling trajectory
[00:22:58.880 --> 00:23:02.160]   of, you know, like every six months of revenue doubling. You know, it's like you're not actually
[00:23:02.160 --> 00:23:06.240]   that far from 100 billion, you know, maybe that's like 26. And so, you know, at some point, you know,
[00:23:06.240 --> 00:23:09.040]   like, you know, sort of what happened to NVIDIA is going to happen to big tech, you know, like
[00:23:09.040 --> 00:23:15.440]   their stocks, you know, that's going to explode. And I mean, I think a lot more people are going
[00:23:15.440 --> 00:23:21.600]   to feel it. Right. I mean, I think the I think 2023 was the sort of moment for me where it went
[00:23:21.600 --> 00:23:25.280]   from kind of AGI as a sort of theoretical abstract thing. And you'd make the models to like,
[00:23:25.280 --> 00:23:29.360]   I see it, I feel it. And like, I see the path. I see where it's going. I like,
[00:23:29.360 --> 00:23:33.280]   I think I can see the cluster where it's trained on, like the rough combination of algorithms,
[00:23:33.280 --> 00:23:36.720]   the people like how it's happening. And I think, you know, most of the world is not, you know,
[00:23:36.720 --> 00:23:41.040]   most of the people feel it are like right here. Right. But but, you know, I think a lot more of
[00:23:41.040 --> 00:23:47.600]   the world is going to start feeling it. And I think that's going to start being kind of intense.
[00:23:47.600 --> 00:23:52.080]   OK, so right now, who feels that you can you go on Twitter and there's these GPT
[00:23:52.080 --> 00:23:55.920]   rapper companies like, whoa, it's going to change our business. I'm so bearish on the
[00:23:55.920 --> 00:23:58.800]   rapper companies, right? Because like they're the ones that are going to be like the rapper
[00:23:58.800 --> 00:24:01.680]   companies are betting on stagnation, right? The rapper companies are betting like you have these
[00:24:01.680 --> 00:24:05.040]   intermediate models and take so much time to create them. And I'm kind of like I'm really
[00:24:05.040 --> 00:24:08.080]   bearish because I'm like, we're just going to sonic boom, you know, we're going to get
[00:24:08.080 --> 00:24:11.520]   the unhollowed ones and get the drop in remote worker. And then, you know, your stuff is not
[00:24:11.520 --> 00:24:19.040]   going to matter. OK, sure. Sure. So that's done now. Who was so SF is paying attention now or
[00:24:19.040 --> 00:24:23.440]   this crowd here is paying attention. Who is going to be paying attention in twenty, twenty six,
[00:24:23.440 --> 00:24:27.120]   twenty, twenty seven. And presumably these are years in which the hundreds of billions
[00:24:27.120 --> 00:24:31.600]   of capex is being spent on the eye. I mean, I think the the national security state is going
[00:24:31.600 --> 00:24:36.560]   to be starting to pay a lot of attention. And, you know, I hope we get to talk about that. OK,
[00:24:36.560 --> 00:24:41.120]   let's talk about it now. What happens now? What is the sort of political reaction immediately?
[00:24:41.120 --> 00:24:44.960]   Yeah. And even like internationally, like what people see like right now? I don't know if like
[00:24:44.960 --> 00:24:48.960]   Xi Jinping like reads the news and sees like, yeah, I don't know before. Oh, my God. Again,
[00:24:48.960 --> 00:24:51.280]   I mean, will you score on that? What are you doing about this comrade?
[00:24:51.280 --> 00:24:58.560]   And so what happens when the like what he's like sees a remote replacement and it has one hundred
[00:24:58.560 --> 00:25:01.360]   billion dollars in revenue? There's a lot of businesses that have one hundred billion dollars
[00:25:01.360 --> 00:25:04.720]   in revenue and people don't like aren't staying up all night talking about it.
[00:25:04.720 --> 00:25:10.480]   The question I think the question is when when does the CCP and when does the sort of American
[00:25:10.480 --> 00:25:15.360]   national security establishment realize that superintelligence is going to be absolutely
[00:25:15.360 --> 00:25:18.240]   decisive for national power? Right. And this is where, you know, the sort of intelligence
[00:25:18.240 --> 00:25:20.960]   explosion stuff comes in, which, you know, we should also talk about later. You know,
[00:25:20.960 --> 00:25:24.560]   it's sort of like, you know, you have a GI, you have the sort of drop in worker that can replace,
[00:25:24.560 --> 00:25:27.920]   you know, you or me, at least that sort of remote jobs, you know, kind of jobs.
[00:25:27.920 --> 00:25:35.520]   And then, you know. I think fairly quickly, you know, I mean, by default, you turn the crank,
[00:25:35.520 --> 00:25:38.160]   you know, one or two more times, you know, and then you get a thing that's smarter than humans.
[00:25:38.160 --> 00:25:44.480]   But I think even even more than just turning the crank a few more times, you know, I think one of
[00:25:44.480 --> 00:25:48.800]   the first jobs to be automated is going to be that of sort of an AI researcher engineer. And if you
[00:25:48.800 --> 00:25:54.960]   can automate AI research, you know, I think things can start going very fast. You know, right now,
[00:25:54.960 --> 00:25:58.320]   there's already this trend of, you know, half an order of magnitude a year of algorithmic progress.
[00:25:58.320 --> 00:26:01.360]   You know, suppose, you know, at this point, you know, you're gonna have GPU fleets in the
[00:26:01.360 --> 00:26:05.920]   tens of millions for inference, you know, or more. And you're gonna be able to run like 100
[00:26:05.920 --> 00:26:11.040]   million human human equivalents of these sort of automated AI researchers. And if you can do that,
[00:26:11.040 --> 00:26:15.120]   you know, you can maybe do, you know, a decade's worth of sort of ML research progress in a year,
[00:26:15.120 --> 00:26:20.640]   you know, you get the some sort of 10x speed up. And if you can do that, I think you can make the
[00:26:20.640 --> 00:26:26.080]   jump to kind of like AI that is vastly smarter than humans, you know, within a year, a couple
[00:26:26.080 --> 00:26:29.920]   years. And then, you know, that broadens, right. So you have this, you have this sort of initial
[00:26:29.920 --> 00:26:34.160]   acceleration of AI research that broadens to like you apply R&D to a bunch of other fields of
[00:26:34.160 --> 00:26:39.600]   technology. And the sort of like extremes, you know, at this point, you have like a billion just
[00:26:39.600 --> 00:26:43.680]   super intelligent researchers, engineers, technicians, everything is probably competent,
[00:26:43.680 --> 00:26:47.120]   all the things, you know, they're going to figure out robotics, or we talked about it being a
[00:26:47.120 --> 00:26:51.520]   software problem. Well, you know, you have you have a billion of super smart, smarter than the
[00:26:51.520 --> 00:26:54.560]   smartest human researchers, AI researchers on your cluster, you know, at some point during the
[00:26:54.560 --> 00:26:58.080]   intelligence explosion, they're going to be able to figure out robotics, you know, and then again,
[00:26:58.080 --> 00:27:05.280]   that expands. And, you know, I think if you play this picture forward, I think it is fairly unlike
[00:27:05.280 --> 00:27:13.360]   any other technology in that it will, I think, you know, a couple years of lead could be utterly
[00:27:13.360 --> 00:27:17.520]   decisive and say, like military competition, right, you know, if you look at like, go for one,
[00:27:17.520 --> 00:27:21.440]   right, go for one, you know, like the Western coalition forces, you know, they had, you know,
[00:27:21.440 --> 00:27:25.040]   like 100 to one kill ratio, right. And that was like, they had better sensors on their tanks,
[00:27:25.040 --> 00:27:29.280]   you know, and they had, they had better, more precision, precision missiles, right, like GPS,
[00:27:29.280 --> 00:27:33.680]   and they had, you know, stealth, and then sort of a few, you know, maybe 2030 years of technological
[00:27:33.680 --> 00:27:40.640]   lead, right. And they, you know, just completely crushed them. super intelligence applied to sort
[00:27:40.640 --> 00:27:44.560]   of broad fields of R&D. And then, you know, the sort of industrial explosion as well, you have
[00:27:44.560 --> 00:27:47.760]   the robots, you're just making lots of material, you know, I think that could compress, I mean,
[00:27:47.760 --> 00:27:52.240]   basically compress kind of like a century worth of technological progress into less than a decade.
[00:27:52.240 --> 00:27:56.160]   And that means that, you know, a couple years could mean a sort of Gulf War one style, like,
[00:27:56.160 --> 00:28:02.480]   you know, advantage in military affairs. And, you know, including, like, you know,
[00:28:02.480 --> 00:28:07.200]   a decisive advantage that even like preempt snoops, right? Suppose, like, you know, how do you find
[00:28:07.200 --> 00:28:10.240]   the stealth and nuclear submarines? Like, right now, that's a problem of like, you have sensors,
[00:28:10.240 --> 00:28:14.000]   you have the software, like tech where they are, you know, you can do that, you can find them,
[00:28:14.000 --> 00:28:17.680]   you have kind of like millions or billions of like mosquito, like, you know, size drones.
[00:28:17.680 --> 00:28:21.280]   And, you know, they take out the nuclear submarines, they take out the mobile launchers,
[00:28:21.280 --> 00:28:26.320]   they take out the other nukes. And anyway, so I think enormously destabilizing, enormously
[00:28:26.320 --> 00:28:31.520]   important for national power. And at some point, I think people are going to realize that not yet,
[00:28:31.520 --> 00:28:38.960]   but they will. And when they will, I think there will be sort of, you know, I don't think it'll
[00:28:38.960 --> 00:28:43.760]   just be the sort of AI researchers in charge. And, you know, I think on the, you know, the CCP is
[00:28:43.760 --> 00:28:47.440]   going to, you know, have sort of an all out effort to like infiltrate American AI labs, right, you
[00:28:47.440 --> 00:28:50.720]   know, like billions of dollars, thousands of people, you know, full force of the sort of, you
[00:28:50.720 --> 00:28:54.400]   know, Ministry of State Security. CCP is going to try to, you know, like outbuild us, right? Like
[00:28:54.400 --> 00:28:57.680]   they, you know, their, you know, power in China, you know, like the electric grid, you know,
[00:28:57.680 --> 00:29:02.800]   they added a U.S. is, you know, a complete, like, they added as much power in the last decade as
[00:29:02.800 --> 00:29:06.720]   like sort of entire U.S. electric grid. So like the 100 gigawatt cluster, at least 100 gigawatts
[00:29:06.720 --> 00:29:10.800]   is going to be a lot easier for them to get. And so I think sort of, you know, by this point,
[00:29:10.800 --> 00:29:14.240]   I think it's going to be like an extremely intense sort of international competition.
[00:29:15.360 --> 00:29:22.800]   Okay. So in this picture, one thing I'm uncertain about is whether it's more like what you say,
[00:29:22.800 --> 00:29:29.120]   where it's more of an implosion of you have developed an AGI and then you make it into an
[00:29:29.120 --> 00:29:36.160]   AI researcher. And for a while, a year or something, you're only using this ability to
[00:29:36.160 --> 00:29:40.240]   make hundreds of millions of other AI researchers. And then like the thing that comes out of this
[00:29:41.360 --> 00:29:46.000]   really frenetic process is a super intelligence. And then that goes out in the world and is
[00:29:46.000 --> 00:29:49.040]   developing robotics and helping you take over other countries and whatever.
[00:29:49.040 --> 00:29:51.440]   I think it's a little bit more, you know, it's a little bit more kind of like, you know, it's not
[00:29:51.440 --> 00:29:54.240]   like, you know, on and off. It's a little bit more gradual, but it's sort of like it's an
[00:29:54.240 --> 00:29:57.680]   explosion that starts narrowly. It's can do cognitive jobs. You know, the highest R.I. use
[00:29:57.680 --> 00:30:01.680]   for cognitive jobs is make the eye better, like solve robotics, you know, and as, as,
[00:30:01.680 --> 00:30:05.600]   as you solve robotics, now you can do R&D and, you know, like biology and other technology.
[00:30:05.600 --> 00:30:09.680]   You know, initially you start with the factory workers, you know, they're wearing the glasses
[00:30:09.680 --> 00:30:12.720]   and the AirPods, you know, and the AI is instructing them, right. Cause you know,
[00:30:12.720 --> 00:30:15.360]   you kind of make any worker into a skilled technician and then, you know, the robots
[00:30:15.360 --> 00:30:18.240]   come in and anyway, so it sort of expands. This process expands.
[00:30:18.240 --> 00:30:21.280]   - Meta's Ray-Bans are a compliment to their llama.
[00:30:21.280 --> 00:30:24.160]   - Well you know, it's like whatever, like, you know, the fabs in the U.S. the constraint is
[00:30:24.160 --> 00:30:27.280]   skilled workers, right? You have, you have the, even if you don't have robots that you have the
[00:30:27.280 --> 00:30:30.160]   cognitive super intelligence and, you know, it can kind of make them all into skilled workers
[00:30:30.160 --> 00:30:32.640]   immediately. But that's, you know, it's a very brief period, you know, robots will come soon.
[00:30:32.640 --> 00:30:38.240]   - Sure. Okay. Okay. So suppose this is actually how the tech progresses in the United States,
[00:30:38.240 --> 00:30:41.680]   maybe because these companies are already experiencing hundreds of billions of dollars
[00:30:41.680 --> 00:30:42.480]   of AI revenue.
[00:30:42.480 --> 00:30:45.120]   - And at this point, you know, companies are borrowing, you know, hundreds of billions
[00:30:45.120 --> 00:30:46.800]   of more in the corporate debt markets, you know.
[00:30:46.800 --> 00:30:51.200]   - But why is a CCP bureaucrat, some 60 year old guy, he looks at this and he's like,
[00:30:51.200 --> 00:30:55.120]   oh, it's like Copilot has gotten better now. Why are they now?
[00:30:55.120 --> 00:30:58.240]   - I mean, this is much more than Copilot has gotten better now. I mean, at this point.
[00:30:58.240 --> 00:31:03.520]   - But I'll add to that, like, yeah, so they're, cause to shift the production of an entire
[00:31:03.520 --> 00:31:10.880]   country to dislocate energy that is otherwise being used for consumer goods or something,
[00:31:10.880 --> 00:31:18.160]   and to make it that all feed into the data centers, what, part of this whole story is
[00:31:18.160 --> 00:31:22.720]   you realize the super intelligence is coming soon, right? And I guess you realize it, maybe
[00:31:22.720 --> 00:31:27.440]   I realize it, I'm not sure how much I realize it, but will the national security apparatus
[00:31:27.440 --> 00:31:29.440]   in the United States and will the CCP realize it?
[00:31:29.440 --> 00:31:32.560]   - Yeah. I mean, look, I think in some sense, this is a really key question.
[00:31:32.800 --> 00:31:37.200]   I think we have sort of a few more years of mid game basically, and where you have a few
[00:31:37.200 --> 00:31:43.280]   more 2023s and that just starts updating more and more people. And I think the trend lines
[00:31:43.280 --> 00:31:49.920]   will become clearer. I think you will see some amount of the sort of COVID dynamic,
[00:31:49.920 --> 00:31:55.840]   right? Like COVID was like, February of 2020, it's like, honestly feels a lot like today,
[00:31:55.840 --> 00:32:00.800]   where it's like, it feels like this utterly crazy thing is happening, is about, is impending,
[00:32:00.800 --> 00:32:05.040]   is coming. You kind of see the exponential and yet most of the world just doesn't realize,
[00:32:05.040 --> 00:32:09.360]   right? The mayor of New York is like, go out to the shows. And this is just like Asian racism or
[00:32:09.360 --> 00:32:18.080]   whatever, but at some point the exponential, at some point people saw it and then just kind of
[00:32:18.080 --> 00:32:22.720]   crazy radical reactions came. - Right. Okay. So by the way,
[00:32:22.720 --> 00:32:27.280]   what were you doing during COVID or when like February, like freshmen, sophomore, what?
[00:32:27.280 --> 00:32:30.240]   - Junior. - But still like a boy,
[00:32:30.240 --> 00:32:36.800]   like 17 year old junior or something. And then you bought, like, did you short the market or
[00:32:36.800 --> 00:32:37.600]   something or? - Yeah, yeah, yeah.
[00:32:37.600 --> 00:32:40.320]   - Okay. Did you, did you, did you sell at the right time?
[00:32:40.320 --> 00:32:41.040]   - Yeah. - Okay.
[00:32:41.040 --> 00:32:46.400]   Yeah. So there'll be like a March, 2020 moment that the thing that was COVID, but here.
[00:32:46.400 --> 00:32:52.240]   Now, then you can like make the analogy that you make in a series that this will then
[00:32:54.320 --> 00:32:59.520]   cause the reaction of like, we got to do the Manhattan project for America here. I wonder
[00:32:59.520 --> 00:33:04.480]   what the politics of this will be like, because the difference here is, it's not just like,
[00:33:04.480 --> 00:33:10.000]   we need the bomb to beat the Nazis. It's, we're building this thing that's making all our entry
[00:33:10.000 --> 00:33:14.560]   prices rise a bunch. And it's automating a bunch of our jobs and the climate change stuff. Like
[00:33:14.560 --> 00:33:17.680]   people are going to be like, oh my God, it's making climate change worse. And it's helping
[00:33:17.680 --> 00:33:22.560]   big tech. Like politically, this doesn't seem like a dynamic where the national security apparatus or
[00:33:22.560 --> 00:33:27.120]   the president is like, we have to step on the gas here and like make sure America wins.
[00:33:27.120 --> 00:33:32.960]   - Yeah. I mean, again, I think a lot of this really depends on sort of how much people are
[00:33:32.960 --> 00:33:40.080]   feeling it, how much people are seeing it. You know, I think there's a thing where, you know,
[00:33:40.080 --> 00:33:44.480]   kind of basically our generation, right. We're kind of so used to kind of, you know, basically
[00:33:44.480 --> 00:33:50.960]   peace and like, you know, the world, you know, American hegemony and nothing matters. But,
[00:33:50.960 --> 00:33:54.800]   you know, the sort of like extremely intense and these extraordinary things happening in the world
[00:33:54.800 --> 00:33:59.120]   and like intense international competition is like very much the historical norm. Like in some
[00:33:59.120 --> 00:34:04.480]   sense, it's like, you know, sort of there's this sort of 20 year, very unique period, but like,
[00:34:04.480 --> 00:34:10.000]   you know, the history of the world is like, you know, you know, like in World War II, right. It
[00:34:10.000 --> 00:34:14.720]   was like 50% of GDP went to, you know, like, you know, war production, the U.S. borrowed over 60%
[00:34:14.720 --> 00:34:20.400]   of GDP, you know, and in, you know, I think Germany and Japan over 100%, World War I, you know, UK,
[00:34:20.400 --> 00:34:29.120]   Japan, sorry, UK, France, Germany all borrowed over 100% of GDP. And, you know, I think the sort
[00:34:29.120 --> 00:34:33.680]   of much more was on the line, right. Like, you know, and, you know, people talk about World War
[00:34:33.680 --> 00:34:38.000]   I being so destructive and, you know, like 20 million Soviet soldiers dying and like 20% of
[00:34:38.000 --> 00:34:41.040]   Poland. But, you know, that was just the sort of like that happened all the time, right. You know,
[00:34:41.040 --> 00:34:45.680]   like seven years war, you know, like whatever, 20, 30% of Prussia died, you know, like 30 years war,
[00:34:45.680 --> 00:34:52.720]   you know, like, I think, you know, up to 50% of like large swath of Germany died. And,
[00:34:52.720 --> 00:35:01.520]   you know, I think the question is, will these sort of like, will people see that the stakes here are
[00:35:01.520 --> 00:35:06.960]   really, really high and that basically is sort of like history is actually back. And I think,
[00:35:06.960 --> 00:35:11.360]   you know, I think the American national security state thinks very seriously about stuff like this.
[00:35:11.360 --> 00:35:14.640]   They think very seriously about competition with China. I think China very much thinks of itself
[00:35:14.640 --> 00:35:18.720]   on this as a historical mission and rejuvenation of the Chinese nation, a lot about national power,
[00:35:18.720 --> 00:35:24.000]   I think a lot about like the world order. And then, you know, I think there's a real question
[00:35:24.000 --> 00:35:28.160]   on timing, right? Like, do they start taking this seriously, right? Like when the intelligence
[00:35:28.160 --> 00:35:31.520]   explosion is already happening, like quite late, or do they start taking this seriously like two
[00:35:31.520 --> 00:35:35.280]   years earlier? And that matters a lot for how things play out. But at some point they will,
[00:35:35.280 --> 00:35:40.240]   and at some point they will realize that this will be sort of utterly decisive for,
[00:35:40.240 --> 00:35:45.680]   you know, not just kind of like some proxy war somewhere, but, you know, like whether liberal
[00:35:45.680 --> 00:35:49.600]   democracy can continue to thrive, whether, you know, whether the CCP will continue existing.
[00:35:49.600 --> 00:35:54.400]   And I think that will activate sort of forces that we haven't seen in a long time.
[00:35:54.400 --> 00:36:01.600]   The great power conflict thing definitely seems compelling. I think just all kinds of different
[00:36:01.600 --> 00:36:06.240]   things seem much more likely when you think from a historical perspective, when you zoom out beyond
[00:36:06.240 --> 00:36:10.480]   the liberal democracy that we've been living in, had the pleasure to live in America, let's say 80
[00:36:10.480 --> 00:36:17.680]   years, including dictatorships, including obviously war, famine, whatever. I was reading the Gulag
[00:36:17.680 --> 00:36:22.640]   Archipelago, and one of the chapters begins with Sojourn Eason saying, if you would have told a
[00:36:22.640 --> 00:36:27.520]   Russian citizen under the czars that because of all these new technologies, we wouldn't see some
[00:36:27.520 --> 00:36:33.040]   great Russian revival or becomes a great power and the citizens are made wealthy. But instead,
[00:36:33.040 --> 00:36:39.360]   what you would see is tens of millions of Soviet citizens tortured by millions of beasts in the
[00:36:39.360 --> 00:36:44.480]   worst possible ways, and that this is what would be the result of the 20th century. They wouldn't
[00:36:44.480 --> 00:36:50.080]   have believed you. They'd have called you a slanderer. Yeah. And, you know, the, you know,
[00:36:50.080 --> 00:36:53.840]   the possibilities for dictatorship with superintelligence are sort of even crazier,
[00:36:53.840 --> 00:36:57.680]   right? I think, you know, imagine you have a perfectly loyal military and security force,
[00:36:57.680 --> 00:37:02.560]   right? That's it. No more, no more rebellions, right? No more popular uprisings, you know,
[00:37:02.560 --> 00:37:06.320]   perfectly loyal, you know, you have, you know, perfect lie detection, you know, you have
[00:37:06.320 --> 00:37:09.760]   surveillance of everybody, you know, you can perfectly figure out who's the dissenter,
[00:37:09.760 --> 00:37:13.200]   weed them out, you know, no Gorbachev would have ever risen to power who had some doubts about the
[00:37:13.200 --> 00:37:18.560]   system, you know, no military coup would have ever happened. And I think you, I mean, you know,
[00:37:18.560 --> 00:37:24.640]   I think there's a real way in which, you know, part of why things have worked out is that,
[00:37:24.640 --> 00:37:30.000]   you know, ideas can evolve. And, you know, there's sort of like some, some sense in which sort of
[00:37:30.000 --> 00:37:34.320]   time heals a lot of wounds and time, you know, and salt solves, solves, you know, a lot of debates.
[00:37:34.320 --> 00:37:37.440]   And a lot of people had really strong convictions, but, you know, a lot of those have been overturned
[00:37:37.440 --> 00:37:40.880]   by time because there's been this continued pluralism and evolution. I think there's a way
[00:37:40.880 --> 00:37:44.800]   in which kind of like, you know, if you take a CCP like approach to kind of like truth, truth is
[00:37:44.800 --> 00:37:48.560]   what the party says, and you supercharge that with super intelligence, I think there's a way in which
[00:37:48.560 --> 00:37:53.280]   that could just be like locked in and trying for, you know, a long time. And I think the possibilities
[00:37:53.280 --> 00:37:59.360]   are pretty terrifying. You know, your point about, you know, history and sort of like living in
[00:37:59.360 --> 00:38:03.680]   America for the past eight years, you know, I think this is one of the things I sort of took
[00:38:03.680 --> 00:38:07.440]   away from growing up in Germany is a lot of this stuff feels more visceral, right? Like, you know,
[00:38:07.440 --> 00:38:11.120]   my mother grew up in the former East, my father in the former West, they like met shortly after
[00:38:11.120 --> 00:38:14.960]   the wall fell, right? Like, the end of the Cold War was the sort of extremely pivotal moment for
[00:38:14.960 --> 00:38:18.800]   me, because it's the reason I exist, right? And then, you know, growing up in Berlin, and, you
[00:38:18.800 --> 00:38:25.120]   know, former wall, you know, my great grandmother, who is, you know, still alive is very important in
[00:38:25.120 --> 00:38:29.920]   my life. You know, she was born in 34, you know, grew up, you know, during the Nazi era during,
[00:38:29.920 --> 00:38:33.920]   you know, all that, you know, then World War Two, you know, like saw the firebombing of Dresden from
[00:38:33.920 --> 00:38:38.640]   the sort of, you know, country cottage or whatever were, you know, they as kids were, you know, then
[00:38:38.640 --> 00:38:42.720]   and then, you know, then spends most of her life in sort of the East German communist dictatorship,
[00:38:42.720 --> 00:38:46.640]   you know, she'd tell me about, you know, in like, 54, when there's like the popular uprising,
[00:38:46.640 --> 00:38:50.320]   you know, and Soviet tanks came in, you know, her husband was telling her to get home really
[00:38:50.320 --> 00:38:55.680]   quickly, you know, get off off the streets, you know, had a son who tried to, you know,
[00:38:55.680 --> 00:39:00.080]   ride a motorcycle across the Iron Curtain, and then was put in a Stasi prison for a while.
[00:39:00.080 --> 00:39:06.720]   You know, and then finally, you know, when she's almost 60, you know, it's the first time she lives
[00:39:06.720 --> 00:39:14.720]   in, you know, a free country, and a wealthy country. And, you know, when I was a kid, she was,
[00:39:14.720 --> 00:39:19.040]   she the thing she always really didn't want me to do was like get involved in politics,
[00:39:19.040 --> 00:39:23.680]   because like joining a political party was just, you know, it was a very bad connotations for her.
[00:39:23.680 --> 00:39:27.600]   Anyway, and she and she sort of raised me when I was young, you know, and so
[00:39:27.600 --> 00:39:32.160]   it, you know, it doesn't feel that long ago, it feels very close.
[00:39:32.160 --> 00:39:38.480]   Yeah. So I wonder when we're talking today about the CCP, listen, the people in China who will be
[00:39:38.480 --> 00:39:45.040]   doing the product, their version of the project will be AI researchers who are somewhat westernized
[00:39:45.040 --> 00:39:50.240]   who interact with either got educated in the West or have colleagues in the West.
[00:39:50.240 --> 00:39:58.400]   Are they going to sign up for the the CCP project that's going to hand over
[00:39:59.200 --> 00:40:03.520]   control to Xi Jinping? What's your sense on? I mean, it's just like, fundamentally,
[00:40:03.520 --> 00:40:06.320]   they're just people, right? Like, can't you like convince them about the dangerous
[00:40:06.320 --> 00:40:07.840]   superintelligence? Will they be in charge, though?
[00:40:07.840 --> 00:40:11.440]   I mean, in some sense, this is I mean, this is also the case, you know,
[00:40:11.440 --> 00:40:16.480]   you know, in the US or whatever, this is sort of like rapidly depreciating influence of the
[00:40:16.480 --> 00:40:19.920]   lab employees. Like right now, the sort of AI lab employees have so much power,
[00:40:19.920 --> 00:40:25.200]   right over this, you know, like, you saw this November event, so much power, right? But both,
[00:40:25.200 --> 00:40:28.080]   I mean, both, they're going to get automated, and they're going to lose all their power.
[00:40:28.080 --> 00:40:31.360]   And it'll just be, you know, kind of like a few people in charge with their sort of armies of
[00:40:31.360 --> 00:40:36.240]   automated eyes. But also, you know, it's sort of like the politicians and the generals and the
[00:40:36.240 --> 00:40:38.800]   sort of national security state, you know, a lot of, you know, it's I mean, there's sort of this
[00:40:38.800 --> 00:40:42.160]   is the sort of some of these classic scenes from the Oppenheimer movies, you know, the scientists
[00:40:42.160 --> 00:40:45.440]   built it, and then it was kind of, you know, and the bomb was shipped away and was out of their
[00:40:45.440 --> 00:40:50.080]   hands. You know, I actually, I think I actually think it's good for like lab employees to be
[00:40:50.080 --> 00:40:55.040]   aware of this is like, you have a lot of power now, but, you know, maybe not for that long,
[00:40:55.040 --> 00:40:59.920]   and, you know, use it wisely. Yeah, I do. I do think they would benefit from some more,
[00:40:59.920 --> 00:41:03.520]   you know, organs of representative democracy. What do you mean by that? Oh, I mean, I, you know,
[00:41:03.520 --> 00:41:07.120]   in the sort of the in the open AI board events, you know, employee power is exercised in a very
[00:41:07.120 --> 00:41:10.720]   sort of direct democracy way. And I feel like that's how some of how that went about, you know,
[00:41:10.720 --> 00:41:13.680]   I think it really highlighted the benefits of representative democracy and having some
[00:41:13.680 --> 00:41:19.040]   deliberative organs. Interesting. Yeah. Well, let's go back to the 100 billion revenue,
[00:41:19.040 --> 00:41:23.600]   whatever. And so these companies are now a cluster. Yeah, the companies are deploying,
[00:41:23.600 --> 00:41:27.600]   we're trying to build clusters that are this big. Yeah. Where are they building it? Because if you
[00:41:27.600 --> 00:41:32.160]   say it's the amount of energy that would require for a small or medium sized US state, is it then
[00:41:32.160 --> 00:41:35.600]   Colorado gets no power and it's happening in the United States? Or is it happening somewhere else?
[00:41:35.600 --> 00:41:38.800]   Oh, I mean, I think that I mean, in some sense, this is the thing that I always find funny is,
[00:41:38.800 --> 00:41:41.520]   you know, you're talking about Colorado gets no power, you know, the easy way to get the power
[00:41:41.520 --> 00:41:44.880]   would be like, you know, displaced, less economically useful stuff, you know, it's like,
[00:41:44.880 --> 00:41:48.240]   whatever, buy up the aluminum smelting plant. And you know, that has a gig a lot. And you know,
[00:41:48.240 --> 00:41:51.520]   we're gonna replace it with with the data center, because that's important. I mean,
[00:41:51.520 --> 00:41:54.320]   that's not actually happening, because a lot of these power contracts are really sort of long
[00:41:54.320 --> 00:41:58.240]   term locked in, you know, because obviously, people don't like things like this. And so it's
[00:41:58.240 --> 00:42:01.680]   sort of it seems like in practice, what it's what it's requiring, at least right now is building new
[00:42:01.680 --> 00:42:05.520]   power, the that might change. And I think that that's when things get really interesting when
[00:42:05.520 --> 00:42:09.360]   it's like, no, we're just dedicating all of the power to the AGI. So right now, it's building new
[00:42:09.360 --> 00:42:15.120]   power 10 gigawatt, I think quite doable. It's like a few percent of like us natural gas production.
[00:42:15.120 --> 00:42:20.240]   You know, when you have the 10 gigawatt chaining cluster, you have a lot more inference. So that
[00:42:20.240 --> 00:42:23.520]   starts getting more, you know, I think 100 gigawatt that starts getting pretty wild. You
[00:42:23.520 --> 00:42:28.720]   know, that's, you know, again, it's like, over 20% of us electricity production. I think it's
[00:42:28.720 --> 00:42:34.240]   pretty doable, especially if you're willing to go for like natural gas. But I do I do think I do
[00:42:34.240 --> 00:42:37.920]   think it is incredibly important, incredibly important that these clusters are in the United
[00:42:37.920 --> 00:42:45.280]   States, when why does it matter? It's in the US. I mean, look, I think there's some people who are,
[00:42:45.280 --> 00:42:49.200]   you know, trying to build clusters elsewhere. And you know, there's like a lot of free flowing
[00:42:49.200 --> 00:42:54.800]   Middle Eastern money that's trying to build clusters elsewhere. I think this comes back to
[00:42:54.800 --> 00:42:57.600]   the sort of like national security question we talked about earlier, like, would you I mean,
[00:42:57.600 --> 00:43:01.440]   would you do the Manhattan Project in the UAE, right? And I think I think basically, like putting
[00:43:01.440 --> 00:43:04.720]   putting the clusters, you know, I think you can put them in the US, you can put them in sort of
[00:43:04.720 --> 00:43:08.560]   like ally democracies. But I think once you put them in kind of like, you know, dictatorships,
[00:43:08.560 --> 00:43:11.680]   authoritarian dictatorships, you kind of create this, you know, irreversible security risk,
[00:43:11.680 --> 00:43:16.560]   right? So I mean, one cluster is there much easier for them to exfiltrate the weights. You know,
[00:43:16.560 --> 00:43:20.720]   they can like literally steal the AGI, the super intelligence, it's like they got a copy of the,
[00:43:20.720 --> 00:43:24.160]   you know, of the of the atomic bomb, you know, and they just got a direct replica of that.
[00:43:24.160 --> 00:43:27.920]   And it makes it much easier to them. I mean, we're ties to China, you can ship that to China.
[00:43:27.920 --> 00:43:31.680]   So that's a huge risk. Another thing is they can just seize the compute, right? Like, maybe right
[00:43:31.680 --> 00:43:34.640]   now, they just think of this. I mean, in general, I think people, you know, I think the issue here
[00:43:34.640 --> 00:43:37.840]   is people are thinking of this as a, you know, chat GPT, big tech product clusters, but I think
[00:43:37.840 --> 00:43:42.240]   the clusters being planned now, you know, three to five years out, like, it will be the like AGI
[00:43:42.240 --> 00:43:45.840]   super intelligence clusters. And so anyway, so like, when things get hot, you know, they might
[00:43:45.840 --> 00:43:50.640]   just seize the compute. And I don't know, suppose we put like, you know, 25% of the compute capacity
[00:43:50.640 --> 00:43:54.320]   in the sort of Middle Eastern decoder ships, well, they seize that. And now it's sort of a ratio of
[00:43:54.320 --> 00:43:59.120]   compute of three to one, and you know, still have some more, but even like even even only only 25%
[00:43:59.120 --> 00:44:02.560]   of compute there, like, I think it starts getting pretty hairy, you know, I think three to one is
[00:44:02.560 --> 00:44:07.200]   like, not that great of a ratio, you can do a lot with that amount of compute. And then look, even
[00:44:07.200 --> 00:44:09.760]   even if they don't actually do this, right, even they don't actually seize the compute, even
[00:44:09.760 --> 00:44:14.080]   actually don't steal the weights. There's just a lot of implicit leverage you get, right, they get
[00:44:14.080 --> 00:44:21.520]   they get the seat at the AGI table. And, you know, I don't know why we're giving authoritarian
[00:44:21.520 --> 00:44:27.040]   dictatorships the seat at the AGI table. Okay, so there's going to be a lot of compute in the
[00:44:27.040 --> 00:44:31.600]   Middle East if these deals go through. First of all, who's who is it just like every single big
[00:44:31.600 --> 00:44:35.600]   tech company is just trying to figure out what everyone's on. Okay, okay. Well, I guess there's
[00:44:35.600 --> 00:44:43.360]   reports, I think Microsoft or Yeah, which we'll get into. Yeah. So they UAE gets a bunch of compute
[00:44:43.360 --> 00:44:50.160]   because we're building the clusters there. And why? So let's say they have 25%. Why does a compute
[00:44:50.160 --> 00:44:56.640]   ratio matter? Is it if it's about them being able to kick off the intelligence explosion?
[00:44:56.640 --> 00:45:00.880]   Isn't it just some threshold where you have 100 million AI researchers or you don't?
[00:45:00.880 --> 00:45:04.560]   I mean, you do a lot with, you know, 33 million extremely smart scientists.
[00:45:04.560 --> 00:45:09.280]   And, you know, again, a lot of the stuff, you know, so first of all, it's like, you know,
[00:45:09.280 --> 00:45:12.160]   that might be enough to build the crazy bioweapons, right? And then you're in a situation
[00:45:12.160 --> 00:45:15.200]   where like, now, wow, we've just like, they stole the weights, they seize the compute.
[00:45:15.200 --> 00:45:19.840]   Now they can make, you know, they can build these crazy new WMDs that, you know, will be possible
[00:45:19.840 --> 00:45:23.040]   super intelligence. And now you've just kind of like proliferated the stuff and, you know,
[00:45:23.040 --> 00:45:28.560]   it'll be really powerful. And also, I mean, I think, you know, three to three acts on compute
[00:45:28.560 --> 00:45:36.000]   isn't actually that much. And so the, you know, the, you know, I think a thing I worry a lot about
[00:45:36.000 --> 00:45:42.800]   is I think everything, I think the riskiest situation is if we're in some sort of like
[00:45:42.800 --> 00:45:48.240]   really tight neck feverish international struggle, right? If we're like really close with the CCP
[00:45:48.240 --> 00:45:52.960]   and we're like months apart, I think the situation we want to be in, we could be in if we played our
[00:45:52.960 --> 00:45:56.640]   cards right, is a little bit more like, you know, the US, you know, building the atomic bomb versus
[00:45:56.640 --> 00:46:01.920]   the German project way behind, you know, years behind. And if we have that, I think we just have
[00:46:01.920 --> 00:46:04.720]   so much more wiggle room, like to get safety, right? We're going to be building like, you know,
[00:46:04.720 --> 00:46:07.760]   there's going to be these crazy new WMDs, you know, things that completely undermine,
[00:46:07.760 --> 00:46:13.600]   you know, nuclear deterrence, you know, intense competition. And that's so much easier to deal
[00:46:13.600 --> 00:46:16.560]   with if, you know, you're like, you know, it's not just, you know, you don't have somebody right on
[00:46:16.560 --> 00:46:20.640]   your tails. You got to go, go, go. You got to go maximum speed. You have no wiggle room. You're
[00:46:20.640 --> 00:46:24.000]   worried that at any time they can overtake you. I mean, they can also just try to outbuild you,
[00:46:24.000 --> 00:46:28.080]   right? Like they can might, they might literally win. Like China might literally win if they can
[00:46:28.080 --> 00:46:32.400]   steal the weights because they can, they can outbuild you. And they maybe have less caution,
[00:46:32.400 --> 00:46:35.680]   both, you know, good and bad caution, you know, kind of like whatever unreasonable regulations we
[00:46:35.680 --> 00:46:40.880]   have. Or you're just in this really tight race. And I think it's that sort of like, if you're in
[00:46:40.880 --> 00:46:44.080]   this really tight race, this sort of feverish struggle, I think that's when sort of there's
[00:46:44.080 --> 00:46:49.280]   the greatest peril of self-destruction. So then presumably the companies that are
[00:46:49.280 --> 00:46:52.560]   trying to build clusters in the Middle East realize this, what is the, is it just that it's
[00:46:52.560 --> 00:46:55.920]   impossible to do this in America? And if you want American companies to do this at all,
[00:46:55.920 --> 00:46:58.560]   then you do it in Middle East or not at all. And then you're just like, I'm trying to build
[00:46:58.560 --> 00:47:01.600]   the three gorgeous damn cluster. I mean, there's a few reasons. I mean, one of them is just like,
[00:47:01.600 --> 00:47:04.480]   people aren't thinking about this as the EGI superintelligence cluster. They're just like,
[00:47:04.480 --> 00:47:07.440]   ah, you know, like cool clusters for my, you know, for my chat.
[00:47:07.440 --> 00:47:12.640]   But basically they're building in the plans right now are clusters, which are ones that are like,
[00:47:12.640 --> 00:47:14.720]   because if you're doing once for inference, presumably you could like spread them out
[00:47:14.720 --> 00:47:18.080]   across the country or something. But the ones they're building, they realize we're going to do
[00:47:18.080 --> 00:47:23.600]   one training run in this thing we're building. I just think it's harder to distinguish between
[00:47:23.600 --> 00:47:26.320]   inference and training compute. And so people can claim it's training compute. But I think
[00:47:26.320 --> 00:47:29.920]   they might realize that actually, you know, this is going to be useful for, you know, sorry,
[00:47:29.920 --> 00:47:32.480]   they might say it's inference compute. And actually it's useful for training compute too.
[00:47:32.480 --> 00:47:34.960]   Because of synthetic data and things like that.
[00:47:34.960 --> 00:47:38.000]   Yeah. The future of training, you know, like RL looks a lot like inference, for example. Right.
[00:47:38.000 --> 00:47:41.200]   Or, or you just kind of like end up connecting them, you know, in time, you know, it's,
[00:47:41.200 --> 00:47:44.480]   you have this like raw material, you know, it's like, you know, it's, it's, it's placing your
[00:47:44.480 --> 00:47:47.840]   uranium refinement facilities there. Sure. So a few reasons, right. One is just like,
[00:47:47.840 --> 00:47:51.120]   they don't think about this as the EGI cluster. Another is just like easy mining from the Middle
[00:47:51.120 --> 00:47:57.360]   East. Right. Another one is like, you know, people saying some people think that, you know,
[00:47:57.360 --> 00:48:02.640]   you can't do it in the U.S. And, you know, I think we actually face a sort of real system
[00:48:02.640 --> 00:48:05.760]   competition here because again, some people think it's only autocracies that can do this,
[00:48:05.760 --> 00:48:09.600]   that can kind of like top down, mobilize the sort of industrial capacity, the power,
[00:48:09.600 --> 00:48:13.520]   you know, get the stuff done fast. And again, this is the sort of thing, you know, we haven't
[00:48:13.520 --> 00:48:17.920]   faced in a while. But, you know, during the Cold War, like we really, there was this sort
[00:48:17.920 --> 00:48:21.680]   of intense system competition, right. Like East West Germany was this, right. Like West Germany,
[00:48:21.680 --> 00:48:26.160]   kind of like liberal democratic capitalism versus kind of, you know, communist state plan.
[00:48:26.160 --> 00:48:31.040]   And, you know, now it's obvious that the sort of, you know, the free world would win. But,
[00:48:31.040 --> 00:48:34.960]   you know, even as late as like '61, you know, Paul Samuelson was predicting that the Soviet
[00:48:34.960 --> 00:48:38.640]   Union would outgrow the United States because they were able to sort of mobilize industry better.
[00:48:38.640 --> 00:48:44.240]   And so, yeah, there's some people who, you know, shitpost about loving America by day,
[00:48:44.240 --> 00:48:47.200]   but then in private they're betting against America. They're betting against the liberal
[00:48:47.200 --> 00:48:51.200]   order. And I think, I basically just think it's a bad bet. And the reason I think it's
[00:48:51.200 --> 00:48:54.960]   a bad bet is I think this stuff is just really possible in the U.S. And so to make it possible
[00:48:54.960 --> 00:48:58.000]   in the U.S., there's some amount that we have to get our act together, right. So I think there's
[00:48:58.000 --> 00:49:02.000]   basically two paths to doing it in the U.S. One is you just got to be willing to do natural gas.
[00:49:02.000 --> 00:49:05.600]   And there's ample natural gas, right. You put your cluster in West Texas, you put it in, you know,
[00:49:05.600 --> 00:49:10.640]   Southwest Pennsylvania by the, you know, Marcello Shale. 10 gigawatt cluster is super easy. 100
[00:49:10.640 --> 00:49:15.120]   gigawatt cluster, also pretty doable. You know, I think, you know, natural gas production in the
[00:49:15.120 --> 00:49:18.720]   United States has, you know, almost doubled in a decade. If you do that, you know, one more time
[00:49:18.720 --> 00:49:23.120]   over the next, you know, seven years or whatever, you know, you could power multiple trillion dollar
[00:49:23.120 --> 00:49:27.840]   data centers. But the issue there is, you know, a lot of people have sort of these made these
[00:49:27.840 --> 00:49:31.040]   climate commitments and not just government. It's actually the private companies themselves,
[00:49:31.040 --> 00:49:34.960]   right. The Microsoft, the Amazons and so on have these climate commitments. So they won't do natural
[00:49:34.960 --> 00:49:39.120]   gas. And, you know, I admire the climate commitments, but I think at some point, you know,
[00:49:39.120 --> 00:49:42.560]   the national interest and national security kind of is more important.
[00:49:42.560 --> 00:49:47.280]   The other path is like, you know, you can do this sort of green energy mega projects, right. You do
[00:49:47.280 --> 00:49:53.760]   the solar and the batteries and the, you know, the SMRs and geothermal. But if we want to do that,
[00:49:53.760 --> 00:49:57.840]   there needs to be sort of a sort of broad regulatory push, right. So like, you can't
[00:49:57.840 --> 00:50:01.360]   have permitting take a decade, right. So you got to reform FERC. You got to like have, you know,
[00:50:01.360 --> 00:50:06.000]   blanket NEPA exemptions for this stuff. There's like inane state level regulations, you know,
[00:50:06.000 --> 00:50:09.360]   that are like, yeah, you could build, you know, build the solar panels and batteries next to your
[00:50:09.360 --> 00:50:13.360]   data center, but it'll still take years because, you know, you actually have to hook it up to the
[00:50:13.360 --> 00:50:18.960]   state electrical grid, you know, and you'll have to like use governmental powers to create rights
[00:50:18.960 --> 00:50:22.400]   away to kind of like, you know, have multiple clusters and connect them, you know, and have
[00:50:22.400 --> 00:50:27.280]   the cables basically. And so, look, I mean, ideally we do both, right. Ideally, we do natural gas and
[00:50:27.280 --> 00:50:31.120]   the broad regulatory agenda. I think we have to do at least one. And then I think this possible
[00:50:31.120 --> 00:50:36.400]   stuff is just possible in the United States. Yeah. I think a good analogy for this, by the way,
[00:50:36.400 --> 00:50:40.880]   before the conversation I was reading, there's a good book about World War Two industrial
[00:50:40.880 --> 00:50:46.560]   mobilization in the United States called Freedom's Forge. And I guess when we think back on that
[00:50:46.560 --> 00:50:51.600]   period, especially if you're from, if you read like the Patrick Hollis and FAST and the progress
[00:50:51.600 --> 00:50:57.520]   study stuff, it's like you had state capacity back then and people just got you done, but now
[00:50:57.520 --> 00:51:02.000]   it's a clusterfuck. Wasn't it all the case? No, so it was really interesting. So you have people
[00:51:02.000 --> 00:51:08.240]   who are from the Detroit auto industry side, like Knudsen, who are running mobilization for the
[00:51:08.240 --> 00:51:13.360]   United States and they were extremely competent. Yeah. But then at the same time, you had labor
[00:51:13.360 --> 00:51:18.080]   organization agitation, which is actually very analogous to the climate pledges and climate
[00:51:18.080 --> 00:51:24.880]   change concern we have today. Yeah. Where they would have these strikes while literally into 1941
[00:51:24.880 --> 00:51:31.840]   that would cost millions of man hours worth of time when we're trying to make tens of millions,
[00:51:31.840 --> 00:51:37.360]   sorry, tens of thousands of planes a month or something. And they would just debilitate
[00:51:37.360 --> 00:51:43.840]   factories before, you know, trivial, like pennies on the dollar kind of concessions from capital.
[00:51:43.840 --> 00:51:51.280]   And it was concerns that, oh, the auto companies are trying to use the pretext of a potential war
[00:51:51.280 --> 00:51:57.760]   to actually prevent paying labor that money deserves. And so what climate change is today,
[00:51:57.760 --> 00:52:01.040]   like you think, ah, fuck America's fucked, like we're not going to be able to build this shit.
[00:52:01.040 --> 00:52:05.760]   Like if you, if you look at NEPA or something, but I didn't realize how debilitating labor was.
[00:52:05.760 --> 00:52:10.560]   And it was just, you know, before at the, you know, it was like 39 or whatever the American
[00:52:10.560 --> 00:52:13.760]   military was in total shambles. Right. You read about it and it reads a little bit like, you know,
[00:52:13.760 --> 00:52:17.360]   the German military today. Right. It's like, you know, military expenditures, I think, were less
[00:52:17.360 --> 00:52:21.040]   than 2% of GDP. You know, all the European countries had gone even in peacetime, you know,
[00:52:21.040 --> 00:52:24.960]   like above 10% of GDP, sort of this like rapid mobilization. There's nothing, you know, like
[00:52:24.960 --> 00:52:28.560]   we're making kind of like no planes. There's no military contracts. Everything had been starved
[00:52:28.560 --> 00:52:32.720]   during the Great Depression. But there was this latent capacity. And, you know, at some point,
[00:52:32.720 --> 00:52:36.320]   the United States got their act together. I mean, the thing I'll say is, I think, you know,
[00:52:36.320 --> 00:52:40.480]   the supply is sort of the other way around too, to basically to China. Right. And I think sometimes
[00:52:40.480 --> 00:52:44.080]   people are, you know, they kind of count them out a little bit and they're like the export controls
[00:52:44.080 --> 00:52:47.760]   and so on. And, you know, they're able to make some nanometer chips now. I think there's a
[00:52:47.760 --> 00:52:51.440]   question of like, how many could they make? But, you know, I think there's at least a possibility
[00:52:51.440 --> 00:52:54.640]   that they're going to be able to mature that ability and make a lot of seven nanometer chips.
[00:52:54.640 --> 00:52:58.960]   And there's a lot of latent industrial capacity in China and they are able to like, you know,
[00:52:58.960 --> 00:53:03.680]   build a lot of power fast. And maybe that isn't activated for AI yet. But at some point, you know,
[00:53:03.680 --> 00:53:07.760]   the same way the United States and like, you know, a lot of people in the US and the United States
[00:53:07.760 --> 00:53:10.400]   government is going to wake up, you know, at some point the CCP is going to wake up.
[00:53:10.400 --> 00:53:17.680]   Yeah. Okay. Going back to the question of presumably companies, if are they blind to
[00:53:17.680 --> 00:53:21.760]   the fact that there's going to be some sort of well, okay, so they realize that there's going
[00:53:21.760 --> 00:53:25.760]   they realize scaling is a thing, right? Obviously, their whole plans are contingent on scaling.
[00:53:25.760 --> 00:53:30.800]   And so they understand that we're going to be in 2028, building the 10 gigawatt data centers. And
[00:53:30.800 --> 00:53:36.640]   at this point, the people who can keep up are big tech, just potentially at like the edge of their
[00:53:36.640 --> 00:53:43.120]   capabilities. Yeah. Then sovereign wealth fund funded things. Yeah. And also big major countries
[00:53:43.120 --> 00:53:50.000]   like America, China, whatever. Yeah. So what's their plan? If you look at like these AI labs,
[00:53:50.000 --> 00:53:54.720]   what's their plan, given this landscape? Do they not want the leverage of having being in the
[00:53:54.720 --> 00:53:58.720]   United States? I mean, I think I don't know. I think I mean, what one thing the Middle East
[00:53:58.720 --> 00:54:04.320]   does offer is capital, but it's like America has plenty of capital, right? It's like, you know,
[00:54:04.320 --> 00:54:06.800]   we have trillion dollar companies, like what are these Middle Eastern states? They're kind
[00:54:06.800 --> 00:54:10.240]   of like trillion dollar oil companies. We have trillion dollar companies, and we have very deep
[00:54:10.240 --> 00:54:13.200]   financial markets. And it's like, you know, Microsoft could issue hundreds of billions
[00:54:13.200 --> 00:54:17.440]   dollars of bonds, and they can pay for these clusters. I mean, look, I think another argument
[00:54:17.440 --> 00:54:21.280]   being made, and I think it's worth taking seriously is an argument that look, if we
[00:54:21.280 --> 00:54:25.680]   don't work with the UAE or with these Middle Eastern countries, they're just going to go to
[00:54:25.680 --> 00:54:29.760]   China, right? And so, you know, they're going to build data centers, they're going to pour money
[00:54:29.760 --> 00:54:33.840]   into AI regardless. And if we don't work with them, you know, they'll just support China.
[00:54:33.840 --> 00:54:41.200]   And look, I mean, I think there's some merit to the argument and in the sense that I think we
[00:54:41.200 --> 00:54:43.680]   should be doing basically benefit sharing with them, right? I think we should talk about this
[00:54:43.680 --> 00:54:47.120]   later, but I think basically sort of on the road to AGI, there should be kind of like two tiers of
[00:54:47.120 --> 00:54:51.200]   coalitions should be the sort of narrow coalition of democracies, that's sort of the coalition that's
[00:54:51.200 --> 00:54:55.280]   developing AGI. And then there should be a broader coalition where we kind of go to other countries,
[00:54:55.280 --> 00:55:00.240]   including, you know, dictatorships, and we're willing to offer them, you know, we're willing
[00:55:00.240 --> 00:55:04.320]   to offer them some of the benefits of AI, some of the sharing. So it's like, look, if the UAE wants
[00:55:04.320 --> 00:55:08.080]   to use AI products, if they want to run, you know, meta recommendation engines, if they want to run,
[00:55:08.080 --> 00:55:12.480]   you know, like last generation models, that's fine. I think by default, they just like wouldn't have
[00:55:12.480 --> 00:55:16.640]   had this seat at the AGI table, right? And so it's like, yeah, they have some money, but a lot
[00:55:16.640 --> 00:55:22.320]   of people have money. And, you know, the only reason they're getting the sort of course at the
[00:55:22.320 --> 00:55:25.760]   AGI table, the only reason we're giving these dictators will have this enormous amount of
[00:55:25.760 --> 00:55:32.560]   leverage over this extremely national security relevant technology is because we're, you know,
[00:55:32.560 --> 00:55:38.080]   we're kind of getting them excited and offering it to them. You know, I think, yeah,
[00:55:38.080 --> 00:55:41.520]   yeah, who like who specifically is doing this, like, just the companies who are going there to
[00:55:41.520 --> 00:55:45.360]   fundraise are like, this is the AGI is happening, and you can find it or you can't been reported
[00:55:45.360 --> 00:55:49.280]   that it's been reported that, you know, Sam is trying to raise, you know, 7 trillion or whatever
[00:55:49.280 --> 00:55:52.880]   for a chip project. And, you know, it's unclear how many of the clusters will be there and so on.
[00:55:52.880 --> 00:55:56.560]   But it's, you know, definitely, definitely stuff is happening. I mean, look, I think another reason
[00:55:56.560 --> 00:56:00.640]   I'm a little bit, at least suspicious of this argument of like, look, if the US doesn't work
[00:56:00.640 --> 00:56:06.480]   with them, they'll go to China is, you know, I've heard heard from multiple people. And this wasn't,
[00:56:06.480 --> 00:56:09.600]   you know, for my time at OpenAI, and I haven't seen the memo, but I have heard from multiple
[00:56:09.600 --> 00:56:14.560]   people that, you know, at some point several years ago, OpenAI leadership had sort of laid
[00:56:14.560 --> 00:56:18.960]   out a plan to fund and sell AGI by starting a bidding war between the governments of, you know,
[00:56:18.960 --> 00:56:23.760]   the United States, China and Russia. And so, you know, it's kind of surprising to me that they're
[00:56:23.760 --> 00:56:27.440]   willing to sell AGI to the Chinese and Russian governments. But also, there's something that
[00:56:27.440 --> 00:56:31.360]   sort of feels a bit eerily familiar about kind of starting this bidding war, and then kind of like
[00:56:31.360 --> 00:56:36.400]   playing them off each other. And well, you know, if you don't do this, China will do it. So anyway,
[00:56:36.400 --> 00:56:42.480]   interesting. Okay, so that's pretty fucked up. But given that, that's, okay, so suppose that you're
[00:56:42.480 --> 00:56:47.760]   right about we ended up in this place, because we got one, the way one of our friends put it is that
[00:56:47.760 --> 00:56:54.000]   the Middle East has, like, no other place in the world, billions of dollars or trillions of dollars
[00:56:54.000 --> 00:57:02.240]   up for persuasion. And like, you know, the Microsoft board, it's only it's only the dictator.
[00:57:02.240 --> 00:57:06.560]   Yeah. But so let's say you're right that you shouldn't have gotten them excited about AGI
[00:57:06.560 --> 00:57:11.920]   in the first place. But now we're in a place where they are excited about AGI. And they're like,
[00:57:11.920 --> 00:57:15.440]   fuck, we want us to have GP five, we're going to be off building super intelligence,
[00:57:15.440 --> 00:57:20.000]   this Adam Sarpis thing doesn't work for us. And if you're in this place,
[00:57:20.000 --> 00:57:26.640]   don't they already have the leverage? Aren't you like, I think the UAE on its own is not
[00:57:26.640 --> 00:57:29.520]   competitive, right? It's like, I mean, they're already export controlled, like, you know,
[00:57:29.520 --> 00:57:32.720]   we're not, you know, there's like, you're not actually supposed to ship and video chips over
[00:57:32.720 --> 00:57:35.600]   there, right? You know, it's not like they have any of the leading AI labs, you know,
[00:57:35.600 --> 00:57:38.400]   it's like they have money, but you know, it's actually hard to just translate money into like,
[00:57:38.400 --> 00:57:42.320]   but the other things you've been saying about laying out your vision is very much there's
[00:57:42.320 --> 00:57:47.040]   this almost industrial process of you put in the compute, and then you put in the algorithms.
[00:57:47.040 --> 00:57:53.520]   Yeah, you add that up, and you get AGI on the other end. Yes. If it's something more like that,
[00:57:53.520 --> 00:57:57.680]   then the case for somebody being able to catch up rapidly seems more compelling than if it's
[00:57:57.680 --> 00:58:01.520]   some bespoke. Well, well, if they can steal the algorithms, and if they can steal the way,
[00:58:01.520 --> 00:58:04.960]   that's really, that's really where sort of, I mean, we should talk about this. This is really
[00:58:04.960 --> 00:58:11.360]   important. And I think, you know, so like, right now, yeah, how easy would it be for for an actor
[00:58:11.360 --> 00:58:16.480]   to steal the things that are like, not not the things that are released? Yeah, about Scarlett
[00:58:16.480 --> 00:58:20.880]   Johansson's voice, but the the RL things are talking about the unhobblings. I mean, I mean,
[00:58:20.880 --> 00:58:25.040]   all extremely easy, right? You know, I, you know, DeepMind, even like, you know, they,
[00:58:25.040 --> 00:58:28.400]   they don't make a claim that it's hard, right? DeepMind put out there, like, whatever frontier
[00:58:28.400 --> 00:58:32.000]   safety, something, and they like layout security levels, and they, you know, security level zero
[00:58:32.000 --> 00:58:36.000]   to four, and four is this new resilient, resistant to state actors, and they say we're at level zero,
[00:58:36.000 --> 00:58:39.440]   right? And then, you know, I mean, just recently, there's like an indictment of a guy
[00:58:39.440 --> 00:58:43.280]   who just like stole the code, a bunch of like, really important AI code and went to China with
[00:58:43.280 --> 00:58:47.440]   it. And, you know, all he had to do to steal the code was, you know, copy the code and put it into
[00:58:47.440 --> 00:58:52.000]   Apple Notes, and then export it as PDF. And that got past their monitoring, right? And Google is
[00:58:52.000 --> 00:58:55.040]   the best security of any of the AI labs, probably because they have the, you know, the Google
[00:58:55.040 --> 00:58:58.400]   infrastructure. I mean, I think, I don't know, roughly, I would think of this as like, you know,
[00:58:59.280 --> 00:59:02.640]   security of a startup, right? And like, what does security of a startup look like, right? You know,
[00:59:02.640 --> 00:59:08.960]   it's not that good. It's, it's easy to steal. So even if that's the case, yeah, a lot of your
[00:59:08.960 --> 00:59:13.840]   post is making the argument that, oh, you know, why are we going to get the intelligence explosion?
[00:59:13.840 --> 00:59:18.000]   Because if we have somebody with the intuition of an Alec Radford, yeah, to become able to come up
[00:59:18.000 --> 00:59:21.920]   with all these ideas, yeah, that intuition is extremely valuable, and you scale that up.
[00:59:21.920 --> 00:59:25.840]   But if it's a matter of these, if it's just in the code,
[00:59:25.840 --> 00:59:33.200]   that, like, if it's just the intuition, then we're, that's not going to be just in the code,
[00:59:33.200 --> 00:59:37.520]   right? And also, because of export controls, these countries are going to have slightly different
[00:59:37.520 --> 00:59:42.320]   hardware, you're going to have to make different trade-offs and probably rewrite things to be able
[00:59:42.320 --> 00:59:47.040]   to be compatible with that. Including all these things, is it just a matter of getting the right
[00:59:47.040 --> 00:59:50.400]   pen drive, and you plug it into the gigawatt data center next to the Three Gorges Dam,
[00:59:50.400 --> 00:59:54.640]   and then you're off to the races? I mean, like, there's a few different things, right? So one,
[00:59:54.640 --> 00:59:58.800]   one threat model is just stealing the weights themselves. And the weights one is sort of
[00:59:58.800 --> 01:00:02.640]   particularly insane, right? Because they can just, like, steal the literal, like, end product,
[01:00:02.640 --> 01:00:06.080]   right? Just, like, make a replica of the atomic bomb, and then they're just, like, ready to go.
[01:00:06.080 --> 01:00:09.840]   And, you know, I think that one just is, you know, extremely important around the time we have AGI
[01:00:09.840 --> 01:00:15.360]   and superintelligence, right? Because it's, you know, China can build a big cluster. By default,
[01:00:15.360 --> 01:00:18.800]   we'd have a big lead, right? Because we have the better scientists, but we make the superintelligence,
[01:00:18.800 --> 01:00:22.640]   they just steal it, they're off to the races. Weights are a little bit less important right now.
[01:00:22.640 --> 01:00:25.920]   Because, you know, who cares if they steal the GPT-4 weights, right? Like, whatever.
[01:00:25.920 --> 01:00:31.200]   And so, you know, we still have to get started on weight security now. Because, you know, look,
[01:00:31.200 --> 01:00:34.960]   if we think AGI by 27, you know, this stuff is going to take a while. And it, you know, it doesn't,
[01:00:34.960 --> 01:00:38.000]   you know, it's not just going to be like, oh, we do some access control. It's going to, you know,
[01:00:38.000 --> 01:00:41.760]   if you actually want to be resistant to sort of Chinese espionage, you know, it needs to be much
[01:00:41.760 --> 01:00:46.080]   more intense. The thing, though, that I think, you know, people aren't paying enough attention to
[01:00:46.080 --> 01:00:52.080]   is the secrets, as you say. And, you know, I think this is, you know, the compute stuff is sexy,
[01:00:52.080 --> 01:00:54.880]   you know, we talk about it. But, you know, I think that, you know, I think people underrate
[01:00:54.880 --> 01:00:59.680]   the secrets. Because they're, you know, I think they're, you know, the half an order of magnitude
[01:00:59.680 --> 01:01:02.640]   a year, just by default, sort of algorithmic progress, that's huge. You know, if we have a
[01:01:02.640 --> 01:01:07.680]   few year lead by default, you know, that's 10, 30x, 100x bigger cluster, if we protected them.
[01:01:07.680 --> 01:01:11.920]   And then there's this additional layer of the data wall, right? And so we have to get through
[01:01:11.920 --> 01:01:14.720]   the data wall, that means we actually have to figure out some sort of basic new paradigm,
[01:01:14.720 --> 01:01:19.440]   sort of the AlphaGo step two, right? AlphaGo step one is learns from human imitation. AlphaGo step
[01:01:19.440 --> 01:01:23.760]   two is the sort of self play RL. And everyone's working on that right now. And maybe we're going
[01:01:23.760 --> 01:01:31.360]   to crack it. And, you know, if China can't steal that, then they, you know, then they're stuck.
[01:01:31.360 --> 01:01:36.880]   If they can't steal it, they're off to the races. But whatever that thing is, is it like, literally,
[01:01:36.880 --> 01:01:40.160]   I can write down on the back of a napkin? Because if it's that easy, then why is it that hard for
[01:01:40.160 --> 01:01:43.760]   them to figure it out? And if it's more about the intuitions, that you just have to hire Alec
[01:01:43.760 --> 01:01:46.800]   Radford? Like, what are you copying down? Well, I think there's a few layers to this,
[01:01:46.800 --> 01:01:53.040]   right? So I think at the top is kind of like sort of the, you know, fundamental approach,
[01:01:53.040 --> 01:01:55.920]   right? And sort of like, I don't know, on pre-training, it might be, you know, like,
[01:01:55.920 --> 01:01:59.840]   you know, unsupervised learning, next token protection, train on the entire internet.
[01:01:59.840 --> 01:02:03.920]   You actually get a lot of juice out of that already. That one's very quick to communicate.
[01:02:03.920 --> 01:02:07.120]   Then there's like, there's a lot of details that matter. And you were talking about this earlier,
[01:02:07.120 --> 01:02:10.480]   right? It's like, probably the way that thing people are going to figure out is going to be
[01:02:10.480 --> 01:02:14.640]   like somewhat obvious. There's going to be some kind of like clear, you know, not that complicated
[01:02:14.640 --> 01:02:17.200]   thing that'll work. But there's going to be a lot of details to getting that right.
[01:02:17.200 --> 01:02:22.960]   But if that's true, then again, why are we even, why do we think that getting state-level security
[01:02:22.960 --> 01:02:26.400]   in these servers will prevent China from catching up? If it's just like, oh, we know some sort of
[01:02:26.400 --> 01:02:31.440]   self-play RL we require to get past the data wall. And if it's as easy as you say in some
[01:02:31.440 --> 01:02:35.280]   fundamental sense, I mean, again, but it's going to be solved by 2027, you say like, right? It's
[01:02:35.280 --> 01:02:38.160]   like, not that hard. I just think, you know, the US and the sort of, I mean,
[01:02:38.160 --> 01:02:41.760]   all the leading AI labs in the United States, and they have this huge lead. I mean, by default,
[01:02:41.760 --> 01:02:45.200]   China actually has some good LLMs. Why do they have good LLMs? They're just using the sort of
[01:02:45.200 --> 01:02:49.840]   open source code, right? You know, Lama or whatever. And so the, I think people really
[01:02:49.840 --> 01:02:54.080]   underrate the sort of, both the sort of divergence on algorithmic progress and the lead the US would
[01:02:54.080 --> 01:02:57.120]   have by default, because by, you know, all this stuff was published until recently, right? Like
[01:02:57.120 --> 01:03:00.720]   Chinchilla scaling laws were published, you know, there's a bunch of MOE papers, there's, you know,
[01:03:00.720 --> 01:03:04.400]   transformers and, you know, all that stuff was published. And so that's why open source is good.
[01:03:04.400 --> 01:03:07.440]   That's why China can make some good models. That stuff is now, I mean, at least they're not
[01:03:07.440 --> 01:03:12.000]   publishing it anymore. And, you know, if we actually kept it secret, it would be this huge
[01:03:12.000 --> 01:03:16.400]   edge. To your point about sort of like some tacit knowledge, like Bradford, you know, there's,
[01:03:16.400 --> 01:03:19.440]   there's another layer at the bottom that is something about like, you know, large scale
[01:03:19.440 --> 01:03:23.120]   engineering work to make these big training ones work. I think that is a little bit more tacit
[01:03:23.120 --> 01:03:26.720]   knowledge. So I think that but I think China will be able to figure that out. That's like
[01:03:26.720 --> 01:03:30.880]   sort of engineering slap, they're gonna figure out how to figure that out, but not how to get
[01:03:30.880 --> 01:03:36.640]   the RL thing working. I mean, look, I don't know, Germany during World War Two, you know,
[01:03:36.640 --> 01:03:39.920]   they went down the wrong path, they did heavy water, and that was wrong. And there's actually
[01:03:39.920 --> 01:03:44.640]   there's an amazing anecdote in, in the making of the atomic bomb on this, right. So secrecy is
[01:03:44.640 --> 01:03:48.880]   actually one of the most contentious issues, you know, early on, as well. And, you know, part of
[01:03:48.880 --> 01:03:52.880]   it was sort of, you know, Zillard or whatever really thought, you know, this sort of nuclear
[01:03:52.880 --> 01:03:57.920]   chain reaction was possible. And so an atomic bomb was possible, and he went around and he was like,
[01:03:57.920 --> 01:04:01.760]   eyes is going to be of enormous strategic importance, military importance. And a lot
[01:04:01.760 --> 01:04:04.640]   of people didn't believe it, or they're kind of like, well, maybe this is possible. But you know,
[01:04:04.640 --> 01:04:08.720]   I'm going to act as though it's not possible. And, you know, science should be open and all these
[01:04:08.720 --> 01:04:14.080]   things. And anyway, and so these early days, so there had been some sort of incorrect measurements
[01:04:14.080 --> 01:04:19.040]   made on on graphite as a moderator, and that Germany had, and so they thought, you know,
[01:04:19.040 --> 01:04:24.960]   graphite was not gonna work, we have to do heavy water. But then Fermi made some new measurements
[01:04:24.960 --> 01:04:29.280]   on graphite. And they indicated that graphite would work, you know, this is really important.
[01:04:30.400 --> 01:04:34.320]   And then, you know, Zillard kind of assaulted Fermi with the kind of another secrecy appeal.
[01:04:34.320 --> 01:04:38.320]   And Fermi was just kind of he was pissed off, you know, to temper tantrum, you know, he was like,
[01:04:38.320 --> 01:04:42.720]   he thought it was absurd, you know, like, come on, this is crazy. But, you know, you know,
[01:04:42.720 --> 01:04:47.920]   Zillard persisted, I think they wrote in another guy, Pegram, and then Fermi didn't publish it.
[01:04:47.920 --> 01:04:52.960]   And, you know, that was just in time, because Fermi not publishing it meant that the Nazis
[01:04:52.960 --> 01:04:56.720]   didn't figure out graphite would work, they went down this path of heavy water. And that was the
[01:04:56.720 --> 01:04:59.840]   wrong path. That was one of the sort of, you know, this is a key reason why the sort of German
[01:04:59.840 --> 01:05:06.560]   project didn't work out, they were kind of way behind. And, you know, I think we face a similar
[01:05:06.560 --> 01:05:10.000]   situation on are we are we just going to instantly leak the sort of how do we get past the data
[01:05:10.000 --> 01:05:15.120]   wall? What's the next paradigm? Are we not? So and the reason this would matter is if there's,
[01:05:15.120 --> 01:05:19.440]   like, being one year ahead would be a huge advantage in the world where it's like you
[01:05:19.440 --> 01:05:24.160]   deploy AI over time, and they're just like, they're gonna catch up anyway. I mean, I interviewed
[01:05:24.160 --> 01:05:30.320]   Richard Rhodes, the guy who wrote making an atomic bomb. Yeah. And one of the anecdotes he had
[01:05:30.320 --> 01:05:36.240]   was when so they'd realized America had the bomb. Obviously, we dropped it in Japan. Yeah.
[01:05:36.240 --> 01:05:42.800]   And Beria goes, the guy who ran the NKVD. Yeah. And just a famously ruthless guy, just evil.
[01:05:42.800 --> 01:05:46.880]   And he goes to I forgot the name of the guy, the Soviet scientist who was running their version of
[01:05:46.880 --> 01:05:53.200]   the Manhattan Project. He says, Comrade, you will get us the American bomb. Yeah. And the guy says,
[01:05:53.200 --> 01:05:56.960]   well, listen, their implosion device actually is not optimal. We should make it a different way.
[01:05:56.960 --> 01:06:03.600]   And Beria says, No, you will get us the American bomb or your family will be camp dust. But the
[01:06:03.600 --> 01:06:08.960]   the thing that's relevant about that anecdote is actually, the Soviets would have had a better
[01:06:08.960 --> 01:06:14.000]   bomb if they hadn't copied the American design, at least initially. And which suggests that often
[01:06:14.000 --> 01:06:18.400]   in history, this is something that's not just for the Manhattan Project. But there's this pattern
[01:06:18.400 --> 01:06:24.400]   of parallel invention where because the tech tree implies that the certain thing is next, in this
[01:06:24.400 --> 01:06:30.080]   case, self-play, RL, whatever, then people are just like working on that and like people are
[01:06:30.080 --> 01:06:33.680]   going to figure out around the same time. There's not there's not going to be that much gap in who
[01:06:33.680 --> 01:06:37.920]   gets it first. It wasn't like famously the bunch of people were invented something like the light
[01:06:37.920 --> 01:06:42.400]   bulb around the same time and so forth. Yeah. So but is it just that like, yeah, that might be true,
[01:06:42.400 --> 01:06:45.760]   but it'll be the one year or the six months or whatever. Two years makes all the difference.
[01:06:45.760 --> 01:06:48.800]   I don't know if it'll be two years, though. I mean, I actually I mean, I actually think if we
[01:06:48.800 --> 01:06:52.000]   lock down the labs, we have we have much better scientists. We're way ahead. It would be two
[01:06:52.000 --> 01:06:55.920]   years. But even I think even I think I think whether you I think yeah, I think even six months
[01:06:55.920 --> 01:06:58.800]   a year would make a huge difference. And this gets back to the sort of intelligence explosion. It's
[01:06:58.800 --> 01:07:03.520]   like a year might be the difference between, you know, a system that's sort of like human level
[01:07:03.520 --> 01:07:07.760]   and a system that is like vastly superhuman. Right. Might be like five, five. You know,
[01:07:07.760 --> 01:07:12.080]   I mean, even on the current pace. Right. We went from, you know, I think on the math benchmark
[01:07:12.080 --> 01:07:16.480]   recently. Right. Like, you know, three years ago on the math benchmark, we you know, that was
[01:07:16.480 --> 01:07:22.320]   this is a sort of really difficult high school competition math problems. You know, we were at,
[01:07:22.320 --> 01:07:26.640]   you know, a few percent couldn't solve anything. Now it's solved. And that was sort of the normal
[01:07:26.640 --> 01:07:30.160]   pace of our progress. You didn't have sort of a billion superintelligent resources, researchers.
[01:07:30.160 --> 01:07:33.600]   So like a year is a huge difference. And then particularly after superintelligence. Right.
[01:07:33.600 --> 01:07:37.040]   Once this is applied to sort of lots of elements of R&D, once you get the sort of like industrial
[01:07:37.040 --> 01:07:41.760]   explosion with the robots and so on, you know, I think a year, you know, a couple of years might
[01:07:41.760 --> 01:07:46.480]   be kind of like decades worth of technological progress. And again, it's like go for one. Right.
[01:07:46.480 --> 01:07:50.800]   Twenty, thirty years of technological lead. Totally decisive. You know, I think it really
[01:07:50.800 --> 01:07:55.920]   matters. The other reason it really matters is, you know, suppose suppose they steal the weight,
[01:07:55.920 --> 01:08:00.320]   suppose they steal the algorithms and they're close on our tails. Suppose we still pull out
[01:08:00.320 --> 01:08:03.840]   ahead. Right. We just kind of we were a little bit faster. You know, we're three months ahead.
[01:08:03.840 --> 01:08:07.760]   I think the sort of like world in which we're really neck and neck, you know,
[01:08:07.760 --> 01:08:12.080]   you only have a three month lead are incredibly dangerous. Right. And we're in this fever
[01:08:12.080 --> 01:08:18.160]   struggle where like if they get ahead, they get to dominate, you know, sort of maybe they'd get
[01:08:18.160 --> 01:08:22.320]   a decisive advantage. They're building clusters like crazy. They're they're willing to throw all
[01:08:22.320 --> 01:08:26.480]   caution to the wind. We have to keep up. There's some crazy new WMDs popping up. And then we're
[01:08:26.480 --> 01:08:30.080]   going to be in the situation where it's like, you know, crazy new military technology, crazy
[01:08:30.080 --> 01:08:34.000]   new WMDs, you know, like deterrence, mutually disturbed destruction, like keeps changing,
[01:08:34.000 --> 01:08:38.320]   you know, every few weeks. And it's like, you know, completely unstable, volatile situation
[01:08:38.320 --> 01:08:42.080]   is incredibly dangerous. So I think I think, you know, both both from just the technologies are
[01:08:42.080 --> 01:08:44.880]   dangerous from the alignment point of view. You know, I think it might be really important during
[01:08:44.880 --> 01:08:49.760]   the intelligence explosion to have the sort of six month wiggle room to be like, look, we're going to
[01:08:49.760 --> 01:08:52.880]   like dedicate more compute to alignment during this period because we have to get it right.
[01:08:52.880 --> 01:08:58.400]   We're feeling uneasy about how it's going. And so I think in some sense, like one of the most
[01:08:58.400 --> 01:09:02.480]   important inputs to whether we will kind of destroy ourselves or whether we will get through this
[01:09:02.480 --> 01:09:10.640]   just incredibly crazy period is whether we have that buffer. Why? So before we go further object
[01:09:10.640 --> 01:09:17.280]   level in this, I think it's very much worth noting that almost nobody, at least nobody I talk to
[01:09:17.280 --> 01:09:23.520]   thinks about the geopolitical implications of AI. And I think I have some object level
[01:09:23.520 --> 01:09:28.000]   disagreements that we'll get into, but or at least things I want to iron out. I may not disagree in
[01:09:28.000 --> 01:09:35.360]   the end. But the basic premise that obviously if you keep scaling and obviously if people realize
[01:09:35.360 --> 01:09:41.840]   that this is where intelligence is headed, it's not just going to be like the the same old world
[01:09:41.840 --> 01:09:47.120]   where like what model are redeploying tomorrow and what is the latest? Like people on Twitter
[01:09:47.120 --> 01:09:51.920]   are like, oh, they're the GP4O is going to shake your expectations or whatever. You know,
[01:09:51.920 --> 01:10:01.040]   COVID is really interesting because before a year or something when March 2020 hit, it became clear
[01:10:01.040 --> 01:10:05.840]   to the world like presidents, CEOs, media, average person, there's other things happening in the
[01:10:05.840 --> 01:10:10.000]   world right now. But the main thing we as a world are dealing with right now is COVID.
[01:10:10.000 --> 01:10:11.040]   Soon on AGI.
[01:10:11.040 --> 01:10:13.600]   Yeah. Okay. And then so...
[01:10:13.600 --> 01:10:17.280]   This is the quiet period, you know, if you want to go on vacation, you know, you want to like,
[01:10:17.280 --> 01:10:20.320]   you want to, yeah, you want to have, you know, maybe like now is the last time you can have some
[01:10:20.320 --> 01:10:27.680]   kids. You know, my girlfriend sometimes complains that, you know, that I, you know, when I'm like,
[01:10:27.680 --> 01:10:30.960]   you know, off doing work or whatever, she's like, I'm not spending time with her. She's like,
[01:10:30.960 --> 01:10:34.720]   you know, she threatens to replace me with like, you know, GP6 or whatever. And I'm like,
[01:10:34.720 --> 01:10:37.120]   GP6 will also be too busy doing AI research.
[01:10:37.120 --> 01:10:44.160]   Okay, anyway, so what's the answer to the question of why? Why aren't other people
[01:10:44.160 --> 01:10:45.120]   talking national security?
[01:10:45.120 --> 01:10:48.560]   I made this mistake with COVID, right? So I, you know, February of 2020,
[01:10:48.560 --> 01:10:52.960]   and I, you know, I thought just it was going to sweep the world and all the hospitals would
[01:10:52.960 --> 01:10:57.680]   collapse and it'd be crazy. And then, you know, and then it'd be over. And a lot of people thought
[01:10:57.680 --> 01:11:00.880]   this kind of the beginning of COVID. They shut down their offices a month or whatever. I think
[01:11:00.880 --> 01:11:05.600]   the thing I just really didn't price in was the societal reaction, right? And within weeks,
[01:11:05.600 --> 01:11:10.960]   you know, Congress spent over 10% of GDP on like COVID measures, right? The entire country was
[01:11:10.960 --> 01:11:16.480]   shut down. It was crazy. And so, I don't know, I didn't price it in with COVID sufficiently.
[01:11:16.720 --> 01:11:22.160]   I don't know, why do people underrate it? I mean, I think there's a sort of way in which
[01:11:22.160 --> 01:11:28.000]   being kind of in the trenches actually kind of, I think, gives you a less clear picture of the
[01:11:28.000 --> 01:11:32.560]   trend lines. You actually have to zoom out that much only like a few years, right? But, you know,
[01:11:32.560 --> 01:11:35.600]   you're in the trenches, you're like trying to get the next model to work, you know, there's always
[01:11:35.600 --> 01:11:39.040]   something that's hard, you know, for example, you might underrate algorithmic progress, because
[01:11:39.040 --> 01:11:42.560]   you're like, ah, things are hard right now, or, you know, data wall or whatever. But you know,
[01:11:42.560 --> 01:11:45.680]   you zoom out just a few years, and you actually try to like count up how much algorithmic progress
[01:11:45.680 --> 01:11:51.680]   made in the last, you know, last few years, and it's enormous. But I also just don't think people
[01:11:51.680 --> 01:11:56.640]   think about this stuff. Like, I think smart people really underrate espionage, right? And, you know,
[01:11:56.640 --> 01:12:00.160]   I think part of the security issue is I think people don't realize like, how intense state
[01:12:00.160 --> 01:12:05.440]   level espionage can be, right? Like, you know, this Israeli company had software that could just
[01:12:05.440 --> 01:12:09.600]   zero click hack any iPhone, right? They just put in your number, and then it's just like straight
[01:12:09.600 --> 01:12:13.920]   download of everything, right? Like, the United States infiltrated an air gapped atomic weapons
[01:12:13.920 --> 01:12:19.280]   program, right? Wild, you know, like, yeah, you know, the, you know, you know, intelligence
[01:12:19.280 --> 01:12:24.400]   agencies have just stockpiles of zero days, you know, when things get really hot, you know,
[01:12:24.400 --> 01:12:27.520]   I don't know, maybe we'll send special forces, right? To like, you know, get go to the data
[01:12:27.520 --> 01:12:31.040]   center or something that's, you know, or, you know, I mean, China does this, they threaten
[01:12:31.040 --> 01:12:34.560]   people's families, right? And they're like, look, if you don't cooperate, if you don't give us the
[01:12:34.560 --> 01:12:41.600]   Intel, there's a good book, you know, along the lines of the Gulag, you know, the inside the
[01:12:41.600 --> 01:12:47.920]   aquarium, which is by a Soviet GRU defector. GRU was like military intelligence, Ilya recommended
[01:12:47.920 --> 01:12:54.960]   this book to me. And, you know, I think reading that, I was just kind of like shocked at how
[01:12:54.960 --> 01:12:58.800]   intense sort of state level espionage is. The whole book was about like, they go to these European
[01:12:58.800 --> 01:13:02.080]   countries, and they try to like get all the technology and recruit all these people to get
[01:13:02.080 --> 01:13:07.760]   the technology. I mean, yeah, maybe one anecdote, you know, so when so the spy, you know, this
[01:13:07.760 --> 01:13:12.000]   eventual defector, you know, so he's being trained, he goes to the kind of GRU spy academy.
[01:13:12.000 --> 01:13:16.240]   And so then to graduate from the spy academy, sort of before you're sent abroad, you kind of
[01:13:16.240 --> 01:13:22.080]   had to pass a test to show that you can do this. And the test was, you know, you had to in Moscow,
[01:13:22.080 --> 01:13:25.760]   recruit a Soviet scientist and recruit them to give you information sort of like you would do
[01:13:25.760 --> 01:13:33.600]   in the foreign country. But of course, for whomever you recruited, the penalty for giving away sort of
[01:13:33.600 --> 01:13:40.000]   secret information was death. And so to graduate from the Soviet spy, the GRU spy academy, you had
[01:13:40.000 --> 01:13:47.600]   to condemn a countryman to death. States do this stuff. I started reading the book on, because I
[01:13:47.600 --> 01:13:52.240]   saw it in the series. Yeah. And I was actually wondering the fact that you use this anecdote.
[01:13:52.240 --> 01:13:58.640]   Yeah. And then you're like, and a book recommended by Ilya, is this some sort of, is this some sort
[01:13:58.640 --> 01:14:07.120]   of Easter egg? We'll leave that for an exercise for the reader. Okay. So the beatings will continue
[01:14:07.120 --> 01:14:15.520]   until the morale improves. So suppose that we live in the world in which these secrets are locked
[01:14:15.520 --> 01:14:24.000]   down, but China still realizes that this progress is happening in America. So in that world,
[01:14:24.000 --> 01:14:27.920]   especially if they realize, and I guess it's a very interesting question,
[01:14:27.920 --> 01:14:31.440]   it probably won't be locked down. Okay. But we're probably going to live in the bad world.
[01:14:31.440 --> 01:14:36.240]   Yeah. It's going to be really bad. Why are you so confident that they won't be locked down?
[01:14:36.240 --> 01:14:39.600]   I mean, I'm not confident that it won't be locked down, but I think it's just, it's not happening.
[01:14:39.600 --> 01:14:47.840]   But so tomorrow, the lab leaders get the message. How hard, like, what do they have to do? They get
[01:14:47.840 --> 01:14:53.600]   the more security guards, they like air gap the, what do they do? So again, I think basically it's,
[01:14:53.600 --> 01:14:57.680]   you know, I think people, there's kind of like two reactions there, which is like, it's,
[01:14:57.680 --> 01:15:03.440]   you know, we're already secure, you know, not, and there's, you know, fatalism, it's impossible.
[01:15:03.440 --> 01:15:06.160]   And I think the thing you need to do is you kind of got to stay ahead of the curve of basically
[01:15:06.160 --> 01:15:10.400]   how EGI pills the CCP. Yeah. Right. So like right now you've got to be resistant to kind of like
[01:15:10.400 --> 01:15:15.760]   normal economic espionage. They're not, right. I mean, I probably wouldn't be talking about the
[01:15:15.760 --> 01:15:19.760]   stuff that the labs were, right. Cause I wouldn't want to wake them up more, the CCP, but they're
[01:15:19.760 --> 01:15:23.680]   not, you know, this is like, this stuff is like really trivial for them to do right now. I mean,
[01:15:23.680 --> 01:15:28.000]   it's also, anyway, so they're not resistant to that. I think it would be possible for a private
[01:15:28.000 --> 01:15:31.600]   company to be resistant to it. Right. So, you know, both of us have, you know, friends in the
[01:15:31.600 --> 01:15:35.200]   kind of like quantitative trading world. Right. And, and, you know, I think actually those secrets
[01:15:35.200 --> 01:15:39.600]   are shaped kind of similarly where it's like, you know, you know, they've said, you know, yeah,
[01:15:39.600 --> 01:15:43.360]   if I got on a call for an hour with somebody from a competitive firm, I could, most of our
[01:15:43.360 --> 01:15:47.680]   alpha would be gone. And that's sort of like, that's the like list of details of like really
[01:15:47.680 --> 01:15:50.880]   how to, how to make sure you're going to worry about that pretty soon. You're going to have to
[01:15:50.880 --> 01:15:55.360]   worry about that pretty soon. Well, anyway, and so, so all alpha could be gone, but in fact,
[01:15:55.360 --> 01:15:59.360]   their alpha persists, right. And, you know, often, often for many years and decades. And so this
[01:15:59.360 --> 01:16:02.480]   doesn't seem to happen. And so I think there's like, you know, I think there's a lot you could
[01:16:02.480 --> 01:16:05.600]   go if you went from kind of current startup security, you know, you just got to look through
[01:16:05.600 --> 01:16:09.440]   the window and you can look at the slides, you know, it's kind of like, you know, you know,
[01:16:09.440 --> 01:16:13.120]   good private sector security hedge funds, you know, the way Google treats, you know,
[01:16:13.120 --> 01:16:19.840]   customer data or whatever. That'd be good right now. The issue is, you know, basically the CCP
[01:16:19.840 --> 01:16:27.280]   will also get more AGI built. And at some point we're going to face kind of the full force of,
[01:16:27.280 --> 01:16:30.320]   you know, the ministry of state security. And again, you're talking about smart people
[01:16:30.320 --> 01:16:33.920]   underwriting espionage and sort of insane capabilities of states. I mean, this stuff
[01:16:33.920 --> 01:16:36.320]   is wild, right? You know, they can get like, you know, there's papers about, you know,
[01:16:36.320 --> 01:16:39.760]   you can find out the location of like where you are in a video game map just from sounds,
[01:16:39.760 --> 01:16:43.600]   right? Like states can do a lot with like electromagnetic emanations, you know, like,
[01:16:43.600 --> 01:16:47.360]   you know, at some point, like you got to be working from a scaff, like your cluster needs
[01:16:47.360 --> 01:16:50.720]   to be air gapped and basically be a military base. It's like, you know, you need to have,
[01:16:50.720 --> 01:16:54.160]   you know, intense kind of security clearance procedures for employees. You know, they have
[01:16:54.160 --> 01:16:57.520]   to be like, you know, all this shit is monitored, you know, they're, you know,
[01:16:57.520 --> 01:17:02.000]   they basically have security guards, you know, it's, you know, you can't use any kind of like,
[01:17:02.000 --> 01:17:05.040]   you know, other dependencies. It's all got to be like intensely vetted and, you know,
[01:17:05.040 --> 01:17:11.920]   all your hardware has to be intensely vetted. And, you know, I think basically if they actually
[01:17:11.920 --> 01:17:15.280]   really face the full force of state level espionage, I don't really think this is a
[01:17:15.280 --> 01:17:18.160]   thing private companies can do. But I mean, empirically, right? Like, you know, Microsoft
[01:17:18.160 --> 01:17:21.840]   recently had executives emails hacked by Russian hackers and, you know, government emails they've
[01:17:21.840 --> 01:17:26.480]   posted hacked by government actors. But also, you know, it's basically there's just a lot of
[01:17:26.480 --> 01:17:29.360]   stuff that only kind of, you know, the people behind the security currencies know and only
[01:17:29.360 --> 01:17:34.720]   they deal with. And so, you know, I think to actually kind of resist the sort of full force
[01:17:34.720 --> 01:17:38.480]   of espionage, you're going to need the government. Anyway, so I think basically we could do it by
[01:17:38.480 --> 01:17:41.120]   always being ahead of the curve. I think we're just going to always be behind the curve.
[01:17:42.560 --> 01:17:45.680]   And I think, you know, maybe unless we get the sort of government project.
[01:17:45.680 --> 01:17:51.760]   Okay, so going back to the naive perspective of we're very much coming at this from there's going
[01:17:51.760 --> 01:17:57.520]   to be a race in the CCP, we must win. And listen, I understand like bad people are in charge of the
[01:17:57.520 --> 01:18:03.200]   Chinese government, like the CCP and everything. But just stepping back in a sort of galactic
[01:18:03.200 --> 01:18:08.720]   perspective, humanity is developing AGI. And do we want to come at this from the perspective of
[01:18:09.360 --> 01:18:14.720]   we need to be China to this are our super intelligent Jupiter brain descendants won't
[01:18:14.720 --> 01:18:19.360]   know which I like China will be something like distant memory that they have America too.
[01:18:19.360 --> 01:18:23.520]   So shouldn't it be a more the initial approach just come to them? Like, listen,
[01:18:23.520 --> 01:18:28.960]   we this is uber intelligence. This is something like we come from a cooperative
[01:18:28.960 --> 01:18:35.680]   perspective, why why immediately sort of rush into it from a hawkish competitive perspective?
[01:18:35.680 --> 01:18:39.200]   I mean, look, I mean, one thing I want to say is like a lot of the stuff I talk about in the series
[01:18:39.200 --> 01:18:43.760]   is, you know, is sort of primarily, you know, descriptive, right? And so I think that on the
[01:18:43.760 --> 01:18:47.760]   China stuff, it's like, you know, yeah, in some ideal world, you know, we, we, you know, it's
[01:18:47.760 --> 01:18:53.200]   just all, you know, merry go round and cooperation. But again, it's sort of, I think, I think people
[01:18:53.200 --> 01:18:57.680]   wake up to AGI. I think the issue particular on sort of like, can we make a deal? Can we make an
[01:18:57.680 --> 01:19:01.120]   international treaty? I think it really relates to sort of what is the stability of sort of
[01:19:01.120 --> 01:19:06.320]   international arms control agreements, right? And so we did very successful arms control on
[01:19:06.320 --> 01:19:10.800]   nuclear weapons in the 80s, right? And the reason it was successful is because the sort of new
[01:19:10.800 --> 01:19:14.160]   equilibrium was stable, right? So you take go down from, you know, whatever 60,000 nukes to
[01:19:14.160 --> 01:19:20.000]   10,000 nukes, you know, when you have 10,000 nukes, you know, basically breakout, breakout
[01:19:20.000 --> 01:19:23.280]   doesn't matter that much, right? Suppose the other guy now try to make 20,000 nukes. Well,
[01:19:23.280 --> 01:19:26.640]   it's like, who cares, right? You know, like, it's still mutually assured destruction. Suppose a
[01:19:26.640 --> 01:19:30.080]   rogue state kind of went from zero nukes to one nukes. It's like, who cares? We still have way
[01:19:30.080 --> 01:19:33.360]   more nukes than you. I mean, it's still not ideal for destabilization. But it's, you know,
[01:19:33.360 --> 01:19:36.720]   it'd be very different if the arms control agreement had been zero nukes, right? Because
[01:19:36.720 --> 01:19:40.560]   if it had been zero nukes, then it's just like one rogue state makes one nuke. The whole thing
[01:19:40.560 --> 01:19:46.480]   is destabilized. Breakout is very easy. You know, your adversary state starts making nukes. And so
[01:19:46.480 --> 01:19:50.320]   basically, when you're going to sort of like very low levels of arms, or when you're going to kind
[01:19:50.320 --> 01:19:55.840]   of, and you're in a very dynamic technological situation, arms control is really tough because
[01:19:55.840 --> 01:19:59.840]   breakout is easy. You know, there's, I mean, there's some other sort of stories about this
[01:19:59.840 --> 01:20:04.160]   in sort of like 1920s, 1930s. You know, it's like, you know, all the European states had done
[01:20:04.160 --> 01:20:09.120]   disarmament and Germany was kind of did this like crash program to build the Luftwaffe. And that was
[01:20:09.120 --> 01:20:12.640]   able to like massively destabilize things because not that, you know, they were the first, they were
[01:20:12.640 --> 01:20:15.600]   able to like pretty easily build kind of a modern, you know, Air Force because the others didn't
[01:20:15.600 --> 01:20:19.920]   really have one. And that, you know, that really destabilized things. And so I think the issue with
[01:20:19.920 --> 01:20:24.720]   AGI and superintelligence is the explosiveness of it, right? So if you have an intelligence
[01:20:24.720 --> 01:20:28.400]   explosion, if you're able to go from kind of AGI to superintelligence, if that superintelligence
[01:20:28.400 --> 01:20:32.640]   is decisive, like either, you know, like a year after, cause you've developed some crazy WMD
[01:20:32.640 --> 01:20:37.200]   or cause you have some like, you know, super hacking ability that lets you, you kind of you
[01:20:37.200 --> 01:20:41.920]   know, completely deactivate the sort of enemy arsenal. That means like, suppose, suppose you're
[01:20:41.920 --> 01:20:45.440]   trying to like put in a break, you know, like we both, we're both gonna like cooperate and we're
[01:20:45.440 --> 01:20:49.840]   gonna go slower, you know, on the cusp of AGI or whatever, there is going to be such an enormous
[01:20:49.840 --> 01:20:53.680]   incentive to kind of race ahead to break out. And we're just going to do the intelligence explosion.
[01:20:53.680 --> 01:20:59.200]   If we can get three months ahead, we win. I think that makes it basically, I think any sort of arms
[01:20:59.200 --> 01:21:05.280]   control agreement that comes as situation where it's close, very unstable. That's really interesting.
[01:21:05.280 --> 01:21:12.000]   This is very analogous to kind of a debate I had with Rose on the podcast where he argued for
[01:21:12.000 --> 01:21:18.000]   nuclear disarmament. But if some country tries to break out and starts developing nuclear weapons,
[01:21:18.000 --> 01:21:23.280]   the six months or whatever that you would get is enough to get international consensus and
[01:21:23.280 --> 01:21:28.000]   invade the country and prevent them from getting nukes. And I thought that was sort of, that's not
[01:21:28.000 --> 01:21:33.600]   a stable equilibrium. It just seemed really tough, yeah. But so on this, right, so like maybe it's a
[01:21:33.600 --> 01:21:37.920]   bit easier because you have AGI and so like you can monitor the other person's cluster or something.
[01:21:37.920 --> 01:21:41.920]   Data centers, you can see them from space, actually. You can see the energy draw they're
[01:21:41.920 --> 01:21:45.280]   getting. There's a lot of things, as you were saying, there's a lot of ways to get information
[01:21:45.280 --> 01:21:51.280]   from an environment if you're really dedicated. And also because unlike a nukes, the data centers
[01:21:51.280 --> 01:21:58.160]   are nukes, you have obviously the submarines, planes, you have bunkers, mountains, whatever you
[01:21:58.160 --> 01:22:02.240]   have in so many different places. A data center, your 100 gigawatt data center, we can blow that
[01:22:02.240 --> 01:22:06.480]   shit up if you're like, we're concerned, right? Like just some cruise missile or something that's
[01:22:06.480 --> 01:22:09.760]   like very vulnerable to sabotage. I mean, that gets to the sort of, I mean, that gets to the
[01:22:09.760 --> 01:22:12.960]   sort of insane vulnerability, the volatility of this period post superintelligence, right?
[01:22:12.960 --> 01:22:16.160]   Because basically I think, so you have the intelligence explosion, you have these like
[01:22:16.160 --> 01:22:19.520]   vastly superhuman things on your cluster, but you're like, you haven't done the industrial
[01:22:19.520 --> 01:22:22.400]   explosion yet. You don't have your robots yet. You haven't kind of, you haven't covered the
[01:22:22.400 --> 01:22:27.680]   desert in like robot factories yet. And that is the sort of crazy moment where, say the United
[01:22:27.680 --> 01:22:32.240]   States is ahead, the CCP is somewhat behind, there's actually an enormous incentive for strike,
[01:22:32.240 --> 01:22:36.320]   right? Because if they can take out your data center, they know you're about to have just this
[01:22:36.320 --> 01:22:41.440]   command and decisive lead. They know if we can just take out this data center, then we can stop
[01:22:41.440 --> 01:22:46.400]   it. And they might get desperate. And so I think basically we're going to get into a position,
[01:22:46.400 --> 01:22:50.560]   it's actually, I think it's going to be pretty hard to defend early on. I think we're basically
[01:22:50.560 --> 01:22:53.440]   going to be in a position where we're protecting data centers with like the threat of nuclear
[01:22:53.440 --> 01:22:56.400]   retaliation. It's like, maybe it sounds kind of crazy though, you know.
[01:22:56.400 --> 01:22:59.840]   Is this the inverse of the LASER? Are we going to keep the data centers?
[01:22:59.840 --> 01:23:04.240]   Nuclear deterrence for data centers. I mean, this is a, you know, Berlin, you know,
[01:23:04.240 --> 01:23:08.400]   in the like late fifties, early sixties, both Eisenhower and Kennedy multiple times kind of
[01:23:08.400 --> 01:23:12.640]   made the threat of full on nuclear war against the Soviets if they tried to encroach on West Berlin.
[01:23:13.600 --> 01:23:16.560]   It's sort of insane. It's kind of insane that that went well. But basically, I think that's
[01:23:16.560 --> 01:23:20.000]   going to be the only option for the data centers. It's a terrible option. This whole scheme is
[01:23:20.000 --> 01:23:25.280]   terrible, right? Like being in this like neck and neck race, sort of at this point is terrible.
[01:23:25.280 --> 01:23:29.440]   And, you know, it's also, you know, I think I have some uncertainty basically on how easy
[01:23:29.440 --> 01:23:32.560]   that decisive advantage will be. I'm pretty confident that if you have super intelligence,
[01:23:32.560 --> 01:23:36.320]   you have two years, you have the robots, you're able to get that 30 year lead. Look, then you're
[01:23:36.320 --> 01:23:40.240]   in this like go for one situation. You have your like, you know, millions or billions of like
[01:23:40.240 --> 01:23:43.600]   mosquito sized drones that can just take it out. I think there's even a possibility you can kind of
[01:23:43.600 --> 01:23:46.800]   get a decisive advantage earlier. So, you know, there's these stories, you know, about these as
[01:23:46.800 --> 01:23:51.280]   well, about, you know, like colonization and like the sort of 1500s where it was, you know,
[01:23:51.280 --> 01:23:56.160]   these like a few hundred kind of Spaniards were able to like topple the Aztec empire, you know,
[01:23:56.160 --> 01:23:59.520]   a couple, I think a couple other empires as well. You know, each of these had a few million people
[01:23:59.520 --> 01:24:03.120]   and it was not like God like technological advantage. It was some technological advantage.
[01:24:03.120 --> 01:24:07.040]   It was, I mean, it was some amount of disease and then it was kind of like cunning strategic
[01:24:07.040 --> 01:24:10.960]   play. And so I think there's a, there's a, there's a possibility that even sort of early on,
[01:24:10.960 --> 01:24:14.080]   you know, you haven't gone through the full industrial explosion yet. You have super
[01:24:14.080 --> 01:24:17.600]   intelligence, but you know, you're able to kind of like manipulate the imposing generals,
[01:24:17.600 --> 01:24:21.120]   claim you're allying with them. Then you have, you have some, you know, you have sort of like
[01:24:21.120 --> 01:24:25.200]   some crazy new bioweapons. Maybe, maybe there's even some way to like pretty easily get a paradigm
[01:24:25.200 --> 01:24:28.400]   that like deactivates enemy nukes anyway. So I think this stuff could get pretty wild.
[01:24:28.400 --> 01:24:35.040]   Here's what I think we should do. I really don't want this volatile period. And so a deal with
[01:24:35.040 --> 01:24:38.320]   China would be nice. It's going to be really tough if you're in this unstable equilibrium.
[01:24:38.320 --> 01:24:44.960]   I think basically we want to get in a position where it is clear that the United States,
[01:24:44.960 --> 01:24:48.640]   that a sort of coalition of democratic allies will win. It's clear the United States would
[01:24:48.640 --> 01:24:52.240]   declare to China, you know, that will require having locked down the secrets that will require
[01:24:52.240 --> 01:24:54.960]   having built the a hundred gigawatt cluster in the United States and having done the natural
[01:24:54.960 --> 01:24:59.680]   gas and doing what's necessary. And then when it is clear that the democratic coalition is well
[01:24:59.680 --> 01:25:04.560]   ahead, then you go to China and then you offer them a deal and you know, China will know they're
[01:25:04.560 --> 01:25:09.520]   going to win. This is going to be, they're very scared of what's going to happen. We're going to
[01:25:09.520 --> 01:25:11.840]   know we're going to win, but we're also very scared of what's going to happen because we
[01:25:11.840 --> 01:25:16.880]   really want to avoid this kind of like breakneck race right at the end and where things could
[01:25:16.880 --> 01:25:22.240]   really go awry. And you know, and then, and then, so then we offer them a deal. I think there's an
[01:25:22.240 --> 01:25:25.440]   incentive to come to the table. I think there's a sort of more stable arrangement you can do.
[01:25:25.440 --> 01:25:28.800]   It's a sort of an atoms for peace arrangement. And we're like, look, we're going to respect you.
[01:25:28.800 --> 01:25:31.680]   We're not, we're not going to like, we're not going to use superintelligence against you.
[01:25:31.680 --> 01:25:34.240]   You can do what you want. You're going to get your, like, you're going to get your slice of
[01:25:34.240 --> 01:25:38.320]   the galaxy. Um, we're going to like, we're going to benefit share with you. We're going to have
[01:25:38.320 --> 01:25:41.200]   some like compute agreement where it's like, there's some ratio of compute that you're allowed
[01:25:41.200 --> 01:25:46.480]   to have. And that's like enforced with her, like opposing AI or whatever. And, um, we're just not
[01:25:46.480 --> 01:25:50.480]   going to do, we're just not going to do this kind of like volatile sort of WMD arms race to the
[01:25:50.480 --> 01:25:55.680]   desk. We're good. Sort of, it's like a new world order. That's us led. That's sort of democratic
[01:25:55.680 --> 01:25:59.680]   led, but that respects China. Let's them do what they want. Okay. There's so much to,
[01:26:00.800 --> 01:26:05.920]   there's so much there. Um, first on the galaxies thing, I think it's just a funny anecdote. So I
[01:26:05.920 --> 01:26:09.920]   want to kind of want to tell it and this, we were at an event and I'm respecting Chatham house rules
[01:26:09.920 --> 01:26:14.880]   here. I'm not revealing anything about it, but we're talking to somebody, um, or at Leopold
[01:26:14.880 --> 01:26:20.720]   was talking to somebody influential afterwards. That person asked the group, Leopold told me
[01:26:20.720 --> 01:26:27.760]   that he wants, he's not going to spend any money on consumption until he's ready to buy galaxies.
[01:26:27.760 --> 01:26:34.000]   And he goes, the guy goes, I honestly don't know if he meant galaxies, like the brand
[01:26:34.000 --> 01:26:40.080]   of private plane galaxy or the physical galaxies. And there was an actual debate. Like he, he went
[01:26:40.080 --> 01:26:44.800]   away to the restroom and there was an actual debate among people who are very influential
[01:26:44.800 --> 01:26:49.600]   about, well, he can't have meant galaxies and other people who knew you better be like, no,
[01:26:49.600 --> 01:26:55.440]   he means galaxies. I mean, the galaxies, I mean, the galaxies, I mean, I think it'd be interesting.
[01:26:55.440 --> 01:26:58.480]   I mean, I, yeah, I think there's a, I mean, there's two ways to buy the galaxies. One is
[01:26:58.480 --> 01:27:01.440]   like at some point, you know, it's like post superintelligence, you know, there's some crazy.
[01:27:01.440 --> 01:27:06.080]   But by the way, I love. Okay. So what happens is he's otherwise I'm laughing my ass off. I'm not
[01:27:06.080 --> 01:27:11.760]   even saying good people were like having this debate. And then so Leopold comes back and the
[01:27:11.760 --> 01:27:15.840]   guy, somebody who's like, Oh, Leopold, we're having this debate about whether you meant,
[01:27:15.840 --> 01:27:22.240]   um, you want to buy the galaxy or you want to buy the other thing. And Leopold assumes they
[01:27:22.240 --> 01:27:27.040]   must mean not the private play in the galaxy versus the actual galaxy. But do you want to
[01:27:27.040 --> 01:27:30.720]   buy the property rights of the galaxy or actually just send out the probes right now?
[01:27:30.720 --> 01:27:38.880]   All right. Back to China.
[01:27:38.880 --> 01:27:45.920]   There's a whole bunch of things I could ask about that plan about whether you're
[01:27:45.920 --> 01:27:49.920]   going to get credible promised. You will get some part of galaxies, whether they care about
[01:27:49.920 --> 01:27:55.040]   that. I just help you enforce stuff. We'll leave that aside. That's a different rabbit hole. The
[01:27:55.040 --> 01:28:00.080]   thing I want to ask is, but it has to be the thing we need. The only way this is possible is if we
[01:28:00.080 --> 01:28:05.760]   lock it down. I see if we don't lock it down, we are in this fever struggle, greatest peril of
[01:28:05.760 --> 01:28:12.800]   mankind will have ever seen. So, but given the fact that in during this period, instead of just
[01:28:12.800 --> 01:28:17.040]   taking their chances and they don't really understand how this AI governance scheme is
[01:28:17.040 --> 01:28:21.280]   going to work, where they're going to check, whether we actually get the galaxies, the data
[01:28:21.280 --> 01:28:24.400]   centers, they can't be built underground. They have to be able to be built above ground. Taiwan
[01:28:24.400 --> 01:28:29.040]   is right off the coast of us. They need the chips from there. Why aren't we just going to invade?
[01:28:29.040 --> 01:28:32.880]   Listen, we don't want like worst case scenario is they win the super intelligence, which they're
[01:28:32.880 --> 01:28:38.240]   on track to do anyways. Wouldn't this instigate them to either invade Taiwan or blow up the data
[01:28:38.240 --> 01:28:41.920]   center in Arizona or something like that? Yeah. I mean, look, I mean, you talked about the data
[01:28:41.920 --> 01:28:45.040]   center one and then, you know, you probably have to like threaten nuclear retaliation to protect
[01:28:45.040 --> 01:28:48.640]   that. They might also just blow it up. There's also maybe ways they can do it without sort of
[01:28:48.640 --> 01:28:52.480]   attribution, right? Like you pay us such net Stuxnet. Yeah. I mean, this is, I mean, this is
[01:28:52.480 --> 01:28:55.440]   part of, we'll talk about this later, but, you know, I think, um, look, I think we need to be
[01:28:55.440 --> 01:29:00.800]   working on the Stuxnet for the Chinese project, but the, um, but by the audience, I want, I mean,
[01:29:00.800 --> 01:29:06.000]   Taiwan, the Taiwan thing, the, um, you know, you know, I talk about, you know, AGI by, you know,
[01:29:06.000 --> 01:29:11.840]   27 or whatever. Um, do you, do you know about the like terrible twenties? No. Okay. Well, I mean,
[01:29:11.840 --> 01:29:15.360]   sort of in this sort of Taiwan watcher circles, people often talk about like the late 2020s is
[01:29:15.360 --> 01:29:19.440]   like maximum period of risk for Taiwan because sort of like, you know, military modernization
[01:29:19.440 --> 01:29:23.040]   cycles and basically extreme fiscal tightening on, on the military budget in the United States
[01:29:23.040 --> 01:29:26.720]   over the last decade or two, um, has meant that sort of, we're in this kind of like, you know,
[01:29:26.720 --> 01:29:30.960]   trough in, in, in the late twenties of like, you know, basically overall naval capacity.
[01:29:30.960 --> 01:29:33.760]   And, you know, that's sort of when China is saying they want to be ready. So it's already
[01:29:33.760 --> 01:29:37.200]   kind of like, it's kind of pitching, you know, there's some sort of like, you know, parallel
[01:29:37.200 --> 01:29:41.600]   timeline there. Um, yeah, look, it looks appealing to invade Taiwan. I mean, maybe not. Cause they,
[01:29:41.600 --> 01:29:46.720]   you know, basically remote cutoff of the chips. Um, um, and so then it doesn't mean they get the
[01:29:46.720 --> 01:29:51.360]   chips, but it just means they, um, they, um, uh, you know, it's just, it's, you know, the machines
[01:29:51.360 --> 01:29:56.640]   are deactivated, but, um, look, I mean, imagine if during the cold war, you know, all of the world's
[01:29:56.640 --> 01:30:00.880]   uranium deposits had been in Berlin, you know, and Berlin was already, I mean, almost multiple
[01:30:00.880 --> 01:30:07.600]   times it was caused nuclear war. So, um, God help us all. Well, the Groves had a plan after the,
[01:30:07.600 --> 01:30:13.040]   after the war that the plan was that America would go around the world and getting the rights to
[01:30:13.040 --> 01:30:16.320]   every single uranium deposit because they didn't realize how much uranium there was in the world.
[01:30:16.320 --> 01:30:19.280]   And they thought this was the thing that was feasible, not realizing, of course, that there's
[01:30:19.280 --> 01:30:25.760]   like huge deposits in the Soviet union itself. Right. Um, um, okay. There's a, there's always a,
[01:30:25.760 --> 01:30:31.760]   there's a lot of East German workers who kind of got screwed and got cancer. Okay. So the framing
[01:30:31.760 --> 01:30:38.320]   we've been talking about that we've been assuming, and I'm not sure I buy yet is that the United
[01:30:38.320 --> 01:30:43.760]   States, this is our leverage. This is our data center. The China is a competitor right now.
[01:30:43.760 --> 01:30:47.360]   Obviously that's not the way things are progressing. Private companies control these AIs.
[01:30:47.360 --> 01:30:53.760]   They're deploying them. It's a market-based thing. Um, why, why, why will it be the case that the,
[01:30:53.760 --> 01:30:58.160]   it's like the United States, it has this leverage or is doing this thing versus China is doing this
[01:30:58.160 --> 01:31:02.880]   thing? Yeah. I mean, look, look on the, on the project, you know, I mean, there's sort of
[01:31:02.880 --> 01:31:06.640]   descriptive and prescriptive claims or sort of normative positive claims. I think the main thing
[01:31:06.640 --> 01:31:10.720]   I'm trying to say is, you know, you know, look, we're at, we're at these SF parties or whatever.
[01:31:10.720 --> 01:31:14.560]   And I think people talk about AGI and they're always just talking about the private AI labs.
[01:31:14.560 --> 01:31:18.640]   And I think I just really want to challenge that assumption. It just seems like it seems pretty
[01:31:18.640 --> 01:31:22.800]   likely to me, you know, as we've talked about, for reasons we've talked about that look like
[01:31:22.800 --> 01:31:26.880]   the national security state is going to get involved. And, um, you know, I think there's
[01:31:26.880 --> 01:31:30.800]   a lot of ways this could look like, right. Is it, is it like nationalization? Is it a public private
[01:31:30.800 --> 01:31:34.480]   partnership? Is it a kind of defense contractor like relationship? Is it a sort of government
[01:31:34.480 --> 01:31:40.960]   project that's soaks up all the people? Um, and so there's a spectrum there. Um, but I think people
[01:31:40.960 --> 01:31:46.320]   are just vastly underrating, um, the chances of this more or less looking like a government project.
[01:31:46.320 --> 01:31:51.120]   Um, and look, I mean, look, if, if, you know, it's sort of like, you know, do you, do you think,
[01:31:51.120 --> 01:31:54.320]   do you think like we all have literal, like, you know, when we have like literal superintelligence
[01:31:54.320 --> 01:31:57.600]   on our cluster, right. And it's like, you know, you have a hundred billion, they're like, sorry,
[01:31:57.600 --> 01:32:00.880]   that you have a billion like superintelligence scientists that they can like hack everything.
[01:32:00.880 --> 01:32:04.640]   They can like Stuxnet, the Chinese data centers, you know, they're starting to build the robo armies,
[01:32:04.640 --> 01:32:07.440]   you know, you like, you really think that'll be like a private company and the government
[01:32:07.440 --> 01:32:13.200]   wouldn't be like, Oh my God, what is going on? You know, like, yeah, suppose there's no China,
[01:32:13.200 --> 01:32:18.240]   suppose there's people like Iran, North Korea, who theoretically at some point will go to do
[01:32:18.240 --> 01:32:22.160]   superintelligence, but they're not on our heels and they don't have the ability to be on our heels
[01:32:22.160 --> 01:32:28.480]   in that world. Are you advocating for the national project or do you prefer the private path forward?
[01:32:28.480 --> 01:32:31.200]   Yeah. So, I mean, two responses to this one is, I mean, you still have like Russia,
[01:32:31.200 --> 01:32:35.520]   you still have these other countries, you know, you've got to have Russia proof security,
[01:32:35.520 --> 01:32:38.800]   right. It's like, you, you can't, you can't just have Russia steal all your stuff. And like,
[01:32:38.800 --> 01:32:41.760]   maybe their clusters aren't going to be as big, but like, they're still going to be able to make
[01:32:41.760 --> 01:32:46.160]   the crazy bioweapons and the, you know, the mosquito size drone swarm, you know, and so on.
[01:32:46.160 --> 01:32:51.920]   And so, I mean, I think, I think, I think the security component is just actually a pretty
[01:32:51.920 --> 01:32:56.480]   large component of the project in the sense of like, I currently do not see another way where
[01:32:56.480 --> 01:33:00.960]   we don't kind of like instantly proliferate this to everybody. And so, yeah, so I think it's sort
[01:33:00.960 --> 01:33:03.920]   of like, you still have to deal with Russia, you know, Iran, North Korea, and, you know, like,
[01:33:03.920 --> 01:33:06.880]   you know, Saudi and Iran are going to be trying to get it because they want to screw each other.
[01:33:06.880 --> 01:33:09.120]   And, you know, Pakistan and India, because they want to screw each other. There's like this
[01:33:09.120 --> 01:33:13.600]   enormous destabilization still. That said, look, I agree with you if, if, you know, if, you know,
[01:33:13.600 --> 01:33:16.880]   by some, somehow things are checking out differently. And like, you know, AGI would
[01:33:16.880 --> 01:33:22.800]   have been in 2005, you know, sort of like unparalleled in American hegemony. I think
[01:33:22.800 --> 01:33:28.080]   there would have been more scope for less government involvement. But again, you know,
[01:33:28.080 --> 01:33:30.160]   as we were talking about earlier, I think that would have been sort of this, like,
[01:33:30.160 --> 01:33:34.000]   very unique moment in history. And I think basically, you know, almost all other moments
[01:33:34.000 --> 01:33:37.120]   in history, there would have been this sort of great power competitor.
[01:33:37.120 --> 01:33:44.160]   So, okay, so let's get into this debate. So I, my position here is, if you look at the people
[01:33:44.160 --> 01:33:48.640]   who are involved in the Manhattan project itself, many of them regretted their participation,
[01:33:48.640 --> 01:33:54.960]   as you said. Now we can infer from that, that we should sort of start off with a cautious approach
[01:33:54.960 --> 01:34:01.520]   to the nationalized ASI project. Then you might say, well, listen, obviously the super.
[01:34:01.520 --> 01:34:06.080]   Did they regret their participation because of the project or because of the technology itself?
[01:34:06.080 --> 01:34:09.120]   I think people will regret it, but I think it's, it's, it's, it's about the nature of
[01:34:09.120 --> 01:34:13.920]   the technology and it's not about the project. I think they also probably had a sense that
[01:34:13.920 --> 01:34:18.240]   different decisions would have been made if it wasn't some concerted effort that everybody had
[01:34:18.240 --> 01:34:23.440]   agreed to participate in. That if it wasn't in the context of this, we need to race to beat
[01:34:23.440 --> 01:34:27.680]   Germany and Japan, you might not develop. So that's the technology part, but also like,
[01:34:27.680 --> 01:34:30.880]   you wouldn't actually like hit them with, you know, it's like the sort of the destructive
[01:34:30.880 --> 01:34:34.640]   potential, the sort of, you know, military potential. It's not, it's not because of
[01:34:34.640 --> 01:34:39.440]   the project. It is because of the technology and that will unfold regardless, you know.
[01:34:39.440 --> 01:34:42.880]   But I think this underrates the power of modeling.
[01:34:42.880 --> 01:34:46.480]   Imagine you go through like the 20th century in like, you know, a decade,
[01:34:46.480 --> 01:34:49.600]   uh, you know, it's just the, the sort of, the sort of, yes, great technological progress.
[01:34:49.600 --> 01:34:53.760]   So let's just actually run that example. Suppose you actually, there was some reason that the 20th
[01:34:53.760 --> 01:34:58.160]   century would be run through in one decade. Do you think the cause of that should have been,
[01:34:58.160 --> 01:35:01.840]   um, should have been like the technologies that happened through the 20th century shouldn't have
[01:35:01.840 --> 01:35:07.760]   been, um, privatized. That it should have been a more sort of concerted, uh, uh, government led
[01:35:07.760 --> 01:35:14.160]   project, you know, look, there is a history of just dual use technologies. Right. And so I think
[01:35:14.160 --> 01:35:17.760]   AI in some sense is going to be dual use in the same way. And so there's going to be lots of
[01:35:17.760 --> 01:35:21.360]   civilian uses of it, right? Like nuclear energy, it's like itself, right. It was like, you know,
[01:35:21.360 --> 01:35:24.320]   there's the government project developed the military angle of it. And then, you know,
[01:35:24.320 --> 01:35:27.440]   it was like, you know, then the government worked with private companies. There's a sort of like
[01:35:27.440 --> 01:35:30.800]   real, like flourishing of nuclear energy until, you know, the environmental has stopped it.
[01:35:30.800 --> 01:35:35.280]   Um, you know, um, um, um, planes, right. Like Boeing, right. Actually, you know,
[01:35:35.280 --> 01:35:38.400]   the Manhattan project wasn't the biggest defense R and D project during world or two,
[01:35:38.400 --> 01:35:41.840]   it was the B 29 bomber, right. Cause they needed the bomber that had long enough range to reach
[01:35:41.840 --> 01:35:47.680]   Japan, um, to destroy their cities. Um, and then, you know, Boeing made some Boeing that B Boeing
[01:35:47.680 --> 01:35:52.160]   made the B 47 made the B 52, you know, the plane, the U S military uses today. And then they use
[01:35:52.160 --> 01:35:57.120]   that technology later on to, um, to, you know, build the seven Oh seven. And there's sort of
[01:35:57.120 --> 01:36:01.440]   the, but what does for later on me in this context, because in the other, like I get what it
[01:36:01.440 --> 01:36:08.080]   means after a war to privatize, but if you have the government has ASI, maybe just let me back up
[01:36:08.080 --> 01:36:13.120]   and explain my concern. So you have the only institution in our society, which has a monopoly
[01:36:13.120 --> 01:36:19.440]   on violence. Um, and then we're going to give the, give it some, uh, in a way that's not broadly
[01:36:19.440 --> 01:36:25.360]   deployed access to the ASI, the counterfactual, and this maybe sounds silly, but listen, we're
[01:36:25.360 --> 01:36:30.320]   going to go through higher and higher levels of intelligence. Yeah. Private companies will be
[01:36:30.320 --> 01:36:35.520]   required by regulation to increase their security, but they'll still be private companies and they'll
[01:36:35.520 --> 01:36:39.920]   deploy this and they're going to release the AGI. Now McDonald's and JP Morgan and some random
[01:36:39.920 --> 01:36:44.240]   startup are now more effective organizations because they have a bunch of AGI workers and
[01:36:44.240 --> 01:36:48.880]   it'll be sort of like the industrial revolution in the sense that the benefits were widely diffused.
[01:36:48.880 --> 01:36:55.200]   If you don't end up in a situation like that, then the, I mean, even backing up, like, what is it?
[01:36:55.200 --> 01:36:58.400]   We're trying to, why do we want to win against China? We want to win against China because we
[01:36:58.400 --> 01:37:06.400]   don't want a top-down authoritarian system to win. Now, if the way to beat that is that the
[01:37:06.400 --> 01:37:12.240]   most important technology that humanity will have has to be controlled by a top-down government,
[01:37:12.240 --> 01:37:17.440]   like what, what was the point? Like maybe, so let's like run our cards with privatization.
[01:37:17.440 --> 01:37:21.280]   That's the way we get to the classic liberal market-based system we want for the ASI.
[01:37:21.280 --> 01:37:25.440]   Yeah. All right. So a lot of talk about here. Um, I think, yeah, maybe I'll start a bit about
[01:37:25.440 --> 01:37:28.720]   like actually looking at what the private world would look like. And I think this is part of where
[01:37:28.720 --> 01:37:32.480]   the sort of, there's no alternative comes from. And then let's look like, look at like what the
[01:37:32.480 --> 01:37:35.920]   government project looks like, what checks and balances look like and so on. All right. Private
[01:37:35.920 --> 01:37:40.320]   world. I mean, first of all, okay. So right. Like a lot of people right now talk about open source.
[01:37:40.320 --> 01:37:43.280]   And I think there's this sort of misconception that like AGI development is going to be like,
[01:37:43.280 --> 01:37:46.160]   oh, it's going to be some like beautiful decentralized thing. And, you know, like,
[01:37:46.160 --> 01:37:49.680]   you know, some giddy community of coders who gets to like, you know, collaborate on it.
[01:37:49.680 --> 01:37:52.720]   That's not how it's going to look like, right. You know, it's, you know, a hundred billion
[01:37:52.720 --> 01:37:55.680]   dollar trillion dollar cluster. It's not going to be that many people that have it. The algorithms,
[01:37:55.680 --> 01:37:58.800]   you know, it's like right now, open source is kind of good because people just use the stuff
[01:37:58.800 --> 01:38:01.920]   that was published. And so they basically, you know, the algorithms were published or, you know,
[01:38:01.920 --> 01:38:05.200]   as Mistral, they just kind of like leave DeepMind and, you know, take all the secrets with them and
[01:38:05.200 --> 01:38:10.240]   they just kind of replicate it. Um, um, but that's not going to continue to be in the case. And so,
[01:38:10.240 --> 01:38:13.920]   you know, the sort of like open source alternative, I mean, also people say stuff like, you know, 10,
[01:38:13.920 --> 01:38:17.840]   26 flops, it'll be in my phone or, you know, it's no, it won't, you know, it's like Moore's law is
[01:38:17.840 --> 01:38:21.120]   really slow. I mean, AI chips are getting better, but like, you know, the a hundred billion dollar
[01:38:21.120 --> 01:38:25.120]   computer will not cost, you know, like a thousand dollars, you know, within your lifetime or
[01:38:25.120 --> 01:38:29.040]   whatever, aside from me. So anyway, so it's going to be, it's going to be like two or three, you
[01:38:29.040 --> 01:38:36.160]   know, big players, um, on the private world. And so look, a few things. So first of all,
[01:38:36.160 --> 01:38:41.440]   you know, you talk about the sort of like, you know, enormous power that
[01:38:41.440 --> 01:38:44.480]   sort of super intelligence will have and that the government will have.
[01:38:44.480 --> 01:38:50.320]   I think it's pretty plausible that the alternative world is that like one AI company has that power,
[01:38:50.320 --> 01:38:52.800]   right. And it's basically, if we're talking about lead, you know, it's like what, I don't know,
[01:38:52.800 --> 01:38:56.720]   open AI has a six month lead. And then, you know, so then you're not talking, you're talking about
[01:38:56.720 --> 01:39:00.880]   basically, you know, the most powerful weapon ever. And it's, you know, you're kind of making
[01:39:00.880 --> 01:39:04.800]   this like radical bet on like a private company CEO is the benevolent dictator.
[01:39:04.800 --> 01:39:08.320]   No, no, this is not necessarily like any other thing that's privatized. We don't count on that
[01:39:08.320 --> 01:39:14.000]   being benevolent. We just look to think of, for example, somebody who manufactures industrial
[01:39:14.000 --> 01:39:19.920]   fertilizer, right. This is the person with this factory. If they went back to an ancient
[01:39:19.920 --> 01:39:23.600]   civilization, they could like blow up Rome. They could probably blow up Washington DC.
[01:39:23.600 --> 01:39:29.200]   And I think in their series, you talk about Tyler Cowen's phrase of muddling through.
[01:39:29.200 --> 01:39:33.040]   And I think even with privatization, people sort of underrate that there are actually a lot of
[01:39:33.040 --> 01:39:37.360]   private actors who have the ability to like, there's a lot of people who control the water
[01:39:37.360 --> 01:39:42.880]   supply or whatever. And we can count on cooperation and market-based incentives to
[01:39:42.880 --> 01:39:46.560]   basically keep a balance of power. Sure. I gather things are proceeding really fast,
[01:39:46.560 --> 01:39:49.200]   but we like, we have a lot of historical evidence that this is the thing that works best.
[01:39:49.200 --> 01:39:53.760]   So look, I mean, I mean, what do we do with nukes, right? The way we keep the sort of nukes in check
[01:39:53.760 --> 01:39:57.440]   is not like, you know, a sort of beefed up second amendment where like each state has their own like
[01:39:57.440 --> 01:40:01.520]   little nuclear arsenal and like, you know, Dario and Sam have their own little nuclear arsenal.
[01:40:01.520 --> 01:40:06.640]   No, no, it's like, it's, it's institutions, it's constitutions, it's laws, it's, it's, it's courts.
[01:40:06.640 --> 01:40:12.000]   And so, so I don't actually, I'm not sure that this, you know, I'm not sure that the sort of
[01:40:12.000 --> 01:40:16.640]   balance of power analogy holds. In fact, you know, sort of the government having the biggest guns
[01:40:16.640 --> 01:40:20.000]   was sort of like an enormous civilizational achievement, right? Like Landfrieden in the
[01:40:20.000 --> 01:40:23.200]   sort of Holy Roman Empire, right? You know, if somebody from the town over kind of committed
[01:40:23.200 --> 01:40:27.840]   a crime on you, you know, you didn't kind of start a sort of a, you know, a big battle between the
[01:40:27.840 --> 01:40:32.240]   two towns. No, you take it to a court of the Holy Roman Empire and they would decide. And it's,
[01:40:32.240 --> 01:40:35.600]   it's a big achievement. Now the, the thing about, you know, the industrial fertilizer,
[01:40:35.600 --> 01:40:38.960]   I think the key difference is kind of speed and offense, defense balance issues, right?
[01:40:38.960 --> 01:40:45.680]   So it's like 20th century and, you know, 10 years and a few years. That is an incredibly scary
[01:40:45.680 --> 01:40:49.760]   period. And it is incredibly scary, you know, cause it's, you know, you're going through just
[01:40:49.760 --> 01:40:53.440]   this sort of enormous array of destructive technology and this sort of like enormous
[01:40:53.440 --> 01:40:57.200]   amount of like, you know, basically military advance. I mean, you would have gone from,
[01:40:57.200 --> 01:41:02.000]   you know, kind of like, you know, you know, bayonets and horses to kind of like tank armies
[01:41:02.000 --> 01:41:05.520]   and fighter jets in like a couple of years. And then from, you know, like, you know, and then to
[01:41:05.520 --> 01:41:08.800]   like, you know, nukes and, you know, ICBMs and stealth, you know, and just like in a matter of
[01:41:08.800 --> 01:41:14.960]   years. And so it is sort of that speed that creates, I think basically the way I think
[01:41:14.960 --> 01:41:18.880]   about it is there's going to be this initial, just incredibly volatile, incredibly dangerous period.
[01:41:18.880 --> 01:41:22.480]   And somehow we have to make it through that. And that's going to be incredibly challenging.
[01:41:22.480 --> 01:41:27.600]   That's where you need the kind of government project. If you can make it through that,
[01:41:27.600 --> 01:41:31.280]   then you kind of go to like, you know, now we can now, you know, the situation has been stabilized.
[01:41:31.280 --> 01:41:34.000]   You know, we don't face this imminent national security threat. You know, it's like, yes,
[01:41:34.000 --> 01:41:37.600]   there were kind of WMDs that came along the way, but either we've managed to kind of like
[01:41:37.600 --> 01:41:40.800]   have a sort of stable offense-defense balance, right? Like I think bioweapons initially are
[01:41:40.800 --> 01:41:44.480]   a huge issue, right? Like an attacker can just create like a thousand different synthetic,
[01:41:44.480 --> 01:41:47.600]   you know, viruses and spread them. And it's like going to be really hard for you to kind of like
[01:41:47.600 --> 01:41:50.720]   make a defense against each, but maybe at some point you figure out the kind of like, you know,
[01:41:50.720 --> 01:41:54.480]   universal defense against every possible virus. And then you're in a stable situation again on
[01:41:54.480 --> 01:41:57.680]   the offense-defense balance, or you do the thing, you know, you do with planes where it's, there's
[01:41:57.680 --> 01:42:01.040]   like, you know, there's certain capabilities that the private sector isn't allowed to have.
[01:42:01.040 --> 01:42:04.320]   And you've like figured out what's going on, restrict those. And then you can kind of like,
[01:42:04.320 --> 01:42:07.760]   let, let, you know, you let this sort of civilian, civilian uses.
[01:42:07.760 --> 01:42:12.320]   - So I'm skeptical of this because, well, there's...
[01:42:12.320 --> 01:42:16.160]   - And then, sorry, I mean, the other important thing is, so I talked about this sort of, you
[01:42:16.160 --> 01:42:20.480]   know, maybe it's like, it's, it's a, you know, it's, you know, it's one company with all this
[01:42:20.480 --> 01:42:23.360]   power. And I think it's like, I think it is unprecedented because it's like the industrial
[01:42:23.360 --> 01:42:26.960]   fertilizer guy cannot overthrow the U.S. government. I think it's quite plausible
[01:42:26.960 --> 01:42:29.280]   that like the AI company with super intelligence can overthrow the U.S. government.
[01:42:29.280 --> 01:42:32.240]   - But there'll be multiple AI companies, right? And I buy that one of them could be ahead.
[01:42:32.240 --> 01:42:35.760]   - So it's not obvious that it'll be multiple. I think it's, again, if there's like a six month
[01:42:35.760 --> 01:42:39.200]   lead, maybe, maybe there's two or three. Sorry, but if there's two or three, then what you have
[01:42:39.200 --> 01:42:42.640]   is just like a crazy race between these two or three companies. You know, it's like, you know,
[01:42:42.640 --> 01:42:46.640]   whatever, Demis and Sam, they're just like, I don't want to let the other one win. And,
[01:42:46.640 --> 01:42:50.640]   and they're both developing their nuclear arsenals and the robot. It's just like, also like, come on,
[01:42:50.640 --> 01:42:53.120]   the government is not going to let these people, you know, are they going to let like, you know,
[01:42:53.120 --> 01:42:57.440]   is Dario going to be the one developing the kind of like, you know, you know, super hacking Stuxnet
[01:42:57.440 --> 01:43:01.920]   and like deploying against the Chinese data center. The other issue though, is it won't just,
[01:43:01.920 --> 01:43:04.880]   if it's two or three, it won't just be two or three, there'll be two or three, and there'll be
[01:43:04.880 --> 01:43:09.440]   China and Russia, North Korea, because the private in the private lab world, there is no way they'll
[01:43:09.440 --> 01:43:12.320]   have security that is good enough. - I think we're also assuming that
[01:43:12.320 --> 01:43:16.560]   somehow if you nationalize it, like the security just, especially in the world where
[01:43:16.560 --> 01:43:23.520]   this stuff is priced in by the CCP, that now you've like got it nailed down. And I'm not
[01:43:23.520 --> 01:43:26.960]   sure why we would expect that to be the case, but on this... - The government's the only one
[01:43:26.960 --> 01:43:32.080]   who does this stuff. - So if it's not Sam or Dario, who's, we don't trust them to be a benevolent
[01:43:32.080 --> 01:43:37.280]   dictator or whatever. - We're just corporate governors. - So, but here we're counting on,
[01:43:37.280 --> 01:43:42.320]   if it's because you can cause a coup, the same capabilities are going to be true of the government
[01:43:42.320 --> 01:43:49.200]   project, right? And so the modal president in 2020, 2025, but Donald Trump will be the person
[01:43:49.200 --> 01:43:54.720]   that you don't trust Sam or Dario to have these capabilities. And why, okay, I agree that I'm
[01:43:54.720 --> 01:44:01.120]   worried if Sam or Dario have a one-year lead on ASI in that world, then I'm concerned about this
[01:44:01.120 --> 01:44:05.760]   being privatized. But in that exact same world, I'm very concerned about Donald Trump having the
[01:44:05.760 --> 01:44:09.760]   capability. And potentially if we're living in a world where the takeoff is slower than you
[01:44:09.760 --> 01:44:15.040]   anticipate, in that world, I'm like very much, I want the private company. So in no part of this
[01:44:15.040 --> 01:44:19.280]   matrix, this is obviously true that the government led project is better than the private project.
[01:44:19.280 --> 01:44:21.360]   - Let's talk about the government project a little bit and checks and balances.
[01:44:21.360 --> 01:44:25.280]   In some sense, I think my argument is a sort of Birkin argument, which is like
[01:44:25.280 --> 01:44:30.480]   American checks and balances have held for over 200 years and through crazy technological
[01:44:30.480 --> 01:44:34.000]   revolutions, the US military could kill like every civilian in the United States.
[01:44:34.000 --> 01:44:37.520]   - But you're going to make that argument, the private public balance of power has held for
[01:44:37.520 --> 01:44:39.520]   hundreds of years. - Corporate, but yeah,
[01:44:39.520 --> 01:44:43.600]   why has it held? Because the government has the biggest guns and has never before has a single
[01:44:43.600 --> 01:44:49.760]   CEO or a random nonprofit board had the ability to launch nukes. And so again, it's like, what is
[01:44:49.760 --> 01:44:52.640]   the track record of the government checks and balances versus the track record of the private
[01:44:52.640 --> 01:44:57.920]   company checks and balances? Well, the iLab, first stress test went really badly, that didn't really
[01:44:57.920 --> 01:45:05.200]   work. I mean, even worse in the sort of private company world. So it's both like, it is like the
[01:45:05.200 --> 01:45:09.280]   two private companies and the CCP and they just like instantly have all the shit. And then it's,
[01:45:10.240 --> 01:45:13.280]   they probably won't have good enough internal control. So it's like, not just like the random
[01:45:13.280 --> 01:45:17.440]   CEO, but it's like, you know, rogue employees that can kind of like use these super intelligences to
[01:45:17.440 --> 01:45:19.440]   do whatever they want. - And this won't be true of the
[01:45:19.440 --> 01:45:22.400]   government? Like the rogue employees won't exist on the project?
[01:45:22.400 --> 01:45:27.600]   - Well, the government actually like, you know, has decades of experience and like actually really
[01:45:27.600 --> 01:45:30.960]   cares about the stuff. I mean, it's like, they deal with nukes, they deal with really powerful
[01:45:30.960 --> 01:45:34.560]   technology. And it's, you know, this is like, this is the stuff that the national security
[01:45:34.560 --> 01:45:38.320]   state cares about. You know, again, let's talk about the government checks and balances a little
[01:45:38.320 --> 01:45:42.160]   bit. So, you know, what are checks and balances in the government world? First of all, I think
[01:45:42.160 --> 01:45:45.200]   it's actually quite important that you have some amount of international coalition. And I talked
[01:45:45.200 --> 01:45:48.720]   about these sort of two tiers before. Basically, I think the inner tier is a sort of model on the
[01:45:48.720 --> 01:45:53.520]   Quebec agreement, right? This was like Churchill and Roosevelt, they kind of agreed secretly,
[01:45:53.520 --> 01:45:58.080]   we're going to like pull our efforts on nukes, but we're not going to use them against each other.
[01:45:58.080 --> 01:46:01.120]   And we're not going to use them against anyone else with their consent. And I think basically,
[01:46:01.120 --> 01:46:04.640]   look, bring in, bring in the UK, they have DeepMind, bring in the kind of like Southeast
[01:46:04.640 --> 01:46:08.160]   Asian states who have the chip supply chain, bring in some more kind of like NATO, close
[01:46:08.160 --> 01:46:12.560]   democratic allies for, you know, talent and industrial resources. And you have this sort
[01:46:12.560 --> 01:46:15.280]   of like, you know, so you have you have those checks and balances in terms of like more
[01:46:15.280 --> 01:46:20.080]   international countries at the table. Sorry, somewhat separately, but then you have the sort
[01:46:20.080 --> 01:46:23.680]   of second tier of coalitions, which is the sort of atoms for peace thing, where you go to a bunch
[01:46:23.680 --> 01:46:27.360]   of countries, including like the UAE. And you're like, look, we're going to basically like, you
[01:46:27.360 --> 01:46:31.200]   know, there's a deal similar to like the NPT stuff, where it's like, you're not allowed to like,
[01:46:31.200 --> 01:46:34.720]   do the crazy military stuff. But we're going to share the civilian applications, we're in fact,
[01:46:34.720 --> 01:46:39.760]   going to help you and share the benefits and, you know, sort of kind of like this new sort of post
[01:46:39.760 --> 01:46:43.920]   superintelligence world order. All right, US checks and balances, right? So obviously, Congress is
[01:46:43.920 --> 01:46:47.280]   going to have to be involved, right? Appropriate trillions of dollars, I think probably ideally,
[01:46:47.280 --> 01:46:51.680]   you have Congress needs to kind of like confirm whoever's running this. So you have Congress,
[01:46:51.680 --> 01:46:55.040]   you have like different factions of government, you have the courts, I expect the First Amendment
[01:46:55.040 --> 01:46:58.080]   to continue being really important. And maybe that I think that sounds kind of crazy to people.
[01:46:58.080 --> 01:47:02.160]   But I actually think again, I think these are like, institutions that have withheld the test of time,
[01:47:02.160 --> 01:47:07.040]   in a really sort of powerful way. You know, eventually, you know, this is why honestly,
[01:47:07.040 --> 01:47:12.160]   alignment is important is like, you know, the eyes, you program the eyes to follow the Constitution.
[01:47:12.160 --> 01:47:17.600]   And it's like, you know, why does the military work? It's like generals, you know, are not allowed
[01:47:17.600 --> 01:47:20.880]   to follow unlawful orders are not allowed to follow unconstitutional orders, you have the
[01:47:20.880 --> 01:47:26.080]   same thing for the eyes. So what's wrong with this argument? When you say listen, maybe you have a
[01:47:26.080 --> 01:47:29.840]   point in the world where we have extremely fast takeoff, it's like one year from AGI to ASI.
[01:47:29.840 --> 01:47:34.400]   Yeah. And then you have the like, sure, years after VSI, where you have this like extraordinary
[01:47:34.400 --> 01:47:40.160]   point. Yeah, we don't know. You have these arguments will like get into the weeds on them
[01:47:40.160 --> 01:47:43.040]   about why that's a more likely world. But like, maybe that's not the world we live in. Yeah. And
[01:47:43.040 --> 01:47:49.200]   in the other world, I'm like, very on the side of making sure that these things are privately held.
[01:47:49.200 --> 01:47:55.280]   Now, why? So when you nationalize? Yeah, that's a one way function, you can't go back.
[01:47:55.840 --> 01:48:02.560]   Why not wait until we have more evidence on which of those worlds we live in? Why? I think like
[01:48:02.560 --> 01:48:08.080]   rushing on the nationalization might be a bad idea while we're not sure. And, okay, I'll listen to
[01:48:08.080 --> 01:48:11.920]   that first. I mean, I don't I don't expect us to nationalize tomorrow. If anything, I expect it to
[01:48:11.920 --> 01:48:15.680]   be kind of with COVID or it's like kind of too late. Like, ideally, you nationalize it early
[01:48:15.680 --> 01:48:19.360]   enough to like actually lock stuff down, it'll probably be kind of chaotic. And like, you're
[01:48:19.360 --> 01:48:22.640]   gonna be trying to like do this crash program to lock stuff down. And it'll be kind of late,
[01:48:22.640 --> 01:48:25.600]   it'll be kind of clear what's happening. We're not going to nationalize when it's not clear what's
[01:48:25.600 --> 01:48:30.240]   happening. I think the whole battle of the whole historically institutions have held up. Well,
[01:48:30.240 --> 01:48:33.760]   first of all, they've actually almost broken a bunch of times. It's like, this is, this is,
[01:48:33.760 --> 01:48:38.080]   this is the first argument that some people who are saying that we shouldn't be that concerned
[01:48:38.080 --> 01:48:42.960]   about nuclear war, say, or it's like, listen, we have the nuke for 80 years. And like, we've been
[01:48:42.960 --> 01:48:47.760]   fine so far. So the risk must be low. And then the answer to that is no, actually, it is a really high
[01:48:47.760 --> 01:48:51.840]   risk. And the reason we've avoided is like people have gone through a lot of effort to make sure
[01:48:51.840 --> 01:48:58.560]   that this thing doesn't happen. I don't think that giving government ASI without knowing what that
[01:48:58.560 --> 01:49:04.560]   implies is going through a lot of effort. And I think the base rate like you can talk about America,
[01:49:04.560 --> 01:49:09.200]   I think America is very exceptional. Yeah, not just in terms of dictatorship. But in terms of
[01:49:09.200 --> 01:49:13.680]   every other country in history has had a complete drawdown of wealth because of war revolution
[01:49:13.680 --> 01:49:18.400]   something. America is very unique and not having that. And the historical base rate, we're talking
[01:49:18.400 --> 01:49:21.680]   about great power competition, I think that has a really big, that's something we haven't been
[01:49:21.680 --> 01:49:25.600]   thinking about the last 80 years, but it's really big. Yeah. Dictatorship is also something that is
[01:49:25.600 --> 01:49:33.280]   just the default state of mankind. Yeah. And I think relying on institutions, which in an ASI
[01:49:33.280 --> 01:49:38.640]   world, like there's a fundamentally right now, if the government tried to overthrow, there's a,
[01:49:38.640 --> 01:49:42.400]   it's much harder if you don't have the ASI, right? Like there's people who have
[01:49:42.400 --> 01:49:48.000]   AK, AR four or 15s. And I really, there's like things that make it harder to crush the hour 15s.
[01:49:48.000 --> 01:49:51.440]   No, I think it should be pretty hard. The reason it was Vietnam and Afghanistan was pretty hard
[01:49:51.440 --> 01:49:56.240]   country. Yeah. Yeah. I agree. But like I'm good. I mean, it's similar to the ASI.
[01:49:56.240 --> 01:50:01.440]   Um, yeah, I think it's just like easier if you have what you're talking about. There are
[01:50:01.440 --> 01:50:04.800]   constitutions, there are legal restraints, there are courts, there are checks and balances.
[01:50:04.800 --> 01:50:09.520]   The crazy bet is the bet, which are like private company CEO, the same thing, by the way,
[01:50:09.520 --> 01:50:12.720]   isn't the same thing true of nukes to where we have these institutional agreements about
[01:50:12.720 --> 01:50:16.880]   non-proliferation and whatever. And we're still very concerned about that being broken
[01:50:16.880 --> 01:50:20.640]   and somebody getting nukes and like, you should stay up at night worrying about that situation.
[01:50:20.640 --> 01:50:25.440]   But ASI is going to be a really precarious situation as well. And like, given, given how
[01:50:25.440 --> 01:50:29.040]   precarious nukes are, we've done pretty well. And so what does privatization in this world even mean?
[01:50:29.040 --> 01:50:31.840]   I mean, I think the other thing is like, what happens after, I mean, the other thing, you know,
[01:50:31.840 --> 01:50:34.400]   cause we're talking about like whether the government project is good or not. And it's
[01:50:34.400 --> 01:50:39.200]   like, I have very mixed feelings about this as well. Again, I think my primary argument is like,
[01:50:39.280 --> 01:50:44.480]   you know, if you're at the point where this thing has like vastly superhuman hacking capabilities,
[01:50:44.480 --> 01:50:47.680]   if you're at the point where this thing can develop, you know, bioweapons, you know,
[01:50:47.680 --> 01:50:50.640]   like in crazy bioweapons, ones that are like targeted, you know, can kill everybody,
[01:50:50.640 --> 01:50:54.880]   but the hand Chinese or, you know, that, you know, you know, would, would wipe out, you know,
[01:50:54.880 --> 01:50:58.320]   entire countries where you're talking about like building robo armors, you're talking about kind
[01:50:58.320 --> 01:51:02.560]   of like drone swarms that are, you know, again, the mosquito sized drones that could take it out,
[01:51:02.560 --> 01:51:07.520]   you know, the United States national security state is going to be intimately involved with
[01:51:07.520 --> 01:51:10.800]   this. And this will, you know, the labs, whether, you know, and I think again, the government,
[01:51:10.800 --> 01:51:13.520]   a lot of what I think is the government project looks like it is basically a joint venture
[01:51:13.520 --> 01:51:17.520]   between like, you know, the cloud providers between some of the labs and the government.
[01:51:17.520 --> 01:51:22.480]   And so I think there is no world in which the government isn't intimately involved in this like
[01:51:22.480 --> 01:51:26.240]   crazy period, the very least basically, you know, like the intelligence agencies need to be running
[01:51:26.240 --> 01:51:29.360]   security for these labs. So they're already kind of like, they're controlling everything. They're
[01:51:29.360 --> 01:51:33.520]   controlling access to everything. Then they're going to be like, probably again, if we're in
[01:51:33.520 --> 01:51:37.440]   this like really volatile international situation, like a lot of the initial applications, it'll,
[01:51:37.440 --> 01:51:41.600]   it'll suck. It's not what I want to use ASI for. We'll be like trying to somehow stabilize this
[01:51:41.600 --> 01:51:47.280]   crazy situation. Somehow we need to prevent like proliferation of like some crazy new WMDs and like
[01:51:47.280 --> 01:51:51.440]   the undermining of mutually assured destruction to kind of like, you know, North Korea and Russia
[01:51:51.440 --> 01:51:58.160]   and China. And so I think, you know, I basically think your world, you know, I think there's much
[01:51:58.160 --> 01:52:01.120]   more spectrum than you're acknowledging here. And I think basically the world in which it's
[01:52:01.120 --> 01:52:04.720]   private labs is like extremely heavy government involvement. And really what we're debating is
[01:52:04.720 --> 01:52:08.560]   like, you know, what form of government project, but it is going to look much more like, you know,
[01:52:08.560 --> 01:52:12.960]   the national security state than anything. It does look like, like a startup as it is right now.
[01:52:12.960 --> 01:52:17.840]   And I think that, yeah, look, I think something like that makes sense. I think, you know,
[01:52:17.840 --> 01:52:22.800]   the, yeah, look, I think something like that makes sense. I would be, if it's like the Manhattan
[01:52:22.800 --> 01:52:28.800]   project, then I'm very worried where it's like, this is part of the U S military. Um, where if
[01:52:28.800 --> 01:52:32.960]   it's more like, listen, you've got to talk to Jake Sullivan before you like run the next training
[01:52:32.960 --> 01:52:36.960]   line. It's like Lockheed Martin skunk words, part of the U S military. It's like they call the shots.
[01:52:36.960 --> 01:52:40.640]   Yeah. I don't think that's great. I think that's, I think that's bad. I think it'd be bad if that
[01:52:40.640 --> 01:52:45.520]   happened with ASI. And like, what is it, what is the scenario? What is it? What is the alternative?
[01:52:45.520 --> 01:52:50.080]   Okay. So it's closer to my end of the spectrum where, yeah, you do have to talk to Jake Sullivan
[01:52:50.080 --> 01:52:54.880]   before you can launch the next training cluster. But there's many companies who are still going
[01:52:54.880 --> 01:53:01.280]   for it and the government will be intimately involved in the security. Yeah. The, but the,
[01:53:01.280 --> 01:53:03.360]   like three different companies are trying to launch the Stuxnet attack.
[01:53:03.360 --> 01:53:06.640]   Yeah. What do you, what do you, is launching, launching. Okay.
[01:53:06.640 --> 01:53:10.480]   Daria is the activating the Chinese data science.
[01:53:10.480 --> 01:53:13.040]   I think this is similar to the story you could tell about. There's a lot of companies,
[01:53:13.040 --> 01:53:17.680]   like literally the big tech right now. I think Sasha, if you wanted to, he probably like could
[01:53:17.680 --> 01:53:21.840]   get his engineers, like what are the zero days in windows and the companies and the, and like,
[01:53:21.840 --> 01:53:25.920]   well, how do we get info straight? The president's computer. So that like, we can shut down.
[01:53:25.920 --> 01:53:30.000]   No, no, no. Like right now I'm saying Sasha could do that. Right. Cause he knows shut down.
[01:53:30.000 --> 01:53:31.840]   What do you mean? Government wouldn't let them do that.
[01:53:31.840 --> 01:53:36.000]   Yeah. I think there's a story you could tell where like they could pull up, pull off a coup,
[01:53:36.000 --> 01:53:41.520]   whatever, but like, I think there's like multiple companies. Okay. Okay. Fine. Fine. Fine. I agree.
[01:53:41.520 --> 01:53:47.680]   I'm just saying like something closer to, so what's wrong with a scenario where, um, you,
[01:53:47.680 --> 01:53:53.360]   the government is, there's like multiple companies going for it, but the AI is still broadly deployed
[01:53:53.360 --> 01:53:58.160]   and alignment works in the sense that you can make sure that it's not, the system level prompt is
[01:53:58.160 --> 01:54:02.320]   like, you can't help people make bio weapons or something, but these are still broadly deployed.
[01:54:02.320 --> 01:54:05.360]   So that, I mean, I expect the eyes to be broadly deployed. I mean, first of all,
[01:54:05.360 --> 01:54:07.360]   even if it's a government project. Yeah. I mean, look, I think, first of all,
[01:54:07.360 --> 01:54:10.240]   like I think the matters of the world, you know, open sourcing their eyes, you know,
[01:54:10.240 --> 01:54:13.440]   that are two years behind or whatever. Yeah. Super valuable role. They're going to like,
[01:54:13.440 --> 01:54:16.720]   you know, and, and so there's going to be some question of like, either the offense
[01:54:16.720 --> 01:54:20.640]   defense balance is fine. And so like, even if they open source two-year-old AIs, it's fine.
[01:54:20.640 --> 01:54:23.680]   Or it's like, there's some restrictions on the most extreme dual use capabilities. Like, you
[01:54:23.680 --> 01:54:27.440]   know, you don't let private companies sell kind of crazy weapons. Um, and that's great. And that
[01:54:27.440 --> 01:54:31.200]   will help with the diffusion. And, you know, you know, after the government project, you know,
[01:54:31.200 --> 01:54:34.240]   there's going to be this initial tense period, hopefully that's stabilized. And then look,
[01:54:34.240 --> 01:54:38.080]   yeah, like Boeing, they're going to go out and they're going to like make, do all the flourishing
[01:54:38.080 --> 01:54:42.400]   civilian applications and, you know, like nuclear energy, you know, you know, like all the civilian
[01:54:42.400 --> 01:54:45.840]   applications will have their day. I think part of my argument here is that. And how does that
[01:54:45.840 --> 01:54:50.080]   proceed? Right? Because in the other world, there's existing stocks of capital that are worth
[01:54:50.080 --> 01:54:55.200]   a lot. There'll be still be Google clusters. And so Google, because they got the contract
[01:54:55.200 --> 01:54:58.000]   from the government, they'll be the ones that control the ASI. But like, why, why are they
[01:54:58.000 --> 01:55:02.080]   trading with anybody else? Why is it a random startup? It'll be the same. It'll be the same
[01:55:02.080 --> 01:55:05.440]   companies that would be doing it anyway. But in this, in this world, they're just contracting
[01:55:05.440 --> 01:55:08.320]   with the government or like their DPA for all their compute goes to the government.
[01:55:08.320 --> 01:55:12.640]   And, but it's very natural.
[01:55:12.640 --> 01:55:17.920]   After you get the ASI and then we're building the robot armies and building fusion reactors or
[01:55:17.920 --> 01:55:23.840]   whatever, that, that's only the government will get to build robo armies. Yeah, now I'm worried.
[01:55:23.840 --> 01:55:28.800]   Or like in the fusion reactors and stuff. It's the same situation we have today.
[01:55:28.800 --> 01:55:31.920]   Because if you already have the robo armies and everything, like the existing society doesn't
[01:55:31.920 --> 01:55:36.480]   have some leverage where it makes sense for the government to, yeah, the sense that there's like,
[01:55:36.480 --> 01:55:39.920]   you have a lot of capital that the government wants and there's other things like, why was
[01:55:39.920 --> 01:55:44.000]   Boeing privatized after? Government has the biggest guns. And the way we regulate is institutions,
[01:55:44.000 --> 01:55:47.920]   constitutions, legal restraints. Okay. So tell me what privatization looks like in the ASI world
[01:55:47.920 --> 01:55:51.360]   afterwards. Afterwards, like the Boeing example, right? It's like, you have this government.
[01:55:51.360 --> 01:55:55.360]   Who gets it? Like the Google, Google, Microsoft. And who are they selling it to? Like they already
[01:55:55.360 --> 01:55:58.240]   have the robo factory. It's like, why are they selling it to us? Like they already have the,
[01:55:58.240 --> 01:56:02.480]   they don't need like our, this is chum change in the ASI world because we didn't get like the,
[01:56:02.480 --> 01:56:07.040]   the ASI broadly deployed through throughout this takeoff. So we don't have the robo,
[01:56:07.040 --> 01:56:10.640]   we don't have like the fusion reactors and whatever advanced decades of advanced science
[01:56:10.640 --> 01:56:13.840]   that you're talking about. So like, it just, what, what are they trading with us for?
[01:56:13.840 --> 01:56:18.640]   Trading with whom for? Everybody who was not part of the project. They've got that technology
[01:56:18.640 --> 01:56:21.040]   that's decades ahead. Yeah. I mean, look, that's a whole nother issue of like, well,
[01:56:21.040 --> 01:56:24.160]   how does like economic distribution work or whatever? I don't know. That'll be rough.
[01:56:25.280 --> 01:56:29.280]   Yeah. I think I'm just saying, I don't, I don't, basically I'm kind of like, I don't see the
[01:56:29.280 --> 01:56:33.680]   alternative. The alternative is you like overturn a 500 year civilizational achievement of land
[01:56:33.680 --> 01:56:39.680]   fleet and you basically instantly leak the stuff to the CCP and either you like barely scrape out
[01:56:39.680 --> 01:56:44.320]   ahead. Um, and, but you're in this fever struggle, you're like proliferating crazy WMDs. It's just
[01:56:44.320 --> 01:56:47.600]   like enormously dangerous situation, enormously dangerous on alignment. Cause you're in this
[01:56:47.600 --> 01:56:51.440]   kind of like crazy race at the end and you don't have the ability to like take six months to get
[01:56:51.440 --> 01:56:57.120]   alignment right. Um, the alternative is, um, you know, alternative is like you aren't actually
[01:56:57.120 --> 01:57:01.280]   bundling your efforts to kind of like win the race against the authoritarian powers. Um, uh,
[01:57:01.280 --> 01:57:10.480]   you know, yeah. And so, you know, I don't like it. You know, I wish, I wish the thing we use
[01:57:10.480 --> 01:57:14.160]   the ASI for is to like, you know, cure the diseases and do all the good in the world.
[01:57:14.160 --> 01:57:23.040]   But it is my prediction that sort of like by the, in the end game what will be at stake will not
[01:57:23.040 --> 01:57:27.840]   just be kind of cool products, but what will be at stake is like whether liberal democracy survives,
[01:57:27.840 --> 01:57:32.320]   like whether the CCP survives, like what the world order for the next century will be.
[01:57:32.320 --> 01:57:36.480]   And when that is at stake, forces will be activated that are sort of way beyond what
[01:57:36.480 --> 01:57:42.400]   we're talking about now. And like, you know, in, in the sort of like crazy race at the end,
[01:57:42.400 --> 01:57:45.840]   like the sort of national security implications will be the most important, you know, sort of
[01:57:45.840 --> 01:57:50.320]   like, you know, World War II. It's like, yeah, you know, nuclear energy had its day, but in the
[01:57:50.320 --> 01:57:54.960]   initial kind of period when, you know, when this technology was first discovered, you had to
[01:57:54.960 --> 01:57:59.280]   stabilize the situation. You had to get nukes, you had to do it right. Um, and then, and then
[01:57:59.280 --> 01:58:03.760]   the civilian applications had their day. I think of closer analogy to what this is because nuclear,
[01:58:03.760 --> 01:58:06.720]   I agree that nuclear energy is a thing that happens later on and it's like dual use in
[01:58:06.720 --> 01:58:09.520]   that way, but it's, it's something that happened like literally a decade after nuclear weapons
[01:58:09.520 --> 01:58:14.080]   were developed. Whereas with AI, like the immediately all the applications are unlocked
[01:58:14.080 --> 01:58:17.680]   and it's closer to literally, I mean, this is an analogy people actually listen to me in the
[01:58:17.680 --> 01:58:23.520]   context of AGI is like, assume your society had a hundred million more John Wayne Neumanns. And I
[01:58:23.520 --> 01:58:26.800]   don't think like, if that was literally what happened, if tomorrow you just have a hundred
[01:58:26.800 --> 01:58:30.240]   million more of them, the approach would have been, well, some of them will convert to ISIS
[01:58:30.240 --> 01:58:34.640]   and we need to like be really careful about that. And then like, Oh, you know, like what if a bunch
[01:58:34.640 --> 01:58:38.880]   of them are born in China? And then we like, if we got to nationalize the John Wayne Neumanns,
[01:58:38.880 --> 01:58:43.280]   I'm like, no, I think it will be generally a good thing. And I'd be concerned about one power had
[01:58:43.280 --> 01:58:46.160]   getting like all the John Wayne Neumanns. I mean, I think the issue is the sort of
[01:58:46.160 --> 01:58:50.640]   like bottling up and the sort of intensely short period of time, like this enormous sort of like,
[01:58:50.640 --> 01:58:55.760]   you know, unfolding of technological progress of an industrial explosion. I mean, I think we do
[01:58:55.760 --> 01:58:58.480]   worry about the a hundred million John Wayne Neumanns and it's like rise of China. Why are
[01:58:58.480 --> 01:59:02.640]   we worried about the rise of China? Because it's like a hundred billion people and they're able to
[01:59:02.640 --> 01:59:06.240]   do a lot of industry and do a lot of technology. And, but it's just like, you know, the rise of
[01:59:06.240 --> 01:59:09.840]   China times like, you know, a hundred, because not just a hundred, one billion people, it's like a
[01:59:09.840 --> 01:59:15.360]   billion super intelligent, crazy, you know, crazy things. And, and in like, in a very short period.
[01:59:15.360 --> 01:59:20.800]   Let's talk practically, because if the goal is we need to beat China,
[01:59:20.800 --> 01:59:23.760]   part of that is protecting. I mean, that's one of the goals, right? I agree. I agree. Well,
[01:59:23.760 --> 01:59:28.240]   one of the goals is to beat China. And also just like manage this incredibly crazy, scary period.
[01:59:28.240 --> 01:59:32.720]   Right. So part of that is making sure we're not leaking algorithmic secrets to them. Part of that
[01:59:32.720 --> 01:59:37.520]   is a cluster. I mean, building the trillion dollar cluster, right? Yeah. But like your whole point,
[01:59:37.520 --> 01:59:40.960]   the Microsoft can release a corporate bonds that are, I think Microsoft can do the,
[01:59:40.960 --> 01:59:43.760]   like hundreds of billions of dollar cluster. I think, I think the trillion dollar cluster
[01:59:43.760 --> 01:59:48.400]   is closer to a national effort. I thought that your earlier point was that American capital
[01:59:48.400 --> 01:59:51.920]   markets are deep and so forth. Pretty good. I mean, I think the trillion, I think it's possible.
[01:59:51.920 --> 01:59:55.760]   It's private. It's possible. But it's going to be like, you know, by the way, at this point,
[01:59:55.760 --> 01:59:59.600]   we have a AGI that's drably accelerating productivity. I think the trillion dollar
[01:59:59.600 --> 02:00:03.920]   cluster is going to be planned before, before the AGI. I think, I think, I think it's sort of like
[02:00:03.920 --> 02:00:07.680]   you get the AGI on the like 10 gigawatt cluster, like intelligent, maybe you have like one more
[02:00:07.680 --> 02:00:10.720]   year where you're kind of doing some final and hobbling to fully unlock it. Then you have the
[02:00:10.720 --> 02:00:14.080]   intelligence explosion. And meanwhile, the like trillion dollar cluster is almost finished. And
[02:00:14.080 --> 02:00:16.800]   then you're like, and then you do your super intelligence on your trillion dollar cluster,
[02:00:16.800 --> 02:00:19.840]   or you run it on your trillion dollar cluster. And by the way, you have not just your trillion
[02:00:19.840 --> 02:00:23.520]   dollar cluster, but like, you know, hundreds of millions of GPUs on inference clusters everywhere.
[02:00:23.520 --> 02:00:27.200]   And this isn't result, like, I think private, in this world, I think private companies have
[02:00:27.200 --> 02:00:31.040]   the capital and can raise capital. I think you will need the government force to do it fast.
[02:00:31.040 --> 02:00:35.360]   Yeah, I was just about to ask, like, wouldn't it be the, like, we know copyright companies
[02:00:35.360 --> 02:00:41.040]   are on track to be able to do this and be China if they're unhindered by climate pledges or
[02:00:41.040 --> 02:00:45.520]   whatever. Well, that's part of what I'm saying. So if that's the case, and if it really matters
[02:00:45.520 --> 02:00:49.360]   that we be China, there's all going to be all kinds of practical difficulties of like,
[02:00:49.360 --> 02:00:55.600]   will the AI researchers actually join the AI effort? If they do, there's going to be three
[02:00:55.600 --> 02:01:02.160]   different teams, at least we're currently doing pre training on different, different companies.
[02:01:02.160 --> 02:01:06.880]   Yeah. Now who decides at some point, you're going to have the you're like YOLO, the hyperparameters
[02:01:06.880 --> 02:01:14.320]   of the trillion dollar cluster. Who decides that just like merging extremely complicated research
[02:01:14.320 --> 02:01:19.280]   and development processes across very different organizations. Yeah. This is somehow supposed to
[02:01:19.280 --> 02:01:23.680]   speed up America against the Chinese. Like, why don't we just let brain and deep mind merge? And
[02:01:23.680 --> 02:01:26.800]   it was like a little messy, but it was fine. It was pretty messy. And it was also the same company
[02:01:26.800 --> 02:01:30.800]   and also much earlier on in the process. Pretty similar, right? Same code, different code bases,
[02:01:30.800 --> 02:01:33.520]   and like lots of different infrastructure and different teams. And it was like, you know,
[02:01:33.520 --> 02:01:36.240]   it wasn't it wasn't like it wasn't the smoothest of all processes. But you know,
[02:01:36.240 --> 02:01:39.920]   deep mind is doing, I think, very well. I mean, look, you give the example of COVID
[02:01:39.920 --> 02:01:43.680]   and the COVID example is like, listen, we woke up to it, maybe it was late, but then we deployed all
[02:01:43.680 --> 02:01:49.600]   this money. And COVID response to government was a clusterfuck over and like, the only part of it
[02:01:49.600 --> 02:01:53.120]   that was worked as I agree, Warpstreet was like enabled by the government was literally just
[02:01:53.120 --> 02:01:57.680]   giving the permission that you can actually do we will also taking making like the big
[02:01:57.680 --> 02:02:01.360]   commitments or whatever. But I agree. But it was like fundamentally was like a private sector led
[02:02:01.360 --> 02:02:04.400]   effort. Yeah, that was the only part of COVID that worked. I mean, I think I think again,
[02:02:04.400 --> 02:02:07.760]   I think the project will look closer to Operation Warp Speed. And it's not even I mean, I think I
[02:02:07.760 --> 02:02:11.680]   think you'll have all the companies involved in the government project. I'm not that sold that
[02:02:11.680 --> 02:02:15.520]   merging is that difficult. You know, you have one, and you select one code base. And you know,
[02:02:15.520 --> 02:02:19.120]   you run free training on like GPUs with, you know, one code base. And then you do the sort
[02:02:19.120 --> 02:02:25.040]   of secondary I'll step on the you know, the other code base with TPUs. I think it's fine. I mean,
[02:02:25.040 --> 02:02:28.480]   to the topic of like, will people sign up for it? They wouldn't sign up for it today. I think this
[02:02:28.480 --> 02:02:32.480]   would be kind of crazy to people. But also, you know, I mean, this is part of the like secrets
[02:02:32.480 --> 02:02:36.400]   thing, you know, people gather at parties or whatever, you know, you know, this, you know,
[02:02:36.400 --> 02:02:40.400]   I don't think anyone has really gotten up in front of these people. And been like, look, you know,
[02:02:40.400 --> 02:02:45.680]   the thing you're building is the most important thing for like the national security of the United
[02:02:45.680 --> 02:02:49.120]   States for like, whether, you know, like, you know, the free world will have another century
[02:02:49.120 --> 02:02:53.200]   ahead of it. Like, this is the thing you're doing is really important, like for your country for
[02:02:53.200 --> 02:02:58.960]   democracy. And, you know, don't talk about the secrets. And it's not just about, you know,
[02:02:58.960 --> 02:03:02.320]   oh, DeepMind or whatever. It's about it's about, you know, these really important things.
[02:03:02.320 --> 02:03:06.000]   And so, you know, I don't know, like, again, we're talking about the Manhattan Project,
[02:03:06.000 --> 02:03:09.840]   right? This stuff was really contentious initially. But, you know, at some point,
[02:03:09.840 --> 02:03:13.920]   it was like clear that this stuff was coming. It was clear that there was like, sort of a real sort
[02:03:13.920 --> 02:03:19.120]   of like exigency on the military, national security front. And, you know, I think a lot
[02:03:19.120 --> 02:03:22.880]   of people come around on the like, whether it'll be competent. I agree. I mean, this is, again,
[02:03:22.880 --> 02:03:26.400]   where it's like a lot of the stuff is more like predictive in the sense, I think this is like,
[02:03:26.400 --> 02:03:29.200]   reasonably likely. And I think not enough people are thinking about it, you know, like a lot of
[02:03:29.200 --> 02:03:34.160]   people think about like AI lab politics or whatever. But like, nobody has a plan for the
[02:03:34.160 --> 02:03:37.440]   project. You know, it's like, you know, like, you're pessimistic about it. And like, we don't
[02:03:37.440 --> 02:03:42.080]   have a plan for it. We need to do it very soon. Because AGI is upon us. Yeah, then fuck the only
[02:03:42.080 --> 02:03:46.800]   capable, competent technical institutions capable of making AI right now are private companies.
[02:03:46.800 --> 02:03:50.800]   Let's go play that leading role. It'll be a sort of a partnership, basically. But the other thing
[02:03:50.800 --> 02:03:53.600]   is like, you know, again, we talked about World War Two and, you know, American unpreparedness,
[02:03:53.600 --> 02:03:57.120]   the beginning of World War Two is complete, you know, complete shambles, right? And so there's
[02:03:57.120 --> 02:04:01.440]   a sort of like, very company, I think America has a very deep bench of just like incredibly
[02:04:01.440 --> 02:04:06.880]   competent managerial talent. You know, I think, you know, there's a lot of really dedicated people.
[02:04:06.880 --> 02:04:11.360]   And, you know, I think basically a sort of Operation Warp Speed public private partnership,
[02:04:11.360 --> 02:04:15.360]   something like that, you know, is sort of what I imagine it would look like. Yeah, I mean,
[02:04:15.360 --> 02:04:20.000]   the recruiting the talent is an interesting question, because the same sort of thing where
[02:04:20.000 --> 02:04:25.680]   initially for the Manhattan Project, you had to convince people, we got to beat the Nazis,
[02:04:25.680 --> 02:04:31.120]   and you got to get on board. I think a lot of them maybe regretted how much they accelerated the bomb.
[02:04:31.120 --> 02:04:34.560]   And I want, I think this is generally a thing of war,
[02:04:35.680 --> 02:04:38.320]   where, I mean, I think they're also wrong to regret it. But
[02:04:38.320 --> 02:04:46.160]   yeah, why? What's the reason for regretting it? I think there's a world in which you don't have
[02:04:46.160 --> 02:04:52.560]   the way in which nuclear weapons were developed after the war was pretty explosive, because there
[02:04:52.560 --> 02:04:57.120]   was a precedent that you actually can use nuclear weapons, then because of the race that was set up,
[02:04:57.120 --> 02:05:02.880]   you immediately go to the H bomb. And my view is, again, this is this is related to the view on AI.
[02:05:02.880 --> 02:05:06.960]   And maybe some of our disagreement is like, that was inevitable. Like, of course, like, you know,
[02:05:06.960 --> 02:05:10.880]   there's this, you know, World War, and then obviously, there was the, you know, Cold War
[02:05:10.880 --> 02:05:15.200]   right after, of course, like, you know, the military and technology angle of this would be
[02:05:15.200 --> 02:05:19.280]   like, you know, pursued with ferocious intensity. And I don't really think there's a world in which
[02:05:19.280 --> 02:05:22.880]   that doesn't happen, which like, ah, we're all not going to build nukes. And also just like,
[02:05:22.880 --> 02:05:26.400]   nukes went really well, I think that could have gone terribly right. You know, like, you know,
[02:05:26.400 --> 02:05:30.160]   again, I mean, this sort of, I think this is like, not physically possible with nukes,
[02:05:30.160 --> 02:05:33.120]   this sort of pocket nukes for everybody. But I think sort of like WMDs that are sort of
[02:05:33.120 --> 02:05:38.160]   proliferated, democratized, and like all the countries have it like the US leading on nukes,
[02:05:38.160 --> 02:05:41.760]   and then sort of like building this new world order that was kind of US led, or at least sort
[02:05:41.760 --> 02:05:46.400]   of like a few great powers, and a non proliferation regime for nukes, a partnership and a deal that's
[02:05:46.400 --> 02:05:50.640]   like, look, no military sort of application of nuclear technology, but we're going to help you
[02:05:50.640 --> 02:05:54.800]   with the civilian technology, we're gonna enforce safety norms on the rest of the world that worked,
[02:05:54.800 --> 02:06:02.480]   it worked, and it could have gone so much worse. I mean, this is, I mean, I say this a bit in the
[02:06:02.480 --> 02:06:05.680]   piece, but it's like, actually, the a bomb, you know, like the a bomb in Hiroshima, it was just
[02:06:05.680 --> 02:06:10.000]   like, you know, the firebombing, the thing, I think the thing that really changed the game was
[02:06:10.000 --> 02:06:15.520]   like the super, you know, the H bomb and ICBMs. And then I think that's really when it took it
[02:06:15.520 --> 02:06:21.280]   to like a whole new level. I think part of me thinks when you say we will tell the people that
[02:06:21.280 --> 02:06:27.600]   for the free world to survive, we need to pursue this project. It sounds similar to World War Two
[02:06:27.600 --> 02:06:32.640]   is so World War Two is a sad story, obviously, the fact that it happened, but also like the
[02:06:32.640 --> 02:06:40.320]   victory is sad in the sense that why Britain goes in to protect Poland. Yeah. And at the end,
[02:06:40.320 --> 02:06:49.120]   the USSR, which is, you know, as your family knows, is incredibly brutal, ends up occupying
[02:06:49.120 --> 02:06:55.840]   half of Europe. And the like part of like, we're protecting the free world. That's why
[02:06:55.840 --> 02:07:00.400]   we got to rush the AI. And like, if we end up with the American AI Leviathan, I think there's
[02:07:00.400 --> 02:07:06.800]   a world where we look back on this, where it's has the same sort of twisted irony that Britain
[02:07:06.800 --> 02:07:09.600]   going into a World War Two had about trying to protect Poland.
[02:07:09.600 --> 02:07:14.400]   Look, I mean, I think there's going to be a lot of unfortunate things that happen. I'm just like,
[02:07:14.400 --> 02:07:18.000]   I'm just hoping we make it through. I mean, to the to the point of it's like, I really don't
[02:07:18.000 --> 02:07:21.200]   think the pitch will only be this sort of like, you know, the race. I think the race will be sort
[02:07:21.200 --> 02:07:25.840]   of a backdrop to it. I think the sort of general like, look, it's important that democracy shape
[02:07:25.840 --> 02:07:29.920]   this technology. We can't just like leak this stuff to, you know, North Korea is going to be
[02:07:29.920 --> 02:07:34.160]   important. I think also for the just safety, including alignment, including the sort of like
[02:07:34.160 --> 02:07:39.200]   creation of new WMDs. I'm not currently sold. There's another path, right? So it's like,
[02:07:39.200 --> 02:07:42.640]   if you just have the breakneck race, both internationally, because you're just instantly
[02:07:42.640 --> 02:07:46.640]   leaking all this stuff, including the weights and just, you know, the commercial race, you know,
[02:07:46.640 --> 02:07:49.920]   Demis and Dario and Sam, you know, just kind of like they all want to be first.
[02:07:49.920 --> 02:07:54.800]   And it's incredibly rough for safety. And then you say, OK, safety regulation.
[02:07:54.800 --> 02:07:57.680]   But, you know, it's sort of like, you know, the safety regulation that people talk about,
[02:07:57.680 --> 02:08:01.520]   it's like, oh, well, NIST, and they take years and they figure out what the expert consensus is.
[02:08:01.520 --> 02:08:06.160]   And then they write the project as well. But I think I mean, I think the sort of
[02:08:06.160 --> 02:08:08.640]   alignment angle during the intelligence explosion, it's going to you know, it's
[02:08:08.640 --> 02:08:12.080]   not a process of like years of bureaucracy. And then you can kind of write some standards.
[02:08:12.080 --> 02:08:16.320]   I think it looks much more like basically a war. And like you have a fog of war. It's
[02:08:16.320 --> 02:08:20.080]   like, look, it's like, is it safe to do the next OOM? You know, and it's like, ah, you know, like,
[02:08:20.080 --> 02:08:22.560]   you know, we're like three rooms into the intelligence explosion. We don't really
[02:08:22.560 --> 02:08:27.840]   understand what's going on anymore. You know, the, you know, like a bunch of
[02:08:27.840 --> 02:08:31.200]   our like generalization scaling curves are like kind of looking not great. You know,
[02:08:31.200 --> 02:08:34.320]   some of our like automated researchers that are doing alignment are saying it's fine,
[02:08:34.320 --> 02:08:38.080]   but we don't quite trust them in this test. You know, the like the eyes started doing
[02:08:38.080 --> 02:08:41.680]   naughty things and but then we like hammered it out and then it was fine.
[02:08:41.680 --> 02:08:45.280]   And like, ah, should we should we go ahead? Should we take, you know, another six months?
[02:08:45.280 --> 02:08:48.160]   Also, by the way, you know, like China just stole the weights or we, you know,
[02:08:48.160 --> 02:08:51.200]   they're about to like deploy the Romo army. Like, what do we do? I think it's this.
[02:08:51.200 --> 02:08:58.240]   I think it is this crazy situation. And, you know, basically, you you're relying
[02:08:58.240 --> 02:09:02.240]   much more on kind of like a sane chain of command than you are on sort of some like,
[02:09:02.240 --> 02:09:05.440]   you know, deliberative regulatory scheme. I wish you had you were able to do the
[02:09:05.440 --> 02:09:08.960]   deliberative regulatory scheme. And this is the thing about the private companies, too. I don't
[02:09:08.960 --> 02:09:15.920]   think, you know, they'll claim they're going to do safety, but I think it's really rough when you're
[02:09:15.920 --> 02:09:20.320]   in the commercial race and their startups, you know, and startups, startups or startups,
[02:09:20.320 --> 02:09:25.360]   you know, I think they're not fit to handle WMDs. Yeah, I'm coming closer to your position.
[02:09:25.360 --> 02:09:33.200]   But part of me also. So with the responsible scaling policies, I was told that people who
[02:09:33.200 --> 02:09:37.360]   are advancing that that the way to think about this, because they know I'm like a libertarian
[02:09:37.360 --> 02:09:44.800]   type of person. And the way they approached me about it was that fundamentally, this is a way
[02:09:44.800 --> 02:09:51.120]   to protect markets based development of AGI in the sense that if you didn't have this at all,
[02:09:51.120 --> 02:09:55.840]   then you would have this sort of misuse and it would have to be nationalized. And the RSPs are
[02:09:55.840 --> 02:10:01.680]   a way to make sure that through this deployment, you can still have a market based order. But
[02:10:01.680 --> 02:10:07.280]   then there's these safeguards that make sure that things don't go off the rails. And I wonder if
[02:10:07.600 --> 02:10:15.280]   the if it seems like your story seems self-consistent, but it does feel I know this
[02:10:15.280 --> 02:10:20.240]   was never your position. So I'm not like I'm not looping you into this, but they're a sort of
[02:10:20.240 --> 02:10:26.160]   Martin Bailey almost in the sense of, well, look, here's what I think about RSP type stuff or sort
[02:10:26.160 --> 02:10:30.080]   of safety regulation that's happening now. I think they're important for helping us figure out what
[02:10:30.080 --> 02:10:35.280]   world we're in and like flashing the warning signs when we're close. Right. And so the story we've
[02:10:35.280 --> 02:10:38.800]   been telling is sort of like, you know, sort of what I think the modal version of this decade is.
[02:10:38.800 --> 02:10:41.520]   But it's like, I think there's lots of ways it could be wrong. I really, you know, we should
[02:10:41.520 --> 02:10:44.400]   talk about the data while more. I think there's like, again, I think there's a world where the
[02:10:44.400 --> 02:10:49.920]   stuff stagnates. Right. There's a world where we don't have AGI. And so I basically know the RSP
[02:10:49.920 --> 02:10:53.760]   thing is like preserving the optionality. Let's see how this stuff goes. But like we need to be
[02:10:53.760 --> 02:10:57.520]   prepared. Like if if the red lights start flashing, if we're getting the automated eye researcher,
[02:10:57.520 --> 02:11:01.920]   then it's like and it's crunch time and then it's time to go. I think, OK, I can be on the same
[02:11:01.920 --> 02:11:07.360]   page on that, that we should have a very, very strong prior on a proceeding in a market based
[02:11:07.360 --> 02:11:13.200]   way, unless you're right about what the explosion looks like, the intelligence explosion. And so
[02:11:13.200 --> 02:11:19.680]   don't move yet. But in that world where it really does seem like Alec Radford can be automated and
[02:11:19.680 --> 02:11:25.120]   that is the only bottleneck to getting TSI. OK, I think we can leave it at that.
[02:11:26.240 --> 02:11:34.480]   I can. I can. Yeah, I am somewhat of the way there. OK. I hope it goes well. It's going to
[02:11:34.480 --> 02:11:41.600]   be very stressful. And again, right now is the chill time during your vacation while it lasts.
[02:11:41.600 --> 02:11:47.680]   It's funny to look out over. I'm just like, this is San Francisco. Yeah. And opening eyes right
[02:11:47.680 --> 02:11:51.280]   there. You know, AnthropX there. I mean, again, this is kind of like, you know, it's like you
[02:11:51.280 --> 02:11:54.960]   guys have this enormous power over how it's how it's going to go for the next couple of years.
[02:11:54.960 --> 02:12:00.240]   And that power is depreciating. Yeah. Who is you guys? Like, you know, people at labs. Yeah.
[02:12:00.240 --> 02:12:04.560]   But it is a sort of crazy world. And you're talking about like, you know, I feel like you
[02:12:04.560 --> 02:12:08.400]   talk about like, oh, maybe they'll nationalize this soon. It's like, you know, almost nobody
[02:12:08.400 --> 02:12:12.880]   like really like feels it, sees what's happening. And it's I think this is the thing that I find
[02:12:12.880 --> 02:12:16.800]   stressful about all this stuff is like, look, maybe I'm wrong. Like, if I'm right, we're in
[02:12:16.800 --> 02:12:20.720]   this crazy situation where there's like, you know, a few hundred guys paying attention.
[02:12:22.800 --> 02:12:28.160]   And it's it's daunting. I went to Washington a few months ago. Yeah. And I was talking to
[02:12:28.160 --> 02:12:32.320]   some people who are doing AI policy stuff there. Yeah. And I was asking them how likely they think
[02:12:32.320 --> 02:12:37.920]   nationalization is. Yeah. And they said, oh, you know, like, it's really hard to nationalize stuff.
[02:12:37.920 --> 02:12:41.760]   It's been a long time since you've done it. There's these very specific procedural constraints
[02:12:41.760 --> 02:12:48.320]   on what kinds of things can be nationalized. And then I was asked, well, like ASI. So that
[02:12:48.320 --> 02:12:52.560]   means because there's there's constraints that a defense production act or whatever that that won't
[02:12:52.560 --> 02:12:57.280]   be nationalized. There's the Supreme Court overturned that. And they're like, yeah, I guess
[02:12:57.280 --> 02:13:04.000]   that would be nationalized. That's the short summary of my post or my view on the project.
[02:13:04.000 --> 02:13:16.960]   OK, so before we go further on the stuff, let's just back up. OK, you we begin the conversation.
[02:13:16.960 --> 02:13:20.640]   I think people will be confused. You graduate valedictorian of Colombia when you were 19.
[02:13:20.640 --> 02:13:24.960]   So you got to college when you were 15. Right. And you were you were in Germany. You got to
[02:13:24.960 --> 02:13:30.960]   college at 15. Yeah. How the fuck did that happen? I really wanted out of Germany.
[02:13:30.960 --> 02:13:37.840]   I you know, I went to kind of a German public school. It was it was not a good environment
[02:13:37.840 --> 02:13:44.160]   for me. And, you know, in what sense is just like no peers that are. Yeah. Look, I mean,
[02:13:44.160 --> 02:13:48.240]   it wasn't. Yeah, it was, you know, there's I mean, there's also just a sense in which
[02:13:48.240 --> 02:13:51.760]   sort of like there's this particular sort of German cultural sense. I think in the U.S.,
[02:13:51.760 --> 02:13:54.560]   you know, there's all these like amazing high schools and like sort of an appreciation of
[02:13:54.560 --> 02:13:58.960]   excellence. And in Germany, there's really this sort of like Paul Poppy syndrome of right where
[02:13:58.960 --> 02:14:02.720]   it's, you know, you're the curious kid in class and you want to learn more instead of the teacher
[02:14:02.720 --> 02:14:06.080]   being like, ah, that's great. They're like they kind of resent you for it and they're like trying
[02:14:06.080 --> 02:14:10.000]   to crush you. And I mean, there's also like there's no kind of like elite universities
[02:14:10.000 --> 02:14:15.440]   for undergraduate, which is kind of crazy. So, you know, the sort of, you know, there's sort of
[02:14:15.440 --> 02:14:21.760]   like basically like the meritocracy was kind of crushed in Germany at some point. Also, I mean,
[02:14:21.760 --> 02:14:28.560]   there's a sort of incredible sense of complacency, you know, across the board. I mean, one of the
[02:14:28.560 --> 02:14:33.040]   things that always puzzles me is like, you know, even just going to a U.S. college was this kind
[02:14:33.040 --> 02:14:36.720]   of like radical act. And like, you know, it doesn't seem radical to anyone here because it's
[02:14:36.720 --> 02:14:40.000]   like, ah, this is obviously the thing you do. And you can go to Columbia, you go to Columbia. But
[02:14:40.000 --> 02:14:45.200]   it's, you know, it is very unusual. And it's wild to me because it's like, you know, this is where
[02:14:45.200 --> 02:14:48.560]   stuff is happening. You can get so much of a better education. And, you know, like America is
[02:14:48.560 --> 02:14:57.680]   where, you know, it's where all the stuff is and people don't do it. And so. Yeah, anyway, so I,
[02:14:57.680 --> 02:15:02.480]   you know, I know I skipped a few grades and, you know, I think at the time it seemed very normal
[02:15:02.480 --> 02:15:08.160]   to me to kind of like go to college and come to America. I think, you know, now one of my sisters
[02:15:08.160 --> 02:15:12.080]   is now like turning 15, you know. And so then I, you know, and I look at her and I'm like,
[02:15:12.080 --> 02:15:17.760]   now I understand how my mother reacted to this plan. And as you get to college, you're like
[02:15:17.760 --> 02:15:22.000]   presumably the only 15 year old. Yeah. Yeah. I was just like normal for you to be a 15 year old.
[02:15:22.000 --> 02:15:25.360]   Like what was the initial years like? It felt so normal at the time. You know, I didn't. Yeah. So
[02:15:25.360 --> 02:15:29.120]   again, it's like now I understand why mothers worry. And, you know, I think, you know, I worked
[02:15:29.120 --> 02:15:33.360]   on my parents for a while. You know, eventually I persuaded them. No, but yeah, it felt felt very
[02:15:33.360 --> 02:15:36.560]   normal at the time. And it was great. It's also great because, you know, I actually really like
[02:15:36.560 --> 02:15:41.360]   college. Right. And in some sense, it sort of came at the right time for me where, you know, I,
[02:15:41.360 --> 02:15:45.840]   I mean, I, you know, for example, I really appreciated the sort of like liberal arts
[02:15:45.840 --> 02:15:49.120]   education and, you know, like the core curriculum and reading sort of core works of political
[02:15:49.120 --> 02:15:55.360]   philosophy and literature. And you did what econ and I mean, my majors were math and statistics
[02:15:55.360 --> 02:16:00.320]   and economics. But, you know, Columbia is a sort of pretty heavy core curriculum and liberal arts
[02:16:00.320 --> 02:16:03.440]   education. And honestly, like, you know, I shouldn't have done all the majors. I should
[02:16:03.440 --> 02:16:06.720]   just I mean, the best courses were sort of the courses where it's like there's some amazing
[02:16:06.720 --> 02:16:11.680]   professor and it's some history class. And I mean, that's that's honestly the thing I would
[02:16:11.680 --> 02:16:16.240]   recommend people spend their time on in college. Was there one professor or class that stood out
[02:16:16.240 --> 02:16:23.040]   that way? I'm a few. There's like a class by Richard Betts on war, peace and strategy.
[02:16:23.920 --> 02:16:29.600]   Adam, too, is obviously fantastic and, you know, has written very riveting books. Yeah,
[02:16:29.600 --> 02:16:33.920]   yeah. You should have on the podcast, by the way. I tried. I tried. I think you tried for
[02:16:33.920 --> 02:16:42.000]   me. You got to get on the pod. I'd be so good. OK, so then in a couple of years. Yeah. We're
[02:16:42.000 --> 02:16:48.720]   talking to Tyler Cowen recently. And he said that when the way we for he first encountered you.
[02:16:48.720 --> 02:16:53.600]   Yeah. Was you wrote this paper on economic growth and existential risk. And he said,
[02:16:53.600 --> 02:16:57.760]   I when I found read it, I couldn't believe that a 17 year old had written it.
[02:16:57.760 --> 02:17:02.240]   I thought if this was an MIT dissertation, I'd be impressed. So you were like, well,
[02:17:02.240 --> 02:17:08.960]   how did you go from you? I guess we've been junior then you're writing. You're writing,
[02:17:08.960 --> 02:17:15.680]   you know, pretty novel economic papers. Why did you get interested in this this kind of thing?
[02:17:15.680 --> 02:17:20.000]   And what was the process to get in that? I don't know, I just, you know, I get
[02:17:20.000 --> 02:17:23.760]   interested in things in some sense. It's sort of like it feels very natural to me. It's like I get
[02:17:23.760 --> 02:17:27.440]   excited about a thing. I read about it. I immerse myself. I think I can you know, I can learn
[02:17:27.440 --> 02:17:32.880]   information very quickly and understand it. The I mean, I think to the paper, I mean, I think one
[02:17:32.880 --> 02:17:37.840]   actual at least for the way I work, I feel like sort of moments of peak productivity matter much
[02:17:37.840 --> 02:17:41.040]   more than sort of average productivity. I think there's some jobs, you know, like CEO or something,
[02:17:41.040 --> 02:17:45.840]   you know, like average productivity really matters. But I think there's sort of I often
[02:17:45.840 --> 02:17:48.800]   feel like I have periods of like, you know, there's some there's a couple of months where
[02:17:48.800 --> 02:17:52.320]   there's sort of nephrolescence and I'm like, you know, and the other times I'm sort of computing
[02:17:52.320 --> 02:17:56.000]   stuff in the background. And at some point, you know, like writing the series, this is also kind
[02:17:56.000 --> 02:18:01.920]   of similar. It's just like you write it and it's like it's really flowing. And that's sort of what
[02:18:01.920 --> 02:18:05.760]   ends up mattering. I think even for CEOs, it might be the case that the peak productivity is very
[02:18:05.760 --> 02:18:11.520]   important. There's one of our following Chatham House rules. One of our friends in a group chat
[02:18:11.520 --> 02:18:18.720]   has pointed out how many famous CEOs and founders have been bipolar manic, which is very much
[02:18:18.800 --> 02:18:23.200]   the peak. Like the call option on your productivity is the most important thing. You get it by just
[02:18:23.200 --> 02:18:29.840]   increasing the volatility through bipolar. Okay, so that's interesting. And so you get interested
[02:18:29.840 --> 02:18:33.520]   in economics first. First of all, why economics like you could read about anything at this move,
[02:18:33.520 --> 02:18:37.040]   like you if you wanted, you know, you could you kind of got a slow start on them. All right.
[02:18:37.040 --> 02:18:43.120]   You wasted all these years on econ. There's an alternate world where you're like on the super
[02:18:43.120 --> 02:18:53.120]   alignment team at 17 instead of 21 or whatever it was. I mean, in some sense, I'm still doing
[02:18:53.120 --> 02:18:57.840]   economics, right? You know, what is what is straight lines on a graph and like figuring
[02:18:57.840 --> 02:19:01.760]   out what the trends are and like thinking about the feedback loops and equilibrium arms control
[02:19:01.760 --> 02:19:06.560]   dynamics. And, you know, it's I think it is a sort of a way of thinking that I find very useful.
[02:19:07.680 --> 02:19:12.640]   And, you know, like what, you know, Dario and Ilya seeing scaling early, in some sense,
[02:19:12.640 --> 02:19:16.160]   that is a sort of very economic way. And also the sort of physics, like empirical physics,
[02:19:16.160 --> 02:19:18.960]   you know, a lot of them are physicists. I think the economists usually can't code well enough,
[02:19:18.960 --> 02:19:23.760]   and that's their issue. But I think it's that sort of way of thinking. I mean, the other thing is,
[02:19:23.760 --> 02:19:28.640]   you know, I thought there were sort of, you know, I thought of a lot of the sort of like
[02:19:28.640 --> 02:19:33.520]   core ideas of economics that were just beautiful. And, you know, in some sense,
[02:19:33.520 --> 02:19:36.320]   I feel like I was a little duped, you know, where it's like actually kind of academia is kind of
[02:19:36.320 --> 02:19:40.160]   decadent now. You know, I think that, you know, for example, the paper I wrote, you know, it's
[02:19:40.160 --> 02:19:44.480]   sort of I think the takeaway, you know, it's a long paper, it's 100 pages of math or whatever.
[02:19:44.480 --> 02:19:48.720]   I think the core takeaway I can, you know, kind of give the core intuition for in like, you know,
[02:19:48.720 --> 02:19:52.640]   30 seconds, and it makes sense. And it's and it's like, you don't actually need the math. I think
[02:19:52.640 --> 02:19:56.720]   that's the sort of the best pieces of economics are like that, where you do the work, but you do
[02:19:56.720 --> 02:20:02.240]   the work to kind of uncover insights that weren't obvious to you before. Once you've done the work,
[02:20:02.240 --> 02:20:06.480]   it's like some sort of like mechanism falls out of it that like makes a lot of crisp, intuitive
[02:20:06.480 --> 02:20:10.400]   sense that like explains some facts about the world that you can then use in arguments. I think,
[02:20:10.400 --> 02:20:14.320]   you know, I think, you know, like a lot of econ 101 like this, and it's great. A lot of econ in
[02:20:14.320 --> 02:20:20.320]   the, you know, in the 50s and the 60s, you know, was like this. And, you know, Chad Jones papers
[02:20:20.320 --> 02:20:24.800]   are often like this. I really like Chad Jones papers for this. You know, I think, you know,
[02:20:24.800 --> 02:20:30.640]   why did I ultimately not pursue econ academia was a number of reasons. One of them was Tyler Cowen.
[02:20:31.120 --> 02:20:35.920]   Um, um, um, you know, he kind of took me aside and he was kind of like, look, I think you're
[02:20:35.920 --> 02:20:39.360]   one of the top young economists I've ever met, but also you should probably not go to grad school.
[02:20:39.360 --> 02:20:42.400]   Oh, interesting. Really? I didn't realize that. Well, yeah. And it was, it was, it was good
[02:20:42.400 --> 02:20:46.160]   because, um, he kind of introduced me to the, you know, I don't know, like the Twitter weirdos
[02:20:46.160 --> 02:20:50.960]   or just like, you know, and I think the takeaway from that was kind of, um, you know, got to move
[02:20:50.960 --> 02:20:54.720]   out west one more time. Wait, Tyler, did you see the Twitter weirdos? A little bit. Yeah. Or just
[02:20:54.720 --> 02:20:58.880]   kind of like the sort of brought you like the 60 year old, the old economist to introduce you to
[02:20:58.880 --> 02:21:03.520]   that Twitter. Well, you know, I, I had been, I, so I went from Germany, you know, completely,
[02:21:03.520 --> 02:21:07.280]   you know, on the periphery to kind of like, you know, in the U S elite institution and sort of
[02:21:07.280 --> 02:21:12.720]   got, got some vibe of like, sort of, you know, meritocratic elite, you know, U S society. And
[02:21:12.720 --> 02:21:16.480]   then sort of, yeah, basically this sort of like, there was a sort of directory then to being like,
[02:21:16.480 --> 02:21:20.400]   look, I, you know, find the true American spirit. I got to come out here, but the other reason I
[02:21:20.400 --> 02:21:24.160]   didn't become economist was because, or at least econ academia was so they think sort of econ
[02:21:24.160 --> 02:21:28.240]   academia has become a bit decadent and maybe it's just ideas getting harder to find. And maybe it's
[02:21:28.240 --> 02:21:31.680]   sort of things, you know, and the sort of beautiful, simple things have been discovered, but
[02:21:31.680 --> 02:21:35.920]   you know, like what are econ papers these days? You know, it's like, you know, it's like, uh, uh,
[02:21:35.920 --> 02:21:39.680]   200 pages of like empirical analyses on what happened when, you know, like Wisconsin bought,
[02:21:39.680 --> 02:21:43.360]   you know, a hundred thousand more textbooks on like educational outcomes. And I'm really happy
[02:21:43.360 --> 02:21:46.640]   that work happened. I think it's important work, but I think it is not in government and covering
[02:21:46.640 --> 02:21:52.160]   these sort of like fundamental insights and sort of mechanisms in society. Um, or, you know, it's
[02:21:52.160 --> 02:21:56.160]   like, even the theory work is kind of like, here's a really complicated model and the model spits
[02:21:56.160 --> 02:22:00.160]   out, you know, if the fed does X, you know, then Y happens and you have no idea what that hat, why
[02:22:00.160 --> 02:22:03.920]   that happened. Cause it's like gazillion parameters and they're all calibrated in some way. And it's
[02:22:03.920 --> 02:22:07.840]   some computer simulation and you have no idea about the validity, you know? Yeah. So I think,
[02:22:07.840 --> 02:22:11.600]   I think the sort of, you know, the most important insights are the ones where you have to do a lot
[02:22:11.600 --> 02:22:16.480]   of work to get them, but then there's sort of this crisp intuition. Yeah. The P versus NP of, uh,
[02:22:16.480 --> 02:22:22.800]   sure. Yeah. Yeah. Yeah. Um, that's really interesting. So just going back to your time
[02:22:22.800 --> 02:22:28.560]   in college, you say that peak productivity kind of explains the paper and things, but the
[02:22:28.560 --> 02:22:34.480]   valedictorian that's getting straight A's or whatever is very much, um, uh, uh, average
[02:22:34.480 --> 02:22:38.960]   productivity phenomenon, right? So there's one award for the highest GPA, which I won,
[02:22:38.960 --> 02:22:42.880]   but the valedictorian is like among the people which have the highest GPA and then like selected
[02:22:42.880 --> 02:22:47.360]   by faculty or something. So it's just not, it's not just peak productivity. It's just, it's, it's,
[02:22:47.360 --> 02:22:51.600]   it's just, it's, I generally just love this stuff. You know, I just, I was curious and I thought it
[02:22:51.600 --> 02:22:55.840]   was really interesting and I love learning about it and, and I love kind of like it made sense to
[02:22:55.840 --> 02:22:59.760]   me and you know, it was, it was very natural. And so, you know, I think I'm, you know, I'm not,
[02:22:59.760 --> 02:23:03.520]   you know, I think one of my faults is I'm not that good at eating glass or whatever. I think
[02:23:03.520 --> 02:23:06.960]   there's some people who are very good at it. I think the sort of like the sort of moments of
[02:23:06.960 --> 02:23:11.840]   pre productivity come when I, you know, I'm just really excited and engaged and, and, and, and,
[02:23:11.840 --> 02:23:17.440]   and love it. And you know, I, I, you know, if you take the right courses, you know, that's what you
[02:23:17.440 --> 02:23:24.480]   got in college. It's the Bruce Banner code and Avengers, you know, I'm always angry. I'm always
[02:23:24.480 --> 02:23:30.240]   excited. I'm always curious. That's why I'm always speaking productivity. So it's interesting by the
[02:23:30.240 --> 02:23:34.880]   way, when you were in college, I was also in college. I think you were, despite being a year
[02:23:34.880 --> 02:23:39.840]   younger than me, I think you're ahead in college than me, or at least two years, maybe two years
[02:23:39.840 --> 02:23:50.080]   ahead. And we met around this time. We also met, I think through the Tyler Cowen universe. And it's
[02:23:50.080 --> 02:23:56.640]   very insane how small the world is. I think I, did I reach out to you? I must have about when
[02:23:56.640 --> 02:24:00.400]   I had a couple of videos and they had a couple hundred views or something.
[02:24:00.400 --> 02:24:04.240]   Yeah. It's a small world. Yeah. I mean, this is the crazy thing about the eye world, right? It's
[02:24:04.240 --> 02:24:09.040]   kind of like, it's the same few people at the kinds of parties and they're the ones, you know,
[02:24:09.040 --> 02:24:14.960]   running the models at DeepMind and, you know, open AI and Anthropic. And, you know, I mean,
[02:24:14.960 --> 02:24:18.160]   I think some other friends of ours have mentioned this. We're now later in their career and very
[02:24:18.160 --> 02:24:22.080]   successful that, you know, they actually met all the people who are also kind of very successful
[02:24:22.080 --> 02:24:24.720]   in Silicon Valley now, like, you know, when they're, when they're in their, you know,
[02:24:24.720 --> 02:24:30.720]   when, before the twenties or really twenties. I mean, look, I actually think, you know,
[02:24:30.720 --> 02:24:35.600]   and why is it a small world? I mean, I think one of the things is some amount of like, you know,
[02:24:35.600 --> 02:24:42.160]   some sort of agency. And I think in a funny way, this is a thing I sort of took away from the sort
[02:24:42.160 --> 02:24:47.760]   of Germany experience where it was, I mean, look, I, I, it was crushing. I really didn't like it.
[02:24:47.760 --> 02:24:52.080]   And it was like, it was such an unusual move to kind of skip grades and such an unusual move to
[02:24:52.080 --> 02:24:55.840]   come to the United States. And, you know, a lot of these things I did were kind of unusual moves.
[02:24:55.840 --> 02:25:03.600]   And, you know, there's some amount where like, just like just trying to do it and then it was
[02:25:03.600 --> 02:25:08.080]   fine and it worked. That kind of reinforced, like, you know, you don't, you don't just have
[02:25:08.080 --> 02:25:11.520]   to kind of conform to what the Overton window is. You can just kind of like try to do the thing,
[02:25:11.520 --> 02:25:16.240]   the thing that seems right to you. And like, you know, most people can be wrong and I don't know,
[02:25:16.240 --> 02:25:20.240]   things like that. And I think that was kind of a, you know, valuable kind of like early experience
[02:25:20.240 --> 02:25:25.280]   that was sort of formative. Okay. So after college, what did you do? I did econ research
[02:25:25.280 --> 02:25:30.880]   for a little bit, you know, Oxford and stuff. And then, then I worked at Future Fund. Yeah.
[02:25:30.880 --> 02:25:38.560]   Okay. So, and so tell me about it. Future Fund was that, you know, it was a foundation that was,
[02:25:38.560 --> 02:25:42.480]   you know, funded by Sam Bankman Freed. I mean, we were our own thing, you know, we were based in the
[02:25:42.480 --> 02:25:49.920]   Bay. You know, at the time this was in sort of early 22. It was, it was this just like incredibly
[02:25:49.920 --> 02:25:53.920]   exciting opportunity, right? It was basically like a startup, you know, foundation, which is
[02:25:53.920 --> 02:25:56.960]   like, you know, it doesn't come along that, that often that, you know, we thought would be able to
[02:25:56.960 --> 02:26:00.400]   give away billions of dollars, you know, thought would be able to kind of like, you know, remake
[02:26:00.400 --> 02:26:04.880]   how philanthropy is done, you know, from first principles thought would be able to have, you
[02:26:04.880 --> 02:26:10.240]   know, this like great impact, you know, we, the causes we focused on were, you know, biosecurity,
[02:26:10.240 --> 02:26:15.840]   you know, AI, you know, finding exceptional talent and putting them to work on hard problems.
[02:26:15.840 --> 02:26:20.000]   And, you know, like a lot of the stuff we did, I was, I was really excited about,
[02:26:20.000 --> 02:26:23.440]   you know, like academics who, you know, usually take six months would send us emails like, ah,
[02:26:23.440 --> 02:26:27.360]   you know, this is great. This is so quick and, you know, and straightforward, you know, in general,
[02:26:27.360 --> 02:26:30.640]   I feel like I've often find that with like, you know, a little bit of encouragement, a little bit
[02:26:30.640 --> 02:26:34.240]   of sort of empowerment, kind of like removing excuses, making the process easy, you know,
[02:26:34.240 --> 02:26:39.760]   you can kind of like, get people to do great things. I think on the future front that I think
[02:26:39.760 --> 02:26:46.400]   is context for people who might not realize, not only were you guys planning on deploying billions
[02:26:46.400 --> 02:26:52.800]   of dollars, but it was a team of four people. Yeah, yeah, yeah. So you at 18 are on a team of
[02:26:52.800 --> 02:26:56.800]   four people. Yeah. That is in charge of deploying billions of dollars. Yeah. I mean, just, I mean,
[02:26:56.800 --> 02:27:01.840]   yeah, future fund, you know, the yeah, I mean, so that was that was sort of the heyday. Right.
[02:27:01.840 --> 02:27:05.680]   And then obviously, you know, when, when in sort of, you know, November of 22,
[02:27:05.680 --> 02:27:11.600]   you know, it was kind of revealed that Sam was this, you know, giant fraud. And from one day
[02:27:11.600 --> 02:27:16.960]   to the next, you know, the whole thing collapsed. I was just really tough. I mean, you know,
[02:27:16.960 --> 02:27:21.760]   obviously it was devastating. It was devastating, obviously, for the people at their money and FTX,
[02:27:21.760 --> 02:27:26.160]   you know, closer to home, you know, all that, you know, all these grantees, you know,
[02:27:26.160 --> 02:27:29.520]   we wanted to help them and we thought they were doing amazing projects. And so but instead of
[02:27:29.520 --> 02:27:34.640]   helping them, we ended up saddling them with like a giant problem. You know, personally, it was,
[02:27:34.640 --> 02:27:38.080]   you know, it was a startup. Right. And so I, you know, I'd worked 70 hour weeks every week for,
[02:27:38.080 --> 02:27:40.960]   you know, basically a year on this to kind of build this up. You know, we're a tiny team.
[02:27:40.960 --> 02:27:46.400]   And then from one day to the next, it was all gone and not just gone. It was associated with
[02:27:46.400 --> 02:27:53.360]   this giant fraud. And so, you know, that was incredibly tough. Yeah. And then were there
[02:27:53.360 --> 02:28:00.240]   any signs early on that SBF was. Yeah, I mean, obviously I didn't know he was a fraud and the
[02:28:00.240 --> 02:28:05.440]   whole, you know, I would have never worked there. You know, and, you know, we weren't,
[02:28:05.440 --> 02:28:10.080]   you know, we were a separate thing. We weren't working with the business. I mean, I think I do
[02:28:10.080 --> 02:28:14.400]   think there are some takeaways for me. I think one takeaway was, you know, I think there's a
[02:28:15.520 --> 02:28:18.720]   I had this tendency. I think people in general have this tendency to kind of like, you know,
[02:28:18.720 --> 02:28:22.720]   give successful CEOs a pass on their behavior because, you know, they're successful CEOs and
[02:28:22.720 --> 02:28:28.400]   that's how they are. And that's just successful CEO things. And, you know, I didn't know Sam
[02:28:28.400 --> 02:28:33.680]   Bankman Fried was a fraud, but I knew SBF and I knew he was extremely risk taking. Right. I knew
[02:28:33.680 --> 02:28:40.240]   he he was narcissistic. He didn't tolerate disagreement. Well, you know, sort of by the
[02:28:40.240 --> 02:28:44.240]   end, he and I just like didn't get along well. And sort of I think the reason for that was like
[02:28:44.240 --> 02:28:48.560]   there's some biosecurity grants you really like because they're kind of cool and flashy. And at
[02:28:48.560 --> 02:28:51.920]   some point I'd kind of run the numbers and it didn't really seem that cost effective. And I
[02:28:51.920 --> 02:28:56.640]   pointed that out and he was pretty unhappy about that. And so I knew his character.
[02:28:56.640 --> 02:29:05.520]   And I think, you know, I feel like one takeaway for me was was, you know, like I think it's really
[02:29:05.520 --> 02:29:09.120]   worth paying attention to people's character, including like people you work for and successful
[02:29:09.120 --> 02:29:17.280]   CEOs. And, you know, that can save you a lot of pain down the line. OK. So after that, FTX implodes
[02:29:17.280 --> 02:29:26.240]   and you're out. And then you got into you, you went to OpenAI, the super alignment team
[02:29:26.240 --> 02:29:32.960]   had just started. I think you were you were like part of the initial team. And so what was the
[02:29:32.960 --> 02:29:38.640]   original idea? What was compelling about that for you to join? Yeah, totally. So, I mean, what was
[02:29:38.640 --> 02:29:45.280]   the goal of the super alignment team? You know, the alignment team at OpenAI, you know, other labs
[02:29:45.280 --> 02:29:49.040]   sort of like several years ago kind of had done sort of basic research and they developed RLHF,
[02:29:49.040 --> 02:29:54.080]   Reinforcement Learning from Human Feedback. And that was sort of, you know, ended up being really
[02:29:54.080 --> 02:29:59.680]   successful technique for controlling sort of current generation of AI models. What we were
[02:29:59.680 --> 02:30:03.600]   trying to do was basically kind of be the basic research bet to figure out what is the successor
[02:30:03.600 --> 02:30:07.520]   to RLHF. And the reason we needed that is, you know, basically, you know, RLHF probably won't
[02:30:07.520 --> 02:30:12.320]   scale to superhuman systems. RLHF relies on sort of human raters who kind of thumbs up, thumbs down,
[02:30:12.320 --> 02:30:15.840]   you know, like the model said something, it looks fine, looks good to me. At some point, you know,
[02:30:15.840 --> 02:30:19.200]   the superhuman models, the super intelligence is going to write, you know, a million lines of,
[02:30:19.200 --> 02:30:22.960]   you know, crazy complex code. You don't know at all what's going on anymore. And so how do you
[02:30:22.960 --> 02:30:27.120]   kind of steer and control these systems? How do you hide side constraints? You know, the reason
[02:30:27.120 --> 02:30:32.720]   I joined was I thought this was an important problem and I thought it was just a really
[02:30:32.720 --> 02:30:37.280]   solvable problem, right? I thought this was basically, you know, I still do. I mean, even
[02:30:37.280 --> 02:30:42.000]   more so do. I think there's a lot of just really promising sort of ML research on alignment,
[02:30:42.000 --> 02:30:47.360]   on sort of aligning superhuman systems. And maybe we should talk about that a bit more later.
[02:30:47.360 --> 02:30:51.120]   And it was so solvable, you solved it in a year.
[02:30:51.120 --> 02:30:58.800]   Anyway, so look, I wanted to do this like really ambitious effort on alignment and,
[02:30:58.800 --> 02:31:02.560]   you know, Elliot was backing it and, you know, I liked a lot of the people there. And so I was,
[02:31:02.560 --> 02:31:05.680]   you know, I was really excited and I was kind of like, you know, I think there was a lot of people
[02:31:05.680 --> 02:31:10.080]   sort of on alignment. There's always a lot of people kind of making hay about it. And, you know,
[02:31:10.080 --> 02:31:14.960]   I appreciate people highlighting the importance of the problem. And I was just really into like,
[02:31:14.960 --> 02:31:17.920]   let's just try to solve it and let's do the ambitious effort, you know, do the, you know,
[02:31:17.920 --> 02:31:22.880]   operation warp speed for solving alignment. And it seemed like an amazing opportunity to do so.
[02:31:22.880 --> 02:31:28.640]   Okay. And now basically the team doesn't exist. I think the head of it has left.
[02:31:28.640 --> 02:31:32.800]   I both had to put it up left, Jan and Ilya. That's what the news of the last week.
[02:31:32.800 --> 02:31:38.880]   What happened? Why did the thing break down? I think opening eyes sort of decided to take
[02:31:38.880 --> 02:31:45.040]   things in a somewhat different direction. Meaning what? I mean, that super alignment
[02:31:45.040 --> 02:31:49.440]   isn't the best way to frame the... No, I mean, look, obviously it's sort of after the November
[02:31:49.440 --> 02:31:52.720]   board events, you know, there are personnel changes. I think Ilya leaving was just incredibly
[02:31:52.720 --> 02:31:58.000]   tragic for opening your eye. And, you know, I think some amount of reprioritization,
[02:31:58.000 --> 02:32:01.520]   I think some amount of, you know, I mean, there's been some reporting on the super alignment
[02:32:01.520 --> 02:32:04.320]   compute commitment. You know, there's this 20% compute commitment as part of, you know,
[02:32:04.320 --> 02:32:06.960]   how a lot of people recruited, you know, it's like, we're going to do this ambitious effort
[02:32:06.960 --> 02:32:13.600]   on alignment and, you know, some amount of, you know, not keeping that and deciding to go in a
[02:32:13.600 --> 02:32:20.720]   different direction. Okay. So now Jan has left, Ilya has left. So this team itself has dissolved,
[02:32:20.720 --> 02:32:25.680]   but you were the sort of first person who left or was forced to leave. You were,
[02:32:25.680 --> 02:32:30.240]   the information reported that you were fired for leaking. Well, what happened? Was this accurate?
[02:32:30.240 --> 02:32:36.400]   Yeah. Look, why don't I, why don't I tell you what they claim I leaked and you can tell me what you
[02:32:36.400 --> 02:32:41.440]   think. Yeah. So opening, I did claim to employees that I was fired for leaking. And, you know,
[02:32:41.440 --> 02:32:45.680]   I and others have sort of pushed them to say what the leak is. And so here's the response in full.
[02:32:45.680 --> 02:32:52.880]   You know, sometime last year, I had written a sort of brainstorming document on preparedness,
[02:32:52.880 --> 02:32:58.000]   on safety and security measures we need in the future on the path to AGI. And I shared that with
[02:32:58.000 --> 02:33:03.840]   three external researchers for feedback. So that's it. That's the leak. You know, I think for context,
[02:33:03.840 --> 02:33:07.920]   it was totally normal at opening eye at the time to share sort of safety ideas with external
[02:33:07.920 --> 02:33:13.360]   researchers for feedback. You know, it happened all the time. You know, the doc was sort of my
[02:33:13.360 --> 02:33:20.160]   ideas, you know, before I shared it, I reviewed it for anything sensitive. The internal version
[02:33:20.160 --> 02:33:24.800]   had a reference to a future cluster, but I redacted that for the external copy. You know,
[02:33:24.800 --> 02:33:29.520]   there's a link in there to some to some slides of mine, internal slides. But, you know, that was a
[02:33:29.520 --> 02:33:32.960]   dead link to the external people I shared it with. You know, the slides weren't shared with them.
[02:33:32.960 --> 02:33:39.120]   And so obviously, I pressed them to sort of tell me what is the confidential information in this
[02:33:39.120 --> 02:33:46.800]   document. And what they came back with was a line in the doc about planning for AGI by 27, 28,
[02:33:46.800 --> 02:33:52.880]   and that setting timelines for preparedness. You know, I wrote the stock, you know, a couple
[02:33:52.880 --> 02:33:56.400]   months after the super alignment announcement, we'd put out, you know, this sort of four year
[02:33:56.400 --> 02:34:00.160]   planning horizon. I didn't think that planning horizon was sensitive. You know, it's it's the
[02:34:00.160 --> 02:34:05.920]   sort of thing Sam says publicly all the time. I think sort of John said on my podcast a couple
[02:34:05.920 --> 02:34:14.000]   weeks ago. Anyway, so that's it. That's it. So that's pretty thin for if the cause was leaking,
[02:34:14.000 --> 02:34:18.960]   that seems pretty thin. Was there anything else to it? Yeah, I mean, so that was that was the
[02:34:18.960 --> 02:34:25.440]   leaking claim. I mean, say a bit more about sort of what happened. Sure. Yeah. So one thing was
[02:34:25.440 --> 02:34:31.280]   last year I had written a memo, internal memo about opening eye security. I thought it was,
[02:34:31.280 --> 02:34:34.320]   you know, egregiously insufficient. You know, I thought it wasn't sufficient to protect,
[02:34:34.320 --> 02:34:37.840]   you know, the theft of model weights or or key algorithmic secrets from foreign actors.
[02:34:37.840 --> 02:34:42.880]   So I wrote this memo. I shared it with a few colleagues, a couple members of leadership
[02:34:43.520 --> 02:34:49.040]   who sort of mostly said it was helpful. But then, you know, a couple weeks later, a sort of major
[02:34:49.040 --> 02:34:53.920]   security incident occurred, and that prompted me to share the memo with a couple members of the
[02:34:53.920 --> 02:34:59.120]   board. And so after I did that, you know, days later, it was made very clear to me that leadership
[02:34:59.120 --> 02:35:04.400]   was very unhappy with me having shared this memo with the board. You know, apparently the board
[02:35:04.400 --> 02:35:09.920]   had hassled leadership about security. And then I got sort of an official H.R. warning for this
[02:35:09.920 --> 02:35:15.760]   memo, you know, for sharing it with the board. The H.R. person told me it was racist to worry
[02:35:15.760 --> 02:35:22.960]   about CCP espionage. And they said it was sort of unconstructive. And, you know, look, I think
[02:35:22.960 --> 02:35:26.240]   I probably wasn't at my most diplomatic. You know, I definitely could have been more politically
[02:35:26.240 --> 02:35:31.360]   savvy. But, you know, I thought it was a really, really important issue. And, you know, the
[02:35:31.360 --> 02:35:35.760]   security incident had made me really worried anyway. And so I guess the reason I bring this
[02:35:35.760 --> 02:35:40.080]   up is when I was fired, it was sort of made very explicit that the security memo is a major reason
[02:35:40.080 --> 02:35:43.760]   for my being fired. You know, I think it was something like, you know, the reason that this
[02:35:43.760 --> 02:35:49.280]   is a firing and not a warning is because of the security memo. But you sharing it with the board.
[02:35:49.280 --> 02:35:56.080]   The warning I'd gotten for the security memo. Anyway, and I mean, some other, you know, what
[02:35:56.080 --> 02:35:59.360]   might also be helpful context is the sort of questions they asked me when they fired me.
[02:35:59.360 --> 02:36:03.920]   So, you know, this was a bit over a month ago. I was pulled aside for a chat with a lawyer,
[02:36:03.920 --> 02:36:09.520]   you know, that quickly turned very adversariable. And, you know, the questions were all about my
[02:36:09.520 --> 02:36:17.920]   views on AI progress, on AGI, on the level of security appropriate for AGI, on, you know,
[02:36:17.920 --> 02:36:25.600]   whether government should be involved in AGI, on, you know, whether I and Superalignment were
[02:36:25.600 --> 02:36:30.960]   loyal to the company, on, you know, what I was up to during the OpenAI board events, you know,
[02:36:30.960 --> 02:36:35.120]   things like that. And, you know, then they, you know, chatted to a couple of my colleagues,
[02:36:35.120 --> 02:36:38.880]   and then they came back and told me I was fired. And, you know, they'd gone through all of my
[02:36:38.880 --> 02:36:43.280]   digital artifacts from the time at OpenAI, you know, messages, docs, and that's when they found,
[02:36:43.280 --> 02:36:48.640]   you know, the leak. Yeah. And so anyway, so the main claim they made was this leaking allegation.
[02:36:48.640 --> 02:36:54.720]   You know, that's what they told employees. They, you know, the security memo. There's a couple
[02:36:54.720 --> 02:36:59.280]   other allegations they threw in. One thing they said was that I was unforthcoming during the
[02:36:59.280 --> 02:37:02.880]   investigation because I didn't initially remember who I had shared the doc with, the sort of
[02:37:02.880 --> 02:37:07.440]   preparedness brainstorming doc, only that I had sort of spoken to some external researchers about
[02:37:07.440 --> 02:37:12.240]   these ideas. And, you know, look, the doc was over six months old. You know, I'd spent a day on it.
[02:37:12.240 --> 02:37:16.960]   You know, it was a Google doc I shared with my OpenAI email. It wasn't a screenshot or anything
[02:37:16.960 --> 02:37:22.720]   I was trying to hide. It simply didn't stick because it was such a non-issue. And then they
[02:37:22.720 --> 02:37:27.760]   also claimed that I was engaging on policy in a way that they didn't like. And so what they
[02:37:27.760 --> 02:37:32.160]   cited there was that I had spoken to a couple of external researchers, you know, somebody got a
[02:37:32.160 --> 02:37:36.800]   think tank about my view that AGI would become a government project, you know, as we discussed.
[02:37:36.800 --> 02:37:40.640]   You know, in fact, I was speaking to lots of sort of people in the field about that at the time. I
[02:37:40.640 --> 02:37:44.640]   thought it was a really important thing to think about. Anyway, and so they found, you know,
[02:37:44.640 --> 02:37:48.240]   they found a DM that I'd written to, like a friendly colleague, you know, five or six months
[02:37:48.240 --> 02:37:53.840]   ago where I relayed this. And, you know, they cited that. And, you know, I had thought it was
[02:37:53.840 --> 02:37:58.080]   well within OpenAI norms to kind of talk about high-level issues on the future of AGI with
[02:37:58.080 --> 02:38:05.120]   external people in the field. So that's what they alleged. That's what happened. You know, I've
[02:38:05.120 --> 02:38:09.360]   spoken to kind of a few dozen former colleagues about this, you know, since I think the sort of
[02:38:09.360 --> 02:38:15.760]   universal reaction is kind of like, you know, that's insane. I was sort of surprised as well.
[02:38:15.760 --> 02:38:22.560]   You know, I had been promoted just a few months before. I think, you know, I think Ilya's comment
[02:38:22.560 --> 02:38:26.160]   for the promotion case at the time was something like, you know, Leopold's amazing. We're lucky to
[02:38:26.160 --> 02:38:33.040]   have him. But look, I mean, I think the thing I understand and I think in some sense is reasonable
[02:38:33.040 --> 02:38:36.400]   is like, you know, I think I ruffled some feathers and, you know, I think I was probably kind of
[02:38:36.400 --> 02:38:41.360]   annoying at times. You know, it's like I security staff and I kind of like repeatedly raised that
[02:38:41.360 --> 02:38:47.280]   and maybe not always in the most diplomatic way. You know, I didn't sign the employee letter
[02:38:47.280 --> 02:38:52.960]   during the board events, you know, despite pressure to do so. And you were one of like
[02:38:52.960 --> 02:38:58.000]   eight people or something. Yeah, I guess the I think the sort of two senior most people didn't
[02:38:58.000 --> 02:39:06.160]   sign were Andre and yeah. And, you know, I mean, on the letter, by the way, I am by the time on
[02:39:06.160 --> 02:39:09.840]   sort of Monday morning when that letter was going around, I think probably it was appropriate for
[02:39:09.840 --> 02:39:13.200]   the board to resign. I think they'd kind of like lost too much credibility and trust with the
[02:39:13.200 --> 02:39:18.640]   employees. But I thought the letter had a bunch of issues. I mean, I think one of them was it just
[02:39:18.640 --> 02:39:22.080]   didn't call for an independent board. I think it's sort of like basics of corporate governance
[02:39:22.080 --> 02:39:27.200]   to have an independent board. Anyway, you know, it's other things, you know, I am in sort of other
[02:39:27.200 --> 02:39:31.680]   discussions. I pressed leadership for sort of opening eye to abide by its public commitments.
[02:39:31.680 --> 02:39:37.840]   You know, I raised a bunch of tough questions about whether it was consistent with the opening
[02:39:37.840 --> 02:39:41.760]   eye mission and consistent with the national interest to sort of partner with authoritarian
[02:39:41.760 --> 02:39:48.320]   dictatorships to build the core infrastructure for AGI. So, you know, look, it's a free country,
[02:39:48.320 --> 02:39:54.160]   right? That's what I love about this country. We talked about it. And so they have no obligation
[02:39:54.160 --> 02:39:59.920]   to keep me on staff. And, you know, I think in some sense, I think it would have been perfectly
[02:39:59.920 --> 02:40:04.160]   reasonable for them to come to me and say, look, you know, we're taking the company in a different
[02:40:04.160 --> 02:40:08.720]   direction. You know, we disagree with your point of view. You know, we don't trust you enough to
[02:40:08.720 --> 02:40:14.000]   sort of tow the company line anymore. And, you know, thank you so much for your work at OpenAI,
[02:40:14.000 --> 02:40:16.800]   but I think it's time to part ways. I think that would have made sense. I think, you know,
[02:40:16.800 --> 02:40:21.440]   we did start sort of materially diverging on sort of views on important issues. I'd come in very
[02:40:21.440 --> 02:40:27.360]   excited in line with OpenAI, but that sort of changed over time. And look, I think I think
[02:40:27.360 --> 02:40:31.680]   there would have been a very amicable way to part ways. And I think it's a it's a bit of a shame
[02:40:31.680 --> 02:40:38.080]   that it sort of this is the way it went down. You know, all that being said, I think, you know,
[02:40:38.080 --> 02:40:43.520]   I really want to emphasize there's just a lot of really incredible people at OpenAI and it was an
[02:40:43.520 --> 02:40:48.160]   incredible privilege to work with them. And, you know, overall, I'm just extremely grateful for my
[02:40:48.160 --> 02:40:57.040]   time there. When you left now that there's now there's been reporting about an NDA that former
[02:40:57.040 --> 02:41:03.680]   employees have to sign in order to have access to their vested equity. Did you sign such an NDA?
[02:41:04.480 --> 02:41:09.200]   No, my situation was a little different in that it was sort of basically right before my cliff.
[02:41:09.200 --> 02:41:15.280]   But then, you know, they still offered me the equity, but I didn't want to sign a non-disparagement.
[02:41:15.280 --> 02:41:18.400]   You know, freedom is priceless. And how much was how much was the equity?
[02:41:18.400 --> 02:41:23.280]   Like close to a million dollars. So it was definitely a thing you were
[02:41:23.280 --> 02:41:28.960]   you and others were aware of that this is like a choice that OpenAI is explicitly offering you.
[02:41:28.960 --> 02:41:34.560]   Yeah. And presumably the person on OpenAI's staff knew that we're offering them equity,
[02:41:34.560 --> 02:41:39.200]   but they had to sign this NDA that has these conditions that you can't, for example, give the
[02:41:39.200 --> 02:41:44.800]   kind of statements about your thoughts on AGI and OpenAI that you're giving on this podcast right
[02:41:44.800 --> 02:41:48.480]   now. Look, I don't know what the whole situation is. I certainly think sort of vested equity is
[02:41:48.480 --> 02:41:52.480]   pretty rough if you're conditioning that onto an NDA. It might be a somewhat different situation
[02:41:52.480 --> 02:41:56.560]   if it's a sort of severance agreement. Right. But an OpenAI employee who had
[02:41:56.560 --> 02:41:59.360]   signed it presumably could not give the podcast that you're giving today.
[02:41:59.360 --> 02:42:07.680]   Quite possibly not. Yeah, I don't know. Okay. So analyzing the situation here,
[02:42:07.680 --> 02:42:13.760]   I guess if you were to, yeah, the board thing is really tough because if you were trying to
[02:42:13.760 --> 02:42:18.720]   defend them, you would say, well, listen, you were just kind of going outside the regular chain of
[02:42:18.720 --> 02:42:24.240]   command. And maybe there's a point there, although the way in which the person from HR thinks that
[02:42:24.240 --> 02:42:28.080]   you have an adversarial relationship with, or you're supposed to have an adversarial relationship
[02:42:28.080 --> 02:42:37.120]   with the board, where to give the board some information, which is relevant to whether OpenAI
[02:42:37.120 --> 02:42:42.160]   is fulfilling its mission and whether it can do that in a better way is part of the leak as if
[02:42:42.160 --> 02:42:46.800]   the board is, that is supposed to ensure that OpenAI is following its mission as some sort of
[02:42:46.800 --> 02:42:51.120]   external actor. That seems pretty... I mean, I think, I mean, to be clear,
[02:42:51.120 --> 02:42:54.400]   the leak allegation was just that sort of documented feedback. This is just sort of
[02:42:54.400 --> 02:42:57.040]   a separate thing that they cited. And they said, I wouldn't have been fired if not for
[02:42:57.040 --> 02:43:01.200]   the security memo. They said you wouldn't have been fired. They said the reason this is a firing
[02:43:01.200 --> 02:43:06.720]   and not a warning is because of the warning you'd gotten for the security memo. Oh, before you left,
[02:43:06.720 --> 02:43:12.960]   the incidents with the board happened, where Sam was fired and then rehired a CEO and now he's on
[02:43:12.960 --> 02:43:19.760]   the board. Now, Ilya and Jan, who are the heads of the super alignment team and Ilya, who is a
[02:43:19.760 --> 02:43:25.120]   co-founder of OpenAI, obviously the most significant in terms of stature, a member of
[02:43:25.120 --> 02:43:30.080]   OpenAI from a research perspective, they've left. It seems like, especially with regards to super
[02:43:30.080 --> 02:43:35.280]   alignment stuff and just generally the OpenAI, a lot of this sort of personnel drama has happened
[02:43:35.280 --> 02:43:42.320]   over the last few months. What's going on? Yeah, there's a lot of drama. Yeah. So why
[02:43:42.320 --> 02:43:48.400]   is there so much drama? You know, I think there would be a lot less drama if all OpenAI claimed
[02:43:48.400 --> 02:43:53.600]   to be was sort of building chat GPT or building business software. I think where a lot of the
[02:43:53.600 --> 02:43:58.560]   drama comes from is OpenAI really believes they're building AGI, right? And it's not just
[02:43:58.560 --> 02:44:05.280]   a claim they make for marketing purposes, whatever. There's this report that Sam is
[02:44:05.280 --> 02:44:09.920]   raising $7 trillion for chips and it's like that stuff only makes sense if you really believe in
[02:44:09.920 --> 02:44:15.280]   AGI. And so I think what gets people sometimes is sort of the cognitive dissonance between sort
[02:44:15.280 --> 02:44:19.680]   of really believing in AGI, but then sort of not taking some of the other implications seriously.
[02:44:19.680 --> 02:44:23.600]   You know, this is going to be incredibly powerful technology, both for good and for bad. And
[02:44:23.600 --> 02:44:28.080]   that implicates really important issues like the national security issues we spoke about. Like,
[02:44:28.080 --> 02:44:32.160]   you know, are you protecting the secrets from the CCP? Like, you know, does America control
[02:44:32.160 --> 02:44:36.480]   the core AGI infrastructure or does it, you know, a Middle Eastern dictator control the core AGI
[02:44:36.480 --> 02:44:43.600]   infrastructure? And then, I mean, I think the thing that, you know, really gets people is
[02:44:43.600 --> 02:44:48.720]   the sort of tendency to kind of then make commitments and sort of like, you know, they
[02:44:48.720 --> 02:44:52.400]   say they take these issues really seriously, they make big commitments on them, but then sort of
[02:44:52.400 --> 02:44:55.760]   frequently don't follow through, right? So, you know, again, as mentioned, there was this
[02:44:55.760 --> 02:45:00.080]   commitment around super alignment compute, you know, sort of 20% of compute for this long-term
[02:45:00.080 --> 02:45:04.080]   safety research effort. And I think, you know, you and I could have a totally reasonable debate
[02:45:04.080 --> 02:45:09.280]   about what is the appropriate level of compute for super alignment. But that's not really the
[02:45:09.280 --> 02:45:13.200]   issue. The issue is that this commitment was made and it was used to recruit people and, you know,
[02:45:13.200 --> 02:45:18.800]   it was very public and it was made because, you know, there's a recognition that there would
[02:45:18.800 --> 02:45:22.000]   always be something more urgent than a long-term safety research effort, you know, like some new
[02:45:22.000 --> 02:45:26.880]   product or whatever. But then, in fact, they just, you know, really didn't keep the commitment.
[02:45:26.880 --> 02:45:30.480]   And so, you know, there was always something more urgent than long-term safety research.
[02:45:30.480 --> 02:45:34.800]   I mean, I think another example of this is, you know, when I raised these issues about
[02:45:34.800 --> 02:45:39.680]   security, you know, they would tell me, you know, security is our number one priority.
[02:45:41.120 --> 02:45:46.720]   But then, you know, invariably, when it came time to sort of invest serious resources,
[02:45:46.720 --> 02:45:50.800]   when it came time to make trade-offs, to sort of take some pretty basic measures,
[02:45:50.800 --> 02:45:55.840]   security would not be prioritized. And so, yeah, I think it's the cognitive dissonance,
[02:45:55.840 --> 02:46:00.000]   and I think it's the sort of unreliability that causes a bunch of the drama.
[02:46:00.000 --> 02:46:08.320]   So let's zoom out, talk about the part, a big part of the story, and also a big motivation
[02:46:08.320 --> 02:46:12.480]   of the way in which it must proceed with regards to geopolitics and everything,
[02:46:12.480 --> 02:46:18.160]   is that once you have the AGI, pretty soon after you proceed to ASI, because superintelligence,
[02:46:18.160 --> 02:46:25.120]   because you have these AGIs which can function as researchers into further AI progress.
[02:46:25.120 --> 02:46:30.480]   And within a matter of years, maybe less, you go to something that is like superintelligence.
[02:46:30.480 --> 02:46:35.360]   And at the high, and then from there, then you can do, according to your story,
[02:46:35.360 --> 02:46:40.640]   do all this research and development into robotics and pocket nukes and whatever other crazy shit.
[02:46:40.640 --> 02:46:49.680]   Okay, but there's, I'm skeptical of this story for many reasons. At a high level,
[02:46:49.680 --> 02:46:56.480]   it's not clear to me this input-output model of research is how things actually happen in research.
[02:46:56.480 --> 02:47:02.320]   We can look at economy-wide, right? Patrick Hollis and others have made this point
[02:47:02.320 --> 02:47:06.560]   that compared to 100 years ago, we have 100x more researchers in the world.
[02:47:06.560 --> 02:47:11.840]   It's not like progress is happening 100x faster. So it's clearly not the case that you can just
[02:47:11.840 --> 02:47:16.880]   pump in more population into research and you get higher research on the other end.
[02:47:16.880 --> 02:47:20.320]   I don't know why it would be different for the AI researchers themselves.
[02:47:20.320 --> 02:47:23.920]   Okay, great. So this is getting into some good stuff. I have a classic disagreement
[02:47:23.920 --> 02:47:28.880]   I have with Patrick and others. All right, so obviously inputs matter, right? So it's like,
[02:47:28.880 --> 02:47:32.640]   the United States produces a lot more scientific and technological progress
[02:47:32.640 --> 02:47:39.440]   than Liechtenstein or Switzerland. And even if I made Patrick Hollis a dictator of Liechtenstein
[02:47:39.440 --> 02:47:44.320]   or Switzerland, and Patrick Hollis was able to implement his utopia of ideal institutions,
[02:47:44.320 --> 02:47:48.080]   keeping the talent pool fixed. He's not able to do some crazy high school immigration thing or
[02:47:48.080 --> 02:47:52.320]   whatever, some crazy genetic breeding scheme or whatever he wants to do.
[02:47:52.320 --> 02:47:58.080]   Keeping the talent pool fixed, but amazing institutions. I claim that still,
[02:47:58.080 --> 02:48:01.280]   even if you made Patrick Hollis a dictator of Switzerland, maybe you get some factor,
[02:48:01.280 --> 02:48:04.480]   but Switzerland is not going to be able to outcompete the United States in scientific
[02:48:04.480 --> 02:48:06.560]   and technological progress. Obviously, magnitudes matter.
[02:48:06.560 --> 02:48:11.440]   Okay. No, I actually, I'm not sure I agree with this. There's been many examples
[02:48:11.440 --> 02:48:16.480]   in history where you have small groups of people who are part of like Bell Labs or Skunkworks or
[02:48:16.480 --> 02:48:20.800]   something. There's a couple hundred researchers, open AI, right? A couple hundred researchers,
[02:48:20.800 --> 02:48:23.840]   they do make- Highly selected though, right? It's like saying, you know-
[02:48:23.840 --> 02:48:27.840]   That's part of why Patrick Hollis as a dictator is going to do a good job of this.
[02:48:27.840 --> 02:48:31.120]   Well, yes. If he can highly select all the best AI researchers in the world,
[02:48:31.120 --> 02:48:34.720]   he might only need a few hundred, but if you, you know, that's the talent pool. It's like you have
[02:48:34.720 --> 02:48:39.280]   300 best AI researchers in the world. But there has been, it's not a case of
[02:48:39.280 --> 02:48:43.440]   from a hundred years to now, there haven't been, the population has increased massively.
[02:48:43.440 --> 02:48:46.880]   A lot of the, in fact, you would expect the density of talent to have increased in the sense
[02:48:46.880 --> 02:48:52.320]   that malnutrition and other kinds of debility, poverty, whatever, that have debilitated past
[02:48:52.320 --> 02:48:55.520]   talent at the same sort of level is no longer debilitated in the same way.
[02:48:55.520 --> 02:48:59.120]   To the 100x point, right? So I don't know if it's 100x. I think it's easy to inflate these things,
[02:48:59.120 --> 02:49:03.680]   probably at least 10x. And so people are sometimes like, ah, you know, like, you know, come on,
[02:49:03.680 --> 02:49:06.720]   ideas haven't gotten that much harder to find. You know, why would you have needed this 10x
[02:49:06.720 --> 02:49:10.880]   increase in research effort? Whereas to me, I think this is an extremely natural story.
[02:49:10.880 --> 02:49:14.240]   And why is it a natural story? It's a straight line on the log-log plot. This is sort of a,
[02:49:14.240 --> 02:49:17.520]   you know, deep learning researcher's dream, right? What is this log-log plot?
[02:49:17.520 --> 02:49:23.200]   On the X-axis, you have log cumulative research effort. On the Y-axis, you have some log GDP or
[02:49:23.200 --> 02:49:28.080]   looms of algorithmic progress, or, you know, log transistors per square inch, or, you know,
[02:49:28.080 --> 02:49:32.560]   in the sort of experience curve for solar, kind of like, you know, whatever the log of, you know,
[02:49:32.560 --> 02:49:37.040]   the price for a gigawatt of solar. And it's extremely natural for that to be a straight
[02:49:37.040 --> 02:49:40.480]   line. You know, this is sort of a class, yeah, it's a classic. And, you know, it's basically
[02:49:40.480 --> 02:49:44.080]   the first thing is very easy. Then basically, you know, you have to have log increments of
[02:49:44.080 --> 02:49:48.320]   cumulative research effort to find the next thing. And so, you know, in some sense, I think
[02:49:48.320 --> 02:49:53.600]   this is a natural story. Now, one objection kind of people then make is like, oh, you know, isn't
[02:49:53.600 --> 02:49:58.560]   it suspicious, right? That like ideas, you know, well, we increased research effort 10X, and ideas
[02:49:58.560 --> 02:50:04.400]   also just got 10X harder to find. And so it perfectly, you know, equilibrates. And to there,
[02:50:04.400 --> 02:50:07.920]   I say, you know, it's just, it's an equilibrium. It's an adagis equilibrium, right? So it's like,
[02:50:07.920 --> 02:50:12.720]   you know, isn't it a coincidence that supply equals demand, you know, and the market clears,
[02:50:12.720 --> 02:50:16.640]   right? And that's, and same thing here, right? So it's, you know, ideas getting,
[02:50:16.640 --> 02:50:19.760]   how much ideas have gotten harder to find is a function of how much progress you've made.
[02:50:19.760 --> 02:50:24.560]   And then, you know, what the overall growth rate has been is a function of how much ideas have
[02:50:24.560 --> 02:50:28.720]   gotten harder to find in ratio to how much you've been able to like increase research effort. What
[02:50:28.720 --> 02:50:32.000]   is the sort of growth of log cumulative research effort? So in some sense, I think the story is
[02:50:32.000 --> 02:50:35.760]   sort of like, fairly natural. And you see this, you see this not just economy wide, you see it
[02:50:35.760 --> 02:50:40.080]   in kind of experience curve for all sorts of individual technologies. So I think there's
[02:50:40.080 --> 02:50:43.760]   some process like this. I think it's totally plausible that, you know, institutions have
[02:50:43.760 --> 02:50:47.440]   gotten worse by some factor. Obviously, there's some sort of exponent of diminishing returns on
[02:50:47.440 --> 02:50:52.720]   more people, right? So like serial time is better than just parallelizing. But still, I think it's
[02:50:52.720 --> 02:51:00.960]   like clearly inputs matter. Yeah, I agree that but if the coefficient of how fast they diminish as
[02:51:00.960 --> 02:51:06.800]   you grow, the input is high enough, then the in the abstract, the fact that inputs matter isn't
[02:51:06.800 --> 02:51:10.960]   that relevant. Okay, so I mean, we're talking to a very high level, but just like take it down to
[02:51:10.960 --> 02:51:18.640]   the actual concrete thing here. OpenAI has a staff of at most low hundreds who are directly involved
[02:51:18.640 --> 02:51:23.680]   in the algorithmic progress in future models. If it was really the case that you could just
[02:51:23.680 --> 02:51:28.400]   arbitrarily scale this number, and you can have much faster algorithmic progress, and that would
[02:51:28.400 --> 02:51:34.640]   result in much higher, much better AI for OpenAI basically, then it's not clear why OpenAI doesn't
[02:51:34.640 --> 02:51:38.800]   just go out and hire every single person with 150 IQ, of which there are hundreds of thousands in
[02:51:38.800 --> 02:51:45.840]   the world. And my story there is there's transaction costs to managing all these people
[02:51:45.840 --> 02:51:52.480]   that don't just go away if you have a bunch of AIs, that these tasks aren't easy to parallelize.
[02:51:52.480 --> 02:51:57.920]   And I think you, I'm not sure how you would explain the fact of like, why doesn't OpenAI
[02:51:57.920 --> 02:52:02.320]   go on a recruiting binge of every single genius in the world? Okay, great. So let's talk about
[02:52:02.320 --> 02:52:06.000]   the OpenAI example. And let's talk about the automated AI researchers. So I mean, the OpenAI
[02:52:06.000 --> 02:52:09.280]   case, I mean, just, you know, just kind of like look at the inflation of like AI researcher
[02:52:09.280 --> 02:52:12.240]   salaries over the last year. I mean, I think like, I don't know, I don't know what it is,
[02:52:12.240 --> 02:52:15.920]   you know, 4x, 5x, it's kind of crazy. So they're clearly really trying to recruit the best AI
[02:52:15.920 --> 02:52:20.320]   researchers in the world. And, you know, I don't know, it's, they do find the best AI researchers
[02:52:20.320 --> 02:52:24.480]   in the world. I think my response to your thing is like, you know, almost all of these 150 IQ
[02:52:24.480 --> 02:52:27.360]   people, you know, if you just hire them tomorrow, they wouldn't be good AI researchers, they
[02:52:27.360 --> 02:52:32.080]   wouldn't be an Alec Radford. But they're willing to make investments that take years to pay out
[02:52:32.080 --> 02:52:36.880]   of the four. The data centers they're buying right now will come online in 2026 or something.
[02:52:36.880 --> 02:52:41.520]   Why wouldn't they be able to make every 150 IQ person, some of them will work out, some of them
[02:52:41.520 --> 02:52:46.160]   won't have the traits we like. But some of them by 2026 will be amazing AI researchers. Why aren't
[02:52:46.160 --> 02:52:49.440]   they making that bet? Yeah. And so sometimes this happens, right? Like smart physicists have been
[02:52:49.440 --> 02:52:52.960]   really good at AI research, you know, it's like all the anthropic co-founders. But like, if you
[02:52:52.960 --> 02:52:57.520]   talk to, I had Daria on the podcast, they have this very careful policy of like, we're not going
[02:52:57.520 --> 02:53:03.040]   to just hire arbitrarily, we're going to be extremely selective. Training is not as easily
[02:53:03.040 --> 02:53:07.040]   scalable, right? So training is very hard. You know, if you just hired, you know, 100,000 people,
[02:53:07.040 --> 02:53:11.120]   it's like, I mean, you couldn't train them all, it'd be really hard to train them all,
[02:53:11.120 --> 02:53:14.240]   you know, you wouldn't be doing any AI research. Like, you know, there's there's huge costs of
[02:53:14.240 --> 02:53:17.440]   bringing on a new person training them. This is very different with the AIs, right? And I think
[02:53:17.440 --> 02:53:20.720]   this is it's really important to talk about the sort of like advantages the AIs will have.
[02:53:20.720 --> 02:53:24.160]   So it's like, you know, training, right? It's like, what does it take to be an Alec Radford,
[02:53:24.160 --> 02:53:27.520]   you know, we need to be in a really good engineer, right? The AIs, they're going to be an amazing
[02:53:27.520 --> 02:53:31.360]   engineer, they're gonna be amazing at coding, you can just train them to do that. They need to have,
[02:53:31.360 --> 02:53:35.600]   you know, not just be a good engineer, but have really good research intuitions, and like really
[02:53:35.600 --> 02:53:39.360]   understand deep learning. And this is stuff that, you know, Alec Radford, or somebody like him has
[02:53:39.360 --> 02:53:43.280]   acquired over years of research over just like being deeply immersed in deep learning, having
[02:53:43.280 --> 02:53:48.240]   tried lots of things himself and failed. The AIs, you know, they're going to be able to read every
[02:53:48.240 --> 02:53:51.920]   research paper I've written, every experiment ever run at the lab, you know, like gain the
[02:53:51.920 --> 02:53:54.880]   intuitions from all of this, they're going to be able to learn in parallel from all of each other's
[02:53:54.880 --> 02:53:59.120]   experiment, you know, experiences. You know, I don't know what else, you know, it's like,
[02:53:59.120 --> 02:54:01.920]   what does it take to be an Alec Radford? Well, there's a there's a sort of cultural
[02:54:01.920 --> 02:54:06.000]   acclimation aspect of it, right? You know, if you hire somebody new, there's like politicking,
[02:54:06.000 --> 02:54:10.080]   maybe they don't fit in. Well, in the AI case, you just make replicas, right? There's a like
[02:54:10.080 --> 02:54:14.240]   motivation aspect for it, right? So it's like, you know, Alec, you know, I could just like duplicate
[02:54:14.240 --> 02:54:18.400]   Alec Radford. And before I run every experiment, I haven't spent like, you know, a decade's worth
[02:54:18.400 --> 02:54:22.080]   of human time, like double checking the code and thinking really carefully about it. I mean,
[02:54:22.080 --> 02:54:25.520]   first of all, I don't have that many Alec Radfords. And, you know, he wouldn't care.
[02:54:25.520 --> 02:54:30.160]   And he would not be motivated. But you know, the AI can just be like, look, I have 100 million of
[02:54:30.160 --> 02:54:34.240]   you guys, I'm just going to put you on just like, really making sure this code is correct. There are
[02:54:34.240 --> 02:54:39.840]   no bugs, this experiment is thought through every hyper parameter is correct. Final thing I'll say
[02:54:39.840 --> 02:54:44.080]   is, you know, the 100 million human equivalent AI researchers, that is just a way to visualize it.
[02:54:44.080 --> 02:54:47.920]   So that doesn't mean you're gonna have literally 100 million copies. So there's trade offs,
[02:54:47.920 --> 02:54:52.240]   you can make between serial speed and in parallel. So you might make the trade off is look, we're
[02:54:52.240 --> 02:54:56.960]   going to run them at 10x 100x serial speed, it's going to result in fewer tokens overall,
[02:54:56.960 --> 02:55:00.000]   because it's sort of inherent trade offs. But you know, then we have, I don't know what the
[02:55:00.000 --> 02:55:04.400]   numbers would be. But then we have, you know, 100,000 of them running at 100x human speed,
[02:55:04.400 --> 02:55:08.320]   and thinking and, you know, and there's other things you can do on coordination, you can kind
[02:55:08.320 --> 02:55:12.320]   of like share latent space attend to each other's context, there's, there's basically this huge range
[02:55:12.320 --> 02:55:16.080]   of possibilities of things you can do. The 100 million thing is more I mean, another illustration
[02:55:16.080 --> 02:55:20.160]   of this is, you know, if you kind of run the math in my series, that's basically, you know,
[02:55:20.160 --> 02:55:25.840]   2728, you have this automated AI researcher, you're going to be able to generate an entire
[02:55:25.840 --> 02:55:30.800]   internet's worth of tokens every single day. So it's clearly sort of a huge amount of like
[02:55:30.800 --> 02:55:36.160]   intellectual work they can do. I think the analogous thing there is today we generate
[02:55:36.160 --> 02:55:41.440]   more patents in a year than during the actual physics revolution in the early 20th century,
[02:55:41.440 --> 02:55:45.920]   they were generating across like half a century or something. And are you making more physics
[02:55:45.920 --> 02:55:49.520]   progress in a year today than we were? So yeah, you're gonna generate all these tokens?
[02:55:49.520 --> 02:55:55.760]   Yeah. Are you generating as much codified knowledge as humanity has been able to generate
[02:55:55.760 --> 02:55:59.760]   in the initial creation of the internet? Internet tokens are usually final output,
[02:55:59.760 --> 02:56:03.520]   right? Right. A lot of these tokens, if we talked, we talked about the unhobbling, right? Right. I
[02:56:03.520 --> 02:56:07.840]   think of a kind of like, you know, a GPN token is sort of like one token of my internal monologue.
[02:56:07.840 --> 02:56:10.560]   Yeah. Right. And so that's how I do this math on human equivalence, you know, it's like 100
[02:56:10.560 --> 02:56:15.040]   tokens a minute. And then, you know, humans working for x hours and you know, what is the what is the
[02:56:15.040 --> 02:56:19.680]   equivalent there? I think this goes back to something we're talking about earlier, where
[02:56:19.680 --> 02:56:26.080]   why haven't we seen the huge revenues from people often ask this question that if you took GPT-4
[02:56:26.080 --> 02:56:29.840]   back 10 years, and you show people this, they think this is gonna automate, this is already
[02:56:29.840 --> 02:56:35.520]   automated half the jobs. And so there's a sort of a modus ponens, modus tollens here, where part of
[02:56:35.520 --> 02:56:39.040]   the explanation is like, oh, it's like just on the verge, you need to do these unhobblings. And part
[02:56:39.040 --> 02:56:43.920]   of that is probably true, right. But there is another lesson to learn there, which is that
[02:56:43.920 --> 02:56:49.680]   just looking at face value at a set of abilities, yeah, there's probably more sort of hobblings that
[02:56:49.680 --> 02:56:53.520]   you don't realize that are hidden behind the scenes. I think the same will be true of the
[02:56:53.520 --> 02:56:58.160]   AGI that you have running as AI researchers. I think a lot of things basically agree, right?
[02:56:58.160 --> 02:57:01.760]   I think I think my story here is like, you know, you know, I talk about I think there's gonna be
[02:57:01.760 --> 02:57:06.000]   some long tail, right. And so, you know, maybe it's like, you know, 2627, you're like the proto
[02:57:06.000 --> 02:57:09.280]   automated engineer, and it's really good at engineering, it doesn't have the research intuition
[02:57:09.280 --> 02:57:13.600]   yet, you don't quite know how to put them to work. But you know, the sort of even the underlying
[02:57:13.600 --> 02:57:17.440]   pace of AI progress is already so fast, right, in three years from not being able to do any kind of
[02:57:17.440 --> 02:57:22.240]   like math at all to now crushing crushing these math competitions. And so you have the initial
[02:57:22.240 --> 02:57:26.320]   thing and like 2627, maybe the sort of audit, it's an automated research engineer, it speeds you up
[02:57:26.320 --> 02:57:30.080]   by two x, you go through a lot more progress in that year, by the end of the year, you figured
[02:57:30.080 --> 02:57:33.920]   out like the remaining kind of unhobblings, you've like got a smarter model. And you know, maybe then
[02:57:33.920 --> 02:57:37.760]   that thing, or maybe it's two years, you know, that thing, just like that thing really can do
[02:57:37.760 --> 02:57:41.440]   automate 100%. And again, you know, they don't need to be doing everything, they don't need to
[02:57:41.440 --> 02:57:44.800]   be making coffee, you know, they don't need to like, you know, maybe there's a bunch of, you know,
[02:57:44.800 --> 02:57:50.400]   tacit knowledge and a bunch of other fields. But you know, AI researchers at AI labs really know
[02:57:50.400 --> 02:57:53.760]   the job of an AI researcher. And it's in some sense, it's a sort of there's lots of clear
[02:57:53.760 --> 02:57:58.000]   metrics, it's all virtual, there's code, it's things you can kind of develop and train for.
[02:57:58.000 --> 02:58:04.720]   So I mean, another thing is how do you actually manage a million AI researchers, humans,
[02:58:04.720 --> 02:58:10.560]   the sort of comparative ability we have that we've been especially trained for is like working in
[02:58:10.560 --> 02:58:16.080]   teams. Yeah. And despite this fact, we have, for 1000s of years, we've been learning about how we
[02:58:16.080 --> 02:58:21.120]   work together in groups. And despite this management is a clusterfuck, right? It's like
[02:58:21.120 --> 02:58:26.880]   most companies are badly managed. It's, it's really hard to do this stuff. Yeah. For AIs,
[02:58:26.880 --> 02:58:35.680]   the sort of like, we talk about AGI, but it'll be some bespoke set of abilities, some of which will
[02:58:35.680 --> 02:58:41.360]   be higher than humans, so much will be at human level. And so it'll be some bundle and we'll need
[02:58:41.360 --> 02:58:48.640]   to figure out how to put these bundles together with their human overseers with the equipment and
[02:58:48.640 --> 02:58:54.480]   everything. And the idea that as soon as you get the bundle, you'll figure out how to get like,
[02:58:54.480 --> 02:59:00.320]   just shove millions of them together and manage them. I'm just very skeptical of like, any other
[02:59:00.320 --> 02:59:08.080]   revolution, technological revolution in history has been very piecemeal, much more piecemeal than
[02:59:08.080 --> 02:59:12.160]   you would expect on paper. If you just thought about what is the industrial revolution? Well,
[02:59:12.160 --> 02:59:16.800]   we dig up coal that powers the steam engines, we use the steam engines to run these railroads
[02:59:16.800 --> 02:59:21.200]   that helps us get more coal out. And there's sort of like factorial story you can tell where in like
[02:59:21.200 --> 02:59:27.680]   a six, six hours, you can be pumping 1000s of times more coal, but in real life, it takes centuries
[02:59:27.680 --> 02:59:34.640]   often, right? In fact, the electrification, there's this famous study about how to initially
[02:59:34.640 --> 02:59:42.880]   to electrify factories. It was decades after electricity to change from the pull pulleys and
[02:59:42.880 --> 02:59:47.600]   water wheel based system that we had for steam engines to one that works with more spread out
[02:59:47.600 --> 02:59:50.480]   electrical motors and everything. I think this will be the same kind of thing. It might take
[02:59:50.480 --> 02:59:53.440]   like decades to actually get millions of AI researchers to work together.
[02:59:53.440 --> 02:59:57.120]   Okay, great. This is great. Okay, so a few responses to that. First of all, I mean,
[02:59:57.120 --> 03:00:00.800]   I totally agree with the kind of like real world bottlenecks type of thing. I think this is sort of,
[03:00:00.800 --> 03:00:05.200]   you know, I think it's easy to underrate. You know, basically, what we're doing is we're
[03:00:05.200 --> 03:00:08.800]   removing the labor constraint, we automate labor, and we like kind of explode technology.
[03:00:08.800 --> 03:00:11.360]   But you know, there's still lots of other bottlenecks in the world. And so I think this
[03:00:11.360 --> 03:00:14.160]   is part of why the story is that kind of like starts pretty narrow at the thing where you don't
[03:00:14.160 --> 03:00:18.400]   have these bottlenecks. And then only over time, as we let it kind of expand to sort of broader
[03:00:18.400 --> 03:00:22.720]   areas. This is part of why I think it's like initially this sort of AI research explosion,
[03:00:22.720 --> 03:00:26.160]   right? It's like AI research doesn't run into these real world bottlenecks. It doesn't require,
[03:00:26.160 --> 03:00:30.000]   you know, like plow a field or dig up coal. It's just you're just doing AI research.
[03:00:30.000 --> 03:00:32.400]   The other thing, you know, the other thing about it, like in your model,
[03:00:32.400 --> 03:00:38.320]   it's not complicated, like about flipping a burger, it's just AI research.
[03:00:38.320 --> 03:00:44.080]   I mean, this is because people make these arguments of like, oh, you know, AGI won't do
[03:00:44.080 --> 03:00:47.440]   anything because it can't flip a burger. Like, yeah, we'll be able to flip a burger, but it's
[03:00:47.440 --> 03:00:52.000]   going to be able to do algorithmic progress, you know, and then when it does algorithmic progress,
[03:00:52.000 --> 03:00:55.760]   it'll figure out how to flip a burger, you know, and then we'll have the burger flipping, you know,
[03:00:55.760 --> 03:01:00.000]   robot. You know, look, so the other thing is about, you know, again, these are the sort of
[03:01:00.000 --> 03:01:04.080]   quantities are lower bound, right? So it's like, this is just like, we can definitely run 100
[03:01:04.080 --> 03:01:07.680]   million of these. Probably what will happen is one of the first things we're going to try to figure
[03:01:07.680 --> 03:01:12.960]   out is how to like, again, run, like, you know, translate quantity into quality, right? And so
[03:01:12.960 --> 03:01:17.040]   it's like, even at the baseline rate of progress, you're like quickly getting smarter and smarter
[03:01:17.040 --> 03:01:20.320]   systems, right? If we said it was like, you know, four years between the preschooler and the high
[03:01:20.320 --> 03:01:23.600]   schooler, right? So I think, you know, pretty quickly, you know, there's probably some like
[03:01:23.600 --> 03:01:27.200]   simple algorithmic changes you find, you know, instead of one Alec Radford, you have 100, you
[03:01:27.200 --> 03:01:31.600]   know, you don't even need 100 million. And then, and then you get even smarter systems. And now
[03:01:31.600 --> 03:01:34.720]   these systems are, you know, they're capable of sort of creative, complicated behavior, you don't
[03:01:34.720 --> 03:01:38.480]   understand, maybe there's some way to like, use all this test time compute in a more unified way,
[03:01:38.480 --> 03:01:43.920]   rather than all these parallel copies. And, you know, so there won't just be quantitatively
[03:01:43.920 --> 03:01:47.520]   superhuman, they'll pretty quickly become kind of qualitatively superhuman. You know, it's sort
[03:01:47.520 --> 03:01:51.120]   of like, look, like, you know, you're a high school student, you're like trying to wrap yourself
[03:01:51.120 --> 03:01:54.960]   wrap your mind around kind of standard physics. And then there's some like super smart professor
[03:01:54.960 --> 03:01:59.120]   who is like, quantum physics, it all makes sense to him. And you're just like, what is going on?
[03:01:59.120 --> 03:02:03.360]   And sort of, I think pretty quickly, you kind of enter that regime, just given even the underlying
[03:02:03.360 --> 03:02:06.960]   pace of AI progress, but even more quickly than that, because you have this sort of accelerated
[03:02:06.960 --> 03:02:13.760]   force of now this automated AI research. I agree that over time, you would, I'm not
[03:02:13.760 --> 03:02:18.480]   denying that ASI is a thing that's possible, you know, I'm just like, how is this happening
[03:02:18.480 --> 03:02:22.240]   in a year? Like you? Okay, first of all, I think the story is sort of like, basically,
[03:02:22.240 --> 03:02:25.360]   I think it's a little bit more continuous, you know, right? Like, I think already, you know,
[03:02:25.360 --> 03:02:28.720]   like I talked about, you know, 2526, you're basically gonna have models as good as a college
[03:02:28.720 --> 03:02:32.640]   graduate. And you know, I don't, I don't know where the unhobbling is going to be. But I think
[03:02:32.640 --> 03:02:36.480]   it's possible that even then you have kind of the proto automated engineer. So there's, I think there
[03:02:36.480 --> 03:02:40.640]   is a bit of like, a smear, kind of an AGI smear or whatever, where it's like, there's sort of
[03:02:40.640 --> 03:02:43.840]   unhobblings that you're missing, there's kind of like ways of connecting them, you're missing,
[03:02:43.840 --> 03:02:47.120]   there's like some level of intelligence you're missing. But then at some point, you are going
[03:02:47.120 --> 03:02:51.760]   to get the thing that is like 100% automated, Alec Radford. Once you have that, you know,
[03:02:51.760 --> 03:02:57.760]   things really take off, I think. Yeah, okay. So let's go back to the unhobbling. Yeah. Is there,
[03:02:57.760 --> 03:03:02.720]   we're gonna get a bunch of models by the end of the year. Is there something, let's suppose we
[03:03:02.720 --> 03:03:07.120]   didn't get some capacity by the end of the year? Yeah. Is there some such capacity, which lacking
[03:03:07.120 --> 03:03:11.280]   would suggest that AI progress is going to take longer than you are projecting? Yeah, I mean,
[03:03:11.280 --> 03:03:14.640]   I think there's two kind of key things. There's the unhobbling and there's the data wall, right?
[03:03:14.640 --> 03:03:18.320]   I think we should talk about the data wall for a moment. I think the data wall is, you know,
[03:03:18.320 --> 03:03:21.280]   even though kind of like all this stuff has been about, you know, crazy AI progress, I think the
[03:03:21.280 --> 03:03:24.240]   data wall is actually sort of underrated. I think there's like a real scenario where we're just
[03:03:24.240 --> 03:03:28.320]   stagnant. Yeah. You know, because we've been running this tailwind of just like, it's really
[03:03:28.320 --> 03:03:31.840]   easy to bootstrap and you just do unsupervised learning, next token prediction, it learns these
[03:03:31.840 --> 03:03:36.000]   amazing world models, like, bam, you know, great model. And you just got to buy some more compute,
[03:03:36.000 --> 03:03:40.720]   you know, do some simple efficiency changes, you know, and, and again, like so much of deep
[03:03:40.720 --> 03:03:44.160]   learning, all these like big gains on efficiency have been like pretty dumb things, right? Like,
[03:03:44.160 --> 03:03:48.480]   you know, you add a normalization layer, you know, you know, you fix the scaling laws, you know,
[03:03:48.480 --> 03:03:52.480]   and these already have been huge things, let alone kind of like obvious ways in which these models
[03:03:52.480 --> 03:03:58.640]   aren't good yet. Anyway, so data wall, big deal. You know, I don't know, some like put some numbers
[03:03:58.640 --> 03:04:03.280]   on this, you know, some like you do common crawl, you know, online is like, you know, 30 trillion
[03:04:03.280 --> 03:04:07.920]   tokens on the three trained on 15 trillion tokens. So you're basically already using all the data.
[03:04:07.920 --> 03:04:11.600]   And then, you know, you can get somewhat further by repeating it. So there's an academic paper by,
[03:04:11.600 --> 03:04:16.320]   you know, Boaz Barak and some others that does scaling laws for this. And they're basically
[03:04:16.320 --> 03:04:20.560]   like, yeah, you can repeat it sometime after 16 times of reputation, it's just like returns
[03:04:20.560 --> 03:04:24.000]   basically go to zero, you're just completely screwed. And so I don't know, say you can get
[03:04:24.000 --> 03:04:28.320]   another 10x on data from repertory, say like Lama three and GP four, you know, Lama three is already
[03:04:28.320 --> 03:04:32.560]   kind of like at the limit of all the data, you know, maybe we can get 10x more by repeating data.
[03:04:33.040 --> 03:04:37.600]   Um, you know, I don't know, maybe that's like at most 100x better model than GP four,
[03:04:37.600 --> 03:04:41.280]   which is like, you know, 100x effective compute from GP four is, you know, not that much, you
[03:04:41.280 --> 03:04:44.800]   know, if you do half an order of magnitude a year of compute half an order of magnitude of algorithmic
[03:04:44.800 --> 03:04:49.360]   progress, you know, that's kind of like two years from GP four. So, you know, GP four finished
[03:04:49.360 --> 03:04:55.840]   pre training in 22, you know, 24. So I think one thing that really matters, I think we won't quite
[03:04:55.840 --> 03:05:00.480]   know by end of the year, but you know, 2526 Are we cracking the data wall?
[03:05:00.480 --> 03:05:08.800]   Okay, so suppose we had three orders of magnitude less data in common crawl on the internet than we
[03:05:08.800 --> 03:05:14.720]   just happen to have. Yeah, now, and for decades, yeah, the internet other things we've been rapidly
[03:05:14.720 --> 03:05:21.200]   increasing the stock of data that humanity has. Yeah. Is it your view that for contingent reasons,
[03:05:21.200 --> 03:05:28.560]   we just happen to have enough data? Yeah, to train models that are just powerful enough yet 4.5 level,
[03:05:28.560 --> 03:05:36.080]   where they can kick off the self play RL loop? Yeah. Or is it just that we, you know, if it had
[03:05:36.080 --> 03:05:40.560]   been three rooms higher, yeah, then it probably would have been slightly faster. Yeah. In that
[03:05:40.560 --> 03:05:43.520]   world, we would have been looking back at like, oh, how hard it would have been to like, kick off
[03:05:43.520 --> 03:05:48.000]   the RL explosion with just 4.5, we would have figured it out. And then so in this world, we
[03:05:48.000 --> 03:05:52.080]   would have gotten to GP three, and then we'd have to kick off some sort of RL explosion. Yeah, but
[03:05:52.080 --> 03:05:55.920]   we would have we would have still figured it out the sort of the we didn't just like luck out on
[03:05:55.920 --> 03:05:58.960]   the amount of data we happen to have in the world. I mean, three rooms is pretty rough, right? Like
[03:05:58.960 --> 03:06:03.360]   three rooms, if less data means like six rooms, smaller, six rooms, less compute model until
[03:06:03.360 --> 03:06:07.760]   scaling laws, you know, that's, it's basically capping out at like GP two. But I think that would
[03:06:07.760 --> 03:06:12.880]   be really rough. I think you do make an interesting point about the contingency. You know, I guess
[03:06:12.880 --> 03:06:16.400]   earlier, we're talking about this sort of like, when in the sort of human trajectory, are you
[03:06:16.400 --> 03:06:20.720]   able to learn from yourself? And so, you know, if we go with that analogy, again, like if you'd only
[03:06:20.720 --> 03:06:24.160]   gotten the preschooler model, it can't learn from itself. If you'd only gotten the elementary
[03:06:24.160 --> 03:06:28.240]   school or model can't learn from itself. And, you know, maybe GP four, you know, smart high school
[03:06:28.240 --> 03:06:31.680]   is really where it starts. Ideally, you have a somewhat better model, but then it really is able
[03:06:31.680 --> 03:06:36.640]   to kind of like learn from itself, or learn by itself. So I think there's, I think, I mean,
[03:06:36.640 --> 03:06:42.480]   I think maybe one room less data, I would be like, more iffy, but maybe still doable. Yeah, I think
[03:06:42.480 --> 03:06:46.080]   it would feel chill or if we had, you know, like one or two, it would be an interesting exercise to
[03:06:46.080 --> 03:06:52.720]   get probably distributions of AGI contingent on. Yeah. Data. Yeah. Okay. I, the thing that makes
[03:06:52.720 --> 03:06:58.320]   me skeptical of this story is that the things it totally makes sense, right? Free training works so
[03:06:58.320 --> 03:07:04.960]   well. Yeah. These other things, their stories of in principle, why they ought to work like a humans
[03:07:04.960 --> 03:07:10.080]   can learn this way and so on. Yes. And maybe they're true. But I worry that a lot of this
[03:07:10.080 --> 03:07:16.720]   case is based on sort of first principles, evaluation of how learning happens, that
[03:07:16.720 --> 03:07:20.320]   fundamentally, we don't understand how humans learn. And maybe there's some key thing we're
[03:07:20.320 --> 03:07:26.400]   missing. Yeah. On the sort of sample efficiency. Yeah. Humans actually, maybe there's you say,
[03:07:26.400 --> 03:07:30.080]   well, the fact that these things are way less sample efficient in terms of learning than humans
[03:07:30.080 --> 03:07:34.880]   are suggested, there's a lot of room for improvement. Yeah. Another perspective is that
[03:07:34.880 --> 03:07:38.800]   we are just on the wrong path altogether, right? That's why there's a sample inefficient when it
[03:07:38.800 --> 03:07:45.360]   comes to pre-training. Yeah. So yeah, I'm just like, there's a lot of like first principles
[03:07:45.360 --> 03:07:49.120]   arguments stacked on top of each other where you get these unhoplings and then you get AGI. Yeah.
[03:07:49.120 --> 03:07:53.440]   Then you, because of these reasons why you can stack all these things on top of each other,
[03:07:53.440 --> 03:07:57.120]   you get to ASI. Yeah. And I'm worried that there's too many steps of this. Yeah. Sort of
[03:07:57.120 --> 03:08:04.320]   first principles thinking. I mean, we'll see, right? I mean, on the sort of sample efficiency
[03:08:04.320 --> 03:08:08.640]   thing, again, sort of first principles, but I think again, there's this clear sort of missing
[03:08:08.640 --> 03:08:14.960]   middle. And so, and sort of like, people hadn't been trying, now people are really trying.
[03:08:14.960 --> 03:08:19.840]   And so it's sort of, I think often again, in deep learning, something like the obvious thing works
[03:08:19.840 --> 03:08:23.520]   and there's a lot of details to get right. So it might take some time, but it's now
[03:08:23.520 --> 03:08:26.080]   where people are really trying. So I think we get a lot of signal in the next couple of years.
[03:08:31.440 --> 03:08:34.640]   On a hobbling, I mean, what is the signal on hobbling that I think would be interesting?
[03:08:34.640 --> 03:08:38.240]   I think, I think the question is basically like, are you making progress on this test time compute
[03:08:38.240 --> 03:08:42.160]   thing, right? Like, is this thing able to think longer horizon than just a couple hundred tokens,
[03:08:42.160 --> 03:08:46.800]   right? That was unlocked by chain of thought. And on that point in particular, the many people
[03:08:46.800 --> 03:08:51.360]   who have longer timelines have come on the podcast, have made the point that the way
[03:08:51.360 --> 03:08:56.240]   to train this long horizon RL, it's not, I mean, earlier talking about like, well,
[03:08:56.240 --> 03:09:01.280]   they can think for five minutes, but not for longer. But it's not because they can't physically
[03:09:01.280 --> 03:09:05.600]   output an hour's worth of tokens. It's just really, at least from what I understand what they say.
[03:09:05.600 --> 03:09:09.040]   Right. Like even like Gemini has like a million in context and the million of context is actually
[03:09:09.040 --> 03:09:12.800]   great for consumption. And it solves one important on hobbling, which is the sort of onboarding
[03:09:12.800 --> 03:09:18.000]   problem, right? Which is, you know, a new coworker, you know, in your first five minutes,
[03:09:18.000 --> 03:09:21.920]   like a new smart high school intern, first five minutes, not useful at all. A month in, you know,
[03:09:21.920 --> 03:09:25.760]   much more useful, right? Because they've like looked at the monorepo and understand how the
[03:09:25.760 --> 03:09:29.520]   code works and they've read your internal docs. And so being able to put that in context, great,
[03:09:29.520 --> 03:09:32.880]   solves this onboarding problem. Yeah. But they're not good at sort of the production
[03:09:32.880 --> 03:09:36.320]   of a million tokens yet. Yeah. Right. But on the production of a million tokens,
[03:09:36.320 --> 03:09:43.520]   there's no public evidence that there's some easy loss function where you can...
[03:09:43.520 --> 03:09:48.160]   GPT-4 has gotten a lot better since it's actually so the GPT-4 gains since launch,
[03:09:48.160 --> 03:09:51.840]   I think, are a huge indicator that there's like, you know, so you talked about this with John
[03:09:51.840 --> 03:09:55.920]   on the podcast. John said this was mostly post-training gains. You know, if you look at
[03:09:55.920 --> 03:10:00.880]   the sort of LMSIS scores, you know, it's like 100 Elo or something. It's like a bigger gap
[03:10:00.880 --> 03:10:05.120]   than between Cloud 3 Opus and Cloud 3 Haiku. And the price difference between those is 60x.
[03:10:05.120 --> 03:10:08.800]   But it's not more agentic. It's like better in the same chatbot way.
[03:10:08.800 --> 03:10:12.080]   Math, right? Like, you know, it went from like, you know, 40% to 70% math.
[03:10:12.080 --> 03:10:14.000]   The crux is like whether like you could be able to like...
[03:10:14.000 --> 03:10:19.360]   No, but I think it indicates that clearly there's stuff to be done on Hobling. I think, yeah, I think
[03:10:19.360 --> 03:10:23.360]   the interesting question is like this time a year from now, you know, is there a model that is able
[03:10:23.360 --> 03:10:27.760]   to think for like, you know, a few thousand tokens coherently, cohesively, agentically?
[03:10:27.760 --> 03:10:31.680]   And I think probably there's, you know, again, this is where I'd feel better if we had an
[03:10:31.680 --> 03:10:35.760]   oom or two more data because it's like the scaling just gives you this sort of like tailwind, right?
[03:10:35.760 --> 03:10:39.840]   We're like, for example, tools, right? Tools, I think, you know, talking to people who try to
[03:10:39.840 --> 03:10:44.480]   make things work with tools, you know, actually sort of GP4 is really when tools start to work.
[03:10:44.480 --> 03:10:47.920]   And it's like, you can kind of make them work with GP3.5, but it's just really tough.
[03:10:47.920 --> 03:10:53.200]   And so it's just like having GP4, you can kind of help it learn tools in a much easier way.
[03:10:53.200 --> 03:10:58.640]   And so there's a bit more tailwind from scaling. And then, yeah, and does, I don't know if it'll
[03:10:58.640 --> 03:11:03.840]   work, but it's a key question. Okay. I think that's a good place to sort of close that part
[03:11:03.840 --> 03:11:09.040]   where we know what the crux is and what the progress, what evidence of that would look like
[03:11:09.040 --> 03:11:15.280]   on the AGI to super intelligence. Maybe it's a case that the games are really easy right now,
[03:11:15.280 --> 03:11:18.720]   and you can just sort of let loose an Alec Radford, give him a compute budget,
[03:11:18.720 --> 03:11:24.640]   and he comes out the other end with something that is an additive, like change this part of
[03:11:24.640 --> 03:11:29.920]   the code. This is a compute multiplier, change this other part. What other parts of the world?
[03:11:29.920 --> 03:11:35.600]   Okay. Maybe here's an interesting way to ask this. How many other domains in the world
[03:11:35.600 --> 03:11:42.640]   are like this where you think you could get the equivalent of in one year, you just throw enough
[03:11:42.640 --> 03:11:49.040]   intelligence across multiple instances and you would just come out the other end with something
[03:11:49.040 --> 03:11:56.320]   that is remarkably decades, centuries ahead. Like you start off with no flight and then you're the
[03:11:56.320 --> 03:12:02.320]   Wright brothers, a million instances of GPT-6 and you come out the other end with Starlink.
[03:12:02.320 --> 03:12:08.000]   Is that your model of how things work? I think you're exaggerating the timelines a little bit,
[03:12:08.000 --> 03:12:11.760]   but I think a decade's worth of progress in a year or something. I think that's a reasonable
[03:12:11.760 --> 03:12:18.480]   prompt. So I think this is where basically the automated AI researcher comes in because it gives
[03:12:18.480 --> 03:12:23.600]   you this enormous tail headwind on all the other stuff. So it's like you automate AI research with
[03:12:23.600 --> 03:12:27.440]   your automated Alec Radfords, you come out the other end, you've done another five booms, you
[03:12:27.440 --> 03:12:32.880]   have a thing that is vastly smarter. Not only is it vastly smarter, you've been able to make it
[03:12:32.880 --> 03:12:37.440]   good at everything else. You're solving robotics. The robots are important because for a lot of
[03:12:37.440 --> 03:12:41.680]   other things, you do actually need to try things in the physical world. I don't know, maybe you can
[03:12:41.680 --> 03:12:45.360]   do a lot in simulation. Those are the really quick worlds. I don't know if you saw the last NVIDIA
[03:12:45.360 --> 03:12:49.760]   GTC. It was all about the digital twins and just having all your manufacturing processes in
[03:12:49.760 --> 03:12:55.520]   simulation. I don't know. Again, if you have these super intelligent cognitive workers, can they just
[03:12:55.520 --> 03:13:00.960]   make simulations of everything, kind of off-the-float style and then make a lot of progress
[03:13:00.960 --> 03:13:06.960]   in simulation possible? But I also just think you're going to get the robots. Again, I agree
[03:13:06.960 --> 03:13:12.240]   about there are a lot of real world bottlenecks. And so, I don't know, it's quite possible that
[03:13:12.240 --> 03:13:16.640]   we're going to have crazy drones forms, but also lawyers and doctors still need to be humans
[03:13:16.640 --> 03:13:23.600]   because of regulation. But I think you kind of start narrowly, you broaden, and then the worlds
[03:13:23.600 --> 03:13:26.880]   in which you kind of let them loose, which again, because of these competitive pressures, we will
[03:13:26.880 --> 03:13:34.160]   have to let them loose in some degree on various national security applications. I think quite
[03:13:34.160 --> 03:13:38.400]   rapid progress is possible. The other thing though, is it's sort of, basically in the sort
[03:13:38.400 --> 03:13:41.520]   of an explosion after, there's kind of two components. There's the A, right in the production
[03:13:41.520 --> 03:13:45.280]   function, like growth of technology. And that's massively accelerated by you. Now you have a
[03:13:45.280 --> 03:13:50.160]   billion super intelligent scientists and engineers and technicians, superbly competent and everything.
[03:13:50.160 --> 03:13:54.560]   You also just automated labor, right? And so it's like, even without the whole technological
[03:13:54.560 --> 03:13:58.240]   explosion thing, you have this industrial explosion, at least if you let them loose,
[03:13:58.240 --> 03:14:02.240]   which is like, now you can just build, you can cover Nevada and you start with one robot
[03:14:02.240 --> 03:14:06.240]   factory that's producing more robots. And basically this like, just the cumulative process
[03:14:06.240 --> 03:14:13.520]   because you've taken labor out of the equation. Yeah, that's super interesting. Although when
[03:14:13.520 --> 03:14:21.120]   you increase the K or the L without increasing the A, you can look at the Soviet Union or China,
[03:14:21.120 --> 03:14:27.120]   where they rapidly increased inputs. And that does have the effect of being geopolitically
[03:14:27.120 --> 03:14:32.480]   game-changing where you, it is remarkable. Like you go to Shanghai over a sequence of decades.
[03:14:32.480 --> 03:14:35.520]   These crazy cities in a decade. Right, right. And that's, I mean, the closest thing to like,
[03:14:35.520 --> 03:14:40.000]   people talk about 30% growth rates or whatever from AI. Asian tigers, 10%. It's totally possible.
[03:14:40.000 --> 03:14:46.080]   But without productivity gains, it's not like the industrial revolution, where like you're,
[03:14:46.080 --> 03:14:49.680]   from the perspective of, you're looking at a system from the outside, your goods have gotten
[03:14:49.680 --> 03:14:55.360]   cheaper. They can manufacture more things, but you know, it's not like the next century is coming at
[03:14:55.360 --> 03:14:58.160]   you. Yeah, it's both. It's both. So it's, you know, both that are important. The other thing
[03:14:58.160 --> 03:15:02.320]   I'll say is like, all of this stuff, I think the magnitudes are really, really important, right?
[03:15:02.320 --> 03:15:08.240]   So, you know, we talked about a 10X of research effort or maybe 10, 30X over a decade, you know,
[03:15:08.240 --> 03:15:12.320]   even without any kind of like self-improvement type loop, you know, we talk, the sort of,
[03:15:12.320 --> 03:15:16.320]   even in the sort of GP4 to AGI story, we're talking about an order of magnitude of effective
[03:15:16.320 --> 03:15:19.920]   compute increase a year, right? Half an order of magnitude of compute, half an order of magnitude
[03:15:19.920 --> 03:15:25.440]   of algorithmic progress that sort of translates into effective compute. And so you're doing a 10X
[03:15:25.440 --> 03:15:29.360]   a year, right? Basically on your labor force, right? So it's like, it's a radically different
[03:15:29.360 --> 03:15:34.000]   world if you're doing a 10X or 30X in a century versus a 10X a year on your labor force. So the
[03:15:34.000 --> 03:15:37.600]   magnitudes really matter. They also really matter on the sort of intelligence explosion, right? So
[03:15:37.600 --> 03:15:41.680]   like just the automated AI research part. So, you know, one story you could tell there is like,
[03:15:41.680 --> 03:15:45.200]   well, ideas get harder to find, right? Algorithmic progress is going to get harder. Yeah,
[03:15:45.200 --> 03:15:48.720]   right now you have the easy wins, but in like four or five years, there'll be fewer easy wins.
[03:15:48.720 --> 03:15:52.480]   And so the sort of automated AI researchers are just going to be what's necessary to just keep
[03:15:52.480 --> 03:15:56.000]   it going, right? Because it's gotten harder. But that's sort of, it's like a really weird knife
[03:15:56.000 --> 03:15:59.120]   edge assumption economics where you assume it's just enough. But isn't that the equilibrium story
[03:15:59.120 --> 03:16:04.560]   you were just telling with why the economy as a whole has 2% economic growth? Because you just
[03:16:04.560 --> 03:16:08.000]   proceed on the equal, I guess you're saying by the time you get to the equilibrium here is it's
[03:16:08.000 --> 03:16:11.680]   like way faster, at least, you know, and it's at least, and it depends on the sort of exponents,
[03:16:11.680 --> 03:16:15.520]   but it's basically, it's the increases, like, suppose you need to like 10X effective research
[03:16:15.520 --> 03:16:19.280]   effort in AI research in the last, you know, four or five years to keep the pace of progress.
[03:16:19.280 --> 03:16:22.800]   We're not just getting a 10X, you're getting, you know, a million X or 100,000X. There's just
[03:16:22.800 --> 03:16:27.040]   the magnitudes really matter. And the magnitude is just basically, you know, one way to think
[03:16:27.040 --> 03:16:30.000]   about this is that you have kind of two exponentials. You have your sort of like
[03:16:30.000 --> 03:16:34.320]   normal economy that's growing at, you know, 2% a year, and you have your like AI economy,
[03:16:34.320 --> 03:16:38.560]   and that's going at like 10X a year. And it's starting out really small, but sort of eventually
[03:16:38.560 --> 03:16:42.800]   it's going to, it's just, it's way faster and eventually it's going to overtake, right? And
[03:16:42.800 --> 03:16:46.560]   even if you have, you can almost sort of just do the simple revenue extrapolation, right? If you
[03:16:46.560 --> 03:16:50.400]   think your AI economy, you know, that has some growth rate, I mean, it's a very simplistic way
[03:16:50.400 --> 03:16:54.960]   and so on, but there's this sort of 10X a year process. And that will eventually kind of like,
[03:16:54.960 --> 03:16:59.120]   you're going to transition the sort of whole economy from, as it broadens from the sort of,
[03:16:59.120 --> 03:17:04.240]   you know, 2% a year to the sort of much faster growing process. And I don't know, I think that's
[03:17:04.240 --> 03:17:09.360]   very like consistent with historical chain, you know, stories of, right, there's this sort of
[03:17:09.360 --> 03:17:13.920]   like, you know, there's this sort of long run hyperbolic trend, you know, it manifested in
[03:17:13.920 --> 03:17:18.240]   the sort of like, sort of change in growth mode in the Austral, you know, revolution, but there's
[03:17:18.240 --> 03:17:22.720]   just this long run hyperbolic trend. And, you know, now you have this sort of, now you have that
[03:17:22.720 --> 03:17:25.840]   another sort of change in growth mode. Yeah, yeah. I mean, that was one of the questions I asked
[03:17:25.840 --> 03:17:32.560]   Tyler. Yeah. When I had him on the podcast is that you do go from the fact that after 1776,
[03:17:32.560 --> 03:17:37.600]   you go from a regime of negligible economic growth to percent. Yeah. It's really interesting. It
[03:17:37.600 --> 03:17:42.160]   shows that, I mean, from the perspective of somebody in the Middle Ages or before. Yeah.
[03:17:42.160 --> 03:17:47.440]   2% is equivalent to the sort of 10%. Yeah. I guess you're projecting even higher for the economy.
[03:17:47.440 --> 03:17:51.200]   But I mean, I think, again, and it's all this stuff, you know, I have a lot of uncertainty,
[03:17:51.200 --> 03:17:54.480]   right? So a lot of the time, I'm trying to kind of tell the modal story. I think it's important
[03:17:54.480 --> 03:17:58.800]   to be kind of concrete and visceral. Sure. And I, you know, I have, I have a lot of uncertainty,
[03:17:58.800 --> 03:18:02.720]   basically, over how the 2030s play out. And basically, the thing I know is it's going to be
[03:18:02.720 --> 03:18:07.840]   fucking crazy. But, you know, exactly what, you know, where the bottlenecks are, and so on. I
[03:18:07.840 --> 03:18:13.120]   think that will be kind of like, so let's talk through the numbers here, you hundreds of millions
[03:18:13.120 --> 03:18:22.160]   of AI researchers. So right now, GPD 4.0 Turbo is like 15 bucks for a million tokens outputted,
[03:18:22.160 --> 03:18:27.280]   and a human thinks 150 tokens a minute, or something. And if you do the math on that,
[03:18:27.280 --> 03:18:36.000]   I think it's for an hour's worth of human output, you it's like 10 cents or something. Now,
[03:18:36.000 --> 03:18:40.720]   cheaper than a human worker, cheaper than a human. Oh, yeah, I can't do the job. That's right. That's
[03:18:40.720 --> 03:18:45.120]   right. But by the time you're talking about models that are trained on the 10 gigawatt cluster,
[03:18:45.120 --> 03:18:50.800]   yeah, then you have something that is four orders of magnitude, more expensive via inference,
[03:18:50.800 --> 03:18:56.000]   three orders of magnitude, something like that. So that's like $100 an hour of labor. And now
[03:18:56.000 --> 03:19:01.920]   you're having hundreds of millions of such laborers. Is there enough compute to do with
[03:19:01.920 --> 03:19:07.840]   the model that is 1000 times bigger, this kind of labor? Great. Okay, great question. So I actually
[03:19:07.840 --> 03:19:11.840]   don't think inference costs for sort of frontier models are necessarily going to go up that much.
[03:19:11.840 --> 03:19:15.760]   So I mean, one historical data point, but isn't the test time sort of thing that it will go up
[03:19:15.760 --> 03:19:19.120]   even higher? I mean, we're just doing per token, right? And then I'm just saying, you know, if
[03:19:19.120 --> 03:19:23.760]   suppose each model token was the same as sort of a human thing at 100 tokens a minute. So it's like,
[03:19:23.760 --> 03:19:29.440]   yeah, it'll use more, but the token calculations is already pricing that in. The question is like
[03:19:29.440 --> 03:19:35.600]   per token pricing, right? And so like GPT-3 when it launched was like actually more expensive
[03:19:35.600 --> 03:19:41.600]   than GPT-4 now. And so over just like fast increases in capability gains, inference costs
[03:19:41.600 --> 03:19:47.200]   remain constant. That's sort of wild. I think it's worth appreciating. And I think it gestures
[03:19:47.200 --> 03:19:52.000]   that sort of an underlying pace of algorithmic progress. I think there's a sort of like more
[03:19:52.000 --> 03:19:56.560]   theoretically grounded way to why inference costs would stay constant. And it's the following story,
[03:19:56.560 --> 03:20:02.080]   right? So on Chacheli scaling laws, right? Half of the additional compute you allocate to bigger
[03:20:02.080 --> 03:20:07.120]   models and half of it you allocate to more data, right? But also if we go with the sort of basic
[03:20:07.120 --> 03:20:10.720]   story of half an order of year more compute and half an order of magnitude a year of algorithmic
[03:20:10.720 --> 03:20:14.800]   progress, you're also kind of like you're saving half an order of magnitude a year. And so that
[03:20:14.800 --> 03:20:18.800]   kind of would exactly compensate for making the model bigger. The caveat on that is, you know,
[03:20:18.800 --> 03:20:22.640]   obviously not all training efficiencies are also inference efficiencies, you know, a bunch of the
[03:20:22.640 --> 03:20:27.040]   time they are separately, you can find inference efficiencies. So I don't know, given this historical
[03:20:27.040 --> 03:20:33.840]   trend, given the sort of like, you know, baseline sort of theoretical reason, you know, I don't know,
[03:20:33.840 --> 03:20:37.840]   I think it's not crazy baseline assumption that actually these models, the frontier models are
[03:20:37.840 --> 03:20:43.760]   not necessarily going to get more expensive per token. Oh, really? Yeah. Like, okay, that's wild.
[03:20:43.760 --> 03:20:48.000]   We'll see. We'll see. I mean, the other thing, you know, even if they get like 10x more expensive,
[03:20:48.000 --> 03:20:51.680]   then you know, you have 10 million instead of 100 million, you know, so it's like, it's not really,
[03:20:51.680 --> 03:20:55.120]   you know, like, but okay, so part of the intelligence solution is that each of them
[03:20:55.120 --> 03:21:03.440]   has to run experiments that are gbd for size, and the result. So that takes up a bunch of compute.
[03:21:03.440 --> 03:21:07.840]   Yes, they need to consolidate the results of experiments. And what is the synthesized?
[03:21:07.840 --> 03:21:12.240]   I mean, you have a much bigger influence street anyway, than your training. Sure. Okay. But I
[03:21:12.240 --> 03:21:17.280]   think the experiment compute is a constraint. Yeah. Okay. I'm going back to maybe a sort of
[03:21:17.280 --> 03:21:24.240]   bigger fundamental thing we're talking about here. We're projecting in a series, you say,
[03:21:24.240 --> 03:21:30.320]   we should denominate the probability of getting to AGI in terms of orders of magnitude of effective
[03:21:30.320 --> 03:21:35.680]   compute effective here, accounting for the fact that yeah, there's a compute, quote, unquote,
[03:21:35.680 --> 03:21:42.880]   compute multiplier, if you have better algorithms. Yes. And I'm not sure that it makes sense to be
[03:21:42.880 --> 03:21:49.200]   confident that this is a sensible way to project progress, it might be, but I'm just like, I have a
[03:21:49.200 --> 03:21:54.560]   lot of uncertainty about it. It seems similar to somebody trying to project when we're going to
[03:21:54.560 --> 03:21:58.720]   get to the moon. And they're like looking at the Apollo program in the 450s or something. And they're
[03:21:58.720 --> 03:22:06.720]   like, we have some amount of effective jet fuel. And if we get more efficient engines, then we have
[03:22:06.720 --> 03:22:12.000]   more effective jet fuel. And so we're going to like probability of getting to the moon based on
[03:22:12.000 --> 03:22:16.560]   the amount of effective jet fuel we have. And I don't deny that jet fuel is important to launch
[03:22:16.560 --> 03:22:20.880]   rockets, but that seems like an odd way to denominate when you're going to get to the moon.
[03:22:20.880 --> 03:22:26.080]   Yeah. So I think these cases are pretty different. I don't know. I don't think there's a sort of
[03:22:26.080 --> 03:22:29.840]   clear, I don't know how rocket science works, but I didn't get the impression that there's
[03:22:29.840 --> 03:22:36.960]   some clear scaling behavior with the amount of jet fuel. I think in AI, I mean, first of all,
[03:22:36.960 --> 03:22:41.440]   the scaling laws, they've just helped. And so a friend of mine pointed this out, and I think it's
[03:22:41.440 --> 03:22:45.760]   a great point. If you kind of concatenate both the sort of original Kaplan scaling laws paper that I
[03:22:45.760 --> 03:22:51.280]   think went from 10 to the negative nine to 10 petaflop days, and then concatenate additional
[03:22:51.280 --> 03:22:56.800]   compute from there to kind of GP4, you assume some algorithmic progress. It's like the scaling laws
[03:22:56.800 --> 03:23:01.040]   have held probably over 15 ooms. I know it was rough calculation, probably maybe even more,
[03:23:01.040 --> 03:23:04.480]   held for a lot of ooms. They held for the specific loss function,
[03:23:04.480 --> 03:23:11.200]   which they're trained on, which is a training mix token. Whereas the progress you are forecasting
[03:23:11.200 --> 03:23:16.160]   will be required for further progress in capabilities. Specifically, we know that
[03:23:16.160 --> 03:23:19.920]   scaling can't work because of the data wall. And so there's some new thing that has to happen.
[03:23:19.920 --> 03:23:25.280]   And I'm not sure whether you can extrapolate that same scaling curve to tell us whether
[03:23:25.280 --> 03:23:28.080]   these hobblings will also, like, is this not on the same graph?
[03:23:28.080 --> 03:23:30.000]   The hobblings are just a separate thing. Yeah, exactly.
[03:23:30.000 --> 03:23:34.720]   So this is sort of like, you know, it's, yeah. So, I mean, a few things here, right? Okay. So
[03:23:34.720 --> 03:23:40.480]   on the effect of compute scaling, you know, in some sense, I think it's like people center the
[03:23:40.480 --> 03:23:44.000]   scaling laws because they're easy to explain and the sort of like, why, why is scaling matter?
[03:23:44.000 --> 03:23:48.720]   The scaling laws like came way after people, at least, you know, like Dario, Ilya realized
[03:23:48.720 --> 03:23:51.840]   that scaling mattered. And I think, you know, I think that almost more important than the sort
[03:23:51.840 --> 03:23:55.920]   of loss curve is just like, just in general, make, you know, there's this great quote from
[03:23:55.920 --> 03:24:00.800]   Dario on your podcast. It's just like, you know, Ilya was like, "models, they just want to learn,
[03:24:00.800 --> 03:24:04.800]   you know, you make them bigger, they learn more." And that just applied just across domains,
[03:24:04.800 --> 03:24:09.520]   generally, you know, all the capabilities. And so, and you can look at this in benchmarks.
[03:24:09.520 --> 03:24:13.840]   Again, like you say, headwind, data wall, and I'm sort of bracketing that and talking about that
[03:24:13.840 --> 03:24:17.920]   separately. The other thing is on hobblings, right? If you just put them on the effect of
[03:24:17.920 --> 03:24:21.360]   compute graph, these on hobblings would be kind of huge, right? So like, I think-
[03:24:21.360 --> 03:24:24.000]   What does it even mean? Like, what is it, what is on the y-axis here?
[03:24:24.000 --> 03:24:29.280]   Like, say MLPR on this benchmark or whatever, right? And so, you know, like, you know, we
[03:24:29.280 --> 03:24:33.040]   mentioned the sort of, you know, the LM sys differences, you know, RLHF, you know, again,
[03:24:33.040 --> 03:24:36.720]   as good as 100x more chain of thought, right? Chain of just going from this prompting change,
[03:24:36.720 --> 03:24:41.600]   a simple algorithmic change can be like 10x effective compute increases on like math benchmarks.
[03:24:41.600 --> 03:24:45.760]   I think this is like, you know, I think this is useful to illustrate that on hobblings are large,
[03:24:45.760 --> 03:24:49.040]   but I think they're like, I kind of think of them as like slightly separate things.
[03:24:49.040 --> 03:24:53.360]   And the kind of the way I think about is that like, at a per token level, I think GP4 is not
[03:24:53.360 --> 03:24:58.560]   that far away from like a token of my internal monologue, right? Even like 3.5 to 4 took us
[03:24:58.560 --> 03:25:02.400]   kind of from like the bottom of the human range to the top of the human range on like a lot of,
[03:25:02.400 --> 03:25:06.400]   you know, on a lot of, you know, kind of like high school tests. And so it's like a few more
[03:25:06.400 --> 03:25:11.040]   3.5 to 4 jumps per token basis, like per token intelligence. And then you've got to unlock the
[03:25:11.040 --> 03:25:15.520]   test time, you've got to solve the onboarding problem, make it use a computer. And then you're
[03:25:15.520 --> 03:25:22.080]   getting real close. I'm reminded of, again, the story might be wrong, right? It is strikingly
[03:25:22.080 --> 03:25:25.840]   plausible. I agree. And so I think actually, I mean, the other thing I'll say is like,
[03:25:25.840 --> 03:25:30.560]   you know, I say this 2027 timeline, I think is unlikely, but I do think there's worlds that are
[03:25:30.560 --> 03:25:34.800]   like AGI next year. And that's basically if the test and compute overhang is really easy to crack,
[03:25:34.800 --> 03:25:38.400]   if it's really easy to crack, then you do like four rooms of test and compute, you know,
[03:25:38.400 --> 03:25:42.320]   from a few hundred tokens to a few million tokens, you know, quickly. And then, you know, again,
[03:25:42.320 --> 03:25:46.720]   maybe it's maybe only takes one or two, 3.5 to four jumps per token, like one or two of those
[03:25:46.720 --> 03:25:51.120]   jumps for token plus uses test and compute. And you basically have the proto automated engineer.
[03:25:51.120 --> 03:25:59.920]   So I'm reminded of Steven Pinker releases his book on what is it the better angels of our
[03:25:59.920 --> 03:26:04.560]   nature. And it's like a couple years ago or something. And he says the secular decline in
[03:26:04.560 --> 03:26:10.480]   violence and war and everything. And you can just like plot the line from the end of World War Two.
[03:26:10.480 --> 03:26:14.480]   In fact, before World War Two, then these are just aberrations, whatever. And basically,
[03:26:14.480 --> 03:26:21.280]   as soon as it happens, Ukraine, Gaza, the everything is like, so basically the ASI
[03:26:21.280 --> 03:26:29.920]   and crazy global conflict. I think this is the sort of thing that happens in history where you
[03:26:29.920 --> 03:26:33.920]   see a straight line and you're like, Oh my gosh, and then just like, as soon as you make that
[03:26:33.920 --> 03:26:38.320]   prediction, yeah, who was that famous author? So yeah, this, you know, again, people are predicting
[03:26:38.320 --> 03:26:41.840]   deep learning will hit a wall every year, right? Maybe one year, they're right, but it's like,
[03:26:41.840 --> 03:26:46.560]   gone a long way. And it hasn't hit a wall. You don't have that much more to go. And you know,
[03:26:46.560 --> 03:26:50.240]   so yeah, I guess I think this is a sort of plausible story. And let's just run with it.
[03:26:50.880 --> 03:26:57.520]   And see what it implies. So we were talking in your series, you talk about alignment from
[03:26:57.520 --> 03:27:06.080]   the perspective of this is not about some doomer scheme to get the 0.01% probability distribution
[03:27:06.080 --> 03:27:10.160]   where things don't go off the rails. It's more about just controlling the systems,
[03:27:10.160 --> 03:27:15.920]   making sure they do what we intend them to do. If that's the case, and we're going to be in this
[03:27:15.920 --> 03:27:22.720]   sort of geopolitical conflict with China, and part of that will involve and what we're worried about
[03:27:22.720 --> 03:27:28.880]   is them making the CCP boss that go out and take the red flag of Mao across the galaxies or
[03:27:28.880 --> 03:27:36.640]   something, then shouldn't we be worried about alignment as something that if you're in the
[03:27:36.640 --> 03:27:42.560]   wrong hands, this is the thing that enables brainwashing, sort of dictatorial control.
[03:27:43.600 --> 03:27:47.280]   This seems like a worrying thing. This should be part of the sort of algorithmic secrets we
[03:27:47.280 --> 03:27:51.360]   keep hidden, right? How to align these models, because that's also something the CCP can use
[03:27:51.360 --> 03:27:54.880]   to control their models. I mean, I think in the world where you get the democratic coalition,
[03:27:54.880 --> 03:27:58.000]   yeah. I mean, also just alignment is often dual use, right? Like Arleigh Chaffee, you know, it's
[03:27:58.000 --> 03:28:01.760]   like alignment team developed. It was great. You know, it was a big win for alignment, but it's
[03:28:01.760 --> 03:28:07.520]   also, you know, obviously makes these models useful. But yeah, so yeah, alignment enables
[03:28:07.520 --> 03:28:12.160]   the CCP bots. Alignment also is what you need to get the, you know, get the sort of, you know,
[03:28:12.160 --> 03:28:16.880]   whatever US AI's like follow the constitution and like disobey law, you know, unlawful orders
[03:28:16.880 --> 03:28:20.400]   and, you know, like respect separation of powers and checks and balances.
[03:28:20.400 --> 03:28:24.640]   And so, yeah, you need alignment for whatever you want to do. It's just it's the sort of
[03:28:24.640 --> 03:28:28.640]   underlying technique. Tell me what you make of this take. I've been struggling with this a little
[03:28:28.640 --> 03:28:35.200]   bit. So fundamentally, there's many different ways the future could go. Yeah. There's one path in
[03:28:35.200 --> 03:28:40.640]   which the Eliezer type crazy AI's with the nanobots take the future and turn everything
[03:28:40.640 --> 03:28:46.240]   into grey goo or paperclips. And the more you solve alignment, the more that path of the decision
[03:28:46.240 --> 03:28:52.000]   tree is circumscribed. And then so the more you solve alignment, the more it is just different
[03:28:52.000 --> 03:28:55.520]   humans and the visions they have. And of course, we know from history that things don't turn out
[03:28:55.520 --> 03:28:59.360]   the way you expect. So it's not like you can decide the future, but it will be part of the
[03:28:59.360 --> 03:29:04.400]   beauty of it. Right. Yeah. You want exactly. Error correction. Exactly. But from the perspective of
[03:29:04.400 --> 03:29:09.120]   anybody who's looking at the system, it will be like I can control where this thing is going to
[03:29:09.120 --> 03:29:15.200]   end up. And so the more you solve alignment and the more you circumscribe the different futures
[03:29:15.200 --> 03:29:21.840]   that are the result of A.I. will, the more that accentuates the conflict between humans. Yes. And
[03:29:21.840 --> 03:29:25.680]   their visions of the future. Yeah. And so in the world where alignment is solved and the world in
[03:29:25.680 --> 03:29:29.760]   which alignment is solved is the one is the world in which you have the most sort of human conflict
[03:29:29.760 --> 03:29:34.480]   over where to take A.I. Yeah. I mean, by removing the worlds in which the A.I.'s take over, then
[03:29:34.480 --> 03:29:37.840]   like, you know, the remaining worlds are the ones where it's like the humans decide what happens.
[03:29:37.840 --> 03:29:42.320]   And then as we talked about, there's a whole lot of, yeah, a lot of worlds and how that could go.
[03:29:42.320 --> 03:29:46.640]   And I worry. So when you think about alignment and it's just controlling these things. Yeah.
[03:29:46.640 --> 03:29:55.040]   Just think a little forward. And there's worlds in which hopefully, you know, human descendants or
[03:29:55.040 --> 03:29:59.600]   some version of things in the future merge with super intelligences and they have the rules of
[03:29:59.600 --> 03:30:06.080]   their own, but they're in some sort of law and market based order. I worry about if you have
[03:30:06.080 --> 03:30:11.920]   things that are conscious and should be treated with rights. If you read about what alignment
[03:30:11.920 --> 03:30:16.160]   schemes actually are, and then you read these books about what actually happened during the
[03:30:16.160 --> 03:30:22.400]   cultural revolution, what happened when Stalin took over Russia and you have a very strong
[03:30:22.400 --> 03:30:27.840]   monitoring from different instances where one everybody's tasked with watching each other.
[03:30:27.840 --> 03:30:33.040]   You have brainwashing, you have red teaming, where you have the spy stuff you were talking
[03:30:33.040 --> 03:30:37.040]   about, where you try to convince somebody you're on like a defector and you see if they defect with
[03:30:37.040 --> 03:30:42.080]   you. And if they do, then you realize they're an enemy. And then you, and listen, I maybe I'm
[03:30:42.080 --> 03:30:48.800]   stretching the analogy too far, but the way like the ease of these alignment techniques actually
[03:30:48.800 --> 03:30:53.600]   map on to something you could have read about during like Mao's cultural revolution is a little
[03:30:53.600 --> 03:30:57.920]   bit troubling. Yeah. I mean, look, I think sentient AI is a whole nother topic. I don't know
[03:30:57.920 --> 03:31:01.680]   if we want to talk about it. I agree that like, it's going to be very important how we treat them.
[03:31:02.000 --> 03:31:06.080]   Um, you know, in terms of like what you're actually programming these systems to do again,
[03:31:06.080 --> 03:31:10.480]   it's like alignment is just, it's a technical, it's a technical problem. A technical solution
[03:31:10.480 --> 03:31:15.600]   enables the CCP bots. I mean, in some sense, I think the, um, you know, I almost feel like
[03:31:15.600 --> 03:31:18.160]   the sort of model and also about talking about checks and balances is sort of, you know,
[03:31:18.160 --> 03:31:21.680]   like the federal reserve or Supreme court justices. And there's a funny way in which
[03:31:21.680 --> 03:31:25.600]   they're kind of this like very dedicated order Supreme court justices. And it's amazing. They're
[03:31:25.600 --> 03:31:29.840]   actually quite high quality. Right. And they like really smart people. They really believe in the
[03:31:29.840 --> 03:31:33.360]   constitution. They love the constitution. They believe in their principles. They have, you know,
[03:31:33.360 --> 03:31:37.600]   these, these, these wonderful, um, you know, you know, and yeah, they have different persuasions,
[03:31:37.600 --> 03:31:41.280]   but they have sort of, I think very sincere kind of debates about what is the meaning of the
[03:31:41.280 --> 03:31:45.360]   constitution? You know, what is the best actuation of these principles? Um, you know, I guess that's
[03:31:45.360 --> 03:31:49.200]   good, you know, by the way, recommendation sort of SCOTUS or arguments is like the best podcast,
[03:31:49.200 --> 03:31:53.120]   you know, when I run out of high quality content on the internet, I mean, I think there's going to
[03:31:53.120 --> 03:31:56.160]   be a process of like figuring out what the constitution should be. I think, you know,
[03:31:56.160 --> 03:31:59.520]   this constitution is like worked for a long time. You start with that. Maybe eventually things
[03:31:59.520 --> 03:32:03.120]   change enough that you want added to that. But anyway, you want them to like, you know, for
[03:32:03.120 --> 03:32:07.200]   example, for the checks and balances, they like, they really love the constitution and they believe
[03:32:07.200 --> 03:32:10.720]   in it and they take it really seriously. And like, look at some point, yeah, you are going to have
[03:32:10.720 --> 03:32:16.240]   like AI police and AI military, but I think sort of like, you know, being able to ensure that they
[03:32:16.240 --> 03:32:19.760]   like, you know, believe in it in the way that like a Supreme court justice does, or like in the way
[03:32:19.760 --> 03:32:25.040]   that like a federal reserve, you know, uh, uh, official takes their job really seriously. Um,
[03:32:25.040 --> 03:32:28.720]   yeah. And I guess the big open question is whether, if you do the project or something like
[03:32:28.720 --> 03:32:32.080]   the project, sorry, the other important thing is like a bunch of different factions need their own
[03:32:32.080 --> 03:32:36.080]   AIs. Right. And so it's, it's really important that like each political party gets to like,
[03:32:36.080 --> 03:32:39.040]   have their own, you know, and like whatever crazy, you might totally disagree with their
[03:32:39.040 --> 03:32:42.080]   values, but it's like, it's really important that they get to like have their own kind of
[03:32:42.080 --> 03:32:46.000]   like super intelligence. And, and again, I think it's that these sort of like classical liberal
[03:32:46.000 --> 03:32:50.080]   processes play out, including like different people of different persuasions and so on.
[03:32:50.080 --> 03:32:53.840]   And I don't know, the advisors might not make them, you know, wise, they might not follow the
[03:32:53.840 --> 03:32:58.000]   advice or whatever, but I think it's important. Okay. So speaking of alignment, you seem pretty
[03:32:58.000 --> 03:33:05.600]   optimistic. So let's run, run through the source of the optimism. Yeah. I think there you laid out
[03:33:05.600 --> 03:33:09.600]   different worlds in which we could get AI. Yeah. There's one that you think is low probability of
[03:33:09.600 --> 03:33:15.520]   next year where a GPT four plus scaffolding plus on hoplings gets you to AGI. Not GPT four, you
[03:33:15.520 --> 03:33:20.640]   know, like, sorry, sorry. So she'd be fine. Yeah. And there's ones where it takes much longer.
[03:33:20.640 --> 03:33:26.960]   There's ones where it's something that's a couple of years ago. Yeah. So GPT four seems
[03:33:26.960 --> 03:33:31.040]   pretty aligned in the sense that I don't expect it to go off the rails and maybe with scaffolding
[03:33:31.040 --> 03:33:37.600]   things might change. Yeah, exactly. So the, and you maybe you'll keep turning at there's cranks,
[03:33:37.600 --> 03:33:42.720]   you keep going up and one of the cranks gets you to ASI. Yeah. Is there any point at which
[03:33:42.720 --> 03:33:48.880]   the sharp left turn happens? Is it when you start, is it the case that you think plausibly when they
[03:33:48.880 --> 03:33:52.880]   act more like agents, this is the thing to worry about? Yeah. Is there anything
[03:33:52.880 --> 03:33:55.920]   qualitatively that you expect to change with regards to the alignment perspective?
[03:33:55.920 --> 03:33:59.760]   Yeah. So I don't know if I believe in this concept of sharp left turn, but I do think there's
[03:33:59.760 --> 03:34:03.520]   basically, I think there's important qualitative changes that happen between now and kind of like
[03:34:03.520 --> 03:34:07.440]   somewhat superhuman systems, kind of like early on the intelligence explosion and then important
[03:34:07.440 --> 03:34:11.680]   qualitative changes that happen from like early in intelligence explosion to kind of like true
[03:34:11.680 --> 03:34:16.960]   superintelligence and all its power and might. And let's talk about both of those. And so,
[03:34:16.960 --> 03:34:19.920]   okay. So the first part of the problem is one, you know, we're going to have to solve ourselves,
[03:34:19.920 --> 03:34:22.880]   right. We have to, going to have to align the like initial AI and the intelligence explosion,
[03:34:22.880 --> 03:34:26.880]   you know, the sort of automated out Bradford. I think there's kind of like, I mean, two important
[03:34:26.880 --> 03:34:32.800]   things that change from GPT-4, right. So one of them is, you know, if you believe the story on
[03:34:32.800 --> 03:34:37.440]   like, you know, synthetic data or L or self play to get past the data wall. And if you believe this
[03:34:37.440 --> 03:34:41.040]   on hobbling story, you know, at the end, you're going to have things, you know, they're agents,
[03:34:41.040 --> 03:34:46.160]   right. Including they do long-term plans, right. They have long, long, you know, they're somehow
[03:34:46.160 --> 03:34:49.200]   they're able to act over long horizons, right. But you need that, right. That's the sort of
[03:34:49.200 --> 03:34:54.880]   prerequisite to be able to do the sort of automated AI research. And so, you know,
[03:34:54.880 --> 03:34:58.000]   I think there's basically, you know, I basically think sort of pre-training is sort of alignment
[03:34:58.000 --> 03:35:02.000]   neutral in the sense of like, it has all these representations. It has good representations that,
[03:35:02.000 --> 03:35:06.400]   you know, as, as representations of doing bad things, you know, but, but there's, there's,
[03:35:06.400 --> 03:35:10.880]   it's not like, you know, scheming against you or whatever. I think the sort of misalignment
[03:35:10.880 --> 03:35:14.960]   can arise once you're doing more kind of long horizon training. Right. And so you're training,
[03:35:14.960 --> 03:35:18.400]   you know, again, too simplified example, but to kind of illustrate, you know, you're training
[03:35:18.400 --> 03:35:23.280]   an AI to make money. And, you know, if you're just doing that with reinforcement learning,
[03:35:23.280 --> 03:35:28.080]   you know, it's, you know, it might learn to commit fraud or lie or deceive or seek power
[03:35:28.080 --> 03:35:31.440]   simply because those are successful strategies in the real world. Right. So maybe, you know,
[03:35:31.440 --> 03:35:35.760]   RL is basically it explores, maybe it figures out like, oh, it tries to like hack and then it gets
[03:35:35.760 --> 03:35:39.280]   some money and that made more money, you know, and then if that's successful, if that gets reward,
[03:35:39.280 --> 03:35:43.040]   that's just reinforced. So basically I think there's sort of more serious misalignments,
[03:35:43.040 --> 03:35:47.680]   kind of like misaligned long-term goals that could arise between now and, or that sort of
[03:35:47.680 --> 03:35:51.760]   necessarily have to be able to arise if you're able to get long horizon system. That's one.
[03:35:51.760 --> 03:35:56.480]   What you want to do in that situation is you want to add side constraints, right? So you want to add,
[03:35:56.480 --> 03:36:01.840]   you know, don't lie, don't deceive, don't commit fraud. And so how do you add those side constraints?
[03:36:01.840 --> 03:36:05.520]   Right. The sort of basic idea you might have is like RLHF, right? You're kind of like, yeah,
[03:36:05.520 --> 03:36:09.040]   it has this goal of like, you know, make money or whatever, but you're watching what it's doing.
[03:36:09.040 --> 03:36:12.240]   If it starts trying to like, you know, lie or deceive or fraud or whatever,
[03:36:12.240 --> 03:36:15.520]   or break the law, you're just kind of like thumbs down. Don't do that. You anti-reinforce that.
[03:36:15.520 --> 03:36:20.160]   The sort of critical issue that comes in is that these AI systems are getting superhuman,
[03:36:20.160 --> 03:36:23.920]   right? And they're going to be able to do things that are too complex for humans to evaluate it.
[03:36:23.920 --> 03:36:28.000]   Right. So again, even early on, you know, in the intelligence explosion, the automated AI
[03:36:28.000 --> 03:36:31.600]   researchers and engineers, you know, they might write millions, you know, billions, trillions of
[03:36:31.600 --> 03:36:35.120]   lines of complicated code. You know, they might be doing all sorts of stuff. You just like don't
[03:36:35.120 --> 03:36:39.920]   understand anymore. And so, you know, in the million lines of code, you know, is it somewhere
[03:36:39.920 --> 03:36:43.600]   kind of like, you know, hacking, hacking or like exfiltrating itself or like, you know, trying to
[03:36:43.600 --> 03:36:46.960]   go for the nukes or whatever, you know, like you don't know anymore. Right. And so this sort of
[03:36:46.960 --> 03:36:51.760]   like, you know, thumbs up, thumbs down, pure RLHF doesn't fully work anymore. Second part of the
[03:36:51.760 --> 03:36:55.760]   picture, and maybe we can talk more about this first part of the picture. I think it's going to
[03:36:55.760 --> 03:36:59.760]   be like, there's a hard technical problem of what do you do sort of post RLHF, but I think it's a
[03:36:59.760 --> 03:37:03.120]   solvable problem. And it's like, you know, there's various things I'm bullish on. I think there's
[03:37:03.120 --> 03:37:07.680]   like ways in which deep learning has shaped out favorably. The second part of the problem is you're
[03:37:07.680 --> 03:37:11.040]   going from your like initial systems, the intelligence explosion to like superintelligence.
[03:37:11.040 --> 03:37:15.040]   And, you know, it's like many ooms ends up being like by the end of it, you have a thing that's
[03:37:15.040 --> 03:37:20.400]   vastly smarter than humans. I think the intelligence explosion is really scary from
[03:37:20.400 --> 03:37:24.240]   an alignment point of view, because basically, if you have this rapid intelligence explosion,
[03:37:24.240 --> 03:37:27.920]   you know, less than a year, two years or whatever you're going, say, in the period of a year from
[03:37:27.920 --> 03:37:32.080]   systems where like, you know, failure would be bad, but it's not catastrophic to like, you know,
[03:37:32.080 --> 03:37:38.000]   saying a bad word. It's like, you know, it's something goes awry to like, you know, failure
[03:37:38.000 --> 03:37:41.840]   is like, you know, it exfiltrated itself, it starts hacking the military can do really bad things.
[03:37:41.840 --> 03:37:45.920]   You're going less than a year from sort of a world in which like, you know, it's some descendant of
[03:37:45.920 --> 03:37:49.600]   current systems, and you kind of understand it. And it's like, you know, has good properties,
[03:37:49.600 --> 03:37:52.720]   something that potentially has a very sort of alien and different architecture, right after
[03:37:52.720 --> 03:37:58.080]   having gone through another decade of maladvances. I think one example there that's very salient to me
[03:37:58.080 --> 03:38:02.320]   is legible and faithful chain of thought, right? So a lot of the time, when we're talking about
[03:38:02.320 --> 03:38:05.200]   these things, we're talking about, you know, it has tokens of thinking, and then it uses many
[03:38:05.200 --> 03:38:09.760]   tokens of thinking. And, you know, maybe we bootstrap ourselves by, you know, it's pre trained,
[03:38:09.760 --> 03:38:13.440]   it learns to think in English, and we do something else on top. So it can do the sort of longer
[03:38:13.440 --> 03:38:19.040]   chains of thought. And so, you know, it's very plausible to me that like, for the initial
[03:38:19.040 --> 03:38:22.480]   automated alignment researchers, you know, we don't need to do any complicated mechanistic
[03:38:22.480 --> 03:38:26.480]   interpretability, and just like, literally, you read what they're thinking, which is great.
[03:38:26.480 --> 03:38:33.200]   You know, it's like huge advantage, right? However, I'm very likely not the most efficient
[03:38:33.200 --> 03:38:36.720]   way to do it, right? There's like, probably some way to have a recurrent architecture,
[03:38:36.720 --> 03:38:40.240]   it's all internal states is a much more efficient way to do it. That's what you get by the end of
[03:38:40.240 --> 03:38:45.600]   the year. You know, you're going this year from like RLHF plus plus some extension works to like,
[03:38:45.600 --> 03:38:51.360]   it's vastly superhuman. It's like, you know, it's to us, like, you know, you know, an expert in the
[03:38:51.360 --> 03:38:55.840]   field might be to like an elementary school or middle schooler. And so, you know, I think it's
[03:38:55.840 --> 03:39:02.080]   this sort of incredibly sort of like hairy period for alignment. Thing you do have is you have the
[03:39:02.080 --> 03:39:06.640]   automated AI researchers, right? And so you can use the automated AI researchers to also do
[03:39:06.640 --> 03:39:13.040]   alignment. And so in this world, yeah, why are we optimistic that the project is being run by
[03:39:13.040 --> 03:39:18.400]   people who are thinking, I think, so here's, here's, here's, here's something to think about,
[03:39:18.400 --> 03:39:25.520]   okay, the opening I Yeah, you starts off with people who are very explicitly thinking about
[03:39:25.520 --> 03:39:28.560]   exactly these kinds of things. Yes. Right. But are they still there?
[03:39:28.560 --> 03:39:33.440]   No, no, but you still hear here's the thing. No, no, even the people who are there, even like the
[03:39:33.440 --> 03:39:36.880]   current leadership is like exactly these things, you can find them in interviews in their blog
[03:39:36.880 --> 03:39:44.480]   posts talking about. And what happens is, when as you were talking about when some sort of trivial,
[03:39:45.360 --> 03:39:49.360]   and y'all talked about it, this is not just you, we all talked about his tweet thread,
[03:39:49.360 --> 03:39:54.960]   when there is some trade off that has to be made with we need to do this flashy release this week,
[03:39:54.960 --> 03:39:59.840]   and not next week, because whatever Google IO is the next week. So yeah, and then the trade
[03:39:59.840 --> 03:40:08.800]   off is made in favor of the less the more careless decision. When we have the government,
[03:40:08.800 --> 03:40:13.840]   or the national security advisor, the military, or whatever, which is much less familiar with
[03:40:13.840 --> 03:40:18.160]   this kind of discourse is a naturally thinking in this way about how and where the chain of
[03:40:18.160 --> 03:40:21.440]   thought is unfaithful. And how do we think about the features that are represented here?
[03:40:21.440 --> 03:40:27.920]   Why should we be optimistic that a project run by people like that will be thoughtful
[03:40:27.920 --> 03:40:36.240]   about these kinds of considerations? I mean, they might not be. You know, I, I agree, I think
[03:40:39.680 --> 03:40:43.680]   I have a few thoughts, right? First of all, I think the private world, even if they sort of
[03:40:43.680 --> 03:40:47.680]   nominally care is extremely tough for alignment, a couple reasons. One, you just have the race
[03:40:47.680 --> 03:40:51.280]   between the sort of commercial labs, right? And it's like, you don't have any headroom there to
[03:40:51.280 --> 03:40:54.720]   like, be like, actually, we're going to hold back for three months, like get this right. And you
[03:40:54.720 --> 03:40:58.080]   know, we're going to dedicate 90% of our compute to automated alignment research instead of just
[03:40:58.080 --> 03:41:02.320]   like pushing the next zoom. The other thing, though, is like, in the private world, you know,
[03:41:02.320 --> 03:41:06.160]   China has stolen your age, China has your secrets, they're right on your tails, you're in this fever
[03:41:06.160 --> 03:41:11.680]   struggle, no room at all for maneuver, though, like the way it's like absolutely essential to
[03:41:11.680 --> 03:41:14.720]   get alignment, right. And you get it during this intelligence explosion, you get it right,
[03:41:14.720 --> 03:41:19.200]   is you need to have that room to maneuver and you need to have that clear lead. And, you know,
[03:41:19.200 --> 03:41:24.560]   again, maybe you've made the deal or whatever, but I think you're an incredibly tough space,
[03:41:24.560 --> 03:41:28.800]   tough spot if you don't have this clearly. So I think the sort of private world is kind of rough
[03:41:28.800 --> 03:41:32.080]   there on like, whether people will take it seriously, you know, I don't know, I have some
[03:41:32.080 --> 03:41:37.120]   faith in sort of sort of normal mechanisms of a liberal society, sort of, if alignment is an
[03:41:37.120 --> 03:41:40.800]   issue, which, you know, we don't fully know yet, but sort of the science will develop, we're going
[03:41:40.800 --> 03:41:44.720]   to get better measurements of alignment, you know, and the case will be clear and obvious.
[03:41:44.720 --> 03:41:49.680]   I worry that there's, you know, I worry about worlds where evidence is ambiguous. And I think
[03:41:49.680 --> 03:41:53.440]   a lot of a lot of the most scary kind of intelligence explosion scenarios are worlds
[03:41:53.440 --> 03:41:57.920]   in which evidence is ambiguous. But again, it's sort of like I, if evidence is ambiguous, then
[03:41:57.920 --> 03:42:01.040]   that's the world in which you really want the safety margins. And that's also the world in
[03:42:01.040 --> 03:42:03.840]   which kind of like running the intelligence explosion is sort of like, you know, running
[03:42:03.840 --> 03:42:07.840]   a war, right? It's like, ah, the evidence is ambiguous, right, make these really tough trade
[03:42:07.840 --> 03:42:11.440]   offs. And you like you better have a really good chain of command for that. Yeah, it's not just
[03:42:11.440 --> 03:42:16.160]   like, you know, yoloing yet. Let's go. You know, it's cool. Yeah, let's talk a little bit about
[03:42:16.160 --> 03:42:22.880]   Germany. We're making the analogy to World War Two. And you made a really interesting point many
[03:42:22.880 --> 03:42:37.360]   hours ago. The fact that throughout history, World War Two, is not unique, at least when you
[03:42:37.360 --> 03:42:44.160]   think in proportion, yeah, to the size of the population. Yeah. But these other sorts of
[03:42:44.160 --> 03:42:49.440]   catastrophes where some significant portion of the population has been killed off. Yeah.
[03:42:50.560 --> 03:42:57.680]   After that, the nation recovers, and they get back to their heights. And so what's interesting
[03:42:57.680 --> 03:43:02.800]   after World War Two, is that Germany, especially, and maybe Europe as a whole, obviously, they
[03:43:02.800 --> 03:43:07.680]   experienced fast economic growth in the direct aftermath because of catch up growth. But
[03:43:07.680 --> 03:43:14.480]   subsequently, we just don't think of Germany as no, we're not talking about Germany potentially
[03:43:14.480 --> 03:43:18.400]   launching an intelligence explosion, and they're going to get into the AI table. We're talking
[03:43:18.400 --> 03:43:22.640]   about Iran and North Korea and Russia. We didn't talk about Germany. Right. Because they're allies.
[03:43:22.640 --> 03:43:29.520]   Yeah. But so what happened? I mean, World War Two, and now it didn't come back after the Seven
[03:43:29.520 --> 03:43:33.440]   Years' War or something, right? Yeah. I mean, look, I'm generally very bearish on Germany.
[03:43:33.440 --> 03:43:36.240]   I think in this context, I'm kind of like, you know, it's a little bit, you know, I think you're
[03:43:36.240 --> 03:43:39.680]   underrating a little bit. I think it's probably still one of the top five most important countries
[03:43:39.680 --> 03:43:45.200]   in the world. You know, I mean, Europe overall, you know, it still has, I mean, it's a GDP that's
[03:43:45.200 --> 03:43:50.080]   close to the United States, the size of the GDP, you know, and there's things actually that
[03:43:50.080 --> 03:43:55.120]   Germany is kind of good at, right? Like state capacity, right? Like, you know, the roads are
[03:43:55.120 --> 03:44:00.960]   good and they're clean and they're well-maintained. And, you know, in some sense, a lot of this is the
[03:44:00.960 --> 03:44:04.480]   sort of flip side of things that I think are bad about Germany, right? So in the US, it's a little
[03:44:04.480 --> 03:44:08.400]   bit like there's a bit more of a sort of Wild West feeling to the United States, right? And it includes
[03:44:08.400 --> 03:44:14.480]   the kind of like crazy bursts of creativity, includes like, you know, political candidates
[03:44:14.480 --> 03:44:17.600]   that are sort of, you know, there's a much broader spectrum and, you know, much, you know,
[03:44:17.600 --> 03:44:21.440]   like both an Obama and a Trump is somebody you just wouldn't see in the sort of much more confined
[03:44:21.440 --> 03:44:25.520]   kind of German political debate. You know, I wrote this blog post at some point, Europe's political
[03:44:25.520 --> 03:44:30.800]   stupor about this. But anyway, and so there's this sort of punctilious sort of rule following
[03:44:30.800 --> 03:44:34.400]   that is like good in terms of like, you know, keeping your kind of state capacity functioning.
[03:44:34.400 --> 03:44:41.760]   But that is also, you know, I think I kind of, I think there's a sort of very constrained view
[03:44:41.760 --> 03:44:47.040]   of the world in some sense, you know, and that includes kind of, you know, I think after World
[03:44:47.040 --> 03:44:52.240]   War Two, there's a real backlash against anything like elite, you know, and, you know, again, no,
[03:44:52.240 --> 03:44:58.240]   you know, no elite high schools or elite colleges and sort of. Why is that the law excellence isn't
[03:44:58.240 --> 03:45:02.240]   cherished? You know, there's Yeah. Why is that the logical intellectual
[03:45:02.240 --> 03:45:08.400]   thing to rebel against if what if you're trying to overcorrect from the Nazis? Yeah.
[03:45:08.400 --> 03:45:12.080]   Was it because the Nazis were very much into elitism? What was I don't understand why that's
[03:45:12.080 --> 03:45:16.240]   a logical sort of counter reaction. I know maybe it was sort of a counter reaction against the sort
[03:45:16.240 --> 03:45:20.320]   of like whole like Aryan race and sort of that sort of thing. I mean, I also just think there
[03:45:20.320 --> 03:45:25.200]   was a certain amount and what amount certain I mean, look, look at sort of World War One and
[03:45:25.200 --> 03:45:29.840]   World War One was in World War Two for Germany. Right. And sort of, you know, a common narrative
[03:45:29.840 --> 03:45:35.200]   is that the piece of Versailles was too strict on Germany. But, you know, the piece imposed after
[03:45:35.200 --> 03:45:38.960]   World War Two was like much more strict. Right. It was a complete, you know, I mean, the whole
[03:45:38.960 --> 03:45:43.520]   country was destroyed. You know, it was, you know, in all the most of the major cities, you know,
[03:45:43.520 --> 03:45:48.160]   over half of the housing stock had been destroyed. Right. Like, you know, in some birth cohorts,
[03:45:48.160 --> 03:45:53.040]   you know, like 40 percent of the men had died. Half the population displaced. Oh, yeah. I mean,
[03:45:53.040 --> 03:45:58.080]   almost 20 million people are displaced. Huge. Crazy. Right. You know, and the borders are way
[03:45:58.080 --> 03:46:02.560]   smaller than the Versailles border. Yeah, exactly. And and and sort of complete imposition of a new
[03:46:02.560 --> 03:46:12.000]   political system and and, you know, on both sides, you know, and. Yeah, so it was but in some sense
[03:46:12.000 --> 03:46:15.360]   that worked out better than the post-World War One peace, where then there was this kind of
[03:46:15.360 --> 03:46:19.200]   resurgence of German nationalism and, you know, in some sense, the thing that has been a pattern.
[03:46:19.200 --> 03:46:22.720]   So it's sort of like it's unclear if you want to wake the sleeping beast. I do think that at this
[03:46:22.720 --> 03:46:28.640]   point, you know, it's gotten a bit too sleepy. Yeah. I do think it's an interesting point about
[03:46:28.640 --> 03:46:32.720]   we underrate the American political system. Yeah. And I've been making the same correction myself.
[03:46:32.720 --> 03:46:39.200]   Yeah. There's there was this book about Verdun by a Chinese economist called China's World View.
[03:46:39.200 --> 03:46:43.520]   Yeah. And overall, I wasn't a big fan, but they made a really interesting point in there.
[03:46:43.520 --> 03:46:51.680]   Yeah. Which was the way in which candidates rise up through the Chinese. Yeah. Hierarchy for
[03:46:51.680 --> 03:46:57.200]   politics, for administration. In some sense that selects for you're not going to get some Marjorie
[03:46:57.200 --> 03:47:01.920]   Taylor Greene or somebody running or something. Don't get that in Germany either. Right. Yeah.
[03:47:01.920 --> 03:47:06.320]   But he explicitly made the point in the book that that also means we're never going to get
[03:47:06.320 --> 03:47:10.560]   a Henry Kissinger or Barack Obama. Right. In China, we're going to get like that by the time
[03:47:10.560 --> 03:47:16.080]   they end up in charge of the Politburo, there'll be like some 60 year old Democrat who's never
[03:47:16.080 --> 03:47:19.600]   like ruffled any feathers. Yeah. I mean, I think I think there's something really important about
[03:47:19.600 --> 03:47:24.320]   the sort of like very raucous political debate. And I mean, yeah, in general, kind of like,
[03:47:24.320 --> 03:47:27.680]   you know, there's the sense in which in America, you know, lots of people live in their kind of
[03:47:27.680 --> 03:47:32.080]   like own world. I mean, like we live in this kind of bizarre little like bubble in San Francisco
[03:47:32.080 --> 03:47:38.480]   and people, you know, and but I think that's important for the sort of evolution of ideas,
[03:47:38.480 --> 03:47:43.840]   error correction, that sort of thing. You know, there's other ways in which the German system
[03:47:43.840 --> 03:47:48.400]   is more functional. Yeah. But it's interesting that there's major mistakes, right? Like the
[03:47:48.400 --> 03:47:52.400]   sort of defense spending. Right. And, you know, then, you know, Russia invades Ukraine and you're
[03:47:52.400 --> 03:47:56.480]   like, wow, what did we do? Right. No, that's a really good point. Right. The main issue is
[03:47:56.480 --> 03:48:02.560]   there's everybody agrees, but exactly. Yeah. I can sense this blob kind of thing. Right.
[03:48:02.560 --> 03:48:06.640]   And on the China point, you know, just having this experience of like reading German newspapers. And
[03:48:06.640 --> 03:48:10.480]   I think how much, you know, how much more poorly I would understand the sort of German debate and
[03:48:10.480 --> 03:48:15.840]   sort of the sort of state of mind from just kind of afar. I worry a lot about, you know,
[03:48:15.840 --> 03:48:20.800]   or I think it is interesting just how kind of impenetrable China is to me.
[03:48:20.800 --> 03:48:24.720]   It's a billion people. Right. And like, you know, almost everything else is really globalized. You
[03:48:24.720 --> 03:48:28.720]   have a globalized Internet. And I kind of have a sense of what's happening in the UK. You know,
[03:48:28.720 --> 03:48:31.440]   I probably even if I didn't read German newspapers, I sort of would have a sense of what's
[03:48:31.440 --> 03:48:37.520]   happening in Germany. But I really don't feel like I have a sense of what like, you know,
[03:48:37.520 --> 03:48:42.240]   what is the state of mind or the state of political debate, you know, of a sort of average Chinese
[03:48:42.240 --> 03:48:47.040]   person or like an average Chinese. Yeah. And yeah, I think that that I find that distance kind of
[03:48:47.040 --> 03:48:50.720]   worrying. And I, you know, you know, and there's you know, there's some people who do this and they
[03:48:50.720 --> 03:48:54.560]   do really great work where they kind of go through the like party documents and the party speeches.
[03:48:54.560 --> 03:48:58.080]   And it seems to require a kind of a lot of interpretive ability where there's like very
[03:48:58.080 --> 03:49:02.080]   specific words in Mandarin that like mean we'll have one connotation, not the other connotation.
[03:49:02.080 --> 03:49:07.040]   But yeah, I think it's sort of interesting given how globalized everything is. And like,
[03:49:07.040 --> 03:49:10.960]   I mean, now we have basically perfect translation machines and it's still so, so impenetrable.
[03:49:10.960 --> 03:49:15.760]   That's really interesting. I've been actually, I'm sort of ashamed almost that I haven't done
[03:49:15.760 --> 03:49:20.880]   this yet. Yeah. I think many months ago, I when Alexi interviewed me on his YouTube channel,
[03:49:20.880 --> 03:49:24.880]   I said, I'm meaning to go to China to actually see for myself what's going on. And actually,
[03:49:24.880 --> 03:49:30.400]   I should. So by the way, if anybody listening has a lot of context on China, if I went to China,
[03:49:30.400 --> 03:49:34.720]   who could introduce me to people, please email me. You got to do some pods and you got to find
[03:49:34.720 --> 03:49:39.520]   some of the Chinese AI researchers, man. I know. I was thinking at some point, again,
[03:49:39.520 --> 03:49:43.920]   this is the fact that I really, but you know, I don't know if they can speak. But I was thinking
[03:49:43.920 --> 03:49:48.560]   of theirs. So they had these papers and on the paper, they'll say who's a coauthor. Yeah. It's
[03:49:48.560 --> 03:49:53.920]   funny because while I was thinking of just emailing, cold emailing everybody, like, here's my
[03:49:53.920 --> 03:49:57.120]   calendar. Can you let's let's just talk. I just want to see what is the vibe even they don't tell
[03:49:57.120 --> 03:50:00.320]   me anything. I'm just like, what kind of person is this? Are they how westernized are they? Yeah.
[03:50:00.320 --> 03:50:08.080]   But I was I was saying this, I just remembered that, in fact, bike dance, according to mutual
[03:50:08.080 --> 03:50:13.520]   friends we have at Google, they cold emailed every single person on the Gemini paper and said,
[03:50:13.520 --> 03:50:18.080]   if you come work for bike dance, we'll make you an elite engineer. You report directly to CTO.
[03:50:18.080 --> 03:50:23.760]   And in fact, that's how the secrets go over. Right. Right. No, I'm not sure. I meant to ask
[03:50:23.760 --> 03:50:28.400]   this earlier. But suppose they hired what if there's only 100 or so people or maybe less
[03:50:28.400 --> 03:50:33.840]   were working on the key algorithm. Yeah. If they hired one such person. Yeah. Is all the alpha gone
[03:50:33.840 --> 03:50:38.320]   that these labs out if this person was intentional about it, they could get a lot. I mean, they
[03:50:38.320 --> 03:50:41.520]   couldn't get the sort of like I mean, actually, you could probably just also exfiltrate the code.
[03:50:41.520 --> 03:50:44.720]   They could get a lot of the key ideas again, like, you know, up until recently, stuff was published.
[03:50:44.720 --> 03:50:48.240]   But, you know, they could get a lot of the key ideas if they tried, if they like, you know,
[03:50:48.240 --> 03:50:50.800]   I think there's a lot of people who don't actually kind of like look around to see what the other
[03:50:50.800 --> 03:50:56.160]   teams are doing. But, you know, I think you kind of can. But yeah, I mean, they could. It's scary.
[03:50:56.160 --> 03:51:01.200]   Right. I think the project makes more sense. There were you can't just recruit a Manhattan
[03:51:01.200 --> 03:51:05.920]   project engineer and then just get it. And it's like these are secrets that can be used for like
[03:51:05.920 --> 03:51:09.680]   probably every training on the future. That'll be like maybe are the key to the data wall that
[03:51:09.680 --> 03:51:13.440]   like they can't go on or they can't go on that are like, you know, they're going to be worth,
[03:51:13.440 --> 03:51:16.240]   you know, given sort of like the multipliers and compute, you know, hundreds of billions,
[03:51:16.240 --> 03:51:20.160]   trillions of dollars, you know, and all it takes is, you know, China to offer one hundred
[03:51:20.160 --> 03:51:23.920]   million dollars to somebody and be like, ah, come work for us. Right. And then and then,
[03:51:23.920 --> 03:51:29.280]   yeah, I mean, I mean, yeah, I'm I'm really uncertain on how sort of seriously China is
[03:51:29.280 --> 03:51:34.720]   taking AGI right now. One one anecdote that was really to me on the topic of anecdotes,
[03:51:34.720 --> 03:51:37.920]   the by another sort of like, you know, kind of researcher in the field was
[03:51:37.920 --> 03:51:42.640]   at some point there at a conference with somebody, a Chinese researcher, and he was talking to him
[03:51:42.640 --> 03:51:45.120]   and he was like, I think it's really good that you're here. And like, you know, we got to have
[03:51:45.120 --> 03:51:50.160]   the international coordination and stuff. And apparently this guy said that I'm the kind of
[03:51:50.160 --> 03:51:54.080]   most senior, most person that they're going to let leave the country to come to things like this.
[03:51:54.080 --> 03:51:59.440]   Wait, what's what's the takeaway? As they're not letting really senior
[03:51:59.440 --> 03:52:05.840]   country, they're saying kind of classic, you know, Eastern bloc move. Yeah. I don't know if this is
[03:52:05.840 --> 03:52:09.840]   true, but it's what I hear. It's interesting. So I thought the point you made earlier about
[03:52:09.840 --> 03:52:15.760]   being exposed to German newspapers and also to because earlier you're interested in economics
[03:52:15.760 --> 03:52:22.000]   and law and national security. You have the variety and intellectual diet there has exposed
[03:52:22.000 --> 03:52:26.160]   you to thinking about the geopolitical question here in ways others talking about. Yeah, I mean,
[03:52:26.160 --> 03:52:29.200]   this is the first episode I've done about this where we've talked about things like this,
[03:52:29.200 --> 03:52:33.360]   which is now that I think about it, we are to give that this is an obvious thing in retrospect.
[03:52:33.360 --> 03:52:37.360]   I should have been thinking about anyways. So that's one thing we've been missing.
[03:52:37.360 --> 03:52:42.240]   What are you missing? And national security you're thinking about? So you can't say national
[03:52:42.240 --> 03:52:48.240]   security. What like perspective are you probably underexposed to as a result? And China, I guess
[03:52:48.240 --> 03:52:53.600]   you mentioned. Yes, I think the China one is an important one. I mean, I think another one would
[03:52:53.600 --> 03:52:57.840]   be a sort of very Tyler Cowness take, which is like you're not exposed to how how like how will
[03:52:57.840 --> 03:53:02.800]   a normal person in America like, you know, both like use AI, you know, probably not, you know,
[03:53:02.800 --> 03:53:07.040]   and and and that being kind of like bottlenecks to the fusion of these things. I'm overrating
[03:53:07.040 --> 03:53:10.640]   the revenue because I'm kind of like, ah, you know, everyone else is adopting it. But, you know,
[03:53:10.640 --> 03:53:14.480]   kind of like, you know, Joe Schmoe engineer at a company, you know, like, will they will they be
[03:53:14.480 --> 03:53:18.640]   able to integrate it? And also the reaction to it. Right. You know, I mean, I think this was a
[03:53:18.640 --> 03:53:25.120]   question again hours ago where it was about like, you know, won't people kind of rebel against this?
[03:53:25.120 --> 03:53:30.400]   Yeah. And they won't want to do the project. I don't know. Maybe they will. Yeah. Here's a
[03:53:30.400 --> 03:53:35.280]   political reaction that I didn't anticipate. Yeah. So Tucker Carlson was recently on the
[03:53:35.280 --> 03:53:39.760]   Joe Rogan episode. I already told you about this, but I'm just going to tell the story again. So
[03:53:39.760 --> 03:53:46.400]   Tucker Carlson is on Joe Rogan. Yeah. And they start talking about World War Two. And Tucker
[03:53:46.400 --> 03:53:50.960]   says, Well, listen, I'm going to say something that my fellow conservatives won't like. But
[03:53:51.600 --> 03:53:55.440]   I think nuclear weapons are immoral. I think it was obviously immoral that we use them on
[03:53:55.440 --> 03:54:01.440]   Nagasaki and Hiroshima. And then he says, In fact, nuclear weapons are always immoral.
[03:54:01.440 --> 03:54:07.920]   Except when we would use them on data centers. In fact, it would be immoral not to use them on data
[03:54:07.920 --> 03:54:13.840]   centers. Because look, these people in Silicon Valley, these fucking nerds are making super
[03:54:13.840 --> 03:54:19.520]   intelligence. And they say that it could enslave humanity. We made machines to serve humanity,
[03:54:19.520 --> 03:54:25.520]   not to enslave humanity. And they're just going on and making these machines. And so we should,
[03:54:25.520 --> 03:54:33.840]   of course, be nuking the data centers. And that is definitely not a political reaction in 2024,
[03:54:33.840 --> 03:54:39.360]   I was expecting. I mean, who knows? It's going to be crazy. It's going to be crazy. The thing
[03:54:39.360 --> 03:54:45.600]   we learned with COVID is that also the left-right reactions that you would anticipate just based on
[03:54:45.600 --> 03:54:53.040]   hunches, it completely flipped. Initially, the right was like, it's so contingent. And then the
[03:54:53.040 --> 03:54:58.000]   left was like, this is racist. And then it flipped. The left was really into the code. And the whole
[03:54:58.000 --> 03:55:05.600]   thing also is just so blunt and crude. And so I think probably in general, people like to make
[03:55:05.600 --> 03:55:09.680]   sort of complicated technocratic AI policy proposals. And I think, especially if things
[03:55:09.680 --> 03:55:17.360]   go fairly rapidly on the left AGI, there might not actually be that much space for complicated,
[03:55:17.360 --> 03:55:21.360]   clever proposals. It might just be a much cruder reaction.
[03:55:21.360 --> 03:55:28.480]   And then also when you mentioned the spies and national security getting involved and everything,
[03:55:28.480 --> 03:55:33.760]   and you can talk about that in the abstract. But now that we're living in San Francisco,
[03:55:33.760 --> 03:55:38.720]   and we know many of the people who are doing the top AI research, it's also a little scary
[03:55:38.720 --> 03:55:43.280]   to think about people I personally know and friends with. It's not unfeasible if they have
[03:55:43.280 --> 03:55:49.200]   secrets in their head that are worth $100 billion or something. Kidnapping, assassination, sabotage.
[03:55:49.200 --> 03:55:53.600]   It's scary. Or their family. Yeah, it's really bad. I guess it's to the point on security. Right
[03:55:53.600 --> 03:56:00.720]   now, it's just really foreign. But at some point, as it becomes really serious, you're going to want
[03:56:00.720 --> 03:56:08.240]   the security cards. Yeah. Yeah. So presumably, you have thought about the fact that people in
[03:56:08.240 --> 03:56:13.200]   China will be listening to this and will be reading your series. Yeah. And somehow you made
[03:56:13.200 --> 03:56:22.240]   the trade-off that it's better to let the whole world know, and also including China, and make
[03:56:22.240 --> 03:56:27.360]   them up to AGI, which is part of the thing you're worried about is China waking up to AGI, than to
[03:56:27.360 --> 03:56:31.600]   stay silent. Yeah. I'm just curious. Walk me through how you've thought about that trade-off.
[03:56:31.600 --> 03:56:36.080]   Yeah. I actually, look, I think this is a tough trade-off. I thought about this a bunch. I think
[03:56:37.600 --> 03:56:48.320]   people in the PRC will read this. I think there's some extent to which the cat is out of the bag.
[03:56:48.320 --> 03:56:53.760]   This is not AGI being a thing people are thinking about very seriously is not new anymore. A lot of
[03:56:53.760 --> 03:56:58.400]   these takes are kind of old. I had similar views a year ago. I might not have written it up a year
[03:56:58.400 --> 03:57:04.640]   ago, in part, because I think this cat wasn't out of the bag enough. I think the other thing is,
[03:57:04.640 --> 03:57:13.200]   I think to be able to manage this challenge, I think much broader swaths of society will need
[03:57:13.200 --> 03:57:18.720]   to wake up. And if we're going to get the project, we actually need a broad bipartisan understanding
[03:57:18.720 --> 03:57:25.520]   of the challenges facing us. And so I think it's a tough trade-off, but I think the need to wake up
[03:57:25.520 --> 03:57:29.120]   people in the United States, in the sort of Western world, in the democratic coalition,
[03:57:29.120 --> 03:57:35.360]   is ultimately imperative. And I think my hope is more people here will read it than in the PRC.
[03:57:35.360 --> 03:57:41.360]   And I think people sometimes underrate the importance of just kind of writing it up,
[03:57:41.360 --> 03:57:46.240]   laying out the strategic picture. And I think you've done actually a great service to sort
[03:57:46.240 --> 03:57:54.400]   of mankind in some sense with your podcast. And I think it's overall been good.
[03:57:55.200 --> 03:58:00.000]   Okay. So by the way, on the topic of Germany, we were talking at some point about kind of
[03:58:00.000 --> 03:58:03.760]   immigration story. I feel like you have a kind of interesting story you haven't told,
[03:58:03.760 --> 03:58:12.400]   and I think you should tell. So a couple of years ago, I was in college and I was 20. I was about
[03:58:12.400 --> 03:58:16.240]   to turn 21. Yeah. I think it was. Yeah. You came from India when you were really young.
[03:58:16.240 --> 03:58:22.000]   Right. Yeah. Yeah. So until I was eight or nine, I lived in India and then we moved around all over
[03:58:22.000 --> 03:58:30.560]   the place. But because of the backlog for Indians. Yeah. The green card backlog. Yeah. We've been in
[03:58:30.560 --> 03:58:35.760]   the queue for like decades. Even though you came at eight, you're still on the H1B. Yeah. And when
[03:58:35.760 --> 03:58:40.320]   you're 21, you get kicked off the queue and you had to restart the process. So I'm on my dad's,
[03:58:40.320 --> 03:58:44.000]   my dad's a doctor and I'm on his H1B as a dependent. But when you're 21, you get kicked off.
[03:58:44.000 --> 03:58:48.240]   Yeah. And so I'm 20 and it just kind of dawns on me that this is my situation.
[03:58:48.240 --> 03:58:53.120]   Yeah. And he's completely screwed. Right. And so I also had the experience that my dad.
[03:58:53.120 --> 03:58:57.680]   Yeah. We've like moved all around the country. They have to prove that him as a doctor is like,
[03:58:57.680 --> 03:59:02.320]   you can't get native talent. Yeah. And you can't start a startup. Yeah. So where can you not get
[03:59:02.320 --> 03:59:07.120]   native? And like even getting the H1B for you would have been like a 20% lottery. So if you're
[03:59:07.120 --> 03:59:09.920]   lucky you're in this. And they had to prove that they can't get native talent, which means like
[03:59:09.920 --> 03:59:13.520]   for him, I'm like, we lived in North Dakota for three years, West Virginia for three years,
[03:59:13.520 --> 03:59:18.400]   Maryland, West Texas. Yeah. And so it kind of dawned on me. This is my situation. As I turn 21,
[03:59:18.400 --> 03:59:23.040]   I'll be like on this lottery. Even if I get the lottery, I'll be a fucking code monkey for the
[03:59:23.040 --> 03:59:27.200]   rest of my life because this, this thing isn't, isn't going to let up. Yeah. Can't do a startup.
[03:59:27.200 --> 03:59:31.520]   Exactly. And so at the same time I had been reading for the last year, I've been super
[03:59:31.520 --> 03:59:36.400]   obsessed with Paul Graham essays. My plan at the time was to make a startup or something. I was
[03:59:36.400 --> 03:59:40.240]   super excited about that. Yeah. And it just occurred to me that I couldn't do this. Yeah.
[03:59:40.240 --> 03:59:45.360]   That like, this is just not in the cards for me. Yeah. And so I was kind of depressed about it. I
[03:59:45.360 --> 03:59:50.720]   remember I kind of just, um, I, uh, I was in a dazed through finals because I had like,
[03:59:50.720 --> 03:59:57.200]   I had just occurred to me and I was really like anxious about it. Yeah. And I, I remember
[03:59:57.200 --> 04:00:03.040]   thinking to myself at the time that if somehow I ended up getting my green card before I turned 21,
[04:00:03.040 --> 04:00:07.520]   there's no fucking way I'm turning and becoming a code monkey. Yeah. Because the thing that I've
[04:00:08.240 --> 04:00:13.440]   like this feeling of dread that I have is this realization that I'm just going to have to be
[04:00:13.440 --> 04:00:19.520]   a code monkey. And I realized that's my default path. Yeah. If I, if I hadn't sort of made a
[04:00:19.520 --> 04:00:22.480]   proactive effort not to do that, I would have graduated college as a computer science student
[04:00:22.480 --> 04:00:25.680]   and I would have just done that. Yeah. And that's the thing I was super scared about. Yeah. So that
[04:00:25.680 --> 04:00:31.840]   was an important sort of, um, uh, realization for me anyway. So COVID happened because of that,
[04:00:31.840 --> 04:00:37.120]   since there weren't, uh, foreigners coming, the backlog cleared fast and by the skin of my teeth,
[04:00:37.120 --> 04:00:41.440]   like a few months before I turned 21, so crazy, extremely contingent reasons. I ended up getting
[04:00:41.440 --> 04:00:45.920]   a green card because I got a green card. I could, you know, the whole podcast, right. Exactly.
[04:00:45.920 --> 04:00:50.720]   Right. I graduated college and I was like bumming around and I got was like, I got,
[04:00:50.720 --> 04:00:55.200]   I graduated a semester early. I'm going to like do this podcast and what happens. And it was,
[04:00:55.200 --> 04:00:59.680]   it hadn't, it didn't have a green card and the best case scenario, you know, and it only existed
[04:00:59.680 --> 04:01:04.400]   because yeah, it was actually, cause I think it's hard. It's probably, it's, you know, what is the
[04:01:04.400 --> 04:01:07.920]   impact of like immigration reform? Right. What is the impact of clearing, you know, like whatever,
[04:01:07.920 --> 04:01:13.120]   50,000 green cards in the backlog. And you're such like an amazing example of like, you know,
[04:01:13.120 --> 04:01:17.440]   all of this is only possible and it's, yeah, it's, I mean, it's just incredibly tragic that
[04:01:17.440 --> 04:01:24.320]   this is so dysfunctional. Yeah. No, it's insane. I'm glad you did it. I'm glad you kind of like,
[04:01:24.320 --> 04:01:29.600]   you know, tried the, you know, the, the, uh, the unusual path. Well, yeah, but I could only do it.
[04:01:30.400 --> 04:01:37.280]   Obviously I was extremely fortunate that I got the green card. I was like, um, I had a little
[04:01:37.280 --> 04:01:42.400]   bit of saved up money and I got a small grant out of college. Thanks to the, uh, future fund to like
[04:01:42.400 --> 04:01:47.280]   do this for basically the equivalent of six months. And so it turned out really well. And
[04:01:47.280 --> 04:01:52.400]   then at each time and I was like, Oh, okay. Podcast. Come on. Like I wasted a few months on
[04:01:52.400 --> 04:01:56.240]   this. Let's now go do something real. Yeah. Something big would happen. Yeah. I would
[04:01:58.000 --> 04:02:02.320]   kept with it. Yeah. Yeah. But there would always be at just like the moment I'm about to quit the
[04:02:02.320 --> 04:02:05.840]   podcast. I'm being like Jeff Bezos will say that it's something that's about me on Twitter. The
[04:02:05.840 --> 04:02:10.880]   alia episodes gets like a half a million views, you know? And then now this is my career, but it
[04:02:10.880 --> 04:02:15.760]   was sort of very looking back on it, incredibly contingent. Yeah. The things worked out the right
[04:02:15.760 --> 04:02:20.800]   way. Yeah. I mean, look, if, if the AGI stuff goes down, you know, it will be, uh, it'll be the most
[04:02:20.800 --> 04:02:25.440]   important kind of like, you know, source of, uh, it'll be how maybe most of the people who kind of
[04:02:25.440 --> 04:02:32.160]   end up feeling the AGI. Yeah. Yeah. Yeah. Also very much. Uh, you're very linked with the story
[04:02:32.160 --> 04:02:39.840]   in many ways. First, the, um, the, I got like a $20,000 grant from, uh, a future fund right out
[04:02:39.840 --> 04:02:45.920]   of college. And that's sustained to me for six months or however long it was. Yeah. And without
[04:02:45.920 --> 04:02:49.840]   that, I wouldn't, it's kind of crazy. Yeah. 10 grand or what was it? No, it's just, it's tiny,
[04:02:49.840 --> 04:02:55.200]   but it goes to show kind of how far small grants can go. Yeah. Sort of the emergent ventures too.
[04:02:55.200 --> 04:03:01.120]   Exactly. The emergent ventures and the, um, well, the last year I've been in San Francisco,
[04:03:01.120 --> 04:03:06.640]   we've just been in close contact the entire time and just bouncing ideas back and forth. I've
[04:03:06.640 --> 04:03:11.440]   learned just basically the alpha I have. I think people would be surprised by how much I got from
[04:03:11.440 --> 04:03:16.080]   you, Sholto, Trenton, a couple others. Just, I mean, it's been, it's been an absolute pleasure.
[04:03:16.080 --> 04:03:22.800]   Yeah. Likewise. It's been super fun. Um, okay. So some random questions for you. Yeah. If you
[04:03:22.800 --> 04:03:26.880]   could convert to Mormonism and you could really believe it, would you do it? Would you push the
[04:03:26.880 --> 04:03:33.200]   button? Um, well, okay. Okay. Uh, before I answer that question, one sort of observation about the
[04:03:33.200 --> 04:03:36.800]   Mormons. So actually there's a, there's an article that actually made a big impact on me. I think it
[04:03:36.800 --> 04:03:40.560]   was by McKay Coppins at some point, you know, in the Atlantic or whatever about the Mormons.
[04:03:40.560 --> 04:03:45.440]   And I think the thing he kind of, you know, and I think he even was like interviewed Romney in it
[04:03:45.440 --> 04:03:49.120]   and so on. And I think the thing I thought was really interesting in this article was he kind
[04:03:49.120 --> 04:03:53.200]   of talked about how the experience of kind of growing up different, you know, growing up very
[04:03:53.200 --> 04:03:56.240]   unusual, especially if you grew up Mormon outside of Utah, you know, like the only person doesn't
[04:03:56.240 --> 04:04:01.760]   drink caffeine, you don't drink alcohol. You're kind of weird. Um, how that kind of got people
[04:04:01.760 --> 04:04:06.240]   prepared for being willing to be kind of outside of the norm later on. And like, you know, Mitt
[04:04:06.240 --> 04:04:10.000]   Romney, you know, was willing to kind of take stands alone, you know, in his party because he
[04:04:10.000 --> 04:04:14.320]   believed, you know, what he believed is true. And, um, I don't know, I mean, probably not to
[04:04:14.320 --> 04:04:17.760]   the same way, but I feel a little bit like this from kind of having grown up in Germany and having,
[04:04:17.760 --> 04:04:21.280]   you know, and really not having like this sort of German system and having been kind of an outsider
[04:04:21.280 --> 04:04:25.520]   or something. I think there's a certain amount in which kind of, yeah, growing up in an outsider
[04:04:25.520 --> 04:04:30.000]   gives you kind of unusual strength, um, later on to be kind of like, you know, willing to say what
[04:04:30.000 --> 04:04:34.320]   you think. And, um, anyway, so that is one thing I really appreciate about the Mormons, at least
[04:04:34.320 --> 04:04:37.360]   the ones that, you know, grow up outside of Utah. I think, you know, the fertility rates, they're
[04:04:37.360 --> 04:04:40.960]   good. They're important. They're going down as well. Right. Right. This is, this, this is the
[04:04:40.960 --> 04:04:45.120]   thing that really clinched the kind of fertility decline story. Yeah. Even the Mormons. Yeah. Even
[04:04:45.120 --> 04:04:48.400]   the Mormons. Right. You're like, oh, this is like a, you're sort of a good start. The Mormons
[04:04:48.400 --> 04:04:51.360]   will replace everybody. I don't know if it's good, but it's like, at least, you know, at least come
[04:04:51.360 --> 04:04:54.400]   on, you know, like it's at least some people will maintain high, you know, but it's no, no,
[04:04:54.400 --> 04:04:57.520]   you know, even the Mormons and sort of basically once the sort of these religious subgroups have
[04:04:57.520 --> 04:05:00.560]   high fertility rates, right. Once they kind of grow big enough, they become, you know,
[04:05:00.560 --> 04:05:04.640]   they're too close in contact with sort of normal society and become normalized more fertility rates
[04:05:04.640 --> 04:05:09.440]   drop from. I remember the exact numbers, maybe like four to two in the course of 10, 20 years.
[04:05:09.440 --> 04:05:13.040]   Um, anyway, so it's like, you know, now people point to the Amish or whatever, but I'm just like,
[04:05:13.040 --> 04:05:16.240]   it's probably just not scalable. And if you grow big enough, then there's just like, you know,
[04:05:16.240 --> 04:05:20.480]   the sort of like, you know, the sort of like overwhelming force of modernity kind of gets you.
[04:05:20.480 --> 04:05:24.000]   Yeah. Um, no, if I could convert to Mormonism, look, I think there's something,
[04:05:24.000 --> 04:05:28.880]   I don't believe it. Right. If I believed it, I obviously would convert to Mormonism. Right.
[04:05:28.880 --> 04:05:31.840]   Cause it's, you gotta, you gotta, you can choose a world in which you do believe it.
[04:05:32.960 --> 04:05:40.640]   Um, I think there's something really valuable in kind of believing in something greater than
[04:05:40.640 --> 04:05:48.320]   yourself and believing, having a certain amount of faith. Um, you do, right. And, and, and, and,
[04:05:48.320 --> 04:05:53.520]   you know, um, you know, there's a, you know, feeling some sort of duty to the thing greater
[04:05:53.520 --> 04:05:57.680]   than yourself. Um, and you know, maybe my version of this is somewhat different. You know, I think
[04:05:57.680 --> 04:06:01.600]   I feel some sort of duty to like, I feel like there's some sort of historical weight on like
[04:06:01.600 --> 04:06:05.200]   how this might play out. And I feel some sort of duty to like, make that go well. I feel some sort
[04:06:05.200 --> 04:06:10.600]   of duty to, you know, our country, to the national security of the United States. And, um, um,
[04:06:10.600 --> 04:06:17.280]   you know, I think, I think that, I think that can be a force for a lot of good.
[04:06:17.280 --> 04:06:21.480]   I, the, um, going back to the opening, I think just, uh,
[04:06:21.480 --> 04:06:30.800]   the, the thing that's especially impressive about that is look, there's people who at the company
[04:06:30.800 --> 04:06:37.600]   who have through years and decades of building up savings from working in tech have probably
[04:06:37.600 --> 04:06:44.640]   tens of millions liquid more than that in terms of their equity. And the person, very many people
[04:06:44.640 --> 04:06:51.040]   were concerned about the clusters and the Middle East and, uh, the secrets leaking to China and all
[04:06:51.040 --> 04:06:56.640]   these things, but the person who actually made a hassle about it, I didn't think hassling people
[04:06:56.640 --> 04:07:03.040]   was so underrated. I think that I have one person who made a hassle about it is the 22 year old who
[04:07:03.040 --> 04:07:08.640]   has less than a year of the company who doesn't have savings built up, uh, who, who, who isn't
[04:07:08.640 --> 04:07:14.000]   like a solidified member of the, um, yeah, I think that's the sort of like, and maybe, maybe it's me
[04:07:14.000 --> 04:07:18.240]   being naive and, you know, not having a, knowing how big companies work and, you know, but like,
[04:07:18.240 --> 04:07:22.080]   there's a, you know, I think sometimes a bit of a speech geontologist, you know, I kind of believe
[04:07:22.080 --> 04:07:26.160]   in saying what you think. Um, sometimes friends tell me I should be more of a speech consequentialist.
[04:07:26.160 --> 04:07:34.080]   No, I think, um, I, I, I, I really think the amount of people who, when they have the
[04:07:34.080 --> 04:07:39.440]   opportunity to talk to the person, we'll just bring up the thing I've been with you in multiple
[04:07:39.440 --> 04:07:43.200]   contexts. And I guess I shouldn't reveal who the person is or what the context was, but I've just
[04:07:43.200 --> 04:07:49.120]   been like very impressed that the dinner begins. And by the end, somebody who has a major voice
[04:07:49.120 --> 04:07:55.680]   in how things go is seriously thinking about a worldview they would have found incredibly alien
[04:07:55.680 --> 04:08:00.720]   before the dinner or something. Um, and I've been impressed with it, like, just like, just give them
[04:08:00.720 --> 04:08:07.680]   the spiel and hassle them. Um, um, I mean, look, I just, I think, I think I feel this stuff pretty
[04:08:07.680 --> 04:08:11.040]   viscerally now. You know, I think there's a time, you know, there's a time when I thought about this
[04:08:11.040 --> 04:08:14.880]   stuff a lot, but it was kind of like econ models and like, you know, kind of like these sort of
[04:08:14.880 --> 04:08:19.120]   theoretical abstractions and, you know, you talk about human brain size or whatever. And I think,
[04:08:19.120 --> 04:08:24.480]   you know, since, um, I think since at least last year, you know, I feel like, you know, I feel like
[04:08:24.480 --> 04:08:29.680]   I can see it, you know, and I, I just, I feel it. Um, and I think I can, like, you know, I can sort
[04:08:29.680 --> 04:08:34.320]   of see the cluster that AGI and I can see the kind of rough combination of algorithms and the people
[04:08:34.320 --> 04:08:39.520]   that be involved in how this is going to play out. And, um, you know, I think, um, look, we'll see
[04:08:39.520 --> 04:08:43.680]   how it plays out. There's many ways this could be wrong. There's many ways it could go, but I think
[04:08:43.680 --> 04:08:49.520]   this could get very real. Yeah. Should we talk about what you're up to next? Sure. Yeah. Okay.
[04:08:49.520 --> 04:08:53.040]   So you're starting an investment firm. Yep. Anchor investments from
[04:08:53.040 --> 04:08:56.560]   Nat Friedman, Daniel Gross, Patrick Lawson, John Collison.
[04:08:56.560 --> 04:09:03.280]   First of all, why is this thing to do? You believe the AGI is coming in a few years.
[04:09:03.280 --> 04:09:08.960]   Why, why the investment firm? A good question. Fair question. All right. So, I mean, a couple
[04:09:08.960 --> 04:09:11.840]   of things. One is just, you know, I think we talked about this earlier, but it's like the
[04:09:11.840 --> 04:09:15.360]   screen doesn't go blank, you know, when sort of AGI or superintelligence happens. I think people
[04:09:15.360 --> 04:09:18.560]   really underrate the sort of basically the sort of decade after it, you have the intelligence
[04:09:18.560 --> 04:09:22.560]   explosion. That's maybe the most sort of wild period. But I think the decade after is also
[04:09:22.560 --> 04:09:26.640]   going to be wild. And, you know, this combination of human institutions, but superintelligence,
[04:09:26.640 --> 04:09:29.840]   you have crazy kind of geopolitical things going on. You have the sort of broadening
[04:09:29.840 --> 04:09:34.400]   of this explosive growth. And, um, basically, yeah, I think it's going to be a really important
[04:09:34.400 --> 04:09:36.960]   period. I think capital really matter, you know, eventually, you know, like, you know,
[04:09:36.960 --> 04:09:41.440]   going to go to the stars, you know, going to go to the galaxies. Um, so anyway, so part of the
[04:09:41.440 --> 04:09:45.200]   answer is just like, look, I think don't do it done. Right. There's a lot of money to be made.
[04:09:45.200 --> 04:09:48.800]   You know, I think if AGI were priced in tomorrow, you could maybe make a hundred X probably you can
[04:09:48.800 --> 04:09:54.240]   make even way more than that because of the sequencing, um, and, um, and, and, you know,
[04:09:54.240 --> 04:10:00.240]   capital matters. Um, I think the other reason is just, um, you know, some amount of freedom
[04:10:00.240 --> 04:10:05.520]   and independence. And I think, you know, you know, I think there's some people who are very
[04:10:05.520 --> 04:10:09.440]   smart about this AI stuff and who are kind of like, see it coming. Um, but I think almost all
[04:10:09.440 --> 04:10:12.560]   of them, you know, are kind of, you know, constrained in various ways, right. They're
[04:10:12.560 --> 04:10:15.920]   in the labs, you know, they're in some, you know, some other position where they can't really talk
[04:10:15.920 --> 04:10:20.800]   about this stuff. And, um, you know, in some sense I've really admired sort of the thing you've done,
[04:10:20.800 --> 04:10:23.520]   which is, I think it's really important that there's sort of voices of reason on this stuff
[04:10:23.520 --> 04:10:27.360]   publicly or people who are in positions to kind of advise important actors and so on.
[04:10:27.360 --> 04:10:30.960]   And so I think there's a, you know, basically the thing this investment firm will be,
[04:10:30.960 --> 04:10:34.080]   will be kind of like, you know, a brain trust on AI. It's going to be all about situational
[04:10:34.080 --> 04:10:36.960]   awareness. We're going to have the best situational awareness in the business. You know,
[04:10:36.960 --> 04:10:39.840]   we're going to have way more situational business than any of the people who manage money in the,
[04:10:39.840 --> 04:10:43.120]   you know, New York. We're definitely going to, you know, we're going to do great on investing,
[04:10:43.120 --> 04:10:47.040]   but it's the same sort of situational awareness that I think is going to be important for
[04:10:47.040 --> 04:10:52.960]   understanding what's happening, being a voice of reason publicly and, and, and sort of being
[04:10:52.960 --> 04:10:59.600]   able to be in a position to advise. Yeah. Um, I, uh, there, the book about Peter Thiel. Yeah.
[04:10:59.600 --> 04:11:05.440]   They had, um, an interesting quote about his hedge fund. I think it got terrible returns. So
[04:11:05.440 --> 04:11:12.560]   this isn't the example, right? Right. But they had an interesting quote that it's, um,
[04:11:12.560 --> 04:11:17.760]   it's, uh, that it's like basically a think tank inside of a hedge fund. Yeah. And we're trying
[04:11:17.760 --> 04:11:23.520]   to build. Right. Yeah. So presumably you've thought about the ways in which these kinds
[04:11:23.520 --> 04:11:27.440]   of things can blow. There's a very, there's a lot of interesting business history books
[04:11:27.440 --> 04:11:34.400]   about people who got the thesis, right. Yeah. But timed it wrong where they, they buy that
[04:11:34.400 --> 04:11:38.160]   internet's going to be a big deal. Yeah. They sell at the wrong time and buy the wrong time
[04:11:38.160 --> 04:11:42.160]   during the.com boom. Yep. And so they miss out on the gains, even though they're right about the,
[04:11:42.160 --> 04:11:46.400]   yeah. Anyways. Yeah. Uh, what, what is that trick to preventing that kind of thing?
[04:11:46.400 --> 04:11:49.680]   Yeah. I mean, look, obviously you can't, you know, not blowing up as sort of like,
[04:11:49.680 --> 04:11:54.400]   you know, task number one and two or whatever. Um, I mean, you know, I think this investment
[04:11:54.400 --> 04:11:58.480]   firm is going to just be betting on AGI, you know, betting on AGI and super intelligence before the
[04:11:58.480 --> 04:12:01.760]   decade is out, taking that seriously, making the bets you would make, you know, if you took that
[04:12:01.760 --> 04:12:06.880]   seriously. So, you know, I think if that's wrong, you know, firm is not going to do that. Well,
[04:12:06.880 --> 04:12:09.760]   the thing you have to be resistant to is like, you have to be able to resist and get, you know,
[04:12:09.760 --> 04:12:13.600]   one or a couple or a few kind of individual calls. Right. You know, it's like AI stagnates for a
[04:12:13.600 --> 04:12:16.960]   year because of the data wall or like, you know, you got, you got the call wrong on like when
[04:12:16.960 --> 04:12:21.200]   revenue would go up. And, um, so anyway, that's pretty critical. You have to get timing. Right.
[04:12:21.200 --> 04:12:25.520]   Um, I do think in general that the sort of sequence of bets on the way to AGI is actually
[04:12:25.520 --> 04:12:30.640]   pretty critical. And I think a thing people underrate. So, all right. I mean, yeah. So like,
[04:12:30.640 --> 04:12:34.320]   where does the story start? Right. So like, obviously the sort of only bet over the last
[04:12:34.320 --> 04:12:41.360]   year was NVIDIA. Um, and, um, you know, it's obvious now very few people did it. Um, this
[04:12:41.360 --> 04:12:45.280]   is sort of also, you know, classic debate I and a friend had with another colleague of ours where
[04:12:45.280 --> 04:12:49.040]   this colleague was really into TSM, you know, TSMC. And he was just kind of like, well, you know,
[04:12:49.040 --> 04:12:52.640]   like these fabs are going to be so valuable. And also like NVIDIA, there's just a lot of
[04:12:52.640 --> 04:12:56.320]   idiosyncratic risk, right? It's like, maybe somebody else makes better GPUs. That was
[04:12:56.320 --> 04:13:00.720]   basically right. But sort of only NVIDIA had the AI beta, right? Because only NVIDIA was kind of
[04:13:00.720 --> 04:13:04.960]   like large fraction AI. The next few doublings would just like meaningfully explode their revenue.
[04:13:04.960 --> 04:13:08.560]   Whereas TSMC was, you know, a couple percent AI. So even though there's going to be a few
[04:13:08.560 --> 04:13:12.320]   doublings of AI, not going to make that big of an impact. All right. So it's sort of like
[04:13:12.320 --> 04:13:17.520]   the only place to find the AI beta basically was NVIDIA for a while. Um, you know, now it's
[04:13:17.520 --> 04:13:22.800]   broadening, right? So now TSM is like, you know, 20% AI by like 27 or something is what they're
[04:13:22.800 --> 04:13:25.920]   saying. You know, one more doubling, it'll be kind of like a large fraction of what they're
[04:13:25.920 --> 04:13:29.280]   doing. And, um, you know, there's a whole, you know, whole stack, you know, there's like,
[04:13:29.280 --> 04:13:33.360]   you know, there's people making memory and co-ops and, um, you know, power, you know,
[04:13:33.360 --> 04:13:36.880]   utilities companies are starting to get excited about AI and they're like, oh, it'll, you know,
[04:13:36.880 --> 04:13:42.080]   power production in the United States will grow, you know, not 2.5%, 5% of the next five years.
[04:13:42.080 --> 04:13:48.560]   And I'm like, no, it'll grow more. Um, you know, at some point, you know, you know, you know,
[04:13:48.560 --> 04:13:51.760]   like a Google or something becomes interesting and, you know, people are excited about them with
[04:13:51.760 --> 04:13:55.440]   AI because it's like, oh, you know, AI revenue will be, you know, 10 billion or tens of billions.
[04:13:55.440 --> 04:13:58.880]   And I'm kind of like, ah, I don't really care about them before then I care about it. You know,
[04:13:58.880 --> 04:14:02.560]   once it, you know, once you get the AI beta, right. And so at some point, you know, Google
[04:14:02.560 --> 04:14:06.000]   will get, you know, a hundred billion dollars of revenue from AI. Probably their stock will
[04:14:06.000 --> 04:14:08.960]   explode. You know, they're going to become, you know, 5 trillion, 10 trillion dollar company.
[04:14:08.960 --> 04:14:12.400]   Anyway, so the timing there is very important. You have to get the timing right. You have to
[04:14:12.400 --> 04:14:15.520]   get the sequence right. You know, at some point actually, I think like, you know, there's going
[04:14:15.520 --> 04:14:20.240]   to be real tailwind to equities from real interest rates. Right. So basically in these sort of
[04:14:20.240 --> 04:14:24.960]   explosive growth worlds, you would expect real interest rates to go up a lot, both on the sort
[04:14:24.960 --> 04:14:29.520]   of like, you know, basically both sides of the equation, right. On the supply side or on the
[04:14:29.520 --> 04:14:34.800]   sort of demand for money side, because, you know, people are going to be making these crazy
[04:14:34.800 --> 04:14:37.920]   investments, you know, initially in clusters and then in the robo factories or whatever. Right.
[04:14:37.920 --> 04:14:43.280]   And so they're going to be borrowing like crazy. They want all this capital, higher ROI. And then
[04:14:43.280 --> 04:14:48.560]   on the sort of like consumer saving side, right. To like, you know, to give up all this capital,
[04:14:48.560 --> 04:14:51.920]   you know, the sort of like Euler equation standard sort of intertemporal transfer,
[04:14:51.920 --> 04:14:55.760]   you know, trade off of consumption. Standard.
[04:14:55.760 --> 04:15:02.000]   Some of our friends have a paper on this, you know, basically if you expect, you know,
[04:15:02.000 --> 04:15:05.040]   if consumers expect real growth rates to be higher, you know, interest rates are going
[04:15:05.040 --> 04:15:08.400]   to be higher because they're less willing to give up consumption, you know, consumption in
[04:15:08.400 --> 04:15:13.120]   the West. They're less willing to give up consumption today for consumption in the future.
[04:15:13.120 --> 04:15:16.400]   Anyway, so at some point, real interest rates will go up. If sort of ADA is greater than one,
[04:15:16.400 --> 04:15:20.640]   that actually means equities, you know, higher growth rate expectations mean equities go down
[04:15:20.640 --> 04:15:24.080]   because the sort of interest rate effect outweighs the growth rate effect. And so,
[04:15:24.080 --> 04:15:28.080]   you know, at some point there's like the big bond short. You got to get that right. You got to get
[04:15:28.080 --> 04:15:31.760]   it right that, you know, nationalization, you know, like you got, you know. So there's this
[04:15:31.760 --> 04:15:35.760]   whole sequence of things. You got to get that right. Unknown unknowns. Unknown unknowns. Yeah.
[04:15:35.760 --> 04:15:39.360]   And so you've look, you've got to be really, really careful about your overall risk positioning,
[04:15:39.360 --> 04:15:42.560]   right. Because, you know, if you expect these kind of crazy events to play out,
[04:15:42.560 --> 04:15:46.800]   there's going to be crazy things you didn't see. You know, you do also want to make the sort of
[04:15:46.800 --> 04:15:50.320]   kind of bets that are tailored to your scenarios in the sense of like, you know, you want to find
[04:15:50.320 --> 04:15:55.200]   bets that are bets on the tails. Right. You know, I don't think anyone is expecting, you know,
[04:15:55.200 --> 04:15:58.880]   interest rates to go above, you know, 10 percent, like real interest rates. But, you know, I think
[04:15:58.880 --> 04:16:03.360]   there's at least a serious chance of that, you know, before the decade is out. And so, you know,
[04:16:03.360 --> 04:16:08.240]   maybe there's some like cheap insurance you can buy on that. Very silly question. Yeah. In these
[04:16:08.240 --> 04:16:14.080]   worlds. Yeah. Are financial markets where you make these kinds of bets going to be respected?
[04:16:14.080 --> 04:16:18.880]   And like, you know, like, is my fidelity account going to mean anything when we have
[04:16:18.880 --> 04:16:23.520]   their 50 percent economic growth? Like who's who's like, we got to respect his property rights into
[04:16:23.520 --> 04:16:27.040]   it. The bond for the sort of 50 percent economic growth. That's pretty deep into it. I mean,
[04:16:27.040 --> 04:16:29.920]   again, there's this whole sequence of things. But yeah, no, I think property rates will be
[04:16:29.920 --> 04:16:34.640]   restricted again in the sort of modal world. The project. Yeah. At some point, at some point,
[04:16:34.640 --> 04:16:37.360]   there's going to be figuring out the property rights for the galaxies. You know, that'll be
[04:16:37.360 --> 04:16:43.600]   interesting. That will be interesting. So there's an interesting question about. Yeah.
[04:16:43.600 --> 04:16:50.240]   Going back to your strategy about, well, the 30s will really matter a lot about how the rest of
[04:16:50.240 --> 04:16:54.800]   the future goes. Yeah. And you want to be in a position of influence by that point because of
[04:16:54.800 --> 04:17:00.960]   capital. It's worth considering, as far as I know, there's probably a whole bunch of literature on
[04:17:00.960 --> 04:17:07.520]   this. I'm just riffing. But the the landed gentry during the before the beginning of the Industrial
[04:17:07.520 --> 04:17:13.840]   Revolution, I'm not sure if they were able to leverage their position in a sort of Georgist
[04:17:13.840 --> 04:17:23.760]   or Piketty type sense in order to accrue the returns that were realized through the Industrial
[04:17:23.760 --> 04:17:28.240]   Revolution. Yeah. And I don't know what happened at some point. They were just weren't the landed
[04:17:28.240 --> 04:17:35.440]   gentry. But I'd be concerned that even if you make great investment calls, you'll be like the guy who
[04:17:35.440 --> 04:17:39.520]   owned a lot of land, farmland before the Industrial Revolution. And like the guy who's actually going
[04:17:39.520 --> 04:17:43.600]   to make a bunch of money is the one of the C mentioned. Even if he doesn't make that much
[04:17:43.600 --> 04:17:49.120]   money, most of the benefits are widely diffused and so forth. I mean, I think the analog is like
[04:17:49.120 --> 04:17:52.880]   you sell your land, you put it all in sort of that, you know, the people who are building the
[04:17:52.880 --> 04:18:00.240]   new industry. I mean, I think the sort of like real appreciating asset, you know, for me is human
[04:18:00.240 --> 04:18:04.400]   capital. Right. Yeah. No, look, I'm serious. Right. It's like, you know, there's something
[04:18:04.400 --> 04:18:07.200]   about like, you know, I don't know. It's like Valedictorian of Columbia. You know, the thing
[04:18:07.200 --> 04:18:10.480]   that made you special is you're smart. Right. But actually, like, you know, that might not matter in
[04:18:10.480 --> 04:18:15.280]   like four years, you know, because it's actually automatable. Right. And so anyway, a friend joke
[04:18:15.280 --> 04:18:20.000]   that the sort of investment was perfectly hedged for me. It's like, you know, either like AGI this
[04:18:20.000 --> 04:18:23.920]   decade and, yeah, your human capital is appreciated, but you've turned that into financial
[04:18:23.920 --> 04:18:28.400]   capital or, you know, like no AGI this decade, in which case maybe the firm doesn't do that well.
[04:18:28.400 --> 04:18:30.800]   But, you know, you're still in your 20s and you're still smart.
[04:18:30.800 --> 04:18:40.080]   Excellent. And what's your story for why AGI hasn't been priced in the story? Financial markets
[04:18:40.080 --> 04:18:47.360]   are supposed to be very efficient. It's very, very hard to get an edge here. Naively, you just say,
[04:18:47.360 --> 04:18:51.280]   well, I've looked at the scaling curves and they imply that we're going to be buying much more
[04:18:51.280 --> 04:18:56.880]   compute and energy than the analysts realize. Yeah. Shouldn't those analysts be broke by now?
[04:18:56.880 --> 04:19:03.840]   What's going on? Yeah. I mean, I used to be a true EMH guy as an economist. You know, yeah,
[04:19:03.840 --> 04:19:10.240]   I am. You know, I think the thing I, you know, change my mind on is that I think there can be
[04:19:10.240 --> 04:19:14.400]   kind of groups of people, smart people, you know, who are, you know, say they're in San Francisco
[04:19:14.400 --> 04:19:18.480]   who do just have off over the rest of society and kind of seeing the future.
[04:19:18.480 --> 04:19:23.600]   And so like COVID, right? Like, I think there's just honestly kind of similar group of people
[04:19:23.600 --> 04:19:28.960]   who just saw that and called it completely correctly. And, you know, they showed at the
[04:19:28.960 --> 04:19:40.160]   market, they did really well, you know, a bunch of other sort of things like that. So, you know,
[04:19:41.920 --> 04:19:46.480]   why is AGI not priced in? You know, it's sort of, you know, why hasn't the government nationalized
[04:19:46.480 --> 04:19:50.080]   the labs yet? Right. It's like, you know, this, you know, society hasn't priced it in yet and
[04:19:50.080 --> 04:19:55.520]   sort of it hasn't completely diffused. And, you know, again, it might be wrong. Right. But I just
[04:19:55.520 --> 04:20:01.920]   think sort of, you know, not that many people take these ideas seriously. Yeah. Yeah. Yeah.
[04:20:01.920 --> 04:20:07.280]   A couple of other sort of ideas that I was playing around with, with regards to
[04:20:07.280 --> 04:20:13.840]   we didn't get a chance to talk about, but the systems competition. Yeah. There's a very
[04:20:13.840 --> 04:20:18.480]   interesting, one of my favorite books about World War Two is a Victor Davis Hanson
[04:20:18.480 --> 04:20:27.920]   summary of everything. And he explains why the allies made better decisions than the Axis.
[04:20:27.920 --> 04:20:32.400]   Why did they? And so obviously, there were some decisions the Axis made that were pretty like
[04:20:32.400 --> 04:20:36.800]   blitzkrieg, whatever. That was sort of by accident though. Well, in what sense? That they just had
[04:20:36.800 --> 04:20:40.640]   the infrastructure left over? Well, no, I mean, the sort of, I think, I mean, I don't, I mean,
[04:20:40.640 --> 04:20:44.560]   I think sort of my read of it is blitzkrieg wasn't kind of some like a genius strategy. It was just
[04:20:44.560 --> 04:20:48.400]   kind of, it was like more like their hand was forced. I mean, this is sort of the very Adam
[04:20:48.400 --> 04:20:52.880]   Tuzian story of World War Two. Right. But it was, you know, there's sort of this long war versus
[04:20:52.880 --> 04:20:56.160]   short war. I think it's actually kind of an important concept. I think sort of Germany
[04:20:56.160 --> 04:21:00.960]   realized that if they were in a long war, including the United States, you know, they would not be
[04:21:00.960 --> 04:21:05.840]   able to compete industrially. So their only path to victory was like make it a short war. Right.
[04:21:05.840 --> 04:21:10.400]   And that sort of worked much more spectacularly than they thought. Right. And sort of take over
[04:21:10.400 --> 04:21:14.480]   France and take over much of Europe. And so then, you know, the decision to invade the Soviet Union,
[04:21:14.480 --> 04:21:19.840]   it was, you know, it was, it was, look, if it was, it was about the Western front in some sense,
[04:21:19.840 --> 04:21:23.120]   because it was like, we've got to get the resources. You know, we don't, we're actually,
[04:21:23.120 --> 04:21:26.480]   we don't actually have a bunch of the stuff we need, like, you know, oil and so on. You know,
[04:21:26.480 --> 04:21:29.840]   Auschwitz was actually just this giant chemical plant to make kind of like synthetic oil and a
[04:21:29.840 --> 04:21:35.040]   bunch of these things. It's the largest industrial project in Nazi Germany. And so, you know, and
[04:21:35.040 --> 04:21:38.480]   sort of they thought, well, you know, we completely crushed them in World War One. You know, it'll be
[04:21:38.480 --> 04:21:42.720]   easy. We'll invade them, we'll get the resources, and then we can fight on the Western front. And
[04:21:42.720 --> 04:21:46.080]   even during the sort of whole invasion of the Soviet Union, even though kind of like a large
[04:21:46.080 --> 04:21:49.360]   amount of the sort of, you know, the sort of deaths happened there, you know, like a large
[04:21:49.360 --> 04:21:52.720]   fraction of German industrial production was actually, you know, like planes and naval,
[04:21:52.720 --> 04:21:58.000]   you know, and so on. It was directed, you know, towards the Western front and towards the Western
[04:21:58.000 --> 04:22:03.040]   allies. Well, and then so the point that Hanson was making was. By the way, I think this concept
[04:22:03.040 --> 04:22:05.920]   of like long war and short war is kind of interesting with respect to thinking about
[04:22:05.920 --> 04:22:10.720]   the China competition, which is like, you know, I worry a lot about kind of, you know,
[04:22:10.720 --> 04:22:17.040]   decline of sort of American, like latent American industrial capacity. You know, like I think China
[04:22:17.040 --> 04:22:22.880]   builds like 200 times more ships than we do right now. You know, some crazy way. And so it's like,
[04:22:22.880 --> 04:22:27.120]   maybe we have this superiority, say, in the non-AI worlds, we have the superiority in military
[04:22:27.120 --> 04:22:31.280]   material, kind of like win a short war, at least, you know, kind of defend Taiwan in some sense.
[04:22:31.280 --> 04:22:35.600]   But like if it actually goes on, you know, it's like maybe China is much better able to mobilize,
[04:22:35.600 --> 04:22:40.960]   mobilize industrial resources in a way that like we just don't have that same ability anymore.
[04:22:40.960 --> 04:22:45.600]   I think this is also relevant to the AI thing in the sense of like, if it comes down to sort
[04:22:45.600 --> 04:22:49.520]   of a game about building, right, including like maybe AGI takes the trillion dollar cluster,
[04:22:49.520 --> 04:22:52.960]   not the hundred billion dollar cluster, maybe, or even maybe AGI takes the, you know,
[04:22:52.960 --> 04:22:56.720]   is on the hundred billion dollar cluster. But, you know, it really matters if you can run,
[04:22:56.720 --> 04:23:00.000]   you know, 10x, you can do one more order of magnitude of compute for your super intelligence
[04:23:00.000 --> 04:23:05.200]   or whatever, that, you know, maybe right now they're behind, but they just have this sort
[04:23:05.200 --> 04:23:09.360]   of like raw latent industrial capacity to outbuild us. And that matters both in the
[04:23:09.360 --> 04:23:13.600]   run up to AGI and after, right, where it's like, you have the super intelligence on your cluster.
[04:23:13.600 --> 04:23:17.360]   Now it's time to kind of like expand the explosive growth. And, you know, like,
[04:23:17.360 --> 04:23:21.360]   will we let the robo factories run wild? Like, maybe not, but like maybe China will.
[04:23:21.360 --> 04:23:23.760]   We're like, you know, will we, will, yeah, will we produce the, how many,
[04:23:23.760 --> 04:23:27.760]   how many of the drones will we produce? And I think, yeah, so there's some sort of like
[04:23:27.760 --> 04:23:30.400]   outbuilding in the industrial explosion that I worry about. You've got to be one of the few people
[04:23:30.400 --> 04:23:35.360]   in the world who is both concerned about alignment, but also wants to make sure that we'll let the
[04:23:35.360 --> 04:23:43.840]   robot factories proceed once we get the ASI to beat out China. It's all, it's all part of the
[04:23:43.840 --> 04:23:51.040]   picture. Yeah. Yeah. Yeah. And, but, by the way, speaking of the ASIs and the robot factories,
[04:23:51.040 --> 04:23:56.320]   one of the interesting things, one of the interesting things, there's this question of
[04:23:56.320 --> 04:24:01.760]   what you do with industrial scale intelligence and obviously it's not chatbots, but it's a,
[04:24:01.760 --> 04:24:09.920]   I think it's very hard to predict. Yeah. Yeah. But the, the, the, the history of oil is very
[04:24:09.920 --> 04:24:15.360]   interesting where in the, I think it's in the 1860s that we figure out how to refine oil, some
[04:24:15.360 --> 04:24:22.320]   geologist. And so then standard oil gets started. There's this huge boom. It changes American
[04:24:22.320 --> 04:24:29.280]   politics. Entire legislators are getting bought out by oil interest and presidents are getting
[04:24:29.280 --> 04:24:34.880]   elected based on the divisions about oil and breaking them up and everything. And all of this
[04:24:34.880 --> 04:24:39.360]   has happened. Yeah. The world has never illusionized before the car has been invented.
[04:24:39.360 --> 04:24:47.120]   And so when the light bulb was invented, I think it was like 50 years after oil refining had been
[04:24:47.120 --> 04:24:52.880]   discovered as majority of standard oils history is before the car's invented the kerosene lamp.
[04:24:52.880 --> 04:24:56.800]   Exactly. So it's, it's just used for lighting. Then they thought oil would just no longer be
[04:24:56.800 --> 04:25:02.800]   relevant. Yeah. So there was a concern that standard old go bankrupt when, when the label
[04:25:02.800 --> 04:25:09.680]   was invented. And, but then there's sort of, you realize that there's immense amount of compressed
[04:25:09.680 --> 04:25:15.280]   energy here. You're going to have billions of gallons of this stuff a year. And it's hard to
[04:25:15.280 --> 04:25:20.240]   sort of predict in advance what you can do with that. And then later on, it turns out, oh,
[04:25:20.240 --> 04:25:27.200]   transportation cars with that's, that's, that's what it used for. Anyways, with intelligence,
[04:25:27.200 --> 04:25:32.080]   maybe one answer is the intelligence explosion. Right. But even after that, so you have all these
[04:25:32.080 --> 04:25:37.200]   ASIs and you have enough compute, especially the compute they'll build to run hundreds of millions
[04:25:37.200 --> 04:25:42.240]   of GPUs will hum. Yeah. But what are we doing with that? And it's very hard to predict in advance.
[04:25:42.240 --> 04:25:45.120]   And I think it'll be very interesting to figure out what the Jupiter brains will be doing.
[04:25:45.120 --> 04:25:55.680]   So look, there's situational awareness of where things stand now and we've gotten a good dose of
[04:25:55.680 --> 04:26:02.400]   that. The obviously a lot of the things we're talking about now, you couldn't have free judged
[04:26:02.400 --> 04:26:08.720]   many years back in the past. Right. And part of your role do implies that things will accelerate
[04:26:08.720 --> 04:26:14.080]   because of AI getting the process, but many other things that we are, that are unpredictable,
[04:26:14.080 --> 04:26:17.840]   fundamentally. Yep. Basically how people will react, how the political system will react,
[04:26:17.840 --> 04:26:23.280]   how foreign adversaries will react. Yep. That those things will become evident over time. Yep.
[04:26:23.280 --> 04:26:28.880]   So the situational awareness is not just knowing where the picture stands now, but being in a
[04:26:28.880 --> 04:26:35.280]   position to react appropriately to new information, to change your worldview as a result, to change
[04:26:35.280 --> 04:26:39.600]   your recommendations as a result. Yep. What is the appropriate way to think about
[04:26:39.600 --> 04:26:46.880]   situational awareness as a continuous process rather than as a one-time thing you realized?
[04:26:46.880 --> 04:26:50.960]   Yep. No, I think this is great. Look, I think there's, there's a sort of mental flexibility
[04:26:50.960 --> 04:26:54.320]   and willing to change your mind. That's really important. I actually think this is sort of like
[04:26:54.320 --> 04:26:58.480]   how a lot of brains have been broken in the AGI debate, right? Sort of the doomers who actually,
[04:26:58.480 --> 04:27:01.760]   you know, I think we're really prescient on AGI and thinking about the stuff, you know,
[04:27:01.760 --> 04:27:05.600]   like a decade ago, but, you know, they haven't actually updated on the empirical realities of
[04:27:05.600 --> 04:27:09.360]   deep learning. They're sort of like, their proposals are really kind of naive and unworkable.
[04:27:09.360 --> 04:27:12.800]   It doesn't really make sense. You know, there's people who come in with sort of a predefined
[04:27:12.800 --> 04:27:15.520]   ideology. There's kind of like, you know, the EAC's a little bit, you know, like they like to
[04:27:15.520 --> 04:27:19.600]   shitpost about technology, but they're not actually thinking through like, you know, I mean, either
[04:27:19.600 --> 04:27:22.880]   the sort of stagnationists who think this stuff is only going to be, you know, a chat bot. And so,
[04:27:22.880 --> 04:27:26.400]   of course it isn't risky, or they're just not thinking through the kind of like actually immense
[04:27:26.400 --> 04:27:30.880]   national security implications and how that's going to go. And, you know, I actually think
[04:27:30.880 --> 04:27:34.720]   there's kind of a risk in kind of like having written this stuff down and like put it online.
[04:27:34.720 --> 04:27:39.360]   And, you know, there's, I think this sometimes happens to people as a sort of calcification
[04:27:39.360 --> 04:27:43.440]   of the worldview, because now they've publicly articulated this position and, you know, maybe
[04:27:43.440 --> 04:27:47.760]   there's some evidence against it, but they're clinging to it. And so I actually, you know,
[04:27:47.760 --> 04:27:51.280]   I want to give the big disclaimer on like, you know, I think it's really valuable to paint a
[04:27:51.280 --> 04:27:56.320]   sort of very concrete and visceral picture. I think this is currently my best guess on how
[04:27:56.320 --> 04:28:03.760]   this decade will go. I think if it goes anywhere like this, it will be wild. But, you know, given
[04:28:03.760 --> 04:28:09.200]   the rapid pace of progress, we're going to keep getting a lot more information. And, you know,
[04:28:09.200 --> 04:28:15.680]   I think it's important to sort of keep your head on straight about that. You know, I feel like the
[04:28:15.680 --> 04:28:20.000]   most important thing here is that, you know, and this relates to some of the stuff we've talked
[04:28:20.000 --> 04:28:25.920]   about and, you know, sort of the world being surprisingly small and so on, you know, I feel
[04:28:25.920 --> 04:28:28.880]   like I used to have this worldview of like, look, there's important things happening in the world,
[04:28:28.880 --> 04:28:31.600]   but there's like people who are taking care of it, you know, and there's like the people in
[04:28:31.600 --> 04:28:37.120]   government and there's again, even like AI labs have idealized and people are on it, you know,
[04:28:37.120 --> 04:28:41.760]   surely they must be on it. Right. And I think just some of this personal experience, even seeing how
[04:28:41.760 --> 04:28:46.000]   kind of COVID went, you know, people aren't necessarily, there's not some specific, there's
[04:28:46.000 --> 04:28:49.520]   not somebody else who's just kind of on it and making sure this goes well, however it goes.
[04:28:49.520 --> 04:28:57.040]   You know, the thing that I think will really matter is that there are sort of good people
[04:28:57.040 --> 04:29:01.760]   who take this stuff as seriously as it deserves and who are willing to kind of take the
[04:29:01.760 --> 04:29:05.360]   implication seriously, who are willing to, you know, have situational awareness, are willing
[04:29:05.360 --> 04:29:12.480]   to change their minds, are willing to sort of stare the picture in the face. And, you know,
[04:29:12.480 --> 04:29:17.840]   I'm counting on those good people. All right. That's a great place to close Leopold.
[04:29:17.840 --> 04:29:18.800]   Thanks so much Tarkash.
[04:29:18.800 --> 04:29:26.320]   Hey everybody. I hope you enjoyed that episode with Leopold. There's actually one more riff
[04:29:26.320 --> 04:29:31.120]   about German history that he had after a break and it was pretty interesting. So I didn't want
[04:29:31.120 --> 04:29:36.400]   to cut it out. So I've just included it after this outro. You can advertise on the show now.
[04:29:36.400 --> 04:29:41.920]   So if you're interested, you can reach out at the form in the description below. Other than that,
[04:29:41.920 --> 04:29:46.480]   the most helpful thing you can do is just share the episode if you enjoyed it. Send it to group
[04:29:46.480 --> 04:29:51.600]   chats, Twitter, wherever else you think people who might like this episode might congregate.
[04:29:51.600 --> 04:29:56.560]   And other than that, I guess here's this riff on Frederick the Great. See you on the next one.
[04:29:56.560 --> 04:30:01.120]   I mean, I think the actual funny thing is, you know, a lot of this sort of German history stuff
[04:30:01.120 --> 04:30:05.600]   we've talked about is sort of like not actually stuff I learned in Germany. It's sort of like
[04:30:05.600 --> 04:30:08.720]   stuff that I learned after. And there's actually, you know, a funny thing where I kind of would go
[04:30:08.720 --> 04:30:12.400]   back to Germany over Christmas or whatever and suddenly understand the street names. You know,
[04:30:12.400 --> 04:30:16.400]   it's like, you know, Gneisenau and Scharnhorst and then all these like Prussian military reformers.
[04:30:16.400 --> 04:30:20.000]   And you're like, finally understood, you know, Sanssouci. And you're like, it was for Frederick,
[04:30:20.000 --> 04:30:25.040]   you know, Frederick the Great is this really interesting figure where he's this sort of,
[04:30:25.040 --> 04:30:34.000]   in some sense, kind of like gay lover of arts, right? Where he, you know, he hates speaking
[04:30:34.000 --> 04:30:37.920]   German. He only wants to speak French. You know, he like plays the flute. He composes. He has all
[04:30:37.920 --> 04:30:45.760]   the sort of great artists of his day, you know, over at Sanssouci. And he actually had this sort
[04:30:45.760 --> 04:30:50.960]   of like really tough upbringing where his father was this sort of like really stern sort of Prussian
[04:30:50.960 --> 04:30:58.640]   military man. And he had had a Frederick the Great as a child, as sort of a 17 year old or whatever.
[04:30:58.640 --> 04:31:06.240]   He basically had a male lover. And what his father did was imprison his son and then, I think, hang
[04:31:06.240 --> 04:31:11.040]   his male lover in front of him. And again, his father was this kind of very stern Prussian guy.
[04:31:11.040 --> 04:31:15.600]   He was this kind of gay, you know, lover of arts. But then later on, Frederick the Great turns out
[04:31:15.600 --> 04:31:22.320]   to be this like, you know, one of the most kind of like, you know, successful kind of Prussian
[04:31:22.320 --> 04:31:26.560]   conquerors, right? Like he gets Silesia. He wins the Seven Years War. You know, also, you know,
[04:31:26.560 --> 04:31:30.240]   amazing military strategists, you know, amazing military strategy at the time consisted of like,
[04:31:30.240 --> 04:31:34.480]   he was able to like flank the army and that was crazy, you know, and that was brilliant.
[04:31:34.480 --> 04:31:38.480]   And then they like almost lose the Seven Years War at the very end, you know, the sort of,
[04:31:38.480 --> 04:31:43.040]   you know, the Russian czar changes. And he's like, ah, I'm actually kind of a Prussian stan.
[04:31:43.040 --> 04:31:45.840]   You know, I think I'm like, I'm into this stuff. And then he lets, you know,
[04:31:45.840 --> 04:31:52.400]   let's Frederick the Great loose and let's let the army be okay. And anyway, sort of like a,
[04:31:53.440 --> 04:32:06.160]   yeah, kind of bizarre, interesting figure in German history.

