
[00:00:00.000 --> 00:00:08.640]   The thing that DBT does is try to get to a ground truth that everybody inside of an organization
[00:00:08.640 --> 00:00:12.280]   can agree on, and so we can at least have productive disagreement.
[00:00:12.280 --> 00:00:16.660]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:16.660 --> 00:00:18.820]   and I'm your host, Lukas Biewald.
[00:00:18.820 --> 00:00:23.920]   Today I'm talking with Tristan Handy, who is the founder and CEO of DBT Labs.
[00:00:23.920 --> 00:00:29.280]   DBT, for those of you who don't know, has gone from an open source project to one of
[00:00:29.280 --> 00:00:35.480]   the most critical components of the modern data stack in under four or five years.
[00:00:35.480 --> 00:00:40.680]   It's been incredible to watch from the outside, and I was excited to talk to him about it.
[00:00:40.680 --> 00:00:45.280]   You're probably our first person that isn't actively working in the ML field, but data
[00:00:45.280 --> 00:00:50.480]   is so critical and tangential that I thought you'd bring a really interesting perspective,
[00:00:50.480 --> 00:00:53.360]   and you might need to make it a little more basic for our audience.
[00:00:53.360 --> 00:00:58.640]   I thought I would start out by asking you to describe what DBT is, because in your world,
[00:00:58.640 --> 00:01:03.120]   it's a really famous, well-known product, but I think for a lot of ML people, they might
[00:01:03.120 --> 00:01:06.320]   not even know what it is.
[00:01:06.320 --> 00:01:07.320]   Yeah.
[00:01:07.320 --> 00:01:13.480]   Gosh, I sometimes get challenged to answer this question from...
[00:01:13.480 --> 00:01:17.280]   Imagine my aunt is on the other end of the conversation, and then it's really challenging.
[00:01:17.280 --> 00:01:19.000]   I know the feeling.
[00:01:19.000 --> 00:01:20.000]   Yeah, right?
[00:01:20.000 --> 00:01:24.520]   I do data stuff.
[00:01:24.520 --> 00:01:31.480]   If you're in more traditional BI and analytics, your world has changed very significantly
[00:01:31.480 --> 00:01:39.560]   over the past 10, 12 years, really driven by the rise of the modern cloud data warehouse.
[00:01:39.560 --> 00:01:45.880]   So now everybody has access to high-performance, scalable, SQL-based compute.
[00:01:45.880 --> 00:01:51.760]   You can just throw data in there, and by and large, queries are just fast.
[00:01:51.760 --> 00:01:56.360]   There's this whole ecosystem of stuff that has arisen to get data in, to organize the
[00:01:56.360 --> 00:02:00.480]   data that's in the warehouse, to report on it, et cetera.
[00:02:00.480 --> 00:02:07.520]   The whole industry had to rebuild all of its tooling around the cloud data warehouse, because
[00:02:07.520 --> 00:02:15.800]   the way that stuff worked before was all constrained around speed and size of data.
[00:02:15.800 --> 00:02:23.120]   The problem that DBT addresses is that now there's this massive profusion of different
[00:02:23.120 --> 00:02:24.920]   types of data that show up in the warehouse.
[00:02:24.920 --> 00:02:28.800]   You get Fivetran or other products that their whole job is to...
[00:02:28.800 --> 00:02:31.880]   You push a button, and now a whole new data source shows up.
[00:02:31.880 --> 00:02:37.000]   But it shows up in exactly the format that it lived in, in the source.
[00:02:37.000 --> 00:02:43.160]   So you connect Facebook ads, and you get 150 tables that mapped one-to-one to a Facebook
[00:02:43.160 --> 00:02:45.360]   ads endpoint.
[00:02:45.360 --> 00:02:50.240]   So then you as a data person, you need to figure out, "What the heck is in there?
[00:02:50.240 --> 00:02:58.840]   How do I organize this in a way that is useful to do some reporting for my end data consumers?"
[00:02:58.840 --> 00:03:04.360]   So DBT creates a new workflow around how to do that work.
[00:03:04.360 --> 00:03:06.040]   And it's very code-first.
[00:03:06.040 --> 00:03:17.480]   It takes DevOps principles as its founding ideas, and it is open-source, open-core.
[00:03:17.480 --> 00:03:19.440]   First commit happened about six years ago.
[00:03:19.440 --> 00:03:24.080]   Over the past six years, there's been a pretty large community that's grown up around it.
[00:03:24.080 --> 00:03:29.520]   And I guess, what does it actually do to address that problem?
[00:03:29.520 --> 00:03:32.160]   Yeah.
[00:03:32.160 --> 00:03:35.280]   So how was this problem solved before?
[00:03:35.280 --> 00:03:39.160]   Before, you couldn't rely on the warehouse to do all this work, because the warehouse
[00:03:39.160 --> 00:03:41.440]   was constrained.
[00:03:41.440 --> 00:03:44.120]   And so you had these intermediate environments.
[00:03:44.120 --> 00:03:50.440]   A lot of times, you had commercial products that sprung up to do traditional data transformation
[00:03:50.440 --> 00:03:53.240]   that happened before you loaded it into the eventual warehouse.
[00:03:53.240 --> 00:03:58.480]   So the big insight behind DBT was that the warehouse now was performing scalable enough
[00:03:58.480 --> 00:04:00.740]   to just do it all itself.
[00:04:00.740 --> 00:04:06.000]   And what that means is if you want to get access to the compute that lives in the warehouse,
[00:04:06.000 --> 00:04:10.360]   you have to, at least traditionally, you had to express your jobs in SQL.
[00:04:10.360 --> 00:04:18.080]   And so DBT is essentially a framework for programming in SQL, building data pipelines
[00:04:18.080 --> 00:04:19.080]   in SQL.
[00:04:19.080 --> 00:04:27.240]   You write in the SQL that is native to your database, and then it has a Jinja layer, a
[00:04:27.240 --> 00:04:31.480]   templating layer over top of that SQL that allows you to build, instead of just having
[00:04:31.480 --> 00:04:36.400]   this collection of random SQL scripts on your hard drive, you have a framework that you
[00:04:36.400 --> 00:04:37.400]   can plug into.
[00:04:37.400 --> 00:04:44.560]   You have references, so you can build DAGs out of SQL files very seamlessly.
[00:04:44.560 --> 00:04:49.040]   You have environment variables, you have CI/CD, all of these things that you would expect
[00:04:49.040 --> 00:04:52.840]   from a programming framework.
[00:04:52.840 --> 00:04:57.320]   It's funny, I feel like from my perspective, running a tech startup where I'm trying to
[00:04:57.320 --> 00:05:04.560]   get official records of data on all these different topics, it seems incredibly obvious
[00:05:04.560 --> 00:05:06.800]   to me that something like this is needed.
[00:05:06.800 --> 00:05:07.800]   But I wonder if my younger self-
[00:05:07.800 --> 00:05:11.640]   Do you want to do the big reveal of how you first got exposed to DBT?
[00:05:11.640 --> 00:05:13.880]   Oh, should we go down that path?
[00:05:13.880 --> 00:05:15.000]   I guess sure, let's do it.
[00:05:15.000 --> 00:05:19.680]   So you were one of the very best consultants we've ever hired, and you came in and did
[00:05:19.680 --> 00:05:21.360]   our analytics.
[00:05:21.360 --> 00:05:26.600]   And it was funny because I actually edited your SQL queries that you wrote quite a bit.
[00:05:26.600 --> 00:05:29.600]   And I should say, I learned a lot of SQL from you.
[00:05:29.600 --> 00:05:33.720]   I felt like working with you is the first time I saw...
[00:05:33.720 --> 00:05:39.000]   I think I learned SQL as a side thing in school, and then I used it a lot.
[00:05:39.000 --> 00:05:44.480]   I think when you're CEO, SQL is the language that you end up writing the most stuff in.
[00:05:44.480 --> 00:05:51.240]   And so I think I went down a bad path, and SQL lets you start to write it like you might
[00:05:51.240 --> 00:05:58.720]   write blocks in Excel, where you just start stuffing more and more chaos into your queries.
[00:05:58.720 --> 00:06:02.440]   And one thing that's actually notable about working with you is you really pulled out
[00:06:02.440 --> 00:06:07.760]   each piece into its own named section, which I didn't even realize some of those things
[00:06:07.760 --> 00:06:09.840]   you could do in SQL.
[00:06:09.840 --> 00:06:15.480]   I mean, generally, either you can't at all, or SQL doesn't make it easy.
[00:06:15.480 --> 00:06:17.320]   And so that is...
[00:06:17.320 --> 00:06:21.200]   I think you experienced part of the magic that data people experience when they use
[00:06:21.200 --> 00:06:22.200]   DBT for the first time.
[00:06:22.200 --> 00:06:28.200]   They're like, "For however long I've been using SQL, it's looked like garbage, and you've
[00:06:28.200 --> 00:06:34.760]   given me some more structure in the language, and I can now engineer it in ways that actually
[00:06:34.760 --> 00:06:35.760]   make sense."
[00:06:35.760 --> 00:06:42.020]   Which is, for many of us, we're used to thinking in...
[00:06:42.020 --> 00:06:47.040]   Whether it's OO or functional or whatever, like programming paradigms, and then SQL becomes
[00:06:47.040 --> 00:06:52.320]   very frustrating because you can't actually organize your code in these similar ways.
[00:06:52.320 --> 00:06:53.320]   Yeah.
[00:06:53.320 --> 00:06:58.880]   I think from my perspective, if I could write a love letter to DBT as someone who doesn't
[00:06:58.880 --> 00:07:05.000]   actually use it, but sort of sees the results of it on my organization, I think you might
[00:07:05.000 --> 00:07:11.760]   not realize how much complexity enters your data pre-processing.
[00:07:11.760 --> 00:07:16.680]   We have a lot of people that come in and use our product as students, and they're sort
[00:07:16.680 --> 00:07:18.880]   of different ways to get at that.
[00:07:18.880 --> 00:07:24.960]   But we often want the students outside our analysis of leads for sales, from that perspective.
[00:07:24.960 --> 00:07:29.280]   There's a lot of different ways you could cut who's a student, but it's really helpful
[00:07:29.280 --> 00:07:33.480]   to have one official way that's really good, and just kind of nail that down, and then
[00:07:33.480 --> 00:07:37.240]   let everybody operate off of it.
[00:07:37.240 --> 00:07:42.960]   It feels like one of the big benefits of DBT for us at Weights and Biases is that we're
[00:07:42.960 --> 00:07:51.240]   able to kind of standardize all these intermediate steps and have an organized way of just standardizing
[00:07:51.240 --> 00:07:55.120]   on these things, which I think has made us operate much better as a company.
[00:07:55.120 --> 00:07:56.120]   Am I off on...
[00:07:56.120 --> 00:07:57.400]   No, you're totally right.
[00:07:57.400 --> 00:08:02.840]   The way that we talk about this is curating knowledge inside of an organization.
[00:08:02.840 --> 00:08:11.160]   It used to be that we kind of, in our wetware, we used English to pass knowledge on to each
[00:08:11.160 --> 00:08:16.200]   other, and then somebody would write a SQL query for themselves based on their kind of
[00:08:16.200 --> 00:08:19.000]   imperfect understanding of who was a student.
[00:08:19.000 --> 00:08:25.360]   And now there's a way to actually take that knowledge and encode it, and then you can
[00:08:25.360 --> 00:08:31.200]   just kind of forget about it as an organization until you say, "Hey, how do we do that?"
[00:08:31.200 --> 00:08:34.560]   And then you look back at the code, and you can even look at the Git blame, and you can
[00:08:34.560 --> 00:08:37.880]   say, "Well, here's how we arrived there."
[00:08:37.880 --> 00:08:41.960]   And if you don't do that, you end up with all these slightly different versions of what's
[00:08:41.960 --> 00:08:44.200]   a student, and it doesn't match, and it's totally bug-ridden.
[00:08:44.200 --> 00:08:45.200]   And so, I don't know.
[00:08:45.200 --> 00:08:49.680]   I feel like dbt has made a big difference.
[00:08:49.680 --> 00:08:55.200]   So here's the funny thing about that project from my experience.
[00:08:55.200 --> 00:09:03.040]   I was working with you folks, and you know about machine learning.
[00:09:03.040 --> 00:09:08.560]   And it's so cool and trendy, and you can do magic stuff with it.
[00:09:08.560 --> 00:09:10.600]   And here I am.
[00:09:10.600 --> 00:09:14.720]   I'm close to the business.
[00:09:14.720 --> 00:09:23.120]   I come at data from a, "I understand the business, so let me get into the asking questions about
[00:09:23.120 --> 00:09:25.880]   it perspective."
[00:09:25.880 --> 00:09:30.880]   And to me, what I do feels not that complicated.
[00:09:30.880 --> 00:09:34.400]   I mean, at least not that technically complicated.
[00:09:34.400 --> 00:09:38.960]   And so, it feels like people who know about something as...
[00:09:38.960 --> 00:09:40.880]   This is my internal monologue.
[00:09:40.880 --> 00:09:45.720]   Something as complicated as machine learning, how can they not have this kind of basic stuff
[00:09:45.720 --> 00:09:46.720]   figured out?
[00:09:46.720 --> 00:09:59.120]   But I think that for whatever reason, it's not a thing that ML folks have widely...
[00:09:59.120 --> 00:10:01.520]   And I'm curious as to...
[00:10:01.520 --> 00:10:07.440]   I have my theories on why this might be true, but I'm curious if you have any thoughts on
[00:10:07.440 --> 00:10:09.560]   why that might be true.
[00:10:09.560 --> 00:10:16.720]   Well, I think one thing that shouldn't be underestimated is that most people in ML have
[00:10:16.720 --> 00:10:19.760]   a lot of academic training.
[00:10:19.760 --> 00:10:21.720]   Just a lot of ML comes from academia.
[00:10:21.720 --> 00:10:24.480]   I think more than almost any other field.
[00:10:24.480 --> 00:10:25.480]   You see people...
[00:10:25.480 --> 00:10:29.360]   Knowledge gets passed down in academia quite a bit.
[00:10:29.360 --> 00:10:32.000]   It's starting to change, but I think it's still...
[00:10:32.000 --> 00:10:36.640]   People are going to original papers and learning through professors and that.
[00:10:36.640 --> 00:10:41.720]   And I do think that academia teaches you incredibly bad habits.
[00:10:41.720 --> 00:10:46.760]   I think everyone coming out of there has to unlearn a lot of things, including myself.
[00:10:46.760 --> 00:10:51.280]   Because if you think about academia, you're trying to get to an interesting result, and
[00:10:51.280 --> 00:10:55.920]   then you never have to make iterative progress from there.
[00:10:55.920 --> 00:10:56.920]   Interesting.
[00:10:56.920 --> 00:10:59.560]   Whereas in work, most of what you do is iterate.
[00:10:59.560 --> 00:11:04.640]   And so you really want things to be stable and contained and clear.
[00:11:04.640 --> 00:11:08.080]   Whereas in academia, most of what you do gets almost immediately thrown away.
[00:11:08.080 --> 00:11:13.700]   So you're racing and trying to...
[00:11:13.700 --> 00:11:15.960]   You write a lot of throwaway code.
[00:11:15.960 --> 00:11:17.720]   You don't think a lot about structuring your code.
[00:11:17.720 --> 00:11:26.680]   And then you especially don't think about making your data pipelines stable and consistent.
[00:11:26.680 --> 00:11:27.680]   Because I think a lot of ML...
[00:11:27.680 --> 00:11:30.040]   Which is how Jupyter Notebooks end up becoming data pipelines.
[00:11:30.040 --> 00:11:31.040]   Exactly.
[00:11:31.040 --> 00:11:32.040]   Exactly.
[00:11:32.040 --> 00:11:33.960]   And yeah, I totally understand how it happens.
[00:11:33.960 --> 00:11:38.520]   And as a CEO of a growing startup, it drives me nuts.
[00:11:38.520 --> 00:11:41.120]   But I actually come from that lineage too.
[00:11:41.120 --> 00:11:46.280]   So I think I've had to unlearn a lot of these instincts as well.
[00:11:46.280 --> 00:11:52.240]   And then I also think it's actually a real skill that I'm still working on to make good
[00:11:52.240 --> 00:11:55.220]   data pipelines for a company.
[00:11:55.220 --> 00:11:59.920]   Every query is more complicated than you think it is at first blush.
[00:11:59.920 --> 00:12:09.520]   And I think a lot of these choices, it's harder to do extremely agile iterative development.
[00:12:09.520 --> 00:12:14.120]   A lot of these choices that you make have long lasting repercussions and need to be
[00:12:14.120 --> 00:12:15.120]   considered.
[00:12:15.120 --> 00:12:20.440]   And it's more important to get it right the first time for some of these things.
[00:12:20.440 --> 00:12:21.440]   We started...
[00:12:21.440 --> 00:12:27.480]   When I say we, I don't mean the company, but the dbt community started using this term
[00:12:27.480 --> 00:12:33.840]   analytics engineer for the people that use dbt or do their work in the way that dbt teaches
[00:12:33.840 --> 00:12:36.000]   you to do your work.
[00:12:36.000 --> 00:12:41.280]   And I think it really gets to this dichotomy where...
[00:12:41.280 --> 00:12:47.160]   So there are data analysts who use the tools of data analysis to come to some net new result.
[00:12:47.160 --> 00:12:53.800]   And in that world, it's actually completely fine if your code looks like garbage.
[00:12:53.800 --> 00:12:59.160]   It's just like poke around until you find something interesting and then wave your hand
[00:12:59.160 --> 00:13:02.200]   and be like, "Hey, does anyone else find this interesting?"
[00:13:02.200 --> 00:13:11.260]   Whereas analytics engineering is this thoughtful effort to slowly construct reality for the
[00:13:11.260 --> 00:13:12.260]   business.
[00:13:12.260 --> 00:13:14.440]   My favorite example of this is actually...
[00:13:14.440 --> 00:13:15.440]   I was working for a...
[00:13:15.440 --> 00:13:18.080]   This is a consulting project.
[00:13:18.080 --> 00:13:22.120]   I was working for a full stack grocery delivery company.
[00:13:22.120 --> 00:13:31.000]   And I had to help them calculate the cost of goods sold for a individual batch of green
[00:13:31.000 --> 00:13:32.440]   onions.
[00:13:32.440 --> 00:13:37.720]   And it turned out to be an incredibly challenging problem.
[00:13:37.720 --> 00:13:41.400]   And in many ways, deeply unsexy.
[00:13:41.400 --> 00:13:47.960]   But it was so fun to me to now, every single time a picker picked a thing of green onions
[00:13:47.960 --> 00:13:52.280]   out, we knew exactly how much cost to allocate to that.
[00:13:52.280 --> 00:13:54.320]   Yeah, totally.
[00:13:54.320 --> 00:14:01.640]   And I think it's funny how CFOs come from a totally different lens from ML, where they
[00:14:01.640 --> 00:14:06.880]   really want things to be precise and accurate and consistent and traceable.
[00:14:06.880 --> 00:14:10.320]   And yeah, I mean, these cost of goods sold calculations always end up being...
[00:14:10.320 --> 00:14:17.760]   To get it that precise, which I understand why finance wants that, is often in deep tension
[00:14:17.760 --> 00:14:20.680]   with the exploratory data analysis that's also important.
[00:14:20.680 --> 00:14:21.680]   Yeah.
[00:14:21.680 --> 00:14:28.800]   Well, I had a question for you that I really wanted to ask, which is, I think both of us
[00:14:28.800 --> 00:14:31.880]   run companies that are kind of...
[00:14:31.880 --> 00:14:33.840]   How do you put this?
[00:14:33.840 --> 00:14:37.640]   It's hard to explain to our aunt or uncle, right?
[00:14:37.640 --> 00:14:41.640]   And behind the scenes and helping a lot of things happen.
[00:14:41.640 --> 00:14:47.840]   But I think one thing that both of us share is we really are passionate about the impact
[00:14:47.840 --> 00:14:48.840]   on the world.
[00:14:48.840 --> 00:14:52.400]   And we're in this maybe more for the impact than the financial gain.
[00:14:52.400 --> 00:14:56.120]   I don't want to put words in your mouth, but that's my sense of you.
[00:14:56.120 --> 00:15:01.400]   And I'm curious how you think about the impact of the work that you do, or how you articulate
[00:15:01.400 --> 00:15:06.760]   it to prospective employees or the world.
[00:15:06.760 --> 00:15:10.240]   You know, yes, I agree with you.
[00:15:10.240 --> 00:15:12.160]   And I'm like, "Okay, let's go there."
[00:15:12.160 --> 00:15:15.640]   But actually, no one asked me this question.
[00:15:15.640 --> 00:15:23.480]   From a commercial perspective, our mission is to help data analysts and help them curate
[00:15:23.480 --> 00:15:26.600]   and disseminate knowledge inside of organizations.
[00:15:26.600 --> 00:15:38.400]   But if I like broaden the lens and think societally, and, you know, okay, there's a lot of tech
[00:15:38.400 --> 00:15:42.040]   where we like to talk about making a dent in the universe.
[00:15:42.040 --> 00:15:45.560]   And so I think that's overplayed.
[00:15:45.560 --> 00:15:48.440]   Sometimes I try to be a little more humble than thinking that like we are going to somehow
[00:15:48.440 --> 00:15:51.160]   impact the trajectory of the universe.
[00:15:51.160 --> 00:16:00.480]   But when I frame it like that to myself, I am deeply concerned with our epistemic reality
[00:16:00.480 --> 00:16:02.200]   as a world today.
[00:16:02.200 --> 00:16:08.640]   Like we, you know, we don't need to go too deeply into politics, but there's been a lot
[00:16:08.640 --> 00:16:14.160]   of interesting conversation happening at the national or international level.
[00:16:14.160 --> 00:16:22.040]   This is not just associated with the United States, but where people disagree on like
[00:16:22.040 --> 00:16:24.720]   basic realities of what is true.
[00:16:24.720 --> 00:16:30.480]   And because of that, we actually have a hard time having conversations or having like productive
[00:16:30.480 --> 00:16:34.240]   debates and maybe some of that's in good faith, maybe some of it's not in good faith, but
[00:16:34.240 --> 00:16:35.240]   whatever.
[00:16:35.240 --> 00:16:42.640]   The thing that DBT does is try to like get to a ground truth that everybody inside of
[00:16:42.640 --> 00:16:43.920]   an organization can agree on.
[00:16:43.920 --> 00:16:47.720]   So we can at least have productive disagreement.
[00:16:47.720 --> 00:16:54.120]   And I don't know that there's some way to like magically organize all structured information
[00:16:54.120 --> 00:17:00.480]   in the entire, like, okay, maybe that's beyond what we will ever get to as a company.
[00:17:00.480 --> 00:17:07.680]   But it does motivate me to think that the world that we are working on, like figuring
[00:17:07.680 --> 00:17:13.560]   out the epistemic reality inside of organizations is like actually a big problem for the entire
[00:17:13.560 --> 00:17:15.240]   world right now.
[00:17:15.240 --> 00:17:16.240]   Interesting.
[00:17:16.240 --> 00:17:17.240]   Great answer.
[00:17:17.240 --> 00:17:20.640]   Is that what you were expecting?
[00:17:20.640 --> 00:17:21.640]   No, not at all.
[00:17:21.640 --> 00:17:22.640]   It's a really interesting answer.
[00:17:22.640 --> 00:17:24.280]   I mean, I'm just contemplating it.
[00:17:24.280 --> 00:17:30.200]   I think it's a great way of looking at DBT.
[00:17:30.200 --> 00:17:35.000]   I mean, I think I always don't want to be the caricature of like a startup CEO saying
[00:17:35.000 --> 00:17:39.640]   like we're changing the world with better MLOps, but at the same time, we are changing
[00:17:39.640 --> 00:17:43.720]   the world with better MLOps and I do feel proud of it myself.
[00:17:43.720 --> 00:17:50.120]   And so I don't want to come off like a blowhard, but I also, for me, I do feel really proud
[00:17:50.120 --> 00:17:57.120]   of the work that we do and think it makes a small dent in the universe and don't want
[00:17:57.120 --> 00:18:03.560]   to be falsely humble either when it feels good to help out all these customers working
[00:18:03.560 --> 00:18:05.000]   on really, really exciting things.
[00:18:05.000 --> 00:18:08.320]   But I think you have such a specific, interesting answer.
[00:18:08.320 --> 00:18:13.320]   That's such a great way of looking at what DBT does.
[00:18:13.320 --> 00:18:20.920]   We have, you talk about the customers building cool stuff.
[00:18:20.920 --> 00:18:29.640]   There's this funny conversation going on inside of our community these days where a lot of
[00:18:29.640 --> 00:18:34.360]   folks who used to be practitioners have gone over to the founder side.
[00:18:34.360 --> 00:18:36.680]   They've gone over to the dark side.
[00:18:36.680 --> 00:18:41.680]   And so when it used to be all of these practitioner to practitioner conversations, now it's a
[00:18:41.680 --> 00:18:45.800]   bunch of tool vendors hyping their own stuff.
[00:18:45.800 --> 00:18:54.880]   So I'm a little bit jealous of, I would love to actually go back to the other side of the
[00:18:54.880 --> 00:19:00.840]   fence and maybe at some point we'll get the opportunity to rejoin the people who are actually
[00:19:00.840 --> 00:19:03.720]   using the shovels as opposed to making the shovels.
[00:19:03.720 --> 00:19:06.520]   I guess here's another question that I think about a lot.
[00:19:06.520 --> 00:19:10.400]   How do you stay current without working on this stuff?
[00:19:10.400 --> 00:19:17.320]   I think for both of us, I imagine it's important to keep doing a little bit of the task.
[00:19:17.320 --> 00:19:24.640]   It's very hard for me to learn about machine learning in theory without practicing it.
[00:19:24.640 --> 00:19:30.240]   I'm always really trying to carve out time to train new models and try out new things
[00:19:30.240 --> 00:19:31.240]   that are coming out.
[00:19:31.240 --> 00:19:38.320]   But the urgent needs of running a fast-growing company encroach aggressively in that time.
[00:19:38.320 --> 00:19:40.800]   How do you think about that?
[00:19:40.800 --> 00:19:42.960]   Yes.
[00:19:42.960 --> 00:19:45.080]   This is something that concerns me a lot.
[00:19:45.080 --> 00:19:50.600]   I think that I might be in a slightly easier position than you.
[00:19:50.600 --> 00:19:58.600]   I think that you can summarize a lot of the characteristics of our world based on the
[00:19:58.600 --> 00:20:04.400]   evolution of the data platforms that all of this stuff runs on.
[00:20:04.400 --> 00:20:09.520]   You can summarize that in price per performance and these kind of characteristics.
[00:20:09.520 --> 00:20:16.840]   Fundamentally, SQL basically does what it has done for freaking 40 years or whatever.
[00:20:16.840 --> 00:20:21.320]   And then the tooling on top of it.
[00:20:21.320 --> 00:20:26.160]   There's areas in our ecosystem that have a lot of movement, data observability, data
[00:20:26.160 --> 00:20:29.720]   quality, cataloging, these kinds of things are very fast moving right now.
[00:20:29.720 --> 00:20:35.840]   And then maybe there's another next wave of data analysis products that are coming
[00:20:35.840 --> 00:20:37.120]   out.
[00:20:37.120 --> 00:20:43.640]   I end up staying on top of stuff by curating a newsletter.
[00:20:43.640 --> 00:20:51.040]   I have for six and a half years now published a newsletter that now is called the Analytics
[00:20:51.040 --> 00:20:52.040]   Engineering Roundup.
[00:20:52.040 --> 00:20:54.240]   It goes out every week.
[00:20:54.240 --> 00:20:58.360]   I write half of the issues.
[00:20:58.360 --> 00:21:04.760]   But it is this really great accountability tool to make sure that I actually have something
[00:21:04.760 --> 00:21:07.320]   new to say every two weeks.
[00:21:07.320 --> 00:21:12.280]   Because otherwise it's incredibly easy if no one's...
[00:21:12.280 --> 00:21:17.760]   When 15,000 people are going to read the thing that you just put out there, you feel a lot
[00:21:17.760 --> 00:21:21.720]   of pressure to say something correct and novel and interesting.
[00:21:21.720 --> 00:21:25.440]   But otherwise it's very easy to not invest that time.
[00:21:25.440 --> 00:21:26.440]   Totally it's funny.
[00:21:26.440 --> 00:21:28.920]   I actually use those external forcing functions too.
[00:21:28.920 --> 00:21:33.040]   They're so effective and I always get really nervous before I have to put out something
[00:21:33.040 --> 00:21:34.040]   like that.
[00:21:34.040 --> 00:21:38.760]   I sometimes set up talks with topics that I don't fully know about yet.
[00:21:38.760 --> 00:21:41.200]   It's hard for me to figure it out.
[00:21:41.200 --> 00:21:44.800]   Sorry for those of you that have watched those talks and thought I didn't look like I knew
[00:21:44.800 --> 00:21:47.160]   what I was talking about.
[00:21:47.160 --> 00:21:50.640]   Sometimes it turns out that they go great.
[00:21:50.640 --> 00:21:53.360]   And then every once in a while you're like, "That wasn't perfect."
[00:21:53.360 --> 00:21:54.880]   Well, do you ever feel...
[00:21:54.880 --> 00:21:59.840]   I feel like sometimes if I give the same talk too much, I find myself getting bored in the
[00:21:59.840 --> 00:22:00.840]   middle of the talk.
[00:22:00.840 --> 00:22:03.640]   And then I feel so sorry for the audience because I figure if I'm getting bored, the
[00:22:03.640 --> 00:22:06.320]   audience must be bored out of their minds.
[00:22:06.320 --> 00:22:11.900]   I have a tremendous amount of respect for professors, for teachers who keep the energy
[00:22:11.900 --> 00:22:14.720]   level up to delivering the same stuff over and over again.
[00:22:14.720 --> 00:22:15.720]   Totally.
[00:22:15.720 --> 00:22:16.720]   Okay.
[00:22:16.720 --> 00:22:17.720]   Well, tell me about starting dbt.
[00:22:17.800 --> 00:22:21.040]   I'm sure everyone asks you that, but it's such an interesting question.
[00:22:21.040 --> 00:22:25.560]   I'm curious what you were thinking when you started it.
[00:22:25.560 --> 00:22:30.960]   Was it just like a rocket ship from the beginning or was there a moment where something changed
[00:22:30.960 --> 00:22:35.920]   and it started to really build traction?
[00:22:35.920 --> 00:22:42.680]   The origin story of dbt is that I was burnt out from venture funded startups.
[00:22:42.680 --> 00:22:46.800]   I'd worked at three of them.
[00:22:46.800 --> 00:22:51.560]   I think that as a community, venture backed startups are getting a little bit better about
[00:22:51.560 --> 00:22:55.120]   work life balance, but inconsistently so.
[00:22:55.120 --> 00:22:57.680]   But certainly back in 2015, that was not the case at all.
[00:22:57.680 --> 00:23:02.400]   I'd been working for whatever, 11, 12 hours a day for seven years.
[00:23:02.400 --> 00:23:09.840]   And so I was like, "Okay, I'm done with that and I really want to go back to data."
[00:23:09.840 --> 00:23:13.520]   I had started my career in data and then I'd gone to different...
[00:23:13.520 --> 00:23:16.600]   Whatever, it doesn't matter.
[00:23:16.600 --> 00:23:20.680]   But I wanted to get back to actually having a pure data job.
[00:23:20.680 --> 00:23:22.840]   And so I was like, "How do I do this?
[00:23:22.840 --> 00:23:24.480]   And how do I do it from Philadelphia?
[00:23:24.480 --> 00:23:28.960]   Because I'm married and my wife has a cool job and she's like, 'We're not moving.'"
[00:23:28.960 --> 00:23:36.960]   And so I decided to start a one person consulting shop and I was just going to help companies
[00:23:36.960 --> 00:23:39.720]   implement what became known as the modern data stack.
[00:23:39.720 --> 00:23:46.160]   So a data warehouse, a data ingestion tool, a BI tool, and I was going to help them do
[00:23:46.160 --> 00:23:47.720]   their internal analytics.
[00:23:47.720 --> 00:23:53.760]   The thing that was clearly to me missing was data transformation, which was a part of how
[00:23:53.760 --> 00:23:57.160]   this stuff had been done in the past, but there wasn't a modern data stack solution.
[00:23:57.160 --> 00:24:05.800]   And so I got my friend and coworker, Drew Benin, to help me build the early versions
[00:24:05.800 --> 00:24:06.800]   of DBT.
[00:24:06.800 --> 00:24:13.240]   I don't know, it was not so many hours that was put into the initial versions of DBT.
[00:24:13.240 --> 00:24:17.360]   At its base, DBT is not that complicated.
[00:24:17.360 --> 00:24:19.800]   So I started using it on...
[00:24:19.800 --> 00:24:23.160]   Drew joined and we started using it on consulting projects.
[00:24:23.160 --> 00:24:32.280]   And it was really our consulting clients who got exposure to DBT and they said, "Hey, I
[00:24:32.280 --> 00:24:33.280]   want to start using that tool."
[00:24:33.280 --> 00:24:35.760]   And so they would train their internal people.
[00:24:35.760 --> 00:24:46.160]   And then the big locus of where the community came from was back in 2016, Casper got turned
[00:24:46.160 --> 00:24:50.440]   on to DBT and they were kind of a big deal in the New York tech scene at the time.
[00:24:50.440 --> 00:24:59.600]   And they told all their friends and so Kickstarter and et cetera, it was a New York tech thing.
[00:24:59.600 --> 00:25:05.280]   But if you look at the graph, we do like anonymous event tracking inside the open source product.
[00:25:05.280 --> 00:25:11.640]   If you look at the graph of the number of different organizations using DBT over time,
[00:25:11.640 --> 00:25:18.720]   that graph has grown at 10% every single month for five and a half years now.
[00:25:18.720 --> 00:25:20.280]   And so it does feel like...
[00:25:20.280 --> 00:25:21.280]   Can I interject?
[00:25:21.280 --> 00:25:22.280]   I'm so jealous.
[00:25:22.280 --> 00:25:23.280]   I'm so jealous.
[00:25:23.280 --> 00:25:24.280]   That's amazing.
[00:25:24.280 --> 00:25:25.280]   All right, go ahead.
[00:25:25.280 --> 00:25:31.160]   Well, I mean, at the beginning, we didn't even focus on it because we didn't have a
[00:25:31.160 --> 00:25:32.520]   way to make money off of that.
[00:25:32.520 --> 00:25:34.360]   It was just like, whatever.
[00:25:34.360 --> 00:25:36.680]   It's cool that the community is growing.
[00:25:36.680 --> 00:25:42.080]   And then we got to a point where we grew from 300 to a thousand companies using it over
[00:25:42.080 --> 00:25:43.080]   the course of a year.
[00:25:43.080 --> 00:25:47.280]   And that's when the Fortune 500 companies started calling us and were like, "Hey, we'd
[00:25:47.280 --> 00:25:48.280]   like to buy stuff from you."
[00:25:48.280 --> 00:25:51.360]   And we're like, "We don't have anything to sell you."
[00:25:51.360 --> 00:25:57.600]   And that was when we kind of changed directions and became more of a software company.
[00:25:57.600 --> 00:26:01.880]   But there was no single point where it all came together.
[00:26:01.880 --> 00:26:07.040]   It was just people underestimate the power of exponentials over long periods of time.
[00:26:07.040 --> 00:26:08.040]   Totally.
[00:26:08.040 --> 00:26:17.920]   I guess another funny thing about DBT is that it seems so conceptually simple, doesn't it?
[00:26:17.920 --> 00:26:18.920]   I mean, it's a funny...
[00:26:18.920 --> 00:26:19.920]   It's funny.
[00:26:19.920 --> 00:26:20.920]   I feel like these are mean questions.
[00:26:20.920 --> 00:26:26.600]   I was asking the Spark founders, like, what makes Spark complicated and what makes Ray
[00:26:26.600 --> 00:26:27.600]   complicated?
[00:26:27.600 --> 00:26:35.600]   I mean, all these things, I guess, at their core seem simple, but what makes DBT hard
[00:26:35.600 --> 00:26:37.600]   to build?
[00:26:37.600 --> 00:26:39.080]   Okay.
[00:26:39.080 --> 00:26:41.160]   The simplicity is...
[00:26:41.160 --> 00:26:46.880]   I don't want to take credit for that, but I think that that is one of our main driving
[00:26:46.880 --> 00:26:47.880]   product goals.
[00:26:47.880 --> 00:26:50.040]   Also, who else would take credit for that?
[00:26:50.040 --> 00:26:51.480]   Can't you take credit for that?
[00:26:51.480 --> 00:26:55.040]   Well, Mitchell Hashimoto should take credit for that because it's a straight up copy of
[00:26:55.040 --> 00:26:56.040]   Terraform.
[00:26:56.040 --> 00:27:02.160]   The user interface paradigm is...
[00:27:02.160 --> 00:27:09.640]   My other co-founder, Connor, was an infrastructure engineer at our last company together.
[00:27:09.640 --> 00:27:16.360]   I was telling him about this need that I had, and he said, "Have you ever seen Terraform?"
[00:27:16.360 --> 00:27:21.120]   This was back in 2016, and so Terraform is still kind of new and cool.
[00:27:21.120 --> 00:27:23.920]   He's like, "Let me show you this thing."
[00:27:23.920 --> 00:27:29.280]   He showed me the HCL behind it, and then he did a TF apply, and I was just like, "Holy
[00:27:29.280 --> 00:27:32.920]   shit, that's really freaking cool."
[00:27:32.920 --> 00:27:37.600]   Once you've seen Terraform and you've used it, you're just like, "Well, obviously that's
[00:27:37.600 --> 00:27:41.560]   how I'm going to do that moving forwards."
[00:27:41.560 --> 00:27:44.240]   That was the protocol of DBT at the outset.
[00:27:44.240 --> 00:27:49.200]   It was Terraform for analytics.
[00:27:49.200 --> 00:28:00.200]   On some level, what DBT does is it takes SQL select statements that are inside of .SQL
[00:28:00.200 --> 00:28:10.120]   files on your machine, and it wraps them in create view or create table as select statements.
[00:28:10.120 --> 00:28:17.200]   Then it does some DAG processing with NetworkX and Python.
[00:28:17.200 --> 00:28:21.080]   On some level, that is actually quite simple.
[00:28:21.080 --> 00:28:29.160]   The hard parts come in when ... I mean, there's a lot, and I'm not the person who built it,
[00:28:29.160 --> 00:28:34.640]   so you're going to hear it pass through a less technical person's mouth.
[00:28:34.640 --> 00:28:44.120]   Jinja is really meant to be used as a web templating language, and it's meant to process
[00:28:44.120 --> 00:28:49.960]   one HTML page at a time, like request to response.
[00:28:49.960 --> 00:28:55.720]   In that context, it works quite performantly and all is well.
[00:28:55.720 --> 00:29:04.840]   In DBT, because all of your pipelines together make a DAG, what DBT has to do is it has to
[00:29:04.840 --> 00:29:12.440]   read all of them at startup time in order to understand the shape of your entire DAG
[00:29:12.440 --> 00:29:16.200]   so it can know what work it needs to do.
[00:29:16.200 --> 00:29:23.520]   If you have 50 of these, that's not a problem, but we have users who have thousands of these,
[00:29:23.520 --> 00:29:31.440]   and it turns out that it's quite challenging to read thousands of files from disk, operate
[00:29:31.440 --> 00:29:37.320]   on them in a way that feels interactive to a user on the command line.
[00:29:37.320 --> 00:29:39.640]   We literally have a ... I don't know.
[00:29:39.640 --> 00:29:41.640]   I think the team last year was four people.
[00:29:41.640 --> 00:29:55.320]   We spent four person years of engineering time last year almost exclusively on performance.
[00:29:55.320 --> 00:29:57.760]   That is an answer.
[00:29:57.760 --> 00:30:01.680]   There's many answers to once you go deeper and deeper down this hole, and I'm sure that
[00:30:01.680 --> 00:30:03.520]   you've experienced this too.
[00:30:03.520 --> 00:30:08.560]   Sometimes the decisions that you make early on in the process of building something, you
[00:30:08.560 --> 00:30:12.480]   come back to it later and you're like, "Wow, gosh, I didn't realize what a bad idea that
[00:30:12.480 --> 00:30:13.480]   was going to be."
[00:30:13.480 --> 00:30:19.280]   Yeah, it's a constant iteration cycle.
[00:30:19.280 --> 00:30:26.100]   How about documentation and API names and things like that?
[00:30:26.100 --> 00:30:28.200]   How do you feel about how well you've done on that?
[00:30:28.200 --> 00:30:35.400]   That's always something I reflect on with Weights and Biases.
[00:30:35.400 --> 00:30:43.040]   We're not great at that today.
[00:30:43.040 --> 00:30:47.760]   Your APIs, your whole product is commercial product, right?
[00:30:47.760 --> 00:30:49.480]   You don't have open source service area?
[00:30:49.480 --> 00:30:50.480]   Well, we do actually.
[00:30:50.480 --> 00:30:51.480]   We have ...
[00:30:51.480 --> 00:30:52.480]   You do, okay.
[00:30:52.480 --> 00:30:57.120]   A client is open source and then the APIs are ... I mean, anyone can call the APIs and
[00:30:57.120 --> 00:30:59.240]   pull stuff out, but yeah, the client is open source.
[00:30:59.240 --> 00:31:00.600]   It could go anywhere.
[00:31:00.600 --> 00:31:01.600]   Okay.
[00:31:01.600 --> 00:31:09.280]   We have this funny thing where we have two different types of users.
[00:31:09.280 --> 00:31:11.520]   We have users who tend to be less technical.
[00:31:11.520 --> 00:31:14.880]   There are people like me who, their primary language is SQL and maybe some scripting and
[00:31:14.880 --> 00:31:15.880]   stuff like that.
[00:31:15.880 --> 00:31:24.600]   Then we have contributors, and that group is much smaller and they tend to be data engineers
[00:31:24.600 --> 00:31:29.200]   and not data analysts.
[00:31:29.200 --> 00:31:40.840]   We have historically prioritized the needs of users over the needs of contributors.
[00:31:40.840 --> 00:31:47.080]   That has meant that we have under-invested, whether it's in the open source context or
[00:31:47.080 --> 00:31:56.840]   in our cloud product, we've historically under-invested in clean APIs.
[00:31:56.840 --> 00:32:02.840]   The open source product really exposes itself as the CLI.
[00:32:02.840 --> 00:32:07.560]   If you try to get in there via Python and call stuff directly, you can, but we don't
[00:32:07.560 --> 00:32:11.120]   make any guarantees about the stability of those APIs.
[00:32:11.120 --> 00:32:16.560]   We need to improve there.
[00:32:16.560 --> 00:32:25.280]   As we mature as a commercial business, we're increasingly taking the needs of data engineers
[00:32:25.280 --> 00:32:31.120]   seriously too, because DBT is increasingly this mature piece of data infrastructure inside
[00:32:31.120 --> 00:32:36.320]   of the companies that use it.
[00:32:36.320 --> 00:32:42.560]   Documentation API design are very, very front and center in our world today.
[00:32:42.560 --> 00:32:49.240]   Is it a command and control style management to keep the names consistent and things like
[00:32:49.240 --> 00:32:50.240]   that?
[00:32:50.240 --> 00:33:00.400]   How do you source community ideas and yet keep predictable names and things like that?
[00:33:00.400 --> 00:33:09.080]   I don't know that we've dealt with the name thing as much, but I will say that we're not
[00:33:09.080 --> 00:33:18.240]   especially good at getting groundbreaking new contributions from the community.
[00:33:18.240 --> 00:33:23.400]   We have a real design ethos, like the product is designed in a certain way and it can be
[00:33:23.400 --> 00:33:29.560]   challenging for folks who aren't a part of all of these conversations about this to do
[00:33:29.560 --> 00:33:31.440]   big new things.
[00:33:31.440 --> 00:33:39.320]   I will say that we have done a better job over time of carving off spaces of the product
[00:33:39.320 --> 00:33:45.720]   that are much safer to get external contribution on.
[00:33:45.720 --> 00:33:51.320]   We now support a dozen or so database adapters.
[00:33:51.320 --> 00:33:57.360]   Increasingly it is the vendors for those database adapters that maintain their own adapter.
[00:33:57.360 --> 00:34:06.040]   That's a very well-defined surface area.
[00:34:06.040 --> 00:34:14.280]   I've never run an Apache project, but I have a lot of empathy for people who are trying
[00:34:14.280 --> 00:34:21.000]   to run open source projects without a benevolent dictator for life.
[00:34:21.000 --> 00:34:27.760]   It's legitimately very hard to work through these kinds of things purely in GitHub issues
[00:34:27.760 --> 00:34:28.760]   or things like that.
[00:34:28.760 --> 00:34:29.760]   Totally.
[00:34:29.760 --> 00:34:35.560]   And you wonder if the outcome of that consensus building might not be as good as if someone
[00:34:35.560 --> 00:34:38.760]   is disappointed, you make the call and drive forward.
[00:34:38.760 --> 00:34:42.760]   I mean, I don't take this necessarily better, but it's something that we think about it
[00:34:42.760 --> 00:34:43.760]   at Weights of Our Society.
[00:34:43.760 --> 00:34:44.760]   It takes more work.
[00:34:44.760 --> 00:34:45.760]   Yeah, for sure.
[00:34:45.760 --> 00:34:51.800]   I want to make sure I ask you about your community because you're so well known for the quality
[00:34:51.800 --> 00:34:52.800]   of your community.
[00:34:52.800 --> 00:34:57.080]   Can you kind of talk about what you do in community building and why they're even...
[00:34:57.080 --> 00:35:01.320]   I feel like a priori you might not even expect there to be such a vibrant community around
[00:35:01.320 --> 00:35:03.360]   a tool like dbt.
[00:35:03.360 --> 00:35:06.360]   How did that happen?
[00:35:06.360 --> 00:35:10.400]   Yeah, I think it is very interesting.
[00:35:10.400 --> 00:35:17.320]   And I want to have some epistemic humility in terms of like, I don't know.
[00:35:17.320 --> 00:35:23.640]   I have my own guesses as to why this happened, but community is an emergent phenomenon.
[00:35:23.640 --> 00:35:27.840]   And I think you could ask different people and different people would have different
[00:35:27.840 --> 00:35:28.840]   stories.
[00:35:28.840 --> 00:35:32.360]   So here's my belief.
[00:35:32.360 --> 00:35:46.320]   I think that there has been multiple decades of history of data people being undervalued.
[00:35:46.320 --> 00:35:51.320]   The tools that are built for them underestimate their capabilities.
[00:35:51.320 --> 00:35:57.520]   And for the first time...
[00:35:57.520 --> 00:36:00.160]   And tools that lock them in.
[00:36:00.160 --> 00:36:05.560]   So you're less willing to give back to a company that feels like it maybe doesn't have
[00:36:05.560 --> 00:36:07.240]   your interest at heart.
[00:36:07.240 --> 00:36:19.320]   So for the first time, I think we said to data people that we believe you're very capable
[00:36:19.320 --> 00:36:26.420]   and we think that there is this new way that you can work.
[00:36:26.420 --> 00:36:32.320]   And here's the little seed of a tool that will help you do that.
[00:36:32.320 --> 00:36:35.040]   And I think that people...
[00:36:35.040 --> 00:36:39.440]   I think all communities are really communities of identity.
[00:36:39.440 --> 00:36:42.900]   They have to feel seen and recognized.
[00:36:42.900 --> 00:36:45.680]   And that's what creates loyalty.
[00:36:45.680 --> 00:36:53.720]   And I think that that's why data people, especially early on, but still today, feel a deep affinity
[00:36:53.720 --> 00:36:59.000]   for the DBT community because it's the place that they feel like they're really seen and
[00:36:59.000 --> 00:37:00.000]   they're not underestimated.
[00:37:00.000 --> 00:37:01.000]   Interesting.
[00:37:01.000 --> 00:37:02.000]   Cool.
[00:37:02.000 --> 00:37:06.920]   That seems very plausible to me.
[00:37:06.920 --> 00:37:14.800]   And I don't think ML engineers are maybe historically disrespected in organizations.
[00:37:14.800 --> 00:37:17.040]   Maybe they're put on a pedestal.
[00:37:17.040 --> 00:37:21.320]   But I think Weights & Biases was one of the first companies with the point of view of
[00:37:21.320 --> 00:37:24.520]   like, "Hey, we're going to really serve this specific group."
[00:37:24.520 --> 00:37:30.080]   Where I think most of the earlier MLOps tools came with a more top-down mindset of like,
[00:37:30.080 --> 00:37:33.920]   "We're going to sell into CIOs and sell high in an organization."
[00:37:33.920 --> 00:37:39.920]   And I think whoever you sell to really ends up controlling your product direction is what
[00:37:39.920 --> 00:37:40.920]   I've seen.
[00:37:40.920 --> 00:37:41.920]   Totally.
[00:37:41.920 --> 00:37:42.920]   Totally.
[00:37:42.920 --> 00:37:44.920]   It does not.
[00:37:44.920 --> 00:37:46.440]   Okay.
[00:37:46.440 --> 00:37:55.880]   We do top-down sales at this point too, but it will always be like a compliment to bottoms-up
[00:37:55.880 --> 00:37:58.600]   community-led motion.
[00:37:58.600 --> 00:38:07.000]   It feels very surprising to me that, and maybe it's just because I don't understand the full
[00:38:07.000 --> 00:38:14.280]   ecosystem as well as I'd like to, but it feels very surprising to me that not all companies
[00:38:14.280 --> 00:38:20.280]   today in data are started with bottoms-up motion.
[00:38:20.280 --> 00:38:25.040]   It's so much more fun to build a business like this.
[00:38:25.040 --> 00:38:27.520]   I want to build a good product for CIOs.
[00:38:27.520 --> 00:38:32.800]   I want them to value what we do, but I want to spend my time talking to people that do
[00:38:32.800 --> 00:38:33.920]   the work.
[00:38:33.920 --> 00:38:35.600]   It's just more fun.
[00:38:35.600 --> 00:38:40.120]   Man, I feel exactly the same way.
[00:38:40.120 --> 00:38:45.320]   Why do you think there's still so many companies that build tools that intend to be top-down?
[00:38:45.320 --> 00:38:54.480]   Well, I think that building a company that sells lower in an organization first is a
[00:38:54.480 --> 00:38:57.480]   slower road.
[00:38:57.480 --> 00:39:00.920]   People have less budget.
[00:39:00.920 --> 00:39:05.360]   So I think in a smaller market where you need to do bigger deals, it might be necessary
[00:39:05.360 --> 00:39:07.120]   to sell higher in an organization.
[00:39:07.120 --> 00:39:13.280]   My first company, CrowdFlower, I think intuitively started off with a bottom-up sale, but towards
[00:39:13.280 --> 00:39:18.960]   the end really ended up serving folks higher in the organization just because I think the
[00:39:18.960 --> 00:39:24.400]   ML market at the time was smaller, so you couldn't do it.
[00:39:24.400 --> 00:39:30.080]   I think me and you have much more of the temperament to sell to people that are actually doing
[00:39:30.080 --> 00:39:33.320]   the work as I think of it.
[00:39:33.320 --> 00:39:37.760]   It is a market maturity thing.
[00:39:37.760 --> 00:39:46.400]   I think that the places in our space that are generally a little bit more tops-down
[00:39:46.400 --> 00:39:54.240]   are things like governance and cataloging and things that you need a lot of standardization.
[00:39:54.240 --> 00:39:56.280]   Maybe there's a compliance buyer, things like that.
[00:39:56.280 --> 00:40:02.680]   Well, do you think of Databricks and Snowflake as a top-down or a bottom-up sale?
[00:40:02.680 --> 00:40:04.040]   Is it obvious to you what they are?
[00:40:04.040 --> 00:40:09.320]   I mean, I sort of feel like people, you can kind of get started off the website, but I
[00:40:09.320 --> 00:40:13.760]   sort of view them as doing more of a top-down sale from my perspective, but you would know
[00:40:13.760 --> 00:40:16.240]   better than me.
[00:40:16.240 --> 00:40:18.880]   It's an interesting question.
[00:40:18.880 --> 00:40:26.360]   I guess when I think about this, I think about when a salesperson engages at a company, do
[00:40:26.360 --> 00:40:35.480]   they have to educate that buyer on what their thing does in the first place?
[00:40:35.480 --> 00:40:40.480]   For Snowflake and Databricks, I think by and large, their buyers already know who they
[00:40:40.480 --> 00:40:41.480]   are.
[00:40:41.480 --> 00:40:47.960]   The job of the salesperson in that context becomes partnering to make sure that there's
[00:40:47.960 --> 00:40:53.640]   like a million hurdles that will prevent you from effectively using Databricks, any data
[00:40:53.640 --> 00:40:55.760]   platform.
[00:40:55.760 --> 00:41:00.720]   The salesperson almost has to just project manage their way through both the consensus
[00:41:00.720 --> 00:41:06.160]   building process and the actual implementation process.
[00:41:06.160 --> 00:41:14.560]   I think that sometimes when you go to buy a data governance tool, it's like, "Well,
[00:41:14.560 --> 00:41:17.400]   I don't know what governance tools exist.
[00:41:17.400 --> 00:41:19.600]   Well, let's research them."
[00:41:19.600 --> 00:41:27.360]   Anyway, I would much rather come in and when we talk to data leaders, they're like, "Yeah,
[00:41:27.360 --> 00:41:28.360]   we know DBT.
[00:41:28.360 --> 00:41:31.600]   We heard you on the A16Z podcast," whatever.
[00:41:31.600 --> 00:41:34.600]   They probably already have some people who have tooled around with it internally.
[00:41:34.600 --> 00:41:37.480]   It's such a more fun conversation to have.
[00:41:37.480 --> 00:41:38.480]   Totally.
[00:41:38.480 --> 00:41:39.480]   Totally.
[00:41:39.480 --> 00:41:40.480]   Well, I mean, it's hard to do that.
[00:41:40.480 --> 00:41:43.600]   You've made a product that many, many, many people use.
[00:41:43.600 --> 00:41:51.360]   Growing 10% every month puts you in a rare category of growth.
[00:41:51.360 --> 00:41:57.840]   Do you have thoughts around where the data world is going?
[00:41:57.840 --> 00:42:01.680]   What parts of the stack are likely to change in the future?
[00:42:01.680 --> 00:42:06.720]   Gosh, that is a very big question.
[00:42:06.720 --> 00:42:16.920]   I just spoke to ... I wrote a blog post at the end of 2020 that made five predictions.
[00:42:16.920 --> 00:42:22.640]   By and large, I think that those stand up pretty well, but I think there's a new set
[00:42:22.640 --> 00:42:26.960]   of things that probably needs to be written.
[00:42:26.960 --> 00:42:37.560]   I just talked to a company that is building a layer that allows you to turn your data
[00:42:37.560 --> 00:42:42.760]   warehouse into a transactional data store.
[00:42:42.760 --> 00:42:48.440]   That is very interesting because if you think about all these SaaS products that have been
[00:42:48.440 --> 00:42:54.280]   built over the past, whatever, 15 years or something, each of them has their own separate
[00:42:54.280 --> 00:43:02.040]   data store, and then you have all this data engineering to do to make sure that the right
[00:43:02.040 --> 00:43:07.880]   data is in Salesforce, and then the Salesforce data comes back over into Zendesk.
[00:43:07.880 --> 00:43:10.380]   It gets a little silly.
[00:43:10.380 --> 00:43:14.840]   You could imagine that, well, we've centralized all of our organizational data with these
[00:43:14.840 --> 00:43:18.840]   data pipelines that were initially built for analytics, and the data warehouses themselves
[00:43:18.840 --> 00:43:21.440]   are primarily built for analytics too.
[00:43:21.440 --> 00:43:26.920]   What if we could have another data store that sat on top of it that had more transactional
[00:43:26.920 --> 00:43:35.680]   capabilities and would allow you to have lots of queries per second and really good insert
[00:43:35.680 --> 00:43:38.760]   and update times?
[00:43:38.760 --> 00:43:47.160]   Not just that capability, but the idea that the data warehouse will stop being just for
[00:43:47.160 --> 00:43:55.200]   analytical use cases and be for operational use cases, I think is a very interesting thread
[00:43:55.200 --> 00:43:57.040]   to pull on.
[00:43:57.040 --> 00:44:04.000]   I think that I have no insider knowledge here whatsoever, but my guess is that Snowflake
[00:44:04.000 --> 00:44:09.180]   and Databricks would love to invest in technology.
[00:44:09.180 --> 00:44:12.800]   If you look from the outside, Snowflake has changed its messaging over the years from
[00:44:12.800 --> 00:44:16.280]   being a data warehouse to a data platform.
[00:44:16.280 --> 00:44:19.760]   Now it's a data cloud.
[00:44:19.760 --> 00:44:26.480]   The game in compute is you want to handle more and more and more and more workloads.
[00:44:26.480 --> 00:44:32.720]   I think there's a lot of reasons that we as data professionals should like that, because
[00:44:32.720 --> 00:44:38.320]   it means that we wouldn't just be doing things in service of analytics.
[00:44:38.320 --> 00:44:45.120]   We can actually be a part of the product development organization inside of companies too.
[00:44:45.120 --> 00:44:47.360]   Wouldn't the latency need to come down to do that?
[00:44:47.360 --> 00:44:52.560]   You're talking about being literally something that the product actually queries in production?
[00:44:52.560 --> 00:44:53.560]   Totally.
[00:44:53.560 --> 00:44:54.560]   Okay.
[00:44:54.560 --> 00:45:02.160]   Imagine, and I think there's different ways to do this, and I've heard different proposals,
[00:45:02.160 --> 00:45:08.960]   but imagine that there's a caching layer on top of the warehouse and it's using replication
[00:45:08.960 --> 00:45:13.220]   to get a very consistent state of the world.
[00:45:13.220 --> 00:45:16.920]   Maybe there's a small lag between the data warehouse.
[00:45:16.920 --> 00:45:21.640]   Then you could imagine latency that actually was acceptable for a production application
[00:45:21.640 --> 00:45:22.640]   use case.
[00:45:22.640 --> 00:45:23.640]   I see.
[00:45:23.640 --> 00:45:24.640]   Interesting.
[00:45:24.640 --> 00:45:25.640]   Interesting.
[00:45:25.640 --> 00:45:29.480]   There's VCs that are all over.
[00:45:29.480 --> 00:45:32.760]   Martin Casado was on our board, very bullish on this trend.
[00:45:32.760 --> 00:45:35.960]   Tom Tunges was writing about this two years ago.
[00:45:35.960 --> 00:45:42.440]   I've always wondered, "Okay, but the data warehouses can't service that type of query
[00:45:42.440 --> 00:45:43.440]   pattern today."
[00:45:43.440 --> 00:45:50.080]   Maybe if you just wave a magic wand, you're like, "Somebody's going to fix that," then
[00:45:50.080 --> 00:45:52.680]   you could see some interesting things happen.
[00:45:52.680 --> 00:45:55.240]   Interesting.
[00:45:55.240 --> 00:46:02.320]   It's funny, I guess one space that I think is unsexy to VCs but still seems surprisingly
[00:46:02.320 --> 00:46:03.880]   broken to me is BI tools.
[00:46:03.880 --> 00:46:07.080]   I guess that's part of the stack, but it's just funny.
[00:46:07.080 --> 00:46:09.520]   I think so much money has gone into it.
[00:46:09.520 --> 00:46:10.640]   Every company uses it.
[00:46:10.640 --> 00:46:16.360]   There's clearly a market there, but I feel like I haven't seen a lot of new things happening
[00:46:16.360 --> 00:46:22.280]   and yet it's still quite a frustrating experience as a CEO.
[00:46:22.280 --> 00:46:30.640]   I do some very, very small scale angel investing and that is the area where I'm most interested
[00:46:30.640 --> 00:46:38.920]   in because I agree that many of the BI or analytics layer products that most companies
[00:46:38.920 --> 00:46:47.600]   use today started roughly 10-ish years ago, which in the world that we are operating in
[00:46:47.600 --> 00:46:49.200]   is kind of a long time.
[00:46:49.200 --> 00:46:50.200]   Totally.
[00:46:50.200 --> 00:46:53.120]   I don't necessarily mean that there's anything wrong with them, but I do want to see new
[00:46:53.120 --> 00:46:56.200]   takes arise.
[00:46:56.200 --> 00:46:58.760]   I think that that's starting to happen.
[00:46:58.760 --> 00:47:05.360]   I think that sometimes it is because in the same way that Redshift back kicking off the
[00:47:05.360 --> 00:47:11.600]   wave of the cloud data warehouse changed the priors for like, "Okay, what has to be true
[00:47:11.600 --> 00:47:16.320]   for me to make an application that looks like this?"
[00:47:16.320 --> 00:47:21.080]   DBT changes those priors again.
[00:47:21.080 --> 00:47:24.480]   If you're building a BI tool, you can just kind of assume that somebody is going to have
[00:47:24.480 --> 00:47:27.200]   a DBT project.
[00:47:27.200 --> 00:47:34.120]   You can actually plug into the graph and you can know a bunch of information about somebody's
[00:47:34.120 --> 00:47:38.200]   data before they've done literally anything in your product.
[00:47:38.200 --> 00:47:39.200]   Interesting.
[00:47:39.200 --> 00:47:47.960]   Well, we've talked a lot about data, but ML is so closely related to data, but I'm curious,
[00:47:47.960 --> 00:47:50.320]   is ML relevant to your company at all?
[00:47:50.320 --> 00:47:53.320]   Do you have any people working on ML internally?
[00:47:53.320 --> 00:48:00.400]   Do you think about ML when you think about what DBT should do?
[00:48:00.400 --> 00:48:07.040]   There are things that we care about from an ML perspective that we have not yet gotten
[00:48:07.040 --> 00:48:08.680]   to.
[00:48:08.680 --> 00:48:16.640]   They are frequently in the realm of developer experience.
[00:48:16.640 --> 00:48:22.400]   We have an IDE, a browser-based IDE that we sell to companies.
[00:48:22.400 --> 00:48:30.040]   There's a lot that you can do in that context to reduce the time to get from point A to
[00:48:30.040 --> 00:48:32.120]   point B.
[00:48:32.120 --> 00:48:44.840]   We have access to a lot of exhaust that comes out of the millions of DBT jobs that we process,
[00:48:44.840 --> 00:48:53.000]   and it would be great to use some of that to predict good and not good patterns for
[00:48:53.000 --> 00:48:55.840]   the way that you've built your DAG, written your code.
[00:48:55.840 --> 00:48:58.040]   None of these are things that we've...
[00:48:58.040 --> 00:49:08.360]   We operate solely in the land today of building developer tooling using very traditional approaches,
[00:49:08.360 --> 00:49:17.800]   but this stuff is not so far around the corner, and I'm excited about it.
[00:49:17.800 --> 00:49:20.760]   One more question before we get to the last two.
[00:49:20.760 --> 00:49:23.160]   How do you feel about SQL?
[00:49:23.160 --> 00:49:27.040]   It's been such a...
[00:49:27.040 --> 00:49:32.120]   I feel like of all the computer languages, it's survived the best.
[00:49:32.120 --> 00:49:38.540]   I feel like everyone knows SQL, everyone uses SQL.
[00:49:38.540 --> 00:49:40.200]   Something must be really good about it, I think.
[00:49:40.200 --> 00:49:42.200]   Do you think it's just sort of...
[00:49:42.200 --> 00:49:46.520]   Do you think it's kind of became a standard early and has just sort of stuck around as
[00:49:46.520 --> 00:49:51.820]   a standard despite its flaws, or do you feel like there's some brilliance in it that makes
[00:49:51.820 --> 00:49:52.820]   it work?
[00:49:52.820 --> 00:49:57.920]   Or do you wish that it would be replaced by something more modern?
[00:49:57.920 --> 00:50:08.440]   I think that standards are really interesting, and I don't know that there's a technical
[00:50:08.440 --> 00:50:17.280]   answer as to why TCP/IP and HTTP are the founding protocols of the internet.
[00:50:17.280 --> 00:50:21.600]   I think that they worked well enough and people consolidated around them, and then you have
[00:50:21.600 --> 00:50:23.520]   an ecosystem and there's...
[00:50:23.520 --> 00:50:26.920]   But wait, but wait, but languages don't usually work like that, right?
[00:50:26.920 --> 00:50:30.760]   I feel like the languages that I learned in school, even now, they're not...
[00:50:30.760 --> 00:50:35.440]   Mostly I learned Perl, that was the thing to use, and you don't see that much anymore.
[00:50:35.440 --> 00:50:36.880]   Okay, but so I think...
[00:50:36.880 --> 00:50:43.600]   And it's a great point, but I think that what happens with these protocols is TCP/IP and
[00:50:43.600 --> 00:50:47.240]   HTTP is that they get baked into products.
[00:50:47.240 --> 00:50:49.380]   They get baked into the Apache web server.
[00:50:49.380 --> 00:50:52.960]   They get baked into, et cetera.
[00:50:52.960 --> 00:51:00.880]   And that has network effects, because when all the other vendors support this thing,
[00:51:00.880 --> 00:51:04.120]   then well, we got to support it too, and then everybody just kind of agree, "Okay, basically
[00:51:04.120 --> 00:51:06.720]   this is good enough."
[00:51:06.720 --> 00:51:11.300]   With a language, with Python, you run it yourself.
[00:51:11.300 --> 00:51:14.160]   You don't need it to be executed anywhere else.
[00:51:14.160 --> 00:51:19.560]   And so every individual engineer or engineering team can kind of choose Python or Go or TypeScript
[00:51:19.560 --> 00:51:24.080]   or whatever, and they get to make that decision without any network effects being involved
[00:51:24.080 --> 00:51:25.080]   at all.
[00:51:25.080 --> 00:51:35.460]   But SQL is more like HTTP than it is like Go, because you, as the person choosing to
[00:51:35.460 --> 00:51:39.840]   write it, are not controlling the execution environment.
[00:51:39.840 --> 00:51:46.240]   You buy a database, and there's only a certain number of databases, and they all use SQL.
[00:51:46.240 --> 00:51:52.120]   And well, okay, maybe not all of them, but by and large, most of them historically.
[00:51:52.120 --> 00:52:00.880]   And so not only are there these network effects around, "Because the vendors support it, then
[00:52:00.880 --> 00:52:05.640]   I have to learn it," but then there's the return network effects where, "Well, because
[00:52:05.640 --> 00:52:11.320]   everyone knows SQL, I'm also going to build a product built around that."
[00:52:11.320 --> 00:52:14.160]   So Snowflake could have said, "Snowflake is a brand new database."
[00:52:14.160 --> 00:52:17.920]   In 2012, they could have said, "We're going to invent our own language," but that doesn't
[00:52:17.920 --> 00:52:21.960]   make any sense because Tableau already works with SQL and everything already works with
[00:52:21.960 --> 00:52:22.960]   SQL.
[00:52:22.960 --> 00:52:32.560]   But I guess it's funny, Java or the JVM has some of that, and then you see stuff like Scala
[00:52:32.560 --> 00:52:34.080]   getting written on top of that or compiling down to that.
[00:52:34.080 --> 00:52:37.040]   But yet everything that compiles down to SQL is just enraging.
[00:52:37.040 --> 00:52:41.200]   I feel like every time I've used a higher level on top of SQL, all the different versions,
[00:52:41.200 --> 00:52:43.840]   I feel like I've tried them and something about it just-
[00:52:43.840 --> 00:52:45.880]   Like Active Record or an ORM or something?
[00:52:45.880 --> 00:52:48.600]   Yeah, every ORM is just like you want to...
[00:52:48.600 --> 00:52:51.040]   At first it feels good, and then you just tear out your hair.
[00:52:51.040 --> 00:52:52.800]   You get into the edge cases and it's terrible.
[00:52:52.800 --> 00:52:53.800]   Yeah.
[00:52:53.800 --> 00:52:54.800]   Yeah.
[00:52:54.800 --> 00:52:58.720]   Why hasn't someone built a higher level construct on top of that that works well?
[00:52:58.720 --> 00:53:00.040]   So I totally agree with that.
[00:53:00.040 --> 00:53:01.840]   I think that that is...
[00:53:01.840 --> 00:53:03.400]   We didn't talk about this pre-taping.
[00:53:03.400 --> 00:53:08.480]   I'm so excited to be talking about this.
[00:53:08.480 --> 00:53:12.840]   That is generally how standards progress.
[00:53:12.840 --> 00:53:16.120]   There's this base thing and then people are like, "Okay, that's good enough for what it
[00:53:16.120 --> 00:53:17.120]   does."
[00:53:17.120 --> 00:53:20.640]   And then they're like, "Well, let's build a higher level of abstraction and we'll solve
[00:53:20.640 --> 00:53:21.640]   some of the..."
[00:53:21.640 --> 00:53:27.200]   This is like JavaScript and et cetera.
[00:53:27.200 --> 00:53:35.880]   We talk about this internally as who's going to build the React for SQL.
[00:53:35.880 --> 00:53:37.800]   I'm very interested in that question.
[00:53:37.800 --> 00:53:40.320]   I believe that will happen over the next five years.
[00:53:40.320 --> 00:53:46.800]   I think that there's too much money floating around and incentive to want...
[00:53:46.800 --> 00:53:50.480]   So we solved it.
[00:53:50.480 --> 00:53:55.200]   The way that dbt works, it's very similar to Ruby on Rails back in the day with .erb
[00:53:55.200 --> 00:53:56.200]   files.
[00:53:56.200 --> 00:54:03.840]   So there's templating, but we didn't build React.
[00:54:03.840 --> 00:54:08.280]   I think that either we will or somebody will.
[00:54:08.280 --> 00:54:12.440]   If somebody builds it and it's not us, then I'm very happy to just have it be another
[00:54:12.440 --> 00:54:16.200]   choice of language that you can put into your dbt DAGs.
[00:54:16.200 --> 00:54:17.560]   I agree.
[00:54:17.560 --> 00:54:23.080]   I think that using templating, we've made a lot of progress in what you can idiomatically
[00:54:23.080 --> 00:54:33.800]   express in SQL, but it's still not as pleasant of an experience as just writing other languages.
[00:54:33.800 --> 00:54:36.120]   Are you working on this?
[00:54:36.120 --> 00:54:37.120]   Not today.
[00:54:37.120 --> 00:54:47.880]   So the one person who is working on this in public is Lloyd Tab, the founder of Looker.
[00:54:47.880 --> 00:54:50.920]   This has been Lloyd's passion project for a little while.
[00:54:50.920 --> 00:54:52.600]   It's called Malloy.
[00:54:52.600 --> 00:54:57.280]   You can find it on their public GitHub.
[00:54:57.280 --> 00:54:59.520]   It's very interesting.
[00:54:59.520 --> 00:55:05.200]   It's not exactly how I would build it, but also I recently got a demo from him and there's
[00:55:05.200 --> 00:55:10.760]   some real magical capabilities there that I had never even thought to want out of my
[00:55:10.760 --> 00:55:11.760]   SQL-like language.
[00:55:11.760 --> 00:55:14.360]   So I don't know.
[00:55:14.360 --> 00:55:18.040]   I would like this as much as you or anybody else.
[00:55:18.040 --> 00:55:19.440]   Very cool.
[00:55:19.440 --> 00:55:25.200]   Well, we always end with two questions and I want to modify them for you, I guess.
[00:55:25.200 --> 00:55:29.480]   We usually ask what's an underrated topic in ML, but maybe I'll ask you what's an underrated
[00:55:29.480 --> 00:55:30.800]   topic in data?
[00:55:30.800 --> 00:55:33.200]   I mean, we've covered some of them, but what-
[00:55:33.200 --> 00:55:35.120]   Wait, can I answer in ML?
[00:55:35.120 --> 00:55:36.120]   Oh, sure.
[00:55:36.120 --> 00:55:37.120]   Please.
[00:55:37.120 --> 00:55:38.120]   Okay.
[00:55:38.120 --> 00:55:39.120]   Yes.
[00:55:39.120 --> 00:55:40.120]   Absolutely.
[00:55:40.120 --> 00:55:46.800]   I think that ML has a persona problem and that there's been some reckoning with this.
[00:55:46.800 --> 00:55:53.720]   There's some, like, make ML more accessible tooling.
[00:55:53.720 --> 00:55:58.060]   In general, I don't feel like that has been spot on.
[00:55:58.060 --> 00:56:06.240]   It's clear that the tools for the big kids are really where everybody's focused on today.
[00:56:06.240 --> 00:56:07.240]   There are some...
[00:56:07.240 --> 00:56:11.960]   There's a company called Continual.
[00:56:11.960 --> 00:56:19.840]   There's a couple other companies in the space of trying to bring ML to the types of workflows
[00:56:19.840 --> 00:56:22.720]   that people in my world use.
[00:56:22.720 --> 00:56:25.000]   And I would desperately love that.
[00:56:25.000 --> 00:56:36.640]   I'm very familiar with what is going on inside of an ML model, but it is also clearly not
[00:56:36.640 --> 00:56:42.440]   exposed to me in a way that is idiomatic for me to participate in this workflow.
[00:56:42.440 --> 00:56:44.400]   So I'm excited about that gap being bridged.
[00:56:44.400 --> 00:56:45.400]   Interesting.
[00:56:45.400 --> 00:56:49.960]   So making it simpler to just make an ML model from a set of data.
[00:56:49.960 --> 00:56:51.000]   Yeah.
[00:56:51.000 --> 00:56:55.680]   So what Continual is doing is they're actually plugging into the metadata inside of DBT and
[00:56:55.680 --> 00:57:03.200]   you can actually add some additional metadata properties that declare certain fields inside
[00:57:03.200 --> 00:57:10.000]   of a DBT model as being your features and here's the success criteria.
[00:57:10.000 --> 00:57:15.880]   And then Continual plugs in with its own auto ML process and trains a model and dumps it
[00:57:15.880 --> 00:57:17.640]   back into your data warehouse for you.
[00:57:17.640 --> 00:57:18.640]   Wow.
[00:57:18.640 --> 00:57:19.640]   And do you actually use that?
[00:57:19.640 --> 00:57:20.640]   I don't.
[00:57:20.640 --> 00:57:23.520]   They're super early.
[00:57:23.520 --> 00:57:26.400]   I would like to get my hands on it and use it myself.
[00:57:26.400 --> 00:57:27.640]   They have customers though.
[00:57:27.640 --> 00:57:28.640]   Cool.
[00:57:28.640 --> 00:57:29.640]   Awesome.
[00:57:29.640 --> 00:57:30.640]   Continual.
[00:57:30.640 --> 00:57:31.640]   I'll check them out.
[00:57:32.480 --> 00:57:33.480]   Okay.
[00:57:33.480 --> 00:57:37.880]   And then the final question is usually what's hard about getting ML working in production?
[00:57:37.880 --> 00:57:39.720]   And people usually answer that question.
[00:57:39.720 --> 00:57:43.800]   We should actually do a graph of this, but I think the most common answer is usually
[00:57:43.800 --> 00:57:47.060]   the data pipeline feeding into the ML model.
[00:57:47.060 --> 00:57:53.400]   So maybe within that, when you see companies trying to set up a working data pipeline,
[00:57:53.400 --> 00:57:54.400]   what's the long pole?
[00:57:54.400 --> 00:58:00.120]   What's the place where people usually get stuck?
[00:58:00.120 --> 00:58:02.880]   Building data pipelines is very hard.
[00:58:02.880 --> 00:58:08.720]   It's not very hard for people who live in this world all the time, every day, but it's
[00:58:08.720 --> 00:58:14.200]   still effort and time intensive for us.
[00:58:14.200 --> 00:58:25.480]   So I think that the whole world of observability, reliability, all of this stuff.
[00:58:25.480 --> 00:58:35.080]   My answer to ML in production is I don't totally understand why...
[00:58:35.080 --> 00:58:41.360]   So dbt runs on Spark, dbt runs on Databricks.
[00:58:41.360 --> 00:58:47.320]   Both Spark and Databricks have SQL runtimes and so we can plug directly into them.
[00:58:47.320 --> 00:58:55.120]   And yet that is not where most of our users are today.
[00:58:55.120 --> 00:59:02.520]   So there's fundamentally not that much difference between doing feature engineering and doing
[00:59:02.520 --> 00:59:04.400]   what we would call data transformation.
[00:59:04.400 --> 00:59:07.000]   You're doing the same damn stuff.
[00:59:07.000 --> 00:59:13.400]   I think that the answer to why these two groups of humans do not consolidate or collaborate
[00:59:13.400 --> 00:59:17.800]   more effectively is again, the same reason that it goes in reverse.
[00:59:17.800 --> 00:59:22.920]   Most ML people I think don't think in SQL.
[00:59:22.920 --> 00:59:32.400]   And I'm excited because more and more of these data platforms are exposing Python remotely.
[00:59:32.400 --> 00:59:37.040]   So dbt does not do any local execution at all.
[00:59:37.040 --> 00:59:42.680]   And we ship SQL to a data warehouse, which executes it.
[00:59:42.680 --> 00:59:51.080]   And the funny thing is that that type of interactive work doesn't exist in the Python ML ecosystem
[00:59:51.080 --> 00:59:52.080]   as much.
[00:59:52.080 --> 00:59:55.800]   Mostly it's like you're on a machine, you're running it there.
[00:59:55.800 --> 01:00:04.520]   And so Databricks has a notebook API that we can plug into to actually run PySpark code
[01:00:04.520 --> 01:00:06.760]   on.
[01:00:06.760 --> 01:00:13.560]   Snowflake has a new thing called Snowpark where you can do remote execution of Python.
[01:00:13.560 --> 01:00:20.200]   So I think that we are going to be working from our end to close this language gap that
[01:00:20.200 --> 01:00:22.000]   exists in practitioners today.
[01:00:22.000 --> 01:00:23.000]   Cool.
[01:00:23.000 --> 01:00:24.000]   Awesome.
[01:00:24.000 --> 01:00:25.000]   Well, thanks for your time.
[01:00:25.000 --> 01:00:26.000]   This is super fun.
[01:00:26.000 --> 01:00:27.000]   And I learned a lot.
[01:00:27.000 --> 01:00:30.000]   So I have a feeling our audience will also learn a lot.
[01:00:30.000 --> 01:00:31.000]   Thanks.
[01:00:31.000 --> 01:00:32.000]   Thank you.
[01:00:32.000 --> 01:00:33.000]   It's been a lot of fun.
[01:00:33.000 --> 01:00:36.240]   If you're enjoying these interviews and you want to learn more, please click on the link
[01:00:36.240 --> 01:00:40.960]   to the show notes in the description where you can find links to all the papers that
[01:00:40.960 --> 01:00:45.360]   are mentioned, supplemental material, and a transcription that we work really hard to
[01:00:45.360 --> 01:00:46.360]   produce.
[01:00:46.360 --> 01:00:46.360]   So check it out.
[01:00:46.360 --> 01:00:49.420]   [MUSIC PLAYING]

