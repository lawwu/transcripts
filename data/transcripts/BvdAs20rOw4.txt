
[00:00:00.000 --> 00:00:03.280]   We have a few things to cover today, so I'm going to dive right in.
[00:00:03.280 --> 00:00:08.080]   First, as always, I want to just kind of quickly review the troubleshooting lecture
[00:00:08.080 --> 00:00:11.360]   and see if there are any questions or points that were unclear there.
[00:00:11.360 --> 00:00:13.960]   But we'll try to move through that relatively fast.
[00:00:13.960 --> 00:00:18.880]   And then we'll talk briefly about what you worked on for your projects last week
[00:00:18.880 --> 00:00:22.440]   and then what we're hoping that you can get done this coming week.
[00:00:22.440 --> 00:00:25.320]   And then we have another lab.
[00:00:25.320 --> 00:00:26.040]   This is a fun one.
[00:00:26.040 --> 00:00:28.080]   It's on debugging neural networks.
[00:00:28.080 --> 00:00:31.920]   And so I'm going to ask you to actually take some really bad neural network
[00:00:31.920 --> 00:00:35.480]   code that I wrote and make it better.
[00:00:35.480 --> 00:00:39.880]   And then we have a great guest lecture at the end.
[00:00:39.880 --> 00:00:42.400]   So troubleshooting lecture.
[00:00:42.400 --> 00:00:46.400]   I think there's-- kind of the core of this lecture was in two parts.
[00:00:46.400 --> 00:00:50.560]   The first is thinking a little bit about why troubleshooting is a hard problem.
[00:00:50.560 --> 00:00:54.600]   And I think there the summary is kind of, well,
[00:00:54.600 --> 00:00:58.240]   one thing that makes deep learning different than traditional machine
[00:00:58.240 --> 00:01:01.080]   learning or traditional software engineering is that it can be really,
[00:01:01.080 --> 00:01:03.320]   really difficult to even tell if you have a bug.
[00:01:03.320 --> 00:01:06.000]   And once you know that your performance is bad,
[00:01:06.000 --> 00:01:09.240]   there's many different contributing sources of error that could be causing that.
[00:01:09.240 --> 00:01:13.320]   And on top of that, to make matters worse,
[00:01:13.320 --> 00:01:17.760]   you can have a single hyperparameter that small fluctuations in its value
[00:01:17.760 --> 00:01:19.760]   cause big changes in performance.
[00:01:19.760 --> 00:01:22.480]   So all these things are sort of contributors
[00:01:22.480 --> 00:01:25.200]   to why this is such a hard problem.
[00:01:25.200 --> 00:01:29.400]   And so because of this, because there's many different sources
[00:01:29.400 --> 00:01:32.840]   of the same error, the key idea that I tried to get across
[00:01:32.840 --> 00:01:35.000]   was really the thing that you should do is you
[00:01:35.000 --> 00:01:36.680]   should start as simple as possible.
[00:01:36.680 --> 00:01:41.000]   So start with the simplest possible version of your model and your data.
[00:01:41.000 --> 00:01:45.840]   And then gradually layer on complexity, kind of one layer at a time,
[00:01:45.840 --> 00:01:48.240]   so that you can rule out the sources of error
[00:01:48.240 --> 00:01:49.840]   as early as possible in your process.
[00:01:49.840 --> 00:01:57.400]   And then I talked through a strategy for troubleshooting deep neural networks.
[00:01:57.400 --> 00:01:58.440]   There's a few steps here.
[00:01:58.440 --> 00:02:00.200]   The first was, again, starting simple.
[00:02:00.200 --> 00:02:03.160]   So choosing the simplest version of your problem,
[00:02:03.160 --> 00:02:04.520]   simplest version of your data.
[00:02:04.520 --> 00:02:07.560]   Then you actually go and implement that.
[00:02:07.560 --> 00:02:10.040]   Once you've implemented it and you convince yourself
[00:02:10.040 --> 00:02:12.440]   that you're pretty sure that there are no bugs there,
[00:02:12.440 --> 00:02:16.320]   then you go and evaluate it on your validation or your test set.
[00:02:16.320 --> 00:02:19.440]   And based on what that says, then you can go and make improvements
[00:02:19.440 --> 00:02:21.280]   to the model and the data set.
[00:02:21.280 --> 00:02:23.240]   Or you can tune hyperparameters.
[00:02:23.240 --> 00:02:26.120]   Or maybe your performance meets what your expectations are,
[00:02:26.120 --> 00:02:31.280]   and you can be done and try to start deploying it.
[00:02:31.280 --> 00:02:34.480]   So starting simple, again, there are a few steps here.
[00:02:34.480 --> 00:02:36.160]   Simple architecture.
[00:02:36.160 --> 00:02:38.560]   To start, there's only a few model architectures
[00:02:38.560 --> 00:02:43.120]   that I really consider for version 0 of the project.
[00:02:43.120 --> 00:02:47.200]   And I would almost always start with AtomOptimizer,
[00:02:47.200 --> 00:02:51.240]   no regularization, normalized inputs, and if you can,
[00:02:51.240 --> 00:02:52.800]   a simpler version of the problem.
[00:02:52.800 --> 00:02:58.720]   So maybe a small synthetic data set or fewer classes or something like that.
[00:02:58.720 --> 00:03:00.800]   And again, this is all in service of making sure
[00:03:00.800 --> 00:03:03.600]   that you can get something working end-to-end right away
[00:03:03.600 --> 00:03:06.760]   and convincing yourself that there are no bugs in your pipeline.
[00:03:10.720 --> 00:03:13.240]   All right, and so once you've made some simple choices,
[00:03:13.240 --> 00:03:17.360]   then the next step is to actually go and implement that model and debug it.
[00:03:17.360 --> 00:03:21.160]   And so the first thing you need to do is make sure that it even runs.
[00:03:21.160 --> 00:03:24.320]   And the main technique here to make sure that that works
[00:03:24.320 --> 00:03:28.160]   is to step through things in a debugger and watch out
[00:03:28.160 --> 00:03:31.000]   for making sure that shapes are all the right sizes
[00:03:31.000 --> 00:03:34.960]   and that you're not casting things incorrectly and things like that.
[00:03:34.960 --> 00:03:38.560]   And once the model runs, then one thing I always recommend doing
[00:03:38.560 --> 00:03:40.960]   is overfitting a single batch of data.
[00:03:40.960 --> 00:03:45.320]   What this means is driving loss all the way to 0 on a single batch of data.
[00:03:45.320 --> 00:03:48.400]   And this will catch a lot of errors, especially
[00:03:48.400 --> 00:03:52.320]   in your data pre-processing and things like that.
[00:03:52.320 --> 00:03:55.440]   And once you've convinced yourself that you can overfit a single batch,
[00:03:55.440 --> 00:04:00.640]   then ideally, you can compare the score that your model gets
[00:04:00.640 --> 00:04:03.800]   to some sort of known result. And there are different levels
[00:04:03.800 --> 00:04:06.960]   of the quality of known results, but even something as simple
[00:04:06.960 --> 00:04:10.760]   as just a heuristic baseline or the average of your model's outputs
[00:04:10.760 --> 00:04:13.320]   is still giving you value in knowing that your model is
[00:04:13.320 --> 00:04:14.600]   doing something useful at all.
[00:04:14.600 --> 00:04:23.720]   OK, so once you've implemented your model and you feel like there's probably
[00:04:23.720 --> 00:04:27.160]   no bugs, then the next step is to look at the performance of your model
[00:04:27.160 --> 00:04:28.680]   and decide what to do next.
[00:04:28.680 --> 00:04:33.080]   And so the key idea here is that if you look at your error on your test set,
[00:04:33.080 --> 00:04:35.280]   you can break that down into some components.
[00:04:35.280 --> 00:04:37.880]   The first component is something called irreducible error, which
[00:04:37.880 --> 00:04:40.400]   just means the error that you're not really expecting
[00:04:40.400 --> 00:04:42.360]   your model to do better than.
[00:04:42.360 --> 00:04:46.920]   And then on top of that is bias, which is how much you're
[00:04:46.920 --> 00:04:48.480]   underfitting your data set.
[00:04:48.480 --> 00:04:51.000]   Variance, which is how much you're overfitting your data set.
[00:04:51.000 --> 00:04:56.080]   Distribution shift, which tells you how far apart your training and test
[00:04:56.080 --> 00:04:57.200]   distributions are.
[00:04:57.200 --> 00:04:59.520]   And then finally, overfitting to your validation set,
[00:04:59.520 --> 00:05:03.840]   which can happen if you optimize hyperparameters on your validation set
[00:05:03.840 --> 00:05:04.520]   too many times.
[00:05:04.520 --> 00:05:10.760]   OK, so now you've evaluated your model's performance
[00:05:10.760 --> 00:05:13.340]   and you've gotten a sense of which of these components
[00:05:13.340 --> 00:05:15.680]   is the main contributor to the error.
[00:05:15.680 --> 00:05:19.560]   And then the next step is to figure out how to improve your model.
[00:05:19.560 --> 00:05:23.320]   And this is the order that I recommend doing it in.
[00:05:23.320 --> 00:05:25.000]   First, you address underfitting.
[00:05:25.000 --> 00:05:29.760]   So first, you make sure that you can get train error as low as you want it to go.
[00:05:29.760 --> 00:05:33.840]   Once you can do that, then I move on to overfitting.
[00:05:33.840 --> 00:05:39.800]   So then make sure that the gap between train and validation is not too large.
[00:05:39.800 --> 00:05:42.960]   Once you're not overfitting, then you can move on to distribution shift.
[00:05:42.960 --> 00:05:47.560]   So if you're in the situation where you can't collect infinite data
[00:05:47.560 --> 00:05:50.360]   from your test distribution, then just making sure
[00:05:50.360 --> 00:05:52.440]   that there's not too big a gap between the data
[00:05:52.440 --> 00:05:54.640]   you do have from your test distribution and the data
[00:05:54.640 --> 00:05:57.120]   from your training distribution.
[00:05:57.120 --> 00:06:01.160]   And then finally, and only infrequently, evaluate performance
[00:06:01.160 --> 00:06:03.360]   on your actual held out test set.
[00:06:03.360 --> 00:06:05.920]   And just see if you've overfitted your validation set
[00:06:05.920 --> 00:06:08.200]   by trying too many hyperparameters.
[00:06:08.200 --> 00:06:14.680]   OK, and then the last topic that I covered
[00:06:14.680 --> 00:06:18.120]   was just how to think about hyperparameter optimization.
[00:06:18.120 --> 00:06:19.720]   And there's two components to this.
[00:06:19.720 --> 00:06:22.760]   One is, how do you pick which hyperparameters to optimize?
[00:06:22.760 --> 00:06:26.080]   And there's kind of a list in the slides
[00:06:26.080 --> 00:06:29.440]   there of how I typically think about that.
[00:06:29.440 --> 00:06:31.280]   But it tends to be very problem dependent.
[00:06:31.280 --> 00:06:35.000]   And so there's no really easy answer to that other than just try things.
[00:06:35.000 --> 00:06:37.680]   And then once you've picked which hyperparameters to optimize,
[00:06:37.680 --> 00:06:40.080]   how do you actually optimize them?
[00:06:40.080 --> 00:06:44.880]   I think early on in a project, you can even just do a grid search.
[00:06:44.880 --> 00:06:47.720]   Or if you want to be fancier than that and better than that,
[00:06:47.720 --> 00:06:50.100]   you could do a course-defined random search.
[00:06:50.100 --> 00:06:53.880]   And as your organization gets more mature and your project gets more mature,
[00:06:53.880 --> 00:06:58.120]   then you can consider using Bayesian hyperparameter optimization, which
[00:06:58.120 --> 00:07:01.040]   generally works better, but can be kind of hard to implement
[00:07:01.040 --> 00:07:02.400]   and can introduce bugs of its own.
[00:07:02.400 --> 00:07:06.720]   Cool.
[00:07:06.720 --> 00:07:11.120]   That's the quick five minute summary of the hour and a half lecture.
[00:07:11.120 --> 00:07:13.800]   I'd love to hear if there are any questions about the lecture, things
[00:07:13.800 --> 00:07:17.480]   I just covered, or things that were in the video that you were curious about.
[00:07:17.480 --> 00:07:17.980]   Yeah?
[00:07:17.980 --> 00:07:27.940]   Yeah.
[00:07:27.940 --> 00:07:31.420]   Since-- I'll start from the very end.
[00:07:31.420 --> 00:07:34.420]   Of course, this should be done at the very end of the development process.
[00:07:34.420 --> 00:07:37.420]   But course-defined random search implies
[00:07:37.420 --> 00:07:43.380]   that you have this space of hyperparameters.
[00:07:43.380 --> 00:07:46.380]   How do you even define this space?
[00:07:46.380 --> 00:07:48.280]   Let's say there are--
[00:07:48.280 --> 00:07:52.240]   I can think of learning rates and big or small learning rates.
[00:07:52.240 --> 00:07:53.720]   But there are also things--
[00:07:53.720 --> 00:07:59.200]   I don't know-- type of initializations and type of things,
[00:07:59.200 --> 00:08:03.080]   upon which defining the order itself is hard.
[00:08:03.080 --> 00:08:09.560]   So how do I go about bisecting those options into subparts?
[00:08:09.560 --> 00:08:15.760]   So is the question for things like, what type of layer should I use?
[00:08:15.760 --> 00:08:18.200]   How do you think about dividing that up?
[00:08:18.200 --> 00:08:20.800]   Yeah, I think for things like that--
[00:08:20.800 --> 00:08:25.320]   so generally, for these hyperparameter optimization methods,
[00:08:25.320 --> 00:08:27.920]   generally, this is useful when you have hyperparameters that
[00:08:27.920 --> 00:08:29.660]   come from some continuous space.
[00:08:29.660 --> 00:08:32.600]   Or at least there's a large number of them, and they have some ordering.
[00:08:32.600 --> 00:08:37.960]   For hyperparameters that are more like design choices of your algorithm,
[00:08:37.960 --> 00:08:42.620]   like which optimizer should you use, or what type of layer should you use,
[00:08:42.620 --> 00:08:45.320]   which non-linearity should you use?
[00:08:45.320 --> 00:08:47.900]   Usually, there's only a few choices that you might want to try.
[00:08:47.900 --> 00:08:52.960]   And so you can just try each one of them.
[00:08:52.960 --> 00:08:53.460]   Yeah.
[00:08:53.460 --> 00:08:56.440]   And you can't try every single possible type of layer, for example.
[00:08:56.440 --> 00:08:58.640]   But this is where you need to use intuition
[00:08:58.640 --> 00:09:02.480]   and figure out which ones are most likely to be successful.
[00:09:02.480 --> 00:09:02.980]   Yeah?
[00:09:02.980 --> 00:09:07.960]   When you talk about trying different things,
[00:09:07.960 --> 00:09:11.960]   when you try one configuration, hyperparameter configuration,
[00:09:11.960 --> 00:09:18.160]   do you need to train it for that for the whole number of epochs?
[00:09:18.160 --> 00:09:22.680]   Or is it a good indication to see how it's doing in the beginning?
[00:09:22.680 --> 00:09:24.520]   Is that going to be a good indication of how
[00:09:24.520 --> 00:09:29.000]   it's going to do just to save resources and to be
[00:09:29.000 --> 00:09:30.840]   able to try different configurations?
[00:09:30.840 --> 00:09:32.240]   Yeah.
[00:09:32.240 --> 00:09:36.160]   So unfortunately, you can't always tell from the first few epochs
[00:09:36.160 --> 00:09:39.520]   of training which hyperparameters are going to end up performing best.
[00:09:39.520 --> 00:09:42.440]   That said, I think a lot of people in practice
[00:09:42.440 --> 00:09:45.480]   do use that because often it's a good indication.
[00:09:45.480 --> 00:09:48.200]   And you can often throw away some hyperparameters that are just
[00:09:48.200 --> 00:09:50.440]   clearly not learning at all.
[00:09:50.440 --> 00:09:52.760]   So if you're early on in this process,
[00:09:52.760 --> 00:09:59.640]   and you're just trying to figure out at a very coarse level what
[00:09:59.640 --> 00:10:03.280]   your learning rate should be, then if something just
[00:10:03.280 --> 00:10:05.200]   doesn't train at all for the first few epochs,
[00:10:05.200 --> 00:10:07.360]   then you can probably just throw that away.
[00:10:07.360 --> 00:10:10.200]   I think also some of the Bayesian hyperparameter optimization
[00:10:10.200 --> 00:10:15.840]   techniques do clever things with varying
[00:10:15.840 --> 00:10:18.120]   which hyperparameters they're training at which stages
[00:10:18.120 --> 00:10:20.200]   of training.
[00:10:20.200 --> 00:10:21.040]   Yeah.
[00:10:21.040 --> 00:10:24.000]   So for this hyperparameter optimization,
[00:10:24.000 --> 00:10:27.960]   should we assess some other kind of time model to do it?
[00:10:27.960 --> 00:10:29.960]   Or do we do it with the other original model,
[00:10:29.960 --> 00:10:32.920]   such as a complex model with all the data?
[00:10:32.920 --> 00:10:35.920]   Or do you want to make it something like auto-argument
[00:10:35.920 --> 00:10:39.400]   which I read about where they use separate techniques
[00:10:39.400 --> 00:10:42.880]   to work on a time model and update tables for--
[00:10:42.880 --> 00:10:43.880]   what do you say?
[00:10:43.880 --> 00:10:47.360]   [INAUDIBLE]
[00:10:47.360 --> 00:10:48.280]   Yeah.
[00:10:48.280 --> 00:10:50.200]   Not totally sure I understand the question.
[00:10:50.200 --> 00:10:55.760]   Like, do we search on our existing model
[00:10:55.760 --> 00:10:57.720]   with all the data sets that we have?
[00:10:57.720 --> 00:10:59.680]   Or should we do like--
[00:10:59.680 --> 00:11:04.640]   take a set of samples and then do a different model
[00:11:04.640 --> 00:11:08.200]   and then select a different set?
[00:11:08.200 --> 00:11:11.000]   Like, putting aside things like auto-augment,
[00:11:11.000 --> 00:11:13.240]   if you're just doing standard hyperparameter tuning,
[00:11:13.240 --> 00:11:15.600]   depends on which stage you're at in the project.
[00:11:15.600 --> 00:11:20.880]   Like, if you're still working on a really simplified version
[00:11:20.880 --> 00:11:23.080]   of your problem and you still can't get your training
[00:11:23.080 --> 00:11:24.560]   error low enough, then I would keep
[00:11:24.560 --> 00:11:26.040]   working with that simplified version of the problem
[00:11:26.040 --> 00:11:27.880]   until you can find hyperparameters that do
[00:11:27.880 --> 00:11:29.560]   get your training error low enough.
[00:11:29.560 --> 00:11:31.800]   And then as you move on, you might need to--
[00:11:31.800 --> 00:11:34.240]   like, your data set will get bigger and more complex
[00:11:34.240 --> 00:11:36.560]   as a function of just being better at working
[00:11:36.560 --> 00:11:37.880]   with a simpler data set.
[00:11:37.880 --> 00:11:38.800]   And then at that point, when you're
[00:11:38.800 --> 00:11:40.040]   doing hyperparameter searches, I would
[00:11:40.040 --> 00:11:42.080]   use the full version of the data set
[00:11:42.080 --> 00:11:45.080]   that you're working with at that time.
[00:11:45.080 --> 00:11:46.080]   Yeah?
[00:11:46.080 --> 00:11:56.560]   [INAUDIBLE]
[00:11:56.560 --> 00:11:58.320]   Yeah.
[00:11:58.320 --> 00:12:00.720]   So this does happen.
[00:12:00.720 --> 00:12:05.000]   Usually, I've found that the hyperparameters you
[00:12:05.000 --> 00:12:07.000]   choose for the smaller and simpler data
[00:12:07.000 --> 00:12:09.200]   set are a reasonable starting point, at least
[00:12:09.200 --> 00:12:10.200]   for the bigger data set.
[00:12:10.200 --> 00:12:15.080]   And so often, the process of hyperparameter optimization
[00:12:15.080 --> 00:12:17.200]   in practice looks something like you just
[00:12:17.200 --> 00:12:20.520]   sample hyperparameters on an exponential grid at first
[00:12:20.520 --> 00:12:23.640]   because you have no idea, should the learning rate be 1
[00:12:23.640 --> 00:12:26.800]   or should it be 10 to the negative 6 or something?
[00:12:26.800 --> 00:12:30.360]   And so you'd sample 1 and 10 to the negative 1, et cetera.
[00:12:30.360 --> 00:12:32.440]   And then once you have a sense of, OK, maybe it's
[00:12:32.440 --> 00:12:34.880]   around 10 to the negative 3, then
[00:12:34.880 --> 00:12:37.440]   you do more of this random search thing
[00:12:37.440 --> 00:12:40.560]   in some area around that.
[00:12:40.560 --> 00:12:43.080]   And so usually, as I move on to different stages
[00:12:43.080 --> 00:12:46.600]   of the project, the exact value of the hyperparameters that
[00:12:46.600 --> 00:12:52.080]   works best will change, but the range hopefully shouldn't.
[00:12:52.080 --> 00:12:53.440]   Yeah?
[00:12:53.440 --> 00:13:15.320]   [INAUDIBLE]
[00:13:15.320 --> 00:13:17.240]   Yeah, I think the way I would think about this
[00:13:17.240 --> 00:13:20.120]   is, so the distribution shift thing
[00:13:20.120 --> 00:13:22.640]   applies when it's hard to collect data that
[00:13:22.640 --> 00:13:24.060]   looks like the data you ultimately
[00:13:24.060 --> 00:13:25.760]   will care about evaluating on.
[00:13:25.760 --> 00:13:28.140]   If it's easy for you to collect and label data like that,
[00:13:28.140 --> 00:13:30.440]   then you should just do as much of it as you can.
[00:13:30.440 --> 00:13:32.800]   But if it's hard, then I would say
[00:13:32.800 --> 00:13:34.100]   you should prioritize making sure
[00:13:34.100 --> 00:13:38.000]   that you have some data that, as close as possible to the data
[00:13:38.000 --> 00:13:40.280]   you ultimately care about, held out
[00:13:40.280 --> 00:13:42.520]   so that you can use it as a final arbiter of your model's
[00:13:42.520 --> 00:13:43.760]   performance.
[00:13:43.760 --> 00:13:45.400]   And then only once you have that should
[00:13:45.400 --> 00:13:47.960]   you start adding data from that distribution
[00:13:47.960 --> 00:13:50.960]   to your training set.
[00:13:50.960 --> 00:13:51.960]   You had a question?
[00:13:51.960 --> 00:13:52.460]   Yeah?
[00:13:52.460 --> 00:13:54.920]   So about the types of errors you get
[00:13:54.920 --> 00:13:58.320]   and the variance errors, how much of that
[00:13:58.320 --> 00:14:01.240]   is just empirical, like, hey, how this model turns out?
[00:14:01.240 --> 00:14:03.760]   Or do you have some intuition for mapping
[00:14:03.760 --> 00:14:05.520]   certain hyperparameters to, like,
[00:14:05.520 --> 00:14:07.080]   will this affect bias more?
[00:14:07.080 --> 00:14:16.480]   Yeah, so there are some hyperparameters
[00:14:16.480 --> 00:14:20.760]   that are really just good for addressing overfitting.
[00:14:20.760 --> 00:14:22.560]   So these are regularizers.
[00:14:22.560 --> 00:14:24.520]   So changing the amount of dropout
[00:14:24.520 --> 00:14:26.480]   that you add and things like that,
[00:14:26.480 --> 00:14:28.760]   that's like, if you're trying to prevent your model
[00:14:28.760 --> 00:14:30.520]   from overfitting, then that's the thing
[00:14:30.520 --> 00:14:31.940]   that those are the types of things
[00:14:31.940 --> 00:14:34.000]   that you should be working on.
[00:14:34.000 --> 00:14:37.160]   For things like learning rate, it's
[00:14:37.160 --> 00:14:40.240]   a little bit more like-- it's a little bit more about affecting
[00:14:40.240 --> 00:14:42.120]   bias, but it can affect both.
[00:14:42.120 --> 00:14:48.240]   And for things like batch norm, it's very much about both.
[00:14:48.240 --> 00:14:51.520]   So it depends on the hyperparameter a little bit.
[00:14:51.520 --> 00:14:52.320]   Yeah?
[00:14:52.320 --> 00:14:55.480]   Should you always train your model on the entire data set,
[00:14:55.480 --> 00:14:57.640]   or should you just have, like, say,
[00:14:57.640 --> 00:15:00.440]   you want to use maybe just 30% of the entire training data
[00:15:00.440 --> 00:15:02.640]   to start with a single model?
[00:15:02.640 --> 00:15:07.040]   And then if we do that, say that the error rate is not ideal,
[00:15:07.040 --> 00:15:09.040]   but then that's the first thing we can try,
[00:15:09.040 --> 00:15:11.040]   is to add in more training data.
[00:15:11.040 --> 00:15:12.840]   Right, maybe then--
[00:15:12.840 --> 00:15:14.000]   I'm not sure which--
[00:15:14.000 --> 00:15:17.040]   should we try to refine the parameter,
[00:15:17.040 --> 00:15:19.440]   or should we try to add more training data?
[00:15:19.440 --> 00:15:19.940]   Yeah.
[00:15:19.940 --> 00:15:22.840]   The parameter might be better if we have more training data,
[00:15:22.840 --> 00:15:24.280]   right?
[00:15:24.280 --> 00:15:27.280]   Yeah, so I think the way I usually think about it is,
[00:15:27.280 --> 00:15:30.520]   your first goal is always to get your training error as low
[00:15:30.520 --> 00:15:31.920]   as you want it to go.
[00:15:31.920 --> 00:15:33.960]   And in order to do that, you're allowed
[00:15:33.960 --> 00:15:37.920]   to make crazy simplifications to your problem if you want to,
[00:15:37.920 --> 00:15:39.000]   just as a starting point.
[00:15:39.000 --> 00:15:41.200]   Because really, the goal here is,
[00:15:41.200 --> 00:15:44.040]   can we make anything work on this problem at all?
[00:15:44.040 --> 00:15:45.800]   Is our pipeline set up in such a way?
[00:15:45.800 --> 00:15:48.240]   And is this data labeled correctly
[00:15:48.240 --> 00:15:50.880]   that we can train any model, even
[00:15:50.880 --> 00:15:52.840]   if it's on a simplified version of our data set,
[00:15:52.840 --> 00:15:54.600]   to work well enough?
[00:15:54.600 --> 00:15:56.880]   So that's in this addressing underfitting stage.
[00:15:56.880 --> 00:15:58.300]   And then usually what will happen
[00:15:58.300 --> 00:16:01.560]   is, if you've made a lot of simplifications to your model
[00:16:01.560 --> 00:16:04.640]   and your data in this stage in order
[00:16:04.640 --> 00:16:08.200]   to overfit your training set, then what will happen
[00:16:08.200 --> 00:16:09.720]   is, the next thing you'll need to do
[00:16:09.720 --> 00:16:12.400]   is, your validation error will be much higher
[00:16:12.400 --> 00:16:13.480]   than your training error.
[00:16:13.480 --> 00:16:15.640]   And then typically, the first thing that you should do there
[00:16:15.640 --> 00:16:18.800]   is think about adding more data, if you can.
[00:16:18.800 --> 00:16:20.600]   And so I guess the short answer is yes.
[00:16:20.600 --> 00:16:23.800]   I usually would start with a smaller version of the data set
[00:16:23.800 --> 00:16:25.120]   that you ultimately care about.
[00:16:25.120 --> 00:16:27.640]   And then once you've seen that you can perform well on that,
[00:16:27.640 --> 00:16:31.400]   then I would start adding back more data.
[00:16:31.400 --> 00:16:31.900]   Yeah?
[00:16:31.900 --> 00:16:32.400]   [INAUDIBLE]
[00:16:44.720 --> 00:16:45.800]   You can do it both ways.
[00:16:45.800 --> 00:16:47.160]   I've seen people do it both ways.
[00:16:47.160 --> 00:16:49.880]   I think one thing, if you really, really want to simplify,
[00:16:49.880 --> 00:16:52.000]   you can try to simplify the problem itself.
[00:16:52.000 --> 00:16:53.680]   And so maybe if you have 1,000 classes,
[00:16:53.680 --> 00:16:57.680]   maybe you can just focus on 10 of them at first.
[00:16:57.680 --> 00:17:00.800]   If it's less clear how to do that for your data set,
[00:17:00.800 --> 00:17:03.120]   then maybe you just sample uniformly.
[00:17:03.120 --> 00:17:06.620]   [INAUDIBLE]
[00:17:06.620 --> 00:17:07.120]   Yeah.
[00:17:07.120 --> 00:17:07.620]   [INAUDIBLE]
[00:17:07.620 --> 00:17:24.300]   Yeah, so there's a slide in the talk about this.
[00:17:24.300 --> 00:17:27.940]   And I think the main idea here is just
[00:17:27.940 --> 00:17:30.260]   to try to do some error analysis.
[00:17:30.260 --> 00:17:32.820]   And so what I would typically do is,
[00:17:32.820 --> 00:17:35.460]   I notice that I'm doing much better on my train validation
[00:17:35.460 --> 00:17:37.220]   set than I am on my test validation set.
[00:17:37.220 --> 00:17:38.940]   And so what I'll do is, I'll go look
[00:17:38.940 --> 00:17:41.980]   at some examples of where it performs badly on my train
[00:17:41.980 --> 00:17:44.100]   validation set and where it performs badly
[00:17:44.100 --> 00:17:45.660]   on my test validation set.
[00:17:45.660 --> 00:17:48.820]   And then I'll try to identify cases, like specific images,
[00:17:48.820 --> 00:17:53.540]   where it seems to be making a type of error on the test
[00:17:53.540 --> 00:17:56.080]   validation set that it doesn't make on the train validation
[00:17:56.080 --> 00:17:56.580]   set.
[00:17:56.580 --> 00:18:00.100]   And those errors are sort of your gradient
[00:18:00.100 --> 00:18:02.700]   towards what types of data you should be
[00:18:02.700 --> 00:18:04.860]   adding to your training set.
[00:18:04.860 --> 00:18:06.180]   And so oftentimes, what you'll see
[00:18:06.180 --> 00:18:09.460]   is there's just not enough dark images in the training set.
[00:18:09.460 --> 00:18:14.180]   Or the distribution of labels in the training set
[00:18:14.180 --> 00:18:16.460]   is totally different than it is in the test set.
[00:18:16.460 --> 00:18:18.460]   And then you can go do targeted data collection
[00:18:18.460 --> 00:18:19.380]   based on that insight.
[00:18:19.380 --> 00:18:29.340]   Great.
[00:18:29.340 --> 00:18:30.460]   Any other questions on this?
[00:18:30.460 --> 00:18:30.960]   Yeah.
[00:18:30.960 --> 00:18:40.340]   [INAUDIBLE]
[00:18:40.340 --> 00:18:42.220]   You can't figure out the irreducible error
[00:18:42.220 --> 00:18:43.420]   in your problem.
[00:18:43.420 --> 00:18:45.540]   And this is why baselines are so important.
[00:18:45.540 --> 00:18:48.380]   So if you have-- the better your baseline--
[00:18:48.380 --> 00:18:50.700]   the baseline is like a lower bound
[00:18:50.700 --> 00:18:53.460]   on how good you can expect your performance to get.
[00:18:53.460 --> 00:18:56.420]   And so the better baseline you get,
[00:18:56.420 --> 00:18:59.220]   the better that lower bound is.
[00:18:59.220 --> 00:19:00.860]   The lower that lower bound is.
[00:19:00.860 --> 00:19:06.500]   And so if you have some way of just kind of judging
[00:19:06.500 --> 00:19:09.500]   what ground truth should be, like if you can have really
[00:19:09.500 --> 00:19:12.220]   competent human labelers label a small data set
[00:19:12.220 --> 00:19:13.780]   and see what error rate they get,
[00:19:13.780 --> 00:19:15.360]   then that's the kind of thing that you
[00:19:15.360 --> 00:19:17.660]   can use to get as close as possible to it.
[00:19:17.660 --> 00:19:19.300]   But you can never actually measure it.
[00:19:19.300 --> 00:19:23.620]   Yeah.
[00:19:23.620 --> 00:19:25.220]   Just as a side question, is there
[00:19:25.220 --> 00:19:28.860]   any way to address the distribution shift
[00:19:28.860 --> 00:19:31.540]   if the data is not image-displaced?
[00:19:31.540 --> 00:19:33.300]   Let's say I can look at the data,
[00:19:33.300 --> 00:19:34.700]   but it's just a bunch of numbers.
[00:19:34.700 --> 00:19:36.820]   And I have no idea what's different
[00:19:36.820 --> 00:19:41.980]   about this particular entry that throws this whole thing.
[00:19:41.980 --> 00:19:43.300]   There are things you can do.
[00:19:43.300 --> 00:19:44.860]   There's sort of a class of techniques
[00:19:44.860 --> 00:19:47.980]   called domain adaptation, where you're actually
[00:19:47.980 --> 00:19:51.620]   sort of explicitly using the unlabeled data
[00:19:51.620 --> 00:19:58.300]   from the test distribution in a smart way that
[00:19:58.300 --> 00:20:00.260]   lets you do better on that distribution,
[00:20:00.260 --> 00:20:03.220]   even though you don't have labels there.
[00:20:03.220 --> 00:20:05.700]   You can also kind of-- another thing you can do
[00:20:05.700 --> 00:20:07.820]   is just sort of measure statistics of that data.
[00:20:07.820 --> 00:20:09.280]   So you can look at the mean and see
[00:20:09.280 --> 00:20:12.780]   if it's more or less the same, and the variance, et cetera.
[00:20:12.780 --> 00:20:15.100]   But in general, it's challenging.
[00:20:15.100 --> 00:20:15.820]   You can plot it.
[00:20:15.820 --> 00:20:19.340]   You can do t-SNE plots, project it down
[00:20:19.340 --> 00:20:21.800]   into a lower dimensional space and see what it looks like.
[00:20:21.800 --> 00:20:23.140]   And you can get some sort of-- you
[00:20:23.140 --> 00:20:25.660]   can reason somehow about whether these distributions really
[00:20:25.660 --> 00:20:26.260]   look the same.
[00:20:26.260 --> 00:20:29.020]   But I don't think there's as good a way
[00:20:29.020 --> 00:20:31.860]   as there is for images or audio or text or anything
[00:20:31.860 --> 00:20:33.140]   that we can interpret.
[00:20:33.140 --> 00:20:39.940]   Cool.
[00:20:39.940 --> 00:20:40.420]   Great.
[00:20:40.420 --> 00:20:42.420]   So I want to move on and talk about homework
[00:20:42.420 --> 00:20:45.220]   before we dive into the debugging challenge.
[00:20:49.100 --> 00:20:53.300]   So quickly, just to review last week's homework,
[00:20:53.300 --> 00:20:56.660]   you all watched the troubleshooting lecture,
[00:20:56.660 --> 00:20:58.740]   collecting an initial data set for your project.
[00:20:58.740 --> 00:21:01.180]   How many feel like you have a good data
[00:21:01.180 --> 00:21:04.780]   set for your project to work with at this point?
[00:21:04.780 --> 00:21:07.500]   OK, relatively small number.
[00:21:07.500 --> 00:21:09.220]   Post on Slack if you need help with this.
[00:21:09.220 --> 00:21:10.620]   I'm happy to sort of help brainstorm
[00:21:10.620 --> 00:21:12.080]   where you might look for a data set
[00:21:12.080 --> 00:21:14.580]   or how to go about collecting it.
[00:21:14.580 --> 00:21:16.660]   Is it-- for those of you that don't have one,
[00:21:16.660 --> 00:21:18.420]   is it-- how many is it because you actually
[00:21:18.420 --> 00:21:19.880]   need to go and collect that data set
[00:21:19.880 --> 00:21:23.060]   and you just haven't gotten around to that yet?
[00:21:23.060 --> 00:21:24.220]   All right, so just a couple.
[00:21:24.220 --> 00:21:25.780]   So for the rest of you, it's just
[00:21:25.780 --> 00:21:26.820]   about finding the right one online
[00:21:26.820 --> 00:21:27.940]   and you should get on that.
[00:21:27.940 --> 00:21:32.740]   Completing project proposal.
[00:21:32.740 --> 00:21:34.660]   I think we're still missing a few of these.
[00:21:34.660 --> 00:21:37.740]   For those that submitted them today, at some point,
[00:21:37.740 --> 00:21:39.580]   I will try to comment on them tomorrow.
[00:21:39.580 --> 00:21:41.220]   If you submitted them yesterday, then I
[00:21:41.220 --> 00:21:42.460]   have already commented on them.
[00:21:45.260 --> 00:21:49.700]   And so for this week, next week, the lecture
[00:21:49.700 --> 00:21:52.380]   is going to be on infrastructure and tooling.
[00:21:52.380 --> 00:21:54.820]   And so there's a infrastructure lecture
[00:21:54.820 --> 00:21:58.420]   to watch as a starting point.
[00:21:58.420 --> 00:22:00.340]   And then for your project, the thing
[00:22:00.340 --> 00:22:01.980]   that I'm hoping that people can do
[00:22:01.980 --> 00:22:06.220]   is to get some sort of baseline working end to end.
[00:22:06.220 --> 00:22:12.020]   So you have your data set or you should have had your data set.
[00:22:12.020 --> 00:22:16.140]   And so the next step is to get some sort of evaluation
[00:22:16.140 --> 00:22:18.140]   of how well you can do on that data set
[00:22:18.140 --> 00:22:20.180]   without using anything crazy.
[00:22:20.180 --> 00:22:22.820]   And so for some of you, this will be really simple neural
[00:22:22.820 --> 00:22:23.340]   network.
[00:22:23.340 --> 00:22:25.240]   For some of you, it'll be linear regression.
[00:22:25.240 --> 00:22:27.620]   For some of you, it'll be some sort of heuristic
[00:22:27.620 --> 00:22:30.580]   or some result from some paper.
[00:22:30.580 --> 00:22:32.300]   But the important thing is just to have
[00:22:32.300 --> 00:22:35.620]   some indication of what a baseline performance looks like.
[00:22:35.620 --> 00:22:38.980]   And you've all put suggestions there in the project proposal,
[00:22:38.980 --> 00:22:40.820]   and so I'll go through and comment on those.
[00:22:40.820 --> 00:22:45.060]   And that's what you should try and execute on this week.
[00:22:45.060 --> 00:22:47.300]   And then I'll give feedback on the outstanding project
[00:22:47.300 --> 00:22:48.420]   proposals.
[00:22:48.420 --> 00:22:50.540]   Any questions about the project or about the homework
[00:22:50.540 --> 00:22:51.100]   for next week?
[00:22:51.100 --> 00:22:55.060]   [INAUDIBLE]
[00:22:55.060 --> 00:23:04.660]   Yeah.
[00:23:04.660 --> 00:23:07.660]   So I think ideally what you would do here, if you can,
[00:23:07.660 --> 00:23:11.260]   is if you're-- in that question that said,
[00:23:11.260 --> 00:23:14.900]   what is the end result of the project going to look like?
[00:23:14.900 --> 00:23:16.500]   Some of you said, well, it's a web app.
[00:23:16.500 --> 00:23:20.660]   Some of you said, well, it's like an iPhone app
[00:23:20.660 --> 00:23:21.700]   or something like that.
[00:23:21.700 --> 00:23:24.860]   So ideally what you'd do is you'd set up
[00:23:24.860 --> 00:23:26.540]   that whole pipeline, and then you'd
[00:23:26.540 --> 00:23:29.740]   just have some dummy model that can stand in
[00:23:29.740 --> 00:23:32.620]   for where your super cool deep learning model is eventually
[00:23:32.620 --> 00:23:33.460]   going to go.
[00:23:33.460 --> 00:23:36.100]   And so then you can actually take the picture on your phone
[00:23:36.100 --> 00:23:41.420]   and see how well the results look with the baseline model.
[00:23:41.420 --> 00:23:42.980]   Understanding that the timeline might
[00:23:42.980 --> 00:23:44.380]   be a little bit tight for that, and if you
[00:23:44.380 --> 00:23:46.220]   have some sort of more advanced deployment,
[00:23:46.220 --> 00:23:48.620]   then it's going to take a little bit longer to set that up.
[00:23:48.620 --> 00:23:50.000]   The next best thing would just be
[00:23:50.000 --> 00:23:52.140]   to make sure that you can evaluate
[00:23:52.140 --> 00:23:55.780]   that, kind of your entire pipeline,
[00:23:55.780 --> 00:23:59.740]   with a stand-in model on your validation set or your test
[00:23:59.740 --> 00:24:00.240]   set.
[00:24:00.240 --> 00:24:04.020]   Yeah.
[00:24:04.020 --> 00:24:05.020]   [INAUDIBLE]
[00:24:05.020 --> 00:24:31.900]   Yeah.
[00:24:31.900 --> 00:24:33.940]   So we looked into whether we could get resources
[00:24:33.940 --> 00:24:35.700]   for the whole class, and unfortunately, it
[00:24:35.700 --> 00:24:38.140]   doesn't seem like we're going to be able to.
[00:24:38.140 --> 00:24:42.540]   One thing that you should do is for GCE and AWS,
[00:24:42.540 --> 00:24:47.020]   they both have sort of credits available for if you're
[00:24:47.020 --> 00:24:48.180]   new to those platforms.
[00:24:48.180 --> 00:24:50.380]   And so if you haven't used that yet,
[00:24:50.380 --> 00:24:53.060]   then everyone on your team should sign up for an account
[00:24:53.060 --> 00:24:55.820]   and make sure that you're taking full advantage of those credits.
[00:24:55.820 --> 00:24:58.300]   And if you're running into issues with it,
[00:24:58.300 --> 00:25:01.220]   then please just ping Stephanie and I,
[00:25:01.220 --> 00:25:03.060]   and we'll work with you to figure out
[00:25:03.060 --> 00:25:05.660]   what the best way is to get some sort of resources.
[00:25:05.660 --> 00:25:15.500]   OK.
[00:25:15.500 --> 00:25:17.940]   Up next is the debugging challenge.
[00:25:17.940 --> 00:25:20.820]   We're going to have about-- let's see.
[00:25:20.820 --> 00:25:23.900]   So it's 2 before the hour, and I think
[00:25:23.900 --> 00:25:27.060]   we're supposed to start Lee at 745.
[00:25:27.060 --> 00:25:27.700]   Is that right?
[00:25:30.580 --> 00:25:31.460]   Yep.
[00:25:31.460 --> 00:25:31.960]   Cool.
[00:25:31.960 --> 00:25:34.900]   So let's spend about 37 minutes on this.
[00:25:34.900 --> 00:25:37.320]   And again, we're going to do this kind of similarly to how
[00:25:37.320 --> 00:25:38.740]   we did last week.
[00:25:38.740 --> 00:25:40.900]   Just work together with people next to you.
[00:25:40.900 --> 00:25:43.420]   Hopefully, you've all set up PyEnv this time,
[00:25:43.420 --> 00:25:45.940]   so we can get right through that part.
[00:25:45.940 --> 00:25:50.580]   And then we'll kind of wrap up and meet back up and discuss
[00:25:50.580 --> 00:25:53.060]   what your approaches were and what issues
[00:25:53.060 --> 00:25:56.380]   you ran into at the end.
[00:25:56.380 --> 00:26:00.540]   It is time to wrap up working on this project for now.
[00:26:00.540 --> 00:26:03.780]   [LAUGHTER]
[00:26:03.780 --> 00:26:04.860]   But I encourage you--
[00:26:04.860 --> 00:26:06.420]   I intentionally put in more bugs
[00:26:06.420 --> 00:26:09.780]   than I thought anyone could find,
[00:26:09.780 --> 00:26:12.360]   beyond hope to be pleasantly surprised that one person found
[00:26:12.360 --> 00:26:13.220]   all the bugs.
[00:26:13.220 --> 00:26:15.300]   I'm guessing no one did.
[00:26:15.300 --> 00:26:16.780]   But I would encourage you just to go through this
[00:26:16.780 --> 00:26:17.660]   on your own time.
[00:26:17.660 --> 00:26:20.660]   I mean, I think the takeaway from this for me
[00:26:20.660 --> 00:26:23.780]   is that debugging in TensorFlow especially,
[00:26:23.780 --> 00:26:26.140]   and in machine learning in general, is something
[00:26:26.140 --> 00:26:28.420]   that you just need to get experience with.
[00:26:28.420 --> 00:26:31.500]   And there's tools and tricks and sort of tactics
[00:26:31.500 --> 00:26:33.740]   that you can use that I tried to cover in the lecture
[00:26:33.740 --> 00:26:35.180]   that you all watched.
[00:26:35.180 --> 00:26:42.620]   But in practice, you need to try these things out yourself.
[00:26:42.620 --> 00:26:44.660]   And so I'd encourage you to keep going through it
[00:26:44.660 --> 00:26:45.620]   on your own time.
[00:26:45.620 --> 00:26:49.420]   But that said, how many were able to find
[00:26:49.420 --> 00:26:52.740]   at least the first bug?
[00:26:52.740 --> 00:26:55.460]   OK, almost everyone.
[00:26:55.460 --> 00:26:58.100]   Yeah, we'll talk about that one in a second.
[00:26:58.100 --> 00:27:00.020]   And how many were able to get the code running,
[00:27:00.020 --> 00:27:02.180]   so you're able to find the first two bugs?
[00:27:02.180 --> 00:27:03.020]   OK, still most.
[00:27:03.020 --> 00:27:03.520]   That's really good.
[00:27:03.520 --> 00:27:05.220]   And then how many were able to find at least one more
[00:27:05.220 --> 00:27:05.860]   bug after that?
[00:27:05.860 --> 00:27:11.860]   Even find is OK, yeah.
[00:27:11.860 --> 00:27:16.940]   OK, so did anyone find, at least identify four bugs?
[00:27:16.940 --> 00:27:21.500]   Yeah, OK.
[00:27:21.500 --> 00:27:22.000]   Five?
[00:27:22.000 --> 00:27:25.660]   Got it.
[00:27:25.660 --> 00:27:27.260]   All right, so what was the first bug?
[00:27:27.260 --> 00:27:34.020]   Yeah.
[00:27:34.020 --> 00:27:36.180]   So pretty much everyone found this.
[00:27:36.180 --> 00:27:37.060]   How did you find it?
[00:27:37.060 --> 00:27:39.380]   Anyone want to kind of suggest what your strategy was
[00:27:39.380 --> 00:27:40.660]   for fixing this bug?
[00:27:40.660 --> 00:27:43.020]   I guess it was obvious because it popped up
[00:27:43.020 --> 00:27:45.380]   when you first ran it.
[00:27:45.380 --> 00:27:47.100]   [INAUDIBLE]
[00:27:47.100 --> 00:27:48.580]   Read the error message, yeah.
[00:27:48.580 --> 00:27:50.580]   And then this is kind of like--
[00:27:50.580 --> 00:27:54.340]   the strategy here, like when I run into variable reuse issues
[00:27:54.340 --> 00:27:56.660]   in TensorFlow is just kind of look at what TensorFlow
[00:27:56.660 --> 00:27:58.580]   is telling you is that you're creating
[00:27:58.580 --> 00:28:02.300]   the same variable twice in your graph with the same name.
[00:28:02.300 --> 00:28:04.420]   And so by default, TensorFlow complains about that
[00:28:04.420 --> 00:28:06.800]   because it's like you can't create the same variable twice
[00:28:06.800 --> 00:28:07.900]   with the same name.
[00:28:07.900 --> 00:28:09.820]   And that's actually kind of a helpful feature
[00:28:09.820 --> 00:28:11.780]   because you don't want to do that by accident.
[00:28:11.780 --> 00:28:13.100]   But what it means is that you need
[00:28:13.100 --> 00:28:14.640]   to go back and look through your code
[00:28:14.640 --> 00:28:16.620]   and look at where you're creating
[00:28:16.620 --> 00:28:17.900]   the same variable twice.
[00:28:17.900 --> 00:28:19.940]   And then make sure if you're doing it intentionally
[00:28:19.940 --> 00:28:23.020]   that you're reusing that variable the second time.
[00:28:23.020 --> 00:28:25.220]   What was the second bug?
[00:28:25.220 --> 00:28:27.420]   So I guess one of the things you were saying
[00:28:27.420 --> 00:28:31.220]   was set reuse equal to true.
[00:28:31.220 --> 00:28:32.180]   That didn't work.
[00:28:32.180 --> 00:28:36.260]   I had to go to reuse equal tf.autoReuse.
[00:28:36.260 --> 00:28:40.140]   Is there a difference why one works for some people and not?
[00:28:40.140 --> 00:28:43.500]   Yeah, so tf.autoReuse is just saying--
[00:28:43.500 --> 00:28:45.300]   it's basically saying ignore that error.
[00:28:45.300 --> 00:28:48.780]   So it's saying every time I create the same variable twice,
[00:28:48.780 --> 00:28:51.780]   just assume that I meant to do it.
[00:28:51.780 --> 00:28:54.060]   So that's useful if you want to get things running.
[00:28:54.060 --> 00:28:56.300]   But it can be a little bit dangerous
[00:28:56.300 --> 00:29:00.620]   because you can create the same variable twice by accident.
[00:29:00.620 --> 00:29:01.780]   Reuse equals true.
[00:29:01.780 --> 00:29:04.140]   The trick there is you need to pass it reuse equals true
[00:29:04.140 --> 00:29:05.800]   the second time only.
[00:29:05.800 --> 00:29:07.700]   Because the first time, you're not reusing it.
[00:29:07.700 --> 00:29:12.260]   The first time, you're actually creating it from scratch.
[00:29:12.260 --> 00:29:15.520]   And then once you did that, you can get the code working.
[00:29:15.520 --> 00:29:17.520]   And then hopefully, the next thing that you did
[00:29:17.520 --> 00:29:21.700]   was run it with the flag overfit batch.
[00:29:21.700 --> 00:29:23.620]   So a couple people found more bugs after that.
[00:29:23.620 --> 00:29:24.580]   What bugs did you find?
[00:29:24.580 --> 00:29:30.380]   Reshaping.
[00:29:30.380 --> 00:29:31.140]   Oh, yes.
[00:29:31.140 --> 00:29:34.020]   We didn't talk about reshaping.
[00:29:34.020 --> 00:29:35.580]   So how did you fix the reshaping bug?
[00:29:35.580 --> 00:29:40.900]   Well, so you explained it to me.
[00:29:40.900 --> 00:29:46.380]   The convolutional layers take four dimension shapes.
[00:29:46.380 --> 00:29:49.260]   The next, that's their expected two dimensional.
[00:29:49.260 --> 00:29:51.740]   So we want to keep the first dimension the same.
[00:29:51.740 --> 00:29:56.820]   And then if the second dimension is spread out like a cloud.
[00:29:56.820 --> 00:29:57.320]   Yeah.
[00:29:57.320 --> 00:29:57.820]   Yeah.
[00:29:57.820 --> 00:29:59.540]   So in ConvNets, at some point, if you
[00:29:59.540 --> 00:30:00.500]   want to use dense layers, you need
[00:30:00.500 --> 00:30:03.040]   to turn a four dimensional thing into a two dimensional thing.
[00:30:03.040 --> 00:30:04.740]   So that's why we're reshaping.
[00:30:04.740 --> 00:30:07.420]   And we had the dimensions of the reshaping wrong.
[00:30:07.420 --> 00:30:09.020]   Strategy for how to fix this.
[00:30:09.020 --> 00:30:10.900]   I think the best way to fix bugs like this
[00:30:10.900 --> 00:30:14.420]   is to just step through your model creation in a debugger.
[00:30:14.420 --> 00:30:19.180]   So you can throw in some IPDB statement.
[00:30:19.180 --> 00:30:21.020]   And sort of catch the execution of your code
[00:30:21.020 --> 00:30:22.220]   as you're creating the model.
[00:30:22.220 --> 00:30:24.420]   Step through it line by line and just print things out
[00:30:24.420 --> 00:30:26.340]   and see if the shapes make sense.
[00:30:26.340 --> 00:30:27.100]   And so what you could have done is
[00:30:27.100 --> 00:30:29.480]   you could have stepped through the creation of that model
[00:30:29.480 --> 00:30:31.620]   and then printed out the shape of the tensor
[00:30:31.620 --> 00:30:33.180]   right before you reshaped.
[00:30:33.180 --> 00:30:37.180]   And you would have noticed it's batch size times 7 times 7
[00:30:37.180 --> 00:30:39.180]   times 64.
[00:30:39.180 --> 00:30:40.940]   And then when you reshaped it, you
[00:30:40.940 --> 00:30:43.980]   were getting something that was not the same as that.
[00:30:43.980 --> 00:30:48.020]   And so that's where you go in and fix the dimensionality
[00:30:48.020 --> 00:30:49.140]   there.
[00:30:49.140 --> 00:30:51.860]   [INAUDIBLE]
[00:30:51.860 --> 00:30:53.660]   I just use IPDB.
[00:30:53.660 --> 00:30:57.300]   There is TFDB, which I've played around with a little bit
[00:30:57.300 --> 00:30:58.940]   and should work better for some things.
[00:30:58.940 --> 00:31:00.580]   But I don't really use in practice.
[00:31:00.580 --> 00:31:04.980]   Any other bugs that people found?
[00:31:04.980 --> 00:31:13.060]   [INAUDIBLE]
[00:31:13.060 --> 00:31:15.260]   Yeah, so it wasn't using the dropout layer.
[00:31:15.260 --> 00:31:17.900]   That's true.
[00:31:17.900 --> 00:31:19.060]   Anything else?
[00:31:19.060 --> 00:31:23.660]   OK, good.
[00:31:23.660 --> 00:31:26.580]   Yeah, so I encourage you, again, all the bugs
[00:31:26.580 --> 00:31:28.500]   are listed in the readme.
[00:31:28.500 --> 00:31:30.920]   And so I encourage you to spend some time going through it
[00:31:30.920 --> 00:31:33.340]   on your own and look at those when you get stuck
[00:31:33.340 --> 00:31:36.220]   and see if you can fix them.
[00:31:36.220 --> 00:31:37.660]   But there's a lot of bugs there.
[00:31:37.660 --> 00:31:40.060]   I think the strategy that I would
[00:31:40.060 --> 00:31:41.780]   use if I was trying to debug this
[00:31:41.780 --> 00:31:44.220]   is once you have the model running,
[00:31:44.220 --> 00:31:47.740]   then I would try to overfit a single batch.
[00:31:47.740 --> 00:31:49.860]   If you're not able to overfit a single batch, which
[00:31:49.860 --> 00:31:51.740]   in this case you won't be able to,
[00:31:51.740 --> 00:31:54.780]   then I would start inspecting the code
[00:31:54.780 --> 00:31:56.760]   and figuring out places where there
[00:31:56.760 --> 00:31:58.460]   could be something that is causing
[00:31:58.460 --> 00:31:59.940]   the model to not overfit.
[00:31:59.940 --> 00:32:01.900]   And in practice, what you'll see here
[00:32:01.900 --> 00:32:04.300]   is that the loss actually doesn't go down at all.
[00:32:04.300 --> 00:32:07.340]   And so that would cause me to instantly think,
[00:32:07.340 --> 00:32:09.440]   well, if the loss isn't going down at all,
[00:32:09.440 --> 00:32:12.820]   then either I'm computing the loss function wrong
[00:32:12.820 --> 00:32:14.660]   or I'm taking gradients wrong.
[00:32:14.660 --> 00:32:17.180]   Because ultimately, even if you're
[00:32:17.180 --> 00:32:18.780]   doing the wrong thing with your model,
[00:32:18.780 --> 00:32:21.420]   your loss should at least go in the right direction.
[00:32:21.420 --> 00:32:23.140]   So that's my hint for the next part.
[00:32:23.140 --> 00:32:36.340]   Any other comments or questions about this or thoughts
[00:32:36.340 --> 00:32:40.020]   about why is this so painful?
[00:32:40.020 --> 00:32:42.420]   Yeah.
[00:32:42.420 --> 00:32:45.060]   When would you expect the overfitting
[00:32:45.060 --> 00:32:49.380]   to get you zero error or would you not?
[00:32:49.380 --> 00:32:54.020]   It's not clear to me that you should expect that.
[00:32:54.020 --> 00:32:56.940]   Yeah, so the place where overfitting will get you
[00:32:56.940 --> 00:33:02.540]   is when you're actually evaluating
[00:33:02.540 --> 00:33:03.980]   the performance of your model.
[00:33:03.980 --> 00:33:06.520]   So overfitting doesn't-- one of the things that's challenging
[00:33:06.520 --> 00:33:08.580]   is that it doesn't manifest as a bug.
[00:33:08.580 --> 00:33:11.740]   It manifests as a gap between your training
[00:33:11.740 --> 00:33:13.500]   error and your validation error.
[00:33:13.500 --> 00:33:14.920]   And so if you have a small data set
[00:33:14.920 --> 00:33:19.600]   or your model is really big or possibly if you have a bug,
[00:33:19.600 --> 00:33:21.640]   then you'll just see your training error is way
[00:33:21.640 --> 00:33:22.700]   lower than your validation error.
[00:33:22.700 --> 00:33:24.940]   And then that's when you know that you're overfitting.
[00:33:24.940 --> 00:33:26.440]   Yeah, and I understand that you said
[00:33:26.440 --> 00:33:30.540]   that you should be able to drive the overfitting error to zero
[00:33:30.540 --> 00:33:32.020]   on it.
[00:33:32.020 --> 00:33:34.220]   It seems like in general, you're not
[00:33:34.220 --> 00:33:37.540]   going to be able to drive even your training error to zero.
[00:33:37.540 --> 00:33:39.780]   So the thing that you should be able to drive to zero
[00:33:39.780 --> 00:33:44.540]   is your error on a single batch.
[00:33:44.540 --> 00:33:46.180]   So for any reasonably sized model,
[00:33:46.180 --> 00:33:48.340]   you should be able to memorize the labels for one
[00:33:48.340 --> 00:33:50.220]   batch of data.
[00:33:50.220 --> 00:33:51.620]   Yeah, and so if you can't do that,
[00:33:51.620 --> 00:33:52.860]   then that means that you have a bug.
[00:33:52.860 --> 00:33:54.060]   But in general, you're right.
[00:33:54.060 --> 00:33:57.300]   Your training error shouldn't always go to zero.
[00:33:57.300 --> 00:33:59.900]   You should just try to get it below whatever
[00:33:59.900 --> 00:34:02.300]   your target performance is, even if you're overfitting,
[00:34:02.300 --> 00:34:06.940]   and then move on to addressing overfitting.
[00:34:06.940 --> 00:34:07.440]   Yeah?
[00:34:07.440 --> 00:34:12.400]   But input scaling takes the overfitting,
[00:34:12.400 --> 00:34:15.360]   takes the training error last 100%.
[00:34:15.360 --> 00:34:17.760]   I think that's a huge--
[00:34:17.760 --> 00:34:19.760]   I think even too small, I suppose,
[00:34:19.760 --> 00:34:23.040]   to be defined by 25 and then 25.
[00:34:23.040 --> 00:34:24.520]   Yeah, totally.
[00:34:24.520 --> 00:34:26.640]   Yeah, scaling inputs the wrong way,
[00:34:26.640 --> 00:34:28.760]   or scaling inputs to loss functions the wrong way,
[00:34:28.760 --> 00:34:30.160]   is a really common source of bugs.
[00:34:30.160 --> 00:34:33.340]   Because it's hard to--
[00:34:33.340 --> 00:34:35.760]   a lot of times, it feels like you're doing the right thing,
[00:34:35.760 --> 00:34:36.960]   and you've just done it twice.
[00:34:36.960 --> 00:34:39.960]   Or you're passing something in that's mathematically correct,
[00:34:39.960 --> 00:34:42.640]   but it's the wrong function signature for TensorFlow.
[00:34:42.640 --> 00:34:45.020]   So those are the types of things to be very careful about.
[00:34:45.020 --> 00:34:51.760]   All right, there's one last announcement
[00:34:51.760 --> 00:34:53.200]   that I want to make about homework
[00:34:53.200 --> 00:34:55.600]   that I forgot to say earlier.
[00:34:55.600 --> 00:35:00.000]   So one thing that we want to do with this class
[00:35:00.000 --> 00:35:03.280]   is, for those of you that want to,
[00:35:03.280 --> 00:35:05.800]   we want to feature your projects and help
[00:35:05.800 --> 00:35:08.840]   you get a job if you're interested in that,
[00:35:08.840 --> 00:35:11.840]   or just get attention to your work.
[00:35:11.840 --> 00:35:14.120]   And so what we're going to do is we're
[00:35:14.120 --> 00:35:15.880]   going to create an online gallery.
[00:35:15.880 --> 00:35:19.240]   And if you want to opt in to doing this,
[00:35:19.240 --> 00:35:23.160]   then please just send a short bio to Stephanie
[00:35:23.160 --> 00:35:27.280]   and be ready to have a headshot taken next week in class.
[00:35:27.280 --> 00:35:29.080]   So again, this is for those of you
[00:35:29.080 --> 00:35:32.880]   that want to be featured, I would encourage you to do this.
[00:35:32.880 --> 00:35:35.040]   And just message Stephanie with your bio.
[00:35:35.040 --> 00:35:36.200]   Yep.
[00:35:36.200 --> 00:35:39.800]   Just a little back.
[00:35:39.800 --> 00:35:43.520]   Once you get a code to run, when do
[00:35:43.520 --> 00:35:48.200]   you start to figure out around hyperparameters
[00:35:48.200 --> 00:35:51.080]   instead of looking for bugs in the code?
[00:35:51.080 --> 00:35:53.240]   Like you said, you mentioned that there
[00:35:53.240 --> 00:35:57.000]   might be optimizers in the code.
[00:35:57.000 --> 00:35:59.400]   Let's take this one offline, because we're just
[00:35:59.400 --> 00:36:00.680]   running a little bit over time.
[00:36:03.360 --> 00:36:05.040]   Cool.
[00:36:05.040 --> 00:36:07.080]   So next up, we have our guest speaker
[00:36:07.080 --> 00:36:10.800]   for the week, who's Lee Redden.
[00:36:10.800 --> 00:36:14.480]   So Lee started a company called Blue River Technology, which
[00:36:14.480 --> 00:36:18.000]   is a company that builds robots that use AI and computer
[00:36:18.000 --> 00:36:21.280]   vision to solve tasks in agriculture.
[00:36:21.280 --> 00:36:25.560]   So really cool, actually putting robots in the world.
[00:36:25.560 --> 00:36:29.320]   And they got acquired a couple of years ago by John Deere.
[00:36:29.320 --> 00:36:32.160]   And before that, Lee has been deeply involved
[00:36:32.160 --> 00:36:35.640]   in robotics and AI research at Johns Hopkins, and NASA,
[00:36:35.640 --> 00:36:36.920]   Stanford, and other places.
[00:36:36.920 --> 00:36:40.040]   So we're really excited to have you here, Lee.

