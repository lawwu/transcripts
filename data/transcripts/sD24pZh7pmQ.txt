
[00:00:00.000 --> 00:00:04.760]   >>Lukas Bielemann These models have seen the internet and so
[00:00:04.760 --> 00:00:06.400]   there might be like leakage, right?
[00:00:06.400 --> 00:00:08.240]   There might be test set leakage.
[00:00:08.240 --> 00:00:12.840]   And so I think the only reliable measure is actually putting it in front of people and
[00:00:12.840 --> 00:00:15.000]   asking them, you know, which models you prefer.
[00:00:15.000 --> 00:00:19.240]   You put two models in front of them, let them interact with both, put prompts into both
[00:00:19.240 --> 00:00:21.440]   and just pick the one that they prefer.
[00:00:21.440 --> 00:00:23.360]   That's the most reliable measure.
[00:00:23.360 --> 00:00:26.360]   >>Dwight Vaught You're listening to Gradient Dissent, a show
[00:00:26.360 --> 00:00:30.240]   about machine learning in the real world, and I'm your host, Lukas Biewald.
[00:00:30.240 --> 00:00:37.560]   Aidan Gomez is the CEO and co-founder of Cohere and the author of the seminal paper, "Attention
[00:00:37.560 --> 00:00:40.680]   is All You Need," which introduced transformers.
[00:00:40.680 --> 00:00:45.440]   I've been looking forward to talk to him for a long time and he's super willing to answer
[00:00:45.440 --> 00:00:51.360]   my crazy questions like how many universes in the metaverse does transformers exist in
[00:00:51.360 --> 00:00:52.660]   the form it's taken.
[00:00:52.660 --> 00:00:55.640]   This is a really fun, thoughtful interview and I hope you enjoy it.
[00:00:55.640 --> 00:01:00.200]   I think the place that I really wanted to start was, you know, you're one of the authors
[00:01:00.200 --> 00:01:04.840]   of the "Attention is All You Need" paper, which has to be one of the most influential
[00:01:04.840 --> 00:01:08.120]   papers of all time.
[00:01:08.120 --> 00:01:13.740]   And I think most people listening to this would have read it, but maybe you could describe
[00:01:13.740 --> 00:01:17.400]   the result just to catch up the few that haven't heard of this paper.
[00:01:17.400 --> 00:01:19.200]   >>Lukas Yeah, for sure.
[00:01:19.200 --> 00:01:23.120]   I appreciate you saying it's one of the most influential papers of all time.
[00:01:23.120 --> 00:01:28.960]   I don't know if I believe that, but it's definitely influential for the current time.
[00:01:28.960 --> 00:01:30.640]   >>Corey For the current time, fair.
[00:01:30.640 --> 00:01:33.680]   >>Lukas Yeah, I was part of the paper back when I was at Google Brain.
[00:01:33.680 --> 00:01:40.800]   I was just like an intern there during my undergrad and I landed under the management
[00:01:40.800 --> 00:01:44.440]   of Lukasz Kaiser.
[00:01:44.440 --> 00:01:50.480]   And he and I built this software platform for training big neural nets called Tensor
[00:01:50.480 --> 00:01:51.480]   to Tensor.
[00:01:51.480 --> 00:02:00.560]   And then during the internship, very early on, we connected with Noam and Yakov and Shish
[00:02:00.560 --> 00:02:06.520]   and Nikki and convinced them to train the models that they were exploring on Tensor
[00:02:06.520 --> 00:02:09.440]   to Tensor.
[00:02:09.440 --> 00:02:15.080]   And that model, what they were building, ended up being the transformer.
[00:02:15.080 --> 00:02:25.960]   So it's an architecture which the purpose of was really to come up with a much simpler,
[00:02:25.960 --> 00:02:34.800]   more refined, scalable architecture than LSTMs, RNNs, which were the category of model deployed
[00:02:34.800 --> 00:02:40.080]   at sequence learning before the transformer.
[00:02:40.080 --> 00:02:45.920]   And some people view transformers as extremely complex.
[00:02:45.920 --> 00:02:53.040]   I think most of those people are putting it in comparison to LSTMs and what came before,
[00:02:53.040 --> 00:02:58.520]   because transformers are really just like an attention block stacked on top of a multi-layer
[00:02:58.520 --> 00:03:05.280]   perceptron or a feed-forward layer, and then a bunch of those stacked on top of each other.
[00:03:05.280 --> 00:03:15.840]   And so I think that's an incredibly simple and, in comparison to what came before, a
[00:03:15.840 --> 00:03:21.120]   very, very simple platform to build off of.
[00:03:21.120 --> 00:03:28.440]   Obviously, with the benefit of hindsight, the importance of scale is now obvious, but
[00:03:28.440 --> 00:03:31.240]   back then I don't think it was.
[00:03:31.240 --> 00:03:36.920]   There was the general rule of thumb, which was the bigger the neural network, the better.
[00:03:36.920 --> 00:03:45.120]   But I don't think people really grasped how far we in the field needed to push that.
[00:03:45.120 --> 00:03:49.720]   There were still fears of big models overfit.
[00:03:49.720 --> 00:03:52.400]   Overfitting was this big fear, and you don't want to get too large because you're going
[00:03:52.400 --> 00:03:58.480]   to start actually performing worse at the tasks that you care about.
[00:03:58.480 --> 00:04:05.000]   But I think we took a bet that we can take this to a more extreme level and scale this
[00:04:05.000 --> 00:04:13.520]   up to not just single-digit accelerators or tens of accelerators, but thousands of accelerators.
[00:04:13.520 --> 00:04:17.760]   And that bet paid off.
[00:04:17.760 --> 00:04:24.480]   I guess what inspired you to try the attention mechanism without the underlying LSTM?
[00:04:24.480 --> 00:04:32.400]   I remember when attention first came out on these recurrent neural networks.
[00:04:32.400 --> 00:04:39.400]   I guess in hindsight, it seems like an obvious thing to try, but I guess what inspired you
[00:04:39.400 --> 00:04:40.400]   to do that?
[00:04:40.400 --> 00:04:47.560]   So, my understanding of where the inspiration came from, attention had been already hugely
[00:04:47.560 --> 00:04:53.040]   successful actually with RNNs and with LSTMs.
[00:04:53.040 --> 00:04:59.360]   And it became this really crucial part of the stack in that old generation of sequence
[00:04:59.360 --> 00:05:03.640]   learning with RNNs.
[00:05:03.640 --> 00:05:09.640]   But I think the notion of attention from a neuroscience perspective was really, really
[00:05:09.640 --> 00:05:19.000]   attractive, and it was viewed as something that was fundamental to intelligence, to human
[00:05:19.000 --> 00:05:22.480]   intelligence.
[00:05:22.480 --> 00:05:27.800]   Folks like Geoff Hinton, I think, really appreciated that.
[00:05:27.800 --> 00:05:36.760]   And the people that I was working with, like Noam and like Jakob, they saw that same sort
[00:05:36.760 --> 00:05:42.360]   of elegance in the structure, and they saw that same fundamental importance of attention
[00:05:42.360 --> 00:05:47.920]   as a computational modeling structure or tool.
[00:05:47.920 --> 00:05:52.400]   And they wanted to put that front and center in a model and see what popped out the other
[00:05:52.400 --> 00:05:53.400]   side.
[00:05:53.400 --> 00:06:00.560]   And so taking away all the other complexity, taking away all the other little tricks and
[00:06:00.560 --> 00:06:07.840]   hacks, they wanted attention to kind of shine through as the fundamental unit of intelligence
[00:06:07.840 --> 00:06:09.400]   within this neural net architecture.
[00:06:09.400 --> 00:06:12.200]   Yeah, so that was the seed.
[00:06:12.200 --> 00:06:17.720]   Attention is such an evocative word.
[00:06:17.720 --> 00:06:24.600]   It's hard not to sort of extrapolate into our own brains and things like that when you
[00:06:24.600 --> 00:06:27.840]   use the word attention.
[00:06:27.840 --> 00:06:31.840]   But yet the math of a transformer is super simple.
[00:06:31.840 --> 00:06:39.920]   And sometimes I wonder if the math came from a different path, it might not be called attention,
[00:06:39.920 --> 00:06:40.920]   I imagine.
[00:06:40.920 --> 00:06:42.520]   Do you think there's any truth to that?
[00:06:42.520 --> 00:06:49.920]   Like how fundamental attention is this thing?
[00:06:49.920 --> 00:06:51.160]   Yeah.
[00:06:51.160 --> 00:06:57.640]   I mean, I think the other way of describing it is like a soft look up, right?
[00:06:57.640 --> 00:07:01.640]   Like you're doing some sort of soft look up in a table.
[00:07:01.640 --> 00:07:07.080]   And by soft, I mean, you know, a hard look up would be just looking at one entry in a
[00:07:07.080 --> 00:07:11.280]   table, a soft look up would be looking at a bunch of different entries with different
[00:07:11.280 --> 00:07:14.840]   weights associated.
[00:07:14.840 --> 00:07:23.440]   And as far as I'm aware, attention was branded attention by Badenow or, you know, whoever
[00:07:23.440 --> 00:07:29.720]   that was, but they were parallel efforts in integrating lookups into neural network.
[00:07:29.720 --> 00:07:40.000]   So I think you're right that that description of attention is very salient and from an intuition
[00:07:40.000 --> 00:07:46.280]   perspective, from making a computational concept intuitive, attention is just way better than
[00:07:46.280 --> 00:07:47.280]   a look up, right?
[00:07:47.280 --> 00:07:48.960]   A soft look up.
[00:07:48.960 --> 00:07:53.680]   Attention we can understand, we can kind of grok it, we get what the model is doing
[00:07:53.680 --> 00:08:01.800]   underneath much more quickly than if you use language from databasing or something like
[00:08:01.800 --> 00:08:02.800]   that.
[00:08:02.800 --> 00:08:08.160]   So from a branding perspective, I think attention just took off because it maps onto our own
[00:08:08.160 --> 00:08:10.800]   understanding of these things much more closely.
[00:08:10.800 --> 00:08:14.920]   It gives us better analogies to work with the concepts.
[00:08:14.920 --> 00:08:22.160]   Okay, so another experience that I've had with neural networks is that it seems like
[00:08:22.160 --> 00:08:28.160]   the exact details of the architecture often doesn't matter and we end up kind of picking
[00:08:28.160 --> 00:08:33.160]   these architectures based on, you know, trying to replicate previous papers and not wanting
[00:08:33.160 --> 00:08:37.000]   to, you know, mess something up.
[00:08:37.000 --> 00:08:40.300]   How fundamental do you think transformers are?
[00:08:40.300 --> 00:08:47.560]   And maybe to make it a more well-formed question, if you ran back history a thousand times,
[00:08:47.560 --> 00:08:53.000]   how many of those times do you think you would get the transformer architecture specifically?
[00:08:53.000 --> 00:08:57.600]   Yeah, that's a really interesting thought experiment.
[00:08:57.600 --> 00:09:00.520]   I don't think it's that fundamental.
[00:09:00.520 --> 00:09:09.320]   I think what you need, I think this is less and less of a hot take, but I think all you
[00:09:09.320 --> 00:09:12.000]   really need to do is saturate compute in a good way.
[00:09:12.000 --> 00:09:17.640]   You need to come up with an architecture that scales, that is sufficiently simple, that
[00:09:17.640 --> 00:09:27.200]   people can mess with the architecture itself and explore their own ideas within it.
[00:09:27.200 --> 00:09:31.360]   But yeah, sufficiently scalable that you can basically go to any parameter count that you
[00:09:31.360 --> 00:09:33.720]   want in a nice way.
[00:09:33.720 --> 00:09:38.680]   So do you think we're the only universe in the multiverse where there's the exact transformer
[00:09:38.680 --> 00:09:39.680]   calculation?
[00:09:39.680 --> 00:09:50.080]   I think there's probably countably infinite, you know, instantiations that do use the transformer.
[00:09:50.080 --> 00:09:52.480]   But do you think it's more than measure zero of the...
[00:09:52.480 --> 00:09:54.080]   Yeah, I think it's measure zero.
[00:09:54.080 --> 00:09:55.080]   I think it's measure zero.
[00:09:55.080 --> 00:09:56.080]   You think it's measure zero?
[00:09:56.080 --> 00:09:57.080]   It's fascinating.
[00:09:57.080 --> 00:10:02.960]   Yeah, I think there's like loads of combinations we could have landed on.
[00:10:02.960 --> 00:10:08.280]   And I mean, I just feel like it's really unlikely that the transformer is optimal.
[00:10:08.280 --> 00:10:15.280]   I think there's like still more exciting avenues to explore in particular SSMs.
[00:10:15.280 --> 00:10:20.520]   So state space models, those are emerging as like really promising alternatives to the
[00:10:20.520 --> 00:10:21.520]   transformer.
[00:10:21.520 --> 00:10:23.200]   How does that, I'm not familiar with that.
[00:10:23.200 --> 00:10:24.200]   How does that work?
[00:10:24.200 --> 00:10:32.400]   Yeah, I'm not familiar with it either, but I do think it's potentially the next step.
[00:10:32.400 --> 00:10:34.040]   There's this idea of like...
[00:10:34.040 --> 00:10:36.480]   Do you want to give me a link to learn more about that?
[00:10:36.480 --> 00:10:42.160]   Yeah, yeah, I think read the S4 paper.
[00:10:42.160 --> 00:10:44.160]   But the general principle is...
[00:10:44.160 --> 00:10:47.840]   What makes you think that they're promising if you don't understand them?
[00:10:47.840 --> 00:10:53.280]   I'm kidding, I'm being facetious.
[00:10:53.280 --> 00:10:59.760]   So SSMs, the idea is that we're trying to cut some middle ground in between transformers,
[00:10:59.760 --> 00:11:01.720]   which are like fully autoregressive.
[00:11:01.720 --> 00:11:04.940]   They tend over the entire past sequence.
[00:11:04.940 --> 00:11:11.920]   And then on the other end of the spectrum are LSTMs or RNNs, which they have a state
[00:11:11.920 --> 00:11:17.720]   and they just, they need to memorize in order to remember the past.
[00:11:17.720 --> 00:11:23.160]   So SSMs are trying to find this middle ground where, yeah, you have some window within which
[00:11:23.160 --> 00:11:25.560]   you can do lookup.
[00:11:25.560 --> 00:11:30.000]   But for everything that's outside of that, you can rely on an internal memory that you
[00:11:30.000 --> 00:11:34.160]   can read and write from.
[00:11:34.160 --> 00:11:41.760]   So doing that, which sounds a lot like a middle ground between the two, doing that while also
[00:11:41.760 --> 00:11:44.520]   being extremely scalable.
[00:11:44.520 --> 00:11:51.640]   So you can paralyze it across thousands of tens of thousands of accelerators.
[00:11:51.640 --> 00:11:55.400]   And it's trying to strike that middle ground.
[00:11:55.400 --> 00:12:02.000]   I think its success is going to be predicated on whether the community builds tooling, because
[00:12:02.000 --> 00:12:07.560]   obviously like the folks at Hugging Face with the Transformers library and many others,
[00:12:07.560 --> 00:12:10.520]   they built incredible software tooling for Transformers.
[00:12:10.520 --> 00:12:18.480]   They make it like trivial to scale from, you know, 10 million parameters up to a trillion
[00:12:18.480 --> 00:12:22.520]   parameters nowadays.
[00:12:22.520 --> 00:12:28.000]   And they've made that trivial, which is tons and tons of work at the software level.
[00:12:28.000 --> 00:12:33.200]   For SSMs, for state space models, it just doesn't exist today.
[00:12:33.200 --> 00:12:34.200]   None of that exists.
[00:12:34.200 --> 00:12:39.960]   There's no mature software platform for scaling these things.
[00:12:39.960 --> 00:12:47.760]   So I could see a world where Transformers get replaced, SSMs, the software support for
[00:12:47.760 --> 00:12:57.160]   them gets more mature and our next generation of model, we lose that context window constraint
[00:12:57.160 --> 00:13:01.320]   that Transformers give us where it's like you have this many tokens, anything outside
[00:13:01.320 --> 00:13:03.080]   of that, sorry, I have no idea about it.
[00:13:03.080 --> 00:13:04.640]   I've never seen it.
[00:13:04.640 --> 00:13:11.160]   Instead, you get something that in theory could have an infinite context window and
[00:13:11.160 --> 00:13:15.800]   it could just continuously read and write from this big buffer of memory.
[00:13:15.800 --> 00:13:21.520]   Although it seemed like the latest GPT-4 release had quite a large context window.
[00:13:21.520 --> 00:13:26.000]   I was kind of surprised to see how big that context window had gotten.
[00:13:26.000 --> 00:13:29.000]   Yeah, yeah, no, it's super exciting.
[00:13:29.000 --> 00:13:32.800]   Do you think they're doing something fancy to make that possible?
[00:13:32.800 --> 00:13:34.440]   Yeah, yeah.
[00:13:34.440 --> 00:13:41.440]   There are nice techniques like Alibi and others that let you quite naturally extend.
[00:13:41.440 --> 00:13:46.200]   So even if they did train it on, let's say, let's say they trained it on a context window
[00:13:46.200 --> 00:13:54.840]   of 8k tokens, they're able to serve one that has access to 32,000 tokens because they have
[00:13:54.840 --> 00:14:04.800]   methods that at inference time can quite naturally, quite easily go beyond that limit.
[00:14:04.800 --> 00:14:12.160]   That was an important breakthrough for not alleviating completely the limitation of transformer
[00:14:12.160 --> 00:14:17.880]   context windows, but giving you a workaround.
[00:14:17.880 --> 00:14:19.640]   I still think we need to push past that.
[00:14:19.640 --> 00:14:26.400]   I still think memory is going to be an important piece of that, but definitely it makes the
[00:14:26.400 --> 00:14:30.840]   issue far less painful.
[00:14:30.840 --> 00:14:35.720]   Can you say more about what it means to saturate compute?
[00:14:35.720 --> 00:14:39.600]   I guess from my perspective, correct me if I'm wrong, the thing that Transformers has
[00:14:39.600 --> 00:14:45.400]   just done phenomenally well is scale as compute gets bigger and bigger.
[00:14:45.400 --> 00:14:48.000]   It seems like almost like nothing else.
[00:14:48.000 --> 00:14:55.560]   In every level of compute, it seems to continue to, the performance continues to improve.
[00:14:55.560 --> 00:15:02.000]   And I don't think that was true of other architectures that folks have used in the past.
[00:15:02.000 --> 00:15:12.000]   But if Transformers is measure zero in the metaverse, what are other ways that you think
[00:15:12.000 --> 00:15:15.000]   the compute could be saturated?
[00:15:15.000 --> 00:15:21.640]   Yeah, I think a very high measure set of architectures are the ones that saturate compute.
[00:15:21.640 --> 00:15:26.720]   That's kind of like a necessary component of success.
[00:15:26.720 --> 00:15:31.840]   What does it take though for an architecture to saturate compute?
[00:15:31.840 --> 00:15:44.100]   It requires tons and tons and tons of matmuls and very few unnecessary ops.
[00:15:44.100 --> 00:15:49.800]   You really want your entire architecture to just look like big matmuls because you can
[00:15:49.800 --> 00:15:51.800]   split those up across.
[00:15:51.800 --> 00:16:00.280]   But wouldn't that be true of just a very classical, fully connected neural network would satisfy
[00:16:00.280 --> 00:16:01.280]   that condition, right?
[00:16:01.280 --> 00:16:04.080]   That would be ideal.
[00:16:04.080 --> 00:16:07.120]   If we could just build MLPs.
[00:16:07.120 --> 00:16:11.400]   Why doesn't a multi-layer perceptron work then?
[00:16:11.400 --> 00:16:13.680]   It seems like it would satisfy your condition as laid out.
[00:16:13.680 --> 00:16:17.760]   I think Transformers are almost MLPs.
[00:16:17.760 --> 00:16:23.080]   They're like a very short skip away from MLPs.
[00:16:23.080 --> 00:16:26.080]   They do look like just a bunch of matmuls.
[00:16:26.080 --> 00:16:33.640]   They add in one more axis to do matmuls across and that's the length of your sequence.
[00:16:33.640 --> 00:16:40.200]   But I think it's basically trying to cut as close to a massive MLP as you possibly can
[00:16:40.200 --> 00:16:47.200]   because those do saturate compute the best.
[00:16:47.200 --> 00:16:56.240]   What you want to avoid are tons of little ops like softmaxes and little activation functions,
[00:16:56.240 --> 00:17:02.680]   dropout layers, all of these little things which break up those big matmuls.
[00:17:02.680 --> 00:17:14.600]   You certainly don't want to introduce operations that break your horizontal scaling of layers,
[00:17:14.600 --> 00:17:26.200]   your ability to split a layer across tens of accelerators, your width-wise parallelism,
[00:17:26.200 --> 00:17:34.360]   anything that you have to do kind of intralayer access across these chunks.
[00:17:34.360 --> 00:17:38.840]   That slows things down because now you need to start communicating between those parallel
[00:17:38.840 --> 00:17:41.840]   lines of compute.
[00:17:41.840 --> 00:17:46.240]   You want to minimize the amount of intralayer communication you have to do and just maximize
[00:17:46.240 --> 00:17:52.480]   your ability to parallelize that, let it run, and then come back with a result.
[00:17:52.480 --> 00:17:55.000]   That's really just a computational optimization, right?
[00:17:55.000 --> 00:17:58.640]   There's nothing intrinsic about that?
[00:17:58.640 --> 00:18:00.200]   Intrinsic in what sense?
[00:18:00.200 --> 00:18:07.520]   I guess, do you think almost any architecture trained for long enough with enough parameters
[00:18:07.520 --> 00:18:10.960]   would have this property that it would improve over time?
[00:18:10.960 --> 00:18:17.120]   And then really what you're looking for is just something where you can do back propagation
[00:18:17.120 --> 00:18:20.480]   distributed quickly?
[00:18:20.480 --> 00:18:21.760]   Is that what you're saying?
[00:18:21.760 --> 00:18:31.680]   I do believe that there are a lot of possible architectures that would be fast, efficient,
[00:18:31.680 --> 00:18:38.400]   and result in performance that we're seeing from the current large language models.
[00:18:38.400 --> 00:18:46.000]   There are some things you can't literally just scale up a MLP with Reloose because that
[00:18:46.000 --> 00:18:47.600]   would be done point-wise, right?
[00:18:47.600 --> 00:18:49.640]   It would just be a bag-of-words model.
[00:18:49.640 --> 00:18:52.240]   You wouldn't be able to learn relationships between words.
[00:18:52.240 --> 00:18:54.360]   You do need sequence structure.
[00:18:54.360 --> 00:18:58.200]   You need to train sequence models.
[00:18:58.200 --> 00:19:06.000]   But so long as you're not breaking that or severely compromising that, I think there's
[00:19:06.000 --> 00:19:10.600]   a huge swath of models that would perform equivalently well and would scale equivalently
[00:19:10.600 --> 00:19:11.600]   well.
[00:19:11.600 --> 00:19:20.000]   And we've mostly just had this joint optimization between our software, like the transformers
[00:19:20.000 --> 00:19:25.760]   and the frameworks that train them, and even our hardware, like the routines that we support
[00:19:25.760 --> 00:19:29.760]   within CUDA and within our accelerators.
[00:19:29.760 --> 00:19:35.880]   It's been feeding back on each other for a little while now.
[00:19:35.880 --> 00:19:40.240]   And so at the moment, there might be a local minimum where it's hard to break off of transformers
[00:19:40.240 --> 00:19:43.920]   because there's just so much software and hardware support for transformers.
[00:19:43.920 --> 00:19:45.960]   It's so heavily optimized.
[00:19:45.960 --> 00:19:52.640]   As soon as you step off of that rail, you're sacrificing some performance because now you're
[00:19:52.640 --> 00:19:59.960]   dropping back to a more general implementation of a kernel, which runs 20% slower, which
[00:19:59.960 --> 00:20:06.480]   for a big model costs you, I don't know, a million dollars or something.
[00:20:06.480 --> 00:20:11.260]   So there are some interesting effects there with that feedback between hardware and software
[00:20:11.260 --> 00:20:16.600]   and us kind of getting locked in to one architecture.
[00:20:16.600 --> 00:20:22.720]   So then do you think that there's no architecture that's sufficiently better than transformers
[00:20:22.720 --> 00:20:28.080]   that there would be an impetus to change?
[00:20:28.080 --> 00:20:29.080]   Do I believe that?
[00:20:29.080 --> 00:20:35.080]   I think stuff like the sequence length constraint could be problematic enough that it actually
[00:20:35.080 --> 00:20:37.840]   might push us off of transformers.
[00:20:37.840 --> 00:20:40.960]   Yeah, I would not be surprised if that was the case.
[00:20:40.960 --> 00:20:43.100]   It's pretty...
[00:20:43.100 --> 00:20:45.260]   That is like a...
[00:20:45.260 --> 00:20:51.720]   Scaling quadratically in the sequence length is a huge problem.
[00:20:51.720 --> 00:20:55.360]   One that you only run into once you start doing more and more interesting stuff.
[00:20:55.360 --> 00:21:00.500]   But I think we're heading into that territory, especially with multimodality.
[00:21:00.500 --> 00:21:09.420]   I would think then that you would predict that the model performance will continue to
[00:21:09.420 --> 00:21:16.640]   scale consistently as compute is added basically forever.
[00:21:16.640 --> 00:21:19.060]   Would you agree with that?
[00:21:19.060 --> 00:21:21.700]   Yeah, definitely.
[00:21:21.700 --> 00:21:24.660]   Yeah, I think it is quite predictable.
[00:21:24.660 --> 00:21:32.380]   If you fix the dataset, you fix the architecture, and you choose your levers of scaling, it
[00:21:32.380 --> 00:21:37.540]   is going to give you quite a predictable scaling pattern or scaling law.
[00:21:37.540 --> 00:21:42.420]   And how do you think about data constraints as the model complexity grows?
[00:21:42.420 --> 00:21:49.300]   Is data going to be the next constraint then for building very large LLMs?
[00:21:49.300 --> 00:21:53.840]   Yeah, it already really, really is.
[00:21:53.840 --> 00:21:59.100]   So I think of these models, like I think of the tech stack for large language models as
[00:21:59.100 --> 00:22:02.540]   a literal stack where each piece builds off the one before.
[00:22:02.540 --> 00:22:10.820]   And we started with base models like GPT-3, and you just train on, I don't know, like
[00:22:10.820 --> 00:22:14.300]   a trillion token scraped from the web and super noisy.
[00:22:14.300 --> 00:22:19.700]   But you learn a little bit about a lot of different stuff.
[00:22:19.700 --> 00:22:24.420]   You consume a ton of knowledge from all over the web.
[00:22:24.420 --> 00:22:26.860]   That model is like, it's not useful.
[00:22:26.860 --> 00:22:28.500]   It's not useful.
[00:22:28.500 --> 00:22:32.020]   It's very cool that it has some, you know, prompting few-shot behavior and that type
[00:22:32.020 --> 00:22:33.020]   of thing.
[00:22:33.020 --> 00:22:36.620]   But it's really hard to make it useful for you.
[00:22:36.620 --> 00:22:41.820]   And so what you need to do is then train what at Cohere we call them command models, OpenAI
[00:22:41.820 --> 00:22:44.220]   calls them instruct models.
[00:22:44.220 --> 00:22:53.900]   You need to train these instruction tuned models from human feedback.
[00:22:53.900 --> 00:22:58.220]   And that changes the model into something that is way more useful.
[00:22:58.220 --> 00:23:00.260]   It changes the UI onto the model.
[00:23:00.260 --> 00:23:03.260]   Now you can just give it a natural language instruction.
[00:23:03.260 --> 00:23:08.100]   It leverages all that stuff that it learned from the first training phase of just the
[00:23:08.100 --> 00:23:10.560]   raw internet.
[00:23:10.560 --> 00:23:12.900]   But it's just a much more pleasant user experience.
[00:23:12.900 --> 00:23:17.100]   Those are like far, far more useful.
[00:23:17.100 --> 00:23:19.980]   The next step above that is dialogue models.
[00:23:19.980 --> 00:23:26.220]   And so again, you go out and collect some human data of dialogue, of conversation, and
[00:23:26.220 --> 00:23:32.980]   you fine tune that command model, that instruction following model to do dialogue.
[00:23:32.980 --> 00:23:36.240]   Again that unlocks another huge improvement in user experience.
[00:23:36.240 --> 00:23:38.460]   People just love conversation, right?
[00:23:38.460 --> 00:23:44.980]   That's what you and I are doing right now.
[00:23:44.980 --> 00:23:45.980]   It's the natural intellectual modality.
[00:23:45.980 --> 00:23:48.340]   The most natural thing for humans to do is chat to each other, have a conversation.
[00:23:48.340 --> 00:23:56.740]   And so when you flip into that modality, you flip into conversation, it's driven by data.
[00:23:56.740 --> 00:24:02.940]   When you flip into command following or instruction following, it's driven by human annotated
[00:24:02.940 --> 00:24:05.580]   data.
[00:24:05.580 --> 00:24:13.500]   So this tech stack that we're working along, the vertical momentum is ostensibly entirely
[00:24:13.500 --> 00:24:14.500]   driven by data.
[00:24:14.500 --> 00:24:15.500]   There's almost no modeling changes.
[00:24:15.500 --> 00:24:16.500]   You're working off of that same.
[00:24:16.500 --> 00:24:17.500]   Sorry, what is vertical momentum?
[00:24:17.500 --> 00:24:21.660]   Oh, vertical momentum up this tech stack.
[00:24:21.660 --> 00:24:27.460]   I'm thinking base model, command model, dialogue model, et cetera, et cetera.
[00:24:27.460 --> 00:24:31.980]   The momentum up that tech stack, the thing that takes you from one layer to the next,
[00:24:31.980 --> 00:24:33.340]   it's all data.
[00:24:33.340 --> 00:24:43.300]   And I guess, what is a model like before it's had this specific human feedback to make it
[00:24:43.300 --> 00:24:44.700]   respond to commands?
[00:24:44.700 --> 00:24:49.020]   What does it feel like to interact with that?
[00:24:49.020 --> 00:24:50.420]   It feels a little bit...
[00:24:50.420 --> 00:24:57.540]   What's a good way to compare?
[00:24:57.540 --> 00:25:02.420]   Feels a little bit schizophrenic.
[00:25:02.420 --> 00:25:09.620]   It's like very all over the place.
[00:25:09.620 --> 00:25:12.340]   It's hard to get it to do what you want.
[00:25:12.340 --> 00:25:17.220]   You're trying to give it instructions and it might break in some very unpredictable
[00:25:17.220 --> 00:25:18.220]   way.
[00:25:18.220 --> 00:25:23.940]   You might change one word and suddenly it gets what you're saying and is capable of
[00:25:23.940 --> 00:25:26.540]   actioning on it.
[00:25:26.540 --> 00:25:35.100]   It's a very inconsistent partner to build with and to create systems with.
[00:25:35.100 --> 00:25:42.260]   So it knows tons of stuff and has tons of ideas and it can vaguely pull from its big,
[00:25:42.260 --> 00:25:47.740]   vast breadth of knowledge, but it's very disorganized.
[00:25:47.740 --> 00:25:54.380]   It's very hard to pull coherent behavior out of it.
[00:25:54.380 --> 00:25:55.380]   And so that's what that...
[00:25:55.380 --> 00:26:02.380]   And then what exactly is happening in that fine-tuning phase with the human feedback?
[00:26:02.380 --> 00:26:05.500]   What exactly are you doing?
[00:26:05.500 --> 00:26:11.700]   So you're collecting examples of the behavior that you want the model to possess.
[00:26:11.700 --> 00:26:18.900]   So in the command phase, which is just one layer above the base model, you're collecting
[00:26:18.900 --> 00:26:27.100]   pairs of examples that are like, I give the model an instruction and the model responds
[00:26:27.100 --> 00:26:29.380]   following that instruction.
[00:26:29.380 --> 00:26:34.300]   You do that a bunch of times for a ton of different tasks and the model just picks up
[00:26:34.300 --> 00:26:39.860]   the general idea of, okay, the user is going to tell me what they want me to do with some
[00:26:39.860 --> 00:26:44.460]   data and my job is to just respond with the answer.
[00:26:44.460 --> 00:26:46.500]   And it starts to behave like that.
[00:26:46.500 --> 00:26:52.540]   And so now it's not super finicky about the placement of words, the word choice, like
[00:26:52.540 --> 00:26:55.340]   kind of indifferent to that.
[00:26:55.340 --> 00:27:00.540]   And that UI is just way more usable, way more useful.
[00:27:00.540 --> 00:27:04.820]   And how do you measure if that's working or how do you know when you're done with that
[00:27:04.820 --> 00:27:05.820]   part?
[00:27:05.820 --> 00:27:06.820]   Yeah.
[00:27:06.820 --> 00:27:14.540]   So measurement of performance, like evaluation of these models is incredibly hard for a bunch
[00:27:14.540 --> 00:27:15.660]   of different reasons.
[00:27:15.660 --> 00:27:20.740]   One of them is like a lot of the academic data sets that we have are super noisy and
[00:27:20.740 --> 00:27:25.980]   low signal and they might actually be like negative signal.
[00:27:25.980 --> 00:27:29.740]   Like if you're doing really well on a particular academic data set, that might be like cause
[00:27:29.740 --> 00:27:34.460]   for concern, which is worrying.
[00:27:34.460 --> 00:27:38.460]   And then the other piece is that these models have seen the internet and so there might
[00:27:38.460 --> 00:27:39.460]   be like leakage, right?
[00:27:39.460 --> 00:27:41.660]   There might be test set leakage.
[00:27:41.660 --> 00:27:49.140]   And so I think the only reliable measure is actually putting it in front of people and
[00:27:49.140 --> 00:27:52.300]   asking them, you know, which models you prefer.
[00:27:52.300 --> 00:27:55.900]   You put two models in front of them, you let them interact with both, put prompts into
[00:27:55.900 --> 00:27:59.740]   both and just pick the one that they prefer.
[00:27:59.740 --> 00:28:03.780]   That's the most reliable measure.
[00:28:03.780 --> 00:28:09.580]   But what I was just describing of like going from the base model up to the command model,
[00:28:09.580 --> 00:28:12.340]   you can just measure that in academic data set performance.
[00:28:12.340 --> 00:28:16.380]   Like if you throw a base model at these academic data sets and you throw a command model, which
[00:28:16.380 --> 00:28:22.860]   is really just like a derivative of the base model, the command model is going to perform
[00:28:22.860 --> 00:28:29.340]   just leaks, leaks better, dramatically better.
[00:28:29.340 --> 00:28:34.060]   And how much like effort is this step?
[00:28:34.060 --> 00:28:41.700]   Like is it like comparable in cost to training the base model or like how, like practically
[00:28:41.700 --> 00:28:42.700]   how long does it take?
[00:28:42.700 --> 00:28:44.180]   Like how expensive is it?
[00:28:44.180 --> 00:28:45.900]   It's really hard to get right.
[00:28:45.900 --> 00:28:49.220]   Of course, it's all data collection.
[00:28:49.220 --> 00:28:56.100]   The scale of data is way less and the like duration of training is way shorter than the
[00:28:56.100 --> 00:28:59.180]   initial phase, like the base model phase.
[00:28:59.180 --> 00:29:06.020]   We're certainly not talking about trillions of tokens in this curated data regime.
[00:29:06.020 --> 00:29:11.740]   But like the specific tokens that you choose are extremely important.
[00:29:11.740 --> 00:29:20.060]   They need to be noiseless, need to be extremely, you know, pint combed over and make sure that
[00:29:20.060 --> 00:29:27.900]   they're clean and make sure that they, there's no, you know, noisy samples in there.
[00:29:27.900 --> 00:29:31.500]   And that's a really difficult, expensive process.
[00:29:31.500 --> 00:29:38.060]   It involves a lot of humans looking over a lot of data.
[00:29:38.060 --> 00:29:43.780]   In terms of like comparison of the cost, it's definitely less than the base model.
[00:29:43.780 --> 00:29:46.300]   It's definitely less.
[00:29:46.300 --> 00:29:49.380]   I don't know like the relative ratio.
[00:29:49.380 --> 00:29:56.980]   Yeah, I would say it's all expensive.
[00:29:56.980 --> 00:29:59.540]   It's certainly expensive to get humans to generate data for you.
[00:29:59.540 --> 00:30:03.300]   It's like extremely costly.
[00:30:03.300 --> 00:30:13.060]   And the more valuable the data, the more, you know, smart or challenging or the more
[00:30:13.060 --> 00:30:15.820]   valuable the data, the more expensive it is to get.
[00:30:15.820 --> 00:30:21.780]   You can imagine if you want your model to be as good as a lawyer in answering legal
[00:30:21.780 --> 00:30:26.860]   questions, the data that you need to collect there is going to be from lawyers that cost
[00:30:26.860 --> 00:30:29.220]   a thousand dollars an hour.
[00:30:29.220 --> 00:30:31.980]   And so the cost can be extreme.
[00:30:31.980 --> 00:30:43.980]   I guess when I look at the performance of large language models, it's hard to not sort
[00:30:43.980 --> 00:30:53.380]   of like infer this exponential curve and sort of expect that, you know, like in a year they're
[00:30:53.380 --> 00:30:58.140]   going to be even more amazing than they are today.
[00:30:58.140 --> 00:31:03.540]   But if your view is that we sort of hit this wall with data for the base model, is that
[00:31:03.540 --> 00:31:04.540]   wrong then?
[00:31:04.540 --> 00:31:10.580]   Are we on the verge of needing a totally new approach?
[00:31:10.580 --> 00:31:22.500]   I think we are approaching the need for another scaling breakthrough.
[00:31:22.500 --> 00:31:26.260]   I guess I don't know how close we are to it, but it definitely feels like it's coming up
[00:31:26.260 --> 00:31:32.460]   and we're starting to hit the limits of these models are increasingly operating at average
[00:31:32.460 --> 00:31:40.740]   human performance or above on such a wide selection of tasks that you can no longer
[00:31:40.740 --> 00:31:45.580]   rely on the average human as a source of data to improve your model.
[00:31:45.580 --> 00:31:51.980]   Obviously once your model performs as well as an average human, it's no longer valuable
[00:31:51.980 --> 00:31:57.700]   to collect data from average humans because they don't add to the model's performance.
[00:31:57.700 --> 00:32:05.380]   You need to start going to exceptional people, start going to experts in particular fields
[00:32:05.380 --> 00:32:11.300]   or outliers in terms of performance at particular things.
[00:32:11.300 --> 00:32:15.380]   And eventually you run into the bottleneck of humanity's performance on a particular
[00:32:15.380 --> 00:32:22.620]   task and then you can't go to humans to offer data that outperforms your model because your
[00:32:22.620 --> 00:32:28.060]   model is already performing as well as the best human at that task.
[00:32:28.060 --> 00:32:36.460]   You need to search for a way for the model to self-improve and to work with itself and
[00:32:36.460 --> 00:32:43.540]   test itself to start getting effects like you saw with AlphaZero where it's obvious
[00:32:43.540 --> 00:32:45.820]   how to do that in a game playing setting.
[00:32:45.820 --> 00:32:50.380]   Two copies of the model play against each other, they self-improve, but each one gets
[00:32:50.380 --> 00:32:52.700]   a little bit better and the other one has to beat it.
[00:32:52.700 --> 00:32:56.940]   It's much more difficult to think about how you would do that with a large language model,
[00:32:56.940 --> 00:33:04.940]   but at some point once these start to reach top tier human performance, we're going to
[00:33:04.940 --> 00:33:09.380]   have to find ways to enable that sort of interaction and self-improvement.
[00:33:09.380 --> 00:33:16.460]   Interesting. I mean, it does appear like these models can do lots of things that the average
[00:33:16.460 --> 00:33:19.300]   person can't do or I can't do.
[00:33:19.300 --> 00:33:30.460]   I guess one domain where it's much easier to check than to generate is code generation.
[00:33:30.460 --> 00:33:36.620]   I'm amazed at the quality of, or maybe the rapid progress in code generation.
[00:33:36.620 --> 00:33:44.420]   It sort of seems to me like it could be easy to at least know if you got the right answer
[00:33:44.420 --> 00:33:48.140]   for a code generation problem.
[00:33:48.140 --> 00:33:51.380]   Does that seem maybe more tractable than other domains?
[00:33:51.380 --> 00:33:55.780]   Yeah, I mean writing software is nice.
[00:33:55.780 --> 00:34:00.540]   You can run it and if it doesn't compile, that's a huge issue.
[00:34:00.540 --> 00:34:10.420]   If it runs but outputs the wrong answer, that's a very clear signal for success there.
[00:34:10.420 --> 00:34:14.500]   It's much softer in other areas.
[00:34:14.500 --> 00:34:22.700]   So I definitely think that code has nice properties for those reasons.
[00:34:22.700 --> 00:34:33.780]   Verification of code is also a super hard problem and there can be very subtle bugs
[00:34:33.780 --> 00:34:36.580]   that your tests don't capture that the model might introduce.
[00:34:36.580 --> 00:34:44.980]   Then you'll miss and then you'll deploy this and it could have catastrophic consequences.
[00:34:44.980 --> 00:34:50.780]   In some sense, it's nice because it feels like a much harder and more objective "yes,
[00:34:50.780 --> 00:34:59.420]   the model did well" or "no, it didn't" although there's nuance to that whole topic as well.
[00:34:59.420 --> 00:35:03.220]   I also think it's a limited setting as well.
[00:35:03.220 --> 00:35:11.700]   There's a lot you can do in code and in software development, but if we're trying to completely
[00:35:11.700 --> 00:35:20.980]   transform human productivity or value creation or whatever, we have to step outside the bounds
[00:35:20.980 --> 00:35:28.900]   of code and we need to be able to automate work in spaces that have nothing to do with
[00:35:28.900 --> 00:35:29.900]   code.
[00:35:29.900 --> 00:35:39.900]   I think it's a great platform for experimenting and developing with these models, but we have
[00:35:39.900 --> 00:35:43.300]   a lot to do outside of code as well.
[00:35:43.300 --> 00:35:48.420]   That makes sense.
[00:35:48.420 --> 00:35:50.820]   I'd love to ask you about your company, Cohere.
[00:35:50.820 --> 00:36:00.300]   You're also CEO of a company that's building these models and making them available.
[00:36:00.300 --> 00:36:07.640]   How would you describe Cohere's positioning in the landscape of companies building and
[00:36:07.640 --> 00:36:09.140]   selling these large models?
[00:36:09.140 --> 00:36:16.820]   Yeah, so for Cohere, we got started I think three and a half years ago.
[00:36:16.820 --> 00:36:24.820]   So that was before GPT-3, but I believe after GPT-2.
[00:36:24.820 --> 00:36:33.240]   Even back then, the founding mission, it was really just to get this tech deployed in as
[00:36:33.240 --> 00:36:39.460]   many hands and as many products as we possibly could.
[00:36:39.460 --> 00:36:48.500]   I think my co-founders and I, Nick and Iben, we were obviously part of the folks that got
[00:36:48.500 --> 00:36:56.620]   the earliest access to these sorts of models, these generative models.
[00:36:56.620 --> 00:37:05.080]   I felt personally like it had pulled my timelines for interesting, compelling AI way, way, way
[00:37:05.080 --> 00:37:09.400]   forward, like decades.
[00:37:09.400 --> 00:37:15.880]   And so it felt like we were getting a glimpse of the future and that there was a very important
[00:37:15.880 --> 00:37:18.940]   technological frontier that was about to get crossed.
[00:37:18.940 --> 00:37:24.320]   We weren't seeing things lining up to make these models accessible to people.
[00:37:24.320 --> 00:37:26.420]   They were behind barriers.
[00:37:26.420 --> 00:37:34.580]   They were inside of large behemoths and they weren't being put into the hands of developers
[00:37:34.580 --> 00:37:41.460]   and enterprises weren't being supported in overcoming the barriers to adoption.
[00:37:41.460 --> 00:37:47.620]   And so Cohere, our product company, the whole point is really undo those barriers, put it
[00:37:47.620 --> 00:37:53.980]   in more hands, make the APIs easy to use, accessible to folks who aren't machine learning
[00:37:53.980 --> 00:37:59.700]   experts and then solve all those barriers that enterprises have to adoption.
[00:37:59.700 --> 00:38:07.260]   I think the reason why it's taken this long to see the surface area of products and technology
[00:38:07.260 --> 00:38:14.460]   change with LLMs is because of mundane stuff like data privacy and people trusting these
[00:38:14.460 --> 00:38:21.580]   models and us building awareness so that there's actual consumer demand for, "No, the way that
[00:38:21.580 --> 00:38:27.940]   I want to interact with your product is through dialogue."
[00:38:27.940 --> 00:38:33.260]   We're now at a point where it feels like the market is seeing that and enterprises are
[00:38:33.260 --> 00:38:40.500]   starting to see that the consumer is going to choose the product that lets them interact
[00:38:40.500 --> 00:38:42.740]   with it in the best possible way.
[00:38:42.740 --> 00:38:47.980]   In the same way that when I was graduating from high school, I chose my bank because
[00:38:47.980 --> 00:38:51.460]   it had a mobile app and that's how I wanted to interact with my bank.
[00:38:51.460 --> 00:38:56.420]   I didn't want to use my browser or walk into an actual physical location.
[00:38:56.420 --> 00:39:00.100]   I just wanted it on my phone.
[00:39:00.100 --> 00:39:03.860]   In the future, I think the next generation that's graduating from high school, they're
[00:39:03.860 --> 00:39:08.700]   going to choose their bank based on the one that they don't have to call in to debug something.
[00:39:08.700 --> 00:39:09.700]   They can just talk to it.
[00:39:09.700 --> 00:39:15.460]   They can just chat to it at 3am.
[00:39:15.460 --> 00:39:21.300]   So those consumer product decisions are going to be based off of the interfaces that there
[00:39:21.300 --> 00:39:25.060]   are able to use products with.
[00:39:25.060 --> 00:39:29.540]   And Cohere wants to help enable that, to help accelerate organizations in adopting that
[00:39:29.540 --> 00:39:35.060]   because it's going to become a competitive necessity.
[00:39:35.060 --> 00:39:43.860]   The vision is to really just support everyone in adopting this tech and to help accelerate
[00:39:43.860 --> 00:39:45.620]   that adoption.
[00:39:45.620 --> 00:39:53.460]   So do you offer a way to use these models without the data leaving your own infrastructure?
[00:39:53.460 --> 00:39:55.500]   Yeah, totally.
[00:39:55.500 --> 00:39:56.940]   So we're cloud agnostic.
[00:39:56.940 --> 00:40:00.980]   We're not locked into any one cloud.
[00:40:00.980 --> 00:40:05.740]   And yeah, the data privacy piece is a super important one, especially if you're working
[00:40:05.740 --> 00:40:12.540]   with customer data or internal documents and secrets.
[00:40:12.540 --> 00:40:18.340]   So we can deploy within your VPC, no visibility on our side of your data.
[00:40:18.340 --> 00:40:23.620]   So yeah, data privacy is a core feature for us.
[00:40:23.620 --> 00:40:34.340]   And I guess it seems like you offer specific use case based APIs.
[00:40:34.340 --> 00:40:40.740]   But I mean, one thing I've been struck by with GPT-3 being publicly available is just
[00:40:40.740 --> 00:40:48.260]   the plethora of use cases that seem like maybe possible or new use cases possible.
[00:40:48.260 --> 00:40:51.900]   How do you think about that?
[00:40:51.900 --> 00:40:56.220]   Or is there a way to use your models more flexibly?
[00:40:56.220 --> 00:41:01.020]   Or do you plan to release tons of APIs on every possible way that someone might want
[00:41:01.020 --> 00:41:04.740]   to use a model like this?
[00:41:04.740 --> 00:41:10.740]   So we have the general models, which are like our command models.
[00:41:10.740 --> 00:41:14.220]   And you can use them for anything within terms of service, right?
[00:41:14.220 --> 00:41:20.160]   Like super general, whatever you want to do, and it extraction, summarization, just chatting
[00:41:20.160 --> 00:41:24.300]   to it, like whatever you want to use it for.
[00:41:24.300 --> 00:41:26.460]   It'll support it.
[00:41:26.460 --> 00:41:28.380]   And if it doesn't support it, let us know.
[00:41:28.380 --> 00:41:36.100]   And we have the ability to improve it next week when we launch a new version.
[00:41:36.100 --> 00:41:42.780]   So I think, yeah, definitely the general purpose nature of the technology is a huge piece of
[00:41:42.780 --> 00:41:47.500]   the value prop that you can just go to one model and you can get it to do tons of different
[00:41:47.500 --> 00:41:49.060]   tasks for you.
[00:41:49.060 --> 00:41:54.980]   At the same time, there is value to specialized endpoints, which we also build stuff like
[00:41:54.980 --> 00:42:01.220]   the summarize endpoint that we built, the classify endpoint, the search endpoint.
[00:42:01.220 --> 00:42:04.380]   And so those specialized ones, they're much more targeted.
[00:42:04.380 --> 00:42:07.580]   And there's only going to be a few of them because we're only going to focus on the most
[00:42:07.580 --> 00:42:15.580]   popular use cases, like summarization, like search, like classification.
[00:42:15.580 --> 00:42:20.380]   But those will be very tailored to that use case.
[00:42:20.380 --> 00:42:21.380]   So there's both.
[00:42:21.380 --> 00:42:26.780]   There's like the highly general command style model, and then there's specific endpoints
[00:42:26.780 --> 00:42:30.900]   targeted at one use case.
[00:42:30.900 --> 00:42:39.100]   How do you think about open source in general and sort of opening up these models more?
[00:42:39.100 --> 00:42:41.980]   Yeah, I mean, I love open source.
[00:42:41.980 --> 00:42:50.860]   I come from research, which is inherently, or it's supposed to be inherently open.
[00:42:50.860 --> 00:42:57.500]   At the same time, I am trying to build a business, something that's sustainable, an economic
[00:42:57.500 --> 00:43:03.140]   engine that lets us continue to innovate, continue to compete.
[00:43:03.140 --> 00:43:10.860]   And giving away your IP for free tends to be a bad business model.
[00:43:10.860 --> 00:43:14.580]   You disintermediate yourself.
[00:43:14.580 --> 00:43:21.900]   And so we've been very hesitant to do that, principally from the fact that we want to
[00:43:21.900 --> 00:43:26.140]   build something healthy and sustainable.
[00:43:26.140 --> 00:43:29.200]   We want to be doing this for the rest of our lives.
[00:43:29.200 --> 00:43:35.380]   And to do that, we need people to pay us for the work that we do.
[00:43:35.380 --> 00:43:42.780]   But I'm super, super supportive of more open models and better performing open source models.
[00:43:42.780 --> 00:43:47.860]   There will always be that category of developer who, like they don't want to use an API.
[00:43:47.860 --> 00:43:52.580]   They want to get right down to the parameters and mess around with that and compress the
[00:43:52.580 --> 00:43:56.660]   model onto their laptop, et cetera.
[00:43:56.660 --> 00:44:03.260]   And I think there's loads of groups out there that are doing that work and are building
[00:44:03.260 --> 00:44:04.260]   that foundation.
[00:44:04.260 --> 00:44:10.860]   I think Aluther, Carper, Stability, there's loads of these folks who are doing that.
[00:44:10.860 --> 00:44:17.260]   And so I'm super happy to see them out there and I really appreciate the work that they
[00:44:17.260 --> 00:44:18.260]   do.
[00:44:18.260 --> 00:44:27.580]   I think I saw a result from Stanford recently, Alpaca, where they hit an API of an LLM quite
[00:44:27.580 --> 00:44:32.280]   a bit to the point where they were able to reconstruct the LLM for a really small amount
[00:44:32.280 --> 00:44:33.280]   of money.
[00:44:33.280 --> 00:44:35.060]   Does that seem right to you?
[00:44:35.060 --> 00:44:45.700]   Does that approach worry you that your customers might rebuild your models?
[00:44:45.700 --> 00:44:47.300]   I think it doesn't seem right.
[00:44:47.300 --> 00:45:01.020]   I think the result may have been a little bit exaggerated or maybe misunderstood by
[00:45:01.020 --> 00:45:02.260]   some folks.
[00:45:02.260 --> 00:45:07.260]   The performance of that model is super impressive and I think it's ostensibly like a distillation
[00:45:07.260 --> 00:45:11.860]   of one of these large models into a smaller model.
[00:45:11.860 --> 00:45:17.260]   When you get down to it, that's principally what's happening.
[00:45:17.260 --> 00:45:23.940]   And maybe the interesting result is the extent to which you can recover interesting behavior
[00:45:23.940 --> 00:45:33.780]   in a small model, but it still leaks behind a larger model.
[00:45:33.780 --> 00:45:39.300]   And its utility is much more narrow than that large model.
[00:45:39.300 --> 00:45:43.540]   It might be good at the few things that it was trained to do and it might evaluate well
[00:45:43.540 --> 00:45:49.140]   on a specific subset of tasks.
[00:45:49.140 --> 00:45:54.420]   You lose a lot of what makes those big models magic.
[00:45:54.420 --> 00:45:58.060]   You lose a lot of that generality.
[00:45:58.060 --> 00:46:04.420]   But yeah, I think if you pick 15-30 tasks, you can train an extremely small model to
[00:46:04.420 --> 00:46:06.700]   perform as well as a big model.
[00:46:06.700 --> 00:46:13.020]   As soon as you narrow in on a small set of abilities, you can get quite good performance
[00:46:13.020 --> 00:46:14.020]   there.
[00:46:14.020 --> 00:46:20.540]   I think that's an exciting truth, right?
[00:46:20.540 --> 00:46:26.180]   Because you can go from using these big general models to, as soon as you know your use case
[00:46:26.180 --> 00:46:31.580]   and as soon as you know the thing you want to do, you can scale down massively and get
[00:46:31.580 --> 00:46:36.020]   a much cheaper version of that system.
[00:46:36.020 --> 00:46:42.100]   So I think that's an interesting result, but I don't think it's fair to say that Stanford
[00:46:42.100 --> 00:46:46.300]   Resolve is the same as the large model.
[00:46:46.300 --> 00:46:47.300]   Those two are not the same.
[00:46:47.300 --> 00:46:52.380]   This one that can run on your cell phone, it's impressive within a limited domain, but
[00:46:52.380 --> 00:46:53.380]   it's lost something.
[00:46:53.380 --> 00:46:59.580]   It's lost some generality, some intelligence relative to the large one it was distilled
[00:46:59.580 --> 00:47:00.580]   from.
[00:47:00.580 --> 00:47:05.460]   So it sounds like your experience of working on these large language models has pulled
[00:47:05.460 --> 00:47:13.220]   up by decades your belief about when they get interesting, whatever that means.
[00:47:13.220 --> 00:47:16.580]   It has for me too, to be honest.
[00:47:16.580 --> 00:47:25.220]   I guess it makes me feel like AGI, which isn't clearly defined, but there's aspects of it
[00:47:25.220 --> 00:47:29.220]   that seem incredibly important for the world.
[00:47:29.220 --> 00:47:34.940]   It makes me think that that really could happen in our lifetime when you have like platform
[00:47:34.940 --> 00:47:35.940]   or the curve.
[00:47:35.940 --> 00:47:41.860]   I mean, I have to ask for you, is that like top of mind for you in your work?
[00:47:41.860 --> 00:47:46.340]   Does that seem like something that's coming?
[00:47:46.340 --> 00:47:49.580]   I don't think so.
[00:47:49.580 --> 00:48:00.180]   Like I don't spend an outsized amount of my time thinking about AGI.
[00:48:00.180 --> 00:48:06.020]   I do spend an outsized amount of my time thinking about how to make models more useful.
[00:48:06.020 --> 00:48:09.180]   I think that's along the critical path towards AGI.
[00:48:09.180 --> 00:48:18.620]   We're going to build a lot of useful stuff that's going to make people way more efficient,
[00:48:18.620 --> 00:48:22.620]   way more productive.
[00:48:22.620 --> 00:48:30.660]   That lofty goal of AGI, I think it's exciting and it's super salient.
[00:48:30.660 --> 00:48:42.140]   It's very easy to get sucked into it and it certainly makes your work feel monumental
[00:48:42.140 --> 00:48:45.900]   in terms of importance.
[00:48:45.900 --> 00:48:53.260]   But I don't think you need AGI for this technology to be extremely impactful.
[00:48:53.260 --> 00:49:03.420]   I think there's going to be a lot of other issues along the way deploying these models
[00:49:03.420 --> 00:49:13.420]   that don't touch on the issues that maybe AGI discourse puts front and center.
[00:49:13.420 --> 00:49:28.500]   So I'm sort of on the side of the AGI discourse matters and we should be investing time and
[00:49:28.500 --> 00:49:36.580]   thought into it, but it's completely consumed the conversation around AI in general to a
[00:49:36.580 --> 00:49:45.420]   point where it's distracting and a lot of the issues that the AGI community tout as
[00:49:45.420 --> 00:49:52.140]   the largest issues and of pressing importance and worthy of regulation and slowing down
[00:49:52.140 --> 00:49:54.360]   and etc. etc. etc.
[00:49:54.360 --> 00:50:03.260]   A lot of those issues I think are, well frankly, overstated.
[00:50:03.260 --> 00:50:08.540]   What do you think then are the most important pressing issues?
[00:50:08.540 --> 00:50:17.660]   Yes, I think stuff like how these models can change public discourse is really concerning.
[00:50:17.660 --> 00:50:27.180]   What the consequences of synthetic media at a scale that we've just never encountered,
[00:50:27.180 --> 00:50:31.700]   what that'll have, what pressures that'll put on society.
[00:50:31.700 --> 00:50:37.220]   I think those issues are much more near term and plausibly implementable with the technology
[00:50:37.220 --> 00:50:42.300]   of today or maybe the next couple years.
[00:50:42.300 --> 00:50:48.940]   And so that's really where public attention should be.
[00:50:48.940 --> 00:50:54.180]   And I don't see, I see very light discussion about that.
[00:50:54.180 --> 00:51:00.260]   There's a lot of like, what happens if the paperclip maximizer turns us all into iron
[00:51:00.260 --> 00:51:03.340]   to generate more paperclips?
[00:51:03.340 --> 00:51:09.260]   And obviously that's an exaggeration, unfair characterization, but a lot of the discourse
[00:51:09.260 --> 00:51:18.820]   has that flavor and it's very far future and disconnected from the present reality of the
[00:51:18.820 --> 00:51:24.180]   technology and the near term reality of the technology.
[00:51:24.180 --> 00:51:31.420]   And so I would be really excited to see social media, like us as a public, putting pressure
[00:51:31.420 --> 00:51:37.180]   on social media to implement verification of who's posting.
[00:51:37.180 --> 00:51:41.660]   How do I know this is a bot or not?
[00:51:41.660 --> 00:51:42.940]   I really want that filter.
[00:51:42.940 --> 00:51:48.300]   I want to be able to filter for I'm hearing from humans, reading the opinions of humans,
[00:51:48.300 --> 00:51:53.580]   not a language model.
[00:51:53.580 --> 00:51:55.820]   But that doesn't seem to be happening.
[00:51:55.820 --> 00:52:04.860]   That conversation seems to be quite niche or restricted to very small communities.
[00:52:04.860 --> 00:52:10.580]   I think the broader public needs to start demanding that as a feature.
[00:52:10.580 --> 00:52:18.380]   Because I think there will be a flood of content, synthetic content.
[00:52:18.380 --> 00:52:26.100]   It's funny though, I would have expected a flood of synthetic content already at this
[00:52:26.100 --> 00:52:27.100]   point.
[00:52:27.100 --> 00:52:32.620]   Like the quality seems very high to me of synthetic content and there's lots of ways
[00:52:32.620 --> 00:52:34.740]   to do it and it's pretty cheap.
[00:52:34.740 --> 00:52:41.020]   Yeah, I think it's an awareness thing.
[00:52:41.020 --> 00:52:49.940]   I think it might already be happening and it's such compelling text that it's hard to
[00:52:49.940 --> 00:52:53.340]   pick up.
[00:52:53.340 --> 00:52:58.100]   It might be difficult for you and I to appreciate when we click on a tweet that's popular and
[00:52:58.100 --> 00:53:04.660]   we read through some of those replies, the extent to which those are machine generated.
[00:53:04.660 --> 00:53:08.500]   You just intuitively believe I'm staring at a bunch of humans giving their thoughts and
[00:53:08.500 --> 00:53:10.460]   opinions on this tweet.
[00:53:10.460 --> 00:53:11.460]   You just trust that.
[00:53:11.460 --> 00:53:15.220]   That's just what social media is.
[00:53:15.220 --> 00:53:21.180]   Similarly, your emails, the flood of emails that you're getting.
[00:53:21.180 --> 00:53:25.300]   You read emails written by someone that you think is trying to market to you and they're
[00:53:25.300 --> 00:53:30.500]   speaking very eloquently and fluently.
[00:53:30.500 --> 00:53:35.460]   Your spam catcher didn't flag this because it's very compelling and it seems targeted
[00:53:35.460 --> 00:53:37.620]   specifically to you.
[00:53:37.620 --> 00:53:42.140]   It's been observing all the emails coming into Gmail servers and no, this one isn't
[00:53:42.140 --> 00:53:43.460]   just a template of another one.
[00:53:43.460 --> 00:53:48.260]   It's specifically written to you so it must be human.
[00:53:48.260 --> 00:53:49.260]   But it's not.
[00:53:49.260 --> 00:53:59.620]   I think it's very easy for this to slip past because of exactly the reason you're describing,
[00:53:59.620 --> 00:54:03.140]   which is these models are extremely fluent.
[00:54:03.140 --> 00:54:09.620]   They write very coherently and yet these models aren't people.
[00:54:09.620 --> 00:54:18.980]   If they're pressing some idea in response to some political tweet, we don't want that.
[00:54:18.980 --> 00:54:21.980]   We don't want synthetic amplification of some position.
[00:54:21.980 --> 00:54:29.980]   Do you have any other things that you think are going to change in the next year or two
[00:54:29.980 --> 00:54:36.300]   based on stuff that's already happened in the capability of these models?
[00:54:36.300 --> 00:54:44.340]   I'll say one example from my perspective is I can imagine a lot more chat-based interfaces
[00:54:44.340 --> 00:54:45.340]   into products.
[00:54:45.340 --> 00:54:48.380]   I think that actually is a nice interface when it actually works.
[00:54:48.380 --> 00:54:55.100]   I feel like I'm starting to see these incredibly evocative demos of using things with just
[00:54:55.100 --> 00:54:57.140]   a chat interface.
[00:54:57.140 --> 00:55:05.620]   I'm curious from your perspective, do you think our modes of interacting with computers
[00:55:05.620 --> 00:55:08.820]   is likely to significantly change?
[00:55:08.820 --> 00:55:10.620]   Definitely.
[00:55:10.620 --> 00:55:12.900]   Definitely.
[00:55:12.900 --> 00:55:21.620]   It's important to remember that the end of November when ChatGPT came out, that was like
[00:55:21.620 --> 00:55:30.140]   for most people who interacted with that product, that was the first time for most humans that
[00:55:30.140 --> 00:55:36.500]   they had a compelling conversation with silicon.
[00:55:36.500 --> 00:55:41.420]   Every other moment in their life, they'd only ever experienced that with people.
[00:55:41.420 --> 00:55:47.620]   I think for those of us who are in the field building these models, it can be like the
[00:55:47.620 --> 00:55:52.020]   frog in the pot where nothing is ever surprising.
[00:55:52.020 --> 00:55:56.420]   It's all one small step from the step behind.
[00:55:56.420 --> 00:56:02.460]   But for most people, that was the first time they had a conversation with a computer.
[00:56:02.460 --> 00:56:11.620]   It was the first time a human talked to a piece of silicon.
[00:56:11.620 --> 00:56:18.140]   I think it's important to remember how massive of a leap that is and also to think about
[00:56:18.140 --> 00:56:20.820]   what that unlocks.
[00:56:20.820 --> 00:56:26.220]   I think it's going to become much, much more common that the default way you interact with
[00:56:26.220 --> 00:56:32.020]   a product or a piece of technology is going to be through conversation.
[00:56:32.020 --> 00:56:36.900]   Instead of having to go through 10 layers of menus or whatever to find the thing that
[00:56:36.900 --> 00:56:43.420]   you want to do, you're just going to have a conversation with that agent and it has
[00:56:43.420 --> 00:56:50.700]   access to the ability to affect the change that you're asking it to do.
[00:56:50.700 --> 00:56:59.180]   It's just so much more convenient to talk to a product than it is to learn a complex
[00:56:59.180 --> 00:57:03.740]   GUI and onboard in that way.
[00:57:03.740 --> 00:57:11.060]   I think this unlock of dialogue as an interface onto the space of products, it's just totally
[00:57:11.060 --> 00:57:18.540]   a transformation in how we interact with the stuff we build, the systems we build.
[00:57:18.540 --> 00:57:22.860]   You haven't raised the massive funding rounds.
[00:57:22.860 --> 00:57:28.980]   You've raised a fair amount of money, but not at the scale of OpenAI or Anthropic.
[00:57:28.980 --> 00:57:31.500]   Does what they're doing make you nervous?
[00:57:31.500 --> 00:57:38.940]   Are you consciously trying to not enter an arms race of building the most compute-intensive
[00:57:38.940 --> 00:57:39.940]   model ever?
[00:57:39.940 --> 00:57:45.860]   Do you plan to enter that realm?
[00:57:45.860 --> 00:57:53.940]   What are your thoughts and plans there?
[00:57:53.940 --> 00:58:01.860]   We have raised a lot of money, not on the level of $10 billion.
[00:58:01.860 --> 00:58:13.100]   We also haven't gone to individuals or patrons to raise money.
[00:58:13.100 --> 00:58:19.260]   I haven't made friends with a bunch of billionaires and gotten them to write checks and to cohere.
[00:58:19.260 --> 00:58:28.660]   I think our prerogative has always been to build a company the right way and to raise
[00:58:28.660 --> 00:58:32.860]   money at healthy milestones when we've proved value creation.
[00:58:32.860 --> 00:58:38.460]   We can convince an institutional investor that we hit these milestones, we need this
[00:58:38.460 --> 00:58:43.900]   much money to hit this next milestone.
[00:58:43.900 --> 00:58:52.420]   We don't do those flashy big rounds from one strategic or one patron or one benefactor.
[00:58:52.420 --> 00:59:01.980]   I think principally because we're building a company in a different way.
[00:59:01.980 --> 00:59:08.860]   I think there's more independence afforded to you by doing that.
[00:59:08.860 --> 00:59:18.300]   If you're completely beholden to one entity or a small pool of entities, it can lead to
[00:59:18.300 --> 00:59:19.300]   problems.
[00:59:19.300 --> 00:59:26.060]   Yeah, it unlocks massive capital, but I think cohere is a proof point that you don't need
[00:59:26.060 --> 00:59:33.700]   $10 billion to build an extremely compelling, very smart model.
[00:59:33.700 --> 00:59:38.540]   I think we're a proof point that you don't need that if you're scrappy and capital efficient
[00:59:38.540 --> 00:59:43.100]   and you have a super motivated, talented team.
[00:59:43.100 --> 00:59:45.020]   But we also don't want to take those shortcuts.
[00:59:45.020 --> 00:59:53.540]   We don't want to sell out and basically give half of our company to one of the tech behemoths
[00:59:53.540 --> 00:59:56.940]   and become a subsidiary.
[00:59:56.940 --> 01:00:08.060]   We want to stay independent and we want to build a new company in a healthy, normal way.
[01:00:08.060 --> 01:00:13.660]   It's weird that we're an outlier because we raise capital like a normal, good, healthy
[01:00:13.660 --> 01:00:16.820]   business, but I guess we are.
[01:00:16.820 --> 01:00:24.380]   But I guess in a world where there are people doing that, I even think at Weights & Biases,
[01:00:24.380 --> 01:00:32.900]   we might run leaner and slower, but we react to a world where the space is growing fast
[01:00:32.900 --> 01:00:36.180]   and certainly we have also well-funded competitors.
[01:00:36.180 --> 01:00:40.500]   We want to make sure that we're...
[01:00:40.500 --> 01:00:44.860]   It feels to me like the space that Weights & Biases is in is probably winner take all
[01:00:44.860 --> 01:00:47.820]   or winner take most.
[01:00:47.820 --> 01:00:53.380]   Do you not believe that's the case with the space that you're in?
[01:00:53.380 --> 01:01:00.580]   Do you imagine a world where there's lots of different foundation models that do different
[01:01:00.580 --> 01:01:01.580]   things?
[01:01:01.580 --> 01:01:06.980]   Why would the world go in that direction?
[01:01:06.980 --> 01:01:14.660]   I sure hope that it's not a monopoly.
[01:01:14.660 --> 01:01:21.740]   I definitely don't believe that it can be.
[01:01:21.740 --> 01:01:23.660]   Our competition, I think they're great.
[01:01:23.660 --> 01:01:25.500]   They're super talented teams.
[01:01:25.500 --> 01:01:31.100]   They build great models, but they have their own flavor of doing it.
[01:01:31.100 --> 01:01:32.620]   They have their own way of doing it.
[01:01:32.620 --> 01:01:41.420]   They have their own concerns that they're optimizing for.
[01:01:41.420 --> 01:01:43.580]   I think our flavor is different, right?
[01:01:43.580 --> 01:01:44.900]   And that's healthy.
[01:01:44.900 --> 01:01:49.020]   And there should be a bunch of folks with a bunch of different takes building these
[01:01:49.020 --> 01:01:52.620]   models, putting them out there, and then the market should decide which one they want to
[01:01:52.620 --> 01:01:53.620]   adopt.
[01:01:53.620 --> 01:01:54.980]   Who do they feel is the best partner?
[01:01:54.980 --> 01:01:56.500]   Who do they trust the most?
[01:01:56.500 --> 01:02:02.380]   Who do they think is going to help them succeed?
[01:02:02.380 --> 01:02:09.300]   And I think for Cohere, we want to win that on our merits.
[01:02:09.300 --> 01:02:18.180]   And I think that the end state of this effort for these foundation model companies like
[01:02:18.180 --> 01:02:22.460]   ourselves, it's very unlikely that it's winner takes all.
[01:02:22.460 --> 01:02:26.620]   We're all kind of within a few months of each other.
[01:02:26.620 --> 01:02:33.900]   And so it feels very, very unlikely that it's winner takes all.
[01:02:33.900 --> 01:02:36.500]   And winner takes all is just bad for the market.
[01:02:36.500 --> 01:02:42.580]   It's just like it's a bad setup for the people who need to consume these models.
[01:02:42.580 --> 01:02:47.620]   And so I'm super optimistic that we'll have diversity and we'll have a handful of folks
[01:02:47.620 --> 01:02:52.500]   building and deploying these models.
[01:02:52.500 --> 01:02:59.900]   Do you think that customers will want to fine tune these large models going forward or will
[01:02:59.900 --> 01:03:04.200]   that approach go away?
[01:03:04.200 --> 01:03:07.820]   There are specific cases where the answer is yes.
[01:03:07.820 --> 01:03:20.060]   I think by and large, fine tuning is like a mature system feature.
[01:03:20.060 --> 01:03:24.220]   It's like you want to fine tune a model after you've exhausted all the other things that
[01:03:24.220 --> 01:03:28.900]   you can do to boost its performance.
[01:03:28.900 --> 01:03:34.300]   And so it's really only once a system has been deployed, optimized, optimized, optimized,
[01:03:34.300 --> 01:03:38.820]   and then eventually you land at the fact, okay, the only way for me to squeeze more
[01:03:38.820 --> 01:03:42.020]   performance out of this is by a fine tuning.
[01:03:42.020 --> 01:03:48.380]   We're probably still too early in the tech adoption curve to actually see strong demand
[01:03:48.380 --> 01:03:49.500]   for that.
[01:03:49.500 --> 01:03:55.900]   I think eventually it does arise in the interim.
[01:03:55.900 --> 01:04:02.660]   The focus for Cohere is just make the models so adaptable via prompting or by grounding
[01:04:02.660 --> 01:04:05.140]   or via these other methods.
[01:04:05.140 --> 01:04:09.460]   Make them as adaptable as possible without having to customize weights.
[01:04:09.460 --> 01:04:15.740]   So just give other levers for people to pull on to squeeze better performance out of the
[01:04:15.740 --> 01:04:16.740]   model.
[01:04:16.740 --> 01:04:20.740]   Okay, we're almost out of time, but before I let you go, can you tell me one thing that's
[01:04:20.740 --> 01:04:22.700]   been surprisingly hard?
[01:04:22.700 --> 01:04:27.300]   And so many things seem hard, but maybe something that people wouldn't necessarily know is hard
[01:04:27.300 --> 01:04:33.620]   about building these large models and building an API around them that customers actually
[01:04:33.620 --> 01:04:35.780]   use for mission critical stuff.
[01:04:35.780 --> 01:04:37.540]   Oh yeah, interesting.
[01:04:37.540 --> 01:04:39.700]   What's something surprising and hard?
[01:04:39.700 --> 01:04:44.060]   Because I feel like a lot of people know how hard it is to train the model itself.
[01:04:44.060 --> 01:04:47.220]   It's obviously thousands of accelerators.
[01:04:47.220 --> 01:04:49.980]   Trying to keep that thing up is really, really difficult.
[01:04:49.980 --> 01:04:53.300]   Sounds stressful too, you're spending so much money.
[01:04:53.300 --> 01:04:54.300]   Yeah.
[01:04:54.300 --> 01:04:58.700]   I like the, in the GPT-4 post, there was like the model babysitters.
[01:04:58.700 --> 01:04:59.700]   That's a real thing.
[01:04:59.700 --> 01:05:05.260]   Like that's like, we have people who just sit and watch models train so that we can
[01:05:05.260 --> 01:05:09.140]   bring them back up when they inevitably fail.
[01:05:09.140 --> 01:05:14.020]   Okay, something surprising about this whole thing.
[01:05:14.020 --> 01:05:15.020]   The importance of sensitivity.
[01:05:15.020 --> 01:05:18.220]   Tell me and I'll tell you if it surprised me.
[01:05:18.220 --> 01:05:24.940]   The importance and the sensitivity to data, I think was a real shock for me this year.
[01:05:24.940 --> 01:05:30.100]   Like as we really started scaling up our data collection efforts from humans as opposed
[01:05:30.100 --> 01:05:32.660]   to the web.
[01:05:32.660 --> 01:05:37.900]   Like the web, when we were collecting data there, the model is super robust to noise.
[01:05:37.900 --> 01:05:43.260]   You can kind of get away with just some like weak heuristics or, and really just want to
[01:05:43.260 --> 01:05:46.900]   throw as much as you can in there.
[01:05:46.900 --> 01:05:54.180]   As soon as we went into the human data collection phase, like one example or two examples, you
[01:05:54.180 --> 01:05:58.860]   only need to like mess up a couple of times and suddenly you've sent the model down a
[01:05:58.860 --> 01:06:01.460]   direction that you really don't want it to go.
[01:06:01.460 --> 01:06:03.860]   It's extremely, extremely sensitive.
[01:06:03.860 --> 01:06:08.940]   If you teach it something that's wrong, it will lock that in and just forever believe
[01:06:08.940 --> 01:06:10.420]   that wrong thing.
[01:06:10.420 --> 01:06:11.420]   And so.
[01:06:11.420 --> 01:06:12.420]   That is surprising.
[01:06:12.420 --> 01:06:13.420]   That's very surprising.
[01:06:13.420 --> 01:06:14.420]   I, I.
[01:06:14.420 --> 01:06:15.420]   Yeah.
[01:06:15.420 --> 01:06:16.420]   Yeah.
[01:06:16.420 --> 01:06:24.660]   So like the, the sensitivity there, I just, I was not prepared for how delicate these
[01:06:24.660 --> 01:06:26.300]   models are.
[01:06:26.300 --> 01:06:27.540]   Okay.
[01:06:27.540 --> 01:06:30.340]   Actually, final question.
[01:06:30.340 --> 01:06:35.100]   Is there some other topic that you're interested in that if you had some extra time, you would
[01:06:35.100 --> 01:06:41.900]   look into some topic around machine learning that you wish you had more time to explore?
[01:06:41.900 --> 01:06:42.900]   Around machine learning.
[01:06:42.900 --> 01:06:48.020]   I would probably be in robotics and embodiment.
[01:06:48.020 --> 01:06:49.620]   I just think that's so cool.
[01:06:49.620 --> 01:06:52.940]   And it's like, there is such a strong consumer demand for that.
[01:06:52.940 --> 01:07:00.940]   Like we all know what that, the imaginary success of extremely intelligent brain plus
[01:07:00.940 --> 01:07:05.500]   extremely capable body.
[01:07:05.500 --> 01:07:10.820]   We know what that would be like and how transformational that would be to have in our lives to have
[01:07:10.820 --> 01:07:14.420]   access to.
[01:07:14.420 --> 01:07:17.380]   And it feels like we're really far off.
[01:07:17.380 --> 01:07:23.180]   So I would love to effect change there and help build that.
[01:07:23.180 --> 01:07:24.740]   Yeah.
[01:07:24.740 --> 01:07:25.740]   Robotics is super sick.
[01:07:25.740 --> 01:07:26.740]   Totally agree.
[01:07:26.740 --> 01:07:27.740]   Love it.
[01:07:27.740 --> 01:07:28.740]   Thank you.
[01:07:28.740 --> 01:07:29.740]   Thanks.
[01:07:29.740 --> 01:07:30.740]   That was really fun.
[01:07:30.740 --> 01:07:31.740]   Yeah.
[01:07:31.740 --> 01:07:32.740]   Thank you so much.
[01:07:32.740 --> 01:07:33.740]   Yeah, that was great.
[01:07:33.740 --> 01:07:37.780]   If you're enjoying these interviews and you want to learn more, please click on the link
[01:07:37.780 --> 01:07:42.500]   to the show notes in the description where you can find links to all the papers that
[01:07:42.500 --> 01:07:46.660]   are mentioned, supplemental material, and a transcription that we worked really hard
[01:07:46.660 --> 01:07:47.660]   to produce.
[01:07:47.660 --> 01:07:48.160]   So check it out.
[01:07:48.160 --> 01:07:50.740]   (upbeat music)
[01:07:50.740 --> 01:07:52.800]   you

