
[00:00:00.000 --> 00:00:03.640]   I'm going to introduce our first speaker, Peter Wellender,
[00:00:03.640 --> 00:00:08.240]   who is my co-worker on the robotics team at OpenAI,
[00:00:08.240 --> 00:00:12.000]   where he's kind of pushed a lot of our vision work forward
[00:00:12.000 --> 00:00:14.880]   and has generally been involved in pretty much every part
[00:00:14.880 --> 00:00:17.760]   of the results that we've had there.
[00:00:17.760 --> 00:00:21.400]   Peter joined OpenAI from Dropbox,
[00:00:21.400 --> 00:00:23.880]   which he joined after selling his startup to Dropbox
[00:00:23.880 --> 00:00:26.520]   and went on to lead machine learning there.
[00:00:26.520 --> 00:00:28.240]   So really excited to welcome Peter.
[00:00:28.240 --> 00:00:29.200]   Thank you.
[00:00:29.200 --> 00:00:32.760]   Thank you, Josh, and thanks for having me.
[00:00:32.760 --> 00:00:33.280]   Cool.
[00:00:33.280 --> 00:00:40.760]   Yeah, so Josh asked me to talk about more tooling and stuff
[00:00:40.760 --> 00:00:41.280]   like that.
[00:00:41.280 --> 00:00:44.280]   And instead, I decided to talk about something
[00:00:44.280 --> 00:00:46.720]   slightly different, which is, how do you actually
[00:00:46.720 --> 00:00:51.000]   make progress on hard research problems?
[00:00:51.000 --> 00:00:54.320]   Because I think this is something
[00:00:54.320 --> 00:00:58.040]   that, if you're working on AI, this
[00:00:58.040 --> 00:01:02.000]   is something you will all have to face, because it's
[00:01:02.000 --> 00:01:06.520]   one of those things where most of AI stuff
[00:01:06.520 --> 00:01:08.280]   is kind of at the forefront.
[00:01:08.280 --> 00:01:10.440]   You need to put together a lot of pieces that might not
[00:01:10.440 --> 00:01:11.900]   be completely working, and you need
[00:01:11.900 --> 00:01:14.760]   to do a lot of experiments to actually get things working
[00:01:14.760 --> 00:01:15.680]   together.
[00:01:15.680 --> 00:01:21.440]   And in some ways, it's similar to engineering.
[00:01:21.440 --> 00:01:23.080]   In some ways, it's not.
[00:01:23.080 --> 00:01:25.040]   And if you're-- more often than not,
[00:01:25.040 --> 00:01:27.040]   you're going to be working with a team of people
[00:01:27.040 --> 00:01:28.480]   to accomplish things.
[00:01:28.480 --> 00:01:30.800]   And so how to think about this, how
[00:01:30.800 --> 00:01:35.680]   do you make progress as a team to solve really hard machine
[00:01:35.680 --> 00:01:39.200]   learning on the edge of research problems?
[00:01:39.200 --> 00:01:44.520]   There are some things that I've learned throughout my career
[00:01:44.520 --> 00:01:45.680]   that are useful for this.
[00:01:45.680 --> 00:01:49.120]   So as Josh said, just to give you a little bit of context,
[00:01:49.120 --> 00:01:51.880]   my background is--
[00:01:51.880 --> 00:01:54.000]   10 years ago, I was in grad school,
[00:01:54.000 --> 00:01:55.240]   so I was doing research there.
[00:01:55.240 --> 00:01:58.560]   And then I did a startup for a while,
[00:01:58.560 --> 00:02:03.120]   which is very different from doing grad school research.
[00:02:03.120 --> 00:02:05.080]   Then at Dropbox, again, very different,
[00:02:05.080 --> 00:02:06.760]   where suddenly from a startup, you
[00:02:06.760 --> 00:02:08.760]   have very little time and very little resources.
[00:02:08.760 --> 00:02:10.840]   You have much more resources, and you can actually
[00:02:10.840 --> 00:02:13.320]   try to solve much harder research problems.
[00:02:13.320 --> 00:02:16.240]   And now at OpenAI, where it's like we're really
[00:02:16.240 --> 00:02:18.200]   trying to solve things that nobody has ever
[00:02:18.200 --> 00:02:21.360]   done before, which is really--
[00:02:21.360 --> 00:02:24.000]   which we have no idea whether we can even solve.
[00:02:24.000 --> 00:02:27.280]   And the interesting thing is that there's
[00:02:27.280 --> 00:02:31.560]   a lot of similarities between these things
[00:02:31.560 --> 00:02:36.120]   and some common patterns of how you can actually make progress
[00:02:36.120 --> 00:02:37.200]   on hard problems.
[00:02:37.200 --> 00:02:40.400]   So that's what I will talk about here.
[00:02:40.400 --> 00:02:43.360]   And if you have questions along the way,
[00:02:43.360 --> 00:02:45.360]   you should just interject, because I want
[00:02:45.360 --> 00:02:48.560]   this to be somewhat interactive.
[00:02:48.560 --> 00:02:53.960]   So before I talk about those general ideas,
[00:02:53.960 --> 00:02:56.480]   I'll give you a bit more context on what I'm working on right
[00:02:56.480 --> 00:02:57.600]   now.
[00:02:57.600 --> 00:03:00.680]   At OpenAI, as Josh said, I'm working on robotics.
[00:03:00.680 --> 00:03:03.080]   We're trying to build a general purpose robot.
[00:03:03.080 --> 00:03:06.440]   What that means is a robot that can do most things
[00:03:06.440 --> 00:03:08.080]   that humans can do.
[00:03:08.080 --> 00:03:09.720]   So that would mean things like--
[00:03:09.720 --> 00:03:19.520]   things like cracking eggs or building things,
[00:03:19.520 --> 00:03:21.760]   putting together furniture.
[00:03:21.760 --> 00:03:24.640]   These are all things that robots can't do today.
[00:03:24.640 --> 00:03:27.000]   But we feel like we're not so far away
[00:03:27.000 --> 00:03:28.040]   from making that happen.
[00:03:28.040 --> 00:03:30.280]   And the key thing that we're missing
[00:03:30.280 --> 00:03:33.880]   is being able to learn how to do these things,
[00:03:33.880 --> 00:03:36.880]   like having the robot actually learn how to do these things.
[00:03:36.880 --> 00:03:38.880]   Today, robots are pretty much pre-programmed
[00:03:38.880 --> 00:03:40.080]   to do most things.
[00:03:40.080 --> 00:03:44.160]   And the big missing component is learning.
[00:03:44.160 --> 00:03:49.000]   The ingredients that we are using at OpenAI is--
[00:03:49.000 --> 00:03:51.960]   there are really three big ones.
[00:03:51.960 --> 00:03:56.080]   And the obvious one is deep learning.
[00:03:56.080 --> 00:03:58.920]   It's just an extremely effective tool
[00:03:58.920 --> 00:04:00.760]   to solve most machine learning problems today.
[00:04:00.760 --> 00:04:03.160]   That wasn't at all the case 10 years ago.
[00:04:03.160 --> 00:04:05.480]   But now it seems to work for almost anything
[00:04:05.480 --> 00:04:07.480]   you throw at it, as long as you have enough data
[00:04:07.480 --> 00:04:09.040]   and enough compute.
[00:04:09.040 --> 00:04:11.600]   The other thing that's-- and this is so successful
[00:04:11.600 --> 00:04:13.640]   that it's obviously used in businesses and industry
[00:04:13.640 --> 00:04:14.200]   and so on.
[00:04:14.200 --> 00:04:17.000]   The thing that's not very used in business settings
[00:04:17.000 --> 00:04:18.280]   yet is reinforcement learning.
[00:04:18.280 --> 00:04:25.000]   So what you see here is this is the OpenAI Dota 2 bot.
[00:04:25.000 --> 00:04:27.920]   I think this is a one versus one setting where it's--
[00:04:27.920 --> 00:04:31.200]   so it's actually the computer who's
[00:04:31.200 --> 00:04:33.120]   making some of the choices here, I think.
[00:04:33.120 --> 00:04:35.160]   And what we use here is reinforcement learning.
[00:04:35.160 --> 00:04:37.960]   How many here know about reinforcement learning?
[00:04:37.960 --> 00:04:39.240]   I guess maybe half of you.
[00:04:39.240 --> 00:04:43.200]   So reinforcement learning is somewhat different
[00:04:43.200 --> 00:04:44.800]   than supervised learning in the sense
[00:04:44.800 --> 00:04:49.600]   that instead of getting both labels of the data,
[00:04:49.600 --> 00:04:51.560]   you're also getting--
[00:04:51.560 --> 00:04:54.320]   instead of those labels, you're getting a reward signal.
[00:04:54.320 --> 00:04:56.040]   So you have-- usually the setting
[00:04:56.040 --> 00:04:59.880]   is you have some agent moving around in some environment.
[00:04:59.880 --> 00:05:01.080]   It does some actions.
[00:05:01.080 --> 00:05:02.560]   It gets observations back.
[00:05:02.560 --> 00:05:04.240]   But it also gets some reward signal.
[00:05:04.240 --> 00:05:06.320]   And it turns out this is a really, really powerful
[00:05:06.320 --> 00:05:14.600]   paradigm for training more open-ended agents.
[00:05:14.600 --> 00:05:16.920]   And so you can train robotics.
[00:05:16.920 --> 00:05:18.360]   Robots do very interesting things.
[00:05:18.360 --> 00:05:20.800]   You can also train things in computer games.
[00:05:20.800 --> 00:05:22.000]   Last thing is scale.
[00:05:22.000 --> 00:05:26.440]   Scale is really what makes things work, especially
[00:05:26.440 --> 00:05:28.000]   for reinforcement learning.
[00:05:28.000 --> 00:05:33.400]   And at OpenAI, we try to be 5 to 10 years ahead
[00:05:33.400 --> 00:05:38.240]   of the rest of most of the world in terms
[00:05:38.240 --> 00:05:41.360]   of having compute available to do these things.
[00:05:41.360 --> 00:05:45.000]   And the idea is that in five years, most of the world
[00:05:45.000 --> 00:05:45.680]   will be there.
[00:05:45.680 --> 00:05:47.380]   So then these things will be very useful.
[00:05:47.380 --> 00:05:51.400]   And it also lets you be more honest in the kind
[00:05:51.400 --> 00:05:53.760]   of experiments you're running because you
[00:05:53.760 --> 00:05:58.040]   can use simple tricks and get pretty far.
[00:05:58.040 --> 00:06:01.680]   So these three ingredients are the primary ones
[00:06:01.680 --> 00:06:03.320]   we're using for a lot of our research.
[00:06:03.320 --> 00:06:07.400]   What does compute five years ahead look like?
[00:06:07.400 --> 00:06:09.160]   I mean, I think the way to think about it
[00:06:09.160 --> 00:06:14.640]   is if you're training models right now, a lot of times,
[00:06:14.640 --> 00:06:17.120]   say, a computer vision model, deep learning model,
[00:06:17.120 --> 00:06:21.280]   you might be using two GPUs, sometimes four.
[00:06:21.280 --> 00:06:26.240]   And we would use 400 or a few thousand.
[00:06:26.240 --> 00:06:31.520]   So it's really just two or three orders of magnitude
[00:06:31.520 --> 00:06:34.040]   more than you would have in most setups.
[00:06:34.040 --> 00:06:38.040]   Yes?
[00:06:38.040 --> 00:06:42.760]   Do you think in five to 10 years, two to four GPUs
[00:06:42.760 --> 00:06:46.760]   might be equivalent in performance to 500 now?
[00:06:46.760 --> 00:06:48.800]   Is that the thinking?
[00:06:48.800 --> 00:06:49.300]   Yes.
[00:06:49.300 --> 00:06:52.560]   Or our ability to distribute would be that much more?
[00:06:52.560 --> 00:06:54.040]   I think it's a little bit of both.
[00:06:54.040 --> 00:06:56.000]   I think it will be easier for everyone
[00:06:56.000 --> 00:06:57.720]   to have more distributed jobs.
[00:06:57.720 --> 00:06:59.600]   It will be cheaper.
[00:06:59.600 --> 00:07:03.040]   So this is a pretty big research topic,
[00:07:03.040 --> 00:07:05.560]   even this one at OpenAI, trying to figure out
[00:07:05.560 --> 00:07:07.960]   where will we be in terms of compute in a couple of years
[00:07:07.960 --> 00:07:10.600]   because we have a really good blog post about this.
[00:07:10.600 --> 00:07:14.760]   And what it looks like right now is
[00:07:14.760 --> 00:07:19.480]   that compared to Moore's law, which is compute doubling
[00:07:19.480 --> 00:07:24.160]   every 18 months, right now for deep learning applications,
[00:07:24.160 --> 00:07:28.280]   for solving hard problems, it seems to be every four months.
[00:07:28.280 --> 00:07:31.640]   So that means 10x approximately every year.
[00:07:31.640 --> 00:07:33.960]   And it looks like we will have that for the next three
[00:07:33.960 --> 00:07:36.680]   to five years.
[00:07:36.680 --> 00:07:43.240]   And so the only thing that's making it hard to use now
[00:07:43.240 --> 00:07:45.520]   is it costs a lot of money, and the infrastructure is not
[00:07:45.520 --> 00:07:46.120]   there.
[00:07:46.120 --> 00:07:48.280]   But prices will go down, and infrastructure
[00:07:48.280 --> 00:07:49.720]   is easy to eventually copy.
[00:07:49.720 --> 00:07:51.400]   So it will happen.
[00:07:51.400 --> 00:07:57.200]   Here's an example of what we can do now.
[00:07:57.200 --> 00:07:57.700]   Let's see.
[00:08:02.400 --> 00:08:08.280]   So this is our robot.
[00:08:08.280 --> 00:08:10.560]   This is a result from last year.
[00:08:10.560 --> 00:08:12.920]   And what is happening here is that the robot
[00:08:12.920 --> 00:08:18.080]   is trying to reorient this cube into different orientations.
[00:08:18.080 --> 00:08:21.000]   The goal orientations is here on the lower right.
[00:08:21.000 --> 00:08:24.200]   So whenever it achieves a particular goal,
[00:08:24.200 --> 00:08:26.400]   we sample randomly a new goal.
[00:08:26.400 --> 00:08:29.960]   And so this is an example of using reinforcement learning
[00:08:29.960 --> 00:08:33.280]   to try to solve a very hard problem in terms
[00:08:33.280 --> 00:08:39.240]   of robotic manipulation, trying to control a robot with a lot
[00:08:39.240 --> 00:08:40.480]   of degrees of freedoms here.
[00:08:40.480 --> 00:08:42.760]   There's lots of joints in the fingers and so on.
[00:08:42.760 --> 00:08:44.400]   It's a really, really hard problem.
[00:08:44.400 --> 00:08:48.440]   And so reinforcement learning is a good way of approaching it.
[00:08:48.440 --> 00:08:48.940]   Yes?
[00:08:48.940 --> 00:08:49.920]   Just a quick thought.
[00:08:49.920 --> 00:08:51.920]   What if you give it an impossible configuration?
[00:08:51.920 --> 00:08:53.480]   Does it realize it's-- eventually it
[00:08:53.480 --> 00:08:54.940]   will realize it's impossible, or it
[00:08:54.940 --> 00:08:57.960]   will be just going through?
[00:08:57.960 --> 00:09:00.600]   Well, I guess here it's really hard
[00:09:00.600 --> 00:09:03.320]   to give it an impossible configuration just
[00:09:03.320 --> 00:09:05.960]   because you would--
[00:09:05.960 --> 00:09:07.880]   it's only rotating this cube in any--
[00:09:07.880 --> 00:09:10.560]   You could just put the letters such
[00:09:10.560 --> 00:09:13.320]   that the configuration doesn't exist on this cube.
[00:09:13.320 --> 00:09:14.280]   Oh, I see.
[00:09:14.280 --> 00:09:15.080]   Yeah, in that case.
[00:09:15.080 --> 00:09:18.440]   I mean, it's not that general.
[00:09:18.440 --> 00:09:21.200]   For a lot of these things, this is the hardest problem
[00:09:21.200 --> 00:09:23.320]   right now in machine learning, is making things
[00:09:23.320 --> 00:09:24.600]   really, really general.
[00:09:24.600 --> 00:09:29.080]   Right now, we're trying to get the not very general,
[00:09:29.080 --> 00:09:31.400]   things that are easy for humans to work.
[00:09:31.400 --> 00:09:32.800]   We can't even get those to work.
[00:09:32.800 --> 00:09:34.840]   But the next thing would be what you said.
[00:09:34.840 --> 00:09:37.280]   That would be really cool.
[00:09:37.280 --> 00:09:40.600]   What's the level of generalization
[00:09:40.600 --> 00:09:42.080]   that this robot has?
[00:09:42.080 --> 00:09:43.960]   So the level of generalization is
[00:09:43.960 --> 00:09:47.360]   you could probably put in slightly bigger or smaller
[00:09:47.360 --> 00:09:47.860]   cubes.
[00:09:47.860 --> 00:09:52.440]   We put in blocks that are squishy, like stress balls
[00:09:52.440 --> 00:09:54.680]   and stuff like that.
[00:09:54.680 --> 00:09:59.520]   Different shapes of objects, heavier, lighter,
[00:09:59.520 --> 00:10:01.320]   and stuff like that, it can deal with.
[00:10:01.320 --> 00:10:05.080]   Also, if the hand here is slightly broken,
[00:10:05.080 --> 00:10:07.000]   it will still work and so on.
[00:10:07.000 --> 00:10:08.920]   So there is some generality to it.
[00:10:08.920 --> 00:10:13.920]   But nothing at all compared to human.
[00:10:18.400 --> 00:10:23.360]   So the image part of it is irrelevant for this purpose.
[00:10:23.360 --> 00:10:24.320]   Is that right?
[00:10:24.320 --> 00:10:26.800]   So different colors or--
[00:10:26.800 --> 00:10:28.800]   That is very specific.
[00:10:28.800 --> 00:10:31.680]   There's no generality really for that.
[00:10:31.680 --> 00:10:33.080]   Here, we're training it.
[00:10:33.080 --> 00:10:35.280]   I mean, maybe we can make it a slightly different hue
[00:10:35.280 --> 00:10:37.760]   of yellow or something like that.
[00:10:37.760 --> 00:10:42.640]   But the way to think about it is this
[00:10:42.640 --> 00:10:44.440]   is showing that it's at all possible to do
[00:10:44.440 --> 00:10:45.640]   something like this.
[00:10:45.640 --> 00:10:47.720]   Very much what our research director at right now
[00:10:47.720 --> 00:10:50.000]   is actually to try to make it much more general,
[00:10:50.000 --> 00:10:52.400]   like putting in any object in that hand
[00:10:52.400 --> 00:10:55.920]   or eventually using any robotic hand and so on.
[00:10:55.920 --> 00:10:58.400]   That's where we want to get to.
[00:10:58.400 --> 00:11:01.520]   But right now, that's not at all the case of the setup.
[00:11:01.520 --> 00:11:02.020]   [INAUDIBLE]
[00:11:02.020 --> 00:11:16.180]   Maybe.
[00:11:17.180 --> 00:11:20.940]   I think in this case, it might be using the letters quite a
[00:11:20.940 --> 00:11:21.440]   lot.
[00:11:21.440 --> 00:11:23.460]   We have trained a model with those letters.
[00:11:23.460 --> 00:11:27.420]   And as you might know, these neural networks
[00:11:27.420 --> 00:11:29.980]   are really locked onto the data distribution.
[00:11:29.980 --> 00:11:32.260]   And they're not good at generalizing.
[00:11:32.260 --> 00:11:36.780]   Sometimes they can, but often they cannot.
[00:11:36.780 --> 00:11:40.260]   And are these models fine-tunable,
[00:11:40.260 --> 00:11:42.740]   like vision and language models?
[00:11:42.740 --> 00:11:44.780]   These are actually trained from scratch.
[00:11:44.780 --> 00:11:46.780]   So given that it learns this now,
[00:11:46.780 --> 00:11:48.780]   how difficult would it be to, say,
[00:11:48.780 --> 00:11:51.500]   train it to open a model cap on top of this?
[00:11:51.500 --> 00:11:53.300]   Right now, very hard.
[00:11:53.300 --> 00:11:55.820]   So it's as if you had to do that whole task again.
[00:11:55.820 --> 00:11:56.660]   More or less, yeah.
[00:11:56.660 --> 00:11:57.860]   [INAUDIBLE]
[00:11:57.860 --> 00:11:58.500]   Exactly.
[00:11:58.500 --> 00:12:01.300]   And that's where we want to get to is things that are much more
[00:12:01.300 --> 00:12:02.660]   general like that.
[00:12:02.660 --> 00:12:03.460]   You had a question?
[00:12:03.460 --> 00:12:07.620]   Is the choice of the humanoid form of the hand intentional?
[00:12:07.620 --> 00:12:11.860]   Yeah, well, so the story behind that is pretty interesting.
[00:12:11.860 --> 00:12:15.780]   We went to a number of robotics conferences
[00:12:15.780 --> 00:12:17.540]   when we started working on this project.
[00:12:17.540 --> 00:12:19.140]   A lot of us actually did not have
[00:12:19.140 --> 00:12:20.700]   a lot of experience in robotics.
[00:12:20.700 --> 00:12:22.700]   We had much more experience in machine learning
[00:12:22.700 --> 00:12:24.660]   and deep learning and reinforcement learning
[00:12:24.660 --> 00:12:25.700]   and stuff like that.
[00:12:25.700 --> 00:12:27.360]   So we asked people at these conferences,
[00:12:27.360 --> 00:12:29.180]   what is the hardest problem in robotics?
[00:12:29.180 --> 00:12:31.340]   And everybody said, well, the problem I'm working on
[00:12:31.340 --> 00:12:32.620]   is really, really hard.
[00:12:32.620 --> 00:12:35.580]   So that wasn't super helpful.
[00:12:35.580 --> 00:12:37.980]   But if you kept on pushing, what came up
[00:12:37.980 --> 00:12:41.540]   was robots that have a lot of degrees of freedom.
[00:12:41.540 --> 00:12:44.700]   That is, a lot of joints are really hard to control.
[00:12:44.700 --> 00:12:47.260]   The other thing that's really hard
[00:12:47.260 --> 00:12:48.780]   is interacting with objects.
[00:12:48.780 --> 00:12:50.860]   Because a lot of the way robotics is done today
[00:12:50.860 --> 00:12:54.060]   is you need to have really accurate models of everything.
[00:12:54.060 --> 00:12:56.180]   And so it's really hard to have good models
[00:12:56.180 --> 00:12:58.700]   of all kinds of objects.
[00:12:58.700 --> 00:13:03.260]   So we picked the robotic hand because it
[00:13:03.260 --> 00:13:07.580]   was one of the robots that had the most degrees of freedom.
[00:13:07.580 --> 00:13:10.980]   And obviously, there was some foresight here also.
[00:13:10.980 --> 00:13:13.340]   If we can control that, then presumably,
[00:13:13.340 --> 00:13:16.220]   there's a lot of interesting tasks
[00:13:16.220 --> 00:13:18.500]   that a human can do that we can tackle using this system.
[00:13:18.500 --> 00:13:21.180]   Because we can do it with our hands.
[00:13:21.180 --> 00:13:24.780]   We should be able to do it with a robotic hand.
[00:13:24.780 --> 00:13:28.220]   Is that a reinforcement learning problem as well?
[00:13:28.220 --> 00:13:30.700]   Where what's the best design?
[00:13:30.700 --> 00:13:33.180]   Genetically, we thought it would be this design.
[00:13:33.180 --> 00:13:33.680]   Yeah.
[00:13:33.680 --> 00:13:35.580]   But it could be a better design.
[00:13:35.580 --> 00:13:36.580]   Yeah.
[00:13:36.580 --> 00:13:38.500]   I think you're right.
[00:13:38.500 --> 00:13:40.140]   You could do that as well.
[00:13:40.140 --> 00:13:41.620]   What is interesting here is sometimes
[00:13:41.620 --> 00:13:46.380]   you see that it doesn't use this finger very much, for example.
[00:13:46.380 --> 00:13:48.720]   So it could have gotten away with four fingers instead,
[00:13:48.720 --> 00:13:50.380]   probably.
[00:13:50.380 --> 00:13:52.620]   The interesting thing is that this hand is somewhat
[00:13:52.620 --> 00:13:55.860]   different from a human hand.
[00:13:55.860 --> 00:14:00.300]   And it does some moves that are different than a human would
[00:14:00.300 --> 00:14:00.900]   do.
[00:14:00.900 --> 00:14:03.500]   For example, it has a much more flexible thumb.
[00:14:03.500 --> 00:14:06.180]   It can do really weird movements.
[00:14:06.180 --> 00:14:09.020]   It hurts in your hand when you look at it a little bit.
[00:14:10.020 --> 00:14:11.540]   But it's been really interesting.
[00:14:11.540 --> 00:14:14.100]   Because again, all of this is actually trained from scratch.
[00:14:14.100 --> 00:14:16.660]   We have not told it how to do these movements at all.
[00:14:16.660 --> 00:14:18.180]   And some of them are very human-like,
[00:14:18.180 --> 00:14:22.020]   just because you've got a human-like hand.
[00:14:22.020 --> 00:14:23.020]   Yes?
[00:14:23.020 --> 00:14:26.020]   Do you train this in the physical world
[00:14:26.020 --> 00:14:27.500]   or in a simulation?
[00:14:27.500 --> 00:14:29.700]   Entirely in a simulation.
[00:14:29.700 --> 00:14:32.020]   And this is the first time it's running on a real robot.
[00:14:32.020 --> 00:14:33.020]   Oh, wow.
[00:14:33.020 --> 00:14:38.020]   So the simulation is-- it has to be perfectly matching
[00:14:38.020 --> 00:14:40.340]   the design.
[00:14:40.340 --> 00:14:42.300]   So this is a very good question.
[00:14:42.300 --> 00:14:44.500]   And the trick is actually-- and this was something
[00:14:44.500 --> 00:14:49.700]   that Josh actually came out of his research is,
[00:14:49.700 --> 00:14:52.140]   the way you train this is you train it in simulations,
[00:14:52.140 --> 00:14:55.060]   lots of simulations, that all have
[00:14:55.060 --> 00:14:56.940]   different parameters of physics.
[00:14:56.940 --> 00:14:59.060]   Like some would have really strong gravity.
[00:14:59.060 --> 00:15:00.860]   Some would have light gravity.
[00:15:00.860 --> 00:15:04.140]   Some would have a very heavy object in the hand.
[00:15:04.140 --> 00:15:07.060]   And some will have a very light object in the hand.
[00:15:07.060 --> 00:15:10.900]   And if you do that, if you train in enough diverse environments,
[00:15:10.900 --> 00:15:13.300]   it turns out that you can transfer to the real world.
[00:15:13.300 --> 00:15:17.200]   Cool.
[00:15:17.200 --> 00:15:21.300]   So anyway, this is what we have done so far.
[00:15:21.300 --> 00:15:25.100]   And so I wanted to think about then
[00:15:25.100 --> 00:15:27.740]   how do you deliver really good research results,
[00:15:27.740 --> 00:15:30.500]   and specifically how you do it as a team.
[00:15:30.500 --> 00:15:32.260]   Because I think this is really important,
[00:15:32.260 --> 00:15:35.420]   because it's really hard to--
[00:15:35.420 --> 00:15:37.220]   especially in AI research today--
[00:15:37.220 --> 00:15:43.940]   to make breakthroughs or even put things together and get
[00:15:43.940 --> 00:15:45.220]   them to work as a single person.
[00:15:45.220 --> 00:15:48.460]   You really need to think about it as a team,
[00:15:48.460 --> 00:15:50.540]   because the problems are just so hard, and there's
[00:15:50.540 --> 00:15:52.620]   so many components.
[00:15:52.620 --> 00:15:55.380]   And so I was thinking of some things
[00:15:55.380 --> 00:15:59.660]   we've learned at OpenAI and at other projects
[00:15:59.660 --> 00:16:03.460]   I've been doing at Dropbox and before that.
[00:16:03.460 --> 00:16:06.140]   But to put it a little bit more in context,
[00:16:06.140 --> 00:16:08.740]   the projects we are doing at OpenAI,
[00:16:08.740 --> 00:16:12.020]   most projects are on the order of a year in length.
[00:16:12.020 --> 00:16:14.540]   And we have 5 to 15 people working on them.
[00:16:14.540 --> 00:16:16.120]   So they're pretty big projects, and we
[00:16:16.120 --> 00:16:20.100]   try to get to a really hard research--
[00:16:20.100 --> 00:16:22.820]   solve a really hard research problem during that time
[00:16:22.820 --> 00:16:25.900]   with such a team.
[00:16:25.900 --> 00:16:29.580]   And again, I feel like there's a lot
[00:16:29.580 --> 00:16:32.380]   of interesting parallels between research and engineering,
[00:16:32.380 --> 00:16:37.380]   but also some differences.
[00:16:37.380 --> 00:16:41.500]   So we really need to think about what
[00:16:41.500 --> 00:16:44.940]   can we learn from engineering and apply it to research
[00:16:44.940 --> 00:16:48.300]   to make progress on research, but what is also different
[00:16:48.300 --> 00:16:52.740]   so we can think about research in the appropriate way.
[00:16:52.740 --> 00:16:56.220]   So there will be--
[00:16:56.220 --> 00:16:58.540]   I will talk about five lessons, basically.
[00:16:58.540 --> 00:17:02.420]   And the first one is hypothesis over tasks,
[00:17:02.420 --> 00:17:04.460]   meaning you have to start thinking about--
[00:17:04.460 --> 00:17:05.920]   when you're doing engineering, you
[00:17:05.920 --> 00:17:09.660]   can think about a lot of things in tasks.
[00:17:09.660 --> 00:17:11.200]   But when you're doing research, you
[00:17:11.200 --> 00:17:13.420]   have to start thinking in hypothesis.
[00:17:13.420 --> 00:17:16.900]   To give you an example, a really good analogy, I think,
[00:17:16.900 --> 00:17:19.620]   for building things with engineering
[00:17:19.620 --> 00:17:22.940]   is you can think of it as building a block tower.
[00:17:22.940 --> 00:17:25.100]   For example, if you're building a camera app,
[00:17:25.100 --> 00:17:29.400]   you need to think about what are the components that
[00:17:29.400 --> 00:17:30.420]   make up my camera app.
[00:17:30.420 --> 00:17:32.180]   I need some camera UI.
[00:17:32.180 --> 00:17:33.860]   I need to be able to save photos.
[00:17:33.860 --> 00:17:36.660]   I need some server back end that can potentially sync these.
[00:17:36.660 --> 00:17:37.940]   I need to think about the APIs.
[00:17:37.940 --> 00:17:39.940]   And all these are different components.
[00:17:39.940 --> 00:17:44.020]   And you know that you will be able to build a camera app.
[00:17:44.020 --> 00:17:46.380]   And if you think about it hard enough,
[00:17:46.380 --> 00:17:48.340]   you can figure out what all the components are.
[00:17:48.340 --> 00:17:50.700]   So you know how to parallelize that work
[00:17:50.700 --> 00:17:54.940]   across multiple people, how to create the right interfaces
[00:17:54.940 --> 00:17:56.980]   and put them all together.
[00:17:56.980 --> 00:18:00.140]   And you can have some confidence in those.
[00:18:00.140 --> 00:18:02.220]   And you can usually estimate pretty well
[00:18:02.220 --> 00:18:05.140]   on how long it will actually take to build this thing,
[00:18:05.140 --> 00:18:08.580]   estimating how long engineering problems are hard.
[00:18:08.580 --> 00:18:11.460]   But you can often do it within a factor of two.
[00:18:11.460 --> 00:18:14.140]   With research, it's much harder.
[00:18:14.140 --> 00:18:16.940]   I think this comic from XKCD really captures this really
[00:18:16.940 --> 00:18:17.420]   well.
[00:18:17.420 --> 00:18:19.420]   I'm sure a lot of you have seen this.
[00:18:19.420 --> 00:18:20.220]   But let me read it.
[00:18:20.220 --> 00:18:23.180]   It's weird to read a comic, but I will do it anyway.
[00:18:23.180 --> 00:18:24.780]   When a user takes a photo, the app
[00:18:24.780 --> 00:18:26.900]   should show whether in a natural park.
[00:18:26.900 --> 00:18:30.700]   Sure, give me a GIS to look up and give me a few hours.
[00:18:30.700 --> 00:18:33.540]   And then it should check whether the photo has a bird.
[00:18:33.540 --> 00:18:36.340]   Well, for that, I need a research team of five years.
[00:18:36.340 --> 00:18:40.220]   And this is really the core issue.
[00:18:40.220 --> 00:18:41.620]   Because if you're doing research,
[00:18:41.620 --> 00:18:44.300]   you're really operating in this realm
[00:18:44.300 --> 00:18:46.700]   of the virtually impossible.
[00:18:46.700 --> 00:18:48.900]   And you're really trying to push the limits.
[00:18:48.900 --> 00:18:51.580]   And you don't know how long things will take.
[00:18:51.580 --> 00:18:55.460]   So you kind of-- and there's no way to get around this.
[00:18:55.460 --> 00:18:57.580]   You kind of need to embrace it and embrace
[00:18:57.580 --> 00:19:00.380]   that for your working style in order to be productive.
[00:19:00.380 --> 00:19:02.820]   So going back to the block analogy,
[00:19:02.820 --> 00:19:04.300]   it just doesn't work anymore.
[00:19:04.300 --> 00:19:06.860]   You don't know all the components beforehand.
[00:19:06.860 --> 00:19:08.620]   Then it would be an engineering problem.
[00:19:08.620 --> 00:19:10.380]   It would not be a research problem.
[00:19:10.380 --> 00:19:12.060]   So you kind of need a better framework
[00:19:12.060 --> 00:19:13.020]   for thinking about that.
[00:19:13.020 --> 00:19:15.340]   And I think an analogy that works much better
[00:19:15.340 --> 00:19:19.140]   is kind of thinking about that there is this kind of mountain
[00:19:19.140 --> 00:19:20.220]   you want to get to.
[00:19:20.220 --> 00:19:21.460]   And you can see this peak.
[00:19:21.460 --> 00:19:26.460]   And you want to reach this peak and feel kind of really
[00:19:26.460 --> 00:19:28.780]   glorious when you get there.
[00:19:28.780 --> 00:19:31.500]   And you will understand the world in a different way.
[00:19:31.500 --> 00:19:34.340]   But between you and that peak, there's all these obstacles.
[00:19:34.340 --> 00:19:35.380]   There's a big forest.
[00:19:35.380 --> 00:19:39.180]   There's potentially dangers and pitfalls along the way.
[00:19:39.180 --> 00:19:41.060]   But you need to get there.
[00:19:41.060 --> 00:19:44.420]   So how do you do it?
[00:19:44.420 --> 00:19:45.500]   Here, it looks very easy.
[00:19:45.500 --> 00:19:48.140]   Just go in between the trees, and you can get to the peak.
[00:19:48.140 --> 00:19:52.260]   But really, it looks more like this.
[00:19:52.260 --> 00:19:54.900]   You can't really see that far ahead.
[00:19:54.900 --> 00:19:57.100]   There's big, thick vegetation.
[00:19:57.100 --> 00:20:01.340]   And you kind of have to make a call on what you're going
[00:20:01.340 --> 00:20:03.100]   to do just in the short term.
[00:20:03.100 --> 00:20:07.580]   And the way you think about it is there are a few hypotheses.
[00:20:07.580 --> 00:20:10.420]   You usually have on the order of anywhere from two to five
[00:20:10.420 --> 00:20:13.500]   different things you could be doing to learn a little bit
[00:20:13.500 --> 00:20:15.660]   more about the problem.
[00:20:15.660 --> 00:20:19.700]   And that is the key.
[00:20:19.700 --> 00:20:23.020]   They're short-term tasks, but they're
[00:20:23.020 --> 00:20:26.260]   very explicit hypotheses.
[00:20:26.260 --> 00:20:32.860]   Now, if you actually reach the peak,
[00:20:32.860 --> 00:20:35.140]   then your path will look something like this.
[00:20:35.140 --> 00:20:37.540]   You will have tried a lot of things that didn't work out.
[00:20:37.540 --> 00:20:39.260]   And often, when you think about research,
[00:20:39.260 --> 00:20:42.060]   it's like 90% of the things you do don't work out.
[00:20:42.060 --> 00:20:45.380]   And you kind of have to accept that.
[00:20:45.380 --> 00:20:50.340]   And that's kind of part of the process of finding the way.
[00:20:50.340 --> 00:20:58.460]   And so if you think about it in this way very explicitly,
[00:20:58.460 --> 00:21:00.380]   where in engineering you have concrete tasks,
[00:21:00.380 --> 00:21:02.700]   in research you have hypotheses, then
[00:21:02.700 --> 00:21:04.620]   you can start thinking a little bit more about,
[00:21:04.620 --> 00:21:05.780]   how do you tackle this?
[00:21:05.780 --> 00:21:07.540]   And how do you measure your progress?
[00:21:07.540 --> 00:21:11.180]   Well, one way to measure your progress
[00:21:11.180 --> 00:21:15.620]   is just how many hypotheses can you try?
[00:21:15.620 --> 00:21:19.900]   And if you're more people, you can try more hypotheses.
[00:21:19.900 --> 00:21:23.500]   And the fact is also that you might
[00:21:23.500 --> 00:21:25.580]   need to try all four hypotheses, or you
[00:21:25.580 --> 00:21:29.460]   might need to try one hypothesis to get to the next stage.
[00:21:29.460 --> 00:21:31.740]   So maybe here you try two of them,
[00:21:31.740 --> 00:21:34.980]   and then you don't even need to try three and four,
[00:21:34.980 --> 00:21:37.460]   because whatever you have is good enough for the next step.
[00:21:37.460 --> 00:21:40.340]   And then you get a new set of hypotheses.
[00:21:40.340 --> 00:21:43.500]   And that's kind of the way you need to operate.
[00:21:43.500 --> 00:21:45.420]   And the really beautiful thing about this is,
[00:21:45.420 --> 00:21:48.500]   if you're a team, the way to make progress
[00:21:48.500 --> 00:21:51.740]   is really to very clearly identify these hypotheses,
[00:21:51.740 --> 00:21:54.180]   and then make progress as a team, split up the work
[00:21:54.180 --> 00:21:56.940]   and do it, and then make progress as a team.
[00:21:56.940 --> 00:22:00.940]   And just accepting the fact that 90% of the team
[00:22:00.940 --> 00:22:02.900]   will be working on things that doesn't work out.
[00:22:02.900 --> 00:22:05.540]   But that's fine, because what you're making progress
[00:22:05.540 --> 00:22:08.420]   is as a team and not as the individuals.
[00:22:08.420 --> 00:22:13.060]   And this can be very powerful, because then you
[00:22:13.060 --> 00:22:15.140]   don't feel as bad when your things don't work out.
[00:22:15.140 --> 00:22:16.600]   You kind of just have to accept it.
[00:22:16.600 --> 00:22:18.780]   And instead, you can be more happy when
[00:22:18.780 --> 00:22:21.860]   your co-worker's thing works out.
[00:22:21.860 --> 00:22:26.180]   And next time, it will be your thing that works out.
[00:22:26.180 --> 00:22:27.900]   But nobody really knows beforehand.
[00:22:27.900 --> 00:22:32.140]   And that's kind of just the way that it is.
[00:22:32.140 --> 00:22:33.500]   Now, yeah?
[00:22:33.500 --> 00:22:36.940]   How do you handle the shifting from the 1, 2, 3 to ABC?
[00:22:36.940 --> 00:22:41.220]   Means if I was assigned to 3 and 4,
[00:22:41.220 --> 00:22:42.980]   just stopping that and asking--
[00:22:42.980 --> 00:22:43.480]   Yeah.
[00:22:43.480 --> 00:22:44.060]   --what does it mean now?
[00:22:44.060 --> 00:22:46.700]   You need to get really good at just abandoning your code
[00:22:46.700 --> 00:22:47.540]   and stuff like that.
[00:22:47.540 --> 00:22:50.180]   Have no emotions to the things you have created,
[00:22:50.180 --> 00:22:54.540]   because it doesn't matter in them.
[00:22:54.540 --> 00:22:56.980]   So that's usually how it works, to be quite honest.
[00:22:56.980 --> 00:22:59.220]   It's very often at OpenAI, we do this.
[00:22:59.220 --> 00:23:00.140]   We work on things.
[00:23:00.140 --> 00:23:02.420]   And then one thing works out.
[00:23:02.420 --> 00:23:04.660]   And then it's like, all these other things we're working on,
[00:23:04.660 --> 00:23:05.740]   it doesn't matter anymore.
[00:23:05.740 --> 00:23:06.780]   We just drop it.
[00:23:06.780 --> 00:23:09.740]   Because now we have something much better
[00:23:09.740 --> 00:23:11.700]   to build upon, which we can move forward with.
[00:23:11.700 --> 00:23:15.140]   And then do you have a notion of matching
[00:23:15.140 --> 00:23:17.300]   how much I believe in a hypothesis
[00:23:17.300 --> 00:23:19.380]   with me actually working on a hypothesis?
[00:23:19.380 --> 00:23:21.100]   Yeah, so this is a good point.
[00:23:21.100 --> 00:23:25.620]   I think you can prioritize the hypothesis a little bit based
[00:23:25.620 --> 00:23:28.060]   on your prior on how well they work.
[00:23:28.060 --> 00:23:36.660]   And after that, it's similar to any task management
[00:23:36.660 --> 00:23:37.940]   in some sense.
[00:23:37.940 --> 00:23:40.580]   Some people will have better skills to do certain things.
[00:23:40.580 --> 00:23:43.380]   Some people will be more motivated to do certain things.
[00:23:43.380 --> 00:23:45.940]   Then you can use pretty standard principles
[00:23:45.940 --> 00:23:48.380]   to figure out who does what.
[00:23:48.380 --> 00:23:52.460]   But I do think the thing that's related to what you said
[00:23:52.460 --> 00:23:56.220]   is that not all hypotheses might be equal.
[00:23:56.220 --> 00:23:58.580]   You might have some that you believe in more as a team
[00:23:58.580 --> 00:24:01.060]   as well.
[00:24:01.060 --> 00:24:04.060]   Did you have to sometimes backtrack
[00:24:04.060 --> 00:24:05.540]   on your hypothesis progress?
[00:24:05.540 --> 00:24:07.820]   Like, you got to almost level three
[00:24:07.820 --> 00:24:10.260]   and then got a dead end, so you kind of go forward?
[00:24:10.260 --> 00:24:10.760]   Yeah.
[00:24:10.760 --> 00:24:14.740]   Sometimes it happens.
[00:24:14.740 --> 00:24:15.820]   You can hit walls.
[00:24:15.820 --> 00:24:19.080]   I think that's happened to us sometimes.
[00:24:19.080 --> 00:24:22.260]   More often than not, you learn other things along the way.
[00:24:22.260 --> 00:24:25.140]   You kind of unlock other paths that
[00:24:25.140 --> 00:24:30.900]   might be earlier from just new discoveries you make and so on.
[00:24:30.900 --> 00:24:32.820]   But it does happen.
[00:24:32.820 --> 00:24:34.860]   You can put a lot of effort into one direction,
[00:24:34.860 --> 00:24:36.420]   and you just have to backtrack and take
[00:24:36.420 --> 00:24:39.020]   a completely different approach because the approach you took
[00:24:39.020 --> 00:24:40.700]   was just the wrong one.
[00:24:40.700 --> 00:24:43.900]   That's also part of research.
[00:24:43.900 --> 00:24:45.140]   So you mentioned backtracking.
[00:24:45.140 --> 00:24:48.460]   So how far out do you actually keep track of where
[00:24:48.460 --> 00:24:50.140]   do you go back?
[00:24:50.140 --> 00:24:52.740]   OK, I will talk more about that later, actually.
[00:24:52.740 --> 00:24:55.540]   That's a good question.
[00:24:55.540 --> 00:24:59.140]   So now the second lesson I want to talk about
[00:24:59.140 --> 00:25:00.980]   is metrics because I think this is something
[00:25:00.980 --> 00:25:06.980]   that's a little bit neglected in doing research,
[00:25:06.980 --> 00:25:09.780]   and I think it's extremely important for team-based
[00:25:09.780 --> 00:25:10.820]   research.
[00:25:10.820 --> 00:25:13.260]   As a community, at least a lot of machine learning--
[00:25:13.260 --> 00:25:15.100]   a big piece of the machine learning community
[00:25:15.100 --> 00:25:16.220]   is relatively good at this.
[00:25:16.220 --> 00:25:18.780]   This is a way to kind of-- in the computer vision community,
[00:25:18.780 --> 00:25:20.740]   for example, everybody has-- there's a number of data sets,
[00:25:20.740 --> 00:25:22.660]   and people make progress on those data sets.
[00:25:22.660 --> 00:25:27.060]   There are some shared metrics, and so that's
[00:25:27.060 --> 00:25:29.060]   the way you move forward as a community.
[00:25:29.060 --> 00:25:34.860]   But I would argue this is extremely important for teams
[00:25:34.860 --> 00:25:37.300]   because it works a little bit like a compass.
[00:25:37.300 --> 00:25:40.220]   It kind of gives you a sense of are you
[00:25:40.220 --> 00:25:42.940]   going in the right direction?
[00:25:42.940 --> 00:25:45.500]   And the first thing you need to do
[00:25:45.500 --> 00:25:47.620]   in order to pick a good metric is to kind of--
[00:25:47.620 --> 00:25:49.420]   you need to think about the global problem.
[00:25:49.420 --> 00:25:51.780]   And if you're a company, that might be,
[00:25:51.780 --> 00:25:54.540]   how do I want to achieve $100 million revenue,
[00:25:54.540 --> 00:25:57.180]   or 1 million users, or a billion dollar valuation?
[00:25:57.180 --> 00:25:58.540]   At OpenAI, we want to build, say,
[00:25:58.540 --> 00:26:02.980]   self-AGI, safe AGI, which is artificial general
[00:26:02.980 --> 00:26:04.500]   intelligence.
[00:26:04.500 --> 00:26:08.580]   But those are really hard things to think about.
[00:26:08.580 --> 00:26:10.180]   So you kind of need to create something
[00:26:10.180 --> 00:26:12.260]   that's a little bit more tractable.
[00:26:12.260 --> 00:26:14.860]   So for robotics team at OpenAI, we
[00:26:14.860 --> 00:26:20.700]   think that general purpose robot is a path towards safe AGI.
[00:26:20.700 --> 00:26:22.700]   But even that is a pretty big thing.
[00:26:22.700 --> 00:26:26.180]   So then the simplest thing to work on in short term
[00:26:26.180 --> 00:26:28.100]   seems to be, well, can we get a robot even
[00:26:28.100 --> 00:26:30.100]   to rotate the thing in its hand?
[00:26:30.100 --> 00:26:32.100]   Pretty far away from a general purpose robot,
[00:26:32.100 --> 00:26:33.740]   but it's something much more concrete.
[00:26:33.740 --> 00:26:40.340]   And you want to pick something very, very concrete.
[00:26:40.340 --> 00:26:43.740]   And then you want to pick your data
[00:26:43.740 --> 00:26:46.380]   sets that's related to that.
[00:26:46.380 --> 00:26:51.580]   And the data set and the metric is extremely important.
[00:26:51.580 --> 00:26:56.900]   Spending the time to, say, clean up a data set
[00:26:56.900 --> 00:26:59.500]   and make it so that you really trust your data set
[00:26:59.500 --> 00:27:00.660]   is always worth the time.
[00:27:00.660 --> 00:27:03.460]   Because the worst thing is if you spend months on things,
[00:27:03.460 --> 00:27:05.660]   and then you realize there's bugs in your data
[00:27:05.660 --> 00:27:08.940]   set, in your evaluation code, or things have been mislabeled
[00:27:08.940 --> 00:27:09.900]   and so on.
[00:27:09.900 --> 00:27:11.700]   That really sucks, because you've basically
[00:27:11.700 --> 00:27:13.580]   wasted all your work.
[00:27:13.580 --> 00:27:16.660]   So getting that very, very clear is super important.
[00:27:16.660 --> 00:27:18.220]   And then picking the metrics that you
[00:27:18.220 --> 00:27:20.860]   want to optimize as a team.
[00:27:20.860 --> 00:27:24.100]   Here's just one example.
[00:27:24.100 --> 00:27:26.980]   Here's a-- I won't say exactly what metric it is,
[00:27:26.980 --> 00:27:29.180]   but it's a real metric that we were tracking
[00:27:29.180 --> 00:27:32.620]   for the past few weeks at OpenAI.
[00:27:32.620 --> 00:27:37.020]   And so we started out at about 30% error,
[00:27:37.020 --> 00:27:39.260]   and then we have maybe three or four people
[00:27:39.260 --> 00:27:41.300]   working parallel on different hypotheses.
[00:27:41.300 --> 00:27:43.700]   And every week, we make progress as a team
[00:27:43.700 --> 00:27:47.860]   to reduce this error rate.
[00:27:47.860 --> 00:27:53.420]   It's a really good way of just motivating a team,
[00:27:53.420 --> 00:27:57.860]   because you feel progress as a team as you're working
[00:27:57.860 --> 00:28:00.460]   towards this metric.
[00:28:00.460 --> 00:28:04.300]   But the other thing that it really provides is clear focus.
[00:28:04.300 --> 00:28:07.100]   It lets you prioritize hypotheses.
[00:28:07.100 --> 00:28:09.820]   As you're doing your research, you
[00:28:09.820 --> 00:28:12.180]   will find a lot of really interesting things
[00:28:12.180 --> 00:28:13.140]   along the way.
[00:28:13.140 --> 00:28:18.580]   And it's really easy to get into this mode
[00:28:18.580 --> 00:28:21.340]   where you just find something really, really interesting,
[00:28:21.340 --> 00:28:23.340]   and then you just go down the rabbit hole.
[00:28:23.340 --> 00:28:26.740]   And this is where metrics help, because you can always
[00:28:26.740 --> 00:28:28.900]   ask yourself, do you really think
[00:28:28.900 --> 00:28:31.500]   that this will help optimizing towards the metrics?
[00:28:31.500 --> 00:28:36.660]   It really helps a team prioritize and discuss
[00:28:36.660 --> 00:28:40.420]   in a very objective way the tasks and hypotheses
[00:28:40.420 --> 00:28:41.340]   that you have.
[00:28:41.340 --> 00:28:45.500]   So just for productivity in general,
[00:28:45.500 --> 00:28:47.940]   spending the time at the beginning of projects
[00:28:47.940 --> 00:28:54.180]   to pick your data sets and pick your metrics, at least what
[00:28:54.180 --> 00:28:56.340]   I've found, it always pays off.
[00:28:56.340 --> 00:29:01.100]   So never neglect this one.
[00:29:01.100 --> 00:29:05.260]   Any questions on the metrics?
[00:29:06.260 --> 00:29:07.260]   OK.
[00:29:07.260 --> 00:29:12.780]   Third thing, which is most related to the stuff
[00:29:12.780 --> 00:29:14.900]   that Joshua was talking about, is tooling.
[00:29:14.900 --> 00:29:19.380]   And this is also one of those things which is often
[00:29:19.380 --> 00:29:20.940]   relatively neglected.
[00:29:20.940 --> 00:29:24.260]   By the way, could I have something to drink?
[00:29:24.260 --> 00:29:27.540]   My throat is itching a little bit.
[00:29:27.540 --> 00:29:28.220]   OK, thank you.
[00:29:33.300 --> 00:29:35.460]   In order to traverse this jungle,
[00:29:35.460 --> 00:29:37.340]   you need a really good set of tools.
[00:29:37.340 --> 00:29:38.780]   And often when you start out, you
[00:29:38.780 --> 00:29:40.460]   don't know what those tools are.
[00:29:40.460 --> 00:29:42.420]   And you're building those tools along the way.
[00:29:42.420 --> 00:29:44.700]   You're trying to figure out what are
[00:29:44.700 --> 00:29:46.700]   the things that are going to make me move fast as quickly
[00:29:46.700 --> 00:29:47.200]   as possible.
[00:29:47.200 --> 00:30:00.100]   Again, it looks like you have to get-- thank you very much.
[00:30:00.100 --> 00:30:01.940]   You have to get through something like this.
[00:30:01.940 --> 00:30:04.180]   And that's really hard.
[00:30:04.180 --> 00:30:12.380]   And thinking about the tools is a worthwhile thing to do.
[00:30:12.380 --> 00:30:14.740]   Now, as you're making progress on your hypothesis,
[00:30:14.740 --> 00:30:15.980]   some things will not work out.
[00:30:15.980 --> 00:30:18.420]   And as we discussed, you might have to throw away code.
[00:30:18.420 --> 00:30:21.300]   But whenever things do work out, it's worth the effort
[00:30:21.300 --> 00:30:25.900]   to make it into something solid that your teammates can use.
[00:30:25.900 --> 00:30:28.180]   Because to get through this, you want
[00:30:28.180 --> 00:30:30.420]   to turn the thing that you had in your experiment
[00:30:30.420 --> 00:30:32.860]   into something that's super, super, super useful.
[00:30:32.860 --> 00:30:34.940]   You want to turn your stone axe into a chainsaw
[00:30:34.940 --> 00:30:37.060]   so you can really get through any problem
[00:30:37.060 --> 00:30:39.420]   that you have along the way.
[00:30:39.420 --> 00:30:42.800]   And that means that once you have figured out those-- once
[00:30:42.800 --> 00:30:44.820]   you have validated, or even in some cases,
[00:30:44.820 --> 00:30:47.940]   invalidated your hypothesis, you need to move your science
[00:30:47.940 --> 00:30:49.780]   to engineering.
[00:30:49.780 --> 00:30:52.140]   And this is where we can use all those best practices
[00:30:52.140 --> 00:30:53.820]   that people have developed in engineering
[00:30:53.820 --> 00:30:58.260]   for a really long time, like code reviews, integration
[00:30:58.260 --> 00:31:00.860]   tests, unit tests, thoughtful interfaces.
[00:31:00.860 --> 00:31:05.940]   And these things are all really, really important.
[00:31:05.940 --> 00:31:10.780]   Integration tests, I would argue,
[00:31:10.780 --> 00:31:13.940]   is probably one of the most important things.
[00:31:13.940 --> 00:31:15.940]   As Josh said, we've had this--
[00:31:15.940 --> 00:31:18.180]   if you don't have them, what happens--
[00:31:18.180 --> 00:31:19.980]   and I've seen this happen in many studies,
[00:31:19.980 --> 00:31:22.540]   many times at Dropbox, many times at OpenAI--
[00:31:22.540 --> 00:31:27.180]   is at some point, you have a regression.
[00:31:27.180 --> 00:31:29.500]   And you didn't know when it happened.
[00:31:29.500 --> 00:31:32.300]   And you have to do a binary search back in your commits.
[00:31:32.300 --> 00:31:34.540]   And your data sets might have changed and stuff.
[00:31:34.540 --> 00:31:36.660]   It's just usually a big nightmare.
[00:31:36.660 --> 00:31:38.340]   And almost every time this happens,
[00:31:38.340 --> 00:31:41.620]   you can spend anywhere from a week to a month
[00:31:41.620 --> 00:31:42.500]   trying to figure out.
[00:31:42.500 --> 00:31:46.820]   I remember there was a famous example of this at Google,
[00:31:46.820 --> 00:31:52.180]   where one of their Google nets or something like that
[00:31:52.180 --> 00:31:53.620]   basically regressed.
[00:31:53.620 --> 00:31:57.580]   And it took a team of five people a month to find it.
[00:31:57.580 --> 00:31:59.620]   This happens to the best of teams.
[00:31:59.620 --> 00:32:03.740]   So it kind of pays off to set up really good processes
[00:32:03.740 --> 00:32:08.140]   around tracking, making sure you don't break things
[00:32:08.140 --> 00:32:08.980]   along the way.
[00:32:08.980 --> 00:32:10.520]   I think another thing that's really important
[00:32:10.520 --> 00:32:12.220]   is actually thoughtful interfaces.
[00:32:12.220 --> 00:32:14.340]   This is something that's even more neglected.
[00:32:14.340 --> 00:32:16.460]   But it's extremely important as you work as a team,
[00:32:16.460 --> 00:32:19.180]   because everybody are going to work on your code.
[00:32:19.180 --> 00:32:21.220]   So at OpenAI, we try--
[00:32:21.220 --> 00:32:22.880]   I wouldn't argue we're the best at this.
[00:32:22.880 --> 00:32:25.980]   But we try to spend a lot of time
[00:32:25.980 --> 00:32:29.140]   thinking about our interfaces, commenting on it,
[00:32:29.140 --> 00:32:32.620]   adding a lot of documentations along the way.
[00:32:32.620 --> 00:32:34.860]   Very simple things that I think are really important
[00:32:34.860 --> 00:32:37.280]   are things like, if you have a model,
[00:32:37.280 --> 00:32:40.160]   being able to very easily load that model,
[00:32:40.160 --> 00:32:41.860]   run a prediction with it, and so on.
[00:32:41.860 --> 00:32:43.560]   You would be surprised the amount of time
[00:32:43.560 --> 00:32:47.460]   I've seen just a whole lot of boilerplate code
[00:32:47.460 --> 00:32:48.780]   in order to even run a model.
[00:32:48.780 --> 00:32:52.080]   And what that usually results in is just lots
[00:32:52.080 --> 00:32:55.080]   of bugs and assumptions.
[00:32:55.080 --> 00:32:58.720]   And you should try to encapsulate
[00:32:58.720 --> 00:33:00.720]   as many assumptions you have as a team
[00:33:00.720 --> 00:33:04.040]   into really, really useful and easy-to-use interfaces,
[00:33:04.040 --> 00:33:06.860]   because that way, you can move much faster.
[00:33:06.860 --> 00:33:09.280]   You don't have to think as much about the things you do.
[00:33:09.280 --> 00:33:12.640]   And all of those conventions that each person has developed
[00:33:12.640 --> 00:33:17.800]   individually, they become codified in the actual code.
[00:33:17.800 --> 00:33:21.720]   Also, as Josh said, at Omnia, we have a lot of tests.
[00:33:21.720 --> 00:33:25.400]   Every one of these folders have additional lots of tests.
[00:33:25.400 --> 00:33:27.160]   It's well worth it.
[00:33:27.160 --> 00:33:31.680]   We try to have a pretty hard policy
[00:33:31.680 --> 00:33:33.240]   around whenever you add new code,
[00:33:33.240 --> 00:33:36.080]   you should make sure that it's tested and also potentially
[00:33:36.080 --> 00:33:38.200]   test some of the old code.
[00:33:38.200 --> 00:33:44.920]   We found that this has just made us move much faster.
[00:33:44.920 --> 00:33:46.620]   And it's one of those things where
[00:33:46.620 --> 00:33:51.600]   you don't do it much if you're doing individual research,
[00:33:51.600 --> 00:33:55.040]   because often, you have a lot of context on the code,
[00:33:55.040 --> 00:33:58.320]   and you don't maybe have the time
[00:33:58.320 --> 00:34:00.560]   to do it because you have a paper deadline or something
[00:34:00.560 --> 00:34:01.080]   like that.
[00:34:01.080 --> 00:34:02.560]   But as a team, it's so important,
[00:34:02.560 --> 00:34:05.920]   because you need to make sure that interfaces don't break.
[00:34:05.920 --> 00:34:11.800]   You need to make sure that you can load old models and stuff
[00:34:11.800 --> 00:34:12.300]   like that.
[00:34:15.800 --> 00:34:18.340]   And it's also a way of documenting even the way
[00:34:18.340 --> 00:34:19.300]   that your code works.
[00:34:19.300 --> 00:34:22.500]   So as a team, it's even more important.
[00:34:22.500 --> 00:34:23.000]   Go ahead.
[00:34:23.000 --> 00:34:26.220]   So as far as I'm concerned, you're
[00:34:26.220 --> 00:34:28.420]   accumulating more and more tests as you go.
[00:34:28.420 --> 00:34:30.940]   And given that you have this hypothesis model,
[00:34:30.940 --> 00:34:32.060]   you're balancing things.
[00:34:32.060 --> 00:34:35.780]   So very quickly, your tests turn into reliability
[00:34:35.780 --> 00:34:37.140]   that you need to maintain.
[00:34:37.140 --> 00:34:39.340]   You need to grow them.
[00:34:39.340 --> 00:34:41.580]   You probably have some set of best practices on that.
[00:34:41.580 --> 00:34:42.500]   Could you share those?
[00:34:42.500 --> 00:34:43.140]   Yeah, sure.
[00:34:43.140 --> 00:34:44.340]   That's a really good question.
[00:34:44.340 --> 00:34:48.120]   We actually try to delete a lot of code as well.
[00:34:48.120 --> 00:34:50.440]   If we find that we don't use some code,
[00:34:50.440 --> 00:34:54.400]   we're pretty brutal in deleting it.
[00:34:54.400 --> 00:34:57.960]   Just knowing that we can always find it back in our Git history
[00:34:57.960 --> 00:35:00.240]   if we ever needed it again, it's usually not
[00:35:00.240 --> 00:35:02.640]   worth keeping around things that we don't use,
[00:35:02.640 --> 00:35:05.220]   because they're just going to hold us back.
[00:35:05.220 --> 00:35:06.760]   So we do that.
[00:35:06.760 --> 00:35:08.800]   We also try to take the time every now and then
[00:35:08.800 --> 00:35:13.000]   to refactor and rethink what we have done
[00:35:13.000 --> 00:35:15.360]   and put it in a much better framework.
[00:35:15.360 --> 00:35:19.320]   This is often a hard thing, but it is also often worth it.
[00:35:19.320 --> 00:35:22.100]   Especially a place where this often happens
[00:35:22.100 --> 00:35:26.680]   is if we have one direction and the code is not super nice,
[00:35:26.680 --> 00:35:29.080]   it's usually good to refactor it before more people come on
[00:35:29.080 --> 00:35:33.280]   board, just because then that's just
[00:35:33.280 --> 00:35:35.400]   going to make it easier for people to collaborate,
[00:35:35.400 --> 00:35:37.840]   modularize things, make things less coupled,
[00:35:37.840 --> 00:35:38.680]   and stuff like that.
[00:35:38.680 --> 00:35:40.840]   Does that mean that you're back from this happening
[00:35:40.840 --> 00:35:44.160]   before you do the next set of hypotheses?
[00:35:44.160 --> 00:35:46.680]   Yes, that happens.
[00:35:46.680 --> 00:35:49.360]   We would just block a new hypothesis for a while,
[00:35:49.360 --> 00:35:53.280]   and it's usually well worth the effort.
[00:35:53.280 --> 00:35:57.160]   How far do you take the engineering analogy
[00:35:57.160 --> 00:36:01.480]   that you have style guidelines and [INAUDIBLE]??
[00:36:01.480 --> 00:36:03.840]   Yeah, we actually try to have a lot of that.
[00:36:03.840 --> 00:36:09.440]   We have linters and stuff for our code reviews.
[00:36:09.440 --> 00:36:13.040]   Again, just to make it very easy to read the code.
[00:36:13.040 --> 00:36:14.880]   The truth with machine learning code
[00:36:14.880 --> 00:36:19.720]   is that bugs are usually extremely subtle.
[00:36:19.720 --> 00:36:21.280]   It's extremely easy to have bugs.
[00:36:21.280 --> 00:36:22.440]   And when you have bugs, you might not even
[00:36:22.440 --> 00:36:25.160]   know about it, because it's just like your model isn't learning
[00:36:25.160 --> 00:36:27.520]   as good as it should be learning.
[00:36:27.520 --> 00:36:30.640]   It's still learning, but it could be learning better.
[00:36:30.640 --> 00:36:31.680]   That happens all the time.
[00:36:31.680 --> 00:36:36.320]   So making code extremely clear and easy to see what's going on.
[00:36:36.320 --> 00:36:39.600]   One pet peeve I have for a lot of implementations
[00:36:39.600 --> 00:36:41.040]   of machine learning algorithms is
[00:36:41.040 --> 00:36:42.680]   that they have all these [INAUDIBLE]
[00:36:42.680 --> 00:36:44.520]   A, alpha, beta, all these things,
[00:36:44.520 --> 00:36:45.600]   and they make no sense.
[00:36:45.600 --> 00:36:47.440]   And usually, there's a pretty good name
[00:36:47.440 --> 00:36:48.920]   that you can apply to it.
[00:36:48.920 --> 00:36:53.040]   You can name it something like learning rate or discount
[00:36:53.040 --> 00:36:54.360]   factor or something like that.
[00:36:54.360 --> 00:36:56.560]   And then the code becomes much, much clearer.
[00:36:56.560 --> 00:36:59.440]   And we try to apply a lot of that to our code
[00:36:59.440 --> 00:37:01.840]   to just make it readable, because that's actually
[00:37:01.840 --> 00:37:04.440]   the best way to avoid these bugs.
[00:37:04.440 --> 00:37:08.320]   Then you get complaints from researchers
[00:37:08.320 --> 00:37:13.120]   who are now forced to follow these very strict guidelines.
[00:37:13.120 --> 00:37:15.000]   I don't think so.
[00:37:15.000 --> 00:37:18.800]   Because often-- and I think Josh alluded to this--
[00:37:18.800 --> 00:37:21.000]   in the beginning, when you try out a new hypothesis,
[00:37:21.000 --> 00:37:22.520]   we're not so strict with this.
[00:37:22.520 --> 00:37:24.520]   If you're just trying something out,
[00:37:24.520 --> 00:37:27.000]   it's actually better for you to move quickly and just try out
[00:37:27.000 --> 00:37:27.960]   a lot of things.
[00:37:27.960 --> 00:37:30.360]   It's more like when you want to merge it into master
[00:37:30.360 --> 00:37:31.960]   or get other people to use it, that's
[00:37:31.960 --> 00:37:34.120]   when it's really, really important to have something
[00:37:34.120 --> 00:37:38.960]   that's really reliable and well-formatted and stuff
[00:37:38.960 --> 00:37:40.200]   like that.
[00:37:40.200 --> 00:37:42.760]   Because when you're at that validate or invalidate
[00:37:42.760 --> 00:37:45.520]   hypothesis stage, more likely than not,
[00:37:45.520 --> 00:37:47.060]   you will throw away your code anyway.
[00:37:47.060 --> 00:37:52.200]   Let's see.
[00:37:52.200 --> 00:37:54.680]   I will just move quickly through the other slides,
[00:37:54.680 --> 00:37:56.960]   because I think I'm running out of time.
[00:37:56.960 --> 00:38:00.040]   The fourth thing I want to spend just a quick amount of time
[00:38:00.040 --> 00:38:01.620]   talking about is building out the map.
[00:38:01.620 --> 00:38:03.740]   And this is alluding to the question I got before,
[00:38:03.740 --> 00:38:11.560]   is how do you build up a sense of what you have learned so far?
[00:38:11.560 --> 00:38:14.760]   And you might know that there are dragons hiding
[00:38:14.760 --> 00:38:17.180]   behind a tree or something, and you don't want to go back
[00:38:17.180 --> 00:38:19.120]   to that dragon.
[00:38:19.120 --> 00:38:21.000]   And so you need really good processes
[00:38:21.000 --> 00:38:24.440]   if you're a large set of people to learn from each other
[00:38:24.440 --> 00:38:27.200]   and spread that knowledge and build up the map together.
[00:38:27.200 --> 00:38:29.320]   And I think there's some very simple things to do.
[00:38:29.320 --> 00:38:30.840]   Lots of tooling around this.
[00:38:30.840 --> 00:38:34.060]   But you should not forget the very simplest things.
[00:38:34.060 --> 00:38:36.140]   For example, one thing we do a lot at Omineo
[00:38:36.140 --> 00:38:39.000]   is for every hypothesis, we try to write up
[00:38:39.000 --> 00:38:40.620]   a little bit of a document.
[00:38:40.620 --> 00:38:42.300]   Like, what is this hypothesis?
[00:38:42.300 --> 00:38:47.540]   And then have a little bit of a logbook of the experiments
[00:38:47.540 --> 00:38:50.940]   that were being run and the conclusions of those experiments
[00:38:50.940 --> 00:38:53.700]   so that we use Google Drive a lot for this,
[00:38:53.700 --> 00:38:56.140]   because it's just very useful for people can comment on it
[00:38:56.140 --> 00:38:56.420]   and so on.
[00:38:56.420 --> 00:38:58.500]   So as you're making progress on your experiments,
[00:38:58.500 --> 00:39:02.680]   you post your experiment log, and you're
[00:39:02.680 --> 00:39:04.100]   sourcing a Slack channel as well.
[00:39:04.100 --> 00:39:05.980]   You get feedback on that.
[00:39:05.980 --> 00:39:07.900]   And it's really useful to go back, like, oh,
[00:39:07.900 --> 00:39:09.900]   how was that experiment going?
[00:39:09.900 --> 00:39:12.940]   Like, why did we make that choice, like, two months back?
[00:39:12.940 --> 00:39:16.180]   It's really good to have this very explicitly.
[00:39:16.180 --> 00:39:21.220]   And so we try to be relatively strict on this.
[00:39:21.220 --> 00:39:23.580]   I think it's one of those things we started--
[00:39:23.580 --> 00:39:25.300]   we tried different formats of doing this.
[00:39:25.300 --> 00:39:27.260]   And then at some point, the simplest way
[00:39:27.260 --> 00:39:29.020]   was just to write these Google documents.
[00:39:29.020 --> 00:39:31.700]   And then once we removed a lot of the process around it,
[00:39:31.700 --> 00:39:32.900]   people just started doing it.
[00:39:32.900 --> 00:39:34.060]   It's like, the way to think about it
[00:39:34.060 --> 00:39:36.540]   is it's just your little logbook for your hypothesis.
[00:39:36.540 --> 00:39:38.620]   And the only thing you should be very clear about
[00:39:38.620 --> 00:39:41.820]   is what is the hypothesis you're trying to prove or disprove.
[00:39:41.820 --> 00:39:43.340]   Make it very, very clear.
[00:39:43.340 --> 00:39:46.780]   Now, the good thing is that there are a lot of cool tools.
[00:39:46.780 --> 00:39:48.740]   I mean, there's Ways and Biases.
[00:39:48.740 --> 00:39:50.340]   They're not paying me for saying this,
[00:39:50.340 --> 00:39:53.660]   but we have been using it a lot at OpenAI.
[00:39:53.660 --> 00:39:55.980]   And it's really useful, because you can go back now.
[00:39:55.980 --> 00:39:59.060]   Often, people link to the Ways and Biases experiments
[00:39:59.060 --> 00:39:59.820]   in these logbooks.
[00:39:59.820 --> 00:40:01.820]   So you can actually go back and look at the logs.
[00:40:01.820 --> 00:40:04.740]   So you can download the models and these things.
[00:40:04.740 --> 00:40:08.780]   It's actually helped us quite a bit.
[00:40:08.780 --> 00:40:11.420]   Our workflow for this before was pretty terrible.
[00:40:11.420 --> 00:40:14.060]   So I'm very happy this tool exists.
[00:40:14.060 --> 00:40:17.900]   But then there are other things that are very simple,
[00:40:17.900 --> 00:40:18.780]   but we still do--
[00:40:18.780 --> 00:40:20.900]   we just use this Google spreadsheet.
[00:40:20.900 --> 00:40:27.420]   And here, we put in how we're doing a core metric.
[00:40:27.420 --> 00:40:28.980]   We have links to Ways and Biases,
[00:40:28.980 --> 00:40:31.420]   some notes about the things we were trying.
[00:40:31.420 --> 00:40:35.020]   And we just try to be very transparent of what
[00:40:35.020 --> 00:40:36.900]   we're doing in a team and logging
[00:40:36.900 --> 00:40:39.020]   all the results in one place.
[00:40:39.020 --> 00:40:41.500]   And again, very simple tools, like a spreadsheet,
[00:40:41.500 --> 00:40:43.340]   a Google Doc.
[00:40:43.340 --> 00:40:45.580]   It's very simple, but it helps so much
[00:40:45.580 --> 00:40:47.700]   in terms of understanding how you're doing
[00:40:47.700 --> 00:40:51.820]   and seeing how you're making progress.
[00:40:51.820 --> 00:40:55.820]   So it's just using these tools to build up
[00:40:55.820 --> 00:40:59.420]   the map along the way lets you learn as a team.
[00:40:59.420 --> 00:41:04.500]   And the risk you run otherwise is
[00:41:04.500 --> 00:41:07.980]   that people have context of the little hypothesis,
[00:41:07.980 --> 00:41:09.140]   but nobody else.
[00:41:09.140 --> 00:41:11.940]   And then you can keep on redoing the same mistakes over and over
[00:41:11.940 --> 00:41:12.420]   again.
[00:41:12.420 --> 00:41:15.420]   That's what you want to avoid.
[00:41:15.420 --> 00:41:17.660]   Any questions on that?
[00:41:17.660 --> 00:41:21.660]   In your logbook, is there some strict rules
[00:41:21.660 --> 00:41:26.700]   about it should be handled in a scientific journal?
[00:41:26.700 --> 00:41:27.340]   Not at all.
[00:41:27.340 --> 00:41:28.780]   We have very little rules about it.
[00:41:28.780 --> 00:41:31.500]   It's mostly put all the stuff that you're
[00:41:31.500 --> 00:41:32.540]   thinking about in there.
[00:41:32.540 --> 00:41:37.620]   And then usually-- again, I feel like if you put too much
[00:41:37.620 --> 00:41:40.060]   process around it, it becomes too cumbersome.
[00:41:40.060 --> 00:41:42.380]   You want to let-- and every person
[00:41:42.380 --> 00:41:43.900]   has a slightly different workflow.
[00:41:43.940 --> 00:41:48.100]   Instead, if I go into a logbook of someone else
[00:41:48.100 --> 00:41:50.060]   I don't understand something, I add a comment
[00:41:50.060 --> 00:41:51.020]   and ask them about that.
[00:41:51.020 --> 00:41:52.100]   And they add that content.
[00:41:52.100 --> 00:41:57.060]   So we try to make it very agile in some sense.
[00:41:57.060 --> 00:42:00.940]   What is your-- do you have a in-person or a Slack stand-up
[00:42:00.940 --> 00:42:03.420]   kind of situation to discuss some of these?
[00:42:03.420 --> 00:42:06.340]   We usually have-- for some of the projects,
[00:42:06.340 --> 00:42:09.340]   we have weekly meetings where we discuss some of these.
[00:42:09.340 --> 00:42:12.540]   A lot of it happens on Slack.
[00:42:12.540 --> 00:42:16.540]   I would say 90% of those discussions happen on Slack.
[00:42:16.540 --> 00:42:22.580]   And then they kind of migrate into in-person discussions
[00:42:22.580 --> 00:42:26.020]   if it's so needed.
[00:42:26.020 --> 00:42:28.980]   And then if someone finds a breakthrough which
[00:42:28.980 --> 00:42:33.660]   stops other people or has to shift other people,
[00:42:33.660 --> 00:42:35.500]   what does that process look like?
[00:42:35.500 --> 00:42:38.580]   I think that's a case where we just--
[00:42:38.580 --> 00:42:41.260]   if that happens, it usually becomes very clear very quickly
[00:42:41.260 --> 00:42:42.260]   to everyone.
[00:42:42.260 --> 00:42:43.700]   And people feel like, why am I even
[00:42:43.700 --> 00:42:46.340]   working on this when this breakthrough happened?
[00:42:46.340 --> 00:42:48.380]   It's not like somebody discovers it in secret
[00:42:48.380 --> 00:42:50.700]   and we need to tell the whole team.
[00:42:50.700 --> 00:42:53.740]   So it happens pretty organically,
[00:42:53.740 --> 00:42:55.620]   either immediately if it's super obvious,
[00:42:55.620 --> 00:42:57.540]   or at the next meeting we just need to replan.
[00:42:57.540 --> 00:43:02.940]   The last thing, real quick, is--
[00:43:02.940 --> 00:43:05.260]   actually, the most important, I think, is if you're
[00:43:05.260 --> 00:43:09.420]   doing research, you really need to nurture your creativity.
[00:43:09.420 --> 00:43:11.980]   I feel like engineering requires a lot of creativity.
[00:43:11.980 --> 00:43:14.180]   But for research, it's like you're really
[00:43:14.180 --> 00:43:17.380]   trying to do things that nobody has been able to do before.
[00:43:17.380 --> 00:43:21.660]   And there's usually, for a lot of things,
[00:43:21.660 --> 00:43:23.460]   it's very unclear what's the right path.
[00:43:23.460 --> 00:43:26.620]   So you need to really be creative along the way.
[00:43:26.620 --> 00:43:29.900]   And I think this is something we can realize more and more,
[00:43:29.900 --> 00:43:31.220]   the importance of it at OpenAI.
[00:43:31.220 --> 00:43:32.780]   You need time to play.
[00:43:32.780 --> 00:43:36.100]   And if you structure your work a lot to make progress,
[00:43:36.100 --> 00:43:37.740]   like I was talking about here, it's
[00:43:37.740 --> 00:43:40.700]   very easy to get into a mode where you just optimize
[00:43:40.700 --> 00:43:42.900]   your milestone all the time, and you don't have time
[00:43:42.900 --> 00:43:44.740]   to really take a big picture and think,
[00:43:44.740 --> 00:43:47.820]   are there completely different ways we could be doing this?
[00:43:47.820 --> 00:43:52.260]   But since this is such an important part of research,
[00:43:52.260 --> 00:43:54.620]   you need to formalize it so that you really give people
[00:43:54.620 --> 00:43:56.940]   the space to be creative and learn new things.
[00:43:56.940 --> 00:43:59.140]   So one way we've done this at OpenAI
[00:43:59.140 --> 00:44:02.900]   that turned out to be very successful
[00:44:02.900 --> 00:44:05.740]   is that we have a weekly learning day.
[00:44:05.740 --> 00:44:09.380]   So every Thursday, this sign lights up in the office.
[00:44:09.380 --> 00:44:12.660]   And that day, everybody should be learning.
[00:44:12.660 --> 00:44:17.620]   And we take this extremely seriously.
[00:44:17.620 --> 00:44:19.580]   You shouldn't do any other work on this day.
[00:44:19.580 --> 00:44:22.540]   You should be learning, only learning.
[00:44:22.540 --> 00:44:25.540]   So we set up a set of rules for it.
[00:44:25.540 --> 00:44:29.020]   It's just one day per week.
[00:44:29.020 --> 00:44:32.780]   And if you don't use it, you lose it.
[00:44:32.780 --> 00:44:35.180]   So you're not allowed to have them roll over
[00:44:35.180 --> 00:44:36.860]   for multiple weeks and so on.
[00:44:36.860 --> 00:44:39.860]   You need to be learning every week.
[00:44:39.860 --> 00:44:40.980]   It must be related to work.
[00:44:40.980 --> 00:44:42.660]   So if you want to learn skydiving,
[00:44:42.660 --> 00:44:45.420]   that's not really what we want it for.
[00:44:45.420 --> 00:44:47.060]   It needs to be--
[00:44:47.060 --> 00:44:49.140]   it's really to make you do your work better.
[00:44:49.140 --> 00:44:50.420]   But that doesn't mean that it has
[00:44:50.420 --> 00:44:51.900]   to be directly applied to the problem you're
[00:44:51.900 --> 00:44:52.860]   working on at the time.
[00:44:52.860 --> 00:44:54.460]   It could be like, I feel like I'm
[00:44:54.460 --> 00:44:56.460]   rusty on my calculus foundation.
[00:44:56.460 --> 00:44:58.740]   Then that's something you might be learning that day.
[00:44:58.740 --> 00:45:01.020]   Because we believe if you spend the time
[00:45:01.020 --> 00:45:04.820]   to build the foundations, you will be able to just accelerate
[00:45:04.820 --> 00:45:06.580]   as a person.
[00:45:06.580 --> 00:45:09.180]   So a lot of people spend time on that.
[00:45:09.180 --> 00:45:11.820]   You basically spend the time reading papers or books.
[00:45:11.820 --> 00:45:12.700]   You implement things.
[00:45:12.700 --> 00:45:14.420]   You experiment with new ideas.
[00:45:14.420 --> 00:45:17.620]   This is the time we give people to really do that.
[00:45:17.620 --> 00:45:19.740]   But then the most important things here
[00:45:19.740 --> 00:45:22.340]   is you need to share what you've learned.
[00:45:22.340 --> 00:45:23.860]   So if you're doing an experiment,
[00:45:23.860 --> 00:45:26.360]   trying out some new idea, you need to tell people about it.
[00:45:26.360 --> 00:45:29.620]   So we have a Slack channel where we talk about this.
[00:45:29.620 --> 00:45:32.060]   If you read a paper, you summarize it.
[00:45:32.060 --> 00:45:36.860]   If you implement something, you talk about the experience
[00:45:36.860 --> 00:45:39.100]   you had implementing it.
[00:45:39.100 --> 00:45:40.940]   And we hold people accountable to this.
[00:45:40.940 --> 00:45:43.180]   So you have to say, what am I learning this day?
[00:45:43.180 --> 00:45:44.720]   And I think this is really important,
[00:45:44.720 --> 00:45:47.780]   because it means if we have somebody who
[00:45:47.780 --> 00:45:50.340]   is really good in reinforcement learning, and they say,
[00:45:50.340 --> 00:45:54.900]   well, today I'm actually reviewing a very basic text
[00:45:54.900 --> 00:45:57.840]   on reinforcement learning, that removes
[00:45:57.840 --> 00:46:01.820]   the stigma of going back and learning the foundations.
[00:46:01.820 --> 00:46:03.900]   And if you see that everybody else are doing this,
[00:46:03.900 --> 00:46:05.520]   you feel like it's OK to learn anything
[00:46:05.520 --> 00:46:10.440]   I want to do that I believe makes my work better.
[00:46:10.440 --> 00:46:12.320]   And so throughout the day, you need
[00:46:12.320 --> 00:46:15.480]   to update this channel about what have I learned so far?
[00:46:15.480 --> 00:46:16.360]   Am I changing?
[00:46:16.360 --> 00:46:18.240]   Maybe I'm bored of reading papers now.
[00:46:18.240 --> 00:46:19.900]   I'm going to do a bit of implementation.
[00:46:19.900 --> 00:46:21.560]   You actually have to say that.
[00:46:21.560 --> 00:46:24.640]   And the other part of this is if somebody is not
[00:46:24.640 --> 00:46:26.680]   learning something, maybe they're
[00:46:26.680 --> 00:46:29.000]   working on something, and that's not a good thing.
[00:46:29.000 --> 00:46:31.160]   They should be learning that day.
[00:46:31.160 --> 00:46:34.440]   It's just as important as the rest of their job
[00:46:34.440 --> 00:46:37.560]   is the learning part, because that's how we all just
[00:46:37.560 --> 00:46:39.840]   become better and level up.
[00:46:39.840 --> 00:46:42.600]   And so far, we've found that it's just paid off
[00:46:42.600 --> 00:46:45.040]   huge dividends.
[00:46:45.040 --> 00:46:49.080]   People are able to just be much faster at implementing things
[00:46:49.080 --> 00:46:51.880]   because they spent learning day just learning new frameworks,
[00:46:51.880 --> 00:46:55.440]   or learning new machine learning concepts.
[00:46:55.440 --> 00:46:59.160]   Overall, it's just a really good way
[00:46:59.160 --> 00:47:03.120]   for people to accelerate what they're doing,
[00:47:03.120 --> 00:47:06.000]   and it's been totally worth it.
[00:47:06.000 --> 00:47:08.600]   So if you're not doing something like this formally,
[00:47:08.600 --> 00:47:12.400]   I really encourage you to, because you're just
[00:47:12.400 --> 00:47:14.520]   going to be so much better as a team.
[00:47:14.520 --> 00:47:16.800]   And that's pretty much it.
[00:47:16.800 --> 00:47:20.440]   Just to summarize these five things,
[00:47:20.440 --> 00:47:22.360]   think about hypothesis and not tasks.
[00:47:22.360 --> 00:47:25.360]   Find a metric that you're optimizing as a team.
[00:47:25.360 --> 00:47:27.720]   Spend the time to improve your tooling
[00:47:27.720 --> 00:47:31.040]   and document your progress and to build out the map.
[00:47:31.040 --> 00:47:34.760]   And then last but not least, nurture that creativity
[00:47:34.760 --> 00:47:37.400]   so you can actually make progress.
[00:47:37.400 --> 00:47:38.640]   That's it for me.
[00:47:38.640 --> 00:47:39.140]   Thank you.
[00:47:39.140 --> 00:47:42.040]   [APPLAUSE]
[00:47:42.040 --> 00:47:43.960]   [VIDEO PLAYBACK]
[00:47:43.960 --> 00:47:45.360]   - I know this is hard to quantify,
[00:47:45.360 --> 00:47:47.400]   but I'm curious what you found was
[00:47:47.400 --> 00:47:52.440]   the right balance between diligently pursuing
[00:47:52.440 --> 00:47:56.240]   tasks and experiments versus just sort of blue sky
[00:47:56.240 --> 00:47:59.920]   and trying things you haven't thought of before.
[00:47:59.920 --> 00:48:02.720]   - So we try to pretty relentlessly
[00:48:02.720 --> 00:48:06.240]   do the first thing, and then we leave the learning
[00:48:06.240 --> 00:48:08.120]   day for the second thing.
[00:48:08.120 --> 00:48:11.600]   So if you have really blue sky things that are somewhat
[00:48:11.600 --> 00:48:13.680]   related to what you're doing, the learning day
[00:48:13.680 --> 00:48:15.280]   would be a really good time to do that.
[00:48:17.920 --> 00:48:22.120]   I think one philosophy that we have
[00:48:22.120 --> 00:48:25.680]   is as you're making progress on your problem,
[00:48:25.680 --> 00:48:28.200]   you will find a lot of what the open questions are,
[00:48:28.200 --> 00:48:29.680]   and you will get ideas to work out.
[00:48:29.680 --> 00:48:32.120]   So you don't necessarily need a lot of the really blue sky
[00:48:32.120 --> 00:48:32.600]   things.
[00:48:32.600 --> 00:48:36.440]   And I think this is also the difference from, say,
[00:48:36.440 --> 00:48:38.560]   the way we approach research from a lot of academia
[00:48:38.560 --> 00:48:40.880]   is that we just--
[00:48:40.880 --> 00:48:43.480]   the more blue sky stuff is what a lot of academia is doing.
[00:48:43.480 --> 00:48:45.200]   It's really good.
[00:48:45.200 --> 00:48:47.840]   It's not at all clear that we would be better at academia
[00:48:47.840 --> 00:48:48.560]   than that.
[00:48:48.560 --> 00:48:53.880]   So we take a more goal-oriented and engineering-oriented way
[00:48:53.880 --> 00:48:56.320]   to approach research because we think we can just--
[00:48:56.320 --> 00:48:59.360]   it's much easier for us to do that than in an academic lab,
[00:48:59.360 --> 00:49:00.480]   for example.
[00:49:00.480 --> 00:49:03.680]   So it's just a different kind of philosophy for us.
[00:49:03.680 --> 00:49:06.840]   Does that make sense?
[00:49:06.840 --> 00:49:13.040]   - What is the process of multi-day learning products?
[00:49:13.040 --> 00:49:15.520]   - Right now, we have no process for that, basically.
[00:49:15.520 --> 00:49:18.160]   I think the idea for that is if you're
[00:49:18.160 --> 00:49:20.280]   really passionate about it, you're probably
[00:49:20.280 --> 00:49:21.800]   going to work nights and weekends
[00:49:21.800 --> 00:49:22.880]   on that kind of stuff.
[00:49:22.880 --> 00:49:25.160]   Sometimes we have hack weeks.
[00:49:25.160 --> 00:49:28.080]   Sometimes if we feel like it's a really--
[00:49:28.080 --> 00:49:31.480]   often people use learning day to do some simple experiments
[00:49:31.480 --> 00:49:32.680]   to validate an approach.
[00:49:32.680 --> 00:49:35.200]   And if it seems to really align with the project,
[00:49:35.200 --> 00:49:36.680]   then that becomes a new hypothesis
[00:49:36.680 --> 00:49:39.280]   of something that might work.
[00:49:39.280 --> 00:49:40.960]   - Great.
[00:49:40.960 --> 00:49:42.400]   Yeah, let's actually wrap up here.
[00:49:42.400 --> 00:49:43.200]   Thanks again, Peter.
[00:49:43.200 --> 00:49:43.800]   - Thank you.
[00:49:43.800 --> 00:49:44.840]   [APPLAUSE]

