
[00:00:00.000 --> 00:00:05.240]   Sam Ortman announced in the last couple of hours that ChatGPT will start trying to assess whether
[00:00:05.240 --> 00:00:12.140]   you are a child and in some circumstances can flag conversations for review by parents and
[00:00:12.140 --> 00:00:17.200]   the authorities. For those of us who aren't children, ChatGPT will also sometimes begin
[00:00:17.200 --> 00:00:23.080]   flirting. This video then will give you the five minute TLDR on this announcement, which is not
[00:00:23.080 --> 00:00:29.800]   unique to ChatGPT by the way, as well as some other things Sam Ortman said this week that 99%
[00:00:29.800 --> 00:00:35.280]   of people may have missed, but a good chunk of those should hear. First, the classic corporation
[00:00:35.280 --> 00:00:42.340]   speak, which is that OpenAI are building toward a long-term system to understand whether someone
[00:00:42.340 --> 00:00:47.820]   is over or under 18. Unless I missed it, I can't find anywhere where they announced when this would
[00:00:47.820 --> 00:00:54.220]   occur or whether it starts as of today. One thing to immediately flag is that as of July, YouTube
[00:00:54.220 --> 00:00:59.600]   already does this based on the type of videos that you watch. Okay, but what will ChatGPT
[00:00:59.600 --> 00:01:04.600]   do if it assesses that you're a teen? Well, first of all, it won't flirt with you ever. And second of
[00:01:04.600 --> 00:01:10.120]   all, in extreme circumstances, depending on the discussion, it may contact law enforcement. You
[00:01:10.120 --> 00:01:16.000]   may of course have seen some recent very sad headlines about why they may have felt they needed
[00:01:16.000 --> 00:01:21.040]   to take this step. Like many of you, I think the goal is admirable. The question is, they better
[00:01:21.040 --> 00:01:26.520]   be really confident they're flagging the right conversations. Then comes a really key sentence.
[00:01:26.520 --> 00:01:31.520]   If we are not confident about someone's age or have incomplete information, we'll take
[00:01:31.520 --> 00:01:38.440]   the safer route and default to the under 18 experience and give adults ways to prove their age to unlock
[00:01:38.440 --> 00:01:44.520]   adult capabilities. In the next two weeks, we do know that there will be parental controls enabling
[00:01:44.520 --> 00:01:51.520]   parents to, for example, for teens, set blackout hours when a teen cannot use ChatGPT. Then, as
[00:01:51.520 --> 00:01:57.520]   before, if the system detects their teen is in a moment of acute distress, it will flag to the parent first
[00:01:57.520 --> 00:02:02.520]   and foremost, and then only afterwards to law enforcement. Again, I totally understand the
[00:02:02.520 --> 00:02:09.520]   motivation. I guess one thing I'd flag to OpenAI is, what happens if, like Twitter, a foreign country with
[00:02:09.520 --> 00:02:16.520]   different standards asks them and says, according to our law, you have to notify us when X occurs, when a user says Y
[00:02:16.520 --> 00:02:22.520]   about the government or does Z. Some tech companies cave into that, others don't, so only time will tell.
[00:02:22.520 --> 00:02:28.520]   The next two announcements, which came just an hour ago as our filming, might have been missed. OpenAI say that
[00:02:28.520 --> 00:02:34.520]   they want to give the same level of protection to your conversations with AI as you might have with your
[00:02:34.520 --> 00:02:40.520]   conversations to a doctor or to a lawyer. They say that people are increasingly turning to AI for sensitive
[00:02:40.520 --> 00:02:46.520]   questions and about their private concerns. Interestingly, we learned today exactly what percentage of people
[00:02:46.520 --> 00:02:52.520]   are turning to ChatGPT for, for different reasons. I spent about half an hour analyzing this image earlier, and
[00:02:52.520 --> 00:03:00.520]   it's pretty fascinating to see how people use ChatGPT. According to this, for the web version at least, only 4.2% are
[00:03:00.520 --> 00:03:08.520]   using it for coding. That compares to 10% to be tutored or taught something, and even 5.7% for fitness, beauty,
[00:03:08.520 --> 00:03:16.520]   self-care, or health advice. I was also kind of shocked how creating an image was a less used capability
[00:03:16.520 --> 00:03:22.520]   than translation. You may also notice that 0.4% of people just spend their time asking about the model.
[00:03:22.520 --> 00:03:28.520]   How are you? Are you conscious? That kind of thing. Back to the announcement though, so what could be the
[00:03:28.520 --> 00:03:34.520]   concerns about this greater level of privilege and privacy, it seems, for adults? My concerns are for
[00:03:34.520 --> 00:03:42.520]   startups, because OpenAI say we are advocating for this protection with policy makers. That is of course, therefore,
[00:03:42.520 --> 00:03:50.520]   the Trump administration. My concern is that if OpenAI gets a law passed, that any Chat with an AI system has to be
[00:03:50.520 --> 00:03:58.520]   protected by numerous layers of privilege and privacy and protection. While that sounds good initially, like needing to
[00:03:58.520 --> 00:04:04.520]   pass the bar to become a lawyer, it raises the bar literally for all startups and open source initiatives.
[00:04:04.520 --> 00:04:08.520]   Or at least it could force them to go through all sorts of hurdles. That's my concern.
[00:04:08.520 --> 00:04:14.520]   Yes, I know, OpenAI will claim that this is just about stopping the New York Times forcing them to retain
[00:04:14.520 --> 00:04:20.520]   user data indefinitely. I guess I'm just a tad skeptical about the possibility for regulatory capture.
[00:04:20.520 --> 00:04:26.520]   Another theory, by the way, is it could have been in response to the FDC launching an inquiry in America
[00:04:26.520 --> 00:04:32.520]   to quote "understand what steps, if any, companies have taken to evaluate the safety of their chatbots
[00:04:32.520 --> 00:04:38.520]   when acting as companions." Last announcement from this afternoon before I get to the quote from Sam Altman
[00:04:38.520 --> 00:04:42.520]   that many people will have missed. I think practically everyone, actually. This is regarding the question
[00:04:42.520 --> 00:04:50.520]   of when ChatGPT will flirt with you. And the answer is simple. If you ask for it, they should get it.
[00:04:50.520 --> 00:04:56.520]   I personally have never asked ChatGPT to flirt, but presumably some of you have and have seen the model refuse.
[00:04:56.520 --> 00:05:02.520]   Well, apparently it won't do so anymore. If the system then believes you're an adult, or you have been forced
[00:05:02.520 --> 00:05:08.520]   to provide ideas showing you're an adult, it will now also help you write a fictional story that involves,
[00:05:08.520 --> 00:05:15.520]   presumably, let's say extreme flirtation and a self-caused tragedy. I guess you could summarize all of these
[00:05:15.520 --> 00:05:22.520]   announcements is that they're great if we could perfectly trust OpenAI to implement them correctly.
[00:05:22.520 --> 00:05:28.520]   If those were the announcements, what was the quote that almost everyone missed? Well, first, for some context,
[00:05:28.520 --> 00:05:35.520]   here's what Sam Altman was telling lawmakers in the US in private in 2024.
[00:05:35.520 --> 00:05:40.520]   I want to talk a little bit about the workforce, but Mr. Altman, when we met last year in my office
[00:05:40.520 --> 00:05:48.520]   and had a great conversation, you said that upwards of 70% of jobs could be eliminated by AI,
[00:05:48.520 --> 00:05:53.520]   and you acknowledged the possible social disruption of this. If that's happening, we have to prepare for it.
[00:05:53.520 --> 00:05:57.520]   We're not going to stand in the way of the incredible opportunities here.
[00:05:57.520 --> 00:06:02.520]   Now, Sam Altman did not backtrack from that quote when speaking to the Senate,
[00:06:02.520 --> 00:06:06.520]   but here's an interview he did just a few days ago with Tucker Carlson. You'll see for yourself,
[00:06:06.520 --> 00:06:12.520]   but he implies that it could be towards the end of this century that the job ramifications fully play out.
[00:06:12.520 --> 00:06:16.520]   There's going to be massive displacement, and maybe those people will find something new and interesting
[00:06:16.520 --> 00:06:21.520]   and lucrative to do. But how big is that displacement, do you think?
[00:06:21.520 --> 00:06:27.520]   Someone told me recently that the historical average is about 50% of jobs significantly change.
[00:06:27.520 --> 00:06:31.520]   Maybe I don't totally go away, but significantly change every 75 years on average.
[00:06:31.520 --> 00:06:34.520]   That's the kind of, that's the half-life of the stuff.
[00:06:34.520 --> 00:06:42.520]   My controversial take would be that this is going to be like a punctuated equilibrium moment where a lot of that will happen in a short period of time.
[00:06:42.520 --> 00:06:46.520]   But if we zoom out, it's not going to be dramatically different than the historical rate.
[00:06:46.520 --> 00:06:55.520]   Like we'll do, we'll have a lot in this short period of time, and then it'll somehow be less total job turnover than we think.
[00:06:55.520 --> 00:06:59.520]   Some may say that's a case of giving different opinions in public versus in private.
[00:06:59.520 --> 00:07:02.520]   Others might say, well, he's just changed his mind.
[00:07:02.520 --> 00:07:06.520]   I've spent the last week or so preparing a video for my Patreon on this fascinating paper,
[00:07:06.520 --> 00:07:10.520]   Why Language Models Hallucidate. I'm trying to get an interview with the author.
[00:07:10.520 --> 00:07:17.520]   Essentially, it argued that we already have a literature on misclassification in machine learning models.
[00:07:17.520 --> 00:07:22.520]   Years before language models, we figured out why it's intractable how classifiers can sometimes get wrong.
[00:07:22.520 --> 00:07:24.520]   Is this a cat or is it a dog?
[00:07:24.520 --> 00:07:30.520]   What the paper does essentially is map the problem of classification to that of generation, i.e. language models.
[00:07:30.520 --> 00:07:36.520]   Because we force models to output one response rather than a probability distribution of responses,
[00:07:36.520 --> 00:07:39.520]   they're essentially forced to sometimes BS.
[00:07:39.520 --> 00:07:43.520]   Multiple choice benchmarks, like my own simple bench, are part of the problem
[00:07:43.520 --> 00:07:46.520]   because it always rewards guessing over saying I don't know.
[00:07:46.520 --> 00:07:49.520]   Solving that will be a socio-technical problem.
[00:07:49.520 --> 00:07:53.520]   But anyway, that was the massive summary. That's not the point of this video.
[00:07:53.520 --> 00:07:57.520]   But I wonder if Sam Ortman read this and is gradually adjusting his opinion.
[00:07:57.520 --> 00:08:01.520]   The paper was so thought-provoking for me and it'll be linked in the description.
[00:08:01.520 --> 00:08:07.520]   It created this grid on what the blockers are to the singularity and how they can be overcome.
[00:08:07.520 --> 00:08:11.520]   It kind of got carried away and created visualizations for each of the categories.
[00:08:11.520 --> 00:08:12.520]   But anyway, that's for another day.
[00:08:12.520 --> 00:08:17.520]   But I do want to be fair to Sam Ortman because he's often vilified for changing his mind.
[00:08:17.520 --> 00:08:19.520]   But he's not actually the only one.
[00:08:19.520 --> 00:08:23.520]   There's one figure who's almost universally praised and I respect him massively.
[00:08:23.520 --> 00:08:27.520]   He comes from my neck of the woods, Demis Asabis, and people say I sound like him.
[00:08:27.520 --> 00:08:31.520]   Of course, he's the Nobel Prize winning leader of Google DeepMind.
[00:08:31.520 --> 00:08:35.520]   But let's just say he's capable of his own about turns. It's not just Sam Ortman.
[00:08:35.520 --> 00:08:43.520]   Back in December of 2023, I think I was one of the only people to really focus on a particular quote from Demis Asabis.
[00:08:43.520 --> 00:08:47.520]   And you listened to the hype he attached to Gemini.
[00:08:47.520 --> 00:08:52.520]   I think this was Gemini 2 or 1.5 Pro beating human experts.
[00:08:52.520 --> 00:08:57.520]   We started seeing that Gemini was better than any other model out there on these very, very important benchmarks.
[00:08:57.520 --> 00:09:05.520]   For example, each of the 50 different subject areas that we tested on, it's as good as the best expert humans in those areas.
[00:09:05.520 --> 00:09:10.520]   Did you catch that? In 50 different domains, different subjects, better than human experts.
[00:09:10.520 --> 00:09:15.520]   Now, I debunked that quote both at the time and more recently, so I'm not going to debunk it again.
[00:09:15.520 --> 00:09:24.520]   But here Demis Asabis is, just the other day, dunking on competitors and how their leaders say that the models are as smart as experts.
[00:09:24.520 --> 00:09:26.520]   That's not something he would ever do, for sure.
[00:09:26.520 --> 00:09:33.520]   You often hear some of our competitors talk about, you know, these modern systems that we have today are PhD intelligences.
[00:09:33.520 --> 00:09:36.520]   I think that's a nonsense. They're not, they're not PhD intelligences.
[00:09:36.520 --> 00:09:41.520]   They have some capabilities that are PhD level, but they're not in general capable.
[00:09:41.520 --> 00:09:47.520]   And that's what exactly what general intelligence should be of performing across the board at the PhD level.
[00:09:47.520 --> 00:09:58.520]   In fact, as we all know, interacting with today's chatbots, if you pose the question in a certain way, they can make simple mistakes with even like high school maths and simple counting.
[00:09:58.520 --> 00:10:08.520]   Do you agree that that was a bit of an about face having earlier said that Gemini Ultra was as good as the best human experts in 50 different subjects?
[00:10:08.520 --> 00:10:15.520]   For me, this is a deceptively interesting time in AI for all of the reasons given in this video and far more.
[00:10:15.520 --> 00:10:21.520]   Almost every week, there is a significant improvement in how language models can help us code and do software engineering.
[00:10:21.520 --> 00:10:24.520]   And yes, I do think that's relevant even if you don't code.
[00:10:24.520 --> 00:10:32.520]   One of my goals for perhaps the end of this year or maybe next is to show how you can build a production level app just with the help of AI.
[00:10:32.520 --> 00:10:35.520]   That's even without a coding background, by the way.
[00:10:35.520 --> 00:10:45.520]   Now, I guess that would be creating your own job, but the more traditional path would be using a job board like this one from the sponsors of today's video, 80,000 hours.
[00:10:45.520 --> 00:10:51.520]   You can use my own link found in the description to take you to this job board, which is updated.
[00:10:51.520 --> 00:10:55.520]   I was going to say almost every day, but I think it's multiple times per day.
[00:10:55.520 --> 00:11:00.520]   These are real jobs from across the world, some of them remote and some of them in person.
[00:11:00.520 --> 00:11:03.520]   The focus is, of course, on having a positive impact.
[00:11:03.520 --> 00:11:10.520]   And the jobs, as you can see, are sourced not just from AI labs or universities, but also think tanks and other organizations.
[00:11:10.520 --> 00:11:14.520]   Again, if you want to check them out, do use the link in the description.
[00:11:14.520 --> 00:11:17.520]   So which of those for you was the most interesting announcement?
[00:11:17.520 --> 00:11:19.520]   For me, they're all interesting.
[00:11:19.520 --> 00:11:26.520]   From the deeply technical, like the hallucinations paper, to the deeply social and emotional, like the child protections.
[00:11:26.520 --> 00:11:31.520]   Thank you anyway for watching this brief recap, and as always, have a wonderful day.

