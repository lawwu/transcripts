
[00:00:00.000 --> 00:00:02.000]   I'm now recording.
[00:00:02.000 --> 00:00:04.000]   Jesus, you look terrible.
[00:00:04.000 --> 00:00:05.000]   What's going on? Did you sleep last night?
[00:00:05.000 --> 00:00:06.000]   Do I look tired?
[00:00:06.000 --> 00:00:07.000]   You do look a little tired.
[00:00:07.000 --> 00:00:08.000]   You look unshaven.
[00:00:08.000 --> 00:00:09.000]   Yeah, what happened last night?
[00:00:09.000 --> 00:00:12.000]   I had a holiday party at my house last night from my office and...
[00:00:12.000 --> 00:00:14.000]   You look destroyed.
[00:00:14.000 --> 00:00:16.000]   Yeah, what's going on? I mean, did you drink?
[00:00:16.000 --> 00:00:17.000]   I did drink, yeah.
[00:00:17.000 --> 00:00:19.000]   What does vodka and oat milk taste like?
[00:00:19.000 --> 00:00:21.000]   He had a White Russian!
[00:00:21.000 --> 00:00:25.000]   A White Russian Big Lebowski style with oat milk.
[00:00:25.000 --> 00:00:27.000]   He's like, "I want a White Russian with oat milk."
[00:00:27.000 --> 00:00:28.000]   Oh my God.
[00:00:28.000 --> 00:00:30.000]   Actually, my White Russian is made with oat leaf.
[00:00:30.000 --> 00:00:33.000]   And please compliment that with a huge punch in the face.
[00:00:33.000 --> 00:00:36.000]   Give me five minutes. I'm going to go shave and...
[00:00:36.000 --> 00:00:38.000]   It's called the hair of the dog.
[00:00:38.000 --> 00:00:39.000]   Yeah.
[00:00:39.000 --> 00:00:40.000]   You know, a little Bloody Mary.
[00:00:40.000 --> 00:00:41.000]   You banana bug.
[00:00:41.000 --> 00:00:42.000]   Banana man.
[00:00:42.000 --> 00:00:45.000]   Should I go get a beer? I'm going to get a beer just to beat this hangover.
[00:00:45.000 --> 00:00:46.000]   Go do it. Yeah, go get a beer.
[00:00:46.000 --> 00:00:47.000]   Hang on, I'll be right back.
[00:00:47.000 --> 00:00:48.000]   I'll be right back.
[00:00:48.000 --> 00:01:09.000]   All right, listen, we have to start with scam, bank run, fraud.
[00:01:09.000 --> 00:01:11.000]   I mean, Sam Bankvin Freed.
[00:01:11.000 --> 00:01:13.000]   I get that wrong sometimes.
[00:01:13.000 --> 00:01:19.000]   He was interviewed by Andrew Orr Sorkin, the suit at Dealbook,
[00:01:19.000 --> 00:01:21.000]   who gave him softball after softball.
[00:01:21.000 --> 00:01:24.000]   Then, the next day, he was on Good Morning America.
[00:01:24.000 --> 00:01:26.000]   You're being really passive aggressive right now.
[00:01:26.000 --> 00:01:27.000]   A little bit.
[00:01:27.000 --> 00:01:29.000]   You're throwing a little shade at our friend Andrew Orr.
[00:01:29.000 --> 00:01:30.000]   Little chippy.
[00:01:30.000 --> 00:01:31.000]   He's sticking the knife in.
[00:01:31.000 --> 00:01:32.000]   He's sticking the knife in.
[00:01:32.000 --> 00:01:33.000]   But J. Cao's got a point.
[00:01:33.000 --> 00:01:34.000]   He's got a point.
[00:01:34.000 --> 00:01:35.000]   A little bit.
[00:01:35.000 --> 00:01:36.000]   A little bit.
[00:01:36.000 --> 00:01:37.000]   A little bit.
[00:01:37.000 --> 00:01:40.000]   Anyway, the New York Times continues to embarrass themselves
[00:01:40.000 --> 00:01:42.000]   by handling Sam Bankvin Fraud with kid gloves.
[00:01:42.000 --> 00:01:43.000]   Wow.
[00:01:43.000 --> 00:01:48.000]   George Stephanopoulos, my Greek brother, Stephanopoulos the Spartan,
[00:01:48.000 --> 00:01:53.000]   came in and absolutely fricassed and filleted Sam Bankvin Freed
[00:01:53.000 --> 00:01:55.000]   on Good Morning America.
[00:01:55.000 --> 00:01:58.000]   Very important to note that Good Morning America,
[00:01:58.000 --> 00:02:04.000]   the segment was between holiday cocktails and the cast of The White Lotus.
[00:02:04.000 --> 00:02:07.000]   And Andrew Orr Sorkin was at the Dealbook finance conference.
[00:02:07.000 --> 00:02:10.000]   But you know, that Stephanopoulos interview was two hours,
[00:02:10.000 --> 00:02:12.000]   but they reduced it down to 10 minutes.
[00:02:12.000 --> 00:02:18.000]   So there were probably a lot of sort of cordial conversation,
[00:02:18.000 --> 00:02:20.000]   banter, and then softball questions.
[00:02:20.000 --> 00:02:24.000]   And then he does stick the knife in and does the fricassee,
[00:02:24.000 --> 00:02:26.000]   but he cuts out all the other stuff.
[00:02:26.000 --> 00:02:28.000]   So he just gets it down to the 10 minutes.
[00:02:28.000 --> 00:02:33.000]   What Stephanopoulos did to him was extraordinary in that he said over
[00:02:33.000 --> 00:02:36.000]   and over again, in the FTX terms of service,
[00:02:36.000 --> 00:02:41.000]   you cannot touch the user accounts, but you at Alameda were taking them
[00:02:41.000 --> 00:02:43.000]   and you were loaning them out.
[00:02:43.000 --> 00:02:48.000]   I don't know who is advising Sam Bankvin Freed at this point,
[00:02:48.000 --> 00:02:50.000]   but why he is talking so much.
[00:02:50.000 --> 00:02:56.000]   He was also on a two-hour Twitter spaces after all this.
[00:02:56.000 --> 00:03:01.000]   At this point, Sax, what do you think is going on here in the mind
[00:03:01.000 --> 00:03:04.000]   of Sam Bankvin Freed and also the media,
[00:03:04.000 --> 00:03:10.000]   which seems to have a very variable way of dealing with this obvious fraud
[00:03:10.000 --> 00:03:11.000]   and crime?
[00:03:11.000 --> 00:03:15.000]   Right. Okay. Well, you know, I can speculate about SBF.
[00:03:15.000 --> 00:03:18.000]   I think if there is a strategy here, it is this.
[00:03:18.000 --> 00:03:25.000]   He is basically copying to criminal negligence in order to avoid the more
[00:03:25.000 --> 00:03:26.000]   serious charges of fraud.
[00:03:26.000 --> 00:03:29.000]   And I think, again, if there's a strategy here, it is,
[00:03:29.000 --> 00:03:34.000]   he saw himself being defined as Bernie Madoff 2.0 in the press.
[00:03:34.000 --> 00:03:41.000]   And if that image, which may well be true, cemented around him,
[00:03:41.000 --> 00:03:43.000]   then prosecutors would never stop.
[00:03:43.000 --> 00:03:48.000]   They would never accept a plea that basically gave him anything less than
[00:03:48.000 --> 00:03:52.000]   a Madoff-like sentence, which would be decades in prison,
[00:03:52.000 --> 00:03:53.000]   maybe a life sentence.
[00:03:53.000 --> 00:03:57.000]   So he is out there doing what lawyers would tell you never to do,
[00:03:57.000 --> 00:04:00.000]   which is basically incriminate yourself, create more of a record,
[00:04:00.000 --> 00:04:03.000]   but he's doing it to change the public perception,
[00:04:03.000 --> 00:04:08.000]   maybe muddy up the public perception, get people thinking that, okay,
[00:04:08.000 --> 00:04:11.000]   he, you know, this, that he's admitting he did something wrong,
[00:04:11.000 --> 00:04:13.000]   but it wasn't deliberate. It wasn't fraudulent.
[00:04:13.000 --> 00:04:17.000]   It was just basically carelessness or sloppiness.
[00:04:17.000 --> 00:04:20.000]   And if he succeeds in muddying the waters enough,
[00:04:20.000 --> 00:04:23.000]   then maybe the prosecutors will give him a plea deal that allows him to have
[00:04:23.000 --> 00:04:24.000]   his life back at some point.
[00:04:24.000 --> 00:04:29.000]   I think that would be the crazy, like a Fox explanation of what's happening.
[00:04:29.000 --> 00:04:32.000]   Now there is, you know, an alternative explanation as well,
[00:04:32.000 --> 00:04:37.000]   which is, I just think that these types of guys, you could call it, you know,
[00:04:37.000 --> 00:04:39.000]   a narcissistic fraudster.
[00:04:39.000 --> 00:04:43.000]   They think they can talk their way out of anything, you know,
[00:04:43.000 --> 00:04:46.000]   because they have, they have, you know,
[00:04:46.000 --> 00:04:50.000]   they've talked their way into getting hundreds of millions of dollars of
[00:04:50.000 --> 00:04:54.000]   investment, billions, billions in some cases.
[00:04:54.000 --> 00:04:58.000]   And so they just feel, and they've been trained by employees,
[00:04:58.000 --> 00:05:01.000]   partners, investors, the press that they can talk their way out of.
[00:05:01.000 --> 00:05:03.000]   Yeah. Well, I think, yeah,
[00:05:03.000 --> 00:05:06.000]   the average person is not really used to dealing with one of these
[00:05:06.000 --> 00:05:09.000]   personality types who is, I mean,
[00:05:09.000 --> 00:05:12.000]   they clearly are smart and they're articulate articulate and they know what
[00:05:12.000 --> 00:05:14.000]   to say and they're crafting their words.
[00:05:14.000 --> 00:05:18.000]   What they've learned through their life is that if they use the precise magic
[00:05:18.000 --> 00:05:21.000]   words with a person, they can pretty much convince them of anything,
[00:05:21.000 --> 00:05:23.000]   get them to do anything. And in particular,
[00:05:23.000 --> 00:05:25.000]   I would say investors tend to fall for this,
[00:05:25.000 --> 00:05:27.000]   not because investors are dumb,
[00:05:27.000 --> 00:05:31.000]   but because investors are so clear about what they're looking for and what
[00:05:31.000 --> 00:05:34.000]   they want. They're predictable. Yeah. Like we're, you know, VCs,
[00:05:34.000 --> 00:05:36.000]   especially we're looking for the a hundred X outcome or whatever.
[00:05:36.000 --> 00:05:43.000]   So it's easy for this type of personality to construct a story to essentially
[00:05:43.000 --> 00:05:46.000]   stroke the erogenous zones of a VC. Whoa. Yeah.
[00:05:46.000 --> 00:05:48.000]   And, and, and sort of trick them.
[00:05:48.000 --> 00:05:52.000]   And so there is probably a positive reinforcement loop that gets created in
[00:05:52.000 --> 00:05:54.000]   the minds of one of these people who,
[00:05:54.000 --> 00:05:57.000]   and they start to think that they can basically talk their way out of any
[00:05:57.000 --> 00:06:01.000]   situation. So I think that would be part of what's going on here.
[00:06:01.000 --> 00:06:02.000]   And, you know,
[00:06:02.000 --> 00:06:06.000]   if you look at sort of the tactics that he's using to do this, you know,
[00:06:06.000 --> 00:06:09.000]   all of a sudden he's trying to portray himself, you know,
[00:06:09.000 --> 00:06:12.000]   before this he was portraying himself as the smartest guy in the room.
[00:06:12.000 --> 00:06:16.000]   Now all of a sudden there's this babe in the woods impression where I didn't
[00:06:16.000 --> 00:06:20.000]   know it was my subordinates. I wasn't really in control.
[00:06:20.000 --> 00:06:22.000]   That was somebody else. Right.
[00:06:22.000 --> 00:06:26.000]   Each individual decision he says looked sensible to him.
[00:06:26.000 --> 00:06:29.000]   It's just, it all added up to something he didn't anticipate. Like, really,
[00:06:29.000 --> 00:06:30.000]   you know,
[00:06:30.000 --> 00:06:35.000]   loaning yourself a billion dollars of, of basically the company's money,
[00:06:35.000 --> 00:06:37.000]   which was basically customer money. That seemed reasonable to you.
[00:06:37.000 --> 00:06:39.000]   I don't know how you defend that individual decision.
[00:06:39.000 --> 00:06:41.000]   And there's many like that, but,
[00:06:41.000 --> 00:06:44.000]   but this is sort of the narrative that he's trying to construct.
[00:06:44.000 --> 00:06:46.000]   And let me stop there,
[00:06:46.000 --> 00:06:50.000]   but I think there's a lot more that can be said to dismantle the narrative
[00:06:50.000 --> 00:06:53.000]   he's trying to create, but I want to let other people get in here.
[00:06:53.000 --> 00:06:54.000]   What's your take on this? And then Freiburg,
[00:06:54.000 --> 00:06:59.000]   can I make a comparison to SBF and Trump through the lens of the media?
[00:06:59.000 --> 00:07:03.000]   So if you go back to 2016, you know,
[00:07:03.000 --> 00:07:08.000]   Donald Trump violated every single establishment bias in the world.
[00:07:08.000 --> 00:07:13.000]   Every single establishment bias that these left progressive journalist
[00:07:13.000 --> 00:07:18.000]   elites had. And so they basically just attacked, attacked, attacked,
[00:07:18.000 --> 00:07:19.000]   attacked,
[00:07:19.000 --> 00:07:24.000]   but then you went into the election and there was a very clear data point
[00:07:24.000 --> 00:07:29.000]   that said, whatever you thought was that was at best limited.
[00:07:29.000 --> 00:07:34.000]   And you missed the tone of the country because 50 plus percent of the country
[00:07:34.000 --> 00:07:37.000]   held a very different view about this person.
[00:07:37.000 --> 00:07:41.000]   And instead of taking a step back and then the left media,
[00:07:41.000 --> 00:07:46.000]   the mainstream media re-underwriting and learning and then saying,
[00:07:46.000 --> 00:07:48.000]   you know what, mea culpa, I got this wrong.
[00:07:48.000 --> 00:07:51.000]   They just double down and they said, no,
[00:07:51.000 --> 00:07:53.000]   it still doesn't meet our priors.
[00:07:53.000 --> 00:07:56.000]   And so we're just going to ring fence this problem.
[00:07:56.000 --> 00:08:00.000]   And we're going to just try to destroy this issue because, you know,
[00:08:00.000 --> 00:08:04.000]   we want to control the narrative and, and, and by result,
[00:08:04.000 --> 00:08:08.000]   we want to control power. Now you look at SBF, it's the exact opposite.
[00:08:08.000 --> 00:08:12.000]   He went to the perfect elite private high school.
[00:08:12.000 --> 00:08:17.000]   Then he went to one of the most prestigious elite private universities,
[00:08:17.000 --> 00:08:22.000]   MIT, his parents teach governance of all things at one of the most elite
[00:08:22.000 --> 00:08:25.000]   liberal institutions in America, Stanford,
[00:08:25.000 --> 00:08:30.000]   they are in the establishment of the progressive left.
[00:08:30.000 --> 00:08:35.000]   And what happened was he took customer funds and all of this money.
[00:08:35.000 --> 00:08:39.000]   He made tens of millions of dollars of political donations.
[00:08:39.000 --> 00:08:43.000]   He wrapped himself in this blanket of a progressive left-leaning cause
[00:08:43.000 --> 00:08:45.000]   called effective altruism.
[00:08:45.000 --> 00:08:50.000]   And all of the mainstream media fell for it and embraced him as well as some
[00:08:50.000 --> 00:08:57.000]   politicians, because it met everything that they themselves also bought into.
[00:08:57.000 --> 00:09:00.000]   And now you have this cataclysmic event,
[00:09:00.000 --> 00:09:04.000]   a multi-deck a billion dollar fraud or bankruptcy,
[00:09:04.000 --> 00:09:08.000]   millions of customer accounts who are frozen, you know,
[00:09:08.000 --> 00:09:12.000]   tens of millions to hundreds of millions to billions of dollars lost and stolen
[00:09:12.000 --> 00:09:16.000]   from them. And they refuse to re-underwrite this kid.
[00:09:16.000 --> 00:09:20.000]   And the reason is because in order to do so, it's like eating your own tail.
[00:09:20.000 --> 00:09:22.000]   And that's why they don't want to do it.
[00:09:22.000 --> 00:09:27.000]   And so this is why you have the media basically allowing him to do an apology
[00:09:27.000 --> 00:09:32.000]   tour. Now, this is his second time at manipulating them.
[00:09:32.000 --> 00:09:36.000]   The first time he was able to manipulate them by basically being one of them.
[00:09:36.000 --> 00:09:42.000]   And now he's allowing them and their desire to basically protect themselves
[00:09:42.000 --> 00:09:46.000]   so that he can create some kind of a defense for himself.
[00:09:46.000 --> 00:09:50.000]   And I just think the whole thing is gross because it misses the entire mood of
[00:09:50.000 --> 00:09:58.000]   the nation. This is an enormous financial fraud that was perpetrated on tens of
[00:09:58.000 --> 00:10:03.000]   millions of people. And there's no accountability because in order to do so,
[00:10:03.000 --> 00:10:06.000]   the media would effectively have to admit that they missed it and they got it
[00:10:06.000 --> 00:10:09.000]   wrong and they refused to do it. And I think that that is the really big
[00:10:09.000 --> 00:10:14.000]   problem that nobody is really speaking out about is like, well, if these folks
[00:10:14.000 --> 00:10:19.000]   are meant to be the last stop to make sure that there's truth and honesty and
[00:10:19.000 --> 00:10:23.000]   transparency in society and you can't count on them. And in fact, they're just
[00:10:23.000 --> 00:10:26.000]   going to reflect their own narrative. What is one supposed to do to learn the
[00:10:26.000 --> 00:10:28.000]   truth?
[00:10:28.000 --> 00:10:32.000]   In a way, what you're saying, and then we'll go to you, Freiburg, is this fraud
[00:10:32.000 --> 00:10:38.000]   was encased in all the gilded facade that America hates right now.
[00:10:38.000 --> 00:10:42.000]   It reflects the institutional rot of America. It reflects every single aspect
[00:10:42.000 --> 00:10:48.000]   of institutional rot that every non elite talks about all the time. But elites
[00:10:48.000 --> 00:10:52.000]   when they have those labels will refuse to give up.
[00:10:52.000 --> 00:10:57.000]   And just to add to that, the thing that's missing, I'd say one of the big issues
[00:10:57.000 --> 00:11:00.000]   with the institutional rot in our country is the lack of accountability when
[00:11:00.000 --> 00:11:05.000]   somebody gets it wrong. We saw this with COVID, right? The health establishment
[00:11:05.000 --> 00:11:09.000]   is saying that they want amnesty and Atlantic magazine was willing to give it
[00:11:09.000 --> 00:11:14.000]   to them. So the point is that this this class of people think that when they get
[00:11:14.000 --> 00:11:16.000]   it wrong, that they're the experts, but when they get it wrong, there should be
[00:11:16.000 --> 00:11:20.000]   no accountability. And so, Jamath, to your point, the media and these
[00:11:20.000 --> 00:11:25.000]   institutions are not willing to re underwrite SBF when he so clearly as a
[00:11:25.000 --> 00:11:27.000]   fraudster.
[00:11:27.000 --> 00:11:32.000]   Freiburg, what do you think the of this theory, you had a large amount of
[00:11:32.000 --> 00:11:37.000]   donations to politicians. Obviously, you have coming from Stanford, MIT, etc.
[00:11:37.000 --> 00:11:45.000]   And then you have these investments, gifts slash advertising slash donations
[00:11:45.000 --> 00:11:50.000]   to ProPublica, Vox, this new publication, Semaphore, the intercept that have all
[00:11:50.000 --> 00:11:56.000]   been uncovered now. Did he do this paying off of all the elites, you know,
[00:11:56.000 --> 00:12:00.000]   splashy cashy giving money to everybody because he knew he was doing a fraud and
[00:12:00.000 --> 00:12:05.000]   that this is evidence of this is a premeditated fraud? Or do you think this
[00:12:05.000 --> 00:12:08.000]   is a deranged individual who just was seeking status?
[00:12:08.000 --> 00:12:13.000]   I don't know. The the motivation, there's a video you can watch this guy.
[00:12:13.000 --> 00:12:19.000]   For some reason, FT x has left up all of their videos on YouTube from three
[00:12:19.000 --> 00:12:24.000]   years ago called quantitative trading seven and a half thousand cell wall of
[00:12:24.000 --> 00:12:30.000]   Bitcoin on finance. It's a 17 and a half minute YouTube video of SBF trading
[00:12:30.000 --> 00:12:36.000]   arbitrage across markets. I think it provides probably the best like natural
[00:12:36.000 --> 00:12:42.000]   non scripted insight into this guy's behavior that you could see.
[00:12:42.000 --> 00:12:45.000]   You know, because it's not like him being interviewed. It's just him living
[00:12:45.000 --> 00:12:50.000]   in his world. And he's just, you know, a mouse trying to get a piece of cheese.
[00:12:50.000 --> 00:12:55.000]   Like, you know, he's like out there. He's, you know, scrambling around in the
[00:12:55.000 --> 00:12:59.000]   markets, he's finding edges, he's finding advantages, and he's, and he's
[00:12:59.000 --> 00:13:03.000]   clearly just taking advantage of them all day, every day. That's who he is.
[00:13:03.000 --> 00:13:08.000]   Now, you put a person like that, in an unregulated environment, and there was
[00:13:08.000 --> 00:13:13.000]   this clustering demand for an unregulated environment because of a lot of what
[00:13:13.000 --> 00:13:18.000]   you guys are saying, which is people have this disdain for the elitism and the
[00:13:18.000 --> 00:13:23.000]   and the institutional rot and all these things. So Bitcoin emerged as a solution
[00:13:23.000 --> 00:13:31.000]   out of 2008 to the you know, the what felt like institutional rot that
[00:13:31.000 --> 00:13:36.000]   governments have a key role in. But when you have no regulation, and you have no
[00:13:36.000 --> 00:13:40.000]   trusted central authority involved, mice that are trying to find cheese will rule
[00:13:40.000 --> 00:13:44.000]   the day. And I think that's what happened here. If it wasn't this guy, it was
[00:13:44.000 --> 00:13:47.000]   someone else, it was going to be someone else. And then all of a sudden,
[00:13:47.000 --> 00:13:49.000]   everyone's clamoring and saying, Hey, we needed the government to protect us. No
[00:13:49.000 --> 00:13:53.000]   one protected us. Someone's got to save us. We're the regulators, we're people
[00:13:53.000 --> 00:13:57.000]   that are supposed to keep an eye on this stuff, when the whole premise of so much
[00:13:57.000 --> 00:14:02.000]   of what was being sold was non regulatory regimes was openness was peer to peer
[00:14:02.000 --> 00:14:07.000]   trust protocols. And it turns out that in that sort of an environment, the mouse
[00:14:07.000 --> 00:14:10.000]   that is hungriest for the cheese will get the cheese. And that's exactly what
[00:14:10.000 --> 00:14:13.000]   happened. I don't know how much of it was I don't agree with him saying I'm
[00:14:13.000 --> 00:14:17.000]   creating intentional fraud, which certainly seems to be the case, versus
[00:14:17.000 --> 00:14:19.000]   him saying I'm going to pay these guys. I don't know how much it was even that
[00:14:19.000 --> 00:14:23.000]   intelligent. But the guy was clearly like, trying to get a piece of cheese.
[00:14:23.000 --> 00:14:30.000]   Okay, so this cheese eater, this rat is a League of Legends. You know, expert
[00:14:30.000 --> 00:14:34.000]   playing on eight different monitors at a time, the cryptocurrency game while
[00:14:34.000 --> 00:14:39.000]   hopped up on speed. Yeah, I'm not saying that to be cruel. I'm saying that
[00:14:39.000 --> 00:14:44.000]   because they admitted it. They talked about it in their staff meetings,
[00:14:44.000 --> 00:14:49.000]   instructing their traders and team members of how to take speed. He admitted
[00:14:49.000 --> 00:14:52.000]   to it in an interview this week. He said that it was all legal prescription
[00:14:52.000 --> 00:14:56.000]   drugs, but they were taking them. Yes, yes. And literally in the same videos
[00:14:56.000 --> 00:15:00.000]   you're referencing people see speed patches, or I don't even understand this.
[00:15:00.000 --> 00:15:03.000]   But there are patches you can put on your body to deliver speed to you at
[00:15:03.000 --> 00:15:09.000]   some, you know, dose or whatever. Saks you buy this theory? No care that this
[00:15:09.000 --> 00:15:14.000]   isn't about the elite side of it. It's about the non elite, the anarchy side of
[00:15:14.000 --> 00:15:19.000]   it. No. And this cheese eating rat was just wanted to eat more cheese. I don't
[00:15:19.000 --> 00:15:24.000]   buy this narrative because I see too much design intentionality, but time
[00:15:24.000 --> 00:15:29.000]   what happened. So in other words, it wasn't just a series of individual
[00:15:29.000 --> 00:15:33.000]   decisions that didn't add up. Many of those individual decisions by themselves
[00:15:33.000 --> 00:15:38.000]   were totally unjustifiable. And moreover, there were too many. There's too
[00:15:38.000 --> 00:15:43.000]   much evidence of sophisticated behavior here. Again, we overnight went from
[00:15:43.000 --> 00:15:46.000]   portraying himself as the smartest guy in the room to the to the babe in the
[00:15:46.000 --> 00:15:51.000]   woods. And so, for example, when you look at the construction of all these
[00:15:51.000 --> 00:15:55.000]   entities and the corporate org chart, you know, of all the related entities,
[00:15:55.000 --> 00:16:01.000]   it's a very sophisticated attempt to obscure and construct certain, you know,
[00:16:01.000 --> 00:16:07.000]   protections. When you look at the way that Alameda was exempted from the
[00:16:07.000 --> 00:16:11.000]   normal margin requirements and FTX, there was the so called backdoors. There
[00:16:11.000 --> 00:16:14.000]   was intentionality there there was intentionality in terms of who was
[00:16:14.000 --> 00:16:19.000]   hired to staff these organizations. Again, they wasn't hired. No board, no
[00:16:19.000 --> 00:16:24.000]   CFO. Yeah, exactly. And the guy who was in charge of compliance was like
[00:16:24.000 --> 00:16:28.000]   tomorrow talked about in previous episode was the guy who was involved in
[00:16:28.000 --> 00:16:33.000]   the ultimate bet poker cheating scandal. You know, not exactly. Yeah,
[00:16:33.000 --> 00:16:37.000]   exactly. Right. Or look at this goofy goofball, Caroline Ellison, who was
[00:16:37.000 --> 00:16:41.000]   put in charge of Alameda, right? His girlfriend. It's it's being done for a
[00:16:41.000 --> 00:16:45.000]   reason, right? He's setting it up in a certain way. And, you know, the one
[00:16:45.000 --> 00:16:49.000]   time I interacted with him at this tech conference, he was sitting there
[00:16:49.000 --> 00:16:52.000]   holding court and he had all of his minions around him who were following
[00:16:52.000 --> 00:16:58.000]   his orders. This was a guy who was controlling his business. He was making
[00:16:58.000 --> 00:17:03.000]   the decisions at, I think, a task level. And he knew exactly what was going on
[00:17:03.000 --> 00:17:10.000]   here. So, look, I just don't buy I do not buy this idea that that he was like
[00:17:10.000 --> 00:17:15.000]   a blind mouse who's just stimulus response, you know, in the moment.
[00:17:15.000 --> 00:17:20.000]   That wasn't that wasn't my point, sex. My point was whether it was him or it
[00:17:20.000 --> 00:17:24.000]   was going to be someone else, it was bound to happen. No, I don't agree
[00:17:24.000 --> 00:17:28.000]   with that criminal would have the idea that we want to have completely free
[00:17:28.000 --> 00:17:33.000]   unregulated Bahamian based trading, you know, environments that we can
[00:17:33.000 --> 00:17:37.000]   supposedly trust because someone puts on a good face when there is no real
[00:17:37.000 --> 00:17:41.000]   regulatory body and regulatory authority overseeing it. At some point, it was
[00:17:41.000 --> 00:17:46.000]   going to happen. Well, no, hold on. It is. Look, Coinbase is a fully regulated
[00:17:46.000 --> 00:17:49.000]   institution. They're not set up in the Bahamas, and they're not unregulated.
[00:17:49.000 --> 00:17:53.000]   He set up and he had no oversight. He had no board. There was no regulatory
[00:17:53.000 --> 00:17:56.000]   regime. There was nothing. How was he able? But okay, so first of all, look,
[00:17:56.000 --> 00:18:00.000]   I don't think this had to happen. I think that again, I think that excuses
[00:18:00.000 --> 00:18:03.000]   too much because it implies that if it wasn't SPF would be somebody else. I
[00:18:03.000 --> 00:18:07.000]   actually think that this was a highly concerted effort. Listen, he courted
[00:18:07.000 --> 00:18:12.000]   regulators. He donated to politicians. He courted the media and donated to
[00:18:12.000 --> 00:18:17.000]   the media. Yeah, he was really good at it. He was unusually good at it. No,
[00:18:17.000 --> 00:18:20.000]   and I totally agree on that. Yeah, super smart, super connected, really
[00:18:20.000 --> 00:18:24.000]   thoughtful design on how he committed this. I only think an insider could
[00:18:24.000 --> 00:18:27.000]   have pulled off something at this scale. I think I agree. This is where
[00:18:27.000 --> 00:18:30.000]   Chamath, you're exactly right. I think you needed to be an insider. This level
[00:18:30.000 --> 00:18:34.000]   of cynicism here is he knew the playbook and he admitted it. You pointed this
[00:18:34.000 --> 00:18:38.000]   out with the chats that were released where he said he he he he. Yeah, look,
[00:18:38.000 --> 00:18:42.000]   I mean, just like he he chat, look how like convoluted and intertwined. All
[00:18:42.000 --> 00:18:46.000]   these people are like Gensler's intertwined with the parents. Yeah,
[00:18:46.000 --> 00:18:50.000]   parents apparently bundled a bunch of money to Elizabeth Warren. You know, he
[00:18:50.000 --> 00:18:54.000]   was dating the CEO of the business that he owned 90% in. There are all these
[00:18:54.000 --> 00:18:58.000]   other random shell companies that he owned 100% of where they were lending
[00:18:58.000 --> 00:19:02.000]   back and forth hundreds of millions to billions of dollars. Yeah, to David's
[00:19:02.000 --> 00:19:07.000]   point, that is a sophisticated con that you have to architect and and
[00:19:07.000 --> 00:19:11.000]   the way that he was able to get away with it is that not a single reporter
[00:19:11.000 --> 00:19:18.000]   or regulator thought to dig in. And the reason I think is because he said all
[00:19:18.000 --> 00:19:23.000]   of the right things that wanted them to embrace him. And the reason is he
[00:19:23.000 --> 00:19:26.000]   admitted it. This is a dumb game that we woke Westerners have to play. I say
[00:19:26.000 --> 00:19:29.000]   the right shibboleth and that everyone thinks we're a good person. Exactly.
[00:19:29.000 --> 00:19:32.000]   Imagine pull that quote up on what he said because it's actually a good
[00:19:32.000 --> 00:19:38.000]   it was what I just said. It's the game that we woke Westerners have to play
[00:19:38.000 --> 00:19:41.000]   and we say the right shibboleth or everyone likes us. He actually said the
[00:19:41.000 --> 00:19:45.000]   most uncomfortable thing out loud, which is look by having gone to Crystal
[00:19:45.000 --> 00:19:49.000]   Springs High School by having professors, my parents that went to Stanford by
[00:19:49.000 --> 00:19:54.000]   having gone to MIT. I can pull this off. That's that's what he said because he
[00:19:54.000 --> 00:19:58.000]   can go with those things because I'm a I'm a champion of effective altruism
[00:19:58.000 --> 00:20:03.000]   that I can justify any of these decisions, how amoral or immoral that they
[00:20:03.000 --> 00:20:07.000]   be because I'm trying to help, you know, my brother stand up a multi-million
[00:20:07.000 --> 00:20:10.000]   dollar pandemic response business. I'm trying to do this. I'm trying to do
[00:20:10.000 --> 00:20:16.000]   that. And all of these regulators and all of these reporters said, okay, you
[00:20:16.000 --> 00:20:22.000]   get the hall pass. Now imagine if you replaced him with some random kid in
[00:20:22.000 --> 00:20:28.000]   some developing country or even from the United States who did went to public
[00:20:28.000 --> 00:20:32.000]   high school who went to some random state school. Do you think that they
[00:20:32.000 --> 00:20:36.000]   could have pulled any of this stuff off? No, you need the patina of the
[00:20:36.000 --> 00:20:40.000]   privilege class. The New York Times what he had the New York Times the
[00:20:40.000 --> 00:20:45.000]   privilege class even after this fraud, the New York Times wrote more of a puff
[00:20:45.000 --> 00:20:48.000]   piece on him than the hit piece. They wrote on Brian Armstrong last year when
[00:20:48.000 --> 00:20:52.000]   Brian Armstrong wouldn't toe the line on allowing politics at work. Remember
[00:20:52.000 --> 00:20:56.000]   that? Yeah, unbelievable. Yeah. So the big I think what your mouth is kind of
[00:20:56.000 --> 00:21:00.000]   saying here is that the big enabler here is not crypto per se. It's all these
[00:21:00.000 --> 00:21:05.000]   institutional biases and elite biases that he was able to play into, partly
[00:21:05.000 --> 00:21:09.000]   because he was a big insider. I mean, in a way he monetized his parents life
[00:21:09.000 --> 00:21:14.000]   work. The problem that I think this allows us to put a fine point on is the
[00:21:14.000 --> 00:21:20.000]   following. You know, in society, we've confused a lot of people to think that
[00:21:20.000 --> 00:21:26.000]   the opposite of liberal is conservative or Republican. And I think that's the
[00:21:26.000 --> 00:21:31.000]   cycle that drives the mind virus inside the mainstream media. The problem is
[00:21:31.000 --> 00:21:36.000]   the opposite of liberal is illiberal. Okay. And what illiberal means is to be
[00:21:36.000 --> 00:21:42.000]   narrow minded and unenlightened. It means to be puritanical. It means to be
[00:21:42.000 --> 00:21:47.000]   fundamentalist. And this is really what it allows us to see. Now we have now had
[00:21:47.000 --> 00:21:56.000]   six years of data, case after case after case, where if you are woke, if you are a
[00:21:56.000 --> 00:22:00.000]   social justice warrior, if you have the right credentials that justify your
[00:22:00.000 --> 00:22:06.000]   upbringing, if you have institutional bona fides that come from your parents,
[00:22:06.000 --> 00:22:13.000]   you get to create the narrative and you get a hall pass. And everybody else
[00:22:13.000 --> 00:22:19.000]   basically is at the subject and the mercy of the mainstream media. And so if you
[00:22:19.000 --> 00:22:23.000]   don't kiss the ring and bow down to them, they will try to destroy you or run you
[00:22:23.000 --> 00:22:27.000]   out of town. But if you are one of them, they will give you a hall pass. And when
[00:22:27.000 --> 00:22:33.000]   it's time for them to change their mind in order to tell the truth, they won't do
[00:22:33.000 --> 00:22:37.000]   it. And so these types of grifts will continue as Friedberg said, because there
[00:22:37.000 --> 00:22:42.000]   is no check and balance without a healthy independent media. There is no way for
[00:22:42.000 --> 00:22:47.000]   all of us to actually know what's really going on. Guys, some person in the media
[00:22:47.000 --> 00:22:51.000]   could have asked the question and dug in deeper around the connections between
[00:22:51.000 --> 00:22:57.000]   Alameda and FTX for the last 24 months. I know could have diligence at a venture
[00:22:57.000 --> 00:23:02.000]   firm. At no point could any person have asked these questions and found ex
[00:23:02.000 --> 00:23:07.000]   employees and said, you know, are there any unseemly connections here between FTX
[00:23:07.000 --> 00:23:11.000]   and Alameda? There was no disgruntled employee. I mean, every company has
[00:23:11.000 --> 00:23:15.000]   disgruntled employee whistleblowers. But here where there was billions of dollars
[00:23:15.000 --> 00:23:19.000]   being made by 10s of people, not a single person who felt on the outs said
[00:23:19.000 --> 00:23:23.000]   anything. Well, he was also giving millions of dollars to press outlets.
[00:23:23.000 --> 00:23:27.000]   Hold on. In donations, the questions weren't asked. And then this kid paid
[00:23:27.000 --> 00:23:30.000]   hush money to the mainstream media. Let me ask a question of you guys. Do you
[00:23:30.000 --> 00:23:33.000]   think that it's the media's responsibility in this context? Or do you
[00:23:33.000 --> 00:23:36.000]   think that there should have been a regulatory authority that had oversight
[00:23:36.000 --> 00:23:40.000]   of this business like there is for every bank and every trading operation in the
[00:23:40.000 --> 00:23:44.000]   United States? And every one of those businesses has a compliance officer
[00:23:44.000 --> 00:23:48.000]   chicken and has regulators up the wazoo making sure that customers are kept safe
[00:23:48.000 --> 00:23:52.000]   and protected? Or do we think that that should should offshore vehicles be
[00:23:52.000 --> 00:23:55.000]   allowed like this? That allow people to operate? It's a reasonable question. But
[00:23:55.000 --> 00:23:59.000]   there was a chicken and egg question. We were all standing around holding our
[00:23:59.000 --> 00:24:04.000]   hands while the CFTC and the SEC were fighting. That's not something that
[00:24:04.000 --> 00:24:09.000]   consumers can be expected to adjudicate. So yes, we should have legislation that
[00:24:09.000 --> 00:24:13.000]   clearly defines all of this. But there were enough parameters that created
[00:24:13.000 --> 00:24:17.000]   regulatory frameworks where a bunch of good actors did operate in them and are
[00:24:17.000 --> 00:24:22.000]   continuing to do so like Coinbase. So I don't think this is a regulatory issue.
[00:24:22.000 --> 00:24:26.000]   I think that if you believe there are people who are supposed to forensically
[00:24:26.000 --> 00:24:31.000]   examine things, and get to the bottom of things and ask hard questions. Those
[00:24:31.000 --> 00:24:36.000]   people did none of that here. And and what's what's even more worrisome is
[00:24:36.000 --> 00:24:40.000]   what they're showing is now with an massive amount of data that shows that
[00:24:40.000 --> 00:24:44.000]   you could ask hard questions. They don't care to because it makes them look bad.
[00:24:44.000 --> 00:24:48.000]   I disagree with this. I think regulators failed here, because they have been
[00:24:48.000 --> 00:24:51.000]   reactive to crypto, they have not been proactive, and they have not been clear
[00:24:51.000 --> 00:24:55.000]   with the crypto community that what they were doing was illegal, and they should
[00:24:55.000 --> 00:24:58.000]   have put the regulations in quicker and they're playing catch up. But all three
[00:24:58.000 --> 00:25:03.000]   groups failed, the media failed, the regulators failed, and VCs failed capital
[00:25:03.000 --> 00:25:08.000]   allocators failed, I apparently to do diligence here and install proper
[00:25:08.000 --> 00:25:13.000]   governance, you cannot put a company like this, you know, in business with
[00:25:13.000 --> 00:25:16.000]   billions of dollars and have no board of directors. No, I agree with you or he
[00:25:16.000 --> 00:25:17.000]   lied to them.
[00:25:17.000 --> 00:25:22.000]   And I agree with you. My point is that while regulators are basically fighting
[00:25:22.000 --> 00:25:27.000]   a territorial turf war, okay, the media could have still done their job. They
[00:25:27.000 --> 00:25:28.000]   chose not to
[00:25:28.000 --> 00:25:31.000]   guys, it was worse than that. Because not only it's not just a case where the
[00:25:31.000 --> 00:25:36.000]   SEC failed to exercise any oversight of him or dig into any of these questions.
[00:25:36.000 --> 00:25:40.000]   He was in the room with them crafting the next set of regulations. He was
[00:25:40.000 --> 00:25:44.000]   working on the regulations that you're talking about that are supposedly needed
[00:25:44.000 --> 00:25:50.000]   breaking them. They let the fox in the hen house. Yeah, he was gonna craft a new
[00:25:50.000 --> 00:25:55.000]   type of regulatory license for these types of exchanges, with the result that
[00:25:55.000 --> 00:25:58.000]   he was going to get one and some of his competitors weren't. This is one of the
[00:25:58.000 --> 00:26:04.000]   things that triggered CZ to basically, you know, do what he did, which is
[00:26:04.000 --> 00:26:10.000]   basically that SPF was trying to get Binance and competitors like that band,
[00:26:10.000 --> 00:26:14.000]   while SPF would be one of the sole people to get the license.
[00:26:14.000 --> 00:26:15.000]   By the way,
[00:26:15.000 --> 00:26:17.000]   he was working with the regulators.
[00:26:17.000 --> 00:26:22.000]   Jason, you just said something derogatory towards CZ he rug pulled him. Did he do
[00:26:22.000 --> 00:26:24.000]   that? Or did he actually expose the fraud?
[00:26:24.000 --> 00:26:26.000]   That's what I mean by rug pulling. Yeah.
[00:26:26.000 --> 00:26:27.000]   He was partners with him.
[00:26:27.000 --> 00:26:28.000]   That's not what rug pull means, Jason.
[00:26:28.000 --> 00:26:33.000]   Okay, well, he was partners with him. And then he realized he was weak, and that
[00:26:33.000 --> 00:26:35.000]   he was doing some stuff that was shady. And he decided he would eliminate a
[00:26:35.000 --> 00:26:39.000]   partner who was creating regulation, as Zach said, whatever you want to say, he
[00:26:39.000 --> 00:26:40.000]   knifed him.
[00:26:40.000 --> 00:26:44.000]   All he did was indicate a desire to liquidate his position in a token that
[00:26:44.000 --> 00:26:49.000]   was supposedly perfectly liquid. And that's basically that caused the
[00:26:49.000 --> 00:26:50.000]   everything to unravel.
[00:26:50.000 --> 00:26:53.000]   Yeah, but these were partners, right? I mean, these were deep business partners.
[00:26:53.000 --> 00:26:54.000]   No, they're not.
[00:26:54.000 --> 00:26:57.000]   And they were partners. They were collaborating on these tokens together.
[00:26:57.000 --> 00:26:58.000]   So they weren't competitors.
[00:26:58.000 --> 00:26:59.000]   No, they weren't.
[00:26:59.000 --> 00:27:04.000]   Jason, like part of the big loan that initiated all of this stuff, like a year
[00:27:04.000 --> 00:27:10.000]   and a half ago, was to buy FTX off the cap table. So, you know, this is all I'm
[00:27:10.000 --> 00:27:15.000]   saying is like you used words and your framing of a guy, you know, who's built
[00:27:15.000 --> 00:27:19.000]   a business is like, he is this nefarious bad actor.
[00:27:19.000 --> 00:27:21.000]   Binance was FTX's first investor.
[00:27:21.000 --> 00:27:26.000]   Hold on a second. But David's right. All the guy did, as far as we can tell
[00:27:26.000 --> 00:27:29.000]   right now, is tweet, I'm selling this token because I don't believe in the
[00:27:29.000 --> 00:27:34.000]   capital structure of this entity. And he got those tokens by being bought out of
[00:27:34.000 --> 00:27:35.000]   the cap table.
[00:27:35.000 --> 00:27:39.000]   I know he was partners with them. So if you don't like rug pulled, how about
[00:27:39.000 --> 00:27:41.000]   backstabbing business partners?
[00:27:41.000 --> 00:27:42.000]   They're not partners.
[00:27:42.000 --> 00:27:43.000]   You still don't understand.
[00:27:43.000 --> 00:27:48.000]   Of course I understand. He was the first investor. So to do this to a company
[00:27:48.000 --> 00:27:49.000]   you were about to.
[00:27:49.000 --> 00:27:50.000]   And then he got bought out.
[00:27:50.000 --> 00:27:51.000]   No, no, he got bought out.
[00:27:51.000 --> 00:27:54.000]   And part of the consideration, hold on, part of the consideration were these
[00:27:54.000 --> 00:27:56.000]   tokens, these FTX tokens.
[00:27:56.000 --> 00:27:58.000]   I understand. Yes. Jason, when Uber went public.
[00:27:58.000 --> 00:27:59.000]   Yes.
[00:27:59.000 --> 00:28:01.000]   And you got distributed stock.
[00:28:01.000 --> 00:28:02.000]   Yeah.
[00:28:02.000 --> 00:28:04.000]   Did you distribute and sell Uber at any point?
[00:28:04.000 --> 00:28:07.000]   Previous to that, I had. I sold some to Masa.
[00:28:07.000 --> 00:28:10.000]   No, no, no, no, no, no. I'm asking you a question. When Uber went public and
[00:28:10.000 --> 00:28:13.000]   you got distributed from Uber, your stock.
[00:28:13.000 --> 00:28:15.000]   So you've never sold a single share of Uber ever?
[00:28:15.000 --> 00:28:16.000]   Not since it's public.
[00:28:16.000 --> 00:28:18.000]   No, I sold it before in private markets.
[00:28:18.000 --> 00:28:21.000]   Okay, but I'm sorry, but that doesn't make Jason a partner of Uber.
[00:28:21.000 --> 00:28:22.000]   You're just a stockholder.
[00:28:22.000 --> 00:28:26.000]   Yeah, this is slightly different, I think. But okay, fine. You guys, if you
[00:28:26.000 --> 00:28:27.000]   guys want to consider.
[00:28:27.000 --> 00:28:29.000]   No, he was the first investor in the company.
[00:28:29.000 --> 00:28:31.000]   They were business partners.
[00:28:31.000 --> 00:28:33.000]   That might have been a historical situation, but their status at the time
[00:28:33.000 --> 00:28:36.000]   that CZ tweeted was that they were competitors.
[00:28:36.000 --> 00:28:39.000]   Okay, fine. I mean, they became competitors. I agree.
[00:28:39.000 --> 00:28:42.000]   And by the way, to Tomas point about this rug pulling language, I think
[00:28:42.000 --> 00:28:44.000]   we're getting kind of down a rabbit hole here.
[00:28:44.000 --> 00:28:45.000]   We're getting on a wheeze here.
[00:28:45.000 --> 00:28:48.000]   CZ did perform a service in this sense.
[00:28:48.000 --> 00:28:49.000]   Okay.
[00:28:49.000 --> 00:28:52.000]   SBF claims that 4 billion more was about to come in.
[00:28:52.000 --> 00:28:54.000]   I personally don't believe that. Sounds like bullshit to me.
[00:28:54.000 --> 00:28:57.000]   But if it is true, that would have been a bad thing.
[00:28:57.000 --> 00:29:01.000]   The more money that came in to that operation, SBF proved that he was a
[00:29:01.000 --> 00:29:04.000]   very poor custodian of customer funds.
[00:29:04.000 --> 00:29:07.000]   For sure. For sure. I'm not defending SBF.
[00:29:07.000 --> 00:29:08.000]   The longer this went on.
[00:29:08.000 --> 00:29:12.000]   I'm just highlighting the language that you use is sort of like, again,
[00:29:12.000 --> 00:29:14.000]   part of that establishment elite narrative.
[00:29:14.000 --> 00:29:17.000]   And I'm just questioning, you should maybe steal man.
[00:29:17.000 --> 00:29:19.000]   Take a second to just steal man.
[00:29:19.000 --> 00:29:20.000]   Yeah.
[00:29:20.000 --> 00:29:23.000]   A more dispassionate view, which is here's a counterparty.
[00:29:23.000 --> 00:29:24.000]   Okay. Yeah.
[00:29:24.000 --> 00:29:29.000]   Who, when he left the cap table, was given half cash, half tokens.
[00:29:29.000 --> 00:29:30.000]   Okay.
[00:29:30.000 --> 00:29:33.000]   And he decided to sell his tokens.
[00:29:33.000 --> 00:29:34.000]   Yeah.
[00:29:34.000 --> 00:29:36.000]   And tweet it publicly and cause a run on the bank.
[00:29:36.000 --> 00:29:38.000]   So it was not a run on the bank.
[00:29:38.000 --> 00:29:40.000]   There was no bank. Where's the bank.
[00:29:40.000 --> 00:29:42.000]   This run on the bank language. Okay.
[00:29:42.000 --> 00:29:45.000]   Is something this was in the semaphore coverage.
[00:29:45.000 --> 00:29:47.000]   Okay. Did it publicly. That's my point.
[00:29:47.000 --> 00:29:49.000]   So I'll steal man.
[00:29:49.000 --> 00:29:50.000]   No, no, no, no.
[00:29:50.000 --> 00:29:51.000]   These tokens are worthless.
[00:29:51.000 --> 00:29:53.000]   I need to liquidate them as fast as possible.
[00:29:53.000 --> 00:29:55.000]   But why would you do that publicly?
[00:29:55.000 --> 00:29:56.000]   Why would you do it privately?
[00:29:56.000 --> 00:29:57.000]   You're about to move the market.
[00:29:57.000 --> 00:29:59.000]   He wanted to move the market to zero.
[00:29:59.000 --> 00:30:00.000]   Yes.
[00:30:00.000 --> 00:30:01.000]   He's letting people know why,
[00:30:01.000 --> 00:30:04.000]   but he wanted to kill his competitor as Chamath was just saying.
[00:30:04.000 --> 00:30:06.000]   He's he wanted to kill his competitor.
[00:30:06.000 --> 00:30:07.000]   He told the market.
[00:30:07.000 --> 00:30:08.000]   I am.
[00:30:08.000 --> 00:30:09.000]   He was trying to do that.
[00:30:09.000 --> 00:30:12.000]   We got to back up here because I think we've done a lot of like 30,000 foot,
[00:30:12.000 --> 00:30:15.000]   like lessons and like takeaways from this whole thing,
[00:30:15.000 --> 00:30:18.000]   but we haven't really established what it is that SBF did wrong.
[00:30:18.000 --> 00:30:21.000]   So I think we need to sort of take a second to unmoddy the waters.
[00:30:21.000 --> 00:30:22.000]   Okay.
[00:30:22.000 --> 00:30:24.000]   And part of that,
[00:30:24.000 --> 00:30:27.000]   I think we should start with this idea of a run on the bank because the
[00:30:27.000 --> 00:30:30.000]   favorite, the press, you've been writing puff pieces about SBF.
[00:30:30.000 --> 00:30:33.000]   I'd say mainly Semaphore, which he was a big donor to.
[00:30:33.000 --> 00:30:34.000]   Yeah.
[00:30:34.000 --> 00:30:35.000]   Even trying to frame it as a run on the bank.
[00:30:35.000 --> 00:30:37.000]   And then that implies that it's not really his fault.
[00:30:37.000 --> 00:30:38.000]   It could happen to anybody.
[00:30:38.000 --> 00:30:40.000]   Lots of banks have had this problem.
[00:30:40.000 --> 00:30:41.000]   Okay.
[00:30:41.000 --> 00:30:42.000]   First of all, they're not a bank.
[00:30:42.000 --> 00:30:46.000]   Banks actually have the legal right under certain conditions to take customer
[00:30:46.000 --> 00:30:48.000]   deposits and loan them out.
[00:30:48.000 --> 00:30:49.000]   Okay.
[00:30:49.000 --> 00:30:50.000]   Yes, they did not.
[00:30:50.000 --> 00:30:53.000]   Their terms of use did not allow that as Stephanopoulos pointed out.
[00:30:53.000 --> 00:30:57.000]   SBF's answer to that was, well, we had this like margin account program.
[00:30:57.000 --> 00:31:00.000]   There were other provisions and other terms of use, but most of the customers
[00:31:00.000 --> 00:31:03.000]   who lost money, the vast majority did not opt into that program.
[00:31:03.000 --> 00:31:04.000]   They never agreed to that.
[00:31:04.000 --> 00:31:06.000]   So that's, that's point number one.
[00:31:06.000 --> 00:31:10.000]   Point number two is I think we need to, to look at this language of margin
[00:31:10.000 --> 00:31:11.000]   account.
[00:31:11.000 --> 00:31:12.000]   Okay.
[00:31:12.000 --> 00:31:18.000]   SBF's explanation of how customer money was siphoned off for his own
[00:31:18.000 --> 00:31:22.000]   personal use, IE to Alameda is that Alameda had a margin account.
[00:31:22.000 --> 00:31:26.000]   So I think we could perform a service here by explaining why it wasn't a
[00:31:26.000 --> 00:31:27.000]   margin account.
[00:31:27.000 --> 00:31:30.000]   And, you know, Jamal and you guys understand this really well.
[00:31:30.000 --> 00:31:34.000]   The way that a margin account works is the following.
[00:31:34.000 --> 00:31:35.000]   Okay.
[00:31:35.000 --> 00:31:38.000]   Because I think some of us have them set up with investment banks.
[00:31:38.000 --> 00:31:43.000]   You go to an investment bank, say Morgan Stanley, and you over, you post
[00:31:43.000 --> 00:31:46.000]   collateral, you actually over collateralize.
[00:31:46.000 --> 00:31:50.000]   So for example, you might take a hundred million dollars of stock posted at
[00:31:50.000 --> 00:31:55.000]   the investment bank, and then they will let you loan a certain percentage,
[00:31:55.000 --> 00:31:57.000]   nowhere near a hundred percent, maybe 50% of that.
[00:31:57.000 --> 00:32:04.000]   So if you have a very, very liquid security, you may get 50% coverage,
[00:32:04.000 --> 00:32:07.000]   which means if you posted a hundred million dollars, you could get a 50
[00:32:07.000 --> 00:32:08.000]   million dollar loan.
[00:32:08.000 --> 00:32:09.000]   Okay.
[00:32:09.000 --> 00:32:12.000]   So you have a hundred million in Amazon stock, you're some Amazon VP, you
[00:32:12.000 --> 00:32:13.000]   can get 50 million loans.
[00:32:13.000 --> 00:32:19.000]   And if it's a private asset, it's anywhere as high as 30%, 35%.
[00:32:19.000 --> 00:32:21.000]   But typically it's about 25%.
[00:32:21.000 --> 00:32:25.000]   My expectation is in a liquid token like this would have basically gotten
[00:32:25.000 --> 00:32:28.000]   five or 10% coverage ratio at the best of it.
[00:32:28.000 --> 00:32:31.000]   And then what happens is you have these maintenance values.
[00:32:31.000 --> 00:32:36.000]   So if all of a sudden the value of these entities multiplied by that
[00:32:36.000 --> 00:32:39.000]   percentage that you're allowed to loan falls below, you have to post money.
[00:32:39.000 --> 00:32:40.000]   That's how a margin account works.
[00:32:40.000 --> 00:32:42.000]   It's just, there is no free lunch in that.
[00:32:42.000 --> 00:32:44.000]   Yes, exactly.
[00:32:44.000 --> 00:32:48.000]   Let me just say quite simply, very simply what I, it appears this guy did.
[00:32:48.000 --> 00:32:52.000]   He took customer deposits in us dollars.
[00:32:52.000 --> 00:32:57.000]   He then converted those dollars into some other asset.
[00:32:57.000 --> 00:32:59.000]   And he had a mark on that asset.
[00:32:59.000 --> 00:33:01.000]   Let's call it a dollar a token.
[00:33:01.000 --> 00:33:04.000]   And then those dollars were moved to somewhere else.
[00:33:04.000 --> 00:33:08.000]   No, this is someone transferred in some other token.
[00:33:08.000 --> 00:33:13.000]   But we need to finish the explainer around the margin account.
[00:33:13.000 --> 00:33:14.000]   Okay.
[00:33:14.000 --> 00:33:17.000]   Because what SPF did is this.
[00:33:17.000 --> 00:33:21.000]   He took customer deposits, gave them to himself.
[00:33:21.000 --> 00:33:22.000]   Just be clear.
[00:33:22.000 --> 00:33:25.000]   No, he gave, he took customer deposits in us dollars.
[00:33:25.000 --> 00:33:27.000]   They were wired in.
[00:33:27.000 --> 00:33:28.000]   Correct.
[00:33:28.000 --> 00:33:32.000]   He took those dollars out and he put a fake token in and he called that.
[00:33:32.000 --> 00:33:33.000]   That's right.
[00:33:33.000 --> 00:33:37.000]   He said, therefore he said, the balance sheet is good.
[00:33:37.000 --> 00:33:40.000]   But the value of that token, it turns out isn't a dollar.
[00:33:40.000 --> 00:33:41.000]   It's 10 cents.
[00:33:41.000 --> 00:33:42.000]   That's right.
[00:33:42.000 --> 00:33:45.000]   It was his, it was his sort of, it was sort of his made up token that
[00:33:45.000 --> 00:33:50.000]   he tightly controlled the trading of and artificially prompted the price.
[00:33:50.000 --> 00:33:52.000]   By the way, it wasn't just FTT.
[00:33:52.000 --> 00:33:53.000]   Yeah.
[00:33:53.000 --> 00:33:54.000]   But here's the thing.
[00:33:54.000 --> 00:33:56.000]   It wasn't just the fact that his collateral was no good.
[00:33:56.000 --> 00:34:01.000]   It was also the fact that, and this is from the bankruptcy filing by the
[00:34:01.000 --> 00:34:05.000]   Enron trustee guy.
[00:34:05.000 --> 00:34:09.000]   He specifically said that Alameda, unlike every other margin account on
[00:34:09.000 --> 00:34:13.000]   the platform, had the auto liquidation provisions turned off.
[00:34:13.000 --> 00:34:15.000]   So wait, we have to finish the thought around how margin works.
[00:34:15.000 --> 00:34:18.000]   So like Tomas said, you over post collateral.
[00:34:18.000 --> 00:34:23.000]   And if the value of that collateral goes down or the, the, the, the, the
[00:34:23.000 --> 00:34:26.000]   position, your trading account, the value of that goes down, you either
[00:34:26.000 --> 00:34:31.000]   have to post more collateral or they will actually liquidate your collateral
[00:34:31.000 --> 00:34:33.000]   to pay off the loan.
[00:34:33.000 --> 00:34:36.000]   So Morgan Stanley will never lose money on a margin account.
[00:34:36.000 --> 00:34:37.000]   Never.
[00:34:37.000 --> 00:34:40.000]   The whole point is because they don't make money on it.
[00:34:40.000 --> 00:34:44.000]   They loan you the money at like, you know, a few percents, like very cheap.
[00:34:44.000 --> 00:34:45.000]   Yeah.
[00:34:45.000 --> 00:34:46.000]   Loan LIBOR plus.
[00:34:46.000 --> 00:34:47.000]   So exactly.
[00:34:47.000 --> 00:34:49.000]   That is not a risk account to them.
[00:34:49.000 --> 00:34:52.000]   And so in the, in the example, let's use an example.
[00:34:52.000 --> 00:34:56.000]   In the case of the VP at Amazon, who's got a hundred million in Amazon,
[00:34:56.000 --> 00:34:58.000]   they have a $50 million loan.
[00:34:58.000 --> 00:35:02.000]   If Amazon loses half its value, then that triggers the automatic selling
[00:35:02.000 --> 00:35:06.000]   of Amazon shares to get it back down to 50% coverage.
[00:35:06.000 --> 00:35:10.000]   So now you have to sell 25 million of Amazon shares.
[00:35:10.000 --> 00:35:13.000]   If the full 50 million was pulled down to get back down to 25 to 50%
[00:35:13.000 --> 00:35:14.000]   leverage.
[00:35:14.000 --> 00:35:15.000]   Yes.
[00:35:15.000 --> 00:35:19.000]   And they don't wait until like Amazon stock is at the exact level where
[00:35:19.000 --> 00:35:21.000]   now the collateral equals a hundred percent of the loan.
[00:35:21.000 --> 00:35:25.000]   They will keep that 50% loan to value and they will liquidate you, you
[00:35:25.000 --> 00:35:28.000]   know, and, and by the way, you can lose your entire amount, right?
[00:35:28.000 --> 00:35:31.000]   So, you know, this is why I'm trading a margin is so risky.
[00:35:31.000 --> 00:35:34.000]   Is that you can get wiped out completely, wiped out very, very quickly
[00:35:34.000 --> 00:35:38.000]   with a small move down because they are the custodians of that Amazon
[00:35:38.000 --> 00:35:39.000]   stock.
[00:35:39.000 --> 00:35:42.000]   They are holding it for you now and they have the right to sell it to
[00:35:42.000 --> 00:35:43.000]   cover your margin.
[00:35:43.000 --> 00:35:47.000]   You're saying that governor, that basic tenant, that basic safety
[00:35:47.000 --> 00:35:51.000]   control was turned off by Alameda and it's even more sinister.
[00:35:51.000 --> 00:35:56.000]   Alameda controlled like 90 or 95% of these FTT tokens and was owned
[00:35:56.000 --> 00:35:59.000]   by Sam Bankman fraud.
[00:35:59.000 --> 00:36:00.000]   So he owned that company.
[00:36:00.000 --> 00:36:02.000]   Then he claims he had no operating.
[00:36:02.000 --> 00:36:07.000]   Listen, what should have happened is with that collateral is that as
[00:36:07.000 --> 00:36:11.000]   the value of their position was going down and or as the value of the
[00:36:11.000 --> 00:36:14.000]   collateral was going down, it should have been liquidated to pay off
[00:36:14.000 --> 00:36:15.000]   the margin loan.
[00:36:15.000 --> 00:36:16.000]   And that did not happen.
[00:36:16.000 --> 00:36:19.000]   And the reason it didn't happen is that Alameda got a special
[00:36:19.000 --> 00:36:22.000]   exception on the platform to turn off auto liquidation.
[00:36:22.000 --> 00:36:24.000]   Therefore it was never a margin account.
[00:36:24.000 --> 00:36:27.000]   If even if it was a margin account, okay.
[00:36:27.000 --> 00:36:31.000]   And, and FTX somehow misadministered the margin account.
[00:36:31.000 --> 00:36:35.000]   It should never have taken other customers deposits and use them to
[00:36:35.000 --> 00:36:36.000]   pay back that money.
[00:36:36.000 --> 00:36:40.000]   What should have happened is if FTX was going to lose money on a margin
[00:36:40.000 --> 00:36:44.000]   account, that would hit the FTX corporate treasury.
[00:36:44.000 --> 00:36:45.000]   Okay.
[00:36:45.000 --> 00:36:48.000]   And when the FTX corporate treasury ran out, the company falls for
[00:36:48.000 --> 00:36:52.000]   bankruptcy then, and then all the other customers hold on their account
[00:36:52.000 --> 00:36:53.000]   is still there.
[00:36:53.000 --> 00:36:57.000]   Their money is there in segregated accounts and in bankruptcy, they get
[00:36:57.000 --> 00:36:58.000]   their money back.
[00:36:58.000 --> 00:37:03.000]   The idea that a margin account could ever cause another customer to
[00:37:03.000 --> 00:37:06.000]   lose money that like, whatever that is, that's not a margin.
[00:37:06.000 --> 00:37:07.000]   There's a great article.
[00:37:07.000 --> 00:37:08.000]   There's a great article.
[00:37:08.000 --> 00:37:11.000]   This one was a journalist that did his job properly.
[00:37:11.000 --> 00:37:12.000]   His name is David Z.
[00:37:12.000 --> 00:37:13.000]   Morris.
[00:37:13.000 --> 00:37:17.000]   He wrote an article in coin desk that summed up for anybody that's
[00:37:17.000 --> 00:37:21.000]   interested, all of the actual fraud and all of the crimes that were
[00:37:21.000 --> 00:37:24.000]   committed in excruciating detail.
[00:37:24.000 --> 00:37:27.000]   And what's so sad about all these interviews in this press tour is if
[00:37:27.000 --> 00:37:30.000]   anybody would just read this article, you can construct the right
[00:37:30.000 --> 00:37:33.000]   questions to ask this guy just based on this one article.
[00:37:33.000 --> 00:37:37.000]   But the point I wanted to make is that one of the most interesting
[00:37:37.000 --> 00:37:43.000]   insights was these guys had lost an enormous amount of money already in
[00:37:43.000 --> 00:37:45.000]   calendar year 21.
[00:37:45.000 --> 00:37:51.000]   And so this is what's so crazy, Jason, about you using language like
[00:37:51.000 --> 00:37:56.000]   rug pulling and nobody actually trying to be clear.
[00:37:56.000 --> 00:37:58.000]   Like you guys are giving this guy a hall pass.
[00:37:58.000 --> 00:38:02.000]   I'm not giving any, any industrious reporter could have found an
[00:38:02.000 --> 00:38:06.000]   employee who said, wait a minute, we just blew a $3 billion hole in
[00:38:06.000 --> 00:38:08.000]   our balance sheet and calendar year 21, 20.
[00:38:08.000 --> 00:38:11.000]   And now we're sitting here at the end of 22.
[00:38:11.000 --> 00:38:14.000]   Hold on.
[00:38:14.000 --> 00:38:15.000]   I need to respond.
[00:38:15.000 --> 00:38:17.000]   I am not giving him a pass.
[00:38:17.000 --> 00:38:22.000]   And for you to blame journalists who are reflecting the crime and not
[00:38:22.000 --> 00:38:26.000]   putting any light on VCs and the capital allocators who made this
[00:38:26.000 --> 00:38:29.000]   investment and who did no diligence and now put governance in it is the
[00:38:29.000 --> 00:38:30.000]   height of arrogance.
[00:38:30.000 --> 00:38:31.000]   Shama.
[00:38:31.000 --> 00:38:32.000]   This is not the press is not doing that.
[00:38:32.000 --> 00:38:34.000]   This is the VCs.
[00:38:34.000 --> 00:38:36.000]   Is the capital allocators fault.
[00:38:36.000 --> 00:38:39.000]   You're blaming the people who are telling the story after the crime
[00:38:39.000 --> 00:38:40.000]   story.
[00:38:40.000 --> 00:38:42.000]   They're covering the story.
[00:38:42.000 --> 00:38:43.000]   I can't handle the truth.
[00:38:43.000 --> 00:38:45.000]   Jason, they're covering the story.
[00:38:45.000 --> 00:38:46.000]   Can I get in here?
[00:38:46.000 --> 00:38:47.000]   Can I get in here?
[00:38:47.000 --> 00:38:50.000]   All right, listen, Jason, I will defend you against tomorrow saying
[00:38:50.000 --> 00:38:54.000]   that somehow you're, you know, that you're letting SPF off the hook.
[00:38:54.000 --> 00:38:56.000]   I know you don't want to let SPF.
[00:38:56.000 --> 00:38:58.000]   However, you are letting the press off the hook.
[00:38:58.000 --> 00:39:00.000]   And the reason why, hold on a second.
[00:39:00.000 --> 00:39:04.000]   The reason why you're using this inaccurate language like rug pulling
[00:39:04.000 --> 00:39:07.000]   and run on the bank when there was no run and there was no bank is
[00:39:07.000 --> 00:39:11.000]   because you've been infected by this language that the media has inserted
[00:39:11.000 --> 00:39:12.000]   into the discourse.
[00:39:12.000 --> 00:39:15.000]   The media, listen, hold on a second.
[00:39:15.000 --> 00:39:17.000]   Investors may have got it wrong last year.
[00:39:17.000 --> 00:39:20.000]   Investors may have got it wrong when they did that last round, but I
[00:39:20.000 --> 00:39:23.000]   think investors now understand what's happening, but the media is still
[00:39:23.000 --> 00:39:27.000]   covering for SPF by miss explaining what happened.
[00:39:27.000 --> 00:39:28.000]   Okay.
[00:39:28.000 --> 00:39:30.000]   Give me a percentage, Saks, of who's to blame here.
[00:39:30.000 --> 00:39:34.000]   VCs who invested and didn't set up any governance regulators who did
[00:39:34.000 --> 00:39:40.000]   not set rules around crypto and then three, the media, what percentage
[00:39:40.000 --> 00:39:44.000]   out of a hundred percent is the investors, the regulators and the
[00:39:44.000 --> 00:39:47.000]   press go three numbers.
[00:39:47.000 --> 00:39:50.000]   I would say that before the fraud got exposed, one third, one third,
[00:39:50.000 --> 00:39:54.000]   one third, one third each before the fraud got exposed, but they were
[00:39:54.000 --> 00:39:56.000]   all jointly and severally liable.
[00:39:56.000 --> 00:40:00.000]   But after the fraud has been exposed, no investor is still defending
[00:40:00.000 --> 00:40:01.000]   SPF.
[00:40:01.000 --> 00:40:04.000]   But I think that the investors who were swindled by him, they feel
[00:40:04.000 --> 00:40:05.000]   bad about it.
[00:40:05.000 --> 00:40:08.000]   So 30, 30, 30 is absurd.
[00:40:08.000 --> 00:40:11.000]   The press had no way to know the fraud was going on.
[00:40:11.000 --> 00:40:14.000]   Just like the VCs who put the money in.
[00:40:14.000 --> 00:40:16.000]   Jason, are you stupid?
[00:40:16.000 --> 00:40:20.000]   Like, wasn't it your stupid journalist that exposed the fraud at
[00:40:20.000 --> 00:40:21.000]   Theranos?
[00:40:21.000 --> 00:40:22.000]   He's the guy that went and did all the work.
[00:40:22.000 --> 00:40:24.000]   John Kerry, you should be celebrated.
[00:40:24.000 --> 00:40:25.000]   Hold on a second.
[00:40:25.000 --> 00:40:29.000]   John Kerry, you went and found this thing when nobody, when everybody
[00:40:29.000 --> 00:40:30.000]   else was like, this is perfect.
[00:40:30.000 --> 00:40:32.000]   It meets all of our priors.
[00:40:32.000 --> 00:40:33.000]   Let me finish, please.
[00:40:33.000 --> 00:40:34.000]   It meets all of our priors.
[00:40:34.000 --> 00:40:35.000]   This is great.
[00:40:35.000 --> 00:40:39.000]   And John Kerry was like, this doesn't pass the smell test to me.
[00:40:39.000 --> 00:40:40.000]   Let me go do some work.
[00:40:40.000 --> 00:40:42.000]   And he pulled one little string.
[00:40:42.000 --> 00:40:45.000]   And over the course of 18 months, he exposed the whole bloody thing.
[00:40:45.000 --> 00:40:46.000]   So hold on a second.
[00:40:46.000 --> 00:40:50.000]   So what is incredible to me is that it was possible to expose this
[00:40:50.000 --> 00:40:52.000]   thing before nobody did.
[00:40:52.000 --> 00:40:53.000]   I agree with David.
[00:40:53.000 --> 00:40:57.000]   It's about equal responsibility before, but afterwards, the bulk of
[00:40:57.000 --> 00:41:00.000]   the responsibilities now sits with regulators to clean it up and
[00:41:00.000 --> 00:41:01.000]   journalists to tell the truth.
[00:41:01.000 --> 00:41:02.000]   Okay.
[00:41:02.000 --> 00:41:05.000]   And now may I respond to that since you call me stupid?
[00:41:05.000 --> 00:41:07.000]   You are delusional.
[00:41:07.000 --> 00:41:11.000]   Number one, every one of those investors in Theranos could have
[00:41:11.000 --> 00:41:15.000]   taking a fucking blood test at two different places like Jean-Louis
[00:41:15.000 --> 00:41:19.000]   Gassier did and write a blog post and prove that Theranos didn't
[00:41:19.000 --> 00:41:20.000]   work.
[00:41:20.000 --> 00:41:22.000]   And they withheld disbelief.
[00:41:22.000 --> 00:41:25.000]   Investors putting in a hundred million dollars, including Rupert
[00:41:25.000 --> 00:41:29.000]   Murdoch, didn't even take a fucking blood test or tell one of their
[00:41:29.000 --> 00:41:30.000]   diligence teams to do it.
[00:41:30.000 --> 00:41:34.000]   The same thing happened here with the investors in FTX.
[00:41:34.000 --> 00:41:36.000]   They did zero diligence.
[00:41:36.000 --> 00:41:37.000]   They set up zero governance.
[00:41:37.000 --> 00:41:43.000]   This was a failure of the investors and the governance for 99% of
[00:41:43.000 --> 00:41:44.000]   the problem.
[00:41:44.000 --> 00:41:45.000]   And then regulators should have caught it.
[00:41:45.000 --> 00:41:48.000]   And the regulators in fact did catch Theranos.
[00:41:48.000 --> 00:41:49.000]   So you're completely wrong.
[00:41:49.000 --> 00:41:54.000]   Chamath again, the journalists come in after the fraud is happening.
[00:41:54.000 --> 00:41:57.000]   The investors and governance is responsible for stopping these
[00:41:57.000 --> 00:41:58.000]   things.
[00:41:58.000 --> 00:42:00.000]   FTX was a failure of governance and investors.
[00:42:00.000 --> 00:42:02.000]   And so was Theranos.
[00:42:02.000 --> 00:42:03.000]   The end.
[00:42:03.000 --> 00:42:04.000]   You're completely wrong.
[00:42:04.000 --> 00:42:07.000]   The question is post post exposure.
[00:42:07.000 --> 00:42:09.000]   Why are you guys obsessed with post?
[00:42:09.000 --> 00:42:10.000]   How about avoiding these things?
[00:42:10.000 --> 00:42:13.000]   You guys are blaming the story is ongoing because the story is
[00:42:13.000 --> 00:42:16.000]   ongoing journalists for something that is capital allocators
[00:42:16.000 --> 00:42:17.000]   responsibility.
[00:42:17.000 --> 00:42:19.000]   It is our responsibility to do diligence.
[00:42:19.000 --> 00:42:23.000]   It is our responsibility to create a board of directors that checks on
[00:42:23.000 --> 00:42:25.000]   Elizabeth Holmes and Sam.
[00:42:25.000 --> 00:42:26.000]   I didn't disagree.
[00:42:26.000 --> 00:42:29.000]   I just call me stupid.
[00:42:29.000 --> 00:42:32.000]   You just call me stupid for pointing out something that you refuse to
[00:42:32.000 --> 00:42:33.000]   accept.
[00:42:33.000 --> 00:42:34.000]   What are you talking about?
[00:42:34.000 --> 00:42:35.000]   I'm the biggest.
[00:42:35.000 --> 00:42:37.000]   I'm the older brother.
[00:42:37.000 --> 00:42:44.000]   I think that maybe you guys are giving a pass to the investors.
[00:42:44.000 --> 00:42:46.000]   You're doing a good impression again.
[00:42:46.000 --> 00:42:47.000]   I'm not doing Fredo.
[00:42:47.000 --> 00:42:48.000]   I'm not doing Fredo.
[00:42:48.000 --> 00:42:49.000]   You guys are being absurd.
[00:42:49.000 --> 00:42:53.000]   This is why people say that we're delusional on this podcast.
[00:42:53.000 --> 00:42:54.000]   Don't call him dumb.
[00:42:54.000 --> 00:42:57.000]   The reason people say we're delusional is that we won't take
[00:42:57.000 --> 00:42:58.000]   acceptance of this issue.
[00:42:58.000 --> 00:43:01.000]   Remember in Fish Called Wanda, like the guy you can't call dumb.
[00:43:01.000 --> 00:43:02.000]   Totally.
[00:43:02.000 --> 00:43:03.000]   He loses it.
[00:43:03.000 --> 00:43:04.000]   He goes berserk.
[00:43:04.000 --> 00:43:05.000]   No, I'm not going berserk.
[00:43:05.000 --> 00:43:06.000]   You guys have a blind spot.
[00:43:06.000 --> 00:43:08.000]   Capital allocators are responsible.
[00:43:08.000 --> 00:43:10.000]   He's like, don't call me dumb.
[00:43:10.000 --> 00:43:14.000]   I'm not going to defend any single one of those investors.
[00:43:14.000 --> 00:43:17.000]   I think that they did a horrible job too.
[00:43:17.000 --> 00:43:18.000]   It's a great episode.
[00:43:18.000 --> 00:43:24.000]   But the reality is I think that if you think that you can -- it's your
[00:43:24.000 --> 00:43:26.000]   decision to defend the mainstream media.
[00:43:26.000 --> 00:43:27.000]   I think that that's fine.
[00:43:27.000 --> 00:43:28.000]   I'm not defending them.
[00:43:28.000 --> 00:43:29.000]   No, you are.
[00:43:29.000 --> 00:43:30.000]   You said they have no responsibility.
[00:43:30.000 --> 00:43:31.000]   No, I'm blaming the VCs.
[00:43:31.000 --> 00:43:32.000]   It's different.
[00:43:32.000 --> 00:43:37.000]   The culpability is with the investor class that has not had proper
[00:43:37.000 --> 00:43:38.000]   governance and diligence.
[00:43:38.000 --> 00:43:41.000]   Jason, how many articles have been written excoriating them?
[00:43:41.000 --> 00:43:43.000]   Yeah, some.
[00:43:43.000 --> 00:43:44.000]   A lot.
[00:43:44.000 --> 00:43:45.000]   They look like fools.
[00:43:45.000 --> 00:43:46.000]   Yeah.
[00:43:46.000 --> 00:43:48.000]   Show me the Washington Post.
[00:43:48.000 --> 00:43:49.000]   Type into Capital.
[00:43:49.000 --> 00:43:52.000]   No, show me the Washington Post, New York Times that's like digging
[00:43:52.000 --> 00:43:56.000]   in to that malfeasance or that lack of oversight and holding them
[00:43:56.000 --> 00:44:00.000]   accountable in a way that you feel exposes this problem to create
[00:44:00.000 --> 00:44:01.000]   change.
[00:44:01.000 --> 00:44:06.000]   Well, if we look at Theranos, those people who invested, including
[00:44:06.000 --> 00:44:09.000]   Draper and --
[00:44:09.000 --> 00:44:10.000]   Just show me the examples.
[00:44:10.000 --> 00:44:13.000]   Rupert Murdoch, they really went after them for sure.
[00:44:13.000 --> 00:44:14.000]   Yeah.
[00:44:14.000 --> 00:44:15.000]   What about here?
[00:44:15.000 --> 00:44:19.000]   Wall Street Journal, one day ago, Sequoia Capital apologizes to its
[00:44:19.000 --> 00:44:20.000]   fund investors for FTX loss.
[00:44:20.000 --> 00:44:22.000]   Venture capital firm tells fund investors that --
[00:44:22.000 --> 00:44:23.000]   That's hard hitting.
[00:44:23.000 --> 00:44:25.000]   -- it will improve due diligence on future investments after 100
[00:44:25.000 --> 00:44:26.000]   billion loss.
[00:44:26.000 --> 00:44:27.000]   They really got them.
[00:44:27.000 --> 00:44:28.000]   They really got them.
[00:44:28.000 --> 00:44:29.000]   Let me read you.
[00:44:29.000 --> 00:44:33.000]   I'm going to read you a sentence from the New York Times coverage of
[00:44:33.000 --> 00:44:34.000]   SBF.
[00:44:34.000 --> 00:44:35.000]   I'm not defending that.
[00:44:35.000 --> 00:44:39.000]   Sam Banquet-Fried is neither a visionary nor a criminal mastermind.
[00:44:39.000 --> 00:44:43.000]   He is a human who made the same poor choice that generations of money
[00:44:43.000 --> 00:44:45.000]   managers have made before him.
[00:44:45.000 --> 00:44:46.000]   Are you effing kidding me?
[00:44:46.000 --> 00:44:48.000]   No, I'm not defending them.
[00:44:48.000 --> 00:44:49.000]   Yes, you are.
[00:44:49.000 --> 00:44:50.000]   I am not defending them.
[00:44:50.000 --> 00:44:53.000]   I just called Andrew Horne's token a suit.
[00:44:53.000 --> 00:44:56.000]   And then Semaphore, who was on the take, who received millions of
[00:44:56.000 --> 00:44:57.000]   dollars in his money --
[00:44:57.000 --> 00:44:58.000]   They're on the payroll.
[00:44:58.000 --> 00:44:59.000]   They're on the bank.
[00:44:59.000 --> 00:45:00.000]   Okay.
[00:45:00.000 --> 00:45:01.000]   How much did they get?
[00:45:01.000 --> 00:45:02.000]   5 million?
[00:45:02.000 --> 00:45:03.000]   Now, where is their apology?
[00:45:03.000 --> 00:45:04.000]   Hold on a second.
[00:45:04.000 --> 00:45:05.000]   Where is their apology?
[00:45:05.000 --> 00:45:06.000]   Sequoia has apologized.
[00:45:06.000 --> 00:45:07.000]   Where is their apology?
[00:45:07.000 --> 00:45:08.000]   Oh, it has to come.
[00:45:08.000 --> 00:45:09.000]   I'm not defending the press.
[00:45:09.000 --> 00:45:10.000]   Yes, you are.
[00:45:10.000 --> 00:45:11.000]   I'm just saying --
[00:45:11.000 --> 00:45:12.000]   Yes, you are.
[00:45:12.000 --> 00:45:13.000]   No, I am not.
[00:45:13.000 --> 00:45:15.000]   I am literally telling you that the New York Times has been asleep at
[00:45:15.000 --> 00:45:16.000]   the wheel and throwing --
[00:45:16.000 --> 00:45:17.000]   Where's the New York Times apology?
[00:45:17.000 --> 00:45:19.000]   The press is always demanding an apology from everybody else.
[00:45:19.000 --> 00:45:20.000]   I'm agreeing with you.
[00:45:20.000 --> 00:45:21.000]   The press has done a shit job.
[00:45:21.000 --> 00:45:22.000]   Hold on a second.
[00:45:22.000 --> 00:45:23.000]   The press is always demanding --
[00:45:23.000 --> 00:45:24.000]   The press is so incompetent on this.
[00:45:24.000 --> 00:45:26.000]   The Twitter spaces yesterday did a better job of trying to ask
[00:45:26.000 --> 00:45:29.000]   questions and getting to the truth than a single journalist has
[00:45:29.000 --> 00:45:31.000]   done or the collective body of all of journalists.
[00:45:31.000 --> 00:45:32.000]   Absolutely.
[00:45:32.000 --> 00:45:35.000]   Literally, randos on Twitter spaces did a better job than Sorkin.
[00:45:35.000 --> 00:45:38.000]   Let me tell you why no one trusts the press, Jason.
[00:45:38.000 --> 00:45:39.000]   First of all, they have an agenda.
[00:45:39.000 --> 00:45:40.000]   I am in agreement.
[00:45:40.000 --> 00:45:42.000]   But second of all, when they make a mistake, they never admit it.
[00:45:42.000 --> 00:45:45.000]   When's the last time they did an apology or retraction?
[00:45:45.000 --> 00:45:47.000]   When's the last time they did what Sequoia did?
[00:45:47.000 --> 00:45:48.000]   I don't know.
[00:45:48.000 --> 00:45:50.000]   And they need to apologize about New York Times.
[00:45:50.000 --> 00:45:51.000]   Exactly.
[00:45:51.000 --> 00:45:52.000]   You came to a point to it.
[00:45:52.000 --> 00:45:54.000]   I am in agreement with you on that, but I think we have to first
[00:45:54.000 --> 00:45:57.000]   point, and this is where you guys have a blind spot, is what is
[00:45:57.000 --> 00:46:00.000]   the responsibility of capital allocators and governance
[00:46:00.000 --> 00:46:02.000]   and regulators?
[00:46:02.000 --> 00:46:04.000]   I think it's one, two, three.
[00:46:04.000 --> 00:46:07.000]   Our industry is responsible for setting up proper governance.
[00:46:07.000 --> 00:46:10.000]   The regulators are responsible for making sure that scientific
[00:46:10.000 --> 00:46:14.000]   claims are backed up, and then press is a distant third.
[00:46:14.000 --> 00:46:16.000]   You know who I think is responsible?
[00:46:16.000 --> 00:46:17.000]   Go.
[00:46:17.000 --> 00:46:18.000]   One, two, and three.
[00:46:18.000 --> 00:46:20.000]   And then we can talk about four, five, and six.
[00:46:20.000 --> 00:46:22.000]   Okay, four, five, and six.
[00:46:22.000 --> 00:46:25.000]   Capital allocators, regulators, the press a distant sixth.
[00:46:25.000 --> 00:46:26.000]   I agree.
[00:46:26.000 --> 00:46:27.000]   Let's go on to China.
[00:46:27.000 --> 00:46:29.000]   God, it's so spicy today in here.
[00:46:29.000 --> 00:46:31.000]   God, it's so hot.
[00:46:31.000 --> 00:46:34.000]   I do think when we attack the mainstream media, Jason feels a
[00:46:34.000 --> 00:46:39.000]   little tinge of like insecurity and illegitimacy because he were
[00:46:39.000 --> 00:46:40.000]   a journalist.
[00:46:40.000 --> 00:46:42.000]   My personal perception is I think that's bullshit.
[00:46:42.000 --> 00:46:43.000]   No.
[00:46:43.000 --> 00:46:48.000]   I think that you have an incredibly romantic view of the
[00:46:48.000 --> 00:46:51.000]   craft as you practiced it back then, which I think was fully
[00:46:51.000 --> 00:46:52.000]   of integrity.
[00:46:52.000 --> 00:46:53.000]   Yes, I think that's true.
[00:46:53.000 --> 00:46:58.000]   I think that you don't adequately realize how massively the
[00:46:58.000 --> 00:47:00.000]   industry has changed in the last 20 years since you've become
[00:47:00.000 --> 00:47:01.000]   a professor.
[00:47:01.000 --> 00:47:02.000]   I realize it more than you do.
[00:47:02.000 --> 00:47:05.000]   I fully realize that the media has absolutely become biased.
[00:47:05.000 --> 00:47:06.000]   Are they corrupt?
[00:47:06.000 --> 00:47:08.000]   And they have lost, in some cases, yes.
[00:47:08.000 --> 00:47:11.000]   I mean, if you're taking money from SBF and then giving him--
[00:47:11.000 --> 00:47:12.000]   Are they blinded?
[00:47:12.000 --> 00:47:13.000]   Are they biased?
[00:47:13.000 --> 00:47:16.000]   They are corrupt if they're taking money from SBF and then
[00:47:16.000 --> 00:47:17.000]   giving him Kigalov coverage.
[00:47:17.000 --> 00:47:18.000]   Absolutely.
[00:47:18.000 --> 00:47:20.000]   That is the definition of corruption in my mind.
[00:47:20.000 --> 00:47:23.000]   What is it called when you don't take money necessarily like
[00:47:23.000 --> 00:47:25.000]   the New York Times and still treat them with kid gloves?
[00:47:25.000 --> 00:47:26.000]   What is that?
[00:47:26.000 --> 00:47:27.000]   It is extreme bias.
[00:47:27.000 --> 00:47:29.000]   And the New York Times became extremely biased.
[00:47:29.000 --> 00:47:31.000]   Why do you think that bias exists?
[00:47:31.000 --> 00:47:34.000]   They were always left-leaning, but I can tell you why.
[00:47:34.000 --> 00:47:39.000]   They-- when Trump came in, a generation of new journalists
[00:47:39.000 --> 00:47:41.000]   became activist journalists.
[00:47:41.000 --> 00:47:44.000]   They didn't want to tell stories and take it straight down the
[00:47:44.000 --> 00:47:46.000]   middle and let the facts tell the story and let the audience
[00:47:46.000 --> 00:47:47.000]   make their own decision.
[00:47:47.000 --> 00:47:50.000]   They felt an existential risk when Trump came into office.
[00:47:50.000 --> 00:47:52.000]   They got Trump derangement syndrome.
[00:47:52.000 --> 00:47:55.000]   They picked a side like MSNBC and Fox did, and the business
[00:47:55.000 --> 00:47:59.000]   model became, for the New York Times, pick a side and get
[00:47:59.000 --> 00:48:00.000]   the subscribers.
[00:48:00.000 --> 00:48:03.000]   It was a deliberate, cynical choice on the New York Times
[00:48:03.000 --> 00:48:08.000]   part to go full MSNBC or full Fox, the two extremes in
[00:48:08.000 --> 00:48:10.000]   mainstream media, in order to get the subs.
[00:48:10.000 --> 00:48:14.000]   And they literally rallied the troops there to do anti-tech,
[00:48:14.000 --> 00:48:18.000]   anti-Trump coverage, and they became activists.
[00:48:18.000 --> 00:48:21.000]   And when journalists become activists, they are no longer
[00:48:21.000 --> 00:48:23.000]   journalists, they're activists or commentators.
[00:48:23.000 --> 00:48:24.000]   And that's the problem.
[00:48:24.000 --> 00:48:27.000]   It's being presented as journalism when in fact it's
[00:48:27.000 --> 00:48:28.000]   activism.
[00:48:28.000 --> 00:48:29.000]   And that's the problem.
[00:48:29.000 --> 00:48:30.000]   So you're right.
[00:48:30.000 --> 00:48:33.000]   So shout out to Matt Taibbi, who just did a monk debate--
[00:48:33.000 --> 00:48:34.000]   That's well said, by the way.
[00:48:34.000 --> 00:48:35.000]   --on this very topic.
[00:48:35.000 --> 00:48:36.000]   Thank you so much.
[00:48:36.000 --> 00:48:39.000]   And he has a great, great sub stack, basically saying what
[00:48:39.000 --> 00:48:40.000]   you're saying, Jason.
[00:48:40.000 --> 00:48:43.000]   And the best quote is, "The story is no longer the boss.
[00:48:43.000 --> 00:48:44.000]   Instead, we sell narrative."
[00:48:44.000 --> 00:48:47.000]   He's a lifelong journalist, whose father was a lifelong
[00:48:47.000 --> 00:48:49.000]   journalist, and he understands the way the business has
[00:48:49.000 --> 00:48:50.000]   changed.
[00:48:50.000 --> 00:48:51.000]   And it's like what you're saying.
[00:48:51.000 --> 00:48:54.000]   And this is why independent media, whether it's sub stacks,
[00:48:54.000 --> 00:48:58.000]   whether it's call-in shows, whether it's all-in podcasts or
[00:48:58.000 --> 00:49:01.000]   other podcasts--Joe Rogan, Sam Harris, whoever it is--
[00:49:01.000 --> 00:49:05.000]   independent voices are now what consumers are seeking out
[00:49:05.000 --> 00:49:07.000]   because they can sense the bias.
[00:49:07.000 --> 00:49:10.000]   They know Rachel Maddow and Tucker have an axe to grind,
[00:49:10.000 --> 00:49:12.000]   and they're left and right.
[00:49:12.000 --> 00:49:15.000]   They didn't expect the New York Times, Washington Post,
[00:49:15.000 --> 00:49:18.000]   and Wall Street Journal to--they knew they were leaning.
[00:49:18.000 --> 00:49:20.000]   They didn't expect them to pick a side.
[00:49:20.000 --> 00:49:23.000]   Do you think we should cancel--do you think folks are
[00:49:23.000 --> 00:49:26.000]   better off keeping their New York Times subscription or
[00:49:26.000 --> 00:49:29.000]   replacing that New York Times subscription with a basket
[00:49:29.000 --> 00:49:31.000]   of sub stacks?
[00:49:31.000 --> 00:49:32.000]   You answered your own question.
[00:49:32.000 --> 00:49:33.000]   It's the latter.
[00:49:33.000 --> 00:49:35.000]   I think you're on your own as a consumer now.
[00:49:35.000 --> 00:49:39.000]   You're going to have to--and I think this podcast and the
[00:49:39.000 --> 00:49:42.000]   nuance we have--shout out to Freeberg for nuance--
[00:49:42.000 --> 00:49:44.000]   what we've done on this podcast is--
[00:49:44.000 --> 00:49:45.000]   Wasn't that funny, Sax?
[00:49:45.000 --> 00:49:46.000]   --to explain to people--
[00:49:46.000 --> 00:49:49.000]   Freeberg's not the only one with nuance, Jake Helms.
[00:49:49.000 --> 00:49:51.000]   Nobody would describe David Sax with the word nuance.
[00:49:51.000 --> 00:49:53.000]   What, he's the nuance department on this podcast?
[00:49:53.000 --> 00:49:54.000]   What am I?
[00:49:54.000 --> 00:49:56.000]   100% he is, but you would know that because you leave when
[00:49:56.000 --> 00:49:57.000]   science course starts.
[00:49:57.000 --> 00:49:58.000]   I'm the truth department, okay?
[00:49:58.000 --> 00:49:59.000]   Sometimes nuance--
[00:49:59.000 --> 00:50:00.000]   You're truth bombs.
[00:50:00.000 --> 00:50:01.000]   I'm the truth department.
[00:50:01.000 --> 00:50:02.000]   You can take the both sides departments.
[00:50:02.000 --> 00:50:06.000]   The point is, consumers need to become extremely literate,
[00:50:06.000 --> 00:50:09.000]   and they have to do their own search for truth in today's age.
[00:50:09.000 --> 00:50:11.000]   They shouldn't trust New York Times.
[00:50:11.000 --> 00:50:12.000]   They shouldn't trust us.
[00:50:12.000 --> 00:50:13.000]   They should trust themselves.
[00:50:13.000 --> 00:50:17.000]   They shouldn't trust necessarily the CDC or the World Health
[00:50:17.000 --> 00:50:18.000]   Organization.
[00:50:18.000 --> 00:50:21.000]   They should trust themselves and come up with their own process
[00:50:21.000 --> 00:50:24.000]   for figuring out the truth in the middle of this mess.
[00:50:24.000 --> 00:50:27.000]   By the way, this is a good reflection on what's happened
[00:50:27.000 --> 00:50:31.000]   with the rest of media with respect to the creator class.
[00:50:31.000 --> 00:50:35.000]   Where it used to be the movie studios and a handful of
[00:50:35.000 --> 00:50:38.000]   aggregated creators that made all of the content,
[00:50:38.000 --> 00:50:41.000]   the record labels, and now independent artists,
[00:50:41.000 --> 00:50:44.000]   independent producers, independent creators,
[00:50:44.000 --> 00:50:47.000]   and now independent journalists are going to become the bulk
[00:50:47.000 --> 00:50:49.000]   of volume that's going to be consumed.
[00:50:49.000 --> 00:50:51.000]   It's just a different consumption model.
[00:50:51.000 --> 00:50:52.000]   We've already seen it happen with music.
[00:50:52.000 --> 00:50:54.000]   We saw it happen with movies, and we've seen this disruption
[00:50:54.000 --> 00:50:57.000]   happen across all of these other media classes.
[00:50:57.000 --> 00:51:01.000]   Journalism and what we call the press is very likely going to
[00:51:01.000 --> 00:51:04.000]   be that next layer of disruption.
[00:51:04.000 --> 00:51:06.000]   I would trust having a conversation with you about
[00:51:06.000 --> 00:51:09.000]   science topics over reading a science article.
[00:51:09.000 --> 00:51:10.000]   100%.
[00:51:10.000 --> 00:51:12.000]   You know, in the New York Times or Wall Street Journal,
[00:51:12.000 --> 00:51:13.000]   if I'm being honest.
[00:51:13.000 --> 00:51:14.000]   100%.
[00:51:14.000 --> 00:51:15.000]   I would much prefer to talk to you about it,
[00:51:15.000 --> 00:51:17.000]   and if it was markets, I'd rather talk to Chamath,
[00:51:17.000 --> 00:51:20.000]   and if it was SaaS, I would talk to Saks or operating a company.
[00:51:20.000 --> 00:51:22.000]   Speaking of operating a company.
[00:51:22.000 --> 00:51:24.000]   Or politics.
[00:51:24.000 --> 00:51:27.000]   [Laughter]
[00:51:27.000 --> 00:51:30.000]   We'll see. We'll see.
[00:51:30.000 --> 00:51:33.000]   But I think it's about having unique inside insight, right?
[00:51:33.000 --> 00:51:35.000]   Like, that wasn't the case.
[00:51:35.000 --> 00:51:37.000]   And what's interesting is that the people who are the professionals
[00:51:37.000 --> 00:51:41.000]   that have the knowledge and the touch points are also becoming
[00:51:41.000 --> 00:51:44.000]   journalists in the sense that they're also becoming speakers
[00:51:44.000 --> 00:51:45.000]   of their truth.
[00:51:45.000 --> 00:51:48.000]   And I think Twitter is a good enabling platform for this.
[00:51:48.000 --> 00:51:51.000]   We see it on YouTube where like scientists are putting out their
[00:51:51.000 --> 00:51:54.000]   own videos, or market actors, like people that are traders in the
[00:51:54.000 --> 00:51:57.000]   market go out and they put out their own videos,
[00:51:57.000 --> 00:51:58.000]   and they put out their own podcasts,
[00:51:58.000 --> 00:52:01.000]   and I think we're probably a good reflection of that.
[00:52:01.000 --> 00:52:04.000]   In the sense that we are the actors in the market,
[00:52:04.000 --> 00:52:08.000]   and we're not just the independent observer that has kind of a
[00:52:08.000 --> 00:52:09.000]   surface level view.
[00:52:09.000 --> 00:52:12.000]   We have the depth to be able to talk about the things that we choose
[00:52:12.000 --> 00:52:13.000]   to talk about.
[00:52:13.000 --> 00:52:16.000]   And I think that's where consumers find value and will continue to
[00:52:16.000 --> 00:52:19.000]   find value in terms of who the journalist or speaker is that they're
[00:52:19.000 --> 00:52:22.000]   going to start to trust for their information.
[00:52:22.000 --> 00:52:25.000]   I saw that interview you did with Newcomer.
[00:52:25.000 --> 00:52:27.000]   Oh, yeah.
[00:52:27.000 --> 00:52:30.000]   Yeah, I saw that coverage.
[00:52:30.000 --> 00:52:34.000]   I mean, he speaks a lot about this phenomenon of going direct.
[00:52:34.000 --> 00:52:36.000]   And of course he's against it.
[00:52:36.000 --> 00:52:41.000]   Now he interprets going direct as an attempt by newsmakers to avoid
[00:52:41.000 --> 00:52:44.000]   answering tough questions or take tough questions.
[00:52:44.000 --> 00:52:47.000]   I think that's ridiculous because, for example,
[00:52:47.000 --> 00:52:49.000]   I go on CNBC all the time.
[00:52:49.000 --> 00:52:53.000]   I go on Emily Ching and Bloomberg all the time.
[00:52:53.000 --> 00:52:55.000]   I submit to like really tough questions.
[00:52:55.000 --> 00:52:58.000]   I actually like those sort of sparring sessions.
[00:52:58.000 --> 00:53:00.000]   I did hard talk this week.
[00:53:00.000 --> 00:53:01.000]   Have you ever done that?
[00:53:01.000 --> 00:53:02.000]   Yeah, exactly.
[00:53:02.000 --> 00:53:04.000]   So that's not what's going on here.
[00:53:04.000 --> 00:53:06.000]   I think what's going on is we have expertise.
[00:53:06.000 --> 00:53:08.000]   We want to communicate them.
[00:53:08.000 --> 00:53:12.000]   And we do feel like the media has become a very unreliable narrator.
[00:53:12.000 --> 00:53:14.000]   There is too much bias and sloppiness.
[00:53:14.000 --> 00:53:16.000]   Not all of it is agenda.
[00:53:16.000 --> 00:53:18.000]   Some of it's just pure sloppiness.
[00:53:18.000 --> 00:53:20.000]   And there's no reason why we shouldn't go direct.
[00:53:20.000 --> 00:53:22.000]   And people want to hear from us.
[00:53:22.000 --> 00:53:24.000]   The audience wants to hear from us.
[00:53:24.000 --> 00:53:25.000]   Look at Draymond.
[00:53:25.000 --> 00:53:27.000]   Look at Draymond and the success he's had with his podcast.
[00:53:27.000 --> 00:53:30.000]   No basketball player has ever gone direct
[00:53:30.000 --> 00:53:32.000]   and created content like Draymond's created.
[00:53:32.000 --> 00:53:34.000]   And it's totally changed the game.
[00:53:34.000 --> 00:53:35.000]   And he was so clear.
[00:53:35.000 --> 00:53:37.000]   He's like, "We are. I am the media now."
[00:53:37.000 --> 00:53:39.000]   JJ read it.
[00:53:39.000 --> 00:53:40.000]   Old Man and the Three.
[00:53:40.000 --> 00:53:42.000]   Amazing podcast.
[00:53:42.000 --> 00:53:43.000]   I was going to tell you guys a story.
[00:53:43.000 --> 00:53:45.000]   So I was in the Middle East last week--
[00:53:45.000 --> 00:53:47.000]   or this week, sorry.
[00:53:47.000 --> 00:53:49.000]   And I had this crazy experience
[00:53:49.000 --> 00:53:52.000]   where I was trying to understand what was going on in China.
[00:53:52.000 --> 00:53:54.000]   And so I started on CNN.
[00:53:54.000 --> 00:53:58.000]   And the whole thing was the propaganda machine
[00:53:58.000 --> 00:54:01.000]   around a democratic revolt,
[00:54:01.000 --> 00:54:04.000]   pushing for democracy and trying to depose Xi.
[00:54:04.000 --> 00:54:06.000]   Then I moved to Al Arabiya.
[00:54:06.000 --> 00:54:10.000]   So one channel up, I went from channel 10 to channel 11.
[00:54:10.000 --> 00:54:12.000]   And instead, what they were actually doing
[00:54:12.000 --> 00:54:14.000]   was interviewing people on the ground.
[00:54:14.000 --> 00:54:16.000]   And what they were talking about was
[00:54:16.000 --> 00:54:20.000]   literally how these PCR tests have become far too burdensome.
[00:54:20.000 --> 00:54:22.000]   And they just wanted it to end
[00:54:22.000 --> 00:54:25.000]   and more reasonable restrictions to get in and out of quarantine.
[00:54:25.000 --> 00:54:28.000]   Then I went from there to BBC.
[00:54:28.000 --> 00:54:30.000]   And at BBC, they had a China scholar
[00:54:30.000 --> 00:54:33.000]   who was talking about how for decades, actually,
[00:54:33.000 --> 00:54:37.000]   the Communist Party supports local-level protests and demonstrations
[00:54:37.000 --> 00:54:42.000]   because they've realized that it is a part of their political system
[00:54:42.000 --> 00:54:45.000]   to make sure that people feel like they have a say.
[00:54:45.000 --> 00:54:47.000]   And I was, like, taking a step back.
[00:54:47.000 --> 00:54:49.000]   And I'm like, "If you listen to the US narrative
[00:54:49.000 --> 00:54:51.000]   and even Jason, like, in our group chat,
[00:54:51.000 --> 00:54:53.000]   people fomenting for, like, revolution,
[00:54:53.000 --> 00:54:55.000]   and this is Tiananmen 2.0."
[00:54:55.000 --> 00:54:58.000]   And I'm like, "Well, I'm reading two other channels
[00:54:58.000 --> 00:55:01.000]   that tell us a completely different set of things."
[00:55:01.000 --> 00:55:04.000]   And I just thought, "Man, people just really fit the data
[00:55:04.000 --> 00:55:06.000]   to fit their bias."
[00:55:06.000 --> 00:55:08.000]   Yeah, we are projecting.
[00:55:08.000 --> 00:55:10.000]   We want to see a revolution in China.
[00:55:10.000 --> 00:55:12.000]   The people in China want to have their lives back.
[00:55:12.000 --> 00:55:14.000]   Well, I would love to see more democracy in the world.
[00:55:14.000 --> 00:55:17.000]   Yes, guilty as charged.
[00:55:17.000 --> 00:55:20.000]   I would like to see people be more free in the world.
[00:55:20.000 --> 00:55:21.000]   Dictator.
[00:55:21.000 --> 00:55:24.000]   I think most people just want to improve their condition.
[00:55:24.000 --> 00:55:26.000]   And I don't think people are as tied up
[00:55:26.000 --> 00:55:28.000]   on the philosophy of the government
[00:55:28.000 --> 00:55:30.000]   as they are about improving their condition.
[00:55:30.000 --> 00:55:32.000]   And as long as their condition is improving,
[00:55:32.000 --> 00:55:34.000]   they're willing to put up with any form of government.
[00:55:34.000 --> 00:55:35.000]   And history shows that.
[00:55:35.000 --> 00:55:37.000]   By the way, the conditions in China have improved
[00:55:37.000 --> 00:55:39.000]   better than everybody's in the last--
[00:55:39.000 --> 00:55:41.000]   It's better than anyone in history.
[00:55:41.000 --> 00:55:43.000]   They've moved 500 million people out of abject poverty,
[00:55:43.000 --> 00:55:47.000]   and that's the great success of engagement.
[00:55:47.000 --> 00:55:52.000]   Isolationism would not have created that amazing outcome.
[00:55:52.000 --> 00:55:54.000]   500 million people going out of abject--
[00:55:54.000 --> 00:55:56.000]   You're referring to us throwing--
[00:55:56.000 --> 00:55:59.000]   Apple building factories is what I'm referring to.
[00:55:59.000 --> 00:56:00.000]   Oh, okay, fine.
[00:56:00.000 --> 00:56:02.000]   If they want to build something over there,
[00:56:02.000 --> 00:56:04.000]   I guess that's better than us throwing open our markets
[00:56:04.000 --> 00:56:09.000]   and giving China MFN status to destroy American manufacturing
[00:56:09.000 --> 00:56:11.000]   and build up their economy so they can become
[00:56:11.000 --> 00:56:14.000]   a peer competitor to the United States.
[00:56:14.000 --> 00:56:16.000]   I mean, this is the balance of engagement.
[00:56:16.000 --> 00:56:19.000]   If you engage too much, you give everything up.
[00:56:19.000 --> 00:56:24.000]   Sax, which of the current Republican agenda
[00:56:24.000 --> 00:56:27.000]   do you disagree with most strongly, just as an aside?
[00:56:27.000 --> 00:56:31.000]   Well, most Republicans are in favor of our Ukraine policy,
[00:56:31.000 --> 00:56:34.000]   this sort of unlimited appropriation of weapons
[00:56:34.000 --> 00:56:35.000]   and aid to them.
[00:56:35.000 --> 00:56:37.000]   Don't you disagree with immigration policy
[00:56:37.000 --> 00:56:38.000]   of Republicans and Democrats?
[00:56:38.000 --> 00:56:40.000]   Well, I have a more nuanced position on immigration,
[00:56:40.000 --> 00:56:43.000]   which is I think we need to have a border.
[00:56:43.000 --> 00:56:45.000]   It can't be just like an open border,
[00:56:45.000 --> 00:56:47.000]   which is the de facto policy we have now.
[00:56:47.000 --> 00:56:50.000]   But at the same time, I do think that we should have H-1B visas
[00:56:50.000 --> 00:56:52.000]   and we want to, like Jamal said,
[00:56:52.000 --> 00:56:54.000]   we want to be an all-star team for the world.
[00:56:54.000 --> 00:56:56.000]   We want to have the best people who want to come here.
[00:56:56.000 --> 00:56:58.000]   So there's a balance. It's a balance.
[00:56:58.000 --> 00:57:00.000]   And then, look, I think that I was happy to see
[00:57:00.000 --> 00:57:03.000]   the marriage equality bill finally pass the Senate.
[00:57:03.000 --> 00:57:05.000]   Yes, they did get about a dozen--
[00:57:05.000 --> 00:57:07.000]   12 Republicans voted for it. That's great.
[00:57:07.000 --> 00:57:08.000]   So that's great.
[00:57:08.000 --> 00:57:11.000]   But that's not the majority, unfortunately.
[00:57:11.000 --> 00:57:13.000]   You know, look, on what I would categorize
[00:57:13.000 --> 00:57:16.000]   as the old social issues,
[00:57:16.000 --> 00:57:20.000]   you know, like gay marriage, like cannabis legalization,
[00:57:20.000 --> 00:57:22.000]   I was on the liberal side.
[00:57:22.000 --> 00:57:26.000]   Yeah, I don't think, you know, banning abortion entirely
[00:57:26.000 --> 00:57:28.000]   and total abolition is going to work for this country.
[00:57:28.000 --> 00:57:30.000]   I think Republicans will lose elections
[00:57:30.000 --> 00:57:31.000]   if they insist on that,
[00:57:31.000 --> 00:57:33.000]   and I think they're getting that message.
[00:57:33.000 --> 00:57:37.000]   So, yeah, I mean, look, I think that--
[00:57:37.000 --> 00:57:40.000]   I've always considered myself to be pretty centrist.
[00:57:40.000 --> 00:57:42.000]   And so you're not a globalist.
[00:57:42.000 --> 00:57:45.000]   You don't believe in open global markets with the U.S.?
[00:57:45.000 --> 00:57:48.000]   In general, I understand the benefits of free trade,
[00:57:48.000 --> 00:57:51.000]   and I don't think we should be isolationist
[00:57:51.000 --> 00:57:52.000]   with respect to trade.
[00:57:52.000 --> 00:57:54.000]   I don't think that we can be a successful country
[00:57:54.000 --> 00:57:57.000]   if we are-- we isolate our economy.
[00:57:57.000 --> 00:57:59.000]   So I do want to trade.
[00:57:59.000 --> 00:58:02.000]   However, with China in particular,
[00:58:02.000 --> 00:58:04.000]   I think we made a mistake
[00:58:04.000 --> 00:58:06.000]   in throwing open our markets to their products,
[00:58:06.000 --> 00:58:08.000]   giving them MFN status, while--
[00:58:08.000 --> 00:58:10.000]   Most favored nation.
[00:58:10.000 --> 00:58:11.000]   Yes, enriching them to the point
[00:58:11.000 --> 00:58:14.000]   where they became a peer competitor to the United States.
[00:58:14.000 --> 00:58:17.000]   Now, look, I understand why we made that mistake 20 years ago,
[00:58:17.000 --> 00:58:19.000]   because everybody thought the theory was
[00:58:19.000 --> 00:58:21.000]   that if we helped China become rich,
[00:58:21.000 --> 00:58:24.000]   that China would inevitably become more democratic,
[00:58:24.000 --> 00:58:26.000]   and they'd be filled with gratitude
[00:58:26.000 --> 00:58:27.000]   towards the United States,
[00:58:27.000 --> 00:58:30.000]   and they'd actually become more--
[00:58:30.000 --> 00:58:32.000]   more hospitable towards us, more westernized.
[00:58:32.000 --> 00:58:35.000]   And I think that theory has just proven to be wrong.
[00:58:35.000 --> 00:58:36.000]   I mean, they have not--
[00:58:36.000 --> 00:58:39.000]   Or it's going very slowly, one or the other.
[00:58:39.000 --> 00:58:41.000]   Let's own our clips here.
[00:58:41.000 --> 00:58:44.000]   Here is Shamat's prediction from episode 61,
[00:58:44.000 --> 00:58:47.000]   and we'll see you on the other side of this quick clip.
[00:58:47.000 --> 00:58:53.000]   My worldwide biggest political winner for 2022 is Xi Jinping.
[00:58:53.000 --> 00:58:56.000]   I think this guy is--
[00:58:56.000 --> 00:58:59.000]   He's firing on all cylinders,
[00:58:59.000 --> 00:59:02.000]   and he is basically ascendant.
[00:59:02.000 --> 00:59:04.000]   So 2022 marks the first year
[00:59:04.000 --> 00:59:06.000]   where he's essentially really ruler for life.
[00:59:06.000 --> 00:59:08.000]   And so I don't think we really know
[00:59:08.000 --> 00:59:10.000]   what he's capable of and what he's going to do.
[00:59:10.000 --> 00:59:11.000]   And so that's just going to play out.
[00:59:11.000 --> 00:59:13.000]   You think he's the biggest political winner, really?
[00:59:13.000 --> 00:59:16.000]   Oh, my God. I think it's going to be a--
[00:59:16.000 --> 00:59:17.000]   He's going to run roughshod,
[00:59:17.000 --> 00:59:19.000]   not just domestically, but also internationally,
[00:59:19.000 --> 00:59:21.000]   because you have to remember,
[00:59:21.000 --> 00:59:24.000]   he controls so much of the critical supply chain
[00:59:24.000 --> 00:59:26.000]   that the Western world needs to be--
[00:59:26.000 --> 00:59:28.000]   I think you're completely wrong.
[00:59:28.000 --> 00:59:29.000]   I think you're completely wrong.
[00:59:29.000 --> 00:59:31.000]   I think he's losing his power. He's scared.
[00:59:31.000 --> 00:59:33.000]   That's why he took out all these CEOs.
[00:59:33.000 --> 00:59:34.000]   He's consolidating power,
[00:59:34.000 --> 00:59:38.000]   because he fears that they're going to win too big
[00:59:38.000 --> 00:59:39.000]   and then displace him.
[00:59:39.000 --> 00:59:42.000]   And he has massive real estate problems over there
[00:59:42.000 --> 00:59:43.000]   that could blow up at any moment in time.
[00:59:43.000 --> 00:59:44.000]   He could face a civil war there.
[00:59:44.000 --> 00:59:46.000]   I think he's totally isolated himself.
[00:59:46.000 --> 00:59:48.000]   Civil war? They don't even have guns.
[00:59:48.000 --> 00:59:51.000]   Every major country is removing their factories
[00:59:51.000 --> 00:59:52.000]   and removing its dependency.
[00:59:52.000 --> 00:59:53.000]   They don't even have any guns there.
[00:59:53.000 --> 00:59:54.000]   What are you talking about?
[00:59:54.000 --> 00:59:56.000]   What are they going to riot with?
[00:59:56.000 --> 00:59:58.000]   Did you not see Tiananmen Square?
[00:59:58.000 --> 01:00:00.000]   Did you not see the riots in Hong Kong?
[01:00:00.000 --> 01:00:01.000]   Are you not paying attention, Shamath?
[01:00:01.000 --> 01:00:03.000]   There's been many riots in China.
[01:00:03.000 --> 01:00:05.000]   Jason, Zulus were crushed.
[01:00:05.000 --> 01:00:07.000]   I'm not saying they will be crushed,
[01:00:07.000 --> 01:00:10.000]   but he still will have massive amounts of,
[01:00:10.000 --> 01:00:12.000]   I believe, protests.
[01:00:12.000 --> 01:00:13.000]   And yeah, he'll have to kill people.
[01:00:13.000 --> 01:00:16.000]   I think the bigger risk is that
[01:00:16.000 --> 01:00:18.000]   China gets better for Xi Jinping,
[01:00:18.000 --> 01:00:20.000]   but worse for everybody else in China.
[01:00:20.000 --> 01:00:22.000]   It's already worse for all the billionaires over there.
[01:00:22.000 --> 01:00:24.000]   It's worse for the tech industry.
[01:00:24.000 --> 01:00:25.000]   You've now got Evergrande,
[01:00:25.000 --> 01:00:29.000]   that whole gigantic debt implosion.
[01:00:29.000 --> 01:00:32.000]   I think there could be contagion from China next year.
[01:00:32.000 --> 01:00:35.000]   I don't think Xi's going to lose his grip in any way,
[01:00:35.000 --> 01:00:38.000]   but I'm not sure China's going to have a good year next year.
[01:00:38.000 --> 01:00:40.000]   Wow. Nailed it.
[01:00:40.000 --> 01:00:42.000]   I think all three of us kind of got this right.
[01:00:42.000 --> 01:00:43.000]   What are you talking about?
[01:00:43.000 --> 01:00:44.000]   You got none of it right.
[01:00:44.000 --> 01:00:47.000]   I said there were going to be riots
[01:00:47.000 --> 01:00:49.000]   and they're going to have a recession.
[01:00:49.000 --> 01:00:50.000]   Jason, let's be honest.
[01:00:50.000 --> 01:00:51.000]   You said that they would be squashed.
[01:00:51.000 --> 01:00:53.000]   That's exactly what happened.
[01:00:53.000 --> 01:00:54.000]   Both things happened.
[01:00:54.000 --> 01:00:56.000]   I actually think I have a pretty decent ability
[01:00:56.000 --> 01:00:59.000]   to steel man pretty concretely the details.
[01:00:59.000 --> 01:01:00.000]   I think that at best,
[01:01:00.000 --> 01:01:02.000]   when it comes to things like democracy
[01:01:02.000 --> 01:01:04.000]   and your belief in US exceptionalism
[01:01:04.000 --> 01:01:06.000]   in a specific political worldview,
[01:01:06.000 --> 01:01:08.000]   at best you straw man.
[01:01:08.000 --> 01:01:09.000]   I think that you get very biased
[01:01:09.000 --> 01:01:12.000]   without seeing the forest from the trees.
[01:01:12.000 --> 01:01:14.000]   The reason I said that is not because I'm like
[01:01:14.000 --> 01:01:16.000]   some huge Xi supporter.
[01:01:16.000 --> 01:01:18.000]   I'm just trying to steel man what happens
[01:01:18.000 --> 01:01:20.000]   when one individual person
[01:01:20.000 --> 01:01:23.000]   gets anointed leader for life
[01:01:23.000 --> 01:01:25.000]   of 1.3 billion people
[01:01:25.000 --> 01:01:28.000]   that then controls 20% of the world's GDP.
[01:01:28.000 --> 01:01:31.000]   There is no other single human being
[01:01:31.000 --> 01:01:36.000]   as powerful as him as of this month.
[01:01:36.000 --> 01:01:38.000]   Can I just say that this show
[01:01:38.000 --> 01:01:40.000]   is going to become insufferable
[01:01:40.000 --> 01:01:42.000]   if every time you sort of said something
[01:01:42.000 --> 01:01:45.000]   in the past that was sort of correct,
[01:01:45.000 --> 01:01:47.000]   we're going to have to replay it?
[01:01:47.000 --> 01:01:50.000]   I was actually playing that one for you, Sax.
[01:01:50.000 --> 01:01:51.000]   I think you're the one who nailed it.
[01:01:51.000 --> 01:01:53.000]   I was giving that was a softball to you.
[01:01:53.000 --> 01:01:55.000]   I nail it every week, J. Cal.
[01:01:55.000 --> 01:01:56.000]   This show is going to slow down
[01:01:56.000 --> 01:01:58.000]   if we play every clip that I got right.
[01:01:58.000 --> 01:02:00.000]   You guys are asking to pull clips all the time.
[01:02:00.000 --> 01:02:02.000]   I just pulled one clip about China.
[01:02:02.000 --> 01:02:03.000]   You nailed it.
[01:02:03.000 --> 01:02:04.000]   No, no, no, look.
[01:02:04.000 --> 01:02:05.000]   I think like and also look what he did.
[01:02:05.000 --> 01:02:06.000]   Chamath's the king of pull a clip.
[01:02:06.000 --> 01:02:09.000]   The Evergrande thing, look what he did this week.
[01:02:09.000 --> 01:02:10.000]   They said, okay, you know what?
[01:02:10.000 --> 01:02:12.000]   The real estate industry can now issue
[01:02:12.000 --> 01:02:13.000]   secondary stock sales,
[01:02:13.000 --> 01:02:15.000]   raise equity and equitize themselves.
[01:02:15.000 --> 01:02:17.000]   So they're going to find a soft landing
[01:02:17.000 --> 01:02:18.000]   for the equity part
[01:02:18.000 --> 01:02:21.000]   for the real estate industry in China.
[01:02:21.000 --> 01:02:23.000]   And now they're reopening.
[01:02:23.000 --> 01:02:24.000]   So I don't know.
[01:02:24.000 --> 01:02:25.000]   I mean, like I'm not sure
[01:02:25.000 --> 01:02:26.000]   what we're supposed to comment.
[01:02:26.000 --> 01:02:28.000]   What I what I will stand by is what I said,
[01:02:28.000 --> 01:02:31.000]   which is I don't think we have a very clear view
[01:02:31.000 --> 01:02:33.000]   about what's going on,
[01:02:33.000 --> 01:02:35.000]   what the substance of these protests are
[01:02:35.000 --> 01:02:37.000]   and what people actually want.
[01:02:37.000 --> 01:02:39.000]   If you're only consuming US media.
[01:02:39.000 --> 01:02:42.000]   And so if you find a way to get a diet
[01:02:42.000 --> 01:02:44.000]   from a bunch of different sources all around the world,
[01:02:44.000 --> 01:02:46.000]   you may get a better sense.
[01:02:46.000 --> 01:02:48.000]   I had an accidental window into that
[01:02:48.000 --> 01:02:50.000]   by being in a completely different part of the world.
[01:02:50.000 --> 01:02:51.000]   This past week,
[01:02:51.000 --> 01:02:53.000]   freeburg, any final thoughts on China?
[01:02:53.000 --> 01:02:55.000]   And what's going on there
[01:02:55.000 --> 01:02:57.000]   before we move on to chat GPT?
[01:02:57.000 --> 01:02:58.000]   Your favorite story?
[01:02:58.000 --> 01:03:00.000]   I think one of the things we often miss
[01:03:00.000 --> 01:03:03.000]   is that China, the CCP
[01:03:03.000 --> 01:03:08.000]   does have their hand on the throttles,
[01:03:08.000 --> 01:03:10.000]   like they throttle up and down.
[01:03:10.000 --> 01:03:13.000]   We always think that it's a linear line
[01:03:13.000 --> 01:03:15.000]   and that it's super dogmatic and fixed,
[01:03:15.000 --> 01:03:17.000]   but there's certainly responsiveness
[01:03:17.000 --> 01:03:19.000]   and the release of the lockdowns
[01:03:19.000 --> 01:03:21.000]   in Guangzhou and Beijing this week
[01:03:21.000 --> 01:03:24.000]   seems to have been a pretty good indication
[01:03:24.000 --> 01:03:25.000]   that when things do get,
[01:03:25.000 --> 01:03:28.000]   when things when the tides do change,
[01:03:28.000 --> 01:03:30.000]   leadership there seems to respond,
[01:03:30.000 --> 01:03:34.000]   not always, but enough to kind of keep things going.
[01:03:34.000 --> 01:03:35.000]   So should they reopen?
[01:03:35.000 --> 01:03:37.000]   I think it's 60% of the population or so
[01:03:37.000 --> 01:03:40.000]   is vaccinated with obviously vaccines
[01:03:40.000 --> 01:03:43.000]   that maybe aren't as don't have the same efficacy
[01:03:43.000 --> 01:03:45.000]   as the ones here in the United States.
[01:03:45.000 --> 01:03:47.000]   Do you think they should open up
[01:03:47.000 --> 01:03:49.000]   and just let it rip?
[01:03:49.000 --> 01:03:51.000]   Or do you think they should still try to
[01:03:51.000 --> 01:03:52.000]   maintain the zero COVID policy?
[01:03:52.000 --> 01:03:54.000]   Because that is the debate right now.
[01:03:54.000 --> 01:03:57.000]   From what point of view, what's the objective?
[01:03:57.000 --> 01:03:59.000]   Because obviously from the objective
[01:03:59.000 --> 01:04:01.000]   of economic growth, they need to open up
[01:04:01.000 --> 01:04:02.000]   and they need to keep their economy working
[01:04:02.000 --> 01:04:05.000]   and they need to keep their labor force engaged
[01:04:05.000 --> 01:04:07.000]   or else they're going to continue to suffer.
[01:04:07.000 --> 01:04:11.000]   So if economic growth is the objective,
[01:04:11.000 --> 01:04:13.000]   they need to open up, right?
[01:04:13.000 --> 01:04:16.000]   If the long term health cost of the nation
[01:04:16.000 --> 01:04:19.000]   balanced against that is the calculus
[01:04:19.000 --> 01:04:20.000]   that they're kind of weighing,
[01:04:20.000 --> 01:04:23.000]   there's probably some more nuance to that.
[01:04:23.000 --> 01:04:28.000]   And certainly my understanding is there may be
[01:04:28.000 --> 01:04:31.000]   a precedent setting, which is,
[01:04:31.000 --> 01:04:33.000]   hey, we've said that it's a zero COVID policy,
[01:04:33.000 --> 01:04:35.000]   therefore we have to hold strict to it,
[01:04:35.000 --> 01:04:37.000]   hold toe to the line,
[01:04:37.000 --> 01:04:39.000]   or else it looks like we're weak.
[01:04:39.000 --> 01:04:40.000]   And so there's also this, you know,
[01:04:40.000 --> 01:04:44.000]   maintaining the authority of the CCP objective.
[01:04:44.000 --> 01:04:47.000]   So there's a lot of maybe competing objectives right now.
[01:04:47.000 --> 01:04:50.000]   Certainly don't have a sense of how they're weighing them all.
[01:04:50.000 --> 01:04:54.000]   But I think that once all those videos came out this week,
[01:04:54.000 --> 01:04:56.000]   you guys saw them, but people were screaming,
[01:04:56.000 --> 01:04:57.000]   there was an apartment on fire,
[01:04:57.000 --> 01:04:59.000]   or the doors were locked with steel beams
[01:04:59.000 --> 01:05:01.000]   on the base of the building.
[01:05:01.000 --> 01:05:03.000]   At least that's what the video said.
[01:05:03.000 --> 01:05:05.000]   I don't know how much truth there is to that,
[01:05:05.000 --> 01:05:06.000]   but that's what was said.
[01:05:06.000 --> 01:05:08.000]   And clearly people are extremely distraught
[01:05:08.000 --> 01:05:11.000]   and unhappy with the conditions of the lockdown.
[01:05:12.000 --> 01:05:15.000]   At some point enough people with enough loud voices,
[01:05:15.000 --> 01:05:19.000]   you know, something's going to change.
[01:05:19.000 --> 01:05:21.000]   I mean, let's just remember like the bargain
[01:05:21.000 --> 01:05:24.000]   that struck in that country and with all countries
[01:05:24.000 --> 01:05:27.000]   is that, you know, the citizenry to some extent
[01:05:27.000 --> 01:05:29.000]   are willing to tolerate
[01:05:29.000 --> 01:05:34.000]   their government so long as their conditions
[01:05:34.000 --> 01:05:35.000]   continue to improve.
[01:05:35.000 --> 01:05:38.000]   And there's a bargain, there's some bargain that struck.
[01:05:38.000 --> 01:05:43.000]   But as that bargain starts to go south for the citizenry,
[01:05:43.000 --> 01:05:46.000]   then that governing entity is at risk.
[01:05:46.000 --> 01:05:48.000]   And I think that that's what we sort of started
[01:05:48.000 --> 01:05:51.000]   to see this week was the conditions are getting
[01:05:51.000 --> 01:05:53.000]   far, far worse and far less livable
[01:05:53.000 --> 01:05:55.000]   for so many people in that country
[01:05:55.000 --> 01:05:58.000]   that the government had to shift.
[01:05:58.000 --> 01:06:01.000]   - Do you think Chamath, this COVID strategy,
[01:06:01.000 --> 01:06:02.000]   and then we'll move on,
[01:06:02.000 --> 01:06:06.000]   was basically Xi Jinping wanting to get to that Congress,
[01:06:06.000 --> 01:06:08.000]   to get to that coronation.
[01:06:08.000 --> 01:06:11.000]   And now that that's over, maybe he can change gears.
[01:06:11.000 --> 01:06:12.000]   And then--
[01:06:12.000 --> 01:06:16.000]   - Like I said, my belief is that I have a very poor access
[01:06:16.000 --> 01:06:19.000]   to enough data to have a, to steel man
[01:06:19.000 --> 01:06:22.000]   what is actually going on there.
[01:06:22.000 --> 01:06:25.000]   But one explanation could actually be that
[01:06:25.000 --> 01:06:28.000]   in the absence of enough hospital infrastructure
[01:06:28.000 --> 01:06:31.000]   and ventilators and a bunch of these other things,
[01:06:31.000 --> 01:06:36.000]   a pretty severe approach to this disease,
[01:06:36.000 --> 01:06:38.000]   I don't know what they know or didn't know.
[01:06:38.000 --> 01:06:42.000]   Maybe they understood the virulence of it.
[01:06:42.000 --> 01:06:44.000]   Maybe that they have a slightly different
[01:06:44.000 --> 01:06:46.000]   aging characteristics of their population.
[01:06:46.000 --> 01:06:50.000]   Maybe they genetically responded to the SARS-CoV-2 virus.
[01:06:50.000 --> 01:06:53.000]   I don't know any of these things enough to tell you, Jason.
[01:06:53.000 --> 01:06:54.000]   - Yeah.
[01:06:54.000 --> 01:06:56.000]   - But the reality is what Freeberg says is right,
[01:06:56.000 --> 01:06:58.000]   which is that you cannot grow an economy
[01:06:58.000 --> 01:07:01.000]   if people are inside locked in their apartments.
[01:07:01.000 --> 01:07:04.000]   And it looks like they have decided
[01:07:04.000 --> 01:07:07.000]   that that's coming to an end and they're going to,
[01:07:07.000 --> 01:07:10.000]   you know, deconstruct all of these things.
[01:07:10.000 --> 01:07:14.000]   So, you know, the Chinese growth engine is coming back.
[01:07:14.000 --> 01:07:17.000]   And I think that that's going to be an important factor
[01:07:17.000 --> 01:07:20.000]   economically that we're going to have to figure out
[01:07:20.000 --> 01:07:23.000]   because it's going to have a huge implication
[01:07:23.000 --> 01:07:26.000]   to American growth and American inflation.
[01:07:26.000 --> 01:07:31.000]   - Sax, if in fact, if the lab league theory is correct,
[01:07:31.000 --> 01:07:34.000]   China might have some insights into this disease
[01:07:34.000 --> 01:07:36.000]   that maybe the West didn't.
[01:07:36.000 --> 01:07:39.000]   Maybe that plays into their policy a bit.
[01:07:39.000 --> 01:07:40.000]   Any final thoughts here?
[01:07:40.000 --> 01:07:42.000]   - Jake, I'm just surprised to hear
[01:07:42.000 --> 01:07:46.000]   that you have a problem with their lockdown policy over there
[01:07:46.000 --> 01:07:48.000]   because aren't they just implementing
[01:07:48.000 --> 01:07:50.000]   Democratic Party orthodoxy?
[01:07:50.000 --> 01:07:53.000]   I mean, isn't this the policy that Tony Fauci
[01:07:53.000 --> 01:07:56.000]   and Barbara Ferrer, you know, all the health experts.
[01:07:56.000 --> 01:07:57.000]   - I was not in favor of lockdowns.
[01:07:57.000 --> 01:07:58.000]   What are you talking about?
[01:07:58.000 --> 01:07:59.000]   You're the one who had all your masks
[01:07:59.000 --> 01:08:01.000]   and had ventilators day one.
[01:08:01.000 --> 01:08:03.000]   - Isn't this the lockdown policy?
[01:08:03.000 --> 01:08:04.000]   - Why are you painting this?
[01:08:04.000 --> 01:08:06.000]   Why am I getting a stray here?
[01:08:06.000 --> 01:08:08.000]   I'm just asking a question as the moderator.
[01:08:08.000 --> 01:08:10.000]   - Isn't this basically--
[01:08:10.000 --> 01:08:11.000]   - Why am I getting a stray?
[01:08:11.000 --> 01:08:13.000]   - Isn't this what Gavin Newsom--
[01:08:13.000 --> 01:08:14.000]   - Don't deflect Jake, I'll answer the question.
[01:08:14.000 --> 01:08:16.000]   - Yes, hold on, let me finish the question.
[01:08:16.000 --> 01:08:18.000]   - I'm not in favor of lockdowns.
[01:08:18.000 --> 01:08:21.000]   I was in favor of if people wanted to stay home, stay home.
[01:08:21.000 --> 01:08:23.000]   And then if people wanted to go out and take the risk,
[01:08:23.000 --> 01:08:24.000]   take the risk.
[01:08:24.000 --> 01:08:25.000]   I was always in personal choice.
[01:08:25.000 --> 01:08:28.000]   - Isn't this what Gretchen Whitmer in Michigan
[01:08:28.000 --> 01:08:31.000]   and Gavin Newsom in California subscribed to,
[01:08:31.000 --> 01:08:34.000]   the idea that the way to fight COVID was through lockdowns.
[01:08:34.000 --> 01:08:37.000]   Now, yes, Newsom had 10 pages of exceptions
[01:08:37.000 --> 01:08:38.000]   for his political donors.
[01:08:38.000 --> 01:08:39.000]   - Absolutely.
[01:08:39.000 --> 01:08:40.000]   - And he didn't use the police
[01:08:40.000 --> 01:08:41.000]   to lock people in apartment buildings.
[01:08:41.000 --> 01:08:44.000]   He may have wanted to, but they didn't actually do that.
[01:08:44.000 --> 01:08:47.000]   But can you really tell me that this lockdown policy
[01:08:47.000 --> 01:08:50.000]   has been disavowed by people like Fauci
[01:08:50.000 --> 01:08:52.000]   or by the health authorities
[01:08:52.000 --> 01:08:54.000]   like the Barbara Ferrer's of this world?
[01:08:54.000 --> 01:08:56.000]   They still subscribe to this view.
[01:08:56.000 --> 01:08:57.000]   - Do they really?
[01:08:57.000 --> 01:08:58.000]   Is anybody doing lockdown now?
[01:08:58.000 --> 01:09:00.000]   - Show me, well, they're not able to do it
[01:09:00.000 --> 01:09:02.000]   because no one agrees anymore.
[01:09:02.000 --> 01:09:04.000]   But tell me where anybody,
[01:09:04.000 --> 01:09:06.000]   tell me where any of the health experts
[01:09:06.000 --> 01:09:08.000]   who said that lockdowns were the correct response
[01:09:08.000 --> 01:09:10.000]   have repented and disavowed that view.
[01:09:10.000 --> 01:09:13.000]   - Yeah, I don't know.
[01:09:13.000 --> 01:09:15.000]   I haven't been tracking their mea copas.
[01:09:15.000 --> 01:09:18.000]   You yourself were in favor of lockdowns
[01:09:18.000 --> 01:09:19.000]   for a period of time.
[01:09:19.000 --> 01:09:20.000]   Yes, you were.
[01:09:20.000 --> 01:09:21.000]   You absolutely were.
[01:09:21.000 --> 01:09:22.000]   I'll pull the tape for the next episode.
[01:09:22.000 --> 01:09:23.000]   - No, I was in favor of a mask mandate.
[01:09:23.000 --> 01:09:24.000]   No, that's not true.
[01:09:24.000 --> 01:09:26.000]   I was in favor of a mask mandate.
[01:09:26.000 --> 01:09:27.000]   And I said the mask mandate
[01:09:27.000 --> 01:09:29.000]   was the alternative to lockdowns.
[01:09:29.000 --> 01:09:30.000]   - Okay.
[01:09:30.000 --> 01:09:32.000]   - I was saying that by May of 2020.
[01:09:32.000 --> 01:09:34.000]   - All right, listen, let's move on to the next thing.
[01:09:34.000 --> 01:09:35.000]   I don't represent, I'm an independent.
[01:09:35.000 --> 01:09:37.000]   I don't represent the Democratic Party.
[01:09:37.000 --> 01:09:39.000]   I don't represent Fauci.
[01:09:39.000 --> 01:09:41.000]   - No, you represent mainstream media.
[01:09:41.000 --> 01:09:43.000]   - I do not represent mainstream media.
[01:09:43.000 --> 01:09:45.000]   - You just said I was an old school journalist
[01:09:45.000 --> 01:09:47.000]   who's mortified with where it is today.
[01:09:47.000 --> 01:09:48.000]   - I'm giving you a hard time.
[01:09:48.000 --> 01:09:51.000]   I thought your explanation was fabulous.
[01:09:51.000 --> 01:09:52.000]   - Oh, thank you.
[01:09:52.000 --> 01:09:54.000]   - Obviously, Jake, I'm giving you a hard time too.
[01:09:54.000 --> 01:09:56.000]   I know that you were not a big lockdown proponent,
[01:09:56.000 --> 01:09:58.000]   but you understand the point I was making,
[01:09:58.000 --> 01:10:00.000]   which is there were a lot of proponents in the US.
[01:10:00.000 --> 01:10:03.000]   - Let's stop making me the Democratic spokesperson.
[01:10:03.000 --> 01:10:04.000]   - I just want to let you guys know
[01:10:04.000 --> 01:10:06.000]   I'm feeling a lot better after my beer.
[01:10:06.000 --> 01:10:08.000]   Let's talk about open AI.
[01:10:08.000 --> 01:10:09.000]   Come on, people.
[01:10:09.000 --> 01:10:10.000]   - All right, let's go.
[01:10:10.000 --> 01:10:11.000]   - Let's talk about open AI.
[01:10:11.000 --> 01:10:12.000]   - All right, open AI.
[01:10:12.000 --> 01:10:17.000]   All right, open AI is a company that builds
[01:10:17.000 --> 01:10:21.000]   artificial intelligence software and platforms.
[01:10:21.000 --> 01:10:24.000]   They have one platform called GPT.
[01:10:24.000 --> 01:10:26.000]   It is on its third version.
[01:10:26.000 --> 01:10:29.000]   As part of GPT-3, they created ChatGPT,
[01:10:29.000 --> 01:10:31.000]   which is a chat interface
[01:10:31.000 --> 01:10:34.000]   where you can ask questions to AI.
[01:10:34.000 --> 01:10:38.000]   The results are nothing short of stunning when they hit.
[01:10:38.000 --> 01:10:40.000]   Some of them are a little bit mixed,
[01:10:40.000 --> 01:10:44.000]   but Freeberg has spent the last 48 hours
[01:10:44.000 --> 01:10:47.000]   drinking white Russians with Oatly milk
[01:10:47.000 --> 01:10:49.000]   and playing with ChatGPT,
[01:10:49.000 --> 01:10:52.000]   including his question to the ChatGPT,
[01:10:52.000 --> 01:10:56.000]   which was write a script of Chamath, Sax, and J. Cal
[01:10:56.000 --> 01:10:58.000]   talking about the future of AI
[01:10:58.000 --> 01:11:01.000]   in the style of a Quentin Tarantino movie.
[01:11:01.000 --> 01:11:05.000]   And I have to say, it was pretty great, the result.
[01:11:05.000 --> 01:11:08.000]   Go ahead, Freeberg, tell us what you discovered.
[01:11:08.000 --> 01:11:09.000]   - I think you guys should read this real quick.
[01:11:09.000 --> 01:11:11.000]   We can cut it out if it's stupid, but okay.
[01:11:11.000 --> 01:11:12.000]   - Just actually read the script?
[01:11:12.000 --> 01:11:13.000]   - Yeah, read the script.
[01:11:13.000 --> 01:11:15.000]   - I don't think they got the Quentin Tarantino part right.
[01:11:15.000 --> 01:11:16.000]   I mean, I think it was impressive,
[01:11:16.000 --> 01:11:19.000]   but there's no F-bombs, there's no Tarantino-esque dialogue.
[01:11:19.000 --> 01:11:22.000]   - And by the way, they excluded all of that from open AI.
[01:11:22.000 --> 01:11:25.000]   They took out all cussing and violence from open AI.
[01:11:25.000 --> 01:11:28.000]   - That explains it, because if this was a Tarantino script,
[01:11:28.000 --> 01:11:29.000]   there would be some choice language in here.
[01:11:29.000 --> 01:11:30.000]   - Totally, but it's interesting.
[01:11:30.000 --> 01:11:32.000]   It's like the PG Tarantino script,
[01:11:32.000 --> 01:11:35.000]   with no cussing, no violence, no sex.
[01:11:35.000 --> 01:11:37.000]   - There's no pop culture references.
[01:11:37.000 --> 01:11:38.000]   - Yeah, you do the narrative.
[01:11:38.000 --> 01:11:39.000]   Go ahead.
[01:11:39.000 --> 01:11:43.000]   (gentle music)
[01:11:43.000 --> 01:11:45.000]   - The scene opens on Chamath Palihapitiya.
[01:11:45.000 --> 01:11:47.000]   David Sachs and Jason Calacanis
[01:11:47.000 --> 01:11:49.000]   sitting at a dimly lit table in a smoky bar.
[01:11:49.000 --> 01:11:53.000]   A bottle of whiskey sits in the center of the table, half empty.
[01:11:53.000 --> 01:11:55.000]   Chamath leaning forward, his eyes intense.
[01:11:55.000 --> 01:11:59.000]   - So, what do you guys think about this chat GPT thing?
[01:11:59.000 --> 01:12:01.000]   - David Sachs takes a sip of his drink.
[01:12:01.000 --> 01:12:03.000]   - It's impressive, no doubt about it.
[01:12:03.000 --> 01:12:05.000]   A large language model trained by open AI
[01:12:05.000 --> 01:12:06.000]   that can generate human-like texts
[01:12:06.000 --> 01:12:08.000]   in a variety of styles and formats.
[01:12:08.000 --> 01:12:09.000]   It's a game changer.
[01:12:09.000 --> 01:12:10.000]   - Jason nods.
[01:12:10.000 --> 01:12:11.000]   - Definitely.
[01:12:11.000 --> 01:12:14.000]   It's one of the most advanced AI systems we've seen to date.
[01:12:14.000 --> 01:12:17.000]   It's gonna have a huge impact on the way we use
[01:12:17.000 --> 01:12:19.000]   and interact with AI.
[01:12:19.000 --> 01:12:21.000]   - Chamath frowns.
[01:12:21.000 --> 01:12:22.000]   - That's what I'm worried about.
[01:12:22.000 --> 01:12:23.000]   With a technology like this,
[01:12:23.000 --> 01:12:25.000]   the possibilities are endless,
[01:12:25.000 --> 01:12:27.000]   but so are the potential dangers.
[01:12:27.000 --> 01:12:29.000]   - Sachs leans back in his chair.
[01:12:29.000 --> 01:12:30.000]   - Oh, come on, Chamath.
[01:12:30.000 --> 01:12:32.000]   You're not still stuck on that whole
[01:12:32.000 --> 01:12:34.000]   AI is gonna kill us all thing, are you?
[01:12:34.000 --> 01:12:36.000]   - Chamath leans forward again,
[01:12:36.000 --> 01:12:38.000]   his voice low and serious.
[01:12:38.000 --> 01:12:41.000]   - Look, I'm not saying that chat GPT
[01:12:41.000 --> 01:12:44.000]   is going to turn into Skynet overnight,
[01:12:44.000 --> 01:12:46.000]   but we need to be careful
[01:12:46.000 --> 01:12:49.000]   about the way we develop and deploy AI.
[01:12:49.000 --> 01:12:51.000]   We need to make sure that we're using it for good
[01:12:51.000 --> 01:12:53.000]   and not just for our own gain.
[01:12:53.000 --> 01:12:54.000]   (laughs)
[01:12:54.000 --> 01:12:55.000]   - J. Cal nods.
[01:12:55.000 --> 01:12:56.000]   - I agree.
[01:12:56.000 --> 01:12:58.000]   We need to make sure that we're responsible
[01:12:58.000 --> 01:12:59.000]   in the way we use AI,
[01:12:59.000 --> 01:13:02.000]   and that includes making sure it's accessible
[01:13:02.000 --> 01:13:04.000]   and affordable to everyone,
[01:13:04.000 --> 01:13:05.000]   not just a select few.
[01:13:05.000 --> 01:13:07.000]   - Chamath with a dirty smirk.
[01:13:07.000 --> 01:13:08.000]   - Ha ha ha ha.
[01:13:08.000 --> 01:13:11.000]   And let's not forget about the importance of regulation.
[01:13:11.000 --> 01:13:12.000]   Ha ha.
[01:13:12.000 --> 01:13:14.000]   We need to make sure that we have the right laws,
[01:13:14.000 --> 01:13:15.000]   ha ha, in place, ha ha.
[01:13:15.000 --> 01:13:16.000]   - Okay, okay, this is the most.
[01:13:16.000 --> 01:13:17.000]   - To prevent the misuse of AI.
[01:13:17.000 --> 01:13:19.000]   - I'm calling, the director's calling, cut.
[01:13:19.000 --> 01:13:21.000]   - Cut, that's it, scene.
[01:13:21.000 --> 01:13:22.000]   - Cut.
[01:13:22.000 --> 01:13:23.000]   - That's it, scene.
[01:13:23.000 --> 01:13:24.000]   We're not gonna finish it.
[01:13:24.000 --> 01:13:25.000]   - I mean, it's not that far off.
[01:13:25.000 --> 01:13:27.000]   I mean, it's 60% of the way there.
[01:13:27.000 --> 01:13:29.000]   - Actually, there isn't a better one.
[01:13:29.000 --> 01:13:31.000]   - All you have to do is put in a Biden for Sachs.
[01:13:31.000 --> 01:13:33.000]   If you blame Biden, it would have been perfect.
[01:13:33.000 --> 01:13:35.000]   - Let me tell you guys something stunning about this,
[01:13:35.000 --> 01:13:37.000]   this platform.
[01:13:37.000 --> 01:13:39.000]   So this is GPT 3.5,
[01:13:39.000 --> 01:13:41.000]   which is an interim model to the,
[01:13:41.000 --> 01:13:45.000]   what people are saying is the long awaited GPT 4.0 model,
[01:13:45.000 --> 01:13:47.000]   which I think they announced in 2020,
[01:13:47.000 --> 01:13:50.000]   and has been in development for some time.
[01:13:50.000 --> 01:13:54.000]   So the model, this GPT 3.5 model was trained in three steps.
[01:13:54.000 --> 01:13:59.000]   They do a great job explaining it on the OpenAI blog site,
[01:13:59.000 --> 01:14:01.000]   where they collect some data,
[01:14:01.000 --> 01:14:03.000]   and then there's a supervised model,
[01:14:03.000 --> 01:14:06.000]   meaning that there are humans that are involved in tagging.
[01:14:06.000 --> 01:14:09.000]   And then the model kind of learns from that system.
[01:14:09.000 --> 01:14:12.000]   Then you ask the model questions, you get output.
[01:14:12.000 --> 01:14:14.000]   And then humans rank the output.
[01:14:14.000 --> 01:14:16.000]   And so the model learns through that ranking system.
[01:14:16.000 --> 01:14:18.000]   And then there's kind of this third optimization thing,
[01:14:18.000 --> 01:14:20.000]   and then it's fine tuned.
[01:14:20.000 --> 01:14:23.000]   So the model itself has several steps
[01:14:23.000 --> 01:14:24.000]   of kind of human involvement,
[01:14:24.000 --> 01:14:28.000]   and it sources its own data and builds it.
[01:14:28.000 --> 01:14:30.000]   You know what's incredible about this model?
[01:14:30.000 --> 01:14:33.000]   The total size of the software package
[01:14:33.000 --> 01:14:35.000]   that runs the model is about 100 gigabytes.
[01:14:35.000 --> 01:14:36.000]   Isn't that amazing?
[01:14:36.000 --> 01:14:39.000]   Like you could fit this model on probably what,
[01:14:39.000 --> 01:14:42.000]   20% of the storage space on your iPhone,
[01:14:42.000 --> 01:14:43.000]   and you could run this thing,
[01:14:43.000 --> 01:14:44.000]   and you could probably just talk to it
[01:14:44.000 --> 01:14:46.000]   for the rest of your life.
[01:14:46.000 --> 01:14:49.000]   And it's really kind of an incredible milestone.
[01:14:49.000 --> 01:14:52.000]   But I think what was so stunning to me about this,
[01:14:52.000 --> 01:14:54.000]   I know you guys are probably expecting something
[01:14:54.000 --> 01:14:55.000]   to be said like this,
[01:14:55.000 --> 01:15:02.000]   but you could see so many human knowledge worker roles
[01:15:02.000 --> 01:15:06.000]   and functions being replaced by this extraordinary interface.
[01:15:06.000 --> 01:15:09.000]   So kids can do homework, that's easy.
[01:15:09.000 --> 01:15:11.000]   Software engineers can get their code optimized
[01:15:11.000 --> 01:15:13.000]   and can get their code written for them.
[01:15:13.000 --> 01:15:15.000]   There's great examples of how software code
[01:15:15.000 --> 01:15:17.000]   has been written by this interface.
[01:15:17.000 --> 01:15:21.000]   You could see real estate insurance salespeople
[01:15:21.000 --> 01:15:24.000]   being replaced by some sort of software-like interface.
[01:15:24.000 --> 01:15:25.000]   Copywriters.
[01:15:25.000 --> 01:15:29.000]   Copywriters, you know, make me 100 versions
[01:15:29.000 --> 01:15:32.000]   of a commercial or an ad that I can then use.
[01:15:32.000 --> 01:15:33.000]   Customer support.
[01:15:33.000 --> 01:15:34.000]   Customer support completely replaced.
[01:15:34.000 --> 01:15:35.000]   If you guys remember,
[01:15:35.000 --> 01:15:37.000]   there were these automated customer support companies
[01:15:37.000 --> 01:15:40.000]   that started two decades ago.
[01:15:40.000 --> 01:15:41.000]   Never worked.
[01:15:41.000 --> 01:15:42.000]   And there was this great flurry.
[01:15:42.000 --> 01:15:44.000]   All BPO businesses were all about lower cost human labor.
[01:15:44.000 --> 01:15:46.000]   Now the cost of human labor goes to zero.
[01:15:46.000 --> 01:15:48.000]   My prediction, which is,
[01:15:48.000 --> 01:15:50.000]   so everyone's got the obvious prediction,
[01:15:50.000 --> 01:15:52.000]   which is there's going to be 100,000 startups
[01:15:52.000 --> 01:15:53.000]   that are going to emerge.
[01:15:53.000 --> 01:15:55.000]   I mean, this is kind of like this moment
[01:15:55.000 --> 01:15:56.000]   where the internet came along
[01:15:56.000 --> 01:15:58.000]   and everyone's like, this changes everything.
[01:15:58.000 --> 01:16:00.000]   I do think everyone thinks and feels that.
[01:16:00.000 --> 01:16:03.000]   So the obvious next step is a bubble will form.
[01:16:03.000 --> 01:16:06.000]   - Can I ask a technical question though, Freeberg?
[01:16:06.000 --> 01:16:08.000]   And then Chamath, you're probably thinking the same thing.
[01:16:08.000 --> 01:16:10.000]   - Let me just finish my market prediction
[01:16:10.000 --> 01:16:11.000]   and then we'll do the,
[01:16:11.000 --> 01:16:13.000]   but I think because everyone's so hyped about this
[01:16:13.000 --> 01:16:14.000]   and we all know this,
[01:16:14.000 --> 01:16:16.000]   all the VC attention, all the investor attention
[01:16:16.000 --> 01:16:18.000]   is shifting to this capability.
[01:16:18.000 --> 01:16:20.000]   And how do you apply this sort of capability
[01:16:20.000 --> 01:16:22.000]   across all of these different industries
[01:16:22.000 --> 01:16:24.000]   and all these different applications?
[01:16:24.000 --> 01:16:26.000]   And as a result, my guess is the next hype cycle,
[01:16:26.000 --> 01:16:28.000]   the next bubble cycle in Silicon Valley
[01:16:28.000 --> 01:16:31.000]   will absolutely be this generative AI business.
[01:16:31.000 --> 01:16:33.000]   - Okay, this is a little technical,
[01:16:33.000 --> 01:16:35.000]   but how would it know the difference
[01:16:35.000 --> 01:16:39.000]   between like, Y-O-U-R and U-R
[01:16:39.000 --> 01:16:42.000]   when it is processing natural language
[01:16:42.000 --> 01:16:45.000]   if you were to do like your anus or your anus?
[01:16:45.000 --> 01:16:47.000]   How would that, Freeberg,
[01:16:47.000 --> 01:16:48.000]   how would it know the difference
[01:16:48.000 --> 01:16:51.000]   between your anus and your space anus?
[01:16:51.000 --> 01:16:53.000]   - It'll learn that, you know.
[01:16:53.000 --> 01:16:55.000]   - It's a joke, it didn't land.
[01:16:55.000 --> 01:16:57.000]   It was a joke about your anus, it didn't land.
[01:16:57.000 --> 01:16:59.000]   - Try again, let's try again.
[01:16:59.000 --> 01:17:01.000]   - The AI probably would have made a better joke than that.
[01:17:01.000 --> 01:17:02.000]   - It would have made a better joke, for sure.
[01:17:02.000 --> 01:17:04.000]   - So, somebody did it in our group chat and said,
[01:17:04.000 --> 01:17:06.000]   "Do intros like J-Cal," and they were terrible.
[01:17:06.000 --> 01:17:08.000]   So at least they have a job for another year.
[01:17:08.000 --> 01:17:11.000]   - Avoyasi AI to pretend you're the all-in-pod besties
[01:17:11.000 --> 01:17:13.000]   telling Uranus jokes.
[01:17:13.000 --> 01:17:14.000]   - That would be pretty hilarious.
[01:17:14.000 --> 01:17:15.000]   - What would it come up with?
[01:17:15.000 --> 01:17:16.000]   - Sorry, let me just say one more thing
[01:17:16.000 --> 01:17:17.000]   about this OpenAI thing.
[01:17:17.000 --> 01:17:20.000]   I do think that the biggest and most interesting
[01:17:20.000 --> 01:17:24.000]   thing to think about is how this will
[01:17:24.000 --> 01:17:26.000]   disrupt the search box.
[01:17:26.000 --> 01:17:29.000]   The search, you know, the way search works at Google,
[01:17:29.000 --> 01:17:31.000]   you know, and the internet search,
[01:17:31.000 --> 01:17:33.000]   is there are these kind of servers,
[01:17:33.000 --> 01:17:35.000]   these web crawlers that go out and gather data.
[01:17:35.000 --> 01:17:37.000]   Some are structured data feeds,
[01:17:37.000 --> 01:17:39.000]   and some of them are just crawlers.
[01:17:39.000 --> 01:17:42.000]   And then that data is indexed, or in the structured way,
[01:17:42.000 --> 01:17:44.000]   it's kind of made available for serving
[01:17:44.000 --> 01:17:47.000]   directly on the search page.
[01:17:47.000 --> 01:17:50.000]   And so much of that is indexing.
[01:17:50.000 --> 01:17:52.000]   So I search for a bunch of keywords, those keywords,
[01:17:52.000 --> 01:17:54.000]   and perhaps there's some natural language context
[01:17:54.000 --> 01:17:57.000]   or match to a result page, and I click on that,
[01:17:57.000 --> 01:17:58.000]   and it's linked out.
[01:17:58.000 --> 01:18:00.000]   Years ago, Google started a product called the OneBox,
[01:18:00.000 --> 01:18:02.000]   where they could take structured data,
[01:18:02.000 --> 01:18:05.000]   like what is the weather in San Francisco today,
[01:18:05.000 --> 01:18:07.000]   and that top of the search result page
[01:18:07.000 --> 01:18:08.000]   just presented that data,
[01:18:08.000 --> 01:18:10.000]   'cause it knows with high certainty
[01:18:10.000 --> 01:18:11.000]   the question you're asking,
[01:18:11.000 --> 01:18:13.000]   and it knows with high certainty the answer it can give you.
[01:18:13.000 --> 01:18:15.000]   - Yeah, clipped it from somebody's website, right.
[01:18:15.000 --> 01:18:18.000]   - So if that starts to become everything,
[01:18:18.000 --> 01:18:20.000]   then that OneBox interface,
[01:18:20.000 --> 01:18:22.000]   and it's not just Google's ability
[01:18:22.000 --> 01:18:24.000]   to access all this data and index it
[01:18:24.000 --> 01:18:26.000]   and serve it and store it,
[01:18:26.000 --> 01:18:29.000]   there could be a lot of competitors to the OneBox
[01:18:29.000 --> 01:18:32.000]   and a lot of competitors ultimately to search.
[01:18:32.000 --> 01:18:34.000]   And ultimately, Google's core product,
[01:18:34.000 --> 01:18:36.000]   their search engine,
[01:18:36.000 --> 01:18:39.000]   could be radically disrupted
[01:18:39.000 --> 01:18:42.000]   by an alternative set system or set of systems
[01:18:42.000 --> 01:18:45.000]   that have more of a natural language chat interface.
[01:18:45.000 --> 01:18:50.000]   - Which is literally why Google bought DeepMind,
[01:18:50.000 --> 01:18:51.000]   and there were a collection
[01:18:51.000 --> 01:18:52.000]   of human-powered search engines,
[01:18:52.000 --> 01:18:55.000]   Mahalo included, ChaCha, Answers.com,
[01:18:55.000 --> 01:18:57.000]   who were trying to do the human-based version of this.
[01:18:57.000 --> 01:18:58.000]   It just didn't scale.
[01:18:58.000 --> 01:18:59.000]   - We don't want to get ahead of ourselves,
[01:18:59.000 --> 01:19:01.000]   because one of the things we don't know
[01:19:01.000 --> 01:19:03.000]   is how much is going on in DeepMind.
[01:19:03.000 --> 01:19:05.000]   They're not very open like OpenAI is.
[01:19:05.000 --> 01:19:07.000]   They talk about some of the advanced frontier stuff
[01:19:07.000 --> 01:19:09.000]   like AlphaFold and so on,
[01:19:09.000 --> 01:19:10.000]   and they've been public about that,
[01:19:10.000 --> 01:19:12.000]   but a lot of that is really to generate interest
[01:19:12.000 --> 01:19:14.000]   and hype in what's next.
[01:19:14.000 --> 01:19:16.000]   But my understanding is DeepMind's been applied
[01:19:16.000 --> 01:19:17.000]   to everything from--
[01:19:17.000 --> 01:19:18.000]   - Ads.
[01:19:18.000 --> 01:19:20.000]   - Ad optimization,
[01:19:20.000 --> 01:19:22.000]   but also the ranking on YouTube videos
[01:19:22.000 --> 01:19:24.000]   to get people more engagement on YouTube,
[01:19:24.000 --> 01:19:25.000]   et cetera, et cetera.
[01:19:25.000 --> 01:19:27.000]   So there's all these ways that DeepMind's been applied
[01:19:27.000 --> 01:19:28.000]   within Google services--
[01:19:28.000 --> 01:19:29.000]   - That we don't see.
[01:19:29.000 --> 01:19:31.000]   - And certainly within search.
[01:19:31.000 --> 01:19:32.000]   But the question is,
[01:19:32.000 --> 01:19:35.000]   is there an entirely new interface for search
[01:19:35.000 --> 01:19:38.000]   that risks Google's core search business?
[01:19:38.000 --> 01:19:40.000]   And I think that there certainly will be
[01:19:40.000 --> 01:19:41.000]   a lot of money thrown at this,
[01:19:41.000 --> 01:19:43.000]   and if anyone has any interesting ideas,
[01:19:43.000 --> 01:19:44.000]   send me an email.
[01:19:44.000 --> 01:19:45.000]   - Saks and then Shamath.
[01:19:45.000 --> 01:19:47.000]   - Yeah, I think that's a really interesting point.
[01:19:47.000 --> 01:19:49.000]   I saw a thread on this
[01:19:49.000 --> 01:19:54.000]   where somebody was asking GPT
[01:19:54.000 --> 01:19:56.000]   a bunch of questions.
[01:19:56.000 --> 01:19:58.000]   They were generally coding questions,
[01:19:58.000 --> 01:19:59.000]   and they were actually comparing
[01:19:59.000 --> 01:20:02.000]   the result in Google versus GPT,
[01:20:02.000 --> 01:20:04.000]   and Google would just give you a reference
[01:20:04.000 --> 01:20:06.000]   to a link to some page,
[01:20:06.000 --> 01:20:09.000]   whereas GPT-3 would actually construct the answer,
[01:20:09.000 --> 01:20:11.000]   like a multi-paragraph answer
[01:20:11.000 --> 01:20:13.000]   that was far more detailed
[01:20:13.000 --> 01:20:15.000]   and in a way user-friendly.
[01:20:15.000 --> 01:20:16.000]   - Yeah.
[01:20:16.000 --> 01:20:17.000]   - Whereas the Google page
[01:20:17.000 --> 01:20:19.000]   would kick you over to a reference
[01:20:19.000 --> 01:20:21.000]   where it was like this one, two, three,
[01:20:21.000 --> 01:20:23.000]   sort of maybe someone had created a checklist,
[01:20:23.000 --> 01:20:24.000]   but it just wasn't that detailed.
[01:20:24.000 --> 01:20:26.000]   It really is pretty interesting.
[01:20:26.000 --> 01:20:29.000]   I thought Andresen tweeted
[01:20:29.000 --> 01:20:32.000]   a really interesting example as well
[01:20:32.000 --> 01:20:38.000]   where he asked GPT to create a scene from a play
[01:20:38.000 --> 01:20:40.000]   starring a New York Times journalist
[01:20:40.000 --> 01:20:41.000]   and a Silicon Valley tech entrepreneur.
[01:20:41.000 --> 01:20:43.000]   They were arguing about free speech,
[01:20:43.000 --> 01:20:45.000]   and each person asserts the view
[01:20:45.000 --> 01:20:48.000]   associated with his profession and social circle.
[01:20:48.000 --> 01:20:49.000]   We don't need to read the whole thing,
[01:20:49.000 --> 01:20:51.000]   but I thought this was spot on
[01:20:51.000 --> 01:20:53.000]   where I was actually like,
[01:20:53.000 --> 01:20:55.000]   both sides are making their best arguments,
[01:20:55.000 --> 01:20:57.000]   and it's like to each other
[01:20:57.000 --> 01:20:59.000]   in a conversation that seems intelligible.
[01:20:59.000 --> 01:21:01.000]   Like they're making their points
[01:21:01.000 --> 01:21:04.000]   at the right time in the conversation.
[01:21:04.000 --> 01:21:06.000]   It's like they're playing off each other.
[01:21:06.000 --> 01:21:07.000]   In other words,
[01:21:07.000 --> 01:21:08.000]   it actually reads like a conversation.
[01:21:08.000 --> 01:21:10.000]   I actually thought this one was more impressive
[01:21:10.000 --> 01:21:13.000]   than the one with the bestie impersonation
[01:21:13.000 --> 01:21:14.000]   because I actually thought that
[01:21:14.000 --> 01:21:15.000]   the one about all in
[01:21:15.000 --> 01:21:19.000]   didn't really capture our personalities per se,
[01:21:19.000 --> 01:21:21.000]   but this one actually does a pretty good job
[01:21:21.000 --> 01:21:24.000]   capturing the arguments in this debate.
[01:21:24.000 --> 01:21:25.000]   - Chamath?
[01:21:25.000 --> 01:21:26.000]   - Pretty impressive.
[01:21:26.000 --> 01:21:27.000]   - Any thoughts here?
[01:21:27.000 --> 01:21:28.000]   - Yeah, lots.
[01:21:28.000 --> 01:21:31.000]   I've spent a lot of time learning about this area.
[01:21:31.000 --> 01:21:32.000]   Six years ago,
[01:21:32.000 --> 01:21:35.000]   a team that I partnered with
[01:21:35.000 --> 01:21:37.000]   who was at Google that built TPU,
[01:21:37.000 --> 01:21:39.000]   we've been building silicon for this space,
[01:21:39.000 --> 01:21:40.000]   so we've been kind of going from the ground up
[01:21:40.000 --> 01:21:42.000]   for the last six years.
[01:21:42.000 --> 01:21:43.000]   A couple things that I'll say.
[01:21:43.000 --> 01:21:46.000]   The first is that I think
[01:21:46.000 --> 01:21:48.000]   we're going to replace SAS
[01:21:48.000 --> 01:21:50.000]   with what I call MAS,
[01:21:50.000 --> 01:21:52.000]   which is Models as a Service.
[01:21:52.000 --> 01:21:53.000]   And so, you know,
[01:21:53.000 --> 01:21:55.000]   a lot of what software will be,
[01:21:55.000 --> 01:21:56.000]   particularly in the enterprise,
[01:21:56.000 --> 01:21:59.000]   will get replaced with a single-use model
[01:21:59.000 --> 01:22:01.000]   that allows you to solve a function.
[01:22:01.000 --> 01:22:03.000]   So these chat examples are one,
[01:22:03.000 --> 01:22:05.000]   and you can name a bunch of SAS companies
[01:22:05.000 --> 01:22:07.000]   that were purveyors of SAS
[01:22:07.000 --> 01:22:10.000]   that'll get replaced by essentially GPT-3
[01:22:10.000 --> 01:22:12.000]   or some other language model.
[01:22:12.000 --> 01:22:14.000]   And then there'll be a whole bunch
[01:22:14.000 --> 01:22:15.000]   of other things like that.
[01:22:15.000 --> 01:22:16.000]   If it's a, you know,
[01:22:16.000 --> 01:22:18.000]   expense management company,
[01:22:18.000 --> 01:22:19.000]   they'll have a model
[01:22:19.000 --> 01:22:21.000]   that'll allow them to actually do
[01:22:21.000 --> 01:22:23.000]   expense management or blah, blah, blah,
[01:22:23.000 --> 01:22:25.000]   forecasting better.
[01:22:25.000 --> 01:22:28.000]   So I think SAS will get replaced over time
[01:22:28.000 --> 01:22:30.000]   with these models incrementally.
[01:22:30.000 --> 01:22:31.000]   That's phase one.
[01:22:31.000 --> 01:22:33.000]   But the problem with all of these models,
[01:22:33.000 --> 01:22:34.000]   in my opinion,
[01:22:34.000 --> 01:22:36.000]   is that they're still largely brittle.
[01:22:36.000 --> 01:22:39.000]   They are good at one thing.
[01:22:39.000 --> 01:22:42.000]   They are a single-mode way
[01:22:42.000 --> 01:22:44.000]   of interfacing with data.
[01:22:44.000 --> 01:22:46.000]   The next big leap,
[01:22:46.000 --> 01:22:47.000]   and I think it will come from
[01:22:47.000 --> 01:22:48.000]   one of the big tech companies
[01:22:48.000 --> 01:22:50.000]   or from OpenAI,
[01:22:50.000 --> 01:22:51.000]   is, and we talked about this,
[01:22:51.000 --> 01:22:53.000]   I talked about this a few episodes ago,
[01:22:53.000 --> 01:22:55.000]   is a multimodal model,
[01:22:55.000 --> 01:22:57.000]   which then allows you to actually
[01:22:57.000 --> 01:22:59.000]   bring together and join
[01:22:59.000 --> 01:23:02.000]   video voice data in a unique way
[01:23:02.000 --> 01:23:05.000]   to answer real substantive problems.
[01:23:05.000 --> 01:23:07.000]   So if I had to steel man
[01:23:07.000 --> 01:23:08.000]   the opposite side reaction,
[01:23:08.000 --> 01:23:10.000]   so I think there's a lot of people gushing
[01:23:10.000 --> 01:23:12.000]   over the novelty of GPT-3,
[01:23:12.000 --> 01:23:14.000]   if I had to, or chat GPT,
[01:23:14.000 --> 01:23:16.000]   if I had to steel man the opposite,
[01:23:16.000 --> 01:23:17.000]   what I would say is
[01:23:17.000 --> 01:23:19.000]   it's going to get somewhere
[01:23:19.000 --> 01:23:21.000]   between 95 to 99%
[01:23:21.000 --> 01:23:24.000]   of all of these very simple questions right
[01:23:24.000 --> 01:23:26.000]   because they're kind of cute and simple.
[01:23:26.000 --> 01:23:28.000]   There is no consequence of saying
[01:23:28.000 --> 01:23:30.000]   "Write a play" because there is no wrong answer.
[01:23:30.000 --> 01:23:32.000]   Right? You either kind of,
[01:23:32.000 --> 01:23:34.000]   it tickles your fancy or it doesn't,
[01:23:34.000 --> 01:23:37.000]   it kind of entertains you or it doesn't.
[01:23:37.000 --> 01:23:39.000]   When this stuff becomes very valuable
[01:23:39.000 --> 01:23:42.000]   is that when you really need a precise answer
[01:23:42.000 --> 01:23:44.000]   and you can guarantee that
[01:23:44.000 --> 01:23:45.000]   to be overwhelmingly right,
[01:23:45.000 --> 01:23:47.000]   that's the last 1% to 2%
[01:23:47.000 --> 01:23:49.000]   that is exceptionally hard.
[01:23:49.000 --> 01:23:51.000]   And I don't think that we're at a place yet
[01:23:51.000 --> 01:23:53.000]   where these models can do that.
[01:23:53.000 --> 01:23:55.000]   But when we get there,
[01:23:55.000 --> 01:23:57.000]   all of these models as a service
[01:23:57.000 --> 01:24:00.000]   will be very much commoditized.
[01:24:00.000 --> 01:24:03.000]   And I think the real value is finding
[01:24:03.000 --> 01:24:06.000]   non-obvious sources of data
[01:24:06.000 --> 01:24:08.000]   that feed it.
[01:24:08.000 --> 01:24:09.000]   So it's all about training.
[01:24:09.000 --> 01:24:11.000]   So meaning you can break down
[01:24:11.000 --> 01:24:13.000]   machine learning and AI into two simple things.
[01:24:13.000 --> 01:24:14.000]   There's training,
[01:24:14.000 --> 01:24:16.000]   which is what you do asynchronously,
[01:24:16.000 --> 01:24:17.000]   and then there's inference,
[01:24:17.000 --> 01:24:19.000]   which is what you're doing in real time.
[01:24:19.000 --> 01:24:21.000]   So when you're typing something into chat API
[01:24:21.000 --> 01:24:23.000]   or a chat GPT,
[01:24:23.000 --> 01:24:25.000]   that's an inference that's running
[01:24:25.000 --> 01:24:26.000]   and then you're generating an output.
[01:24:26.000 --> 01:24:28.000]   But the real key is where do you find
[01:24:28.000 --> 01:24:31.000]   proprietary sources of data
[01:24:31.000 --> 01:24:33.000]   that you can learn on top of?
[01:24:33.000 --> 01:24:35.000]   That's the real arms race.
[01:24:35.000 --> 01:24:37.000]   So one example would be,
[01:24:37.000 --> 01:24:39.000]   let's say you build a model
[01:24:39.000 --> 01:24:41.000]   to detect tumors.
[01:24:41.000 --> 01:24:43.000]   Right? There's a lot of people doing that.
[01:24:43.000 --> 01:24:45.000]   Well, the company that will win
[01:24:45.000 --> 01:24:47.000]   may be the company that actually
[01:24:47.000 --> 01:24:49.000]   then vertically integrates,
[01:24:49.000 --> 01:24:51.000]   buys a hospital system,
[01:24:51.000 --> 01:24:53.000]   and get access to patient data
[01:24:53.000 --> 01:24:55.000]   that is completely proprietary to them
[01:24:55.000 --> 01:24:57.000]   and covers the most number of women
[01:24:57.000 --> 01:24:59.000]   of all age groups and of all ethnic
[01:24:59.000 --> 01:25:01.000]   categories.
[01:25:01.000 --> 01:25:03.000]   Those are the kinds of moves
[01:25:03.000 --> 01:25:05.000]   in business that we will see in the next
[01:25:05.000 --> 01:25:07.000]   five to ten years that I find much more
[01:25:07.000 --> 01:25:09.000]   exciting and trying to figure out
[01:25:09.000 --> 01:25:11.000]   how to play in that space.
[01:25:11.000 --> 01:25:13.000]   But I do think that chat GPT is a wonderful
[01:25:13.000 --> 01:25:15.000]   example to point
[01:25:15.000 --> 01:25:17.000]   us in that direction. But I'm sort
[01:25:17.000 --> 01:25:19.000]   of more of that case, which is
[01:25:19.000 --> 01:25:21.000]   it's a cute toy, but we
[01:25:21.000 --> 01:25:23.000]   haven't yet cracked the 1 to 2% of
[01:25:23.000 --> 01:25:25.000]   use cases that makes it super useful.
[01:25:25.000 --> 01:25:27.000]   But I think the first step,
[01:25:27.000 --> 01:25:29.000]   but the first step will be the transformation
[01:25:29.000 --> 01:25:31.000]   of SAS to MAS.
[01:25:31.000 --> 01:25:33.000]   And then from there, we think we can try to figure
[01:25:33.000 --> 01:25:35.000]   this out. It reminds me of in a way when you
[01:25:35.000 --> 01:25:37.000]   give that description of like, "Hey, this is really
[01:25:37.000 --> 01:25:39.000]   interesting, but it's not complete."
[01:25:39.000 --> 01:25:41.000]   Is remember when GPS came out and
[01:25:41.000 --> 01:25:43.000]   people were doing turn-by-turn
[01:25:43.000 --> 01:25:45.000]   navigation, they'd drive off the road because they were
[01:25:45.000 --> 01:25:47.000]   trusting it too much. And then
[01:25:47.000 --> 01:25:49.000]   over 20 years of GPS, we're
[01:25:49.000 --> 01:25:51.000]   kind of like, "Yeah, it's pretty bulletproof, but keep your eyes on
[01:25:51.000 --> 01:25:53.000]   the road." Same thing that's happening.
[01:25:53.000 --> 01:25:55.000]   And these changes tend to be slow.
[01:25:55.000 --> 01:25:57.000]   You said it right. These last 100 or 200 basis
[01:25:57.000 --> 01:25:59.000]   points literally takes decades.
[01:25:59.000 --> 01:26:01.000]   Exactly. So the last 15% of self-driving
[01:26:01.000 --> 01:26:03.000]   is like the decade-long
[01:26:03.000 --> 01:26:05.000]   slog. That may take a century. 15% may
[01:26:05.000 --> 01:26:07.000]   take a century. But the last 2% will take a few
[01:26:07.000 --> 01:26:09.000]   decades. It's like the change happens very slowly and then
[01:26:09.000 --> 01:26:11.000]   it all happens at once. For people who don't know
[01:26:11.000 --> 01:26:13.000]   what a TPU is, that's a
[01:26:13.000 --> 01:26:15.000]   Tensor Processing Unit. This is Google's
[01:26:15.000 --> 01:26:17.000]   application-specific
[01:26:17.000 --> 01:26:19.000]   circuits, right? Custom silicon
[01:26:19.000 --> 01:26:21.000]   that they invented for TensorFlow at the time.
[01:26:21.000 --> 01:26:23.000]   Yeah, so if you want to try to... Although now the modality
[01:26:23.000 --> 01:26:25.000]   of AI, we've changed that as well. So now we're totally
[01:26:25.000 --> 01:26:27.000]   in the world of transformers. So we're not even using...
[01:26:27.000 --> 01:26:31.000]   You're not letting the tensors flow the way they used to.
[01:26:31.000 --> 01:26:33.000]   All right, there's been a slowdown in SAS.
[01:26:33.000 --> 01:26:35.000]   Sax, what is happening
[01:26:35.000 --> 01:26:37.000]   in the software-as-a-service world?
[01:26:37.000 --> 01:26:39.000]   This is a good update by Jamin
[01:26:39.000 --> 01:26:41.000]   Ball, who works for Altimeter, our friend Brad Gerstner.
[01:26:41.000 --> 01:26:43.000]   He does these really great updates on
[01:26:43.000 --> 01:26:45.000]   what's happening in the SAS world. The big thing
[01:26:45.000 --> 01:26:47.000]   this week is that Salesforce had
[01:26:47.000 --> 01:26:49.000]   its quarter, and I would consider Salesforce
[01:26:49.000 --> 01:26:51.000]   to be the bellwether for the whole
[01:26:51.000 --> 01:26:53.000]   SAS category. I think they're the largest
[01:26:53.000 --> 01:26:55.000]   pure SAS company. They were
[01:26:55.000 --> 01:26:57.000]   the first multi-tenant SAS
[01:26:57.000 --> 01:26:59.000]   company at scale.
[01:26:59.000 --> 01:27:01.000]   What they've shown is
[01:27:01.000 --> 01:27:03.000]   a huge slowdown. Basically,
[01:27:03.000 --> 01:27:05.000]   their net new ARR
[01:27:05.000 --> 01:27:07.000]   that they just added in the previous quarter
[01:27:07.000 --> 01:27:09.000]   dropped two-thirds compared
[01:27:09.000 --> 01:27:11.000]   to the previous quarter.
[01:27:11.000 --> 01:27:13.000]   But because their sales and marketing spend
[01:27:13.000 --> 01:27:15.000]   was the same as the previous quarter,
[01:27:15.000 --> 01:27:17.000]   it exploded their
[01:27:17.000 --> 01:27:19.000]   CAC payback, which means the amount of time
[01:27:19.000 --> 01:27:21.000]   it takes to pay back your
[01:27:21.000 --> 01:27:23.000]   customer acquisition costs for a given
[01:27:23.000 --> 01:27:25.000]   customer. So you see there, 155
[01:27:25.000 --> 01:27:27.000]   months it would
[01:27:27.000 --> 01:27:29.000]   take now to pay back
[01:27:29.000 --> 01:27:31.000]   the customer acquisition costs.
[01:27:31.000 --> 01:27:33.000]   That's over 10 years. That doesn't work.
[01:27:33.000 --> 01:27:35.000]   I think before this quarter
[01:27:35.000 --> 01:27:37.000]   it was more like
[01:27:37.000 --> 01:27:39.000]   two and a half years. That's something that you can
[01:27:39.000 --> 01:27:41.000]   afford. A company can't
[01:27:41.000 --> 01:27:43.000]   if you're spending 10 years of
[01:27:43.000 --> 01:27:45.000]   gross profit on a customer
[01:27:45.000 --> 01:27:47.000]   to acquire them. The business doesn't make sense.
[01:27:47.000 --> 01:27:49.000]   So now, I'm not saying any of this
[01:27:49.000 --> 01:27:51.000]   to pick on Salesforce. It's an exceptionally
[01:27:51.000 --> 01:27:53.000]   run company.
[01:27:53.000 --> 01:27:55.000]   One of the absolute best.
[01:27:55.000 --> 01:27:57.000]   Marc Benioff, fantastic
[01:27:57.000 --> 01:27:59.000]   CEO, founder, great human
[01:27:59.000 --> 01:28:01.000]   being. But I think the point here is that
[01:28:01.000 --> 01:28:03.000]   what you're seeing is
[01:28:03.000 --> 01:28:05.000]   the whole SaaS industry
[01:28:05.000 --> 01:28:07.000]   is really slowing down here.
[01:28:07.000 --> 01:28:09.000]   In the first half of the year, you saw
[01:28:09.000 --> 01:28:11.000]   SaaS valuations correct.
[01:28:11.000 --> 01:28:13.000]   Now we're actually seeing SaaS
[01:28:13.000 --> 01:28:15.000]   top line correct.
[01:28:15.000 --> 01:28:17.000]   There's an interesting question
[01:28:17.000 --> 01:28:19.000]   here. If your CAC payback goes from
[01:28:19.000 --> 01:28:21.000]   two and a half years to 10 years, you have to
[01:28:21.000 --> 01:28:23.000]   bring your CAC down. How do you do that?
[01:28:23.000 --> 01:28:25.000]   You can either reduce marketing
[01:28:25.000 --> 01:28:27.000]   or you can reduce sales.
[01:28:27.000 --> 01:28:29.000]   So in other words, you can reduce... You mean cut the sales
[01:28:29.000 --> 01:28:31.000]   team? You can either cut
[01:28:31.000 --> 01:28:33.000]   people and headcount from your own team
[01:28:33.000 --> 01:28:35.000]   or you can cut spending you do
[01:28:35.000 --> 01:28:37.000]   on advertising or
[01:28:37.000 --> 01:28:39.000]   events or money that you spend
[01:28:39.000 --> 01:28:41.000]   on other companies. Either way,
[01:28:41.000 --> 01:28:43.000]   there's going to be a big contraction
[01:28:43.000 --> 01:28:45.000]   in jobs, basically,
[01:28:45.000 --> 01:28:47.000]   around this industry. And I think
[01:28:47.000 --> 01:28:49.000]   that what that could do is cause
[01:28:49.000 --> 01:28:51.000]   a vicious cycle where
[01:28:51.000 --> 01:28:53.000]   we start seeing... Death spiral?
[01:28:53.000 --> 01:28:55.000]   I wouldn't say death spiral. I think this vicious cycle
[01:28:55.000 --> 01:28:57.000]   for the next year or so where
[01:28:57.000 --> 01:28:59.000]   seat contraction becomes the
[01:28:59.000 --> 01:29:01.000]   norm instead of seat expansion.
[01:29:01.000 --> 01:29:03.000]   So if you go back over the last 10
[01:29:03.000 --> 01:29:05.000]   years, a major tailwind
[01:29:05.000 --> 01:29:07.000]   at the backs of SaaS startups has
[01:29:07.000 --> 01:29:09.000]   been that every year
[01:29:09.000 --> 01:29:11.000]   you start with 120%, 130%,
[01:29:11.000 --> 01:29:13.000]   150% of last year's
[01:29:13.000 --> 01:29:15.000]   revenue just from your existing customers.
[01:29:15.000 --> 01:29:17.000]   Why? Because they were hiring more
[01:29:17.000 --> 01:29:19.000]   and more people and they needed to buy more and more
[01:29:19.000 --> 01:29:21.000]   seats. But now headcount growth
[01:29:21.000 --> 01:29:23.000]   is frozen and in fact, companies
[01:29:23.000 --> 01:29:25.000]   are doing major layoffs. So the
[01:29:25.000 --> 01:29:27.000]   baseline for next year could be
[01:29:27.000 --> 01:29:29.000]   seat contraction. So instead of starting
[01:29:29.000 --> 01:29:31.000]   with 120% of last year's revenue,
[01:29:31.000 --> 01:29:33.000]   you might start with 80% or 90%
[01:29:33.000 --> 01:29:35.000]   because there's going to be so much churn.
[01:29:35.000 --> 01:29:37.000]   So I think that SaaS companies need
[01:29:37.000 --> 01:29:39.000]   to take this into account. This idea
[01:29:39.000 --> 01:29:41.000]   that growth is on autopilot,
[01:29:41.000 --> 01:29:43.000]   that could start to go in reverse. I don't
[01:29:43.000 --> 01:29:45.000]   think permanently, but I think for the next year
[01:29:45.000 --> 01:29:47.000]   or so. This is why I also tweeted
[01:29:47.000 --> 01:29:49.000]   2X is the new 3X. If you can
[01:29:49.000 --> 01:29:51.000]   grow 2X year over year in this environment,
[01:29:51.000 --> 01:29:53.000]   that is as good as
[01:29:53.000 --> 01:29:55.000]   better than growing 3X last year.
[01:29:55.000 --> 01:29:57.000]   Clearly, it's better. Like a
[01:29:57.000 --> 01:29:59.000]   lot of companies that weren't that great could
[01:29:59.000 --> 01:30:01.000]   grow 3X last year because it was
[01:30:01.000 --> 01:30:03.000]   so times are so frothy. Everyone was
[01:30:03.000 --> 01:30:05.000]   buying everything. But now it is going to be
[01:30:05.000 --> 01:30:07.000]   really hard to even double year over
[01:30:07.000 --> 01:30:09.000]   year. Companies need to take that into account
[01:30:09.000 --> 01:30:11.000]   into their financial planning.
[01:30:11.000 --> 01:30:13.000]   You need to restrain your burn because
[01:30:13.000 --> 01:30:15.000]   a lot of the revenue that you predict is going to be there
[01:30:15.000 --> 01:30:17.000]   may not be there.
[01:30:17.000 --> 01:30:19.000]   All right. Thanks so much
[01:30:19.000 --> 01:30:21.000]   to the Secretary of SaaS.
[01:30:21.000 --> 01:30:23.000]   I think you got a board meeting. I do. I got to run.
[01:30:23.000 --> 01:30:25.000]   One of the interesting things I saw in terms of use cases
[01:30:25.000 --> 01:30:27.000]   is somebody used the
[01:30:27.000 --> 01:30:29.000]   chat GPT to describe
[01:30:29.000 --> 01:30:31.000]   rooms. Then they took the descriptions
[01:30:31.000 --> 01:30:33.000]   of those rooms and then they put them into like
[01:30:33.000 --> 01:30:35.000]   Dolly or Stable Diffusion, one of those
[01:30:35.000 --> 01:30:37.000]   and it created the visual. I'm curious if
[01:30:37.000 --> 01:30:39.000]   you think the self-driving
[01:30:39.000 --> 01:30:41.000]   APIs and machine
[01:30:41.000 --> 01:30:43.000]   learning that's going on.
[01:30:43.000 --> 01:30:45.000]   Then you got images, then you got chat.
[01:30:45.000 --> 01:30:47.000]   Maybe you have proteins going on with
[01:30:47.000 --> 01:30:49.000]   the AlphaFold stuff. When
[01:30:49.000 --> 01:30:51.000]   these things start talking to each other,
[01:30:51.000 --> 01:30:53.000]   is that going to be the emergent
[01:30:53.000 --> 01:30:55.000]   behavior that we see
[01:30:55.000 --> 01:30:57.000]   of general AI and that's how we'll interpret
[01:30:57.000 --> 01:30:59.000]   it in our world is these
[01:30:59.000 --> 01:31:01.000]   100 different vertical
[01:31:01.000 --> 01:31:03.000]   AIs
[01:31:03.000 --> 01:31:05.000]   hitting some level of reasonableness
[01:31:05.000 --> 01:31:07.000]   to Chamath's point on data sets
[01:31:07.000 --> 01:31:09.000]   and then all of a sudden the self-driving AI
[01:31:09.000 --> 01:31:11.000]   is talking to the one that's looking at
[01:31:11.000 --> 01:31:13.000]   cancer and tumor diagnosis in the chat
[01:31:13.000 --> 01:31:15.000]   and the image ones and maybe Stable Diffusion,
[01:31:15.000 --> 01:31:17.000]   the protein AI
[01:31:17.000 --> 01:31:19.000]   and the one that's looking at cancer
[01:31:19.000 --> 01:31:21.000]   cells start talking to each other. Yeah,
[01:31:21.000 --> 01:31:23.000]   I'm not sure that's as
[01:31:23.000 --> 01:31:25.000]   likely as
[01:31:25.000 --> 01:31:27.000]   the... there's a lot of
[01:31:27.000 --> 01:31:29.000]   solutions that will emerge
[01:31:29.000 --> 01:31:31.000]   within verticals
[01:31:31.000 --> 01:31:33.000]   and I think you can distinguish them.
[01:31:33.000 --> 01:31:35.000]   So I kind of gave this example a few
[01:31:35.000 --> 01:31:37.000]   months ago.
[01:31:37.000 --> 01:31:39.000]   If you remember Kai's Power Tools
[01:31:39.000 --> 01:31:41.000]   was a plugin for Adobe Photoshop
[01:31:41.000 --> 01:31:43.000]   came out in 1993, I
[01:31:43.000 --> 01:31:45.000]   believe. Of course. And Kai's Power Tools
[01:31:45.000 --> 01:31:47.000]   completely transformed the
[01:31:47.000 --> 01:31:49.000]   potential of Adobe Photoshop. Because
[01:31:49.000 --> 01:31:51.000]   Photoshop had all the basic brushing and
[01:31:51.000 --> 01:31:53.000]   editing capabilities within it.
[01:31:53.000 --> 01:31:55.000]   Kai's Power Tools was statistical models
[01:31:55.000 --> 01:31:57.000]   that basically took the
[01:31:57.000 --> 01:31:59.000]   matrix of the pixels
[01:31:59.000 --> 01:32:01.000]   and created some
[01:32:01.000 --> 01:32:03.000]   evolution of them into some visual
[01:32:03.000 --> 01:32:05.000]   output like a blur. And so you
[01:32:05.000 --> 01:32:07.000]   could blur, motion blur something and you could change
[01:32:07.000 --> 01:32:09.000]   the parameters and now your photo
[01:32:09.000 --> 01:32:11.000]   looked like it was going through a motion blur.
[01:32:11.000 --> 01:32:13.000]   Ultimately, Photoshop bought and
[01:32:13.000 --> 01:32:15.000]   implemented those tools. But those
[01:32:15.000 --> 01:32:17.000]   were similar. They were statistical models
[01:32:17.000 --> 01:32:19.000]   that made some representation of the input
[01:32:19.000 --> 01:32:21.000]   which was the image and then created an
[01:32:21.000 --> 01:32:23.000]   output which was an adjusted image.
[01:32:23.000 --> 01:32:25.000]   I would argue that that is very similar
[01:32:25.000 --> 01:32:27.000]   although the models behind
[01:32:27.000 --> 01:32:29.000]   it are very different in
[01:32:29.000 --> 01:32:31.000]   terms of the contextual application
[01:32:31.000 --> 01:32:33.000]   of statistical
[01:32:33.000 --> 01:32:35.000]   models in software. And
[01:32:35.000 --> 01:32:37.000]   you could see stuff like, for example,
[01:32:37.000 --> 01:32:39.000]   a chatbot that replaces
[01:32:39.000 --> 01:32:41.000]   "Help me figure out
[01:32:41.000 --> 01:32:43.000]   whether my credit card charges are correct or not."
[01:32:43.000 --> 01:32:45.000]   Instead of having a customer service
[01:32:45.000 --> 01:32:47.000]   agent, an offshore customer service
[01:32:47.000 --> 01:32:49.000]   agent helping you resolve that.
[01:32:49.000 --> 01:32:51.000]   Or "Help me return my item."
[01:32:51.000 --> 01:32:53.000]   Or there are very specific
[01:32:53.000 --> 01:32:55.000]   kind of verticalized applications
[01:32:55.000 --> 01:32:57.000]   that can plug in
[01:32:57.000 --> 01:32:59.000]   that ultimately replace what was
[01:32:59.000 --> 01:33:01.000]   manual and human driven before.
[01:33:01.000 --> 01:33:03.000]   Because humans used to manually make the motion
[01:33:03.000 --> 01:33:05.000]   blur in Photoshop and then it was automated
[01:33:05.000 --> 01:33:07.000]   with these software packages. And I think you
[01:33:07.000 --> 01:33:09.000]   can kind of think about it in that same way that these
[01:33:09.000 --> 01:33:11.000]   are known knowns. They don't require
[01:33:11.000 --> 01:33:13.000]   necessarily human
[01:33:13.000 --> 01:33:15.000]   physical labor or some
[01:33:15.000 --> 01:33:17.000]   human responsiveness. That if 95%
[01:33:17.000 --> 01:33:19.000]   of the work can be handled, it will
[01:33:19.000 --> 01:33:21.000]   get handled by some verticalized
[01:33:21.000 --> 01:33:23.000]   solution. So I think the
[01:33:23.000 --> 01:33:25.000]   physical labor versus the non-physical labor is
[01:33:25.000 --> 01:33:27.000]   one way to think about the distinction. Meaning is there some
[01:33:27.000 --> 01:33:29.000]   change in the physical world?
[01:33:29.000 --> 01:33:31.000]   Driving is absolutely a change in the
[01:33:31.000 --> 01:33:33.000]   physical world. You have to move physically through
[01:33:33.000 --> 01:33:35.000]   space. So that one is a very
[01:33:35.000 --> 01:33:37.000]   distinct class. All the stuff that's like
[01:33:37.000 --> 01:33:39.000]   communication,
[01:33:39.000 --> 01:33:41.000]   imagery, static imagery, audio,
[01:33:41.000 --> 01:33:43.000]   and then visual, video,
[01:33:43.000 --> 01:33:45.000]   there's some stacking that
[01:33:45.000 --> 01:33:47.000]   happens there. And some of those will be kind
[01:33:47.000 --> 01:33:49.000]   of siloed and then some of them will merge
[01:33:49.000 --> 01:33:51.000]   and you'll have these kind of unique kind of combo models.
[01:33:51.000 --> 01:33:53.000]   And so look, as
[01:33:53.000 --> 01:33:55.000]   they start to work together, I think we'll see them
[01:33:55.000 --> 01:33:57.000]   completely rewrite
[01:33:57.000 --> 01:33:59.000]   some of these verticals like movie
[01:33:59.000 --> 01:34:01.000]   production or music production,
[01:34:01.000 --> 01:34:03.000]   right, or advertising,
[01:34:03.000 --> 01:34:05.000]   or we're seeing it now with
[01:34:05.000 --> 01:34:07.000]   with video and creative
[01:34:07.000 --> 01:34:09.000]   arts with
[01:34:09.000 --> 01:34:11.000]   some of the visual stuff on
[01:34:11.000 --> 01:34:13.000]   OpenAI. And to be honest, a lot of journalism, a lot
[01:34:13.000 --> 01:34:15.000]   of creative arts have become the wisdom of the
[01:34:15.000 --> 01:34:17.000]   crowds over the last two decades where
[01:34:17.000 --> 01:34:19.000]   artists were looking
[01:34:19.000 --> 01:34:21.000]   at the collective works of
[01:34:21.000 --> 01:34:23.000]   the internet, interpreting it,
[01:34:23.000 --> 01:34:25.000]   and then coming up with content, which is
[01:34:25.000 --> 01:34:27.000]   kind of what these AIs are doing. And
[01:34:27.000 --> 01:34:29.000]   then who legally owns
[01:34:29.000 --> 01:34:31.000]   the collective content is
[01:34:31.000 --> 01:34:33.000]   going to be a big question, Chamath. You talked about
[01:34:33.000 --> 01:34:35.000]   datasets, and Microsoft is being
[01:34:35.000 --> 01:34:37.000]   sued right now, and GitHub, because
[01:34:37.000 --> 01:34:39.000]   they used open source
[01:34:39.000 --> 01:34:41.000]   to create tools in AI
[01:34:41.000 --> 01:34:43.000]   to help augment
[01:34:43.000 --> 01:34:45.000]   programmers, like while they're programming, writing code,
[01:34:45.000 --> 01:34:47.000]   it gives them suggestions, and now the open source
[01:34:47.000 --> 01:34:49.000]   community is suing them for using their
[01:34:49.000 --> 01:34:51.000]   datasets. So what do you think about the legality
[01:34:51.000 --> 01:34:53.000]   of datasets, Chamath, and should
[01:34:53.000 --> 01:34:55.000]   they get some kind of protection
[01:34:55.000 --> 01:34:57.000]   if you make a GPT-3
[01:34:57.000 --> 01:34:59.000]   based on Quora or based on Wikipedia?
[01:34:59.000 --> 01:35:01.000]   Should you have to get approval
[01:35:01.000 --> 01:35:03.000]   to use that data?
[01:35:03.000 --> 01:35:05.000]   I think it's the exact opposite.
[01:35:05.000 --> 01:35:07.000]   It's the exact opposite. They say that
[01:35:07.000 --> 01:35:09.000]   this is actually your work.
[01:35:09.000 --> 01:35:11.000]   And I think that that's the right legal
[01:35:11.000 --> 01:35:13.000]   framework. But the answer to your other question
[01:35:13.000 --> 01:35:15.000]   is, this is why I think the hunt
[01:35:15.000 --> 01:35:17.000]   for proprietary data
[01:35:17.000 --> 01:35:19.000]   actually becomes the hunt that matters.
[01:35:19.000 --> 01:35:21.000]   All of this other stuff, I think,
[01:35:21.000 --> 01:35:23.000]   is a lot less important because I think you have
[01:35:23.000 --> 01:35:25.000]   to assume that all of these models will
[01:35:25.000 --> 01:35:27.000]   eventually just get commoditized.
[01:35:27.000 --> 01:35:29.000]   So there'll be a, you know, like you see
[01:35:29.000 --> 01:35:31.000]   like Jasper AI, and you see a bunch of these
[01:35:31.000 --> 01:35:33.000]   generative AI companies. It's really interesting.
[01:35:33.000 --> 01:35:35.000]   But the problem is, when you sit it
[01:35:35.000 --> 01:35:37.000]   on top of the same substrate, you'll have a
[01:35:37.000 --> 01:35:39.000]   convergence. Right?
[01:35:39.000 --> 01:35:41.000]   Everybody's chat model will eventually look and sound
[01:35:41.000 --> 01:35:43.000]   and feel like the same thing, unless
[01:35:43.000 --> 01:35:45.000]   you're giving it a few
[01:35:45.000 --> 01:35:47.000]   special ingredients that other
[01:35:47.000 --> 01:35:49.000]   people are not. And so it's the
[01:35:49.000 --> 01:35:51.000]   hunt for those ingredients that will make
[01:35:51.000 --> 01:35:53.000]   this next generation of
[01:35:53.000 --> 01:35:55.000]   models really valuable.
[01:35:55.000 --> 01:35:57.000]   So to give an example, you'd have Wikipedia,
[01:35:57.000 --> 01:35:59.000]   which is creative comments anybody can
[01:35:59.000 --> 01:36:01.000]   use, but Quora as a data set, not
[01:36:01.000 --> 01:36:03.000]   everybody can use that's owned by a company.
[01:36:03.000 --> 01:36:05.000]   So Quora would have an advantage. Take an
[01:36:05.000 --> 01:36:07.000]   extreme example. If Quora
[01:36:07.000 --> 01:36:09.000]   didn't allow themselves
[01:36:09.000 --> 01:36:11.000]   to be crawled,
[01:36:11.000 --> 01:36:13.000]   okay, which they don't.
[01:36:13.000 --> 01:36:15.000]   But then they
[01:36:15.000 --> 01:36:17.000]   developed their own language model, which used
[01:36:17.000 --> 01:36:19.000]   the best of the internet, so
[01:36:19.000 --> 01:36:21.000]   call it, you know, GPT
[01:36:21.000 --> 01:36:23.000]   and Quora,
[01:36:23.000 --> 01:36:25.000]   maybe they are slightly
[01:36:25.000 --> 01:36:27.000]   better in certain domains than others.
[01:36:27.000 --> 01:36:29.000]   The other extreme example is the one that I
[01:36:29.000 --> 01:36:31.000]   used in healthcare, which is, you know,
[01:36:31.000 --> 01:36:33.000]   if you have access to patient data
[01:36:33.000 --> 01:36:35.000]   that you will not license to anybody
[01:36:35.000 --> 01:36:37.000]   else, you know, it stands to
[01:36:37.000 --> 01:36:39.000]   reason that that model
[01:36:39.000 --> 01:36:41.000]   actually then has much
[01:36:41.000 --> 01:36:43.000]   better chances of highly
[01:36:43.000 --> 01:36:45.000]   effective clinical outcomes versus any
[01:36:45.000 --> 01:36:47.000]   other model. Apple Watch comes to mind,
[01:36:47.000 --> 01:36:49.000]   right? Apple has all that watch
[01:36:49.000 --> 01:36:51.000]   data. If they could pair that with epics,
[01:36:51.000 --> 01:36:53.000]   another data set.
[01:36:53.000 --> 01:36:55.000]   What could they do together?
[01:36:55.000 --> 01:36:57.000]   So this is going to be like, this is the new oil
[01:36:57.000 --> 01:36:59.000]   is going to be data. And by the way,
[01:36:59.000 --> 01:37:01.000]   to talk about Apple
[01:37:01.000 --> 01:37:03.000]   for a second, the smart thing is they've gotten
[01:37:03.000 --> 01:37:05.000]   so methodically, they've never
[01:37:05.000 --> 01:37:07.000]   touted the AI. You know, they introduce
[01:37:07.000 --> 01:37:09.000]   one or two distinguishing
[01:37:09.000 --> 01:37:11.000]   features every year, right?
[01:37:11.000 --> 01:37:13.000]   So like the ECG,
[01:37:13.000 --> 01:37:15.000]   which was introduced many, many years ago,
[01:37:15.000 --> 01:37:17.000]   has only gotten
[01:37:17.000 --> 01:37:19.000]   slightly more usable like five or six years
[01:37:19.000 --> 01:37:21.000]   later, but in the meantime, there's, you
[01:37:21.000 --> 01:37:23.000]   know, 10s of millions of watches
[01:37:23.000 --> 01:37:25.000]   collecting this kind of data. So to your
[01:37:25.000 --> 01:37:27.000]   point, it's it's using
[01:37:27.000 --> 01:37:29.000]   these devices as Trojan horses to
[01:37:29.000 --> 01:37:31.000]   collect training data. That is the
[01:37:31.000 --> 01:37:33.000]   oil. Uber and
[01:37:33.000 --> 01:37:35.000]   Tesla have all this data
[01:37:35.000 --> 01:37:37.000]   of the data being collected by,
[01:37:37.000 --> 01:37:39.000]   you know, the
[01:37:39.000 --> 01:37:41.000]   phones or the cameras in the
[01:37:41.000 --> 01:37:43.000]   cars. The other difference though, is that
[01:37:43.000 --> 01:37:45.000]   you have to be in a realm
[01:37:45.000 --> 01:37:47.000]   where you don't need regulators
[01:37:47.000 --> 01:37:49.000]   to go the last mile. So the
[01:37:49.000 --> 01:37:51.000]   problem with ADAS, I think, or
[01:37:51.000 --> 01:37:53.000]   Level 5 autonomy, is that
[01:37:53.000 --> 01:37:55.000]   eventually you get to a point where even if
[01:37:55.000 --> 01:37:57.000]   the model becomes quote unquote
[01:37:57.000 --> 01:37:59.000]   "perfect,"
[01:37:59.000 --> 01:38:01.000]   you still need regulatory approval. And
[01:38:01.000 --> 01:38:03.000]   what I'm saying is I think you have to focus on
[01:38:03.000 --> 01:38:05.000]   areas of the economy
[01:38:05.000 --> 01:38:07.000]   that are not subject to that or
[01:38:07.000 --> 01:38:09.000]   where the regulatory pathway is already defined.
[01:38:09.000 --> 01:38:11.000]   So for example, if you use that healthcare
[01:38:11.000 --> 01:38:13.000]   example, let's say that you had the largest corpus of
[01:38:13.000 --> 01:38:15.000]   breast cancer image data and you could actually build an
[01:38:15.000 --> 01:38:17.000]   AI that was a much better classifier
[01:38:17.000 --> 01:38:19.000]   of tumors versus other things,
[01:38:19.000 --> 01:38:21.000]   the FDA
[01:38:21.000 --> 01:38:23.000]   actually has a pathway to get that approved
[01:38:23.000 --> 01:38:25.000]   very quickly. The problem with, you know,
[01:38:25.000 --> 01:38:27.000]   Level 5 autonomy is that there is no clear
[01:38:27.000 --> 01:38:29.000]   pathway. It's not, again, we go back to almost
[01:38:29.000 --> 01:38:31.000]   a crypto example.
[01:38:31.000 --> 01:38:33.000]   We don't really know who will govern that decision
[01:38:33.000 --> 01:38:35.000]   and we don't know how that will be governed.
[01:38:35.000 --> 01:38:37.000]   So I think the
[01:38:37.000 --> 01:38:39.000]   thing that investors have to do
[01:38:39.000 --> 01:38:41.000]   and entrepreneurs, entrepreneurs have to pick their end
[01:38:41.000 --> 01:38:43.000]   market very carefully, and investors
[01:38:43.000 --> 01:38:45.000]   have to realize that this dynamic exists
[01:38:45.000 --> 01:38:47.000]   as well. If you're going to do this right, make money.
[01:38:47.000 --> 01:38:49.000]   Imagine the Robin Hood trading, you know,
[01:38:49.000 --> 01:38:51.000]   trader data set, watching
[01:38:51.000 --> 01:38:53.000]   people sell in shares and then predicting
[01:38:53.000 --> 01:38:55.000]   markets with it, with AI, I mean, it could be crazy.
[01:38:55.000 --> 01:38:57.000]   Well, you have that payment for order flow that's used
[01:38:57.000 --> 01:38:59.000]   by Citadel and the other big... But not AI.
[01:38:59.000 --> 01:39:01.000]   Right. Or who knows? Maybe they are using
[01:39:01.000 --> 01:39:03.000]   AI on their side. I don't know if they are. I can
[01:39:03.000 --> 01:39:05.000]   tell you as somebody who sells,
[01:39:05.000 --> 01:39:07.000]   we sell a lot
[01:39:07.000 --> 01:39:09.000]   of machine learning hardware into this market.
[01:39:09.000 --> 01:39:11.000]   The biggest buyers are
[01:39:11.000 --> 01:39:13.000]   the US government and
[01:39:13.000 --> 01:39:15.000]   these ultra high frequency trading organizations.
[01:39:15.000 --> 01:39:17.000]   Freeberg, any final thoughts here? I'll give you the final word.
[01:39:17.000 --> 01:39:19.000]   How could this affect
[01:39:19.000 --> 01:39:21.000]   astronomy? How could this affect
[01:39:21.000 --> 01:39:23.000]   our search of the galaxies,
[01:39:23.000 --> 01:39:25.000]   you know, going out past Pluto,
[01:39:25.000 --> 01:39:27.000]   Saturn, breaching Uranus,
[01:39:27.000 --> 01:39:29.000]   any of those things, how could it impact?
[01:39:29.000 --> 01:39:33.000]   Any...
[01:39:33.000 --> 01:39:35.000]   I'm trying to get a Uranus joke
[01:39:35.000 --> 01:39:37.000]   to land. Help me out there, Tramont.
[01:39:37.000 --> 01:39:39.000]   You got me. It's not that... I think
[01:39:39.000 --> 01:39:41.000]   you need to have more
[01:39:41.000 --> 01:39:43.000]   space related...
[01:39:43.000 --> 01:39:45.000]   Yeah, workshop this one with me.
[01:39:45.000 --> 01:39:47.000]   Or gut biome related,
[01:39:47.000 --> 01:39:49.000]   you know... Gut biome, yeah. So how would this affect
[01:39:49.000 --> 01:39:51.000]   super gut? Use the promo code TWIST.
[01:39:51.000 --> 01:39:53.000]   You have to trick Freeberg into thinking
[01:39:53.000 --> 01:39:55.000]   we're asking a serious question. Get him down
[01:39:55.000 --> 01:39:57.000]   the science path and then rug pull him.
[01:39:57.000 --> 01:39:59.000]   Now that's the right use of rug pull. That's a good proper rug pull.
[01:39:59.000 --> 01:40:01.000]   Exactly. Okay, let's do it. Here we go. Let's workshop. That's the rug pull.
[01:40:01.000 --> 01:40:03.000]   So tell us, you know, when you're doing like
[01:40:03.000 --> 01:40:05.000]   super gut, use promo code TWIST to get 25% off.
[01:40:05.000 --> 01:40:07.000]   When you're doing super gut,
[01:40:07.000 --> 01:40:09.000]   you're analyzing people's guts.
[01:40:09.000 --> 01:40:11.000]   How would you then
[01:40:11.000 --> 01:40:13.000]   have machine learning in this, you know,
[01:40:13.000 --> 01:40:15.000]   API, this chat API
[01:40:15.000 --> 01:40:17.000]   in GPT-3? How could that help
[01:40:17.000 --> 01:40:19.000]   with processing all of that,
[01:40:19.000 --> 01:40:21.000]   especially when it passes through Uranus?
[01:40:21.000 --> 01:40:25.000]   Freeberg. You okay up there, Freeberg?
[01:40:25.000 --> 01:40:27.000]   You are
[01:40:27.000 --> 01:40:29.000]   hung over. I'm hung over, but I
[01:40:29.000 --> 01:40:31.000]   also had like a 7am board
[01:40:31.000 --> 01:40:33.000]   meeting. So I'm also just a little
[01:40:33.000 --> 01:40:35.000]   beat up. Were you grumpy on the board meeting?
[01:40:35.000 --> 01:40:37.000]   Did you get a little cantankerous with your... No, no, it was fine.
[01:40:37.000 --> 01:40:39.000]   You were fine? You kept the rage in control?
[01:40:39.000 --> 01:40:41.000]   I think I had my caffeine
[01:40:41.000 --> 01:40:43.000]   fuel and then I kind of
[01:40:43.000 --> 01:40:45.000]   cranked down afterwards.
[01:40:45.000 --> 01:40:47.000]   Alright, everybody. We will see you next time
[01:40:47.000 --> 01:40:49.000]   for the Secretary of
[01:40:49.000 --> 01:40:51.000]   SAS, the Dictator,
[01:40:51.000 --> 01:40:53.000]   and the Sultan of Hungover.
[01:40:53.000 --> 01:40:55.000]   We will see you
[01:40:55.000 --> 01:40:57.000]   next time. Bye-bye.
[01:40:57.000 --> 01:40:59.000]   Love you guys. Bye-bye.
[01:40:59.000 --> 01:41:01.000]   [outro music]
[01:41:01.000 --> 01:41:03.000]   [outro music]
[01:41:03.000 --> 01:41:05.000]   [outro music]
[01:41:05.000 --> 01:41:07.340]    I'm going all in 
[01:41:07.340 --> 01:41:08.140]   And they said,
[01:41:08.140 --> 01:41:09.780]   "We open source it to the fans,
[01:41:09.780 --> 01:41:11.240]   and they've just gone crazy with it."
[01:41:11.240 --> 01:41:12.380]   Love you West, baby!
[01:41:12.380 --> 01:41:13.480]   I'm queen of Kinwa!
[01:41:13.480 --> 01:41:14.940]   I'm going all in 
[01:41:14.940 --> 01:41:16.120]    What your winners like? 
[01:41:16.120 --> 01:41:18.180]    What-What are your winners like? 
[01:41:18.180 --> 01:41:20.380]    What your winners like? 
[01:41:20.380 --> 01:41:21.480]   Besties are gone!
[01:41:21.480 --> 01:41:22.820]   Go, 13!
[01:41:22.820 --> 01:41:25.020]   That is my dog taking a noise in your driveway.
[01:41:25.020 --> 01:41:26.520]   No, no, no, no, no!
[01:41:26.520 --> 01:41:28.060]   [Laughing]
[01:41:28.060 --> 01:41:29.000]   Oh, man!
[01:41:29.000 --> 01:41:30.820]   My avatars will meet me at "Lakeligt".
[01:41:30.820 --> 01:41:32.000]   We should all just get a room
[01:41:32.000 --> 01:41:33.440]   and just have one big huge orgy,
[01:41:33.440 --> 01:41:34.860]   cause they're all just useless.
[01:41:34.860 --> 01:41:36.200]   It's like this sexual tension
[01:41:36.200 --> 01:41:37.740]   that they just need to release somehow.
[01:41:37.740 --> 01:41:38.800]   
[01:41:38.800 --> 01:41:40.540]    What your beat? 
[01:41:40.540 --> 01:41:42.440]    What your beat? 
[01:41:42.440 --> 01:41:44.040]    What your beat? 
[01:41:44.040 --> 01:41:44.940]   We need to get merch!
[01:41:44.940 --> 01:41:45.780]   Besties are back!
[01:41:45.780 --> 01:41:47.440]    I'm going all in 
[01:41:47.440 --> 01:41:53.480]   
[01:41:53.480 --> 01:41:55.480]    I'm going all in 
[01:41:55.480 --> 01:41:57.540]   you

