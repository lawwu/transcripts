
[00:00:00.000 --> 00:00:06.080]   California had the most desirable object in the history of the world, and rather than adopting
[00:00:06.080 --> 00:00:11.040]   any sensible strategy for getting it into people's arms, was bickering over who should get it first.
[00:00:11.040 --> 00:00:15.360]   We should be outraged about this, and we're mostly not. If you googled vaccine near me,
[00:00:15.360 --> 00:00:18.880]   before a certain day there was no answer, after that day there was an answer, that answer came
[00:00:18.880 --> 00:00:25.920]   from us. The successful project plan was made by a bunch of rank amateurs at this topic on discord
[00:00:25.920 --> 00:00:32.560]   in the course of a couple of hours. Society should not rely on us as plan A. How did this happen?
[00:00:32.560 --> 00:00:38.240]   It is enormously to the United States credit that we have an extremely functional,
[00:00:38.240 --> 00:00:42.720]   extremely capable tech industry. Maybe we shouldn't treat it like the enemy.
[00:00:42.720 --> 00:00:49.280]   Today I'm chatting with Patrick McKenzie. He is known for many things. On the internet he's known
[00:00:49.280 --> 00:00:56.000]   as Patio11. Most recently he ran VaccinateCA, which probably saved on the order of high
[00:00:56.000 --> 00:01:00.000]   four-figure number of lives during COVID. He also writes an excellent newsletter called
[00:01:00.000 --> 00:01:03.440]   Bits About Money. Patrick, welcome to the podcast. Thanks very much for having me.
[00:01:03.440 --> 00:01:09.440]   So what was VaccinateCA? In early 2021, we were quite concerned that people were making 20, 40,
[00:01:09.440 --> 00:01:14.320]   60 phone calls to try to find a pharmacy that actually had a dose of the COVID vaccine in stock
[00:01:14.320 --> 00:01:20.480]   and could successfully deliver it to them. I tweeted out randomly, "It's insane that every
[00:01:20.480 --> 00:01:24.560]   person or every caregiver is attempting to contact every medical provider in the state of California
[00:01:24.560 --> 00:01:28.880]   to find doses of the vaccine. California clearly has at least one person capable of building a
[00:01:28.880 --> 00:01:32.880]   website where we can centralize that information and send everybody to the website. If you build
[00:01:32.880 --> 00:01:37.440]   that website, I'll pay for the server bill or whatever." Carl Yang took up the gauntlet and
[00:01:37.440 --> 00:01:41.360]   invited 10 of his best friends and said basically, "All right, get in, guys. We're going to open
[00:01:41.360 --> 00:01:45.280]   source the availability of the vaccine in California by tomorrow morning." This is at
[00:01:45.280 --> 00:01:50.320]   10 p.m. at night California time. So I lurked on into the Discord where, of course, all medical
[00:01:50.320 --> 00:01:55.920]   infrastructure is built and gave a few pointers on making scaled calling operations. Then one
[00:01:55.920 --> 00:02:01.280]   thing led to another and I ended up becoming the CEO of this initiative. At the start, it was just
[00:02:01.280 --> 00:02:05.040]   this hackathon project of a bunch of random tech people who thought, "Hey, we can build a website,
[00:02:05.040 --> 00:02:10.320]   make some phone calls, maybe help some people find the vaccine at the margin." It grew a little bit
[00:02:10.320 --> 00:02:14.960]   from there. We ended up becoming essentially the public-private partnership, which was the
[00:02:14.960 --> 00:02:19.200]   clearinghouse for vaccine location information for the United States of America. That felt a
[00:02:19.200 --> 00:02:25.760]   little weird at the time and continues to. Okay, so the obvious question is, why was this something
[00:02:25.760 --> 00:02:31.440]   that people randomly picked up on a Discord server? Why wasn't this an initiative either
[00:02:31.440 --> 00:02:38.160]   by an entity delegated by the government or by the White House or the pharmacies have a website
[00:02:38.160 --> 00:02:42.800]   you're going to sign up for an appointment? Oh, there are so many reasons and a whole lot of
[00:02:42.800 --> 00:02:47.920]   finger pointing going on. One of the things was that there were almost no actors anywhere in the
[00:02:47.920 --> 00:02:54.480]   system who said, "Yes, this is definitely my responsibility." Various parts of our nation's
[00:02:54.480 --> 00:03:00.400]   institutions, county-level public health departments, governor's offices, the presidency,
[00:03:00.400 --> 00:03:06.320]   two presidencies over this interval, which will become relevant, they all said, "Well, I have a
[00:03:06.320 --> 00:03:10.240]   narrow part to play in this, but someone else has to do the hard yards of actually putting shots in
[00:03:10.240 --> 00:03:16.560]   people's arms." Someone else is clearly dealing with the logistics problem. The ball was just
[00:03:16.560 --> 00:03:23.200]   dropped comprehensively. No one at the time really had a plan for picking it up or didn't feel like
[00:03:23.200 --> 00:03:26.800]   it was incentive compatible for them specifically to pick it up right now. It would be great if
[00:03:26.800 --> 00:03:31.840]   someone could do this, but just not me. Okay, so to explain the context and how
[00:03:31.840 --> 00:03:36.320]   important it was that people get vaccines at the time and how much these delays mattered,
[00:03:36.320 --> 00:03:39.920]   you can account for it, obviously, in the amount of lives saved. You can even look at it in terms
[00:03:39.920 --> 00:03:44.960]   of when vaccine news was announced, how much the stock market moved. It was clear it was worth
[00:03:44.960 --> 00:03:50.000]   trillions of dollars to the economy that the vaccine be delivered on time. So it should be
[00:03:50.000 --> 00:03:57.280]   priority number one that people know where the vaccine is. A meta question is, I heard about
[00:03:57.280 --> 00:04:03.520]   this problem for the first time when I read your article about this. There was a bunch of
[00:04:03.520 --> 00:04:10.320]   controversy after COVID about people pointing fingers about masks or different kinds of
[00:04:10.320 --> 00:04:15.120]   protocols. Why was this not... People are getting called in front of Congress. Why were we not able
[00:04:15.120 --> 00:04:18.960]   to deliver the one thing that was needed to arrest the pandemic as fast as possible?
[00:04:18.960 --> 00:04:25.120]   Well, folks, why don't we... 27,000 words on this. My article in "Works in Progress"
[00:04:25.120 --> 00:04:29.040]   called "The Story of Vaccinate CA" goes into some of the nitty-gritty. Broadly, I think,
[00:04:29.040 --> 00:04:32.640]   a matter of incentive is more than a matter of people choosing to do evil things. Although,
[00:04:32.640 --> 00:04:37.040]   I will say, we did choose to do evil things, and we can probe on that if you want to.
[00:04:37.040 --> 00:04:43.040]   But if you look at the federal government specifically, the federal government
[00:04:43.040 --> 00:04:49.680]   institutionally learned one, I believe, wrong lesson, which has terrible consequences from
[00:04:49.680 --> 00:04:54.400]   the healthcare.gov rollout a number of years ago when Obamacare was first disputing. And the thing
[00:04:54.400 --> 00:04:59.360]   which many actors in the federal government and the political parties came away from is that a
[00:04:59.360 --> 00:05:05.520]   president can doom their legacy of their signature initiative if those bleeping tech folks don't get
[00:05:05.520 --> 00:05:12.800]   their bleeping act together. And so the United States has decided there is virtually nothing,
[00:05:12.800 --> 00:05:17.840]   up to and including the potential of national annihilation, that will cause us to actually
[00:05:17.840 --> 00:05:22.320]   put our chips behind making a software problem. That is somebody else who does not have to deal
[00:05:22.320 --> 00:05:26.480]   with an electoral mandate or getting called in front of Congress or etc., etc. Somebody else's
[00:05:26.480 --> 00:05:30.640]   problem. Unfortunately, software is eating the world, and delivering competence in the modern
[00:05:30.640 --> 00:05:34.880]   world requires being competent at software. And the United States, it will tell you differently.
[00:05:34.880 --> 00:05:38.880]   And there are wonderful people in the government who are attempting to change this. But
[00:05:38.880 --> 00:05:45.120]   broad strokes, on an institutional level, the United States federal government has abdicated
[00:05:45.120 --> 00:05:47.440]   software as a core responsibility of the government.
[00:05:47.440 --> 00:05:52.640]   - I understand why they didn't initially want to pursue this project. I still don't
[00:05:52.640 --> 00:05:56.480]   understand the answer to the question of, after everything went down, now,
[00:05:56.480 --> 00:06:00.560]   why is it not more of a news item that this was a problem that was not solved?
[00:06:00.560 --> 00:06:06.800]   - So I think we're memory-holing a lot of things that happened in the pandemic. And I wish we
[00:06:06.800 --> 00:06:11.280]   wouldn't, partly because of political incentives, and we're approaching an election year. And
[00:06:11.280 --> 00:06:15.920]   because of the quirky way that the American parties and the candidates bounce off each other,
[00:06:16.560 --> 00:06:22.320]   no one's real incentive to say, "Okay, I would like to re-litigate the mask issue for a moment.
[00:06:22.320 --> 00:06:27.680]   We told people that masks don't block airborne viruses. And we were quite confident of that,
[00:06:27.680 --> 00:06:31.360]   and the entire news media backed us up on it. And then we like 180'd a month later."
[00:06:31.360 --> 00:06:37.920]   No one wants to re-litigate that. No one wants to re-litigate California imposed redlining
[00:06:37.920 --> 00:06:46.080]   and the provision of medical care. And that was wrong and evil. But the party that was,
[00:06:46.080 --> 00:06:51.520]   in the case, pro-redlining, does not normally like saying that it is pro-redlining. And the
[00:06:51.520 --> 00:06:57.680]   other party does not really consider that a hugely salient issue. And so there are no debates,
[00:06:57.680 --> 00:07:02.720]   and no one is asking Governor Newsom, "So when you got on TV and said that you were doing
[00:07:02.720 --> 00:07:06.880]   geofencing for the provision of medical care, geofencing in that context was the same as
[00:07:06.880 --> 00:07:12.320]   redlining. Can you explain your support for redlining?" And no one has asked Newsom that
[00:07:12.320 --> 00:07:19.600]   question. Maybe someone should. But we're surrounded by the effects of incentives and
[00:07:19.600 --> 00:07:23.440]   the effects of iterated games. And sometimes they don't play out the way we would ideally
[00:07:23.440 --> 00:07:28.160]   like them to play out. Yeah. We can come back to this. I still don't feel like I really understand.
[00:07:28.160 --> 00:07:36.080]   So maybe the fact that it was that everybody has the blood on their hands. And so that's so
[00:07:36.080 --> 00:07:40.240]   confusing. I think if you have other kinds of emergencies, like if you lost a war, I don't
[00:07:40.240 --> 00:07:45.040]   think you'd just brush it aside. The generals would have to come up in front of Congress and
[00:07:45.040 --> 00:07:51.360]   be like, "What happened? Why didn't we get that battlefield? Actually, we just lost a war. Maybe
[00:07:51.360 --> 00:07:55.360]   that didn't happen." If one goes over the history of military conflicts, I don't know how many
[00:07:55.360 --> 00:07:59.920]   losers on either side of the conflict ever actually did that reckoning of like, "Hey,
[00:07:59.920 --> 00:08:05.520]   could we attempt to win in the future?" I think there was a broad lack of seriousness across many
[00:08:05.520 --> 00:08:10.560]   trusted institutions in American society, in the government, in civil society, in the tech industry,
[00:08:10.560 --> 00:08:17.200]   about really approaching this like a problem we want to win. And I think a wonderful thing about
[00:08:17.200 --> 00:08:22.160]   our country and our institutions is on things that are truly important to us, we win outlandishly
[00:08:22.160 --> 00:08:26.960]   because we are a rich and powerful nation. And yet, this was obviously a thing where we should
[00:08:26.960 --> 00:08:31.520]   have decided to win and we fundamentally did not approach it as a problem that we needed to win on.
[00:08:31.520 --> 00:08:37.280]   Okay. So going back to the object level here, one would think that instead of different people
[00:08:37.280 --> 00:08:43.360]   calling different pharmacies and asking whether they have the vaccine, the obvious thing that
[00:08:43.360 --> 00:08:47.680]   people who have not read your article would assume is that either there were just some
[00:08:47.680 --> 00:08:51.200]   company would build a platform like this, the government would build a platform like this,
[00:08:51.200 --> 00:08:54.400]   I guess you explained that the government didn't do it. The pharmacies might build a platform like
[00:08:54.400 --> 00:09:00.000]   this. And I want to meditate on the incentives that prevented random big tech company or
[00:09:00.000 --> 00:09:02.720]   Walgreens from building this themselves. Can you explain that?
[00:09:02.720 --> 00:09:08.480]   Sure. So the federal government and the state government, the American governmental system is
[00:09:08.480 --> 00:09:12.240]   quite complex. And there were multiple distinct supply chains with multiple distinct technological
[00:09:12.240 --> 00:09:17.920]   systems tracking where these vials were headed all over the country. And there were many attempts at
[00:09:17.920 --> 00:09:22.400]   various levels of the government to say, "Hey, can we commission a consultancy to build a magical
[00:09:22.400 --> 00:09:26.960]   IT solution, which will get these databases to talk to each other?" And those largely failed
[00:09:26.960 --> 00:09:32.160]   for the usual reasons that government software procurement projects fail. Why didn't tech build
[00:09:32.160 --> 00:09:39.120]   it? I'm constrained on what I can say and cannot say, so I know a little more than this answer,
[00:09:39.120 --> 00:09:46.720]   but I will give you part of the answer. The tech industry, both at the level of like Appamagoofasoft,
[00:09:46.720 --> 00:09:51.200]   which is my funny sardonic way to refer to some of the most powerful institutions in the world,
[00:09:51.200 --> 00:09:57.520]   and many other places that hire many number of smart engineers who can build the world's
[00:09:57.520 --> 00:10:05.040]   least impressive inventory tracking system, felt political pressure in the wake of the January 6
[00:10:05.040 --> 00:10:09.760]   events in the United States. This is another thing that's gone down the rabbit hole, but in the
[00:10:09.760 --> 00:10:15.360]   immediate wake of the January 6 events, people in positions of authority very clearly tried to lay
[00:10:15.360 --> 00:10:19.360]   that at the feet of the tech companies. And internally in the tech companies, their policy
[00:10:19.360 --> 00:10:23.920]   teams, the teams that are supposed to make the company legible to government and avoid government
[00:10:23.920 --> 00:10:29.440]   yanking its permission to do business, and their communications teams, PR departments, told everyone
[00:10:29.440 --> 00:10:35.920]   in the company, "Mission number one right now, do not get in the newspaper for any reason. We are
[00:10:35.920 --> 00:10:41.440]   putting our heads down." And when people in those companies who work on public health, and a thing
[00:10:41.440 --> 00:10:46.080]   that might not be obvious to most people in the world is that like Appamagoo Booksoft, they're
[00:10:46.080 --> 00:10:50.960]   literally like teams of people who their job is public health, because they are the operating
[00:10:50.960 --> 00:10:54.240]   system of the world right now, and the operating system of the world needs public health care.
[00:10:54.240 --> 00:11:02.400]   Those teams said, "Hey, we've got this thing," and other people in the company might have overruled
[00:11:02.400 --> 00:11:07.040]   them and said, "It would be really, really bad right now to have the tech industry saying we're
[00:11:07.040 --> 00:11:10.720]   better at the government's job than the government is, so shut that down."
[00:11:11.600 --> 00:11:13.840]   Okay, so that's so insane.
[00:11:13.840 --> 00:11:21.360]   It is absolutely. The local incentives, like it makes sense in the meeting when you're saying it,
[00:11:21.360 --> 00:11:25.760]   and you are not in that meeting projecting, "I'm going to cause tens of thousands of
[00:11:25.760 --> 00:11:30.080]   people to die at the margin by making this call," and yet that call was made.
[00:11:30.080 --> 00:11:36.320]   Right. Okay, so there's two culpable actors here. One, you could say, "Well,
[00:11:36.320 --> 00:11:40.960]   the big tech companies for not taking the political risk." I think what is even more
[00:11:40.960 --> 00:11:47.840]   reprehensible is the fact that they probably correctly thought that appearing more competent
[00:11:47.840 --> 00:11:52.720]   than the government and saving tens of thousands of lives as a result will be held against them
[00:11:52.720 --> 00:11:57.680]   in a way that significantly matters for their ability to continue their other businesses.
[00:11:57.680 --> 00:12:02.800]   Okay, so suppose that they had built the software. Let's play with the scenario.
[00:12:02.800 --> 00:12:08.640]   Then what would happen? They would get hauled in front of Congress and explain why they weren't
[00:12:08.640 --> 00:12:12.800]   delivered even faster because of the ultimate bottlenecks in the supply chain because you can
[00:12:12.800 --> 00:12:15.920]   only manage ... What would happen? They build it and it's better than the government's.
[00:12:15.920 --> 00:12:19.920]   So many things could happen. One, if you build the thing, this has sometimes been called the
[00:12:19.920 --> 00:12:26.320]   Copenhagen principle of culpability. If you build the thing, various actors in our system will
[00:12:26.320 --> 00:12:30.400]   assume, "Okay, now your responsibility for not just the consequences of the thing you built,
[00:12:30.400 --> 00:12:34.800]   but for the totality of consequences of everything associated with the American vaccination effort."
[00:12:34.800 --> 00:12:40.240]   So you built the thing. Oh, you big tech geniuses. Well, what did you do about localization? You
[00:12:40.240 --> 00:12:45.520]   didn't do enough about localization yet. You hate name group of people here. Don't you see the
[00:12:45.520 --> 00:12:49.520]   disparity in death rates between the demographic A and demographic B? Why haven't you fixed that
[00:12:49.520 --> 00:12:54.400]   yet? You have killed so many people, et cetera, et cetera. And no one in government, no one who
[00:12:54.400 --> 00:12:59.600]   is making that moral calculation says, "I have responsibility for killing people by doing
[00:12:59.600 --> 00:13:04.400]   nothing." The person who is doing anything has the responsibility for killing people by taking
[00:13:04.400 --> 00:13:09.920]   up the burden of doing something. And it is a absolutely morally defensible thing, which you
[00:13:09.920 --> 00:13:14.560]   will see over and over and over again in our discourse. They get hauled in front of Congress,
[00:13:14.560 --> 00:13:20.720]   but it's not just because they made a sin of commission. It's also because if you're right,
[00:13:20.720 --> 00:13:26.480]   then after January 6th, they would be held in... There's one answer where it's like they touched
[00:13:26.480 --> 00:13:31.280]   the problem. And there's another where it's, they did it better than the government could have.
[00:13:31.280 --> 00:13:35.520]   And those seem like two different... Right. You touched the problem. And so you've
[00:13:35.520 --> 00:13:39.680]   immediately taken liability for any number of sins of omission because even at the scale of
[00:13:39.680 --> 00:13:43.840]   the largest companies in the world, you have not allocated infinite resources to this problem.
[00:13:43.840 --> 00:13:49.440]   And also the stealing a march on the government and embarrassing us will be held against you.
[00:13:49.440 --> 00:13:54.240]   And so you can point back to the Cambridge Analytica thing where Cambridge Analytica
[00:13:54.240 --> 00:14:01.280]   is like shorthand for... There was this one time back in the day where the news media in New York
[00:14:01.280 --> 00:14:06.880]   and the government in DC and places like it talked to each other a bit and convinced themselves that
[00:14:06.880 --> 00:14:11.200]   a small team of people with a budget of approximately $200,000 have rooted the
[00:14:11.200 --> 00:14:15.840]   United States presidential election. Now, rooting the United States presidential election, perhaps
[00:14:15.840 --> 00:14:20.400]   on behalf of a foreign power would be an enormously consequential thing. Good thing that did not
[00:14:20.400 --> 00:14:25.600]   happen in the world that we live in. However, people believe very passionately in that narrative.
[00:14:25.600 --> 00:14:30.560]   And as a result of that narrative, they did very aggressively attempt to clip the wings of tech
[00:14:30.560 --> 00:14:38.000]   and tech's core businesses, like say advertising. Right. There's so much that's crazier. Okay.
[00:14:38.000 --> 00:14:44.560]   One question you might have is we figured out that we couldn't delegate to big tech or any
[00:14:44.560 --> 00:14:50.160]   of the competent actors and that the native infrastructure that we had that was specifically
[00:14:50.160 --> 00:14:53.920]   earmarked for dealing with public health emergencies was extremely incompetent to
[00:14:53.920 --> 00:14:59.760]   the extent that discord servers vastly outperformed them. Supposing that public
[00:14:59.760 --> 00:15:05.040]   health is not uniquely incompetent among the different functions that the government is
[00:15:05.040 --> 00:15:10.000]   supposed to perform that don't get tested until the actual emergency is upon hand,
[00:15:10.000 --> 00:15:15.440]   how would we go about, if the president hears this and is concerned that the people who are
[00:15:15.440 --> 00:15:23.120]   running that nuclear bomb reaction aren't up to snuff or the earthquake reaction, is there some
[00:15:23.120 --> 00:15:27.200]   stress that you test that you can perform on these institutions before the thing goes down that you
[00:15:27.200 --> 00:15:31.120]   could learn beforehand whether they're competent or not? If I look at the experience of the last
[00:15:31.120 --> 00:15:36.400]   hundred years and a little more back to the flu pandemic in 1918, if you read histories of the
[00:15:36.400 --> 00:15:42.480]   flu pandemic, a vastly less wealthy, vastly less technologically sophisticated nation with many
[00:15:42.480 --> 00:15:47.680]   less people involved in the actual fixing of this problem, competently executed on nationwide vaccine
[00:15:47.680 --> 00:15:54.960]   campaigns and other various measures. And so in some fashion, we should be urgently concerned with
[00:15:54.960 --> 00:16:00.000]   what decayed institutionally in the interim. I think one of the things that health departments
[00:16:00.000 --> 00:16:05.680]   specifically faced is that if you had just given them these kind of vanilla vaccination campaign,
[00:16:05.680 --> 00:16:10.080]   maybe they would have done better than they actually did. I'm not positive of that. I do
[00:16:10.080 --> 00:16:16.320]   think there are actually, and here I have to stop for a disclaimer, I think that people in county
[00:16:16.320 --> 00:16:19.680]   health departments did real important work. I think they probably did work that saved lives
[00:16:19.680 --> 00:16:23.840]   on the margin. I do not think that the United States should be satisfied with our performance
[00:16:23.840 --> 00:16:28.480]   in 2020 and 2021. We should be very dissatisfied and we should get better for the future.
[00:16:29.440 --> 00:16:33.280]   And that requires like recognizing that we underperformed by a lot.
[00:16:33.280 --> 00:16:41.440]   There was a political decision made that the successful administration of vaccination was not
[00:16:41.440 --> 00:16:48.400]   going to be measured solely by saving lives. The prioritization schedule that came up, which was
[00:16:48.400 --> 00:16:52.880]   Byzantine and complicated and routinely befuddled professional software engineers and health
[00:16:52.880 --> 00:16:58.320]   administrators, and which I could not diagram out on a whiteboard, even if you paid me a million
[00:16:58.320 --> 00:17:03.760]   dollars to get it right on the first time, was downstream of the United States' political
[00:17:03.760 --> 00:17:12.400]   preferences. And so schedule 1A versus 1B versus 1C was in the first five seconds of the discussion
[00:17:12.400 --> 00:17:17.600]   dictated by medical necessity. But immediately after that was about rewarding plums to politically
[00:17:17.600 --> 00:17:27.520]   favored groups. And so one of the complexities of this is that the pharmacies and healthcare
[00:17:27.520 --> 00:17:32.080]   departments are not set up to discriminate along the axes of whether one is politically powerful
[00:17:32.080 --> 00:17:35.520]   or not, because that is not a thing that they have to do in most vaccination campaigns and
[00:17:35.520 --> 00:17:39.840]   not a thing that they have to do on the typical Tuesday of providing medical services. We asked
[00:17:39.840 --> 00:17:44.960]   them to do this radically new thing, which is in part responsible for the failures that we had.
[00:17:44.960 --> 00:17:51.680]   If we had had a much simpler tiering system, for example, we would have had more than 25%
[00:17:51.680 --> 00:17:59.120]   of the shots successfully being delivered in the state of California in January of 2021.
[00:17:59.120 --> 00:18:03.680]   For people who are not aware, what was the political tiering system that you're referring to?
[00:18:03.680 --> 00:18:07.680]   Oh, goodness. So this was different in different places, and confusingly,
[00:18:07.680 --> 00:18:11.760]   different places in the United States use the same names for these tiers for different people. But
[00:18:11.760 --> 00:18:20.320]   in the state of California, tier 1A comes before 1B, comes before 1C, comes before the tier 2,
[00:18:20.320 --> 00:18:24.640]   et cetera, et cetera. So 1A was, descriptively speaking, at the start, and this changed over
[00:18:24.640 --> 00:18:28.560]   time on a day-to-day, week-to-week basis, sometimes in mutually incompatible ways at
[00:18:28.560 --> 00:18:34.000]   the same time. It was an entire mess. But descriptively, tier 1A was, okay,
[00:18:34.000 --> 00:18:39.760]   healthcare professionals and a few others, and people above the age of 75. No, wait,
[00:18:39.760 --> 00:18:46.960]   we'll change that to 65. Tier 1B was, we're going to put a few favored occupational groups here,
[00:18:46.960 --> 00:18:52.080]   and some other folks. And tier 1C was people who doctors think will probably die if they don't
[00:18:52.080 --> 00:18:59.040]   get the vaccine, if they contract COVID, but who have not appeared in group 1A or 1B yet.
[00:18:59.040 --> 00:19:04.800]   And so who got 1A? Well, healthcare professionals, so like doctors administering the vaccine. That
[00:19:04.800 --> 00:19:10.400]   sounds pretty reasonable. Also, veterinarians, because veterinarians are urgently required
[00:19:10.400 --> 00:19:14.720]   by society at the time? Not so much, because the California Veterinarians Association is good at
[00:19:14.720 --> 00:19:19.440]   lobbying. And that isn't just me alleging that. They sent a letter out to their membership saying,
[00:19:19.440 --> 00:19:23.040]   "We are so good at lobbying. We got you guys into 1A. Congrats, and go get your vaccine now."
[00:19:23.040 --> 00:19:30.080]   I have that on my website. Okay. So like tier 1B, school teachers were classified as
[00:19:30.080 --> 00:19:37.040]   tier 1B. Why? Because, go figure, teachers unions have political power in the state of California.
[00:19:37.040 --> 00:19:42.720]   And they said, "Well, we'll accept not being in 1A, but we are going lower than 1B."
[00:19:42.720 --> 00:19:47.840]   And probably no one in that meeting ever said, "I definitely think that 25-year-old teachers
[00:19:47.840 --> 00:19:52.320]   who are currently under stay-at-home orders should be in front of the line of people who
[00:19:52.320 --> 00:19:58.720]   will die if they get COVID." But we made that choice. So then in your article, you discussed
[00:19:58.720 --> 00:20:03.200]   that the consequence of that was not only the misprioritization of the vaccine, but
[00:20:03.200 --> 00:20:09.440]   the bureaucracy around allocating it according to these tiers, resulting in 75-year-olds not
[00:20:09.440 --> 00:20:14.240]   having the capacity to fill out the pages of paperwork that require you to decide what tier
[00:20:14.240 --> 00:20:19.680]   you're in. The state of New York commissioned a consultancy to administer to 75-year-olds a
[00:20:19.680 --> 00:20:25.600]   57-page web application, which required uploading multiple attachments to check for their eligibility.
[00:20:25.600 --> 00:20:30.480]   And talk with a technologist if you don't believe me. We try to remove everything from a web page
[00:20:30.480 --> 00:20:35.120]   that people successfully get through it. If you can make it two to four form fields, that's already
[00:20:35.120 --> 00:20:39.840]   taxing people's patience. And you were asking people who might be suffering from cognitive
[00:20:39.840 --> 00:20:45.280]   decline or be less comfortable in using computers to do something which would literally tax the
[00:20:45.280 --> 00:20:53.040]   patience and cognitive abilities of a professional software engineer. And that wasn't an accident.
[00:20:53.040 --> 00:20:57.360]   We wanted to do that. Why did we want to do that? Because it was extremely important to
[00:20:57.360 --> 00:21:02.880]   successfully implement the tiering system that we had agreed upon. Why was it extremely important
[00:21:02.880 --> 00:21:06.560]   to implement the tiering system? Because that was society's prioritization. Was that the correct
[00:21:06.560 --> 00:21:10.960]   prioritization? Hell no. Right. Okay. So can we just count off everything?
[00:21:10.960 --> 00:21:21.520]   So it's enraging, not only because obviously people died, but because nobody talks about it.
[00:21:21.520 --> 00:21:27.680]   There's all kinds of controversies about COVID, about whether, I don't know, a vaccination and
[00:21:29.360 --> 00:21:34.800]   side effect kind of things, and whether the masking orders were too late, too early, whatever.
[00:21:34.800 --> 00:21:39.520]   And then the main thing about whether we got vaccines in people's arms on time because of
[00:21:39.520 --> 00:21:42.720]   these political considerations. So you're not allowed... Yeah, go ahead.
[00:21:42.720 --> 00:21:47.440]   Can I jump in with one bit of optimism? We achieved something incredible, which was getting
[00:21:47.440 --> 00:21:52.240]   the first cut of the vaccine done in two days as a result of many decades of science done by
[00:21:52.240 --> 00:21:57.360]   very incredible people. And we successfully got that vaccine productionized in a year.
[00:21:57.360 --> 00:22:01.840]   We should have gotten it productionized in far less than a year, but the fact that we were able
[00:22:01.840 --> 00:22:07.760]   to do it in one year and not three was enormously consequential. And so we should feel happy about
[00:22:07.760 --> 00:22:13.200]   that. A little annoyed that we didn't have better protocols at the FDA and places to get that
[00:22:13.200 --> 00:22:18.560]   vaccine prioritized for testing much faster than was. Quite annoyed at the fact that that was a
[00:22:18.560 --> 00:22:22.720]   political football and people probably made decisions that pessimized for human lives and
[00:22:22.720 --> 00:22:26.800]   optimized for defeating a non-preferred political candidate. Are you talking about the fact that the
[00:22:26.800 --> 00:22:30.960]   vaccine was announced the day after the election results or something, right?
[00:22:30.960 --> 00:22:36.000]   Yes, I'm basically subtweeting that. And I strongly believe that was a political decision,
[00:22:36.000 --> 00:22:41.840]   but I don't. I'm just a software guy. So, okay, the particular kind of craziness
[00:22:41.840 --> 00:22:51.040]   that we had during the 2020 and 2021 about equity and wokeness, how much was that uniquely
[00:22:51.040 --> 00:22:58.640]   responsible for the dysfunctions of this tiering system and geolocation slash redlining? Basically,
[00:22:58.640 --> 00:23:02.000]   if it happened in another year where there wasn't a bunch of cultural craziness, would
[00:23:02.000 --> 00:23:05.200]   it have gone significantly better? It's difficult to ask that question
[00:23:05.200 --> 00:23:12.960]   because we were clearly in a unique time in 2020 and 2021. And yet, point to me in the year in
[00:23:12.960 --> 00:23:17.600]   American history in which American society was truly united and had no social issues going on.
[00:23:17.600 --> 00:23:22.800]   And if people counterfactually point to, say, World War II, I will say read more history there,
[00:23:22.800 --> 00:23:30.560]   but be that as it may. Was it the case that strong societal feelings in the wake of
[00:23:30.560 --> 00:23:39.440]   George Lloyd's death in 2020 and the racial reckoning strongly dictated policy? Yes.
[00:23:39.440 --> 00:23:42.960]   As a positive statement rather than a normative statement, that is absolutely the case.
[00:23:45.120 --> 00:23:47.920]   There's this thing we often say in the tech industry called bike shedding,
[00:23:47.920 --> 00:23:53.840]   which is if you're building a nuclear power plant, and many people cannot sensibly comment
[00:23:53.840 --> 00:23:57.680]   on what is the flow rate through the pipes to a cool nuclear reactor. But if you build a bike
[00:23:57.680 --> 00:24:01.360]   shed next to the nuclear power plant, it's very easy to have opinions on the color of the bike
[00:24:01.360 --> 00:24:06.480]   shed. And so in the meetings about the nuclear power plant, you will have a truly stupid amount
[00:24:06.480 --> 00:24:11.920]   of human effort devoted into what colors you should paint the bike shed. So it is very difficult
[00:24:11.920 --> 00:24:17.440]   for most people in civil society to successfully inject a vaccine into someone's arms, to successfully
[00:24:17.440 --> 00:24:22.880]   manage a logistics network, to successfully build a nationwide information gathering system,
[00:24:22.880 --> 00:24:29.120]   to centralize this information and pass it out to everyone. And we aggressively trained the entire
[00:24:29.120 --> 00:24:34.000]   American professional managerial class, starting at seventh grade or earlier,
[00:24:34.000 --> 00:24:43.200]   in decrying systemic racism, which to be clear, is a problem. And so any discussion about what
[00:24:43.200 --> 00:24:48.960]   should we do with regards to information distribution, which goes out to a broad
[00:24:48.960 --> 00:24:52.640]   audience in the American professional managerial class who essentially call all the shots in the
[00:24:52.640 --> 00:24:58.560]   U.S. system, will almost invariably get bent to, "I have no particular opinions on server
[00:24:58.560 --> 00:25:04.160]   architecture here and nothing useful to comment, but what's our equity strategy?" And the equity
[00:25:04.160 --> 00:25:12.800]   strategy dominated discussions of the correct way to run the rollout to the exclusion of
[00:25:12.800 --> 00:25:21.760]   operationalizing it via medical necessity. People brag about that fact.
[00:25:21.760 --> 00:25:26.800]   That fact is enormously frustrating to me. And if you say it with exactly those words
[00:25:26.800 --> 00:25:29.920]   and emotional valence, people will say, "No, no, that's not exactly what we meant."
[00:25:29.920 --> 00:25:34.000]   But when they're talking to other audiences, they say, "No, this is absolutely what we mean."
[00:25:34.000 --> 00:25:40.640]   Yeah. Okay. So I mean, even on that point, maybe the culprit here is a scarcity mindset
[00:25:40.640 --> 00:25:46.080]   involved here with caring more about the proportion rather than just solving the problem.
[00:25:46.080 --> 00:25:50.800]   This is one of those few times where we were genuinely up against a scarcity constraint.
[00:25:50.800 --> 00:25:56.000]   Physical reality was there were a scarce number of vials and we needed to have a prioritization
[00:25:56.000 --> 00:26:00.080]   system. And some people who urgently needed the vials were not going to get them first.
[00:26:00.080 --> 00:26:05.600]   Everyone was going to get them eventually. But the mad rush in our political system
[00:26:05.600 --> 00:26:09.680]   to dole out favors around the prioritization for those first vials
[00:26:09.680 --> 00:26:17.440]   exceeded the actual distribution and successful injection of the vials as a goal. Again,
[00:26:17.440 --> 00:26:20.640]   California reported to the federal government that it was only successfully
[00:26:20.640 --> 00:26:27.680]   injecting 25% of its allocation. It had the most desirable object in the history of the world.
[00:26:27.680 --> 00:26:31.760]   And rather than adopting any sensible strategy for getting it into people's arms,
[00:26:31.760 --> 00:26:36.880]   was bickering over who should get it first. We should be outraged about this. And we're mostly
[00:26:36.880 --> 00:26:42.160]   not. I don't even know what to ask the next because it's so obviously outrageous. And
[00:26:42.160 --> 00:26:48.720]   there's no clear answer, at least to me, of why there isn't more outrage about it.
[00:26:50.000 --> 00:26:56.800]   Also what the solution to it is, I mean, literally in the exact context of what will we do the next
[00:26:56.800 --> 00:27:00.480]   time there's a pandemic, it's not clear to me that we've learned the lesson, let alone the broader
[00:27:00.480 --> 00:27:06.560]   lesson of if there's a different kind of emergency, if there's some isomorphic emergency, what would
[00:27:06.560 --> 00:27:10.720]   our state capacity be better? And you mentioned the point about a hundred years ago, we maybe
[00:27:10.720 --> 00:27:14.320]   would be able to deal with this problem better. I don't know what changed. One of the things that
[00:27:14.320 --> 00:27:18.560]   America used to do is when the federal government lacked state capacity for something, it would say
[00:27:18.560 --> 00:27:24.160]   who in civil society or private industry has capacity for this? And then say, congratulations,
[00:27:24.160 --> 00:27:28.800]   by order of the president, you are now a Colonel in the United States Army. Like, what do we need
[00:27:28.800 --> 00:27:33.440]   to get it done, sir? And that was an option. That was an option that was not taken. But
[00:27:33.440 --> 00:27:39.920]   I will play no fights in either of the two administrations that both individually made
[00:27:39.920 --> 00:27:46.320]   terrible decisions. But plausibly some more enlightened counterfactual administration
[00:27:46.320 --> 00:27:50.000]   could have gone to Google and said, who is literally your best person for solving the
[00:27:50.000 --> 00:27:55.680]   data problem that they currently find ourselves in? Great. Will they accept a commission as Colonel?
[00:27:55.680 --> 00:28:00.320]   Great. Here's an order from the president. You have a swearing-in ceremony starting in 30 seconds.
[00:28:00.320 --> 00:28:07.280]   And you will present your project plan tomorrow. And again, the successful project plan
[00:28:07.280 --> 00:28:15.520]   was made by a bunch of rank amateurs at this topic on Discord in the course of a couple of hours.
[00:28:16.080 --> 00:28:22.960]   And similarly, this is one part of the huge overall vaccination effort. But you could imagine
[00:28:22.960 --> 00:28:27.280]   going to Amazon and saying, hey, Amazon, we hear you're pretty good about getting packages between
[00:28:27.280 --> 00:28:33.360]   A and B. This package has a really hard thing about it. It has to be cool as it's delivered.
[00:28:33.360 --> 00:28:37.520]   That's like a totally unsolved problem in material science, right? And Amazon would say,
[00:28:37.520 --> 00:28:43.360]   we literally do that every day. Back in December, people were getting on the night in the news and
[00:28:43.360 --> 00:28:48.640]   saying this is going to be an unprecedented logistics challenge because the vaccine has
[00:28:48.640 --> 00:28:55.040]   to be kept at ultra low temperatures, which are the same temperatures at which milk is transported.
[00:28:55.040 --> 00:29:00.000]   We understand how to do cold chain logistics. So Amazon would correct that misperception. They say,
[00:29:00.000 --> 00:29:04.400]   oh, you guys seem to know what you're doing. We have an absence of that here. Congratulations.
[00:29:04.400 --> 00:29:09.440]   Here's your Colonel uniform in the United States military. And now your job is we are going to give
[00:29:09.440 --> 00:29:14.320]   you a CSV file every day. Interface with this other Colonel, please, from Google on where this
[00:29:14.320 --> 00:29:18.480]   thing needs to go. And you get it there on time every time. And if you can't get it there on time
[00:29:18.480 --> 00:29:24.560]   every time, call the White House and we will find you political cover. Is what a functioning system
[00:29:24.560 --> 00:29:29.840]   would have done. Granted, the American system is dysfunctional in its own way. I think another
[00:29:29.840 --> 00:29:33.760]   thing that's been underdone in the course of the last couple of years is looking internationally.
[00:29:33.760 --> 00:29:38.400]   I don't know if any country anywhere with vastly different political systems is happy with the
[00:29:38.400 --> 00:29:44.800]   outcomes that it got. Some were obviously vastly better than others, but there are journals of
[00:29:44.800 --> 00:29:50.000]   comparative international politics. And why are those journals writing anything but who succeeded
[00:29:50.000 --> 00:29:54.320]   at what margins and who did not? And what do we learn about the proper functioning of political
[00:29:54.320 --> 00:30:00.480]   systems, civil society, and the United States considered as one hugely complex machine?
[00:30:00.480 --> 00:30:05.360]   No, that's a really interesting point. I actually asked Tony Blair Institute,
[00:30:05.360 --> 00:30:08.800]   they were recommending to the British government, different ways of distributing
[00:30:08.800 --> 00:30:13.520]   the vaccine. And they made the obvious recommendation that you should give everybody
[00:30:13.520 --> 00:30:16.960]   one dose now and then do the second dose later. You know, obvious things like this, which would
[00:30:16.960 --> 00:30:23.520]   save lives. Any British government didn't do a good job there. I think it's actually a very
[00:30:23.520 --> 00:30:26.320]   interesting question. There's governments all across the world, which have very different
[00:30:26.320 --> 00:30:30.560]   political systems. They have hopefully, I don't know, different infrastructure
[00:30:30.560 --> 00:30:37.120]   radial mix to this. Why did nobody get this right? So on the, like give people the catchphrase for
[00:30:37.120 --> 00:30:43.680]   this was first doses first. That was not the procedure in many nations, which have many
[00:30:43.680 --> 00:30:48.000]   smart people in them. And it was not the procedure in the United States until I think, you know,
[00:30:48.000 --> 00:30:52.880]   first doses first, you can like sometimes trace policy back to individual blog posts. And so to
[00:30:52.880 --> 00:30:57.520]   the extent that one can be traced, I think it was Alex Tabarrok on Marginal Revolution, who's right.
[00:30:58.240 --> 00:31:02.800]   This is very obvious and overdetermined. If we want to win at this, like first doses first is
[00:31:02.800 --> 00:31:08.320]   objectively the correct policy. And after, you know, this like ping ponged around the political
[00:31:08.320 --> 00:31:12.720]   system for a while and they, you know, talk to medical experts and et cetera, they were like,
[00:31:12.720 --> 00:31:20.480]   yeah, yeah. It turns out that, you know, this is the equivalent of like saying you should probably
[00:31:20.480 --> 00:31:25.920]   consume calories at some point in the typical week. That is better than not consuming calories.
[00:31:25.920 --> 00:31:30.880]   Well, we checked with the medical experts and it took six weeks of meeting, but they definitely
[00:31:30.880 --> 00:31:39.280]   agree eating beats not eating for living. So we're going to do that now. And on the one hand,
[00:31:39.280 --> 00:31:44.240]   like it is a genuine strength of the United States that like, you know, he's not just
[00:31:44.240 --> 00:31:49.280]   actually some rando, but relative to like the topic in question, some rando on the internet,
[00:31:49.280 --> 00:31:55.840]   like wrote up a 2000 word blog post and we stopped doing stupid things. We could have stopped doing
[00:31:55.840 --> 00:32:01.920]   the stupid thing sooner. Yeah. Oh, that doesn't answer the question of why, like nobody got it
[00:32:01.920 --> 00:32:06.320]   right. If you, if you think there's something particular to the late stage bureaucracy that
[00:32:06.320 --> 00:32:10.640]   we have or something, maybe another country is fresher, but even the countries that have more
[00:32:10.640 --> 00:32:16.240]   authoritarian models who can just, you know, crack down or something, they did abysmally as well.
[00:32:16.240 --> 00:32:21.760]   They made errors often in many cases that were worse in America. There's like so many countries,
[00:32:21.760 --> 00:32:26.800]   Patrick, why did none of them get it right? I'm under-informed on much of the international
[00:32:26.800 --> 00:32:35.360]   comparison, partly because in 2021, I was sort of busy. But I remember Israel for a variety of
[00:32:35.360 --> 00:32:41.680]   institutional reasons, having a broadly functional response on this in particular around end of the
[00:32:41.680 --> 00:32:48.240]   day shots, which end of the day shots are in the grand scheme of things, a minor issue, but they're
[00:32:50.080 --> 00:32:55.760]   a good, like quick heuristic for, do you have good epistemics on this at all? Okay. You know,
[00:32:55.760 --> 00:33:00.320]   physical reality of the COVID shots is there's a five, eight or 10 shots in a single vial.
[00:33:00.320 --> 00:33:06.880]   That single vial is, goes bad after 12 hours. That is a bit of an oversimplification. It doesn't
[00:33:06.880 --> 00:33:11.520]   actually go bad, but for, for essentially regulatory reasons, we have to pretend it
[00:33:11.520 --> 00:33:18.640]   goes bad after 12 hours. It can't be resealed. Okay. So if you vaccinate two people, then the
[00:33:18.640 --> 00:33:25.280]   other shots are on a timer and those shots will decrease in value to zero after however many
[00:33:25.280 --> 00:33:29.760]   hours are remaining on the timer and then get thrown in the trash can. So quick question to
[00:33:29.760 --> 00:33:34.560]   test if you are a rational human being at the margin, would you prefer giving a shot to the
[00:33:34.560 --> 00:33:41.280]   most preferred patient in your queue of patients who needs it for medical needs or like the trash
[00:33:41.280 --> 00:33:46.240]   can? You prefer the most preferred patient. Now, follow-up question, would you prefer giving it to
[00:33:46.240 --> 00:33:51.760]   the least preferred patient or the trash can? You still give it to the human rather than the trash
[00:33:51.760 --> 00:33:57.600]   can. And Israel adopted the policy of like, if the shots are expiring, forget the tiering system,
[00:33:57.600 --> 00:34:01.520]   forget any, forget anything else. Literally walk out into the street and say, I've got the COVID
[00:34:01.520 --> 00:34:05.760]   shot. I need to administer it in the next 15 minutes. Who wants it? And in the United States,
[00:34:05.760 --> 00:34:11.840]   we had a policy ban on doing that. We said no to protect the integrity of the tiering system to
[00:34:11.840 --> 00:34:18.880]   like, you know, embrace our glorious cause of health equity. You should throw that shot out.
[00:34:18.880 --> 00:34:28.000]   And that policy was stupid. And it was announced by governors proudly in December in front of
[00:34:28.000 --> 00:34:34.560]   news cameras. And then a couple of weeks later, reality set in and they were like, people told
[00:34:34.560 --> 00:34:42.960]   them, sir, it turns out that throwing out the vaccine is stupid. And the governor did not go
[00:34:42.960 --> 00:34:49.840]   on the nightly news again and say, I gave a, you know, a very confident policy speech a month ago
[00:34:49.840 --> 00:34:54.400]   in front of this news camera, where I said that I would prosecute anyone who gives out end of day
[00:34:54.400 --> 00:35:01.120]   shots. But he literally said that. He literally said that. Oh man, this is almost a direct quote.
[00:35:01.120 --> 00:35:04.800]   And you can see the actual direct quote in my previous writing on this, but I will not just
[00:35:04.800 --> 00:35:10.160]   prosecute people. I will go aggressively to try to maximize the reputational impact to your firms
[00:35:10.160 --> 00:35:16.800]   and your licenses. Like we were pointing metaphorical and when it came down to it,
[00:35:16.800 --> 00:35:22.080]   literal guns at physicians in the middle of a pandemic for doing unauthorized medical care.
[00:35:22.080 --> 00:35:29.440]   Crazy. But when the system corrected, it did not correct all the way. The governor did not go out
[00:35:29.440 --> 00:35:34.880]   and say, Hey, that thing I said a month ago was effing insane. I take that back and apologize.
[00:35:34.880 --> 00:35:39.840]   No, it was like, okay, we're going to like quietly pass out the word. That's no longer the policy,
[00:35:39.840 --> 00:35:44.400]   but we don't want to own up to the mistake. And people in say, like the regulatory departments
[00:35:44.400 --> 00:35:49.280]   of pharmacies make rational decisions based on the, uh, the signals that you were giving them.
[00:35:49.280 --> 00:35:54.320]   And the rational decision of pharmacy makes is not, okay, we've been quietly past the word that
[00:35:54.320 --> 00:36:01.200]   the old policy is persona and grata, but can we really trust the quiet word here? One, because
[00:36:01.200 --> 00:36:05.440]   like, do we trust that this actor is not going to change their mind in two weeks and consequence us
[00:36:05.440 --> 00:36:12.080]   for something we authorized today, just throw out the shots. And pharmacies did not cover themselves
[00:36:12.080 --> 00:36:18.720]   in glory. A lot of pharmacists did some pharmacists did, but, um, uh, pharmacies like institutionally,
[00:36:18.720 --> 00:36:23.760]   we deliver almost all the medications for almost all the diseases routinely in America. We cannot
[00:36:23.760 --> 00:36:30.400]   blow up either that like position of societal trust or our business results over one drug for
[00:36:30.400 --> 00:36:36.720]   one disease. And so throw out the shots and make sure we can still delivering medical care in
[00:36:36.720 --> 00:36:45.680]   California tomorrow. That I understand how that decision was made. We should not endorse that
[00:36:45.680 --> 00:36:51.760]   decision. We, we, there, there were individual acts of heroism by, by particular pharmacists
[00:36:51.760 --> 00:36:56.560]   who said essentially in as many words to us when we, when we called them and said, Hey, what's the
[00:36:56.560 --> 00:37:01.440]   procedure for getting the shot? Okay. An individual like the one you just described cannot formally
[00:37:01.440 --> 00:37:06.240]   get the shot right now. So I would tell that individual to go to the County website, tell
[00:37:06.240 --> 00:37:10.720]   whatever lies are necessary to get an appointment with me. They come in for an appointment and I
[00:37:10.720 --> 00:37:16.880]   will inject them rather than verifying the lies that were on the appointment card, because basically
[00:37:16.880 --> 00:37:21.200]   F through rules, I swore an oath. I honestly, I don't know where to begin with some of these
[00:37:21.200 --> 00:37:27.920]   things. Okay. I want to understand a bunch of that. First of all, can we just go back to 25%
[00:37:27.920 --> 00:37:33.600]   of the vaccines that were allocated to California were actually delivered in people's arms?
[00:37:33.600 --> 00:37:38.800]   Literally the entire world economy was bottlenecked on this, right? It's like,
[00:37:38.800 --> 00:37:44.160]   so if you want another funny anecdote, and then asked some people in positions that might know,
[00:37:44.880 --> 00:37:49.520]   so how real do you think that 25% number is? And they said, well, the good news is
[00:37:49.520 --> 00:37:53.600]   in addition to being incompetent at delivering the vaccine, we're also incompetent in counting.
[00:37:53.600 --> 00:37:57.760]   So it was probably a bit of an undercount. I'm like, Oh, so the good news is like the true count
[00:37:57.760 --> 00:38:02.640]   was like a hundred percent or 95% or something. Well, no, not, not nearly close to that, but we
[00:38:02.640 --> 00:38:05.680]   got better at counting after the governor yelled at us because he was embarrassed. We were the
[00:38:05.680 --> 00:38:10.240]   48th state in the nation. Counting the item, which is a bottle, like, where are, where's the thing
[00:38:10.240 --> 00:38:15.280]   that is going to like rescue us from the thing that is destroying the world? So like pharmacies,
[00:38:15.280 --> 00:38:20.640]   generally speaking, could today, if you ask someone deep in the balls of pharmacies, accounting
[00:38:20.640 --> 00:38:24.320]   department, could you, by the end of business today, give us a count of like how many bottles
[00:38:24.320 --> 00:38:29.040]   of Aspirin the pharmacy has physically in the world? By the end of the day, they would have
[00:38:29.040 --> 00:38:33.440]   a shockingly accurate number for that. It wouldn't be exact, but it would be like shockingly accurate
[00:38:33.440 --> 00:38:37.360]   relative to that number is like truly millions. And if you could say like, break down by address,
[00:38:37.360 --> 00:38:41.120]   please. Like where are they physically present in the world? Yeah. Easy problem. You know,
[00:38:41.120 --> 00:38:46.240]   like managing inventories of drugs. That's what we do. The United States could not do that. Like,
[00:38:46.240 --> 00:38:49.360]   and did not perceive that to be an urgent problem to be solved.
[00:38:49.360 --> 00:38:52.960]   I mean, I do want to ask you about like the actual finance and software stuff at some point,
[00:38:52.960 --> 00:38:58.320]   but I think this is like such an important, I mean, the world is watch or standstill. We still
[00:38:58.320 --> 00:39:01.360]   haven't learned the lesson. So I'm just going to keep going on this topic because I still don't
[00:39:01.360 --> 00:39:06.480]   understand the, okay. So here's, here's another question that's sort of related to this.
[00:39:07.200 --> 00:39:11.920]   You have many rich tech industry friends and I read your article and you're saying we're trying,
[00:39:11.920 --> 00:39:16.320]   I'm filling out these grants for 50 K her and that's like taking up all my time. And I'm trying
[00:39:16.320 --> 00:39:20.240]   to raise, you know, a couple hundred grand a year, a couple of tens here. And I'm thinking to myself,
[00:39:20.240 --> 00:39:27.600]   how is this not as trivial a problem as, Hey, XYZ, uh, if you give me a money that you can find
[00:39:27.600 --> 00:39:33.600]   between your couch cushions, we will save thousands of lives and get the world economy back on track.
[00:39:33.600 --> 00:39:38.000]   Uh, how, well, how was raising money for this hard or why was it hard? You know?
[00:39:38.000 --> 00:39:41.840]   So again, like trillions of dollars are on the line. The United States is spending
[00:39:41.840 --> 00:39:47.360]   tens of billions of dollars or more on its COVID response strategy. Like the true biggest issue is
[00:39:47.360 --> 00:39:51.920]   like, why is it coming down to like Patrick McKenzie's ability to fundraise in the tech
[00:39:51.920 --> 00:39:57.520]   industry for us to like have a system here? Okay. Bracketing that, like the tech industry
[00:39:57.520 --> 00:40:00.880]   underperformed my expectations for what the tech industry should have, should have accomplished
[00:40:00.880 --> 00:40:04.880]   here. There were some bright spots and less bright spots with regards to the fundraising project.
[00:40:04.880 --> 00:40:08.640]   Um, for those of you who don't know, the total budget of this project was $1.2 million, which
[00:40:08.640 --> 00:40:12.800]   is not quite couch cushion money, uh, but is not large relative to the total amount of resources
[00:40:12.800 --> 00:40:18.080]   that the tech industry can deploy on problems. Um, and in some cases, uh, I looked at my email
[00:40:18.080 --> 00:40:22.800]   this morning to refresh my memory. I sent a CEO at a particular company, and I'm not going to name
[00:40:22.800 --> 00:40:27.040]   people, but they're welcome to claim credit if they want to claim credit. I emailed the CEO at
[00:40:27.040 --> 00:40:32.320]   a particular company, uh, "Hey, I saw you liked to tweet about this on Twitter. Uh, uh, I'm
[00:40:32.320 --> 00:40:37.920]   essentially raising a seed round except for a 501(c)(3) charity. And, uh, you know, we urgently
[00:40:37.920 --> 00:40:44.560]   need money for this. Here's a two page memo." Uh, and that, I sent that email at 4.30 PM and, uh,
[00:40:44.560 --> 00:40:50.320]   4.30 PM California time. And, uh, he got back to me, uh, there was some internal emails of like
[00:40:50.320 --> 00:40:54.720]   routers to this person, routers to this person. He run that by blah, blah, blah. 9.30 the next
[00:40:54.720 --> 00:40:58.320]   morning he said, "I'm personally in for a hundred thousand dollars out of my own pocket. My banker
[00:40:58.320 --> 00:41:03.920]   is going to contact you as a wire clerk the same day." So yay for that. Uh, on the like, yes,
[00:41:03.920 --> 00:41:08.880]   less yay side, like, uh, tech is not exactly a stranger to having bureaucracies. And in some
[00:41:08.880 --> 00:41:13.680]   cases it was a matter of like, "Oh, indicatively, uh, we want to support that, but we have a
[00:41:13.680 --> 00:41:17.920]   process." And that process, uh, went on for six weeks. And by the time six weeks was over,
[00:41:17.920 --> 00:41:26.320]   it was May and, uh, uh, not to the credit of funders, uh, by May, most people in the professional
[00:41:26.320 --> 00:41:30.000]   managerial class who had prioritized getting a vaccine for themselves and their loved ones
[00:41:30.000 --> 00:41:34.400]   had succeeded at that. And they said, "Okay, so the vaccination supply problem pretty much
[00:41:34.400 --> 00:41:38.640]   solved, right?" I'm like, "No, it is not solved right now. It is solved for the people who are
[00:41:38.640 --> 00:41:43.520]   smartest about working, work in the system in a way it was not solved for even them back in January.
[00:41:43.520 --> 00:41:47.040]   But there are many people who are not yet vaccinated." And they say, "That's a vaccine
[00:41:47.040 --> 00:41:52.160]   hesitancy issue." No, it is not merely a vaccine hesitancy issue. It is still the case that there
[00:41:52.160 --> 00:41:55.920]   are logistical problems. That is still the case that, that people don't know that you can just
[00:41:55.920 --> 00:42:00.560]   Google the vaccine now. It is, uh, uh, still the case that around the edges of the American medical
[00:42:00.560 --> 00:42:04.720]   system in places that are like underserved, et cetera, uh, people, uh, don't have it or they
[00:42:04.720 --> 00:42:08.080]   can't get transportation, et cetera, et cetera. You should continue funding this team for the
[00:42:08.080 --> 00:42:14.080]   next couple of months so that we can do what we can around the edges here. And, uh, uh, I was told,
[00:42:15.200 --> 00:42:19.920]   again, you know, people can do what they want with their own money. And, uh, I understand
[00:42:19.920 --> 00:42:24.640]   that charitable founders, uh, funders rather, uh, have, uh, many things. It's just like, okay,
[00:42:24.640 --> 00:42:29.840]   relative to the other places we can put money to work in the, in the world, uh, further investment
[00:42:29.840 --> 00:42:34.160]   in the American, uh, vaccine supply situation as of, uh, May and looking forward, it doesn't
[00:42:34.160 --> 00:42:39.920]   make sense for us. Could you do it in another nation? And, uh, we said like, okay, uh, we're
[00:42:39.920 --> 00:42:43.280]   the American effort. We have some advantages here. We would not have them in the other,
[00:42:43.280 --> 00:42:48.000]   the other nation. We did talk to people there. We, we, uh, tried to see if we could, uh,
[00:42:48.000 --> 00:42:52.000]   help a team there or go or go there, et cetera. But we don't find, we don't see that there's a
[00:42:52.000 --> 00:42:57.040]   path to, uh, like positively impacting the problem there in a way that there's manifestly a path to
[00:42:57.040 --> 00:43:02.000]   positive impact here. And, uh, we lost that argument. We didn't get the money. The last
[00:43:02.000 --> 00:43:06.960]   hundred thousand dollars in was my daughter's college education fund. Oh my God. Okay. Look,
[00:43:06.960 --> 00:43:13.840]   I, I agree that, uh, it shouldn't be up to tech to solve this huge society wide problem,
[00:43:13.840 --> 00:43:17.520]   but given that nobody else was solving it, I still don't understand.
[00:43:17.520 --> 00:43:23.520]   Um, have, have you gone back to any of them or have any of them reflected on? Yeah, maybe I
[00:43:23.520 --> 00:43:26.960]   should have just, you know, wrote you a million dollar check and saved you all this hassle. So
[00:43:26.960 --> 00:43:31.840]   you could have got back to business. So, uh, you know, ultimately I'm the CEO, like responsibility
[00:43:31.840 --> 00:43:35.920]   for fundraising, uh, lies with me. And so, uh, like I thought any number of things about how
[00:43:35.920 --> 00:43:41.760]   could I have done that better? How could I have strategized that? You know, I, I did not stop
[00:43:41.760 --> 00:43:45.120]   fundraising efforts, but I stopped lighting up new conversations for a number of weeks because
[00:43:45.120 --> 00:43:48.720]   I thought, okay, we've got, we've got the $2 million that we need to run this till the end
[00:43:48.720 --> 00:43:53.600]   of August. And that's my sort of internal target for, uh, the point at which it doesn't quite stop
[00:43:53.600 --> 00:43:58.400]   being useful, but it, uh, uh, starts like actually being, you know, a question on the margins where
[00:43:58.400 --> 00:44:04.400]   it's not a question until the end of August. Uh, so could I have done better? Probably, uh, the,
[00:44:04.400 --> 00:44:08.640]   some of the folks in the broader effective altruist community, uh, I'm not a member,
[00:44:08.640 --> 00:44:12.720]   but I've read a lot of stuff that they, uh, uh, have written over the years and I broadly consider
[00:44:12.720 --> 00:44:17.440]   them positive. They are the, but for cause of vaccinate CA, but, uh, ask me about that in a
[00:44:17.440 --> 00:44:22.880]   moment. But some EA funders, uh, uh, talked to me after my piece about it had come out and said,
[00:44:22.880 --> 00:44:28.320]   this is physically painful to read. We wrote bigger checks with less consideration to projects
[00:44:28.320 --> 00:44:33.680]   that had far less indices of success. Why didn't you just ask us for money? And, uh, like the
[00:44:33.680 --> 00:44:37.760]   answer there was twofold. One, I thought I had high quality introductions and a high quality
[00:44:37.760 --> 00:44:41.280]   personal network to people who are likely already going to fund it. And so I didn't light up
[00:44:41.280 --> 00:44:47.520]   additional funding sources. And two, uh, like this is a true answer. I'm a flawed human who has a
[00:44:47.520 --> 00:44:52.480]   limited number of cycles in their day and was running a very, very complex operation. And it
[00:44:52.480 --> 00:44:56.720]   literally didn't occur to me, Hey, maybe those people that have been making a lot of noise about
[00:44:56.720 --> 00:45:01.120]   writing a lot of money for pandemic checks would be willing to write a pandemic check, which like,
[00:45:01.120 --> 00:45:04.960]   that was not entirely an irrational belief for me because I had reached out to people who were
[00:45:04.960 --> 00:45:08.480]   making a lot of noise about writing money for pandemic checks. And they said, not in the United
[00:45:08.480 --> 00:45:12.480]   States, not in May. And I thought, Oh, well it's, uh, you know, if I'd light up a conversation,
[00:45:12.480 --> 00:45:17.680]   totally cold with someone now, it's likely to just get, get a no again. I should, uh, try to like
[00:45:17.680 --> 00:45:21.840]   scrimp and save and break the piggy bank for my daughter's college education fund, um, which by
[00:45:21.840 --> 00:45:27.440]   the way, she'll go to college folks, no worries. But, uh, uh, it's a, uh, like how far down the
[00:45:27.440 --> 00:45:33.840]   list of like plan a plan B plan C we were down to like plan C at that point. I'm just to be clear,
[00:45:33.840 --> 00:45:40.080]   I'm definitely not blaming you. I, it goes back to the Copenhagen a little bit because I should
[00:45:40.080 --> 00:45:44.080]   be rigorous about my performance. I think like, you know, you go back to the commission versus
[00:45:44.080 --> 00:45:48.480]   omission. It's like the exact same reason that we shouldn't have blamed Google. If they got involved,
[00:45:48.480 --> 00:45:53.600]   I did that to themselves and maybe made a mistake. I'd say, come on to remove the bottleneck. That
[00:45:53.600 --> 00:46:01.040]   was basically stopping all global economic activity and for causing, you know, millions
[00:46:01.040 --> 00:46:08.960]   of debts you had to in your, for the action you were taking about it, uh, take money out of your
[00:46:08.960 --> 00:46:15.280]   daughter's college fund. It's so insane. And I, so can I say this in like, there's a positive
[00:46:15.280 --> 00:46:20.880]   takeaway here. So there actually is a positive takeaway in that there is a one tiny actor who
[00:46:20.880 --> 00:46:25.360]   understands that he has unitary control over some decisions who is capable of like betting boldly
[00:46:25.360 --> 00:46:28.480]   on those without a huge amount of process when it is important to bet boldly on things.
[00:46:28.480 --> 00:46:34.640]   Not to shoot my own horn here, this is literally what happened. So like on the first day, uh,
[00:46:34.640 --> 00:46:38.400]   there, uh, you know, we're, we're getting in discord together and there's a bunch of infrastructure
[00:46:38.400 --> 00:46:42.320]   where you have to sign up. We have to like, you know, get hosting, yada, yada, yada. And, uh,
[00:46:42.320 --> 00:46:46.080]   there's a like annoying mechanical step at this point where it's like, you have to put down a
[00:46:46.080 --> 00:46:51.200]   credit card, uh, for a potentially unbounded expense. And people were like, okay, you know,
[00:46:51.200 --> 00:46:55.920]   there, there's a list of things that we want to do, but, uh, since there is no money here, uh,
[00:46:55.920 --> 00:47:00.480]   you know, like I'll take this one and you take this one. And after I like heard this conversation
[00:47:00.480 --> 00:47:04.960]   go on for two minutes, I said, this is not a conversation we should be having. Here is a,
[00:47:04.960 --> 00:47:09.200]   like a debit card for my business, which have just spun up on the backend, uh, because like,
[00:47:09.200 --> 00:47:14.560]   this is literally my job, uh, which has $10,000 on it. Spend the $10,000 on anything that accelerates
[00:47:14.560 --> 00:47:18.800]   this project. There is no approval process. There is, don't get a receipt. Don't worry about the
[00:47:18.800 --> 00:47:23.680]   paperwork right now. Um, and why did I do that? Because we were like doing things like, well,
[00:47:23.680 --> 00:47:28.480]   you know, okay. There's like a scrape, like the information about where hospitals exist and what
[00:47:28.480 --> 00:47:32.080]   their phone numbers are is probably like scrapable from the internet for free. Or we could buy a
[00:47:32.080 --> 00:47:35.840]   commercial database, but that's like a stupid amount of money. It's like $2,000. I'm like,
[00:47:35.840 --> 00:47:40.000]   relative to the importance of this project, $2,000 is a trivial amount of money. Just spend
[00:47:40.000 --> 00:47:46.800]   the $2,000 immediately rather than spending like four hours writing a scraper. Uh, and, uh, uh,
[00:47:46.800 --> 00:47:51.280]   we don't think about that in government procurement and in charities. We have some
[00:47:51.280 --> 00:47:55.680]   sacred virtues about like, you must minimize waste. You must minimize opportunities for
[00:47:55.680 --> 00:48:00.400]   corruption. You must maximize, maximize for like the, the funders of the charities for their,
[00:48:00.400 --> 00:48:06.720]   for their, like, you know, line item, uh, uh, uh, support of individual things that charities buys.
[00:48:07.360 --> 00:48:13.680]   And those sacred virtues conflict with winning and at the margins where they, where they conflict,
[00:48:13.680 --> 00:48:20.160]   we should choose winning. We should choose human lives over reducing corruption. Uh, like, uh,
[00:48:20.160 --> 00:48:25.440]   and one of the few things we were reflecting on is the, uh, uh, the tremendous amount of
[00:48:25.440 --> 00:48:30.880]   waste and fraud that happened in the, uh, like PPP loans and, uh, other, uh, Corona stimulus things.
[00:48:30.880 --> 00:48:34.800]   Like, okay, I'm not just saying this to be contrarian folks.
[00:48:35.760 --> 00:48:41.040]   We should be glad there was waste in, uh, in like COVID stimulus. If there was no waste,
[00:48:41.040 --> 00:48:46.000]   we were clearly not choosing the right margin to, uh, uh, to focus our efforts on.
[00:48:46.000 --> 00:48:51.360]   So, by the way, for the people who don't have context on how much money typically goes around
[00:48:51.360 --> 00:48:57.440]   in Silicon Valley, they think, oh, 1.4 million. How hard should that be to raise? If you right
[00:48:57.440 --> 00:49:01.680]   now, given your reputation, like literally treat it out. I'm not going to tell you my idea, but
[00:49:01.680 --> 00:49:06.960]   I'm raising $50 million seed around or something. I'm, I would be like, that's going to get filled.
[00:49:06.960 --> 00:49:13.120]   Um, and I think people don't understand, like, I have like friends who are 16 years old who have
[00:49:13.120 --> 00:49:17.200]   some GBT wrapper and they like, they don't have to worry twice about raising $1.4 million.
[00:49:17.200 --> 00:49:22.080]   Not trying to brag folks, just telling you like the reality of Silicon Valley and also the reality
[00:49:22.080 --> 00:49:28.640]   I put in a document. I, um, misapplied the, like, I have some knowledge of how, uh, uh, seed funding
[00:49:28.640 --> 00:49:33.120]   would work if I attempted to raise seed funding for a for-profit company. And I thought, um,
[00:49:33.120 --> 00:49:38.080]   originally like we're probably going to be charitable, but I'm going to pitch this to
[00:49:38.080 --> 00:49:41.920]   people as essentially like a seed investment, which they expect to like spend all the money
[00:49:41.920 --> 00:49:45.600]   as quickly as possible and go to zero, uh, while driving the total addressable market
[00:49:45.600 --> 00:49:50.960]   of the company to zero. Um, but I'm bummed. This is what passes for humor with me. Uh,
[00:49:50.960 --> 00:49:54.800]   and, uh, so I told folks pretty confidently in the first couple of days, I'm pretty sure I can
[00:49:54.800 --> 00:50:01.360]   get us $8 million and then, uh, like was actually able to deliver on 1.2 after far more tooth pulling.
[00:50:01.360 --> 00:50:05.920]   And, but like, yeah, descriptively, if I was asking for a seed, a seed stage investment,
[00:50:05.920 --> 00:50:10.960]   if I wanted to get $8 million wired by tomorrow, I think I could probably do that. And that is a
[00:50:10.960 --> 00:50:16.480]   civilizational inadequacy because like can literally get $8 million for a blank check for
[00:50:16.480 --> 00:50:20.800]   something that has a profit motive behind it. But if I write on the check, uh, Hey, we want to fit
[00:50:20.800 --> 00:50:25.040]   to fix the manifest inability of the United States to figure out where the COVID vials are,
[00:50:25.040 --> 00:50:28.880]   uh, that, that blank paper becomes less valuable by the fact of writing that.
[00:50:28.880 --> 00:50:35.120]   So maybe on reflection, I shouldn't, I just shouldn't have told people, uh, and, uh, said,
[00:50:35.120 --> 00:50:40.400]   Oh, the blank check company was, uh, you know, this thing and, uh, we're making it a 501c3, uh,
[00:50:40.400 --> 00:50:45.440]   which some ethics issues in that, but, uh, the ethics issues are less bad than allowing people
[00:50:45.440 --> 00:50:54.560]   to die. Yeah. Yeah. Um, okay. So I, the last episode I released, I, at least when, while we're
[00:50:54.560 --> 00:51:01.680]   recording this was about within AI, um, a former AI researcher who thinks that the field is
[00:51:01.680 --> 00:51:06.640]   progressing in such a way that you will need to nationalize the research in order to protect
[00:51:06.640 --> 00:51:11.920]   American national security. And when I hear this about the inability of the government to
[00:51:13.040 --> 00:51:19.920]   keep track of vials of COVID vaccines or to get them in people's arms, should for any other
[00:51:19.920 --> 00:51:24.880]   emergency that we might be worried about, whether AI, I don't know the fallout of a nuclear war or
[00:51:24.880 --> 00:51:32.080]   something, should we just discount any government response to zero? And then just, if your plan
[00:51:32.080 --> 00:51:38.240]   requires some sort of, um, competent administration by the government discounted zero, if it has to
[00:51:38.240 --> 00:51:43.120]   be something on the side. So discounting to zero is like the opposite of wisdom here because we,
[00:51:43.120 --> 00:51:48.320]   we didn't accomplish zero. We accomplished a extremely impressive thing in aggregate,
[00:51:48.320 --> 00:51:52.720]   which vastly underperformed, like the, the true thing that we were capable of. And so
[00:51:52.720 --> 00:51:56.000]   have to keep both, both of those parts of the equation in our minds at the same time.
[00:51:56.000 --> 00:52:01.600]   Um, I think that people in tech need to become radically more skilled at interfacing with
[00:52:01.600 --> 00:52:06.640]   government, uh, uh, to the extent that it is, uh, you know, we have some manifest competency
[00:52:06.640 --> 00:52:13.600]   issues in government right now. We can't simply, uh, uh, you know, sit out here and gripe about
[00:52:13.600 --> 00:52:17.360]   this on podcasts and et cetera, et cetera. Like we've got to go out and do something about it.
[00:52:17.360 --> 00:52:22.160]   And there was a, um, uh, I think it's been reported that there was a meeting among like,
[00:52:22.160 --> 00:52:26.080]   uh, tech leaders, uh, in early in the vaccination effort, where a bunch of people got in a room
[00:52:26.080 --> 00:52:30.320]   and were like, this is going terribly. I hope someone fixes it. And like, I hope someone fixes
[00:52:30.320 --> 00:52:35.520]   it is no longer a realistic alternative. I think we have to be, uh, be part of that solution there.
[00:52:35.520 --> 00:52:40.480]   Um, uh, partly it's like having, you know, higher fidelity models for how Washington works than
[00:52:40.480 --> 00:52:45.120]   simply, oh, they're bad at everything. Um, uh, like it is important to understand
[00:52:45.120 --> 00:52:50.240]   that, uh, the government has some manifest competence issues. It's also important when
[00:52:50.240 --> 00:52:52.880]   working with the government to understand like telling the government to its face,
[00:52:52.880 --> 00:52:56.800]   you have manifest competence issues is not the maximally effective way to keep getting invited
[00:52:56.800 --> 00:53:01.920]   to the meetings. Um, I was very religious about not criticizing anything about this
[00:53:01.920 --> 00:53:06.880]   Californian response effort in 2021, because we needed to be in the room where it happens.
[00:53:06.880 --> 00:53:12.480]   Um, uh, and you know, that was a choice made and, uh, I'm not a hundred percent happy with
[00:53:12.480 --> 00:53:19.280]   that choice. No, but we, we kept some relationships that we really needed. Um, the, uh, and I'm not
[00:53:19.280 --> 00:53:23.360]   saying don't criticize the government, obviously, but, uh, be, be strategic about the sort of
[00:53:23.360 --> 00:53:27.840]   things like play, like you were attempting to win the game. Uh, and, uh, you know, on the government
[00:53:27.840 --> 00:53:32.720]   side, uh, one like dispelling the massive UG field that surrounds software, this is going to be a
[00:53:32.720 --> 00:53:36.560]   part of the future, whether you like it or not, we need to get good at it. We can no longer accept
[00:53:36.560 --> 00:53:42.800]   incompetence at this as the, like Stan, a routine standard of practice in Washington, uh, to, um,
[00:53:42.800 --> 00:53:48.000]   you know, it is enormously to the United States credit that we have a extremely functional,
[00:53:48.000 --> 00:53:55.520]   extremely capable tech industry. Maybe we shouldn't treat it like the enemy,
[00:53:57.120 --> 00:54:01.600]   putting that out there. Again, this is the thing the United States has done before it,
[00:54:01.600 --> 00:54:07.040]   like there are laws in place. There are decades of practice. We could put a colonel's uniform on
[00:54:07.040 --> 00:54:13.040]   somebody like, think seriously about doing that next time. Um, do I think we have institutionally
[00:54:13.040 --> 00:54:22.160]   like absorbed all the correct lessons from this? No. Uh, when I see like after action reports,
[00:54:22.160 --> 00:54:26.640]   they have to actor after action reports, uh, praise a lot of the things that people think
[00:54:26.640 --> 00:54:32.800]   are like very important for maintaining their political coalition, which were either not
[00:54:32.800 --> 00:54:38.000]   productive or anti-productive. Uh, and they fail to identify things that were the, you know,
[00:54:38.000 --> 00:54:42.080]   the true issues. And to the extent that they identify things that were the true issues,
[00:54:42.080 --> 00:54:47.760]   uh, the, uh, uh, sort of recommended action is, well, I hope someone fixes this next time.
[00:54:47.760 --> 00:54:52.720]   And there's no longer sufficient, like that the default case is that the ball will be dropped.
[00:54:52.720 --> 00:54:57.120]   And goodness, those of us who were involved in vaccinate CA kind of dread what we call the
[00:54:57.120 --> 00:55:02.720]   bat signal where God willing, there will not be another like worldwide pandemic killing millions
[00:55:02.720 --> 00:55:08.480]   of people as long as we live. If there is one, like we know what numbers to text to get the band
[00:55:08.480 --> 00:55:22.320]   back together. Society should not rely on us as plan a how did this happen? I mean, the, the,
[00:55:22.320 --> 00:55:26.400]   the point about, uh, gripping a podcast, that's, that's definitely what I'm doing, but I just,
[00:55:26.400 --> 00:55:33.760]   maybe you're, um, humble to say this yourself, but I, I do want to commend you for, there are
[00:55:33.760 --> 00:55:39.200]   very few people I think who kind of, you, you tweeted it out. There are probably other, other
[00:55:39.200 --> 00:55:43.040]   projects that other people could have taken up that are not taken up. In this case, you tweeted
[00:55:43.040 --> 00:55:47.440]   it out. You saw that there was a thing that could be done and you did it. You like quit your job
[00:55:47.440 --> 00:55:51.920]   and you did this full time. You even, what this, and the reason you had to dip into your
[00:55:51.920 --> 00:55:56.080]   kid's college fund was because somebody who had promised a donation didn't follow up on it.
[00:55:56.080 --> 00:56:03.520]   Right. So effectively every time that the, uh, uh, we had, let me say a verbal green light with
[00:56:03.520 --> 00:56:08.320]   regards to money, I would, uh, advance the, uh, the company, the charity, charities are companies,
[00:56:08.320 --> 00:56:12.800]   by the way, folks, I don't know if that has, that is obvious, uh, was called the shots incorporated.
[00:56:12.800 --> 00:56:18.880]   So I would advance, uh, call the shots, uh, the money that was, uh, sort of soft committed, uh,
[00:56:18.880 --> 00:56:23.280]   before the money would actually arrive in the bank and the theory that like this accelerates our
[00:56:23.280 --> 00:56:28.080]   impact. We should always choose acceleration over other things, uh, like say minimizing credit risk.
[00:56:28.080 --> 00:56:33.520]   Uh, and then, uh, you know, some of the people who had like soft committed, uh, did not actually end
[00:56:33.520 --> 00:56:38.800]   up wiring money at the end of the day. Uh, and I was like, oh shoot. Well, uh, you know, uh,
[00:56:38.800 --> 00:56:44.240]   choices now are either don't run the last payroll or do run the last payroll. Uh, and you know,
[00:56:44.240 --> 00:56:49.040]   do not recover the money. I've advanced the company. Well, okay. Uh, you know, do run the
[00:56:49.040 --> 00:56:53.760]   last payroll. Did you end up recovering it in the end or? No, uh, ended up being a donation of, uh,
[00:56:53.760 --> 00:56:59.280]   from, uh, me personally to the effort. What the fuck? That is, that is the least important part
[00:56:59.280 --> 00:57:05.600]   of the story, folks. Uh, sure. But overall, I'm just like, like kudos isn't obviously enough to
[00:57:05.600 --> 00:57:11.280]   commend, uh, to convey what I mean to say here, but like, I mean, yeah, I'm, I'm glad you did that.
[00:57:11.280 --> 00:57:15.760]   Um, and I'm grateful, like four figure amount of lies. You just like, you, it's hard to sort of
[00:57:15.760 --> 00:57:21.680]   plot that on a graph and make sense of what that means, but. If I can be pretty explicit about it,
[00:57:21.680 --> 00:57:26.160]   you know, uh, to the extent kudos are, uh, deserve to anyone, you know, Carl Yang for taking up the
[00:57:26.160 --> 00:57:29.920]   torch and finding like 10 people in tech industry who would jump into something at nine o'clock.
[00:57:29.920 --> 00:57:35.120]   Those 10 people, uh, the, uh, the other board members, the hundreds of volunteers, the team of
[00:57:35.120 --> 00:57:40.960]   about 12 people who worked at a full time, uh, for very full definitions of full time, uh, virtually
[00:57:40.960 --> 00:57:46.000]   ceaseless, uh, for five, six months. And, uh, there were other, you know, projects in civil
[00:57:46.000 --> 00:57:51.360]   society. There were many people doing this as their day jobs. Uh, like the American response
[00:57:51.360 --> 00:57:55.040]   effort is not one, like small group of people anywhere. It's the collection of all these things
[00:57:55.040 --> 00:58:00.480]   bouncing off of each other. And, uh, I think like, um, you know, I'm happy about our individual
[00:58:00.480 --> 00:58:04.880]   impact. I'm happy that if you, uh, like descriptively speaking, if you Googled, uh,
[00:58:04.880 --> 00:58:08.960]   for the vaccine at any point, like vaccine near me after a certain day, uh, you know,
[00:58:08.960 --> 00:58:12.480]   before a certain day, there was no answer. After that day, there was an answer. That answer came
[00:58:12.480 --> 00:58:17.920]   from us a little dissatisfied that that didn't come from like people with, you know, vastly more
[00:58:17.920 --> 00:58:24.400]   ability to have caused that to happen much earlier. Uh, but, um, like the ultimate takeaway is not
[00:58:24.400 --> 00:58:28.720]   about this, you know, little tiny piece of the puzzle. It's how can we make the total puzzle
[00:58:28.720 --> 00:58:34.400]   better next time? Yep. So as you can tell by now, patio 11 is somebody who loves digging into the
[00:58:34.400 --> 00:58:40.400]   weeds of how the financial system actually works. And so it goes to figure that he used to work for
[00:58:40.400 --> 00:58:47.520]   this episode sponsor Stripe. Stripe is how millions of businesses around the world move money and
[00:58:47.520 --> 00:58:53.280]   accept payments. And one of the advantages of Stripe scale is they can invest in fractional
[00:58:53.280 --> 00:58:59.600]   optimizations, which are not viable for individual businesses, but add up to massive revenue and
[00:58:59.600 --> 00:59:06.080]   conversion increases. For example, last year, Stripe shaved 300 milliseconds off the render
[00:59:06.080 --> 00:59:11.600]   time for its payment links. You can think of Stripe as an entire company full of patio 11s
[00:59:11.600 --> 00:59:17.040]   dedicated to making sure they can figure out how to make the payment rails work smoothly for you.
[00:59:17.040 --> 00:59:22.720]   And that's why technology companies like Amazon and open AI and even traditional companies like
[00:59:22.720 --> 00:59:28.400]   Ford and Hertz choose to partner with Stripe. Now back to my conversation with former Stripe,
[00:59:28.400 --> 00:59:35.440]   Patrick McKenzie. Okay. Let's talk about some finance. So you've right. In addition to, you
[00:59:35.440 --> 00:59:40.160]   know, saving thousands of lives to vaccinate CA, what you've been doing over the last year or two
[00:59:40.160 --> 00:59:44.880]   is writing this finance newsletter, which is very excellent called that's about money. And you
[00:59:44.880 --> 00:59:51.120]   explore the plumbing in the financial system. My first question about this crypto at its peak was
[00:59:51.120 --> 00:59:57.520]   worth $3 trillion or something like that. If you buy the crypto skeptic perspective that you have,
[00:59:57.520 --> 01:00:01.120]   how do we think about this number? What, what does it represent? Was it just
[01:00:01.120 --> 01:00:05.920]   the redistribution of wealth from savvy people or from dupes to savvy people,
[01:00:05.920 --> 01:00:10.240]   or to the extent that useful applications didn't come out as 3 trillion, what does it represent?
[01:00:10.240 --> 01:00:16.080]   So I think I have two broad perspectives on this one. People often treat the market cap
[01:00:16.080 --> 01:00:19.840]   of something. It's like implicitly that some sort of cost on society. And I think the true
[01:00:19.840 --> 01:00:26.160]   cost of society of crypto has been that anytime one engages in attempting to do a productive
[01:00:26.160 --> 01:00:32.320]   enterprise, some actor in society has said, okay, I will stake you with some of society's resources,
[01:00:32.320 --> 01:00:36.800]   which these resources are rivalrous. They cannot be applied to any other things society needs in
[01:00:36.800 --> 01:00:39.600]   the hope that you will produce something that is worthy of being staked with this.
[01:00:39.600 --> 01:00:44.800]   How much have we spent on crypto, not on trading tokens around, but on building infrastructure
[01:00:44.800 --> 01:00:51.600]   and spending rivalrous resources that we can't get back, whether that's, you know, GPUs or ASICs or
[01:00:51.600 --> 01:00:54.720]   electricity that could have gone to other things in China, but went into mining or
[01:00:55.360 --> 01:00:59.120]   the time of talented and intelligent people that could have been building other software products,
[01:00:59.120 --> 01:01:03.280]   but was instead building crypto. That number is in the tens of billions or hundreds of billions
[01:01:03.280 --> 01:01:07.440]   of dollars. What do we have to show for that tens of billions or hundreds of billions of dollars?
[01:01:07.440 --> 01:01:14.880]   I am very crypto skeptical, and I could give you an answer to that question. I think crypto fans
[01:01:14.880 --> 01:01:19.600]   would not like to hear it from me, so I prefer Vitalik Buterin's articulation of this question
[01:01:19.600 --> 01:01:26.480]   from 2017. He asked, at the time it was 0.5 trillion, a trivial number, only 500 billion
[01:01:26.480 --> 01:01:31.440]   dollars in market cap. He said, "Have we truly," and I'm paraphrasing a tweet thread, "Have we
[01:01:31.440 --> 01:01:37.440]   truly earned this number? How many of the unbanked have we actually banked? How many distributed
[01:01:37.440 --> 01:01:41.280]   applications have a meaningful amount of value doing something which is meaningful?" And he has
[01:01:41.280 --> 01:01:50.720]   about six other meditations on this. Crypto folks certainly aren't accountable to me. In some
[01:01:50.720 --> 01:01:58.160]   manner, you're not even accountable to Buterin, even though he's clearly a leading intellectual
[01:01:58.160 --> 01:02:02.080]   in the community. You're accountable to producing positive value in the world.
[01:02:02.080 --> 01:02:08.560]   But what is the answer to Buterin in 2024? How many of the unbanked have we truly banked? What
[01:02:08.560 --> 01:02:13.360]   is the best use case for crypto right now? Once crypto has a responsive answer for that, that is
[01:02:13.360 --> 01:02:17.760]   sized anything like proportionate to the hundreds of billions of dollars, tens of billions or hundreds
[01:02:17.760 --> 01:02:23.280]   of billions of dollars of resources that we've staked crypto with, I think crypto people should
[01:02:23.280 --> 01:02:27.600]   feel enormously proud of that accomplishment in some future where it hypothetically arrives.
[01:02:27.600 --> 01:02:32.720]   And in some future where that hypothetically arrives, you have my sword. I will love your
[01:02:32.720 --> 01:02:37.840]   initiative. However, for the last many years, we have been saying, "You can still get in early.
[01:02:37.840 --> 01:02:42.640]   You can still get in early. You can still get in early because the value has not arrived yet."
[01:02:42.640 --> 01:02:50.960]   And so that is my capsule summary on crypto 14 years in. We've staked a group of talented people
[01:02:50.960 --> 01:02:56.480]   who are very good at giving a sales pitch with tens of billions or hundreds of billions of dollars,
[01:02:56.480 --> 01:03:00.800]   and look at what we have built.
[01:03:04.240 --> 01:03:12.320]   This would be a failure in any other tech company. Capital F failure. So either radically pivot and
[01:03:12.320 --> 01:03:16.560]   unfail it, or maybe we should stop continuing to stake you with money.
[01:03:16.560 --> 01:03:23.120]   So two potential responses from the crypto-optimistic perspective. One,
[01:03:23.120 --> 01:03:28.800]   I have people who help me with a podcast who are around the world. Not that many,
[01:03:28.800 --> 01:03:34.320]   but the couple I have are around the world. I have a clip editor in Argentina. I have a
[01:03:34.320 --> 01:03:41.360]   shorts editor in Sri Lanka. And all of these people have asked me, I haven't prompted them,
[01:03:41.360 --> 01:03:47.760]   I asked them, "How should I pay you?" And they say, "USDC." And so maybe it wouldn't be that
[01:03:47.760 --> 01:03:52.480]   much harder for them to set up a wise account, but I think it's notable that all of them prefer
[01:03:52.480 --> 01:03:57.440]   just, "How should I get paid?" Their first answer is a stablecoin.
[01:03:57.440 --> 01:04:01.920]   Absolutely. That is evidence. And some tech-savvy people have a good payment rail,
[01:04:01.920 --> 01:04:06.000]   they have a payment rail that they did not have access to 15 years ago,
[01:04:06.000 --> 01:04:13.840]   but at the cost of tens or hundreds of billions of dollars. Counterfactually, if one had thought,
[01:04:13.840 --> 01:04:17.520]   "Okay, we really want to work on that payment rail specifically," another way one could
[01:04:17.520 --> 01:04:23.920]   hypothetically have deployed $10 billion is on the best-funded lobbying campaign in history in the
[01:04:23.920 --> 01:04:30.480]   United States to work on AML and KYC regulation to allow more easy transfers of money worldwide.
[01:04:30.480 --> 01:04:33.680]   But why does it have to be compared against the best possible counterfactual use case?
[01:04:33.680 --> 01:04:36.320]   It's the sins of commission versus omission again, where it's like,
[01:04:36.320 --> 01:04:39.920]   "On the margin, it made things better." Don't judge it by hypothetical rules,
[01:04:39.920 --> 01:04:44.720]   just keep in mind that hypothetical worlds might exist. Judge it by the actual realized utility at
[01:04:44.720 --> 01:04:49.600]   the moment relative to the amount of resources consumed. The second point is, if you look at,
[01:04:49.600 --> 01:04:54.000]   for example, the dot-com bubble, literally close to a trillion dollars were invested in laying out
[01:04:54.000 --> 01:04:57.520]   the fiber and the cable for this artifact that now you consider is the most valuable
[01:04:57.520 --> 01:05:05.280]   thing that humanity has built. And at the time, reasonably, a lot of the companies that built this
[01:05:05.280 --> 01:05:10.720]   went bust. There was a bubble-like dynamic where many of the investors who spent the capital to
[01:05:10.720 --> 01:05:14.560]   build out this infrastructure weren't paid back, didn't see immediate use cases from what they'd
[01:05:14.560 --> 01:05:19.040]   built. And the bubble had sort of served as a shelling point to then in the future get things
[01:05:19.040 --> 01:05:22.960]   rolling. And that was hundreds of billions of dollars, a trillion dollars, tens of billions
[01:05:22.960 --> 01:05:26.080]   of dollars. If in the future, something cool comes out of it and it's a useful use case,
[01:05:26.080 --> 01:05:28.080]   that's probably worth it, right?
[01:05:28.080 --> 01:05:31.920]   Cool. At what point do we get to say that didn't happen?
[01:05:31.920 --> 01:05:34.880]   A trillion dollars?
[01:05:34.880 --> 01:05:41.120]   At what date in the future do we judge someone has been right or someone has been not with respect to
[01:05:41.120 --> 01:05:46.720]   we have created... People in crypto have very confidently stated in various places that this
[01:05:46.720 --> 01:05:52.800]   is the next iteration of the internet. This will revolutionize the world, not just how payments are
[01:05:52.800 --> 01:05:59.280]   conducted, but it will be a fundamentally new computing architecture. Okay. At what day do we
[01:05:59.280 --> 01:06:01.680]   compare notes on whether that claim was accurate or not?
[01:06:01.680 --> 01:06:04.080]   Does 2030 seem like a reasonable year or too far?
[01:06:04.080 --> 01:06:08.160]   It seems reasonable to me. Can I make a prediction of what is said in 2030?
[01:06:08.160 --> 01:06:14.480]   You can still be early. Crypto has created huge amounts of things, but it's not achieved
[01:06:14.480 --> 01:06:19.680]   anywhere near its true potential. Please invest in our new crypto startup. Let's check back in
[01:06:19.680 --> 01:06:26.560]   2030, folks. Please tweet me if I'm wrong in 2030. I will happily eat crow. I want to eat crow.
[01:06:26.560 --> 01:06:32.080]   Crypto people are like, "How couldn't you be interested in programmable money?" I'm like,
[01:06:32.080 --> 01:06:36.320]   "I'm interested in programmable money." Obviously, money is programmable money.
[01:06:36.320 --> 01:06:42.000]   My friends who are trying to sell me on this since 2010 weren't wrong. This should totally
[01:06:42.000 --> 01:06:50.720]   smash my interest based on what I usually find intellectually edifying. I don't not find crypto
[01:06:50.720 --> 01:06:55.360]   intellectually edifying. I actually think there are some interesting things that have come out
[01:06:55.360 --> 01:07:01.680]   of the movement. But I find a computer built in Minecraft out of redstone to be intellectually
[01:07:01.680 --> 01:07:06.080]   edifying. It's a wonderful educational device for people who don't understand how a CPU works.
[01:07:08.480 --> 01:07:16.320]   I'm not proposing to use the redstone-emulated computer in Minecraft to be the next computational
[01:07:16.320 --> 01:07:21.040]   infrastructure for the world because that obviously will not work very well.
[01:07:21.040 --> 01:07:24.640]   Another answer of what the value here is,
[01:07:24.640 --> 01:07:31.600]   listen, we want some sort of hedge against — and I think this is actually a reasonable argument. I
[01:07:31.600 --> 01:07:35.200]   actually don't buy the capabilities that have been unlocked by crypto, but I do buy the argument that
[01:07:35.200 --> 01:07:42.080]   we want some kind of hedge against the government going crazy and KYC/AML leading to state
[01:07:42.080 --> 01:07:47.360]   surveillance. All the compliance departments in the banks just start seeing if you've been to
[01:07:47.360 --> 01:07:54.720]   political protest and debanking you. It may seem unlikely, but it's good to have this alternative
[01:07:54.720 --> 01:08:01.680]   rail which keeps the system honest given that there's an alternative. If things go off the
[01:08:01.680 --> 01:08:08.400]   rail, is this a worthwhile investment? Society as a whole can't even count that low in terms of the
[01:08:08.400 --> 01:08:12.080]   other resources that it spends, so it's a good hedge against that kind of outcome.
[01:08:12.080 --> 01:08:17.680]   I'm actually much more sympathetic to crypto people than I expect most people who have a
[01:08:17.680 --> 01:08:22.800]   traditional financial background to be. I think it is descriptively accurate that the banking system
[01:08:22.800 --> 01:08:29.280]   and all companies which are necessarily tightly tied to the banking system — which might be all
[01:08:29.280 --> 01:08:35.520]   companies, let's come with a central example so that's second set first — are a policy arm of the
[01:08:35.520 --> 01:08:44.640]   government. I think whether people articulate that in exactly those words or not varies a little bit,
[01:08:44.640 --> 01:08:48.080]   but when you have your mandatory compliance training, you'll be told in no uncertain terms
[01:08:48.080 --> 01:08:55.680]   that you are a policy arm of the government. I feel for crypto folks that say that this feels
[01:08:55.680 --> 01:09:01.040]   like warrantless search and seizures of people's information in a very undirected, dragnet fashion.
[01:09:01.040 --> 01:09:08.640]   Some somewhat complicated thoughts about this. The modern edifice of know-your-customer, KYC,
[01:09:08.640 --> 01:09:13.040]   and AML, anti-money laundering, dates back to the BSA Bank Secrecy Act in the United States,
[01:09:13.040 --> 01:09:20.720]   which was late 1970s, early 1980s. At the time, the federal government in the United States was
[01:09:20.720 --> 01:09:27.440]   strictly rate-limited in how much attention it could give to KYC and AML. Maybe because we thought
[01:09:27.440 --> 01:09:32.000]   we had very limited state capacity at the time, the government would make rational decisions and
[01:09:32.000 --> 01:09:38.240]   maybe it will go after 10 or 100 enormous white-collar crooks and drug-trafficking cartels
[01:09:38.240 --> 01:09:44.480]   a year and not surveil down to literally everyone in society. The regulations we wrote and have
[01:09:44.480 --> 01:09:49.760]   continued to write and have continued to tighten on over the years do effectively ask for transaction
[01:09:49.760 --> 01:09:55.600]   level surveillance of every transaction that goes through a bank. It is the actual practice,
[01:09:55.600 --> 01:10:01.840]   and this is not a conspiracy theory. I'm making nothing up, folks. This is acknowledged that the
[01:10:01.840 --> 01:10:05.920]   actual practice in banks is that they have, descriptively, about as many intelligence
[01:10:05.920 --> 01:10:10.880]   analysts as the American intelligence community has, who get this scrolling feed of alerts that
[01:10:10.880 --> 01:10:16.960]   are generated by automated systems. For each alert, they either go, "Don't worry, don't worry,
[01:10:16.960 --> 01:10:20.560]   don't worry, don't worry, don't worry, don't worry, don't worry, don't worry, don't worry."
[01:10:20.560 --> 01:10:25.920]   Millions of these alerts every day, and then for some tiny percentage, they say, "Oh, dear. This
[01:10:25.920 --> 01:10:31.760]   one might actually have been a problem. I'm going to write a two-to-four-page memo and file it with
[01:10:31.760 --> 01:10:35.840]   the Financial Crimes Enforcement Network, and in all probability, no one is ever actually going to
[01:10:35.840 --> 01:10:41.200]   read that memo." But we have an intelligence community-sized operation running in banks
[01:10:41.200 --> 01:10:45.600]   to write memos that no one ever reads, because some tiny portion of those memos will be useful
[01:10:45.600 --> 01:10:50.000]   to law enforcement in the future. If you had explained that trade in a presidential debate
[01:10:50.000 --> 01:10:56.880]   in the 1980s, I find it extremely unlikely that any part of the American polity would say, "Yeah,
[01:10:56.880 --> 01:11:00.240]   yeah, I want to buy that. Could we perhaps spend tens of billions of dollars on it?"
[01:11:00.240 --> 01:11:06.400]   But we did that. To the extent I'm extremely copacetic with crypto folks on this point,
[01:11:06.400 --> 01:11:12.240]   A, this thing factually exists in the world, I agree with you that it does. B, in an ideal world,
[01:11:13.040 --> 01:11:18.240]   I don't think this thing would exist. I agree with you. There are very real privacy fears.
[01:11:18.240 --> 01:11:30.240]   However, crypto has this habit. People who are good at sales have various sales pitches that
[01:11:30.240 --> 01:11:36.560]   they give to various people, and crypto actors within the crypto ecosystem will talk an excellent
[01:11:36.560 --> 01:11:42.400]   game about privacy as long as number goes up. When you say, "But you can choose between being
[01:11:42.400 --> 01:11:46.240]   tied into the banking system, which is necessary for number go up, or you can choose privacy,"
[01:11:46.240 --> 01:11:48.400]   they will say, "Excellent. I choose number go up."
[01:11:48.400 --> 01:11:53.280]   But there's different protocols. You can use the ones that allow privacy if you care more about
[01:11:53.280 --> 01:12:01.280]   privacy, right? Yeah. Descriptively, that is a very tiny portion of crypto.
[01:12:01.280 --> 01:12:07.520]   Also, to riff on what you were saying about the analysts, that number as many as would
[01:12:07.520 --> 01:12:13.440]   be an intelligence agency, these apparatchiks who are connected to the government's policy,
[01:12:13.440 --> 01:12:20.160]   just analyzing each transaction. As soon as the government gets the competence to run an LLM
[01:12:20.160 --> 01:12:24.480]   across each of these millions of queries... Right. This is a legitimate worry because as we...
[01:12:24.480 --> 01:12:32.640]   This is funny in the echoes of we have extremely low state capacity for this thing that we didn't
[01:12:32.640 --> 01:12:37.040]   think was important, which was successfully administering vaccines. But we do have extremely
[01:12:37.040 --> 01:12:44.080]   high state capacity with regards to running the security state. If they successfully managed to
[01:12:44.080 --> 01:12:49.680]   get their technological ducks in order, which there are pluses and minuses there, but they have
[01:12:49.680 --> 01:12:54.240]   built some things that are extremely impressive technologically, and then just run it on this
[01:12:54.240 --> 01:13:02.160]   data set that we've passively been producing, the implicit ongoing invasion of privacy is much worse
[01:13:02.160 --> 01:13:06.320]   than we baked into the system in 1980 when it would have been people going down to archives
[01:13:06.320 --> 01:13:10.480]   to look at things in microfiche to try to do this. I mean, I'm not even necessarily making
[01:13:10.480 --> 01:13:15.600]   a point over crypto here, but I think it's worth meditating on the fact that the default path for
[01:13:15.600 --> 01:13:21.760]   this technology is that a very smart LLM is going to be looking at every single transaction that is
[01:13:21.760 --> 01:13:28.800]   electronically, aka every single transaction ever. And it's intelligent enough to sort of
[01:13:28.800 --> 01:13:31.600]   understand the implications, how it connects to your other transactions and what's the broader
[01:13:31.600 --> 01:13:35.680]   activity you're doing here. And maybe this is just a broader point about how... Step back from
[01:13:35.680 --> 01:13:40.480]   crypto and finance for a moment. I think this is one of the least understood things about the tech
[01:13:40.480 --> 01:13:48.160]   industry, where we have this societal level question that is not being addressed directly,
[01:13:48.160 --> 01:13:53.440]   but it's being addressed by misunderstood proxy questions on taking as written that the finance
[01:13:53.440 --> 01:13:59.280]   industry is a branch of government in a meaningful sense. Should the tech industry also be a branch
[01:13:59.280 --> 01:14:03.920]   of government? And we don't ask that question directly. We have asked instead things like,
[01:14:03.920 --> 01:14:07.920]   should the tech industry be responsible for minimizing the spread of misinformation,
[01:14:07.920 --> 01:14:16.320]   et cetera, et cetera. And there was an injunction issued in a court case last year on the 4th of
[01:14:16.320 --> 01:14:22.720]   July, which I find oddly aesthetically motivating. The court case is Missouri et al versus Biden et
[01:14:22.720 --> 01:14:30.240]   al. But the argument made in the court case, which the judge accepted, which is extremely
[01:14:31.760 --> 01:14:37.200]   well supported by the record in front of him, is that various actors within the United States
[01:14:37.200 --> 01:14:44.480]   government puppeted the tech companies and used them as cat's paws to do, frankly,
[01:14:44.480 --> 01:14:49.280]   shocking violations of constitutionally protected freedoms like the freedom of speech.
[01:14:49.280 --> 01:14:57.360]   And that there were not on the level of, we've built this unaccountable, hard to inspect system
[01:14:57.360 --> 01:15:03.680]   of LLMs and heuristics. And we started turning off a lot of people's feeds in Facebook. But no,
[01:15:03.680 --> 01:15:06.640]   there was an individual person in the White House who was sending out emails like,
[01:15:06.640 --> 01:15:10.400]   when are you going to address me on this tweet, guys? We can't have things like this anymore,
[01:15:10.400 --> 01:15:15.440]   et cetera, et cetera. Again, a feature of the United States, we are very good about keeping
[01:15:15.440 --> 01:15:20.800]   records and transparency and having a functioning legal system. And the record before the court is,
[01:15:22.720 --> 01:15:27.280]   I was following along as this was happening. And what was happening was much worse than I
[01:15:27.280 --> 01:15:32.720]   understood to be happening. An example of something like, as we were growing up as children,
[01:15:32.720 --> 01:15:37.360]   would you ever think that the United States federal government would tell, I believe it was
[01:15:37.360 --> 01:15:42.720]   the state of Missouri, hey, you have town halls where like citizens can come in and speak their
[01:15:42.720 --> 01:15:47.440]   mind and advocate for their policy preferences. And you probably have a civics class and talk
[01:15:47.440 --> 01:15:52.400]   about like the first amendment and things. Yeah. Someone said something we don't like in a
[01:15:52.400 --> 01:15:58.640]   particular town hall, take down the recording from YouTube. That happened. That is a violation of the
[01:15:58.640 --> 01:16:03.360]   constitution of the United States that is against everything in the traditions and laws and culture
[01:16:03.360 --> 01:16:12.160]   of the United States. That is outrageous. And yet it happened. And we have not repudiated the notion
[01:16:12.160 --> 01:16:20.000]   of using tech as cat's paws or using, in some cases, this is literally written in the decision,
[01:16:20.000 --> 01:16:24.320]   by the way, which I would urge everyone to read. There was an individual and a non-governmental
[01:16:24.320 --> 01:16:28.720]   organization, which was collaborating with the governmental organizations in doing this,
[01:16:28.720 --> 01:16:38.720]   which said to get around, I'm not quoting exactly, to get around legal uncertainty,
[01:16:38.720 --> 01:16:43.760]   comma, including very real first amendment concerns, comma, in our ability to do this,
[01:16:43.760 --> 01:16:47.440]   rather than doing it in the government directly, we are outsourcing it to a bunch of college
[01:16:47.440 --> 01:16:54.800]   students who we have hired under the auspices of this program. Like what? Like one, just as a
[01:16:54.800 --> 01:17:02.080]   dangerous professional here, you violated the character from The Wire, Stringer Bell. Stringer
[01:17:02.080 --> 01:17:06.720]   Bell's dictum on the wisdom of taking notes on a criminal conspiracy. But you literally wrote that
[01:17:06.720 --> 01:17:12.640]   down in an email. The outrageous part is not that you wrote that down in an email. The outrageous
[01:17:12.640 --> 01:17:19.840]   part is you, with full knowledge of it, engaged in something that is outrageously unconstitutional,
[01:17:19.840 --> 01:17:27.360]   immoral, illegal, and evil to the applause of people in your social circle. And everyone
[01:17:27.360 --> 01:17:33.120]   involved in the story thinks that they're the good guy in it. If you write that email,
[01:17:33.120 --> 01:17:39.440]   you are not the good guy in the story. Okay. So on that, by the way, so what is your sense of what
[01:17:39.440 --> 01:17:46.640]   the judicial end result of deliberations on this will be? I think there will be a limited hangout
[01:17:46.640 --> 01:17:52.880]   and walk back of some particular things. And do I predict that? And probably there does exist an
[01:17:52.880 --> 01:18:00.240]   injunction. I predict that that continues to happen in the future. What do I know? I'm just
[01:18:00.240 --> 01:18:07.920]   a software guy. But do people want to achieve power? The tech industry descriptively has power
[01:18:07.920 --> 01:18:13.200]   because it is good at achieving results in the physical world. This is certainly not going to be
[01:18:13.200 --> 01:18:20.240]   the last time that someone desiring power thinks, "Okay, can I force you to give me the power that
[01:18:20.240 --> 01:18:27.440]   you have accumulated?" And ultimately, this is fundamentally a political decision about how we
[01:18:27.440 --> 01:18:32.560]   construct our democracy, and we should make good decisions about that. Well, I mean, maybe that's
[01:18:32.560 --> 01:18:37.840]   the crux here, which is that through the story you illustrated of the Vaccinate CA and the lack
[01:18:37.840 --> 01:18:42.400]   of accountability that showed in our institutions, the idea that you're going to go back to Congress
[01:18:42.400 --> 01:18:47.680]   and get them to pass some law that says, "Oh, we're KYC, AML. We realize that without alums,
[01:18:47.680 --> 01:18:51.120]   it's going to be a bad deal with regards to privacy. We're going to roll that back." We
[01:18:51.120 --> 01:18:56.480]   don't like that the collusion between tech and whoever's in power and it being able to dictate
[01:18:56.480 --> 01:19:01.920]   what can get taken off the platform, and that we think without violence, free speech, we'll pass
[01:19:01.920 --> 01:19:07.360]   laws and take that back. There's no sense in which society comes to a consensus and, "Here's the
[01:19:07.360 --> 01:19:13.200]   privacy concerns we have. Here's the free speech concerns we have." So at least one argument goes,
[01:19:13.200 --> 01:19:18.000]   the solution has to be that you just go off a rail. You start a new rail for these kinds of
[01:19:18.000 --> 01:19:22.720]   things that cannot be constrained in this way. And it's not a matter of just changing the KYC
[01:19:22.720 --> 01:19:28.960]   law. That's implausible given the manifest decline in state capacity we've talked about.
[01:19:28.960 --> 01:19:33.920]   Yeah. I don't accept that that is the only thing possible for us. I don't accept that
[01:19:33.920 --> 01:19:38.560]   the United States is incapable of doing nice things. We can't accept that. We have to be
[01:19:38.560 --> 01:19:42.720]   optimistic about the future. Otherwise, what are we doing here? In the tech industry,
[01:19:42.720 --> 01:19:49.120]   we know it is not a physical law of reality or of large institutions that one cannot make systems
[01:19:49.120 --> 01:19:52.960]   that work. Making systems that work is the job. We have a few existence proofs.
[01:19:52.960 --> 01:20:02.400]   We should increase our engagement with government on, "Hey, state capacity, we can help you build
[01:20:02.400 --> 01:20:09.120]   some of that stuff." Also, Constitution of the United States is a document we feel attached to
[01:20:09.120 --> 01:20:16.640]   that document. We understand, again, incentives rule everything around us. Tech industry was,
[01:20:16.640 --> 01:20:21.520]   in early 2021, very concerned about being told in no uncertain way by people in power,
[01:20:21.520 --> 01:20:31.360]   "If you embarrass us, we'll end you." One thing in the judicial record of this case
[01:20:31.360 --> 01:20:37.920]   is that the White House and other government actors routinely overreached and asked the tech
[01:20:37.920 --> 01:20:43.600]   companies, "We would like you to censor this and this and this and this." The tech companies said
[01:20:43.600 --> 01:20:49.280]   no in a bunch of cases. Continuing to negotiate for the right outcome rather than the one that
[01:20:49.280 --> 01:20:58.880]   people in power merely want is important. There are some things that will feel unfortunate and
[01:20:58.880 --> 01:21:05.600]   maybe a little bit outside of our true sweet spot of what we would want to be doing on Tuesday
[01:21:05.600 --> 01:21:11.360]   in the tech industry, where maybe we have to ratchet up the amount of public policy advocacy
[01:21:11.360 --> 01:21:14.400]   that we do. Lobbying is a dirty word in the tech industry. It probably shouldn't be.
[01:21:14.400 --> 01:21:25.120]   Not just lobbying, but when the "do not embarrass us" order came down and people were getting very
[01:21:25.120 --> 01:21:30.800]   quiet about the fact of feeling constrained by this, maybe we should have spoken out more and
[01:21:30.800 --> 01:21:38.400]   spoken more boldly about it. Maybe when it was the routine case that the White House was telling
[01:21:38.400 --> 01:21:44.800]   Facebook, Twitter, et cetera, et cetera, everyone knows the names. Companies on a tweet by tweet,
[01:21:44.800 --> 01:21:49.360]   communication by communication basis, and also with regards to broad rules that affected
[01:21:49.360 --> 01:21:53.840]   the entire of the citizenry of the United States and residents of the United States,
[01:21:53.840 --> 01:21:57.120]   and also everyone else in the world, because these are the operating systems of the world.
[01:21:57.120 --> 01:22:03.360]   Giving direct orders on ... There's a certain kind of free speech, rather. There's a certain
[01:22:03.360 --> 01:22:07.520]   kind of speech act which we find vexatious, and we would like you to stop that everywhere.
[01:22:08.480 --> 01:22:13.200]   Like, very plausibly, you should get on the nightly news and say, "I received the following
[01:22:13.200 --> 01:22:21.120]   email from the White House that says we should stop this everywhere. If you point a gun at me,
[01:22:21.120 --> 01:22:24.240]   I will comply with this at the point of a gun." That is what it will take.
[01:22:24.240 --> 01:22:29.200]   It almost requires civil disobedience in the sense of, if you're right about the earlier
[01:22:29.200 --> 01:22:32.240]   statement that in 2021, there are going to be serious political repercussions on the tech
[01:22:32.240 --> 01:22:38.240]   companies. Say that's right, and then they publish this email, like, "Here's what the
[01:22:38.240 --> 01:22:42.400]   White House just sent me," they take down this tweet, and now Twitter's market cap has just
[01:22:42.400 --> 01:22:47.280]   collapsed because people realize the political implications of what Jack Dorsey just got up
[01:22:47.280 --> 01:22:54.560]   and said at the time. Then the solution is that you need tech companies to basically sacrifice their
[01:22:56.720 --> 01:23:02.160]   capacity to do business in order to... I mean, maybe that is a solution, but that's not a story
[01:23:02.160 --> 01:23:06.080]   about optimism about the ability of the US government to solve problems this way.
[01:23:06.080 --> 01:23:11.200]   You need to have a risk tolerance, right? Every business everywhere, including the financial
[01:23:11.200 --> 01:23:17.280]   industry, including the tech industry, is balancing various risks, and the risk tolerance was poorly
[01:23:17.280 --> 01:23:24.800]   calibrated. One can achieve results in the world by doing things like embarrassing government
[01:23:24.800 --> 01:23:32.880]   officials. Embarrassing a person in a position of authority is not a zero-risk behavior. It is
[01:23:32.880 --> 01:23:37.600]   relatively low risk in the United States relative to other places. That was extremely important for
[01:23:37.600 --> 01:23:42.880]   Vaccinate C8. People thought at the beginning, "Are we going to get in trouble for publishing
[01:23:42.880 --> 01:23:46.480]   true information about vaccine availability that will embarrass the state of California?"
[01:23:46.480 --> 01:23:50.640]   I said, "I have a very high confidence that no matter what we do vis-a-vis the state of California,
[01:23:50.640 --> 01:23:53.600]   that you cannot get in serious trouble in the United States for saying true things."
[01:23:54.640 --> 01:24:01.280]   The First Amendment exists. We have backstopping infrastructure here, and if push comes to shove,
[01:24:01.280 --> 01:24:08.640]   we will shove back and we will win. This is just me as a guy who took the same civics course that
[01:24:08.640 --> 01:24:15.600]   everyone took and does not have a huge amount of resources relative to, say, the entire tech
[01:24:15.600 --> 01:24:22.000]   industry. Maybe we need to have a certain amount of intestinal fortitude on a, "Okay,
[01:24:23.840 --> 01:24:29.440]   you've asked us to do something. You've threatened us with taking away all the wonderful toys and the
[01:24:29.440 --> 01:24:36.640]   great business models that make this an extremely lucrative area to work in, and to sacrifice a
[01:24:36.640 --> 01:24:42.320]   value that is very important for us to continue to do that. No, we're going to fight you on this one."
[01:24:42.320 --> 01:24:50.080]   The comms-trained part of me is like, "Don't use the word 'fight.' We are going to collaborate
[01:24:50.080 --> 01:24:54.960]   with stakeholders across civil society to achieve an optimal outcome, balancing the
[01:24:54.960 --> 01:25:00.880]   multiple disparate and legitimate interests of various arms of the government and civil society."
[01:25:00.880 --> 01:25:04.800]   Sometimes that requires fighting. We should fight when it does.
[01:25:04.800 --> 01:25:10.400]   On Tyler's podcast, you said something like, "America doesn't have the will to have nice
[01:25:10.400 --> 01:25:13.040]   things, and Japan does." But if you think about-
[01:25:13.040 --> 01:25:13.760]   In some ways, yes.
[01:25:13.760 --> 01:25:17.280]   If you think about your own essay about working as a salaried man, you're
[01:25:17.280 --> 01:25:21.760]   working 70-hour weeks, and you're killing yourself to get that marginal adornment
[01:25:21.760 --> 01:25:26.640]   on the products you're making. Isn't it more efficient? Isn't it better that we have a system
[01:25:26.640 --> 01:25:31.760]   where we put in 20% of the work to get 80% of the results? We spend the rest of the effort on,
[01:25:31.760 --> 01:25:37.600]   I don't know, expanding the production possibilities frontier, and it's good that
[01:25:37.600 --> 01:25:39.920]   we don't have the will to have nice things. We just get it done.
[01:25:39.920 --> 01:25:43.360]   I don't think these trade off against each other at the relevant margins.
[01:25:45.360 --> 01:25:49.520]   Nothing about culture is monocultizational. Also, I don't think culture is a
[01:25:49.520 --> 01:25:56.320]   sufficient explanation for some of the differences that are achieved in the United States versus
[01:25:56.320 --> 01:26:01.120]   the Japan, for example. There's a great book, Making Common Sense of Japan, by a person whose
[01:26:01.120 --> 01:26:06.880]   name I'm blanking on at the moment. An argument he makes in it persuasively and at length, which
[01:26:06.880 --> 01:26:12.080]   I don't think is 100% true, but is more true than most people well-informed on either side of the
[01:26:12.080 --> 01:26:19.040]   Pacific belief, is that when people say they do this because it is Japanese culture, what they're
[01:26:19.040 --> 01:26:25.760]   often saying is, "I usually have a model for why people do things driven by incentives. I understand
[01:26:25.760 --> 01:26:29.360]   this incentive, this incentive, this incentive, and then there's some error term in this equation
[01:26:29.360 --> 01:26:33.200]   that I don't understand. I'm going to call that error term culture." I think culture is a real
[01:26:33.200 --> 01:26:38.640]   thing in the world, to be clear, but I often think that we reach for that error term far
[01:26:38.640 --> 01:26:47.920]   faster than we should. As my minor observation about culture with respect to, there are places
[01:26:47.920 --> 01:26:51.920]   and pockets of the United States that have the will to have nice things, and often they discover,
[01:26:51.920 --> 01:26:56.640]   sometimes surprisingly, that the only thing you need to do nice things at the relevant margin,
[01:26:56.640 --> 01:27:01.840]   without spending more money, without having people kill themselves over 90-hour weeks for
[01:27:01.840 --> 01:27:05.680]   the entirety of the career, you can just choose to have nice things. Let's choose to have nice
[01:27:05.680 --> 01:27:09.760]   things. Let's not be embarrassed about choosing to have nice things. So you understand all this
[01:27:09.760 --> 01:27:14.960]   financial plumbing. If you were an investigative reporter, what is the thing you're looking at that
[01:27:14.960 --> 01:27:19.760]   the average reporter at some newspaper wouldn't know to look at to investigate a person or a
[01:27:19.760 --> 01:27:25.440]   company or a government institution? I have an enthusiasm for the minutiae of banking procedure
[01:27:25.440 --> 01:27:30.640]   in a way that few people have enthusiasm for the minutiae. Sometimes banking procedure causes
[01:27:31.360 --> 01:27:37.040]   physically observable facts to emanate into the world. If you know that those facts are
[01:27:37.040 --> 01:27:42.320]   going to emanate, then you can have a claim made about a past state of the world. I did this thing,
[01:27:42.320 --> 01:27:47.600]   or I did not do this thing. That claim will, if it is true, cause metadata in other places,
[01:27:47.600 --> 01:27:51.120]   and you can look for the metadata. This is actually how a lot of frauds are discovered,
[01:27:51.120 --> 01:27:58.160]   because basically the definition of fraud is you're telling someone a story, the story alleges
[01:27:58.160 --> 01:28:03.120]   the fact about the world, the story is not true, and you're using the story to extract value from
[01:28:03.120 --> 01:28:10.480]   them. Most frauds will allege facts about the physical world. As the physical world gets more
[01:28:10.480 --> 01:28:15.920]   and more mediated by computers, as it gets increasingly sharded between different institutions,
[01:28:15.920 --> 01:28:22.960]   there will often be institutions who are not under control of the fraudster, who have information
[01:28:22.960 --> 01:28:33.200]   available to them, which will very dispositively answer the question of whether the alleged fact
[01:28:33.200 --> 01:28:39.680]   happened or not. As a reporter, understanding how institutions and society interact with each other,
[01:28:39.680 --> 01:28:44.480]   and the physical reality of, "Okay, if this thing happens as alleged, then these papers will be
[01:28:44.480 --> 01:28:52.240]   filed, then these API calls will be made," etc., etc. Doing the core job of reporting and finding
[01:28:52.240 --> 01:28:57.520]   people at the institutions who will tell you the truth. As an example of this, Mount Gox many years
[01:28:57.520 --> 01:29:05.920]   ago was insolvent, and that fact was widely rumored but not reported, presumably because the
[01:29:05.920 --> 01:29:15.200]   global financial news industry didn't find it convenient to have someone call into the
[01:29:15.200 --> 01:29:17.760]   Japanese banking system and ask the right questions in the right way.
[01:29:19.520 --> 01:29:25.520]   The CEO of Mount Gox alleged on Bitcoin Talk that the reason that they were not able to make
[01:29:25.520 --> 01:29:31.360]   outgoing wires was because they had caused a distributed denial-of-service attack
[01:29:31.360 --> 01:29:34.240]   on their bank's ability to send foreign currency wires.
[01:29:34.240 --> 01:29:39.120]   That bank was Mizuho. Mizuho was the second largest bank in Japan.
[01:29:39.120 --> 01:29:49.120]   Many people at, say, well-regarded financial reporting institutions in New York City
[01:29:49.520 --> 01:29:56.160]   find it incredibly exotic and difficult, and maybe in some ways kind of unknowable,
[01:29:56.160 --> 01:30:03.280]   to extract facts from Mizuho. Which, there are addresses. FedEx will deliver letters to them.
[01:30:03.280 --> 01:30:06.880]   They have phone lines. We also have fax machines. We love our fax machines.
[01:30:06.880 --> 01:30:10.560]   Like, could you send a fax to anybody at Mizuho and say, "Hey, quick question,
[01:30:10.560 --> 01:30:17.840]   are you sending wires today?" And Mizuho would receive the fax, look at it kind of quizzically,
[01:30:17.840 --> 01:30:22.400]   and say, "In response to your fax earlier, yes, we are still sending wires because we
[01:30:22.400 --> 01:30:26.160]   are the second largest bank in Japan. Do you have any other easy to answer questions for us?"
[01:30:26.160 --> 01:30:33.600]   Financial reporting dropped the ball on just asking individuals at Mizuho simple
[01:30:33.600 --> 01:30:39.120]   questions about reality. Maybe you should do that next time. To the extent that you understand,
[01:30:39.120 --> 01:30:46.400]   one, understand that the CEO is giving out gold on Bitcoin Talk under his own name,
[01:30:46.400 --> 01:30:50.640]   where these are obviously reportable statements. The statements are alleged facts about material
[01:30:50.640 --> 01:30:55.920]   reality, and maybe chase down the truth value of that. That's hard. It's so much easier to
[01:30:55.920 --> 01:31:01.520]   just repeat what he says on Twitter, and say, as said by this person on Twitter, and then quote the
[01:31:01.520 --> 01:31:07.040]   Bitcoin price feed. But reporting is hard. Why aren't short sellers doing this? Because they
[01:31:07.040 --> 01:31:10.560]   should have an economic incentive to dig to the bottom of this, right? And so we should have a
[01:31:10.560 --> 01:31:16.640]   deluge of financial information from short sellers who call the banks and trace through the API calls.
[01:31:16.640 --> 01:31:23.840]   Yeah, that is an ongoing interesting question. I think short sellers provide an enormous service
[01:31:23.840 --> 01:31:31.440]   for the world in being essentially society's best solution to financial fraud. And yet,
[01:31:31.440 --> 01:31:39.360]   they fail to detect lots of them. And not just throwing short sellers or reporters or anybody
[01:31:39.360 --> 01:31:45.280]   else under the bus. I failed to detect SPF's various craziness, despite having sufficient
[01:31:45.280 --> 01:31:48.960]   information available to me as a well-read person on the internet to have detected that.
[01:31:48.960 --> 01:31:54.080]   Like, where were the freaking wallets? Everybody assumed someone else was looking at it,
[01:31:54.080 --> 01:32:02.080]   essentially. So that's one reason. Short sellers often assume, "Okay, I need to first get put on
[01:32:02.080 --> 01:32:07.040]   the path of something and have a differentiated point of view." And then another issue for short
[01:32:07.040 --> 01:32:11.200]   sellers is you have to find an instrument and you have to find another side of the trade
[01:32:11.200 --> 01:32:17.520]   to successfully do that. And there was, without being an expert on Bitcoin micro-mechanics,
[01:32:17.520 --> 01:32:24.800]   it was difficult in size to make the trade. Mt. Gox is insolvent right now, other than pull money
[01:32:24.800 --> 01:32:29.200]   out of Mt. Gox, which people were definitely trying to do. I got a number of interesting
[01:32:29.200 --> 01:32:37.360]   business proposals in the 2012-ish timeframe from people who said, "Hey, you're an American and you
[01:32:37.360 --> 01:32:42.880]   clearly understand international banking and you live in Japan. Could I cause you to get some yen
[01:32:42.880 --> 01:32:48.240]   and have you wire that to me in America and you can take a percentage?" And I said, "I really
[01:32:48.240 --> 01:32:51.760]   don't like where this is going." And they said, "Well, there's this company and I've got some
[01:32:51.760 --> 01:32:55.120]   money over there and they can send yen, but they can't send dollars." And I'm like, "Is that because
[01:32:55.120 --> 01:32:59.040]   they don't actually have the money?" And they're like, "No, no, no. It's a Japanese banking thing."
[01:32:59.040 --> 01:33:05.200]   And I'm like, "No, it's not. Japanese banks are very good at sending wires." And they said, "No,
[01:33:05.200 --> 01:33:08.720]   no, no. It's really this thing. This is totally clean." I'm like, "You would not be having this
[01:33:08.720 --> 01:33:13.360]   conversation with me if it was totally clean. You need a money launderer. I will not be your
[01:33:13.360 --> 01:33:18.080]   money launderer." How hard is money laundering? On one hand, you mentioned earlier the steep
[01:33:18.080 --> 01:33:24.480]   capacity that banks have where every transaction is analyzed and flagged. And if it's notable
[01:33:24.480 --> 01:33:29.760]   enough, they write a report about it. How sophisticated does the cartel need to be in
[01:33:29.760 --> 01:33:36.880]   order to move around seven-figure amounts of money, let's say? So the definition of money
[01:33:36.880 --> 01:33:41.840]   laundering is extremely stretchy and toughy. And there's a spectrum of people, much like there's a
[01:33:41.840 --> 01:33:45.840]   spectrum of sophistication in financial fraud, there's a spectrum of sophistication in money
[01:33:45.840 --> 01:33:53.920]   laundering. So if you want to look at probably the most sophisticated money launderer in history,
[01:33:53.920 --> 01:33:57.280]   he's currently a guest of the U.S. government, but wherever SBF is staying.
[01:33:57.280 --> 01:33:58.480]   He was sophisticated?
[01:33:58.480 --> 01:34:03.680]   Oh, this is a disagreement I have with a lot of people. SBF was extremely sophisticated,
[01:34:03.680 --> 01:34:12.240]   like a person at not just SBF. I think people identify him uniquely, right? They identify the
[01:34:12.240 --> 01:34:17.520]   inner circle uniquely as being a faultier. There was an entire power structure there,
[01:34:17.520 --> 01:34:23.680]   which was extremely adept at figuring out how power worked in the United States and exercising
[01:34:23.680 --> 01:34:32.160]   it towards their own hands. And then it blew up. But until then, goodness, they decided we need
[01:34:32.160 --> 01:34:35.840]   regulatory licenses. They're called money transmission licenses in the United States,
[01:34:35.840 --> 01:34:39.520]   and those are done on a state-by-state basis. They got 50 regulators to sign off on it,
[01:34:39.520 --> 01:34:44.400]   et cetera, et cetera. There were many objective indicia of them being very good at their jobs
[01:34:44.400 --> 01:34:46.000]   until they lost all the money.
[01:34:46.000 --> 01:34:50.320]   But it was more about politically getting people to look the other way rather than we figured out
[01:34:50.320 --> 01:34:53.600]   how to structure the wire in a way that won't get flagged.
[01:34:53.600 --> 01:34:58.080]   It's not merely a matter of getting them to look the other way. But if you go back to the original
[01:34:58.080 --> 01:35:04.720]   SBF interviews where he's telling the founding myth of Alameda, he says very loudly, "The reason
[01:35:04.720 --> 01:35:09.280]   why I got this opportunity to do Bitcoin arbitrage between Japan and the United States is because I
[01:35:09.280 --> 01:35:12.880]   was able to do something that the rest of the world wasn't. I was able to," he doesn't say
[01:35:12.880 --> 01:35:18.000]   this in this many words, I will say it, "suborn a Japanese bank because you need that as one of
[01:35:18.000 --> 01:35:21.920]   the pieces to run this arb, and then I pulled tens of millions of dollars out of this."
[01:35:21.920 --> 01:35:26.800]   I don't think people really listened to what he was saying there. He literally says in the
[01:35:26.800 --> 01:35:30.560]   interview on Bloomberg, "If I was a compliance person, this would look like the sketchiest
[01:35:30.560 --> 01:35:36.320]   thing in the world. This looks like it's obviously money laundering," because it is money laundering.
[01:35:36.320 --> 01:35:44.640]   And then, interestingly, Michael Lewis retells this story, and he locates the story in South
[01:35:44.640 --> 01:35:48.640]   Korea rather than in Japan, and some people who were involved say, "We tried it in South Korea
[01:35:48.640 --> 01:35:52.480]   and Japan," which people would pull on more threads there. There's still lots of that story
[01:35:52.480 --> 01:35:56.880]   that we don't know. But anyhow, how sophisticated do you have to be to launder tens of billions of
[01:35:56.880 --> 01:36:02.320]   dollars around? SBF did that. That is a bar for sophistication. He was eventually caught. He was
[01:36:02.320 --> 01:36:05.760]   not caught for laundering tens of billions of dollars around. He wasn't even under suspicion
[01:36:05.760 --> 01:36:11.280]   for laundering tens of billions of dollars around. SBF was Tether's banker. Alameda research,
[01:36:11.280 --> 01:36:15.600]   one of the parts of the corporate shell game that they were playing, moved tens of billions
[01:36:15.600 --> 01:36:23.040]   of dollars of cash around the financial system, largely under full color of law, on behalf of
[01:36:23.040 --> 01:36:28.160]   Tether to move it from wherever Tether had it, goodness knows, or wherever their buying customers
[01:36:28.160 --> 01:36:33.200]   had it, to ... I think at the moment it's mostly at Cantor Fitzgerald. Some shoe has to eventually
[01:36:33.200 --> 01:36:38.480]   drop there. I will eat a lot of popcorn when it does, but be that as it may. There's many other
[01:36:38.480 --> 01:36:49.120]   ways to launder money. You can do things like ... Let's say I establish a shell corporation,
[01:36:49.120 --> 01:36:54.080]   and I buy a piece of real estate in New York City, and then I rent that real estate out to people,
[01:36:54.080 --> 01:36:59.840]   and I collect a stream of rents from that. That money looks clean, because there is an
[01:36:59.840 --> 01:37:03.440]   executable business. It's my shell corporation that is renting this real estate that really exists
[01:37:03.440 --> 01:37:08.640]   to a totally legitimate person. This money is clean. The money that I put into the system to
[01:37:08.640 --> 01:37:13.360]   buy this on behalf of the shell corporation, I'm just going to wire it to a lawyer, and the lawyer
[01:37:13.360 --> 01:37:16.560]   is going to answer any question from the bank with, "I don't know where it came from. I don't
[01:37:16.560 --> 01:37:19.360]   have to tell you. I'm a lawyer. It's a real estate transaction. What do you want from me?"
[01:37:19.360 --> 01:37:28.000]   In one sense, that's money laundering. If the original money was the proceeds of crime,
[01:37:28.000 --> 01:37:32.000]   in another sense, that's how every real estate transaction goes down at those scales.
[01:37:32.000 --> 01:37:40.640]   Often, a facility at money laundering is one facility at operating the economy,
[01:37:40.640 --> 01:37:44.240]   plus willingness to do that to hide the proceeds of some other crime.
[01:37:44.240 --> 01:37:48.640]   I think I would be really good at money laundering. I'm glad I haven't done it
[01:37:48.640 --> 01:37:55.280]   professionally, but it's fascinating intellectually. Previous communications
[01:37:55.280 --> 01:37:58.640]   departments I've worked at probably would explicitly anti-endorse that sentence.
[01:37:58.640 --> 01:38:03.200]   We are reasonably confident that you're not laundering money.
[01:38:03.200 --> 01:38:05.520]   I would be much wealthier if I was.
[01:38:05.520 --> 01:38:12.240]   Well, a separate topic, but when you look, you emphasize that people tend to undercharge
[01:38:12.240 --> 01:38:16.640]   for the products they serve. If you have identified somebody who actually does charge
[01:38:16.640 --> 01:38:21.920]   for products that they can get away with, what psychologically do they have that the rest of us
[01:38:21.920 --> 01:38:26.720]   don't? I think interestingly, here is one of those places where culture is not necessarily
[01:38:26.720 --> 01:38:33.600]   merely their term, but is actually descriptive in some ways. Without pointing fingers at particular
[01:38:33.600 --> 01:38:40.400]   examples, because it gets contentious, there are some cultures in the world that institutionally
[01:38:40.400 --> 01:38:47.200]   have adopted more of a, how do you put it, pro-capitalist, pro-mercantilist, et cetera,
[01:38:48.480 --> 01:38:53.840]   less of an ingrained skepticism regarding earning money and accumulating resources as a goal.
[01:38:53.840 --> 01:38:58.080]   There's other cultures which have an extreme ingrained skepticism about earning money and
[01:38:58.080 --> 01:39:03.360]   accumulating resources as a plausible goal. Those cultures generate people who have very
[01:39:03.360 --> 01:39:08.560]   different negotiation strategies. When you impact people with different negotiation strategies
[01:39:08.560 --> 01:39:14.000]   against the reality of a well-operated, I don't know, Google, for example, they arrive at very
[01:39:14.000 --> 01:39:21.760]   different numbers. What's the ... Is it Amy Chua who wrote "Market Dominant Minorities", a piece
[01:39:21.760 --> 01:39:32.800]   about, a book about that subject, et cetera? One of the things in that is that all people are equal
[01:39:32.800 --> 01:39:37.440]   in the eyes of God and hopefully in the eyes of the law, but not all cultures physically make the
[01:39:37.440 --> 01:39:42.160]   same decisions with regards to the same facts on the ground, and that causes some disparity
[01:39:42.160 --> 01:39:48.720]   outcomes. That is one tiny part of that thesis, which I don't ... I've read a lot of books. I
[01:39:48.720 --> 01:39:52.320]   don't necessarily endorse every word in every book that I read, but I think there is something
[01:39:52.320 --> 01:40:02.320]   to that. I think another thing is that there's a certain personality type cluster, I guess you
[01:40:02.320 --> 01:40:09.680]   would say, of people that got into tech. Many of us ... It is over-advanced that the tech industry
[01:40:09.680 --> 01:40:13.360]   and the pathologies of the tech industry are caused by the nerd versus jock distinction in
[01:40:13.360 --> 01:40:21.120]   American high schools. Heavily over-advanced. Amount of truth to that? Not zero. Many of us
[01:40:21.120 --> 01:40:27.840]   ... We came up, we feel we were largely getting beaten down by the system around us, we were not
[01:40:27.840 --> 01:40:33.920]   worthy, yada, yada, yada, and then we have carried those issues into our professional lives. Some
[01:40:33.920 --> 01:40:40.640]   people work their way out of it quickly, some do not. Some people work ... For class and et cetera
[01:40:40.640 --> 01:40:44.960]   reasons, go to institutions like Stanford and hear from ... I don't know who you hear things from if
[01:40:44.960 --> 01:40:48.480]   you go to Stanford, because I certainly didn't go there, but I don't know, an elder fraternity
[01:40:48.480 --> 01:40:52.240]   brother that says, "Yo, bro, this is the way the world works. You really got to negotiate when you
[01:40:52.240 --> 01:40:57.760]   get in your discussion with Google. I've talked to so many brothers and they don't negotiate,
[01:40:57.760 --> 01:41:01.280]   and so the ones that do make a whole lot more money." You're like, "Wow, good for that." Most
[01:41:01.280 --> 01:41:05.120]   people don't get that talk from their elder fraternity brother because they do not go to
[01:41:05.120 --> 01:41:10.480]   Stanford or don't have an elder fraternity brother. Until Vaccinate CA, the most important
[01:41:10.480 --> 01:41:15.600]   thing I've probably done professionally was writing a piece on the internet about salary
[01:41:15.600 --> 01:41:21.440]   negotiation that gets subtitled, "Make more money and be more valued," which is just an exhortation
[01:41:21.440 --> 01:41:26.800]   for descriptively mostly young people who had some of the issues I had when I was young and
[01:41:26.800 --> 01:41:32.480]   growing up. "Hey, you're allowed to negotiate. That's not a moral failing if you have no less
[01:41:32.480 --> 01:41:38.640]   right to the marginal dollar than a company has to the marginal dollar. Go get it." Then you can
[01:41:38.640 --> 01:41:45.920]   put it towards all sorts of interesting ends. 500,000 people a year read that piece, and it's
[01:41:45.920 --> 01:41:53.360]   now 12 plus years old. I keep a folder in Gmail about who has written and said, "I got $25,000
[01:41:53.360 --> 01:41:58.960]   to $100,000 per year more as a result of reading this piece." I used to keep a spreadsheet. I
[01:41:58.960 --> 01:42:02.800]   stopped keeping the spreadsheet after it ticked into the eight figures. Keeping the spreadsheet
[01:42:02.800 --> 01:42:05.440]   was just an ongoing source of stress for me. Eight figures?
[01:42:05.440 --> 01:42:11.200]   Yeah, per year. You would assume that 500,000 people read it per year. Some take the advice.
[01:42:11.200 --> 01:42:14.160]   Most who take the advice probably don't write me to say, "Hey, I took the advice. Thank you."
[01:42:14.160 --> 01:42:19.520]   Maybe I miss some emails, yada, yada. The true economic impact is probably larger than that.
[01:42:20.880 --> 01:42:24.560]   There are probably people who have the inverse of that spreadsheet, where it's like, "Darn it.
[01:42:24.560 --> 01:42:31.920]   We got quoted a petty 11 against us again." Because we've got these numbers, and there's
[01:42:31.920 --> 01:42:38.880]   only a few firms in the tech industry that do scaled hiring. Anyhow.
[01:42:38.880 --> 01:42:46.640]   Okay. There's less people today who are in their 20s who had prominent software businesses than
[01:42:46.640 --> 01:42:52.480]   maybe 10 or 20 years ago, having been in the software industry last 10, I don't know how,
[01:42:52.480 --> 01:42:58.080]   when you started exactly. Is your sense that this is because the nature of software businesses has
[01:42:58.080 --> 01:43:02.400]   changed? Or is it because the 20-year-olds today are just less good?
[01:43:02.400 --> 01:43:07.520]   I've met many young and talented people over the course of 20 years in the software industry. Young
[01:43:07.520 --> 01:43:11.760]   and talented people continue being young and talented. I think one thing that is partially
[01:43:11.760 --> 01:43:18.320]   explanatory is that when there's a new frontier that opens up, the existing incumbents, both in
[01:43:18.320 --> 01:43:22.640]   terms of institutions and in terms of people with deep professional networks and personal resources
[01:43:22.640 --> 01:43:28.560]   and similar, do not immediately grab all the value in that new thing. It's Terra Nova.
[01:43:28.560 --> 01:43:32.640]   To the extent that tech is no longer Terra Nova, I think you would expect
[01:43:32.640 --> 01:43:38.400]   less people who are less resourced, who are younger, etc., to rise to the heights of prominence
[01:43:38.400 --> 01:43:43.600]   in tech. To be clear, I'm not at heights of prominence in tech. When I ran companies,
[01:43:43.600 --> 01:43:47.840]   I was not running companies like some other guest's podcast-run companies. It was Bingo
[01:43:47.840 --> 01:43:51.200]   Card Creator. I was making bingo cards for elementary school teachers while living next
[01:43:51.200 --> 01:43:59.760]   to a rice paddy in central Japan. That's my dominant hypothesis. There are some things
[01:43:59.760 --> 01:44:03.920]   that are affecting the youth that I think are negative. I think some products that the tech
[01:44:03.920 --> 01:44:08.880]   industry has created do not maximize for the happiness or productivity of people that consume
[01:44:08.880 --> 01:44:17.360]   those products, TikTok, etc., but I continue to be bullish about the youth. I have two children who,
[01:44:17.360 --> 01:44:24.000]   knock on wood, will accomplish things in their lives. I'm intrinsically skeptical about,
[01:44:24.000 --> 01:44:27.440]   "Oh, the kids these days, they're just bad kids." How much do you worry about
[01:44:29.440 --> 01:44:36.240]   video games as a sort of wireheading that somebody like you, 20, 30 years ago,
[01:44:36.240 --> 01:44:42.960]   or 20, sorry, now, has access to Factorio and will just wirehead themselves to that instead of
[01:44:42.960 --> 01:44:47.360]   making a really cool software product? How much should we see this in the productivity numbers?
[01:44:47.360 --> 01:44:52.080]   Oh, goodness. I don't know about the productivity numbers. Generally, I do know that Steam keeps
[01:44:52.080 --> 01:44:58.080]   the counter of how much I am playing video games in a year time. Knock on wood, I've accomplished
[01:44:58.080 --> 01:45:04.400]   a few things in my career. Also, against that, what was my Steam counter up to?
[01:45:04.400 --> 01:45:09.520]   Steam didn't include World of Warcraft. World of Warcraft was at least 1,000 hours. For me,
[01:45:09.520 --> 01:45:15.280]   Factorio recently was 750. Then if you sum over 20 years, I've probably played video games for
[01:45:15.280 --> 01:45:24.960]   4,000, 6,000 hours. That's two to three years of professional effort if one thinks that it
[01:45:24.960 --> 01:45:30.160]   trades off directly with professional effort. Do you? Because if you then include every single
[01:45:30.160 --> 01:45:37.600]   young guy who's a nerd, how much should we worry that a bunch of their productive time is going to
[01:45:37.600 --> 01:45:42.560]   video games instead of making the next software business? I worry at least a little bit about it
[01:45:42.560 --> 01:45:47.840]   for myself. I recently started working for an executive assistant. One of the first suggestions
[01:45:47.840 --> 01:45:53.360]   that he gave was, "Hey, Patrick, will you friend me on Steam so that I can see how much you're
[01:45:53.360 --> 01:45:57.120]   playing in any given week? If you're not making your priorities happen, we can have an honest
[01:45:57.120 --> 01:46:02.640]   discussion about priorities." I said, "That's really good advice, given that I spent far too
[01:46:02.640 --> 01:46:06.560]   much time rat-holing at Factorio relative to my true preferences." You've got a really confident
[01:46:06.560 --> 01:46:16.320]   DA. Go, Sammy, go. "Hey, boss, can we have an honest conversation tonight?" I don't know if
[01:46:16.320 --> 01:46:20.320]   I'm inserting those words into his mouth, but the suggestion was genuinely his. He heard,
[01:46:21.200 --> 01:46:25.520]   "On the one hand, Factorio, wonderful game. I actually think Factorio matters far more to
[01:46:25.520 --> 01:46:28.480]   the world than most video games do, but that's an entirely different piece that I'm trying to
[01:46:28.480 --> 01:46:31.280]   write at the moment. Be that as it may." Have you read Byrne's piece on this?
[01:46:31.280 --> 01:46:36.800]   The Factorio mindset? Yes. I love many things Byrne writes. I think I luckily have a
[01:46:36.800 --> 01:46:40.080]   differentiated point of view on this one, so I will hope to get it out to the internet someday.
[01:46:40.080 --> 01:46:46.880]   On the one hand, I loved it. I think Factorio, space exploration mod specifically, was the best
[01:46:46.880 --> 01:46:52.800]   video game I ever played, and yet I spent 750 hours on that over the course of a year. I was
[01:46:52.800 --> 01:46:58.000]   on sabbatical and recharging from six very hard-charging years at Stripe and also
[01:46:58.000 --> 01:47:01.840]   running the United States vaccination location information effort.
[01:47:01.840 --> 01:47:09.040]   There's a question of, at relevant margins, are you maximizing for your true values? I was a
[01:47:09.040 --> 01:47:14.160]   little worried about that, and so now my EA checks on me. Do I worry about it for other people?
[01:47:15.920 --> 01:47:20.080]   I will say that when I was young, and a World of Warcraft Raid Guild leader,
[01:47:20.080 --> 01:47:24.880]   who spent a thousand hours on that, that it was a substitute advancement ladder for me.
[01:47:24.880 --> 01:47:30.160]   The actual job I was working, sailing around in central Japan, gave me no scope of control
[01:47:30.160 --> 01:47:34.640]   over things. I thought if I was a startup CEO, I could make decisions right now. I could build
[01:47:34.640 --> 01:47:38.080]   something awesome, but I don't have that ability, and I don't see myself as being the kind of person
[01:47:38.080 --> 01:47:45.280]   who could become a startup CEO. If I can't be satisfied in my nine to five in terms of making
[01:47:45.280 --> 01:47:49.760]   things happen in the world, at the very least, I can make sure we kill the dragon in two hours.
[01:47:49.760 --> 01:47:54.000]   Don't stand in the fire. Come on, team leaders, you need to make sure that people are equipped
[01:47:54.000 --> 01:47:59.680]   with the resist gear before they get there. Oh, we're having internal spats about allocation of
[01:47:59.680 --> 01:48:05.440]   resources. We need to have a better DKP system. By the way, World of Warcraft Raid Guilds and all
[01:48:05.440 --> 01:48:11.440]   other places where intellectual effort comes together in the video game community are much
[01:48:11.440 --> 01:48:19.680]   more sophisticated than people give them credit for. When I started VaccinateCA, I told people my
[01:48:19.680 --> 01:48:25.280]   sole prior leadership experience is having 60 direct reports in a raid guild, and that's true.
[01:48:25.280 --> 01:48:31.280]   We don't rat hole on the subject, but there are parts of VaccinateCA that are very,
[01:48:31.280 --> 01:48:37.520]   very definitely downstream of the intellectual efforts about managing raid guilds specifically.
[01:48:38.160 --> 01:48:42.000]   There are multiple people internally who are like, "Yep, we are running the raid guild playbook right
[01:48:42.000 --> 01:48:50.480]   now," but be that as it may, you can do something with your life. Choose to do something with your
[01:48:50.480 --> 01:48:54.480]   life, and then if you want to play a reasonable amount of video games, then play a reasonable
[01:48:54.480 --> 01:49:00.720]   amount of video games. With respect to individual people, I sometimes worry that you get into ... I
[01:49:00.720 --> 01:49:04.720]   think I've struggled with depression at some points in my life. Many people struggle with
[01:49:04.720 --> 01:49:10.800]   underdiagnosed, undertreated depression. I think sometimes you get into a self-destructed spiral
[01:49:10.800 --> 01:49:20.720]   measured against your true values and preferences, where due to depression and other factors,
[01:49:20.720 --> 01:49:28.320]   you aren't making as much progress on the true goals you do. You use video games as an escape
[01:49:28.320 --> 01:49:32.960]   from that, and not just video games, books, television, etc., etc. There are many poisons
[01:49:32.960 --> 01:49:38.320]   available in life. You pick your poison, use that to escape. The amount of effort you put into the
[01:49:38.320 --> 01:49:43.680]   poison causes you to have less effort available to do the thing, so you get less good results
[01:49:43.680 --> 01:49:49.200]   in the thing. Helping people out of those self-destructed spirals is something that we,
[01:49:49.200 --> 01:49:54.320]   as a society, could stand to get much better at. I was speaking of Bird, by the way. I noticed that
[01:49:54.320 --> 01:50:00.080]   a bunch of my favorite writers are finance writers. There's you, Byrne, Matt Levine.
[01:50:00.960 --> 01:50:04.320]   Is there some reason why finance has hogged all the writing talent?
[01:50:04.320 --> 01:50:07.840]   I think there are many good writers in the world. Derek Thompson, for example,
[01:50:07.840 --> 01:50:14.080]   he's a chemical engineer, and he's written some things which I barely understand. I have enough
[01:50:14.080 --> 01:50:17.680]   of an engineering degree to appreciate half of the chemistry, and I can't appreciate
[01:50:17.680 --> 01:50:24.560]   the full totality of why uranium hexafluoride or whatever is a terrible substance to work with.
[01:50:24.560 --> 01:50:27.280]   He has some excellent writing on why that is a terrible substance to work with.
[01:50:27.280 --> 01:50:30.480]   The broader question being, why does finance have a greater concentration
[01:50:30.480 --> 01:50:37.280]   of writing talent? Not just about current bloggers, but finance histories are some of the
[01:50:37.280 --> 01:50:44.000]   best history books out there. The Bethany McLean's and so forth. I'm curious why this is. Is it just
[01:50:44.000 --> 01:50:48.800]   an intrinsically more interesting subject? There's some path dependence. If I had ended up
[01:50:48.800 --> 01:50:52.240]   working in a water treatment plant, I'd be writing about water treatment plants because I like
[01:50:52.240 --> 01:50:58.320]   writing, and I am positive. I know enough about myself to know that a discussion about how alum
[01:50:58.320 --> 01:51:02.400]   works in water treatment plants, which is something I read when I was like six, that could
[01:51:02.400 --> 01:51:06.160]   totally captivate me for multiple years on end, and I would write about that if I was captivated
[01:51:06.160 --> 01:51:11.680]   by it. I agree with the point Matt Levine made once, which is that finance and the tech industry
[01:51:11.680 --> 01:51:17.360]   have for a while been a relatively reliable way to turn intelligence into money.
[01:51:18.640 --> 01:51:23.200]   Many good writers are very intelligent. Not all people who are very intelligent are good writers.
[01:51:23.200 --> 01:51:28.400]   I think it is a skill that more intelligent people could learn, but simply if we create
[01:51:28.400 --> 01:51:35.840]   an incentive system which will tend to allocate not yours truly, but descriptively, like a lot of
[01:51:35.840 --> 01:51:41.280]   the country's top brains into particular fields where they will become experts at those particular
[01:51:41.280 --> 01:51:45.520]   fields, I would expect also a lot of the writing talent to be there because good writing is good
[01:51:45.520 --> 01:51:48.960]   thinking. That's a Paul Graham quote, I think, but that is broadly true.
[01:51:48.960 --> 01:51:54.080]   Speaking of which, so you went to Japan because, if I'm remembering this correctly,
[01:51:54.080 --> 01:52:01.360]   you thought that after the dot-com boom that the programmer alone demand will diminish,
[01:52:01.360 --> 01:52:06.320]   and it will require some combination of skills. You weren't the only one. There were many other
[01:52:06.320 --> 01:52:08.400]   people who thought this way. The Wall Street Journal said it,
[01:52:08.400 --> 01:52:11.840]   and the Wall Street Journal had never been wrong in my experience as a 19-year-old
[01:52:11.840 --> 01:52:15.920]   who didn't know a thing about anything. But separate from the object-level predictions
[01:52:15.920 --> 01:52:20.320]   that you might have gotten wrong about the software industry, at a meta-level,
[01:52:20.320 --> 01:52:24.960]   what was a mistake you made? How would you characterize that?
[01:52:24.960 --> 01:52:31.600]   Sure. So I had a whole lot of rigor chasing a decision that had no basis in fact. So
[01:52:31.600 --> 01:52:35.440]   believing in the Wall Street Journal that all future engineers would be hired in places like
[01:52:35.440 --> 01:52:39.040]   India and China and not the United States, and so there would be no future engineering
[01:52:39.040 --> 01:52:42.640]   employment in the United States. I made a spreadsheet of like, "Okay, here are the
[01:52:42.640 --> 01:52:48.080]   languages my university teaches. Here's my best estimate of the number of people in that country
[01:52:48.080 --> 01:52:54.640]   who speak it, Americans who speak it, the amount of their software that gets sold here, amount of
[01:52:54.640 --> 01:52:58.880]   US software that gets sold there, blah, blah, blah, blah, multiply these together, sort by column H
[01:52:58.880 --> 01:53:04.640]   descending." And this is like larping and having rigor here, but it felt like a good decision-making
[01:53:04.640 --> 01:53:10.720]   process to me at the time. Now, the meta discussion is like, "Don't larp it having rigor.
[01:53:10.720 --> 01:53:13.360]   Actually have rigor." But what would that look like in this context?
[01:53:13.360 --> 01:53:17.600]   What would you have done differently? Ultimately happy with the decision I made,
[01:53:17.600 --> 01:53:24.880]   although as presented, it's the wrong decision. I'm happy about it for other factors about life.
[01:53:24.880 --> 01:53:30.400]   Working in Japan as a young engineer, there are some very rough parts about that. But
[01:53:31.600 --> 01:53:36.800]   on a meta level, maybe this is an early opportunity to trust institutions less than trust
[01:53:36.800 --> 01:53:41.840]   systemically viable reasoning patterns more. Like, okay, The Wall Street Journal has asserted,
[01:53:41.840 --> 01:53:44.800]   assuming I'm remembering this article right, and I'm pretty sure I am because it was a
[01:53:44.800 --> 01:53:48.640]   flexion point in life for me. The Wall Street Journal has asserted that no future Americans
[01:53:48.640 --> 01:53:53.040]   will get hired at software companies. Assume this is true. What happens in the American software
[01:53:53.040 --> 01:54:01.520]   industry? And maybe I did not have enough knowledge to confidently predict that at the moment, but I
[01:54:01.520 --> 01:54:06.000]   can confidently predict some things now. Software companies are going to start to break as older
[01:54:06.000 --> 01:54:09.840]   engineers age out of the engineering population, and they have no one coming up to replace them,
[01:54:09.840 --> 01:54:16.400]   et cetera. The industry institutionally must hire people every year, and hiring freezes are
[01:54:16.400 --> 01:54:22.800]   necessarily temporary as a result of that. That's as close to a law of physics as one can have.
[01:54:22.800 --> 01:54:29.920]   So if you are telling me something which says the law of physics has been suspended and will
[01:54:29.920 --> 01:54:36.400]   be in the future, I don't agree to that. How would I learn the law of physics? I did not know anyone
[01:54:36.400 --> 01:54:39.600]   in software engineering at the time, which is partly why I was getting my advice from The Wall
[01:54:39.600 --> 01:54:46.080]   Street Journal, but if I had been slightly more agentic about it, I was at a research university
[01:54:46.080 --> 01:54:50.160]   in the United States. I could have found someone who knew someone who was in the software industry
[01:54:50.160 --> 01:54:54.240]   to explain this to me. Couldn't you use that same logic to say that the journalism jobs won't go
[01:54:54.240 --> 01:54:58.640]   down because senior journalists will have to be replaced by new journalists? And that's true,
[01:54:58.640 --> 01:55:02.560]   but that doesn't take that many people to study journalism, and it actually would have been a bad
[01:55:02.560 --> 01:55:08.160]   call to major in journalism and pursue that as a career. But the fundamental thesis was that
[01:55:08.160 --> 01:55:12.080]   for journalism is that the total size of the pie is decreasing in journalism due to structural
[01:55:12.080 --> 01:55:18.320]   factors, and The Wall Street Journal's thesis was not simply that the size of the pie would
[01:55:18.320 --> 01:55:22.960]   decrease in the wake of the dot-com bust, which I don't think they even got as far as articulating
[01:55:22.960 --> 01:55:28.080]   that, although it's been 25 years, but was more like, "Oh, companies will maximize for
[01:55:28.080 --> 01:55:30.720]   cost of labor, et cetera, and therefore ship out all the jobs."
[01:55:30.720 --> 01:55:38.960]   I see. I see. You've emphasized that founders should do more A/B testing. That's one of the
[01:55:38.960 --> 01:55:45.280]   main themes of your blog, if you go back through it. So they're under-optimizing on this. What do
[01:55:45.280 --> 01:55:48.560]   they tend to over-optimize on? So interestingly, I was the marketing
[01:55:48.560 --> 01:55:52.720]   engineer earlier in my career and really thought that that was important. And in terms of high-level
[01:55:52.720 --> 01:55:56.640]   advice I would give a founder, I would probably tell them a little bit about marketing engineering,
[01:55:56.640 --> 01:56:00.880]   but wouldn't spend 95% of my time talking about that unless that was explicitly what they brought
[01:56:00.880 --> 01:56:08.560]   me in on, but founders spend too much time on. Playing house and chasing status are both two
[01:56:08.560 --> 01:56:13.760]   sort of like well-known pitfalls where you can get addicted, which is not quite the right word.
[01:56:13.760 --> 01:56:20.400]   Your incentives will draw you into playing the role of the CEO of a successful company
[01:56:22.000 --> 01:56:29.200]   before your actions have earned the company its level of success. The fundamental nature of early
[01:56:29.200 --> 01:56:33.280]   stage businesses is that investment in you is not a vindication of what you have achieved.
[01:56:33.280 --> 01:56:38.160]   It's an advance on your future accomplishments and you need to rigorously pursue actually making
[01:56:38.160 --> 01:56:43.600]   those future accomplishments. And there's many ways to rigorously pursue it. Talk to more users,
[01:56:43.600 --> 01:56:47.200]   write more software, make something excellent, get more people to use it,
[01:56:47.200 --> 01:56:54.400]   get better at selling it, etc. etc. That's an important strain of Silicon Valley culture. I'm
[01:56:54.400 --> 01:56:58.000]   glad we have it. I'm glad we are popularizing it to as many places in the world, including to
[01:56:58.000 --> 01:57:05.760]   me in central Japan. And yeah, you know, there are always other games that are going on and those
[01:57:05.760 --> 01:57:11.760]   other games are less important, but they are very, very attractive games. Not video games here, but
[01:57:11.760 --> 01:57:15.760]   why don't you go to a conference? Why don't you meet more interesting people? Why don't you
[01:57:15.760 --> 01:57:20.960]   show up at the best parties, etc. etc. Showing up at the best parties does not, at most margins,
[01:57:20.960 --> 01:57:26.320]   increase the number of users you talk to. It doesn't write functioning code. Do those things
[01:57:26.320 --> 01:57:33.760]   that actually matter. And some distractions proudly wave, "I'm a distraction from everything
[01:57:33.760 --> 01:57:38.640]   that matters." And some don't. Some definitely say, "I'm real work. I definitely feel like real
[01:57:38.640 --> 01:57:45.200]   work." And they're just not. And so don't do the things that don't matter, which sounds vacuous,
[01:57:45.200 --> 01:57:51.760]   and yet. Okay. I want to go back to VaccinateCA and I want to close the loop on one question that
[01:57:51.760 --> 01:57:58.720]   I had that I don't know if we got a good answer to that I think is important, which was, suppose
[01:57:58.720 --> 01:58:02.320]   you're the president of the United States, maybe you're a new one. So this one's been replaced and
[01:58:02.320 --> 01:58:07.360]   you're looking back on what happened. If you personally were president, what is it that you're
[01:58:07.360 --> 01:58:13.200]   doing to bring accountability to what happened and making sure that in a future crisis, which
[01:58:13.200 --> 01:58:19.200]   might look different than COVID, things are ready to snuff? Maybe you fire the right people, but
[01:58:19.200 --> 01:58:23.120]   beyond that, there's a lot of different things that could go wrong. How do you make sure that
[01:58:23.120 --> 01:58:27.440]   we're ready for them? One cultural practice of the tech industry that I think it would be more
[01:58:27.440 --> 01:58:36.320]   salubrious for the broader civil society to adopt is the concept of blameless postmortems.
[01:58:36.320 --> 01:58:42.560]   We talked earlier about who to blame for various failures, et cetera, et cetera. I broadly believe
[01:58:42.560 --> 01:58:50.480]   that there is some amount of blame which performs useful societal purposes. And then beyond that,
[01:58:50.480 --> 01:58:56.480]   the diminishing returns set in pretty quickly. The magic word in Washington is accountability.
[01:58:56.480 --> 01:59:00.480]   We want accountability for failures, et cetera, et cetera. People are terrified of accountability.
[01:59:00.480 --> 01:59:07.920]   And that causes there to be fields of distortion around things that actually happened,
[01:59:07.920 --> 01:59:10.640]   mistakes that were actually made, opportunities that were not pursued,
[01:59:12.000 --> 01:59:19.680]   and similar. And so changing our general practice about how we accomplish accountability towards the
[01:59:19.680 --> 01:59:24.560]   direction of, okay, first, let's get a dispassionate record of what actually happened.
[01:59:24.560 --> 01:59:30.080]   It's less important that it was a disnamed official. It's less important
[01:59:30.080 --> 01:59:36.320]   that it was under this legislative authority, et cetera, et cetera. What did we do? Okay.
[01:59:36.320 --> 01:59:42.960]   Now, how did what we do lead to the outcome that we got? Given that what we did led to this outcome,
[01:59:42.960 --> 01:59:52.800]   what could we have done better? Okay. Given that there are these things that we could do better,
[01:59:52.800 --> 01:59:58.320]   how do we inject that back into the currently running system such that the next time this
[01:59:58.320 --> 02:00:03.200]   happens, we don't do the mistake again? Sometimes that will involve someone losing
[02:00:03.200 --> 02:00:09.360]   their employment, although hopefully not that frequently. A non-zero amount, that is important
[02:00:09.360 --> 02:00:14.800]   to say. But there's many things that we would post-mortem of the experience. I don't know if
[02:00:14.800 --> 02:00:21.840]   we have several thousand hours of time to go over all of them. There should be an inquiry. It's an
[02:00:21.840 --> 02:00:29.120]   easy thing to say. Let's set a minimum. Ask all involved parties, "Hey, can you write down the
[02:00:29.120 --> 02:00:34.640]   history of the COVID experience?" Just passionately. Record dates, times, actions taken.
[02:00:34.640 --> 02:00:40.880]   We want it to be truthful and comprehensive and highlight what you think is the most important
[02:00:40.880 --> 02:00:47.760]   part about this. That is step one. Maybe we ask for step one and then try to get to step two.
[02:00:47.760 --> 02:00:50.160]   Yeah. Final question. What are you going to work on next?
[02:00:50.160 --> 02:00:55.920]   I don't exactly know what will be my next big professional splash-in. I've been on
[02:00:55.920 --> 02:01:00.480]   semi-sabbatical this year. I've been writing bits about money, but bouncing between 20 and
[02:01:00.480 --> 02:01:06.000]   optimistically 80% productive relative to what 100% productive looks like to me.
[02:01:06.000 --> 02:01:12.000]   I might do a software company next. I might raise a small VC fund. Not entirely decided.
[02:01:12.000 --> 02:01:17.520]   Might do something entirely different at the moment, focusing on family-oriented things.
[02:01:17.520 --> 02:01:24.320]   Our family immigrated from Japan to the United States and are going through all the fund
[02:01:24.320 --> 02:01:30.240]   adjustment issues. Partly, I focused very much on career and other things over the course of the
[02:01:30.240 --> 02:01:34.720]   last eight years. I'm rebalancing that to help them on this adjustment and then figure out
[02:01:34.720 --> 02:01:38.960]   whatever happens for the next chapter of life. Excellent. Patrick, thanks so much for coming
[02:01:38.960 --> 02:01:43.360]   on the podcast. Thanks very much for having me. Hey, everybody. I hope you enjoyed that episode
[02:01:43.360 --> 02:01:49.200]   with Patrick McKenzie. As always, if you did, the most helpful thing you can do is just share it,
[02:01:49.200 --> 02:01:53.760]   send it to your friends, group chats, Twitter, wherever else. It's also helpful if you can leave
[02:01:53.760 --> 02:02:00.160]   reviews and ratings on Apple Podcasts, Spotify, YouTube, wherever else you listen. And of course,
[02:02:00.160 --> 02:02:04.640]   as you can tell, I'm doing ads on the podcast now. So if you're interested in advertising,
[02:02:04.640 --> 02:02:17.120]   go to DwarkeshPatel.com/advertise. Other than that, I'll see you on the next one. Cheers.

