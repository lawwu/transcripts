
[00:00:00.000 --> 00:00:09.800]   This is an example from the Go emotions data set, which is comments from Reddit that have been rated as containing some of 28 possible different emotions.
[00:00:09.800 --> 00:00:16.080]   Before I start, I want to note this data comes from Reddit, so you might see some words that aren't quite PG.
[00:00:16.080 --> 00:00:21.840]   So we have for each row here, we have the text.
[00:00:21.840 --> 00:00:28.080]   That's the comment, the ID of that comment, the author, the subreddit it comes from.
[00:00:28.360 --> 00:00:38.400]   Some other linking information, the ID of the human rater, and then 28 different emotions like admiration, amusement, anger, caring, etc.
[00:00:38.400 --> 00:00:43.400]   And for each of those, we have a one if that emotion was present and a zero otherwise.
[00:00:43.400 --> 00:00:46.760]   And neutral is a very popular emotion.
[00:00:46.760 --> 00:00:50.600]   And this is a multi-label classification problem.
[00:00:50.600 --> 00:00:54.800]   You'll see we have one hundred and five thousand six hundred and twelve rows total.
[00:00:55.000 --> 00:00:58.840]   This is a sample of 50 percent of the training data.
[00:00:58.840 --> 00:01:01.440]   So what are some ways we can explore this data?
[00:01:01.440 --> 00:01:06.480]   I'd like to see what are some expressions of gratitude in this data.
[00:01:06.480 --> 00:01:14.520]   So I can type row gratitude equals one and then hit apply on my filter.
[00:01:14.520 --> 00:01:22.520]   And I'll see that now we have five thousand eight hundred rows and I can look at some text that contains gratitude.
[00:01:23.120 --> 00:01:26.040]   And now let's say I want to modify this.
[00:01:26.040 --> 00:01:33.160]   I want to see a combination of gratitude and let's say surprise.
[00:01:33.160 --> 00:01:41.840]   And here I'll add surprise equals one and we'll see that this went down to 64 rows.
[00:01:41.840 --> 00:01:46.520]   And here we have some grateful and surprising comments like, OMG, that's awesome.
[00:01:46.520 --> 00:01:47.360]   Thank you so much.
[00:01:47.360 --> 00:01:51.160]   And you'll see that I can do this interactively.
[00:01:51.160 --> 00:01:52.440]   I don't need to rerun a script.
[00:01:52.440 --> 00:01:57.320]   I don't need to log to any files and the state will be saved in my browser.
[00:01:57.320 --> 00:02:03.600]   Another question I might want to answer right away is what is the distribution of subreddits in my data?
[00:02:03.600 --> 00:02:05.880]   Which ones are more or less represented?
[00:02:05.880 --> 00:02:12.960]   So I can group by the subreddit field and I can see some examples of comments that come from each subreddit.
[00:02:12.960 --> 00:02:18.800]   And here I can insert a column and count up the comments from each subreddit.
[00:02:19.920 --> 00:02:25.520]   And then I can see which ones are the least frequent.
[00:02:25.520 --> 00:02:28.440]   Looks like Far Cry only has 49 comments.
[00:02:28.440 --> 00:02:31.040]   And then which ones are the most frequent?
[00:02:31.040 --> 00:02:38.080]   Looks like Cringe, Love After Lockup and Social Anxiety all have around 450 comments.
[00:02:38.080 --> 00:02:39.520]   I can see examples.
[00:02:39.520 --> 00:02:41.120]   I can see some of the authors here.
[00:02:41.120 --> 00:02:44.480]   Now, what are some ways we can explore this data?
[00:02:44.480 --> 00:02:48.680]   One thing I might want to look at is the representation of a particular emotion.
[00:02:48.960 --> 00:02:50.400]   So let's say caring.
[00:02:50.400 --> 00:02:56.600]   Let's say I want to sum up all the comments ranked as caring in these subreddits.
[00:02:56.600 --> 00:03:02.120]   And let's say I want to divide by the total number of comments.
[00:03:02.120 --> 00:03:04.440]   So we normalize for the frequency.
[00:03:04.440 --> 00:03:13.480]   And then I can get that custom metric without rewriting any code, rerunning any scripts.
[00:03:14.040 --> 00:03:23.240]   And I can sort by that to see that as a fraction of the comments, we have the most caring, as rated by humans in this particular sample.
[00:03:23.240 --> 00:03:27.920]   We have Suicide Watch, Relationship Advice, BPD Loved Ones, Divorce.
[00:03:27.920 --> 00:03:30.720]   And of course, I would dig into this data more.
[00:03:30.720 --> 00:03:32.000]   I would look at the actual text.
[00:03:32.000 --> 00:03:34.880]   And this is only a starting point for my analysis.
[00:03:34.880 --> 00:03:39.800]   Another thing I could do is make this expression more complex.
[00:03:39.800 --> 00:03:42.120]   And again, I don't need to rerun any code.
[00:03:42.520 --> 00:03:47.880]   Let's say I want to look at the ratio of something like excitement.
[00:03:47.880 --> 00:03:50.600]   And I'll show you a more complex one here.
[00:03:50.600 --> 00:03:53.920]   I'll say excitement.sum.
[00:03:53.920 --> 00:03:57.000]   And we'll add a 1 for smoothing.
[00:03:57.000 --> 00:04:01.360]   And then we'll divide by a different emotion.
[00:04:01.360 --> 00:04:03.280]   Let's say gratitude.
[00:04:03.280 --> 00:04:10.360]   And also smooth here in the denominator in case we get zeros.
[00:04:11.440 --> 00:04:15.160]   And we don't need to change this to anything in particular.
[00:04:15.160 --> 00:04:16.160]   It'll recompute.
[00:04:16.160 --> 00:04:21.760]   And now we'll see where we have the most excitement relative to gratitude.
[00:04:21.760 --> 00:04:27.560]   And where do we have the most gratitude relative to excitement?
[00:04:27.560 --> 00:04:28.560]   Legal advice.
[00:04:28.560 --> 00:04:32.840]   And this is a type of exploratory analysis you might run.
[00:04:32.840 --> 00:04:37.200]   And then you can always reset and run more analyses.
[00:04:39.920 --> 00:04:43.800]   I want to show how easy it is to get that data into Weights & Biases.
[00:04:43.800 --> 00:04:46.800]   Here I install datasets from Hugging Face.
[00:04:46.800 --> 00:04:49.760]   I install Weights & Biases into my Colab.
[00:04:49.760 --> 00:04:53.760]   I import the load dataset function and Weights & Biases.
[00:04:53.760 --> 00:04:57.320]   I then load the GoEmotions dataset, the raw form.
[00:04:57.320 --> 00:05:01.800]   I take the train subset of that and convert it to a pandas dataframe.
[00:05:01.800 --> 00:05:06.680]   I subsample that dataframe to get 50% of the data.
[00:05:07.080 --> 00:05:10.840]   I initialize a run to my Weights & Biases project called emotions.
[00:05:10.840 --> 00:05:13.160]   And I name that run sample50%.
[00:05:13.160 --> 00:05:18.760]   And then to create the table, I simply pass dataframe equals with my sample,
[00:05:18.760 --> 00:05:25.320]   a wandb.table wrapper, and then I log that table under the key sample50% train.
[00:05:25.320 --> 00:05:26.520]   And that's it.
[00:05:26.520 --> 00:05:34.120]   Here you can see my really quick notes in a report on the GoEmotions Reddit dataset.
[00:05:34.720 --> 00:05:38.640]   I take notes on how many items are in each split.
[00:05:38.640 --> 00:05:42.600]   I take notes on some of the challenges and next steps.
[00:05:42.600 --> 00:05:50.000]   Here's sample code to load the data in with pandas, a few observations and queries to try.
[00:05:50.000 --> 00:05:56.440]   And here I've embedded a 5% sample of the data, 10,000 rows.
[00:05:56.440 --> 00:05:59.200]   I have a 20% sample.
[00:05:59.200 --> 00:06:04.520]   And here I've already added columns to show the ratio of approval to disapproval.
[00:06:05.040 --> 00:06:06.880]   And some of these subreddits.
[00:06:06.880 --> 00:06:10.200]   And then I have additional resources.
[00:06:10.200 --> 00:06:14.320]   And if you'd like to get started and play with the data yourself, there's a link to a
[00:06:14.320 --> 00:06:16.400]   colab in the description of this video.
[00:06:16.400 --> 00:06:17.440]   Thanks so much.
[00:06:17.440 --> 00:06:18.400]   Hope you enjoy.
[00:06:19.120 --> 00:06:31.120]   [music]

