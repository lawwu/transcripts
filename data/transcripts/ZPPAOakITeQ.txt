
[00:00:00.000 --> 00:00:03.480]   The following is a conversation with Sebastian Thrun.
[00:00:03.480 --> 00:00:05.280]   He's one of the greatest roboticists,
[00:00:05.280 --> 00:00:08.120]   computer scientists, and educators of our time.
[00:00:08.120 --> 00:00:10.600]   He led the development of the autonomous vehicles
[00:00:10.600 --> 00:00:14.280]   at Stanford that won the 2005 DARPA Grand Challenge
[00:00:14.280 --> 00:00:18.160]   and placed second in the 2007 DARPA Urban Challenge.
[00:00:18.160 --> 00:00:20.960]   He then led the Google Self-Driving Car Program
[00:00:20.960 --> 00:00:24.640]   which launched the self-driving car revolution.
[00:00:24.640 --> 00:00:26.480]   He taught the popular Stanford course
[00:00:26.480 --> 00:00:29.080]   on artificial intelligence in 2011,
[00:00:29.080 --> 00:00:32.440]   which was one of the first massive open online courses
[00:00:32.440 --> 00:00:35.040]   or MOOCs as they're commonly called.
[00:00:35.040 --> 00:00:37.560]   That experience led him to co-found Udacity,
[00:00:37.560 --> 00:00:39.840]   an online education platform.
[00:00:39.840 --> 00:00:41.600]   If you haven't taken courses on it yet,
[00:00:41.600 --> 00:00:43.320]   I highly recommend it.
[00:00:43.320 --> 00:00:47.120]   Their self-driving car program, for example, is excellent.
[00:00:47.120 --> 00:00:49.880]   He's also the CEO of Kitty Hawk,
[00:00:49.880 --> 00:00:53.000]   a company working on building flying cars
[00:00:53.000 --> 00:00:54.960]   or more technically EVTOLs,
[00:00:54.960 --> 00:00:56.880]   which stands for electric vertical takeoff
[00:00:56.880 --> 00:00:58.640]   and landing aircraft.
[00:00:58.640 --> 00:01:00.640]   He has launched several revolutions
[00:01:00.640 --> 00:01:02.720]   and inspired millions of people,
[00:01:02.720 --> 00:01:06.840]   but also, as many know, he's just a really nice guy.
[00:01:06.840 --> 00:01:10.560]   It was an honor and a pleasure to talk with him.
[00:01:10.560 --> 00:01:12.800]   This is the Artificial Intelligence Podcast.
[00:01:12.800 --> 00:01:15.160]   If you enjoy it, subscribe on YouTube,
[00:01:15.160 --> 00:01:17.120]   give it five stars on Apple Podcasts,
[00:01:17.120 --> 00:01:19.800]   follow it on Spotify, support it on Patreon,
[00:01:19.800 --> 00:01:21.880]   or simply connect with me on Twitter
[00:01:21.880 --> 00:01:25.840]   at Lex Friedman, spelled F-R-I-D-M-A-N.
[00:01:25.840 --> 00:01:27.840]   If you leave a review on Apple Podcasts
[00:01:27.840 --> 00:01:30.600]   or YouTube or Twitter, consider mentioning ideas,
[00:01:30.600 --> 00:01:32.880]   people, topics you find interesting.
[00:01:32.880 --> 00:01:35.840]   It helps guide the future of this podcast.
[00:01:35.840 --> 00:01:38.200]   But in general, I just love comments
[00:01:38.200 --> 00:01:40.160]   with kindness and thoughtfulness in them.
[00:01:40.160 --> 00:01:43.640]   This podcast is a side project for me, as many people know,
[00:01:43.640 --> 00:01:45.880]   but I still put a lot of effort into it.
[00:01:45.880 --> 00:01:47.720]   So the positive words of support
[00:01:47.720 --> 00:01:52.220]   from an amazing community, from you, really help.
[00:01:52.220 --> 00:01:55.240]   I recently started doing ads at the end of the introduction.
[00:01:55.240 --> 00:01:58.120]   I'll do one or two minutes after introducing the episode
[00:01:58.120 --> 00:01:59.680]   and never any ads in the middle
[00:01:59.680 --> 00:02:01.840]   that can break the flow of the conversation.
[00:02:01.840 --> 00:02:03.120]   I hope that works for you
[00:02:03.120 --> 00:02:05.420]   and doesn't hurt the listening experience.
[00:02:05.420 --> 00:02:08.000]   I provide timestamps for the start of the conversation
[00:02:08.000 --> 00:02:11.480]   that you can skip to, but it helps if you listen to the ad
[00:02:11.480 --> 00:02:13.880]   and support this podcast by trying out the product
[00:02:13.880 --> 00:02:15.480]   or service being advertised.
[00:02:15.480 --> 00:02:18.800]   This show is presented by Cash App,
[00:02:18.800 --> 00:02:21.460]   the number one finance app in the App Store.
[00:02:21.460 --> 00:02:24.080]   I personally use Cash App to send money to friends,
[00:02:24.080 --> 00:02:25.920]   but you can also use it to buy, sell,
[00:02:25.920 --> 00:02:28.240]   and deposit Bitcoin in just seconds.
[00:02:28.240 --> 00:02:31.080]   Cash App also has a new investing feature.
[00:02:31.080 --> 00:02:34.480]   You can buy fractions of a stock, say $1 worth,
[00:02:34.480 --> 00:02:36.600]   no matter what the stock price is.
[00:02:36.600 --> 00:02:39.520]   Brokerage services are provided by Cash App Investing,
[00:02:39.520 --> 00:02:42.960]   a subsidiary of Square and member SIPC.
[00:02:42.960 --> 00:02:44.720]   I'm excited to be working with Cash App
[00:02:44.720 --> 00:02:47.920]   to support one of my favorite organizations called FIRST,
[00:02:47.920 --> 00:02:51.340]   best known for their FIRST robotics and Lego competitions.
[00:02:51.340 --> 00:02:54.680]   They educate and inspire hundreds of thousands of students
[00:02:54.680 --> 00:02:56.520]   in over 110 countries
[00:02:56.520 --> 00:02:59.040]   and have a perfect rating on Charity Navigator,
[00:02:59.040 --> 00:03:00.720]   which means the donated money
[00:03:00.720 --> 00:03:03.120]   is used to maximum effectiveness.
[00:03:03.120 --> 00:03:06.080]   When you get Cash App from the App Store or Google Play
[00:03:06.080 --> 00:03:09.360]   and use code LEXPODCAST, you'll get $10
[00:03:09.360 --> 00:03:12.120]   and Cash App will also donate $10 to FIRST,
[00:03:12.120 --> 00:03:13.960]   which again is an organization
[00:03:13.960 --> 00:03:16.680]   that I've personally seen inspire girls and boys
[00:03:16.680 --> 00:03:19.760]   to dream of engineering a better world.
[00:03:19.760 --> 00:03:24.000]   And now, here's my conversation with Sebastian Thrun.
[00:03:24.000 --> 00:03:26.760]   You've mentioned that "The Matrix"
[00:03:26.760 --> 00:03:29.020]   may be your favorite movie.
[00:03:29.020 --> 00:03:32.220]   So let's start with a crazy philosophical question.
[00:03:32.220 --> 00:03:34.860]   Do you think we're living in a simulation?
[00:03:34.860 --> 00:03:37.980]   And in general, do you find the thought experiment
[00:03:37.980 --> 00:03:40.060]   interesting?
[00:03:40.060 --> 00:03:42.280]   - Define simulation, I would say.
[00:03:42.280 --> 00:03:43.780]   Maybe we are, maybe we are not,
[00:03:43.780 --> 00:03:47.220]   but it's completely irrelevant to the way we should act.
[00:03:47.220 --> 00:03:49.900]   Putting aside for a moment,
[00:03:49.900 --> 00:03:52.400]   the fact that it might not have any impact
[00:03:52.400 --> 00:03:54.940]   on how we should act as human beings,
[00:03:54.940 --> 00:03:57.340]   for people studying theoretical physics,
[00:03:57.340 --> 00:03:59.620]   these kinds of questions might be kind of interesting,
[00:03:59.620 --> 00:04:03.820]   looking at the universe as a information processing system.
[00:04:03.820 --> 00:04:05.980]   - Universe is an information processing system.
[00:04:05.980 --> 00:04:08.500]   It's a huge physical, biological, chemical computer.
[00:04:08.500 --> 00:04:09.600]   There's no question.
[00:04:09.600 --> 00:04:12.940]   But I live here and now.
[00:04:12.940 --> 00:04:15.660]   I care about people, I care about us.
[00:04:15.660 --> 00:04:17.700]   What do you think is trying to compute?
[00:04:17.700 --> 00:04:18.860]   - I don't think there's an intention.
[00:04:18.860 --> 00:04:22.060]   I think it's just the world evolves the way it evolves,
[00:04:22.060 --> 00:04:25.380]   and it's beautiful, it's unpredictable,
[00:04:25.380 --> 00:04:28.060]   and I'm really, really grateful to be alive.
[00:04:28.060 --> 00:04:29.520]   - Spoken like a true human.
[00:04:29.520 --> 00:04:32.140]   - Which last time I checked I was.
[00:04:32.140 --> 00:04:35.300]   - Well, in fact, this whole conversation
[00:04:35.300 --> 00:04:39.360]   is just a Turing test to see if indeed you are.
[00:04:39.360 --> 00:04:42.780]   You've also said that one of the first programs
[00:04:42.780 --> 00:04:45.100]   or the first few programs you've written
[00:04:45.100 --> 00:04:49.160]   was, wait for it, TI-57 calculator.
[00:04:49.160 --> 00:04:50.040]   - Yeah.
[00:04:50.040 --> 00:04:52.060]   - Maybe that's early '80s.
[00:04:52.060 --> 00:04:54.380]   I don't wanna date calculators or anything.
[00:04:54.380 --> 00:04:55.580]   - That's early '80s, correct.
[00:04:55.580 --> 00:04:56.460]   - Yeah.
[00:04:56.460 --> 00:04:59.820]   So if you were to place yourself back into that time,
[00:04:59.820 --> 00:05:02.140]   into the mindset you were in,
[00:05:02.140 --> 00:05:05.620]   could you have predicted the evolution of computing,
[00:05:05.620 --> 00:05:10.620]   AI, the internet, technology, in the decades that followed?
[00:05:10.620 --> 00:05:13.180]   - I was super fascinated by Silicon Valley,
[00:05:13.180 --> 00:05:15.260]   which I'd seen on television once and thought,
[00:05:15.260 --> 00:05:16.420]   my God, this is so cool.
[00:05:16.420 --> 00:05:19.620]   They build like DRAMs there and CPUs.
[00:05:19.620 --> 00:05:20.460]   How cool is that?
[00:05:20.460 --> 00:05:23.980]   And as a college student a few years later,
[00:05:23.980 --> 00:05:25.860]   I decided to really study intelligence
[00:05:25.860 --> 00:05:28.100]   and study human beings and found that,
[00:05:28.100 --> 00:05:30.540]   even back then in the '80s and '90s,
[00:05:30.540 --> 00:05:31.580]   that artificial intelligence
[00:05:31.580 --> 00:05:33.340]   is what fascinated me the most.
[00:05:33.340 --> 00:05:35.820]   What's missing is that back in the day,
[00:05:35.820 --> 00:05:37.700]   the computers are really small.
[00:05:37.700 --> 00:05:39.100]   They're like the brains you could build
[00:05:39.100 --> 00:05:41.580]   were not anywhere bigger than a cockroach,
[00:05:41.580 --> 00:05:43.740]   and cockroaches aren't very smart.
[00:05:43.740 --> 00:05:46.300]   So we weren't at the scale yet where we are today.
[00:05:46.300 --> 00:05:48.700]   - Did you dream at that time
[00:05:48.700 --> 00:05:51.060]   to achieve the kind of scale we have today?
[00:05:51.060 --> 00:05:52.660]   Or did that seem possible?
[00:05:52.660 --> 00:05:54.340]   - I always wanted to make robots smart.
[00:05:54.340 --> 00:05:57.940]   I felt it was super cool to build an artificial human.
[00:05:57.940 --> 00:05:59.660]   And the best way to build an artificial human
[00:05:59.660 --> 00:06:00.660]   would have been a robot,
[00:06:00.660 --> 00:06:03.060]   because that's kind of the closest we could do.
[00:06:03.060 --> 00:06:04.900]   Unfortunately, we aren't there yet.
[00:06:04.900 --> 00:06:07.180]   The robots today are still very brittle.
[00:06:07.180 --> 00:06:09.420]   But it's fascinating to study intelligence
[00:06:09.420 --> 00:06:10.700]   from a constructive perspective
[00:06:10.700 --> 00:06:12.020]   where you build something.
[00:06:12.020 --> 00:06:15.140]   - To understand you build,
[00:06:15.140 --> 00:06:19.500]   what do you think it takes to build an intelligent system
[00:06:19.500 --> 00:06:20.900]   and an intelligent robot?
[00:06:20.900 --> 00:06:22.140]   - I think the biggest innovation
[00:06:22.140 --> 00:06:23.780]   that we've seen is machine learning.
[00:06:23.780 --> 00:06:26.140]   And it's the idea that the computers
[00:06:26.140 --> 00:06:27.700]   can basically teach themselves.
[00:06:27.700 --> 00:06:29.740]   Let's give an example.
[00:06:29.740 --> 00:06:33.100]   I'd say everybody pretty much knows how to walk.
[00:06:33.100 --> 00:06:35.340]   And we learn how to walk in the first year or two
[00:06:35.340 --> 00:06:36.860]   of our lives.
[00:06:36.860 --> 00:06:38.820]   But no scientist has ever been able
[00:06:38.820 --> 00:06:41.180]   to write down the rules of human gait.
[00:06:41.180 --> 00:06:42.180]   We don't understand it.
[00:06:42.180 --> 00:06:44.060]   We have it in our brains somehow.
[00:06:44.060 --> 00:06:45.220]   We can practice it.
[00:06:45.220 --> 00:06:46.620]   We understand it.
[00:06:46.620 --> 00:06:47.820]   But we can't articulate it.
[00:06:47.820 --> 00:06:50.300]   We can't pass it on by language.
[00:06:50.300 --> 00:06:52.140]   And that to me is kind of the deficiency
[00:06:52.140 --> 00:06:53.380]   of today's computer programming.
[00:06:53.380 --> 00:06:55.300]   When you program a computer,
[00:06:55.300 --> 00:06:56.780]   they're so insanely dumb
[00:06:56.780 --> 00:06:59.900]   that you have to give them rules for every contingencies.
[00:06:59.900 --> 00:07:01.780]   Very unlike the way people learn
[00:07:01.780 --> 00:07:03.500]   but learn from data and experience,
[00:07:03.500 --> 00:07:05.500]   computers are being instructed.
[00:07:05.500 --> 00:07:07.900]   And because it's so hard to get this instruction set right,
[00:07:07.900 --> 00:07:11.500]   you pay software engineers $200,000 a year.
[00:07:11.500 --> 00:07:13.140]   Now, the most recent innovation,
[00:07:13.140 --> 00:07:15.860]   which has been to make for like 30, 40 years,
[00:07:15.860 --> 00:07:18.500]   is an idea that computers can find their own rules.
[00:07:18.500 --> 00:07:20.660]   So they can learn from falling down and getting up
[00:07:20.660 --> 00:07:21.980]   the same way children can learn
[00:07:21.980 --> 00:07:23.820]   from falling down and getting up.
[00:07:23.820 --> 00:07:26.220]   And that revolution has led to a capability
[00:07:26.220 --> 00:07:28.700]   that's completely unmatched.
[00:07:28.700 --> 00:07:31.740]   Today's computers can watch experts do their jobs,
[00:07:31.740 --> 00:07:33.860]   whether you're a doctor or a lawyer,
[00:07:33.860 --> 00:07:36.900]   pick up the regularities, learn those rules,
[00:07:36.900 --> 00:07:39.380]   and then become as good as the best experts.
[00:07:39.380 --> 00:07:41.700]   - So the dream of in the '80s of expert systems,
[00:07:41.700 --> 00:07:44.820]   for example, had at its core the idea
[00:07:44.820 --> 00:07:47.820]   that humans could boil down their expertise
[00:07:47.820 --> 00:07:49.340]   on a sheet of paper.
[00:07:49.340 --> 00:07:53.220]   So to sort of reduce, sort of be able to explain to machines
[00:07:53.220 --> 00:07:55.460]   how to do something explicitly.
[00:07:55.460 --> 00:07:59.100]   So do you think, what's the use of human expertise
[00:07:59.100 --> 00:08:00.060]   into this whole picture?
[00:08:00.060 --> 00:08:01.740]   Do you think most of the intelligence
[00:08:01.740 --> 00:08:04.260]   will come from machines learning from experience
[00:08:04.260 --> 00:08:06.460]   without human expertise input?
[00:08:06.460 --> 00:08:08.380]   - So the question for me is much more
[00:08:08.380 --> 00:08:10.700]   how do you express expertise?
[00:08:10.700 --> 00:08:12.940]   You can express expertise by writing a book.
[00:08:12.940 --> 00:08:15.140]   You can express expertise by showing someone
[00:08:15.140 --> 00:08:16.260]   what you're doing.
[00:08:16.260 --> 00:08:18.380]   You can express expertise by applying it
[00:08:18.380 --> 00:08:20.020]   by many different ways.
[00:08:20.020 --> 00:08:23.700]   And I think the expert systems was our best attempt in AI
[00:08:23.700 --> 00:08:25.980]   to capture expertise and rules.
[00:08:25.980 --> 00:08:27.100]   But someone sat down and say,
[00:08:27.100 --> 00:08:28.540]   "Here are the rules of human gait.
[00:08:28.540 --> 00:08:31.260]   "Here's when you put your big toe forward
[00:08:31.260 --> 00:08:34.700]   "and your heel backwards and here how you stop stumbling."
[00:08:34.700 --> 00:08:37.100]   And as we now know, the set of rules,
[00:08:37.100 --> 00:08:39.420]   the set of language that we can command
[00:08:39.420 --> 00:08:41.180]   is incredibly limited.
[00:08:41.180 --> 00:08:43.780]   The majority of the human brain doesn't deal with language.
[00:08:43.780 --> 00:08:48.180]   It deals with like subconscious numerical perceptual things
[00:08:48.180 --> 00:08:50.420]   that we don't even, that we're self aware of.
[00:08:50.420 --> 00:08:55.740]   Now, when a AI system watches an expert do their job
[00:08:55.740 --> 00:08:57.900]   and practice their job,
[00:08:57.900 --> 00:09:01.140]   it can pick up things that people can't even put into
[00:09:01.140 --> 00:09:03.020]   writing into books or rules.
[00:09:03.020 --> 00:09:04.500]   And that's where the real power is.
[00:09:04.500 --> 00:09:07.140]   We now have AI systems that, for example,
[00:09:07.140 --> 00:09:10.540]   look over the shoulders of highly paid human doctors
[00:09:10.540 --> 00:09:12.820]   like dermatologists or radiologists,
[00:09:12.820 --> 00:09:15.340]   and they can somehow pick up those skills
[00:09:15.340 --> 00:09:16.980]   that no one can express in words.
[00:09:16.980 --> 00:09:22.140]   - So you were a key person in launching three revolutions,
[00:09:22.140 --> 00:09:25.140]   online education, autonomous vehicles,
[00:09:25.140 --> 00:09:28.180]   and flying cars or VTOLs.
[00:09:28.180 --> 00:09:30.220]   So high level,
[00:09:30.220 --> 00:09:34.380]   and I apologize for all the philosophical questions.
[00:09:35.020 --> 00:09:36.180]   - No apology necessary.
[00:09:36.180 --> 00:09:40.540]   - How do you choose what problems to try and solve?
[00:09:40.540 --> 00:09:43.340]   What drives you to make those solutions a reality?
[00:09:43.340 --> 00:09:44.820]   - I have two desires in life.
[00:09:44.820 --> 00:09:48.500]   I wanna literally make the lives of others better.
[00:09:48.500 --> 00:09:50.460]   Or as we often say,
[00:09:50.460 --> 00:09:52.900]   maybe jokingly, make the world a better place.
[00:09:52.900 --> 00:09:54.940]   I actually believe in this.
[00:09:54.940 --> 00:09:56.340]   It's as funny as it sounds.
[00:09:56.340 --> 00:09:59.100]   And second, I wanna learn.
[00:09:59.100 --> 00:10:00.420]   I wanna get in the skillset.
[00:10:00.420 --> 00:10:01.820]   I don't wanna be in a job I'm good at,
[00:10:01.820 --> 00:10:04.260]   because if I'm in a job that I'm good at,
[00:10:04.260 --> 00:10:05.820]   the chances for me to learn something interesting
[00:10:05.820 --> 00:10:06.660]   is actually minimized.
[00:10:06.660 --> 00:10:08.340]   So I wanna be in a job I'm bad at.
[00:10:08.340 --> 00:10:10.180]   That's really important to me.
[00:10:10.180 --> 00:10:11.540]   So in a build, for example,
[00:10:11.540 --> 00:10:12.980]   what people often call flying cars,
[00:10:12.980 --> 00:10:14.860]   these are electrical, vertical,
[00:10:14.860 --> 00:10:16.500]   takeoff and landing vehicles.
[00:10:16.500 --> 00:10:19.660]   I'm just no expert in any of this.
[00:10:19.660 --> 00:10:22.220]   And it's so much fun to learn on the job
[00:10:22.220 --> 00:10:24.860]   what it actually means to build something like this.
[00:10:24.860 --> 00:10:27.500]   Now I'd say the stuff that I've done lately
[00:10:27.500 --> 00:10:31.060]   after I finished my professorship at Stanford,
[00:10:31.060 --> 00:10:33.740]   they really focused on like what has the maximum impact
[00:10:33.740 --> 00:10:34.820]   on society.
[00:10:34.820 --> 00:10:37.500]   Like transportation is something that has transformed
[00:10:37.500 --> 00:10:40.100]   the 21st or 20th century more than any other invention,
[00:10:40.100 --> 00:10:42.500]   in my opinion, even more than communication.
[00:10:42.500 --> 00:10:45.020]   And cities are different, workers different,
[00:10:45.020 --> 00:10:47.860]   women's rights are different because of transportation.
[00:10:47.860 --> 00:10:50.740]   And yet we still have a very suboptimal
[00:10:50.740 --> 00:10:54.660]   transportation solution where we kill 1.2
[00:10:54.660 --> 00:10:57.620]   or so million people every year in traffic.
[00:10:57.620 --> 00:10:58.860]   It's like the leading cause of death
[00:10:58.860 --> 00:11:01.100]   for young people in many countries,
[00:11:01.100 --> 00:11:03.580]   where we are extremely inefficient resource wise,
[00:11:03.580 --> 00:11:06.780]   just go to your average neighborhood city
[00:11:06.780 --> 00:11:08.300]   and look at the number of parked cars,
[00:11:08.300 --> 00:11:10.380]   that's a travesty in my opinion,
[00:11:10.380 --> 00:11:13.820]   or where we spend endless hours in traffic jams.
[00:11:13.820 --> 00:11:16.820]   And very, very simple innovations like a self-driving car
[00:11:16.820 --> 00:11:18.820]   or what people call a flying car
[00:11:18.820 --> 00:11:20.220]   could completely change this.
[00:11:20.220 --> 00:11:23.260]   And it's there, I mean, the technology is basically there.
[00:11:23.260 --> 00:11:25.380]   You have to close your eyes not to see it.
[00:11:25.380 --> 00:11:29.380]   - So lingering on autonomous vehicles,
[00:11:29.380 --> 00:11:32.060]   a fascinating space, some incredible work
[00:11:32.060 --> 00:11:33.580]   you've done throughout your career there.
[00:11:33.580 --> 00:11:38.340]   So let's start with DARPA, I think,
[00:11:38.340 --> 00:11:40.340]   the DARPA challenge through the desert
[00:11:40.340 --> 00:11:42.820]   and then urban to the streets.
[00:11:42.820 --> 00:11:45.700]   I think that inspired an entire generation of roboticists
[00:11:45.700 --> 00:11:49.460]   and obviously sprung this whole excitement
[00:11:49.460 --> 00:11:52.620]   about this particular kind of four wheeled robots
[00:11:52.620 --> 00:11:55.500]   we called autonomous cars, self-driving cars.
[00:11:55.500 --> 00:11:58.180]   So you led the development of Stanley,
[00:11:58.180 --> 00:12:01.220]   the autonomous car that won the race to the desert,
[00:12:01.220 --> 00:12:03.940]   the DARPA challenge in 2005.
[00:12:03.940 --> 00:12:07.300]   And Junior, the car that finished second
[00:12:07.300 --> 00:12:09.140]   in the DARPA urban challenge,
[00:12:09.140 --> 00:12:13.340]   also did incredibly well in 2007, I think.
[00:12:13.340 --> 00:12:16.540]   What are some painful, inspiring
[00:12:16.540 --> 00:12:19.420]   or enlightening experiences from that time
[00:12:19.420 --> 00:12:20.580]   that stand out to you?
[00:12:20.580 --> 00:12:25.580]   - Oh my God, painful were all these incredibly complicated,
[00:12:25.580 --> 00:12:30.420]   stupid bugs that had to be found.
[00:12:30.420 --> 00:12:33.020]   We had a phase where Stanley,
[00:12:33.020 --> 00:12:36.180]   our car that eventually won the DARPA urban challenge
[00:12:36.180 --> 00:12:39.300]   would every 30 miles just commit suicide
[00:12:39.300 --> 00:12:40.860]   and we didn't know why.
[00:12:40.860 --> 00:12:43.500]   And it ended up to be that in the syncing
[00:12:43.500 --> 00:12:47.700]   of two computer clocks, occasionally a clock went backwards
[00:12:47.700 --> 00:12:50.340]   and that negative time elapsed,
[00:12:50.340 --> 00:12:51.820]   screwed up the entire internal logic,
[00:12:51.820 --> 00:12:54.380]   but it took ages to find this.
[00:12:54.380 --> 00:12:56.300]   It were like bugs like that.
[00:12:56.300 --> 00:12:59.300]   I'd say enlightening is the Stanford team
[00:12:59.300 --> 00:13:02.460]   immediately focused on machine learning and on software,
[00:13:02.460 --> 00:13:03.740]   whereas everybody else seemed to focus
[00:13:03.740 --> 00:13:05.180]   on building better hardware.
[00:13:05.180 --> 00:13:07.620]   Our analysis had been a human being
[00:13:07.620 --> 00:13:10.220]   with an existing rental car can perfectly drive the course.
[00:13:10.220 --> 00:13:12.100]   Why do I have to build a better rental car?
[00:13:12.100 --> 00:13:14.780]   I just should replace the human being.
[00:13:14.780 --> 00:13:18.780]   And the human being to me was a conjunction of three steps.
[00:13:18.780 --> 00:13:22.300]   We had sensors, eyes and ears, mostly eyes.
[00:13:22.300 --> 00:13:23.740]   We had brains in the middle
[00:13:23.740 --> 00:13:26.260]   and then we had actuators, our hands and our feet.
[00:13:26.260 --> 00:13:28.140]   Now the actuators are easy to build.
[00:13:28.140 --> 00:13:29.660]   The sensors are actually also easy to build.
[00:13:29.660 --> 00:13:30.660]   What was missing was the brain.
[00:13:30.660 --> 00:13:32.540]   So we had to build a human brain
[00:13:32.540 --> 00:13:35.180]   and nothing clear then to me
[00:13:35.180 --> 00:13:36.940]   that the human brain is a learning machine.
[00:13:36.940 --> 00:13:38.180]   So why not just train our robot?
[00:13:38.180 --> 00:13:42.260]   So we would build massive machine learning into our machine.
[00:13:42.260 --> 00:13:44.780]   And with that, we're able to not just learn
[00:13:44.780 --> 00:13:47.340]   from human drivers, we had the entire speed control
[00:13:47.340 --> 00:13:49.780]   of the vehicle was copied from human driving,
[00:13:49.780 --> 00:13:51.620]   but also have the robot learn from experience
[00:13:51.620 --> 00:13:53.620]   where it made a mistake and go to recover from it
[00:13:53.620 --> 00:13:54.540]   and learn from it.
[00:13:55.580 --> 00:14:00.580]   - You mentioned the pain point of software and clocks.
[00:14:00.580 --> 00:14:04.580]   Synchronization seems to be a problem
[00:14:04.580 --> 00:14:06.020]   that continues with robotics.
[00:14:06.020 --> 00:14:08.020]   It's a tricky one with drones and so on.
[00:14:08.020 --> 00:14:13.380]   What does it take to build a thing,
[00:14:13.380 --> 00:14:16.620]   a system with so many constraints?
[00:14:16.620 --> 00:14:20.300]   You have a deadline, no time.
[00:14:20.300 --> 00:14:22.100]   You're unsure about anything really.
[00:14:22.100 --> 00:14:24.980]   It's the first time that people really even explore.
[00:14:24.980 --> 00:14:26.780]   It's not even sure that anybody can finish
[00:14:26.780 --> 00:14:28.340]   when we're talking about the race
[00:14:28.340 --> 00:14:30.620]   of the desert the year before nobody finished.
[00:14:30.620 --> 00:14:33.420]   What does it take to scramble and finish a product
[00:14:33.420 --> 00:14:35.780]   that actually, a system that actually works?
[00:14:35.780 --> 00:14:36.780]   - I mean, we were very lucky.
[00:14:36.780 --> 00:14:38.300]   We were a really small team.
[00:14:38.300 --> 00:14:40.460]   The core of the team were four people.
[00:14:40.460 --> 00:14:43.100]   It was four because five couldn't comfortably sit
[00:14:43.100 --> 00:14:45.300]   inside a car, but four could.
[00:14:45.300 --> 00:14:47.700]   And I, as a team leader, my job was to get pizza
[00:14:47.700 --> 00:14:51.180]   for everybody and wash the car and stuff like this
[00:14:51.180 --> 00:14:52.860]   and repair the radiator when it broke
[00:14:52.860 --> 00:14:54.380]   and debug the system.
[00:14:55.260 --> 00:14:56.900]   And we were very kind of open-minded.
[00:14:56.900 --> 00:14:58.420]   We had like no egos involved in this.
[00:14:58.420 --> 00:15:00.820]   We just wanted to see how far we can get.
[00:15:00.820 --> 00:15:03.300]   What we did really, really well was time management.
[00:15:03.300 --> 00:15:06.220]   We were done with everything a month before the race.
[00:15:06.220 --> 00:15:08.740]   And we froze the entire software a month before the race.
[00:15:08.740 --> 00:15:11.420]   And it turned out, looking at other teams,
[00:15:11.420 --> 00:15:14.100]   every other team complained if they had just one more week,
[00:15:14.100 --> 00:15:15.420]   they would have won.
[00:15:15.420 --> 00:15:18.740]   And we decided, we're not gonna fall into that mistake.
[00:15:18.740 --> 00:15:19.860]   We're gonna be early.
[00:15:19.860 --> 00:15:22.700]   And we had an entire month to shake the system.
[00:15:22.700 --> 00:15:24.940]   And we actually found two or three minor bugs
[00:15:24.940 --> 00:15:27.060]   in the last month that we had to fix.
[00:15:27.060 --> 00:15:30.020]   And we were completely prepared when the race occurred.
[00:15:30.020 --> 00:15:30.860]   - Okay, so first of all,
[00:15:30.860 --> 00:15:34.620]   that's such an incredibly rare achievement
[00:15:34.620 --> 00:15:38.980]   in terms of being able to be done on time or ahead of time.
[00:15:38.980 --> 00:15:43.060]   What do you, how do you do that in your future work?
[00:15:43.060 --> 00:15:44.780]   What advice do you have in general?
[00:15:44.780 --> 00:15:46.340]   Because it seems to be so rare,
[00:15:46.340 --> 00:15:49.300]   especially in highly innovative projects like this.
[00:15:49.300 --> 00:15:50.860]   People work till the last second.
[00:15:50.860 --> 00:15:52.540]   - Well, the nice thing about the Dark Background Challenge
[00:15:52.540 --> 00:15:55.300]   is that the problem was incredibly well-defined.
[00:15:55.300 --> 00:15:56.540]   We were able for a while
[00:15:56.540 --> 00:15:58.780]   to drive the old Dark Background Challenge course,
[00:15:58.780 --> 00:16:00.780]   which had been used the year before.
[00:16:00.780 --> 00:16:04.020]   And then at some reason, we were kicked out of the region.
[00:16:04.020 --> 00:16:06.300]   So we had to go to different deserts, the Snorren Desert,
[00:16:06.300 --> 00:16:08.860]   and we were able to drive desert trails
[00:16:08.860 --> 00:16:10.580]   just at the same time.
[00:16:10.580 --> 00:16:12.180]   So there was never any debate about,
[00:16:12.180 --> 00:16:13.220]   like, what is actually the problem?
[00:16:13.220 --> 00:16:14.340]   We didn't sit down and say,
[00:16:14.340 --> 00:16:16.660]   "Hey, should we build a car or a plane?"
[00:16:16.660 --> 00:16:18.260]   We had to build a car.
[00:16:18.260 --> 00:16:20.380]   That made it very, very easy.
[00:16:20.380 --> 00:16:23.780]   Then I studied my own life and life of others
[00:16:23.780 --> 00:16:26.340]   and realized that the typical mistake that people make
[00:16:26.340 --> 00:16:29.580]   is that there's this kind of crazy bug left
[00:16:29.580 --> 00:16:31.220]   that they haven't found yet.
[00:16:31.220 --> 00:16:34.340]   And it's just, they regret it.
[00:16:34.340 --> 00:16:36.140]   And the bug would have been trivial to fix.
[00:16:36.140 --> 00:16:37.740]   They just haven't fixed it yet.
[00:16:37.740 --> 00:16:39.580]   They didn't want to fall into that trap.
[00:16:39.580 --> 00:16:41.060]   So I built a testing team.
[00:16:41.060 --> 00:16:43.740]   We had a testing team that built a testing booklet
[00:16:43.740 --> 00:16:46.740]   of 160 pages of tests we had to go through
[00:16:46.740 --> 00:16:49.100]   just to make sure we shake out the system appropriately.
[00:16:49.100 --> 00:16:49.940]   - Wow.
[00:16:49.940 --> 00:16:51.820]   - And the testing team was with us all the time
[00:16:51.820 --> 00:16:53.020]   and dictated to us,
[00:16:53.020 --> 00:16:55.540]   today we do railroad crossings,
[00:16:55.540 --> 00:16:58.460]   tomorrow we do, we practice the start of the event.
[00:16:58.460 --> 00:17:00.660]   And in all of these, we thought,
[00:17:00.660 --> 00:17:02.260]   "Oh my God, this long solve trivial."
[00:17:02.260 --> 00:17:03.180]   And then we tested it out.
[00:17:03.180 --> 00:17:05.100]   "Oh my God, it doesn't do a railroad crossing, why not?
[00:17:05.100 --> 00:17:09.340]   "Oh my God, it mistakes the rails for metal barriers.
[00:17:09.340 --> 00:17:10.620]   "Shit, we have to fix this."
[00:17:10.620 --> 00:17:11.620]   - Yes.
[00:17:11.620 --> 00:17:14.460]   - So it was really a continuous focus
[00:17:14.460 --> 00:17:16.380]   on improving the weakest part of the system.
[00:17:16.380 --> 00:17:18.540]   And as long as you focus
[00:17:18.540 --> 00:17:20.540]   on improving the weakest part of the system,
[00:17:20.540 --> 00:17:23.100]   you eventually build a really great system.
[00:17:23.100 --> 00:17:24.780]   - Let me just pause in that.
[00:17:24.780 --> 00:17:27.100]   To me as an engineer, it's just super exciting
[00:17:27.100 --> 00:17:28.300]   that you were thinking like that,
[00:17:28.300 --> 00:17:30.460]   especially at that stage as brilliant
[00:17:30.460 --> 00:17:33.380]   that testing was such a core part of it.
[00:17:33.380 --> 00:17:35.740]   It may be to linger on the point of leadership.
[00:17:35.740 --> 00:17:41.700]   I think it's one of the first times you were really a leader
[00:17:41.700 --> 00:17:45.420]   and you've led many very successful teams since then.
[00:17:45.420 --> 00:17:48.460]   What does it take to be a good leader?
[00:17:48.460 --> 00:17:50.820]   - I would say most of all, just take credit
[00:17:50.820 --> 00:17:54.220]   for the work of others.
[00:17:54.220 --> 00:17:57.580]   That's very convenient, turns out,
[00:17:57.580 --> 00:18:00.220]   'cause I can't do all these things myself.
[00:18:00.220 --> 00:18:03.740]   I'm an engineer at heart, so I care about engineering.
[00:18:03.740 --> 00:18:06.140]   So I don't know what the chicken and the egg is,
[00:18:06.140 --> 00:18:07.860]   but as a kid, I loved computers
[00:18:07.860 --> 00:18:09.540]   because you could tell them to do something
[00:18:09.540 --> 00:18:10.700]   and they actually did it.
[00:18:10.700 --> 00:18:11.540]   It was very cool.
[00:18:11.540 --> 00:18:12.740]   And you could like in the middle of the night,
[00:18:12.740 --> 00:18:15.180]   wake up at one in the morning and switch on your computer.
[00:18:15.180 --> 00:18:18.140]   And what you told you to yesterday, it would still do.
[00:18:18.140 --> 00:18:19.380]   That was really cool.
[00:18:19.380 --> 00:18:21.300]   Unfortunately, that didn't quite work with people.
[00:18:21.300 --> 00:18:22.820]   So you go to people and tell them what to do
[00:18:22.820 --> 00:18:25.780]   and they don't do it and they hate you for it.
[00:18:25.780 --> 00:18:28.060]   Or you do it today and then they go a day later
[00:18:28.060 --> 00:18:29.140]   and they stop doing it.
[00:18:29.140 --> 00:18:30.220]   So you have to...
[00:18:30.220 --> 00:18:31.460]   So then the question really became,
[00:18:31.460 --> 00:18:34.100]   how can you put yourself in the brain of people
[00:18:34.100 --> 00:18:35.100]   as opposed to computers?
[00:18:35.100 --> 00:18:37.340]   And in terms of computers, that's super dumb.
[00:18:37.340 --> 00:18:38.220]   That's so dumb.
[00:18:38.220 --> 00:18:39.620]   If people were as dumb as computers,
[00:18:39.620 --> 00:18:41.260]   I wouldn't wanna work with them.
[00:18:41.260 --> 00:18:43.580]   But people are smart and people are emotional
[00:18:43.580 --> 00:18:45.900]   and people have pride and people have aspirations.
[00:18:45.900 --> 00:18:48.860]   So how can I connect to that?
[00:18:48.860 --> 00:18:52.540]   And that's the thing where most of our leadership just fails
[00:18:52.540 --> 00:18:56.220]   because many, many engineers turn manager
[00:18:56.220 --> 00:18:58.340]   believe they can treat their team just the same
[00:18:58.340 --> 00:18:59.300]   way they can treat your computer
[00:18:59.300 --> 00:19:00.420]   and it just doesn't work this way.
[00:19:00.420 --> 00:19:02.300]   It's just really bad.
[00:19:02.300 --> 00:19:05.100]   So how can I connect to people?
[00:19:05.100 --> 00:19:07.620]   And it turns out as a college professor,
[00:19:07.620 --> 00:19:09.940]   the wonderful thing you do all the time
[00:19:09.940 --> 00:19:10.980]   is to empower other people.
[00:19:10.980 --> 00:19:14.700]   Like your job is to make your students look great.
[00:19:14.700 --> 00:19:15.540]   That's all you do.
[00:19:15.540 --> 00:19:16.900]   You're the best coach.
[00:19:16.900 --> 00:19:18.740]   And it turns out if you do a fantastic job
[00:19:18.740 --> 00:19:20.820]   with making your students look great,
[00:19:20.820 --> 00:19:22.700]   they actually love you and their parents love you.
[00:19:22.700 --> 00:19:23.780]   And they give you all the credit
[00:19:23.780 --> 00:19:24.700]   for stuff you don't deserve.
[00:19:24.700 --> 00:19:27.180]   Turns out all my students were smarter than me.
[00:19:27.180 --> 00:19:28.740]   All the great stuff invented at Stanford
[00:19:28.740 --> 00:19:30.020]   was their stuff, not my stuff.
[00:19:30.020 --> 00:19:31.140]   And they give me credit and say,
[00:19:31.140 --> 00:19:33.620]   oh, Sebastian, we're just making them
[00:19:33.620 --> 00:19:35.100]   feel good about themselves.
[00:19:35.100 --> 00:19:38.020]   So the question really is, can you take a team of people
[00:19:38.020 --> 00:19:40.380]   and what does it take to make them,
[00:19:40.380 --> 00:19:43.340]   to connect to what they actually want in life
[00:19:43.340 --> 00:19:45.740]   and turn this into productive action?
[00:19:45.740 --> 00:19:48.500]   It turns out every human being that I know
[00:19:48.500 --> 00:19:50.100]   has incredibly good intentions.
[00:19:50.100 --> 00:19:53.220]   I've really rarely met a person with bad intentions.
[00:19:53.220 --> 00:19:55.900]   I believe every person wants to contribute.
[00:19:55.900 --> 00:19:59.420]   I think every person I've met wants to help others.
[00:19:59.420 --> 00:20:01.860]   It's amazing how much of a urge we have
[00:20:01.860 --> 00:20:04.420]   not to just help ourselves, but to help others.
[00:20:04.420 --> 00:20:05.940]   So how can we empower people
[00:20:05.940 --> 00:20:07.700]   and give them the right framework
[00:20:07.700 --> 00:20:09.340]   that they can accomplish this?
[00:20:09.340 --> 00:20:12.420]   In moments when it works, it's magical
[00:20:12.420 --> 00:20:17.180]   because you'd see the confluence of people
[00:20:17.180 --> 00:20:19.180]   being able to make the world a better place
[00:20:19.180 --> 00:20:22.900]   and just having enormous confidence and pride out of this.
[00:20:22.900 --> 00:20:27.220]   And that's when my environment works the best.
[00:20:27.220 --> 00:20:29.460]   These are moments where I can disappear for a month
[00:20:29.460 --> 00:20:31.620]   and come back and things still work.
[00:20:31.620 --> 00:20:32.780]   It's very hard to accomplish,
[00:20:32.780 --> 00:20:35.100]   but when it works, it's amazing.
[00:20:35.100 --> 00:20:37.260]   - So I agree with you very much.
[00:20:37.260 --> 00:20:42.020]   It's not often heard that most people in the world
[00:20:42.020 --> 00:20:43.500]   have good intentions.
[00:20:43.500 --> 00:20:45.900]   At the core, their intentions are good
[00:20:45.900 --> 00:20:47.380]   and they're good people.
[00:20:47.380 --> 00:20:48.860]   That's a beautiful message.
[00:20:48.860 --> 00:20:50.180]   It's not often heard.
[00:20:50.180 --> 00:20:51.180]   - We make this mistake,
[00:20:51.180 --> 00:20:53.460]   and this is a friend of mine, Alex Voda,
[00:20:53.460 --> 00:20:57.700]   talking this, that we judge ourselves by our intentions
[00:20:57.700 --> 00:20:59.140]   and others by their actions.
[00:20:59.140 --> 00:21:01.860]   And I think that the biggest skill,
[00:21:01.860 --> 00:21:03.540]   I mean, here in Silicon Valley, we're full of engineers
[00:21:03.540 --> 00:21:05.140]   who have very little empathy
[00:21:05.140 --> 00:21:09.180]   and are kind of befuddled by why it doesn't work for them.
[00:21:09.180 --> 00:21:13.060]   The biggest skill I think that people should acquire
[00:21:13.060 --> 00:21:16.860]   is to put themselves into the position of the other
[00:21:16.860 --> 00:21:19.980]   and listen, and listen to what the other has to say.
[00:21:19.980 --> 00:21:23.380]   And they'd be shocked how similar they are to themselves.
[00:21:23.380 --> 00:21:24.580]   And they might even be shocked
[00:21:24.580 --> 00:21:28.540]   how their own actions don't reflect their intentions.
[00:21:28.540 --> 00:21:30.900]   I often have conversations with engineers
[00:21:30.900 --> 00:21:32.540]   where I say, "Look, hey, I love you.
[00:21:32.540 --> 00:21:33.380]   "You're doing a great job.
[00:21:33.380 --> 00:21:37.220]   "And by the way, what you just did has the following effect.
[00:21:37.220 --> 00:21:38.820]   "Are you aware of that?"
[00:21:38.820 --> 00:21:41.180]   And then people would say, "Oh my God, not I wasn't
[00:21:41.180 --> 00:21:43.100]   "because my intention was that."
[00:21:43.100 --> 00:21:44.900]   And I say, "Yeah, I trust your intention.
[00:21:44.900 --> 00:21:46.260]   "You're a good human being.
[00:21:46.260 --> 00:21:48.340]   "But just to help you in the future,
[00:21:48.340 --> 00:21:51.220]   "if you keep expressing it that way,
[00:21:51.220 --> 00:21:53.380]   "then people will just hate you."
[00:21:53.380 --> 00:21:55.140]   And I've had many instances where people say,
[00:21:55.140 --> 00:21:56.460]   "Oh my God, thank you for telling me this
[00:21:56.460 --> 00:21:59.180]   "because it wasn't my intention to look like an idiot.
[00:21:59.180 --> 00:22:00.580]   "It wasn't my intention to help other people.
[00:22:00.580 --> 00:22:02.460]   "I just didn't know how to do it."
[00:22:02.460 --> 00:22:03.980]   - Very simple, by the way.
[00:22:03.980 --> 00:22:07.420]   There's a book, Dale Carnegie, 1936,
[00:22:07.420 --> 00:22:10.460]   "How to Make Friends and How to Influence Others."
[00:22:10.460 --> 00:22:11.500]   Has the entire Bible.
[00:22:11.500 --> 00:22:12.780]   You just read it and you're done
[00:22:12.780 --> 00:22:14.020]   and you apply it every day.
[00:22:14.020 --> 00:22:16.820]   And I wish I was good enough to apply it every day.
[00:22:16.820 --> 00:22:18.940]   But it's just simple things, right?
[00:22:18.940 --> 00:22:22.620]   Like be positive, remember people's names, smile,
[00:22:22.620 --> 00:22:24.540]   and eventually have empathy.
[00:22:24.540 --> 00:22:27.460]   Really think that the person that you hate
[00:22:27.460 --> 00:22:30.500]   and you think is an idiot is actually just like yourself.
[00:22:30.500 --> 00:22:33.260]   It's a person who's struggling, who means well,
[00:22:33.260 --> 00:22:34.220]   and who might need help.
[00:22:34.220 --> 00:22:35.060]   And guess what?
[00:22:35.060 --> 00:22:36.620]   You need help.
[00:22:36.620 --> 00:22:39.980]   I've recently spoken with Stephen Schwarzman.
[00:22:39.980 --> 00:22:41.660]   I'm not sure if you know who that is.
[00:22:41.660 --> 00:22:42.500]   - I do.
[00:22:42.500 --> 00:22:45.220]   It's on my list.
[00:22:45.220 --> 00:22:46.040]   (laughing)
[00:22:46.040 --> 00:22:46.880]   - On the list.
[00:22:46.880 --> 00:22:47.720]   (laughing)
[00:22:47.720 --> 00:22:52.720]   But he said, sort of to expand on what you're saying,
[00:22:52.720 --> 00:22:56.020]   that one of the biggest things you can do
[00:22:56.020 --> 00:23:00.060]   is hear people when they tell you what their problem is
[00:23:00.060 --> 00:23:02.380]   and then help them with that problem.
[00:23:02.380 --> 00:23:06.020]   He says it's surprising how few people
[00:23:06.020 --> 00:23:08.700]   actually listen to what troubles others.
[00:23:08.700 --> 00:23:09.540]   - Yeah.
[00:23:09.540 --> 00:23:12.620]   - And because it's right there in front of you
[00:23:12.620 --> 00:23:15.220]   and you can benefit the world the most.
[00:23:15.220 --> 00:23:18.020]   And in fact, yourself and everybody around you
[00:23:18.020 --> 00:23:20.860]   by just hearing the problems and solving them.
[00:23:20.860 --> 00:23:23.980]   - I mean, that's my little history of engineering.
[00:23:23.980 --> 00:23:28.260]   That is, while I was engineering with computers,
[00:23:28.260 --> 00:23:32.380]   I didn't care at all what the computer's problems were.
[00:23:32.380 --> 00:23:34.820]   I just told them what to do and they do it.
[00:23:34.820 --> 00:23:37.580]   And it just doesn't work this way with people.
[00:23:37.580 --> 00:23:38.500]   - It doesn't work with me.
[00:23:38.500 --> 00:23:41.260]   If you come to me and say, "Do A," I do the opposite.
[00:23:41.260 --> 00:23:43.620]   (laughing)
[00:23:43.620 --> 00:23:47.140]   - But let's return to the comfortable world of engineering.
[00:23:47.140 --> 00:23:52.140]   And can you tell me in broad strokes in how you see it?
[00:23:52.140 --> 00:23:53.860]   Because you're at the core of starting it,
[00:23:53.860 --> 00:23:55.100]   the core of driving it,
[00:23:55.100 --> 00:23:58.020]   the technical evolution of autonomous vehicles
[00:23:58.020 --> 00:24:00.420]   from the first DARPA Grand Challenge
[00:24:00.420 --> 00:24:03.660]   to the incredible success we see with the program
[00:24:03.660 --> 00:24:05.420]   you started with Google Self-Driving Car
[00:24:05.420 --> 00:24:08.340]   and Waymo and the entire industry that sprung up
[00:24:08.340 --> 00:24:11.220]   of different kinds of approaches, debates, and so on.
[00:24:11.220 --> 00:24:14.180]   - Well, the idea of self-driving car goes back to the '80s.
[00:24:14.180 --> 00:24:16.140]   There was a team in Germany, another team in Carnegie Mellon
[00:24:16.140 --> 00:24:18.700]   that did some very pioneering work.
[00:24:18.700 --> 00:24:21.780]   But back in the day, I'd say the computers were so deficient
[00:24:21.780 --> 00:24:25.900]   that even the best professors and engineers in the world
[00:24:25.900 --> 00:24:27.300]   basically stood no chance.
[00:24:27.300 --> 00:24:31.220]   It then folded into a phase where the US government
[00:24:31.220 --> 00:24:33.340]   spent at least half a billion dollars
[00:24:33.340 --> 00:24:36.180]   that I could count on research projects.
[00:24:36.180 --> 00:24:37.980]   But the way the procurement works,
[00:24:37.980 --> 00:24:42.820]   a successful stack of paper describing lots of stuff
[00:24:42.820 --> 00:24:43.900]   that no one's ever gonna read
[00:24:43.900 --> 00:24:47.660]   was a successful product of a research project.
[00:24:47.660 --> 00:24:50.540]   So we trained our researchers to produce lots of paper.
[00:24:50.540 --> 00:24:54.340]   That all changed with the DARPA Grand Challenge.
[00:24:54.340 --> 00:24:58.500]   And I really gotta credit the ingenious people at DARPA
[00:24:58.500 --> 00:25:00.420]   and the US government and Congress
[00:25:00.420 --> 00:25:02.220]   that took a complete new funding model
[00:25:02.220 --> 00:25:04.100]   where they said, "Let's not fund effort,
[00:25:04.100 --> 00:25:05.580]   "let's fund outcomes."
[00:25:05.580 --> 00:25:08.820]   And it sounds very trivial, but there was no tax code
[00:25:08.820 --> 00:25:13.700]   that allowed the use of congressional tax money for a price.
[00:25:13.700 --> 00:25:15.100]   It was all effort-based.
[00:25:15.100 --> 00:25:17.460]   So if you put in 100 hours in, you could charge 100 hours.
[00:25:17.460 --> 00:25:20.660]   If you put in 1,000 hours in, you could build 1,000 hours.
[00:25:20.660 --> 00:25:22.780]   By changing the focus and say, "We're making the price.
[00:25:22.780 --> 00:25:23.940]   "We don't pay you for development,
[00:25:23.940 --> 00:25:26.340]   "we pay you for the accomplishment."
[00:25:26.340 --> 00:25:28.940]   They drew in, they automatically drew out
[00:25:28.940 --> 00:25:31.700]   all these contractors who are used to the drug
[00:25:31.700 --> 00:25:33.380]   of getting money per hour.
[00:25:33.380 --> 00:25:35.700]   And they drew in a whole bunch of new people.
[00:25:35.700 --> 00:25:37.620]   And these people are mostly crazy people.
[00:25:37.620 --> 00:25:40.740]   They were people who had a car and a computer
[00:25:40.740 --> 00:25:42.420]   and they wanted to make a million bucks.
[00:25:42.420 --> 00:25:43.900]   The million bucks was the original price money,
[00:25:43.900 --> 00:25:45.460]   it was then doubled.
[00:25:45.460 --> 00:25:47.980]   And they felt, "If I put my computer in my car
[00:25:47.980 --> 00:25:50.860]   "and program it, I can be rich."
[00:25:50.860 --> 00:25:52.060]   And that was so awesome.
[00:25:52.060 --> 00:25:55.500]   Like half the teams, there was a team that was surfer dudes
[00:25:55.500 --> 00:25:58.540]   and they had like two surfboards on their vehicle
[00:25:58.540 --> 00:26:00.460]   and brought like these fashion girls,
[00:26:00.460 --> 00:26:02.820]   super cute girls, like twin sisters.
[00:26:02.820 --> 00:26:06.700]   And you could tell these guys were not your common
[00:26:06.700 --> 00:26:10.860]   fails babe bandit who gets all these big multi-million
[00:26:10.860 --> 00:26:13.540]   and billion dollar countries from the US government.
[00:26:13.540 --> 00:26:15.260]   And there was a great reset.
[00:26:15.260 --> 00:26:18.580]   Universities moved in.
[00:26:18.580 --> 00:26:21.820]   I was very fortunate at Stanford that I'd just received tenure
[00:26:21.820 --> 00:26:23.380]   so I couldn't get fired no matter what I do,
[00:26:23.380 --> 00:26:25.140]   otherwise I wouldn't have done it.
[00:26:25.140 --> 00:26:28.260]   And I had enough money to finance this thing.
[00:26:28.260 --> 00:26:31.180]   And I was able to attract a lot of money from third parties.
[00:26:31.180 --> 00:26:32.540]   And even car companies moved in.
[00:26:32.540 --> 00:26:34.060]   They kind of moved in very quietly
[00:26:34.060 --> 00:26:36.580]   because they were super scared to be embarrassed
[00:26:36.580 --> 00:26:38.580]   that their car would flip over.
[00:26:38.580 --> 00:26:40.700]   But Ford was there and Volkswagen was there
[00:26:40.700 --> 00:26:43.380]   and a few others and GM was there.
[00:26:43.380 --> 00:26:46.340]   So it kind of reset the entire landscape of people.
[00:26:46.340 --> 00:26:48.220]   And if you look at who's a big name
[00:26:48.220 --> 00:26:49.500]   in self-driving cars today,
[00:26:49.500 --> 00:26:51.340]   these were mostly people who participated
[00:26:51.340 --> 00:26:52.300]   in those challenges.
[00:26:52.300 --> 00:26:54.260]   - Okay, that's incredible.
[00:26:54.260 --> 00:26:59.060]   Can you just comment quickly on your sense of lessons learned
[00:26:59.060 --> 00:27:01.220]   from that kind of funding model
[00:27:01.220 --> 00:27:04.380]   and the research that's going on in academia
[00:27:04.380 --> 00:27:06.100]   in terms of producing papers?
[00:27:06.100 --> 00:27:09.860]   Is there something to be learned and scaled up bigger?
[00:27:09.860 --> 00:27:11.700]   These having these kinds of grand challenges
[00:27:11.700 --> 00:27:14.540]   that could improve outcomes?
[00:27:14.540 --> 00:27:16.300]   - So I'm a big believer in focusing
[00:27:16.300 --> 00:27:19.620]   on kind of an end-to-end system.
[00:27:19.620 --> 00:27:21.900]   I'm a really big believer in systems building.
[00:27:21.900 --> 00:27:23.660]   I've always built systems in my academic career,
[00:27:23.660 --> 00:27:27.020]   even though I do a lot of math and abstract stuff,
[00:27:27.020 --> 00:27:28.100]   but it's all derived from the idea
[00:27:28.100 --> 00:27:29.620]   of let's solve a real problem.
[00:27:29.620 --> 00:27:33.780]   And it's very hard for me to be an academic
[00:27:33.780 --> 00:27:35.740]   and say, let me solve a component of a problem.
[00:27:35.740 --> 00:27:38.620]   Like with someone, there's fields like non-monetary logic
[00:27:38.620 --> 00:27:41.740]   or AI planning systems where people believe
[00:27:41.740 --> 00:27:44.260]   that a certain style of problem solving
[00:27:44.260 --> 00:27:47.220]   is the ultimate end objective.
[00:27:47.220 --> 00:27:49.500]   And I would always turn it around and say,
[00:27:49.500 --> 00:27:52.580]   hey, what problem would my grandmother care about
[00:27:52.580 --> 00:27:54.620]   that doesn't understand computer technology
[00:27:54.620 --> 00:27:56.460]   and doesn't want to understand?
[00:27:56.460 --> 00:27:58.460]   And how could I make her love what I do?
[00:27:58.460 --> 00:28:01.300]   Because only then do I have an impact on the world.
[00:28:01.300 --> 00:28:02.900]   I can easily impress my colleagues.
[00:28:02.900 --> 00:28:04.700]   That is much easier,
[00:28:04.700 --> 00:28:07.580]   but impressing my grandmother is very, very hard.
[00:28:07.580 --> 00:28:10.700]   So I would always thought if I can build a self-driving car
[00:28:10.700 --> 00:28:12.820]   and my grandmother can use it
[00:28:12.820 --> 00:28:14.660]   even after she loses her driving privileges
[00:28:14.660 --> 00:28:16.100]   or children can use it,
[00:28:16.100 --> 00:28:20.500]   or we save maybe a million lives a year,
[00:28:20.500 --> 00:28:22.420]   that would be very impressive.
[00:28:22.420 --> 00:28:23.900]   And there's so many problems like these.
[00:28:23.900 --> 00:28:25.300]   Like there's a problem with curing cancer,
[00:28:25.300 --> 00:28:27.780]   or whatever it is, live twice as long.
[00:28:27.780 --> 00:28:29.580]   Once a problem is defined,
[00:28:29.580 --> 00:28:31.420]   of course I can't solve it in its entirety.
[00:28:31.420 --> 00:28:34.180]   Like it takes sometimes tens of thousands of people
[00:28:34.180 --> 00:28:35.340]   to find a solution.
[00:28:35.340 --> 00:28:39.340]   There's no way you can fund an army of 10,000 at Stanford.
[00:28:39.340 --> 00:28:41.060]   So you're gonna build a prototype.
[00:28:41.060 --> 00:28:42.500]   Let's build a meaningful prototype.
[00:28:42.500 --> 00:28:43.900]   And the Dark Background Challenge was beautiful
[00:28:43.900 --> 00:28:46.340]   because it told me what this prototype had to do.
[00:28:46.340 --> 00:28:47.660]   I didn't have to think about what it had to do.
[00:28:47.660 --> 00:28:48.820]   I just had to read the rules.
[00:28:48.820 --> 00:28:51.060]   And it was really, really beautiful.
[00:28:51.060 --> 00:28:52.820]   - And it's most beautiful, you think,
[00:28:52.820 --> 00:28:56.220]   what academia could aspire to is to build a prototype
[00:28:56.220 --> 00:29:00.260]   that's the systems level that solves,
[00:29:00.260 --> 00:29:02.140]   gives you an inkling that this problem
[00:29:02.140 --> 00:29:03.540]   could be solved with this prototype.
[00:29:03.540 --> 00:29:06.540]   - First of all, I wanna emphasize what academia really is.
[00:29:06.540 --> 00:29:08.580]   And I think people misunderstand it.
[00:29:08.580 --> 00:29:10.340]   First and foremost,
[00:29:10.340 --> 00:29:13.340]   academia is a way to educate young people.
[00:29:13.340 --> 00:29:15.420]   First and foremost, a professor is an educator.
[00:29:15.420 --> 00:29:17.060]   No matter where you are at,
[00:29:17.060 --> 00:29:18.580]   a small suburban college,
[00:29:18.580 --> 00:29:21.980]   or whether you are a Harvard or Stanford professor.
[00:29:21.980 --> 00:29:25.020]   That's not the way most people think of themselves
[00:29:25.020 --> 00:29:28.020]   in academia because we have this kind of competition
[00:29:28.020 --> 00:29:31.460]   going on for citations and publication.
[00:29:31.460 --> 00:29:32.860]   That's a measurable thing,
[00:29:32.860 --> 00:29:35.500]   but that is secondary to the primary purpose
[00:29:35.500 --> 00:29:37.820]   of educating people to think.
[00:29:37.820 --> 00:29:40.020]   Now, in terms of research,
[00:29:40.020 --> 00:29:42.900]   most of the great science,
[00:29:42.900 --> 00:29:45.540]   the great research comes out of universities.
[00:29:45.540 --> 00:29:46.980]   You can trace almost everything back,
[00:29:46.980 --> 00:29:48.860]   including Google to universities.
[00:29:48.860 --> 00:29:52.180]   So there's nothing really fundamentally broken here.
[00:29:52.180 --> 00:29:53.420]   It's a good system.
[00:29:53.420 --> 00:29:55.980]   And I think America has the finest university system
[00:29:55.980 --> 00:29:56.820]   on the planet.
[00:29:56.820 --> 00:30:00.620]   We can talk about reach and how to reach people
[00:30:00.620 --> 00:30:01.500]   outside the system.
[00:30:01.500 --> 00:30:02.340]   It's a different topic,
[00:30:02.340 --> 00:30:04.820]   but the system itself is a good system.
[00:30:04.820 --> 00:30:07.020]   If I had one wish, I would say,
[00:30:07.020 --> 00:30:10.860]   it'd be really great if there was more debate
[00:30:10.860 --> 00:30:15.860]   about what the great big problems are in society.
[00:30:15.940 --> 00:30:18.780]   And focus on those.
[00:30:18.780 --> 00:30:21.620]   And most of them are interdisciplinary.
[00:30:21.620 --> 00:30:24.660]   Unfortunately, it's very easy to fall
[00:30:24.660 --> 00:30:28.180]   into a interdisciplinary viewpoint
[00:30:28.180 --> 00:30:30.460]   where your problem is dictated
[00:30:30.460 --> 00:30:33.700]   by what your closest colleagues believe the problem is.
[00:30:33.700 --> 00:30:35.300]   It's very hard to break out and say,
[00:30:35.300 --> 00:30:37.940]   well, there's an entire new field of problems.
[00:30:37.940 --> 00:30:39.860]   So to give an example,
[00:30:39.860 --> 00:30:41.660]   prior to me working on self-driving cars,
[00:30:41.660 --> 00:30:44.660]   I was a roboticist and a machine learning expert.
[00:30:44.660 --> 00:30:46.860]   And I wrote books on robotics,
[00:30:46.860 --> 00:30:48.540]   something called probabilistic robotics.
[00:30:48.540 --> 00:30:51.540]   It's a very methods driven kind of viewpoint of the world.
[00:30:51.540 --> 00:30:54.060]   I built robots that acted in museums as tour guides,
[00:30:54.060 --> 00:30:55.660]   that like led children around.
[00:30:55.660 --> 00:31:00.060]   It is something that at the time was moderately challenging.
[00:31:00.060 --> 00:31:02.260]   When I started working on cars,
[00:31:02.260 --> 00:31:03.700]   several colleagues told me,
[00:31:03.700 --> 00:31:06.060]   "Sebastian, you're destroying your career
[00:31:06.060 --> 00:31:08.140]   "because in our field of robotics,
[00:31:08.140 --> 00:31:10.380]   "cars are looked like as a gimmick
[00:31:10.380 --> 00:31:11.740]   "and they're not expressive enough.
[00:31:11.740 --> 00:31:14.140]   "They can only push this bottle
[00:31:14.140 --> 00:31:16.420]   "and the brakes, there's no dexterity,
[00:31:16.420 --> 00:31:19.540]   "there's no complexity, it's just too simple."
[00:31:19.540 --> 00:31:21.180]   And no one came to me and said,
[00:31:21.180 --> 00:31:22.700]   "Wow, if you solve that problem,
[00:31:22.700 --> 00:31:25.020]   "you can save a million lives."
[00:31:25.020 --> 00:31:27.260]   Among all robotic problems that I've seen in my life,
[00:31:27.260 --> 00:31:29.780]   I would say the self-driving car, transportation,
[00:31:29.780 --> 00:31:32.100]   is the one that has the most hope for society.
[00:31:32.100 --> 00:31:33.540]   So how come the robotics community
[00:31:33.540 --> 00:31:35.140]   wasn't all over the place?
[00:31:35.140 --> 00:31:37.940]   And it was because we focused on methods and solutions
[00:31:37.940 --> 00:31:39.900]   and not on problems.
[00:31:39.900 --> 00:31:42.460]   Like if you go around today and ask your grandmother,
[00:31:42.460 --> 00:31:45.260]   what bugs you, what really makes you upset?
[00:31:45.260 --> 00:31:48.780]   I challenge any academic to do this
[00:31:48.780 --> 00:31:51.820]   and then realize how far your research
[00:31:51.820 --> 00:31:53.860]   is probably away from that today.
[00:31:53.860 --> 00:31:56.820]   At the very least, that's a good thing
[00:31:56.820 --> 00:31:59.300]   for academics to deliberate on.
[00:31:59.300 --> 00:32:00.900]   The other thing that's really nice in Silicon Valley
[00:32:00.900 --> 00:32:04.380]   is Silicon Valley is full of smart people outside academia.
[00:32:04.380 --> 00:32:06.340]   So there's the Larry Pages and Mark Zuckerbergs
[00:32:06.340 --> 00:32:09.020]   in the world who are anywhere as smart or smarter
[00:32:09.020 --> 00:32:11.460]   than the best academics I've met in my life.
[00:32:11.460 --> 00:32:15.420]   And what they do is they are at a different level.
[00:32:15.420 --> 00:32:16.740]   They build the systems,
[00:32:16.740 --> 00:32:19.340]   they build the customer-facing systems,
[00:32:19.340 --> 00:32:21.980]   they build things that people can use
[00:32:21.980 --> 00:32:23.820]   without technical education.
[00:32:23.820 --> 00:32:25.860]   And they are inspired by research,
[00:32:25.860 --> 00:32:27.540]   they're inspired by scientists.
[00:32:27.540 --> 00:32:30.340]   They hire the best PhDs from the best universities
[00:32:30.340 --> 00:32:31.180]   for a reason.
[00:32:31.180 --> 00:32:35.140]   So I think there's kind of vertical integration
[00:32:35.140 --> 00:32:37.780]   between the real product, the real impact,
[00:32:37.780 --> 00:32:39.860]   and the real thought, the real ideas.
[00:32:39.860 --> 00:32:41.300]   That's actually working surprisingly well
[00:32:41.300 --> 00:32:42.780]   in Silicon Valley.
[00:32:42.780 --> 00:32:44.900]   It did not work as well in other places in this nation.
[00:32:44.900 --> 00:32:46.700]   So when I worked at Carnegie Mellon,
[00:32:46.700 --> 00:32:49.860]   we had the world's finest computer science university,
[00:32:49.860 --> 00:32:52.780]   but there wasn't those people in Pittsburgh
[00:32:52.780 --> 00:32:55.140]   that would be able to take these very fine
[00:32:55.140 --> 00:32:57.140]   computer science ideas and turn them into
[00:32:57.140 --> 00:33:00.620]   massively impactful products.
[00:33:00.620 --> 00:33:03.900]   That symbiosis seemed to exist pretty much only
[00:33:03.900 --> 00:33:06.620]   in Silicon Valley and maybe a bit in Boston and Austin.
[00:33:06.620 --> 00:33:07.980]   - Yeah, with Stanford.
[00:33:07.980 --> 00:33:11.100]   That's really interesting.
[00:33:11.100 --> 00:33:14.060]   So if we look a little bit further on
[00:33:14.060 --> 00:33:17.180]   from the DARPA Grand Challenge
[00:33:17.180 --> 00:33:20.100]   and the launch of the Google self-driving car,
[00:33:20.100 --> 00:33:22.060]   what do you see as the state,
[00:33:22.060 --> 00:33:25.900]   the challenges of autonomous vehicles as they are now?
[00:33:25.900 --> 00:33:29.220]   Is actually achieving that huge scale
[00:33:29.220 --> 00:33:31.700]   and having a huge impact on society?
[00:33:31.700 --> 00:33:35.300]   - I'm extremely proud of what has been accomplished.
[00:33:35.300 --> 00:33:37.620]   And again, I'm taking a lot of credit for the work that I do.
[00:33:37.620 --> 00:33:38.460]   (laughing)
[00:33:38.460 --> 00:33:40.220]   And I'm actually very optimistic
[00:33:40.220 --> 00:33:42.380]   and people have been kind of worrying,
[00:33:42.380 --> 00:33:43.220]   is it too fast?
[00:33:43.220 --> 00:33:44.060]   Is it too slow?
[00:33:44.060 --> 00:33:44.900]   Why is it not there yet?
[00:33:44.900 --> 00:33:45.900]   And so on.
[00:33:45.900 --> 00:33:48.860]   It is actually quite an interesting hard problem.
[00:33:48.860 --> 00:33:51.660]   And in that a self-driving car,
[00:33:51.660 --> 00:33:55.340]   to build one that manages 90% of the problems
[00:33:55.340 --> 00:33:57.260]   and count them every day driving is easy.
[00:33:57.260 --> 00:33:59.500]   We can literally do this over a weekend.
[00:33:59.500 --> 00:34:02.100]   To do 99% might take a month.
[00:34:02.100 --> 00:34:03.260]   Then there's 1% left.
[00:34:03.260 --> 00:34:05.660]   So 1% would mean that you still
[00:34:05.660 --> 00:34:08.140]   have a fatal accident every week.
[00:34:08.140 --> 00:34:09.020]   Very unacceptable.
[00:34:09.020 --> 00:34:10.940]   So now you work on this 1%
[00:34:10.940 --> 00:34:12.900]   and the 99% of that,
[00:34:12.900 --> 00:34:15.780]   the remaining 1% is actually still relatively easy.
[00:34:15.780 --> 00:34:18.220]   But now you're down to like a hundredth of 1%
[00:34:18.220 --> 00:34:21.580]   and it's still completely unacceptable in terms of safety.
[00:34:21.580 --> 00:34:24.260]   So the variety of things you encounter are just enormous.
[00:34:24.260 --> 00:34:26.500]   And that gives me enormous respect for human beings
[00:34:26.500 --> 00:34:30.220]   that we're able to deal with the couch on the highway,
[00:34:30.220 --> 00:34:31.060]   right?
[00:34:31.060 --> 00:34:31.900]   Or the deer in the headlight
[00:34:31.900 --> 00:34:34.940]   or the blown tire that we've never been trained for.
[00:34:34.940 --> 00:34:35.980]   And all of a sudden I have to handle it
[00:34:35.980 --> 00:34:37.140]   in an emergency situation
[00:34:37.140 --> 00:34:38.780]   and often do very, very successfully.
[00:34:38.780 --> 00:34:40.700]   It's amazing from that perspective
[00:34:40.700 --> 00:34:42.220]   how safe driving actually is
[00:34:42.220 --> 00:34:44.860]   given how many millions of miles we drive
[00:34:44.860 --> 00:34:46.220]   every year in this country.
[00:34:46.220 --> 00:34:48.500]   We are now at a point
[00:34:48.500 --> 00:34:50.060]   where I believe the technology is there.
[00:34:50.060 --> 00:34:51.620]   And I've seen it.
[00:34:51.620 --> 00:34:52.540]   I've seen it in Waymo.
[00:34:52.540 --> 00:34:53.580]   I've seen it in Aptiv.
[00:34:53.580 --> 00:34:55.460]   I've seen it in Cruise
[00:34:55.460 --> 00:34:58.300]   and in a number of companies and Voyage
[00:34:58.300 --> 00:35:00.980]   where vehicles not driving around
[00:35:00.980 --> 00:35:04.260]   and basically flawlessly are able to drive people around
[00:35:04.260 --> 00:35:06.100]   in limited scenarios.
[00:35:06.100 --> 00:35:08.020]   In fact, you can go to Vegas today
[00:35:08.020 --> 00:35:09.940]   and order a summon a lift.
[00:35:09.940 --> 00:35:13.540]   And if you got the right setting of your app
[00:35:13.540 --> 00:35:15.820]   you'd be picked up by a driverless car.
[00:35:15.820 --> 00:35:18.100]   Now there's still safety drivers in there
[00:35:18.100 --> 00:35:21.340]   but that's a fantastic way to kind of learn
[00:35:21.340 --> 00:35:22.980]   what the limits of technology today.
[00:35:22.980 --> 00:35:24.740]   And there's still some glitches
[00:35:24.740 --> 00:35:26.580]   but the glitches have become very, very rare.
[00:35:26.580 --> 00:35:29.700]   I think the next step is gonna be to down cost it
[00:35:29.700 --> 00:35:31.300]   to harden it.
[00:35:31.300 --> 00:35:33.780]   The entrapment, the sensors
[00:35:33.780 --> 00:35:36.220]   are not quite an automotive grade standard yet.
[00:35:36.220 --> 00:35:37.820]   And then to really build the business models
[00:35:37.820 --> 00:35:40.980]   to really kind of go somewhere and make the business case.
[00:35:40.980 --> 00:35:42.580]   And the business case is hard work.
[00:35:42.580 --> 00:35:44.580]   It's not just, oh my God, we have this capability
[00:35:44.580 --> 00:35:45.540]   people are just gonna buy it.
[00:35:45.540 --> 00:35:46.740]   You have to make it affordable.
[00:35:46.740 --> 00:35:50.220]   You have to give people the,
[00:35:50.220 --> 00:35:52.300]   find the social acceptance of people.
[00:35:52.300 --> 00:35:54.460]   None of the teams yet has been able to
[00:35:54.460 --> 00:35:56.180]   or gutsy enough to drive around
[00:35:56.180 --> 00:35:59.260]   without a person inside the car.
[00:35:59.260 --> 00:36:01.300]   And that's the next magical hurdle.
[00:36:01.300 --> 00:36:03.820]   We'll be able to send these vehicles around
[00:36:03.820 --> 00:36:05.780]   completely empty in traffic.
[00:36:05.780 --> 00:36:08.140]   And I think, I mean, I wait every day,
[00:36:08.140 --> 00:36:10.700]   wait for the news that Waymo has just done this.
[00:36:10.700 --> 00:36:14.700]   - So, you know, it's interesting you mentioned gutsy.
[00:36:14.700 --> 00:36:19.700]   Let me ask some maybe unanswerable question,
[00:36:19.700 --> 00:36:24.460]   maybe edgy questions, but in terms of
[00:36:24.460 --> 00:36:28.700]   how much risk is required, some guts
[00:36:28.700 --> 00:36:30.340]   in terms of leadership style,
[00:36:30.340 --> 00:36:32.580]   it would be good to contrast approaches.
[00:36:32.580 --> 00:36:34.660]   And I don't think anyone knows what's right.
[00:36:34.660 --> 00:36:38.540]   But if we compare Tesla and Waymo, for example,
[00:36:38.540 --> 00:36:41.420]   Elon Musk and the Waymo team,
[00:36:41.420 --> 00:36:45.660]   there's slight differences in approach.
[00:36:45.660 --> 00:36:49.540]   So on the Elon side, there's more,
[00:36:49.540 --> 00:36:50.860]   I don't know what the right word to use,
[00:36:50.860 --> 00:36:53.900]   but aggression in terms of innovation.
[00:36:53.900 --> 00:36:58.900]   And on Waymo side, there's more sort of cautious,
[00:36:58.900 --> 00:37:03.460]   safety focused approach to the problem.
[00:37:03.460 --> 00:37:06.180]   What do you think it takes?
[00:37:06.180 --> 00:37:09.140]   What leadership at which moment is right?
[00:37:09.140 --> 00:37:10.660]   Which approach is right?
[00:37:10.660 --> 00:37:13.860]   - Look, I don't sit in either of those teams.
[00:37:13.860 --> 00:37:16.220]   So I'm unable to even verify,
[00:37:16.220 --> 00:37:17.980]   like somebody says, correct.
[00:37:17.980 --> 00:37:21.660]   In the end of the day, every innovator in that space
[00:37:21.660 --> 00:37:23.140]   will face a fundamental dilemma.
[00:37:23.140 --> 00:37:27.140]   And I would say you could put aerospace titans
[00:37:27.140 --> 00:37:30.220]   into the same bucket, which is you have to balance
[00:37:30.220 --> 00:37:34.300]   public safety with your drive to innovate.
[00:37:34.300 --> 00:37:36.780]   And this country in particular, in the States,
[00:37:36.780 --> 00:37:38.340]   has a hundred plus year history
[00:37:38.340 --> 00:37:40.620]   of doing this very successfully.
[00:37:40.620 --> 00:37:43.900]   Air travel is what a hundred times as safe per mile
[00:37:43.900 --> 00:37:46.620]   than ground travel, than cars.
[00:37:46.620 --> 00:37:48.500]   And there's a reason for it,
[00:37:48.500 --> 00:37:52.900]   because people have found ways to be very methodological
[00:37:52.900 --> 00:37:55.140]   about ensuring public safety,
[00:37:55.140 --> 00:37:56.940]   while still being able to make progress
[00:37:56.940 --> 00:37:59.060]   on important aspects, for example,
[00:37:59.060 --> 00:38:01.780]   like airline noise and fuel consumption.
[00:38:01.780 --> 00:38:06.180]   So I think that those practices are proven
[00:38:06.180 --> 00:38:07.900]   and they actually work.
[00:38:07.900 --> 00:38:09.900]   We live in a world safer than ever before.
[00:38:09.900 --> 00:38:11.900]   And yes, there will always be the provision
[00:38:11.900 --> 00:38:12.740]   that something goes wrong.
[00:38:12.740 --> 00:38:15.260]   There's always the possibility that someone makes a mistake
[00:38:15.260 --> 00:38:17.140]   or there's an unexpected failure.
[00:38:17.140 --> 00:38:20.980]   We can never guarantee to a hundred percent absolute safety
[00:38:20.980 --> 00:38:23.340]   rather than just not doing it.
[00:38:23.340 --> 00:38:27.100]   But I think I'm very proud of the history of the United States
[00:38:27.100 --> 00:38:30.140]   I mean, we've dealt with much more dangerous technology
[00:38:30.140 --> 00:38:32.740]   like nuclear energy and kept that safe too.
[00:38:32.740 --> 00:38:36.420]   We have nuclear weapons and we keep those safe.
[00:38:36.420 --> 00:38:39.460]   So we have methods and procedures
[00:38:39.460 --> 00:38:42.980]   that really balance these two things very, very successfully.
[00:38:42.980 --> 00:38:44.900]   - You've mentioned a lot of great autonomous vehicle
[00:38:44.900 --> 00:38:48.780]   companies that are taking sort of the level four, level five
[00:38:48.780 --> 00:38:51.900]   they jump in full autonomy with a safety driver
[00:38:51.900 --> 00:38:53.140]   and take that kind of approach.
[00:38:53.140 --> 00:38:55.780]   And also through simulation and so on.
[00:38:55.780 --> 00:38:59.580]   There's also the approach that Tesla Autopilot is doing
[00:38:59.580 --> 00:39:03.740]   which is kind of incrementally taking a level two vehicle
[00:39:03.740 --> 00:39:05.860]   and using machine learning and learning
[00:39:05.860 --> 00:39:10.580]   from the driving of human beings and trying to creep up
[00:39:10.580 --> 00:39:12.380]   trying to incrementally improve the system
[00:39:12.380 --> 00:39:15.580]   until it's able to achieve level four autonomy.
[00:39:15.580 --> 00:39:19.820]   So perfect autonomy in certain kind of geographical regions.
[00:39:19.820 --> 00:39:23.180]   What are your thoughts on these contrasting approaches?
[00:39:23.180 --> 00:39:25.620]   - Well, so first of all, I'm a very proud Tesla owner
[00:39:25.620 --> 00:39:27.900]   and I literally use the Autopilot every day
[00:39:27.900 --> 00:39:29.620]   and it literally has kept me safe.
[00:39:29.620 --> 00:39:33.980]   It is a beautiful technology specifically
[00:39:33.980 --> 00:39:37.660]   for highway driving when I'm slightly tired
[00:39:37.660 --> 00:39:42.260]   because then it turns me into a much safer driver
[00:39:42.260 --> 00:39:45.100]   and that I'm a hundred percent confident that's the case.
[00:39:46.100 --> 00:39:47.700]   In terms of the right approach,
[00:39:47.700 --> 00:39:49.900]   I think the biggest change I've seen
[00:39:49.900 --> 00:39:54.500]   since I ran the Waymo team is this thing called deep learning.
[00:39:54.500 --> 00:39:58.020]   Deep learning was not a hot topic when I started Waymo
[00:39:58.020 --> 00:39:59.420]   or Google self-driving cars.
[00:39:59.420 --> 00:40:01.740]   It was there, in fact, we started Google Brain
[00:40:01.740 --> 00:40:02.820]   at the same time in Google X.
[00:40:02.820 --> 00:40:05.980]   So I invested in deep learning, but people didn't talk
[00:40:05.980 --> 00:40:07.860]   about it, it wasn't a hot topic.
[00:40:07.860 --> 00:40:08.700]   And now it is.
[00:40:08.700 --> 00:40:10.380]   There's a shift of emphasis
[00:40:10.380 --> 00:40:12.460]   from a more geometric perspective
[00:40:12.460 --> 00:40:14.340]   where you use geometric sensors.
[00:40:14.340 --> 00:40:15.740]   They give you a full 3D view
[00:40:15.740 --> 00:40:17.300]   and you do a geometric reasoning about,
[00:40:17.300 --> 00:40:19.660]   oh, this box over here might be a car,
[00:40:19.660 --> 00:40:24.180]   towards a more human-like, oh, let's just learn about it.
[00:40:24.180 --> 00:40:26.540]   This looks like the thing I've seen 10,000 times before.
[00:40:26.540 --> 00:40:30.300]   So maybe it's the same thing, machine learning perspective.
[00:40:30.300 --> 00:40:32.180]   And that has really put, I think,
[00:40:32.180 --> 00:40:34.740]   all these approaches on steroids.
[00:40:34.740 --> 00:40:38.740]   At Udacity, we teach a course in self-driving cars.
[00:40:38.740 --> 00:40:43.740]   In fact, I think we've graduated over 20,000 or so people
[00:40:43.820 --> 00:40:45.020]   on self-driving car skills.
[00:40:45.020 --> 00:40:47.060]   So every self-driving car team
[00:40:47.060 --> 00:40:49.260]   in the world now uses our engineers.
[00:40:49.260 --> 00:40:51.900]   And in this course, the very first homework assignment
[00:40:51.900 --> 00:40:54.900]   is to do lane finding on images.
[00:40:54.900 --> 00:40:56.940]   And lane finding images for laymen,
[00:40:56.940 --> 00:40:59.060]   what this means is you put a camera into your car
[00:40:59.060 --> 00:40:59.900]   or you open your eyes
[00:40:59.900 --> 00:41:01.820]   and you wouldn't know where the lane is, right?
[00:41:01.820 --> 00:41:04.980]   So you can stay inside the lane with your car.
[00:41:04.980 --> 00:41:06.540]   Humans can do this super easily.
[00:41:06.540 --> 00:41:08.100]   You just look and you know where the lane is,
[00:41:08.100 --> 00:41:10.180]   just intuitively.
[00:41:10.180 --> 00:41:12.180]   For machines, for a long time, it was super hard
[00:41:12.180 --> 00:41:14.660]   because people would write these kind of crazy rules.
[00:41:14.660 --> 00:41:16.100]   If there's like one lane markers
[00:41:16.100 --> 00:41:17.660]   and here's what white really means,
[00:41:17.660 --> 00:41:19.100]   this is not quite white enough.
[00:41:19.100 --> 00:41:20.340]   So let's, oh, it's not white.
[00:41:20.340 --> 00:41:21.420]   Or maybe the sun is shining.
[00:41:21.420 --> 00:41:23.500]   So when the sun shines and this is white
[00:41:23.500 --> 00:41:24.660]   and this is a straight line,
[00:41:24.660 --> 00:41:25.700]   or maybe it's not quite a straight line
[00:41:25.700 --> 00:41:27.260]   because the road is curved.
[00:41:27.260 --> 00:41:29.260]   And do we know that there's really six feet
[00:41:29.260 --> 00:41:32.220]   between lane markings or not, or 12 feet, whatever it is.
[00:41:32.220 --> 00:41:36.260]   And now, what the students are doing,
[00:41:36.260 --> 00:41:37.380]   they would take machine learning.
[00:41:37.380 --> 00:41:39.620]   So instead of like writing these crazy rules
[00:41:39.620 --> 00:41:40.820]   for the lane marker, it's just say,
[00:41:40.820 --> 00:41:42.700]   hey, let's take an hour of driving
[00:41:42.700 --> 00:41:44.420]   and label it and tell the vehicle
[00:41:44.420 --> 00:41:45.780]   this is actually the lane by hand.
[00:41:45.780 --> 00:41:47.380]   And then these are examples
[00:41:47.380 --> 00:41:49.420]   and have the machine find its own rules
[00:41:49.420 --> 00:41:51.420]   what lane markings are.
[00:41:51.420 --> 00:41:53.820]   And within 24 hours, now every student
[00:41:53.820 --> 00:41:56.060]   that's never done any programming before in this space
[00:41:56.060 --> 00:41:58.340]   can write a perfect lane finder
[00:41:58.340 --> 00:42:00.900]   as good as the best commercial lane finders.
[00:42:00.900 --> 00:42:02.780]   And that's completely amazing to me.
[00:42:02.780 --> 00:42:05.540]   We've seen progress using machine learning
[00:42:05.540 --> 00:42:08.180]   that completely dwarfs anything
[00:42:08.180 --> 00:42:09.940]   that I saw 10 years ago.
[00:42:10.940 --> 00:42:12.820]   - Yeah, and just as a side note,
[00:42:12.820 --> 00:42:15.220]   the self-driving car nanodegree,
[00:42:15.220 --> 00:42:18.900]   the fact that you launched that many years ago now,
[00:42:18.900 --> 00:42:20.100]   maybe four years ago.
[00:42:20.100 --> 00:42:20.940]   - Three years ago.
[00:42:20.940 --> 00:42:22.060]   - Three years ago is incredible.
[00:42:22.060 --> 00:42:24.740]   That's a great example of system level thinking.
[00:42:24.740 --> 00:42:27.580]   So just taking an entire course that teaches you
[00:42:27.580 --> 00:42:29.260]   how to solve the entire problem,
[00:42:29.260 --> 00:42:31.220]   I definitely recommend people.
[00:42:31.220 --> 00:42:32.380]   - It's become super popular
[00:42:32.380 --> 00:42:34.220]   and it's become actually incredibly high quality.
[00:42:34.220 --> 00:42:35.260]   We've been with Mercedes
[00:42:35.260 --> 00:42:38.060]   and various other companies in that space.
[00:42:38.060 --> 00:42:40.620]   And we find that engineers from Tesla and Waymo
[00:42:40.620 --> 00:42:41.980]   are taking it to bay.
[00:42:41.980 --> 00:42:45.500]   The insight was that two things,
[00:42:45.500 --> 00:42:49.220]   one is existing universities will be very slow to move
[00:42:49.220 --> 00:42:50.500]   because the department lies
[00:42:50.500 --> 00:42:52.380]   and there's no department for self-driving cars.
[00:42:52.380 --> 00:42:56.260]   So between McGee and EE and computer science,
[00:42:56.260 --> 00:42:57.700]   getting those folks together into one room
[00:42:57.700 --> 00:42:59.660]   is really, really hard.
[00:42:59.660 --> 00:43:01.260]   And every professor listening here will know,
[00:43:01.260 --> 00:43:02.940]   they'll probably agree to that.
[00:43:02.940 --> 00:43:06.460]   And secondly, even if all the great universities
[00:43:06.460 --> 00:43:08.340]   did this, which none so far
[00:43:08.340 --> 00:43:11.060]   has developed a curriculum in this field,
[00:43:11.060 --> 00:43:13.660]   it is just a few thousand students that can partake
[00:43:13.660 --> 00:43:16.180]   because all the great universities are super selective.
[00:43:16.180 --> 00:43:18.060]   So how about people in India?
[00:43:18.060 --> 00:43:20.580]   How about people in China or in the Middle East
[00:43:20.580 --> 00:43:23.380]   or Indonesia or Africa?
[00:43:23.380 --> 00:43:25.100]   Why should those be excluded
[00:43:25.100 --> 00:43:27.180]   from the skill of building self-driving cars?
[00:43:27.180 --> 00:43:28.380]   Are they any dumber than we are?
[00:43:28.380 --> 00:43:30.140]   Are they any less privileged?
[00:43:30.140 --> 00:43:34.780]   And the answer is, we should just give everybody the skill
[00:43:34.780 --> 00:43:35.820]   to build a self-driving car.
[00:43:35.820 --> 00:43:37.380]   Because if we do this,
[00:43:37.380 --> 00:43:40.300]   then we have like a thousand self-driving car startups.
[00:43:40.300 --> 00:43:42.900]   And if 10% succeed, that's like a hundred,
[00:43:42.900 --> 00:43:44.140]   that means a hundred countries now
[00:43:44.140 --> 00:43:46.740]   will have self-driving cars and be safer.
[00:43:46.740 --> 00:43:48.940]   - It's kind of interesting to imagine,
[00:43:48.940 --> 00:43:52.500]   impossible to quantify, but the number,
[00:43:52.500 --> 00:43:54.980]   the, you know, over a period of several decades,
[00:43:54.980 --> 00:43:57.900]   the impact that has, like a single course,
[00:43:57.900 --> 00:43:59.780]   like a ripple effect of society.
[00:43:59.780 --> 00:44:02.900]   I just recently talked to Andrew
[00:44:02.900 --> 00:44:05.340]   and who was creator of Cosmos.
[00:44:05.340 --> 00:44:08.180]   So it's a show, it's interesting to think about
[00:44:08.180 --> 00:44:10.700]   how many scientists that show launched.
[00:44:10.700 --> 00:44:15.580]   And so it's really, in terms of impact,
[00:44:15.580 --> 00:44:17.180]   I can't imagine a better course
[00:44:17.180 --> 00:44:18.620]   than the self-driving car course.
[00:44:18.620 --> 00:44:21.820]   That's, you know, there's other more specific disciplines
[00:44:21.820 --> 00:44:24.100]   like deep learning and so on that Udacity is also teaching,
[00:44:24.100 --> 00:44:25.900]   but self-driving cars, it's really,
[00:44:25.900 --> 00:44:26.860]   really interesting course.
[00:44:26.860 --> 00:44:28.420]   - Yeah, and it came at the right moment.
[00:44:28.420 --> 00:44:31.700]   It came at a time when there were a bunch of acqui-hires.
[00:44:31.700 --> 00:44:34.180]   Acqui-hire is acquisition of a company,
[00:44:34.180 --> 00:44:36.380]   not for its technology or its products or business,
[00:44:36.380 --> 00:44:38.300]   but for its people.
[00:44:38.300 --> 00:44:40.620]   So acqui-hire means maybe the company of 70 people,
[00:44:40.620 --> 00:44:43.180]   they have no product yet, but they're super smart people
[00:44:43.180 --> 00:44:44.340]   and they pay a certain amount of money.
[00:44:44.340 --> 00:44:48.420]   So I took acqui-hires like GM Cruise and Uber and others
[00:44:48.420 --> 00:44:52.220]   and did the math and said, hey, how many people are there
[00:44:52.220 --> 00:44:53.780]   and how much money was paid?
[00:44:53.780 --> 00:44:56.980]   And as a lower bound, I estimated the value
[00:44:56.980 --> 00:45:00.420]   of a self-driving car engineer in these acquisitions
[00:45:00.420 --> 00:45:02.220]   to be at least $10 million, right?
[00:45:02.220 --> 00:45:05.060]   So think about this, you get yourself a skill
[00:45:05.060 --> 00:45:06.700]   and you team up and build a company
[00:45:06.700 --> 00:45:09.820]   and your worth now is $10 million.
[00:45:09.820 --> 00:45:10.820]   I mean, that's kind of cool.
[00:45:10.820 --> 00:45:13.420]   I mean, what other thing could you do in life
[00:45:13.420 --> 00:45:15.940]   to be worth $10 million within a year?
[00:45:15.940 --> 00:45:17.620]   - Yeah, amazing.
[00:45:17.620 --> 00:45:21.020]   But to come back for a moment onto deep learning
[00:45:21.020 --> 00:45:23.760]   and its application in autonomous vehicles,
[00:45:23.760 --> 00:45:28.500]   what are your thoughts on Elon Musk's statement,
[00:45:28.500 --> 00:45:31.100]   provocative statement perhaps, that lighter is a crutch?
[00:45:31.100 --> 00:45:34.060]   So this geometric way of thinking about the world
[00:45:34.060 --> 00:45:38.980]   may be holding us back if what we should instead be doing
[00:45:38.980 --> 00:45:41.020]   in this robotics, in this particular space
[00:45:41.020 --> 00:45:46.020]   of autonomous vehicles is using camera as a primary sensor
[00:45:46.020 --> 00:45:48.260]   and using computer vision and machine learning
[00:45:48.260 --> 00:45:49.780]   as the primary way to--
[00:45:49.780 --> 00:45:50.620]   - Look, I have two comments.
[00:45:50.620 --> 00:45:55.420]   I think first of all, we all know that people can drive cars
[00:45:55.420 --> 00:45:59.020]   without lighters in their hands because we only have eyes
[00:45:59.020 --> 00:46:02.140]   and we mostly just use eyes for driving.
[00:46:02.140 --> 00:46:04.620]   Maybe we use some other perception about our bodies,
[00:46:04.620 --> 00:46:06.720]   accelerations, occasionally our ears,
[00:46:06.720 --> 00:46:10.740]   certainly not our noses. (laughs)
[00:46:10.740 --> 00:46:12.500]   So the existence proof is there
[00:46:12.500 --> 00:46:14.640]   that eyes must be sufficient.
[00:46:14.640 --> 00:46:17.980]   In fact, we could even drive a car
[00:46:17.980 --> 00:46:19.480]   if someone put a camera out
[00:46:19.480 --> 00:46:23.460]   and then gave us the camera image with no latency,
[00:46:23.460 --> 00:46:26.380]   we would be able to drive a car that way, the same way.
[00:46:26.380 --> 00:46:28.780]   So a camera is also sufficient.
[00:46:28.780 --> 00:46:31.860]   Secondly, I really love the idea that in the Western world,
[00:46:31.860 --> 00:46:33.620]   we have many, many different people
[00:46:33.620 --> 00:46:35.700]   trying different hypotheses.
[00:46:35.700 --> 00:46:37.340]   It's almost like an ant hill.
[00:46:37.340 --> 00:46:39.580]   An ant hill tries to forge for food, right?
[00:46:39.580 --> 00:46:40.820]   You can sit there as two ants
[00:46:40.820 --> 00:46:42.580]   and agree what the perfect path is
[00:46:42.580 --> 00:46:44.060]   and then every single ant marches
[00:46:44.060 --> 00:46:46.380]   for the most likely location of food is
[00:46:46.380 --> 00:46:47.980]   or you can have them just spread out.
[00:46:47.980 --> 00:46:50.500]   And I promise you the spread out solution will be better
[00:46:50.500 --> 00:46:54.020]   because if the disgusting philosophical,
[00:46:54.020 --> 00:46:55.620]   intellectual ants get it wrong
[00:46:55.620 --> 00:46:56.980]   and they're all moving the wrong direction,
[00:46:56.980 --> 00:46:58.340]   they're gonna waste the day
[00:46:58.340 --> 00:47:00.540]   and then they're gonna discuss again for another week.
[00:47:00.540 --> 00:47:02.500]   Whereas if all these ants go in a random direction,
[00:47:02.500 --> 00:47:03.540]   someone's gonna succeed
[00:47:03.540 --> 00:47:05.620]   and they're gonna come back and claim victory
[00:47:05.620 --> 00:47:08.700]   and get the Nobel prize or whatever the ant equivalent is.
[00:47:08.700 --> 00:47:10.580]   And then they all march in the same direction.
[00:47:10.580 --> 00:47:11.860]   And that's great about society.
[00:47:11.860 --> 00:47:13.220]   That's great about the Western society.
[00:47:13.220 --> 00:47:15.540]   We're not plan-based, we're not central-based.
[00:47:15.540 --> 00:47:19.140]   We don't have a Soviet Union style central government
[00:47:19.140 --> 00:47:21.020]   that tells us where to forge.
[00:47:21.020 --> 00:47:21.860]   We just forge.
[00:47:21.860 --> 00:47:23.140]   We start in C-Corp.
[00:47:23.140 --> 00:47:25.860]   We get investor money, go out and try it out.
[00:47:25.860 --> 00:47:27.500]   And who knows who's gonna win?
[00:47:27.820 --> 00:47:28.740]   (laughs)
[00:47:28.740 --> 00:47:29.580]   - I like it.
[00:47:29.580 --> 00:47:33.500]   When you look at the long-term vision
[00:47:33.500 --> 00:47:35.220]   of autonomous vehicles,
[00:47:35.220 --> 00:47:36.940]   do you see machine learning
[00:47:36.940 --> 00:47:39.620]   as fundamentally being able to solve most of the problems?
[00:47:39.620 --> 00:47:42.300]   So learning from experience.
[00:47:42.300 --> 00:47:44.260]   - I'd say we should be very clear
[00:47:44.260 --> 00:47:46.100]   about what machine learning is and is not.
[00:47:46.100 --> 00:47:48.180]   And I think there's a lot of confusion.
[00:47:48.180 --> 00:47:50.900]   What it is today is a technology
[00:47:50.900 --> 00:47:54.700]   that can go through large databases
[00:47:54.700 --> 00:47:59.700]   of repetitive patterns and find those patterns.
[00:47:59.700 --> 00:48:03.580]   So an example, we did a study at Stanford two years ago
[00:48:03.580 --> 00:48:05.460]   where we applied machine learning
[00:48:05.460 --> 00:48:07.900]   to detecting skin cancer in images.
[00:48:07.900 --> 00:48:10.780]   And we harvested or built a data set
[00:48:10.780 --> 00:48:15.100]   of 129,000 skin photo shots
[00:48:15.100 --> 00:48:19.460]   that all had been biopsied for what the actual situation was.
[00:48:19.460 --> 00:48:22.700]   And those included melanomas and carcinomas,
[00:48:22.700 --> 00:48:26.460]   also included rashes and other skin conditions, lesions.
[00:48:26.460 --> 00:48:30.740]   And then we had a network find those patterns
[00:48:30.740 --> 00:48:34.540]   and it was by and large able to then detect skin cancer
[00:48:34.540 --> 00:48:36.700]   with an iPhone as accurately
[00:48:36.700 --> 00:48:41.380]   as the best board-certified Stanford-level dermatologist.
[00:48:41.380 --> 00:48:42.780]   We proved that.
[00:48:42.780 --> 00:48:45.940]   Now this thing was great in this one thing,
[00:48:45.940 --> 00:48:48.540]   finding skin cancer, but it couldn't drive a car.
[00:48:48.540 --> 00:48:51.580]   So the difference to human intelligence
[00:48:51.580 --> 00:48:53.260]   is we do all these many, many things
[00:48:53.260 --> 00:48:56.700]   and we can often learn from a very small data set
[00:48:56.700 --> 00:48:59.580]   of experiences, whereas machines still need
[00:48:59.580 --> 00:49:03.340]   very large data sets and things that would be very repetitive.
[00:49:03.340 --> 00:49:04.660]   Now that's still super impactful
[00:49:04.660 --> 00:49:06.420]   because almost everything we do is repetitive.
[00:49:06.420 --> 00:49:09.980]   So that's gonna really transform human labor,
[00:49:09.980 --> 00:49:13.100]   but it's not this almighty general intelligence.
[00:49:13.100 --> 00:49:15.300]   We're really far away from a system
[00:49:15.300 --> 00:49:17.300]   that would exhibit general intelligence.
[00:49:17.300 --> 00:49:21.300]   To that end, I actually commiserate the naming a little bit
[00:49:21.300 --> 00:49:24.420]   because artificial intelligence, if you believe Hollywood,
[00:49:24.420 --> 00:49:27.300]   is immediately mixed into the idea of human suppression
[00:49:27.300 --> 00:49:30.340]   and machine superiority.
[00:49:30.340 --> 00:49:32.940]   I don't think that we're gonna see this in my lifetime.
[00:49:32.940 --> 00:49:35.460]   I don't think human suppression is a good idea.
[00:49:35.460 --> 00:49:37.420]   I don't see it coming.
[00:49:37.420 --> 00:49:39.700]   I don't see the technology being there.
[00:49:39.700 --> 00:49:42.300]   What I see instead is a very pointed,
[00:49:42.300 --> 00:49:44.300]   focused pattern recognition technology
[00:49:44.300 --> 00:49:48.420]   that's able to extract patterns from large data sets.
[00:49:48.420 --> 00:49:53.420]   And in doing so, it can be super impactful, super impactful.
[00:49:53.420 --> 00:49:55.900]   Let's take the impact of artificial intelligence
[00:49:55.900 --> 00:49:57.620]   on human work.
[00:49:57.620 --> 00:50:00.540]   We all know that it takes something like 10,000 hours
[00:50:00.540 --> 00:50:01.540]   to become an expert.
[00:50:01.540 --> 00:50:04.260]   If you're gonna be a doctor or a lawyer
[00:50:04.260 --> 00:50:06.300]   or even a really good driver,
[00:50:06.300 --> 00:50:09.500]   it takes a certain amount of time to become experts.
[00:50:09.500 --> 00:50:12.220]   Machines now are able and have been shown
[00:50:12.220 --> 00:50:16.700]   to observe people become experts and observe experts
[00:50:16.700 --> 00:50:18.460]   and then extract those rules from experts
[00:50:18.460 --> 00:50:23.100]   in some interesting way that could go from law to sales
[00:50:23.100 --> 00:50:28.100]   to driving cars to diagnosing cancer
[00:50:28.100 --> 00:50:31.460]   and then giving that capability to people
[00:50:31.460 --> 00:50:33.060]   who are completely new in their job.
[00:50:33.060 --> 00:50:35.460]   We now can, and that's been done.
[00:50:35.460 --> 00:50:38.460]   It's been done commercially in many, many instantiations.
[00:50:38.460 --> 00:50:40.780]   So that means we can use machine learning
[00:50:40.780 --> 00:50:45.500]   to make people expert on the very first day of their work.
[00:50:45.500 --> 00:50:46.540]   Like think about the impact.
[00:50:46.540 --> 00:50:51.020]   If your doctor is still in their first 10,000 hours,
[00:50:51.020 --> 00:50:53.780]   you have a doctor who's not quite an expert yet.
[00:50:53.780 --> 00:50:57.340]   Who would not want a doctor who is the world's best expert?
[00:50:57.340 --> 00:50:59.180]   And now we can leverage machines
[00:50:59.180 --> 00:51:02.700]   to really eradicate error in decision-making,
[00:51:02.700 --> 00:51:06.180]   error in lack of expertise for human doctors.
[00:51:06.180 --> 00:51:08.340]   That could save your life.
[00:51:08.340 --> 00:51:10.300]   - If we can link on that for a little bit,
[00:51:10.300 --> 00:51:14.780]   in which way do you hope machines in the medical field
[00:51:14.780 --> 00:51:16.340]   could help assist doctors?
[00:51:16.340 --> 00:51:21.300]   You mentioned this sort of accelerating the learning curve
[00:51:21.300 --> 00:51:24.500]   or people, if they start a job,
[00:51:24.500 --> 00:51:27.340]   or in the first 10,000 hours can be assisted by machines.
[00:51:27.340 --> 00:51:29.740]   How do you envision that assistance looking?
[00:51:29.740 --> 00:51:32.340]   - So we built this app for an iPhone
[00:51:32.340 --> 00:51:36.300]   that can detect and classify and diagnose skin cancer.
[00:51:36.300 --> 00:51:40.540]   And we proved two years ago that it does pretty much
[00:51:40.540 --> 00:51:42.220]   as good or better than the best human doctors.
[00:51:42.220 --> 00:51:43.580]   So let me tell you a story.
[00:51:43.580 --> 00:51:45.460]   So there's a friend of mine, let's call him Ben.
[00:51:45.460 --> 00:51:47.660]   Ben is a very famous venture capitalist.
[00:51:47.660 --> 00:51:50.700]   He goes to his doctor and the doctor looks at a mole
[00:51:50.700 --> 00:51:55.340]   and says, "Hey, that mole is probably harmless."
[00:51:55.340 --> 00:51:58.660]   And for some very funny reason,
[00:51:58.660 --> 00:52:00.420]   he pulls out that phone with our app.
[00:52:00.420 --> 00:52:02.620]   He's a collaborator in our study.
[00:52:02.620 --> 00:52:06.260]   And the app says, "No, no, no, no, this is a melanoma."
[00:52:06.260 --> 00:52:08.660]   And for background, melanomas are,
[00:52:08.660 --> 00:52:12.380]   and skin cancer is the most common cancer in this country.
[00:52:12.380 --> 00:52:16.620]   Melanomas can go from stage zero to stage four
[00:52:16.620 --> 00:52:18.100]   within less than a year.
[00:52:18.100 --> 00:52:20.860]   Stage zero means you can basically cut it out yourself
[00:52:20.860 --> 00:52:23.180]   with a kitchen knife and be safe.
[00:52:23.180 --> 00:52:25.300]   And stage four means your chances of living
[00:52:25.300 --> 00:52:27.980]   for five more years are less than 20%.
[00:52:27.980 --> 00:52:31.180]   So it's a very serious, serious, serious condition.
[00:52:31.180 --> 00:52:36.140]   So this doctor who took out the iPhone
[00:52:36.140 --> 00:52:37.700]   looked at the iPhone and was a little bit puzzled.
[00:52:37.700 --> 00:52:39.700]   He said, "I mean, but just to be safe,
[00:52:39.700 --> 00:52:41.580]   let's cut it out and biopsy it."
[00:52:41.580 --> 00:52:43.220]   That's the technical term for it.
[00:52:43.220 --> 00:52:45.380]   Let's get an in-depth diagnostics
[00:52:45.380 --> 00:52:47.660]   that is more than just looking at it.
[00:52:47.660 --> 00:52:50.740]   And it came back as cancerous, as a melanoma.
[00:52:50.740 --> 00:52:52.180]   And it was then removed.
[00:52:52.180 --> 00:52:54.900]   And my friend Ben, I was hiking with him
[00:52:54.900 --> 00:52:56.220]   and we were talking about AI.
[00:52:56.220 --> 00:52:58.820]   And I said, I told him I do this book on skin cancer.
[00:52:58.820 --> 00:53:00.660]   And he said, "Oh, funny.
[00:53:00.660 --> 00:53:03.740]   My doctor just had an iPhone that found my cancer."
[00:53:03.740 --> 00:53:04.580]   (both laughing)
[00:53:04.580 --> 00:53:05.420]   Wow.
[00:53:05.420 --> 00:53:06.860]   So I was like completely intrigued.
[00:53:06.860 --> 00:53:08.140]   I didn't even know about this.
[00:53:08.140 --> 00:53:09.420]   So here's a person, I mean,
[00:53:09.420 --> 00:53:11.260]   this is a real human life, right?
[00:53:11.260 --> 00:53:12.100]   - Yes.
[00:53:12.100 --> 00:53:12.940]   - Now who doesn't know somebody
[00:53:12.940 --> 00:53:13.980]   who has been affected by cancer?
[00:53:13.980 --> 00:53:16.100]   Cancer is cause of death number two.
[00:53:16.100 --> 00:53:19.420]   Cancer is this kind of disease that is mean
[00:53:19.420 --> 00:53:21.020]   in the following way.
[00:53:21.020 --> 00:53:24.460]   Most cancers can actually be cured relatively easily
[00:53:24.460 --> 00:53:25.820]   if we catch them early.
[00:53:25.820 --> 00:53:28.300]   And the reason why we don't tend to catch them early
[00:53:28.300 --> 00:53:30.540]   is because they have no symptoms.
[00:53:30.540 --> 00:53:33.820]   Like your very first symptom of a gallbladder cancer
[00:53:33.820 --> 00:53:37.020]   or a pancreas cancer might be a headache.
[00:53:37.020 --> 00:53:38.620]   And when you finally go to your doctor
[00:53:38.620 --> 00:53:41.580]   because of these headaches or your back pain
[00:53:41.580 --> 00:53:45.860]   and you're being imaged, it's usually stage four plus.
[00:53:45.860 --> 00:53:48.180]   And that's the time when the occurring chances
[00:53:48.180 --> 00:53:50.820]   might be dropped to a single digit percentage.
[00:53:50.820 --> 00:53:54.500]   So if we could leverage AI to inspect your body
[00:53:54.500 --> 00:53:58.060]   on a regular basis without even a doctor in the room,
[00:53:58.060 --> 00:54:00.300]   maybe when you take a shower or what have you,
[00:54:00.300 --> 00:54:01.420]   I know this sounds creepy,
[00:54:01.420 --> 00:54:03.740]   but then we might be able to save millions
[00:54:03.740 --> 00:54:04.860]   and millions of lives.
[00:54:06.260 --> 00:54:09.420]   - You've mentioned there's a concern that people have
[00:54:09.420 --> 00:54:12.780]   about near-term impacts of AI in terms of job loss.
[00:54:12.780 --> 00:54:15.460]   So you've mentioned being able to assist doctors,
[00:54:15.460 --> 00:54:17.860]   being able to assist people in their jobs.
[00:54:17.860 --> 00:54:21.060]   Do you have a worry of people losing their jobs
[00:54:21.060 --> 00:54:25.420]   or the economy being affected by the improvements in AI?
[00:54:25.420 --> 00:54:27.660]   - Yeah, anybody concerned about job losses,
[00:54:27.660 --> 00:54:30.020]   please come to Gerdasady.com.
[00:54:30.020 --> 00:54:32.300]   We teach contemporary tech skills
[00:54:32.300 --> 00:54:35.780]   and we have our kind of implicit job promise.
[00:54:36.740 --> 00:54:38.700]   We often, when we measure,
[00:54:38.700 --> 00:54:41.860]   we spend way over 50% of our graduates in new jobs
[00:54:41.860 --> 00:54:43.780]   and they're very satisfied about it.
[00:54:43.780 --> 00:54:44.860]   And it costs almost nothing,
[00:54:44.860 --> 00:54:47.180]   costs like 1,500 max or something like that.
[00:54:47.180 --> 00:54:48.980]   - And I saw there's a cool new program
[00:54:48.980 --> 00:54:51.180]   that you agree with the US government,
[00:54:51.180 --> 00:54:54.940]   guaranteeing that you will help give scholarships
[00:54:54.940 --> 00:54:57.900]   that educate people in this kind of situation.
[00:54:57.900 --> 00:55:00.020]   - Yeah, we're working with the US government
[00:55:00.020 --> 00:55:03.900]   on the idea of basically rebuilding the American dream.
[00:55:03.900 --> 00:55:07.460]   So Udacity has just dedicated 100,000 scholarships
[00:55:07.460 --> 00:55:12.100]   for citizens of America for various levels of courses
[00:55:12.100 --> 00:55:15.620]   that eventually will get you a job.
[00:55:15.620 --> 00:55:18.740]   And those courses are all somewhat related
[00:55:18.740 --> 00:55:20.500]   to the tech sector because the tech sector
[00:55:20.500 --> 00:55:22.100]   is kind of the hottest sector right now.
[00:55:22.100 --> 00:55:24.980]   And they range from inter-level digital marketing
[00:55:24.980 --> 00:55:28.100]   to very advanced self-driving car engineering.
[00:55:28.100 --> 00:55:29.460]   And we're doing this with the White House
[00:55:29.460 --> 00:55:30.900]   because we think it's bipartisan.
[00:55:30.900 --> 00:55:35.900]   It's an issue that if you wanna really make America great,
[00:55:35.900 --> 00:55:40.060]   being able to be a part of the solution
[00:55:40.060 --> 00:55:43.020]   and live the American dream requires us
[00:55:43.020 --> 00:55:45.780]   to be proactive about our education and our skillset.
[00:55:45.780 --> 00:55:47.700]   It's just the way it is today.
[00:55:47.700 --> 00:55:48.700]   And it's always been this way.
[00:55:48.700 --> 00:55:49.940]   And we always had this American dream
[00:55:49.940 --> 00:55:51.140]   to send our kids to college.
[00:55:51.140 --> 00:55:53.260]   And now the American dream has to be
[00:55:53.260 --> 00:55:54.660]   to send ourselves to college.
[00:55:54.660 --> 00:55:58.220]   We can do this very, very, very efficiently
[00:55:58.220 --> 00:56:01.780]   and we can squeeze in the evenings and things to online.
[00:56:01.780 --> 00:56:03.140]   - At all ages.
[00:56:03.140 --> 00:56:03.980]   - All ages.
[00:56:03.980 --> 00:56:08.980]   So our learners go from age 11 to age 80.
[00:56:08.980 --> 00:56:14.060]   I just traveled Germany and the guy
[00:56:14.060 --> 00:56:17.940]   in the train compartment next to me was one of my students.
[00:56:17.940 --> 00:56:19.820]   It's like, wow, that's amazing.
[00:56:19.820 --> 00:56:21.300]   Think about impact.
[00:56:21.300 --> 00:56:24.020]   We've become the educator of choice for now,
[00:56:24.020 --> 00:56:26.500]   I believe officially six countries or five countries.
[00:56:26.500 --> 00:56:30.100]   Mostly in the Middle East, like Saudi Arabia and in Egypt.
[00:56:30.100 --> 00:56:33.420]   In Egypt, we just had a cohort graduate
[00:56:33.420 --> 00:56:37.300]   where we had 1100 high school students
[00:56:37.300 --> 00:56:40.340]   that went through programming skills proficient
[00:56:40.340 --> 00:56:42.940]   at the level of a computer science undergrad.
[00:56:42.940 --> 00:56:45.220]   And we had a 95% graduation rate
[00:56:45.220 --> 00:56:46.260]   even though everything's online.
[00:56:46.260 --> 00:56:48.260]   It's kind of tough, but we kind of trying to figure out
[00:56:48.260 --> 00:56:50.140]   how to make this effective.
[00:56:50.140 --> 00:56:52.540]   The vision is very, very simple.
[00:56:52.540 --> 00:56:57.540]   The vision is education ought to be a basic human right.
[00:56:57.540 --> 00:57:02.300]   It cannot be locked up behind ivory tower walls
[00:57:02.300 --> 00:57:03.620]   only for the rich people,
[00:57:03.620 --> 00:57:05.620]   for the parents who might be bribe themselves
[00:57:05.620 --> 00:57:08.380]   into the system and only for young people
[00:57:08.380 --> 00:57:10.460]   and only for people from the right demographics
[00:57:10.460 --> 00:57:14.220]   and the right geography and possibly even the right race.
[00:57:14.220 --> 00:57:15.820]   It has to be opened up to everybody.
[00:57:15.820 --> 00:57:18.700]   If we are truthful to the human mission,
[00:57:18.700 --> 00:57:20.620]   if we are truthful to our values,
[00:57:20.620 --> 00:57:23.420]   we're gonna open up education to everybody in the world.
[00:57:23.420 --> 00:57:27.180]   So Udacity's pledge of 100,000 scholarships,
[00:57:27.180 --> 00:57:29.180]   I think is the biggest pledge of scholarships ever
[00:57:29.180 --> 00:57:30.740]   in terms of numbers.
[00:57:30.740 --> 00:57:32.980]   And we're working, as I said, with the White House
[00:57:32.980 --> 00:57:35.460]   and with very accomplished CEOs
[00:57:35.460 --> 00:57:37.460]   like Tim Cook from Apple and others
[00:57:37.460 --> 00:57:40.100]   to really bring education to everywhere in the world.
[00:57:40.100 --> 00:57:44.580]   - Not to ask you to pick the favorite of your children,
[00:57:44.580 --> 00:57:46.620]   but at this point-- - Oh, that's Jasper.
[00:57:46.620 --> 00:57:47.460]   (laughing)
[00:57:47.460 --> 00:57:49.700]   I only have one that I know of.
[00:57:49.700 --> 00:57:50.540]   - Okay, good.
[00:57:50.540 --> 00:57:55.780]   In this particular moment, what nano degree,
[00:57:55.780 --> 00:58:00.020]   what set of courses are you most excited about at Udacity?
[00:58:00.020 --> 00:58:01.980]   Or is that too impossible to pick?
[00:58:01.980 --> 00:58:03.780]   - I've been super excited about something
[00:58:03.780 --> 00:58:05.460]   we haven't launched yet and we're building,
[00:58:05.460 --> 00:58:09.100]   which is when we talk to our partner companies,
[00:58:09.100 --> 00:58:12.700]   we have now a very strong footing in the enterprise world.
[00:58:12.700 --> 00:58:14.580]   And also to our students,
[00:58:14.580 --> 00:58:17.260]   we've kind of always focused on these hard skills
[00:58:17.260 --> 00:58:19.740]   like the programming skills or math skills
[00:58:19.740 --> 00:58:22.180]   or building skills or design skills.
[00:58:22.180 --> 00:58:25.180]   And a very common ask is soft skills.
[00:58:25.180 --> 00:58:26.860]   Like how do you behave in your work?
[00:58:26.860 --> 00:58:28.260]   How do you develop empathy?
[00:58:28.260 --> 00:58:29.540]   How do you work in a team?
[00:58:29.540 --> 00:58:32.380]   What are the very basics of management?
[00:58:32.380 --> 00:58:33.700]   How do you do time management?
[00:58:33.700 --> 00:58:36.260]   How do you advance your career
[00:58:36.260 --> 00:58:39.260]   in the context of a broader community?
[00:58:39.260 --> 00:58:41.740]   And that's something that we haven't done very well
[00:58:41.740 --> 00:58:43.860]   at Udacity and I would say most universities
[00:58:43.860 --> 00:58:45.180]   are doing very poorly as well
[00:58:45.180 --> 00:58:47.900]   because we are so obsessed with individual test scores
[00:58:47.900 --> 00:58:52.620]   and pay so little attention to teamwork in education.
[00:58:52.620 --> 00:58:55.500]   So that's something I see us moving into as a company
[00:58:55.500 --> 00:58:56.940]   because I'm excited about this.
[00:58:56.940 --> 00:59:00.100]   And I think, look, we can teach people tech skills
[00:59:00.100 --> 00:59:00.940]   and they're gonna be great.
[00:59:00.940 --> 00:59:02.700]   But if we teach people empathy,
[00:59:02.700 --> 00:59:04.940]   that's gonna have the same impact.
[00:59:04.940 --> 00:59:08.100]   - Maybe harder than self-driving cars, but--
[00:59:08.100 --> 00:59:08.940]   - I don't think so.
[00:59:08.940 --> 00:59:11.300]   I think the rules are really simple.
[00:59:11.300 --> 00:59:14.380]   You just have to want to engage.
[00:59:14.380 --> 00:59:18.180]   It's weird, we literally went in school in K through 12,
[00:59:18.180 --> 00:59:20.460]   we teach kids like get the highest math score.
[00:59:20.460 --> 00:59:22.860]   And if you are a rational human being,
[00:59:22.860 --> 00:59:24.860]   you might evolve from this education,
[00:59:24.860 --> 00:59:26.740]   say having the best math score
[00:59:26.740 --> 00:59:29.620]   and the best English scores, make me the best leader.
[00:59:29.620 --> 00:59:31.060]   And it turns out not to be the case.
[00:59:31.060 --> 00:59:32.860]   It's actually really wrong
[00:59:32.860 --> 00:59:35.820]   because making, first of all, in terms of math scores,
[00:59:35.820 --> 00:59:37.620]   I think it's perfectly fine to hire somebody
[00:59:37.620 --> 00:59:38.500]   with great math skills.
[00:59:38.500 --> 00:59:40.620]   You don't have to do it yourself.
[00:59:40.620 --> 00:59:42.740]   You can hire someone with great empathy for you,
[00:59:42.740 --> 00:59:44.980]   that's much harder, but it can always hire someone
[00:59:44.980 --> 00:59:46.340]   with great math skills.
[00:59:46.340 --> 00:59:48.980]   But we live in an affluent world
[00:59:48.980 --> 00:59:51.020]   where we constantly deal with other people.
[00:59:51.020 --> 00:59:51.900]   And that's a beauty.
[00:59:51.900 --> 00:59:53.340]   It's not a nuisance, it's a beauty.
[00:59:53.340 --> 00:59:55.940]   So if we somewhat develop that muscle
[00:59:55.940 --> 00:59:59.700]   that we can do that well and empower others
[00:59:59.700 --> 01:00:02.900]   in the workplace, I think we're gonna be super successful.
[01:00:02.900 --> 01:00:05.780]   - And I know many fellow roboticists
[01:00:05.780 --> 01:00:08.660]   and computer scientists that I will insist
[01:00:08.660 --> 01:00:09.820]   to take this course.
[01:00:09.820 --> 01:00:11.020]   (laughing)
[01:00:11.020 --> 01:00:12.180]   - Not to be named here.
[01:00:12.180 --> 01:00:13.780]   - Not to be named.
[01:00:13.780 --> 01:00:17.980]   Many, many years ago, 1903,
[01:00:17.980 --> 01:00:22.660]   the Wright brothers flew in Kitty Hawk for the first time.
[01:00:22.660 --> 01:00:26.980]   And you've launched a company of the same name, Kitty Hawk,
[01:00:26.980 --> 01:00:31.980]   with the dream of building flying cars, EVtols.
[01:00:31.980 --> 01:00:35.780]   So at the big picture, what are the big challenges
[01:00:35.780 --> 01:00:38.700]   of making this thing that actually have inspired
[01:00:38.700 --> 01:00:41.780]   generations of people about what the future looks like?
[01:00:41.780 --> 01:00:42.620]   What does it take?
[01:00:42.620 --> 01:00:43.700]   What are the biggest challenges?
[01:00:43.700 --> 01:00:47.260]   - So flying cars has always been a dream.
[01:00:47.260 --> 01:00:49.740]   Every boy, every girl wants to fly.
[01:00:49.740 --> 01:00:50.580]   Let's be honest.
[01:00:50.580 --> 01:00:51.420]   - Yes.
[01:00:51.420 --> 01:00:52.380]   - And let's go back in our history
[01:00:52.380 --> 01:00:53.820]   of your dreaming of flying.
[01:00:53.820 --> 01:00:56.540]   I think my, honestly, my single most remembered
[01:00:56.540 --> 01:00:58.340]   childhood dream has been a dream
[01:00:58.340 --> 01:01:00.780]   where I was sitting on a pillow and I could fly.
[01:01:00.780 --> 01:01:02.060]   I was like five years old.
[01:01:02.060 --> 01:01:04.180]   I remember like maybe three dreams of my childhood,
[01:01:04.180 --> 01:01:06.420]   but that's the one I remember most vividly.
[01:01:06.420 --> 01:01:09.380]   And then Peter Thiel famously said,
[01:01:09.380 --> 01:01:10.660]   "They promised us flying cars
[01:01:10.660 --> 01:01:12.780]   and they gave us 140 characters,"
[01:01:12.780 --> 01:01:15.220]   pointing at Twitter at the time,
[01:01:15.220 --> 01:01:18.380]   limited message size to 140 characters.
[01:01:18.380 --> 01:01:20.220]   So we're coming back now to really go
[01:01:20.220 --> 01:01:23.220]   for this super impactful stuff like flying cars.
[01:01:23.220 --> 01:01:25.900]   And to be precise, they're not really cars.
[01:01:25.900 --> 01:01:27.180]   They don't have wheels.
[01:01:27.180 --> 01:01:28.580]   They're actually much closer to a helicopter
[01:01:28.580 --> 01:01:29.620]   than anything else.
[01:01:29.620 --> 01:01:32.060]   They take off vertically and they fly horizontally,
[01:01:32.060 --> 01:01:34.380]   but they have important differences.
[01:01:34.380 --> 01:01:37.740]   One difference is that they are much quieter.
[01:01:37.740 --> 01:01:41.620]   We just released a vehicle called Project Heaviside
[01:01:41.620 --> 01:01:43.540]   that can fly over you as low as a helicopter.
[01:01:43.540 --> 01:01:45.300]   And you basically can't hear it.
[01:01:45.300 --> 01:01:46.740]   It's like 38 decibels.
[01:01:46.740 --> 01:01:49.300]   It's like, if you were inside the library,
[01:01:49.300 --> 01:01:50.260]   you might be able to hear it,
[01:01:50.260 --> 01:01:53.020]   but anywhere outdoors, your ambient noise is higher.
[01:01:53.020 --> 01:01:57.060]   Secondly, they're much more affordable.
[01:01:57.060 --> 01:01:59.020]   They're much more affordable than helicopters.
[01:01:59.020 --> 01:02:01.940]   And the reason is helicopters are expensive
[01:02:01.940 --> 01:02:03.060]   for many reasons.
[01:02:03.060 --> 01:02:07.020]   There's lots of single point of figures in a helicopter.
[01:02:07.020 --> 01:02:09.140]   There's a bolt between the blades
[01:02:09.140 --> 01:02:10.780]   that's called Jesus bolt.
[01:02:10.780 --> 01:02:12.420]   And the reason why it's called Jesus bolt
[01:02:12.420 --> 01:02:16.380]   is that if this bolt breaks, you will die.
[01:02:16.380 --> 01:02:19.500]   There is no second solution in helicopter flight.
[01:02:19.500 --> 01:02:21.500]   Whereas we have these distributed mechanism.
[01:02:21.500 --> 01:02:23.740]   When you go from gasoline to electric,
[01:02:23.740 --> 01:02:25.820]   you can now have many, many, many small motors
[01:02:25.820 --> 01:02:27.260]   as opposed to one big motor.
[01:02:27.260 --> 01:02:28.780]   And that means if you lose one of those motors,
[01:02:28.780 --> 01:02:29.620]   not a big deal.
[01:02:29.620 --> 01:02:32.820]   Heaviside, if it loses a motor, has eight of those,
[01:02:32.820 --> 01:02:34.020]   if it loses one of those eight motors,
[01:02:34.020 --> 01:02:35.180]   so it's seven left,
[01:02:35.180 --> 01:02:37.260]   it can take off just like before
[01:02:37.260 --> 01:02:38.820]   and land just like before.
[01:02:38.820 --> 01:02:42.020]   We are now also moving into a technology
[01:02:42.020 --> 01:02:44.140]   that doesn't require a commercial pilot
[01:02:44.140 --> 01:02:45.500]   because in some level,
[01:02:45.500 --> 01:02:48.980]   flight is actually easier than ground transportation.
[01:02:48.980 --> 01:02:50.700]   Like in self-driving cars,
[01:02:50.700 --> 01:02:54.500]   the world is full of like children and bicycles
[01:02:54.500 --> 01:02:56.620]   and other cars and mailboxes and curbs
[01:02:56.620 --> 01:02:58.420]   and shrubs and what have you,
[01:02:58.420 --> 01:03:00.500]   all these things you have to avoid.
[01:03:00.500 --> 01:03:03.740]   When you go above the buildings and tree lines,
[01:03:03.740 --> 01:03:04.620]   there's nothing there.
[01:03:04.620 --> 01:03:06.100]   I mean, you can do the test right now,
[01:03:06.100 --> 01:03:09.420]   look outside and count the number of things you see flying.
[01:03:09.420 --> 01:03:11.500]   I'd be shocked if you could see more than two things.
[01:03:11.500 --> 01:03:12.860]   It's probably just zero.
[01:03:12.860 --> 01:03:16.940]   In the Bay Area, the most I've ever seen was six.
[01:03:16.940 --> 01:03:20.420]   And maybe it's 15 or 20, but not 10,000.
[01:03:20.420 --> 01:03:23.980]   So the sky is very ample and very empty and very free.
[01:03:23.980 --> 01:03:27.820]   So the vision is, can we build a socially acceptable
[01:03:27.820 --> 01:03:32.340]   mass transit solution for daily transportation
[01:03:32.340 --> 01:03:34.260]   that is affordable?
[01:03:34.260 --> 01:03:36.300]   And we have an existence proof.
[01:03:36.300 --> 01:03:39.780]   Heaviside can fly 100 miles in range
[01:03:39.780 --> 01:03:43.260]   with still 30% electric reserves.
[01:03:43.260 --> 01:03:46.060]   It can fly up to like 180 miles an hour.
[01:03:46.060 --> 01:03:48.860]   We know that that solution at scale
[01:03:48.860 --> 01:03:51.380]   would make your ground transportation
[01:03:51.380 --> 01:03:53.780]   10 times as fast as a car
[01:03:53.780 --> 01:03:57.340]   based on US census or statistics data,
[01:03:57.340 --> 01:04:00.860]   which means we would take your 300 hours of daily,
[01:04:00.860 --> 01:04:02.980]   of yearly commute down to 30 hours
[01:04:02.980 --> 01:04:05.180]   and give you 270 hours back.
[01:04:05.180 --> 01:04:07.660]   Who wouldn't want, I mean, who doesn't hate traffic?
[01:04:07.660 --> 01:04:10.780]   Like I hate, give me the person who doesn't hate traffic.
[01:04:10.780 --> 01:04:11.620]   I hate traffic.
[01:04:11.620 --> 01:04:13.620]   Every time I'm in traffic, I hate it.
[01:04:13.620 --> 01:04:17.540]   And if we could free the world from traffic,
[01:04:17.540 --> 01:04:20.020]   we have technology, we can free the world from traffic.
[01:04:20.020 --> 01:04:21.300]   We have the technology.
[01:04:21.300 --> 01:04:23.020]   It's there, we have an existence proof.
[01:04:23.020 --> 01:04:25.420]   It's not a technological problem anymore.
[01:04:25.420 --> 01:04:29.300]   - Do you think there is a future where tens of thousands,
[01:04:29.300 --> 01:04:34.300]   maybe hundreds of thousands of both delivery drones
[01:04:34.300 --> 01:04:39.340]   and flying cars of this kind, EV towers fill the sky?
[01:04:39.340 --> 01:04:40.900]   - I absolutely believe this.
[01:04:40.900 --> 01:04:43.540]   And there's obviously the societal acceptance
[01:04:43.540 --> 01:04:45.420]   is a major question.
[01:04:45.420 --> 01:04:46.900]   And of course, safety is.
[01:04:46.900 --> 01:04:48.020]   I believe in safety,
[01:04:48.020 --> 01:04:50.300]   we're gonna exceed ground transportation safety
[01:04:50.300 --> 01:04:54.460]   as has happened for aviation already, commercial aviation.
[01:04:54.460 --> 01:04:56.580]   And in terms of acceptance,
[01:04:56.580 --> 01:04:58.260]   I think one of the key things is noise.
[01:04:58.260 --> 01:05:00.900]   That's why we are focusing relentlessly on noise
[01:05:00.900 --> 01:05:05.580]   and we built perhaps the quietest electric VTOL vehicle
[01:05:05.580 --> 01:05:06.420]   ever built.
[01:05:06.420 --> 01:05:09.700]   The nice thing about the sky is it's three dimensional.
[01:05:09.700 --> 01:05:12.460]   So any mathematician will immediately recognize
[01:05:12.460 --> 01:05:14.900]   the difference between 1D of like a regular highway
[01:05:14.900 --> 01:05:16.180]   to 3D of a sky.
[01:05:16.180 --> 01:05:19.300]   But to make it clear for the layman,
[01:05:19.300 --> 01:05:22.700]   say you wanna make 100 vertical lanes
[01:05:22.700 --> 01:05:24.980]   of highway 101 in San Francisco,
[01:05:24.980 --> 01:05:27.180]   because you believe building 100 vertical lanes
[01:05:27.180 --> 01:05:28.820]   is the right solution.
[01:05:28.820 --> 01:05:30.140]   Imagine how much it would cost
[01:05:30.140 --> 01:05:33.380]   to stack 100 vertical lanes physically onto 101.
[01:05:33.380 --> 01:05:34.300]   That would be prohibitive.
[01:05:34.300 --> 01:05:37.740]   That would be consuming the world's GDP for an entire year
[01:05:37.740 --> 01:05:39.180]   just for one highway.
[01:05:39.180 --> 01:05:41.220]   It's amazingly expensive.
[01:05:41.220 --> 01:05:43.660]   In the sky, it would just be a recompilation
[01:05:43.660 --> 01:05:44.540]   of a piece of software
[01:05:44.540 --> 01:05:46.500]   because all these lanes are virtual.
[01:05:46.500 --> 01:05:49.180]   That means any vehicle that is in conflict
[01:05:49.180 --> 01:05:51.780]   with another vehicle would just go to different altitudes
[01:05:51.780 --> 01:05:53.260]   and then the conflict is gone.
[01:05:53.260 --> 01:05:55.300]   And if you don't believe this,
[01:05:55.300 --> 01:05:58.500]   that's exactly how commercial aviation works.
[01:05:58.500 --> 01:06:01.380]   When you fly from New York to San Francisco,
[01:06:01.380 --> 01:06:04.140]   another plane flies from San Francisco to New York,
[01:06:04.140 --> 01:06:05.180]   they're at different altitudes
[01:06:05.180 --> 01:06:06.660]   so they don't hit each other.
[01:06:06.660 --> 01:06:10.300]   It's a solved problem for the jet space
[01:06:10.300 --> 01:06:12.660]   and it will be a solved problem for the urban space.
[01:06:12.660 --> 01:06:15.300]   There's companies like Google, Bing and Amazon
[01:06:15.300 --> 01:06:16.980]   working on very innovative solutions
[01:06:16.980 --> 01:06:18.500]   how do we have space management.
[01:06:18.500 --> 01:06:21.580]   They use exactly the same principles as we use today
[01:06:21.580 --> 01:06:23.220]   to route today's jets.
[01:06:23.220 --> 01:06:25.860]   There's nothing hard about this.
[01:06:25.860 --> 01:06:28.940]   - Do you envision autonomy being a key part of it
[01:06:28.940 --> 01:06:33.940]   so that the flying vehicles are either semi-autonomous
[01:06:33.940 --> 01:06:36.860]   or fully autonomous?
[01:06:36.860 --> 01:06:37.820]   - 100% autonomous.
[01:06:37.820 --> 01:06:40.420]   You don't want idiots like me flying in the sky.
[01:06:40.420 --> 01:06:41.900]   I promise you.
[01:06:41.900 --> 01:06:43.180]   And if you have 10,000,
[01:06:43.180 --> 01:06:45.980]   watch the movie "The Fifth Element"
[01:06:45.980 --> 01:06:48.180]   to get a fee for what would happen
[01:06:48.180 --> 01:06:49.420]   if it's not autonomous.
[01:06:49.420 --> 01:06:51.660]   - And a centralized, that's a really interesting idea
[01:06:51.660 --> 01:06:55.260]   of a centralized sort of management system
[01:06:55.260 --> 01:06:56.300]   for lanes and so on.
[01:06:56.300 --> 01:06:58.780]   So actually just being able to have
[01:06:58.780 --> 01:07:02.980]   similar as we have in the current commercial aviation
[01:07:02.980 --> 01:07:05.540]   but scale it up to much more vehicles.
[01:07:05.540 --> 01:07:07.660]   That's a really interesting optimization problem.
[01:07:07.660 --> 01:07:11.060]   - It is mathematically very, very straightforward.
[01:07:11.060 --> 01:07:13.500]   Like the gap we leave between jets is gargantuous.
[01:07:13.500 --> 01:07:16.380]   And part of the reason is there isn't that many jets.
[01:07:16.380 --> 01:07:18.820]   So it just feels like a good solution.
[01:07:18.820 --> 01:07:22.380]   Today, when you get vectored by air traffic control,
[01:07:22.380 --> 01:07:23.900]   someone talks to you, right?
[01:07:23.900 --> 01:07:26.980]   So an ATC controller might have up to maybe 20 planes
[01:07:26.980 --> 01:07:28.140]   on the same frequency.
[01:07:28.140 --> 01:07:30.340]   And then they talk to you, you have to talk back.
[01:07:30.340 --> 01:07:32.700]   And that feels right because there isn't more than 20 planes
[01:07:32.700 --> 01:07:34.980]   around anyhow, so you can talk to everybody.
[01:07:34.980 --> 01:07:36.740]   But if there's 20,000 things around,
[01:07:36.740 --> 01:07:37.980]   you can't talk to everybody anymore.
[01:07:37.980 --> 01:07:40.260]   So we have to do something that's called digital,
[01:07:40.260 --> 01:07:41.540]   like text messaging.
[01:07:41.540 --> 01:07:43.060]   Like we do have solutions.
[01:07:43.060 --> 01:07:45.540]   Like we have what, four or five billion smartphones
[01:07:45.540 --> 01:07:46.460]   in the world now, right?
[01:07:46.460 --> 01:07:47.740]   And they're all connected.
[01:07:47.740 --> 01:07:50.740]   And somehow we solve the scale problem for smartphones.
[01:07:50.740 --> 01:07:51.940]   We know where they all are.
[01:07:51.940 --> 01:07:54.860]   They can talk to somebody and they're very reliable.
[01:07:54.860 --> 01:07:56.460]   They're amazingly reliable.
[01:07:56.460 --> 01:07:58.620]   We could use the same system,
[01:07:58.620 --> 01:08:01.060]   the same scale for air traffic control.
[01:08:01.060 --> 01:08:04.060]   So instead of me as a pilot talking to a human being
[01:08:04.060 --> 01:08:06.260]   in the middle of the conversation,
[01:08:06.260 --> 01:08:09.660]   receiving a new frequency, like how ancient is that?
[01:08:09.660 --> 01:08:11.260]   We could digitize this stuff
[01:08:11.260 --> 01:08:15.260]   and digitally transmit the right flight coordinates.
[01:08:15.260 --> 01:08:18.060]   And that solution will automatically scale
[01:08:18.060 --> 01:08:20.060]   to 10,000 vehicles.
[01:08:20.060 --> 01:08:22.180]   - We talked about empathy a little bit.
[01:08:22.180 --> 01:08:25.780]   Do you think we'll one day build an AI system
[01:08:25.780 --> 01:08:30.140]   that a human being can love and that loves that human back?
[01:08:30.140 --> 01:08:31.340]   Like in the movie "Her."
[01:08:31.340 --> 01:08:33.980]   - Look, I'm a pragmatist.
[01:08:33.980 --> 01:08:35.620]   For me, AI is a tool.
[01:08:35.620 --> 01:08:37.020]   It's like a shovel.
[01:08:37.020 --> 01:08:40.800]   And the ethics of using the shovel are always
[01:08:40.800 --> 01:08:41.860]   with us, the people.
[01:08:41.860 --> 01:08:44.060]   And it has to be this way.
[01:08:44.060 --> 01:08:49.060]   In terms of emotions, I would hate to come into my kitchen
[01:08:49.060 --> 01:08:54.220]   and see that my refrigerator spoiled all my food,
[01:08:54.220 --> 01:08:56.500]   then have it explained to me that it fell in love
[01:08:56.500 --> 01:08:59.660]   with the dishwasher and I wasn't as nice as the dishwasher.
[01:08:59.660 --> 01:09:02.180]   So as a result, it neglected me.
[01:09:02.180 --> 01:09:05.140]   That would just be a bad experience
[01:09:05.140 --> 01:09:07.060]   and it would be a bad product.
[01:09:07.060 --> 01:09:09.540]   I would probably not recommend this refrigerator
[01:09:09.540 --> 01:09:10.420]   to my friends.
[01:09:10.420 --> 01:09:12.900]   And that's where I draw the line.
[01:09:12.900 --> 01:09:16.620]   I think to me, technology has to be reliable.
[01:09:16.620 --> 01:09:17.700]   It has to be predictable.
[01:09:17.700 --> 01:09:19.860]   I want my car to work.
[01:09:19.860 --> 01:09:22.860]   I don't want to fall in love with my car.
[01:09:22.860 --> 01:09:24.620]   I just want it to work.
[01:09:24.620 --> 01:09:27.180]   I want it to compliment me, not to replace me.
[01:09:27.180 --> 01:09:30.620]   I have very unique human properties
[01:09:30.620 --> 01:09:33.420]   and I want the machines to make me,
[01:09:33.420 --> 01:09:35.700]   turn me into a superhuman.
[01:09:35.700 --> 01:09:37.820]   Like I'm already a superhuman today,
[01:09:37.820 --> 01:09:39.260]   thanks to the machines that surround me.
[01:09:39.260 --> 01:09:40.780]   And I give you examples.
[01:09:40.780 --> 01:09:45.700]   I can run across the Atlantic near the speed of sound
[01:09:45.700 --> 01:09:48.460]   at 36,000 feet today.
[01:09:48.460 --> 01:09:49.580]   That's kind of amazing.
[01:09:49.580 --> 01:09:53.620]   My voice now carries me all the way to Australia
[01:09:53.620 --> 01:09:56.580]   using a smartphone today.
[01:09:56.580 --> 01:10:00.060]   And it's not the speed of sound, which would take hours.
[01:10:00.060 --> 01:10:01.300]   It's the speed of light.
[01:10:01.300 --> 01:10:03.820]   My voice travels at the speed of light.
[01:10:03.820 --> 01:10:04.660]   How cool is that?
[01:10:04.660 --> 01:10:06.300]   That makes me superhuman.
[01:10:06.300 --> 01:10:10.500]   I would even argue my flushing toilet makes me superhuman.
[01:10:10.500 --> 01:10:13.780]   Just think of the time before flushing toilets.
[01:10:13.780 --> 01:10:16.460]   And maybe you have a very old person in your family
[01:10:16.460 --> 01:10:18.460]   that you can ask about this
[01:10:18.460 --> 01:10:22.100]   or take a trip to rural India to experience it.
[01:10:22.100 --> 01:10:25.780]   It makes me superhuman.
[01:10:25.780 --> 01:10:28.860]   So to me, what technology does, it compliments me.
[01:10:28.860 --> 01:10:30.900]   It makes me stronger.
[01:10:30.900 --> 01:10:34.900]   Therefore, words like love and compassion have very little,
[01:10:34.900 --> 01:10:38.580]   have very little interest in this for machines.
[01:10:38.580 --> 01:10:39.900]   I have interest in people.
[01:10:40.700 --> 01:10:44.260]   - You don't think, first of all, beautifully put,
[01:10:44.260 --> 01:10:45.660]   beautifully argued,
[01:10:45.660 --> 01:10:50.420]   but do you think love has use in our tools, compassion?
[01:10:50.420 --> 01:10:53.260]   - I think love is a beautiful human concept.
[01:10:53.260 --> 01:10:55.380]   And if you think of what love really is,
[01:10:55.380 --> 01:11:00.380]   love is a means to convey safety, to convey trust.
[01:11:00.380 --> 01:11:07.420]   I think trust has a huge need in technology as well,
[01:11:07.420 --> 01:11:09.140]   not just people.
[01:11:09.140 --> 01:11:11.220]   We want to trust our technology
[01:11:11.220 --> 01:11:14.260]   the same way we, or in a similar way we trust people.
[01:11:14.260 --> 01:11:19.340]   In human interaction, standards have emerged
[01:11:19.340 --> 01:11:21.740]   and feelings, emotions have emerged,
[01:11:21.740 --> 01:11:23.900]   maybe genetically, maybe biologically,
[01:11:23.900 --> 01:11:26.540]   that are able to convey sense of trust, sense of safety,
[01:11:26.540 --> 01:11:28.860]   sense of passion, of love, of dedication
[01:11:28.860 --> 01:11:30.780]   that makes the human fabric.
[01:11:30.780 --> 01:11:33.700]   And I'm a big slacker for love.
[01:11:33.700 --> 01:11:34.580]   I want to be loved.
[01:11:34.580 --> 01:11:35.420]   I want to be trusted.
[01:11:35.420 --> 01:11:36.820]   I want to be admired.
[01:11:36.820 --> 01:11:38.820]   All these wonderful things.
[01:11:38.820 --> 01:11:42.140]   And because all of us, we have this beautiful system,
[01:11:42.140 --> 01:11:44.780]   I wouldn't just blindly copy this to the machines.
[01:11:44.780 --> 01:11:46.140]   Here's why.
[01:11:46.140 --> 01:11:49.300]   When you look at, say, transportation,
[01:11:49.300 --> 01:11:53.260]   you could have observed that up to the end
[01:11:53.260 --> 01:11:56.180]   of the 19th century, almost all transportation
[01:11:56.180 --> 01:11:58.060]   used any number of legs,
[01:11:58.060 --> 01:12:01.660]   from one leg to two legs to a thousand legs.
[01:12:01.660 --> 01:12:03.740]   And you could have concluded that is the right way
[01:12:03.740 --> 01:12:05.580]   to move about the environment.
[01:12:06.780 --> 01:12:08.940]   We've made the exception of birds, who use flapping wings.
[01:12:08.940 --> 01:12:10.820]   In fact, there were many people in aviation
[01:12:10.820 --> 01:12:13.700]   that flapped wings to their arms and jumped from cliffs.
[01:12:13.700 --> 01:12:15.100]   Most of them didn't survive.
[01:12:15.100 --> 01:12:19.900]   Then the interesting thing is that the technology solutions
[01:12:19.900 --> 01:12:21.580]   are very different.
[01:12:21.580 --> 01:12:23.900]   Like, in technology, it's really easy to build a wheel.
[01:12:23.900 --> 01:12:25.700]   In biology, it's super hard to build a wheel.
[01:12:25.700 --> 01:12:30.100]   There's very few perpetually rotating things in biology,
[01:12:30.100 --> 01:12:34.180]   and they usually run cells and things.
[01:12:34.180 --> 01:12:37.180]   In engineering, we can build wheels,
[01:12:37.180 --> 01:12:39.980]   and those wheels gave rise to cars.
[01:12:39.980 --> 01:12:44.380]   Similar wheels gave rise to aviation.
[01:12:44.380 --> 01:12:46.700]   Like, there's no thing that flies
[01:12:46.700 --> 01:12:48.820]   that wouldn't have something that rotates,
[01:12:48.820 --> 01:12:52.420]   like a jet engine or helicopter blades.
[01:12:52.420 --> 01:12:55.500]   So the solutions have used very different physical laws
[01:12:55.500 --> 01:12:58.060]   than nature, and that's great.
[01:12:58.060 --> 01:13:00.100]   So for me to be too much focused on,
[01:13:00.100 --> 01:13:03.340]   oh, this is how nature does it, let's just replicate it,
[01:13:03.340 --> 01:13:05.380]   if we really believed that the solution
[01:13:05.380 --> 01:13:08.700]   to the agricultural revolution was a humanoid robot,
[01:13:08.700 --> 01:13:10.940]   it would still be waiting today.
[01:13:10.940 --> 01:13:12.540]   - Again, beautifully put.
[01:13:12.540 --> 01:13:15.860]   You said that you don't take yourself too seriously.
[01:13:15.860 --> 01:13:16.700]   - Did I say that?
[01:13:16.700 --> 01:13:19.180]   You want me to say that?
[01:13:19.180 --> 01:13:20.020]   - Maybe I did.
[01:13:20.020 --> 01:13:20.980]   - You're not taking me seriously.
[01:13:20.980 --> 01:13:22.860]   - I'm not, yeah, that's right.
[01:13:22.860 --> 01:13:24.460]   - Good, you're right, I don't wanna.
[01:13:24.460 --> 01:13:25.740]   - I just made that up.
[01:13:25.740 --> 01:13:29.100]   But you have a humor and a lightness about life
[01:13:29.100 --> 01:13:33.500]   that I think is beautiful and inspiring to a lot of people.
[01:13:33.500 --> 01:13:35.020]   Where does that come from?
[01:13:35.020 --> 01:13:40.020]   The smile, the humor, the lightness amidst all the chaos
[01:13:40.020 --> 01:13:43.660]   of the hard work that you're in, where does that come from?
[01:13:43.660 --> 01:13:44.540]   - I just love my life.
[01:13:44.540 --> 01:13:46.100]   I love the people around me.
[01:13:46.100 --> 01:13:49.740]   I'm just so glad to be alive.
[01:13:49.740 --> 01:13:53.620]   Like, I'm, what, 52, hard to believe.
[01:13:53.620 --> 01:13:56.220]   People say 52 is a new 51, so now I feel better.
[01:13:56.220 --> 01:13:58.540]   (Lex laughing)
[01:13:58.540 --> 01:14:02.340]   But in looking around the world,
[01:14:02.340 --> 01:14:05.180]   looking, just go back 200, 300 years.
[01:14:05.180 --> 01:14:09.340]   Humanity is, what, 300,000 years old?
[01:14:09.340 --> 01:14:13.980]   But for the first 300,000 years minus the last 100,
[01:14:13.980 --> 01:14:17.060]   our life expectancy would have been
[01:14:17.060 --> 01:14:20.260]   plus or minus 30 years, roughly, give or take.
[01:14:20.260 --> 01:14:22.620]   So I would be long dead now.
[01:14:22.620 --> 01:14:26.860]   That makes me just enjoy every single day of my life
[01:14:26.860 --> 01:14:28.260]   because I don't deserve this.
[01:14:28.260 --> 01:14:32.460]   Why am I born today when so many of my ancestors
[01:14:32.460 --> 01:14:37.460]   died of horrible deaths, like famines, massive wars
[01:14:37.460 --> 01:14:41.860]   that ravaged Europe for the last 1,000 years,
[01:14:41.860 --> 01:14:44.540]   mystically disappeared after World War II
[01:14:44.540 --> 01:14:47.580]   when the Americans and the Allies did something amazing
[01:14:47.580 --> 01:14:49.780]   to my country that didn't deserve it,
[01:14:49.780 --> 01:14:51.460]   the country of Germany.
[01:14:51.460 --> 01:14:52.620]   This is so amazing.
[01:14:52.620 --> 01:14:56.940]   And then when you're alive and feel this every day,
[01:14:56.940 --> 01:15:01.940]   then it's just so amazing what we can accomplish,
[01:15:01.940 --> 01:15:03.460]   what we can do.
[01:15:03.460 --> 01:15:06.380]   We live in a world that is so incredibly
[01:15:06.380 --> 01:15:08.700]   vastly changing every day.
[01:15:08.700 --> 01:15:10.980]   Almost everything that we cherish,
[01:15:10.980 --> 01:15:14.540]   from your smartphone to your flushing toilet,
[01:15:14.540 --> 01:15:16.180]   to all these basic inventions,
[01:15:16.180 --> 01:15:19.580]   your new clothes you're wearing, your watch, your plane,
[01:15:19.580 --> 01:15:24.580]   penicillin, I don't know, anesthesia for surgery,
[01:15:25.780 --> 01:15:30.020]   penicillin, have been invented in the last 150 years.
[01:15:30.020 --> 01:15:32.380]   So in the last 150 years, something magical happened.
[01:15:32.380 --> 01:15:34.340]   And I would trace it back to Gutenberg
[01:15:34.340 --> 01:15:35.900]   and the printing press that has been able
[01:15:35.900 --> 01:15:38.780]   to disseminate information more efficiently than before,
[01:15:38.780 --> 01:15:42.860]   that all of a sudden we're able to invent agriculture
[01:15:42.860 --> 01:15:45.940]   and nitrogen fertilization that made agriculture
[01:15:45.940 --> 01:15:48.020]   so much more potent that we didn't have to work
[01:15:48.020 --> 01:15:50.140]   in the farms anymore and we could start reading and writing
[01:15:50.140 --> 01:15:52.300]   and we could become all these wonderful things
[01:15:52.300 --> 01:15:54.740]   we are today, from airline pilot to massage therapist
[01:15:54.740 --> 01:15:56.780]   to software engineer.
[01:15:56.780 --> 01:15:57.620]   It's just amazing.
[01:15:57.620 --> 01:16:00.620]   Like living in that time is such a blessing.
[01:16:00.620 --> 01:16:03.340]   We should sometimes really think about this.
[01:16:03.340 --> 01:16:07.340]   Steven Pinker, who is a very famous author and philosopher
[01:16:07.340 --> 01:16:09.420]   whom I really adore, wrote a great book called
[01:16:09.420 --> 01:16:11.140]   "Enlightenment Now" and that's maybe the one book
[01:16:11.140 --> 01:16:11.980]   I would recommend.
[01:16:11.980 --> 01:16:14.500]   And he asked the question if there was only
[01:16:14.500 --> 01:16:17.140]   a single article written in the 20th century,
[01:16:17.140 --> 01:16:19.100]   only one article, what would it be?
[01:16:19.100 --> 01:16:21.140]   What's the most important innovation,
[01:16:21.140 --> 01:16:23.060]   the most important thing that happened?
[01:16:23.060 --> 01:16:24.700]   And he would say this article would credit
[01:16:24.700 --> 01:16:27.020]   a guy named Carl Bosch.
[01:16:27.020 --> 01:16:29.460]   And I challenge anybody, have you ever heard
[01:16:29.460 --> 01:16:31.180]   of the name Carl Bosch?
[01:16:31.180 --> 01:16:32.940]   I hadn't, okay.
[01:16:32.940 --> 01:16:35.420]   There's a Bosch Corporation in Germany,
[01:16:35.420 --> 01:16:37.420]   but it's not associated with Carl Bosch.
[01:16:37.420 --> 01:16:39.860]   So I looked it up.
[01:16:39.860 --> 01:16:42.620]   Carl Bosch invented nitrogen fertilization.
[01:16:42.620 --> 01:16:45.540]   And in doing so, together with an older invention
[01:16:45.540 --> 01:16:49.180]   of irrigation, was able to increase the yield
[01:16:49.180 --> 01:16:52.860]   per agricultural land by a factor of 26.
[01:16:52.860 --> 01:16:57.700]   So a 2,500% increase in fertility of land.
[01:16:57.700 --> 01:17:00.540]   And that, so Steve Pinker argues,
[01:17:00.540 --> 01:17:03.900]   saved over two billion lives today.
[01:17:03.900 --> 01:17:05.700]   Two billion people who would be dead
[01:17:05.700 --> 01:17:08.420]   if this man hadn't done what he had done, okay?
[01:17:08.420 --> 01:17:12.180]   Think about that impact and what that means to society.
[01:17:12.180 --> 01:17:14.180]   That's the way I look at the world.
[01:17:14.180 --> 01:17:16.180]   I mean, it's just so amazing to be alive
[01:17:16.180 --> 01:17:17.020]   and to be part of this.
[01:17:17.020 --> 01:17:20.300]   And I'm so glad I lived after Carl Bosch and not before.
[01:17:21.340 --> 01:17:24.020]   - I don't think there's a better way to end this, Sebastian.
[01:17:24.020 --> 01:17:25.500]   It's an honor to talk to you,
[01:17:25.500 --> 01:17:27.380]   to have had the chance to learn from you.
[01:17:27.380 --> 01:17:28.340]   Thank you so much for talking to me.
[01:17:28.340 --> 01:17:31.020]   - Thanks for coming on, it's a real pleasure.
[01:17:31.020 --> 01:17:32.820]   - Thank you for listening to this conversation
[01:17:32.820 --> 01:17:34.420]   with Sebastian Thrun.
[01:17:34.420 --> 01:17:37.500]   And thank you to our presenting sponsor, Cash App.
[01:17:37.500 --> 01:17:40.260]   Download it, use code LEXPODCAST,
[01:17:40.260 --> 01:17:43.260]   you'll get $10 and $10 will go to FIRST,
[01:17:43.260 --> 01:17:45.540]   a STEM education nonprofit that inspires
[01:17:45.540 --> 01:17:47.500]   hundreds of thousands of young minds
[01:17:47.500 --> 01:17:50.580]   to learn and to dream of engineering our future.
[01:17:50.580 --> 01:17:53.380]   If you enjoy this podcast, subscribe on YouTube,
[01:17:53.380 --> 01:17:56.660]   get five stars on Apple Podcast, support on Patreon,
[01:17:56.660 --> 01:17:58.860]   or connect with me on Twitter.
[01:17:58.860 --> 01:18:01.300]   And now let me leave you with some words of wisdom
[01:18:01.300 --> 01:18:03.260]   from Sebastian Thrun.
[01:18:03.260 --> 01:18:05.440]   It's important to celebrate your failures
[01:18:05.440 --> 01:18:07.740]   as much as your successes.
[01:18:07.740 --> 01:18:09.820]   If you celebrate your failures really well,
[01:18:09.820 --> 01:18:13.940]   if you say, wow, I failed, I tried, I was wrong,
[01:18:13.940 --> 01:18:15.620]   but I learned something.
[01:18:15.620 --> 01:18:18.300]   Then you realize you have no fear.
[01:18:18.300 --> 01:18:21.660]   And when your fear goes away, you can move the world.
[01:18:21.660 --> 01:18:25.500]   Thank you for listening and hope to see you next time.
[01:18:25.500 --> 01:18:28.080]   (upbeat music)
[01:18:28.080 --> 01:18:30.660]   (upbeat music)
[01:18:30.660 --> 01:18:40.660]   [BLANK_AUDIO]

