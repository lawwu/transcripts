
[00:00:00.000 --> 00:00:03.960]   Hey, Armor from Redstone Basics here.
[00:00:03.960 --> 00:00:07.540]   In a previous video, we've made this RC car drive by itself.
[00:00:07.540 --> 00:00:11.720]   To do so, we used a neural net for perception and a rule-based system to determine how to
[00:00:11.720 --> 00:00:12.720]   steer the car.
[00:00:12.720 --> 00:00:14.760]   You can think of this system as lane assist.
[00:00:14.760 --> 00:00:17.640]   To get to full self-driving, we would need to do more than steering.
[00:00:17.640 --> 00:00:21.040]   Driving is about achieving a goal with the car, and to do that we need a goal-oriented
[00:00:21.040 --> 00:00:22.040]   system.
[00:00:22.040 --> 00:00:25.560]   For this video, we set the simple goal of spinning fast to get the car to do donuts.
[00:00:25.560 --> 00:00:29.480]   I'll start by explaining how to create a custom OpenAI gym environment to prepare
[00:00:29.480 --> 00:00:32.520]   the car for reinforcement learning, and then we'll train it.
[00:00:32.520 --> 00:00:34.080]   Alright, so what is Gym?
[00:00:34.080 --> 00:00:37.920]   Gym is an initiative by OpenAI which has been a pioneer in reinforcement learning.
[00:00:37.920 --> 00:00:41.920]   It is used to interact with a simulated, or in our case physical environment, in which
[00:00:41.920 --> 00:00:43.200]   an agent can take actions.
[00:00:43.200 --> 00:00:46.520]   There are a few functions we need to implement this interface.
[00:00:46.520 --> 00:00:49.720]   The first one is the init function, where we'll define the communication with the car
[00:00:49.720 --> 00:00:54.160]   so that we can drive it, and the communication with the sensor we'll use to observe the world.
[00:00:54.160 --> 00:00:56.840]   We'll also define the action and observation spaces.
[00:00:56.840 --> 00:01:01.640]   Then, the most important one is the step function, it defines what happens once the agent chooses
[00:01:01.640 --> 00:01:06.640]   an action, it applies the action to the environment and returns the observation, which is what
[00:01:06.640 --> 00:01:12.000]   the agent sees, the reward for taking that action, and it tells us if the episode is done.
[00:01:12.000 --> 00:01:14.540]   The last three functions we need to implement are simple.
[00:01:14.540 --> 00:01:18.240]   The render function is the one that displays the environment on our screens.
[00:01:18.240 --> 00:01:21.120]   Since the environment is in the physical world, we don't need it.
[00:01:21.120 --> 00:01:25.200]   Next, since I train in a limited space, I need to manually prevent the car from crashing
[00:01:25.200 --> 00:01:27.840]   into walls.
[00:01:27.840 --> 00:01:31.680]   So I made the close function break to a full stop, and I called it by hitting a panic key
[00:01:31.680 --> 00:01:32.760]   on my keyboard.
[00:01:32.760 --> 00:01:35.000]   Now that our environment is ready, let's try it out.
[00:01:35.000 --> 00:01:47.240]   We use a dummy agent that takes random actions at each step, so let's go.
[00:01:47.240 --> 00:01:50.580]   So the environment is working and didn't crash, but the car did.
[00:01:50.580 --> 00:01:52.240]   So let's train a smarter agent.
[00:01:52.240 --> 00:01:56.600]   To do so, we're going to use the TD tree algorithm, along with weights and biases to log the average
[00:01:56.600 --> 00:01:59.420]   reward, the episode length, and the agent actions.
[00:01:59.420 --> 00:02:03.040]   These are metrics that will otherwise be lost and that are really useful for debugging our
[00:02:03.040 --> 00:02:04.560]   codes and our agents.
[00:02:04.560 --> 00:02:17.560]   So let's get to it.
[00:02:17.560 --> 00:02:30.060]   As you can see, the car just learned to do circles.
[00:02:30.060 --> 00:02:50.100]   We are now going to increase the speed and continue training.
[00:02:50.100 --> 00:02:54.060]   And that's it for donuts.
[00:02:54.060 --> 00:02:55.060]   That's it for donuts.
[00:02:55.060 --> 00:02:57.940]   Let's take a quick look at our weights and biases dashboard.
[00:02:57.940 --> 00:03:01.060]   Looking at the car, I found it difficult to see if it was simply steering to one side
[00:03:01.060 --> 00:03:04.060]   to do donuts, or if it was learning a more complex behavior.
[00:03:04.060 --> 00:03:07.980]   Looking at the dashboard, I can clearly see that it is indeed steering fully to one side.
[00:03:07.980 --> 00:03:12.060]   I can also see that at first the agent is taking random actions and the reward is pretty
[00:03:12.060 --> 00:03:13.060]   low.
[00:03:13.060 --> 00:03:16.340]   And then when it's doing the desired behavior, the reward shoots up.
[00:03:16.340 --> 00:03:18.380]   That's it for this video, I hope you liked it.
[00:03:18.380 --> 00:03:21.340]   If you have any questions, please leave a comment down below.
[00:03:21.340 --> 00:03:24.860]   And don't forget to subscribe for more interviews, tutorials, and talks.
[00:03:24.860 --> 00:03:25.580]   Thanks for watching.
[00:03:25.580 --> 00:03:28.940]   [MUSIC PLAYING]
[00:03:28.940 --> 00:03:32.300]   [MUSIC PLAYING]
[00:03:32.300 --> 00:03:34.860]   (gentle upbeat music)

