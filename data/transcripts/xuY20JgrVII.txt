
[00:00:00.000 --> 00:00:21.000]   Awesome, let me hop over to YouTube just to make sure I have the right mic connected.
[00:00:21.000 --> 00:00:24.680]   I can hear myself in high quality audio which means the right mic is connected.
[00:00:24.680 --> 00:00:31.840]   I can remove the starting soon banner and double check once more.
[00:00:31.840 --> 00:00:33.200]   Alright, we're looking good.
[00:00:33.200 --> 00:00:34.200]   Welcome back everybody.
[00:00:34.200 --> 00:00:37.000]   Thanks for joining the paper reading group.
[00:00:37.000 --> 00:00:40.840]   Today we'll be going to the next architecture.
[00:00:40.840 --> 00:00:45.360]   The paper is titled a ConvNet for 2020.
[00:00:45.360 --> 00:00:46.680]   That's the title of the paper.
[00:00:46.680 --> 00:00:48.800]   It's by Liu et al.
[00:00:48.800 --> 00:00:54.720]   And we'll be going through the paper as well as a PyTorch implementation of it.
[00:00:54.720 --> 00:00:56.760]   So we won't be going through the official implementation.
[00:00:56.760 --> 00:01:03.360]   We'll be using the temp library which is one of my favorite, not just mine, everyone's
[00:01:03.360 --> 00:01:07.720]   favorite framework in the machine learning community for anything related to computer
[00:01:07.720 --> 00:01:08.720]   vision.
[00:01:08.720 --> 00:01:14.080]   So I got a lot of feedback from the community that suggested there should be a shorter version
[00:01:14.080 --> 00:01:15.720]   of these videos as well.
[00:01:15.720 --> 00:01:21.840]   So for that reason, I'll start by summarizing the paper.
[00:01:21.840 --> 00:01:25.520]   So I put out a summary in a tweet and I'll be using that.
[00:01:25.520 --> 00:01:28.720]   So the first two minutes for anyone watching is the summary of the paper.
[00:01:28.720 --> 00:01:31.960]   I hope you remain with us throughout the session, but this will give you the overview if you
[00:01:31.960 --> 00:01:36.540]   want a quick summary to get you started.
[00:01:36.540 --> 00:01:40.920]   So I had worked with my awesome colleagues.
[00:01:40.920 --> 00:01:45.960]   I'm a fan of his Twitter handle, Adresh and Shomik.
[00:01:45.960 --> 00:01:49.800]   Both are really talented and together with them we had annotated this paper which I'll
[00:01:49.800 --> 00:01:51.600]   tell you more about.
[00:01:51.600 --> 00:01:58.240]   But inside of this Twitter thread, which I'll drop the link to in the chart, I summarize
[00:01:58.240 --> 00:01:59.840]   what the paper does.
[00:01:59.840 --> 00:02:07.720]   So the goal of the authors, Liu and everyone of the co-authors is to modernize a ResNet.
[00:02:07.720 --> 00:02:13.640]   They ask the question, hey, transformers have done so well in recent times.
[00:02:13.640 --> 00:02:16.360]   Can we take any tricks?
[00:02:16.360 --> 00:02:25.680]   Sorry, can we take any tricks from transformers and bring them over to a ResNet like architecture?
[00:02:25.680 --> 00:02:29.180]   Can we bring them over to CNNs and make them perform better?
[00:02:29.180 --> 00:02:31.560]   Can we improve their performance?
[00:02:31.560 --> 00:02:35.800]   That's the question that this paper tries to answer by applying all of the ideas and
[00:02:35.800 --> 00:02:40.680]   the paper is actually a very nice readable format where they individually apply ideas
[00:02:40.680 --> 00:02:43.720]   and showcase how much of an improvement it shows.
[00:02:43.720 --> 00:02:46.240]   This is the graph that tells you about that.
[00:02:46.240 --> 00:02:49.280]   If you actually read the original paper, there should be a gray bar.
[00:02:49.280 --> 00:02:52.080]   I scanned this back so that's why you don't see it.
[00:02:52.080 --> 00:02:58.000]   But the authors apply step by step decisions and compare its impact on two things, accuracy
[00:02:58.000 --> 00:02:59.560]   and computation.
[00:02:59.560 --> 00:03:01.080]   These are really discussed together.
[00:03:01.080 --> 00:03:07.480]   Now we're in 2022 and these are being discussed in the same paper, which is really nice to
[00:03:07.480 --> 00:03:08.480]   see.
[00:03:08.480 --> 00:03:14.120]   So what they do is they apply some changes in the micro design, they copy the steel ideas
[00:03:14.120 --> 00:03:16.880]   from ResNext, bring it over to this.
[00:03:16.880 --> 00:03:21.360]   They apply an inverted bottleneck, change the kernel size, change some micro designs
[00:03:21.360 --> 00:03:24.720]   and all of these individual contributions are visible in this graph.
[00:03:24.720 --> 00:03:27.320]   So how does that affect the accuracy?
[00:03:27.320 --> 00:03:29.960]   How does that affect the computation?
[00:03:29.960 --> 00:03:35.460]   And towards the end, they reach a conclusion or reach an architecture which is slightly
[00:03:35.460 --> 00:03:41.400]   better than a Swin T transformer and has the equal amount of computation requirements,
[00:03:41.400 --> 00:03:43.320]   which is really good to see.
[00:03:43.320 --> 00:03:45.480]   So what are the key learnings from here?
[00:03:45.480 --> 00:03:49.840]   First of all, there's no novel additions done by the authors.
[00:03:49.840 --> 00:03:51.120]   Nothing new has been applied here.
[00:03:51.120 --> 00:03:56.400]   They've just taken older techniques and brought them over.
[00:03:56.400 --> 00:04:00.640]   They've added a patchify layer from, if you've watched our previous paper reading groups,
[00:04:00.640 --> 00:04:04.480]   you would know this, patchify layer from a transformer model.
[00:04:04.480 --> 00:04:09.520]   They have used grouped convolutions to reduce computation.
[00:04:09.520 --> 00:04:14.360]   They've reduced the number of inverted blocks, used a large kernel size and convolutions
[00:04:14.360 --> 00:04:21.200]   of 7x7, replaced ReLU by GeLU and used fewer normalization layers, wherein they replaced
[00:04:21.200 --> 00:04:24.460]   patch norm with layer norm.
[00:04:24.460 --> 00:04:25.720]   This is a summary of the paper.
[00:04:25.720 --> 00:04:34.400]   So using all of this, they arrive at an architecture which is a bit comparable to modern transformers.
[00:04:34.400 --> 00:04:35.480]   I hope you don't leave now.
[00:04:35.480 --> 00:04:36.480]   There's more to this.
[00:04:36.480 --> 00:04:38.360]   So I'll continue further.
[00:04:38.360 --> 00:04:41.480]   Let me hop over to my slides and continue from there.
[00:04:41.480 --> 00:04:43.240]   I'll quickly make a short announcement.
[00:04:43.240 --> 00:04:49.260]   We got a few requests of making our paper reading group more structured.
[00:04:49.260 --> 00:04:53.920]   So for that reason, we now have a repository which will contain more resources.
[00:04:53.920 --> 00:05:00.480]   So the vision for me is for anyone to be able to read a paper and read it in a format where
[00:05:00.480 --> 00:05:03.480]   you can learn everything that you need to know from a paper.
[00:05:03.480 --> 00:05:07.880]   You don't, the end goal for me is you don't need to come to our paper reading groups for
[00:05:07.880 --> 00:05:09.620]   you to get the knowledge.
[00:05:09.620 --> 00:05:15.880]   So this will have all of the groundbreaking papers, so to speak, in an arranged fashion
[00:05:15.880 --> 00:05:23.500]   where you can read them, take your learnings from there and also find more resources if
[00:05:23.500 --> 00:05:26.360]   you want to apply this knowledge elsewhere.
[00:05:26.360 --> 00:05:29.400]   Let me quickly hop over to that repository in a second.
[00:05:29.400 --> 00:05:33.900]   I know I'm jumping out quite a bit today, but I'll quickly get this out of the way.
[00:05:33.900 --> 00:05:40.000]   So for today, if you want to hop over to the repository, you'll just find Connext.
[00:05:40.000 --> 00:05:44.680]   But starting next week, every week, you'll have a new paper being added at the minimum,
[00:05:44.680 --> 00:05:49.600]   which will span across different subcategories of the field, computer vision, NLP, all of
[00:05:49.600 --> 00:05:50.600]   those.
[00:05:50.600 --> 00:05:54.040]   So you'll eventually be able to find all of the papers you want to read in just this one
[00:05:54.040 --> 00:05:58.080]   repository.
[00:05:58.080 --> 00:06:02.880]   Coming back to my slides, and I'll resume from here.
[00:06:02.880 --> 00:06:04.200]   Here's the agenda for today.
[00:06:04.200 --> 00:06:07.160]   I wanted to tell you about the Weights and Biases reading group.
[00:06:07.160 --> 00:06:08.440]   So I've done that.
[00:06:08.440 --> 00:06:10.600]   We'll go about reading the Connext paper.
[00:06:10.600 --> 00:06:15.640]   We'll do a walkthrough of the implementation from the TIM framework.
[00:06:15.640 --> 00:06:19.600]   And we'll also walk away with some suggested homework.
[00:06:19.600 --> 00:06:22.240]   I try to always leave that as a suggestion.
[00:06:22.240 --> 00:06:24.040]   It's always great to see someone implement that.
[00:06:24.040 --> 00:06:27.800]   So we'll continue following that format.
[00:06:27.800 --> 00:06:35.120]   I've used the following resources, the Connext paper, the WANB papers repository, the TIM
[00:06:35.120 --> 00:06:40.000]   repository, and the videos by AI Epiphany and Leticia.
[00:06:40.000 --> 00:06:45.040]   I'll quickly show their channel in a second.
[00:06:45.040 --> 00:06:52.200]   And coming back to the agenda, so to give you more context, this has been the history
[00:06:52.200 --> 00:06:55.600]   of computer vision at a very broad level.
[00:06:55.600 --> 00:07:00.560]   This by the way, I learned of TANIT through AI Epiphany's video.
[00:07:00.560 --> 00:07:04.120]   So shout out to Alexei for making awesome videos.
[00:07:04.120 --> 00:07:09.520]   This was one of the first groundbreaking papers, so to speak, in computer vision that really
[00:07:09.520 --> 00:07:15.360]   changed or showed everyone that, hey, CNNs can be applied really well in this field.
[00:07:15.360 --> 00:07:21.120]   In 2012, we had the AlexNet moment that got us all hopefully jobs by this point in the
[00:07:21.120 --> 00:07:23.640]   field of machine learning and kicked off this field.
[00:07:23.640 --> 00:07:25.520]   2015 had ResNet.
[00:07:25.520 --> 00:07:28.560]   In 2017, Attention is All You Need came out.
[00:07:28.560 --> 00:07:34.000]   And then two years ago, we started seeing the same transformer models being ported over
[00:07:34.000 --> 00:07:35.840]   to computer vision.
[00:07:35.840 --> 00:07:38.080]   So we saw the VIT era.
[00:07:38.080 --> 00:07:42.040]   In the paper, as you'll see, the authors mentioned this as an entire era.
[00:07:42.040 --> 00:07:50.520]   So the VIT paper sort of revolutionized a transformer movement, where after it, we saw
[00:07:50.520 --> 00:07:54.560]   many new, I want to say, architectures come in.
[00:07:54.560 --> 00:08:02.560]   So this has been the computer vision overview of the last decade or so.
[00:08:02.560 --> 00:08:08.880]   And I wanted to put in the Thor meme, but I hope you get the point.
[00:08:08.880 --> 00:08:13.320]   So in this, the authors try to, in this paper, the authors try to answer the question, are
[00:08:13.320 --> 00:08:19.120]   CNNs even worthy because transformers have become much more accurate and similar to computer
[00:08:19.120 --> 00:08:22.800]   vision, they've really taken over this field.
[00:08:22.800 --> 00:08:30.760]   So can they be also effectively applied here and make our life super easy?
[00:08:30.760 --> 00:08:34.560]   Because historically, they have used lesser computation.
[00:08:34.560 --> 00:08:42.280]   So as a refresher, and to credit this comes from Irhom Shafkat's blog.
[00:08:42.280 --> 00:08:43.680]   I don't think I can see the gif here.
[00:08:43.680 --> 00:08:46.160]   So I'll hop over to my slides like so.
[00:08:46.160 --> 00:08:47.800]   This is how CNNs work.
[00:08:47.800 --> 00:08:54.120]   So they are translation invariant, since they use a sliding window approach, wherein you
[00:08:54.120 --> 00:09:00.640]   convolve with the original image followed by the next few layers over and over and over
[00:09:00.640 --> 00:09:01.800]   again.
[00:09:01.800 --> 00:09:06.360]   So it makes it translation invariance in the sense that you don't need a multi layer perceptron
[00:09:06.360 --> 00:09:08.620]   connecting with every layer.
[00:09:08.620 --> 00:09:15.760]   But the sliding window approach has made CNNs really useful.
[00:09:15.760 --> 00:09:16.760]   Back to my slides.
[00:09:16.760 --> 00:09:17.760]   Today, I'm fast.
[00:09:17.760 --> 00:09:22.360]   I'm really happy that I'm not being really slow at switching windows.
[00:09:22.360 --> 00:09:28.840]   So the overview is, authors ask a question, how can we modernize a convolutional neural
[00:09:28.840 --> 00:09:29.840]   network?
[00:09:29.840 --> 00:09:32.000]   Can we apply tricks from a transformer?
[00:09:32.000 --> 00:09:38.280]   And then inside of the paper, it's a step by step study to see the gains in performance
[00:09:38.280 --> 00:09:40.460]   and accuracy.
[00:09:40.460 --> 00:09:45.880]   And towards the end of this talk, I'll try to paper reading talk, I'll try to answer
[00:09:45.880 --> 00:09:47.600]   the question, are CNNs back?
[00:09:47.600 --> 00:09:56.560]   Will we see more CNNs network or what should we expect?
[00:09:56.560 --> 00:10:01.000]   This is the image that got some debate on Twitter.
[00:10:01.000 --> 00:10:04.460]   And I'll tell you what the debate was about.
[00:10:04.460 --> 00:10:08.120]   This shows the accuracy on the y axis.
[00:10:08.120 --> 00:10:13.440]   And on the x axis, the first few models are ImageNet funky trained.
[00:10:13.440 --> 00:10:15.040]   So the smaller data set.
[00:10:15.040 --> 00:10:17.800]   The ones on the right are ImageNet 22k.
[00:10:17.800 --> 00:10:19.800]   Please pay attention to that.
[00:10:19.800 --> 00:10:25.880]   Now if you look at the size of these modules, the diameter denotes how much compute do they
[00:10:25.880 --> 00:10:30.880]   eat and how much are you going to cry in AWS bills.
[00:10:30.880 --> 00:10:35.180]   If you were to train these, luckily we have pre trained weights available everywhere.
[00:10:35.180 --> 00:10:41.520]   So as you can see, next is a convolutional architecture and it's a bit more accurate
[00:10:41.520 --> 00:10:46.360]   than Swin transformer in terms of accuracy.
[00:10:46.360 --> 00:10:50.600]   And I would say it's hard to compare just based on the image, but it looks similar in
[00:10:50.600 --> 00:10:58.520]   size or a bit smaller for computation compared to the VIT model, right for both ImageNet
[00:10:58.520 --> 00:11:02.480]   22k and ImageNet 1k.
[00:11:02.480 --> 00:11:10.160]   Now the authors of VIT paper actually themselves jumped onto this discussion.
[00:11:10.160 --> 00:11:15.000]   Lucas Bayer, I think he was one of the authors of the original VIT paper.
[00:11:15.000 --> 00:11:24.200]   He showed this image wherein he connected this original representation from the paper.
[00:11:24.200 --> 00:11:29.600]   So the authors and this is not to condemn any practice.
[00:11:29.600 --> 00:11:32.380]   We all showcase our work in the best practice.
[00:11:32.380 --> 00:11:38.520]   It's in the fine prints that the authors don't use image augmentations when doing the comparison
[00:11:38.520 --> 00:11:39.740]   for VIT.
[00:11:39.740 --> 00:11:47.680]   So if they were to use image augmentations, the original image would look like so.
[00:11:47.680 --> 00:11:53.360]   And as you can see, it's coming really close in terms of the accuracy.
[00:11:53.360 --> 00:11:56.080]   So let's read this thread real quickly.
[00:11:56.080 --> 00:11:59.360]   Hopefully that doesn't close it.
[00:11:59.360 --> 00:12:01.840]   It didn't.
[00:12:01.840 --> 00:12:07.440]   I wanted to point out that Ross Whitman, whose library we'll be using, he's one of the most
[00:12:07.440 --> 00:12:14.480]   incredible engineers in our field of computer vision and deep learning.
[00:12:14.480 --> 00:12:19.000]   So he jumped on and mentioned while they did mention that resonant training is the core
[00:12:19.000 --> 00:12:20.360]   of the paper.
[00:12:20.360 --> 00:12:27.720]   He rewrote the layer norm channel stuff and got an extra 20% boost.
[00:12:27.720 --> 00:12:33.080]   So Rohan, who's from, I think Google brain made a nice comment that echoes with everyone
[00:12:33.080 --> 00:12:35.480]   good engineering is all you need.
[00:12:35.480 --> 00:12:40.440]   So again, this paper is quite a lot about the details and it's good to know that even
[00:12:40.440 --> 00:12:44.120]   such techniques can make a lot of difference.
[00:12:44.120 --> 00:12:51.440]   Now coming back to my slide again, now that we have corrected the premise, I want to again,
[00:12:51.440 --> 00:12:54.840]   bring your attention to see diagram still.
[00:12:54.840 --> 00:12:59.120]   Connects is still somewhat of an improvement in terms of architecture, because if you pay
[00:12:59.120 --> 00:13:03.920]   attention, again, no one is going to sit down with a scale, I think to try and measure which
[00:13:03.920 --> 00:13:10.280]   has a bigger diameter, but broadly speaking, just eyeballing it in by while using the same
[00:13:10.280 --> 00:13:13.320]   amount of computation.
[00:13:13.320 --> 00:13:19.440]   It is somewhat more accurate than VIT for ImageNet 22k.
[00:13:19.440 --> 00:13:25.640]   And definitely one of the most accurate architectures for ImageNet 1k.
[00:13:25.640 --> 00:13:29.820]   That's the main thing to walk away with from here.
[00:13:29.820 --> 00:13:32.060]   So now I'll jump on to the paper reading bit.
[00:13:32.060 --> 00:13:38.400]   And actually, before that, I'll try to give a very quick overview of what's in the architecture
[00:13:38.400 --> 00:13:39.680]   is my Colab connected?
[00:13:39.680 --> 00:13:40.680]   Hopefully it is.
[00:13:40.680 --> 00:13:47.360]   So I'll pip install Tim by Ross Whitman.
[00:13:47.360 --> 00:13:50.760]   And import Tim.
[00:13:50.760 --> 00:13:55.200]   I want to showcase what's actually inside a conf next model to everyone.
[00:13:55.200 --> 00:13:56.200]   Awesome.
[00:13:56.200 --> 00:13:57.200]   I was able to import it.
[00:13:57.200 --> 00:13:59.200]   So things are looking good.
[00:13:59.200 --> 00:14:05.520]   So model is equal to Tim dot create model.
[00:14:05.520 --> 00:14:11.320]   Also, if someone knows this, please help me understand how the stab completion work in
[00:14:11.320 --> 00:14:12.320]   Google Colab.
[00:14:12.320 --> 00:14:15.880]   It's one of my frustrations with the platform.
[00:14:15.880 --> 00:14:19.760]   So if someone knows how to make it work, please let me know.
[00:14:19.760 --> 00:14:30.200]   Con next is equal to true.
[00:14:30.200 --> 00:14:34.400]   So maybe it has some annotation associated with it.
[00:14:34.400 --> 00:14:43.440]   I should probably do Tim dot list models.
[00:14:43.440 --> 00:14:46.960]   Let's mention con next in there.
[00:14:46.960 --> 00:14:49.520]   See if that works.
[00:14:49.520 --> 00:14:50.700]   Yes, it did.
[00:14:50.700 --> 00:14:52.160]   So we have all of these.
[00:14:52.160 --> 00:14:55.200]   And for the sake of simplicity, I'll grab the con next tiny.
[00:14:55.200 --> 00:15:00.080]   Again, I'm doing this to showcase inside of fire torch what all modules go into this.
[00:15:00.080 --> 00:15:01.280]   Right.
[00:15:01.280 --> 00:15:03.560]   So let me replace that with the correct one.
[00:15:03.560 --> 00:15:06.280]   Hopefully this should download it.
[00:15:06.280 --> 00:15:12.520]   Ross actually mentioned in a Twitter thread, he has this awesome garage filled with GPUs,
[00:15:12.520 --> 00:15:14.640]   which feels like a dream to me.
[00:15:14.640 --> 00:15:19.120]   But he himself trains all of these modules and makes them available for all of us to
[00:15:19.120 --> 00:15:20.400]   utilize.
[00:15:20.400 --> 00:15:22.560]   So please do check out the Tim library.
[00:15:22.560 --> 00:15:26.240]   I think it's an awesome framework.
[00:15:26.240 --> 00:15:32.080]   And the best way to find it would be if I just Google T I am T I am and GitHub, it should
[00:15:32.080 --> 00:15:33.080]   take me there.
[00:15:33.080 --> 00:15:38.040]   Yep, that looks like it.
[00:15:38.040 --> 00:15:40.200]   This is the library I wanted to mention.
[00:15:40.200 --> 00:15:44.960]   I'm sure everyone in this call is already familiar with it.
[00:15:44.960 --> 00:15:46.640]   So we've downloaded the model weights.
[00:15:46.640 --> 00:15:48.400]   And now let's take a look.
[00:15:48.400 --> 00:15:51.800]   I think every one of us just wants to apply these models.
[00:15:51.800 --> 00:15:55.680]   So to me, it's more practical to first of all, take a look at what's there inside of
[00:15:55.680 --> 00:16:02.240]   this implementation and then take a look at the paper and try to read it.
[00:16:02.240 --> 00:16:06.900]   That's what I'm trying to do here.
[00:16:06.900 --> 00:16:09.800]   Coming back to the model.
[00:16:09.800 --> 00:16:13.600]   And let's scroll up to the top.
[00:16:13.600 --> 00:16:14.600]   I'm quite zoomed in.
[00:16:14.600 --> 00:16:20.160]   So this might be a little annoying, please bear with me, let me hide myself for a second.
[00:16:20.160 --> 00:16:27.160]   So this is conf next tiny, the smallest module possible, I think.
[00:16:27.160 --> 00:16:31.200]   Inside of this, we see a conf 2d layer with the following parameters.
[00:16:31.200 --> 00:16:36.960]   So let's first of all check in, I touch talks.
[00:16:36.960 --> 00:16:39.960]   What do these do?
[00:16:39.960 --> 00:16:44.080]   I almost start rhyming when I'm trying to look up stuff.
[00:16:44.080 --> 00:16:49.080]   That's not a good sign.
[00:16:49.080 --> 00:16:56.980]   So the first would be the number of channels, in channels, out channels and kernel size.
[00:16:56.980 --> 00:17:03.080]   So this connects tiny model starts with three input 96 output, four by four kernel size
[00:17:03.080 --> 00:17:09.400]   with a four by four stride, followed by layer norm 2d layer.
[00:17:09.400 --> 00:17:16.100]   And this is the stem, followed by different stages inside of the stages.
[00:17:16.100 --> 00:17:22.600]   You will see more conf next blocks following the same approach, but this time, the kernel
[00:17:22.600 --> 00:17:25.540]   size becomes seven by seven.
[00:17:25.540 --> 00:17:29.320]   And this persists throughout all connects blocks.
[00:17:29.320 --> 00:17:33.420]   So these blocks will be repeated a bunch of times as you can imagine, and they'll have
[00:17:33.420 --> 00:17:36.920]   an identity layer between them.
[00:17:36.920 --> 00:17:41.760]   After the conf 2d layers after the convolutions will always have a layer norm.
[00:17:41.760 --> 00:17:47.020]   In this paper, the authors have replaced batch normalization with layer normalization.
[00:17:47.020 --> 00:17:49.780]   That's one of the key implementation details.
[00:17:49.780 --> 00:17:54.580]   And they have also replaced ReLU with GELU.
[00:17:54.580 --> 00:17:57.480]   I've read the paper that's I can tell you all.
[00:17:57.480 --> 00:18:02.200]   So as you can see, I remembered it correctly.
[00:18:02.200 --> 00:18:05.040]   We see a GELU activation.
[00:18:05.040 --> 00:18:09.580]   And I think all of these are pretty standard inside of the MLP layer.
[00:18:09.580 --> 00:18:12.280]   So I won't highlight these approaches.
[00:18:12.280 --> 00:18:17.200]   One thing to note here is the number of activations inside of the MLP layer is also reduced.
[00:18:17.200 --> 00:18:21.480]   So if you actually look into the fine details of any other implementation, you would observe
[00:18:21.480 --> 00:18:23.480]   that.
[00:18:23.480 --> 00:18:34.800]   These blocks are repeated a few times, followed by a lot of scrolling, followed by some more
[00:18:34.800 --> 00:18:39.160]   scrolling.
[00:18:39.160 --> 00:18:44.120]   And finally, we get to the global pooling layer.
[00:18:44.120 --> 00:18:51.640]   One last normalization, then we flatten this output and get the outputs from here.
[00:18:51.640 --> 00:18:57.520]   Since I assume this would be on ImageNet 1k, the outputs in this case is 1000.
[00:18:57.520 --> 00:19:01.720]   So now we have taken a look at the implementation.
[00:19:01.720 --> 00:19:06.420]   So this was for the smallest architecture that came out of the paper.
[00:19:06.420 --> 00:19:12.480]   Now we're good to hop on to the paper annotation.
[00:19:12.480 --> 00:19:15.760]   But I'll quickly mention the channels I referred to earlier.
[00:19:15.760 --> 00:19:22.880]   The AI Epiphany is an awesome channel to watch these paper discussions as well.
[00:19:22.880 --> 00:19:28.240]   And AI Coffee Break, although I'm not a fan of one of the words in the title, with Leticia
[00:19:28.240 --> 00:19:32.320]   is also an absolutely incredible, lovely channel to watch.
[00:19:32.320 --> 00:19:34.920]   These videos are much shorter as well if you'd like to check them out.
[00:19:34.920 --> 00:19:41.440]   So I watch these videos and I'm going to use some of their discussions today as well.
[00:19:41.440 --> 00:19:48.240]   Let me now hop over to my trusty OneNote real quick.
[00:19:48.240 --> 00:19:49.240]   I'm quite fast today.
[00:19:49.240 --> 00:19:52.720]   I'm honestly impressed because I'm usually really slow at this.
[00:19:52.720 --> 00:19:59.400]   So as I mentioned earlier, you can find this exact annotation in this link where you can
[00:19:59.400 --> 00:20:02.400]   download it for yourself if you want to follow along today.
[00:20:02.400 --> 00:20:07.160]   So this was based on all of your feedback and we have made this available ahead of time
[00:20:07.160 --> 00:20:13.260]   and we'll continue doing so for all of the future paper reading groups.
[00:20:13.260 --> 00:20:23.080]   So as I mentioned, the main question addressed, this is the summary of the paper above.
[00:20:23.080 --> 00:20:31.200]   The main point addressed in this paper is how do the design decisions in a transformer
[00:20:31.200 --> 00:20:34.240]   affect convolutional neural networks?
[00:20:34.240 --> 00:20:43.840]   And they take this approach of modernizing a ResNet-50.
[00:20:43.840 --> 00:20:49.600]   I need a smaller pen.
[00:20:49.600 --> 00:20:56.880]   Making macro design changes, making micro design changes and applying an inverted bottleneck.
[00:20:56.880 --> 00:21:04.560]   So they apply all of these ideas from a transformer to a connet and see how they improve.
[00:21:04.560 --> 00:21:12.040]   So they start by mentioning the Roaring 20s was kicked off by the VIT network.
[00:21:12.040 --> 00:21:14.560]   I assume this is visible to everyone.
[00:21:14.560 --> 00:21:15.760]   Should I zoom in a bit more?
[00:21:15.760 --> 00:21:21.760]   Could someone please confirm?
[00:21:21.760 --> 00:21:40.480]   Could someone please confirm in the chat real quick if this is visible to everyone?
[00:21:40.480 --> 00:21:42.360]   Awesome.
[00:21:42.360 --> 00:21:43.360]   Someone says all good.
[00:21:43.360 --> 00:21:45.240]   So I trust them enough to continue further.
[00:21:45.240 --> 00:21:46.720]   Thank you, Alan.
[00:21:46.720 --> 00:21:51.440]   It's this weird anxious moment where I have to wait for those 20 seconds for someone to
[00:21:51.440 --> 00:21:52.440]   write.
[00:21:52.440 --> 00:21:53.960]   Sorry for the awkward pause.
[00:21:53.960 --> 00:21:59.760]   So as I mentioned, the Roaring 20s here refers to, this would be too big, but I'll write
[00:21:59.760 --> 00:22:00.760]   still.
[00:22:00.760 --> 00:22:04.200]   VIT, how do I make this smaller?
[00:22:04.200 --> 00:22:09.200]   Can I make it smaller?
[00:22:09.200 --> 00:22:13.320]   Yes.
[00:22:13.320 --> 00:22:17.920]   So the Roaring 20s is actually referring to the VIT paper which kicked off the transformer
[00:22:17.920 --> 00:22:18.920]   movement.
[00:22:18.920 --> 00:22:25.600]   They really took over the convolutional neural network world.
[00:22:25.600 --> 00:22:32.880]   And a vanilla VIT has these challenges of not being able to generalize to object detection
[00:22:32.880 --> 00:22:34.340]   and semantic segmentation.
[00:22:34.340 --> 00:22:39.640]   So while they work really well with image classification, the transformer models don't
[00:22:39.640 --> 00:22:45.800]   work well with the other image tasks.
[00:22:45.800 --> 00:22:48.660]   Computer vision task is a more accurate term.
[00:22:48.660 --> 00:22:56.720]   So Swin transformer uses a hierarchical approach.
[00:22:56.720 --> 00:22:57.720]   I got it right.
[00:22:57.720 --> 00:22:58.720]   Sorry.
[00:22:58.720 --> 00:23:08.440]   Where it, actually, if you have read the paper, it has a lot of slices where attention is
[00:23:08.440 --> 00:23:10.800]   being applied and these get reduced.
[00:23:10.800 --> 00:23:15.600]   So let's say if you had four, it would be two furthermore.
[00:23:15.600 --> 00:23:24.360]   And it follows this hierarchical, I got it right this time, approach of applying attention.
[00:23:24.360 --> 00:23:27.760]   So that's what the authors are referring to.
[00:23:27.760 --> 00:23:33.480]   But the key thing about Swin transformer was while they were applying all of these things,
[00:23:33.480 --> 00:23:38.240]   they were also using a lot of convolutions in there and a lot of tricks from the world
[00:23:38.240 --> 00:23:40.380]   of CNNs.
[00:23:40.380 --> 00:23:48.020]   So this brings the author to the question, the large effectiveness of these hybrid approaches
[00:23:48.020 --> 00:23:57.180]   has to be credited to the superiority of transformers rather than convolutions.
[00:23:57.180 --> 00:24:04.980]   So they re-examine the design space and gradually modernize a ResNet while applying designs
[00:24:04.980 --> 00:24:09.180]   of a transformer to check and see if they can improve.
[00:24:09.180 --> 00:24:18.500]   And they do improve since they claim furthermore in this paragraph, one second, that just doing
[00:24:18.500 --> 00:24:27.920]   this they are able to achieve 87.8, sorry about that, 87.8 ImageNet top 1 accuracy outperforming
[00:24:27.920 --> 00:24:36.100]   Swin transformer on COCO and ADEC20 segmentation as well while just remaining pure convolution
[00:24:36.100 --> 00:24:39.100]   neural networks.
[00:24:39.100 --> 00:24:44.200]   So I will skip over the introduction since that just talks over all of these architectures
[00:24:44.200 --> 00:24:52.080]   that have been introduced and we have already gone through these images.
[00:24:52.080 --> 00:24:56.420]   The authors mentioned one powerful paragraph in the introduction that the dominance of
[00:24:56.420 --> 00:25:03.020]   ConNets in computer vision was not a coincidence in many applications, the sliding window that
[00:25:03.020 --> 00:25:08.420]   we had looked at, so remember the blue animation we just saw where the window slides over and
[00:25:08.420 --> 00:25:14.020]   performs convolution, that's what they are talking about.
[00:25:14.020 --> 00:25:19.260]   The sliding window strategy is intrinsic to visual processing particularly when with high
[00:25:19.260 --> 00:25:21.420]   resolution images.
[00:25:21.420 --> 00:25:26.100]   There's one challenge with transformers that when you are applying all of these things
[00:25:26.100 --> 00:25:29.420]   there is a quadratic dependency.
[00:25:29.420 --> 00:25:37.300]   So remember those scary interviews hopefully that gave everyone some terror where you had
[00:25:37.300 --> 00:25:41.780]   to figure out how much computation requirements different algorithms have.
[00:25:41.780 --> 00:25:44.940]   These do play into your GPU as well.
[00:25:44.940 --> 00:25:51.340]   Transformers have this problem that this dependency which would make you fail an interview but
[00:25:51.340 --> 00:25:55.060]   makes transformer super reasonable.
[00:25:55.060 --> 00:25:56.660]   They have a quadratic dependency.
[00:25:56.660 --> 00:26:00.660]   I'm simplifying this for the sake of the discussion which means if you are to increase the image
[00:26:00.660 --> 00:26:08.260]   dimension, you would need a DGX station and a lot of money in your AWS account credit
[00:26:08.260 --> 00:26:10.180]   card.
[00:26:10.180 --> 00:26:13.420]   So that's the challenge that they mentioned.
[00:26:13.420 --> 00:26:19.060]   ConNets have several built-in inductive biases that make them well suited for computer vision.
[00:26:19.060 --> 00:26:25.800]   One of them is translation equivariance which we had earlier looked at.
[00:26:25.800 --> 00:26:33.740]   So that is the introduction and from here they mentioned that VIT took over the world
[00:26:33.740 --> 00:26:39.380]   and the key thing to remember from VIT is except from the initial patchify layer, so
[00:26:39.380 --> 00:26:44.140]   what they do inside of the VIT paper which we have covered but I'll quickly summarize,
[00:26:44.140 --> 00:26:50.380]   they take the images and cut it into smaller bits and then feed this into the model.
[00:26:50.380 --> 00:26:57.360]   So imagine my image and just cutting it into smaller bits and feeding it into a model.
[00:26:57.360 --> 00:27:00.760]   Think of it like an embedding or something like this.
[00:27:00.760 --> 00:27:04.740]   As I mentioned the AI epiphany also covers this in this video much better.
[00:27:04.740 --> 00:27:08.080]   You can also watch that for an alternate explanation.
[00:27:08.080 --> 00:27:15.760]   So the key thing about vision transformers is except for the patchify layer, VIT introduces
[00:27:15.760 --> 00:27:22.840]   no image specific inductive bias and makes minimal changes to original NLP transformers.
[00:27:22.840 --> 00:27:24.840]   So remember attention is all you need.
[00:27:24.840 --> 00:27:31.800]   VIT just uses start approach with a patchify layer and with zero changes and just using
[00:27:31.800 --> 00:27:36.520]   the original architecture they are able to really achieve a lot of things.
[00:27:36.520 --> 00:27:43.680]   Sorry I was just checking the questions if there were any.
[00:27:43.680 --> 00:27:48.360]   But VIT needs more compute.
[00:27:48.360 --> 00:27:54.600]   And Swin transformer which as I mentioned has this hierarchical, I got it right again,
[00:27:54.600 --> 00:27:59.880]   approach of applying attention to image inputs.
[00:27:59.880 --> 00:28:05.440]   Its success and rapid adoption revealed one thing that convolutions are important as well
[00:28:05.440 --> 00:28:11.440]   since it applies convolutions and it's this hybrid model that's also an overlap of transformers
[00:28:11.440 --> 00:28:14.280]   and CNN since it comes into this territory.
[00:28:14.280 --> 00:28:16.880]   It showcased that CNNs were quite important.
[00:28:16.880 --> 00:28:22.200]   So far the authors are just trying to get this fact that hey you should still consider
[00:28:22.200 --> 00:28:25.280]   CNNs it's not just attention is all you need at this point.
[00:28:25.280 --> 00:28:36.280]   We are not at that point yet.
[00:28:36.280 --> 00:28:42.200]   But the challenge here is self-attention scales really well and transformers have been outperforming
[00:28:42.200 --> 00:28:47.080]   ConNets because of that.
[00:28:47.080 --> 00:28:52.600]   So finally the authors mentioned that they use the design decisions in transformers to
[00:28:52.600 --> 00:28:58.560]   check the ConNets performance and they propose a family of pure ConNets and inside the family
[00:28:58.560 --> 00:29:04.400]   we looked at the smallest version called ConNext Tiny if I remember correctly and this entire
[00:29:04.400 --> 00:29:13.480]   family is called ConNext Tiny, Base, Extra Large etc.
[00:29:13.480 --> 00:29:19.140]   So now the authors share their road map of modernizing a ConNet.
[00:29:19.140 --> 00:29:24.280]   In this section they provide a trajectory of going from a ResNet to a ConNet that bears
[00:29:24.280 --> 00:29:30.940]   resemblance to a transformer model and they use two model size.
[00:29:30.940 --> 00:29:35.280]   So one is the ResNet-50 and the other one is a ResNet-200.
[00:29:35.280 --> 00:29:41.120]   ResNet-50 fights with, I've been watching too many boxing matches sorry for the hostile
[00:29:41.120 --> 00:29:50.320]   word but ResNet-50 compares with Swin Tiny and ResNet-200 compares with Swin Base and
[00:29:50.320 --> 00:29:59.760]   both of these have flops around the 10 to the power 9 range broadly speaking.
[00:29:59.760 --> 00:30:04.800]   So ResNet-50 is trained like a transformer and the key things that they apply here is
[00:30:04.800 --> 00:30:14.640]   they apply, sorry item W, they train it for more epochs and apply some image augmentations.
[00:30:14.640 --> 00:30:22.600]   So let's come back to this image after going through this text real quickly.
[00:30:22.600 --> 00:30:25.460]   First of all the authors mentioned their training techniques.
[00:30:25.460 --> 00:30:31.120]   So as I mentioned they use a training recipe similar to DEIT which I also believe we have
[00:30:31.120 --> 00:30:39.840]   covered in our playlist so please check that out to learn the exact details.
[00:30:39.840 --> 00:30:42.280]   Sorry I see a question which I should address now.
[00:30:42.280 --> 00:30:48.600]   What does scaling mean for self-attention?
[00:30:48.600 --> 00:30:51.540]   Throw GPU problem, go burr.
[00:30:51.540 --> 00:30:58.720]   So if you try to scale up the model size and train it on larger resolution and try to increase
[00:30:58.720 --> 00:31:05.500]   the model size that scales well and results in a higher correlation of accuracy being
[00:31:05.500 --> 00:31:06.640]   improved as well.
[00:31:06.640 --> 00:31:11.840]   That's what scaling for self-attention means I believe.
[00:31:11.840 --> 00:31:15.160]   I hope that answers your question.
[00:31:15.160 --> 00:31:23.320]   So they use the recipe from DEIT and Swin transformer using which they first of all
[00:31:23.320 --> 00:31:32.380]   train the model for 300 epochs, seems reasonable and use the item W optimizer and then they
[00:31:32.380 --> 00:31:40.820]   apply all of these data augmentations, mix up, cut mix, random augment, random erasing
[00:31:40.820 --> 00:31:45.300]   and some regularization schemes like stochastic depth and label smoothing.
[00:31:45.300 --> 00:31:54.700]   So these are the broad things that they use from training techniques in DEIT and Swin
[00:31:54.700 --> 00:32:01.260]   transformer and just using these techniques they are able to improve the accuracy by 2.7%.
[00:32:01.260 --> 00:32:06.500]   One thing that's really fun to read about this paper is they give you a reason, here's
[00:32:06.500 --> 00:32:11.500]   why we think you should do this because it worked for something earlier.
[00:32:11.500 --> 00:32:14.460]   We tried this and here's what it did.
[00:32:14.460 --> 00:32:16.220]   It gave us this much boost.
[00:32:16.220 --> 00:32:20.420]   This entire paper or the text from here will read like that.
[00:32:20.420 --> 00:32:22.900]   So we think you should increase the kernel size.
[00:32:22.900 --> 00:32:27.140]   We increase the kernel size, here's how much accuracy improved and from here we'll assume
[00:32:27.140 --> 00:32:29.140]   we always do that.
[00:32:29.140 --> 00:32:33.180]   So that's one thing I really enjoyed about this paper.
[00:32:33.180 --> 00:32:37.640]   So here say this just increased the accuracy by 2.7%.
[00:32:37.640 --> 00:32:43.580]   That's a small number, yes, but they're also microly optimizing different things.
[00:32:43.580 --> 00:32:48.740]   Please remember that we're looking at small things at every stage.
[00:32:48.740 --> 00:32:52.900]   So I've written that, sounds like something I would write.
[00:32:52.900 --> 00:32:57.720]   But one more thing that they mention here is they change the stage compute ratio.
[00:32:57.720 --> 00:33:03.300]   So I think AI Epiphany mentioned in his video, but if you go into the ResNet-50 implementation,
[00:33:03.300 --> 00:33:13.340]   you can see how the blocks are defined in there in the PyTorch official repository.
[00:33:13.340 --> 00:33:17.400]   I'm trying to highlight the key things here by quickly reading.
[00:33:17.400 --> 00:33:25.060]   So the main thing addressed here is ResNets use a different compute ratio compared to
[00:33:25.060 --> 00:33:32.340]   Swin transformer and they adjust the number of blocks from this original number to 339
[00:33:32.340 --> 00:33:41.900]   and S3, which makes the flops or the computation requirement of this ResNet that we are trying
[00:33:41.900 --> 00:33:43.500]   to modernize.
[00:33:43.500 --> 00:33:52.000]   Everything applied here is applied to ResNet-50 and ResNet-200 model and our competitor or
[00:33:52.000 --> 00:33:58.220]   the thing we're trying to compete against is a Swin tiny and Swin based architecture.
[00:33:58.220 --> 00:34:00.780]   Please remember that.
[00:34:00.780 --> 00:34:10.260]   So using these approaches, we bring in the flops close to Swin transformer, sorry, and
[00:34:10.260 --> 00:34:14.160]   increase the accuracy by looks like 0.6%.
[00:34:14.160 --> 00:34:19.540]   So from now on, they will use this stage compute ratio again, logic, reasoning, application,
[00:34:19.540 --> 00:34:22.160]   from there they continue.
[00:34:22.160 --> 00:34:26.140]   Then they change the stem to patchify.
[00:34:26.140 --> 00:34:28.060]   I had mentioned this earlier.
[00:34:28.060 --> 00:34:29.620]   So I'll skip over this.
[00:34:29.620 --> 00:34:34.640]   They replace the ResNet style stem cell with a patchify layer implemented using a 4 by
[00:34:34.640 --> 00:34:38.940]   4 and a stride of 4.
[00:34:38.940 --> 00:34:41.820]   We had looked at this in the implementation.
[00:34:41.820 --> 00:34:46.460]   One thing to note here, this is just the stem of the model.
[00:34:46.460 --> 00:34:49.980]   Furthermore in the model, we'll just read in the next paragraph, the convolutions are
[00:34:49.980 --> 00:34:51.500]   of 7 by 7 dimensions.
[00:34:51.500 --> 00:34:55.580]   So just for the patchify layer, they are of a 4 by 4 dimension.
[00:34:55.580 --> 00:34:56.580]   Please pay attention to that.
[00:34:56.580 --> 00:34:58.900]   I got confused and I was looking at the implementation.
[00:34:58.900 --> 00:35:00.980]   Hey, Ross couldn't get this wrong.
[00:35:00.980 --> 00:35:01.980]   How could he get it wrong?
[00:35:01.980 --> 00:35:02.980]   Why is it 4 by 4?
[00:35:02.980 --> 00:35:12.340]   And then I read the paper and realized, oh, it's just the stem of the model.
[00:35:12.340 --> 00:35:19.460]   This thing, this patchify layer improves the accuracy by 0.1%.
[00:35:19.460 --> 00:35:26.140]   There have been some discussions, are we micro-optimizing models at this point?
[00:35:26.140 --> 00:35:32.340]   Yes, but very few papers talk about individual details, right?
[00:35:32.340 --> 00:35:34.540]   Of how did they arrive at this?
[00:35:34.540 --> 00:35:40.180]   If you've read historical papers, they're just like, oh, we made 10 changes.
[00:35:40.180 --> 00:35:41.540]   Here's the architecture.
[00:35:41.540 --> 00:35:46.260]   But to me, this is really interesting to read in a way, because you see these individual
[00:35:46.260 --> 00:35:47.980]   changes and their contribution.
[00:35:47.980 --> 00:35:53.380]   And one of the homeworks, as I'll tell you later, is what can we take from this approach
[00:35:53.380 --> 00:35:57.340]   and how can we also consider modernizing other architectures?
[00:35:57.340 --> 00:36:02.480]   So yes, it's a small improvement, but again, it's a small step as well.
[00:36:02.480 --> 00:36:10.580]   So from now on, then use the patchify stem for all the models.
[00:36:10.580 --> 00:36:13.420]   Now they resnextify the model.
[00:36:13.420 --> 00:36:18.460]   So they look at the resnext architecture, which has better flops and accuracy trade-off
[00:36:18.460 --> 00:36:20.460]   than a resnet.
[00:36:20.460 --> 00:36:26.860]   And then say, the key thing here was a grouped convolution, where the convolutional filters
[00:36:26.860 --> 00:36:30.740]   are separated into different groups.
[00:36:30.740 --> 00:36:37.220]   The guiding principle there was use more groups, expand width.
[00:36:37.220 --> 00:36:42.580]   So they apply that and they use depthwise convolution, a special case of grouped convolution,
[00:36:42.580 --> 00:36:49.020]   where the number of groups equals the number of channels.
[00:36:49.020 --> 00:36:52.960]   This has also been used in MobileNet and Exception.
[00:36:52.960 --> 00:36:59.300]   So depthwise convolution, they also mentioned, is similar to weighted sum operation in self-attention.
[00:36:59.300 --> 00:37:02.620]   This is, again, a comparison to transformers.
[00:37:02.620 --> 00:37:05.740]   They say this brings them closer.
[00:37:05.740 --> 00:37:10.940]   And they say this reduces the network's flops as expected, since we started with the assumption,
[00:37:10.940 --> 00:37:16.820]   hey, we should look at resnext because they have a better flop/accuracy trade-off.
[00:37:16.820 --> 00:37:18.040]   They try that.
[00:37:18.040 --> 00:37:20.000]   It brings down the flops as expected.
[00:37:20.000 --> 00:37:22.240]   The hypothesis worked.
[00:37:22.240 --> 00:37:29.180]   And this brings the network performance to 80.5% accuracy with an increased flops requirement.
[00:37:29.180 --> 00:37:34.580]   So for now, they'll use resnext design.
[00:37:34.580 --> 00:37:38.900]   Then they use an inverted bottleneck.
[00:37:38.900 --> 00:37:42.340]   Or they discuss the inverted bottleneck, sorry.
[00:37:42.340 --> 00:37:47.820]   In every transformer block, the hidden dimension of the multilayer perceptron is four times
[00:37:47.820 --> 00:37:49.900]   wider than the input dimension.
[00:37:49.900 --> 00:37:54.420]   They look at transformers and say, hmm, maybe you should learn something from here.
[00:37:54.420 --> 00:37:57.900]   So what do they do, as expected?
[00:37:57.900 --> 00:38:01.340]   This they can also mention another hypothesis.
[00:38:01.340 --> 00:38:07.900]   They say this is connected with the inverted bottleneck design of a ratio of four used
[00:38:07.900 --> 00:38:09.660]   in coordinates.
[00:38:09.660 --> 00:38:17.040]   This idea originally came from MobileNet V2 and has gained traction.
[00:38:17.040 --> 00:38:21.180]   So they explore this inverted bottleneck design.
[00:38:21.180 --> 00:38:28.420]   And in these images, they actually change the dimensions of the depth-wise corner layer
[00:38:28.420 --> 00:38:30.060]   and change different things.
[00:38:30.060 --> 00:38:34.460]   And this leads to the flops being reduced for 4.6.
[00:38:34.460 --> 00:38:39.260]   They were 5.3 when we applied the resnextification.
[00:38:39.260 --> 00:38:43.380]   And now they managed to bring it down to 4.6.
[00:38:43.380 --> 00:38:48.100]   This also finally improves the accuracy by 0.1%.
[00:38:48.100 --> 00:38:53.260]   And even for the larger model, it improves it even further.
[00:38:53.260 --> 00:38:56.200]   Again, very small gain 0.1%.
[00:38:56.200 --> 00:38:58.660]   If you ask a Kaggler, that's a lot.
[00:38:58.660 --> 00:39:01.500]   If you ask a researcher, maybe not so much.
[00:39:01.500 --> 00:39:06.980]   But we are micro-comparing decisions, which is really interesting to see.
[00:39:06.980 --> 00:39:10.340]   Next they mentioned that they use large kernels.
[00:39:10.340 --> 00:39:14.140]   And the summary of here is they just use 7x7 kernels.
[00:39:14.140 --> 00:39:16.740]   So remember, we had looked at the implementation.
[00:39:16.740 --> 00:39:21.740]   We start with the same 4x4 convolutions.
[00:39:21.740 --> 00:39:26.960]   And then inside the architecture, it's always 7x7.
[00:39:26.960 --> 00:39:32.340]   So they use 7x7 window sizes.
[00:39:32.340 --> 00:39:36.940]   Then they move up the depth-wise convolution layer.
[00:39:36.940 --> 00:39:43.780]   They said to explore large kernels, the prerequisite is to move up the position of depth-wise convolution
[00:39:43.780 --> 00:39:45.100]   layer.
[00:39:45.100 --> 00:39:50.620]   This design was also apparent in transformers where the attention block is placed prior
[00:39:50.620 --> 00:39:56.100]   to the MLP blocks.
[00:39:56.100 --> 00:39:59.200]   This reduces the flops to 4.1g.
[00:39:59.200 --> 00:40:07.480]   And also we take a performance hit and come down to 79.9%.
[00:40:07.480 --> 00:40:10.700]   They experiment with different kernel sizes.
[00:40:10.700 --> 00:40:13.900]   And they say around 7x7, you start to see a saturation.
[00:40:13.900 --> 00:40:15.860]   So let's stick with that number.
[00:40:15.860 --> 00:40:21.640]   So that's what they employ here.
[00:40:21.640 --> 00:40:25.800]   Now they're on to the part where we're talking about micro-decisions.
[00:40:25.800 --> 00:40:31.120]   So now we've looked at, I was calling them micro-decisions, but these are even smaller
[00:40:31.120 --> 00:40:32.620]   decisions.
[00:40:32.620 --> 00:40:36.960]   If anyone in the audience would be interested in answering this, I wasn't too clear about
[00:40:36.960 --> 00:40:41.640]   this, but the authors say they replace ReLU with GeLU.
[00:40:41.640 --> 00:40:46.440]   And I wasn't too clear of the logic behind it because first of all, it didn't change
[00:40:46.440 --> 00:40:48.160]   the accuracy.
[00:40:48.160 --> 00:40:49.160]   And I'm not too sure.
[00:40:49.160 --> 00:40:55.480]   Maybe I should just train the model with the ReLU layer.
[00:40:55.480 --> 00:40:59.200]   But the logic was some transformers use GeLU.
[00:40:59.200 --> 00:41:00.400]   So let's try that.
[00:41:00.400 --> 00:41:01.840]   And that doesn't change the accuracy.
[00:41:01.840 --> 00:41:08.960]   So my question to you all is, is there any reason apart from making the changes?
[00:41:08.960 --> 00:41:11.240]   Or what's the logic behind using GeLU here?
[00:41:11.240 --> 00:41:13.240]   I wasn't too clear on this.
[00:41:13.240 --> 00:41:17.480]   Anyways, I'll keep exploring that question.
[00:41:17.480 --> 00:41:22.360]   One more minor distinction they make between a transformer and a ResNet block is that transformer
[00:41:22.360 --> 00:41:26.240]   have fewer activation function.
[00:41:26.240 --> 00:41:35.360]   So that leads them to deciding GeLU activation, a single GeLU activation in every block.
[00:41:35.360 --> 00:41:42.520]   And this also improves the accuracy by 0.7%.
[00:41:42.520 --> 00:41:49.600]   They also decide that by looking at transformers, hey, transformers have fewer normalization
[00:41:49.600 --> 00:41:50.600]   layers.
[00:41:50.600 --> 00:41:53.960]   So they can take that decision and apply that here.
[00:41:53.960 --> 00:41:55.680]   They do that.
[00:41:55.680 --> 00:42:00.160]   Then they also substitute patch norm with layer norm.
[00:42:00.160 --> 00:42:04.760]   And the accuracy improves slightly.
[00:42:04.760 --> 00:42:07.040]   They separate the downsampling layers.
[00:42:07.040 --> 00:42:12.080]   They say in the ResNet, the spatial downsampling is achieved by the Res block.
[00:42:12.080 --> 00:42:19.280]   So if you remember the ResNet architecture, this is again a simplified representation,
[00:42:19.280 --> 00:42:22.360]   you would have some convolutions happening.
[00:42:22.360 --> 00:42:23.880]   This would be the input.
[00:42:23.880 --> 00:42:26.800]   This would be the hidden layer, let's say.
[00:42:26.800 --> 00:42:28.760]   And this would be the hidden layer 2.
[00:42:28.760 --> 00:42:30.440]   Again, very simplified representation.
[00:42:30.440 --> 00:42:35.840]   We would have this short circuit, I would say, of a residual block.
[00:42:35.840 --> 00:42:37.040]   They're referring to that.
[00:42:37.040 --> 00:42:43.680]   The downsampling is achieved by residual block at the start of each stage by using a 3 by
[00:42:43.680 --> 00:42:47.400]   3 convainer stride of 2.
[00:42:47.400 --> 00:42:52.640]   Using Swin transformers, so they look at ResNets and look at Swin transformers and
[00:42:52.640 --> 00:43:01.520]   they say, Swin transformers use a 2 by 2 convainer layer with a stride of 2 for downsampling.
[00:43:01.520 --> 00:43:07.000]   They apply that approach and say, let's use this as well.
[00:43:07.000 --> 00:43:09.000]   Let me confirm if they actually use that.
[00:43:09.000 --> 00:43:16.160]   Oh, no, sorry, I scrolled way too ahead.
[00:43:16.160 --> 00:43:19.800]   So they say, surprisingly, apologies, they don't apply that.
[00:43:19.800 --> 00:43:22.360]   They claim that this leads to diverged training.
[00:43:22.360 --> 00:43:24.760]   We want the models to converge.
[00:43:24.760 --> 00:43:30.040]   And then they investigate and say adding normalization layers where spatial training is changed can
[00:43:30.040 --> 00:43:33.240]   help stabilize training.
[00:43:33.240 --> 00:43:39.200]   So they say in the Swin transformer, you see several layer norm layers before each downsampling
[00:43:39.200 --> 00:43:41.040]   layer.
[00:43:41.040 --> 00:43:47.240]   And using this approach, they improve the accuracy to 82% and this time they outperform
[00:43:47.240 --> 00:43:50.360]   Swin transformer.
[00:43:50.360 --> 00:43:56.640]   Now to conclude all of these decisions, they claim that, no, we didn't discover a new approach.
[00:43:56.640 --> 00:43:58.760]   Nothing was novel here.
[00:43:58.760 --> 00:44:06.400]   But all of these design options led to, and I would say, a thorough analysis and actually
[00:44:06.400 --> 00:44:10.140]   outperforming and state of the art architecture.
[00:44:10.140 --> 00:44:15.360]   So now let's come back to this diagram that I wanted to mention.
[00:44:15.360 --> 00:44:19.440]   I'm not sure if the numbers are visible to everyone, but the graph is quite visual.
[00:44:19.440 --> 00:44:22.040]   So that should give you an idea.
[00:44:22.040 --> 00:44:29.760]   The bottom here represents a Swin tiny model and the Swin base model.
[00:44:29.760 --> 00:44:33.300]   And here we are looking at ResNet 50 and 200.
[00:44:33.300 --> 00:44:35.960]   So there's a gray bar in the background.
[00:44:35.960 --> 00:44:39.840]   I'm not sure it's visible to everyone that would represent ResNet 200.
[00:44:39.840 --> 00:44:41.880]   But let's just pay attention to ResNet 50.
[00:44:41.880 --> 00:44:47.960]   So the base model uses 4.1 GigaFLOPs and we start applying these changes.
[00:44:47.960 --> 00:44:50.600]   So as we looked earlier, we changed the stage ratio.
[00:44:50.600 --> 00:44:52.680]   We apply a patchify.
[00:44:52.680 --> 00:44:58.180]   Now we're up to 79.5% accuracy and 4.4 FLOPs.
[00:44:58.180 --> 00:45:05.840]   Then we ResNextify, apply inverted blocks, change the kernel sizes, make the micro decisions.
[00:45:05.840 --> 00:45:12.200]   And all of these decisions lead us to the ConfNext architecture.
[00:45:12.200 --> 00:45:16.360]   ConfNext tiny and ConfNext base.
[00:45:16.360 --> 00:45:18.680]   The gray bar is the ConfNext base.
[00:45:18.680 --> 00:45:21.120]   Blue bar is ConfNext tiny.
[00:45:21.120 --> 00:45:28.080]   So this uses 4.5 GigaFLOPs, which is the exact number that Swin transformer, our chief competitor
[00:45:28.080 --> 00:45:31.680]   does.
[00:45:31.680 --> 00:45:33.280]   And it's a little bit more accurate.
[00:45:33.280 --> 00:45:38.960]   So it's 0.7% more accurate, which is quite good considering that this is just a CNN model
[00:45:38.960 --> 00:45:42.320]   and doesn't use attention.
[00:45:42.320 --> 00:45:51.680]   So we looked at a ResNet, applied these decisions and beat a transformer model in terms of accuracy.
[00:45:51.680 --> 00:45:58.020]   I really loved how the authors actually outlined every single step.
[00:45:58.020 --> 00:46:04.680]   So now let's continue our reading further.
[00:46:04.680 --> 00:46:10.280]   The family of ConfNext have all of these models.
[00:46:10.280 --> 00:46:15.480]   ConfNext tiny, small, base, large, extra large.
[00:46:15.480 --> 00:46:17.600]   I'm super happy that I discovered this fact.
[00:46:17.600 --> 00:46:22.080]   I asked my colleague Shomik who told me these facts, and I was just scratching my head trying
[00:46:22.080 --> 00:46:23.980]   to understand what these names mean.
[00:46:23.980 --> 00:46:27.200]   So if like me, you don't know them, now you do.
[00:46:27.200 --> 00:46:36.340]   The key thing to note here is these just differ in the number of channels and number of blocks.
[00:46:36.340 --> 00:46:41.320]   If you were to compare tiny with base, small, base, extra, etc.
[00:46:41.320 --> 00:46:43.840]   That's the key difference there.
[00:46:43.840 --> 00:46:46.480]   So now they talk about the training settings.
[00:46:46.480 --> 00:46:47.480]   They use AdamW.
[00:46:47.480 --> 00:46:50.580]   They line out the learning rate.
[00:46:50.580 --> 00:46:54.040]   They say that they use a linear form up and cosine decay.
[00:46:54.040 --> 00:47:00.000]   I would say, I was discussing this with a few colleagues and they say, isn't this too
[00:47:00.000 --> 00:47:01.000]   much detail?
[00:47:01.000 --> 00:47:06.400]   I would rather have this level of detail than try to train the model myself and not have
[00:47:06.400 --> 00:47:09.320]   the details and then learn it through the hard way.
[00:47:09.320 --> 00:47:14.840]   So the authors mentioned all of these facts that they use a learning rate of 4 ENEC 3,
[00:47:14.840 --> 00:47:18.040]   20 epoch linear warm up, a batch size of 4096.
[00:47:18.040 --> 00:47:21.080]   They had a lot of GPUs, I would imagine.
[00:47:21.080 --> 00:47:27.480]   A weight decay of so and so and they apply all of these image augmentation.
[00:47:27.480 --> 00:47:36.160]   Mix up, cut mix, random augment, random erasing and they apply exponential moving average
[00:47:36.160 --> 00:47:39.880]   as well.
[00:47:39.880 --> 00:47:46.440]   Compared and after doing all of this, they also fine tune on image rate 1K and they conclude
[00:47:46.440 --> 00:47:52.240]   that compared to VITs or certain transformer, Connext are simpler to fine tune at different
[00:47:52.240 --> 00:47:57.880]   resolution since the model is fully convolution and there is no need to adjust the batch size.
[00:47:57.880 --> 00:48:04.080]   This is again one of the key improvements over, I would say, transformer models.
[00:48:04.080 --> 00:48:10.280]   Let me see if there are any questions.
[00:48:10.280 --> 00:48:14.840]   Himanshu is asking, is there a concrete example where JLU leads to a better performance?
[00:48:14.840 --> 00:48:17.080]   No, there is not.
[00:48:17.080 --> 00:48:19.760]   At least I couldn't find it in my searches.
[00:48:19.760 --> 00:48:24.600]   Maybe other people could answer this, but I'm not too clear on this.
[00:48:24.600 --> 00:48:26.200]   Thanks for that question though.
[00:48:26.200 --> 00:48:30.080]   So finally, they outline all of these results in this table.
[00:48:30.080 --> 00:48:34.240]   I'll skip over this because again, these are smaller examples and you can check out the
[00:48:34.240 --> 00:48:36.120]   results we were here to on.
[00:48:36.120 --> 00:48:42.360]   I was I was here to highlight how did this architecture make a difference and what steps
[00:48:42.360 --> 00:48:43.480]   went into this.
[00:48:43.480 --> 00:48:47.520]   So I'm skipping over that.
[00:48:47.520 --> 00:48:52.240]   Then they compare isotropic Connext versus VIT.
[00:48:52.240 --> 00:48:56.040]   Isotropic architectures, they say, have no downsampling layers and keep the same feature
[00:48:56.040 --> 00:48:58.080]   resolution at all depths.
[00:48:58.080 --> 00:49:04.300]   They construct isotropic Connext with the same feature dimensions as VIT and they outline
[00:49:04.300 --> 00:49:08.900]   further more details, which leads them to observing that Connext can perform generally
[00:49:08.900 --> 00:49:14.000]   on par with VIT for such cases as well.
[00:49:14.000 --> 00:49:22.700]   Now they also take Connext and so far we've just been looking at image classification.
[00:49:22.700 --> 00:49:26.080]   Now we graduate to image segmentation and other things.
[00:49:26.080 --> 00:49:32.600]   So they say when scaled up to bigger models, pre-trained on ImageNet, it leads to better
[00:49:32.600 --> 00:49:38.840]   performance on these.
[00:49:38.840 --> 00:49:40.580]   Actually I'm mixing two things.
[00:49:40.580 --> 00:49:49.400]   When scaled to bigger models, Connext is significantly better.
[00:49:49.400 --> 00:49:51.920]   Is this on a different dataset?
[00:49:51.920 --> 00:49:53.600]   I'm trying to make sure.
[00:49:53.600 --> 00:49:54.840]   I believe so.
[00:49:54.840 --> 00:50:01.400]   So what they're doing here is, let's say you have your Swin model, which you're comparing
[00:50:01.400 --> 00:50:02.820]   against.
[00:50:02.820 --> 00:50:17.020]   What you do is we take the Connext, we pre-train it on ImageNet 22k and now we fine tune it
[00:50:17.020 --> 00:50:18.820]   towards the target task.
[00:50:18.820 --> 00:50:27.860]   So they say using this approach, there is a significantly better improvement.
[00:50:27.860 --> 00:50:36.800]   Now they come to model efficiency, which looks at flops and throughput of the model.
[00:50:36.800 --> 00:50:40.560]   So how many images can you pass through the model?
[00:50:40.560 --> 00:50:45.980]   They say Connext requires less memory than Swin transformer and the throughputs are comparable
[00:50:45.980 --> 00:50:50.740]   or exceed Swin transformer.
[00:50:50.740 --> 00:50:55.780]   They claim that this comes from the inductive bias, meaning the sliding window, the blue
[00:50:55.780 --> 00:50:59.580]   animation we had seen earlier.
[00:50:59.580 --> 00:51:07.140]   I'll skip over this section.
[00:51:07.140 --> 00:51:10.900]   And I want to highlight a few things here.
[00:51:10.900 --> 00:51:17.020]   In the experimental settings for pre-training, they say EMA severely hurts models with batch
[00:51:17.020 --> 00:51:18.760]   norm layers.
[00:51:18.760 --> 00:51:30.440]   This is one of the reasons why they use layer norm.
[00:51:30.440 --> 00:51:37.220]   And they say Connext large pre-trained on ImageNet 1k is the exception where accuracy
[00:51:37.220 --> 00:51:44.520]   significantly lower than EMA accuracy because it overfits to that.
[00:51:44.520 --> 00:51:48.560]   Furthermore, they highlight different details.
[00:51:48.560 --> 00:51:52.800]   I'm going to skip over them.
[00:51:52.800 --> 00:51:59.160]   And in one of the evaluations, they mentioned with extra ImageNet 22k data, Connext Excel
[00:51:59.160 --> 00:52:03.960]   demonstrates strong generalization capabilities outside of ImageNet.
[00:52:03.960 --> 00:52:10.640]   Our end goal always is to find architectures that perform well outside of ImageNet.
[00:52:10.640 --> 00:52:15.760]   But the key thing to note here is these are extra large and large versions of the model.
[00:52:15.760 --> 00:52:20.040]   So please take that with a grain of salt because they might be difficult to use.
[00:52:20.040 --> 00:52:25.120]   Sorry, it's just Connext extra large, which is quite more accurate.
[00:52:25.120 --> 00:52:29.680]   So if you're on a Kaggle competition trying to squeeze out that accuracy, by all means
[00:52:29.680 --> 00:52:33.960]   use that.
[00:52:33.960 --> 00:52:49.960]   One more interesting thing in the appendix, I want to say, ImageNet Connext are 49% faster
[00:52:49.960 --> 00:52:57.720]   because they can effectively use FP16 models.
[00:52:57.720 --> 00:53:06.240]   So they can effectively use half precision training, which leads into performing better.
[00:53:06.240 --> 00:53:09.880]   I believe that's all I wanted to cover from this paper.
[00:53:09.880 --> 00:53:13.600]   So to summarize, we started with this question.
[00:53:13.600 --> 00:53:16.640]   Sorry for the nauseatic scrolling there.
[00:53:16.640 --> 00:53:21.560]   We started with this question, how do we take decisions in a transformer, apply it to a
[00:53:21.560 --> 00:53:24.240]   CNN and make it perform better?
[00:53:24.240 --> 00:53:29.360]   The authors outlined every single step, they applied it to a ResNet-50.
[00:53:29.360 --> 00:53:33.280]   So this speaks to the power of a ResNet model as well.
[00:53:33.280 --> 00:53:40.200]   They apply all of these steps to a ResNet model, these different decisions.
[00:53:40.200 --> 00:53:48.000]   And all of these lead to very much increased accuracy compared to Swin transformer.
[00:53:48.000 --> 00:53:53.680]   That might be a bit exaggerated, but it's 0.7% higher accuracy, which still is a decent
[00:53:53.680 --> 00:53:57.520]   number, I would argue, since these are just CNNs.
[00:53:57.520 --> 00:54:04.640]   So CNNs do beat transformers in a way by doing that.
[00:54:04.640 --> 00:54:09.080]   I'll switch sharing and start taking any questions.
[00:54:09.080 --> 00:54:19.240]   I have covered all of the details I wanted to hear.
[00:54:19.240 --> 00:54:21.960]   So here's some suggested homework.
[00:54:21.960 --> 00:54:25.040]   And again, as a reminder, please consider asking any questions.
[00:54:25.040 --> 00:54:29.400]   Now is the best time to do that.
[00:54:29.400 --> 00:54:32.560]   They swap out Jelu with ReLU.
[00:54:32.560 --> 00:54:34.120]   I would say swap out Jelu.
[00:54:34.120 --> 00:54:36.200]   Sorry, they swap out ReLU with Jelu.
[00:54:36.200 --> 00:54:40.680]   I would say try with ReLU and see if you can improve the accuracy.
[00:54:40.680 --> 00:54:44.200]   Just try that as an experiment, see what that leads to.
[00:54:44.200 --> 00:54:46.280]   Think of other architectures we can modernize.
[00:54:46.280 --> 00:54:47.600]   I know this is a long stretch.
[00:54:47.600 --> 00:54:48.600]   We're not researchers.
[00:54:48.600 --> 00:54:53.600]   I'm definitely not a researcher, but I trust you all to be better equipped than I am.
[00:54:53.600 --> 00:54:58.920]   Please try modernizing other architectures or just considering it.
[00:54:58.920 --> 00:55:02.320]   Try your scores on ImageNet.
[00:55:02.320 --> 00:55:04.640]   That might not be the best pronunciation.
[00:55:04.640 --> 00:55:07.440]   I'll show what the data set is.
[00:55:07.440 --> 00:55:11.920]   Don't compare ensemble, try ensembling these models with transformers.
[00:55:11.920 --> 00:55:16.960]   And check out the paper annotations repository I had mentioned earlier.
[00:55:16.960 --> 00:55:37.560]   So now let me switch my screens back to tell you about ImageNet real quick.
[00:55:37.560 --> 00:55:39.000]   Am I looking at the right window?
[00:55:39.000 --> 00:55:40.000]   No, I'm not.
[00:55:40.000 --> 00:55:41.000]   I think I am now.
[00:55:41.000 --> 00:55:42.000]   Yes.
[00:55:42.000 --> 00:55:58.160]   I have a lot of windows open and that's really confusing me.
[00:55:58.160 --> 00:56:06.120]   ImageNet, mixed up pronunciation, but this comes from FastAI.
[00:56:06.120 --> 00:56:12.080]   It's a data set of 10 easily classified classes.
[00:56:12.080 --> 00:56:14.480]   ImageWoof is the one I wanted to suggest.
[00:56:14.480 --> 00:56:16.040]   I apologize.
[00:56:16.040 --> 00:56:19.840]   ImageWoof aren't easy to classify.
[00:56:19.840 --> 00:56:24.200]   They're a little more challenging and they're very much approachable because it's just 10
[00:56:24.200 --> 00:56:25.200]   classes.
[00:56:25.200 --> 00:56:27.400]   It's not 1000 or 22,000 classes.
[00:56:27.400 --> 00:56:32.400]   So as a suggestion for homework, please try applying these techniques to this data set
[00:56:32.400 --> 00:56:36.800]   and see how you perform because it's quite a challenging data set and you already have
[00:56:36.800 --> 00:56:38.520]   a leaderboard to compare.
[00:56:38.520 --> 00:56:42.600]   So I'd invite you all to try that as well.
[00:56:42.600 --> 00:56:44.880]   Let's see if there's anything else I want to mention.
[00:56:44.880 --> 00:56:53.600]   I'll again point everyone to this repository and invite any feedback if you all want of
[00:56:53.600 --> 00:56:55.320]   what papers you would like to see here.
[00:56:55.320 --> 00:57:02.480]   My vision is to have different categories of people covered in this repository where
[00:57:02.480 --> 00:57:07.160]   you can find the summary, find the paper, read it, find collabs.
[00:57:07.160 --> 00:57:11.720]   You can find a collab here, play with it.
[00:57:11.720 --> 00:57:15.480]   And this would cover different subsections of machine learning.
[00:57:15.480 --> 00:57:17.440]   So NLP, computer vision.
[00:57:17.440 --> 00:57:23.720]   I'll be releasing new papers every week, new annotations every week.
[00:57:23.720 --> 00:57:25.800]   Not new papers, I'm not a researcher.
[00:57:25.800 --> 00:57:27.400]   So that's it.
[00:57:27.400 --> 00:57:31.240]   Consider trying all of these homebooks and thanks for joining.
[00:57:31.240 --> 00:57:33.480]   Now I'll check to see if there are any questions.
[00:57:33.480 --> 00:57:57.040]   Awesome.
[00:57:57.040 --> 00:58:01.120]   I'm seeing a bunch of questions from the people who've signed up.
[00:58:01.120 --> 00:58:07.560]   So let me see their Pranav mentions this is their first time joining a paper reading session.
[00:58:07.560 --> 00:58:09.080]   We're happy to have you.
[00:58:09.080 --> 00:58:10.080]   Thank you for joining.
[00:58:10.080 --> 00:58:17.360]   Anupam says great to be a part of the reading group and connexed.
[00:58:17.360 --> 00:58:18.360]   Thank you Anupam.
[00:58:18.360 --> 00:58:19.360]   Amazing.
[00:58:19.360 --> 00:58:30.320]   I don't see any more questions there.
[00:58:30.320 --> 00:58:34.160]   So I'll come back to the screen and see the YouTube questions.
[00:58:34.160 --> 00:58:36.880]   What are the prerequisites before reading this paper?
[00:58:36.880 --> 00:58:44.240]   Again, if you go to this repository, I think this should also be what should you read after
[00:58:44.240 --> 00:58:45.240]   this or before this.
[00:58:45.240 --> 00:58:46.680]   I'll add that column.
[00:58:46.680 --> 00:58:53.600]   But if you're familiar with VAT and Swin transformer, that'll help understand or solidify your knowledge.
[00:58:53.600 --> 00:58:58.320]   But I would say Sumit if you're here, sorry, Sumant, I apologize.
[00:58:58.320 --> 00:59:02.120]   Sumant if you're here, I've covered all of the prerequisites in this video.
[00:59:02.120 --> 00:59:04.880]   So you don't need to read the papers.
[00:59:04.880 --> 00:59:11.000]   And again, hopefully by next week, the idea is to have all of the prerequisites inside
[00:59:11.000 --> 00:59:12.000]   of this repo as well.
[00:59:12.000 --> 00:59:18.520]   So in two minutes, you can find the summary and also the papers.
[00:59:18.520 --> 00:59:22.800]   Is there an intuitive understanding for why there should be a saturation at seven by seven
[00:59:22.800 --> 00:59:25.800]   kernel size?
[00:59:25.800 --> 00:59:29.560]   Yes, I can think of one.
[00:59:29.560 --> 00:59:39.100]   So your images usually live at a resolution of 224, 320, maximum 512.
[00:59:39.100 --> 00:59:43.560]   If you are to apply convolutions there, you can find the formula and check how much of
[00:59:43.560 --> 00:59:44.920]   an area do they cover.
[00:59:44.920 --> 00:59:48.960]   But let's say you're working with 224 by 224 image.
[00:59:48.960 --> 00:59:52.280]   A seven by seven kernel would cover a lot of it, right?
[00:59:52.280 --> 00:59:56.880]   224 by 224 is really small and that just takes a few convolutions.
[00:59:56.880 --> 01:00:01.400]   If you increase the size of the kernel, this is my understanding.
[01:00:01.400 --> 01:00:06.560]   So if you increase the size of the kernel, you don't get as much knowledge from performing
[01:00:06.560 --> 01:00:09.480]   the convolutions.
[01:00:09.480 --> 01:00:14.540]   Or maybe it starts to saturate since again, now you're just covering as much of the image.
[01:00:14.540 --> 01:00:15.540]   That's my understanding.
[01:00:15.540 --> 01:00:19.840]   But thanks for the great question, Himanshu.
[01:00:19.840 --> 01:00:21.920]   Awesome.
[01:00:21.920 --> 01:00:23.200]   I don't see any other questions.
[01:00:23.200 --> 01:00:25.920]   I've been managed to cover everything in time.
[01:00:25.920 --> 01:00:26.920]   Thank you everyone for joining.
[01:00:26.920 --> 01:00:29.240]   I'll wrap up here.
[01:00:29.240 --> 01:00:35.440]   Please let us know if you want to join or want us to cover any specific sessions in
[01:00:35.440 --> 01:00:37.160]   the future, any specific papers.
[01:00:37.160 --> 01:00:42.320]   I'll be covering these according to my interest and I'm just checking the question.
[01:00:42.320 --> 01:00:44.440]   That's why I sound a bit distracted.
[01:00:44.440 --> 01:00:45.440]   But awesome.
[01:00:45.440 --> 01:00:46.440]   I'll conclude here.
[01:00:46.440 --> 01:00:47.440]   Thank you for joining.
[01:00:47.440 --> 01:00:51.980]   Please consider doing the homework and we'll see you all every month for the live stream
[01:00:51.980 --> 01:00:56.800]   and I'll be releasing paper annotations every week for you all to read to keep this more
[01:00:56.800 --> 01:00:57.800]   regular.
[01:00:57.800 --> 01:00:58.300]   Thank you.
[01:00:58.300 --> 01:01:05.300]   Thanks for watching.

